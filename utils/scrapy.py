from typing import List, Tuple, Dict, Optional
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
import re
import json
import arxiv
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

US_EASTERN = ZoneInfo("US/Eastern")

def load_tags(tags_file: str) -> List[str]:
    with open(tags_file, 'r', encoding='utf-8') as f:
        tags = json.load(f)
    return tags['tags']


def load_interests(interests_file: str) -> Dict:
    """åŠ è½½ç”¨æˆ·æ„Ÿå…´è¶£çš„é¢†åŸŸé…ç½®"""
    try:
        with open(interests_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None


def _generate_plural_variants(word: str) -> List[str]:
    """
    ç”Ÿæˆå•è¯çš„å•å¤æ•°å˜ä½“
    æ”¯æŒå¸¸è§çš„è‹±è¯­å•å¤æ•°å˜åŒ–è§„åˆ™
    """
    variants = [word]
    
    # å¦‚æœå·²ç»æ˜¯å¤æ•°å½¢å¼ï¼Œå°è¯•ç”Ÿæˆå•æ•°
    if word.endswith('ies') and len(word) > 3:
        # policies -> policy
        variants.append(word[:-3] + 'y')
    elif word.endswith('es') and len(word) > 2:
        # matches -> match, classes -> class
        variants.append(word[:-2])
        variants.append(word[:-1])  # ä¹Ÿå°è¯•åªå»æ‰ s
    elif word.endswith('s') and not word.endswith('ss') and len(word) > 1:
        # models -> model
        variants.append(word[:-1])
    
    # å¦‚æœæ˜¯å•æ•°å½¢å¼ï¼Œå°è¯•ç”Ÿæˆå¤æ•°
    if word.endswith('y') and len(word) > 1 and word[-2] not in 'aeiou':
        # policy -> policies
        variants.append(word[:-1] + 'ies')
    elif word.endswith(('s', 'x', 'z', 'ch', 'sh')):
        # match -> matches
        variants.append(word + 'es')
    else:
        # model -> models
        variants.append(word + 's')
    
    return list(set(variants))


def _keyword_match(keyword: str, text: str) -> bool:
    """
    æ™ºèƒ½å…³é”®è¯åŒ¹é…ï¼š
    - çŸ­å…³é”®è¯ï¼ˆ<=4å­—ç¬¦ï¼‰ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯åŒ¹é…
    - é•¿å…³é”®è¯ä½¿ç”¨å­ä¸²åŒ¹é…
    - æ”¯æŒè¿å­—ç¬¦å’Œç©ºæ ¼çš„å˜ä½“åŒ¹é…
    - æ”¯æŒå•å¤æ•°å˜ä½“åŒ¹é…
    """
    kw = keyword.lower()
    
    # ç”Ÿæˆå˜ä½“ï¼šè¿å­—ç¬¦ <-> ç©ºæ ¼
    variants = [kw]
    if '-' in kw:
        variants.append(kw.replace('-', ' '))
        variants.append(kw.replace('-', ''))
    if ' ' in kw:
        variants.append(kw.replace(' ', '-'))
        variants.append(kw.replace(' ', ''))
    
    # å¯¹å¤šè¯çŸ­è¯­ï¼Œå¯¹æœ€åä¸€ä¸ªè¯ç”Ÿæˆå•å¤æ•°å˜ä½“
    expanded_variants = []
    for variant in variants:
        expanded_variants.append(variant)
        words = variant.split()
        if words:
            last_word = words[-1]
            plural_forms = _generate_plural_variants(last_word)
            for pf in plural_forms:
                if pf != last_word:
                    new_variant = ' '.join(words[:-1] + [pf]) if len(words) > 1 else pf
                    expanded_variants.append(new_variant)
    
    # å»é‡
    variants = list(set(expanded_variants))
    
    for variant in variants:
        # çŸ­å…³é”®è¯ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼ˆé¿å… "VO" åŒ¹é… "evolution"ï¼‰
        if len(variant) <= 4:
            pattern = r'\b' + re.escape(variant) + r'\b'
            if re.search(pattern, text, re.IGNORECASE):
                return True
        else:
            # é•¿å…³é”®è¯ä½¿ç”¨å­ä¸²åŒ¹é…
            if variant in text:
                return True
    
    return False


def _check_negative_keywords(title: str, abstract: str, negative_keywords: List[str]) -> Tuple[bool, List[str]]:
    """
    æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ¹é…è´Ÿé¢å…³é”®è¯ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
    è¿”å›: (æ˜¯å¦æ’é™¤, åŒ¹é…åˆ°çš„è´Ÿé¢å…³é”®è¯åˆ—è¡¨)
    """
    if not negative_keywords:
        return False, []
    
    text = f"{title} {abstract}".lower()
    matched_negatives = []
    
    for kw in negative_keywords:
        if _keyword_match(kw, text):
            matched_negatives.append(kw)
    
    return len(matched_negatives) > 0, matched_negatives


def _match_concept_groups(title: str, abstract: str, concept_groups: List[Dict], 
                          title_multiplier: float = 3.0, abstract_multiplier: float = 1.0) -> Dict:
    """
    åŒ¹é…æ¦‚å¿µç»„ï¼Œè¿”å›å‘½ä¸­çš„ç»„åŠå…¶å¾—åˆ†
    """
    hit_groups = {}  # group_id -> {name, score, keywords}
    
    for group in concept_groups:
        group_id = group.get("id", "")
        group_name = group.get("name", "")
        keywords = group.get("keywords", [])
        weight = group.get("weight", 1.0)
        
        group_score = 0.0
        matched_keywords = []
        
        for kw in keywords:
            # æ ‡é¢˜åŒ¹é…ï¼šé«˜æƒé‡
            if _keyword_match(kw, title):
                group_score += title_multiplier * weight
                matched_keywords.append(f"[T]{kw}")
            # æ‘˜è¦åŒ¹é…ï¼šåŸºç¡€æƒé‡
            elif _keyword_match(kw, abstract):
                group_score += abstract_multiplier * weight
                matched_keywords.append(kw)
        
        if group_score > 0:
            hit_groups[group_id] = {
                "name": group_name,
                "score": group_score,
                "matched_keywords": matched_keywords
            }
    
    return hit_groups


def _cross_validate(hit_groups: Dict, rules: List[Dict]) -> Tuple[bool, List[str]]:
    """
    äº¤å‰éªŒè¯ï¼šæ£€æŸ¥å‘½ä¸­çš„æ¦‚å¿µç»„æ˜¯å¦æ»¡è¶³è‡³å°‘ä¸€æ¡è§„åˆ™
    è¿”å›: (æ˜¯å¦é€šè¿‡éªŒè¯, æ»¡è¶³çš„è§„åˆ™åç§°åˆ—è¡¨)
    
    è§„åˆ™ç±»å‹ï¼š
    1. äº¤å‰è§„åˆ™ï¼šrequired + any_ofï¼Œéœ€è¦å¤šä¸ªæ¦‚å¿µç»„
    2. å•ç‹¬è§„åˆ™ï¼šrequired + ç©ºçš„ any_ofï¼Œå•ä¸ªæ¦‚å¿µç»„å³å¯é€šè¿‡
    """
    hit_group_ids = set(hit_groups.keys())
    satisfied_rules = []
    
    for rule in rules:
        rule_name = rule.get("name", "")
        required = set(rule.get("required", []))
        any_of = set(rule.get("any_of", []))
        
        # æ£€æŸ¥ required ç»„æ˜¯å¦éƒ½å‘½ä¸­
        if not required.issubset(hit_group_ids):
            continue
        
        # å¦‚æœ any_of ä¸ºç©ºï¼Œåˆ™åªéœ€è¦ required å‘½ä¸­å³å¯ï¼ˆå•ç‹¬è§„åˆ™ï¼‰
        if not any_of:
            satisfied_rules.append(rule_name)
            continue
        
        # æ£€æŸ¥ any_of ç»„æ˜¯å¦è‡³å°‘å‘½ä¸­ä¸€ä¸ªï¼ˆäº¤å‰è§„åˆ™ï¼‰
        if any_of.intersection(hit_group_ids):
            satisfied_rules.append(rule_name)
    
    return len(satisfied_rules) > 0, satisfied_rules


def match_interests(paper: Dict, interests_config: Dict) -> Dict:
    """
    å¤šæ”¯æŸ±åŒ¹é…ç³»ç»Ÿ (v5.1 - ä¹å¤§æ”¯æŸ±)
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. è´Ÿé¢å…³é”®è¯ä¸€ç¥¨å¦å†³ï¼ˆä¸¥æ ¼è¿‡æ»¤ï¼‰
    2. åŒ¹é…æ”¯æŸ±ï¼ˆORé€»è¾‘ï¼‰ï¼šå‘½ä¸­ä»»æ„ä¸€ä¸ªæ”¯æŸ±å³å¯
    3. è®¡ç®—åŠ æƒå¾—åˆ†ï¼ˆæ ‡é¢˜æƒé‡æ›´é«˜ï¼‰
    
    ä¹å¤§æ”¯æŸ±ï¼š
    - æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ï¼ˆç§»åŠ¨ã€æ“ä½œã€Sim2Realã€Loco-Manipulationï¼‰
    - æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ï¼ˆå¼ºåŒ–å­¦ä¹ ã€ç¦»çº¿RLã€DPOã€ç½‘ç»œæ¶æ„ï¼‰
    - æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ï¼ˆæ·±åº¦ä¼°è®¡ã€SLAMã€3DGSã€è¯­ä¹‰ç†è§£ï¼‰
    - æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œï¼ˆMDMã€Text-to-Motionã€Diffusionï¼‰
    - æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº”ï¼ˆHOIã€HSIã€ååº”åˆæˆï¼‰
    - æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é…ï¼ˆHMRã€Egocentricã€Motion Matchingï¼‰
    - æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ï¼ˆHuman-to-Robotã€è·¨ä½“æ€è¿ç§»ï¼‰
    - æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”»ï¼ˆDeepMimicã€AMPã€Character Controlï¼‰
    - æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ï¼ˆVLAã€VLNã€æŒ‡ä»¤è·Ÿéšï¼‰
    """
    if not interests_config:
        return {
            "matched_interests": [], 
            "relevance_score": 0.0, 
            "excluded": False, 
            "exclusion_keywords": [],
            "hit_pillars": []
        }
    
    title = paper.get("title", "").lower()
    abstract = paper.get("summary", "").lower()
    
    # è·å–é…ç½®å‚æ•°
    filter_settings = interests_config.get("filter_settings", {})
    title_multiplier = filter_settings.get("title_multiplier", 3.0)
    abstract_multiplier = filter_settings.get("abstract_multiplier", 1.0)
    
    concept_groups = interests_config.get("concept_groups", [])
    negative_keywords = interests_config.get("negative_keywords", [])
    
    # 1. ä¸¥æ ¼çš„è´Ÿé¢å…³é”®è¯è¿‡æ»¤
    is_negative, matched_negatives = _check_negative_keywords(title, abstract, negative_keywords)
    
    # 2. åŒ¹é…äº”å¤§æ”¯æŸ±ï¼ˆORé€»è¾‘ï¼‰
    hit_groups = _match_concept_groups(title, abstract, concept_groups, 
                                       title_multiplier, abstract_multiplier)
    
    # 3. è®¡ç®—æ€»åˆ†
    total_score = sum(g["score"] for g in hit_groups.values())
    
    # 4. æ„å»ºåŒ¹é…ç»“æœ
    matched = []
    for group_id, group_info in hit_groups.items():
        matched.append({
            "name": group_info["name"],
            "id": group_id,
            "matched_keywords": group_info["matched_keywords"],
            "score": round(group_info["score"], 2)
        })
    
    # 5. åˆ¤æ–­æ˜¯å¦æ’é™¤
    # è´Ÿé¢å…³é”®è¯ä¸¥æ ¼å¦å†³ï¼Œé™¤éåˆ†æ•°æé«˜ï¼ˆ>=3å€é˜ˆå€¼ï¼‰
    should_exclude = False
    if is_negative:
        min_threshold = filter_settings.get("min_relevance_score", 1.5)
        if total_score < min_threshold * 3:
            should_exclude = True
    
    return {
        "matched_interests": matched,
        "relevance_score": round(total_score, 2),
        "excluded": should_exclude,
        "exclusion_keywords": matched_negatives,
        "hit_pillars": list(hit_groups.keys())
    }


def filter_by_interests(papers: List[Dict], interests_file: str = "interests.json") -> List[Dict]:
    """
    å¤šæ”¯æŸ±ç­›é€‰ç³»ç»Ÿ (åŠ¨æ€æ”¯æŒ interests.json ä¸­å®šä¹‰çš„æ‰€æœ‰æ”¯æŸ±)
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. è´Ÿé¢å…³é”®è¯ä¸¥æ ¼è¿‡æ»¤ï¼ˆåŒ»å­¦/é‡‘è/NLPç­‰ï¼‰
    2. OR é€»è¾‘ï¼šå‘½ä¸­ä»»æ„ä¸€ä¸ªæ”¯æŸ±å³ä¿ç•™
    3. åˆ†æ•°é˜ˆå€¼è¿‡æ»¤
    """
    interests_config = load_interests(interests_file)
    
    if not interests_config:
        print("[INFO] æœªæ‰¾åˆ° interests.jsonï¼Œè·³è¿‡å…´è¶£ç­›é€‰")
        return papers
    
    # è·å–é˜ˆå€¼é…ç½®
    filter_settings = interests_config.get("filter_settings", {})
    min_threshold = filter_settings.get("min_relevance_score", 1.5)
    
    filtered = []
    excluded_count = 0
    below_threshold_count = 0
    no_match_count = 0
    
    for paper in papers:
        match_info = match_interests(paper, interests_config)
        paper["matched_interests"] = match_info["matched_interests"]
        paper["relevance_score"] = match_info["relevance_score"]
        paper["hit_pillars"] = match_info.get("hit_pillars", [])
        
        # æ£€æŸ¥æ˜¯å¦è¢«è´Ÿé¢å…³é”®è¯æ’é™¤
        if match_info.get("excluded", False):
            excluded_count += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åˆ†æ•°é˜ˆå€¼ï¼ˆä»»æ„æ”¯æŸ±å‘½ä¸­å³å¯ï¼‰
        if match_info["relevance_score"] >= min_threshold:
            filtered.append(paper)
        elif len(match_info.get("hit_pillars", [])) > 0:
            below_threshold_count += 1
        else:
            no_match_count += 1
    
    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
    filtered.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å¤šæ”¯æŸ±ç­›é€‰ç»Ÿè®¡ (ORé€»è¾‘)")
    print(f"{'='*60}")
    print(f"   åŸå§‹è®ºæ–‡æ•°: {len(papers)}")
    print(f"   âœ… é€šè¿‡ç­›é€‰: {len(filtered)} ç¯‡ (å‘½ä¸­æ”¯æŸ± + åˆ†æ•° â‰¥ {min_threshold})")
    print(f"   âŒ è´Ÿé¢æ’é™¤: {excluded_count} ç¯‡ (åŒ»å­¦/é‡‘è/NLPç­‰)")
    print(f"   âšª åˆ†æ•°ä¸è¶³: {below_threshold_count} ç¯‡")
    print(f"   â¬œ æ— åŒ¹é…: {no_match_count} ç¯‡")
    
    # æ˜¾ç¤ºå„æ”¯æŸ±å‘½ä¸­ç»Ÿè®¡
    pillar_counts = {}
    pillar_scores = {}
    for p in filtered:
        for m in p.get("matched_interests", []):
            name = m["name"]
            pillar_counts[name] = pillar_counts.get(name, 0) + 1
            pillar_scores[name] = pillar_scores.get(name, 0) + m.get("score", 0)
    
    if pillar_counts:
        print(f"\nğŸ“ˆ å„æ”¯æŸ±å‘½ä¸­ç»Ÿè®¡:")
        for name, count in sorted(pillar_counts.items(), key=lambda x: -x[1]):
            avg_score = pillar_scores[name] / count if count > 0 else 0
            print(f"   {name}: {count} ç¯‡ (å¹³å‡åˆ†: {avg_score:.1f})")
    
    # æ˜¾ç¤º Top 5 é«˜åˆ†è®ºæ–‡
    if filtered:
        print(f"\nğŸ† Top 5 é«˜åˆ†è®ºæ–‡:")
        for i, p in enumerate(filtered[:5], 1):
            title = p.get("title", "")[:48]
            score = p.get("relevance_score", 0)
            pillars = p.get("hit_pillars", [])
            pillar_names = [name.split("ï¼š")[1].split(" ")[0] if "ï¼š" in name else name 
                          for name in [m["name"] for m in p.get("matched_interests", [])[:2]]]
            pillar_str = f" [{len(pillars)}ä¸ªæ”¯æŸ±]" if len(pillars) > 1 else ""
            print(f"   {i}. [{score:.1f}åˆ†{pillar_str}] {title}...")
            print(f"      æ”¯æŸ±: {', '.join(pillar_names)}")
    
    print(f"{'='*60}\n")
    
    return filtered

def get_UTC_range() -> Tuple[str, str, str]:
    fmt = "%Y%m%d%H%M"
    
    now_utc = datetime.now(timezone.utc)
    now_et = now_utc.astimezone(US_EASTERN)
    today_et = now_et.date()
    t2000_et = datetime(today_et.year, today_et.month, today_et.day, 20, 0, 0, tzinfo=US_EASTERN)
    
    if now_et < t2000_et:
        end_et = t2000_et - timedelta(days=1, minutes=1)
    else:
        end_et = t2000_et
    if end_et.weekday() in (4, 5):  # Friday or Saturday
        end_et -= timedelta(days=end_et.weekday() - 3, minutes=1)  # Move to Thursday
    
    if end_et.weekday() == 6:
        start_et = end_et - timedelta(days=3, minutes=-1)
    else:
        start_et = end_et - timedelta(days=1, minutes=-1)
    
    return (start_et.astimezone(timezone.utc).strftime(fmt),
            end_et.astimezone(timezone.utc).strftime(fmt),
            end_et.strftime("%Y-%m-%d"))


def extract_code_links(text: str) -> List[Dict[str, str]]:
    """ä»æ–‡æœ¬ä¸­æå–ä»£ç ä»“åº“é“¾æ¥ï¼ˆGitHub, GitLab, Hugging Face ç­‰ï¼‰"""
    patterns = [
        # GitHub
        (r'https?://github\.com/[\w\-\.]+/[\w\-\.]+(?:/[\w\-\.]*)?', 'github'),
        # GitLab
        (r'https?://gitlab\.com/[\w\-\.]+/[\w\-\.]+(?:/[\w\-\.]*)?', 'gitlab'),
        # Hugging Face
        (r'https?://huggingface\.co/[\w\-\.]+(?:/[\w\-\.]+)?', 'huggingface'),
        # é¡¹ç›®ä¸»é¡µï¼ˆå¸¸è§æ¨¡å¼ï¼‰
        (r'https?://[\w\-\.]+\.github\.io/[\w\-\.]+/?', 'project_page'),
    ]
    
    links = []
    seen = set()
    for pattern, link_type in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for url in matches:
            # æ¸…ç† URLï¼ˆç§»é™¤æœ«å°¾çš„æ ‡ç‚¹ï¼‰
            url = re.sub(r'[.,;:!?)}\]]+$', '', url)
            if url not in seen:
                seen.add(url)
                links.append({"url": url, "type": link_type})
    return links


def fetch_arxiv_thumbnail(arxiv_id: str, timeout: int = 10) -> Optional[str]:
    """
    ä» arXiv é¡µé¢æŠ“å–è®ºæ–‡é¢„è§ˆç¼©ç•¥å›¾
    è¿”å›å›¾ç‰‡ URL æˆ– None
    """
    base_id = arxiv_id.split('v')[0]
    abs_url = f"https://arxiv.org/abs/{base_id}"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; ArxivDailyBot/1.0)'
        }
        resp = requests.get(abs_url, headers=headers, timeout=timeout)
        if resp.status_code != 200:
            return None
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # æ–¹å¼1: æŸ¥æ‰¾ og:image meta æ ‡ç­¾
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            return og_image['content']
        
        # æ–¹å¼2: æŸ¥æ‰¾è®ºæ–‡ç¼©ç•¥å›¾ (é€šå¸¸åœ¨ arXiv é¡µé¢çš„ç‰¹å®šä½ç½®)
        # arXiv ä½¿ç”¨ https://arxiv.org/html/{id}/extracted/figure1.png æ ¼å¼
        # æˆ–è€… https://static.arxiv.org/static/browse/0.3.4/images/...
        
        # æŸ¥æ‰¾é¡µé¢ä¸­çš„ç¬¬ä¸€å¼ ç›¸å…³å›¾ç‰‡
        for img in soup.find_all('img'):
            src = img.get('src', '')
            # æ’é™¤å›¾æ ‡å’Œè£…é¥°å›¾ç‰‡
            if any(x in src.lower() for x in ['icon', 'logo', 'button', 'arrow', 'social']):
                continue
            # æ‰¾åˆ°æœ‰æ„ä¹‰çš„å›¾ç‰‡
            if 'arxiv' in src or src.startswith('/'):
                if src.startswith('/'):
                    src = f"https://arxiv.org{src}"
                return src
        
        return None
    except Exception:
        return None


def fetch_thumbnails_batch(papers: List[Dict], max_workers: int = 10) -> List[Dict]:
    """æ‰¹é‡æŠ“å–è®ºæ–‡ç¼©ç•¥å›¾"""
    print(f"[INFO] æ­£åœ¨æŠ“å– {len(papers)} ç¯‡è®ºæ–‡çš„é¢„è§ˆå›¾...")
    
    def fetch_one(paper: Dict) -> Tuple[str, Optional[str]]:
        arxiv_id = paper.get('arxiv_id', '')
        thumb = fetch_arxiv_thumbnail(arxiv_id)
        return arxiv_id, thumb
    
    thumbnails = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_one, p): p for p in papers}
        for future in tqdm(as_completed(futures), total=len(futures), desc="æŠ“å–ç¼©ç•¥å›¾"):
            arxiv_id, thumb = future.result()
            if thumb:
                thumbnails[arxiv_id] = thumb
    
    # æ›´æ–°è®ºæ–‡æ•°æ®
    for paper in papers:
        arxiv_id = paper.get('arxiv_id', '')
        if arxiv_id in thumbnails:
            paper['thumbnail'] = thumbnails[arxiv_id]
    
    print(f"[INFO] æˆåŠŸè·å– {len(thumbnails)}/{len(papers)} ç¯‡è®ºæ–‡çš„é¢„è§ˆå›¾")
    return papers


def _result_to_minimal(r: arxiv.Result) -> Dict:
    arxiv_id = r.get_short_id() if hasattr(r, "get_short_id") else r.entry_id.split("/abs/")[-1]
    authors = [a.name for a in r.authors] if r.authors else []
    
    # è·å–åˆ†ç±»ä¿¡æ¯
    categories = list(r.categories) if r.categories else []
    primary_category = r.primary_category if hasattr(r, "primary_category") and r.primary_category else (categories[0] if categories else "")
    
    # è·å–æ›´å¤šå…ƒæ•°æ®
    summary = (r.summary or "").strip()
    
    # æå–ä»£ç é“¾æ¥
    code_links = extract_code_links(summary)
    
    # å‘å¸ƒå’Œæ›´æ–°æ—¥æœŸ
    published = r.published.strftime("%Y-%m-%d") if r.published else ""
    updated = r.updated.strftime("%Y-%m-%d") if r.updated else ""
    
    # è¯„è®ºä¿¡æ¯ï¼ˆé€šå¸¸åŒ…å«é¡µæ•°ã€ä¼šè®®ç­‰ï¼‰
    comment = (r.comment or "").strip() if hasattr(r, 'comment') and r.comment else ""
    
    # DOI å’ŒæœŸåˆŠå¼•ç”¨
    doi = r.doi if hasattr(r, 'doi') and r.doi else ""
    journal_ref = (r.journal_ref or "").strip() if hasattr(r, 'journal_ref') and r.journal_ref else ""
    
    # PDF URL
    pdf_url = r.pdf_url if hasattr(r, 'pdf_url') else f"https://arxiv.org/pdf/{arxiv_id.split('v')[0]}.pdf"
    
    return {
        "title": (r.title or "").strip().replace("\n", " "),
        "authors": authors,
        "arxiv_id": arxiv_id,
        "summary": summary,
        "categories": categories,
        "primary_category": primary_category,
        "published": published,
        "updated": updated,
        "comment": comment,
        "doi": doi,
        "journal_ref": journal_ref,
        "pdf_url": pdf_url,
        "code_links": code_links,
    }


def _scrape_arxiv_list(tag: str) -> List[Dict]:
    """
    å¤‡ç”¨æ–¹æ³•ï¼šä» arXiv ç½‘é¡µæŠ“å–æœ€æ–°è®ºæ–‡åˆ—è¡¨
    å½“ API è¿”å› 406 é”™è¯¯æ—¶ä½¿ç”¨
    """
    url = f"https://arxiv.org/list/{tag}/new"
    try:
        resp = requests.get(url, timeout=30, headers={
            "User-Agent": "Mozilla/5.0 (compatible; arxiv-daily/1.0)"
        })
        if resp.status_code != 200:
            return []
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        papers = []
        
        # æŸ¥æ‰¾è®ºæ–‡æ¡ç›®
        for dt in soup.find_all('dt'):
            dd = dt.find_next_sibling('dd')
            if not dd:
                continue
            
            # æå– arXiv ID
            span = dt.find('a', {'title': 'Abstract'})
            if not span:
                continue
            arxiv_id = span.text.strip().replace('arXiv:', '')
            
            # æå–æ ‡é¢˜
            title_div = dd.find('div', class_='list-title')
            title = title_div.get_text(strip=True).replace('Title:', '').strip() if title_div else ""
            
            # æå–ä½œè€…
            authors_div = dd.find('div', class_='list-authors')
            authors = []
            if authors_div:
                for a in authors_div.find_all('a'):
                    authors.append(a.get_text(strip=True))
            
            # æå–æ‘˜è¦ï¼ˆç®€çŸ­ç‰ˆæœ¬ï¼‰
            abstract_p = dd.find('p', class_='mathjax')
            abstract = abstract_p.get_text(strip=True) if abstract_p else ""
            
            # æå–åˆ†ç±»
            subjects_div = dd.find('div', class_='list-subjects')
            categories = []
            primary_category = tag
            if subjects_div:
                text = subjects_div.get_text()
                # æå–æ‹¬å·ä¸­çš„åˆ†ç±»ä»£ç 
                matches = re.findall(r'([a-z]+\.[A-Z]+)', text)
                categories = matches if matches else [tag]
                primary_category = categories[0] if categories else tag
            
            papers.append({
                "title": title,
                "authors": authors,
                "arxiv_id": arxiv_id,
                "summary": abstract,
                "categories": categories,
                "primary_category": primary_category,
                "published": datetime.now().strftime("%Y-%m-%d"),
                "updated": datetime.now().strftime("%Y-%m-%d"),
                "comment": "",
                "doi": "",
                "journal_ref": "",
                "pdf_url": f"https://arxiv.org/pdf/{arxiv_id}",
                "code_links": [],
            })
        
        return papers
    except Exception as e:
        print(f"  [ERROR] ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
        return []


def query_arxiv(tags: List[str], time_range: Tuple[str, str], max_results: int = 500, fetch_thumbnails: bool = False) -> Dict:
    """
    åˆ†æ‰¹æŸ¥è¯¢ arXiv APIï¼ˆé¿å… HTTP 406 é”™è¯¯ï¼‰
    æ¯ä¸ªåˆ†ç±»å•ç‹¬æŸ¥è¯¢ï¼Œç„¶ååˆå¹¶å»é‡
    å¦‚æœ API å¤±è´¥ï¼Œè‡ªåŠ¨å›é€€åˆ°ç½‘é¡µæŠ“å–
    """
    import time
    start, end = time_range
    
    client = arxiv.Client(page_size=100, delay_seconds=1.5, num_retries=5)
    
    seen = set()
    papers = []
    api_failed = False
    print(f"[INFO] æ­£åœ¨ä» arXiv æŠ“å–è®ºæ–‡ï¼ˆåˆ† {len(tags)} ä¸ªåˆ†ç±»ï¼‰...")
    
    # åˆ†æ‰¹æŸ¥è¯¢æ¯ä¸ªåˆ†ç±»
    for tag in tags:
        query = f"cat:{tag} AND submittedDate:[{start} TO {end}]"
        
        search = arxiv.Search(
            query=query,
            max_results=max_results // len(tags) + 50,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending,
        )
        
        tag_count = 0
        try:
            for r in tqdm(client.results(search), desc=f"  {tag}", unit="paper", leave=False):
                item = _result_to_minimal(r)
                if item["arxiv_id"] in seen:
                    continue
                seen.add(item["arxiv_id"])
                papers.append(item)
                tag_count += 1
        except Exception as e:
            if "406" in str(e):
                api_failed = True
                print(f"  [WARN] API è¿”å› 406ï¼Œå°†ä½¿ç”¨ç½‘é¡µæŠ“å–")
                break
            print(f"  [WARN] {tag} æŸ¥è¯¢å¤±è´¥: {e}")
        
        if not api_failed:
            print(f"  âœ“ {tag}: {tag_count} ç¯‡")
        time.sleep(1.0)
    
    # å¦‚æœ API å¤±è´¥ï¼Œå›é€€åˆ°ç½‘é¡µæŠ“å–
    if api_failed:
        print(f"\n[INFO] åˆ‡æ¢åˆ°ç½‘é¡µæŠ“å–æ¨¡å¼...")
        seen = set()
        papers = []
        for tag in tags:
            tag_papers = _scrape_arxiv_list(tag)
            tag_count = 0
            for p in tag_papers:
                if p["arxiv_id"] in seen:
                    continue
                seen.add(p["arxiv_id"])
                papers.append(p)
                tag_count += 1
            print(f"  âœ“ {tag}: {tag_count} ç¯‡")
            time.sleep(0.5)
    
    # å¯é€‰ï¼šæ‰¹é‡æŠ“å–ç¼©ç•¥å›¾
    if fetch_thumbnails and papers:
        papers = fetch_thumbnails_batch(papers)
    
    print(f"[OK] å…±è·å– {len(papers)} ç¯‡è®ºæ–‡ï¼ˆå»é‡åï¼‰")
    return {"count": len(papers), "papers": papers}


def get_today_arxiv(tags: List[str], max_results: int = 500, fetch_thumbnails: bool = False) -> Tuple[Dict, str]:
    start, end, label_date = get_UTC_range()
    return query_arxiv(tags, (start, end), max_results=max_results, fetch_thumbnails=fetch_thumbnails), label_date


if __name__ == "__main__":
    tags = load_tags('../tags.json')
    result, label_date = get_today_arxiv(tags, fetch_thumbnails=True)
    print(f'Tags: {tags}')
    print(f"Date: {label_date}, Found {result['count']} papers")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    if result['papers']:
        p = result['papers'][0]
        print(f"\nç¤ºä¾‹è®ºæ–‡:")
        print(f"  æ ‡é¢˜: {p['title'][:60]}...")
        print(f"  å‘å¸ƒ: {p['published']}")
        print(f"  ä»£ç : {p['code_links']}")
        print(f"  ç¼©ç•¥å›¾: {p.get('thumbnail', 'N/A')}")
