#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸ºå·²æœ‰çš„ AI åˆ†æç»“æœè¡¥å……å›¾ç‰‡

ç”¨æ³•:
    python add_images.py data/2025-09/ai_summary.json           # ä¸ºæŒ‡å®šæ–‡ä»¶è¡¥å……å›¾ç‰‡
    python add_images.py data/2025-09                           # ä¸ºæŒ‡å®šç›®å½•ä¸‹çš„ ai_summary.json è¡¥å……å›¾ç‰‡
    python add_images.py data/2025-09 --max-images 5            # æ¯ç¯‡è®ºæ–‡æœ€å¤šæå–5å¼ å›¾ç‰‡
    python add_images.py data/2025-09 --concurrency 10          # ä½¿ç”¨10ä¸ªå¹¶å‘
    python add_images.py data/2025-09 --skip-existing           # è·³è¿‡å·²æœ‰å›¾ç‰‡çš„è®ºæ–‡
"""

import argparse
import json
import os
import sys

from utils.image_extractor import batch_extract_images


def main():
    parser = argparse.ArgumentParser(description="ä¸ºå·²æœ‰çš„ AI åˆ†æç»“æœè¡¥å……å›¾ç‰‡")
    parser.add_argument("path", type=str,
                       help="ai_summary.json æ–‡ä»¶è·¯å¾„æˆ–åŒ…å«è¯¥æ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--max-images", type=int, default=3,
                       help="æ¯ç¯‡è®ºæ–‡æœ€å¤šæå–å›¾ç‰‡æ•° (é»˜è®¤: 3)")
    parser.add_argument("--concurrency", type=int, default=5,
                       help="å¹¶å‘æ•° (é»˜è®¤: 5)")
    parser.add_argument("--skip-existing", action="store_true",
                       help="è·³è¿‡å·²æœ‰å›¾ç‰‡çš„è®ºæ–‡")
    
    args = parser.parse_args()
    
    # ç¡®å®šæ–‡ä»¶è·¯å¾„
    if os.path.isdir(args.path):
        json_path = os.path.join(args.path, "ai_summary.json")
    else:
        json_path = args.path
    
    if not os.path.exists(json_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        sys.exit(1)
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ åŠ è½½: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    papers = data.get("papers", [])
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®")
        sys.exit(1)
    
    print(f"[INFO] å…± {len(papers)} ç¯‡è®ºæ–‡")
    
    # ç­›é€‰éœ€è¦å¤„ç†çš„è®ºæ–‡
    if args.skip_existing:
        papers_to_process = [p for p in papers if not p.get("figures")]
        print(f"[INFO] è·³è¿‡å·²æœ‰å›¾ç‰‡çš„è®ºæ–‡ï¼Œå‰©ä½™ {len(papers_to_process)} ç¯‡éœ€è¦å¤„ç†")
    else:
        papers_to_process = papers
    
    if not papers_to_process:
        print("âœ… æ‰€æœ‰è®ºæ–‡éƒ½å·²æœ‰å›¾ç‰‡ï¼Œæ— éœ€å¤„ç†")
        return
    
    # æå–å›¾ç‰‡
    print(f"\nğŸ–¼ï¸ æ­£åœ¨æå–è®ºæ–‡å›¾ç‰‡...")
    print(f"[INFO] å¹¶å‘æ•°: {args.concurrency}, æ¯ç¯‡æœ€å¤š: {args.max_images} å¼ ")
    
    image_results = batch_extract_images(
        papers=papers_to_process,
        max_images_per_paper=args.max_images,
        concurrency=args.concurrency
    )
    
    # å°†å›¾ç‰‡ä¿¡æ¯åˆå¹¶åˆ°ç»“æœä¸­
    # åˆ›å»º arxiv_id -> paper çš„æ˜ å°„
    paper_map = {p.get("arxiv_id", ""): p for p in papers}
    
    updated_count = 0
    for arxiv_id, figures in image_results.items():
        if arxiv_id in paper_map:
            paper_map[arxiv_id]["figures"] = figures
            updated_count += 1
    
    img_count = sum(len(v) for v in image_results.values())
    print(f"[OK] æå–å›¾ç‰‡: {img_count} å¼  (æ¥è‡ª {len(image_results)} ç¯‡è®ºæ–‡)")
    
    # ä¿å­˜ç»“æœ
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"[OK] å·²ä¿å­˜: {json_path}")
    
    # ç»Ÿè®¡
    papers_with_figures = sum(1 for p in papers if p.get("figures"))
    print(f"\nâœ… å®Œæˆï¼")
    print(f"   ğŸ“„ æ€»è®ºæ–‡æ•°: {len(papers)}")
    print(f"   ğŸ–¼ï¸ æœ‰å›¾ç‰‡çš„è®ºæ–‡: {papers_with_figures}")
    print(f"   ğŸ“· æœ¬æ¬¡æ–°å¢: {updated_count} ç¯‡è®ºæ–‡çš„å›¾ç‰‡")


if __name__ == '__main__':
    main()

