---
layout: default
title: 3D-LATTE: Latent Space 3D Editing from Textual Instructions
---

# 3D-LATTE: Latent Space 3D Editing from Textual Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00269" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00269v3</a>
  <a href="https://arxiv.org/pdf/2509.00269.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00269v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00269v3', '3D-LATTE: Latent Space 3D Editing from Textual Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maria Parelli, Michael Oechsle, Michael Niemeyer, Federico Tombari, Andreas Geiger

**åˆ†ç±»**: cs.GR, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-12-12)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://mparelli.github.io/3d-latte)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º3D-LATTEä»¥è§£å†³3Dèµ„äº§åŸºäºæŒ‡ä»¤çš„ç¼–è¾‘é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `3Dç¼–è¾‘` `æ½œåœ¨ç©ºé—´` `æ‰©æ•£æ¨¡å‹` `å‡ ä½•æ“ä½œ` `é«˜ä¿çœŸç¼–è¾‘` `è¯­ä¹‰æ“æ§` `æ— è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäº2Då…ˆéªŒçš„3Dèµ„äº§ç¼–è¾‘æ–¹æ³•å­˜åœ¨è§†å›¾ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¼è‡´ç¼–è¾‘è´¨é‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„ç¼–è¾‘æ–¹æ³•ï¼Œç›´æ¥åœ¨3Dæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå‡ ä½•æ“ä½œï¼Œæå‡ç¼–è¾‘æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å½¢çŠ¶å’Œè¯­ä¹‰æ“ä½œä¸­å®ç°äº†é«˜ä¿çœŸå’Œç²¾ç¡®çš„ç¼–è¾‘ï¼Œè¶…è¶Šäº†ç°æœ‰3Dç¼–è¾‘æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šè§†è§’æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬/å›¾åƒåŸºç¡€çš„3Dèµ„äº§ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åŸºäºæŒ‡ä»¤çš„3Dèµ„äº§ç¼–è¾‘è´¨é‡å´è¿œè¿œè½åäºç”Ÿæˆæ¨¡å‹ã€‚ä¸»è¦åŸå› åœ¨äºï¼Œç°æœ‰æ–¹æ³•ä½¿ç”¨çš„2Då…ˆéªŒå­˜åœ¨è§†å›¾ä¸ä¸€è‡´çš„ç¼–è¾‘ä¿¡å·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„ç¼–è¾‘æ–¹æ³•ï¼Œç›´æ¥åœ¨åŸç”Ÿ3Dæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œå…è®¸æˆ‘ä»¬ç›´æ¥æ“æ§3Då‡ ä½•å½¢çŠ¶ã€‚é€šè¿‡å°†ç”Ÿæˆçš„3Dæ³¨æ„åŠ›å›¾ä¸æºå¯¹è±¡è¿›è¡Œèåˆï¼Œå¹¶ç»“åˆå‡ ä½•æ„ŸçŸ¥çš„æ­£åˆ™åŒ–æŒ‡å¯¼ã€å‚…é‡Œå¶åŸŸçš„è°±è°ƒåˆ¶ç­–ç•¥ä»¥åŠ3Då¢å¼ºçš„ç²¾ç‚¼æ­¥éª¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜ä¿çœŸå’Œç²¾ç¡®ç¼–è¾‘æ–¹é¢è¶…è¶Šäº†ä»¥å¾€çš„3Dç¼–è¾‘æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¹¿æ³›çš„å½¢çŠ¶å’Œè¯­ä¹‰æ“ä½œä¸­å®ç°é«˜è´¨é‡ç¼–è¾‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäº2Då…ˆéªŒçš„3Dèµ„äº§ç¼–è¾‘æ–¹æ³•åœ¨è§†å›¾ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå¯¼è‡´ç¼–è¾‘æ•ˆæœä¸ç†æƒ³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„ç¼–è¾‘æ–¹æ³•ï¼Œç›´æ¥åœ¨3Dæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œé€šè¿‡èåˆ3Dæ³¨æ„åŠ›å›¾æ¥å¼•å¯¼ç¼–è¾‘åˆæˆï¼Œä»è€Œå®ç°å¯¹3Då‡ ä½•å½¢çŠ¶çš„ç›´æ¥æ“æ§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ç”Ÿæˆ3Dæ³¨æ„åŠ›å›¾ï¼Œå…¶æ¬¡æ˜¯ä¸æºå¯¹è±¡çš„èåˆï¼Œæœ€åæ˜¯å‡ ä½•æ„ŸçŸ¥çš„æ­£åˆ™åŒ–å’Œå‚…é‡Œå¶åŸŸçš„è°±è°ƒåˆ¶ç­–ç•¥ï¼Œæœ€åè¿›è¡Œ3Då¢å¼ºçš„ç²¾ç‚¼æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œ3Dç¼–è¾‘ï¼Œé¿å…äº†2Då…ˆéªŒå¸¦æ¥çš„è§†å›¾ä¸ä¸€è‡´é—®é¢˜ï¼Œä»è€Œå®ç°æ›´é«˜è´¨é‡çš„ç¼–è¾‘æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å‡ ä½•æ„ŸçŸ¥çš„æ­£åˆ™åŒ–æŒ‡å¯¼å’Œå‚…é‡Œå¶åŸŸçš„è°±è°ƒåˆ¶ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç¼–è¾‘çš„é«˜ä¿çœŸåº¦å’Œç²¾ç¡®æ€§ï¼ŒåŒæ—¶ç»“åˆç²¾ç‚¼æ­¥éª¤ä»¥å¢å¼ºæœ€ç»ˆçš„3Dæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ3D-LATTEåœ¨å¤šç§å½¢çŠ¶å’Œè¯­ä¹‰æ“ä½œä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿ3Dç¼–è¾‘æ–¹æ³•ï¼Œç¼–è¾‘è´¨é‡æé«˜äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨é«˜ä¿çœŸåº¦å’Œç²¾ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®ã€å»ºç­‘è®¾è®¡ç­‰ï¼Œèƒ½å¤Ÿä¸º3Dèµ„äº§çš„å¿«é€Ÿç¼–è¾‘å’Œç”Ÿæˆæä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæå‡åˆ›ä½œæ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨åŒ–è®¾è®¡å’Œä¸ªæ€§åŒ–å®šåˆ¶æ–¹é¢äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the recent success of multi-view diffusion models for text/image-based 3D asset generation, instruction-based editing of 3D assets lacks surprisingly far behind the quality of generation models. The main reason is that recent approaches using 2D priors suffer from view-inconsistent editing signals. Going beyond 2D prior distillation methods and multi-view editing strategies, we propose a training-free editing method that operates within the latent space of a native 3D diffusion model, allowing us to directly manipulate 3D geometry. We guide the edit synthesis by blending 3D attention maps from the generation with the source object. Coupled with geometry-aware regularization guidance, a spectral modulation strategy in the Fourier domain and a refinement step for 3D enhancement, our method outperforms previous 3D editing methods enabling high-fidelity and precise edits across a wide range of shapes and semantic manipulations. Our project webpage is https://mparelli.github.io/3d-latte

