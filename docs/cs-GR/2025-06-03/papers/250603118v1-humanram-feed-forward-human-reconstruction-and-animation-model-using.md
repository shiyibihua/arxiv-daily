---
layout: default
title: HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers
---

# HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03118" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03118v1</a>
  <a href="https://arxiv.org/pdf/2506.03118.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03118v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03118v1', 'HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, Xiaowei Zhou

**åˆ†ç±»**: cs.GR, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03

**å¤‡æ³¨**: Accepted by SIGGRAPH 2025 (Conference Track). Project page: https://zju3dv.github.io/humanram/

**æœŸåˆŠ**: SIGGRAPH 2025 Conference Proceedings

**DOI**: [10.1145/3721238.3730605](https://doi.org/10.1145/3721238.3730605)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://zju3dv.github.io/humanram/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHumanRAMä»¥è§£å†³3Däººç±»é‡å»ºä¸åŠ¨ç”»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `3Dé‡å»º` `äººç±»åŠ¨ç”»` `å˜æ¢å™¨æ¨¡å‹` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Däººç±»é‡å»ºä¸åŠ¨ç”»æ–¹æ³•ä¾èµ–å¤æ‚çš„æ•æ‰æŠ€æœ¯å’Œä¸ªä½“ä¼˜åŒ–ï¼Œæ•ˆç‡ä½ä¸‹ä¸”éš¾ä»¥æ¨å¹¿ã€‚
2. HumanRAMæå‡ºäº†ä¸€ç§å‰é¦ˆæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ˜¾å¼å§¿æ€æ¡ä»¶ï¼Œç»“åˆå˜æ¢å™¨æ¨¡å‹å®ç°é«˜æ•ˆçš„äººç±»é‡å»ºä¸åŠ¨ç”»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHumanRAMåœ¨é‡å»ºç²¾åº¦å’ŒåŠ¨ç”»è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Däººç±»é‡å»ºä¸åŠ¨ç”»æ˜¯è®¡ç®—æœºå›¾å½¢å­¦å’Œè§†è§‰é¢†åŸŸçš„é•¿æœŸç ”ç©¶è¯¾é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤æ‚çš„å¯†é›†è§†è§’æ•æ‰å’Œè€—æ—¶çš„ä¸ªä½“ä¼˜åŒ–è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†HumanRAMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å‰é¦ˆæ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•ç›®æˆ–ç¨€ç–çš„äººç±»å›¾åƒä¸­è¿›è¡Œé€šç”¨çš„äººç±»é‡å»ºå’ŒåŠ¨ç”»ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ˜¾å¼çš„å§¿æ€æ¡ä»¶å¼•å…¥åŸºäºå˜æ¢å™¨çš„å¤§å‹é‡å»ºæ¨¡å‹ä¸­ï¼Œå°†äººç±»é‡å»ºå’ŒåŠ¨ç”»æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHumanRAMåœ¨é‡å»ºç²¾åº¦ã€åŠ¨ç”»ä¿çœŸåº¦å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Däººç±»é‡å»ºä¸åŠ¨ç”»æ–¹æ³•åœ¨æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ä¾èµ–å¤æ‚æ•æ‰å’Œä¸ªä½“ä¼˜åŒ–çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHumanRAMé€šè¿‡å¼•å…¥æ˜¾å¼çš„å§¿æ€æ¡ä»¶ï¼Œåˆ©ç”¨å˜æ¢å™¨æ¨¡å‹å®ç°äº†äººç±»é‡å»ºä¸åŠ¨ç”»çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç®€åŒ–äº†è¾“å…¥è¦æ±‚å¹¶æé«˜äº†å¤„ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å•ç›®æˆ–ç¨€ç–å›¾åƒã€å…³è”çš„ç›¸æœºå‚æ•°å’ŒSMPL-Xå§¿æ€ï¼Œé‡‡ç”¨å¯æ‰©å±•çš„å˜æ¢å™¨å’ŒDPTè§£ç å™¨è¿›è¡Œäººç±»æ¸²æŸ“åˆæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šHumanRAMçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ˜¾å¼å§¿æ€æ¡ä»¶ä¸å¤§å‹é‡å»ºæ¨¡å‹ç»“åˆï¼Œå®ç°äº†é«˜è´¨é‡é‡å»ºä¸é«˜ä¿çœŸåŠ¨ç”»çš„åŒæ—¶æ§åˆ¶ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„åˆ†ç¦»å¤„ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­ä½¿ç”¨äº†å…±äº«çš„SMPL-Xç¥ç»çº¹ç†ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥å¹³è¡¡é‡å»ºä¸åŠ¨ç”»è´¨é‡ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHumanRAMåœ¨é‡å»ºç²¾åº¦ä¸Šæé«˜äº†XX%ï¼Œåœ¨åŠ¨ç”»ä¿çœŸåº¦ä¸Šæå‡äº†YY%ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’ŒåŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸã€‚é€šè¿‡é«˜æ•ˆçš„äººç±»é‡å»ºä¸åŠ¨ç”»ç”Ÿæˆï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D human reconstruction and animation are long-standing topics in computer graphics and vision. However, existing methods typically rely on sophisticated dense-view capture and/or time-consuming per-subject optimization procedures. To address these limitations, we propose HumanRAM, a novel feed-forward approach for generalizable human reconstruction and animation from monocular or sparse human images. Our approach integrates human reconstruction and animation into a unified framework by introducing explicit pose conditions, parameterized by a shared SMPL-X neural texture, into transformer-based large reconstruction models (LRM). Given monocular or sparse input images with associated camera parameters and SMPL-X poses, our model employs scalable transformers and a DPT-based decoder to synthesize realistic human renderings under novel viewpoints and novel poses. By leveraging the explicit pose conditions, our model simultaneously enables high-quality human reconstruction and high-fidelity pose-controlled animation. Experiments show that HumanRAM significantly surpasses previous methods in terms of reconstruction accuracy, animation fidelity, and generalization performance on real-world datasets. Video results are available at https://zju3dv.github.io/humanram/.

