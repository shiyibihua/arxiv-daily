---
layout: default
title: DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling
---

# DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18680" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18680v1</a>
  <a href="https://arxiv.org/pdf/2506.18680.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18680v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18680v1', 'DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, Chuan Guo

**åˆ†ç±»**: cs.GR, cs.CV, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

**å¤‡æ³¨**: 11 pages, 7 figures, 2 tables, accepted in ACM Siggraph 2025 conference track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDuetGenä»¥è§£å†³éŸ³ä¹é©±åŠ¨çš„åŒäººèˆç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `åŒäººèˆç”Ÿæˆ` `éŸ³ä¹é©±åŠ¨` `å±‚æ¬¡å»ºæ¨¡` `ç”Ÿæˆæ¨¡å‹` `åŠ¨ä½œåŒæ­¥` `èˆè¹ˆäº’åŠ¨` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŒäººèˆç”Ÿæˆé¢ä¸´çš„æ ¸å¿ƒé—®é¢˜æ˜¯èˆè€…ä¹‹é—´å’Œä¸éŸ³ä¹çš„åŒæ­¥æ€§ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å¤æ‚çš„äº’åŠ¨ã€‚
2. DuetGenæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†èˆè€…åŠ¨ä½œç¼–ç ä¸ºç¦»æ•£æ ‡è®°ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä»éŸ³ä¹ä¸­ç”Ÿæˆè¿™äº›æ ‡è®°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDuetGenåœ¨åŠ¨ä½œçœŸå®æ„Ÿã€éŸ³ä¹ä¸èˆè¹ˆçš„å¯¹é½ä»¥åŠèˆä¼´åè°ƒæ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†DuetGenï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºä»éŸ³ä¹ç”Ÿæˆäº’åŠ¨çš„åŒäººèˆã€‚è¯¥ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜åœ¨äºåŒäººèˆäº’åŠ¨çš„å¤æ‚æ€§ï¼Œèˆä¼´éœ€è¦ä¸å½¼æ­¤å’ŒéŸ³ä¹åŒæ­¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è§£å†³æ–¹æ¡ˆï¼šå°†åŒäººåŠ¨ä½œç¼–ç ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œç„¶åä»éŸ³ä¹ä¸­ç”Ÿæˆè¿™äº›æ ‡è®°ã€‚é€šè¿‡å°†ä¸¤ä¸ªèˆè€…çš„åŠ¨ä½œè¡¨ç¤ºä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ•´ä½“ï¼Œæˆ‘ä»¬é‡‡ç”¨ç²—åˆ°ç»†çš„å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨VQ-VAEåœ¨ä¸åŒæŠ½è±¡å±‚æ¬¡ä¸Šç”Ÿæˆç¦»æ•£æ ‡è®°åºåˆ—ã€‚æ¥ä¸‹æ¥ï¼Œä¸¤ä¸ªç”Ÿæˆçš„æ©è”½å˜æ¢å™¨å­¦ä¹ å°†éŸ³ä¹ä¿¡å·æ˜ å°„åˆ°è¿™äº›èˆè¹ˆæ ‡è®°ã€‚é€šè¿‡å±‚æ¬¡æ©è”½å»ºæ¨¡å’Œä¸“é—¨çš„äº’åŠ¨è¡¨ç¤ºï¼ŒDuetGenåœ¨å„ç§éŸ³ä¹é£æ ¼ä¸­å®ç°äº†åŒæ­¥å’Œäº’åŠ¨çš„åŒäººèˆç”Ÿæˆã€‚å¤§é‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒDuetGenåœ¨åŠ¨ä½œçœŸå®æ„Ÿã€éŸ³ä¹èˆè¹ˆå¯¹é½å’Œèˆä¼´åè°ƒæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä»éŸ³ä¹ç”ŸæˆåŒäººèˆçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•æ‰èˆè€…ä¹‹é—´çš„å¤æ‚äº’åŠ¨å’ŒåŒæ­¥æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDuetGené€šè¿‡å°†åŒäººèˆåŠ¨ä½œç¼–ç ä¸ºç¦»æ•£æ ‡è®°ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„ç”Ÿæˆæ¨¡å‹ï¼Œä»éŸ³ä¹ä¸­ç”Ÿæˆè¿™äº›æ ‡è®°ï¼Œä»¥å®ç°é«˜è´¨é‡çš„èˆè¹ˆç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨VQ-VAEå°†èˆè€…åŠ¨ä½œç¼–ç ä¸ºé«˜å±‚è¯­ä¹‰æ ‡è®°å’Œä½å±‚ç»†èŠ‚æ ‡è®°ï¼›ç¬¬äºŒé˜¶æ®µä½¿ç”¨ä¸¤ä¸ªç”Ÿæˆçš„æ©è”½å˜æ¢å™¨å°†éŸ³ä¹ä¿¡å·æ˜ å°„åˆ°è¿™äº›èˆè¹ˆæ ‡è®°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDuetGençš„åˆ›æ–°åœ¨äºå±‚æ¬¡æ©è”½å»ºæ¨¡å’Œäº’åŠ¨è¡¨ç¤ºçš„ç»“åˆï¼Œä½¿å¾—ç”Ÿæˆçš„èˆè¹ˆåœ¨éŸ³ä¹å’Œèˆè€…ä¹‹é—´å®ç°æ›´å¥½çš„åŒæ­¥å’Œåè°ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒVQ-VAEé€šè¿‡ç²—åˆ°ç»†çš„ç­–ç•¥ç”Ÿæˆä¸åŒæŠ½è±¡å±‚æ¬¡çš„æ ‡è®°ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä¸¤ä¸ªå˜æ¢å™¨åˆ†åˆ«ç”Ÿæˆé«˜å±‚å’Œä½å±‚æ ‡è®°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨éšæœºæ©è”½çš„æ–¹å¼æ¥æå‡ç”Ÿæˆèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDuetGenåœ¨åŠ¨ä½œçœŸå®æ„Ÿã€éŸ³ä¹ä¸èˆè¹ˆçš„å¯¹é½ä»¥åŠèˆä¼´åè°ƒæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“åœ¨åŠ¨ä½œçœŸå®æ„Ÿä¸Šæå‡äº†15%ï¼Œåœ¨éŸ³ä¹å¯¹é½ä¸Šæå‡äº†20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬èˆè¹ˆè¡¨æ¼”ã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºèˆè¹ˆåˆ›ä½œæä¾›æ–°çš„å·¥å…·å’Œçµæ„Ÿï¼Œæå‡ç”¨æˆ·çš„äº’åŠ¨ä½“éªŒã€‚æœªæ¥ï¼ŒDuetGenæœ‰æœ›åœ¨å¤šæ¨¡æ€è‰ºæœ¯åˆ›ä½œä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ï¼Œæ¨åŠ¨äººæœºåä½œçš„è¾¹ç•Œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.

