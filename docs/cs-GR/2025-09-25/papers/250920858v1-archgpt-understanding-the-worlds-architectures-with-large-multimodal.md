---
layout: default
title: ArchGPT: Understanding the World's Architectures with Large Multimodal Models
---

# ArchGPT: Understanding the World's Architectures with Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20858" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20858v1</a>
  <a href="https://arxiv.org/pdf/2509.20858.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20858v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20858v1', 'ArchGPT: Understanding the World\'s Architectures with Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuze Wang, Luo Yang, Junyi Wang, Yue Qi

**åˆ†ç±»**: cs.GR, cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ArchGPTï¼šåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç†è§£ä¸–ç•Œå»ºç­‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å»ºç­‘è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®å¢å¼º` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VR/MR/ARç³»ç»Ÿåœ¨å»ºç­‘é¢†åŸŸçš„åº”ç”¨ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œä¾èµ–äºç‰¹å®šæ¡ˆä¾‹å’Œç¡¬ç¼–ç æ³¨é‡Šï¼Œéš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„å»ºç­‘ç¯å¢ƒã€‚
2. ArchGPTæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å»ºç­‘è§†è§‰é—®ç­”æ¨¡å‹ï¼Œå¹¶æ„å»ºäº†å¯æ‰©å±•çš„æ•°æ®æµç¨‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€ç‰¹å®šäºå»ºç­‘çš„VQAæ³¨é‡Šã€‚
3. é€šè¿‡åœ¨Arch-300Kæ•°æ®é›†ä¸Šå¾®è°ƒShareGPT4V-7Bï¼ŒArchGPTåœ¨å»ºç­‘è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœï¼Œä¸ºå»ºç­‘ç†è§£æä¾›äº†æ–°é€”å¾„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å»ºç­‘ä½“ç°äº†ç¾å­¦ã€æ–‡åŒ–å’Œå†å²ä»·å€¼ï¼Œæ˜¯äººç±»æ–‡æ˜çš„åˆ‡å®è¯æ˜ã€‚ç ”ç©¶äººå‘˜é•¿æœŸä»¥æ¥åˆ©ç”¨è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ã€æ··åˆç°å®ï¼ˆMRï¼‰å’Œå¢å¼ºç°å®ï¼ˆARï¼‰æ¥å®ç°å¯¹å»ºç­‘çš„æ²‰æµ¸å¼æ¢ç´¢å’Œè§£è¯»ï¼Œä»è€Œæé«˜æ•™è‚²ã€é—äº§ä¿æŠ¤å’Œä¸“ä¸šè®¾è®¡å®è·µä¸­å»ºç­‘çš„å¯è®¿é—®æ€§ã€å…¬ä¼—ç†è§£å’Œåˆ›æ„å·¥ä½œæµç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VR/MR/ARç³»ç»Ÿé€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šæ¡ˆä¾‹å¼€å‘çš„ï¼Œä¾èµ–äºç¡¬ç¼–ç çš„æ³¨é‡Šå’Œç‰¹å®šäºä»»åŠ¡çš„äº¤äº’ï¼Œæ— æ³•è·¨è¶Šä¸åŒçš„å»ºç­‘ç¯å¢ƒè¿›è¡Œæ‰©å±•ã€‚æœ¬æ–‡æå‡ºäº†ArchGPTï¼Œä¸€ä¸ªå¤šæ¨¡æ€å»ºç­‘è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œç”¨äºç®¡ç†é«˜è´¨é‡çš„ã€ç‰¹å®šäºå»ºç­‘çš„VQAæ³¨é‡Šã€‚è¯¥æµç¨‹ç”Ÿæˆäº†Arch-300Kï¼Œä¸€ä¸ªåŒ…å«çº¦315,000ä¸ªå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„çš„é¢†åŸŸä¸“ç”¨æ•°æ®é›†ã€‚Arch-300Kæ˜¯é€šè¿‡ä¸€ä¸ªå¤šé˜¶æ®µè¿‡ç¨‹æ„å»ºçš„ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä»ç»´åŸºå…±äº«èµ„æºä¸­ç®¡ç†å»ºç­‘åœºæ™¯ï¼Œå¹¶ä½¿ç”¨ä¸€ç§æ–°é¢–çš„ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥è¿‡æ»¤éçº¦æŸçš„æ¸¸å®¢ç…§ç‰‡é›†ï¼Œè¯¥ç­–ç•¥é›†æˆäº†3Dé‡å»ºå’Œè¯­ä¹‰åˆ†å‰²ï¼Œä»¥é€‰æ‹©æ— é®æŒ¡ã€ç»“æ„ä¸€è‡´çš„å»ºç­‘å›¾åƒã€‚ä¸ºäº†å‡è½»åŸå§‹æ–‡æœ¬å…ƒæ•°æ®ä¸­çš„å™ªå£°å’Œä¸ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±LLMå¼•å¯¼çš„æ–‡æœ¬éªŒè¯å’ŒçŸ¥è¯†è’¸é¦æµç¨‹ï¼Œä»¥ç”Ÿæˆå¯é çš„ã€ç‰¹å®šäºå»ºç­‘çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚ä½¿ç”¨è¿™äº›ç²¾å¿ƒç­–åˆ’çš„å›¾åƒå’Œæ”¹è¿›çš„å…ƒæ•°æ®ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç»¼åˆå½¢å¼åˆ†ææ³¨é‡Šâ€”â€”åŒ…æ‹¬è¯¦ç»†çš„æè¿°å’Œæ–¹é¢å¼•å¯¼çš„å¯¹è¯â€”â€”ä»¥æä¾›æ›´ä¸°å¯Œçš„è¯­ä¹‰å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒå¯¹æ•°æ®çš„å¿ å®æ€§ã€‚æˆ‘ä»¬åœ¨Arch-300Kä¸Šå¯¹å¼€æºå¤šæ¨¡æ€éª¨å¹²ç½‘ç»œShareGPT4V-7Bè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»è€Œå¾—åˆ°ArchGPTã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰VR/MR/ARç³»ç»Ÿåœ¨å»ºç­‘é¢†åŸŸåº”ç”¨ä¸­ï¼Œç¼ºä¹å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºé’ˆå¯¹ç‰¹å®šå»ºç­‘çš„ç¡¬ç¼–ç æ³¨é‡Šå’Œä»»åŠ¡ç‰¹å®šäº¤äº’ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„å»ºç­‘ç¯å¢ƒï¼Œä¸”æ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§èƒ½åŠ›ï¼Œç»“åˆé«˜è´¨é‡çš„å»ºç­‘å›¾åƒå’Œæ–‡æœ¬æ•°æ®ï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿç†è§£å’Œå›ç­”å…³äºå»ºç­‘è§†è§‰é—®é¢˜çš„æ¨¡å‹ã€‚é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼ï¼Œé¿å…äº†äººå·¥æ ‡æ³¨çš„å±€é™æ€§ï¼Œæé«˜äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šArchGPTçš„æ„å»ºåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æ•°æ®æ”¶é›†ä¸ç­›é€‰**ï¼šä»Wikimedia Commonsç­‰æ¥æºæ”¶é›†å»ºç­‘å›¾åƒï¼Œå¹¶ä½¿ç”¨åŸºäº3Dé‡å»ºå’Œè¯­ä¹‰åˆ†å‰²çš„ç²—åˆ°ç²¾ç­–ç•¥ï¼Œç­›é€‰å‡ºé«˜è´¨é‡ã€æ— é®æŒ¡çš„å»ºç­‘å›¾åƒã€‚2) **æ•°æ®æ ‡æ³¨ä¸å¢å¼º**ï¼šåˆ©ç”¨LLMå¯¹å›¾åƒçš„åŸå§‹æ–‡æœ¬å…ƒæ•°æ®è¿›è¡ŒéªŒè¯å’ŒçŸ¥è¯†è’¸é¦ï¼Œç”Ÿæˆå¯é çš„ã€ç‰¹å®šäºå»ºç­‘çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚åŒæ—¶ï¼Œåˆæˆå½¢å¼åˆ†ææ³¨é‡Šï¼ŒåŒ…æ‹¬è¯¦ç»†æè¿°å’Œæ–¹é¢å¼•å¯¼çš„å¯¹è¯ï¼Œä»¥å¢åŠ æ•°æ®çš„è¯­ä¹‰å¤šæ ·æ€§ã€‚3) **æ¨¡å‹è®­ç»ƒ**ï¼šåœ¨æ„å»ºçš„Arch-300Kæ•°æ®é›†ä¸Šï¼Œå¯¹å¼€æºå¤šæ¨¡æ€éª¨å¹²ç½‘ç»œShareGPT4V-7Bè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¾—åˆ°ArchGPTæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å»ºç­‘VQAæ•°æ®é›†ï¼Œé™ä½äº†æ•°æ®æ ‡æ³¨æˆæœ¬ã€‚2) åˆ©ç”¨LLMè¿›è¡Œæ–‡æœ¬éªŒè¯å’ŒçŸ¥è¯†è’¸é¦ï¼Œæœ‰æ•ˆå‡è½»äº†åŸå§‹æ–‡æœ¬å…ƒæ•°æ®ä¸­çš„å™ªå£°å’Œä¸ä¸€è‡´æ€§ã€‚3) é€šè¿‡åˆæˆå½¢å¼åˆ†ææ³¨é‡Šï¼Œå¢åŠ äº†æ•°æ®çš„è¯­ä¹‰å¤šæ ·æ€§ï¼Œæå‡äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®ç­›é€‰é˜¶æ®µï¼Œé‡‡ç”¨äº†åŸºäº3Dé‡å»ºå’Œè¯­ä¹‰åˆ†å‰²çš„ç²—åˆ°ç²¾ç­–ç•¥ï¼Œä»¥ç¡®ä¿å›¾åƒçš„ç»“æ„ä¸€è‡´æ€§å’Œæ— é®æŒ¡æ€§ã€‚åœ¨æ•°æ®æ ‡æ³¨é˜¶æ®µï¼Œåˆ©ç”¨LLMè¿›è¡Œæ–‡æœ¬éªŒè¯å’ŒçŸ¥è¯†è’¸é¦ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„promptæ¥ç”Ÿæˆé«˜è´¨é‡çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œé€‰æ‹©äº†ShareGPT4V-7Bä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶é‡‡ç”¨äº†ç›‘ç£å¾®è°ƒçš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†åŒ…å«315,000ä¸ªå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„çš„Arch-300Kæ•°æ®é›†ï¼Œå¹¶é€šè¿‡åœ¨ShareGPT4V-7Bä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°äº†ArchGPTæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArchGPTåœ¨å»ºç­‘è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ArchGPTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå¢å¼ºç°å®å»ºç­‘å¯¼è§ˆã€å»ºç­‘è®¾è®¡è¾…åŠ©ã€å»ºç­‘é—äº§ä¿æŠ¤å’Œæ•™è‚²ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´æ·±å…¥åœ°ç†è§£å»ºç­‘çš„ç»“æ„ã€å†å²å’Œæ–‡åŒ–ä»·å€¼ï¼Œå¹¶ä¸ºå»ºç­‘å¸ˆå’Œè®¾è®¡å¸ˆæä¾›æ›´æ™ºèƒ½çš„è®¾è®¡å·¥å…·ã€‚æœªæ¥ï¼ŒArchGPTæœ‰æœ›æˆä¸ºå»ºç­‘é¢†åŸŸçš„é‡è¦åŸºç¡€è®¾æ–½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Architecture embodies aesthetic, cultural, and historical values, standing as a tangible testament to human civilization. Researchers have long leveraged virtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable immersive exploration and interpretation of architecture, enhancing accessibility, public understanding, and creative workflows around architecture in education, heritage preservation, and professional design practice. However, existing VR/MR/AR systems are often developed case-by-case, relying on hard-coded annotations and task-specific interactions that do not scale across diverse built environments. In this work, we present ArchGPT, a multimodal architectural visual question answering (VQA) model, together with a scalable data-construction pipeline for curating high-quality, architecture-specific VQA annotations. This pipeline yields Arch-300K, a domain-specialized dataset of approximately 315,000 image-question-answer triplets. Arch-300K is built via a multi-stage process: first, we curate architectural scenes from Wikimedia Commons and filter unconstrained tourist photo collections using a novel coarse-to-fine strategy that integrates 3D reconstruction and semantic segmentation to select occlusion-free, structurally consistent architectural images. To mitigate noise and inconsistency in raw textual metadata, we propose an LLM-guided text verification and knowledge-distillation pipeline to generate reliable, architecture-specific question-answer pairs. Using these curated images and refined metadata, we further synthesize formal analysis annotations-including detailed descriptions and aspect-guided conversations-to provide richer semantic variety while remaining faithful to the data. We perform supervised fine-tuning of an open-source multimodal backbone ,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT.

