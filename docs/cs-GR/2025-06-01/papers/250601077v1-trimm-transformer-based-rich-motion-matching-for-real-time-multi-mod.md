---
layout: default
title: TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans
---

# TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01077" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01077v1</a>
  <a href="https://arxiv.org/pdf/2506.01077.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01077v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01077v1', 'TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, Fan Zhang

**åˆ†ç±»**: cs.GR, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01

**å¤‡æ³¨**: 24 pages,12 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTRiMMä»¥è§£å†³å®æ—¶å¤šæ¨¡æ€äº¤äº’ä¸­çš„æ‰‹åŠ¿ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®æ—¶æ‰‹åŠ¿ç”Ÿæˆ` `å¤šæ¨¡æ€äº¤äº’` `è·¨æ¨¡æ€æ³¨æ„åŠ›` `é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡` `æ•°å­—äººç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…±è¯­æ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•åœ¨å®æ—¶åˆæˆå’Œé•¿æ–‡æœ¬ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. TRiMMé€šè¿‡å¼•å…¥è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ã€é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹å’Œå¤§è§„æ¨¡æ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ‰‹åŠ¿ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚
3. åœ¨ZEGGSå’ŒBEATæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒTRiMMåœ¨æ‰‹åŠ¿ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­—äººç±»å¼•å‘äº†ä¸€ç³»åˆ—å…³äºå…±è¯­æ‰‹åŠ¿ç”Ÿæˆç³»ç»Ÿçš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®æ—¶åˆæˆå’Œé•¿æ–‡æœ¬ç†è§£æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ¡†æ¶TRiMMï¼Œæ—¨åœ¨å®ç°å®æ—¶3Dæ‰‹åŠ¿ç”Ÿæˆã€‚è¯¥æ–¹æ³•åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š1ï¼‰è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°è¯­éŸ³ä¸æ‰‹åŠ¿ä¹‹é—´çš„ç²¾ç¡®æ—¶é—´å¯¹é½ï¼›2ï¼‰é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹ï¼Œç»“åˆæ»‘åŠ¨çª—å£æœºåˆ¶è¿›è¡Œæœ‰æ•ˆçš„åºåˆ—å»ºæ¨¡ï¼›3ï¼‰å¤§è§„æ¨¡æ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿï¼Œæ„å»ºåŸå­åŠ¨ä½œåº“å¹¶å®ç°å®æ—¶æ£€ç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¶ˆè´¹çº§GPUä¸Šä»¥120 fpsçš„é€Ÿåº¦å®ç°å®æ—¶æ¨ç†ï¼Œæ¯å¥çš„å»¶è¿Ÿä¸º0.15ç§’ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å®æ—¶å¤šæ¨¡æ€äº¤äº’ä¸­æ‰‹åŠ¿ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿æ–‡æœ¬å’Œå®æ—¶åˆæˆæ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ‰‹åŠ¿ä¸è¯­éŸ³ä¸å¤Ÿåè°ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTRiMMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å®ç°è¯­éŸ³ä¸æ‰‹åŠ¿ä¹‹é—´çš„ç²¾ç¡®æ—¶é—´å¯¹é½ï¼ŒåŒæ—¶ç»“åˆé•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹æ¥å¤„ç†å¤æ‚çš„åºåˆ—ä¿¡æ¯ï¼Œä»¥æé«˜ç”Ÿæˆçš„å®æ—¶æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTRiMMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ã€é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹å’Œå¤§è§„æ¨¡æ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿã€‚è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ç”¨äºå¯¹é½è¯­éŸ³ä¸æ‰‹åŠ¿ï¼Œé•¿ä¸Šä¸‹æ–‡æ¨¡å‹åˆ™é€šè¿‡æ»‘åŠ¨çª—å£å¤„ç†é•¿æ–‡æœ¬ï¼Œè€Œæ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿåˆ™æä¾›å®æ—¶æ£€ç´¢åŠŸèƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTRiMMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œé•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ‰‹åŠ¿ç”Ÿæˆçš„å®æ—¶æ€§å’Œè´¨é‡ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬å’Œå¤æ‚çš„è¯­éŸ³è¾“å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ»‘åŠ¨çª—å£æœºåˆ¶æ¥å¢å¼ºåºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŸå­åŠ¨ä½œåº“ä»¥æ”¯æŒå®æ—¶æ‰‹åŠ¿æ£€ç´¢ã€‚æ¨¡å‹åœ¨æ¶ˆè´¹çº§GPUä¸Šä¼˜åŒ–ï¼Œç¡®ä¿äº†120 fpsçš„å®æ—¶æ¨ç†é€Ÿåº¦å’Œ0.15ç§’çš„æ¯å¥å»¶è¿Ÿã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TRiMMåœ¨ZEGGSå’ŒBEATæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶åœ¨æ‰‹åŠ¿ç”Ÿæˆé€Ÿåº¦ä¸Šè¾¾åˆ°äº†120 fpsï¼Œå¹¶ä¸”æ¯å¥çš„å»¶è¿Ÿä»…ä¸º0.15ç§’ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™ä¸€æ€§èƒ½æå‡ä½¿å¾—æ•°å­—äººç±»èƒ½å¤Ÿå®æ—¶å“åº”è¯­éŸ³å¹¶ç”Ÿæˆç›¸åº”çš„æ‰‹åŠ¿ï¼Œæå¤§åœ°å¢å¼ºäº†äº¤äº’ä½“éªŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TRiMMçš„ç ”ç©¶æˆæœåœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å®ç°å®æ—¶çš„æ‰‹åŠ¿ç”Ÿæˆï¼Œæ•°å­—äººç±»èƒ½å¤Ÿæ›´è‡ªç„¶åœ°ä¸ç”¨æˆ·è¿›è¡Œäº’åŠ¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºæ•™è‚²ã€åŸ¹è®­å’Œå¨±ä¹ç­‰å¤šä¸ªåœºæ™¯ï¼Œæ¨åŠ¨ç›¸å…³è¡Œä¸šçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model (LLM)-driven digital humans have sparked a series of recent studies on co-speech gesture generation systems. However, existing approaches struggle with real-time synthesis and long-text comprehension. This paper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel multi-modal framework for real-time 3D gesture generation. Our method incorporates three modules: 1) a cross-modal attention mechanism to achieve precise temporal alignment between speech and gestures; 2) a long-context autoregressive model with a sliding window mechanism for effective sequence modeling; 3) a large-scale gesture matching system that constructs an atomic action library and enables real-time retrieval. Additionally, we develop a lightweight pipeline implemented in the Unreal Engine for experimentation. Our approach achieves real-time inference at 120 fps and maintains a per-sentence latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive subjective and objective evaluations on the ZEGGS, and BEAT datasets demonstrate that our model outperforms current state-of-the-art methods. TRiMM enhances the speed of co-speech gesture generation while ensuring gesture quality, enabling LLM-driven digital humans to respond to speech in real time and synthesize corresponding gestures. Our code is available at https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching

