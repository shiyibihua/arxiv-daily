---
layout: default
title: Handle-based Mesh Deformation Guided By Vision Language Model
---

# Handle-based Mesh Deformation Guided By Vision Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04562" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04562v2</a>
  <a href="https://arxiv.org/pdf/2506.04562.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04562v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04562v2', 'Handle-based Mesh Deformation Guided By Vision Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingpeng Sun, Shiyang Jia, Zherong Pan, Kui Wu, Aniket Bera

**åˆ†ç±»**: cs.GR, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-08-20)

**å¤‡æ³¨**: 19 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ— è®­ç»ƒæ‰‹æŸ„ç½‘æ ¼å˜å½¢æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `ç½‘æ ¼å˜å½¢` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ— è®­ç»ƒæ–¹æ³•` `3Då†…å®¹æ“ä½œ` `ç”¨æˆ·æ„å›¾å¯¹é½` `å¤šè§†è§’æŠ•ç¥¨` `è‡ªåŠ¨åŒ–è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç½‘æ ¼å˜å½¢æ–¹æ³•è¾“å‡ºè´¨é‡ä½ï¼Œéœ€å¤§é‡æ‰‹åŠ¨è°ƒä¼˜ï¼Œä¸”ä¾èµ–æ•°æ®å¯†é›†å‹è®­ç»ƒã€‚
2. æå‡ºä¸€ç§æ— è®­ç»ƒçš„æ‰‹æŸ„ç½‘æ ¼å˜å½¢æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæç¤ºå·¥ç¨‹ä»¥å®ç°è‡ªåŠ¨åŒ–æ“ä½œã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç”¨æˆ·æ„å›¾å¯¹é½å’Œä½å¤±çœŸæ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç½‘æ ¼å˜å½¢æ˜¯3Då†…å®¹æ“ä½œä¸­çš„åŸºæœ¬å·¥å…·ã€‚å°½ç®¡å·²æœ‰å¤§é‡ç ”ç©¶ï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸è¾“å‡ºè´¨é‡ä½ã€éœ€è¦å¤§é‡æ‰‹åŠ¨è°ƒä¼˜æˆ–ä¾èµ–æ•°æ®å¯†é›†å‹è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ‰‹æŸ„ç½‘æ ¼å˜å½¢æ–¹æ³•ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡æç¤ºå·¥ç¨‹æ¥è§£é‡Šå’Œæ“ä½œåŸºäºæ‰‹æŸ„çš„ç•Œé¢ã€‚æˆ‘ä»¬é¦–å…ˆåº”ç”¨é”¥å½¢å¥‡ç‚¹æ£€æµ‹æ¥è¯†åˆ«ä¸€ç»„ç¨€ç–çš„æ½œåœ¨æ‰‹æŸ„ã€‚ç„¶åï¼ŒVLMè¢«æç¤ºé€‰æ‹©ä¸ç”¨æˆ·æŒ‡ä»¤æœ€ä¸€è‡´çš„å¯å˜å½¢å­éƒ¨åˆ†å’Œæ‰‹æŸ„ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å±å¹•ç©ºé—´æŸ¥è¯¢æ‰€é€‰æ‰‹æŸ„çš„æœŸæœ›å˜å½¢ä½ç½®ã€‚é€šè¿‡å¤šè§†è§’æŠ•ç¥¨æ–¹æ¡ˆå‡å°‘VLMé¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”¨æˆ·æ„å›¾å¯¹é½å’Œä½å¤±çœŸæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç½‘æ ¼å˜å½¢æ–¹æ³•åœ¨è¾“å‡ºè´¨é‡ã€æ‰‹åŠ¨è°ƒä¼˜éœ€æ±‚å’Œæ•°æ®ä¾èµ–æ€§æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸æ— æ³•æ»¡è¶³ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œå¯¼è‡´å˜å½¢æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ‰‹æŸ„ç½‘æ ¼å˜å½¢æ–¹æ³•ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ä½¿å¾—ç”¨æˆ·å¯ä»¥æ›´ç›´è§‚åœ°æ“æ§ç½‘æ ¼å˜å½¢ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„è®­ç»ƒéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬é”¥å½¢å¥‡ç‚¹æ£€æµ‹ä»¥è¯†åˆ«æ½œåœ¨æ‰‹æŸ„ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹é€‰æ‹©å¯å˜å½¢å­éƒ¨åˆ†å’Œæ‰‹æŸ„ï¼Œå¹¶é€šè¿‡å¤šè§†è§’æŠ•ç¥¨æ–¹æ¡ˆæ¥ç¡®å®šæœ€ç»ˆçš„å˜å½¢ä½ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ— è®­ç»ƒçš„ç½‘æ ¼å˜å½¢ï¼Œæ˜¾è‘—æé«˜äº†ç”¨æˆ·äº¤äº’çš„çµæ´»æ€§å’Œå˜å½¢çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ‰‹æŸ„è¯†åˆ«å’Œé€‰æ‹©è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†é”¥å½¢å¥‡ç‚¹æ£€æµ‹ç®—æ³•ï¼ŒVLMçš„æç¤ºè®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿå‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾ã€‚å¤šè§†è§’æŠ•ç¥¨æ–¹æ¡ˆåˆ™æœ‰æ•ˆé™ä½äº†é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨CLIPå’ŒGPTEval3Dè¯„åˆ†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œç”¨æˆ·æ„å›¾å¯¹é½åº¦æ›´é«˜ï¼ŒåŒæ—¶å¼•å…¥çš„è†œèƒ½é‡é‡åŒ–æ˜¾ç¤ºå‡ºä½å¤±çœŸç‰¹æ€§ï¼Œæ•´ä½“å˜å½¢æ•ˆæœæ›´ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œå’Œè™šæ‹Ÿç°å®ç­‰3Då†…å®¹åˆ›ä½œåœºæ™¯ã€‚é€šè¿‡æä¾›ä¸€ç§é«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„ç½‘æ ¼å˜å½¢å·¥å…·ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡åˆ›ä½œè€…çš„å·¥ä½œæ•ˆç‡å’Œåˆ›ä½œè´¨é‡ï¼Œæœªæ¥å¯èƒ½å¯¹3Dè®¾è®¡è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mesh deformation is a fundamental tool in 3D content manipulation. Despite extensive prior research, existing approaches often suffer from low output quality, require significant manual tuning, or depend on data-intensive training. To address these limitations, we introduce a training-free, handle-based mesh deformation method. % Our core idea is to leverage a Vision-Language Model (VLM) to interpret and manipulate a handle-based interface through prompt engineering. We begin by applying cone singularity detection to identify a sparse set of potential handles. The VLM is then prompted to select both the deformable sub-parts of the mesh and the handles that best align with user instructions. Subsequently, we query the desired deformed positions of the selected handles in screen space. To reduce uncertainty inherent in VLM predictions, we aggregate the results from multiple camera views using a novel multi-view voting scheme. % Across a suite of benchmarks, our method produces deformations that align more closely with user intent, as measured by CLIP and GPTEval3D scores, while introducing low distortion -- quantified via membrane energy. In summary, our approach is training-free, highly automated, and consistently delivers high-quality mesh deformations.

