---
layout: default
title: READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation
---

# READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03457" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03457v3</a>
  <a href="https://arxiv.org/pdf/2508.03457.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03457v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03457v3', 'READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haotian Wang, Yuzhe Weng, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Jianqing Gao, Qingfeng Liu

**åˆ†ç±»**: cs.GR, cs.CV, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-11-15)

**å¤‡æ³¨**: Project page: https://readportrait.github.io/READ/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREADæ¡†æ¶ä»¥è§£å†³éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿäººç”Ÿæˆé€Ÿåº¦æ…¢çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `éŸ³é¢‘é©±åŠ¨ç”Ÿæˆ` `è™šæ‹ŸäººæŠ€æœ¯` `æ‰©æ•£æ¨¡å‹` `æ—¶ç©ºå‹ç¼©` `å¤šæ¨¡æ€å¯¹é½` `å¼‚æ­¥è°ƒåº¦` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿäººç”Ÿæˆä¸­æ¨ç†é€Ÿåº¦ææ…¢ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºREADæ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ å‹ç¼©çš„è§†é¢‘æ½œåœ¨ç©ºé—´å’ŒéŸ³é¢‘æ½œåœ¨ç¼–ç ï¼Œå®ç°é«˜æ•ˆçš„è™šæ‹Ÿäººç”Ÿæˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREADåœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹çš„å¼•å…¥ä¸ºéŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿäººç”Ÿæˆå¸¦æ¥äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ¨ç†é€Ÿåº¦ææ…¢é™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºREADï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£-å˜æ¢å™¨çš„å®æ—¶è™šæ‹Ÿäººç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡æ—¶é—´å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ é«˜åº¦å‹ç¼©çš„æ—¶ç©ºè§†é¢‘æ½œåœ¨ç©ºé—´ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆæ‰€éœ€çš„tokenæ•°é‡ã€‚ä¸ºå®ç°éŸ³é¢‘ä¸è§†è§‰çš„æ›´å¥½å¯¹é½ï¼Œæå‡ºäº†é¢„è®­ç»ƒçš„è¯­éŸ³è‡ªç¼–ç å™¨ç”Ÿæˆä¸è§†é¢‘æ½œåœ¨ç©ºé—´å¯¹åº”çš„å‹ç¼©è¯­éŸ³æ½œåœ¨ç¼–ç ã€‚è¿™äº›æ½œåœ¨è¡¨ç¤ºé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£å˜æ¢å™¨è¿›è¡Œå»ºæ¨¡ï¼Œç¡®ä¿ç”Ÿæˆçš„ä¸€è‡´æ€§å’ŒåŠ é€Ÿæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREADåœ¨ç”Ÿæˆç«äº‰åŠ›çš„è™šæ‹Ÿäººè§†é¢‘çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è¿è¡Œæ—¶é—´ï¼Œå®ç°äº†è´¨é‡ä¸é€Ÿåº¦çš„æœ€ä½³å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿäººç”Ÿæˆæ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦å¤„ç†å¤§é‡tokenï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºREADæ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ é«˜åº¦å‹ç¼©çš„æ—¶ç©ºè§†é¢‘æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå‡å°‘ç”Ÿæˆæ‰€éœ€çš„tokenæ•°é‡ï¼Œå¹¶å¼•å…¥é¢„è®­ç»ƒçš„è¯­éŸ³è‡ªç¼–ç å™¨ï¼ˆSpeechAEï¼‰ä»¥å®ç°éŸ³é¢‘ä¸è§†é¢‘çš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šREADæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼š1) æ—¶é—´å˜åˆ†è‡ªç¼–ç å™¨ç”¨äºç”Ÿæˆå‹ç¼©çš„è§†é¢‘æ½œåœ¨ç©ºé—´ï¼›2) é¢„è®­ç»ƒçš„è¯­éŸ³è‡ªç¼–ç å™¨ç”Ÿæˆå¯¹åº”çš„éŸ³é¢‘æ½œåœ¨ç¼–ç ï¼›3) éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼ˆA2V-DiTï¼‰ç”¨äºé«˜æ•ˆåˆæˆè™šæ‹Ÿäººã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ï¼ˆANSï¼‰ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å®ç°å¼‚æ­¥åŠ å™ªå£°å’Œè¿åŠ¨å¼•å¯¼ç”Ÿæˆï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘ç‰‡æ®µçš„ä¸€è‡´æ€§å’ŒåŠ é€Ÿæ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–éŸ³é¢‘ä¸è§†é¢‘çš„å¯¹é½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”å‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼Œç¡®ä¿ç”Ÿæˆè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒREADåœ¨ç”Ÿæˆè™šæ‹Ÿäººè§†é¢‘æ—¶çš„è¿è¡Œæ—¶é—´æ˜¾è‘—ä½äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¿æŒç«äº‰åŠ›ã€‚å…·ä½“è€Œè¨€ï¼ŒREADåœ¨é•¿æ—¶é—´ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§ï¼Œè¿è¡Œæ—¶é—´å‡å°‘äº†çº¦50%ï¼ŒåŒæ—¶ç”Ÿæˆè´¨é‡æœªå—å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åœ¨çº¿æ•™è‚²å’Œç¤¾äº¤åª’ä½“ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´çœŸå®çš„äº¤äº’ä½“éªŒã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒREADæ¡†æ¶æœ‰æœ›åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, a real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference processes of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.

