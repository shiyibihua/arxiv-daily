---
layout: default
title: Show-o2: Improved Native Unified Multimodal Models
---

# Show-o2: Improved Native Unified Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15564" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15564v3</a>
  <a href="https://arxiv.org/pdf/2506.15564.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15564v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15564v3', 'Show-o2: Improved Native Unified Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinheng Xie, Zhenheng Yang, Mike Zheng Shou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: NeurIPS 2025. (v3: update to include video understanding, OneIG, and more ablation study results)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/showlab/Show-o)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºShow-o2ä»¥æå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `è‡ªå›å½’å»ºæ¨¡` `æµåŒ¹é…` `è§†è§‰è¡¨ç¤º` `å›¾åƒç”Ÿæˆ` `è§†é¢‘ç†è§£` `ç©ºé—´-æ—¶é—´èåˆ` `å˜åˆ†è‡ªç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ¨¡æ€æ—¶å­˜åœ¨å¯æ‰©å±•æ€§ä¸è¶³å’Œç†è§£èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºçš„Show-o2æ¨¡å‹é€šè¿‡è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…ï¼Œæ„å»ºç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºï¼Œæå‡äº†å¤šæ¨¡æ€çš„ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒShow-o2åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘å¤„ç†ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æœ¬åœ°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹Show-o2ï¼Œåˆ©ç”¨è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…æŠ€æœ¯ã€‚åœ¨3Då› æœå˜åˆ†è‡ªç¼–ç å™¨ç©ºé—´çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ç©ºé—´-æ—¶é—´èåˆçš„åŒè·¯å¾„æ„å»ºç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºï¼Œç¡®ä¿åœ¨å›¾åƒå’Œè§†é¢‘æ¨¡æ€é—´çš„å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚åŸºäºè¯­è¨€æ¨¡å‹ï¼Œè‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…åˆ†åˆ«åº”ç”¨äºè¯­è¨€å¤´å’Œæµå¤´ï¼Œä»¥ä¿ƒè¿›æ–‡æœ¬æ ‡è®°é¢„æµ‹å’Œå›¾åƒ/è§†é¢‘ç”Ÿæˆã€‚è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥æœ‰æ•ˆå­¦ä¹ å¹¶æ‰©å±•åˆ°æ›´å¤§æ¨¡å‹ã€‚Show-o2æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€çš„ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€é—´çš„å¯æ‰©å±•æ€§ä¸è¶³å’Œç†è§£èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå’Œè§†é¢‘çš„å¤„ç†ä¸Šå­˜åœ¨çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Show-o2æ¨¡å‹é€šè¿‡è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œæ„å»ºç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºï¼Œé‡‡ç”¨åŒè·¯å¾„çš„ç©ºé—´-æ—¶é—´èåˆç­–ç•¥ï¼Œä»¥å®ç°æ›´å¥½çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŸºäº3Då› æœå˜åˆ†è‡ªç¼–ç å™¨ï¼ŒåŒ…å«è¯­è¨€å¤´å’Œæµå¤´ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£æ–‡æœ¬æ ‡è®°é¢„æµ‹å’Œå›¾åƒ/è§†é¢‘ç”Ÿæˆã€‚æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆè¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„æœ‰æ•ˆå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šShow-o2çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…æŠ€æœ¯åŸç”Ÿåº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è‡ªå›å½’å’ŒæµåŒ¹é…çš„æ•ˆæœï¼Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒæ¨¡æ€çš„ç‰¹å¾æå–éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒShow-o2åœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨æ–‡æœ¬ç”Ÿæˆå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Show-o2æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œé€‚ç”¨äºæ–‡æœ¬ç”Ÿæˆã€å›¾åƒæè¿°ã€è§†é¢‘åˆ†æç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆèƒ½åŠ›èƒ½å¤Ÿä¸ºæ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œå’Œè‡ªåŠ¨åŒ–è§†é¢‘ç¼–è¾‘ç­‰å®é™…åº”ç”¨æä¾›æ”¯æŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents improved native unified multimodal models, \emph{i.e.,} Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.

