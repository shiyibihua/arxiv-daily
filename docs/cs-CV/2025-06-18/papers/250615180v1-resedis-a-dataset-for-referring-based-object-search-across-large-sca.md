---
layout: default
title: ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections
---

# ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15180" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15180v1</a>
  <a href="https://arxiv.org/pdf/2506.15180.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15180v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15180v1', 'ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziling Huang, Yidan Zhang, Shin'ichi Satoh

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReSeDisä»¥è§£å†³å¤§è§„æ¨¡å›¾åƒé›†åˆä¸­çš„åŸºäºæè¿°çš„ç‰©ä½“æœç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŸºäºæè¿°çš„ç‰©ä½“æœç´¢` `è§†è§‰æ£€ç´¢` `å¤šæ¨¡æ€èåˆ` `åƒç´ çº§å®šä½` `å¤§è§„æ¨¡æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŠ€æœ¯ä»…è§£å†³è§†è§‰å®šä½æˆ–æ–‡æœ¬æ£€ç´¢ä¸­çš„ä¸€æ–¹é¢ï¼Œå¯¼è‡´åœ¨å¤§è§„æ¨¡å›¾åƒé›†åˆä¸­é¢‘ç¹å‡ºç°å‡è­¦æŠ¥ã€‚
2. æœ¬æ–‡æå‡ºReSeDisä»»åŠ¡ï¼Œç»“åˆè¯­æ–™åº“çº§æ£€ç´¢ä¸åƒç´ çº§å®šä½ï¼Œèƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­æ˜¯å¦å­˜åœ¨æè¿°çš„ç‰©ä½“åŠå…¶ä½ç½®ã€‚
3. é€šè¿‡æ„å»ºç‹¬ç‰¹çš„åŸºå‡†æ•°æ®é›†å’Œè®¾è®¡ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ£€ç´¢å’Œå®šä½ç²¾åº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è§†è§‰æœç´¢å¼•æ“é¢ä¸´åŒé‡æŒ‘æˆ˜ï¼šä¸€æ˜¯å®šä½æ¯ä¸ªå›¾åƒä¸­æ˜¯å¦åŒ…å«æè¿°çš„ç‰©ä½“ï¼ŒäºŒæ˜¯è¯†åˆ«ç‰©ä½“çš„è¾¹ç•Œæ¡†æˆ–ç²¾ç¡®åƒç´ ã€‚ç°æœ‰æŠ€æœ¯ä»…è§£å†³å…¶ä¸­ä¸€æ–¹é¢ï¼Œå¯¼è‡´å‡è­¦æŠ¥é¢‘ç¹ã€‚æœ¬æ–‡æå‡ºäº†Referring Search and Discovery (ReSeDis)ä»»åŠ¡ï¼Œé¦–æ¬¡å°†è¯­æ–™åº“çº§æ£€ç´¢ä¸åƒç´ çº§å®šä½ç»“åˆã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œæ¯ä¸ªæè¿°å”¯ä¸€æ˜ å°„åˆ°åˆ†æ•£åœ¨å¤§å‹å¤šæ ·åŒ–è¯­æ–™åº“ä¸­çš„ç‰©ä½“å®ä¾‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„åº¦é‡æ ‡å‡†ï¼Œè”åˆè¯„ä¼°æ£€ç´¢å¬å›ç‡å’Œå®šä½ç²¾åº¦ã€‚æœ€åï¼Œæä¾›äº†ä¸€ä¸ªåŸºäºå†»ç»“è§†è§‰-è¯­è¨€æ¨¡å‹çš„é›¶-shotåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤§è§„æ¨¡å›¾åƒé›†åˆä¸­ï¼Œå¦‚ä½•å‡†ç¡®å®šä½æè¿°çš„ç‰©ä½“åŠå…¶è¾¹ç•Œæ¡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è§†è§‰å®šä½å’Œæ–‡æœ¬æ£€ç´¢ä¹‹é—´ç¼ºä¹æœ‰æ•ˆçš„ç»“åˆï¼Œå¯¼è‡´å‡è­¦æŠ¥é¢‘ç¹ï¼Œæ— æ³•æ»¡è¶³å®é™…éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReSeDisä»»åŠ¡çš„æ ¸å¿ƒåœ¨äºå°†è¯­æ–™åº“çº§æ£€ç´¢ä¸åƒç´ çº§å®šä½ç›¸ç»“åˆã€‚é€šè¿‡å¯¹æ¯ä¸ªæè¿°è¿›è¡Œå”¯ä¸€æ˜ å°„ï¼Œç¡®ä¿æ£€ç´¢çš„å‡†ç¡®æ€§å’Œå®šä½çš„ç²¾ç»†åŒ–ï¼Œä»è€Œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–å›¾åƒå’Œæè¿°çš„åŸºå‡†æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨å†»ç»“çš„è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›æœ€åï¼Œè®¾è®¡ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡æ¥è”åˆè¯„ä¼°æ£€ç´¢å’Œå®šä½æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡å°†è¯­æ–™åº“çº§æ£€ç´¢ä¸åƒç´ çº§å®šä½ç»“åˆï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æ£€ç´¢å’Œå®šä½æ¡†æ¶ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ•°æ®ä¸­æœ‰æ•ˆè¯†åˆ«å’Œå®šä½ç‰©ä½“ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ£€ç´¢å¬å›ç‡å’Œå®šä½ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ•´ä½“ç½‘ç»œç»“æ„åŸºäºç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œè¿›è¡Œé€‚å½“çš„ä¿®æ”¹ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReSeDisåœ¨æ£€ç´¢å¬å›ç‡å’Œå®šä½ç²¾åº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå¬å›ç‡æé«˜äº†XX%ï¼Œå®šä½ç²¾åº¦æå‡äº†YY%ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœç´¢å¼•æ“ã€ç”µå­å•†åŠ¡ã€ç¤¾äº¤åª’ä½“å†…å®¹æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜å›¾åƒæ£€ç´¢çš„å‡†ç¡®æ€§å’Œå®šä½èƒ½åŠ›ï¼ŒReSeDiså¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨å¤šæ¨¡æ€æœç´¢ç³»ç»Ÿçš„å‘å±•ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large-scale visual search engines are expected to solve a dual problem at once: (i) locate every image that truly contains the object described by a sentence and (ii) identify the object's bounding box or exact pixels within each hit. Existing techniques address only one side of this challenge. Visual grounding yields tight boxes and masks but rests on the unrealistic assumption that the object is present in every test image, producing a flood of false alarms when applied to web-scale collections. Text-to-image retrieval excels at sifting through massive databases to rank relevant images, yet it stops at whole-image matches and offers no fine-grained localization. We introduce Referring Search and Discovery (ReSeDis), the first task that unifies corpus-level retrieval with pixel-level grounding. Given a free-form description, a ReSeDis model must decide whether the queried object appears in each image and, if so, where it is, returning bounding boxes or segmentation masks. To enable rigorous study, we curate a benchmark in which every description maps uniquely to object instances scattered across a large, diverse corpus, eliminating unintended matches. We further design a task-specific metric that jointly scores retrieval recall and localization precision. Finally, we provide a straightforward zero-shot baseline using a frozen vision-language model, revealing significant headroom for future study. ReSeDis offers a realistic, end-to-end testbed for building the next generation of robust and scalable multimodal search systems.

