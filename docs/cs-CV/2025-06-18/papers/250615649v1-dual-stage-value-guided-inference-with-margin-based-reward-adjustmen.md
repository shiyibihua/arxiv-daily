---
layout: default
title: Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning
---

# Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15649" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.15649v1</a>
  <a href="https://arxiv.org/pdf/2506.15649.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15649v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15649v1', 'Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ankan Deria, Adinath Madhavrao Dukre, Feilong Tang, Sara Atito, Sudipta Roy, Muhammad Awais, Muhammad Haris Khan, Imran Razzak

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ViMaR‰ª•Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁîüÊàê‰ΩéÁΩÆ‰ø°Â∫¶ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜ‰ºòÂåñ` `Â•ñÂä±Ë∞ÉÊï¥` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ëá™Âä®ÊèèËø∞ÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ËÆ°ÁÆóÂºÄÈîÄÂ§ßÔºå‰∏îÂÆπÊòìÁîüÊàê‰ΩéÁΩÆ‰ø°Â∫¶ÁöÑÊèèËø∞ÔºåÂØºËá¥‰ø°ÊÅØ‰∏çÂáÜÁ°Æ„ÄÇ
2. ViMaRÈÄöËøáÂèåÈò∂ÊÆµÊé®ÁêÜÊ°ÜÊû∂ÔºåÁªìÂêà‰ª∑ÂÄºÊ®°Âûã‰∏éËæπÈôÖÂ•ñÂä±Ë∞ÉÊï¥ÔºåÊèêÂçáÁîüÊàêÊïàÁéá‰∏éÊèèËø∞Ë¥®Èáè„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåViMaRÂú®Â§ö‰∏™VLMÊû∂ÊûÑ‰∏äÁîüÊàêÁöÑÊèèËø∞Êõ¥ÂèØÈù†„ÄÅÂáÜÁ°ÆÔºå‰∏îÈÄüÂ∫¶ÊèêÂçáË∂ÖËøá4ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®Êé®ÁêÜÊó∂Èó¥ÊêúÁ¥¢ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰ªçÁÑ∂ËÆ°ÁÆóÂºÄÈîÄÂ§ß‰∏îÂÆπÊòì‰∫ßÁîü‰ΩéÁΩÆ‰ø°Â∫¶ÁöÑÁîüÊàêÔºåÂØºËá¥ÊåÅÁª≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂèåÈò∂ÊÆµÁöÑ‰ª∑ÂÄºÂºïÂØºÊé®ÁêÜÊ°ÜÊû∂ViMaRÔºåÈÄöËøáÁªìÂêàÊó∂Èó¥Â∑ÆÂàÜ‰ª∑ÂÄºÊ®°Âûã‰∏éÂü∫‰∫éËæπÈôÖÁöÑÂ•ñÂä±Ë∞ÉÊï¥ÔºåÊèêÈ´ò‰∫ÜÊïàÁéáÂíåËæìÂá∫ÁöÑÁúüÂÆûÊÄß„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåËØÜÂà´Âá∫Â§öÊ†∑ÂÄôÈÄâ‰∏≠ÊúÄÈ´ò‰ª∑ÂÄºÁöÑÊèèËø∞ÔºõÂú®Á¨¨‰∫åÈò∂ÊÆµÔºå‰ªÖÂØπË¢´ÂøΩËßÜÊàñËßÜËßâÂü∫Á°ÄËñÑÂº±ÁöÑÁâáÊÆµËøõË°åÈÄâÊã©ÊÄßÁªÜÂåñ„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåViMaRÁîüÊàêÁöÑÊèèËø∞Âú®ÂèØÈù†ÊÄß„ÄÅÂáÜÁ°ÆÊÄßÂíåÁªÜËäÇ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂ÈÄüÂ∫¶ÊèêÂçáË∂ÖËøá4ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËÆ°ÁÆóÂºÄÈîÄÂ§ßÂíå‰ΩéÁΩÆ‰ø°Â∫¶ÁîüÊàêÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÁîüÊàêÁöÑÊèèËø∞Â∏∏Â∏∏‰∏çÂáÜÁ°ÆÊàñÁº∫‰πè‰ø°ÊÅØÈáè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöViMaRÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂèåÈò∂ÊÆµÊé®ÁêÜÊ°ÜÊû∂ÔºåÈ¶ñÂÖàËØÜÂà´Âá∫ÊúÄÈ´ò‰ª∑ÂÄºÁöÑÊèèËø∞ÔºåÁÑ∂ÂêéÂØπË¢´ÂøΩËßÜÊàñÂü∫Á°ÄËñÑÂº±ÁöÑÁâáÊÆµËøõË°åÁªÜÂåñÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑÊèèËø∞Ë¥®ÈáèÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöViMaRÁöÑÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµËøõË°åÂçïÊ¨°Êé®ÁêÜ‰ª•ËØÜÂà´ÊúÄÈ´ò‰ª∑ÂÄºÁöÑÊèèËø∞ÔºåÁ¨¨‰∫åÈò∂ÊÆµÂàôÂØπÁâπÂÆöÁöÑÁâáÊÆµËøõË°åÈÄâÊã©ÊÄßÁªÜÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöViMaRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂü∫‰∫éËæπÈôÖÁöÑÂ•ñÂä±Ë∞ÉÊï¥Êú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÊäëÂà∂‰ΩéÁΩÆ‰ø°Â∫¶ÁöÑÁîüÊàêÔºåÂêåÊó∂‰øùÊåÅÊèèËø∞ÁöÑ‰∏∞ÂØåÊÄßÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑËÆæËÆ°ÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåViMaRÈááÁî®‰∫ÜÊó∂Èó¥Â∑ÆÂàÜ‰ª∑ÂÄºÊ®°ÂûãÔºåÂπ∂ËÆæÁΩÆ‰∫ÜÁªèËøáÊ†°ÂáÜÁöÑËæπÈôÖÊÉ©ÁΩöÔºå‰ª•ÈºìÂä±È´òÁΩÆ‰ø°Â∫¶ÁöÑÁîüÊàêÔºåÁ°Æ‰øùÁîüÊàêÁöÑÊèèËø∞Âú®ÂáÜÁ°ÆÊÄßÂíå‰ø°ÊÅØÈáè‰∏äÈÉΩÊúâÊâÄÊèêÂçá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåViMaRÁîüÊàêÁöÑÊèèËø∞Âú®ÂèØÈù†ÊÄß„ÄÅÂáÜÁ°ÆÊÄßÂíåÁªÜËäÇ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈÄüÂ∫¶ÊèêÂçáË∂ÖËøá4ÂÄç„ÄÇÊ≠§Â§ñÔºåViMaRÂú®‰∏çÂêåÊ®°ÂûãÈó¥ÁöÑËøÅÁßªËÉΩÂäõ‰πüÂæóÂà∞‰∫ÜÈ™åËØÅÔºåÊòæÁ§∫Âá∫ÂÖ∂ÁÅµÊ¥ªÊÄßÂíåÊ®°ÂùóÂåñÁâπÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®ÂõæÂÉèÊèèËø∞ÁîüÊàê„ÄÅËßÜÈ¢ëÁêÜËß£Âíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇÈÄöËøáÊèêÈ´òËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁîüÊàêË¥®ÈáèÂíåÊïàÁéáÔºåViMaRËÉΩÂ§ü‰∏∫Â§öÊ®°ÊÄÅÂ≠¶‰π†ÂíåÊô∫ËÉΩÁ≥ªÁªüÊèê‰æõÊõ¥Âº∫Â§ßÁöÑÊîØÊåÅÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite significant advances in inference-time search for vision-language models (VLMs), existing approaches remain both computationally expensive and prone to unpenalized, low-confidence generations which often lead to persistent hallucinations. We introduce \textbf{Value-guided Inference with Margin-based Reward (ViMaR)}, a two-stage inference framework that improves both efficiency and output fidelity by combining a temporal-difference value model with a margin-aware reward adjustment. In the first stage, we perform a single pass to identify the highest-value caption among diverse candidates. In the second stage, we selectively refine only those segments that were overlooked or exhibit weak visual grounding, thereby eliminating frequently rewarded evaluations. A calibrated margin-based penalty discourages low-confidence continuations while preserving descriptive richness. Extensive experiments across multiple VLM architectures demonstrate that ViMaR generates captions that are significantly more reliable, factually accurate, detailed, and explanatory, while achieving over 4$\times$ speedup compared to existing value-guided methods. Specifically, we show that ViMaR trained solely on LLaVA Mistral-7B, \textit{generalizes effectively to guide decoding in a stronger unseen model}. To further validate this, we adapt the ViMaR to steer generation in LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption quality and demonstrating robust cross-model guidance. This cross-model generalization highlights ViMaR's flexibility and modularity, positioning it as a scalable and transferable inference-time decoding strategy. Furthermore, when ViMaR-generated captions are used for self-training, the underlying models achieve substantial gains across a broad suite of visual comprehension benchmarks, underscoring the potential of fast, accurate, and self-improving VLM pipelines.

