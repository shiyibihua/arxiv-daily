---
layout: default
title: Demystifying the Visual Quality Paradox in Multimodal Large Language Models
---

# Demystifying the Visual Quality Paradox in Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15645" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15645v1</a>
  <a href="https://arxiv.org/pdf/2506.15645.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15645v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15645v1', 'Demystifying the Visual Quality Paradox in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuo Xing, Lanqing Guo, Hongyuan Hua, Seoyoung Lee, Peiran Li, Yufei Wang, Zhangyang Wang, Zhengzhong Tu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: 18 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVQ-TTTä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è´¨é‡æ‚–è®ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰è´¨é‡` `æµ‹è¯•æ—¶è°ƒä¼˜` `å›¾åƒå¤„ç†` `æ¨¡å‹é€‚åº”æ€§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è´¨é‡ä¸æ¨¡å‹ç†è§£ä¹‹é—´çš„å…³ç³»å°šä¸æ˜ç¡®ï¼Œå¯¼è‡´å¯¹è¾“å…¥å›¾åƒè´¨é‡çš„ç†è§£ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºè§†è§‰è´¨é‡æµ‹è¯•æ—¶è°ƒä¼˜ï¼ˆVQ-TTTï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è¾“å…¥å›¾åƒä»¥é€‚åº”ä»»åŠ¡ç‰¹å®šçš„æ¨¡å‹åå¥½ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVQ-TTTæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ¨¡å‹æˆ–é¢å¤–æ•°æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰-è¯­è¨€åŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…³äºè¾“å…¥è§†è§‰è´¨é‡å¦‚ä½•å½±å“å…¶å“åº”ä»çŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç³»ç»Ÿç ”ç©¶ï¼Œå‘ç°è§†è§‰è´¨é‡æ‚–è®ºï¼šå½“å›¾åƒåç¦»äººç±»æ„ŸçŸ¥çš„çœŸå®åº¦æ—¶ï¼Œæ¨¡å‹å’Œä»»åŠ¡çš„è¡¨ç°åè€Œå¯èƒ½æé«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰è´¨é‡æµ‹è¯•æ—¶è°ƒä¼˜ï¼ˆVQ-TTTï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡åœ¨å†»ç»“çš„è§†è§‰ç¼–ç å™¨å‰æ’å…¥å¯å­¦ä¹ çš„ä½ç§©æ ¸ï¼Œè°ƒèŠ‚é¢‘ç‡å†…å®¹ï¼Œå¹¶é€šè¿‡LoRAå¾®è°ƒæµ…å±‚è§†è§‰ç¼–ç å™¨ã€‚VQ-TTTåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­åŠ¨æ€è°ƒæ•´æ¯ä¸ªè¾“å…¥å›¾åƒï¼Œæ˜¾è‘—æå‡äº†è¯„ä¼°çš„MLLMsåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹æˆ–é¢å¤–è®­ç»ƒæ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¾“å…¥è´¨é‡ä¸æ¨¡å‹å“åº”ä¹‹é—´çš„æ‚–è®ºï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å›¾åƒè´¨é‡çš„å˜åŒ–æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè§†è§‰è´¨é‡æµ‹è¯•æ—¶è°ƒä¼˜ï¼ˆVQ-TTTï¼‰ï¼Œé€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨å‰æ’å…¥å¯å­¦ä¹ çš„ä½ç§©æ ¸ï¼Œè°ƒæ•´è¾“å…¥å›¾åƒçš„é¢‘ç‡å†…å®¹ï¼Œä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVQ-TTTæ¨¡å—åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œåœ¨å†»ç»“çš„è§†è§‰ç¼–ç å™¨å‰æ’å…¥ä½ç§©æ ¸ï¼›å…¶æ¬¡ï¼Œé€šè¿‡LoRAæŠ€æœ¯å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„æµ…å±‚å±‚æ¬¡ï¼Œä»¥å®ç°å¯¹è¾“å…¥å›¾åƒçš„åŠ¨æ€è°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šVQ-TTTçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶èƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹æˆ–é¢å¤–è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€ä¼˜åŒ–è¾“å…¥å›¾åƒï¼Œä»è€Œæå‡æ¨¡å‹çš„ä»»åŠ¡é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVQ-TTTçš„è®¾è®¡åŒ…æ‹¬å¯å­¦ä¹ çš„ä½ç§©æ ¸çš„å‚æ•°è®¾ç½®ï¼Œä»¥åŠé€šè¿‡LoRAå¾®è°ƒçš„å…·ä½“å±‚æ¬¡é€‰æ‹©ï¼Œç¡®ä¿åœ¨ä¿æŒæ¨¡å‹ç¨³å®šæ€§çš„åŒæ—¶ï¼Œæå‡å…¶å¯¹è§†è§‰è¾“å…¥çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVQ-TTTåœ¨æ‰€æœ‰è¯„ä¼°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†å¹³å‡å‡†ç¡®ç‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œä¸”åœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ™®é€‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººæœºäº¤äº’ç­‰å¤šä¸ªé¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¾“å…¥è´¨é‡ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘å’Œå†…å®¹ç”Ÿæˆç­‰æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.

