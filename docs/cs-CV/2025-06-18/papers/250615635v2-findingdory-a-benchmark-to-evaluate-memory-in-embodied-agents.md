---
layout: default
title: FindingDory: A Benchmark to Evaluate Memory in Embodied Agents
---

# FindingDory: A Benchmark to Evaluate Memory in Embodied Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15635v2</a>
  <a href="https://arxiv.org/pdf/2506.15635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15635v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15635v2', 'FindingDory: A Benchmark to Evaluate Memory in Embodied Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Karmesh Yadav, Yusuf Ali, Gunshi Gupta, Yarin Gal, Zsolt Kira

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: Our dataset and code can be found at: https://findingdory-benchmark.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFindingDoryåŸºå‡†ä»¥è¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `é•¿æœŸè®°å¿†` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ä»»åŠ¡è¯„ä¼°` `æœºå™¨äººå¯¼èˆª` `Habitatæ¨¡æ‹Ÿå™¨` `ä¸Šä¸‹æ–‡æ„è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æœŸè®°å¿†æ—¶å­˜åœ¨æ˜¾è‘—é™åˆ¶ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å…·èº«ç¯å¢ƒä¸­çš„å¤æ‚ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºäº†FindingDoryåŸºå‡†ï¼Œä¸“é—¨è¯„ä¼°å…·èº«æ™ºèƒ½ä½“åœ¨é•¿æœŸæ§åˆ¶ä»»åŠ¡ä¸­çš„è®°å¿†èƒ½åŠ›ï¼Œå¼ºè°ƒè®°å¿†ä¸è¡ŒåŠ¨çš„ç»“åˆã€‚
3. é€šè¿‡åœ¨Habitatæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œçš„å®éªŒï¼Œå±•ç¤ºäº†æ–°åŸºå‡†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ç°æœ‰æ¨¡å‹è¿›è¡Œäº†æ€§èƒ½å¯¹æ¯”ï¼ŒæŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’å’Œæ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å…·èº«ç¯å¢ƒä¸­åº”ç”¨å—é™äºå…¶å¤„ç†é•¿æœŸç»éªŒçš„èƒ½åŠ›ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸éš¾ä»¥åŒæ—¶å¤„ç†æ•°ç™¾å¼ å›¾åƒï¼ŒäºŸéœ€æ›´é«˜æ•ˆçš„é•¿æœŸè®°å¿†æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°å…·èº«ä»»åŠ¡ä¸­çš„è®°å¿†èƒ½åŠ›ï¼Œæ¶µç›–60ä¸ªéœ€è¦æŒç»­å‚ä¸å’Œä¸Šä¸‹æ–‡æ„è¯†çš„ä»»åŠ¡ï¼Œå¹¶å¯æ‰©å±•ä¸ºæ›´é•¿æ›´å…·æŒ‘æˆ˜æ€§çš„ç‰ˆæœ¬ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å°†å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä¸ä½çº§å¯¼èˆªç­–ç•¥ç»“åˆçš„åŸºçº¿ï¼Œè¯„ä¼°å…¶åœ¨è¿™äº›è®°å¿†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºæ”¹è¿›ç©ºé—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­å¤„ç†é•¿æœŸè®°å¿†çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„ç‰©ä½“æ“ä½œå’Œå¯¼èˆªä»»åŠ¡ä¸­ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸æ— æ³•æœ‰æ•ˆæ•´åˆå†å²ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„FindingDoryåŸºå‡†ä¸“æ³¨äºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“åœ¨é•¿æœŸæ§åˆ¶ä»»åŠ¡ä¸­çš„è®°å¿†èƒ½åŠ›ï¼Œå¼ºè°ƒè®°å¿†çš„å›å¿†ä¸åŸºäºå†å²ä¿¡æ¯çš„è¡ŒåŠ¨æ‰§è¡Œç›¸ç»“åˆï¼Œä»¥æå‡æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥åŸºå‡†åœ¨Habitatæ¨¡æ‹Ÿå™¨ä¸­æ„å»ºï¼ŒåŒ…å«60ä¸ªä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æŒç»­å‚ä¸å¹¶ä¿æŒä¸Šä¸‹æ–‡æ„è¯†ã€‚ä»»åŠ¡è®¾è®¡å…è®¸ç¨‹åºåŒ–æ‰©å±•ï¼Œå¢åŠ éš¾åº¦å’Œå¤æ‚æ€§ï¼Œä»¥ä¾¿è¿›è¡Œå¯æ‰©å±•çš„è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†é•¿æœŸè®°å¿†çš„è¯„ä¼°ä¸å…·èº«æ™ºèƒ½ä½“çš„å®é™…æ“ä½œç»“åˆèµ·æ¥ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨ç‰©ä½“æ“ä½œå’Œå¯¼èˆªæ–¹é¢çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œç»“åˆäº†å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä¸ä½çº§å¯¼èˆªç­–ç•¥ï¼Œè®¾ç½®äº†å¤šç§å‚æ•°ä»¥ä¼˜åŒ–è®°å¿†çš„æ•´åˆå’Œä»»åŠ¡æ‰§è¡Œï¼Œå…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FindingDoryåŸºå‡†çš„æ™ºèƒ½ä½“åœ¨è®°å¿†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„ä¸Šä¸‹æ–‡ç†è§£å’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰å…·èº«æ™ºèƒ½ä½“çš„å¼€å‘ã€‚é€šè¿‡æœ‰æ•ˆè¯„ä¼°å’Œæå‡æ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºå…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–å’Œæ“ä½œèƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models have recently demonstrated impressive performance in planning and control tasks, driving interest in their application to real-world robotics. However, deploying these models for reasoning in embodied contexts is limited by their ability to incorporate long-term experience collected across multiple days and represented by vast collections of images. Current VLMs typically struggle to process more than a few hundred images concurrently, highlighting the need for more efficient mechanisms to handle long-term memory in embodied settings. To effectively evaluate these models for long-horizon control, a benchmark must specifically target scenarios where memory is crucial for success. Existing long-video QA benchmarks overlook embodied challenges like object manipulation and navigation, which demand low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together rather than in isolation. In this work, we introduce a new benchmark for long-range embodied tasks in the Habitat simulator. This benchmark evaluates memory-based capabilities across 60 tasks requiring sustained engagement and contextual awareness in an environment. The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We also present baselines that integrate state-of-the-art VLMs with low level navigation policies, assessing their performance on these memory-intensive tasks and highlight areas for improvement.

