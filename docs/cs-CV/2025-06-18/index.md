---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-18
---

# cs.CVï¼ˆ2025-06-18ï¼‰

ğŸ“Š å…± **21** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250615645v1-demystifying-the-visual-quality-paradox-in-multimodal-large-language.html">Demystifying the Visual Quality Paradox in Multimodal Large Language Models</a></td>
  <td>æå‡ºVQ-TTTä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è´¨é‡æ‚–è®ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15645v1" data-paper-url="./papers/250615645v1-demystifying-the-visual-quality-paradox-in-multimodal-large-language.html" onclick="toggleFavorite(this, '2506.15645v1', 'Demystifying the Visual Quality Paradox in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250615477v1-multimodal-large-language-models-for-medical-report-generation-via-c.html">Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning</a></td>
  <td>æå‡ºMRG-LLMä»¥è§£å†³åŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15477v1" data-paper-url="./papers/250615477v1-multimodal-large-language-models-for-medical-report-generation-via-c.html" onclick="toggleFavorite(this, '2506.15477v1', 'Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250615218v1-dm-fnet-unified-multimodal-medical-image-fusion-via-diffusion-proces.html">DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder</a></td>
  <td>æå‡ºDM-FNetä»¥è§£å†³å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆè´¨é‡ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15218v1" data-paper-url="./papers/250615218v1-dm-fnet-unified-multimodal-medical-image-fusion-via-diffusion-proces.html" onclick="toggleFavorite(this, '2506.15218v1', 'DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250615298v2-megc2025-micro-expression-grand-challenge-on-spot-then-recognize-and.html">MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering</a></td>
  <td>æå‡ºMEGC2025ä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15298v2" data-paper-url="./papers/250615298v2-megc2025-micro-expression-grand-challenge-on-spot-then-recognize-and.html" onclick="toggleFavorite(this, '2506.15298v2', 'MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250615180v1-resedis-a-dataset-for-referring-based-object-search-across-large-sca.html">ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections</a></td>
  <td>æå‡ºReSeDisä»¥è§£å†³å¤§è§„æ¨¡å›¾åƒé›†åˆä¸­çš„åŸºäºæè¿°çš„ç‰©ä½“æœç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15180v1" data-paper-url="./papers/250615180v1-resedis-a-dataset-for-referring-based-object-search-across-large-sca.html" onclick="toggleFavorite(this, '2506.15180v1', 'ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250615649v1-dual-stage-value-guided-inference-with-margin-based-reward-adjustmen.html">Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning</a></td>
  <td>æå‡ºViMaRä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆä½ç½®ä¿¡åº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15649v1" data-paper-url="./papers/250615649v1-dual-stage-value-guided-inference-with-margin-based-reward-adjustmen.html" onclick="toggleFavorite(this, '2506.15649v1', 'Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250615747v2-a-strong-view-free-baseline-approach-for-single-view-image-guided-po.html">A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion</a></td>
  <td>æå‡ºä¸€ç§æ— è§†è§’å›¾åƒå¼•å¯¼çš„å•è§†å›¾ç‚¹äº‘è¡¥å…¨åŸºçº¿æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15747v2" data-paper-url="./papers/250615747v2-a-strong-view-free-baseline-approach-for-single-view-image-guided-po.html" onclick="toggleFavorite(this, '2506.15747v2', 'A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250615242v2-ra-nerf-robust-neural-radiance-field-reconstruction-with-accurate-ca.html">RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories</a></td>
  <td>æå‡ºRA-NeRFä»¥è§£å†³å¤æ‚è½¨è¿¹ä¸‹ç›¸æœºå§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15242v2" data-paper-url="./papers/250615242v2-ra-nerf-robust-neural-radiance-field-reconstruction-with-accurate-ca.html" onclick="toggleFavorite(this, '2506.15242v2', 'RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250615610v3-boxfusion-reconstruction-free-open-vocabulary-3d-object-detection-vi.html">BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</a></td>
  <td>æå‡ºé‡å»ºæ— å…³çš„åœ¨çº¿æ¡†æ¶ä»¥è§£å†³å®æ—¶3Dç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15610v3" data-paper-url="./papers/250615610v3-boxfusion-reconstruction-free-open-vocabulary-3d-object-detection-vi.html" onclick="toggleFavorite(this, '2506.15610v3', 'BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250615560v2-racalnet-radar-calibration-network-for-sparse-supervised-metric-dept.html">RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation</a></td>
  <td>æå‡ºRaCalNetä»¥è§£å†³ç¨€ç–ç›‘ç£ä¸‹çš„æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">metric depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15560v2" data-paper-url="./papers/250615560v2-racalnet-radar-calibration-network-for-sparse-supervised-metric-dept.html" onclick="toggleFavorite(this, '2506.15560v2', 'RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250615313v1-mapfm-foundation-model-driven-hd-mapping-with-multi-task-contextual-.html">MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning</a></td>
  <td>æå‡ºMapFMä»¥è§£å†³é«˜ç²¾åº¦åœ°å›¾ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">semantic map</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15313v1" data-paper-url="./papers/250615313v1-mapfm-foundation-model-driven-hd-mapping-with-multi-task-contextual-.html" onclick="toggleFavorite(this, '2506.15313v1', 'MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250615806v1-implicit-3d-scene-reconstruction-using-deep-learning-towards-efficie.html">Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„éšå¼3Dåœºæ™¯é‡å»ºæ–¹æ³•ä»¥æå‡è‡ªåŠ¨é©¾é©¶ä¸­çš„ç¢°æ’ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15806v1" data-paper-url="./papers/250615806v1-implicit-3d-scene-reconstruction-using-deep-learning-towards-efficie.html" onclick="toggleFavorite(this, '2506.15806v1', 'Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250615564v3-show-o2-improved-native-unified-multimodal-models.html">Show-o2: Improved Native Unified Multimodal Models</a></td>
  <td>æå‡ºShow-o2ä»¥æå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15564v3" data-paper-url="./papers/250615564v3-show-o2-improved-native-unified-multimodal-models.html" onclick="toggleFavorite(this, '2506.15564v3', 'Show-o2: Improved Native Unified Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250615757v1-weakly-supervised-vlm-guided-partial-contrastive-learning-for-visual.html">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></td>
  <td>æå‡ºå¼±ç›‘ç£éƒ¨åˆ†å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„åŠ¨æ€è§†è§’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15757v1" data-paper-url="./papers/250615757v1-weakly-supervised-vlm-guided-partial-contrastive-learning-for-visual.html" onclick="toggleFavorite(this, '2506.15757v1', 'Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250615220v3-video-salmonn-2-caption-enhanced-audio-visual-large-language-models.html">video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models</a></td>
  <td>æå‡ºvideo-SALMONN 2ä»¥è§£å†³è§†é¢‘æè¿°ä¸é—®ç­”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15220v3" data-paper-url="./papers/250615220v3-video-salmonn-2-caption-enhanced-audio-visual-large-language-models.html" onclick="toggleFavorite(this, '2506.15220v3', 'video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250615365v1-fedwsidd-federated-whole-slide-image-classification-via-dataset-dist.html">FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation</a></td>
  <td>æå‡ºFedWSIDDä»¥è§£å†³WSIåˆ†ç±»ä¸­çš„éšç§ä¸èµ„æºå¼‚æ„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">predictive model</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15365v1" data-paper-url="./papers/250615365v1-fedwsidd-federated-whole-slide-image-classification-via-dataset-dist.html" onclick="toggleFavorite(this, '2506.15365v1', 'FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250615625v2-hoidini-human-object-interaction-through-diffusion-noise-optimizatio.html">HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization</a></td>
  <td>æå‡ºHOIDiNiä»¥è§£å†³äººæœºäº¤äº’ç”Ÿæˆä¸­çš„çœŸå®æ„Ÿä¸ç‰©ç†å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15625v2" data-paper-url="./papers/250615625v2-hoidini-human-object-interaction-through-diffusion-noise-optimizatio.html" onclick="toggleFavorite(this, '2506.15625v2', 'HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250615258v2-privacy-preserving-chest-x-ray-classification-in-latent-space-with-h.html">Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference</a></td>
  <td>æå‡ºåŒæ€åŠ å¯†ç¥ç»æ¨ç†æ¡†æ¶ä»¥ä¿æŠ¤èƒ¸éƒ¨Xå…‰å›¾åƒéšç§</td>
  <td class="tags-cell"><span class="paper-tag">OMOMO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15258v2" data-paper-url="./papers/250615258v2-privacy-preserving-chest-x-ray-classification-in-latent-space-with-h.html" onclick="toggleFavorite(this, '2506.15258v2', 'Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250615483v1-genhoi-generalizing-text-driven-4d-human-object-interaction-synthesi.html">GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</a></td>
  <td>æå‡ºGenHOIä»¥è§£å†³4Däººæœºäº¤äº’åˆæˆä¸­çš„ç‰©ä½“æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion synthesis</span> <span class="paper-tag">contact-aware</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15483v1" data-paper-url="./papers/250615483v1-genhoi-generalizing-text-driven-4d-human-object-interaction-synthesi.html" onclick="toggleFavorite(this, '2506.15483v1', 'GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250615369v1-unsupervised-pelage-pattern-unwrapping-for-animal-re-identification.html">Unsupervised Pelage Pattern Unwrapping for Animal Re-identification</a></td>
  <td>æå‡ºå‡ ä½•æ„ŸçŸ¥çº¹ç†æ˜ å°„ä»¥è§£å†³åŠ¨ç‰©é‡è¯†åˆ«ä¸­çš„çš®æ¯›æ¨¡å¼æ‰­æ›²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15369v1" data-paper-url="./papers/250615369v1-unsupervised-pelage-pattern-unwrapping-for-animal-re-identification.html" onclick="toggleFavorite(this, '2506.15369v1', 'Unsupervised Pelage Pattern Unwrapping for Animal Re-identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250615635v2-findingdory-a-benchmark-to-evaluate-memory-in-embodied-agents.html">FindingDory: A Benchmark to Evaluate Memory in Embodied Agents</a></td>
  <td>æå‡ºFindingDoryåŸºå‡†ä»¥è¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.15635v2" data-paper-url="./papers/250615635v2-findingdory-a-benchmark-to-evaluate-memory-in-embodied-agents.html" onclick="toggleFavorite(this, '2506.15635v2', 'FindingDory: A Benchmark to Evaluate Memory in Embodied Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)