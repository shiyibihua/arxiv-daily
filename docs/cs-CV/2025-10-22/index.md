---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-22
---

# cs.CVï¼ˆ2025-10-22ï¼‰

ğŸ“Š å…± **30** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251019451v1-reasoning-like-experts-leveraging-multimodal-large-language-models-f.html">Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis</a></td>
  <td>æå‡ºPICKæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºç»˜ç”»çš„å¿ƒç†åˆ†æï¼Œæå‡ä¸“å®¶çº§æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19451v1" onclick="toggleFavorite(this, '2510.19451v1', 'Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251019273v1-mobiact-efficient-mav-action-recognition-using-mobilenetv4-with-cont.html">MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation</a></td>
  <td>æå‡ºMobiActï¼šä¸€ç§åŸºäºMobileNetV4ã€å¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦çš„é«˜æ•ˆMAVåŠ¨ä½œè¯†åˆ«æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19273v1" onclick="toggleFavorite(this, '2510.19273v1', 'MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251019955v1-transformed-multi-view-3d-shape-features-with-contrastive-learning.html">Transformed Multi-view 3D Shape Features with Contrastive Learning</a></td>
  <td>æå‡ºåŸºäºå¯¹æ¯”å­¦ä¹ çš„Transformerå¤šè§†è§’3Då½¢çŠ¶ç‰¹å¾æå–æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19955v1" onclick="toggleFavorite(this, '2510.19955v1', 'Transformed Multi-view 3D Shape Features with Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251019307v1-unified-reinforcement-and-imitation-learning-for-vision-language-mod.html">Unified Reinforcement and Imitation Learning for Vision-Language Models</a></td>
  <td>æå‡ºç»Ÿä¸€å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ (RIL)ç®—æ³•ï¼Œç”¨äºè®­ç»ƒè½»é‡çº§è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19307v1" onclick="toggleFavorite(this, '2510.19307v1', 'Unified Reinforcement and Imitation Learning for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251021850v1-scope-vlm-selective-context-processing-for-efficient-document-naviga.html">SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models</a></td>
  <td>SCoPE VLMï¼šé¢å‘é«˜æ•ˆæ–‡æ¡£å¯¼èˆªçš„è§†è§‰è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§ä¸Šä¸‹æ–‡å¤„ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21850v1" onclick="toggleFavorite(this, '2510.21850v1', 'SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251019654v2-from-forecasting-to-planning-policy-world-model-for-collaborative-st.html">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></td>
  <td>æå‡ºç­–ç•¥ä¸–ç•Œæ¨¡å‹PWMï¼Œç”¨äºååŒçŠ¶æ€-åŠ¨ä½œé¢„æµ‹ï¼Œæå‡è‡ªåŠ¨é©¾é©¶è§„åˆ’èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19654v2" onclick="toggleFavorite(this, '2510.19654v2', 'From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251019560v1-had-hierarchical-asymmetric-distillation-to-bridge-spatio-temporal-g.html">HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking</a></td>
  <td>æå‡ºåˆ†å±‚éå¯¹ç§°è’¸é¦ï¼ˆHADï¼‰æ¡†æ¶ï¼Œå¼¥åˆäº‹ä»¶ç›¸æœºç›®æ ‡è·Ÿè¸ªä¸­çš„æ—¶ç©ºå·®å¼‚ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19560v1" onclick="toggleFavorite(this, '2510.19560v1', 'HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251019475v1-prgcn-a-graph-memory-network-for-cross-sequence-pattern-reuse-in-3d-.html">PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation</a></td>
  <td>æå‡ºPRGCNï¼Œåˆ©ç”¨å›¾è®°å¿†ç½‘ç»œå®ç°è·¨åºåˆ—äººä½“å§¿æ€æ¨¡å¼å¤ç”¨ï¼Œæå‡3Däººä½“å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19475v1" onclick="toggleFavorite(this, '2510.19475v1', 'PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251019622v2-augmenting-moment-retrieval-zero-dependency-two-stage-learning.html">Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning</a></td>
  <td>æå‡ºé›¶å¤–éƒ¨ä¾èµ–çš„å¢å¼ºæ—¶åˆ»æ£€ç´¢æ¡†æ¶AMRï¼Œè§£å†³æ•°æ®ç¨€ç–ã€è¾¹ç•Œæ¨¡ç³Šå’Œè¯­ä¹‰åŒºåˆ†ä¸è¶³é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19622v2" onclick="toggleFavorite(this, '2510.19622v2', 'Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/251019578v1-vgd-visual-geometry-gaussian-splatting-for-feed-forward-surround-vie.html">VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction</a></td>
  <td>VGDï¼šç”¨äºå‰é¦ˆç¯è§†é©¾é©¶åœºæ™¯é‡å»ºçš„è§†è§‰å‡ ä½•é«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19578v1" onclick="toggleFavorite(this, '2510.19578v1', 'VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251020027v1-extreme-views-3dgs-filter-for-novel-view-synthesis-from-out-of-distr.html">Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses</a></td>
  <td>æå‡ºåŸºäºæ¢¯åº¦çš„3DGSæ»¤æ³¢æ–¹æ³•ï¼Œè§£å†³æç«¯è§†è§’ä¸‹æ–°è§†è§’åˆæˆçš„ä¼ªå½±é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20027v1" onclick="toggleFavorite(this, '2510.20027v1', 'Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251019255v1-advances-in-4d-representation-geometry-motion-and-interaction.html">Advances in 4D Representation: Geometry, Motion, and Interaction</a></td>
  <td>é’ˆå¯¹4Dç”Ÿæˆä¸é‡å»ºï¼Œæå‡ºåŸºäºå‡ ä½•ã€è¿åŠ¨å’Œäº¤äº’çš„4Dè¡¨å¾æ–¹æ³•ç»¼è¿°ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19255v1" onclick="toggleFavorite(this, '2510.19255v1', 'Advances in 4D Representation: Geometry, Motion, and Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251019333v2-a-training-free-framework-for-open-vocabulary-image-segmentation-and.html">A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP</a></td>
  <td>æå‡ºä¸€ç§åŸºäºEfficientNetå’ŒCLIPçš„æ— è®­ç»ƒå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ä¸è¯†åˆ«æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19333v2" onclick="toggleFavorite(this, '2510.19333v2', 'A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251019814v3-toward-a-better-understanding-of-monocular-depth-evaluation.html">Toward A Better Understanding of Monocular Depth Evaluation</a></td>
  <td>æå‡ºå•ç›®æ·±åº¦ä¼°è®¡è¯„ä¼°æ–°æŒ‡æ ‡ï¼Œæå‡ä¸äººç±»æ„ŸçŸ¥çš„å¯¹é½æ€§</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19814v3" onclick="toggleFavorite(this, '2510.19814v3', 'Toward A Better Understanding of Monocular Depth Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251019371v1-aegisrf-adversarial-perturbations-guided-with-sensitivity-for-protec.html">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a></td>
  <td>AegisRFï¼šåˆ©ç”¨æ•æ„Ÿåº¦å¼•å¯¼çš„å¯¹æŠ—æ‰°åŠ¨ä¿æŠ¤NeRFçš„çŸ¥è¯†äº§æƒ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19371v1" onclick="toggleFavorite(this, '2510.19371v1', 'AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251019559v1-a-matter-of-time-revealing-the-structure-of-time-in-vision-language-.html">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></td>
  <td>æå‡ºTIME10kåŸºå‡†ï¼Œæ­ç¤ºè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­æ—¶é—´ä¿¡æ¯çš„ä½ç»´éçº¿æ€§ç»“æ„ï¼Œå¹¶æ„å»ºæ—¶é—´è½´è¡¨ç¤ºã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19559v1" onclick="toggleFavorite(this, '2510.19559v1', 'A Matter of Time: Revealing the Structure of Time in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251019330v1-exploring-scale-shift-in-crowd-localization-under-the-context-of-dom.html">Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization</a></td>
  <td>é’ˆå¯¹äººç¾¤å®šä½ä¸­å°ºåº¦åç§»é—®é¢˜ï¼Œæå‡ºå› æœç‰¹å¾è§£è€¦å’Œå¼‚æ„å¤„ç†æ–¹æ³•ï¼Œæå‡é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19330v1" onclick="toggleFavorite(this, '2510.19330v1', 'Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251019336v1-damo-data-mixing-optimizer-in-fine-tuning-multimodal-llms-for-mobile.html">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a></td>
  <td>DaMoï¼šç”¨äºæ‰‹æœºAgentå¤šæ¨¡æ€LLMå¾®è°ƒçš„æ•°æ®æ··åˆä¼˜åŒ–å™¨</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19336v1" onclick="toggleFavorite(this, '2510.19336v1', 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251021842v1-modal-aphasia-can-unified-multimodal-models-describe-images-from-mem.html">Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?</a></td>
  <td>æ­ç¤ºå¤šæ¨¡æ€æ¨¡å‹ä¸­çš„â€œæ¨¡æ€å¤±è¯­ç—‡â€ç°è±¡ï¼Œå³è§†è§‰è®°å¿†å‡†ç¡®ä½†æ–‡æœ¬æè¿°å¤±è´¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21842v1" onclick="toggleFavorite(this, '2510.21842v1', 'Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251019678v1-i-spy-with-my-models-eye-visual-search-as-a-behavioural-test-for-mll.html">I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs</a></td>
  <td>åˆ©ç”¨è§†è§‰æœç´¢è¡Œä¸ºæµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19678v1" onclick="toggleFavorite(this, '2510.19678v1', 'I Spy With My Model&#39;s Eye: Visual Search as a Behavioural Test for MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251019592v1-decomposed-attention-fusion-in-mllms-for-training-free-video-reasoni.html">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a></td>
  <td>æå‡ºDecomposed Attention Fusion (DecAF)ï¼Œç”¨äºMLLMçš„å…è®­ç»ƒè§†é¢‘æ¨ç†åˆ†å‰²</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19592v1" onclick="toggleFavorite(this, '2510.19592v1', 'Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251019986v1-automating-iconclass-llms-and-rag-for-large-scale-classification-of-.html">Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts</a></td>
  <td>åˆ©ç”¨LLMå’ŒRAGè‡ªåŠ¨åŒ–å®—æ•™æœ¨åˆ»å›¾åƒçš„Iconclasså¤§è§„æ¨¡åˆ†ç±»</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19986v1" onclick="toggleFavorite(this, '2510.19986v1', 'Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251019981v2-futrtrack-a-camera-lidar-fusion-transformer-for-3d-multiple-object-t.html">FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</a></td>
  <td>FutrTrackï¼šä¸€ç§ç”¨äº3Då¤šç›®æ ‡è·Ÿè¸ªçš„ç›¸æœº-æ¿€å…‰é›·è¾¾èåˆTransformer</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19981v2" onclick="toggleFavorite(this, '2510.19981v2', 'FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251019808v1-pico-banana-400k-a-large-scale-dataset-for-text-guided-image-editing.html">Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</a></td>
  <td>æå‡ºPico-Banana-400Kå¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¿ƒè¿›æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç ”ç©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19808v1" onclick="toggleFavorite(this, '2510.19808v1', 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251019496v1-cares-context-aware-resolution-selector-for-vlms.html">CARES: Context-Aware Resolution Selector for VLMs</a></td>
  <td>æå‡ºCARESä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å™¨ï¼Œé™ä½VLMè®¡ç®—æˆæœ¬å¹¶ä¿æŒæ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19496v1" onclick="toggleFavorite(this, '2510.19496v1', 'CARES: Context-Aware Resolution Selector for VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251019400v1-seeing-across-views-benchmarking-spatial-reasoning-of-vision-languag.html">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a></td>
  <td>æå‡ºMV-RoboBenchåŸºå‡†ï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººåœºæ™¯ä¸­çš„å¤šè§†è§’ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19400v1" onclick="toggleFavorite(this, '2510.19400v1', 'Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251019944v1-seed3d-10-from-images-to-high-fidelity-simulation-ready-3d-assets.html">Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</a></td>
  <td>Seed3D 1.0ï¼šæå‡ºä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡ã€å¯ç”¨äºç‰©ç†ä»¿çœŸçš„3Dèµ„äº§çš„æ¡†æ¶ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19944v1" onclick="toggleFavorite(this, '2510.19944v1', 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251019819v1-is-this-tracker-on-a-benchmark-protocol-for-dynamic-tracking.html">Is This Tracker On? A Benchmark Protocol for Dynamic Tracking</a></td>
  <td>æå‡ºITTOï¼šä¸€ä¸ªç”¨äºåŠ¨æ€ç‚¹è·Ÿè¸ªçš„å…¨æ–°åŸºå‡†æµ‹è¯•åè®®ï¼Œèšç„¦çœŸå®åœºæ™¯æŒ‘æˆ˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19819v1" onclick="toggleFavorite(this, '2510.19819v1', 'Is This Tracker On? A Benchmark Protocol for Dynamic Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251019527v1-posecrafter-extreme-pose-estimation-with-hybrid-video-synthesis.html">PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis</a></td>
  <td>PoseCrafterï¼šåˆ©ç”¨æ··åˆè§†é¢‘åˆæˆå¢å¼ºæç«¯ä½å§¿ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19527v1" onclick="toggleFavorite(this, '2510.19527v1', 'PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/251019789v1-omnimotion-x-versatile-multimodal-whole-body-motion-generation.html">OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</a></td>
  <td>OmniMotion-Xï¼šå¤šåŠŸèƒ½å¤šæ¨¡æ€å…¨èº«è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°é€¼çœŸå¯æ§çš„äº¤äº’å¼é•¿æ—¶è¿åŠ¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19789v1" onclick="toggleFavorite(this, '2510.19789v1', 'OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)