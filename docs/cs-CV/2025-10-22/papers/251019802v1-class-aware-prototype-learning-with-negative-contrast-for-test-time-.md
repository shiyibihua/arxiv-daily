---
layout: default
title: Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models
---

# Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models

**arXiv**: [2510.19802v1](https://arxiv.org/abs/2510.19802) | [PDF](https://arxiv.org/pdf/2510.19802.pdf)

**ä½œè€…**: Xiaozhen Qiao, Jingkai Zhao, Yuqiu Jiang, Xianda Guo, Zhe Sun, Hongyuan Zhang, Xuelong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç±»æ„ŸçŸ¥åŽŸåž‹å­¦ä¹ ä¸Žè´Ÿå¯¹æ¯”æ–¹æ³•ï¼Œä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æµ‹è¯•æ—¶åˆ†å¸ƒåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `æµ‹è¯•æ—¶é€‚åº”` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŽŸåž‹å­¦ä¹ ` `è´Ÿå¯¹æ¯”å­¦ä¹ ` `åˆ†å¸ƒåç§»` `ç±»æ„ŸçŸ¥ç¼“å­˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨åˆ†å¸ƒåç§»æ—¶æ€§èƒ½ä¸‹é™ï¼ŒåŽŸåž‹é€€åŒ–å’Œç±»é—´æ··æ·†æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŠ¨æ€è°ƒæ•´ç±»åŽŸåž‹ç¼“å­˜ï¼Œç»“åˆè´Ÿå¯¹æ¯”å­¦ä¹ æœºåˆ¶æå‡ç±»å¯åˆ†æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œé€‚ç”¨äºŽResNet-50å’ŒViT-B/16éª¨å¹²ç½‘ç»œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization
> through large-scale image-text pretraining, yet their performance can drop once
> the deployment distribution diverges from the training distribution. To address
> this, Test-Time Adaptation (TTA) methods update models using unlabeled target
> data. However, existing approaches often ignore two key challenges: prototype
> degradation in long-tailed distributions and confusion between semantically
> similar classes. To tackle these issues, we propose \textbf{C}lass-Aware
> \textbf{P}rototype \textbf{L}earning with \textbf{N}egative
> \textbf{C}ontrast(\textbf{CPL-NC}), a lightweight TTA framework designed
> specifically for VLMs to enhance generalization under distribution shifts.
> CPL-NC introduces a \textit{Class-Aware Prototype Cache} Module that
> dynamically adjusts per-class capacity based on test-time frequency and
> activation history, with a rejuvenation mechanism for inactive classes to
> retain rare-category knowledge. Additionally, a \textit{Negative Contrastive
> Learning} Mechanism identifies and constrains hard visual-textual negatives to
> improve class separability. The framework employs asymmetric optimization,
> refining only textual prototypes while anchoring on stable visual features.
> Experiments on 15 benchmarks show that CPL-NC consistently outperforms prior
> TTA methods across both ResNet-50 and ViT-B/16 backbones.

