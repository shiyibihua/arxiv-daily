---
layout: default
title: BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP
---

# BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP

**arXiv**: [2510.19332v1](https://arxiv.org/abs/2510.19332) | [PDF](https://arxiv.org/pdf/2510.19332.pdf)

**ä½œè€…**: Tian Xia, Zihan Ma, Xinlong Wang, Qing Liu, Xiaowei He, Tianming Liu, Yudan Ren

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBrainMCLIPä»¥é«˜æ•ˆè§£ç fMRIè„‘å›¾åƒï¼ŒèžåˆCLIPå¤šå±‚ç‰¹å¾å¹¶é¿å…VAEè·¯å¾„ã€‚**

**å…³é”®è¯**: `è„‘å›¾åƒè§£ç ` `CLIPç‰¹å¾èžåˆ` `åŠŸèƒ½å±‚æ¬¡å¯¹é½` `å‚æ•°é«˜æ•ˆæ¨¡åž‹` `fMRIä¿¡å·å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¿½ç•¥CLIPä¸­é—´å±‚ä¿¡æ¯ï¼Œä¸”ä¸Žå¤§è„‘åŠŸèƒ½å±‚æ¬¡ä¸ç¬¦ï¼Œä¾èµ–å‚æ•°å¯†é›†çš„VAEã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽè§†è§‰ç³»ç»ŸåŠŸèƒ½å±‚æ¬¡ï¼Œå¯¹é½fMRIä¿¡å·ä¸ŽCLIPå¤šå±‚ç‰¹å¾ï¼Œå¼•å…¥è·¨é‡å»ºç­–ç•¥å’Œå¤šç²’åº¦æŸå¤±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é«˜çº§è¯­ä¹‰æŒ‡æ ‡ä¸Šåª²ç¾Žæˆ–è¶…è¶ŠSOTAï¼Œå‚æ•°å‡å°‘71.7%ï¼Œæ— éœ€VAEè·¯å¾„ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Decoding images from fMRI often involves mapping brain activity to CLIP's
> final semantic layer. To capture finer visual details, many approaches add a
> parameter-intensive VAE-based pipeline. However, these approaches overlook rich
> object information within CLIP's intermediate layers and contradicts the
> brain's functionally hierarchical. We introduce BrainMCLIP, which pioneers a
> parameter-efficient, multi-layer fusion approach guided by human visual
> system's functional hierarchy, eliminating the need for such a separate VAE
> pathway. BrainMCLIP aligns fMRI signals from functionally distinct visual areas
> (low-/high-level) to corresponding intermediate and final CLIP layers,
> respecting functional hierarchy. We further introduce a Cross-Reconstruction
> strategy and a novel multi-granularity loss. Results show BrainMCLIP achieves
> highly competitive performance, particularly excelling on high-level semantic
> metrics where it matches or surpasses SOTA(state-of-the-art) methods, including
> those using VAE pipelines. Crucially, it achieves this with substantially fewer
> parameters, demonstrating a reduction of
> 71.7\%(Table.\ref{tab:compare_clip_vae}) compared to top VAE-based SOTA
> methods, by avoiding the VAE pathway. By leveraging intermediate CLIP features,
> it effectively captures visual details often missed by CLIP-only approaches,
> striking a compelling balance between semantic accuracy and detail fidelity
> without requiring a separate VAE pipeline.

