---
layout: default
title: DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents
---

# DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents

**arXiv**: [2510.19336v1](https://arxiv.org/abs/2510.19336) | [PDF](https://arxiv.org/pdf/2510.19336.pdf)

**ä½œè€…**: Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDaMoæ•°æ®æ··åˆä¼˜åŒ–å™¨ä»¥æå‡å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨ç§»åŠ¨æ‰‹æœºä»£ç†ä¸­çš„å¤šä»»åŠ¡æ€§èƒ½**

**å…³é”®è¯**: `æ•°æ®æ··åˆä¼˜åŒ–` `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `ç§»åŠ¨æ‰‹æœºä»£ç†` `å¤šä»»åŠ¡å­¦ä¹ ` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨ç§»åŠ¨æ‰‹æœºä»£ç†ä¸­å¤„ç†å¤šä»»åŠ¡æ—¶ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥ç¡®å®šæœ€ä¼˜è®­ç»ƒæ•°æ®ç»„åˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šDaMoä½¿ç”¨å¯è®­ç»ƒç½‘ç»œé¢„æµ‹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œä»¥ä¼˜åŒ–æ•°æ®æ··åˆé…ç½®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PhoneAgentBenchä¸Šæ€§èƒ½æå‡3.38%ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•çŽ°ä¼˜è¶Šæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mobile Phone Agents (MPAs) have emerged as a promising research direction due
> to their broad applicability across diverse scenarios. While Multimodal Large
> Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness
> in handling multiple mobile phone tasks simultaneously remains limited.
> Although multitask supervised fine-tuning (SFT) is widely adopted for multitask
> learning, existing approaches struggle to determine optimal training data
> compositions for peak performance. To address this challenge, we propose DaMo
> (Data Mixture Optimizer) - a novel solution employing a trainable network that
> predicts optimal data mixtures by forecasting downstream task performance for
> any given dataset ratio. To support comprehensive evaluation, we introduce
> PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
> multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
> real-world industrial mobile application scenarios. Demonstrating strong
> predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo
> efficiently extrapolates optimal data mixing configurations. Our results show
> DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
> alternative methods. Furthermore, extensive experiments across established
> benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench
> reveal DaMo's superior generalization, outperforming other approaches by 2.57%
> in terms of average score. When used solely for MLLM optimization on the
> BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,
> DaMo maintains robust scalability, preserving its effectiveness when applied to
> other model architectures. The code and dataset are available at
> https://github.com/OPPO-Mente-Lab/DaMo.git

