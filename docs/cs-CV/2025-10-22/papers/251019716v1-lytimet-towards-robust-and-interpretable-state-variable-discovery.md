---
layout: default
title: LyTimeT: Towards Robust and Interpretable State-Variable Discovery
---

# LyTimeT: Towards Robust and Interpretable State-Variable Discovery

**arXiv**: [2510.19716v1](https://arxiv.org/abs/2510.19716) | [PDF](https://arxiv.org/pdf/2510.19716.pdf)

**ä½œè€…**: Kuai Yu, Crystal Su, Xiang Liu, Judah Goldfeder, Mingyuan Shao, Hod Lipson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLyTimeTæ¡†æž¶ä»¥ä»Žé«˜ç»´è§†é¢‘ä¸­æå–é²æ£’ä¸”å¯è§£é‡Šçš„åŠ¨æ€ç³»ç»ŸçŠ¶æ€å˜é‡**

**å…³é”®è¯**: `åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡` `è§†é¢‘é¢„æµ‹` `å¯è§£é‡ŠAI` `æ—¶ç©ºæ³¨æ„åŠ›` `ç¨³å®šæ€§æ­£åˆ™åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé«˜ç»´è§†é¢‘ä¸­åŠ¨æ€å˜é‡æå–å—èƒŒæ™¯è¿åŠ¨ã€é®æŒ¡å’Œçº¹ç†å˜åŒ–ç­‰å¹²æ‰°å› ç´ å½±å“
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æž¶ï¼Œç»“åˆæ—¶ç©ºæ³¨æ„åŠ›ç¼–ç å’ŒLyapunovç¨³å®šæ€§æ­£åˆ™åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒçœŸå®žç³»ç»Ÿä¸ŠéªŒè¯ï¼Œå®žçŽ°é«˜ç²¾åº¦é¢„æµ‹å’Œé²æ£’æ€§ï¼Œä¼˜äºŽåŸºçº¿æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Extracting the true dynamical variables of a system from high-dimensional
> video is challenging due to distracting visual factors such as background
> motion, occlusions, and texture changes. We propose LyTimeT, a two-phase
> framework for interpretable variable extraction that learns robust and stable
> latent representations of dynamical systems. In Phase 1, LyTimeT employs a
> spatio-temporal TimeSformer-based autoencoder that uses global attention to
> focus on dynamically relevant regions while suppressing nuisance variation,
> enabling distraction-robust latent state learning and accurate long-horizon
> video prediction. In Phase 2, we probe the learned latent space, select the
> most physically meaningful dimensions using linear correlation analysis, and
> refine the transition dynamics with a Lyapunov-based stability regularizer to
> enforce contraction and reduce error accumulation during roll-outs. Experiments
> on five synthetic benchmarks and four real-world dynamical systems, including
> chaotic phenomena, show that LyTimeT achieves mutual information and intrinsic
> dimension estimates closest to ground truth, remains invariant under background
> perturbations, and delivers the lowest analytical mean squared error among
> CNN-based (TIDE) and transformer-only baselines. Our results demonstrate that
> combining spatio-temporal attention with stability constraints yields
> predictive models that are not only accurate but also physically interpretable.

