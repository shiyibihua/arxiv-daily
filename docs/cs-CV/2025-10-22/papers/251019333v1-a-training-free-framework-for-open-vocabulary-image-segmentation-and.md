---
layout: default
title: A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP
---

# A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP

**arXiv**: [2510.19333v1](https://arxiv.org/abs/2510.19333) | [PDF](https://arxiv.org/pdf/2510.19333.pdf)

**ä½œè€…**: Ying Dai, Wei Yu Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— éœ€è®­ç»ƒæ¡†æž¶ï¼Œç»“åˆEfficientNetå’ŒCLIPå®žçŽ°å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ä¸Žè¯†åˆ«**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²` `æ— ç›‘ç£åˆ†å‰²` `è·¨æ¨¡æ€å¯¹é½` `EfficientNet` `CLIPæ¨¡åž‹` `å›¾åƒè¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ä¸Žè¯†åˆ«ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæ•°æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä¸¤é˜¶æ®µæµç¨‹ï¼Œå…ˆæ— ç›‘ç£åˆ†å‰²ï¼ŒåŽè·¨æ¨¡æ€å¯¹é½è¯†åˆ«ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨COCOç­‰åŸºå‡†ä¸Šï¼Œå®žçŽ°SOTAæ€§èƒ½ï¼ŒéªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a novel training-free framework for open-vocabulary image
> segmentation and object recognition (OVSR), which leverages EfficientNetB0, a
> convolutional neural network, for unsupervised segmentation and CLIP, a
> vision-language model, for open-vocabulary object recognition. The proposed
> framework adopts a two stage pipeline: unsupervised image segmentation followed
> by segment-level recognition via vision-language alignment. In the first stage,
> pixel-wise features extracted from EfficientNetB0 are decomposed using singular
> value decomposition to obtain latent representations, which are then clustered
> using hierarchical clustering to segment semantically meaningful regions. The
> number of clusters is adaptively determined by the distribution of singular
> values. In the second stage, the segmented regions are localized and encoded
> into image embeddings using the Vision Transformer backbone of CLIP. Text
> embeddings are precomputed using CLIP's text encoder from category-specific
> prompts, including a generic something else prompt to support open set
> recognition. The image and text embeddings are concatenated and projected into
> a shared latent feature space via SVD to enhance cross-modal alignment.
> Recognition is performed by computing the softmax over the similarities between
> the projected image and text embeddings. The proposed method is evaluated on
> standard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving
> state-of-the-art performance in terms of Hungarian mIoU, precision, recall, and
> F1-score. These results demonstrate the effectiveness, flexibility, and
> generalizability of the proposed framework.

