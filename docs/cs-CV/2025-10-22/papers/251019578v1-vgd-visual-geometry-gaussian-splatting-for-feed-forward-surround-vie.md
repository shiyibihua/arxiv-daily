---
layout: default
title: VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction
---

# VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction

**arXiv**: [2510.19578v1](https://arxiv.org/abs/2510.19578) | [PDF](https://arxiv.org/pdf/2510.19578.pdf)

**ä½œè€…**: Junhong Lin, Kangli Wang, Shunzhou Wang, Songlin Fan, Ge Li, Wei Gao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGDæ¡†æž¶ä»¥è§£å†³çŽ¯è§†é©¾é©¶åœºæ™¯é‡å»ºä¸­çš„å‡ ä½•ä¸€è‡´æ€§ä¸Žæ–°è§†å›¾è´¨é‡é—®é¢˜**

**å…³é”®è¯**: `çŽ¯è§†é©¾é©¶é‡å»º` `å‡ ä½•ä¸€è‡´æ€§` `é«˜æ–¯æ¸²æŸ“` `å¤šå°ºåº¦ç‰¹å¾èžåˆ` `å‰é¦ˆå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ¯è§†é©¾é©¶åœºæ™¯é‡å»ºä¸­ï¼Œè§†å›¾é‡å å°‘å¯¼è‡´å‡ ä½•ä¸€è‡´æ€§ä¸Žæ–°è§†å›¾è´¨é‡éš¾ä»¥ä¿è¯
2. é‡‡ç”¨è½»é‡VGGTå˜ä½“æå–å‡ ä½•å…ˆéªŒï¼Œå¹¶è®¾è®¡é«˜æ–¯å¤´èžåˆå¤šå°ºåº¦å‡ ä½•ç‰¹å¾é¢„æµ‹æ¸²æŸ“å‚æ•°
3. åœ¨nuScenesæ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒVGDåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è´¨é‡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Feed-forward surround-view autonomous driving scene reconstruction offers
> fast, generalizable inference ability, which faces the core challenge of
> ensuring generalization while elevating novel view quality. Due to the
> surround-view with minimal overlap regions, existing methods typically fail to
> ensure geometric consistency and reconstruction quality for novel views. To
> tackle this tension, we claim that geometric information must be learned
> explicitly, and the resulting features should be leveraged to guide the
> elevating of semantic quality in novel views. In this paper, we introduce
> \textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end
> learning framework designed to address this challenge. To achieve generalizable
> geometric estimation, we design a lightweight variant of the VGGT architecture
> to efficiently distill its geometric priors from the pre-trained VGGT to the
> geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale
> geometry tokens to predict Gaussian parameters for novel view rendering, which
> shares the same patch backbone as the geometry branch. Finally, we integrate
> multi-scale features from both geometry and Gaussian head branches to jointly
> supervise a semantic refinement model, optimizing rendering quality through
> feature-consistent learning. Experiments on nuScenes demonstrate that our
> approach significantly outperforms state-of-the-art methods in both objective
> metrics and subjective quality under various settings, which validates VGD's
> scalability and high-fidelity surround-view reconstruction.

