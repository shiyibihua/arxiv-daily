---
layout: default
title: The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models
---

# The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models

**arXiv**: [2510.19557v1](https://arxiv.org/abs/2510.19557) | [PDF](https://arxiv.org/pdf/2510.19557.pdf)

**ä½œè€…**: Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†ææç¤ºå¤æ‚åº¦å¯¹T2Iæ¨¡å‹åˆæˆæ•°æ®è´¨é‡ã€å¤šæ ·æ€§å’Œä¸€è‡´æ€§çš„å½±å“**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹` `åˆæˆæ•°æ®è¯„ä¼°` `æç¤ºå¤æ‚åº¦` `åˆ†å¸ƒåç§»` `æ¨ç†æ—¶å¹²é¢„` `æç¤ºæ‰©å±•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæç¤ºå¤æ‚åº¦å¯¹T2Iæ¨¡å‹åˆæˆæ•°æ®æ•ˆç”¨ï¼ˆè´¨é‡ã€å¤šæ ·æ€§ã€ä¸€è‡´æ€§ï¼‰çš„ç³»ç»Ÿå½±å“æœªå……åˆ†ç ”ç©¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ–°è¯„ä¼°æ¡†æ¶ï¼Œæ¯”è¾ƒçœŸå®ä¸åˆæˆæ•°æ®æ•ˆç”¨ï¼Œå¹¶è¿›è¡Œå¤§è§„æ¨¡å®éªŒåˆ†æã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå¢åŠ æç¤ºå¤æ‚åº¦é™ä½æ¡ä»¶å¤šæ ·æ€§å’Œæç¤ºä¸€è‡´æ€§ï¼Œä½†å‡å°‘åˆ†å¸ƒåç§»ï¼›æç¤ºæ‰©å±•æ–¹æ³•è¡¨ç°æœ€ä½³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-to-image (T2I) models offer great potential for creating virtually
> limitless synthetic data, a valuable resource compared to fixed and finite real
> datasets. Previous works evaluate the utility of synthetic data from T2I models
> on three key desiderata: quality, diversity, and consistency. While prompt
> engineering is the primary means of interacting with T2I models, the systematic
> impact of prompt complexity on these critical utility axes remains
> underexplored. In this paper, we first conduct synthetic experiments to
> motivate the difficulty of generalization w.r.t. prompt complexity and explain
> the observed difficulty with theoretical derivations. Then, we introduce a new
> evaluation framework that can compare the utility of real data and synthetic
> data, and present a comprehensive analysis of how prompt complexity influences
> the utility of synthetic data generated by commonly used T2I models. We conduct
> our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and
> evaluate different inference-time intervention methods. Our synthetic
> experiments show that generalizing to more general conditions is harder than
> the other way round, since the former needs an estimated likelihood that is not
> learned by diffusion models. Our large-scale empirical experiments reveal that
> increasing prompt complexity results in lower conditional diversity and prompt
> consistency, while reducing the synthetic-to-real distribution shift, which
> aligns with the synthetic experiments. Moreover, current inference-time
> interventions can augment the diversity of the generations at the expense of
> moving outside the support of real data. Among those interventions, prompt
> expansion, by deliberately using a pre-trained language model as a likelihood
> estimator, consistently achieves the highest performance in both image
> diversity and aesthetics, even higher than that of real data.

