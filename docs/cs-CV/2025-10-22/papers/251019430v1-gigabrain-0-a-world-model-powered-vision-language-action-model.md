---
layout: default
title: GigaBrain-0: A World Model-Powered Vision-Language-Action Model
---

# GigaBrain-0: A World Model-Powered Vision-Language-Action Model

**arXiv**: [2510.19430v1](https://arxiv.org/abs/2510.19430) | [PDF](https://arxiv.org/pdf/2510.19430.pdf)

**ä½œè€…**: GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGigaBrain-0ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡åž‹ç”Ÿæˆæ•°æ®ä»¥è§£å†³æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æ•°æ®æ”¶é›†éš¾é¢˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä¸–ç•Œæ¨¡åž‹æ•°æ®ç”Ÿæˆ` `RGBDè¾“å…¥å»ºæ¨¡` `å…·èº«é“¾å¼æ€ç»´` `æœºå™¨äººæ³›åŒ–` `è½»é‡çº§å˜ä½“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§„æ¨¡çœŸå®žæœºå™¨äººæ•°æ®æ”¶é›†æˆæœ¬é«˜ï¼Œé™åˆ¶è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ä¸–ç•Œæ¨¡åž‹ç”Ÿæˆå¤šæ ·åŒ–æ•°æ®ï¼Œå‡å°‘å¯¹çœŸå®žæ•°æ®çš„ä¾èµ–ï¼Œå¹¶å¼•å…¥RGBDè¾“å…¥å’Œå…·èº«é“¾å¼æ€ç»´ç›‘ç£ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çµå·§ã€é•¿è§†ç•Œå’Œç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­å®žçŽ°ä¼˜è¶Šæ³›åŒ–ï¼ŒåŒ…æ‹¬å¤–è§‚ã€ç‰©ä½“æ”¾ç½®å’Œè§†è§’å˜åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Training Vision-Language-Action (VLA) models for generalist robots typically
> requires large-scale real-world robot data, which is expensive and
> time-consuming to collect. The inefficiency of physical data collection
> severely limits the scalability, and generalization capacity of current VLA
> systems. To address this challenge, we introduce GigaBrain-0, a novel VLA
> foundation model empowered by world model-generated data (e.g., video
> generation, real2real transfer, human transfer, view transfer, sim2real
> transfer data). By leveraging world models to generate diverse data at scale,
> GigaBrain-0 significantly reduces reliance on real robot data while improving
> cross-task generalization. Our approach further improves policy robustness
> through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,
> enabling the model to reason about spatial geometry, object states, and
> long-horizon dependencies during task execution. This leads to substantial
> gains in real-world performance on dexterous, long-horizon, and mobile
> manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves
> superior generalization across variations in appearances (e.g., textures,
> colors), object placements, and camera viewpoints. Additionally, we present
> GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently
> on devices such as the NVIDIA Jetson AGX Orin.

