---
layout: default
title: Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks
---

# Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks

**arXiv**: [2510.19195v1](https://arxiv.org/abs/2510.19195) | [PDF](https://arxiv.org/pdf/2510.19195.pdf)

**ä½œè€…**: Kai Zeng, Zhanqian Wu, Kaixin Xiong, Xiaobao Wei, Xiangyu Guo, Zhenxin Zhu, Kalok Ho, Lijun Zhou, Bohan Zeng, Ming Lu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wentao Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDream4Driveæ¡†æž¶ä½œä¸ºåˆæˆæ•°æ®ç”Ÿæˆå™¨ï¼Œä»¥å¢žå¼ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä»»åŠ¡æ€§èƒ½**

**å…³é”®è¯**: `é©¾é©¶ä¸–ç•Œæ¨¡åž‹` `åˆæˆæ•°æ®ç”Ÿæˆ` `3Dæ„ŸçŸ¥å¼•å¯¼` `å¤šè§†è§’è§†é¢‘ç¼–è¾‘` `è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥` `è§’è½æ¡ˆä¾‹å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰é©¾é©¶ä¸–ç•Œæ¨¡åž‹å¿½è§†ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡è¯„ä¼°ï¼Œå¯¼è‡´åˆæˆæ•°æ®ç›Šå¤„ä¸æ˜Žæ˜¾
2. Dream4Driveåˆ†è§£è§†é¢‘ä¸º3Dæ„ŸçŸ¥å¼•å¯¼å›¾ï¼Œæ¸²æŸ“3Dèµ„äº§å¹¶å¾®è°ƒæ¨¡åž‹ç”Ÿæˆå¤šè§†è§’è§†é¢‘
3. å®žéªŒæ˜¾ç¤ºDream4Driveåœ¨å„ç§è®­ç»ƒè½®æ¬¡ä¸‹æœ‰æ•ˆæå‡ä¸‹æ¸¸æ„ŸçŸ¥æ¨¡åž‹æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in driving world models enable controllable generation of
> high-quality RGB videos or multimodal videos. Existing methods primarily focus
> on metrics related to generation quality and controllability. However, they
> often overlook the evaluation of downstream perception tasks, which are
> $\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing
> methods usually leverage a training strategy that first pretrains on synthetic
> data and finetunes on real data, resulting in twice the epochs compared to the
> baseline (real data only). When we double the epochs in the baseline, the
> benefit of synthetic data becomes negligible. To thoroughly demonstrate the
> benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data
> generation framework designed for enhancing the downstream perception tasks.
> Dream4Drive first decomposes the input video into several 3D-aware guidance
> maps and subsequently renders the 3D assets onto these guidance maps. Finally,
> the driving world model is fine-tuned to produce the edited, multi-view
> photorealistic videos, which can be used to train the downstream perception
> models. Dream4Drive enables unprecedented flexibility in generating multi-view
> corner cases at scale, significantly boosting corner case perception in
> autonomous driving. To facilitate future research, we also contribute a
> large-scale 3D asset dataset named DriveObj3D, covering the typical categories
> in driving scenarios and enabling diverse 3D-aware video editing. We conduct
> comprehensive experiments to show that Dream4Drive can effectively boost the
> performance of downstream perception models under various training epochs.
> Project: $\href{https://wm-research.github.io/Dream4Drive/}{this\ https\ URL}$

