---
layout: default
title: SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion
---

# SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion

**arXiv**: [2510.19215v1](https://arxiv.org/abs/2510.19215) | [PDF](https://arxiv.org/pdf/2510.19215.pdf)

**ä½œè€…**: Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSFGFusioné€šè¿‡è¡¨é¢æ‹Ÿåˆå¼•å¯¼ç›¸æœºä¸Ž4Dé›·è¾¾èžåˆï¼Œæå‡è‡ªåŠ¨é©¾é©¶3Dç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚**

**å…³é”®è¯**: `3Dç‰©ä½“æ£€æµ‹` `å¤šæ¨¡æ€èžåˆ` `è¡¨é¢æ‹Ÿåˆ` `4Dé›·è¾¾` `é¸Ÿçž°å›¾` `è‡ªåŠ¨é©¾é©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š4Dé›·è¾¾ç‚¹äº‘ç¨€ç–ä¸”åˆ†è¾¨çŽ‡ä½Žï¼Œé™åˆ¶ç‰©ä½“å‡ ä½•è¡¨ç¤ºå’Œå¤šæ¨¡æ€èžåˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¡¨é¢æ‹Ÿåˆæ¨¡åž‹ä¼°è®¡ç‰©ä½“å‚æ•°ï¼Œç”Ÿæˆå¯†é›†ä¼ªç‚¹äº‘å’Œç»Ÿä¸€BEVç‰¹å¾ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨TJ4DRadSetå’ŒVoDåŸºå‡†ä¸Šå®žçŽ°ä¼˜è¶Šæ£€æµ‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D object detection is essential for autonomous driving. As an emerging
> sensor, 4D imaging radar offers advantages as low cost, long-range detection,
> and accurate velocity measurement, making it highly suitable for object
> detection. However, its sparse point clouds and low resolution limit object
> geometric representation and hinder multi-modal fusion. In this study, we
> introduce SFGFusion, a novel camera-4D imaging radar detection network guided
> by surface fitting. By estimating quadratic surface parameters of objects from
> image and radar data, the explicit surface fitting model enhances spatial
> representation and cross-modal interaction, enabling more reliable prediction
> of fine-grained dense depth. The predicted depth serves two purposes: 1) in an
> image branch to guide the transformation of image features from perspective
> view (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improving
> spatial mapping accuracy; and 2) in a surface pseudo-point branch to generate
> dense pseudo-point cloud, mitigating the radar point sparsity. The original
> radar point cloud is also encoded in a separate radar branch. These two point
> cloud branches adopt a pillar-based method and subsequently transform the
> features into the BEV space. Finally, a standard 2D backbone and detection head
> are used to predict object labels and bounding boxes from BEV features.
> Experimental results show that SFGFusion effectively fuses camera and 4D radar
> features, achieving superior performance on the TJ4DRadSet and view-of-delft
> (VoD) object detection benchmarks.

