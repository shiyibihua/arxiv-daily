---
layout: default
title: XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography
---

# XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography

**arXiv**: [2510.19599v1](https://arxiv.org/abs/2510.19599) | [PDF](https://arxiv.org/pdf/2510.19599.pdf)

**ä½œè€…**: Haozhe Luo, Shelley Zixin Shu, Ziyu Zhou, Sebastian Otalora, Mauricio Reyes

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºXBenchåŸºå‡†ä»¥è¯„ä¼°èƒ¸ç‰‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„è·¨æ¨¡æ€å¯è§£é‡Šæ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŒ»å­¦å›¾åƒè§£é‡Š` `èƒ¸ç‰‡åŸºå‡†` `è·¨æ¨¡æ€å®šä½` `å¯è§£é‡Šæ€§è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨åŒ»å­¦å›¾åƒä¸­çš„è§†è§‰è¯æ®å¯¹é½èƒ½åŠ›ä¸è¶³ï¼Œå½±å“ä¸´åºŠå¯é æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å’Œç›¸ä¼¼æ€§å®šä½å›¾ç”Ÿæˆè§†è§‰è§£é‡Šï¼Œå¹¶ä¸Žæ”¾å°„ç§‘åŒ»ç”Ÿæ ‡æ³¨åŒºåŸŸå¯¹é½è¯„ä¼°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°æ¨¡åž‹å¯¹å°ç—…ç¶å®šä½æ€§èƒ½ä¸‹é™ï¼Œç‰¹å®šæ•°æ®é›†é¢„è®­ç»ƒå¯æ”¹å–„å¯¹é½ï¼Œè¯†åˆ«ä¸Žå®šä½èƒ½åŠ›ç›¸å…³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) have recently shown remarkable zero-shot
> performance in medical image understanding, yet their grounding ability, the
> extent to which textual concepts align with visual evidence, remains
> underexplored. In the medical domain, however, reliable grounding is essential
> for interpretability and clinical adoption. In this work, we present the first
> systematic benchmark for evaluating cross-modal interpretability in chest
> X-rays across seven CLIP-style VLM variants. We generate visual explanations
> using cross-attention and similarity-based localization maps, and
> quantitatively assess their alignment with radiologist-annotated regions across
> multiple pathologies. Our analysis reveals that: (1) while all VLM variants
> demonstrate reasonable localization for large and well-defined pathologies,
> their performance substantially degrades for small or diffuse lesions; (2)
> models that are pretrained on chest X-ray-specific datasets exhibit improved
> alignment compared to those trained on general-domain data. (3) The overall
> recognition ability and grounding ability of the model are strongly correlated.
> These findings underscore that current VLMs, despite their strong recognition
> ability, still fall short in clinically reliable grounding, highlighting the
> need for targeted interpretability benchmarks before deployment in medical
> practice. XBench code is available at
> https://github.com/Roypic/Benchmarkingattention

