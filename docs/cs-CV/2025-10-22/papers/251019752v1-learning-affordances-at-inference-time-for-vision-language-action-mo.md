---
layout: default
title: Learning Affordances at Inference-Time for Vision-Language-Action Models
---

# Learning Affordances at Inference-Time for Vision-Language-Action Models

**arXiv**: [2510.19752v1](https://arxiv.org/abs/2510.19752) | [PDF](https://arxiv.org/pdf/2510.19752.pdf)

**ä½œè€…**: Ameesh Shah, William Chen, Adwait Godbole, Federico Mora, Sanjit A. Seshia, Sergey Levine

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLITENæ–¹æ³•ï¼Œé€šè¿‡æŽ¨ç†æ—¶å­¦ä¹ æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„åŠ¨æ€è°ƒæ•´èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æŽ¨ç†æ—¶å­¦ä¹ ` `æœºå™¨äººæŽ§åˆ¶` `é•¿æ—¶ç¨‹ä»»åŠ¡` `è‡ªæˆ‘åæ€`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨æœºå™¨äººæŽ§åˆ¶ä¸­ç¼ºä¹å¤±è´¥åŽçš„ä¸Šä¸‹æ–‡åŠ¨æ€è¡Œä¸ºè°ƒæ•´èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆé«˜ä½Žå±‚æ¨¡åž‹ï¼Œé€šè¿‡æŽ¨ç†ä¸Žè¯„ä¼°é˜¶æ®µè¿­ä»£å­¦ä¹ ä½Žå±‚æ¨¡åž‹çš„å¯ç”¨æ€§ä¸Žèƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šLITENèƒ½ä»Žç»éªŒä¸­å­¦ä¹ ï¼Œç”Ÿæˆé«˜å¯ç”¨æ€§æŒ‡ä»¤ä»¥å®Œæˆé•¿æ—¶ç¨‹ä»»åŠ¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Solving complex real-world control tasks often takes multiple tries: if we
> fail at first, we reflect on what went wrong, and change our strategy
> accordingly to avoid making the same mistake. In robotics,
> Vision-Language-Action models (VLAs) offer a promising path towards solving
> complex control tasks, but lack the ability to contextually and dynamically
> readjust behavior when they fail to accomplish a task. In this work, we
> introduce Learning from Inference-Time Execution (LITEN), which connects a VLA
> low-level policy to a high-level VLM that conditions on past experiences by
> including them in-context, allowing it to learn the affordances and
> capabilities of the low-level VLA. Our approach iterates between a reasoning
> phase that generates and executes plans for the low-level VLA, and an
> assessment phase that reflects on the resulting execution and draws useful
> conclusions to be included in future reasoning contexts. Unlike similar
> approaches to self-refinement in non-robotics domains, LITEN must reflect on
> unstructured real-world robot trajectories (e.g., raw videos), which requires
> structured guiderails during assessment. Our experimental results demonstrate
> LITEN is able to effectively learn from past experience to generate plans that
> use high-affordance instructions to accomplish long-horizon tasks.

