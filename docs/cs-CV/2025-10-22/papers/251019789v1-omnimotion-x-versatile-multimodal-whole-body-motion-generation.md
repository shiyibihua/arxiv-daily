---
layout: default
title: OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation
---

# OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation

**arXiv**: [2510.19789v1](https://arxiv.org/abs/2510.19789) | [PDF](https://arxiv.org/pdf/2510.19789.pdf)

**ä½œè€…**: Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniMotion-Xæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€å…¨èº«è¿åŠ¨ç”Ÿæˆé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ` `è‡ªå›žå½’æ‰©æ•£å˜æ¢å™¨` `å‚è€ƒè¿åŠ¨æ¡ä»¶` `å¼±åˆ°å¼ºæ··åˆè®­ç»ƒ` `SMPL-Xæ•°æ®é›†` `é•¿æ—¶ç¨‹è¿åŠ¨æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¿åŠ¨ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸Žå¯æŽ§æ€§ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è‡ªå›žå½’æ‰©æ•£å˜æ¢å™¨ä¸Žå‚è€ƒè¿åŠ¨æ¡ä»¶å¢žå¼ºç”Ÿæˆè´¨é‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œå®žçŽ°é•¿æ—¶ç¨‹å¯æŽ§è¿åŠ¨ç”Ÿæˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper introduces OmniMotion-X, a versatile multimodal framework for
> whole-body human motion generation, leveraging an autoregressive diffusion
> transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently
> supports diverse multimodal tasks, including text-to-motion, music-to-dance,
> speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion
> prediction, in-betweening, completion, and joint/trajectory-guided synthesis),
> as well as flexible combinations of these tasks. Specifically, we propose the
> use of reference motion as a novel conditioning signal, substantially enhancing
> the consistency of generated content, style, and temporal dynamics crucial for
> realistic animations. To handle multimodal conflicts, we introduce a
> progressive weak-to-strong mixed-condition training strategy. To enable
> high-quality multimodal training, we construct OmniMoCap-X, the largest unified
> multimodal motion dataset to date, integrating 28 publicly available MoCap
> sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.
> To ensure detailed and consistent annotations, we render sequences into videos
> and use GPT-4o to automatically generate structured and hierarchical captions,
> capturing both low-level actions and high-level semantics. Extensive
> experimental evaluations confirm that OmniMotion-X significantly surpasses
> existing methods, demonstrating state-of-the-art performance across multiple
> multimodal tasks and enabling the interactive generation of realistic,
> coherent, and controllable long-duration motions.

