---
layout: default
title: Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism
---

# Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism

**arXiv**: [2510.19618v1](https://arxiv.org/abs/2510.19618) | [PDF](https://arxiv.org/pdf/2510.19618.pdf)

**ä½œè€…**: Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆé€šä¿¡æœºåˆ¶ä»¥è§£å†³å¼‚æž„å¤šæ™ºèƒ½ä½“åä½œæ„ŸçŸ¥ä¸­çš„é¢†åŸŸå·®è·é—®é¢˜**

**å…³é”®è¯**: `å¼‚æž„å¤šæ™ºèƒ½ä½“åä½œ` `ç”Ÿæˆé€šä¿¡æœºåˆ¶` `ç‰¹å¾ç”Ÿæˆ` `ç©ºé—´ä¿¡æ¯å¯¹é½` `è½»é‡çº§é›†æˆ` `æ¡ä»¶æ‰©æ•£æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼‚æž„æ™ºèƒ½ä½“å› ä¼ æ„Ÿå™¨å’Œæ¨¡åž‹å·®å¼‚å¯¼è‡´åä½œæ—¶å‡ºçŽ°é¢†åŸŸå·®è·ï¼ŒçŽ°æœ‰æ–¹æ³•ç ´åè¯­ä¹‰ä¸€è‡´æ€§ä¸”æ‰©å±•æˆæœ¬é«˜
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç”Ÿæˆé€šä¿¡æœºåˆ¶ï¼Œé€šè¿‡ç‰¹å¾ç”Ÿæˆå’Œç©ºé—´ä¿¡æ¯å¯¹é½ï¼Œæ— éœ€ä¿®æ”¹åŽŸç½‘ç»œï¼Œå®žçŽ°è½»é‡çº§æ–°æ™ºèƒ½ä½“é›†æˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ–°æ™ºèƒ½ä½“é›†æˆæ—¶è®¡ç®—æˆæœ¬å’Œå‚æ•°æ•°é‡å‡å°‘81%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-agent collaboration enhances the perception capabilities of individual
> agents through information sharing. However, in real-world applications,
> differences in sensors and models across heterogeneous agents inevitably lead
> to domain gaps during collaboration. Existing approaches based on adaptation
> and reconstruction fail to support pragmatic heterogeneous collaboration due to
> two key limitations: (1) Intrusive retraining of the encoder or core modules
> disrupts the established semantic consistency among agents; and (2)
> accommodating new agents incurs high computational costs, limiting scalability.
> To address these challenges, we present a novel Generative Communication
> mechanism (GenComm) that facilitates seamless perception across heterogeneous
> multi-agent systems through feature generation, without altering the original
> network, and employs lightweight numerical alignment of spatial information to
> efficiently integrate new agents at minimal cost. Specifically, a tailored
> Deformable Message Extractor is designed to extract spatial message for each
> collaborator, which is then transmitted in place of intermediate features. The
> Spatial-Aware Feature Generator, utilizing a conditional diffusion model,
> generates features aligned with the ego agent's semantic space while preserving
> the spatial information of the collaborators. These generated features are
> further refined by a Channel Enhancer before fusion. Experiments conducted on
> the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm
> outperforms existing state-of-the-art methods, achieving an 81\% reduction in
> both computational cost and parameter count when incorporating new agents. Our
> code is available at https://github.com/jeffreychou777/GenComm.

