---
layout: default
title: Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning
---

# Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning

**arXiv**: [2510.19495v1](https://arxiv.org/abs/2510.19495) | [PDF](https://arxiv.org/pdf/2510.19495.pdf)

**ä½œè€…**: Kevin Huang, Rosario Scalise, Cleah Winston, Ayush Agrawal, Yunchu Zhang, Rohan Baijal, Markus Grotz, Byron Boots, Benjamin Burchfiel, Hongkai Dai, Masha Itkina, Paarth Shah, Abhishek Gupta

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥åˆ©ç”¨éžä¸“å®¶æ•°æ®å¢žå¼ºæ¨¡ä»¿å­¦ä¹ çš„é²æ£’æ€§**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `éžä¸“å®¶æ•°æ®` `æœºå™¨äººæ“ä½œ` `ç­–ç•¥é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ¨¡ä»¿å­¦ä¹ ä¾èµ–é«˜è´¨é‡ä¸“å®¶æ•°æ®ï¼Œéš¾ä»¥é€‚åº”çœŸå®žä¸–ç•Œå¤šæ ·åœºæ™¯
2. é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•æ”¹è¿›ï¼Œåˆ©ç”¨éžä¸“å®¶æ•°æ®æ‰©å±•ç­–ç•¥åˆ†å¸ƒæ”¯æŒ
3. åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æå‡ç­–ç•¥çš„æ¢å¤èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Imitation learning has proven effective for training robots to perform
> complex tasks from expert human demonstrations. However, it remains limited by
> its reliance on high-quality, task-specific data, restricting adaptability to
> the diverse range of real-world object configurations and scenarios. In
> contrast, non-expert data -- such as play data, suboptimal demonstrations,
> partial task completions, or rollouts from suboptimal policies -- can offer
> broader coverage and lower collection costs. However, conventional imitation
> learning approaches fail to utilize this data effectively. To address these
> challenges, we posit that with right design decisions, offline reinforcement
> learning can be used as a tool to harness non-expert data to enhance the
> performance of imitation learning policies. We show that while standard offline
> RL approaches can be ineffective at actually leveraging non-expert data under
> the sparse data coverage settings typically encountered in the real world,
> simple algorithmic modifications can allow for the utilization of this data,
> without significant additional assumptions. Our approach shows that broadening
> the support of the policy distribution can allow imitation algorithms augmented
> by offline RL to solve tasks robustly, showing considerably enhanced recovery
> and generalization behavior. In manipulation tasks, these innovations
> significantly increase the range of initial conditions where learned policies
> are successful when non-expert data is incorporated. Moreover, we show that
> these methods are able to leverage all collected data, including partial or
> suboptimal demonstrations, to bolster task-directed policy performance. This
> underscores the importance of algorithmic techniques for using non-expert data
> for robust policy learning in robotics.

