---
layout: default
title: Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks
---

# Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks

**arXiv**: [2510.19760v1](https://arxiv.org/abs/2510.19760) | [PDF](https://arxiv.org/pdf/2510.19760.pdf)

**ä½œè€…**: Shaohang Jia, Zhiyong Huang, Zhi Yu, Mingyang Hou, Shuai Miao, Han Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”åˆ†å¸ƒæ„ŸçŸ¥é‡åŒ–æ¡†æž¶ä»¥è§£å†³æ··åˆç²¾åº¦ç¥žç»ç½‘ç»œé‡åŒ–é—®é¢˜**

**å…³é”®è¯**: `æ··åˆç²¾åº¦é‡åŒ–` `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ` `è‡ªé€‚åº”ç æœ¬` `éžå‡åŒ€åˆ†å¸ƒ` `ç¡¬ä»¶å‹å¥½æ˜ å°„` `ç¥žç»ç½‘ç»œåŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¿€æ´»åˆ†å¸ƒé«˜åº¦éžå‡åŒ€å’Œæƒé‡é‡åŒ–ç æœ¬é™æ€ä¸åŒ¹é…
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†ä½æ•°åˆå§‹åŒ–ã€åœ¨çº¿ç æœ¬è‡ªé€‚åº”å’Œæ•æ„Ÿåº¦æ··åˆç²¾åº¦åˆ†é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNetä¸ŠResNet-18è¾¾71.512%å‡†ç¡®çŽ‡ï¼Œå¹³å‡ä½å®½2.81ä½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Quantization-Aware Training (QAT) is a critical technique for deploying deep
> neural networks on resource-constrained devices. However, existing methods
> often face two major challenges: the highly non-uniform distribution of
> activations and the static, mismatched codebooks used in weight quantization.
> To address these challenges, we propose Adaptive Distribution-aware
> Quantization (ADQ), a mixed-precision quantization framework that employs a
> differentiated strategy. The core of ADQ is a novel adaptive weight
> quantization scheme comprising three key innovations: (1) a quantile-based
> initialization method that constructs a codebook closely aligned with the
> initial weight distribution; (2) an online codebook adaptation mechanism based
> on Exponential Moving Average (EMA) to dynamically track distributional shifts;
> and (3) a sensitivity-informed strategy for mixed-precision allocation. For
> activations, we integrate a hardware-friendly non-uniform-to-uniform mapping
> scheme. Comprehensive experiments validate the effectiveness of our method. On
> ImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with an
> average bit-width of only 2.81 bits, outperforming state-of-the-art methods
> under comparable conditions. Furthermore, detailed ablation studies on CIFAR-10
> systematically demonstrate the individual contributions of each innovative
> component, validating the rationale and effectiveness of our design.

