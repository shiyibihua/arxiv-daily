---
layout: default
title: PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation
---

# PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation

**arXiv**: [2510.19475v1](https://arxiv.org/abs/2510.19475) | [PDF](https://arxiv.org/pdf/2510.19475.pdf)

**ä½œè€…**: Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRGCNä»¥è§£å†³å•ç›®3Däººä½“å§¿æ€ä¼°è®¡ä¸­çš„è·¨åºåˆ—æ¨¡å¼é‡ç”¨é—®é¢˜**

**å…³é”®è¯**: `3Däººä½“å§¿æ€ä¼°è®¡` `å›¾å·ç§¯ç½‘ç»œ` `æ¨¡å¼é‡ç”¨` `è®°å¿†ç½‘ç»œ` `æ—¶ç©ºå»ºæ¨¡` `è·¨åºåˆ—å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•ç›®3Då§¿æ€ä¼°è®¡å› 2Dåˆ°3Dæå‡çš„æ·±åº¦æ¨¡ç³Šæ€§è€Œç—…æ€ï¼ŒçŽ°æœ‰æ–¹æ³•å­¤ç«‹å¤„ç†åºåˆ—ï¼Œå¿½ç•¥è·¨åºåˆ—è¿åŠ¨æ¨¡å¼ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å›¾è®°å¿†ç½‘ç»œï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ£€ç´¢å’Œèžåˆå§¿æ€åŽŸåž‹ï¼Œç»“åˆMambaä¸Žè‡ªæ³¨æ„åŠ›è¿›è¡Œæ—¶ç©ºç‰¹å¾æå–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Human3.6Må’ŒMPI-INF-3DHPåŸºå‡†ä¸Šè¾¾åˆ°SOTAï¼ŒMPJPEåˆ†åˆ«ä¸º37.1mmå’Œ13.4mmï¼Œæå‡è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Monocular 3D human pose estimation remains a fundamentally ill-posed inverse
> problem due to the inherent depth ambiguity in 2D-to-3D lifting. While
> contemporary video-based methods leverage temporal context to enhance spatial
> reasoning, they operate under a critical paradigm limitation: processing each
> sequence in isolation, thereby failing to exploit the strong structural
> regularities and repetitive motion patterns that pervade human movement across
> sequences. This work introduces the Pattern Reuse Graph Convolutional Network
> (PRGCN), a novel framework that formalizes pose estimation as a problem of
> pattern retrieval and adaptation. At its core, PRGCN features a graph memory
> bank that learns and stores a compact set of pose prototypes, encoded as
> relational graphs, which are dynamically retrieved via an attention mechanism
> to provide structured priors. These priors are adaptively fused with hard-coded
> anatomical constraints through a memory-driven graph convolution, ensuring
> geometrical plausibility. To underpin this retrieval process with robust
> spatiotemporal features, we design a dual-stream hybrid architecture that
> synergistically combines the linear-complexity, local temporal modeling of
> Mamba-based state-space models with the global relational capacity of
> self-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks
> demonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE
> of 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain
> generalization capability. Our work posits that the long-overlooked mechanism
> of cross-sequence pattern reuse is pivotal to advancing the field, shifting the
> paradigm from per-sequence optimization towards cumulative knowledge learning.

