---
layout: default
title: A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation
---

# A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation

**arXiv**: [2510.19755v1](https://arxiv.org/abs/2510.19755) | [PDF](https://arxiv.org/pdf/2510.19755.pdf)

**ä½œè€…**: Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ‰©æ•£ç¼“å­˜æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€ç”Ÿæˆæ•ˆçŽ‡**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `ç¼“å­˜æ–¹æ³•` `å¤šæ¨¡æ€ç”Ÿæˆ` `é«˜æ•ˆæŽ¨ç†` `è®­ç»ƒè‡ªç”±åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡åž‹å› å¤šæ­¥è¿­ä»£å’Œå¤æ‚ç½‘ç»œå¯¼è‡´é«˜è®¡ç®—å¼€é”€å’Œå»¶è¿Ÿ
2. æ‰©æ•£ç¼“å­˜é€šè¿‡ç‰¹å¾çº§è·¨æ­¥é‡ç”¨å’Œå±‚é—´è°ƒåº¦å®žçŽ°æ— è®­ç»ƒåŠ é€Ÿ
3. åˆ†æžæ˜¾ç¤ºç¼“å­˜ä»Žé™æ€é‡ç”¨æ¼”è¿›åˆ°åŠ¨æ€é¢„æµ‹ï¼Œå¢žå¼ºçµæ´»æ€§å’Œé›†æˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion Models have become a cornerstone of modern generative AI for their
> exceptional generation quality and controllability. However, their inherent
> \textit{multi-step iterations} and \textit{complex backbone networks} lead to
> prohibitive computational overhead and generation latency, forming a major
> bottleneck for real-time applications. Although existing acceleration
> techniques have made progress, they still face challenges such as limited
> applicability, high training costs, or quality degradation.
>   Against this backdrop, \textbf{Diffusion Caching} offers a promising
> training-free, architecture-agnostic, and efficient inference paradigm. Its
> core mechanism identifies and reuses intrinsic computational redundancies in
> the diffusion process. By enabling feature-level cross-step reuse and
> inter-layer scheduling, it reduces computation without modifying model
> parameters. This paper systematically reviews the theoretical foundations and
> evolution of Diffusion Caching and proposes a unified framework for its
> classification and analysis.
>   Through comparative analysis of representative methods, we show that
> Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
> prediction}. This trend enhances caching flexibility across diverse tasks and
> enables integration with other acceleration techniques such as sampling
> optimization and model distillation, paving the way for a unified, efficient
> inference framework for future multimodal and interactive applications. We
> argue that this paradigm will become a key enabler of real-time and efficient
> generative AI, injecting new vitality into both theory and practice of
> \textit{Efficient Generative Intelligence}.

