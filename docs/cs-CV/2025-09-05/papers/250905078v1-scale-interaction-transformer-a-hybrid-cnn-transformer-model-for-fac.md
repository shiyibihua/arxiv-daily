---
layout: default
title: Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction
---

# Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05078" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.05078v1</a>
  <a href="https://arxiv.org/pdf/2509.05078.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05078v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05078v1', 'Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Djamel Eddine Boukhari

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Scale-Interaction Transformer (SIT)Ê®°ÂûãÔºåÁî®‰∫éÊèêÂçáÈù¢ÈÉ®ÁæéÂ≠¶È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `Èù¢ÈÉ®ÁæéÂ≠¶È¢ÑÊµã` `Âç∑ÁßØÁ•ûÁªèÁΩëÁªú` `Transformer` `Â§öÂ∞∫Â∫¶ÁâπÂæÅ` `Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈù¢ÈÉ®ÁæéÂ≠¶È¢ÑÊµãÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÊçïÊçâ‰∏çÂêåÂ∞∫Â∫¶Èù¢ÈÉ®ÁâπÂæÅ‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥ËÅî„ÄÇ
2. ÊèêÂá∫Scale-Interaction Transformer (SIT)Ê®°ÂûãÔºåÁªìÂêàCNNÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæÅÊèêÂèñÂíåTransformerÁöÑÂÖ≥Á≥ªÂª∫Ê®°ËÉΩÂäõ„ÄÇ
3. Âú®SCUT-FBP5500Êï∞ÊçÆÈõÜ‰∏äÔºåSITÊ®°ÂûãÂèñÂæó‰∫Ü0.9187ÁöÑPearsonÁõ∏ÂÖ≥ÊÄßÔºåËææÂà∞Êñ∞ÁöÑstate-of-the-art„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Èù¢ÈÉ®ÁæéÂ≠¶È¢ÑÊµã(FBP)ÊòØ‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËÆ°ÁÆóÊú∫ËßÜËßâ‰ªªÂä°ÔºåÂõ†‰∏∫ÂÆÉÂèóÂà∞ÂΩ±Âìç‰∫∫Á±ªÊÑüÁü•ÁöÑÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÈù¢ÈÉ®ÁâπÂæÅÂ§çÊùÇÁõ∏‰∫í‰ΩúÁî®ÁöÑÂΩ±Âìç„ÄÇÂç∑ÁßØÁ•ûÁªèÁΩëÁªú(CNN)ÊìÖÈïøÁâπÂæÅÊèêÂèñÔºå‰ΩÜÈÄöÂ∏∏‰ª•Âõ∫ÂÆöÂ∞∫Â∫¶Â§ÑÁêÜ‰ø°ÊÅØÔºåÂèØËÉΩÂøΩÁï•‰∫Ü‰∏çÂêåÁ≤íÂ∫¶Á∫ßÂà´ÁâπÂæÅ‰πãÈó¥ÁöÑÂÖ≥ÈîÆÁõ∏‰∫í‰æùËµñÂÖ≥Á≥ª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜScale-Interaction Transformer (SIT)ÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÂûãÊ∑∑ÂêàÊ∑±Â∫¶Â≠¶‰π†Êû∂ÊûÑÔºåÂÆÉÂ∞ÜCNNÁöÑÁâπÂæÅÊèêÂèñËÉΩÂäõ‰∏éTransformerÁöÑÂÖ≥Á≥ªÂª∫Ê®°ËÉΩÂäõÁõ∏ÁªìÂêà„ÄÇSITÈ¶ñÂÖàÈááÁî®ÂÖ∑ÊúâÂπ∂Ë°åÂç∑ÁßØÁöÑÂ§öÂ∞∫Â∫¶Ê®°ÂùóÔºå‰ª•ÊçïËé∑‰∏çÂêåÊÑüÂèóÈáéÁöÑÈù¢ÈÉ®ÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºåËøô‰∫õÂ§öÂ∞∫Â∫¶Ë°®Á§∫Ë¢´ÊûÑÂª∫‰∏∫Â∫èÂàóÔºåÂπ∂Áî±TransformerÁºñÁ†ÅÂô®Â§ÑÁêÜÔºåËØ•ÁºñÁ†ÅÂô®ÈÄöËøáËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòæÂºèÂú∞Âª∫Ê®°ÂÆÉ‰ª¨ÁöÑ‰∫§‰∫íÂíå‰∏ä‰∏ãÊñáÂÖ≥Á≥ª„ÄÇÊàë‰ª¨Âú®ÂπøÊ≥õ‰ΩøÁî®ÁöÑSCUT-FBP5500Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™åÔºåÊâÄÊèêÂá∫ÁöÑSITÊ®°ÂûãÂª∫Á´ã‰∫ÜÊñ∞ÁöÑstate-of-the-artÔºåÂÆûÁé∞‰∫Ü0.9187ÁöÑPearsonÁõ∏ÂÖ≥ÊÄßÔºå‰ºò‰∫é‰ª•ÂâçÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÊòæÂºèÂú∞Âª∫Ê®°Â§öÂ∞∫Â∫¶ËßÜËßâÁ∫øÁ¥¢‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®ÂØπ‰∫éÈ´òÊÄßËÉΩFBPËá≥ÂÖ≥ÈáçË¶Å„ÄÇSITÊû∂ÊûÑÁöÑÊàêÂäüÁ™ÅÂá∫‰∫ÜÊ∑∑ÂêàCNN-TransformerÊ®°ÂûãÂú®ÈúÄË¶ÅÊï¥‰ΩìÁöÑ„ÄÅ‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÁêÜËß£ÁöÑÂ§çÊùÇÂõæÂÉèÂõûÂΩí‰ªªÂä°‰∏≠ÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈù¢ÈÉ®ÁæéÂ≠¶È¢ÑÊµã(FBP)Êó®Âú®Ëá™Âä®ËØÑ‰º∞Èù¢ÈÉ®ÁöÑÁæéËßÇÁ®ãÂ∫¶„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éCNNÁöÑÊñπÊ≥ïÔºåËôΩÁÑ∂ÊìÖÈïøÁâπÂæÅÊèêÂèñÔºå‰ΩÜÈÄöÂ∏∏‰ª•Âõ∫ÂÆöÂ∞∫Â∫¶Â§ÑÁêÜ‰ø°ÊÅØÔºåÈöæ‰ª•ÊçïÊçâ‰∏çÂêåÂ∞∫Â∫¶Èù¢ÈÉ®ÁâπÂæÅ‰πãÈó¥ÁöÑÂ§çÊùÇÁõ∏‰∫í‰æùËµñÂÖ≥Á≥ªÔºåËøôÈôêÂà∂‰∫ÜÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê∑∑ÂêàCNN-TransformerÊû∂ÊûÑÔºåÁªìÂêàCNNÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæÅÊèêÂèñËÉΩÂäõÂíåTransformerÁöÑÂÖ≥Á≥ªÂª∫Ê®°ËÉΩÂäõ„ÄÇÈÄöËøáÂ§öÂ∞∫Â∫¶Âç∑ÁßØÊèêÂèñ‰∏çÂêåÊÑüÂèóÈáéÁöÑÁâπÂæÅÔºåÁÑ∂ÂêéÂà©Áî®TransformerÊòæÂºèÂú∞Âª∫Ê®°Ëøô‰∫õÁâπÂæÅ‰πãÈó¥ÁöÑ‰∫§‰∫íÂíå‰∏ä‰∏ãÊñáÂÖ≥Á≥ªÔºå‰ªéËÄåÊõ¥ÂÖ®Èù¢Âú∞ÁêÜËß£Èù¢ÈÉ®ÁæéÂ≠¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSITÊ®°Âûã‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÂ§öÂ∞∫Â∫¶ÁâπÂæÅÊèêÂèñÂíåTransformerÁºñÁ†Å„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®‰∏Ä‰∏™Â§öÂ∞∫Â∫¶Ê®°ÂùóÔºåÂåÖÂê´Â§ö‰∏™Âπ∂Ë°åÁöÑÂç∑ÁßØÂ±ÇÔºå‰ª•‰∏çÂêåÁöÑÊÑüÂèóÈáéÊèêÂèñÈù¢ÈÉ®ÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜËøô‰∫õÂ§öÂ∞∫Â∫¶ÁâπÂæÅË°®Á§∫Êàê‰∏Ä‰∏™Â∫èÂàóÔºåËæìÂÖ•Âà∞TransformerÁºñÁ†ÅÂô®‰∏≠„ÄÇTransformerÁºñÁ†ÅÂô®Âà©Áî®Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊòæÂºèÂú∞Âª∫Ê®°‰∏çÂêåÂ∞∫Â∫¶ÁâπÂæÅ‰πãÈó¥ÁöÑ‰∫§‰∫íÂíå‰∏ä‰∏ãÊñáÂÖ≥Á≥ª„ÄÇÊúÄÂêéÔºåTransformerÁöÑËæìÂá∫Ë¢´Áî®‰∫éÈ¢ÑÊµãÈù¢ÈÉ®ÁæéÂ≠¶ÂæóÂàÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSITÊ®°ÂûãÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊòæÂºèÂú∞Âª∫Ê®°Â§öÂ∞∫Â∫¶Èù¢ÈÉ®ÁâπÂæÅ‰πãÈó¥ÁöÑ‰∫§‰∫í„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏Áã¨Á´ãÂú∞Â§ÑÁêÜ‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæÅÔºåÂøΩÁï•‰∫ÜÂÆÉ‰ª¨‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇSITÈÄöËøáTransformerÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÂ≠¶‰π†Âà∞‰∏çÂêåÂ∞∫Â∫¶ÁâπÂæÅ‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµãÈù¢ÈÉ®ÁæéÂ≠¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ§öÂ∞∫Â∫¶Ê®°Âùó‰ΩøÁî®‰∫ÜÂ§ö‰∏™Âπ∂Ë°åÁöÑÂç∑ÁßØÂ±ÇÔºåÊØè‰∏™Âç∑ÁßØÂ±ÇÂÖ∑Êúâ‰∏çÂêåÁöÑÂç∑ÁßØÊ†∏Â§ßÂ∞èÔºå‰ª•ÊèêÂèñ‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæÅ„ÄÇTransformerÁºñÁ†ÅÂô®‰ΩøÁî®‰∫ÜÊ†áÂáÜÁöÑTransformerÁªìÊûÑÔºåÂåÖÊã¨Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂâçÈ¶àÁ•ûÁªèÁΩëÁªú„ÄÇÊçüÂ§±ÂáΩÊï∞‰ΩøÁî®‰∫ÜÂùáÊñπËØØÂ∑Æ(MSE)ÊçüÂ§±ÔºåÁî®‰∫éË°°ÈáèÈ¢ÑÊµãÂæóÂàÜÂíåÁúüÂÆûÂæóÂàÜ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SITÊ®°ÂûãÂú®SCUT-FBP5500Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåPearsonÁõ∏ÂÖ≥ÊÄßËææÂà∞0.9187ÔºåË∂ÖË∂ä‰∫Ü‰πãÂâçÊâÄÊúâÊñπÊ≥ï„ÄÇÁõ∏ÊØî‰∫é‰πãÂâçÁöÑstate-of-the-artÊñπÊ≥ïÔºåSITÊ®°ÂûãÂú®È¢ÑÊµãÁ≤æÂ∫¶‰∏äÂèñÂæó‰∫ÜÊòéÊòæÁöÑËøõÊ≠•ÔºåËØÅÊòé‰∫ÜÊòæÂºèÂª∫Ê®°Â§öÂ∞∫Â∫¶ÁâπÂæÅ‰∫§‰∫íÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰∏™ÊÄßÂåñÁæéÂÆπÊé®Ëçê„ÄÅËôöÊãüÂΩ¢Ë±°ËÆæËÆ°„ÄÅ‰ª•Âèä‰∫∫ËÑ∏ËØÜÂà´Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂØπÈù¢ÈÉ®ÁæéÂ≠¶ËøõË°åËá™Âä®ËØÑ‰º∞ÔºåÂèØ‰ª•‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Á≤æÂáÜÁöÑÁæéÂÆπÂª∫ËÆÆÔºåÊèêÂçáËôöÊãüÂΩ¢Ë±°ÁöÑÂê∏ÂºïÂäõÔºåÂπ∂ÊîπÂñÑ‰∫∫ËÑ∏ËØÜÂà´Á≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®ÂåªÁñóÁæéÂÆπ„ÄÅÁ§æ‰∫§Â™í‰ΩìÁ≠âÈ¢ÜÂüüÂèëÊå•Êõ¥Â§ßÁöÑ‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Automated Facial Beauty Prediction (FBP) is a challenging computer vision task due to the complex interplay of local and global facial features that influence human perception. While Convolutional Neural Networks (CNNs) excel at feature extraction, they often process information at a fixed scale, potentially overlooking the critical inter-dependencies between features at different levels of granularity. To address this limitation, we introduce the Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture that synergizes the feature extraction power of CNNs with the relational modeling capabilities of Transformers. The SIT first employs a multi-scale module with parallel convolutions to capture facial characteristics at varying receptive fields. These multi-scale representations are then framed as a sequence and processed by a Transformer encoder, which explicitly models their interactions and contextual relationships via a self-attention mechanism. We conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark dataset, where the proposed SIT model establishes a new state-of-the-art. It achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our findings demonstrate that explicitly modeling the interplay between multi-scale visual cues is crucial for high-performance FBP. The success of the SIT architecture highlights the potential of hybrid CNN-Transformer models for complex image regression tasks that demand a holistic, context-aware understanding.

