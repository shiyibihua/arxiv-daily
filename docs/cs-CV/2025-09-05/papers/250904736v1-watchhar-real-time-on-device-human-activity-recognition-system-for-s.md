---
layout: default
title: WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches
---

# WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04736" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04736v1</a>
  <a href="https://arxiv.org/pdf/2509.04736.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04736v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04736v1', 'WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Taeyoung Yeon, Vasco Xu, Henry Hoffmann, Karan Ahuja

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: 8 pages, 4 figures, ICMI '25 (27th International Conference on Multimodal Interaction), October 13-17, 2025, Canberra, ACT, Australia

**DOI**: [10.1145/3716553.3750775](https://doi.org/10.1145/3716553.3750775)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WatchHARï¼šé¢å‘æ™ºèƒ½æ‰‹è¡¨çš„å®æ—¶ã€ç«¯ä¾§äººä½“æ´»åŠ¨è¯†åˆ«ç³»ç»Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººä½“æ´»åŠ¨è¯†åˆ«` `æ™ºèƒ½æ‰‹è¡¨` `ç«¯ä¾§è®¡ç®—` `å¤šæ¨¡æ€èåˆ` `å®æ—¶ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç»†ç²’åº¦äººä½“æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰ç³»ç»Ÿéš¾ä»¥åœ¨æ™ºèƒ½æ‰‹è¡¨ä¸Šç‹¬ç«‹è¿è¡Œï¼Œé¢ä¸´éšç§å’Œå»¶è¿ŸæŒ‘æˆ˜ã€‚
2. WatchHARé€šè¿‡ä¼˜åŒ–æµæ°´çº¿ç»„ä»¶ï¼Œå¹¶æå‡ºç«¯åˆ°ç«¯å¯è®­ç»ƒæ¶æ„ï¼Œç»Ÿä¸€é¢„å¤„ç†å’Œæ¨ç†ï¼Œæå‡æ•ˆç‡ã€‚
3. WatchHARåœ¨æ™ºèƒ½æ‰‹è¡¨ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ï¼Œäº‹ä»¶æ£€æµ‹9.3msï¼Œå¤šæ¨¡æ€åˆ†ç±»11.8msã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºWatchHARï¼Œä¸€ä¸ªå®Œå…¨è¿è¡Œåœ¨æ™ºèƒ½æ‰‹è¡¨ä¸Šçš„ã€åŸºäºéŸ³é¢‘å’Œæƒ¯æ€§ä¼ æ„Ÿå™¨çš„HARç³»ç»Ÿï¼Œè§£å†³äº†å¤–éƒ¨æ•°æ®å¤„ç†å¸¦æ¥çš„éšç§å’Œå»¶è¿Ÿé—®é¢˜ã€‚é€šè¿‡ä¼˜åŒ–æµæ°´çº¿çš„æ¯ä¸ªç»„ä»¶ï¼ŒWatchHARå®ç°äº†ç´¯ç§¯çš„æ€§èƒ½æå‡ã€‚è®ºæ–‡å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼Œå°†ä¼ æ„Ÿå™¨æ•°æ®é¢„å¤„ç†å’Œæ¨ç†ç»Ÿä¸€åˆ°ä¸€ä¸ªç«¯åˆ°ç«¯å¯è®­ç»ƒçš„æ¨¡å—ä¸­ï¼Œåœ¨ä¿æŒè¶…è¿‡90%å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¤„ç†é€Ÿåº¦æé«˜äº†5å€ï¼Œæ”¯æŒè¶…è¿‡25ç§æ´»åŠ¨ç±»åˆ«ã€‚WatchHARåœ¨æ™ºèƒ½æ‰‹è¡¨ä¸Šç›´æ¥è¿è¡Œï¼Œå…¶äº‹ä»¶æ£€æµ‹å’Œæ´»åŠ¨åˆ†ç±»æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæ´»åŠ¨äº‹ä»¶æ£€æµ‹çš„å¤„ç†æ—¶é—´ä¸º9.3æ¯«ç§’ï¼Œå¤šæ¨¡æ€æ´»åŠ¨åˆ†ç±»çš„å¤„ç†æ—¶é—´ä¸º11.8æ¯«ç§’ã€‚è¿™é¡¹ç ”ç©¶æ¨è¿›äº†ç«¯ä¾§æ´»åŠ¨è¯†åˆ«æŠ€æœ¯ï¼Œå®ç°äº†æ™ºèƒ½æ‰‹è¡¨ä½œä¸ºç‹¬ç«‹çš„ã€æ³¨é‡éšç§çš„ã€å¾®åˆ›çš„è¿ç»­æ´»åŠ¨è·Ÿè¸ªè®¾å¤‡çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººä½“æ´»åŠ¨è¯†åˆ«ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå¤–éƒ¨æ•°æ®å¤„ç†ï¼Œè¿™å¸¦æ¥äº†éšç§æ³„éœ²çš„é£é™©ï¼Œå¹¶ä¸”ç”±äºæ•°æ®ä¼ è¾“å’Œå¤„ç†çš„å»¶è¿Ÿï¼Œæ— æ³•æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚ç‰¹åˆ«æ˜¯åœ¨æ™ºèƒ½æ‰‹è¡¨ç­‰èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œå¦‚ä½•é«˜æ•ˆåœ°è¿›è¡Œæ´»åŠ¨è¯†åˆ«æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWatchHARçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ•´ä¸ªæ´»åŠ¨è¯†åˆ«æµç¨‹ï¼ŒåŒ…æ‹¬ä¼ æ„Ÿå™¨æ•°æ®é¢„å¤„ç†å’Œæ´»åŠ¨åˆ†ç±»ï¼Œéƒ½æ”¾åœ¨æ™ºèƒ½æ‰‹è¡¨ç«¯è¿›è¡Œã€‚é€šè¿‡ä¼˜åŒ–ç®—æ³•å’Œæ¨¡å‹æ¶æ„ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜å¤„ç†é€Ÿåº¦ï¼Œä»è€Œåœ¨ä¿è¯å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå®ç°å®æ—¶ã€ä½å»¶è¿Ÿçš„æ´»åŠ¨è¯†åˆ«ã€‚åŒæ—¶ï¼Œç«¯ä¾§å¤„ç†é¿å…äº†æ•°æ®ä¸Šä¼ ï¼Œä¿æŠ¤äº†ç”¨æˆ·éšç§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWatchHARçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†**ï¼šä»æ™ºèƒ½æ‰‹è¡¨çš„æƒ¯æ€§ä¼ æ„Ÿå™¨ï¼ˆåŠ é€Ÿåº¦è®¡ã€é™€èºä»ªï¼‰å’Œéº¦å…‹é£é‡‡é›†æ•°æ®ã€‚2) **æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾æå–**ï¼šå°†ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œæ»¤æ³¢ã€é™å™ªç­‰é¢„å¤„ç†ï¼Œå¹¶æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼Œè¿™é€šå¸¸æ˜¯ç‹¬ç«‹çš„æ­¥éª¤ã€‚WatchHARå°†å…¶ä¸æ¨ç†æ¨¡å—ç»Ÿä¸€ã€‚3) **æ´»åŠ¨åˆ†ç±»**ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æå–çš„ç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œè¯†åˆ«ç”¨æˆ·çš„æ´»åŠ¨ç±»å‹ã€‚4) **äº‹ä»¶æ£€æµ‹**ï¼šæ£€æµ‹æ´»åŠ¨å‘ç”Ÿçš„èµ·å§‹å’Œç»“æŸæ—¶é—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šWatchHARæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„æ¶æ„ï¼Œè¯¥æ¶æ„å°†ä¼ æ„Ÿå™¨æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œæ´»åŠ¨åˆ†ç±»é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä¸­ã€‚è¿™ç§è®¾è®¡é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å„ä¸ªæ¨¡å—ä¹‹é—´çš„ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è”åˆä¼˜åŒ–æ¥æé«˜æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ™ºèƒ½æ‰‹è¡¨çš„èµ„æºé™åˆ¶ï¼Œè®ºæ–‡å¯¹æ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºå…³é”®å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„å…·ä½“ç»†èŠ‚ã€‚ä½†å¯ä»¥æ¨æ–­ï¼Œä¸ºäº†å®ç°ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¯èƒ½ä½¿ç”¨äº†å¯å¾®åˆ†çš„é¢„å¤„ç†æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†è½»é‡çº§çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œä»¥é™ä½è®¡ç®—é‡ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬åˆ†ç±»æŸå¤±ï¼ˆä¾‹å¦‚äº¤å‰ç†µæŸå¤±ï¼‰å’Œæ­£åˆ™åŒ–é¡¹ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®å¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„ç¡¬ä»¶å¹³å°å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

WatchHARåœ¨æ™ºèƒ½æ‰‹è¡¨ä¸Šå®ç°äº†9.3æ¯«ç§’çš„æ´»åŠ¨äº‹ä»¶æ£€æµ‹å¤„ç†æ—¶é—´å’Œ11.8æ¯«ç§’çš„å¤šæ¨¡æ€æ´»åŠ¨åˆ†ç±»å¤„ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†è¶…è¿‡90%çš„å‡†ç¡®ç‡ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒWatchHARåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ç«¯ä¾§è®¾å¤‡ä¸Šè¿è¡Œæ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚è¯¥ç³»ç»Ÿæ”¯æŒè¶…è¿‡25ç§æ´»åŠ¨ç±»åˆ«ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

WatchHARå¯åº”ç”¨äºå¥åº·ç›‘æµ‹ã€è¿åŠ¨è¿½è¸ªã€è·Œå€’æ£€æµ‹ã€è€å¹´äººçœ‹æŠ¤ç­‰é¢†åŸŸã€‚é€šè¿‡åœ¨æ™ºèƒ½æ‰‹è¡¨ä¸Šå®æ—¶è¯†åˆ«ç”¨æˆ·çš„æ´»åŠ¨çŠ¶æ€ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„å¥åº·å»ºè®®å’Œé¢„è­¦ã€‚ç”±äºæ•°æ®å®Œå…¨åœ¨æœ¬åœ°å¤„ç†ï¼Œå› æ­¤å…·æœ‰å¾ˆé«˜çš„éšç§ä¿æŠ¤æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–å¯ç©¿æˆ´è®¾å¤‡ï¼Œå®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite advances in practical and multimodal fine-grained Human Activity Recognition (HAR), a system that runs entirely on smartwatches in unconstrained environments remains elusive. We present WatchHAR, an audio and inertial-based HAR system that operates fully on smartwatches, addressing privacy and latency issues associated with external data processing. By optimizing each component of the pipeline, WatchHAR achieves compounding performance gains. We introduce a novel architecture that unifies sensor data preprocessing and inference into an end-to-end trainable module, achieving 5x faster processing while maintaining over 90% accuracy across more than 25 activity classes. WatchHAR outperforms state-of-the-art models for event detection and activity classification while running directly on the smartwatch, achieving 9.3 ms processing time for activity event detection and 11.8 ms for multimodal activity classification. This research advances on-device activity recognition, realizing smartwatches' potential as standalone, privacy-aware, and minimally-invasive continuous activity tracking devices.

