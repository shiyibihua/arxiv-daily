---
layout: default
title: Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper
---

# Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04957" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04957v1</a>
  <a href="https://arxiv.org/pdf/2509.04957.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04957v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04957v1', 'Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gehui Chen, Guan'an Wang, Xiaowen Huang, Jitao Sang

**åˆ†ç±»**: cs.CV, cs.MM, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šåŸºç¡€æ¨¡å‹æ˜ å°„å™¨(MFM-Mapper)ï¼Œé«˜æ•ˆç”Ÿæˆä¸è§†é¢‘å†…å®¹åŒ¹é…çš„éŸ³é¢‘ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `åŸºç¡€æ¨¡å‹` `ç‰¹å¾æ˜ å°„` `GPT-2` `è‡ªå›å½’æ¨¡å‹` `è·¨æ¨¡æ€å¯¹é½` `é«˜æ•ˆè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰V2Aæ–¹æ³•ä¾èµ–å¤§é‡èµ„æºä»å¤´è®­ç»ƒæ¨¡å‹ï¼Œæˆ–ä»…åˆ©ç”¨å•ä¸€è§†è§‰ç¼–ç å™¨æå–æœ‰é™çš„è§†é¢‘ç‰¹å¾ã€‚
2. MFM-MapperèåˆåŒè§†è§‰ç¼–ç å™¨ç‰¹å¾ï¼Œå¹¶ç”¨GPT-2æ›¿æ¢çº¿æ€§æ˜ å°„å™¨ï¼Œæå‡ç‰¹å¾å¯¹é½èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆè·¨æ¨¡æ€æ˜ å°„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMFM-Mapperä»…éœ€å°‘é‡è®­ç»ƒèµ„æºå³å¯è¾¾åˆ°ä¸å¤§è§„æ¨¡æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨è¯­ä¹‰å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šæœ‰æ‰€æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šåŸºç¡€æ¨¡å‹æ˜ å°„å™¨(MFM-Mapper)çš„é«˜æ•ˆè§†é¢‘åˆ°éŸ³é¢‘(V2A)ç”Ÿæˆæ–¹æ³•ã€‚ç°æœ‰çš„V2Aç”Ÿæˆä¾èµ–äºä»è§†é¢‘ä¸­æå–è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾æ¥è°ƒèŠ‚ç”Ÿæˆæ¨¡å‹ï¼Œè€Œä»å¤´è®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„èµ„æºã€‚å› æ­¤ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹(FMs)çš„è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»å’Œæ³›åŒ–èƒ½åŠ›è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚å…ˆå‰çš„å·¥ä½œæ¢ç´¢äº†å¾®è°ƒä¸€ä¸ªè½»é‡çº§çš„æ˜ å°„å™¨ç½‘ç»œï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹è¿æ¥èµ·æ¥ï¼Œç”¨äºV2Aã€‚å—æ­¤å¯å‘ï¼ŒMFM-Mapperé€šè¿‡èåˆæ¥è‡ªåŒè§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ï¼Œå—ç›Šäºæ›´ä¸°å¯Œçš„è¯­ä¹‰å’Œæ—¶é—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç”¨GPT-2æ›¿æ¢çº¿æ€§æ˜ å°„å™¨ï¼ŒMFM-Mapperæ”¹è¿›äº†ç‰¹å¾å¯¹é½ï¼Œå°†è·¨æ¨¡æ€ç‰¹å¾æ˜ å°„ä¸è‡ªå›å½’ç¿»è¯‘ä»»åŠ¡è”ç³»èµ·æ¥ã€‚MFM-Mapperè¡¨ç°å‡ºå“è¶Šçš„è®­ç»ƒæ•ˆç‡ï¼Œä»¥æ›´å°‘çš„è®­ç»ƒæ¶ˆè€—åœ¨è¯­ä¹‰å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å®ç°äº†æ›´å¥½çš„æ€§èƒ½ï¼Œä»…éœ€å…ˆå‰åŸºäºæ˜ å°„å™¨çš„å·¥ä½œçš„16%çš„è®­ç»ƒè§„æ¨¡ï¼Œä½†å®ç°äº†ä¸åœ¨æ›´å¤§è§„æ¨¡ä¸Šè®­ç»ƒçš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆ(V2A)æ—¨åœ¨æ ¹æ®ç»™å®šçš„è§†é¢‘å†…å®¹ç”Ÿæˆç›¸åº”çš„éŸ³é¢‘ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ä»å¤´è®­ç»ƒå¤æ‚çš„ç”Ÿæˆæ¨¡å‹ï¼Œè®¡ç®—èµ„æºéœ€æ±‚å·¨å¤§ã€‚æ­¤å¤–ï¼Œä¸€äº›åŸºäºæ˜ å°„å™¨çš„æ–¹æ³•è™½ç„¶é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œä½†å¯èƒ½ä»…ä¾èµ–å•ä¸€è§†è§‰ç¼–ç å™¨ï¼Œå¯¼è‡´æå–çš„è§†é¢‘ç‰¹å¾ä¸å¤Ÿä¸°å¯Œï¼Œå½±å“ç”ŸæˆéŸ³é¢‘çš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMFM-Mapperçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œåˆ†åˆ«æå–è§†é¢‘çš„ä¸åŒç‰¹å¾ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªé«˜æ•ˆçš„æ˜ å°„å™¨ç½‘ç»œå°†è¿™äº›ç‰¹å¾å¯¹é½åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„è¾“å…¥ç©ºé—´ã€‚é€šè¿‡èåˆæ¥è‡ªä¸åŒè§†è§‰ç¼–ç å™¨çš„ä¿¡æ¯ï¼Œå¯ä»¥è·å¾—æ›´å…¨é¢ã€æ›´ä¸°å¯Œçš„è§†é¢‘è¯­ä¹‰å’Œæ—¶é—´ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå°†çº¿æ€§æ˜ å°„å™¨æ›¿æ¢ä¸ºGPT-2ï¼Œå€Ÿé‰´è‡ªå›å½’ç¿»è¯‘çš„æ€æƒ³ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾çš„å¯¹é½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMFM-Mapperçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŒè§†è§‰ç¼–ç å™¨ï¼šä½¿ç”¨ä¸¤ä¸ªé¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰ä»è§†é¢‘ä¸­æå–è§†è§‰ç‰¹å¾ã€‚2) ç‰¹å¾èåˆæ¨¡å—ï¼šå°†ä¸¤ä¸ªè§†è§‰ç¼–ç å™¨æå–çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æ›´ä¸°å¯Œçš„è§†é¢‘è¡¨ç¤ºã€‚3) GPT-2æ˜ å°„å™¨ï¼šä½¿ç”¨GPT-2æ¨¡å‹ä½œä¸ºæ˜ å°„å™¨ï¼Œå°†èåˆåçš„è§†é¢‘ç‰¹å¾æ˜ å°„åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„è¾“å…¥ç©ºé—´ã€‚4) éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰ç”Ÿæˆæœ€ç»ˆçš„éŸ³é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šMFM-Mapperçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) èåˆäº†æ¥è‡ªå¤šä¸ªåŸºç¡€æ¨¡å‹çš„ç‰¹å¾ï¼Œä»è€Œè·å¾—æ›´ä¸°å¯Œçš„è§†é¢‘è¯­ä¹‰å’Œæ—¶é—´ä¿¡æ¯ã€‚2) ä½¿ç”¨GPT-2ä½œä¸ºæ˜ å°„å™¨ï¼Œå°†è·¨æ¨¡æ€ç‰¹å¾æ˜ å°„é—®é¢˜è½¬åŒ–ä¸ºè‡ªå›å½’ç¿»è¯‘é—®é¢˜ï¼Œå¢å¼ºäº†ç‰¹å¾å¯¹é½èƒ½åŠ›ã€‚3) å®ç°äº†æé«˜çš„è®­ç»ƒæ•ˆç‡ï¼Œä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼š1) å¦‚ä½•é€‰æ‹©å’Œé…ç½®åŒè§†è§‰ç¼–ç å™¨ï¼Œä»¥æå–äº’è¡¥çš„è§†é¢‘ç‰¹å¾ï¼ˆæœªçŸ¥ï¼‰ã€‚2) ç‰¹å¾èåˆæ¨¡å—çš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–ç®€å•çš„æ‹¼æ¥ï¼ˆæœªçŸ¥ï¼‰ã€‚3) GPT-2æ˜ å°„å™¨çš„è®­ç»ƒæ–¹å¼å’Œå‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚ä½¿ç”¨ä½•ç§æŸå¤±å‡½æ•°è¿›è¡Œå¾®è°ƒï¼ˆæœªçŸ¥ï¼‰ã€‚4) éŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„é€‰æ‹©å’Œé…ç½®ï¼Œä»¥åŠå¦‚ä½•å°†æ˜ å°„åçš„è§†é¢‘ç‰¹å¾è¾“å…¥åˆ°è¯¥æ¨¡å‹ä¸­ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MFM-Mapperåœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šçš„è®­ç»ƒæ•ˆç‡ï¼Œä»…ä½¿ç”¨å…ˆå‰åŸºäºæ˜ å°„å™¨çš„å·¥ä½œçš„16%çš„è®­ç»ƒè§„æ¨¡ï¼Œå³å¯åœ¨è¯­ä¹‰å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶è¾¾åˆ°ä¸åœ¨æ›´å¤§è§„æ¨¡ä¸Šè®­ç»ƒçš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å…·ä½“æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æ¯”åŸºçº¿æœªåœ¨æ‘˜è¦ä¸­æ˜ç¡®ç»™å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MFM-Mapperåœ¨è§†é¢‘å†…å®¹åˆ›ä½œã€ç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥è‡ªåŠ¨ç”Ÿæˆä¸è§†é¢‘å†…å®¹åŒ¹é…çš„èƒŒæ™¯éŸ³ä¹ã€éŸ³æ•ˆç­‰ï¼Œæé«˜å†…å®¹åˆ›ä½œæ•ˆç‡ï¼Œé™ä½åˆ¶ä½œæˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºè¾…åŠ©å¬è§‰éšœç¢äººå£«ç†è§£è§†é¢‘å†…å®¹ï¼Œæå‡ä»–ä»¬çš„ç”Ÿæ´»è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Video-to-Audio (V2A) generation relies on extracting semantic and temporal features from video to condition generative models. Training these models from scratch is resource intensive. Consequently, leveraging foundation models (FMs) has gained traction due to their cross-modal knowledge transfer and generalization capabilities. One prior work has explored fine-tuning a lightweight mapper network to connect a pre-trained visual encoder with a text-to-audio generation model for V2A. Inspired by this, we introduce the Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper approach, MFM-Mapper benefits from richer semantic and temporal information by fusing features from dual visual encoders. Furthermore, by replacing a linear mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels between cross-modal features mapping and autoregressive translation tasks. Our MFM-Mapper exhibits remarkable training efficiency. It achieves better performance in semantic and temporal consistency with fewer training consuming, requiring only 16\% of the training scale compared to previous mapper-based work, yet achieves competitive performance with models trained on a much larger scale.

