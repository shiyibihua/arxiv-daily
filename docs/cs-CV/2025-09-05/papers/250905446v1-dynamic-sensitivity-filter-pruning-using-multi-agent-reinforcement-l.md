---
layout: default
title: Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's
---

# Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05446v1</a>
  <a href="https://arxiv.org/pdf/2509.05446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05446v1', 'Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN\'s')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Iftekhar Haider Chowdhury, Zaed Ikbal Syed, Ahmed Faizul Haque Dhrubo, Mohammad Abdul Qayum

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: This paper includes figures and two tables, and our work outperforms the existing research that has been published in a journal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªæç®—æ³•ï¼Œç”¨äºé«˜æ•ˆå‹ç¼©æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹å‰ªæ` `æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ` `æ¨¡å‹å‹ç¼©` `æ»¤æ³¢å™¨é€‰æ‹©` `æ•æ„Ÿåº¦åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦å·ç§¯ç½‘ç»œè®¡ç®—å’Œå†…å­˜å¼€é”€å¤§ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚
2. æå‡ºå·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªæï¼Œé€šè¿‡èåˆå¤šç§æ•æ„Ÿåº¦æŒ‡æ ‡æ¥è¯„ä¼°æ»¤æ³¢å™¨çš„é‡è¦æ€§ï¼Œå¹¶è¿›è¡Œå•æ¬¡å‰ªæã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¿æŒäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œåœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å…¶è®¡ç®—å’Œå†…å­˜å¼€é”€é™åˆ¶äº†å®é™…éƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å•æ¬¡æ»¤æ³¢å™¨å‰ªææ¡†æ¶â€”â€”å·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªæï¼ˆDifferential Sensitivity Fusion Pruningï¼‰ã€‚è¯¥æ¡†æ¶ä¾§é‡äºè¯„ä¼°æ»¤æ³¢å™¨é‡è¦æ€§è¯„åˆ†åœ¨å¤šä¸ªæ ‡å‡†ä¸‹çš„ç¨³å®šæ€§å’Œå†—ä½™æ€§ã€‚å·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªæé€šè¿‡èåˆåŸºäºæ¢¯åº¦çš„æ•æ„Ÿåº¦ã€ä¸€é˜¶æ³°å‹’å±•å¼€å’Œæ¿€æ´»åˆ†å¸ƒçš„KLæ•£åº¦ä¹‹é—´çš„å·®å¼‚ï¼Œä¸ºæ¯ä¸ªæ»¤æ³¢å™¨è®¡ç®—å·®åˆ†æ•æ„Ÿåº¦è¯„åˆ†ã€‚é‡‡ç”¨æŒ‡æ•°ç¼©æ”¾æœºåˆ¶æ¥å¼ºè°ƒåœ¨ä¸åŒæŒ‡æ ‡ä¸‹é‡è¦æ€§ä¸ä¸€è‡´çš„æ»¤æ³¢å™¨ï¼Œä»è€Œè¯†åˆ«ç»“æ„ä¸ç¨³å®šæˆ–å¯¹æ¨¡å‹æ€§èƒ½ä¸å¤ªå…³é”®çš„å€™é€‰æ»¤æ³¢å™¨ã€‚ä¸è¿­ä»£æˆ–åŸºäºå¼ºåŒ–å­¦ä¹ çš„å‰ªæç­–ç•¥ä¸åŒï¼Œå·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªææ˜¯é«˜æ•ˆä¸”ç¡®å®šæ€§çš„ï¼Œåªéœ€è¦ä¸€æ¬¡å‰å‘-åå‘ä¼ é€’å³å¯è¿›è¡Œè¯„åˆ†å’Œå‰ªæã€‚åœ¨50%åˆ°70%çš„ä¸åŒå‰ªæç‡ä¸‹è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªææ˜¾è‘—é™ä½äº†æ¨¡å‹å¤æ‚åº¦ï¼Œå®ç°äº†è¶…è¿‡80%çš„æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPSï¼‰çš„å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨70%çš„å‰ªæç‡ä¸‹ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†é«˜è¾¾98.23%çš„åŸºçº¿å‡†ç¡®ç‡ï¼Œåœ¨å‹ç¼©å’Œæ³›åŒ–æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºå¯æ‰©å±•å’Œè‡ªé€‚åº”çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œå‹ç¼©æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºåœ¨è¾¹ç¼˜å’Œç§»åŠ¨å¹³å°ä¸Šé«˜æ•ˆéƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDCNNï¼‰æ¨¡å‹è¿‡å¤§ï¼Œè®¡ç®—å’Œå†…å­˜å¼€é”€é«˜çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨å¹³å°ä¸Šçš„éƒ¨ç½²ã€‚ç°æœ‰å‰ªææ–¹æ³•ï¼Œå¦‚è¿­ä»£å‰ªææˆ–åŸºäºå¼ºåŒ–å­¦ä¹ çš„å‰ªæï¼Œé€šå¸¸è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œæˆ–è€…éœ€è¦å¤šæ¬¡è¿­ä»£æ‰èƒ½è¾¾åˆ°ç†æƒ³çš„å‹ç¼©æ•ˆæœã€‚ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•åœ¨å‹ç¼©ç‡è¾ƒé«˜æ—¶ï¼Œå®¹æ˜“å¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡èåˆå¤šç§æ»¤æ³¢å™¨æ•æ„Ÿåº¦æŒ‡æ ‡ï¼Œç»¼åˆè¯„ä¼°æ¯ä¸ªæ»¤æ³¢å™¨çš„é‡è¦æ€§ï¼Œå¹¶è¿›è¡Œå•æ¬¡å‰ªæã€‚é€šè¿‡å…³æ³¨æ»¤æ³¢å™¨åœ¨ä¸åŒæŒ‡æ ‡ä¸‹çš„ç¨³å®šæ€§ï¼Œè¯†åˆ«å‡ºå†—ä½™æˆ–ä¸é‡è¦çš„æ»¤æ³¢å™¨ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„æ¨¡å‹å‹ç¼©ï¼Œé¿å…äº†è¿­ä»£å‰ªæçš„è®¡ç®—å¼€é”€å’Œæ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) è®¡ç®—åŸºäºæ¢¯åº¦çš„æ•æ„Ÿåº¦ã€ä¸€é˜¶æ³°å‹’å±•å¼€å’Œæ¿€æ´»åˆ†å¸ƒçš„KLæ•£åº¦ç­‰å¤šç§æ»¤æ³¢å™¨æ•æ„Ÿåº¦æŒ‡æ ‡ã€‚2) è®¡ç®—è¿™äº›æŒ‡æ ‡ä¹‹é—´çš„å·®å¼‚ï¼Œå¾—åˆ°æ¯ä¸ªæ»¤æ³¢å™¨çš„å·®åˆ†æ•æ„Ÿåº¦è¯„åˆ†ã€‚3) åº”ç”¨æŒ‡æ•°ç¼©æ”¾æœºåˆ¶ï¼Œæ”¾å¤§ä¸ç¨³å®šçš„æ»¤æ³¢å™¨çš„è¯„åˆ†ã€‚4) æ ¹æ®è¯„åˆ†è¿›è¡Œå•æ¬¡å‰ªæï¼Œç§»é™¤è¯„åˆ†è¾ƒä½çš„æ»¤æ³¢å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å·®åˆ†æ•æ„Ÿåº¦èåˆçš„æ¦‚å¿µï¼Œé€šè¿‡èåˆå¤šç§æ•æ„Ÿåº¦æŒ‡æ ‡çš„å·®å¼‚æ¥è¯„ä¼°æ»¤æ³¢å™¨çš„é‡è¦æ€§ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€æŒ‡æ ‡æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ»¤æ³¢å™¨çš„è´¡çŒ®ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯†åˆ«å†—ä½™æ»¤æ³¢å™¨ã€‚æ­¤å¤–ï¼Œå•æ¬¡å‰ªæçš„è®¾è®¡é¿å…äº†è¿­ä»£å‰ªæçš„è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†åŸºäºæ¢¯åº¦çš„æ•æ„Ÿåº¦ã€ä¸€é˜¶æ³°å‹’å±•å¼€å’Œæ¿€æ´»åˆ†å¸ƒçš„KLæ•£åº¦ä½œä¸ºæ•æ„Ÿåº¦æŒ‡æ ‡ã€‚2) ä½¿ç”¨æŒ‡æ•°ç¼©æ”¾æœºåˆ¶æ¥å¼ºè°ƒä¸ç¨³å®šçš„æ»¤æ³¢å™¨ï¼Œå…·ä½“ç¼©æ”¾å› å­æœªçŸ¥ã€‚3) å‰ªæç‡çš„é€‰æ‹©ï¼Œè®ºæ–‡åœ¨50%åˆ°70%çš„ä¸åŒå‰ªæç‡ä¸‹è¿›è¡Œäº†å®éªŒï¼Œå…·ä½“å¦‚ä½•é€‰æ‹©æœ€ä½³å‰ªæç‡æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨50%åˆ°70%çš„å‰ªæç‡ä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œå®ç°è¶…è¿‡80%çš„FLOPSå‡å°‘ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨70%çš„å‰ªæç‡ä¸‹ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†é«˜è¾¾98.23%çš„åŸºçº¿å‡†ç¡®ç‡ï¼Œä¼˜äºä¼ ç»Ÿçš„å¯å‘å¼å‰ªææ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹å‹ç¼©å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å›¾åƒè¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡æ£€æµ‹ã€ä»¥åŠç‰©è”ç½‘è®¾å¤‡ä¸Šçš„æ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡é™ä½æ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šè¿è¡Œï¼Œä»è€Œæ‹“å±•äº†æ·±åº¦å­¦ä¹ çš„åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep Convolutional Neural Networks have achieved state of the art performance across various computer vision tasks, however their practical deployment is limited by computational and memory overhead. This paper introduces Differential Sensitivity Fusion Pruning, a novel single shot filter pruning framework that focuses on evaluating the stability and redundancy of filter importance scores across multiple criteria. Differential Sensitivity Fusion Pruning computes a differential sensitivity score for each filter by fusing the discrepancies among gradient based sensitivity, first order Taylor expansion, and KL divergence of activation distributions. An exponential scaling mechanism is applied to emphasize filters with inconsistent importance across metrics, identifying candidates that are structurally unstable or less critical to the model performance. Unlike iterative or reinforcement learning based pruning strategies, Differential Sensitivity Fusion Pruning is efficient and deterministic, requiring only a single forward-backward pass for scoring and pruning. Extensive experiments across varying pruning rates between 50 to 70 percent demonstrate that Differential Sensitivity Fusion Pruning significantly reduces model complexity, achieving over 80 percent Floating point Operations Per Seconds reduction while maintaining high accuracy. For instance, at 70 percent pruning, our approach retains up to 98.23 percent of baseline accuracy, surpassing traditional heuristics in both compression and generalization. The proposed method presents an effective solution for scalable and adaptive Deep Convolutional Neural Networks compression, paving the way for efficient deployment on edge and mobile platforms.

