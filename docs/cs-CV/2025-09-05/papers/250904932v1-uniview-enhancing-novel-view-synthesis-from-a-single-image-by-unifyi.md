---
layout: default
title: UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features
---

# UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04932" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04932v1</a>
  <a href="https://arxiv.org/pdf/2509.04932.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04932v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04932v1', 'UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haowang Cui, Rui Chen, Tao Luo, Rui Li, Jiaze Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

**å¤‡æ³¨**: Submitted to ACM TOMM

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniViewï¼šé€šè¿‡ç»Ÿä¸€å‚è€ƒç‰¹å¾å¢å¼ºå•å›¾åƒçš„æ–°è§†è§’åˆæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–°è§†è§’åˆæˆ` `å•å›¾åƒé‡å»º` `å‚è€ƒå›¾åƒ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å•å›¾æ–°è§†è§’åˆæˆé¢ä¸´æœªè§‚æµ‹åŒºåŸŸä¿¡æ¯ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æ˜“äº§ç”Ÿå¤±çœŸã€‚
2. UniViewåˆ©ç”¨ç›¸ä¼¼å¯¹è±¡çš„å‚è€ƒå›¾åƒä½œä¸ºå…ˆéªŒï¼Œè¾…åŠ©ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒUniViewåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ–°è§†è§’åˆæˆçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å•å›¾åƒæ–°è§†è§’åˆæˆä»»åŠ¡å…·æœ‰é«˜åº¦ä¸é€‚å®šæ€§ï¼Œå› ä¸ºæœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸå­˜åœ¨å¤šç§å¯èƒ½çš„è§£é‡Šã€‚ç°æœ‰æ–¹æ³•å€¾å‘äºä»æ¨¡ç³Šå…ˆéªŒå’Œè¾“å…¥è§†è§’çš„æ’å€¼æ¥ç”Ÿæˆæœªè§åŒºåŸŸï¼Œè¿™é€šå¸¸å¯¼è‡´ä¸¥é‡çš„å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºUniViewçš„æ–°æ¨¡å‹ï¼Œå®ƒå¯ä»¥åˆ©ç”¨æ¥è‡ªç›¸ä¼¼å¯¹è±¡çš„å‚è€ƒå›¾åƒï¼Œä¸ºè§†è§’åˆæˆæä¾›å¼ºå¤§çš„å…ˆéªŒä¿¡æ¯ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ£€ç´¢å’Œå¢å¼ºç³»ç»Ÿï¼Œå¹¶é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è¾…åŠ©é€‰æ‹©æ»¡è¶³æˆ‘ä»¬è¦æ±‚çš„å‚è€ƒå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¸¦æœ‰å¤šçº§éš”ç¦»å±‚çš„å³æ’å³ç”¨é€‚é…å™¨æ¨¡å—ï¼Œä»¥åŠ¨æ€ç”Ÿæˆç›®æ ‡è§†è§’çš„å‚è€ƒç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¿ç•™åŸå§‹è¾“å…¥å›¾åƒçš„ç»†èŠ‚ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè§£è€¦çš„ä¸‰é‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¯¹é½å’Œæ•´åˆå¤šåˆ†æ”¯ç‰¹å¾åˆ°åˆæˆè¿‡ç¨‹ä¸­ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„UniViewæ˜¾è‘—æé«˜äº†æ–°è§†è§’åˆæˆæ€§èƒ½ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå•å›¾åƒæ–°è§†è§’åˆæˆæ—¨åœ¨ä»å•ä¸ªå›¾åƒç”ŸæˆåŒä¸€åœºæ™¯æˆ–å¯¹è±¡åœ¨ä¸åŒè§†è§’ä¸‹çš„å›¾åƒã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä»è¾“å…¥å›¾åƒé™„è¿‘çš„è§†è§’è¿›è¡Œæ’å€¼æˆ–åˆ©ç”¨æ¨¡ç³Šå…ˆéªŒæ¥æ¨æ–­æœªè§åŒºåŸŸï¼Œè¿™å¾€å¾€å¯¼è‡´åˆæˆå›¾åƒå‡ºç°ä¸¥é‡çš„å¤±çœŸå’Œç»†èŠ‚ä¸¢å¤±ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨ä¿¡æ¯æ¥æŒ‡å¯¼æ–°è§†è§’çš„ç”Ÿæˆæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniViewçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¥è‡ªç›¸ä¼¼å¯¹è±¡çš„å‚è€ƒå›¾åƒä½œä¸ºå¼ºå…ˆéªŒä¿¡æ¯ï¼Œè¾…åŠ©æ–°è§†è§’çš„åˆæˆã€‚é€šè¿‡æ£€ç´¢ä¸è¾“å…¥å›¾åƒç›¸ä¼¼çš„å‚è€ƒå›¾åƒï¼Œå¹¶ä»ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼¥è¡¥å•å›¾åƒæ–°è§†è§’åˆæˆä¸­ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚è¿™ç§æ–¹æ³•é¿å…äº†è¿‡åº¦ä¾èµ–æ¨¡ç³Šå…ˆéªŒå’Œæ’å€¼ï¼Œä»è€Œæé«˜äº†åˆæˆå›¾åƒçš„è´¨é‡å’ŒçœŸå®æ„Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniViewçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å‚è€ƒå›¾åƒæ£€ç´¢å’Œå¢å¼ºç³»ç»Ÿï¼šä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è¾…åŠ©é€‰æ‹©ä¸è¾“å…¥å›¾åƒç›¸ä¼¼çš„å‚è€ƒå›¾åƒã€‚2) å³æ’å³ç”¨é€‚é…å™¨æ¨¡å—ï¼šè¯¥æ¨¡å—åŒ…å«å¤šçº§éš”ç¦»å±‚ï¼Œç”¨äºåŠ¨æ€ç”Ÿæˆç›®æ ‡è§†è§’çš„å‚è€ƒç‰¹å¾ã€‚3) è§£è€¦ä¸‰é‡æ³¨æ„åŠ›æœºåˆ¶ï¼šç”¨äºå¯¹é½å’Œæ•´åˆæ¥è‡ªä¸åŒåˆ†æ”¯çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬è¾“å…¥å›¾åƒç‰¹å¾å’Œå‚è€ƒç‰¹å¾ï¼Œä»è€Œä¿ç•™åŸå§‹è¾“å…¥å›¾åƒçš„ç»†èŠ‚ã€‚æ•´ä¸ªæµç¨‹é¦–å…ˆæ£€ç´¢åˆé€‚çš„å‚è€ƒå›¾åƒï¼Œç„¶åæå–å‚è€ƒç‰¹å¾å¹¶å°†å…¶ä¸è¾“å…¥å›¾åƒç‰¹å¾èåˆï¼Œæœ€åç”Ÿæˆç›®æ ‡è§†è§’å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šUniViewçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) å¼•å…¥å‚è€ƒå›¾åƒä½œä¸ºå¼ºå…ˆéªŒä¿¡æ¯ï¼Œå…‹æœäº†å•å›¾åƒæ–°è§†è§’åˆæˆçš„ä¿¡æ¯ä¸è¶³é—®é¢˜ã€‚2) è®¾è®¡äº†å³æ’å³ç”¨é€‚é…å™¨æ¨¡å—ï¼Œå¯ä»¥åŠ¨æ€ç”Ÿæˆç›®æ ‡è§†è§’çš„å‚è€ƒç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚3) æå‡ºäº†è§£è€¦ä¸‰é‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°å¯¹é½å’Œæ•´åˆäº†å¤šåˆ†æ”¯ç‰¹å¾ï¼Œä¿ç•™äº†åŸå§‹è¾“å…¥å›¾åƒçš„ç»†èŠ‚ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒUniViewèƒ½å¤Ÿç”Ÿæˆæ›´çœŸå®ã€æ›´æ¸…æ™°çš„æ–°è§†è§’å›¾åƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚è€ƒå›¾åƒæ£€ç´¢æ–¹é¢ï¼Œä½¿ç”¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è¯„ä¼°å›¾åƒçš„ç›¸ä¼¼æ€§ã€‚é€‚é…å™¨æ¨¡å—é‡‡ç”¨äº†å¤šçº§éš”ç¦»å±‚ï¼Œä»¥é˜²æ­¢å‚è€ƒç‰¹å¾å¯¹è¾“å…¥å›¾åƒç‰¹å¾äº§ç”Ÿè¿‡åº¦å¹²æ‰°ã€‚è§£è€¦ä¸‰é‡æ³¨æ„åŠ›æœºåˆ¶å°†æ³¨æ„åŠ›æ“ä½œåˆ†è§£ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„æ­¥éª¤ï¼Œåˆ†åˆ«å…³æ³¨é€šé“ã€ç©ºé—´å’Œç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯èƒ½é‡‡ç”¨äº†L1æŸå¤±ã€æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±ç­‰ï¼Œä»¥æé«˜åˆæˆå›¾åƒçš„è´¨é‡å’ŒçœŸå®æ„Ÿã€‚ï¼ˆå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚æœªçŸ¥ï¼‰

## ğŸ“Š å®éªŒäº®ç‚¹

UniViewåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æåˆ°â€œæ˜¾è‘—æé«˜äº†æ–°è§†è§’åˆæˆæ€§èƒ½â€ï¼Œè¡¨æ˜UniViewåœ¨ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€çœŸå®æ„Ÿå’Œç»†èŠ‚ä¿ç•™æ–¹é¢éƒ½æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniViewåœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºä»å•å¼ ç…§ç‰‡ç”Ÿæˆ3Dæ¨¡å‹ï¼Œæˆ–è€…åœ¨æ¸¸æˆä¸­åˆ›å»ºé€¼çœŸçš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºåŒ»å­¦å›¾åƒå¤„ç†ã€é¥æ„Ÿå›¾åƒåˆ†æç­‰é¢†åŸŸï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The task of synthesizing novel views from a single image is highly ill-posed due to multiple explanations for unobserved areas. Most current methods tend to generate unseen regions from ambiguity priors and interpolation near input views, which often lead to severe distortions. To address this limitation, we propose a novel model dubbed as UniView, which can leverage reference images from a similar object to provide strong prior information during view synthesis. More specifically, we construct a retrieval and augmentation system and employ a multimodal large language model (MLLM) to assist in selecting reference images that meet our requirements. Additionally, a plug-and-play adapter module with multi-level isolation layers is introduced to dynamically generate reference features for the target views. Moreover, in order to preserve the details of an original input image, we design a decoupled triple attention mechanism, which can effectively align and integrate multi-branch features into the synthesis process. Extensive experiments have demonstrated that our UniView significantly improves novel view synthesis performance and outperforms state-of-the-art methods on the challenging datasets.

