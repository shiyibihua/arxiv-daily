---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-05
---

# cs.CVï¼ˆ2025-09-05ï¼‰

ğŸ“Š å…± **20** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250905515v1-visibility-aware-language-aggregation-for-open-vocabulary-segmentati.html">Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting</a></td>
  <td>æå‡ºVALAï¼Œè§£å†³3Dé«˜æ–¯æº…å°„å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­èƒŒæ™¯å™ªå£°å’Œå¤šè§†è§’ä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05515v1" data-paper-url="./papers/250905515v1-visibility-aware-language-aggregation-for-open-vocabulary-segmentati.html" onclick="toggleFavorite(this, '2509.05515v1', 'Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250905297v1-flowseek-optical-flow-made-easier-with-depth-foundation-models-and-m.html">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a></td>
  <td>FlowSeekï¼šåˆ©ç”¨æ·±åº¦åŸºç¡€æ¨¡å‹å’Œè¿åŠ¨åŸºçš„å…‰æµä¼°è®¡æ¡†æ¶ï¼Œé™ä½è®­ç»ƒæˆæœ¬å¹¶æå‡æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05297v1" data-paper-url="./papers/250905297v1-flowseek-optical-flow-made-easier-with-depth-foundation-models-and-m.html" onclick="toggleFavorite(this, '2509.05297v1', 'FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250904859v2-core-gs-coarse-to-refined-gaussian-splatting-with-semantic-object-fo.html">CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus</a></td>
  <td>CoRe-GSï¼šé¢å‘è¯­ä¹‰å…´è¶£ç‚¹çš„ç²—åˆ°ç²¾é«˜æ–¯æº…å°„ï¼ŒåŠ é€Ÿç§»åŠ¨é‡å»ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04859v2" data-paper-url="./papers/250904859v2-core-gs-coarse-to-refined-gaussian-splatting-with-semantic-object-fo.html" onclick="toggleFavorite(this, '2509.04859v2', 'CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250905075v3-geosplat-a-deep-dive-into-geometry-constrained-gaussian-splatting.html">GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting</a></td>
  <td>GeoSplatï¼šæå‡ºå‡ ä½•çº¦æŸé«˜æ–¯æº…å°„æ¡†æ¶ï¼Œæå‡æ–°è§†è§’åˆæˆæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05075v3" data-paper-url="./papers/250905075v3-geosplat-a-deep-dive-into-geometry-constrained-gaussian-splatting.html" onclick="toggleFavorite(this, '2509.05075v3', 'GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250904772v1-floodvision-urban-flood-depth-estimation-using-foundation-vision-lan.html">FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph</a></td>
  <td>FloodVisionï¼šç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ä¸é¢†åŸŸçŸ¥è¯†å›¾è°±çš„åŸå¸‚æ´ªæ°´æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04772v1" data-paper-url="./papers/250904772v1-floodvision-urban-flood-depth-estimation-using-foundation-vision-lan.html" onclick="toggleFavorite(this, '2509.04772v1', 'FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250905144v1-sgs-3d-high-fidelity-3d-instance-segmentation-via-reliable-semantic-.html">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a></td>
  <td>SGS-3Dï¼šé€šè¿‡å¯é è¯­ä¹‰æ©ç åˆ†å‰²ä¸ç”Ÿé•¿å®ç°é«˜ä¿çœŸ3Då®ä¾‹åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05144v1" data-paper-url="./papers/250905144v1-sgs-3d-high-fidelity-3d-instance-segmentation-via-reliable-semantic-.html" onclick="toggleFavorite(this, '2509.05144v1', 'SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250905012v1-a-biologically-inspired-separable-learning-vision-model-for-real-tim.html">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a></td>
  <td>æå‡ºä¸€ç§ç”Ÿç‰©å¯å‘å¼å¯åˆ†ç¦»å­¦ä¹ è§†è§‰æ¨¡å‹ï¼Œç”¨äºé»‘æš—ç¯å¢ƒä¸‹çš„å®æ—¶äº¤é€šç›®æ ‡æ„ŸçŸ¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05012v1" data-paper-url="./papers/250905012v1-a-biologically-inspired-separable-learning-vision-model-for-real-tim.html" onclick="toggleFavorite(this, '2509.05012v1', 'A biologically inspired separable learning vision model for real-time traffic object perception in Dark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250905208v1-symbolic-graphics-programming-with-large-language-models.html">Symbolic Graphics Programming with Large Language Models</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç²¾ç¡®å¯æ§SVGå›¾åƒçš„èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05208v1" data-paper-url="./papers/250905208v1-symbolic-graphics-programming-with-large-language-models.html" onclick="toggleFavorite(this, '2509.05208v1', 'Symbolic Graphics Programming with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250904833v1-propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granul.html">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a></td>
  <td>PropVGï¼šæå‡ºç«¯åˆ°ç«¯çš„åŸºäºæè®®çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œæå‡å¤æ‚åœºæ™¯ä¸‹çš„ç›®æ ‡è¯†åˆ«èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">visual grounding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04833v1" data-paper-url="./papers/250904833v1-propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granul.html" onclick="toggleFavorite(this, '2509.04833v1', 'PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250905543v1-duoclr-dual-surrogate-contrastive-learning-for-skeleton-based-human-.html">DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation</a></td>
  <td>DuoCLRï¼šé€šè¿‡åŒä»£ç†å¯¹æ¯”å­¦ä¹ å¢å¼ºéª¨éª¼åŠ¨ä½œåˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05543v1" data-paper-url="./papers/250905543v1-duoclr-dual-surrogate-contrastive-learning-for-skeleton-based-human-.html" onclick="toggleFavorite(this, '2509.05543v1', 'DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250905188v1-sl-slr-self-supervised-representation-learning-for-sign-language-rec.html">SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition</a></td>
  <td>æå‡ºSL-SLRæ¡†æ¶ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æå‡æ‰‹è¯­è¯†åˆ«çš„è¡¨å¾èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05188v1" data-paper-url="./papers/250905188v1-sl-slr-self-supervised-representation-learning-for-sign-language-rec.html" onclick="toggleFavorite(this, '2509.05188v1', 'SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250905446v1-dynamic-sensitivity-filter-pruning-using-multi-agent-reinforcement-l.html">Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's</a></td>
  <td>æå‡ºå·®åˆ†æ•æ„Ÿåº¦èåˆå‰ªæç®—æ³•ï¼Œç”¨äºé«˜æ•ˆå‹ç¼©æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05446v1" data-paper-url="./papers/250905446v1-dynamic-sensitivity-filter-pruning-using-multi-agent-reinforcement-l.html" onclick="toggleFavorite(this, '2509.05446v1', 'Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN&#39;s')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250904957v1-efficient-video-to-audio-generation-via-multiple-foundation-models-m.html">Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</a></td>
  <td>æå‡ºå¤šåŸºç¡€æ¨¡å‹æ˜ å°„å™¨(MFM-Mapper)ï¼Œé«˜æ•ˆç”Ÿæˆä¸è§†é¢‘å†…å®¹åŒ¹é…çš„éŸ³é¢‘ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04957v1" data-paper-url="./papers/250904957v1-efficient-video-to-audio-generation-via-multiple-foundation-models-m.html" onclick="toggleFavorite(this, '2509.04957v1', 'Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250904932v1-uniview-enhancing-novel-view-synthesis-from-a-single-image-by-unifyi.html">UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features</a></td>
  <td>UniViewï¼šé€šè¿‡ç»Ÿä¸€å‚è€ƒç‰¹å¾å¢å¼ºå•å›¾åƒçš„æ–°è§†è§’åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04932v1" data-paper-url="./papers/250904932v1-uniview-enhancing-novel-view-synthesis-from-a-single-image-by-unifyi.html" onclick="toggleFavorite(this, '2509.04932v1', 'UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250904757v1-mcanet-a-multi-scale-class-specific-attention-network-for-multi-labe.html">MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery</a></td>
  <td>æå‡ºMCANetï¼Œåˆ©ç”¨å¤šå°ºåº¦ç±»ç‰¹å®šæ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œæ— äººæœºå›¾åƒçš„é£“é£ç¾åå¤šæ ‡ç­¾è¯„ä¼°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04757v1" data-paper-url="./papers/250904757v1-mcanet-a-multi-scale-class-specific-attention-network-for-multi-labe.html" onclick="toggleFavorite(this, '2509.04757v1', 'MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250904736v1-watchhar-real-time-on-device-human-activity-recognition-system-for-s.html">WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches</a></td>
  <td>WatchHARï¼šé¢å‘æ™ºèƒ½æ‰‹è¡¨çš„å®æ—¶ã€ç«¯ä¾§äººä½“æ´»åŠ¨è¯†åˆ«ç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04736v1" data-paper-url="./papers/250904736v1-watchhar-real-time-on-device-human-activity-recognition-system-for-s.html" onclick="toggleFavorite(this, '2509.04736v1', 'WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250905513v1-openego-a-large-scale-multimodal-egocentric-dataset-for-dexterous-ma.html">OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation</a></td>
  <td>OpenEgoï¼šç”¨äºçµå·§æ“ä½œçš„å¤§è§„æ¨¡å¤šæ¨¡æ€ç¬¬ä¸€äººç§°æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05513v1" data-paper-url="./papers/250905513v1-openego-a-large-scale-multimodal-egocentric-dataset-for-dexterous-ma.html" onclick="toggleFavorite(this, '2509.05513v1', 'OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250905030v1-luiviton-learned-universal-interoperable-virtual-try-on.html">LUIVITON: Learned Universal Interoperable VIrtual Try-ON</a></td>
  <td>LUIVITONï¼šå­¦ä¹ çš„é€šç”¨äº’æ“ä½œè™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿï¼Œé€‚ç”¨äºå¤æ‚æœè£…å’Œå¤šæ ·åŒ–äººä½“</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">SMPL</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05030v1" data-paper-url="./papers/250905030v1-luiviton-learned-universal-interoperable-virtual-try-on.html" onclick="toggleFavorite(this, '2509.05030v1', 'LUIVITON: Learned Universal Interoperable VIrtual Try-ON')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250905078v1-scale-interaction-transformer-a-hybrid-cnn-transformer-model-for-fac.html">Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction</a></td>
  <td>æå‡ºScale-Interaction Transformer (SIT)æ¨¡å‹ï¼Œç”¨äºæå‡é¢éƒ¨ç¾å­¦é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">interaction transformer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05078v1" data-paper-url="./papers/250905078v1-scale-interaction-transformer-a-hybrid-cnn-transformer-model-for-fac.html" onclick="toggleFavorite(this, '2509.05078v1', 'Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250904775v1-comparative-evaluation-of-traditional-and-deep-learning-feature-matc.html">Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data</a></td>
  <td>åˆ©ç”¨å«¦å¨¥äºŒå·æœˆçƒæ•°æ®ï¼Œå¯¹æ¯”ä¼ ç»Ÿä¸æ·±åº¦å­¦ä¹ ç‰¹å¾åŒ¹é…ç®—æ³•ï¼Œå®ç°ç²¾å‡†å›¾åƒé…å‡†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04775v1" data-paper-url="./papers/250904775v1-comparative-evaluation-of-traditional-and-deep-learning-feature-matc.html" onclick="toggleFavorite(this, '2509.04775v1', 'Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)