---
layout: default
title: StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios
---

# StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios

**arXiv**: [2512.04451v1](https://arxiv.org/abs/2512.04451) | [PDF](https://arxiv.org/pdf/2512.04451.pdf)

**ä½œè€…**: Yifei Wang, Zhenkai Li, Tianwen Qian, Huanran Zheng, Zheng Wang, Yuqian Fu, Xiaoling Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStreamEQAåŸºå‡†ä»¥è¯„ä¼°å…·èº«åœºæ™¯ä¸‹çš„æµå¼è§†é¢‘ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `æµå¼è§†é¢‘ç†è§£` `å…·èº«æ™ºèƒ½` `è§†é¢‘é—®ç­”åŸºå‡†` `é•¿è§†é¢‘åˆ†æž` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…·èº«æ™ºèƒ½éœ€åœ¨æµå¼è§†é¢‘ä¸­æŒç»­æ„ŸçŸ¥ä¸ŽæŽ¨ç†ï¼ŒçŽ°æœ‰æ¨¡åž‹èƒ½åŠ›ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé¦–ä¸ªå…·èº«æµå¼è§†é¢‘é—®ç­”åŸºå‡†ï¼Œæ¶µç›–æ„ŸçŸ¥ã€äº¤äº’ã€è§„åˆ’ä¸‰ä¸ªå±‚æ¬¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°13ä¸ªå…ˆè¿›æ¨¡åž‹ï¼Œå‘çŽ°å…¶åœ¨æµå¼è§†é¢‘ç†è§£ä¸Šä»é¢ä¸´æŒ‘æˆ˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.

