---
layout: default
title: Gradient Descent with Provably Tuned Learning-rate Schedules
---

# Gradient Descent with Provably Tuned Learning-rate Schedules

**arXiv**: [2512.05084v1](https://arxiv.org/abs/2512.05084) | [PDF](https://arxiv.org/pdf/2512.05084.pdf)

**ä½œè€…**: Dravyansh Sharma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è¯æ˜Žè°ƒä¼˜å­¦ä¹ çŽ‡è°ƒåº¦çš„æ–¹æ³•ï¼Œé€‚ç”¨äºŽéžå‡¸éžå…‰æ»‘å‡½æ•°ï¼Œæ‰©å±•è‡³ç¥žç»ç½‘ç»œä¼˜åŒ–ã€‚**

**å…³é”®è¯**: `æ¢¯åº¦ä¸‹é™` `è¶…å‚æ•°è°ƒä¼˜` `éžå‡¸ä¼˜åŒ–` `å­¦ä¹ çŽ‡è°ƒåº¦` `ç¥žç»ç½‘ç»œä¼˜åŒ–` `æ ·æœ¬å¤æ‚åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¢¯åº¦ä¼˜åŒ–ä¸­å­¦ä¹ çŽ‡ç­‰è¶…å‚æ•°é€šå¸¸ä¾èµ–å¯å‘å¼è°ƒä¼˜ï¼Œç¼ºä¹ç†è®ºä¿è¯ï¼Œä¸”çŽ°æœ‰ç†è®ºå‡è®¾è¿‡å¼ºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘æ–°åˆ†æžå·¥å…·ï¼Œå¯è¯æ˜Žè°ƒä¼˜è¶…å‚æ•°ï¼Œé€‚ç”¨äºŽéžå‡¸éžå…‰æ»‘å‡½æ•°ï¼ŒåŒ¹é…å…ˆå‰æ ·æœ¬å¤æ‚åº¦ç•Œé™ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåº”ç”¨äºŽç¥žç»ç½‘ç»œå¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œæ‰©å±•è‡³å¤šè¶…å‚æ•°è°ƒä¼˜ï¼ŒåŒ…æ‹¬å­¦ä¹ çŽ‡è°ƒåº¦å’ŒåŠ¨é‡åŒæ­¥è°ƒä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.

