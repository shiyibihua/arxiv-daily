---
layout: default
title: Learning Causality for Longitudinal Data
---

# Learning Causality for Longitudinal Data

**arXiv**: [2512.04980v1](https://arxiv.org/abs/2512.04980) | [PDF](https://arxiv.org/pdf/2512.04980.pdf)

**ä½œè€…**: Mouad EL Bouchattaoui

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCDVAEç­‰å› æžœæŽ¨æ–­æ–¹æ³•ï¼Œè§£å†³é«˜ç»´æ—¶åºæ•°æ®ä¸­çš„å› æžœè¡¨ç¤ºå­¦ä¹ é—®é¢˜**

**å…³é”®è¯**: `å› æžœæŽ¨æ–­` `æ—¶åºæ•°æ®` `å˜åˆ†è‡ªç¼–ç å™¨` `åäº‹å®žå›žå½’` `è¡¨ç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é’ˆå¯¹é«˜ç»´æ—¶åºæ•°æ®ï¼Œå¼€å‘å› æžœæŽ¨æ–­ä¸Žå› æžœè¡¨ç¤ºå­¦ä¹ æ–¹æ³•
2. æå‡ºCDVAEæ¨¡åž‹ä¼°è®¡ä¸ªä½“å¤„ç†æ•ˆåº”ï¼Œå¼•å…¥RNN-CPCæ¡†æž¶è¿›è¡Œé•¿æœŸåäº‹å®žå›žå½’
3. å®žéªŒæ˜¾ç¤ºCDVAEä¼˜äºŽåŸºçº¿ï¼ŒRNN-CPCæ¡†æž¶è¾¾åˆ°å…ˆè¿›æ°´å¹³

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.
>   The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.
>   The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.
>   The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.

