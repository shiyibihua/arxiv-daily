---
layout: default
title: Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models
---

# Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models

**arXiv**: [2512.04981v1](https://arxiv.org/abs/2512.04981) | [PDF](https://arxiv.org/pdf/2512.04981.pdf)

**ä½œè€…**: NaHyeon Park, Namin An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFairProæ¡†æž¶ä»¥å‡å°‘åŸºäºŽLVLMçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹ä¸­çš„ç¤¾ä¼šåè§ï¼Œæ­ç¤ºç³»ç»Ÿæç¤ºçš„å…³é”®ä½œç”¨ã€‚**

**å…³é”®è¯**: `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `ç¤¾ä¼šåè§` `ç³»ç»Ÿæç¤º` `å…¬å¹³æ€§æ¡†æž¶` `å›¾åƒåˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLVLMæ¨¡åž‹åœ¨å›¾åƒç”Ÿæˆä¸­æ”¾å¤§ç¤¾ä¼šåè§ï¼Œç³»ç»Ÿæç¤ºæ˜¯ä¸»è¦é©±åŠ¨å› ç´ ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§£ç ä¸­é—´è¡¨ç¤ºå’ŒåµŒå…¥å…³è”åˆ†æžï¼Œå¼€å‘FairProæ¡†æž¶å®žçŽ°è®­ç»ƒæ—¶è‡ªå®¡è®¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SANAå’ŒQwen-Imageæ¨¡åž‹ä¸Šï¼ŒFairProæ˜¾è‘—é™ä½Žåè§å¹¶ä¿æŒæ–‡æœ¬å›¾åƒå¯¹é½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.

