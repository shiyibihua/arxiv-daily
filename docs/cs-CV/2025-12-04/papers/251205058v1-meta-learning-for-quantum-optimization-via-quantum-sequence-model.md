---
layout: default
title: Meta-Learning for Quantum Optimization via Quantum Sequence Model
---

# Meta-Learning for Quantum Optimization via Quantum Sequence Model

**arXiv**: [2512.05058v1](https://arxiv.org/abs/2512.05058) | [PDF](https://arxiv.org/pdf/2512.05058.pdf)

**ä½œè€…**: Yu-Cheng Lin, Yu-Chao Hsu, Samuel Yen-Chi Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé‡å­å…ƒå­¦ä¹ æ¡†æž¶ï¼Œé€šè¿‡é‡å­åºåˆ—æ¨¡åž‹ä¼˜åŒ–QAOAå‚æ•°åˆå§‹åŒ–ä»¥æå‡æ€§èƒ½ã€‚**

**å…³é”®è¯**: `é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•` `å…ƒå­¦ä¹ ` `é‡å­åºåˆ—æ¨¡åž‹` `å‚æ•°åˆå§‹åŒ–` `é‡å­æ ¸` `NISQæ—¶ä»£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šQAOAå‚æ•°ä¼˜åŒ–å› éžå‡¸èƒ½é‡æ™¯è§‚å¯¼è‡´æ”¶æ•›æ…¢ã€è§£è´¨é‡å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®­ç»ƒé‡å­åºåˆ—æ¨¡åž‹å¦‚QK-LSTMä½œä¸ºå…ƒå­¦ä¹ ä¼˜åŒ–å™¨ï¼Œç”Ÿæˆé«˜æ•ˆå‚æ•°åˆå§‹åŒ–ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Max-Cuté—®é¢˜ä¸Šï¼ŒQK-LSTMå®žçŽ°æœ€é«˜è¿‘ä¼¼æ¯”å’Œæœ€å¿«æ”¶æ•›ï¼Œå‚æ•°å¯è¿ç§»æ€§å¼ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a "learning to learn" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.

