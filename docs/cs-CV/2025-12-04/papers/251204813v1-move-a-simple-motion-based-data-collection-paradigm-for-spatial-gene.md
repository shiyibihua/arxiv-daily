---
layout: default
title: MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation
---

# MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation

**arXiv**: [2512.04813v1](https://arxiv.org/abs/2512.04813) | [PDF](https://arxiv.org/pdf/2512.04813.pdf)

**ä½œè€…**: Huanqian Wang, Chi Bene Chen, Yang Yue, Danhua Tao, Tong Guo, Shaoxuan Xie, Denghang Huang, Shiji Song, Guocai Yao, Gao Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMOVEæ•°æ®æ”¶é›†èŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€æ¼”ç¤ºå¢žå¼ºç©ºé—´ä¿¡æ¯ä»¥æå‡æœºå™¨äººæ“ä½œçš„ç©ºé—´æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ¨¡ä»¿å­¦ä¹ ` `æ•°æ®æ”¶é›†` `ç©ºé—´æ³›åŒ–` `åŠ¨æ€æ¼”ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡ä»¿å­¦ä¹ å› æ•°æ®ç¨€ç¼ºå—é™ï¼Œé™æ€çŽ¯å¢ƒé…ç½®å¯¼è‡´ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æ¼”ç¤ºä¸­ä¸ºå¯ç§»åŠ¨ç‰©ä½“æ³¨å…¥è¿åŠ¨ï¼Œå•è½¨è¿¹å†…ç”Ÿæˆå¤šæ ·ç©ºé—´é…ç½®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žçŽ¯å¢ƒä¸­éªŒè¯ï¼ŒMOVEç›¸æ¯”é™æ€èŒƒå¼æå‡76.1%æˆåŠŸçŽ‡ï¼Œæ•°æ®æ•ˆçŽ‡æœ€é«˜è¾¾2-5å€å¢žç›Šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \textbf{MOtion-Based Variability Enhancement} (\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \emph{MOVE} achieves an average success rate of 39.1\%, a 76.1\% relative improvement over the static data collection paradigm (22.2\%), and yields up to 2--5$\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.

