---
layout: default
title: Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing
---

# Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.04862" target="_blank" class="toolbar-btn">arXiv: 2512.04862v1</a>
    <a href="https://arxiv.org/pdf/2512.04862.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04862v1" 
            onclick="toggleFavorite(this, '2512.04862v1', 'Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Maria-Paola Forte, Nikos Athanasiou, Giulia Ballardini, Jan Ulrich Bartels, Katherine J. Kuchenbecker, Michael J. Black

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-04

**Â§áÊ≥®**: * Equal contribution. Minor figure corrections compared to the ICCV 2025 version

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BioTUCHÔºåÁªìÂêàÁîüÁâ©ÈòªÊäóÊÑüÁü•‰ºòÂåñËá™Êé•Ëß¶Âú∫ÊôØ‰∏ãÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º™Ê†áÁ≠æ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°` `Ëá™Êé•Ëß¶` `ÁîüÁâ©ÈòªÊäó‰º†ÊÑü` `ÂßøÊÄÅ‰ºòÂåñ` `ÂèØÁ©øÊà¥ËÆæÂ§á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éËßÜËßâÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÂú®Ëá™Êé•Ëß¶Âú∫ÊôØ‰∏ãË°®Áé∞‰∏ç‰Ω≥Ôºå‰æãÂ¶ÇÊâã‰∏éËÑ∏ÈÉ®ÁöÑÊé•Ëß¶„ÄÇ
2. BioTUCHÁªìÂêàËßÜËßâÂßøÊÄÅ‰º∞ËÆ°Âô®ÂíåÁîüÁâ©ÈòªÊäó‰º†ÊÑüÔºåÂà©Áî®ÁîüÁâ©ÈòªÊäóÊÑüÁü•ÁöÆËÇ§Êé•Ëß¶Ôºå‰ºòÂåñ‰∫∫‰ΩìÂßøÊÄÅ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBioTUCHÂú®ÈáçÂª∫Á≤æÂ∫¶‰∏äÂπ≥ÂùáÊèêÂçá‰∫Ü11.7%ÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜËé∑ÂèñÁ≤æÁ°ÆÁöÑÈáéÂ§ñ3D‰∫∫‰ΩìÂßøÊÄÅÔºå‰ªéËÄå‰∏∫ÂßøÊÄÅ‰º∞ËÆ°ÂíåÂä®‰ΩúÁîüÊàêÊñπÊ≥ïÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑÊï∞ÊçÆÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂„ÄÇËôΩÁÑ∂Âü∫‰∫éËßÜÈ¢ëÁöÑ‰º∞ËÆ°ÊñπÊ≥ïÂ∑≤ÁªèÂèòÂæóË∂äÊù•Ë∂äÁ≤æÁ°ÆÔºå‰ΩÜÂÆÉ‰ª¨Âú®Ê∂âÂèäËá™Êé•Ëß¶ÁöÑÂ∏∏ËßÅÂú∫ÊôØ‰∏≠ÁªèÂ∏∏Â§±ÊïàÔºå‰æãÂ¶ÇÊâãËß¶Êë∏ËÑ∏ÈÉ®„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂèØÁ©øÊà¥ÁîüÁâ©ÈòªÊäó‰º†ÊÑüÂèØ‰ª•Âªâ‰ª∑‰∏î‰∏çÂºï‰∫∫Ê≥®ÁõÆÂú∞ÊµãÈáèÁúüÂÆûÁöÑÁöÆËÇ§Êé•Ëß¶„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªìÂêàËßÜËßâÂßøÊÄÅ‰º∞ËÆ°Âô®ÂíåÁîüÁâ©ÈòªÊäó‰º†ÊÑüÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáËÄÉËôëËá™Êé•Ëß¶Êù•ÊçïËé∑‰∫∫ÁöÑ3DÂßøÊÄÅ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïBioTUCHÔºå‰ΩøÁî®Áé∞ÊàêÁöÑ‰º∞ËÆ°Âô®ÂàùÂßãÂåñÂßøÊÄÅÔºåÂπ∂Âú®ÊµãÈáèÁöÑËá™Êé•Ëß¶ÊúüÈó¥ÂºïÂÖ•Êé•Ëß¶ÊÑüÁü•ÂßøÊÄÅ‰ºòÂåñÔºöÂú®Âº∫Âà∂ÊâßË°åÈ°∂ÁÇπÈÇªËøëÁ∫¶ÊùüÁöÑÂêåÊó∂ÔºåÊúÄÂ∞èÂåñÈáçÊäïÂΩ±ËØØÂ∑ÆÂíå‰∏éËæìÂÖ•‰º∞ËÆ°ÁöÑÂÅèÂ∑Æ„ÄÇÊàë‰ª¨‰ΩøÁî®ÂêåÊ≠•RGBËßÜÈ¢ë„ÄÅÁîüÁâ©ÈòªÊäóÊµãÈáèÂíå3DËøêÂä®ÊçïÊçâÁöÑÊñ∞Êï∞ÊçÆÈõÜÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ï„ÄÇÈÄöËøá‰ΩøÁî®‰∏â‰∏™ËæìÂÖ•ÂßøÊÄÅ‰º∞ËÆ°Âô®ËøõË°åÊµãËØïÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÈáçÂª∫Á≤æÂ∫¶Âπ≥ÂùáÊèêÈ´ò‰∫Ü11.7%„ÄÇÊàë‰ª¨ËøòÂ±ïÁ§∫‰∫Ü‰∏ÄÁßçÂæÆÂûãÂèØÁ©øÊà¥ÁîüÁâ©ÈòªÊäó‰º†ÊÑüÂô®ÔºåËØ•‰º†ÊÑüÂô®ËÉΩÂ§üÊúâÊïàÁöÑÂ§ßËßÑÊ®°Êî∂ÈõÜÊé•Ëß¶ÊÑüÁü•ËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ªéËÄå‰ΩøÁî®BioTUCHÊîπËøõÂßøÊÄÅ‰º∞ËÆ°ÂíåÁîüÊàê„ÄÇ‰ª£Á†ÅÂíåÊï∞ÊçÆÂèØÂú®biotuch.is.tue.mpg.deËé∑Âæó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®Ëá™Êé•Ëß¶Âú∫ÊôØ‰∏ãÔºåÂü∫‰∫éËßÜËßâÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Á≤æÂ∫¶‰ΩéÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈöæ‰ª•ÂáÜÁ°ÆÂ§ÑÁêÜÈÅÆÊå°ÂíåËá™Êé•Ëß¶Â∏¶Êù•ÁöÑÊ≠ß‰πâÊÄßÔºåÂØºËá¥ÂßøÊÄÅ‰º∞ËÆ°ËØØÂ∑ÆÂ¢ûÂ§ß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÁîüÁâ©ÈòªÊäó‰º†ÊÑüÊù•Ëé∑ÂèñÁöÆËÇ§Èó¥ÁöÑÊé•Ëß¶‰ø°ÊÅØÔºåÂπ∂Â∞ÜËøô‰∫õ‰ø°ÊÅØËûçÂÖ•Âà∞ÂßøÊÄÅ‰ºòÂåñËøáÁ®ã‰∏≠„ÄÇÈÄöËøáÁîüÁâ©ÈòªÊäó‰º†ÊÑüÂô®Ê£ÄÊµãÁöÆËÇ§Êé•Ëß¶Ôºå‰∏∫ÂßøÊÄÅ‰º∞ËÆ°Êèê‰æõÈ¢ùÂ§ñÁöÑÁ∫¶ÊùüÔºå‰ªéËÄåÊèêÈ´òÂú®Ëá™Êé•Ëß¶Âú∫ÊôØ‰∏ãÁöÑÂßøÊÄÅ‰º∞ËÆ°Á≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöBioTUCHÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ‰ΩøÁî®Áé∞ÊàêÁöÑÂßøÊÄÅ‰º∞ËÆ°Âô®ÂàùÂßãÂåñ‰∫∫‰ΩìÂßøÊÄÅÔºõ2) Âà©Áî®ÁîüÁâ©ÈòªÊäó‰º†ÊÑüÂô®Ê£ÄÊµãÁöÆËÇ§Èó¥ÁöÑÊé•Ëß¶Ôºõ3) Âü∫‰∫éÊ£ÄÊµãÂà∞ÁöÑÊé•Ëß¶‰ø°ÊÅØÔºåËøõË°åÊé•Ëß¶ÊÑüÁü•ÁöÑÂßøÊÄÅ‰ºòÂåñ„ÄÇÂßøÊÄÅ‰ºòÂåñËøáÁ®ãÂêåÊó∂ËÄÉËôë‰∫ÜÈáçÊäïÂΩ±ËØØÂ∑Æ„ÄÅ‰∏éÂàùÂßãÂßøÊÄÅÁöÑÂÅèÂ∑Æ‰ª•ÂèäÈ°∂ÁÇπÈÇªËøëÁ∫¶Êùü„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÁîüÁâ©ÈòªÊäó‰º†ÊÑü‰∏éËßÜËßâÂßøÊÄÅ‰º∞ËÆ°Áõ∏ÁªìÂêàÔºåÂà©Áî®ÁîüÁâ©ÈòªÊäóÊèê‰æõÁöÑÊé•Ëß¶‰ø°ÊÅØÊù•Á∫¶ÊùüÂßøÊÄÅ‰ºòÂåñËøáÁ®ã„ÄÇËøôÁßçÁªìÂêàÊñπÂºèËÉΩÂ§üÊúâÊïàÂú∞Ëß£ÂÜ≥Ëá™Êé•Ëß¶Âú∫ÊôØ‰∏ãÁöÑÊ≠ß‰πâÊÄßÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂßøÊÄÅ‰ºòÂåñËøáÁ®ã‰∏≠ÔºåËÆ∫ÊñáËÆæËÆ°‰∫ÜÂåÖÂê´ÈáçÊäïÂΩ±ËØØÂ∑ÆÈ°π„ÄÅ‰∏éÂàùÂßãÂßøÊÄÅÂÅèÂ∑ÆÈ°π‰ª•ÂèäÈ°∂ÁÇπÈÇªËøëÁ∫¶ÊùüÈ°πÁöÑÊçüÂ§±ÂáΩÊï∞„ÄÇÈáçÊäïÂΩ±ËØØÂ∑ÆÈ°πÁî®‰∫é‰øùËØÅ‰º∞ËÆ°ÁöÑÂßøÊÄÅ‰∏éÂõæÂÉèËßÇÊµã‰∏ÄËá¥ÔºõÂÅèÂ∑ÆÈ°πÁî®‰∫éÈò≤Ê≠¢ÂßøÊÄÅËøáÂ∫¶ÂÅèÁ¶ªÂàùÂßã‰º∞ËÆ°ÔºõÈ°∂ÁÇπÈÇªËøëÁ∫¶ÊùüÈ°πÁî®‰∫é‰øùËØÅ‰∫∫‰ΩìÁªìÊûÑÁöÑÂêàÁêÜÊÄß„ÄÇÁîüÁâ©ÈòªÊäó‰º†ÊÑüÂô®ÈááÁî®ÂæÆÂûãÂèØÁ©øÊà¥ËÆæËÆ°ÔºåÊñπ‰æøÂ§ßËßÑÊ®°Êï∞ÊçÆÈááÈõÜ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBioTUCHÊñπÊ≥ïÂú®‰∏â‰∏™‰∏çÂêåÁöÑËæìÂÖ•ÂßøÊÄÅ‰º∞ËÆ°Âô®‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ≥ÂùáÈáçÂª∫Á≤æÂ∫¶ÊèêÈ´ò‰∫Ü11.7%„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂ±ïÁ§∫‰∫Ü‰∏ÄÁßçÂæÆÂûãÂèØÁ©øÊà¥ÁîüÁâ©ÈòªÊäó‰º†ÊÑüÂô®Ôºå‰∏∫Â§ßËßÑÊ®°Êî∂ÈõÜÊé•Ëß¶ÊÑüÁü•ËÆ≠ÁªÉÊï∞ÊçÆÊèê‰æõ‰∫ÜÂèØËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰∫∫Êú∫‰∫§‰∫í„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅËøêÂä®ÂàÜÊûê„ÄÅÂåªÁñóÂ∫∑Â§çÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÈ´òËá™Êé•Ëß¶Âú∫ÊôØ‰∏ãÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Á≤æÂ∫¶ÔºåÂèØ‰ª•ÊîπÂñÑ‰∫∫Êú∫‰∫§‰∫íÁöÑËá™ÁÑ∂ÊÄßÂíåÂáÜÁ°ÆÊÄßÔºåÊèêÂçáËôöÊãüÁé∞ÂÆû‰ΩìÈ™åÁöÑÊ≤âÊµ∏ÊÑüÔºå‰∏∫ËøêÂä®ÂàÜÊûêÊèê‰æõÊõ¥ÂèØÈù†ÁöÑÊï∞ÊçÆÔºåÂπ∂‰∏∫ÂåªÁñóÂ∫∑Â§çÊèê‰æõÊõ¥Á≤æÁ°ÆÁöÑÂßøÊÄÅËØÑ‰º∞„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de

