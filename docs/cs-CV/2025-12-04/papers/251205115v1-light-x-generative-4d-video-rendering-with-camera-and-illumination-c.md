---
layout: default
title: Light-X: Generative 4D Video Rendering with Camera and Illumination Control
---

# Light-X: Generative 4D Video Rendering with Camera and Illumination Control

**arXiv**: [2512.05115v1](https://arxiv.org/abs/2512.05115) | [PDF](https://arxiv.org/pdf/2512.05115.pdf)

**ä½œè€…**: Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye, Bohan Li, Zhiguo Cao, Wei Li, Hao Zhao, Ziwei Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLight-Xæ¡†æž¶ï¼Œå®žçŽ°å•ç›®è§†é¢‘çš„ç›¸æœºè½¨è¿¹ä¸Žå…‰ç…§è”åˆå¯æŽ§ç”Ÿæˆæ¸²æŸ“**

**å…³é”®è¯**: `4Dè§†é¢‘ç”Ÿæˆ` `ç›¸æœºè½¨è¿¹æŽ§åˆ¶` `å…‰ç…§æŽ§åˆ¶` `è§£è€¦æ¸²æŸ“` `åˆæˆæ•°æ®é›†` `å•ç›®è§†é¢‘å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘å…‰ç…§æŽ§åˆ¶æ–¹æ³•åœ¨å…‰ç…§ä¿çœŸåº¦ä¸Žæ—¶é—´ä¸€è‡´æ€§é—´å­˜åœ¨æƒè¡¡ï¼Œä¸”ç¼ºä¹ç›¸æœºä¸Žå…‰ç…§çš„è”åˆæŽ§åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§£è€¦å‡ ä½•ä¸Žå…‰ç…§ä¿¡å·ï¼Œåˆ©ç”¨åŠ¨æ€ç‚¹äº‘å’Œé‡å…‰ç…§å¸§æä¾›æ˜¾å¼ç»†ç²’åº¦çº¿ç´¢ï¼Œå¹¶å¼•å…¥Light-Synåˆæˆè®­ç»ƒæ•°æ®
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è”åˆç›¸æœº-å…‰ç…§æŽ§åˆ¶ä¸Šä¼˜äºŽåŸºçº¿æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬å’ŒèƒŒæ™¯æ¡ä»¶ä¸‹è¶…è¶Šå…ˆå‰è§†é¢‘é‡å…‰ç…§æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.

