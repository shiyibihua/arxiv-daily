---
layout: default
title: MemLoRA: Distilling Expert Adapters for On-Device Memory Systems
---

# MemLoRA: Distilling Expert Adapters for On-Device Memory Systems

**arXiv**: [2512.04763v1](https://arxiv.org/abs/2512.04763) | [PDF](https://arxiv.org/pdf/2512.04763.pdf)

**ä½œè€…**: Massimo Bini, Ondrej Bohdal, Umberto Michieli, Zeynep Akata, Mete Ozay, Taha Ceritli

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMemLoRAä¸ŽMemLoRA-Vï¼Œé€šè¿‡è’¸é¦ä¸“å®¶é€‚é…å™¨å®žçŽ°è®¾å¤‡ç«¯å†…å­˜ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æœ¬ä¸Žè§†è§‰ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `è®¾å¤‡ç«¯å†…å­˜ç³»ç»Ÿ` `ä¸“å®¶é€‚é…å™¨è’¸é¦` `å°è¯­è¨€æ¨¡åž‹` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€æŽ¨ç†` `çŸ¥è¯†è’¸é¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šLLMå†…å­˜å¢žå¼ºç³»ç»Ÿæˆæœ¬é«˜ï¼ŒSLMæ€§èƒ½ä¸è¶³ï¼Œä¸”ç¼ºä¹è§†è§‰èƒ½åŠ›ï¼Œé™åˆ¶è®¾å¤‡ç«¯ä¸Žå¤šæ¨¡æ€åº”ç”¨ã€‚
2. æ–¹æ³•ï¼šä¸ºSLMå’ŒSVLMè®­ç»ƒä¸“ç”¨å†…å­˜é€‚é…å™¨ï¼ŒåŸºäºŽçŸ¥è¯†è’¸é¦ï¼Œåˆ†åˆ«å¤„ç†çŸ¥è¯†æå–ã€æ›´æ–°å’Œç”Ÿæˆæ“ä½œã€‚
3. æ•ˆæžœï¼šåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šè¶…è¶Šå¤§æ¨¡åž‹åŸºçº¿ï¼Œè§†è§‰ä»»åŠ¡ä¸Šå¤§å¹…æå‡å‡†ç¡®çŽ‡ï¼Œä¿æŒè®¾å¤‡ç«¯éƒ¨ç½²ä¼˜åŠ¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.

