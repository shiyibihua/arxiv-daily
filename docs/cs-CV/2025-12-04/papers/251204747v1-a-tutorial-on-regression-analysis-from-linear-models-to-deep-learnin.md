---
layout: default
title: A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence
---

# A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence

**arXiv**: [2512.04747v1](https://arxiv.org/abs/2512.04747) | [PDF](https://arxiv.org/pdf/2512.04747.pdf)

**ä½œè€…**: Jingyuan Wang, Jiahao Ji

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æä¾›å›žå½’åˆ†æžæ•™ç¨‹ï¼Œæ¶µç›–çº¿æ€§æ¨¡åž‹åˆ°æ·±åº¦å­¦ä¹ ï¼Œä½œä¸ºæ™ºèƒ½è®¡ç®—è¯¾ç¨‹çš„è‡ªå­¦ææ–™ã€‚**

**å…³é”®è¯**: `å›žå½’åˆ†æž` `çº¿æ€§æ¨¡åž‹` `æ·±åº¦å­¦ä¹ ` `æœºå™¨å­¦ä¹ ` `ç»Ÿè®¡å»ºæ¨¡` `ä¼˜åŒ–ç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç³»ç»Ÿä»‹ç»å›žå½’åˆ†æžåŸºç¡€æ¦‚å¿µã€å»ºæ¨¡ç»„ä»¶å’Œç†è®ºï¼Œæ— éœ€é¢å¤–å‚è€ƒèµ„æ–™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè¦†ç›–çº¿æ€§å›žå½’ã€é€»è¾‘å›žå½’ã€å¤šé¡¹å¼å›žå½’ã€æ ¸æ–¹æ³•åŠç¥žç»ç½‘ç»œéžçº¿æ€§å›žå½’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡æ•°å­¦æŽ¨å¯¼ã€ç¤ºä¾‹å’Œå¯è§†åŒ–ï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£æ¨¡åž‹æž„å»ºã€ä¼˜åŒ–åŠç‰¹å¾ä¸Žå“åº”å˜é‡å…³ç³»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This article serves as the regression analysis lecture notes in the Intelligent Computing course cluster (including the courses of Artificial Intelligence, Data Mining, Machine Learning, and Pattern Recognition). It aims to provide students -- who are assumed to possess only basic university-level mathematics (i.e., with prerequisite courses in calculus, linear algebra, and probability theory) -- with a comprehensive and self-contained understanding of regression analysis without requiring any additional references. The lecture notes systematically introduce the fundamental concepts, modeling components, and theoretical foundations of regression analysis, covering linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. Core methodological topics include loss-function design, parameter-estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques such as Ridge and LASSO regression. Through detailed mathematical derivations, illustrative examples, and intuitive visual explanations, the materials help students understand not only how regression models are constructed and optimized, but also how they reveal the underlying relationships between features and response variables. By bridging classical statistical modeling and modern machine-learning practice, these lecture notes aim to equip students with a solid conceptual and technical foundation for further study in advanced artificial intelligence models.

