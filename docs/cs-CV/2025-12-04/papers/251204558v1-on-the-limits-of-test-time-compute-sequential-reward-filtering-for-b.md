---
layout: default
title: On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference
---

# On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference

**arXiv**: [2512.04558v1](https://arxiv.org/abs/2512.04558) | [PDF](https://arxiv.org/pdf/2512.04558.pdf)

**ä½œè€…**: Yue Yu, Qiwei Di, Quanquan Gu, Dongruo Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¥–åŠ±è¿‡æ»¤é¡ºåºæŽ¨ç†ä»¥ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—ï¼Œæå‡å¤§è¯­è¨€æ¨¡åž‹æ€§èƒ½**

**å…³é”®è¯**: `æµ‹è¯•æ—¶è®¡ç®—` `é¡ºåºæŽ¨ç†` `å¥–åŠ±è¿‡æ»¤` `å¤§è¯­è¨€æ¨¡åž‹ä¼˜åŒ–` `ç†è®ºåˆ†æž` `å®žè¯è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åˆ†æžæµ‹è¯•æ—¶è®¡ç®—èŒƒå¼ï¼Œè¯æ˜Žæ ‡å‡†æœ€ä½³-né‡‡æ ·å­˜åœ¨å›ºæœ‰æ¬¡ä¼˜æ€§
2. å¼•å…¥å¥–åŠ±è¿‡æ»¤æœºåˆ¶ï¼Œé€‰æ‹©æ€§å°†é«˜å¥–åŠ±ç”Ÿæˆçº³å…¥ä¸Šä¸‹æ–‡ï¼Œé›†ä¸­è®¡ç®—äºŽä¼˜è´¨å€™é€‰
3. ç†è®ºè¯æ˜Žä¼˜äºŽæ ‡å‡†æ–¹æ³•ï¼Œå®žéªŒéªŒè¯åœ¨å¤šæ ·åŸºå‡†ä¸Šå®žçŽ°ä¸€è‡´æ”¹è¿›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.

