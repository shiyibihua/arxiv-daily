---
layout: default
title: SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs
---

# SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs

**arXiv**: [2512.04746v1](https://arxiv.org/abs/2512.04746) | [PDF](https://arxiv.org/pdf/2512.04746.pdf)

**ä½œè€…**: Wenhua Cheng, Weiwei Zhang, Heng Guo, Haihao Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSignRoundV2æ¡†æž¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹æžä½Žæ¯”ç‰¹åŽè®­ç»ƒé‡åŒ–ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–` `åŽè®­ç»ƒé‡åŒ–` `æžä½Žæ¯”ç‰¹é‡åŒ–` `æ•æ„Ÿåº¦åº¦é‡` `æ¯”ç‰¹åˆ†é…` `é‡åŒ–å°ºåº¦æœç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæžä½Žæ¯”ç‰¹é‡åŒ–ï¼ˆå¦‚2ä½å’Œ4ä½ï¼‰å¸¸å¯¼è‡´å¤§è¯­è¨€æ¨¡åž‹æ€§èƒ½ä¸¥é‡ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ¢¯åº¦ä¸Žé‡åŒ–åå·®çš„å¿«é€Ÿæ•æ„Ÿåº¦åº¦é‡ï¼ŒæŒ‡å¯¼å±‚é—´æ¯”ç‰¹åˆ†é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨4-5ä½å®žçŽ°çº¦1%æ–¹å·®ï¼Œ2ä½ä¿æŒå¼ºæ€§èƒ½ï¼ŒæŽ¥è¿‘å…¨ç²¾åº¦æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.

