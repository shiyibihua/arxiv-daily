---
layout: default
title: A result relating convex n-widths to covering numbers with some applications to neural networks
---

# A result relating convex n-widths to covering numbers with some applications to neural networks

**arXiv**: [2512.04912v1](https://arxiv.org/abs/2512.04912) | [PDF](https://arxiv.org/pdf/2512.04912.pdf)

**ä½œè€…**: Jonathan Baxter, Peter Bartlett

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡¸æ ¸è¦†ç›–æ•°ä¸Žå‡½æ•°ç±»é€¼è¿‘è¯¯å·®çš„å…³ç³»ï¼Œåº”ç”¨äºŽç¥žç»ç½‘ç»œé€¼è¿‘çŽ‡ä¸Šç•Œåˆ†æžã€‚**

**å…³é”®è¯**: `å‡¸æ ¸è¦†ç›–æ•°` `å‡½æ•°é€¼è¿‘` `ç¥žç»ç½‘ç»œé€¼è¿‘` `ç»´åº¦è¯…å’’` `é«˜ç»´æ¨¡å¼è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé«˜ç»´å‡½æ•°ç±»å¦‚ä½•è¢«å°ç‰¹å¾é›†çº¿æ€§ç»„åˆæœ‰æ•ˆé€¼è¿‘ï¼Œé¿å…ç»´åº¦è¯…å’’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºç«‹å‡½æ•°ç±»é€¼è¿‘è¯¯å·®ä¸Žå…¶å‡¸æ ¸è¦†ç›–æ•°ä¹‹é—´çš„ç†è®ºå…³ç³»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåˆ©ç”¨å•éšè—èŠ‚ç‚¹å‡½æ•°ç±»çš„è¦†ç›–æ•°ä¸Šç•Œï¼ŒæŽ¨å¯¼ç¥žç»ç½‘ç»œé€¼è¿‘çŽ‡ä¸Šç•Œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In general, approximating classes of functions defined over high-dimensional input spaces by linear combinations of a fixed set of basis functions or ``features'' is known to be hard. Typically, the worst-case error of the best basis set decays only as fast as $Î˜\(n^{-1/d}\)$, where $n$ is the number of basis functions and $d$ is the input dimension. However, there are many examples of high-dimensional pattern recognition problems (such as face recognition) where linear combinations of small sets of features do solve the problem well. Hence these function classes do not suffer from the ``curse of dimensionality'' associated with more general classes. It is natural then, to look for characterizations of high-dimensional function classes that nevertheless are approximated well by linear combinations of small sets of features. In this paper we give a general result relating the error of approximation of a function class to the covering number of its ``convex core''. For one-hidden-layer neural networks, covering numbers of the class of functions computed by a single hidden node upper bound the covering numbers of the convex core. Hence, using standard results we obtain upper bounds on the approximation rate of neural network classes.

