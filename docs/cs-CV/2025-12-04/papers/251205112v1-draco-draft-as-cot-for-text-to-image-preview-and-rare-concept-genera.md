---
layout: default
title: DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation
---

# DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation

**arXiv**: [2512.05112v1](https://arxiv.org/abs/2512.05112) | [PDF](https://arxiv.org/pdf/2512.05112.pdf)

**ä½œè€…**: Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDraCoæ–¹æ³•ï¼Œé€šè¿‡è‰ç¨¿å›¾åƒä½œä¸ºæ€ç»´é“¾ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é¢„è§ˆä¸Žç½•è§æ¦‚å¿µç”Ÿæˆèƒ½åŠ›ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ€ç»´é“¾æŽ¨ç†` `è‰ç¨¿å›¾åƒé¢„è§ˆ` `ç½•è§æ¦‚å¿µç”Ÿæˆ` `é€‰æ‹©æ€§ä¿®æ­£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–æŠ½è±¡æ–‡æœ¬è§„åˆ’ï¼Œå¯¼è‡´æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è§„åˆ’ç²’åº¦ç²—å’Œç½•è§å±žæ€§ç»„åˆç”Ÿæˆå›°éš¾ã€‚
2. DraCoé¦–å…ˆç”Ÿæˆä½Žåˆ†è¾¨çŽ‡è‰ç¨¿å›¾åƒä½œä¸ºè§†è§‰è§„åˆ’ï¼Œç„¶åŽåˆ©ç”¨æ¨¡åž‹ç†è§£èƒ½åŠ›éªŒè¯è¯­ä¹‰å¯¹é½å¹¶è¿›è¡Œé€‰æ‹©æ€§ä¿®æ­£ã€‚
3. åœ¨GenEvalç­‰åŸºå‡†ä¸Šï¼ŒDraCoç›¸æ¯”ç›´æŽ¥ç”Ÿæˆå’Œå…¶ä»–CoTæ–¹æ³•å–å¾—æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¦‚GenEvalå¢žåŠ 8%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.

