---
layout: default
title: Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function
---

# Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function

**arXiv**: [2512.04559v1](https://arxiv.org/abs/2512.04559) | [PDF](https://arxiv.org/pdf/2512.04559.pdf)

**ä½œè€…**: Hyeongyu Kang, Jaewoo Lee, Woocheol Shin, Kiyoung Om, Jinkyoo Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSQDFæ–¹æ³•ä»¥è§£å†³æ‰©æ•£æ¨¡åž‹å¾®è°ƒä¸­çš„å¥–åŠ±è¿‡ä¼˜åŒ–é—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹å¾®è°ƒ` `å¥–åŠ±è¿‡ä¼˜åŒ–` `è½¯Qå‡½æ•°` `ç­–ç•¥æ¢¯åº¦` `æ–‡æœ¬åˆ°å›¾åƒå¯¹é½` `åœ¨çº¿ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹å¾®è°ƒæ˜“å¯¼è‡´å¥–åŠ±è¿‡ä¼˜åŒ–ï¼Œç”Ÿæˆé«˜å¥–åŠ±ä½†ä¸è‡ªç„¶ä¸”å¤šæ ·æ€§å·®çš„æ ·æœ¬ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽè½¯Qå‡½æ•°çš„é‡å‚æ•°åŒ–ç­–ç•¥æ¢¯åº¦ï¼Œç»“åˆKLæ­£åˆ™åŒ–ã€æŠ˜æ‰£å› å­ã€ä¸€è‡´æ€§æ¨¡åž‹å’Œç¦»ç­–ç•¥å›žæ”¾ç¼“å†²åŒºã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒå¯¹é½ä¸­å®žçŽ°é«˜ç›®æ ‡å¥–åŠ±å¹¶ä¿æŒå¤šæ ·æ€§ï¼Œåœ¨åœ¨çº¿é»‘ç›’ä¼˜åŒ–ä¸­æ ·æœ¬æ•ˆçŽ‡é«˜ä¸”è‡ªç„¶å¤šæ ·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.

