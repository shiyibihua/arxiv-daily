---
layout: default
title: VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management
---

# VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management

**arXiv**: [2512.04540v1](https://arxiv.org/abs/2512.04540) | [PDF](https://arxiv.org/pdf/2512.04540.pdf)

**ä½œè€…**: Hongbo Jin, Qingyuan Wang, Wenhao Zhang, Yang Liu, Sijie Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoMemæ¡†æž¶ï¼Œé€šè¿‡è‡ªé€‚åº”å†…å­˜ç®¡ç†è§£å†³è¶…é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¶…é•¿è§†é¢‘ç†è§£` `è‡ªé€‚åº”å†…å­˜ç®¡ç†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–` `åºåˆ—ç”Ÿæˆä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è¶…é•¿è§†é¢‘ç†è§£ä¸­å—é™äºŽä¸Šä¸‹æ–‡é•¿åº¦å’Œé•¿æœŸè®°å¿†ä¿ç•™æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è‡ªé€‚åº”å…¨å±€å†…å­˜ç¼“å†²åŠ¨æ€æ›´æ–°å…³é”®ä¿¡æ¯ï¼Œé›†æˆPRPOç®—æ³•ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªè¶…é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰å¼€æºæ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.

