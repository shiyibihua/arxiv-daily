---
layout: default
title: Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models
---

# Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.04395" target="_blank" class="toolbar-btn">arXiv: 2512.04395v1</a>
    <a href="https://arxiv.org/pdf/2512.04395.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04395v1" 
            onclick="toggleFavorite(this, '2512.04395v1', 'Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Hieu Dinh Trung Pham, Huy Minh Nhat Nguyen, Cuong Tuan Nguyen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFARLæ¡†æ¶ï¼Œåˆ©ç”¨å‚…é‡Œå¶åˆ†æè§£è€¦è§†è§‰è¡¨å¾ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å°‘æ ·æœ¬å­¦ä¹ ` `å‚…é‡Œå¶åˆ†æ` `è¡¨å¾è§£è€¦` `äº¤å‰æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­ï¼Œå›¾åƒçš„é¢†åŸŸä¸å˜ç»“æ„ä¸é¢†åŸŸç‰¹å®šé£æ ¼çº ç¼ ï¼Œé™åˆ¶äº†æ³›åŒ–èƒ½åŠ›ã€‚
2. FARLæ¡†æ¶åˆ©ç”¨å‚…é‡Œå¶åˆ†ææ˜¾å¼è§£è€¦è§†è§‰è¡¨å¾ï¼Œé€šè¿‡åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åˆ†åˆ«æå–ç»“æ„å’Œé£æ ¼ç‰¹å¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFARLæ¡†æ¶åœ¨15ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œæå‡äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)å·²ç»å±•ç¤ºäº†å¼ºå¤§çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­¦ä¹ æ•´ä½“è¡¨å¾ï¼Œå…¶ä¸­å›¾åƒçš„é¢†åŸŸä¸å˜ç»“æ„ä¸å…¶é¢†åŸŸç‰¹å®šçš„é£æ ¼éšå¼åœ°çº ç¼ åœ¨ä¸€èµ·ã€‚è¿™ä¸ºé€šè¿‡è§£è€¦è¿™äº›è§†è§‰çº¿ç´¢æ¥è¿›ä¸€æ­¥å¢å¼ºæ³›åŒ–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å‚…é‡Œå¶æ³¨æ„åŠ›è¡¨å¾å­¦ä¹ (FARL)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨å‚…é‡Œå¶åˆ†ææ˜¾å¼åœ°è§£è€¦è§†è§‰è¡¨å¾æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ç§åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­å¯å­¦ä¹ çš„è¡¨å¾tokenåˆ†åˆ«æŸ¥è¯¢å›¾åƒçš„ç»“æ„ç‰¹å¾(æ¥è‡ªç›¸ä½è°±)å’Œé£æ ¼ç‰¹å¾(æ¥è‡ªå¹…åº¦è°±)ã€‚è¿™ä¸ªè¿‡ç¨‹äº§ç”Ÿä¸°å¯Œçš„ã€è§£è€¦çš„tokenï¼Œç„¶åå°†å…¶æ³¨å…¥åˆ°VLMç¼–ç å™¨ä¸­ä»¥æŒ‡å¯¼é€‚åº”ã€‚æˆ‘ä»¬çš„è®¾è®¡ï¼ŒåŒ…æ‹¬éå¯¹ç§°æ³¨å…¥ç­–ç•¥ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„è§†è§‰-è¯­è¨€å¯¹é½ã€‚åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºä¸€å®šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å­¦ä¹ åˆ°çš„è§†è§‰è¡¨å¾é€šå¸¸æ˜¯æ•´ä½“æ€§çš„ï¼Œå›¾åƒçš„ç»“æ„ä¿¡æ¯ï¼ˆé¢†åŸŸä¸å˜ï¼‰å’Œé£æ ¼ä¿¡æ¯ï¼ˆé¢†åŸŸç‰¹å®šï¼‰æ··åˆåœ¨ä¸€èµ·ã€‚è¿™ç§çº ç¼ ä½¿å¾—æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„é¢†åŸŸæˆ–æ•°æ®é›†ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ã€‚å› æ­¤ï¼Œå¦‚ä½•è§£è€¦å›¾åƒçš„ç»“æ„å’Œé£æ ¼ä¿¡æ¯ï¼Œä»è€Œæå‡VLMsçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFARLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å‚…é‡Œå¶åˆ†æå°†å›¾åƒåˆ†è§£ä¸ºå¹…åº¦è°±å’Œç›¸ä½è°±ï¼Œåˆ†åˆ«å¯¹åº”é£æ ¼å’Œç»“æ„ä¿¡æ¯ã€‚é€šè¿‡åˆ†åˆ«å¤„ç†è¿™ä¸¤ä¸ªè°±ï¼Œå¯ä»¥å®ç°è§†è§‰è¡¨å¾çš„è§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼ŒFARLä½¿ç”¨åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«ä»å¹…åº¦è°±å’Œç›¸ä½è°±ä¸­æå–ç‰¹å¾ï¼Œå¹¶å°†è¿™äº›è§£è€¦çš„ç‰¹å¾æ³¨å…¥åˆ°VLMsçš„ç¼–ç å™¨ä¸­ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„è§†è§‰-è¯­è¨€å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFARLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å‚…é‡Œå¶å˜æ¢ï¼šå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºå¹…åº¦è°±å’Œç›¸ä½è°±ã€‚2) åŒé‡äº¤å‰æ³¨æ„åŠ›ï¼šä½¿ç”¨ä¸¤ä¸ªå¯å­¦ä¹ çš„è¡¨å¾tokenï¼Œåˆ†åˆ«æŸ¥è¯¢å¹…åº¦è°±å’Œç›¸ä½è°±ï¼Œæå–é£æ ¼å’Œç»“æ„ç‰¹å¾ã€‚3) éå¯¹ç§°æ³¨å…¥ï¼šå°†æå–çš„ç‰¹å¾ä»¥éå¯¹ç§°çš„æ–¹å¼æ³¨å…¥åˆ°VLMsçš„ç¼–ç å™¨ä¸­ï¼Œå³å¯¹ç»“æ„å’Œé£æ ¼ç‰¹å¾é‡‡ç”¨ä¸åŒçš„æ³¨å…¥ç­–ç•¥ã€‚4) è§†è§‰-è¯­è¨€æ¨¡å‹ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„VLMsä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œä¾‹å¦‚CLIPã€‚

**å…³é”®åˆ›æ–°**ï¼šFARLçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ˜¾å¼è§£è€¦ï¼šé€šè¿‡å‚…é‡Œå¶åˆ†ææ˜¾å¼åœ°å°†å›¾åƒçš„ç»“æ„å’Œé£æ ¼ä¿¡æ¯è§£è€¦ã€‚2) åŒé‡äº¤å‰æ³¨æ„åŠ›ï¼šè®¾è®¡äº†ä¸€ç§åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«ä»å¹…åº¦è°±å’Œç›¸ä½è°±ä¸­æå–ç‰¹å¾ã€‚3) éå¯¹ç§°æ³¨å…¥ï¼šæå‡ºäº†ä¸€ç§éå¯¹ç§°çš„ç‰¹å¾æ³¨å…¥ç­–ç•¥ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFARLä¸æ˜¯éšå¼åœ°å­¦ä¹ è§£è€¦çš„è¡¨å¾ï¼Œè€Œæ˜¯é€šè¿‡å‚…é‡Œå¶åˆ†ææ˜¾å¼åœ°è¿›è¡Œè§£è€¦ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼š1) å‚…é‡Œå¶å˜æ¢ï¼šä½¿ç”¨æ ‡å‡†çš„äºŒç»´ç¦»æ•£å‚…é‡Œå¶å˜æ¢ã€‚2) åŒé‡äº¤å‰æ³¨æ„åŠ›ï¼šä½¿ç”¨Transformerä¸­çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚3) éå¯¹ç§°æ³¨å…¥ï¼šå¯¹ç»“æ„ç‰¹å¾å’Œé£æ ¼ç‰¹å¾é‡‡ç”¨ä¸åŒçš„æ³¨å…¥å±‚ï¼Œä¾‹å¦‚ï¼Œç»“æ„ç‰¹å¾æ³¨å…¥åˆ°æ›´æ·±çš„å±‚ï¼Œé£æ ¼ç‰¹å¾æ³¨å…¥åˆ°æ›´æµ…çš„å±‚ã€‚4) æŸå¤±å‡½æ•°ï¼šä½¿ç”¨æ ‡å‡†çš„è§†è§‰-è¯­è¨€å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚InfoNCEã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFARLæ¡†æ¶åœ¨15ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒFARLç›¸æ¯”äºåŸºçº¿æ–¹æ³•å–å¾—äº†è¶…è¿‡5%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†å‚…é‡Œå¶åˆ†æã€åŒé‡äº¤å‰æ³¨æ„åŠ›å’Œéå¯¹ç§°æ³¨å…¥ç­‰å…³é”®æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FARLæ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€å›¾åƒæ£€ç´¢ã€è§†è§‰é—®ç­”ç­‰ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæå‡äº†è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé™ä½äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æœªæ¥ï¼ŒFARLå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œæ–‡æœ¬ï¼Œä»è€Œæ„å»ºæ›´é€šç”¨çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.

