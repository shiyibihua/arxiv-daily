---
layout: default
title: Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation
---

# Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation

**arXiv**: [2512.04678v1](https://arxiv.org/abs/2512.04678) | [PDF](https://arxiv.org/pdf/2512.04678.pdf)

**ä½œè€…**: Yunhong Lu, Yanhong Zeng, Haobo Li, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jiapeng Zhu, Hengyuan Cao, Zhipeng Zhang, Xing Zhu, Yujun Shen, Min Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReward Forcingæ¡†æž¶ï¼Œé€šè¿‡EMA-Sinkå’ŒRe-DMDè§£å†³æµå¼è§†é¢‘ç”Ÿæˆä¸­åˆå§‹å¸§å¤åˆ¶å’ŒåŠ¨æ€è´¨é‡ä¸è¶³çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `æµå¼è§†é¢‘ç”Ÿæˆ` `æ³¨æ„åŠ›æœºåˆ¶` `è’¸é¦è®­ç»ƒ` `åŠ¨æ€è´¨é‡ä¼˜åŒ–` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–é™æ€åˆå§‹å¸§ä½œä¸ºæ³¨æ„åŠ›ä»¤ç‰Œï¼Œå¯¼è‡´è§†é¢‘å¸§å¤åˆ¶åˆå§‹å¸§ï¼ŒåŠ¨æ€è¿åŠ¨å‡å¼±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥EMA-Sinkä»¤ç‰Œï¼Œé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡èžåˆé€€å‡ºçª—å£çš„ä»¤ç‰Œï¼Œæ•èŽ·é•¿æœŸä¸Šä¸‹æ–‡å’Œè¿‘æœŸåŠ¨æ€ï¼›æå‡ºRe-DMDè’¸é¦ï¼ŒåŸºäºŽè§†è§‰è¯­è¨€æ¨¡åž‹å¥–åŠ±ä¼˜å…ˆåŠ¨æ€æ ·æœ¬ï¼Œæå‡è¿åŠ¨è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†åŸºå‡†ä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œå•H100 GPUä¸Šå®žçŽ°23.1 FPSçš„é«˜è´¨é‡æµå¼è§†é¢‘ç”Ÿæˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.

