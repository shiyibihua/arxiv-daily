---
layout: default
title: When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering
---

# When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering

**arXiv**: [2512.04597v1](https://arxiv.org/abs/2512.04597) | [PDF](https://arxiv.org/pdf/2512.04597.pdf)

**ä½œè€…**: Tao Wu, Chuhao Zhou, Guangyu Zhao, Haozhi Cao, Yewen Pu, Jianfei Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAbstainEQAåŸºå‡†ä»¥è¯„ä¼°å…·èº«é—®ç­”ä¸­æœºå™¨äººä½•æ—¶åº”è¯´â€œä¸çŸ¥é“â€çš„æ‹’ç»èƒ½åŠ›**

**å…³é”®è¯**: `å…·èº«é—®ç­”` `æ‹’ç»èƒ½åŠ›` `åŸºå‡†è¯„ä¼°` `æ•°æ®é›†æž„å»º` `äººæœºäº¤äº’` `è®¤çŸ¥ç†è®º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å…·èº«é—®ç­”åŸºå‡†å‡è®¾æ‰€æœ‰é—®é¢˜å¿…é¡»å›žç­”ï¼Œä½†æœºå™¨äººéœ€çŸ¥é“ä½•æ—¶ä¿¡æ¯ä¸è¶³è€Œæ‹’ç»å›žç­”
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽäººç±»æŸ¥è¯¢åˆ†æžå’Œè®¤çŸ¥ç†è®ºï¼Œå®šä¹‰äº”ç±»æ‹’ç»åœºæ™¯ï¼Œå¹¶æž„å»ºåŒ…å«1636ä¸ªæ¡ˆä¾‹çš„AbstainEQAæ•°æ®é›†
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºå‰æ²¿æ¨¡åž‹æ‹’ç»å¬å›žçŽ‡ä»…42.79%ï¼Œè¿œä½ŽäºŽäººç±»çš„91.17%ï¼Œä¸”ç¼©æ”¾ã€æç¤ºå’ŒæŽ¨ç†æ”¹è¿›æœ‰é™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.

