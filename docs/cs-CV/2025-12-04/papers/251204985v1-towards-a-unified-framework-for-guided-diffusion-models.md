---
layout: default
title: Towards a unified framework for guided diffusion models
---

# Towards a unified framework for guided diffusion models

**arXiv**: [2512.04985v1](https://arxiv.org/abs/2512.04985) | [PDF](https://arxiv.org/pdf/2512.04985.pdf)

**ä½œè€…**: Yuchen Jiao, Yuxin Chen, Gen Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æž¶ä»¥ç†è®ºåˆ†æžå¼•å¯¼æ‰©æ•£æ¨¡åž‹ï¼Œé‡åŒ–å¥–åŠ±æå‡å¹¶è§£é‡Šåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æ•ˆæžœã€‚**

**å…³é”®è¯**: `å¼•å¯¼æ‰©æ•£æ¨¡åž‹` `ç†è®ºåˆ†æž` `å¥–åŠ±ä¼˜åŒ–` `åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼` `æ‰©æ•£é‡‡æ ·å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼•å¯¼æ‰©æ•£æ¨¡åž‹çš„ç†è®ºç†è§£ä¸è¶³ï¼Œç¼ºä¹ç»Ÿä¸€åˆ†æžæ¡†æž¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ³¨å…¥å¥–åŠ±å¼•å¯¼é¡¹è‡³åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œé‡åŒ–å¥–åŠ±æ”¹è¿›ï¼Œå¹¶ç†è®ºè§£é‡Šåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ•°å€¼å®žéªŒéªŒè¯ç†è®ºï¼Œæ–°é‡‡æ ·å™¨æ˜“äºŽè®­ç»ƒä¸”æ— éœ€å®Œæ•´æ‰©æ•£è½¨è¿¹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Guided or controlled data generation with diffusion models\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.

