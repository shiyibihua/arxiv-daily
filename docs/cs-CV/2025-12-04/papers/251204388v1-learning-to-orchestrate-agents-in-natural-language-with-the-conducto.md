---
layout: default
title: Learning to Orchestrate Agents in Natural Language with the Conductor
---

# Learning to Orchestrate Agents in Natural Language with the Conductor

**arXiv**: [2512.04388v1](https://arxiv.org/abs/2512.04388) | [PDF](https://arxiv.org/pdf/2512.04388.pdf)

**ä½œè€…**: Stefan Nielsen, Edoardo Cetin, Peter Schwendeman, Qi Sun, Jinglue Xu, Yujin Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConductoræ¨¡åž‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åè°ƒå¤šLLMä»¥æå‡æŽ¨ç†æ€§èƒ½**

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹åè°ƒ` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“åä½œ` `æç¤ºå·¥ç¨‹` `æŽ¨ç†åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•è‡ªåŠ¨å‘çŽ°ä¸åŒLLMé—´çš„æœ‰æ•ˆåä½œç­–ç•¥ä»¥æœ€å¤§åŒ–æ•´ä½“æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒConductoræ¨¡åž‹ï¼Œè®¾è®¡é€šä¿¡æ‹“æ‰‘å’Œæç¤ºå·¥ç¨‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LiveCodeBenchå’ŒGPQAç­‰åŸºå‡†ä¸Šè¾¾åˆ°SOTAï¼Œé€‚åº”ä»»æ„ä»£ç†æ± 

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.

