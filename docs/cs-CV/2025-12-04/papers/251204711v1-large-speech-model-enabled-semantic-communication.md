---
layout: default
title: Large Speech Model Enabled Semantic Communication
---

# Large Speech Model Enabled Semantic Communication

**arXiv**: [2512.04711v1](https://arxiv.org/abs/2512.04711) | [PDF](https://arxiv.org/pdf/2512.04711.pdf)

**ä½œè€…**: Yun Tian, Zhijin Qin, Guocheng Lv, Ye Jin, Kaibin Huang, Zhu Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤§åž‹è¯­éŸ³æ¨¡åž‹çš„è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œå®žçŽ°è‡ªé€‚åº”åŽ‹ç¼©ä¸Žé²æ£’ä¼ è¾“**

**å…³é”®è¯**: `è¯­ä¹‰é€šä¿¡` `å¤§åž‹è¯­éŸ³æ¨¡åž‹` `è‡ªé€‚åº”ä¼ è¾“` `ä¸ç­‰é”™è¯¯ä¿æŠ¤` `ä½Žç§©é€‚åº”` `å®žæ—¶éƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è¯­éŸ³è¯­ä¹‰é€šä¿¡ç³»ç»Ÿå—é™äºŽç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œæ€§èƒ½æœ‰é™
2. é‡‡ç”¨Mimiè¯­éŸ³ç¼–è§£ç å™¨ä¸Žè‡ªé€‚åº”æŽ§åˆ¶å™¨ï¼Œæ”¯æŒå¸¦å®½è‡ªé€‚åº”ä¸Žä¸ç­‰é”™è¯¯ä¿æŠ¤
3. ä»¿çœŸæ˜¾ç¤ºç³»ç»Ÿåœ¨550 bpsè‡³2.06 kbpså¸¦å®½ä¸‹ï¼Œé«˜ä¸¢åŒ…çŽ‡æ—¶è¯­éŸ³è´¨é‡ä¼˜äºŽåŸºçº¿ï¼Œå»¶è¿Ÿçº¦460 ms

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing speech semantic communication systems mainly based on Joint Source-Channel Coding (JSCC) architectures have demonstrated impressive performance, but their effectiveness remains limited by model structures specifically designed for particular tasks and datasets. Recent advances indicate that generative large models pre-trained on massive datasets, can achieve outstanding performance arexhibit exceptional performance across diverse downstream tasks with minimal fine-tuning. To exploit the rich semantic knowledge embedded in large models and enable adaptive transmission over lossy channels, we propose a Large Speech Model enabled Semantic Communication (LargeSC) system. Simultaneously achieving adaptive compression and robust transmission over lossy channels remains challenging, requiring trade-offs among compression efficiency, speech quality, and latency. In this work, we employ the Mimi as a speech codec, converting speech into discrete tokens compatible with existing network architectures. We propose an adaptive controller module that enables adaptive transmission and in-band Unequal Error Protection (UEP), dynamically adjusting to both speech content and packet loss probability under bandwidth constraints. Additionally, we employ Low-Rank Adaptation (LoRA) to finetune the Moshi foundation model for generative recovery of lost speech tokens. Simulation results show that the proposed system supports bandwidths ranging from 550 bps to 2.06 kbps, outperforms conventional baselines in speech quality under high packet loss rates and achieves an end-to-end latency of approximately 460 ms, thereby demonstrating its potential for real-time deployment.

