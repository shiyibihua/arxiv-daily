---
layout: default
title: Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning
---

# Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning

**arXiv**: [2512.04632v1](https://arxiv.org/abs/2512.04632) | [PDF](https://arxiv.org/pdf/2512.04632.pdf)

**ä½œè€…**: Thibaut Boissin, Thomas Massena, Franck Mamalet, Mathieu Serrurier

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢„æ¡ä»¶åŒ–æ–¹æ³•ä»¥åŠ é€ŸåŸºäºŽæ­£äº¤æ€§çš„ä¼˜åŒ–å™¨ä¸­çš„ç‰›é¡¿-èˆ’å°”èŒ¨è¿‘ä¼¼æ”¶æ•›**

**å…³é”®è¯**: `æ­£äº¤æ€§ä¼˜åŒ–` `ç‰›é¡¿-èˆ’å°”èŒ¨è¿‘ä¼¼` `é¢„æ¡ä»¶åŒ–` `è®­ç»ƒåŠ é€Ÿ` `è®¡ç®—æ•ˆçŽ‡` `æ·±åº¦å­¦ä¹ ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºŽæ­£äº¤æ€§çš„ä¼˜åŒ–å™¨å¦‚Muonä¾èµ–æ˜‚è´µçš„æ¢¯åº¦æ­£äº¤åŒ–æ­¥éª¤ï¼Œç‰›é¡¿-èˆ’å°”èŒ¨è¿‘ä¼¼éœ€æ•°åæ¬¡çŸ©é˜µä¹˜æ³•
2. å¼•å…¥é¢„æ¡ä»¶åŒ–ç¨‹åºåŠ é€Ÿç‰›é¡¿-èˆ’å°”èŒ¨æ”¶æ•›ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ï¼Œå¼€é”€å¯å¿½ç•¥
3. å®žéªŒæ˜¾ç¤ºç‰›é¡¿-èˆ’å°”èŒ¨è¿‘ä¼¼åŠ é€Ÿè¾¾2.8å€ï¼Œç«¯åˆ°ç«¯è®­ç»ƒè¿è¡Œæ—¶æå‡5-10%ï¼Œæ¨¡åž‹æ€§èƒ½ä¿æŒæˆ–æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

