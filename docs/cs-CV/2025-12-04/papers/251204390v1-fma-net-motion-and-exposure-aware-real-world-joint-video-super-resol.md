---
layout: default
title: FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring
---

# FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring

**arXiv**: [2512.04390v1](https://arxiv.org/abs/2512.04390) | [PDF](https://arxiv.org/pdf/2512.04390.pdf)

**ä½œè€…**: Geunhyuk Youk, Jihyong Oh, Munchurl Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFMA-Net++æ¡†æž¶ï¼Œä»¥è§£å†³çœŸå®žè§†é¢‘ä¸­è¿åŠ¨ä¸ŽåŠ¨æ€æ›å…‰è€¦åˆçš„è”åˆè¶…åˆ†è¾¨çŽ‡ä¸ŽåŽ»æ¨¡ç³Šé—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘æ¢å¤` `è¿åŠ¨ä¸Žæ›å…‰è€¦åˆ` `è”åˆè¶…åˆ†è¾¨çŽ‡åŽ»æ¨¡ç³Š` `åºåˆ—çº§æž¶æž„` `åŠ¨æ€æ»¤æ³¢` `åˆæˆæ•°æ®è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçœŸå®žè§†é¢‘æ¢å¤é¢ä¸´è¿åŠ¨ä¸ŽåŠ¨æ€æ›å…‰è€¦åˆçš„å¤æ‚é€€åŒ–ï¼ŒçŽ°æœ‰æ–¹æ³•å¸¸å¿½ç•¥æ­¤æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åºåˆ—çº§æž¶æž„ï¼Œé€šè¿‡æ›å…‰æ—¶é—´æ„ŸçŸ¥è°ƒåˆ¶å’Œæµå¼•å¯¼åŠ¨æ€æ»¤æ³¢ï¼Œè§£è€¦é€€åŒ–å­¦ä¹ ä¸Žæ¢å¤ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨REDS-MEç­‰æ–°åŸºå‡†ä¸Šå®žçŽ°æœ€ä¼˜ç²¾åº¦ä¸Žæ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶æå‡æŽ¨ç†é€Ÿåº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.

