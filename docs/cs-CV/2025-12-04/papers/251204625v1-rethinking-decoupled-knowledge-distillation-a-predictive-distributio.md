---
layout: default
title: Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective
---

# Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective

**arXiv**: [2512.04625v1](https://arxiv.org/abs/2512.04625) | [PDF](https://arxiv.org/pdf/2512.04625.pdf)

**ä½œè€…**: Bowen Zheng, Ran Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹¿ä¹‰è§£è€¦çŸ¥è¯†è’¸é¦ä»¥ä¼˜åŒ–é¢„æµ‹åˆ†å¸ƒè§†è§’ä¸‹çš„çŸ¥è¯†è’¸é¦æ€§èƒ½**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `é¢„æµ‹åˆ†å¸ƒ` `è§£è€¦è’¸é¦` `logitè§£è€¦` `æ¢¯åº¦åˆ†æž` `æ¨¡åž‹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä»Žé¢„æµ‹åˆ†å¸ƒè§†è§’é‡æ–°å®¡è§†è§£è€¦çŸ¥è¯†è’¸é¦ï¼Œæ­ç¤ºæ•™å¸ˆæ¨¡åž‹é¢„æµ‹åˆ†å¸ƒå¯¹æ¢¯åº¦çš„å½±å“
2. å¼•å…¥å¹¿ä¹‰è§£è€¦çŸ¥è¯†è’¸é¦æŸå¤±ï¼Œæä¾›æ›´é€šç”¨çš„logitè§£è€¦æ–¹æ³•ï¼Œå¹¶è®¾è®¡é«˜æ•ˆåˆ†åŒºç­–ç•¥
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒGDKDä¼˜äºŽåŽŸå§‹DKDåŠå…¶ä»–é¢†å…ˆçŸ¥è¯†è’¸é¦æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.

