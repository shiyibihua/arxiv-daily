---
layout: default
title: E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving
---

# E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving

**arXiv**: [2512.04733v1](https://arxiv.org/abs/2512.04733) | [PDF](https://arxiv.org/pdf/2512.04733.pdf)

**ä½œè€…**: Yihong Tang, Haicheng Liao, Tong Nie, Junlin He, Ao Qu, Kehua Chen, Wei Ma, Zhenning Li, Lijun Sun, Chengzhong Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºE3ADæ¨¡åž‹ï¼Œé€šè¿‡æƒ…æ„Ÿæ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æž¶è§£å†³å¼€æ”¾åŸŸç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­ä¹˜å®¢æƒ…æ„Ÿå¿½ç•¥é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æƒ…æ„Ÿæ„ŸçŸ¥` `ç©ºé—´æŽ¨ç†` `å¼€æ”¾åŸŸé©¾é©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¿½è§†ä¹˜å®¢æƒ…æ„ŸçŠ¶æ€ï¼Œå½±å“èˆ’é€‚åº¦å’ŒæŽ¥å—åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è¿žç»­VADæƒ…æ„Ÿæ¨¡åž‹å’ŒåŒé€šè·¯ç©ºé—´æŽ¨ç†æ¨¡å—ï¼Œå¢žå¼ºè¯­ä¹‰ç†è§£å’Œç©ºé—´è®¤çŸ¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šæå‡è§†è§‰å®šä½å’Œè·¯å¾„è§„åˆ’ï¼Œæƒ…æ„Ÿä¼°è®¡è¾¾åˆ°SOTAç›¸å…³æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.

