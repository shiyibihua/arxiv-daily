---
layout: default
title: SAM3-I: Segment Anything with Instructions
---

# SAM3-I: Segment Anything with Instructions

**arXiv**: [2512.04585v1](https://arxiv.org/abs/2512.04585) | [PDF](https://arxiv.org/pdf/2512.04585.pdf)

**ä½œè€…**: Jingjing Li, Yue Feng, Yuchen Guo, Jincai Huang, Yongri Piao, Qi Bi, Miao Zhang, Xiaoqi Zhao, Qiang Chen, Shihao Zou, Wei Ji, Huchuan Lu, Li Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAM3-Iæ¡†æž¶ï¼Œé€šè¿‡æŒ‡ä»¤æ„ŸçŸ¥çº§è”é€‚é…æœºåˆ¶ï¼Œä½¿SAM3èƒ½ç›´æŽ¥éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œåˆ†å‰²ã€‚**

**å…³é”®è¯**: `æŒ‡ä»¤æ„ŸçŸ¥åˆ†å‰²` `çº§è”é€‚é…æœºåˆ¶` `å¼€æ”¾è¯æ±‡åˆ†å‰²` `è‡ªç„¶è¯­è¨€æŒ‡ä»¤` `è§†è§‰è¯­è¨€å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSAM3ä¾èµ–å¤–éƒ¨ä»£ç†å°†å¤æ‚æŒ‡ä»¤è½¬æ¢ä¸ºåè¯çŸ­è¯­ï¼Œå¯¼è‡´åˆ†å‰²ç²¾åº¦ä¸è¶³ï¼Œæ— æ³•ç²¾ç¡®è¡¨ç¤ºç‰¹å®šå®žä¾‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æŒ‡ä»¤æ„ŸçŸ¥çº§è”é€‚é…æœºåˆ¶ï¼Œé€æ­¥å¯¹é½æŒ‡ä»¤è¯­ä¹‰ä¸Žè§†è§‰è¯­è¨€è¡¨ç¤ºï¼Œæ”¯æŒç›´æŽ¥æŒ‡ä»¤è·Ÿéšåˆ†å‰²ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºSAM3-Iåœ¨ä¿æŒæ¦‚å¿µé©±åŠ¨èƒ½åŠ›çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ‰©å±•è‡³éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæ€§èƒ½å¸å¼•äººã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.

