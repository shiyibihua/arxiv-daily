---
layout: default
title: PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement
---

# PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement

**arXiv**: [2512.04532v1](https://arxiv.org/abs/2512.04532) | [PDF](https://arxiv.org/pdf/2512.04532.pdf)

**ä½œè€…**: Yu-Wei Zhan, Xin Wang, Hong Chen, Tongtong Feng, Wei Feng, Ren Wang, Guangyao Li, Qing Li, Wenwu Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPhyVLLMæ¡†æž¶ï¼Œé€šè¿‡è§£è€¦è¿åŠ¨ä¸Žå¤–è§‚å¹¶é›†æˆç‰©ç†åŠ¨æ€å»ºæ¨¡ï¼Œä»¥å¢žå¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡åž‹çš„ç‰©ç†æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡åž‹` `ç‰©ç†åŠ¨æ€å»ºæ¨¡` `è¿åŠ¨-å¤–è§‚è§£è€¦` `ç¥žç»å¸¸å¾®åˆ†æ–¹ç¨‹` `è‡ªç›‘ç£å­¦ä¹ ` `è§†é¢‘ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡åž‹ä¾èµ–å¤–è§‚åŒ¹é…ï¼Œç¼ºä¹å¯¹ç‰©ç†åŠ¨æ€çš„æ·±å±‚ç†è§£ï¼Œå¯¼è‡´åœ¨ç‰©ç†æŽ¨ç†åœºæ™¯ä¸­è¡¨çŽ°ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨è§£è€¦å¤–è§‚ä¸Žè¿åŠ¨ï¼Œé›†æˆç¥žç»å¸¸å¾®åˆ†æ–¹ç¨‹æ¨¡å—å»ºæ¨¡è¿žç»­ç‰©ç†åŠ¨æ€ï¼Œå¹¶ä»¥è‡ªç›‘ç£æ–¹å¼è®­ç»ƒé¿å…ä¾èµ–ç‰©ç†æ ‡æ³¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç‰©ç†æŽ¨ç†å’Œé€šç”¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰å…ˆè¿›æ¨¡åž‹ï¼ŒéªŒè¯äº†æ˜¾å¼ç‰©ç†å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.

