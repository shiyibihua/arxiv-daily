---
layout: default
title: LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics
---

# LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics

**arXiv**: [2512.04957v1](https://arxiv.org/abs/2512.04957) | [PDF](https://arxiv.org/pdf/2512.04957.pdf)

**ä½œè€…**: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€ä½“è£åˆ†ç±»æ•°æ®é›†ä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡åž‹å¯¹æ·±å±‚è¯­è¨€ç‰¹å¾çš„æ•èŽ·èƒ½åŠ›**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `ä½“è£åˆ†ç±»` `å¥æ³•åˆ†æž` `éšå–»æ£€æµ‹` `è¯­éŸ³ç‰¹å¾` `å¤šè¯­è¨€æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹æ˜¯å¦èƒ½ä»ŽåŽŸå§‹æ–‡æœ¬ä¸­æœ‰æ•ˆå­¦ä¹ å¥æ³•ã€è¯­éŸ³å’ŒéŸµå¾‹ç­‰æ·±å±‚è¯­è¨€ç‰¹å¾
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¤ç™»å ¡è®¡åˆ’æž„å»ºå¤šè¯­è¨€ä½“è£åˆ†ç±»æ•°æ®é›†ï¼Œå¹¶èžå…¥å¥æ³•æ ‘ã€éšå–»è®¡æ•°å’Œè¯­éŸ³æŒ‡æ ‡ç­‰æ˜¾å¼ç‰¹å¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¡¨æ˜Žå¤§è¯­è¨€æ¨¡åž‹å¯ä»ŽåŽŸå§‹æ–‡æœ¬æˆ–æ˜¾å¼ç‰¹å¾å­¦ä¹ è¯­è¨€ç»“æž„ï¼Œä½†ä¸åŒç‰¹å¾å¯¹ä»»åŠ¡è´¡çŒ®ä¸å‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.

