---
layout: default
title: The Universal Weight Subspace Hypothesis
---

# The Universal Weight Subspace Hypothesis

**arXiv**: [2512.05117v1](https://arxiv.org/abs/2512.05117) | [PDF](https://arxiv.org/pdf/2512.05117.pdf)

**ä½œè€…**: Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æƒé‡å­ç©ºé—´å‡è®¾ï¼Œæ­ç¤ºè·¨ä»»åŠ¡ç¥žç»ç½‘ç»œå…±äº«ä½Žç»´å‚æ•°å­ç©ºé—´**

**å…³é”®è¯**: `æƒé‡å­ç©ºé—´` `è°±åˆ†æž` `å¤šä»»åŠ¡å­¦ä¹ ` `æ¨¡åž‹åˆå¹¶` `ç¥žç»ç½‘ç»œç»“æž„` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç©¶æ·±åº¦ç¥žç»ç½‘ç»œåœ¨ä¸åŒä»»åŠ¡è®­ç»ƒåŽæ˜¯å¦å…·æœ‰å†…åœ¨çš„ç›¸ä¼¼å‚æ•°ç»“æž„
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è°±åˆ†è§£æŠ€æœ¯åˆ†æžè¶…1100ä¸ªæ¨¡åž‹çš„æƒé‡çŸ©é˜µï¼Œè¯†åˆ«å…±äº«çš„ç¨€ç–å­ç©ºé—´
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°æ¨¡åž‹åœ¨å°‘æ•°ä¸»æ–¹å‘ä¸Šæ•èŽ·å¤§éƒ¨åˆ†æ–¹å·®ï¼Œæ”¯æŒå­ç©ºé—´é€šç”¨æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.

