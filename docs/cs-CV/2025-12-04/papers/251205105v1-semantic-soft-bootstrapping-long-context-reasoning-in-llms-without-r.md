---
layout: default
title: Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning
---

# Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning

**arXiv**: [2512.05105v1](https://arxiv.org/abs/2512.05105) | [PDF](https://arxiv.org/pdf/2512.05105.pdf)

**ä½œè€…**: Purbesh Mitra, Sennur Ulukus

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰è½¯è‡ªä¸¾æ–¹æ³•ï¼Œä»¥è‡ªè’¸é¦æŠ€æœ¯æå‡å¤§è¯­è¨€æ¨¡åž‹åœ¨æ•°å­¦æŽ¨ç†ä»»åŠ¡ä¸­çš„é•¿ä¸Šä¸‹æ–‡æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡æŽ¨ç†` `è‡ªè’¸é¦è®­ç»ƒ` `æ•°å­¦æŽ¨ç†` `å¤§è¯­è¨€æ¨¡åž‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ åœ¨é•¿ä¸Šä¸‹æ–‡æŽ¨ç†ä¸­é¢ä¸´å¥–åŠ±ç¨€ç–å’Œæ ·æœ¬æ•ˆçŽ‡ä½Žç­‰ç“¶é¢ˆï¼Œå¯¼è‡´è®­ç»ƒèµ„æºéœ€æ±‚é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åŒä¸€æ¨¡åž‹ç”Ÿæˆæ­£ç¡®ä¸Žé”™è¯¯å“åº”ï¼Œæž„å»ºæ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒå¯¹ï¼Œå®žçŽ°æ— äººå·¥å¹²é¢„çš„è‡ªè’¸é¦è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨GSM8Kæ•°æ®é›†ä¸Šå¾®è°ƒQwen2.5-3B-Instructï¼Œåœ¨MATH500å’ŒAIME2024åŸºå‡†ä¸Šå‡†ç¡®çŽ‡åˆ†åˆ«æå‡10.6%å’Œ10%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.

