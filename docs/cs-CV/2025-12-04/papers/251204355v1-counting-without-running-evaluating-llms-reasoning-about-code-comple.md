---
layout: default
title: Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity
---

# Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity

**arXiv**: [2512.04355v1](https://arxiv.org/abs/2512.04355) | [PDF](https://arxiv.org/pdf/2512.04355.pdf)

**ä½œè€…**: Gregory Bolet, Giorgis Georgakoudis, Konstantinos Parasyris, Harshitha Menon, Niranjan Hasabnis, Kirk W. Cameron, Gal Oren

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºgpuFLOPBenchåŸºå‡†ä»¥è¯„ä¼°LLMsåœ¨é¢„æµ‹GPUä»£ç FLOPè®¡æ•°ä¸­çš„æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `GPUæ€§èƒ½åˆ†æž` `å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°` `ä»£ç å¤æ‚åº¦æŽ¨ç†` `FLOPè®¡æ•°é¢„æµ‹` `CUDAå†…æ ¸åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰LLMsç¼ºä¹å‰çž»æ€§æŽ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥é¢„æµ‹GPUä»£ç æ€§èƒ½ç“¶é¢ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«577ä¸ªCUDAå†…æ ¸çš„åŸºå‡†ï¼Œæ ‡æ³¨çœŸå®žFLOPè®¡æ•°å’Œå…«ç§æ‰§è¡Œå±žæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœ€æ–°LLMsåœ¨ç®€å•å†…æ ¸ä¸Šè¡¨çŽ°å®Œç¾Žï¼Œä½†åœ¨éšå«FLOPåœºæ™¯ä¸‹è¯¯å·®å·¨å¤§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

