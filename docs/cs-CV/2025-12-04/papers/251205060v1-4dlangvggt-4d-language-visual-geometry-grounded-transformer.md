---
layout: default
title: 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer
---

# 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.05060" target="_blank" class="toolbar-btn">arXiv: 2512.05060v1</a>
    <a href="https://arxiv.org/pdf/2512.05060.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.05060v1" 
            onclick="toggleFavorite(this, '2512.05060v1', '4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xianfeng Wu, Yajing Bai, Minghan Li, Xianzu Wu, Xueqi Zhao, Zhongyuan Lai, Wenyu Liu, Xinggang Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-04

**Â§áÊ≥®**: Code: https://github.com/hustvl/4DLangVGGT, Webpage: https://hustvl.github.io/4DLangVGGT

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/hustvl/4DLangVGGT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫4DLangVGGTÔºåÁî®‰∫éÈ´òÊïà‰∏îÂèØÊ≥õÂåñÁöÑ4DËØ≠Ë®Ä-ËßÜËßâÂá†‰ΩïÂØπÈΩê**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `4DÂú∫ÊôØÁêÜËß£` `ËØ≠Ë®ÄÂØπÈΩê` `ËßÜËßâÂá†‰Ωï` `Transformer` `Âä®ÊÄÅÂú∫ÊôØ` `Á•ûÁªèËæêÂ∞ÑÂú∫` `ÂºÄÊîæËØçÊ±á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ4DËØ≠‰πâÂú∫ÊûÑÂª∫ÊñπÊ≥ï‰æùËµñÈÄêÂú∫ÊôØ‰ºòÂåñÔºåÊ≥õÂåñÊÄßÂ∑ÆÔºåÈöæ‰ª•Êâ©Â±ïÂà∞ÁúüÂÆûÂú∫ÊôØ„ÄÇ
2. ÊèêÂá∫4DLangVGGTÔºåÈÄöËøáTransformerËÅîÂêàÂ≠¶‰π†Âá†‰ΩïÊÑüÁü•ÂíåËØ≠Ë®ÄÂØπÈΩêÔºåÊó†ÈúÄÈÄêÂú∫ÊôØ‰ºòÂåñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºå4DLangVGGTÂú®HyperNeRFÂíåNeu3DÊï∞ÊçÆÈõÜ‰∏äÂùáËææÂà∞SOTAÊÄßËÉΩÔºåÊ≥õÂåñËÉΩÂäõÂº∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊûÑÂª∫4DËØ≠Ë®ÄÂú∫ÂØπ‰∫éÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÅÂ¢ûÂº∫/ËôöÊãüÁé∞ÂÆûÂíå4DÂú∫ÊôØÁêÜËß£Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÊèê‰æõ‰∫ÜÂä®ÊÄÅÁéØÂ¢ÉÁöÑ‰∏∞ÂØåËØ≠‰πâË°®Á§∫ÔºåÂπ∂ÊîØÊåÅÂ§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂºÄÊîæËØçÊ±áÊü•ËØ¢„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑ4DËØ≠‰πâÂú∫ÊûÑÂª∫ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÁâπÂÆöÂú∫ÊôØÁöÑÈ´òÊñØÊ∫ÖÂ∞ÑÔºåËøôÈúÄË¶ÅÈÄêÂú∫ÊôØ‰ºòÂåñÔºåÊ≥õÂåñËÉΩÂäõÊúâÈôêÔºåÂπ∂‰∏îÈöæ‰ª•Êâ©Â±ïÂà∞ÂÆûÈôÖÂ∫îÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü4DLangVGGTÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Âü∫‰∫éTransformerÁöÑÂâçÈ¶àÁªü‰∏ÄÊ°ÜÊû∂ÔºåÁî®‰∫é4DËØ≠Ë®ÄÂØπÈΩêÔºåÂÆÉÂú®Âçï‰∏™Êû∂ÊûÑ‰∏≠ËÅîÂêàÈõÜÊàê‰∫ÜÂá†‰ΩïÊÑüÁü•ÂíåËØ≠Ë®ÄÂØπÈΩê„ÄÇ4DLangVGGTÊúâ‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂Ôºö4DËßÜËßâÂá†‰ΩïTransformerÔºåStreamVGGTÔºåÂÆÉÊçïËé∑Âä®ÊÄÅÂú∫ÊôØÁöÑÊó∂Á©∫Âá†‰ΩïË°®Á§∫Ôºõ‰ª•ÂèäËØ≠‰πâÊ°•Êé•Ëß£Á†ÅÂô®ÔºàSBDÔºâÔºåÂÆÉÂ∞ÜÂá†‰ΩïÊÑüÁü•ÁâπÂæÅÊäïÂΩ±Âà∞ËØ≠Ë®ÄÂØπÈΩêÁöÑËØ≠‰πâÁ©∫Èó¥‰∏≠Ôºå‰ªéËÄåÂ¢ûÂº∫ËØ≠‰πâÂèØËß£ÈáäÊÄßÔºåÂêåÊó∂‰øùÊåÅÁªìÊûÑ‰øùÁúüÂ∫¶„ÄÇ‰∏é‰æùËµñ‰∫éÊòÇË¥µÁöÑÈÄêÂú∫ÊôØ‰ºòÂåñÊñπÊ≥ï‰∏çÂêåÔºå4DLangVGGTÂèØ‰ª•Âú®Â§ö‰∏™Âä®ÊÄÅÂú∫ÊôØ‰∏≠ËÅîÂêàËÆ≠ÁªÉÔºåÂπ∂Âú®Êé®ÁêÜÊúüÈó¥Áõ¥Êé•Â∫îÁî®Ôºå‰ªéËÄåÂÆûÁé∞ÈÉ®ÁΩ≤ÊïàÁéáÂíåÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçËÆæËÆ°ÊòæËëóÊèêÈ´ò‰∫ÜÂ§ßËßÑÊ®°ÈÉ®ÁΩ≤ÁöÑÂÆûÁî®ÊÄßÔºåÂπ∂‰∏∫ÂºÄÊîæËØçÊ±á4DÂú∫ÊôØÁêÜËß£Âª∫Á´ã‰∫Ü‰∏ÄÁßçÊñ∞ËåÉÂºè„ÄÇÂú®HyperNeRFÂíåNeu3DÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏ç‰ªÖËÉΩÊúâÊïàÂú∞Ê≥õÂåñÔºåËÄå‰∏îËøòËÉΩËææÂà∞ÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂú®ÈÄêÂú∫ÊôØËÆ≠ÁªÉ‰∏ãÂÆûÁé∞‰∫ÜÈ´òËææ2%ÁöÑÂ¢ûÁõäÔºåÂú®Â§öÂú∫ÊôØËÆ≠ÁªÉ‰∏ãÂÆûÁé∞‰∫Ü1%ÁöÑÊîπËøõ„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂ∑≤Âú®https://github.com/hustvl/4DLangVGGTÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ4DÂú∫ÊôØÁêÜËß£ÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éÁ•ûÁªèËæêÂ∞ÑÂú∫ÁöÑÊñπÊ≥ïÔºåÈÄöÂ∏∏ÈúÄË¶ÅÈíàÂØπÊØè‰∏™ÁâπÂÆöÂú∫ÊôØËøõË°å‰ºòÂåñÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈöæ‰ª•Â∫îÁî®‰∫éÂ§ßËßÑÊ®°Âä®ÊÄÅÂú∫ÊôØ„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÊñπÊ≥ïÂú®Â§ÑÁêÜÂºÄÊîæËØçÊ±áÁöÑËØ≠Ë®ÄÊü•ËØ¢Êó∂ÔºåËØ≠‰πâÁêÜËß£ËÉΩÂäõÊúâÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**Ôºö4DLangVGGTÁöÑÊ†∏ÂøÉÂú®‰∫éÊûÑÂª∫‰∏Ä‰∏™ÂèØÊ≥õÂåñÁöÑ„ÄÅÁ´ØÂà∞Á´ØÁöÑ4DËØ≠Ë®Ä-ËßÜËßâÂá†‰ΩïÂØπÈΩêÊ°ÜÊû∂„ÄÇÈÄöËøáTransformerÊû∂ÊûÑÔºåÂ∞ÜÂä®ÊÄÅÂú∫ÊôØÁöÑÂá†‰Ωï‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØËøõË°åËÅîÂêàÁºñÁ†ÅÂíåËß£Á†ÅÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑËØ≠‰πâÁêÜËß£ÂíåÂú∫ÊôØÈáçÂª∫ÔºåÈÅøÂÖç‰∫ÜÈÄêÂú∫ÊôØ‰ºòÂåñÂ∏¶Êù•ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**Ôºö4DLangVGGT‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöStreamVGGTÔºà4DËßÜËßâÂá†‰ΩïTransformerÔºâÂíåSemantic Bridging Decoder (SBD)„ÄÇStreamVGGTË¥üË¥£ÊçïËé∑Âä®ÊÄÅÂú∫ÊôØÁöÑÊó∂Á©∫Âá†‰ΩïË°®Á§∫ÔºåÂ∞Ü4DÂú∫ÊôØ‰ø°ÊÅØÁºñÁ†ÅÊàêÂá†‰ΩïÁâπÂæÅ„ÄÇSBDÂàôÂ∞ÜËøô‰∫õÂá†‰ΩïÁâπÂæÅÊäïÂΩ±Âà∞ËØ≠Ë®ÄÂØπÈΩêÁöÑËØ≠‰πâÁ©∫Èó¥Ôºå‰ªéËÄåÂÆûÁé∞Âá†‰Ωï‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØÁöÑËûçÂêà„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈÄöËøáËÅîÂêàËÆ≠ÁªÉÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑ4DËØ≠Ë®ÄÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö4DLangVGGTÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Áªü‰∏ÄÁöÑTransformerÊû∂ÊûÑÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂá†‰Ωï‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØÔºåÂπ∂ÂÆûÁé∞Ë∑®Âú∫ÊôØÁöÑÊ≥õÂåñ„ÄÇ‰∏é‰ª•ÂæÄ‰æùËµñ‰∫éÁâπÂÆöÂú∫ÊôØ‰ºòÂåñÁöÑÊñπÊ≥ï‰∏çÂêåÔºå4DLangVGGTÂèØ‰ª•Âú®Â§ö‰∏™Âú∫ÊôØ‰∏äËøõË°åËÅîÂêàËÆ≠ÁªÉÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥ÈÄöÁî®ÁöÑÂú∫ÊôØË°®Á§∫„ÄÇÊ≠§Â§ñÔºåSBDÊ®°ÂùóÁöÑËÆæËÆ°ÊúâÊïàÂú∞Â∞ÜÂá†‰ΩïÁâπÂæÅÊò†Â∞ÑÂà∞ËØ≠‰πâÁ©∫Èó¥ÔºåÂ¢ûÂº∫‰∫ÜËØ≠‰πâÂèØËß£ÈáäÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöStreamVGGTÈááÁî®TransformerÁªìÊûÑÔºåËæìÂÖ•ÊòØ4DÁÇπ‰∫ëÊï∞ÊçÆÔºåÈÄöËøáËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Â≠¶‰π†Êó∂Á©∫Âá†‰ΩïÁâπÂæÅ„ÄÇSBD‰ΩøÁî®‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂ∞ÜStreamVGGTËæìÂá∫ÁöÑÂá†‰ΩïÁâπÂæÅ‰∏éËØ≠Ë®ÄÂµåÂÖ•ËøõË°åÂØπÈΩê„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Âá†‰ΩïÈáçÂª∫ÊçüÂ§±ÂíåËØ≠Ë®ÄÂØπÈΩêÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊï¥‰∏™ÁΩëÁªú„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

4DLangVGGTÂú®HyperNeRFÂíåNeu3DÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇÂú®per-sceneËÆ≠ÁªÉ‰∏ãÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ2%ÔºõÂú®multi-sceneËÆ≠ÁªÉ‰∏ãÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ1%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºå4DLangVGGTÂÖ∑ÊúâÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ´òÊïàÁöÑÂú∫ÊôØÁêÜËß£ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

4DLangVGGTÂú®ÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÅÂ¢ûÂº∫/ËôöÊãüÁé∞ÂÆû„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÊûÑÂª∫Âä®ÊÄÅÁéØÂ¢ÉÁöÑËØ≠‰πâÂú∞ÂõæÔºåÊîØÊåÅÊú∫Âô®‰∫∫ËøõË°åÂ§çÊùÇÁöÑÂú∫ÊôØÁêÜËß£Âíå‰∫§‰∫íÔºåÂπ∂‰∏∫AR/VRÂ∫îÁî®Êèê‰æõÊõ¥ÈÄºÁúüÁöÑÊ≤âÊµ∏Âºè‰ΩìÈ™å„ÄÇËØ•Á†îÁ©∂‰∏∫ÂºÄÊîæËØçÊ±á4DÂú∫ÊôØÁêÜËß£ÂºÄËæü‰∫ÜÊñ∞ÁöÑÊñπÂêë„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT

