---
layout: default
title: Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty
---

# Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty

**arXiv**: [2512.04918v1](https://arxiv.org/abs/2512.04918) | [PDF](https://arxiv.org/pdf/2512.04918.pdf)

**ä½œè€…**: Kailiang Liu, Ying Chen, Ralf BorndÃ¶rfer, Thorsten Koch

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽä¸ç¡®å®šçŽ¯å¢ƒä¸‹æ‰‹æœ¯å®¤æ—¥å†…è°ƒåº¦ä¼˜åŒ–ã€‚**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ‰‹æœ¯å®¤è°ƒåº¦` `ä¸ç¡®å®šæ€§ä¼˜åŒ–` `é›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡Œ` `PPOç®—æ³•` `åºåˆ—åˆ†é…åè®®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹æœ¯å®¤æ—¥å†…è°ƒåº¦éœ€å¹³è¡¡æ‹©æœŸæ‰‹æœ¯ã€ç´§æ€¥éœ€æ±‚ã€å»¶è¿Ÿç­‰å¤šç›®æ ‡ä¸ç¡®å®šæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨é›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡Œçš„MARLï¼ŒåŸºäºŽPPOå…±äº«ç­–ç•¥ï¼Œç»“åˆåºåˆ—åˆ†é…åè®®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹ŸåŒ»é™¢çŽ¯å¢ƒä¸­ï¼Œç­–ç•¥ä¼˜äºŽå…­ç§å¯å‘å¼æ–¹æ³•ï¼Œå¹¶é‡åŒ–äº†ä¸Žæœ€ä¼˜è§£çš„å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.

