---
layout: default
title: STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models
---

# STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models

**arXiv**: [2512.05107v1](https://arxiv.org/abs/2512.05107) | [PDF](https://arxiv.org/pdf/2512.05107.pdf)

**ä½œè€…**: Feng Xu, Guangyao Zhai, Xin Kong, Tingzhong Fu, Daniel F. N. Gordon, Xueli An, Benjamin Busam

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTARE-VLAï¼Œé€šè¿‡é˜¶æ®µæ„ŸçŸ¥å¼ºåŒ–å¾®è°ƒæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„é•¿æ—¶ç¨‹åŠ¨ä½œå‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `é˜¶æ®µåˆ†è§£` `é•¿æ—¶ç¨‹åŠ¨ä½œ` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å°†é•¿æ—¶ç¨‹åŠ¨ä½œè§†ä¸ºè¯­è¨€åºåˆ—ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…ç²—ç³™å’Œè®­ç»ƒä¸ç¨³å®šã€‚
2. STAREæ¨¡å—å°†åŠ¨ä½œè½¨è¿¹åˆ†è§£ä¸ºè¯­ä¹‰é˜¶æ®µï¼Œæä¾›å¯†é›†ã€å¯è§£é‡Šçš„é˜¶æ®µå¯¹é½å¼ºåŒ–ä¿¡å·ã€‚
3. åœ¨SimplerEnvå’ŒManiSkill3ä¸Šå®žçŽ°æœ€é«˜æˆåŠŸçŽ‡ï¼Œåˆ†åˆ«è¾¾98.0%å’Œ96.4%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.

