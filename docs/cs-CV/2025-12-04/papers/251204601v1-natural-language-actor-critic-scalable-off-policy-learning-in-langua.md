---
layout: default
title: Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space
---

# Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space

**arXiv**: [2512.04601v1](https://arxiv.org/abs/2512.04601) | [PDF](https://arxiv.org/pdf/2512.04601.pdf)

**ä½œè€…**: Joey Hong, Kang Liu, Zhan Ling, Jiecao Chen, Sergey Levine

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç„¶è¯­è¨€æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ï¼Œä»¥æå‡å¤§è¯­è¨€æ¨¡åž‹ä»£ç†åœ¨ç¨€ç–å¥–åŠ±é•¿æ—¶ä»»åŠ¡ä¸­çš„è®­ç»ƒæ•ˆçŽ‡ä¸Žç¨³å®šæ€§ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹ä»£ç†` `æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•` `è‡ªç„¶è¯­è¨€åŠ¨ä½œç©ºé—´` `ç¦»ç­–ç•¥å­¦ä¹ ` `ç¨€ç–å¥–åŠ±ä»»åŠ¡` `è®­ç»ƒç¨³å®šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿æ—¶ä»»åŠ¡ä¸­ç¨€ç–å¥–åŠ±å¯¼è‡´ç­–ç•¥æ¢¯åº¦æ–¹æ³•è®­ç»ƒä¸ç¨³å®šã€æ ·æœ¬å¤æ‚åº¦é«˜ï¼Œä¸”è‡ªç„¶è¯­è¨€åŠ¨ä½œç©ºé—´æŽ¢ç´¢å›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡åž‹è¯„è®ºå®¶è¾“å‡ºè‡ªç„¶è¯­è¨€è€Œéžæ ‡é‡å€¼ï¼Œæä¾›æ›´ä¸°å¯Œçš„è®­ç»ƒä¿¡å·ï¼Œæ”¯æŒç¦»ç­–ç•¥è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æŽ¨ç†ã€ç½‘é¡µæµè§ˆå’Œå·¥å…·ä½¿ç”¨ç­‰ä»»åŠ¡ä¸­ï¼ŒNLACå±•çŽ°å‡ºä¼˜äºŽçŽ°æœ‰æ–¹æ³•çš„æ½œåŠ›ï¼Œæä¾›æ›´å¯æ‰©å±•å’Œç¨³å®šçš„è®­ç»ƒèŒƒå¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.

