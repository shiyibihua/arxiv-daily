---
layout: default
title: Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning
---

# Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning

**arXiv**: [2512.04958v1](https://arxiv.org/abs/2512.04958) | [PDF](https://arxiv.org/pdf/2512.04958.pdf)

**ä½œè€…**: Roberto Cipollone, Luca Iocchi, Matteo Leonetti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯å®žçŽ°çš„æŠ½è±¡ä»¥è§£å†³åˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¸­æŠ½è±¡è¡¨è¾¾åŠ›ä¸è¶³å’Œç¼ºä¹æ•ˆçŽ‡ä¿è¯çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¯å®žçŽ°çš„æŠ½è±¡` `é€‰é¡¹ç»„åˆ` `è¿‘ä¼¼æœ€ä¼˜ç­–ç•¥` `RARLç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¸­çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æŠ½è±¡è¡¨è¾¾åŠ›æœ‰é™æˆ–ç¼ºä¹å½¢å¼åŒ–æ•ˆçŽ‡ä¿è¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå®šä¹‰å¯å®žçŽ°çš„æŠ½è±¡å…³ç³»ï¼Œé€šè¿‡é€‰é¡¹ç»„åˆå°†æŠ½è±¡ç­–ç•¥è½¬æ¢ä¸ºè¿‘ä¼¼æœ€ä¼˜çš„ä½Žå±‚ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæå‡ºRARLç®—æ³•ï¼Œå…·æœ‰æ¦‚çŽ‡è¿‘ä¼¼æ­£ç¡®æ€§ã€å¤šé¡¹å¼æ ·æœ¬æ”¶æ•›æ€§å’Œå¯¹æŠ½è±¡ä¸ç²¾ç¡®çš„é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.

