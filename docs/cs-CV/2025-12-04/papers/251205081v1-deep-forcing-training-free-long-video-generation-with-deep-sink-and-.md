---
layout: default
title: Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression
---

# Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression

**arXiv**: [2512.05081v1](https://arxiv.org/abs/2512.05081) | [PDF](https://arxiv.org/pdf/2512.05081.pdf)

**ä½œè€…**: Jung Yi, Wooseok Jang, Paul Hyunbin Cho, Jisu Nam, Heeji Yoon, Seungryong Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeep Forcingï¼Œé€šè¿‡æ·±åº¦æ±‡ä¸Žå‚ä¸Žå¼åŽ‹ç¼©å®žçŽ°æ— éœ€è®­ç»ƒçš„é•¿è§†é¢‘ç”Ÿæˆï¼Œè§£å†³è‡ªå›žå½’è§†é¢‘æ‰©æ•£ä¸­çš„æ—¶é—´é‡å¤ã€æ¼‚ç§»å’Œè¿åŠ¨å‡é€Ÿé—®é¢˜ã€‚**

**å…³é”®è¯**: `é•¿è§†é¢‘ç”Ÿæˆ` `è‡ªå›žå½’è§†é¢‘æ‰©æ•£` `æ³¨æ„åŠ›æœºåˆ¶` `KVç¼“å­˜ç®¡ç†` `è®­ç»ƒå…è´¹æ–¹æ³•` `å®žæ—¶ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå›žå½’è§†é¢‘æ‰©æ•£åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­å­˜åœ¨æ—¶é—´é‡å¤ã€æ¼‚ç§»å’Œè¿åŠ¨å‡é€Ÿï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚StreamingLLMå¼æ³¨æ„åŠ›æ±‡å¯¼è‡´ä¿çœŸåº¦ä¸‹é™å’Œè¿åŠ¨åœæ»žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šDeep ForcingåŒ…å«æ·±åº¦æ±‡ï¼ˆåˆ†é…æ»‘åŠ¨çª—å£ä¸€åŠç»™æŒä¹…æ±‡ä»¤ç‰Œå¹¶é‡æ–°å¯¹é½æ—¶é—´RoPEç›¸ä½ï¼‰å’Œå‚ä¸Žå¼åŽ‹ç¼©ï¼ˆåŸºäºŽé‡è¦æ€§ä¿®å‰ªKVç¼“å­˜ï¼Œä¿ç•™æ´»è·ƒå‚ä¸Žä»¤ç‰Œï¼‰ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žçŽ°è¶…è¿‡12å€å¤–æŽ¨ï¼ˆå¦‚5ç§’è®­ç»ƒç”Ÿæˆ60ç§’ä»¥ä¸Šè§†é¢‘ï¼‰ï¼Œå›¾åƒè´¨é‡ä¼˜äºŽLongLiveï¼Œç¾Žå­¦è´¨é‡ä¼˜äºŽRollingForcingï¼Œä¿æŒå®žæ—¶ç”Ÿæˆï¼ŒåŠ¨æ€ç¨‹åº¦æ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.

