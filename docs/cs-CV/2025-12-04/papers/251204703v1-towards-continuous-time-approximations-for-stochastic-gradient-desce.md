---
layout: default
title: Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement
---

# Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement

**arXiv**: [2512.04703v1](https://arxiv.org/abs/2512.04703) | [PDF](https://arxiv.org/pdf/2512.04703.pdf)

**ä½œè€…**: Stefan Perko

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽYoungå¾®åˆ†æ–¹ç¨‹çš„è¿žç»­æ—¶é—´è¿‘ä¼¼ï¼Œä»¥åˆ†æžæ— æ”¾å›žéšæœºæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›æ€§ã€‚**

**å…³é”®è¯**: `éšæœºæ¢¯åº¦ä¸‹é™` `è¿žç»­æ—¶é—´è¿‘ä¼¼` `Youngå¾®åˆ†æ–¹ç¨‹` `æ”¶æ•›åˆ†æž` `æœºå™¨å­¦ä¹ ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ— æ”¾å›žéšæœºæ¢¯åº¦ä¸‹é™çš„ç†è®ºåˆ†æžä¸è¶³ï¼Œç›¸æ¯”æœ‰æ”¾å›žå’Œå•æ¬¡éåŽ†æ–¹æ³•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨'åˆ†æ—¶æ®µå¸ƒæœ—è¿åŠ¨'é©±åŠ¨çš„Youngå¾®åˆ†æ–¹ç¨‹ï¼Œæž„å»ºå¸¦åŠ æ€§å™ªå£°çš„è¿žç»­æ—¶é—´è¿‘ä¼¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯æ˜Žå¼ºå‡¸ç›®æ ‡ä¸‹è¿žç»­æ—¶é—´è¿‘ä¼¼å‡ ä¹Žå¿…ç„¶æ”¶æ•›ï¼Œå¹¶ç»™å‡ºæ”¶æ•›é€ŸçŽ‡ä¸Šç•Œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their "with replacement" and "one-pass" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an "epoched Brownian motion". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form $u_t = \frac{1}{(1+t)^Î²}, Î²\in (0,1)$. Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.

