---
layout: default
title: Towards Cross-View Point Correspondence in Vision-Language Models
---

# Towards Cross-View Point Correspondence in Vision-Language Models

**arXiv**: [2512.04686v1](https://arxiv.org/abs/2512.04686) | [PDF](https://arxiv.org/pdf/2512.04686.pdf)

**ä½œè€…**: Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, Zhiyuan Ma, Daniel Dajun Zeng, Xiaolong Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨è§†å›¾ç‚¹å¯¹åº”ä»»åŠ¡ä¸ŽåŸºå‡†ï¼Œä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç©ºé—´ç†è§£ä¸­çš„ç²¾ç¡®å¯¹åº”èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è·¨è§†å›¾å¯¹åº”` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç‚¹çº§å¯¹åº”` `ç©ºé—´ç†è§£` `åŸºå‡†è¯„ä¼°` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è·¨è§†å›¾ç‚¹å¯¹åº”ä»»åŠ¡ä¸Šè¡¨çŽ°ä¸ä½³ï¼Œå½±å“ç²¾ç¡®äº¤äº’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåˆ†å±‚åŸºå‡†CrossPoint-Benchå’Œæ•°æ®é›†CrossPoint-378Kï¼Œå¹¶è®­ç»ƒæ¨¡åž‹CroPondã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šCroPondåœ¨åŸºå‡†ä¸Šè¶…è¶ŠGemini-2.5-Pro 39.7%ï¼Œç¼©å°ä¸Žäººç±»å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.

