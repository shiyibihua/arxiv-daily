---
layout: default
title: Refa√ßade: Editing Object with Given Reference Texture
---

# Refa√ßade: Editing Object with Given Reference Texture

**arXiv**: [2512.04534v1](https://arxiv.org/abs/2512.04534) | [PDF](https://arxiv.org/pdf/2512.04534.pdf)

**‰ΩúËÄÖ**: Youze Huang, Penghui Ruan, Bojia Zi, Xianbiao Qi, Jianan Wang, Rong Xiao

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Refa√ßadeÊñπÊ≥ïÔºåÈÄöËøáÁ∫πÁêÜÁßªÈô§ÂíåÊãºÂõæÁΩÆÊç¢ÂÆûÁé∞ÂõæÂÉèÂíåËßÜÈ¢ë‰∏≠ÂØπË±°ÁöÑÁ≤æÁ°ÆÁ∫πÁêÜÁºñËæë„ÄÇ**

**ÂÖ≥ÈîÆËØç**: `ÂØπË±°ÈáçÁ∫πÁêÜ` `Êâ©Êï£Ê®°Âûã` `Á∫πÁêÜÁßªÈô§` `ÊãºÂõæÁΩÆÊç¢` `ËßÜÈ¢ëÁºñËæë` `ÂèØÊéßÁºñËæë`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÁé∞ÊúâÊñπÊ≥ïÂú®ÂØπË±°ÈáçÁ∫πÁêÜ‰ªªÂä°‰∏≠Âõ†ÂèÇËÄÉÂõæÂÉèÁªìÊûÑÂπ≤Êâ∞ÂíåÁ∫πÁêÜ-ÁªìÊûÑËß£ËÄ¶‰∏çË∂≥ÂØºËá¥ÂèØÊéßÊÄßÂèóÈôê„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºö‰ΩøÁî®Á∫πÁêÜÁßªÈô§Âô®‰øùÁïôÊ∫êÂá†‰ΩïÂíåËøêÂä®ÔºåÁªìÂêàÊãºÂõæÁΩÆÊç¢Á†¥ÂùèÂèÇËÄÉÂÖ®Â±ÄÂ∏ÉÂ±Ä‰ª•ËÅöÁÑ¶Â±ÄÈÉ®Á∫πÁêÜÁªüËÆ°„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®ÂÆöÈáèÂíå‰∫∫Â∑•ËØÑ‰º∞‰∏≠‰ºò‰∫éÂü∫Á∫øÔºåÂ±ïÁ§∫Âá∫‰ºòË∂äÁöÑËßÜËßâË¥®Èáè„ÄÅÁ≤æÁ°ÆÁºñËæëÂíåÂèØÊéßÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored. In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos. To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture. However, this approach suffers from limited controllability for two reasons: conditioning on the raw reference image introduces unwanted structural information, and it fails to disentangle the visual texture and structure information of the source. To address this problem, we propose Refa√ßade, a method that consists of two key designs to achieve precise and controllable texture transfer in both images and videos. First, we employ a texture remover trained on paired textured/untextured 3D mesh renderings to remove appearance information while preserving the geometry and motion of source videos. Second, we disrupt the reference global layout using a jigsaw permutation, encouraging the model to focus on local texture statistics rather than the global layout of the object. Extensive experiments demonstrate superior visual quality, precise editing, and controllability, outperforming strong baselines in both quantitative and human evaluations. Code is available at https://github.com/fishZe233/Refacade.

