---
layout: default
title: RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation
---

# RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation

**arXiv**: [2512.05025v1](https://arxiv.org/abs/2512.05025) | [PDF](https://arxiv.org/pdf/2512.05025.pdf)

**ä½œè€…**: Nicolas HoudrÃ©, Diego Marcos, Hugo Riffaud de Turckheim, Dino Ienco, Laurent Wendling, Camille Kurtz, Sylvain Lobry

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAMENä»¥è§£å†³åœ°çƒè§‚æµ‹ä¸­å¤šæ¨¡æ€æ•°æ®åˆ†è¾¨çŽ‡å›ºå®šå’Œä¼ æ„Ÿå™¨ä¾èµ–çš„è¡¨ç¤ºå­¦ä¹ é—®é¢˜ã€‚**

**å…³é”®è¯**: `åœ°çƒè§‚æµ‹` `å¤šæ¨¡æ€ç¼–ç å™¨` `åˆ†è¾¨çŽ‡å¯è°ƒ` `ä¼ æ„Ÿå™¨æ— å…³` `è¡¨ç¤ºå­¦ä¹ ` `Transformer`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºç¡€æ¨¡åž‹å¯¹å›ºå®šè¾“å…¥åˆ†è¾¨çŽ‡æˆ–ä¼ æ„Ÿå™¨ç‰¹å®šç¼–ç å™¨çš„ä¾èµ–ï¼Œé™åˆ¶äº†è·¨å¼‚æž„åœ°çƒè§‚æµ‹æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šRAMENä½œä¸ºåˆ†è¾¨çŽ‡å¯è°ƒçš„å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œä»¥ä¼ æ„Ÿå™¨æ— å…³æ–¹å¼å­¦ä¹ å…±äº«è§†è§‰è¡¨ç¤ºï¼Œå°†ç©ºé—´åˆ†è¾¨çŽ‡å®šä¹‰ä¸ºå¯æŽ§è¾“å‡ºå‚æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PANGAEAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAMENä¼˜äºŽæ›´å¤§è§„æ¨¡çš„æœ€å…ˆè¿›æ¨¡åž‹ï¼Œæœ‰æ•ˆè¿ç§»åˆ°å·²çŸ¥å’Œæœªè§ä¼ æ„Ÿå™¨é…ç½®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.

