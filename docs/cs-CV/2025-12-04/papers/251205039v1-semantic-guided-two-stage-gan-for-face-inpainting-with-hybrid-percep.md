---
layout: default
title: Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding
---

# Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding

**arXiv**: [2512.05039v1](https://arxiv.org/abs/2512.05039) | [PDF](https://arxiv.org/pdf/2512.05039.pdf)

**ä½œè€…**: Abhigyan Bhattacharya, Hiranmoy Roy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰å¼•å¯¼ä¸¤é˜¶æ®µGANï¼Œé€šè¿‡æ··åˆæ„ŸçŸ¥ç¼–ç è§£å†³äººè„¸ä¿®å¤ä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´å’Œçº¹ç†æ¨¡ç³Šé—®é¢˜ã€‚**

**å…³é”®è¯**: `äººè„¸ä¿®å¤` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ` `è¯­ä¹‰å¼•å¯¼` `æ··åˆæ„ŸçŸ¥ç¼–ç ` `ä¸¤é˜¶æ®µåˆæˆ` `åŠ¨æ€æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨å¤§é¢ç§¯ä¸è§„åˆ™æŽ©ç ä¸‹æ˜“äº§ç”Ÿæ¨¡ç³Šçº¹ç†ã€è¯­ä¹‰ä¸ä¸€è‡´æˆ–ç»“æž„å¤±çœŸã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è¯­ä¹‰å¼•å¯¼åˆ†å±‚åˆæˆï¼Œç¬¬ä¸€é˜¶æ®µç»“åˆCNNå’ŒVision Transformerç”Ÿæˆæ¸…æ™°è¯­ä¹‰å¸ƒå±€ï¼Œç¬¬äºŒé˜¶æ®µå¤šæ¨¡æ€çº¹ç†ç”Ÿæˆå™¨ç»†åŒ–çº¹ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CelebA-HQå’ŒFFHQæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡LPIPSã€PSNRå’ŒSSIMæŒ‡æ ‡ï¼Œè§†è§‰ç»“æžœæ›´ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.

