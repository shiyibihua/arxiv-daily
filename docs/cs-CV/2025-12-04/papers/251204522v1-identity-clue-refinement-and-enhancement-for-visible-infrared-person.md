---
layout: default
title: Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification
---

# Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification

**arXiv**: [2512.04522v1](https://arxiv.org/abs/2512.04522) | [PDF](https://arxiv.org/pdf/2512.04522.pdf)

**ä½œè€…**: Guoqing Zhang, Zhun Wang, Hairui Wang, Zhonglin Ye, Yuhui Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºICREç½‘ç»œä»¥è§£å†³å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«` `æ¨¡æ€ç‰¹å®šå±žæ€§` `èº«ä»½çŸ¥è¯†è’¸é¦` `è·¨æ¨¡æ€åŒ¹é…` `ç‰¹å¾å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¿½è§†æ¨¡æ€ç‰¹å®šèº«ä»½çŸ¥è¯†ï¼Œå¯¼è‡´è·¨æ¨¡æ€åŒ¹é…å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡MPFRæ¨¡å—æ•èŽ·æ¨¡æ€ç‰¹å®šå±žæ€§ï¼ŒSDCEæ¨¡å—è’¸é¦èº«ä»½çŸ¥è¯†æŒ‡å¯¼ç‰¹å¾å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰SOTAæ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.

