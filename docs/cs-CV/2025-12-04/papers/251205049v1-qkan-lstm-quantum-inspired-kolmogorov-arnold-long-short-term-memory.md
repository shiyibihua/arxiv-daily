---
layout: default
title: QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory
---

# QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory

**arXiv**: [2512.05049v1](https://arxiv.org/abs/2512.05049) | [PDF](https://arxiv.org/pdf/2512.05049.pdf)

**ä½œè€…**: Yu-Chao Hsu, Jiun-Cheng Jiang, Chun-Hua Lin, Kuo-Chung Peng, Nan-Yow Chen, Samuel Yen-Chi Chen, En-Jui Kuo, Hsi-Sheng Goan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQKAN-LSTMä»¥è§£å†³LSTMå‚æ•°å†—ä½™å’Œéžçº¿æ€§è¡¨è¾¾èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œåº”ç”¨äºŽåºåˆ—å»ºæ¨¡ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `é‡å­å¯å‘ç¥žç»ç½‘ç»œ` `é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ` `åºåˆ—å»ºæ¨¡` `å‚æ•°ä¼˜åŒ–` `éžçº¿æ€§æ¿€æ´»å‡½æ•°` `åŸŽå¸‚ç”µä¿¡é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸLSTMå­˜åœ¨é«˜å‚æ•°å†—ä½™å’Œæœ‰é™éžçº¿æ€§è¡¨è¾¾èƒ½åŠ›çš„é—®é¢˜ã€‚
2. é›†æˆDARUANæ¨¡å—ä½œä¸ºé‡å­å˜åˆ†æ¿€æ´»å‡½æ•°ï¼Œå¢žå¼ºé¢‘çŽ‡é€‚åº”æ€§å’Œè°±è¡¨ç¤ºï¼Œæ— éœ€å¤šé‡å­æ¯”ç‰¹çº ç¼ ã€‚
3. åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®žéªŒæ˜¾ç¤ºï¼Œå‚æ•°å‡å°‘79%ä¸”é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¼˜äºŽç»å…¸LSTMã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.

