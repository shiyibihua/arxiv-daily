---
layout: default
title: Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems
---

# Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems

**arXiv**: [2512.04476v1](https://arxiv.org/abs/2512.04476) | [PDF](https://arxiv.org/pdf/2512.04476.pdf)

**ä½œè€…**: Zehao Fan, Zhenyu Liu, Yunzhen Liu, Yayue Hou, Hadjer Benmeziane, Kaoutar El Maghraoui, Liu Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„MoEæŽ¨ç†ç³»ç»Ÿï¼Œåˆ©ç”¨CXL-NDPè§£å†³GPUå†…å­˜å—é™é—®é¢˜**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡åž‹` `CXLè¿‘æ•°æ®å¤„ç†` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŽ¨ç†` `åŠ¨æ€ä¸“å®¶æ”¾ç½®` `æ··åˆç²¾åº¦é‡åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MoEæ¨¡åž‹æŽ¨ç†åœ¨ä¸“å®¶æƒé‡è¶…å‡ºGPUå†…å­˜æ—¶æˆä¸ºå†…å­˜ç“¶é¢ˆï¼Œéœ€é¢‘ç¹å¸è½½æƒé‡ã€‚
2. é‡‡ç”¨CXL-NDPä½œä¸ºå¸è½½å±‚ï¼ŒåŸºäºŽé¢„å¡«å……é˜¶æ®µæ¿€æ´»ç»Ÿè®¡åŠ¨æ€æ”¾ç½®ä¸“å®¶ï¼Œå‡å°‘å‚æ•°ç§»åŠ¨ã€‚
3. ç³»ç»Ÿåœ¨GPU-NDPä¸Šå®žçŽ°æœ€é«˜8.7å€è§£ç åžåæå‡ï¼Œå¹³å‡ç²¾åº¦ä»…ä¸‹é™0.13%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.

