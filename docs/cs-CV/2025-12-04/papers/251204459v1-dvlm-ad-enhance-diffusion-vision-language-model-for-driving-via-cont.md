---
layout: default
title: dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning
---

# dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning

**arXiv**: [2512.04459v1](https://arxiv.org/abs/2512.04459) | [PDF](https://arxiv.org/pdf/2512.04459.pdf)

**ä½œè€…**: Yingzi Ma, Yulong Cao, Wenhao Ding, Shuibai Zhang, Yan Wang, Boris Ivanovic, Ming Jiang, Marco Pavone, Chaowei Xiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºdVLM-ADæ‰©æ•£è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œé€šè¿‡å¯æŽ§æŽ¨ç†å¢žå¼ºç«¯åˆ°ç«¯é©¾é©¶ç³»ç»Ÿåœ¨åˆ†å¸ƒå¤–åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯é©¾é©¶` `æ‰©æ•£è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯æŽ§æŽ¨ç†` `åˆ†å¸ƒå¤–åœºæ™¯` `è¡Œä¸º-è½¨è¿¹ä¸€è‡´æ€§` `è¿­ä»£åŽ»å™ª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è‡ªå›žå½’è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é©¾é©¶ä¸­å› å› æžœæ³¨æ„åŠ›å’Œé¡ºåºç”Ÿæˆï¼Œå¯¼è‡´é«˜å±‚æŽ¨ç†ä¸Žä½Žå±‚è§„åˆ’é—´ä¸€è‡´æ€§å’Œå¯æŽ§æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç¦»æ•£æ‰©æ•£è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œåˆ©ç”¨åŒå‘æ³¨æ„åŠ›å’Œè¿­ä»£åŽ»å™ªï¼Œç»Ÿä¸€æ„ŸçŸ¥ã€ç»“æž„åŒ–æŽ¨ç†å’Œä½Žå±‚è§„åˆ’ï¼Œæå‡å¯æŽ§æ€§å’Œå¯é æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨nuSceneså’ŒWOD-E2Eè¯„ä¼°ä¸­ï¼ŒdVLM-ADåœ¨è¡Œä¸º-è½¨è¿¹ä¸€è‡´æ€§ä¸Šä¼˜äºŽè‡ªå›žå½’åŸºçº¿9%ï¼Œé•¿å°¾åœºæ™¯RFSæå‡6%ï¼Œè§„åˆ’æ€§èƒ½å¯æ¯”ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.

