---
layout: default
title: Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models
---

# Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models

**arXiv**: [2512.04425v1](https://arxiv.org/abs/2512.04425) | [PDF](https://arxiv.org/pdf/2512.04425.pdf)

**ä½œè€…**: Manar Alnaasan, Md Selim Sarowar, Sungho Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽRGB-Då¤šæ¨¡æ€èžåˆä¸Žå¤§åž‹è¯­è¨€æ¨¡åž‹çš„å¯è§£é‡Šå¸•é‡‘æ£®ç—…æ­¥æ€è¯†åˆ«æ¡†æž¶**

**å…³é”®è¯**: `å¸•é‡‘æ£®ç—…æ­¥æ€è¯†åˆ«` `RGB-Då¤šæ¨¡æ€èžåˆ` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `æ—¶ç©ºç‰¹å¾æå–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¸•é‡‘æ£®ç—…æ­¥æ€è¯†åˆ«æ–¹æ³•å­˜åœ¨å•æ¨¡æ€è¾“å…¥ã€é²æ£’æ€§ä½Žå’Œä¸´åºŠå¯è§£é‡Šæ€§ä¸è¶³çš„å±€é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒYOLOv11ç¼–ç å™¨æå–RGB-Dç‰¹å¾ï¼Œç»“åˆå¤šå°ºåº¦å±€éƒ¨-å…¨å±€æå–æ¨¡å—å’Œè·¨ç©ºé—´èžåˆæœºåˆ¶å¢žå¼ºæ—¶ç©ºè¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ¨¡æ€æ­¥æ€æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œè¯¥æ¡†æž¶æé«˜äº†è¯†åˆ«å‡†ç¡®çŽ‡ã€çŽ¯å¢ƒé²æ£’æ€§ï¼Œå¹¶é€šè¿‡å†»ç»“å¤§åž‹è¯­è¨€æ¨¡åž‹ç”Ÿæˆä¸´åºŠå¯è§£é‡Šæ–‡æœ¬ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM

