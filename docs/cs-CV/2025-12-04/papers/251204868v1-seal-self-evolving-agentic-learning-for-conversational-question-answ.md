---
layout: default
title: SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs
---

# SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs

**arXiv**: [2512.04868v1](https://arxiv.org/abs/2512.04868) | [PDF](https://arxiv.org/pdf/2512.04868.pdf)

**ä½œè€…**: Hao Wang, Jialun Zhong, Changcheng Wang, Zhujun Nie, Zheng Li, Shunyu Yao, Yanzeng Li, Xinchi Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEALæ¡†æž¶ï¼Œé€šè¿‡è‡ªæ¼”è¿›ä»£ç†å­¦ä¹ è§£å†³çŸ¥è¯†å›¾è°±å¯¹è¯é—®ç­”ä¸­çš„ç»“æž„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆçŽ‡é—®é¢˜ã€‚**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±å¯¹è¯é—®ç­”` `è¯­ä¹‰è§£æž` `è‡ªæ¼”è¿›å­¦ä¹ ` `ä»£ç†æ ¡å‡†` `Sè¡¨è¾¾å¼ç”Ÿæˆ` `å¤šè·³æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŸ¥è¯†å›¾è°±å¯¹è¯é—®ç­”é¢ä¸´æŒ‡ä»£æ¶ˆè§£ã€ä¸Šä¸‹æ–‡ä¾èµ–å»ºæ¨¡å’Œå¤æ‚é€»è¾‘æŽ¨ç†çš„æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•å­˜åœ¨ç»“æž„ä¸å‡†ç¡®å’Œé«˜è®¡ç®—æˆæœ¬ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè¯­ä¹‰è§£æžï¼Œå…ˆæå–æœ€å°Sè¡¨è¾¾å¼æ ¸å¿ƒå¹¶ä»£ç†æ ¡å‡†ï¼Œå†æ¨¡æ¿åŒ–è¡¥å…¨ï¼Œç»“åˆè‡ªæ¼”è¿›æœºåˆ¶ä»Žå¯¹è¯åŽ†å²å’Œåé¦ˆä¸­æŒç»­é€‚åº”ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SPICEåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤šè·³æŽ¨ç†ã€æ¯”è¾ƒå’Œèšåˆä»»åŠ¡ä¸Šï¼ŒéªŒè¯äº†ç»“æž„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆçŽ‡çš„æ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.

