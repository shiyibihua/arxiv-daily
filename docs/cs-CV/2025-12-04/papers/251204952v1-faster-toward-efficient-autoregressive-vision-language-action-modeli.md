---
layout: default
title: FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization
---

# FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization

**arXiv**: [2512.04952v1](https://arxiv.org/abs/2512.04952) | [PDF](https://arxiv.org/pdf/2512.04952.pdf)

**ä½œè€…**: Yicheng Liu, Shiduo Zhang, Zibin Dong, Baijun Ye, Tianyuan Yuan, Xiaopeng Yu, Linqi Yin, Chenhao Lu, Junhao Shi, Luca Jiang-Tao Yu, Liangtao Zheng, Tao Jiang, Jingjing Gong, Xipeng Qiu, Hang Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFASTeræ¡†æž¶ä»¥è§£å†³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹ä¸­åŠ¨ä½œæ ‡è®°åŒ–åœ¨é‡å»ºä¿çœŸåº¦ä¸ŽæŽ¨ç†æ•ˆçŽ‡é—´çš„æƒè¡¡é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `åŠ¨ä½œæ ‡è®°åŒ–` `è‡ªå›žå½’è§£ç ` `æœºå™¨äººå­¦ä¹ ` `æŽ¨ç†æ•ˆçŽ‡` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå›žå½’è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹çš„åŠ¨ä½œæ ‡è®°åŒ–å­˜åœ¨é‡å»ºä¿çœŸåº¦ä¸ŽæŽ¨ç†æ•ˆçŽ‡çš„æƒè¡¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šFASTerVQå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒä»¥æ•èŽ·æ—¶ç©ºä¾èµ–ï¼ŒFASTerVLAåŸºäºŽæ­¤é‡‡ç”¨å—çº§è‡ªå›žå½’è§£ç å’Œè½»é‡åŠ¨ä½œä¸“å®¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFASTerVQæä¾›é«˜è´¨é‡é‡å»ºå’Œå¼ºæ³›åŒ–ï¼ŒFASTerVLAåœ¨æŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ä¸Šè¶…è¶Šå…ˆå‰æœ€ä¼˜æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.

