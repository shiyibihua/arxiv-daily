---
layout: default
title: X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale
---

# X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale

**arXiv**: [2512.04537v1](https://arxiv.org/abs/2512.04537) | [PDF](https://arxiv.org/pdf/2512.04537.pdf)

**ä½œè€…**: Pei Yang, Hai Ci, Yiren Song, Mike Zheng Shou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºX-Humanoidæ–¹æ³•ï¼Œé€šè¿‡è§†é¢‘ç¼–è¾‘å°†äººç±»è§†é¢‘è½¬æ¢ä¸ºå¤§è§„æ¨¡äººå½¢æœºå™¨äººè§†é¢‘ï¼Œä»¥è§£å†³è®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `äººå½¢æœºå™¨äºº` `æ•°æ®å¢žå¼º` `è§†é¢‘ç¼–è¾‘` `åˆæˆæ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäººå½¢æœºå™¨äººAIè®­ç»ƒç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„è§†é¢‘æ•°æ®ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å…¨èº«è¿åŠ¨å’Œé®æŒ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽWan 2.2æ¨¡åž‹æž„å»ºè§†é¢‘åˆ°è§†é¢‘ç»“æž„ï¼Œå¾®è°ƒç”¨äºŽäººç±»åˆ°äººå½¢æœºå™¨äººè½¬æ¢ï¼Œå¹¶è®¾è®¡å¯æ‰©å±•çš„æ•°æ®åˆ›å»ºç®¡é“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç”Ÿæˆè¶…è¿‡360ä¸‡å¸§æœºå™¨äººåŒ–è§†é¢‘ï¼Œç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºåœ¨è¿åŠ¨ä¸€è‡´æ€§å’Œä½“çŽ°æ­£ç¡®æ€§ä¸Šä¼˜äºŽåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.

