---
layout: default
title: From Generated Human Videos to Physically Plausible Robot Trajectories
---

# From Generated Human Videos to Physically Plausible Robot Trajectories

**arXiv**: [2512.05094v1](https://arxiv.org/abs/2512.05094) | [PDF](https://arxiv.org/pdf/2512.05094.pdf)

**ä½œè€…**: James Ni, Zekai Wang, Wei Lin, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik, Roei Herzig

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGenMimicä»¥ä»Žç”Ÿæˆè§†é¢‘ä¸­é›¶æ¨¡ä»¿äººç±»åŠ¨ä½œï¼Œå®žçŽ°æœºå™¨äººç‰©ç†ç¨³å®šæŽ§åˆ¶**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆæ¨¡åž‹` `æœºå™¨äººæŽ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `äººä½“åŠ¨ä½œæ¨¡ä»¿` `é›¶æ ·æœ¬æ³›åŒ–` `ç‰©ç†ä»¿çœŸ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ä»Žå™ªå£°ç”Ÿæˆè§†é¢‘ä¸­é›¶æ ·æœ¬æ‰§è¡Œäººç±»åŠ¨ä½œï¼Œå…‹æœå½¢æ€å¤±çœŸå’Œç‰©ç†çº¦æŸ
2. æ–¹æ³•è¦ç‚¹ï¼šä¸¤é˜¶æ®µæµç¨‹ï¼Œå…ˆæå–4Däººä½“è¡¨ç¤ºå¹¶é‡å®šå‘ï¼ŒåŽè®­ç»ƒç‰©ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ç­–ç•¥GenMimic
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨GenMimicBenchåŸºå‡†ä¸Šä¼˜äºŽåŸºçº¿ï¼ŒUnitree G1æœºå™¨äººä¸Šå®žçŽ°ç¨³å®šè¿åŠ¨è·Ÿè¸ª

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.

