---
layout: default
title: Rethinking the Use of Vision Transformers for AI-Generated Image Detection
---

# Rethinking the Use of Vision Transformers for AI-Generated Image Detection

**arXiv**: [2512.04969v1](https://arxiv.org/abs/2512.04969) | [PDF](https://arxiv.org/pdf/2512.04969.pdf)

**ä½œè€…**: NaHyeon Park, Kunhee Kim, Junsuk Choe, Hyunjung Shim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoLDæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é›†æˆViTå¤šå±‚ç‰¹å¾ä»¥æå‡AIç”Ÿæˆå›¾åƒæ£€æµ‹æ€§èƒ½ã€‚**

**å…³é”®è¯**: `AIç”Ÿæˆå›¾åƒæ£€æµ‹` `è§†è§‰Transformer` `å¤šå±‚ç‰¹å¾é›†æˆ` `é—¨æŽ§æœºåˆ¶` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–CLIP-ViTæœ€ç»ˆå±‚ç‰¹å¾ï¼Œå¿½ç•¥äº†å¤šå±‚ç‰¹å¾çš„è´¡çŒ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŸºäºŽé—¨æŽ§æœºåˆ¶çš„è‡ªé€‚åº”æ–¹æ³•MoLDï¼ŒåŠ¨æ€æ•´åˆViTå¤šå±‚ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨GANå’Œæ‰©æ•£ç”Ÿæˆå›¾åƒä¸ŠéªŒè¯ï¼ŒMoLDæ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.

