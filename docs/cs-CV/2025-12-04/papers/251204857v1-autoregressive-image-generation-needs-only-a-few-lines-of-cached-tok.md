---
layout: default
title: Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens
---

# Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens

**arXiv**: [2512.04857v1](https://arxiv.org/abs/2512.04857) | [PDF](https://arxiv.org/pdf/2512.04857.pdf)

**ä½œè€…**: Ziran Qin, Youru Lv, Mingbao Lin, Zeren Zhang, Chanfan Gan, Tieyuan Chen, Weiyao Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLineARä»¥è§£å†³è‡ªå›žå½’å›¾åƒç”Ÿæˆä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜**

**å…³é”®è¯**: `è‡ªå›žå½’å›¾åƒç”Ÿæˆ` `é”®å€¼ç¼“å­˜åŽ‹ç¼©` `å†…å­˜ä¼˜åŒ–` `åžåé‡åŠ é€Ÿ` `è®­ç»ƒæ— å…³æ–¹æ³•` `è§†è§‰æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªå›žå½’å›¾åƒç”Ÿæˆå› éœ€ç¼“å­˜æ‰€æœ‰å…ˆå‰ç”Ÿæˆçš„è§†è§‰ä»¤ç‰Œè€Œé¢ä¸´ä¸¥é‡å†…å­˜ç“¶é¢ˆï¼Œå¯¼è‡´é«˜å­˜å‚¨éœ€æ±‚å’Œä½Žåžåé‡ã€‚
2. LineARæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¸è¿›é”®å€¼ç¼“å­˜åŽ‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡è¡Œçº§ç®¡ç†å’Œ2Dè§†å›¾ï¼ŒåŸºäºŽè¡Œé—´æ³¨æ„åŠ›é€æ­¥æ·˜æ±°å¯¹åŽç»­è¡Œç”Ÿæˆæ— å®³çš„ä½Žä¿¡æ¯ä»¤ç‰Œã€‚
3. å®žéªŒåœ¨å…­ä¸ªè‡ªå›žå½’å›¾åƒç”Ÿæˆæ¨¡åž‹ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå®žçŽ°äº†å†…å­˜èŠ‚çœå’Œåžåé‡åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡ç”Ÿæˆè´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.

