---
layout: default
title: SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security
---

# SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security

**arXiv**: [2512.04841v1](https://arxiv.org/abs/2512.04841) | [PDF](https://arxiv.org/pdf/2512.04841.pdf)

**ä½œè€…**: Wei Zhao, Zhe Li, Jun Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å› æžœåˆ†æžæ¡†æž¶ä»¥ç³»ç»Ÿç ”ç©¶å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨æ¼æ´ž**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨` `å› æžœåˆ†æžæ¡†æž¶` `è¶Šç‹±æ”»å‡»` `æ¨¡åž‹å¯è§£é‡Šæ€§` `å®‰å…¨æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹æ˜“å—è¶Šç‹±ç­‰å¯¹æŠ—æ”»å‡»ï¼Œéœ€ç†è§£å…¶å› æžœå› ç´ ä»¥æž„å»ºå¯é é˜²å¾¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¡†æž¶æ”¯æŒä»Žä»¤ç‰Œåˆ°è¡¨ç¤ºå±‚çš„å¤šçº§å› æžœå¹²é¢„ï¼Œå®žçŽ°ä¸€è‡´å®žéªŒä¸Žæ¯”è¾ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ¨¡åž‹å’ŒåŸºå‡†ä¸Šè¯„ä¼°ï¼Œæ­ç¤ºå®‰å…¨æœºåˆ¶é«˜åº¦å±€éƒ¨åŒ–ï¼Œæ£€æµ‹å‡†ç¡®çŽ‡è¶…95%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
>   In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
>   By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

