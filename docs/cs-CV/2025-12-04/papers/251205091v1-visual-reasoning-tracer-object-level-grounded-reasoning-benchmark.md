---
layout: default
title: Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark
---

# Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark

**arXiv**: [2512.05091v1](https://arxiv.org/abs/2512.05091) | [PDF](https://arxiv.org/pdf/2512.05091.pdf)

**ä½œè€…**: Haobo Yuan, Yueyi Sun, Yanwei Li, Tao Zhang, Xueqing Deng, Henghui Ding, Lu Qi, Anran Wang, Xiangtai Li, Ming-Hsuan Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰æŽ¨ç†è¿½è¸ªå™¨ä»»åŠ¡ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†è¿‡ç¨‹ä¸é€æ˜Žçš„é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰æŽ¨ç†è¿½è¸ª` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¯¹è±¡çº§æŽ¨ç†` `åŸºå‡†æ•°æ®é›†` `æŽ¨ç†è·¯å¾„è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†è¿‡ç¨‹ä¸é€æ˜Žï¼Œç¼ºä¹ä¸­é—´æ­¥éª¤å’Œç»†ç²’åº¦è¯æ®
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥VRTä»»åŠ¡ï¼Œè¦æ±‚æ¨¡åž‹å®šä½ç›®æ ‡å¯¹è±¡å¹¶é¢„æµ‹ä¸­é—´æŽ¨ç†è·¯å¾„å¯¹è±¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºäºŽVRT-80kæ•°æ®é›†è®­ç»ƒçš„æ¨¡åž‹åœ¨æŽ¨ç†è·¯å¾„è¿½è¸ªä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.

