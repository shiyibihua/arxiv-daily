---
layout: default
title: Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion
---

# Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion

**arXiv**: [2512.04926v1](https://arxiv.org/abs/2512.04926) | [PDF](https://arxiv.org/pdf/2512.04926.pdf)

**ä½œè€…**: Yueming Pan, Ruoyu Feng, Qi Dai, Yuqi Wang, Wenfeng Lin, Mingyu Guo, Chong Luo, Nanning Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰ä¼˜å…ˆæ‰©æ•£ä»¥å¼‚æ­¥å»ºæ¨¡è¯­ä¹‰ä¸Žçº¹ç†ï¼Œæå‡æ½œåœ¨æ‰©æ•£æ¨¡åž‹çš„ç”Ÿæˆè´¨é‡ä¸Žæ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `æ½œåœ¨æ‰©æ•£æ¨¡åž‹` `è¯­ä¹‰ä¼˜å…ˆç”Ÿæˆ` `å¼‚æ­¥åŽ»å™ª` `å›¾åƒç”Ÿæˆ` `è¯­ä¹‰å»ºæ¨¡` `çº¹ç†ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ½œåœ¨æ‰©æ•£æ¨¡åž‹ç”Ÿæˆä¸­è¯­ä¹‰ä¸Žçº¹ç†åŒæ­¥åŽ»å™ªï¼Œå¿½ç•¥è¯­ä¹‰å…ˆäºŽçº¹ç†çš„é¡ºåºï¼Œå½±å“ç”Ÿæˆæ•ˆæžœã€‚
2. æž„å»ºå¤åˆæ½œåœ¨ç©ºé—´ï¼Œé€šè¿‡ä¸“ç”¨è¯­ä¹‰VAEæå–è¯­ä¹‰ï¼Œå¹¶å¼‚æ­¥åŽ»å™ªè¯­ä¹‰ä¸Žçº¹ç†ï¼Œè¯­ä¹‰å…ˆè¡Œæä¾›é«˜å±‚æŒ‡å¯¼ã€‚
3. åœ¨ImageNet 256x256ä¸Šå®žçŽ°FID 1.04ï¼Œæ”¶æ•›é€Ÿåº¦æå‡è¾¾100å€ï¼Œå¹¶æ”¹è¿›çŽ°æœ‰æ–¹æ³•å¦‚ReDiå’ŒVA-VAEã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.

