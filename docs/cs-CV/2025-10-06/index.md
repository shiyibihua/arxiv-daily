---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-06
---

# cs.CVï¼ˆ2025-10-06ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ğŸ”—4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251004587v2-pathology-cot-learning-visual-chain-of-thought-agent-from-expert-who.html">Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior</a></td>
  <td>æå‡ºPathology-CoTæ¡†æ¶ï¼Œä»ä¸“å®¶WSIè¯Šæ–­è¡Œä¸ºä¸­å­¦ä¹ è§†è§‰é“¾å¼æ¨ç†Agent</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04587v2" onclick="toggleFavorite(this, '2510.04587v2', 'Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251004966v1-activemark-on-watermarking-of-visual-foundation-models-via-massive-a.html">ActiveMark: on watermarking of visual foundation models via massive activations</a></td>
  <td>æå‡ºActiveMarkä»¥è§£å†³è§†è§‰åŸºç¡€æ¨¡å‹çš„æ°´å°ä¿æŠ¤é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04966v1" onclick="toggleFavorite(this, '2510.04966v1', 'ActiveMark: on watermarking of visual foundation models via massive activations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251004628v1-a-spatial-spectral-frequency-interactive-network-for-multimodal-remo.html">A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification</a></td>
  <td>æå‡ºç©ºé—´-å…‰è°±-é¢‘ç‡äº¤äº’ç½‘ç»œSÂ²Finï¼Œç”¨äºæå‡å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒåˆ†ç±»ç²¾åº¦ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04628v1" onclick="toggleFavorite(this, '2510.04628v1', 'A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251005091v1-factuality-matters-when-image-generation-and-editing-meet-structured.html">Factuality Matters: When Image Generation and Editing Meet Structured Visuals</a></td>
  <td>é’ˆå¯¹ç»“æ„åŒ–è§†è§‰ç”Ÿæˆä¸ç¼–è¾‘çš„äº‹å®æ€§é—®é¢˜ï¼Œæå‡ºStructBenchåŸºå‡†å’Œå¤šæ¨¡æ€èåˆæ¨¡å‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.05091v1" onclick="toggleFavorite(this, '2510.05091v1', 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251004477v1-medclm-learning-to-localize-and-reason-via-a-cot-curriculum-in-medic.html">MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models</a></td>
  <td>MedCLMï¼šé€šè¿‡CoTè¯¾ç¨‹å­¦ä¹ åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å®šä½å’Œæ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04477v1" onclick="toggleFavorite(this, '2510.04477v1', 'MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251005094v1-vchain-chain-of-visual-thought-for-reasoning-in-video-generation.html">VChain: Chain-of-Visual-Thought for Reasoning in Video Generation</a></td>
  <td>VChainï¼šç”¨äºè§†é¢‘ç”Ÿæˆä¸­æ¨ç†çš„è§†è§‰æ€ç»´é“¾</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.05094v1" onclick="toggleFavorite(this, '2510.05094v1', 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251005093v1-character-mixing-for-video-generation.html">Character Mixing for Video Generation</a></td>
  <td>æå‡ºCCEå’ŒCCAæ¡†æ¶ï¼Œå®ç°è·¨ä¸–ç•Œè§‚è§’è‰²èåˆçš„è§†é¢‘ç”Ÿæˆï¼Œè§£å†³é£æ ¼é€€åŒ–é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.05093v1" onclick="toggleFavorite(this, '2510.05093v1', 'Character Mixing for Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251004819v1-visual-representations-inside-the-language-model.html">Visual Representations inside the Language Model</a></td>
  <td>åˆ†æå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å†…éƒ¨è§†è§‰è¡¨å¾ï¼Œæ­ç¤ºå…¶æ„ŸçŸ¥èƒ½åŠ›ç“¶é¢ˆä¸æ”¹è¿›æ–¹å‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04819v1" onclick="toggleFavorite(this, '2510.04819v1', 'Visual Representations inside the Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251004753v1-beyond-appearance-transformer-based-person-identification-from-conve.html">Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics</a></td>
  <td>æå‡ºåŸºäºTransformerçš„å¯¹è¯åŠ¨æ€äººä½“è¯†åˆ«æ–¹æ³•ï¼Œæå‡è‡ªç„¶äº¤äº’åœºæ™¯ä¸‹èº«ä»½è¯†åˆ«ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04753v1" onclick="toggleFavorite(this, '2510.04753v1', 'Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251004706v1-id-consistent-precise-expression-generation-with-blendshape-guided-d.html">ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</a></td>
  <td>æå‡ºBlendshapeå¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°èº«ä»½ä¿æŒå’Œç²¾å‡†è¡¨æƒ…ç”Ÿæˆã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04706v1" onclick="toggleFavorite(this, '2510.04706v1', 'ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251004479v2-vasevqa-3d-benchmarking-3d-vlms-on-ancient-greek-pottery.html">VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery</a></td>
  <td>æå‡ºVaseVQA-3Dæ•°æ®é›†å’ŒVaseVLMæ¨¡å‹ï¼Œè§£å†³3Dæ–‡ç‰©é¢†åŸŸè§†è§‰é—®ç­”çš„æ•°æ®ç¨€ç¼ºå’ŒçŸ¥è¯†ä¸è¶³é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04479v2" onclick="toggleFavorite(this, '2510.04479v2', 'VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251004401v1-your-vision-language-model-cant-even-count-to-20-exposing-the-failur.html">Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting</a></td>
  <td>VLMCountBenchæ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»„åˆè®¡æ•°ä»»åŠ¡ä¸Šçš„æ˜¾è‘—ç¼ºé™·</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04401v1" onclick="toggleFavorite(this, '2510.04401v1', 'Your Vision-Language Model Can&#39;t Even Count to 20: Exposing the Failures of VLMs in Compositional Counting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251004723v1-benchmark-on-monocular-metric-depth-estimation-in-wildlife-setting.html">Benchmark on Monocular Metric Depth Estimation in Wildlife Setting</a></td>
  <td>æ„å»ºé‡ç”ŸåŠ¨ç‰©åœºæ™¯ä¸‹å•ç›®æ·±åº¦ä¼°è®¡åŸºå‡†ï¼Œè¯„ä¼°ç°æœ‰æ–¹æ³•æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04723v1" onclick="toggleFavorite(this, '2510.04723v1', 'Benchmark on Monocular Metric Depth Estimation in Wildlife Setting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251005034v6-video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-.html">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></td>
  <td>å…¨é¢å‰–æè§†é¢‘å¤§æ¨¡å‹åè®­ç»ƒæ–¹æ³•ï¼Œæå‡è§†é¢‘æ¨ç†èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.05034v6" onclick="toggleFavorite(this, '2510.05034v6', 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251004714v1-object-centric-representation-learning-for-enhanced-3d-scene-graph-p.html">Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</a></td>
  <td>æå‡ºé¢å‘å¯¹è±¡çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œæå‡3Dåœºæ™¯å›¾é¢„æµ‹ç²¾åº¦</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04714v1" onclick="toggleFavorite(this, '2510.04714v1', 'Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251004564v2-conditional-representation-learning-for-customized-tasks.html">Conditional Representation Learning for Customized Tasks</a></td>
  <td>æå‡ºæ¡ä»¶è¡¨ç¤ºå­¦ä¹ (CRL)ï¼Œä¸ºå®šåˆ¶ä»»åŠ¡æå–ç‰¹å®šè¯­ä¹‰çš„å›¾åƒè¡¨å¾ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04564v2" onclick="toggleFavorite(this, '2510.04564v2', 'Conditional Representation Learning for Customized Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251004794v1-a-comparative-study-of-vision-transformers-and-cnns-for-few-shot-rig.html">A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation</a></td>
  <td>å¯¹æ¯”ViTä¸CNNåœ¨å°‘æ ·æœ¬åˆšæ€§å˜æ¢å’Œæœ¬è´¨çŸ©é˜µä¼°è®¡ä¸­çš„æ€§èƒ½ï¼Œæ­ç¤ºä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„æ¶æ„é€‰æ‹©ç­–ç•¥ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04794v1" onclick="toggleFavorite(this, '2510.04794v1', 'A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251004856v1-erde-entropy-regularized-distillation-for-early-exit.html">ERDE: Entropy-Regularized Distillation for Early-exit</a></td>
  <td>æå‡ºåŸºäºç†µæ­£åˆ™åŒ–çš„çŸ¥è¯†è’¸é¦æ—©æœŸé€€å‡ºæ–¹æ³•ï¼Œæå‡è¾¹ç¼˜è®¾å¤‡å›¾åƒåˆ†ç±»æ•ˆç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04856v1" onclick="toggleFavorite(this, '2510.04856v1', 'ERDE: Entropy-Regularized Distillation for Early-exit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251004838v1-beyond-random-automatic-inner-loop-optimization-in-dataset-distillat.html">Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation</a></td>
  <td>æå‡ºAT-BPTTï¼Œé€šè¿‡è‡ªåŠ¨å†…å¾ªç¯ä¼˜åŒ–æå‡æ•°æ®é›†è’¸é¦æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04838v1" onclick="toggleFavorite(this, '2510.04838v1', 'Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251004648v1-edupersona-benchmarking-subjective-ability-boundaries-of-virtual-stu.html">EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents</a></td>
  <td>EduPersonaï¼šè¯„ä¼°è™šæ‹Ÿå­¦ç”ŸAgentä¸»è§‚èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04648v1" onclick="toggleFavorite(this, '2510.04648v1', 'EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251004759v2-progressive-gaussian-transformer-with-anisotropy-aware-sampling-for-.html">Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</a></td>
  <td>æå‡ºPG-Occæ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›å¼é«˜æ–¯Transformerå®ç°å¼€æ”¾è¯æ±‡ä¸‰ç»´ occupancy é¢„æµ‹ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04759v2" onclick="toggleFavorite(this, '2510.04759v2', 'Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251004770v1-beyond-the-seen-bounded-distribution-estimation-for-open-vocabulary-.html">Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning</a></td>
  <td>æå‡ºåŸºäºæœ‰ç•Œåˆ†å¸ƒä¼°è®¡çš„å¼€æ”¾è¯æ±‡å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæœªè§ç±»æ•°æ®æå‡æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04770v1" onclick="toggleFavorite(this, '2510.04770v1', 'Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251005408v1-see-the-past-time-reversed-scene-reconstruction-from-thermal-traces-.html">See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models</a></td>
  <td>æå‡ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ—¶åºé€†è½¬åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œåˆ©ç”¨çƒ­æˆåƒç—•è¿¹æ¨æ–­è¿‡å»åœºæ™¯çŠ¶æ€ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.05408v1" onclick="toggleFavorite(this, '2510.05408v1', 'See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251004822v1-avatarvton-4d-virtual-try-on-for-animatable-avatars.html">AvatarVTON: 4D Virtual Try-On for Animatable Avatars</a></td>
  <td>AvatarVTONï¼šæå‡ºé¦–ä¸ªç”¨äºå¯åŠ¨ç”»Avatarçš„4Dè™šæ‹Ÿè¯•ç©¿æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04822v1" onclick="toggleFavorite(this, '2510.04822v1', 'AvatarVTON: 4D Virtual Try-On for Animatable Avatars')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/251006277v1-general-and-efficient-visual-goal-conditioned-reinforcement-learning.html">General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks</a></td>
  <td>æå‡ºåŸºäºå¯¹è±¡æ— å…³æ©ç çš„è§†è§‰ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡æ³›åŒ–æ€§å’Œæ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.06277v1" onclick="toggleFavorite(this, '2510.06277v1', 'General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251004781v2-hands-free-heritage-automated-3d-scanning-for-cultural-heritage-digi.html">Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization</a></td>
  <td>æå‡ºä¸€ç§è‡ªåŠ¨åŒ–åŒæœºå™¨äººæ‰«æç³»ç»Ÿï¼Œç”¨äºæ–‡åŒ–é—äº§é«˜ç²¾åº¦ä¸‰ç»´æ•°å­—åŒ–</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04781v2" onclick="toggleFavorite(this, '2510.04781v2', 'Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/251004802v1-did-you-just-see-that-arbitrary-view-synthesis-for-egocentric-replay.html">Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors</a></td>
  <td>EgoSurgï¼šåŸºäºç¯å¢ƒä¼ æ„Ÿå™¨ï¼Œä¸ºæ‰‹æœ¯å®¤å·¥ä½œæµç¨‹é‡å»ºä»»æ„è§†è§’çš„è‡ªæˆ‘ä¸­å¿ƒå›æ”¾ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04802v1" onclick="toggleFavorite(this, '2510.04802v1', 'Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251005051v2-segmast3r-geometry-grounded-segment-matching.html">SegMASt3R: Geometry Grounded Segment Matching</a></td>
  <td>SegMASt3Rï¼šåˆ©ç”¨3DåŸºç¡€æ¨¡å‹å®ç°å‡ ä½•æ„ŸçŸ¥çš„å›¾åƒåˆ†å‰²åŒ¹é…</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.05051v2" onclick="toggleFavorite(this, '2510.05051v2', 'SegMASt3R: Geometry Grounded Segment Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251004854v1-read-the-room-inferring-social-context-through-dyadic-interaction-re.html">Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦ä¼ æ„Ÿå™¨çš„ç¾¤ä½“äº¤äº’è¯†åˆ«æ–¹æ³•ï¼Œç”¨äºå¢å¼ºç½‘ç»œç‰©ç†ç¤¾ä¼šåŸºç¡€è®¾æ–½ç³»ç»Ÿä¸­çš„ç¤¾ä¼šæ„ŸçŸ¥ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.04854v1" onclick="toggleFavorite(this, '2510.04854v1', 'Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)