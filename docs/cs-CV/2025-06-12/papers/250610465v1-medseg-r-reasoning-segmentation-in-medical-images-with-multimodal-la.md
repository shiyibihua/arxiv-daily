---
layout: default
title: MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models
---

# MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10465" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10465v1</a>
  <a href="https://arxiv.org/pdf/2506.10465.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10465v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10465v1', 'MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Huang, Zelin Peng, Yichen Zhao, Piao Yang, Xiaokang Yang, Wei Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: â€ : Equal contribution

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMedSeg-Rä»¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒåˆ†å‰²` `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `ä¸´åºŠè¯Šæ–­` `æ•°æ®é›†æ„å»º` `è‡ªåŠ¨åŒ–è¯Šæ–­` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ä¾èµ–äºæ˜ç¡®çš„äººç±»æŒ‡ä»¤ï¼Œç¼ºä¹ä¸»åŠ¨æ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„ä¸´åºŠé—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºMedSeg-Ræ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç”ŸæˆåŸºäºéšå«åŒ»å­¦æŒ‡ä»¤çš„åˆ†å‰²æ©è†œã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedSeg-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²å‡†ç¡®ç‡ï¼Œå¹¶å®ç°äº†åŒ»å­¦å›¾åƒçš„å¯è§£é‡Šåˆ†æã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ¨¡å‹ä¾èµ–äºæ˜ç¡®çš„äººç±»æŒ‡ä»¤ï¼Œç¼ºä¹ç†è§£å¤æ‚ä¸´åºŠé—®é¢˜çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•åœ¨ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©è†œæ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å…¶åœ¨è‡ªåŠ¨åŒ»å­¦è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºåŒ»å­¦å›¾åƒæ¨ç†åˆ†å‰²è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨åŸºäºå¤æ‚å’Œéšå«çš„åŒ»å­¦æŒ‡ä»¤ç”Ÿæˆåˆ†å‰²æ©è†œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MedSeg-Rï¼Œä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œåˆ©ç”¨MLLMsçš„æ¨ç†èƒ½åŠ›æ¥è§£é‡Šä¸´åºŠé—®é¢˜ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„ç²¾ç¡®åˆ†å‰²æ©è†œå’Œæ–‡æœ¬å“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedSeg-QAï¼Œä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒæ¨ç†åˆ†å‰²ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10,000ä¸ªå›¾åƒ-æ©è†œå¯¹å’Œå¤šè½®å¯¹è¯ï¼Œç»è¿‡å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ³¨é‡Šå¹¶ç»è¿‡åŒ»ç”Ÿå®¡æ ¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedSeg-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå®ç°äº†é«˜åˆ†å‰²å‡†ç¡®ç‡ï¼Œå¹¶æ”¯æŒåŒ»å­¦å›¾åƒçš„å¯è§£é‡Šæ–‡æœ¬åˆ†æã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¯¹å¤æ‚å’Œéšå«åŒ»å­¦æŒ‡ä»¤çš„ç†è§£ä¸å¤„ç†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„æŒ‡ä»¤ï¼Œå¯¼è‡´åœ¨å¤„ç†å¤æ‚ä¸´åºŠé—®é¢˜æ—¶çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMedSeg-Ræ¡†æ¶é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿç†è§£å¤æ‚çš„åŒ»å­¦æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„åˆ†å‰²æ©è†œã€‚è®¾è®¡æ­¤æ¡†æ¶çš„ç›®çš„æ˜¯æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMedSeg-Ræ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š1) å…¨çƒä¸Šä¸‹æ–‡ç†è§£æ¨¡å—ï¼Œè´Ÿè´£è§£æåŒ»å­¦å›¾åƒå’Œå¤æ‚æŒ‡ä»¤ï¼Œç”Ÿæˆå¤šæ¨¡æ€ä¸­é—´æ ‡è®°ï¼›2) åƒç´ çº§å®šä½æ¨¡å—ï¼Œè§£ç è¿™äº›æ ‡è®°ä»¥ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©è†œå’Œæ–‡æœ¬å“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒ»å­¦å›¾åƒæ¨ç†åˆ†å‰²è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨MLLMsçš„æ¨ç†èƒ½åŠ›æ¥å¤„ç†éšå«æŒ‡ä»¤ï¼Œä»è€Œå®ç°æ›´é«˜çš„åˆ†å‰²ç²¾åº¦å’Œå¯è§£é‡Šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMedSeg-Rèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤æ‚çš„ä¸´åºŠé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å‰²æ©è†œçš„ç”Ÿæˆï¼ŒåŒæ—¶åœ¨æ•°æ®é›†æ„å»ºä¸­å¼•å…¥äº†åŒ»ç”Ÿå®¡æ ¸çš„æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMedSeg-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œåˆ†å‰²å‡†ç¡®ç‡æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒMedSeg-Rå±•ç¤ºäº†æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’Œæ›´é«˜çš„åˆ†å‰²ç²¾åº¦ï¼Œæ”¯æŒåŒ»å­¦å›¾åƒçš„å¯è§£é‡Šåˆ†æã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€è¾…åŠ©è¯Šæ–­ç³»ç»Ÿå’Œä¸´åºŠå†³ç­–æ”¯æŒã€‚é€šè¿‡æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼ŒMedSeg-Ræœ‰æœ›åœ¨ä¸´åºŠå®è·µä¸­æä¾›æ›´ä¸ºæœ‰æ•ˆçš„æ”¯æŒï¼Œå¸®åŠ©åŒ»ç”Ÿåšå‡ºæ›´ç²¾å‡†çš„è¯Šæ–­å†³ç­–ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨è‡ªåŠ¨åŒ–åŒ»å­¦è¯Šæ–­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Medical image segmentation is crucial for clinical diagnosis, yet existing models are limited by their reliance on explicit human instructions and lack the active reasoning capabilities to understand complex clinical questions. While recent advancements in multimodal large language models (MLLMs) have improved medical question-answering (QA) tasks, most methods struggle to generate precise segmentation masks, limiting their application in automatic medical diagnosis. In this paper, we introduce medical image reasoning segmentation, a novel task that aims to generate segmentation masks based on complex and implicit medical instructions. To address this, we propose MedSeg-R, an end-to-end framework that leverages the reasoning abilities of MLLMs to interpret clinical questions while also capable of producing corresponding precise segmentation masks for medical images. It is built on two core components: 1) a global context understanding module that interprets images and comprehends complex medical instructions to generate multi-modal intermediate tokens, and 2) a pixel-level grounding module that decodes these tokens to produce precise segmentation masks and textual responses. Furthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the medical image reasoning segmentation task. It includes over 10,000 image-mask pairs and multi-turn conversations, automatically annotated using large language models and refined through physician reviews. Experiments show MedSeg-R's superior performance across several benchmarks, achieving high segmentation accuracy and enabling interpretable textual analysis of medical images.

