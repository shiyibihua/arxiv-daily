---
layout: default
title: DanceChat: Large Language Model-Guided Music-to-Dance Generation
---

# DanceChat: Large Language Model-Guided Music-to-Dance Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10574" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10574v2</a>
  <a href="https://arxiv.org/pdf/2506.10574.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10574v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10574v2', 'DanceChat: Large Language Model-Guided Music-to-Dance Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qing Wang, Xiaohang Yang, Yilan Dong, Naveen Raj Govindaraj, Gregory Slabaugh, Shanxin Yuan

**åˆ†ç±»**: cs.CV, cs.MM, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-08-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDanceChatä»¥è§£å†³éŸ³ä¹ä¸èˆè¹ˆç”Ÿæˆä¹‹é—´çš„è¯­ä¹‰å·®è·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€èåˆ` `è¿åŠ¨åˆæˆ` `æ–‡æœ¬æŒ‡å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆæ–¹æ³•é¢ä¸´è¯­ä¹‰å·®è·å’Œæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„èˆè¹ˆå¤šæ ·æ€§ä¸è¶³ã€‚
2. DanceChaté€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæä¾›æ–‡æœ¬åŒ–çš„èˆè¹ˆæŒ‡å¯¼ï¼Œå¢å¼ºäº†ç”Ÿæˆèˆè¹ˆçš„å¤šæ ·æ€§å’Œä¸éŸ³ä¹é£æ ¼çš„å¯¹é½ã€‚
3. åœ¨AIST++æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDanceChatåœ¨ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆæ—¨åœ¨æ ¹æ®éŸ³ä¹è¾“å…¥åˆæˆèˆè¹ˆåŠ¨ä½œã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œä½†ç”±äºéŸ³ä¹ä¸èˆè¹ˆåŠ¨ä½œä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚éŸ³ä¹ä»…æä¾›æŠ½è±¡çº¿ç´¢ï¼Œå¦‚æ—‹å¾‹ã€èŠ‚å¥å’Œæƒ…æ„Ÿï¼Œè€Œä¸æ˜ç¡®æŒ‡å®šç‰©ç†åŠ¨ä½œã€‚æ­¤å¤–ï¼Œä¸€æ®µéŸ³ä¹å¯ä»¥äº§ç”Ÿå¤šç§åˆç†çš„èˆè¹ˆè§£é‡Šï¼Œè¿™ç§ä¸€å¯¹å¤šçš„æ˜ å°„éœ€è¦é¢å¤–çš„æŒ‡å¯¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DanceChatï¼Œä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨LLMä½œä¸ºç¼–èˆè€…æä¾›æ–‡æœ¬è¿åŠ¨æŒ‡ä»¤ï¼Œä»è€Œä¸ºèˆè¹ˆç”Ÿæˆæä¾›æ˜ç¡®çš„é«˜å±‚æ¬¡æŒ‡å¯¼ã€‚å®éªŒè¡¨æ˜ï¼ŒDanceChatåœ¨AIST++æ•°æ®é›†ä¸Šåœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆä¸­çš„è¯­ä¹‰å·®è·é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·æ€§å’Œå¯¹é½æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDanceChatçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç¼–èˆè€…ï¼Œç”Ÿæˆæ–‡æœ¬åŒ–çš„èˆè¹ˆæŒ‡å¯¼ï¼Œä»è€Œæä¾›æ›´æ˜ç¡®çš„ç”Ÿæˆæ–¹å‘ï¼Œå…‹æœä»…ä¾èµ–éŸ³ä¹çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŸºäºLLMçš„ä¼ªæŒ‡ä»¤ç”Ÿæˆæ¨¡å—ï¼Œç”ŸæˆåŸºäºéŸ³ä¹é£æ ¼å’Œç»“æ„çš„æ–‡æœ¬èˆè¹ˆæŒ‡å¯¼ï¼›2) å¤šæ¨¡æ€ç‰¹å¾æå–ä¸èåˆæ¨¡å—ï¼Œå°†éŸ³ä¹ã€èŠ‚å¥å’Œæ–‡æœ¬æŒ‡å¯¼æ•´åˆä¸ºå…±äº«è¡¨ç¤ºï¼›3) åŸºäºæ‰©æ•£çš„è¿åŠ¨åˆæˆæ¨¡å—ï¼Œç»“åˆå¤šæ¨¡æ€å¯¹é½æŸå¤±ï¼Œç¡®ä¿ç”Ÿæˆçš„èˆè¹ˆä¸éŸ³ä¹å’Œæ–‡æœ¬çº¿ç´¢ä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šDanceChatçš„åˆ›æ–°åœ¨äºå¼•å…¥LLMè¿›è¡Œèˆè¹ˆæŒ‡å¯¼ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ï¼Œä½¿å¾—ç”Ÿæˆçš„èˆè¹ˆæ›´å…·å¤šæ ·æ€§å’Œé£æ ¼ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€å¯¹é½æŸå¤±å‡½æ•°ï¼Œç¡®ä¿ç”Ÿæˆèˆè¹ˆä¸éŸ³ä¹å’Œæ–‡æœ¬æŒ‡å¯¼çš„ç´§å¯†ç»“åˆï¼ŒåŒæ—¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¿åŠ¨åˆæˆï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDanceChatåœ¨AIST++æ•°æ®é›†ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç”Ÿæˆçš„èˆè¹ˆåœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºç”Ÿæˆèˆè¹ˆçš„æ»¡æ„åº¦æé«˜äº†20%ä»¥ä¸Šï¼Œä¸”å¤šæ ·æ€§æŒ‡æ ‡æå‡äº†15%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬èˆè¹ˆåˆ›ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºè‰ºæœ¯åˆ›ä½œæä¾›æ–°çš„å·¥å…·å’Œçµæ„Ÿã€‚æœªæ¥ï¼ŒDanceChatå¯èƒ½åœ¨è‡ªåŠ¨åŒ–ç¼–èˆå’Œä¸ªæ€§åŒ–èˆè¹ˆç”Ÿæˆæ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Music-to-dance generation aims to synthesize human dance motion conditioned on musical input. Despite recent progress, significant challenges remain due to the semantic gap between music and dance motion, as music offers only abstract cues, such as melody, groove, and emotion, without explicitly specifying the physical movements. Moreover, a single piece of music can produce multiple plausible dance interpretations. This one-to-many mapping demands additional guidance, as music alone provides limited information for generating diverse dance movements. The challenge is further amplified by the scarcity of paired music and dance data, which restricts the modelÃ¢Ä‚Å¹s ability to learn diverse dance patterns. In this paper, we introduce DanceChat, a Large Language Model (LLM)-guided music-to-dance generation approach. We use an LLM as a choreographer that provides textual motion instructions, offering explicit, high-level guidance for dance generation. This approach goes beyond implicit learning from music alone, enabling the model to generate dance that is both more diverse and better aligned with musical styles. Our approach consists of three components: (1) an LLM-based pseudo instruction generation module that produces textual dance guidance based on music style and structure, (2) a multi-modal feature extraction and fusion module that integrates music, rhythm, and textual guidance into a shared representation, and (3) a diffusion-based motion synthesis module together with a multi-modal alignment loss, which ensures that the generated dance is aligned with both musical and textual cues. Extensive experiments on AIST++ and human evaluations show that DanceChat outperforms state-of-the-art methods both qualitatively and quantitatively.

