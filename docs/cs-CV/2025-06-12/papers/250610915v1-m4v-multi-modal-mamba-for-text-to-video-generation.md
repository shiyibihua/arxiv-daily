---
layout: default
title: M4V: Multi-Modal Mamba for Text-to-Video Generation
---

# M4V: Multi-Modal Mamba for Text-to-Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10915" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10915v1</a>
  <a href="https://arxiv.org/pdf/2506.10915.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10915v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10915v1', 'M4V: Multi-Modal Mamba for Text-to-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiancheng Huang, Gengwei Zhang, Zequn Jie, Siyu Jiao, Yinlong Qian, Ling Chen, Yunchao Wei, Lin Ma

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://huangjch526.github.io/M4V_project)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM4Væ¡†æ¶ä»¥è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ` `å¤šæ¨¡æ€èåˆ` `æ—¶ç©ºå»ºæ¨¡` `è®¡ç®—å¤æ‚æ€§` `å¥–åŠ±å­¦ä¹ ` `Mambaæ¶æ„` `è§†é¢‘ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†æ—¶ç©ºå»ºæ¨¡æ—¶è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨Transformeræ—¶ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºM4Væ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£Mambaæ¨¡å—å®ç°å¤šæ¨¡æ€ä¿¡æ¯ä¸æ—¶ç©ºå»ºæ¨¡çš„é«˜æ•ˆé›†æˆï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒM4Våœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„åŒæ—¶ï¼ŒFLOPså‡å°‘äº†45%ï¼Œå¹¶é€šè¿‡å¥–åŠ±å­¦ä¹ ç­–ç•¥æå‡äº†è§†è§‰è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ˜¾è‘—ä¸°å¯Œäº†å†…å®¹åˆ›ä½œï¼Œå¹¶æœ‰æ½œåŠ›æ¼”å˜ä¸ºå¼ºå¤§çš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨ã€‚ç„¶è€Œï¼Œå»ºæ¨¡å¹¿æ³›çš„æ—¶ç©ºç©ºé—´åœ¨è®¡ç®—ä¸Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨Transformeræ—¶ï¼Œå…¶åºåˆ—å¤„ç†çš„å¹³æ–¹å¤æ‚åº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†M4Vï¼Œä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€Mambaæ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€æ‰©æ•£Mambaï¼ˆMM-DiMï¼‰æ¨¡å—é€šè¿‡å¤šæ¨¡æ€ä»¤ç‰Œé‡ç»„è®¾è®¡ï¼Œå®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯ä¸æ—¶ç©ºå»ºæ¨¡çš„æ— ç¼é›†æˆã€‚å®éªŒè¡¨æ˜ï¼ŒM4Våœ¨ç”Ÿæˆ768Ã—1280åˆ†è¾¨ç‡è§†é¢‘æ—¶ï¼Œç›¸è¾ƒäºåŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼ŒFLOPså‡å°‘äº†45%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¥–åŠ±å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¯å¸§è§†è§‰çœŸå®æ„Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ—¶ç©ºå»ºæ¨¡æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨Transformeræ—¶çš„å¹³æ–¹å¤æ‚åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºM4Væ¡†æ¶ï¼Œæ ¸å¿ƒåœ¨äºå¤šæ¨¡æ€æ‰©æ•£Mambaï¼ˆMM-DiMï¼‰æ¨¡å—ï¼Œé€šè¿‡å¤šæ¨¡æ€ä»¤ç‰Œé‡ç»„è®¾è®¡ï¼Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€ä¿¡æ¯é›†æˆä¸æ—¶ç©ºå»ºæ¨¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM4Væ¡†æ¶åŒ…å«å¤šä¸ªMambaæ¨¡å—ï¼Œåˆ©ç”¨MM-DiMæ¨¡å—è¿›è¡Œå¤šæ¨¡æ€ä¿¡æ¯å¤„ç†ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ–‡æœ¬è¾“å…¥ã€ä¿¡æ¯é‡ç»„ã€è§†é¢‘ç”Ÿæˆç­‰é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šM4Vçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥MM-DiMæ¨¡å—ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆè§†é¢‘æ—¶çš„è®¡ç®—å¤æ‚åº¦ï¼Œä¸ä¼ ç»ŸåŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFLOPså‡å°‘äº†45%ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€ä»¤ç‰Œé‡ç»„ç­–ç•¥ï¼Œå¹¶å¼•å…¥å¥–åŠ±å­¦ä¹ ç­–ç•¥ä»¥æå‡ç”Ÿæˆè§†é¢‘çš„è§†è§‰è´¨é‡ï¼Œç¡®ä¿æ¯å¸§çš„çœŸå®æ„Ÿã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM4Våœ¨ç”Ÿæˆ768Ã—1280åˆ†è¾¨ç‡è§†é¢‘æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ï¼ŒFLOPså‡å°‘äº†45%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥å¥–åŠ±å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„æ¯å¸§è§†è§‰è´¨é‡ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ä¸ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å½±è§†åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºå†…å®¹åˆ›ä½œè€…æä¾›é«˜æ•ˆçš„å·¥å…·ï¼Œé™ä½è§†é¢‘ç”Ÿæˆçš„è®¡ç®—æˆæœ¬ï¼Œæå‡åˆ›ä½œæ•ˆç‡ã€‚æœªæ¥ï¼ŒM4Væ¡†æ¶å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„è‡ªç„¶æ€§ä¸æ™ºèƒ½åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-to-video generation has significantly enriched content creation and holds the potential to evolve into powerful world simulators. However, modeling the vast spatiotemporal space remains computationally demanding, particularly when employing Transformers, which incur quadratic complexity in sequence processing and thus limit practical applications. Recent advancements in linear-time sequence modeling, particularly the Mamba architecture, offer a more efficient alternative. Nevertheless, its plain design limits its direct applicability to multi-modal and spatiotemporal video generation tasks. To address these challenges, we introduce M4V, a Multi-Modal Mamba framework for text-to-video generation. Specifically, we propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling through a multi-modal token re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45% compared to the attention-based alternative when generating videos at 768$\times$1280 resolution. Additionally, to mitigate the visual quality degradation in long-context autoregressive generation processes, we introduce a reward learning strategy that further enhances per-frame visual realism. Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to produce high-quality videos while significantly lowering computational costs. Code and models will be publicly available at https://huangjch526.github.io/M4V_project.

