---
layout: default
title: Motion-R1: Enhancing Motion Generation with Decomposed Chain-of-Thought and RL Binding
---

# Motion-R1: Enhancing Motion Generation with Decomposed Chain-of-Thought and RL Binding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10353" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.10353v4</a>
  <a href="https://arxiv.org/pdf/2506.10353.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10353v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10353v4', 'Motion-R1: Enhancing Motion Generation with Decomposed Chain-of-Thought and RL Binding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zeyu Zhang, Zheng Zhu, Guan Huang, Sirui Han, Xingang Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-12 (Êõ¥Êñ∞: 2025-11-24)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://motion-r1.github.io/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Motion-R1‰ª•Ëß£ÂÜ≥ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàê‰∏≠ÁöÑÂ§çÊùÇÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàê` `Âº∫ÂåñÂ≠¶‰π†` `ÊÄùÁª¥ÈìæÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂØπÈΩê` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ïÂú®ÊçïÊçâËá™ÁÑ∂ËØ≠Ë®Ä‰∏≠ÁöÑÊó∂Èó¥ÂíåÂõ†ÊûúÂ§çÊùÇÊÄßÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥ÁîüÊàêÁöÑÂä®‰ΩúÂæÄÂæÄ‰∏çËøûË¥Ø„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜMotion-R1Ê°ÜÊû∂ÔºåÁªìÂêàÂàÜËß£ÁöÑÊÄùÁª¥ÈìæÊé®ÁêÜ‰∏éÂº∫ÂåñÂ≠¶‰π†ÔºåÊó®Âú®ÊèêÈ´òÁîüÊàêÂä®‰ΩúÁöÑË¥®ÈáèÂíåÂèØËß£ÈáäÊÄß„ÄÇ
3. Âú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÔºåMotion-R1Âú®MM-Dist„ÄÅR-PrecisionÂíåFIDÁ≠âÂÖ≥ÈîÆÊåáÊ†á‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇÂä®‰ΩúÁîüÊàê‰ªªÂä°‰∏≠ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÂ∑≤Êàê‰∏∫‰∫∫Êú∫‰∫§‰∫í‰∏≠ÁöÑ‰∏ÄÈ°πÂü∫Á°Ä‰ªªÂä°ÔºåËÉΩÂ§ü‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰∏≠ÂêàÊàêÈÄºÁúüÁöÑ‰∫∫Á±ªÂä®‰Ωú„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑËøõÂ±ï‰∏∫È´òË¥®ÈáèÂä®‰ΩúÁîüÊàêÂÅöÂá∫‰∫ÜË¥°ÁåÆÔºå‰ΩÜ‰ªçÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÊó†Ê≥ïÊçïÊçâËá™ÁÑ∂ËØ≠Ë®Ä‰∏≠ÁöÑÊó∂Èó¥ÂíåÂõ†ÊûúÂ§çÊùÇÊÄßÔºåÂØºËá¥ÁîüÊàêÁöÑÂä®‰ΩúËøá‰∫éÁÆÄÂåñÊàñ‰∏çËøûË¥ØÔºõËÄåÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ïÈÄöÂ∏∏Ëøá‰∫éÂ§çÊùÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂêÑÁßçÂä®‰ΩúÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜMotion-R1Ôºå‰∏Ä‰∏™ÁªìÂêàÂàÜËß£ÁöÑÊÄùÁª¥ÈìæÊé®ÁêÜ‰∏éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñ∞Ê°ÜÊû∂Ôºå‰ª•Â¢ûÂº∫ÁîüÊàêÂä®‰ΩúÁöÑË¥®ÈáèÂíåÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂàÜËß£ÁöÑCoTÊï∞ÊçÆÂºïÊìéÔºåÂà©Áî®Ëá™Âä®ÂåñÁÆ°ÈÅìÂêàÊàêÈ´òË¥®ÈáèÊé®ÁêÜÊï∞ÊçÆÔºå‰ΩøÊ®°ÂûãÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∫∫Á±ªÂä®‰ΩúÁöÑÊó∂Èó¥‰æùËµñÊÄßÂíåÂõ†ÊûúÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜRLÁªëÂÆöÔºå‰∏ÄÁßçÂ∞ÜÂ§öÊ®°ÊÄÅÊñáÊú¨-Âä®‰ΩúÂØπÈΩêÁ∫≥ÂÖ•Âº∫ÂåñÂ≠¶‰π†Â•ñÂä±ÂáΩÊï∞ÁöÑÁ≠ñÁï•ÔºåÊåáÂØºÊ®°ÂûãÁîüÊàêËØ≠‰πâÂáÜÁ°Æ‰∏îÂä®‰ΩúÁúüÂÆûÁöÑÂä®‰Ωú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMotion-R1Âú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ïÂú®ÊçïÊçâËá™ÁÑ∂ËØ≠Ë®Ä‰∏≠ÁöÑÊó∂Èó¥ÂíåÂõ†ÊûúÂ§çÊùÇÊÄßÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂØºËá¥ÁîüÊàêÁöÑÂä®‰ΩúÂæÄÂæÄ‰∏çËøûË¥ØÊàñ‰∏çÁúüÂÆû„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMotion-R1Ê°ÜÊû∂ÈÄöËøáÁªìÂêàÂàÜËß£ÁöÑÊÄùÁª¥ÈìæÊé®ÁêÜ‰∏éÂº∫ÂåñÂ≠¶‰π†ÔºåÊó®Âú®ÊèêÈ´òÁîüÊàêÂä®‰ΩúÁöÑË¥®ÈáèÂíåÂèØËß£ÈáäÊÄß„ÄÇÂàÜËß£ÁöÑÊÄùÁª¥ÈìæÊé®ÁêÜ‰ΩøÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ§ÑÁêÜÂ§çÊùÇÁöÑÊó∂Èó¥‰æùËµñÂíåÂõ†ÊûúÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMotion-R1ÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂàÜËß£ÁöÑCoTÊï∞ÊçÆÂºïÊìéÂíåRLÁªëÂÆöÊ®°Âùó„ÄÇÊï∞ÊçÆÂºïÊìéË¥üË¥£ÂêàÊàêÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊï∞ÊçÆÔºåËÄåRLÁªëÂÆöÂàôÂ∞ÜÂ§öÊ®°ÊÄÅÊñáÊú¨-Âä®‰ΩúÂØπÈΩêÁ∫≥ÂÖ•Â•ñÂä±ÂáΩÊï∞Ôºå‰ª•ÊåáÂØºÊ®°ÂûãÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫ÜÂàÜËß£ÁöÑCoTÊï∞ÊçÆÂºïÊìéÂíåRLÁªëÂÆöÁ≠ñÁï•ÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∫ÜÂä®‰ΩúÁîüÊàê‰∏≠ÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°ËØ≠‰πâÂáÜÁ°ÆÊÄßÂíåÂä®‰ΩúÁúüÂÆûÊÑüÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÁîüÊàêËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåMotion-R1Âú®HumanML3DÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü3.5%ÁöÑMM-DistÊèêÂçáÔºåÂπ∂Âú®KIT-MLÂíåBABELÊï∞ÊçÆÈõÜ‰∏äÂú®R-PrecisionÂíåFIDÊåáÊ†á‰∏äÂùáÊúâÊòæËëóÊîπÂñÑÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇÂä®‰ΩúÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂçìË∂äÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèëÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÂú∫ÊôØÔºåËÉΩÂ§ü‰∏∫Ëøô‰∫õÈ¢ÜÂüüÊèê‰æõÊõ¥Ëá™ÁÑ∂ÂíåÈÄºÁúüÁöÑÂä®‰ΩúÁîüÊàêËÉΩÂäõ„ÄÇÊú™Êù•ÔºåMotion-R1ÂèØËÉΩÊé®Âä®Êõ¥Â§çÊùÇÁöÑ‰∫∫Êú∫‰∫§‰∫í‰ΩìÈ™åÔºåÊèêÂçáÁî®Êà∑ÁöÑÊ≤âÊµ∏ÊÑüÂíåÂèÇ‰∏éÊÑü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Text-to-Motion generation has become a fundamental task in human-machine interaction, enabling the synthesis of realistic human motions from natural language descriptions. Although recent advances in large language models and reinforcement learning have contributed to high-quality motion generation, two major challenges remain. Existing approaches often fail to capture the temporal and causal complexities inherent in natural language, leading to oversimplified or incoherent motions. Additionally, RL-based methods are frequently overly complex, hindering their scalability and adaptability across various motion generation tasks. To address these challenges, we propose Motion-R1, a novel framework that combines decomposed Chain-of-Thought reasoning with reinforcement learning to enhance both the quality and interpretability of generated motions. Specifically, we introduce the Decomposed CoT Data Engine, which leverages an automated pipeline to synthesize high-quality reasoning data, allowing the model to better capture the temporal dependencies and causal relationships of human motion. We also propose RL Binding, a reinforcement learning strategy that incorporates multi-modal text-motion alignment into the RL reward function, guiding the model to produce motions that are both semantically accurate and motionally realistic. Extensive experiments across benchmark datasets demonstrate that Motion-R1 achieves state-of-the-art performance, with a 3.5% improvement in MM-Dist on HumanML3D and improvements in R-Precision and FID on KIT-ML and BABEL, surpassing existing methods across key metrics and highlighting its superior capability in handling complex motion generation tasks. Project page: https://motion-r1.github.io/.

