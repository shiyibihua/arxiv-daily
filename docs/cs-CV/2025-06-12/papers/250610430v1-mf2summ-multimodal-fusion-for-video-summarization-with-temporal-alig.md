---
layout: default
title: MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment
---

# MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10430" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10430v1</a>
  <a href="https://arxiv.org/pdf/2506.10430.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10430v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10430v1', 'MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuo wang, Jihao Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMF2Summä»¥è§£å†³è§†é¢‘æ‘˜è¦ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯èåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ‘˜è¦` `å¤šæ¨¡æ€èåˆ` `è·¨æ¨¡æ€å­¦ä¹ ` `ç‰¹å¾æå–` `è‡ªæ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ‘˜è¦æ–¹æ³•å¤šä¾èµ–å•ä¸€æ¨¡æ€ï¼Œæ— æ³•å…¨é¢æ•æ‰è§†é¢‘çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ‘˜è¦æ•ˆæœä¸ä½³ã€‚
2. MF2Summé€šè¿‡æ•´åˆè§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œé‡‡ç”¨è·¨æ¨¡æ€Transformerå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆå»ºæ¨¡æ¨¡æ€é—´ä¾èµ–å…³ç³»ã€‚
3. åœ¨SumMeå’ŒTVSumæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMF2Summåœ¨F1åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äºDSNetç­‰å…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åœ¨çº¿è§†é¢‘å†…å®¹çš„å¿«é€Ÿå¢é•¿ï¼Œæœ‰æ•ˆçš„è§†é¢‘æ‘˜è¦æŠ€æœ¯å˜å¾—æ„ˆå‘é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–å•ä¸€æ¨¡æ€ï¼ˆé€šå¸¸æ˜¯è§†è§‰ï¼‰ï¼Œéš¾ä»¥æ•æ‰è§†é¢‘çš„å…¨éƒ¨è¯­ä¹‰ä¸°å¯Œæ€§ã€‚æœ¬æ–‡æå‡ºäº†MF2Summï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€å†…å®¹ç†è§£çš„æ–°å‹è§†é¢‘æ‘˜è¦æ¨¡å‹ï¼Œæ•´åˆäº†è§†è§‰å’Œå¬è§‰ä¿¡æ¯ã€‚MF2Summé‡‡ç”¨äº”ä¸ªé˜¶æ®µçš„å¤„ç†æµç¨‹ï¼šç‰¹å¾æå–ã€è·¨æ¨¡æ€æ³¨æ„åŠ›äº¤äº’ã€ç‰¹å¾èåˆã€ç‰‡æ®µé¢„æµ‹å’Œå…³é”®é•œå¤´é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMF2Summåœ¨SumMeå’ŒTVSumæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒF1åˆ†æ•°åˆ†åˆ«æ¯”DSNetæ¨¡å‹æé«˜äº†1.9%å’Œ0.6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†é¢‘æ‘˜è¦æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¿¡æ¯èåˆæ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆæ•æ‰è§†é¢‘çš„è§†è§‰å’Œå¬è§‰ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªä¾èµ–è§†è§‰æ¨¡æ€ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å’Œæ‘˜è¦è´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMF2Summçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šæ¨¡æ€èåˆï¼Œç»“åˆè§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶æ¥å»ºæ¨¡æ¨¡æ€é—´çš„ä¾èµ–å…³ç³»å’Œæ—¶é—´å¯¹åº”æ€§ï¼Œä»è€Œæå‡æ‘˜è¦çš„è¯­ä¹‰ä¸°å¯Œæ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMF2Summçš„æ•´ä½“æ¶æ„åŒ…æ‹¬äº”ä¸ªä¸»è¦é˜¶æ®µï¼šç‰¹å¾æå–ï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„GoogLeNetæå–è§†è§‰ç‰¹å¾å’ŒSoundNetæå–å¬è§‰ç‰¹å¾ï¼‰ã€è·¨æ¨¡æ€æ³¨æ„åŠ›äº¤äº’ã€ç‰¹å¾èåˆã€ç‰‡æ®µé¢„æµ‹ä»¥åŠå…³é”®é•œå¤´é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šMF2Summçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è·¨æ¨¡æ€Transformerå’Œå¯¹é½å¼•å¯¼çš„è‡ªæ³¨æ„åŠ›Transformerï¼Œè¿™ç§è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¨¡æ€é—´çš„å¤æ‚å…³ç³»ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æ‘˜è¦çš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰¹å¾æå–é˜¶æ®µï¼Œè§†è§‰ç‰¹å¾é€šè¿‡GoogLeNetæå–ï¼Œå¬è§‰ç‰¹å¾é€šè¿‡SoundNetæå–ã€‚å…³é”®é•œå¤´é€‰æ‹©ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰å’Œå†…æ ¸æ—¶é—´åˆ†å‰²ï¼ˆKTSï¼‰ç®—æ³•ï¼Œç¡®ä¿é€‰å‡ºçš„é•œå¤´åœ¨æ—¶é—´å’Œè¯­ä¹‰ä¸Šéƒ½å…·æœ‰é‡è¦æ€§ã€‚å®éªŒä¸­è¿˜ä¼˜åŒ–äº†æ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MF2Summåœ¨SumMeå’ŒTVSumæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶F1åˆ†æ•°åˆ†åˆ«æ¯”DSNetæ¨¡å‹æé«˜äº†1.9%å’Œ0.6%ã€‚è¯¥æ¨¡å‹åœ¨ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•çš„å¯¹æ¯”ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€è§†é¢‘æ‘˜è¦ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MF2Summçš„ç ”ç©¶æˆæœåœ¨è§†é¢‘å†…å®¹ç®¡ç†ã€ç¤¾äº¤åª’ä½“å¹³å°ã€æ•™è‚²è§†é¢‘æ‘˜è¦ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æœ‰æ•ˆæå–å’Œæ€»ç»“è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·å¿«é€Ÿè·å–æ‰€éœ€å†…å®¹ï¼Œæå‡ä¿¡æ¯è·å–çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ä¹Ÿå¯ä¸ºå…¶ä»–é¢†åŸŸçš„å¤šæ¨¡æ€å­¦ä¹ æä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid proliferation of online video content necessitates effective video summarization techniques. Traditional methods, often relying on a single modality (typically visual), struggle to capture the full semantic richness of videos. This paper introduces MF2Summ, a novel video summarization model based on multimodal content understanding, integrating both visual and auditory information. MF2Summ employs a five-stage process: feature extraction, cross-modal attention interaction, feature fusion, segment prediction, and key shot selection. Visual features are extracted using a pre-trained GoogLeNet model, while auditory features are derived using SoundNet. The core of our fusion mechanism involves a cross-modal Transformer and an alignment-guided self-attention Transformer, designed to effectively model inter-modal dependencies and temporal correspondences. Segment importance, location, and center-ness are predicted, followed by key shot selection using Non-Maximum Suppression (NMS) and the Kernel Temporal Segmentation (KTS) algorithm. Experimental results on the SumMe and TVSum datasets demonstrate that MF2Summ achieves competitive performance, notably improving F1-scores by 1.9\% and 0.6\% respectively over the DSNet model, and performing favorably against other state-of-the-art methods.

