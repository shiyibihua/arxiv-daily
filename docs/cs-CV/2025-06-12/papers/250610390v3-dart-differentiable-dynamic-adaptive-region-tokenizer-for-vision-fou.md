---
layout: default
title: DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Foundation Models
---

# DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10390" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10390v3</a>
  <a href="https://arxiv.org/pdf/2506.10390.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10390v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10390v3', 'DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shicheng Yin, Kaixuan Yin, Yang Liu, Weixing Chen, Liang Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: Code is available at https://github.com/HCPLab-SYSU/DART

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HCPLab-SYSU/DART)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDARTä»¥è§£å†³å›ºå®šç½‘æ ¼åˆ†å—çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€è‡ªé€‚åº”åˆ†å—` `è§†è§‰æ¨¡å‹` `å†…å®¹æ„ŸçŸ¥` `é«˜æ•ˆæ¨ç†` `å¤šæ¨¡æ€AI`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›ºå®šç½‘æ ¼åˆ†å—å™¨åœ¨æ•æ‰ç»†èŠ‚å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé™åˆ¶äº†è§†è§‰æ¨¡å‹çš„æ€§èƒ½ã€‚
2. DARTé€šè¿‡å¯å­¦ä¹ çš„åŒºåŸŸè¯„åˆ†å’Œåˆ†ä½æ•°åˆ†åŒºæ–¹æ³•ï¼ŒåŠ¨æ€ç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„è¡¥ä¸ï¼Œä¼˜åŒ–ä»¤ç‰Œå¯†åº¦åˆ†é…ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDARTæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€AIå’Œæœºå™¨äººé¢†åŸŸçš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ ‡å‡†å¤§è§„æ¨¡è§†è§‰æ¨¡å‹ï¼ˆå¦‚Vision Transformerå’ŒVision Mambaï¼‰ä½¿ç”¨çš„å†…å®¹æ— å…³å›ºå®šç½‘æ ¼åˆ†å—å™¨å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œå¯¼è‡´åœ¨æ•æ‰ç»†èŠ‚ä¸å†—ä½™è®¡ç®—ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DARTï¼Œä¸€ä¸ªå®Œå…¨å¯å¾®çš„åŠ¨æ€è‡ªé€‚åº”åŒºåŸŸåˆ†å—å™¨ã€‚DARTåˆ©ç”¨å¯å­¦ä¹ çš„åŒºåŸŸè¯„åˆ†å’ŒåŸºäºåˆ†ä½æ•°çš„åˆ†åŒºæ–¹æ³•ï¼Œåˆ›å»ºå†…å®¹æ„ŸçŸ¥çš„ä¸åŒå¤§å°çš„è¡¥ä¸ï¼Œæ™ºèƒ½åœ°å°†æ›´é«˜çš„ä»¤ç‰Œå¯†åº¦åˆ†é…ç»™ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡DARTçš„DeiT-Smallï¼ˆ2200ä¸‡å‚æ•°ï¼‰åœ¨æ¨ç†é€Ÿåº¦å‡ ä¹ç¿»å€çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¾¾åˆ°äº†DeiT-Baseï¼ˆ8600ä¸‡å‚æ•°ï¼‰çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”åˆ†å—çš„åŸåˆ™åœ¨å¯†é›†é¢„æµ‹å’Œæ—¶ç©ºè§†é¢‘ä»»åŠ¡ä¸­ä¹Ÿå±•ç°äº†å…¶é€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰æ¨¡å‹é€šå¸¸ä½¿ç”¨å›ºå®šç½‘æ ¼åˆ†å—å™¨ï¼Œè¿™ç§æ–¹æ³•æ— æ³•æ ¹æ®å†…å®¹çš„ä¸åŒè€Œçµæ´»è°ƒæ•´ï¼Œå¯¼è‡´åœ¨ç»†èŠ‚æ•æ‰å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDARTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯å­¦ä¹ çš„åŒºåŸŸè¯„åˆ†å’ŒåŸºäºåˆ†ä½æ•°çš„åˆ†åŒºæ–¹æ³•ï¼ŒåŠ¨æ€ç”Ÿæˆä¸åŒå¤§å°çš„å†…å®¹æ„ŸçŸ¥è¡¥ä¸ï¼Œä»è€Œæ™ºèƒ½åœ°åˆ†é…ä»¤ç‰Œå¯†åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDARTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¯å­¦ä¹ çš„åŒºåŸŸè¯„åˆ†æ¨¡å—ã€åˆ†ä½æ•°åˆ†åŒºæ¨¡å—å’ŒåŠ¨æ€è¡¥ä¸ç”Ÿæˆæ¨¡å—ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œå®ç°äº†è‡ªé€‚åº”çš„ä»¤ç‰Œç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šDARTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å®Œå…¨å¯å¾®çš„è®¾è®¡ï¼Œä½¿å¾—åˆ†å—è¿‡ç¨‹èƒ½å¤Ÿé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œä¼˜åŒ–ï¼Œä¸ä¼ ç»Ÿçš„å›ºå®šç½‘æ ¼æ–¹æ³•ç›¸æ¯”ï¼ŒDARTèƒ½å¤Ÿæ ¹æ®å†…å®¹åŠ¨æ€è°ƒæ•´è¡¥ä¸å¤§å°ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DARTä¸­ï¼ŒåŒºåŸŸè¯„åˆ†é€šè¿‡ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œåˆ†ä½æ•°åˆ†åŒºåˆ™ä¾æ®åŒºåŸŸè¯„åˆ†çš„åˆ†å¸ƒè¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†ä¿¡æ¯å¯†åº¦å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé…å¤‡DARTçš„DeiT-Smallåœ¨æ¨ç†é€Ÿåº¦ä¸Šå‡ ä¹ç¿»å€ï¼ŒåŒæ—¶æ€§èƒ½è¾¾åˆ°äº†DeiT-Baseçš„æ°´å¹³ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚è¿™ä¸€æˆæœä¸ºæœªæ¥çš„è§†è§‰æ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DARTçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€AIã€æœºå™¨äººæŠ€æœ¯å’Œå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜è§†è§‰æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ï¼ŒDARTå¯ä»¥å¸®åŠ©å®ç°æ›´æ™ºèƒ½çš„è§†è§‰ç†è§£å’Œäº¤äº’ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The content-agnostic, fixed-grid tokenizers used by standard large-scale vision models like Vision Transformer (ViT) and Vision Mamba (Vim) represent a fundamental performance bottleneck, creating a trade-off between capturing fine-grained detail and suffering from redundant computation. To resolve this dilemma, we introduce DART, a fully differentiable Dynamic Adaptive Region Tokenizer. DART employs learnable region scores and quantile-based partitioning to create content-aware patches of varying sizes, intelligently allocating a higher token density to information-rich regions. The impact of this approach is profound: it unlocks a more intelligent scaling paradigm, where a DART-equipped DeiT-Small (22M parameters) matches the performance of a DeiT-Base (86M) with nearly double the inference speed by efficiently capturing high-resolution details in key regions. Furthermore, the principle of adaptive tokenization proves its generality with clear benefits in dense prediction and spatiotemporal video tasks. We argue that by resolving the tokenizer bottleneck at its source, adaptive tokenization is a key component for building the next generation of more efficient and capable foundation models for multimodal AI, robotics, and content generation. Code is available at https://github.com/HCPLab-SYSU/DART.

