---
layout: default
title: Rethinking Random Masking in Self-Distillation on ViT
---

# Rethinking Random Masking in Self-Distillation on ViT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10582" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10582v3</a>
  <a href="https://arxiv.org/pdf/2506.10582.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10582v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10582v3', 'Rethinking Random Masking in Self-Distillation on ViT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jihyeon Seong, Hyunkyung Han

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-09-10)

**å¤‡æ³¨**: 4 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ”¹è¿›éšæœºæ©ç ç­–ç•¥ä»¥å¢å¼ºViTè‡ªè’¸é¦æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰å˜æ¢å™¨` `è‡ªè’¸é¦` `éšæœºæ©ç ` `å¤šè§†å›¾å¢å¼º` `å›¾åƒåˆ†ç±»` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªè’¸é¦æ–¹æ³•åœ¨éšæœºæ©ç ä½¿ç”¨ä¸Šå­˜åœ¨æ— å·®åˆ«çš„ç¼ºé™·ï¼Œå¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯çš„ä¸¢å¤±ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ©ç ç­–ç•¥ï¼Œä»…å¯¹å­¦ç”Ÿçš„å…¨å±€è§†å›¾åº”ç”¨éšæœºæ©ç ï¼Œä¿ç•™å…¶ä»–è§†å›¾ä¸å˜ï¼Œä»¥å¢å¼ºè®­ç»ƒæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¿™ç§ä¸å¯¹ç§°çš„æ©ç ç­–ç•¥åï¼Œæ¨¡å‹åœ¨mini-ImageNetæ•°æ®é›†ä¸Šçš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯è‡ªè’¸é¦æ¡†æ¶å¦‚DINOå¯¹è¿™äº›è¿›å±•è´¡çŒ®æ˜¾è‘—ã€‚ç„¶è€Œï¼Œéšæœºæ©ç çš„æ— å·®åˆ«ä½¿ç”¨å¯èƒ½ä¼šæ¶ˆé™¤å…³é”®çš„è¯­ä¹‰ä¿¡æ¯ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨è‡ªè’¸é¦è®¾ç½®ä¸­éšæœºæ©ç çš„ä½œç”¨ï¼Œæå‡ºä»…å¯¹å­¦ç”Ÿçš„å…¨å±€è§†å›¾åº”ç”¨éšæœºæ©ç ï¼ŒåŒæ—¶ä¿ç•™å­¦ç”Ÿçš„å±€éƒ¨è§†å›¾å’Œæ•™å¸ˆçš„å…¨å±€è§†å›¾ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œåˆ©ç”¨DINOçš„å¤šè§†å›¾å¢å¼ºæ–¹æ¡ˆï¼Œä¿æŒå¹²å‡€çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶é€šè¿‡æ©ç è¾“å…¥å¢å¼ºé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿™ç§ä¸å¯¹ç§°è®¾ç½®ä¸‹çš„éšæœºæ©ç èƒ½å¤Ÿäº§ç”Ÿæ›´é²æ£’ä¸”ç»†è‡´çš„æ³¨æ„åŠ›å›¾ï¼Œä»è€Œæå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªè’¸é¦æ¡†æ¶ä¸­éšæœºæ©ç çš„æ— å·®åˆ«ä½¿ç”¨æ‰€å¯¼è‡´çš„å…³é”®ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨éšæœºæ©ç çš„åº”ç”¨ä¸Šç¼ºä¹é’ˆå¯¹æ€§ï¼Œå½±å“äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„éšæœºæ©ç ç­–ç•¥ï¼Œä¸“é—¨å¯¹å­¦ç”Ÿçš„å…¨å±€è§†å›¾åº”ç”¨æ©ç ï¼Œè€Œä¿ç•™å­¦ç”Ÿçš„å±€éƒ¨è§†å›¾å’Œæ•™å¸ˆçš„å…¨å±€è§†å›¾ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨ä¿æŒè¯­ä¹‰ä¿¡æ¯çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶é€šè¿‡æ©ç è¾“å…¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŸºäºDINOæ¡†æ¶ï¼ŒåŒ…å«å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡å¤šè§†å›¾å¢å¼ºæŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­å…¨å±€è§†å›¾åº”ç”¨éšæœºæ©ç ï¼Œå±€éƒ¨è§†å›¾å’Œæ•™å¸ˆè§†å›¾ä¿æŒåŸæ ·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸å¯¹ç§°çš„éšæœºæ©ç ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æ— å·®åˆ«æ©ç ä½¿ç”¨å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å…³é”®ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œéšæœºæ©ç çš„æ¯”ä¾‹å’Œåº”ç”¨æ–¹å¼ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿å­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å¾—è¶³å¤Ÿçš„ç›‘ç£ä¿¡å·ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„è‡ªè’¸é¦æŸå¤±ï¼Œç»“åˆæ©ç è¾“å…¥è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨mini-ImageNetæ•°æ®é›†ä¸Šï¼Œé‡‡ç”¨ä¸å¯¹ç§°éšæœºæ©ç ç­–ç•¥çš„DINO-Tinyæ¨¡å‹ç›¸æ¯”äºåŸºçº¿æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºæ›´é²æ£’çš„æ³¨æ„åŠ›å›¾å’Œæ›´é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ã€‚é€šè¿‡æ”¹è¿›çš„è‡ªè’¸é¦ç­–ç•¥ï¼Œæ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡æ—¶èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒè¯­ä¹‰ä¿¡æ¯ï¼Œæå‡æ•´ä½“æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Transformers (ViTs) have demonstrated remarkable performance across a wide range of vision tasks. In particular, self-distillation frameworks such as DINO have contributed significantly to these advances. Within such frameworks, random masking is often utilized to improve training efficiency and introduce regularization. However, recent studies have raised concerns that indiscriminate random masking may inadvertently eliminate critical semantic information, motivating the development of more informed masking strategies. In this study, we explore the role of random masking in the self-distillation setting, focusing on the DINO framework. Specifically, we apply random masking exclusively to the student's global view, while preserving the student's local views and the teacher's global view in their original, unmasked forms. This design leverages DINO's multi-view augmentation scheme to retain clean supervision while inducing robustness through masked inputs. We evaluate our approach using DINO-Tiny on the mini-ImageNet dataset and show that random masking under this asymmetric setup yields more robust and fine-grained attention maps, ultimately enhancing downstream performance.

