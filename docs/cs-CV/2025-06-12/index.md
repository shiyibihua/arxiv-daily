---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-12
---

# cs.CVï¼ˆ2025-06-12ï¼‰

ğŸ“Š å…± **32** ç¯‡è®ºæ–‡
 | ğŸ”— **13** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (14 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250610465v1-medseg-r-reasoning-segmentation-in-medical-images-with-multimodal-la.html">MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models</a></td>
  <td>æå‡ºMedSeg-Rä»¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10465v1" data-paper-url="./papers/250610465v1-medseg-r-reasoning-segmentation-in-medical-images-with-multimodal-la.html" onclick="toggleFavorite(this, '2506.10465v1', 'MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250610395v2-pisces-an-auto-regressive-foundation-model-for-image-understanding-a.html">Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation</a></td>
  <td>æå‡ºPiscesä»¥è§£å†³å¤šæ¨¡æ€å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10395v2" data-paper-url="./papers/250610395v2-pisces-an-auto-regressive-foundation-model-for-image-understanding-a.html" onclick="toggleFavorite(this, '2506.10395v2', 'Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250610342v2-urbansensea-framework-for-quantitative-analysis-of-urban-streetscape.html">UrbanSense:A Framework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models</a></td>
  <td>æå‡ºUrbanSenseæ¡†æ¶ä»¥è§£å†³åŸå¸‚è¡—æ™¯å®šé‡åˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10342v2" data-paper-url="./papers/250610342v2-urbansensea-framework-for-quantitative-analysis-of-urban-streetscape.html" onclick="toggleFavorite(this, '2506.10342v2', 'UrbanSense:A Framework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250611253v1-lifting-data-tracing-machine-unlearning-to-knowledge-tracing-for-fou.html">Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models</a></td>
  <td>æå‡ºçŸ¥è¯†è¿½è¸ªæœºå™¨é—å¿˜ä»¥è§£å†³åŸºç¡€æ¨¡å‹çš„å¤šæ ·åŒ–éœ€æ±‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11253v1" data-paper-url="./papers/250611253v1-lifting-data-tracing-machine-unlearning-to-knowledge-tracing-for-fou.html" onclick="toggleFavorite(this, '2506.11253v1', 'Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250611178v1-brainmap-multimodal-graph-learning-for-efficient-brain-disease-local.html">BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization</a></td>
  <td>æå‡ºBrainMAPä»¥è§£å†³è„‘éƒ¨ç–¾ç—…å®šä½æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11178v1" data-paper-url="./papers/250611178v1-brainmap-multimodal-graph-learning-for-efficient-brain-disease-local.html" onclick="toggleFavorite(this, '2506.11178v1', 'BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250610430v1-mf2summ-multimodal-fusion-for-video-summarization-with-temporal-alig.html">MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment</a></td>
  <td>æå‡ºMF2Summä»¥è§£å†³è§†é¢‘æ‘˜è¦ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10430v1" data-paper-url="./papers/250610430v1-mf2summ-multimodal-fusion-for-video-summarization-with-temporal-alig.html" onclick="toggleFavorite(this, '2506.10430v1', 'MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250610337v2-geocad-local-geometry-controllable-cad-generation-with-large-languag.html">GeoCAD: Local Geometry-Controllable CAD Generation with Large Language Models</a></td>
  <td>æå‡ºGeoCADä»¥è§£å†³å±€éƒ¨å‡ ä½•å¯æ§CADç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10337v2" data-paper-url="./papers/250610337v2-geocad-local-geometry-controllable-cad-generation-with-large-languag.html" onclick="toggleFavorite(this, '2506.10337v2', 'GeoCAD: Local Geometry-Controllable CAD Generation with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250610328v1-towards-scalable-soap-note-generation-a-weakly-supervised-multimodal.html">Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework</a></td>
  <td>æå‡ºå¼±ç›‘ç£å¤šæ¨¡æ€æ¡†æ¶ä»¥ç”ŸæˆSOAPç¬”è®°ï¼Œè§£å†³ä¸´åºŠæ–‡æ¡£è´Ÿæ‹…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10328v1" data-paper-url="./papers/250610328v1-towards-scalable-soap-note-generation-a-weakly-supervised-multimodal.html" onclick="toggleFavorite(this, '2506.10328v1', 'Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250610967v2-beyond-attention-or-similarity-maximizing-conditional-diversity-for-.html">Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</a></td>
  <td>æå‡ºCDPrunerä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰tokenå†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10967v2" data-paper-url="./papers/250610967v2-beyond-attention-or-similarity-maximizing-conditional-diversity-for-.html" onclick="toggleFavorite(this, '2506.10967v2', 'Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250610807v1-prompts-to-summaries-zero-shot-language-guided-video-summarization.html">Prompts to Summaries: Zero-Shot Language-Guided Video Summarization</a></td>
  <td>æå‡ºé›¶-shotè§†é¢‘æ‘˜è¦æ–¹æ³•ä»¥è§£å†³ç”¨æˆ·æ„å›¾è¡¨è¾¾ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10807v1" data-paper-url="./papers/250610807v1-prompts-to-summaries-zero-shot-language-guided-video-summarization.html" onclick="toggleFavorite(this, '2506.10807v1', 'Prompts to Summaries: Zero-Shot Language-Guided Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250610685v3-defensive-adversarial-captcha-a-semantics-driven-framework-for-natur.html">Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural Adversarial Example Generation</a></td>
  <td>æå‡ºæ— æºå¯¹æŠ—CAPTCHAä»¥è§£å†³ä¼ ç»ŸCAPTCHAæ˜“å—æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10685v3" data-paper-url="./papers/250610685v3-defensive-adversarial-captcha-a-semantics-driven-framework-for-natur.html" onclick="toggleFavorite(this, '2506.10685v3', 'Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural Adversarial Example Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250610559v2-from-images-to-insights-explainable-biodiversity-monitoring-with-pla.html">From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations</a></td>
  <td>æå‡ºå¯è§£é‡Šçš„ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹æ¡†æ¶ä»¥è§£å†³ç”Ÿæ€ç³»ç»Ÿç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10559v2" data-paper-url="./papers/250610559v2-from-images-to-insights-explainable-biodiversity-monitoring-with-pla.html" onclick="toggleFavorite(this, '2506.10559v2', 'From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250610516v2-cogstream-context-guided-streaming-video-question-answering.html">CogStream: Context-guided Streaming Video Question Answering</a></td>
  <td>æå‡ºCogStreamä»¥è§£å†³æµåª’ä½“è§†é¢‘é—®ç­”ä¸­çš„ä¸Šä¸‹æ–‡ä¾èµ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10516v2" data-paper-url="./papers/250610516v2-cogstream-context-guided-streaming-video-question-answering.html" onclick="toggleFavorite(this, '2506.10516v2', 'CogStream: Context-guided Streaming Video Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250610890v1-creatiposter-towards-editable-and-controllable-multi-layer-graphic-d.html">CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation</a></td>
  <td>æå‡ºCreatiPosterä»¥è§£å†³å¯ç¼–è¾‘å¤šå±‚å›¾å½¢è®¾è®¡ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10890v1" data-paper-url="./papers/250610890v1-creatiposter-towards-editable-and-controllable-multi-layer-graphic-d.html" onclick="toggleFavorite(this, '2506.10890v1', 'CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250610353v4-motion-r1-enhancing-motion-generation-with-decomposed-chain-of-thoug.html">Motion-R1: Enhancing Motion Generation with Decomposed Chain-of-Thought and RL Binding</a></td>
  <td>æå‡ºMotion-R1ä»¥è§£å†³æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆä¸­çš„å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10353v4" data-paper-url="./papers/250610353v4-motion-r1-enhancing-motion-generation-with-decomposed-chain-of-thoug.html" onclick="toggleFavorite(this, '2506.10353v4', 'Motion-R1: Enhancing Motion Generation with Decomposed Chain-of-Thought and RL Binding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250610390v3-dart-differentiable-dynamic-adaptive-region-tokenizer-for-vision-fou.html">DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Foundation Models</a></td>
  <td>æå‡ºDARTä»¥è§£å†³å›ºå®šç½‘æ ¼åˆ†å—çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10390v3" data-paper-url="./papers/250610390v3-dart-differentiable-dynamic-adaptive-region-tokenizer-for-vision-fou.html" onclick="toggleFavorite(this, '2506.10390v3', 'DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250611302v3-tardis-stride-a-spatio-temporal-road-image-dataset-and-world-model-f.html">TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy</a></td>
  <td>æå‡ºSTRIDEæ•°æ®é›†ä¸TARDISæ¨¡å‹ä»¥è§£å†³åŠ¨æ€ç¯å¢ƒå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">egocentric</span> <span class="paper-tag">generalist agent</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11302v3" data-paper-url="./papers/250611302v3-tardis-stride-a-spatio-temporal-road-image-dataset-and-world-model-f.html" onclick="toggleFavorite(this, '2506.11302v3', 'TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250610452v1-towards-robust-multimodal-emotion-recognition-under-missing-modaliti.html">Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts</a></td>
  <td>æå‡ºCIDeræ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ç¼ºå¤±æ¨¡æ€å’Œåˆ†å¸ƒåç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10452v1" data-paper-url="./papers/250610452v1-towards-robust-multimodal-emotion-recognition-under-missing-modaliti.html" onclick="toggleFavorite(this, '2506.10452v1', 'Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250610915v1-m4v-multi-modal-mamba-for-text-to-video-generation.html">M4V: Multi-Modal Mamba for Text-to-Video Generation</a></td>
  <td>æå‡ºM4Væ¡†æ¶ä»¥è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10915v1" data-paper-url="./papers/250610915v1-m4v-multi-modal-mamba-for-text-to-video-generation.html" onclick="toggleFavorite(this, '2506.10915v1', 'M4V: Multi-Modal Mamba for Text-to-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250610790v1-human-robot-navigation-using-event-based-cameras-and-reinforcement-l.html">Human-Robot Navigation using Event-based Cameras and Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºäº‹ä»¶ç›¸æœºä¸å¼ºåŒ–å­¦ä¹ çš„äººæœºå¯¼èˆªæ§åˆ¶å™¨ä»¥è§£å†³å®æ—¶å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10790v1" data-paper-url="./papers/250610790v1-human-robot-navigation-using-event-based-cameras-and-reinforcement-l.html" onclick="toggleFavorite(this, '2506.10790v1', 'Human-Robot Navigation using Event-based Cameras and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250610816v1-occlusion-aware-3d-hand-object-pose-estimation-with-masked-autoencod.html">Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders</a></td>
  <td>æå‡ºåŸºäºæ©ç è‡ªç¼–ç å™¨çš„æ‰‹-ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ä»¥è§£å†³é®æŒ¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10816v1" data-paper-url="./papers/250610816v1-occlusion-aware-3d-hand-object-pose-estimation-with-masked-autoencod.html" onclick="toggleFavorite(this, '2506.10816v1', 'Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250610582v3-rethinking-random-masking-in-self-distillation-on-vit.html">Rethinking Random Masking in Self-Distillation on ViT</a></td>
  <td>æå‡ºæ”¹è¿›éšæœºæ©ç ç­–ç•¥ä»¥å¢å¼ºViTè‡ªè’¸é¦æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10582v3" data-paper-url="./papers/250610582v3-rethinking-random-masking-in-self-distillation-on-vit.html" onclick="toggleFavorite(this, '2506.10582v3', 'Rethinking Random Masking in Self-Distillation on ViT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250610335v1-pointgs-point-attention-aware-sparse-view-synthesis-with-gaussian-sp.html">PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting</a></td>
  <td>æå‡ºPointGSä»¥è§£å†³ç¨€ç–è§†å›¾åˆæˆä¸­çš„æ¸²æŸ“è´¨é‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10335v1" data-paper-url="./papers/250610335v1-pointgs-point-attention-aware-sparse-view-synthesis-with-gaussian-sp.html" onclick="toggleFavorite(this, '2506.10335v1', 'PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250610386v1-leveraging-6dof-pose-foundation-models-for-mapping-marine-sediment-b.html">Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial</a></td>
  <td>æå‡ºPoseIDONä»¥è§£å†³æµ·åº•æ²‰ç§¯ç‰©åŸ‹è—æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10386v1" data-paper-url="./papers/250610386v1-leveraging-6dof-pose-foundation-models-for-mapping-marine-sediment-b.html" onclick="toggleFavorite(this, '2506.10386v1', 'Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250610567v1-lrslam-low-rank-representation-of-signed-distance-fields-in-dense-vi.html">LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System</a></td>
  <td>æå‡ºLRSLAMä»¥è§£å†³å¯†é›†è§†è§‰SLAMä¸­çš„è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10567v1" data-paper-url="./papers/250610567v1-lrslam-low-rank-representation-of-signed-distance-fields-in-dense-vi.html" onclick="toggleFavorite(this, '2506.10567v1', 'LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250610981v1-scenecompleter-dense-3d-scene-completion-for-generative-novel-view-s.html">SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis</a></td>
  <td>æå‡ºSceneCompleterä»¥è§£å†³3Dåœºæ™¯è¡¥å…¨ä¸ç”Ÿæˆè§†å›¾ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10981v1" data-paper-url="./papers/250610981v1-scenecompleter-dense-3d-scene-completion-for-generative-novel-view-s.html" onclick="toggleFavorite(this, '2506.10981v1', 'SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250610840v1-post-training-quantization-for-video-matting.html">Post-Training Quantization for Video Matting</a></td>
  <td>æå‡ºåè®­ç»ƒé‡åŒ–æ¡†æ¶ä»¥è§£å†³è§†é¢‘æŠ å›¾æ¨¡å‹çš„èµ„æºé™åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10840v1" data-paper-url="./papers/250610840v1-post-training-quantization-for-video-matting.html" onclick="toggleFavorite(this, '2506.10840v1', 'Post-Training Quantization for Video Matting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250610574v2-dancechat-large-language-model-guided-music-to-dance-generation.html">DanceChat: Large Language Model-Guided Music-to-Dance Generation</a></td>
  <td>æå‡ºDanceChatä»¥è§£å†³éŸ³ä¹ä¸èˆè¹ˆç”Ÿæˆä¹‹é—´çš„è¯­ä¹‰å·®è·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion synthesis</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10574v2" data-paper-url="./papers/250610574v2-dancechat-large-language-model-guided-music-to-dance-generation.html" onclick="toggleFavorite(this, '2506.10574v2', 'DanceChat: Large Language Model-Guided Music-to-Dance Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250610391v1-reconmost-multi-layer-sea-temperature-reconstruction-with-observatio.html">ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion</a></td>
  <td>æå‡ºReconMOSTä»¥è§£å†³æµ·æ´‹æ¸©åº¦é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10391v1" data-paper-url="./papers/250610391v1-reconmost-multi-layer-sea-temperature-reconstruction-with-observatio.html" onclick="toggleFavorite(this, '2506.10391v1', 'ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250611314v1-hybiomass-global-hyperspectral-imagery-benchmark-dataset-for-evaluat.html">HyBiomass: Global Hyperspectral Imagery Benchmark Dataset for Evaluating Geospatial Foundation Models in Forest Aboveground Biomass Estimation</a></td>
  <td>æå‡ºHyBiomassæ•°æ®é›†ä»¥è§£å†³æ£®æ—ç”Ÿç‰©é‡ä¼°è®¡çš„åŸºå‡†è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">HSI</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11314v1" data-paper-url="./papers/250611314v1-hybiomass-global-hyperspectral-imagery-benchmark-dataset-for-evaluat.html" onclick="toggleFavorite(this, '2506.11314v1', 'HyBiomass: Global Hyperspectral Imagery Benchmark Dataset for Evaluating Geospatial Foundation Models in Forest Aboveground Biomass Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250610265v1-ground-reaction-force-estimation-via-time-aware-knowledge-distillati.html">Ground Reaction Force Estimation via Time-aware Knowledge Distillation</a></td>
  <td>æå‡ºæ—¶é—´æ„ŸçŸ¥çŸ¥è¯†è’¸é¦æ¡†æ¶ä»¥è§£å†³GRFä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10265v1" data-paper-url="./papers/250610265v1-ground-reaction-force-estimation-via-time-aware-knowledge-distillati.html" onclick="toggleFavorite(this, '2506.10265v1', 'Ground Reaction Force Estimation via Time-aware Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/250610568v2-dreamactor-h1-high-fidelity-human-product-demonstration-video-genera.html">DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</a></td>
  <td>æå‡ºåŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶ä»¥è§£å†³äººæœºäº§å“æ¼”ç¤ºè§†é¢‘ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10568v2" data-paper-url="./papers/250610568v2-dreamactor-h1-high-fidelity-human-product-demonstration-video-genera.html" onclick="toggleFavorite(this, '2506.10568v2', 'DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)