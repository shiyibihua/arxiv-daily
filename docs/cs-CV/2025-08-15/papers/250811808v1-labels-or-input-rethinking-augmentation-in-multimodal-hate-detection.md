---
layout: default
title: Labels or Input? Rethinking Augmentation in Multimodal Hate Detection
---

# Labels or Input? Rethinking Augmentation in Multimodal Hate Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11808" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11808v1</a>
  <a href="https://arxiv.org/pdf/2508.11808.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11808v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11808v1', 'Labels or Input? Rethinking Augmentation in Multimodal Hate Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sahajpreet Singh, Rongxin Ouyang, Subhayan Mukerjee, Kokil Jaidka

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.CY, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-08-15

**å¤‡æ³¨**: 13 pages, 2 figures, 7 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒé‡æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `æç¤ºä¼˜åŒ–` `æ•°æ®å¢å¼º` `é²æ£’æ€§æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹æ–¹æ³•åœ¨ç»†ç²’åº¦ç›‘ç£å’Œéšæ€§ä»‡æ¨è¨€è®ºçš„è¯†åˆ«ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„æç¤ºä¼˜åŒ–æ¡†æ¶å’Œå¤šæ¨¡æ€æ•°æ®å¢å¼ºç®¡é“ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æç¤ºè®¾è®¡å’Œç”Ÿæˆä¸­ç«‹æ•°æ®æ¥æå‡æ£€æµ‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–æç¤ºå’Œæ•°æ®ç»„æˆå¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼ŒInternVL2åœ¨äºŒå…ƒå’Œæ‰©å±•è®¾ç½®ä¸­å–å¾—äº†æœ€ä½³F1åˆ†æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£ç½‘ç»œå……æ–¥ç€å¤šæ¨¡æ€å†…å®¹ï¼Œä½¿å¾—æ£€æµ‹ä»‡æ¨è¡¨æƒ…åŒ…çš„æŒ‘æˆ˜åŠ å‰§ã€‚è¿™äº›è¡¨æƒ…åŒ…å¸¸é€šè¿‡æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å¾®å¦™äº’åŠ¨ä¼ è¾¾æœ‰å®³æ„å›¾ã€‚å°½ç®¡è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ç»†ç²’åº¦ç›‘ç£æ–¹é¢ä»æ˜¾ä¸è¶³ï¼Œå¹¶å®¹æ˜“å—åˆ°éšæ€§ä»‡æ¨è¨€è®ºçš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé‡æ–¹æ³•æ¥æ”¹å–„å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹ï¼Œé¦–å…ˆæ˜¯ä¸€ä¸ªæç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°å˜åŒ–æç¤ºç»“æ„ã€ç›‘ç£ç²’åº¦å’Œè®­ç»ƒæ¨¡å¼ï¼Œå‘ç°ç»“æ„åŒ–æç¤ºèƒ½åœ¨å°æ¨¡å‹ä¸­æå‡é²æ£’æ€§ï¼›å…¶æ¬¡ï¼Œä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€æ•°æ®å¢å¼ºç®¡é“ï¼Œç”Ÿæˆ2479ä¸ªåäº‹å®ä¸­ç«‹çš„è¡¨æƒ…åŒ…ï¼Œæœ‰æ•ˆå‡å°‘è™šå‡ç›¸å…³æ€§å¹¶æé«˜åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºåˆæˆæ•°æ®ä»¥è®­ç»ƒç¨³å¥å’Œå…¬å¹³çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹ä¸­å­˜åœ¨çš„ç»†ç²’åº¦ç›‘ç£ä¸è¶³å’Œéšæ€§ä»‡æ¨è¨€è®ºè¯†åˆ«å›°éš¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„æ–‡æœ¬ä¸å›¾åƒäº¤äº’æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¼˜åŒ–æç¤ºè®¾è®¡å’Œå¼•å…¥å¤šæ¨¡æ€æ•°æ®å¢å¼ºï¼Œæ¥æå‡æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°è°ƒæ•´æç¤ºç»“æ„å’Œç›‘ç£ç²’åº¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„æ¨¡å‹åœ¨å°æ ·æœ¬æƒ…å†µä¸‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæç¤ºä¼˜åŒ–æ¡†æ¶å’Œå¤šæ¨¡æ€æ•°æ®å¢å¼ºç®¡é“ã€‚æç¤ºä¼˜åŒ–æ¡†æ¶é€šè¿‡ä¸åŒçš„æç¤ºç»“æ„å’Œè®­ç»ƒæ¨¡å¼è¿›è¡Œå®éªŒï¼Œè€Œæ•°æ®å¢å¼ºç®¡é“åˆ™åˆ©ç”¨å¤šä»£ç†çš„LLM-VLMè®¾ç½®ç”Ÿæˆä¸­ç«‹è¡¨æƒ…åŒ…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ç³»ç»ŸåŒ–çš„æç¤ºä¼˜åŒ–å’Œé’ˆå¯¹æ€§çš„å¤šæ¨¡æ€æ•°æ®å¢å¼ºï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ¨¡å‹è®­ç»ƒå’Œæ•°æ®å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æç¤ºä¼˜åŒ–ä¸­ï¼Œè®¾è®¡äº†å¤šç§æç¤ºç»“æ„å’Œç›‘ç£ç²’åº¦ï¼Œå®éªŒè¡¨æ˜ç»“æ„åŒ–æç¤ºèƒ½æ˜¾è‘—æå‡å°æ¨¡å‹çš„é²æ£’æ€§ï¼›æ•°æ®å¢å¼ºç®¡é“ç”Ÿæˆçš„ä¸­ç«‹è¡¨æƒ…åŒ…æ•°é‡è¾¾åˆ°2479ä¸ªï¼Œæœ‰æ•ˆå‡å°‘äº†è™šå‡ç›¸å…³æ€§ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒå’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“æ„åŒ–æç¤ºè®¾è®¡å’Œæ•°æ®å¢å¼ºæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼ŒInternVL2åœ¨äºŒå…ƒå’Œæ‰©å±•è®¾ç½®ä¸­åˆ†åˆ«è¾¾åˆ°äº†æœ€ä½³F1åˆ†æ•°ã€‚é€šè¿‡å¼•å…¥2479ä¸ªåäº‹å®ä¸­ç«‹è¡¨æƒ…åŒ…ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†æœ‰æ•ˆå¢å¼ºï¼Œå‡å°‘äº†è™šå‡ç›¸å…³æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹ç›‘æµ‹ã€åœ¨çº¿ç¤¾åŒºç®¡ç†å’Œè‡ªåŠ¨åŒ–å†…å®¹å®¡æ ¸ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘ç½‘ç»œæš´åŠ›å’Œä»‡æ¨è¨€è®ºçš„ä¼ æ’­ï¼Œä¿ƒè¿›æ›´å®‰å…¨çš„ç½‘ç»œç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¸ºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡æä¾›å€Ÿé‰´ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The modern web is saturated with multimodal content, intensifying the challenge of detecting hateful memes, where harmful intent is often conveyed through subtle interactions between text and image under the guise of humor or satire. While recent advances in Vision-Language Models (VLMs) show promise, these models lack support for fine-grained supervision and remain susceptible to implicit hate speech. In this paper, we present a dual-pronged approach to improve multimodal hate detection. First, we propose a prompt optimization framework that systematically varies prompt structure, supervision granularity, and training modality. We show that prompt design and label scaling both influence performance, with structured prompts improving robustness even in small models, and InternVL2 achieving the best F1-scores across binary and scaled settings. Second, we introduce a multimodal data augmentation pipeline that generates 2,479 counterfactually neutral memes by isolating and rewriting the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup, successfully reduces spurious correlations and improves classifier generalization. Our approaches inspire new directions for building synthetic data to train robust and fair vision-language models. Our findings demonstrate that prompt structure and data composition are as critical as model size, and that targeted augmentation can support more trustworthy and context-sensitive hate detection.

