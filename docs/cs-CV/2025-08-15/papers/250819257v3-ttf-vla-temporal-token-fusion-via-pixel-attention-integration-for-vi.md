---
layout: default
title: TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models
---

# TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19257" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.19257v3</a>
  <a href="https://arxiv.org/pdf/2508.19257.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19257v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19257v3', 'TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-15 (Êõ¥Êñ∞: 2025-11-14)

**Â§áÊ≥®**: Accepted to AAAI 2026. Camera-ready version

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TTF‰ª•Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã‰∏≠ÁöÑÊó∂Èó¥‰ø°ÊÅØÁº∫Â§±ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂä®‰Ωú` `Êó∂Èó¥‰ª§ÁâåËûçÂêà` `Â§öÊ®°ÊÄÅËûçÂêà` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `Êú∫Âô®‰∫∫Êìç‰Ωú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑVLAÊ®°ÂûãÂú®ÈÄêÂ∏ßÂ§ÑÁêÜÊó∂ÂøΩËßÜ‰∫ÜÊó∂Èó¥‰ø°ÊÅØÔºåÂØºËá¥ÂØπËßÜËßâÂô™Â£∞ÁöÑËÑÜÂº±ÊÄßÂíåËøûÁª≠Â∏ß‰∏ÄËá¥ÊÄßÁöÑÁº∫Â§±„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÁöÑTTFÊñπÊ≥ïÈÄöËøáÊô∫ËÉΩÊï¥ÂêàÂéÜÂè≤‰∏éÂΩìÂâçËßÜËßâË°®Á§∫ÔºåÂà©Áî®ÂèåÁª¥Ê£ÄÊµãÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂Â¢ûÂº∫Êé®ÁêÜË¥®Èáè„ÄÇ
3. Âú®LIBERO‰∏äÔºåTTFÊñπÊ≥ïÁõ∏ËæÉ‰∫éÂü∫Á∫øÊèêÂçá‰∫Ü4.0‰∏™ÁôæÂàÜÁÇπÔºåÂú®SimplerEnvÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂàÜÂà´ÊèêÂçá‰∫Ü4.8%Âíå8.7%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®ÊØè‰∏™Êó∂Èó¥Ê≠•Áã¨Á´ãÂ§ÑÁêÜËßÜËßâËæìÂÖ•ÔºåÂøΩÁï•‰∫ÜÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Âõ∫ÊúâÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇËøôÁßçÈÄêÂ∏ßÂ§ÑÁêÜ‰ΩøÊ®°ÂûãÂÆπÊòìÂèóÂà∞ËßÜËßâÂô™Â£∞ÁöÑÂΩ±ÂìçÔºåÂêåÊó∂ÂøΩËßÜ‰∫ÜÊìç‰ΩúÂ∫èÂàó‰∏≠ËøûÁª≠Â∏ß‰πãÈó¥ÁöÑÊòæËëó‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËÆ≠ÁªÉÊó†ÂÖ≥ÁöÑÊñπÊ≥ï‚Äî‚ÄîÊó∂Èó¥‰ª§ÁâåËûçÂêàÔºàTTFÔºâÔºåÊô∫ËÉΩÊï¥ÂêàÂéÜÂè≤ÂíåÂΩìÂâçÁöÑËßÜËßâË°®Á§∫Ôºå‰ª•ÊèêÈ´òVLAÊé®ÁêÜË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÁªìÂêàÈ´òÊïàÁöÑÁÅ∞Â∫¶ÂÉèÁ¥†Â∑ÆÂºÇÂàÜÊûêÂíåÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄßËØÑ‰º∞ÔºåÈÄöËøáÁ°¨ËûçÂêàÁ≠ñÁï•ÂíåÂÖ≥ÈîÆÂ∏ßÈîöÂÆöÂÆûÁé∞ÈÄâÊã©ÊÄßÊó∂Èó¥‰ª§ÁâåËûçÂêàÔºåÈò≤Ê≠¢ÈîôËØØÁ¥ØÁßØ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂú®LIBERO„ÄÅSimplerEnvÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®ÈÄêÂ∏ßÂ§ÑÁêÜÊó∂ÂøΩÁï•Êó∂Èó¥‰ø°ÊÅØÁöÑÈóÆÈ¢òÔºåÂØºËá¥Ê®°ÂûãÂØπËßÜËßâÂô™Â£∞ÊïèÊÑü‰∏îÊó†Ê≥ïÊúâÊïàÂà©Áî®ËøûÁª≠Â∏ß‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑTTFÊñπÊ≥ïÈÄöËøáËûçÂêàÂéÜÂè≤ÂíåÂΩìÂâçÁöÑËßÜËßâË°®Á§∫ÔºåÂà©Áî®ÂèåÁª¥Ê£ÄÊµãÊäÄÊúØÔºåÁªìÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÈÄâÊã©ÊÄßÂú∞ËøõË°åÊó∂Èó¥‰ª§ÁâåËûçÂêàÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTTFÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰∏ÄÊòØÈ´òÊïàÁöÑÁÅ∞Â∫¶ÂÉèÁ¥†Â∑ÆÂºÇÂàÜÊûêÔºå‰∫åÊòØÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄßËØÑ‰º∞„ÄÇÈÄöËøáËøô‰∏§‰∏™Ê®°ÂùóÔºåÊ®°ÂûãËÉΩÂ§üÂú®ÂÖ≥ÈîÆÂ∏ß‰∏äËøõË°åÈîöÂÆöÔºåÈÅøÂÖçÈîôËØØÁöÑÁ¥ØÁßØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTTFÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈÄâÊã©ÊÄßÊü•ËØ¢Áü©ÈòµÈáçÁî®Á≠ñÁï•ÔºåËøô‰∏ÄÁ≠ñÁï•‰∏ç‰ªÖÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåËøò‰∏∫Áõ¥Êé•ÁöÑKQVÁü©ÈòµÈáçÁî®Á≠ñÁï•Êèê‰æõ‰∫ÜÊñ∞ÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÊòæÁ§∫Âá∫Âú®Âä†ÈÄüËÆ°ÁÆóÁöÑÂêåÊó∂ÊèêÈ´ò‰ªªÂä°ÊàêÂäüÁéáÁöÑÊΩúÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåTTFÈááÁî®‰∫ÜÁ°¨ËûçÂêàÁ≠ñÁï•ÂíåÂÖ≥ÈîÆÂ∏ßÈîöÂÆöÊäÄÊúØÔºåÁ°Æ‰øù‰∫ÜÊó∂Èó¥‰ª§ÁâåÁöÑÊúâÊïàËûçÂêàÔºåÈÅøÂÖç‰∫Ü‰ø°ÊÅØÁöÑ‰∏¢Â§±ÂíåÈîôËØØÁöÑ‰º†Êí≠„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåTTFÊñπÊ≥ïÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊèêÂçá‰∫Ü4.0‰∏™ÁôæÂàÜÁÇπÔºà72.4%ÂØπÊØî68.4%Âü∫Á∫øÔºâÔºåÂú®SimplerEnv‰∏äÂÆûÁé∞‰∫Ü4.8%ÁöÑÁõ∏ÂØπÊèêÂçáÔºåÂπ∂Âú®ÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÊèêÂçá‰∫Ü8.7%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéTTFÂú®‰∏çÂêåÁéØÂ¢É‰∏ãÂùáËÉΩÊúâÊïàÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®ÂåñÁîü‰∫ßÁ∫øÂíåÊô∫ËÉΩÁõëÊéßÁ≠âÂú∫ÊôØ„ÄÇÈÄöËøáÂ¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåTTFÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.

