---
layout: default
title: RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness
---

# RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05333" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05333v1</a>
  <a href="https://arxiv.org/pdf/2509.05333.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05333v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05333v1', 'RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junghyun Park, Tuan Anh Nguyen, Dugki Min

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRT-VLMä»¥è§£å†³ç°å®ä¸–ç•Œç‰©ä½“è¯†åˆ«çš„é²æ£’æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‰©ä½“è¯†åˆ«` `é²æ£’æ€§` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€èåˆ` `è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶` `åˆæˆæ•°æ®é›†` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç‰©ä½“è¯†åˆ«æ¨¡å‹åœ¨é¢å¯¹é¢†åŸŸè½¬ç§»æ—¶ï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œéš¾ä»¥é€‚åº”ä¸åŒçš„è§†è§‰ç¯å¢ƒã€‚
2. RT-VLMæ¡†æ¶é€šè¿‡ç”Ÿæˆå¸¦æœ‰å››ä¸ªçº¿ç´¢çš„åˆæˆæ•°æ®é›†ï¼Œç»“åˆè‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ï¼Œæå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRT-VLMåœ¨å¤šä¸ªé²æ£’æ€§åŸºå‡†æµ‹è¯•ä¸­æŒç»­è¶…è¶Šå¼ºåŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œä¸­çš„ç‰©ä½“è¯†åˆ«æ¨¡å‹å¸¸å¸¸é¢ä¸´é¢†åŸŸè½¬ç§»ï¼Œå¯¼è‡´å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚è¿™äº›è½¬ç§»åŒ…æ‹¬ä½çº§å›¾åƒç»Ÿè®¡çš„å˜åŒ–ã€ç‰©ä½“å§¿æ€å’Œè§†è§’çš„å˜åŒ–ã€éƒ¨åˆ†é®æŒ¡ä»¥åŠç›¸é‚»ç±»åˆ«ä¹‹é—´çš„è§†è§‰æ··æ·†ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†é‡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆRT-VLMï¼‰æ¡†æ¶ï¼ŒåŸºäºç‹¬ç‰¹çš„åˆæˆæ•°æ®é›†ç”Ÿæˆç®¡é“ï¼Œç”Ÿæˆå¸¦æœ‰å››ä¸ªçº¿ç´¢çš„å›¾åƒï¼šç²¾ç¡®çš„è¾¹ç•Œæ¡†ã€ç±»åˆ«åç§°ã€è¯¦ç»†çš„ç‰©ä½“çº§æè¿°ä»¥åŠæ•´ä¸ªåœºæ™¯çš„ç»¼åˆä¸Šä¸‹æ–‡æè¿°ã€‚é€šè¿‡å¯¹Llama 3.2 11B Vision Instructè¿›è¡Œé«˜æ•ˆçš„ç›‘ç£è°ƒä¼˜ï¼ŒRT-VLMåœ¨å¤šä¸ªé²æ£’æ€§åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œè¡¨æ˜ç»“æ„åŒ–çš„å¤šæ¨¡æ€è¯æ®ä¸æ˜ç¡®çš„è‡ªæˆ‘æ‰¹åˆ¤å¾ªç¯çš„ç»“åˆæ˜¯å®ç°å¯é å’Œå¯è½¬ç§»è§†è§‰ç†è§£çš„æœ‰æ•ˆé€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°ä»£ç‰©ä½“è¯†åˆ«æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­å› é¢†åŸŸè½¬ç§»å¯¼è‡´çš„å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä½çº§å›¾åƒç»Ÿè®¡å˜åŒ–ã€ç‰©ä½“å§¿æ€å˜åŒ–ã€éƒ¨åˆ†é®æŒ¡å’Œè§†è§‰æ··æ·†æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRT-VLMæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç”Ÿæˆå¸¦æœ‰å››ä¸ªçº¿ç´¢çš„åˆæˆæ•°æ®é›†ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶å¼•å…¥è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶æ¥è¿­ä»£ä¿®æ­£æ¨¡å‹è¾“å‡ºï¼Œä»è€Œæå‡é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRT-VLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆç”Ÿæˆå¸¦æœ‰å››ä¸ªçº¿ç´¢çš„åˆæˆå›¾åƒæ•°æ®é›†ï¼Œç„¶åå¯¹Llama 3.2 11B Vision Instructè¿›è¡Œé«˜æ•ˆçš„ç›‘ç£è°ƒä¼˜ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹é¦–å…ˆè¾“å‡ºå››ä¸ªçº¿ç´¢ï¼Œç„¶åå¯¹è¿™äº›çº¿ç´¢è¿›è¡Œè‡ªæˆ‘å®¡æŸ¥å’Œè¿­ä»£ä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šRT-VLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ç»“æ„åŒ–çš„å¤šæ¨¡æ€è¯æ®ä¸è‡ªæˆ‘æ‰¹åˆ¤å¾ªç¯ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è§†è§‰ç†è§£æ–¹æ³•ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é¢å¯¹ä¸åŒé¢†åŸŸè½¬ç§»æ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å››ä¸ªçº¿ç´¢çš„ç”Ÿæˆï¼Œå¹¶é€šè¿‡å‚æ•°é«˜æ•ˆçš„è°ƒä¼˜ç­–ç•¥æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRT-VLMåœ¨å¤šä¸ªé²æ£’æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹ä½çº§å›¾åƒç»Ÿè®¡å˜åŒ–å’Œç‰©ä½“å§¿æ€å˜åŒ–æ—¶ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç‰©ä½“è¯†åˆ«ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒRT-VLMæ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­æ¨å¹¿ï¼Œæ¨åŠ¨è§†è§‰ç†è§£æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real world deployments often expose modern object recognition models to domain shifts that precipitate a severe drop in accuracy. Such shifts encompass (i) variations in low level image statistics, (ii) changes in object pose and viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent classes. To mitigate this degradation, we introduce the Re-Thinking Vision Language Model (RT-VLM) framework. The foundation of this framework is a unique synthetic dataset generation pipeline that produces images annotated with "4-Clues": precise bounding boxes, class names, detailed object-level captions, and a comprehensive context-level caption for the entire scene. We then perform parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this resource. At inference time, a two stage Re-Thinking scheme is executed: the model first emits its own four clues, then re examines these responses as evidence and iteratively corrects them. Across robustness benchmarks that isolate individual domain shifts, RT-VLM consistently surpasses strong baselines. These findings indicate that the integration of structured multimodal evidence with an explicit self critique loop constitutes a promising route toward reliable and transferable visual understanding.

