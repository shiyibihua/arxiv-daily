---
layout: default
title: Large Pre-Trained Models for Bimanual Manipulation in 3D
---

# Large Pre-Trained Models for Bimanual Manipulation in 3D

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20579v1</a>
  <a href="https://arxiv.org/pdf/2509.20579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20579v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20579v1', 'Large Pre-Trained Models for Bimanual Manipulation in 3D')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanna Yurchyk, Wei-Di Chang, Gregory Dudek, David Meger

**åˆ†ç±»**: cs.CV, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

**å¤‡æ³¨**: Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid Robots

**DOI**: [10.1109/Humanoids65713.2025.11203079](https://doi.org/10.1109/Humanoids65713.2025.11203079)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨é¢„è®­ç»ƒViTæ³¨æ„åŠ›å›¾å¢å¼º3DåŒè‡‚æ“ä½œçš„ç­–ç•¥å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŒè‡‚æ“ä½œ` `æœºå™¨äººå­¦ä¹ ` `è§†è§‰Transformer` `æ³¨æ„åŠ›æœºåˆ¶` `ä½“ç´ è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºä½“ç´ çš„åŒè‡‚æ“ä½œç­–ç•¥ç¼ºä¹å¯¹å›¾åƒè¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨é¢„è®­ç»ƒViTæ¨¡å‹DINOv2çš„æ³¨æ„åŠ›å›¾ä½œä¸ºåƒç´ çº§æ˜¾è‘—æ€§å¾—åˆ†ï¼Œå¹¶å°†å…¶èå…¥3Dä½“ç´ è¡¨ç¤ºä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RLBenchåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†åŒè‡‚æ“ä½œç­–ç•¥çš„æ€§èƒ½ï¼Œå¹³å‡ç»å¯¹æ”¹è¿›8.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å°†é¢„è®­ç»ƒè§†è§‰Transformerçš„æ³¨æ„åŠ›å›¾é›†æˆåˆ°ä½“ç´ è¡¨ç¤ºä¸­ï¼Œä»¥å¢å¼ºåŒè‡‚æœºå™¨äººæ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»è‡ªç›‘ç£ViTæ¨¡å‹DINOv2ä¸­æå–æ³¨æ„åŠ›å›¾ï¼Œå¹¶å°†å…¶è§£é‡Šä¸ºRGBå›¾åƒä¸Šçš„åƒç´ çº§æ˜¾è‘—æ€§å¾—åˆ†ã€‚è¿™äº›å›¾è¢«æå‡åˆ°3Dä½“ç´ ç½‘æ ¼ä¸­ï¼Œä»è€Œäº§ç”Ÿä½“ç´ çº§çš„è¯­ä¹‰çº¿ç´¢ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°è¡Œä¸ºå…‹éš†ç­–ç•¥ä¸­ã€‚å½“é›†æˆåˆ°æœ€å…ˆè¿›çš„åŸºäºä½“ç´ çš„ç­–ç•¥ä¸­æ—¶ï¼Œæˆ‘ä»¬çš„æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾åŒ–åœ¨RLBenchåŒè‡‚åŸºå‡†æµ‹è¯•çš„æ‰€æœ‰ä»»åŠ¡ä¸­ï¼Œå¹³å‡ç»å¯¹æ”¹è¿›ä¸º8.2%ï¼Œç›¸å¯¹å¢ç›Šä¸º21.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨æå‡åŒè‡‚æœºå™¨äººåœ¨3Dç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ã€‚ç°æœ‰åŸºäºä½“ç´ çš„ç­–ç•¥è™½ç„¶èƒ½å¤Ÿå¤„ç†3Dä¿¡æ¯ï¼Œä½†ç¼ºä¹å¯¹å›¾åƒä¸­è¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´ç­–ç•¥å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å®Œæˆå¤æ‚æ“ä½œä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é¢„è®­ç»ƒè§†è§‰Transformer (ViT) æ¨¡å‹å­¦ä¹ åˆ°çš„å›¾åƒè¯­ä¹‰ä¿¡æ¯èå…¥åˆ°ä½“ç´ è¡¨ç¤ºä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨ViTçš„æ³¨æ„åŠ›æœºåˆ¶æå–å›¾åƒä¸­æ¯ä¸ªåƒç´ çš„é‡è¦æ€§ï¼Œå¹¶å°†è¿™äº›æ³¨æ„åŠ›æƒé‡æ˜ å°„åˆ°3Dä½“ç´ ç©ºé—´ï¼Œä»è€Œä¸ºä½“ç´ è¡¨ç¤ºèµ‹äºˆè¯­ä¹‰ä¿¡æ¯ã€‚è¿™æ ·ï¼Œæœºå™¨äººç­–ç•¥å¯ä»¥æ›´å¥½åœ°ç†è§£åœºæ™¯ï¼Œå¹¶åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼›2) åˆ©ç”¨é¢„è®­ç»ƒçš„DINOv2æ¨¡å‹æå–æ³¨æ„åŠ›å›¾ï¼Œè¯¥æ³¨æ„åŠ›å›¾åæ˜ äº†å›¾åƒä¸­æ¯ä¸ªåƒç´ çš„æ˜¾è‘—æ€§ï¼›3) å°†2Dæ³¨æ„åŠ›å›¾åæŠ•å½±åˆ°3Dä½“ç´ ç½‘æ ¼ä¸­ï¼Œä¸ºæ¯ä¸ªä½“ç´ èµ‹äºˆä¸€ä¸ªè¯­ä¹‰å¾—åˆ†ï¼›4) å°†å¸¦æœ‰è¯­ä¹‰ä¿¡æ¯çš„ä½“ç´ è¡¨ç¤ºè¾“å…¥åˆ°è¡Œä¸ºå…‹éš†ç­–ç•¥ä¸­è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†é¢„è®­ç»ƒè§†è§‰Transformerçš„æ³¨æ„åŠ›æœºåˆ¶ä¸ä½“ç´ è¡¨ç¤ºç›¸ç»“åˆã€‚ä¸ç›´æ¥ä½¿ç”¨RGBå›¾åƒæˆ–æ·±åº¦å›¾åƒä½œä¸ºè¾“å…¥ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æå–å’Œåˆ©ç”¨å›¾åƒä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæå‡ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é¿å…ä»å¤´å¼€å§‹è®­ç»ƒè§†è§‰æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨DINOv2ä½œä¸ºé¢„è®­ç»ƒçš„ViTæ¨¡å‹ï¼Œå› ä¸ºå®ƒåœ¨è‡ªç›‘ç£å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°ä¸°å¯Œçš„å›¾åƒç‰¹å¾ã€‚æ³¨æ„åŠ›å›¾è¢«è§£é‡Šä¸ºåƒç´ çº§çš„æ˜¾è‘—æ€§å¾—åˆ†ï¼Œå¹¶é€šè¿‡åæŠ•å½±çš„æ–¹å¼æ˜ å°„åˆ°3Dä½“ç´ ç½‘æ ¼ä¸­ã€‚è¡Œä¸ºå…‹éš†ç­–ç•¥é‡‡ç”¨æ ‡å‡†çš„ç½‘ç»œç»“æ„ï¼ŒæŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RLBenchåŒè‡‚åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸æœ€å…ˆè¿›çš„åŸºäºä½“ç´ çš„ç­–ç•¥ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å¹³å‡ç»å¯¹æ”¹è¿›ä¸º8.2%ï¼Œç›¸å¯¹å¢ç›Šä¸º21.9%ã€‚è¿™è¡¨æ˜åˆ©ç”¨é¢„è®­ç»ƒViTæ³¨æ„åŠ›å›¾èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºåŒè‡‚æœºå™¨äººçš„æ“ä½œèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åŒè‡‚æœºå™¨äººæ“ä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šå·¥ä¸šè‡ªåŠ¨åŒ–ä¸­çš„è£…é…ã€æ‹£é€‰å’ŒåŒ…è£…ï¼›åŒ»ç–—é¢†åŸŸçš„è¾…åŠ©æ‰‹æœ¯å’Œåº·å¤è®­ç»ƒï¼›å®¶åº­æœåŠ¡ä¸­çš„ç‰©å“æ•´ç†å’Œæ¸…æ´ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººçš„æ“ä½œèƒ½åŠ›ï¼Œå¯ä»¥æé«˜ç”Ÿäº§æ•ˆç‡ã€é™ä½äººå·¥æˆæœ¬ï¼Œå¹¶æ”¹å–„äººä»¬çš„ç”Ÿæ´»è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate the integration of attention maps from a pre-trained Vision Transformer into voxel representations to enhance bimanual robotic manipulation. Specifically, we extract attention maps from DINOv2, a self-supervised ViT model, and interpret them as pixel-level saliency scores over RGB images. These maps are lifted into a 3D voxel grid, resulting in voxel-level semantic cues that are incorporated into a behavior cloning policy. When integrated into a state-of-the-art voxel-based policy, our attention-guided featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all tasks in the RLBench bimanual benchmark.

