---
layout: default
title: VIMD: Monocular Visual-Inertial Motion and Depth Estimation
---

# VIMD: Monocular Visual-Inertial Motion and Depth Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19713" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19713v2</a>
  <a href="https://arxiv.org/pdf/2509.19713.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19713v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19713v2', 'VIMD: Monocular Visual-Inertial Motion and Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Saimouli Katragadda, Guoquan Huang

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VIMDï¼šå•ç›®è§†è§‰æƒ¯æ€§è¿åŠ¨ä¸æ·±åº¦ä¼°è®¡ï¼Œæå‡æœºå™¨äººå’ŒXRçš„3Dæ„ŸçŸ¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡` `å¤šè§†è§’å‡ ä½•` `æ·±åº¦å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª` `å¢å¼ºç°å®` `ä¸‰ç»´é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ç²¾åº¦å’Œæ•ˆç‡ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³æœºå™¨äººå’ŒXRç­‰é¢†åŸŸå¯¹3Dè§†è§‰æ„ŸçŸ¥çš„éœ€æ±‚ã€‚
2. VIMDæ¡†æ¶åˆ©ç”¨å¤šè§†è§’ä¿¡æ¯è¿­ä»£ç»†åŒ–åƒç´ å°ºåº¦ï¼Œé¿å…äº†å…¨å±€ä»¿å°„æ¨¡å‹æ‹Ÿåˆçš„å±€é™æ€§ï¼Œæå‡äº†æ·±åº¦ä¼°è®¡ç²¾åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVIMDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå³ä½¿åœ¨æç¨€ç–æ·±åº¦ç‚¹æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒé«˜ç²¾åº¦ï¼Œå¹¶å…·å¤‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å•ç›®è§†è§‰æƒ¯æ€§è¿åŠ¨å’Œæ·±åº¦(VIMD)å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨åŸºäºMSCKFçš„ç²¾ç¡®é«˜æ•ˆçš„å•ç›®è§†è§‰æƒ¯æ€§è¿åŠ¨è·Ÿè¸ªæ¥ä¼°è®¡ç¨ å¯†åº¦é‡æ·±åº¦ã€‚VIMDçš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¤šè§†è§’ä¿¡æ¯è¿­ä»£åœ°ç»†åŒ–æ¯ä¸ªåƒç´ çš„å°ºåº¦ï¼Œè€Œä¸æ˜¯åƒå…ˆå‰çš„å·¥ä½œé‚£æ ·å…¨å±€æ‹Ÿåˆä¸å˜çš„ä»¿å°„æ¨¡å‹ã€‚VIMDæ¡†æ¶å…·æœ‰é«˜åº¦æ¨¡å—åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿä¸å„ç§ç°æœ‰çš„æ·±åº¦ä¼°è®¡éª¨å¹²ç½‘ç»œå…¼å®¹ã€‚æˆ‘ä»¬åœ¨TartanAirå’ŒVOIDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨AR Tableæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ¯ä¸ªå›¾åƒåªæœ‰10-20ä¸ªåº¦é‡æ·±åº¦ç‚¹çš„æç¨€ç–ç‚¹æƒ…å†µä¸‹ï¼ŒVIMDä¹Ÿèƒ½å®ç°å‡ºè‰²çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚è¿™ä½¿å¾—æ‰€æå‡ºçš„VIMDæˆä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²çš„å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å…¶å¼ºå¤§çš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ä¸ºå„ç§åœºæ™¯æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å•ç›®è§†è§‰æƒ¯æ€§ç³»ç»Ÿä¸­çš„ç¨ å¯†æ·±åº¦ä¼°è®¡é—®é¢˜ã€‚ç°æœ‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•é€šå¸¸ä¾èµ–äºå…¨å±€ä»¿å°„æ¨¡å‹æ‹Ÿåˆï¼Œè¿™åœ¨å¤æ‚åœºæ™¯ä¸‹å®¹æ˜“å¤±æ•ˆï¼Œå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚æ­¤å¤–ï¼Œè®¡ç®—æ•ˆç‡ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šè§†è§’å‡ ä½•ä¿¡æ¯ï¼Œè¿­ä»£åœ°ç»†åŒ–æ¯ä¸ªåƒç´ çš„å°ºåº¦ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„ç¨ å¯†æ·±åº¦ä¼°è®¡ã€‚é€šè¿‡èåˆè§†è§‰å’Œæƒ¯æ€§ä¿¡æ¯ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ä¼°è®¡ç›¸æœºè¿åŠ¨ï¼Œè¿›è€Œä¸ºå¤šè§†è§’æ·±åº¦ä¼°è®¡æä¾›æ›´å¯é çš„çº¦æŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVIMDæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŸºäºMSCKFçš„å•ç›®è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ï¼Œç”¨äºç²¾ç¡®ä¼°è®¡ç›¸æœºè¿åŠ¨ï¼›2) æ·±åº¦ä¼°è®¡éª¨å¹²ç½‘ç»œï¼Œç”¨äºä»å•ç›®å›¾åƒä¸­é¢„æµ‹åˆå§‹æ·±åº¦å›¾ï¼›3) å¤šè§†è§’æ·±åº¦èåˆæ¨¡å—ï¼Œåˆ©ç”¨ç›¸æœºè¿åŠ¨ä¿¡æ¯å’Œåˆå§‹æ·±åº¦å›¾ï¼Œè¿­ä»£åœ°ç»†åŒ–æ¯ä¸ªåƒç´ çš„å°ºåº¦ï¼Œå¾—åˆ°æœ€ç»ˆçš„ç¨ å¯†æ·±åº¦å›¾ã€‚è¯¥æ¡†æ¶å…·æœ‰é«˜åº¦æ¨¡å—åŒ–ï¼Œå¯ä»¥çµæ´»åœ°é€‰æ‹©ä¸åŒçš„æ·±åº¦ä¼°è®¡éª¨å¹²ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šVIMDçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¿­ä»£å¼çš„å¤šè§†è§’æ·±åº¦ç»†åŒ–æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å…¨å±€ä»¿å°„æ¨¡å‹æ‹Ÿåˆæ–¹æ³•ä¸åŒï¼ŒVIMDé€šè¿‡é€åƒç´ åœ°ä¼˜åŒ–å°ºåº¦ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚åœºæ™¯ä¸‹çš„æ·±åº¦å˜åŒ–ã€‚æ­¤å¤–ï¼ŒVIMDæ¡†æ¶å……åˆ†åˆ©ç”¨äº†è§†è§‰å’Œæƒ¯æ€§ä¿¡æ¯ï¼Œæé«˜äº†æ·±åº¦ä¼°è®¡çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVIMDæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åŸºäºMSCKFçš„è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ï¼Œæä¾›ç²¾ç¡®çš„ç›¸æœºä½å§¿ä¼°è®¡ï¼›2) å¯é€‰æ‹©çš„æ·±åº¦ä¼°è®¡éª¨å¹²ç½‘ç»œï¼Œä¾‹å¦‚ResNetæˆ–DenseNetï¼›3) å¤šè§†è§’æ·±åº¦èåˆæ¨¡å—ï¼Œé‡‡ç”¨è¿­ä»£ä¼˜åŒ–çš„æ–¹å¼ï¼Œæœ€å°åŒ–æ·±åº¦ä¸€è‡´æ€§è¯¯å·®å’Œé‡æŠ•å½±è¯¯å·®ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡å’Œç½‘ç»œç»“æ„é€‰æ‹©ä¼šå½±å“æœ€ç»ˆçš„æ€§èƒ½ï¼Œéœ€è¦åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VIMDåœ¨TartanAirå’ŒVOIDæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨AR Tableæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è¡¨æ˜ï¼ŒVIMDå…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚å³ä½¿åœ¨æ¯ä¸ªå›¾åƒåªæœ‰10-20ä¸ªåº¦é‡æ·±åº¦ç‚¹çš„æç¨€ç–æƒ…å†µä¸‹ï¼ŒVIMDä»ç„¶èƒ½å¤Ÿå®ç°é«˜ç²¾åº¦çš„æ·±åº¦ä¼°è®¡ï¼Œè¿™ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¾ˆå¤§çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VIMDåœ¨æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®(AR)ã€è™šæ‹Ÿç°å®(VR)ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ç²¾ç¡®çš„ç¨ å¯†æ·±åº¦ä¼°è®¡å¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå®ç°è‡ªä¸»å¯¼èˆªå’Œé¿éšœã€‚åœ¨AR/VRåº”ç”¨ä¸­ï¼ŒVIMDå¯ä»¥æä¾›æ›´çœŸå®çš„3Dåœºæ™¯é‡å»ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒVIMDåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„é«˜æ•ˆæ€§èƒ½ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºç§»åŠ¨æœºå™¨äººå’Œå¯ç©¿æˆ´è®¾å¤‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR. In this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking. At the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale, instead of globally fitting an invariant affine model as in the prior work. The VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones. We conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset. Our results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points as few as 10-20 metric depth points per image. This makes the proposed VIMD a practical solution for deployment in resource constrained settings, while its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.

