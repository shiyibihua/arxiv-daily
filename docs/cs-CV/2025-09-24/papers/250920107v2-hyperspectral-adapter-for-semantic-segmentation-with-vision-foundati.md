---
layout: default
title: Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models
---

# Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20107" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20107v2</a>
  <a href="https://arxiv.org/pdf/2509.20107.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20107v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20107v2', 'Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Juana Valeria Hurtado, Rohit Mohan, Abhinav Valada

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24 (æ›´æ–°: 2025-09-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹çš„è¶…å…‰è°±é€‚é…å™¨ï¼Œæå‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é«˜å…‰è°±å›¾åƒ` `è¯­ä¹‰åˆ†å‰²` `è§†è§‰åŸºç¡€æ¨¡å‹` `Transformer` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é«˜å…‰è°±è¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¾èµ–ä¸ºRGBå›¾åƒè®¾è®¡çš„æ¶æ„ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨é«˜å…‰è°±æ•°æ®çš„ä¸°å¯Œä¿¡æ¯ã€‚
2. æå‡ºä¸€ç§é«˜å…‰è°±é€‚é…å™¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç»“åˆå…‰è°±Transformerå’Œç©ºé—´å…ˆéªŒæ¨¡å—æå–ç‰¹å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜å…‰è°±æˆåƒ(HSI)æ•è·ç©ºé—´ä¿¡æ¯ä»¥åŠè·¨å¤šä¸ªçª„æ³¢æ®µçš„å¯†é›†å…‰è°±æµ‹é‡ã€‚è¿™ç§ä¸°å¯Œçš„å…‰è°±å†…å®¹æœ‰æ½œåŠ›ä¿ƒè¿›é²æ£’çš„æœºå™¨äººæ„ŸçŸ¥ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¤æ‚ææ–™ç»„æˆã€å˜åŒ–çš„å…‰ç…§æˆ–å…¶ä»–è§†è§‰æŒ‘æˆ˜æ€§æ¡ä»¶çš„ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œå½“å‰çš„HSIè¯­ä¹‰åˆ†å‰²æ–¹æ³•ç”±äºä¾èµ–äºé’ˆå¯¹RGBè¾“å…¥ä¼˜åŒ–çš„æ¶æ„å’Œå­¦ä¹ æ¡†æ¶è€Œè¡¨ç°ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é«˜å…‰è°±é€‚é…å™¨ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹æ¥æœ‰æ•ˆåœ°ä»é«˜å…‰è°±æ•°æ®ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¶æ„åŒ…å«ä¸€ä¸ªå…‰è°±Transformerå’Œä¸€ä¸ªé¢‘è°±æ„ŸçŸ¥ç©ºé—´å…ˆéªŒæ¨¡å—ï¼Œä»¥æå–ä¸°å¯Œçš„ç©ºé—´-å…‰è°±ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡æ€æ„ŸçŸ¥äº¤äº’å—ï¼Œé€šè¿‡ä¸“ç”¨çš„æå–å’Œæ³¨å…¥æœºåˆ¶ï¼Œä¿ƒè¿›é«˜å…‰è°±è¡¨ç¤ºå’Œå†»ç»“çš„è§†è§‰Transformerç‰¹å¾çš„æœ‰æ•ˆé›†æˆã€‚åœ¨ä¸‰ä¸ªåŸºå‡†è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„å®ç°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶ç›´æ¥ä½¿ç”¨HSIè¾“å…¥ï¼Œä¼˜äºåŸºäºè§†è§‰å’Œé«˜å…‰è°±çš„åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯ç›´æ¥å°†RGBå›¾åƒçš„åˆ†å‰²æ¨¡å‹åº”ç”¨äºHSIæ•°æ®ï¼Œå¿½ç•¥äº†HSIæ•°æ®çš„å…‰è°±ç‰¹æ€§ï¼›äºŒæ˜¯ç¼ºä¹åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªé«˜å…‰è°±é€‚é…å™¨ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ä¸é«˜å…‰è°±æ•°æ®ç›¸ç»“åˆã€‚é€šè¿‡ä¸“é—¨è®¾è®¡çš„æ¨¡å—ï¼Œæå–å¹¶èåˆé«˜å…‰è°±æ•°æ®çš„ç©ºé—´å’Œå…‰è°±ç‰¹å¾ï¼ŒåŒæ—¶åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ï¼Œä»è€Œæå‡è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **å…‰è°±Transformer**ï¼šç”¨äºæå–é«˜å…‰è°±æ•°æ®çš„å…‰è°±ç‰¹å¾ã€‚2) **é¢‘è°±æ„ŸçŸ¥ç©ºé—´å…ˆéªŒæ¨¡å—**ï¼šç”¨äºæå–ç©ºé—´ä¿¡æ¯ï¼Œå¹¶ç»“åˆå…‰è°±ä¿¡æ¯ã€‚3) **æ¨¡æ€æ„ŸçŸ¥äº¤äº’å—**ï¼šç”¨äºèåˆé«˜å…‰è°±ç‰¹å¾å’Œé¢„è®­ç»ƒè§†è§‰Transformerçš„ç‰¹å¾ã€‚æ•´ä¸ªæµç¨‹æ˜¯å…ˆåˆ†åˆ«æå–é«˜å…‰è°±å’Œè§†è§‰ç‰¹å¾ï¼Œç„¶åé€šè¿‡äº¤äº’å—è¿›è¡Œèåˆï¼Œæœ€åè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ¨¡æ€æ„ŸçŸ¥äº¤äº’å—çš„è®¾è®¡ã€‚è¯¥æ¨¡å—é€šè¿‡ä¸“ç”¨çš„æå–å’Œæ³¨å…¥æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é«˜å…‰è°±æ•°æ®çš„ç‰¹å¾èå…¥åˆ°é¢„è®­ç»ƒçš„è§†è§‰Transformerä¸­ï¼Œé¿å…äº†ç®€å•æ‹¼æ¥æˆ–åŠ æƒèåˆå¯¼è‡´çš„ä¿¡æ¯æŸå¤±ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é«˜å…‰è°±æ•°æ®çš„ç‰¹æ€§ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…‰è°±Transformeré‡‡ç”¨æ ‡å‡†çš„Transformerç»“æ„ï¼Œä½†è¾“å…¥æ˜¯é«˜å…‰è°±æ•°æ®çš„å…‰è°±å‘é‡ã€‚é¢‘è°±æ„ŸçŸ¥ç©ºé—´å…ˆéªŒæ¨¡å—é€šè¿‡å·ç§¯æ“ä½œæå–ç©ºé—´ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å°†ç©ºé—´ç‰¹å¾ä¸å…‰è°±ç‰¹å¾è¿›è¡Œèåˆã€‚æ¨¡æ€æ„ŸçŸ¥äº¤äº’å—åŒ…å«æå–æ¨¡å—å’Œæ³¨å…¥æ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºæå–é«˜å…‰è°±ç‰¹å¾å’Œå°†é«˜å…‰è°±ç‰¹å¾æ³¨å…¥åˆ°è§†è§‰Transformerçš„ä¸­é—´å±‚ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªè‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚ä¸ç°æœ‰çš„åŸºäºè§†è§‰å’Œé«˜å…‰è°±çš„åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨mIoUç­‰æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€é¥æ„Ÿå›¾åƒåˆ†æã€å†œä¸šç›‘æµ‹ã€ç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸã€‚é€šè¿‡é«˜å…‰è°±å›¾åƒçš„è¯­ä¹‰åˆ†å‰²ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«é“è·¯åœºæ™¯ä¸­çš„ç‰©ä½“ã€å†œä½œç‰©çš„ç§ç±»å’Œå¥åº·çŠ¶å†µã€ä»¥åŠç¯å¢ƒæ±¡æŸ“çš„ç¨‹åº¦ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„å†³ç­–æä¾›æ›´å¯é çš„ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–é«˜å…‰è°±å›¾åƒå¤„ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms. Extensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at https://hsi-adapter.cs.uni-freiburg.de.

