---
layout: default
title: OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction
---

# OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23541" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23541v1</a>
  <a href="https://arxiv.org/pdf/2509.23541.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23541v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23541v1', 'OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyang Li, Jinyuan Qu, Lei Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-28

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OVSeg3Rï¼šé€šè¿‡3Dé‡å»ºä»2Då­¦ä¹ å¼€æ”¾è¯æ±‡å®ä¾‹åˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡åˆ†å‰²` `3Då®ä¾‹åˆ†å‰²` `3Dé‡å»º` `çŸ¥è¯†è¿ç§»` `ä¼ªæ ‡ç­¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥ç›´æ¥åˆ©ç”¨2Då¼€æ”¾è¯æ±‡åˆ†å‰²çš„ä¼˜åŠ¿è¿›è¡Œ3Dåœºæ™¯çš„å¼€æ”¾è¯æ±‡å®ä¾‹åˆ†å‰²ã€‚
2. OVSeg3Ré€šè¿‡3Dé‡å»ºå»ºç«‹2Då’Œ3Dçš„å¯¹åº”å…³ç³»ï¼Œå°†2Dçš„å¼€æ”¾è¯æ±‡çŸ¥è¯†è¿ç§»åˆ°3Dåœºæ™¯ä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOVSeg3Råœ¨ScanNet200æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOVSeg3Rçš„è®­ç»ƒæ–¹æ¡ˆï¼Œæ—¨åœ¨å€ŸåŠ©3Dé‡å»ºï¼Œä»å……åˆ†ç ”ç©¶çš„2Dæ„ŸçŸ¥æ¨¡å‹ä¸­å­¦ä¹ å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²ã€‚OVSeg3Rç›´æ¥é‡‡ç”¨ä»2Dè§†é¢‘é‡å»ºçš„åœºæ™¯ä½œä¸ºè¾“å…¥ï¼Œé¿å…äº†æ˜‚è´µçš„æ‰‹åŠ¨è°ƒæ•´ï¼ŒåŒæ—¶ä½¿è¾“å…¥ä¸å®é™…åº”ç”¨å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨3Dé‡å»ºæ¨¡å‹æä¾›çš„2Dåˆ°3Dçš„å¯¹åº”å…³ç³»ï¼ŒOVSeg3Rå°†æ¯ä¸ªè§†å›¾çš„2Då®ä¾‹æ©ç é¢„æµ‹ï¼ˆä»å¼€æ”¾è¯æ±‡2Dæ¨¡å‹è·å¾—ï¼‰æŠ•å½±åˆ°3Dï¼Œä»è€Œä¸ºè¯¥è§†å›¾å¯¹åº”çš„å­åœºæ™¯ç”Ÿæˆæ³¨é‡Šã€‚ä¸ºäº†é¿å…ç”±äº2Dåˆ°3Dçš„éƒ¨åˆ†æ³¨é‡Šè€Œé”™è¯¯åœ°å¼•å…¥å‡é˜³æ€§ä½œä¸ºç›‘ç£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†å›¾çº§å®ä¾‹åˆ’åˆ†ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†é¢„æµ‹åˆ’åˆ†åˆ°å„è‡ªçš„è§†å›¾ä»¥è¿›è¡Œç›‘ç£ï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œç”±äº3Dé‡å»ºæ¨¡å‹å€¾å‘äºè¿‡åº¦å¹³æ»‘å‡ ä½•ç»†èŠ‚ï¼Œå› æ­¤åƒä¸»æµ3Dåˆ†å‰²æ–¹æ³•ä¸­é‚£æ ·ï¼Œä»…åŸºäºå‡ ä½•å½¢çŠ¶å°†é‡å»ºç‚¹èšç±»ä¸ºä»£è¡¨æ€§çš„è¶…ç‚¹å¯èƒ½ä¼šå¿½ç•¥å‡ ä½•å½¢çŠ¶ä¸Šä¸æ˜¾è‘—çš„å¯¹è±¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†2Då®ä¾‹è¾¹ç•Œæ„ŸçŸ¥è¶…ç‚¹ï¼Œè¯¥è¶…ç‚¹åˆ©ç”¨2Dæ©ç æ¥çº¦æŸè¶…ç‚¹èšç±»ï¼Œé˜²æ­¢è¶…ç‚¹è¿åå®ä¾‹è¾¹ç•Œã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒOVSeg3Rä¸ä»…å°†æœ€å…ˆè¿›çš„å°é—­è¯æ±‡3Då®ä¾‹åˆ†å‰²æ¨¡å‹æ‰©å±•åˆ°å¼€æ”¾è¯æ±‡ï¼Œè€Œä¸”å¤§å¤§ç¼©å°äº†å°¾éƒ¨ç±»å’Œå¤´éƒ¨ç±»ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œæœ€ç»ˆåœ¨ScanNet200åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†+2.3 mAPçš„æ€»ä½“æ”¹è¿›ã€‚æ­¤å¤–ï¼Œåœ¨æ ‡å‡†å¼€æ”¾è¯æ±‡è®¾ç½®ä¸‹ï¼ŒOVSeg3Rè¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•çº¦+7.1 mAPçš„æ–°ç±»åˆ«ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„3Då®ä¾‹åˆ†å‰²æ–¹æ³•é€šå¸¸ä¾èµ–äºå°é—­è¯æ±‡è¡¨ï¼Œæ— æ³•è¯†åˆ«è®­ç»ƒé›†ä¸­æœªå‡ºç°çš„æ–°ç±»åˆ«ã€‚ç›´æ¥è®­ç»ƒå¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æ¨¡å‹éœ€è¦å¤§é‡çš„3Dæ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å·²æœ‰çš„2Då¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨3Dé‡å»ºæŠ€æœ¯ï¼Œå°†2Då›¾åƒçš„å¼€æ”¾è¯æ±‡åˆ†å‰²ç»“æœæŠ•å½±åˆ°3Dç©ºé—´ï¼Œç”Ÿæˆ3Dåœºæ™¯çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œè®­ç»ƒå¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æ¨¡å‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨2Dæ¨¡å‹çš„çŸ¥è¯†ï¼Œé¿å…ç›´æ¥æ ‡æ³¨3Dæ•°æ®çš„æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOVSeg3Rçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨2Då¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹å¯¹è§†é¢‘å¸§è¿›è¡Œå®ä¾‹åˆ†å‰²ï¼›2) ä½¿ç”¨3Dé‡å»ºç®—æ³•ï¼ˆå¦‚COLMAPï¼‰é‡å»º3Dåœºæ™¯ï¼›3) å°†2Dåˆ†å‰²ç»“æœæŠ•å½±åˆ°3Dç©ºé—´ï¼Œç”Ÿæˆ3Då®ä¾‹åˆ†å‰²çš„ä¼ªæ ‡ç­¾ï¼›4) ä½¿ç”¨ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è®­ç»ƒ3Då®ä¾‹åˆ†å‰²æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨3Dé‡å»ºæŠ€æœ¯ï¼Œå°†2Då¼€æ”¾è¯æ±‡åˆ†å‰²çš„çŸ¥è¯†è¿ç§»åˆ°3Dåœºæ™¯ä¸­ï¼Œä»è€Œå®ç°äº†å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†è§†å›¾çº§å®ä¾‹åˆ’åˆ†ç®—æ³•å’Œ2Då®ä¾‹è¾¹ç•Œæ„ŸçŸ¥è¶…ç‚¹èšç±»æ–¹æ³•ï¼Œä»¥æé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡å’Œåˆ†å‰²ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè§†å›¾çº§å®ä¾‹åˆ’åˆ†ç®—æ³•æ—¨åœ¨è§£å†³ç”±äº2Dåˆ°3DæŠ•å½±çš„ä¸å®Œæ•´æ€§å¯¼è‡´çš„å‡é˜³æ€§é—®é¢˜ã€‚2Då®ä¾‹è¾¹ç•Œæ„ŸçŸ¥è¶…ç‚¹èšç±»æ–¹æ³•é€šè¿‡å¼•å…¥2Dåˆ†å‰²ç»“æœçš„è¾¹ç•Œä¿¡æ¯ï¼Œçº¦æŸè¶…ç‚¹çš„èšç±»è¿‡ç¨‹ï¼Œé˜²æ­¢è¶…ç‚¹è·¨è¶Šå®ä¾‹è¾¹ç•Œï¼Œä»è€Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±æˆ–DiceæŸå¤±ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OVSeg3Råœ¨ScanNet200æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ ‡å‡†å¼€æ”¾è¯æ±‡è®¾ç½®ä¸‹ï¼ŒOVSeg3Rè¶…è¿‡äº†ä¹‹å‰çš„æ–¹æ³•çº¦+7.1 mAPçš„æ–°ç±»åˆ«ã€‚æ­¤å¤–ï¼ŒOVSeg3Rè¿˜ç¼©å°äº†å°¾éƒ¨ç±»å’Œå¤´éƒ¨ç±»ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œæœ€ç»ˆåœ¨ScanNet200åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†+2.3 mAPçš„æ€»ä½“æ”¹è¿›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒOVSeg3Ræ˜¯ä¸€ç§æœ‰æ•ˆçš„å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æŠ€æœ¯è¯†åˆ«å’Œç†è§£å‘¨å›´ç¯å¢ƒä¸­çš„å„ç§ç‰©ä½“ï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œå¯¼èˆªå’Œäº¤äº’ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œè¯¥æŠ€æœ¯å¯ä»¥å¸®åŠ©è½¦è¾†è¯†åˆ«é“è·¯ä¸Šçš„å„ç§äº¤é€šå‚ä¸è€…å’Œéšœç¢ç‰©ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨å¢å¼ºç°å®é¢†åŸŸï¼Œè¯¥æŠ€æœ¯å¯ä»¥ç”¨äºå°†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯è¿›è¡Œç²¾ç¡®çš„èåˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we propose a training scheme called OVSeg3R to learn open-vocabulary 3D instance segmentation from well-studied 2D perception models with the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes from 2D videos as input, avoiding costly manual adjustment while aligning input with real-world applications. By exploiting the 2D to 3D correspondences provided by 3D reconstruction models, OVSeg3R projects each view's 2D instance mask predictions, obtained from an open-vocabulary 2D model, onto 3D to generate annotations for the view's corresponding sub-scene. To avoid incorrectly introduced false positives as supervision due to partial annotations from 2D to 3D, we propose a View-wise Instance Partition algorithm, which partitions predictions to their respective views for supervision, stabilizing the training process. Furthermore, since 3D reconstruction models tend to over-smooth geometric details, clustering reconstructed points into representative super-points based solely on geometry, as commonly done in mainstream 3D segmentation methods, may overlook geometrically non-salient objects. We therefore introduce 2D Instance Boundary-aware Superpoint, which leverages 2D masks to constrain the superpoint clustering, preventing superpoints from violating instance boundaries. With these designs, OVSeg3R not only extends a state-of-the-art closed-vocabulary 3D instance segmentation model to open-vocabulary, but also substantially narrows the performance gap between tail and head classes, ultimately leading to an overall improvement of +2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard open-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP on the novel classes, further validating its effectiveness.

