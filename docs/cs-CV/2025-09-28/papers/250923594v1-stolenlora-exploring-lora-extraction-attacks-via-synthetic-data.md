---
layout: default
title: StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data
---

# StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23594" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23594v1</a>
  <a href="https://arxiv.org/pdf/2509.23594.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23594v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23594v1', 'StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixu Wang, Yan Teng, Yingchun Wang, Xingjun Ma

**åˆ†ç±»**: cs.CR, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-28

**å¤‡æ³¨**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStolenLoRAï¼Œåˆ©ç”¨åˆæˆæ•°æ®å®ç°å¯¹LoRAé€‚é…æ¨¡å‹çš„æå–æ”»å‡»ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LoRAæå–` `æ¨¡å‹æå–æ”»å‡»` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `åˆæˆæ•°æ®` `åŠç›‘ç£å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹å®‰å…¨` `è§†è§‰æ¨¡å‹é€‚é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LoRAç­‰PEFTæ–¹æ³•è™½ç„¶é«˜æ•ˆï¼Œä½†å…¶ç´§å‡‘æ€§ä½¿å…¶æ˜“å—æ¨¡å‹æå–æ”»å‡»ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ã€‚
2. StolenLoRAåˆ©ç”¨LLMç”Ÿæˆåˆæˆæ•°æ®ï¼Œå¹¶ç»“åˆåˆ†æ­§åŠç›‘ç£å­¦ä¹ ï¼Œé«˜æ•ˆæå–LoRAé€‚é…æ¨¡å‹çš„åŠŸèƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒStolenLoRAåœ¨ä½æŸ¥è¯¢æ¬¡æ•°ä¸‹å…·æœ‰é«˜æ”»å‡»æˆåŠŸç‡ï¼Œæ­ç¤ºäº†LoRAé€‚é…æ¨¡å‹çš„è„†å¼±æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚LoRAå·²ç»æ”¹å˜äº†è§†è§‰æ¨¡å‹çš„é€‚é…æ–¹å¼ï¼Œå®ç°äº†å®šåˆ¶æ¨¡å‹çš„å¿«é€Ÿéƒ¨ç½²ã€‚ç„¶è€Œï¼ŒLoRAé€‚é…çš„ç´§å‡‘æ€§å¼•å…¥äº†æ–°çš„å®‰å…¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å®¹æ˜“å—åˆ°æ¨¡å‹æå–æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æå–æ”»å‡»ï¼Œåä¸ºLoRAæå–ï¼Œå®ƒåŸºäºå…¬å…±é¢„è®­ç»ƒæ¨¡å‹æå–LoRAé€‚é…æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æå–æ–¹æ³•ï¼Œç§°ä¸ºStolenLoRAï¼Œå®ƒè®­ç»ƒä¸€ä¸ªæ›¿ä»£æ¨¡å‹ï¼Œåˆ©ç”¨åˆæˆæ•°æ®æå–LoRAé€‚é…æ¨¡å‹çš„åŠŸèƒ½ã€‚StolenLoRAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆ¶ä½œæœ‰æ•ˆçš„æ•°æ®ç”Ÿæˆæç¤ºï¼Œå¹¶ç»“åˆåŸºäºåˆ†æ­§çš„åŠç›‘ç£å­¦ä¹ ï¼ˆDSLï¼‰ç­–ç•¥ï¼Œä»¥æœ€å¤§é™åº¦åœ°ä»æœ‰é™çš„æŸ¥è¯¢ä¸­è·å–ä¿¡æ¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜äº†StolenLoRAçš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿åœ¨æ”»å‡»è€…å’Œå—å®³è€…æ¨¡å‹ä½¿ç”¨ä¸åŒé¢„è®­ç»ƒéª¨å¹²ç½‘ç»œçš„è·¨éª¨å¹²åœºæ™¯ä¸­ï¼Œä»…ä½¿ç”¨1ä¸‡æ¬¡æŸ¥è¯¢å³å¯è¾¾åˆ°é«˜è¾¾96.60%çš„æ”»å‡»æˆåŠŸç‡ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†LoRAé€‚é…æ¨¡å‹å¯¹æ­¤ç±»æå–çš„ç‰¹å®šè„†å¼±æ€§ï¼Œå¹¶å¼ºè°ƒäº†è¿«åˆ‡éœ€è¦é’ˆå¯¹PEFTæ–¹æ³•é‡èº«å®šåˆ¶çš„å¼ºå¤§é˜²å¾¡æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸€ç§åŸºäºå¤šæ ·åŒ–LoRAéƒ¨ç½²çš„åˆæ­¥é˜²å¾¡ç­–ç•¥ï¼Œçªå‡ºäº†å…¶å‡è½»æ­¤ç±»æ”»å‡»çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LoRAé€‚é…æ¨¡å‹å®¹æ˜“å—åˆ°æ¨¡å‹æå–æ”»å‡»çš„é—®é¢˜ã€‚ç°æœ‰çš„æ¨¡å‹æå–æ”»å‡»æ–¹æ³•é€šå¸¸é’ˆå¯¹å®Œæ•´æ¨¡å‹ï¼Œè€Œå¿½ç•¥äº†PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰çš„ç‰¹æ®Šæ€§ï¼Œç¼ºä¹é’ˆå¯¹LoRAé€‚é…æ¨¡å‹çš„æœ‰æ•ˆæ”»å‡»æ‰‹æ®µã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆåœ°æå–LoRAé€‚é…æ¨¡å‹çš„åŠŸèƒ½æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªæ›¿ä»£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿LoRAé€‚é…æ¨¡å‹çš„è¡Œä¸ºã€‚é€šè¿‡ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒæ›¿ä»£æ¨¡å‹ï¼Œæ”»å‡»è€…å¯ä»¥åœ¨ä¸ç›´æ¥è®¿é—®å—å®³è€…æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæå–å…¶åŠŸèƒ½ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œå¹¶é‡‡ç”¨åŸºäºåˆ†æ­§çš„åŠç›‘ç£å­¦ä¹ ï¼ˆDSLï¼‰ç­–ç•¥ï¼Œå¯ä»¥æé«˜æå–æ•ˆç‡å’Œæ”»å‡»æˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStolenLoRAçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æç¤ºç”Ÿæˆ**ï¼šåˆ©ç”¨LLMç”Ÿæˆç”¨äºåˆæˆæ•°æ®çš„æç¤ºã€‚2) **æ•°æ®åˆæˆ**ï¼šä½¿ç”¨ç”Ÿæˆçš„æç¤ºï¼Œé€šè¿‡æŸ¥è¯¢å—å®³è€…LoRAé€‚é…æ¨¡å‹ï¼Œç”Ÿæˆåˆæˆæ•°æ®é›†ã€‚3) **æ›¿ä»£æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒæ›¿ä»£æ¨¡å‹ï¼Œä½¿å…¶æ¨¡ä»¿å—å®³è€…æ¨¡å‹çš„åŠŸèƒ½ã€‚4) **åˆ†æ­§åŠç›‘ç£å­¦ä¹ **ï¼šåˆ©ç”¨DSLç­–ç•¥ï¼Œé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„æœªæ ‡è®°æ•°æ®è¿›è¡ŒæŸ¥è¯¢ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šStolenLoRAçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **é’ˆå¯¹LoRAé€‚é…æ¨¡å‹çš„æå–æ”»å‡»**ï¼šä¸“æ³¨äºæå–LoRAé€‚é…æ¨¡å‹çš„åŠŸèƒ½ï¼Œè€Œéæ•´ä¸ªæ¨¡å‹ã€‚2) **åŸºäºLLMçš„æç¤ºç”Ÿæˆ**ï¼šåˆ©ç”¨LLMç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œæé«˜äº†æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚3) **åŸºäºåˆ†æ­§çš„åŠç›‘ç£å­¦ä¹ ï¼ˆDSLï¼‰**ï¼šé€šè¿‡é€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„æœªæ ‡è®°æ•°æ®è¿›è¡ŒæŸ¥è¯¢ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ”»å‡»æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æç¤ºç”Ÿæˆé˜¶æ®µï¼Œè®ºæ–‡ä½¿ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„æç¤ºæ¨¡æ¿ï¼Œä»¥å¼•å¯¼LLMç”Ÿæˆå¤šæ ·åŒ–çš„æ•°æ®ã€‚åœ¨DSLé˜¶æ®µï¼Œè®ºæ–‡é‡‡ç”¨äº†åŸºäºæ¨¡å‹é¢„æµ‹åˆ†æ­§çš„é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©é¢„æµ‹ç»“æœå·®å¼‚æœ€å¤§çš„æ ·æœ¬è¿›è¡ŒæŸ¥è¯¢ã€‚æ›¿ä»£æ¨¡å‹çš„ç»“æ„ä¸å—å®³è€…æ¨¡å‹ç±»ä¼¼ï¼Œä½†å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–å™¨é‡‡ç”¨Adamã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

StolenLoRAåœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”»å‡»æ•ˆæœï¼Œå³ä½¿åœ¨è·¨éª¨å¹²ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨1ä¸‡æ¬¡æŸ¥è¯¢å³å¯è¾¾åˆ°é«˜è¾¾96.60%çš„æ”»å‡»æˆåŠŸç‡ã€‚ä¸æ²¡æœ‰ä½¿ç”¨LLMç”Ÿæˆpromptçš„æ–¹æ³•ç›¸æ¯”ï¼Œæ”»å‡»æˆåŠŸç‡æœ‰æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRAé€‚é…æ¨¡å‹å®¹æ˜“å—åˆ°åŸºäºåˆæˆæ•°æ®çš„æå–æ”»å‡»ï¼Œå¹¶éªŒè¯äº†StolenLoRAçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯„ä¼°å’Œå¢å¼ºåŸºäºLoRAç­‰PEFTæ–¹æ³•çš„æ¨¡å‹çš„å®‰å…¨æ€§ã€‚é€šè¿‡æ¨¡æ‹Ÿæ”»å‡»ï¼Œå¯ä»¥å‘ç°LoRAé€‚é…æ¨¡å‹çš„æ½œåœ¨æ¼æ´ï¼Œå¹¶å¼€å‘ç›¸åº”çš„é˜²å¾¡æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›å¯¹PEFTæ–¹æ³•å®‰å…¨æ€§çš„æ›´æ·±å…¥ç†è§£ï¼Œå¹¶æ¨åŠ¨æ›´å®‰å…¨çš„æ¨¡å‹éƒ¨ç½²å®è·µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.

