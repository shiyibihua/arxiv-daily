---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-11-12
---

# cs.CVï¼ˆ2025-11-12ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (14 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction-matching" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction & Matching) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251110699v1-dualvision-arthronav-investigating-opportunities-to-enhance-localiza.html">DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras</a></td>
  <td>DualVision ArthroNavï¼šåˆ©ç”¨å¤–éƒ¨ç›¸æœºå¢å¼ºå›¾åƒå¼•å¯¼å…³èŠ‚é•œå¯¼èˆªçš„å®šä½ä¸é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10699v1" onclick="toggleFavorite(this, '2511.10699v1', 'DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251109724v1-palms-modular-image-based-floor-plan-localization-leveraging-depth-f.html">PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model</a></td>
  <td>æå‡ºPALMS+ä»¥è§£å†³å®¤å†…å®šä½ç²¾åº¦ä¸è¶³é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09724v1" onclick="toggleFavorite(this, '2511.09724v1', 'PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251109397v2-ougs-active-view-selection-via-object-aware-uncertainty-estimation-i.html">OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS</a></td>
  <td>OUGSï¼šåŸºäºå¯¹è±¡æ„ŸçŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡çš„3DGSä¸»åŠ¨è§†è§’é€‰æ‹©</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09397v2" onclick="toggleFavorite(this, '2511.09397v2', 'OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251109771v2-storm-segment-track-and-object-re-localization-from-a-single-image.html">STORM: Segment, Track, and Object Re-Localization from a Single Image</a></td>
  <td>æå‡ºSTORMï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œå®ç°å•å›¾åƒçš„ç‰©ä½“åˆ†å‰²ã€è·Ÿè¸ªå’Œé‡å®šä½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09771v2" onclick="toggleFavorite(this, '2511.09771v2', 'STORM: Segment, Track, and Object Re-Localization from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251109502v1-dreampose3d-hallucinative-diffusion-with-prompt-learning-for-3d-huma.html">DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation</a></td>
  <td>DreamPose3Dï¼šç»“åˆæç¤ºå­¦ä¹ çš„å¹»è§‰æ‰©æ•£æ¨¡å‹ç”¨äº3Däººä½“å§¿æ€ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09502v1" onclick="toggleFavorite(this, '2511.09502v1', 'DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251109443v1-bronchopt-vision-based-pose-optimization-with-fine-tuned-foundation-.html">BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation</a></td>
  <td>BronchOptï¼šåŸºäºè§†è§‰å’Œå¾®è°ƒåŸºç¡€æ¨¡å‹çš„æ”¯æ°”ç®¡é•œå¯¼èˆªä½å§¿ä¼˜åŒ–</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09443v1" onclick="toggleFavorite(this, '2511.09443v1', 'BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251108978v1-spatio-temporal-data-enhanced-vision-language-model-for-traffic-scen.html">Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding</a></td>
  <td>æå‡ºST-CLIPæ¨¡å‹ï¼Œåˆ©ç”¨æ—¶ç©ºä¿¡æ¯å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºäº¤é€šåœºæ™¯ç†è§£ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.08978v1" onclick="toggleFavorite(this, '2511.08978v1', 'Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251111700v1-epsegfz-efficient-point-cloud-semantic-segmentation-for-few-and-zero.html">EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance</a></td>
  <td>æå‡ºEPSegFZï¼Œåˆ©ç”¨è¯­è¨€å¼•å¯¼å®ç°é«˜æ•ˆçš„ç‚¹äº‘å°‘æ ·æœ¬/é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.11700v1" onclick="toggleFavorite(this, '2511.11700v1', 'EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251108910v1-og-pcl-efficient-sparse-point-cloud-processing-for-human-activity-re.html">OG-PCL: Efficient Sparse Point Cloud Processing for Human Activity Recognition</a></td>
  <td>æå‡ºOG-PCLç½‘ç»œï¼Œç”¨äºé«˜æ•ˆå¤„ç†ç¨€ç–é›·è¾¾ç‚¹äº‘çš„äººä½“æ´»åŠ¨è¯†åˆ«</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.08910v1" onclick="toggleFavorite(this, '2511.08910v1', 'OG-PCL: Efficient Sparse Point Cloud Processing for Human Activity Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251111702v1-task-aware-3d-affordance-segmentation-via-2d-guidance-and-geometric-.html">Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement</a></td>
  <td>æå‡ºTASAæ¡†æ¶ï¼Œèåˆ2Då¼•å¯¼ä¸å‡ ä½•ä¼˜åŒ–ï¼Œå®ç°ä»»åŠ¡æ„ŸçŸ¥çš„3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.11702v1" onclick="toggleFavorite(this, '2511.11702v1', 'Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251109022v1-radharsimulator-v2-video-to-doppler-generator.html">RadHARSimulator V2: Video to Doppler Generator</a></td>
  <td>RadHARSimulator V2ï¼šæå‡ºä¸€ç§è§†é¢‘åˆ°å¤šæ™®å‹’è°±çš„é›·è¾¾äººä½“æ´»åŠ¨è¯†åˆ«æ¨¡æ‹Ÿå™¨ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09022v1" onclick="toggleFavorite(this, '2511.09022v1', 'RadHARSimulator V2: Video to Doppler Generator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251109170v1-hotfloc-end-to-end-hierarchical-lidar-place-recognition-re-ranking-a.html">HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests</a></td>
  <td>HOTFLoc++ï¼šæ£®æ—ç¯å¢ƒä¸‹ç«¯åˆ°ç«¯åˆ†å±‚LiDARå®šä½ä¸é‡æ’åº</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09170v1" onclick="toggleFavorite(this, '2511.09170v1', 'HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251109130v1-piff-a-physics-informed-generative-flow-model-for-real-time-flood-de.html">PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping</a></td>
  <td>æå‡ºPIFFæ¨¡å‹ä»¥è§£å†³å®æ—¶æ´ªæ°´æ·±åº¦æ˜ å°„é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09130v1" onclick="toggleFavorite(this, '2511.09130v1', 'PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251108938v1-neural-b-frame-video-compression-with-bi-directional-reference-harmo.html">Neural B-frame Video Compression with Bi-directional Reference Harmonization</a></td>
  <td>æå‡ºBRHVCï¼Œé€šè¿‡åŒå‘å‚è€ƒå¸§åè°ƒä¼˜åŒ–ç¥ç»Bå¸§è§†é¢‘å‹ç¼©æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.08938v1" onclick="toggleFavorite(this, '2511.08938v1', 'Neural B-frame Video Compression with Bi-directional Reference Harmonization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/251108872v1-sasmamba-a-lightweight-structure-aware-stride-state-space-model-for-.html">SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation</a></td>
  <td>SasMambaï¼šè½»é‡çº§ç»“æ„æ„ŸçŸ¥æ­¥å¹…çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œç”¨äº3Däººä½“å§¿æ€ä¼°è®¡</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.08872v1" onclick="toggleFavorite(this, '2511.08872v1', 'SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251109057v3-pan-a-world-model-for-general-interactable-and-long-horizon-world-si.html">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a></td>
  <td>PANï¼šé€šç”¨ã€å¯äº¤äº’ã€é•¿æ—¶ç¨‹ä¸–ç•Œæ¨¡æ‹Ÿçš„ä¸–ç•Œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09057v3" onclick="toggleFavorite(this, '2511.09057v3', 'PAN: A World Model for General, Interactable, and Long-Horizon World Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251109388v1-learning-by-neighbor-aware-semantics-deciding-by-open-form-flows-tow.html">Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition</a></td>
  <td>æå‡ºFloraï¼Œé€šè¿‡é‚»åŸŸæ„ŸçŸ¥è¯­ä¹‰å’Œå¼€æ”¾å¼æµè§£å†³é²æ£’çš„é›¶æ ·æœ¬éª¨éª¼åŠ¨ä½œè¯†åˆ«é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09388v1" onclick="toggleFavorite(this, '2511.09388v1', 'Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251109055v1-4kdehazeflow-ultra-high-definition-image-dehazing-via-flow-matching.html">4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching</a></td>
  <td>æå‡º4KDehazeFlowï¼Œé€šè¿‡Flow Matchingå®ç°è¶…é«˜æ¸…å›¾åƒå»é›¾ï¼Œæå‡è‰²å½©ä¿çœŸåº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09055v1" onclick="toggleFavorite(this, '2511.09055v1', '4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251111713v1-understanding-the-representation-of-older-adults-in-motion-capture-l.html">Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets</a></td>
  <td>åˆ†æMoCapè€å¹´äººè¿åŠ¨æ•°æ®é›†ï¼Œæ­ç¤ºç°æœ‰æ•°æ®é›†å¯¹è€å¹´äººæ­¥æ€è¡¨å¾çš„ä¸è¶³</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.11713v1" onclick="toggleFavorite(this, '2511.11713v1', 'Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251109554v1-rf-detr-neural-architecture-search-for-real-time-detection-transform.html">RF-DETR: Neural Architecture Search for Real-Time Detection Transformers</a></td>
  <td>RF-DETRï¼šé¢å‘å®æ—¶ç›®æ ‡æ£€æµ‹Transformerçš„ç¥ç»æ¶æ„æœç´¢</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09554v1" onclick="toggleFavorite(this, '2511.09554v1', 'RF-DETR: Neural Architecture Search for Real-Time Detection Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251109147v2-presstrack-hmr-pressure-based-top-down-multi-person-global-human-mes.html">PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery</a></td>
  <td>PressTrack-HMRï¼šæå‡ºåŸºäºå‹åŠ›æ„ŸçŸ¥çš„å¤šäººå…¨å±€äººä½“ç½‘æ ¼é‡å»ºæ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09147v2" onclick="toggleFavorite(this, '2511.09147v2', 'PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction-matching">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction & Matching) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/251109286v1-enriching-knowledge-distillation-with-cross-modal-teacher-fusion.html">Enriching Knowledge Distillation with Cross-Modal Teacher Fusion</a></td>
  <td>æå‡ºRichKDï¼Œé€šè¿‡è·¨æ¨¡æ€CLIPçŸ¥è¯†èåˆæå‡çŸ¥è¯†è’¸é¦æ•ˆæœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09286v1" onclick="toggleFavorite(this, '2511.09286v1', 'Enriching Knowledge Distillation with Cross-Modal Teacher Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)