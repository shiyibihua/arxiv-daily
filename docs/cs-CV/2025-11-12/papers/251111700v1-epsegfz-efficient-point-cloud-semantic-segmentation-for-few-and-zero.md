---
layout: default
title: EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance
---

# EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance

**arXiv**: [2511.11700v1](https://arxiv.org/abs/2511.11700) | [PDF](https://arxiv.org/pdf/2511.11700.pdf)

**ä½œè€…**: Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee

**åˆ†ç±»**: cs.CV, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

**å¤‡æ³¨**: AAAI 2026

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEPSegFZï¼Œåˆ©ç”¨è¯­è¨€å¼•å¯¼å®žçŽ°é«˜æ•ˆçš„ç‚¹äº‘å°‘æ ·æœ¬/é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç‚¹äº‘è¯­ä¹‰åˆ†å‰²` `å°‘æ ·æœ¬å­¦ä¹ ` `é›¶æ ·æœ¬å­¦ä¹ ` `è¯­è¨€å¼•å¯¼` `åŽŸåž‹å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å°‘æ ·æœ¬ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹æ³•è¿‡åº¦ä¾èµ–é¢„è®­ç»ƒï¼Œé™åˆ¶äº†æ¨¡åž‹çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. EPSegFZé€šè¿‡ProERAã€DRPEå’ŒLGPEæ¨¡å—ï¼Œæ— éœ€é¢„è®­ç»ƒå³å¯å®žçŽ°é«˜æ•ˆçš„ç‰¹å¾æå–å’Œè¯­è¨€å¼•å¯¼çš„è¯­ä¹‰åˆ†å‰²ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒEPSegFZåœ¨S3DISå’ŒScanNetæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¯æ˜Žäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEPSegFZçš„æ— éœ€é¢„è®­ç»ƒçš„ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œç”¨äºŽè§£å†³å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹çš„åˆ†å‰²é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽä¸¤é˜¶æ®µå­¦ä¹ ï¼Œå³é¢„è®­ç»ƒé˜¶æ®µå’Œå°‘æ ·æœ¬è®­ç»ƒé˜¶æ®µï¼Œè¿™å¯¼è‡´æ¨¡åž‹è¿‡åº¦ä¾èµ–é¢„è®­ç»ƒï¼Œç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼ŒçŽ°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ”¯æŒé›†ä¸­çš„æ–‡æœ¬æ ‡æ³¨ç­‰ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒEPSegFZåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŽŸåž‹å¢žå¼ºå¯„å­˜å™¨æ³¨æ„åŠ›ï¼ˆProERAï¼‰æ¨¡å—å’ŒåŸºäºŽåŒé‡ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆDRPEï¼‰çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºŽæ”¹è¿›ç‰¹å¾æå–å’Œæž„å»ºå‡†ç¡®çš„æŸ¥è¯¢-åŽŸåž‹å¯¹åº”å…³ç³»ï¼Œæ— éœ€é¢„è®­ç»ƒï¼›è¯­è¨€å¼•å¯¼åŽŸåž‹åµŒå…¥ï¼ˆLGPEï¼‰æ¨¡å—ï¼Œæœ‰æ•ˆåˆ©ç”¨æ”¯æŒé›†ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæé«˜å°‘æ ·æœ¬æ€§èƒ½å¹¶å®žçŽ°é›¶æ ·æœ¬æŽ¨ç†ã€‚åœ¨S3DISå’ŒScanNetåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«ä¼˜äºŽçŽ°æœ‰æŠ€æœ¯5.68%å’Œ3.82%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰å°‘æ ·æœ¬ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯è¿‡åº¦ä¾èµ–é¢„è®­ç»ƒï¼Œå¯¼è‡´æ¨¡åž‹ç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ï¼›äºŒæ˜¯æœªèƒ½å……åˆ†åˆ©ç”¨æ”¯æŒé›†ä¸­çš„æ–‡æœ¬æ ‡æ³¨ä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨¡åž‹çš„æ€§èƒ½å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ— éœ€é¢„è®­ç»ƒä¸”èƒ½æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯çš„å°‘æ ·æœ¬/é›¶æ ·æœ¬ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEPSegFZçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¯­è¨€ä¿¡æ¯å¼•å¯¼ç‚¹äº‘ç‰¹å¾çš„å­¦ä¹ å’Œåˆ†å‰²ï¼Œä»Žè€Œåœ¨æ— éœ€é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæé«˜æ¨¡åž‹çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡ProERAå’ŒDRPEæ¨¡å—å¢žå¼ºç‚¹äº‘ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡LGPEæ¨¡å—å°†æ–‡æœ¬ä¿¡æ¯èžå…¥åˆ°åŽŸåž‹è¡¨ç¤ºä¸­ï¼Œä»Žè€Œå®žçŽ°æ›´å‡†ç¡®çš„è¯­ä¹‰åˆ†å‰²ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šEPSegFZçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) Prototype-Enhanced Registers Attention (ProERA)æ¨¡å—ï¼Œç”¨äºŽå¢žå¼ºç‚¹äº‘ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼›2) Dual Relative Positional Encoding (DRPE)-based cross-attentionæœºåˆ¶ï¼Œç”¨äºŽæž„å»ºå‡†ç¡®çš„æŸ¥è¯¢-åŽŸåž‹å¯¹åº”å…³ç³»ï¼›3) Language-Guided Prototype Embedding (LGPE)æ¨¡å—ï¼Œç”¨äºŽå°†æ–‡æœ¬ä¿¡æ¯èžå…¥åˆ°åŽŸåž‹è¡¨ç¤ºä¸­ã€‚æ•´ä¸ªæµç¨‹æ˜¯å…ˆé€šè¿‡ProERAæå–ç‚¹äº‘ç‰¹å¾ï¼Œç„¶åŽåˆ©ç”¨DRPEæž„å»ºæŸ¥è¯¢-åŽŸåž‹å¯¹åº”å…³ç³»ï¼Œæœ€åŽé€šè¿‡LGPEå°†æ–‡æœ¬ä¿¡æ¯èžå…¥åŽŸåž‹ï¼Œè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šEPSegFZçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†ä¸€ç§æ— éœ€é¢„è®­ç»ƒçš„ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ¡†æž¶ï¼Œé¿å…äº†å¯¹é¢„è®­ç»ƒæ¨¡åž‹çš„ä¾èµ–ï¼›2) æå‡ºäº†ProERAæ¨¡å—å’ŒDRPEæœºåˆ¶ï¼Œç”¨äºŽå¢žå¼ºç‚¹äº‘ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›å’Œæž„å»ºå‡†ç¡®çš„æŸ¥è¯¢-åŽŸåž‹å¯¹åº”å…³ç³»ï¼›3) æå‡ºäº†LGPEæ¨¡å—ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†æ”¯æŒé›†ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡åž‹çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šProERAæ¨¡å—åˆ©ç”¨å¯„å­˜å™¨æ³¨æ„åŠ›æœºåˆ¶å¢žå¼ºç‚¹äº‘ç‰¹å¾ï¼›DRPEæœºåˆ¶é€šè¿‡è€ƒè™‘ç‚¹ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ï¼Œæé«˜äº¤å‰æ³¨æ„åŠ›çš„å‡†ç¡®æ€§ï¼›LGPEæ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚BERTï¼‰æå–æ–‡æœ¬ç‰¹å¾ï¼Œå¹¶å°†å…¶èžå…¥åˆ°åŽŸåž‹è¡¨ç¤ºä¸­ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åˆ†å‰²æŸå¤±å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºŽä¼˜åŒ–æ¨¡åž‹çš„åˆ†å‰²æ€§èƒ½å’ŒåŽŸåž‹è¡¨ç¤ºã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

EPSegFZåœ¨S3DISå’ŒScanNetæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨S3DISæ•°æ®é›†ä¸Šï¼ŒEPSegFZçš„å¹³å‡IoUæ¯”çŽ°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†5.68%ã€‚åœ¨ScanNetæ•°æ®é›†ä¸Šï¼ŒEPSegFZçš„å¹³å‡IoUæ¯”çŽ°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†3.82%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒEPSegFZåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

EPSegFZåœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººåœºæ™¯ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®žçŽ°å¯¹æœªçŸ¥ç‰©ä½“çš„å¿«é€Ÿè¯†åˆ«å’Œåˆ†å‰²ï¼Œä»Žè€Œæé«˜æœºå™¨äººçš„çŽ¯å¢ƒé€‚åº”èƒ½åŠ›ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®žçŽ°å¯¹é“è·¯åœºæ™¯çš„ç²¾ç¡®åˆ†å‰²ï¼Œä»Žè€Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚åœ¨å¢žå¼ºçŽ°å®žé¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®žçŽ°å¯¹è™šæ‹Ÿç‰©ä½“çš„ç²¾ç¡®æ”¾ç½®å’Œäº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

