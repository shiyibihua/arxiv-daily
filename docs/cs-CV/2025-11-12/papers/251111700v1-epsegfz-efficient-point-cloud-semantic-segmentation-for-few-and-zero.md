---
layout: default
title: EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance
---

# EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.11700" target="_blank" class="toolbar-btn">arXiv: 2511.11700v1</a>
    <a href="https://arxiv.org/pdf/2511.11700.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.11700v1" 
            onclick="toggleFavorite(this, '2511.11700v1', 'EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee

**ÂàÜÁ±ª**: cs.CV, eess.IV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

**Â§áÊ≥®**: AAAI 2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EPSegFZÔºåÂà©Áî®ËØ≠Ë®ÄÂºïÂØºÂÆûÁé∞È´òÊïàÁöÑÁÇπ‰∫ëÂ∞ëÊ†∑Êú¨/Èõ∂Ê†∑Êú¨ËØ≠‰πâÂàÜÂâ≤**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤` `Â∞ëÊ†∑Êú¨Â≠¶‰π†` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `ËØ≠Ë®ÄÂºïÂØº` `ÂéüÂûãÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ∞ëÊ†∑Êú¨ÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ïËøáÂ∫¶‰æùËµñÈ¢ÑËÆ≠ÁªÉÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. EPSegFZÈÄöËøáProERA„ÄÅDRPEÂíåLGPEÊ®°ÂùóÔºåÊó†ÈúÄÈ¢ÑËÆ≠ÁªÉÂç≥ÂèØÂÆûÁé∞È´òÊïàÁöÑÁâπÂæÅÊèêÂèñÂíåËØ≠Ë®ÄÂºïÂØºÁöÑËØ≠‰πâÂàÜÂâ≤„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEPSegFZÂú®S3DISÂíåScanNetÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫EPSegFZÁöÑÊó†ÈúÄÈ¢ÑËÆ≠ÁªÉÁöÑÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤ÁΩëÁªúÔºåÁî®‰∫éËß£ÂÜ≥Â∞ëÊ†∑Êú¨ÂíåÈõ∂Ê†∑Êú¨Âú∫ÊôØ‰∏ãÁöÑÂàÜÂâ≤ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫é‰∏§Èò∂ÊÆµÂ≠¶‰π†ÔºåÂç≥È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÂíåÂ∞ëÊ†∑Êú¨ËÆ≠ÁªÉÈò∂ÊÆµÔºåËøôÂØºËá¥Ê®°ÂûãËøáÂ∫¶‰æùËµñÈ¢ÑËÆ≠ÁªÉÔºåÁº∫‰πèÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂÖÖÂàÜÂà©Áî®ÊîØÊåÅÈõÜ‰∏≠ÁöÑÊñáÊú¨Ê†áÊ≥®Á≠â‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåEPSegFZÂåÖÂê´‰∏â‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöÂéüÂûãÂ¢ûÂº∫ÂØÑÂ≠òÂô®Ê≥®ÊÑèÂäõÔºàProERAÔºâÊ®°ÂùóÂíåÂü∫‰∫éÂèåÈáçÁõ∏ÂØπ‰ΩçÁΩÆÁºñÁ†ÅÔºàDRPEÔºâÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁî®‰∫éÊîπËøõÁâπÂæÅÊèêÂèñÂíåÊûÑÂª∫ÂáÜÁ°ÆÁöÑÊü•ËØ¢-ÂéüÂûãÂØπÂ∫îÂÖ≥Á≥ªÔºåÊó†ÈúÄÈ¢ÑËÆ≠ÁªÉÔºõËØ≠Ë®ÄÂºïÂØºÂéüÂûãÂµåÂÖ•ÔºàLGPEÔºâÊ®°ÂùóÔºåÊúâÊïàÂà©Áî®ÊîØÊåÅÈõÜ‰∏≠ÁöÑÊñáÊú¨‰ø°ÊÅØÔºåÊèêÈ´òÂ∞ëÊ†∑Êú¨ÊÄßËÉΩÂπ∂ÂÆûÁé∞Èõ∂Ê†∑Êú¨Êé®ÁêÜ„ÄÇÂú®S3DISÂíåScanNetÂü∫ÂáÜÊµãËØï‰∏≠ÔºåËØ•ÊñπÊ≥ïÂàÜÂà´‰ºò‰∫éÁé∞ÊúâÊäÄÊúØ5.68%Âíå3.82%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂ∞ëÊ†∑Êú¨ÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ï‰∏ªË¶ÅÂ≠òÂú®‰∏§‰∏™ÁóõÁÇπÔºö‰∏ÄÊòØËøáÂ∫¶‰æùËµñÈ¢ÑËÆ≠ÁªÉÔºåÂØºËá¥Ê®°ÂûãÁº∫‰πèÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄßÔºõ‰∫åÊòØÊú™ËÉΩÂÖÖÂàÜÂà©Áî®ÊîØÊåÅÈõÜ‰∏≠ÁöÑÊñáÊú¨Ê†áÊ≥®‰ø°ÊÅØÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÈõ∂Ê†∑Êú¨ËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊó†ÈúÄÈ¢ÑËÆ≠ÁªÉ‰∏îËÉΩÊúâÊïàÂà©Áî®ÊñáÊú¨‰ø°ÊÅØÁöÑÂ∞ëÊ†∑Êú¨/Èõ∂Ê†∑Êú¨ÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEPSegFZÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËØ≠Ë®Ä‰ø°ÊÅØÂºïÂØºÁÇπ‰∫ëÁâπÂæÅÁöÑÂ≠¶‰π†ÂíåÂàÜÂâ≤Ôºå‰ªéËÄåÂú®Êó†ÈúÄÈ¢ÑËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂ∞ëÊ†∑Êú¨ÂíåÈõ∂Ê†∑Êú¨ÂàÜÂâ≤ÊÄßËÉΩ„ÄÇÈÄöËøáProERAÂíåDRPEÊ®°ÂùóÂ¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅÁöÑË°®ËææËÉΩÂäõÔºåÂπ∂ÈÄöËøáLGPEÊ®°ÂùóÂ∞ÜÊñáÊú¨‰ø°ÊÅØËûçÂÖ•Âà∞ÂéüÂûãË°®Á§∫‰∏≠Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑËØ≠‰πâÂàÜÂâ≤„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEPSegFZÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Prototype-Enhanced Registers Attention (ProERA)Ê®°ÂùóÔºåÁî®‰∫éÂ¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅÁöÑË°®ËææËÉΩÂäõÔºõ2) Dual Relative Positional Encoding (DRPE)-based cross-attentionÊú∫Âà∂ÔºåÁî®‰∫éÊûÑÂª∫ÂáÜÁ°ÆÁöÑÊü•ËØ¢-ÂéüÂûãÂØπÂ∫îÂÖ≥Á≥ªÔºõ3) Language-Guided Prototype Embedding (LGPE)Ê®°ÂùóÔºåÁî®‰∫éÂ∞ÜÊñáÊú¨‰ø°ÊÅØËûçÂÖ•Âà∞ÂéüÂûãË°®Á§∫‰∏≠„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÂÖàÈÄöËøáProERAÊèêÂèñÁÇπ‰∫ëÁâπÂæÅÔºåÁÑ∂ÂêéÂà©Áî®DRPEÊûÑÂª∫Êü•ËØ¢-ÂéüÂûãÂØπÂ∫îÂÖ≥Á≥ªÔºåÊúÄÂêéÈÄöËøáLGPEÂ∞ÜÊñáÊú¨‰ø°ÊÅØËûçÂÖ•ÂéüÂûãÔºåËøõË°åËØ≠‰πâÂàÜÂâ≤„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEPSegFZÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÈúÄÈ¢ÑËÆ≠ÁªÉÁöÑÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤Ê°ÜÊû∂ÔºåÈÅøÂÖç‰∫ÜÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑ‰æùËµñÔºõ2) ÊèêÂá∫‰∫ÜProERAÊ®°ÂùóÂíåDRPEÊú∫Âà∂ÔºåÁî®‰∫éÂ¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅÁöÑË°®ËææËÉΩÂäõÂíåÊûÑÂª∫ÂáÜÁ°ÆÁöÑÊü•ËØ¢-ÂéüÂûãÂØπÂ∫îÂÖ≥Á≥ªÔºõ3) ÊèêÂá∫‰∫ÜLGPEÊ®°ÂùóÔºåÊúâÊïàÂà©Áî®‰∫ÜÊîØÊåÅÈõÜ‰∏≠ÁöÑÊñáÊú¨‰ø°ÊÅØÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂ∞ëÊ†∑Êú¨ÂíåÈõ∂Ê†∑Êú¨ÂàÜÂâ≤ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöProERAÊ®°ÂùóÂà©Áî®ÂØÑÂ≠òÂô®Ê≥®ÊÑèÂäõÊú∫Âà∂Â¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅÔºõDRPEÊú∫Âà∂ÈÄöËøáËÄÉËôëÁÇπ‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ªÔºåÊèêÈ´ò‰∫§ÂèâÊ≥®ÊÑèÂäõÁöÑÂáÜÁ°ÆÊÄßÔºõLGPEÊ®°ÂùóÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇBERTÔºâÊèêÂèñÊñáÊú¨ÁâπÂæÅÔºåÂπ∂Â∞ÜÂÖ∂ËûçÂÖ•Âà∞ÂéüÂûãË°®Á§∫‰∏≠„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂàÜÂâ≤ÊçüÂ§±ÂíåÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂàÜÂâ≤ÊÄßËÉΩÂíåÂéüÂûãË°®Á§∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EPSegFZÂú®S3DISÂíåScanNetÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®S3DISÊï∞ÊçÆÈõÜ‰∏äÔºåEPSegFZÁöÑÂπ≥ÂùáIoUÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫Ü5.68%„ÄÇÂú®ScanNetÊï∞ÊçÆÈõÜ‰∏äÔºåEPSegFZÁöÑÂπ≥ÂùáIoUÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫Ü3.82%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåEPSegFZÂú®Â∞ëÊ†∑Êú¨ÂíåÈõ∂Ê†∑Êú¨ÁÇπ‰∫ëËØ≠‰πâÂàÜÂâ≤ÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EPSegFZÂú®Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫Âú∫ÊôØ‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊñπÊ≥ïÂÆûÁé∞ÂØπÊú™Áü•Áâ©‰ΩìÁöÑÂø´ÈÄüËØÜÂà´ÂíåÂàÜÂâ≤Ôºå‰ªéËÄåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÁéØÂ¢ÉÈÄÇÂ∫îËÉΩÂäõ„ÄÇÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊñπÊ≥ïÂÆûÁé∞ÂØπÈÅìË∑ØÂú∫ÊôØÁöÑÁ≤æÁ°ÆÂàÜÂâ≤Ôºå‰ªéËÄåÊèêÈ´òËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÂÆâÂÖ®ÊÄß„ÄÇÂú®Â¢ûÂº∫Áé∞ÂÆûÈ¢ÜÂüüÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊñπÊ≥ïÂÆûÁé∞ÂØπËôöÊãüÁâ©‰ΩìÁöÑÁ≤æÁ°ÆÊîæÁΩÆÂíå‰∫§‰∫í„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

