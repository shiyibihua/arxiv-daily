---
layout: default
title: BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation
---

# BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation

**arXiv**: [2511.09443v1](https://arxiv.org/abs/2511.09443) | [PDF](https://arxiv.org/pdf/2511.09443.pdf)

**ä½œè€…**: Hongchao Shu, Roger D. Soberanis-Mukul, Jiru Xu, Hao Ding, Morgan Ringel, Mali Shen, Saif Iftekar Sayed, Hedyeh Rafii-Tari, Mathias Unberath

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè§†è§‰çš„ä½å§¿ä¼˜åŒ–æ¡†æž¶ï¼Œç”¨äºŽæ”¯æ°”ç®¡é•œå¯¼èˆªä¸­çš„å‡†ç¡®2D-3Dé…å‡†ã€‚**

**å…³é”®è¯**: `æ”¯æ°”ç®¡é•œå¯¼èˆª` `2D-3Dé…å‡†` `è§†è§‰ä½å§¿ä¼˜åŒ–` `åˆæˆåŸºå‡†æ•°æ®é›†` `è·¨åŸŸæ³›åŒ–` `å¯å¾®åˆ†æ¸²æŸ“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ”¯æ°”ç®¡é•œå¯¼èˆªä¸­å› å‘¼å¸è¿åŠ¨å’ŒCT-èº«ä½“å·®å¼‚å¯¼è‡´é…å‡†è¯¯å·®ï¼ŒçŽ°æœ‰æ–¹æ³•æ³›åŒ–æ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¾®è°ƒç¼–ç å™¨è®¡ç®—RGBä¸Žæ·±åº¦å›¾ç›¸ä¼¼æ€§ï¼Œé€šè¿‡å¯å¾®åˆ†æ¸²æŸ“è¿­ä»£ä¼˜åŒ–ç›¸æœºä½å§¿ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼Œå¹³å‡å¹³ç§»è¯¯å·®2.65æ¯«ç±³ï¼Œæ—‹è½¬è¯¯å·®0.19å¼§åº¦ï¼Œå®žçŽ°è·¨åŸŸæ³›åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

