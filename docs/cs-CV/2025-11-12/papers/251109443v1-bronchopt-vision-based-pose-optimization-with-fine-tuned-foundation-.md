---
layout: default
title: BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation
---

# BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.09443" target="_blank" class="toolbar-btn">arXiv: 2511.09443v1</a>
    <a href="https://arxiv.org/pdf/2511.09443.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09443v1" 
            onclick="toggleFavorite(this, '2511.09443v1', 'BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongchao Shu, Roger D. Soberanis-Mukul, Jiru Xu, Hao Ding, Morgan Ringel, Mali Shen, Saif Iftekar Sayed, Hedyeh Rafii-Tari, Mathias Unberath

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**BronchOptÔºöÂü∫‰∫éËßÜËßâÂíåÂæÆË∞ÉÂü∫Á°ÄÊ®°ÂûãÁöÑÊîØÊ∞îÁÆ°ÈïúÂØºËà™‰ΩçÂßø‰ºòÂåñ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÊîØÊ∞îÁÆ°ÈïúÂØºËà™` `‰ΩçÂßø‰ºòÂåñ` `ËßÜËßâÈÖçÂáÜ` `Ê∑±Â∫¶Â≠¶‰π†` `ÂåªÂ≠¶ÂΩ±ÂÉè` `ÂêàÊàêÊï∞ÊçÆ` `ÂüüÊ≥õÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éËßÜËßâÁöÑÊîØÊ∞îÁÆ°ÈïúÂØºËà™ÊñπÊ≥ïÈöæ‰ª•Âú®‰∏çÂêåÈ¢ÜÂüüÂíåÊÇ£ËÄÖ‰πãÈó¥Ê≥õÂåñÔºåÂØºËá¥ÈÖçÂáÜËØØÂ∑Æ„ÄÇ
2. ÊèêÂá∫Âü∫‰∫éËßÜËßâÁöÑ‰ΩçÂßø‰ºòÂåñÊ°ÜÊû∂ÔºåÈÄöËøáÂæÆË∞ÉÁöÑÂü∫Á°ÄÊ®°ÂûãÂÆûÁé∞ÂÜÖÁ™•ÈïúÂõæÂÉè‰∏éCTÂõæÂÉèÁöÑÁõ¥Êé•Áõ∏‰ººÂ∫¶ËÆ°ÁÆó„ÄÇ
3. ÊûÑÂª∫‰∫ÜÂÖ¨ÂºÄÁöÑÂêàÊàêÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÊ®°Âûã‰ªÖÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÔºåÂç≥ÂèØÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏äÂÆûÁé∞ËâØÂ•ΩÁöÑÊ≥õÂåñÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂ÈíàÂØπÊîØÊ∞îÁÆ°ÈïúÊúØ‰∏≠ÂÆö‰ΩçÈöæÈ¢òÔºåÂç≥ÂëºÂê∏ËøêÂä®„ÄÅËß£ÂâñÂèòÂºÇÂíåCT-‰ΩìË°®Â∑ÆÂºÇÂØºËá¥ÁöÑÊúØ‰∏≠ËßÜÂõæ‰∏éÊúØÂâçCTÈÖçÂáÜËØØÂ∑ÆÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈÄöÁî®ÁöÑ„ÄÅÂü∫‰∫éËßÜËßâÁöÑÊîØÊ∞îÁÆ°ÈïúÂØºËà™Ê°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ÂæÆË∞ÉÁöÑÊ®°ÊÄÅÂíåÂüü‰∏çÂèòÁºñÁ†ÅÂô®ÔºåÁõ¥Êé•ËÆ°ÁÆóÁúüÂÆûÂÜÖÁ™•ÈïúRGBÂõæÂÉè‰∏éCTÊ∏≤ÊüìÊ∑±Â∫¶Âõæ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºåÂπ∂ÈÄöËøáÂèØÂæÆÊ∏≤ÊüìÊ®°ÂùóËø≠‰ª£‰ºòÂåñÁõ∏Êú∫‰ΩçÂßøÔºåÂÆûÁé∞Â∏ßÈó¥2D-3DÈÖçÂáÜ„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÊèêÈ´òÂèØÈáçÂ§çÊÄßÔºåÊàë‰ª¨ÂèëÂ∏É‰∫ÜÈ¶ñ‰∏™Áî®‰∫éÊîØÊ∞îÁÆ°ÈïúÂØºËà™ÁöÑÂêàÊàêÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÊ®°Âûã‰ªÖÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÔºåÂú®Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá2.65mmÁöÑÂπ≥ÁßªËØØÂ∑ÆÂíå0.19radÁöÑÊóãËΩ¨ËØØÂ∑ÆÔºåËØÅÊòé‰∫ÜÂÖ∂ÂáÜÁ°ÆÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇÂú®ÁúüÂÆûÊÇ£ËÄÖÊï∞ÊçÆ‰∏äÁöÑÂÆöÊÄßÁªìÊûúËøõ‰∏ÄÊ≠•ËØÅÂÆû‰∫ÜÂÖ∂Âº∫Â§ßÁöÑË∑®ÂüüÊ≥õÂåñËÉΩÂäõÔºåÊó†ÈúÄÁâπÂÆöÈ¢ÜÂüüÁöÑÈÄÇÂ∫îÂç≥ÂèØÂÆûÁé∞‰∏ÄËá¥ÁöÑÂ∏ßÈó¥2D-3DÂØπÈΩê„ÄÇÊÄªËÄåË®Ä‰πãÔºåËØ•Ê°ÜÊû∂ÈÄöËøáËø≠‰ª£ÁöÑËßÜËßâ‰ºòÂåñÂÆûÁé∞‰∫ÜÈ≤ÅÊ£íÁöÑ„ÄÅÂüü‰∏çÂèòÁöÑÂÆö‰ΩçÔºåËÄåÊñ∞ÁöÑÂü∫ÂáÜ‰∏∫Âü∫‰∫éËßÜËßâÁöÑÊîØÊ∞îÁÆ°ÈïúÂØºËà™Ê†áÂáÜÂåñËøõÂ±ïÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊîØÊ∞îÁÆ°ÈïúÊúØ‰∏≠ÔºåÁî±‰∫éÂëºÂê∏ËøêÂä®„ÄÅËß£ÂâñÁªìÊûÑÂ∑ÆÂºÇ‰ª•ÂèäCTÊâ´Êèè‰∏éÂÆûÈôÖ‰ΩìË°®‰πãÈó¥ÁöÑÂÅèÂ∑ÆÔºåÂØºËá¥ÊúØ‰∏≠ÂÜÖÁ™•ÈïúÂõæÂÉè‰∏éÊúØÂâçCTÂõæÂÉè‰πãÈó¥Â≠òÂú®Èîô‰ΩçÔºåÈöæ‰ª•ÂÆûÁé∞Á≤æÂáÜÂÆö‰Ωç„ÄÇÁé∞ÊúâÂü∫‰∫éËßÜËßâÁöÑÊñπÊ≥ïÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÊó†Ê≥ïÊúâÊïàËß£ÂÜ≥Ë∑®ÊÇ£ËÄÖÂíåË∑®ÂüüÁöÑÈÖçÂáÜÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂ≠¶‰π†ÂÜÖÁ™•ÈïúÂõæÂÉèÂíåCTÂõæÂÉè‰πãÈó¥ÁöÑÂüü‰∏çÂèòÁâπÂæÅË°®ËææÔºå‰ªéËÄåÂÆûÁé∞‰∏§ÁßçÊ®°ÊÄÅÂõæÂÉèÁöÑÁõ¥Êé•Áõ∏‰ººÂ∫¶ËÆ°ÁÆó„ÄÇÈÄöËøáÂèØÂæÆÊ∏≤ÊüìÊäÄÊúØÔºåÂ∞Ü‰ΩçÂßø‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÂõæÂÉèÁõ∏‰ººÂ∫¶ÊúÄÂ§ßÂåñÈóÆÈ¢òÔºåÂπ∂ËøõË°åËø≠‰ª£‰ºòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê®°ÂùóÔºö1) ÂæÆË∞ÉÁöÑÊ®°ÊÄÅÂíåÂüü‰∏çÂèòÁºñÁ†ÅÂô®ÔºåÁî®‰∫éÊèêÂèñÂÜÖÁ™•ÈïúÂõæÂÉèÂíåCTÊ∏≤ÊüìÊ∑±Â∫¶ÂõæÁöÑÁâπÂæÅÔºõ2) ÂèØÂæÆÊ∏≤ÊüìÊ®°ÂùóÔºåÁî®‰∫éÊ†πÊçÆÁõ∏Êú∫‰ΩçÂßøÂ∞ÜCTÂõæÂÉèÊ∏≤ÊüìÊàêÊ∑±Â∫¶ÂõæÔºõ3) ‰ΩçÂßø‰ºòÂåñÊ®°ÂùóÔºåÈÄöËøáËø≠‰ª£‰ºòÂåñÁõ∏Êú∫‰ΩçÂßøÔºå‰ΩøÂæóÊ∏≤ÊüìÁöÑÊ∑±Â∫¶Âõæ‰∏éÂÜÖÁ™•ÈïúÂõæÂÉèÁöÑÁâπÂæÅÁõ∏‰ººÂ∫¶ÊúÄÂ§ßÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂæÆË∞ÉÁöÑÊ®°ÊÄÅÂíåÂüü‰∏çÂèòÁºñÁ†ÅÂô®ÔºåËÉΩÂ§üÊúâÊïàÊèêÂèñÂÜÖÁ™•ÈïúÂõæÂÉèÂíåCTÂõæÂÉèÁöÑÂÖ±‰∫´ÁâπÂæÅÔºå‰ªéËÄåÂÆûÁé∞Ë∑®Ê®°ÊÄÅÂíåË∑®ÂüüÁöÑÂõæÂÉèÈÖçÂáÜÔºõ2) ÊûÑÂª∫‰∫ÜÈ¶ñ‰∏™ÂÖ¨ÂºÄÁöÑÊîØÊ∞îÁÆ°ÈïúÂØºËà™ÂêàÊàêÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºå‰∏∫ËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊ†áÂáÜÂåñÁöÑËØÑ‰º∞Âπ≥Âè∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÁºñÁ†ÅÂô®ÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâTransformerÊ®°ÂûãÔºåÂπ∂Âú®ÂêàÊàêÊï∞ÊçÆ‰∏äËøõË°åÂæÆË∞ÉÔºå‰ª•ÈÄÇÂ∫îÂÜÖÁ™•ÈïúÂõæÂÉèÂíåCTÂõæÂÉèÁöÑÁâπÂæÅ„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®ÂõæÂÉèÁâπÂæÅÁöÑ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÔºåÁî®‰∫éË°°ÈáèÊ∏≤ÊüìÊ∑±Â∫¶Âõæ‰∏éÂÜÖÁ™•ÈïúÂõæÂÉè‰πãÈó¥ÁöÑÁõ∏‰ººÁ®ãÂ∫¶„ÄÇ‰ΩçÂßø‰ºòÂåñÈááÁî®Adam‰ºòÂåñÂô®ÔºåËø≠‰ª£Êõ¥Êñ∞Áõ∏Êú∫‰ΩçÂßø„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Ê®°ÂûãÂú®ÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÂπ∂Âú®ÂÖ¨ÂºÄÁöÑÊîØÊ∞îÁÆ°ÈïúÂØºËà™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂÆûÁé∞‰∫ÜÂπ≥Âùá2.65mmÁöÑÂπ≥ÁßªËØØÂ∑ÆÂíå0.19radÁöÑÊóãËΩ¨ËØØÂ∑Æ„ÄÇÂú®ÁúüÂÆûÊÇ£ËÄÖÊï∞ÊçÆ‰∏äÁöÑÂÆöÊÄßÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂÖ∑ÊúâËâØÂ•ΩÁöÑË∑®ÂüüÊ≥õÂåñËÉΩÂäõÔºåÊó†ÈúÄÈíàÂØπÁâπÂÆöÈ¢ÜÂüüËøõË°åË∞ÉÊï¥„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰∏¥Â∫äÊîØÊ∞îÁÆ°ÈïúÂØºËà™ÔºåËæÖÂä©ÂåªÁîüËøõË°åÁ≤æÂáÜÂÆö‰ΩçÂíåÊâãÊúØÊìç‰ΩúÔºåÂáèÂ∞ëÊâãÊúØÈ£éÈô©ÔºåÊèêÈ´òÊâãÊúØÊàêÂäüÁéá„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ï‰πüÂèØÊé®ÂπøÂà∞ÂÖ∂‰ªñÂåªÂ≠¶ÂΩ±ÂÉèÂºïÂØºÊâãÊúØÔºåÂ¶ÇËÖπËÖîÈïúÊâãÊúØ„ÄÅÁ•ûÁªèÂ§ñÁßëÊâãÊúØÁ≠âÔºåÂÖ∑ÊúâÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

