---
layout: default
title: Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement
---

# Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.11702" target="_blank" class="toolbar-btn">arXiv: 2511.11702v1</a>
    <a href="https://arxiv.org/pdf/2511.11702.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.11702v1" 
            onclick="toggleFavorite(this, '2511.11702v1', 'Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Lian He, Meng Liu, Qilang Ye, Yu Zhou, Xiang Deng, Gangyi Ding

**ÂàÜÁ±ª**: cs.CV, cs.AI, eess.IV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TASAÊ°ÜÊû∂ÔºåËûçÂêà2DÂºïÂØº‰∏éÂá†‰Ωï‰ºòÂåñÔºåÂÆûÁé∞‰ªªÂä°ÊÑüÁü•ÁöÑ3DÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `3DÂú∫ÊôØÁêÜËß£` `ÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤` `ÂÖ∑Ë∫´Êô∫ËÉΩ` `Âá†‰ΩïÊé®ÁêÜ` `Ëá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®3DÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤‰∏≠ÂøΩÁï•‰∫ÜÂá†‰Ωï‰ø°ÊÅØÔºå‰∏îËÆ°ÁÆóÊàêÊú¨È´òÔºåÈôêÂà∂‰∫ÜÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. TASAÊ°ÜÊû∂ËûçÂêà2DËØ≠‰πâÁ∫øÁ¥¢Âíå3DÂá†‰ΩïÊé®ÁêÜÔºå‰ª•Á≤óÂà∞Á≤æÁöÑÊñπÂºèÂàÜÂâ≤ÔºåÊèêÂçá‰∫ÜÂàÜÂâ≤ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåTASAÂú®SceneFun3DÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªªÂä°ÊÑüÁü•ÁöÑ3DÂú∫ÊôØÁ∫ßÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤ÔºàTASAÔºâÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÁêÜËß£3DÂú∫ÊôØ‰∏≠ÁöÑÂèØ‰∫§‰∫íÂå∫ÂüüÔºå‰ªéËÄå‰ΩøÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ËøõË°åÊúâÊÑè‰πâÁöÑ‰∫§‰∫í„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂØπË±°Á∫ßÂà´ÁöÑÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤ÔºåÊàñËÄÖÁÆÄÂçïÂú∞Â∞Ü2DÈ¢ÑÊµãÊèêÂçáÂà∞3DÔºåÂøΩÁï•‰∫ÜÁÇπ‰∫ë‰∏≠‰∏∞ÂØåÁöÑÂá†‰ΩïÁªìÊûÑ‰ø°ÊÅØÔºåÂπ∂‰∏îËÆ°ÁÆóÊàêÊú¨È´òÊòÇ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÂ±ÄÈôêÊÄßÔºåTASAËÅîÂêàÂà©Áî®2DËØ≠‰πâÁ∫øÁ¥¢Âíå3DÂá†‰ΩïÊé®ÁêÜÔºå‰ª•Á≤óÂà∞Á≤æÁöÑÊñπÂºèËøõË°åÂàÜÂâ≤„ÄÇTASAÂåÖÂê´‰∏Ä‰∏™‰ªªÂä°ÊÑüÁü•ÁöÑ2DÂèØ‰∫§‰∫íÂå∫ÂüüÊ£ÄÊµãÊ®°ÂùóÔºå‰ªéËØ≠Ë®ÄÂíåËßÜËßâËæìÂÖ•‰∏≠ËØÜÂà´ÂèØÊìç‰ΩúÁöÑÁÇπÔºå‰ªéËÄåÂºïÂØº‰ªªÂä°Áõ∏ÂÖ≥ËßÜÁÇπÁöÑÈÄâÊã©ÔºåÊèêÈ´òÊ£ÄÊµãÊïàÁéá„ÄÇÊ≠§Â§ñÔºåËøòÊèêÂá∫‰∫Ü‰∏Ä‰∏™3DÂèØ‰∫§‰∫íÂå∫ÂüüÁªÜÂåñÊ®°ÂùóÔºåÂ∞Ü2DËØ≠‰πâÂÖàÈ™å‰∏éÂ±ÄÈÉ®3DÂá†‰Ωï‰ø°ÊÅØÁõ∏ÁªìÂêàÔºåÁîüÊàêÁ≤æÁ°Æ‰∏îÁ©∫Èó¥‰∏ÄËá¥ÁöÑ3DÂèØ‰∫§‰∫íÂå∫ÂüüÊé©Á†Å„ÄÇÂú®SceneFun3DÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåTASAÂú®Âú∫ÊôØÁ∫ßÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢ÂùáÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®3DÂú∫ÊôØÁ∫ßÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤‰ªªÂä°‰∏≠Ôºå‰∏ªË¶ÅÂ≠òÂú®‰∏§‰∏™ÁóõÁÇπ„ÄÇ‰∏ÄÊòØÂøΩÁï•‰∫ÜÁÇπ‰∫ë‰∏≠‰∏∞ÂØåÁöÑÂá†‰ΩïÁªìÊûÑ‰ø°ÊÅØÔºåÂØºËá¥ÂàÜÂâ≤Á≤æÂ∫¶‰∏çÈ´ò„ÄÇ‰∫åÊòØÁõ¥Êé•Â∞Ü2DÈ¢ÑÊµãÊèêÂçáÂà∞3DÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈöæ‰ª•Â∫îÁî®‰∫éÂ§ßËßÑÊ®°Âú∫ÊôØ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊúâÊïàÂà©Áî®Âá†‰Ωï‰ø°ÊÅØÔºåÂêåÊó∂Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊòØËØ•‰ªªÂä°ÁöÑÂÖ≥ÈîÆÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTASAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÅîÂêàÂà©Áî®2DËØ≠‰πâÁ∫øÁ¥¢Âíå3DÂá†‰ΩïÊé®ÁêÜÔºå‰ª•Á≤óÂà∞Á≤æÁöÑÊñπÂºèËøõË°åÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤„ÄÇÈ¶ñÂÖàÔºåÂà©Áî®‰ªªÂä°ÊÑüÁü•ÁöÑ2DÊ£ÄÊµãÊ®°ÂùóÂø´ÈÄüÂÆö‰ΩçÊΩúÂú®ÁöÑÂèØ‰∫§‰∫íÂå∫ÂüüÔºåÂáèÂ∞ëÂêéÁª≠3DÂ§ÑÁêÜÁöÑËåÉÂõ¥„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®3DÁªÜÂåñÊ®°ÂùóÔºåÁªìÂêà2DËØ≠‰πâÂÖàÈ™åÂíåÂ±ÄÈÉ®3DÂá†‰Ωï‰ø°ÊÅØÔºåÊèêÈ´òÂàÜÂâ≤Á≤æÂ∫¶„ÄÇËøôÁßçËÆæËÆ°Êó¢ËÄÉËôë‰∫ÜËØ≠‰πâ‰ø°ÊÅØÔºåÂèàÂÖÖÂàÜÂà©Áî®‰∫ÜÂá†‰Ωï‰ø°ÊÅØÔºåÂêåÊó∂Èôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTASAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê®°ÂùóÔºö‰ªªÂä°ÊÑüÁü•ÁöÑ2DÂèØ‰∫§‰∫íÂå∫ÂüüÊ£ÄÊµãÊ®°ÂùóÂíå3DÂèØ‰∫§‰∫íÂå∫ÂüüÁªÜÂåñÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºå2DÊ£ÄÊµãÊ®°ÂùóÊé•Êî∂ËØ≠Ë®ÄÊåá‰ª§ÂíåËßÜËßâËæìÂÖ•ÔºåÈ¢ÑÊµã2DÂõæÂÉè‰∏≠ÁöÑÂèØ‰∫§‰∫íÂå∫Âüü„ÄÇÁÑ∂ÂêéÔºåÊ†πÊçÆ2DÈ¢ÑÊµãÁªìÊûúÈÄâÊã©‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜÁÇπ„ÄÇÊé•‰∏ãÊù•Ôºå3DÁªÜÂåñÊ®°ÂùóÂ∞Ü2DËØ≠‰πâÂÖàÈ™åÊäïÂΩ±Âà∞3DÁÇπ‰∫ëÔºåÂπ∂ÁªìÂêàÂ±ÄÈÉ®3DÂá†‰Ωï‰ø°ÊÅØÔºåÁîüÊàêÊúÄÁªàÁöÑ3DÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤ÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTASAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‰ªªÂä°ÊÑüÁü•ÁöÑ2DÊ£ÄÊµãÊ®°ÂùóÂíå3DÁªÜÂåñÊ®°ÂùóÁöÑÁªìÂêà„ÄÇ‰ªªÂä°ÊÑüÁü•ÁöÑ2DÊ£ÄÊµãÊ®°ÂùóËÉΩÂ§üÊ†πÊçÆËØ≠Ë®ÄÊåá‰ª§Âø´ÈÄüÂÆö‰ΩçÊΩúÂú®ÁöÑÂèØ‰∫§‰∫íÂå∫ÂüüÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÈáè„ÄÇ3DÁªÜÂåñÊ®°ÂùóÂàôËÉΩÂ§üÊúâÊïàËûçÂêà2DËØ≠‰πâÂÖàÈ™åÂíå3DÂá†‰Ωï‰ø°ÊÅØÔºåÊèêÈ´ò‰∫ÜÂàÜÂâ≤Á≤æÂ∫¶„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåTASAËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Âá†‰Ωï‰ø°ÊÅØÔºåÂêåÊó∂Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®2DÊ£ÄÊµãÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®‰∫ÜTransformerÁªìÊûÑÊù•Â§ÑÁêÜËØ≠Ë®ÄÊåá‰ª§ÂíåËßÜËßâËæìÂÖ•Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£‰ªªÂä°ÈúÄÊ±Ç„ÄÇÂú®3DÁªÜÂåñÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®‰∫ÜPointNet++ÁªìÊûÑÊù•ÊèêÂèñÁÇπ‰∫ëÁöÑÂ±ÄÈÉ®Âá†‰ΩïÁâπÂæÅÔºåÂπ∂ÁªìÂêà2DËØ≠‰πâÂÖàÈ™åËøõË°åÂàÜÂâ≤„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢Ôºå‰ΩøÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÂàÜÂâ≤ÁªìÊûú„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTASAÂú®SceneFun3DÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®Âú∫ÊôØÁ∫ßÂèØ‰∫§‰∫íÂå∫ÂüüÂàÜÂâ≤‰ªªÂä°‰∏≠ÔºåTASAÁöÑmIoUÊåáÊ†á‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞ÊòæËëóÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåTASAÂú®ÊïàÁéáÊñπÈù¢‰πüË°®Áé∞Âá∫Ëâ≤ÔºåËÆ°ÁÆóÊàêÊú¨ÊòéÊòæ‰Ωé‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåËØÜÂà´Âú∫ÊôØ‰∏≠ÁöÑÂèØ‰∫§‰∫íÂå∫ÂüüÔºåÂπ∂ÊâßË°åÁõ∏Â∫îÁöÑÊìç‰Ωú„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆû‰∏≠ÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáËá™ÁÑ∂ËØ≠Ë®Ä‰∏éËôöÊãüÁéØÂ¢ÉËøõË°å‰∫§‰∫íÔºå‰ªéËÄåËé∑ÂæóÊõ¥Ê≤âÊµ∏ÂºèÁöÑ‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ËøòÂèØ‰ª•Â∫îÁî®‰∫éËá™Âä®È©æÈ©∂È¢ÜÂüüÔºåÂ∏ÆÂä©ËΩ¶ËæÜÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÂπ∂ÂÅöÂá∫Áõ∏Â∫îÁöÑÂÜ≥Á≠ñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

