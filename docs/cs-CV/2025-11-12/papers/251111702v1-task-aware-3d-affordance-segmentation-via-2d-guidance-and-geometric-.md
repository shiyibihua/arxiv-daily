---
layout: default
title: Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement
---

# Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement

**arXiv**: [2511.11702v1](https://arxiv.org/abs/2511.11702) | [PDF](https://arxiv.org/pdf/2511.11702.pdf)

**ä½œè€…**: Lian He, Meng Liu, Qilang Ye, Yu Zhou, Xiang Deng, Gangyi Ding

**åˆ†ç±»**: cs.CV, cs.AI, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTASAæ¡†æž¶ï¼Œèžåˆ2Då¼•å¯¼ä¸Žå‡ ä½•ä¼˜åŒ–ï¼Œå®žçŽ°ä»»åŠ¡æ„ŸçŸ¥çš„3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¯äº¤äº’åŒºåŸŸåˆ†å‰²` `å…·èº«æ™ºèƒ½` `å‡ ä½•æŽ¨ç†` `è‡ªç„¶è¯­è¨€æŒ‡ä»¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²ä¸­å¿½ç•¥äº†å‡ ä½•ä¿¡æ¯ï¼Œä¸”è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…·èº«æ™ºèƒ½ä½“åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. TASAæ¡†æž¶èžåˆ2Dè¯­ä¹‰çº¿ç´¢å’Œ3Då‡ ä½•æŽ¨ç†ï¼Œä»¥ç²—åˆ°ç²¾çš„æ–¹å¼åˆ†å‰²ï¼Œæå‡äº†åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒTASAåœ¨SceneFun3Dæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„3Dåœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²ï¼ˆTASAï¼‰æ¡†æž¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç†è§£3Dåœºæ™¯ä¸­çš„å¯äº¤äº’åŒºåŸŸï¼Œä»Žè€Œä½¿å…·èº«æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚çŽ¯å¢ƒä¸­è¿›è¡Œæœ‰æ„ä¹‰çš„äº¤äº’ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¯¹è±¡çº§åˆ«çš„å¯äº¤äº’åŒºåŸŸåˆ†å‰²ï¼Œæˆ–è€…ç®€å•åœ°å°†2Dé¢„æµ‹æå‡åˆ°3Dï¼Œå¿½ç•¥äº†ç‚¹äº‘ä¸­ä¸°å¯Œçš„å‡ ä½•ç»“æž„ä¿¡æ¯ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼ŒTASAè”åˆåˆ©ç”¨2Dè¯­ä¹‰çº¿ç´¢å’Œ3Då‡ ä½•æŽ¨ç†ï¼Œä»¥ç²—åˆ°ç²¾çš„æ–¹å¼è¿›è¡Œåˆ†å‰²ã€‚TASAåŒ…å«ä¸€ä¸ªä»»åŠ¡æ„ŸçŸ¥çš„2Då¯äº¤äº’åŒºåŸŸæ£€æµ‹æ¨¡å—ï¼Œä»Žè¯­è¨€å’Œè§†è§‰è¾“å…¥ä¸­è¯†åˆ«å¯æ“ä½œçš„ç‚¹ï¼Œä»Žè€Œå¼•å¯¼ä»»åŠ¡ç›¸å…³è§†ç‚¹çš„é€‰æ‹©ï¼Œæé«˜æ£€æµ‹æ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ª3Då¯äº¤äº’åŒºåŸŸç»†åŒ–æ¨¡å—ï¼Œå°†2Dè¯­ä¹‰å…ˆéªŒä¸Žå±€éƒ¨3Då‡ ä½•ä¿¡æ¯ç›¸ç»“åˆï¼Œç”Ÿæˆç²¾ç¡®ä¸”ç©ºé—´ä¸€è‡´çš„3Då¯äº¤äº’åŒºåŸŸæŽ©ç ã€‚åœ¨SceneFun3Dæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒTASAåœ¨åœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•åœ¨3Dåœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ã€‚ä¸€æ˜¯å¿½ç•¥äº†ç‚¹äº‘ä¸­ä¸°å¯Œçš„å‡ ä½•ç»“æž„ä¿¡æ¯ï¼Œå¯¼è‡´åˆ†å‰²ç²¾åº¦ä¸é«˜ã€‚äºŒæ˜¯ç›´æŽ¥å°†2Dé¢„æµ‹æå‡åˆ°3Dï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åº”ç”¨äºŽå¤§è§„æ¨¡åœºæ™¯ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½Žè®¡ç®—å¤æ‚åº¦ï¼Œæ˜¯è¯¥ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTASAçš„æ ¸å¿ƒæ€è·¯æ˜¯è”åˆåˆ©ç”¨2Dè¯­ä¹‰çº¿ç´¢å’Œ3Då‡ ä½•æŽ¨ç†ï¼Œä»¥ç²—åˆ°ç²¾çš„æ–¹å¼è¿›è¡Œå¯äº¤äº’åŒºåŸŸåˆ†å‰²ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ä»»åŠ¡æ„ŸçŸ¥çš„2Dæ£€æµ‹æ¨¡å—å¿«é€Ÿå®šä½æ½œåœ¨çš„å¯äº¤äº’åŒºåŸŸï¼Œå‡å°‘åŽç»­3Då¤„ç†çš„èŒƒå›´ã€‚ç„¶åŽï¼Œåˆ©ç”¨3Dç»†åŒ–æ¨¡å—ï¼Œç»“åˆ2Dè¯­ä¹‰å…ˆéªŒå’Œå±€éƒ¨3Då‡ ä½•ä¿¡æ¯ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚è¿™ç§è®¾è®¡æ—¢è€ƒè™‘äº†è¯­ä¹‰ä¿¡æ¯ï¼Œåˆå……åˆ†åˆ©ç”¨äº†å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½Žäº†è®¡ç®—å¤æ‚åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTASAæ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šä»»åŠ¡æ„ŸçŸ¥çš„2Då¯äº¤äº’åŒºåŸŸæ£€æµ‹æ¨¡å—å’Œ3Då¯äº¤äº’åŒºåŸŸç»†åŒ–æ¨¡å—ã€‚é¦–å…ˆï¼Œ2Dæ£€æµ‹æ¨¡å—æŽ¥æ”¶è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ï¼Œé¢„æµ‹2Då›¾åƒä¸­çš„å¯äº¤äº’åŒºåŸŸã€‚ç„¶åŽï¼Œæ ¹æ®2Dé¢„æµ‹ç»“æžœé€‰æ‹©ä»»åŠ¡ç›¸å…³çš„è§†ç‚¹ã€‚æŽ¥ä¸‹æ¥ï¼Œ3Dç»†åŒ–æ¨¡å—å°†2Dè¯­ä¹‰å…ˆéªŒæŠ•å½±åˆ°3Dç‚¹äº‘ï¼Œå¹¶ç»“åˆå±€éƒ¨3Då‡ ä½•ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²ç»“æžœã€‚

**å…³é”®åˆ›æ–°**ï¼šTASAçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ä»»åŠ¡æ„ŸçŸ¥çš„2Dæ£€æµ‹æ¨¡å—å’Œ3Dç»†åŒ–æ¨¡å—çš„ç»“åˆã€‚ä»»åŠ¡æ„ŸçŸ¥çš„2Dæ£€æµ‹æ¨¡å—èƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤å¿«é€Ÿå®šä½æ½œåœ¨çš„å¯äº¤äº’åŒºåŸŸï¼Œå‡å°‘äº†è®¡ç®—é‡ã€‚3Dç»†åŒ–æ¨¡å—åˆ™èƒ½å¤Ÿæœ‰æ•ˆèžåˆ2Dè¯­ä¹‰å…ˆéªŒå’Œ3Då‡ ä½•ä¿¡æ¯ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTASAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½Žè®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨2Dæ£€æµ‹æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†Transformerç»“æž„æ¥å¤„ç†è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ï¼Œä»Žè€Œæ›´å¥½åœ°ç†è§£ä»»åŠ¡éœ€æ±‚ã€‚åœ¨3Dç»†åŒ–æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†PointNet++ç»“æž„æ¥æå–ç‚¹äº‘çš„å±€éƒ¨å‡ ä½•ç‰¹å¾ï¼Œå¹¶ç»“åˆ2Dè¯­ä¹‰å…ˆéªŒè¿›è¡Œåˆ†å‰²ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åˆ†å‰²ç»“æžœã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTASAåœ¨SceneFun3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨åœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒTASAçš„mIoUæŒ‡æ ‡ä¼˜äºŽçŽ°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—æ°´å¹³ã€‚æ­¤å¤–ï¼ŒTASAåœ¨æ•ˆçŽ‡æ–¹é¢ä¹Ÿè¡¨çŽ°å‡ºè‰²ï¼Œè®¡ç®—æˆæœ¬æ˜Žæ˜¾ä½ŽäºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¯æ˜Žäº†å…¶åœ¨å®žé™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººæ“ä½œã€è™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¯†åˆ«åœºæ™¯ä¸­çš„å¯äº¤äº’åŒºåŸŸï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚åœ¨è™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸Žè™šæ‹ŸçŽ¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»Žè€ŒèŽ·å¾—æ›´æ²‰æµ¸å¼çš„ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¸®åŠ©è½¦è¾†ç†è§£å‘¨å›´çŽ¯å¢ƒï¼Œå¹¶åšå‡ºç›¸åº”çš„å†³ç­–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

