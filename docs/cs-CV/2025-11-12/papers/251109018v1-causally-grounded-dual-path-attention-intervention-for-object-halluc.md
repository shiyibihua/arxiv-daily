---
layout: default
title: Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs
---

# Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs

**arXiv**: [2511.09018v1](https://arxiv.org/abs/2511.09018) | [PDF](https://arxiv.org/pdf/2511.09018.pdf)

**ä½œè€…**: Liu Yu, Zhonghao Chen, Ping Kuang, Zhikun Feng, Fan Zhou, Lan Wang, Gillian Dobbie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOwlæ¡†æž¶ä»¥ç¼“è§£å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜**

**å…³é”®è¯**: `ç‰©ä½“å¹»è§‰ç¼“è§£` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ³¨æ„åŠ›å¹²é¢„` `å› æžœå»ºæ¨¡` `å¯¹æ¯”è§£ç ` `VTACRæŒ‡æ ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç‰©ä½“å¹»è§‰å¯¼è‡´æ¨¡åž‹ç”Ÿæˆä¸Žè§†è§‰è¾“å…¥ä¸ä¸€è‡´çš„å†…å®¹
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå› æžœå›¾å»ºæ¨¡ï¼Œå¼•å…¥VTACRæŒ‡æ ‡å’ŒåŒè·¯å¾„æ³¨æ„åŠ›å¹²é¢„æœºåˆ¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨POPEå’ŒCHAIRåŸºå‡†ä¸Šæ˜¾è‘—å‡å°‘å¹»è§‰ï¼Œä¿æŒè§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

