---
layout: default
title: UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving
---

# UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving

**arXiv**: [2511.09013v1](https://arxiv.org/abs/2511.09013) | [PDF](https://arxiv.org/pdf/2511.09013.pdf)

**ä½œè€…**: Ziyi Song, Chen Xia, Chenbing Wang, Haibao Yu, Sheng Zhou, Zhisheng Niu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoEå¢žå¼ºå¤šçº§èžåˆæ¡†æž¶ä»¥æå‡ç«¯åˆ°ç«¯ååŒè‡ªåŠ¨é©¾é©¶æ€§èƒ½**

**å…³é”®è¯**: `ååŒè‡ªåŠ¨é©¾é©¶` `å¤šçº§èžåˆ` `MoEæž¶æž„` `BEVè¡¨ç¤º` `ç«¯åˆ°ç«¯å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•è½¦æ„ŸçŸ¥ä¸Žå†³ç­–å—é™ï¼ŒçŽ°æœ‰å¤šæ™ºèƒ½ä½“æ–¹æ³•å¿½è§†æ„ŸçŸ¥ä¸Žä¸‹æ¸¸è§„åˆ’å¯¹é½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šçº§èžåˆç­–ç•¥å’ŒMoEæž¶æž„ï¼ŒåŠ¨æ€å¢žå¼ºBEVè¡¨ç¤ºå¹¶æ•æ‰å¤šæ ·è¿åŠ¨æ¨¡å¼ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨DAIR-V2Xæ•°æ®é›†ä¸Šï¼Œæ„ŸçŸ¥ç²¾åº¦æå‡39.7%ï¼Œé¢„æµ‹è¯¯å·®é™ä½Ž7.2%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

