---
layout: default
title: RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation
---

# RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation

**arXiv**: [2511.09141v1](https://arxiv.org/abs/2511.09141) | [PDF](https://arxiv.org/pdf/2511.09141.pdf)

**ä½œè€…**: Xuetao Li, Wenke Huang, Nengyuan Pan, Kaiyan Zhao, Songhua Yang, Yiming Wang, Mengde Li, Mang Ye, Jifeng Xuan, Miao Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRGMPæ¡†æž¶ä»¥è§£å†³äººå½¢æœºå™¨äººæ³›åŒ–æ“ä½œä¸­çš„æ•°æ®æ•ˆçŽ‡ä¸Žå‡ ä½•æŽ¨ç†é—®é¢˜**

**å…³é”®è¯**: `äººå½¢æœºå™¨äººæ“ä½œ` `å‡ ä½•è¯­ä¹‰æŽ¨ç†` `æ•°æ®é«˜æ•ˆæŽ§åˆ¶` `é€’å½’é«˜æ–¯ç½‘ç»œ` `æŠ€èƒ½é€‰æ‹©å™¨` `æ³›åŒ–æ€§èƒ½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•°æ®é©±åŠ¨æ–¹æ³•åœ¨æœªè§åœºæ™¯ä¸­å¿½è§†å‡ ä½•æŽ¨ç†ï¼Œå¯¼è‡´è®­ç»ƒèµ„æºæµªè´¹å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨å’Œè‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œï¼Œå®žçŽ°ç«¯åˆ°ç«¯å‡ ä½•è¯­ä¹‰æŽ¨ç†ä¸Žæ•°æ®é«˜æ•ˆæŽ§åˆ¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ³›åŒ–æµ‹è¯•ä¸­ä»»åŠ¡æˆåŠŸçŽ‡87%ï¼Œæ•°æ®æ•ˆçŽ‡æ¯”çŽ°æœ‰æœ€ä¼˜æ¨¡åž‹é«˜5å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

