---
layout: default
title: Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models
---

# Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models

**arXiv**: [2511.09469v1](https://arxiv.org/abs/2511.09469) | [PDF](https://arxiv.org/pdf/2511.09469.pdf)

**ä½œè€…**: Ying Peng, Hongsen Ye, Changxin Huang, Xiping Hu, Jian Chen, Runhao Zeng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ•™å¸ˆçŸ¥è¯†è’¸é¦æ¡†æž¶ä»¥è§£å†³è½»é‡è§†é¢‘æ¨¡åž‹ç²¾åº¦ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è§†é¢‘åŠ¨ä½œè¯†åˆ«` `è½»é‡æ¨¡åž‹` `å¼‚æž„æž¶æž„` `åŒæ•™å¸ˆå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè½»é‡CNNåœ¨è§†é¢‘åŠ¨ä½œè¯†åˆ«ä¸­ç²¾åº¦ä½Žï¼Œå¼‚æž„æž¶æž„è’¸é¦å­˜åœ¨ä¸åŒ¹é…
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ViTå’ŒCNNåŒæ•™å¸ˆï¼ŒåŠ¨æ€åŠ æƒèžåˆé¢„æµ‹ä¸Žæ®‹å·®ç‰¹å¾å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HMDB51ç­‰åŸºå‡†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæœ€é«˜ç²¾åº¦æå‡5.95%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

