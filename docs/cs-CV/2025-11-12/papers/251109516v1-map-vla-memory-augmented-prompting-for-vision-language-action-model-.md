---
layout: default
title: MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation
---

# MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation

**arXiv**: [2511.09516v1](https://arxiv.org/abs/2511.09516) | [PDF](https://arxiv.org/pdf/2511.09516.pdf)

**ä½œè€…**: Runhao Li, Wenkai Guo, Zhenyu Wu, Changyuan Wang, Haoyuan Deng, Zhenyu Weng, Yap-Peng Tan, Ziwei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAP-VLAæ¡†æž¶ï¼Œé€šè¿‡è®°å¿†å¢žå¼ºæç¤ºè§£å†³æœºå™¨äººé•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ä¸­çš„è®°å¿†ç¼ºå¤±é—®é¢˜ã€‚**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `è®°å¿†å¢žå¼º` `æç¤ºè°ƒä¼˜` `é•¿æ—¶ç¨‹ä»»åŠ¡` `æ£€ç´¢å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¢„è®­ç»ƒVLAæ¨¡åž‹åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­å› ç¼ºä¹è®°å¿†è€Œè¡¨çŽ°ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºæ¼”ç¤ºè®°å¿†åº“ï¼Œé€šè¿‡ç›¸ä¼¼æ€§æ£€ç´¢åŠ¨æ€é›†æˆè½¯æç¤ºå¢žå¼ºåŠ¨ä½œç”Ÿæˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žæœºå™¨äººè¯„ä¼°ä¸­æ€§èƒ½æå‡è¾¾7.0%å’Œ25.0%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

