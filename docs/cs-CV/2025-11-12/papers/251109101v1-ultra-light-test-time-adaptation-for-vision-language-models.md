---
layout: default
title: Ultra-Light Test-Time Adaptation for Vision--Language Models
---

# Ultra-Light Test-Time Adaptation for Vision--Language Models

**arXiv**: [2511.09101v1](https://arxiv.org/abs/2511.09101) | [PDF](https://arxiv.org/pdf/2511.09101.pdf)

**ä½œè€…**: Byunghyun Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¶…è½»æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ï¼Œè§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨åŸŸåç§»ä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `æµ‹è¯•æ—¶é€‚åº”` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŸŸåç§»` `è´å¶æ–¯æ›´æ–°` `æ— è®­ç»ƒä¼˜åŒ–` `åœ¨çº¿å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨åŸŸåç§»æ—¶é¢ä¸´ç‰¹å¾æ¼‚ç§»ã€ç±»å…ˆéªŒä¸åŒ¹é…å’Œæ ¡å‡†é”™è¯¯é—®é¢˜
2. é‡‡ç”¨æ— è®­ç»ƒã€æ— åå‘ä¼ æ’­çš„åœ¨çº¿EMé£Žæ ¼æ–¹æ³•ï¼Œä»…è°ƒæ•´logitçº§å‚æ•°
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æå‡å‡†ç¡®çŽ‡å¹¶é™ä½Žæ ¡å‡†è¯¯å·®ï¼Œå»¶è¿Ÿå¼€é”€ä½ŽäºŽ8%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.

