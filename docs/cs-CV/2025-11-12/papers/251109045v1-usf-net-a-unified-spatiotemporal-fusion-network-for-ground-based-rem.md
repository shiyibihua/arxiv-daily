---
layout: default
title: USF-Net: A Unified Spatiotemporal Fusion Network for Ground-Based Remote Sensing Cloud Image Sequence Extrapolation
---

# USF-Net: A Unified Spatiotemporal Fusion Network for Ground-Based Remote Sensing Cloud Image Sequence Extrapolation

**arXiv**: [2511.09045v1](https://arxiv.org/abs/2511.09045) | [PDF](https://arxiv.org/pdf/2511.09045.pdf)

**ä½œè€…**: Penghui Niu, Taotao Cai, Jiashuai She, Yajuan Zhang, Junhua Gua, Ping Zhanga, Jungong Hane, Jianxin Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUSF-Netä»¥è§£å†³åœ°åŸºé¥æ„Ÿäº‘å›¾åƒåºåˆ—å¤–æŽ¨ä¸­çš„è‡ªé€‚åº”ç‰¹å¾æå–å’Œé•¿ç¨‹æ—¶ç©ºä¾èµ–å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `åœ°åŸºé¥æ„Ÿäº‘å›¾åƒå¤–æŽ¨` `æ—¶ç©ºèžåˆç½‘ç»œ` `è‡ªé€‚åº”å·ç§¯` `ä½Žå¤æ‚åº¦æ³¨æ„åŠ›` `é•¿ç¨‹æ—¶ç©ºä¾èµ–` `ç¼–ç å™¨-è§£ç å™¨æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–é™æ€å·ç§¯æ ¸ï¼Œç¼ºä¹è‡ªé€‚åº”ç‰¹å¾æå–ï¼Œä¸”æ—¶ç©ºä¾èµ–å»ºæ¨¡ä¸è¶³ï¼Œè®¡ç®—æ•ˆçŽ‡ä½Ž
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè‡ªé€‚åº”å¤§æ ¸å·ç§¯å’Œä½Žå¤æ‚åº¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆæ—¶é—´æµä¿¡æ¯ï¼Œä½¿ç”¨USTMå’ŒDSMæ¨¡å—
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ASI-CISæ•°æ®é›†ä¸Šï¼ŒUSF-Netåœ¨é¢„æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆçŽ‡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Ground-based remote sensing cloud image sequence extrapolation is a key research area in the development of photovoltaic power systems. However, existing approaches exhibit several limitations:(1)they primarily rely on static kernels to augment feature information, lacking adaptive mechanisms to extract features at varying resolutions dynamically;(2)temporal guidance is insufficient, leading to suboptimal modeling of long-range spatiotemporal dependencies; and(3)the quadratic computational cost of attention mechanisms is often overlooked, limiting efficiency in practical deployment. To address these challenges, we propose USF-Net, a Unified Spatiotemporal Fusion Network that integrates adaptive large-kernel convolutions and a low-complexity attention mechanism, combining temporal flow information within an encoder-decoder framework. Specifically, the encoder employs three basic layers to extract features. Followed by the USTM, which comprises:(1)a SiB equipped with a SSM that dynamically captures multi-scale contextual information, and(2)a TiB featuring a TAM that effectively models long-range temporal dependencies while maintaining computational efficiency. In addition, a DSM with a TGM is introduced to enable unified modeling of temporally guided spatiotemporal dependencies. On the decoder side, a DUM is employed to address the common "ghosting effect." It utilizes the initial temporal state as an attention operator to preserve critical motion signatures. As a key contribution, we also introduce and release the ASI-CIS dataset. Extensive experiments on ASI-CIS demonstrate that USF-Net significantly outperforms state-of-the-art methods, establishing a superior balance between prediction accuracy and computational efficiency for ground-based cloud extrapolation. The dataset and source code will be available at https://github.com/she1110/ASI-CIS.

