---
layout: default
title: Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding
---

# Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding

**arXiv**: [2511.08978v1](https://arxiv.org/abs/2511.08978) | [PDF](https://arxiv.org/pdf/2511.08978.pdf)

**ä½œè€…**: Jingtian Ma, Jingyuan Wang, Wayne Xin Zhao, Guoping Liu, Xiang Wen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºST-CLIPæ¨¡åž‹ä»¥è§£å†³äº¤é€šåœºæ™¯ç†è§£ä¸­æ—¶ç©ºä¿¡æ¯ç¼ºå¤±é—®é¢˜**

**å…³é”®è¯**: `äº¤é€šåœºæ™¯ç†è§£` `æ—¶ç©ºæ•°æ®å¢žå¼º` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æç¤ºå­¦ä¹ ` `å°‘æ ·æœ¬å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäº¤é€šåœºæ™¯ç†è§£ä¾èµ–æ—¶ç©ºå’Œè§†è§‰æ–‡æœ¬æ•°æ®ï¼ŒçŽ°æœ‰æ–¹æ³•å¸¸å¿½ç•¥æ—¶ç©ºä¿¡æ¯
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽCLIPè®¾è®¡SCAMPæç¤ºå­¦ä¹ ï¼Œé›†æˆæ—¶ç©ºä¸Šä¸‹æ–‡è¡¨ç¤ºåˆ°è¯åµŒå…¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šï¼Œå°‘æ ·æœ¬å­¦ä¹ ç­–ç•¥ä¸‹è¡¨çŽ°ä¼˜è¶Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy.

