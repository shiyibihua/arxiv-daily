---
layout: default
title: Spatial Information Bottleneck for Interpretable Visual Recognition
---

# Spatial Information Bottleneck for Interpretable Visual Recognition

**arXiv**: [2511.09239v1](https://arxiv.org/abs/2511.09239) | [PDF](https://arxiv.org/pdf/2511.09239.pdf)

**ä½œè€…**: Kaixiang Shu, Kai Meng, Junqin Luo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´ä¿¡æ¯ç“¶é¢ˆä»¥æå‡è§†è§‰è¯†åˆ«çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§**

**å…³é”®è¯**: `å¯è§£é‡Šè§†è§‰è¯†åˆ«` `ä¿¡æ¯ç“¶é¢ˆ` `æ¢¯åº¦å½’å› ` `ç©ºé—´è§£ç¼ ` `å‘é‡-é›…å¯æ¯”ç§¯` `äº’ä¿¡æ¯ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦ç½‘ç»œå­¦ä¹ ç©ºé—´çº ç¼ è¡¨ç¤ºï¼Œæ··æ·†å‰æ™¯ä¸ŽèƒŒæ™¯ç‰¹å¾ï¼ŒæŸå®³å¯è§£é‡Šæ€§
2. åŸºäºŽä¿¡æ¯è®ºï¼Œä¼˜åŒ–å‘é‡-é›…å¯æ¯”ç§¯ç©ºé—´ç»“æž„ï¼Œæœ€å¤§åŒ–å‰æ™¯äº’ä¿¡æ¯å¹¶æœ€å°åŒ–èƒŒæ™¯äº’ä¿¡æ¯
3. åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…­ç§è§£é‡Šæ–¹æ³•å‡èŽ·æ”¹è¿›ï¼Œå‰æ™¯é›†ä¸­ä¸”åˆ†ç±»ç²¾åº¦æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep neural networks typically learn spatially entangled representations that conflate discriminative foreground features with spurious background correlations, thereby undermining model interpretability and robustness. We propose a novel understanding framework for gradient-based attribution from an information-theoretic perspective. We prove that, under mild conditions, the Vector-Jacobian Products (VJP) computed during backpropagation form minimal sufficient statistics of input features with respect to class labels. Motivated by this finding, we propose an encoding-decoding perspective : forward propagation encodes inputs into class space, while VJP in backpropagation decodes this encoding back to feature space. Therefore, we propose Spatial Information Bottleneck (S-IB) to spatially disentangle information flow. By maximizing mutual information between foreground VJP and inputs while minimizing mutual information in background regions, S-IB encourages networks to encode information only in class-relevant spatial regions. Since post-hoc explanation methods fundamentally derive from VJP computations, directly optimizing VJP's spatial structure during training improves visualization quality across diverse explanation paradigms. Experiments on five benchmarks demonstrate universal improvements across six explanation methods, achieving better foreground concentration and background suppression without method-specific tuning, alongside consistent classification accuracy gains.

