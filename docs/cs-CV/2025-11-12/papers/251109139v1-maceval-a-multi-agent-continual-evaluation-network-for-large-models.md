---
layout: default
title: MACEval: A Multi-Agent Continual Evaluation Network for Large Models
---

# MACEval: A Multi-Agent Continual Evaluation Network for Large Models

**arXiv**: [2511.09139v1](https://arxiv.org/abs/2511.09139) | [PDF](https://arxiv.org/pdf/2511.09139.pdf)

**ä½œè€…**: Zijian Chen, Yuze Sun, Yuan Tian, Wenjun Zhang, Guangtao Zhai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMACEvalå¤šæ™ºèƒ½ä½“æŒç»­è¯„ä¼°ç½‘ç»œä»¥åŠ¨æ€è¯„ä¼°å¤§æ¨¡åž‹**

**å…³é”®è¯**: `å¤§æ¨¡åž‹è¯„ä¼°` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `æŒç»­è¯„ä¼°` `è‡ªåŠ¨è¯„ä¼°` `è¯„ä¼°åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤§æ¨¡åž‹è¯„ä¼°åŸºå‡†å°é—­ã€æ˜“è¿‡æ‹Ÿåˆï¼Œä¸”ç»´æŠ¤æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šæ™ºèƒ½ä½“ç½‘ç»œï¼Œé€šè¿‡è§’è‰²åˆ†é…å’Œè·¯ç”±å®žçŽ°è‡ªåŠ¨äº¤äº’è¯„ä¼°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨9ä¸ªå¼€æ”¾ä»»åŠ¡ä¸­éªŒè¯ï¼Œå®žçŽ°é«˜æ•ˆã€çµæ´»å’Œå¯æ‰©å±•è¯„ä¼°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.

