---
layout: default
title: WMPO: World Model-based Policy Optimization for Vision-Language-Action Models
---

# WMPO: World Model-based Policy Optimization for Vision-Language-Action Models

**arXiv**: [2511.09515v1](https://arxiv.org/abs/2511.09515) | [PDF](https://arxiv.org/pdf/2511.09515.pdf)

**ä½œè€…**: Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWMPOæ¡†æž¶ï¼Œé€šè¿‡åƒç´ é¢„æµ‹å®žçŽ°æ— çœŸå®žçŽ¯å¢ƒäº¤äº’çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ç­–ç•¥ä¼˜åŒ–**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä¸–ç•Œæ¨¡åž‹` `ç­–ç•¥ä¼˜åŒ–` `æ ·æœ¬æ•ˆçŽ‡` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ä¾èµ–ä¸“å®¶æ¼”ç¤ºï¼Œéš¾ä»¥ä»Žå¤±è´¥ä¸­å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ æ ·æœ¬æ•ˆçŽ‡ä½Ž
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽåƒç´ é¢„æµ‹çš„ä¸–ç•Œæ¨¡åž‹å¯¹é½é¢„è®­ç»ƒç‰¹å¾ï¼Œæ”¯æŒåœ¨çº¿ç­–ç•¥ä¼˜åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žæœºå™¨äººä¸­æå‡æ ·æœ¬æ•ˆçŽ‡ã€æ€§èƒ½ï¼Œå¹¶å±•çŽ°è‡ªæ ¡æ­£å’Œæ³›åŒ–èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

