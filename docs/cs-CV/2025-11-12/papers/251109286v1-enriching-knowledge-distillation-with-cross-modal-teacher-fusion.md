---
layout: default
title: Enriching Knowledge Distillation with Cross-Modal Teacher Fusion
---

# Enriching Knowledge Distillation with Cross-Modal Teacher Fusion

**arXiv**: [2511.09286v1](https://arxiv.org/abs/2511.09286) | [PDF](https://arxiv.org/pdf/2511.09286.pdf)

**ä½œè€…**: Amir M. Mansourian, Amir Mohammad Babaei, Shohreh Kasaei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRichKDæ¡†æž¶ï¼ŒèžåˆCLIPä¸Žè§†è§‰æ•™å¸ˆä»¥å¢žå¼ºçŸ¥è¯†è’¸é¦æ•ˆæžœ**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€èžåˆ` `CLIPæ¨¡åž‹` `æ•™å¸ˆèžåˆ` `é²æ£’æ€§æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ç¼ºä¹çŸ¥è¯†å¤šæ ·æ€§ï¼Œä»…ä¾èµ–å•æ¨¡æ€è§†è§‰ä¿¡æ¯
2. èžåˆCLIPè§†è§‰-è¯­è¨€çŸ¥è¯†ä¸Žä¼ ç»Ÿæ•™å¸ˆï¼Œä½¿ç”¨å¤šæç¤ºæ–‡æœ¬æŒ‡å¯¼
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽåŸºçº¿ï¼Œæå‡é²æ£’æ€§å’Œé¢„æµ‹ç½®ä¿¡åº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

