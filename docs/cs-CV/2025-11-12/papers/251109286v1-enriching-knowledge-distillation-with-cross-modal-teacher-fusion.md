---
layout: default
title: Enriching Knowledge Distillation with Cross-Modal Teacher Fusion
---

# Enriching Knowledge Distillation with Cross-Modal Teacher Fusion

**arXiv**: [2511.09286v1](https://arxiv.org/abs/2511.09286) | [PDF](https://arxiv.org/pdf/2511.09286.pdf)

**ä½œè€…**: Amir M. Mansourian, Amir Mohammad Babaei, Shohreh Kasaei

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

**å¤‡æ³¨**: 11 pages, 5 figures, 8 tables

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRichKDï¼Œé€šè¿‡è·¨æ¨¡æ€CLIPçŸ¥è¯†èžåˆæå‡çŸ¥è¯†è’¸é¦æ•ˆæžœ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸ŽåŒ¹é… (Video Extraction & Matching)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è·¨æ¨¡æ€å­¦ä¹ ` `CLIPæ¨¡åž‹` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `æ¨¡åž‹åŽ‹ç¼©` `é²æ£’æ€§` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸»è¦ä¾èµ–å•æ¨¡æ€è§†è§‰ä¿¡æ¯ï¼Œå¿½ç•¥äº†è·¨æ¨¡æ€çŸ¥è¯†çš„æ½œåŠ›ï¼Œå¯¼è‡´çŸ¥è¯†å¤šæ ·æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºRichKDæ¡†æž¶ï¼Œèžåˆä¼ ç»Ÿæ•™å¸ˆå’ŒCLIPæ¨¡åž‹çš„logitså’Œç‰¹å¾ï¼Œåˆ©ç”¨CLIPçš„è§†è§‰-è¯­è¨€çŸ¥è¯†ä½œä¸ºè¡¥å……ç›‘ç£ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRichKDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰åŸºçº¿ï¼Œå¹¶åœ¨åˆ†å¸ƒåç§»å’Œè¾“å…¥æŸåä¸‹è¡¨çŽ°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦(KD)æ˜¯ä¸€ç§æ¯”ä¼ ç»Ÿå•æ•™å¸ˆæ–¹æ³•æ›´æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨logitæˆ–ç‰¹å¾åŒ¹é…å°†ä¸“å®¶æ•™å¸ˆçš„çŸ¥è¯†è½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡åž‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°çŽ°æœ‰æ–¹æ³•ç¼ºä¹çŸ¥è¯†å¤šæ ·æ€§ï¼Œå› ä¸ºå®ƒä»¬ä»…ä¾èµ–äºŽå•æ¨¡æ€è§†è§‰ä¿¡æ¯ï¼Œå¿½ç•¥äº†è·¨æ¨¡æ€è¡¨ç¤ºçš„æ½œåŠ›ã€‚æœ¬æ–‡æŽ¢ç´¢ä½¿ç”¨CLIPçš„è§†è§‰-è¯­è¨€çŸ¥è¯†ä½œä¸ºKDçš„è¡¥å……ç›‘ç£æ¥æºï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æŽ¢ç´¢çš„é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æž¶ï¼Œå°†ä¼ ç»Ÿæ•™å¸ˆçš„logitså’Œç‰¹å¾ä¸ŽCLIPçš„logitså’Œç‰¹å¾èžåˆã€‚é€šè¿‡ç»“åˆCLIPçš„å¤šæç¤ºæ–‡æœ¬æŒ‡å¯¼ï¼Œèžåˆçš„ç›‘ç£æ•èŽ·äº†æ•°æ®é›†ç‰¹å®šå’Œè¯­ä¹‰ä¸°å¯Œçš„è§†è§‰çº¿ç´¢ã€‚åˆ†æžè¡¨æ˜Žï¼Œèžåˆçš„æ•™å¸ˆäº§ç”Ÿæ›´è‡ªä¿¡å’Œå¯é çš„é¢„æµ‹ï¼Œæ˜¾è‘—å¢žåŠ è‡ªä¿¡ä¸”æ­£ç¡®çš„æ¡ˆä¾‹ï¼ŒåŒæ—¶å‡å°‘è‡ªä¿¡ä½†é”™è¯¯çš„æ¡ˆä¾‹ã€‚æ­¤å¤–ï¼Œä¸ŽCLIPçš„èžåˆæ”¹è¿›äº†æ•´ä¸ªlogitåˆ†å¸ƒï¼Œä¸ºéžç›®æ ‡ç±»åˆ«ç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ¦‚çŽ‡ï¼Œä»Žè€Œæé«˜ç±»é—´ä¸€è‡´æ€§å’Œè’¸é¦è´¨é‡ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ï¼Œå³ä¸°å¯ŒçŸ¥è¯†è’¸é¦(RichKD)ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºŽå¤§å¤šæ•°çŽ°æœ‰åŸºçº¿ï¼Œå¹¶åœ¨åˆ†å¸ƒåç§»å’Œè¾“å…¥æŸåä¸‹è¡¨çŽ°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸»è¦ä¾èµ–äºŽå•æ¨¡æ€çš„è§†è§‰ä¿¡æ¯ï¼Œç¼ºä¹å¯¹å›¾åƒè¯­ä¹‰ä¿¡æ¯çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡åž‹å­¦ä¹ åˆ°çš„çŸ¥è¯†ä¸å¤Ÿä¸°å¯Œå’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†åˆ†å¸ƒåç§»æˆ–è¾“å…¥æŸåçš„æƒ…å†µä¸‹ï¼Œæ¨¡åž‹æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨CLIPæ¨¡åž‹æä¾›çš„è·¨æ¨¡æ€ï¼ˆè§†è§‰-è¯­è¨€ï¼‰çŸ¥è¯†æ¥ä¸°å¯ŒçŸ¥è¯†è’¸é¦è¿‡ç¨‹ã€‚CLIPæ¨¡åž‹åœ¨å¤§é‡æ–‡æœ¬-å›¾åƒå¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå°†å…¶ä¸Žä¼ ç»Ÿè§†è§‰æ¨¡åž‹çš„çŸ¥è¯†è¿›è¡Œèžåˆï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿæ¨¡åž‹æä¾›æ›´å…¨é¢ã€æ›´é²æ£’çš„ç›‘ç£ä¿¡å·ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRichKDæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ä¼ ç»Ÿè§†è§‰æ•™å¸ˆæ¨¡åž‹ï¼›2) CLIPæ¨¡åž‹ï¼ˆä½œä¸ºè·¨æ¨¡æ€æ•™å¸ˆï¼‰ï¼›3) å­¦ç”Ÿæ¨¡åž‹ï¼›4) ç‰¹å¾èžåˆæ¨¡å—ï¼›5) æŸå¤±å‡½æ•°ã€‚é¦–å…ˆï¼Œå°†è¾“å…¥å›¾åƒåˆ†åˆ«è¾“å…¥åˆ°ä¼ ç»Ÿæ•™å¸ˆæ¨¡åž‹å’ŒCLIPæ¨¡åž‹ä¸­ï¼Œå¾—åˆ°å„è‡ªçš„logitså’Œç‰¹å¾è¡¨ç¤ºã€‚ç„¶åŽï¼Œé€šè¿‡ç‰¹å¾èžåˆæ¨¡å—å°†ä¸¤è€…çš„ç‰¹å¾è¿›è¡Œèžåˆã€‚æœ€åŽï¼Œåˆ©ç”¨èžåˆåŽçš„ç‰¹å¾å’Œlogitsæ¥æŒ‡å¯¼å­¦ç”Ÿæ¨¡åž‹çš„è®­ç»ƒï¼Œé€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°æ¥å®ŒæˆçŸ¥è¯†è’¸é¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†è·¨æ¨¡æ€çš„CLIPæ¨¡åž‹å¼•å…¥åˆ°çŸ¥è¯†è’¸é¦æ¡†æž¶ä¸­ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›æ¥æå‡å­¦ç”Ÿæ¨¡åž‹çš„æ€§èƒ½ã€‚ä¸Žä¼ ç»Ÿçš„å•æ¨¡æ€çŸ¥è¯†è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼ŒRichKDèƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œã€æ›´é²æ£’çš„ç›‘ç£ä¿¡å·ï¼Œä»Žè€Œæé«˜å­¦ç”Ÿæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰¹å¾èžåˆæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†ç®€å•çš„åŠ æƒå¹³å‡æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´æƒé‡æ¥æŽ§åˆ¶ä¼ ç»Ÿæ•™å¸ˆæ¨¡åž‹å’ŒCLIPæ¨¡åž‹å¯¹æœ€ç»ˆç‰¹å¾çš„å½±å“ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†logitsåŒ¹é…å’Œç‰¹å¾åŒ¹é…ä¸¤ç§æŸå¤±å‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºŽçº¦æŸå­¦ç”Ÿæ¨¡åž‹çš„logitså’Œç‰¹å¾è¡¨ç¤ºä¸ŽèžåˆåŽçš„æ•™å¸ˆæ¨¡åž‹ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä½¿ç”¨äº†å¤šæç¤ºæ–‡æœ¬æŒ‡å¯¼ï¼Œé€šè¿‡ä¸åŒçš„æ–‡æœ¬æç¤ºæ¥å¼•å¯¼CLIPæ¨¡åž‹æå–æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

RichKDåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨CIFAR-100æ•°æ®é›†ä¸Šï¼Œç›¸æ¯”äºŽä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ŒRichKDçš„å‡†ç¡®çŽ‡æå‡äº†2-3ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼ŒRichKDåœ¨ImageNetæ•°æ®é›†ä¸Šä¹Ÿè¡¨çŽ°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨é¢å¯¹åˆ†å¸ƒåç§»å’Œè¾“å…¥æŸåæ—¶ï¼Œå…¶é²æ£’æ€§æ˜Žæ˜¾ä¼˜äºŽå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡åž‹éƒ¨ç½²åˆ°èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ—¶ï¼Œå¯ä»¥é€šè¿‡çŸ¥è¯†è’¸é¦å°†å¤§åž‹æ¨¡åž‹çš„çŸ¥è¯†è¿ç§»åˆ°å°åž‹æ¨¡åž‹ï¼Œä»Žè€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶é™ä½Žè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»ç–—å½±åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶ç­‰å¯¹æ¨¡åž‹é²æ£’æ€§è¦æ±‚è¾ƒé«˜çš„é¢†åŸŸä¹Ÿå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

