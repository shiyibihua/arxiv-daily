---
layout: default
title: Enriching Knowledge Distillation with Cross-Modal Teacher Fusion
---

# Enriching Knowledge Distillation with Cross-Modal Teacher Fusion

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.09286" target="_blank" class="toolbar-btn">arXiv: 2511.09286v1</a>
    <a href="https://arxiv.org/pdf/2511.09286.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09286v1" 
            onclick="toggleFavorite(this, '2511.09286v1', 'Enriching Knowledge Distillation with Cross-Modal Teacher Fusion')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Amir M. Mansourian, Amir Mohammad Babaei, Shohreh Kasaei

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

**Â§áÊ≥®**: 11 pages, 5 figures, 8 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RichKDÔºåÈÄöËøáË∑®Ê®°ÊÄÅCLIPÁü•ËØÜËûçÂêàÊèêÂçáÁü•ËØÜËí∏È¶èÊïàÊûú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction & Matching)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `CLIPÊ®°Âûã` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Ê®°ÂûãÂéãÁº©` `È≤ÅÊ£íÊÄß` `ÂõæÂÉèÂàÜÁ±ª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÂçïÊ®°ÊÄÅËßÜËßâ‰ø°ÊÅØÔºåÂøΩÁï•‰∫ÜË∑®Ê®°ÊÄÅÁü•ËØÜÁöÑÊΩúÂäõÔºåÂØºËá¥Áü•ËØÜÂ§öÊ†∑ÊÄß‰∏çË∂≥„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫RichKDÊ°ÜÊû∂ÔºåËûçÂêà‰º†ÁªüÊïôÂ∏àÂíåCLIPÊ®°ÂûãÁöÑlogitsÂíåÁâπÂæÅÔºåÂà©Áî®CLIPÁöÑËßÜËßâ-ËØ≠Ë®ÄÁü•ËØÜ‰Ωú‰∏∫Ë°•ÂÖÖÁõëÁù£„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRichKDÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂Âú®ÂàÜÂ∏ÉÂÅèÁßªÂíåËæìÂÖ•ÊçüÂùè‰∏ãË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊïôÂ∏àÁü•ËØÜËí∏È¶è(KD)ÊòØ‰∏ÄÁßçÊØî‰º†ÁªüÂçïÊïôÂ∏àÊñπÊ≥ïÊõ¥ÊúâÊïàÁöÑÊäÄÊúØÔºåÂÆÉ‰ΩøÁî®logitÊàñÁâπÂæÅÂåπÈÖçÂ∞Ü‰∏ìÂÆ∂ÊïôÂ∏àÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞Á¥ßÂáëÁöÑÂ≠¶ÁîüÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∞Áé∞ÊúâÊñπÊ≥ïÁº∫‰πèÁü•ËØÜÂ§öÊ†∑ÊÄßÔºåÂõ†‰∏∫ÂÆÉ‰ª¨‰ªÖ‰æùËµñ‰∫éÂçïÊ®°ÊÄÅËßÜËßâ‰ø°ÊÅØÔºåÂøΩÁï•‰∫ÜË∑®Ê®°ÊÄÅË°®Á§∫ÁöÑÊΩúÂäõ„ÄÇÊú¨ÊñáÊé¢Á¥¢‰ΩøÁî®CLIPÁöÑËßÜËßâ-ËØ≠Ë®ÄÁü•ËØÜ‰Ωú‰∏∫KDÁöÑË°•ÂÖÖÁõëÁù£Êù•Ê∫êÔºåËøôÊòØ‰∏Ä‰∏™ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Ë¢´Êé¢Á¥¢ÁöÑÈ¢ÜÂüü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïËÄåÊúâÊïàÁöÑÊ°ÜÊû∂ÔºåÂ∞Ü‰º†ÁªüÊïôÂ∏àÁöÑlogitsÂíåÁâπÂæÅ‰∏éCLIPÁöÑlogitsÂíåÁâπÂæÅËûçÂêà„ÄÇÈÄöËøáÁªìÂêàCLIPÁöÑÂ§öÊèêÁ§∫ÊñáÊú¨ÊåáÂØºÔºåËûçÂêàÁöÑÁõëÁù£ÊçïËé∑‰∫ÜÊï∞ÊçÆÈõÜÁâπÂÆöÂíåËØ≠‰πâ‰∏∞ÂØåÁöÑËßÜËßâÁ∫øÁ¥¢„ÄÇÂàÜÊûêË°®ÊòéÔºåËûçÂêàÁöÑÊïôÂ∏à‰∫ßÁîüÊõ¥Ëá™‰ø°ÂíåÂèØÈù†ÁöÑÈ¢ÑÊµãÔºåÊòæËëóÂ¢ûÂä†Ëá™‰ø°‰∏îÊ≠£Á°ÆÁöÑÊ°à‰æãÔºåÂêåÊó∂ÂáèÂ∞ëËá™‰ø°‰ΩÜÈîôËØØÁöÑÊ°à‰æã„ÄÇÊ≠§Â§ñÔºå‰∏éCLIPÁöÑËûçÂêàÊîπËøõ‰∫ÜÊï¥‰∏™logitÂàÜÂ∏ÉÔºå‰∏∫ÈùûÁõÆÊ†áÁ±ªÂà´ÁîüÊàêËØ≠‰πâ‰∏äÊúâÊÑè‰πâÁöÑÊ¶ÇÁéáÔºå‰ªéËÄåÊèêÈ´òÁ±ªÈó¥‰∏ÄËá¥ÊÄßÂíåËí∏È¶èË¥®Èáè„ÄÇÂ∞ΩÁÆ°ÂÖ∂ÁÆÄÂçïÊÄßÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÂç≥‰∏∞ÂØåÁü•ËØÜËí∏È¶è(RichKD)ÔºåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂßãÁªà‰ºò‰∫éÂ§ßÂ§öÊï∞Áé∞ÊúâÂü∫Á∫øÔºåÂπ∂Âú®ÂàÜÂ∏ÉÂÅèÁßªÂíåËæìÂÖ•ÊçüÂùè‰∏ãË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂçïÊ®°ÊÄÅÁöÑËßÜËßâ‰ø°ÊÅØÔºåÁº∫‰πèÂØπÂõæÂÉèËØ≠‰πâ‰ø°ÊÅØÁöÑÊ∑±ÂÖ•ÁêÜËß£ÔºåÂØºËá¥Â≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†Âà∞ÁöÑÁü•ËØÜ‰∏çÂ§ü‰∏∞ÂØåÂíåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂàÜÂ∏ÉÂÅèÁßªÊàñËæìÂÖ•ÊçüÂùèÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ®°ÂûãÊÄßËÉΩ‰ºöÊòæËëó‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®CLIPÊ®°ÂûãÊèê‰æõÁöÑË∑®Ê®°ÊÄÅÔºàËßÜËßâ-ËØ≠Ë®ÄÔºâÁü•ËØÜÊù•‰∏∞ÂØåÁü•ËØÜËí∏È¶èËøáÁ®ã„ÄÇCLIPÊ®°ÂûãÂú®Â§ßÈáèÊñáÊú¨-ÂõæÂÉèÂØπ‰∏äËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÂ≠¶‰π†Âà∞ÂõæÂÉèÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂ∞ÜÂÖ∂‰∏é‰º†ÁªüËßÜËßâÊ®°ÂûãÁöÑÁü•ËØÜËøõË°åËûçÂêàÔºåÂèØ‰ª•‰∏∫Â≠¶ÁîüÊ®°ÂûãÊèê‰æõÊõ¥ÂÖ®Èù¢„ÄÅÊõ¥È≤ÅÊ£íÁöÑÁõëÁù£‰ø°Âè∑„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRichKDÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ‰º†ÁªüËßÜËßâÊïôÂ∏àÊ®°ÂûãÔºõ2) CLIPÊ®°ÂûãÔºà‰Ωú‰∏∫Ë∑®Ê®°ÊÄÅÊïôÂ∏àÔºâÔºõ3) Â≠¶ÁîüÊ®°ÂûãÔºõ4) ÁâπÂæÅËûçÂêàÊ®°ÂùóÔºõ5) ÊçüÂ§±ÂáΩÊï∞„ÄÇÈ¶ñÂÖàÔºåÂ∞ÜËæìÂÖ•ÂõæÂÉèÂàÜÂà´ËæìÂÖ•Âà∞‰º†ÁªüÊïôÂ∏àÊ®°ÂûãÂíåCLIPÊ®°Âûã‰∏≠ÔºåÂæóÂà∞ÂêÑËá™ÁöÑlogitsÂíåÁâπÂæÅË°®Á§∫„ÄÇÁÑ∂ÂêéÔºåÈÄöËøáÁâπÂæÅËûçÂêàÊ®°ÂùóÂ∞Ü‰∏§ËÄÖÁöÑÁâπÂæÅËøõË°åËûçÂêà„ÄÇÊúÄÂêéÔºåÂà©Áî®ËûçÂêàÂêéÁöÑÁâπÂæÅÂíålogitsÊù•ÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁöÑËÆ≠ÁªÉÔºåÈÄöËøáÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞Êù•ÂÆåÊàêÁü•ËØÜËí∏È¶è„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜË∑®Ê®°ÊÄÅÁöÑCLIPÊ®°ÂûãÂºïÂÖ•Âà∞Áü•ËØÜËí∏È¶èÊ°ÜÊû∂‰∏≠ÔºåÂà©Áî®ÂÖ∂Âº∫Â§ßÁöÑËØ≠‰πâÁêÜËß£ËÉΩÂäõÊù•ÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïÊ®°ÊÄÅÁü•ËØÜËí∏È¶èÊñπÊ≥ïÁõ∏ÊØîÔºåRichKDËÉΩÂ§üÊèê‰æõÊõ¥‰∏∞ÂØå„ÄÅÊõ¥È≤ÅÊ£íÁöÑÁõëÁù£‰ø°Âè∑Ôºå‰ªéËÄåÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁâπÂæÅËûçÂêàÊñπÈù¢ÔºåËÆ∫ÊñáÈááÁî®‰∫ÜÁÆÄÂçïÁöÑÂä†ÊùÉÂπ≥ÂùáÊñπÊ≥ïÔºåÈÄöËøáË∞ÉÊï¥ÊùÉÈáçÊù•ÊéßÂà∂‰º†ÁªüÊïôÂ∏àÊ®°ÂûãÂíåCLIPÊ®°ÂûãÂØπÊúÄÁªàÁâπÂæÅÁöÑÂΩ±Âìç„ÄÇÂú®ÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢ÔºåËÆ∫ÊñáÈááÁî®‰∫ÜlogitsÂåπÈÖçÂíåÁâπÂæÅÂåπÈÖç‰∏§ÁßçÊçüÂ§±ÂáΩÊï∞ÔºåÂàÜÂà´Áî®‰∫éÁ∫¶ÊùüÂ≠¶ÁîüÊ®°ÂûãÁöÑlogitsÂíåÁâπÂæÅË°®Á§∫‰∏éËûçÂêàÂêéÁöÑÊïôÂ∏àÊ®°Âûã‰øùÊåÅ‰∏ÄËá¥„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøò‰ΩøÁî®‰∫ÜÂ§öÊèêÁ§∫ÊñáÊú¨ÊåáÂØºÔºåÈÄöËøá‰∏çÂêåÁöÑÊñáÊú¨ÊèêÁ§∫Êù•ÂºïÂØºCLIPÊ®°ÂûãÊèêÂèñÊõ¥‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

RichKDÂú®Â§ö‰∏™ÂõæÂÉèÂàÜÁ±ªÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºå‰æãÂ¶ÇÂú®CIFAR-100Êï∞ÊçÆÈõÜ‰∏äÔºåÁõ∏ÊØî‰∫é‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåRichKDÁöÑÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü2-3‰∏™ÁôæÂàÜÁÇπ„ÄÇÊ≠§Â§ñÔºåRichKDÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏ä‰πüË°®Áé∞Âá∫‰ºòÂºÇÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îÂú®Èù¢ÂØπÂàÜÂ∏ÉÂÅèÁßªÂíåËæìÂÖ•ÊçüÂùèÊó∂ÔºåÂÖ∂È≤ÅÊ£íÊÄßÊòéÊòæ‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂõæÂÉèÂàÜÁ±ª„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅÂõæÂÉèÂàÜÂâ≤Á≠âËÆ°ÁÆóÊú∫ËßÜËßâ‰ªªÂä°‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê®°ÂûãÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÊó∂ÔºåÂèØ‰ª•ÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÂ§ßÂûãÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â∞èÂûãÊ®°ÂûãÔºå‰ªéËÄåÂú®‰øùËØÅÊÄßËÉΩÁöÑÂêåÊó∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂú®ÂåªÁñóÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™Âä®È©æÈ©∂Á≠âÂØπÊ®°ÂûãÈ≤ÅÊ£íÊÄßË¶ÅÊ±ÇËæÉÈ´òÁöÑÈ¢ÜÂüü‰πüÂÖ∑ÊúâÈáçË¶ÅÁöÑÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

