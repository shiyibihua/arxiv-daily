---
layout: default
title: Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning
---

# Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning

**arXiv**: [2511.08922v1](https://arxiv.org/abs/2511.08922) | [PDF](https://arxiv.org/pdf/2511.08922.pdf)

**ä½œè€…**: Yunchang Ma, Tenglong Liu, Yixing Lan, Xin Yin, Changxin Zhang, Xinglong Zhang, Xin Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDIVOä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ä»·å€¼é«˜ä¼°å’Œä¿å®ˆæ€§å¤±è¡¡é—®é¢˜**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ‰©æ•£æ¨¡åž‹` `ä»·å€¼ä¼˜åŒ–` `ç­–ç•¥æ”¹è¿›` `D4RLåŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œåˆ†å¸ƒå¤–åŠ¨ä½œå¯¼è‡´ä»·å€¼é«˜ä¼°ï¼Œé™åˆ¶ç­–ç•¥æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŸºäºŽä¼˜åŠ¿å€¼çš„äºŒå…ƒåŠ æƒæœºåˆ¶ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡åž‹è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIVOåœ¨è¿åŠ¨ä»»åŠ¡å’ŒAntMazeé¢†åŸŸè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.

