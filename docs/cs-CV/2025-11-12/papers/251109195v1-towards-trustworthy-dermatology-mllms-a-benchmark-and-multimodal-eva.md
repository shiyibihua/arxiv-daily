---
layout: default
title: Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives
---

# Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives

**arXiv**: [2511.09195v1](https://arxiv.org/abs/2511.09195) | [PDF](https://arxiv.org/pdf/2511.09195.pdf)

**ä½œè€…**: Yuhao Shen, Jiahe Qian, Shuping Zhang, Zhangtianyi Chen, Tao Lu, Juexiao Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDermBenchåŸºå‡†ä¸ŽDermEvalè¯„ä¼°å™¨ä»¥è§£å†³çš®è‚¤ç—…å­¦å¤šæ¨¡æ€å¤§æ¨¡åž‹å¯é è¯„ä¼°é—®é¢˜**

**å…³é”®è¯**: `çš®è‚¤ç—…å­¦è¯Šæ–­` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `è‡ªåŠ¨è¯„ä¼°å™¨` `ä¸´åºŠéƒ¨ç½²` `æ¨¡åž‹å¯ä¿¡åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçš®è‚¤ç—…å­¦å¤šæ¨¡æ€å¤§æ¨¡åž‹ç”Ÿæˆè¯Šæ–­å™è¿°çš„å¯é è¯„ä¼°æ˜¯ä¸´åºŠéƒ¨ç½²çš„ä¸»è¦ç“¶é¢ˆ
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºDermBenchåŸºå‡†å’ŒDermEvalæ— å‚è€ƒå¤šæ¨¡æ€è¯„ä¼°å™¨ï¼Œæ”¯æŒç»“æž„åŒ–è¯„åˆ†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨4500ä¸ªæ¡ˆä¾‹ä¸Šï¼Œè¯„ä¼°ç»“æžœä¸Žä¸“å®¶è¯„åˆ†åå·®åˆ†åˆ«ä¸º0.251å’Œ0.117

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

