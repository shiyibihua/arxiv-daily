---
layout: default
title: Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks
---

# Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks

**arXiv**: [2511.08883v1](https://arxiv.org/abs/2511.08883) | [PDF](https://arxiv.org/pdf/2511.08883.pdf)

**ä½œè€…**: Cheng Wang, Shuisheng Zhou, Fengjiao Peng, Jin Sheng, Feng Ye, Yinli Dong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šèžåˆå¢žå¼ºViTå—ä»¥æå‡å¯¹æ¯”èšç±»æ€§èƒ½**

**å…³é”®è¯**: `å¯¹æ¯”èšç±»` `è§†è§‰å˜æ¢å™¨` `ç‰¹å¾èžåˆ` `æ•°æ®å¢žå¼º` `å›¾åƒèšç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¯¹æ¯”å­¦ä¹ ç½‘ç»œæœªå……åˆ†åˆ©ç”¨æ­£æ ·æœ¬å¯¹çš„äº’è¡¥æ€§æå–èšç±»ç‰¹å¾
2. è®¾è®¡MFAVBsï¼Œé€šè¿‡ViTèžåˆæ­£æ ·æœ¬ç‰¹å¾å¹¶å¤šæ¬¡å¢žå¼º
3. åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šå®žéªŒï¼Œèšç±»æ€§èƒ½ä¼˜äºŽçŽ°æœ‰æŠ€æœ¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.

