---
layout: default
title: MedDINOv3: How to adapt vision foundation models for medical image segmentation?
---

# MedDINOv3: How to adapt vision foundation models for medical image segmentation?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02379" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02379v3</a>
  <a href="https://arxiv.org/pdf/2509.02379.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02379v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02379v3', 'MedDINOv3: How to adapt vision foundation models for medical image segmentation?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-10-15)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ricklisz/MedDINOv3)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MedDINOv3ï¼šä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è§†è§‰åŸºç¡€æ¨¡å‹è‡ªé€‚åº”æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒåˆ†å‰²` `è§†è§‰åŸºç¡€æ¨¡å‹` `é¢†åŸŸè‡ªé€‚åº”` `è‡ªç›‘ç£å­¦ä¹ ` `ViT` `DINOv3` `CTå›¾åƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹æ³›åŒ–æ€§å·®ï¼Œéš¾ä»¥è·¨æ¨¡æ€å’Œæœºæ„åº”ç”¨ï¼Œé™åˆ¶äº†ä¸´åºŠä»·å€¼ã€‚
2. MedDINOv3é€šè¿‡å¤šå°ºåº¦tokenèšåˆçš„ViTæ¶æ„å’Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œæå‡æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä¸Šçš„åˆ†å‰²æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMedDINOv3åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šå–å¾—äº†SOTAæˆ–æ¥è¿‘SOTAçš„ç»“æœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨CTå’ŒMRIæ‰«æä¸­å¯¹å™¨å®˜å’Œè‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†è‡ªåŠ¨åˆ†å‰²çš„å‘å±•ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç„¶æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ï¼Œç¼ºä¹è·¨æ¨¡æ€å’Œæœºæ„çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ•°åäº¿è§„æ¨¡çš„è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹(FMs)æä¾›äº†å¼ºå¤§ä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°†å®ƒä»¬åº”ç”¨äºåŒ»å­¦æˆåƒé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š(1)å¤§å¤šæ•°åŸºç¡€æ¨¡å‹çš„ViTéª¨å¹²ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä»ç„¶ä¸å¦‚ä¸“é—¨çš„CNNï¼›(2)è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„é¢†åŸŸå·®è·ï¼Œé™åˆ¶äº†å¯è¿ç§»æ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†MedDINOv3ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå°†DINOv3åº”ç”¨äºåŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆé‡æ–°å®¡è§†äº†plain ViTï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰å¤šå°ºåº¦tokenèšåˆçš„ç®€å•è€Œæœ‰æ•ˆçš„æ¶æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šé˜¶æ®µDINOv3æ–¹æ³•åœ¨CT-3Mï¼ˆä¸€ä¸ªåŒ…å«387ä¸‡å¼ è½´å‘CTåˆ‡ç‰‡çš„ç²¾é€‰é›†åˆï¼‰ä¸Šæ‰§è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ é²æ£’çš„å¯†é›†ç‰¹å¾ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç»Ÿä¸€éª¨å¹²ç½‘ç»œçš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨https://github.com/ricklisz/MedDINOv3è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡é¢ä¸´ç€æ•°æ®æ¨¡æ€å¤šæ ·ã€æœºæ„å·®å¼‚å¤§ç­‰é—®é¢˜ï¼Œå¯¼è‡´ç°æœ‰æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚åŒæ—¶ï¼Œç›´æ¥å°†è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œç”±äºé¢†åŸŸå·®å¼‚è¾ƒå¤§ï¼Œæ•ˆæœå¾€å¾€ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMedDINOv3çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è§„æ¨¡åŒ»å­¦å›¾åƒæ•°æ®è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°åŒ»å­¦å›¾åƒçš„é€šç”¨ç‰¹å¾è¡¨ç¤ºã€‚åŒæ—¶ï¼Œé€šè¿‡æ”¹è¿›ViTæ¶æ„ï¼Œä½¿å…¶æ›´é€‚åˆåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œä»è€Œæå‡åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMedDINOv3æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š(1) æ¶æ„æ”¹è¿›ï¼šè®¾è®¡äº†ä¸€ç§å…·æœ‰å¤šå°ºåº¦tokenèšåˆçš„ViTæ¶æ„ï¼Œä»¥æ›´å¥½åœ°æ•æ‰åŒ»å­¦å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚(2) é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼šä½¿ç”¨CT-3Mæ•°æ®é›†ï¼Œé‡‡ç”¨å¤šé˜¶æ®µDINOv3æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹é€‚åº”åŒ»å­¦å›¾åƒçš„ç‰¹å¾åˆ†å¸ƒã€‚é¢„è®­ç»ƒåï¼Œæ¨¡å‹å¯ä»¥ä½œä¸ºä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„éª¨å¹²ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šMedDINOv3çš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†æ”¹è¿›çš„ViTæ¶æ„å’Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡å¤šå°ºåº¦tokenèšåˆï¼ŒViTèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åŒ»å­¦å›¾åƒä¸­çš„å¤æ‚ç»“æ„ã€‚é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒåˆ™æœ‰æ•ˆç¼©å°äº†è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ViTæ¶æ„æ–¹é¢ï¼Œé‡‡ç”¨äº†å¤šå°ºåº¦tokenèšåˆç­–ç•¥ï¼Œé€šè¿‡ä¸åŒå°ºåº¦çš„å·ç§¯æ“ä½œæå–ç‰¹å¾ï¼Œå¹¶è¿›è¡Œèåˆã€‚åœ¨é¢„è®­ç»ƒæ–¹é¢ï¼Œé‡‡ç”¨äº†å¤šé˜¶æ®µDINOv3æ–¹æ³•ï¼Œé€æ­¥æå‡æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚CT-3Mæ•°æ®é›†åŒ…å«387ä¸‡å¼ è½´å‘CTåˆ‡ç‰‡ï¼Œæä¾›äº†å……è¶³çš„è®­ç»ƒæ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MedDINOv3åœ¨å››ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨XXXæ•°æ®é›†ä¸Šï¼ŒMedDINOv3çš„Diceç³»æ•°è¾¾åˆ°äº†X.XXï¼Œè¶…è¿‡äº†ä¹‹å‰çš„SOTAæ–¹æ³•Y.YYã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedDINOv3èƒ½å¤Ÿæœ‰æ•ˆæå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MedDINOv3å¯åº”ç”¨äºå¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œä¾‹å¦‚å™¨å®˜åˆ†å‰²ã€è‚¿ç˜¤åˆ†å‰²ç­‰ï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€æ²»ç–—è®¡åˆ’åˆ¶å®šå’Œç–¾ç—…ç›‘æµ‹ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨åŒ»å­¦å½±åƒåˆ†æçš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–ï¼Œæé«˜è¯Šæ–­æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3.

