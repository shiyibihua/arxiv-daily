---
layout: default
title: Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture
---

# Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02359" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02359v1</a>
  <a href="https://arxiv.org/pdf/2509.02359.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02359v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02359v1', 'Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanyue Zhang, Yibin Huang, Yangbin Xu, JingJing Huang, Helu Zhi, Shuo Ren, Wang Xu, Jiajun Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

**å¤‡æ³¨**: The benchmark MulSeT is available at https://huggingface.co/datasets/WanyueZhang/MulSeT

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿåˆ†æMLLMç©ºé—´ç†è§£èƒ½åŠ›ç“¶é¢ˆï¼Œæå‡ºMulSeTåŸºå‡†å¹¶æ¢ç©¶æ•°æ®ä¸æ¶æ„çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç©ºé—´ç†è§£` `å¤šè§†å›¾å­¦ä¹ ` `åŸºå‡†æµ‹è¯•` `ä½ç½®ç¼–ç `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ç©ºé—´ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„è¯„ä¼°ï¼Œé€šå¸¸å±€é™äºå•ä¸€åœºæ™¯ã€‚
2. æå‡ºMulSeTåŸºå‡†ï¼Œä»æ•°æ®å’Œæ¶æ„è§’åº¦ç³»ç»Ÿåˆ†æMLLMåœ¨å•è§†å›¾ã€å¤šè§†å›¾å’Œè§†é¢‘åœºæ™¯ä¸‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå•çº¯å¢åŠ è®­ç»ƒæ•°æ®æ•ˆæœæœ‰é™ï¼Œç©ºé—´ç†è§£æ›´ä¾èµ–è§†è§‰ç¼–ç å™¨çš„ä½ç½®ç¼–ç ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´ç†è§£å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨å…·èº«ç¯å¢ƒä¸­æ”¯æŒæ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰ç ”ç©¶è¡¨æ˜MLLMåœ¨ç©ºé—´ç†è§£æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ç¼ºä¹å¯¹è¿™äº›å±€é™æ€§çš„å…¨é¢å’Œç³»ç»Ÿçš„è¯„ä¼°ï¼Œé€šå¸¸ä»…é™äºå­¤ç«‹çš„åœºæ™¯ï¼Œå¦‚å•è§†å›¾æˆ–è§†é¢‘ã€‚æœ¬æ–‡ä»æ•°æ®å’Œæ¶æ„çš„è§’åº¦ï¼Œå¯¹å•è§†å›¾ã€å¤šè§†å›¾å’Œè§†é¢‘ä¸‰ç§ä»£è¡¨æ€§åœºæ™¯ä¸‹çš„ç©ºé—´ç†è§£è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºMulSeTï¼ˆå¤šè§†å›¾ç©ºé—´ç†è§£ä»»åŠ¡ï¼‰çš„åŸºå‡†ï¼Œå¹¶è®¾è®¡äº†ä¸€ç³»åˆ—å®éªŒæ¥åˆ†æMLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä»æ•°æ®è§’åº¦æ¥çœ‹ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œç©ºé—´ç†è§£çš„æ€§èƒ½è¿…é€Ÿæ”¶æ•›ï¼Œå¹¶ä¸”ä¸Šé™ç›¸å¯¹è¾ƒä½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦ç©ºé—´æƒ³è±¡çš„ä»»åŠ¡ã€‚è¿™è¡¨æ˜ä»…ä»…æ‰©å±•è®­ç»ƒæ•°æ®ä¸è¶³ä»¥è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚ä»æ¶æ„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å‘ç°ç©ºé—´ç†è§£æ›´ä¾èµ–äºè§†è§‰ç¼–ç å™¨å†…çš„ä½ç½®ç¼–ç ï¼Œè€Œä¸æ˜¯è¯­è¨€æ¨¡å‹å†…çš„ä½ç½®ç¼–ç ï¼Œæ— è®ºæ˜¯åœ¨çº§è”MLLMè¿˜æ˜¯åŸç”ŸMLLMä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨ç†æ³¨å…¥ï¼Œå¹¶é€šè¿‡æ¶æ„è®¾è®¡æ¥ä¼˜åŒ–ç©ºé—´ç†è§£ï¼Œä»è€Œå±•æœ›æœªæ¥çš„æ”¹è¿›ã€‚è¿™äº›è§è§£æ­ç¤ºäº†å½“å‰MLLMçš„å±€é™æ€§ï¼Œå¹¶ä¸ºé€šè¿‡æ•°æ®ç¼©æ”¾å’Œæ¶æ„è°ƒæ•´æ¥æé«˜ç©ºé—´æ¨ç†èƒ½åŠ›æå‡ºäº†æ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šMLLMåœ¨å…·èº«ç¯å¢ƒä¸­è¿›è¡Œæ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’æ—¶ï¼Œéœ€è¦å…·å¤‡å¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰MLLMåœ¨å¤„ç†å•è§†å›¾ã€å¤šè§†å›¾ä»¥åŠè§†é¢‘ç­‰åœºæ™¯ä¸‹çš„ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹è¿™äº›å±€é™æ€§çš„ç³»ç»Ÿæ€§åˆ†æï¼Œéš¾ä»¥æŒ‡å¯¼æ¨¡å‹æ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»æ•°æ®å’Œæ¶æ„ä¸¤ä¸ªç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æMLLMåœ¨ç©ºé—´ç†è§£æ–¹é¢çš„ç“¶é¢ˆã€‚é€šè¿‡æ„å»ºå¤šè§†å›¾ç©ºé—´ç†è§£ä»»åŠ¡åŸºå‡†ï¼ˆMulSeTï¼‰ï¼Œå¹¶è®¾è®¡ä¸€ç³»åˆ—å®éªŒï¼Œæ·±å…¥æ¢ç©¶æ•°æ®è§„æ¨¡ã€æ•°æ®è´¨é‡ä»¥åŠæ¨¡å‹æ¶æ„å¯¹ç©ºé—´ç†è§£èƒ½åŠ›çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡æå‡ºçš„MulSeTåŸºå‡†åŒ…å«å•è§†å›¾ã€å¤šè§†å›¾å’Œè§†é¢‘ä¸‰ç§åœºæ™¯ä¸‹çš„ç©ºé—´ç†è§£ä»»åŠ¡ã€‚ç ”ç©¶äººå‘˜é¦–å…ˆåˆ©ç”¨è¯¥åŸºå‡†è¯„ä¼°ç°æœ‰MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œä»æ•°æ®è§’åº¦ï¼Œåˆ†æè®­ç»ƒæ•°æ®è§„æ¨¡å¯¹æ€§èƒ½çš„å½±å“ï¼›ä»æ¶æ„è§’åº¦ï¼Œåˆ†æè§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¸­ä½ç½®ç¼–ç çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†æ¨ç†æ³¨å…¥æ–¹æ³•ï¼Œå¹¶æå‡ºäº†é€šè¿‡æ¶æ„è®¾è®¡ä¼˜åŒ–ç©ºé—´ç†è§£çš„æ€è·¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¯¹MLLMç©ºé—´ç†è§£èƒ½åŠ›çš„ç³»ç»Ÿæ€§åˆ†ææ¡†æ¶ã€‚é€šè¿‡MulSeTåŸºå‡†ï¼Œç ”ç©¶äººå‘˜å¯ä»¥å…¨é¢è¯„ä¼°MLLMåœ¨ä¸åŒåœºæ™¯ä¸‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†æ•°æ®è§„æ¨¡å’Œæ¨¡å‹æ¶æ„å¯¹ç©ºé—´ç†è§£èƒ½åŠ›çš„ä¸åŒå½±å“ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹æ”¹è¿›æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šMulSeTåŸºå‡†åŒ…å«å¤šç§ç©ºé—´ç†è§£ä»»åŠ¡ï¼Œä¾‹å¦‚ç›®æ ‡å®šä½ã€åœºæ™¯ç†è§£å’Œè¿åŠ¨é¢„æµ‹ã€‚åœ¨å®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†ä¸åŒçš„MLLMæ¨¡å‹ï¼Œå¹¶è°ƒæ•´äº†è®­ç»ƒæ•°æ®è§„æ¨¡å’Œæ¨¡å‹æ¶æ„ã€‚ä»–ä»¬è¿˜æ¢ç´¢äº†ä¸åŒçš„ä½ç½®ç¼–ç æ–¹æ³•å’Œæ¨ç†æ³¨å…¥ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå•çº¯å¢åŠ è®­ç»ƒæ•°æ®å¯¹ç©ºé—´ç†è§£èƒ½åŠ›çš„æå‡æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç©ºé—´æƒ³è±¡çš„ä»»åŠ¡ä¸­ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰ç¼–ç å™¨ä¸­çš„ä½ç½®ç¼–ç å¯¹ç©ºé—´ç†è§£è‡³å…³é‡è¦ï¼Œå…¶é‡è¦æ€§é«˜äºè¯­è¨€æ¨¡å‹ä¸­çš„ä½ç½®ç¼–ç ã€‚é€šè¿‡æ¨ç†æ³¨å…¥å’Œæ¶æ„è°ƒæ•´ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„è‡ªä¸»è¡Œä¸ºã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ï¼Œä»¥æé«˜MLLMåœ¨æ›´å¤æ‚åœºæ™¯ä¸‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial understanding is essential for Multimodal Large Language Models (MLLMs) to support perception, reasoning, and planning in embodied environments. Despite recent progress, existing studies reveal that MLLMs still struggle with spatial understanding. However, existing research lacks a comprehensive and systematic evaluation of these limitations, often restricted to isolated scenarios, such as single-view or video. In this work, we present a systematic analysis of spatial understanding from both data and architectural perspectives across three representative scenarios: single-view, multi-view, and video. We propose a benchmark named MulSeT (Multi-view Spatial Understanding Tasks), and design a series of experiments to analyze the spatial reasoning capabilities of MLLMs. From the data perspective, the performance of spatial understanding converges quickly as the training data increases, and the upper bound is relatively low, especially for tasks that require spatial imagination. This indicates that merely expanding training data is insufficient to achieve satisfactory performance. From the architectural perspective, we find that spatial understanding relies more heavily on the positional encoding within the visual encoder than within the language model, in both cascaded and native MLLMs. Moreover, we explore reasoning injection and envision future improvements through architectural design to optimize spatial understanding. These insights shed light on the limitations of current MLLMs and suggest new directions for improving spatial reasoning capabilities through data scaling and architectural tuning.

