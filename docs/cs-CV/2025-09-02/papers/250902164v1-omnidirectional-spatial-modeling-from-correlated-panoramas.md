---
layout: default
title: Omnidirectional Spatial Modeling from Correlated Panoramas
---

# Omnidirectional Spatial Modeling from Correlated Panoramas

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02164" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02164v1</a>
  <a href="https://arxiv.org/pdf/2509.02164.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02164v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02164v1', 'Omnidirectional Spatial Modeling from Correlated Panoramas')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinshen Zhang, Tongxi Fu, Xu Zheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCFpanoæ•°æ®é›†ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥è§£å†³å…¨æ™¯å›¾åƒç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…¨æ™¯å›¾åƒç†è§£` `è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `æ•°æ®é›†æ„å»º` `è·¨å¸§æ¨ç†` `è‡ªåŠ¨é©¾é©¶` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…¨æ™¯å›¾åƒç†è§£æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•å¸§å†…ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨è·¨å¸§çš„ç›¸å…³ä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£èƒ½åŠ›ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºCFpanoæ•°æ®é›†ï¼Œä¸“æ³¨äºè·¨å¸§ç›¸å…³å…¨æ™¯è§†è§‰é—®ç­”ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œæ•´ä½“æ€§èƒ½æå‡5.37%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨æ™¯åœºæ™¯ç†è§£å¯¹äºå…·èº«äººå·¥æ™ºèƒ½ã€è‡ªåŠ¨é©¾é©¶å’Œæ²‰æµ¸å¼ç¯å¢ƒç­‰å¤šç§ä¸‹æ¸¸åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ç”±äº360Â°å›¾åƒä¸­çš„å‡ ä½•å¤±çœŸå’Œå¤æ‚ç©ºé—´å…³ç³»ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„å…¨æ™¯æ–¹æ³•ä¸»è¦åœ¨å•å¸§å†…è¿›è¡Œåœºæ™¯ç†è§£ï¼Œè€Œå¿½è§†äº†è·¨å¸§ç›¸å…³å…¨æ™¯å›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CFpanoï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºè·¨å¸§ç›¸å…³å…¨æ™¯è§†è§‰é—®ç­”çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«2700å¤šå¼ å›¾åƒå’Œ8000å¤šä¸ªé—®ç­”å¯¹ã€‚åŸºäºCFpanoï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ—¨åœ¨å®ç°å¯¹è·¨å¸§ç›¸å…³å…¨æ™¯çš„ç¨³å¥æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ•´ä½“è¡¨ç°æå‡äº†5.37%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å…¨æ™¯å›¾åƒç†è§£ä¸­çš„è·¨å¸§ä¿¡æ¯åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªåœ¨å•å¸§å†…è¿›è¡Œåˆ†æï¼Œå¿½è§†äº†ä¸åŒå¸§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºCFpanoæ•°æ®é›†ï¼Œä¸“æ³¨äºè·¨å¸§ç›¸å…³å…¨æ™¯è§†è§‰é—®ç­”ï¼Œé€šè¿‡å¼•å…¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¹¶ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå®ç°å¯¹å…¨æ™¯åœºæ™¯çš„å…¨é¢ç†è§£å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚CFpanoæ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„å›¾åƒå’Œé—®ç­”å¯¹ï¼ŒMLLMåˆ™é€šè¿‡GRPOè¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºå…¶åœ¨è·¨å¸§æ¨ç†ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºCFpanoæ•°æ®é›†çš„æ„å»ºå’ŒGRPOçš„åº”ç”¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è·¨å¸§ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†å¤šç§é—®ç­”ç±»å‹æ—¶çš„ç¨³å¥æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨CFpanoæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ•´ä½“æ€§èƒ½æå‡5.37%ã€‚è¯¥æ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸­å‡è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å…¨æ™¯åœºæ™¯ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æé«˜å…¨æ™¯å›¾åƒçš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´æ™ºèƒ½çš„å†³ç­–æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Omnidirectional scene understanding is vital for various downstream applications, such as embodied AI, autonomous driving, and immersive environments, yet remains challenging due to geometric distortion and complex spatial relations in 360Â° imagery. Existing omnidirectional methods achieve scene understanding within a single frame while neglecting cross-frame correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the \textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas visual question answering in the holistic 360Â° scenes. CFpano consists of over 2700 images together with over 8000 question-answer pairs, and the question types include both multiple choice and open-ended VQA. Building upon our CFpano, we further present \methodname, a multi-modal large language model (MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of tailored reward functions for robust and consistent reasoning with cross-frame correlated panoramas. Benchmark experiments with existing MLLMs are conducted with our CFpano. The experimental results demonstrate that \methodname achieves state-of-the-art performance across both multiple-choice and open-ended VQA tasks, outperforming strong baselines on all major reasoning categories (\textbf{+5.37\% } in overall performance). Our analyses validate the effectiveness of GRPO and establish a new benchmark for panoramic scene understanding.

