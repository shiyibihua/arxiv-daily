---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-02
---

# cs.CVï¼ˆ2025-09-02ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250907994v1-strokevision-bench-a-multimodal-video-and-2d-pose-benchmark-for-trac.html">STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery</a></td>
  <td>StrokeVision-Benchï¼šç”¨äºè·Ÿè¸ªä¸­é£æ¢å¤çš„å¤šæ¨¡æ€è§†é¢‘å’Œ2Då§¿æ€åŸºå‡†æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07994v1" data-paper-url="./papers/250907994v1-strokevision-bench-a-multimodal-video-and-2d-pose-benchmark-for-trac.html" onclick="toggleFavorite(this, '2509.07994v1', 'STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250902710v1-toward-a-robust-lesion-detection-model-in-breast-dce-mri-adapting-fo.html">Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women</a></td>
  <td>é’ˆå¯¹é«˜å±å¥³æ€§ï¼Œæå‡ºåŸºäºåŒ»å­¦åˆ‡ç‰‡Transformerå’ŒKANçš„ä¹³è…ºDCE-MRIç—…ç¶æ£€æµ‹æ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02710v1" data-paper-url="./papers/250902710v1-toward-a-robust-lesion-detection-model-in-breast-dce-mri-adapting-fo.html" onclick="toggleFavorite(this, '2509.02710v1', 'Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250902379v3-meddinov3-how-to-adapt-vision-foundation-models-for-medical-image-se.html">MedDINOv3: How to adapt vision foundation models for medical image segmentation?</a></td>
  <td>MedDINOv3ï¼šä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è§†è§‰åŸºç¡€æ¨¡å‹è‡ªé€‚åº”æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02379v3" data-paper-url="./papers/250902379v3-meddinov3-how-to-adapt-vision-foundation-models-for-medical-image-se.html" onclick="toggleFavorite(this, '2509.02379v3', 'MedDINOv3: How to adapt vision foundation models for medical image segmentation?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250902322v1-omniactor-a-generalist-gui-and-embodied-agent-for-2d3d-worlds.html">OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds</a></td>
  <td>OmniActorï¼šä¸€ç§ç”¨äº2Då’Œ3Dä¸–ç•Œçš„é€šç”¨GUIå’Œå…·èº«æ™ºèƒ½ä½“</td>
  <td class="tags-cell"><span class="paper-tag">generalist agent</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02322v1" data-paper-url="./papers/250902322v1-omniactor-a-generalist-gui-and-embodied-agent-for-2d3d-worlds.html" onclick="toggleFavorite(this, '2509.02322v1', 'OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250902256v1-a-multimodal-cross-view-model-for-predicting-postoperative-neck-pain.html">A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients</a></td>
  <td>æå‡ºABPDCå’ŒFPRANæ¨¡å‹ï¼Œé¢„æµ‹é¢ˆæ¤ç—…æ‚£è€…æœ¯åé¢ˆéƒ¨ç–¼ç—›æ¢å¤æƒ…å†µ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02256v1" data-paper-url="./papers/250902256v1-a-multimodal-cross-view-model-for-predicting-postoperative-neck-pain.html" onclick="toggleFavorite(this, '2509.02256v1', 'A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250906987v1-fusway-multimodal-hybrid-fusion-approach-application-to-railway-defe.html">FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection</a></td>
  <td>æå‡ºFusWayå¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œç”¨äºæå‡é“è·¯ç¼ºé™·æ£€æµ‹ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.06987v1" data-paper-url="./papers/250906987v1-fusway-multimodal-hybrid-fusion-approach-application-to-railway-defe.html" onclick="toggleFavorite(this, '2509.06987v1', 'FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250902359v1-why-do-mllms-struggle-with-spatial-understanding-a-systematic-analys.html">Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture</a></td>
  <td>ç³»ç»Ÿåˆ†æMLLMç©ºé—´ç†è§£èƒ½åŠ›ç“¶é¢ˆï¼Œæå‡ºMulSeTåŸºå‡†å¹¶æ¢ç©¶æ•°æ®ä¸æ¶æ„çš„å½±å“</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02359v1" data-paper-url="./papers/250902359v1-why-do-mllms-struggle-with-spatial-understanding-a-systematic-analys.html" onclick="toggleFavorite(this, '2509.02359v1', 'Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250906990v1-diet-cp-lightweight-and-data-efficient-self-supervised-continued-pre.html">DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining</a></td>
  <td>DIET-CPï¼šè½»é‡çº§ä¸”æ•°æ®é«˜æ•ˆçš„è‡ªç›‘ç£æŒç»­é¢„è®­ç»ƒæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.06990v1" data-paper-url="./papers/250906990v1-diet-cp-lightweight-and-data-efficient-self-supervised-continued-pre.html" onclick="toggleFavorite(this, '2509.06990v1', 'DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250902175v2-understanding-space-is-rocket-science-only-top-reasoning-models-can-.html">Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks</a></td>
  <td>æå‡ºRocketScienceåŸºå‡†ï¼Œæ­ç¤ºç°æœ‰VLMåœ¨ç©ºé—´å…³ç³»ç†è§£ä¸Šçš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02175v2" data-paper-url="./papers/250902175v2-understanding-space-is-rocket-science-only-top-reasoning-models-can-.html" onclick="toggleFavorite(this, '2509.02175v2', 'Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250902164v1-omnidirectional-spatial-modeling-from-correlated-panoramas.html">Omnidirectional Spatial Modeling from Correlated Panoramas</a></td>
  <td>æå‡ºCFpanoæ•°æ®é›†ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥è§£å†³å…¨æ™¯å›¾åƒç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02164v1" data-paper-url="./papers/250902164v1-omnidirectional-spatial-modeling-from-correlated-panoramas.html" onclick="toggleFavorite(this, '2509.02164v1', 'Omnidirectional Spatial Modeling from Correlated Panoramas')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250902560v2-fastvggt-training-free-acceleration-of-visual-geometry-transformer.html">FastVGGT: Training-Free Acceleration of Visual Geometry Transformer</a></td>
  <td>FastVGGTï¼šé€šè¿‡æ— è®­ç»ƒTokenåˆå¹¶åŠ é€Ÿè§†è§‰å‡ ä½•Transformerï¼Œæå‡3Dè§†è§‰æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">VGGT</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02560v2" data-paper-url="./papers/250902560v2-fastvggt-training-free-acceleration-of-visual-geometry-transformer.html" onclick="toggleFavorite(this, '2509.02560v2', 'FastVGGT: Training-Free Acceleration of Visual Geometry Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250902545v1-motion-refined-dinosaur-for-unsupervised-multi-object-discovery.html">Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery</a></td>
  <td>æå‡ºMotion-Refined DINOSAURï¼Œç”¨äºæ— ç›‘ç£å¤šç›®æ ‡å‘ç°ï¼Œæ— éœ€ä¼ªæ ‡ç­¾ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02545v1" data-paper-url="./papers/250902545v1-motion-refined-dinosaur-for-unsupervised-multi-object-discovery.html" onclick="toggleFavorite(this, '2509.02545v1', 'Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250902807v1-pixfoundation-20-do-video-multi-modal-llms-use-motion-in-visual-grou.html">PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</a></td>
  <td>PixFoundation 2.0ï¼šè¯„ä¼°è§†é¢‘å¤šæ¨¡æ€LLMåœ¨è§†è§‰å®šä½ä¸­å¯¹è¿åŠ¨ä¿¡æ¯çš„åˆ©ç”¨ç¨‹åº¦</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span> <span class="paper-tag">visual grounding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02807v1" data-paper-url="./papers/250902807v1-pixfoundation-20-do-video-multi-modal-llms-use-motion-in-visual-grou.html" onclick="toggleFavorite(this, '2509.02807v1', 'PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250902424v2-faster-and-better-reinforced-collaborative-distillation-and-self-lea.html">Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„ååŒè’¸é¦ä¸è‡ªå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºçº¢å¤–-å¯è§å…‰å›¾åƒèåˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02424v2" data-paper-url="./papers/250902424v2-faster-and-better-reinforced-collaborative-distillation-and-self-lea.html" onclick="toggleFavorite(this, '2509.02424v2', 'Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250902287v1-synthgennet-a-self-supervised-approach-for-test-time-generalization-.html">SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images</a></td>
  <td>SynthGenNetï¼šåˆ©ç”¨åˆæˆè¡—æ™¯å›¾åƒå¤šæºåŸŸæ··åˆå®ç°æµ‹è¯•æ—¶æ³›åŒ–çš„è‡ªç›‘ç£æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02287v1" data-paper-url="./papers/250902287v1-synthgennet-a-self-supervised-approach-for-test-time-generalization-.html" onclick="toggleFavorite(this, '2509.02287v1', 'SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)