---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-20
---

# cs.CVï¼ˆ2025-09-20ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250921354v2-kv-efficient-vla-a-method-to-speed-up-vision-language-models-with-rn.html">KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache</a></td>
  <td>KV-Efficient VLAï¼šåˆ©ç”¨RNNé—¨æ§åˆ†å—KVç¼“å­˜åŠ é€Ÿè§†è§‰è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21354v2" data-paper-url="./papers/250921354v2-kv-efficient-vla-a-method-to-speed-up-vision-language-models-with-rn.html" onclick="toggleFavorite(this, '2509.21354v2', 'KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250916768v1-mmpart-harnessing-multi-modal-large-language-models-for-part-aware-3.html">MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation</a></td>
  <td>MMPartï¼šåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œéƒ¨ä»¶æ„ŸçŸ¥çš„3Dç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16768v1" data-paper-url="./papers/250916768v1-mmpart-harnessing-multi-modal-large-language-models-for-part-aware-3.html" onclick="toggleFavorite(this, '2509.16768v1', 'MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250916702v1-animalbooth-multimodal-feature-enhancement-for-animal-subject-person.html">Animalbooth: multimodal feature enhancement for animal subject personalization</a></td>
  <td>AnimalBoothï¼šé€šè¿‡å¤šæ¨¡æ€ç‰¹å¾å¢å¼ºå®ç°åŠ¨ç‰©ä¸»é¢˜ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16702v1" data-paper-url="./papers/250916702v1-animalbooth-multimodal-feature-enhancement-for-animal-subject-person.html" onclick="toggleFavorite(this, '2509.16702v1', 'Animalbooth: multimodal feature enhancement for animal subject personalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250916617v1-detection-and-simulation-of-urban-heat-islands-using-a-fine-tuned-ge.html">Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model</a></td>
  <td>åˆ©ç”¨å¾®è°ƒçš„åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹è¿›è¡ŒåŸå¸‚çƒ­å²›æ£€æµ‹ä¸æ¨¡æ‹Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16617v1" data-paper-url="./papers/250916617v1-detection-and-simulation-of-urban-heat-islands-using-a-fine-tuned-ge.html" onclick="toggleFavorite(this, '2509.16617v1', 'Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250916538v1-advancing-reference-free-evaluation-of-video-captions-with-factual-a.html">Advancing Reference-free Evaluation of Video Captions with Factual Analysis</a></td>
  <td>æå‡ºVC-Inspectorï¼Œä¸€ç§åŸºäºäº‹å®åˆ†æçš„è§†é¢‘å­—å¹•æ— å‚è€ƒè¯„ä»·æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16538v1" data-paper-url="./papers/250916538v1-advancing-reference-free-evaluation-of-video-captions-with-factual-a.html" onclick="toggleFavorite(this, '2509.16538v1', 'Advancing Reference-free Evaluation of Video Captions with Factual Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250916677v1-segment-to-act-label-noise-robust-action-prompted-video-segmentation.html">Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence</a></td>
  <td>æå‡ºActiSeg-NLåŸºå‡†ï¼Œç ”ç©¶æ ‡ç­¾å™ªå£°ä¸‹åŠ¨ä½œå¼•å¯¼çš„è§†é¢‘åˆ†å‰²ï¼Œå¹¶æå‡ºPMHMæå‡é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16677v1" data-paper-url="./papers/250916677v1-segment-to-act-label-noise-robust-action-prompted-video-segmentation.html" onclick="toggleFavorite(this, '2509.16677v1', 'Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/250916721v1-text-scene-a-scene-to-language-parsing-framework-for-3d-scene-unders.html">Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding</a></td>
  <td>Text-Sceneï¼šæå‡ºä¸€ç§åœºæ™¯åˆ°è¯­è¨€çš„è§£ææ¡†æ¶ï¼Œç”¨äº3Dåœºæ™¯ç†è§£ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">affordance</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16721v1" data-paper-url="./papers/250916721v1-text-scene-a-scene-to-language-parsing-framework-for-3d-scene-unders.html" onclick="toggleFavorite(this, '2509.16721v1', 'Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250916552v1-st-gs-vision-based-3d-semantic-occupancy-prediction-with-spatial-tem.html">ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting</a></td>
  <td>æå‡ºST-GSæ¡†æ¶ï¼Œé€šè¿‡æ—¶ç©ºé«˜æ–¯æº…å°„æå‡è§†è§‰ä¸­å¿ƒè‡ªåŠ¨é©¾é©¶ä¸­çš„3Dè¯­ä¹‰å æ®é¢„æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16552v1" data-paper-url="./papers/250916552v1-st-gs-vision-based-3d-semantic-occupancy-prediction-with-spatial-tem.html" onclick="toggleFavorite(this, '2509.16552v1', 'ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250916806v1-medgs-gaussian-splatting-for-multi-modal-3d-medical-imaging.html">MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging</a></td>
  <td>MedGSï¼šåŸºäºé«˜æ–¯æº…å°„çš„å¤šæ¨¡æ€3DåŒ»å­¦å½±åƒé‡å»ºä¸æ’å€¼</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16806v1" data-paper-url="./papers/250916806v1-medgs-gaussian-splatting-for-multi-modal-3d-medical-imaging.html" onclick="toggleFavorite(this, '2509.16806v1', 'MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250916588v1-sqs-enhancing-sparse-perception-models-via-query-based-splatting-in-.html">SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving</a></td>
  <td>SQSï¼šåŸºäºæŸ¥è¯¢Splattingå¢å¼ºè‡ªåŠ¨é©¾é©¶ç¨€ç–æ„ŸçŸ¥æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16588v1" data-paper-url="./papers/250916588v1-sqs-enhancing-sparse-perception-models-via-query-based-splatting-in-.html" onclick="toggleFavorite(this, '2509.16588v1', 'SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250916618v1-surgical-mamballm-mamba2-enhanced-multimodal-large-language-model-fo.html">Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery</a></td>
  <td>Surgical-MambaLLMï¼šåŸºäºMamba2å¢å¼ºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæœºå™¨äººæ‰‹æœ¯ä¸­çš„è§†è§‰é—®é¢˜å®šä½å›ç­”</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16618v1" data-paper-url="./papers/250916618v1-surgical-mamballm-mamba2-enhanced-multimodal-large-language-model-fo.html" onclick="toggleFavorite(this, '2509.16618v1', 'Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250922697v1-learning-hyperspectral-images-with-curated-text-prompts-for-efficien.html">Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment</a></td>
  <td>åˆ©ç”¨æ–‡æœ¬æç¤ºå­¦ä¹ é«˜å…‰è°±å›¾åƒï¼Œå®ç°é«˜æ•ˆå¤šæ¨¡æ€å¯¹é½</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">HSI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22697v1" data-paper-url="./papers/250922697v1-learning-hyperspectral-images-with-curated-text-prompts-for-efficien.html" onclick="toggleFavorite(this, '2509.22697v1', 'Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250916560v1-captioning-for-text-video-retrieval-via-dual-group-direct-preference.html">Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization</a></td>
  <td>æå‡ºCaRe-DPOæ¡†æ¶ï¼Œé€šè¿‡åŒç»„ç›´æ¥åå¥½ä¼˜åŒ–æå‡æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä¸­å­—å¹•ç”Ÿæˆè´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16560v1" data-paper-url="./papers/250916560v1-captioning-for-text-video-retrieval-via-dual-group-direct-preference.html" onclick="toggleFavorite(this, '2509.16560v1', 'Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250916557v1-person-identification-from-egocentric-human-object-interactions-usin.html">Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose</a></td>
  <td>I2Sæ¡†æ¶ï¼šåˆ©ç”¨3Dæ‰‹éƒ¨å§¿æ€è¿›è¡Œäºº-ç‰©äº¤äº’çš„ç”¨æˆ·èº«ä»½è¯†åˆ«</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16557v1" data-paper-url="./papers/250916557v1-person-identification-from-egocentric-human-object-interactions-usin.html" onclick="toggleFavorite(this, '2509.16557v1', 'Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250916748v1-hyplanehead-rethinking-tri-plane-like-representations-in-full-head-i.html">HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis</a></td>
  <td>æå‡ºHyPlaneHeadï¼Œé€šè¿‡æ··åˆå¹³é¢è¡¨ç¤ºå®ç°é«˜è´¨é‡å…¨å¤´éƒ¨å›¾åƒåˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">penetration</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16748v1" data-paper-url="./papers/250916748v1-hyplanehead-rethinking-tri-plane-like-representations-in-full-head-i.html" onclick="toggleFavorite(this, '2509.16748v1', 'HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)