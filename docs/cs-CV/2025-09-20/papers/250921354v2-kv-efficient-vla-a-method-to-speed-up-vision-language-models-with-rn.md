---
layout: default
title: KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache
---

# KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21354" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21354v2</a>
  <a href="https://arxiv.org/pdf/2509.21354.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21354v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21354v2', 'KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanshun Xu, Long Zhuang, Lianlei Shan

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-11-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**KV-Efficient VLAï¼šåˆ©ç”¨RNNé—¨æ§åˆ†å—KVç¼“å­˜åŠ é€Ÿè§†è§‰è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `KVç¼“å­˜å‹ç¼©` `RNNé—¨æ§` `æ¨ç†åŠ é€Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡å‹åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’ŒKVç¼“å­˜å¤§å†…å­˜éœ€æ±‚çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…æœºå™¨äººåº”ç”¨ä¸­çš„æ‰©å±•æ€§ã€‚
2. KV-Efficient VLAé€šè¿‡RNNé—¨æ§æœºåˆ¶ï¼Œå¯¹KVç¼“å­˜è¿›è¡Œåˆ†å—å’Œé€‰æ‹©æ€§è¿‡æ»¤ï¼Œä¿ç•™é«˜å®ç”¨æ€§ä¸Šä¸‹æ–‡ï¼Œé™ä½è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨FLOPsã€æ¨ç†é€Ÿåº¦å’ŒKVå†…å­˜å ç”¨æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”æ˜“äºé›†æˆåˆ°ç°æœ‰VLAæ¡†æ¶ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹ä¸ºæœºå™¨äººæ„ŸçŸ¥å’Œæ§åˆ¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä½†å…¶æ‰©å±•åˆ°çœŸå®ä¸–ç•Œã€é•¿æ—¶ç¨‹ä»»åŠ¡çš„èƒ½åŠ›å—åˆ°æ³¨æ„åŠ›æœºåˆ¶çš„é«˜è®¡ç®—æˆæœ¬ä»¥åŠæ¨ç†è¿‡ç¨‹ä¸­å­˜å‚¨é”®å€¼(KV)å¯¹æ‰€éœ€çš„å¤§é‡å†…å­˜çš„é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨ä¿ç•™å†å²å›¾åƒtokensä½œä¸ºä¸Šä¸‹æ–‡æ—¶ã€‚æœ€è¿‘çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ‰©å±•éª¨å¹²æ¶æ„ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œè€Œè¾ƒå°‘å¼ºè°ƒè§£å†³å®æ—¶ä½¿ç”¨å¿…ä¸å¯å°‘çš„æ¨ç†æ•ˆç‡é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†KV-Efficient VLAï¼Œè¿™æ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„å†…å­˜å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥è½»é‡çº§æœºåˆ¶æ¥é€‰æ‹©æ€§åœ°ä¿ç•™é«˜å®ç”¨æ€§çš„ä¸Šä¸‹æ–‡æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†KVç¼“å­˜åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„å—ï¼Œå¹¶é‡‡ç”¨å¾ªç¯é—¨æ§æ¨¡å—æ¥æ ¹æ®å­¦ä¹ åˆ°çš„æ•ˆç”¨åˆ†æ•°æ€»ç»“å’Œè¿‡æ»¤å†å²ä¸Šä¸‹æ–‡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨ä¿ç•™æœ€è¿‘çš„ç»†ç²’åº¦ç»†èŠ‚ï¼ŒåŒæ—¶ç§¯æåœ°ä¿®å‰ªé™ˆæ—§çš„ã€ä½ç›¸å…³æ€§çš„å†…å­˜ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¹³å‡å¯ä»¥èŠ‚çœ24.6%çš„FLOPsï¼Œæé«˜1.34å€çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶å‡å°‘1.87å€çš„KVå†…å­˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æœ€æ–°çš„VLAå †æ ˆä¸­ï¼Œä»è€Œå®ç°å¯æ‰©å±•çš„æ¨ç†ï¼Œè€Œæ— éœ€ä¿®æ”¹ä¸‹æ¸¸æ§åˆ¶é€»è¾‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡å‹åœ¨å¤„ç†é•¿æ—¶ç¨‹æœºå™¨äººä»»åŠ¡æ—¶ï¼Œéœ€è¦å­˜å‚¨å¤§é‡çš„å†å²å›¾åƒtokensä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´KVç¼“å­˜å ç”¨å¤§é‡å†…å­˜ï¼Œæ¨ç†é€Ÿåº¦æ…¢ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æå‡æ¨¡å‹æœ¬èº«çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå¿½ç•¥äº†æ¨ç†æ•ˆç‡çš„ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸€ç§è½»é‡çº§çš„æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°ä¿ç•™é«˜å®ç”¨æ€§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå‡å°‘KVç¼“å­˜çš„å¤§å°ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åœ¨ä¸æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå°½å¯èƒ½åœ°å‹ç¼©KVç¼“å­˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKV-Efficient VLAä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) KVç¼“å­˜åˆ†å—ï¼šå°†KVç¼“å­˜åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„å—ã€‚2) RNNé—¨æ§æ¨¡å—ï¼šä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å¯¹æ¯ä¸ªKVç¼“å­˜å—è¿›è¡Œç¼–ç ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªæ•ˆç”¨åˆ†æ•°ï¼Œç”¨äºè¡¡é‡è¯¥å—çš„é‡è¦æ€§ã€‚3) ä¸Šä¸‹æ–‡è¿‡æ»¤ï¼šæ ¹æ®å­¦ä¹ åˆ°çš„æ•ˆç”¨åˆ†æ•°ï¼Œé€‰æ‹©æ€§åœ°ä¿ç•™é«˜å®ç”¨æ€§çš„KVç¼“å­˜å—ï¼Œä¸¢å¼ƒä½å®ç”¨æ€§çš„å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨RNNé—¨æ§æœºåˆ¶æ¥åŠ¨æ€åœ°è¯„ä¼°å’Œè¿‡æ»¤KVç¼“å­˜å—ã€‚ä¸ä¼ ç»Ÿçš„é™æ€å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”åœ°é€‰æ‹©ä¿ç•™å“ªäº›ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šRNNé—¨æ§æ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚åŒ…æ‹¬ï¼šRNNçš„ç±»å‹ï¼ˆä¾‹å¦‚GRUæˆ–LSTMï¼‰ã€éšè—å±‚å¤§å°ã€æ•ˆç”¨åˆ†æ•°çš„è®¡ç®—æ–¹å¼ï¼ˆä¾‹å¦‚ä½¿ç”¨sigmoidå‡½æ•°å°†RNNçš„è¾“å‡ºæ˜ å°„åˆ°0-1ä¹‹é—´ï¼‰ä»¥åŠé€‰æ‹©ä¿ç•™çš„KVç¼“å­˜å—çš„æ¯”ä¾‹ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘å¦‚ä½•å¹³è¡¡æ¨¡å‹æ€§èƒ½å’ŒKVç¼“å­˜å‹ç¼©ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒKV-Efficient VLAåœ¨VLAæ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•å¹³å‡èŠ‚çœäº†24.6%çš„FLOPsï¼Œæé«˜äº†1.34å€çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶å‡å°‘äº†1.87å€çš„KVå†…å­˜å ç”¨ã€‚è¿™äº›æå‡æ˜¯åœ¨æ²¡æœ‰æ˜¾è‘—é™ä½æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹å®ç°çš„ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é•¿æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è‡ªä¸»å¯¼èˆªã€é•¿æœŸè§„åˆ’å’Œå¤æ‚æ“ä½œã€‚é€šè¿‡é™ä½VLAæ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œå¯ä»¥ä½¿å…¶æ›´å®¹æ˜“éƒ¨ç½²åœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šï¼Œå¹¶æé«˜å…¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–åŸºäºTransformerçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥æé«˜å…¶æ¨ç†æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models offer a unified framework for robotic perception and control, but their ability to scale to real-world, long-horizon tasks is limited by the high computational cost of attention and the large memory required for storing key-value (KV) pairs during inference, particularly when retaining historical image tokens as context. Recent methods have focused on scaling backbone architectures to improve generalization, with less emphasis on addressing inference inefficiencies essential for real-time use. In this work, we present KV-Efficient VLA, a model-agnostic memory compression approach designed to address these limitations by introducing a lightweight mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed-size chunks and employs a recurrent gating module to summarize and filter the historical context according to learned utility scores. This design aims to preserve recent fine-grained detail while aggressively pruning stale, low-relevance memory. Based on experiments, our approach can yield an average of 24.6% FLOPs savings, 1.34x inference speedup, and 1.87x reduction in KV memory. Our method integrates seamlessly into recent VLA stacks, enabling scalable inference without modifying downstream control logic.

