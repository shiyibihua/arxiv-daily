---
layout: default
title: Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose
---

# Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16557" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16557v1</a>
  <a href="https://arxiv.org/pdf/2509.16557.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16557v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16557v1', 'Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muhammad Hamza, Danish Hamid, Muhammad Tahir Akram

**åˆ†ç±»**: cs.CV, cs.ET, cs.HC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: 21 pages, 8 figures, 7 tables. Preprint of a manuscript submitted to CCF Transactions on Pervasive Computing and Interaction (Springer), currently under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**I2Sæ¡†æ¶ï¼šåˆ©ç”¨3Dæ‰‹éƒ¨å§¿æ€è¿›è¡Œäºº-ç‰©äº¤äº’çš„ç”¨æˆ·èº«ä»½è¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `äºº-ç‰©äº¤äº’è¯†åˆ«` `3Dæ‰‹éƒ¨å§¿æ€` `ç”¨æˆ·èº«ä»½è¯†åˆ«` `å¢å¼ºç°å®` `ç‰¹å¾å·¥ç¨‹` `æ·±åº¦å­¦ä¹ ` `è½»é‡çº§æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¢å¼ºç°å®è¾…åŠ©æŠ€æœ¯åœ¨äººæœºäº¤äº’ç¯å¢ƒä¸­é¢ä¸´ç”¨æˆ·èº«ä»½è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´ç²¾å‡†å’Œéä¾µå…¥å¼çš„è§£å†³æ–¹æ¡ˆã€‚
2. I2Sæ¡†æ¶é€šè¿‡åˆ†æ3Dæ‰‹éƒ¨å§¿æ€å’Œäºº-ç‰©äº¤äº’ï¼Œå®ç°ç”¨æˆ·èº«ä»½çš„æ— å¹²æ‰°è¯†åˆ«ï¼Œæå‡äº†ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œä¸ªæ€§åŒ–ä½“éªŒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒI2Sæ¡†æ¶åœ¨ç”¨æˆ·è¯†åˆ«æ–¹é¢å–å¾—äº†97.52%çš„F1åˆ†æ•°ï¼Œæ¨¡å‹è½»é‡ä¸”æ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºI2Sï¼ˆInteract2Signï¼‰ï¼Œä¸€ä¸ªå¤šé˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡äºº-ç‰©äº¤äº’è¯†åˆ«è¿›è¡Œæ— å¹²æ‰°çš„ç”¨æˆ·èº«ä»½è¯†åˆ«ï¼Œåˆ©ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­çš„3Dæ‰‹éƒ¨å§¿æ€åˆ†æã€‚I2Såˆ©ç”¨ä»3Dæ‰‹éƒ¨å§¿æ€ä¸­æå–çš„æ‰‹å·¥ç‰¹å¾ï¼Œå¹¶æ‰§è¡Œé¡ºåºç‰¹å¾å¢å¼ºï¼šé¦–å…ˆè¯†åˆ«ç‰©ä½“ç±»åˆ«ï¼Œç„¶åè¿›è¡ŒHOIè¯†åˆ«ï¼Œæœ€åè¿›è¡Œç”¨æˆ·èº«ä»½è¯†åˆ«ã€‚å¯¹3Dæ‰‹éƒ¨å§¿æ€è¿›è¡Œäº†å…¨é¢çš„ç‰¹å¾æå–å’Œæè¿°è¿‡ç¨‹ï¼Œå°†æå–çš„ç‰¹å¾ç»„ç»‡æˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç±»åˆ«ï¼šç©ºé—´ã€é¢‘ç‡ã€è¿åŠ¨å­¦ã€æ–¹å‘ï¼Œä»¥åŠæœ¬ç ”ç©¶ä¸­å¼•å…¥çš„æ–°å‹æè¿°ç¬¦ï¼Œå³æ‰‹é—´ç©ºé—´åŒ…ç»œï¼ˆIHSEï¼‰ã€‚è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„ç‰¹å¾ç»„åˆã€‚åœ¨ä»ARCTICå’ŒH2Oæ•°æ®é›†è¡ç”Ÿçš„åŒæ‰‹ç‰©ä½“æ“ä½œæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæœ€ä½³é…ç½®åœ¨ç”¨æˆ·èº«ä»½è¯†åˆ«æ–¹é¢å®ç°äº†97.52%çš„å¹³å‡F1åˆ†æ•°ã€‚I2Så±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å°äº4 MBçš„è½»é‡çº§æ¨¡å‹å¤§å°å’Œ0.1ç§’çš„å¿«é€Ÿæ¨ç†æ—¶é—´ã€‚è¿™äº›ç‰¹æ€§ä½¿æ‰€æå‡ºçš„æ¡†æ¶éå¸¸é€‚åˆåœ¨å®‰å…¨å…³é”®çš„åŸºäºARçš„ç³»ç»Ÿä¸­è¿›è¡Œå®æ—¶ã€è®¾å¤‡ä¸Šçš„èº«ä»½éªŒè¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¢å¼ºç°å®ï¼ˆARï¼‰ç¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯é«˜é£é™©äººæœºäº¤äº’åœºæ™¯ï¼ˆå¦‚é£æœºé©¾é©¶èˆ±ã€èˆªç©ºèˆªå¤©ç»´æŠ¤ã€å¤–ç§‘æ‰‹æœ¯ï¼‰ä¸­ï¼Œå¦‚ä½•å‡†ç¡®ã€éä¾µå…¥å¼åœ°è¿›è¡Œç”¨æˆ·èº«ä»½è¯†åˆ«çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¯èƒ½ä¾èµ–äºç¹ççš„èº«ä»½éªŒè¯æµç¨‹æˆ–ä¾µå…¥å¼çš„ç”Ÿç‰©ç‰¹å¾è¯†åˆ«ï¼Œå½±å“ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç”¨æˆ·ä¸ç‰©ä½“äº¤äº’æ—¶çš„3Dæ‰‹éƒ¨å§¿æ€ä¿¡æ¯ï¼Œé€šè¿‡åˆ†ææ‰‹éƒ¨åŠ¨ä½œã€ç‰©ä½“ç±»å‹ä»¥åŠäº¤äº’æ¨¡å¼ï¼Œæ¥æ¨æ–­ç”¨æˆ·çš„èº«ä»½ã€‚è¿™ç§æ–¹æ³•æ— éœ€ç”¨æˆ·ä¸»åŠ¨é…åˆï¼Œå…·æœ‰éä¾µå…¥æ€§å’Œè‡ªç„¶æ€§çš„ä¼˜ç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šI2Sæ¡†æ¶æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µæµç¨‹ï¼Œé¦–å…ˆä»ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­æå–3Dæ‰‹éƒ¨å§¿æ€ä¿¡æ¯ã€‚ç„¶åï¼Œè¿›è¡Œé¡ºåºç‰¹å¾å¢å¼ºï¼ŒåŒ…æ‹¬ï¼š1) ç‰©ä½“ç±»åˆ«è¯†åˆ«ï¼›2) äºº-ç‰©äº¤äº’ï¼ˆHOIï¼‰è¯†åˆ«ï¼›3) ç”¨æˆ·èº«ä»½è¯†åˆ«ã€‚æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºå¯¹3Dæ‰‹éƒ¨å§¿æ€è¿›è¡Œç‰¹å¾æå–å’Œæè¿°ï¼Œå¹¶å°†æå–çš„ç‰¹å¾ç»„ç»‡æˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç±»åˆ«ï¼Œå¦‚ç©ºé—´ã€é¢‘ç‡ã€è¿åŠ¨å­¦ã€æ–¹å‘ç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Inter-Hand Spatial Envelope (IHSE) æè¿°ç¬¦ï¼Œç”¨äºæ•æ‰åŒæ‰‹ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œç¡®å®šäº†æœ€æœ‰æ•ˆçš„ç‰¹å¾ç»„åˆï¼Œå®ç°äº†é«˜æ€§èƒ½çš„ç”¨æˆ·èº«ä»½è¯†åˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡è¯¦ç»†æè¿°äº†3Dæ‰‹éƒ¨å§¿æ€çš„ç‰¹å¾æå–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç©ºé—´ç‰¹å¾ï¼ˆå¦‚æ‰‹éƒ¨å…³é”®ç‚¹åæ ‡ï¼‰ã€é¢‘ç‡ç‰¹å¾ï¼ˆå¦‚æ‰‹éƒ¨è¿åŠ¨é€Ÿåº¦çš„å‚…é‡Œå¶å˜æ¢ï¼‰ã€è¿åŠ¨å­¦ç‰¹å¾ï¼ˆå¦‚å…³èŠ‚è§’åº¦ï¼‰ã€æ–¹å‘ç‰¹å¾ï¼ˆå¦‚æ‰‹éƒ¨æœå‘ï¼‰ä»¥åŠIHSEã€‚é€šè¿‡æ¶ˆèå®éªŒï¼Œç¡®å®šäº†æœ€ä½³çš„ç‰¹å¾ç»„åˆå’Œæƒé‡ã€‚æ¨¡å‹å¤§å°è¢«æ§åˆ¶åœ¨4MBä»¥ä¸‹ï¼Œæ¨ç†æ—¶é—´ä¸º0.1ç§’ï¼Œä¿è¯äº†å®æ—¶æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

I2Sæ¡†æ¶åœ¨åŸºäºARCTICå’ŒH2Oæ•°æ®é›†çš„åŒæ‰‹ç‰©ä½“æ“ä½œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†97.52%çš„å¹³å‡F1åˆ†æ•°ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹ä¿æŒäº†å°äº4MBçš„è½»é‡çº§å¤§å°å’Œ0.1ç§’çš„å¿«é€Ÿæ¨ç†æ—¶é—´ï¼Œä½¿å…¶éå¸¸é€‚åˆåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œå®æ—¶éƒ¨ç½²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§å¢å¼ºç°å®è¾…åŠ©ç³»ç»Ÿï¼Œä¾‹å¦‚åœ¨é£æœºé©¾é©¶èˆ±ä¸­è‡ªåŠ¨è¯†åˆ«é£è¡Œå‘˜èº«ä»½å¹¶åŠ è½½ä¸ªæ€§åŒ–è®¾ç½®ï¼Œåœ¨èˆªç©ºèˆªå¤©ç»´æŠ¤ä¸­è¾…åŠ©æŠ€æœ¯äººå‘˜è¿›è¡Œæ“ä½œæŒ‡å¯¼ï¼Œä»¥åŠåœ¨å¤–ç§‘æ‰‹æœ¯ä¸­éªŒè¯åŒ»æŠ¤äººå‘˜èº«ä»½å¹¶æä¾›å®šåˆ¶åŒ–ä¿¡æ¯ã€‚è¯¥æŠ€æœ¯æœ‰åŠ©äºæé«˜å·¥ä½œæ•ˆç‡ã€å‡å°‘äººä¸ºé”™è¯¯ï¼Œå¹¶å¢å¼ºç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-Object Interaction Recognition (HOIR) and user identification play a crucial role in advancing augmented reality (AR)-based personalized assistive technologies. These systems are increasingly being deployed in high-stakes, human-centric environments such as aircraft cockpits, aerospace maintenance, and surgical procedures. This research introduces I2S (Interact2Sign), a multi stage framework designed for unobtrusive user identification through human object interaction recognition, leveraging 3D hand pose analysis in egocentric videos. I2S utilizes handcrafted features extracted from 3D hand poses and per forms sequential feature augmentation: first identifying the object class, followed by HOI recognition, and ultimately, user identification. A comprehensive feature extraction and description process was carried out for 3D hand poses, organizing the extracted features into semantically meaningful categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive ablation studies were conducted to determine the most effective combination of features. The optimal configuration achieved an impressive average F1-score of 97.52% for user identification, evaluated on a bimanual object manipulation dataset derived from the ARCTIC and H2O datasets. I2S demonstrates state-of-the-art performance while maintaining a lightweight model size of under 4 MB and a fast inference time of 0.1 seconds. These characteristics make the proposed framework highly suitable for real-time, on-device authentication in security-critical, AR-based systems.

