---
layout: default
title: Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization
---

# Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16560v1</a>
  <a href="https://arxiv.org/pdf/2509.16560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16560v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16560v1', 'Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ji Soo Lee, Byungoh Ko, Jaewon Cho, Howoong Lee, Jaewoon Byun, Hyunwoo J. Kim

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: EMNLP 2025 Findings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mlvlab/CaReDPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCaRe-DPOæ¡†æ¶ï¼Œé€šè¿‡åŒç»„ç›´æ¥åå¥½ä¼˜åŒ–æå‡æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä¸­å­—å¹•ç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬-è§†é¢‘æ£€ç´¢` `å­—å¹•ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `ç›´æ¥åå¥½ä¼˜åŒ–` `åŒç»„ä¼˜åŒ–` `MLLM` `è§’è‰²åµŒå…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMç”Ÿæˆçš„å­—å¹•åœ¨æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä¸­å­˜åœ¨é€šç”¨æ€§é—®é¢˜ï¼Œéš¾ä»¥åŒºåˆ†è§†è§‰ç›¸ä¼¼çš„è§†é¢‘ï¼Œé™åˆ¶äº†ç»†ç²’åº¦æ£€ç´¢çš„æ€§èƒ½ã€‚
2. æå‡ºCaRe-DPOæ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯åŒç»„ç›´æ¥åå¥½ä¼˜åŒ–(DG-DPO)ï¼Œé€šè¿‡å»ºæ¨¡ä¸åŒè§†é¢‘-å­—å¹•å¯¹çš„åå¥½æ¥ä¼˜åŒ–å­—å¹•ç”Ÿæˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCaRe-DPOèƒ½æœ‰æ•ˆåˆ©ç”¨è¾…åŠ©çŸ¥è¯†ç”Ÿæˆç»†ç²’åº¦å­—å¹•ï¼Œæ˜¾è‘—æå‡æ–‡æœ¬-è§†é¢‘æ£€ç´¢çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä¸­ï¼Œè¾…åŠ©å­—å¹•å¸¸è¢«ç”¨äºå¢å¼ºè§†é¢‘ç†è§£ï¼Œå¼¥åˆæ¨¡æ€é—´çš„å·®è·ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)çš„æœ€æ–°è¿›å±•å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬å­—å¹•ç”Ÿæˆï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™äº›å­—å¹•å¾€å¾€æ˜¯é€šç”¨çš„ï¼Œå¹¶ä¸”åœ¨è§†è§‰ä¸Šç›¸ä¼¼çš„è§†é¢‘ä¸­éš¾ä»¥åŒºåˆ†ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨ç»†ç²’åº¦æ£€ç´¢ä¸­çš„æ•ˆç”¨ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„å­—å¹•ç”Ÿæˆæ–¹æ³•é€šå¸¸ä½¿ç”¨è¯­è¨€ç”ŸæˆæŒ‡æ ‡ï¼ˆå¦‚BLEUï¼‰è¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›æŒ‡æ ‡é€šå¸¸ä¸æ˜¯ä¸ºéœ€è¦åŒºåˆ†å€™é€‰è€…çš„æ£€ç´¢ä»»åŠ¡é‡èº«å®šåˆ¶çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CaRe-DPOï¼Œä¸€ä¸ªé€šè¿‡ä½¿ç”¨æ£€ç´¢ç›¸å…³æ€§åˆ†æ•°ç›´æ¥ä¼˜åŒ–å­—å¹•ç”Ÿæˆçš„æ£€ç´¢æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯åŒç»„ç›´æ¥åå¥½ä¼˜åŒ–(DG-DPO)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¯¹ä¸åŒè§†é¢‘å’Œå­—å¹•å¯¹ç»„ä¹‹é—´çš„åå¥½è¿›è¡Œå»ºæ¨¡æ¥ç›‘ç£å­—å¹•ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºMLLMçš„æ£€ç´¢æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§’è‰²åµŒå…¥ï¼Œä»¥æ›´å¥½åœ°åŒºåˆ†å…·æœ‰ä¸åŒåŠŸèƒ½è§’è‰²çš„æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚è¾…åŠ©å­—å¹•å’Œæ–‡æœ¬æŸ¥è¯¢ã€‚é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CaRe-DPOé€šè¿‡æœ‰æ•ˆåœ°åˆ©ç”¨è¾…åŠ©çŸ¥è¯†æ¥ç”Ÿæˆç”¨äºæ£€ç´¢çš„ç»†ç²’åº¦å­—å¹•ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•ç”Ÿæˆçš„è¾…åŠ©å­—å¹•æ³›åŒ–æ€§å¼ºï¼Œéš¾ä»¥åŒºåˆ†è§†è§‰ç›¸ä¼¼è§†é¢‘çš„é—®é¢˜ã€‚ä¼ ç»Ÿå­—å¹•ç”Ÿæˆæ–¹æ³•ä½¿ç”¨BLEUç­‰è¯­è¨€ç”ŸæˆæŒ‡æ ‡è¯„ä¼°ï¼Œä¸æ£€ç´¢ä»»åŠ¡çš„éœ€æ±‚ä¸åŒ¹é…ï¼Œæ— æ³•æœ‰æ•ˆæå‡æ£€ç´¢æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åˆ©ç”¨æ£€ç´¢ç›¸å…³æ€§åˆ†æ•°æ¥ä¼˜åŒ–å­—å¹•ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡å­¦ä¹ ä¸åŒè§†é¢‘-å­—å¹•å¯¹ä¹‹é—´çš„åå¥½å…³ç³»ï¼Œä½¿å¾—ç”Ÿæˆçš„å­—å¹•æ›´å…·åŒºåˆ†æ€§ï¼Œä»è€Œæå‡æ£€ç´¢æ•ˆæœã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿå­—å¹•ç”ŸæˆæŒ‡æ ‡ä¸æ£€ç´¢ä»»åŠ¡ç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCaRe-DPOæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šå­—å¹•ç”Ÿæˆå™¨å’Œæ£€ç´¢æ¨¡å‹ã€‚å­—å¹•ç”Ÿæˆå™¨åŸºäºMLLMï¼Œè´Ÿè´£ç”Ÿæˆè§†é¢‘çš„è¾…åŠ©å­—å¹•ã€‚æ£€ç´¢æ¨¡å‹åˆ™ç”¨äºè®¡ç®—æ–‡æœ¬æŸ¥è¯¢ä¸è§†é¢‘ï¼ˆåŠå…¶è¾…åŠ©å­—å¹•ï¼‰ä¹‹é—´çš„ç›¸å…³æ€§ã€‚DG-DPOä½œä¸ºæ ¸å¿ƒå­¦ä¹ ç­–ç•¥ï¼Œç”¨äºä¼˜åŒ–å­—å¹•ç”Ÿæˆå™¨ï¼Œä½¿å…¶ç”Ÿæˆçš„å­—å¹•æ›´ç¬¦åˆæ£€ç´¢ä»»åŠ¡çš„éœ€æ±‚ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œä½¿ç”¨DG-DPOä¼˜åŒ–å­—å¹•ç”Ÿæˆå™¨ï¼›ç„¶åï¼Œä½¿ç”¨ä¼˜åŒ–åçš„å­—å¹•ç”Ÿæˆå™¨ä¸ºè§†é¢‘ç”Ÿæˆè¾…åŠ©å­—å¹•ï¼›æœ€åï¼Œä½¿ç”¨æ£€ç´¢æ¨¡å‹è¿›è¡Œæ–‡æœ¬-è§†é¢‘æ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒç»„ç›´æ¥åå¥½ä¼˜åŒ–(DG-DPO)ç­–ç•¥ã€‚DG-DPOé€šè¿‡å»ºæ¨¡ä¸åŒè§†é¢‘å’Œå­—å¹•å¯¹ç»„ä¹‹é—´çš„åå¥½å…³ç³»ï¼Œç›´æ¥ä¼˜åŒ–å­—å¹•ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å…¶ç”Ÿæˆçš„å­—å¹•æ›´å…·åŒºåˆ†æ€§ï¼Œä»è€Œæå‡æ£€ç´¢æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªåŸºäºMLLMçš„æ£€ç´¢æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§’è‰²åµŒå…¥ï¼Œä»¥æ›´å¥½åœ°åŒºåˆ†å…·æœ‰ä¸åŒåŠŸèƒ½è§’è‰²çš„æ–‡æœ¬è¾“å…¥ï¼ˆä¾‹å¦‚è¾…åŠ©å­—å¹•å’Œæ–‡æœ¬æŸ¥è¯¢ï¼‰ã€‚

**å…³é”®è®¾è®¡**ï¼šDG-DPOçš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹å…¶æŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯åŸºäºä¸åŒè§†é¢‘-å­—å¹•å¯¹çš„æ£€ç´¢å¾—åˆ†å·®å¼‚ã€‚é€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„æ£€ç´¢å¾—åˆ†ï¼ŒåŒæ—¶æœ€å°åŒ–è´Ÿæ ·æœ¬å¯¹çš„æ£€ç´¢å¾—åˆ†ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·åŒºåˆ†æ€§çš„å­—å¹•ç”Ÿæˆç­–ç•¥ã€‚è§’è‰²åµŒå…¥çš„å…·ä½“å®ç°æ–¹å¼ä¹ŸæœªçŸ¥ï¼Œä½†å…¶ç›®çš„æ˜¯ä¸ºäº†åŒºåˆ†æ–‡æœ¬æŸ¥è¯¢å’Œè¾…åŠ©å­—å¹•åœ¨æ£€ç´¢æ¨¡å‹ä¸­çš„ä½œç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„CaRe-DPOæ¡†æ¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬-è§†é¢‘æ£€ç´¢çš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠå…·ä½“çš„æå‡å¹…åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaRe-DPOèƒ½å¤Ÿç”Ÿæˆæ›´å…·åŒºåˆ†æ€§çš„å­—å¹•ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨è¾…åŠ©çŸ¥è¯†è¿›è¡Œæ£€ç´¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘æœç´¢å¼•æ“ã€æ™ºèƒ½è§†é¢‘æ¨èç³»ç»Ÿã€è§†é¢‘å†…å®¹ç†è§£ç­‰é¢†åŸŸã€‚é€šè¿‡ç”Ÿæˆæ›´å…·åŒºåˆ†æ€§çš„è§†é¢‘å­—å¹•ï¼Œå¯ä»¥æå‡æ£€ç´¢å’Œæ¨èçš„å‡†ç¡®æ€§ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In text-video retrieval, auxiliary captions are often used to enhance video understanding, bridging the gap between the modalities. While recent advances in multi-modal large language models (MLLMs) have enabled strong zero-shot caption generation, we observe that such captions tend to be generic and indistinguishable across visually similar videos, limiting their utility for fine-grained retrieval. Moreover, conventional captioning approaches are typically evaluated using language generation metrics, such as BLEU, which are not typically tailored for retrieval tasks that require making discriminative distinctions between candidates. To address this, we propose $\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption generation using retrieval relevance scores. At its core is Dual-Group Direct Preference Optimization (DG-DPO), a novel learning strategy that supervises captioning by modeling preferences across groups of distinct video and caption pairs. In addition, we present an MLLM-based retrieval model that incorporates role-embeddings to better distinguish between textual inputs with different functional roles, such as an auxiliary caption and a text query. Through extensive experiments, we demonstrate that CaRe-DPO significantly enhances retrieval performance by effectively leveraging auxiliary knowledge to generate fine-grained captions for retrieval. Code is available at https://github.com/mlvlab/CaReDPO.

