---
layout: default
title: Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding
---

# Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16721" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16721v1</a>
  <a href="https://arxiv.org/pdf/2509.16721.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16721v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16721v1', 'Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyuan Li, Rui Liu, Hehe Fan, Yi Yang

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: 19 pages, 12 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Text-Sceneï¼šæå‡ºä¸€ç§åœºæ™¯åˆ°è¯­è¨€çš„è§£ææ¡†æ¶ï¼Œç”¨äº3Dåœºæ™¯ç†è§£ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `åœºæ™¯åˆ°è¯­è¨€è§£æ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å‡ ä½•åˆ†æ` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆç†è§£3Dåœºæ™¯ï¼Œå› ä¸º3Dç¯å¢ƒåŒ…å«ä¸°å¯Œçš„ç©ºé—´å…³ç³»ã€åŠŸèƒ½ã€ç‰©ç†å±æ€§å’Œå¸ƒå±€ç­‰å¤æ‚æ¦‚å¿µã€‚
2. Text-Sceneæ¡†æ¶é€šè¿‡å‡ ä½•åˆ†æå’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨å°†3Dåœºæ™¯è§£æä¸ºæ–‡æœ¬æè¿°ï¼Œå¼¥åˆ3Dè§‚å¯Ÿå’Œè¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒText-Sceneç”Ÿæˆçš„æ–‡æœ¬è§£æèƒ½å¤Ÿå‡†ç¡®è¡¨ç¤º3Dåœºæ™¯ï¼Œå¹¶èƒ½æœ‰æ•ˆæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºText-Sceneçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªåŠ¨å°†3Dåœºæ™¯è§£æä¸ºæ–‡æœ¬æè¿°ï¼Œä»¥å®ç°åœºæ™¯ç†è§£ã€‚é’ˆå¯¹å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­æ™ºèƒ½ä½“ç†è§£å’Œäº¤äº’å¤æ‚3Dåœºæ™¯çš„æŒ‘æˆ˜ï¼Œä»¥åŠç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼ˆåŒ…æ‹¬3Dç¯å¢ƒæ¶‰åŠæ›´ä¸°å¯Œçš„æ¦‚å¿µä»¥åŠç¼ºä¹å¤§è§„æ¨¡3Dè§†è§‰-è¯­è¨€æ•°æ®é›†ï¼‰ï¼ŒText-Sceneæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¯¹è±¡å±æ€§å’Œç©ºé—´å…³ç³»ï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„åœºæ™¯æ€»ç»“ï¼Œä»è€Œå¼¥åˆäº†3Dè§‚å¯Ÿå’Œè¯­è¨€ä¹‹é—´çš„å·®è·ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚é€šè¿‡åˆ©ç”¨å‡ ä½•åˆ†æå’ŒMLLMï¼ŒText-Sceneç”Ÿæˆå‡†ç¡®ã€è¯¦ç»†ä¸”æ˜“äºç†è§£çš„æè¿°ï¼Œæ•æ‰å¯¹è±¡çº§åˆ«çš„ç»†èŠ‚å’Œå…¨å±€çº§åˆ«çš„ä¸Šä¸‹æ–‡ã€‚åŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬è§£æèƒ½å¤Ÿå¿ å®åœ°è¡¨ç¤º3Dåœºæ™¯å¹¶æœ‰ç›Šäºä¸‹æ¸¸ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†InPlan3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„3Dä»»åŠ¡è§„åˆ’åŸºå‡†ï¼ŒåŒ…å«è·¨636ä¸ªå®¤å†…åœºæ™¯çš„3174ä¸ªé•¿æœŸè§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒæ¸…æ™°æ€§å’Œå¯è®¿é—®æ€§ï¼Œæ—¨åœ¨é€šè¿‡è¯­è¨€ä½¿3Dåœºæ™¯å†…å®¹æ˜“äºç†è§£ã€‚ä»£ç å’Œæ•°æ®é›†å°†ä¼šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸º3Dåœºæ™¯åŒ…å«æ¯”2Då›¾åƒæ›´ä¸°å¯Œçš„æ¦‚å¿µï¼Œä¾‹å¦‚ç©ºé—´å…³ç³»ã€å¯ä¾›æ€§ã€ç‰©ç†å±æ€§å’Œå¸ƒå±€ç­‰ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤§è§„æ¨¡çš„3Dè§†è§‰-è¯­è¨€æ•°æ®é›†ä¹Ÿé™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°å°†3Dåœºæ™¯ä¿¡æ¯è½¬åŒ–ä¸ºå¯ç†è§£çš„è¯­è¨€æè¿°ï¼Œä»è€Œé˜»ç¢äº†æ™ºèƒ½ä½“ä¸3Dç¯å¢ƒçš„äº¤äº’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šText-Sceneçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†3Dåœºæ™¯è§£æä¸ºæ–‡æœ¬æè¿°ï¼Œä»è€Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€ç†è§£æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå®ç°å¯¹3Dåœºæ™¯çš„æœ‰æ•ˆç†è§£ã€‚é€šè¿‡ç»“åˆå‡ ä½•åˆ†æå’ŒMLLMï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å¯¹è±¡å±æ€§å’Œç©ºé—´å…³ç³»ï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„åœºæ™¯æ€»ç»“ï¼Œä»è€Œå¼¥åˆäº†3Dè§‚å¯Ÿå’Œè¯­è¨€ä¹‹é—´çš„å·®è·ã€‚è¿™ç§æ–¹æ³•æ— éœ€äººå·¥å¹²é¢„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå‡†ç¡®ã€è¯¦ç»†ä¸”æ˜“äºç†è§£çš„æè¿°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šText-Sceneæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) 3Dåœºæ™¯è¾“å…¥æ¨¡å—ï¼šæ¥æ”¶3Dåœºæ™¯æ•°æ®ä½œä¸ºè¾“å…¥ã€‚2) å¯¹è±¡å±æ€§å’Œç©ºé—´å…³ç³»è¯†åˆ«æ¨¡å—ï¼šåˆ©ç”¨å‡ ä½•åˆ†ææ–¹æ³•è¯†åˆ«åœºæ™¯ä¸­çš„å¯¹è±¡åŠå…¶å±æ€§ï¼Œä»¥åŠå¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚3) æ–‡æœ¬æè¿°ç”Ÿæˆæ¨¡å—ï¼šåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®è¯†åˆ«å‡ºçš„å¯¹è±¡å±æ€§å’Œç©ºé—´å…³ç³»ï¼Œç”Ÿæˆè¿è´¯çš„åœºæ™¯æ–‡æœ¬æè¿°ã€‚4) è¯„ä¼°æ¨¡å—ï¼šä½¿ç”¨InPlan3DåŸºå‡†æµ‹è¯•è¯„ä¼°ç”Ÿæˆçš„æ–‡æœ¬æè¿°çš„è´¨é‡å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šText-Sceneçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„3Dåœºæ™¯åˆ°æ–‡æœ¬çš„è§£ææ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†3Dåœºæ™¯ä¿¡æ¯è½¬åŒ–ä¸ºå¯ç†è§£çš„è¯­è¨€æè¿°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒText-Sceneæ— éœ€äººå·¥å¹²é¢„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå‡†ç¡®ã€è¯¦ç»†ä¸”æ˜“äºç†è§£çš„æè¿°ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ•æ‰å¯¹è±¡çº§åˆ«çš„ç»†èŠ‚å’Œå…¨å±€çº§åˆ«çš„ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼ŒInPlan3DåŸºå‡†æµ‹è¯•çš„æå‡ºä¹Ÿä¸ºè¯„ä¼°3Dåœºæ™¯ç†è§£æ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šText-Sceneæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å‡ ä½•åˆ†ææ–¹æ³•çš„é€‰æ‹©ï¼šé€‰æ‹©åˆé€‚çš„å‡ ä½•åˆ†ææ–¹æ³•æ¥æœ‰æ•ˆåœ°è¯†åˆ«å¯¹è±¡å±æ€§å’Œç©ºé—´å…³ç³»ã€‚2) å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€‰æ‹©å’Œè®­ç»ƒï¼šé€‰æ‹©åˆé€‚çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨3Dè§†è§‰-è¯­è¨€æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ–‡æœ¬æè¿°ç”Ÿæˆçš„è´¨é‡ã€‚3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šè®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ–‡æœ¬æè¿°ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒText-Sceneæ¡†æ¶ç”Ÿæˆçš„æ–‡æœ¬è§£æèƒ½å¤Ÿå¿ å®åœ°è¡¨ç¤º3Dåœºæ™¯ï¼Œå¹¶èƒ½æœ‰æ•ˆæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒInPlan3DåŸºå‡†æµ‹è¯•çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒText-Sceneæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨3Dä»»åŠ¡è§„åˆ’æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†ç»™å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Text-Sceneæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼š1) æœºå™¨äººå¯¼èˆªï¼šæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ¡†æ¶ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œå¯¼èˆªå’Œäº¤äº’ã€‚2) è™šæ‹Ÿç°å®ï¼šè¯¥æ¡†æ¶å¯ä»¥ç”¨äºç”Ÿæˆè™šæ‹Ÿåœºæ™¯çš„æ–‡æœ¬æè¿°ï¼Œä»è€Œæé«˜ç”¨æˆ·ä½“éªŒã€‚3) æ™ºèƒ½å®¶å±…ï¼šæ™ºèƒ½å®¶å±…ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥æ¡†æ¶ç†è§£å®¶åº­ç¯å¢ƒï¼Œä»è€Œæä¾›æ›´æ™ºèƒ½åŒ–çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å…·èº«æ™ºèƒ½å’Œäººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enabling agents to understand and interact with complex 3D scenes is a fundamental challenge for embodied artificial intelligence systems. While Multimodal Large Language Models (MLLMs) have achieved significant progress in 2D image understanding, extending such capabilities to 3D scenes remains difficult: 1) 3D environment involves richer concepts such as spatial relationships, affordances, physics, layout, and so on, 2) the absence of large-scale 3D vision-language datasets has posed a significant obstacle. In this paper, we introduce Text-Scene, a framework that automatically parses 3D scenes into textual descriptions for scene understanding. Given a 3D scene, our model identifies object attributes and spatial relationships, and then generates a coherent summary of the whole scene, bridging the gap between 3D observation and language without requiring human-in-the-loop intervention. By leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions that are accurate, detailed, and human-interpretable, capturing object-level details and global-level context. Experimental results on benchmarks demonstrate that our textual parses can faithfully represent 3D scenes and benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of 3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity and accessibility in our approach, aiming to make 3D scene content understandable through language. Code and datasets will be released.

