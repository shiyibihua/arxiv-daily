---
layout: default
title: Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment
---

# Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22697" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22697v1</a>
  <a href="https://arxiv.org/pdf/2509.22697.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22697v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22697v1', 'Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abhiroop Chatterjee, Susmita Ghosh

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: Accepted at the IEEE/CVF International Conference on Computer Vision (ICCV 2025), Workshop on Curated Data for Efficient Learning

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨æ–‡æœ¬æç¤ºå­¦ä¹ é«˜å…‰è°±å›¾åƒï¼Œå®ç°é«˜æ•ˆå¤šæ¨¡æ€å¯¹é½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é«˜å…‰è°±å›¾åƒ` `å¤šæ¨¡æ€å¯¹é½` `å¯¹æ¯”å­¦ä¹ ` `æ–‡æœ¬æç¤º` `è§†è§‰-è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é«˜å…‰è°±å›¾åƒå¤„ç†é¢ä¸´é«˜ç»´æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é«˜å…‰è°±é¢†åŸŸçš„è·¨æ¨¡æ€å¯¹é½æ•ˆæœä¸ä½³ã€‚
2. æå‡ºä¸€ç§åŸºäºCLIPé£æ ¼å¯¹æ¯”å­¦ä¹ çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºä½œä¸ºé”šç‚¹ï¼Œä¼˜åŒ–è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…éœ€æ›´æ–°å°‘é‡å‚æ•°å³å¯è¾¾åˆ°SOTAæ€§èƒ½ï¼Œå¹¶åœ¨Indian Pineså’ŒPavia Universityæ•°æ®é›†ä¸Šå–å¾—æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ•°æ®éœ€æ±‚çš„æŒç»­å¢é•¿ï¼Œé«˜æ•ˆå­¦ä¹ è¶Šæ¥è¶Šä¾èµ–äºé«˜è´¨é‡æ•°æ®çš„ç®¡ç†å’Œæç‚¼ï¼Œè€Œä¸æ˜¯ç²—æš´åœ°æ‰©å¤§æ¨¡å‹è§„æ¨¡ã€‚å¯¹äºé«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰è€Œè¨€ï¼Œç”±äºå…¶é«˜ç»´3Dä½“ç´ ç»“æ„ï¼ˆæ¯ä¸ªç©ºé—´ä½ç½®ä¸æ•°ç™¾ä¸ªè¿ç»­å…‰è°±é€šé“ç›¸å…³è”ï¼‰ï¼ŒæŒ‘æˆ˜æ›´åŠ ä¸¥å³»ã€‚è™½ç„¶è§†è§‰å’Œè¯­è¨€æ¨¡å‹å·²é’ˆå¯¹è‡ªç„¶å›¾åƒæˆ–æ–‡æœ¬ä»»åŠ¡è¿›è¡Œäº†æœ‰æ•ˆä¼˜åŒ–ï¼Œä½†å®ƒä»¬åœ¨é«˜å…‰è°±é¢†åŸŸä¸­çš„è·¨æ¨¡æ€å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚æœ¬æ–‡å°è¯•é€šè¿‡åˆ©ç”¨CLIPé£æ ¼çš„å¯¹æ¯”è®­ç»ƒæ¡†æ¶æ¥ä¼˜åŒ–ç”¨äºé«˜å…‰è°±åœºæ™¯ç†è§£çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚è¯¥æ¡†æ¶å°†æ¥è‡ªè§†è§‰ä¸»å¹²ç½‘ç»œçš„ä½“ç´ çº§åµŒå…¥æ˜ å°„åˆ°å†»ç»“çš„å¤§å‹åµŒå…¥æ¨¡å‹ï¼ˆLEMï¼‰çš„æ½œåœ¨ç©ºé—´ï¼Œå…¶ä¸­å¯è®­ç»ƒçš„æ¢é’ˆå°†è§†è§‰ç‰¹å¾ä¸æ¨¡å‹çš„æ–‡æœ¬tokenè¡¨ç¤ºå¯¹é½ã€‚ä¸¤ç§æ¨¡æ€é€šè¿‡å¯¹æ¯”æŸå¤±å¯¹é½ï¼Œè¯¥å¯¹æ¯”æŸå¤±ä»…é™äºç²¾å¿ƒæŒ‘é€‰çš„éš¾è´Ÿæ ·æœ¬ï¼ˆæœ€æ¥è¿‘çš„é”™è¯¯ç±»åˆ«ï¼‰å’ŒåŠéš¾è´Ÿæ ·æœ¬ï¼ˆéšæœºå¹²æ‰°é¡¹ï¼‰ä»¥åŠæ­£æ ·æœ¬å¯¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¯¹é½ï¼Œå¼•å…¥äº†ç¼–ç ç±»åˆ«è¯­ä¹‰çš„æè¿°æ€§æç¤ºï¼Œå¹¶ä½œä¸ºHSIåµŒå…¥çš„ç»“æ„åŒ–é”šç‚¹ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…æ›´æ–°æ€»å‚æ•°çš„0.07%ï¼Œä½†äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨Indian Pinesï¼ˆIPï¼‰æ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹æ¯”å•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºçº¿æé«˜äº†+0.92çš„æ€»ä½“ç²¾åº¦ï¼ˆOAï¼‰å’Œ+1.60çš„Kappaï¼ˆÎºï¼‰ï¼Œè€Œåœ¨Pavia Universityï¼ˆPUï¼‰æ•°æ®é›†ä¸Šï¼Œå®ƒæä¾›äº†+0.69çš„OAå’Œ+0.90çš„Îºå¢ç›Šã€‚æ­¤å¤–ï¼Œè¿™æ˜¯åœ¨å‚æ•°é›†æ¯”DCTNå°è¿‘50å€ï¼Œæ¯”SS-TMNetå°90å€çš„æƒ…å†µä¸‹å®ç°çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é«˜å…‰è°±å›¾åƒåœºæ™¯ç†è§£ä¸­è§†è§‰-è¯­è¨€æ¨¡å‹è·¨æ¨¡æ€å¯¹é½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„å‚æ•°å’Œè®¡ç®—èµ„æºï¼Œä¸”åœ¨é«˜å…‰è°±å›¾åƒè¿™ç§é«˜ç»´æ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ç—›ç‚¹åœ¨äºå¦‚ä½•é«˜æ•ˆåœ°å°†é«˜å…‰è°±å›¾åƒçš„è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æè¿°å¯¹é½ï¼Œå¹¶åˆ©ç”¨æœ‰é™çš„è®¡ç®—èµ„æºè¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå°†é«˜å…‰è°±å›¾åƒçš„è§†è§‰ç‰¹å¾æ˜ å°„åˆ°é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡æ–‡æœ¬æç¤ºä½œä¸ºé”šç‚¹æ¥å¼•å¯¼è§†è§‰ç‰¹å¾çš„å­¦ä¹ ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ä¸»å¹²ç½‘ç»œï¼šç”¨äºæå–é«˜å…‰è°±å›¾åƒçš„ä½“ç´ çº§åµŒå…¥ã€‚2) å¤§å‹åµŒå…¥æ¨¡å‹ï¼ˆLEMï¼‰ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„ã€å‚æ•°å†»ç»“çš„è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚3) å¯è®­ç»ƒæ¢é’ˆï¼šç”¨äºå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°LEMçš„æ½œåœ¨ç©ºé—´ã€‚4) å¯¹æ¯”æŸå¤±å‡½æ•°ï¼šç”¨äºå¯¹é½è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ã€‚5) æ–‡æœ¬æç¤ºï¼šç”¨äºç¼–ç ç±»åˆ«è¯­ä¹‰ï¼Œä½œä¸ºç»“æ„åŒ–é”šç‚¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨æ–‡æœ¬æç¤ºä½œä¸ºé”šç‚¹æ¥å¼•å¯¼è§†è§‰ç‰¹å¾çš„å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä»…éœ€æ›´æ–°å°‘é‡å‚æ•°ï¼Œå³å¯è¾¾åˆ°SOTAæ€§èƒ½ï¼Œå…·æœ‰å¾ˆé«˜çš„è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CLIPé£æ ¼çš„å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå¹¶é€‰æ‹©éš¾è´Ÿæ ·æœ¬å’ŒåŠéš¾è´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚2) è®¾è®¡æè¿°æ€§çš„æ–‡æœ¬æç¤ºï¼Œç”¨äºç¼–ç ç±»åˆ«è¯­ä¹‰ã€‚3) ä½¿ç”¨å¯è®­ç»ƒçš„æ¢é’ˆå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°LEMçš„æ½œåœ¨ç©ºé—´ã€‚4) å†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ï¼Œåªæ›´æ–°å°‘é‡å‚æ•°ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Indian Pinesæ•°æ®é›†ä¸Šï¼Œæ€»ä½“ç²¾åº¦ï¼ˆOAï¼‰æå‡äº†+0.92ï¼ŒKappaç³»æ•°æå‡äº†+1.60ï¼›åœ¨Pavia Universityæ•°æ®é›†ä¸Šï¼ŒOAæå‡äº†+0.69ï¼ŒKappaç³»æ•°æå‡äº†+0.90ã€‚è¯¥æ–¹æ³•ä»…æ›´æ–°æ€»å‚æ•°çš„0.07%ï¼Œå‚æ•°é‡è¿œå°äºDCTNå’ŒSS-TMNetï¼Œä½†æ€§èƒ½ä¼˜äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé¥æ„Ÿå›¾åƒåˆ†æã€ç²¾å‡†å†œä¸šã€ç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸã€‚é€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€å¯¹é½ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£é«˜å…‰è°±å›¾åƒä¸­çš„åœºæ™¯ä¿¡æ¯ï¼Œä»è€Œä¸ºç›¸å…³é¢†åŸŸçš„å†³ç­–æä¾›æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°å…¶ä»–é«˜ç»´æ•°æ®å’Œè·¨æ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As data requirements continue to grow, efficient learning increasingly depends on the curation and distillation of high-value data rather than brute-force scaling of model sizes. In the case of a hyperspectral image (HSI), the challenge is amplified by the high-dimensional 3D voxel structure, where each spatial location is associated with hundreds of contiguous spectral channels. While vision and language models have been optimized effectively for natural image or text tasks, their cross-modal alignment in the hyperspectral domain remains an open and underexplored problem. In this article, we make an attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene understanding by exploiting a CLIP-style contrastive training framework. Our framework maps voxel-level embeddings from a vision backbone onto the latent space of a frozen large embedding model (LEM), where a trainable probe aligns vision features with the model's textual token representations. The two modalities are aligned via a contrastive loss restricted to a curated set of hard (closest wrong classes) and semi-hard (random distractors) negatives, along with positive pairs. To further enhance alignment, descriptive prompts that encode class semantics are introduced and act as structured anchors for the HSI embeddings. It is seen that the proposed method updates only 0.07 percent of the total parameters, yet yields state-of-the-art performance. For example, on Indian Pines (IP) the model produces better results over unimodal and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa ($Îº$), while on Pavia University (PU) data it provides gains of +0.69 OA and +0.90 $Îº$. Moreover, this is achieved with the set of parameters, nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet.

