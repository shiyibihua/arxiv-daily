---
layout: default
title: EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything
---

# EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00863" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00863v1</a>
  <a href="https://arxiv.org/pdf/2312.00863.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00863v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00863v1', 'EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEfficientSAMä»¥è§£å†³SAMæ¨¡å‹è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è½»é‡çº§æ¨¡å‹` `è§†è§‰è¡¨ç¤ºå­¦ä¹ ` `å®ä¾‹åˆ†å‰²` `æ©ç å›¾åƒé¢„è®­ç»ƒ` `è®¡ç®—æ•ˆç‡` `Transformeræ¨¡å‹` `SA-1Bæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„SAMæ¨¡å‹ç”±äºè®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›ä½¿ç”¨ã€‚
2. æœ¬æ–‡æå‡ºEfficientSAMsï¼Œé€šè¿‡masked image pretrainingï¼ˆSAMIï¼‰æ¥å­¦ä¹ æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºï¼Œæ„å»ºè½»é‡çº§çš„SAMæ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficientSAMsåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶-shotå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰çº¦4 APçš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Segment Anything Model (SAM)ä½œä¸ºä¸€ç§å¼ºå¤§çš„è§†è§‰åº”ç”¨å·¥å…·ï¼Œå…¶å“è¶Šçš„é›¶-shotè¿ç§»èƒ½åŠ›å’Œé«˜é€‚åº”æ€§å¾—ç›Šäºåœ¨å¤§è§„æ¨¡é«˜è´¨é‡SA-1Bæ•°æ®é›†ä¸Šè®­ç»ƒçš„è¶…å¤§Transformeræ¨¡å‹ã€‚ç„¶è€Œï¼ŒSAMæ¨¡å‹çš„å·¨å¤§è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›å®é™…åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†EfficientSAMsï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„SAMæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤§å¹…é™ä½å¤æ‚åº¦çš„åŒæ—¶ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºmasked image pretrainingï¼Œå³SAMIï¼Œæ—¨åœ¨é€šè¿‡é‡å»ºSAMå›¾åƒç¼–ç å™¨çš„ç‰¹å¾æ¥æœ‰æ•ˆå­¦ä¹ è§†è§‰è¡¨ç¤ºã€‚ç»è¿‡å¤šé¡¹è§†è§‰ä»»åŠ¡çš„è¯„ä¼°ï¼ŒEfficientSAMsåœ¨é›¶-shotå®ä¾‹åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–å¿«é€ŸSAMæ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Segment Anything Model (SAM)åœ¨å®é™…åº”ç”¨ä¸­ç”±äºè®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„SAMæ¨¡å‹è™½ç„¶æ€§èƒ½å“è¶Šï¼Œä½†å…¶åºå¤§çš„è®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„EfficientSAMsé€šè¿‡å¼•å…¥masked image pretrainingï¼ˆSAMIï¼‰ï¼Œæœ‰æ•ˆåœ°å­¦ä¹ è§†è§‰è¡¨ç¤ºï¼Œä»è€Œæ„å»ºè½»é‡çº§çš„SAMæ¨¡å‹ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬SAMIé¢„è®­ç»ƒçš„è½»é‡çº§å›¾åƒç¼–ç å™¨å’Œæ©ç è§£ç å™¨ï¼Œé¦–å…ˆé€šè¿‡SAMIè¿›è¡Œç‰¹å¾é‡å»ºï¼Œç„¶ååœ¨SA-1Bæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”â€œsegment anythingâ€ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†SAMIä½œä¸ºé¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿å¾—è½»é‡çº§æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–æ©ç å›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶-shotä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è½»é‡çº§ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡éœ€æ±‚ï¼ŒåŒæ—¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨é‡ç‰¹å¾é‡å»ºçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒEfficientSAMsåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEfficientSAMsåœ¨é›¶-shotå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šç›¸è¾ƒäºå…¶ä»–å¿«é€ŸSAMæ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨COCO/LVISæ•°æ®é›†ä¸Šçº¦æå‡4 APã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒSAMIé¢„è®­ç»ƒæ–¹æ³•åœ¨å¤šé¡¹è§†è§‰ä»»åŠ¡ä¸­å‡å…·æœ‰ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€åŒ»ç–—å½±åƒåˆ†æç­‰å¤šä¸ªè§†è§‰ä»»åŠ¡ã€‚EfficientSAMsçš„è½»é‡çº§ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œèƒ½å¤Ÿæ¨åŠ¨æ›´å¤šè§†è§‰åº”ç”¨çš„è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.

