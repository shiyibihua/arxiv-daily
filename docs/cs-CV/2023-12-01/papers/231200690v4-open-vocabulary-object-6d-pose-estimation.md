---
layout: default
title: Open-vocabulary object 6D pose estimation
---

# Open-vocabulary object 6D pose estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00690" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00690v4</a>
  <a href="https://arxiv.org/pdf/2312.00690.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00690v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00690v4', 'Open-vocabulary object 6D pose estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, Fabio Poiesi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-06-25)

**å¤‡æ³¨**: Camera ready version (CVPR 2024, poster highlight). New Oryon version: arXiv:2406.16384

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://jcorsetti.github.io/oryon)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼€æ”¾è¯æ±‡å¯¹è±¡6Då§¿æ€ä¼°è®¡ä»¥è§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡` `å¯¹è±¡å§¿æ€ä¼°è®¡` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `å›¾åƒåˆ†å‰²` `æœºå™¨äººè§†è§‰` `å¢å¼ºç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„6Då§¿æ€ä¼°è®¡æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯¹è±¡æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡æ–‡æœ¬æç¤ºæ¥åˆ†å‰²å¯¹è±¡å¹¶ä¼°è®¡å…¶å§¿æ€ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨REAL275å’ŒToyota-Lightæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ‰‹å·¥æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬å¼•å…¥äº†å¼€æ”¾è¯æ±‡å¯¹è±¡6Då§¿æ€ä¼°è®¡çš„æ–°è®¾ç½®ï¼Œå…¶ä¸­ä½¿ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å®šæ„Ÿå…´è¶£çš„å¯¹è±¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œ(i) æ„Ÿå…´è¶£çš„å¯¹è±¡ä»…é€šè¿‡æ–‡æœ¬æç¤ºæŒ‡å®šï¼Œ(ii) æ¨ç†æ—¶ä¸éœ€è¦å¯¹è±¡æ¨¡å‹ï¼ˆå¦‚CADæˆ–è§†é¢‘åºåˆ—ï¼‰ï¼Œ(iii) å¯¹è±¡ä»ä¸åŒåœºæ™¯çš„ä¸¤ä¸ªRGBDè§†è§’è¿›è¡Œæˆåƒã€‚ä¸ºåœ¨æ­¤è®¾ç½®ä¸­æ“ä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ä»åœºæ™¯ä¸­åˆ†å‰²æ„Ÿå…´è¶£çš„å¯¹è±¡å¹¶ä¼°è®¡å…¶ç›¸å¯¹6Då§¿æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºç²¾å¿ƒè®¾è®¡çš„ç­–ç•¥ï¼Œå°†æç¤ºæä¾›çš„å¯¹è±¡çº§ä¿¡æ¯ä¸å±€éƒ¨å›¾åƒç‰¹å¾èåˆï¼Œä»è€Œå½¢æˆä¸€ä¸ªèƒ½å¤Ÿæ¨å¹¿åˆ°æ–°æ¦‚å¿µçš„ç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬åœ¨åŸºäºä¸¤ä¸ªæµè¡Œæ•°æ®é›†REAL275å’ŒToyota-Lightçš„æ–°åŸºå‡†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸­ä¼°è®¡å¯¹è±¡çš„ç›¸å¯¹6Då§¿æ€æ–¹é¢ä¼˜äºæˆç†Ÿçš„æ‰‹å·¥æ–¹æ³•å’Œæœ€è¿‘çš„æ·±åº¦å­¦ä¹ åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡å¯¹è±¡6Då§¿æ€ä¼°è®¡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…·ä½“çš„å¯¹è±¡æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ–‡æœ¬æç¤ºæ¥æŒ‡å®šå¯¹è±¡ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è±¡åˆ†å‰²å’Œå§¿æ€ä¼°è®¡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¯¹è±¡æ¨¡å‹çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æœ¬æç¤ºè¾“å…¥ã€è§†è§‰-è¯­è¨€æ¨¡å‹å¤„ç†ã€å¯¹è±¡åˆ†å‰²æ¨¡å—å’Œå§¿æ€ä¼°è®¡æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„å¤„ç†æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ–‡æœ¬æç¤ºä¸å±€éƒ¨å›¾åƒç‰¹å¾æœ‰æ•ˆèåˆï¼Œå½¢æˆä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ–°æ¦‚å¿µçš„ç‰¹å¾ç©ºé—´ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ä¾èµ–å¯¹è±¡æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å‰²å’Œå§¿æ€ä¼°è®¡çš„ç²¾åº¦ï¼ŒåŒæ—¶é‡‡ç”¨äº†é€‚åº”æ€§ç½‘ç»œç»“æ„ä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨REAL275å’ŒToyota-Lightæ•°æ®é›†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ‰‹å·¥æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œå§¿æ€ä¼°è®¡ç²¾åº¦æå‡æ˜¾è‘—ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†æ•´ä½“è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å…·ä½“å¯¹è±¡æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹æ–°å¯¹è±¡çš„å¿«é€Ÿè¯†åˆ«å’Œå§¿æ€ä¼°è®¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce the new setting of open-vocabulary object 6D pose estimation, in which a textual prompt is used to specify the object of interest. In contrast to existing approaches, in our setting (i) the object of interest is specified solely through the textual prompt, (ii) no object model (e.g., CAD or video sequence) is required at inference, and (iii) the object is imaged from two RGBD viewpoints of different scenes. To operate in this setting, we introduce a novel approach that leverages a Vision-Language Model to segment the object of interest from the scenes and to estimate its relative 6D pose. The key of our approach is a carefully devised strategy to fuse object-level information provided by the prompt with local image features, resulting in a feature space that can generalize to novel concepts. We validate our approach on a new benchmark based on two popular datasets, REAL275 and Toyota-Light, which collectively encompass 34 object instances appearing in four thousand image pairs. The results demonstrate that our approach outperforms both a well-established hand-crafted method and a recent deep learning-based baseline in estimating the relative 6D pose of objects in different scenes. Code and dataset are available at https://jcorsetti.github.io/oryon.

