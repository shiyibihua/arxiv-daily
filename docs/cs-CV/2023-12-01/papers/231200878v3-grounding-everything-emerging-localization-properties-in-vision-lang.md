---
layout: default
title: Grounding Everything: Emerging Localization Properties in Vision-Language Transformers
---

# Grounding Everything: Emerging Localization Properties in Vision-Language Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00878" class="toolbar-btn" target="_blank">üìÑ arXiv: 2312.00878v3</a>
  <a href="https://arxiv.org/pdf/2312.00878.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00878v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00878v3', 'Grounding Everything: Emerging Localization Properties in Vision-Language Transformers')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2023-12-01 (Êõ¥Êñ∞: 2023-12-14)

**Â§áÊ≥®**: Code available at https://github.com/WalBouss/GEM

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫GEMÊ®°Âùó‰ª•ÂÆûÁé∞Èõ∂-shotÂºÄÊîæËØçÊ±áÁâ©‰ΩìÂÆö‰Ωç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Èõ∂-shotÂ≠¶‰π†` `Áâ©‰ΩìÂÆö‰Ωç` `Ëá™ÊàëÊ≥®ÊÑèÂäõ` `ÂºÄÊîæËØçÊ±á` `ËÅöÁ±ª` `Ê≠£ÂàôÂåñ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Èõ∂-shotÁâ©‰ΩìÂÆö‰Ωç‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈÄöÂ∏∏ÈúÄË¶ÅËøõË°åÂæÆË∞É‰ª•ÈÄÇÂ∫îÁâπÂÆö‰ªªÂä°„ÄÇ
2. ÊèêÂá∫ÁöÑGEMÊ®°ÂùóÈÄöËøáËá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõÊú∫Âà∂ÂÆûÁé∞ÂºÄÊîæËØçÊ±áÁâ©‰ΩìÂÆö‰ΩçÔºåÈÅøÂÖç‰∫ÜÂæÆË∞ÉÁöÑÈúÄÊ±Ç„ÄÇ
3. GEMÂú®Â§ö‰∏™Âü∫ÂáÜ‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÊó†ËÆ≠ÁªÉÊñπÊ≥ïÔºåÂπ∂Âú®OpenImagesV7‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°ÂûãÂú®ÂõæÂÉèÊ£ÄÁ¥¢„ÄÅÂàÜÁ±ªÂíåÊèèËø∞Á≠âÈõ∂-shot‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÂõæÂÉè‰∏≠ÂÆö‰ΩçÊåáÁß∞Ë°®ËææÂíåÁâ©‰ΩìÊñπÈù¢‰ªçÊòæ‰∏çË∂≥„ÄÇÊú¨ÊñáÂ±ïÁ§∫‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÂú®Êó†ÈúÄÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞Èõ∂-shotÂºÄÊîæËØçÊ±áÁâ©‰ΩìÂÆö‰Ωç„ÄÇ‰∏∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫Grounding Everything ModuleÔºàGEMÔºâÁöÑÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂ∞ÜCLIPSurgery‰∏≠ÊèêÂá∫ÁöÑÂÄº-ÂÄºÊ≥®ÊÑèÂäõÁöÑÊÄùÊÉ≥Êé®ÂπøÂà∞Ëá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõË∑ØÂæÑ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõÁöÑÊ¶ÇÂøµ‰∏éËÅöÁ±ªÁõ∏ÂØπÂ∫îÔºå‰ªéËÄå‰ΩøÊù•Ëá™Âêå‰∏ÄÁâ©‰ΩìÁöÑ‰ª§ÁâåÁªÑ‰øùÊåÅÁõ∏‰ººÔºåÂêåÊó∂‰øùÊåÅ‰∏éËØ≠Ë®ÄÁ©∫Èó¥ÁöÑÂØπÈΩê„ÄÇÈÄöËøá‰∏ÄÁ≥ªÂàóÊ≠£ÂàôÂåñÊñπÊ≥ïÔºåÊ®°ÂûãËÉΩÂ§üÂú®‰∏çÂêåÊï∞ÊçÆÈõÜÂíåÈ™®Âπ≤ÁΩëÁªú‰πãÈó¥ËøõË°åÊ≥õÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGEMÂú®Â§ö‰∏™Âü∫ÂáÜ‰ªªÂä°ÂíåÊï∞ÊçÆÈõÜ‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÊó†ËÆ≠ÁªÉÁöÑÂºÄÊîæËØçÊ±áÂÆö‰ΩçÊñπÊ≥ïÔºåÂπ∂Âú®OpenImagesV7Â§ßËßÑÊ®°ÂàÜÂâ≤Âü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Èõ∂-shotÁâ©‰ΩìÂÆö‰Ωç‰∏≠ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÊåáÁß∞Ë°®ËææÂíåÁâ©‰ΩìÊó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åÂæÆË∞ÉÔºåÈôêÂà∂‰∫ÜÂÖ∂ÁÅµÊ¥ªÊÄßÂíåÈÄÇÁî®ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑGEMÊ®°ÂùóÈÄöËøáËá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®Êó†ÈúÄÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ÂºÄÊîæËØçÊ±áÁâ©‰ΩìÂÆö‰Ωç„ÄÇËØ•ËÆæËÆ°Êó®Âú®ÈÄöËøáËÅöÁ±ª‰ª§ÁâåÊù•Â¢ûÂº∫Êù•Ëá™Âêå‰∏ÄÁâ©‰ΩìÁöÑ‰ª§Áâå‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∏éËØ≠Ë®ÄÁ©∫Èó¥ÁöÑÂØπÈΩê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGEMÊ®°ÂùóÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Ëá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõÊú∫Âà∂Âíå‰∏ÄÁ≥ªÂàóÊ≠£ÂàôÂåñÊñπÊ≥ï„ÄÇËá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõÊú∫Âà∂Áî®‰∫éËÅöÁ±ªÂíåÂØπÈΩêÔºåËÄåÊ≠£ÂàôÂåñÊñπÊ≥ïÂàôÂ∏ÆÂä©Ê®°ÂûãÂú®‰∏çÂêåÊï∞ÊçÆÈõÜÂíåÈ™®Âπ≤ÁΩëÁªú‰πãÈó¥ËøõË°åÊ≥õÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGEMÊ®°ÂùóÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜËá™Êàë-Ëá™ÊàëÊ≥®ÊÑèÂäõ‰∏éËÅöÁ±ªÁõ∏ÁªìÂêàÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇËøô‰∏ÄÊú∫Âà∂‰∏é‰º†ÁªüÁöÑÂÄº-ÂÄºÊ≥®ÊÑèÂäõÊñπÊ≥ï‰∏çÂêåÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂºÄÊîæËØçÊ±áÁâ©‰ΩìÂÆö‰Ωç‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®GEMÊ®°Âùó‰∏≠ÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊ≠£ÂàôÂåñÁ≠ñÁï•Ôºå‰ª•‰øÉËøõ‰ª§ÁâåÁöÑËÅöÁ±ªÂíåÂØπÈΩê„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùÂú®‰∏çÂêå‰ªªÂä°ÂíåÊï∞ÊçÆÈõÜ‰∏äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGEMÊ®°ÂùóÂú®Â§ö‰∏™Âü∫ÂáÜ‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÊó†ËÆ≠ÁªÉÁöÑÂºÄÊîæËØçÊ±áÂÆö‰ΩçÊñπÊ≥ï„ÄÇÂú®OpenImagesV7Â§ßËßÑÊ®°ÂàÜÂâ≤Âü∫ÂáÜ‰∏äÔºåGEMÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Èõ∂-shotÁâ©‰ΩìÂÆö‰Ωç‰∏≠ÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÂú∫ÊôØÔºåËÉΩÂ§üÂ∏ÆÂä©Á≥ªÁªüÊõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£ÂíåÂÆö‰ΩçÂõæÂÉè‰∏≠ÁöÑÁâ©‰Ωì„ÄÇÊú™Êù•ÔºåGEMÊ®°ÂùóÊúâÊúõÂú®Â§öÊ®°ÊÄÅÂ≠¶‰π†Âíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂèëÊå•Êõ¥Â§ß‰ΩúÁî®ÔºåÊèêÂçáÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language foundation models have shown remarkable performance in various zero-shot settings such as image retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot localization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object localization without any fine-tuning. To leverage those capabilities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery to a self-self attention path. We show that the concept of self-self attention corresponds to clustering, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various benchmark tasks and datasets for semantic segmentation. It shows that GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art results on the recently proposed OpenImagesV7 large-scale segmentation benchmark.

