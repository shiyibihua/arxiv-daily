---
layout: default
title: Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts
---

# Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00968" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00968v2</a>
  <a href="https://arxiv.org/pdf/2312.00968.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00968v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00968v2', 'Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, Radu Soricut

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-04-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmni-SMoLAä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é€šç”¨æ€§ä¸æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `ä¸“å®¶æ··åˆ` `è½¯æ··åˆä¸“å®¶` `ä½ç§©å­¦ä¹ ` `ç”Ÿæˆä»»åŠ¡` `æ€§èƒ½æå‡` `é€šç”¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€šç”¨å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é¢å¯¹å¤§é‡ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½å¸¸å¸¸å‡ºç°ä¸‹é™ï¼Œéš¾ä»¥ä¿æŒç¨³å®šçš„æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„Omni-SMoLAæ¶æ„é€šè¿‡è½¯æ··åˆå¤šä¸ªä½ç§©ä¸“å®¶ï¼Œå‡å°‘äº†æ–°å‚æ•°çš„å¼•å…¥ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„å¤šæ¨¡æ€å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmni-SMoLAåœ¨å¤šé¡¹ç”Ÿæˆè§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶Šäº†è®¸å¤šå•ä¸€ä¸“ä¸šæ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é’ˆå¯¹å¤§é‡ä»»åŠ¡è¿›è¡Œè°ƒä¼˜æ—¶ï¼Œé€šç”¨å‹LMMså¸¸å¸¸é¢ä¸´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„å¯¹æŒ‡ä»¤è°ƒä¼˜æœ‰å¸®åŠ©ï¼Œä½†å¯¹äºå‚æ•°è§„æ¨¡åœ¨O(50-100B)çš„LMMsï¼Œå¤åˆ¶å’Œå­˜å‚¨ä¸“å®¶æ¨¡å‹çš„é«˜æ˜‚æˆæœ¬é™åˆ¶äº†å¯ç”¨ä¸“å®¶æ•°é‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºOmni-SMoLAæ¶æ„ï¼Œé‡‡ç”¨è½¯æ··åˆä¸“å®¶ï¼ˆSoft MoEï¼‰æ–¹æ³•ï¼Œå·§å¦™åœ°æ··åˆå¤šä¸ªä½ç§©å¤šæ¨¡æ€ä¸“å®¶ï¼Œé¿å…äº†ä¸ä¼ ç»ŸMoEæ¨¡å‹ç›¸æ¯”å¼•å…¥å¤§é‡æ–°å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒSMoLAæ–¹æ³•åœ¨å¹¿æ³›çš„ç”Ÿæˆè§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸­æå‡äº†é€šç”¨æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸”å¸¸å¸¸è¶…è¶Šå•ä¸€ä¸“ä¸šLMMåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šä»»åŠ¡è°ƒä¼˜æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰çš„ä¸“å®¶æ··åˆæ–¹æ³•å› é«˜æ˜‚çš„å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œé™åˆ¶äº†ä¸“å®¶æ•°é‡ï¼Œå¯¼è‡´æ— æ³•å……åˆ†åˆ©ç”¨ä¸“å®¶çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmni-SMoLAé€šè¿‡è½¯æ··åˆå¤šä¸ªä½ç§©ä¸“å®¶ï¼Œåˆ©ç”¨å¤§å‹æ¨¡å‹ä½œä¸ºåŸºç¡€éª¨æ¶ï¼Œè½»é‡çº§ä¸“å®¶åˆ™ä¸“æ³¨äºç‰¹å®šçŸ¥è¯†çš„å­¦ä¹ ï¼Œä»è€Œæå‡æ¨¡å‹çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmni-SMoLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå¤§å‹åŸºç¡€æ¨¡å‹å’Œå¤šä¸ªä½ç§©ä¸“å®¶æ¨¡å—ã€‚åŸºç¡€æ¨¡å‹è´Ÿè´£æä¾›é€šç”¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸“å®¶æ¨¡å—åˆ™é€šè¿‡æ®‹å·®å­¦ä¹ æ–¹å¼ï¼Œé’ˆå¯¹ä¸åŒæ¨¡æ€æˆ–å¤šæ¨¡æ€ä»»åŠ¡è¿›è¡Œä¸“é—¨åŒ–è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥è½¯æ··åˆä¸“å®¶æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†æ–°å‚æ•°çš„å¼•å…¥ï¼ŒåŒæ—¶æå‡äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¸“å®¶æ··åˆæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°åˆ©ç”¨æ¨¡å‹å®¹é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒOmni-SMoLAé‡‡ç”¨äº†ä½ç§©çŸ©é˜µåˆ†è§£æŠ€æœ¯æ¥æ„å»ºä¸“å®¶æ¨¡å‹ï¼Œç¡®ä¿äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¤šæ¨¡æ€ç‰¹å¾çš„èåˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-SMoLAåœ¨å¤šé¡¹ç”Ÿæˆè§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå°¤å…¶åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†å•ä¸€ä¸“ä¸šLMMåŸºçº¿ï¼Œæå‡å¹…åº¦å¯è¾¾10%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Omni-SMoLAçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”å’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ¨¡å‹çš„é€šç”¨æ€§å’Œæ€§èƒ½ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach helps improve the generalist performance across a broad range of generative vision-and-language tasks, achieving new SoTA generalist performance that often matches or outperforms single specialized LMM baselines, as well as new SoTA specialist performance.

