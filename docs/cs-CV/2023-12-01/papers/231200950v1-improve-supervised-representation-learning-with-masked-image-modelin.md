---
layout: default
title: Improve Supervised Representation Learning with Masked Image Modeling
---

# Improve Supervised Representation Learning with Masked Image Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00950" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00950v1</a>
  <a href="https://arxiv.org/pdf/2312.00950.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00950v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00950v1', 'Improve Supervised Representation Learning with Masked Image Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaifeng Chen, Daniel Salz, Huiwen Chang, Kihyuk Sohn, Dilip Krishnan, Mojtaba Seyedhosseini

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„æ©ç å›¾åƒå»ºæ¨¡ä»¥æå‡ç›‘ç£è¡¨ç¤ºå­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ©ç å›¾åƒå»ºæ¨¡` `ç›‘ç£è¡¨ç¤ºå­¦ä¹ ` `è§†è§‰å˜æ¢å™¨` `å›¾åƒåˆ†ç±»` `å›¾åƒæ£€ç´¢` `è¯­ä¹‰åˆ†å‰²` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•åœ¨åˆ©ç”¨æ ‡è®°æ•°æ®æ—¶å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œéš¾ä»¥å……åˆ†æŒ–æ˜æ•°æ®ä¸­çš„æ½œåœ¨ä¿¡æ¯ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰é›†æˆåˆ°ç›‘ç£è®­ç»ƒä¸­çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ è§£ç å™¨å’ŒMIMä»»åŠ¡æ¥å¢å¼ºè¡¨ç¤ºå­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ImageNet-1kä¸Šå®ç°äº†81.72%çš„éªŒè¯å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†2.01%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè§†è§‰åµŒå…¥è®­ç»ƒå·²æˆä¸ºè¡¨ç¤ºå­¦ä¹ çš„å¸¸è§„è®¾ç½®ã€‚å—æœ€è¿‘æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰åœ¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ä¸­æˆåŠŸåº”ç”¨çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾ç½®ï¼Œå¯ä»¥è½»æ¾å°†MIMé›†æˆåˆ°ç°æœ‰çš„ç›‘ç£è®­ç»ƒèŒƒå¼ä¸­ã€‚æˆ‘ä»¬åœ¨è§†è§‰å˜æ¢å™¨å›¾åƒç¼–ç å™¨ä¸Šå¢åŠ äº†ä¸€ä¸ªæµ…å±‚å˜æ¢å™¨è§£ç å™¨ï¼Œå¹¶å¼•å…¥äº†MIMä»»åŠ¡ï¼Œä»¥æ ¹æ®æ©ç å›¾åƒè¾“å…¥é‡å»ºå›¾åƒæ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡æœ€å°çš„æ¶æ„æ›´æ”¹å’Œæ— æ¨ç†å¼€é”€ï¼Œè¯¥è®¾ç½®èƒ½å¤Ÿæé«˜ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ã€å›¾åƒæ£€ç´¢å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä¸­å­¦ä¹ åˆ°çš„è¡¨ç¤ºè´¨é‡ã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†ä¸Šè¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶å’Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨ImageNet-1kä¸Šï¼ŒViT-B/14æ¨¡å‹çš„éªŒè¯å‡†ç¡®ç‡è¾¾åˆ°81.72%ï¼Œæ¯”åŸºçº¿æ¨¡å‹é«˜å‡º2.01%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•åœ¨åˆ©ç”¨æ ‡è®°æ•°æ®æ—¶çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•æ›´æœ‰æ•ˆåœ°æŒ–æ˜å›¾åƒæ•°æ®ä¸­çš„ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„æ½œåŠ›ï¼Œå¯¼è‡´è¡¨ç¤ºå­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å¼•å…¥åˆ°ç›‘ç£å­¦ä¹ æ¡†æ¶ä¸­ï¼Œé€šè¿‡åœ¨è§†è§‰å˜æ¢å™¨ç¼–ç å™¨ä¸Šå¢åŠ ä¸€ä¸ªæµ…å±‚è§£ç å™¨ï¼Œå¹¶å¼•å…¥MIMä»»åŠ¡ï¼Œä»¥é‡å»ºæ©ç å›¾åƒè¾“å…¥ï¼Œä»è€Œæå‡å­¦ä¹ åˆ°çš„è¡¨ç¤ºè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè§†è§‰å˜æ¢å™¨å›¾åƒç¼–ç å™¨å’Œä¸€ä¸ªæµ…å±‚å˜æ¢å™¨è§£ç å™¨ã€‚ç¼–ç å™¨è´Ÿè´£æå–å›¾åƒç‰¹å¾ï¼Œè€Œè§£ç å™¨åˆ™é€šè¿‡MIMä»»åŠ¡é‡å»ºå›¾åƒæ ‡è®°ï¼Œå½¢æˆä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒæµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†MIMä¸ç›‘ç£å­¦ä¹ ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœï¼Œè€Œä¸”åœ¨æ¨ç†æ—¶æ²¡æœ‰å¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é‡å»ºä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒäº†åŸæœ‰åˆ†ç±»ä»»åŠ¡çš„ç»“æ„ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œè§£ç å™¨çš„è®¾è®¡è¾ƒä¸ºç®€å•ï¼Œæ—¨åœ¨å‡å°‘è®¡ç®—å¤æ‚åº¦å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œæå‡è¡¨ç¤ºçš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æViT-B/14æ¨¡å‹åœ¨ImageNet-1kä¸Šè¾¾åˆ°äº†81.72%çš„éªŒè¯å‡†ç¡®ç‡ï¼Œæ¯”åŸºçº¿æ¨¡å‹æå‡äº†2.01%ã€‚åœ¨K-Nearest-Neighborå›¾åƒæ£€ç´¢è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹åŒæ ·è¶…è¶Šäº†åŸºçº¿ï¼Œæå‡å¹…åº¦ä¸º1.32%ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å‡å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒåˆ†ç±»ã€å›¾åƒæ£€ç´¢å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡è¡¨ç¤ºå­¦ä¹ çš„è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½è§†è§‰ç³»ç»Ÿçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„æ•°æ®é›†å’Œæ›´å¤æ‚çš„æ¨¡å‹ä¸­ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training visual embeddings with labeled data supervision has been the de facto setup for representation learning in computer vision. Inspired by recent success of adopting masked image modeling (MIM) in self-supervised representation learning, we propose a simple yet effective setup that can easily integrate MIM into existing supervised training paradigms. In our design, in addition to the original classification task applied to a vision transformer image encoder, we add a shallow transformer-based decoder on top of the encoder and introduce an MIM task which tries to reconstruct image tokens based on masked image inputs. We show with minimal change in architecture and no overhead in inference that this setup is able to improve the quality of the learned representations for downstream tasks such as classification, image retrieval, and semantic segmentation. We conduct a comprehensive study and evaluation of our setup on public benchmarks. On ImageNet-1k, our ViT-B/14 model achieves 81.72% validation accuracy, 2.01% higher than the baseline model. On K-Nearest-Neighbor image retrieval evaluation with ImageNet-1k, the same model outperforms the baseline by 1.32%. We also show that this setup can be easily scaled to larger models and datasets. Code and checkpoints will be released.

