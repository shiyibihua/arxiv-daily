---
layout: default
title: Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning
---

# Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00360" class="toolbar-btn" target="_blank">üìÑ arXiv: 2312.00360v2</a>
  <a href="https://arxiv.org/pdf/2312.00360.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00360v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00360v2', 'Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shaohua Dong, Yunhe Feng, Qing Yang, Yan Huang, Dongfang Liu, Heng Fan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2023-12-01 (Êõ¥Êñ∞: 2023-12-04)

**Â§áÊ≥®**: 11 pages, 4 figures, 9 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DPLNet‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅËØ≠‰πâÂàÜÂâ≤ËÆ≠ÁªÉÊïàÁéá‰ΩéÁöÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅËûçÂêà` `ËØ≠‰πâÂàÜÂâ≤` `Ê∑±Â∫¶Â≠¶‰π†` `ËΩªÈáèÁ∫ßÊ®°Âûã` `ÁâπÂæÅÈÄÇÈÖç` `ÊèêÁ§∫Â≠¶‰π†` `ËÆ°ÁÆóÊú∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂØπÂ§çÊùÇÁöÑÂèåÂàÜÊîØÊ°ÜÊû∂ËøõË°åÂÖ®Èù¢ÂæÆË∞ÉÔºåÂØºËá¥ËÆ≠ÁªÉÊàêÊú¨È´ò‰∏îÊïàÁéá‰Ωé„ÄÇ
2. Êú¨ÊñáÊèêÂá∫DPLNetÔºåÈÄöËøáÂºïÂÖ•ËΩªÈáèÁ∫ßÁöÑÂ§öÊ®°ÊÄÅÊèêÁ§∫ÁîüÊàêÂô®ÂíåÁâπÂæÅÈÄÇÈÖçÂô®ÔºåÁõ¥Êé•ÈÄÇÂ∫îÂÜªÁªìÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ª•ÊèêÈ´òËÆ≠ÁªÉÊïàÁéá„ÄÇ
3. DPLNetÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊÄßËÉΩÔºå‰∏î‰ªÖÂºïÂÖ•‰∫ÜÂ∞ëÈáèÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÂπøÊ≥õÈÄÇÁî®ÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÔºàÂ¶ÇRGB-Ê∑±Â∫¶/RGB-ÁÉ≠ÊàêÂÉèÔºâËûçÂêàÂú®Â§çÊùÇÂú∫ÊôØÔºàÂ¶ÇÂÆ§ÂÜÖ/‰ΩéÂÖâÁÖßÊù°‰ª∂Ôºâ‰∏ãÁöÑËØ≠‰πâÂàÜÂâ≤‰∏≠Â±ïÁé∞Âá∫Â∑®Â§ßÊΩúÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂØπÂèåÂàÜÊîØÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Ê°ÜÊû∂ËøõË°åÂÖ®Èù¢ÂæÆË∞ÉÔºåÂØºËá¥ËÆ≠ÁªÉÊàêÊú¨È´òÊòÇ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÂèåÊèêÁ§∫Â≠¶‰π†ÁΩëÁªúÔºàDPLNetÔºâÔºåÈÄöËøáÁõ¥Êé•ÈÄÇÂ∫îÂÜªÁªìÁöÑÈ¢ÑËÆ≠ÁªÉRGBÊ®°ÂûãÊù•ÂÆûÁé∞Â§öÊ®°ÊÄÅËØ≠‰πâÂàÜÂâ≤Ôºå‰ªéËÄåÂáèÂ∞ëÂèÇÊï∞Êõ¥Êñ∞„ÄÇDPLNetÁöÑÊ†∏ÂøÉÂú®‰∫éÂºïÂÖ•‰∫ÜÂ§öÊ®°ÊÄÅÊèêÁ§∫ÁîüÊàêÂô®ÔºàMPGÔºâÂíåÂ§öÊ®°ÊÄÅÁâπÂæÅÈÄÇÈÖçÂô®ÔºàMFAÔºâÔºåËøô‰∏§‰∏™Ê®°ÂùóËΩªÈáè‰∏î‰ªÖÂºïÂÖ•Â∞ëÈáèÂèØËÆ≠ÁªÉÂèÇÊï∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDPLNetÂú®Âõõ‰∏™RGB-D/TËØ≠‰πâÂàÜÂâ≤Êï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊÄßËÉΩÔºåÂπ∂‰∏îÂú®ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠‰πüË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅËØ≠‰πâÂàÜÂâ≤‰∏≠ÁöÑËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂØπÂ§çÊùÇÁöÑÂèåÂàÜÊîØÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Ê°ÜÊû∂ËøõË°åÂÖ®Èù¢ÂæÆË∞ÉÔºåÂØºËá¥ËÆ≠ÁªÉÊàêÊú¨È´òÊòÇ‰∏îÂèÇÊï∞Êõ¥Êñ∞ÈáèÂ§ß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDPLNetÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁõ¥Êé•ÈÄÇÂ∫îÂÜªÁªìÁöÑÈ¢ÑËÆ≠ÁªÉRGBÊ®°ÂûãÊù•ÂÆûÁé∞Â§öÊ®°ÊÄÅËØ≠‰πâÂàÜÂâ≤ÔºåÂáèÂ∞ë‰∫ÜÈúÄË¶ÅÊõ¥Êñ∞ÁöÑÂèÇÊï∞Êï∞Èáè„ÄÇÈÄöËøáÂºïÂÖ•Â§öÊ®°ÊÄÅÊèêÁ§∫ÁîüÊàêÂô®ÔºàMPGÔºâÂíåÂ§öÊ®°ÊÄÅÁâπÂæÅÈÄÇÈÖçÂô®ÔºàMFAÔºâÔºåDPLNetËÉΩÂ§üÊúâÊïàËûçÂêà‰∏çÂêåÊ®°ÊÄÅÁöÑÁâπÂæÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDPLNetÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂ§öÊ®°ÊÄÅÊèêÁ§∫ÁîüÊàêÂô®ÔºàMPGÔºâÂíåÂ§öÊ®°ÊÄÅÁâπÂæÅÈÄÇÈÖçÂô®ÔºàMFAÔºâ„ÄÇMPGË¥üË¥£Âú®‰∏çÂêåÊ∑±Â∫¶Èò∂ÊÆµÁîüÊàêÂ§öÂ±ÇÊ¨°ÁöÑÂ§öÊ®°ÊÄÅÊèêÁ§∫ÔºåÂπ∂Â∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞ÂÜªÁªìÁöÑ‰∏ªÂπ≤ÁΩëÁªú‰∏≠ÔºåËÄåMFAÂàôÈÄÇÂ∫îËøô‰∫õÊèêÁ§∫‰ª•‰ºòÂåñÂ§öÊ®°ÊÄÅÁâπÂæÅÁöÑÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDPLNetÁöÑÊúÄÈáçË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ËΩªÈáèÁ∫ßËÆæËÆ°Ôºå‰ªÖÂºïÂÖ•‰∫Ü3.88MÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞ÔºàÂç†È¢ÑËÆ≠ÁªÉ‰∏ªÂπ≤ÂèÇÊï∞ÁöÑ4.4%ÔºâÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÂ§çÊùÇÊñπÊ≥ïÁõ∏ÊØîÔºåDPLNetÂú®ÂèÇÊï∞ÊïàÁéá‰∏äÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDPLNetÈááÁî®ÁÆÄÂçïÁöÑËß£Á†ÅÂô®Ôºà3.27MÂèÇÊï∞ÔºâÔºåÂπ∂ÈÄöËøáËΩªÈáèÁ∫ßÁöÑMPGÂíåMFAÊ®°ÂùóÂÆûÁé∞ÁâπÂæÅËûçÂêàÂíåÂ≠¶‰π†„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑËÆæËÆ°‰∏äÔºåDPLNetÊ≤°ÊúâÁâπÂà´ÁöÑËÆæËÆ°Ôºå‰ΩøÂÖ∂Âú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DPLNetÂú®Âõõ‰∏™RGB-D/TËØ≠‰πâÂàÜÂâ≤Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊÄßËÉΩÔºå‰∏î‰∏éÂÖ∂‰ªñÂ§çÊùÇÊñπÊ≥ïÁõ∏ÊØîÔºåÂèÇÊï∞ÊïàÁéáÊòæËëóÊèêÈ´ò„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåDPLNetÂú®‰øùÊåÅËæÉ‰ΩéÂèÇÊï∞ÈáèÁöÑÂêåÊó∂ÔºåËææÂà∞‰∫Ü‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÂΩìÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÂπøÊ≥õÈÄÇÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DPLNetÂú®Â§öÊ®°ÊÄÅËØ≠‰πâÂàÜÂâ≤‰∏≠ÁöÑÊàêÂäüÂ∫îÁî®Ë°®ÊòéÂÖ∂Âú®ÂÖ∂‰ªñÁõ∏ÂÖ≥‰ªªÂä°‰∏≠ÁöÑÊΩúÂäõÔºåÂ¶ÇÊòæËëóÁõÆÊ†áÊ£ÄÊµãÂíåËßÜÈ¢ëËØ≠‰πâÂàÜÂâ≤„ÄÇÂÖ∂È´òÊïàÁöÑËÆ≠ÁªÉÊñπÂºèÂíåËæÉ‰ΩéÁöÑÂèÇÊï∞ÈúÄÊ±Ç‰ΩøÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÖ∑ÊúâËæÉÈ´òÁöÑ‰ª∑ÂÄºÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇÊú™Êù•ÔºåDPLNetÁöÑËÆæËÆ°ÁêÜÂøµÂèØËÉΩ‰ºöÊé®Âä®Êõ¥Â§öËΩªÈáèÁ∫ßÊ®°ÂûãÁöÑÂºÄÂèëÔºå‰øÉËøõÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for improving semantic segmentation in complex scenes (e.g., indoor/low-light conditions). Existing approaches often fully fine-tune a dual-branch encoder-decoder framework with a complicated feature fusion strategy for achieving multimodal semantic segmentation, which is training-costly due to the massive parameter updates in feature extraction and fusion. To address this issue, we propose a surprisingly simple yet effective dual-prompt learning network (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T) semantic segmentation. The core of DPLNet is to directly adapt a frozen pre-trained RGB model to multimodal semantic segmentation, reducing parameter updates. For this purpose, we present two prompt learning modules, comprising multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG works to fuse the features from different modalities in a compact manner and is inserted from shadow to deep stages to generate the multi-level multimodal prompts that are injected into the frozen backbone, while MPG adapts prompted multimodal features in the frozen backbone for better multimodal semantic segmentation. Since both the MPG and MFA are lightweight, only a few trainable parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced for multimodal feature fusion and learning. Using a simple decoder (3.27M parameters), DPLNet achieves new state-of-the-art performance or is on a par with other complex approaches on four RGB-D/T semantic segmentation datasets while satisfying parameter efficiency. Moreover, we show that DPLNet is general and applicable to other multimodal tasks such as salient object detection and video semantic segmentation. Without special design, DPLNet outperforms many complicated models. Our code will be available at github.com/ShaohuaDong2021/DPLNet.

