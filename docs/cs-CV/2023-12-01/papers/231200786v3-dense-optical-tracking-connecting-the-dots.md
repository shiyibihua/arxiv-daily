---
layout: default
title: Dense Optical Tracking: Connecting the Dots
---

# Dense Optical Tracking: Connecting the Dots

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00786" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00786v3</a>
  <a href="https://arxiv.org/pdf/2312.00786.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00786v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00786v3', 'Dense Optical Tracking: Connecting the Dots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guillaume Le Moing, Jean Ponce, Cordelia Schmid

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-03-04)

**å¤‡æ³¨**: Accepted to CVPR 2024

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://16lemoing.github.io/dot)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDOTæ–¹æ³•ä»¥è§£å†³è§†é¢‘ç‚¹è·Ÿè¸ªé€Ÿåº¦æ…¢çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†é¢‘ç‚¹è·Ÿè¸ª` `å…‰æµä¼°è®¡` `é®æŒ¡å¤„ç†` `è®¡ç®—æœºè§†è§‰` `å®æ—¶ç›‘æ§` `æœºå™¨äººå¯¼èˆª` `åˆæˆæ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç‚¹è·Ÿè¸ªæ–¹æ³•åœ¨å¤„ç†è§†é¢‘æ—¶é€Ÿåº¦è¾ƒæ…¢ï¼Œæ— æ³•åœ¨åˆç†æ—¶é—´å†…è·Ÿè¸ªæ¯ä¸ªè§‚å¯Ÿåˆ°çš„ç‚¹ã€‚
2. DOTæ–¹æ³•é€šè¿‡æå–è¿åŠ¨è¾¹ç•Œçš„å…³é”®åŒºåŸŸè½¨è¿¹ï¼Œå¹¶åˆ©ç”¨å¯å­¦ä¹ çš„å…‰æµä¼°è®¡å™¨æ¥å¤„ç†é®æŒ¡é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†è·Ÿè¸ªé€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDOTåœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰å…‰æµæŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šè‡³å°‘å¿«ä¸¤ä¸ªæ•°é‡çº§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥çš„ç‚¹è·Ÿè¸ªæ–¹æ³•èƒ½å¤Ÿåœ¨è§†é¢‘ä¸­æ¢å¤åœºæ™¯ç‚¹çš„è½¨è¿¹ï¼Œå°½ç®¡å­˜åœ¨é®æŒ¡é—®é¢˜ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­è·Ÿè¸ªæ¯ä¸ªç‚¹çš„é€Ÿåº¦ä»ç„¶è¿‡æ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ã€ç®€å•ä¸”é«˜æ•ˆçš„æ–¹æ³•DOTï¼Œé¦–å…ˆé€šè¿‡ç°æˆçš„ç‚¹è·Ÿè¸ªç®—æ³•ä»è¿åŠ¨è¾¹ç•Œçš„å…³é”®åŒºåŸŸæå–ä¸€å°éƒ¨åˆ†è½¨è¿¹ã€‚ç„¶åï¼ŒDOTåœ¨ç»™å®šæºå¸§å’Œç›®æ ‡å¸§çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æœ€è¿‘é‚»æ’å€¼è®¡ç®—ç¨ å¯†å…‰æµåœºå’Œå¯è§æ€§æ©ç çš„ç²—ç•¥åˆå§‹ä¼°è®¡ï¼Œæ¥ç€åˆ©ç”¨å¯å­¦ä¹ çš„å…‰æµä¼°è®¡å™¨è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œè¯¥ä¼°è®¡å™¨æ˜¾å¼å¤„ç†é®æŒ¡ï¼Œå¹¶å¯ä»¥åœ¨å…·æœ‰çœŸå®å¯¹åº”å…³ç³»çš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDOTåœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰çš„å…‰æµæŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šè‡³å°‘å¿«ä¸¤ä¸ªæ•°é‡çº§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç‚¹è·Ÿè¸ªæ–¹æ³•åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é®æŒ¡æ—¶çš„æ€§èƒ½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åœ¨åˆç†æ—¶é—´å†…è·Ÿè¸ªæ¯ä¸ªç‚¹ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDOTæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æå–è¿åŠ¨è¾¹ç•Œçš„å…³é”®åŒºåŸŸè½¨è¿¹ï¼Œç»“åˆæœ€è¿‘é‚»æ’å€¼å’Œå¯å­¦ä¹ çš„å…‰æµä¼°è®¡å™¨ï¼Œæ¥å¿«é€Ÿä¸”å‡†ç¡®åœ°ä¼°è®¡ç¨ å¯†å…‰æµåœºå’Œå¯è§æ€§æ©ç ï¼Œä»è€Œæé«˜è·Ÿè¸ªæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDOTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨ç°æˆçš„ç‚¹è·Ÿè¸ªç®—æ³•ä»å…³é”®åŒºåŸŸæå–è½¨è¿¹ï¼›å…¶æ¬¡ï¼Œé€šè¿‡æœ€è¿‘é‚»æ’å€¼è®¡ç®—åˆå§‹çš„å…‰æµåœºå’Œå¯è§æ€§æ©ç ï¼›æœ€åï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„å…‰æµä¼°è®¡å™¨è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç‰¹åˆ«å¤„ç†é®æŒ¡æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šDOTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„å¤„ç†æµç¨‹å’Œå¯å­¦ä¹ çš„å…‰æµä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿæ˜¾å¼å¤„ç†é®æŒ¡é—®é¢˜ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒDOTé‡‡ç”¨äº†ç®€å•çš„æœ€è¿‘é‚»æ’å€¼æ–¹æ³•æ¥åˆæ­¥ä¼°è®¡å…‰æµåœºï¼Œå¹¶é€šè¿‡è®­ç»ƒå¾—åˆ°çš„æŸå¤±å‡½æ•°ä¼˜åŒ–å…‰æµä¼°è®¡å™¨ï¼Œç¡®ä¿å…¶åœ¨åˆæˆæ•°æ®ä¸Šèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ çœŸå®å¯¹åº”å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDOTåœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰çš„å…‰æµæŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šè‡³å°‘å¿«ä¸¤ä¸ªæ•°é‡çº§ã€‚ä¸å¤æ‚çš„â€œé€šç”¨â€è·Ÿè¸ªå™¨OmniMotionç›¸æ¯”ï¼ŒDOTè¡¨ç°æ›´ä½³ï¼Œä¸”ä¸æœ€ä½³ç‚¹è·Ÿè¸ªç®—æ³•CoTrackerç›¸å½“æˆ–æ›´å¥½ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DOTæ–¹æ³•åœ¨è§†é¢‘åˆ†æã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„ç‚¹è·Ÿè¸ªèƒ½åŠ›èƒ½å¤Ÿæ”¯æŒå®æ—¶ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ä»¥åŠå¢å¼ºç°å®ç­‰æŠ€æœ¯çš„å‘å±•ï¼Œæå‡è¿™äº›é¢†åŸŸçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent approaches to point tracking are able to recover the trajectory of any scene point through a large portion of a video despite the presence of occlusions. They are, however, too slow in practice to track every point observed in a single frame in a reasonable amount of time. This paper introduces DOT, a novel, simple and efficient method for solving this problem. It first extracts a small set of tracks from key regions at motion boundaries using an off-the-shelf point tracking algorithm. Given source and target frames, DOT then computes rough initial estimates of a dense flow field and visibility mask through nearest-neighbor interpolation, before refining them using a learnable optical flow estimator that explicitly handles occlusions and can be trained on synthetic data with ground-truth correspondences. We show that DOT is significantly more accurate than current optical flow techniques, outperforms sophisticated "universal" trackers like OmniMotion, and is on par with, or better than, the best point tracking algorithms like CoTracker while being at least two orders of magnitude faster. Quantitative and qualitative experiments with synthetic and real videos validate the promise of the proposed approach. Code, data, and videos showcasing the capabilities of our approach are available in the project webpage: https://16lemoing.github.io/dot .

