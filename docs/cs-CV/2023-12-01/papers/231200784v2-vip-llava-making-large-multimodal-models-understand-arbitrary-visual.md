---
layout: default
title: ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts
---

# ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00784v2</a>
  <a href="https://arxiv.org/pdf/2312.00784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00784v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00784v2', 'ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mu Cai, Haotian Liu, Dennis Park, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Yong Jae Lee

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-04-27)

**å¤‡æ³¨**: Accepted to CVPR2024. Project page: https://vip-llava.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViP-LLaVAä»¥è§£å†³åŒºåŸŸç‰¹å®šè§†è§‰ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `è§†è§‰ç†è§£` `ç”¨æˆ·äº¤äº’` `è§†è§‰æç¤º` `åŒºåŸŸç‰¹å®šç†è§£` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŒºåŸŸç‰¹å®šç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ç”¨æˆ·çš„è§†è§‰æç¤ºéœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç›´è§‚çš„è§†è§‰æ ‡è®°ä¸æ¨¡å‹äº¤äº’ï¼Œç®€åŒ–äº†è§†è§‰æç¤ºçš„ä½¿ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŒºåŸŸç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤§å‹è§†è§‰-è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ä¸»è¦é›†ä¸­äºæ•´ä½“å›¾åƒç†è§£ï¼Œä½†åœ¨å®ç°åŒºåŸŸç‰¹å®šç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å½“å‰ä½¿ç”¨æ–‡æœ¬åæ ‡æˆ–ç©ºé—´ç¼–ç çš„æ–¹æ³•å¾€å¾€æ— æ³•æä¾›ç”¨æˆ·å‹å¥½çš„è§†è§‰æç¤ºæ¥å£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿè§£ç ä»»æ„è§†è§‰æç¤ºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°æ ‡è®°å›¾åƒå¹¶ä½¿ç”¨è‡ªç„¶æç¤ºä¸æ¨¡å‹äº¤äº’ã€‚è¯¥è®¾è®¡ç›´æ¥å°†è§†è§‰æ ‡è®°å åŠ åˆ°RGBå›¾åƒä¸Šï¼Œæ¶ˆé™¤äº†å¤æ‚åŒºåŸŸç¼–ç çš„éœ€æ±‚ï¼Œå¹¶åœ¨Visual7Wã€PointQAå’Œè§†è§‰å¸¸è¯†æ¨ç†åŸºå‡†ç­‰åŒºåŸŸç†è§£ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ViP-Benchï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç†è§£è§†è§‰æç¤ºæ–¹é¢çš„èƒ½åŠ›ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è§†è§‰-è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒºåŸŸç‰¹å®šç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤æ‚çš„æ–‡æœ¬åæ ‡æˆ–ç©ºé—´ç¼–ç ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ¨¡å‹é€šè¿‡ç›´æ¥åœ¨å›¾åƒä¸Šå åŠ è§†è§‰æ ‡è®°ï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºï¼ˆå¦‚â€œçº¢è‰²è¾¹æ¡†â€æˆ–â€œæŒ‡å‘ç®­å¤´â€ï¼‰ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä»è€Œç®€åŒ–äº†è§†è§‰æç¤ºçš„ä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†ã€è§†è§‰æ ‡è®°è§£ç å’ŒåŒºåŸŸç†è§£ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç”¨æˆ·çš„è§†è§‰æç¤ºé€šè¿‡ç®€å•çš„æ ‡è®°æ–¹å¼è¾“å…¥ï¼Œæ¨¡å‹åˆ™é€šè¿‡è§£ç è¿™äº›æ ‡è®°æ¥ç†è§£å›¾åƒçš„ç‰¹å®šåŒºåŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ¨¡å‹èƒ½å¤Ÿç›´æ¥å¤„ç†ä»»æ„è§†è§‰æç¤ºï¼Œè€Œä¸éœ€è¦å¤æ‚çš„åŒºåŸŸç¼–ç ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿæ›´ç›´è§‚åœ°ä¸æ¨¡å‹äº¤äº’ï¼Œæå‡äº†æ¨¡å‹çš„å¯ç”¨æ€§å’Œç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å…³é”®è®¾è®¡åŒ…æ‹¬è§†è§‰æ ‡è®°çš„å åŠ æ–¹å¼ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠç½‘ç»œç»“æ„çš„ä¼˜åŒ–ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨åŒºåŸŸç†è§£ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViP-LLaVAåœ¨Visual7Wã€PointQAå’Œè§†è§‰å¸¸è¯†æ¨ç†åŸºå‡†ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œç†è§£èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æ¯”è¾ƒï¼Œå±•ç¤ºäº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å›¾åƒç¼–è¾‘ã€å¢å¼ºç°å®å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›æ›´ç›´è§‚çš„è§†è§‰æç¤ºæ¥å£ï¼Œç”¨æˆ·èƒ½å¤Ÿæ›´æ–¹ä¾¿åœ°ä¸å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä»è€Œæå‡å„ç§åº”ç”¨çš„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ•™è‚²ã€åŒ»ç–—å’Œå¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a "red bounding box" or "pointed arrow". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.

