---
layout: default
title: WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models
---

# WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00700" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00700v5</a>
  <a href="https://arxiv.org/pdf/2312.00700.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00700v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00700v5', 'WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chinmay Savadikar, Xi Song, Tianfu Wu

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2025-07-13)

**å¤‡æ³¨**: Accepted to ICML25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/savadikarc/wegeft)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWeGeFTä»¥å®ç°å¤§å‹æ¨¡å‹çš„é«˜æ•ˆé€‚åº”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¾®è°ƒ` `å‚æ•°æ•ˆç‡` `è¡¨ç¤ºæ•ˆç‡` `ä½ç§©é€‚åº”` `Transformeræ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰` `æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¾®è°ƒæ–¹æ³•åœ¨å‚æ•°æ•ˆç‡å’Œè¡¨ç¤ºæ•ˆç‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå¯¼è‡´æ€§èƒ½å’Œèµ„æºåˆ©ç”¨çš„ä¸è¶³ã€‚
2. WeGeFTé€šè¿‡ä»é¢„è®­ç»ƒæƒé‡ç”Ÿæˆå¾®è°ƒæƒé‡ï¼Œé‡‡ç”¨ä½ç§©ç»“æ„å®ç°å‚æ•°å’Œè¡¨ç¤ºçš„é«˜æ•ˆåˆ©ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒWeGeFTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºLoRAåŠå…¶å˜ä½“ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹å¤§å‹é¢„è®­ç»ƒTransformeræ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œé€šå¸¸éœ€è¦åœ¨å¼•å…¥å°‘é‡æ–°å¯å­¦ä¹ å‚æ•°æˆ–ä½¿ç”¨è½»é‡æ¨¡å—ç¼–è¾‘å°‘é‡tokenè¡¨ç¤ºä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚å°½ç®¡LoRAæ–¹æ³•åœ¨å‚æ•°ã€è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä¸Šå–å¾—äº†å¹³è¡¡ï¼Œä½†è®¸å¤šåç»­å˜ä½“åœ¨è¿›ä¸€æ­¥å‡å°‘å¾®è°ƒå‚æ•°æ—¶ç‰ºç‰²äº†è®¡ç®—å’Œå†…å­˜æ•ˆç‡åŠæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§å¹¶ç»Ÿä¸€å‚æ•°é«˜æ•ˆå’Œè¡¨ç¤ºé«˜æ•ˆçš„å¾®è°ƒï¼Œæœ¬æ–‡æå‡ºäº†Weight-Generative Fine-Tuningï¼ˆWeGeFTï¼‰ï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç›´æ¥ä»é¢„è®­ç»ƒæƒé‡ç”Ÿæˆå¾®è°ƒæƒé‡ã€‚WeGeFTé‡‡ç”¨ç®€å•çš„ä½ç§©å½¢å¼ï¼Œç”±ä¸¤ä¸ªçº¿æ€§å±‚ç»„æˆï¼Œè¿™äº›å±‚å¯ä»¥åœ¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å±‚ä¹‹é—´å…±äº«ï¼Œæˆ–ä¸ºä¸åŒå±‚å•ç‹¬å­¦ä¹ ã€‚è¯¥è®¾è®¡åœ¨å‚æ•°ã€è¡¨ç¤ºã€è®¡ç®—å’Œå†…å­˜æ–¹é¢å®ç°äº†å¤šæ–¹é¢çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæˆ–è¶…è¿‡äº†LoRAåŠå…¶å˜ä½“çš„æ€§èƒ½ã€‚å¤§é‡å®éªŒéªŒè¯äº†WeGeFTåœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯æ¨ç†ã€æŒ‡ä»¤è·Ÿéšã€ä»£ç ç”Ÿæˆå’Œè§†è§‰è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­çš„å‚æ•°æ•ˆç‡å’Œè¡¨ç¤ºæ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚LoRAè™½ç„¶åœ¨æŸç§ç¨‹åº¦ä¸Šå¹³è¡¡äº†è¿™äº›æ•ˆç‡ï¼Œä½†åœ¨è¿›ä¸€æ­¥å‡å°‘å¾®è°ƒå‚æ•°æ—¶ï¼Œå¾€å¾€ä¼šç‰ºç‰²è®¡ç®—å’Œå†…å­˜æ•ˆç‡åŠæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWeGeFTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ç›´æ¥ä»é¢„è®­ç»ƒæƒé‡ç”Ÿæˆå¾®è°ƒæƒé‡ï¼Œé‡‡ç”¨ä½ç§©ç»“æ„ä»¥å®ç°å‚æ•°å’Œè¡¨ç¤ºçš„é«˜æ•ˆåˆ©ç”¨ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒWeGeFTèƒ½å¤Ÿåœ¨ä¿æŒæˆ–è¶…è¶ŠLoRAæ€§èƒ½çš„åŒæ—¶ï¼Œä¼˜åŒ–è®¡ç®—å’Œå†…å­˜ä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWeGeFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç”Ÿæˆå¾®è°ƒæƒé‡çš„ä½ç§©çº¿æ€§å±‚å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å¤šä¸ªå±‚ã€‚çº¿æ€§å±‚å¯ä»¥åœ¨ä¸åŒå±‚ä¹‹é—´å…±äº«ï¼Œæˆ–ä¸ºæ¯ä¸ªå±‚å•ç‹¬å­¦ä¹ ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šWeGeFTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æƒé‡ç”Ÿæˆæœºåˆ¶ï¼Œé€šè¿‡ä½ç§©çº¿æ€§å±‚ç›´æ¥ç”Ÿæˆå¾®è°ƒæƒé‡ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…é€šå¸¸ä¾èµ–äºå¼•å…¥æ–°çš„å¯å­¦ä¹ å‚æ•°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒWeGeFTçš„ä½ç§©çº¿æ€§å±‚ç»“æ„ç®€å•ï¼Œæ˜“äºå®ç°ï¼Œä¸”åœ¨å‚æ•°è®¾ç½®ä¸Šçµæ´»ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒå±‚çš„éœ€æ±‚è¿›è¡Œå…±äº«æˆ–ç‹¬ç«‹å­¦ä¹ ã€‚æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWeGeFTåœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯æ¨ç†ã€æŒ‡ä»¤è·Ÿéšã€ä»£ç ç”Ÿæˆå’Œè§†è§‰è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜äºLoRAåŠå…¶å˜ä½“ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°5%-10%ã€‚è¿™äº›ç»“æœéªŒè¯äº†WeGeFTåœ¨å¤šæ–¹é¢æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ï¼Œå±•ç°äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

WeGeFTçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡é«˜æ•ˆçš„å¾®è°ƒæœºåˆ¶ï¼ŒWeGeFTèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿé€‚åº”å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large pretrained Transformer models can focus on either introducing a small number of new learnable parameters (parameter efficiency) or editing representations of a small number of tokens using lightweight modules (representation efficiency). While the pioneering method LoRA (Low-Rank Adaptation) inherently balances parameter, compute, and memory efficiency, many subsequent variants trade off compute and memory efficiency and/or performance to further reduce fine-tuning parameters. To address this limitation and unify parameter-efficient and representation-efficient fine-tuning, we propose Weight-Generative Fine-Tuning (WeGeFT, pronounced wee-gift), a novel approach that learns to generate fine-tuning weights directly from the pretrained weights. WeGeFT employs a simple low-rank formulation consisting of two linear layers, either shared across multiple layers of the pretrained model or individually learned for different layers. This design achieves multi-faceted efficiency in parameters, representations, compute, and memory, while maintaining or exceeding the performance of LoRA and its variants. Extensive experiments on commonsense reasoning, arithmetic reasoning, instruction following, code generation, and visual recognition verify the effectiveness of our proposed WeGeFT. Our code is available at https://github.com/savadikarc/wegeft

