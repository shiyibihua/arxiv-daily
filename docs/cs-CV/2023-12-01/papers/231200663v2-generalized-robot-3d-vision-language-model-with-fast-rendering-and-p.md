---
layout: default
title: Generalized Robot 3D Vision-Language Model with Fast Rendering and Pre-Training Vision-Language Alignment
---

# Generalized Robot 3D Vision-Language Model with Fast Rendering and Pre-Training Vision-Language Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00663" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00663v2</a>
  <a href="https://arxiv.org/pdf/2312.00663.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00663v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00663v2', 'Generalized Robot 3D Vision-Language Model with Fast Rendering and Pre-Training Vision-Language Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kangcheng Liu, Yong-Jin Liu, Baoquan Chen

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2025-02-19)

**å¤‡æ³¨**: IEEE Transactions on Pattern Analysis and Machine Intelligence, Manuscript Info: 17 Pages, 13 Figures, and 6 Tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æœºå™¨äºº3Dè§†è§‰è¯­è¨€æ¨¡å‹ä»¥è§£å†³ç¨€ç¼ºæ ‡ç­¾ä¸‹çš„åœºæ™¯ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `è§†è§‰è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è’¸é¦` `æ— ç›‘ç£å­¦ä¹ ` `å¼€æ”¾è¯æ±‡` `ç‚¹äº‘åˆ†å‰²` `å®ä¾‹åˆ†å‰²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dåœºæ™¯ç†è§£æ–¹æ³•åœ¨å¤„ç†æœªçŸ¥ç±»åˆ«æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•é€‚åº”çœŸå®ä¸–ç•Œçš„å¤šæ ·æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡ç‰¹å¾å¯¹é½çš„é¢„è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œæ—¨åœ¨ä»è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æå–æ–°ç±»åˆ«çŸ¥è¯†ã€‚
3. WS3D++æ–¹æ³•åœ¨ScanNetåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡å‡å–å¾—ç¬¬ä¸€åï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å°é—­é›†è®¾ç½®å’Œå…¨æ ‡ç­¾è®­ç»ƒä¸‹å­˜åœ¨ç“¶é¢ˆï¼Œæ— æ³•è¯†åˆ«è®­ç»ƒç±»åˆ«ä¹‹å¤–çš„æœªçŸ¥æ–°ç±»åˆ«ã€‚å› æ­¤ï¼Œæ€¥éœ€ä¸€ä¸ªæ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶é€‚ç”¨äº3Dç‚¹äº‘åˆ†å‰²å’Œæ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨ä¸”ç®€å•çš„æ¡†æ¶ï¼Œåˆ©ç”¨å±‚æ¬¡ç‰¹å¾å¯¹é½çš„é¢„è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä»å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æå–å’Œè’¸é¦æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œä¿ƒè¿›å¼€æ”¾è¯æ±‡åœºæ™¯ç†è§£ä»»åŠ¡ã€‚é€šè¿‡æ— ç›‘ç£åŒºåŸŸçº§è¯­ä¹‰å¯¹æ¯”å­¦ä¹ æ–¹æ¡ˆï¼Œç¡®ä¿äº†æ•ˆç‡å’Œæ½œåœ¨å®ä¾‹åŒºåˆ†ã€‚æˆ‘ä»¬çš„WS3D++æ–¹æ³•åœ¨å¤§è§„æ¨¡ScanNetåŸºå‡†ä¸Šï¼Œåœ¨è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å‡æ’åç¬¬ä¸€ï¼Œè¯æ˜äº†å…¶åœ¨æ•°æ®é«˜æ•ˆå­¦ä¹ å’Œå¼€æ”¾ä¸–ç•Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ ‡ç­¾ç¨€ç¼ºæƒ…å†µä¸‹çš„3Dåœºæ™¯ç†è§£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªçŸ¥ç±»åˆ«æ—¶è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡å±‚æ¬¡ç‰¹å¾å¯¹é½çš„é¢„è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦ï¼Œä»å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æå–æœ‰ç”¨ä¿¡æ¯ï¼Œä»¥æ”¯æŒå¼€æ”¾è¯æ±‡çš„åœºæ™¯ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–ã€çŸ¥è¯†è’¸é¦å’Œæ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç‰¹å¾æå–é˜¶æ®µåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è·å–åˆå§‹ç‰¹å¾ï¼ŒçŸ¥è¯†è’¸é¦é˜¶æ®µæå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œè€Œå¯¹æ¯”å­¦ä¹ é˜¶æ®µåˆ™é€šè¿‡æ— ç›‘ç£æ–¹å¼å¢å¼ºç‰¹å¾çš„åŒºåˆ†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†å±‚æ¬¡ç‰¹å¾å¯¹é½å’Œæ— ç›‘ç£åŒºåŸŸçº§è¯­ä¹‰å¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æœªçŸ¥ç±»åˆ«è¯†åˆ«ä¸Šçš„èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„å¼€æ”¾æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šé˜¶æ®µç‰¹å¾åµŒå…¥å’Œè‡ªä¿¡é¢„æµ‹æœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡äºè¯­ä¹‰å¯¹æ¯”ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒé˜¶æ®µçš„ç‰¹å¾æœ‰æ•ˆæ€§å’ŒåŒºåˆ†æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒé«˜æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

WS3D++æ–¹æ³•åœ¨ScanNetåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡å‡æ’åç¬¬ä¸€ï¼Œå±•ç¤ºäº†åœ¨æ•°æ®é«˜æ•ˆå­¦ä¹ å’Œå¼€æ”¾ä¸–ç•Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ˜¾è‘—æå‡ã€‚ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡æå‡3Dåœºæ™¯ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´æ™ºèƒ½çš„å†³ç­–å’Œæ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¤šå¼€æ”¾ä¸–ç•Œåœºæ™¯ç†è§£ä»»åŠ¡çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep neural network models have achieved remarkable progress in 3D scene understanding while trained in the closed-set setting and with full labels. However, the major bottleneck is that these models do not have the capacity to recognize any unseen novel classes beyond the training categories in diverse real-world applications. Therefore, we are in urgent need of a framework that can simultaneously be applicable to both 3D point cloud segmentation and detection, particularly in the circumstances where the labels are rather scarce. This work presents a generalized and straightforward framework for dealing with 3D scene understanding when the labeled scenes are quite limited. To extract knowledge for novel categories from the pre-trained vision-language models, we propose a hierarchical feature-aligned pre-training and knowledge distillation strategy to extract and distill meaningful information from large-scale vision-language models, which helps benefit the open-vocabulary scene understanding tasks. To encourage latent instance discrimination and to guarantee efficiency, we propose the unsupervised region-level semantic contrastive learning scheme for point clouds, using confident predictions of the neural network to discriminate the intermediate feature embeddings at multiple stages. In the limited reconstruction case, our proposed approach, termed WS3D++, ranks 1st on the large-scale ScanNet benchmark on both the task of semantic segmentation and instance segmentation. Extensive experiments with both indoor and outdoor scenes demonstrated the effectiveness of our approach in both data-efficient learning and open-world few-shot learning. The code is made publicly available at: https://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.

