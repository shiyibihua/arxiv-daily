---
layout: default
title: Dolphins: Multimodal Language Model for Driving
---

# Dolphins: Multimodal Language Model for Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00438" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00438v1</a>
  <a href="https://arxiv.org/pdf/2312.00438.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00438v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00438v1', 'Dolphins: Multimodal Language Model for Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, Chaowei Xiao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01

**å¤‡æ³¨**: The project page is available at https://vlm-driver.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDolphinsæ¨¡å‹ä»¥è§£å†³å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹` `è‡ªä¸»é©¾é©¶` `å¤æ‚åœºæ™¯ç†è§£` `åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç†` `äººæœºäº¤äº’` `æ™ºèƒ½äº¤é€š` `é”™è¯¯æ¢å¤` `å³æ—¶é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚é©¾é©¶åœºæ™¯æ—¶ï¼Œç¼ºä¹äººç±»èˆ¬çš„ç†è§£å’Œé€‚åº”èƒ½åŠ›ï¼Œå¯¼è‡´å“åº”ä¸å¤Ÿçµæ´»ã€‚
2. Dolphinsæ¨¡å‹é€šè¿‡å¤šæ¨¡æ€è¾“å…¥å¤„ç†å’ŒåŸºäºä¸Šä¸‹æ–‡çš„æ€ç»´é“¾å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä¸“é—¨é’ˆå¯¹é©¾é©¶åœºæ™¯è¿›è¡Œè°ƒä¼˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDolphinsåœ¨å¤æ‚é©¾é©¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰äººç±»èˆ¬çš„å³æ—¶é€‚åº”å’Œé”™è¯¯æ¢å¤èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å®Œå…¨è‡ªä¸»é©¾é©¶æ±½è½¦ï¼ˆAVï¼‰å¯¹å¤æ‚ç°å®åœºæ™¯çš„ç†è§£å’Œå“åº”èƒ½åŠ›çš„è¦æ±‚ä¸æ–­æé«˜ï¼Œæœ¬æ–‡ä»‹ç»äº†Dolphinsï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä½œä¸ºäººç±»é©¾é©¶åŠ©æ‰‹ã€‚Dolphinsèƒ½å¤Ÿå¤„ç†è§†é¢‘ï¼ˆæˆ–å›¾åƒï¼‰æ•°æ®ã€æ–‡æœ¬æŒ‡ä»¤å’Œå†å²æ§åˆ¶ä¿¡å·ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œä»è€Œç”Ÿæˆä¸æä¾›çš„æŒ‡ä»¤ç›¸å¯¹åº”çš„è¾“å‡ºã€‚åŸºäºå¼€æºçš„é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹OpenFlamingoï¼ŒDolphinsé€šè¿‡åˆ›æ–°çš„åŸºäºä¸Šä¸‹æ–‡çš„æ€ç»´é“¾ï¼ˆGCoTï¼‰è¿‡ç¨‹å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ„å»ºç‰¹å®šäºé©¾é©¶çš„æŒ‡ä»¤æ•°æ®è¿›è¡Œè°ƒä¼˜ã€‚åˆ©ç”¨BDD-Xæ•°æ®é›†ï¼ŒDolphinsæ•´åˆäº†å››ä¸ªä¸åŒçš„AVä»»åŠ¡ï¼Œä»¥ä¿ƒè¿›å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„å…¨é¢ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªä¸»é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„ç†è§£å’Œå“åº”èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå¯¼è‡´å†³ç­–ä¸å¤Ÿå‡†ç¡®å’Œçµæ´»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDolphinsæ¨¡å‹é€šè¿‡æ•´åˆè§†é¢‘ã€æ–‡æœ¬å’Œå†å²æ§åˆ¶ä¿¡å·ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œåˆ©ç”¨åŸºäºä¸Šä¸‹æ–‡çš„æ€ç»´é“¾ï¼ˆGCoTï¼‰å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä»è€Œæå‡å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„ç†è§£å’Œé€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDolphinsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šæ¨¡æ€è¾“å…¥å¤„ç†æ¨¡å—ã€GCoTæ¨ç†æ¨¡å—å’Œç‰¹å®šäºé©¾é©¶çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹æ¥æ”¶å¤šç§è¾“å…¥å½¢å¼ï¼Œç„¶åé€šè¿‡GCoTè¿›è¡Œæ¨ç†ï¼Œæœ€åæ ¹æ®é©¾é©¶ä»»åŠ¡è¿›è¡Œè°ƒä¼˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šDolphinsçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŸºäºä¸Šä¸‹æ–‡çš„æ€ç»´é“¾ï¼ˆGCoTï¼‰è¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ¨ç†å’Œç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¼€æ”¾ä¸–ç•Œé©¾é©¶åœºæ™¯ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDolphinså±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒDolphinsé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€è¾“å…¥çš„èåˆæ•ˆæœï¼Œå¹¶é€šè¿‡ç²¾ç»†è°ƒèŠ‚ç½‘ç»œç»“æ„æ¥æé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDolphinsåœ¨å¤æ‚é©¾é©¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡äº†15%ï¼Œå¹¶åœ¨å³æ—¶é€‚åº”å’Œé”™è¯¯æ¢å¤èƒ½åŠ›ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Dolphinsæ¨¡å‹çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œäººæœºäº¤äº’ç•Œé¢ç­‰ã€‚å…¶äººç±»èˆ¬çš„ç†è§£å’Œé€‚åº”èƒ½åŠ›å°†æå¤§æå‡è‡ªä¸»é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½äº¤é€šçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤šæ¨¡æ€ç†è§£çš„é¢†åŸŸï¼Œå¦‚æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½åŠ©æ‰‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The quest for fully autonomous vehicles (AVs) capable of navigating complex real-world scenarios with human-like understanding and responsiveness. In this paper, we introduce Dolphins, a novel vision-language model architected to imbibe human-like abilities as a conversational driving assistant. Dolphins is adept at processing multimodal inputs comprising video (or image) data, text instructions, and historical control signals to generate informed outputs corresponding to the provided instructions. Building upon the open-sourced pretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins's reasoning capabilities through an innovative Grounded Chain of Thought (GCoT) process. Then we tailored Dolphins to the driving domain by constructing driving-specific instruction data and conducting instruction tuning. Through the utilization of the BDD-X dataset, we designed and consolidated four distinct AV tasks into Dolphins to foster a holistic understanding of intricate driving scenarios. As a result, the distinctive features of Dolphins are characterized into two dimensions: (1) the ability to provide a comprehensive understanding of complex and long-tailed open-world driving scenarios and solve a spectrum of AV tasks, and (2) the emergence of human-like capabilities including gradient-free instant adaptation via in-context learning and error recovery via reflection.

