---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2023-12-01
---

# cs.CVï¼ˆ2023-12-01ï¼‰

ğŸ“Š å…± **24** ç¯‡è®ºæ–‡
 | ğŸ”— **9** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/231200690v4-open-vocabulary-object-6d-pose-estimation.html">Open-vocabulary object 6D pose estimation</a></td>
  <td>æå‡ºå¼€æ”¾è¯æ±‡å¯¹è±¡6Då§¿æ€ä¼°è®¡ä»¥è§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">6D pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00690v4" data-paper-url="./papers/231200690v4-open-vocabulary-object-6d-pose-estimation.html" onclick="toggleFavorite(this, '2312.00690v4', 'Open-vocabulary object 6D pose estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/231200451v2-fsgs-real-time-few-shot-view-synthesis-using-gaussian-splatting.html">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</a></td>
  <td>æå‡ºFSGSæ¡†æ¶ä»¥å®ç°å®æ—¶å°‘æ ·æœ¬è§†å›¾åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00451v2" data-paper-url="./papers/231200451v2-fsgs-real-time-few-shot-view-synthesis-using-gaussian-splatting.html" onclick="toggleFavorite(this, '2312.00451v2', 'FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/231200846v2-neusg-neural-implicit-surface-reconstruction-with-3d-gaussian-splatt.html">NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance</a></td>
  <td>æå‡ºNeuSGä»¥è§£å†³ç¥ç»éšå¼è¡¨é¢é‡å»ºä¸­ç»†èŠ‚ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00846v2" data-paper-url="./papers/231200846v2-neusg-neural-implicit-surface-reconstruction-with-3d-gaussian-splatt.html" onclick="toggleFavorite(this, '2312.00846v2', 'NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/231200947v3-freeze-training-free-zero-shot-6d-pose-estimation-with-geometric-and.html">FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models</a></td>
  <td>æå‡ºFreeZeä»¥è§£å†³æ— è®­ç»ƒçš„é›¶-shot 6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00947v3" data-paper-url="./papers/231200947v3-freeze-training-free-zero-shot-6d-pose-estimation-with-geometric-and.html" onclick="toggleFavorite(this, '2312.00947v3', 'FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/231200732v2-gaussian-grouping-segment-and-edit-anything-in-3d-scenes.html">Gaussian Grouping: Segment and Edit Anything in 3D Scenes</a></td>
  <td>æå‡ºGaussian Groupingä»¥è§£å†³3Dåœºæ™¯ç»†ç²’åº¦ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">NeRF</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00732v2" data-paper-url="./papers/231200732v2-gaussian-grouping-segment-and-edit-anything-in-3d-scenes.html" onclick="toggleFavorite(this, '2312.00732v2', 'Gaussian Grouping: Segment and Edit Anything in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/231200944v1-enhancing-diffusion-models-with-3d-perspective-geometry-constraints.html">Enhancing Diffusion Models with 3D Perspective Geometry Constraints</a></td>
  <td>æå‡ºå‡ ä½•çº¦æŸä»¥å¢å¼ºæ‰©æ•£æ¨¡å‹çš„é€è§†å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">zero-shot transfer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00944v1" data-paper-url="./papers/231200944v1-enhancing-diffusion-models-with-3d-perspective-geometry-constraints.html" onclick="toggleFavorite(this, '2312.00944v1', 'Enhancing Diffusion Models with 3D Perspective Geometry Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/231200878v3-grounding-everything-emerging-localization-properties-in-vision-lang.html">Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</a></td>
  <td>æå‡ºGEMæ¨¡å—ä»¥å®ç°é›¶-shotå¼€æ”¾è¯æ±‡ç‰©ä½“å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00878v3" data-paper-url="./papers/231200878v3-grounding-everything-emerging-localization-properties-in-vision-lang.html" onclick="toggleFavorite(this, '2312.00878v3', 'Grounding Everything: Emerging Localization Properties in Vision-Language Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/240105345v1-distwar-fast-differentiable-rendering-on-raster-based-rendering-pipe.html">DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines</a></td>
  <td>æå‡ºDISTWARä»¥åŠ é€Ÿå…‰æ …åŒ–æ¸²æŸ“ä¸­çš„åŸå­æ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2401.05345v1" data-paper-url="./papers/240105345v1-distwar-fast-differentiable-rendering-on-raster-based-rendering-pipe.html" onclick="toggleFavorite(this, '2401.05345v1', 'DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/231200860v3-segment-any-3d-gaussians.html">Segment Any 3D Gaussians</a></td>
  <td>æå‡ºSAGAæ–¹æ³•ä»¥å®ç°é«˜æ•ˆçš„3Dé«˜æ–¯åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00860v3" data-paper-url="./papers/231200860v3-segment-any-3d-gaussians.html" onclick="toggleFavorite(this, '2312.00860v3', 'Segment Any 3D Gaussians')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/231200786v3-dense-optical-tracking-connecting-the-dots.html">Dense Optical Tracking: Connecting the Dots</a></td>
  <td>æå‡ºDOTæ–¹æ³•ä»¥è§£å†³è§†é¢‘ç‚¹è·Ÿè¸ªé€Ÿåº¦æ…¢çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00786v3" data-paper-url="./papers/231200786v3-dense-optical-tracking-connecting-the-dots.html" onclick="toggleFavorite(this, '2312.00786v3', 'Dense Optical Tracking: Connecting the Dots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/231200778v2-morpheus-neural-dynamic-360-surface-reconstruction-from-monocular-rg.html">MorpheuS: Neural Dynamic 360Â° Surface Reconstruction from Monocular RGB-D Video</a></td>
  <td>æå‡ºMorpheuSä»¥è§£å†³åŠ¨æ€åœºæ™¯360Â°è¡¨é¢é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00778v2" data-paper-url="./papers/231200778v2-morpheus-neural-dynamic-360-surface-reconstruction-from-monocular-rg.html" onclick="toggleFavorite(this, '2312.00778v2', 'MorpheuS: Neural Dynamic 360Â° Surface Reconstruction from Monocular RGB-D Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/231200784v2-vip-llava-making-large-multimodal-models-understand-arbitrary-visual.html">ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</a></td>
  <td>æå‡ºViP-LLaVAä»¥è§£å†³åŒºåŸŸç‰¹å®šè§†è§‰ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VIP</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00784v2" data-paper-url="./papers/231200784v2-vip-llava-making-large-multimodal-models-understand-arbitrary-visual.html" onclick="toggleFavorite(this, '2312.00784v2', 'ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/231200663v2-generalized-robot-3d-vision-language-model-with-fast-rendering-and-p.html">Generalized Robot 3D Vision-Language Model with Fast Rendering and Pre-Training Vision-Language Alignment</a></td>
  <td>æå‡ºé€šç”¨æœºå™¨äºº3Dè§†è§‰è¯­è¨€æ¨¡å‹ä»¥è§£å†³ç¨€ç¼ºæ ‡ç­¾ä¸‹çš„åœºæ™¯ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00663v2" data-paper-url="./papers/231200663v2-generalized-robot-3d-vision-language-model-with-fast-rendering-and-p.html" onclick="toggleFavorite(this, '2312.00663v2', 'Generalized Robot 3D Vision-Language Model with Fast Rendering and Pre-Training Vision-Language Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/231200739v2-adversarial-score-distillation-when-score-distillation-meets-gan.html">Adversarial Score Distillation: When score distillation meets GAN</a></td>
  <td>æå‡ºå¯¹æŠ—æ€§åˆ†æ•°è’¸é¦æ–¹æ³•ä»¥è§£å†³ç°æœ‰æ–¹æ³•çš„æ•æ„Ÿæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">classifier-free guidance</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00739v2" data-paper-url="./papers/231200739v2-adversarial-score-distillation-when-score-distillation-meets-gan.html" onclick="toggleFavorite(this, '2312.00739v2', 'Adversarial Score Distillation: When score distillation meets GAN')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/231200950v1-improve-supervised-representation-learning-with-masked-image-modelin.html">Improve Supervised Representation Learning with Masked Image Modeling</a></td>
  <td>æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„æ©ç å›¾åƒå»ºæ¨¡ä»¥æå‡ç›‘ç£è¡¨ç¤ºå­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00950v1" data-paper-url="./papers/231200950v1-improve-supervised-representation-learning-with-masked-image-modelin.html" onclick="toggleFavorite(this, '2312.00950v1', 'Improve Supervised Representation Learning with Masked Image Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/231200863v1-efficientsam-leveraged-masked-image-pretraining-for-efficient-segmen.html">EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</a></td>
  <td>æå‡ºEfficientSAMä»¥è§£å†³SAMæ¨¡å‹è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">zero-shot transfer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00863v1" data-paper-url="./papers/231200863v1-efficientsam-leveraged-masked-image-pretraining-for-efficient-segmen.html" onclick="toggleFavorite(this, '2312.00863v1', 'EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/231200362v2-dancing-with-still-images-video-distillation-via-static-dynamic-dise.html">Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement</a></td>
  <td>æå‡ºé™æ€-åŠ¨æ€åˆ†ç¦»æ¡†æ¶ä»¥å®ç°è§†é¢‘è’¸é¦</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00362v2" data-paper-url="./papers/231200362v2-dancing-with-still-images-video-distillation-via-static-dynamic-dise.html" onclick="toggleFavorite(this, '2312.00362v2', 'Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/231200438v1-dolphins-multimodal-language-model-for-driving.html">Dolphins: Multimodal Language Model for Driving</a></td>
  <td>æå‡ºDolphinsæ¨¡å‹ä»¥è§£å†³å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00438v1" data-paper-url="./papers/231200438v1-dolphins-multimodal-language-model-for-driving.html" onclick="toggleFavorite(this, '2312.00438v1', 'Dolphins: Multimodal Language Model for Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/231200968v2-omni-smola-boosting-generalist-multimodal-models-with-soft-mixture-o.html">Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts</a></td>
  <td>æå‡ºOmni-SMoLAä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é€šç”¨æ€§ä¸æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00968v2" data-paper-url="./papers/231200968v2-omni-smola-boosting-generalist-multimodal-models-with-soft-mixture-o.html" onclick="toggleFavorite(this, '2312.00968v2', 'Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/231200360v2-efficient-multimodal-semantic-segmentation-via-dual-prompt-learning.html">Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning</a></td>
  <td>æå‡ºDPLNetä»¥è§£å†³å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²è®­ç»ƒæ•ˆç‡ä½çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00360v2" data-paper-url="./papers/231200360v2-efficient-multimodal-semantic-segmentation-via-dual-prompt-learning.html" onclick="toggleFavorite(this, '2312.00360v2', 'Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/231200937v1-zero-shot-video-question-answering-with-procedural-programs.html">Zero-Shot Video Question Answering with Procedural Programs</a></td>
  <td>æå‡ºProViQä»¥è§£å†³è§†é¢‘é›¶-shoté—®ç­”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00937v1" data-paper-url="./papers/231200937v1-zero-shot-video-question-answering-with-procedural-programs.html" onclick="toggleFavorite(this, '2312.00937v1', 'Zero-Shot Video Question Answering with Procedural Programs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/231200700v5-wegeft-weight-generative-fine-tuning-for-multi-faceted-efficient-ada.html">WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models</a></td>
  <td>æå‡ºWeGeFTä»¥å®ç°å¤§å‹æ¨¡å‹çš„é«˜æ•ˆé€‚åº”</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00700v5" data-paper-url="./papers/231200700v5-wegeft-weight-generative-fine-tuning-for-multi-faceted-efficient-ada.html" onclick="toggleFavorite(this, '2312.00700v5', 'WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/231200306v1-radiogalaxynet-dataset-and-novel-computer-vision-algorithms-for-the-.html">RadioGalaxyNET: Dataset and Novel Computer Vision Algorithms for the Detection of Extended Radio Galaxies and Infrared Hosts</a></td>
  <td>æå‡ºRadioGalaxyNETä»¥è‡ªåŠ¨æ£€æµ‹æ‰©å±•å°„ç”µæ˜Ÿç³»åŠå…¶çº¢å¤–å®¿ä¸»</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00306v1" data-paper-url="./papers/231200306v1-radiogalaxynet-dataset-and-novel-computer-vision-algorithms-for-the-.html" onclick="toggleFavorite(this, '2312.00306v1', 'RadioGalaxyNET: Dataset and Novel Computer Vision Algorithms for the Detection of Extended Radio Galaxies and Infrared Hosts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/231200651v2-trackdiffusion-tracklet-conditioned-video-generation-via-diffusion-m.html">TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models</a></td>
  <td>æå‡ºTrackDiffusionä»¥è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„åŠ¨æ€æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2312.00651v2" data-paper-url="./papers/231200651v2-trackdiffusion-tracklet-conditioned-video-generation-via-diffusion-m.html" onclick="toggleFavorite(this, '2312.00651v2', 'TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)