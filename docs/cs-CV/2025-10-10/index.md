---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-10
---

# cs.CVï¼ˆ2025-10-10ï¼‰

ğŸ“Š å…± **36** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251009230v1-diagnosing-shoulder-disorders-using-multimodal-large-language-models.html">Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥è§£å†³è‚©éƒ¨ç–¾ç—…è¯Šæ–­é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09230v1" onclick="toggleFavorite(this, '2510.09230v1', 'Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251009507v1-phystoolbench-benchmarking-physical-tool-understanding-for-mllms.html">PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs</a></td>
  <td>PhysToolBenchï¼šé¦–ä¸ªé¢å‘MLLMçš„ç‰©ç†å·¥å…·ç†è§£èƒ½åŠ›è¯„æµ‹åŸºå‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09507v1" onclick="toggleFavorite(this, '2510.09507v1', 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251009269v1-goal-oriented-backdoor-attack-against-vision-language-action-models-.html">Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects</a></td>
  <td>æå‡ºé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç‰©ç†å¯¹è±¡åé—¨æ”»å‡»GoBAï¼Œå®ç°ç›®æ ‡å¯¼å‘çš„æ¶æ„è¡Œä¸ºã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09269v1" onclick="toggleFavorite(this, '2510.09269v1', 'Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251009361v1-blink-twice-you-see-but-do-you-observe-a-reasoning-benchmark-on-visu.html">BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception</a></td>
  <td>BLINK-Twiceï¼šæå‡ºè§†è§‰æ„ŸçŸ¥æ¨ç†åŸºå‡†ï¼Œå¼ºè°ƒç»†ç²’åº¦è§‚å¯Ÿä¸åˆ†æï¼ŒæŒ‘æˆ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09361v1" onclick="toggleFavorite(this, '2510.09361v1', 'BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251009822v1-task-aware-resolution-optimization-for-visual-large-language-models.html">Task-Aware Resolution Optimization for Visual Large Language Models</a></td>
  <td>æå‡ºä»»åŠ¡æ„ŸçŸ¥åˆ†è¾¨ç‡ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡è§†è§‰å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09822v1" onclick="toggleFavorite(this, '2510.09822v1', 'Task-Aware Resolution Optimization for Visual Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251009815v1-towards-understanding-ambiguity-resolution-in-multimodal-inference-o.html">Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning</a></td>
  <td>ç ”ç©¶å¤šæ¨¡æ€è¯­å¢ƒä¸‹å¤–è¯­å­¦ä¹ è€…å¯¹è¯ä¹‰æ­§ä¹‰æ¶ˆè§£çš„æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09815v1" onclick="toggleFavorite(this, '2510.09815v1', 'Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251009358v1-boosting-multi-modal-keyphrase-prediction-with-dynamic-chain-of-thou.html">Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models</a></td>
  <td>æå‡ºåŠ¨æ€é“¾å¼æ€è€ƒæ–¹æ³•ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å…³é”®çŸ­è¯­é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09358v1" onclick="toggleFavorite(this, '2510.09358v1', 'Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251009224v2-tag-enriched-multi-attention-with-large-language-models-for-cross-do.html">Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation</a></td>
  <td>æå‡ºTEMA-LLMï¼Œåˆ©ç”¨LLMå¢å¼ºçš„å¤šæ³¨æ„åŠ›æœºåˆ¶è§£å†³è·¨åŸŸåºåˆ—æ¨èé—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09224v2" onclick="toggleFavorite(this, '2510.09224v2', 'Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251009203v1-cattle-clip-a-multimodal-framework-for-cattle-behaviour-recognition.html">Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition</a></td>
  <td>Cattle-CLIPï¼šåˆ©ç”¨å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶è¿›è¡Œç‰›è¡Œä¸ºè¯†åˆ«ï¼Œæå‡æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09203v1" onclick="toggleFavorite(this, '2510.09203v1', 'Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251009121v2-msdm-generating-task-specific-pathology-images-with-a-multimodal-con.html">MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation</a></td>
  <td>æå‡ºMSDMï¼Œä¸€ç§å¤šæ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆç»†èƒå’Œç»†èƒæ ¸åˆ†å‰²ä»»åŠ¡çš„ç—…ç†å›¾åƒã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09121v2" onclick="toggleFavorite(this, '2510.09121v2', 'MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251009741v1-constructive-distortion-improving-mllms-with-attention-guided-image-.html">Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping</a></td>
  <td>æå‡ºAttWarpï¼Œåˆ©ç”¨æ³¨æ„åŠ›å¼•å¯¼å›¾åƒæ‰­æ›²æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09741v1" onclick="toggleFavorite(this, '2510.09741v1', 'Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251009302v1-capgeo-a-caption-assisted-approach-to-geometric-reasoning.html">CapGeo: A Caption-Assisted Approach to Geometric Reasoning</a></td>
  <td>CapGeoï¼šä¸€ç§åŸºäºå›¾æ–‡æè¿°çš„å‡ ä½•æ¨ç†æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09302v1" onclick="toggleFavorite(this, '2510.09302v1', 'CapGeo: A Caption-Assisted Approach to Geometric Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251008978v1-handeval-taking-the-first-step-towards-hand-quality-evaluation-in-ge.html">HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images</a></td>
  <td>æå‡ºHandEvalï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆå›¾åƒä¸­æ‰‹éƒ¨è´¨é‡ï¼Œæå‡AIGCåº”ç”¨æ•ˆæœã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08978v1" onclick="toggleFavorite(this, '2510.08978v1', 'HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251008976v1-hierarchical-scheduling-for-multi-vector-image-retrieval.html">Hierarchical Scheduling for Multi-Vector Image Retrieval</a></td>
  <td>HiMIRï¼šé¢å‘å¤šå‘é‡å›¾åƒæ£€ç´¢çš„åˆ†å±‚è°ƒåº¦æ¡†æ¶ï¼Œæå‡ç²¾åº¦å’Œæ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08976v1" onclick="toggleFavorite(this, '2510.08976v1', 'Hierarchical Scheduling for Multi-Vector Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251009867v1-cluster-aware-prompt-ensemble-learning-for-few-shot-vision-language-.html">Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation</a></td>
  <td>æå‡ºèšç±»æ„ŸçŸ¥çš„æç¤ºé›†æˆå­¦ä¹ æ¡†æ¶ï¼Œæå‡å°‘æ ·æœ¬è§†è§‰-è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09867v1" onclick="toggleFavorite(this, '2510.09867v1', 'Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251009008v1-on-epistemic-uncertainty-of-visual-tokens-for-object-hallucinations-.html">On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models</a></td>
  <td>é’ˆå¯¹å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹è±¡å¹»è§‰ï¼Œæå‡ºåŸºäºè§†è§‰tokenè®¤çŸ¥ä¸ç¡®å®šæ€§çš„ç¼“è§£æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09008v1" onclick="toggleFavorite(this, '2510.09008v1', 'On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251008936v1-ro-bench-large-scale-robustness-evaluation-of-mllms-with-text-driven.html">RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos</a></td>
  <td>æå‡ºRO-Benchï¼Œç”¨äºå¤§è§„æ¨¡è¯„ä¼°MLLMåœ¨æ–‡æœ¬é©±åŠ¨å¯¹æŠ—è§†é¢‘ä¸Šçš„é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08936v1" onclick="toggleFavorite(this, '2510.08936v1', 'RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251009285v1-spotlight-on-token-perception-for-multimodal-reinforcement-learning.html">Spotlight on Token Perception for Multimodal Reinforcement Learning</a></td>
  <td>æå‡ºVPPOï¼Œé€šè¿‡å…³æ³¨tokenæ„ŸçŸ¥ä¼˜åŒ–å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ï¼Œæå‡LVLMçš„æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09285v1" onclick="toggleFavorite(this, '2510.09285v1', 'Spotlight on Token Perception for Multimodal Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251009586v1-vision-language-models-a-survey-of-26k-papers.html">Vision Language Models: A Survey of 26K Papers</a></td>
  <td>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶è¶‹åŠ¿åˆ†æï¼šåŸºäº2.6ä¸‡ç¯‡è®ºæ–‡çš„ç»¼åˆè°ƒç ”</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09586v1" onclick="toggleFavorite(this, '2510.09586v1', 'Vision Language Models: A Survey of 26K Papers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251009367v1-minkowski-mambanet-a-point-cloud-framework-with-selective-state-spac.html">Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification</a></td>
  <td>æå‡ºMinkowski-MambaNetï¼Œåˆ©ç”¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹è¿›è¡Œæ£®æ—ç”Ÿç‰©é‡é‡åŒ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09367v1" onclick="toggleFavorite(this, '2510.09367v1', 'Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251008964v1-unleashing-perception-time-scaling-to-multimodal-reasoning-models.html">Unleashing Perception-Time Scaling to Multimodal Reasoning Models</a></td>
  <td>æå‡ºæ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´(PTS)ï¼Œæå‡å¤šæ¨¡æ€æ¨ç†æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08964v1" onclick="toggleFavorite(this, '2510.08964v1', 'Unleashing Perception-Time Scaling to Multimodal Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251009088v1-mambah-fit-rethinking-hyper-surface-fitting-based-point-cloud-normal.html">MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling</a></td>
  <td>æå‡ºMambaH-Fitï¼Œåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹æå‡ç‚¹äº‘æ³•å‘é‡ä¼°è®¡ç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09088v1" onclick="toggleFavorite(this, '2510.09088v1', 'MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251009299v1-foraging-with-the-eyes-dynamics-in-human-visual-gaze-and-deep-predic.html">Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling</a></td>
  <td>æ­ç¤ºäººç±»è§†è§‰æœå¯»æ¨¡å¼ï¼šåŸºäºçœ¼åŠ¨æ•°æ®çš„Levyè¡Œèµ°ä¸æ·±åº¦é¢„æµ‹æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09299v1" onclick="toggleFavorite(this, '2510.09299v1', 'Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251009903v1-an-uncertainty-aware-framework-for-data-efficient-multi-view-animal-.html">An uncertainty-aware framework for data-efficient multi-view animal pose estimation</a></td>
  <td>æå‡ºä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶ï¼Œé«˜æ•ˆè§£å†³æ•°æ®ç¨€ç¼ºä¸‹çš„å¤šè§†è§’åŠ¨ç‰©å§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09903v1" onclick="toggleFavorite(this, '2510.09903v1', 'An uncertainty-aware framework for data-efficient multi-view animal pose estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251009314v1-radioflow-efficient-radio-map-construction-framework-with-flow-match.html">RadioFlow: Efficient Radio Map Construction Framework with Flow Matching</a></td>
  <td>æå‡ºRadioFlowä»¥è§£å†³æ— çº¿ç”µå›¾ç”Ÿæˆæ•ˆç‡ä½çš„é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09314v1" onclick="toggleFavorite(this, '2510.09314v1', 'RadioFlow: Efficient Radio Map Construction Framework with Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251009171v1-instance-level-generation-for-representation-learning.html">Instance-Level Generation for Representation Learning</a></td>
  <td>æå‡ºä¸€ç§å®ä¾‹çº§åˆ«ç”Ÿæˆæ–¹æ³•ï¼Œæ— éœ€çœŸå®å›¾åƒå³å¯æå‡å®ä¾‹è¯†åˆ«è¡¨å¾å­¦ä¹ ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09171v1" onclick="toggleFavorite(this, '2510.09171v1', 'Instance-Level Generation for Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251008919v1-phyclip-ell-1-product-of-hyperbolic-factors-unifies-hierarchy-and-co.html">PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning</a></td>
  <td>æå‡ºPHyCLIPä»¥è§£å†³è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ä¸­çš„å±‚æ¬¡ä¸ç»„åˆæ€§é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08919v1" onclick="toggleFavorite(this, '2510.08919v1', 'PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251009364v1-visibility-aware-densification-for-3d-gaussian-splatting-in-dynamic-.html">Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes</a></td>
  <td>VAD-GSï¼šé¢å‘åŠ¨æ€åŸå¸‚åœºæ™¯ï¼ŒåŸºäºå¯è§æ€§æ¨ç†çš„3Dé«˜æ–¯æº…å°„ç¨ å¯†åŒ–æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09364v1" onclick="toggleFavorite(this, '2510.09364v1', 'Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251009320v1-hybrid-grained-feature-aggregation-with-coarse-to-fine-language-guid.html">Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation</a></td>
  <td>æå‡ºHybrid-depthæ¡†æ¶ï¼Œåˆ©ç”¨ç²—ç»†ç²’åº¦ç‰¹å¾èåˆå’Œè¯­è¨€å¼•å¯¼æå‡è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09320v1" onclick="toggleFavorite(this, '2510.09320v1', 'Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251009182v1-online-video-depth-anything-temporally-consistent-depth-prediction-w.html">Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption</a></td>
  <td>æå‡ºoVDAï¼Œé€šè¿‡ç¼“å­˜å’Œæ©ç æŠ€æœ¯å®ç°ä½å†…å­˜ã€åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09182v1" onclick="toggleFavorite(this, '2510.09182v1', 'Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251009110v3-synthetic-object-compositions-for-scalable-and-accurate-learning-in-.html">Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding</a></td>
  <td>æå‡ºSOCï¼šä¸€ç§å¯æ‰©å±•ã€ç²¾ç¡®çš„åˆæˆå¯¹è±¡ç»„åˆæ–¹æ³•ï¼Œç”¨äºæå‡æ£€æµ‹ã€åˆ†å‰²å’Œå®šä½ä»»åŠ¡æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09110v3" onclick="toggleFavorite(this, '2510.09110v3', 'Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251009881v1-ltgs-long-term-gaussian-scene-chronology-from-sparse-view-updates.html">LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates</a></td>
  <td>LTGSï¼šåŸºäºç¨€ç–è§†å›¾æ›´æ–°çš„é•¿æ—¶é«˜æ–¯åœºæ™¯æ—¶é—´çº¿å»ºæ¨¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09881v1" onclick="toggleFavorite(this, '2510.09881v1', 'LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251009880v1-geometry-aware-scene-configurations-for-novel-view-synthesis.html">Geometry-Aware Scene Configurations for Novel View Synthesis</a></td>
  <td>æå‡ºå‡ ä½•æ„ŸçŸ¥åœºæ™¯é…ç½®æ–¹æ³•ï¼Œæå‡å®¤å†…åœºæ™¯æ–°è§†è§’åˆæˆæ•ˆæœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09880v1" onclick="toggleFavorite(this, '2510.09880v1', 'Geometry-Aware Scene Configurations for Novel View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251009537v1-flowing-implicit-neural-flows-for-structure-preserving-morphing.html">FLOWING: Implicit Neural Flows for Structure-Preserving Morphing</a></td>
  <td>FLOWINGï¼šæå‡ºéšå¼ç¥ç»æµæ–¹æ³•ï¼Œå®ç°ç»“æ„ä¿æŒçš„å½¢å˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09537v1" onclick="toggleFavorite(this, '2510.09537v1', 'FLOWING: Implicit Neural Flows for Structure-Preserving Morphing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251009450v1-dynamic-weight-based-temporal-aggregation-for-low-light-video-enhanc.html">Dynamic Weight-based Temporal Aggregation for Low-light Video Enhancement</a></td>
  <td>æå‡ºDWTA-Netï¼Œé€šè¿‡åŠ¨æ€æƒé‡æ—¶åºèšåˆå¢å¼ºä½å…‰è§†é¢‘è´¨é‡ï¼Œæœ‰æ•ˆæŠ‘åˆ¶å™ªå£°ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09450v1" onclick="toggleFavorite(this, '2510.09450v1', 'Dynamic Weight-based Temporal Aggregation for Low-light Video Enhancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/251009607v2-vita-vla-efficiently-teaching-vision-language-models-to-act-via-acti.html">VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation</a></td>
  <td>æå‡ºVITA-VLAï¼Œé€šè¿‡åŠ¨ä½œä¸“å®¶è’¸é¦é«˜æ•ˆè®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ä»¥æ‰§è¡Œæœºå™¨äººåŠ¨ä½œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.09607v2" onclick="toggleFavorite(this, '2510.09607v2', 'VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)