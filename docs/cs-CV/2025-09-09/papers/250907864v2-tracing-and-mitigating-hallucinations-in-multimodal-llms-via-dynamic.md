---
layout: default
title: Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization
---

# Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07864" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07864v2</a>
  <a href="https://arxiv.org/pdf/2509.07864.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07864v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07864v2', 'Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tiancheng Yang, Lin Zhang, Jiaye Lin, Guimin Hu, Di Wang, Lijie Hu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09 (æ›´æ–°: 2025-11-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD-LEAFä»¥è§£å†³å¤šæ¨¡æ€LLMä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»è§‰æŠ‘åˆ¶` `åŠ¨æ€æ³¨æ„åŠ›` `å›¾åƒæè¿°` `è§†è§‰é—®ç­”` `æ¨¡å‹ä¼˜åŒ–` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å®¹æ˜“å‡ºç°å¹»è§‰ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ï¼Œå½±å“æ¨¡å‹çš„å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†åŠ¨æ€å±‚æ¬¡ç†µå’Œæ³¨æ„åŠ›èåˆï¼ˆD-LEAFï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å®šä½å’Œä¿®æ­£æ¨¡å‹ä¸­çš„é”™è¯¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒD-LEAFåœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šç›¸å¯¹æå‡53%ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­å‡†ç¡®ç‡å’ŒF1åˆ†æ•°å‡æé«˜çº¦4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»ç„¶å®¹æ˜“å‡ºç°å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆçš„æ–‡æœ¬ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†æ³¨æ„åŠ›è°ƒæ•´å‡åŒ€åº”ç”¨äºå„å±‚å’Œå¤´ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®å®šä½é”™è¯¯æ¥æºã€‚æœ¬æ–‡é¦–å…ˆæŒ‡å‡ºè¿™äº›æ–¹æ³•åœ¨å®šä½é—®é¢˜å±‚æ—¶çš„ä¸è¶³ï¼Œæ¥ç€å¼•å…¥äº†å±‚å›¾åƒæ³¨æ„åŠ›ç†µï¼ˆLIAEï¼‰å’Œå›¾åƒæ³¨æ„åŠ›èšç„¦ï¼ˆIAFï¼‰ä¸¤ä¸ªè¯Šæ–­å·¥å…·ï¼Œå‰è€…ç”¨äºæ ‡è®°å¼‚å¸¸å±‚ï¼Œåè€…ç”¨äºè¯„åˆ†è¿™äº›å±‚å†…çš„æ³¨æ„åŠ›å¤´ã€‚åŸºäºè¿™äº›ä¿¡å·ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€å±‚æ¬¡ç†µå’Œæ³¨æ„åŠ›èåˆï¼ˆD-LEAFï¼‰çš„æ–¹æ³•ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€å®šä½å’Œä¿®æ­£é”™è¯¯ï¼Œä¸”å¼€é”€æå°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD-LEAFåœ¨æ ‡å‡†æè¿°åŸºå‡†ä¸Šå®ç°äº†53%çš„ç›¸å¯¹æå‡ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­å‡†ç¡®ç‡å’ŒF1åˆ†æ•°å‡æé«˜çº¦4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå³å¹»è§‰ç°è±¡ã€‚ç°æœ‰æ–¹æ³•åœ¨è°ƒæ•´æ³¨æ„åŠ›æ—¶æœªèƒ½å‡†ç¡®å®šä½é—®é¢˜å±‚ï¼Œå¯¼è‡´é”™è¯¯éš¾ä»¥ä¿®æ­£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„D-LEAFæ–¹æ³•é€šè¿‡å¼•å…¥å±‚å›¾åƒæ³¨æ„åŠ›ç†µï¼ˆLIAEï¼‰å’Œå›¾åƒæ³¨æ„åŠ›èšç„¦ï¼ˆIAFï¼‰ä¸¤ä¸ªè¯Šæ–­å·¥å…·ï¼ŒåŠ¨æ€å®šä½å¹¶ä¿®æ­£æ¨¡å‹ä¸­çš„é”™è¯¯ï¼Œä»è€Œæé«˜ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šD-LEAFçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šLIAEç”¨äºè¯†åˆ«å¼‚å¸¸å±‚ï¼ŒIAFç”¨äºè¯„åˆ†æ³¨æ„åŠ›å¤´ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—çš„ååŒä½œç”¨ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è°ƒæ•´æ³¨æ„åŠ›åˆ†é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šD-LEAFçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶æ ¹æ®å®æ—¶åé¦ˆä¿®æ­£æ³¨æ„åŠ›åˆ†é…ï¼Œè€Œä¸æ˜¯é‡‡ç”¨é™æ€çš„å‡åŒ€è°ƒæ•´æ–¹å¼ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¾“å…¥æ—¶æ›´å…·çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨D-LEAFä¸­ï¼ŒLIAEå’ŒIAFçš„è®¡ç®—æ–¹æ³•ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œè¯„åˆ†æ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥å®ç°æœ€ä½³çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒD-LEAFåœ¨æ ‡å‡†å›¾åƒæè¿°åŸºå‡†ä¸Šå®ç°äº†53%çš„ç›¸å¯¹æå‡ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°å‡æé«˜çº¦4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒD-LEAFåœ¨æŠ‘åˆ¶å¹»è§‰ç°è±¡çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„é«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ŒD-LEAFèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜è´¨é‡çš„ç”Ÿæˆç»“æœï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Furthermore, by establishing a connection between D-LEAF and DPO, we provide theoretical justification for the effectiveness of D-LEAF. Results show our D-LEAF delivers a 53\% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4\%, substantially suppressing hallucinations while preserving efficiency.

