---
layout: default
title: SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning
---

# SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10555" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10555v1</a>
  <a href="https://arxiv.org/pdf/2509.10555.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10555v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10555v1', 'SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alejandra Perez, Chinedu Nwoye, Ramtin Raji Kermani, Omid Mohareri, Muhammad Abdullah Jamal

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SurgLaViï¼šæ„å»ºå¤§è§„æ¨¡æ‰‹æœ¯è§†è§‰-è¯­è¨€åˆ†å±‚æ•°æ®é›†ï¼Œç”¨äºæ‰‹æœ¯è§†è§‰-è¯­è¨€è¡¨å¾å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹æœ¯è§†é¢‘ç†è§£` `è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ` `å¯¹æ¯”å­¦ä¹ ` `åˆ†å±‚æ•°æ®é›†` `æ‰‹æœ¯æœºå™¨äºº` `åŒ»ç–—äººå·¥æ™ºèƒ½` `SurgCLIP`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹æœ¯è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ•°æ®é›†åœ¨è§„æ¨¡ã€ç¨‹åºå¤šæ ·æ€§ã€è¯­ä¹‰è´¨é‡å’Œåˆ†å±‚ç»“æ„æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. SurgLaVié€šè¿‡å…¨è‡ªåŠ¨æµç¨‹ç”Ÿæˆç»†ç²’åº¦æ‰‹æœ¯è§†é¢‘è½¬å½•ï¼Œå¹¶è¿›è¡ŒåŒæ¨¡æ€è¿‡æ»¤ï¼Œæ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€åˆ†å±‚ç»“æ„çš„è§†è§‰-è¯­è¨€æ•°æ®é›†ã€‚
3. SurgCLIPæ¨¡å‹åœ¨SurgLaViæ•°æ®é›†ä¸Šè®­ç»ƒåï¼Œåœ¨æ‰‹æœ¯é˜¶æ®µã€æ­¥éª¤ã€åŠ¨ä½œå’Œå·¥å…·è¯†åˆ«ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ(VLP)é€šè¿‡å°†è¯­è¨€ä¸æ‰‹æœ¯è§†é¢‘å¯¹é½ï¼Œä¸ºæ‰‹æœ¯æä¾›ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œæ— éœ€ä¾èµ–ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†å³å¯å®ç°å·¥ä½œæµç¨‹ç†è§£å’Œè·¨ä»»åŠ¡è¿ç§»ã€‚ç„¶è€Œï¼Œæ‰‹æœ¯VLPçš„è¿›å±•å—åˆ°ç°æœ‰æ•°æ®é›†çš„è§„æ¨¡ã€ç¨‹åºå¤šæ ·æ€§ã€è¯­ä¹‰è´¨é‡å’Œåˆ†å±‚ç»“æ„çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†SurgLaViï¼Œè¿„ä»Šä¸ºæ­¢æœ€å¤§ã€æœ€å¤šæ ·åŒ–çš„æ‰‹æœ¯è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª200å¤šä¸ªæ‰‹æœ¯çš„è¿‘24ä¸‡ä¸ªç‰‡æ®µ-å­—å¹•å¯¹ï¼Œå¹¶åŒ…å«é˜¶æ®µã€æ­¥éª¤å’Œä»»åŠ¡çº§åˆ«çš„åˆ†å±‚ç»“æ„ã€‚SurgLaViçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œå¯ä»¥ç³»ç»Ÿåœ°ç”Ÿæˆæ‰‹æœ¯è§†é¢‘çš„ç»†ç²’åº¦è½¬å½•ï¼Œå¹¶å°†å…¶åˆ†å‰²æˆè¿è´¯çš„ç¨‹åºå•å…ƒã€‚ä¸ºäº†ç¡®ä¿é«˜è´¨é‡çš„æ³¨é‡Šï¼Œå®ƒåº”ç”¨åŒæ¨¡æ€è¿‡æ»¤æ¥åˆ é™¤ä¸ç›¸å…³å’Œå˜ˆæ‚çš„æ ·æœ¬ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œç”Ÿæˆçš„å­—å¹•é€šè¿‡ä¸Šä¸‹æ–‡ç»†èŠ‚å¾—åˆ°ä¸°å¯Œï¼Œä»è€Œäº§ç”Ÿè¯­ä¹‰ä¸°å¯Œä¸”æ˜“äºè§£é‡Šçš„æ³¨é‡Šã€‚ä¸ºäº†ç¡®ä¿å¯è®¿é—®æ€§ï¼Œæˆ‘ä»¬å‘å¸ƒäº†SurgLaVi-{eta}ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç”±å…¬å…±æ•°æ®æ„å»ºçš„11.3ä¸‡ä¸ªç‰‡æ®µ-å­—å¹•å¯¹çš„å¼€æºè¡ç”Ÿç‰ˆæœ¬ï¼Œæ¯”ç°æœ‰çš„æ‰‹æœ¯VLPæ•°æ®é›†å¤§å››å€ä»¥ä¸Šã€‚ä¸ºäº†è¯æ˜SurgLaViæ•°æ®é›†çš„ä»·å€¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†SurgCLIPï¼Œä¸€ä¸ªå…·æœ‰åŒç¼–ç å™¨çš„CLIPé£æ ¼çš„è§†é¢‘-æ–‡æœ¬å¯¹æ¯”æ¡†æ¶ï¼Œä½œä¸ºä»£è¡¨æ€§çš„åŸºç¡€æ¨¡å‹ã€‚SurgCLIPåœ¨é˜¶æ®µã€æ­¥éª¤ã€åŠ¨ä½œå’Œå·¥å…·è¯†åˆ«æ–¹é¢å–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼Œè¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œé€šå¸¸å¹…åº¦å¾ˆå¤§ã€‚è¿™äº›ç»“æœéªŒè¯äº†å¤§è§„æ¨¡ã€è¯­ä¹‰ä¸°å¯Œå’Œåˆ†å±‚ç»“æ„çš„æ•°æ®é›†ç›´æ¥è½¬åŒ–ä¸ºæ›´å¼ºå¤§å’Œæ›´é€šç”¨çš„è¡¨ç¤ºï¼Œä»è€Œå°†SurgLaViç¡®ç«‹ä¸ºå¼€å‘æ‰‹æœ¯åŸºç¡€æ¨¡å‹çš„å…³é”®èµ„æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ‰‹æœ¯è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡å°ã€ç¨‹åºå¤šæ ·æ€§ä¸è¶³ã€è¯­ä¹‰è´¨é‡ä¸é«˜ï¼Œä¸”ç¼ºä¹åˆ†å±‚ç»“æ„ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥åº”ç”¨äºå¤æ‚çš„æ‰‹æœ¯ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€åˆ†å±‚ç»“æ„çš„æ‰‹æœ¯è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´é€šç”¨ã€æ›´å…·è¡¨è¾¾èƒ½åŠ›çš„æ‰‹æœ¯è§†é¢‘è¡¨å¾ã€‚é€šè¿‡å…¨è‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œé™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼Œæé«˜æ•°æ®è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSurgLaViçš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šä»å¤šä¸ªæ¥æºæ”¶é›†æ‰‹æœ¯è§†é¢‘æ•°æ®ã€‚2) è‡ªåŠ¨è½¬å½•ï¼šåˆ©ç”¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆæ‰‹æœ¯è§†é¢‘çš„æ–‡æœ¬è½¬å½•ã€‚3) åˆ†å‰²ï¼šå°†è§†é¢‘åˆ†å‰²æˆé˜¶æ®µã€æ­¥éª¤å’Œä»»åŠ¡ç­‰ä¸åŒå±‚æ¬¡çš„ç‰‡æ®µã€‚4) åŒæ¨¡æ€è¿‡æ»¤ï¼šåˆ©ç”¨è§†è§‰å’Œè¯­è¨€ä¿¡æ¯è¿‡æ»¤æ‰ä¸ç›¸å…³å’Œå™ªå£°æ•°æ®ã€‚5) æ•°æ®é›†å‘å¸ƒï¼šå‘å¸ƒSurgLaViæ•°æ®é›†åŠå…¶å¼€æºè¡ç”Ÿç‰ˆæœ¬SurgLaVi-{eta}ã€‚SurgCLIPæ¨¡å‹é‡‡ç”¨åŒç¼–ç å™¨ç»“æ„ï¼Œåˆ†åˆ«ç¼–ç è§†é¢‘å’Œæ–‡æœ¬ï¼Œç„¶åé€šè¿‡å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼š1) æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§ã€æœ€å¤šæ ·åŒ–çš„æ‰‹æœ¯è§†è§‰-è¯­è¨€æ•°æ®é›†SurgLaViã€‚2) æå‡ºäº†ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®æ ‡æ³¨æµç¨‹ï¼Œé™ä½äº†æ•°æ®æ ‡æ³¨æˆæœ¬ã€‚3) åˆ©ç”¨åŒæ¨¡æ€ä¿¡æ¯è¿›è¡Œæ•°æ®è¿‡æ»¤ï¼Œæé«˜äº†æ•°æ®è´¨é‡ã€‚4) å¼•å…¥äº†åˆ†å±‚ç»“æ„ï¼Œæ›´å¥½åœ°åæ˜ äº†æ‰‹æœ¯è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSurgCLIPæ¨¡å‹é‡‡ç”¨CLIPé£æ ¼çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨Transformerä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿ç”¨ResNetæˆ–ç±»ä¼¼ç»“æ„ä½œä¸ºè§†é¢‘ç¼–ç å™¨ã€‚å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°é‡‡ç”¨InfoNCEæŸå¤±ã€‚æ•°æ®é›†çš„åˆ’åˆ†æ–¹å¼å’Œè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿å®éªŒç»“æœçš„å¯é æ€§å’Œå¯æ¯”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SurgCLIPåœ¨SurgLaViæ•°æ®é›†ä¸Šè®­ç»ƒåï¼Œåœ¨æ‰‹æœ¯é˜¶æ®µã€æ­¥éª¤ã€åŠ¨ä½œå’Œå·¥å…·è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸé¡¹ä»»åŠ¡ä¸Šï¼ŒSurgCLIPçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€åˆ†å±‚ç»“æ„æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SurgLaViæ•°æ®é›†å’ŒSurgCLIPæ¨¡å‹å¯åº”ç”¨äºæ‰‹æœ¯æœºå™¨äººå¯¼èˆªã€æ‰‹æœ¯æŠ€èƒ½è¯„ä¼°ã€æ‰‹æœ¯æµç¨‹è‡ªåŠ¨åŒ–ã€æœ¯ååº·å¤æŒ‡å¯¼ç­‰é¢†åŸŸã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ‰‹æœ¯åŸºç¡€æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œæœ‰æœ›æ¨åŠ¨æ‰‹æœ¯æ™ºèƒ½åŒ–å‘å±•ï¼Œæé«˜æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œæ”¹å–„æ‚£è€…é¢„åã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language pre-training (VLP) offers unique advantages for surgery by aligning language with surgical videos, enabling workflow understanding and transfer across tasks without relying on expert-labeled datasets. However, progress in surgical VLP remains constrained by the limited scale, procedural diversity, semantic quality, and hierarchical structure of existing datasets. In this work, we present SurgLaVi, the largest and most diverse surgical vision-language dataset to date, comprising nearly 240k clip-caption pairs from more than 200 procedures, and comprising hierarchical levels at phase-, step-, and task-level. At the core of SurgLaVi lies a fully automated pipeline that systematically generates fine-grained transcriptions of surgical videos and segments them into coherent procedural units. To ensure high-quality annotations, it applies dual-modality filtering to remove irrelevant and noisy samples. Within this framework, the resulting captions are enriched with contextual detail, producing annotations that are both semantically rich and easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an open-source derivative of 113k clip-caption pairs constructed entirely from public data, which is over four times larger than existing surgical VLP datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP, a CLIP-style video-text contrastive framework with dual encoders, as a representative base model. SurgCLIP achieves consistent improvements across phase, step, action, and tool recognition, surpassing prior state-of-the-art methods, often by large margins. These results validate that large-scale, semantically rich, and hierarchically structured datasets directly translate into stronger and more generalizable representations, establishing SurgLaVi as a key resource for developing surgical foundation models.

