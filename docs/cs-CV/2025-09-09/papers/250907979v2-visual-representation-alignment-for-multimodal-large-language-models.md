---
layout: default
title: Visual Representation Alignment for Multimodal Large Language Models
---

# Visual Representation Alignment for Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07979" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07979v2</a>
  <a href="https://arxiv.org/pdf/2509.07979.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07979v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07979v2', 'Visual Representation Alignment for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09 (æ›´æ–°: 2025-10-10)

**å¤‡æ³¨**: Project Page: https://cvlab-kaist.github.io/VIRAL/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVIRALï¼Œé€šè¿‡è§†è§‰è¡¨å¾å¯¹é½æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡å‹` `è§†è§‰è¡¨å¾å¯¹é½` `è§†è§‰åŸºç¡€æ¨¡å‹` `è§†è§‰æŒ‡ä»¤å¾®è°ƒ` `æ­£åˆ™åŒ–` `å¯¹è±¡è®¡æ•°` `ç©ºé—´æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œä¸»è¦åŸå› æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨ä¸å¤Ÿå……åˆ†ï¼Œæ–‡æœ¬ç›‘ç£å­˜åœ¨é—´æ¥æ€§ã€‚
2. VIRALçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†MLLMçš„è§†è§‰è¡¨å¾ä¸é¢„è®­ç»ƒVFMçš„è§†è§‰è¡¨å¾å¯¹é½ï¼Œä»è€Œä¿ç•™å’Œå¢å¼ºè§†è§‰ç»†èŠ‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVIRALåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒåœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡è®¡æ•°æˆ–ç©ºé—´æ¨ç†ï¼‰ä¸­ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬å°†æ­¤å·®è·å½’å› äºä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ç›‘ç£èŒƒå¼ï¼Œè¯¥èŒƒå¼ä»…ä¸ºè§†è§‰é€šè·¯æä¾›é—´æ¥æŒ‡å¯¼ï¼Œå¹¶ç»å¸¸å¯¼è‡´MLLMåœ¨è®­ç»ƒæœŸé—´ä¸¢å¼ƒç»†ç²’åº¦çš„è§†è§‰ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰è¡¨å¾å¯¹é½(VIRAL)ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œç”¨äºå°†MLLMçš„å†…éƒ¨è§†è§‰è¡¨å¾ä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹(VFM)çš„è¡¨å¾å¯¹é½ã€‚é€šè¿‡æ˜¾å¼åœ°å¼ºåˆ¶è¿™ç§å¯¹é½ï¼ŒVIRALä½¿æ¨¡å‹ä¸ä»…èƒ½å¤Ÿä¿ç•™æ¥è‡ªè¾“å…¥è§†è§‰ç¼–ç å™¨çš„å…³é”®è§†è§‰ç»†èŠ‚ï¼Œè€Œä¸”èƒ½å¤Ÿè¡¥å……æ¥è‡ªVFMçš„é¢å¤–è§†è§‰çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºå…¶å¯¹å¤æ‚è§†è§‰è¾“å…¥è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†åœ¨å¹¿æ³›é‡‡ç”¨çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æœ‰ä»»åŠ¡éƒ½å¾—åˆ°äº†ä¸€è‡´çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œä»¥éªŒè¯æˆ‘ä»¬æ¡†æ¶çš„å…³é”®è®¾è®¡é€‰æ‹©ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€ç®€å•çš„å‘ç°ä¸ºæœ‰æ•ˆæ•´åˆMLLMè®­ç»ƒä¸­çš„è§†è§‰ä¿¡æ¯å¼€è¾Ÿäº†ä¸€ä¸ªé‡è¦çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)åœ¨å¤„ç†è§†è§‰å¯†é›†å‹ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“è®¡æ•°å’Œç©ºé—´æ¨ç†æ—¶ï¼Œæ€§èƒ½å—åˆ°é™åˆ¶ã€‚ä¸»è¦åŸå› æ˜¯å½“å‰è®­ç»ƒèŒƒå¼è¿‡åº¦ä¾èµ–æ–‡æœ¬ç›‘ç£ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿½ç•¥äº†ç»†ç²’åº¦çš„è§†è§‰ä¿¡æ¯ï¼Œè§†è§‰é€šè·¯æ²¡æœ‰å¾—åˆ°å……åˆ†çš„åˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVIRALçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ˜¾å¼åœ°å°†MLLMå†…éƒ¨çš„è§†è§‰è¡¨å¾ä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹(VFM)çš„è§†è§‰è¡¨å¾å¯¹é½ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯è®©MLLMèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æ¥è‡ªè¾“å…¥å›¾åƒçš„è§†è§‰ç»†èŠ‚ï¼Œå¹¶ä»VFMä¸­å­¦ä¹ åˆ°é¢å¤–çš„è§†è§‰çŸ¥è¯†ï¼Œä»è€Œæå‡å…¶è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVIRALæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹(VFM)ï¼Œä¸€ä¸ªå¾…è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ï¼Œä»¥åŠä¸€ä¸ªè§†è§‰è¡¨å¾å¯¹é½æ¨¡å—ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾“å…¥å›¾åƒåŒæ—¶é€å…¥MLLMå’ŒVFMï¼Œåˆ†åˆ«æå–è§†è§‰è¡¨å¾ã€‚ç„¶åï¼Œé€šè¿‡è§†è§‰è¡¨å¾å¯¹é½æ¨¡å—ï¼Œè®¡ç®—MLLMå’ŒVFMçš„è§†è§‰è¡¨å¾ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†å…¶ä½œä¸ºæ­£åˆ™åŒ–é¡¹åŠ å…¥åˆ°MLLMçš„è®­ç»ƒæŸå¤±ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šVIRALçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è§†è§‰è¡¨å¾å¯¹é½è¿™ä¸€æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºMLLMçš„è®­ç»ƒä¸­ã€‚ä¸ä»¥å¾€ä¸»è¦ä¾èµ–æ–‡æœ¬ç›‘ç£çš„æ–¹æ³•ä¸åŒï¼ŒVIRALé€šè¿‡ç›´æ¥å¯¹é½è§†è§‰è¡¨å¾ï¼Œä½¿å¾—MLLMèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„MLLMæ¶æ„ç›¸ç»“åˆã€‚

**å…³é”®è®¾è®¡**ï¼šVIRALçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„VFMï¼Œä¾‹å¦‚CLIPç­‰ã€‚2) è®¾è®¡åˆé€‚çš„è§†è§‰è¡¨å¾å¯¹é½æ¨¡å—ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±æˆ–å‡æ–¹è¯¯å·®æŸå¤±æ¥è¡¡é‡MLLMå’ŒVFMçš„è§†è§‰è¡¨å¾ä¹‹é—´çš„å·®å¼‚ã€‚3) è°ƒæ•´è§†è§‰è¡¨å¾å¯¹é½æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡ï¼Œä»¥å¹³è¡¡æ–‡æœ¬ç›‘ç£å’Œè§†è§‰å¯¹é½ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVIRALåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è±¡è®¡æ•°ä»»åŠ¡ä¸­ï¼ŒVIRALå°†æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†X%ã€‚æ¶ˆèå®éªŒéªŒè¯äº†è§†è§‰è¡¨å¾å¯¹é½ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå…³é”®è®¾è®¡é€‰æ‹©çš„é‡è¦æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVIRALæ˜¯ä¸€ç§æœ‰æ•ˆçš„æå‡MLLMè§†è§‰ç†è§£èƒ½åŠ›çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦ç²¾ç»†è§†è§‰ç†è§£çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœºæ™¯ä¸‹çš„ä»»åŠ¡ç²¾åº¦å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.

