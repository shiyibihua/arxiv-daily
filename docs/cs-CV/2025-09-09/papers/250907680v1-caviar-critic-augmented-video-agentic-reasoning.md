---
layout: default
title: CAViAR: Critic-Augmented Video Agentic Reasoning
---

# CAViAR: Critic-Augmented Video Agentic Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07680" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07680v1</a>
  <a href="https://arxiv.org/pdf/2509.07680.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07680v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07680v1', 'CAViAR: Critic-Augmented Video Agentic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CAViARï¼šåŸºäºè¯„è®ºå¢å¼ºçš„è§†é¢‘Agentæ¨ç†ï¼Œæå‡å¤æ‚è§†é¢‘ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `Agentæ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„è®ºå®¶æœºåˆ¶` `åŠ¨æ€è§„åˆ’` `é•¿è§†é¢‘åˆ†æ` `å¤æ‚æ¨ç†` `è§†é¢‘æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£æ¨¡å‹åœ¨é•¿è§†é¢‘å’Œå¤æ‚æŸ¥è¯¢ä¸‹ï¼Œæ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ä»»åŠ¡ã€‚
2. æå‡ºCAViARï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹Agentï¼Œç»“åˆè§†é¢‘æ¨¡å—å·¥å…·ï¼ŒåŠ¨æ€è§„åˆ’æ¨ç†æ­¥éª¤ã€‚
3. å¼•å…¥è¯„è®ºå®¶æœºåˆ¶ï¼ŒåŒºåˆ†AgentæˆåŠŸå’Œå¤±è´¥çš„æ¨ç†åºåˆ—ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†é¢‘ç†è§£é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¨¡å‹åœ¨çŸ­è§†é¢‘ç‰‡æ®µæ„ŸçŸ¥æ–¹é¢çš„æ€§èƒ½ä¸æ–­æé«˜ã€‚ç„¶è€Œï¼ŒLVBenchã€Neptuneå’ŒActivityNet-RTLç­‰å¤šä¸ªæœ€æ–°åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå½“æŸ¥è¯¢å˜å¾—æ›´å¤æ‚ä¸”è§†é¢‘å˜å¾—æ›´é•¿æ—¶ï¼Œéœ€è¦å¯¹è§†é¢‘è¿›è¡Œå¤æ‚æ¨ç†çš„ä»»åŠ¡çš„æ€§èƒ½ä¼šä¸‹é™ã€‚æœ¬æ–‡æ¢è®¨äº†ï¼šç°æœ‰çš„æ„ŸçŸ¥èƒ½åŠ›æ˜¯å¦å¯ä»¥è¢«åˆ©ç”¨æ¥æˆåŠŸåœ°æ‰§è¡Œæ›´å¤æ‚çš„è§†é¢‘æ¨ç†ï¼Ÿå…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹Agentï¼Œå®ƒå¯ä»¥è®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­Agentæˆ–å·¥å…·ã€‚ä¸Visual Programmingã€ViperGPTå’ŒMoReVQAç­‰å…ˆå‰å·¥ä½œä¸­éµå¾ªå›ºå®šç¨‹åºæ¥è§£å†³æŸ¥è¯¢ä¸åŒï¼Œè¯¥Agentä½¿ç”¨æ¯æ¬¡è°ƒç”¨æ¨¡å—çš„ç»“æœæ¥ç¡®å®šåç»­æ­¥éª¤ã€‚å—åˆ°æ–‡æœ¬æ¨ç†é¢†åŸŸå·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„è®ºå®¶æ¥åŒºåˆ†AgentæˆåŠŸå’Œä¸æˆåŠŸåºåˆ—çš„å®ä¾‹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„Agentå’Œè¯„è®ºå®¶çš„ç»“åˆåœ¨å‰é¢æåˆ°çš„æ•°æ®é›†ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç†è§£æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘å’Œå¤æ‚æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡åœ¨çŸ­è§†é¢‘ç‰‡æ®µæ„ŸçŸ¥æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚éœ€è¦ç†è§£è§†é¢‘ä¸­å¤šä¸ªäº‹ä»¶ä¹‹é—´çš„å…³ç³»æˆ–å›ç­”å¤æ‚é—®é¢˜æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„ç¨‹åºæˆ–è§†è§‰ç¼–ç¨‹æ–¹å¼ï¼Œç¼ºä¹çµæ´»æ€§ï¼Œæ— æ³•æ ¹æ®è§†é¢‘å†…å®¹åŠ¨æ€è°ƒæ•´æ¨ç†è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAViARçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºAgentï¼Œèµ‹äºˆå…¶è°ƒç”¨å„ç§è§†é¢‘æ¨¡å—ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€åŠ¨ä½œè¯†åˆ«ç­‰ï¼‰çš„èƒ½åŠ›ï¼Œå¹¶æ ¹æ®æ¨¡å—çš„è¾“å‡ºåŠ¨æ€è§„åˆ’æ¨ç†æ­¥éª¤ã€‚Agenté€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œé€æ­¥è§£å†³å¤æ‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¼•å…¥ä¸€ä¸ªè¯„è®ºå®¶ï¼ˆCriticï¼‰æ¥è¯„ä¼°Agentçš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒºåˆ†æˆåŠŸå’Œå¤±è´¥çš„åºåˆ—ï¼Œä»è€ŒæŒ‡å¯¼Agentçš„å­¦ä¹ å’Œæ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAViARçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **LLM Agent**ï¼šä½œä¸ºæ ¸å¿ƒæ§åˆ¶å™¨ï¼Œè´Ÿè´£æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢ï¼Œè§„åˆ’æ¨ç†æ­¥éª¤ï¼Œå¹¶è°ƒç”¨ç›¸åº”çš„è§†é¢‘æ¨¡å—ã€‚2) **è§†é¢‘æ¨¡å—**ï¼šæä¾›å„ç§è§†é¢‘åˆ†æèƒ½åŠ›ï¼Œå¦‚ç›®æ ‡æ£€æµ‹ã€åŠ¨ä½œè¯†åˆ«ã€åœºæ™¯ç†è§£ç­‰ã€‚è¿™äº›æ¨¡å—è¢«è§†ä¸ºAgentçš„å·¥å…·ã€‚3) **è¯„è®ºå®¶**ï¼šè¯„ä¼°Agentçš„æ¨ç†è¿‡ç¨‹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦æˆåŠŸã€‚è¯„è®ºå®¶å¯ä»¥åŸºäºé¢„å®šä¹‰çš„è§„åˆ™æˆ–å­¦ä¹ åˆ°çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚4) **ç¯å¢ƒ**ï¼šåŒ…å«è§†é¢‘æ•°æ®å’Œç”¨æˆ·æŸ¥è¯¢ã€‚Agenté€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œè·å–ä¿¡æ¯å¹¶æ‰§è¡Œæ¨ç†ã€‚æ¨ç†æµç¨‹æ˜¯å¾ªç¯è¿­ä»£çš„ï¼ŒAgentæ ¹æ®å½“å‰çŠ¶æ€å’Œè¯„è®ºå®¶çš„åé¦ˆï¼Œå†³å®šä¸‹ä¸€æ­¥è¦è°ƒç”¨çš„æ¨¡å—å’Œæ‰§è¡Œçš„åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šCAViARçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åŠ¨æ€æ¨ç†è§„åˆ’**ï¼šAgentå¯ä»¥æ ¹æ®è§†é¢‘å†…å®¹å’ŒæŸ¥è¯¢åŠ¨æ€è°ƒæ•´æ¨ç†æ­¥éª¤ï¼Œè€Œä¸æ˜¯é‡‡ç”¨å›ºå®šçš„ç¨‹åºã€‚2) **è¯„è®ºå®¶æœºåˆ¶**ï¼šé€šè¿‡å¼•å…¥è¯„è®ºå®¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°Agentçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æŒ‡å¯¼å…¶å­¦ä¹ å’Œæ”¹è¿›ã€‚3) **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†è§†é¢‘åˆ†æèƒ½åŠ›å°è£…æˆç‹¬ç«‹çš„æ¨¡å—ï¼Œæ–¹ä¾¿Agentè°ƒç”¨å’Œç»„åˆï¼Œæé«˜äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒCAViARæ›´åŠ æ³¨é‡Agentçš„è‡ªä¸»æ€§å’Œæ¨ç†è¿‡ç¨‹çš„ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šAgentä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”è§†é¢‘æ¨ç†ä»»åŠ¡ã€‚è¯„è®ºå®¶å¯ä»¥ä½¿ç”¨äºŒå…ƒåˆ†ç±»å™¨ï¼Œåˆ¤æ–­Agentçš„æ¨ç†åºåˆ—æ˜¯å¦æˆåŠŸã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚ç­–ç•¥æ¢¯åº¦æˆ–Q-learningï¼Œæ¥ä¼˜åŒ–Agentçš„ç­–ç•¥ã€‚æŸå¤±å‡½æ•°å¯ä»¥åŒ…æ‹¬å¥–åŠ±å‡½æ•°ï¼ˆåŸºäºè¯„è®ºå®¶çš„è¯„ä¼°ï¼‰å’Œæ­£åˆ™åŒ–é¡¹ï¼ˆä¾‹å¦‚é¼“åŠ±Agentè°ƒç”¨æ›´å°‘çš„æ¨¡å—ï¼‰ã€‚å…·ä½“å‚æ•°è®¾ç½®å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œéœ€è¦è¿›è¡Œå®éªŒè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CAViARåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ActivityNet-RTLæ•°æ®é›†ä¸Šï¼ŒCAViARçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œè¡¨æ˜å…¶åœ¨å¤æ‚è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥è¯„è®ºå®¶æœºåˆ¶ï¼ŒCAViARèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å’Œæ”¹è¿›æ¨ç†ç­–ç•¥ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAViARèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ç°æœ‰çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œè§£å†³é•¿è§†é¢‘å’Œå¤æ‚æŸ¥è¯¢å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CAViARå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è§†é¢‘æœç´¢ã€è§†é¢‘æ‘˜è¦ã€æ™ºèƒ½å®¢æœç­‰ã€‚å®ƒå¯ä»¥ç”¨äºåˆ†æç›‘æ§è§†é¢‘ä¸­çš„å¼‚å¸¸è¡Œä¸ºï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿæ‰¾åˆ°æ„Ÿå…´è¶£çš„è§†é¢‘ç‰‡æ®µï¼Œè‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ‘˜è¦ï¼Œä»¥åŠå›ç­”ç”¨æˆ·å…³äºè§†é¢‘å†…å®¹çš„å¤æ‚é—®é¢˜ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæå‡äº†è§†é¢‘ç†è§£çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä¸ºå„ç§è§†é¢‘åº”ç”¨æä¾›äº†æ›´å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒã€‚æœªæ¥ï¼ŒCAViARå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.

