---
layout: default
title: EHWGesture -- A dataset for multimodal understanding of clinical gestures
---

# EHWGesture -- A dataset for multimodal understanding of clinical gestures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07525" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07525v1</a>
  <a href="https://arxiv.org/pdf/2509.07525.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07525v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07525v1', 'EHWGesture -- A dataset for multimodal understanding of clinical gestures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gianluca Amprimo, Alberto Ancilotto, Alessandro Savino, Fabio Quazzolo, Claudia Ferraris, Gabriella Olmo, Elisabetta Farella, Stefano Di Carlo

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09

**å¤‡æ³¨**: Accepted at ICCV 2025 Workshop on AI-driven Skilled Activity Understanding, Assessment & Feedback Generation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EHWGestureï¼šç”¨äºä¸´åºŠæ‰‹åŠ¿å¤šæ¨¡æ€ç†è§£çš„æ•°æ®é›†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸´åºŠæ‰‹åŠ¿ç†è§£` `å¤šæ¨¡æ€æ•°æ®é›†` `æ‰‹éƒ¨å…³é”®ç‚¹è¿½è¸ª` `åŠ¨ä½œè´¨é‡è¯„ä¼°` `RGB-Dç›¸æœº` `äº‹ä»¶ç›¸æœº` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ¨æ€æ‰‹åŠ¿ç†è§£å› å…¶å¤æ‚çš„æ—¶ç©ºå˜åŒ–è€Œå……æ»¡æŒ‘æˆ˜ï¼Œç°æœ‰æ•°æ®é›†ç¼ºä¹å¤šæ¨¡æ€ã€å¤šè§†è§’æ•°æ®ä»¥åŠç²¾ç¡®çš„çœŸå€¼è¿½è¸ªã€‚
2. EHWGestureæ•°æ®é›†åˆ©ç”¨RGB-Dç›¸æœºã€äº‹ä»¶ç›¸æœºå’Œè¿åŠ¨æ•æ‰ç³»ç»Ÿï¼Œæä¾›å¤šæ¨¡æ€æ•°æ®å’Œç²¾ç¡®çš„æ‰‹éƒ¨å…³é”®ç‚¹çœŸå€¼ï¼Œå¹¶åŒ…å«åŠ¨ä½œè´¨é‡è¯„ä¼°ã€‚
3. åŸºçº¿å®éªŒè¡¨æ˜ï¼ŒEHWGestureæ•°æ®é›†åœ¨æ‰‹åŠ¿åˆ†ç±»ã€æ‰‹åŠ¿è§¦å‘æ£€æµ‹å’ŒåŠ¨ä½œè´¨é‡è¯„ä¼°æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯ä½œä¸ºä¸´åºŠæ‰‹åŠ¿ç†è§£çš„åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†EHWGestureï¼Œä¸€ä¸ªç”¨äºæ‰‹åŠ¿ç†è§£çš„å¤šæ¨¡æ€è§†é¢‘æ•°æ®é›†ï¼Œä¸“æ³¨äºäº”ä¸ªä¸´åºŠç›¸å…³çš„æ‰‹åŠ¿ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡1100ä¸ªå½•åˆ¶è§†é¢‘ï¼ˆ6å°æ—¶ï¼‰ï¼Œç”±25åå¥åº·å—è¯•è€…ä½¿ç”¨ä¸¤ä¸ªé«˜åˆ†è¾¨ç‡RGB-Depthç›¸æœºå’Œä¸€ä¸ªäº‹ä»¶ç›¸æœºé‡‡é›†ã€‚åŒæ—¶ï¼Œåˆ©ç”¨è¿åŠ¨æ•æ‰ç³»ç»Ÿæä¾›ç²¾ç¡®çš„æ‰‹éƒ¨å…³é”®ç‚¹è¿½è¸ªçœŸå€¼ã€‚æ‰€æœ‰è®¾å¤‡ç»è¿‡ç©ºé—´æ ¡å‡†å’ŒåŒæ­¥ï¼Œç¡®ä¿è·¨æ¨¡æ€å¯¹é½ã€‚ä¸ºäº†å°†åŠ¨ä½œè´¨é‡è¯„ä¼°èå…¥æ‰‹åŠ¿ç†è§£ï¼Œå½•åˆ¶è§†é¢‘æŒ‰ç…§æ‰§è¡Œé€Ÿåº¦åˆ†ç±»ï¼Œæ¨¡æ‹Ÿä¸´åºŠæ‰‹éƒ¨çµå·§æ€§è¯„ä¼°ã€‚åŸºçº¿å®éªŒéªŒè¯äº†è¯¥æ•°æ®é›†åœ¨æ‰‹åŠ¿åˆ†ç±»ã€æ‰‹åŠ¿è§¦å‘æ£€æµ‹å’ŒåŠ¨ä½œè´¨é‡è¯„ä¼°æ–¹é¢çš„æ½œåŠ›ã€‚EHWGestureæœ‰æœ›æˆä¸ºæ¨è¿›å¤šæ¨¡æ€ä¸´åºŠæ‰‹åŠ¿ç†è§£çš„ç»¼åˆåŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ‰‹åŠ¿ç†è§£æ•°æ®é›†åœ¨ä¸´åºŠåº”ç”¨æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€æ‰‹åŠ¿ç†è§£ä¸­ï¼Œç¼ºä¹å¤šæ¨¡æ€ä¿¡æ¯ã€å¤šè§†è§’æ•°æ®ä»¥åŠç²¾ç¡®çš„åŠ¨ä½œè´¨é‡è¯„ä¼°ã€‚è¿™é™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¸´åºŠæ‰‹éƒ¨çµå·§æ€§è‡ªåŠ¨è¯„ä¼°ç­‰æ–¹é¢çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰æ‰‹åŠ¿çš„æ—¶ç©ºå˜åŒ–å’Œç»†å¾®çš„åŠ¨ä½œè´¨é‡å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«å¤šæ¨¡æ€ä¿¡æ¯ã€ç²¾ç¡®çœŸå€¼å’ŒåŠ¨ä½œè´¨é‡æ ‡ç­¾çš„ä¸´åºŠæ‰‹åŠ¿æ•°æ®é›†ï¼Œä»è€Œä¸ºè®­ç»ƒå’Œè¯„ä¼°æ›´å¼ºå¤§çš„æ‰‹åŠ¿ç†è§£æ¨¡å‹æä¾›åŸºç¡€ã€‚é€šè¿‡ç»“åˆRGB-Dç›¸æœºã€äº‹ä»¶ç›¸æœºå’Œè¿åŠ¨æ•æ‰ç³»ç»Ÿï¼Œå…¨é¢æ•æ‰æ‰‹åŠ¿çš„è§†è§‰ä¿¡æ¯å’Œè¿åŠ¨ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEHWGestureæ•°æ®é›†çš„æ„å»ºæµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®é‡‡é›†ï¼šä½¿ç”¨RGB-Dç›¸æœºã€äº‹ä»¶ç›¸æœºå’Œè¿åŠ¨æ•æ‰ç³»ç»ŸåŒæ­¥è®°å½•25åå—è¯•è€…æ‰§è¡Œäº”ä¸ªä¸´åºŠç›¸å…³æ‰‹åŠ¿çš„è§†é¢‘ã€‚2) æ•°æ®æ ¡å‡†ï¼šå¯¹æ‰€æœ‰è®¾å¤‡è¿›è¡Œç©ºé—´æ ¡å‡†å’Œæ—¶é—´åŒæ­¥ï¼Œç¡®ä¿è·¨æ¨¡æ€æ•°æ®å¯¹é½ã€‚3) çœŸå€¼æ ‡æ³¨ï¼šåˆ©ç”¨è¿åŠ¨æ•æ‰ç³»ç»Ÿæä¾›ç²¾ç¡®çš„æ‰‹éƒ¨å…³é”®ç‚¹è¿½è¸ªçœŸå€¼ã€‚4) åŠ¨ä½œè´¨é‡åˆ†ç±»ï¼šæ ¹æ®æ‰§è¡Œé€Ÿåº¦å°†å½•åˆ¶è§†é¢‘åˆ†ä¸ºä¸åŒç±»åˆ«ï¼Œæ¨¡æ‹Ÿä¸´åºŠæ‰‹éƒ¨çµå·§æ€§è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šEHWGestureæ•°æ®é›†çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€æ€§ã€ç²¾ç¡®çœŸå€¼å’ŒåŠ¨ä½œè´¨é‡è¯„ä¼°çš„ç»“åˆã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒEHWGestureæä¾›äº†æ›´å…¨é¢çš„æ‰‹åŠ¿ä¿¡æ¯ï¼ŒåŒ…æ‹¬RGB-Då›¾åƒã€äº‹ä»¶æ•°æ®å’Œç²¾ç¡®çš„æ‰‹éƒ¨å…³é”®ç‚¹ä½ç½®ã€‚æ­¤å¤–ï¼ŒåŠ¨ä½œè´¨é‡åˆ†ç±»çš„å¼•å…¥ä½¿å¾—è¯¥æ•°æ®é›†èƒ½å¤Ÿç”¨äºè®­ç»ƒå’Œè¯„ä¼°èƒ½å¤Ÿç†è§£æ‰‹åŠ¿åŠ¨ä½œè´¨é‡çš„æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªé«˜åˆ†è¾¨ç‡RGB-Dç›¸æœºå’Œä¸€ä¸ªäº‹ä»¶ç›¸æœºï¼Œä»¥æ•æ‰ä¸åŒè§†è§’çš„è§†è§‰ä¿¡æ¯ã€‚è¿åŠ¨æ•æ‰ç³»ç»Ÿç”¨äºæä¾›ç²¾ç¡®çš„æ‰‹éƒ¨å…³é”®ç‚¹è¿½è¸ªçœŸå€¼ï¼Œç¡®ä¿æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚æ•°æ®é›†ä¸­çš„æ‰‹åŠ¿æŒ‰ç…§æ‰§è¡Œé€Ÿåº¦åˆ†ä¸ºä¸åŒç±»åˆ«ï¼Œæ¨¡æ‹Ÿä¸´åºŠæ‰‹éƒ¨çµå·§æ€§è¯„ä¼°ï¼Œä»è€Œå¯ä»¥è®­ç»ƒæ¨¡å‹æ¥è¯„ä¼°åŠ¨ä½œè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EHWGestureæ•°æ®é›†åŒ…å«è¶…è¿‡1100ä¸ªå½•åˆ¶è§†é¢‘ï¼Œæ€»æ—¶é•¿è¾¾6å°æ—¶ï¼Œæ¶µç›–äº”ä¸ªä¸´åºŠç›¸å…³çš„æ‰‹åŠ¿ã€‚åŸºçº¿å®éªŒè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†åœ¨æ‰‹åŠ¿åˆ†ç±»ã€æ‰‹åŠ¿è§¦å‘æ£€æµ‹å’ŒåŠ¨ä½œè´¨é‡è¯„ä¼°æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰‹åŠ¿åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒåŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†EHWGestureæ•°æ®é›†ä½œä¸ºä¸´åºŠæ‰‹åŠ¿ç†è§£åŸºå‡†çš„ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EHWGestureæ•°æ®é›†å¯å¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’ã€ä¸´åºŠè¯„ä¼°å’Œåº·å¤è®­ç»ƒç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ç”¨äºå¼€å‘è‡ªåŠ¨åŒ–çš„æ‰‹éƒ¨çµå·§æ€§è¯„ä¼°ç³»ç»Ÿï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ç”¨äºå¼€å‘åŸºäºæ‰‹åŠ¿çš„äº¤äº’ç•Œé¢ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ•°æ®é›†æœ‰æœ›æ¨åŠ¨ä¸´åºŠæ‰‹åŠ¿ç†è§£æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºåŒ»ç–—å¥åº·é¢†åŸŸå¸¦æ¥æ›´å¤šåˆ›æ–°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.

