---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-09
---

# cs.CVï¼ˆ2025-09-09ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250907979v2-visual-representation-alignment-for-multimodal-large-language-models.html">Visual Representation Alignment for Multimodal Large Language Models</a></td>
  <td>æå‡ºVIRALï¼Œé€šè¿‡è§†è§‰è¡¨å¾å¯¹é½æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07979v2" data-paper-url="./papers/250907979v2-visual-representation-alignment-for-multimodal-large-language-models.html" onclick="toggleFavorite(this, '2509.07979v2', 'Visual Representation Alignment for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250908024v1-two-stage-context-learning-with-large-language-models-for-multimodal.html">Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change</a></td>
  <td>æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åŒé˜¶æ®µä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ°”å€™å˜åŒ–å¤šæ¨¡æ€ç«‹åœºæ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08024v1" data-paper-url="./papers/250908024v1-two-stage-context-learning-with-large-language-models-for-multimodal.html" onclick="toggleFavorite(this, '2509.08024v1', 'Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250907450v2-gleam-learning-to-match-and-explain-in-cross-view-geo-localization.html">GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</a></td>
  <td>GLEAMï¼šæå‡ºä¸€ç§å¤šè§†è§’åœ°ç†å®šä½æ¡†æ¶ï¼ŒèåˆåŒ¹é…ä¸å¯è§£é‡Šæ¨ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07450v2" data-paper-url="./papers/250907450v2-gleam-learning-to-match-and-explain-in-cross-view-geo-localization.html" onclick="toggleFavorite(this, '2509.07450v2', 'GLEAM: Learning to Match and Explain in Cross-View Geo-Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250907680v1-caviar-critic-augmented-video-agentic-reasoning.html">CAViAR: Critic-Augmented Video Agentic Reasoning</a></td>
  <td>CAViARï¼šåŸºäºè¯„è®ºå¢å¼ºçš„è§†é¢‘Agentæ¨ç†ï¼Œæå‡å¤æ‚è§†é¢‘ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07680v1" data-paper-url="./papers/250907680v1-caviar-critic-augmented-video-agentic-reasoning.html" onclick="toggleFavorite(this, '2509.07680v1', 'CAViAR: Critic-Augmented Video Agentic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250907966v1-visual-tableqa-open-domain-benchmark-for-reasoning-over-table-images.html">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></td>
  <td>æå‡ºVisual-TableQAï¼Œç”¨äºè¯„ä¼°å’Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼å›¾åƒä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07966v1" data-paper-url="./papers/250907966v1-visual-tableqa-open-domain-benchmark-for-reasoning-over-table-images.html" onclick="toggleFavorite(this, '2509.07966v1', 'Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250907825v1-point-linguist-model-segment-any-object-via-bridged-large-3d-languag.html">Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</a></td>
  <td>æå‡ºPoint Linguist Modelï¼Œé€šè¿‡æ¡¥æ¥3D-è¯­è¨€å¤§æ¨¡å‹å®ç°ä»»æ„ç‰©ä½“åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07825v1" data-paper-url="./papers/250907825v1-point-linguist-model-segment-any-object-via-bridged-large-3d-languag.html" onclick="toggleFavorite(this, '2509.07825v1', 'Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250907772v1-xsrd-net-explainable-stroke-relapse-detection.html">XSRD-Net: EXplainable Stroke Relapse Detection</a></td>
  <td>XSRD-Netï¼šç”¨äºå¯è§£é‡Šçš„ä¸­é£å¤å‘æ£€æµ‹ï¼ŒåŠ©åŠ›æ—©æœŸæ²»ç–—è§„åˆ’</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07772v1" data-paper-url="./papers/250907772v1-xsrd-net-explainable-stroke-relapse-detection.html" onclick="toggleFavorite(this, '2509.07772v1', 'XSRD-Net: EXplainable Stroke Relapse Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250907596v2-bias-in-gender-bias-benchmarks-how-spurious-features-distort-evaluat.html">Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</a></td>
  <td>æ­ç¤ºæ€§åˆ«åè§åŸºå‡†æµ‹è¯•ä¸­çš„è™šå‡ç‰¹å¾é—®é¢˜ï¼Œå¹¶æå‡ºæ›´å¯é çš„è¯„ä¼°æ–¹æ³•ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07596v2" data-paper-url="./papers/250907596v2-bias-in-gender-bias-benchmarks-how-spurious-features-distort-evaluat.html" onclick="toggleFavorite(this, '2509.07596v2', 'Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250907774v1-hairgs-hair-strand-reconstruction-based-on-3d-gaussian-splatting.html">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a></td>
  <td>HairGSï¼šåŸºäº3Dé«˜æ–¯æº…å°„çš„å¤´å‘ä¸é‡å»ºæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07774v1" data-paper-url="./papers/250907774v1-hairgs-hair-strand-reconstruction-based-on-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2509.07774v1', 'HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250907809v1-splatfill-3d-scene-inpainting-via-depth-guided-gaussian-splatting.html">SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting</a></td>
  <td>SplatFillï¼šæå‡ºæ·±åº¦å¼•å¯¼çš„é«˜æ–¯æº…å°„æ–¹æ³•ç”¨äºä¸‰ç»´åœºæ™¯ä¿®å¤</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07809v1" data-paper-url="./papers/250907809v1-splatfill-3d-scene-inpainting-via-depth-guided-gaussian-splatting.html" onclick="toggleFavorite(this, '2509.07809v1', 'SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250907493v2-accurate-and-complete-surface-reconstruction-from-3d-gaussians-via-d.html">Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning</a></td>
  <td>DiGSï¼šé€šè¿‡ç›´æ¥SDFå­¦ä¹ ï¼Œä»3Dé«˜æ–¯æ¨¡å‹ä¸­å®ç°ç²¾ç¡®å’Œå®Œæ•´çš„è¡¨é¢é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07493v2" data-paper-url="./papers/250907493v2-accurate-and-complete-surface-reconstruction-from-3d-gaussians-via-d.html" onclick="toggleFavorite(this, '2509.07493v2', 'Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250908027v2-mcted-a-machine-learning-ready-dataset-for-digital-elevation-model-g.html">MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</a></td>
  <td>MCTEDï¼šä¸€ä¸ªä¸ºç«æ˜Ÿå›¾åƒæ•°å­—é«˜ç¨‹æ¨¡å‹ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„æœºå™¨å­¦ä¹ æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">Depth Anything</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08027v2" data-paper-url="./papers/250908027v2-mcted-a-machine-learning-ready-dataset-for-digital-elevation-model-g.html" onclick="toggleFavorite(this, '2509.08027v2', 'MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250907932v1-dynamic-scene-3d-reconstruction-of-an-uncooperative-resident-space-o.html">Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space Object</a></td>
  <td>é’ˆå¯¹éåˆä½œç©ºé—´ç›®æ ‡çš„åŠ¨æ€åœºæ™¯ä¸‰ç»´é‡å»ºï¼Œè¯„ä¼°å¹¶ä¼˜åŒ–ç°æœ‰ç®—æ³•æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07932v1" data-paper-url="./papers/250907932v1-dynamic-scene-3d-reconstruction-of-an-uncooperative-resident-space-o.html" onclick="toggleFavorite(this, '2509.07932v1', 'Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space Object')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250907864v2-tracing-and-mitigating-hallucinations-in-multimodal-llms-via-dynamic.html">Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization</a></td>
  <td>æå‡ºD-LEAFä»¥è§£å†³å¤šæ¨¡æ€LLMä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07864v2" data-paper-url="./papers/250907864v2-tracing-and-mitigating-hallucinations-in-multimodal-llms-via-dynamic.html" onclick="toggleFavorite(this, '2509.07864v2', 'Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250907923v1-multimodal-contrastive-pretraining-of-cbct-and-ios-for-enhanced-toot.html">Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</a></td>
  <td>æå‡ºToothMCLï¼Œç”¨äºCBCTå’ŒIOSå¤šæ¨¡æ€å¯¹æ¯”é¢„è®­ç»ƒï¼Œæå‡ç‰™é½¿åˆ†å‰²ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07923v1" data-paper-url="./papers/250907923v1-multimodal-contrastive-pretraining-of-cbct-and-ios-for-enhanced-toot.html" onclick="toggleFavorite(this, '2509.07923v1', 'Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250910555v1-surglavi-large-scale-hierarchical-dataset-for-surgical-vision-langua.html">SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning</a></td>
  <td>SurgLaViï¼šæ„å»ºå¤§è§„æ¨¡æ‰‹æœ¯è§†è§‰-è¯­è¨€åˆ†å±‚æ•°æ®é›†ï¼Œç”¨äºæ‰‹æœ¯è§†è§‰-è¯­è¨€è¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10555v1" data-paper-url="./papers/250910555v1-surglavi-large-scale-hierarchical-dataset-for-surgical-vision-langua.html" onclick="toggleFavorite(this, '2509.10555v1', 'SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250907969v1-mini-o3-scaling-up-reasoning-patterns-and-interaction-turns-for-visu.html">Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</a></td>
  <td>Mini-o3ï¼šé€šè¿‡æ‰©å±•æ¨ç†æ¨¡å¼å’Œäº¤äº’è½®æ•°ï¼Œæå‡è§†è§‰æœç´¢æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07969v1" data-paper-url="./papers/250907969v1-mini-o3-scaling-up-reasoning-patterns-and-interaction-turns-for-visu.html" onclick="toggleFavorite(this, '2509.07969v1', 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250907507v1-mvat-multi-view-aware-teacher-for-weakly-supervised-3d-object-detect.html">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></td>
  <td>MVATï¼šå¤šè§†è§’æ„ŸçŸ¥æ•™å¸ˆç½‘ç»œç”¨äºå¼±ç›‘ç£3Dç›®æ ‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07507v1" data-paper-url="./papers/250907507v1-mvat-multi-view-aware-teacher-for-weakly-supervised-3d-object-detect.html" onclick="toggleFavorite(this, '2509.07507v1', 'MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250907525v1-ehwgesture-a-dataset-for-multimodal-understanding-of-clinical-gestur.html">EHWGesture -- A dataset for multimodal understanding of clinical gestures</a></td>
  <td>EHWGestureï¼šç”¨äºä¸´åºŠæ‰‹åŠ¿å¤šæ¨¡æ€ç†è§£çš„æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07525v1" data-paper-url="./papers/250907525v1-ehwgesture-a-dataset-for-multimodal-understanding-of-clinical-gestur.html" onclick="toggleFavorite(this, '2509.07525v1', 'EHWGesture -- A dataset for multimodal understanding of clinical gestures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250908104v1-apml-adaptive-probabilistic-matching-loss-for-robust-3d-point-cloud-.html">APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</a></td>
  <td>æå‡ºè‡ªé€‚åº”æ¦‚ç‡åŒ¹é…æŸå¤±ä»¥è§£å†³3Dç‚¹äº‘é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08104v1" data-paper-url="./papers/250908104v1-apml-adaptive-probabilistic-matching-loss-for-robust-3d-point-cloud-.html" onclick="toggleFavorite(this, '2509.08104v1', 'APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250907978v1-one-view-many-worlds-single-image-to-3d-object-meets-generative-doma.html">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></td>
  <td>OnePoseViaGenï¼šç»“åˆå•å›¾3Dç”Ÿæˆä¸ç”ŸæˆåŸŸéšæœºåŒ–çš„ä¸€é˜¶æ®µ6Dä½å§¿ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">domain randomization</span> <span class="paper-tag">6D pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07978v1" data-paper-url="./papers/250907978v1-one-view-many-worlds-single-image-to-3d-object-meets-generative-doma.html" onclick="toggleFavorite(this, '2509.07978v1', 'One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250907920v1-scorehoi-physically-plausible-reconstruction-of-human-object-interac.html">ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion</a></td>
  <td>ScoreHOIï¼šæå‡ºåŸºäºScoreå¼•å¯¼æ‰©æ•£çš„ç‰©ç†å¯ä¿¡äºº-ç‰©äº¤äº’é‡å»ºæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07920v1" data-paper-url="./papers/250907920v1-scorehoi-physically-plausible-reconstruction-of-human-object-interac.html" onclick="toggleFavorite(this, '2509.07920v1', 'ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)