---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-29
---

# cs.CVï¼ˆ2025-12-29ï¼‰

ğŸ“Š å…± **30** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (11 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251223464v1-hy-motion-10-scaling-flow-matching-models-for-text-to-motion-generat.html">HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation</a></td>
  <td>HY-Motion 1.0ï¼šæ‰©å±•Flow Matchingæ¨¡å‹è‡³åäº¿å‚æ•°è§„æ¨¡ï¼Œå®ç°æ–‡æœ¬é©±åŠ¨çš„3Däººä½“åŠ¨ä½œç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">flow matching</span> <span class="paper-tag">text-to-motion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23464v1" data-paper-url="./papers/251223464v1-hy-motion-10-scaling-flow-matching-models-for-text-to-motion-generat.html" onclick="toggleFavorite(this, '2512.23464v1', 'HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251223545v1-pathfound-an-agentic-multimodal-model-activating-evidence-seeking-pa.html">PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</a></td>
  <td>PathFoundï¼šä¸€ç§ä¸»åŠ¨è¯æ®æœå¯»çš„ç—…ç†è¯Šæ–­å¤šæ¨¡æ€Agentæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23545v1" data-paper-url="./papers/251223545v1-pathfound-an-agentic-multimodal-model-activating-evidence-seeking-pa.html" onclick="toggleFavorite(this, '2512.23545v1', 'PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251223576v1-livetalk-real-time-multimodal-interactive-video-diffusion-via-improv.html">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a></td>
  <td>æå‡ºæ”¹è¿›çš„On-Policyè’¸é¦æ–¹æ³•ï¼Œå®ç°å¤šæ¨¡æ€äº¤äº’å¼å®æ—¶è§†é¢‘æ‰©æ•£</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23576v1" data-paper-url="./papers/251223576v1-livetalk-real-time-multimodal-interactive-video-diffusion-via-improv.html" onclick="toggleFavorite(this, '2512.23576v1', 'LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251223573v1-proguard-towards-proactive-multimodal-safeguard.html">ProGuard: Towards Proactive Multimodal Safeguard</a></td>
  <td>æå‡ºProGuardï¼Œä¸€ç§ä¸»åŠ¨å¼å¤šæ¨¡æ€å®‰å…¨é˜²æŠ¤æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å’Œæè¿°ç”Ÿæˆæ¨¡å‹ä¸­çš„OODå®‰å…¨é£é™©ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23573v1" data-paper-url="./papers/251223573v1-proguard-towards-proactive-multimodal-safeguard.html" onclick="toggleFavorite(this, '2512.23573v1', 'ProGuard: Towards Proactive Multimodal Safeguard')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251223568v1-thinkgen-generalized-thinking-for-visual-generation.html">ThinkGen: Generalized Thinking for Visual Generation</a></td>
  <td>ThinkGenï¼šæå‡ºåŸºäºå¹¿ä¹‰æ€ç»´çš„è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œæå‡å¤šåœºæ™¯é€‚åº”æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23568v1" data-paper-url="./papers/251223568v1-thinkgen-generalized-thinking-for-visual-generation.html" onclick="toggleFavorite(this, '2512.23568v1', 'ThinkGen: Generalized Thinking for Visual Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251223180v1-gaussiandwm-3d-gaussian-driving-world-model-for-unified-scene-unders.html">GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation</a></td>
  <td>æå‡ºåŸºäº3Dé«˜æ–¯è¡¨ç¤ºçš„é©¾é©¶ä¸–ç•Œæ¨¡å‹GaussianDWMï¼Œå®ç°ç»Ÿä¸€çš„åœºæ™¯ç†è§£å’Œå¤šæ¨¡æ€ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23180v1" data-paper-url="./papers/251223180v1-gaussiandwm-3d-gaussian-driving-world-model-for-unified-scene-unders.html" onclick="toggleFavorite(this, '2512.23180v1', 'GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251223333v1-cme-cad-heterogeneous-collaborative-multi-expert-reinforcement-learn.html">CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation</a></td>
  <td>æå‡ºCME-CADå¼‚æ„åä½œå¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé«˜ç²¾åº¦å¯ç¼–è¾‘CADä»£ç ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23333v1" data-paper-url="./papers/251223333v1-cme-cad-heterogeneous-collaborative-multi-expert-reinforcement-learn.html" onclick="toggleFavorite(this, '2512.23333v1', 'CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251223176v1-gvsynergy-det-synergistic-gaussian-voxel-representations-for-multi-v.html">GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</a></td>
  <td>GVSynergy-Detï¼šååŒé«˜æ–¯-ä½“ç´ è¡¨ç¤ºç”¨äºå¤šè§†è§’3Dç›®æ ‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23176v1" data-paper-url="./papers/251223176v1-gvsynergy-det-synergistic-gaussian-voxel-representations-for-multi-v.html" onclick="toggleFavorite(this, '2512.23176v1', 'GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251223413v1-bridging-cognitive-gap-hierarchical-description-learning-for-artisti.html">Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment</a></td>
  <td>æå‡ºArtQuantæ¡†æ¶ï¼Œé€šè¿‡å±‚çº§æè¿°å­¦ä¹ è§£å†³è‰ºæœ¯å›¾åƒç¾å­¦è¯„ä¼°ä¸­çš„è®¤çŸ¥é¸¿æ²Ÿã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23413v1" data-paper-url="./papers/251223413v1-bridging-cognitive-gap-hierarchical-description-learning-for-artisti.html" onclick="toggleFavorite(this, '2512.23413v1', 'Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251223335v1-visual-language-hypothesis.html">Visual Language Hypothesis</a></td>
  <td>æå‡ºè§†è§‰è¯­è¨€å‡è®¾ï¼Œä»ç»“æ„å’Œæ‹“æ‰‘è§’åº¦åˆ†æè§†è§‰è¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23335v1" data-paper-url="./papers/251223335v1-visual-language-hypothesis.html" onclick="toggleFavorite(this, '2512.23335v1', 'Visual Language Hypothesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251223379v1-soulx-livetalk-technical-report.html">SoulX-LiveTalk Technical Report</a></td>
  <td>æå‡ºSoulX-LiveTalkæ¡†æ¶ï¼Œå®ç°é«˜ä¿çœŸå®æ—¶éŸ³é¢‘é©±åŠ¨çš„æ•°å­—äººç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23379v1" data-paper-url="./papers/251223379v1-soulx-livetalk-technical-report.html" onclick="toggleFavorite(this, '2512.23379v1', 'SoulX-LiveTalk Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/251223565v1-rxnbench-a-multimodal-benchmark-for-evaluating-large-language-models.html">RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</a></td>
  <td>RxnBenchï¼šä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å¯¹ç§‘å­¦æ–‡çŒ®ä¸­åŒ–å­¦ååº”çš„ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23565v1" data-paper-url="./papers/251223565v1-rxnbench-a-multimodal-benchmark-for-evaluating-large-language-models.html" onclick="toggleFavorite(this, '2512.23565v1', 'RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251223219v1-mm-uavbench-how-well-do-multimodal-large-language-models-see-think-a.html">MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?</a></td>
  <td>æå‡ºMM-UAVBenchï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä½ç©ºæ— äººæœºåœºæ™¯ä¸‹çš„æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè§„åˆ’èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23219v1" data-paper-url="./papers/251223219v1-mm-uavbench-how-well-do-multimodal-large-language-models-see-think-a.html" onclick="toggleFavorite(this, '2512.23219v1', 'MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251223597v1-scalable-residual-feature-aggregation-framework-with-hybrid-metaheur.html">Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging</a></td>
  <td>æå‡ºåŸºäºæ··åˆå…ƒå¯å‘å¼ä¼˜åŒ–çš„å¯æ‰©å±•æ®‹å·®ç‰¹å¾èšåˆæ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€CTå½±åƒä¸­æ—©æœŸèƒ°è…ºè‚¿ç˜¤çš„ç¨³å¥æ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23597v1" data-paper-url="./papers/251223597v1-scalable-residual-feature-aggregation-framework-with-hybrid-metaheur.html" onclick="toggleFavorite(this, '2512.23597v1', 'Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251223291v1-multi-track-multimodal-learning-on-imigue-micro-gesture-and-emotion-.html">Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition</a></td>
  <td>é’ˆå¯¹iMiGUEæ•°æ®é›†ï¼Œæå‡ºå¤šè½¨å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ç”¨äºå¾®æ‰‹åŠ¿å’Œæƒ…æ„Ÿè¯†åˆ«</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23291v1" data-paper-url="./papers/251223291v1-multi-track-multimodal-learning-on-imigue-micro-gesture-and-emotion-.html" onclick="toggleFavorite(this, '2512.23291v1', 'Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251223243v1-multimodal-interpretation-of-remote-sensing-images-dynamic-resolutio.html">Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism</a></td>
  <td>æå‡ºDRISå’ŒMS-VLAMï¼Œç”¨äºæå‡é¥æ„Ÿå›¾åƒå¤šæ¨¡æ€èåˆçš„æ•ˆç‡å’Œè¯­ä¹‰ç†è§£ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23243v1" data-paper-url="./papers/251223243v1-multimodal-interpretation-of-remote-sensing-images-dynamic-resolutio.html" onclick="toggleFavorite(this, '2512.23243v1', 'Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251223239v1-rs-prune-training-free-data-pruning-at-high-ratios-for-efficient-rem.html">RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models</a></td>
  <td>RS-Pruneï¼šé¢å‘é¥æ„Ÿæ‰©æ•£æ¨¡å‹çš„é«˜æ¯”ä¾‹å…è®­ç»ƒæ•°æ®å‰ªæ</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23239v1" data-paper-url="./papers/251223239v1-rs-prune-training-free-data-pruning-at-high-ratios-for-efficient-rem.html" onclick="toggleFavorite(this, '2512.23239v1', 'RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251223646v1-omniagent-audio-guided-active-perception-agent-for-omnimodal-audio-v.html">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a></td>
  <td>OmniAgentï¼šä¸€ç§éŸ³é¢‘å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥Agentï¼Œç”¨äºå…¨æ¨¡æ€éŸ³è§†é¢‘ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23646v1" data-paper-url="./papers/251223646v1-omniagent-audio-guided-active-perception-agent-for-omnimodal-audio-v.html" onclick="toggleFavorite(this, '2512.23646v1', 'OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251223304v1-medgemma-vs-gpt-4-open-source-and-proprietary-zero-shot-medical-dise.html">MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images</a></td>
  <td>MedGemmaåœ¨åŒ»å­¦å›¾åƒç–¾ç—…åˆ†ç±»ä¸­ä¼˜äºGPT-4ï¼Œé¢†åŸŸå¾®è°ƒè‡³å…³é‡è¦</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23304v1" data-paper-url="./papers/251223304v1-medgemma-vs-gpt-4-open-source-and-proprietary-zero-shot-medical-dise.html" onclick="toggleFavorite(this, '2512.23304v1', 'MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251223169v1-revealer-reinforcement-guided-visual-reasoning-for-element-level-tex.html">REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation</a></td>
  <td>REVEALERï¼šæå‡ºå¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„è§†è§‰æ¨ç†æ¡†æ¶ï¼Œç”¨äºå…ƒç´ çº§æ–‡æœ¬-å›¾åƒå¯¹é½è¯„ä¼°</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23169v1" data-paper-url="./papers/251223169v1-revealer-reinforcement-guided-visual-reasoning-for-element-level-tex.html" onclick="toggleFavorite(this, '2512.23169v1', 'REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251223454v1-automated-river-gauge-plate-reading-using-a-hybrid-object-detection-.html">Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin</a></td>
  <td>æå‡ºæ··åˆAIæ¡†æ¶ï¼Œç”¨äºåˆ©å§†æ³¢æ³¢æ²³æµåŸŸè‡ªåŠ¨è¯»å–æ°´ä½æ ‡å°º</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23454v1" data-paper-url="./papers/251223454v1-automated-river-gauge-plate-reading-using-a-hybrid-object-detection-.html" onclick="toggleFavorite(this, '2512.23454v1', 'Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251223427v1-towards-integrating-uncertainty-for-domain-agnostic-segmentation.html">Towards Integrating Uncertainty for Domain-Agnostic Segmentation</a></td>
  <td>æå‡ºUncertSAMåŸºå‡†å¹¶æ¢ç´¢ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæå‡åˆ†å‰²æ¨¡å‹åœ¨æœªçŸ¥é¢†åŸŸçš„æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23427v1" data-paper-url="./papers/251223427v1-towards-integrating-uncertainty-for-domain-agnostic-segmentation.html" onclick="toggleFavorite(this, '2512.23427v1', 'Towards Integrating Uncertainty for Domain-Agnostic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251223255v1-contour-information-aware-2d-gaussian-splatting-for-image-representa.html">Contour Information Aware 2D Gaussian Splatting for Image Representation</a></td>
  <td>æå‡ºè½®å»“ä¿¡æ¯æ„ŸçŸ¥çš„2Dé«˜æ–¯æº…å°„ï¼Œæå‡å›¾åƒè¡¨ç¤ºä¸­è¾¹ç¼˜é‡å»ºè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23255v1" data-paper-url="./papers/251223255v1-contour-information-aware-2d-gaussian-splatting-for-image-representa.html" onclick="toggleFavorite(this, '2512.23255v1', 'Contour Information Aware 2D Gaussian Splatting for Image Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251223365v1-spatialmosaic-a-multiview-vlm-dataset-for-partial-visibility.html">SpatialMosaic: A Multiview VLM Dataset for Partial Visibility</a></td>
  <td>æå‡ºSpatialMosaicæ•°æ®é›†ï¼Œå¢å¼ºå¤šè§†è§’VLMåœ¨éƒ¨åˆ†å¯è§åœºæ™¯ä¸‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23365v1" data-paper-url="./papers/251223365v1-spatialmosaic-a-multiview-vlm-dataset-for-partial-visibility.html" onclick="toggleFavorite(this, '2512.23365v1', 'SpatialMosaic: A Multiview VLM Dataset for Partial Visibility')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251223215v1-avoid-the-adverse-visual-conditions-dataset-with-obstacles-for-drivi.html">AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding</a></td>
  <td>AVOIDï¼šç”¨äºé©¾é©¶åœºæ™¯ç†è§£çš„å«éšœç¢ç‰©æ¶åŠ£è§†è§‰æ¡ä»¶æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23215v1" data-paper-url="./papers/251223215v1-avoid-the-adverse-visual-conditions-dataset-with-obstacles-for-drivi.html" onclick="toggleFavorite(this, '2512.23215v1', 'AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251223486v1-multi-label-classification-with-panoptic-context-aggregation-network.html">Multi-label Classification with Panoptic Context Aggregation Networks</a></td>
  <td>æå‡ºPanCANï¼Œé€šè¿‡å…¨æ™¯ä¸Šä¸‹æ–‡èšåˆç½‘ç»œæå‡å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23486v1" data-paper-url="./papers/251223486v1-multi-label-classification-with-panoptic-context-aggregation-network.html" onclick="toggleFavorite(this, '2512.23486v1', 'Multi-label Classification with Panoptic Context Aggregation Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251223437v1-realx3d-a-physically-degraded-3d-benchmark-for-multi-view-visual-res.html">RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction</a></td>
  <td>RealX3Dï¼šä¸€ä¸ªç”¨äºå¤šè§†è§’è§†è§‰æ¢å¤ä¸é‡å»ºçš„ç‰©ç†é€€åŒ–3DåŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23437v1" data-paper-url="./papers/251223437v1-realx3d-a-physically-degraded-3d-benchmark-for-multi-view-visual-res.html" onclick="toggleFavorite(this, '2512.23437v1', 'RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251223374v1-next-imdl-build-benchmark-for-next-generation-image-manipulation-det.html">NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization</a></td>
  <td>NeXT-IMDLï¼šæ„å»ºä¸‹ä¸€ä»£å›¾åƒç¯¡æ”¹æ£€æµ‹ä¸å®šä½çš„åŸºå‡†æµ‹è¯•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23374v1" data-paper-url="./papers/251223374v1-next-imdl-build-benchmark-for-next-generation-image-manipulation-det.html" onclick="toggleFavorite(this, '2512.23374v1', 'NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251223705v1-diffusion-knows-transparency-repurposing-video-diffusion-for-transpa.html">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a></td>
  <td>åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ŒDKTå®ç°äº†é€æ˜ç‰©ä½“æ·±åº¦å’Œæ³•å‘é‡çš„é›¶æ ·æœ¬SOTAä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23705v1" data-paper-url="./papers/251223705v1-diffusion-knows-transparency-repurposing-video-diffusion-for-transpa.html" onclick="toggleFavorite(this, '2512.23705v1', 'Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/251223221v1-holi-detr-holistic-fashion-item-detection-leveraging-contextual-info.html">Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information</a></td>
  <td>æå‡ºHoli-DETRï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ•´ä½“æ—¶å°šå•å“æ£€æµ‹ï¼Œæå‡æ£€æµ‹ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.23221v1" data-paper-url="./papers/251223221v1-holi-detr-holistic-fashion-item-detection-leveraging-contextual-info.html" onclick="toggleFavorite(this, '2512.23221v1', 'Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)