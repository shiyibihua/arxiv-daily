---
layout: default
title: "HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation"
---

# HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23464" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23464v1</a>
  <a href="https://arxiv.org/pdf/2512.23464.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23464v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23464v1', 'HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao

**åˆ†ç±»**: cs.CV, cs.AI, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

**å¤‡æ³¨**: Github: see https://github.com/Tencent-Hunyuan/HY-Motion-1.0

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HY-Motion 1.0ï¼šæ‰©å±•Flow Matchingæ¨¡å‹è‡³åäº¿å‚æ•°è§„æ¨¡ï¼Œå®ç°æ–‡æœ¬é©±åŠ¨çš„3Däººä½“åŠ¨ä½œç”Ÿæˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆ` `3Däººä½“è¿åŠ¨` `Diffusion Transformer` `Flow Matching` `å¤§è§„æ¨¡é¢„è®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `è¿åŠ¨æ•æ‰` `åŠ¨ä½œç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’ŒåŠ¨ä½œè´¨é‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚åœºæ™¯éœ€æ±‚ã€‚
2. HY-Motion 1.0 é€šè¿‡æ‰©å±• Diffusion Transformer (DiT) æ¶æ„ï¼Œå¹¶ç»“åˆå¤§è§„æ¨¡æ•°æ®è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. è¯¥æ¨¡å‹åœ¨è¶…è¿‡200ä¸ªè¿åŠ¨ç±»åˆ«ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰å¼€æºåŸºå‡†ï¼Œå¹¶å·²å¼€æºä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

HY-Motion 1.0 æ˜¯ä¸€ç³»åˆ—å…ˆè¿›çš„å¤§è§„æ¨¡è¿åŠ¨ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æè¿°ç”Ÿæˆ 3D äººä½“è¿åŠ¨ã€‚å®ƒæ˜¯é¦–æ¬¡æˆåŠŸåœ°å°†åŸºäº Diffusion Transformer (DiT) çš„ flow matching æ¨¡å‹æ‰©å±•åˆ°è¿åŠ¨ç”Ÿæˆé¢†åŸŸä¸­æ•°åäº¿å‚æ•°è§„æ¨¡çš„å°è¯•ï¼Œæä¾›äº†æ˜¾è‘—ä¼˜äºå½“å‰å¼€æºåŸºå‡†çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„å…¨é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬è¶…è¿‡ 3,000 å°æ—¶çš„è¿åŠ¨æ•°æ®çš„å¤§è§„æ¨¡é¢„è®­ç»ƒã€400 å°æ—¶ç²¾é€‰æ•°æ®ä¸Šçš„é«˜è´¨é‡å¾®è°ƒï¼Œä»¥åŠæ¥è‡ªäººç±»åé¦ˆå’Œå¥–åŠ±æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥ç¡®ä¿ä¸æ–‡æœ¬æŒ‡ä»¤çš„ç²¾ç¡®å¯¹é½å’Œé«˜è´¨é‡çš„è¿åŠ¨ã€‚è¿™ä¸€æ¡†æ¶ç”±æˆ‘ä»¬ç»†è‡´çš„æ•°æ®å¤„ç†æµç¨‹æ”¯æŒï¼Œè¯¥æµç¨‹æ‰§è¡Œä¸¥æ ¼çš„è¿åŠ¨æ¸…ç†å’Œæ ‡æ³¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†æœ€å¹¿æ³›çš„è¦†ç›–ï¼Œè·¨è¶Š 6 ä¸ªä¸»è¦ç±»åˆ«ä¸­çš„ 200 å¤šä¸ªè¿åŠ¨ç±»åˆ«ã€‚æˆ‘ä»¬å°† HY-Motion 1.0 å¼€æºï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶å¹¶åŠ é€Ÿ 3D äººä½“è¿åŠ¨ç”Ÿæˆæ¨¡å‹å‘å•†ä¸šæˆç†Ÿçš„è¿‡æ¸¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å‹éš¾ä»¥ç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆæ–‡æœ¬æè¿°çš„3Däººä½“è¿åŠ¨ã€‚ç—›ç‚¹åœ¨äºæ¨¡å‹è§„æ¨¡å°ï¼Œæ•°æ®é‡ä¸è¶³ï¼Œä»¥åŠç¼ºä¹æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥æ¥ä¿è¯æ–‡æœ¬ä¸åŠ¨ä½œçš„å¯¹é½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHY-Motion 1.0çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®å’Œæ¨¡å‹å‚æ•°æ¥æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨å…¨é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ç†è§£æ–‡æœ¬æŒ‡ä»¤å¹¶ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHY-Motion 1.0 é‡‡ç”¨åŸºäº Diffusion Transformer (DiT) çš„ flow matching æ¨¡å‹ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) å¤§è§„æ¨¡è¿åŠ¨æ•°æ®é¢„è®­ç»ƒï¼›2) é«˜è´¨é‡æ•°æ®å¾®è°ƒï¼›3) åŸºäºäººç±»åé¦ˆå’Œå¥–åŠ±æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ã€‚æ•°æ®å¤„ç†æµç¨‹åŒ…æ‹¬è¿åŠ¨æ¸…ç†å’Œæ ‡æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæˆåŠŸå°† DiT-based flow matching æ¨¡å‹æ‰©å±•åˆ°æ•°åäº¿å‚æ•°è§„æ¨¡ï¼Œå¹¶åº”ç”¨äºè¿åŠ¨ç”Ÿæˆé¢†åŸŸã€‚æ­¤å¤–ï¼Œå…¨é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œç‰¹åˆ«æ˜¯ç»“åˆäººç±»åé¦ˆå’Œå¥–åŠ±æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¯æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†çš„æ„å»ºï¼Œé«˜è´¨é‡å¾®è°ƒæ•°æ®é›†çš„ç­›é€‰ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚æ¨¡å‹å‚æ•°è§„æ¨¡è¾¾åˆ°æ•°åäº¿çº§åˆ«ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨ Diffusion Transformer (DiT)ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23464v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23464v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23464v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

HY-Motion 1.0 åœ¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºå‡†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¦†ç›–è¶…è¿‡200ä¸ªè¿åŠ¨ç±»åˆ«çš„é«˜è´¨é‡3Däººä½“è¿åŠ¨ã€‚é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é«˜è´¨é‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’ŒåŠ¨ä½œè´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HY-Motion 1.0 å¯å¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨ï¼Œæå¤§åœ°é™ä½äº†ç›¸å…³å†…å®¹çš„åˆ¶ä½œæˆæœ¬ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›äº†æ›´ä¸°å¯Œçš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½åº·å¤ã€è¿åŠ¨è®­ç»ƒç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

