---
layout: default
title: "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"
---

# OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23646" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23646v1</a>
  <a href="https://arxiv.org/pdf/2512.23646.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23646v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23646v1', 'OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

**å¤‡æ³¨**: Website:https://kd-tao.github.io/OmniAgent/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniAgentï¼šä¸€ç§éŸ³é¢‘å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥Agentï¼Œç”¨äºå…¨æ¨¡æ€éŸ³è§†é¢‘ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³è§†é¢‘ç†è§£` `ä¸»åŠ¨æ„ŸçŸ¥` `å¤šæ¨¡æ€èåˆ` `éŸ³é¢‘å¼•å¯¼` `åŠ¨æ€è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³è§†é¢‘ç†è§£ä¸­ç¼ºä¹ç»†ç²’åº¦çš„è·¨æ¨¡æ€ç†è§£å’Œå¤šæ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚
2. OmniAgenté€šè¿‡éŸ³é¢‘å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥ï¼ŒåŠ¨æ€ç¼–æ’å·¥å…·è°ƒç”¨ï¼Œå®ç°ç”±ç²—åˆ°ç²¾çš„éŸ³è§†é¢‘ç†è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniAgentåœ¨éŸ³è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œè¶…è¶Šç°æœ‰æ¨¡å‹10%-20%çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºOmniAgentï¼Œä¸€ä¸ªå®Œå…¨ç”±éŸ³é¢‘å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥Agentï¼Œæ—¨åœ¨æå‡å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³è§†é¢‘ç†è§£ä¸­çš„ç»†ç²’åº¦è·¨æ¨¡æ€ç†è§£å’Œå¤šæ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚ä¸ä¾èµ–é™æ€å·¥ä½œæµå’Œå¯†é›†å¸§å­—å¹•çš„æ–¹æ³•ä¸åŒï¼ŒOmniAgentå®ç°äº†ä»è¢«åŠ¨å“åº”ç”Ÿæˆåˆ°ä¸»åŠ¨å¤šæ¨¡æ€æŸ¥è¯¢çš„èŒƒå¼è½¬å˜ã€‚å®ƒé‡‡ç”¨åŠ¨æ€è§„åˆ’ï¼Œè‡ªä¸»åœ°æŒ‰éœ€ç¼–æ’å·¥å…·è°ƒç”¨ï¼Œå¹¶å°†æ„ŸçŸ¥æ³¨æ„åŠ›é›†ä¸­åœ¨ä»»åŠ¡ç›¸å…³çš„çº¿ç´¢ä¸Šã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°é¢–çš„ç”±ç²—åˆ°ç²¾çš„éŸ³é¢‘å¼•å¯¼æ„ŸçŸ¥èŒƒå¼ï¼Œåˆ©ç”¨éŸ³é¢‘çº¿ç´¢æ¥å®šä½æ—¶é—´äº‹ä»¶ï¼Œå¹¶æŒ‡å¯¼åç»­çš„æ¨ç†ã€‚åœ¨ä¸‰ä¸ªéŸ³è§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒOmniAgentå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†é¢†å…ˆçš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†10%-20%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œé€šå¸¸é‡‡ç”¨é™æ€çš„å·¥ä½œæµç¨‹å’Œå¯†é›†çš„å¸§å­—å¹•æ–¹å¼ï¼Œç¼ºä¹å¯¹éŸ³é¢‘å’Œè§†é¢‘ä¿¡æ¯ä¹‹é—´ç»†ç²’åº¦å…³è”çš„ç†è§£èƒ½åŠ›ï¼Œéš¾ä»¥å®ç°ç²¾å‡†çš„å¤šæ¨¡æ€å¯¹é½ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›å—é™ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨éŸ³é¢‘ä¿¡æ¯å¼•å¯¼è§†è§‰æ„ŸçŸ¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniAgentçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éŸ³é¢‘ä¿¡æ¯ä½œä¸ºå¼•å¯¼ï¼Œä¸»åŠ¨åœ°è¿›è¡Œå¤šæ¨¡æ€æ„ŸçŸ¥å’Œæ¨ç†ã€‚é€šè¿‡åŠ¨æ€è§„åˆ’å·¥å…·è°ƒç”¨ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®éŸ³é¢‘çº¿ç´¢è‡ªä¸»åœ°é€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯ä¸Šï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„éŸ³è§†é¢‘ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniAgentçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) éŸ³é¢‘äº‹ä»¶æ£€æµ‹æ¨¡å—ï¼Œç”¨äºä»éŸ³é¢‘ä¸­æå–å…³é”®äº‹ä»¶ä¿¡æ¯ï¼›2) åŠ¨æ€è§„åˆ’æ¨¡å—ï¼Œæ ¹æ®éŸ³é¢‘äº‹ä»¶ä¿¡æ¯ï¼Œè§„åˆ’å·¥å…·è°ƒç”¨é¡ºåºï¼›3) å¤šæ¨¡æ€æ„ŸçŸ¥æ¨¡å—ï¼Œåˆ©ç”¨é€‰å®šçš„å·¥å…·å¯¹è§†é¢‘è¿›è¡Œå¤„ç†ï¼Œæå–ç›¸å…³è§†è§‰ç‰¹å¾ï¼›4) æ¨ç†æ¨¡å—ï¼Œç»“åˆéŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œå®Œæˆæœ€ç»ˆä»»åŠ¡ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç”±ç²—åˆ°ç²¾çš„ï¼Œé¦–å…ˆé€šè¿‡éŸ³é¢‘å®šä½å…³é”®äº‹ä»¶ï¼Œç„¶åå¼•å¯¼è§†è§‰æ„ŸçŸ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniAgentçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸»åŠ¨æ„ŸçŸ¥çš„æ¨¡å¼å’Œç”±ç²—åˆ°ç²¾çš„éŸ³é¢‘å¼•å¯¼ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„è¢«åŠ¨å¼æ¨¡å‹ä¸åŒï¼ŒOmniAgentå¯ä»¥æ ¹æ®éŸ³é¢‘ä¿¡æ¯ä¸»åŠ¨åœ°é€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯ä¸Šã€‚è¿™ç§ä¸»åŠ¨æ„ŸçŸ¥çš„æ¨¡å¼ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæé«˜ç†è§£èƒ½åŠ›ã€‚ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥åˆ™ä¿è¯äº†æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°å®šä½å…³é”®ä¿¡æ¯ï¼Œé¿å…äº†å¯¹æ‰€æœ‰å¸§è¿›è¡Œå¯†é›†å¤„ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šOmniAgentçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) éŸ³é¢‘äº‹ä»¶æ£€æµ‹æ¨¡å—çš„è®¾è®¡ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„éŸ³é¢‘ç‰¹å¾å’Œæ¨¡å‹ç»“æ„ï¼Œä»¥ä¿è¯èƒ½å¤Ÿå‡†ç¡®åœ°æ£€æµ‹åˆ°å…³é”®äº‹ä»¶ï¼›2) åŠ¨æ€è§„åˆ’æ¨¡å—çš„è®¾è®¡ï¼Œéœ€è¦å®šä¹‰åˆé€‚çš„å¥–åŠ±å‡½æ•°å’Œæœç´¢ç­–ç•¥ï¼Œä»¥ä¿è¯èƒ½å¤Ÿæ‰¾åˆ°æœ€ä¼˜çš„å·¥å…·è°ƒç”¨é¡ºåºï¼›3) å¤šæ¨¡æ€æ„ŸçŸ¥æ¨¡å—çš„è®¾è®¡ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„è§†è§‰ç‰¹å¾æå–å™¨å’Œèåˆç­–ç•¥ï¼Œä»¥ä¿è¯èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23646v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23646v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23646v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

OmniAgentåœ¨ä¸‰ä¸ªéŸ³è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œç›¸æ¯”äºé¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡äº†10%-20%ã€‚è¿™è¡¨æ˜OmniAgentåœ¨éŸ³è§†é¢‘ç†è§£æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniAgentåœ¨æ™ºèƒ½ç›‘æ§ã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥é€šè¿‡åˆ†æç›‘æ§è§†é¢‘ä¸­çš„å£°éŸ³äº‹ä»¶ï¼ˆå¦‚ç»ç’ƒç ´ç¢ã€å‘¼æ•‘å£°ç­‰ï¼‰æ¥è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸æƒ…å†µå¹¶æŠ¥è­¦ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥é€šè¿‡åˆ†æè½¦è¾†å‘¨å›´çš„å£°éŸ³ä¿¡æ¯ï¼ˆå¦‚è­¦ç¬›å£°ã€å–‡å­å£°ç­‰ï¼‰æ¥è¾…åŠ©é©¾é©¶å†³ç­–ï¼Œæé«˜å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

