---
layout: default
title: "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"
---

# Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23705" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23705v1</a>
  <a href="https://arxiv.org/pdf/2512.23705.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23705v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23705v1', 'Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

**å¤‡æ³¨**: Project Page: https://daniellli.github.io/projects/DKT/; Code: https://github.com/Daniellli/DKT; Dataset: https://huggingface.co/datasets/Daniellesry/TransPhy3D

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ŒDKTå®ç°äº†é€æ˜ç‰©ä½“æ·±åº¦å’Œæ³•å‘é‡çš„é›¶æ ·æœ¬SOTAä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `é€æ˜ç‰©ä½“æ„ŸçŸ¥` `æ·±åº¦ä¼°è®¡` `æ³•å‘é‡ä¼°è®¡` `è§†é¢‘æ‰©æ•£æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæ–¹æ³•åœ¨é€æ˜ç‰©ä½“æ·±åº¦ä¼°è®¡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé€æ˜ç‰©ä½“çš„æŠ˜å°„ã€åå°„ç­‰ç‰¹æ€§è¿åäº†ä¼ ç»Ÿè§†è§‰ç®—æ³•çš„å‡è®¾ã€‚
2. è¯¥è®ºæ–‡åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„é€æ˜æ•ˆæœçš„ç‰¹æ€§ï¼Œé€šè¿‡è®­ç»ƒå°†è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºæ·±åº¦å’Œæ³•å‘é‡ä¼°è®¡å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šå‡å–å¾—äº†SOTAç»“æœï¼Œå¹¶åœ¨æŠ“å–ä»»åŠ¡ä¸­æé«˜äº†æˆåŠŸç‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€æ˜ç‰©ä½“å¯¹æ„ŸçŸ¥ç³»ç»Ÿæ¥è¯´ä¸€ç›´æ˜¯ä¸ªéš¾é¢˜ï¼ŒæŠ˜å°„ã€åå°„å’Œé€å°„ç ´åäº†ç«‹ä½“è§†è§‰ã€é£è¡Œæ—¶é—´ç›¸æœºä»¥åŠçº¯ç²¹åˆ¤åˆ«å¼å•ç›®æ·±åº¦ä¼°è®¡çš„å‡è®¾ï¼Œå¯¼è‡´ç©ºæ´å’Œæ—¶é—´ä¸Šä¸ç¨³å®šçš„ä¼°è®¡ã€‚æœ¬æ–‡çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œç°ä»£è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»åˆæˆäº†ä»¤äººä¿¡æœçš„é€æ˜ç°è±¡ï¼Œè¡¨æ˜å®ƒä»¬å·²ç»å†…åŒ–äº†å…‰å­¦è§„åˆ™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†TransPhy3Dï¼Œä¸€ä¸ªé€æ˜/åå°„åœºæ™¯çš„åˆæˆè§†é¢‘è¯­æ–™åº“ï¼ŒåŒ…å«1.1ä¸‡ä¸ªä½¿ç”¨Blender/Cyclesæ¸²æŸ“çš„åºåˆ—ã€‚åœºæ™¯ç”±ä¸°å¯Œçš„é™æ€èµ„äº§å’Œç¨‹åºåŒ–èµ„äº§ç»„æˆï¼Œå¹¶é…ä»¥ç»ç’ƒ/å¡‘æ–™/é‡‘å±ææ–™ã€‚ä½¿ç”¨åŸºäºç‰©ç†çš„å…‰çº¿è¿½è¸ªå’ŒOptiXé™å™ªæ¸²æŸ“RGB +æ·±åº¦+æ³•å‘é‡ã€‚ä»å¤§å‹è§†é¢‘æ‰©æ•£æ¨¡å‹å‡ºå‘ï¼Œé€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨å­¦ä¹ è§†é¢‘åˆ°è§†é¢‘çš„æ·±åº¦ï¼ˆå’Œæ³•å‘é‡ï¼‰è½¬æ¢å™¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†RGBå’Œï¼ˆå¸¦å™ªå£°çš„ï¼‰æ·±åº¦æ½œåœ¨å˜é‡è¿æ¥åœ¨DiTéª¨å¹²ç½‘ç»œä¸­ï¼Œå¹¶åœ¨TransPhy3Då’Œç°æœ‰çš„é€å¸§åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»è€Œä¸ºä»»æ„é•¿åº¦çš„è¾“å…¥è§†é¢‘äº§ç”Ÿæ—¶é—´ä¸€è‡´çš„é¢„æµ‹ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹DKTåœ¨æ¶‰åŠé€æ˜åº¦çš„çœŸå®å’Œåˆæˆè§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆClearPoseã€DREDSå’ŒTransPhy3D-Testï¼‰ä¸Šå®ç°äº†é›¶æ ·æœ¬SOTAã€‚å®ƒæé«˜äº†ç²¾åº¦å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶ä¸”ä¸€ä¸ªæ³•å‘é‡å˜ä½“åœ¨ClearPoseä¸Šè®¾ç½®äº†æœ€ä½³è§†é¢‘æ³•å‘é‡ä¼°è®¡ç»“æœã€‚ä¸€ä¸ªç´§å‡‘çš„1.3Bç‰ˆæœ¬ä»¥çº¦0.17ç§’/å¸§çš„é€Ÿåº¦è¿è¡Œã€‚é›†æˆåˆ°æŠ“å–å †æ ˆä¸­ï¼ŒDKTçš„æ·±åº¦æé«˜äº†åŠé€æ˜ã€åå°„å’Œæ¼«åå°„è¡¨é¢çš„æˆåŠŸç‡ï¼Œä¼˜äºå…ˆå‰çš„ä¼°è®¡å™¨ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœæ”¯æŒä¸€ä¸ªæ›´å¹¿æ³›çš„è®ºç‚¹ï¼šâ€œæ‰©æ•£çŸ¥é“é€æ˜åº¦ã€‚â€ç”Ÿæˆè§†é¢‘å…ˆéªŒå¯ä»¥è¢«é‡æ–°åˆ©ç”¨ï¼Œé«˜æ•ˆä¸”æ— æ ‡ç­¾åœ°è½¬åŒ–ä¸ºé²æ£’çš„ã€æ—¶é—´è¿è´¯çš„æ„ŸçŸ¥ï¼Œç”¨äºå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ“ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é€æ˜å’Œåå°„ç‰©ä½“æ·±åº¦å’Œæ³•å‘é‡ä¼°è®¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç«‹ä½“è§†è§‰ã€ToFä¼ æ„Ÿå™¨å’Œå•ç›®æ·±åº¦ä¼°è®¡ï¼Œåœ¨å¤„ç†é€æ˜ç‰©ä½“æ—¶ä¼šå¤±æ•ˆï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡ä¸å‡†ç¡®ã€å­˜åœ¨ç©ºæ´ä»¥åŠæ—¶é—´ä¸Šçš„ä¸ç¨³å®šæ€§ã€‚è¿™äº›æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†ç”±äºæŠ˜å°„ã€åå°„å’Œé€å°„ç­‰ç°è±¡å¼•èµ·çš„å…‰çº¿ä¼ æ’­å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»å­¦ä¹ åˆ°çš„å…³äºé€æ˜ç‰©ä½“å…‰å­¦ç‰¹æ€§çš„å…ˆéªŒçŸ¥è¯†ã€‚ä½œè€…è®¤ä¸ºï¼Œå¦‚æœæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„é€æ˜æ•ˆæœï¼Œé‚£ä¹ˆå®ƒä¸€å®šå·²ç»å†…åŒ–äº†ç›¸å…³çš„ç‰©ç†è§„åˆ™ã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡è®­ç»ƒå°†è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºæ·±åº¦å’Œæ³•å‘é‡ä¼°è®¡å™¨ï¼Œä»è€Œè§£å†³é€æ˜ç‰©ä½“çš„æ„ŸçŸ¥é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ„å»ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†TransPhy3Dï¼ŒåŒ…å«é€æ˜å’Œåå°„åœºæ™¯çš„è§†é¢‘åºåˆ—ï¼Œå¹¶æä¾›ç²¾ç¡®çš„æ·±åº¦å’Œæ³•å‘é‡æ ‡ç­¾ã€‚2) åŸºäºå¤§å‹è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨LoRAé€‚é…å™¨å­¦ä¹ è§†é¢‘åˆ°è§†é¢‘çš„è½¬æ¢å™¨ï¼Œç”¨äºé¢„æµ‹æ·±åº¦å’Œæ³•å‘é‡ã€‚3) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†RGBå›¾åƒå’Œå¸¦å™ªå£°çš„æ·±åº¦æ½œåœ¨å˜é‡è¿æ¥åˆ°DiTéª¨å¹²ç½‘ç»œä¸­ï¼Œå¹¶åœ¨TransPhy3Då’Œç°æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†é¢‘æ‰©æ•£æ¨¡å‹ç”¨äºé€æ˜ç‰©ä½“çš„æ·±åº¦å’Œæ³•å‘é‡ä¼°è®¡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šçš„å‡ ä½•æˆ–å…‰åº¦å‡è®¾ï¼Œè€Œæ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥æ¨æ–­é€æ˜ç‰©ä½“çš„æ·±åº¦å’Œæ³•å‘é‡ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LoRAé€‚é…å™¨å¯ä»¥é«˜æ•ˆåœ°å°†å¤§å‹æ‰©æ•£æ¨¡å‹é€‚åº”äºæ–°çš„ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šTransPhy3Dæ•°æ®é›†åŒ…å«1.1ä¸‡ä¸ªè§†é¢‘åºåˆ—ï¼Œä½¿ç”¨Blender/Cyclesæ¸²æŸ“ï¼Œå¹¶ä½¿ç”¨OptiXé™å™ªã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨LoRAé€‚é…å™¨æ¥å¾®è°ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ï¼Œç»“åˆTransPhy3Då’Œç°æœ‰æ•°æ®é›†ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ·±åº¦å’Œæ³•å‘é‡çš„L1æŸå¤±ï¼Œä»¥åŠæ—¶é—´ä¸€è‡´æ€§æŸå¤±ã€‚æ¨¡å‹ä½¿ç”¨DiTä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶ä½¿ç”¨RGBå’Œæ·±åº¦æ½œåœ¨å˜é‡çš„è¿æ¥ä½œä¸ºè¾“å…¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23705v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23705v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23705v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

DKTåœ¨ClearPoseã€DREDSå’ŒTransPhy3D-Testç­‰æ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬SOTAï¼Œæ˜¾è‘—æé«˜äº†é€æ˜ç‰©ä½“æ·±åº¦å’Œæ³•å‘é‡ä¼°è®¡çš„å‡†ç¡®æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚åœ¨ClearPoseæ•°æ®é›†ä¸Šï¼Œæ³•å‘é‡ä¼°è®¡ç»“æœè¾¾åˆ°äº†æœ€ä½³æ°´å¹³ã€‚æ­¤å¤–ï¼Œä¸€ä¸ªç´§å‡‘çš„1.3Bç‰ˆæœ¬æ¨¡å‹è¿è¡Œé€Ÿåº¦è¾¾åˆ°0.17ç§’/å¸§ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚é›†æˆåˆ°æŠ“å–å †æ ˆåï¼ŒDKTæ˜¾è‘—æé«˜äº†åŠé€æ˜ã€åå°„å’Œæ¼«åå°„è¡¨é¢çš„æŠ“å–æˆåŠŸç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæŠ“å–ã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººæŠ“å–ä¸­ï¼Œå‡†ç¡®çš„æ·±åº¦ä¼°è®¡å¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°è¯†åˆ«å’ŒæŠ“å–é€æ˜æˆ–åå°„ç‰©ä½“ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æé«˜è½¦è¾†å¯¹é€æ˜ç‰©ä½“çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¾‹å¦‚æŒ¡é£ç»ç’ƒã€æ°´é¢ç­‰ã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼Œå¯ä»¥æ›´çœŸå®åœ°æ¸²æŸ“è™šæ‹Ÿç‰©ä½“ä¸çœŸå®é€æ˜ç‰©ä½“çš„äº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

