---
layout: default
title: "GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection"
---

# GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.23176" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.23176v1</a>
  <a href="https://arxiv.org/pdf/2512.23176.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.23176v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.23176v1', 'GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-29

**å¤‡æ³¨**: 11 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GVSynergy-Detï¼šååŒé«˜æ–¯-ä½“ç´ è¡¨ç¤ºç”¨äºå¤šè§†è§’3Dç›®æ ‡æ£€æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dç›®æ ‡æ£€æµ‹` `å¤šè§†è§’å‡ ä½•` `é«˜æ–¯æº…å°„` `ä½“ç´ è¡¨ç¤º` `æ— ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå›¾åƒçš„3Dç›®æ ‡æ£€æµ‹æ–¹æ³•éš¾ä»¥å…¼é¡¾é«˜ç²¾åº¦å’Œå¼±ç›‘ç£ï¼Œé«˜ç²¾åº¦æ–¹æ³•ä¾èµ–å¯†é›†3Dç›‘ç£ï¼Œè€Œå¼±ç›‘ç£æ–¹æ³•éš¾ä»¥æå–å‡†ç¡®å‡ ä½•ä¿¡æ¯ã€‚
2. GVSynergy-Deté€šè¿‡ååŒé«˜æ–¯-ä½“ç´ è¡¨ç¤ºå­¦ä¹ å¢å¼º3Dæ£€æµ‹ï¼Œåˆ©ç”¨é«˜æ–¯è¡¨ç¤ºçš„ç²¾ç»†è¡¨é¢ç»†èŠ‚å’Œä½“ç´ è¡¨ç¤ºçš„ç»“æ„åŒ–ç©ºé—´ä¸Šä¸‹æ–‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGVSynergy-Detåœ¨ScanNetV2å’ŒARKitScenesæ•°æ®é›†ä¸Šå–å¾—äº†SOTAç»“æœï¼Œä¸”æ— éœ€æ·±åº¦æˆ–å¯†é›†3Då‡ ä½•ç›‘ç£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGVSynergy-Detçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ååŒé«˜æ–¯-ä½“ç´ è¡¨ç¤ºå­¦ä¹ æ¥å¢å¼ºåŸºäºå›¾åƒçš„3Dç›®æ ‡æ£€æµ‹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯è¿ç»­é«˜æ–¯å’Œç¦»æ•£ä½“ç´ è¡¨ç¤ºèƒ½å¤Ÿæ•æ‰äº’è¡¥çš„å‡ ä½•ä¿¡æ¯ï¼šé«˜æ–¯æ“…é•¿å»ºæ¨¡ç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œè€Œä½“ç´ æä¾›ç»“æ„åŒ–çš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŒé‡è¡¨ç¤ºæ¶æ„ï¼Œè¯¥æ¶æ„ï¼šï¼ˆ1ï¼‰è°ƒæ•´é€šç”¨é«˜æ–¯æº…å°„ä»¥æå–ç”¨äºæ£€æµ‹ä»»åŠ¡çš„äº’è¡¥å‡ ä½•ç‰¹å¾ï¼›ï¼ˆ2ï¼‰å¼€å‘äº†ä¸€ç§è·¨è¡¨ç¤ºå¢å¼ºæœºåˆ¶ï¼Œåˆ©ç”¨é«˜æ–¯åœºçš„å‡ ä½•ç»†èŠ‚æ¥ä¸°å¯Œä½“ç´ ç‰¹å¾ã€‚ä¸ä»¥å¾€ä¾èµ–è€—æ—¶çš„å•åœºæ™¯ä¼˜åŒ–æˆ–ä»…ä½¿ç”¨é«˜æ–¯è¡¨ç¤ºè¿›è¡Œæ·±åº¦æ­£åˆ™åŒ–çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„ååŒç­–ç•¥é€šè¿‡å¯å­¦ä¹ çš„é›†æˆç›´æ¥åˆ©ç”¨æ¥è‡ªä¸¤ç§è¡¨ç¤ºçš„ç‰¹å¾ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„ç›®æ ‡å®šä½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGVSynergy-Detåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å†…åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨ScanNetV2å’ŒARKitScenesæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»ä½•æ·±åº¦æˆ–å¯†é›†3Då‡ ä½•ç›‘ç£ï¼ˆä¾‹å¦‚ï¼Œç‚¹äº‘æˆ–TSDFï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŸºäºå›¾åƒçš„3Dç›®æ ‡æ£€æµ‹æ—¨åœ¨ä»…ä½¿ç”¨RGBå›¾åƒè¯†åˆ«å’Œå®šä½3Dç©ºé—´ä¸­çš„å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¯†é›†çš„3Dç›‘ç£æ‰èƒ½å®ç°é«˜ç²¾åº¦ï¼Œè¦ä¹ˆåœ¨æ²¡æœ‰è¿™ç§ç›‘ç£çš„æƒ…å†µä¸‹éš¾ä»¥ä»å›¾åƒä¸­æå–å‡†ç¡®çš„å‡ ä½•ä¿¡æ¯ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨å¼±ç›‘ç£æˆ–æ— ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œæå‡åŸºäºå›¾åƒçš„3Dç›®æ ‡æ£€æµ‹ç²¾åº¦æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é«˜æ–¯è¡¨ç¤ºå’Œä½“ç´ è¡¨ç¤ºçš„äº’è¡¥æ€§ã€‚é«˜æ–¯è¡¨ç¤ºæ“…é•¿æ•æ‰ç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œè€Œä½“ç´ è¡¨ç¤ºæä¾›ç»“æ„åŒ–çš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚é€šè¿‡ååŒåˆ©ç”¨è¿™ä¸¤ç§è¡¨ç¤ºï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ã€‚è¿™ç§ååŒä½œç”¨é¿å…äº†å¯¹å¯†é›†3Dç›‘ç£çš„ä¾èµ–ï¼Œå¹¶æå‡äº†å¼±ç›‘ç£æ¡ä»¶ä¸‹çš„å‡ ä½•ä¿¡æ¯æå–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGVSynergy-Detæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé«˜æ–¯ç‰¹å¾æå–æ¨¡å—å’Œä½“ç´ ç‰¹å¾å¢å¼ºæ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ”¹è¿›çš„é«˜æ–¯æº…å°„ï¼ˆGaussian Splattingï¼‰ä»å¤šè§†è§’å›¾åƒä¸­æå–é«˜æ–¯ç‰¹å¾ã€‚ç„¶åï¼Œå°†åœºæ™¯åˆ’åˆ†ä¸ºä½“ç´ ï¼Œå¹¶æå–ä½“ç´ ç‰¹å¾ã€‚æ¥ç€ï¼Œé€šè¿‡è·¨è¡¨ç¤ºå¢å¼ºæœºåˆ¶ï¼Œåˆ©ç”¨é«˜æ–¯ç‰¹å¾æ¥ä¸°å¯Œä½“ç´ ç‰¹å¾ã€‚æœ€åï¼Œå°†å¢å¼ºåçš„ä½“ç´ ç‰¹å¾è¾“å…¥åˆ°3Dç›®æ ‡æ£€æµ‹å™¨ä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ååŒé«˜æ–¯-ä½“ç´ è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸æ˜¯ç®€å•åœ°å°†é«˜æ–¯è¡¨ç¤ºä½œä¸ºæ·±åº¦æ­£åˆ™åŒ–é¡¹ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨é«˜æ–¯ç‰¹å¾å’Œä½“ç´ ç‰¹å¾è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æå‡ºäº†è·¨è¡¨ç¤ºå¢å¼ºæœºåˆ¶ï¼Œåˆ©ç”¨é«˜æ–¯ç‰¹å¾æ¥å¢å¼ºä½“ç´ ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ä¸¤ç§è¡¨ç¤ºçš„äº’è¡¥æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸Šï¼Œé‡‡ç”¨äº†å¯å­¦ä¹ çš„é›†æˆæ–¹å¼æ¥èåˆé«˜æ–¯ç‰¹å¾å’Œä½“ç´ ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ ä¸åŒç‰¹å¾çš„é‡è¦æ€§ï¼Œå¹¶æ ¹æ®é‡è¦æ€§å¯¹ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†é«˜æ–¯è¡¨ç¤ºå’Œä½“ç´ è¡¨ç¤ºçš„ç‰¹ç‚¹ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒçš„KLæ•£åº¦æ¥çº¦æŸé«˜æ–¯è¡¨ç¤ºçš„å½¢çŠ¶ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±æ¥è®­ç»ƒç›®æ ‡æ£€æµ‹å™¨ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23176v1/model_size_performance_comparison.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23176v1/x1.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.23176v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

GVSynergy-Detåœ¨ScanNetV2å’ŒARKitScenesæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ScanNetV2æ•°æ®é›†ä¸Šï¼ŒGVSynergy-Detåœ¨ä¸ä½¿ç”¨ä»»ä½•æ·±åº¦ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºå›¾åƒçš„3Dç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚åœ¨ARKitScenesæ•°æ®é›†ä¸Šï¼ŒGVSynergy-Detä¹Ÿå–å¾—äº†SOTAç»“æœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GVSynergy-Detåœ¨å®¤å†…åœºæ™¯ç†è§£ã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚è¯¥æ–¹æ³•æ— éœ€æ·±åº¦ä¼ æ„Ÿå™¨ï¼Œä»…ä¾èµ–RGBå›¾åƒå³å¯å®ç°é«˜ç²¾åº¦çš„3Dç›®æ ‡æ£€æµ‹ï¼Œé™ä½äº†ç¡¬ä»¶æˆæœ¬ï¼Œå¹¶æé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

