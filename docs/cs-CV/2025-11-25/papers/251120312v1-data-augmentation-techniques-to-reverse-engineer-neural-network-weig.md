---
layout: default
title: Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries
---

# Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries

**arXiv**: [2511.20312v1](https://arxiv.org/abs/2511.20312) | [PDF](https://arxiv.org/pdf/2511.20312.pdf)

**ä½œè€…**: Alexander Beiser, Flavio Martinelli, Wulfram Gerstner, Johanni Brea

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®šåˆ¶æ•°æ®å¢žå¼ºæŠ€æœ¯ä»¥åœ¨å‚æ•°å¤šäºŽè®­ç»ƒæ•°æ®æ—¶æ¢å¤ç¥žç»ç½‘ç»œæƒé‡**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œæƒé‡æ¢å¤` `æ•°æ®å¢žå¼º` `æ•™å¸ˆ-å­¦ç”Ÿè®¾ç½®` `è¡¨ç¤ºç©ºé—´é‡‡æ ·` `è¿‡æ‹Ÿåˆç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•™å¸ˆç½‘ç»œå‚æ•°å¤šäºŽè®­ç»ƒæ•°æ®æ—¶ï¼Œå­¦ç”Ÿç½‘ç»œè¿‡æ‹ŸåˆæŸ¥è¯¢ï¼Œæ— æ³•å¯¹é½æƒé‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ–°æ•°æ®å¢žå¼ºæŠ€æœ¯ï¼Œä¼˜åŒ–æ•™å¸ˆéšè—å±‚è¡¨ç¤ºç©ºé—´çš„é‡‡æ ·ã€‚
3. å®žéªŒæ•ˆæžœï¼šæ‰©å±•å¯æ¢å¤ç½‘ç»œè§„æ¨¡ï¼Œå‚æ•°æ¯”è®­ç»ƒæ•°æ®ç‚¹å¤šè¾¾100å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Network weights can be reverse-engineered given enough informative samples of a network's input-output function. In a teacher-student setup, this translates into collecting a dataset of the teacher mapping -- querying the teacher -- and fitting a student to imitate such mapping. A sensible choice of queries is the dataset the teacher is trained on. But current methods fail when the teacher parameters are more numerous than the training data, because the student overfits to the queries instead of aligning its parameters to the teacher. In this work, we explore augmentation techniques to best sample the input-output mapping of a teacher network, with the goal of eliciting a rich set of representations from the teacher hidden layers. We discover that standard augmentations such as rotation, flipping, and adding noise, bring little to no improvement to the identification problem. We design new data augmentation techniques tailored to better sample the representational space of the network's hidden layers. With our augmentations we extend the state-of-the-art range of recoverable network sizes. To test their scalability, we show that we can recover networks of up to 100 times more parameters than training data-points.

