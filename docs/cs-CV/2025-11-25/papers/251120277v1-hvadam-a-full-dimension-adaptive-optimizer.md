---
layout: default
title: HVAdam: A Full-Dimension Adaptive Optimizer
---

# HVAdam: A Full-Dimension Adaptive Optimizer

**arXiv**: [2511.20277v1](https://arxiv.org/abs/2511.20277) | [PDF](https://arxiv.org/pdf/2511.20277.pdf)

**ä½œè€…**: Yiheng Zhang, Shaowu Wu, Yuanzhuo Xu, Jiajun Wu, Shang Xu, Steve Drew, Xiaoguang Niu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAnonä¼˜åŒ–å™¨ï¼Œé€šè¿‡å¯è°ƒè‡ªé€‚åº”è§£å†³Adamæ³›åŒ–å·®é—®é¢˜**

**å…³é”®è¯**: `è‡ªé€‚åº”ä¼˜åŒ–å™¨` `æ·±åº¦å­¦ä¹ ä¼˜åŒ–` `æ”¶æ•›ä¿è¯` `å›¾åƒåˆ†ç±»` `è¯­è¨€å»ºæ¨¡` `æ‰©æ•£æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAdamç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨åœ¨CNNç­‰æž¶æž„ä¸Šæ³›åŒ–èƒ½åŠ›ä¸å¦‚SGD
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¯è°ƒè‡ªé€‚åº”æœºåˆ¶å’Œå¢žé‡å»¶è¿Ÿæ›´æ–°ï¼Œç¡®ä¿æ”¶æ•›
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒåˆ†ç±»ã€æ‰©æ•£å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ä¼˜äºŽçŽ°æœ‰ä¼˜åŒ–å™¨

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
>   , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

