---
layout: default
title: Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance
---

# Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance

**arXiv**: [2511.19909v1](https://arxiv.org/abs/2511.19909) | [PDF](https://arxiv.org/pdf/2511.19909.pdf)

**ä½œè€…**: Haoxuan Wang, Jiachen Tao, Junyi Wu, Gaowen Liu, Ramana Rao Kompella, Yan Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMotion Marionetteæ¡†æž¶ï¼Œé€šè¿‡å†…éƒ¨å…ˆéªŒå®žçŽ°å•ç›®è§†é¢‘åˆ°å›¾åƒçš„é›¶æ ·æœ¬åˆšæ€§è¿åŠ¨è¿ç§»ã€‚**

**å…³é”®è¯**: `åˆšæ€§è¿åŠ¨è¿ç§»` `é›¶æ ·æœ¬å­¦ä¹ ` `ç©ºé—´-æ—¶é—´å…ˆéªŒ` `é€Ÿåº¦åœºåˆæˆ` `è§†é¢‘ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨å…ˆéªŒï¼Œå¯¼è‡´æ³›åŒ–æ€§ä¸Žæ—¶é—´ä¸€è‡´æ€§é—´çš„æƒè¡¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºç©ºé—´-æ—¶é—´å…ˆéªŒï¼Œé›†æˆç›®æ ‡å¯¹è±¡ç”Ÿæˆå¯æŽ§é€Ÿåº¦åœºï¼Œå¹¶ä¼˜åŒ–è§†è§‰è¿žè´¯æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¡†æž¶æ³›åŒ–æ€§å¼ºï¼Œç”Ÿæˆè§†é¢‘æ—¶é—´ä¸€è‡´ï¼Œæ”¯æŒå¯æŽ§è§†é¢‘ç”Ÿæˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.

