---
layout: default
title: Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance
---

# Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance

**arXiv**: [2511.19909v1](https://arxiv.org/abs/2511.19909) | [PDF](https://arxiv.org/pdf/2511.19909.pdf)

**ä½œè€…**: Haoxuan Wang, Jiachen Tao, Junyi Wu, Gaowen Liu, Ramana Rao Kompella, Yan Yan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMotion Marionetteä»¥è§£å†³åˆšæ€§è¿åŠ¨è½¬ç§»é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `åˆšæ€§è¿åŠ¨è½¬ç§»` `æ—¶ç©ºå…ˆéªŒ` `è§†é¢‘ç”Ÿæˆ` `å¯æŽ§ç”Ÿæˆ` `è§†è§‰ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨å‡ ä½•æˆ–ç”Ÿæˆå…ˆéªŒï¼Œå¯¼è‡´åœ¨å¯æ³›åŒ–æ€§ä¸Žæ—¶é—´ä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡å†…éƒ¨æ—¶ç©ºå…ˆéªŒæ¥æŒ‡å¯¼è¿åŠ¨è½¬ç§»ï¼Œé¿å…äº†å¤–éƒ¨çº¦æŸçš„å½±å“ï¼Œæå‡äº†çµæ´»æ€§ã€‚
3. å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒMotion Marionetteåœ¨å¤šç§å¯¹è±¡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œç”Ÿæˆçš„è§†é¢‘åœ¨æ—¶é—´ä¸Šä¿æŒä¸€è‡´ï¼Œå¹¶èƒ½è¿›è¡Œå¯æŽ§ç”Ÿæˆã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Motion Marionetteï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶-shotæ¡†æž¶ï¼Œæ—¨åœ¨å°†å•ç›®æºè§†é¢‘ä¸­çš„åˆšæ€§è¿åŠ¨è½¬ç§»åˆ°å•è§†å›¾ç›®æ ‡å›¾åƒä¸­ã€‚ä»¥å¾€çš„ç ”ç©¶é€šå¸¸ä¾èµ–å‡ ä½•ã€ç”Ÿæˆæˆ–ä»¿çœŸå…ˆéªŒæ¥æŒ‡å¯¼è½¬ç§»è¿‡ç¨‹ï¼Œä½†è¿™äº›å¤–éƒ¨å…ˆéªŒå¼•å…¥äº†è¾…åŠ©çº¦æŸï¼Œå¯¼è‡´äº†å¯æ³›åŒ–æ€§ä¸Žæ—¶é—´ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å†…éƒ¨å…ˆéªŒæ¥æŒ‡å¯¼è¿åŠ¨è½¬ç§»è¿‡ç¨‹ï¼Œè¯¥å…ˆéªŒä¸“é—¨æ•æ‰æºè§†é¢‘ä¸Žä»»ä½•è½¬ç§»ç›®æ ‡è§†é¢‘ä¹‹é—´çš„æ—¶ç©ºå˜æ¢ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æºè§†é¢‘å’Œç›®æ ‡å›¾åƒæå‡åˆ°ç»Ÿä¸€çš„3Dè¡¨ç¤ºç©ºé—´ï¼Œä»Žæºè§†é¢‘ä¸­æå–è¿åŠ¨è½¨è¿¹ä»¥æž„å»ºç‹¬ç«‹äºŽç‰©ä½“å‡ ä½•å’Œè¯­ä¹‰çš„æ—¶ç©ºå…ˆéªŒï¼Œç¼–ç ç›¸å¯¹çš„ç©ºé—´å˜åŒ–ã€‚è¯¥å…ˆéªŒä¸Žç›®æ ‡å¯¹è±¡ç»“åˆï¼Œåˆæˆå¯æŽ§çš„é€Ÿåº¦åœºï¼Œå¹¶é€šè¿‡åŸºäºŽä½ç½®çš„åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç»†åŒ–ï¼Œä»¥å‡å°‘ä¼ªå½±å¹¶å¢žå¼ºè§†è§‰ä¸€è‡´æ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMotion Marionetteèƒ½å¤Ÿåœ¨ä¸åŒå¯¹è±¡é—´æ³›åŒ–ï¼Œç”Ÿæˆä¸Žæºè¿åŠ¨é«˜åº¦ä¸€è‡´çš„æ—¶é—´ä¸€è‡´æ€§è§†é¢‘ï¼Œå¹¶æ”¯æŒå¯æŽ§çš„è§†é¢‘ç”Ÿæˆã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åˆšæ€§è¿åŠ¨è½¬ç§»çš„é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨å…ˆéªŒï¼Œå¯¼è‡´å¯æ³›åŒ–æ€§ä¸Žæ—¶é—´ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡å†…éƒ¨å…ˆéªŒæ¥æŒ‡å¯¼è¿åŠ¨è½¬ç§»ï¼Œè¯¥å…ˆéªŒæ•æ‰æºè§†é¢‘ä¸Žç›®æ ‡è§†é¢‘ä¹‹é—´çš„æ—¶ç©ºå˜æ¢ï¼Œé¿å…äº†å¤–éƒ¨çº¦æŸçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬å°†æºè§†é¢‘å’Œç›®æ ‡å›¾åƒæå‡åˆ°ç»Ÿä¸€çš„3Dè¡¨ç¤ºç©ºé—´ï¼Œæå–è¿åŠ¨è½¨è¿¹æž„å»ºæ—¶ç©ºå…ˆéªŒï¼Œå¹¶ä¸Žç›®æ ‡å¯¹è±¡ç»“åˆç”Ÿæˆé€Ÿåº¦åœºï¼Œæœ€åŽé€šè¿‡åŸºäºŽä½ç½®çš„åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºŽå¼•å…¥å†…éƒ¨æ—¶ç©ºå…ˆéªŒï¼Œç‹¬ç«‹äºŽç‰©ä½“å‡ ä½•å’Œè¯­ä¹‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼–ç ç›¸å¯¹ç©ºé—´å˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†è¿åŠ¨è½¬ç§»çš„çµæ´»æ€§ä¸Žä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæž„å»ºçš„æ—¶ç©ºå…ˆéªŒé€šè¿‡è¿åŠ¨è½¨è¿¹æå–ï¼Œé€Ÿåº¦åœºçš„åˆæˆä¸Žç²¾ç»†åŒ–å¤„ç†é‡‡ç”¨åŸºäºŽä½ç½®çš„åŠ¨åŠ›å­¦æ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘çš„è§†è§‰ä¸€è‡´æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMotion Marionetteåœ¨å¤šç§å¯¹è±¡ä¸Šå‡èƒ½æœ‰æ•ˆæ³›åŒ–ï¼Œç”Ÿæˆçš„è§†é¢‘åœ¨æ—¶é—´ä¸€è‡´æ€§ä¸Šä¸Žæºè¿åŠ¨é«˜åº¦å¯¹é½ã€‚ä¸ŽåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆè§†é¢‘çš„è´¨é‡æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªæä¾›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŠ¨ç”»åˆ¶ä½œã€è™šæ‹ŸçŽ°å®žå’Œæ¸¸æˆå¼€å‘ç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜æ•ˆçš„è¿åŠ¨è½¬ç§»è§£å†³æ–¹æ¡ˆï¼Œæå‡å†…å®¹ç”Ÿæˆçš„çµæ´»æ€§å’Œè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“å®žæ—¶è§†é¢‘ç¼–è¾‘å’Œäº¤äº’å¼åª’ä½“çš„ç”Ÿæˆæ–¹å¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.

