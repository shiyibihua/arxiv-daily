---
layout: default
title: Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum
---

# Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum

**arXiv**: [2511.19846v1](https://arxiv.org/abs/2511.19846) | [PDF](https://arxiv.org/pdf/2511.19846.pdf)

**ä½œè€…**: Thomas M Metz, Matthew Q Hill, Alice J O'Toole

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤é”™å¤šåŸŸèº«ä»½è¯¾ç¨‹ä»¥åœ¨ç»Ÿä¸€åµŒå…¥ç©ºé—´ä¸­å®žçŽ°å¤šä»»åŠ¡åˆ†ç±»ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜**

**å…³é”®è¯**: `å¤šä»»åŠ¡å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `åµŒå…¥ç©ºé—´ç»Ÿä¸€` `åŸºç¡€æ¨¡åž‹å¾®è°ƒ` `äº¤é”™è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰åŸºç¡€æ¨¡åž‹å¾®è°ƒåŽæ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œéš¾ä»¥åŒæ—¶å¤„ç†å¤šä»»åŠ¡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥äº¤é”™å¤šåŸŸèº«ä»½è¯¾ç¨‹ï¼ŒåŒæ­¥å¾®è°ƒåŸºç¡€æ¨¡åž‹äºŽå››ä¸ªä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EVA-02å’ŒCLIPä¸Šè¡¨çŽ°åª²ç¾Žä¸“å®¶ï¼Œä¼˜äºŽäººç±»å¤šä»»åŠ¡èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision foundation models can perform generalized object classification in zero-shot mode, and face/person recognition when they are fine-tuned. However, fine-tuned models suffer from catastrophic forgetting. We create models that perform four tasks (object recognition, face recognition from high- and low-quality images, and person recognition from whole-body images) in a single embedding space -- without incurring substantial catastrophic forgetting. To accomplish this, we introduce two variants of the Interleaved Multi-Domain Identity Curriculum (IMIC): a gradient-coupled, interleaving training schedule that fine-tunes a foundation backbone simultaneously on all four tasks. The IMIC method proved effective with three foundation model bases: DINOv3, CLIP, and EVA-02. Two of these (EVA-02 and CLIP) performed comparably with domain experts on all four tasks concurrently and were more accurate than humans at multi-tasking across face, body, and object datasets. Further, we demonstrate that our approach does not substantially harm out-of-distribution generalization, thus maintaining a key property of foundation models. Analysis of the most accurate model variants (EVA-02 + IMIC A and B) showed linearly separable representations of the four tasks in the unified embedding space, but with substantial sharing of features across tasks. Fewer than 100 PCs calculated from any one task could perform all other tasks with nearly zero performance degradation.

