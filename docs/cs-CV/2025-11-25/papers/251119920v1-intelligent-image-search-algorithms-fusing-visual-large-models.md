---
layout: default
title: Intelligent Image Search Algorithms Fusing Visual Large Models
---

# Intelligent Image Search Algorithms Fusing Visual Large Models

**arXiv**: [2511.19920v1](https://arxiv.org/abs/2511.19920) | [PDF](https://arxiv.org/pdf/2511.19920.pdf)

**ä½œè€…**: Kehan Wang, Tingqiong Cui, Yang Zhang, Yu Chen, Shifeng Wu, Zhenzhang Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDetVLMæ¡†æž¶ï¼Œèžåˆç›®æ ‡æ£€æµ‹ä¸Žè§†è§‰å¤§æ¨¡åž‹ï¼Œè§£å†³ç»†ç²’åº¦å›¾åƒæ£€ç´¢ä¸­çš„çŠ¶æ€æœç´¢å’Œé›¶æ ·æœ¬æœç´¢é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç»†ç²’åº¦å›¾åƒæ£€ç´¢` `è§†è§‰å¤§æ¨¡åž‹` `ç›®æ ‡æ£€æµ‹` `é›¶æ ·æœ¬æœç´¢` `çŠ¶æ€æœç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•åœ¨ç»†ç²’åº¦å›¾åƒæ£€ç´¢ä¸­ç¼ºä¹çŠ¶æ€åˆ¤æ–­å’Œé›¶æ ·æœ¬èƒ½åŠ›ï¼Œè§†è§‰å¤§æ¨¡åž‹ç©ºé—´å®šä½å·®ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼ŒYOLOæ£€æµ‹å™¨è¿›è¡Œç»„ä»¶ç­›é€‰ï¼Œè§†è§‰å¤§æ¨¡åž‹è¿›è¡ŒäºŒæ¬¡éªŒè¯å’ŒçŠ¶æ€åˆ¤æ–­ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨è½¦è¾†ç»„ä»¶æ•°æ®é›†ä¸Šï¼Œæ•´ä½“æ£€ç´¢å‡†ç¡®çŽ‡è¾¾94.82%ï¼Œé›¶æ ·æœ¬æœç´¢å‡†ç¡®çŽ‡è¾¾94.95%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained image retrieval, which aims to find images containing specific object components and assess their detailed states, is critical in fields like security and industrial inspection. However, conventional methods face significant limitations: manual features (e.g., SIFT) lack robustness; deep learning-based detectors (e.g., YOLO) can identify component presence but cannot perform state-specific retrieval or zero-shot search; Visual Large Models (VLMs) offer semantic and zero-shot capabilities but suffer from poor spatial grounding and high computational cost, making them inefficient for direct retrieval. To bridge these gaps, this paper proposes DetVLM, a novel intelligent image search framework that synergistically fuses object detection with VLMs. The framework pioneers a search-enhancement paradigm via a two-stage pipeline: a YOLO detector first conducts efficient, high-recall component-level screening to determine component presence; then, a VLM acts as a recall-enhancement unit, performing secondary verification for components missed by the detector. This architecture directly enables two advanced capabilities: 1) State Search: Guided by task-specific prompts, the VLM refines results by verifying component existence and executing sophisticated state judgments (e.g., "sun visor lowered"), allowing retrieval based on component state. 2) Zero-shot Search: The framework leverages the VLM's inherent zero-shot capability to recognize and retrieve images containing unseen components or attributes (e.g., "driver wearing a mask") without any task-specific training. Experiments on a vehicle component dataset show DetVLM achieves a state-of-the-art overall retrieval accuracy of 94.82\%, significantly outperforming detection-only baselines. It also attains 94.95\% accuracy in zero-shot search for driver mask-wearing and over 90\% average accuracy in state search tasks.

