---
layout: default
title: PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling
---

# PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling

**arXiv**: [2511.20251v1](https://arxiv.org/abs/2511.20251) | [PDF](https://arxiv.org/pdf/2511.20251.pdf)

**ä½œè€…**: Bo-Kai Ruan, Teng-Fang Hsiao, Ling Lo, Yi-Lun Wu, Hong-Han Shuai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPromptMoGæ–¹æ³•ä»¥è§£å†³é•¿æç¤ºå›¾åƒç”Ÿæˆä¸­çš„å¤šæ ·æ€§ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `é•¿æç¤ºç”Ÿæˆ` `å¤šæ ·æ€§å¢žå¼º` `æ··åˆé«˜æ–¯é‡‡æ ·` `è®­ç»ƒå…è´¹æ–¹æ³•` `è¯­ä¹‰ä¿æŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿æç¤ºåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å¢žå¼ºä¿çœŸåº¦ä½†æŠ‘åˆ¶å¤šæ ·æ€§ï¼Œå¯¼è‡´è¾“å‡ºé‡å¤
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ··åˆé«˜æ–¯é‡‡æ ·æç¤ºåµŒå…¥ï¼Œå¢žåŠ é‡‡æ ·ç†µä»¥æå‡å¤šæ ·æ€§ï¼Œæ— éœ€è®­ç»ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå…ˆè¿›æ¨¡åž‹ä¸ŠéªŒè¯ï¼ŒPromptMoGä¸€è‡´æ”¹å–„å¤šæ ·æ€§ä¸”ä¿æŒè¯­ä¹‰

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in text-to-image (T2I) generation have achieved remarkable visual outcomes through large-scale rectified flow models. However, how these models behave under long prompts remains underexplored. Long prompts encode rich content, spatial, and stylistic information that enhances fidelity but often suppresses diversity, leading to repetitive and less creative outputs. In this work, we systematically study this fidelity-diversity dilemma and reveal that state-of-the-art models exhibit a clear drop in diversity as prompt length increases. To enable consistent evaluation, we introduce LPD-Bench, a benchmark designed for assessing both fidelity and diversity in long-prompt generation. Building on our analysis, we develop a theoretical framework that increases sampling entropy through prompt reformulation and propose a training-free method, PromptMoG, which samples prompt embeddings from a Mixture-of-Gaussians in the embedding space to enhance diversity while preserving semantics. Extensive experiments on four state-of-the-art models, SD3.5-Large, Flux.1-Krea-Dev, CogView4, and Qwen-Image, demonstrate that PromptMoG consistently improves long-prompt generation diversity without semantic drifting.

