---
layout: default
title: While recognizing actions, LMMs struggle to detect core interaction events
---

# While recognizing actions, LMMs struggle to detect core interaction events

**arXiv**: [2511.20162v1](https://arxiv.org/abs/2511.20162) | [PDF](https://arxiv.org/pdf/2511.20162.pdf)

**ä½œè€…**: Daniel Harari, Michael Sidorov, Liel David, Chen Shterental, Abrham Kahsay Gebreselasie, Muhammad Haris Khan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤§è§„æ¨¡äº¤äº’äº‹ä»¶æ•°æ®é›†ï¼Œæ­ç¤ºLMMsåœ¨æ£€æµ‹ç‰©ç†æŽ¥è§¦äº‹ä»¶æ—¶ç¼ºä¹æ„ŸçŸ¥åŸºç¡€ã€‚**

**å…³é”®è¯**: `å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡åž‹` `è§†é¢‘äº¤äº’äº‹ä»¶æ£€æµ‹` `æ„ŸçŸ¥åŸºç¡€` `æ•°æ®é›†æž„å»º` `ç‰©ç†æŽ¥è§¦å®šä½` `åŠ¨ä½œè¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLMMsåœ¨è§†é¢‘ä¸­éš¾ä»¥å®šä½äº¤äº’å¼€å§‹æˆ–ç»“æŸçš„å¸§å’Œä½ç½®ï¼Œå°½ç®¡èƒ½è¯†åˆ«åŠ¨ä½œå’Œå¯¹è±¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé¦–ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ ‡æ³¨20K+è§†é¢‘äº¤äº’äº‹ä»¶ï¼ŒåŒ…æ‹¬æŽ¥è§¦å’Œé‡Šæ”¾æ—¶åˆ»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæµ‹è¯•Qwen-2.5VLå’ŒGPT-4oï¼Œæ¨¡åž‹æ— æ³•å‡†ç¡®æ£€æµ‹äº‹ä»¶å¸§å’Œä½ç½®ï¼Œæ˜¾ç¤ºæ„ŸçŸ¥åŸºç¡€ä¸è¶³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.

