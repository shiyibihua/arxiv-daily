---
layout: default
title: MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models
---

# MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models

**arXiv**: [2511.20629v1](https://arxiv.org/abs/2511.20629) | [PDF](https://arxiv.org/pdf/2511.20629.pdf)

**ä½œè€…**: Chieh-Yun Chen, Zhonghao Wang, Qi Chen, Zhifan Ye, Min Shi, Yue Zhao, Yinan Zhao, Hui Qu, Wei-An Lin, Yiru Shen, Ajinkya Kale, Irfan Essa, Humphrey Shi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMapReduce LoRAä¸ŽRaTEä»¥è§£å†³ç”Ÿæˆæ¨¡åž‹å¤šåå¥½ä¼˜åŒ–ä¸­çš„å¯¹é½ç¨Žé—®é¢˜**

**å…³é”®è¯**: `å¤šåå¥½ä¼˜åŒ–` `LoRAä¸“å®¶è®­ç»ƒ` `å¥–åŠ±ç‰¹å®šåµŒå…¥` `ç”Ÿæˆæ¨¡åž‹å¯¹é½` `å¼ºåŒ–å­¦ä¹ åé¦ˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šå¥–åŠ±è”åˆä¼˜åŒ–å¸¸å¯¼è‡´å¯¹é½ç¨Žï¼Œæ”¹å–„ä¸€ç»´å´æŸå®³å…¶ä»–ç»´åº¦
2. MapReduce LoRAå¹¶è¡Œè®­ç»ƒåå¥½ä¸“å®¶å¹¶è¿­ä»£åˆå¹¶ï¼ŒRaTEå­¦ä¹ å¥–åŠ±ç‰¹å®šåµŒå…¥ä»¥çµæ´»æŽ§åˆ¶
3. å®žéªŒåœ¨æ–‡æœ¬åˆ°å›¾åƒã€è§†é¢‘å’Œè¯­è¨€ä»»åŠ¡ä¸­æ˜¾è‘—æå‡å¤šé¡¹æŒ‡æ ‡ï¼Œè®¾å®šæ–°SOTA

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.

