---
layout: default
title: Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning
---

# Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning

**arXiv**: [2511.19942v1](https://arxiv.org/abs/2511.19942) | [PDF](https://arxiv.org/pdf/2511.19942.pdf)

**ä½œè€…**: Jingchu Gai, Guanning Zeng, Huaqing Zhang, Aditi Raghunathan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå·®åˆ†å¹³æ»‘æ–¹æ³•ä»¥ç¼“è§£å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„å¤šæ ·æ€§å´©æºƒé—®é¢˜**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `å¤šæ ·æ€§å´©æºƒ` `å·®åˆ†å¹³æ»‘` `è¯­è¨€æ¨¡åž‹æŽ¨ç†` `æ•°å­¦æŽ¨ç†` `å¥–åŠ±ä¿®æ”¹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒå¯¼è‡´å¤šæ ·æ€§å´©æºƒï¼Œè¾“å‡ºç¼ºä¹å¤šæ ·æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæ­£ç¡®è½¨è¿¹åº”ç”¨å·®åˆ†å¹³æ»‘ï¼Œç†è®ºä¸Šæå‡æ­£ç¡®æ€§å’Œå¤šæ ·æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨1Bè‡³7Bæ¨¡åž‹ä¸Šå®žéªŒï¼ŒPass@1å’ŒPass@kæŒ‡æ ‡æå‡ï¼ŒAIME24æ•°æ®é›†æ”¹è¿›6.7%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

