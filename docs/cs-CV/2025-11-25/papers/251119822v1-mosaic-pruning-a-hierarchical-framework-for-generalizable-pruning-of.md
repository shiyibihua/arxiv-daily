---
layout: default
title: Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models
---

# Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models

**arXiv**: [2511.19822v1](https://arxiv.org/abs/2511.19822) | [PDF](https://arxiv.org/pdf/2511.19822.pdf)

**ä½œè€…**: Wentao Hu, Mingkuan Zhao, Shuangyong Song, Xiaoyan Zhu, Xin Lai, Jiayin Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMosaic Pruningä»¥è§£å†³ç¨€ç–ä¸“å®¶æ¨¡åž‹å‰ªæžçš„æ³›åŒ–æ€§é—®é¢˜**

**å…³é”®è¯**: `ç¨€ç–ä¸“å®¶æ¨¡åž‹` `æ¨¡åž‹å‰ªæž` `æ³›åŒ–æ€§` `èšç±»é€‰æ‹©` `æ¿€æ´»å˜å¼‚æ€§è¯„åˆ†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¨€ç–ä¸“å®¶æ¨¡åž‹å‰ªæžåŽè·¨åŸŸæ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œéœ€é‡å¤å‰ªæž
2. é€šè¿‡èšç±»é€‰æ‹©æž„å»ºåŠŸèƒ½äº’è¡¥ä¸“å®¶é›†ï¼Œæå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›
3. å®žéªŒæ˜¾ç¤ºåœ¨é€šç”¨å’Œä¸“ç”¨ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.

