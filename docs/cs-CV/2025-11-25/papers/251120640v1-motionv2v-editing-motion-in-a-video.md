---
layout: default
title: MotionV2V: Editing Motion in a Video
---

# MotionV2V: Editing Motion in a Video

**arXiv**: [2511.20640v1](https://arxiv.org/abs/2511.20640) | [PDF](https://arxiv.org/pdf/2511.20640.pdf)

**ä½œè€…**: Ryan Burgert, Charles Herrmann, Forrester Cole, Michael S Ryoo, Neal Wadhwa, Andrey Voynov, Nataniel Ruiz

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMotionV2Væ–¹æ³•ï¼Œé€šè¿‡ç¼–è¾‘ç¨€ç–è½¨è¿¹å®žçŽ°è§†é¢‘è¿åŠ¨ç¼–è¾‘**

**å…³é”®è¯**: `è§†é¢‘è¿åŠ¨ç¼–è¾‘` `ç¨€ç–è½¨è¿¹ç¼–è¾‘` `è¿åŠ¨åäº‹å®žç”Ÿæˆ` `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `ç”¨æˆ·åå¥½ç ”ç©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘ç¼–è¾‘æ–¹æ³•éš¾ä»¥å®žçŽ°ç²¾ç¡®è¿åŠ¨æŽ§åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šæå–å¹¶ç¼–è¾‘è¾“å…¥è§†é¢‘çš„ç¨€ç–è½¨è¿¹ï¼Œç»“åˆç”Ÿæˆæ¨¡åž‹ç”Ÿæˆè¿åŠ¨åäº‹å®žè§†é¢‘å¯¹
3. å®žéªŒæˆ–æ•ˆæžœï¼šç”¨æˆ·ç ”ç©¶ä¸­åå¥½çŽ‡è¶…65%ï¼Œæ”¯æŒä»»æ„æ—¶é—´æˆ³ç¼–è¾‘å’Œè‡ªç„¶ä¼ æ’­

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V

