---
layout: default
title: GazeProphetV2: Head-Movement-Based Gaze Prediction Enabling Efficient Foveated Rendering on Mobile VR
---

# GazeProphetV2: Head-Movement-Based Gaze Prediction Enabling Efficient Foveated Rendering on Mobile VR

**arXiv**: [2511.19988v1](https://arxiv.org/abs/2511.19988) | [PDF](https://arxiv.org/pdf/2511.19988.pdf)

**ä½œè€…**: Farhaan Ebadulla, Chiraag Mudlpaur, Shreya Chaurasia, Gaurav BV

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ–¹æ³•ç»“åˆå¤´éƒ¨è¿åŠ¨é¢„æµ‹VRæ³¨è§†ï¼Œä»¥ä¼˜åŒ–ç§»åŠ¨VRæ¸²æŸ“æ•ˆçŽ‡**

**å…³é”®è¯**: `è™šæ‹ŸçŽ°å®žæ³¨è§†é¢„æµ‹` `å¤šæ¨¡æ€èžåˆ` `å¤´éƒ¨è¿åŠ¨åˆ†æž` `é—¨æŽ§æ³¨æ„åŠ›æœºåˆ¶` `æ¸²æŸ“ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVRä¸­æ³¨è§†è¡Œä¸ºé¢„æµ‹å›°éš¾ï¼Œå½±å“æ¸²æŸ“ä¼˜åŒ–å’Œç•Œé¢è®¾è®¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆæ—¶é—´æ³¨è§†æ¨¡å¼ã€å¤´éƒ¨è¿åŠ¨å’Œè§†è§‰åœºæ™¯ï¼Œä½¿ç”¨é—¨æŽ§èžåˆä¸Žè·¨æ¨¡æ€æ³¨æ„åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨22ä¸ªVRåœºæ™¯æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå¤šæ¨¡æ€ç»„åˆæå‡é¢„æµ‹å‡†ç¡®çŽ‡è‡³93.1%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Predicting gaze behavior in virtual reality environments remains a significant challenge with implications for rendering optimization and interface design. This paper introduces a multimodal approach to VR gaze prediction that combines temporal gaze patterns, head movement data, and visual scene information. By leveraging a gated fusion mechanism with cross-modal attention, the approach learns to adaptively weight gaze history, head movement, and scene content based on contextual relevance. Evaluations using a dataset spanning 22 VR scenes with 5.3M gaze samples demonstrate improvements in predictive accuracy when combining modalities compared to using individual data streams alone. The results indicate that integrating past gaze trajectories with head orientation and scene content enhances prediction accuracy across 1-3 future frames. Cross-scene generalization testing shows consistent performance with 93.1% validation accuracy and temporal consistency in predicted gaze trajectories. These findings contribute to understanding attention mechanisms in virtual environments while suggesting potential applications in rendering optimization, interaction design, and user experience evaluation. The approach represents a step toward more efficient virtual reality systems that can anticipate user attention patterns without requiring expensive eye tracking hardware.

