---
layout: default
title: Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models
---

# Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models

**arXiv**: [2511.20531v1](https://arxiv.org/abs/2511.20531) | [PDF](https://arxiv.org/pdf/2511.20531.pdf)

**ä½œè€…**: Shamima Hossain

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†å¼•å¯¼å¤šè·³æŽ¨ç†æ¡†æž¶ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„äº‹å®žå‡†ç¡®æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šè·³æŽ¨ç†` `çŸ¥è¯†å›¾è°±` `äº‹å®žå‡†ç¡®æ€§` `å›¾åƒæè¿°ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹ç”Ÿæˆå†…å®¹å¸¸å› æŽ¨ç†èƒ½åŠ›ä¸è¶³è€Œäº‹å®žé”™è¯¯
2. åˆ©ç”¨çŸ¥è¯†å›¾è°±è¿›è¡Œå¤šæ­¥æŽ¨ç†ï¼ŒåŒ…æ‹¬å®žä½“è¯†åˆ«å’Œå›¾éåŽ†
3. å®žéªŒæ˜¾ç¤ºäº‹å®žå‡†ç¡®æ€§æå‡çº¦31%ï¼Œåˆ†æžä¸åŒçŸ¥è¯†è¡¨ç¤ºæ•ˆæžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual Language Models (VLMs) are powerful generative tools but often produce factually in- accurate outputs due to a lack of robust reason- ing capabilities. While extensive research has been conducted on integrating external knowl- edge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seam- lessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leverag- ing structured knowledge graphs for multi-hop verification using image-captioning task to il- lustrate our framework. Our approach enables systematic reasoning across multiple steps, in- cluding visual entity recognition, knowledge graph traversal, and fact-based caption refine- ment. We evaluate the framework using hi- erarchical, triple-based and bullet-point based knowledge representations, analyzing their ef- fectiveness in factual accuracy and logical infer- ence. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions re- vealing key insights into reasoning patterns and failure modes. This work demonstrates the po- tential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.

