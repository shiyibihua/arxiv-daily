---
layout: default
title: DesignPref: Capturing Personal Preferences in Visual Design Generation
---

# DesignPref: Capturing Personal Preferences in Visual Design Generation

**arXiv**: [2511.20513v1](https://arxiv.org/abs/2511.20513) | [PDF](https://arxiv.org/pdf/2511.20513.pdf)

**ä½œè€…**: Yi-Hao Peng, Jeffrey P. Bigham, Jason Wu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDesignPrefæ•°æ®é›†ä»¥è§£å†³è§†è§‰è®¾è®¡ç”Ÿæˆä¸­çš„ä¸ªæ€§åŒ–åå¥½å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è®¾è®¡ç”Ÿæˆ` `ä¸ªæ€§åŒ–åå¥½å»ºæ¨¡` `æ•°æ®é›†æž„å»º` `UIè®¾è®¡è¯„ä¼°` `RAGç®¡é“` `å¾®è°ƒç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è®¾è®¡åå¥½é«˜åº¦ä¸»è§‚ï¼Œä¸“ä¸šè®¾è®¡å¸ˆé—´å­˜åœ¨æ˜¾è‘—åˆ†æ­§
2. å¼•å…¥12kå¯¹UIè®¾è®¡æ¯”è¾ƒæ•°æ®é›†ï¼ŒæŽ¢ç´¢å¾®è°ƒä¸ŽRAGç­‰ä¸ªæ€§åŒ–ç­–ç•¥
3. ä¸ªæ€§åŒ–æ¨¡åž‹åœ¨é¢„æµ‹ä¸ªä½“åå¥½æ—¶ä¼˜äºŽèšåˆåŸºçº¿ï¼Œæ ·æœ¬æ•ˆçŽ‡é«˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.

