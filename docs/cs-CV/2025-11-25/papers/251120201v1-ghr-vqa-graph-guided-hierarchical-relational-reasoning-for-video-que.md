---
layout: default
title: GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering
---

# GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering

**arXiv**: [2511.20201v1](https://arxiv.org/abs/2511.20201) | [PDF](https://arxiv.org/pdf/2511.20201.pdf)

**ä½œè€…**: Dionysia Danai Brilli, Dimitrios Mallis, Vassilis Pitsikalis, Petros Maragos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGHR-VQAæ¡†æž¶ï¼Œåˆ©ç”¨åœºæ™¯å›¾å¢žå¼ºè§†é¢‘é—®ç­”ä¸­çš„äºº-ç‰©äº¤äº’æŽ¨ç†ã€‚**

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `åœºæ™¯å›¾` `å›¾ç¥žç»ç½‘ç»œ` `äºº-ç‰©äº¤äº’` `æ—¶ç©ºæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘é—®ç­”ä¸­å¦‚ä½•æœ‰æ•ˆå»ºæ¨¡äºº-ç‰©äº¤äº’å’Œæ—¶ç©ºåŠ¨æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¸§çº§åœºæ™¯å›¾å¹¶é“¾æŽ¥åˆ°å…¨å±€æ ¹èŠ‚ç‚¹ï¼Œä½¿ç”¨å›¾ç¥žç»ç½‘ç»œå¤„ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨AGQAæ•°æ®é›†ä¸Šå®žçŽ°å¯¹è±¡å…³ç³»æŽ¨ç†æ€§èƒ½æå‡7.3%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose GHR-VQA, Graph-guided Hierarchical Relational Reasoning for Video Question Answering (Video QA), a novel human-centric framework that incorporates scene graphs to capture intricate human-object interactions within video sequences. Unlike traditional pixel-based methods, each frame is represented as a scene graph and human nodes across frames are linked to a global root, forming the video-level graph and enabling cross-frame reasoning centered on human actors. The video-level graphs are then processed by Graph Neural Networks (GNNs), transforming them into rich, context-aware embeddings for efficient processing. Finally, these embeddings are integrated with question features in a hierarchical network operating across different abstraction levels, enhancing both local and global understanding of video content. This explicit human-rooted structure enhances interpretability by decomposing actions into human-object interactions and enables a more profound understanding of spatiotemporal dynamics. We validate our approach on the Action Genome Question Answering (AGQA) dataset, achieving significant performance improvements, including a 7.3% improvement in object-relation reasoning over the state of the art.

