---
layout: default
title: Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement
---

# Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement

**arXiv**: [2511.20280v1](https://arxiv.org/abs/2511.20280) | [PDF](https://arxiv.org/pdf/2511.20280.pdf)

**ä½œè€…**: Yang Liu, Xilin Zhao, Peisong Wen, Siran Dai, Qingming Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽVLMå¼•å¯¼çš„è¿­ä»£è‡ªä¼˜åŒ–æ¡†æž¶ä»¥æå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `ç‰©ç†ä¸€è‡´æ€§` `è¿­ä»£è‡ªä¼˜åŒ–` `å¤šæ¨¡æ€æ€ç»´é“¾` `VLMå¼•å¯¼` `è®­ç»ƒæ— å…³æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡åž‹éš¾ä»¥éµå¾ªçœŸå®žä¸–ç•Œç‰©ç†åŽŸç†ï¼Œå¯¼è‡´ç‰©ç†ä¸ä¸€è‡´é—®é¢˜
2. é‡‡ç”¨å¤šæ¨¡æ€æ€ç»´é“¾è¿‡ç¨‹ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹å’Œè§†è§‰è¯­è¨€æ¨¡åž‹æä¾›ç‰©ç†æ„ŸçŸ¥æŒ‡å¯¼
3. åœ¨PhyIQåŸºå‡†æµ‹è¯•ä¸­ï¼Œç‰©ç†æ™ºå•†åˆ†æ•°ä»Ž56.31æå‡è‡³62.38ï¼Œæ— éœ€é¢å¤–è®­ç»ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.

