---
layout: default
title: LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight
---

# LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight

**arXiv**: [2511.20648v1](https://arxiv.org/abs/2511.20648) | [PDF](https://arxiv.org/pdf/2511.20648.pdf)

**ä½œè€…**: Yunze Man, Shihao Wang, Guowen Zhang, Johan Bjorck, Zhiqi Li, Liang-Yan Gui, Jim Fan, Jan Kautz, Yu-Xiong Wang, Zhiding Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLocateAnything3Dæ–¹æ³•ï¼Œé€šè¿‡é“¾å¼è§†è§‰æŽ¨ç†å®žçŽ°å¼€æ”¾è¯æ±‡3Dæ£€æµ‹**

**å…³é”®è¯**: `3Dæ£€æµ‹` `è§†è§‰è¯­è¨€æ¨¡åž‹` `é“¾å¼è§†è§‰æŽ¨ç†` `å¼€æ”¾è¯æ±‡æ£€æµ‹` `é›¶æ ·æœ¬æ³›åŒ–` `è¾¹ç•Œæ¡†é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹ç¼ºä¹å¤šå¯¹è±¡3Dæ£€æµ‹èƒ½åŠ›ï¼Œéš¾ä»¥åœ¨3Dç©ºé—´ä¸­å®šä½ç‰©ä½“
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨é“¾å¼è§†è§‰åºåˆ—ï¼Œå…ˆé¢„æµ‹2Dæ£€æµ‹ï¼Œå†æŒ‰ä»Žè¿‘åˆ°è¿œé¡ºåºé¢„æµ‹3Dè¾¹ç•Œæ¡†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Omni3DåŸºå‡†ä¸Šè¾¾åˆ°49.89 AP_3Dï¼Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.

