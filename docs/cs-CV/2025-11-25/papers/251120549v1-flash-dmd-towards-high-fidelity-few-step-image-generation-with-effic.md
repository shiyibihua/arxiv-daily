---
layout: default
title: Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning
---

# Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.20549" target="_blank" class="toolbar-btn">arXiv: 2511.20549v1</a>
    <a href="https://arxiv.org/pdf/2511.20549.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20549v1" 
            onclick="toggleFavorite(this, '2511.20549v1', 'Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Guanjie Chen, Shirui Huang, Kai Liu, Jianchen Zhu, Xiaoye Qu, Peng Chen, Yu Cheng, Yifu Sun

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-25

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Flash-DMDÔºöÈÄöËøáÈ´òÊïàËí∏È¶è‰∏éËÅîÂêàÂº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞È´ò‰øùÁúüÂø´ÈÄüÂõæÂÉèÁîüÊàê**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `ÂõæÂÉèÁîüÊàê` `Êó∂Èó¥Ê≠•Ëí∏È¶è` `Âº∫ÂåñÂ≠¶‰π†` `ËÅîÂêàËÆ≠ÁªÉ` `Âø´ÈÄüÈááÊ†∑` `Ê®°ÂûãÂä†ÈÄü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Êâ©Êï£Ê®°ÂûãÁîüÊàêÂõæÂÉèË¥®ÈáèÈ´ò‰ΩÜÈááÊ†∑ÈÄüÂ∫¶ÊÖ¢ÔºåÊó∂Èó¥Ê≠•Ëí∏È¶èÊòØÂä†ÈÄüÊñπÊ≥ïÔºå‰ΩÜËÆ≠ÁªÉÊàêÊú¨È´ò‰∏îË¥®ÈáèÊòì‰∏ãÈôç„ÄÇ
2. Flash-DMDÊèêÂá∫È´òÊïàÁöÑÊó∂Èó¥Ê≠•ÊÑüÁü•Ëí∏È¶èÁ≠ñÁï•ÔºåÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨Âπ∂ÊèêÂçáÂõæÂÉèÁúüÂÆûÊÑüÔºåÂêåÊó∂ÈÅøÂÖçË¥®Èáè‰∏ãÈôç„ÄÇ
3. Flash-DMDÈááÁî®ËÅîÂêàËÆ≠ÁªÉÊñπÊ°àÔºåÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞É‰∏éËí∏È¶èËÆ≠ÁªÉÁªìÂêàÔºåÂà©Áî®Ëí∏È¶èÊçüÂ§±Á®≥ÂÆöÂº∫ÂåñÂ≠¶‰π†ËøáÁ®ã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Êï£Ê®°ÂûãÂ∑≤Êàê‰∏∫È¢ÜÂÖàÁöÑÁîüÊàêÊ®°ÂûãÔºå‰ΩÜÂÖ∂Ëø≠‰ª£ÈááÊ†∑ËøáÁ®ãËÆ°ÁÆóÊàêÊú¨È´òÊòÇ„ÄÇÊó∂Èó¥Ê≠•Ëí∏È¶èÊòØ‰∏ÄÁßçÂæàÊúâÂâçÊôØÁöÑÂä†ÈÄüÁîüÊàêÊäÄÊúØÔºå‰ΩÜÂÆÉÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÔºåÂπ∂ÂØºËá¥ÂõæÂÉèË¥®Èáè‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂØπËøô‰∫õËí∏È¶èÊ®°ÂûãËøõË°åÂæÆË∞É‰ª•ÂÆûÁé∞ÁâπÂÆöÁõÆÊ†áÔºàÂ¶ÇÁæéÂ≠¶Âê∏ÂºïÂäõÊàñÁî®Êà∑ÂÅèÂ•ΩÔºâÈùûÂ∏∏‰∏çÁ®≥ÂÆöÔºåÂπ∂‰∏îÂÆπÊòìÈô∑ÂÖ•Â•ñÂä±Âà©Áî®„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂Flash-DMDÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÈÄöËøáËí∏È¶èÂíåÂü∫‰∫éËÅîÂêàRLÁöÑÁªÜÂåñÂÆûÁé∞Âø´ÈÄüÊî∂Êïõ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨È¶ñÂÖàÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑ„ÄÅÊó∂Èó¥Ê≠•ÊÑüÁü•ÁöÑËí∏È¶èÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•ÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨Âπ∂Â¢ûÂº∫‰∫ÜÁúüÂÆûÊÑüÔºå‰ªÖÁî®DMD2ÁöÑ2.1%ÁöÑËÆ≠ÁªÉÊàêÊú¨Â∞±Ë∂ÖË∂ä‰∫ÜÂÆÉ„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçËÅîÂêàËÆ≠ÁªÉÊñπÊ°àÔºåÂÖ∂‰∏≠Ê®°ÂûãÂú®RLÁõÆÊ†á‰∏ãËøõË°åÂæÆË∞ÉÔºåÂêåÊó∂Êó∂Èó¥Ê≠•Ëí∏È¶èËÆ≠ÁªÉÊåÅÁª≠ËøõË°å„ÄÇÊàë‰ª¨ËØÅÊòé‰∫ÜÊù•Ëá™ÊåÅÁª≠Ëí∏È¶èÁöÑÁ®≥ÂÆö„ÄÅÂÆö‰πâÊòéÁ°ÆÁöÑÊçüÂ§±ÂÖÖÂΩì‰∫ÜÂº∫Â§ßÁöÑÊ≠£ÂàôÂåñÂô®ÔºåÊúâÊïàÂú∞Á®≥ÂÆö‰∫ÜRLËÆ≠ÁªÉËøáÁ®ãÂπ∂Èò≤Ê≠¢Á≠ñÁï•Â¥©Ê∫É„ÄÇÂú®Âü∫‰∫éÂàÜÊï∞ÁöÑÊ®°ÂûãÂíåÊµÅÂåπÈÖçÊ®°Âûã‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑFlash-DMD‰∏ç‰ªÖÊî∂ÊïõÈÄüÂ∫¶ÊòæËëóÂä†Âø´ÔºåËÄå‰∏îÂú®Â∞ëÊ≠•ÈááÊ†∑Êú∫Âà∂‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁîüÊàêË¥®ÈáèÔºåÂú®ËßÜËßâË¥®Èáè„ÄÅ‰∫∫Á±ªÂÅèÂ•ΩÂíåÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÊåáÊ†áÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏∫ËÆ≠ÁªÉÈ´òÊïà„ÄÅÈ´ò‰øùÁúüÂíåÁ®≥ÂÆöÁöÑÁîüÊàêÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑËåÉ‰æã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊâ©Êï£Ê®°ÂûãÁîüÊàêÂõæÂÉèË¥®ÈáèÈ´òÔºå‰ΩÜÈúÄË¶ÅÂ§öÊ¨°Ëø≠‰ª£ÈááÊ†∑ÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇ„ÄÇÊó∂Èó¥Ê≠•Ëí∏È¶èÊó®Âú®ÂáèÂ∞ëÈááÊ†∑Ê≠•È™§ÔºåÂä†ÈÄüÁîüÊàêËøáÁ®ãÔºå‰ΩÜÁé∞ÊúâËí∏È¶èÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉËµÑÊ∫êÔºåÂπ∂‰∏îÂÆπÊòìÂØºËá¥ÁîüÊàêÂõæÂÉèË¥®Èáè‰∏ãÈôçÔºåÂêåÊó∂Ôºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÂØπËí∏È¶èÂêéÁöÑÊ®°ÂûãËøõË°åÂæÆË∞É‰ª•ÈÄÇÂ∫îÁâπÂÆöÁõÆÊ†áÊó∂ÔºåÂÆπÊòìÂá∫Áé∞ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÂ•ñÂä±Âà©Áî®ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöFlash-DMDÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÈ´òÊïàÁöÑËí∏È¶èÁ≠ñÁï•Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨ÔºåÂπ∂Âà©Áî®ËÅîÂêàËÆ≠ÁªÉÁöÑÊñπÂºèÔºåÂ∞ÜÊó∂Èó¥Ê≠•Ëí∏È¶è‰∏éÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÁõ∏ÁªìÂêà„ÄÇËí∏È¶èËøáÁ®ãÊèê‰æõÁ®≥ÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ªéËÄåÁ®≥ÂÆöÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉËøáÁ®ãÔºåÈÅøÂÖçÁ≠ñÁï•Â¥©Ê∫É„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFlash-DMDÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ´òÊïàÁöÑÊó∂Èó¥Ê≠•ÊÑüÁü•Ëí∏È¶èÂíåËÅîÂêàÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞É„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®ÊèêÂá∫ÁöÑËí∏È¶èÁ≠ñÁï•ËÆ≠ÁªÉ‰∏Ä‰∏™Âø´ÈÄüÈááÊ†∑ÁöÑÁîüÊàêÊ®°Âûã„ÄÇÁÑ∂ÂêéÔºåÂú®Ëí∏È¶èËÆ≠ÁªÉÁöÑÂêåÊó∂Ôºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÂØπÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•‰ºòÂåñÁâπÂÆöÁõÆÊ†áÔºàÂ¶ÇÁæéÂ≠¶ËØÑÂàÜÔºâ„ÄÇËí∏È¶èÊçüÂ§±‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ≠£ÂàôÂåñÈ°πÔºåÁ°Æ‰øùËÆ≠ÁªÉËøáÁ®ãÁöÑÁ®≥ÂÆö„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöFlash-DMDÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÊó∂Èó¥Ê≠•ÊÑüÁü•Ëí∏È¶èÁ≠ñÁï•ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂõæÂÉèË¥®Èáè„ÄÇ2) ÂºïÂÖ•‰∫Ü‰∏ÄÁßçËÅîÂêàËÆ≠ÁªÉÊñπÊ°àÔºåÂ∞ÜÊó∂Èó¥Ê≠•Ëí∏È¶è‰∏éÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÁõ∏ÁªìÂêàÔºåÂà©Áî®Ëí∏È¶èÊçüÂ§±‰Ωú‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ≠£ÂàôÂåñÈ°πÔºåÁ®≥ÂÆö‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êó∂Èó¥Ê≠•ÊÑüÁü•Ëí∏È¶è‰∏≠ÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•Êõ¥Â•ΩÂú∞‰øùÁïô‰∏çÂêåÊó∂Èó¥Ê≠•ÁöÑ‰ø°ÊÅØ„ÄÇÂú®ËÅîÂêàËÆ≠ÁªÉ‰∏≠ÔºåÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ•ñÂä±ÂáΩÊï∞‰∏éËí∏È¶èÊçüÂ§±ÂáΩÊï∞ËøõË°åÂä†ÊùÉÁªÑÂêàÔºåÊùÉÈáçÁ≥ªÊï∞ÈúÄË¶Å‰ªîÁªÜË∞ÉÊï¥Ôºå‰ª•Âπ≥Ë°°ÁîüÊàêË¥®ÈáèÂíåÁâπÂÆöÁõÆÊ†á‰ºòÂåñ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂèñÂÜ≥‰∫éÊâÄ‰ΩøÁî®ÁöÑÊâ©Êï£Ê®°ÂûãÊû∂ÊûÑ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Flash-DMDÂú®Â∞ëÊ≠•ÈááÊ†∑Êú∫Âà∂‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁîüÊàêË¥®ÈáèÔºåÂú®ËßÜËßâË¥®Èáè„ÄÅ‰∫∫Á±ªÂÅèÂ•ΩÂíåÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÊåáÊ†áÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Áõ∏ÂêåÁöÑËÆ≠ÁªÉÊàêÊú¨‰∏ãÔºåFlash-DMDÁöÑÁîüÊàêË¥®ÈáèÊòæËëó‰ºò‰∫éDMD2ÔºåÂπ∂‰∏îËÉΩÂ§üÁ®≥ÂÆöÂú∞ËøõË°åÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÔºåÈÅøÂÖçÁ≠ñÁï•Â¥©Ê∫É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlash-DMDÊòØ‰∏ÄÁßçÈ´òÊïà„ÄÅÈ´ò‰øùÁúüÂíåÁ®≥ÂÆöÁöÑÂõæÂÉèÁîüÊàêÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Flash-DMDÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂø´ÈÄüÂõæÂÉèÁîüÊàêÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÊ∏∏ÊàèÂºÄÂèë„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅÂÜÖÂÆπÂàõ‰Ωú„ÄÅÂõæÂÉèÁºñËæëÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§ü‰ª•ËæÉ‰ΩéÁöÑËÆ°ÁÆóÊàêÊú¨ÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂÉèÔºåÂπ∂ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÂÅèÂ•ΩËøõË°åÂÆöÂà∂ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËÉΩË¢´Áî®‰∫éÂÆûÊó∂ÂõæÂÉèÁîüÊàê„ÄÅ‰∏™ÊÄßÂåñÂÜÖÂÆπÊé®ËçêÁ≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.

