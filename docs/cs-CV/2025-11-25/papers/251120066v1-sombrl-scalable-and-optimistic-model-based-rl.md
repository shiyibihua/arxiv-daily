---
layout: default
title: SOMBRL: Scalable and Optimistic Model-Based RL
---

# SOMBRL: Scalable and Optimistic Model-Based RL

**arXiv**: [2511.20066v1](https://arxiv.org/abs/2511.20066) | [PDF](https://arxiv.org/pdf/2511.20066.pdf)

**ä½œè€…**: Bhavya Sukhija, Lenart Treven, Carmelo Sferrazza, Florian DÃ¶rfler, Pieter Abbeel, Andreas Krause

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSOMBRLä»¥è§£å†³æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ä¸­é«˜æ•ˆæŽ¢ç´¢çš„æŒ‘æˆ˜**

**å…³é”®è¯**: `æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ` `é«˜æ•ˆæŽ¢ç´¢` `ä¹è§‚ä¸ç¡®å®šæ€§` `éžçº¿æ€§åŠ¨æ€` `åœ¨çº¿å­¦ä¹ ` `ç¡¬ä»¶éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡åž‹å¼ºåŒ–å­¦ä¹ ä¸­ç³»ç»ŸåŠ¨æ€æœªçŸ¥ï¼Œéœ€åœ¨çº¿äº¤äº’é«˜æ•ˆæŽ¢ç´¢
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä¹è§‚ä¸ç¡®å®šæ€§åŽŸåˆ™ï¼Œå­¦ä¹ ä¸ç¡®å®šæ€§åŠ¨æ€æ¨¡åž‹å¹¶ä¼˜åŒ–å¥–åŠ±ä¸Žä¸ç¡®å®šæ€§åŠ æƒå’Œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çŠ¶æ€å’Œè§†è§‰æŽ§åˆ¶çŽ¯å¢ƒä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œç¡¬ä»¶æµ‹è¯•ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

