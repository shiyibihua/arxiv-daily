---
layout: default
title: A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction
---

# A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction

**arXiv**: [2511.19858v1](https://arxiv.org/abs/2511.19858) | [PDF](https://arxiv.org/pdf/2511.19858.pdf)

**ä½œè€…**: Farzad Ahmed, Joniel Augustine Jerome, Meliha Yetisgen, Ã–zlem Uzuner

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ£€ç´¢å¢žå¼ºåŠ¨æ€æç¤ºæ–¹æ³•ä»¥æå‡åŒ»å­¦é”™è¯¯æ£€æµ‹ä¸Žçº æ­£æ€§èƒ½**

**å…³é”®è¯**: `åŒ»å­¦é”™è¯¯æ£€æµ‹` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `åŠ¨æ€æç¤º` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `ä¸´åºŠæ–‡æ¡£å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¸´åºŠæ–‡æ¡£å­˜åœ¨äº‹å®žã€è¯Šæ–­å’Œç®¡ç†é”™è¯¯ï¼Œå¯èƒ½å±å®³æ‚£è€…å®‰å…¨ã€‚
2. æ¯”è¾ƒé›¶æ ·æœ¬æç¤ºã€é™æ€æç¤ºå’Œæ£€ç´¢å¢žå¼ºåŠ¨æ€æç¤ºåœ¨åŒ»å­¦é”™è¯¯å¤„ç†ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚
3. æ£€ç´¢å¢žå¼ºåŠ¨æ€æç¤ºé™ä½Žå‡é˜³æ€§çŽ‡ï¼Œæé«˜å¬å›žçŽ‡ï¼Œç”Ÿæˆæ›´å‡†ç¡®çº æ­£ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
>   Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
>   Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
>   Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

