---
layout: default
title: Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos
---

# Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos

**arXiv**: [2511.19936v1](https://arxiv.org/abs/2511.19936) | [PDF](https://arxiv.org/pdf/2511.19936.pdf)

**ä½œè€…**: Youngseo Kim, Dohyun Kim, Geohee Han, Paul Hongsuck Seo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRIFTæ¡†æž¶ï¼Œåˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡åž‹å®žçŽ°è§†é¢‘ä¸­é›¶æ ·æœ¬å¯¹è±¡è·Ÿè¸ª**

**å…³é”®è¯**: `å›¾åƒæ‰©æ•£æ¨¡åž‹` `é›¶æ ·æœ¬å¯¹è±¡è·Ÿè¸ª` `è¯­ä¹‰ä¼ æ’­` `è§†é¢‘åˆ†å‰²` `è‡ªæ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒæ‰©æ•£æ¨¡åž‹çš„è‡ªæ³¨æ„åŠ›å›¾å¦‚ä½•ç”¨äºŽè§†é¢‘ä¸­çš„è¯­ä¹‰ä¼ æ’­å’Œå¯¹è±¡è·Ÿè¸ª
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è‡ªæ³¨æ„åŠ›å›¾é‡æ–°è§£é‡Šä¸ºä¼ æ’­æ ¸ï¼Œç»“åˆæµ‹è¯•æ—¶ä¼˜åŒ–ç­–ç•¥æå‡é²æ£’æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†è§†é¢‘å¯¹è±¡åˆ†å‰²åŸºå‡†ä¸Šå®žçŽ°é›¶æ ·æœ¬æœ€å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.

