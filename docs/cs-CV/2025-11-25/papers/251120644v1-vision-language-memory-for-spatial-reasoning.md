---
layout: default
title: Vision-Language Memory for Spatial Reasoning
---

# Vision-Language Memory for Spatial Reasoning

**arXiv**: [2511.20644v1](https://arxiv.org/abs/2511.20644) | [PDF](https://arxiv.org/pdf/2511.20644.pdf)

**ä½œè€…**: Zuntao Liu, Yi Du, Taimeng Fu, Shaoshu Su, Cherie Ho, Chen Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLM^2æ¨¡åž‹ä»¥è§£å†³è§†é¢‘ç©ºé—´æŽ¨ç†ä¸­çš„è¯­ä¹‰å‡ ä½•é”™ä½å’Œè®°å¿†ç¼ºå¤±é—®é¢˜**

**å…³é”®è¯**: `ç©ºé—´æŽ¨ç†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŒä¹…è®°å¿†` `åŒè®°å¿†æ¨¡å—` `è§†é¢‘ç†è§£` `3Dè¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯­ä¹‰å‡ ä½•é”™ä½å’Œç¼ºä¹æŒä¹…è®°å¿†é˜»ç¢è§†é¢‘ç©ºé—´æŽ¨ç†è¾¾åˆ°äººç±»æ°´å¹³
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŒè®°å¿†æ¨¡å—ï¼ŒåŒ…æ‹¬å·¥ä½œè®°å¿†å’Œæƒ…æ™¯è®°å¿†ï¼Œå®žçŽ°å›ºå®šè®¡ç®—æˆæœ¬çš„é•¿æ—¶æŽ¨ç†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLM^2åœ¨çº¯è§†é¢‘æ¨¡åž‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.

