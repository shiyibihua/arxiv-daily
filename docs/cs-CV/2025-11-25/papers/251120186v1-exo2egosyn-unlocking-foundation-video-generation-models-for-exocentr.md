---
layout: default
title: Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis
---

# Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis

**arXiv**: [2511.20186v1](https://arxiv.org/abs/2511.20186) | [PDF](https://arxiv.org/pdf/2511.20186.pdf)

**ä½œè€…**: Mohammad Mahdi, Yuqian Fu, Nedko Savov, Jiancheng Pan, Danda Pani Paudel, Luc Van Gool

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºExo2EgoSynä»¥è§£é”åŸºç¡€è§†é¢‘ç”Ÿæˆæ¨¡åž‹çš„å¤–ä¸­å¿ƒåˆ°å†…ä¸­å¿ƒè§†é¢‘åˆæˆ**

**å…³é”®è¯**: `è·¨è§†è§’è§†é¢‘åˆæˆ` `åŸºç¡€è§†é¢‘ç”Ÿæˆæ¨¡åž‹` `è§†å›¾å¯¹é½` `å¤šè§†è§’æ¡ä»¶` `å§¿æ€æ³¨å…¥` `å¤–ä¸­å¿ƒåˆ°å†…ä¸­å¿ƒè½¬æ¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºç¡€è§†é¢‘ç”Ÿæˆæ¨¡åž‹å±€é™äºŽåŒè§†è§’ç”Ÿæˆï¼Œæ— æ³•å®žçŽ°è·¨è§†è§’åˆæˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§†å›¾å¯¹é½ã€å¤šè§†è§’æ¡ä»¶è¾“å…¥å’Œå§¿æ€æ³¨å…¥æ¨¡å—ï¼Œé€‚é…WAN 2.2æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ExoEgo4Dæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ˜¾è‘—æå‡è·¨è§†è§’åˆæˆè´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation video generation models such as WAN 2.2 exhibit strong text- and image-conditioned synthesis abilities but remain constrained to the same-view generation setting. In this work, we introduce Exo2EgoSyn, an adaptation of WAN 2.2 that unlocks Exocentric-to-Egocentric(Exo2Ego) cross-view video synthesis. Our framework consists of three key modules. Ego-Exo View Alignment(EgoExo-Align) enforces latent-space alignment between exocentric and egocentric first-frame representations, reorienting the generative space from the given exo view toward the ego view. Multi-view Exocentric Video Conditioning (MultiExoCon) aggregates multi-view exocentric videos into a unified conditioning signal, extending WAN2.2 beyond its vanilla single-image or text conditioning. Furthermore, Pose-Aware Latent Injection (PoseInj) injects relative exo-to-ego camera pose information into the latent state, guiding geometry-aware synthesis across viewpoints. Together, these modules enable high-fidelity ego view video generation from third-person observations without retraining from scratch. Experiments on ExoEgo4D validate that Exo2EgoSyn significantly improves Ego2Exo synthesis, paving the way for scalable cross-view video generation with foundation models. Source code and models will be released publicly.

