---
layout: default
title: One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues
---

# One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.20784" target="_blank" class="toolbar-btn">arXiv: 2511.20784v1</a>
    <a href="https://arxiv.org/pdf/2511.20784.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20784v1" 
            onclick="toggleFavorite(this, '2511.20784v1', 'One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sindhuja Penchala, Gavin Money, Gabriel Marques, Samuel Wood, Jessica Kirschman, Travis Atkison, Shahram Rahimi, Noorbakhsh Amiri Golilarz

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-25

**Â§áÊ≥®**: 9 pages,3 figures, 5 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SMARCÔºö‰ªÖÈúÄÂõæÂÉè10%Âå∫ÂüüÔºåÂç≥ÂèØÂÆûÁé∞Ë°®Èù¢ÊùêË¥®ÈáçÂª∫‰∏éÂàÜÁ±ª**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ë°®Èù¢ÊùêË¥®ÈáçÂª∫` `ÊùêË¥®ÂàÜÁ±ª` `ÈÉ®ÂàÜÂç∑ÁßØ` `U-Net` `Á®ÄÁñèËßÜËßâ‰ø°ÊÅØ` `Êú∫Âô®‰∫∫` `ÂõæÂÉè‰øÆÂ§ç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊùêË¥®ÁêÜËß£ÊñπÊ≥ï‰æùËµñÁ®†ÂØÜËßÇÊµãÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂèóÈôêÊàñÈÉ®ÂàÜËßÜÂõæÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. SMARCÊ®°Âûã‰ªÖÈúÄÂõæÂÉèÁöÑ10%Âå∫ÂüüÔºåÂç≥ÂèØÂêåÊó∂ÂÆåÊàêË°®Èù¢ÈáçÂª∫ÂíåÊùêË¥®ÂàÜÁ±ª‰ªªÂä°„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSMARCÂú®Ë°®Èù¢ÈáçÂª∫ÂíåÊùêË¥®ÂàÜÁ±ª‰ªªÂä°‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SMARCÁöÑÁªü‰∏ÄÊ®°ÂûãÔºåÁî®‰∫é‰ªéÊûÅÂ∞ëÁöÑËßÜËßâËæìÂÖ•‰∏≠ËøõË°åË°®Èù¢ÊùêË¥®ÈáçÂª∫ÂíåÂàÜÁ±ª„ÄÇÈíàÂØπÊú∫Âô®‰∫∫„ÄÅ‰ªøÁúüÂíåÊùêË¥®ÊÑüÁü•Á≠âÂ∫îÁî®‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÁ®†ÂØÜÊàñÂÆåÊï¥Âú∫ÊôØËßÇÊµãÁöÑÂ±ÄÈôêÊÄßÔºåSMARC‰ªÖÈúÄÂõæÂÉè‰∏≠‰∏Ä‰∏™ËøûÁª≠ÁöÑ10%Âå∫ÂüüÔºåÂç≥ÂèØËØÜÂà´Âπ∂ÈáçÂª∫ÂÆåÊï¥ÁöÑRGBË°®Èù¢ÔºåÂêåÊó∂ÂØπÊùêË¥®Á±ªÂà´ËøõË°åÂàÜÁ±ª„ÄÇËØ•Êû∂ÊûÑÁªìÂêà‰∫ÜÈÉ®ÂàÜÂç∑ÁßØU-NetÂíå‰∏Ä‰∏™ÂàÜÁ±ªÂ§¥ÔºåÂÆûÁé∞‰∫ÜÂú®ÊûÅÁ´ØËßÇÊµãÁ®ÄÁñèÊÉÖÂÜµ‰∏ãÁöÑÁ©∫Èó¥‰øÆÂ§çÂíåËØ≠‰πâÁêÜËß£„ÄÇÂú®Touch and GoÊï∞ÊçÆÈõÜ‰∏äÔºåSMARC‰∏éÂåÖÊã¨Âç∑ÁßØËá™ÁºñÁ†ÅÂô®„ÄÅVision Transformer (ViT)„ÄÅMasked Autoencoder (MAE)„ÄÅSwin TransformerÂíåDETRÂú®ÂÜÖÁöÑ‰∫îÁßçÊ®°ÂûãËøõË°å‰∫ÜÊØîËæÉÔºåÂèñÂæó‰∫Üstate-of-the-artÁöÑÁªìÊûúÔºåPSNRËææÂà∞17.55 dBÔºåÊùêË¥®ÂàÜÁ±ªÂáÜÁ°ÆÁéáËææÂà∞85.10%„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÈÉ®ÂàÜÂç∑ÁßØÂú®Áº∫Â§±Êï∞ÊçÆ‰∏ãÁöÑÁ©∫Èó¥Êé®ÁêÜÊñπÈù¢ÂÖ∑Êúâ‰ºòÂäøÔºåÂπ∂‰∏∫ÊûÅÁÆÄËßÜËßâË°®Èù¢ÁêÜËß£Â•†ÂÆö‰∫ÜÂùöÂÆûÁöÑÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâË°®Èù¢ÊùêË¥®ÁêÜËß£ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂÆåÊï¥ÁöÑÂõæÂÉèÊàñËÄÖÁ®†ÂØÜÁöÑËßÇÊµãÊï∞ÊçÆÔºåËøôÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ôºå‰æãÂ¶ÇÊú∫Âô®‰∫∫Êìç‰ΩúÊàñËÄÖËµÑÊ∫êÂèóÈôêÁöÑÂµåÂÖ•ÂºèÁ≥ªÁªü‰∏≠ÔºåÈöæ‰ª•Êª°Ë∂≥„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®ÊûÅÂ∫¶Á®ÄÁñèÁöÑËßÜËßâ‰ø°ÊÅØ‰∏ãÔºåÂáÜÁ°ÆÂú∞ÈáçÂª∫Ë°®Èù¢ÊùêË¥®Âπ∂ËøõË°åÂàÜÁ±ªÔºåÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÊó†Ê≥ïÊúâÊïàÂà©Áî®Â±ÄÈÉ®‰ø°ÊÅØËøõË°åÂÖ®Â±ÄÊé®ÁêÜÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSMARCÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÈÉ®ÂàÜÂç∑ÁßØÔºàPartial ConvolutionÔºâÊù•Â§ÑÁêÜÁº∫Â§±Êï∞ÊçÆÔºåÂπ∂ÁªìÂêàU-NetÁöÑÁªìÊûÑËøõË°åÁ©∫Èó¥‰ø°ÊÅØÁöÑ‰º†ÈÄíÂíåÈáçÂª∫„ÄÇÈÄöËøáÈÉ®ÂàÜÂç∑ÁßØÔºåÊ®°ÂûãÂèØ‰ª•Âè™ÂÖ≥Ê≥®Â∑≤Áü•ÁöÑÂÉèÁ¥†‰ø°ÊÅØÔºåÈÅøÂÖçÁº∫Â§±ÂÉèÁ¥†Â∏¶Êù•ÁöÑÂπ≤Êâ∞„ÄÇÂêåÊó∂ÔºåU-NetÁöÑË∑≥Ë∑ÉËøûÊé•ÂèØ‰ª•ÊúâÊïàÂú∞Â∞Ü‰ΩéÂ±ÇÁâπÂæÅ‰º†ÈÄíÂà∞È´òÂ±ÇÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈáçÂª∫ÂõæÂÉè„ÄÇÊ≠§Â§ñÔºåÂ¢ûÂä†‰∏Ä‰∏™ÂàÜÁ±ªÂ§¥Ôºå‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•ÂêåÊó∂ËøõË°åË°®Èù¢ÈáçÂª∫ÂíåÊùêË¥®ÂàÜÁ±ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSMARCÁöÑÊï¥‰ΩìÊû∂ÊûÑÊòØ‰∏Ä‰∏™Âü∫‰∫éU-NetÁöÑÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®ÁªìÊûÑÔºåÂÖ∂‰∏≠ÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®ÈÉΩ‰ΩøÁî®‰∫ÜÈÉ®ÂàÜÂç∑ÁßØÂ±Ç„ÄÇÂú®ÁºñÁ†ÅÂô®ÁöÑÊúÄÂêéÔºåËøûÊé•‰∏Ä‰∏™ÂàÜÁ±ªÂ§¥ÔºåÁî®‰∫éÈ¢ÑÊµãÊùêË¥®Á±ªÂà´„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂ¶Ç‰∏ãÔºöÈ¶ñÂÖàÔºåËæìÂÖ•‰∏Ä‰∏™ÂåÖÂê´10%Âå∫ÂüüÁöÑÂõæÂÉèpatchÔºõÁÑ∂ÂêéÔºåÈÄöËøáÁºñÁ†ÅÂô®ÊèêÂèñÁâπÂæÅÔºõÊé•ÁùÄÔºåÈÄöËøáËß£Á†ÅÂô®ÈáçÂª∫ÂÆåÊï¥ÁöÑRGBÂõæÂÉèÔºõÂêåÊó∂ÔºåÈÄöËøáÂàÜÁ±ªÂ§¥È¢ÑÊµãÊùêË¥®Á±ªÂà´„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSMARCÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÈÉ®ÂàÜÂç∑ÁßØÂ∫îÁî®‰∫éË°®Èù¢ÊùêË¥®ÈáçÂª∫ÂíåÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÂπ∂ÁªìÂêàU-NetÁöÑÁªìÊûÑÔºåÂÆûÁé∞‰∫ÜÂú®ÊûÅÂ∫¶Á®ÄÁñèËßÇÊµã‰∏ãÁöÑÈ´òÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSMARCËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Â±ÄÈÉ®‰ø°ÊÅØËøõË°åÂÖ®Â±ÄÊé®ÁêÜÔºå‰ªéËÄåÂú®Áº∫Â§±Â§ßÈáèÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºå‰æùÁÑ∂ËÉΩÂ§üÂáÜÁ°ÆÂú∞ÈáçÂª∫Ë°®Èù¢ÊùêË¥®Âπ∂ËøõË°åÂàÜÁ±ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSMARCÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®ÈÉ®ÂàÜÂç∑ÁßØÂ±ÇÊù•Â§ÑÁêÜÁº∫Â§±Êï∞ÊçÆÔºåÈÅøÂÖçÊó†ÊïàÂÉèÁ¥†ÁöÑÂπ≤Êâ∞Ôºõ2) ‰ΩøÁî®U-NetÁªìÊûÑËøõË°åÁ©∫Èó¥‰ø°ÊÅØÁöÑ‰º†ÈÄíÂíåÈáçÂª∫Ôºõ3) Â¢ûÂä†‰∏Ä‰∏™ÂàÜÁ±ªÂ§¥Ôºå‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•ÂêåÊó∂ËøõË°åË°®Èù¢ÈáçÂª∫ÂíåÊùêË¥®ÂàÜÁ±ªÔºõ4) ÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÈáçÂª∫ÊçüÂ§±Ôºà‰æãÂ¶ÇL1ÊçüÂ§±ÊàñPSNRÔºâÂíåÂàÜÁ±ªÊçüÂ§±Ôºà‰æãÂ¶Ç‰∫§ÂèâÁÜµÊçüÂ§±Ôºâ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖÊï∞ÊçÆÈõÜËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SMARCÂú®Touch and GoÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Üstate-of-the-artÁöÑÁªìÊûúÔºåPSNRËææÂà∞17.55 dBÔºåÊùêË¥®ÂàÜÁ±ªÂáÜÁ°ÆÁéáËææÂà∞85.10%„ÄÇÁõ∏ÊØî‰∫éÂç∑ÁßØËá™ÁºñÁ†ÅÂô®„ÄÅVision Transformer (ViT)„ÄÅMasked Autoencoder (MAE)„ÄÅSwin TransformerÂíåDETRÁ≠âÂü∫Á∫øÊ®°ÂûãÔºåSMARCÂú®Ë°®Èù¢ÈáçÂª∫ÂíåÊùêË¥®ÂàÜÁ±ª‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçáÔºåËØÅÊòé‰∫ÜÈÉ®ÂàÜÂç∑ÁßØÂú®Â§ÑÁêÜÁº∫Â§±Êï∞ÊçÆÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SMARCÂú®Êú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆû„ÄÅÊùêË¥®ËØÜÂà´„ÄÅÂ∑•‰∏öÊ£ÄÊµãÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®SMARC‰ªéÂ∞ëÈáèËßÜËßâ‰ø°ÊÅØ‰∏≠ÁêÜËß£Áâ©‰ΩìË°®Èù¢ÊùêË¥®Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ËøõË°åÊäìÂèñÂíåÊìç‰Ωú„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆû‰∏≠ÔºåSMARCÂèØ‰ª•Áî®‰∫éÁîüÊàêÈÄºÁúüÁöÑË°®Èù¢Á∫πÁêÜÔºåÊèêÈ´òÁî®Êà∑‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåSMARCËøòÂèØ‰ª•Â∫îÁî®‰∫éÂ∑•‰∏öÊ£ÄÊµã‰∏≠ÔºåÁî®‰∫éËØÜÂà´‰∫ßÂìÅË°®Èù¢ÁöÑÁº∫Èô∑„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.

