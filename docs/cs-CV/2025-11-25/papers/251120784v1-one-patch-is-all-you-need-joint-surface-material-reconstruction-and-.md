---
layout: default
title: One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues
---

# One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues

**arXiv**: [2511.20784v1](https://arxiv.org/abs/2511.20784) | [PDF](https://arxiv.org/pdf/2511.20784.pdf)

**ä½œè€…**: Sindhuja Penchala, Gavin Money, Gabriel Marques, Samuel Wood, Jessica Kirschman, Travis Atkison, Shahram Rahimi, Noorbakhsh Amiri Golilarz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: 9 pages,3 figures, 5 tables

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SMARCï¼šä»…éœ€å›¾åƒ10%åŒºåŸŸï¼Œå³å¯å®žçŽ°è¡¨é¢æè´¨é‡å»ºä¸Žåˆ†ç±»**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨é¢æè´¨é‡å»º` `æè´¨åˆ†ç±»` `éƒ¨åˆ†å·ç§¯` `U-Net` `ç¨€ç–è§†è§‰ä¿¡æ¯` `æœºå™¨äºº` `å›¾åƒä¿®å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æè´¨ç†è§£æ–¹æ³•ä¾èµ–ç¨ å¯†è§‚æµ‹ï¼Œé™åˆ¶äº†å…¶åœ¨å—é™æˆ–éƒ¨åˆ†è§†å›¾çŽ¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. SMARCæ¨¡åž‹ä»…éœ€å›¾åƒçš„10%åŒºåŸŸï¼Œå³å¯åŒæ—¶å®Œæˆè¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ä»»åŠ¡ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒSMARCåœ¨è¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSMARCçš„ç»Ÿä¸€æ¨¡åž‹ï¼Œç”¨äºŽä»Žæžå°‘çš„è§†è§‰è¾“å…¥ä¸­è¿›è¡Œè¡¨é¢æè´¨é‡å»ºå’Œåˆ†ç±»ã€‚é’ˆå¯¹æœºå™¨äººã€ä»¿çœŸå’Œæè´¨æ„ŸçŸ¥ç­‰åº”ç”¨ä¸­ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–äºŽç¨ å¯†æˆ–å®Œæ•´åœºæ™¯è§‚æµ‹çš„å±€é™æ€§ï¼ŒSMARCä»…éœ€å›¾åƒä¸­ä¸€ä¸ªè¿žç»­çš„10%åŒºåŸŸï¼Œå³å¯è¯†åˆ«å¹¶é‡å»ºå®Œæ•´çš„RGBè¡¨é¢ï¼ŒåŒæ—¶å¯¹æè´¨ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚è¯¥æž¶æž„ç»“åˆäº†éƒ¨åˆ†å·ç§¯U-Netå’Œä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå®žçŽ°äº†åœ¨æžç«¯è§‚æµ‹ç¨€ç–æƒ…å†µä¸‹çš„ç©ºé—´ä¿®å¤å’Œè¯­ä¹‰ç†è§£ã€‚åœ¨Touch and Goæ•°æ®é›†ä¸Šï¼ŒSMARCä¸ŽåŒ…æ‹¬å·ç§¯è‡ªç¼–ç å™¨ã€Vision Transformer (ViT)ã€Masked Autoencoder (MAE)ã€Swin Transformerå’ŒDETRåœ¨å†…çš„äº”ç§æ¨¡åž‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå–å¾—äº†state-of-the-artçš„ç»“æžœï¼ŒPSNRè¾¾åˆ°17.55 dBï¼Œæè´¨åˆ†ç±»å‡†ç¡®çŽ‡è¾¾åˆ°85.10%ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œéƒ¨åˆ†å·ç§¯åœ¨ç¼ºå¤±æ•°æ®ä¸‹çš„ç©ºé—´æŽ¨ç†æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæžç®€è§†è§‰è¡¨é¢ç†è§£å¥ å®šäº†åšå®žçš„åŸºç¡€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è¡¨é¢æè´¨ç†è§£æ–¹æ³•é€šå¸¸éœ€è¦å®Œæ•´çš„å›¾åƒæˆ–è€…ç¨ å¯†çš„è§‚æµ‹æ•°æ®ï¼Œè¿™åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œæˆ–è€…èµ„æºå—é™çš„åµŒå…¥å¼ç³»ç»Ÿä¸­ï¼Œéš¾ä»¥æ»¡è¶³ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æžåº¦ç¨€ç–çš„è§†è§‰ä¿¡æ¯ä¸‹ï¼Œå‡†ç¡®åœ°é‡å»ºè¡¨é¢æè´¨å¹¶è¿›è¡Œåˆ†ç±»ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽæ— æ³•æœ‰æ•ˆåˆ©ç”¨å±€éƒ¨ä¿¡æ¯è¿›è¡Œå…¨å±€æŽ¨ç†ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSMARCçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éƒ¨åˆ†å·ç§¯ï¼ˆPartial Convolutionï¼‰æ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œå¹¶ç»“åˆU-Netçš„ç»“æž„è¿›è¡Œç©ºé—´ä¿¡æ¯çš„ä¼ é€’å’Œé‡å»ºã€‚é€šè¿‡éƒ¨åˆ†å·ç§¯ï¼Œæ¨¡åž‹å¯ä»¥åªå…³æ³¨å·²çŸ¥çš„åƒç´ ä¿¡æ¯ï¼Œé¿å…ç¼ºå¤±åƒç´ å¸¦æ¥çš„å¹²æ‰°ã€‚åŒæ—¶ï¼ŒU-Netçš„è·³è·ƒè¿žæŽ¥å¯ä»¥æœ‰æ•ˆåœ°å°†ä½Žå±‚ç‰¹å¾ä¼ é€’åˆ°é«˜å±‚ï¼Œä»Žè€Œæ›´å¥½åœ°é‡å»ºå›¾åƒã€‚æ­¤å¤–ï¼Œå¢žåŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œä½¿å¾—æ¨¡åž‹å¯ä»¥åŒæ—¶è¿›è¡Œè¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSMARCçš„æ•´ä½“æž¶æž„æ˜¯ä¸€ä¸ªåŸºäºŽU-Netçš„ç¼–ç å™¨-è§£ç å™¨ç»“æž„ï¼Œå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ä½¿ç”¨äº†éƒ¨åˆ†å·ç§¯å±‚ã€‚åœ¨ç¼–ç å™¨çš„æœ€åŽï¼Œè¿žæŽ¥ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œç”¨äºŽé¢„æµ‹æè´¨ç±»åˆ«ã€‚æ•´ä¸ªæµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œè¾“å…¥ä¸€ä¸ªåŒ…å«10%åŒºåŸŸçš„å›¾åƒpatchï¼›ç„¶åŽï¼Œé€šè¿‡ç¼–ç å™¨æå–ç‰¹å¾ï¼›æŽ¥ç€ï¼Œé€šè¿‡è§£ç å™¨é‡å»ºå®Œæ•´çš„RGBå›¾åƒï¼›åŒæ—¶ï¼Œé€šè¿‡åˆ†ç±»å¤´é¢„æµ‹æè´¨ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šSMARCæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†éƒ¨åˆ†å·ç§¯åº”ç”¨äºŽè¡¨é¢æè´¨é‡å»ºå’Œåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¹¶ç»“åˆU-Netçš„ç»“æž„ï¼Œå®žçŽ°äº†åœ¨æžåº¦ç¨€ç–è§‚æµ‹ä¸‹çš„é«˜æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSMARCèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å±€éƒ¨ä¿¡æ¯è¿›è¡Œå…¨å±€æŽ¨ç†ï¼Œä»Žè€Œåœ¨ç¼ºå¤±å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å¤Ÿå‡†ç¡®åœ°é‡å»ºè¡¨é¢æè´¨å¹¶è¿›è¡Œåˆ†ç±»ã€‚

**å…³é”®è®¾è®¡**ï¼šSMARCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨éƒ¨åˆ†å·ç§¯å±‚æ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œé¿å…æ— æ•ˆåƒç´ çš„å¹²æ‰°ï¼›2) ä½¿ç”¨U-Netç»“æž„è¿›è¡Œç©ºé—´ä¿¡æ¯çš„ä¼ é€’å’Œé‡å»ºï¼›3) å¢žåŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œä½¿å¾—æ¨¡åž‹å¯ä»¥åŒæ—¶è¿›è¡Œè¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ï¼›4) æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±ï¼ˆä¾‹å¦‚L1æŸå¤±æˆ–PSNRï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆä¾‹å¦‚äº¤å‰ç†µæŸå¤±ï¼‰ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å®žé™…æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SMARCåœ¨Touch and Goæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„ç»“æžœï¼ŒPSNRè¾¾åˆ°17.55 dBï¼Œæè´¨åˆ†ç±»å‡†ç¡®çŽ‡è¾¾åˆ°85.10%ã€‚ç›¸æ¯”äºŽå·ç§¯è‡ªç¼–ç å™¨ã€Vision Transformer (ViT)ã€Masked Autoencoder (MAE)ã€Swin Transformerå’ŒDETRç­‰åŸºçº¿æ¨¡åž‹ï¼ŒSMARCåœ¨è¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜Žäº†éƒ¨åˆ†å·ç§¯åœ¨å¤„ç†ç¼ºå¤±æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SMARCåœ¨æœºå™¨äººæ“ä½œã€è™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žã€æè´¨è¯†åˆ«ã€å·¥ä¸šæ£€æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨SMARCä»Žå°‘é‡è§†è§‰ä¿¡æ¯ä¸­ç†è§£ç‰©ä½“è¡¨é¢æè´¨ï¼Œä»Žè€Œæ›´å¥½åœ°è¿›è¡ŒæŠ“å–å’Œæ“ä½œã€‚åœ¨è™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žä¸­ï¼ŒSMARCå¯ä»¥ç”¨äºŽç”Ÿæˆé€¼çœŸçš„è¡¨é¢çº¹ç†ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒSMARCè¿˜å¯ä»¥åº”ç”¨äºŽå·¥ä¸šæ£€æµ‹ä¸­ï¼Œç”¨äºŽè¯†åˆ«äº§å“è¡¨é¢çš„ç¼ºé™·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.

