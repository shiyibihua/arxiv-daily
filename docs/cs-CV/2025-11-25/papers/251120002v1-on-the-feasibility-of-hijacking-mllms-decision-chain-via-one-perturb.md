---
layout: default
title: On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation
---

# On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation

**arXiv**: [2511.20002v1](https://arxiv.org/abs/2511.20002) | [PDF](https://arxiv.org/pdf/2511.20002.pdf)

**ä½œè€…**: Changyue Li, Jiaying Li, Youliang Yuan, Jiaming He, Zhicong Huang, Pinjia He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰æ„ŸçŸ¥é€šç”¨æ‰°åŠ¨ä»¥åŠ«æŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å†³ç­–é“¾**

**å…³é”®è¯**: `å¯¹æŠ—æ”»å‡»` `å†³ç­–é“¾åŠ«æŒ` `è¯­ä¹‰æ„ŸçŸ¥æ‰°åŠ¨` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å®‰å…¨é£Žé™©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå¯¹æŠ—æ”»å‡»ä»…æ“çºµå•ä¸€å†³ç­–ï¼Œè€ŒçœŸå®žæ¨¡åž‹å†³ç­–é“¾ä¸­çš„çº§è”é”™è¯¯å¯èƒ½å¯¼è‡´ä¸¥é‡é£Žé™©ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è¯­ä¹‰æ„ŸçŸ¥é€šç”¨æ‰°åŠ¨ï¼Œé€šè¿‡å½’ä¸€åŒ–ç©ºé—´æœç´¢å’Œè¯­ä¹‰åˆ†ç¦»ç­–ç•¥å®žçŽ°å¤šç›®æ ‡æ“æŽ§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ä¸Šæµ‹è¯•ï¼Œä½¿ç”¨å¯¹æŠ—å¸§æŽ§åˆ¶äº”ä¸ªç›®æ ‡æ—¶æ”»å‡»æˆåŠŸçŽ‡é«˜è¾¾70%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Conventional adversarial attacks focus on manipulating a single decision of neural networks. However, real-world models often operate in a sequence of decisions, where an isolated mistake can be easily corrected, but cascading errors can lead to severe risks.
>   This paper reveals a novel threat: a single perturbation can hijack the whole decision chain. We demonstrate the feasibility of manipulating a model's outputs toward multiple, predefined outcomes, such as simultaneously misclassifying "non-motorized lane" signs as "motorized lane" and "pedestrian" as "plastic bag".
>   To expose this threat, we introduce Semantic-Aware Universal Perturbations (SAUPs), which induce varied outcomes based on the semantics of the inputs. We overcome optimization challenges by developing an effective algorithm, which searches for perturbations in normalized space with a semantic separation strategy. To evaluate the practical threat of SAUPs, we present RIST, a new real-world image dataset with fine-grained semantic annotations. Extensive experiments on three multimodal large language models demonstrate their vulnerability, achieving a 70% attack success rate when controlling five distinct targets using just an adversarial frame.

