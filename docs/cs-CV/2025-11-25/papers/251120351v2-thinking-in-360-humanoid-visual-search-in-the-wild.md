---
layout: default
title: Thinking in 360Â°: Humanoid Visual Search in the Wild
---

# Thinking in 360Â°: Humanoid Visual Search in the Wild

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.20351" target="_blank" class="toolbar-btn">arXiv: 2511.20351v2</a>
    <a href="https://arxiv.org/pdf/2511.20351.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20351v2" 
            onclick="toggleFavorite(this, '2511.20351v2', 'Thinking in 360Â°: Humanoid Visual Search in the Wild')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Heyang Yu, Yinan Han, Xiangyu Zhang, Baiqiao Yin, Bowen Chang, Xiangyu Han, Xinhao Liu, Jing Zhang, Marco Pavone, Chen Feng, Saining Xie, Yiming Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25 (æ›´æ–°: 2025-11-26)

**å¤‡æ³¨**: Website: https://humanoid-vstar.github.io/ ; Code: https://github.com/humanoid-vstar/hstar

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºH* BenchåŸºå‡†ï¼Œç ”ç©¶å…·èº«æ™ºèƒ½ä½“åœ¨360Â°å…¨æ™¯å›¾åƒä¸­çš„è§†è§‰æœç´¢èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `è§†è§‰æœç´¢` `360Â°å…¨æ™¯å›¾åƒ` `äººå‹æ™ºèƒ½ä½“` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æœç´¢æ–¹æ³•å¿½ç•¥äº†å…·èº«æ™ºèƒ½ä½“ä¸3Dä¸–ç•Œçš„äº¤äº’ï¼Œæ— æ³•æ¨¡æ‹Ÿäººç±»åœ¨360Â°ç¯å¢ƒä¸‹çš„æœç´¢è¡Œä¸ºã€‚
2. æå‡ºäººå‹è§†è§‰æœç´¢ï¼Œè®©äººå‹æ™ºèƒ½ä½“ä¸»åŠ¨æ—‹è½¬å¤´éƒ¨ï¼Œåœ¨360Â°å…¨æ™¯å›¾åƒä¸­æœç´¢ç‰©ä½“æˆ–è·¯å¾„ï¼Œæ¨¡æ‹Ÿäººç±»è§†è§‰æœç´¢ã€‚
3. æ„å»ºH* BenchåŸºå‡†ï¼ŒåŒ…å«å¤æ‚çš„çœŸå®åœºæ™¯ï¼Œå®éªŒè¡¨æ˜ä¼˜åŒ–åçš„Qwen2.5-VLæ¨¡å‹åœ¨ç‰©ä½“å’Œè·¯å¾„æœç´¢ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§äººå‹è§†è§‰æœç´¢æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººç±»é€šè¿‡å¤´éƒ¨å’Œçœ¼ç›çš„ååŒæ§åˆ¶åœ¨360Â°ç¯å¢ƒä¸­è¿›è¡Œè§†è§‰ä¿¡æ¯æœç´¢ã€‚ä¸ºäº†å…‹æœç°æœ‰è§†è§‰æœç´¢æ–¹æ³•å±€é™äºé™æ€å›¾åƒçš„ç¼ºç‚¹ï¼Œå¹¶æ‘†è„±ç°å®ä¸–ç•Œç¡¬ä»¶çš„é™åˆ¶ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåä¸ºH* Benchçš„æ–°åŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«äº¤é€šæ¢çº½ã€å¤§å‹é›¶å”®ç©ºé—´ã€åŸå¸‚è¡—é“å’Œå…¬å…±æœºæ„ç­‰å¤æ‚çš„çœŸå®åœºæ™¯ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨è§†è§‰æ‹¥æŒ¤ç¯å¢ƒä¸­çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯é¡¶çº§çš„å•†ä¸šæ¨¡å‹åœ¨ç‰©ä½“å’Œè·¯å¾„æœç´¢ä»»åŠ¡ä¸­ä¹Ÿä»…èƒ½è¾¾åˆ°çº¦30%çš„æˆåŠŸç‡ã€‚é€šè¿‡åè®­ç»ƒæŠ€æœ¯ä¼˜åŒ–å¼€æºçš„Qwen2.5-VLæ¨¡å‹ï¼Œç‰©ä½“æœç´¢çš„æˆåŠŸç‡ä»14.83%æå‡è‡³47.38%ï¼Œè·¯å¾„æœç´¢çš„æˆåŠŸç‡ä»6.44%æå‡è‡³24.94%ã€‚è·¯å¾„æœç´¢è¾ƒä½çš„ä¸Šé™è¡¨æ˜å…¶éš¾åº¦æ›´é«˜ï¼Œè¿™å½’å› äºå¯¹å¤æ‚ç©ºé—´å¸¸è¯†çš„éœ€æ±‚ã€‚ç ”ç©¶ç»“æœå±•ç¤ºäº†å…·èº«æ™ºèƒ½ä½“çš„å‘å±•å‰æ™¯ï¼ŒåŒæ—¶ä¹Ÿé‡åŒ–äº†æ„å»ºèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°æ—¥å¸¸äººç±»ç”Ÿæ´»ä¸­çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ‰€é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰æœç´¢æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿäººç±»åœ¨360Â°å…¨æ™¯ç¯å¢ƒä¸­è¿›è¡Œè§†è§‰æœç´¢çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å¤„ç†é™æ€å›¾åƒï¼Œå¿½ç•¥äº†å¤´éƒ¨è¿åŠ¨å’Œç¯å¢ƒäº¤äº’ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“åœ¨å¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œç¼ºä¹é’ˆå¯¹çœŸå®ä¸–ç•Œå¤æ‚åœºæ™¯çš„åŸºå‡†æ•°æ®é›†ï¼Œé˜»ç¢äº†ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®©äººå‹æ™ºèƒ½ä½“æ¨¡æ‹Ÿäººç±»çš„å¤´éƒ¨è¿åŠ¨ï¼Œé€šè¿‡ä¸»åŠ¨æ—‹è½¬å¤´éƒ¨æ¥æ¢ç´¢360Â°å…¨æ™¯å›¾åƒï¼Œä»è€Œè¿›è¡Œç‰©ä½“æˆ–è·¯å¾„æœç´¢ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»è§†è§‰æœç´¢çš„è‡ªç„¶æ–¹å¼ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ç¯å¢ƒä¿¡æ¯ï¼Œæé«˜æœç´¢æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1ï¼‰360Â°å…¨æ™¯å›¾åƒè¾“å…¥ï¼›2ï¼‰äººå‹æ™ºèƒ½ä½“æ¨¡å‹ï¼Œè´Ÿè´£æ§åˆ¶å¤´éƒ¨æ—‹è½¬ï¼›3ï¼‰è§†è§‰æœç´¢æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«ç›®æ ‡ç‰©ä½“æˆ–è·¯å¾„ï¼›4ï¼‰H* BenchåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸æ–­è°ƒæ•´å¤´éƒ¨å§¿æ€ï¼Œè§‚å¯Ÿå…¨æ™¯å›¾åƒï¼Œå¹¶åˆ©ç”¨è§†è§‰æœç´¢æ¨¡å‹åˆ¤æ–­ç›®æ ‡æ˜¯å¦å­˜åœ¨æˆ–è·¯å¾„æ˜¯å¦å¯è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†äººå‹è§†è§‰æœç´¢çš„æ¦‚å¿µï¼Œå¹¶å°†å…·èº«æ™ºèƒ½ä½“å¼•å…¥åˆ°360Â°å…¨æ™¯å›¾åƒæœç´¢ä¸­ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†H* BenchåŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤æ‚çš„çœŸå®ä¸–ç•Œåœºæ™¯ï¼Œæ›´å…·æŒ‘æˆ˜æ€§ã€‚é€šè¿‡åè®­ç»ƒæŠ€æœ¯ä¼˜åŒ–å¼€æºçš„Qwen2.5-VLæ¨¡å‹ï¼Œä½¿å…¶åœ¨H* BenchåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨Qwen2.5-VLä½œä¸ºè§†è§‰æœç´¢æ¨¡å‹ï¼Œå¹¶é€šè¿‡åè®­ç»ƒæŠ€æœ¯å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚åè®­ç»ƒçš„å…·ä½“ç»†èŠ‚æœªçŸ¥ï¼Œä½†ç›®æ ‡æ˜¯æé«˜æ¨¡å‹åœ¨H* BenchåŸºå‡†ä¸Šçš„ç‰©ä½“å’Œè·¯å¾„æœç´¢èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯é¡¶çº§çš„å•†ä¸šæ¨¡å‹åœ¨H* BenchåŸºå‡†ä¸Šçš„ç‰©ä½“å’Œè·¯å¾„æœç´¢ä»»åŠ¡ä¸­ä¹Ÿä»…èƒ½è¾¾åˆ°çº¦30%çš„æˆåŠŸç‡ã€‚é€šè¿‡åè®­ç»ƒæŠ€æœ¯ä¼˜åŒ–å¼€æºçš„Qwen2.5-VLæ¨¡å‹ï¼Œç‰©ä½“æœç´¢çš„æˆåŠŸç‡ä»14.83%æå‡è‡³47.38%ï¼Œè·¯å¾„æœç´¢çš„æˆåŠŸç‡ä»6.44%æå‡è‡³24.94%ã€‚è¿™è¡¨æ˜ï¼Œé€šè¿‡é€‚å½“çš„æ¨¡å‹ä¼˜åŒ–å’Œè®­ç»ƒï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«è§†è§‰æœç´¢ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å¼€å‘æ™ºèƒ½å¯¼ç›²æœºå™¨äººï¼Œå¸®åŠ©è§†éšœäººå£«åœ¨å¤æ‚ç¯å¢ƒä¸­å®‰å…¨å¯¼èˆªï¼›ä¹Ÿå¯ä»¥åº”ç”¨äºè™šæ‹Ÿæ—…æ¸¸ï¼Œè®©ç”¨æˆ·é€šè¿‡æ§åˆ¶è™šæ‹Ÿäººå‹æ™ºèƒ½ä½“æ¢ç´¢360Â°å…¨æ™¯åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360Â°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360Â° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.

