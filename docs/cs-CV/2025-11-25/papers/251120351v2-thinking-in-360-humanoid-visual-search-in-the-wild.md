---
layout: default
title: Thinking in 360Â°: Humanoid Visual Search in the Wild
---

# Thinking in 360Â°: Humanoid Visual Search in the Wild

**arXiv**: [2511.20351v2](https://arxiv.org/abs/2511.20351) | [PDF](https://arxiv.org/pdf/2511.20351.pdf)

**ä½œè€…**: Heyang Yu, Yinan Han, Xiangyu Zhang, Baiqiao Yin, Bowen Chang, Xiangyu Han, Xinhao Liu, Jing Zhang, Marco Pavone, Chen Feng, Saining Xie, Yiming Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25 (æ›´æ–°: 2025-11-26)

**å¤‡æ³¨**: Website: https://humanoid-vstar.github.io/ ; Code: https://github.com/humanoid-vstar/hstar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºH* BenchåŸºå‡†ï¼Œç ”ç©¶å…·èº«æ™ºèƒ½ä½“åœ¨360Â°å…¨æ™¯å›¾åƒä¸­çš„è§†è§‰æœç´¢èƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `è§†è§‰æœç´¢` `360Â°å…¨æ™¯å›¾åƒ` `äººåž‹æ™ºèƒ½ä½“` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰æœç´¢æ–¹æ³•å¿½ç•¥äº†å…·èº«æ™ºèƒ½ä½“ä¸Ž3Dä¸–ç•Œçš„äº¤äº’ï¼Œæ— æ³•æ¨¡æ‹Ÿäººç±»åœ¨360Â°çŽ¯å¢ƒä¸‹çš„æœç´¢è¡Œä¸ºã€‚
2. æå‡ºäººåž‹è§†è§‰æœç´¢ï¼Œè®©äººåž‹æ™ºèƒ½ä½“ä¸»åŠ¨æ—‹è½¬å¤´éƒ¨ï¼Œåœ¨360Â°å…¨æ™¯å›¾åƒä¸­æœç´¢ç‰©ä½“æˆ–è·¯å¾„ï¼Œæ¨¡æ‹Ÿäººç±»è§†è§‰æœç´¢ã€‚
3. æž„å»ºH* BenchåŸºå‡†ï¼ŒåŒ…å«å¤æ‚çš„çœŸå®žåœºæ™¯ï¼Œå®žéªŒè¡¨æ˜Žä¼˜åŒ–åŽçš„Qwen2.5-VLæ¨¡åž‹åœ¨ç‰©ä½“å’Œè·¯å¾„æœç´¢ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§äººåž‹è§†è§‰æœç´¢æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººç±»é€šè¿‡å¤´éƒ¨å’Œçœ¼ç›çš„ååŒæŽ§åˆ¶åœ¨360Â°çŽ¯å¢ƒä¸­è¿›è¡Œè§†è§‰ä¿¡æ¯æœç´¢ã€‚ä¸ºäº†å…‹æœçŽ°æœ‰è§†è§‰æœç´¢æ–¹æ³•å±€é™äºŽé™æ€å›¾åƒçš„ç¼ºç‚¹ï¼Œå¹¶æ‘†è„±çŽ°å®žä¸–ç•Œç¡¬ä»¶çš„é™åˆ¶ï¼Œæœ¬æ–‡æž„å»ºäº†ä¸€ä¸ªåä¸ºH* Benchçš„æ–°åŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«äº¤é€šæž¢çº½ã€å¤§åž‹é›¶å”®ç©ºé—´ã€åŸŽå¸‚è¡—é“å’Œå…¬å…±æœºæž„ç­‰å¤æ‚çš„çœŸå®žåœºæ™¯ï¼Œç”¨äºŽè¯„ä¼°æ™ºèƒ½ä½“åœ¨è§†è§‰æ‹¥æŒ¤çŽ¯å¢ƒä¸­çš„è§†è§‰ç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚å®žéªŒè¡¨æ˜Žï¼Œå³ä½¿æ˜¯é¡¶çº§çš„å•†ä¸šæ¨¡åž‹åœ¨ç‰©ä½“å’Œè·¯å¾„æœç´¢ä»»åŠ¡ä¸­ä¹Ÿä»…èƒ½è¾¾åˆ°çº¦30%çš„æˆåŠŸçŽ‡ã€‚é€šè¿‡åŽè®­ç»ƒæŠ€æœ¯ä¼˜åŒ–å¼€æºçš„Qwen2.5-VLæ¨¡åž‹ï¼Œç‰©ä½“æœç´¢çš„æˆåŠŸçŽ‡ä»Ž14.83%æå‡è‡³47.38%ï¼Œè·¯å¾„æœç´¢çš„æˆåŠŸçŽ‡ä»Ž6.44%æå‡è‡³24.94%ã€‚è·¯å¾„æœç´¢è¾ƒä½Žçš„ä¸Šé™è¡¨æ˜Žå…¶éš¾åº¦æ›´é«˜ï¼Œè¿™å½’å› äºŽå¯¹å¤æ‚ç©ºé—´å¸¸è¯†çš„éœ€æ±‚ã€‚ç ”ç©¶ç»“æžœå±•ç¤ºäº†å…·èº«æ™ºèƒ½ä½“çš„å‘å±•å‰æ™¯ï¼ŒåŒæ—¶ä¹Ÿé‡åŒ–äº†æž„å»ºèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°æ—¥å¸¸äººç±»ç”Ÿæ´»ä¸­çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹æ™ºèƒ½ä½“æ‰€é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çŽ°æœ‰è§†è§‰æœç´¢æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿäººç±»åœ¨360Â°å…¨æ™¯çŽ¯å¢ƒä¸­è¿›è¡Œè§†è§‰æœç´¢çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦å¤„ç†é™æ€å›¾åƒï¼Œå¿½ç•¥äº†å¤´éƒ¨è¿åŠ¨å’ŒçŽ¯å¢ƒäº¤äº’ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“åœ¨å¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œç¼ºä¹é’ˆå¯¹çœŸå®žä¸–ç•Œå¤æ‚åœºæ™¯çš„åŸºå‡†æ•°æ®é›†ï¼Œé˜»ç¢äº†ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®©äººåž‹æ™ºèƒ½ä½“æ¨¡æ‹Ÿäººç±»çš„å¤´éƒ¨è¿åŠ¨ï¼Œé€šè¿‡ä¸»åŠ¨æ—‹è½¬å¤´éƒ¨æ¥æŽ¢ç´¢360Â°å…¨æ™¯å›¾åƒï¼Œä»Žè€Œè¿›è¡Œç‰©ä½“æˆ–è·¯å¾„æœç´¢ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»è§†è§‰æœç´¢çš„è‡ªç„¶æ–¹å¼ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨çŽ¯å¢ƒä¿¡æ¯ï¼Œæé«˜æœç´¢æ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1ï¼‰360Â°å…¨æ™¯å›¾åƒè¾“å…¥ï¼›2ï¼‰äººåž‹æ™ºèƒ½ä½“æ¨¡åž‹ï¼Œè´Ÿè´£æŽ§åˆ¶å¤´éƒ¨æ—‹è½¬ï¼›3ï¼‰è§†è§‰æœç´¢æ¨¡åž‹ï¼Œç”¨äºŽè¯†åˆ«ç›®æ ‡ç‰©ä½“æˆ–è·¯å¾„ï¼›4ï¼‰H* BenchåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºŽè¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸æ–­è°ƒæ•´å¤´éƒ¨å§¿æ€ï¼Œè§‚å¯Ÿå…¨æ™¯å›¾åƒï¼Œå¹¶åˆ©ç”¨è§†è§‰æœç´¢æ¨¡åž‹åˆ¤æ–­ç›®æ ‡æ˜¯å¦å­˜åœ¨æˆ–è·¯å¾„æ˜¯å¦å¯è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†äººåž‹è§†è§‰æœç´¢çš„æ¦‚å¿µï¼Œå¹¶å°†å…·èº«æ™ºèƒ½ä½“å¼•å…¥åˆ°360Â°å…¨æ™¯å›¾åƒæœç´¢ä¸­ã€‚æ­¤å¤–ï¼Œæž„å»ºäº†H* BenchåŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤æ‚çš„çœŸå®žä¸–ç•Œåœºæ™¯ï¼Œæ›´å…·æŒ‘æˆ˜æ€§ã€‚é€šè¿‡åŽè®­ç»ƒæŠ€æœ¯ä¼˜åŒ–å¼€æºçš„Qwen2.5-VLæ¨¡åž‹ï¼Œä½¿å…¶åœ¨H* BenchåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨Qwen2.5-VLä½œä¸ºè§†è§‰æœç´¢æ¨¡åž‹ï¼Œå¹¶é€šè¿‡åŽè®­ç»ƒæŠ€æœ¯å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚åŽè®­ç»ƒçš„å…·ä½“ç»†èŠ‚æœªçŸ¥ï¼Œä½†ç›®æ ‡æ˜¯æé«˜æ¨¡åž‹åœ¨H* BenchåŸºå‡†ä¸Šçš„ç‰©ä½“å’Œè·¯å¾„æœç´¢èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜Žã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œå³ä½¿æ˜¯é¡¶çº§çš„å•†ä¸šæ¨¡åž‹åœ¨H* BenchåŸºå‡†ä¸Šçš„ç‰©ä½“å’Œè·¯å¾„æœç´¢ä»»åŠ¡ä¸­ä¹Ÿä»…èƒ½è¾¾åˆ°çº¦30%çš„æˆåŠŸçŽ‡ã€‚é€šè¿‡åŽè®­ç»ƒæŠ€æœ¯ä¼˜åŒ–å¼€æºçš„Qwen2.5-VLæ¨¡åž‹ï¼Œç‰©ä½“æœç´¢çš„æˆåŠŸçŽ‡ä»Ž14.83%æå‡è‡³47.38%ï¼Œè·¯å¾„æœç´¢çš„æˆåŠŸçŽ‡ä»Ž6.44%æå‡è‡³24.94%ã€‚è¿™è¡¨æ˜Žï¼Œé€šè¿‡é€‚å½“çš„æ¨¡åž‹ä¼˜åŒ–å’Œè®­ç»ƒï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å…·èº«è§†è§‰æœç´¢ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººå¯¼èˆªã€è™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å¼€å‘æ™ºèƒ½å¯¼ç›²æœºå™¨äººï¼Œå¸®åŠ©è§†éšœäººå£«åœ¨å¤æ‚çŽ¯å¢ƒä¸­å®‰å…¨å¯¼èˆªï¼›ä¹Ÿå¯ä»¥åº”ç”¨äºŽè™šæ‹Ÿæ—…æ¸¸ï¼Œè®©ç”¨æˆ·é€šè¿‡æŽ§åˆ¶è™šæ‹Ÿäººåž‹æ™ºèƒ½ä½“æŽ¢ç´¢360Â°å…¨æ™¯åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºŽæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360Â°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360Â° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.

