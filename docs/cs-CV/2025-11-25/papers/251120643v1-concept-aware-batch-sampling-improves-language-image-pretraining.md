---
layout: default
title: Concept-Aware Batch Sampling Improves Language-Image Pretraining
---

# Concept-Aware Batch Sampling Improves Language-Image Pretraining

**arXiv**: [2511.20643v1](https://arxiv.org/abs/2511.20643) | [PDF](https://arxiv.org/pdf/2511.20643.pdf)

**ä½œè€…**: Adhiraj Ghosh, Vishaal Udandarao, Thao Nguyen, Matteo Farina, Mehdi Cherti, Jenia Jitsev, Sewoong Oh, Elisa Ricci, Ludwig Schmidt, Matthias Bethge

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¦‚å¿µæ„ŸçŸ¥æ‰¹é‡‡æ ·ä»¥æ”¹è¿›è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ**

**å…³é”®è¯**: `è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ` `æ¦‚å¿µæ„ŸçŸ¥é‡‡æ ·` `æ•°æ®ç­›é€‰` `åœ¨çº¿å­¦ä¹ ` `CLIPæ¨¡åž‹` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ•°æ®ç­›é€‰æ–¹æ³•ç¦»çº¿ä¸”æ¦‚å¿µæ— å…³ï¼Œæ˜“å¼•å…¥æ•°æ®åè§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽDataConceptæ•°æ®é›†ï¼Œå¼€å‘åœ¨çº¿æ¦‚å¿µæ„ŸçŸ¥æ‰¹é‡‡æ ·æ¡†æž¶CABSã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨28ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡CLIP/SigLIPæ¨¡åž‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.

