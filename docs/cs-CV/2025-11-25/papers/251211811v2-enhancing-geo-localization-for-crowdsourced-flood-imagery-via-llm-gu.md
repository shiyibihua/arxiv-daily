---
layout: default
title: Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention
---

# Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention

**arXiv**: [2512.11811v2](https://arxiv.org/abs/2512.11811) | [PDF](https://arxiv.org/pdf/2512.11811.pdf)

**ä½œè€…**: Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo, Jack C. P. Cheng

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25 (æ›´æ–°: 2025-12-16)

**å¤‡æ³¨**: Updated author list to include additional contributor. Revised title and improved methodology section based on collaborative feedback

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VPR-AttLLMï¼šåˆ©ç”¨LLMå¼•å¯¼çš„æ³¨æ„åŠ›å¢žå¼ºä¼—åŒ…æ´ªæ°´å›¾åƒçš„åœ°ç†å®šä½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰ä½ç½®è¯†åˆ«` `åœ°ç†å®šä½` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `æ³¨æ„åŠ›æœºåˆ¶` `ä¼—åŒ…å›¾åƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰ä½ç½®è¯†åˆ«æ¨¡åž‹åœ¨å¤„ç†ä¼—åŒ…å›¾åƒæ—¶ï¼Œç”±äºŽè§†è§‰å¤±çœŸå’Œè·¨æºåŸŸåç§»ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œéš¾ä»¥æ»¡è¶³åº”æ€¥å“åº”éœ€æ±‚ã€‚
2. VPR-AttLLMåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹çš„è¯­ä¹‰æŽ¨ç†å’Œåœ°ç†çŸ¥è¯†ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼æè¿°ç¬¦å¢žå¼ºï¼Œæå‡VPRæ¨¡åž‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ£€ç´¢èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒVPR-AttLLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡èƒ½æœ‰æ•ˆæå‡çŽ°æœ‰VPRæ¨¡åž‹çš„å¬å›žæ€§èƒ½ï¼Œåœ¨çœŸå®žæ´ªæ°´å›¾åƒä¸Šæå‡é«˜è¾¾8%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVPR-AttLLMçš„æ¨¡åž‹æ— å…³æ¡†æž¶ï¼Œè¯¥æ¡†æž¶é€šè¿‡æ³¨æ„åŠ›å¼•å¯¼çš„æè¿°ç¬¦å¢žå¼ºï¼Œå°†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰æŽ¨ç†å’Œåœ°ç†çŸ¥è¯†é›†æˆåˆ°çŽ°æœ‰çš„è§†è§‰ä½ç½®è¯†åˆ«ï¼ˆVPRï¼‰æµç¨‹ä¸­ã€‚é€šè¿‡åˆ©ç”¨LLMè¯†åˆ«åŸŽå¸‚çŽ¯å¢ƒä¸­å…·æœ‰ä½ç½®ä¿¡æ¯çš„åŒºåŸŸå¹¶æŠ‘åˆ¶è§†è§‰å™ªå£°ï¼ŒVPR-AttLLMæé«˜äº†æ£€ç´¢æ€§èƒ½ï¼Œè€Œæ— éœ€æ¨¡åž‹é‡æ–°è®­ç»ƒæˆ–é¢å¤–æ•°æ®ã€‚åœ¨æ‰©å±•çš„åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ç”¨çœŸå®žç¤¾äº¤åª’ä½“æ´ªæ°´å›¾åƒä¸°å¯Œçš„SF-XLã€æ—¢å®šæŸ¥è¯¢é›†ä¸Šçš„åˆæˆæ´ªæ°´åœºæ™¯å’ŒMapillaryç…§ç‰‡ï¼‰ä»¥åŠæ•èŽ·å½¢æ€å„å¼‚åŸŽå¸‚æ™¯è§‚çš„æ–°HK-URBANæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å°†VPR-AttLLMä¸Žä¸‰ç§æœ€å…ˆè¿›çš„VPRæ¨¡åž‹ï¼ˆCosPlaceã€EigenPlaceså’ŒSALADï¼‰é›†æˆï¼Œå§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†å¬å›žæ€§èƒ½ï¼Œç›¸å¯¹å¢žç›Šé€šå¸¸åœ¨1-3%ä¹‹é—´ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„çœŸå®žæ´ªæ°´å›¾åƒä¸Šè¾¾åˆ°8%ã€‚é™¤äº†å¯è¡¡é‡çš„æ£€ç´¢å‡†ç¡®çŽ‡æå‡å¤–ï¼Œæœ¬ç ”ç©¶è¿˜ä¸ºè§†è§‰æ£€ç´¢ç³»ç»Ÿä¸­LLMå¼•å¯¼çš„å¤šæ¨¡æ€èžåˆå»ºç«‹äº†ä¸€ç§é€šç”¨èŒƒä¾‹ã€‚é€šè¿‡å°†åŸŽå¸‚æ„ŸçŸ¥ç†è®ºçš„åŽŸåˆ™åµŒå…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒVPR-AttLLMå°†ç±»äººç©ºé—´æŽ¨ç†ä¸ŽçŽ°ä»£VPRæž¶æž„è”ç³»èµ·æ¥ã€‚å…¶å³æ’å³ç”¨è®¾è®¡ã€å¼ºå¤§çš„è·¨æºé²æ£’æ€§å’Œå¯è§£é‡Šæ€§çªå‡ºäº†å…¶åœ¨å¯æ‰©å±•åŸŽå¸‚ç›‘æµ‹å’Œä¼—åŒ…å±æœºå›¾åƒå¿«é€Ÿåœ°ç†å®šä½æ–¹é¢çš„æ½œåŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¼—åŒ…å›¾åƒåœ°ç†å®šä½é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸Šç¼ºä¹å¯é åœ°ç†å…ƒæ•°æ®çš„æ´ªæ°´å›¾åƒã€‚çŽ°æœ‰è§†è§‰ä½ç½®è¯†åˆ«ï¼ˆVPRï¼‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»å›¾åƒæ—¶ï¼Œç”±äºŽå›¾åƒè´¨é‡å·®ã€è§†è§’å˜åŒ–å¤§ã€å…‰ç…§æ¡ä»¶æ¶åŠ£ç­‰å› ç´ ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œéš¾ä»¥æ»¡è¶³åº”æ€¥å“åº”çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰ç†è§£å’Œåœ°ç†çŸ¥è¯†ï¼Œå¼•å¯¼VPRæ¨¡åž‹å…³æ³¨å›¾åƒä¸­ä¸Žä½ç½®ä¿¡æ¯ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶æŠ‘åˆ¶å™ªå£°å¹²æ‰°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¢žå¼ºVPRæ¨¡åž‹å¯¹å›¾åƒç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œæé«˜åœ°ç†å®šä½çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVPR-AttLLMæ˜¯ä¸€ä¸ªæ¨¡åž‹æ— å…³çš„æ¡†æž¶ï¼Œå¯ä»¥ä¸ŽçŽ°æœ‰çš„VPRæ¨¡åž‹é›†æˆã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨LLMåˆ†æžå›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­ä¸Žä½ç½®ä¿¡æ¯ç›¸å…³çš„åŒºåŸŸï¼›2) åŸºäºŽLLMçš„åˆ†æžç»“æžœï¼Œç”Ÿæˆæ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºŽå¢žå¼ºVPRæ¨¡åž‹çš„å›¾åƒæè¿°ç¬¦ï¼›3) ä½¿ç”¨å¢žå¼ºåŽçš„æè¿°ç¬¦è¿›è¡Œè§†è§‰ä½ç½®æ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†LLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›èžå…¥åˆ°VPRä»»åŠ¡ä¸­ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼æ¨¡åž‹å…³æ³¨å›¾åƒä¸­ä¸Žä½ç½®ä¿¡æ¯ç›¸å…³çš„åŒºåŸŸã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†LLMçš„çŸ¥è¯†ï¼Œæé«˜äº†VPRæ¨¡åž‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVPR-AttLLMæ— éœ€é‡æ–°è®­ç»ƒVPRæ¨¡åž‹æˆ–æ”¶é›†é¢å¤–çš„æ•°æ®ï¼Œå…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§å’Œå®žç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVPR-AttLLMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„LLMï¼ˆå¦‚GPT-3ï¼‰è¿›è¡Œå›¾åƒåˆ†æžï¼Œæå–å›¾åƒä¸­çš„è¯­ä¹‰ä¿¡æ¯å’Œåœ°ç†çŸ¥è¯†ï¼›2) è®¾è®¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®LLMçš„åˆ†æžç»“æžœï¼Œå¯¹å›¾åƒçš„ä¸åŒåŒºåŸŸèµ‹äºˆä¸åŒçš„æƒé‡ï¼›3) å°†æ³¨æ„åŠ›æƒé‡ä¸ŽVPRæ¨¡åž‹çš„å›¾åƒæè¿°ç¬¦è¿›è¡Œèžåˆï¼Œç”Ÿæˆå¢žå¼ºåŽçš„æè¿°ç¬¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„å–å†³äºŽæ‰€ä½¿ç”¨çš„VPRæ¨¡åž‹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVPR-AttLLMèƒ½å¤Ÿæ˜¾è‘—æå‡çŽ°æœ‰VPRæ¨¡åž‹çš„æ€§èƒ½ã€‚åœ¨SF-XLæ•°æ®é›†ä¸Šï¼Œä¸ŽCosPlaceã€EigenPlaceså’ŒSALADç­‰åŸºçº¿æ¨¡åž‹ç›¸æ¯”ï¼ŒVPR-AttLLMçš„å¬å›žçŽ‡åˆ†åˆ«æå‡äº†1-3%ã€‚åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„çœŸå®žæ´ªæ°´å›¾åƒä¸Šï¼ŒVPR-AttLLMçš„å¬å›žçŽ‡æå‡é«˜è¾¾8%ã€‚æ­¤å¤–ï¼ŒVPR-AttLLMåœ¨HK-URBANæ•°æ®é›†ä¸Šä¹Ÿè¡¨çŽ°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜Žäº†å…¶åœ¨ä¸åŒåŸŽå¸‚æ™¯è§‚ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽåŸŽå¸‚åº”æ€¥ç®¡ç†ã€ç¾å®³ç›‘æµ‹ã€æ™ºèƒ½äº¤é€šç­‰é¢†åŸŸã€‚é€šè¿‡å¿«é€Ÿå‡†ç¡®åœ°å®šä½ä¼—åŒ…å›¾åƒï¼Œå¯ä»¥å¸®åŠ©åº”æ€¥å“åº”äººå‘˜åŠæ—¶äº†è§£ç¾æƒ…ï¼Œåˆ¶å®šåˆç†çš„æ•‘æ´æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽåŸŽå¸‚è§„åˆ’ã€çŽ¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸï¼Œä¸ºåŸŽå¸‚ç®¡ç†æä¾›æ›´å…¨é¢çš„ä¿¡æ¯æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºŽæ›´å¹¿æ³›çš„è§†è§‰æ£€ç´¢ä»»åŠ¡ï¼Œæå‡å¤šæ¨¡æ€ä¿¡æ¯èžåˆçš„èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

