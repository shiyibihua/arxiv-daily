---
layout: default
title: Reinforcing Action Policies by Prophesying
---

# Reinforcing Action Policies by Prophesying

**arXiv**: [2511.20633v1](https://arxiv.org/abs/2511.20633) | [PDF](https://arxiv.org/pdf/2511.20633.pdf)

**ä½œè€…**: Jiahui Zhang, Ze Huang, Chun Gu, Zipei Ma, Li Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProphRLæ–¹æ³•ï¼Œé€šè¿‡ä¸–ç•Œæ¨¡åž‹å’Œå¼ºåŒ–å­¦ä¹ å¢žå¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥çš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡åž‹` `æœºå™¨äººæŽ§åˆ¶` `ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥æ¨¡ä»¿è®­ç»ƒæ˜“è¿‡æ‹Ÿåˆï¼Œå¼ºåŒ–å­¦ä¹ åœ¨çœŸå®žæœºå™¨äººä¸Šæˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Prophetä¸–ç•Œæ¨¡åž‹é¢„æµ‹åŠ¨ä½œç»“æžœï¼Œç»“åˆFA-GRPOå’ŒFlowScaleä¼˜åŒ–ç­–ç•¥ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨åŸºå‡†æµ‹è¯•å’ŒçœŸå®žæœºå™¨äººä¸Šåˆ†åˆ«æå‡5-17%å’Œ24-30%æˆåŠŸçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.

