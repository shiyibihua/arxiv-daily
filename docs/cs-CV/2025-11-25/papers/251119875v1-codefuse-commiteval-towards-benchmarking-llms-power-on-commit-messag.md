---
layout: default
title: CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection
---

# CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection

**arXiv**: [2511.19875v1](https://arxiv.org/abs/2511.19875) | [PDF](https://arxiv.org/pdf/2511.19875.pdf)

**ä½œè€…**: Qingyu Zhang, Puzhuo Liu, Peng Di, Chenxiong Qian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCODEFUSE-COMMITEVALåŸºå‡†ä»¥è¯„ä¼°LLMåœ¨æäº¤æ¶ˆæ¯ä¸Žä»£ç å˜æ›´ä¸ä¸€è‡´æ£€æµ‹ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `æäº¤æ¶ˆæ¯ä¸ä¸€è‡´æ£€æµ‹` `å¤§åž‹è¯­è¨€æ¨¡åž‹åŸºå‡†` `ä»£ç å˜æ›´åˆ†æž` `å¢žå¼ºç­–ç•¥è¯„ä¼°` `è¯­ä¹‰ä¸ä¸€è‡´æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæäº¤æ¶ˆæ¯ä¸Žä»£ç å˜æ›´ä¸ä¸€è‡´è¯¯å¯¼ä»£ç å®¡æŸ¥å’Œç»´æŠ¤ï¼Œç¼ºä¹ä¸“ç”¨åŸºå‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽApacheCMæ•°æ®é›†ï¼Œé€šè¿‡è§„åˆ™çªå˜ç”Ÿæˆä¸ä¸€è‡´æ¶ˆæ¯ï¼Œå¹¶éªŒè¯æ ·æœ¬è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°å…­ç§LLMï¼Œæ˜¾ç¤ºä¸ä¸€è‡´æ£€æµ‹æ›´å¯é ï¼Œå¢žå¼ºç­–ç•¥æ•ˆæžœå„å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

