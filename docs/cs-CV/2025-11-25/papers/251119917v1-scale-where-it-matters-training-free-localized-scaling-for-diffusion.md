---
layout: default
title: Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models
---

# Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models

**arXiv**: [2511.19917v1](https://arxiv.org/abs/2511.19917) | [PDF](https://arxiv.org/pdf/2511.19917.pdf)

**ä½œè€…**: Qin Ren, Yufei Wang, Lanqing Guo, Wen Zhang, Zhiwen Fan, Chenyu You

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoTTSæ¡†æž¶ä»¥è§£å†³æ‰©æ•£æ¨¡åž‹æŽ¨ç†æ—¶å…¨å›¾ç¼©æ”¾çš„ä½Žæ•ˆé—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `æµ‹è¯•æ—¶ç¼©æ”¾` `å±€éƒ¨ä¼˜åŒ–` `æ³¨æ„åŠ›æœºåˆ¶` `è®­ç»ƒå…è´¹æ–¹æ³•` `å›¾åƒç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•åœ¨å…¨å›¾æ“ä½œï¼Œå¿½ç•¥å›¾åƒè´¨é‡ç©ºé—´å¼‚è´¨æ€§ï¼Œå¯¼è‡´è®¡ç®—æµªè´¹å’Œå±€éƒ¨ç¼ºé™·ä¿®æ­£ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¯¹æ¯”æ³¨æ„åŠ›ä¿¡å·å®šä½ç¼ºé™·åŒºåŸŸï¼Œå¹¶å±€éƒ¨æ‰°åŠ¨å’ŒåŽ»å™ªä»¥ä¿æŒå…¨å±€ä¸€è‡´æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SD2.1ç­‰æ¨¡åž‹ä¸Šï¼ŒLoTTSæå‡å±€éƒ¨è´¨é‡å’Œå…¨å±€ä¿çœŸåº¦ï¼ŒåŒæ—¶GPUæˆæœ¬é™ä½Ž2-4å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have become the dominant paradigm in text-to-image generation, and test-time scaling (TTS) further improves quality by allocating more computation during inference. However, existing TTS methods operate at the full-image level, overlooking the fact that image quality is often spatially heterogeneous. This leads to unnecessary computation on already satisfactory regions and insufficient correction of localized defects. In this paper, we explore a new direction - Localized TTS - that adaptively resamples defective regions while preserving high-quality regions, thereby substantially reducing the search space. This paradigm poses two central challenges: accurately localizing defects and maintaining global consistency. We propose LoTTS, the first fully training-free framework for localized TTS. For defect localization, LoTTS contrasts cross- and self-attention signals under quality-aware prompts (e.g., high-quality vs. low-quality) to identify defective regions, and then refines them into coherent masks. For consistency, LoTTS perturbs only defective regions and denoises them locally, ensuring that corrections remain confined while the rest of the image remains undisturbed. Extensive experiments on SD2.1, SDXL, and FLUX demonstrate that LoTTS achieves state-of-the-art performance: it consistently improves both local quality and global fidelity, while reducing GPU cost by 2-4x compared to Best-of-N sampling. These findings establish localized TTS as a promising new direction for scaling diffusion models at inference time.

