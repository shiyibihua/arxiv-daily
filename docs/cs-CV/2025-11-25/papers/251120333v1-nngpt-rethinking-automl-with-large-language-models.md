---
layout: default
title: NNGPT: Rethinking AutoML with Large Language Models
---

# NNGPT: Rethinking AutoML with Large Language Models

**arXiv**: [2511.20333v1](https://arxiv.org/abs/2511.20333) | [PDF](https://arxiv.org/pdf/2511.20333.pdf)

**ä½œè€…**: Roman Kochnev, Waleed Khalid, Tolgay Atinc Uzun, Xi Zhang, Yashkumar Sanjaybhai Dhameliya, Furui Qin, Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNNGPTæ¡†æž¶ï¼Œå°†å¤§è¯­è¨€æ¨¡åž‹è½¬åŒ–ä¸ºè‡ªæ”¹è¿›AutoMLå¼•æ“Žï¼Œç”¨äºŽè®¡ç®—æœºè§†è§‰ç¥žç»ç½‘ç»œå¼€å‘ã€‚**

**å…³é”®è¯**: `è‡ªæ”¹è¿›AI` `AutoML` `ç¥žç»ç½‘ç»œåˆæˆ` `å¤§è¯­è¨€æ¨¡åž‹åº”ç”¨` `è®¡ç®—æœºè§†è§‰` `è¶…å‚æ•°ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæž„å»ºè‡ªæ”¹è¿›AIç³»ç»Ÿæ˜¯AIé¢†åŸŸçš„æ ¹æœ¬æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆäº”ä¸ªLLMç®¡é“ï¼Œå®žçŽ°ç”Ÿæˆã€è¯„ä¼°å’Œè‡ªæ”¹è¿›çš„é—­çŽ¯ç³»ç»Ÿã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LEMURæ•°æ®é›†ä¸Šï¼ŒHPOçš„RMSEä¸º0.60ï¼Œä¼˜äºŽOptunaã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.

