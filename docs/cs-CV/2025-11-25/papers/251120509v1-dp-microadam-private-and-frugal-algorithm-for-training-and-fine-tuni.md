---
layout: default
title: DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning
---

# DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning

**arXiv**: [2511.20509v1](https://arxiv.org/abs/2511.20509) | [PDF](https://arxiv.org/pdf/2511.20509.pdf)

**ä½œè€…**: Mihaela HudiÅŸteanu, Edwige Cyffers, Nikita P. Kalinin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDP-MicroAdamä»¥æ”¹è¿›å·®åˆ†éšç§è®­ç»ƒçš„æ€§èƒ½ä¸Žæ•ˆçŽ‡**

**å…³é”®è¯**: `å·®åˆ†éšç§ä¼˜åŒ–` `è‡ªé€‚åº”ä¼˜åŒ–å™¨` `å†…å­˜æ•ˆçŽ‡` `ç¨€ç–æ„ŸçŸ¥è®­ç»ƒ` `éžå‡¸ä¼˜åŒ–æ”¶æ•›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å·®åˆ†éšç§è®­ç»ƒä¾èµ–DP-SGDï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”éœ€å¤§é‡è°ƒå‚
2. DP-MicroAdamä¸ºå†…å­˜é«˜æ•ˆã€ç¨€ç–æ„ŸçŸ¥çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨
3. åœ¨CIFAR-10ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ”¶æ•›ç¨³å®š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

