---
layout: default
title: From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations
---

# From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations

**arXiv**: [2511.20359v1](https://arxiv.org/abs/2511.20359) | [PDF](https://arxiv.org/pdf/2511.20359.pdf)

**ä½œè€…**: Zhiqing Guo, Dongdong Xi, Songlin Li, Gaobo Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBoxPromptIMLæ¡†æž¶ä»¥è§£å†³å›¾åƒç¯¡æ”¹å®šä½ä¸­æ ‡æ³¨æˆæœ¬ä¸Žç²¾åº¦å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `å›¾åƒç¯¡æ”¹å®šä½` `å¼±ç›‘ç£å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `ç²—ç²’åº¦æ ‡æ³¨` `ç‰¹å¾èžåˆ` `è½»é‡æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒç¯¡æ”¹å®šä½éœ€å¹³è¡¡åƒç´ çº§æ ‡æ³¨é«˜æˆæœ¬ä¸Žå›¾åƒçº§æ ‡ç­¾å®šä½ç²¾åº¦ä¸è¶³çš„å›°å¢ƒ
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç²—ç²’åº¦åŒºåŸŸæ ‡æ³¨ä¸ŽçŸ¥è¯†è’¸é¦ï¼Œç»“åˆåŒå¼•å¯¼ç‰¹å¾èžåˆæå‡å®šä½å‡†ç¡®æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆ†å¸ƒå†…å¤–æ•°æ®é›†ä¸Šè¡¨çŽ°ä¼˜äºŽæˆ–åª²ç¾Žå…¨ç›‘ç£æ¨¡åž‹ï¼Œæ³›åŒ–æ€§å¼ºä¸”éƒ¨ç½²é«˜æ•ˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.

