---
layout: default
title: 4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models
---

# 4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models

**arXiv**: [2511.19836v1](https://arxiv.org/abs/2511.19836) | [PDF](https://arxiv.org/pdf/2511.19836.pdf)

**ä½œè€…**: Yiting Lu, Wei Luo, Peiyan Tu, Haoran Li, Hanxin Zhu, Zihao Yu, Xingrui Wang, Xinyi Chen, Xinge Peng, Xin Li, Zhibo Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4DWorldBenchä»¥ç»Ÿä¸€è¯„ä¼°3D/4Dä¸–ç•Œç”Ÿæˆæ¨¡åž‹çš„çœŸå®žæ€§ä¸Žä¸€è‡´æ€§**

**å…³é”®è¯**: `ä¸–ç•Œç”Ÿæˆæ¨¡åž‹` `4Dè¯„ä¼°åŸºå‡†` `å¤šæ¨¡æ€å¯¹é½` `ç‰©ç†ä¸€è‡´æ€§` `è‡ªé€‚åº”è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†ç¼ºä¹å¯¹ä¸–ç•Œç”Ÿæˆæ¨¡åž‹çœŸå®žæ€§ä¸Žä¸€è‡´æ€§çš„ç»Ÿä¸€è¯„ä¼°
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šç»´åº¦è¯„ä¼°æ¡†æž¶ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥è´¨é‡ã€æ¡ä»¶å¯¹é½ã€ç‰©ç†çœŸå®žæ€§å’Œ4Dä¸€è‡´æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šè‡ªé€‚åº”å·¥å…·é€‰æ‹©ä¸Žäººç±»ä¸»è§‚åˆ¤æ–­æ›´ä¸€è‡´ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ç»Ÿä¸€è¯„ä¼°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> World Generation Models are emerging as a cornerstone of next-generation multimodal intelligence systems. Unlike traditional 2D visual generation, World Models aim to construct realistic, dynamic, and physically consistent 3D/4D worlds from images, videos, or text. These models not only need to produce high-fidelity visual content but also maintain coherence across space, time, physics, and instruction control, enabling applications in virtual reality, autonomous driving, embodied intelligence, and content creation. However, prior benchmarks emphasize different evaluation dimensions and lack a unified assessment of world-realism capability. To systematically evaluate World Models, we introduce the 4DWorldBench, which measures models across four key dimensions: Perceptual Quality, Condition-4D Alignment, Physical Realism, and 4D Consistency. The benchmark covers tasks such as Image-to-3D/4D, Video-to-4D, Text-to-3D/4D. Beyond these, we innovatively introduce adaptive conditioning across multiple modalities, which not only integrates but also extends traditional evaluation paradigms. To accommodate different modality-conditioned inputs, we map all modality conditions into a unified textual space during evaluation, and further integrate LLM-as-judge, MLLM-as-judge, and traditional network-based methods. This unified and adaptive design enables more comprehensive and consistent evaluation of alignment, physical realism, and cross-modal coherence. Preliminary human studies further demonstrate that our adaptive tool selection achieves closer agreement with subjective human judgments. We hope this benchmark will serve as a foundation for objective comparisons and improvements, accelerating the transition from "visual generation" to "world generation." Our project can be found at https://yeppp27.github.io/4DWorldBench.github.io/.

