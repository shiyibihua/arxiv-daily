---
layout: default
title: Latent Diffusion Inversion Requires Understanding the Latent Space
---

# Latent Diffusion Inversion Requires Understanding the Latent Space

**arXiv**: [2511.20592v1](https://arxiv.org/abs/2511.20592) | [PDF](https://arxiv.org/pdf/2511.20592.pdf)

**‰ΩúËÄÖ**: Mingxing Rao, Bowen Qu, Daniel Moyer

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊΩúÂú®Áª¥Â∫¶ÊéíÂ∫èÁöÑÊñπÊ≥ï‰ª•ÊîπËøõÊΩúÂú®Êâ©Êï£Ê®°ÂûãÁöÑÊàêÂëòÊé®ÁêÜÊîªÂáªÊÄßËÉΩ**

**ÂÖ≥ÈîÆËØç**: `ÊΩúÂú®Êâ©Êï£Ê®°Âûã` `Ê®°ÂûãÂèçËΩ¨` `ÊàêÂëòÊé®ÁêÜÊîªÂáª` `ÊΩúÂú®Á©∫Èó¥ÂàÜÊûê` `Ëß£Á†ÅÂô®ÂõûÊãâÂ∫¶Èáè`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÊΩúÂú®Êâ©Êï£Ê®°ÂûãÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠Â≠òÂú®ÈùûÂùáÂåÄËÆ∞ÂøÜÂåñÔºåÂΩ±ÂìçÊ®°ÂûãÂèçËΩ¨ÊîªÂáªÊïàÊûú
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÈÄöËøáËß£Á†ÅÂô®ÂõûÊãâÂ∫¶ÈáèÊéíÂ∫èÊΩúÂú®Áª¥Â∫¶ÔºåËØÜÂà´È´òËÆ∞ÂøÜÂåñÁª¥Â∫¶
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÔºåÁßªÈô§‰ΩéËÆ∞ÂøÜÂåñÁª¥Â∫¶ÊòæËëóÊèêÂçáAUROCÂíåTPR@1%FPR

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pok√©mon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

