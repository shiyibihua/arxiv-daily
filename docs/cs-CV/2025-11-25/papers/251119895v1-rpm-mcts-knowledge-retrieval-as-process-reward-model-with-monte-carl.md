---
layout: default
title: RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation
---

# RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation

**arXiv**: [2511.19895v1](https://arxiv.org/abs/2511.19895) | [PDF](https://arxiv.org/pdf/2511.19895.pdf)

**ä½œè€…**: Yuanyuan Lin, Xiangyu Ouyang, Teng Zhang, Kaixin Sui

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRPM-MCTSæ–¹æ³•ä»¥è§£å†³ä»£ç ç”Ÿæˆä¸­ä¸­é—´æ­¥éª¤è¯„ä¼°éš¾å’Œé”™è¯¯å®šä½é—®é¢˜**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹` `çŸ¥è¯†æ£€ç´¢` `æ²™ç®±æ‰§è¡Œ` `ä»¤ç‰Œä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ ‘æœç´¢æ–¹æ³•éš¾ä»¥è¯„ä¼°ä¸­é—´ç®—æ³•æ­¥éª¤ï¼Œæ— æ³•åŠæ—¶çº æ­£é”™è¯¯ï¼Œå¯¼è‡´ä»£ç é”™è¯¯å’Œè®¡ç®—æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨çŸ¥è¯†æ£€ç´¢ä½œä¸ºè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼Œç»“åˆMCTSè¯„ä¼°æ­¥éª¤ï¼Œè¿‡æ»¤å†—ä½™èŠ‚ç‚¹å¹¶ä½¿ç”¨æ²™ç®±åé¦ˆå®šä½é”™è¯¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå‡å°‘çº¦15%ä»¤ç‰Œæ¶ˆè€—ï¼Œå¾®è°ƒåŽæå‡æ¨¡åž‹ä»£ç èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

