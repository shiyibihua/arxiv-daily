---
layout: default
title: Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization
---

# Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization

**arXiv**: [2511.19851v1](https://arxiv.org/abs/2511.19851) | [PDF](https://arxiv.org/pdf/2511.19851.pdf)

**ä½œè€…**: Kun Guo, Xuefei Li, Xijun Wang, Howard H. Yang, Wei Feng, Tony Q. S. Quek

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆåˆ†å‰²ä¸Žè”é‚¦å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ä»¥åŠ é€Ÿæ— çº¿åˆ†å¸ƒå¼å­¦ä¹ **

**å…³é”®è¯**: `æ— çº¿åˆ†å¸ƒå¼å­¦ä¹ ` `æ··åˆå­¦ä¹ ä¼˜åŒ–` `å»¶è¿Ÿæœ€å°åŒ–` `å—åæ ‡ä¸‹é™` `è”é‚¦å­¦ä¹ ` `åˆ†å‰²å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå­¦ä¹ æ¨¡å¼é€‰æ‹©å’Œæ‰¹å¤§å°å¦‚ä½•å½±å“æ— çº¿åˆ†å¸ƒå¼å­¦ä¹ æ€§èƒ½ä¸Žå»¶è¿Ÿ
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å—åæ ‡ä¸‹é™å’Œèˆå…¥ç®—æ³•è”åˆä¼˜åŒ–å­¦ä¹ æ¨¡å¼ã€æ‰¹å¤§å°åŠèµ„æºåˆ†é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—åŠ é€Ÿæ”¶æ•›è‡³ç›®æ ‡ç²¾åº¦ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.

