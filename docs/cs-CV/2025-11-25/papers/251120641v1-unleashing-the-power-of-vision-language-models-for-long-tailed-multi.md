---
layout: default
title: Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition
---

# Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition

**arXiv**: [2511.20641v1](https://arxiv.org/abs/2511.20641) | [PDF](https://arxiv.org/pdf/2511.20641.pdf)

**ä½œè€…**: Wei Tang, Zuo-Zheng Wang, Kun Zhang, Tong Wei, Min-Ling Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAPNETä»¥è§£å†³é•¿å°¾å¤šæ ‡ç­¾è§†è§‰è¯†åˆ«ä¸­çš„æ ‡ç­¾ç›¸å…³æ€§ä¸Žä¸å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `é•¿å°¾å¤šæ ‡ç­¾è¯†åˆ«` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ ‡ç­¾ç›¸å…³æ€§å»ºæ¨¡` `å›¾å·ç§¯ç½‘ç»œ` `ä¸å¹³è¡¡å­¦ä¹ ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿å°¾å¤šæ ‡ç­¾è§†è§‰è¯†åˆ«ä¸­ï¼Œç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡å¯¼è‡´æ¨¡åž‹åå‘å¤´éƒ¨ç±»åˆ«ï¼Œå°¾éƒ¨ç±»åˆ«æ€§èƒ½å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å»ºæ¨¡æ ‡ç­¾ç›¸å…³æ€§ï¼Œç»“åˆå›¾å·ç§¯ç½‘ç»œå’Œå¯å­¦ä¹ è½¯æç¤ºä¼˜åŒ–åµŒå…¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VOC-LTç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPNETæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.

