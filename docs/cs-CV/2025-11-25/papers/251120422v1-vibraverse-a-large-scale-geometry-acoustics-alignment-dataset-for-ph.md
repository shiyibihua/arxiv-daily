---
layout: default
title: VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning
---

# VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning

**arXiv**: [2511.20422v1](https://arxiv.org/abs/2511.20422) | [PDF](https://arxiv.org/pdf/2511.20422.pdf)

**ä½œè€…**: Bo Pang, Chenxi Xu, Jierui Ren, Guoping Wang, Sheng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVibraVerseæ•°æ®é›†ä¸ŽCLASPæ¡†æž¶ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­ç‰©ç†ä¸€è‡´æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç‰©ç†ä¸€è‡´æ€§` `å‡ ä½•-å£°å­¦å¯¹é½` `å¯¹æ¯”å­¦ä¹ ` `æ•°æ®é›†æž„å»º` `å› æžœæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰-è¯­è¨€å¤šæ¨¡æ€å­¦ä¹ ç¼ºä¹ç‰©ç†ä¸€è‡´æ€§ï¼Œå¿½ç•¥å‡ ä½•ã€ææ–™ä¸Žå£°éŸ³çš„å› æžœé“¾ã€‚
2. æž„å»ºå¤§è§„æ¨¡å‡ ä½•-å£°å­¦å¯¹é½æ•°æ®é›†ï¼Œé€šè¿‡ç‰©ç†å±žæ€§è®¡ç®—æ¨¡æ€å‚æ•°åˆæˆå£°éŸ³ã€‚
3. å®žéªŒéªŒè¯æ¨¡åž‹åœ¨å‡ ä½•-å£°éŸ³é¢„æµ‹ç­‰ä»»åŠ¡ä¸­å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§æ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

