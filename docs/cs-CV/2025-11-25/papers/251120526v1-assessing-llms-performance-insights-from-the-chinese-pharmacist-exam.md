---
layout: default
title: Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam
---

# Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam

**arXiv**: [2511.20526v1](https://arxiv.org/abs/2511.20526) | [PDF](https://arxiv.org/pdf/2511.20526.pdf)

**ä½œè€…**: Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°ChatGPT-4oä¸ŽDeepSeek-R1åœ¨ä¸­å›½è¯å¸ˆè€ƒè¯•ä¸­çš„è¡¨çŽ°å·®å¼‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°` `è¯å¸ˆæ‰§ç…§è€ƒè¯•` `åŒ»ç–—AI` `æ¨¡åž‹æ€§èƒ½æ¯”è¾ƒ` `ç»Ÿè®¡æ£€éªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨è¯å¸ˆæ‰§ç…§ç­‰é«˜é£Žé™©ä¸“ä¸šè®¤è¯ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨2306é“çº¯æ–‡æœ¬é€‰æ‹©é¢˜ï¼ŒåŸºäºŽå‡†ç¡®çŽ‡è¿›è¡Œç»Ÿè®¡æ¯”è¾ƒ
3. å®žéªŒæ•ˆæžœï¼šDeepSeek-R1æ€»ä½“å‡†ç¡®çŽ‡æ˜¾è‘—é«˜äºŽChatGPT-4oï¼Œè¾¾90.0%å¯¹76.1%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.

