---
layout: default
title: Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding
---

# Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding

**arXiv**: [2511.20073v1](https://arxiv.org/abs/2511.20073) | [PDF](https://arxiv.org/pdf/2511.20073.pdf)

**ä½œè€…**: Jinghan Zhao, Yifei Huang, Feng Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»»åŠ¡-æ­¥éª¤-çŠ¶æ€æ¡†æž¶ï¼Œé€šè¿‡çŠ¶æ€ç›‘ç£æå‡ç¨‹åºæ„ŸçŸ¥è§†é¢‘è¡¨ç¤ºå­¦ä¹ **

**å…³é”®è¯**: `ç¨‹åºæ„ŸçŸ¥è§†é¢‘è¡¨ç¤º` `çŠ¶æ€ç›‘ç£` `å±‚æ¬¡ç»“æž„å±•å¼€` `æ¸è¿›é¢„è®­ç»ƒ` `ä»»åŠ¡è¯†åˆ«` `æ­¥éª¤é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¸­ä»»åŠ¡å’Œæ­¥éª¤æè¿°ä¸Žè§†è§‰ç»†èŠ‚å¯¹é½ä¸ç¨³å¥ï¼Œå½±å“ç¨‹åºè¯­ä¹‰æ³¨å…¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥çŠ¶æ€ä½œä¸ºè§†è§‰åŸºç¡€è¯­ä¹‰å±‚ï¼Œé‡‡ç”¨æ¸è¿›é¢„è®­ç»ƒç­–ç•¥å±•å¼€å±‚æ¬¡ç»“æž„ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨COINå’ŒCrossTaskæ•°æ®é›†ä¸Šï¼Œä»»åŠ¡è¯†åˆ«ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¼˜äºŽåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning procedural-aware video representations is a key step towards building agents that can reason about and execute complex tasks. Existing methods typically address this problem by aligning visual content with textual descriptions at the task and step levels to inject procedural semantics into video representations. However, due to their high level of abstraction, 'task' and 'step' descriptions fail to form a robust alignment with the concrete, observable details in visual data. To address this, we introduce 'states', i.e., textual snapshots of object configurations, as a visually-grounded semantic layer that anchors abstract procedures to what a model can actually see. We formalize this insight in a novel Task-Step-State (TSS) framework, where tasks are achieved via steps that drive transitions between observable states. To enforce this structure, we propose a progressive pre-training strategy that unfolds the TSS hierarchy, forcing the model to ground representations in states while associating them with steps and high-level tasks. Extensive experiments on the COIN and CrossTask datasets show that our method outperforms baseline models on multiple downstream tasks, including task recognition, step recognition, and next step prediction. Ablation studies show that introducing state supervision is a key driver of performance gains across all tasks. Additionally, our progressive pretraining strategy proves more effective than standard joint training, as it better enforces the intended hierarchical structure.

