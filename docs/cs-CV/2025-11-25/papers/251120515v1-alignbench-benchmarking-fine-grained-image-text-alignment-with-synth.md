---
layout: default
title: AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs
---

# AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs

**arXiv**: [2511.20515v1](https://arxiv.org/abs/2511.20515) | [PDF](https://arxiv.org/pdf/2511.20515.pdf)

**ä½œè€…**: Kuniaki Saito, Risa Shinoda, Shohei Tanaka, Tosho Hirasawa, Fumio Okura, Yoshitaka Ushiku

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAlignBenchåŸºå‡†ï¼Œé€šè¿‡åˆæˆå›¾åƒ-æ ‡é¢˜å¯¹è¯„ä¼°ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚**

**å…³é”®è¯**: `å›¾åƒ-æ–‡æœ¬å¯¹é½` `åŸºå‡†æµ‹è¯•` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åˆæˆæ•°æ®` `ç»†ç²’åº¦è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†ä¾èµ–è§„åˆ™æ‰°åŠ¨æˆ–çŸ­æ ‡é¢˜ï¼Œéš¾ä»¥è¡¡é‡ç»†ç²’åº¦å¯¹é½ã€‚
2. ä½¿ç”¨å¤šæ ·å›¾åƒ-æ–‡æœ¬æ¨¡åž‹ç”Ÿæˆè¯¦ç»†å›¾åƒ-æ ‡é¢˜å¯¹ï¼Œå¹¶æ ‡æ³¨å¥å­æ­£ç¡®æ€§ã€‚
3. è¯„ä¼°æ˜¾ç¤ºCLIPæ¨¡åž‹å‡ ä¹Žç›²è§†ï¼Œæ£€æµ‹å™¨æœ‰è‡ªåå¥½å’Œè¿‡è¯„åˆ†é—®é¢˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.

