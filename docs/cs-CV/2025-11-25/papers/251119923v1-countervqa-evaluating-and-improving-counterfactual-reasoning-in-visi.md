---
layout: default
title: CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding
---

# CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding

**arXiv**: [2511.19923v1](https://arxiv.org/abs/2511.19923) | [PDF](https://arxiv.org/pdf/2511.19923.pdf)

**ä½œè€…**: Yuefei Chen, Jiang Liu, Xiaodong Lin, Ruixiang Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCounterVQAåŸºå‡†å’ŒCFGPTæ–¹æ³•ä»¥å¢žå¼ºè§†é¢‘è¯­è¨€æ¨¡åž‹çš„åäº‹å®žæŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘è¯­è¨€æ¨¡åž‹` `åäº‹å®žæŽ¨ç†` `åŸºå‡†è¯„ä¼°` `è’¸é¦è®­ç»ƒ` `è§†é¢‘ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨åäº‹å®žæŽ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤æ‚å› æžœé“¾ä¸Šè¡¨çŽ°ä¸ä½³
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘CFGPTåŽè®­ç»ƒæ–¹æ³•ï¼Œä»Žè¯­è¨€æ¨¡æ€è’¸é¦åäº‹å®žæŽ¨ç†èƒ½åŠ›åˆ°è§†è§‰æ¨¡æ€
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CounterVQAåŸºå‡†ä¸Šè¯„ä¼°ï¼ŒCFGPTåœ¨æ‰€æœ‰éš¾åº¦çº§åˆ«å‡å¸¦æ¥ä¸€è‡´æ”¹è¿›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.

