---
layout: default
title: In-Context Compositional Learning via Sparse Coding Transformer
---

# In-Context Compositional Learning via Sparse Coding Transformer

**arXiv**: [2511.20194v1](https://arxiv.org/abs/2511.20194) | [PDF](https://arxiv.org/pdf/2511.20194.pdf)

**ä½œè€…**: Wei Chen, Jingxi Yu, Zichen Miao, Qiang Qiu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–ç¼–ç Transformerä»¥å¢žå¼ºTransformeråœ¨ä¸Šä¸‹æ–‡ç»„åˆå­¦ä¹ ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡ç»„åˆå­¦ä¹ ` `ç¨€ç–ç¼–ç ` `Transformeræž¶æž„` `æ³¨æ„åŠ›æœºåˆ¶` `ç»„åˆè§„åˆ™æŽ¨æ–­` `RAVENæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeråœ¨ç»„åˆå­¦ä¹ ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œç¼ºä¹ç»“æž„å½’çº³åç½®
2. å°†æ³¨æ„åŠ›å—é‡æ–°è§£é‡Šä¸ºç¼–ç å’Œè§£ç å­—å…¸ï¼Œå¯¹ç³»æ•°æ–½åŠ ç¨€ç–æ€§ä»¥æ•æ‰ç»„åˆç»“æž„
3. åœ¨S-RAVENå’ŒRAVENæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œåœ¨æ ‡å‡†Transformerå¤±è´¥æ—¶ä¿æŒæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

