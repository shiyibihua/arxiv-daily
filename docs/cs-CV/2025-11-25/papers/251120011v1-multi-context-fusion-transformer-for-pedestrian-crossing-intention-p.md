---
layout: default
title: Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments
---

# Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments

**arXiv**: [2511.20011v1](https://arxiv.org/abs/2511.20011) | [PDF](https://arxiv.org/pdf/2511.20011.pdf)

**ä½œè€…**: Yuanzhe Li, Hang Zhong, Steffen MÃ¼ller

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä¸Šä¸‹æ–‡èžåˆTransformerï¼ˆMFTï¼‰ç”¨äºŽåŸŽå¸‚çŽ¯å¢ƒä¸­è¡Œäººæ„å›¾é¢„æµ‹ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è¡Œäººæ„å›¾é¢„æµ‹` `å¤šä¸Šä¸‹æ–‡èžåˆ` `Transformer` `æ³¨æ„åŠ›æœºåˆ¶` `è‡ªåŠ¨é©¾é©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸŽå¸‚çŽ¯å¢ƒä¸­è¡Œäººæ„å›¾é¢„æµ‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèžåˆå¤šç»´åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
2. MFTé€šè¿‡æ¸è¿›å¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¸Šä¸‹æ–‡å†…å’Œä¸Šä¸‹æ–‡é—´è¿›è¡Œç‰¹å¾èžåˆï¼Œæå–æ›´é²æ£’çš„è¡Œäººæ„å›¾è¡¨ç¤ºã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMFTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡Œäººæ„å›¾é¢„æµ‹å¯¹äºŽè‡ªåŠ¨é©¾é©¶è½¦è¾†è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæé«˜è¡Œäººå®‰å…¨å¹¶å‡å°‘äº¤é€šäº‹æ•…ã€‚ç„¶è€Œï¼Œç”±äºŽå½±å“è¡Œäººè¡Œä¸ºçš„å› ç´ ä¼—å¤šï¼Œåœ¨åŸŽå¸‚çŽ¯å¢ƒä¸­å‡†ç¡®é¢„æµ‹è¡Œäººæ„å›¾ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä¸Šä¸‹æ–‡èžåˆTransformerï¼ˆMFTï¼‰ï¼Œå®ƒåˆ©ç”¨å››ä¸ªå…³é”®ç»´åº¦ä¸Šçš„å„ç§æ•°å€¼ä¸Šä¸‹æ–‡å±žæ€§ï¼ŒåŒ…æ‹¬è¡Œäººè¡Œä¸ºä¸Šä¸‹æ–‡ã€çŽ¯å¢ƒä¸Šä¸‹æ–‡ã€è¡Œäººå®šä½ä¸Šä¸‹æ–‡å’Œè½¦è¾†è¿åŠ¨ä¸Šä¸‹æ–‡ï¼Œä»¥å®žçŽ°å‡†ç¡®çš„è¡Œäººæ„å›¾é¢„æµ‹ã€‚MFTé‡‡ç”¨æ¸è¿›å¼èžåˆç­–ç•¥ï¼Œå…¶ä¸­äº’ä¸Šä¸‹æ–‡å†…æ³¨æ„åŠ›èƒ½å¤Ÿå®žçŽ°æ¯ä¸ªä¸Šä¸‹æ–‡å†…çš„ç›¸äº’äº¤äº’ï¼Œä»Žè€Œä¿ƒè¿›ç‰¹å¾åºåˆ—èžåˆå¹¶äº§ç”Ÿä¸Šä¸‹æ–‡tokenä½œä¸ºä¸Šä¸‹æ–‡ç‰¹å®šçš„è¡¨ç¤ºã€‚éšåŽæ˜¯äº’ä¸Šä¸‹æ–‡é—´æ³¨æ„åŠ›ï¼Œå®ƒé€šè¿‡å…¨å±€CLS tokené›†æˆè·¨ä¸Šä¸‹æ–‡çš„ç‰¹å¾ï¼Œè¯¥tokenå……å½“ç´§å‡‘çš„å¤šä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æœ€åŽï¼Œå¼•å¯¼çš„ä¸Šä¸‹æ–‡å†…æ³¨æ„åŠ›é€šè¿‡å®šå‘äº¤äº’æ¥ç»†åŒ–æ¯ä¸ªä¸Šä¸‹æ–‡å†…çš„ä¸Šä¸‹æ–‡tokenï¼Œè€Œå¼•å¯¼çš„è·¨ä¸Šä¸‹æ–‡æ³¨æ„åŠ›åˆ™åŠ å¼ºå…¨å±€CLS tokenï¼Œä»¥é€šè¿‡å¼•å¯¼çš„ä¿¡æ¯ä¼ æ’­æ¥ä¿ƒè¿›å¤šä¸Šä¸‹æ–‡èžåˆï¼Œä»Žè€Œäº§ç”Ÿæ›´æ·±å…¥ã€æ›´æœ‰æ•ˆçš„é›†æˆã€‚å®žéªŒç»“æžœéªŒè¯äº†MFTä¼˜äºŽæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨JAADbehã€JAADallå’ŒPIEæ•°æ®é›†ä¸Šåˆ†åˆ«å®žçŽ°äº†73%ã€93%å’Œ90%çš„å‡†ç¡®çŽ‡ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèžç ”ç©¶ï¼Œä»¥ç ”ç©¶ç½‘ç»œæž¶æž„çš„æœ‰æ•ˆæ€§å’Œä¸åŒè¾“å…¥ä¸Šä¸‹æ–‡çš„è´¡çŒ®ã€‚ä»£ç å·²å¼€æºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸŽå¸‚çŽ¯å¢ƒä¸­å‡†ç¡®é¢„æµ‹è¡Œäººæ„å›¾çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèžåˆè¡Œäººè¡Œä¸ºã€çŽ¯å¢ƒã€å®šä½å’Œè½¦è¾†è¿åŠ¨ç­‰å¤šæ–¹é¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸é«˜ã€‚è¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•å……åˆ†æ•æ‰ä¸åŒä¸Šä¸‹æ–‡ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œé™åˆ¶äº†æ¨¡åž‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Transformeræž¶æž„ï¼Œé€šè¿‡å¤šå±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€æ­¥èžåˆä¸åŒä¸Šä¸‹æ–‡çš„ä¿¡æ¯ã€‚é¦–å…ˆåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡å†…éƒ¨è¿›è¡Œç‰¹å¾æå–å’Œèžåˆï¼Œç„¶åŽè·¨ä¸Šä¸‹æ–‡è¿›è¡Œä¿¡æ¯äº¤äº’ï¼Œæœ€åŽé€šè¿‡å¼•å¯¼æœºåˆ¶è¿›ä¸€æ­¥ä¼˜åŒ–èžåˆç»“æžœã€‚è¿™ç§æ¸è¿›å¼çš„èžåˆç­–ç•¥èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰ä¸åŒä¸Šä¸‹æ–‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæé«˜é¢„æµ‹ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMFTçš„æ•´ä½“æž¶æž„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä¸Šä¸‹æ–‡ç‰¹å¾æå–æ¨¡å—ï¼šæå–è¡Œäººè¡Œä¸ºã€çŽ¯å¢ƒã€å®šä½å’Œè½¦è¾†è¿åŠ¨ç­‰ä¸Šä¸‹æ–‡çš„ç‰¹å¾åºåˆ—ã€‚2) äº’ä¸Šä¸‹æ–‡å†…æ³¨æ„åŠ›æ¨¡å—ï¼šåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡å†…éƒ¨è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œå¢žå¼ºä¸Šä¸‹æ–‡å†…éƒ¨çš„ç‰¹å¾è¡¨ç¤ºã€‚3) äº’ä¸Šä¸‹æ–‡é—´æ³¨æ„åŠ›æ¨¡å—ï¼šåˆ©ç”¨å…¨å±€CLS tokenï¼Œå°†ä¸åŒä¸Šä¸‹æ–‡çš„ç‰¹å¾è¿›è¡Œèžåˆï¼Œæ•æ‰ä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³ç³»ã€‚4) å¼•å¯¼çš„ä¸Šä¸‹æ–‡å†…æ³¨æ„åŠ›æ¨¡å—ï¼šåˆ©ç”¨å…¨å±€CLS tokenå¼•å¯¼ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¯ä¸ªä¸Šä¸‹æ–‡å†…éƒ¨çš„ç‰¹å¾è¡¨ç¤ºã€‚5) å¼•å¯¼çš„ä¸Šä¸‹æ–‡é—´æ³¨æ„åŠ›æ¨¡å—ï¼šåˆ©ç”¨å…¨å±€CLS tokenå¼•å¯¼ï¼Œè¿›ä¸€æ­¥åŠ å¼ºä¸åŒä¸Šä¸‹æ–‡ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šMFTçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶æ¸è¿›å¼çš„å¤šä¸Šä¸‹æ–‡èžåˆç­–ç•¥ã€‚é€šè¿‡äº’ä¸Šä¸‹æ–‡å†…æ³¨æ„åŠ›ã€äº’ä¸Šä¸‹æ–‡é—´æ³¨æ„åŠ›å’Œå¼•å¯¼çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒMFTèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èžåˆä¸åŒä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œæ•æ‰ä¸Šä¸‹æ–‡ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMFTèƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨å¤šç»´åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜è¡Œäººæ„å›¾é¢„æµ‹çš„ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šMFTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræž¶æž„ä½œä¸ºåŸºç¡€æ¨¡åž‹ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ç‰¹å¾æå–å’Œèžåˆèƒ½åŠ›ã€‚2) é‡‡ç”¨æ¸è¿›å¼çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€æ­¥èžåˆä¸åŒä¸Šä¸‹æ–‡çš„ä¿¡æ¯ã€‚3) å¼•å…¥å…¨å±€CLS tokenï¼Œä½œä¸ºå¤šä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç´§å‡‘è¡¨ç¤ºï¼Œå¹¶ç”¨äºŽå¼•å¯¼æ³¨æ„åŠ›è®¡ç®—ã€‚4) è®¾è®¡äº†äº’ä¸Šä¸‹æ–‡å†…æ³¨æ„åŠ›ã€äº’ä¸Šä¸‹æ–‡é—´æ³¨æ„åŠ›å’Œå¼•å¯¼çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«ç”¨äºŽå¢žå¼ºä¸Šä¸‹æ–‡å†…éƒ¨çš„ç‰¹å¾è¡¨ç¤ºã€æ•æ‰ä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³ç³»å’Œä¼˜åŒ–èžåˆç»“æžœã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMFTåœ¨JAADbehã€JAADallå’ŒPIEæ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº†73%ã€93%å’Œ90%çš„å‡†ç¡®çŽ‡ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ¶ˆèžå®žéªŒè¿›ä¸€æ­¥éªŒè¯äº†MFTå„ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠä¸åŒä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹é¢„æµ‹ç»“æžœçš„è´¡çŒ®ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€é«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿï¼ˆADASï¼‰ç­‰é¢†åŸŸï¼Œæé«˜è½¦è¾†å¯¹è¡Œäººæ„å›¾çš„ç†è§£èƒ½åŠ›ï¼Œä»Žè€Œå‡å°‘äº¤é€šäº‹æ•…ï¼Œæå‡è¡Œäººå®‰å…¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯åº”ç”¨äºŽæ™ºèƒ½ç›‘æŽ§ã€æœºå™¨äººå¯¼èˆªç­‰åœºæ™¯ï¼Œå®žçŽ°æ›´æ™ºèƒ½ã€æ›´å®‰å…¨çš„äº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.

