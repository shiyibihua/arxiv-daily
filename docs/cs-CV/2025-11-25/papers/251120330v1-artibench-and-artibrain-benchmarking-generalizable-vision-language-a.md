---
layout: default
title: ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation
---

# ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation

**arXiv**: [2511.20330v1](https://arxiv.org/abs/2511.20330) | [PDF](https://arxiv.org/pdf/2511.20330.pdf)

**ä½œè€…**: Yuhan Wu, Tiantian Wei, Shuo Wang, ZhiChao Wang, Yanyong Zhang, Daniel Cremers, Yan Xia

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºArtiBrainæ¡†æž¶ä»¥è§£å†³é“°æŽ¥ç‰©ä½“æ“ä½œä¸­çš„æ³›åŒ–æŒ‘æˆ˜**

**å…³é”®è¯**: `é“°æŽ¥ç‰©ä½“æ“ä½œ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŸºå‡†æµ‹è¯•` `æ‰©æ•£ç­–ç•¥` `æ³›åŒ–èƒ½åŠ›` `æ¨¡å—åŒ–æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€å’Œæ‰©æ•£ç­–ç•¥åœ¨é“°æŽ¥ç‰©ä½“æ“ä½œä¸­éš¾ä»¥è·¨éƒ¨ä»¶ã€å®žä¾‹å’Œç±»åˆ«æ³›åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šArtiBrainç»“åˆé«˜å±‚æŽ¨ç†ä¸Žè‡ªé€‚åº”ä½Žå±‚æŽ§åˆ¶ï¼Œä½¿ç”¨VLMåˆ†è§£ä»»åŠ¡å’Œæ··åˆæŽ§åˆ¶å™¨æ‰§è¡Œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ArtiBenchåŸºå‡†ä¸Šï¼ŒArtiBrainåœ¨é²æ£’æ€§å’Œæ³›åŒ–æ€§ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.

