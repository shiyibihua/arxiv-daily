---
layout: default
title: VQ-VA World: Towards High-Quality Visual Question-Visual Answering
---

# VQ-VA World: Towards High-Quality Visual Question-Visual Answering

**arXiv**: [2511.20573v1](https://arxiv.org/abs/2511.20573) | [PDF](https://arxiv.org/pdf/2511.20573.pdf)

**ä½œè€…**: Chenhui Gou, Zilong Chen, Zeyu Wang, Feng Li, Deyao Zhu, Zicheng Duan, Kunchang Li, Chaorui Deng, Hongyi Yuan, Haoqi Fan, Cihang Xie, Jianfei Cai, Hamid Rezatofighi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVQ-VA Worldæ¡†æž¶ä»¥æž„å»ºé«˜è´¨é‡è§†è§‰é—®ç­”-è§†è§‰å›žç­”å¼€æºæ¨¡åž‹**

**å…³é”®è¯**: `è§†è§‰é—®ç­”-è§†è§‰å›žç­”` `æ•°æ®æž„å»ºæ¡†æž¶` `å¼€æºæ¨¡åž‹è®­ç»ƒ` `å¤§è§„æ¨¡æ•°æ®é›†` `æ™ºèƒ½åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰é—®ç­”-è§†è§‰å›žç­”ä»»åŠ¡ï¼Œå³æ ¹æ®è§†è§‰é—®é¢˜ç”Ÿæˆå›¾åƒè€Œéžæ–‡æœ¬ï¼Œå¼€æºæ¨¡åž‹èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ•°æ®é©±åŠ¨æ¡†æž¶ï¼Œé€šè¿‡ä»£ç†ç®¡é“å¤§è§„æ¨¡çˆ¬å–çº¦180ä¸‡é«˜è´¨é‡å›¾æ–‡æ ·æœ¬ç”¨äºŽè®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè®­ç»ƒåŽæ¨¡åž‹åœ¨IntelligentBenchåŸºå‡†ä¸Šå¾—åˆ†53.06ï¼Œæ˜¾è‘—è¶…è¶Šå¼€æºåŸºçº¿å¹¶æŽ¥è¿‘ä¸“æœ‰ç³»ç»Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.

