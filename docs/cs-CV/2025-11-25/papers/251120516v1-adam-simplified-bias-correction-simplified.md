---
layout: default
title: Adam Simplified: Bias Correction Simplified
---

# Adam Simplified: Bias Correction Simplified

**arXiv**: [2511.20516v1](https://arxiv.org/abs/2511.20516) | [PDF](https://arxiv.org/pdf/2511.20516.pdf)

**ä½œè€…**: Sam Laing, Antonio Orvieto

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è´¨ç–‘Adamä¼˜åŒ–å™¨ä¸­åç½®æ ¡æ­£çš„å¿…è¦æ€§ï¼Œé€šè¿‡å®žéªŒæ­ç¤ºå…¶æ— ç›Šæˆ–æœ‰å®³**

**å…³é”®è¯**: `Adamä¼˜åŒ–å™¨` `åç½®æ ¡æ­£` `æ·±åº¦å­¦ä¹ ä¼˜åŒ–` `è¶…å‚æ•°è°ƒä¼˜` `å­¦ä¹ çŽ‡è°ƒåº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAdamä¼˜åŒ–å™¨ä¸­åç½®æ ¡æ­£ç»„ä»¶çš„å®žé™…è´¡çŒ®æœªè¢«å……åˆ†ç†è§£ï¼Œå¸¸è¢«ç›²ç›®é‡‡ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç³»ç»Ÿæ¶ˆèžå®žéªŒï¼Œåˆ†æžåç½®æ ¡æ­£åœ¨è§†è§‰å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­çš„å½±å“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æœ€ä¼˜è¶…å‚æ•°ä¸‹ï¼Œåç½®æ ¡æ­£æ— æ€§èƒ½æå‡ï¼›æ— é€‚å½“å­¦ä¹ çŽ‡è°ƒåº¦æ—¶å¯èƒ½æœ‰å®³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $Î²_1, Î²_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

