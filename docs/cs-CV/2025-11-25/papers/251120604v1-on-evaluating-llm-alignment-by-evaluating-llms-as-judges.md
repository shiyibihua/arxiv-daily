---
layout: default
title: On Evaluating LLM Alignment by Evaluating LLMs as Judges
---

# On Evaluating LLM Alignment by Evaluating LLMs as Judges

**arXiv**: [2511.20604v1](https://arxiv.org/abs/2511.20604) | [PDF](https://arxiv.org/pdf/2511.20604.pdf)

**ä½œè€…**: Yixin Liu, Pengfei Liu, Arman Cohan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAlignEvalåŸºå‡†ï¼Œé€šè¿‡è¯„ä¼°LLMä½œä¸ºè¯„åˆ¤è€…æ¥è¡¡é‡å…¶ä¸Žäººç±»åå¥½å¯¹é½**

**å…³é”®è¯**: `LLMå¯¹é½è¯„ä¼°` `ç”Ÿæˆ-è¯„ä¼°ä¸€è‡´æ€§` `è‡ªåŠ¨åŸºå‡†æž„å»º` `äººç±»åå¥½å¯¹é½` `LLMè¯„åˆ¤èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMå¯¹é½è¯„ä¼°ä¾èµ–äººç±»æˆ–å¼ºLLMè¯„åˆ¤ï¼Œæˆæœ¬é«˜ä¸”å¤æ‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†æžç”Ÿæˆ-è¯„ä¼°ä¸€è‡´æ€§ï¼Œæž„å»ºåŸºäºŽLLMè¯„åˆ¤çš„åŸºå‡†AlignEval
3. å®žéªŒæˆ–æ•ˆæžœï¼šAlignEvalåœ¨æŽ’åLLMæ—¶ä¼˜äºŽæˆ–åŒ¹é…çŽ°æœ‰è‡ªåŠ¨è¯„ä¼°åŸºå‡†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.

