---
layout: default
title: 3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding
---

# 3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.20646" target="_blank" class="toolbar-btn">arXiv: 2511.20646v1</a>
    <a href="https://arxiv.org/pdf/2511.20646.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20646v1" 
            onclick="toggleFavorite(this, '2511.20646v1', '3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoye Wang, Chen Tang, Xiangyu Yue, Wei-Hong Li

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-25

**Â§áÊ≥®**: 3D-aware Multi-task Learning, Cross-view Correlations, Code will be available at https://github.com/WeiHongLee/CrossView3DMTL

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éË∑®ËßÜËßíÁõ∏ÂÖ≥ÊÄßÁöÑ3DÊÑüÁü•Â§ö‰ªªÂä°Â≠¶‰π†ÔºåÁî®‰∫éÂØÜÈõÜÂú∫ÊôØÁêÜËß£**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `Â§ö‰ªªÂä°Â≠¶‰π†` `3DÊÑüÁü•` `Ë∑®ËßÜËßíÁõ∏ÂÖ≥ÊÄß` `‰ª£‰ª∑‰Ωì` `Âú∫ÊôØÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMTLÊñπÊ≥ïÂú®2DÂõæÂÉèÁ©∫Èó¥ÊçïËé∑Ë∑®‰ªªÂä°ÂÖ≥Á≥ªÔºåÁº∫‰πè3DÊÑüÁü•ÔºåÈôêÂà∂‰∫ÜÂú∫ÊôØÁêÜËß£ËÉΩÂäõ„ÄÇ
2. ÊèêÂá∫Ë∑®ËßÜËßíÊ®°ÂùóÔºàCvMÔºâÔºåÈÄöËøá‰ª£‰ª∑‰ΩìÊï¥ÂêàË∑®ËßÜËßí‰ø°ÊÅØÔºåÊ≥®ÂÖ•Âá†‰Ωï‰∏ÄËá¥ÊÄßÔºåÂ¢ûÂº∫3DÊÑüÁü•„ÄÇ
3. CvMÊ®°ÂùóÊòì‰∫éÈõÜÊàêÔºåÂú®NYUv2ÂíåPASCAL-ContextÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÔºåÊèêÂçá‰∫ÜMTLÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËÆ≠ÁªÉÂçï‰∏™ÁΩëÁªú‰ª•ËÅîÂêàÊâßË°åÂ§ö‰∏™ÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°ÔºàÂ¶ÇÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°ÔºâÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÔºàMTLÔºâÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂú®2DÂõæÂÉèÁ©∫Èó¥‰∏≠ÊçïËé∑Ë∑®‰ªªÂä°ÂÖ≥Á≥ªÔºåÈÄöÂ∏∏ÂØºËá¥Áº∫‰πè3DÊÑüÁü•ÁöÑÈùûÁªìÊûÑÂåñÁâπÂæÅ„ÄÇÊàë‰ª¨ËÆ§‰∏∫Ôºå3DÊÑüÁü•ÂØπ‰∫éÂª∫Ê®°ÂØπÂÖ®Èù¢Âú∫ÊôØÁêÜËß£Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑË∑®‰ªªÂä°Áõ∏ÂÖ≥ÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫ÈÄöËøáÊï¥ÂêàË∑®ËßÜËßíÁöÑÂÖ≥ËÅîÔºàÂç≥‰ª£‰ª∑‰ΩìÔºâ‰Ωú‰∏∫MTLÁΩëÁªú‰∏≠ÁöÑÂá†‰Ωï‰∏ÄËá¥ÊÄßÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑË∑®ËßÜËßíÊ®°ÂùóÔºàCvMÔºâÔºåËØ•Ê®°ÂùóÂú®‰ªªÂä°‰πãÈó¥ÂÖ±‰∫´Ôºå‰ª•‰∫§Êç¢Ë∑®ËßÜËßíÁöÑ‰ø°ÊÅØÂπ∂ÊçïËé∑Ë∑®ËßÜËßíÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÂπ∂‰∏éÊù•Ëá™MTLÁºñÁ†ÅÂô®ÁöÑÁâπÂæÅÈõÜÊàêÔºåÁî®‰∫éÂ§ö‰ªªÂä°È¢ÑÊµã„ÄÇËØ•Ê®°Âùó‰∏éÊû∂ÊûÑÊó†ÂÖ≥ÔºåÂèØ‰ª•Â∫îÁî®‰∫éÂçïËßÜÂõæÂíåÂ§öËßÜÂõæÊï∞ÊçÆ„ÄÇÂú®NYUv2ÂíåPASCAL-Context‰∏äÁöÑÂ§ßÈáèÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊúâÊïàÂú∞Â∞ÜÂá†‰Ωï‰∏ÄËá¥ÊÄßÊ≥®ÂÖ•Âà∞Áé∞ÊúâÁöÑMTLÊñπÊ≥ï‰∏≠Ôºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÊñπÊ≥ïÂú®ÂØÜÈõÜÂú∫ÊôØÁêÜËß£‰ªªÂä°‰∏≠Ôºå‰∏ªË¶Å‰æùËµñ‰∫é2DÂõæÂÉèÁ©∫Èó¥‰∏≠ÁöÑÁâπÂæÅÂÖ≥ËÅîÔºåÂøΩÁï•‰∫ÜÂú∫ÊôØÁöÑ3DÂá†‰Ωï‰ø°ÊÅØ„ÄÇËøôÂØºËá¥ÁΩëÁªúÂ≠¶‰π†Âà∞ÁöÑÁâπÂæÅÁº∫‰πè3DÊÑüÁü•ËÉΩÂäõÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂØπÂú∫ÊôØÁöÑÂÖ®Èù¢ÁêÜËß£ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°Á≠â‰ªªÂä°‰∏≠Ôºå3D‰ø°ÊÅØËá≥ÂÖ≥ÈáçË¶Å„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü3DÂá†‰Ωï‰ø°ÊÅØÊòæÂºèÂú∞ÂºïÂÖ•Âà∞Â§ö‰ªªÂä°Â≠¶‰π†Ê°ÜÊû∂‰∏≠„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈÄöËøáÊûÑÂª∫Ë∑®ËßÜËßíÁöÑ‰ª£‰ª∑‰ΩìÔºàcost volumeÔºâÊù•ÊçïÊçâ‰∏çÂêåËßÜËßí‰πãÈó¥ÁöÑÂá†‰Ωï‰∏ÄËá¥ÊÄßÔºåÂπ∂Â∞ÜËøôÁßçÂá†‰Ωï‰∏ÄËá¥ÊÄß‰Ωú‰∏∫‰∏ÄÁßçÂÖàÈ™åÁü•ËØÜÊ≥®ÂÖ•Âà∞ÁΩëÁªú‰∏≠Ôºå‰ªéËÄåÂ¢ûÂº∫ÁΩëÁªúÂØπ3DÂú∫ÊôØÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏Ä‰∏™Â§ö‰ªªÂä°Â≠¶‰π†ÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™Ë∑®ËßÜËßíÊ®°ÂùóÔºàCvMÔºâ„ÄÇÁºñÁ†ÅÂô®Ë¥üË¥£ÊèêÂèñÂõæÂÉèÁâπÂæÅÔºåCvMÊ®°ÂùóÂàôË¥üË¥£Âú®‰∏çÂêåËßÜËßí‰πãÈó¥‰∫§Êç¢‰ø°ÊÅØÔºåÊûÑÂª∫‰ª£‰ª∑‰ΩìÔºåÂπ∂ÊèêÂèñË∑®ËßÜËßíÁöÑÁõ∏ÂÖ≥ÊÄßÁâπÂæÅ„ÄÇËøô‰∫õÁâπÂæÅÈöèÂêé‰∏éÁºñÁ†ÅÂô®ÁöÑÁâπÂæÅËûçÂêàÔºåÁî®‰∫éÂ§ö‰ªªÂä°È¢ÑÊµã„ÄÇËØ•Ê°ÜÊû∂ÊòØÊû∂ÊûÑÊó†ÂÖ≥ÁöÑÔºåÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÁΩëÁªúÁªìÂêà‰ΩøÁî®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜË∑®ËßÜËßíÊ®°ÂùóÔºàCvMÔºâÔºåÈÄöËøá‰ª£‰ª∑‰ΩìÊòæÂºèÂú∞Âª∫Ê®°‰∫ÜË∑®ËßÜËßíÁöÑÂá†‰Ωï‰∏ÄËá¥ÊÄß„ÄÇËøô‰∏é‰ª•ÂæÄ‰∏ªË¶ÅÂÖ≥Ê≥®2DÂõæÂÉèÁ©∫Èó¥ÁâπÂæÅÂÖ≥ËÅîÁöÑÊñπÊ≥ï‰∏çÂêåÔºåCvMÊ®°ÂùóËÉΩÂ§üÊúâÊïàÂú∞Â∞Ü3DÂá†‰Ωï‰ø°ÊÅØËûçÂÖ•Âà∞Â§ö‰ªªÂä°Â≠¶‰π†ËøáÁ®ã‰∏≠Ôºå‰ªéËÄåÊèêÂçá‰∫ÜÁΩëÁªúÁöÑ3DÊÑüÁü•ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCvMÊ®°ÂùóÁöÑËÆæËÆ°ÊòØËΩªÈáèÁ∫ßÁöÑÔºåÊòì‰∫éÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÁΩëÁªú‰∏≠„ÄÇÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÂåÖÊã¨ÔºöÂ¶Ç‰ΩïÊûÑÂª∫‰ª£‰ª∑‰ΩìÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®Âì™‰∫õÁâπÂæÅËøõË°åÂåπÈÖçÔºâÔºåÂ¶Ç‰ΩïÊèêÂèñË∑®ËßÜËßíÁõ∏ÂÖ≥ÊÄßÁâπÂæÅÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºâÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∞ÜËøô‰∫õÁâπÂæÅ‰∏éÁºñÁ†ÅÂô®ÁöÑÁâπÂæÅËøõË°åËûçÂêàÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºâ„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüÈúÄË¶ÅËÄÉËôëÂ¶Ç‰ΩïÂπ≥Ë°°‰∏çÂêå‰ªªÂä°‰πãÈó¥ÁöÑÂ≠¶‰π†Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂà©Áî®Âá†‰Ωï‰∏ÄËá¥ÊÄß‰ø°ÊÅØÊù•Á∫¶ÊùüÁΩëÁªúÁöÑÂ≠¶‰π†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®NYUv2ÂíåPASCAL-ContextÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáÂ§ö‰ªªÂä°Â≠¶‰π†ÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®NYUv2Êï∞ÊçÆÈõÜ‰∏äÔºåÂàÜÂâ≤‰ªªÂä°ÁöÑÊÄßËÉΩÊèêÂçá‰∫ÜX%ÔºåÊ∑±Â∫¶‰º∞ËÆ°‰ªªÂä°ÁöÑÊÄßËÉΩÊèêÂçá‰∫ÜY%„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÂú∫ÊôØÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•ÊèêÈ´òËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÁéØÂ¢ÉÊÑüÁü•Á≤æÂ∫¶ÔºåÂ¢ûÂº∫Êú∫Âô®‰∫∫ÂØπÂ§çÊùÇÁéØÂ¢ÉÁöÑÈÄÇÂ∫îÊÄßÔºåÂπ∂‰∏∫ARÂ∫îÁî®Êèê‰æõÊõ¥ÈÄºÁúüÁöÑ3DÂú∫ÊôØÈáçÂª∫„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.

