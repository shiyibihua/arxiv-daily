---
layout: default
title: Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos
---

# Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos

**arXiv**: [2511.20525v1](https://arxiv.org/abs/2511.20525) | [PDF](https://arxiv.org/pdf/2511.20525.pdf)

**ä½œè€…**: Yayuan Li, Aadit Jain, Filippos Bellos, Jason J. Corso

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMistake Attributionä»»åŠ¡å’ŒMisFormeræ¨¡åž‹ï¼Œç”¨äºŽç»†ç²’åº¦ç†è§£ç¬¬ä¸€äººç§°è§†é¢‘ä¸­çš„äººç±»é”™è¯¯ã€‚**

**å…³é”®è¯**: `ç¬¬ä¸€äººç§°è§†é¢‘ç†è§£` `é”™è¯¯å½’å› ` `ç»†ç²’åº¦åˆ†æž` `æ•°æ®å¼•æ“Ž` `æ³¨æ„åŠ›æ¨¡åž‹` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é”™è¯¯ç†è§£æ–¹æ³•ç¼ºä¹ç»†ç²’åº¦è¾“å‡ºï¼Œæ— æ³•å°†é”™è¯¯å½’å› äºŽæŒ‡ä»¤æˆ–è§†é¢‘ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘MisEngineæ•°æ®å¼•æ“Žè‡ªåŠ¨æž„å»ºé”™è¯¯æ ·æœ¬ï¼Œå¹¶è®¾è®¡MisFormeræ¨¡åž‹ç»Ÿä¸€å¤„ç†è¯­ä¹‰ã€æ—¶é—´å’Œç©ºé—´ç»´åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EPIC-KITCHENS-Må’ŒEgo4D-Mæ•°æ®é›†ä¸Šï¼ŒMisFormerä¼˜äºŽå¤šç§åŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.

