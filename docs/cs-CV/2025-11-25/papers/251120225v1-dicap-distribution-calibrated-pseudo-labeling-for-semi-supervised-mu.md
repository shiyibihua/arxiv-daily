---
layout: default
title: DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning
---

# DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning

**arXiv**: [2511.20225v1](https://arxiv.org/abs/2511.20225) | [PDF](https://arxiv.org/pdf/2511.20225.pdf)

**ä½œè€…**: Bo Han, Zhuoming Li, Xiaoyu Wang, Yaxin Hou, Hui Liu, Junhui Hou, Yuheng Jia

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiCaPæ¡†æž¶ä»¥è§£å†³åŠç›‘ç£å¤šæ ‡ç­¾å­¦ä¹ ä¸­ä¼ªæ ‡ç­¾æƒé‡åˆ†é…é—®é¢˜**

**å…³é”®è¯**: `åŠç›‘ç£å¤šæ ‡ç­¾å­¦ä¹ ` `ä¼ªæ ‡ç­¾æ ¡å‡†` `åˆ†å¸ƒæ ¡å‡†` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ ‡ç­¾åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¯¹ä¼ªæ ‡ç­¾åˆ†é…ç­‰æƒé‡ï¼Œæ˜“æ”¾å¤§å™ªå£°é¢„æµ‹å½±å“æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽåŽéªŒç²¾åº¦ä¼°è®¡ä¼ªæ ‡ç­¾æƒé‡ï¼Œå¹¶é‡‡ç”¨åŒé˜ˆå€¼æœºåˆ¶åŒºåˆ†æ ·æœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ€§èƒ½æå‡ï¼Œæœ€é«˜è¶…è¶ŠSOTAæ–¹æ³•4.27%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.

