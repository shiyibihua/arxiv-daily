---
layout: default
title: Thinking in 360Â°: Humanoid Visual Search in the Wild
---

# Thinking in 360Â°: Humanoid Visual Search in the Wild

**arXiv**: [2511.20351v1](https://arxiv.org/abs/2511.20351) | [PDF](https://arxiv.org/pdf/2511.20351.pdf)

**ä½œè€…**: Heyang Yu, Yinan Han, Xiangyu Zhang, Baiqiao Yin, Bowen Chang, Xiangyu Han, Xinhao Liu, Jing Zhang, Marco Pavone, Chen Feng, Saining Xie, Yiming Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäººå½¢è§†è§‰æœç´¢æ–¹æ³•ä»¥åœ¨360Â°å…¨æ™¯ä¸­é«˜æ•ˆæœç´¢ç‰©ä½“å’Œè·¯å¾„**

**å…³é”®è¯**: `äººå½¢è§†è§‰æœç´¢` `360Â°å…¨æ™¯å›¾åƒ` `è§†è§‰ç©ºé—´æŽ¨ç†` `åŽè®­ç»ƒæŠ€æœ¯` `åŸºå‡†æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰æœç´¢æ–¹æ³•å±€é™äºŽé™æ€å›¾åƒï¼Œå¿½ç•¥ç‰©ç†äº¤äº’å’Œ3Dä¸–ç•Œã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºäººå½¢ä»£ç†ä¸»åŠ¨æ—‹è½¬å¤´éƒ¨ï¼Œåœ¨H* BenchåŸºå‡†ä¸­æ¨¡æ‹ŸçœŸå®žåœºæ™¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡åŽè®­ç»ƒæå‡Qwen2.5-VLï¼Œç‰©ä½“æœç´¢æˆåŠŸçŽ‡ä»Ž14.83%å¢žè‡³47.38%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360Â°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360Â° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.

