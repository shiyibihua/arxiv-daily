---
layout: default
title: Forgetting by Pruning: Data Deletion in Join Cardinality Estimation
---

# Forgetting by Pruning: Data Deletion in Join Cardinality Estimation

**arXiv**: [2511.20293v1](https://arxiv.org/abs/2511.20293) | [PDF](https://arxiv.org/pdf/2511.20293.pdf)

**ä½œè€…**: Chaowei He, Yuanjun Liu, Qingzhi Ma, Shenyuan Ren, Xizhao Luo, Lei Zhao, An Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCEPæ¡†æž¶ä»¥è§£å†³å¤šè¡¨å­¦ä¹ åŸºæ•°ä¼°è®¡ä¸­çš„æ•°æ®åˆ é™¤æŒ‘æˆ˜**

**å…³é”®è¯**: `åŸºæ•°ä¼°è®¡` `æœºå™¨åŽ»å­¦ä¹ ` `æ•°æ®åˆ é™¤` `å¤šè¡¨æŸ¥è¯¢` `å‰ªæžä¼˜åŒ–` `æ•°æ®åº“ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šè¡¨å…³ç³»æ•°æ®ä¸­æ•°æ®åˆ é™¤å¯¼è‡´å±žæ€§æ•æ„Ÿã€è¡¨é—´ä¼ æ’­å’ŒåŸŸæ¶ˆå¤±ï¼Œå¼•å‘ä¸¥é‡é«˜ä¼°
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åˆ†å¸ƒæ•æ„Ÿå‰ªæžå’ŒåŸŸå‰ªæžï¼Œæž„å»ºåŠè¿žæŽ¥åˆ é™¤ç»“æžœå¹¶æŒ‡å¯¼å‚æ•°å‰ªæž
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨IMDBå’ŒTPC-Hæ•°æ®é›†ä¸ŠCEPå®žçŽ°æœ€ä½ŽQè¯¯å·®ï¼Œè®¡ç®—å¼€é”€ä»…0.3%-2.5%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.

