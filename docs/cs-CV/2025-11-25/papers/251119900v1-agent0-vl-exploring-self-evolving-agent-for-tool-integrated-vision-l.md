---
layout: default
title: Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning
---

# Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning

**arXiv**: [2511.19900v1](https://arxiv.org/abs/2511.19900) | [PDF](https://arxiv.org/pdf/2511.19900.pdf)

**ä½œè€…**: Jiaqi Liu, Kaiwen Xiong, Peng Xia, Yiyang Zhou, Haonian Ji, Lu Feng, Siwei Han, Mingyu Ding, Huaxiu Yao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAgent0-VLè‡ªè¿›åŒ–è§†è§‰è¯­è¨€ä»£ç†ï¼Œé€šè¿‡å·¥å…·é›†æˆæŽ¨ç†è§£å†³è§†è§‰æŽ¨ç†è‡ªç›‘ç£å­¦ä¹ é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€ä»£ç†` `å·¥å…·é›†æˆæŽ¨ç†` `è‡ªè¿›åŒ–å­¦ä¹ ` `è‡ªç›‘ç£è¯„ä¼°` `å¤šæ¨¡æ€æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€ä»£ç†ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œæ–‡æœ¬è‡ªè¯„ä¼°æ˜“äº§ç”Ÿå¹»è§‰ï¼Œéš¾ä»¥éªŒè¯å¤æ‚è§†è§‰æŽ¨ç†
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆå·¥å…·ç”¨äºŽæŽ¨ç†ã€è‡ªè¯„ä¼°å’Œè‡ªä¿®å¤ï¼Œé€šè¿‡æ±‚è§£å™¨å’ŒéªŒè¯å™¨è§’è‰²å®žçŽ°è‡ªè¿›åŒ–å¾ªçŽ¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å‡ ä½•é—®é¢˜è§£å†³å’Œè§†è§‰ç§‘å­¦åˆ†æžä¸­ï¼Œæ¯”åŸºç¡€æ¨¡åž‹æå‡12.5%ï¼Œæ— éœ€å¤–éƒ¨å¥–åŠ±

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at \href{https://github.com/aiming-lab/Agent0/Agent0-VL}{this https URL}.

