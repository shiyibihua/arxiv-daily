---
layout: default
title: New York Smells: A Large Multimodal Dataset for Olfaction
---

# New York Smells: A Large Multimodal Dataset for Olfaction

**arXiv**: [2511.20544v1](https://arxiv.org/abs/2511.20544) | [PDF](https://arxiv.org/pdf/2511.20544.pdf)

**ä½œè€…**: Ege Ozguroglu, Junbang Liang, Ruoshi Liu, Mia Chiquier, Michael DeTienne, Wesley Wei Qian, Alexandra Horowitz, Andrew Owens, Carl Vondrick

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤§åž‹å¤šæ¨¡æ€å—…è§‰æ•°æ®é›†ä»¥è§£å†³è‡ªç„¶åœºæ™¯å—…è§‰æ•°æ®ç¼ºä¹é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ•°æ®é›†` `å—…è§‰è¡¨ç¤ºå­¦ä¹ ` `è·¨æ¨¡æ€æ£€ç´¢` `åœºæ™¯è¯†åˆ«` `ç»†ç²’åº¦åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå—…è§‰ä½œä¸ºåŠ¨ç‰©æ„ŸçŸ¥ä¸–ç•Œçš„é‡è¦æ–¹å¼ï¼Œåœ¨æœºå™¨ä¸­éš¾ä»¥è®¿é—®ï¼Œç¼ºä¹è‡ªç„¶åœºæ™¯å¤šæ¨¡æ€æ•°æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ”¶é›†7000å¯¹å›¾åƒ-å—…è§‰ä¿¡å·ï¼Œè¦†ç›–3500ä¸ªå¯¹è±¡ï¼Œå®¤å†…å¤–çŽ¯å¢ƒï¼Œæ•°æ®è§„æ¨¡è¿œè¶…çŽ°æœ‰ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè§†è§‰æ•°æ®æ”¯æŒè·¨æ¨¡æ€å—…è§‰å­¦ä¹ ï¼Œå­¦ä¹ è¡¨ç¤ºä¼˜äºŽæ‰‹å·¥ç‰¹å¾ï¼Œåº”ç”¨äºŽæ£€ç´¢å’Œè¯†åˆ«ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.

