---
layout: default
title: Softmax Transformers are Turing-Complete
---

# Softmax Transformers are Turing-Complete

**arXiv**: [2511.20038v1](https://arxiv.org/abs/2511.20038) | [PDF](https://arxiv.org/pdf/2511.20038.pdf)

**ä½œè€…**: Hongjian Jiang, Michael Hahn, Georg Zetzsche, Anthony Widjaja Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯æ˜Žé•¿åº¦å¯æ³›åŒ–çš„è½¯æ³¨æ„åŠ›é“¾å¼æ€ç»´å˜æ¢å™¨æ˜¯å›¾çµå®Œå¤‡çš„**

**å…³é”®è¯**: `è½¯æ³¨æ„åŠ›å˜æ¢å™¨` `å›¾çµå®Œå¤‡æ€§` `é“¾å¼æ€ç»´` `é•¿åº¦æ³›åŒ–` `ç›¸å¯¹ä½ç½®ç¼–ç ` `ç®—æœ¯æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè½¯æ³¨æ„åŠ›é“¾å¼æ€ç»´å˜æ¢å™¨æ˜¯å¦å›¾çµå®Œå¤‡æ˜¯å¼€æ”¾é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é“¾å¼æ€ç»´C-RASPæ‰©å±•è¯æ˜Žå›¾çµå®Œå¤‡æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šè®­ç»ƒå˜æ¢å™¨éªŒè¯å¤æ‚ç®—æœ¯æŽ¨ç†è¯­è¨€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Hard attention Chain-of-Thought (CoT) transformers are known to be Turing-complete. However, it is an open problem whether softmax attention Chain-of-Thought (CoT) transformers are Turing-complete. In this paper, we prove a stronger result that length-generalizable softmax CoT transformers are Turing-complete. More precisely, our Turing-completeness proof goes via the CoT extension of the Counting RASP (C-RASP), which correspond to softmax CoT transformers that admit length generalization. We prove Turing-completeness for CoT C-RASP with causal masking over a unary alphabet (more generally, for letter-bounded languages). While we show this is not Turing-complete for arbitrary languages, we prove that its extension with relative positional encoding is Turing-complete for arbitrary languages. We empirically validate our theory by training transformers for languages requiring complex (non-linear) arithmetic reasoning.

