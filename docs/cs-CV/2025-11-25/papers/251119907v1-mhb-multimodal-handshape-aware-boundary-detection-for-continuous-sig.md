---
layout: default
title: MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition
---

# MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition

**arXiv**: [2511.19907v1](https://arxiv.org/abs/2511.19907) | [PDF](https://arxiv.org/pdf/2511.19907.pdf)

**ä½œè€…**: Mingyu Zhao, Zhanfu Yang, Yang Zhou, Zhaoyang Xia, Can Jin, Xiaoxiao He, Carol Neidle, Dimitris N. Metaxas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ‰‹å½¢æ„ŸçŸ¥è¾¹ç•Œæ£€æµ‹æ–¹æ³•ï¼Œç”¨äºŽè¿žç»­æ‰‹è¯­è¯†åˆ«ä¸­çš„ç¬¦å·åˆ†å‰²ä¸Žè¯†åˆ«ã€‚**

**å…³é”®è¯**: `è¿žç»­æ‰‹è¯­è¯†åˆ«` `è¾¹ç•Œæ£€æµ‹` `å¤šæ¨¡æ€èžåˆ` `æ‰‹å½¢åˆ†ç±»` `3Déª¨éª¼ç‰¹å¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¿žç»­æ‰‹è¯­è§†é¢‘ä¸­ç¬¦å·è¾¹ç•Œæ£€æµ‹çš„å‡†ç¡®æ€§ä¸è¶³ï¼Œå½±å“è¯†åˆ«æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆ3Déª¨éª¼ç‰¹å¾å’Œæ‰‹å½¢åˆ†ç±»å™¨ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¨¡å—æå‡è¾¹ç•Œæ£€æµ‹é²æ£’æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ASLLRPè¯­æ–™åº“ä¸Šè¯„ä¼°ï¼Œç›¸æ¯”å…ˆå‰å·¥ä½œæœ‰æ˜¾è‘—æ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a multimodal approach for continuous sign recognition that first uses machine learning to detect the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then recognizes the segmented signs. For improved robustness, we use 3D skeletal features extracted from sign language videos to capture the convergence of sign properties and their dynamics, which tend to cluster at sign boundaries. Another focus of this work is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and the handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing, as such signs often differ in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.

