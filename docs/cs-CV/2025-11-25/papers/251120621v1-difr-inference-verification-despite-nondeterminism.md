---
layout: default
title: DiFR: Inference Verification Despite Nondeterminism
---

# DiFR: Inference Verification Despite Nondeterminism

**arXiv**: [2511.20621v1](https://arxiv.org/abs/2511.20621) | [PDF](https://arxiv.org/pdf/2511.20621.pdf)

**ä½œè€…**: Adam Karvonen, Daniel Reuter, Roy Rinberg, Luke Marks, AdriÃ  Garriga-Alonso, Keri Warr

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToken-DiFRå’ŒActivation-DiFRæ–¹æ³•ä»¥éªŒè¯LLMæŽ¨ç†æ­£ç¡®æ€§**

**å…³é”®è¯**: `æŽ¨ç†éªŒè¯` `å¤§è¯­è¨€æ¨¡åž‹` `éšæœºç§å­åŒæ­¥` `æ¿€æ´»åŽ‹ç¼©` `é‡åŒ–æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LLMæŽ¨ç†ä¸­æ•°å€¼å™ªå£°å¯¼è‡´ç»“æžœå·®å¼‚ï¼Œéš¾ä»¥åŒºåˆ†åˆæ³•å˜åŒ–ä¸Žé”™è¯¯
2. Token-DiFRæ¯”è¾ƒç”Ÿæˆä»¤ç‰Œä¸Žå‚è€ƒå®žçŽ°é¢„æµ‹ï¼ŒActivation-DiFRåŽ‹ç¼©æ¿€æ´»ç”¨äºŽéªŒè¯
3. å®žéªŒæ˜¾ç¤ºé«˜ç²¾åº¦æ£€æµ‹é‡åŒ–é”™è¯¯ï¼ŒAUC>0.999ï¼Œå‡å°‘é€šä¿¡å¼€é”€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

