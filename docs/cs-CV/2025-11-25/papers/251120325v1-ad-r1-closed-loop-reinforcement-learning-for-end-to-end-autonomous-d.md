---
layout: default
title: AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models
---

# AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models

**arXiv**: [2511.20325v1](https://arxiv.org/abs/2511.20325) | [PDF](https://arxiv.org/pdf/2511.20325.pdf)

**ä½œè€…**: Tianyi Yan, Tao Tang, Xingtai Gui, Yongkang Li, Jiasen Zhesng, Weiyao Huang, Lingdong Kong, Wencheng Han, Xia Zhou, Xueyang Zhang, Yifei Zhan, Kun Zhan, Cheng-zhong Xu, Jianbing Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå…¬æ­£ä¸–ç•Œæ¨¡åž‹çš„é—­çŽ¯å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œä»¥æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡åž‹` `åäº‹å®žåˆæˆ` `å®‰å…¨è¯„ä¼°` `é—­çŽ¯æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å› ä¸–ç•Œæ¨¡åž‹å­˜åœ¨ä¹è§‚åè§ï¼Œéš¾ä»¥å¤„ç†é•¿å°¾å®‰å…¨äº‹ä»¶
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åäº‹å®žåˆæˆç”Ÿæˆç¢°æ’žå’Œè¶Šé‡Žäº‹ä»¶æ•°æ®ï¼Œè®­ç»ƒå…¬æ­£ä¸–ç•Œæ¨¡åž‹ä½œä¸ºå†…éƒ¨æ‰¹è¯„å™¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é£Žé™©é¢„è§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡å¤±è´¥é¢„æµ‹èƒ½åŠ›ï¼Œå‡å°‘æ¨¡æ‹Ÿä¸­çš„å®‰å…¨è¿è§„

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.

