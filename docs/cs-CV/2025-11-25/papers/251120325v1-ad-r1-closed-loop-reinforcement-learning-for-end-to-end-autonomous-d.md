---
layout: default
title: AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models
---

# AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models

**arXiv**: [2511.20325v1](https://arxiv.org/abs/2511.20325) | [PDF](https://arxiv.org/pdf/2511.20325.pdf)

**ä½œè€…**: Tianyi Yan, Tao Tang, Xingtai Gui, Yongkang Li, Jiasen Zhesng, Weiyao Huang, Lingdong Kong, Wencheng Han, Xia Zhou, Xueyang Zhang, Yifei Zhan, Kun Zhan, Cheng-zhong Xu, Jianbing Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AD-R1ï¼šåŸºäºŽå…¬æ­£ä¸–ç•Œæ¨¡åž‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶é—­çŽ¯å¼ºåŒ–å­¦ä¹ **

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡åž‹` `é£Žé™©é¢„æµ‹` `åäº‹å®žåˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”±äºŽä¸–ç•Œæ¨¡åž‹ä¸­å›ºæœ‰çš„ä¹è§‚åå·®ï¼Œéš¾ä»¥ä¿è¯å®‰å…¨æ€§å’Œå¤„ç†é•¿å°¾äº‹ä»¶ã€‚
2. æå‡ºä¸€ç§åŸºäºŽå…¬æ­£ä¸–ç•Œæ¨¡åž‹çš„åŽè®­ç»ƒç­–ç•¥æ”¹è¿›æ¡†æž¶ï¼Œé€šè¿‡åäº‹å®žåˆæˆæ•°æ®ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å±é™©ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ¨¡åž‹åœ¨é¢„æµ‹å¤±è´¥æ–¹é¢ä¼˜äºŽåŸºçº¿ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ¨¡æ‹ŸçŽ¯å¢ƒä¸­çš„å®‰å…¨è¿è§„è¡Œä¸ºã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡åž‹æœ‰æœ›ç›´æŽ¥ä»Žä¼ æ„Ÿå™¨æ•°æ®ä¸­å­¦ä¹ å¤æ‚è¡Œä¸ºï¼Œä½†é¢ä¸´å®‰å…¨æ€§å’Œå¤„ç†é•¿å°¾äº‹ä»¶çš„å…³é”®æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ (RL)ä¸ºå…‹æœè¿™äº›é™åˆ¶æä¾›äº†ä¸€æ¡æœ‰å¸Œæœ›çš„é€”å¾„ï¼Œä½†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æˆåŠŸä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚æˆ‘ä»¬å‘çŽ°äº†ä¸€ä¸ªé˜»ç¢è¿™ä¸€è¿›å±•çš„æ ¹æœ¬ç¼ºé™·ï¼šç”¨äºŽRLçš„ä¸–ç•Œæ¨¡åž‹ä¸­å­˜åœ¨æ ¹æ·±è’‚å›ºçš„ä¹è§‚åå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå›´ç»•å…¬æ­£ä¸–ç•Œæ¨¡åž‹æž„å»ºçš„åŽè®­ç»ƒç­–ç•¥æ”¹è¿›æ¡†æž¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯æ•™ä¼šè¿™ä¸ªæ¨¡åž‹è¯šå®žåœ°é¢å¯¹å±é™©ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆç®¡é“â€”â€”åäº‹å®žåˆæˆæ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œè¯¥ç®¡é“ç³»ç»Ÿåœ°ç”Ÿæˆäº†ä¸°å¯Œçš„ã€çœ‹ä¼¼åˆç†çš„ç¢°æ’žå’Œè¶Šé‡Žäº‹ä»¶è¯¾ç¨‹ã€‚è¿™ä½¿å¾—æ¨¡åž‹ä»Žè¢«åŠ¨çš„åœºæ™¯è¡¥å…¨å™¨è½¬å˜ä¸ºçœŸå®žçš„é¢„æµ‹å™¨ï¼Œä¿æŒäº†è¡ŒåŠ¨å’Œç»“æžœä¹‹é—´çš„å› æžœè”ç³»ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†è¿™ä¸ªå…¬æ­£çš„ä¸–ç•Œæ¨¡åž‹é›†æˆåˆ°æˆ‘ä»¬çš„é—­çŽ¯RLæ¡†æž¶ä¸­ï¼Œå®ƒåœ¨å…¶ä¸­å……å½“å†…éƒ¨è¯„è®ºå‘˜ã€‚åœ¨æ”¹è¿›è¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“æŸ¥è¯¢è¯„è®ºå‘˜ä»¥â€œæ¢¦æƒ³â€å€™é€‰è¡ŒåŠ¨çš„ç»“æžœã€‚é€šè¿‡åŒ…æ‹¬æ–°çš„é£Žé™©é¢„æµ‹åŸºå‡†åœ¨å†…çš„å¤§é‡å®žéªŒï¼Œæˆ‘ä»¬è¯æ˜Žäº†æˆ‘ä»¬çš„æ¨¡åž‹åœ¨é¢„æµ‹å¤±è´¥æ–¹é¢æ˜Žæ˜¾ä¼˜äºŽåŸºçº¿ã€‚å› æ­¤ï¼Œå½“ç”¨ä½œè¯„è®ºå‘˜æ—¶ï¼Œå®ƒèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿä¸­çš„å®‰å…¨è¿è§„è¡Œä¸ºï¼Œè¯æ˜Žäº†æ•™ä¼šæ¨¡åž‹æ¢¦æƒ³å±é™©æ˜¯æž„å»ºçœŸæ­£å®‰å…¨å’Œæ™ºèƒ½çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“çš„å…³é”®ä¸€æ­¥ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽå¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ–¹æ³•ï¼Œå…¶ä¸–ç•Œæ¨¡åž‹å¾€å¾€å­˜åœ¨ä¹è§‚åå·®ï¼Œå³å€¾å‘äºŽä½Žä¼°å±é™©æƒ…å†µå‘ç”Ÿçš„æ¦‚çŽ‡ï¼Œå¯¼è‡´æ™ºèƒ½ä½“åœ¨è®­ç»ƒå’Œå®žé™…åº”ç”¨ä¸­åšå‡ºä¸å®‰å…¨çš„å†³ç­–ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†ç½•è§ä½†å±é™©çš„é•¿å°¾äº‹ä»¶æ—¶ï¼Œè¿™ç§åå·®ä¼šæ›´åŠ æ˜Žæ˜¾ã€‚å› æ­¤ï¼Œå¦‚ä½•æž„å»ºä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å±é™©æƒ…å†µçš„ä¸–ç•Œæ¨¡åž‹ï¼Œæ˜¯æå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªâ€œå…¬æ­£â€çš„ä¸–ç•Œæ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¦‚å®žåœ°é¢„æµ‹å±é™©æƒ…å†µï¼Œé¿å…ä¹è§‚åå·®ã€‚é€šè¿‡è®©æ¨¡åž‹â€œæ¢¦æƒ³â€å„ç§å¯èƒ½å‘ç”Ÿçš„å±é™©åœºæ™¯ï¼Œå¹¶å­¦ä¹ è¿™äº›åœºæ™¯çš„åŽæžœï¼Œä»Žè€Œæé«˜å…¶å¯¹é£Žé™©çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºŽè®©æ¨¡åž‹è¿›è¡Œâ€œåŽ‹åŠ›æµ‹è¯•â€ï¼Œä½¿å…¶åœ¨å„ç§æžç«¯æƒ…å†µä¸‹éƒ½èƒ½ä¿æŒæ¸…é†’çš„å¤´è„‘ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šAD-R1æ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡åäº‹å®žåˆæˆæ•°æ®è®­ç»ƒä¸€ä¸ªå…¬æ­£çš„ä¸–ç•Œæ¨¡åž‹ã€‚ç„¶åŽï¼Œå°†è¯¥ä¸–ç•Œæ¨¡åž‹é›†æˆåˆ°é—­çŽ¯å¼ºåŒ–å­¦ä¹ æ¡†æž¶ä¸­ï¼Œä½œä¸ºå†…éƒ¨è¯„è®ºå‘˜ï¼Œç”¨äºŽè¯„ä¼°å€™é€‰åŠ¨ä½œçš„å®‰å…¨æ€§ã€‚åœ¨ç­–ç•¥æ”¹è¿›é˜¶æ®µï¼Œæ™ºèƒ½ä½“ä¼šæŸ¥è¯¢ä¸–ç•Œæ¨¡åž‹ï¼Œé¢„æµ‹ä¸åŒåŠ¨ä½œå¯èƒ½å¯¼è‡´çš„åŽæžœï¼Œå¹¶é€‰æ‹©æœ€å®‰å…¨çš„åŠ¨ä½œæ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†åäº‹å®žåˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºŽè®­ç»ƒå…¬æ­£çš„ä¸–ç•Œæ¨¡åž‹ã€‚ä¸Žä¼ ç»Ÿçš„ä¾èµ–çœŸå®žæ•°æ®çš„æ–¹æ³•ä¸åŒï¼Œåäº‹å®žåˆæˆèƒ½å¤Ÿç³»ç»Ÿåœ°ç”Ÿæˆå„ç§å¯èƒ½å‘ç”Ÿçš„å±é™©åœºæ™¯ï¼ŒåŒ…æ‹¬ç¢°æ’žå’Œè¶Šé‡Žäº‹ä»¶ï¼Œä»Žè€Œæœ‰æ•ˆåœ°å…‹æœäº†é•¿å°¾äº‹ä»¶æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå°†ä¸–ç•Œæ¨¡åž‹ä½œä¸ºå†…éƒ¨è¯„è®ºå‘˜ï¼Œä¹Ÿä¸ºå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å®‰å…¨æ€§è¯„ä¼°æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåäº‹å®žåˆæˆæ•°æ®ç”Ÿæˆç®¡é“æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒã€‚å®ƒé€šè¿‡å¯¹çœŸå®žåœºæ™¯è¿›è¡Œä¿®æ”¹ï¼Œä¾‹å¦‚æ”¹å˜è½¦è¾†çš„è¿åŠ¨è½¨è¿¹ã€å¢žåŠ éšœç¢ç‰©ç­‰ï¼Œæ¥ç”Ÿæˆå„ç§å±é™©åœºæ™¯ã€‚ä¸ºäº†ä¿è¯åˆæˆæ•°æ®çš„åˆç†æ€§ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç³»åˆ—çº¦æŸæ¡ä»¶ï¼Œä¾‹å¦‚ä¿æŒåœºæ™¯çš„ç‰©ç†ä¸€è‡´æ€§ã€‚åœ¨è®­ç»ƒä¸–ç•Œæ¨¡åž‹æ—¶ï¼Œé‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡åž‹åŒºåˆ†ä¸åŒçš„å±é™©åœºæ™¯ï¼Œå¹¶å‡†ç¡®é¢„æµ‹å…¶åŽæžœã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ¨¡åž‹åœ¨æ–°çš„é£Žé™©é¢„æµ‹åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºŽåŸºçº¿æ¨¡åž‹ï¼Œè¯æ˜Žäº†å…¶åœ¨é¢„æµ‹å¤±è´¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹ŸçŽ¯å¢ƒä¸­ï¼Œä½¿ç”¨è¯¥æ¨¡åž‹ä½œä¸ºè¯„è®ºå‘˜çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œå…¶å®‰å…¨è¿è§„è¡Œä¸ºæ˜¾è‘—å‡å°‘ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§æ–¹é¢çš„æ½œåŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§æå‡ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£Žé™©åœºæ™¯ä¸‹çš„å†³ç­–ã€‚é€šè¿‡æ›´å‡†ç¡®çš„é£Žé™©é¢„æµ‹ï¼Œè‡ªåŠ¨é©¾é©¶è½¦è¾†èƒ½å¤Ÿæ›´å¥½åœ°é¿å…äº‹æ•…ï¼Œæé«˜è¡Œé©¶å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æŽ¨å¹¿åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºã€æœåŠ¡æœºå™¨äººç­‰ï¼Œç”¨äºŽæé«˜å…¶åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå®‰å…¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.

