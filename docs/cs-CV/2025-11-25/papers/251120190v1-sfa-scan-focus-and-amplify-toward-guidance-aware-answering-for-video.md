---
layout: default
title: SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA
---

# SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA

**arXiv**: [2511.20190v1](https://arxiv.org/abs/2511.20190) | [PDF](https://arxiv.org/pdf/2511.20190.pdf)

**ä½œè€…**: Haibin He, Qihuang Zhong, Juhua Liu, Bo Du, Peng Wang, Jing Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSFAæ¡†æž¶ä»¥è§£å†³è§†é¢‘æ–‡æœ¬è§†è§‰é—®ç­”ä¸­çš„æ–‡æœ¬æ„ŸçŸ¥ä¸Žå¼•å¯¼é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘æ–‡æœ¬è§†è§‰é—®ç­”` `æ— è®­ç»ƒæ¡†æž¶` `æ³¨æ„åŠ›å¼•å¯¼` `æ–‡æœ¬æ„ŸçŸ¥` `æ—¶ç©ºæ•´åˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘ä¸­æ–‡æœ¬å°ºåº¦ã€æ–¹å‘ã€æ¸…æ™°åº¦å˜åŒ–å¤§ï¼Œéœ€æ•´åˆæ—¶ç©ºè¯­ä¹‰å¹¶è¿‡æ»¤å†—ä½™ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ— è®­ç»ƒæ¡†æž¶ï¼Œé€šè¿‡æ‰«æã€èšç„¦ã€æ”¾å¤§å…³é”®åŒºåŸŸå¼•å¯¼Video-LLMæ³¨æ„åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®žçŽ°æ–°SOTAï¼Œè¶…è¶Šå…ˆå‰æ–¹æ³•ï¼ŒéªŒè¯æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video text-based visual question answering (Video TextVQA) task aims to answer questions about videos by leveraging the visual text appearing within the videos. This task poses significant challenges, requiring models to accurately perceive and comprehend scene text that varies in scale, orientation, and clarity across frames, while effectively integrating temporal and semantic context to generate precise answers. Moreover, the model must identify question-relevant textual cues and filter out redundant or irrelevant information to ensure answering is guided by the most relevant and informative cues. To address these challenges, we propose SFA, a training-free framework and the first Video-LLM-based method tailored for Video TextVQA, motivated by the human process of answering questions. By adaptively scanning video frames, selectively focusing on key regions, and directly amplifying them, SFA effectively guides the Video-LLM's attention toward essential cues, enabling it to generate more accurate answers. SFA achieves new state-of-the-art results across several public Video TextVQA datasets and surpasses previous methods by a substantial margin, demonstrating its effectiveness and generalizability.

