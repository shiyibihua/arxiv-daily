---
layout: default
title: On the Limits of Momentum in Decentralized and Federated Optimization
---

# On the Limits of Momentum in Decentralized and Federated Optimization

**arXiv**: [2511.20168v1](https://arxiv.org/abs/2511.20168) | [PDF](https://arxiv.org/pdf/2511.20168.pdf)

**ä½œè€…**: Riccardo Zaccone, Sai Praneeth Karimireddy, Carlo Masone

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æžåŠ¨é‡åœ¨åŽ»ä¸­å¿ƒåŒ–å’Œè”é‚¦ä¼˜åŒ–ä¸­çš„å±€é™æ€§ï¼Œè¯æ˜Žå…¶å—ç»Ÿè®¡å¼‚è´¨æ€§å½±å“**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `åŽ»ä¸­å¿ƒåŒ–ä¼˜åŒ–` `åŠ¨é‡æ–¹æ³•` `ç»Ÿè®¡å¼‚è´¨æ€§` `æ”¶æ•›åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŠ¨é‡åœ¨åŽ»ä¸­å¿ƒåŒ–å’Œè”é‚¦å­¦ä¹ ä¸­æ˜¯å¦èƒ½å…‹æœç»Ÿè®¡å¼‚è´¨æ€§ä¿è¯æ”¶æ•›
2. æ–¹æ³•è¦ç‚¹ï¼šç†è®ºåˆ†æžåŠ¨é‡åœ¨å¾ªçŽ¯å®¢æˆ·ç«¯å‚ä¸Žä¸‹çš„è¡Œä¸ºï¼Œè¯æ˜Žå…¶æ”¶æ•›å—é™
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ•°å€¼å’Œæ·±åº¦å­¦ä¹ å®žéªŒéªŒè¯ç†è®ºï¼Œç¡®è®¤åœ¨çŽ°å®žåœºæ™¯ä¸­çš„ç›¸å…³æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Î˜\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

