---
layout: default
title: RubricRL: Simple Generalizable Rewards for Text-to-Image Generation
---

# RubricRL: Simple Generalizable Rewards for Text-to-Image Generation

**arXiv**: [2511.20651v1](https://arxiv.org/abs/2511.20651) | [PDF](https://arxiv.org/pdf/2511.20651.pdf)

**ä½œè€…**: Xuelu Feng, Yunsheng Li, Ziyu Wan, Zixuan Gao, Junsong Yuan, Dongdong Chen, Chunming Qiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRubricRLæ¡†æž¶ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å¥–åŠ±è®¾è®¡ç¼ºä¹å¯è§£é‡Šæ€§å’Œçµæ´»æ€§çš„é—®é¢˜**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ å¯¹é½` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¯è§£é‡Šå¥–åŠ±è®¾è®¡` `å¤šæ¨¡æ€è¯„ä¼°` `ç»“æž„åŒ–è¯„åˆ†è¡¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¥–åŠ±æ–¹æ³•ä¾èµ–å›ºå®šæƒé‡å¤åˆæŒ‡æ ‡æˆ–é»‘ç›’æ ‡é‡ï¼Œé™åˆ¶å¯è§£é‡Šæ€§å’Œç”¨æˆ·æŽ§åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šåŠ¨æ€æž„å»ºç»“æž„åŒ–è¯„åˆ†è¡¨ï¼Œåˆ†è§£ä¸ºç»†ç²’åº¦è§†è§‰æ ‡å‡†ï¼Œå¹¶ä½¿ç”¨å¤šæ¨¡æ€è¯„ä¼°å™¨ç‹¬ç«‹è¯„åˆ†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è‡ªå›žå½’æ¨¡åž‹ä¸­æå‡æç¤ºå¿ å®žåº¦ã€è§†è§‰ç»†èŠ‚å’Œæ³›åŒ–æ€§ï¼Œæä¾›å¯æ‰©å±•å¯¹é½åŸºç¡€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.

