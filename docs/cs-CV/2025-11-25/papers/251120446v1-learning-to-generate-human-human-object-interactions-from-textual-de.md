---
layout: default
title: Learning to Generate Human-Human-Object Interactions from Textual Descriptions
---

# Learning to Generate Human-Human-Object Interactions from Textual Descriptions

**arXiv**: [2511.20446v1](https://arxiv.org/abs/2511.20446) | [PDF](https://arxiv.org/pdf/2511.20446.pdf)

**ä½œè€…**: Jeonghyeon Na, Sangwon Baik, Inhee Lee, Junyoung Lee, Hanbyul Joo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: Project Page: https://tlb-miss.github.io/hhoi/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHHOIç”Ÿæˆæ¡†æž¶ï¼Œä»Žæ–‡æœ¬æè¿°ç”Ÿæˆäºº-äºº-ç‰©äº¤äº’åœºæ™¯ï¼Œå¹¶æž„å»ºäº†ç›¸å…³æ•°æ®é›†ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸Žååº” (Interaction & Reaction)**

**å…³é”®è¯**: `äºº-äºº-ç‰©äº¤äº’` `HHOIç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `æ–‡æœ¬ç”Ÿæˆå›¾åƒ` `å¤šä¸»ä½“äº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•éš¾ä»¥ç†è§£å¤æ‚ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„äººé™…äº’åŠ¨è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤šä¸ªä¸ªä½“å’Œåœºæ™¯ç‰©ä½“æ—¶ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ–°é¢–çš„HHOIç”Ÿæˆæ¡†æž¶ï¼Œé€šè¿‡è§£è€¦äºº-ç‰©å’Œäºº-äººäº¤äº’ï¼Œå¹¶ç»“åˆæ‰©æ•£æ¨¡åž‹å®žçŽ°é«˜è´¨é‡ç”Ÿæˆã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬é©±åŠ¨çš„HHOIç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶å¯æ‰©å±•åˆ°å¤šäººè¿åŠ¨ç”Ÿæˆã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç ”ç©¶é—®é¢˜ï¼Œå³å»ºæ¨¡æ¶‰åŠç‰©ä½“çš„ä¸¤ä¸ªäººä¹‹é—´çš„äºº-äºº-ç‰©äº¤äº’(HHOI)ã€‚ä¸ºäº†è§£å†³HHOIä¸“ç”¨æ•°æ®é›†çš„ç¼ºä¹é—®é¢˜ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªæ–°çš„HHOIæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾åƒç”Ÿæˆæ¨¡åž‹åˆæˆHHOIæ•°æ®çš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆä»ŽHHOIä¸­æå–å‡ºå•ä¸ªäºº-ç‰©äº¤äº’(HOI)å’Œäºº-äººäº¤äº’(HHI)ï¼Œå¹¶ä½¿ç”¨åŸºäºŽåˆ†æ•°çš„æ‰©æ•£æ¨¡åž‹è®­ç»ƒæ–‡æœ¬åˆ°HOIå’Œæ–‡æœ¬åˆ°HHIæ¨¡åž‹ã€‚æœ€åŽï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æž¶ï¼Œé›†æˆäº†è¿™ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡åž‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªé«˜çº§é‡‡æ ·è¿‡ç¨‹ä¸­åˆæˆå®Œæ•´çš„HHOIã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†HHOIç”Ÿæˆæ‰©å±•åˆ°å¤šäººè®¾ç½®ï¼Œå®žçŽ°æ¶‰åŠä¸¤ä¸ªä»¥ä¸Šä¸ªä½“çš„äº¤äº’ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé€¼çœŸçš„HHOIï¼Œä¼˜äºŽä»¥å¾€ä»…å…³æ³¨å•äººHOIçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†æ¶‰åŠç‰©ä½“çš„å¤šäººè¿åŠ¨ç”Ÿæˆä½œä¸ºæˆ‘ä»¬æ¡†æž¶çš„ä¸€ä¸ªåº”ç”¨ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºŽå•ä¸ªäººä¸Žç‰©ä½“çš„äº¤äº’(HOI)ç”Ÿæˆï¼Œå¿½ç•¥äº†äººä¸Žäººä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œä»¥åŠè¿™ç§å…³ç³»å¦‚ä½•å—åˆ°ç‰©ä½“çš„å½±å“ã€‚å› æ­¤ï¼ŒçŽ°æœ‰æ–¹æ³•æ— æ³•ç”Ÿæˆå¤æ‚çš„äºº-äºº-ç‰©äº¤äº’(HHOI)åœºæ™¯ï¼Œç¼ºä¹å¯¹å¤šä¸»ä½“äº¤äº’è¡Œä¸ºçš„å»ºæ¨¡èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†HHOIåˆ†è§£ä¸ºä¸¤ä¸ªæ›´ç®€å•çš„å­é—®é¢˜ï¼šäºº-ç‰©äº¤äº’(HOI)å’Œäºº-äººäº¤äº’(HHI)ã€‚é€šè¿‡åˆ†åˆ«å»ºæ¨¡è¿™ä¸¤ä¸ªå­é—®é¢˜ï¼Œç„¶åŽå°†å®ƒä»¬é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æž¶ä¸­ï¼Œä»Žè€Œå®žçŽ°HHOIçš„ç”Ÿæˆã€‚è¿™ç§è§£è€¦çš„æ–¹å¼é™ä½Žäº†å»ºæ¨¡çš„å¤æ‚æ€§ï¼Œå¹¶å…è®¸åˆ©ç”¨çŽ°æœ‰çš„HOIå’ŒHHIæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®é›†æž„å»ºï¼šæ”¶é›†å¹¶æ ‡æ³¨HHOIæ•°æ®ï¼Œå¹¶ä»Žä¸­æå–HOIå’ŒHHIæ•°æ®ã€‚2) æ–‡æœ¬åˆ°HOIæ¨¡åž‹ï¼šä½¿ç”¨åŸºäºŽåˆ†æ•°çš„æ‰©æ•£æ¨¡åž‹ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆHOIã€‚3) æ–‡æœ¬åˆ°HHIæ¨¡åž‹ï¼šä½¿ç”¨åŸºäºŽåˆ†æ•°çš„æ‰©æ•£æ¨¡åž‹ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆHHIã€‚4) ç»Ÿä¸€ç”Ÿæˆæ¡†æž¶ï¼šå°†HOIå’ŒHHIæ¨¡åž‹é›†æˆåˆ°ä¸€ä¸ªæ¡†æž¶ä¸­ï¼Œé€šè¿‡è”åˆé‡‡æ ·ç”Ÿæˆå®Œæ•´çš„HHOIåœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†HHOIçš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè§£è€¦çš„ç”Ÿæˆæ¡†æž¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡äººä¸Žäººä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œå¹¶å°†å…¶ä¸Žäººä¸Žç‰©ä½“çš„äº¤äº’å…³ç³»ç›¸ç»“åˆï¼Œä»Žè€Œç”Ÿæˆæ›´é€¼çœŸã€æ›´ç¬¦åˆä¸Šä¸‹æ–‡çš„äº¤äº’åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æå‡ºäº†ä¸€ä¸ªæ•°æ®åˆæˆç­–ç•¥ï¼Œç”¨äºŽè§£å†³HHOIæ•°æ®é›†çš„ç¼ºä¹é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†åŸºäºŽåˆ†æ•°çš„æ‰©æ•£æ¨¡åž‹ä½œä¸ºHOIå’ŒHHIç”Ÿæˆå™¨çš„æ ¸å¿ƒã€‚æ‰©æ•£æ¨¡åž‹é€šè¿‡é€æ­¥æ·»åŠ å™ªå£°åˆ°æ•°æ®ä¸­ï¼Œç„¶åŽå­¦ä¹ å¦‚ä½•ä»Žå™ªå£°ä¸­æ¢å¤æ•°æ®ï¼Œä»Žè€Œå®žçŽ°é«˜è´¨é‡çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„é‡‡æ ·è¿‡ç¨‹ï¼Œç”¨äºŽå°†HOIå’ŒHHIæ¨¡åž‹é›†æˆåœ¨ä¸€èµ·ï¼Œç”Ÿæˆå®Œæ•´çš„HHOIåœºæ™¯ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæ˜Žç¡®æåŠã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬é©±åŠ¨çš„HHOIç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜Žäº†è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸã€æ›´ç¬¦åˆæ–‡æœ¬æè¿°çš„HHOIåœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æˆåŠŸåœ°åº”ç”¨äºŽå¤šäººè¿åŠ¨ç”Ÿæˆä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè™šæ‹ŸçŽ°å®žã€æ¸¸æˆå¼€å‘ã€ç¤¾äº¤æœºå™¨äººç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºŽç”Ÿæˆæ›´é€¼çœŸçš„è™šæ‹Ÿç¤¾äº¤åœºæ™¯ï¼Œè®­ç»ƒç¤¾äº¤æœºå™¨äººç†è§£å’Œæ¨¡æ‹Ÿäººç±»çš„äº¤äº’è¡Œä¸ºï¼Œä»¥åŠè¾…åŠ©è®¾è®¡æ›´ç¬¦åˆäººç±»ä¹ æƒ¯çš„äº¤äº’ç•Œé¢ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šäººäº¤äº’åœºæ™¯ï¼Œå¹¶åº”ç”¨äºŽæ™ºèƒ½ç›‘æŽ§ã€è¡Œä¸ºåˆ†æžç­‰é¢†åŸŸã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.

