---
layout: default
title: Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification
---

# Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification

**arXiv**: [2511.20474v1](https://arxiv.org/abs/2511.20474) | [PDF](https://arxiv.org/pdf/2511.20474.pdf)

**ä½œè€…**: Akshit Pramod Anchan, Jewelith Thomas, Sritama Roy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡å—åŒ–æ·±åº¦å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽè¾…åŠ©æ„ŸçŸ¥ä¸­çš„æ³¨è§†ã€æƒ…æ„Ÿå’Œè¯´è¯äººè¯†åˆ«ã€‚**

**å…³é”®è¯**: `æ¨¡å—åŒ–æ·±åº¦å­¦ä¹ ` `çœ¼çŠ¶æ€æ£€æµ‹` `é¢éƒ¨è¡¨æƒ…è¯†åˆ«` `è¯´è¯äººè¯†åˆ«` `è¾…åŠ©æŠ€æœ¯` `å¤šæ¨¡æ€æ„ŸçŸ¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€å‘ç»¼åˆè¾…åŠ©æŠ€æœ¯éœ€æ•´åˆè§†è§‰å’Œå¬è§‰æ„ŸçŸ¥ï¼Œé€‚åº”èµ„æºå—é™è®¾å¤‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ¨¡å—åŒ–æž¶æž„ï¼ŒåŒ…æ‹¬CNNæ£€æµ‹çœ¼çŠ¶æ€ã€CNNè¯†åˆ«è¡¨æƒ…å’ŒLSTMè¯†åˆ«è¯´è¯äººã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œæ¨¡åž‹å‡†ç¡®çŽ‡åˆ†åˆ«è¾¾93.0%ã€97.8%å’Œ96.89%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.

