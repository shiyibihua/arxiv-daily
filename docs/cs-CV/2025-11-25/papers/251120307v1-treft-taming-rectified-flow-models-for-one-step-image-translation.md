---
layout: default
title: TReFT: Taming Rectified Flow Models For One-Step Image Translation
---

# TReFT: Taming Rectified Flow Models For One-Step Image Translation

**arXiv**: [2511.20307v1](https://arxiv.org/abs/2511.20307) | [PDF](https://arxiv.org/pdf/2511.20307.pdf)

**ä½œè€…**: Shengqian Li, Ming Gao, Yi Liu, Zuzeng Lin, Feng Wang, Feng Dai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTReFTæ–¹æ³•ä»¥è§£å†³Rectified Flowæ¨¡åž‹åœ¨ä¸€æ­¥å›¾åƒç¿»è¯‘ä¸­çš„æ”¶æ•›é—®é¢˜**

**å…³é”®è¯**: `å›¾åƒç¿»è¯‘` `Rectified Flowæ¨¡åž‹` `ä¸€æ­¥æŽ¨ç†` `å¯¹æŠ—è®­ç»ƒ` `å®žæ—¶åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Rectified Flowæ¨¡åž‹åœ¨å›¾åƒç¿»è¯‘ä¸­ä¾èµ–å¤šæ­¥åŽ»å™ªï¼Œé˜»ç¢å®žæ—¶åº”ç”¨
2. TReFTç›´æŽ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„é¢„æµ‹é€Ÿåº¦ä½œä¸ºè¾“å‡ºï¼Œå®žçŽ°ä¸€æ­¥æŽ¨ç†
3. åœ¨SD3.5å’ŒFLUXç­‰æ¨¡åž‹ä¸Šå¾®è°ƒï¼Œæ€§èƒ½åª²ç¾ŽSOTAä¸”æ”¯æŒå®žæ—¶æŽ¨ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.

