---
layout: default
title: VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction
---

# VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction

**arXiv**: [2511.19971v1](https://arxiv.org/abs/2511.19971) | [PDF](https://arxiv.org/pdf/2511.19971.pdf)

**ä½œè€…**: Yu Hu, Chong Cheng, Sicheng Yu, Xiaoyang Guo, Hao Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VGGT4Dï¼šæŒ–æŽ˜è§†è§‰å‡ ä½•Transformerä¸­çš„è¿åŠ¨çº¿ç´¢ï¼Œç”¨äºŽ4Dåœºæ™¯é‡å»º**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `4Dåœºæ™¯é‡å»º` `åŠ¨æ€åœºæ™¯ç†è§£` `è§†è§‰å‡ ä½•Transformer` `è¿åŠ¨çº¿ç´¢æŒ–æŽ˜` `å…è®­ç»ƒå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ¨æ€åœºæ™¯é‡å»ºçš„å…³é”®æŒ‘æˆ˜åœ¨äºŽå¦‚ä½•æœ‰æ•ˆåˆ†ç¦»åŠ¨æ€ç‰©ä½“å’Œé™æ€èƒŒæ™¯ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨ä¿¡æ¯æˆ–éœ€è¦å¤§é‡ä¼˜åŒ–ã€‚
2. VGGT4Dé€šè¿‡æŒ–æŽ˜VGGTä¸­éšå«çš„åŠ¨æ€çº¿ç´¢ï¼Œåˆ©ç”¨Gramç›¸ä¼¼æ€§å’ŒæŠ•å½±æ¢¯åº¦ç»†åŒ–ï¼Œå®žçŽ°åŠ¨æ€ç‰©ä½“çš„ç²¾ç¡®åˆ†å‰²ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒVGGT4Dåœ¨åŠ¨æ€ç‰©ä½“åˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†é‡å»ºæ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä¸”æ”¯æŒé•¿åºåˆ—æŽ¨ç†ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŠ¨æ€4Dåœºæ™¯é‡å»ºæžå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦ç¨³å¥åœ°å°†åŠ¨æ€ç‰©ä½“ä»Žé™æ€èƒŒæ™¯ä¸­åˆ†ç¦»å‡ºæ¥ã€‚è™½ç„¶åƒVGGTè¿™æ ·çš„3DåŸºç¡€æ¨¡åž‹æä¾›äº†ç²¾ç¡®çš„3Då‡ ä½•ä¿¡æ¯ï¼Œä½†å½“ç§»åŠ¨ç‰©ä½“å æ®ä¸»å¯¼åœ°ä½æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚çŽ°æœ‰çš„4Dæ–¹æ³•é€šå¸¸ä¾èµ–äºŽå¤–éƒ¨å…ˆéªŒã€ç¹é‡çš„åŽä¼˜åŒ–ï¼Œæˆ–è€…éœ€è¦åœ¨4Dæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡æå‡ºäº†VGGT4Dï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æž¶ï¼Œæ‰©å±•äº†3DåŸºç¡€æ¨¡åž‹VGGTï¼Œç”¨äºŽç¨³å¥çš„4Dåœºæ™¯é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºŽä¸€ä¸ªå…³é”®å‘çŽ°ï¼šVGGTçš„å…¨å±€æ³¨æ„åŠ›å±‚å·²ç»éšå¼åœ°ç¼–ç äº†ä¸°å¯Œçš„ã€é€å±‚çš„åŠ¨æ€çº¿ç´¢ã€‚ä¸ºäº†èŽ·å¾—è§£è€¦é™æ€å’ŒåŠ¨æ€å…ƒç´ çš„æŽ©ç ï¼Œæˆ‘ä»¬é€šè¿‡Gramç›¸ä¼¼æ€§æŒ–æŽ˜å’Œæ”¾å¤§å…¨å±€åŠ¨æ€çº¿ç´¢ï¼Œå¹¶åœ¨æ—¶é—´çª—å£å†…èšåˆå®ƒä»¬ã€‚ä¸ºäº†è¿›ä¸€æ­¥é”åŒ–æŽ©ç è¾¹ç•Œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±æŠ•å½±æ¢¯åº¦é©±åŠ¨çš„ç»†åŒ–ç­–ç•¥ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†è¿™äº›ç²¾ç¡®çš„æŽ©ç é›†æˆåˆ°VGGTçš„æ—©æœŸæŽ¨ç†é˜¶æ®µï¼Œæœ‰æ•ˆåœ°å‡è½»äº†è¿åŠ¨å¯¹å§¿æ€ä¼°è®¡å’Œå‡ ä½•é‡å»ºçš„å¹²æ‰°ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠ¨æ€å¯¹è±¡åˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†é‡å»ºæ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å®ƒè¿˜æ”¯æŒå¯¹è¶…è¿‡500å¸§çš„åºåˆ—è¿›è¡Œå•æ¬¡æŽ¨ç†ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŠ¨æ€4Dåœºæ™¯é‡å»ºæ—¨åœ¨ä»Žè§†é¢‘åºåˆ—ä¸­æ¢å¤åœºæ™¯çš„3Då‡ ä½•ç»“æž„ä»¥åŠéšæ—¶é—´å˜åŒ–çš„åŠ¨æ€ç‰©ä½“ã€‚çŽ°æœ‰æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¤§é‡è¿åŠ¨ç‰©ä½“çš„åœºæ™¯æ—¶ï¼Œå¾€å¾€ç”±äºŽåŠ¨æ€ç‰©ä½“ä¸Žé™æ€èƒŒæ™¯çš„æ··æ·†è€Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºŽé¢å¤–çš„å…ˆéªŒçŸ¥è¯†æˆ–éœ€è¦è€—æ—¶çš„åŽå¤„ç†ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVGGT4Dçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨3DåŸºç¡€æ¨¡åž‹VGGTä¸­å·²ç»å­˜åœ¨çš„ã€ä½†æœªè¢«å……åˆ†åˆ©ç”¨çš„åŠ¨æ€çº¿ç´¢ã€‚ä½œè€…å‘çŽ°ï¼ŒVGGTçš„å…¨å±€æ³¨æ„åŠ›å±‚åœ¨ä¸åŒå±‚çº§ä¸Šéšå¼åœ°ç¼–ç äº†åœºæ™¯ä¸­çš„è¿åŠ¨ä¿¡æ¯ã€‚é€šè¿‡æœ‰æ•ˆåœ°æŒ–æŽ˜å’Œæ”¾å¤§è¿™äº›åŠ¨æ€çº¿ç´¢ï¼Œå¯ä»¥å®žçŽ°åŠ¨æ€ç‰©ä½“å’Œé™æ€èƒŒæ™¯çš„è§£è€¦ï¼Œä»Žè€Œæé«˜4Dåœºæ™¯é‡å»ºçš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVGGT4Dæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) åŠ¨æ€çº¿ç´¢æŒ–æŽ˜ä¸Žèšåˆï¼šåˆ©ç”¨GramçŸ©é˜µè®¡ç®—VGGTå„å±‚ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæå–åŠ¨æ€çº¿ç´¢ï¼Œå¹¶åœ¨æ—¶é—´çª—å£å†…è¿›è¡Œèšåˆï¼Œç”Ÿæˆåˆå§‹çš„åŠ¨æ€ç‰©ä½“æŽ©ç ã€‚2) æŽ©ç ç»†åŒ–ï¼šé€šè¿‡æŠ•å½±æ¢¯åº¦é©±åŠ¨çš„ç»†åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–æŽ©ç è¾¹ç•Œï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚3) é›†æˆåˆ°VGGTæŽ¨ç†ï¼šå°†ç»†åŒ–åŽçš„æŽ©ç é›†æˆåˆ°VGGTçš„æ—©æœŸæŽ¨ç†é˜¶æ®µï¼ŒæŠ‘åˆ¶è¿åŠ¨å¹²æ‰°ï¼Œæå‡å§¿æ€ä¼°è®¡å’Œå‡ ä½•é‡å»ºçš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGGT4Dçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶æ— éœ€è®­ç»ƒçš„ç‰¹æ€§ï¼Œä»¥åŠå¯¹çŽ°æœ‰3DåŸºç¡€æ¨¡åž‹VGGTçš„æœ‰æ•ˆåˆ©ç”¨ã€‚ä¸Žéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æˆ–ä¾èµ–å¤–éƒ¨å…ˆéªŒçš„æ–¹æ³•ä¸åŒï¼ŒVGGT4Dç›´æŽ¥ä»ŽVGGTçš„å†…éƒ¨è¡¨ç¤ºä¸­æŒ–æŽ˜åŠ¨æ€çº¿ç´¢ï¼Œå®žçŽ°äº†å¯¹åŠ¨æ€åœºæ™¯çš„é²æ£’é‡å»ºã€‚æ­¤å¤–ï¼ŒæŠ•å½±æ¢¯åº¦é©±åŠ¨çš„æŽ©ç ç»†åŒ–ç­–ç•¥è¿›ä¸€æ­¥æå‡äº†åˆ†å‰²ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŠ¨æ€çº¿ç´¢æŒ–æŽ˜é˜¶æ®µï¼Œä½¿ç”¨GramçŸ©é˜µè®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒå±‚çº§ä¸Šçš„è¿åŠ¨ä¿¡æ¯ã€‚æ—¶é—´çª—å£çš„é•¿åº¦éœ€è¦æ ¹æ®åœºæ™¯çš„è¿åŠ¨é€Ÿåº¦è¿›è¡Œè°ƒæ•´ã€‚æŠ•å½±æ¢¯åº¦ç»†åŒ–ç­–ç•¥åˆ©ç”¨å›¾åƒæ¢¯åº¦ä¿¡æ¯æ¥ä¼˜åŒ–æŽ©ç è¾¹ç•Œï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦å¹³è¡¡åˆ†å‰²ç²¾åº¦å’Œè¾¹ç•Œé”åŒ–ã€‚å°†æŽ©ç é›†æˆåˆ°VGGTæ—©æœŸæŽ¨ç†é˜¶æ®µï¼Œå¯ä»¥æœ‰æ•ˆæŠ‘åˆ¶è¿åŠ¨å¯¹åŽç»­å¤„ç†çš„å½±å“ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

VGGT4Dåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨åŠ¨æ€ç‰©ä½“åˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†é‡å»ºæ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå¯ä»¥ç›´æŽ¥åº”ç”¨äºŽçŽ°æœ‰çš„3DåŸºç¡€æ¨¡åž‹VGGTã€‚æ­¤å¤–ï¼ŒVGGT4Dæ”¯æŒå¯¹è¶…è¿‡500å¸§çš„é•¿åºåˆ—è¿›è¡Œå•æ¬¡æŽ¨ç†ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

VGGT4Dåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£åŠ¨æ€çŽ¯å¢ƒï¼Œä»Žè€Œåšå‡ºæ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå‡†ç¡®çš„4Dåœºæ™¯é‡å»ºå¯ä»¥æé«˜è½¦è¾†å¯¹å‘¨å›´çŽ¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢žå¼ºé©¾é©¶å®‰å…¨æ€§ã€‚åœ¨å¢žå¼ºçŽ°å®žä¸­ï¼ŒVGGT4Då¯ä»¥å®žçŽ°æ›´é€¼çœŸçš„è™šæ‹Ÿç‰©ä½“ä¸ŽçœŸå®žåœºæ™¯çš„äº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.

