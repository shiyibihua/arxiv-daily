---
layout: default
title: VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction
---

# VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction

**arXiv**: [2511.19971v1](https://arxiv.org/abs/2511.19971) | [PDF](https://arxiv.org/pdf/2511.19971.pdf)

**ä½œè€…**: Yu Hu, Chong Cheng, Sicheng Yu, Xiaoyang Guo, Hao Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGGT4Dæ¡†æž¶ï¼Œæ— éœ€è®­ç»ƒæ‰©å±•VGGTä»¥å®žçŽ°é²æ£’4Dåœºæ™¯é‡å»º**

**å…³é”®è¯**: `4Dåœºæ™¯é‡å»º` `åŠ¨æ€ç‰©ä½“åˆ†å‰²` `è§†è§‰å‡ ä½•å˜æ¢å™¨` `æ— è®­ç»ƒæ¡†æž¶` `å§¿æ€ä¼°è®¡` `é•¿è§†é¢‘æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŠ¨æ€4Dåœºæ™¯é‡å»ºä¸­ï¼ŒåŠ¨æ€ç‰©ä½“å¹²æ‰°å¯¼è‡´3DåŸºç¡€æ¨¡åž‹æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šæŒ–æŽ˜VGGTå…¨å±€æ³¨æ„åŠ›å±‚çš„åŠ¨æ€çº¿ç´¢ï¼Œé€šè¿‡Gramç›¸ä¼¼æ€§å’ŒæŠ•å½±æ¢¯åº¦ä¼˜åŒ–æŽ©ç 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼ŒåŠ¨æ€åˆ†å‰²ã€å§¿æ€ä¼°è®¡å’Œé‡å»ºæ€§èƒ½ä¼˜è¶Šï¼Œæ”¯æŒé•¿åºåˆ—å•æ¬¡æŽ¨ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.

