---
layout: default
title: Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization
---

# Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization

**arXiv**: [2511.19937v1](https://arxiv.org/abs/2511.19937) | [PDF](https://arxiv.org/pdf/2511.19937.pdf)

**ä½œè€…**: Peng Zhao, Yu-Hu Yan, Hang Yu, Zhi-Hua Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniGradæ–¹æ³•å®žçŽ°é€šç”¨åœ¨çº¿ä¼˜åŒ–ï¼Œå…¼å…·è‡ªé€‚åº”æ€§å’Œä½Žè®¡ç®—æˆæœ¬ã€‚**

**å…³é”®è¯**: `åœ¨çº¿å‡¸ä¼˜åŒ–` `é€šç”¨é—æ†¾ç•Œ` `æ¢¯åº¦å˜åŒ–è‡ªé€‚åº”` `å…ƒç®—æ³•` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰é€šç”¨åœ¨çº¿å­¦ä¹ æ–¹æ³•ç¼ºä¹å¯¹æ¢¯åº¦å˜åŒ–çš„è‡ªé€‚åº”ï¼Œæ— æ³•å®žçŽ°é—®é¢˜ä¾èµ–çš„é—æ†¾ç•Œã€‚
2. UniGradæ–¹æ³•é€šè¿‡å…ƒç®—æ³•è®¾è®¡ï¼Œè‡ªé€‚åº”æ¢¯åº¦å˜åŒ–ï¼ŒèŽ·å¾—å¼ºå‡¸å’ŒæŒ‡æ•°å‡¹å‡½æ•°çš„å¯¹æ•°é—æ†¾ç•Œã€‚
3. UniGrad++åœ¨ä¿æŒé—æ†¾ç•Œçš„åŒæ—¶ï¼Œå°†æ¯è½®æ¢¯åº¦æŸ¥è¯¢é™è‡³1æ¬¡ï¼Œæå‡è®¡ç®—æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Universal online learning aims to achieve optimal regret guarantees without requiring prior knowledge of the curvature of online functions. Existing methods have established minimax-optimal regret bounds for universal online learning, where a single algorithm can simultaneously attain $\mathcal{O}(\sqrt{T})$ regret for convex functions, $\mathcal{O}(d \log T)$ for exp-concave functions, and $\mathcal{O}(\log T)$ for strongly convex functions, where $T$ is the number of rounds and $d$ is the dimension of the feasible domain. However, these methods still lack problem-dependent adaptivity. In particular, no universal method provides regret bounds that scale with the gradient variation $V_T$, a key quantity that plays a crucial role in applications such as stochastic optimization and fast-rate convergence in games. In this work, we introduce UniGrad, a novel approach that achieves both universality and adaptivity, with two distinct realizations: UniGrad.Correct and UniGrad.Bregman. Both methods achieve universal regret guarantees that adapt to gradient variation, simultaneously attaining $\mathcal{O}(\log V_T)$ regret for strongly convex functions and $\mathcal{O}(d \log V_T)$ regret for exp-concave functions. For convex functions, the regret bounds differ: UniGrad.Correct achieves an $\mathcal{O}(\sqrt{V_T \log V_T})$ bound while preserving the RVU property that is crucial for fast convergence in online games, whereas UniGrad.Bregman achieves the optimal $\mathcal{O}(\sqrt{V_T})$ regret bound through a novel design. Both methods employ a meta algorithm with $\mathcal{O}(\log T)$ base learners, which naturally requires $\mathcal{O}(\log T)$ gradient queries per round. To enhance computational efficiency, we introduce UniGrad++, which retains the regret while reducing the gradient query to just $1$ per round via surrogate optimization. We further provide various implications.

