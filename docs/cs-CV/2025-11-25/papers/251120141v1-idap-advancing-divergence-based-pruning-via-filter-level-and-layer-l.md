---
layout: default
title: IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization
---

# IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization

**arXiv**: [2511.20141v1](https://arxiv.org/abs/2511.20141) | [PDF](https://arxiv.org/pdf/2511.20141.pdf)

**ä½œè€…**: Aleksei Samarin, Artem Nazarenko, Egor Kotenko, Valentin Malykh, Alexander Savelev, Aleksei Toropov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽä¿¡æ¯æµå¼ é‡æ•£åº¦çš„æ»¤æ³¢å™¨ä¸Žå±‚çº§ä¼˜åŒ–æ–¹æ³•ï¼Œå®žçŽ°ç¥žç»ç½‘ç»œåŽ‹ç¼©ã€‚**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œåŽ‹ç¼©` `ä¿¡æ¯æµåˆ†æž` `æ»¤æ³¢å™¨å‰ªæž` `å±‚çº§ä¼˜åŒ–` `å¼ é‡æ•£åº¦` `æ¨¡åž‹éƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¥žç»ç½‘ç»œåœ¨æ»¤æ³¢å™¨å’Œæž¶æž„å±‚é¢å­˜åœ¨å†—ä½™ï¼Œå½±å“éƒ¨ç½²æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¼ é‡æµæ•£åº¦åˆ†æžä¿¡æ¯æµï¼Œåˆ†é˜¶æ®µå‰ªæžå†—ä½™æ»¤æ³¢å™¨å’Œå±‚ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šç§æž¶æž„ä¸Šå®žçŽ°é«˜åŽ‹ç¼©çŽ‡ï¼Œä¿æŒå‡†ç¡®æ€§ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.

