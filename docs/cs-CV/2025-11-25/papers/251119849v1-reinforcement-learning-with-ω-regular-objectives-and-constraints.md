---
layout: default
title: Reinforcement Learning with $Ï‰$-Regular Objectives and Constraints
---

# Reinforcement Learning with $Ï‰$-Regular Objectives and Constraints

**arXiv**: [2511.19849v1](https://arxiv.org/abs/2511.19849) | [PDF](https://arxiv.org/pdf/2511.19849.pdf)

**ä½œè€…**: Dominik Wagner, Leon Witzman, Luke Ong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçº¿æ€§è§„åˆ’çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥åœ¨æ»¡è¶³Ï‰-æ­£åˆ™çº¦æŸä¸‹æœ€å¤§åŒ–ç›®æ ‡æ¦‚çŽ‡ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `Ï‰-æ­£åˆ™ç›®æ ‡` `çº¦æŸä¼˜åŒ–` `çº¿æ€§è§„åˆ’` `æ¨¡åž‹å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ æ ‡é‡å¥–åŠ±éš¾ä»¥è¡¨è¾¾å¤æ‚è¡Œä¸ºç›®æ ‡ï¼Œæ˜“å¯¼è‡´å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚
2. ç»“åˆÏ‰-æ­£åˆ™ç›®æ ‡ä¸Žçº¦æŸï¼Œå¼€å‘æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç¡®ä¿å®‰å…¨ä¸Žä¼˜åŒ–åˆ†ç¦»ã€‚
3. ç®—æ³•åœ¨æžé™ä¸‹ä¿è¯ç­–ç•¥æ»¡è¶³çº¦æŸé˜ˆå€¼ï¼Œå¹¶è½¬åŒ–ä¸ºçº¦æŸå¹³å‡é—®é¢˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $Ï‰$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
>   We address both limitations simultaneously by combining $Ï‰$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $Ï‰$-regular objective while also adhering to $Ï‰$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

