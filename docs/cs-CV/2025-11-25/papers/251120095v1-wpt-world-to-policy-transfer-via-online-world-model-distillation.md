---
layout: default
title: WPT: World-to-Policy Transfer via Online World Model Distillation
---

# WPT: World-to-Policy Transfer via Online World Model Distillation

**arXiv**: [2511.20095v1](https://arxiv.org/abs/2511.20095) | [PDF](https://arxiv.org/pdf/2511.20095.pdf)

**ä½œè€…**: Guangfeng Jiang, Yueru Luo, Jun Liu, Yi Huang, Yiyao Zhu, Zhan Qu, Dave Zhenyu Chen, Bingbing Liu, Xu Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWPTè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åœ¨çº¿ä¸–ç•Œæ¨¡åž‹è’¸é¦æå‡ç­–ç•¥æ€§èƒ½ä¸ŽæŽ¨ç†é€Ÿåº¦ã€‚**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡åž‹` `ç­–ç•¥è’¸é¦` `åœ¨çº¿è’¸é¦` `å¥–åŠ±æ¨¡åž‹` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ä¸–ç•Œæ¨¡åž‹å­˜åœ¨è¿è¡Œæ—¶è€¦åˆæˆ–ä¾èµ–ç¦»çº¿å¥–åŠ±ï¼Œå¯¼è‡´æŽ¨ç†å¼€é”€å¤§æˆ–ä¼˜åŒ–å›°éš¾ã€‚
2. WPTä½¿ç”¨å¯è®­ç»ƒå¥–åŠ±æ¨¡åž‹å’Œç­–ç•¥è’¸é¦ï¼Œå°†ä¸–ç•ŒçŸ¥è¯†è½¬ç§»åˆ°è½»é‡å­¦ç”Ÿç­–ç•¥ã€‚
3. å®žéªŒæ˜¾ç¤ºWPTåœ¨å¼€çŽ¯å’Œé—­çŽ¯åŸºå‡†ä¸Šå®žçŽ°SOTAæ€§èƒ½ï¼ŒæŽ¨ç†é€Ÿåº¦æå‡4.9å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent years have witnessed remarkable progress in world models, which primarily aim to capture the spatio-temporal correlations between an agent's actions and the evolving environment. However, existing approaches often suffer from tight runtime coupling or depend on offline reward signals, resulting in substantial inference overhead or hindering end-to-end optimization. To overcome these limitations, we introduce WPT, a World-to-Policy Transfer training paradigm that enables online distillation under the guidance of an end-to-end world model. Specifically, we develop a trainable reward model that infuses world knowledge into a teacher policy by aligning candidate trajectories with the future dynamics predicted by the world model. Subsequently, we propose policy distillation and world reward distillation to transfer the teacher's reasoning ability into a lightweight student policy, enhancing planning performance while preserving real-time deployability. Extensive experiments on both open-loop and closed-loop benchmarks show that our WPT achieves state-of-the-art performance with a simple policy architecture: it attains a 0.11 collision rate (open-loop) and achieves a 79.23 driving score (closed-loop) surpassing both world-model-based and imitation-learning methods in accuracy and safety. Moreover, the student sustains up to 4.9x faster inference, while retaining most of the gains.

