---
layout: default
title: Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning
---

# Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning

**arXiv**: [2511.20613v1](https://arxiv.org/abs/2511.20613) | [PDF](https://arxiv.org/pdf/2511.20613.pdf)

**ä½œè€…**: Panayiotis Danassis, Naman Goel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä»£ç†æŽ¨ç†åŸºå‡†è¯„ä¼°LLMä»£ç ç”Ÿæˆï¼Œå‘çŽ°äººç±»ä»£ç åœ¨ç‰©æµä¼˜åŒ–ä¸­æ›´ä¼˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `ä»£ç ç”ŸæˆåŸºå‡†` `å¤šä»£ç†ç³»ç»Ÿ` `ç‰©æµä¼˜åŒ–` `æˆ˜ç•¥è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†ä½Žä¼°LLMåœ¨è§„åˆ’ä¸Žæˆ˜ç•¥äº¤äº’ä»£ç ç”Ÿæˆä¸­çš„èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç‰©æµä¼˜åŒ–é—®é¢˜æž„å»ºå¤šä»£ç†åŸºå‡†ï¼Œç»“åˆæ‹å–ä¸Žè·¯ç”±
3. å®žéªŒæ•ˆæžœï¼šäººç±»ä»£ç ä»£ç†åœ¨æ¯”èµ›ä¸­è¡¨çŽ°ä¼˜äºŽå¤šæ•°LLMç”Ÿæˆä»£ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.

