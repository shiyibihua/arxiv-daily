---
layout: default
title: Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization
---

# Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization

**arXiv**: [2511.20258v1](https://arxiv.org/abs/2511.20258) | [PDF](https://arxiv.org/pdf/2511.20258.pdf)

**ä½œè€…**: Xiaohan Wang, Zhangtao Cheng, Ting Zhong, Leiting Chen, Fan Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMBCDä»¥è§£å†³å¤šæ¨¡æ€åŸŸæ³›åŒ–ä¸­æƒé‡å¹³å‡å¯¼è‡´çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸŸæ³›åŒ–` `æƒé‡å¹³å‡` `æ¨¡æ€ä¸å¹³è¡¡` `åä½œè’¸é¦` `æ¢¯åº¦ä¸€è‡´æ€§` `è·¨æ¨¡æ€äº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæƒé‡å¹³å‡åœ¨å¤šæ¨¡æ€åŸŸæ³›åŒ–ä¸­å› æ¨¡æ€ä¼˜åŒ–é€Ÿåº¦å·®å¼‚å¯¼è‡´åå‘å¿«é€Ÿæ¨¡æ€ï¼ŒæŠ‘åˆ¶äº’è¡¥æ¨¡æ€èžåˆ
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è‡ªé€‚åº”æ¨¡æ€ä¸¢å¼ƒã€æ¢¯åº¦ä¸€è‡´æ€§çº¦æŸå’ŒåŸºäºŽæƒé‡å¹³å‡çš„è·¨æ¨¡æ€è’¸é¦æ¥å¹³è¡¡æ¨¡æ€å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MMDGåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMBCDä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡æœªè§åŸŸçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.

