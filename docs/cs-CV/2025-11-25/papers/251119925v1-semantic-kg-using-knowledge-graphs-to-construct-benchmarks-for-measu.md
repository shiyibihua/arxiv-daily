---
layout: default
title: Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity
---

# Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity

**arXiv**: [2511.19925v1](https://arxiv.org/abs/2511.19925) | [PDF](https://arxiv.org/pdf/2511.19925.pdf)

**ä½œè€…**: Qiyao Wei, Edward Morrell, Lea Goetz, Mihaela van der Schaar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçŸ¥è¯†å›¾è°±çš„åŸºå‡†æž„å»ºæ–¹æ³•ï¼Œä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡åž‹è¾“å‡ºçš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚**

**å…³é”®è¯**: `è¯­ä¹‰ç›¸ä¼¼æ€§è¯„ä¼°` `çŸ¥è¯†å›¾è°±åŸºå‡†` `å¤§è¯­è¨€æ¨¡åž‹è¾“å‡º` `å¤šé¢†åŸŸæ•°æ®é›†` `LLMä½œä¸ºè¯„åˆ¤è€…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è¯­ä¹‰ç›¸ä¼¼æ€§æ–¹æ³•å¯èƒ½åå‘å¥æ³•è€Œéžè¯­ä¹‰ï¼Œä¸”åŸºå‡†ç”Ÿæˆæˆæœ¬é«˜ã€å¯ç”¨æ€§æœ‰é™ã€‚
2. åˆ©ç”¨çŸ¥è¯†å›¾è°±ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼æˆ–ä¸ç›¸ä¼¼çš„è¯­å¥å¯¹ï¼Œå¹¶åˆ†ç±»ä¸ç›¸ä¼¼å­ç±»åž‹ã€‚
3. åœ¨å¤šä¸ªé¢†åŸŸç”Ÿæˆæ•°æ®é›†ï¼Œæ¯”è¾ƒæ–¹æ³•æ€§èƒ½ï¼Œå‘çŽ°æ— æ–¹æ³•å§‹ç»ˆæœ€ä¼˜ï¼Œå½±å“LLMä½œä¸ºè¯„åˆ¤è€…çš„ä½¿ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

