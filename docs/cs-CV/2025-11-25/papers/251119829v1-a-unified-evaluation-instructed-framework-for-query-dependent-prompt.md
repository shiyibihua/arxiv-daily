---
layout: default
title: A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization
---

# A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization

**arXiv**: [2511.19829v1](https://arxiv.org/abs/2511.19829) | [PDF](https://arxiv.org/pdf/2511.19829.pdf)

**ä½œè€…**: Ke Chen, Yifeng Wang, Hassan Almosapeeh, Haohan Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€è¯„ä¼°æŒ‡å¯¼æ¡†æž¶ä»¥ä¼˜åŒ–æŸ¥è¯¢ä¾èµ–æç¤ºï¼Œè§£å†³åŠ¨æ€åœºæ™¯ä¸­æç¤ºè´¨é‡è¯„ä¼°ä¸Žä¼˜åŒ–é—®é¢˜ã€‚**

**å…³é”®è¯**: `æç¤ºä¼˜åŒ–` `æŸ¥è¯¢ä¾èµ–ä¼˜åŒ–` `è¯„ä¼°æ¡†æž¶` `å…æ‰§è¡Œè¯„ä¼°å™¨` `å¯è§£é‡ŠAI` `æ¨¡åž‹æ— å…³æ”¹è¿›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæç¤ºè´¨é‡ç¼ºä¹ç»Ÿä¸€ç³»ç»Ÿå®šä¹‰ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–ä¸ç¨³å®šåé¦ˆï¼Œå¯¼è‡´ä¼˜åŒ–ä¿¡å·å¼±ä¸”ä¸å¯è§£é‡Šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºç«‹æ€§èƒ½å¯¼å‘çš„è¯„ä¼°æ¡†æž¶ï¼Œå¼€å‘å…æ‰§è¡Œè¯„ä¼°å™¨é¢„æµ‹å¤šç»´è´¨é‡åˆ†æ•°ï¼ŒæŒ‡å¯¼å¯è§£é‡Šçš„æŸ¥è¯¢ä¾èµ–ä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…«ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªéª¨å¹²æ¨¡åž‹ä¸Šï¼Œè¯„ä¼°å™¨é¢„æµ‹å‡†ç¡®çŽ‡æœ€é«˜ï¼Œä¼˜åŒ–æ–¹æ³•è¶…è¶Šé™æ€å’ŒæŸ¥è¯¢ä¾èµ–åŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

