---
layout: default
title: A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control
---

# A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control

**arXiv**: [2511.20401v1](https://arxiv.org/abs/2511.20401) | [PDF](https://arxiv.org/pdf/2511.20401.pdf)

**ä½œè€…**: Jiawei Lin, Guanlong Jiao, Jianjin Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMultiIDæ–¹æ³•ä»¥æ— è®­ç»ƒæ–¹å¼è§£å†³å¤šIDå®šåˆ¶ä¸­çš„å¤åˆ¶ç²˜è´´å’Œæ–‡æœ¬æŽ§åˆ¶é—®é¢˜**

**å…³é”®è¯**: `å¤šIDå®šåˆ¶` `æ³¨æ„åŠ›è°ƒæ•´` `ç©ºé—´æŽ§åˆ¶` `è®­ç»ƒå…è´¹æ–¹æ³•` `å›¾åƒç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šIDå®šåˆ¶å­˜åœ¨å¤åˆ¶ç²˜è´´å¯¼è‡´è´¨é‡ä½Žå’Œæ–‡æœ¬æŽ§åˆ¶æ€§å·®çš„é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨IDè§£è€¦äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œç©ºé—´æŽ§åˆ¶ç­–ç•¥å¢žå¼ºç”Ÿæˆè´¨é‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨IDBenchåŸºå‡†ä¸Šè¡¨çŽ°ä¼˜äºŽæˆ–å¯æ¯”è®­ç»ƒæ–¹æ³•ï¼ŒéªŒè¯æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.

