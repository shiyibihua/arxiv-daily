---
layout: default
title: WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving
---

# WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving

**arXiv**: [2511.20022v1](https://arxiv.org/abs/2511.20022) | [PDF](https://arxiv.org/pdf/2511.20022.pdf)

**ä½œè€…**: Seungjun Yu, Seonho Lee, Namho Kim, Jaeyo Shin, Junsung Park, Wonjeong Ryu, Raehyuk Jung, Hyunjung Shim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWaymoQAæ•°æ®é›†ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­å®‰å…¨å…³é”®æŽ¨ç†çš„æŒ‘æˆ˜**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å®‰å…¨å…³é”®æŽ¨ç†` `å¤šè§†å›¾è¾“å…¥` `è§†è§‰é—®ç­”æ•°æ®é›†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `é£Žé™©è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨é©¾é©¶ä¸­å®‰å…¨å…³é”®åœºæ™¯çš„é«˜å±‚æŽ¨ç†ï¼Œé¿å…ä¸€ä¸ªé£Žé™©å¯èƒ½å¼•å‘å¦ä¸€ä¸ªé£Žé™©
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤šè§†å›¾è¾“å…¥å®šä¹‰å®‰å…¨å…³é”®æŽ¨ç†ä»»åŠ¡ï¼Œå¹¶åˆ†è§£ä¸ºä¸¤é˜¶æ®µå¤„ç†
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¾®è°ƒåŽå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å®‰å…¨å…³é”®åœºæ™¯æŽ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in multimodal large language models (MLLMs) have shown strong understanding of driving scenes, drawing interest in their application to autonomous driving. However, high-level reasoning in safety-critical scenarios, where avoiding one traffic risk can create another, remains a major challenge. Such reasoning is often infeasible with only a single front view and requires a comprehensive view of the environment, which we achieve through multi-view inputs. We define Safety-Critical Reasoning as a new task that leverages multi-view inputs to address this challenge. Then, we distill Safety-Critical Reasoning into two stages: first resolve the immediate risk, then mitigate the decision-induced downstream risks. To support this, we introduce WaymoQA, a dataset of 35,000 human-annotated question-answer pairs covering complex, high-risk driving scenarios. The dataset includes multiple-choice and open-ended formats across both image and video modalities. Experiments reveal that existing MLLMs underperform in safety-critical scenarios compared to normal scenes, but fine-tuning with WaymoQA significantly improves their reasoning ability, highlighting the effectiveness of our dataset in developing safer and more reasoning-capable driving agents.

