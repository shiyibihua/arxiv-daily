---
layout: default
title: DINO-Tok: Adapting DINO for Visual Tokenizers
---

# DINO-Tok: Adapting DINO for Visual Tokenizers

**arXiv**: [2511.20565v1](https://arxiv.org/abs/2511.20565) | [PDF](https://arxiv.org/pdf/2511.20565.pdf)

**ä½œè€…**: Mingkai Jia, Mingxiao Li, Liaoyuan Fan, Tianxing Shi, Jiaxin Guo, Zeming Li, Xiaoyang Guo, Xiao-Xiao Long, Qian Zhang, Ping Tan, Wei Yin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDINO-Tokè§†è§‰åˆ†è¯å™¨ï¼Œç»“åˆæµ…å±‚ä¸Žæ·±å±‚ç‰¹å¾ä»¥æ”¹è¿›è¯­ä¹‰ä¸Žé‡å»ºå¹³è¡¡ã€‚**

**å…³é”®è¯**: `è§†è§‰åˆ†è¯å™¨` `DINOæ¨¡åž‹` `å‘é‡é‡åŒ–` `æ½œåœ¨ç”Ÿæˆæ¨¡åž‹` `å›¾åƒé‡å»º` `PCAé‡åŠ æƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰åˆ†è¯å™¨è®­ç»ƒå›°éš¾ï¼Œéš¾ä»¥å¹³è¡¡è¯­ä¹‰è¡¨ç¤ºä¸Žé‡å»ºä¿çœŸåº¦ã€‚
2. é›†æˆDINOçš„å±‚æ¬¡ç‰¹å¾ï¼Œä½¿ç”¨å…¨å±€PCAé‡åŠ æƒç¨³å®šå‘é‡é‡åŒ–ã€‚
3. åœ¨ImageNetä¸Šå®žçŽ°SOTAé‡å»ºæ€§èƒ½ï¼ŒPSNRè¾¾28.54å’Œ23.98ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.

