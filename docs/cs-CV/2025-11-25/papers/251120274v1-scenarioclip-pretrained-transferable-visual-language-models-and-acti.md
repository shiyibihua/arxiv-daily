---
layout: default
title: ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis
---

# ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis

**arXiv**: [2511.20274v1](https://arxiv.org/abs/2511.20274) | [PDF](https://arxiv.org/pdf/2511.20274.pdf)

**ä½œè€…**: Advik Sinha, Saurabh Atreya, Aashutosh A, Sk Aziz Ali, Abhijit Das

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºScenarioCLIPæ¨¡åž‹å’ŒAction-Genomeæ•°æ®é›†ï¼Œç”¨äºŽè‡ªç„¶åœºæ™¯çš„å¤šå¯¹è±¡å…³ç³»åˆ†æž**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `åœºæ™¯åˆ†æž` `è·¨æ¨¡æ€æ£€ç´¢` `å…³ç³»å»ºæ¨¡` `æ•°æ®é›†æž„å»º` `é›¶æ ·æœ¬å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰CLIPæ¨¡åž‹ç¼ºä¹å¯¹åœºæ™¯ä¸­å¤šå¯¹è±¡å’ŒåŠ¨ä½œå…³ç³»çš„æ˜¾å¼å»ºæ¨¡
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ–‡æœ¬ã€æŽ¥åœ°å…³ç³»å’Œå›¾åƒè¾“å…¥ï¼Œé¢„è®­ç»ƒå¹¶å¾®è°ƒä»¥æå‡è·¨æ¨¡æ€æ£€ç´¢èƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§åœºæ™¯ä»»åŠ¡ä¸­å±•ç¤ºé›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½ï¼Œä¼˜äºŽåŸºçº¿æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Until recently, the general corpus of CLIP-type fundamental models has widely explored either the retrieval of short descriptions or the classification of objects in the scene as SINGLE-object image classification task. The same holds for retrieving the image embedding (image retrieval task) given a text prompt. However, real-world scene images exhibit rich compositional structure involving multiple objects and actions. The latest methods in the CLIP-based literature improve class-level discrimination by mining harder negative image-text pairs and by refining permanent text prompts, often using LLMs. However, these improvements remain confined to predefined class lists and do not explicitly model relational or compositional structure. PyramidCLIP partially addresses this gap by aligning global and local visual features, yet it still lacks explicit modeling of inter-object relations. Hence, to further leverage this aspect for scene analysis, the proposed ScenarioCLIP model accepts input texts, grounded relations, and input images, along with focused regions highlighting relations. The proposed model is pretrained on curated scenario data, and finetuned for specialized downstream tasks, such as cross-modal retrieval and fine-grained visual understanding tasks. To address the lack of domain-specific datasets, we generate a novel dataset by extending image-text pairs from existing diverse indoor and outdoor scenario datasets that are publicly available. We used a pipeline of existing language models to ground action, object, and relations, filled by manual and automatic curation. We established a comprehensive benchmark for several scenario-based tasks and compared it with many baseline methods. ScenarioCLIP demonstrates robust zero-shot and finetune performance on various domain-specific tasks. Our code and dataset are available at https://github.com/scenario-clip/ScenarioCLIP

