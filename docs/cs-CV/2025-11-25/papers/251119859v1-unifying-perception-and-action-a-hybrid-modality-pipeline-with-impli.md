---
layout: default
title: Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation
---

# Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation

**arXiv**: [2511.19859v1](https://arxiv.org/abs/2511.19859) | [PDF](https://arxiv.org/pdf/2511.19859.pdf)

**ä½œè€…**: Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, Sanglu Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVITAæ¡†æž¶ä»¥è§£å†³è§†è§‰ä¸ŽåŠ¨ä½œæ¨¡æ€å·®è·åŠè®­ç»ƒä¸ç¨³å®šé—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `éšå¼è§†è§‰æ€ç»´é“¾` `æœºå™¨äººåŠ¨ä½œç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `è½¨è¿¹å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è§‚å¯Ÿä¸Žä½Žçº§åŠ¨ä½œé—´å­˜åœ¨æ¨¡æ€å·®è·ï¼Œè§†è§‰é¢„æµ‹ä¸ŽåŠ¨ä½œç”Ÿæˆç›®æ ‡å†²çªå¯¼è‡´è®­ç»ƒä¸ç¨³å®š
2. æ–¹æ³•è¦ç‚¹ï¼šå­¦ä¹ å…±äº«ç¦»æ•£æ½œç©ºé—´ï¼Œé€šè¿‡éšå¼è§†è§‰æ€ç»´é“¾è”åˆè§£ç æœªæ¥å¸§é¢„æµ‹å’Œæœºå™¨äººåŠ¨ä½œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CALVINç­‰åŸºå‡†ä¸Šæå‡9.6%-14.5%ï¼ŒçœŸå®žä¸–ç•Œä»»åŠ¡å¹³å‡æˆåŠŸçŽ‡80.5%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\%, 9.6\% and 12.1\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.

