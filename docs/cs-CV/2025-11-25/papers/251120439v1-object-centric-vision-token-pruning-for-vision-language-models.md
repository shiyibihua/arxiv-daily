---
layout: default
title: Object-Centric Vision Token Pruning for Vision Language Models
---

# Object-Centric Vision Token Pruning for Vision Language Models

**arXiv**: [2511.20439v1](https://arxiv.org/abs/2511.20439) | [PDF](https://arxiv.org/pdf/2511.20439.pdf)

**ä½œè€…**: Guangyuan Li, Rongzhen Zhao, Jinhong Deng, Yanbo Wang, Joni Pajarinen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOC-VTPä»¥ç›´æŽ¥é€‰æ‹©ä»£è¡¨æ€§è§†è§‰ä»¤ç‰Œï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†æ•ˆçŽ‡**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `å¯¹è±¡ä¸­å¿ƒå‰ªæž` `æŽ¨ç†æ•ˆçŽ‡` `é‡æž„è¯¯å·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­è§†è§‰ä»¤ç‰Œæ•°é‡å¤šä½†ä¿¡æ¯åˆ†æ•£ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™
2. OC-VTPé€šè¿‡è½»é‡é¢„è®­ç»ƒå¯¹è±¡ä¸­å¿ƒå‰ªæžå™¨ï¼Œæœ€å°åŒ–é‡æž„è¯¯å·®é€‰æ‹©ä»¤ç‰Œ
3. å®žéªŒæ˜¾ç¤ºOC-VTPåœ¨å¤šç§å‰ªæžæ¯”ä¸‹ä¿æŒæœ€é«˜æŽ¨ç†ç²¾åº¦ï¼Œæ— éœ€å¾®è°ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.

