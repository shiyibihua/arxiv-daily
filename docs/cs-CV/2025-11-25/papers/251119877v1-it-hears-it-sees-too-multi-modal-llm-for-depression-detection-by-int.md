---
layout: default
title: It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models
---

# It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models

**arXiv**: [2511.19877v1](https://arxiv.org/abs/2511.19877) | [PDF](https://arxiv.org/pdf/2511.19877.pdf)

**ä½œè€…**: Xiangyu Zhao, Yaling Shen, Yiwen Jiang, Zimu Wang, Jiahe Liu, Maxmartwell H Cheng, Guilherme C Oliveira, Robert Desimone, Dominic Dwyer, Zongyuan Ge

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€LLMæ¡†æž¶ï¼Œé€šè¿‡éŸ³è§†é¢‘ç‰¹å¾å¯¹é½æ”¹è¿›æŠ‘éƒç—‡æ£€æµ‹**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æŠ‘éƒç—‡æ£€æµ‹` `éŸ³è§†é¢‘ç‰¹å¾å¯¹é½` `å¿ƒç†å¥åº·è¯„ä¼°` `æ—¶é—´æˆ³çº§å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸLLMæ— æ³•å¤„ç†éŸ³é¢‘å’Œè§†è§‰ä¸­çš„éžè¯­è¨€çº¿ç´¢ï¼Œé™åˆ¶å¿ƒç†å¥åº·è¯„ä¼°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¢žå¼ºéŸ³é¢‘è¯­è¨€æ¨¡åž‹ï¼Œé›†æˆè§†è§‰ç†è§£ï¼Œå®žçŽ°æ—¶é—´æˆ³çº§éŸ³è§†é¢‘ç‰¹å¾å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨DAIC-WoZæ•°æ®é›†ä¸Šä¼˜äºŽå•æ¨¡æ€å’Œå…ˆå‰å¤šæ¨¡æ€æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.

