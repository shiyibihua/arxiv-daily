---
layout: default
title: Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving
---

# Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving

**arXiv**: [2511.19912v1](https://arxiv.org/abs/2511.19912) | [PDF](https://arxiv.org/pdf/2511.19912.pdf)

**ä½œè€…**: Dapeng Zhang, Zhenlong Yuan, Zhangquan Chen, Chih-Ting Liao, Yinda Chen, Fei Shen, Qingguo Zhou, Tat-Seng Chua

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReasoning-VLAä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­æŽ¨ç†æ•ˆçŽ‡ä½Žå’Œæ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æŽ¨ç†å¢žå¼º` `åŠ¨ä½œè½¨è¿¹ç”Ÿæˆ` `æ³›åŒ–èƒ½åŠ›` `å¿«é€ŸæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æŽ¨ç†æ•ˆçŽ‡ä½Žä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°åœºæ™¯å’Œè½¦è¾†é…ç½®
2. ä½¿ç”¨å¯å­¦ä¹ åŠ¨ä½œæŸ¥è¯¢ä¸ŽæŽ¨ç†å¢žå¼ºç‰¹å¾å¹¶è¡Œç”Ÿæˆè¿žç»­åŠ¨ä½œè½¨è¿¹
3. æ•´åˆå¤šæ•°æ®é›†è®­ç»ƒï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°æœ€ä¼˜æ€§èƒ½å’Œå¿«é€ŸæŽ¨ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.

