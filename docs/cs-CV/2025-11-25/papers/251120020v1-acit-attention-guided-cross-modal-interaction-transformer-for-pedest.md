---
layout: default
title: ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction
---

# ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction

**arXiv**: [2511.20020v1](https://arxiv.org/abs/2511.20020) | [PDF](https://arxiv.org/pdf/2511.20020.pdf)

**ä½œè€…**: Yuanzhe Li, Steffen MÃ¼ller

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºACITæ¨¡åž‹ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè·¨æ¨¡æ€äº¤äº’Transformeræå‡è¡Œäººè¿‡è¡—æ„å›¾é¢„æµ‹ç²¾åº¦ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸Žååº” (Interaction & Reaction)**

**å…³é”®è¯**: `è¡Œäººæ„å›¾é¢„æµ‹` `è·¨æ¨¡æ€äº¤äº’` `æ³¨æ„åŠ›æœºåˆ¶` `Transformer` `è‡ªåŠ¨é©¾é©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è¡Œäººæ„å›¾é¢„æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæå–å’Œæ•´åˆæ¥è‡ªä¸åŒç±»åž‹æ•°æ®çš„äº’è¡¥ä¿¡æ¯ã€‚
2. ACITæ¨¡åž‹é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ï¼Œå¹¶åˆ©ç”¨Transformeræ•èŽ·æ—¶åºä¾èµ–å…³ç³»ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒACITåœ¨JAADæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¯æ˜Žäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºŽæ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼çš„è·¨æ¨¡æ€äº¤äº’Transformer (ACIT) ç”¨äºŽè¡Œäººè¿‡è¡—æ„å›¾é¢„æµ‹ã€‚ACITåˆ©ç”¨å…­ç§è§†è§‰å’Œè¿åŠ¨æ¨¡æ€æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬åˆ†ä¸ºä¸‰ä¸ªäº¤äº’å¯¹ï¼š(1) å…¨å±€è¯­ä¹‰åœ°å›¾å’Œå…¨å±€å…‰æµï¼Œ(2) å±€éƒ¨RGBå›¾åƒå’Œå±€éƒ¨å…‰æµï¼Œ(3) è‡ªè½¦é€Ÿåº¦å’Œè¡Œäººè¾¹ç•Œæ¡†ã€‚åœ¨æ¯ä¸ªè§†è§‰äº¤äº’å¯¹ä¸­ï¼ŒåŒè·¯å¾„æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å†…æ¨¡æ€è‡ªæ³¨æ„åŠ›å¢žå¼ºä¸»è¦æ¨¡æ€ä¸­çš„æ˜¾è‘—åŒºåŸŸï¼Œå¹¶é€šè¿‡å…‰æµå¼•å¯¼çš„æ³¨æ„åŠ›ä¿ƒè¿›ä¸Žè¾…åŠ©æ¨¡æ€ï¼ˆå³å…‰æµï¼‰çš„æ·±åº¦äº¤äº’ã€‚åœ¨è¿åŠ¨äº¤äº’å¯¹ä¸­ï¼Œé‡‡ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æ¥å»ºæ¨¡è·¨æ¨¡æ€åŠ¨æ€ï¼Œä»Žè€Œæœ‰æ•ˆæå–äº’è¡¥çš„è¿åŠ¨ç‰¹å¾ã€‚é™¤äº†æˆå¯¹äº¤äº’ä¹‹å¤–ï¼Œå¤šæ¨¡æ€ç‰¹å¾èžåˆæ¨¡å—è¿›ä¸€æ­¥ä¿ƒè¿›æ¯ä¸ªæ—¶é—´æ­¥çš„è·¨æ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºŽTransformerçš„æ—¶åºç‰¹å¾èšåˆæ¨¡å—æ¥æ•èŽ·åºåˆ—ä¾èµ–æ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒACITä¼˜äºŽæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨JAADbehå’ŒJAADallæ•°æ®é›†ä¸Šåˆ†åˆ«å®žçŽ°äº†70%å’Œ89%çš„å‡†ç¡®çŽ‡ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèžç ”ç©¶ï¼Œä»¥ç ”ç©¶ACITä¸åŒæ¨¡å—çš„è´¡çŒ®ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¡Œäººè¿‡è¡—æ„å›¾é¢„æµ‹æ˜¯è‡ªåŠ¨é©¾é©¶çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨å‡å°‘è¡Œäººç›¸å…³çš„äº¤é€šäº‹æ•…ã€‚çŽ°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèžåˆæ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆå¦‚è§†è§‰å’Œè¿åŠ¨ï¼‰çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦å—é™ã€‚å°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œå¦‚ä½•å‡†ç¡®æ•æ‰è¡Œäººè¡Œä¸ºçš„æ—¶åºä¾èµ–æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šACITçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼è·¨æ¨¡æ€ç‰¹å¾çš„äº¤äº’ï¼Œå¹¶åˆ©ç”¨Transformeræ¨¡åž‹æ•èŽ·æ—¶åºä¾èµ–å…³ç³»ã€‚é€šè¿‡å°†è§†è§‰å’Œè¿åŠ¨æ¨¡æ€è¿›è¡Œé…å¯¹ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡åž‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå–å’Œèžåˆä¸åŒæ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ã€‚Transformeræ¨¡åž‹åˆ™ç”¨äºŽå»ºæ¨¡è¡Œäººè¡Œä¸ºçš„æ—¶åºåŠ¨æ€ï¼Œä»Žè€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šACITçš„æ•´ä½“æž¶æž„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **è·¨æ¨¡æ€äº¤äº’æ¨¡å—**ï¼šå°†å…­ç§æ¨¡æ€æ•°æ®åˆ†ä¸ºä¸‰ä¸ªäº¤äº’å¯¹ï¼Œåˆ†åˆ«æ˜¯å…¨å±€è¯­ä¹‰åœ°å›¾å’Œå…¨å±€å…‰æµã€å±€éƒ¨RGBå›¾åƒå’Œå±€éƒ¨å…‰æµã€è‡ªè½¦é€Ÿåº¦å’Œè¡Œäººè¾¹ç•Œæ¡†ã€‚2) **åŒè·¯å¾„æ³¨æ„åŠ›æœºåˆ¶**ï¼šåœ¨è§†è§‰äº¤äº’å¯¹ä¸­ï¼Œåˆ©ç”¨åŒè·¯å¾„æ³¨æ„åŠ›æœºåˆ¶å¢žå¼ºä¸»è¦æ¨¡æ€çš„æ˜¾è‘—åŒºåŸŸï¼Œå¹¶é€šè¿‡å…‰æµå¼•å¯¼çš„æ³¨æ„åŠ›ä¿ƒè¿›ä¸Žè¾…åŠ©æ¨¡æ€çš„æ·±åº¦äº¤äº’ã€‚3) **è·¨æ¨¡æ€æ³¨æ„åŠ›**ï¼šåœ¨è¿åŠ¨äº¤äº’å¯¹ä¸­ï¼Œé‡‡ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æ¥å»ºæ¨¡è·¨æ¨¡æ€åŠ¨æ€ã€‚4) **å¤šæ¨¡æ€ç‰¹å¾èžåˆæ¨¡å—**ï¼šè¿›ä¸€æ­¥ä¿ƒè¿›æ¯ä¸ªæ—¶é—´æ­¥çš„è·¨æ¨¡æ€äº¤äº’ã€‚5) **Transformeræ—¶åºç‰¹å¾èšåˆæ¨¡å—**ï¼šæ•èŽ·åºåˆ—ä¾èµ–æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šACITçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶æ³¨æ„åŠ›å¼•å¯¼çš„è·¨æ¨¡æ€äº¤äº’æœºåˆ¶ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒACITèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå–å’Œèžåˆä¸åŒæ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œä»Žè€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒACITè¿˜å¼•å…¥äº†Transformeræ¨¡åž‹æ¥æ•èŽ·è¡Œäººè¡Œä¸ºçš„æ—¶åºä¾èµ–æ€§ï¼Œè¿›ä¸€æ­¥æå‡äº†é¢„æµ‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§†è§‰äº¤äº’å¯¹ä¸­ï¼ŒåŒè·¯å¾„æ³¨æ„åŠ›æœºåˆ¶åŒ…å«å†…æ¨¡æ€è‡ªæ³¨æ„åŠ›å’Œå…‰æµå¼•å¯¼çš„æ³¨æ„åŠ›ã€‚å†…æ¨¡æ€è‡ªæ³¨æ„åŠ›ç”¨äºŽå¢žå¼ºä¸»è¦æ¨¡æ€çš„æ˜¾è‘—åŒºåŸŸï¼Œå…‰æµå¼•å¯¼çš„æ³¨æ„åŠ›åˆ™ç”¨äºŽä¿ƒè¿›ä¸Žå…‰æµæ¨¡æ€çš„äº¤äº’ã€‚åœ¨è¿åŠ¨äº¤äº’å¯¹ä¸­ï¼Œè·¨æ¨¡æ€æ³¨æ„åŠ›é‡‡ç”¨æ ‡å‡†çš„Transformeræ³¨æ„åŠ›æœºåˆ¶ã€‚Transformeræ—¶åºç‰¹å¾èšåˆæ¨¡å—é‡‡ç”¨å¤šå±‚Transformerç¼–ç å™¨ç»“æž„ï¼Œç”¨äºŽæ•èŽ·æ—¶åºä¾èµ–æ€§ã€‚æŸå¤±å‡½æ•°æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ACITæ¨¡åž‹åœ¨JAADbehæ•°æ®é›†ä¸Šè¾¾åˆ°äº†70%çš„å‡†ç¡®çŽ‡ï¼Œåœ¨JAADallæ•°æ®é›†ä¸Šè¾¾åˆ°äº†89%çš„å‡†ç¡®çŽ‡ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ¶ˆèžå®žéªŒè¡¨æ˜Žï¼Œå„ä¸ªæ¨¡å—éƒ½å¯¹æœ€ç»ˆæ€§èƒ½æœ‰è´¡çŒ®ï¼ŒéªŒè¯äº†ACITæ¨¡åž‹çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œæé«˜è½¦è¾†å¯¹è¡Œäººè¿‡è¡—æ„å›¾çš„é¢„æµ‹èƒ½åŠ›ï¼Œä»Žè€Œå‡å°‘äº¤é€šäº‹æ•…ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯åº”ç”¨äºŽæ™ºèƒ½ç›‘æŽ§ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œæå‡ç³»ç»Ÿå¯¹è¡Œäººè¡Œä¸ºçš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®žé™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.

