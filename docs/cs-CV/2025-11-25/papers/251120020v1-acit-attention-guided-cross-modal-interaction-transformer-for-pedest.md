---
layout: default
title: ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction
---

# ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction

**arXiv**: [2511.20020v1](https://arxiv.org/abs/2511.20020) | [PDF](https://arxiv.org/pdf/2511.20020.pdf)

**ä½œè€…**: Yuanzhe Li, Steffen MÃ¼ller

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ³¨æ„åŠ›å¼•å¯¼è·¨æ¨¡æ€äº¤äº’Transformerä»¥é¢„æµ‹è¡Œäººè¿‡è¡—æ„å›¾**

**å…³é”®è¯**: `è¡Œäººæ„å›¾é¢„æµ‹` `è·¨æ¨¡æ€äº¤äº’` `æ³¨æ„åŠ›æœºåˆ¶` `Transformer` `è‡ªåŠ¨é©¾é©¶å®‰å…¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žå¤šæ¨¡æ€æ•°æ®ä¸­æœ‰æ•ˆæå–å’Œæ•´åˆäº’è¡¥çº¿ç´¢é¢„æµ‹è¡Œäººè¿‡è¡—æ„å›¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å…­ç§æ¨¡æ€åˆ†ç»„ä¸ºä¸‰å¯¹ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¢žå¼ºæ¨¡æ€é—´äº¤äº’ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨JAADbehå’ŒJAADallæ•°æ®é›†ä¸Šå‡†ç¡®çŽ‡åˆ†åˆ«è¾¾70%å’Œ89%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.

