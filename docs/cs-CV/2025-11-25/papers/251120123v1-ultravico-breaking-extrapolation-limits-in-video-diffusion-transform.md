---
layout: default
title: UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers
---

# UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers

**arXiv**: [2511.20123v1](https://arxiv.org/abs/2511.20123) | [PDF](https://arxiv.org/pdf/2511.20123.pdf)

**ä½œè€…**: Min Zhao, Hongzhou Zhu, Yingze Wang, Bokai Yan, Jintao Zhang, Guande He, Ling Yang, Chongxuan Li, Jun Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUltraViCoæ–¹æ³•ä»¥è§£å†³è§†é¢‘æ‰©æ•£å˜æ¢å™¨é•¿åº¦å¤–æŽ¨ä¸­çš„æ³¨æ„åŠ›åˆ†æ•£é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£å˜æ¢å™¨` `é•¿åº¦å¤–æŽ¨` `æ³¨æ„åŠ›æœºåˆ¶` `è®­ç»ƒå…è´¹æ–¹æ³•` `å¯æŽ§è§†é¢‘åˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘é•¿åº¦å¤–æŽ¨æ—¶å‡ºçŽ°æ³¨æ„åŠ›åˆ†æ•£ï¼Œå¯¼è‡´è´¨é‡ä¸‹é™å’Œå‘¨æœŸæ€§å†…å®¹é‡å¤
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ’å®šè¡°å‡å› å­æŠ‘åˆ¶è®­ç»ƒçª—å£å¤–ä»¤ç‰Œçš„æ³¨æ„åŠ›ï¼Œæ— éœ€è®­ç»ƒå³å¯åº”ç”¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨4å€å¤–æŽ¨ä¸‹ï¼ŒåŠ¨æ€åº¦å’Œæˆåƒè´¨é‡åˆ†åˆ«æå‡233%å’Œ40.5%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.

