---
layout: default
title: Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations
---

# Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations

**arXiv**: [2511.20295v1](https://arxiv.org/abs/2511.20295) | [PDF](https://arxiv.org/pdf/2511.20295.pdf)

**ä½œè€…**: Chao Wang, Chengan Che, Xinyue Chen, Sophia Tsoka, Luis C. Garcia-Peraza-Herrera

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBTTFä¼˜åŒ–æ¡†æž¶ä»¥ç”Ÿæˆè§†é¢‘åˆ†ç±»å™¨çš„åäº‹å®žè§£é‡Š**

**å…³é”®è¯**: `åäº‹å®žè§£é‡Š` `è§†é¢‘åˆ†ç±»` `ä¼˜åŒ–æ¡†æž¶` `æ—¶é—´ä¸€è‡´æ€§` `ç‰©ç†åˆç†æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åäº‹å®žè§£é‡Šæ–¹æ³•æ— æ³•ç”Ÿæˆæ—¶é—´ä¸€è‡´ä¸”ç‰©ç†åˆç†çš„è§†é¢‘è§£é‡Š
2. BTTFå¼•å…¥æ¡ä»¶åˆå§‹å™ªå£°å’Œä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œä»…ä¾èµ–ç›®æ ‡åˆ†ç±»å™¨æŒ‡å¯¼
3. åœ¨å¤šä¸ªè§†é¢‘æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œç”Ÿæˆæœ‰æ•ˆã€ç›¸ä¼¼ä¸”çœŸå®žçš„è§†é¢‘è§£é‡Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions. They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers. State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers. The generation of CFEs for video classifiers remains largely underexplored. For the counterfactual videos to be useful, they have to be physically plausible, temporally coherent, and exhibit smooth motion trajectories. Existing CFE image-based methods, designed to explain image classifiers, lack the capacity to generate temporally coherent, smooth and physically plausible video CFEs. To address this, we propose Back To The Feature (BTTF), an optimization framework that generates video CFEs. Our method introduces two novel features, 1) an optimization scheme to retrieve the initial latent noise conditioned by the first frame of the input video, 2) a two-stage optimization strategy to enable the search for counterfactual videos in the vicinity of the input video. Both optimization processes are guided solely by the target classifier, ensuring the explanation is faithful. To accelerate convergence, we also introduce a progressive optimization strategy that incrementally increases the number of denoising steps. Extensive experiments on video datasets such as Shape-Moving (motion classification), MEAD (emotion classification), and NTU RGB+D (action classification) show that our BTTF effectively generates valid, visually similar and realistic counterfactual videos that provide concrete insights into the classifier's decision-making mechanism.

