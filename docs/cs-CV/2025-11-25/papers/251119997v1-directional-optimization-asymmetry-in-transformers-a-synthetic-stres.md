---
layout: default
title: Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test
---

# Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test

**arXiv**: [2511.19997v1](https://arxiv.org/abs/2511.19997) | [PDF](https://arxiv.org/pdf/2511.19997.pdf)

**ä½œè€…**: Mihir Sahasrabudhe

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆæˆåŸºå‡†ä»¥æ­ç¤ºTransformeråœ¨æ–¹å‘ä¼˜åŒ–ä¸­çš„å†…åœ¨ä¸å¯¹ç§°æ€§**

**å…³é”®è¯**: `Transformeræž¶æž„` `æ–¹å‘ä¼˜åŒ–` `åˆæˆåŸºå‡†` `ç†µæŽ§åˆ¶` `å› æžœè®­ç»ƒ` `æ¨¡åž‹åå·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformeræž¶æž„æ˜¯å¦æœ¬èº«å­˜åœ¨æ–¹å‘å­¦ä¹ åå·®ï¼Œè€Œéžä»…æºäºŽè¯­è¨€ç»Ÿè®¡
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¯æŽ§ç†µçš„éšæœºå­—ç¬¦ä¸²æ˜ å°„æž„å»ºæ­£å‘å’Œé€†å‘ä»»åŠ¡è¿›è¡ŒåŽ‹åŠ›æµ‹è¯•
3. å®žéªŒæˆ–æ•ˆæžœï¼šGPT-2æ¨¡åž‹åœ¨é€†å‘ä»»åŠ¡ä¸­æŸå¤±æ˜¾è‘—é«˜äºŽç†è®ºä¸‹é™ï¼Œæ˜¾ç¤ºæ–¹å‘ä¼˜åŒ–å·®è·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.

