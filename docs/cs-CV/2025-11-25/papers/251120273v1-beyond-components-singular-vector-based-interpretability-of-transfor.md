---
layout: default
title: Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits
---

# Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits

**arXiv**: [2511.20273v1](https://arxiv.org/abs/2511.20273) | [PDF](https://arxiv.org/pdf/2511.20273.pdf)

**ä½œè€…**: Areeb Ahmad, Abhinav Joshi, Ashutosh Modi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¥‡å¼‚å‘é‡çš„Transformerç»„ä»¶åˆ†è§£æ–¹æ³•ï¼Œä»¥æ­ç¤ºå†…éƒ¨è®¡ç®—å­ç»“æž„ã€‚**

**å…³é”®è¯**: `Transformerè§£é‡Šæ€§` `å¥‡å¼‚å‘é‡åˆ†è§£` `æœºåˆ¶å¯è§£é‡Šæ€§` `è®¡ç®—å›¾åˆ†æž` `åŠŸèƒ½å­ç»“æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformeræ¨¡åž‹å†…éƒ¨è®¡ç®—å¤æ‚ä¸”åˆ†å¸ƒï¼ŒçŽ°æœ‰æ–¹æ³•å¿½ç•¥ç»„ä»¶å†…åŠŸèƒ½å­ç»“æž„ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ³¨æ„åŠ›å¤´å’ŒMLPåˆ†è§£ä¸ºæ­£äº¤å¥‡å¼‚æ–¹å‘ï¼Œè¯†åˆ«å åŠ å’Œç‹¬ç«‹è®¡ç®—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨IOIã€GPå’ŒGTä»»åŠ¡ä¸­éªŒè¯ï¼Œå‘çŽ°åŠŸèƒ½å¤´ç¼–ç å¤šä¸ªé‡å å­åŠŸèƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

