---
layout: default
title: VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering
---

# VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering

**arXiv**: [2511.19899v1](https://arxiv.org/abs/2511.19899) | [PDF](https://arxiv.org/pdf/2511.19899.pdf)

**ä½œè€…**: Yuyi Li, Daoyuan Chen, Zhen Wang, Yutong Lu, Yaliang Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVeriSciQAæ•°æ®é›†ä»¥è§£å†³ç§‘å­¦è§†è§‰é—®ç­”ä¸­æ•°æ®è´¨é‡ä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `ç§‘å­¦è§†è§‰é—®ç­”` `æ•°æ®é›†æž„å»º` `è·¨æ¨¡æ€éªŒè¯` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼€æºåŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç§‘å­¦è§†è§‰é—®ç­”ç¼ºä¹é«˜è´¨é‡å…¬å¼€æ•°æ®é›†ï¼Œé˜»ç¢å¼€æºæ¨¡åž‹å‘å±•
2. é‡‡ç”¨ç”Ÿæˆ-éªŒè¯æ¡†æž¶ï¼Œç»“åˆè·¨æ¨¡æ€ä¸€è‡´æ€§æ£€æŸ¥è¿‡æ»¤é”™è¯¯QAå¯¹
3. å®žéªŒæ˜¾ç¤ºVeriSciQAæå‡æ¨¡åž‹æ€§èƒ½ï¼Œäººç±»è¯„ä¼°éªŒè¯å…¶æ­£ç¡®æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) show promise for scientific applications, yet open-source models still struggle with Scientific Visual Question Answering (SVQA), namely answering questions about figures from scientific papers. A key bottleneck lies in the lack of public, large-scale, high-quality SVQA datasets. Although recent work uses LVLMs to synthesize data at scale, we identify systematic errors in their resulting QA pairs, stemming from LVLMs' inherent limitations and information asymmetry between figures and text. To address these challenges, we propose a verification-centric Generate-then-Verify framework that first generates QA pairs with figure-associated textual context, then applies cross-modal consistency checks against figures along with auxiliary filters to eliminate erroneous pairs. We instantiate this framework to curate VeriSciQA, a dataset of 20,351 QA pairs spanning 20 scientific domains and 12 figure types. VeriSciQA poses a challenging benchmark for open-source models, with a substantial accuracy gap between the leading open-source models (64%) and a proprietary model (82%). Moreover, models fine-tuned on VeriSciQA achieve consistent improvements on SVQA benchmarks, with performance gains that scale with data size and surpass models trained on existing datasets. Human evaluation further validates the superior correctness of VeriSciQA. Together, these evidences demonstrate that continued data expansion by our scalable framework can further advance SVQA capability in the open-source community.

