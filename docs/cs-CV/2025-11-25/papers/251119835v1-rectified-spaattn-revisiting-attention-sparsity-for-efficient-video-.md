---
layout: default
title: Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation
---

# Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation

**arXiv**: [2511.19835v1](https://arxiv.org/abs/2511.19835) | [PDF](https://arxiv.org/pdf/2511.19835.pdf)

**ä½œè€…**: Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRectified SpaAttnä»¥è§£å†³è§†é¢‘ç”Ÿæˆä¸­æ³¨æ„åŠ›ç¨€ç–å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `æ³¨æ„åŠ›ç¨€ç–` `æ‰©æ•£å˜æ¢å™¨` `è®¡ç®—æ•ˆçŽ‡` `Tritonä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ³¨æ„åŠ›ç¨€ç–æ–¹æ³•å› ç³»ç»Ÿåå·®å¯¼è‡´æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼ŒåŒ…æ‹¬å…³é”®ä»¤ç‰Œæƒé‡æ”¾å¤§å’Œéžå…³é”®ä»¤ç‰Œå¿½ç•¥
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥éšå¼å…¨æ³¨æ„åŠ›å‚è€ƒï¼Œé€šè¿‡å­¤ç«‹æ± åŒ–æ³¨æ„åŠ›é‡åˆ†é…å’Œå¢žç›Šæ„ŸçŸ¥æ± åŒ–æ ¡æ­£æ¥ä¿®æ­£æ³¨æ„åŠ›åˆ†é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HunyuanVideoå’ŒWan 2.1ä¸Šå®žçŽ°æœ€é«˜3.33å€å’Œ2.08å€åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒé«˜ç”Ÿæˆè´¨é‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion Transformers dominate video generation, but the quadratic complexity of attention computation introduces substantial latency. Attention sparsity reduces computational costs by focusing on critical tokens while ignoring non-critical tokens. However, existing methods suffer from severe performance degradation. In this paper, we revisit attention sparsity and reveal that existing methods induce systematic biases in attention allocation: (1) excessive focus on critical tokens amplifies their attention weights; (2) complete neglect of non-critical tokens causes the loss of relevant attention weights. To address these issues, we propose Rectified SpaAttn, which rectifies attention allocation with implicit full attention reference, thereby enhancing the alignment between sparse and full attention maps. Specifically: (1) for critical tokens, we show that their bias is proportional to the sparse attention weights, with the ratio governed by the amplified weights. Accordingly, we propose Isolated-Pooling Attention Reallocation, which calculates accurate rectification factors by reallocating multimodal pooled weights. (2) for non-critical tokens, recovering attention weights from the pooled query-key yields attention gains but also introduces pooling errors. Therefore, we propose Gain-Aware Pooling Rectification, which ensures that the rectified gain consistently surpasses the induced error. Moreover, we customize and integrate the Rectified SpaAttn kernel using Triton, achieving up to 3.33 and 2.08 times speedups on HunyuanVideo and Wan 2.1, respectively, while maintaining high generation quality. We release Rectified SpaAttn as open-source at https://github.com/BienLuky/Rectified-SpaAttn .

