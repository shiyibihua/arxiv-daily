---
layout: default
title: XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface
---

# XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface

**arXiv**: [2511.20254v1](https://arxiv.org/abs/2511.20254) | [PDF](https://arxiv.org/pdf/2511.20254.pdf)

**ä½œè€…**: Alexander C. Jenke, Gregor Just, Claas de Boer, Martin Wagner, Sebastian Bodenstedt, Stefanie Speidel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºXiCADæ–¹æ³•ä»¥æ£€æµ‹è¾¾èŠ¬å¥‡Xiæ‰‹æœ¯ç³»ç»Ÿä¸­æ‘„åƒå¤´æ¿€æ´»çŠ¶æ€**

**å…³é”®è¯**: `æ‘„åƒå¤´æ¿€æ´»æ£€æµ‹` `æœºå™¨äººè¾…åŠ©æ‰‹æœ¯` `å·ç§¯ç¥žç»ç½‘ç»œ` `æ‰‹æœ¯æ•°æ®ç§‘å­¦` `å®žæ—¶å…ƒæ•°æ®æå–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººè¾…åŠ©å¾®åˆ›æ‰‹æœ¯ä¸­ï¼Œå†…çª¥é•œè§†é¢‘æ˜¯å”¯ä¸€è§†è§‰åé¦ˆï¼Œéœ€è‡ªåŠ¨æ£€æµ‹æ‘„åƒå¤´æ¿€æ´»çŠ¶æ€ä»¥æä¾›å…ƒæ•°æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽResNet18å·ç§¯ç¥žç»ç½‘ç»œæž„å»ºè½»é‡çº§ç®¡é“ï¼Œå®šä½æ‘„åƒå¤´å›¾å—å¹¶è¯†åˆ«å…¶æ¿€æ´»çŠ¶æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒF1åˆ†æ•°è¾¾0.993è‡³1.000ï¼Œæ— å‡é˜³æ€§å¤šæ‘„åƒå¤´æ£€æµ‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation.
>   Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames.
>   Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections.
>   Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.

