---
layout: default
title: VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs
---

# VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs

**arXiv**: [2511.20272v1](https://arxiv.org/abs/2511.20272) | [PDF](https://arxiv.org/pdf/2511.20272.pdf)

**ä½œè€…**: Tianxiang Jiang, Sheng Xia, Yicheng Xu, Linquan Wu, Xiangyu Zeng, Limin Wang, Yu Qiao, Yi Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVKnowUåŸºå‡†å’ŒVideoKnow+æ¨¡åž‹ä»¥è¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§æ¨¡åž‹çš„è§†è§‰çŸ¥è¯†ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `è§†è§‰çŸ¥è¯†ç†è§£` `åŸºå‡†è¯„ä¼°` `è§†é¢‘é—®ç­”` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•ŒçŸ¥è¯†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§æ¨¡åž‹ç¼ºä¹å¯¹ç‰©ç†å’Œç¤¾ä¼šä¸–ç•Œåº•å±‚åŽŸç†çš„ç›´è§‚è§†è§‰çŸ¥è¯†ç†è§£
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºVKnowUåŸºå‡†å’ŒVideoKnow+æ¨¡åž‹ï¼Œé‡‡ç”¨See-Think-AnswerèŒƒå¼å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šVideoKnow+åœ¨VKnowUä¸Šæå‡3.7%ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨çŽ°ä¸€è‡´æ”¹è¿›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This high-level vision-grounded semantics, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored area in current MLLMs. To systematically evaluate this capability, we present VKnowU, a comprehensive benchmark featuring 1,680 questions in 1,249 videos, covering 8 core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Evaluation of 23 SOTA MLLMs reveals that leading models still fall short of human performance, with particularly notable gaps in the world-centric. To bridge this gap, we introduce a new dataset, VKnowQA, and VideoKnow+, a baseline model that explicitly incorporates visual knowledge into MLLMs. VideoKnow+ follows a structured See-Think-Answer paradigm and adopts reinforcement learning with visual knowledge reward, achieving a +3.7% improvement on VKnowU and consistent gains on MVBench, Video-MME, and MMVU. Our work highlights visual knowledge as a missing cornerstone for developing more generalizable MLLMs that can not only see but also truly understand our physical and social worlds.

