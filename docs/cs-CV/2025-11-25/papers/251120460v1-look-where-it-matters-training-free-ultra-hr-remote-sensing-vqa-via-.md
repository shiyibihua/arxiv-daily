---
layout: default
title: Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search
---

# Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search

**arXiv**: [2511.20460v1](https://arxiv.org/abs/2511.20460) | [PDF](https://arxiv.org/pdf/2511.20460.pdf)

**ä½œè€…**: Yunqi Zhou, Chengjie Jiang, Chun Yuan, Jing Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZoomSearchä»¥è§£å†³è¶…é«˜åˆ†é¥æ„ŸVQAä¸­ç»†èŠ‚ä¸¢å¤±ä¸Žæ•ˆçŽ‡ä½Žä¸‹çš„é—®é¢˜**

**å…³é”®è¯**: `é¥æ„Ÿè§†è§‰é—®ç­”` `è¶…é«˜åˆ†å›¾åƒå¤„ç†` `è‡ªé€‚åº”ç¼©æ”¾æœç´¢` `å¸ƒå±€æ„ŸçŸ¥é‡ç»„` `è®­ç»ƒå…è´¹æ–¹æ³•` `æŽ¨ç†æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¶…é«˜åˆ†é¥æ„Ÿå›¾åƒå…¨å›¾ç¼–ç å¯¼è‡´å†…å­˜å’Œtokenä¸è¶³ï¼Œç¼©æ”¾é¢„å¤„ç†ä¸¢å¤±å…³é”®ç»†èŠ‚
2. é‡‡ç”¨è‡ªé€‚åº”å¤šåˆ†æ”¯ç¼©æ”¾æœç´¢å’Œå¸ƒå±€æ„ŸçŸ¥è¡¥ä¸é‡ç»„ï¼Œæ— éœ€è®­ç»ƒå³å¯å®šä½å¹¶æ•´åˆç›¸å…³åŒºåŸŸ
3. åœ¨LRS-VQAå’ŒMME-RealWorld-RSåŸºå‡†ä¸Šï¼Œé›†æˆLLaVA-ovå®žçŽ°SOTAç²¾åº¦å’Œæ›´é«˜æŽ¨ç†æ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.

