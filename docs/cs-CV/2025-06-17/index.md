---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-17
---

# cs.CVï¼ˆ2025-06-17ï¼‰

ğŸ“Š å…± **25** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250617302v1-fine-scale-soil-mapping-in-alaska-with-multimodal-machine-learning.html">Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning</a></td>
  <td>æå‡ºMISOæ¨¡å‹ä»¥è§£å†³é˜¿æ‹‰æ–¯åŠ ç»†å°ºåº¦åœŸå£¤åˆ¶å›¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17302v1" data-paper-url="./papers/250617302v1-fine-scale-soil-mapping-in-alaska-with-multimodal-machine-learning.html" onclick="toggleFavorite(this, '2506.17302v1', 'Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250614495v1-i-speak-and-you-find-robust-3d-visual-grounding-with-noisy-and-ambig.html">I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs</a></td>
  <td>æå‡ºSpeechReferä»¥è§£å†³å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³è¾“å…¥ä¸‹çš„3Dè§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14495v1" data-paper-url="./papers/250614495v1-i-speak-and-you-find-robust-3d-visual-grounding-with-noisy-and-ambig.html" onclick="toggleFavorite(this, '2506.14495v1', 'I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250614765v4-earth-observation-foundation-model-phileo-pretraining-on-the-majorto.html">Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets</a></td>
  <td>æå‡ºPhilEOä»¥æå‡åœ°çƒè§‚æµ‹æ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14765v4" data-paper-url="./papers/250614765v4-earth-observation-foundation-model-phileo-pretraining-on-the-majorto.html" onclick="toggleFavorite(this, '2506.14765v4', 'Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250614903v1-detonate-a-benchmark-for-text-to-image-alignment-and-kernelized-dire.html">DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization</a></td>
  <td>æå‡ºDETONATEåŸºå‡†ä»¥ä¼˜åŒ–æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14903v1" data-paper-url="./papers/250614903v1-detonate-a-benchmark-for-text-to-image-alignment-and-kernelized-dire.html" onclick="toggleFavorite(this, '2506.14903v1', 'DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250614907v1-perl-permutation-enhanced-reinforcement-learning-for-interleaved-vis.html">PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning</a></td>
  <td>æå‡ºPeRLä»¥è§£å†³å¤šå›¾åƒæ¨ç†ä¸­çš„ç©ºé—´å…³ç³»ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14907v1" data-paper-url="./papers/250614907v1-perl-permutation-enhanced-reinforcement-learning-for-interleaved-vis.html" onclick="toggleFavorite(this, '2506.14907v1', 'PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250614130v1-kdmosknowledge-distillation-for-motion-segmentation.html">KDMOS:Knowledge Distillation for Motion Segmentation</a></td>
  <td>æå‡ºKDMOSä»¥è§£å†³è¿åŠ¨ç‰©ä½“åˆ†å‰²ä¸­çš„å®æ—¶æ€§ä¸å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14130v1" data-paper-url="./papers/250614130v1-kdmosknowledge-distillation-for-motion-segmentation.html" onclick="toggleFavorite(this, '2506.14130v1', 'KDMOS:Knowledge Distillation for Motion Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250614603v1-align-your-flow-scaling-continuous-time-flow-map-distillation.html">Align Your Flow: Scaling Continuous-Time Flow Map Distillation</a></td>
  <td>æå‡ºè¿ç»­æ—¶é—´æµå›¾è’¸é¦æ–¹æ³•ä»¥æå‡ç”Ÿæˆæ¨¡å‹æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14603v1" data-paper-url="./papers/250614603v1-align-your-flow-scaling-continuous-time-flow-map-distillation.html" onclick="toggleFavorite(this, '2506.14603v1', 'Align Your Flow: Scaling Continuous-Time Flow Map Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250614512v3-siri-bench-challenging-vlms-spatial-intelligence-through-complex-rea.html">SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks</a></td>
  <td>æå‡ºSIRI-Benchä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14512v3" data-paper-url="./papers/250614512v3-siri-bench-challenging-vlms-spatial-intelligence-through-complex-rea.html" onclick="toggleFavorite(this, '2506.14512v3', 'SIRI-Bench: Challenging VLMs&#39; Spatial Intelligence through Complex Reasoning Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250614440v1-model-compression-using-knowledge-distillation-with-integrated-gradi.html">Model compression using knowledge distillation with integrated gradients</a></td>
  <td>æå‡ºåŸºäºé›†æˆæ¢¯åº¦çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä»¥å®ç°æ¨¡å‹å‹ç¼©</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14440v1" data-paper-url="./papers/250614440v1-model-compression-using-knowledge-distillation-with-integrated-gradi.html" onclick="toggleFavorite(this, '2506.14440v1', 'Model compression using knowledge distillation with integrated gradients')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250614362v2-hydrochronos-forecasting-decades-of-surface-water-change.html">HydroChronos: Forecasting Decades of Surface Water Change</a></td>
  <td>æå‡ºHydroChronosä»¥è§£å†³æ°´ä½“åŠ¨æ€é¢„æµ‹æ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14362v2" data-paper-url="./papers/250614362v2-hydrochronos-forecasting-decades-of-surface-water-change.html" onclick="toggleFavorite(this, '2506.14362v2', 'HydroChronos: Forecasting Decades of Surface Water Change')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250614642v2-3dgs-ieval-15k-a-large-scale-image-quality-evaluation-database-for-3.html">3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting</a></td>
  <td>æå‡º3DGS-IEval-15Kä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘å‹ç¼©è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14642v2" data-paper-url="./papers/250614642v2-3dgs-ieval-15k-a-large-scale-image-quality-evaluation-database-for-3.html" onclick="toggleFavorite(this, '2506.14642v2', '3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250614742v1-synctalk-high-fidelity-and-efficient-synchronized-talking-heads-synt.html">SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</a></td>
  <td>æå‡ºSyncTalk++ä»¥è§£å†³é«˜ä¿çœŸåŒæ­¥äººå¤´åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14742v1" data-paper-url="./papers/250614742v1-synctalk-high-fidelity-and-efficient-synchronized-talking-heads-synt.html" onclick="toggleFavorite(this, '2506.14742v1', 'SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250614856v1-peering-into-the-unknown-active-view-selection-with-neural-uncertain.html">Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction</a></td>
  <td>æå‡ºåŸºäºç¥ç»ä¸ç¡®å®šæ€§å›¾çš„ä¸»åŠ¨è§†è§’é€‰æ‹©ä»¥ä¼˜åŒ–3Dé‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14856v1" data-paper-url="./papers/250614856v1-peering-into-the-unknown-active-view-selection-with-neural-uncertain.html" onclick="toggleFavorite(this, '2506.14856v1', 'Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250614271v1-leader360v-the-large-scale-real-world-360-video-dataset-for-multi-ta.html">Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment</a></td>
  <td>æå‡ºLeader360Vä»¥è§£å†³360è§†é¢‘æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14271v1" data-paper-url="./papers/250614271v1-leader360v-the-large-scale-real-world-360-video-dataset-for-multi-ta.html" onclick="toggleFavorite(this, '2506.14271v1', 'Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250614709v2-difuse-net-rgb-and-dual-pixel-depth-estimation-using-window-bi-direc.html">DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning</a></td>
  <td>æå‡ºDiFuse-Netä»¥è§£å†³RGBå’ŒåŒåƒç´ æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14709v2" data-paper-url="./papers/250614709v2-difuse-net-rgb-and-dual-pixel-depth-estimation-using-window-bi-direc.html" onclick="toggleFavorite(this, '2506.14709v2', 'DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250614511v1-mol-joint-estimation-of-micro-expression-optical-flow-and-landmark-v.html">MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution</a></td>
  <td>æå‡ºMOLæ¡†æ¶ä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­çš„æ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14511v1" data-paper-url="./papers/250614511v1-mol-joint-estimation-of-micro-expression-optical-flow-and-landmark-v.html" onclick="toggleFavorite(this, '2506.14511v1', 'MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250614629v2-vistext-mosquito-a-unified-multimodal-benchmark-dataset-for-visual-d.html">VisText-Mosquito: A Unified Multimodal Benchmark Dataset for Visual Detection, Segmentation, and Textual Reasoning on Mosquito Breeding Sites</a></td>
  <td>æå‡ºVisText-Mosquitoä»¥è§£å†³èšŠè™«æ»‹ç”Ÿåœ°æ£€æµ‹ä¸åˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14629v2" data-paper-url="./papers/250614629v2-vistext-mosquito-a-unified-multimodal-benchmark-dataset-for-visual-d.html" onclick="toggleFavorite(this, '2506.14629v2', 'VisText-Mosquito: A Unified Multimodal Benchmark Dataset for Visual Detection, Segmentation, and Textual Reasoning on Mosquito Breeding Sites')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250614473v2-foundation-model-insights-and-a-multi-model-approach-for-superior-fi.html">Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection</a></td>
  <td>æå‡ºRAM-APLä»¥è§£å†³ç»†ç²’åº¦ä¸€-shotå­é›†é€‰æ‹©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14473v2" data-paper-url="./papers/250614473v2-foundation-model-insights-and-a-multi-model-approach-for-superior-fi.html" onclick="toggleFavorite(this, '2506.14473v2', 'Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250614435v1-mote-mixture-of-ternary-experts-for-memory-efficient-large-multimoda.html">MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models</a></td>
  <td>æå‡ºMoTEä»¥è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„å†…å­˜æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14435v1" data-paper-url="./papers/250614435v1-mote-mixture-of-ternary-experts-for-memory-efficient-large-multimoda.html" onclick="toggleFavorite(this, '2506.14435v1', 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250614766v2-ascd-attention-steerable-contrastive-decoding-for-reducing-hallucina.html">ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM</a></td>
  <td>æå‡ºASCDä»¥å‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14766v2" data-paper-url="./papers/250614766v2-ascd-attention-steerable-contrastive-decoding-for-reducing-hallucina.html" onclick="toggleFavorite(this, '2506.14766v2', 'ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250614471v1-dense360-dense-understanding-from-omnidirectional-panoramas.html">Dense360: Dense Understanding from Omnidirectional Panoramas</a></td>
  <td>æå‡ºDense360ä»¥è§£å†³å…¨æ™¯å›¾åƒç†è§£çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14471v1" data-paper-url="./papers/250614471v1-dense360-dense-understanding-from-omnidirectional-panoramas.html" onclick="toggleFavorite(this, '2506.14471v1', 'Dense360: Dense Understanding from Omnidirectional Panoramas')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250614696v2-yolov11-rgbt-towards-a-comprehensive-single-stage-multispectral-obje.html">YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework</a></td>
  <td>æå‡ºYOLOv11-RGBTä»¥è§£å†³å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ¡†æ¶ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14696v2" data-paper-url="./papers/250614696v2-yolov11-rgbt-towards-a-comprehensive-single-stage-multispectral-obje.html" onclick="toggleFavorite(this, '2506.14696v2', 'YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250614356v1-eva02-at-egocentric-video-language-understanding-with-spatial-tempor.html">EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization</a></td>
  <td>æå‡ºEVA02-ATä»¥è§£å†³è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è¯­è¨€ç†è§£ä¸­çš„å¤šé‡æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">Ego4D</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14356v1" data-paper-url="./papers/250614356v1-eva02-at-egocentric-video-language-understanding-with-spatial-tempor.html" onclick="toggleFavorite(this, '2506.14356v1', 'EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250614769v2-cdp-towards-robust-autoregressive-visuomotor-policy-learning-via-cau.html">CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion</a></td>
  <td>æå‡ºCausal Diffusion Policyä»¥è§£å†³æœºå™¨äººæ§åˆ¶ä¸­çš„æ•°æ®è´¨é‡å’Œå®æ—¶æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14769v2" data-paper-url="./papers/250614769v2-cdp-towards-robust-autoregressive-visuomotor-policy-learning-via-cau.html" onclick="toggleFavorite(this, '2506.14769v2', 'CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250614399v4-decoupled-classifier-free-guidance-for-counterfactual-diffusion-mode.html">Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models</a></td>
  <td>æå‡ºè§£è€¦åˆ†ç±»å™¨æ— å…³å¼•å¯¼ä»¥è§£å†³åäº‹å®ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14399v4" data-paper-url="./papers/250614399v4-decoupled-classifier-free-guidance-for-counterfactual-diffusion-mode.html" onclick="toggleFavorite(this, '2506.14399v4', 'Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)