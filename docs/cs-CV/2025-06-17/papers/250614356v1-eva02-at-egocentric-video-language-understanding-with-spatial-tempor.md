---
layout: default
title: EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization
---

# EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14356" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.14356v1</a>
  <a href="https://arxiv.org/pdf/2506.14356.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14356v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14356v1', 'EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiaoqi Wang, Yi Wang, Lap-Pui Chau

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-17

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/xqwang14/EVA02-AT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EVA02-AT‰ª•Ëß£ÂÜ≥Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëËØ≠Ë®ÄÁêÜËß£‰∏≠ÁöÑÂ§öÈáçÊåëÊàò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁêÜËß£` `Êó∂Á©∫Âª∫Ê®°` `Â§öÂÆû‰æãÊ£ÄÁ¥¢` `ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•` `ÂØπÁß∞Â§öÁõ∏‰ººÊÄßÊçüÂ§±` `ËßÜÈ¢ëËØ≠Ë®ÄÊ®°Âûã` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëËØ≠Ë®ÄÁêÜËß£‰∏≠Èù¢‰∏¥È´òÈ¢ÑËÆ≠ÁªÉÊàêÊú¨„ÄÅÊó∂Á©∫ÁºñÁ†ÅÊó†ÊïàÂíåÂ≠¶‰π†ÁõÆÊ†á‰∏çÁ≤æÁ°ÆÁ≠âÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫EVA02-ATÔºåÈÄöËøáÂçïÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÂ∞ÜCLIPÊ®°ÂûãËΩ¨Âåñ‰∏∫ËßÜÈ¢ëÁºñÁ†ÅÂô®ÔºåÂπ∂ÂºïÂÖ•Êó∂Á©∫ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÂíåËÅîÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇ
3. Âú®Ego4D„ÄÅEPIC-Kitchens-100ÂíåCharades-EgoÁ≠âÊï∞ÊçÆÈõÜ‰∏äÔºåEVA02-ATÂú®Èõ∂-shotÂíåÂæÆË∞ÉËÆæÁΩÆ‰∏ãÂùáÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëËØ≠Ë®ÄÁêÜËß£ÈúÄË¶ÅÈ´òÊïà‰∏îÂáÜÁ°ÆÁöÑÊó∂Á©∫Âª∫Ê®°„ÄÇÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥‰∏âÂ§ßÊåëÊàòÔºö‰∏ÄÊòØÂ§öÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÂØºËá¥ÁöÑÈ´òÊàêÊú¨Ôºå‰∫åÊòØÊâãÂä®ÂàÜÂâ≤ÁöÑ3DÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÂΩ±ÂìçÁâπÂæÅ‰∫§‰∫íÔºå‰∏âÊòØËΩØÊ†áÁ≠æÂ§öÂÆû‰æãÊ£ÄÁ¥¢‰∏≠ÁöÑÂ≠¶‰π†ÁõÆÊ†á‰∏çÁ≤æÁ°ÆÔºåÂøΩËßÜË¥üÊ†∑Êú¨Áõ∏ÂÖ≥ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫EVA02-ATÔºå‰∏Ä‰∏™Âü∫‰∫éEVA02ÁöÑËßÜÈ¢ëËØ≠Ë®ÄÂü∫Á°ÄÊ®°ÂûãÔºå‰∏ì‰∏∫Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁêÜËß£‰ªªÂä°ËÆæËÆ°„ÄÇEVA02-ATÈÄöËøáÂçïÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÈ´òÊïàÂú∞Â∞ÜÂõæÂÉèÂü∫Á°ÄÁöÑCLIPÊ®°ÂûãËΩ¨Âåñ‰∏∫Áªü‰∏ÄÁöÑËßÜÈ¢ëÁºñÁ†ÅÂô®ÔºåÂπ∂ÂºïÂÖ•Êó∂Á©∫ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÂíåËÅîÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊúâÊïàÁºñÁ†ÅÁ©∫Èó¥ÂíåÊó∂Èó¥‰ø°ÊÅØ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫ÂØπÁß∞Â§öÁõ∏‰ººÊÄßÊçüÂ§±ÔºàSMSÔºâÔºå‰∏∫Ê≠£Ë¥üÊ†∑Êú¨Êèê‰æõÊõ¥Á≤æÁ°ÆÁöÑÂ≠¶‰π†ÁõÆÊ†á„ÄÇÂÆûÈ™åË°®ÊòéÔºåEVA02-ATÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºå‰∏îÂèÇÊï∞Êõ¥Â∞ë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëËØ≠Ë®ÄÁêÜËß£‰∏≠ÁöÑÈ´òÈ¢ÑËÆ≠ÁªÉÊàêÊú¨„ÄÅÊó∂Á©∫ÁºñÁ†ÅÊó†ÊïàÂèäÂ≠¶‰π†ÁõÆÊ†á‰∏çÁ≤æÁ°ÆÁ≠âÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÂ§çÊùÇÁöÑÂ§öÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÊµÅÁ®ãÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÔºåÂêåÊó∂ÊâãÂä®ÂàÜÂâ≤ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÂΩ±ÂìçÁâπÂæÅÁöÑÊúâÊïà‰∫§‰∫í„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEVA02-ATÈÄöËøáÂçïÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÈ´òÊïàÂú∞Â∞ÜÂõæÂÉèÂü∫Á°ÄÁöÑCLIPÊ®°ÂûãËΩ¨Âåñ‰∏∫ËßÜÈ¢ëÁºñÁ†ÅÂô®ÔºåÈÅøÂÖç‰∫ÜÂ§öÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÁöÑÂ§çÊùÇÊÄß„ÄÇÂêåÊó∂ÔºåÈááÁî®Êó∂Á©∫ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•‰∏éËÅîÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÂú®Êï¥‰∏™ÈöêËóèÁª¥Â∫¶‰∏äÊúâÊïàÁºñÁ†ÅÁ©∫Èó¥ÂíåÊó∂Èó¥‰ø°ÊÅØÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Ë∑®ËΩ¥ÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEVA02-ATÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™Áªü‰∏ÄÁöÑËßÜÈ¢ëÁºñÁ†ÅÂô®ÔºåÊó∂Á©∫ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•Ê®°ÂùóÔºå‰ª•ÂèäÂØπÁß∞Â§öÁõ∏‰ººÊÄßÊçüÂ§±ÔºàSMSÔºâÊ®°Âùó„ÄÇÊ®°ÂûãÈ¶ñÂÖàÈÄöËøáÂçïÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉËøõË°åÂàùÂßãÂåñÔºåÁÑ∂ÂêéÂú®Â§öÂÆû‰æãËßÜÈ¢ëËØ≠Ë®ÄÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ËøõË°åÂæÆË∞É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•Êó∂Á©∫ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÂíåËÅîÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂêåÊó∂ÊúâÊïàÂú∞ÊçïÊçâÁ©∫Èó¥ÂíåÊó∂Èó¥‰ø°ÊÅØÔºåËøõËÄåÊîπÂñÑËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®Âíå‰∫§‰∫íÂª∫Ê®°„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåEVA02-ATÂú®ÁâπÂæÅ‰∫§‰∫í‰∏äÂÖ∑ÊúâÊõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢ÔºåÈááÁî®ÂØπÁß∞Â§öÁõ∏‰ººÊÄßÊçüÂ§±ÔºàSMSÔºâÔºå‰∏∫Ê≠£Ë¥üÊ†∑Êú¨Êèê‰æõÊõ¥Á≤æÁ°ÆÁöÑÂ≠¶‰π†ÁõÆÊ†á„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãËÆæËÆ°‰∏≠ËøòÂåÖÊã¨‰ºòÂåñÁöÑÂèÇÊï∞ËÆæÁΩÆÔºå‰ª•ÂáèÂ∞ëÊ®°ÂûãÁöÑÂ§çÊùÇÊÄßÂπ∂ÊèêÂçáÊÄßËÉΩ„ÄÇÈÄöËøáËøô‰∫õËÆæËÆ°ÔºåEVA02-ATÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂ±ïÁé∞‰∫Ü‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EVA02-ATÂú®Ego4D„ÄÅEPIC-Kitchens-100ÂíåCharades-EgoÁ≠âÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂Âú®Â§öÂÆû‰æãÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äÔºåÈááÁî®SMSÊçüÂ§±ÁöÑÊ®°ÂûãË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂèÇÊï∞Êï∞ÈáèÊõ¥Â∞ëÔºåÊïàÁéáÊõ¥È´ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁêÜËß£È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Êô∫ËÉΩÁõëÊéß„ÄÅËôöÊãüÁé∞ÂÆûÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÂú∫ÊôØ‰∏≠„ÄÇÈÄöËøáÊèêÈ´òËßÜÈ¢ë‰∏éËØ≠Ë®ÄÁöÑÁêÜËß£ËÉΩÂäõÔºåEVA02-ATËÉΩÂ§ü‰∏∫Ëá™Âä®ÂåñËßÜÈ¢ëÂàÜÊûê„ÄÅÂÜÖÂÆπÊ£ÄÁ¥¢ÂíåÁî®Êà∑‰∫§‰∫íÊèê‰æõÊõ¥Êô∫ËÉΩÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

