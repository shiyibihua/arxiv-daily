---
layout: default
title: MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models
---

# MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14435" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.14435v1</a>
  <a href="https://arxiv.org/pdf/2506.14435.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14435v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14435v1', 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hongyu Wang, Jiayu Xu, Ruiping Wang, Yan Feng, Yitao Zhai, Peng Pei, Xunliang Cai, Xilin Chen

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-17

**Â§áÊ≥®**: Work in progress

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MoTE‰ª•Ëß£ÂÜ≥Â§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂÜÖÂ≠òÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã` `‰∏âÂÖÉ‰∏ìÂÆ∂` `ÂÜÖÂ≠òÊïàÁéá` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ËæπÁºòËÆ°ÁÆó` `ÈáèÂåñÊñπÊ≥ï`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÂú®‰ΩøÁî®ÂÖ®Á≤æÂ∫¶‰∏ìÂÆ∂Êó∂ÔºåÂØºËá¥‰∫ÜËæÉÈ´òÁöÑÂÜÖÂ≠òÂç†Áî®ÔºåÈôêÂà∂‰∫ÜÂú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫MoTEÔºåÈÄöËøáËÆ≠ÁªÉ‰ΩéÁ≤æÂ∫¶ÁöÑ‰∏âÂÖÉ‰∏ìÂÆ∂ÔºåÂà©Áî®ÂÖ±‰∫´ÁöÑÈ¢ÑËÆ≠ÁªÉÂâçÈ¶àÁΩëÁªúÊù•Èôç‰ΩéÂÜÖÂ≠òÊ∂àËÄó„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMoTEÂú®ÂÜÖÂ≠òÂç†Áî®Êõ¥‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºåÊÄßËÉΩ‰∏éÂÖ®Á≤æÂ∫¶Ê®°ÂûãÁõ∏ÂΩìÔºåÂπ∂Âú®ÁâπÂÆöÊù°‰ª∂‰∏ãÊòæËëóÊèêÂçá‰∫ÜÂáÜÁ°ÆÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÔºàMoEsÔºâÂú®ÊèêÂçáÊÄßËÉΩÁöÑÂêåÊó∂Ôºå‰øùÊåÅÂõ∫ÂÆöÁöÑÊ¥ªË∑ÉÂèÇÊï∞„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰ΩøÁî®ÂÖ®Á≤æÂ∫¶‰∏ìÂÆ∂ËøõË°åÁ®ÄÁñè‰∏äÂçáÔºåÂØºËá¥ËæÉÈ´òÁöÑÂÜÖÂ≠òÂç†Áî®ÔºåÁªôËæπÁºòËÆæÂ§áÁöÑÈÉ®ÁΩ≤Â∏¶Êù•ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫MoTEÔºå‰∏ÄÁßçÂèØÊâ©Â±ï‰∏îÂÜÖÂ≠òÈ´òÊïàÁöÑÊ∑∑Âêà‰∏âÂÖÉ‰∏ìÂÆ∂Ê®°ÂûãËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÊàë‰ª¨ÈÄöËøáËÆ≠ÁªÉÊõ¥Â§ö‰ΩéÁ≤æÂ∫¶‰∏ìÂÆ∂ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂâçÈ¶àÁΩëÁªú‰Ωú‰∏∫ÂÖ±‰∫´‰∏ìÂÆ∂ÔºåËÆ≠ÁªÉÂèÇÊï∞‰∏∫{-1, 0, 1}ÁöÑ‰∏âÂÖÉË∑ØÁî±‰∏ìÂÆ∂„ÄÇÂÆûÈ™åË°®ÊòéÔºåMoTEÂú®Ê®°ÂûãËßÑÊ®°‰∏äÂÖ∑ÊúâËâØÂ•ΩÁöÑÊâ©Â±ïË∂ãÂäøÔºåÂÖ∂ÊÄßËÉΩ‰∏éÂÖ®Á≤æÂ∫¶Âü∫Á∫øMoE-LLaVAÁõ∏ÂΩìÔºå‰ΩÜÂÜÖÂ≠òÂç†Áî®Êõ¥‰Ωé„ÄÇÂú®Áõ∏ÂêåÁöÑ3.4GB‰∏ìÂÆ∂ÂÜÖÂ≠òÂç†Áî®‰∏ãÔºåÁªìÂêàÂêéËÆ≠ÁªÉÈáèÂåñÔºåMoTEÂú®ÊúÄÁªà‰ªªÂä°‰∏äÊØîMoE-LLaVAÊèêÈ´ò‰∫Ü4.3%ÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÜÖÂ≠òÂèóÈôêËÆæÂ§á‰∏äÁöÑÊúâÊïàÊÄßÂíåÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÂú®Á®ÄÁñè‰∏äÂçáËøáÁ®ã‰∏≠ÂÜÖÂ≠òÂç†Áî®ËøáÈ´òÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÂÖ®Á≤æÂ∫¶‰∏ìÂÆ∂ÔºåÂØºËá¥Âú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤Èù¢‰∏¥ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫MoTEÔºåÈÄöËøáËÆ≠ÁªÉÊõ¥Â§ö‰ΩéÁ≤æÂ∫¶ÁöÑ‰∏âÂÖÉ‰∏ìÂÆ∂Êù•Êõø‰ª£È´òÁ≤æÂ∫¶‰∏ìÂÆ∂ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂâçÈ¶àÁΩëÁªú‰Ωú‰∏∫ÂÖ±‰∫´‰∏ìÂÆ∂Ôºå‰ªéËÄåÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®Âπ∂‰øùÊåÅÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMoTEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨È¢ÑËÆ≠ÁªÉÁöÑÂâçÈ¶àÁΩëÁªú‰Ωú‰∏∫ÂÖ±‰∫´‰∏ìÂÆ∂ÔºåÂíåÂ§ö‰∏™ÂèÇÊï∞‰∏∫{-1, 0, 1}ÁöÑ‰∏âÂÖÉË∑ØÁî±‰∏ìÂÆ∂„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ΩéÁ≤æÂ∫¶‰∏ìÂÆ∂ÁöÑÊï∞ÈáèÊòæËëóÂ¢ûÂä†Ôºå‰ª•ÊèêÂçáÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMoTEÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫Ü‰∏âÂÖÉ‰∏ìÂÆ∂ÁöÑÊ¶ÇÂøµÔºåÈÄöËøá‰ΩéÁ≤æÂ∫¶ÂèÇÊï∞ÁöÑ‰ΩøÁî®ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®ÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éÂÖ®Á≤æÂ∫¶Ê®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇËøô‰∏ÄËÆæËÆ°‰∏éÁé∞ÊúâÊñπÊ≥ïÂΩ¢Êàê‰∫ÜÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫Ü‰∏âÂÖÉÈáèÂåñÁöÑÂèÇÊï∞ËÆæÁΩÆÔºåÊçüÂ§±ÂáΩÊï∞ÁªèËøáË∞ÉÊï¥‰ª•ÈÄÇÂ∫î‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÔºåÂêåÊó∂Á°Æ‰øù‰∫ÜÊ®°ÂûãÁöÑÊî∂ÊïõÊÄßÂíåÊÄßËÉΩË°®Áé∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMoTEÂú®Áõ∏ÂêåÁöÑ3.4GB‰∏ìÂÆ∂ÂÜÖÂ≠òÂç†Áî®‰∏ãÔºåËæÉÂÖ®Á≤æÂ∫¶Âü∫Á∫øMoE-LLaVAÊèêÈ´ò‰∫Ü4.3%ÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéáÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®ÂÜÖÂ≠òÂèóÈôêÊù°‰ª∂‰∏ãÁöÑ‰ºòË∂äÊÄßËÉΩÂíåÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËæπÁºòËÆ°ÁÆó„ÄÅÁßªÂä®ËÆæÂ§áÂíåËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂ∫îÁî®„ÄÇÈÄöËøáÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÔºåMoTEËÉΩÂ§ü‰ΩøÂæóÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãÂú®Êõ¥ÂπøÊ≥õÁöÑËÆæÂ§á‰∏äÂæóÂà∞Â∫îÁî®ÔºåÊé®Âä®Êô∫ËÉΩËÆæÂ§áÁöÑÊôÆÂèäÂíåÂäüËÉΩÊèêÂçá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

