---
layout: default
title: Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets
---

# Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14765" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14765v4</a>
  <a href="https://arxiv.org/pdf/2506.14765.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14765v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14765v4', 'Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikolaos Dionelis, Riccardo Musto, Jente Bosmans, Simone Sarti, Giancarlo Paoletti, SÃ©bastien LefÃ¨vre, Bertrand Le Saux, Nicolas LongÃ©pÃ©

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: 15 pages, 22 figures, 2 tables, 64 references

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPhilEOä»¥æå‡åœ°çƒè§‚æµ‹æ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åœ°çƒè§‚æµ‹` `åŸºç¡€æ¨¡å‹` `é¢„è®­ç»ƒ` `U-Net` `è§†è§‰å˜æ¢å™¨` `Mambaæ¨¡å‹` `ä¸‹æ¸¸ä»»åŠ¡` `æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åœ°çƒè§‚æµ‹æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®æ—¶æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨å«æ˜Ÿæ•°æ®ã€‚
2. æœ¬æ–‡æå‡ºåœ¨MajorTOMæ•°æ®é›†ä¸Šé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¹¶æ¢ç´¢ä¸åŒç½‘ç»œæ¶æ„çš„æœ‰æ•ˆæ€§ï¼Œä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒU-Net 200M-2Tåœ¨é“è·¯å’Œå»ºç­‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”Mambaæ¨¡å‹åœ¨è®¡ç®—å¼€é”€ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰ï¼Œåœ°çƒè§‚æµ‹å«æ˜Ÿç”Ÿæˆå¤§é‡æ•°æ®ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œå¿…é¡»åœ¨å¤§å‹æœªæ ‡è®°æ•°æ®é›†ä¸Šé¢„è®­ç»ƒåœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ï¼Œä»¥ä¾¿åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚æœ¬æ–‡ç ”ç©¶äº†FMsçš„æ‰©å±•ï¼Œä½¿ç”¨åŒ…å«æ‰€æœ‰åŒºåŸŸçš„MajorTOM 23TBæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ä¸ä¸“é—¨å°å‹æ•°æ®é›†é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚é¢å¤–çš„æµ·æ´‹å’Œå†°å·æ•°æ®å¹¶æœªé™ä½åœ¨é™†åœ°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯æ¢ç´¢U-Netå·ç§¯ç¥ç»ç½‘ç»œã€è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰å’ŒMambaçŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºFMsçš„åº”ç”¨ã€‚æˆ‘ä»¬å¼€å‘äº†å¤šç§ä¸åŒæ¶æ„çš„æ¨¡å‹ï¼Œå¹¶åœ¨PhilEOåŸºå‡†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç»“æœè¡¨æ˜U-Net 200M-2Tåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ°çƒè§‚æµ‹æ¨¡å‹åœ¨å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè¾ƒå°çš„ä¸“é—¨æ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåœ¨åŒ…å«å…¨çƒèŒƒå›´æ•°æ®çš„MajorTOMæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥æå‡æ¨¡å‹åœ¨å¤šæ ·åŒ–ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡æ¢ç´¢ä¸åŒç½‘ç»œæ¶æ„ï¼ˆå¦‚U-Netã€ViTå’ŒMambaï¼‰ï¼Œå®ç°å¯¹å±€éƒ¨å’Œè¿œç¨‹ç›¸å…³æ€§çš„æœ‰æ•ˆæ•æ‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨MajorTOMæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨PhilEOåŸºå‡†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé’ˆå¯¹é“è·¯ã€å»ºç­‘å’ŒåœŸåœ°è¦†ç›–ç­‰ä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å¤§è§„æ¨¡çš„å…¨çƒæ•°æ®é›†è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œè¯æ˜äº†é¢å¤–çš„æµ·æ´‹å’Œå†°å·æ•°æ®ä¸ä¼šå½±å“é™†åœ°ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ‹“å®½äº†æ¨¡å‹çš„åº”ç”¨èŒƒå›´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ï¼Œç‰¹åˆ«æ˜¯U-Netçš„200M-2Tç‰ˆæœ¬åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼ŒåŒæ—¶Mambaæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚å®éªŒä¸­è¿˜è¯„ä¼°äº†æ¨¡å‹æ‰€éœ€çš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒU-Net 200M-2Tåœ¨é“è·¯å’Œå»ºç­‘ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå°¤å…¶åœ¨n-shotå­¦ä¹ åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚åŒæ—¶ï¼ŒMambaæ¨¡å‹åœ¨è®¡ç®—å¼€é”€ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œæä¾›äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†æ›´é«˜çš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŸå¸‚è§„åˆ’ã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³å“åº”ç­‰ã€‚é€šè¿‡æå‡åœ°çƒè§‚æµ‹æ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆç‡ï¼Œå¯ä»¥æ›´å¥½åœ°æ”¯æŒå®æ—¶æ•°æ®åˆ†æå’Œå†³ç­–åˆ¶å®šï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Today, Earth Observation (EO) satellites generate massive volumes of data. To fully exploit this, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for downstream tasks with minimal labeled data. In this paper, we study scaling-up FMs: we train our models on the pretraining dataset MajorTOM 23TB which includes all regions, and the performance on average is competitive versus models pretrained on more specialized datasets which are substantially smaller and include only land. The additional data of oceans and ice do not decrease the performance on land-focused downstream tasks. These results indicate that large FMs trained on global datasets for a wider variety of downstream tasks can be useful for downstream applications that only require a subset of the information included in their training. The second contribution is the exploration of U-Net Convolutional Neural Network (CNN), Vision Transformers (ViT), and Mamba State-Space Models (SSM) as FMs. U-Net captures local correlations amongst pixels, while ViT and Mamba capture local and distant correlations. We develop various models using different architectures, including U-Net, ViT, and Mamba, and different number of parameters. We evaluate the FLoating-point OPerations (FLOPs) needed by the models. We fine-tune on the PhilEO Bench for different downstream tasks: roads, buildings, and land cover. For most n-shots for roads and buildings, U-Net 200M-2T outperforms the other models. Using Mamba, we achieve comparable results on the downstream tasks, with less computational expenses. We also compare with the recent FM TerraMind which we evaluate on PhilEO Bench.

