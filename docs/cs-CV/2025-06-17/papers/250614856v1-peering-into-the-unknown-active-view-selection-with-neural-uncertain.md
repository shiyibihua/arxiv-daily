---
layout: default
title: Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction
---

# Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14856" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14856v1</a>
  <a href="https://arxiv.org/pdf/2506.14856.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14856v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14856v1', 'Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengquan Zhang, Feng Xu, Mengmi Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: 9 pages, 3 figures in the main text. Under review for NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¥ç»ä¸ç¡®å®šæ€§å›¾çš„ä¸»åŠ¨è§†è§’é€‰æ‹©ä»¥ä¼˜åŒ–3Dé‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `ä¸»åŠ¨è§†è§’é€‰æ‹©` `3Dé‡å»º` `ç¥ç»ç½‘ç»œ` `ä¸ç¡®å®šæ€§å›¾` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ ` `é«˜æ•ˆç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¸»åŠ¨è§†è§’é€‰æ‹©ï¼ˆAVSï¼‰åœ¨3Dé‡å»ºä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„è§†è§’ã€‚
2. æœ¬æ–‡æå‡ºUPNetï¼Œé€šè¿‡é¢„æµ‹ç¥ç»ä¸ç¡®å®šæ€§å›¾ï¼Œç›´æ¥ä»è§†è§’å¤–è§‚æ˜ å°„åˆ°ä¸ç¡®å®šæ€§ï¼Œä¼˜åŒ–è§†è§’é€‰æ‹©è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æœ¬æ–¹æ³•çš„è§†è§’æ•°é‡å‡å°‘ä¸€åŠï¼Œä½†é‡å»ºç²¾åº¦ä¸ç«äº‰æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨3Dç‰©ä½“é‡å»ºä¸­ï¼Œä¸åŒè§†è§’æä¾›çš„ä¿¡æ¯é‡å·®å¼‚æ˜¾è‘—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸»åŠ¨è§†è§’é€‰æ‹©ï¼ˆAVSï¼‰æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§å‰é¦ˆæ·±åº¦ç¥ç»ç½‘ç»œUPNeté¢„æµ‹ç¥ç»ä¸ç¡®å®šæ€§å›¾ï¼Œå¸®åŠ©é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’ã€‚UPNetä»å•å¼ è¾“å…¥å›¾åƒä¸­è¾“å‡ºä¸ç¡®å®šæ€§å›¾ï¼Œè¡¨ç¤ºæ‰€æœ‰å€™é€‰è§†è§’çš„ä¸ç¡®å®šæ€§å€¼ã€‚é€šè¿‡èšåˆå…ˆå‰é¢„æµ‹çš„ä¸ç¡®å®šæ€§å›¾ï¼ŒæŠ‘åˆ¶å†—ä½™è§†è§’ï¼Œæœ€ç»ˆé€‰æ‹©å‡ºæœ€æœ‰ä»·å€¼çš„è§†è§’ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ä½¿ç”¨çš„è§†è§’æ•°é‡ä»…ä¸ºä¸Šé™çš„ä¸€åŠï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºç²¾åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨è®¡ç®—å¼€é”€ä¸Šå®ç°é«˜è¾¾400å€çš„åŠ é€Ÿï¼ŒåŒæ—¶CPUã€RAMå’ŒGPUçš„ä½¿ç”¨é‡å‡å°‘è¶…è¿‡50%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸»åŠ¨è§†è§’é€‰æ‹©ï¼ˆAVSï¼‰åœ¨3Dé‡å»ºä¸­çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆè¯†åˆ«å‡ºæœ€å…·ä¿¡æ¯é‡çš„è§†è§’ï¼Œå¯¼è‡´é‡å»ºç²¾åº¦ä¸è¶³å’Œè®¡ç®—èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ä¸ç¡®å®šæ€§å›¾çš„AVSæ–¹æ³•ï¼Œåˆ©ç”¨è½»é‡çº§æ·±åº¦ç¥ç»ç½‘ç»œUPNetï¼Œä»å•å¼ è¾“å…¥å›¾åƒä¸­é¢„æµ‹ä¸ç¡®å®šæ€§å›¾ï¼Œä»¥æ­¤æŒ‡å¯¼è§†è§’é€‰æ‹©ã€‚è¯¥è®¾è®¡æ—¨åœ¨é€šè¿‡ç›´æ¥æ˜ å°„è§†è§’å¤–è§‚ä¸ä¸ç¡®å®šæ€§ï¼Œæå‡é€‰æ‹©æ•ˆç‡å’Œé‡å»ºè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬è¾“å…¥å•å¼ å›¾åƒï¼Œä½¿ç”¨UPNetç”Ÿæˆä¸ç¡®å®šæ€§å›¾ï¼Œèšåˆå¤šä¸ªä¸ç¡®å®šæ€§å›¾ä»¥æŠ‘åˆ¶å†—ä½™è§†è§’ï¼Œæœ€ç»ˆé€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œ3Dé‡å»ºã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬UPNetç½‘ç»œã€è§†è§’èšåˆæ¨¡å—å’Œé‡å»ºæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç¥ç»ä¸ç¡®å®šæ€§å›¾æ¥æŒ‡å¯¼è§†è§’é€‰æ‹©ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºè¾å°„åœºå­¦ä¹ çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šUPNetçš„ç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥å®ç°å¿«é€Ÿæ¨ç†ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†ä¸ç¡®å®šæ€§é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤šç§ç‰©ä½“ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä»…ä½¿ç”¨ä¸Šé™ä¸€åŠçš„è§†è§’ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨é‡å»ºç²¾åº¦ä¸Šä¸å…¶ä»–ç«äº‰AVSæ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œè®¡ç®—å¼€é”€æ˜¾è‘—é™ä½ï¼Œè¾¾åˆ°400å€çš„åŠ é€Ÿï¼ŒCPUã€RAMå’ŒGPUçš„ä½¿ç”¨é‡å‡å°‘è¶…è¿‡50%ï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨3Dé‡å»ºã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ä¼˜åŒ–è§†è§’é€‰æ‹©ï¼Œå¯ä»¥åœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„3Då»ºæ¨¡å’Œç¯å¢ƒç†è§£ï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿è‡³æ›´å¤šæ–°ç‰©ä½“ç±»åˆ«çš„AVSä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æ‹“å±•å…¶åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.

