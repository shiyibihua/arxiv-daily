---
layout: default
title: I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs
---

# I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14495" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14495v1</a>
  <a href="https://arxiv.org/pdf/2506.14495.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14495v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14495v1', 'I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Qi, Lipeng Gu, Honghua Chen, Liangliang Nan, Mingqiang Wei

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpeechReferä»¥è§£å†³å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³è¾“å…¥ä¸‹çš„3Dè§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `è¯­éŸ³è¯†åˆ«` `å¤šæ¨¡æ€èåˆ` `å¯¹æ¯”å­¦ä¹ ` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•ä¾èµ–ç²¾ç¡®æ–‡æœ¬æç¤ºï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å™ªå£°å’Œæ¨¡ç³Šçš„è¯­éŸ³è¾“å…¥ã€‚
2. æå‡ºSpeechReferæ¡†æ¶ï¼Œé€šè¿‡è¯­éŸ³è¡¥å……æ¨¡å—å’Œå¯¹æ¯”è¡¥å……æ¨¡å—å¢å¼ºå¯¹å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³çš„é²æ£’æ€§ã€‚
3. åœ¨SpeechReferå’ŒSpeechNr3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpeechReferæ˜¾è‘—æå‡äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„3Dè§†è§‰å®šä½æ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„æ–‡æœ¬æç¤ºæ¥å®šä½3Dåœºæ™¯ä¸­çš„ç‰©ä½“ã€‚ç„¶è€Œï¼Œç°å®ä¸­çš„è¯­éŸ³è¾“å…¥å¸¸å¸¸å—åˆ°å£éŸ³ã€èƒŒæ™¯å™ªéŸ³å’Œè¯­é€Ÿå˜åŒ–ç­‰å› ç´ çš„å½±å“ï¼Œå¯¼è‡´è½¬å½•é”™è¯¯ï¼Œä»è€Œé™åˆ¶äº†ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†SpeechReferï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºåœ¨å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³è½¬å½•æƒ…å†µä¸‹æ€§èƒ½çš„3Dè§†è§‰å®šä½æ¡†æ¶ã€‚SpeechReferä¸ç°æœ‰çš„3Dè§†è§‰å®šä½æ¨¡å‹æ— ç¼é›†æˆï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šè¯­éŸ³è¡¥å……æ¨¡å—å’Œå¯¹æ¯”è¡¥å……æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpeechReferæ˜¾è‘—æå‡äº†ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•åœ¨å¤„ç†å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³è¾“å…¥æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯ç”±äºè½¬å½•é”™è¯¯å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•å¯¹ç²¾ç¡®æ–‡æœ¬çš„ä¾èµ–ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSpeechReferæ¡†æ¶é€šè¿‡å¼•å…¥è¯­éŸ³è¡¥å……æ¨¡å—å’Œå¯¹æ¯”è¡¥å……æ¨¡å—ï¼Œå‡å°‘å¯¹é”™è¯¯è½¬å½•çš„ä¾èµ–ï¼Œå¢å¼ºç³»ç»Ÿåœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è¯­éŸ³ä¿¡å·ä¸­çš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpeechReferæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­éŸ³è¡¥å……æ¨¡å—ç”¨äºæ•æ‰å£°å­¦ç›¸ä¼¼æ€§å¹¶ç”Ÿæˆè¡¥å……ææ¡ˆåˆ†æ•°ï¼›å¯¹æ¯”è¡¥å……æ¨¡å—åˆ™é€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹é½é”™è¯¯æ–‡æœ¬ç‰¹å¾å’Œç›¸åº”çš„è¯­éŸ³ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è¯­éŸ³è¡¥å……æ¨¡å—å’Œå¯¹æ¯”è¡¥å……æ¨¡å—ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†å•çº¯ä¾èµ–æ–‡æœ¬è½¬å½•ï¼Œè€Œæ˜¯é€šè¿‡å£°å­¦ç‰¹å¾å¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè¯­éŸ³è¡¥å……æ¨¡å—é€šè¿‡åˆ†æå£°å­¦ç›¸ä¼¼æ€§æ¥ç”Ÿæˆè¡¥å……åˆ†æ•°ï¼Œè€Œå¯¹æ¯”è¡¥å……æ¨¡å—åˆ™ä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°æ¥ç¡®ä¿æ–‡æœ¬å’Œè¯­éŸ³ç‰¹å¾çš„å¯¹é½ï¼Œå…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†è°ƒä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒSpeechReferåœ¨SpeechReferå’ŒSpeechNr3Dæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³è¾“å…¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€‚é€šè¿‡æé«˜ç³»ç»Ÿå¯¹å™ªå£°å’Œæ¨¡ç³Šè¯­éŸ³çš„ç†è§£èƒ½åŠ›ï¼ŒSpeechReferèƒ½å¤Ÿåœ¨å®é™…åœºæ™¯ä¸­æä¾›æ›´ä¸ºç›´è§‚å’Œè‡ªç„¶çš„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨å¤šæ¨¡æ€æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.

