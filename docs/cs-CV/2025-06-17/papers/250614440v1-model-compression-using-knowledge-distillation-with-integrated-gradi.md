---
layout: default
title: Model compression using knowledge distillation with integrated gradients
---

# Model compression using knowledge distillation with integrated gradients

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14440" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14440v1</a>
  <a href="https://arxiv.org/pdf/2506.14440.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14440v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14440v1', 'Model compression using knowledge distillation with integrated gradients')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David E. Hernandez, Jose Chang, TorbjÃ¶rn E. M. Nordling

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: 49 pages, 12 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé›†æˆæ¢¯åº¦çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä»¥å®ç°æ¨¡å‹å‹ç¼©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è’¸é¦` `é›†æˆæ¢¯åº¦` `æ·±åº¦å­¦ä¹ ` `æ•°æ®å¢å¼º` `è¾¹ç¼˜è®¡ç®—` `CIFAR-10` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹å‹ç¼©æ–¹æ³•åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶é¢ä¸´å‡†ç¡®ç‡ä¸æ•ˆç‡çš„æƒè¡¡æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é›†æˆæ¢¯åº¦å¢å¼ºçŸ¥è¯†è’¸é¦ï¼Œåˆ©ç”¨IGå›¾ä½œä¸ºæ•°æ®å¢å¼ºç­–ç•¥ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹å¯¹æ•™å¸ˆæ¨¡å‹å†³ç­–çš„ç†è§£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIGå¢å¼ºçš„çŸ¥è¯†è’¸é¦åœ¨CIFAR-10ä¸Šå®ç°92.6%çš„å‡†ç¡®ç‡ï¼Œå‹ç¼©å› å­4.1xï¼Œç›¸è¾ƒäºæœªè’¸é¦æ¨¡å‹æå‡1.1ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹å‹ç¼©å¯¹äºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡é›†æˆæ¢¯åº¦ï¼ˆIGï¼‰å¢å¼ºçŸ¥è¯†è’¸é¦ï¼Œä½œä¸ºæ•°æ®å¢å¼ºç­–ç•¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†IGå›¾å åŠ åˆ°è¾“å…¥å›¾åƒä¸Šï¼Œä½¿å­¦ç”Ÿæ¨¡å‹æ›´æ·±å…¥åœ°ç†è§£æ•™å¸ˆæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨CIFAR-10ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒIGå¢å¼ºçš„çŸ¥è¯†è’¸é¦å®ç°äº†92.6%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œå‹ç¼©å› å­ä¸º4.1xï¼Œç›¸è¾ƒäºæœªè’¸é¦æ¨¡å‹ï¼ˆ91.5%ï¼‰æœ‰æ˜¾è‘—æå‡ï¼ˆp<0.001ï¼‰ã€‚è¯¥æ–¹æ³•å°†æ¨ç†æ—¶é—´ä»140æ¯«ç§’å‡å°‘åˆ°13æ¯«ç§’ï¼Œå¹¶å°†IGå›¾çš„é¢„è®¡ç®—è½¬åŒ–ä¸ºä¸€æ¬¡æ€§é¢„å¤„ç†æ­¥éª¤ã€‚ç»¼åˆå®éªŒåŒ…æ‹¬ä¸æ³¨æ„åŠ›è½¬ç§»çš„æ¯”è¾ƒã€è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿçš„ç»Ÿè®¡ç¨³å¥æ€§éªŒè¯ã€å‹ç¼©å› å­ä¸å‡†ç¡®ç‡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œä»¥åŠåœ¨ä¸CIFAR-10ç±»å¯¹é½çš„ImageNetå­é›†ä¸Šçš„éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæ¨¡å‹å‡†ç¡®ç‡ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„çŸ›ç›¾ã€‚ç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•å¾€å¾€æ— æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å®ç°æ˜¾è‘—çš„å‹ç¼©æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé›†æˆæ¢¯åº¦ï¼ˆIGï¼‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡å°†IGå›¾å åŠ åˆ°è¾“å…¥å›¾åƒä¸Šï¼Œå¢å¼ºå­¦ç”Ÿæ¨¡å‹å¯¹æ•™å¸ˆæ¨¡å‹å†³ç­–è¿‡ç¨‹çš„ç†è§£ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨è®­ç»ƒå‰é¢„è®¡ç®—IGå›¾ï¼›å…¶æ¬¡ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†IGå›¾ä½œä¸ºæ•°æ®å¢å¼ºè¾“å…¥åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚è¿™ä¸€æµç¨‹ä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†é›†æˆæ¢¯åº¦ä¸çŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œåˆ©ç”¨IGå›¾ä½œä¸ºæ•°æ®å¢å¼ºæ‰‹æ®µï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‹ç¼©æ•ˆæœå’Œå‡†ç¡®ç‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„æ¨¡å‹ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸åŒçš„å‹ç¼©å› å­ï¼Œå¹¶ä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†å¤šç§ç½‘ç»œç»“æ„è¿›è¡ŒéªŒè¯ï¼Œä»¥ç¡®ä¿æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIGå¢å¼ºçš„çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨CIFAR-10æ•°æ®é›†ä¸Šè¾¾åˆ°äº†92.6%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œå‹ç¼©å› å­ä¸º4.1xï¼Œç›¸è¾ƒäºæœªè’¸é¦æ¨¡å‹æå‡äº†1.1ä¸ªç™¾åˆ†ç‚¹ï¼ˆp<0.001ï¼‰ï¼Œå¹¶å°†æ¨ç†æ—¶é—´ä»140æ¯«ç§’å‡å°‘åˆ°13æ¯«ç§’ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§»åŠ¨è®¾å¤‡ã€ç‰©è”ç½‘è®¾å¤‡åŠè¾¹ç¼˜è®¡ç®—ç­‰èµ„æºå—é™ç¯å¢ƒã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œæ¨åŠ¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

