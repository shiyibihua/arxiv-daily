---
layout: default
title: MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution
---

# MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14511" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.14511v1</a>
  <a href="https://arxiv.org/pdf/2506.14511.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14511v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14511v1', 'MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhiwen Shao, Yifan Cheng, Feiran Li, Yong Zhou, Xuequan Lu, Yuan Xie, Lizhuang Ma

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-17

**Â§áÊ≥®**: This paper has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence

**DOI**: [10.1109/TPAMI.2025.3581162](https://doi.org/10.1109/TPAMI.2025.3581162)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/CYF-cuber/MOL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MOLÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥ÂæÆË°®ÊÉÖËØÜÂà´‰∏≠ÁöÑÊï∞ÊçÆ‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `ÂæÆË°®ÊÉÖËØÜÂà´` `Ê∑±Â∫¶Â≠¶‰π†` `ÂèòÊç¢Âô®` `ÂõæÂç∑ÁßØ` `ÂÖâÊµÅ‰º∞ËÆ°` `Èù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÊ£ÄÊµã` `ÁâπÂæÅÊèêÂèñ` `ËÅîÂêàËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂæÆË°®ÊÉÖËØÜÂà´Èù¢‰∏¥ÁöÑ‰∏ªË¶ÅÊåëÊàòÊòØÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÊâãÂ∑•ÁâπÂæÅÂíåÊúâÈôêÁöÑÊï∞ÊçÆÈõÜÔºåÂØºËá¥ÊÄßËÉΩÂèóÈôê„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêàÂèòÊç¢Âô®ÂíåÂõæÂç∑ÁßØÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂéüÂßãÂ∏ß‰∏≠ÊèêÂèñÁâπÂæÅÔºåÈÅøÂÖç‰∫ÜÂØπÂÖ≥ÈîÆÂ∏ßÁöÑ‰æùËµñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•Ê°ÜÊû∂Âú®CASME II„ÄÅSAMMÂíåSMICÂü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊúÄÂÖàËøõÁöÑMERÊñπÊ≥ïÔºåÂπ∂ÊúâÊïàÊçïÊçâÈù¢ÈÉ®ÂæÆÂ∞èËÇåËÇâÂä®‰Ωú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂæÆË°®ÊÉÖËØÜÂà´ÔºàMERÔºâÊòØ‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°Ôºå‰∏ªË¶ÅÁî±‰∫éÂæÆË°®ÊÉÖÂä®‰ΩúÁöÑÁû¨Êó∂ÊÄßÂíåÁªÜÂæÆÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊâãÂ∑•ÁâπÂæÅ„ÄÅÂÖ≥ÈîÆÂ∏ßÊàñÂèóÈôê‰∫éÂ∞èËßÑÊ®°Âíå‰ΩéÂ§öÊ†∑ÊÄßÊï∞ÊçÆÈõÜÁöÑÊ∑±Â∫¶ÁΩëÁªú„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ´ØÂà∞Á´ØÁöÑÂæÆÂä®‰ΩúÊÑüÁü•Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÂèòÊç¢Âô®„ÄÅÂõæÂç∑ÁßØÂíå‰º†ÁªüÂç∑ÁßØÁöÑ‰ºòÂäø„ÄÇÁâπÂà´Âú∞ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑF5CÊ®°ÂùóÔºåÁî±ÂÖ®ËøûÊé•Âç∑ÁßØÂíåÈÄöÈÅìÂØπÂ∫îÂç∑ÁßØÁªÑÊàêÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂéüÂßãÂ∏ßÂ∫èÂàó‰∏≠ÊèêÂèñÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÔºåËÄåÊó†ÈúÄÂÖ≥ÈîÆÂ∏ßÁöÑÂÖàÈ™åÁü•ËØÜ„ÄÇÈÄöËøáÂÖ±‰∫´Â±ÄÈÉ®-ÂÖ®Â±ÄÁâπÂæÅÔºåMER„ÄÅÂÖâÊµÅ‰º∞ËÆ°ÂíåÈù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÂÖ±ÂêåËÆ≠ÁªÉÔºå‰ªéËÄåÊçïÊçâÈù¢ÈÉ®ÂæÆÂ¶ôÂä®‰Ωú‰ø°ÊÅØÔºåÁºìËß£ËÆ≠ÁªÉÊï∞ÊçÆ‰∏çË∂≥ÁöÑÂΩ±Âìç„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™Âü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑMERÊñπÊ≥ïÔºåÂπ∂Âú®ÂÖâÊµÅ‰º∞ËÆ°ÂíåÈù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÊñπÈù¢Ë°®Áé∞ËâØÂ•Ω„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂæÆË°®ÊÉÖËØÜÂà´‰∏≠ÁöÑÊï∞ÊçÆ‰∏çË∂≥ÂíåÁâπÂæÅÊèêÂèñÂõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊâãÂ∑•ÁâπÂæÅÂíåÂÖ≥ÈîÆÂ∏ßÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ´ØÂà∞Á´ØÁöÑÂæÆÂä®‰ΩúÊÑüÁü•Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºåÂà©Áî®ÂèòÊç¢Âô®ÂíåÂõæÂç∑ÁßØÁöÑ‰ºòÂäøÔºåËÉΩÂ§ü‰ªéÂéüÂßãËßÜÈ¢ëÂ∏ß‰∏≠Áõ¥Êé•ÊèêÂèñÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÔºåÈÅøÂÖç‰∫ÜÂØπÂÖ≥ÈîÆÂ∏ßÁöÑ‰æùËµñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨F5CÊ®°ÂùóÔºåËØ•Ê®°ÂùóÁî±ÂÖ®ËøûÊé•Âç∑ÁßØÂíåÈÄöÈÅìÂØπÂ∫îÂç∑ÁßØÁªÑÊàêÔºåËÉΩÂ§üÊúâÊïàÊèêÂèñÁâπÂæÅÂπ∂Âª∫Ê®°ÁâπÂæÅÊ®°Âºè‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇÂêåÊó∂ÔºåMER„ÄÅÂÖâÊµÅ‰º∞ËÆ°ÂíåÈù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÊ£ÄÊµã‰ªªÂä°ÂÖ±‰∫´Â±ÄÈÉ®-ÂÖ®Â±ÄÁâπÂæÅËøõË°åËÅîÂêàËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éF5CÊ®°ÂùóÁöÑËÆæËÆ°ÔºåÁªìÂêà‰∫ÜÂÖ®ËøûÊé•Âç∑ÁßØÂíåÂõæÂç∑ÁßØÔºåËÉΩÂ§üÂêåÊó∂ÊçïÊçâÂ±ÄÈÉ®ÁâπÂæÅÂíåÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÁâπÂæÅÊèêÂèñÊñπÂºèÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÔºåÈááÁî®‰∫ÜÂèòÊç¢Âô®È£éÊ†ºÁöÑÂÖ®ËøûÊé•Âç∑ÁßØÊù•ÊèêÂèñÂ±ÄÈÉ®ÁâπÂæÅÔºåÂêåÊó∂ÂºïÂÖ•ÂõæÈ£éÊ†ºÁöÑÈÄöÈÅìÂØπÂ∫îÂç∑ÁßØÊù•Âª∫Ê®°ÁâπÂæÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÔºåËÄÉËôë‰∫ÜMER„ÄÅÂÖâÊµÅ‰º∞ËÆ°ÂíåÈù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÁöÑËÅîÂêà‰ºòÂåñÔºå‰ª•ÊèêÂçáÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®CASME II„ÄÅSAMMÂíåSMICÂü∫ÂáÜ‰∏äÂùáË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊúÄÂÖàËøõÁöÑMERÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂Âú®ÂÖâÊµÅ‰º∞ËÆ°ÂíåÈù¢ÈÉ®ÂÖ≥ÈîÆÁÇπÊ£ÄÊµã‰ªªÂä°‰∏≠‰πüË°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÊçïÊçâÈù¢ÈÉ®ÂæÆÂ∞èÂä®‰ΩúÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®ÂæÆË°®ÊÉÖËØÜÂà´„ÄÅÊÉÖÊÑüÂàÜÊûêÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÂæÆË°®ÊÉÖËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÔºåÂèØ‰ª•Âú®ÂøÉÁêÜÂÅ•Â∫∑ÁõëÊµã„ÄÅÁ§æ‰∫§Êú∫Âô®‰∫∫ÂíåËôöÊãüÁé∞ÂÆûÁ≠âÂú∫ÊôØ‰∏≠ÂÆûÁé∞Êõ¥Ëá™ÁÑ∂ÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñËßÜËßâ‰ªªÂä°ÔºåÂ¶ÇÂä®‰ΩúËØÜÂà´ÂíåË°å‰∏∫ÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.

