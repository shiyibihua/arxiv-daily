---
layout: default
title: DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning
---

# DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14709" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14709v2</a>
  <a href="https://arxiv.org/pdf/2506.14709.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14709v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14709v2', 'DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kunal Swami, Debtanu Gupta, Amrit Kumar Muduli, Chirag Jaiswal, Pankaj Kumar Bajpai

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17 (æ›´æ–°: 2025-07-31)

**å¤‡æ³¨**: Accepted in IROS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiFuse-Netä»¥è§£å†³RGBå’ŒåŒåƒç´ æ·±åº¦ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ·±åº¦ä¼°è®¡` `åŒåƒç´ æŠ€æœ¯` `è·¨æ¨¡æ€å­¦ä¹ ` `ç‰¹å¾èåˆ` `æ™ºèƒ½ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨æˆæœ¬ã€åŠŸè€—å’Œé²æ£’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å°å…‰åœˆçš„æ™ºèƒ½æ‰‹æœºç›¸æœºä¸­ã€‚
2. DiFuse-Neté€šè¿‡çª—å£åŒå‘è§†å·®æ³¨æ„æœºåˆ¶å’Œè·¨æ¨¡æ€è¿ç§»å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„RGBå’ŒåŒåƒç´ æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDiFuse-Netåœ¨æ·±åº¦é¢„æµ‹ä¸Šä¼˜äºä¼ ç»Ÿçš„DPå’Œç«‹ä½“åŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¼°è®¡å¯¹äºæ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œåº”ç”¨èŒƒå›´ä»è‡ªä¸»å¯¼èˆªåˆ°å¢å¼ºç°å®ã€‚ä¼ ç»Ÿçš„ç«‹ä½“å’Œä¸»åŠ¨æ·±åº¦ä¼ æ„Ÿå™¨åœ¨æˆæœ¬ã€åŠŸè€—å’Œé²æ£’æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œè€ŒåŒåƒç´ æŠ€æœ¯ä½œä¸ºç°ä»£ç›¸æœºçš„æ™®éé€‰æ‹©ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†DiFuse-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¨¡æ€è§£è€¦ç½‘ç»œè®¾è®¡ï¼Œç”¨äºåŸºäºRGBå’ŒåŒåƒç´ çš„æ·±åº¦ä¼°è®¡ã€‚DiFuse-Neté‡‡ç”¨äº†çª—å£åŒå‘è§†å·®æ³¨æ„æœºåˆ¶ï¼ˆWBiPAMï¼‰ï¼Œæ—¨åœ¨æ•æ‰æ™ºèƒ½æ‰‹æœºç›¸æœºç‰¹æœ‰çš„å°å…‰åœˆä¸‹çš„å¾®å¦™DPè§†å·®çº¿ç´¢ã€‚é€šè¿‡ç‹¬ç«‹çš„ç¼–ç å™¨æå–RGBå›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾èåˆä»¥å¢å¼ºæ·±åº¦é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€è¿ç§»å­¦ä¹ æœºåˆ¶ï¼ˆCmTLï¼‰ï¼Œä»¥åˆ©ç”¨æ–‡çŒ®ä¸­çš„å¤§è§„æ¨¡RGB-Dæ•°æ®é›†ï¼Œå…‹æœè·å–å¤§è§„æ¨¡RGB-DP-Dæ•°æ®é›†çš„å±€é™ã€‚æˆ‘ä»¬çš„è¯„ä¼°å’Œæ¯”è¾ƒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºäºDPå’Œç«‹ä½“çš„åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡çœŸå®ä¸–ç•ŒRGB-DP-Dè®­ç»ƒæ•°æ®é›†ï¼Œç§°ä¸ºåŒæ‘„åƒå¤´åŒåƒç´ ï¼ˆDCDPï¼‰æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³RGBå’ŒåŒåƒç´ æ·±åº¦ä¼°è®¡ä¸­çš„æ¨¡æ€èåˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å°å…‰åœˆç›¸æœºä¸‹çš„è§†å·®æ•æ‰èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiFuse-Neté€šè¿‡çª—å£åŒå‘è§†å·®æ³¨æ„æœºåˆ¶ï¼ˆWBiPAMï¼‰æœ‰æ•ˆæ•æ‰DPè§†å·®çº¿ç´¢ï¼Œå¹¶ç»“åˆç‹¬ç«‹çš„RGBç‰¹å¾æå–ï¼Œå¢å¼ºæ·±åº¦é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiFuse-Netçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªç”¨äºRGBå›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æå–çš„ç¼–ç å™¨ï¼Œå¦ä¸€ä¸ªç”¨äºDPè§†å·®çš„æ³¨æ„æœºåˆ¶ã€‚é€šè¿‡ç‰¹å¾èåˆï¼Œæå‡æ·±åº¦ä¼°è®¡çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºçª—å£åŒå‘è§†å·®æ³¨æ„æœºåˆ¶çš„è®¾è®¡ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å°å…‰åœˆç›¸æœºçš„è§†å·®ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜äº†æ·±åº¦ä¼°è®¡çš„ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šç½‘ç»œç»“æ„ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ·±åº¦é¢„æµ‹ï¼Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiFuse-Netåœ¨æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»ŸDPå’Œç«‹ä½“æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†çº¦15%ã€‚æ­¤å¤–ï¼ŒDCDPæ•°æ®é›†çš„å¼•å…¥ä¸ºåç»­ç ”ç©¶æä¾›äº†é«˜è´¨é‡çš„è®­ç»ƒåŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»é©¾é©¶ã€å¢å¼ºç°å®å’Œæœºå™¨äººå¯¼èˆªç­‰æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›é¢†åŸŸçš„æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒDiFuse-Netæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½è®¾å¤‡çš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.

