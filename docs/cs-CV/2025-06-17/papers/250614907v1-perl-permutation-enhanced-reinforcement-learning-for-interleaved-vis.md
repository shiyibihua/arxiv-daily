---
layout: default
title: PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning
---

# PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14907" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14907v1</a>
  <a href="https://arxiv.org/pdf/2506.14907.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14907v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14907v1', 'PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-zhi Li, Peijie Wang, Jie Wu, Lei Ji, Yelong Shen, Yujiu Yang, Yeyun Gong

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPeRLä»¥è§£å†³å¤šå›¾åƒæ¨ç†ä¸­çš„ç©ºé—´å…³ç³»ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç©ºé—´å…³ç³»ç†è§£` `å›¾åƒåºåˆ—æ’åˆ—` `å›æ»šè¿‡æ»¤æœºåˆ¶` `å­¦ä¹ æ•ˆç‡` `å¤æ‚åœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦å±€é™äºå•å›¾åƒçš„ç©ºé—´æ¨ç†ï¼Œéš¾ä»¥å¤„ç†å¤šå›¾åƒé—´çš„å¤æ‚å…³ç³»ã€‚
2. æœ¬æ–‡æå‡ºPeRLæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å›¾åƒåºåˆ—çš„æ’åˆ—å’Œå›æ»šè¿‡æ»¤æœºåˆ¶ï¼Œå¢å¼ºå¤šæ¨¡æ€ä»»åŠ¡çš„å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPeRLåœ¨5ä¸ªå¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸”åœ¨å•å›¾åƒä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å—DeepSeek-R1ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å±•ç¤ºå‡ºçš„æ¨ç†èƒ½åŠ›å¯å‘ï¼Œè¿‘æœŸç ”ç©¶å¼€å§‹æ¢ç´¢åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦å±€é™äºå•å›¾åƒçš„ç©ºé—´æ¨ç†ï¼Œéš¾ä»¥æ¨å¹¿åˆ°æ¶‰åŠå¤šå›¾åƒä½ç½®æ¨ç†çš„å¤æ‚ç°å®åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹äº¤é”™å¤šæ¨¡æ€ä»»åŠ¡çš„é€šç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•PeRLï¼Œå¹¶è®¾è®¡äº†å¤šé˜¶æ®µç­–ç•¥ä»¥å¢å¼ºæ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œä»»åŠ¡è¡¨ç°ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥å›¾åƒåºåˆ—çš„æ’åˆ—ä»¥æ¨¡æ‹Ÿä¸åŒçš„ç©ºé—´å…³ç³»ï¼Œæ¢ç´¢æ›´å¤šçš„ç©ºé—´å’Œä½ç½®å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å›æ»šè¿‡æ»¤æœºåˆ¶ï¼Œé‡æ–°é‡‡æ ·ä»¥èšç„¦äºå¯¹å­¦ä¹ æœ€ä¼˜è¡Œä¸ºè´¡çŒ®æœ€å¤§çš„è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPeRLæ¨¡å‹åœ¨å¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç›¸å…³åŸºçº¿ï¼ŒåŒæ—¶åœ¨å•å›¾åƒä»»åŠ¡ä¸­ä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤šå›¾åƒä½ç½®æ¨ç†æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ç†è§£å›¾åƒé—´å…³ç³»çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºPeRLæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å›¾åƒåºåˆ—çš„æ’åˆ—æ¥æ¨¡æ‹Ÿä¸åŒçš„ç©ºé—´å…³ç³»ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´å’Œä½ç½®å¤šæ ·æ€§ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPeRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡æ’åˆ—å›¾åƒåºåˆ—è¿›è¡Œå¤šæ ·æ€§æ¢ç´¢ï¼Œç„¶ååº”ç”¨å›æ»šè¿‡æ»¤æœºåˆ¶å¯¹è½¨è¿¹è¿›è¡Œé‡æ–°é‡‡æ ·ï¼Œèšç„¦äºæœ€ä¼˜è¡Œä¸ºçš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å›¾åƒåºåˆ—çš„æ’åˆ—æœºåˆ¶å’Œå›æ»šè¿‡æ»¤ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€ç©ºé—´æ¨ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒPeRLé‡‡ç”¨äº†å¤šé˜¶æ®µç­–ç•¥ä»¥ä¼˜åŒ–æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™æ³¨é‡å¯¹æœ€ä¼˜è¡Œä¸ºçš„å¼ºåŒ–å­¦ä¹ ï¼Œç½‘ç»œç»“æ„ä¸Šç»“åˆäº†å›¾åƒåºåˆ—çš„æ’åˆ—ä¸è¿‡æ»¤æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPeRLæ¨¡å‹åœ¨5ä¸ªå¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†R1ç›¸å…³å’Œäº¤é”™VLMåŸºçº¿ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚åŒæ—¶ï¼Œåœ¨3ä¸ªå•å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒæ£€ç´¢ã€ä»¥åŠå¤šæ¨¡æ€äº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å¤šå›¾åƒæ¨ç†èƒ½åŠ›ï¼ŒPeRLèƒ½å¤Ÿåœ¨å¤æ‚çš„ç°å®ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„æ¨ç†ç»“æœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks.

