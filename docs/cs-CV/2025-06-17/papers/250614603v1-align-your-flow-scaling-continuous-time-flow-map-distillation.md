---
layout: default
title: Align Your Flow: Scaling Continuous-Time Flow Map Distillation
---

# Align Your Flow: Scaling Continuous-Time Flow Map Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14603" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14603v1</a>
  <a href="https://arxiv.org/pdf/2506.14603.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14603v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14603v1', 'Align Your Flow: Scaling Continuous-Time Flow Map Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-17

**å¤‡æ³¨**: Project page: https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿ç»­æ—¶é—´æµå›¾è’¸é¦æ–¹æ³•ä»¥æå‡ç”Ÿæˆæ¨¡å‹æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç”Ÿæˆæ¨¡å‹` `æµå›¾` `è’¸é¦è®­ç»ƒ` `å›¾åƒç”Ÿæˆ` `è‡ªå¼•å¯¼` `å¯¹æŠ—å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£å’ŒæµåŸºæ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦å¤šä¸ªé‡‡æ ·æ­¥éª¤ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œä¸”ä¸€è‡´æ€§æ¨¡å‹åœ¨å¢åŠ æ­¥éª¤æ—¶æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†æ–°çš„è¿ç»­æ—¶é—´ç›®æ ‡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æµå›¾è¿æ¥ä¸åŒå™ªå£°æ°´å¹³ï¼Œä»è€Œæå‡ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚
3. åœ¨å¤šä¸ªå›¾åƒç”ŸæˆåŸºå‡†ä¸Šï¼ŒAlign Your Flowæ¨¡å‹å±•ç¤ºäº†åœ¨å°‘æ­¥ç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨å°å‹é«˜æ•ˆç¥ç»ç½‘ç»œä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£å’ŒæµåŸºæ¨¡å‹å·²æˆä¸ºæœ€å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œä½†éœ€è¦å¤šä¸ªé‡‡æ ·æ­¥éª¤ã€‚å°½ç®¡ä¸€è‡´æ€§æ¨¡å‹å¯ä»¥å°†è¿™äº›æ¨¡å‹è’¸é¦ä¸ºé«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆå™¨ï¼Œä½†å…¶æ€§èƒ½åœ¨å¢åŠ æ­¥éª¤æ—¶ä¸å¯é¿å…åœ°ä¸‹é™ã€‚æµå›¾é€šè¿‡åœ¨å•æ­¥ä¸­è¿æ¥ä»»æ„ä¸¤ä¸ªå™ªå£°æ°´å¹³æ¥æ¨å¹¿è¿™äº›æ–¹æ³•ï¼Œå¹¶åœ¨æ‰€æœ‰æ­¥éª¤è®¡æ•°ä¸­ä¿æŒæœ‰æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„è¿ç»­æ—¶é—´ç›®æ ‡ç”¨äºè®­ç»ƒæµå›¾ï¼Œå¹¶å±•ç¤ºäº†è‡ªå¼•å¯¼å’Œå¯¹æŠ—å¾®è°ƒå¯¹æ€§èƒ½çš„æå‡ã€‚æˆ‘ä»¬åœ¨å›¾åƒç”ŸæˆåŸºå‡†ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æµå›¾æ¨¡å‹ï¼Œç§°ä¸ºAlign Your Flowï¼Œå¹¶åœ¨ImageNet 64x64å’Œ512x512ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å°‘æ­¥ç”Ÿæˆæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªé‡‡æ ·æ­¥éª¤ä¸‹æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä¸€è‡´æ€§æ¨¡å‹åœ¨å¢åŠ æ­¥éª¤æ—¶æ€§èƒ½ä¸‹é™çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è¿ç»­æ—¶é—´ç›®æ ‡ï¼Œæµå›¾èƒ½å¤Ÿåœ¨å•æ­¥ä¸­è¿æ¥ä¸åŒå™ªå£°æ°´å¹³ï¼Œä»è€Œä¿æŒåœ¨æ‰€æœ‰æ­¥éª¤è®¡æ•°ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œæå‡ç”Ÿæˆæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æµå›¾çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨æ–°çš„è¿ç»­æ—¶é—´ç›®æ ‡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œç»“åˆè‡ªå¼•å¯¼å’Œå¯¹æŠ—å¾®è°ƒï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„ç”Ÿæˆæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ–°çš„è¿ç»­æ—¶é—´ç›®æ ‡å’Œè®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æµå›¾ä¸ä¸€è‡´æ€§æ¨¡å‹å’ŒæµåŒ¹é…ç›®æ ‡ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡ç”Ÿæˆæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ä½è´¨é‡æ¨¡å‹è¿›è¡Œè‡ªå¼•å¯¼ï¼Œå¹¶é€šè¿‡å¯¹æŠ—å¾®è°ƒæ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç¡®ä¿æ ·æœ¬å¤šæ ·æ€§æŸå¤±æœ€å°åŒ–ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨è®ºæ–‡ä¸­è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ImageNet 64x64å’Œ512x512çš„å›¾åƒç”ŸæˆåŸºå‡†ä¸Šï¼ŒAlign Your Flowæ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„å°‘æ­¥ç”Ÿæˆæ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„éå¯¹æŠ—è®­ç»ƒçš„å°‘æ­¥é‡‡æ ·å™¨ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒåˆæˆç­‰ï¼Œèƒ½å¤Ÿä¸ºè‰ºæœ¯åˆ›ä½œã€è™šæ‹Ÿç°å®å’Œæ¸¸æˆå¼€å‘ç­‰è¡Œä¸šæä¾›é«˜æ•ˆçš„ç”Ÿæˆå·¥å…·ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

