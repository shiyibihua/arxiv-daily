---
layout: default
title: Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks
---

# Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks

**arXiv**: [2511.10008v1](https://arxiv.org/abs/2511.10008) | [PDF](https://arxiv.org/pdf/2511.10008.pdf)

**ä½œè€…**: Xuancun Lu, Jiaxiang Chen, Shilin Xiao, Zizhi Jin, Zhangrui Chen, Hanwen Yu, Bohan Qian, Ruochen Zhou, Xiaoyu Ji, Wenyuan Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç‰©ç†ä¼ æ„Ÿå™¨æ”»å‡»æ¡†æž¶ä¸Žé˜²å¾¡æ–¹æ³•ä»¥å¢žå¼ºVLAæ¨¡åž‹åœ¨æœºå™¨äººç³»ç»Ÿä¸­çš„é²æ£’æ€§**

**å…³é”®è¯**: `VLAæ¨¡åž‹` `ç‰©ç†ä¼ æ„Ÿå™¨æ”»å‡»` `é²æ£’æ€§å¢žå¼º` `å¯¹æŠ—è®­ç»ƒ` `æœºå™¨äººå®‰å…¨` `å¤šæ¨¡æ€é›†æˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹å¯¹ç‰©ç†ä¸–ç•Œä¼ æ„Ÿå™¨æ”»å‡»çš„å®‰å…¨æ€§æœªå……åˆ†æŽ¢ç´¢ï¼Œå­˜åœ¨æ˜¾è‘—æ¼æ´žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Real-Sim-Realæ¡†æž¶ï¼Œæ¨¡æ‹Ÿå¹¶éªŒè¯é’ˆå¯¹æ‘„åƒå¤´å’Œéº¦å…‹é£Žçš„ç‰©ç†æ”»å‡»å‘é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡å¤§è§„æ¨¡è¯„ä¼°æ­ç¤ºæ¼æ´žæ¨¡å¼ï¼Œå¹¶å¼€å‘åŸºäºŽå¯¹æŠ—è®­ç»ƒçš„é˜²å¾¡æ–¹æ³•æå‡é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored.
>   To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel ``Real-Sim-Real'' framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.

