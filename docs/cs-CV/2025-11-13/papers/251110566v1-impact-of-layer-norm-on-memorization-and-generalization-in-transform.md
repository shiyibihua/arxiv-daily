---
layout: default
title: Impact of Layer Norm on Memorization and Generalization in Transformers
---

# Impact of Layer Norm on Memorization and Generalization in Transformers

**arXiv**: [2511.10566v1](https://arxiv.org/abs/2511.10566) | [PDF](https://arxiv.org/pdf/2511.10566.pdf)

**ä½œè€…**: Rishi Singhal, Jung-Eun Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æžLayerNormåœ¨Pre-ä¸ŽPost-LayerNorm Transformerä¸­å¯¹è®°å¿†åŒ–å’Œå­¦ä¹ çš„å½±å“**

**å…³é”®è¯**: `LayerNorm` `Transformeræž¶æž„` `è®°å¿†åŒ–åˆ†æž` `å­¦ä¹ ç¨³å®šæ€§` `è§†è§‰è¯­è¨€æ•°æ®é›†` `æ¢¯åº¦æµä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLayerNormåœ¨Pre-å’ŒPost-LayerNorm Transformerä¸­å¯¹è®°å¿†åŒ–å’Œå­¦ä¹ çš„ä½œç”¨æœºåˆ¶ä¸æ˜Žç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¯”è¾ƒPre-å’ŒPost-LayerNormæž¶æž„ï¼Œç§»é™¤LayerNormå‚æ•°ä»¥åˆ†æžå…¶å¯¹ç¨³å®šæ€§å’Œè®°å¿†åŒ–çš„å½±å“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨6ä¸ªè§†è§‰å’Œè¯­è¨€æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå‘çŽ°æ—©æœŸå±‚LayerNormæœ€å…³é”®ï¼Œå½±å“å› æž¶æž„è€Œå¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.

