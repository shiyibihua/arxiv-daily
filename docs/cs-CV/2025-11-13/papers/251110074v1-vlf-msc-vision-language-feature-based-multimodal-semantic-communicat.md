---
layout: default
title: VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System
---

# VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System

**arXiv**: [2511.10074v1](https://arxiv.org/abs/2511.10074) | [PDF](https://arxiv.org/pdf/2511.10074.pdf)

**ä½œè€…**: Gwangyeon Ahn, Jiwan Seo, Joonhyuk Kang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè§†è§‰è¯­è¨€ç‰¹å¾çš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œä»¥ç»Ÿä¸€è¡¨ç¤ºæ”¯æŒå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆï¼Œæå‡é¢‘è°±æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡` `è§†è§‰è¯­è¨€ç‰¹å¾` `å›¾åƒç”Ÿæˆ` `æ–‡æœ¬ç”Ÿæˆ` `é¢‘è°±æ•ˆçŽ‡` `ä¿¡é“å™ªå£°é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è¯­ä¹‰é€šä¿¡ç³»ç»Ÿéœ€ç‹¬ç«‹å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œå¯¼è‡´é¢‘è°±æ•ˆçŽ‡ä½Žå’Œé€‚åº”æ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡åž‹ç¼–ç å›¾åƒä¸ºç´§å‡‘ç‰¹å¾ï¼Œä¼ è¾“åŽé©±åŠ¨æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä½Žä¿¡å™ªæ¯”ä¸‹ï¼Œç³»ç»Ÿä¼˜äºŽå•æ¨¡æ€åŸºçº¿ï¼Œè¯­ä¹‰å‡†ç¡®åº¦é«˜ä¸”å¸¦å®½æ˜¾è‘—å‡å°‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.

