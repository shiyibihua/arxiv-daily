---
layout: default
title: GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs
---

# GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs

**arXiv**: [2511.10081v1](https://arxiv.org/abs/2511.10081) | [PDF](https://arxiv.org/pdf/2511.10081.pdf)

**ä½œè€…**: Yuxiang Duan, Ao Li, Yingqin Li, Luyu Li, Pengwei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGridPruneæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ä¸­è§†è§‰ä»¤ç‰Œä¿®å‰ªçš„ä½Žæ•ˆé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰ä»¤ç‰Œä¿®å‰ª` `ç©ºé—´åˆ†é…` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–` `åŒºåŸŸé€‰æ‹©æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰ä»¤ç‰Œä¿®å‰ªæ–¹æ³•å¿½ç•¥ç©ºé—´åˆ†é…ï¼Œå¯¼è‡´è®¡ç®—æ•ˆçŽ‡ä½Žå’Œä½ç½®åå·®
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µåŒºåŸŸé€‰æ‹©ï¼Œå…ˆå…¨å±€åˆ†é…ä»¤ç‰Œé¢„ç®—ï¼Œå†å±€éƒ¨ç²¾ç»†é€‰æ‹©
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLaVA-NeXT-7Bä¸Šï¼Œä½¿ç”¨11.1%ä»¤ç‰Œä¿ç•™96.98%æ€§èƒ½ï¼Œä¼˜äºŽåŸºçº¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have shown remarkable capabilities in a wide range of vision-language tasks. However, the large number of visual tokens introduces significant computational overhead. To address this issue, visual token pruning has emerged as a key technique for enhancing the efficiency of MLLMs. In cognitive science, humans tend to first determine which regions of a scene to attend to ("where to look") before deciding which specific elements within those regions to process in detail ("what to select"). This two-stage strategy enables the visual system to efficiently allocate attention at a coarse spatial level before performing fine-grained selection. However, existing pruning methods primarily focus on directly optimizing "what to select", typically using attention scores or similarity metrics. They rarely consider "where to look", which has been shown to lead to inefficient spatial allocation, positional bias, and the retention of irrelevant or redundant tokens. In this paper, we propose GridPrune, a method that replaces the global Top-K mechanism with a "guide-globally, select-locally" zonal selection system. GridPrune splits the pruning process into two steps: first, it uses text-conditional guidance to dynamically allocate a token budget across spatial zones; and then, it performs local selection within each budgeted zone. Experimental results demonstrate that GridPrune achieves superior performance across various MLLM architectures. On LLaVA-NeXT-7B, GridPrune retains 96.98% of the full performance while using 11.1% of the tokens, outperforming the best-performing baseline by 2.34% at the same pruning rate.

