---
layout: default
title: Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning
---

# Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning

**arXiv**: [2511.09893v1](https://arxiv.org/abs/2511.09893) | [PDF](https://arxiv.org/pdf/2511.09893.pdf)

**ä½œè€…**: Zubia Naz, Farhan Asghar, Muhammad Ishfaq Hussain, Yahya Hadadi, Muhammad Aasim Rafique, Wookjin Choi, Moongu Jeon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒºåŸŸæ³¨æ„åŠ›å¢žå¼ºSwin-BARTæ¨¡åž‹ä»¥æå‡åŒ»å­¦å›¾åƒæè¿°ä¸´åºŠå‡†ç¡®æ€§**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒæè¿°` `åŒºåŸŸæ³¨æ„åŠ›` `Swin-BARTæ¨¡åž‹` `ROCOæ•°æ®é›†` `è¯­ä¹‰ä¿çœŸåº¦` `ä¸´åºŠæŠ¥å‘Šæ”¯æŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨åŒ–åŒ»å­¦å›¾åƒæè¿°éœ€ç”Ÿæˆè¯Šæ–­æ€§å™è¿°ï¼Œæ”¯æŒæŠ¥å‘Šå·¥ä½œæµã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆSwin-BARTç¼–ç å™¨-è§£ç å™¨ï¼Œæ·»åŠ è½»é‡çº§åŒºåŸŸæ³¨æ„åŠ›æ¨¡å—å¢žå¼ºè¯Šæ–­åŒºåŸŸã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ROCOæ•°æ®é›†ä¸Šå®žçŽ°SOTAè¯­ä¹‰ä¿çœŸåº¦ï¼ŒROUGEå’ŒBERTScoreæ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\_repeat\_ngram\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.

