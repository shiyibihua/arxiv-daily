---
layout: default
title: Dynamic Avatar-Scene Rendering from Human-centric Context
---

# Dynamic Avatar-Scene Rendering from Human-centric Context

**arXiv**: [2511.10539v1](https://arxiv.org/abs/2511.10539) | [PDF](https://arxiv.org/pdf/2511.10539.pdf)

**ä½œè€…**: Wenqing Wang, Haosen Yang, Josef Kittler, Xiatian Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeparate-then-Mapç­–ç•¥ä»¥è§£å†³åŠ¨æ€äºº-åœºæ™¯æ¸²æŸ“ä¸­çš„ç©ºé—´ä¸ä¸€è‡´é—®é¢˜**

**å…³é”®è¯**: `åŠ¨æ€äººä½“é‡å»º` `ç¥žç»æ¸²æŸ“` `å•ç›®è§†é¢‘` `äºº-åœºæ™¯äº¤äº’` `é«˜æ–¯å±žæ€§æ˜ å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¿½è§†äºº-åœºæ™¯ç»„ä»¶é—´ä¿¡æ¯äº¤æ¢ï¼Œå¯¼è‡´è¾¹ç•Œå¤„ç©ºé—´ä¸ä¸€è‡´å’Œè§†è§‰ä¼ªå½±
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ä¸“ç”¨ä¿¡æ¯æ˜ å°„æœºåˆ¶ï¼Œç»Ÿä¸€åˆ†ç¦»å»ºæ¨¡ç»„ä»¶ï¼Œæå‡è®¡ç®—æ•ˆçŽ‡å’Œè§†è§‰è¿žè´¯æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å•ç›®è§†é¢‘æ•°æ®é›†ä¸Šï¼Œè§†è§‰è´¨é‡å’Œæ¸²æŸ“ç²¾åº¦æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.

