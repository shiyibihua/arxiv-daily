---
layout: default
title: Dynamic Avatar-Scene Rendering from Human-centric Context
---

# Dynamic Avatar-Scene Rendering from Human-centric Context

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.10539" target="_blank" class="toolbar-btn">arXiv: 2511.10539v1</a>
    <a href="https://arxiv.org/pdf/2511.10539.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10539v1" 
            onclick="toggleFavorite(this, '2511.10539v1', 'Dynamic Avatar-Scene Rendering from Human-centric Context')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Wenqing Wang, Haosen Yang, Josef Kittler, Xiatian Zhu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-13

**å¤‡æ³¨**: 13 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeparate-then-Mapç­–ç•¥ï¼Œè§£å†³å•ç›®è§†é¢‘ä¸­åŠ¨æ€äººä¸åœºæ™¯äº¤äº’çš„ç¥ç»æ¸²æŸ“é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `ç¥ç»æ¸²æŸ“` `åŠ¨æ€åœºæ™¯é‡å»º` `äººä½“å»ºæ¨¡` `å•ç›®è§†é¢‘` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å•ç›®è§†é¢‘ä¸­é‡å»ºåŠ¨æ€äººä¸åœºæ™¯äº¤äº’æ—¶ï¼Œéš¾ä»¥å…¼é¡¾äººä½“è¿åŠ¨ç‰¹æ€§å’Œåœºæ™¯ä¸€è‡´æ€§ã€‚
2. Separate-then-Mapç­–ç•¥é€šè¿‡ä¿¡æ¯æ˜ å°„æœºåˆ¶è¿æ¥ç‹¬ç«‹å»ºæ¨¡çš„äººä½“å’Œåœºæ™¯ï¼Œå®ç°é«˜æ•ˆä¸”è¿è´¯çš„æ¸²æŸ“ã€‚
3. å®éªŒè¯æ˜ï¼ŒStMåœ¨è§†è§‰è´¨é‡å’Œæ¸²æŸ“ç²¾åº¦ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå°¤å…¶åœ¨äººä½“-åœºæ™¯äº¤äº’è¾¹ç•Œè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘ä¸­é‡å»ºä¸çœŸå®ç¯å¢ƒäº¤äº’çš„åŠ¨æ€äººä½“è¿™ä¸€é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡4Dç¥ç»æ¸²æŸ“å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆæ•´ä½“å»ºæ¨¡åŠ¨æ€åœºæ™¯ï¼Œè¦ä¹ˆåˆ†åˆ«å»ºæ¨¡åœºæ™¯å’ŒèƒŒæ™¯ï¼Œå¹¶å¼•å…¥å‚æ•°åŒ–çš„äººä½“å…ˆéªŒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆå¿½ç•¥äº†åœºæ™¯ä¸­å„ä¸ªç»„æˆéƒ¨åˆ†ï¼ˆç‰¹åˆ«æ˜¯äººä½“ï¼‰çš„ä¸åŒè¿åŠ¨ç‰¹å¾ï¼Œå¯¼è‡´é‡å»ºä¸å®Œæ•´ï¼Œè¦ä¹ˆå¿½ç•¥äº†å•ç‹¬å»ºæ¨¡çš„ç»„ä»¶ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œå¯¼è‡´äººä½“-åœºæ™¯è¾¹ç•Œå¤„å‡ºç°ç©ºé—´ä¸ä¸€è‡´å’Œè§†è§‰ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Separate-then-Map (StM)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¼•å…¥äº†ä¸€ç§ä¸“é—¨çš„ä¿¡æ¯æ˜ å°„æœºåˆ¶æ¥æ¡¥æ¥å•ç‹¬å®šä¹‰å’Œä¼˜åŒ–çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¯ä¸ªé«˜æ–¯å±æ€§é‡‡ç”¨å…±äº«å˜æ¢å‡½æ•°æ¥ç»Ÿä¸€å•ç‹¬å»ºæ¨¡çš„ç»„ä»¶ï¼Œé€šè¿‡é¿å…è¯¦å°½çš„æˆå¯¹äº¤äº’æ¥æé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ç¡®ä¿äººä½“åŠå…¶å‘¨å›´ç¯å¢ƒä¹‹é—´çš„ç©ºé—´å’Œè§†è§‰è¿è´¯æ€§ã€‚åœ¨å•ç›®è§†é¢‘æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStMåœ¨è§†è§‰è´¨é‡å’Œæ¸²æŸ“ç²¾åº¦æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„äººä½“-åœºæ™¯äº¤äº’è¾¹ç•Œå¤„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘ä¸­é‡å»ºåŠ¨æ€äººä½“ä¸çœŸå®åœºæ™¯äº¤äº’çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œè¦ä¹ˆæ•´ä½“å»ºæ¨¡å¿½ç•¥äº†äººä½“è¿åŠ¨çš„ç‰¹æ®Šæ€§ï¼Œå¯¼è‡´é‡å»ºä¸å®Œæ•´ï¼›è¦ä¹ˆåˆ†åˆ«å»ºæ¨¡ï¼Œå¿½ç•¥äº†äººä½“å’Œåœºæ™¯ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ï¼Œå¯¼è‡´è¾¹ç•Œå¤„å‡ºç°ä¼ªå½±å’Œä¸ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯â€œSeparate-then-Mapâ€ï¼Œå³å…ˆåˆ†åˆ«å»ºæ¨¡äººä½“å’Œåœºæ™¯ï¼Œç„¶åé€šè¿‡ä¿¡æ¯æ˜ å°„æœºåˆ¶å°†äºŒè€…æ¡¥æ¥èµ·æ¥ã€‚è¿™ç§è®¾è®¡å…è®¸å¯¹äººä½“å’Œåœºæ™¯è¿›è¡Œæ›´ç²¾ç»†çš„ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿è¯å®ƒä»¬ä¹‹é—´çš„ç©ºé—´å’Œè§†è§‰ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåˆ†åˆ«å¯¹äººä½“å’Œåœºæ™¯è¿›è¡Œå»ºæ¨¡å’Œä¼˜åŒ–ã€‚ç„¶åï¼Œå¼•å…¥ä¸€ä¸ªä¿¡æ¯æ˜ å°„æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å…±äº«çš„å˜æ¢å‡½æ•°æ¥ç»Ÿä¸€ç‹¬ç«‹å»ºæ¨¡çš„ç»„ä»¶ã€‚è¿™ä¸ªå˜æ¢å‡½æ•°ä½œç”¨äºæ¯ä¸ªé«˜æ–¯å±æ€§ï¼Œä»è€Œåœ¨äººä½“å’Œåœºæ™¯ä¹‹é—´å»ºç«‹å¯¹åº”å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºSeparate-then-Mapç­–ç•¥ä»¥åŠå…±äº«å˜æ¢å‡½æ•°çš„è®¾è®¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒStMèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººä½“è¿åŠ¨çš„ç‰¹æ®Šæ€§ï¼ŒåŒæ—¶ä¿è¯äººä½“å’Œåœºæ™¯ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ã€‚å…±äº«å˜æ¢å‡½æ•°é¿å…äº†è¯¦å°½çš„æˆå¯¹äº¤äº’ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨é«˜æ–¯è¡¨ç¤ºæ¥å»ºæ¨¡äººä½“å’Œåœºæ™¯ã€‚å…±äº«å˜æ¢å‡½æ•°çš„å…·ä½“å½¢å¼æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯ç»Ÿä¸€ä¸åŒç»„ä»¶çš„é«˜æ–¯å±æ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘é‡å»ºç²¾åº¦ã€è§†è§‰è´¨é‡å’Œç©ºé—´ä¸€è‡´æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰è¯¦ç»†æè¿°ï¼Œæ­¤å¤„æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒStMæ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ¸²æŸ“ç²¾åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å°¤å…¶æ˜¯åœ¨äººä½“ä¸åœºæ™¯äº¤äº’çš„è¾¹ç•ŒåŒºåŸŸï¼ŒStMèƒ½å¤Ÿç”Ÿæˆæ›´æ¸…æ™°ã€æ›´è‡ªç„¶çš„æ¸²æŸ“ç»“æœï¼Œæœ‰æ•ˆå‡å°‘äº†è§†è§‰ä¼ªå½±ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸï¼Œå®ç°æ›´é€¼çœŸã€è‡ªç„¶çš„è™šæ‹Ÿäººä¸çœŸå®ç¯å¢ƒçš„äº¤äº’ã€‚ä¾‹å¦‚ï¼Œåœ¨VRæ¸¸æˆä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯å°†ç©å®¶çš„è™šæ‹ŸåŒ–èº«æ— ç¼åœ°èå…¥åˆ°æ¸¸æˆåœºæ™¯ä¸­ï¼Œæå‡æ²‰æµ¸æ„Ÿå’Œäº¤äº’ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºç”µå½±åˆ¶ä½œã€è¿œç¨‹åä½œç­‰åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.

