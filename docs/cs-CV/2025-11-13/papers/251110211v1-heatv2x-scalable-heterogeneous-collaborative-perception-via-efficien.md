---
layout: default
title: HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction
---

# HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction

**arXiv**: [2511.10211v1](https://arxiv.org/abs/2511.10211) | [PDF](https://arxiv.org/pdf/2511.10211.pdf)

**ä½œè€…**: Yueran Zhao, Zhang Zhang, Chao Sun, Tianze Wang, Chao Yue, Nuoran Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHeatV2Xæ¡†æž¶ä»¥è§£å†³V2Xå¼‚æž„åä½œæ„ŸçŸ¥çš„å¯æ‰©å±•æ€§é—®é¢˜**

**å…³é”®è¯**: `V2Xåä½œæ„ŸçŸ¥` `å¼‚æž„ç‰¹å¾å¯¹é½` `å¯æ‰©å±•æ¡†æž¶` `å›¾æ³¨æ„åŠ›ç½‘ç»œ` `å¾®è°ƒæ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šV2Xåä½œæ„ŸçŸ¥ä¸­å¤šæ¨¡æ€å¼‚æž„æ€§å’Œå¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œå¯¼è‡´ç‰¹å¾å¯¹é½å›°éš¾å’Œè®­ç»ƒæˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¼‚æž„å›¾æ³¨æ„åŠ›è®­ç»ƒåŸºç¡€ä»£ç†ï¼Œå¹¶é€šè¿‡å±€éƒ¨å’Œå…¨å±€å¾®è°ƒå®žçŽ°é«˜æ•ˆå¯¹é½ä¸Žäº¤äº’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨OPV2V-Hå’ŒDAIR-V2Xæ•°æ®é›†ä¸Šï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½Žè®­ç»ƒå¼€é”€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.

