---
layout: default
title: PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning
---

# PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.10279" target="_blank" class="toolbar-btn">arXiv: 2511.10279v1</a>
    <a href="https://arxiv.org/pdf/2511.10279.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10279v1" 
            onclick="toggleFavorite(this, '2511.10279v1', 'PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-13

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/YanbeiJiang/PROPA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PROPAÊ°ÜÊû∂ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñËßÜËßâÊé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁ∫ß‰æùËµñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÊé®ÁêÜ` `Âº∫ÂåñÂ≠¶‰π†` `ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢` `ËøáÁ®ãÁ∫ß‰ºòÂåñ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰∏≠ÊòìÂá∫ÈîôÔºåÂéüÂõ†ÊòØÂ§öÊ≠•‰æùËµñÂØºËá¥Êó©ÊúüÈîôËØØÁ¥ØÁßØ„ÄÇ
2. PROPAÂà©Áî®MCTS‰∏éGRPOÁªìÂêàÔºåÁîüÊàêÂØÜÈõÜËøáÁ®ãÂ•ñÂä±ÔºåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®Âç≥ÂèØ‰ºòÂåñ‰∏≠Èó¥Ê≠•È™§„ÄÇ
3. PROPAÈÄöËøá‰∫§ÈîôGRPOÂíåSFTÊõ¥Êñ∞ÔºåÂπ∂ËÆ≠ÁªÉPRMÔºåÊòæËëóÊèêÂçá‰∫ÜËßÜËßâÊé®ÁêÜÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Âú®Â§çÊùÇËßÜËßâÊé®ÁêÜ‰∏≠Èù¢‰∏¥ÊåëÊàòÔºåÂ§öÊ≠•‰æùËµñÂÖ≥Á≥ªÂØºËá¥Êó©ÊúüÈîôËØØÂú®Êé®ÁêÜÈìæ‰∏≠Á¥ØÁßØ„ÄÇÁé∞ÊúâÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ïÂ≠òÂú®Â±ÄÈôêÔºöÁõëÁù£ÂæÆË∞É(SFT)‰æùËµñ‰∫éÊòÇË¥µÁöÑÊ≠•È™§Á∫ßÊ†áÊ≥®ÔºåËÄåÂü∫‰∫éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†(RLVR)ÊñπÊ≥ïÔºåÂ¶ÇGRPOÔºå‰ªÖÊèê‰æõÁ®ÄÁñèÁöÑÁªìÊûúÁ∫ßÂèçÈ¶àÔºåÈòªÁ¢ç‰∫ÜÁ®≥ÂÆö‰ºòÂåñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPROPAÔºàÂÖ∑Êúâ‰∫§ÈîôÁ≠ñÁï•ÂØπÈΩêÁöÑËøáÁ®ãÁ∫ßÊé®ÁêÜ‰ºòÂåñÔºâÔºåËØ•Ê°ÜÊû∂ÈõÜÊàê‰∫ÜËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢(MCTS)‰∏éGRPOÔºå‰ª•ÁîüÊàêÂØÜÈõÜÁöÑ„ÄÅËøáÁ®ãÁ∫ßÁöÑÂ•ñÂä±ÔºåÂπ∂Âú®Ê≤°Êúâ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ã‰ºòÂåñÊØè‰∏™‰∏≠Èó¥Ê≠•È™§ÁöÑÊé®ÁêÜ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÂÜ∑ÂêØÂä®ÈóÆÈ¢òÔºåPROPAÂ∞ÜGRPOÊõ¥Êñ∞‰∏éSFT‰∫§ÈîôËøõË°åÔºå‰ΩøÊ®°ÂûãËÉΩÂ§ü‰ªéÊàêÂäüÂíåÂ§±Ë¥•ÁöÑÊé®ÁêÜËΩ®Ëøπ‰∏≠Â≠¶‰π†„ÄÇËøõ‰∏ÄÊ≠•ËÆ≠ÁªÉËøáÁ®ãÂ•ñÂä±Ê®°Âûã(PRM)‰ª•ÊåáÂØºÊé®ÁêÜÊó∂ÁöÑÊêúÁ¥¢Ôºå‰ΩøÊµãËØïÊó∂ÊêúÁ¥¢‰∏éËÆ≠ÁªÉ‰ø°Âè∑ÂØπÈΩê„ÄÇÂú®‰∏É‰∏™Âü∫ÂáÜÊµãËØïÂíåÂõõ‰∏™VLMÈ™®Âπ≤ÁΩëÁªú‰∏äÔºåPROPAÂßãÁªà‰ºò‰∫éÂü∫‰∫éSFTÂíåRLVRÁöÑÂü∫Á∫øÔºåÂú®È¢ÜÂüüÂÜÖ‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ17.0%ÁöÑÂ¢ûÁõäÔºåÂú®È¢ÜÂüüÂ§ñ‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ21.0%ÁöÑÂ¢ûÁõäÔºå‰∏∫ËßÜËßâÊé®ÁêÜ‰ªªÂä°Âª∫Á´ã‰∫ÜÂº∫Â§ßÁöÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ÔºåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁî±‰∫éÂ§öÊ≠•‰æùËµñÂÖ≥Á≥ªÂØºËá¥ÁöÑÊó©ÊúüÈîôËØØÁ¥ØÁßØÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÁõëÁù£ÂæÆË∞É(SFT)ÔºåÈúÄË¶ÅÊòÇË¥µÁöÑÊ≠•È™§Á∫ßÊ†áÊ≥®ÔºåËÄåÂü∫‰∫éÁªìÊûúÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†(RLVR)ÊñπÊ≥ïÔºåÂ¶ÇGRPOÔºåÊèê‰æõÁöÑÂèçÈ¶àËøá‰∫éÁ®ÄÁñèÔºåÈöæ‰ª•ËøõË°åÊúâÊïàÁöÑËøáÁ®ãÁ∫ß‰ºòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPROPAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÂØπËßÜËßâÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÊØè‰∏ÄÊ≠•ËøõË°å‰ºòÂåñÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®„ÄÇÂÆÉÂà©Áî®ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢(MCTS)Êù•Êé¢Á¥¢‰∏çÂêåÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÂπ∂ÁªìÂêàGRPOÊù•Â≠¶‰π†Á≠ñÁï•„ÄÇÈÄöËøáMCTSÁîüÊàêÂØÜÈõÜÁöÑ„ÄÅËøáÁ®ãÁ∫ßÁöÑÂ•ñÂä±‰ø°Âè∑Ôºå‰ªéËÄåÂÖãÊúç‰∫Ü‰º†ÁªüRLVRÊñπÊ≥ï‰∏≠Â•ñÂä±Á®ÄÁñèÁöÑÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPROPAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Ôºö‰Ωú‰∏∫Êé®ÁêÜÁöÑ‰∏ª‰ΩìÔºõ2) ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢(MCTS)ÔºöÁî®‰∫éÊé¢Á¥¢‰∏çÂêåÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÂπ∂ÁîüÊàêËøáÁ®ãÁ∫ßÁöÑÂ•ñÂä±‰ø°Âè∑Ôºõ3) GRPOÔºöÁî®‰∫éÊõ¥Êñ∞VLMÁöÑÁ≠ñÁï•Ôºõ4) ËøáÁ®ãÂ•ñÂä±Ê®°Âûã(PRM)ÔºöÁî®‰∫éÊåáÂØºÊé®ÁêÜÊó∂ÁöÑÊêúÁ¥¢Ôºå‰ΩøÊµãËØïÊó∂ÊêúÁ¥¢‰∏éËÆ≠ÁªÉ‰ø°Âè∑ÂØπÈΩê„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖà‰ΩøÁî®SFTËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂Âêé‰∫§ÈîôËøõË°åGRPOÊõ¥Êñ∞ÂíåSFTÊõ¥Êñ∞ÔºåÂêåÊó∂ËÆ≠ÁªÉPRM„ÄÇÂú®Êé®ÁêÜÊó∂Ôºå‰ΩøÁî®PRMÊåáÂØºMCTSÊêúÁ¥¢ÔºåÂπ∂ÈÄâÊã©ÊúÄ‰ºòÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPROPAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜËøáÁ®ãÁ∫ßÊé®ÁêÜ‰ºòÂåñÁöÑÊ¶ÇÂøµÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂØπÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÊØè‰∏ÄÊ≠•ËøõË°å‰ºòÂåñÔºõ2) Â∞ÜMCTS‰∏éGRPOÁªìÂêàÔºåÁîüÊàêÂØÜÈõÜÁöÑ„ÄÅËøáÁ®ãÁ∫ßÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüRLVRÊñπÊ≥ï‰∏≠Â•ñÂä±Á®ÄÁñèÁöÑÈóÆÈ¢òÔºõ3) ÊèêÂá∫‰∫Ü‰∫§ÈîôÊõ¥Êñ∞Á≠ñÁï•ÔºåÂ∞ÜGRPOÊõ¥Êñ∞‰∏éSFTÊõ¥Êñ∞‰∫§ÈîôËøõË°åÔºåÂÖãÊúç‰∫ÜÂÜ∑ÂêØÂä®ÈóÆÈ¢òÔºõ4) ËÆ≠ÁªÉPRMÊù•ÊåáÂØºÊé®ÁêÜÊó∂ÁöÑÊêúÁ¥¢Ôºå‰ΩøÊµãËØïÊó∂ÊêúÁ¥¢‰∏éËÆ≠ÁªÉ‰ø°Âè∑ÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPROPAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºöÂ•ñÂä±ÂáΩÊï∞ÁªºÂêàËÄÉËôë‰∫ÜÊé®ÁêÜÁöÑÊ≠£Á°ÆÊÄßÂíåÊïàÁéáÔºåÈºìÂä±Ê®°ÂûãÈÄâÊã©Êõ¥Áü≠„ÄÅÊõ¥Ê≠£Á°ÆÁöÑÊé®ÁêÜË∑ØÂæÑÔºõ2) MCTSÁöÑÊêúÁ¥¢Á≠ñÁï•ÔºöMCTS‰ΩøÁî®UCTÁÆóÊ≥ïËøõË°åÊêúÁ¥¢ÔºåÂπ≥Ë°°‰∫ÜÊé¢Á¥¢ÂíåÂà©Áî®Ôºõ3) PRMÁöÑÁΩëÁªúÁªìÊûÑÔºöPRMÊòØ‰∏Ä‰∏™TransformerÊ®°ÂûãÔºåËæìÂÖ•ÊòØÊé®ÁêÜÊ≠•È™§ÁöÑ‰∏ä‰∏ãÊñáÔºåËæìÂá∫ÊòØËØ•Ê≠•È™§ÁöÑÂ•ñÂä±È¢ÑÊµãÔºõ4) ‰∫§ÈîôÊõ¥Êñ∞ÁöÑÊØî‰æãÔºöËÆ∫Êñá‰∏≠ÂÆûÈ™å‰∫Ü‰∏çÂêåÁöÑGRPOÊõ¥Êñ∞ÂíåSFTÊõ¥Êñ∞ÁöÑÊØî‰æãÔºåÊúÄÁªàÈÄâÊã©‰∫Ü‰∏Ä‰∏™ÊúÄ‰ºòÁöÑÊØî‰æã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PROPAÂú®‰∏É‰∏™Âü∫ÂáÜÊµãËØïÂíåÂõõ‰∏™VLMÈ™®Âπ≤ÁΩëÁªú‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéPROPAÂßãÁªà‰ºò‰∫éÂü∫‰∫éSFTÂíåRLVRÁöÑÂü∫Á∫ø„ÄÇÂú®È¢ÜÂüüÂÜÖ‰ªªÂä°‰∏äÔºåPROPAÂÆûÁé∞‰∫ÜÈ´òËææ17.0%ÁöÑÂ¢ûÁõäÔºåÂú®È¢ÜÂüüÂ§ñ‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ21.0%ÁöÑÂ¢ûÁõä„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåPROPAÂÖ∑ÊúâÂº∫Â§ßÁöÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÊúâÊïàÂú∞Ëß£ÂÜ≥ËßÜËßâÊé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁ∫ß‰æùËµñÈóÆÈ¢ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PROPAÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§çÊùÇËßÜËßâÊé®ÁêÜÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞ÁîüÊàê„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇÈÄöËøá‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºåÂèØ‰ª•ÊèêÈ´òËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÂíåÂèØÈù†ÊÄßÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÂíåÂÆûÈôÖ‰ª∑ÂÄº„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.

