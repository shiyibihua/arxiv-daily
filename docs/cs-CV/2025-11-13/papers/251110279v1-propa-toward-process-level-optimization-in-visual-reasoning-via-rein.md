---
layout: default
title: PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning
---

# PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning

**arXiv**: [2511.10279v1](https://arxiv.org/abs/2511.10279) | [PDF](https://arxiv.org/pdf/2511.10279.pdf)

**ä½œè€…**: Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-13

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/YanbeiJiang/PROPA)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPROPAæ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è§†è§‰æŽ¨ç†ä¸­çš„è¿‡ç¨‹çº§ä¾èµ–é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è¿‡ç¨‹çº§ä¼˜åŒ–` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¤æ‚æŽ¨ç†ä¸­æ˜“å‡ºé”™ï¼ŒåŽŸå› æ˜¯å¤šæ­¥ä¾èµ–å¯¼è‡´æ—©æœŸé”™è¯¯ç´¯ç§¯ã€‚
2. PROPAåˆ©ç”¨MCTSä¸ŽGRPOç»“åˆï¼Œç”Ÿæˆå¯†é›†è¿‡ç¨‹å¥–åŠ±ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯ä¼˜åŒ–ä¸­é—´æ­¥éª¤ã€‚
3. PROPAé€šè¿‡äº¤é”™GRPOå’ŒSFTæ›´æ–°ï¼Œå¹¶è®­ç»ƒPRMï¼Œæ˜¾è‘—æå‡äº†è§†è§‰æŽ¨ç†çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡åž‹(VLM)åœ¨å¤æ‚è§†è§‰æŽ¨ç†ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¤šæ­¥ä¾èµ–å…³ç³»å¯¼è‡´æ—©æœŸé”™è¯¯åœ¨æŽ¨ç†é“¾ä¸­ç´¯ç§¯ã€‚çŽ°æœ‰çš„åŽè®­ç»ƒæ–¹æ³•å­˜åœ¨å±€é™ï¼šç›‘ç£å¾®è°ƒ(SFT)ä¾èµ–äºŽæ˜‚è´µçš„æ­¥éª¤çº§æ ‡æ³¨ï¼Œè€ŒåŸºäºŽå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)æ–¹æ³•ï¼Œå¦‚GRPOï¼Œä»…æä¾›ç¨€ç–çš„ç»“æžœçº§åé¦ˆï¼Œé˜»ç¢äº†ç¨³å®šä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†PROPAï¼ˆå…·æœ‰äº¤é”™ç­–ç•¥å¯¹é½çš„è¿‡ç¨‹çº§æŽ¨ç†ä¼˜åŒ–ï¼‰ï¼Œè¯¥æ¡†æž¶é›†æˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ä¸ŽGRPOï¼Œä»¥ç”Ÿæˆå¯†é›†çš„ã€è¿‡ç¨‹çº§çš„å¥–åŠ±ï¼Œå¹¶åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¯ä¸ªä¸­é—´æ­¥éª¤çš„æŽ¨ç†ã€‚ä¸ºäº†å…‹æœå†·å¯åŠ¨é—®é¢˜ï¼ŒPROPAå°†GRPOæ›´æ–°ä¸ŽSFTäº¤é”™è¿›è¡Œï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿä»ŽæˆåŠŸå’Œå¤±è´¥çš„æŽ¨ç†è½¨è¿¹ä¸­å­¦ä¹ ã€‚è¿›ä¸€æ­¥è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹(PRM)ä»¥æŒ‡å¯¼æŽ¨ç†æ—¶çš„æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸Žè®­ç»ƒä¿¡å·å¯¹é½ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•å’Œå››ä¸ªVLMéª¨å¹²ç½‘ç»œä¸Šï¼ŒPROPAå§‹ç»ˆä¼˜äºŽåŸºäºŽSFTå’ŒRLVRçš„åŸºçº¿ï¼Œåœ¨é¢†åŸŸå†…ä»»åŠ¡ä¸Šå®žçŽ°äº†é«˜è¾¾17.0%çš„å¢žç›Šï¼Œåœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸Šå®žçŽ°äº†é«˜è¾¾21.0%çš„å¢žç›Šï¼Œä¸ºè§†è§‰æŽ¨ç†ä»»åŠ¡å»ºç«‹äº†å¼ºå¤§çš„æŽ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰æŽ¨ç†ä»»åŠ¡ä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡åž‹ç”±äºŽå¤šæ­¥ä¾èµ–å…³ç³»å¯¼è‡´çš„æ—©æœŸé”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒ(SFT)ï¼Œéœ€è¦æ˜‚è´µçš„æ­¥éª¤çº§æ ‡æ³¨ï¼Œè€ŒåŸºäºŽç»“æžœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)æ–¹æ³•ï¼Œå¦‚GRPOï¼Œæä¾›çš„åé¦ˆè¿‡äºŽç¨€ç–ï¼Œéš¾ä»¥è¿›è¡Œæœ‰æ•ˆçš„è¿‡ç¨‹çº§ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPROPAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œå¯¹è§†è§‰æŽ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥è¿›è¡Œä¼˜åŒ–ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å®ƒåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)æ¥æŽ¢ç´¢ä¸åŒçš„æŽ¨ç†è·¯å¾„ï¼Œå¹¶ç»“åˆGRPOæ¥å­¦ä¹ ç­–ç•¥ã€‚é€šè¿‡MCTSç”Ÿæˆå¯†é›†çš„ã€è¿‡ç¨‹çº§çš„å¥–åŠ±ä¿¡å·ï¼Œä»Žè€Œå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•ä¸­å¥–åŠ±ç¨€ç–çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šPROPAæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰è¯­è¨€æ¨¡åž‹(VLM)ï¼šä½œä¸ºæŽ¨ç†çš„ä¸»ä½“ï¼›2) è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ï¼šç”¨äºŽæŽ¢ç´¢ä¸åŒçš„æŽ¨ç†è·¯å¾„ï¼Œå¹¶ç”Ÿæˆè¿‡ç¨‹çº§çš„å¥–åŠ±ä¿¡å·ï¼›3) GRPOï¼šç”¨äºŽæ›´æ–°VLMçš„ç­–ç•¥ï¼›4) è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹(PRM)ï¼šç”¨äºŽæŒ‡å¯¼æŽ¨ç†æ—¶çš„æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸Žè®­ç»ƒä¿¡å·å¯¹é½ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆä½¿ç”¨SFTè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åŽäº¤é”™è¿›è¡ŒGRPOæ›´æ–°å’ŒSFTæ›´æ–°ï¼ŒåŒæ—¶è®­ç»ƒPRMã€‚åœ¨æŽ¨ç†æ—¶ï¼Œä½¿ç”¨PRMæŒ‡å¯¼MCTSæœç´¢ï¼Œå¹¶é€‰æ‹©æœ€ä¼˜çš„æŽ¨ç†è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šPROPAçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†è¿‡ç¨‹çº§æŽ¨ç†ä¼˜åŒ–çš„æ¦‚å¿µï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹æŽ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥è¿›è¡Œä¼˜åŒ–ï¼›2) å°†MCTSä¸ŽGRPOç»“åˆï¼Œç”Ÿæˆå¯†é›†çš„ã€è¿‡ç¨‹çº§çš„å¥–åŠ±ä¿¡å·ï¼Œå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•ä¸­å¥–åŠ±ç¨€ç–çš„é—®é¢˜ï¼›3) æå‡ºäº†äº¤é”™æ›´æ–°ç­–ç•¥ï¼Œå°†GRPOæ›´æ–°ä¸ŽSFTæ›´æ–°äº¤é”™è¿›è¡Œï¼Œå…‹æœäº†å†·å¯åŠ¨é—®é¢˜ï¼›4) è®­ç»ƒPRMæ¥æŒ‡å¯¼æŽ¨ç†æ—¶çš„æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸Žè®­ç»ƒä¿¡å·å¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šPROPAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼šå¥–åŠ±å‡½æ•°ç»¼åˆè€ƒè™‘äº†æŽ¨ç†çš„æ­£ç¡®æ€§å’Œæ•ˆçŽ‡ï¼Œé¼“åŠ±æ¨¡åž‹é€‰æ‹©æ›´çŸ­ã€æ›´æ­£ç¡®çš„æŽ¨ç†è·¯å¾„ï¼›2) MCTSçš„æœç´¢ç­–ç•¥ï¼šMCTSä½¿ç”¨UCTç®—æ³•è¿›è¡Œæœç´¢ï¼Œå¹³è¡¡äº†æŽ¢ç´¢å’Œåˆ©ç”¨ï¼›3) PRMçš„ç½‘ç»œç»“æž„ï¼šPRMæ˜¯ä¸€ä¸ªTransformeræ¨¡åž‹ï¼Œè¾“å…¥æ˜¯æŽ¨ç†æ­¥éª¤çš„ä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ˜¯è¯¥æ­¥éª¤çš„å¥–åŠ±é¢„æµ‹ï¼›4) äº¤é”™æ›´æ–°çš„æ¯”ä¾‹ï¼šè®ºæ–‡ä¸­å®žéªŒäº†ä¸åŒçš„GRPOæ›´æ–°å’ŒSFTæ›´æ–°çš„æ¯”ä¾‹ï¼Œæœ€ç»ˆé€‰æ‹©äº†ä¸€ä¸ªæœ€ä¼˜çš„æ¯”ä¾‹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

PROPAåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•å’Œå››ä¸ªVLMéª¨å¹²ç½‘ç»œä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æžœè¡¨æ˜ŽPROPAå§‹ç»ˆä¼˜äºŽåŸºäºŽSFTå’ŒRLVRçš„åŸºçº¿ã€‚åœ¨é¢†åŸŸå†…ä»»åŠ¡ä¸Šï¼ŒPROPAå®žçŽ°äº†é«˜è¾¾17.0%çš„å¢žç›Šï¼Œåœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸Šå®žçŽ°äº†é«˜è¾¾21.0%çš„å¢žç›Šã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒPROPAå…·æœ‰å¼ºå¤§çš„æŽ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è§†è§‰æŽ¨ç†ä¸­çš„è¿‡ç¨‹çº§ä¾èµ–é—®é¢˜ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

PROPAæ¡†æž¶å¯åº”ç”¨äºŽå„ç§éœ€è¦å¤æ‚è§†è§‰æŽ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡ä¼˜åŒ–æŽ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥æé«˜è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®žé™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€æŽ¨ç†ä»»åŠ¡ä¸­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.

