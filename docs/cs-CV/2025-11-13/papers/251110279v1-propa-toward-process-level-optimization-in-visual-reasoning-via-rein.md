---
layout: default
title: PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning
---

# PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning

**arXiv**: [2511.10279v1](https://arxiv.org/abs/2511.10279) | [PDF](https://arxiv.org/pdf/2511.10279.pdf)

**ä½œè€…**: Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPROPAæ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è§†è§‰æŽ¨ç†è¿‡ç¨‹ï¼Œæ— éœ€äººå·¥æ ‡æ³¨**

**å…³é”®è¯**: `è§†è§‰æŽ¨ç†ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `è¿‡ç¨‹çº§å¥–åŠ±` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¤æ‚æŽ¨ç†ä¸­æ˜“å› æ—©æœŸé”™è¯¯ä¼ æ’­è€Œå¤±è´¥
2. ç»“åˆMCTSä¸ŽGRPOç”Ÿæˆå¯†é›†è¿‡ç¨‹å¥–åŠ±ï¼Œå¹¶äº¤æ›¿ä½¿ç”¨SFTè§£å†³å†·å¯åŠ¨é—®é¢˜
3. åœ¨å¤šä¸ªåŸºå‡†å’Œéª¨å¹²ç½‘ç»œä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡æ³›åŒ–èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.

