---
layout: default
title: Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals
---

# Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.10615" target="_blank" class="toolbar-btn">arXiv: 2511.10615v1</a>
    <a href="https://arxiv.org/pdf/2511.10615.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10615v1" 
            onclick="toggleFavorite(this, '2511.10615v1', 'Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shruti Singh Baghel, Yash Pratap Singh Rathore, Sushovan Jena, Anurag Pradhan, Amit Shukla, Arnav Bhavsar, Pawan Goyal

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-13

**Â§áÊ≥®**: 8 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÈíàÂØπËßÜÈöú‰∫∫Â£´ÔºåËØÑ‰º∞ËΩªÈáèÁ∫ßVLMÂú®ËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑÂèØËÆøÈóÆÊÄßÔºåÂπ∂ÊèêÂá∫ÂÆöÂà∂ÂåñËØÑ‰º∞Ê°ÜÊû∂„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÂèØËÆøÈóÆÊÄß` `Áõ≤‰∫∫ËæÖÂä©` `ËßÜÈ¢ëÁêÜËß£` `ËΩªÈáèÁ∫ßÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMÊ®°Âûã‰ΩìÁßØÂ∫ûÂ§ßÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÈÉ®ÁΩ≤ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËßÜÈöú‰∫∫Â£´ËæÖÂä©Â∫îÁî®‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈíàÂØπËßÜÈöú‰∫∫Â£´ÁöÑËßÜÈ¢ëÁêÜËß£ËØÑ‰º∞Ê°ÜÊû∂ÔºåÂπ∂Á†îÁ©∂‰∏çÂêåÂ§ßÂ∞èÁöÑËΩªÈáèÁ∫ßVLMÊ®°ÂûãÂú®ÂèØËÆøÈóÆÊÄßÊñπÈù¢ÁöÑË°®Áé∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËΩªÈáèÁ∫ßVLMÂú®ÁßªÂä®ËÆæÂ§á‰∏äÂÖ∑ÊúâÂèØË°åÊÄßÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÊ®°Âûã‰ºòÂåñÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Âú®ÁêÜËß£ÂíåÁîüÊàêËßÜÈ¢ëÊèèËø∞ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂È´òÂÜÖÂ≠ò„ÄÅËÆ°ÁÆóÂíåÈÉ®ÁΩ≤ÈúÄÊ±ÇÈòªÁ¢ç‰∫ÜÂÆûÈôÖÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂØπ‰∫é‰æùËµñËØ¶ÁªÜ„ÄÅ‰∏ä‰∏ãÊñáÊÑüÁü•ÊèèËø∞ÁöÑÁõ≤‰∫∫Âíå‰ΩéËßÜÂäõ(BLV)Áî®Êà∑„ÄÇ‰∏∫‰∫ÜÁ†îÁ©∂Ê®°ÂûãÂ§ßÂ∞èÂØπ‰ª•ÂèØËÆøÈóÆÊÄß‰∏∫‰∏≠ÂøÉÁöÑÊèèËø∞Ë¥®ÈáèÁöÑÂΩ±ÂìçÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÂèÇÊï∞ÈáèÂàÜÂà´‰∏∫500MÂíå2.2BÁöÑSmolVLM2Âèò‰ΩìÔºåÊï∞ÊçÆÈõÜÂåÖÊã¨AVCaps(ÂÆ§Â§ñ)ÂíåCharades(ÂÆ§ÂÜÖ)„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§‰∏™‰∏ìÈó®‰∏∫BLVÂèØËÆøÈóÆÊÄßËØÑ‰º∞ËÄåËÆæËÆ°ÁöÑÊñ∞ÂûãËØÑ‰º∞Ê°ÜÊû∂ÔºöÂ§ö‰∏ä‰∏ãÊñáBLVÊ°ÜÊû∂ÔºåËØÑ‰º∞Á©∫Èó¥ÂÆö‰Ωç„ÄÅÁ§æ‰∫§‰∫íÂä®„ÄÅÂä®‰Ωú‰∫ã‰ª∂ÂíåÁéØÂ¢É‰∏ä‰∏ãÊñáÔºõÂØºËà™ËæÖÂä©Ê°ÜÊû∂Ôºå‰æßÈáç‰∫éÁßªÂä®ÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Á≥ªÁªüÂú∞ËØÑ‰º∞‰∫ÜÂõõÁßç‰∏çÂêåÁöÑÊèêÁ§∫ËÆæËÆ°Á≠ñÁï•ÔºåÂπ∂Âú®Êô∫ËÉΩÊâãÊú∫‰∏äÈÉ®ÁΩ≤‰∫ÜËøô‰∏§‰∏™Ê®°ÂûãÔºåËØÑ‰º∞FP32ÂíåINT8Á≤æÂ∫¶Âèò‰ΩìÔºå‰ª•ËØÑ‰º∞ËµÑÊ∫êÂèóÈôêÁöÑÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂÆûÈôÖÊÄßËÉΩÁ∫¶Êùü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËôΩÁÑ∂Âú®ËßÜÈ¢ëÊèèËø∞ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂Â∫ûÂ§ßÁöÑÊ®°Âûã‰ΩìÁßØÂíåËÆ°ÁÆóÈúÄÊ±Ç‰ΩøÂÖ∂Èöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁßªÂä®ËÆæÂ§á‰∏äÈÉ®ÁΩ≤„ÄÇËøôÂØπ‰∫é‰æùËµñVLMÊèê‰æõËßÜÈ¢ëÁêÜËß£ÂíåÊèèËø∞ÁöÑÁõ≤‰∫∫Âíå‰ΩéËßÜÂäõÔºàBLVÔºâÁî®Êà∑Êù•ËØ¥ÊòØ‰∏Ä‰∏™ÊòæËëóÁöÑÊåëÊàòÔºåÂõ†‰∏∫‰ªñ‰ª¨ÈúÄË¶ÅËÉΩÂ§üÈöèÊó∂ÈöèÂú∞ËÆøÈóÆËøô‰∫õÂäüËÉΩ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïËØÑ‰º∞Âíå‰ºòÂåñËΩªÈáèÁ∫ßVLMÂú®BLVËæÖÂä©Â∫îÁî®‰∏≠ÁöÑÊÄßËÉΩÔºåÊòØ‰∏Ä‰∏™‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËØÑ‰º∞‰∏çÂêåÂ§ßÂ∞èÁöÑËΩªÈáèÁ∫ßVLMÔºàSmolVLM2ÔºâÂú®‰∏§‰∏™‰∏ìÈó®ËÆæËÆ°ÁöÑBLVÂèØËÆøÈóÆÊÄßËØÑ‰º∞Ê°ÜÊû∂‰∏äÁöÑË°®Áé∞ÔºåÊù•Á†îÁ©∂Ê®°ÂûãÂ§ßÂ∞èÂØπÊèèËø∞Ë¥®ÈáèÁöÑÂΩ±Âìç„ÄÇÂêåÊó∂ÔºåÊé¢Á¥¢‰∏çÂêåÁöÑÊèêÁ§∫ËÆæËÆ°Á≠ñÁï•ÂíåÈáèÂåñÊñπÊ≥ïÔºàFP32ÂíåINT8ÔºâÔºå‰ª•‰ºòÂåñÊ®°ÂûãÂú®ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤ÂíåÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Á†îÁ©∂ÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™ÈÉ®ÂàÜÔºö1) ÈÄâÊã©ËΩªÈáèÁ∫ßVLMÊ®°ÂûãSmolVLM2ÔºåÂπ∂‰ΩøÁî®‰∏çÂêåÂèÇÊï∞ÈáèÁöÑÂèò‰ΩìÔºà500MÂíå2.2BÔºâ„ÄÇ2) ÊûÑÂª∫‰∏§‰∏™Êñ∞ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºöÂ§ö‰∏ä‰∏ãÊñáBLVÊ°ÜÊû∂ÂíåÂØºËà™ËæÖÂä©Ê°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®Á©∫Èó¥ÂÆö‰Ωç„ÄÅÁ§æ‰∫§‰∫íÂä®„ÄÅÂä®‰Ωú‰∫ã‰ª∂„ÄÅÁéØÂ¢É‰∏ä‰∏ãÊñáÂíåÁßªÂä®ÂÖ≥ÈîÆ‰ø°ÊÅØÁ≠âÊñπÈù¢ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇ3) ËÆæËÆ°ÂõõÁßç‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•Ôºå‰ª•Êé¢Á¥¢ÊúÄ‰Ω≥ÁöÑÊèêÁ§∫ÊñπÂºè„ÄÇ4) Âú®Êô∫ËÉΩÊâãÊú∫‰∏äÈÉ®ÁΩ≤Ê®°ÂûãÔºåÂπ∂ËØÑ‰º∞FP32ÂíåINT8Á≤æÂ∫¶‰∏ãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏§‰∏™‰∏ìÈó®ÈíàÂØπBLVÂèØËÆøÈóÆÊÄßËØÑ‰º∞ÁöÑÊ°ÜÊû∂ÔºöÂ§ö‰∏ä‰∏ãÊñáBLVÊ°ÜÊû∂ÂíåÂØºËà™ËæÖÂä©Ê°ÜÊû∂„ÄÇËøô‰∫õÊ°ÜÊû∂ËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞VLMÂú®ÁêÜËß£ÂíåÊèèËø∞ËßÜÈ¢ëÂÜÖÂÆπÔºåÂπ∂‰∏∫BLVÁî®Êà∑Êèê‰æõÊúâ‰ª∑ÂÄº‰ø°ÊÅØÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ËøòÁ≥ªÁªüÂú∞ËØÑ‰º∞‰∫Ü‰∏çÂêåÊèêÁ§∫Á≠ñÁï•ÂíåÈáèÂåñÊñπÊ≥ïÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºå‰∏∫ËΩªÈáèÁ∫ßVLMÂú®ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÊåáÂØº„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ§ö‰∏ä‰∏ãÊñáBLVÊ°ÜÊû∂ËØÑ‰º∞Âõõ‰∏™ÂÖ≥ÈîÆ‰∏ä‰∏ãÊñáÔºöÁ©∫Èó¥ÂÆö‰Ωç„ÄÅÁ§æ‰∫§‰∫íÂä®„ÄÅÂä®‰Ωú‰∫ã‰ª∂ÂíåÁéØÂ¢É„ÄÇÂØºËà™ËæÖÂä©Ê°ÜÊû∂Âàô‰æßÈáç‰∫éËØÑ‰º∞Ê®°ÂûãÊèêÂèñÁßªÂä®ÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÈöúÁ¢çÁâ©„ÄÅÊñπÂêëÊåáÁ§∫Á≠â„ÄÇÊèêÁ§∫Á≠ñÁï•ÂåÖÊã¨‰∏çÂêåÁöÑÊåá‰ª§Âíå‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºå‰ª•ÂºïÂØºÊ®°ÂûãÁîüÊàêÊõ¥ÂáÜÁ°ÆÂíåÊúâÁî®ÁöÑÊèèËø∞„ÄÇÈáèÂåñÊñπÊ≥ïÂåÖÊã¨FP32ÂíåINT8ÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåÁ≤æÂ∫¶‰∏ãÁöÑÊÄßËÉΩÂíåËµÑÊ∫êÊ∂àËÄó„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSmolVLM2Ê®°ÂûãÂú®‰∏§‰∏™ËØÑ‰º∞Ê°ÜÊû∂‰∏äÂùáË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁêÜËß£Á©∫Èó¥ÂÆö‰ΩçÂíåÂä®‰Ωú‰∫ã‰ª∂ÊñπÈù¢„ÄÇÈÄöËøáÂØπÊØî‰∏çÂêåÊèêÁ§∫Á≠ñÁï•ÔºåÂèëÁé∞ÁâπÂÆöÁöÑÊèêÁ§∫ÊñπÂºèËÉΩÂ§üÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑÊèèËø∞Ë¥®Èáè„ÄÇÊ≠§Â§ñÔºåINT8ÈáèÂåñÂú®‰øùËØÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÊ®°Âûã‰ΩìÁßØÂíåËÆ°ÁÆóÈúÄÊ±ÇÔºå‰ΩøÂÖ∂Êõ¥ÈÄÇÂêàÂú®ÁßªÂä®ËÆæÂ§á‰∏äÈÉ®ÁΩ≤„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂºÄÂèëÈù¢ÂêëËßÜÈöú‰∫∫Â£´ÁöÑÊô∫ËÉΩËæÖÂä©ËÆæÂ§áÂíåÂ∫îÁî®Ôºå‰æãÂ¶ÇÊô∫ËÉΩÁúºÈïú„ÄÅÊâãÊú∫Â∫îÁî®Á≠âÔºåÂ∏ÆÂä©‰ªñ‰ª¨Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÊèêÈ´òÁîüÊ¥ªË¥®ÈáèÂíåÂá∫Ë°åÂÆâÂÖ®„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ÊèêÂá∫ÁöÑËØÑ‰º∞Ê°ÜÊû∂‰πüÂèØÁî®‰∫éËØÑ‰º∞ÂÖ∂‰ªñVLMÊ®°ÂûãÂú®ÂèØËÆøÈóÆÊÄßÊñπÈù¢ÁöÑË°®Áé∞ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

