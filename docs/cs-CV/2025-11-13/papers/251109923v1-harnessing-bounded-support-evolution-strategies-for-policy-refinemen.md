---
layout: default
title: Harnessing Bounded-Support Evolution Strategies for Policy Refinement
---

# Harnessing Bounded-Support Evolution Strategies for Policy Refinement

**arXiv**: [2511.09923v1](https://arxiv.org/abs/2511.09923) | [PDF](https://arxiv.org/pdf/2511.09923.pdf)

**ä½œè€…**: Ethan Hirschowitz, Fabio Ramos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸‰è§’åˆ†å¸ƒè¿›åŒ–ç­–ç•¥ä»¥è§£å†³æœºå™¨äººç­–ç•¥ç²¾ç‚¼ä¸­çš„æ¢¯åº¦å™ªå£°é—®é¢˜**

**å…³é”®è¯**: `è¿›åŒ–ç­–ç•¥` `ç­–ç•¥ç²¾ç‚¼` `æœºå™¨äººæ“ä½œ` `æ— æ¢¯åº¦ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `å™ªå£°æŠ‘åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººç­–ç•¥ç²¾ç‚¼ä¸­é¢ä¸´æ¢¯åº¦å™ªå£°å¤§ã€ä¿¡å·å¼±çš„é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æœ‰ç•Œä¸‰è§’å™ªå£°å’Œä¸­å¿ƒç§©æœ‰é™å·®åˆ†ä¼°è®¡å™¨å®žçŽ°ç¨³å®šã€å¯å¹¶è¡Œã€æ— æ¢¯åº¦æ›´æ–°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œç›¸æ¯”PPOæå‡æˆåŠŸçŽ‡26.5%ï¼Œæ˜¾è‘—é™ä½Žæ–¹å·®

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Improving competent robot policies with on-policy RL is often hampered by noisy, low-signal gradients. We revisit Evolution Strategies (ES) as a policy-gradient proxy and localize exploration with bounded, antithetic triangular perturbations, suitable for policy refinement. We propose Triangular-Distribution ES (TD-ES) which pairs bounded triangular noise with a centered-rank finite-difference estimator to deliver stable, parallelizable, gradient-free updates. In a two-stage pipeline -- PPO pretraining followed by TD-ES refinement -- this preserves early sample efficiency while enabling robust late-stage gains. Across a suite of robotic manipulation tasks, TD-ES raises success rates by 26.5% relative to PPO and greatly reduces variance, offering a simple, compute-light path to reliable refinement.

