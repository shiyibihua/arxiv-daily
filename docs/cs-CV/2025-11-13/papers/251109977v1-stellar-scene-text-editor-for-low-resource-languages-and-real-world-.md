---
layout: default
title: STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data
---

# STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data

**arXiv**: [2511.09977v1](https://arxiv.org/abs/2511.09977) | [PDF](https://arxiv.org/pdf/2511.09977.pdf)

**ä½œè€…**: Yongdeuk Seo, Hyun-seok Min, Sungchul Choi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTELLARåœºæ™¯æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œè§£å†³ä½Žèµ„æºè¯­è¨€å’ŒçœŸå®žæ•°æ®åœºæ™¯ä¸­çš„æ–‡æœ¬ç¼–è¾‘é—®é¢˜**

**å…³é”®è¯**: `åœºæ™¯æ–‡æœ¬ç¼–è¾‘` `ä½Žèµ„æºè¯­è¨€` `çœŸå®žæ•°æ®è®­ç»ƒ` `å­—å½¢ç¼–ç å™¨` `æ–‡æœ¬å¤–è§‚ç›¸ä¼¼æ€§` `å¤šè¯­è¨€ç¼–è¾‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ–¹æ³•ç¼ºä¹å¯¹ä½Žèµ„æºè¯­è¨€æ”¯æŒã€åˆæˆä¸ŽçœŸå®žæ•°æ®åŸŸå·®è·ï¼Œä»¥åŠæ–‡æœ¬é£Žæ ¼ä¿ç•™è¯„ä¼°æŒ‡æ ‡ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è¯­è¨€è‡ªé€‚åº”å­—å½¢ç¼–ç å™¨å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆåˆæˆæ•°æ®é¢„è®­ç»ƒï¼Œå†çœŸå®žå›¾åƒå¾®è°ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®çŽ‡ä¸Šä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ï¼ŒTASæŒ‡æ ‡å¹³å‡æå‡2.2%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.

