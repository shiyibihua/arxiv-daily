---
layout: default
title: A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space
---

# A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space

**arXiv**: [2511.10555v1](https://arxiv.org/abs/2511.10555) | [PDF](https://arxiv.org/pdf/2511.10555.pdf)

**ä½œè€…**: Huijie Liu, Shuhao Cui, Haoxiang Cao, Shuai Ma, Kai Wu, Guoliang Kang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoTyleæ–¹æ³•ï¼Œé€šè¿‡æ•°å€¼ä»£ç ç”Ÿæˆæ–°é¢–ä¸€è‡´çš„è§†è§‰é£Žæ ¼å›¾åƒã€‚**

**å…³é”®è¯**: `ä»£ç åˆ°é£Žæ ¼ç”Ÿæˆ` `ç¦»æ•£é£Žæ ¼ç©ºé—´` `æ‰©æ•£æ¨¡åž‹` `é£Žæ ¼åµŒå…¥` `è‡ªå›žå½’ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–æ–‡æœ¬æç¤ºæˆ–å‚è€ƒå›¾åƒï¼Œéš¾ä»¥ä¿è¯é£Žæ ¼ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚
2. è®­ç»ƒç¦»æ•£é£Žæ ¼ç æœ¬å’Œè‡ªå›žå½’é£Žæ ¼ç”Ÿæˆå™¨ï¼Œå°†æ•°å€¼ä»£ç æ˜ å°„ä¸ºé£Žæ ¼åµŒå…¥ä»¥å¼•å¯¼æ‰©æ•£æ¨¡åž‹ã€‚
3. å®žéªŒéªŒè¯CoTyleèƒ½æœ‰æ•ˆå°†æ•°å€¼ä»£ç è½¬åŒ–ä¸ºé£Žæ ¼æŽ§åˆ¶å™¨ï¼Œç”Ÿæˆå¤šæ ·ä¸”ä¸€è‡´çš„é£Žæ ¼å›¾åƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.

