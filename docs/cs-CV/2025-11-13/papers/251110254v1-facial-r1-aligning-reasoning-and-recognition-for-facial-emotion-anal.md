---
layout: default
title: Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis
---

# Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis

**arXiv**: [2511.10254v1](https://arxiv.org/abs/2511.10254) | [PDF](https://arxiv.org/pdf/2511.10254.pdf)

**ä½œè€…**: Jiulong Wu, Yucheng Shen, Lingyong Yan, Haixin Sun, Deguo Xia, Jizhou Huang, Min Cao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFacial-R1æ¡†æž¶ä»¥è§£å†³é¢éƒ¨æƒ…æ„Ÿåˆ†æžä¸­çš„å¹»è§‰æŽ¨ç†å’Œè¯†åˆ«ä¸å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `é¢éƒ¨æƒ…æ„Ÿåˆ†æž` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯¹é½æ¡†æž¶` `å¼ºåŒ–è®­ç»ƒ` `æ•°æ®åˆæˆ` `åŸºå‡†æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†æžä¸­äº§ç”Ÿå¹»è§‰æŽ¨ç†å’Œè¯†åˆ«ä¸å¯¹é½
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µå¯¹é½æ¡†æž¶ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒã€å¼ºåŒ–è®­ç»ƒå’Œæ•°æ®åˆæˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶å¼•å…¥FEA-20Kæ•°æ®é›†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.

