---
layout: default
title: Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers
---

# Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers

**arXiv**: [2511.09926v1](https://arxiv.org/abs/2511.09926) | [PDF](https://arxiv.org/pdf/2511.09926.pdf)

**ä½œè€…**: Xuan Rao, Simian Xu, Zheng Li, Bo Zhao, Derong Liu, Mingming Ha, Cesare Alippi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSLDCæ–¹æ³•ä»¥è¡¥å¿ç±»å¢žé‡å­¦ä¹ ä¸­é¢„è®­ç»ƒViTçš„åˆ†å¸ƒæ¼‚ç§»**

**å…³é”®è¯**: `ç±»å¢žé‡å­¦ä¹ ` `åˆ†å¸ƒæ¼‚ç§»è¡¥å¿` `çŸ¥è¯†è’¸é¦` `é¢„è®­ç»ƒè§†è§‰Transformer` `ç‰¹å¾å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSeqFTå¯¼è‡´ç‰¹å¾åˆ†å¸ƒæ¼‚ç§»ï¼Œå½±å“åˆ†ç±»å™¨æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥çº¿æ€§ä¸Žå¼±éžçº¿æ€§å˜æ¢ç®—å­å¯¹é½ç‰¹å¾åˆ†å¸ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šç»“åˆçŸ¥è¯†è’¸é¦ï¼Œæ€§èƒ½æŽ¥è¿‘è”åˆè®­ç»ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.

