---
layout: default
title: Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images
---

# Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images

**arXiv**: [2511.09891v1](https://arxiv.org/abs/2511.09891) | [PDF](https://arxiv.org/pdf/2511.09891.pdf)

**ä½œè€…**: Jinfu Li, Yuqi Huang, Hong Song, Ting Wang, Jianghan Xia, Yucong Lin, Jingfan Fan, Jian Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå°ºåº¦æ„ŸçŸ¥ä¸­ç»§å±‚ä¸Žå°ºåº¦è‡ªé€‚åº”æŸå¤±ä»¥è§£å†³èˆªæ‹å›¾åƒå¾®å°ç›®æ ‡æ£€æµ‹é—®é¢˜**

**å…³é”®è¯**: `å¾®å°ç›®æ ‡æ£€æµ‹` `èˆªæ‹å›¾åƒ` `å°ºåº¦æ„ŸçŸ¥` `æ³¨æ„åŠ›æœºåˆ¶` `æŸå¤±å‡½æ•°ä¼˜åŒ–` `ç›®æ ‡æ£€æµ‹æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¾®å°ç›®æ ‡ç‰¹å¾åœ¨é•¿è·ç¦»ç½‘ç»œä¼ æ’­ä¸­æ˜“é€€åŒ–ï¼Œä¸”è®­ç»ƒä¸­å›žå½’æƒ©ç½šä¸å‡è¡¡
2. æ–¹æ³•è¦ç‚¹ï¼šSARLé€šè¿‡è·¨å°ºåº¦ç©ºé—´-é€šé“æ³¨æ„åŠ›å¢žå¼ºç‰¹å¾ï¼ŒSALåŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡èšç„¦å¾®å°ç›®æ ‡
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæå‡APè¾¾5.5%ï¼Œå¹¶åœ¨å™ªå£°æ•°æ®é›†ä¸Šå®žçŽ°29.0% AP

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, despite the remarkable advancements in object detection, modern detectors still struggle to detect tiny objects in aerial images. One key reason is that tiny objects carry limited features that are inevitably degraded or lost during long-distance network propagation. Another is that smaller objects receive disproportionately greater regression penalties than larger ones during training. To tackle these issues, we propose a Scale-Aware Relay Layer (SARL) and a Scale-Adaptive Loss (SAL) for tiny object detection, both of which are seamlessly compatible with the top-performing frameworks. Specifically, SARL employs a cross-scale spatial-channel attention to progressively enrich the meaningful features of each layer and strengthen the cross-layer feature sharing. SAL reshapes the vanilla IoU-based losses so as to dynamically assign lower weights to larger objects. This loss is able to focus training on tiny objects while reducing the influence on large objects. Extensive experiments are conducted on three benchmarks (\textit{i.e.,} AI-TOD, DOTA-v2.0 and VisDrone2019), and the results demonstrate that the proposed method boosts the generalization ability by 5.5\% Average Precision (AP) when embedded in YOLOv5 (anchor-based) and YOLOx (anchor-free) baselines. Moreover, it also promotes the robust performance with 29.0\% AP on the real-world noisy dataset (\textit{i.e.,} AI-TOD-v2.0).

