---
layout: default
title: SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection
---

# SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection

**arXiv**: [2511.09870v1](https://arxiv.org/abs/2511.09870) | [PDF](https://arxiv.org/pdf/2511.09870.pdf)

**ä½œè€…**: Jia Lin, Xiaofei Zhou, Jiyuan Liu, Runmin Cong, Guodao Zhang, Zhi Liu, Jiyong Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAM-DAQä»¥è§£å†³RGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä¸­çš„æç¤ºä¾èµ–ä¸Žè®¡ç®—è´Ÿæ‹…é—®é¢˜**

**å…³é”®è¯**: `RGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹` `Segment Anythingæ¨¡åž‹` `æ·±åº¦å¼•å¯¼é€‚é…å™¨` `æŸ¥è¯¢é©±åŠ¨æ—¶åºå†…å­˜` `å¤šæ¨¡æ€ç‰¹å¾èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSAMç›´æŽ¥ç”¨äºŽRGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹æ—¶ä¾èµ–æ‰‹åŠ¨æç¤ºã€å†…å­˜æ¶ˆè€—é«˜ä¸”è®¡ç®—è´Ÿæ‹…é‡
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ·±åº¦å¼•å¯¼å¹¶è¡Œé€‚é…å™¨å’ŒæŸ¥è¯¢é©±åŠ¨æ—¶åºå†…å­˜æ¨¡å—é›†æˆæ·±åº¦ä¸Žæ—¶é—´çº¿ç´¢
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªRGB-D VSODæ•°æ®é›†ä¸Šå®žéªŒï¼Œæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.

