---
layout: default
title: SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation
---

# SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation

**arXiv**: [2511.10518v1](https://arxiv.org/abs/2511.10518) | [PDF](https://arxiv.org/pdf/2511.10518.pdf)

**ä½œè€…**: Wei Li, Renshan Zhang, Rui Shao, Zhijian Fang, Kaiwen Zhou, Zhuotao Tian, Liqiang Nie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSemanticVLAæ¡†æž¶ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½ç¨€ç–åŒ–ä¸Žå¢žå¼ºæå‡æœºå™¨äººæ“ä½œæ•ˆçŽ‡**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `è¯­ä¹‰å¯¹é½` `ç¨€ç–åŒ–` `ç‰¹å¾èžåˆ` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹å­˜åœ¨æ„ŸçŸ¥å†—ä½™å’ŒæŒ‡ä»¤-è§†è§‰å¯¹é½ä¸è¶³ï¼Œå½±å“æœºå™¨äººæ“ä½œæ•ˆçŽ‡
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¯­ä¹‰å¼•å¯¼åŒè§†è§‰å‰ªæžå™¨å’Œåˆ†å±‚èžåˆå™¨ï¼Œç¨€ç–åŒ–è§†è§‰è¾“å…¥å¹¶èžåˆè¯­ä¹‰ä¸Žå‡ ä½•ç‰¹å¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†ä¸ŠæˆåŠŸçŽ‡æå‡21.1%ï¼Œè®­ç»ƒå’ŒæŽ¨ç†æˆæœ¬åˆ†åˆ«é™ä½Ž3.0å€å’Œ2.7å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA

