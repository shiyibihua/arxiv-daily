---
layout: default
title: Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling
---

# Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling

**arXiv**: [2511.10648v1](https://arxiv.org/abs/2511.10648) | [PDF](https://arxiv.org/pdf/2511.10648.pdf)

**ä½œè€…**: Jiahao Wang, Weiye Xu, Aijun Yang, Wengang Zhou, Lewei Lu, Houqiang Li, Xiaohua Wang, Jinguo Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªä¸€è‡´æ€§é‡‡æ ·ä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­åŸºäºŽç»“æžœå¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„ä¸å¯é è½¨è¿¹é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `è‡ªä¸€è‡´æ€§é‡‡æ ·` `ç»“æžœå¥–åŠ±` `å¤šæ¨¡æ€æŽ¨ç†` `è½¨è¿¹å¯é æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æŽ¨ç†ä¸­ï¼Œé”™è¯¯æ€ç»´é“¾ä½†çŒœå¯¹é€‰é¡¹çš„è½¨è¿¹èŽ·å¾—ç›¸åŒå¥–åŠ±ï¼Œå½±å“è®­ç»ƒå¿ å®žæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§†è§‰æ‰°åŠ¨å’Œè½¨è¿¹æˆªæ–­é‡é‡‡æ ·ï¼Œè®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼Œåœ¨ç­–ç•¥æ›´æ–°ä¸­é™ä½Žä¸å¯é è½¨è¿¹æƒé‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šæå‡å‡†ç¡®çŽ‡é«˜è¾¾7.7ä¸ªç™¾åˆ†ç‚¹ï¼Œè®¡ç®—å¼€é”€å¯å¿½ç•¥ï¼Œé€‚ç”¨äºŽä¸åŒæ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

