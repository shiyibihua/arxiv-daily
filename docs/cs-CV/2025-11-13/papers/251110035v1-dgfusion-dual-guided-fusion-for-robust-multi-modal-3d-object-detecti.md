---
layout: default
title: DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection
---

# DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection

**arXiv**: [2511.10035v1](https://arxiv.org/abs/2511.10035) | [PDF](https://arxiv.org/pdf/2511.10035.pdf)

**ä½œè€…**: Feiyang Jia, Caiyan Jia, Ailin Liu, Shaoqing Xu, Qiming Xia, Lin Liu, Lei Yang, Yan Gong, Ziying Song

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDGFusionåŒå¼•å¯¼èžåˆæ–¹æ³•ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­å¤šæ¨¡æ€3Dç›®æ ‡æ£€æµ‹å¯¹å›°éš¾å®žä¾‹çš„æŒ‘æˆ˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€3Dç›®æ ‡æ£€æµ‹` `åŒå¼•å¯¼èžåˆ` `éš¾åº¦æ„ŸçŸ¥åŒ¹é…` `è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥` `å›°éš¾å®žä¾‹æ£€æµ‹` `ç‰¹å¾èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šæ¨¡æ€3Dæ£€æµ‹æ–¹æ³•é‡‡ç”¨å•å¼•å¯¼èŒƒå¼ï¼Œéš¾ä»¥å¤„ç†è¿œè·ç¦»ã€å°å°ºå¯¸æˆ–é®æŒ¡å¯¹è±¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŒå¼•å¯¼èŒƒå¼ï¼Œç»“åˆç‚¹äº‘å¼•å¯¼å›¾åƒå’Œå›¾åƒå¼•å¯¼ç‚¹äº‘ï¼Œé€šè¿‡éš¾åº¦æ„ŸçŸ¥å®žä¾‹åŒ¹é…å®žçŽ°ç‰¹å¾èžåˆã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨nuScenesæ•°æ®é›†ä¸Šï¼ŒmAPã€NDSå’Œå¹³å‡å¬å›žçŽ‡åˆ†åˆ«æå‡1.0%ã€0.8%å’Œ1.3%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.

