---
layout: default
title: Rethinking Visual Information Processing in Multimodal LLMs
---

# Rethinking Visual Information Processing in Multimodal LLMs

**arXiv**: [2511.10301v1](https://arxiv.org/abs/2511.10301) | [PDF](https://arxiv.org/pdf/2511.10301.pdf)

**ä½œè€…**: Dongwan Kim, Viresh Ranjan, Takashi Nagata, Arnab Dhua, Amit Kumar K C

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLaViTä»¥è§£å†³å¤šæ¨¡æ€LLMä¸­è§†è§‰ç‰¹å¾æ•´åˆä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰è¯­è¨€å»ºæ¨¡` `LLaViTæž¶æž„` `è§†è§‰ç‰¹å¾æ•´åˆ` `åŒå‘æ³¨æ„åŠ›` `å…¨å±€å±€éƒ¨è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLaVAæž¶æž„å› æ–‡æœ¬ä¸Žè§†è§‰æ¨¡æ€ä¸åŒ¹é…ï¼Œéš¾ä»¥æœ‰æ•ˆæ•´åˆè§†è§‰ç‰¹å¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç‹¬ç«‹è§†è§‰QKVæŠ•å½±ã€åŒå‘è§†è§‰æ³¨æ„åŠ›å’Œå…¨å±€å±€éƒ¨è¡¨ç¤ºï¼Œä½¿LLMåŒæ—¶ä½œä¸ºè§†è§‰ç¼–ç å™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽLLaVAï¼Œç”šè‡³è¶…è¶Šå‚æ•°ç¿»å€çš„æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.

