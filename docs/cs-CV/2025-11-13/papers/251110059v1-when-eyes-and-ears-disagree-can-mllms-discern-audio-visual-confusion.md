---
layout: default
title: When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?
---

# When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?

**arXiv**: [2511.10059v1](https://arxiv.org/abs/2511.10059) | [PDF](https://arxiv.org/pdf/2511.10059.pdf)

**ä½œè€…**: Qilang Ye, Wei Zeng, Meng Liu, Jie Zhang, Yupeng Hu, Zitong Yu, Yu Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRL-CoMMä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†å¬æ··æ·†åœºæ™¯ä¸­çš„éŸ³é¢‘æŽ¨ç†ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†å¬æ··æ·†` `å¼ºåŒ–å­¦ä¹ ` `éŸ³é¢‘æŽ¨ç†ä¼˜åŒ–` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMsåœ¨è§†è§‰ä¸»å¯¼ä¸‹éš¾ä»¥è¯†åˆ«éŸ³é¢‘ç¼ºå¤±çš„æ··æ·†å¯¹è±¡ï¼Œå¦‚è§†é¢‘ä¸­ç‰©ä½“æ— å£°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤–éƒ¨éŸ³é¢‘æ¨¡åž‹ç”ŸæˆéŸ³é¢‘æŽ¨ç†ï¼Œå¹¶è®¾è®¡å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ä¼˜åŒ–å¤šæ¨¡æ€æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†å¬é—®ç­”å’Œå¹»è§‰ä»»åŠ¡ä¸­ï¼ŒRL-CoMMæ¯”åŸºçº¿æ¨¡åž‹å‡†ç¡®çŽ‡æå‡10~30%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.

