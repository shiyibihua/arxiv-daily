---
layout: default
title: CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification
---

# CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification

**arXiv**: [2511.10309v1](https://arxiv.org/abs/2511.10309) | [PDF](https://arxiv.org/pdf/2511.10309.pdf)

**ä½œè€…**: Xiaomei Yang, Xizhan Gao, Sijie Niu, Fa Zhu, Guang Feng, Xiaofeng Qu, David Camacho

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLIP4VI-ReIDä»¥è§£å†³å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«` `æ¨¡æ€å…±äº«è¡¨ç¤º` `CLIPè¯­ä¹‰æ¡¥æŽ¥` `è·¨æ¨¡æ€å¯¹é½` `æ–‡æœ¬è¯­ä¹‰ç”Ÿæˆ` `ç‰¹å¾åµŒå…¥ä¿®æ­£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯è§å…‰ä¸Žçº¢å¤–å›¾åƒç‰©ç†ç‰¹æ€§å·®å¼‚å¤§ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½å›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ–‡æœ¬è¯­ä¹‰æ¡¥æŽ¥ï¼Œç”Ÿæˆå¯è§å…‰æ–‡æœ¬å¹¶ä¿®æ­£çº¢å¤–ç‰¹å¾ï¼Œå®žçŽ°æ¨¡æ€å…±äº«è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªVI-ReIDæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡è¯†åˆ«æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.

