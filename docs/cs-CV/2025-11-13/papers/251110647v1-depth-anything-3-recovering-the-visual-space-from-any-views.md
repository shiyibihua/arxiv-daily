---
layout: default
title: Depth Anything 3: Recovering the Visual Space from Any Views
---

# Depth Anything 3: Recovering the Visual Space from Any Views

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.10647" target="_blank" class="toolbar-btn">arXiv: 2511.10647v1</a>
    <a href="https://arxiv.org/pdf/2511.10647.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10647v1" 
            onclick="toggleFavorite(this, '2511.10647v1', 'Depth Anything 3: Recovering the Visual Space from Any Views')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-13

**Â§áÊ≥®**: https://depth-anything-3.github.io/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Depth Anything 3Ôºö‰ªé‰ªªÊÑèËßÜËßíÊÅ¢Â§çÁ©∫Èó¥Âá†‰Ωï‰ø°ÊÅØÔºåÊó†ÈúÄÊû∂ÊûÑÁâπÂåñ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶‰º∞ËÆ°` `ËßÜËßâÂá†‰Ωï` `Transformer` `Ëá™ÁõëÁù£Â≠¶‰π†` `Â§öËßÜËßíÈáçÂª∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰ªéÂ§öËßÜËßíÂõæÂÉè‰∏≠ÊÅ¢Â§çÂá†‰Ωï‰ø°ÊÅØÊó∂ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂ§çÊùÇÁöÑÁΩëÁªúÁªìÊûÑÂíåÂ§ö‰ªªÂä°Â≠¶‰π†ÔºåÂ¢ûÂä†‰∫ÜÊ®°ÂûãÂ§çÊùÇÂ∫¶ÂíåËÆ≠ÁªÉÈöæÂ∫¶„ÄÇ
2. DA3ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈááÁî®ÊûÅÁÆÄÂª∫Ê®°ÊñπÂºèÔºå‰ΩøÁî®Âçï‰∏ÄÁöÑTransformerÈ™®Âπ≤ÁΩëÁªúÂíåÊ∑±Â∫¶Â∞ÑÁ∫øÈ¢ÑÊµãÁõÆÊ†áÔºåÈÅøÂÖç‰∫ÜÊû∂ÊûÑÁâπÂåñÂíåÂ§çÊùÇÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†„ÄÇ
3. DA3Âú®Êñ∞ÁöÑËßÜËßâÂá†‰ΩïÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú®Áõ∏Êú∫‰ΩçÂßøÂíåÂá†‰ΩïÁ≤æÂ∫¶ÊñπÈù¢ÂùáË∂ÖË∂ä‰∫ÜÁé∞ÊúâSOTAÊñπÊ≥ïÔºåÂπ∂Âú®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰∏ä‰ºò‰∫éDA2„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜDepth Anything 3 (DA3)Ôºå‰∏Ä‰∏™ËÉΩÂ§ü‰ªé‰ªªÊÑèÊï∞ÈáèÁöÑËßÜËßâËæìÂÖ•‰∏≠È¢ÑÊµãÁ©∫Èó¥‰∏ÄËá¥Âá†‰Ωï‰ø°ÊÅØÁöÑÊ®°ÂûãÔºåÊó†ËÆ∫ÊòØÂê¶Â∑≤Áü•Áõ∏Êú∫‰ΩçÂßø„ÄÇDA3ËøΩÊ±ÇÊúÄÂ∞èÂåñÂª∫Ê®°ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏§‰∏™ÂÖ≥ÈîÆËßÅËß£Ôºö‰∏Ä‰∏™ÁÆÄÂçïÁöÑTransformerÔºà‰æãÂ¶ÇÔºåvanilla DINOÁºñÁ†ÅÂô®ÔºâË∂≥‰ª•‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÔºåÊó†ÈúÄÊû∂ÊûÑÁâπÂåñÔºõ‰ª•ÂèäÂçï‰∏ÄÁöÑÊ∑±Â∫¶Â∞ÑÁ∫øÈ¢ÑÊµãÁõÆÊ†áÊ∂àÈô§‰∫ÜÂ§çÊùÇÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÁöÑÈúÄË¶Å„ÄÇÈÄöËøáÊïôÂ∏à-Â≠¶ÁîüËÆ≠ÁªÉËåÉÂºèÔºåËØ•Ê®°ÂûãÂÆûÁé∞‰∫Ü‰∏éDepth Anything 2 (DA2)Áõ∏ÂΩìÁöÑÁªÜËäÇÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊàë‰ª¨Âª∫Á´ã‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËßÜËßâÂá†‰ΩïÂü∫ÂáÜÔºåÊ∂µÁõñÁõ∏Êú∫‰ΩçÂßø‰º∞ËÆ°„ÄÅ‰ªªÊÑèËßÜËßíÂá†‰ΩïÂíåËßÜËßâÊ∏≤Êüì„ÄÇÂú®ËØ•Âü∫ÂáÜ‰∏äÔºåDA3Âú®ÊâÄÊúâ‰ªªÂä°‰∏äÈÉΩËææÂà∞‰∫ÜÊñ∞ÁöÑstate-of-the-artÔºåÂú®Áõ∏Êú∫‰ΩçÂßøÁ≤æÂ∫¶ÊñπÈù¢Âπ≥ÂùáË∂ÖËøáÂÖàÂâçÁöÑSOTA VGGT 44.3%ÔºåÂú®Âá†‰ΩïÁ≤æÂ∫¶ÊñπÈù¢Ë∂ÖËøá25.1%„ÄÇÊ≠§Â§ñÔºåÂÆÉÂú®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÊñπÈù¢‰ºò‰∫éDA2„ÄÇÊâÄÊúâÊ®°ÂûãÈÉΩ‰ªÖÂú®ÂÖ¨ÂÖ±Â≠¶ÊúØÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªé‰ªªÊÑèÊï∞ÈáèÁöÑËßÜËßâËæìÂÖ•‰∏≠ÊÅ¢Â§çÁ©∫Èó¥‰∏ÄËá¥ÁöÑÂá†‰Ωï‰ø°ÊÅØÁöÑÈóÆÈ¢òÔºåÊó†ËÆ∫ÊòØÂê¶Â∑≤Áü•Áõ∏Êú∫‰ΩçÂßø„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§çÊùÇÁöÑÁΩëÁªúÊû∂ÊûÑÂíåÂ§ö‰ªªÂä°Â≠¶‰π†ÔºåËøôÂ¢ûÂä†‰∫ÜÊ®°ÂûãÁöÑÂ§çÊùÇÊÄßÔºåÂπ∂ÂèØËÉΩÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÊ≥õÂåñËÉΩÂäõ‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÈíàÂØπÁâπÂÆö‰ªªÂä°ËÆæËÆ°ÁöÑÊû∂ÊûÑÂèØËÉΩÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDA3ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®ÊûÅÁÆÄÂª∫Ê®°ÊñπÂºèÔºåÂç≥‰ΩøÁî®‰∏Ä‰∏™ÁÆÄÂçïÁöÑTransformerÈ™®Âπ≤ÁΩëÁªúÔºàÂ¶Çvanilla DINOÁºñÁ†ÅÂô®ÔºâÂíåÂçï‰∏ÄÁöÑÊ∑±Â∫¶Â∞ÑÁ∫øÈ¢ÑÊµãÁõÆÊ†á„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÂáèÂ∞ëÊ®°ÂûãÁöÑÂ§çÊùÇÊÄßÔºåÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÔºåÂπ∂Â¢ûÂº∫Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÈÅøÂÖçÊû∂ÊûÑÁâπÂåñÂíåÂ§çÊùÇÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÔºåDA3ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†ËßÜËßâÁ©∫Èó¥‰∏≠ÁöÑÂá†‰Ωï‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDA3ÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏Ä‰∏™TransformerÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™Ê∑±Â∫¶È¢ÑÊµãÊ®°Âùó„ÄÇTransformerÁºñÁ†ÅÂô®Ë¥üË¥£ÊèêÂèñËæìÂÖ•ÂõæÂÉèÁöÑÁâπÂæÅÔºåÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóÂàôÊ†πÊçÆËøô‰∫õÁâπÂæÅÈ¢ÑÊµãÊØè‰∏™ÂÉèÁ¥†ÁöÑÊ∑±Â∫¶ÂÄº„ÄÇÊ®°ÂûãÈááÁî®ÊïôÂ∏à-Â≠¶ÁîüËÆ≠ÁªÉËåÉÂºèÔºåÂÖ∂‰∏≠ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÈ´òË¥®ÈáèÁöÑÊ∑±Â∫¶‰ø°ÊÅØÔºåÂ≠¶ÁîüÊ®°ÂûãÂàôÂ≠¶‰π†Ê®°‰ªøÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDA3ÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂ÊûÅÁÆÄÂª∫Ê®°ÊñπÂºè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåDA3ÈÅøÂÖç‰∫ÜÂ§çÊùÇÁöÑÁΩëÁªúÊû∂ÊûÑÂíåÂ§ö‰ªªÂä°Â≠¶‰π†ÔºåËÄåÊòØÈááÁî®‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑTransformerÈ™®Âπ≤ÁΩëÁªúÂíåÂçï‰∏ÄÁöÑÊ∑±Â∫¶Â∞ÑÁ∫øÈ¢ÑÊµãÁõÆÊ†á„ÄÇËøôÁßçËÆæËÆ°‰∏ç‰ªÖÈôç‰Ωé‰∫ÜÊ®°ÂûãÁöÑÂ§çÊùÇÊÄßÔºåËøòÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDA3ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®vanilla DINOÁºñÁ†ÅÂô®‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÔºåÊó†ÈúÄ‰ªª‰Ωï‰øÆÊîπÔºõ2) ÈááÁî®Ê∑±Â∫¶Â∞ÑÁ∫øÈ¢ÑÊµã‰Ωú‰∏∫Âçï‰∏ÄÁöÑÂ≠¶‰π†ÁõÆÊ†áÔºåÈÅøÂÖç‰∫ÜÂ§ö‰ªªÂä°Â≠¶‰π†‰∏≠ÁöÑ‰ªªÂä°ÂÜ≤Á™ÅÔºõ3) ‰ΩøÁî®ÊïôÂ∏à-Â≠¶ÁîüËÆ≠ÁªÉËåÉÂºèÔºåÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÈ´òË¥®ÈáèÁöÑÊ∑±Â∫¶‰ø°ÊÅØÔºåÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DA3Âú®Êñ∞ÁöÑËßÜËßâÂá†‰ΩïÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú®Áõ∏Êú∫‰ΩçÂßøÁ≤æÂ∫¶ÊñπÈù¢Âπ≥ÂùáË∂ÖËøáÂÖàÂâçÁöÑSOTA VGGT 44.3%ÔºåÂú®Âá†‰ΩïÁ≤æÂ∫¶ÊñπÈù¢Ë∂ÖËøá25.1%„ÄÇÊ≠§Â§ñÔºåDA3Âú®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÊñπÈù¢‰πü‰ºò‰∫éDA2ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊâÄÊúâÊ®°ÂûãÈÉΩ‰ªÖÂú®ÂÖ¨ÂÖ±Â≠¶ÊúØÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DA3ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂåÖÊã¨Êú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆû„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÊûÑÂª∫‰∏âÁª¥Âú∫ÊôØÂú∞Âõæ„ÄÅËøõË°åÁâ©‰ΩìËØÜÂà´ÂíåË∑üË∏™„ÄÅ‰ª•ÂèäÂÆûÁé∞ÈÄºÁúüÁöÑËßÜËßâÊ∏≤Êüì„ÄÇÈÄöËøá‰ªé‰ªªÊÑèËßÜËßíÊÅ¢Â§çÁ©∫Èó¥Âá†‰Ωï‰ø°ÊÅØÔºåDA3ÂèØ‰ª•‰∏∫Ëøô‰∫õÂ∫îÁî®Êèê‰æõÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÂèØÈù†ÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òÁ≥ªÁªüÁöÑÊÄßËÉΩÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

