---
layout: default
title: Depth Anything 3: Recovering the Visual Space from Any Views
---

# Depth Anything 3: Recovering the Visual Space from Any Views

**arXiv**: [2511.10647v1](https://arxiv.org/abs/2511.10647) | [PDF](https://arxiv.org/pdf/2511.10647.pdf)

**ä½œè€…**: Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDepth Anything 3æ¨¡åž‹ï¼Œä»Žä»»æ„è§†å›¾é¢„æµ‹ç©ºé—´ä¸€è‡´å‡ ä½•ï¼Œæ— éœ€å·²çŸ¥ç›¸æœºä½å§¿ã€‚**

**å…³é”®è¯**: `å¤šè§†å›¾å‡ ä½•` `æ·±åº¦ä¼°è®¡` `Transformeréª¨å¹²` `å¸ˆç”Ÿè®­ç»ƒ` `ç›¸æœºä½å§¿ä¼°è®¡` `è§†è§‰æ¸²æŸ“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žä»»æ„æ•°é‡è§†è§‰è¾“å…¥ä¸­æ¢å¤ç©ºé—´ä¸€è‡´å‡ ä½•ï¼Œæ— éœ€å·²çŸ¥ç›¸æœºä½å§¿ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å•ä¸€Transformeréª¨å¹²å’Œæ·±åº¦å°„çº¿é¢„æµ‹ç›®æ ‡ï¼Œé¿å…å¤æ‚å¤šä»»åŠ¡å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†è§‰å‡ ä½•åŸºå‡†ä¸Šè¶…è¶ŠSOTAï¼Œç›¸æœºä½å§¿ç²¾åº¦å¹³å‡æå‡44.3%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

