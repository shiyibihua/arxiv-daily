---
layout: default
title: Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment
---

# Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment

**arXiv**: [2511.09948v1](https://arxiv.org/abs/2511.09948) | [PDF](https://arxiv.org/pdf/2511.09948.pdf)

**ä½œè€…**: Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”èžåˆæ¡†æž¶ï¼Œç»“åˆä½™å¼¦ç›¸ä¼¼åº¦ä¸Žç‰¹å¾å¹…åº¦ï¼Œæå‡æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ€§èƒ½ã€‚**

**å…³é”®è¯**: `æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°` `CLIPæ¨¡åž‹` `ç‰¹å¾å¹…åº¦` `è‡ªé€‚åº”èžåˆ` `Box-Coxå˜æ¢` `ä½™å¼¦ç›¸ä¼¼åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPæ¨¡åž‹ç”¨äºŽNR-IQAæ—¶ï¼Œä»…ä¾èµ–è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå¿½ç•¥å›¾åƒç‰¹å¾å¹…åº¦ä¸Žæ„ŸçŸ¥è´¨é‡çš„ç›¸å…³æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå–CLIPå›¾åƒç‰¹å¾ç»å¯¹å¹…åº¦ï¼Œåº”ç”¨Box-Coxå˜æ¢å½’ä¸€åŒ–ï¼Œå¹¶ä¸Žä½™å¼¦ç›¸ä¼¼åº¦è‡ªé€‚åº”èžåˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªIQAåŸºå‡†æ•°æ®é›†ä¸Šï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.

