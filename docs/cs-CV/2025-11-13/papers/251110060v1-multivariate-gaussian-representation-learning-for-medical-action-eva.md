---
layout: default
title: Multivariate Gaussian Representation Learning for Medical Action Evaluation
---

# Multivariate Gaussian Representation Learning for Medical Action Evaluation

**arXiv**: [2511.10060v1](https://arxiv.org/abs/2511.10060) | [PDF](https://arxiv.org/pdf/2511.10060.pdf)

**ä½œè€…**: Luming Yang, Haoxian Liu, Siqing Li, Alper Yilmaz

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGaussMedActæ¡†æž¶ä»¥è§£å†³åŒ»ç–—åŠ¨ä½œè¯„ä¼°ä¸­çš„ç²¾ç»†å»ºæ¨¡æŒ‘æˆ˜**

**å…³é”®è¯**: `åŒ»ç–—åŠ¨ä½œè¯„ä¼°` `å¤šå…ƒé«˜æ–¯è¡¨ç¤º` `æ—¶ç©ºå»ºæ¨¡` `éª¨éª¼ç‰¹å¾ç¼–ç ` `å®žæ—¶æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŒ»ç–—åŠ¨ä½œè¯„ä¼°é¢ä¸´æ•°æ®é›†ç¨€ç¼ºã€ç²¾åº¦è¦æ±‚é«˜å’Œå¿«é€ŸåŠ¨ä½œå»ºæ¨¡ä¸è¶³çš„é—®é¢˜
2. é‡‡ç”¨å¤šå…ƒé«˜æ–¯ç¼–ç å°†åŠ¨ä½œåˆ†è§£ä¸ºè‡ªé€‚åº”3Dé«˜æ–¯ä»¤ç‰Œï¼Œç»“åˆåŒæµç©ºé—´ç¼–ç 
3. åœ¨CPREval-6kåŸºå‡†ä¸Šè¾¾åˆ°92.1%å‡†ç¡®çŽ‡ï¼Œä¼˜äºŽåŸºçº¿ä¸”è®¡ç®—æ•ˆçŽ‡é«˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming the ST-GCN baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.

