---
layout: default
title: eXIAA: eXplainable Injections for Adversarial Attack
---

# eXIAA: eXplainable Injections for Adversarial Attack

**arXiv**: [2511.10088v1](https://arxiv.org/abs/2511.10088) | [PDF](https://arxiv.org/pdf/2511.10088.pdf)

**ä½œè€…**: Leonardo Pesce, Jiawen Wei, Gianmarco Mengaldo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºeXIAAé»‘ç›’å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œä»¥ä¿®æ”¹å›¾åƒè§£é‡Šè€Œä¸æ”¹å˜é¢„æµ‹ç±»åˆ«**

**å…³é”®è¯**: `å¯¹æŠ—æ”»å‡»` `åŽéªŒå¯è§£é‡Šæ€§` `é»‘ç›’æ”»å‡»` `å›¾åƒè§£é‡Š` `æ¨¡åž‹ä¸å¯çŸ¥` `å®‰å…¨æ¼æ´ž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŽéªŒå¯è§£é‡Šæ€§æ–¹æ³•åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å­˜åœ¨å¯é æ€§æ¼æ´ž
2. æ–¹æ³•è¦ç‚¹ï¼šæ— éœ€æ¨¡åž‹æƒé‡ï¼Œä»…ç”¨é¢„æµ‹å’Œè§£é‡Šï¼Œå•æ­¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNetä¸Šæ˜¾è‘—æ”¹å˜è§£é‡Šï¼Œä¿æŒé¢„æµ‹æ¦‚çŽ‡å’Œå›¾åƒç›¸ä¼¼æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Post-hoc explainability methods are a subset of Machine Learning (ML) that aim to provide a reason for why a model behaves in a certain way. In this paper, we show a new black-box model-agnostic adversarial attack for post-hoc explainable Artificial Intelligence (XAI), particularly in the image domain. The goal of the attack is to modify the original explanations while being undetected by the human eye and maintain the same predicted class. In contrast to previous methods, we do not require any access to the model or its weights, but only to the model's computed predictions and explanations. Additionally, the attack is accomplished in a single step while significantly changing the provided explanations, as demonstrated by empirical evaluation. The low requirements of our method expose a critical vulnerability in current explainability methods, raising concerns about their reliability in safety-critical applications. We systematically generate attacks based on the explanations generated by post-hoc explainability methods (saliency maps, integrated gradients, and DeepLIFT SHAP) for pretrained ResNet-18 and ViT-B16 on ImageNet. The results show that our attacks could lead to dramatically different explanations without changing the predictive probabilities. We validate the effectiveness of our attack, compute the induced change based on the explanation with mean absolute difference, and verify the closeness of the original image and the corrupted one with the Structural Similarity Index Measure (SSIM).

