---
layout: default
title: Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching
---

# Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching

**arXiv**: [2511.09955v1](https://arxiv.org/abs/2511.09955) | [PDF](https://arxiv.org/pdf/2511.09955.pdf)

**ä½œè€…**: Uday Bhaskar, Rishabh Bhattacharya, Avinash Patel, Sarthak Khoche, Praveen Anil Kulkarni, Naresh Manwani

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ¯å¯¹è±¡ååŒæ•™å­¦çš„ä¼ªæ ‡ç­¾è®­ç»ƒæ–¹æ³•ï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹æ£€æµ‹å™ªå£°é—®é¢˜ã€‚**

**å…³é”®è¯**: `ä¼ªæ ‡ç­¾è®­ç»ƒ` `æ¯å¯¹è±¡ååŒæ•™å­¦` `é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹` `è‡ªåŠ¨é©¾é©¶` `å™ªå£°è¿‡æ»¤` `YOLOæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é›¶æ ·æœ¬æ£€æµ‹ä¸­å»¶è¿Ÿé«˜ä¸”æ˜“äº§ç”Ÿå¹»è§‰é¢„æµ‹ï¼Œä¸é€‚åˆç›´æŽ¥éƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¸¤ä¸ªYOLOæ¨¡åž‹é€šè¿‡æ¯å¯¹è±¡æŸå¤±ååŒè¿‡æ»¤å™ªå£°è¾¹ç•Œæ¡†ï¼Œæå‡è®­ç»ƒé²æ£’æ€§ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨KITTIæ•°æ®é›†ä¸ŠmAP@0.5ä»Ž31.12%æå‡è‡³46.61%ï¼Œå¹¶æ”¯æŒå®žæ—¶æ£€æµ‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\%$ to $46.61\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\%$) leads to further performance gains, reaching $57.97\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.

