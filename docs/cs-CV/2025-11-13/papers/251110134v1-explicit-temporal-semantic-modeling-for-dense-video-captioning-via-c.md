---
layout: default
title: Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction
---

# Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction

**arXiv**: [2511.10134v1](https://arxiv.org/abs/2511.10134) | [PDF](https://arxiv.org/pdf/2511.10134.pdf)

**ä½œè€…**: Mingda Jia, Weiliang Meng, Zenghuang Fu, Yiheng Li, Qi Zeng, Yifan Zhang, Ju Xin, Rongtao Xu, Jiguang Zhang, Xiaopeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCACMIæ¡†æž¶ä»¥è§£å†³å¯†é›†è§†é¢‘æè¿°ä¸­æ—¶é—´è¿žè´¯æ€§å’Œè¯­ä¹‰å®Œæ•´æ€§é—®é¢˜**

**å…³é”®è¯**: `å¯†é›†è§†é¢‘æè¿°` `æ—¶é—´è¯­ä¹‰å»ºæ¨¡` `è·¨æ¨¡æ€äº¤äº’` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `äº‹ä»¶å®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–éšå¼å»ºæ¨¡ï¼Œæ— æ³•æ•æ‰äº‹ä»¶åºåˆ—çš„æ—¶é—´è¿žè´¯æ€§å’Œè§†è§‰ä¸Šä¸‹æ–‡è¯­ä¹‰
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è·¨æ¨¡æ€å¸§èšåˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾å¢žå¼ºï¼Œæ˜¾å¼å»ºæ¨¡æ—¶é—´è¯­ä¹‰
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ActivityNet Captionså’ŒYouCook2æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.

