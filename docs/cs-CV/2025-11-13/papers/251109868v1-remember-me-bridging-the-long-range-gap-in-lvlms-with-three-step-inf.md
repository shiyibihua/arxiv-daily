---
layout: default
title: Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies
---

# Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies

**arXiv**: [2511.09868v1](https://arxiv.org/abs/2511.09868) | [PDF](https://arxiv.org/pdf/2511.09868.pdf)

**ä½œè€…**: Peng Gao, Yujian Lee, Xiaofeng Zhang, Zailong Chen, Hui Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸‰æ­¥éª¤è¡°å‡éŸ§æ€§ç­–ç•¥ä»¥è§£å†³LVLMä¸­é•¿è·ç¦»ä¾èµ–å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `é•¿è·ç¦»ä¾èµ–` `æŽ¨ç†ä¼˜åŒ–` `æ³¨æ„åŠ›æœºåˆ¶` `è§†è§‰é—®ç­”` `ä½ç½®ç¼–ç `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLVLMä½¿ç”¨ROPEæ—¶ï¼Œé•¿è·ç¦»tokenå¯¹æ³¨æ„åŠ›è¡°å‡ï¼Œå½±å“å…¨å±€ä¸Šä¸‹æ–‡è®°å¿†
2. æ–¹æ³•è¦ç‚¹ï¼šæŽ¨ç†é˜¶æ®µåº”ç”¨ä¸‰æ­¥ç­–ç•¥ï¼Œå¢žå¼ºè¯­ä¹‰ä¿¡å·ã€è°ƒæŽ§è·ç¦»æƒé‡ã€å¼ºåŒ–è¿œç¨‹ä¾èµ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VQAåŸºå‡†ä¸Šè®­ç»ƒå…è´¹æå‡æ€§èƒ½ï¼Œä»£ç å·²å¼€æº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have achieved impressive performance across a wide range of multimodal tasks. However, they still face critical challenges in modeling long-range dependencies under the usage of Rotary Positional Encoding (ROPE). Although it can facilitate precise modeling of token positions, it induces progressive attention decay as token distance increases, especially with progressive attention decay over distant token pairs, which severely impairs the model's ability to remember global context. To alleviate this issue, we propose inference-only Three-step Decay Resilience Strategies (T-DRS), comprising (1) Semantic-Driven DRS (SD-DRS), amplifying semantically meaningful but distant signals via content-aware residuals, (2) Distance-aware Control DRS (DC-DRS), which can purify attention by smoothly modulating weights based on positional distances, suppressing noise while preserving locality, and (3) re-Reinforce Distant DRS (reRD-DRS), consolidating the remaining informative remote dependencies to maintain global coherence. Together, the T-DRS recover suppressed long-range token pairs without harming local inductive biases. Extensive experiments on Vision Question Answering (VQA) benchmarks demonstrate that T-DRS can consistently improve performance in a training-free manner. The code can be accessed in https://github.com/labixiaoq-qq/Remember-me

