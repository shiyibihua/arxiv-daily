---
layout: default
title: FreDFT: Frequency Domain Fusion Transformer for Visible-Infrared Object Detection
---

# FreDFT: Frequency Domain Fusion Transformer for Visible-Infrared Object Detection

**arXiv**: [2511.10046v1](https://arxiv.org/abs/2511.10046) | [PDF](https://arxiv.org/pdf/2511.10046.pdf)

**ä½œè€…**: Wencong Wu, Xiuwei Zhang, Hanlin Yin, Shun Dai, Hongxi Zhang, Yanning Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFreDFTé¢‘çŽ‡åŸŸèžåˆTransformerä»¥è§£å†³å¯è§å…‰-çº¢å¤–ç›®æ ‡æ£€æµ‹ä¸­çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `å¯è§å…‰-çº¢å¤–ç›®æ ‡æ£€æµ‹` `é¢‘çŽ‡åŸŸTransformer` `è·¨æ¨¡æ€èžåˆ` `å¤šæ¨¡æ€æ³¨æ„åŠ›` `ç›®æ ‡æ£€æµ‹ç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯è§å…‰å’Œçº¢å¤–æ¨¡æ€åœ¨å¤æ‚åœºæ™¯ä¸­å­˜åœ¨ä¿¡æ¯ä¸å¹³è¡¡ï¼Œå¯¼è‡´è·¨æ¨¡æ€èžåˆä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨é¢‘çŽ‡åŸŸæ³¨æ„åŠ›æŒ–æŽ˜äº’è¡¥ä¿¡æ¯ï¼Œå¹¶è®¾è®¡ç©ºé—´-é€šé“äº¤äº’æ¨¡å—æ¶ˆé™¤ä¸å¹³è¡¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸ŠéªŒè¯äº†ä¼˜äºŽçŽ°æœ‰æ–¹æ³•çš„æ£€æµ‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visible-infrared object detection has gained sufficient attention due to its detection performance in low light, fog, and rain conditions. However, visible and infrared modalities captured by different sensors exist the information imbalance problem in complex scenarios, which can cause inadequate cross-modal fusion, resulting in degraded detection performance. \textcolor{red}{Furthermore, most existing methods use transformers in the spatial domain to capture complementary features, ignoring the advantages of developing frequency domain transformers to mine complementary information.} To solve these weaknesses, we propose a frequency domain fusion transformer, called FreDFT, for visible-infrared object detection. The proposed approach employs a novel multimodal frequency domain attention (MFDA) to mine complementary information between modalities and a frequency domain feed-forward layer (FDFFL) via a mixed-scale frequency feature fusion strategy is designed to better enhance multimodal features. To eliminate the imbalance of multimodal information, a cross-modal global modeling module (CGMM) is constructed to perform pixel-wise inter-modal feature interaction in a spatial and channel manner. Moreover, a local feature enhancement module (LFEM) is developed to strengthen multimodal local feature representation and promote multimodal feature fusion by using various convolution layers and applying a channel shuffle. Extensive experimental results have verified that our proposed FreDFT achieves excellent performance on multiple public datasets compared with other state-of-the-art methods. The code of our FreDFT is linked at https://github.com/WenCongWu/FreDFT.

