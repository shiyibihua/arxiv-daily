---
layout: default
title: Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation
---

# Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation

**arXiv**: [2511.09958v1](https://arxiv.org/abs/2511.09958) | [PDF](https://arxiv.org/pdf/2511.09958.pdf)

**ä½œè€…**: Xiangyi Wei, Haotian Zhang, Xinyi Cao, Siyu Xie, Weifeng Ge, Yang Li, Changbo Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAudio-VLAæ¨¡åž‹ï¼Œåˆ©ç”¨æŽ¥è§¦éŸ³é¢‘å¢žå¼ºæœºå™¨äººæ“ä½œä¸­çš„åŠ¨æ€è¿‡ç¨‹æ„ŸçŸ¥ã€‚**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¤šæ¨¡æ€å­¦ä¹ ` `æŽ¥è§¦éŸ³é¢‘æ„ŸçŸ¥` `åŠ¨æ€è¿‡ç¨‹è¯„ä¼°` `è·¨æ¨¡æ€å¯¹é½` `LoRAå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨æ„ŸçŸ¥äº¤äº’åŠ¨æ€è¿‡ç¨‹æ–¹é¢å­˜åœ¨å±€é™ã€‚
2. èžåˆè§†è§‰ä¸ŽéŸ³é¢‘æ¨¡æ€ï¼Œé‡‡ç”¨é¢„è®­ç»ƒæ¨¡åž‹å’ŒLoRAå¾®è°ƒå®žçŽ°è·¨æ¨¡æ€ç†è§£ã€‚
3. åœ¨ä»¿çœŸå’ŒçœŸå®žä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜äºŽçº¯è§†è§‰æ–¹æ³•ï¼Œå¹¶å¼•å…¥TCRæŒ‡æ ‡è¯„ä¼°åŠ¨æ€è¿‡ç¨‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.

