---
layout: default
title: PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors
---

# PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors

**arXiv**: [2511.09905v1](https://arxiv.org/abs/2511.09905) | [PDF](https://arxiv.org/pdf/2511.09905.pdf)

**ä½œè€…**: Brian B. Moser, Shalini Strode, Federico Raue, Stanislav Frolov, Krzysztof Adamkiewicz, Arundhati Shanbhag, Joachim Folk, Tobias C. Nauen, Andreas Dengel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRISMæ¡†æž¶ä»¥è§£å†³æ•°æ®é›†è’¸é¦ä¸­å•ä¸€æ•™å¸ˆæ¨¡åž‹åå·®é—®é¢˜**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `æ¨¡åž‹åå·®è§£è€¦` `å¤šæ•™å¸ˆå­¦ä¹ ` `ç‰¹å¾å¤šæ ·æ€§` `æ‰¹é‡å½’ä¸€åŒ–å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ•°æ®é›†è’¸é¦æ–¹æ³•å¸¸å—å•ä¸€æ•™å¸ˆæ¨¡åž‹å½’çº³åå·®å½±å“ï¼Œå¯¼è‡´æ ·æœ¬åŒè´¨åŒ–
2. PRISMé€šè¿‡è§£è€¦å¯¹æ•°åŒ¹é…å’Œæ­£åˆ™åŒ–ç›®æ ‡ï¼Œä½¿ç”¨ä¸åŒæ•™å¸ˆæž¶æž„ç›‘ç£
3. åœ¨ImageNet-1Kä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡ç±»å†…å¤šæ ·æ€§å¹¶é™ä½Žç‰¹å¾ä½™å¼¦ç›¸ä¼¼åº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dataset distillation (DD) promises compact yet faithful synthetic data, but existing approaches often inherit the inductive bias of a single teacher model. As dataset size increases, this bias drives generation toward overly smooth, homogeneous samples, reducing intra-class diversity and limiting generalization. We present PRISM (PRIors from diverse Source Models), a framework that disentangles architectural priors during synthesis. PRISM decouples the logit-matching and regularization objectives, supervising them with different teacher architectures: a primary model for logits and a stochastic subset for batch-normalization (BN) alignment. On ImageNet-1K, PRISM consistently and reproducibly outperforms single-teacher methods (e.g., SRe2L) and recent multi-teacher variants (e.g., G-VBSM) at low- and mid-IPC regimes. The generated data also show significantly richer intra-class diversity, as reflected by a notable drop in cosine similarity between features. We further analyze teacher selection strategies (pre- vs. intra-distillation) and introduce a scalable cross-class batch formation scheme for fast parallel synthesis. Code will be released after the review period.

