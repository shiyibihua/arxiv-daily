---
layout: default
title: Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization
---

# Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.10212" target="_blank" class="toolbar-btn">arXiv: 2511.10212v1</a>
    <a href="https://arxiv.org/pdf/2511.10212.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10212v1" 
            onclick="toggleFavorite(this, '2511.10212v1', 'Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ashutosh Anshul, Shreyas Gopal, Deepu Rajan, Eng Siong Chng

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-13

**Â§áÊ≥®**: Under Review, Multimodal Deepfake detection

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫é‰∏ã‰∏ÄÂ∏ßÁâπÂæÅÈ¢ÑÊµãÁöÑÂ§öÊ®°ÊÄÅDeepfakeÊ£ÄÊµã‰∏éÊó∂Â∫èÂÆö‰ΩçÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `DeepfakeÊ£ÄÊµã` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµã` `Êó∂Â∫èÂÆö‰Ωç` `Ê≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâDeepfakeÊ£ÄÊµãÊñπÊ≥ïÊ≥õÂåñÊÄß‰∏çË∂≥Ôºå‰∏î‰æßÈáçÈü≥ËßÜÈ¢ë‰∏ç‰∏ÄËá¥ÔºåÂøΩÁï•‰∫ÜÊ®°ÊÄÅÂÜÖÈÉ®ÁöÑ‰º™ÈÄ†„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÂçïÈò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËûçÂêàÂçïÊ®°ÊÄÅÂíåË∑®Ê®°ÊÄÅÁöÑ‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãÔºåÂ¢ûÂº∫Ê®°ÂûãÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂºïÂÖ•Á™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊçïÊçâÈ¢ÑÊµãÂ∏ß‰∏éÂÆûÈôÖÂ∏ßÂ∑ÆÂºÇÔºåÂÆûÁé∞Â±ÄÈÉ®‰º™ÈÄ†ÁóïËøπÊ£ÄÊµã‰∏éÊó∂Â∫èÂÆö‰Ωç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅDeepfakeÊ£ÄÊµãÊñπÊ≥ï‰∏∫‰∫ÜÊèêÂçáÊ≥õÂåñËÉΩÂäõÔºåÈÄöÂ∏∏ÈááÁî®È¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂπ∂‰∏î‰∏ªË¶ÅÂÖ≥Ê≥®Èü≥ËßÜÈ¢ë‰∏ç‰∏ÄËá¥ÊÄßÔºåÂÆπÊòìÂøΩÁï•Ê®°ÊÄÅÂÜÖÈÉ®ÁöÑ‰º™ÈÄ†ÁóïËøπÔºåÂØºËá¥Âú®Èü≥ËßÜÈ¢ëÂØπÈΩêÁöÑÁØ°ÊîπÊ†∑Êú¨‰∏äÂ§±Êïà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂçïÈò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÈÄöËøáÊï¥ÂêàÂçïÊ®°ÊÄÅÂíåË∑®Ê®°ÊÄÅÁöÑ‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãÊù•Â¢ûÂº∫Ê≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•Á™óÂè£Á∫ßÂà´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ÊçïÊçâÈ¢ÑÊµãÂ∏ßÂíåÂÆûÈôÖÂ∏ß‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºå‰ªéËÄåÊ£ÄÊµãÊØè‰∏™Â∏ßÂë®Âõ¥ÁöÑÂ±ÄÈÉ®‰º™ÈÄ†ÁóïËøπ„ÄÇËøôÂØπ‰∫éÂáÜÁ°ÆÂàÜÁ±ªÂÆåÂÖ®ÁØ°ÊîπÁöÑËßÜÈ¢ëÂíåÊúâÊïàÂÆö‰ΩçÈÉ®ÂàÜÁØ°ÊîπÊ†∑Êú¨‰∏≠ÁöÑDeepfakeÁâáÊÆµËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑËØÑ‰º∞Ë°®ÊòéÔºåËØ•Ê®°ÂûãÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÁ≤æÁ°ÆÁöÑÊó∂Â∫èÂÆö‰ΩçËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâDeepfakeÊ£ÄÊµãÊñπÊ≥ïÂú®Ê≥õÂåñÊÄßÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Èù¢ÂØπÊú™ËßÅËøáÁöÑÁØ°ÊîπÁ±ªÂûãÂíåÊï∞ÊçÆÈõÜÊó∂„ÄÇÊ≠§Â§ñÔºåËÆ∏Â§öÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Èü≥ËßÜÈ¢ë‰πãÈó¥ÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÔºåËÄåÂøΩÁï•‰∫ÜÂçï‰∏™Ê®°ÊÄÅÂÜÖÈÉ®ÁöÑ‰º™ÈÄ†ÁóïËøπ„ÄÇËøôÂØºËá¥Ëøô‰∫õÊñπÊ≥ïÂú®Â§ÑÁêÜÈü≥ËßÜÈ¢ëÂØπÈΩêÁöÑDeepfakeËßÜÈ¢ëÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÊõ¥Â•ΩÂú∞Ê≥õÂåñÂπ∂ËÉΩÊ£ÄÊµãÊ®°ÊÄÅÂÜÖÈÉ®‰º™ÈÄ†ÁóïËøπÁöÑDeepfakeÊ£ÄÊµãÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‰∏ã‰∏ÄÂ∏ßÁâπÂæÅÈ¢ÑÊµã‰Ωú‰∏∫‰∏ÄÁßçËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÂºèÔºåÊù•Â≠¶‰π†ÁúüÂÆûËßÜÈ¢ëÁöÑÂÜÖÂú®Ë°®Á§∫„ÄÇÈÄöËøáÈ¢ÑÊµã‰∏ã‰∏ÄÂ∏ßÁöÑÁâπÂæÅÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ËßÜÈ¢ëÁöÑÊó∂Â∫èÂä®ÊÄÅÂíåÊ®°ÊÄÅÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂêåÊó∂ÔºåÈÄöËøáÊØîËæÉÈ¢ÑÊµãÁöÑÁâπÂæÅÂíåÂÆûÈôÖÁöÑÁâπÂæÅÔºåÂèØ‰ª•Ê£ÄÊµãÂá∫ËßÜÈ¢ë‰∏≠Â≠òÂú®ÁöÑÂºÇÂ∏∏Ôºå‰ªéËÄåÂà§Êñ≠ËßÜÈ¢ëÊòØÂê¶Ë¢´ÁØ°Êîπ„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÂèØ‰ª•Ê£ÄÊµãÈü≥ËßÜÈ¢ë‰∏ç‰∏ÄËá¥ÁöÑÊÉÖÂÜµÔºåËøòÂèØ‰ª•Ê£ÄÊµãÊ®°ÊÄÅÂÜÖÈÉ®ÁöÑ‰º™ÈÄ†ÁóïËøπ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê®°ÂûãÈááÁî®ÂçïÈò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºöÂàÜÂà´ÊèêÂèñÈü≥ËßÜÈ¢ëÁâπÂæÅ„ÄÇ2) ‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãÊ®°ÂùóÔºöÂü∫‰∫éÂΩìÂâçÂ∏ßÁöÑÁâπÂæÅÈ¢ÑÊµã‰∏ã‰∏ÄÂ∏ßÁöÑÁâπÂæÅÔºåÂåÖÊã¨ÂçïÊ®°ÊÄÅÂíåË∑®Ê®°ÊÄÅÁöÑÈ¢ÑÊµã„ÄÇ3) Ê≥®ÊÑèÂäõÊ®°ÂùóÔºöÂºïÂÖ•Á™óÂè£Á∫ßÂà´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÆ°ÁÆóÈ¢ÑÊµãÂ∏ßÂíåÂÆûÈôÖÂ∏ß‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂπ∂ÊèêÂèñÂÖ≥ÈîÆÁöÑ‰º™ÈÄ†Âå∫Âüü„ÄÇ4) ÂàÜÁ±ªÊ®°ÂùóÔºöÂü∫‰∫éÊèêÂèñÁöÑÁâπÂæÅÂíåÊ≥®ÊÑèÂäõÊùÉÈáçÔºåÂà§Êñ≠ËßÜÈ¢ëÊòØÂê¶‰∏∫DeepfakeÔºåÂπ∂ËøõË°åÊó∂Â∫èÂÆö‰Ωç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜÂü∫‰∫é‰∏ã‰∏ÄÂ∏ßÁâπÂæÅÈ¢ÑÊµãÁöÑDeepfakeÊ£ÄÊµãÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ2) ÂºïÂÖ•‰∫ÜÁ™óÂè£Á∫ßÂà´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÁ≤æÁ°ÆÂÆö‰ΩçDeepfakeËßÜÈ¢ë‰∏≠ÁöÑÁØ°ÊîπÁâáÊÆµ„ÄÇ3) ÈááÁî®ÂçïÈò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÈÅøÂÖç‰∫ÜÈ¢ÑËÆ≠ÁªÉÂ∏¶Êù•ÁöÑÈ¢ùÂ§ñÂºÄÈîÄ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãÊ®°Âùó‰∏≠ÔºåÈááÁî®‰∫ÜLSTMÁΩëÁªúÊù•Âª∫Ê®°Êó∂Â∫èÂÖ≥Á≥ª„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨È¢ÑÊµãÊçüÂ§±ÂíåÂàÜÁ±ªÊçüÂ§±ÔºåÂÖ∂‰∏≠È¢ÑÊµãÊçüÂ§±Áî®‰∫éÁ∫¶Êùü‰∏ã‰∏ÄÂ∏ßÁâπÂæÅÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÔºåÂàÜÁ±ªÊçüÂ§±Áî®‰∫éÁ∫¶ÊùüDeepfakeÂàÜÁ±ªÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁ™óÂè£Â§ßÂ∞èÂíåÊ≥®ÊÑèÂäõÊùÉÈáçÊòØÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÊï∞ÊçÆÈõÜËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Ê®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂåÖÊã¨FaceForensics++„ÄÅDFDCÁ≠â„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Ê≥õÂåñËÉΩÂäõÂíåÊó∂Â∫èÂÆö‰ΩçÁ≤æÂ∫¶ÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®FaceForensics++Êï∞ÊçÆÈõÜ‰∏äÔºåËØ•Ê®°ÂûãÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü5%‰ª•‰∏äÔºåÊó∂Â∫èÂÆö‰ΩçÁöÑIoUÊèêÈ´ò‰∫Ü10%‰ª•‰∏ä„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂÖ∑ÊúâÂæàÂº∫ÁöÑÂÆûÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞„ÄÅÊñ∞ÈóªÂ™í‰ΩìÊú∫ÊûÑÁ≠âÔºåÁî®‰∫éÊ£ÄÊµãÂíåËØÜÂà´DeepfakeËßÜÈ¢ëÔºåÈò≤Ê≠¢ËôöÂÅá‰ø°ÊÅØÁöÑ‰º†Êí≠ÂíåÊÅ∂ÊÑèÊîªÂáª„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Â∫îÁî®‰∫éÂÆâÂÖ®ÁõëÊéßÈ¢ÜÂüüÔºåÁî®‰∫éÊ£ÄÊµãËßÜÈ¢ë‰∏≠ÁöÑÂºÇÂ∏∏Ë°å‰∏∫ÔºåÊèêÈ´òÂÆâÂÖ®Èò≤ËåÉËÉΩÂäõ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõ‰∏éÂå∫ÂùóÈìæÁ≠âÊäÄÊúØÁªìÂêàÔºåÂÆûÁé∞DeepfakeËßÜÈ¢ëÁöÑÂèØ‰ø°Ê∫ØÊ∫ê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.

