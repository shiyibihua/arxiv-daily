---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-11-13
---

# cs.CVï¼ˆ2025-11-13ï¼‰

ğŸ“Š å…± **28** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (18 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (18 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251110316v1-depth-consistent-3d-gaussian-splatting-via-physical-defocus-modeling.html">Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision</a></td>
  <td>æå‡ºåŸºäºç‰©ç†æ•£ç„¦å»ºæ¨¡å’Œå¤šè§†è§’å‡ ä½•ç›‘ç£çš„æ·±åº¦ä¸€è‡´æ€§3Dé«˜æ–¯æº…å°„æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10316v1" onclick="toggleFavorite(this, '2511.10316v1', 'Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251109827v1-aha-animating-human-avatars-in-diverse-scenes-with-gaussian-splattin.html">AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting</a></td>
  <td>æå‡ºåŸºäºé«˜æ–¯æº…å°„çš„äººä½“åŠ¨ç”»æ¡†æ¶ï¼Œå®ç°åœºæ™¯ä¸­é€¼çœŸçš„äººä½“è‡ªç”±è§†è§’æ¸²æŸ“ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09827v1" onclick="toggleFavorite(this, '2511.09827v1', 'AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251109944v1-tspe-gs-probabilistic-depth-extraction-for-semi-transparent-surface-.html">TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting</a></td>
  <td>TSPE-GSï¼šåŸºäº3Dé«˜æ–¯æº…å°„çš„åŠé€æ˜è¡¨é¢æ¦‚ç‡æ·±åº¦æå–æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09944v1" onclick="toggleFavorite(this, '2511.09944v1', 'TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251110560v2-omnivggt-omni-modality-driven-visual-geometry-grounded-transformer.html">OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer</a></td>
  <td>OmniVGGTï¼šå¤šæ¨¡æ€é©±åŠ¨çš„è§†è§‰å‡ ä½•å¯¹é½Transformerï¼Œæå‡3Dè§†è§‰ä»»åŠ¡æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10560v2" onclick="toggleFavorite(this, '2511.10560v2', 'OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251110799v2-gft-graph-feature-tuning-for-efficient-point-cloud-analysis.html">GFT: Graph Feature Tuning for Efficient Point Cloud Analysis</a></td>
  <td>æå‡ºå›¾ç‰¹å¾è°ƒä¼˜(GFT)æ–¹æ³•ï¼Œé«˜æ•ˆåˆ†æç‚¹äº‘æ•°æ®å¹¶æ˜¾è‘—é™ä½å‚æ•°é‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10799v2" onclick="toggleFavorite(this, '2511.10799v2', 'GFT: Graph Feature Tuning for Efficient Point Cloud Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251110376v2-msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-s.html">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€3Dåœºæ™¯å›¾MSGNavï¼Œç”¨äºé›¶æ ·æœ¬å…·èº«å¯¼èˆª</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10376v2" onclick="toggleFavorite(this, '2511.10376v2', 'MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251109878v1-rwkv-pcssc-exploring-rwkv-model-for-point-cloud-semantic-scene-compl.html">RWKV-PCSSC: Exploring RWKV Model for Point Cloud Semantic Scene Completion</a></td>
  <td>æå‡ºRWKV-PCSSCï¼Œåˆ©ç”¨RWKVæœºåˆ¶å®ç°è½»é‡é«˜æ•ˆçš„ç‚¹äº‘è¯­ä¹‰åœºæ™¯è¡¥å…¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09878v1" onclick="toggleFavorite(this, '2511.09878v1', 'RWKV-PCSSC: Exploring RWKV Model for Point Cloud Semantic Scene Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251109866v1-ipcd-intrinsic-point-cloud-decomposition.html">IPCD: Intrinsic Point-Cloud Decomposition</a></td>
  <td>æå‡ºIPCDï¼Œç”¨äºç‚¹äº‘çš„æœ¬å¾åˆ†è§£ï¼Œå®ç°å…‰ç…§ç¼–è¾‘å’Œçº¹ç†ä¿®æ”¹ç­‰åº”ç”¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09866v1" onclick="toggleFavorite(this, '2511.09866v1', 'IPCD: Intrinsic Point-Cloud Decomposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251110615v1-towards-blind-and-low-vision-accessibility-of-lightweight-vlms-and-c.html">Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals</a></td>
  <td>é’ˆå¯¹è§†éšœäººå£«ï¼Œè¯„ä¼°è½»é‡çº§VLMåœ¨è§†é¢‘ç†è§£ä¸­çš„å¯è®¿é—®æ€§ï¼Œå¹¶æå‡ºå®šåˆ¶åŒ–è¯„ä¼°æ¡†æ¶ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10615v1" onclick="toggleFavorite(this, '2511.10615v1', 'Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251110107v1-robia-robust-instance-aware-continual-test-time-adaptation-for-deep-.html">RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo</a></td>
  <td>æå‡ºRobIAæ¡†æ¶ï¼Œç”¨äºæ·±åº¦ç«‹ä½“åŒ¹é…ä¸­é²æ£’çš„ã€å®ä¾‹æ„ŸçŸ¥çš„æŒç»­æµ‹è¯•æ—¶è‡ªé€‚åº”</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10107v1" onclick="toggleFavorite(this, '2511.10107v1', 'RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251110209v2-linext-revisiting-lidar-completion-with-efficient-non-diffusion-arch.html">LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures</a></td>
  <td>LiNeXtï¼šæå‡ºé«˜æ•ˆéæ‰©æ•£æ¶æ„ï¼ŒåŠ é€ŸLiDARç‚¹äº‘è¡¥å…¨å¹¶æå‡ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10209v2" onclick="toggleFavorite(this, '2511.10209v2', 'LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251110142v1-split-layer-enhancing-implicit-neural-representation-by-maximizing-t.html">Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space</a></td>
  <td>æå‡ºSplit-Layerä»¥æå‡éšå¼ç¥ç»è¡¨ç¤ºçš„ç‰¹å¾ç©ºé—´ç»´åº¦ï¼Œå¢å¼ºè¡¨å¾èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10142v1" onclick="toggleFavorite(this, '2511.10142v1', 'Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251111735v1-toward-bilipshiz-geometric-models.html">Toward bilipshiz geometric models</a></td>
  <td>æå‡ºä¿æŒåŒåˆ©æ™®å¸ŒèŒ¨å‡ ä½•ç»“æ„çš„3Dç‚¹äº‘ç¥ç»ç½‘ç»œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.11735v1" onclick="toggleFavorite(this, '2511.11735v1', 'Toward bilipshiz geometric models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251110040v2-log3d-ultra-high-resolution-3d-shape-modeling-via-local-to-global-pa.html">LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning</a></td>
  <td>LoG3Dï¼šé€šè¿‡å±€éƒ¨åˆ°å…¨å±€åˆ†å‰²å®ç°è¶…é«˜åˆ†è¾¨ç‡3Då½¢çŠ¶å»ºæ¨¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10040v2" onclick="toggleFavorite(this, '2511.10040v2', 'LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251110017v1-affordbot-3d-fine-grained-embodied-reasoning-via-multimodal-large-la.html">AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</a></td>
  <td>AffordBotï¼šåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°ç»†ç²’åº¦3Då…·èº«æ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10017v1" onclick="toggleFavorite(this, '2511.10017v1', 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251110003v2-dbgroup-dual-branch-point-grouping-for-weakly-supervised-3d-semantic.html">DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Semantic Instance Segmentation</a></td>
  <td>æå‡ºDBGroupï¼šåŒåˆ†æ”¯ç‚¹äº‘åˆ†ç»„ç½‘ç»œï¼Œç”¨äºå¼±ç›‘ç£3Dè¯­ä¹‰å®ä¾‹åˆ†å‰²</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10003v2" onclick="toggleFavorite(this, '2511.10003v2', 'DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Semantic Instance Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251109919v1-mosaicdoc-a-large-scale-bilingual-benchmark-for-visually-rich-docume.html">MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding</a></td>
  <td>æå‡ºMosaicDocï¼šä¸€ä¸ªå¤§è§„æ¨¡åŒè¯­è§†è§‰æ–‡æ¡£ç†è§£åŸºå‡†ï¼Œè§£å†³ç°æœ‰åŸºå‡†çš„å±€é™æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09919v1" onclick="toggleFavorite(this, '2511.09919v1', 'MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251109883v1-hcc-3d-hierarchical-compensatory-compression-for-98-3d-token-reducti.html">HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models</a></td>
  <td>æå‡ºHCC-3Dï¼Œé€šè¿‡åˆ†å±‚è¡¥å¿å‹ç¼©å®ç°3Dè§†è§‰è¯­è¨€æ¨¡å‹ä¸­98%çš„Tokenç¼©å‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.09883v1" onclick="toggleFavorite(this, '2511.09883v1', 'HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251110647v1-depth-anything-3-recovering-the-visual-space-from-any-views.html">Depth Anything 3: Recovering the Visual Space from Any Views</a></td>
  <td>Depth Anything 3ï¼šä»ä»»æ„è§†è§’æ¢å¤ç©ºé—´å‡ ä½•ä¿¡æ¯ï¼Œæ— éœ€æ¶æ„ç‰¹åŒ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10647v1" onclick="toggleFavorite(this, '2511.10647v1', 'Depth Anything 3: Recovering the Visual Space from Any Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251110604v1-multitask-glocal-obia-mamba-for-sentinel-2-landcover-mapping.html">Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping</a></td>
  <td>æå‡ºå¤šä»»åŠ¡GLocal OBIA-Mambaæ¨¡å‹ï¼Œæå‡Sentinel-2åœŸåœ°è¦†ç›–åˆ†ç±»ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10604v1" onclick="toggleFavorite(this, '2511.10604v1', 'Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251110279v1-propa-toward-process-level-optimization-in-visual-reasoning-via-rein.html">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a></td>
  <td>æå‡ºPROPAæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è§†è§‰æ¨ç†ä¸­çš„è¿‡ç¨‹çº§ä¾èµ–é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10279v1" onclick="toggleFavorite(this, '2511.10279v1', 'PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251111725v1-do-blind-spots-matter-for-word-referent-mapping-a-computational-stud.html">Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video</a></td>
  <td>æå‡ºåŸºäºç›²ç‚¹æ„ŸçŸ¥çš„è‡ªç›‘ç£è§†è§‰è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæå‡å©´å„¿è§†è§’è§†é¢‘ä¸­çš„è¯-ç‰©æ˜ å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.11725v1" onclick="toggleFavorite(this, '2511.11725v1', 'Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251110212v1-next-frame-feature-prediction-for-multimodal-deepfake-detection-and-.html">Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization</a></td>
  <td>æå‡ºåŸºäºä¸‹ä¸€å¸§ç‰¹å¾é¢„æµ‹çš„å¤šæ¨¡æ€Deepfakeæ£€æµ‹ä¸æ—¶åºå®šä½æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10212v1" onclick="toggleFavorite(this, '2511.10212v1', 'Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251110518v1-semanticvla-semantic-aligned-sparsification-and-enhancement-for-effi.html">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a></td>
  <td>SemanticVLAï¼šé¢å‘é«˜æ•ˆæœºå™¨äººæ“ä½œçš„è¯­ä¹‰å¯¹é½ç¨€ç–åŒ–ä¸å¢å¼º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10518v1" onclick="toggleFavorite(this, '2511.10518v1', 'SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251110627v1-querying-labeled-time-series-data-with-scenario-programs.html">Querying Labeled Time Series Data with Scenario Programs</a></td>
  <td>æå‡ºåŸºäºåœºæ™¯ç¨‹åºçš„æ—¶åºæ•°æ®æŸ¥è¯¢æ–¹æ³•ï¼Œç”¨äºéªŒè¯ä»¿çœŸç¯å¢ƒä¸­è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¤±æ•ˆåœºæ™¯ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10627v1" onclick="toggleFavorite(this, '2511.10627v1', 'Querying Labeled Time Series Data with Scenario Programs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251110539v1-dynamic-avatar-scene-rendering-from-human-centric-context.html">Dynamic Avatar-Scene Rendering from Human-centric Context</a></td>
  <td>æå‡ºSeparate-then-Mapç­–ç•¥ï¼Œè§£å†³å•ç›®è§†é¢‘ä¸­åŠ¨æ€äººä¸åœºæ™¯äº¤äº’çš„ç¥ç»æ¸²æŸ“é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10539v1" onclick="toggleFavorite(this, '2511.10539v1', 'Dynamic Avatar-Scene Rendering from Human-centric Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251110203v1-vista-a-vision-and-intent-aware-social-attention-framework-for-multi.html">VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction</a></td>
  <td>VISTAï¼šä¸€ç§ç”¨äºå¤šæ™ºèƒ½ä½“è½¨è¿¹é¢„æµ‹çš„è§†è§‰å’Œæ„å›¾æ„ŸçŸ¥ç¤¾äº¤æ³¨æ„åŠ›æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10203v1" onclick="toggleFavorite(this, '2511.10203v1', 'VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251110076v1-mitigating-error-accumulation-in-co-speech-motion-generation-via-glo.html">Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints</a></td>
  <td>æå‡ºGlobalDiffï¼Œé€šè¿‡å…¨å±€æ—‹è½¬æ‰©æ•£å’Œå¤šçº§çº¦æŸç¼“è§£å…±è¯­è¿åŠ¨ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.10076v1" onclick="toggleFavorite(this, '2511.10076v1', 'Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)