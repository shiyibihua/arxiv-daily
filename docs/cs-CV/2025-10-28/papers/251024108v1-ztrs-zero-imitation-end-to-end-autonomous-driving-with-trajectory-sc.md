---
layout: default
title: ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring
---

# ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring

**arXiv**: [2510.24108v1](https://arxiv.org/abs/2510.24108) | [PDF](https://arxiv.org/pdf/2510.24108.pdf)

**ä½œè€…**: Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Jingde Chen, Nadine Chang, Maying Shen, Jingyu Song, Zuxuan Wu, Shiyi Lan, Jose M. Alvarez

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZTRSæ¡†æž¶ï¼Œç»“åˆä¼ æ„Ÿå™¨è¾“å…¥ä¸Žå¼ºåŒ–å­¦ä¹ ï¼Œå®žçŽ°ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ— éœ€æ¨¡ä»¿å­¦ä¹ ã€‚**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `è½¨è¿¹è¯„åˆ†` `ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†` `ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œæ˜“å—ä¸“å®¶æ¼”ç¤ºå’Œåå˜é‡åç§»é™åˆ¶ã€‚
2. ZTRSä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’ŒEPOä¼˜åŒ–ï¼Œç›´æŽ¥ä»Žé«˜ç»´ä¼ æ„Ÿå™¨æ•°æ®å­¦ä¹ è½¨è¿¹ã€‚
3. åœ¨Navhardå’ŒHUGSIMåŸºå‡†ä¸Šå®žçŽ°å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶Šæ¨¡ä»¿å­¦ä¹ åŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> End-to-end autonomous driving maps raw sensor inputs directly into
> ego-vehicle trajectories to avoid cascading errors from perception modules and
> to leverage rich semantic cues. Existing frameworks largely rely on Imitation
> Learning (IL), which can be limited by sub-optimal expert demonstrations and
> covariate shift during deployment. On the other hand, Reinforcement Learning
> (RL) has recently shown potential in scaling up with simulations, but is
> typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
> maps), falling short of full end-to-end learning from raw sensor data. We
> introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
> Scoring), a framework that combines the strengths of both worlds: sensor inputs
> without losing information and RL training for robust planning. To the best of
> our knowledge, ZTRS is the first framework that eliminates IL entirely by only
> learning from rewards while operating directly on high-dimensional sensor data.
> ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
> Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
> actions and rewards. ZTRS demonstrates strong performance across three
> benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
> planning in challenging real-world and synthetic scenarios), and HUGSIM
> (simulated closed-loop driving). Specifically, ZTRS achieves the
> state-of-the-art result on Navhard and outperforms IL-based baselines on
> HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.

