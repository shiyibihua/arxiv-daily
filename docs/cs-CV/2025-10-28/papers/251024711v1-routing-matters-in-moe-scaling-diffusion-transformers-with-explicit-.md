---
layout: default
title: Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance
---

# Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance

**arXiv**: [2510.24711v1](https://arxiv.org/abs/2510.24711) | [PDF](https://arxiv.org/pdf/2510.24711.pdf)

**ä½œè€…**: Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProMoEæ¡†æ¶ä»¥è§£å†³è§†è§‰MoEä¸­ä¸“å®¶ä¸“ä¸šåŒ–ä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `æ‰©æ•£å˜æ¢å™¨` `è·¯ç”±æŒ‡å¯¼` `è§†è§‰ä»¤ç‰Œå¤„ç†` `åŸå‹è·¯ç”±` `å¯¹æ¯”æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰ä»¤ç‰Œå­˜åœ¨ç©ºé—´å†—ä½™å’ŒåŠŸèƒ½å¼‚è´¨æ€§ï¼Œé˜»ç¢MoEåœ¨æ‰©æ•£å˜æ¢å™¨ä¸­çš„ä¸“å®¶ä¸“ä¸šåŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤æ­¥è·¯ç”±å™¨ï¼Œé€šè¿‡æ¡ä»¶è·¯ç”±å’ŒåŸå‹è·¯ç”±æä¾›æ˜¾å¼è·¯ç”±æŒ‡å¯¼
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨ImageNetåŸºå‡†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ”¯æŒRectified Flowå’ŒDDPMè®­ç»ƒç›®æ ‡

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
> capacity while preserving computational efficiency. Despite its notable success
> in large language models (LLMs), existing attempts to apply MoE to Diffusion
> Transformers (DiTs) have yielded limited gains. We attribute this gap to
> fundamental differences between language and visual tokens. Language tokens are
> semantically dense with pronounced inter-token variation, while visual tokens
> exhibit spatial redundancy and functional heterogeneity, hindering expert
> specialization in vision MoE. To this end, we present ProMoE, an MoE framework
> featuring a two-step router with explicit routing guidance that promotes expert
> specialization. Specifically, this guidance encourages the router to partition
> image tokens into conditional and unconditional sets via conditional routing
> according to their functional roles, and refine the assignments of conditional
> image tokens through prototypical routing with learnable prototypes based on
> semantic content. Moreover, the similarity-based expert allocation in latent
> space enabled by prototypical routing offers a natural mechanism for
> incorporating explicit semantic guidance, and we validate that such guidance is
> crucial for vision MoE. Building on this, we propose a routing contrastive loss
> that explicitly enhances the prototypical routing process, promoting
> intra-expert coherence and inter-expert diversity. Extensive experiments on
> ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
> under both Rectified Flow and DDPM training objectives. Code and models will be
> made publicly available.

