---
layout: default
title: PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI
---

# PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI

**arXiv**: [2510.24109v1](https://arxiv.org/abs/2510.24109) | [PDF](https://arxiv.org/pdf/2510.24109.pdf)

**ä½œè€…**: Wenbin Ding, Jun Chen, Mingjia Chen, Fei Xie, Qi Mao, Philip Dames

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPFEAæ¡†æ¶ä»¥æå‡æœºå™¨äººæ‰§è¡Œé«˜çº§è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä»»åŠ¡çš„æˆåŠŸç‡**

**å…³é”®è¯**: `å…·èº«ä»£ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `ä»»åŠ¡è§„åˆ’` `è‡ªç„¶è¯­è¨€äº¤äº’` `æœºå™¨äººæ“ä½œ` `åé¦ˆè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„å…·èº«ä»£ç†ç¼ºä¹åœ¨çº¿è§„åˆ’å’Œæ‰§è¡Œå¤æ‚è‡ªç„¶è¯­è¨€æ§åˆ¶ä»»åŠ¡çš„èƒ½åŠ›
2. æ¡†æ¶åŒ…å«äººæœºè¯­éŸ³äº¤äº’ã€è§†è§‰è¯­è¨€ä»£ç†å’ŒåŠ¨ä½œæ‰§è¡Œæ¨¡å—ï¼Œé›†æˆä»»åŠ¡è§„åˆ’ä¸åé¦ˆè¯„ä¼°
3. å®éªŒæ˜¾ç¤ºåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­ä»»åŠ¡æˆåŠŸç‡æ¯”LLM+CLIPæ–¹æ³•æé«˜28%

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of Large Language Models (LLMs) has marked a
> significant breakthrough in Artificial Intelligence (AI), ushering in a new era
> of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human
> welfare and needs, thereby placing higher demands on the intelligence level of
> robots, particularly in aspects such as natural language interaction, complex
> task planning, and execution. Intelligent agents powered by LLMs have opened up
> new pathways for realizing HAI. However, existing LLM-based embodied agents
> often lack the ability to plan and execute complex natural language control
> tasks online. This paper explores the implementation of intelligent robotic
> manipulating agents based on Vision-Language Models (VLMs) in the physical
> world. We propose a novel embodied agent framework for robots, which comprises
> a human-robot voice interaction module, a vision-language agent module and an
> action execution module. The vision-language agent itself includes a
> vision-based task planner, a natural language instruction converter, and a task
> performance feedback evaluator. Experimental results demonstrate that our agent
> achieves a 28\% higher average task success rate in both simulated and real
> environments compared to approaches relying solely on LLM+CLIP, significantly
> improving the execution success rate of high-level natural language instruction
> tasks.

