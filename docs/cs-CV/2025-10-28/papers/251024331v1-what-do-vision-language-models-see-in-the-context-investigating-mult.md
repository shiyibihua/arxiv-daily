---
layout: default
title: What do vision-language models see in the context? Investigating multimodal in-context learning
---

# What do vision-language models see in the context? Investigating multimodal in-context learning

**arXiv**: [2510.24331v1](https://arxiv.org/abs/2510.24331) | [PDF](https://arxiv.org/pdf/2510.24331.pdf)

**ä½œè€…**: Gabriel O. dos Santos, Esther Colombini, Sandra Avila

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿç ”ç©¶è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ­ç¤ºå…¶å±€é™ä¸Žå½±å“å› ç´ **

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å¤šæ¨¡æ€é›†æˆ` `æ³¨æ„åŠ›æœºåˆ¶` `æŒ‡ä»¤è°ƒä¼˜` `å›¾åƒæè¿°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§æœªå……åˆ†æŽ¢ç´¢ï¼Œå­˜åœ¨è§†è§‰ä¸Žæ–‡æœ¬ä¿¡æ¯æ•´åˆä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šè¯„ä¼°ä¸ƒç§æ¨¡åž‹åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šï¼Œåˆ†æžæç¤ºè®¾è®¡ã€æž¶æž„é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šè®­ç»ƒäºŽå›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®æå‡æ€§èƒ½ï¼Œä½†æ³¨æ„åŠ›åˆ†æžæ˜¾ç¤ºæ¨¡åž‹ä¸»è¦ä¾èµ–æ–‡æœ¬çº¿ç´¢

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
> from demonstration examples without parameter updates. Although it has been
> extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
> remains underexplored. In this work, we present a systematic study of ICL in
> VLMs, evaluating seven models spanning four architectures on three image
> captioning benchmarks. We analyze how prompt design, architectural choices, and
> training strategies influence multimodal ICL. To our knowledge, we are the
> first to analyze how attention patterns in VLMs vary with an increasing number
> of in-context demonstrations. Our results reveal that training on imag-text
> interleaved data enhances ICL performance but does not imply effective
> integration of visual and textual information from demonstration examples. In
> contrast, instruction tuning improves instruction-following but can reduce
> reliance on in-context demonstrations, suggesting a trade-off between
> instruction alignment and in-context adaptation. Attention analyses further
> show that current VLMs primarily focus on textual cues and fail to leverage
> visual information, suggesting a limited capacity for multimodal integration.
> These findings highlight key limitations in the ICL abilities of current VLMs
> and provide insights for enhancing their ability to learn from multimodal
> in-context examples.

