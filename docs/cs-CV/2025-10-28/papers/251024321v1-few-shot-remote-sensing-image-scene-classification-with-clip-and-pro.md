---
layout: default
title: Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning
---

# Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning

**arXiv**: [2510.24321v1](https://arxiv.org/abs/2510.24321) | [PDF](https://arxiv.org/pdf/2510.24321.pdf)

**ä½œè€…**: Ivica Dimitrovski, Vlatko Spasev, Ivan Kitanovski

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæç¤ºå­¦ä¹ ä»¥è§£å†³é¥æ„Ÿå›¾åƒå°‘æ ·æœ¬åœºæ™¯åˆ†ç±»ä¸­çš„é¢†åŸŸå·®è·é—®é¢˜**

**å…³é”®è¯**: `é¥æ„Ÿå›¾åƒåˆ†ç±»` `å°‘æ ·æœ¬å­¦ä¹ ` `æç¤ºå­¦ä¹ ` `CLIPæ¨¡åž‹` `é¢†åŸŸé€‚åº”` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»å—é™äºŽæ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸå·®è·ï¼Œå¯¼è‡´CLIPæ¨¡åž‹ç›´æŽ¥åº”ç”¨æ•ˆæžœä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç³»ç»Ÿè¯„ä¼°å¤šç§æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–å’Œå¤šæ¨¡æ€æç¤ºï¼Œä»¥è½»é‡çº§æ–¹å¼é€‚åº”è¯­ä¹‰ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œæç¤ºå­¦ä¹ ä¼˜äºŽé›¶æ ·æœ¬CLIPå’Œçº¿æ€§æŽ¢é’ˆï¼Œå°¤å…¶åœ¨è·¨åŸŸåœºæ™¯ä¸­è¡¨çŽ°ç¨³å¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Remote sensing applications increasingly rely on deep learning for scene
> classification. However, their performance is often constrained by the scarcity
> of labeled data and the high cost of annotation across diverse geographic and
> sensor domains. While recent vision-language models like CLIP have shown
> promise by learning transferable representations at scale by aligning visual
> and textual modalities, their direct application to remote sensing remains
> suboptimal due to significant domain gaps and the need for task-specific
> semantic adaptation. To address this critical challenge, we systematically
> explore prompt learning as a lightweight and efficient adaptation strategy for
> few-shot remote sensing image scene classification. We evaluate several
> representative methods, including Context Optimization, Conditional Context
> Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
> Constraints. These approaches reflect complementary design philosophies: from
> static context optimization to conditional prompts for enhanced generalization,
> multi-modal prompts for joint vision-language adaptation, and semantically
> regularized prompts for stable learning without forgetting. We benchmark these
> prompt-learning methods against two standard baselines: zero-shot CLIP with
> hand-crafted prompts and a linear probe trained on frozen CLIP features.
> Through extensive experiments on multiple benchmark remote sensing datasets,
> including cross-dataset generalization tests, we demonstrate that prompt
> learning consistently outperforms both baselines in few-shot scenarios.
> Notably, Prompting with Self-Regulating Constraints achieves the most robust
> cross-domain performance. Our findings underscore prompt learning as a scalable
> and efficient solution for bridging the domain gap in satellite and aerial
> imagery, providing a strong foundation for future research in this field.

