---
layout: default
title: Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?
---

# Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?

**arXiv**: [2510.24259v1](https://arxiv.org/abs/2510.24259) | [PDF](https://arxiv.org/pdf/2510.24259.pdf)

**ä½œè€…**: Ziqi Ma, Sao Mai Nguyen, Philippe Xu

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°LLMså°†äººç±»æŒ‡ä»¤ç¿»è¯‘ä¸ºå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å†…éƒ¨æ¶Œç°ç¬¦å·è¡¨ç¤ºçš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç¬¦å·è¡¨ç¤º` `è¡¨ç¤ºå¯¹é½` `ç¿»è¯‘æ€§èƒ½` `ä»»åŠ¡å¤æ‚åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMsèƒ½å¦å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç¿»è¯‘ä¸ºå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å†…éƒ¨æ¶Œç°çš„ç¬¦å·è¡¨ç¤ºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç»“æ„åŒ–è¯„ä¼°æ¡†æ¶ï¼Œæµ‹è¯•GPTã€Claudeç­‰LLMsåœ¨ä¸åŒç¬¦å·åˆ†åŒºä¸Šçš„ç¿»è¯‘æ€§èƒ½ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šLLMsè¡¨ç°å—åˆ†åŒºç²’åº¦å’Œä»»åŠ¡å¤æ‚åº¦å½±å“ï¼Œæ­ç¤ºè¡¨ç¤ºå¯¹é½çš„å±€é™æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emergent symbolic representations are critical for enabling developmental
> learning agents to plan and generalize across tasks. In this work, we
> investigate whether large language models (LLMs) can translate human natural
> language instructions into the internal symbolic representations that emerge
> during hierarchical reinforcement learning. We apply a structured evaluation
> framework to measure the translation performance of commonly seen LLMs -- GPT,
> Claude, Deepseek and Grok -- across different internal symbolic partitions
> generated by a hierarchical reinforcement learning algorithm in the Ant Maze
> and Ant Fall environments. Our findings reveal that although LLMs demonstrate
> some ability to translate natural language into a symbolic representation of
> the environment dynamics, their performance is highly sensitive to partition
> granularity and task complexity. The results expose limitations in current LLMs
> capacity for representation alignment, highlighting the need for further
> research on robust alignment between language and internal agent
> representations.

