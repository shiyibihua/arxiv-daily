---
layout: default
title: BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning
---

# BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning

**arXiv**: [2510.24161v1](https://arxiv.org/abs/2510.24161) | [PDF](https://arxiv.org/pdf/2510.24161.pdf)

**ä½œè€…**: Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBLM$_1$æ¨¡å‹ä»¥å®ç°è·¨ç©ºé—´ã€è·¨ä»»åŠ¡å’Œè·¨å…·èº«çš„ç»Ÿä¸€å­¦ä¹ **

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è·¨ç©ºé—´å­¦ä¹ ` `è·¨ä»»åŠ¡å­¦ä¹ ` `è·¨å…·èº«æ³›åŒ–` `ä¸¤é˜¶æ®µè®­ç»ƒ` `æ„å›¾æ¡¥æ¥æ¥å£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰MLLMsåœ¨æ•°å­—-ç‰©ç†ç©ºé—´å’Œå…·èº«é—´æ³›åŒ–èƒ½åŠ›å·®ï¼Œç¼ºä¹ç»Ÿä¸€æ¨¡å‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œæ³¨å…¥å…·èº«çŸ¥è¯†å¹¶æ„å»ºæ„å›¾æ¡¥æ¥æ¥å£æŒ‡å¯¼æ§åˆ¶ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨æ•°å­—å’Œç‰©ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBLM$_1$ä¼˜äºå¤šç±»æ¨¡å‹ï¼Œæ€§èƒ½æå‡çº¦6%å’Œ3%ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have advanced vision-language
> reasoning and are increasingly deployed in embodied agents. However,
> significant limitations remain: MLLMs generalize poorly across digital-physical
> spaces and embodiments; vision-language-action models (VLAs) produce low-level
> actions yet lack robust high-level embodied reasoning; and most embodied large
> language models (ELLMs) are constrained to digital-space with poor
> generalization to the physical world. Thus, unified models that operate
> seamlessly across digital and physical spaces while generalizing across
> embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
> Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
> instruction following and reasoning, incorporates embodied knowledge, and
> supports robust cross-embodiment control. BLM$_1$ integrates three key
> capabilities -- \textit{cross-space transfer, cross-task learning, and
> cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
> injects embodied knowledge into the MLLM through curated digital corpora while
> maintaining language competence. Stage II trains a policy module through an
> intent-bridging interface that extracts high-level semantics from the MLLM to
> guide control, without fine-tuning the MLLM backbone. This process is supported
> by a self-collected cross-embodiment demonstration suite spanning four robot
> embodiments and six progressively challenging tasks. Evaluations across digital
> and physical benchmarks show that a single BLM$_1$ instance outperforms four
> model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
> $\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
> tasks.

