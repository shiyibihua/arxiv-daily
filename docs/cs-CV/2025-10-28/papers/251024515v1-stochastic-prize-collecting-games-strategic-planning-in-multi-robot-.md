---
layout: default
title: Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems
---

# Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems

**arXiv**: [2510.24515v1](https://arxiv.org/abs/2510.24515) | [PDF](https://arxiv.org/pdf/2510.24515.pdf)

**ä½œè€…**: Malintha Fernando, Petter Ã–gren, Silun Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéšæœºå¥–å“æ”¶é›†æ¸¸æˆä»¥è§£å†³å¤šæœºå™¨äººç³»ç»Ÿä¸­è‡ªåˆ©æœºå™¨äººåœ¨éšæœºçŽ¯å¢ƒä¸‹çš„è§„åˆ’é—®é¢˜**

**å…³é”®è¯**: `å¤šæœºå™¨äººç³»ç»Ÿ` `éšæœºè§„åˆ’` `çº³ä»€å‡è¡¡` `å›¢é˜Ÿå®šå‘é—®é¢˜` `å¼ºåŒ–å­¦ä¹ ` `ç«žäº‰çŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæœºå™¨äººç³»ç»Ÿåœ¨å¥–åŠ±ç¨€ç¼ºçŽ¯å¢ƒä¸­ç«žäº‰ï¼Œä¼ ç»Ÿå›¢é˜Ÿå®šå‘é—®é¢˜å‡è®¾åˆä½œï¼Œä¸é€‚ç”¨äºŽè‡ªåˆ©æœºå™¨äººã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºSPCGæ‰©å±•TOPï¼Œå¼•å…¥åºæ•°ç§©æœç´¢å’Œè™šæž„åºæ•°å“åº”å­¦ä¹ ç®—æ³•ï¼Œè®¡ç®—å‡è¡¡ç­–ç•¥ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨é“è·¯ç½‘ç»œå’Œåˆæˆå›¾ä¸Šè¯„ä¼°ï¼Œå­¦ä¹ ç­–ç•¥åœ¨å›¢é˜Ÿè§„æ¨¡æ‰©å±•å’Œå¥–åŠ±åˆ†å¸ƒæ³›åŒ–æ–¹é¢ä¼˜äºŽåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Team Orienteering Problem (TOP) generalizes many real-world multi-robot
> scheduling and routing tasks that occur in autonomous mobility, aerial
> logistics, and surveillance applications. While many flavors of the TOP exist
> for planning in multi-robot systems, they assume that all the robots cooperate
> toward a single objective; thus, they do not extend to settings where the
> robots compete in reward-scarce environments. We propose Stochastic
> Prize-Collecting Games (SPCG) as an extension of the TOP to plan in the
> presence of self-interested robots operating on a graph, under energy
> constraints and stochastic transitions. A theoretical study on complete and
> star graphs establishes that there is a unique pure Nash equilibrium in SPCGs
> that coincides with the optimal routing solution of an equivalent TOP given a
> rank-based conflict resolution rule. This work proposes two algorithms: Ordinal
> Rank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in
> temporarily-formed local neighborhoods during the games' stages, and Fictitious
> Ordinal Response Learning (FORL) to obtain best-response policies against one's
> senior-rank opponents. Empirical evaluations conducted on road networks and
> synthetic graphs under both dynamic and stationary prize distributions show
> that 1) the state-aliasing induced by OR-conditioning enables learning policies
> that scale more efficiently to large team sizes than those trained with the
> global index, and 2) Policies trained with FORL generalize better to imbalanced
> prize distributions than those with other multi-agent training methods.
> Finally, the learned policies in the SPCG achieved between 87% and 95%
> optimality compared to an equivalent TOP solution obtained by mixed-integer
> linear programming.

