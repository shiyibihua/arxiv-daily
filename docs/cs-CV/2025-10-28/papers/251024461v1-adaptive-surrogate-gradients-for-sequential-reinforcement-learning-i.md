---
layout: default
title: Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks
---

# Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks

**arXiv**: [2510.24461v1](https://arxiv.org/abs/2510.24461) | [PDF](https://arxiv.org/pdf/2510.24461.pdf)

**ä½œè€…**: Korneel Van den Berghe, Stein Stroobants, Vijay Janapa Reddi, G. C. H. E. de Croon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”ä»£ç†æ¢¯åº¦å’Œå¼•å¯¼ç­–ç•¥ä»¥ä¼˜åŒ–è„‰å†²ç¥žç»ç½‘ç»œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒæ€§èƒ½**

**å…³é”®è¯**: `è„‰å†²ç¥žç»ç½‘ç»œ` `ä»£ç†æ¢¯åº¦` `å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”è®­ç»ƒ` `æœºå™¨äººæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè„‰å†²ç¥žç»å…ƒçš„ä¸å¯å¾®æ€§å’ŒçŠ¶æ€åŠ¨æ€åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¯¼è‡´æ¢¯åº¦ä¼˜åŒ–å›°éš¾å’Œåºåˆ—è®­ç»ƒå—é™
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†æžä»£ç†æ¢¯åº¦æ–œçŽ‡å½±å“ï¼Œç»“åˆè‡ªé€‚åº”æ–œçŽ‡è°ƒåº¦å’Œç‰¹æƒå¼•å¯¼ç­–ç•¥æå‡è®­ç»ƒæ•ˆçŽ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ— äººæœºä½ç½®æŽ§åˆ¶ä»»åŠ¡ä¸­ï¼Œå¹³å‡å›žæŠ¥è¾¾400ç‚¹ï¼Œæ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Neuromorphic computing systems are set to revolutionize energy-constrained
> robotics by achieving orders-of-magnitude efficiency gains, while enabling
> native temporal processing. Spiking Neural Networks (SNNs) represent a
> promising algorithmic approach for these systems, yet their application to
> complex control tasks faces two critical challenges: (1) the non-differentiable
> nature of spiking neurons necessitates surrogate gradients with unclear
> optimization properties, and (2) the stateful dynamics of SNNs require training
> on sequences, which in reinforcement learning (RL) is hindered by limited
> sequence lengths during early training, preventing the network from bridging
> its warm-up period.
>   We address these challenges by systematically analyzing surrogate gradient
> slope settings, showing that shallower slopes increase gradient magnitude in
> deeper layers but reduce alignment with true gradients. In supervised learning,
> we find no clear preference for fixed or scheduled slopes. The effect is much
> more pronounced in RL settings, where shallower slopes or scheduled slopes lead
> to a 2.1x improvement in both training and final deployed performance. Next, we
> propose a novel training approach that leverages a privileged guiding policy to
> bootstrap the learning process, while still exploiting online environment
> interactions with the spiking policy. Combining our method with an adaptive
> slope schedule for a real-world drone position control task, we achieve an
> average return of 400 points, substantially outperforming prior techniques,
> including Behavioral Cloning and TD3BC, which achieve at most --200 points
> under the same conditions. This work advances both the theoretical
> understanding of surrogate gradient learning in SNNs and practical training
> methodologies for neuromorphic controllers demonstrated in real-world robotic
> systems.

