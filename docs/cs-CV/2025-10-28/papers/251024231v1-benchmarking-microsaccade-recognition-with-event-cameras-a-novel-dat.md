---
layout: default
title: Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation
---

# Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation

**arXiv**: [2510.24231v1](https://arxiv.org/abs/2510.24231) | [PDF](https://arxiv.org/pdf/2510.24231.pdf)

**ä½œè€…**: Waseem Shariff, Timothy Hanley, Maciej Stec, Hossein Javidnia, Peter Corcoran

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº‹ä»¶ç›¸æœºå¾®æ‰«è§†æ•°æ®é›†ä¸Žè¯„ä¼°æ–¹æ³•ï¼Œä»¥æ”¯æŒç²¾ç»†çœ¼åŠ¨ç ”ç©¶ã€‚**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `å¾®æ‰«è§†è¯†åˆ«` `è„‰å†²ç¥žç»ç½‘ç»œ` `æ•°æ®é›†æž„å»º` `å…‰å­¦æµå¢žå¼º` `çœ¼åŠ¨åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿçœ¼åŠ¨è¿½è¸ªæ–¹æ³•æˆæœ¬é«˜ã€æ‰©å±•æ€§å·®ï¼Œéš¾ä»¥æ•æ‰å¾®æ‰«è§†çš„ç²¾ç»†åŠ¨æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Blenderæ¨¡æ‹Ÿå¾®æ‰«è§†ï¼Œå¹¶é€šè¿‡v2eè½¬æ¢ä¸ºäº‹ä»¶æµï¼Œæž„å»ºä¸ƒç±»è§’ä½ç§»æ•°æ®é›†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé‡‡ç”¨Spiking-VGGæ¨¡åž‹ï¼Œå¹³å‡å‡†ç¡®çŽ‡è¾¾çº¦90%ï¼ŒæˆåŠŸåˆ†ç±»å¾®æ‰«è§†è§’ä½ç§»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Microsaccades are small, involuntary eye movements vital for visual
> perception and neural processing. Traditional microsaccade studies typically
> use eye trackers or frame-based analysis, which, while precise, are costly and
> limited in scalability and temporal resolution. Event-based sensing offers a
> high-speed, low-latency alternative by capturing fine-grained spatiotemporal
> changes efficiently. This work introduces a pioneering event-based microsaccade
> dataset to support research on small eye movement dynamics in cognitive
> computing. Using Blender, we render high-fidelity eye movement scenarios and
> simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
> divided into seven distinct classes. These are converted to event streams using
> v2e, preserving the natural temporal dynamics of microsaccades, with durations
> ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
> Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
> optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
> around 90 percent average accuracy, successfully classifying microsaccades by
> angular displacement, independent of event count or duration. These results
> demonstrate the potential of spiking neural networks for fine motion
> recognition and establish a benchmark for event-based vision research. The
> dataset, code, and trained models will be publicly available at
> https://waseemshariff126.github.io/microsaccades/ .

