---
layout: default
title: Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments
---

# Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments

**arXiv**: [2510.24503v1](https://arxiv.org/abs/2510.24503) | [PDF](https://arxiv.org/pdf/2510.24503.pdf)

**ä½œè€…**: Mortesa Hussaini, Jan TheiÃŸ, Anthony Stein

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLIUæ–¹æ³•ä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­ä¸ªæ€§åŒ–ä¸Žæ³›åŒ–æ€§èƒ½çš„æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ` `å®¢æˆ·ç«¯æ¼‚ç§»` `æ³›åŒ–æ€§èƒ½` `å¼‚æž„æ•°æ®` `FedAvgæ‰©å±•` `è‡ªé€‚åº”ä¸ªæ€§åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼‚æž„æ•°æ®ä¸‹æœ¬åœ°æ¨¡åž‹åç¦»å…¨å±€æœ€ä¼˜ï¼Œå¯¼è‡´å®¢æˆ·ç«¯æ¼‚ç§»å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šæ‰©å±•FedAvgï¼Œå¼•å…¥è‡ªé€‚åº”ä¸ªæ€§åŒ–å› å­å®žçŽ°ä¸ªä½“åŒ–æ›´æ–°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MNISTå’ŒCIFAR-10ä¸Šè¯„ä¼°ï¼Œæ¶µç›–IIDã€éžIIDå’ŒDirichletåˆ†å¸ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In the context of Federated Learning with heterogeneous data environments,
> local models tend to converge to their own local model optima during local
> training steps, deviating from the overall data distributions. Aggregation of
> these local updates, e.g., with FedAvg, often does not align with the global
> model optimum (client drift), resulting in an update that is suboptimal for
> most clients. Personalized Federated Learning approaches address this challenge
> by exclusively focusing on the average local performances of clients' models on
> their own data distribution. Generalization to out-of-distribution samples,
> which is a substantial benefit of FedAvg and represents a significant component
> of robustness, appears to be inadequately incorporated into the assessment and
> evaluation processes. This study involves a thorough evaluation of Federated
> Learning approaches, encompassing both their local performance and their
> generalization capabilities. Therefore, we examine different stages within a
> single communication round to enable a more nuanced understanding of the
> considered metrics. Furthermore, we propose and incorporate a modified approach
> of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
> extending the algorithm by a straightforward individualization step with an
> adaptive personalization factor. We evaluate and compare the approaches
> empirically using MNIST and CIFAR-10 under various distributional conditions,
> including benchmark IID and pathological non-IID, as well as additional novel
> test environments with Dirichlet distribution specifically developed to stress
> the algorithms on complex data heterogeneity.

