---
layout: default
title: Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs
---

# Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs

**arXiv**: [2510.24514v1](https://arxiv.org/abs/2510.24514) | [PDF](https://arxiv.org/pdf/2510.24514.pdf)

**ä½œè€…**: Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLatent Sketchpadæ¡†æž¶ï¼Œé€šè¿‡å†…éƒ¨è§†è§‰è‰å›¾å¢žå¼ºMLLMsåœ¨å¤æ‚åœºæ™¯ä¸­çš„å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰æŽ¨ç†` `è‰å›¾ç”Ÿæˆ` `è‡ªå›žå½’æŽ¨ç†` `å¯è§£é‡Šæ€§` `è§†è§‰è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMsåœ¨éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡çš„å¤æ‚åœºæ™¯ä¸­è¡¨çŽ°ä¸ä½³ï¼Œç¼ºä¹å†…éƒ¨è§†è§‰æ€è€ƒèƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè§†è§‰ç”Ÿæˆåˆ°è‡ªå›žå½’æŽ¨ç†è¿‡ç¨‹ï¼Œä½¿ç”¨Context-Aware Vision Headå’ŒSketch Decoderç”Ÿæˆå¯è§£é‡Šè‰å›¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MazePlanningæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒæŽ¨ç†æ€§èƒ½ä¸Žéª¨å¹²æ¨¡åž‹ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå¹¶æ³›åŒ–è‡³Gemma3å’ŒQwen2.5-VLç­‰æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) excel at visual understanding,
> they often struggle in complex scenarios that require visual planning and
> imagination. Inspired by how humans use sketching as a form of visual thinking
> to develop and communicate ideas, we introduce Latent Sketchpad, a framework
> that equips MLLMs with an internal visual scratchpad. The internal visual
> representations of MLLMs have traditionally been confined to perceptual
> understanding. We repurpose them to support generative visual thought without
> compromising reasoning ability. Building on frontier MLLMs, our approach
> integrates visual generation directly into their native autoregressive
> reasoning process. It allows the model to interleave textual reasoning with the
> generation of visual latents. These latents guide the internal thought process
> and can be translated into sketch images for interpretability. To realize this,
> we introduce two components: a Context-Aware Vision Head autoregressively
> produces visual representations, and a pretrained Sketch Decoder renders these
> into human-interpretable images. We evaluate the framework on our new dataset
> MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
> delivers comparable or even superior reasoning performance to their backbone.
> It further generalizes across distinct frontier MLLMs, including Gemma3 and
> Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
> framework opens new opportunities for richer human-computer interaction and
> broader applications. More details and resources are available on our project
> page: https://latent-sketchpad.github.io/.

