---
layout: default
title: DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation
---

# DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation

**arXiv**: [2510.24261v1](https://arxiv.org/abs/2510.24261) | [PDF](https://arxiv.org/pdf/2510.24261.pdf)

**ä½œè€…**: Jingyi Tian, Le Wang, Sanping Zhou, Sen Wang, Jiayi Li, Gang Hua

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDynaRendæ¡†æž¶ï¼Œé€šè¿‡æŽ©ç æœªæ¥æ¸²æŸ“å­¦ä¹ 3DåŠ¨æ€ç‰¹å¾ä»¥æå‡æœºå™¨äººæ“ä½œæ€§èƒ½**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `3DåŠ¨æ€å­¦ä¹ ` `æŽ©ç æ¸²æŸ“` `è¡¨ç¤ºå­¦ä¹ ` `ä½“ç§¯æ¸²æŸ“` `åŠ¨ä½œä»·å€¼é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººæ“ä½œç­–ç•¥æ³›åŒ–æ€§å·®ï¼ŒæºäºŽçœŸå®žä¸–ç•Œæ•°æ®ç¨€ç¼ºå’ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥è”åˆå­¦ä¹ å‡ ä½•ã€è¯­ä¹‰ä¸ŽåŠ¨æ€
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æŽ©ç é‡å»ºå’Œæœªæ¥é¢„æµ‹ï¼Œé€šè¿‡å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“å­¦ä¹ 3Dæ„ŸçŸ¥çš„åŠ¨æ€ä¸‰å¹³é¢ç‰¹å¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨RLBenchå’ŒColosseumåŸºå‡†åŠçœŸå®žå®žéªŒä¸­ï¼Œæ˜¾è‘—æé«˜ç­–ç•¥æˆåŠŸçŽ‡ã€æ³›åŒ–æ€§å’Œå®žé™…åº”ç”¨æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning generalizable robotic manipulation policies remains a key challenge
> due to the scarcity of diverse real-world training data. While recent
> approaches have attempted to mitigate this through self-supervised
> representation learning, most either rely on 2D vision pretraining paradigms
> such as masked image modeling, which primarily focus on static semantics or
> scene geometry, or utilize large-scale video prediction models that emphasize
> 2D dynamics, thus failing to jointly learn the geometry, semantics, and
> dynamics required for effective manipulation. In this paper, we present
> DynaRend, a representation learning framework that learns 3D-aware and
> dynamics-informed triplane features via masked reconstruction and future
> prediction using differentiable volumetric rendering. By pretraining on
> multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
> dynamics, and task semantics in a unified triplane representation. The learned
> representations can be effectively transferred to downstream robotic
> manipulation tasks via action value map prediction. We evaluate DynaRend on two
> challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
> experiments, demonstrating substantial improvements in policy success rate,
> generalization to environmental perturbations, and real-world applicability
> across diverse manipulation tasks.

