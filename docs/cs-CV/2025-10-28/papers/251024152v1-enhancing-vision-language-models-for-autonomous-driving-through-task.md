---
layout: default
title: Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning
---

# Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning

**arXiv**: [2510.24152v1](https://arxiv.org/abs/2510.24152) | [PDF](https://arxiv.org/pdf/2510.24152.pdf)

**ä½œè€…**: Aodi Wu, Xubo Luo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»»åŠ¡ç‰¹å®šæç¤ºä¸Žç©ºé—´æŽ¨ç†æ¡†æž¶ï¼Œä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `è‡ªåŠ¨é©¾é©¶` `ä»»åŠ¡ç‰¹å®šæç¤º` `ç©ºé—´æŽ¨ç†` `å¤šè§†å›¾å›¾åƒ` `æ¨¡åž‹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£ä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡åž‹éœ€å¤„ç†å¤šä»»åŠ¡å¹²æ‰°é—®é¢˜ã€‚
2. é‡‡ç”¨æ··åˆæç¤ºè·¯ç”±ã€ä»»åŠ¡ç‰¹å®šæç¤ºã€è§†è§‰ç»„è£…å’ŒæŽ¨ç†å‚æ•°ä¼˜åŒ–æ–¹æ³•ã€‚
3. åœ¨Qwen2.5-VL-72Bä¸Šå®žçŽ°å¹³å‡å‡†ç¡®çŽ‡70.87%è‡³72.85%ï¼Œæå‡æ¨¡åž‹é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This technical report presents our solution for the RoboSense Challenge at
> IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
> scene understanding across perception, prediction, planning, and corruption
> detection tasks. We propose a systematic framework built on four core
> components. First, a Mixture-of-Prompts router classifies questions and
> dispatches them to task-specific expert prompts, eliminating interference
> across diverse question types. Second, task-specific prompts embed explicit
> coordinate systems, spatial reasoning rules, role-playing,
> Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
> each task. Third, a visual assembly module composes multi-view images with
> object crops, magenta markers, and adaptive historical frames based on question
> requirements. Fourth, we configure model inference parameters (temperature,
> top-p, message roles) per task to optimize output quality. Implemented on
> Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
> data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
> prompting and spatial grounding substantially enhance VLM performance on
> safety-critical autonomous driving tasks. Code and prompt are available at
> https://github.com/wuaodi/UCAS-CSU-phase2.

