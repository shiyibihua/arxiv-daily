---
layout: default
title: Rethinking Visual Intelligence: Insights from Video Pretraining
---

# Rethinking Visual Intelligence: Insights from Video Pretraining

**arXiv**: [2510.24448v1](https://arxiv.org/abs/2510.24448) | [PDF](https://arxiv.org/pdf/2510.24448.pdf)

**ä½œè€…**: Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†é¢‘æ‰©æ•£æ¨¡åž‹é¢„è®­ç»ƒä»¥æå‡è§†è§‰æ™ºèƒ½çš„æ•°æ®æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `è§†è§‰é¢„è®­ç»ƒ` `å½’çº³åç½®` `æ•°æ®æ•ˆçŽ‡` `æ—¶ç©ºæ•°æ®` `è§†è§‰åŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰æ¨¡åž‹åœ¨ç»„åˆç†è§£ã€æ ·æœ¬æ•ˆçŽ‡å’Œé€šç”¨é—®é¢˜è§£å†³æ–¹é¢è½åŽäºŽè¯­è¨€æ¨¡åž‹
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡åž‹é¢„è®­ç»ƒï¼Œå¼•å…¥æ—¶ç©ºæ•°æ®çš„å½’çº³åç½®
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè§†é¢‘æ¨¡åž‹æ¯”è¯­è¨€æ¨¡åž‹æ•°æ®æ•ˆçŽ‡æ›´é«˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated that large-scale pretraining
> enables systems to adapt rapidly to new problems with little supervision in the
> language domain. This success, however, has not translated as effectively to
> the visual domain, where models, including LLMs, continue to struggle with
> compositional understanding, sample efficiency, and general-purpose
> problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
> direction for bridging this gap. Pretraining on spatiotemporal data endows
> these models with strong inductive biases for structure and dynamics, which we
> hypothesize can support broad task adaptability. To test this, we design a
> controlled evaluation in which both a pretrained LLM and a pretrained VDM are
> equipped with lightweight adapters and presented with tasks in their natural
> modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
> route planning, and cellular automata, VDMs demonstrate higher data efficiency
> than their language counterparts. Taken together, our results indicate that
> video pretraining offers inductive biases that support progress toward visual
> foundation models.

