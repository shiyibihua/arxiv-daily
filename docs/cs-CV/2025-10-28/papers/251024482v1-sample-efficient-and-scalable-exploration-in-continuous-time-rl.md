---
layout: default
title: Sample-efficient and Scalable Exploration in Continuous-Time RL
---

# Sample-efficient and Scalable Exploration in Continuous-Time RL

**arXiv**: [2510.24482v1](https://arxiv.org/abs/2510.24482) | [PDF](https://arxiv.org/pdf/2510.24482.pdf)

**ä½œè€…**: Klemens Iten, Lenart Treven, Bhavya Sukhija, Florian DÃ¶rfler, Andreas Krause

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOMBRLç®—æ³•ä»¥è§£å†³è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ä¸å¯æ‰©å±•æ€§é—®é¢˜**

**å…³é”®è¯**: `è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ` `æ¦‚ç‡æ¨¡å‹` `æ ·æœ¬æ•ˆç‡` `å¯æ‰©å±•æ€§` `æ¨¡å‹ä¸ç¡®å®šæ€§` `æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•é€šå¸¸é’ˆå¯¹ç¦»æ•£æ—¶é—´è®¾è®¡ï¼Œè€ŒçœŸå®æ§åˆ¶ç³»ç»Ÿå¤šä¸ºè¿ç»­æ—¶é—´
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ¦‚ç‡æ¨¡å‹å­¦ä¹ éçº¿æ€§ODEåŠ¨æ€ï¼Œè´ªå©ªæœ€å¤§åŒ–å¥–åŠ±ä¸æ¨¡å‹ä¸ç¡®å®šæ€§åŠ æƒå’Œ
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨æ ‡å‡†å’Œæ— ç›‘ç£RLè®¾ç½®ä¸­ï¼ŒCOMBRLæ ·æœ¬æ•ˆç‡æ›´é«˜ã€å¯æ‰©å±•æ€§æ›´å¼ºï¼Œä¼˜äºåŸºçº¿æ–¹æ³•

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning algorithms are typically designed for discrete-time
> dynamics, even though the underlying real-world control systems are often
> continuous in time. In this paper, we study the problem of continuous-time
> reinforcement learning, where the unknown system dynamics are represented using
> nonlinear ordinary differential equations (ODEs). We leverage probabilistic
> models, such as Gaussian processes and Bayesian neural networks, to learn an
> uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily
> maximizes a weighted sum of the extrinsic reward and model epistemic
> uncertainty. This yields a scalable and sample-efficient approach to
> continuous-time model-based RL. We show that COMBRL achieves sublinear regret
> in the reward-driven setting, and in the unsupervised RL setting (i.e., without
> extrinsic rewards), we provide a sample complexity bound. In our experiments,
> we evaluate COMBRL in both standard and unsupervised RL settings and
> demonstrate that it scales better, is more sample-efficient than prior methods,
> and outperforms baselines across several deep RL tasks.

