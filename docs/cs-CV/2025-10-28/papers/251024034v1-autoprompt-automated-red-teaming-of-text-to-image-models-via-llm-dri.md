---
layout: default
title: AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts
---

# AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts

**arXiv**: [2510.24034v1](https://arxiv.org/abs/2510.24034) | [PDF](https://arxiv.org/pdf/2510.24034.pdf)

**ä½œè€…**: Yufan Liu, Wanqian Zhang, Huashan Chen, Lin Wang, Xiaojun Jia, Zheng Lin, Weiping Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoPromptæ¡†æž¶ï¼Œåˆ©ç”¨LLMç”Ÿæˆå¯è¯»å¯¹æŠ—æç¤ºä»¥é»‘ç›’æµ‹è¯•æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹å®‰å…¨æ¼æ´ž**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹` `å¯¹æŠ—æç¤º` `çº¢é˜Ÿæµ‹è¯•` `å¤§è¯­è¨€æ¨¡åž‹` `é»‘ç›’æ”»å‡»` `é›¶æ ·æœ¬è¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹æ˜“å—å¯¹æŠ—æç¤ºæ”»å‡»ï¼ŒçŽ°æœ‰çº¢é˜Ÿæ–¹æ³•éœ€ç™½ç›’è®¿é—®ä¸”æ•ˆçŽ‡ä½Ž
2. æ–¹æ³•ï¼šé‡‡ç”¨äº¤æ›¿ä¼˜åŒ–å¾®è°ƒæµç¨‹ï¼Œç»“åˆåŒè§„é¿ç­–ç•¥ç»•è¿‡è¿‡æ»¤å™¨å’Œé»‘åå•
3. æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºé«˜çº¢é˜Ÿæ€§èƒ½ã€å¼ºé›¶æ ·æœ¬è¿ç§»æ€§ï¼Œå¯æ”»å‡»å•†ä¸šAPIå¦‚Leonardo.Ai

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite rapid advancements in text-to-image (T2I) models, their safety
> mechanisms are vulnerable to adversarial prompts, which maliciously generate
> unsafe images. Current red-teaming methods for proactively assessing such
> vulnerabilities usually require white-box access to T2I models, and rely on
> inefficient per-prompt optimization, as well as inevitably generate
> semantically meaningless prompts easily blocked by filters. In this paper, we
> propose APT (AutoPrompT), a black-box framework that leverages large language
> models (LLMs) to automatically generate human-readable adversarial suffixes for
> benign prompts. We first introduce an alternating optimization-finetuning
> pipeline between adversarial suffix optimization and fine-tuning the LLM
> utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
> strategy in optimization phase, enabling the bypass of both perplexity-based
> filter and blacklist word filter: (1) we constrain the LLM generating
> human-readable prompts through an auxiliary LLM perplexity scoring, which
> starkly contrasts with prior token-level gibberish, and (2) we also introduce
> banned-token penalties to suppress the explicit generation of banned-tokens in
> blacklist. Extensive experiments demonstrate the excellent red-teaming
> performance of our human-readable, filter-resistant adversarial prompts, as
> well as superior zero-shot transferability which enables instant adaptation to
> unseen prompts and exposes critical vulnerabilities even in commercial APIs
> (e.g., Leonardo.Ai.).

