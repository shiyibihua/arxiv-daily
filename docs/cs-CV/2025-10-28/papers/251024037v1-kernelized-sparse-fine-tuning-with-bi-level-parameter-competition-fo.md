---
layout: default
title: Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models
---

# Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models

**arXiv**: [2510.24037v1](https://arxiv.org/abs/2510.24037) | [PDF](https://arxiv.org/pdf/2510.24037.pdf)

**ä½œè€…**: Shufan Shen, Junshu Sun, Shuhui Wang, Qingming Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSNELLAæ–¹æ³•ä»¥é«˜æ•ˆå¾®è°ƒè§†è§‰æ¨¡åž‹ï¼Œè§£å†³å†…å­˜é«˜å’Œæƒé‡å®šä½ä¸å‡†é—®é¢˜ã€‚**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ç¨€ç–è°ƒä¼˜` `è§†è§‰æ¨¡åž‹` `å†…å­˜ä¼˜åŒ–` `éžçº¿æ€§æ ¸å‡½æ•°` `åŒå±‚ç¨€ç–åˆ†é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç¨€ç–å¾®è°ƒæ–¹æ³•å†…å­˜ä½¿ç”¨é«˜ï¼Œä¸”æƒé‡å®šä½å¿½ç•¥å¾®è°ƒè¿‡ç¨‹è°ƒæ•´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨éžçº¿æ€§æ ¸å‡½æ•°æ‰©å±•ä½Žç§©åˆ†è§£ï¼Œå¹¶å¼•å…¥åŒå±‚ç¨€ç–åˆ†é…æœºåˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆ†ç±»ç­‰ä»»åŠ¡ä¸­å®žçŽ°SOTAæ€§èƒ½ï¼Œå†…å­˜å‡å°‘31.1%-39.9%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
> models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
> remarkable performance by adjusting only the weights most relevant to
> downstream tasks, rather than densely tuning the entire weight matrix. Current
> methods follow a two-stage paradigm. First, it locates task-relevant weights by
> gradient information, which overlooks the parameter adjustments during
> fine-tuning and limits the performance. Second, it updates only the located
> weights by applying a sparse mask to the gradient of the weight matrix, which
> results in high memory usage due to the storage of all weight matrices in the
> optimizer. In this paper, we propose a one-stage method named SNELLA to
> overcome the above limitations. For memory usage, SNELLA selectively updates
> the weight matrix by adding it to another sparse matrix that is merged by two
> low-rank learnable matrices. We extend the low-rank decomposition by
> introducing nonlinear kernel functions, thereby increasing the rank of the
> resulting merged matrix to prevent the interdependency among weight updates,
> enabling better adaptation to downstream tasks. For locating task-relevant
> weights, we propose an adaptive bi-level sparsity allocation mechanism that
> encourages weights to compete across and inside layers based on their
> importance scores in an end-to-end manner. Extensive experiments are conducted
> on classification, segmentation, and generation tasks using different
> pre-trained vision models. The results show that SNELLA achieves SOTA
> performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
> 90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
> Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
> across models with parameter scales from 86M to 632M. Our source codes are
> available at https://github.com/ssfgunner/SNELL.

