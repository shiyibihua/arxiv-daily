---
layout: default
title: ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model
---

# ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model

**arXiv**: [2510.24285v1](https://arxiv.org/abs/2510.24285) | [PDF](https://arxiv.org/pdf/2510.24285.pdf)

**ä½œè€…**: Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai Lin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViPERæ¡†æž¶ä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹çš„ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥` `è‡ªä¸¾æ¡†æž¶` `ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ` `å›¾åƒé‡å»º` `è‡ªè¿›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œå½±å“å®žé™…åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ä¸¤é˜¶æ®µä»»åŠ¡å’Œè‡ªä¸¾æ¡†æž¶ï¼Œé€šè¿‡è‡ªæ‰¹åˆ¤å’Œè‡ªé¢„æµ‹å®žçŽ°è¿­ä»£è¿›åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡1.7%ï¼Œç»†ç²’åº¦æ„ŸçŸ¥æœ€é«˜æå‡6.0%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The limited capacity for fine-grained visual perception presents a critical
> bottleneck for Vision-Language Models (VLMs) in real-world applications.
> Addressing this is challenging due to the scarcity of high-quality data and the
> limitations of existing methods: supervised fine-tuning (SFT) often compromises
> general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
> reasoning over visual perception. To bridge this gap, we propose a novel
> two-stage task that structures visual perception learning as a coarse-to-fine
> progressive process. Based on this task formulation, we develop ViPER, a
> self-bootstrapping framework specifically designed to enable iterative
> evolution through self-critiquing and self-prediction. By synergistically
> integrating image-level and instance-level reconstruction with a two-stage
> reinforcement learning strategy, ViPER establishes a closed-loop training
> paradigm, where internally synthesized data directly fuel the enhancement of
> perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
> Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
> benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
> Qwen-Viper consistently demonstrates superior performance across different
> vision-language scenarios while maintaining generalizability. Beyond enabling
> self-improvement in perceptual capabilities, ViPER provides concrete evidence
> for the reciprocal relationship between generation and understanding, a
> breakthrough to developing more autonomous and capable VLMs.

