---
layout: default
title: SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs
---

# SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs

**arXiv**: [2510.24214v1](https://arxiv.org/abs/2510.24214) | [PDF](https://arxiv.org/pdf/2510.24214.pdf)

**ä½œè€…**: Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCOPEæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­è§†è§‰ä»¤ç‰Œå†—ä½™é—®é¢˜ï¼Œæå‡è¯­ä¹‰å®Œæ•´æ€§ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰ä»¤ç‰Œå‰ªæž` `è¯­ä¹‰å®Œæ•´æ€§` `æ˜¾è‘—æ€§ä¸Žè¦†ç›–åº¦` `é«˜æ•ˆè®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰ä»¤ç‰Œå‰ªæžæ–¹æ³•ä¾èµ–æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¯¼è‡´è¯­ä¹‰ä¸å®Œæ•´ã€‚
2. SCOPEè”åˆå»ºæ¨¡æ˜¾è‘—æ€§å’Œè¦†ç›–åº¦ï¼Œè¿­ä»£é€‰æ‹©ä»¤ç‰Œä»¥ä¼˜åŒ–è¯­ä¹‰ä¿ç•™ã€‚
3. åœ¨LLaVAæ¨¡åž‹ä¸Šå®žéªŒï¼ŒSCOPEåœ¨å¤šä¸ªåŸºå‡†ä¸Šä¼˜äºŽå…ˆå‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) typically process a large number of
> visual tokens, leading to considerable computational overhead, even though many
> of these tokens are redundant. Existing visual token pruning methods primarily
> focus on selecting the most salient tokens based on attention scores, resulting
> in the semantic incompleteness of the selected tokens. In this paper, we
> propose a novel visual token pruning strategy, called
> \textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
> for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
> coverage of the selected visual tokens to better preserve semantic
> completeness. Specifically, we introduce a set-coverage for a given set of
> selected tokens, computed based on the token relationships. We then define a
> token-coverage gain for each unselected token, quantifying how much additional
> coverage would be obtained by including it. By integrating the saliency score
> into the token-coverage gain, we propose our SCOPE score and iteratively select
> the token with the highest SCOPE score. We conduct extensive experiments on
> multiple vision-language understanding benchmarks using the LLaVA-1.5 and
> LLaVA-Next models. Experimental results demonstrate that our method
> consistently outperforms prior approaches. Our code is available at
> \href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

