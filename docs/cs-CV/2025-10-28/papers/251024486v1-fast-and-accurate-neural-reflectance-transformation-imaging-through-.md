---
layout: default
title: Fast and accurate neural reflectance transformation imaging through knowledge distillation
---

# Fast and accurate neural reflectance transformation imaging through knowledge distillation

**arXiv**: [2510.24486v1](https://arxiv.org/abs/2510.24486) | [PDF](https://arxiv.org/pdf/2510.24486.pdf)

**ä½œè€…**: Tinsae G. Dulecha, Leonardo Righetto, Ruggero Pintus, Enrico Gobbetti, Andrea Giachetti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçŸ¥è¯†è’¸é¦çš„å¿«é€Ÿå‡†ç¡®ç¥žç»åå°„å˜æ¢æˆåƒæ–¹æ³•ï¼Œä»¥é™ä½Žè®¡ç®—æˆæœ¬ã€‚**

**å…³é”®è¯**: `åå°„å˜æ¢æˆåƒ` `çŸ¥è¯†è’¸é¦` `ç¥žç»æ¸²æŸ“` `è®¡ç®—ä¼˜åŒ–` `è¡¨é¢åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¥žç»åå°„å˜æ¢æˆåƒæ¸²æŸ“è®¡ç®—æ˜‚è´µï¼Œéš¾ä»¥åœ¨æœ‰é™ç¡¬ä»¶ä¸Šå¤„ç†å¤§å›¾åƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œè®­ç»ƒå°åž‹ç½‘ç»œä»¥è¿‘ä¼¼åŽŸæ¨¡åž‹ï¼Œå‡å°‘å‚æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥ï¼Œä½†æ—¨åœ¨å®žçŽ°é«˜è´¨é‡æ¸²æŸ“ï¼ŒåŒæ—¶é™ä½Žè®¡ç®—éœ€æ±‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reflectance Transformation Imaging (RTI) is very popular for its ability to
> visually analyze surfaces by enhancing surface details through interactive
> relighting, starting from only a few tens of photographs taken with a fixed
> camera and variable illumination. Traditional methods like Polynomial Texture
> Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
> to accurately capture complex reflectance fields using few per-pixel
> coefficients and fixed bases, leading to artifacts, especially in highly
> reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
> autoencoder to learn a compact function that better approximates the local
> reflectance as a function of light directions, has been shown to produce
> superior quality at comparable storage cost. However, as it performs
> interactive relighting with custom decoder networks with many parameters, the
> rendering step is computationally expensive and not feasible at full resolution
> for large images on limited hardware. Earlier attempts to reduce costs by
> directly training smaller networks have failed to produce valid results. For
> this reason, we propose to reduce its computational cost through a novel
> solution based on Knowledge Distillation (DisK-NeuralRTI). ...

