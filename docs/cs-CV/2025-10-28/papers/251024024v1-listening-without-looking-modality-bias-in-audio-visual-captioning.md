---
layout: default
title: Listening without Looking: Modality Bias in Audio-Visual Captioning
---

# Listening without Looking: Modality Bias in Audio-Visual Captioning

**arXiv**: [2510.24024v1](https://arxiv.org/abs/2510.24024) | [PDF](https://arxiv.org/pdf/2510.24024.pdf)

**ä½œè€…**: Yuchi Ishikawa, Toranosuke Manabe, Tatsuya Komatsu, Yoshimitsu Aoki

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºéŸ³é¢‘-è§†è§‰å­—å¹•æ¨¡åž‹ä¸­çš„æ¨¡æ€åå·®ï¼Œå¹¶æå‡ºå¹³è¡¡è®­ç»ƒæ–¹æ³•ä»¥å‡å°‘åå·®**

**å…³é”®è¯**: `éŸ³é¢‘-è§†è§‰å­—å¹•` `æ¨¡æ€åå·®` `é²æ£’æ€§æµ‹è¯•` `æ•°æ®é›†å¢žå¼º` `æ¨¡æ€èžåˆ` `æ¨¡åž‹è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šéŸ³é¢‘-è§†è§‰å­—å¹•æ¨¡åž‹å­˜åœ¨æ¨¡æ€åå·®ï¼ŒéŸ³é¢‘æµä¸»å¯¼ï¼Œå½±å“äº’è¡¥æ€§å’Œé²æ£’æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é€‰æ‹©æ€§æŠ‘åˆ¶æˆ–ç ´åæ¨¡æ€æµï¼Œç³»ç»Ÿæµ‹è¯•æ¨¡åž‹å¯¹éŸ³é¢‘å’Œè§†è§‰çš„æ•æ„Ÿåº¦
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨AudioVisualCapsæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ¨¡åž‹æ¨¡æ€åå·®å‡å°‘ï¼Œé²æ£’æ€§æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Audio-visual captioning aims to generate holistic scene descriptions by
> jointly modeling sound and vision. While recent methods have improved
> performance through sophisticated modality fusion, it remains unclear to what
> extent the two modalities are complementary in current audio-visual captioning
> models and how robust these models are when one modality is degraded. We
> address these questions by conducting systematic modality robustness tests on
> LAVCap, a state-of-the-art audio-visual captioning model, in which we
> selectively suppress or corrupt the audio or visual streams to quantify
> sensitivity and complementarity. The analysis reveals a pronounced bias toward
> the audio stream in LAVCap. To evaluate how balanced audio-visual captioning
> models are in their use of both modalities, we augment AudioCaps with textual
> annotations that jointly describe the audio and visual streams, yielding the
> AudioVisualCaps dataset. In our experiments, we report LAVCap baseline results
> on AudioVisualCaps. We also evaluate the model under modality robustness tests
> on AudioVisualCaps and the results indicate that LAVCap trained on
> AudioVisualCaps exhibits less modality bias than when trained on AudioCaps.

