---
layout: default
title: UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations
---

# UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations

**arXiv**: [2510.24116v1](https://arxiv.org/abs/2510.24116) | [PDF](https://arxiv.org/pdf/2510.24116.pdf)

**ä½œè€…**: Fengming Yu, Haiwei Pan, Kejia Zhang, Jian Guan, Haiying Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUHKDæ¡†æž¶ï¼Œé€šè¿‡é¢‘åŸŸè¡¨ç¤ºè§£å†³å¼‚æž„æ¨¡åž‹çŸ¥è¯†è’¸é¦ä¸­çš„è¯­ä¹‰å·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `å¼‚æž„çŸ¥è¯†è’¸é¦` `é¢‘åŸŸè¡¨ç¤º` `ç‰¹å¾å¯¹é½` `æ¨¡åž‹åŽ‹ç¼©` `è§†è§‰åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼‚æž„æ¨¡åž‹çŸ¥è¯†è’¸é¦ä¸­ï¼Œä¸­é—´ç‰¹å¾è¯­ä¹‰å·®å¼‚é˜»ç¢çŸ¥è¯†è¿ç§»ï¼ŒçŽ°æœ‰æ–¹æ³•å¤šé’ˆå¯¹åŒæž„æ¨¡åž‹
2. åˆ©ç”¨å‚…é‡Œå¶å˜æ¢æ•èŽ·å…¨å±€ç‰¹å¾ï¼Œç»“åˆç‰¹å¾å˜æ¢å’Œå¯¹é½æ¨¡å—å®žçŽ°è·¨æž¶æž„çŸ¥è¯†ä¼ é€’
3. åœ¨CIFAR-100å’ŒImageNet-1Kä¸Šå®žéªŒï¼Œå‡†ç¡®çŽ‡æå‡5.59%å’Œ0.83%ï¼Œä¼˜äºŽæœ€æ–°æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge distillation (KD) is an effective model compression technique that
> transfers knowledge from a high-performance teacher to a lightweight student,
> reducing cost while maintaining accuracy. In visual applications, where
> large-scale image models are widely used, KD enables efficient deployment.
> However, architectural diversity introduces semantic discrepancies that hinder
> the use of intermediate representations. Most existing KD methods are designed
> for homogeneous models and degrade in heterogeneous scenarios, especially when
> intermediate features are involved. Prior studies mainly focus on the logits
> space, making limited use of the semantic information in intermediate layers.
> To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
> is proposed as a framework that leverages intermediate features in the
> frequency domain for cross-architecture transfer. Fourier transform is applied
> to capture global feature information, alleviating representational
> discrepancies between heterogeneous teacher-student pairs. A Feature
> Transformation Module (FTM) produces compact frequency-domain representations
> of teacher features, while a learnable Feature Alignment Module (FAM) projects
> student features and aligns them via multi-level matching. Training is guided
> by a joint objective combining mean squared error on intermediate features with
> Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
> demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
> as an effective approach for unifying heterogeneous representations and
> enabling efficient utilization of visual knowledge

