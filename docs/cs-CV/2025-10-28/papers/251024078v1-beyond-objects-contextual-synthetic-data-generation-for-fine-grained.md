---
layout: default
title: Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification
---

# Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification

**arXiv**: [2510.24078v1](https://arxiv.org/abs/2510.24078) | [PDF](https://arxiv.org/pdf/2510.24078.pdf)

**ä½œè€…**: William Yang, Xindi Wu, Zhiwei Deng, Esin Tureci, Olga Russakovsky

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBOBæ–¹æ³•ä»¥è§£å†³ç»†ç²’åº¦åˆ†ç±»ä¸­åˆæˆæ•°æ®è¿‡æ‹Ÿåˆä¸Žå¤šæ ·æ€§ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `ç»†ç²’åº¦åˆ†ç±»` `åˆæˆæ•°æ®ç”Ÿæˆ` `æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹` `ä½Žæ ·æœ¬å­¦ä¹ ` `æ¡ä»¶ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹ç”Ÿæˆåˆæˆæ•°æ®æ—¶æ˜“è¿‡æ‹Ÿåˆä¸”å¤šæ ·æ€§é™ä½Ž
2. æ–¹æ³•è¦ç‚¹ï¼šæå–ç±»æ— å…³å±žæ€§å¹¶æ¡ä»¶åŒ–å¾®è°ƒï¼Œç”Ÿæˆæ—¶è¾¹ç¼˜åŒ–ä»¥ä¿ç•™å…ˆéªŒ
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®žçŽ°SOTAï¼Œæå‡åˆ†ç±»å‡†ç¡®çŽ‡å¹¶å‡å°‘çœŸå®žæ•°æ®éœ€æ±‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-image (T2I) models are increasingly used for synthetic dataset
> generation, but generating effective synthetic training data for classification
> remains challenging. Fine-tuning a T2I model with a few real examples can help
> improve the quality of synthetic training data; however, it may also cause
> overfitting and reduce diversity in the generated samples. We propose a
> fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
> fine-grained classification. Given a small set of real examples, we first
> extract class-agnostic attributes such as scene background and object pose. We
> then explicitly condition on these attributes during fine-tuning of the T2I
> model and marginalize them out during generation. This design mitigates
> overfitting, preserves the T2I model's generative prior, reduces estimation
> errors, and further minimizes unintended inter-class associations. Extensive
> experiments across multiple T2I models, backbones, and datasets show that our
> method achieves state-of-the-art performance in low-shot fine-grained
> classification when augmented with synthetic data. Concretely, BOB outperforms
> DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
> a CLIP classifier with five real images augmented with 100 synthetic images).
> In three of the four benchmarks, fine-tuning downstream models with 5 real
> images augmented with BOB achieves better performance than fine-tuning with 10
> real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
> settings, with 2+% accuracy improvements in 14 of these settings.

