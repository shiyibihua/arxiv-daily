---
layout: default
title: Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding
---

# Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15178v1</a>
  <a href="https://arxiv.org/pdf/2509.15178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15178v1', 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**æœŸåˆŠ**: NeurIPS2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zaiquanyang/LLaVA_Next_STVG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤šæ¨¡æ€LLMè¿›è¡Œé›¶æ ·æœ¬æ—¶ç©ºè§†é¢‘å®šä½ï¼Œæå‡ºDSTHå’ŒTASç­–ç•¥ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶ç©ºè§†é¢‘å®šä½` `å¤šæ¨¡æ€LLM` `é›¶æ ·æœ¬å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰STVGæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬æŸ¥è¯¢ä¸­çš„å±æ€§å’ŒåŠ¨ä½œä¿¡æ¯ï¼Œå¯¼è‡´å®šä½ç²¾åº¦å—é™ã€‚
2. æå‡ºåˆ†è§£æ—¶ç©ºé«˜äº®ï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºç»„è£…ï¼ˆTASï¼‰ç­–ç•¥ï¼Œæå‡MLLMåœ¨STVGä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªSTVGåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¢ç´¢é›¶æ ·æœ¬æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰æ–¹æ³•ï¼Œå³æ ¹æ®æ–‡æœ¬æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„æ—¶ç©ºåŒºåŸŸã€‚ç ”ç©¶æ­ç¤ºäº†MLLMçš„ä¸¤ä¸ªå…³é”®ç‰¹æ€§ï¼šä¸€æ˜¯MLLMå€¾å‘äºåŠ¨æ€åˆ†é…ç‰¹æ®Štokenï¼ˆç§°ä¸ºâ€œgrounding tokenâ€ï¼‰æ¥å®šä½æ–‡æœ¬æŸ¥è¯¢ï¼›äºŒæ˜¯MLLMç”±äºæ— æ³•å……åˆ†æ•´åˆæ–‡æœ¬æŸ¥è¯¢ä¸­çš„çº¿ç´¢ï¼ˆä¾‹å¦‚å±æ€§ã€åŠ¨ä½œï¼‰è¿›è¡Œæ¨ç†ï¼Œå¸¸å¸¸å¯¼è‡´æ¬¡ä¼˜çš„å®šä½æ•ˆæœã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºMLLMçš„é›¶æ ·æœ¬STVGæ¡†æ¶ï¼ŒåŒ…å«æ–°é¢–çš„åˆ†è§£æ—¶ç©ºé«˜äº®ï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºç»„è£…ï¼ˆTASï¼‰ç­–ç•¥ï¼Œä»¥é‡Šæ”¾MLLMçš„æ¨ç†èƒ½åŠ›ã€‚DSTHç­–ç•¥é¦–å…ˆå°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§å’ŒåŠ¨ä½œå­æŸ¥è¯¢ï¼Œä»è€Œåœ¨ç©ºé—´å’Œæ—¶é—´ä¸ŠæŸ¥è¯¢ç›®æ ‡çš„å­˜åœ¨æ€§ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨ä¸€ç§æ–°é¢–çš„logitå¼•å¯¼çš„é‡æ³¨æ„åŠ›ï¼ˆLRAï¼‰æ¨¡å—ï¼Œé€šè¿‡æ­£åˆ™åŒ–æ¯ä¸ªå­æŸ¥è¯¢çš„tokené¢„æµ‹æ¥å­¦ä¹ æ½œåœ¨å˜é‡ä½œä¸ºç©ºé—´å’Œæ—¶é—´æç¤ºã€‚è¿™äº›æç¤ºåˆ†åˆ«çªå‡ºæ˜¾ç¤ºå±æ€§å’ŒåŠ¨ä½œçº¿ç´¢ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å¯é çš„ç©ºé—´å’Œæ—¶é—´ç›¸å…³è§†è§‰åŒºåŸŸã€‚æ­¤å¤–ï¼Œç”±äºå±æ€§å­æŸ¥è¯¢çš„ç©ºé—´å®šä½åº”åœ¨æ—¶é—´ä¸Šä¿æŒä¸€è‡´ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†TASç­–ç•¥ï¼Œä½¿ç”¨åŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§ä½œä¸ºè¾“å…¥æ¥ç»„è£…é¢„æµ‹ï¼Œä»¥å¸®åŠ©æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§MLLMä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜å®ƒåœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºSOTAæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æŸ¥è¯¢ï¼Œåœ¨è§†é¢‘ä¸­å®šä½å¯¹åº”çš„æ—¶ç©ºåŒºåŸŸã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥å……åˆ†åˆ©ç”¨æ–‡æœ¬æŸ¥è¯¢ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä¾‹å¦‚å±æ€§å’ŒåŠ¨ä½œï¼Œå¯¼è‡´å®šä½ç²¾åº¦ä¸é«˜ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œå¹¶é’ˆå¯¹MLLMåœ¨STVGä»»åŠ¡ä¸­çš„ä¸è¶³è¿›è¡Œæ”¹è¿›ã€‚é€šè¿‡åˆ†è§£æŸ¥è¯¢ã€å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶å’Œæ—¶é—´å¢å¼ºï¼Œå¼•å¯¼MLLMæ›´å‡†ç¡®åœ°ç†è§£æ–‡æœ¬æŸ¥è¯¢å¹¶å®šä½è§†é¢‘ä¸­çš„ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æŸ¥è¯¢åˆ†è§£ï¼šå°†åŸå§‹æ–‡æœ¬æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§å’ŒåŠ¨ä½œä¸¤ä¸ªå­æŸ¥è¯¢ã€‚2) åˆ†è§£æ—¶ç©ºé«˜äº®ï¼ˆDSTHï¼‰ï¼šåˆ©ç”¨logitå¼•å¯¼çš„é‡æ³¨æ„åŠ›ï¼ˆLRAï¼‰æ¨¡å—ï¼Œå­¦ä¹ ç©ºé—´å’Œæ—¶é—´æç¤ºï¼Œçªå‡ºæ˜¾ç¤ºå±æ€§å’ŒåŠ¨ä½œçº¿ç´¢ã€‚3) æ—¶é—´å¢å¼ºç»„è£…ï¼ˆTASï¼‰ï¼šä½¿ç”¨åŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§ä½œä¸ºè¾“å…¥ï¼Œç»„è£…é¢„æµ‹ç»“æœï¼Œæé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºDSTHç­–ç•¥å’ŒTASç­–ç•¥ã€‚DSTHç­–ç•¥é€šè¿‡åˆ†è§£æŸ¥è¯¢å’Œå¼•å…¥LRAæ¨¡å—ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ–‡æœ¬æŸ¥è¯¢ä¸­çš„å±æ€§å’ŒåŠ¨ä½œä¿¡æ¯ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚TASç­–ç•¥é€šè¿‡æ—¶é—´å¢å¼ºï¼Œæé«˜äº†å®šä½ç»“æœçš„æ—¶é—´ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLRAæ¨¡å—é€šè¿‡æ­£åˆ™åŒ–æ¯ä¸ªå­æŸ¥è¯¢çš„tokené¢„æµ‹æ¥å­¦ä¹ æ½œåœ¨å˜é‡ä½œä¸ºç©ºé—´å’Œæ—¶é—´æç¤ºã€‚TASç­–ç•¥ä¸­ï¼Œæ—¶é—´å¢å¼ºçš„å…·ä½“æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå¸§é‡‡æ ·ç­–ç•¥ã€æ—¶é—´çª—å£å¤§å°ï¼‰ä»¥åŠå¦‚ä½•èåˆåŸå§‹å¸§å’Œå¢å¼ºå¸§çš„é¢„æµ‹ç»“æœæ˜¯å…³é”®è®¾è®¡ç»†èŠ‚ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦è€ƒè™‘ç©ºé—´å®šä½çš„å‡†ç¡®æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€ä¼˜æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚ï¼ŒIoUæŒ‡æ ‡ï¼‰å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€æ™ºèƒ½å®‰é˜²ã€è‡ªåŠ¨é©¾é©¶ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œå¯ä»¥é€šè¿‡æ–‡æœ¬æŸ¥è¯¢å¿«é€Ÿå®šä½ç‰¹å®šäº‹ä»¶æˆ–äººç‰©ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥å¸®åŠ©è½¦è¾†ç†è§£å‘¨å›´ç¯å¢ƒï¼Œæé«˜å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.
>   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.

