---
layout: default
title: Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model
---

# Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14664" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14664v1</a>
  <a href="https://arxiv.org/pdf/2509.14664.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14664v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14664v1', 'Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shinnosuke Hirano, Yuiga Wada, Tsumugi Iida, Komei Sugiura

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: Accepted for presentation at ICONIP2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ³¨æ„åŠ›æ ¼é€‚é…å™¨(ALA)ä¸äº¤æ›¿å‘¨æœŸæ¶æ„(AEA)ï¼Œç”¨äºè§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰è§£é‡Šç”Ÿæˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è§£é‡Š` `å¯è§£é‡Šæ€§AI` `è§†è§‰åŸºç¡€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡å‹é€‚é…` `äº¤æ›¿è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è§£é‡Šæ–¹æ³•ç¼ºä¹å¯¹å¤æ‚è§†è§‰åŸºç¡€æ¨¡å‹çš„é€‚åº”æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡çš„è§†è§‰è§£é‡Šã€‚
2. æå‡ºæ³¨æ„åŠ›æ ¼é€‚é…å™¨ï¼ˆALAï¼‰å’Œäº¤æ›¿å‘¨æœŸæ¶æ„ï¼ˆAEAï¼‰ï¼Œé€šè¿‡å‚æ•°æ›´æ–°å¢å¼ºæ¨¡å‹å¯è§£é‡Šæ€§ï¼Œå¹¶é¿å…æ‰‹åŠ¨é€‰æ‹©å±‚ã€‚
3. åœ¨CUB-200-2011å’ŒImageNet-Sæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨IoUã€æ’å…¥/åˆ é™¤åˆ†æ•°ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæå‡äº†è§£é‡Šè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è‡´åŠ›äºè§£å†³è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„è§†è§‰è§£é‡Šç”Ÿæˆé—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å› ç¼ºä¹é€‚åº”æ€§è€Œéš¾ä»¥åº”ç”¨äºå¤æ‚æ¨¡å‹çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰åŸºç¡€æ¨¡å‹è§£é‡Šç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆè§£é‡Šå¹¶éƒ¨åˆ†æ›´æ–°æ¨¡å‹å‚æ•°ä»¥å¢å¼ºå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ç§æ–°æœºåˆ¶ï¼šæ³¨æ„åŠ›æ ¼é€‚é…å™¨ï¼ˆALAï¼‰å’Œäº¤æ›¿å‘¨æœŸæ¶æ„ï¼ˆAEAï¼‰ã€‚ALAæœºåˆ¶é€šè¿‡æ¶ˆé™¤æ‰‹åŠ¨å±‚é€‰æ‹©çš„éœ€æ±‚æ¥ç®€åŒ–è¿‡ç¨‹ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒAEAæœºåˆ¶æ¯éš”ä¸€ä¸ªepochæ›´æ–°ALAçš„å‚æ•°ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ³¨æ„åŠ›åŒºåŸŸè¿‡å°çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨CUB-200-2011å’ŒImageNet-Sä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ã€æ’å…¥åˆ†æ•°ã€åˆ é™¤åˆ†æ•°å’Œæ’å…¥-åˆ é™¤åˆ†æ•°æ–¹é¢å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨CUB-200-2011æ•°æ®é›†ä¸Šçš„å¹³å‡IoUæé«˜äº†53.2ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰åŸºç¡€æ¨¡å‹ä¸­è§†è§‰è§£é‡Šç”Ÿæˆçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„é€‚åº”æ€§ï¼Œéš¾ä»¥åº”ç”¨äºå‚æ•°é‡å¤§ã€ç»“æ„å¤æ‚çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†è§‰è§£é‡Šè´¨é‡ä¸é«˜ï¼Œå¯ä¿¡åº¦ä¸è¶³ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦æ‰‹åŠ¨é€‰æ‹©å‚ä¸è§£é‡Šç”Ÿæˆçš„ç½‘ç»œå±‚ï¼Œè¿‡ç¨‹ç¹çä¸”ä¾èµ–ç»éªŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„é€‚é…å™¨æ¨¡å—ï¼ˆALAï¼‰æ¥å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶é¿å…å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡ä¿®æ”¹ã€‚é€šè¿‡äº¤æ›¿è®­ç»ƒé€‚é…å™¨æ¨¡å—çš„å‚æ•°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è°ƒæ•´æ³¨æ„åŠ›åŒºåŸŸçš„å¤§å°ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„è§†è§‰è§£é‡Šã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åœ¨æ¨¡å‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä»¥åŠæ’å…¥åˆ°è¯¥æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æ ¼é€‚é…å™¨ï¼ˆALAï¼‰ã€‚ALAæ¨¡å—çš„å‚æ•°é€šè¿‡äº¤æ›¿å‘¨æœŸæ¶æ„ï¼ˆAEAï¼‰è¿›è¡Œè®­ç»ƒï¼Œå³æ¯éš”ä¸€ä¸ªepochæ›´æ–°ALAçš„å‚æ•°ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ground truthçš„è§†è§‰è§£é‡Šä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä¼˜åŒ–ALAçš„å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®çš„æ³¨æ„åŠ›å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ³¨æ„åŠ›æ ¼é€‚é…å™¨ï¼ˆALAï¼‰å’Œäº¤æ›¿å‘¨æœŸæ¶æ„ï¼ˆAEAï¼‰ã€‚ALAé€šè¿‡å¯å­¦ä¹ çš„å‚æ•°æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œæ— éœ€æ‰‹åŠ¨é€‰æ‹©å±‚ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚AEAé€šè¿‡äº¤æ›¿æ›´æ–°ALAçš„å‚æ•°ï¼Œè§£å†³äº†æ³¨æ„åŠ›åŒºåŸŸè¿‡å°çš„é—®é¢˜ï¼Œä»è€Œç”Ÿæˆæ›´å…¨é¢çš„è§†è§‰è§£é‡Šã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šALAæ¨¡å—çš„å…·ä½“ç»“æ„æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒåŠŸèƒ½æ˜¯è°ƒæ•´æ¨¡å‹å†…éƒ¨çš„æ³¨æ„åŠ›æƒé‡ã€‚AEAæœºåˆ¶çš„å…³é”®åœ¨äºäº¤æ›¿æ›´æ–°çš„é¢‘ç‡ï¼Œå³æ¯éš”ä¸€ä¸ªepochæ›´æ–°ALAçš„å‚æ•°ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯ä½¿ALAç”Ÿæˆçš„æ³¨æ„åŠ›å›¾å°½å¯èƒ½æ¥è¿‘ground truthçš„è§†è§‰è§£é‡Šã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å½¢å¼æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CUB-200-2011å’ŒImageNet-Sæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å°¤å…¶æ˜¯åœ¨CUB-200-2011æ•°æ®é›†ä¸Šï¼Œæœ€ä½³æ¨¡å‹ç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡IoUæå‡äº†53.2ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜è§†è§‰è§£é‡Šçš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å¯è§£é‡Šæ€§åˆ†æï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹çš„å¯ä¿¡åº¦å’Œé€æ˜åº¦ã€‚åœ¨åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨æ”¸å…³é¢†åŸŸï¼Œè¯¥æŠ€æœ¯æœ‰åŠ©äºå‘ç°æ¨¡å‹æ½œåœ¨çš„åå·®å’Œé£é™©ï¼Œæå‡ç³»ç»Ÿçš„å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this study, we consider the problem of generating visual explanations in visual foundation models. Numerous methods have been proposed for this purpose; however, they often cannot be applied to complex models due to their lack of adaptability. To overcome these limitations, we propose a novel explanation generation method in visual foundation models that is aimed at both generating explanations and partially updating model parameters to enhance interpretability. Our approach introduces two novel mechanisms: Attention Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism simplifies the process by eliminating the need for manual layer selection, thus enhancing the model's adaptability and interpretability. Moreover, the AEA mechanism, which updates ALA's parameters every other epoch, effectively addresses the common issue of overly small attention regions. We evaluated our method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results showed that our method outperformed the baseline methods in terms of mean intersection over union (IoU), insertion score, deletion score, and insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets. Notably, our best model achieved a 53.2-point improvement in mean IoU on the CUB-200-2011 dataset compared with the baselines.

