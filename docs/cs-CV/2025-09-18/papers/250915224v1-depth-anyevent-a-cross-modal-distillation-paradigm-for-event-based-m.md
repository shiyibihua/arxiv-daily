---
layout: default
title: Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation
---

# Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15224" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15224v1</a>
  <a href="https://arxiv.org/pdf/2509.15224.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15224v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15224v1', 'Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: ICCV 2025. Code: https://github.com/bartn8/depthanyevent/ Project Page: https://bartn8.github.io/depthanyevent/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè·¨æ¨¡æ€è’¸é¦çš„äº‹ä»¶ç›¸æœºå•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `å•ç›®æ·±åº¦ä¼°è®¡` `è·¨æ¨¡æ€è’¸é¦` `è§†è§‰åŸºç¡€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `ä»£ç†æ ‡ç­¾` `å¾ªç¯ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰äº‹ä»¶ç›¸æœºå•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•å—é™äºç¼ºä¹å¤§è§„æ¨¡ã€å¸¦æ·±åº¦æ ‡æ³¨çš„æ•°æ®é›†ï¼Œè®­ç»ƒå›°éš¾ã€‚
2. æå‡ºä¸€ç§è·¨æ¨¡æ€è’¸é¦èŒƒå¼ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ä»RGBå›¾åƒç”Ÿæˆäº‹ä»¶æ•°æ®çš„å¯†é›†ä»£ç†æ ‡ç­¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€æ˜‚è´µçš„æ·±åº¦æ ‡æ³¨å³å¯è¾¾åˆ°SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶ç›¸æœºèƒ½å¤Ÿæ•æ‰ç¨€ç–ã€é«˜æ—¶é—´åˆ†è¾¨ç‡çš„è§†è§‰ä¿¡æ¯ï¼Œä½¿å…¶ç‰¹åˆ«é€‚ç”¨äºé«˜é€Ÿè¿åŠ¨å’Œå…‰ç…§æ¡ä»¶å‰§çƒˆå˜åŒ–ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒã€‚ç„¶è€Œï¼Œç¼ºä¹å¸¦æœ‰å¯†é›†çœŸå€¼æ·±åº¦æ ‡æ³¨çš„å¤§å‹æ•°æ®é›†é˜»ç¢äº†åŸºäºå­¦ä¹ çš„äº‹ä»¶æ•°æ®å•ç›®æ·±åº¦ä¼°è®¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€è’¸é¦èŒƒå¼ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ç”Ÿæˆå¯†é›†çš„ä»£ç†æ ‡ç­¾ã€‚æˆ‘ä»¬çš„ç­–ç•¥éœ€è¦ä¸RGBå¸§ç©ºé—´å¯¹é½çš„äº‹ä»¶æµï¼Œå³ä½¿æ˜¯ç°æˆçš„è®¾ç½®ï¼Œå¹¶åˆ©ç”¨å¤§è§„æ¨¡VFMçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºè®®è°ƒæ•´VFMï¼Œæ— è®ºæ˜¯åƒDepth Anything v2 (DAv2)è¿™æ ·çš„åŸå§‹æ¨¡å‹ï¼Œè¿˜æ˜¯ä»ä¸­æ´¾ç”Ÿå‡ºä¸€ç§æ–°çš„å¾ªç¯æ¶æ„ï¼Œä»¥ä»å•ç›®äº‹ä»¶ç›¸æœºæ¨æ–­æ·±åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†i) ä¸ä¸éœ€è¦æ˜‚è´µæ·±åº¦æ ‡æ³¨çš„å®Œå…¨ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è·¨æ¨¡æ€èŒƒå¼å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶ä¸” ii) æˆ‘ä»¬åŸºäºVFMçš„æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³äº‹ä»¶ç›¸æœºå•ç›®æ·±åº¦ä¼°è®¡ä¸­ï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡å¸¦æ·±åº¦æ ‡æ³¨æ•°æ®é›†è€Œå¯¼è‡´çš„è®­ç»ƒå›°éš¾é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–åˆæˆæ•°æ®ï¼Œè¦ä¹ˆéœ€è¦æ˜‚è´µçš„æ·±åº¦ä¼ æ„Ÿå™¨è¿›è¡Œæ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è·¨æ¨¡æ€è’¸é¦ï¼Œå°†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰åœ¨RGBå›¾åƒä¸Šå­¦ä¹ åˆ°çš„æ·±åº¦ä¿¡æ¯è¿ç§»åˆ°äº‹ä»¶æ•°æ®ä¸Šã€‚é€šè¿‡å°†RGBå›¾åƒçš„æ·±åº¦é¢„æµ‹ä½œä¸ºäº‹ä»¶æ•°æ®çš„ä»£ç†æ ‡ç­¾ï¼Œä»è€Œé¿å…äº†ç›´æ¥æ ‡æ³¨äº‹ä»¶æ•°æ®çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä»£ç†æ ‡ç­¾ç”Ÿæˆé˜¶æ®µï¼šä½¿ç”¨é¢„è®­ç»ƒçš„VFMï¼ˆå¦‚Depth Anything v2ï¼‰å¯¹ä¸äº‹ä»¶æ•°æ®åŒæ­¥çš„RGBå›¾åƒè¿›è¡Œæ·±åº¦é¢„æµ‹ï¼Œç”Ÿæˆäº‹ä»¶æ•°æ®çš„ä»£ç†æ·±åº¦æ ‡ç­¾ã€‚2) äº‹ä»¶æ·±åº¦ä¼°è®¡æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼šä½¿ç”¨ç”Ÿæˆçš„ä»£ç†æ ‡ç­¾è®­ç»ƒåŸºäºäº‹ä»¶æ•°æ®çš„æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ˜¯ç›´æ¥ä½¿ç”¨VFMè¿›è¡Œå¾®è°ƒï¼Œä¹Ÿå¯ä»¥æ˜¯åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„å®šåˆ¶æ¶æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†è·¨æ¨¡æ€è’¸é¦èŒƒå¼ï¼Œå°†RGBå›¾åƒçš„æ·±åº¦ä¿¡æ¯è¿ç§»åˆ°äº‹ä»¶æ•°æ®ä¸Šï¼Œä»è€Œè§£å†³äº†äº‹ä»¶æ•°æ®æ·±åº¦ä¼°è®¡ä¸­æ•°æ®æ ‡æ³¨çš„ç“¶é¢ˆé—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€æ˜‚è´µçš„æ·±åº¦ä¼ æ„Ÿå™¨æˆ–å¤æ‚çš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Depth Anything v2ç­‰é¢„è®­ç»ƒVFMä½œä¸ºæ·±åº¦é¢„æµ‹å™¨ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚2) æå‡ºåŸºäºRNNçš„äº‹ä»¶æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œä»¥é€‚åº”äº‹ä»¶æ•°æ®çš„æ—¶åºç‰¹æ€§ã€‚3) ä½¿ç”¨ç©ºé—´å¯¹é½çš„RGBå›¾åƒå’Œäº‹ä»¶æ•°æ®ï¼Œç¡®ä¿ä»£ç†æ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚4) æŸå¤±å‡½æ•°é‡‡ç”¨å¸¸ç”¨çš„æ·±åº¦ä¼°è®¡æŸå¤±å‡½æ•°ï¼Œå¦‚L1æŸå¤±æˆ–HuberæŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸å®Œå…¨ç›‘ç£çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦çœŸå®æ·±åº¦æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è¾¾åˆ°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åŸºäºVFMçš„æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†state-of-the-artçš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥è·¨æ¨¡æ€è’¸é¦èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨å…‰ç…§æ¡ä»¶æ¶åŠ£æˆ–é«˜é€Ÿè¿åŠ¨åœºæ™¯ä¸‹ï¼Œäº‹ä»¶ç›¸æœºèƒ½å¤Ÿæä¾›æ›´å¯é çš„æ·±åº¦ä¿¡æ¯ã€‚è¯¥æ–¹æ³•é™ä½äº†äº‹ä»¶ç›¸æœºæ·±åº¦ä¼°è®¡çš„æ•°æ®æ ‡æ³¨æˆæœ¬ï¼Œæœ‰æœ›åŠ é€Ÿäº‹ä»¶ç›¸æœºåœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.

