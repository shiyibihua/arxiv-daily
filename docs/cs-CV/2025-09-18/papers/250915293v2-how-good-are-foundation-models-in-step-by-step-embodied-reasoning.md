---
layout: default
title: How Good are Foundation Models in Step-by-Step Embodied Reasoning?
---

# How Good are Foundation Models in Step-by-Step Embodied Reasoning?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15293" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15293v2</a>
  <a href="https://arxiv.org/pdf/2509.15293.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15293v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15293v2', 'How Good are Foundation Models in Step-by-Step Embodied Reasoning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dinura Dissanayake, Ahmed Heakl, Omkar Thawakar, Noor Ahsan, Ritesh Thawkar, Ketan More, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Ivan Laptev, Fahad Shahbaz Khan, Salman Khan

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: Project page: https://mbzuai-oryx.github.io/FoMER-Bench/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFoMERåŸºå‡†ï¼Œè¯„ä¼°å…·èº«ç¯å¢ƒä¸­åŸºç¡€æ¨¡å‹é€æ­¥æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `å¤§å‹å¤šæ¨¡æ€æ¨¡å‹` `æ¨ç†èƒ½åŠ›è¯„ä¼°` `æœºå™¨äºº` `åŸºå‡†æµ‹è¯•` `ç‰©ç†çº¦æŸ` `å®‰å…¨æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LMMsåœ¨å…·èº«ä»»åŠ¡ä¸­è¿›è¡Œç»“æ„åŒ–æ¨ç†çš„èƒ½åŠ›æœ‰å¾…æ¢ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨æ€§å’Œç©ºé—´è¿è´¯æ€§æ–¹é¢ã€‚
2. æå‡ºFoMERåŸºå‡†ï¼ŒåŒ…å«å¤šç§å…·èº«ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°LMMsåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€æ­¥æ¨ç†èƒ½åŠ›ã€‚
3. é€šè¿‡å®éªŒåˆ†æäº†å¤šä¸ªLMMsåœ¨FoMERåŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«æ™ºèƒ½ä½“åœ¨ç‰©ç†ä¸–ç•Œä¸­æ“ä½œæ—¶ï¼Œå¿…é¡»åšå‡ºæœ‰æ•ˆã€å®‰å…¨ã€ç©ºé—´è¿è´¯ä¸”åŸºäºä¸Šä¸‹æ–‡çš„å†³ç­–ã€‚å°½ç®¡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)åœ¨è§†è§‰ç†è§£å’Œè¯­è¨€ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œå…·èº«ä»»åŠ¡ä¸­æ‰§è¡Œç»“æ„åŒ–æ¨ç†çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨äº†è§£åŸºç¡€æ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­æ‰§è¡Œé€æ­¥æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºç¡€æ¨¡å‹å…·èº«æ¨ç†(FoMER)åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LMMsåœ¨å¤æ‚å…·èº«å†³ç­–åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æ¶µç›–äº†ä¸€ç³»åˆ—ä¸åŒçš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“è§£é‡Šå¤šæ¨¡æ€è§‚å¯Ÿï¼Œæ¨ç†ç‰©ç†çº¦æŸå’Œå®‰å…¨æ€§ï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€ç”Ÿæˆæœ‰æ•ˆçš„ä¸‹ä¸€æ­¥åŠ¨ä½œã€‚æˆ‘ä»¬æå‡ºäº†(i)ä¸€ä¸ªå¤§è§„æ¨¡ã€ç²¾å¿ƒç­–åˆ’çš„å…·èº«æ¨ç†ä»»åŠ¡å¥—ä»¶ï¼Œ(ii)ä¸€ç§æ–°é¢–çš„è¯„ä¼°æ¡†æ¶ï¼Œå°†æ„ŸçŸ¥åŸºç¡€ä¸åŠ¨ä½œæ¨ç†åˆ†ç¦»ï¼Œä»¥åŠ(iii)åœ¨è¿™ç§è®¾ç½®ä¸‹å¯¹å‡ ç§é¢†å…ˆLMMsçš„å®è¯åˆ†æã€‚æˆ‘ä»¬çš„åŸºå‡†åŒ…æ‹¬è¶…è¿‡1.1kä¸ªæ ·æœ¬ï¼Œæ¶µç›–10ä¸ªä»»åŠ¡å’Œ8ä¸ªå…·èº«ï¼Œæ¶‰åŠä¸‰ç§ä¸åŒçš„æœºå™¨äººç±»å‹ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†LMMsåœ¨å…·èº«æ¨ç†ä¸­çš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼ŒæŒ‡å‡ºäº†æœºå™¨äººæ™ºèƒ½æœªæ¥ç ”ç©¶çš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å°†å…¬å¼€æä¾›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å…·èº«ç¯å¢ƒä¸­è¿›è¡Œé€æ­¥æ¨ç†èƒ½åŠ›è¯„ä¼°çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯„ä¼°LMMsåœ¨ç‰©ç†çº¦æŸã€å®‰å…¨æ€§å’Œç©ºé—´è¿è´¯æ€§ç­‰æ–¹é¢çš„æ¨ç†èƒ½åŠ›ï¼Œç¼ºä¹ä¸€ä¸ªå…¨é¢ä¸”ç»†ç²’åº¦çš„è¯„ä¼°åŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LMMsåœ¨å…·èº«ç¯å¢ƒä¸­æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é›†FoMERã€‚é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—éœ€è¦æ™ºèƒ½ä½“ç†è§£å¤šæ¨¡æ€ä¿¡æ¯ã€æ¨ç†ç‰©ç†è§„åˆ™å¹¶ç”Ÿæˆåˆç†åŠ¨ä½œçš„ä»»åŠ¡ï¼Œæ¥è€ƒå¯ŸLMMsçš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°LMMsåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFoMERåŸºå‡†åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1) å¤§è§„æ¨¡çš„å…·èº«æ¨ç†ä»»åŠ¡å¥—ä»¶ï¼Œæ¶µç›–10ä¸ªä»»åŠ¡å’Œ8ä¸ªå…·èº«ï¼Œæ¶‰åŠä¸‰ç§ä¸åŒçš„æœºå™¨äººç±»å‹ï¼›2) æ–°é¢–çš„è¯„ä¼°æ¡†æ¶ï¼Œå°†æ„ŸçŸ¥åŸºç¡€ä¸åŠ¨ä½œæ¨ç†åˆ†ç¦»ï¼Œä»¥ä¾¿æ›´ç²¾ç¡®åœ°è¯„ä¼°LMMsçš„æ¨ç†èƒ½åŠ›ï¼›3) è¯¦ç»†çš„æ­¥éª¤çº§æ¨ç†æ•°æ®ï¼ŒåŒ…å«è¶…è¿‡1.1kä¸ªæ ·æœ¬ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼ŒLMMsæ¥æ”¶å¤šæ¨¡æ€è¾“å…¥ï¼ˆä¾‹å¦‚å›¾åƒã€æ–‡æœ¬ï¼‰ï¼Œç„¶åç”Ÿæˆä¸‹ä¸€æ­¥åŠ¨ä½œçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œæœ€åé€šè¿‡è¯„ä¼°æ¡†æ¶åˆ¤æ–­åŠ¨ä½œçš„åˆç†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†FoMERåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°LMMsåœ¨å…·èº«ç¯å¢ƒä¸­æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„åŸºå‡†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFoMERæ›´åŠ å…³æ³¨ç‰©ç†çº¦æŸã€å®‰å…¨æ€§å’Œç©ºé—´è¿è´¯æ€§ç­‰å› ç´ ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°LMMsåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè¯„ä¼°æ¡†æ¶å°†æ„ŸçŸ¥åŸºç¡€ä¸åŠ¨ä½œæ¨ç†åˆ†ç¦»ï¼Œä»è€Œå¯ä»¥æ›´ç²¾ç¡®åœ°è¯„ä¼°LMMsçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šFoMERåŸºå‡†ä¸­çš„ä»»åŠ¡è®¾è®¡æ¶µç›–äº†å¤šç§ä¸åŒçš„åœºæ™¯å’ŒæŒ‘æˆ˜ï¼Œä¾‹å¦‚å¯¼èˆªã€æ“ä½œã€è§„åˆ’ç­‰ã€‚æ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«è¯¦ç»†çš„æ­¥éª¤çº§æ¨ç†æ•°æ®ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå’Œè¯„ä¼°LMMsã€‚è¯„ä¼°æ¡†æ¶é‡‡ç”¨å¤šç§æŒ‡æ ‡æ¥è¡¡é‡LMMsçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŠ¨ä½œçš„æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€ç©ºé—´è¿è´¯æ€§ç­‰ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©å–å†³äºæ‰€ä½¿ç”¨çš„LMMsã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„LMMsåœ¨FoMERåŸºå‡†ä¸Šè¡¨ç°å‡ºä¸€å®šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„ç‰©ç†çº¦æŸå’Œå®‰å…¨é—®é¢˜æ—¶ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LMMsåœ¨å…·èº«æ¨ç†æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„å…³é”®æ–¹å‘ï¼Œä¾‹å¦‚å¦‚ä½•æé«˜LMMså¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡LMMsåœ¨å…·èº«ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£ç¯å¢ƒã€åšå‡ºåˆç†çš„å†³ç­–ï¼Œä»è€Œå®ç°æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„äººæœºåä½œã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æœºå™¨äººæ™ºèƒ½çš„å‘å±•ï¼Œä½¿å…¶åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å‘æŒ¥ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied agents operating in the physical world must make decisions that are not only effective but also safe, spatially coherent, and grounded in context. While recent advances in large multimodal models (LMMs) have shown promising capabilities in visual understanding and language generation, their ability to perform structured reasoning for real-world embodied tasks remains underexplored. In this work, we aim to understand how well foundation models can perform step-by-step reasoning in embodied environments. To this end, we propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to evaluate the reasoning capabilities of LMMs in complex embodied decision-making scenarios. Our benchmark spans a diverse set of tasks that require agents to interpret multimodal observations, reason about physical constraints and safety, and generate valid next actions in natural language. We present (i) a large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation framework that disentangles perceptual grounding from action reasoning, and (iii) empirical analysis of several leading LMMs under this setting. Our benchmark includes over 1.1k samples with detailed step-by-step reasoning across 10 tasks and 8 embodiments, covering three different robot types. Our results highlight both the potential and current limitations of LMMs in embodied reasoning, pointing towards key challenges and opportunities for future research in robot intelligence. Our data and code will be made publicly available.

