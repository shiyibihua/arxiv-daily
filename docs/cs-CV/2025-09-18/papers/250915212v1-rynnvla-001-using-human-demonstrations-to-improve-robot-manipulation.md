---
layout: default
title: RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation
---

# RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15212" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15212v1</a>
  <a href="https://arxiv.org/pdf/2509.15212.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15212v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15212v1', 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: GitHub Project: https://github.com/alibaba-damo-academy/RynnVLA-001

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RynnVLA-001ï¼šåˆ©ç”¨äººç±»æ¼”ç¤ºæå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œæå‡ºåŒé˜¶æ®µé¢„è®­ç»ƒVLAæ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `è§†é¢‘ç”Ÿæˆ` `é¢„è®­ç»ƒ` `äººç±»æ¼”ç¤ºå­¦ä¹ ` `è½¨è¿¹é¢„æµ‹` `å˜åˆ†è‡ªç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­é¢ä¸´åŠ¨ä½œè¡¨ç¤ºå¤æ‚å’Œæ³›åŒ–æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. RynnVLA-001é€šè¿‡åŒé˜¶æ®µé¢„è®­ç»ƒï¼Œç»“åˆè§†é¢‘ç”Ÿæˆå’Œè½¨è¿¹é¢„æµ‹ï¼Œæå‡æ¨¡å‹å¯¹åŠ¨ä½œçš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRynnVLA-001åœ¨ä¸‹æ¸¸æœºå™¨äººæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†é¢„è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†RynnVLA-001ï¼Œä¸€ä¸ªåŸºäºå¤§è§„æ¨¡äººç±»æ¼”ç¤ºè§†é¢‘ç”Ÿæˆé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆé¢„è®­ç»ƒï¼Œåœ¨1200ä¸‡ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ“ä½œè§†é¢‘ä¸Šè®­ç»ƒä¸€ä¸ªå›¾åƒåˆ°è§†é¢‘æ¨¡å‹ï¼Œä»¥é¢„æµ‹åœ¨åˆå§‹å¸§å’Œè¯­è¨€æŒ‡ä»¤æ¡ä»¶ä¸‹çš„æœªæ¥å¸§ã€‚ç¬¬äºŒé˜¶æ®µï¼Œä»¥äººä¸ºä¸­å¿ƒçš„è½¨è¿¹æ„ŸçŸ¥å»ºæ¨¡ï¼Œé€šè¿‡è”åˆé¢„æµ‹æœªæ¥å…³é”®ç‚¹è½¨è¿¹æ¥æ‰©å±•è¿™ä¸€æ–¹æ³•ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†è§†è§‰å¸§é¢„æµ‹ä¸åŠ¨ä½œé¢„æµ‹è”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºåŠ¨ä½œè¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ActionVAEï¼Œä¸€ä¸ªå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå®ƒå°†åŠ¨ä½œåºåˆ—å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨åµŒå…¥ï¼Œä»è€Œé™ä½äº†VLAè¾“å‡ºç©ºé—´çš„å¤æ‚æ€§ã€‚åœ¨ç›¸åŒçš„ä¸‹æ¸¸æœºå™¨äººæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼ŒRynnVLA-001å®ç°äº†ä¼˜äºæœ€å…ˆè¿›åŸºçº¿çš„æ€§èƒ½ï¼Œè¡¨æ˜æ‰€æå‡ºçš„é¢„è®­ç»ƒç­–ç•¥ä¸ºVLAæ¨¡å‹æä¾›äº†æ›´æœ‰æ•ˆçš„åˆå§‹åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨äººç±»æ¼”ç¤ºæ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨ä½œè¡¨ç¤ºå’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨ç“¶é¢ˆã€‚ç›´æ¥ä»åƒç´ ç©ºé—´é¢„æµ‹åŠ¨ä½œå¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥é€‚åº”ä¸åŒçš„æ“ä½œåœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è§„æ¨¡äººç±»æ¼”ç¤ºè§†é¢‘è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¹‹é—´çš„å…³è”ã€‚é€šè¿‡ä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼Œé¦–å…ˆå­¦ä¹ ä»å›¾åƒå’Œè¯­è¨€åˆ°æœªæ¥è§†é¢‘å¸§çš„ç”Ÿæˆï¼Œç„¶åå­¦ä¹ å…³é”®ç‚¹è½¨è¿¹çš„é¢„æµ‹ï¼Œä»è€Œå°†è§†è§‰ä¿¡æ¯ä¸åŠ¨ä½œä¿¡æ¯æœ‰æ•ˆæ¡¥æ¥ã€‚ActionVAEçš„å¼•å…¥è¿›ä¸€æ­¥å‹ç¼©åŠ¨ä½œç©ºé—´ï¼Œé™ä½å­¦ä¹ éš¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRynnVLA-001çš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„é¢„è®­ç»ƒé˜¶æ®µï¼š1) Ego-Centric Video Generative Pretrainingï¼šè®­ç»ƒä¸€ä¸ªImage-to-Videoæ¨¡å‹ï¼Œè¾“å…¥ä¸ºåˆå§‹å¸§å’Œè¯­è¨€æŒ‡ä»¤ï¼Œè¾“å‡ºä¸ºé¢„æµ‹çš„æœªæ¥å¸§ã€‚2) Human-Centric Trajectory-Aware Modelingï¼šåœ¨ç¬¬ä¸€é˜¶æ®µçš„åŸºç¡€ä¸Šï¼Œè”åˆé¢„æµ‹æœªæ¥å…³é”®ç‚¹è½¨è¿¹ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…å«ä¸€ä¸ªActionVAEæ¨¡å—ï¼Œç”¨äºå°†åŠ¨ä½œåºåˆ—å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨åµŒå…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šä¸»è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†åŒé˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆç»“åˆäº†è§†é¢‘ç”Ÿæˆå’Œè½¨è¿¹é¢„æµ‹ï¼Œæå‡äº†VLAæ¨¡å‹çš„æ€§èƒ½ã€‚2) å¼•å…¥äº†ActionVAEï¼Œé€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨å‹ç¼©åŠ¨ä½œç©ºé—´ï¼Œé™ä½äº†å­¦ä¹ çš„å¤æ‚æ€§ã€‚3) åˆ©ç”¨å¤§è§„æ¨¡ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äººç±»æ¼”ç¤ºè§†é¢‘è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ äººç±»çš„æ“ä½œè¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šEgo-Centric Video Generative Pretrainingé˜¶æ®µä½¿ç”¨Transformeræ¶æ„è¿›è¡Œè§†é¢‘ç”Ÿæˆï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬åƒç´ çº§åˆ«çš„é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ã€‚Human-Centric Trajectory-Aware Modelingé˜¶æ®µï¼Œå…³é”®ç‚¹è½¨è¿¹é¢„æµ‹é‡‡ç”¨å›å½’æŸå¤±ã€‚ActionVAEä½¿ç”¨æ ‡å‡†çš„å˜åˆ†è‡ªç¼–ç å™¨ç»“æ„ï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’ŒKLæ•£åº¦æŸå¤±ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†æè¿°ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡å…¨æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RynnVLA-001åœ¨ä¸‹æ¸¸æœºå™¨äººæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRynnVLA-001å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„é¢„è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡å…¨æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººç­‰ã€‚é€šè¿‡å­¦ä¹ äººç±»çš„æ¼”ç¤ºï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£ä»»åŠ¡æŒ‡ä»¤ï¼Œå¹¶æ‰§è¡Œå¤æ‚çš„åŠ¨ä½œã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæå‡æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.

