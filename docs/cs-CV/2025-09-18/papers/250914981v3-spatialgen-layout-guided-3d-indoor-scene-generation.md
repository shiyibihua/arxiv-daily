---
layout: default
title: SPATIALGEN: Layout-guided 3D Indoor Scene Generation
---

# SPATIALGEN: Layout-guided 3D Indoor Scene Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14981" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14981v3</a>
  <a href="https://arxiv.org/pdf/2509.14981.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14981v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14981v3', 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-26)

**å¤‡æ³¨**: 3D scene generation; diffusion model; Scene reconstruction and understanding

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SpatialGenï¼šå¸ƒå±€å¼•å¯¼çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯ç”Ÿæˆ` `å®¤å†…åœºæ™¯` `æ‰©æ•£æ¨¡å‹` `å¤šè§†è§’å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Då®¤å†…åœºæ™¯ç”Ÿæˆæ–¹æ³•éš¾ä»¥å…¼é¡¾è§†è§‰è´¨é‡ã€å¤šæ ·æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œç”¨æˆ·æ§åˆ¶ï¼Œç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†æ˜¯ä¸»è¦ç“¶é¢ˆã€‚
2. SpatialGenæå‡ºä¸€ç§å¤šè§†è§’å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨3Då¸ƒå±€å’Œå‚è€ƒå›¾åƒï¼Œåˆæˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä¿æŒç©ºé—´ä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSpatialGenç”Ÿæˆçš„3Då®¤å†…åœºæ™¯åœ¨è´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¼€æºæ•°æ®å’Œæ¨¡å‹ä»¥ä¿ƒè¿›ç ”ç©¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ›å»ºé«˜ä¿çœŸ3Då®¤å†…ç¯å¢ƒæ¨¡å‹å¯¹äºè®¾è®¡ã€è™šæ‹Ÿç°å®å’Œæœºå™¨äººç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨3Då»ºæ¨¡ä»ç„¶è€—æ—¶ä¸”è´¹åŠ›ã€‚è™½ç„¶ç”Ÿæˆå¼AIçš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†è‡ªåŠ¨åŒ–åœºæ™¯åˆæˆï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡è§†è§‰è´¨é‡ã€å¤šæ ·æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œç”¨æˆ·æ§åˆ¶æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸€ä¸ªä¸»è¦çš„ç“¶é¢ˆæ˜¯ç¼ºä¹é’ˆå¯¹æ­¤ä»»åŠ¡çš„å¤§è§„æ¨¡ã€é«˜è´¨é‡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«12,328ä¸ªç»“æ„åŒ–æ ‡æ³¨åœºæ™¯ï¼Œ57,440ä¸ªæˆ¿é—´å’Œ470ä¸‡ä¸ªé€¼çœŸçš„2Dæ¸²æŸ“ã€‚åˆ©ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†SpatialGenï¼Œä¸€ç§æ–°é¢–çš„å¤šè§†è§’å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸä¸”è¯­ä¹‰ä¸€è‡´çš„3Då®¤å†…åœºæ™¯ã€‚ç»™å®š3Då¸ƒå±€å’Œå‚è€ƒå›¾åƒï¼ˆæ¥è‡ªæ–‡æœ¬æç¤ºï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä»ä»»æ„è§†ç‚¹åˆæˆå¤–è§‚ï¼ˆå½©è‰²å›¾åƒï¼‰ã€å‡ ä½•ï¼ˆåœºæ™¯åæ ‡å›¾ï¼‰å’Œè¯­ä¹‰ï¼ˆè¯­ä¹‰åˆ†å‰²å›¾ï¼‰ï¼ŒåŒæ—¶ä¿æŒè·¨æ¨¡æ€çš„ç©ºé—´ä¸€è‡´æ€§ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒSpatialGenå§‹ç»ˆç”Ÿæˆä¼˜äºå…ˆå‰æ–¹æ³•çš„ç»“æœã€‚æˆ‘ä»¬å°†å¼€æºæˆ‘ä»¬çš„æ•°æ®å’Œæ¨¡å‹ï¼Œä»¥å¢å¼ºç¤¾åŒºèƒ½åŠ›å¹¶æ¨è¿›å®¤å†…åœºæ™¯ç†è§£å’Œç”Ÿæˆé¢†åŸŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Då®¤å†…åœºæ™¯ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”è¯­ä¹‰ä¸€è‡´çš„åœºæ™¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ‰‹åŠ¨å»ºæ¨¡è€—æ—¶è´¹åŠ›ï¼Œè€Œç°æœ‰çš„è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•éš¾ä»¥å¹³è¡¡è§†è§‰è´¨é‡ã€å¤šæ ·æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œç”¨æˆ·æ§åˆ¶ã€‚ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†æ˜¯åˆ¶çº¦è¿™äº›æ–¹æ³•æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSpatialGençš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç»“åˆ3Då¸ƒå±€ä½œä¸ºç©ºé—´çº¦æŸï¼Œå¹¶å¼•å…¥å‚è€ƒå›¾åƒï¼ˆæˆ–æ–‡æœ¬æç¤ºï¼‰ä½œä¸ºå¤–è§‚å¼•å¯¼ï¼Œä»è€Œç”Ÿæˆé€¼çœŸä¸”è¯­ä¹‰ä¸€è‡´çš„3Då®¤å†…åœºæ™¯ã€‚é€šè¿‡å¤šè§†è§’åˆæˆï¼Œä¿è¯åœºæ™¯åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpatialGené‡‡ç”¨å¤šè§†è§’å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰è¾“å…¥3Då¸ƒå±€å’Œå‚è€ƒå›¾åƒï¼ˆæˆ–æ–‡æœ¬æç¤ºï¼‰ï¼›2ï¼‰åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»ä»»æ„è§†ç‚¹ç”Ÿæˆå¤–è§‚ï¼ˆå½©è‰²å›¾åƒï¼‰ã€å‡ ä½•ï¼ˆåœºæ™¯åæ ‡å›¾ï¼‰å’Œè¯­ä¹‰ï¼ˆè¯­ä¹‰åˆ†å‰²å›¾ï¼‰ï¼›3ï¼‰é€šè¿‡æŸå¤±å‡½æ•°çº¦æŸï¼Œä¿è¯ä¸åŒè§†è§’ä¸‹ç”Ÿæˆç»“æœçš„ç©ºé—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«å¸ƒå±€ç¼–ç å™¨ã€å›¾åƒ/æ–‡æœ¬ç¼–ç å™¨ã€å¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œç©ºé—´ä¸€è‡´æ€§çº¦æŸæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šSpatialGençš„å…³é”®åˆ›æ–°åœ¨äºï¼š1ï¼‰æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„3Då®¤å†…åœºæ™¯åˆæˆæ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†å……è¶³çš„æ•°æ®æ”¯æŒï¼›2ï¼‰è®¾è®¡äº†ä¸€ç§å¤šè§†è§’å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä¿è¯ç©ºé—´ä¸€è‡´æ€§ï¼›3ï¼‰ç»“åˆ3Då¸ƒå±€å’Œå‚è€ƒå›¾åƒ/æ–‡æœ¬æç¤ºï¼Œå®ç°äº†å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æœ‰æ•ˆæ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šSpatialGençš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨Transformerç½‘ç»œä½œä¸ºå¸ƒå±€ç¼–ç å™¨å’Œå›¾åƒ/æ–‡æœ¬ç¼–ç å™¨ï¼Œæå–å¸ƒå±€å’Œå¤–è§‚ç‰¹å¾ï¼›2ï¼‰é‡‡ç”¨U-Netç»“æ„çš„æ‰©æ•£æ¨¡å‹ï¼Œé€æ­¥å»å™ªç”Ÿæˆå›¾åƒã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼›3ï¼‰è®¾è®¡äº†ç©ºé—´ä¸€è‡´æ€§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬å…‰åº¦ä¸€è‡´æ€§æŸå¤±ã€å‡ ä½•ä¸€è‡´æ€§æŸå¤±å’Œè¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥çº¦æŸä¸åŒè§†è§’ä¸‹ç”Ÿæˆç»“æœçš„ä¸€è‡´æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SpatialGenåœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚é€šè¿‡ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼ŒSpatialGenåœ¨è§†è§‰è´¨é‡ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œç©ºé—´ä¸€è‡´æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚è®ºæ–‡å¼€æºäº†æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SpatialGenåœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ¸¸æˆå¼€å‘ã€å®¤å†…è®¾è®¡ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºå¿«é€Ÿç”Ÿæˆå„ç§é£æ ¼çš„å®¤å†…åœºæ™¯ï¼Œé™ä½å»ºæ¨¡æˆæœ¬ï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºè®­ç»ƒæœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å­¦ä¹ å¯¼èˆªå’Œäº¤äº’ï¼Œä»è€Œæé«˜å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.

