---
layout: default
title: Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning
---

# Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15250" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15250v2</a>
  <a href="https://arxiv.org/pdf/2509.15250.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15250v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15250v2', 'Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenda Qin, Andrea Burns, Bryan A. Plummer, Margrit Betke

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: Accepted to EMNLP 2025. Data and code to be released at https://github.com/wdqin/VLN-NAP

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¼èˆªæ„ŸçŸ¥å‰ªæ(NAP)ï¼Œé€šè¿‡æ— ç›‘ç£å¤šæ¨¡æ€tokenå‰ªææå‡è§†è§‰è¯­è¨€å¯¼èˆªæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `Tokenå‰ªæ` `æ¨¡å‹å‹ç¼©` `å¯¼èˆªæ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLNæ¨¡å‹è®¡ç®—æˆæœ¬é«˜ï¼Œtokenå‰ªææ–¹æ³•å¿½ç•¥äº†å¯¼èˆªä»»åŠ¡çš„ç‰¹æ®Šæ€§ï¼Œæ˜“é€ æˆä¿¡æ¯æŸå¤±å’Œå¯¼èˆªè·¯å¾„å¢åŠ ã€‚
2. æå‡ºå¯¼èˆªæ„ŸçŸ¥å‰ªæ(NAP)ï¼Œé€šè¿‡é¢„è¿‡æ»¤tokenåŒºåˆ†å‰æ™¯å’ŒèƒŒæ™¯ï¼Œå¹¶åˆ©ç”¨LLMæå–å¯¼èˆªç›¸å…³æŒ‡ä»¤ï¼Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNAPåœ¨æ ‡å‡†VLNåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨èŠ‚çœè¶…è¿‡50% FLOPSçš„åŒæ—¶ï¼Œä¿æŒäº†æ›´é«˜çš„æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨¡å‹åœ¨è§†è§‰è¯­è¨€å¯¼èˆª(VLN)ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿è¡Œæˆæœ¬é«˜æ˜‚ã€‚Tokenå‰ªæé€šè¿‡å‡å°‘æ¨¡å‹è¾“å…¥å¤§å°ï¼Œåœ¨æœ€å°åŒ–æ€§èƒ½æŸå¤±çš„åŒæ—¶ï¼Œä¸ºæ•ˆç‡æä¾›äº†æœ‰å¸å¼•åŠ›çš„æŠ˜è¡·æ–¹æ¡ˆï¼Œä½†å…ˆå‰çš„å·¥ä½œå¿½ç•¥äº†VLNç‰¹æœ‰çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå‰ªæé€ æˆçš„ä¿¡æ¯ä¸¢å¤±å®é™…ä¸Šä¼šå› æ›´é•¿çš„è¡Œèµ°è·¯å¾„è€Œå¢åŠ è®¡ç®—æˆæœ¬ã€‚å› æ­¤ï¼Œæ— æ³•è¯†åˆ«æ— ä¿¡æ¯tokenä¼šç ´åå‰ªææœ¬åº”å¸¦æ¥çš„æ•ˆç‡æå‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¼èˆªæ„ŸçŸ¥å‰ªæ(NAP)ï¼Œå®ƒåˆ©ç”¨å¯¼èˆªç‰¹å®šçš„ç‰¹å¾ï¼Œé€šè¿‡å°†tokené¢„è¿‡æ»¤åˆ°å‰æ™¯å’ŒèƒŒæ™¯ä¸­æ¥ç®€åŒ–å‰ªæè¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œå›¾åƒè§†å›¾æ ¹æ®æ™ºèƒ½ä½“æ˜¯å¦å¯ä»¥åœ¨è¯¥æ–¹å‘ä¸Šå¯¼èˆªè¿›è¡Œè¿‡æ»¤ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–å¯¼èˆªç›¸å…³çš„æŒ‡ä»¤ã€‚è¿‡æ»¤åï¼Œæˆ‘ä»¬ä¸“æ³¨äºèƒŒæ™¯tokençš„å‰ªæï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†è¿›ä¸€æ­¥é¿å…å¯¼èˆªé•¿åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬é€šè¿‡ç§»é™¤ä½é‡è¦æ€§çš„å¯¼èˆªèŠ‚ç‚¹æ¥é˜»æ­¢å›æº¯ã€‚åœ¨æ ‡å‡†VLNåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNAPæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å·¥ä½œï¼Œåœ¨èŠ‚çœè¶…è¿‡50% FLOPSçš„åŒæ—¶ï¼Œä¿æŒäº†æ›´é«˜çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡æ—¨åœ¨è®©æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨çœŸå®ç¯å¢ƒä¸­å¯¼èˆªã€‚ç°æœ‰çš„å¤§å‹æ¨¡å‹è™½ç„¶æ€§èƒ½ä¼˜å¼‚ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²ã€‚Tokenå‰ªææ˜¯ä¸€ç§é™ä½è®¡ç®—æˆæœ¬çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•æ²¡æœ‰å……åˆ†è€ƒè™‘VLNä»»åŠ¡çš„ç‰¹æ®Šæ€§ï¼Œä¾‹å¦‚ï¼Œä¸åŠ é€‰æ‹©çš„å‰ªæå¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œè¿«ä½¿æ™ºèƒ½ä½“èµ°æ›´é•¿çš„å¼¯è·¯ï¼Œåè€Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆåœ°è¯†åˆ«å¹¶å»é™¤VLNä»»åŠ¡ä¸­ä¸é‡è¦çš„tokenï¼ŒåŒæ—¶é¿å…å¯¼èˆªè·¯å¾„çš„å¢åŠ ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯¼èˆªä»»åŠ¡çš„å›ºæœ‰ç‰¹æ€§ï¼Œå¯¹tokenè¿›è¡Œé¢„è¿‡æ»¤ï¼ŒåŒºåˆ†å‰æ™¯ï¼ˆå¯¼èˆªç›¸å…³ï¼‰å’ŒèƒŒæ™¯ï¼ˆå¯¼èˆªä¸ç›¸å…³ï¼‰tokenï¼Œç„¶åä¸»è¦å¯¹èƒŒæ™¯tokenè¿›è¡Œå‰ªæï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘ä¿¡æ¯æŸå¤±ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…æ™ºèƒ½ä½“å›æº¯ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é˜»æ­¢å›æº¯çš„æœºåˆ¶ï¼Œå³ç§»é™¤ä½é‡è¦æ€§çš„å¯¼èˆªèŠ‚ç‚¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¿è¯å¯¼èˆªæˆåŠŸç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNAPæ–¹æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå–å¯¼èˆªç›¸å…³çš„æŒ‡ä»¤ä¿¡æ¯ï¼›2) æ ¹æ®æ™ºèƒ½ä½“æ˜¯å¦å¯ä»¥åœ¨è¯¥æ–¹å‘ä¸Šå¯¼èˆªï¼Œå¯¹å›¾åƒè§†å›¾è¿›è¡Œè¿‡æ»¤ï¼ŒåŒºåˆ†å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸï¼›3) å¯¹èƒŒæ™¯åŒºåŸŸçš„tokenè¿›è¡Œå‰ªæï¼›4) ç§»é™¤ä½é‡è¦æ€§çš„å¯¼èˆªèŠ‚ç‚¹ï¼Œä»¥é˜»æ­¢å›æº¯ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡å¯¼èˆªæ„ŸçŸ¥çš„é¢„å¤„ç†å’Œå‰ªæç­–ç•¥ï¼Œæé«˜VLNä»»åŠ¡çš„æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šNAPæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¼èˆªæ„ŸçŸ¥çš„tokené¢„è¿‡æ»¤ç­–ç•¥ã€‚ä¸ä»¥å¾€çš„tokenå‰ªææ–¹æ³•ä¸åŒï¼ŒNAPä¸æ˜¯ç›²ç›®åœ°å¯¹æ‰€æœ‰tokenè¿›è¡Œå‰ªæï¼Œè€Œæ˜¯é¦–å…ˆåˆ©ç”¨å¯¼èˆªä»»åŠ¡çš„ç‰¹æ€§ï¼Œå°†tokenåˆ†ä¸ºå‰æ™¯å’ŒèƒŒæ™¯ï¼Œç„¶åä¸»è¦å¯¹èƒŒæ™¯tokenè¿›è¡Œå‰ªæã€‚è¿™ç§ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘ä¿¡æ¯æŸå¤±ï¼Œé¿å…æ™ºèƒ½ä½“èµ°å¼¯è·¯ï¼Œä»è€Œæé«˜å¯¼èˆªæ•ˆç‡ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMæå–å¯¼èˆªç›¸å…³æŒ‡ä»¤ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£æŒ‡ä»¤ï¼Œæé«˜å¯¼èˆªçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šNAPçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LLMï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰æå–å¯¼èˆªæŒ‡ä»¤ï¼Œæå–æ–¹å¼æœªçŸ¥ï¼›2) åŸºäºå¯¼èˆªå¯è¡Œæ€§çš„å›¾åƒè§†å›¾è¿‡æ»¤æœºåˆ¶ï¼Œå…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼›3) åŸºäºtokené‡è¦æ€§çš„å‰ªæç­–ç•¥ï¼Œå…·ä½“é‡è¦æ€§è¯„ä¼°æ–¹æ³•æœªçŸ¥ï¼›4) ç§»é™¤ä½é‡è¦æ€§å¯¼èˆªèŠ‚ç‚¹çš„å…·ä½“ç®—æ³•æœªçŸ¥ã€‚è®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºè¿™äº›å…³é”®è®¾è®¡çš„å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NAPæ–¹æ³•åœ¨æ ‡å‡†VLNåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNAPåœ¨ä¿æŒè¾ƒé«˜æˆåŠŸç‡çš„åŒæ—¶ï¼Œèƒ½å¤ŸèŠ‚çœè¶…è¿‡50%çš„FLOPSã€‚ä¸ç°æœ‰tokenå‰ªææ–¹æ³•ç›¸æ¯”ï¼ŒNAPèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¸ºVLNä»»åŠ¡çš„å®é™…åº”ç”¨æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬ï¼Œå¯ä»¥ä½¿VLNæ¨¡å‹åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œä¾‹å¦‚ç§»åŠ¨æœºå™¨äººå’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æé«˜å¯¼èˆªçš„æ•ˆç‡å’Œå¯é æ€§ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„å¯¼èˆªä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨VLNæŠ€æœ¯åœ¨æ›´å¹¿æ³›çš„å®é™…åœºæ™¯ä¸­åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.

