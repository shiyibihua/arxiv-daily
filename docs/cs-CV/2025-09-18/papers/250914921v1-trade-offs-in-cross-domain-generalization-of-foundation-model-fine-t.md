---
layout: default
title: Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications
---

# Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14921" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.14921v1</a>
  <a href="https://arxiv.org/pdf/2509.14921.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14921v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14921v1', 'Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tahar Chettaoui, Naser Damer, Fadi Boutros

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-18

**Â§áÊ≥®**: Accepted at the IEEE International Joint Conference on Biometrics 2025 (IJCB 2025)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Á†îÁ©∂CLIPÂæÆË∞ÉÂú®ÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°‰∏≠Ê≥õÂåñËÉΩÂäõ‰∏éËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ÁöÑÊùÉË°°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁîüÁâ©ÁâπÂæÅËØÜÂà´` `‰∫∫ËÑ∏ËØÜÂà´` `Ê¥ª‰ΩìÊîªÂáªÊ£ÄÊµã` `‰∫∫ËÑ∏ÂèòÈÄ†ÊîªÂáªÊ£ÄÊµã` `CLIPÊ®°Âûã` `Ë∑®ÂüüÊ≥õÂåñ` `Ëøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏` `Ê®°ÂûãÂæÆË∞É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°‰∏≠ÂæÆË∞ÉÂü∫Á°ÄÊ®°ÂûãÊó∂ÔºåÂÆπÊòìÂá∫Áé∞Ëøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ÈóÆÈ¢òÔºåÂØºËá¥Ë∑®ÂüüÊ≥õÂåñËÉΩÂäõ‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÈÄöËøáÁ≥ªÁªüËØÑ‰º∞CLIPÊ®°ÂûãÂú®‰∏çÂêåÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°ÂæÆË∞ÉÂêéÁöÑÊÄßËÉΩÔºåÈáèÂåñ‰∫ÜÊ≥õÂåñËÉΩÂäõ‰∏éËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÂú®ÈÄöÁî®ËßÜËßâ‰ªªÂä°‰∏äÊÄßËÉΩ‰∏ãÈôçÔºå‰∏î‰ªªÂä°Â§çÊùÇÊÄßÂíåÊ®°ÂûãÂÆπÈáè‰∏éÊ≥õÂåñËÉΩÂäõÊçüÂ§±Á®ãÂ∫¶Áõ∏ÂÖ≥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

CLIPÁ≠âÂü∫Á°ÄÊ®°ÂûãÂú®ÂêÑÁßçËßÜËßâ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÂçìË∂äÁöÑÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨ËøÅÁßªËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂΩìÈíàÂØπÈ´òÂ∫¶‰∏ì‰∏öÂåñÁöÑÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°ÔºàÂ¶Ç‰∫∫ËÑ∏ËØÜÂà´(FR)„ÄÅ‰∫∫ËÑ∏ÂèòÈÄ†ÊîªÂáªÊ£ÄÊµã(MAD)ÂíåÊ¥ª‰ΩìÊîªÂáªÊ£ÄÊµã(PAD)ÔºâËøõË°åÂæÆË∞ÉÊó∂ÔºåËøô‰∫õÊ®°ÂûãÂèØËÉΩ‰ºöËøáÂ∫¶‰∏ì‰∏öÂåñÔºå‰ªéËÄåÂ§±ÂéªÂÖ∂Âü∫Á°Ä‰ºòÂäø‰πã‰∏ÄÔºöË∑®ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇÊú¨ÊñáÁ≥ªÁªüÂú∞ÈáèÂåñ‰∫ÜËøô‰∫õÊùÉË°°ÔºåÈÄöËøáËØÑ‰º∞‰∏â‰∏™ÈíàÂØπFR„ÄÅMADÂíåPADÂæÆË∞ÉÁöÑCLIPÂÆû‰æãÊù•ÂÆûÁé∞„ÄÇÈô§‰∫ÜÂ∏∏ËßÅÁöÑFR„ÄÅMADÂíåPADÂü∫ÂáÜÊµãËØïÂ§ñÔºåÊàë‰ª¨ËøòÂú®Èõ∂Ê†∑Êú¨ÂíåÁ∫øÊÄßÊé¢ÈíàÂçèËÆÆ‰∏ãÔºåÂú®14‰∏™ÈÄöÁî®ËßÜËßâÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜÊØè‰∏™Ë∞ÉÊï¥ÂêéÁöÑÊ®°Âûã‰ª•ÂèäÂéüÂßãCLIPÂü∫Á∫ø„ÄÇÁªìÊûúË°®ÊòéÔºåÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÂ≠òÂú®ËøáÂ∫¶‰∏ì‰∏öÂåñÁöÑÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂΩìÈíàÂØπÂ§çÊùÇÁöÑ‰∫∫ËÑ∏ËØÜÂà´‰ªªÂä°ËøõË°åÂæÆË∞ÉÊó∂„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºå‰ªªÂä°Â§çÊùÇÊÄßÂíåÂàÜÁ±ªÂ§¥ËÆæËÆ°ÔºàÂ§öÁ±ªFR‰∏é‰∫åÂÖÉMADÂíåPADÔºâ‰∏éÁÅæÈöæÊÄßÈÅóÂøòÁöÑÁ®ãÂ∫¶Áõ∏ÂÖ≥„ÄÇ‰ΩøÁî®ViT-LÈ™®Âπ≤ÁöÑFRoundationÊ®°ÂûãÂú®Â§ßÂûãFRÂü∫ÂáÜIJB-C‰∏ä‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÈ´òËææ58.52%ÁöÑÊîπËøõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂú®ImageNetV2‰∏äÁöÑÊÄßËÉΩÂ§ßÂπÖ‰∏ãÈôçÔºå‰ªÖËææÂà∞51.63%ÔºåËÄåÂü∫Á∫øCLIPÊ®°ÂûãËææÂà∞‰∫Ü69.84%„ÄÇÊ≠§Â§ñÔºåËæÉÂ§ßÁöÑCLIPÊû∂ÊûÑÂßãÁªàÊØîËæÉÂ∞èÁöÑÂèò‰Ωì‰øùÁïô‰∫ÜÊõ¥Â§öÁöÑÊ®°ÂûãÂéüÂßãÊ≥õÂåñËÉΩÂäõÔºåË°®ÊòéÂ¢ûÂä†Ê®°ÂûãÂÆπÈáèÂèØËÉΩÊúâÂä©‰∫éÁºìËß£ËøáÂ∫¶‰∏ì‰∏öÂåñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Á†îÁ©∂ÂΩìÂü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇCLIPÔºâÈíàÂØπÁâπÂÆöÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°ÔºàÂ¶Ç‰∫∫ËÑ∏ËØÜÂà´„ÄÅ‰∫∫ËÑ∏ÂèòÈÄ†ÊîªÂáªÊ£ÄÊµãÂíåÊ¥ª‰ΩìÊîªÂáªÊ£ÄÊµãÔºâËøõË°åÂæÆË∞ÉÊó∂ÔºåÂÖ∂Ë∑®ÂüüÊ≥õÂåñËÉΩÂäõ‰ºöÂèóÂà∞ÊÄéÊ†∑ÁöÑÂΩ±Âìç„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ËøΩÊ±ÇÁâπÂÆö‰ªªÂä°ÊÄßËÉΩÊèêÂçáÁöÑÂêåÊó∂ÔºåÂæÄÂæÄÂøΩÁï•‰∫ÜÊ®°ÂûãÂú®ÂÖ∂‰ªñÈ¢ÜÂüüÁöÑË°®Áé∞ÔºåÂØºËá¥Ê®°ÂûãËøáÂ∫¶ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ôºå‰∏ßÂ§±‰∫ÜÂü∫Á°ÄÊ®°ÂûãÁöÑÈÄöÁî®ÊÄß‰ºòÂäø„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁ≥ªÁªüÊÄßÁöÑÂÆûÈ™åËØÑ‰º∞ÔºåÈáèÂåñCLIPÊ®°ÂûãÂú®‰∏çÂêåÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°‰∏äÂæÆË∞ÉÂêéÔºåÂÖ∂Âú®ÈÄöÁî®ËßÜËßâ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÂèòÂåñ„ÄÇÈÄöËøáÂØπÊØîÂæÆË∞ÉÂâçÂêéÊ®°ÂûãÂú®‰∏çÂêåÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÔºåÂàÜÊûêËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Áé∞Ë±°ÁöÑ‰∏•ÈáçÁ®ãÂ∫¶ÔºåÂπ∂Êé¢ËÆ®‰ªªÂä°Â§çÊùÇÊÄß„ÄÅÊ®°ÂûãÂÆπÈáèÁ≠âÂõ†Á¥†ÂØπÊ≥õÂåñËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÈááÁî®ÁöÑÂÆûÈ™åÊ°ÜÊû∂ÂåÖÊã¨Ôºö1) ÈÄâÊã©CLIP‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåÂπ∂ÈíàÂØπ‰∫∫ËÑ∏ËØÜÂà´„ÄÅ‰∫∫ËÑ∏ÂèòÈÄ†ÊîªÂáªÊ£ÄÊµãÂíåÊ¥ª‰ΩìÊîªÂáªÊ£ÄÊµã‰∏â‰∏™‰ªªÂä°ËøõË°åÂæÆË∞ÉÔºõ2) Âú®14‰∏™ÈÄöÁî®ËßÜËßâÊï∞ÊçÆÈõÜÂíåÁõ∏Â∫îÁöÑÁîüÁâ©ÁâπÂæÅËØÜÂà´Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞ÂæÆË∞ÉÂâçÂêéÊ®°ÂûãÁöÑÊÄßËÉΩÔºõ3) ÈááÁî®Èõ∂Ê†∑Êú¨ÂíåÁ∫øÊÄßÊé¢Èíà‰∏§ÁßçËØÑ‰º∞ÂçèËÆÆÔºå‰ª•Êõ¥ÂÖ®Èù¢Âú∞Ë°°ÈáèÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ4) ÂàÜÊûê‰ªªÂä°Â§çÊùÇÊÄßÔºàÂ§öÂàÜÁ±ª vs. ‰∫åÂàÜÁ±ªÔºâÂíåÊ®°ÂûãÂÆπÈáèÔºà‰∏çÂêåÂ§ßÂ∞èÁöÑCLIPÊ®°ÂûãÔºâÂØπËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Áé∞Ë±°ÁöÑÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÁ≥ªÁªüÊÄßÂú∞Á†îÁ©∂‰∫ÜÂü∫Á°ÄÊ®°ÂûãÂú®ÁîüÁâ©ÁâπÂæÅËØÜÂà´È¢ÜÂüüÂæÆË∞ÉÊó∂ÔºåÊ≥õÂåñËÉΩÂäõ‰∏éËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ‰πãÈó¥ÁöÑÊùÉË°°ÂÖ≥Á≥ª„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂÖ≥Ê≥®ÁâπÂÆö‰ªªÂä°ÁöÑÊÄßËÉΩÊèêÂçáÔºåËÄåÂøΩÁï•‰∫ÜÊ®°ÂûãÂú®ÂÖ∂‰ªñÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇÊú¨ÊñáÈÄöËøáÂÖ®Èù¢ÁöÑÂÆûÈ™åËØÑ‰º∞ÔºåÊè≠Á§∫‰∫ÜÂæÆË∞ÉÂèØËÉΩÂØºËá¥ÁöÑÊ≥õÂåñËÉΩÂäõÊçüÂ§±ÔºåÂπ∂Êé¢ËÆ®‰∫ÜÂΩ±ÂìçÂõ†Á¥†Ôºå‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÂèÇËÄÉ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÁîüÁâ©ÁâπÂæÅËØÜÂà´‰ªªÂä°ÔºåÊ∂µÁõñ‰∫Ü‰∫∫ËÑ∏ËØÜÂà´„ÄÅÂÆâÂÖ®Ê£ÄÊµãÁ≠âÂ§ö‰∏™ÊñπÈù¢Ôºõ2) ÈááÁî®Â§öÁßçËØÑ‰º∞ÂçèËÆÆÔºåÂåÖÊã¨Èõ∂Ê†∑Êú¨ÂíåÁ∫øÊÄßÊé¢ÈíàÔºå‰ª•Êõ¥ÂÖ®Èù¢Âú∞Ë°°ÈáèÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ3) ÂØπÊØî‰∏çÂêåÂ§ßÂ∞èÁöÑCLIPÊ®°ÂûãÔºå‰ª•Á†îÁ©∂Ê®°ÂûãÂÆπÈáèÂØπËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Áé∞Ë±°ÁöÑÂΩ±ÂìçÔºõ4) ÂàÜÊûê‰ªªÂä°Â§çÊùÇÊÄßÂØπÊ≥õÂåñËÉΩÂäõÁöÑÂΩ±ÂìçÔºå‰æãÂ¶ÇÂØπÊØîÂ§öÂàÜÁ±ªÁöÑ‰∫∫ËÑ∏ËØÜÂà´‰ªªÂä°Âíå‰∫åÂàÜÁ±ªÁöÑÊîªÂáªÊ£ÄÊµã‰ªªÂä°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈíàÂØπ‰∫∫ËÑ∏ËØÜÂà´‰ªªÂä°ÂæÆË∞ÉÁöÑCLIPÊ®°ÂûãÂú®IJB-CÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÈ´òËææ58.52%ÁöÑÊÄßËÉΩÊèêÂçáÔºå‰ΩÜÂú®ImageNetV2Êï∞ÊçÆÈõÜ‰∏äÁöÑÊÄßËÉΩÂç¥‰∏ãÈôç‰∫Ü18.21%Ôºà‰ªé69.84%ÈôçËá≥51.63%ÔºâÔºåËøôË°®ÊòéÂæÆË∞É‰ºöÂØºËá¥ÊòéÊòæÁöÑËøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Áé∞Ë±°„ÄÇÊ≠§Â§ñÔºåËæÉÂ§ßÁöÑCLIPÊ®°ÂûãÔºàViT-LÔºâÊØîÂ∞èÁöÑCLIPÊ®°ÂûãÊõ¥ËÉΩ‰øùÁïôÂéüÂßãÁöÑÊ≥õÂåñËÉΩÂäõÔºåË°®ÊòéÂ¢ûÂä†Ê®°ÂûãÂÆπÈáèÂèØ‰ª•ÁºìËß£Ëøá —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ÈóÆÈ¢ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊåáÂØºÁîüÁâ©ÁâπÂæÅËØÜÂà´Á≥ªÁªüÁöÑËÆæËÆ°ÂíåÈÉ®ÁΩ≤ÔºåÂú®ËøΩÊ±ÇÁâπÂÆö‰ªªÂä°È´òÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÈÅøÂÖçÊ®°ÂûãËøáÂ∫¶ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ôºå‰øùÊåÅËâØÂ•ΩÁöÑË∑®ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂºÄÂèë‰∫∫ËÑ∏ËØÜÂà´Á≥ªÁªüÊó∂ÔºåÂèØ‰ª•ÂÄüÈâ¥ËØ•Á†îÁ©∂ÁöÑÁªìËÆ∫ÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÂæÆË∞ÉÁ≠ñÁï•ÂíåÊ®°ÂûãÂÆπÈáèÔºå‰ª•Á°Æ‰øùÁ≥ªÁªüÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÈÉΩËÉΩ‰øùÊåÅËâØÂ•ΩÁöÑËØÜÂà´Á≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂‰πü‰∏∫ÂÖ∂‰ªñÈ¢ÜÂüüÁöÑÊ®°ÂûãÂæÆË∞ÉÊèê‰æõ‰∫ÜÂèÇËÄÉÔºåÊúâÂä©‰∫éÊèêÂçáÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model's original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization.

