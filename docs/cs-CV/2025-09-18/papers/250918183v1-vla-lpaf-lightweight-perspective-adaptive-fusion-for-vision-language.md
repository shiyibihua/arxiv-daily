---
layout: default
title: VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation
---

# VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18183" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18183v1</a>
  <a href="https://arxiv.org/pdf/2509.18183.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18183v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18183v1', 'VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinyue Bian, Zhaoxing Zhang, Zhengyu Liang, Shiwei Zheng, Shengtao Zhang, Rong Shen, Chen Yang, Anzhou Hou

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLA-LPAFè½»é‡çº§è§†è§’è‡ªé€‚åº”èåˆæ¨¡å—ï¼Œæå‡VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ“ä½œ` `è§†è§’è‡ªé€‚åº”` `å¤šè§†è§’èåˆ` `è½»é‡çº§æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸­è§†è§’å¼‚è´¨æ€§å¯¼è‡´è§†è§‰ç‰¹å¾å·®å¼‚ã€‚
2. æå‡ºVLA-LPAFæ¨¡å—ï¼Œé€šè¿‡å•è§†è§’å¾®è°ƒå’Œæ½œåœ¨ç©ºé—´èåˆï¼Œå®ç°è§†è§’è‡ªé€‚åº”ï¼Œå¼¥åˆè§†è§’ä¸ä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRoboFlamingo-LPAFåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­éªŒè¯äº†è§†è§’è‡ªé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å¯¹å‘¨å›´ç¯å¢ƒçš„è§†è§‰è§‚å¯Ÿï¼Œéµå¾ªæ–‡æœ¬æŒ‡ä»¤æ‰§è¡ŒåŠ¨ä½œã€‚è¿™ç§å°†å¤šæ¨¡æ€è¾“å…¥æ˜ å°„åˆ°åŠ¨ä½œçš„èƒ½åŠ›æºäºVLAæ¨¡å‹åœ¨å¤§é‡æ ‡å‡†æ¼”ç¤ºæ•°æ®ä¸Šçš„è®­ç»ƒã€‚ç„¶è€Œï¼Œç”±ç¬¬ä¸‰äººç§°å…¨å±€ç›¸æœºå’Œæ‰‹è…•å±€éƒ¨ç›¸æœºæ•è·çš„è§†è§‰è§‚å¯Ÿåœ¨ä¸åŒç¯å¢ƒä¸­æ•°é‡å’Œè§†è§’ä¸Šä¸å¯é¿å…åœ°å­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´è§†è§‰ç‰¹å¾çš„æ˜¾è‘—å·®å¼‚ã€‚è¿™ç§è§†è§’å¼‚è´¨æ€§é™åˆ¶äº†VLAæ¨¡å‹çš„æ³›åŒ–æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè½»é‡çº§æ¨¡å—VLA-LPAFï¼Œä»…ä½¿ç”¨2Dæ•°æ®æ¥ä¿ƒè¿›VLAæ¨¡å‹çš„è§†è§’è‡ªé€‚åº”æ€§ã€‚VLA-LPAFä½¿ç”¨æ¥è‡ªå•ä¸ªè§†è§’çš„å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­èåˆå…¶ä»–å¤šè§†è§’è§‚å¯Ÿï¼Œä»è€Œæœ‰æ•ˆä¸”é«˜æ•ˆåœ°å¼¥åˆäº†ç”±è§†è§’ä¸ä¸€è‡´å¼•èµ·çš„å·®è·ã€‚æˆ‘ä»¬å°†VLA-LPAFæ¡†æ¶å®ä¾‹åŒ–ä¸ºRoboFlamingoï¼Œæ„å»ºRoboFlamingo-LPAFã€‚å®éªŒè¡¨æ˜ï¼ŒRoboFlamingo-LPAFåœ¨CALVINä¸Šå¹³å‡å®ç°äº†çº¦8%çš„ä»»åŠ¡æˆåŠŸç‡æå‡ï¼Œåœ¨LIBEROä¸Šæå‡äº†15%ï¼Œåœ¨å®šåˆ¶çš„æ¨¡æ‹ŸåŸºå‡†ä¸Šæå‡äº†30%ã€‚æˆ‘ä»¬è¿˜åœ¨çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ä¸­å±•ç¤ºäº†æ‰€æå‡ºçš„RoboFlamingo-LPAFæ‰€å¼€å‘çš„è§†è§’è‡ªé€‚åº”ç‰¹æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œç”±äºè®­ç»ƒæ•°æ®é›†ä¸­å­˜åœ¨ä¸åŒè§†è§’ï¼ˆä¾‹å¦‚å…¨å±€è§†è§’å’Œæ‰‹è…•è§†è§’ï¼‰çš„å›¾åƒï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé¢å¯¹æ–°çš„è§†è§’æ—¶æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†è¿™ç§è§†è§’å¼‚è´¨æ€§ï¼Œé™åˆ¶äº†VLAæ¨¡å‹åœ¨æ›´å¤æ‚ã€æ›´ä¸å—çº¦æŸçš„ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLA-LPAFçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„æ¨¡å—ï¼Œå­¦ä¹ ä¸åŒè§†è§’ä¹‹é—´çš„æ½œåœ¨ç©ºé—´æ˜ å°„å…³ç³»ï¼Œä»è€Œå®ç°è§†è§’è‡ªé€‚åº”ã€‚è¯¥æ¨¡å—é€šè¿‡å•è§†è§’æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­èåˆå…¶ä»–è§†è§’çš„ç‰¹å¾ï¼Œä»è€Œå¼¥åˆè§†è§’å·®å¼‚ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥åœ¨å›¾åƒç©ºé—´è¿›è¡Œå¤æ‚çš„è§†è§’å˜æ¢ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLA-LPAFæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰æå–ä¸åŒè§†è§’çš„å›¾åƒç‰¹å¾ã€‚2) å•è§†è§’å¾®è°ƒï¼šä½¿ç”¨æ¥è‡ªå•ä¸ªè§†è§’çš„å›¾åƒæ•°æ®å¯¹VLA-LPAFæ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ç›®æ ‡è§†è§’ã€‚3) æ½œåœ¨ç©ºé—´èåˆï¼šå°†å…¶ä»–è§†è§’çš„å›¾åƒç‰¹å¾æŠ•å½±åˆ°ä¸ç›®æ ‡è§†è§’ç›¸åŒçš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶è¿›è¡Œèåˆã€‚4) åŠ¨ä½œé¢„æµ‹ï¼šå°†èåˆåçš„ç‰¹å¾è¾“å…¥åˆ°VLAæ¨¡å‹ä¸­ï¼Œé¢„æµ‹æœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šVLA-LPAFçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è½»é‡çº§çš„è®¾è®¡å’Œæ½œåœ¨ç©ºé—´èåˆç­–ç•¥ã€‚ä¸ç›´æ¥è¿›è¡Œå›¾åƒç©ºé—´å˜æ¢çš„æ–¹æ³•ç›¸æ¯”ï¼ŒVLA-LPAFè®¡ç®—æˆæœ¬æ›´ä½ï¼Œæ›´å®¹æ˜“éƒ¨ç½²åˆ°å®é™…æœºå™¨äººç³»ç»Ÿä¸­ã€‚æ­¤å¤–ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œèåˆå¯ä»¥æ›´å¥½åœ°ä¿ç•™ä¸åŒè§†è§’çš„äº’è¡¥ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVLA-LPAFæ¨¡å—çš„å…·ä½“ç½‘ç»œç»“æ„æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æåˆ°æ˜¯ä½¿ç”¨2Dæ•°æ®ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œèåˆã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ æŸå¤±ï¼ˆç”¨äºå­¦ä¹ ä¸“å®¶ç­–ç•¥ï¼‰å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆç”¨äºæ‹‰è¿‘ä¸åŒè§†è§’åœ¨æ½œåœ¨ç©ºé—´çš„è·ç¦»ï¼‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoboFlamingo-LPAFåœ¨CALVINæ•°æ®é›†ä¸Šå¹³å‡æå‡äº†8%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œåœ¨LIBEROæ•°æ®é›†ä¸Šæå‡äº†15%ï¼Œåœ¨å®šåˆ¶çš„æ¨¡æ‹ŸåŸºå‡†ä¸Šæå‡äº†30%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLA-LPAFèƒ½å¤Ÿæœ‰æ•ˆæé«˜VLAæ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLA-LPAFæŠ€æœ¯å¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººã€‚é€šè¿‡æé«˜VLAæ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´è‡ªä¸»ã€æ›´å¯é çš„æ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Visual-Language-Action (VLA) models can follow text instructions according to visual observations of the surrounding environment. This ability to map multimodal inputs to actions is derived from the training of the VLA model on extensive standard demonstrations. These visual observations captured by third-personal global and in-wrist local cameras are inevitably varied in number and perspective across different environments, resulting in significant differences in the visual features. This perspective heterogeneity constrains the generality of VLA models. In light of this, we first propose the lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models using only 2D data. VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space, which effectively and efficiently bridge the gap caused by perspective inconsistency. We instantiate our VLA-LPAF framework with the VLA model RoboFlamingo to construct RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. We also demonstrate the developed viewadaptive characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

