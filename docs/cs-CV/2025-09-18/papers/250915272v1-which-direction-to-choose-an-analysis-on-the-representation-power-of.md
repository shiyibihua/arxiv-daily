---
layout: default
title: Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks
---

# Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15272" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15272v1</a>
  <a href="https://arxiv.org/pdf/2509.15272.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15272v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15272v1', 'Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yannis Kaltampanidis, Alexandros Doumanoglou, Dimitrios Zarpalas

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: 24 pages, XAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æè‡ªç›‘ç£ViTåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨å¾èƒ½åŠ›ï¼Œæ¢ç©¶æœ€ä¼˜ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `Vision Transformer` `ç‰¹å¾è¡¨å¾` `ä¸‹æ¸¸ä»»åŠ¡` `å›¾åƒåˆ†ç±»` `å›¾åƒåˆ†å‰²` `å°‘æ ·æœ¬å­¦ä¹ ` `é¢„è®­ç»ƒæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸å¯¹é¢„è®­ç»ƒViTç‰¹å¾è¿›è¡Œé¢å¤–å¤„ç†ï¼Œç¼ºä¹å¯¹åŸå§‹ViTç‰¹å¾å†…åœ¨è¡¨å¾èƒ½åŠ›çš„æ·±å…¥åˆ†æã€‚
2. æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†æœªç»ä¿®æ”¹çš„ViTç‰¹å¾åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæ¢ç´¢æœ€ä¼˜ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ­ç¤ºäº†ä¸åŒtokenç±»å‹å’Œå†³ç­–è§„åˆ™åœ¨ä¸åŒä»»åŠ¡å’Œé¢„è®­ç»ƒç›®æ ‡ä¸‹çš„é€‚ç”¨æ€§ï¼Œä¸ºViTç‰¹å¾é€‰æ‹©æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè‡ªç›‘ç£å­¦ä¹ (SSL)çš„Vision Transformer (ViT)ä½œä¸ºä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œåœ¨å›¾åƒåˆ†ç±»ã€åˆ†å‰²ç­‰å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡å‡†å’Œå°‘æ ·æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚å¯¹æ¯”å­¦ä¹ å’Œæ©ç å›¾åƒå»ºæ¨¡æ˜¯SSLæŠ€æœ¯é¢†åŸŸä¸­çš„ä¸¤ç§ä¸»è¦ç›®æ ‡ã€‚ä»æœ€ç»ˆTransformeræ³¨æ„åŠ›å—æå–çš„ç‰¹å¾ï¼ˆæˆ–tokensï¼‰ï¼Œç‰¹åˆ«æ˜¯é”®(keys)ã€æŸ¥è¯¢(queries)å’Œå€¼(values)ï¼Œä»¥åŠæœ€ç»ˆå‰é¦ˆå±‚ä¹‹åçš„ç‰¹å¾ï¼Œå·²æˆä¸ºè§£å†³ä¸‹æ¸¸ä»»åŠ¡çš„å¸¸è§åŸºç¡€ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šç°æœ‰æ–¹æ³•ä¸­ï¼Œè¿™äº›é¢„è®­ç»ƒçš„ViTç‰¹å¾ä¼šé€šè¿‡é¢å¤–çš„è½¬æ¢å±‚è¿›ä¸€æ­¥å¤„ç†ï¼Œé€šå¸¸æ¶‰åŠè½»é‡çº§å¤´éƒ¨æˆ–ä¸è’¸é¦ç»“åˆï¼Œä»¥å®ç°å“è¶Šçš„ä»»åŠ¡æ€§èƒ½ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å¯ä»¥æ”¹å–„ä»»åŠ¡ç»“æœï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæœªå¯¹æœªç»ä¿®æ”¹çš„ViTç‰¹å¾çš„å†…åœ¨è¡¨å¾èƒ½åŠ›è¿›è¡Œå…¨é¢åˆ†æã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡åœ¨æ ‡å‡†å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­ï¼Œç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›æœªç»ä¿®æ”¹çš„ç‰¹å¾åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä½¿ç”¨çš„åˆ†ç±»å’Œåˆ†å‰²è§„åˆ™æ˜¯åŸºäºè¶…å¹³é¢çš„ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰æˆ–åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„ï¼Œä¸¤è€…éƒ½ä¾èµ–äºViTæ½œåœ¨ç©ºé—´ä¸­å¯è§£é‡Šæ–¹å‘çš„å­˜åœ¨ã€‚åŸºäºä¸Šè¿°è§„åˆ™ï¼Œåœ¨ä¸ä½¿ç”¨é¢å¤–ç‰¹å¾è½¬æ¢çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯¹tokenç±»å‹ã€ä»»åŠ¡å’Œé¢„è®­ç»ƒçš„ViTæ¨¡å‹è¿›è¡Œäº†åˆ†æã€‚æœ¬ç ”ç©¶æ·±å…¥äº†è§£äº†åŸºäºä»»åŠ¡ã€ä¸Šä¸‹æ–‡å’Œé¢„è®­ç»ƒç›®æ ‡çš„æœ€ä¼˜tokenç±»å‹å’Œå†³ç­–è§„åˆ™é€‰æ‹©ï¼ŒåŒæ—¶æŠ¥å‘Šäº†ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†çš„è¯¦ç»†å‘ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒçš„ViTæ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šæ·»åŠ é¢å¤–çš„ç‰¹å¾è½¬æ¢å±‚ï¼ˆå¦‚è½»é‡çº§å¤´éƒ¨æˆ–è’¸é¦ï¼‰æ¥æå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§åšæ³•æ©ç›–äº†åŸå§‹ViTç‰¹å¾æœ¬èº«çš„è¡¨å¾èƒ½åŠ›ï¼Œç¼ºä¹å¯¹ä¸åŒViTç‰¹å¾ï¼ˆå¦‚keys, queries, valuesï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸‹çš„é€‚ç”¨æ€§çš„ç³»ç»Ÿç ”ç©¶ã€‚å› æ­¤ï¼Œéœ€è¦ç ”ç©¶æœªç»ä¿®æ”¹çš„ViTç‰¹å¾åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨ViTçš„å†…åœ¨è¡¨å¾èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒViTæ¨¡å‹æå–çš„åŸå§‹ç‰¹å¾ï¼Œè€Œä¸è¿›è¡Œé¢å¤–çš„ç‰¹å¾è½¬æ¢ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒç±»å‹çš„ViTç‰¹å¾ï¼ˆkeys, queries, valuesä»¥åŠæœ€ç»ˆå‰é¦ˆå±‚è¾“å‡ºï¼‰åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶ç»“åˆä¸åŒçš„å†³ç­–è§„åˆ™ï¼ˆåŸºäºè¶…å¹³é¢æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œæ¥åˆ†æä¸åŒç‰¹å¾çš„è¡¨å¾èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æ­ç¤ºä¸åŒç‰¹å¾åœ¨ä¸åŒä»»åŠ¡å’Œé¢„è®­ç»ƒç›®æ ‡ä¸‹çš„æœ€ä¼˜é€‰æ‹©ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚å¯¹æ¯”å­¦ä¹ æˆ–æ©ç å›¾åƒå»ºæ¨¡ï¼‰é¢„è®­ç»ƒViTæ¨¡å‹ã€‚2) ä»é¢„è®­ç»ƒçš„ViTæ¨¡å‹ä¸­æå–ä¸åŒç±»å‹çš„ç‰¹å¾ï¼ˆkeys, queries, values, feed-forward layer outputï¼‰ã€‚3) ä½¿ç”¨æå–çš„ç‰¹å¾è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒå’Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€‚4) é‡‡ç”¨ä¸åŒçš„å†³ç­–è§„åˆ™ï¼ˆåŸºäºè¶…å¹³é¢æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰è¿›è¡Œåˆ†ç±»å’Œåˆ†å‰²ã€‚5) å¯¹ä¸åŒç‰¹å¾ã€ä»»åŠ¡å’Œå†³ç­–è§„åˆ™çš„ç»„åˆè¿›è¡Œç³»ç»Ÿæ€§çš„å®éªŒåˆ†æï¼Œä»è€Œç¡®å®šæœ€ä¼˜çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹æœªç»ä¿®æ”¹çš„ViTç‰¹å¾çš„å†…åœ¨è¡¨å¾èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„åˆ†æã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæœ¬ç ”ç©¶é¿å…äº†é¢å¤–çš„ç‰¹å¾è½¬æ¢ï¼Œç›´æ¥è¯„ä¼°äº†åŸå§‹ViTç‰¹å¾åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒç±»å‹çš„ç‰¹å¾å’Œå†³ç­–è§„åˆ™ï¼Œæ­ç¤ºäº†ä¸åŒç‰¹å¾åœ¨ä¸åŒä»»åŠ¡å’Œé¢„è®­ç»ƒç›®æ ‡ä¸‹çš„é€‚ç”¨æ€§ï¼Œä¸ºViTç‰¹å¾é€‰æ‹©æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬ç ”ç©¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†ä¸¤ç§å¸¸ç”¨çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆå¯¹æ¯”å­¦ä¹ å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼‰è¿›è¡ŒViTé¢„è®­ç»ƒã€‚2) æå–äº†ViTæ¨¡å‹ä¸­ä¸åŒä½ç½®çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬keys, queries, valuesä»¥åŠæœ€ç»ˆå‰é¦ˆå±‚è¾“å‡ºã€‚3) é‡‡ç”¨äº†ä¸¤ç§ä¸åŒçš„å†³ç­–è§„åˆ™ï¼šåŸºäºè¶…å¹³é¢çš„è§„åˆ™ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰å’ŒåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„è§„åˆ™ã€‚4) åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬æ ‡å‡†å’Œå°‘æ ·æœ¬è®¾ç½®ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæœ¬ç ”ç©¶èƒ½å¤Ÿå…¨é¢åœ°è¯„ä¼°ä¸åŒç‰¹å¾å’Œå†³ç­–è§„åˆ™çš„æ€§èƒ½ï¼Œå¹¶ç¡®å®šæœ€ä¼˜çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶é€šè¿‡å®éªŒå‘ç°ï¼Œä¸åŒç±»å‹çš„ViTç‰¹å¾åœ¨ä¸åŒä»»åŠ¡å’Œé¢„è®­ç»ƒç›®æ ‡ä¸‹è¡¨ç°å‡ºä¸åŒçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒæŸäº›ç‰¹å¾åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Œè€Œå¦ä¸€äº›ç‰¹å¾åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ›´æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼ŒåŸºäºè¶…å¹³é¢çš„å†³ç­–è§„åˆ™åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„è§„åˆ™ï¼Œåä¹‹äº¦ç„¶ã€‚è¿™äº›å‘ç°ä¸ºViTç‰¹å¾é€‰æ‹©æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„ViTç‰¹å¾ï¼Œå¯ä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™æˆ–éœ€è¦å¿«é€Ÿéƒ¨ç½²çš„åœºæ™¯ä¸‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£ViTæ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œä¼˜åŒ–æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently demonstrated considerable potential as a pre-training strategy for a variety of computer vision tasks, including image classification and segmentation, both in standard and few-shot downstream contexts. Two pre-training objectives dominate the landscape of SSL techniques: Contrastive Learning and Masked Image Modeling. Features (or tokens) extracted from the final transformer attention block -- specifically, the keys, queries, and values -- as well as features obtained after the final block's feed-forward layer, have become a common foundation for addressing downstream tasks. However, in many existing approaches, these pre-trained ViT features are further processed through additional transformation layers, often involving lightweight heads or combined with distillation, to achieve superior task performance. Although such methods can improve task outcomes, to the best of our knowledge, a comprehensive analysis of the intrinsic representation capabilities of unaltered ViT features has yet to be conducted. This study aims to bridge this gap by systematically evaluating the use of these unmodified features across image classification and segmentation tasks, in both standard and few-shot contexts. The classification and segmentation rules that we use are either hyperplane based (as in logistic regression) or cosine-similarity based, both of which rely on the presence of interpretable directions in the ViT's latent space. Based on the previous rules and without the use of additional feature transformations, we conduct an analysis across token types, tasks, and pre-trained ViT models. This study provides insights into the optimal choice for token type and decision rule based on the task, context, and the pre-training objective, while reporting detailed findings on two widely-used datasets.

