---
layout: default
title: Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification
---

# Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14958" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14958v2</a>
  <a href="https://arxiv.org/pdf/2509.14958.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14958v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14958v2', 'Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tuo Xiang, Xuemiao Xu, Bangzhen Liu, Jinyi Li, Yong Li, Shengfeng He

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-21)

**å¤‡æ³¨**: ICCV2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æ¨¡æ€å‡ ä½•æ ¡æ­£ï¼ˆCMGRï¼‰æ¡†æ¶ï¼Œè§£å†³3Då°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä¸­çš„å‡ ä½•å¤±å‡†å’Œçº¹ç†åå·®é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰` `å°‘æ ·æœ¬å­¦ä¹ ` `ç±»å¢é‡å­¦ä¹ ` `è·¨æ¨¡æ€å­¦ä¹ ` `å‡ ä½•æ ¡æ­£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dç±»å¢é‡å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæ—¶ï¼Œæ˜“å—å‡ ä½•å¤±å‡†å’Œçº¹ç†åå·®å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. CMGRæ¡†æ¶åˆ©ç”¨CLIPçš„å±‚çº§ç©ºé—´è¯­ä¹‰ï¼Œé€šè¿‡å‡ ä½•æ ¡æ­£å’Œçº¹ç†æ”¾å¤§ï¼Œå¢å¼º3Då‡ ä½•ä¿çœŸåº¦å¹¶æŠ‘åˆ¶å™ªå£°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCMGRæ˜¾è‘—æå‡äº†3Då°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ æ€§èƒ½ï¼Œæé«˜äº†å‡ ä½•ä¸€è‡´æ€§å’Œå¯¹çº¹ç†åå·®çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹3Dæ•°å­—å†…å®¹å¿«é€Ÿå¢é•¿å¸¦æ¥çš„å¼€æ”¾ä¸–ç•Œåœºæ™¯éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºè·¨æ¨¡æ€å‡ ä½•æ ¡æ­£ï¼ˆCMGRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰3Dç±»å¢é‡å­¦ä¹ æ–¹æ³•åœ¨æç«¯æ•°æ®ç¨€ç¼ºä¸‹çš„å‡ ä½•å¤±å‡†å’Œçº¹ç†åå·®é—®é¢˜ã€‚CMGRåˆ©ç”¨CLIPçš„å±‚çº§ç©ºé—´è¯­ä¹‰å¢å¼º3Då‡ ä½•ä¿çœŸåº¦ã€‚å…·ä½“è€Œè¨€ï¼Œç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£æ¨¡å—é€šè¿‡æ³¨æ„åŠ›é©±åŠ¨çš„å‡ ä½•èåˆï¼Œå°†3Déƒ¨ä»¶ç»“æ„ä¸CLIPçš„ä¸­é—´ç©ºé—´å…ˆéªŒè¿›è¡Œå±‚çº§å¯¹é½ã€‚çº¹ç†æ”¾å¤§æ¨¡å—åˆæˆæœ€å°ä½†å…·æœ‰åŒºåˆ†æ€§çš„çº¹ç†ï¼Œä»¥æŠ‘åˆ¶å™ªå£°å¹¶å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒåŸºç±»-æ–°ç±»åˆ¤åˆ«å™¨éš”ç¦»å‡ ä½•å˜åŒ–ï¼Œè¿›ä¸€æ­¥ç¨³å®šå¢é‡åŸå‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†3Då°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„æ€§èƒ½ï¼Œåœ¨è·¨åŸŸå’ŒåŸŸå†…è®¾ç½®ä¸­å‡å®ç°äº†å“è¶Šçš„å‡ ä½•ä¸€è‡´æ€§å’Œå¯¹çº¹ç†åå·®çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„3Dç±»å¢é‡å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®æåº¦ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œé¢ä¸´ç€ä¸¥é‡çš„å‡ ä½•å¤±å‡†å’Œçº¹ç†åå·®é—®é¢˜ã€‚ç®€å•åœ°å°†3Dæ•°æ®ä¸2DåŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰èåˆï¼Œä¼šå¯¼è‡´è¯­ä¹‰æ¨¡ç³Šï¼Œå› ä¸ºçº¹ç†åå·®çš„æŠ•å½±å’Œå¯¹å‡ ä½•-çº¹ç†çº¿ç´¢çš„ä¸åŠ åŒºåˆ†çš„èåˆä¼šé€ æˆä¸ç¨³å®šçš„å†³ç­–åŸå‹å’Œç¾éš¾æ€§é—å¿˜ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„3Dæ•°æ®ï¼ŒåŒæ—¶å…‹æœå‡ ä½•å¤±å‡†å’Œçº¹ç†åå·®ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„2Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆCLIPï¼‰çš„å¼ºå¤§è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯å…¶å±‚çº§ç©ºé—´è¯­ä¹‰ï¼Œæ¥æ ¡æ­£3Dæ•°æ®çš„å‡ ä½•ç»“æ„ï¼Œå¹¶æŠ‘åˆ¶çº¹ç†åå·®ã€‚é€šè¿‡å°†3Då‡ ä½•ç»“æ„ä¸CLIPçš„ä¸­é—´å±‚ç‰¹å¾å¯¹é½ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¢å¼º3Då‡ ä½•çš„ä¿çœŸåº¦ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆæˆå…·æœ‰åŒºåˆ†æ€§çš„çº¹ç†ï¼Œå¯ä»¥å‡å°‘å™ªå£°å¹¶æé«˜è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCMGRæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£æ¨¡å—ï¼ˆStructure-Aware Geometric Rectificationï¼‰ã€çº¹ç†æ”¾å¤§æ¨¡å—ï¼ˆTexture Amplification Moduleï¼‰å’ŒåŸºç±»-æ–°ç±»åˆ¤åˆ«å™¨ï¼ˆBase-Novel Discriminatorï¼‰ã€‚é¦–å…ˆï¼Œç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£æ¨¡å—å°†3Déƒ¨ä»¶ç»“æ„ä¸CLIPçš„ä¸­é—´ç©ºé—´å…ˆéªŒè¿›è¡Œå±‚çº§å¯¹é½ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å®ç°å‡ ä½•èåˆã€‚ç„¶åï¼Œçº¹ç†æ”¾å¤§æ¨¡å—åˆæˆæœ€å°ä½†å…·æœ‰åŒºåˆ†æ€§çš„çº¹ç†ï¼Œä»¥æŠ‘åˆ¶å™ªå£°å¹¶å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚æœ€åï¼ŒåŸºç±»-æ–°ç±»åˆ¤åˆ«å™¨ç”¨äºéš”ç¦»å‡ ä½•å˜åŒ–ï¼Œä»è€Œç¨³å®šå¢é‡åŸå‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†è·¨æ¨¡æ€å‡ ä½•æ ¡æ­£çš„æ€æƒ³ï¼Œå³åˆ©ç”¨é¢„è®­ç»ƒçš„2Dè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼3Då‡ ä½•ç»“æ„çš„æ ¡æ­£ã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥èåˆ3Då’Œ2Dç‰¹å¾ä¸åŒï¼ŒCMGRæ›´åŠ æ³¨é‡å‡ ä½•ç»“æ„çš„å¯¹é½å’Œçº¹ç†åå·®çš„æŠ‘åˆ¶ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£æ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆ3Déƒ¨ä»¶ç»“æ„å’ŒCLIPçš„ä¸­é—´å±‚ç‰¹å¾ï¼Œæ³¨æ„åŠ›æƒé‡ç”¨äºæŒ‡å¯¼å‡ ä½•èåˆã€‚çº¹ç†æ”¾å¤§æ¨¡å—é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åˆæˆå…·æœ‰åŒºåˆ†æ€§çš„çº¹ç†ï¼ŒGANçš„ç›®æ ‡æ˜¯ç”Ÿæˆèƒ½å¤Ÿæ¬ºéª—åˆ¤åˆ«å™¨çš„çº¹ç†ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹3Då‡ ä½•ç»“æ„çš„ä¸€è‡´æ€§ã€‚åŸºç±»-æ–°ç±»åˆ¤åˆ«å™¨é‡‡ç”¨äºŒå…ƒåˆ†ç±»å™¨ï¼Œç”¨äºåŒºåˆ†åŸºç±»å’Œæ–°ç±»ï¼Œä»è€Œéš”ç¦»å‡ ä½•å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCMGRåœ¨3Då°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨è·¨åŸŸå’ŒåŸŸå†…è®¾ç½®ä¸­ï¼ŒCMGRå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å‡ ä½•ä¸€è‡´æ€§å’Œå¯¹çº¹ç†åå·®çš„é²æ£’æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¯æ˜äº†CMGRåœ¨è§£å†³3Dæ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€ä¸‰ç»´åœºæ™¯ç†è§£ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡3Dæ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„è¯†åˆ«èƒ½åŠ›ï¼Œå¯ä»¥é™ä½å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€Ÿç›¸å…³æŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€çš„æ•°æ®èåˆå’Œæ›´å¤æ‚çš„3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIP's intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings.

