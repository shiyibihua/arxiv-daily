---
layout: default
title: Chain-of-Thought Re-ranking for Image Retrieval Tasks
---

# Chain-of-Thought Re-ranking for Image Retrieval Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14746" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14746v1</a>
  <a href="https://arxiv.org/pdf/2509.14746.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14746v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14746v1', 'Chain-of-Thought Re-ranking for Image Retrieval Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shangrong Wu, Yanghong Zhou, Yang Chen, Feng Zhang, P. Y. Mok

**åˆ†ç±»**: cs.CV, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/freshfish15/CoTRR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé“¾å¼æ€è€ƒé‡æ’åºæ–¹æ³•CoTRRï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒæ£€ç´¢` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `é“¾å¼æ€è€ƒ` `é‡æ’åº` `æç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒæ£€ç´¢æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´æ£€ç´¢æ€§èƒ½å—é™ã€‚
2. æå‡ºé“¾å¼æ€è€ƒé‡æ’åº(CoTRR)æ–¹æ³•ï¼Œé€šè¿‡åˆ—è¡¨å¼æ’åºæç¤ºï¼Œä½¿MLLMç›´æ¥å‚ä¸å€™é€‰å›¾åƒçš„é‡æ’åºè¿‡ç¨‹ã€‚
3. åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoTRRåœ¨æ–‡æœ¬åˆ°å›¾åƒã€ç»„åˆå›¾åƒå’ŒèŠå¤©å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­å‡å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾åƒæ£€ç´¢æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªåŸºç¡€ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚å°½ç®¡æœ€è¿‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å±•ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä»…å°†å…¶ç”¨äºè¯„ä¼°ï¼Œè€Œæ²¡æœ‰ç›´æ¥å°†å…¶çº³å…¥æ’åºè¿‡ç¨‹ã€‚å› æ­¤ï¼Œå®ƒä»¬ä¸°å¯Œçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œå¯¼è‡´æ€§èƒ½æ¬ ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é“¾å¼æ€è€ƒé‡æ’åº(CoTRR)æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆ—è¡¨å¼æ’åºæç¤ºï¼Œä½¿MLLMèƒ½å¤Ÿç›´æ¥å‚ä¸å€™é€‰å›¾åƒçš„é‡æ’åºã€‚æ­¤æ’åºè¿‡ç¨‹åŸºäºå›¾åƒè¯„ä¼°æç¤ºï¼Œè¯¥æç¤ºè¯„ä¼°æ¯ä¸ªå€™é€‰å›¾åƒä¸ç”¨æˆ·æŸ¥è¯¢çš„å¯¹é½ç¨‹åº¦ã€‚é€šè¿‡å…è®¸MLLMæ‰§è¡Œåˆ—è¡¨å¼æ¨ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå…¨å±€æ¯”è¾ƒã€ä¸€è‡´çš„æ¨ç†å’Œå¯è§£é‡Šçš„å†³ç­–ï¼Œæ‰€æœ‰è¿™äº›å¯¹äºå‡†ç¡®çš„å›¾åƒæ£€ç´¢è‡³å…³é‡è¦ã€‚ä¸ºäº†å®ç°ç»“æ„åŒ–å’Œç»†ç²’åº¦çš„åˆ†æï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†æŸ¥è¯¢è§£æ„æç¤ºï¼Œå°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªè¯­ä¹‰ç»„ä»¶ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„CoTRRæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå›¾åƒæ£€ç´¢ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢(TIR)ã€ç»„åˆå›¾åƒæ£€ç´¢(CIR)å’ŒåŸºäºèŠå¤©çš„å›¾åƒæ£€ç´¢(Chat-IR)ï¼‰ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å›¾åƒæ£€ç´¢æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šå¸¸åªåˆ©ç”¨MLLMè¿›è¡Œæœ€ç»ˆçš„è¯„ä¼°ï¼Œè€Œå¿½ç•¥äº†å…¶åœ¨æ’åºè¿‡ç¨‹ä¸­çš„æ½œåŠ›ã€‚è¿™å¯¼è‡´MLLMå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ²¡æœ‰è¢«å……åˆ†åˆ©ç”¨ï¼Œä»è€Œé™åˆ¶äº†æ£€ç´¢æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å…¨å±€æ¯”è¾ƒå’Œä¸€è‡´æ€§æ¨ç†çš„èƒ½åŠ›ï¼Œéš¾ä»¥åšå‡ºå‡†ç¡®çš„æ’åºå†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoTRRçš„æ ¸å¿ƒæ€è·¯æ˜¯è®©MLLMç›´æ¥å‚ä¸åˆ°å€™é€‰å›¾åƒçš„é‡æ’åºè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„æç¤º(prompt)æ¥å¼•å¯¼MLLMè¿›è¡Œé“¾å¼æ€è€ƒ(Chain-of-Thought)ã€‚è¿™ç§æ–¹æ³•å…è®¸MLLMå¯¹å€™é€‰å›¾åƒè¿›è¡Œå…¨å±€æ¯”è¾ƒï¼Œå¹¶åŸºäºä¸€è‡´çš„æ¨ç†è¿‡ç¨‹åšå‡ºæ’åºå†³ç­–ã€‚é€šè¿‡å¼•å…¥æŸ¥è¯¢è§£æ„æç¤ºï¼Œè¿›ä¸€æ­¥æå‡äº†MLLMå¯¹å¤æ‚æŸ¥è¯¢çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoTRRæ–¹æ³•ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) å€™é€‰å›¾åƒç”Ÿæˆï¼šä½¿ç”¨ç°æœ‰çš„å›¾åƒæ£€ç´¢æ¨¡å‹ç”Ÿæˆåˆå§‹çš„å€™é€‰å›¾åƒåˆ—è¡¨ã€‚2) é“¾å¼æ€è€ƒé‡æ’åºï¼šåˆ©ç”¨è®¾è®¡çš„åˆ—è¡¨å¼æ’åºæç¤ºï¼Œå¼•å¯¼MLLMå¯¹å€™é€‰å›¾åƒè¿›è¡Œé‡æ’åºã€‚è¯¥é˜¶æ®µåŒ…å«å›¾åƒè¯„ä¼°æç¤ºå’ŒæŸ¥è¯¢è§£æ„æç¤ºã€‚å›¾åƒè¯„ä¼°æç¤ºç”¨äºè¯„ä¼°æ¯ä¸ªå€™é€‰å›¾åƒä¸ç”¨æˆ·æŸ¥è¯¢çš„å¯¹é½ç¨‹åº¦ã€‚æŸ¥è¯¢è§£æ„æç¤ºå°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªè¯­ä¹‰ç»„ä»¶ï¼Œä»¥æ”¯æŒæ›´ç»†ç²’åº¦çš„åˆ†æã€‚3) æœ€ç»ˆæ’åºï¼šæ ¹æ®MLLMçš„é‡æ’åºç»“æœï¼Œç”Ÿæˆæœ€ç»ˆçš„å›¾åƒæ’åºåˆ—è¡¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoTRRçš„å…³é”®åˆ›æ–°åœ¨äºå°†MLLMç›´æ¥å¼•å…¥åˆ°å›¾åƒæ£€ç´¢çš„æ’åºè¿‡ç¨‹ä¸­ï¼Œå¹¶è®¾è®¡äº†é“¾å¼æ€è€ƒæç¤ºæ¥å¼•å¯¼MLLMè¿›è¡Œå…¨å±€æ¯”è¾ƒå’Œä¸€è‡´æ€§æ¨ç†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCoTRRèƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨MLLMçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæå‡æ£€ç´¢æ€§èƒ½ã€‚æŸ¥è¯¢è§£æ„æç¤ºçš„å¼•å…¥è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚æŸ¥è¯¢çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCoTRRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åˆ—è¡¨å¼æ’åºæç¤ºï¼šè¯¥æç¤ºå¼•å¯¼MLLMå¯¹å€™é€‰å›¾åƒè¿›è¡Œåˆ—è¡¨å¼æ’åºï¼Œå¹¶ç»™å‡ºæ’åºçš„ç†ç”±ã€‚2) å›¾åƒè¯„ä¼°æç¤ºï¼šè¯¥æç¤ºç”¨äºè¯„ä¼°æ¯ä¸ªå€™é€‰å›¾åƒä¸ç”¨æˆ·æŸ¥è¯¢çš„å¯¹é½ç¨‹åº¦ï¼Œå¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚3) æŸ¥è¯¢è§£æ„æç¤ºï¼šè¯¥æç¤ºå°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªè¯­ä¹‰ç»„ä»¶ï¼Œä¾‹å¦‚ï¼Œå°†â€œa red car in front of a buildingâ€åˆ†è§£ä¸ºâ€œred carâ€å’Œâ€œbuildingâ€ã€‚å…·ä½“çš„æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰æ˜¯å½±å“æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œéœ€è¦æ ¹æ®ä¸åŒçš„MLLMå’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoTRRåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢(TIR)ã€ç»„åˆå›¾åƒæ£€ç´¢(CIR)å’ŒåŸºäºèŠå¤©çš„å›¾åƒæ£€ç´¢(Chat-IR)ä¸‰ä¸ªä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒCoTRRç›¸æ¯”äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œæ£€ç´¢å‡†ç¡®ç‡æå‡äº†5%ä»¥ä¸Šã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†CoTRRæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoTRRæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§å›¾åƒæ£€ç´¢åœºæ™¯ï¼Œä¾‹å¦‚ç”µå•†å¹³å°çš„å•†å“æœç´¢ã€æœç´¢å¼•æ“çš„å›¾åƒæœç´¢ã€ä»¥åŠæ™ºèƒ½åŠ©æ‰‹çš„å›¾åƒç†è§£ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ£€ç´¢é¢†åŸŸçš„åº”ç”¨æä¾›æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼ŒCoTRRå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘æ£€ç´¢ã€è·¨æ¨¡æ€æ£€ç´¢ç­‰æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Image retrieval remains a fundamental yet challenging problem in computer vision. While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process. As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance. In this paper, we propose a novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue. Specifically, we design a listwise ranking prompt that enables MLLM to directly participate in re-ranking candidate images. This ranking process is grounded in an image evaluation prompt, which assesses how well each candidate aligns with users query. By allowing MLLM to perform listwise reasoning, our method supports global comparison, consistent reasoning, and interpretable decision-making - all of which are essential for accurate image retrieval. To enable structured and fine-grained analysis, we further introduce a query deconstruction prompt, which breaks down the original query into multiple semantic components. Extensive experiments on five datasets demonstrate the effectiveness of our CoTRR method, which achieves state-of-the-art performance across three image retrieval tasks, including text-to-image retrieval (TIR), composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our code is available at https://github.com/freshfish15/CoTRR .

