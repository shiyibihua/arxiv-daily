---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-18
---

# cs.CVï¼ˆ2025-09-18ï¼‰

ğŸ“Š å…± **34** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (14 ğŸ”—4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250914746v1-chain-of-thought-re-ranking-for-image-retrieval-tasks.html">Chain-of-Thought Re-ranking for Image Retrieval Tasks</a></td>
  <td>æå‡ºé“¾å¼æ€è€ƒé‡æ’åºæ–¹æ³•CoTRRï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14746v1" data-paper-url="./papers/250914746v1-chain-of-thought-re-ranking-for-image-retrieval-tasks.html" onclick="toggleFavorite(this, '2509.14746v1', 'Chain-of-Thought Re-ranking for Image Retrieval Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250915250v2-walk-and-read-less-improving-the-efficiency-of-vision-and-language-n.html">Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning</a></td>
  <td>æå‡ºå¯¼èˆªæ„ŸçŸ¥å‰ªæ(NAP)ï¼Œé€šè¿‡æ— ç›‘ç£å¤šæ¨¡æ€tokenå‰ªææå‡è§†è§‰è¯­è¨€å¯¼èˆªæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15250v2" data-paper-url="./papers/250915250v2-walk-and-read-less-improving-the-efficiency-of-vision-and-language-n.html" onclick="toggleFavorite(this, '2509.15250v2', 'Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250915293v2-how-good-are-foundation-models-in-step-by-step-embodied-reasoning.html">How Good are Foundation Models in Step-by-Step Embodied Reasoning?</a></td>
  <td>æå‡ºFoMERåŸºå‡†ï¼Œè¯„ä¼°å…·èº«ç¯å¢ƒä¸­åŸºç¡€æ¨¡å‹é€æ­¥æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15293v2" data-paper-url="./papers/250915293v2-how-good-are-foundation-models-in-step-by-step-embodied-reasoning.html" onclick="toggleFavorite(this, '2509.15293v2', 'How Good are Foundation Models in Step-by-Step Embodied Reasoning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250915178v1-unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-tem.html">Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</a></td>
  <td>åˆ©ç”¨å¤šæ¨¡æ€LLMè¿›è¡Œé›¶æ ·æœ¬æ—¶ç©ºè§†é¢‘å®šä½ï¼Œæå‡ºDSTHå’ŒTASç­–ç•¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15178v1" data-paper-url="./papers/250915178v1-unleashing-the-potential-of-multimodal-llms-for-zero-shot-spatio-tem.html" onclick="toggleFavorite(this, '2509.15178v1', 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250915132v1-from-pixels-to-urban-policy-intelligence-recovering-legacy-effects-o.html">From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal LLM</a></td>
  <td>åˆ©ç”¨å¤šæ¨¡æ€LLMä»åƒç´ åˆ°åŸå¸‚æ”¿ç­–æ™ºèƒ½ï¼šé‡ç°çº¢çº¿æ”¿ç­–çš„å†å²å½±å“</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15132v1" data-paper-url="./papers/250915132v1-from-pixels-to-urban-policy-intelligence-recovering-legacy-effects-o.html" onclick="toggleFavorite(this, '2509.15132v1', 'From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250915222v1-two-web-toolkits-for-multimodal-piano-performance-dataset-acquisitio.html">Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation</a></td>
  <td>æå‡ºç”¨äºå¤šæ¨¡æ€é’¢ç´æ¼”å¥æ•°æ®é›†é‡‡é›†ä¸æŒ‡æ³•æ ‡æ³¨çš„Webå·¥å…·åŒ…</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15222v1" data-paper-url="./papers/250915222v1-two-web-toolkits-for-multimodal-piano-performance-dataset-acquisitio.html" onclick="toggleFavorite(this, '2509.15222v1', 'Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250914921v1-trade-offs-in-cross-domain-generalization-of-foundation-model-fine-t.html">Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</a></td>
  <td>ç ”ç©¶CLIPå¾®è°ƒåœ¨ç”Ÿç‰©ç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¿‡ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ çš„æƒè¡¡</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14921v1" data-paper-url="./papers/250914921v1-trade-offs-in-cross-domain-generalization-of-foundation-model-fine-t.html" onclick="toggleFavorite(this, '2509.14921v1', 'Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250914664v1-attention-lattice-adapter-visual-explanation-generation-for-visual-f.html">Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model</a></td>
  <td>æå‡ºæ³¨æ„åŠ›æ ¼é€‚é…å™¨(ALA)ä¸äº¤æ›¿å‘¨æœŸæ¶æ„(AEA)ï¼Œç”¨äºè§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰è§£é‡Šç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14664v1" data-paper-url="./papers/250914664v1-attention-lattice-adapter-visual-explanation-generation-for-visual-f.html" onclick="toggleFavorite(this, '2509.14664v1', 'Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250918187v1-v-sensedrive-a-privacy-preserving-road-video-and-in-vehicle-sensor-f.html">V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling</a></td>
  <td>V-SenseDriveï¼šé¢å‘é“è·¯å®‰å…¨ä¸é©¾é©¶è¡Œä¸ºå»ºæ¨¡çš„éšç§ä¿æŠ¤å‹é“è·¯è§†é¢‘ä¸è½¦å†…ä¼ æ„Ÿå™¨èåˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18187v1" data-paper-url="./papers/250918187v1-v-sensedrive-a-privacy-preserving-road-video-and-in-vehicle-sensor-f.html" onclick="toggleFavorite(this, '2509.18187v1', 'V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250915435v1-orca-agentic-reasoning-for-hallucination-and-adversarial-robustness-.html">ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models</a></td>
  <td>æå‡ºORCAæ¡†æ¶ï¼Œé€šè¿‡æ™ºèƒ½ä½“æ¨ç†æå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¹»è§‰æŠ‘åˆ¶å’Œå¯¹æŠ—é²æ£’æ€§ä¸Šçš„è¡¨ç°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15435v1" data-paper-url="./papers/250915435v1-orca-agentic-reasoning-for-hallucination-and-adversarial-robustness-.html" onclick="toggleFavorite(this, '2509.15435v1', 'ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250915221v2-scalecua-scaling-open-source-computer-use-agents-with-cross-platform.html">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</a></td>
  <td>ScaleCUAï¼šé€šè¿‡è·¨å¹³å°æ•°æ®æ‰©å±•å¼€æºè®¡ç®—æœºä½¿ç”¨Agent</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15221v2" data-paper-url="./papers/250915221v2-scalecua-scaling-open-source-computer-use-agents-with-cross-platform.html" onclick="toggleFavorite(this, '2509.15221v2', 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250915059v2-quizrank-picking-images-by-quizzing-vlms.html">QuizRank: Picking Images by Quizzing VLMs</a></td>
  <td>QuizRankï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œé—®ç­”å¼å›¾åƒæ’åºï¼Œæå‡ç»´åŸºç™¾ç§‘æ–‡ç« é…å›¾è´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15059v2" data-paper-url="./papers/250915059v2-quizrank-picking-images-by-quizzing-vlms.html" onclick="toggleFavorite(this, '2509.15059v2', 'QuizRank: Picking Images by Quizzing VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250914958v2-seeing-3d-through-2d-lenses-3d-few-shot-class-incremental-learning-v.html">Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification</a></td>
  <td>æå‡ºè·¨æ¨¡æ€å‡ ä½•æ ¡æ­£ï¼ˆCMGRï¼‰æ¡†æ¶ï¼Œè§£å†³3Då°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä¸­çš„å‡ ä½•å¤±å‡†å’Œçº¹ç†åå·®é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14958v2" data-paper-url="./papers/250914958v2-seeing-3d-through-2d-lenses-3d-few-shot-class-incremental-learning-v.html" onclick="toggleFavorite(this, '2509.14958v2', 'Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250914685v2-dacon-dino-for-anime-paint-bucket-colorization-with-any-number-of-re.html">DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images</a></td>
  <td>DACoNï¼šåˆ©ç”¨DINOå’Œä»»æ„æ•°é‡å‚è€ƒå›¾åƒçš„åŠ¨æ¼«çº¿ç¨¿è‡ªåŠ¨ç€è‰²</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14685v2" data-paper-url="./papers/250914685v2-dacon-dino-for-anime-paint-bucket-colorization-with-any-number-of-re.html" onclick="toggleFavorite(this, '2509.14685v2', 'DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250914739v1-fmgs-avatar-mesh-guided-2d-gaussian-splatting-with-foundation-model-.html">FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</a></td>
  <td>FMGS-Avatarï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹å…ˆéªŒçš„ç½‘æ ¼å¼•å¯¼2Dé«˜æ–¯æº…å°„å•ç›®3Däººåƒé‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14739v1" data-paper-url="./papers/250914739v1-fmgs-avatar-mesh-guided-2d-gaussian-splatting-with-foundation-model-.html" onclick="toggleFavorite(this, '2509.14739v1', 'FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250915224v1-depth-anyevent-a-cross-modal-distillation-paradigm-for-event-based-m.html">Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation</a></td>
  <td>æå‡ºåŸºäºè·¨æ¨¡æ€è’¸é¦çš„äº‹ä»¶ç›¸æœºå•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15224v1" data-paper-url="./papers/250915224v1-depth-anyevent-a-cross-modal-distillation-paradigm-for-event-based-m.html" onclick="toggleFavorite(this, '2509.15224v1', 'Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250915472v2-efficient-multimodal-dataset-distillation-via-generative-models.html">Efficient Multimodal Dataset Distillation via Generative Models</a></td>
  <td>æå‡ºEDGEæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€æ•°æ®é›†è’¸é¦æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15472v2" data-paper-url="./papers/250915472v2-efficient-multimodal-dataset-distillation-via-generative-models.html" onclick="toggleFavorite(this, '2509.15472v2', 'Efficient Multimodal Dataset Distillation via Generative Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250915482v2-comparing-computational-pathology-foundation-models-using-representa.html">Comparing Computational Pathology Foundation Models using Representational Similarity Analysis</a></td>
  <td>åˆ©ç”¨è¡¨å¾ç›¸ä¼¼æ€§åˆ†ææ¯”è¾ƒè®¡ç®—ç—…ç†å­¦é¢†åŸŸå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ­ç¤ºå…¶è¡¨å¾ç»“æ„å·®å¼‚ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15482v2" data-paper-url="./papers/250915482v2-comparing-computational-pathology-foundation-models-using-representa.html" onclick="toggleFavorite(this, '2509.15482v2', 'Comparing Computational Pathology Foundation Models using Representational Similarity Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250915470v1-self-supervised-learning-of-imaging-and-clinical-signatures-using-a-.html">Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture</a></td>
  <td>åˆ©ç”¨å¤šæ¨¡æ€è”åˆåµŒå…¥é¢„æµ‹æ¶æ„çš„è‡ªç›‘ç£å­¦ä¹ æå‡è‚ºç»“èŠ‚è¯Šæ–­</td>
  <td class="tags-cell"><span class="paper-tag">predictive model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15470v1" data-paper-url="./papers/250915470v1-self-supervised-learning-of-imaging-and-clinical-signatures-using-a-.html" onclick="toggleFavorite(this, '2509.15470v1', 'Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250915416v1-neurorad-fm-a-foundation-model-for-neuro-oncology-with-distributiona.html">NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training</a></td>
  <td>NeuroRAD-FMï¼šåŸºäºåˆ†å¸ƒé²æ£’è®­ç»ƒçš„ç¥ç»è‚¿ç˜¤å­¦Foundation Model</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15416v1" data-paper-url="./papers/250915416v1-neurorad-fm-a-foundation-model-for-neuro-oncology-with-distributiona.html" onclick="toggleFavorite(this, '2509.15416v1', 'NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250914975v1-beyond-random-masking-a-dual-stream-approach-for-rotation-invariant-.html">Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders</a></td>
  <td>æå‡ºåŒæµæ©ç è‡ªç¼–ç å™¨ï¼Œæå‡ç‚¹äº‘åœ¨æ—‹è½¬ä¸å˜æ€§ä¸‹çš„è¡¨å¾å­¦ä¹ èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14975v1" data-paper-url="./papers/250914975v1-beyond-random-masking-a-dual-stream-approach-for-rotation-invariant-.html" onclick="toggleFavorite(this, '2509.14975v1', 'Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250915333v1-emulating-human-like-adaptive-vision-for-efficient-and-flexible-mach.html">Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</a></td>
  <td>æå‡ºAdaptiveNNï¼Œé€šè¿‡æ¨¡ä»¿äººç±»è‡ªé€‚åº”è§†è§‰å®ç°é«˜æ•ˆçµæ´»çš„æœºå™¨è§†è§‰æ„ŸçŸ¥</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">embodied AI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15333v1" data-paper-url="./papers/250915333v1-emulating-human-like-adaptive-vision-for-efficient-and-flexible-mach.html" onclick="toggleFavorite(this, '2509.15333v1', 'Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250915272v1-which-direction-to-choose-an-analysis-on-the-representation-power-of.html">Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks</a></td>
  <td>åˆ†æè‡ªç›‘ç£ViTåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨å¾èƒ½åŠ›ï¼Œæ¢ç©¶æœ€ä¼˜ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15272v1" data-paper-url="./papers/250915272v1-which-direction-to-choose-an-analysis-on-the-representation-power-of.html" onclick="toggleFavorite(this, '2509.15272v1', 'Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250915225v3-lost-in-translation-vocabulary-alignment-for-source-free-adaptation-.html">Lost in Translation? Vocabulary Alignment for Source-Free Adaptation in Open-Vocabulary Semantic Segmentation</a></td>
  <td>VocAlignï¼šé¢å‘å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ— æºåŸŸè‡ªé€‚åº”è¯æ±‡å¯¹é½æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15225v3" data-paper-url="./papers/250915225v3-lost-in-translation-vocabulary-alignment-for-source-free-adaptation-.html" onclick="toggleFavorite(this, '2509.15225v3', 'Lost in Translation? Vocabulary Alignment for Source-Free Adaptation in Open-Vocabulary Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250918184v1-urnet-uncertainty-aware-refinement-network-for-event-based-stereo-de.html">URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation</a></td>
  <td>URNetï¼šé¢å‘äº‹ä»¶ç›¸æœºç«‹ä½“æ·±åº¦ä¼°è®¡çš„ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ä¼˜åŒ–ç½‘ç»œ</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">stereo depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18184v1" data-paper-url="./papers/250918184v1-urnet-uncertainty-aware-refinement-network-for-event-based-stereo-de.html" onclick="toggleFavorite(this, '2509.18184v1', 'URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250914890v2-nerf-based-visualization-of-3d-cues-supporting-data-driven-spacecraf.html">NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation</a></td>
  <td>æå‡ºåŸºäºNeRFçš„3Dè§†è§‰çº¿ç´¢å¯è§†åŒ–æ–¹æ³•ï¼Œç”¨äºç†è§£æ•°æ®é©±åŠ¨çš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">implicit representation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14890v2" data-paper-url="./papers/250914890v2-nerf-based-visualization-of-3d-cues-supporting-data-driven-spacecraf.html" onclick="toggleFavorite(this, '2509.14890v2', 'NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250914989v1-ucorr-wire-detection-and-depth-estimation-for-autonomous-drones.html">UCorr: Wire Detection and Depth Estimation for Autonomous Drones</a></td>
  <td>æå‡ºUCorrï¼Œç”¨äºè‡ªä¸»æ— äººæœºç»†é•¿ç‰©ä½“ï¼ˆå¦‚ç”µçº¿ï¼‰çš„æ£€æµ‹ä¸æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14989v1" data-paper-url="./papers/250914989v1-ucorr-wire-detection-and-depth-estimation-for-autonomous-drones.html" onclick="toggleFavorite(this, '2509.14989v1', 'UCorr: Wire Detection and Depth Estimation for Autonomous Drones')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250915123v2-rgb-only-supervised-camera-parameter-optimization-in-dynamic-scenes.html">RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</a></td>
  <td>æå‡ºROS-Camï¼Œä»…ç”¨RGBè§†é¢‘å³å¯é«˜æ•ˆä¼˜åŒ–åŠ¨æ€åœºæ™¯ç›¸æœºå‚æ•°</td>
  <td class="tags-cell"><span class="paper-tag">metric depth</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15123v2" data-paper-url="./papers/250915123v2-rgb-only-supervised-camera-parameter-optimization-in-dynamic-scenes.html" onclick="toggleFavorite(this, '2509.15123v2', 'RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250915220v1-lightweight-and-accurate-multi-view-stereo-with-confidence-aware-dif.html">Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</a></td>
  <td>æå‡ºåŸºäºç½®ä¿¡åº¦æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆè½»é‡å¤šè§†å›¾ç«‹ä½“æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15220v1" data-paper-url="./papers/250915220v1-lightweight-and-accurate-multi-view-stereo-with-confidence-aware-dif.html" onclick="toggleFavorite(this, '2509.15220v1', 'Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250914981v3-spatialgen-layout-guided-3d-indoor-scene-generation.html">SPATIALGEN: Layout-guided 3D Indoor Scene Generation</a></td>
  <td>SpatialGenï¼šå¸ƒå±€å¼•å¯¼çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.14981v3" data-paper-url="./papers/250914981v3-spatialgen-layout-guided-3d-indoor-scene-generation.html" onclick="toggleFavorite(this, '2509.14981v3', 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250918183v1-vla-lpaf-lightweight-perspective-adaptive-fusion-for-vision-language.html">VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation</a></td>
  <td>æå‡ºVLA-LPAFè½»é‡çº§è§†è§’è‡ªé€‚åº”èåˆæ¨¡å—ï¼Œæå‡VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18183v1" data-paper-url="./papers/250918183v1-vla-lpaf-lightweight-perspective-adaptive-fusion-for-vision-language.html" onclick="toggleFavorite(this, '2509.18183v1', 'VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250915212v1-rynnvla-001-using-human-demonstrations-to-improve-robot-manipulation.html">RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</a></td>
  <td>RynnVLA-001ï¼šåˆ©ç”¨äººç±»æ¼”ç¤ºæå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œæå‡ºåŒé˜¶æ®µé¢„è®­ç»ƒVLAæ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15212v1" data-paper-url="./papers/250915212v1-rynnvla-001-using-human-demonstrations-to-improve-robot-manipulation.html" onclick="toggleFavorite(this, '2509.15212v1', 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250915045v1-synthetic-to-real-object-detection-using-yolov11-and-domain-randomiz.html">Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies</a></td>
  <td>åˆ©ç”¨YOLOv11å’ŒåŸŸéšæœºåŒ–ç­–ç•¥å®ç°åˆæˆæ•°æ®åˆ°çœŸå®åœºæ™¯çš„ç›®æ ‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">domain randomization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15045v1" data-paper-url="./papers/250915045v1-synthetic-to-real-object-detection-using-yolov11-and-domain-randomiz.html" onclick="toggleFavorite(this, '2509.15045v1', 'Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/250915490v1-smolrgpt-efficient-spatial-reasoning-for-warehouse-environments-with.html">SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters</a></td>
  <td>SmolRGPTï¼šç”¨äºä»“åº“ç¯å¢ƒçš„é«˜æ•ˆç©ºé—´æ¨ç†600Må‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15490v1" data-paper-url="./papers/250915490v1-smolrgpt-efficient-spatial-reasoning-for-warehouse-environments-with.html" onclick="toggleFavorite(this, '2509.15490v1', 'SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)