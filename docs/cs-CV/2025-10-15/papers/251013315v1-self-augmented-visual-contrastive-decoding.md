---
layout: default
title: Self-Augmented Visual Contrastive Decoding
---

# Self-Augmented Visual Contrastive Decoding

**arXiv**: [2510.13315v1](https://arxiv.org/abs/2510.13315) | [PDF](https://arxiv.org/pdf/2510.13315.pdf)

**ä½œè€…**: Eun Woo Im, Muhammad Kashif Ali, Vivek Gupta

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªå¢žå¼ºè§†è§‰å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œä»¥è§£å†³å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹å¹»è§‰é—®é¢˜**

**å…³é”®è¯**: `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `è§†è§‰å¯¹æ¯”è§£ç ` `è‡ªå¢žå¼ºæç¤º` `è‡ªé€‚åº”é˜ˆå€¼` `äº‹å®žä¸€è‡´æ€§` `è§£ç ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ç»§æ‰¿è¯­è¨€æ¨¡åž‹å¹»è§‰å€¾å‘ï¼ŒçŽ°æœ‰è§†è§‰å¯¹æ¯”è§£ç æ–¹æ³•å¿½è§†æ–‡æœ¬æŸ¥è¯¢ä¸Šä¸‹æ–‡
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è‡ªå¢žå¼ºæç¤ºç­–ç•¥åŠ¨æ€å¯¹é½è¯­ä¹‰ï¼Œè‡ªé€‚åº”é˜ˆå€¼ç®—æ³•è°ƒæ•´å€™é€‰ä»¤ç‰Œå¤§å°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªæ¨¡åž‹å’Œä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº‹å®žä¸€è‡´æ€§ï¼Œä¼˜äºŽå…ˆè¿›æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
> capabilities, but they inherit the tendency to hallucinate from their
> underlying language models. While visual contrastive decoding has been proposed
> to mitigate this issue, existing methods often apply generic visual
> augmentations that disregard the specific context provided by the text query,
> limiting their effectiveness. This study introduces a novel training-free
> decoding strategy that addresses these limitations, featuring two key
> contributions. First, a self-augmentation prompting strategy that leverages the
> intrinsic knowledge of the model to dynamically align semantics between the
> query and the visual augmentation. Second, an adaptive thresholding algorithm
> that adaptively adjusts next token candidate size based on the output sparsity,
> utilizing full information from the logit distribution. Extensive experiments
> across four LVLMs and seven benchmarks demonstrate that the proposed decoding
> significantly enhances factual consistency compared to state-of-the-art
> decoding methods. This work highlights the importance of integrating
> query-dependent augmentation and entropy-aware decoding for improving effective
> generation of LVLMs.

