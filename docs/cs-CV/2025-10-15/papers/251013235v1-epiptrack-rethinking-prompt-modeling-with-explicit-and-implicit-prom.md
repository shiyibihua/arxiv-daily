---
layout: default
title: EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking
---

# EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking

**arXiv**: [2510.13235v1](https://arxiv.org/abs/2510.13235) | [PDF](https://arxiv.org/pdf/2510.13235.pdf)

**ä½œè€…**: Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEPIPTrackæ¡†æž¶ï¼Œåˆ©ç”¨æ˜¾å¼å’Œéšå¼æç¤ºè§£å†³å¤šç›®æ ‡è·Ÿè¸ªä¸­è¯­ä¹‰æç¤ºé€‚åº”æ€§ä¸è¶³é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šç›®æ ‡è·Ÿè¸ª` `è§†è§‰è¯­è¨€æ¡†æž¶` `æ˜¾å¼æç¤º` `éšå¼æç¤º` `åŠ¨æ€å»ºæ¨¡` `è¯­ä¹‰å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–é™æ€æ–‡æœ¬æè¿°ï¼Œç¼ºä¹å¯¹ç›®æ ‡çŠ¶æ€å˜åŒ–çš„é€‚åº”æ€§ä¸”æ˜“äº§ç”Ÿå¹»è§‰ã€‚
2. EPIPTracké€šè¿‡æ˜¾å¼æç¤ºè½¬æ¢è¿åŠ¨ä¿¡æ¯ä¸ºè¯­è¨€ï¼Œéšå¼æç¤ºæž„å»ºä¸ªæ€§åŒ–çŸ¥è¯†è¡¨ç¤ºï¼ŒåŠ¨æ€è°ƒæ•´å“åº”å˜åŒ–ã€‚
3. åœ¨MOT17ã€MOT20å’ŒDanceTrackæ•°æ®é›†ä¸Šå®žéªŒï¼Œè¡¨çŽ°ä¼˜äºŽçŽ°æœ‰è·Ÿè¸ªå™¨ï¼Œå…·æœ‰å¼ºé€‚åº”æ€§å’Œé«˜æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal semantic cues, such as textual descriptions, have shown strong
> potential in enhancing target perception for tracking. However, existing
> methods rely on static textual descriptions from large language models, which
> lack adaptability to real-time target state changes and prone to
> hallucinations. To address these challenges, we propose a unified multimodal
> vision-language tracking framework, named EPIPTrack, which leverages explicit
> and implicit prompts for dynamic target modeling and semantic alignment.
> Specifically, explicit prompts transform spatial motion information into
> natural language descriptions to provide spatiotemporal guidance. Implicit
> prompts combine pseudo-words with learnable descriptors to construct
> individualized knowledge representations capturing appearance attributes. Both
> prompts undergo dynamic adjustment via the CLIP text encoder to respond to
> changes in target state. Furthermore, we design a Discriminative Feature
> Augmentor to enhance visual and cross-modal representations. Extensive
> experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack
> outperforms existing trackers in diverse scenarios, exhibiting robust
> adaptability and superior performance.

