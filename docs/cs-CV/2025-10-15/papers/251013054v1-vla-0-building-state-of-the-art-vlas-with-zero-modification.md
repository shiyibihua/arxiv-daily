---
layout: default
title: VLA-0: Building State-of-the-Art VLAs with Zero Modification
---

# VLA-0: Building State-of-the-Art VLAs with Zero Modification

**arXiv**: [2510.13054v1](https://arxiv.org/abs/2510.13054) | [PDF](https://arxiv.org/pdf/2510.13054.pdf)

**ä½œè€…**: Ankit Goyal, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLA-0æ–¹æ³•ï¼Œä»¥æ–‡æœ¬è¡¨ç¤ºåŠ¨ä½œæž„å»ºé«˜æ€§èƒ½è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æ–‡æœ¬åŠ¨ä½œè¡¨ç¤º` `æœºå™¨äººæ“ä½œ` `æ¨¡åž‹æž„å»ºæ–¹æ³•` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹æž„å»ºæ–¹æ³•å¤æ‚ï¼Œç®€å•æ–‡æœ¬åŠ¨ä½œè¡¨ç¤ºæœªè¢«å……åˆ†æŽ¢ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç›´æŽ¥ä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºåŠ¨ä½œï¼Œæ— éœ€ä¿®æ”¹è§†è§‰è¯­è¨€æ¨¡åž‹è¯æ±‡æˆ–æ·»åŠ ç‰¹æ®Šå¤´ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®žä¸–ç•Œè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action models (VLAs) hold immense promise for enabling
> generalist robot manipulation. However, the best way to build them remains an
> open question. Current approaches often add complexity, such as modifying the
> existing vocabulary of a Vision-Language Model (VLM) with action tokens or
> introducing special action heads. Curiously, the simplest strategy of
> representing actions directly as text has remained largely unexplored. This
> work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
> effective; it is surprisingly powerful. With the right design, VLA-0
> outperforms more involved models. On LIBERO, a popular benchmark for evaluating
> VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
> including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
> large-scale robotics-specific training, it outperforms methods trained on
> large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
> These findings also translate to the real world, where VLA-0 outperforms
> SmolVLA, a VLA model pre-trained on large-scale real data. This paper
> summarizes our unexpected findings and spells out the specific techniques
> required to unlock the high performance of this simple yet potent VLA design.
> Visual results, code, and trained models are provided here:
> https://vla0.github.io/.

