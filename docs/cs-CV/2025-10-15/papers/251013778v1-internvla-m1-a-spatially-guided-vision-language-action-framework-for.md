---
layout: default
title: InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy
---

# InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy

**arXiv**: [2510.13778v1](https://arxiv.org/abs/2510.13778) | [PDF](https://arxiv.org/pdf/2510.13778.pdf)

**ä½œè€…**: Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInternVLA-M1æ¡†æž¶ï¼Œé€šè¿‡ç©ºé—´å¼•å¯¼è®­ç»ƒæå‡é€šç”¨æœºå™¨äººæŒ‡ä»¤è·Ÿéšèƒ½åŠ›**

**å…³é”®è¯**: `ç©ºé—´åŸºç¡€` `è§†è§‰è¯­è¨€åŠ¨ä½œ` `æœºå™¨äººæŽ§åˆ¶` `é€šç”¨æ™ºèƒ½` `æŒ‡ä»¤è·Ÿéš`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººéš¾ä»¥å°†æŒ‡ä»¤ä¸Žè§†è§‰ç©ºé—´ä½ç½®å¯¹é½ï¼Œå½±å“é€šç”¨æ™ºèƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå…ˆç©ºé—´é¢„è®­ç»ƒå®šä½ï¼ŒåŽç©ºé—´å¼•å¯¼åŠ¨ä½œç”Ÿæˆã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå¦‚SimperEnvæå‡14.6%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce InternVLA-M1, a unified framework for spatial grounding and
> robot control that advances instruction-following robots toward scalable,
> general-purpose intelligence. Its core idea is spatially guided
> vision-language-action training, where spatial grounding serves as the critical
> link between instructions and robot actions. InternVLA-M1 employs a two-stage
> pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
> data to determine ``where to act'' by aligning instructions with visual,
> embodiment-agnostic positions, and (ii) spatially guided action post-training
> to decide ``how to act'' by generating embodiment-aware actions through
> plug-and-play spatial prompting. This spatially guided training recipe yields
> consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
> by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
> Franka, while demonstrating stronger spatial reasoning capability in box,
> point, and trace prediction. To further scale instruction following, we built a
> simulation engine to collect 244K generalizable pick-and-place episodes,
> enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
> real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
> synthetic co-training, achieved +20.6% on unseen objects and novel
> configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
> surpassed existing works by over 10%. These results highlight spatially guided
> training as a unifying principle for scalable and resilient generalist robots.
> Code and models are available at
> https://github.com/InternRobotics/InternVLA-M1.

