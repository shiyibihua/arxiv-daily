---
layout: default
title: ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition
---

# ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition

**arXiv**: [2510.13493v1](https://arxiv.org/abs/2510.13493) | [PDF](https://arxiv.org/pdf/2510.13493.pdf)

**ä½œè€…**: Deeptimaan Banerjee, Prateek Gothwal, Ashis Kumer Biswas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºExpressNet-MoEæ··åˆæ·±åº¦å­¦ä¹ æ¨¡åž‹ä»¥è§£å†³é¢éƒ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ³›åŒ–ä¸Žé€‚åº”æ€§é—®é¢˜**

**å…³é”®è¯**: `é¢éƒ¨æƒ…æ„Ÿè¯†åˆ«` `æ··åˆæ·±åº¦å­¦ä¹ ` `CNN` `MoEæ¡†æž¶` `å¤šå°ºåº¦ç‰¹å¾æå–` `è‡ªé€‚åº”æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é¢éƒ¨æƒ…æ„Ÿè¯†åˆ«åœ¨çœŸå®žåœºæ™¯ä¸­é¢ä¸´å¤´éƒ¨å§¿æ€ã€é®æŒ¡å’Œå…‰ç…§å˜åŒ–ç­‰æŒ‘æˆ˜
2. æ¨¡åž‹ç»“åˆCNNä¸ŽMoEæ¡†æž¶ï¼ŒåŠ¨æ€é€‰æ‹©ä¸“å®¶ç½‘ç»œï¼Œå®žçŽ°å¤šå°ºåº¦ç‰¹å¾æå–
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå‡†ç¡®çŽ‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¦‚AffectNetè¾¾74.77%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In many domains, including online education, healthcare, security, and
> human-computer interaction, facial emotion recognition (FER) is essential.
> Real-world FER is still difficult despite its significance because of some
> factors such as variable head positions, occlusions, illumination shifts, and
> demographic diversity. Engagement detection, which is essential for
> applications like virtual learning and customer services, is frequently
> challenging due to FER limitations by many current models. In this article, we
> propose ExpressNet-MoE, a novel hybrid deep learning model that blends both
> Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to
> overcome the difficulties. Our model dynamically chooses the most pertinent
> expert networks, thus it aids in the generalization and providing flexibility
> to model across a wide variety of datasets. Our model improves on the accuracy
> of emotion recognition by utilizing multi-scale feature extraction to collect
> both global and local facial features. ExpressNet-MoE includes numerous
> CNN-based feature extractors, a MoE module for adaptive feature selection, and
> finally a residual network backbone for deep feature learning. To demonstrate
> efficacy of our proposed model we evaluated on several datasets, and compared
> with current state-of-the-art methods. Our model achieves accuracies of 74.77%
> on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on
> FER-2013. The results show how adaptive our model is and how it may be used to
> develop end-to-end emotion recognition systems in practical settings.
> Reproducible codes and results are made publicly accessible at
> https://github.com/DeeptimaanB/ExpressNet-MoE.

