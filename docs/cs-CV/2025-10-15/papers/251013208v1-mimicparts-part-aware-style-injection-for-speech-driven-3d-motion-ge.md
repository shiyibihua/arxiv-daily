---
layout: default
title: MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation
---

# MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation

**arXiv**: [2510.13208v1](https://arxiv.org/abs/2510.13208) | [PDF](https://arxiv.org/pdf/2510.13208.pdf)

**ä½œè€…**: Lianlian Liu, YongKang He, Zhaojie Chu, Xiaofen Xing, Xiangmin Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMimicPartsæ¡†æž¶ä»¥è§£å†³è¯­éŸ³é©±åŠ¨3Dè¿åŠ¨ç”Ÿæˆä¸­çš„é£Žæ ¼å¤šæ ·æ€§å’ŒåŒºåŸŸå·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `è¯­éŸ³é©±åŠ¨è¿åŠ¨ç”Ÿæˆ` `3Däººä½“åŠ¨ç”»` `é£Žæ ¼æ³¨å…¥` `åŒºåŸŸæ„ŸçŸ¥` `åŽ»å™ªç½‘ç»œ` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ç®€åŒ–é£Žæ ¼å¤šæ ·æ€§æˆ–å¿½ç•¥èº«ä½“åŒºåŸŸè¿åŠ¨é£Žæ ¼å·®å¼‚ï¼Œé™åˆ¶è¿åŠ¨çœŸå®žæ„Ÿ
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†åŒºåŸŸé£Žæ ¼æ³¨å…¥å’ŒåŽ»å™ªç½‘ç»œï¼Œæ•æ‰ç»†ç²’åº¦åŒºåŸŸå·®å¼‚å¹¶åŠ¨æ€é€‚åº”è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„Ÿ
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºæ–¹æ³•ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆæ›´è‡ªç„¶å’Œå¯Œæœ‰è¡¨çŽ°åŠ›çš„3Däººä½“è¿åŠ¨åºåˆ—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating stylized 3D human motion from speech signals presents substantial
> challenges, primarily due to the intricate and fine-grained relationships among
> speech signals, individual styles, and the corresponding body movements.
> Current style encoding approaches either oversimplify stylistic diversity or
> ignore regional motion style differences (e.g., upper vs. lower body), limiting
> motion realism. Additionally, motion style should dynamically adapt to changes
> in speech rhythm and emotion, but existing methods often overlook this. To
> address these issues, we propose MimicParts, a novel framework designed to
> enhance stylized motion generation based on part-aware style injection and
> part-aware denoising network. It divides the body into different regions to
> encode localized motion styles, enabling the model to capture fine-grained
> regional differences. Furthermore, our part-aware attention block allows rhythm
> and emotion cues to guide each body region precisely, ensuring that the
> generated motion aligns with variations in speech rhythm and emotional state.
> Experimental results show that our method outperforming existing methods
> showcasing naturalness and expressive 3D human motion sequences.

