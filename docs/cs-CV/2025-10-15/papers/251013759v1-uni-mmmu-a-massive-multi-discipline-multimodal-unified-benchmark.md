---
layout: default
title: Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark
---

# Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark

**arXiv**: [2510.13759v1](https://arxiv.org/abs/2510.13759) | [PDF](https://arxiv.org/pdf/2510.13759.pdf)

**ä½œè€…**: Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni-MMMUåŸºå‡†ä»¥è¯„ä¼°å¤šé¢†åŸŸå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡åž‹çš„ç”Ÿæˆä¸Žç†è§£åŒå‘ååŒ**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `è§†è§‰ç”Ÿæˆ` `è§†è§‰ç†è§£` `è·¨æ¨¡æ€æŽ¨ç†` `å¤šé¢†åŸŸä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†å­¤ç«‹è¯„ä¼°è§†è§‰ç”Ÿæˆä¸Žç†è§£ï¼Œå¿½è§†å…¶å†…åœ¨è€¦åˆä»»åŠ¡
2. æž„å»ºæ¶µç›–ç§‘å­¦ã€ç¼–ç ç­‰å…«é¢†åŸŸçš„åŒå‘è€¦åˆä»»åŠ¡ï¼Œå¼ºè°ƒæŽ¨ç†æ­¥éª¤ä¸Žå¯éªŒè¯è¾“å‡º
3. è¯„ä¼°æ˜¾ç¤ºç»Ÿä¸€æ¨¡åž‹æ€§èƒ½å·®è·ï¼Œæ­ç¤ºè·¨æ¨¡æ€ä¾èµ–ä¸Žèƒ½åŠ›å¼ºåŒ–æœºåˆ¶

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unified multimodal models aim to jointly enable visual understanding and
> generation, yet current benchmarks rarely examine their true integration.
> Existing evaluations either treat the two abilities in isolation or overlook
> tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
> comprehensive and discipline-aware benchmark that systematically unfolds the
> bidirectional synergy between generation and understanding across eight
> reasoning-centric domains, including science, coding, mathematics, and puzzles.
> Each task is bidirectionally coupled, demanding models to (i) leverage
> conceptual understanding to guide precise visual synthesis, or (ii) utilize
> generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
> incorporates verifiable intermediate reasoning steps, unique ground truths, and
> a reproducible scoring protocol for both textual and visual outputs. Through
> extensive evaluation of state-of-the-art unified, generation-only, and
> understanding-only models, we reveal substantial performance disparities and
> cross-modal dependencies, offering new insights into when and how these
> abilities reinforce one another, and establishing a reliable foundation for
> advancing unified models.

