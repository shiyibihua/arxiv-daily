---
layout: default
title: Scaling Vision Transformers for Functional MRI with Flat Maps
---

# Scaling Vision Transformers for Functional MRI with Flat Maps

**arXiv**: [2510.13768v1](https://arxiv.org/abs/2510.13768) | [PDF](https://arxiv.org/pdf/2510.13768.pdf)

**ä½œè€…**: Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ‰å¹³å›¾çš„è§†è§‰å˜æ¢å™¨ï¼Œç”¨äºŽåŠŸèƒ½ç£å…±æŒ¯æˆåƒè§†é¢‘åˆ†æž**

**å…³é”®è¯**: `åŠŸèƒ½ç£å…±æŒ¯æˆåƒ` `è§†è§‰å˜æ¢å™¨` `æ‰å¹³å›¾è¡¨ç¤º` `æ—¶ç©ºæŽ©ç è‡ªç¼–ç ` `è„‘çŠ¶æ€è§£ç ` `åŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•å°†4D fMRIæ•°æ®è¡¨ç¤ºä¸ºé€‚åˆæ·±åº¦å­¦ä¹ æ¨¡åž‹çš„è¾“å…¥å½¢å¼ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†fMRIæ•°æ®è½¬æ¢ä¸º2Dæ‰å¹³å›¾è§†é¢‘ï¼Œå¹¶é‡‡ç”¨æ—¶ç©ºæŽ©ç è‡ªç¼–ç å™¨è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹æ€§èƒ½éšæ•°æ®é›†è§„æ¨¡æå‡ï¼Œæ”¯æŒè·¨å—è¯•è€…çŠ¶æ€è§£ç å’Œä¸ªä½“ç‰¹å¾è§£ç ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A key question for adapting modern deep learning architectures to functional
> MRI (fMRI) is how to represent the data for model input. To bridge the modality
> gap between fMRI and natural images, we transform the 4D volumetric fMRI data
> into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
> hours of fMRI flat map videos from the Human Connectome Project using the
> spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
> modeling performance improves with dataset size according to a strict power
> scaling law. Downstream classification benchmarks show that our model learns
> rich representations supporting both fine-grained state decoding across
> subjects, as well as subject-specific trait decoding across changes in brain
> state. This work is part of an ongoing open science project to build foundation
> models for fMRI data. Our code and datasets are available at
> https://github.com/MedARC-AI/fmri-fm.

