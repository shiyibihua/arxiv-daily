---
layout: default
title: Trace Anything: Representing Any Video in 4D via Trajectory Fields
---

# Trace Anything: Representing Any Video in 4D via Trajectory Fields

**arXiv**: [2510.13802v1](https://arxiv.org/abs/2510.13802) | [PDF](https://arxiv.org/pdf/2510.13802.pdf)

**ä½œè€…**: Xinhang Liu, Yuxi Xiao, Donny Y. Chen, Jiashi Feng, Yu-Wing Tai, Chi-Keung Tang, Bingyi Kang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½¨è¿¹åœºè¡¨ç¤ºæ³•ï¼Œé€šè¿‡å•æ¬¡å‰é¦ˆé¢„æµ‹è§†é¢‘åƒç´ è½¨è¿¹ï¼Œæå‡åŠ¨æ€å»ºæ¨¡æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `è½¨è¿¹åœºè¡¨ç¤º` `è§†é¢‘åŠ¨æ€å»ºæ¨¡` `Bæ ·æ¡å‚æ•°åŒ–` `å•æ¬¡å‰é¦ˆé¢„æµ‹` `æ—¶ç©ºèžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘åŠ¨æ€å»ºæ¨¡éœ€æœ‰æ•ˆæ—¶ç©ºè¡¨ç¤ºï¼Œåƒç´ è½¨è¿¹ä½œä¸ºåŽŸå­å•å…ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è½¨è¿¹åœºæ˜ å°„åƒç´ åˆ°è¿žç»­3Dè½¨è¿¹ï¼ŒåŸºäºŽBæ ·æ¡å‚æ•°åŒ–ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨è½¨è¿¹åœºä¼°è®¡åŸºå‡†ä¸Šè¾¾åˆ°SOTAï¼Œæ•ˆçŽ‡é«˜ä¸”å…·æ–°å…´èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Effective spatio-temporal representation is fundamental to modeling,
> understanding, and predicting dynamics in videos. The atomic unit of a video,
> the pixel, traces a continuous 3D trajectory over time, serving as the
> primitive element of dynamics. Based on this principle, we propose representing
> any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
> trajectory function of time to each pixel in every frame. With this
> representation, we introduce Trace Anything, a neural network that predicts the
> entire trajectory field in a single feed-forward pass. Specifically, for each
> pixel in each frame, our model predicts a set of control points that
> parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at
> arbitrary query time instants. We trained the Trace Anything model on
> large-scale 4D data, including data from our new platform, and our experiments
> demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
> our new benchmark for trajectory field estimation and performs competitively on
> established point-tracking benchmarks; (ii) it offers significant efficiency
> gains thanks to its one-pass paradigm, without requiring iterative optimization
> or auxiliary estimators; and (iii) it exhibits emergent abilities, including
> goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.
> Project page: https://trace-anything.github.io/.

