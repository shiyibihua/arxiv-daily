---
layout: default
title: NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching
---

# NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching

**arXiv**: [2510.13721v1](https://arxiv.org/abs/2510.13721) | [PDF](https://arxiv.org/pdf/2510.13721.pdf)

**ä½œè€…**: Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNExT-OMNIæ¨¡åž‹ï¼Œé€šè¿‡ç¦»æ•£æµåŒ¹é…å®žçŽ°ä»»æ„æ¨¡æ€é—´ç†è§£ä¸Žç”Ÿæˆ**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºç¡€æ¨¡åž‹` `ç¦»æ•£æµåŒ¹é…` `ä»»æ„æ¨¡æ€ç”Ÿæˆ` `è·¨æ¨¡æ€æ£€ç´¢` `ç»Ÿä¸€å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è‡ªå›žå½’æ¨¡åž‹éš¾ä»¥å¹³è¡¡ç†è§£ä¸Žç”Ÿæˆèƒ½åŠ›ï¼Œé™åˆ¶å¤šæ¨¡æ€åº”ç”¨
2. é‡‡ç”¨ç¦»æ•£æµèŒƒå¼ç»Ÿä¸€å»ºæ¨¡ï¼Œæ”¯æŒé«˜æ•ˆä»»æ„æ¨¡æ€è½¬æ¢ä¸Žäº¤äº’
3. åœ¨å¤§è§„æ¨¡æ•°æ®è®­ç»ƒä¸‹ï¼Œåœ¨å¤šè½®äº¤äº’å’Œè·¨æ¨¡æ€æ£€ç´¢ä¸­è¡¨çŽ°ä¼˜å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Next-generation multimodal foundation models capable of any-to-any
> cross-modal generation and multi-turn interaction will serve as core components
> of artificial general intelligence systems, playing a pivotal role in
> human-machine interaction. However, most existing multimodal models remain
> constrained by autoregressive architectures, whose inherent limitations prevent
> a balanced integration of understanding and generation capabilities. Although
> hybrid and decoupling strategies have been explored to address these tasks
> within unified frameworks separately, their redundant, non-integrated designs
> limit their applicability to broader scenarios, such as cross-modal
> retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
> foundation model that achieves unified modeling through discrete flow
> paradigms. By leveraging metric-induced probability paths and kinetic optimal
> velocities, NExT-OMNI natively supports any-to-any understanding and generation
> with enhanced response efficiency, while enabling broader application scenarios
> through concise unified representations rather than task-decoupled designs.
> Trained on large-scale interleaved text, image, video, and audio data,
> NExT-OMNI delivers competitive performance on multimodal generation and
> understanding benchmarks, while outperforming prior unified models in
> multi-turn multimodal interaction and cross-modal retrieval, highlighting its
> architectural advantages as a next-generation multimodal foundation model. To
> advance further research, we release training details, data protocols, and
> open-source both the code and model checkpoints.

