---
layout: default
title: UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations
---

# UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations

**arXiv**: [2510.13774v1](https://arxiv.org/abs/2510.13774) | [PDF](https://arxiv.org/pdf/2510.13774.pdf)

**ä½œè€…**: Dominik J. MÃ¼hlematter, Lin Che, Ye Hong, Martin Raubal, Nina Wiedemann

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUrbanFusionæ¨¡åž‹ï¼Œé€šè¿‡éšæœºå¤šæ¨¡æ€èžåˆå­¦ä¹ ç¨³å¥ç©ºé—´è¡¨ç¤ºä»¥é¢„æµ‹åŸŽå¸‚çŽ°è±¡**

**å…³é”®è¯**: `ç©ºé—´è¡¨ç¤ºå­¦ä¹ ` `å¤šæ¨¡æ€èžåˆ` `åœ°ç†äººå·¥æ™ºèƒ½` `åŸŽå¸‚é¢„æµ‹` `Transformeræ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç©ºé—´åŸºç¡€æ¨¡åž‹æ¨¡æ€æœ‰é™ä¸”ç¼ºä¹å¤šæ¨¡æ€èžåˆèƒ½åŠ›ï¼Œéš¾ä»¥æ•´åˆåœ°ç†æ•°æ®é¢„æµ‹åŸŽå¸‚çŽ°è±¡
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ¨¡æ€ç‰¹å®šç¼–ç å™¨å’ŒTransformerèžåˆæ¨¡å—ï¼Œé›†æˆè¡—æ™¯ã€é¥æ„Ÿç­‰å¤šæ¨¡æ€è¾“å…¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨41ä¸ªä»»åŠ¡å’Œ56ä¸ªåŸŽå¸‚è¯„ä¼°ä¸­ï¼Œä¼˜äºŽå…ˆè¿›æ¨¡åž‹ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºä¸”æ”¯æŒçµæ´»æ¨¡æ€è¾“å…¥

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Forecasting urban phenomena such as housing prices and public health
> indicators requires the effective integration of various geospatial data.
> Current methods primarily utilize task-specific models, while recent foundation
> models for spatial representations often support only limited modalities and
> lack multimodal fusion capabilities. To overcome these challenges, we present
> UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal
> Fusion (SMF). The framework employs modality-specific encoders to process
> different types of inputs, including street view imagery, remote sensing data,
> cartographic maps, and points of interest (POIs) data. These multimodal inputs
> are integrated via a Transformer-based fusion module that learns unified
> representations. An extensive evaluation across 41 tasks in 56 cities worldwide
> demonstrates UrbanFusion's strong generalization and predictive performance
> compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms
> prior foundation models on location-encoding, 2) allows multimodal input during
> inference, and 3) generalizes well to regions unseen during training.
> UrbanFusion can flexibly utilize any subset of available modalities for a given
> location during both pretraining and inference, enabling broad applicability
> across diverse data availability scenarios. All source code is available at
> https://github.com/DominikM198/UrbanFusion.

