---
layout: default
title: Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models
---

# Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models

**arXiv**: [2510.13237v1](https://arxiv.org/abs/2510.13237) | [PDF](https://arxiv.org/pdf/2510.13237.pdf)

**ä½œè€…**: Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡åž‹æ— å…³çš„å¯¹æŠ—æ”»å‡»EDPAä¸Žé˜²å¾¡ç­–ç•¥ï¼Œæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `å¯¹æŠ—æ”»å‡»` `æ¨¡åž‹æ— å…³é˜²å¾¡` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æœºå™¨äººå­¦ä¹ ` `è¯­ä¹‰å¯¹é½` `æ½œè¡¨ç¤ºä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨æœºå™¨äººä»»åŠ¡ä¸­å¯¹æŠ—é²æ£’æ€§ä¸è¶³ï¼Œæ˜“å—æ”»å‡»å¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šEDPAé€šè¿‡ç ´åè§†è§‰-æ–‡æœ¬è¯­ä¹‰å¯¹é½å’Œæœ€å¤§åŒ–æ½œè¡¨ç¤ºå·®å¼‚ï¼Œç”Ÿæˆå¯æ”¾ç½®çš„å¯¹æŠ—è¡¥ä¸ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†ä¸Šï¼ŒEDPAæ˜¾è‘—å¢žåŠ ä»»åŠ¡å¤±è´¥çŽ‡ï¼Œé˜²å¾¡ç­–ç•¥æœ‰æ•ˆç¼“è§£æ€§èƒ½ä¸‹é™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have achieved revolutionary progress in
> robot learning, enabling robots to execute complex physical robot tasks from
> natural language instructions. Despite this progress, their adversarial
> robustness remains underexplored. In this work, we propose both adversarial
> patch attack and corresponding defense strategies for VLA models. We first
> introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic
> adversarial attack that generates patches directly placeable within the
> camera's view. In comparison to prior methods, EDPA can be readily applied to
> different VLA models without requiring prior knowledge of the model
> architecture, or the controlled robotic manipulator. EDPA constructs these
> patches by (i) disrupting the semantic alignment between visual and textual
> latent representations, and (ii) maximizing the discrepancy of latent
> representations between adversarial and corresponding clean visual inputs.
> Through the optimization of these objectives, EDPA distorts the VLA's
> interpretation of visual information, causing the model to repeatedly generate
> incorrect actions and ultimately result in failure to complete the given
> robotic task. To counter this, we propose an adversarial fine-tuning scheme for
> the visual encoder, in which the encoder is optimized to produce similar latent
> representations for both clean and adversarially perturbed visual inputs.
> Extensive evaluations on the widely recognized LIBERO robotic simulation
> benchmark demonstrate that EDPA substantially increases the task failure rate
> of cutting-edge VLA models, while our proposed defense effectively mitigates
> this degradation. The codebase is accessible via the homepage at
> https://edpa-attack.github.io/.

