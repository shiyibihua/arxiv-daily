---
layout: default
title: Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control
---

# Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control

**arXiv**: [2510.13358v1](https://arxiv.org/abs/2510.13358) | [PDF](https://arxiv.org/pdf/2510.13358.pdf)

**ä½œè€…**: Shingo Ayabe, Hiroshi Kera, Kazuhiko Kawamoto

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—æ€§å¾®è°ƒæ¡†æ¶ä»¥å¢å¼ºç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„é²æ£’æ€§**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—æ€§å¾®è°ƒ` `æœºå™¨äººæ§åˆ¶` `é²æ£’æ€§å¢å¼º` `è‡ªé€‚åº”è¯¾ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨åŠ¨ä½œç©ºé—´æ‰°åŠ¨ä¸‹è„†å¼±ï¼Œå½±å“æœºå™¨äººæ§åˆ¶ç¨³å®šæ€§
2. é‡‡ç”¨ç¦»çº¿è®­ç»ƒååœ¨çº¿å¯¹æŠ—å¾®è°ƒï¼Œæ³¨å…¥æ‰°åŠ¨è¯±å¯¼è¡¥å¿è¡Œä¸ºæå‡éŸ§æ€§
3. å®éªŒæ˜¾ç¤ºæ–¹æ³•ä¼˜äºç¦»çº¿åŸºçº¿ï¼Œæ”¶æ•›æ›´å¿«ï¼Œè‡ªé€‚åº”è¯¾ç¨‹ç­–ç•¥ä¿æŒæ€§èƒ½

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning enables sample-efficient policy acquisition
> without risky online interaction, yet policies trained on static datasets
> remain brittle under action-space perturbations such as actuator faults. This
> study introduces an offline-to-online framework that trains policies on clean
> data and then performs adversarial fine-tuning, where perturbations are
> injected into executed actions to induce compensatory behavior and improve
> resilience. A performance-aware curriculum further adjusts the perturbation
> probability during training via an exponential-moving-average signal, balancing
> robustness and stability throughout the learning process. Experiments on
> continuous-control locomotion tasks demonstrate that the proposed method
> consistently improves robustness over offline-only baselines and converges
> faster than training from scratch. Matching the fine-tuning and evaluation
> conditions yields the strongest robustness to action-space perturbations, while
> the adaptive curriculum strategy mitigates the degradation of nominal
> performance observed with the linear curriculum strategy. Overall, the results
> show that adversarial fine-tuning enables adaptive and robust control under
> uncertain environments, bridging the gap between offline efficiency and online
> adaptability.

