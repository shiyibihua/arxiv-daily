---
layout: default
title: FlashWorld: High-quality 3D Scene Generation within Seconds
---

# FlashWorld: High-quality 3D Scene Generation within Seconds

**arXiv**: [2510.13678v1](https://arxiv.org/abs/2510.13678) | [PDF](https://arxiv.org/pdf/2510.13678.pdf)

**ä½œè€…**: Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlashWorldä»¥å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯ï¼Œä»Žå•å›¾æˆ–æ–‡æœ¬æç¤ºåœ¨ç§’çº§å®Œæˆ**

**å…³é”®è¯**: `3Dåœºæ™¯ç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `å¤šè§†å›¾ç”Ÿæˆ` `è’¸é¦è®­ç»ƒ` `é«˜æ–¯è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå¤šè§†å›¾å¯¼å‘æ–¹æ³•ç”Ÿæˆæ…¢ï¼Œ3Då¯¼å‘æ–¹æ³•è§†è§‰è´¨é‡å·®ä¸”ç¼ºä¹ä¸€è‡´æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒæ¨¡å¼é¢„è®­ç»ƒå’Œè·¨æ¨¡å¼åŽè®­ç»ƒè’¸é¦ï¼Œç»“åˆè§†é¢‘æ‰©æ•£å…ˆéªŒæå‡è´¨é‡ä¸Žæ•ˆçŽ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¯”å…ˆå‰å·¥ä½œå¿«10~100å€ï¼Œæ¸²æŸ“è´¨é‡ä¼˜è¶Šï¼Œæ³›åŒ–èƒ½åŠ›å¢žå¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose FlashWorld, a generative model that produces 3D scenes from a
> single image or text prompt in seconds, 10~100$\times$ faster than previous
> works while possessing superior rendering quality. Our approach shifts from the
> conventional multi-view-oriented (MV-oriented) paradigm, which generates
> multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach
> where the model directly produces 3D Gaussian representations during multi-view
> generation. While ensuring 3D consistency, 3D-oriented method typically suffers
> poor visual quality. FlashWorld includes a dual-mode pre-training phase
> followed by a cross-mode post-training phase, effectively integrating the
> strengths of both paradigms. Specifically, leveraging the prior from a video
> diffusion model, we first pre-train a dual-mode multi-view diffusion model,
> which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
> the quality gap in 3D-oriented generation, we further propose a cross-mode
> post-training distillation by matching distribution from consistent 3D-oriented
> mode to high-quality MV-oriented mode. This not only enhances visual quality
> while maintaining 3D consistency, but also reduces the required denoising steps
> for inference. Also, we propose a strategy to leverage massive single-view
> images and text prompts during this process to enhance the model's
> generalization to out-of-distribution inputs. Extensive experiments demonstrate
> the superiority and efficiency of our method.

