---
layout: default
title: VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator
---

# VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator

**arXiv**: [2510.13454v1](https://arxiv.org/abs/2510.13454) | [PDF](https://arxiv.org/pdf/2510.13454.pdf)

**ä½œè€…**: Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVIST3Aæ¡†æž¶ï¼Œé€šè¿‡ç¼åˆæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå™¨ä¸Ž3Dé‡å»ºç½‘ç»œå®žçŽ°æ–‡æœ¬åˆ°3Dç”Ÿæˆ**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°3Dç”Ÿæˆ` `æ¨¡åž‹ç¼åˆ` `3Dé‡å»º` `è§†é¢‘ç”Ÿæˆ` `ç›´æŽ¥å¥–åŠ±å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»“åˆæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸Ž3Dé‡å»ºæ¨¡åž‹ï¼Œä»¥ç”Ÿæˆä¸€è‡´ä¸”æ„ŸçŸ¥å¯ä¿¡çš„3Dåœºæ™¯
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ¨¡åž‹ç¼åˆæŠ€æœ¯è¿žæŽ¥ç»„ä»¶ï¼Œå¹¶é€šè¿‡ç›´æŽ¥å¥–åŠ±å¾®è°ƒå¯¹é½ç”Ÿæˆå™¨ä¸Žè§£ç å™¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºä¼˜äºŽå…ˆå‰çš„æ–‡æœ¬åˆ°3Dæ¨¡åž‹ï¼Œå¹¶æ”¯æŒé«˜è´¨é‡æ–‡æœ¬åˆ°ç‚¹äº‘ç”Ÿæˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid progress of large, pretrained models for both visual content
> generation and 3D reconstruction opens up new possibilities for text-to-3D
> generation. Intuitively, one could obtain a formidable 3D scene generator if
> one were able to combine the power of a modern latent text-to-video model as
> "generator" with the geometric abilities of a recent (feedforward) 3D
> reconstruction system as "decoder". We introduce VIST3A, a general framework
> that does just that, addressing two main challenges. First, the two components
> must be joined in a way that preserves the rich knowledge encoded in their
> weights. We revisit model stitching, i.e., we identify the layer in the 3D
> decoder that best matches the latent representation produced by the
> text-to-video generator and stitch the two parts together. That operation
> requires only a small dataset and no labels. Second, the text-to-video
> generator must be aligned with the stitched 3D decoder, to ensure that the
> generated latents are decodable into consistent, perceptually convincing 3D
> scene geometry. To that end, we adapt direct reward finetuning, a popular
> technique for human preference alignment. We evaluate the proposed VIST3A
> approach with different video generators and 3D reconstruction models. All
> tested pairings markedly improve over prior text-to-3D models that output
> Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
> enables high-quality text-to-pointmap generation.

