---
layout: default
title: Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity
---

# Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity

**arXiv**: [2510.13364v1](https://arxiv.org/abs/2510.13364) | [PDF](https://arxiv.org/pdf/2510.13364.pdf)

**ä½œè€…**: MingZe Tang, Jubal Chandy Jacob

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æç¤ºè®¾è®¡å¯¹é›¶æ ·æœ¬å¤šæ¨¡æ€åˆ†ç±»çš„å½±å“ï¼Œå‘çŽ°ç®€å•æç¤ºåœ¨æ•°æ®ç¨€ç¼ºä¸‹è¡¨çŽ°æ›´ä¼˜ã€‚**

**å…³é”®è¯**: `é›¶æ ·æœ¬åˆ†ç±»` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æç¤ºè®¾è®¡` `æ•°æ®ç¨€ç¼º` `äººä½“å§¿æ€è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæç¤ºè®¾è®¡å¦‚ä½•å½±å“è§†è§‰ç›¸ä¼¼ç±»åˆ«çš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œå¦‚åã€ç«™ã€èµ°/è·‘ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¸‰å±‚æç¤ºè®¾è®¡ï¼Œè¯„ä¼°OpenCLIPã€MetaCLIP 2å’ŒSigLipç­‰æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç®€å•æç¤ºåœ¨MetaCLIP 2å’ŒOpenCLIPä¸­å‡†ç¡®çŽ‡æœ€é«˜ï¼Œè¯¦ç»†æç¤ºå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent Vision-Language Models (VLMs) enable zero-shot classification by
> aligning images and text in a shared space, a promising approach for
> data-scarce conditions. However, the influence of prompt design on recognizing
> visually similar categories, such as human postures, is not well understood.
> This study investigates how prompt specificity affects the zero-shot
> classification of sitting, standing, and walking/running on a small, 285-image
> COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
> and SigLip, were evaluated using a three-tiered prompt design that
> systematically increases linguistic detail. Our findings reveal a compelling,
> counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
> OpenCLIP), the simplest, most basic prompts consistently achieve the best
> results. Adding descriptive detail significantly degrades performance for
> instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
> phenomenon we term "prompt overfitting". Conversely, the lower-performing
> SigLip model shows improved classification on ambiguous classes when given more
> descriptive, body-cue-based prompts.

