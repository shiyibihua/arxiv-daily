---
layout: default
title: Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning
---

# Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning

**arXiv**: [2510.13675v1](https://arxiv.org/abs/2510.13675) | [PDF](https://arxiv.org/pdf/2510.13675.pdf)

**ä½œè€…**: Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Jingcheng Wu, Nadeem Nazer, Steffen Staab

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œè§£å†³å¼€æ”¾åŸŸè§†è§‰å®žä½“è¯†åˆ«ä¸­çš„é›¶æ ·æœ¬å’Œé•¿å°¾é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¼€æ”¾åŸŸè§†è§‰å®žä½“è¯†åˆ«` `å¯¹æ¯”å­¦ä¹ ` `çŸ¥è¯†å›¾è°±` `é›¶æ ·æœ¬å­¦ä¹ ` `é•¿å°¾åˆ†å¸ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€æ”¾åŸŸè§†è§‰å®žä½“è¯†åˆ«é¢ä¸´é›¶æ ·æœ¬ã€é•¿å°¾åˆ†å¸ƒå’Œè¯­ä¹‰æ­§ä¹‰æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå›¾åƒã€æ–‡æœ¬å’ŒçŸ¥è¯†å›¾è°±ï¼Œåœ¨å…±äº«è¯­ä¹‰ç©ºé—´è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨OVENåŸºå‡†ä¸Šï¼Œå°æ¨¡åž‹å¯¹æœªè§å®žä½“å‡†ç¡®çŽ‡æå‡10.5%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Open-domain visual entity recognition aims to identify and link entities
> depicted in images to a vast and evolving set of real-world concepts, such as
> those found in Wikidata. Unlike conventional classification tasks with fixed
> label sets, it operates under open-set conditions, where most target entities
> are unseen during training and exhibit long-tail distributions. This makes the
> task inherently challenging due to limited supervision, high visual ambiguity,
> and the need for semantic disambiguation. In this work, we propose a
> Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
> images and text descriptions into a shared semantic space grounded by
> structured information from Wikidata. By abstracting visual and textual inputs
> to a conceptual level, the model leverages entity descriptions, type
> hierarchies, and relational context to support zero-shot entity recognition. We
> evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
> recognition dataset with Wikidata IDs as the label space. Our experiments show
> that using visual, textual, and structured knowledge greatly improves accuracy,
> especially for rare and unseen entities. Our smallest model improves the
> accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
> being 35 times smaller.

