---
layout: default
title: Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues
---

# Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues

**arXiv**: [2510.13620v1](https://arxiv.org/abs/2510.13620) | [PDF](https://arxiv.org/pdf/2510.13620.pdf)

**ä½œè€…**: Chen Chen, Kangcheng Bin, Ting Hu, Jiahao Qi, Xingyue Liu, Tianpeng Liu, Zhen Liu, Yongxiang Liu, Ping Zhong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPCDFæ–¹æ³•ä»¥è§£å†³æ— äººæœºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹åœ¨å¤šæ ·æ¡ä»¶ä¸‹çš„è‡ªé€‚åº”èžåˆé—®é¢˜**

**å…³é”®è¯**: `æ— äººæœºç›®æ ‡æ£€æµ‹` `å¤šæ¨¡æ€èžåˆ` `æ¡ä»¶æ„ŸçŸ¥` `RGB-IRå›¾åƒ` `åŠ¨æ€èžåˆ` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ•°æ®é›†éš¾ä»¥è¦†ç›–çœŸå®žä¸–ç•Œå¤æ‚æˆåƒæ¡ä»¶ï¼Œé™åˆ¶äº†æ— äººæœºRGB-IRç›®æ ‡æ£€æµ‹çš„é²æ£’æ€§ã€‚
2. PCDFåˆ©ç”¨æ¡ä»¶æç¤ºè‡ªé€‚åº”è°ƒæ•´å¤šæ¨¡æ€è´¡çŒ®ï¼Œé€šè¿‡è½¯é—¨æŽ§å˜æ¢å»ºæ¨¡æ¡ä»¶ä¸Žæ¨¡æ€å…³ç³»ã€‚
3. åœ¨ATR-UMODæ•°æ®é›†ä¸Šå®žéªŒéªŒè¯PCDFæœ‰æ•ˆæ€§ï¼Œæå‡æ£€æµ‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and
> infrared (IR) images facilitates robust around-the-clock detection, driven by
> advancements in deep learning techniques and the availability of high-quality
> dataset. However, the existing dataset struggles to fully capture real-world
> complexity for limited imaging conditions. To this end, we introduce a
> high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes
> from 80m to 300m, angles from 0{\deg} to 75{\deg}, and all-day, all-year time
> variations in rich weather and illumination conditions. Moreover, each RGB-IR
> image pair is annotated with 6 condition attributes, offering valuable
> high-level contextual information. To meet the challenge raised by such diverse
> conditions, we propose a novel prompt-guided condition-aware dynamic fusion
> (PCDF) to adaptively reassign multimodal contributions by leveraging annotated
> condition cues. By encoding imaging conditions as text prompts, PCDF
> effectively models the relationship between conditions and multimodal
> contributions through a task-specific soft-gating transformation. A
> prompt-guided condition-decoupling module further ensures the availability in
> practice without condition annotations. Experiments on ATR-UMOD dataset reveal
> the effectiveness of PCDF.

