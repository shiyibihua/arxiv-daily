---
layout: default
title: InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue
---

# InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue

**arXiv**: [2510.13747v1](https://arxiv.org/abs/2510.13747) | [PDF](https://arxiv.org/pdf/2510.13747.pdf)

**ä½œè€…**: Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInteractiveOmniç»Ÿä¸€å…¨æ¨¡æ€æ¨¡åž‹ï¼Œç”¨äºŽéŸ³é¢‘-è§†è§‰å¤šè½®å¯¹è¯ï¼Œå®žçŽ°è½»é‡çº§æ™ºèƒ½äº¤äº’ã€‚**

**å…³é”®è¯**: `å…¨æ¨¡æ€ç†è§£` `å¤šè½®å¯¹è¯` `è¯­éŸ³ç”Ÿæˆ` `è½»é‡çº§æ¨¡åž‹` `å¤šæ¨¡æ€åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæž„å»ºè½»é‡çº§å…¨æ¨¡æ€æ¨¡åž‹ï¼Œæ”¯æŒéŸ³é¢‘-è§†è§‰å¤šè½®äº¤äº’ä¸Žè¯­éŸ³ç”Ÿæˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè§†è§‰ã€éŸ³é¢‘ç¼–ç å™¨ã€å¤§è¯­è¨€æ¨¡åž‹å’Œè¯­éŸ³è§£ç å™¨ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽå¼€æºæ¨¡åž‹ï¼Œ4Bç‰ˆæœ¬æ€§èƒ½æŽ¥è¿‘æ›´å¤§æ¨¡åž‹ï¼Œå†…å­˜èƒ½åŠ›çªå‡ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce InteractiveOmni, a unified and open-source omni-modal large
> language model for audio-visual multi-turn interaction, ranging from 4B to 8B
> parameters, designed to lead the field of lightweight models by offering
> comprehensive omni-modal understanding and speech generation capabilities. To
> achieve this, we integrate the vision encoder, audio encoder, large language
> model, and speech decoder into a unified model for understanding and generation
> tasks. We design a multi-stage training strategy to ensure robust cross-modal
> capabilities, including pre-training for omni-modal understanding, followed by
> post-training with speech conversation and audio-visual interaction. To enable
> human-like long-term conversational ability, we meticulously curate a
> multi-turn training dataset that enhances the model's ability to handle complex
> and multi-turn interactions. To effectively evaluate the multi-turn memory and
> speech interaction capabilities, we construct the multi-modal multi-turn memory
> benchmark and the multi-turn speech interaction benchmark. Experiments
> demonstrate that InteractiveOmni significantly outperforms leading open-source
> models and provides a more intelligent multi-turn audio-visual experience,
> particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
> is comparable to the much larger model like Qwen2.5-Omni-7B on general
> benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
> while utilizing only 50% of the model size. Achieving state-of-the-art results
> against similarly sized models across image, audio, video understanding, and
> speech generation tasks, InteractiveOmni is an accessible, open-source
> foundation for next-generation intelligent interactive systems.

