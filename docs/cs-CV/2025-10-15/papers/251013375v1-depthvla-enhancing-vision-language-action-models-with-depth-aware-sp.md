---
layout: default
title: DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning
---

# DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning

**arXiv**: [2510.13375v1](https://arxiv.org/abs/2510.13375) | [PDF](https://arxiv.org/pdf/2510.13375.pdf)

**ä½œè€…**: Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDepthVLAä»¥å¢žå¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æ·±åº¦æ„ŸçŸ¥` `ç©ºé—´æŽ¨ç†` `æ··åˆTransformer` `ç«¯åˆ°ç«¯å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLAæ¨¡åž‹åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´æŽ¨ç†çš„ä»»åŠ¡ä¸­æ€§èƒ½ä¸‹é™ï¼ŒæºäºŽè§†è§‰-è¯­è¨€æ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›æœ‰é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥é¢„è®­ç»ƒæ·±åº¦é¢„æµ‹æ¨¡å—ï¼Œé‡‡ç”¨æ··åˆTransformerè®¾è®¡ç»Ÿä¸€VLMã€æ·±åº¦Transformerå’ŒåŠ¨ä½œä¸“å®¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žä¸–ç•Œå’Œæ¨¡æ‹ŸçŽ¯å¢ƒä¸­è¯„ä¼°ï¼ŒDepthVLAåœ¨å¤šä¸ªåŸºå‡†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡ä»»åŠ¡å®ŒæˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have recently shown impressive
> generalization and language-guided manipulation capabilities. However, their
> performance degrades on tasks requiring precise spatial reasoning due to
> limited spatial reasoning inherited from Vision-Language Models (VLMs).
> Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D
> space, which reduces training efficiency and is still insufficient for accurate
> spatial understanding. In this work, we present DepthVLA, a simple yet
> effective VLA architecture that explicitly incorporates spatial awareness
> through a pretrained depth prediction module. DepthVLA adopts a
> mixture-of-transformers design that unifies a VLM, a depth transformer, and an
> action expert with fully shared attentions, forming an end-to-end model with
> enhanced spatial reasoning. Extensive evaluations in both real-world and
> simulated environments show that DepthVLA outperforms state-of-the-art
> approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.
> 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.
> Our code will be made publicly available.

