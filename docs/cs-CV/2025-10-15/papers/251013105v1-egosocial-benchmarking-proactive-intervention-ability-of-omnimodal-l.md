---
layout: default
title: EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception
---

# EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception

**arXiv**: [2510.13105v1](https://arxiv.org/abs/2510.13105) | [PDF](https://arxiv.org/pdf/2510.13105.pdf)

**ä½œè€…**: Xijun Wang, Tanay Sharma, Achin Kulshrestha, Abhimitra Meka, Aveek Purohit, Dinesh Manocha

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEgoSocialåŸºå‡†ä¸ŽEgoSoDæ–¹æ³•ä»¥æå‡AR/VRä¸­AIçš„ä¸»åŠ¨å¹²é¢„èƒ½åŠ›**

**å…³é”®è¯**: `ç¬¬ä¸€äººç§°ç¤¾äº¤æ„ŸçŸ¥` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ä¸»åŠ¨å¹²é¢„åŸºå‡†` `ç¤¾äº¤åŠ¨æ€å»ºæ¨¡` `è§†é¢‘é—®ç­”æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰LLMsç¼ºä¹ä»Žç¬¬ä¸€äººç§°è§†è§’æ„ŸçŸ¥ç¤¾äº¤åŠ¨æ€çš„èƒ½åŠ›ï¼Œå¯¼è‡´å¹²é¢„æ—¶æœºä¸å½“
2. æå‡ºEgoSoDæ–¹æ³•ï¼Œæ•´åˆå¤šæ¨¡æ€çº¿ç´¢æž„å»ºç¤¾äº¤å›¾ï¼ŒåŠ¨æ€å»ºæ¨¡äº¤äº’ä¸Žå‚ä¸Žè€…
3. å®žéªŒæ˜¾ç¤ºEgoSoDæ˜¾è‘—æå‡å¹²é¢„æ—¶æœºæ£€æµ‹æ€§èƒ½ï¼Œå¦‚Phi-4æé«˜45.6%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As AR/VR technologies become integral to daily life, there's a growing need
> for AI that understands human social dynamics from an egocentric perspective.
> However, current LLMs often lack the social awareness to discern when to
> intervene as AI assistant. This leads to constant, socially unaware responses
> that may disrupt natural conversation and negatively impact user focus. To
> address these limitations, we introduce EgoSocial, a large-scale egocentric
> dataset with 13,500 social video-question pairs, specifically designed to
> benchmark intervention in social interaction perception. We also present an
> in-depth analysis of current omnimodal LLMs (OLLMs) to assess their
> effectiveness in detecting diverse social contextual cues. Experiments show
> that OLLMs still struggle to detect the intervention timing (14.4% for Gemini
> 2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method
> for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD
> integrates multimodal contextual cues (e.g., audio and visual cues) into a
> social thinking graph, dynamically modeling participants and interactions. Our
> method proactively detects intervention timing and social interactions,
> precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and
> Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4
> by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.
> We will release the dataset and code soon.

