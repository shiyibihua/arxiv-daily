---
layout: default
title: Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs
---

# Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs

**arXiv**: [2510.13795v1](https://arxiv.org/abs/2510.13795) | [PDF](https://arxiv.org/pdf/2510.13795.pdf)

**ä½œè€…**: Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜è´¨é‡æ•°æ®é›†ä¸å…¨æ ˆå·¥å…·å¥—ä»¶ä»¥æå‡å…¨å¼€æ”¾å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒæ•°æ®é›†` `é“¾å¼æ€ç»´å¢å¼º` `æ•°æ®æ¸…æ´—ç®¡é“` `å…¨å¼€æ”¾æ¨¡å‹è®­ç»ƒ` `æ¨¡å‹æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å…¨å¼€æ”¾MLLMsæ€§èƒ½è½åï¼Œä¸»è¦å› SFTæ•°æ®è´¨é‡å·®ï¼Œç¼ºä¹å¤æ‚æ¨ç†æ•°æ®å¦‚CoTã€‚
2. å¼•å…¥Honey-Data-15Mæ•°æ®é›†ï¼Œç»å¤šè½®æ¸…æ´—å’ŒåŒçº§CoTå¢å¼ºï¼Œæå‡æ•°æ®è´¨é‡ã€‚
3. è®­ç»ƒBee-8Bæ¨¡å‹ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ€§èƒ½è¾¾åˆ°SOTAï¼Œå¯ä¸åŠå¼€æ”¾æ¨¡å‹ç«äº‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fully open multimodal large language models (MLLMs) currently lag behind
> proprietary counterparts, primarily due to a significant gap in data quality
> for supervised fine-tuning (SFT). Existing open-source datasets are often
> plagued by widespread noise and a critical deficit in complex reasoning data,
> such as Chain-of-Thought (CoT), which hinders the development of advanced model
> capabilities. Addressing these challenges, our work makes three primary
> contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
> approximately 15 million QA pairs, processed through multiple cleaning
> techniques and enhanced with a novel dual-level (short and long) CoT enrichment
> strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
> underlying framework DataStudio, providing the community with a transparent and
> adaptable methodology for data curation that moves beyond static dataset
> releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
> model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
> state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
> competitive with, and in some cases surpasses, recent semi-open models such as
> InternVL3.5-8B. Our work delivers to the community a suite of foundational
> resources, including: the Honey-Data-15M corpus; the full-stack suite
> comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
> and the model weights. This effort demonstrates that a principled focus on data
> quality is a key pathway to developing fully open MLLMs that are highly
> competitive with their semi-open counterparts.

