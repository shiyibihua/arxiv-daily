---
layout: default
title: AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset
---

# AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset

**arXiv**: [2510.13630v1](https://arxiv.org/abs/2510.13630) | [PDF](https://arxiv.org/pdf/2510.13630.pdf)

**ä½œè€…**: Amjid Ali, Zulfiqar Ahmad Khan, Altaf Hussain, Muhammad Munsif, Adnan Hussain, Sung Wook Baik

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAVAR-Netè½»é‡çº§éŸ³è§†é¢‘å¼‚å¸¸è¯†åˆ«æ¡†æ¶ï¼Œè§£å†³è§†è§‰å•æ¨¡æ€åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„ä¸å¯é é—®é¢˜ã€‚**

**å…³é”®è¯**: `éŸ³è§†é¢‘å¼‚å¸¸è¯†åˆ«` `è½»é‡çº§æ¡†æ¶` `å¤šæ¨¡æ€èåˆ` `æ—¶åºå·ç§¯ç½‘ç»œ` `åŸºå‡†æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¾èµ–è§†è§‰æ•°æ®ï¼Œåœ¨é®æŒ¡ã€ä½å…‰ç…§ç­‰æ¡ä»¶ä¸‹ä¸å¯é ï¼Œä¸”ç¼ºä¹å¤§è§„æ¨¡éŸ³è§†é¢‘æ•°æ®é›†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Wav2Vec2å’ŒMobileViTæå–éŸ³è§†é¢‘ç‰¹å¾ï¼Œæ—©æœŸèåˆåé€šè¿‡MTCNå­¦ä¹ æ—¶ç©ºä¾èµ–ã€‚
3. å®éªŒæ•ˆæœï¼šåœ¨VAARæ•°æ®é›†å‡†ç¡®ç‡è¾¾89.29%ï¼ŒXD-Violenceæ•°æ®é›†å¹³å‡ç²¾åº¦æå‡2.8%ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Anomaly recognition plays a vital role in surveillance, transportation,
> healthcare, and public safety. However, most existing approaches rely solely on
> visual data, making them unreliable under challenging conditions such as
> occlusion, low illumination, and adverse weather. Moreover, the absence of
> large-scale synchronized audio-visual datasets has hindered progress in
> multimodal anomaly recognition. To address these limitations, this study
> presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition
> framework designed for real-world environments. AVAR-Net consists of four main
> modules: an audio feature extractor, a video feature extractor, fusion
> strategy, and a sequential pattern learning network that models cross-modal
> relationships for anomaly recognition. Specifically, the Wav2Vec2 model
> extracts robust temporal features from raw audio, while MobileViT captures both
> local and global visual representations from video frames. An early fusion
> mechanism combines these modalities, and a Multi-Stage Temporal Convolutional
> Network (MTCN) model that learns long-range temporal dependencies within the
> fused representation, enabling robust spatiotemporal reasoning. A novel
> Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as
> a medium-scale benchmark containing 3,000 real-world videos with synchronized
> audio across ten diverse anomaly classes. Experimental evaluations demonstrate
> that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on
> the XD-Violence dataset, improving Average Precision by 2.8% over existing
> state-of-the-art methods. These results highlight the effectiveness,
> efficiency, and generalization capability of the proposed framework, as well as
> the utility of VAAR as a benchmark for advancing multimodal anomaly recognition
> research.

