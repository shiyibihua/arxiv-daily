---
layout: default
title: Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs
---

# Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs

**arXiv**: [2510.13251v1](https://arxiv.org/abs/2510.13251) | [PDF](https://arxiv.org/pdf/2510.13251.pdf)

**ä½œè€…**: Minji Kim, Taekyung Kim, Bohyung Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºVideoLLMsä¿¡æ¯æµæœºåˆ¶ä»¥æå‡è§†é¢‘é—®ç­”æ€§èƒ½ä¸Žå¯è§£é‡Šæ€§**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡åž‹` `æœºåˆ¶å¯è§£é‡Šæ€§` `è§†é¢‘é—®ç­”` `ä¿¡æ¯æµåˆ†æž` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡åž‹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVideoLLMså†…éƒ¨ä¿¡æ¯æå–ä¸Žä¼ æ’­æœºåˆ¶ä¸æ˜Žç¡®ï¼Œå½±å“æ¨¡åž‹å¯è§£é‡Šæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯åˆ†æžä¿¡æ¯æµï¼Œè¯†åˆ«è·¨å¸§äº¤äº’ä¸Žè§†é¢‘è¯­è¨€é›†æˆæ¨¡å¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡é€‰æ‹©æœ‰æ•ˆä¿¡æ¯è·¯å¾„ï¼Œåœ¨ä¿ç•™æ€§èƒ½ä¸‹å‡å°‘58%æ³¨æ„åŠ›è¾¹ï¼Œæå‡æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Large Language Models (VideoLLMs) extend the capabilities of
> vision-language models to spatiotemporal inputs, enabling tasks such as video
> question answering (VideoQA). Despite recent advances in VideoLLMs, their
> internal mechanisms on where and how they extract and propagate video and
> textual information remain less explored. In this study, we investigate the
> internal information flow of VideoLLMs using mechanistic interpretability
> techniques. Our analysis reveals consistent patterns across diverse VideoQA
> tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame
> interactions in early-to-middle layers, (2) followed by progressive
> video-language integration in middle layers. This is facilitated by alignment
> between video representations and linguistic embeddings containing temporal
> concepts. (3) Upon completion of this integration, the model is ready to
> generate correct answers in middle-to-late layers. (4) Based on our analysis,
> we show that VideoLLMs can retain their VideoQA performance by selecting these
> effective information pathways while suppressing a substantial amount of
> attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a
> blueprint on how VideoLLMs perform temporal reasoning and offer practical
> insights for improving model interpretability and downstream generalization.
> Our project page with the source code is available at
> https://map-the-flow.github.io

