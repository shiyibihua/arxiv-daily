---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-15
---

# cs.CVï¼ˆ2025-10-15ï¼‰

ğŸ“Š å…± **46** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (18 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (18 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251013698v2-risk-adaptive-activation-steering-for-safe-multimodal-large-language.html">Risk-adaptive Activation Steering for Safe Multimodal Large Language Models</a></td>
  <td>æå‡ºé£é™©è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼(RAS)æ–¹æ³•ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§å¹¶åŠ é€Ÿæ¨ç†ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13698v2" onclick="toggleFavorite(this, '2510.13698v2', 'Risk-adaptive Activation Steering for Safe Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251013237v1-model-agnostic-adversarial-attack-and-defense-for-vision-language-ac.html">Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models</a></td>
  <td>æå‡ºé’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ¨¡å‹æ— å…³å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13237v1" onclick="toggleFavorite(this, '2510.13237v1', 'Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251013795v3-bee-a-high-quality-corpus-and-full-stack-suite-to-unlock-advanced-fu.html">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a></td>
  <td>æå‡ºHoney-Data-15Mæ•°æ®é›†å’ŒBee-8Bæ¨¡å‹ï¼Œæå‡å…¨å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13795v3" onclick="toggleFavorite(this, '2510.13795v3', 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251013759v1-uni-mmmu-a-massive-multi-discipline-multimodal-unified-benchmark.html">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></td>
  <td>æå‡ºUni-MMMUï¼šä¸€ä¸ªå¤§è§„æ¨¡å¤šå­¦ç§‘å¤šæ¨¡æ€ç»Ÿä¸€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰ç†è§£ä¸ç”Ÿæˆæ¨¡å‹çš„åŒå‘ååŒèƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13759v1" onclick="toggleFavorite(this, '2510.13759v1', 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251013620v1-fusion-meets-diverse-conditions-a-high-diversity-benchmark-and-basel.html">Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues</a></td>
  <td>æå‡ºä¸€ç§æ¡ä»¶æ„ŸçŸ¥çš„åŠ¨æ€èåˆæ–¹æ³•ï¼Œç”¨äºè§£å†³æ— äººæœºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é²æ£’æ€§é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13620v1" onclick="toggleFavorite(this, '2510.13620v1', 'Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251013364v1-language-as-a-label-zero-shot-multimodal-classification-of-everyday-.html">Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity</a></td>
  <td>åˆ©ç”¨è¯­è¨€æ ‡ç­¾è¿›è¡Œé›¶æ ·æœ¬å¤šæ¨¡æ€åˆ†ç±»ï¼Œè§£å†³æ•°æ®ç¨€ç¼ºä¸‹çš„æ—¥å¸¸å§¿æ€è¯†åˆ«é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13364v1" onclick="toggleFavorite(this, '2510.13364v1', 'Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251013131v1-os-hgadapter-open-semantic-hypergraph-adapter-for-large-language-mod.html">OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment</a></td>
  <td>æå‡ºOS-HGAdapterï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¢å¼ºå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œæ˜¾è‘—æå‡è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13131v1" onclick="toggleFavorite(this, '2510.13131v1', 'OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251013800v2-reasoning-in-space-via-grounding-in-the-world.html">Reasoning in Space via Grounding in the World</a></td>
  <td>æå‡ºåŸºäºä¸–ç•Œæ„ŸçŸ¥çš„Grounded-Spatial Reasonerï¼Œç”¨äºæå‡3Dç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13800v2" onclick="toggleFavorite(this, '2510.13800v2', 'Reasoning in Space via Grounding in the World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251013756v1-recode-reasoning-through-code-generation-for-visual-question-answeri.html">RECODE: Reasoning Through Code Generation for Visual Question Answering</a></td>
  <td>æå‡ºRECODEæ¡†æ¶ï¼Œé€šè¿‡ä»£ç ç”Ÿæˆå®ç°è§†è§‰é—®ç­”ä¸­æ›´ç²¾ç¡®çš„å¯éªŒè¯æ¨ç†ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13756v1" onclick="toggleFavorite(this, '2510.13756v1', 'RECODE: Reasoning Through Code Generation for Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251013660v2-omnigaze-reward-inspired-generalizable-gaze-estimation-in-the-wild.html">OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild</a></td>
  <td>OmniGazeï¼šæå‡ºå¥–åŠ±é©±åŠ¨çš„é€šç”¨å‡è§†ä¼°è®¡æ¡†æ¶ï¼Œè§£å†³é‡å¤–åœºæ™¯æ³›åŒ–æ€§é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13660v2" onclick="toggleFavorite(this, '2510.13660v2', 'OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251014032v1-vgent-graph-based-retrieval-reasoning-augmented-generation-for-long-.html">Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</a></td>
  <td>æå‡ºVgentï¼Œé€šè¿‡å›¾ç»“æ„æ£€ç´¢-æ¨ç†å¢å¼ºç”Ÿæˆï¼Œæå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14032v1" onclick="toggleFavorite(this, '2510.14032v1', 'Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251013787v1-adaptive-visual-conditioning-for-semantic-consistency-in-diffusion-b.html">Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation</a></td>
  <td>æå‡ºAVCæ¡†æ¶ï¼Œè‡ªé€‚åº”è§†è§‰æ¡ä»¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œæå‡æ•…äº‹å»¶ç»­ç”Ÿæˆè¯­ä¹‰ä¸€è‡´æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13787v1" onclick="toggleFavorite(this, '2510.13787v1', 'Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251013747v2-interactiveomni-a-unified-omni-modal-model-for-audio-visual-multi-tu.html">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a></td>
  <td>æå‡ºInteractiveOmniï¼Œä¸€ä¸ªç”¨äºéŸ³è§†é¢‘å¤šè½®äº¤äº’çš„ç»Ÿä¸€å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13747v2" onclick="toggleFavorite(this, '2510.13747v2', 'InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251013643v1-towards-adversarial-robustness-and-uncertainty-quantification-in-din.html">Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection</a></td>
  <td>ç ”ç©¶DINOv2åœ¨å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­çš„å¯¹æŠ—é²æ£’æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13643v1" onclick="toggleFavorite(this, '2510.13643v1', 'Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251013316v1-visual-interestingness-decoded-how-gpt-4o-mirrors-human-interests.html">Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests</a></td>
  <td>æ¢ç´¢GPT-4oå¯¹è§†è§‰è¶£å‘³æ€§çš„ç†è§£ï¼Œå¹¶ç”¨äºæå‡å­¦ä¹ æ’åºæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13316v1" onclick="toggleFavorite(this, '2510.13316v1', 'Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251013315v1-self-augmented-visual-contrastive-decoding.html">Self-Augmented Visual Contrastive Decoding</a></td>
  <td>æå‡ºè‡ªå¢å¼ºè§†è§‰å¯¹æ¯”è§£ç ï¼Œæå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13315v1" onclick="toggleFavorite(this, '2510.13315v1', 'Self-Augmented Visual Contrastive Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251013276v1-mmlongcite-a-benchmark-for-evaluating-fidelity-of-long-context-visio.html">MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models</a></td>
  <td>æå‡ºMMLongCiteåŸºå‡†ï¼Œè¯„ä¼°é•¿ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯ä¿çœŸåº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13276v1" onclick="toggleFavorite(this, '2510.13276v1', 'MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251013232v1-what-not-to-detect-negation-aware-vlms-via-structured-reasoning-and-.html">What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging</a></td>
  <td>æå‡ºNegToMeæ¨¡å—å’ŒCoVANDæ•°æ®é›†ï¼Œæå‡VLMåœ¨å¦å®šæè¿°å¯¹è±¡æ£€æµ‹ä¸­çš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13232v1" onclick="toggleFavorite(this, '2510.13232v1', 'What &quot;Not&quot; to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251013809v1-physmaster-mastering-physical-representation-for-video-generation-vi.html">PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning</a></td>
  <td>æå‡ºPhysMasterï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç‰©ç†è¡¨å¾ï¼Œæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†åˆç†æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13809v1" onclick="toggleFavorite(this, '2510.13809v1', 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251013565v1-xd-rcdepth-lightweight-radar-camera-depth-estimation-with-explainabi.html">XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation</a></td>
  <td>XD-RCDepthï¼šé¢å‘è‡ªåŠ¨é©¾é©¶ï¼Œæå‡ºè½»é‡çº§é›·è¾¾-ç›¸æœºæ·±åº¦ä¼°è®¡ä¸å¯è§£é‡Šæ€§å¯¹é½çš„çŸ¥è¯†è’¸é¦æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13565v1" onclick="toggleFavorite(this, '2510.13565v1', 'XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251013804v1-generative-universal-verifier-as-multimodal-meta-reasoner.html">Generative Universal Verifier as Multimodal Meta-Reasoner</a></td>
  <td>æå‡ºç”Ÿæˆå¼é€šç”¨éªŒè¯å™¨ï¼Œèµ‹èƒ½å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè§†è§‰ç»“æœåæ€ä¸ä¼˜åŒ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13804v1" onclick="toggleFavorite(this, '2510.13804v1', 'Generative Universal Verifier as Multimodal Meta-Reasoner')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251013515v3-unime-v2-mllm-as-a-judge-for-universal-multimodal-embedding-learning.html">UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</a></td>
  <td>UniME-V2ï¼šåˆ©ç”¨MLLMä½œä¸ºåˆ¤åˆ«å™¨è¿›è¡Œé€šç”¨å¤šæ¨¡æ€åµŒå…¥å­¦ä¹ </td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13515v3" onclick="toggleFavorite(this, '2510.13515v3', 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251013390v1-generalizing-wifi-gesture-recognition-via-large-model-aware-semantic.html">Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment</a></td>
  <td>æå‡ºGLSDAæ¡†æ¶ï¼Œåˆ©ç”¨å¤§æ¨¡å‹è¯­ä¹‰çŸ¥è¯†æå‡WiFiæ‰‹åŠ¿è¯†åˆ«æ³›åŒ–èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13390v1" onclick="toggleFavorite(this, '2510.13390v1', 'Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251013253v1-end-to-end-multi-modal-diffusion-mamba.html">End-to-End Multi-Modal Diffusion Mamba</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ‰©æ•£Mambaï¼ˆMDMï¼‰ï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†å¹¶æå‡ç”Ÿæˆæ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13253v1" onclick="toggleFavorite(this, '2510.13253v1', 'End-to-End Multi-Modal Diffusion Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251013768v1-scaling-vision-transformers-for-functional-mri-with-flat-maps.html">Scaling Vision Transformers for Functional MRI with Flat Maps</a></td>
  <td>åˆ©ç”¨å¹³é¢å›¾å’Œè§†è§‰Transformeræ‰©å±•åŠŸèƒ½ç£å…±æŒ¯æˆåƒç ”ç©¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13768v1" onclick="toggleFavorite(this, '2510.13768v1', 'Scaling Vision Transformers for Functional MRI with Flat Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251017858v1-shortcutting-pre-trained-flow-matching-diffusion-models-is-almost-fr.html">Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch</a></td>
  <td>æå‡ºåŸºäºé€Ÿåº¦åœºè‡ªè’¸é¦çš„Flow Matchingæ¨¡å‹åŠ é€Ÿæ–¹æ³•ï¼Œå®ç°é«˜æ•ˆå°‘æ­¥é‡‡æ ·</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17858v1" onclick="toggleFavorite(this, '2510.17858v1', 'Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251013675v2-seeing-and-knowing-in-the-wild-open-domain-visual-entity-recognition.html">Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning</a></td>
  <td>æå‡ºçŸ¥è¯†å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¼€æ”¾åŸŸè§†è§‰å®ä½“è¯†åˆ«é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13675v2" onclick="toggleFavorite(this, '2510.13675v2', 'Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251013243v1-flyawarev2-a-multimodal-cross-domain-uav-dataset-for-urban-scene-und.html">FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding</a></td>
  <td>FlyAwareV2ï¼šç”¨äºåŸå¸‚åœºæ™¯ç†è§£çš„å¤šæ¨¡æ€è·¨åŸŸæ— äººæœºæ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13243v1" onclick="toggleFavorite(this, '2510.13243v1', 'FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251017864v1-insideout-integrated-rgb-radiative-gaussian-splatting-for-comprehens.html">InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation</a></td>
  <td>InsideOutï¼šé›†æˆRGBä¸è¾å°„é«˜æ–¯æº…å°„çš„ç»¼åˆ3Dç‰©ä½“è¡¨ç¤º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17864v1" onclick="toggleFavorite(this, '2510.17864v1', 'InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251013186v4-stt-gs-sample-then-transmit-edge-gaussian-splatting-with-joint-clien.html">STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control</a></td>
  <td>æå‡ºSTT-GSè¾¹ç¼˜é«˜æ–¯æº…å°„æ–¹æ³•ï¼Œè”åˆä¼˜åŒ–å®¢æˆ·ç«¯é€‰æ‹©å’ŒåŠŸç‡æ§åˆ¶ï¼Œæå‡ä½ç©ºåœºæ™¯é‡å»ºè´¨é‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13186v4" onclick="toggleFavorite(this, '2510.13186v4', 'STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251013381v1-leveraging-2d-priors-and-sdf-guidance-for-dynamic-urban-scene-render.html">Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering</a></td>
  <td>æå‡ºç»“åˆ2Då…ˆéªŒä¸SDFå¼•å¯¼çš„åŠ¨æ€åŸå¸‚åœºæ™¯æ¸²æŸ“æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13381v1" onclick="toggleFavorite(this, '2510.13381v1', 'Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251013546v1-accelerated-feature-detectors-for-visual-slam-a-comparative-study-of.html">Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU</a></td>
  <td>å¯¹æ¯”FPGAä¸GPUåŠ é€Ÿçš„ç‰¹å¾æ£€æµ‹å™¨åœ¨è§†è§‰SLAMä¸­çš„æ€§èƒ½ä¸èƒ½æ•ˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13546v1" onclick="toggleFavorite(this, '2510.13546v1', 'Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251013317v1-removing-cost-volumes-from-optical-flow-estimators.html">Removing Cost Volumes from Optical Flow Estimators</a></td>
  <td>æå‡ºä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œå¯åœ¨å…‰æµä¼°è®¡ä¸­ç§»é™¤ä»£ä»·ä½“ï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜å ç”¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13317v1" onclick="toggleFavorite(this, '2510.13317v1', 'Removing Cost Volumes from Optical Flow Estimators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251014081v3-capture-canonicalize-splat-zero-shot-3d-gaussian-avatars-from-unstru.html">Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images</a></td>
  <td>æå‡ºCapture, Canonicalize, Splaté›¶æ ·æœ¬3Dé«˜æ–¯å¤´åƒç”Ÿæˆæ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14081v3" onclick="toggleFavorite(this, '2510.14081v3', 'Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251013310v1-instantsfm-fully-sparse-and-parallel-structure-from-motion.html">InstantSfM: Fully Sparse and Parallel Structure-from-Motion</a></td>
  <td>InstantSfMï¼šå…¨ç¨€ç–å¹¶è¡ŒStructure-from-Motionï¼ŒåŠ é€Ÿå¤§è§„æ¨¡åœºæ™¯é‡å»ºã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13310v1" onclick="toggleFavorite(this, '2510.13310v1', 'InstantSfM: Fully Sparse and Parallel Structure-from-Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/251013375v1-depthvla-enhancing-vision-language-action-models-with-depth-aware-sp.html">DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</a></td>
  <td>DepthVLAï¼šé€šè¿‡æ·±åº¦æ„ŸçŸ¥çš„ç©ºé—´æ¨ç†å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13375v1" onclick="toggleFavorite(this, '2510.13375v1', 'DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251013802v1-trace-anything-representing-any-video-in-4d-via-trajectory-fields.html">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a></td>
  <td>Trace Anythingï¼šæå‡ºåŸºäºè½¨è¿¹åœºçš„è§†é¢‘4Dè¡¨ç¤ºæ–¹æ³•ï¼Œå®ç°é«˜æ•ˆæ—¶ç©ºå»ºæ¨¡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13802v1" onclick="toggleFavorite(this, '2510.13802v1', 'Trace Anything: Representing Any Video in 4D via Trajectory Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251013793v1-noiseprints-distortion-free-watermarks-for-authorship-in-private-dif.html">NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models</a></td>
  <td>æå‡ºNoisePrintsï¼Œä¸€ç§ç”¨äºç§æœ‰æ‰©æ•£æ¨¡å‹ä¸­æ— å¤±çœŸæ°´å°çš„ä½œè€…èº«ä»½éªŒè¯æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13793v1" onclick="toggleFavorite(this, '2510.13793v1', 'NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/251013235v1-epiptrack-rethinking-prompt-modeling-with-explicit-and-implicit-prom.html">EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking</a></td>
  <td>EPIPTrackï¼šåˆ©ç”¨æ˜¾å¼å’Œéšå¼æç¤ºè¿›è¡Œå¤šç›®æ ‡è·Ÿè¸ªçš„æç¤ºå»ºæ¨¡æ–°æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13235v1" onclick="toggleFavorite(this, '2510.13235v1', 'EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/251013251v1-map-the-flow-revealing-hidden-pathways-of-information-in-videollms.html">Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</a></td>
  <td>æ­ç¤ºVideoLLMä¿¡æ¯æµåŠ¨è·¯å¾„ï¼šé€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§åˆ†ææ—¶åºæ¨ç†è¿‡ç¨‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13251v1" onclick="toggleFavorite(this, '2510.13251v1', 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/251013137v2-real-time-sign-language-to-text-translation-using-deep-learning-a-co.html">Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN</a></td>
  <td>å¯¹æ¯”LSTMä¸3D CNNï¼Œå®ç°å®æ—¶æ‰‹è¯­åˆ°æ–‡æœ¬çš„æ·±åº¦å­¦ä¹ ç¿»è¯‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13137v2" onclick="toggleFavorite(this, '2510.13137v2', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/251013208v1-mimicparts-part-aware-style-injection-for-speech-driven-3d-motion-ge.html">MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation</a></td>
  <td>MimicPartsï¼šç”¨äºè¯­éŸ³é©±åŠ¨3Däººä½“åŠ¨ä½œç”Ÿæˆçš„éƒ¨ä»¶æ„ŸçŸ¥é£æ ¼æ³¨å…¥æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13208v1" onclick="toggleFavorite(this, '2510.13208v1', 'MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/251013669v1-canvasmar-improving-masked-autoregressive-video-generation-with-canv.html">CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas</a></td>
  <td>CanvasMARï¼šé€šè¿‡ç”»å¸ƒæœºåˆ¶æ”¹è¿›æ©ç è‡ªå›å½’è§†é¢‘ç”Ÿæˆï¼Œè§£å†³æ…¢å¯åŠ¨å’Œè¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13669v1" onclick="toggleFavorite(this, '2510.13669v1', 'CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/251013331v2-group-wise-optimization-for-self-extensible-codebooks-in-vector-quan.html">Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models</a></td>
  <td>æå‡ºGroup-VQï¼Œé€šè¿‡åˆ†ç»„ä¼˜åŒ–è‡ªæ‰©å±•ç ä¹¦è§£å†³VQ-VAEä¸­çš„ç ä¹¦åå¡Œé—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13331v2" onclick="toggleFavorite(this, '2510.13331v2', 'Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>45</td>
  <td><a href="./papers/251013702v1-mvcustom-multi-view-customized-diffusion-via-geometric-latent-render.html">MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion</a></td>
  <td>MVCustomï¼šé€šè¿‡å‡ ä½•æ½œåœ¨æ¸²æŸ“å’Œè¡¥å…¨å®ç°å¤šè§†è§’å®šåˆ¶åŒ–æ‰©æ•£æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13702v1" onclick="toggleFavorite(this, '2510.13702v1', 'MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/251013808v1-viscop-visual-probing-for-video-domain-adaptation-of-vision-language.html">VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models</a></td>
  <td>VisCoPï¼šé€šè¿‡è§†è§‰æ¢é’ˆå®ç°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘é¢†åŸŸçš„åŸŸè‡ªé€‚åº”</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.13808v1" onclick="toggleFavorite(this, '2510.13808v1', 'VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)