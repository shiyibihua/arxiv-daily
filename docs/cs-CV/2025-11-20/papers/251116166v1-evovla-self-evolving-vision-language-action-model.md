---
layout: default
title: EvoVLA: Self-Evolving Vision-Language-Action Model
---

# EvoVLA: Self-Evolving Vision-Language-Action Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.16166" target="_blank" class="toolbar-btn">arXiv: 2511.16166v1</a>
    <a href="https://arxiv.org/pdf/2511.16166.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16166v1" 
            onclick="toggleFavorite(this, '2511.16166v1', 'EvoVLA: Self-Evolving Vision-Language-Action Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zeting Liu, Zida Yang, Zeyu Zhang, Hao Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-20

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/AIGeeksGroup/EvoVLA) | [PROJECT_PAGE](https://aigeeksgroup.github.io/EvoVLA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**EvoVLAÔºö‰∏ÄÁßçËá™ËøõÂåñËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºåËß£ÂÜ≥ÈïøÊó∂Á®ãÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÈò∂ÊÆµÂπªËßâÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÈïøÊó∂Á®ã‰ªªÂä°` `Ëá™ÁõëÁù£Â≠¶‰π†` `Èò∂ÊÆµÂπªËßâ` `ÂØπÊØîÂ≠¶‰π†` `sim-to-real`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®ÈïøÊó∂Á®ãÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠Â≠òÂú®Èò∂ÊÆµÂπªËßâÈóÆÈ¢òÔºåÂç≥Âà©Áî®Á≤óÁ≥ôÁöÑËØÑ‰º∞‰ø°Âè∑Êù•Ë∑≥ËøáÂ§öÊ≠•È™§‰ªªÂä°„ÄÇ
2. EvoVLAÈÄöËøáÈò∂ÊÆµÂØπÈΩêÂ•ñÂä±„ÄÅÂü∫‰∫éÂßøÊÄÅÁöÑÁâ©‰ΩìÊé¢Á¥¢ÂíåÈïøÊó∂Á®ãËÆ∞ÂøÜ‰∏â‰∏™ÁªÑ‰ª∂ÔºåËß£ÂÜ≥Èò∂ÊÆµÂπªËßâÈóÆÈ¢òÔºåÊèêÂçáÈïøÊó∂Á®ãÊìç‰ΩúÊÄßËÉΩ„ÄÇ
3. EvoVLAÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫EvoVLAÔºå‰∏ÄÁßçËá™ÁõëÁù£ÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÈïøÊó∂Á®ãÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠Â≠òÂú®ÁöÑÈò∂ÊÆµÂπªËßâÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∫íË°•ÁªÑ‰ª∂ÔºöÈò∂ÊÆµÂØπÈΩêÂ•ñÂä±ÔºàSARÔºâÔºåÈÄöËøá‰∏éGeminiÁîüÊàêÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ËøõË°å‰∏âÂÖÉÁªÑÂØπÊØîÂ≠¶‰π†Êù•Èò≤Ê≠¢ËßÜËßâÊç∑ÂæÑÔºõÂü∫‰∫éÂßøÊÄÅÁöÑÁâ©‰ΩìÊé¢Á¥¢ÔºàPOEÔºâÔºåÂ∞ÜÂ•ΩÂ•áÂøÉÂª∫Á´ãÂú®Áõ∏ÂØπÁâ©‰Ωì-Â§πÁà™ÂßøÊÄÅ‰∏äÔºåËÄåÈùûÂéüÂßãÂÉèÁ¥†Ôºõ‰ª•ÂèäÈïøÊó∂Á®ãËÆ∞ÂøÜÔºåÂà©Áî®ÈÄâÊã©ÊÄß‰∏ä‰∏ãÊñá‰øùÁïôÂíåÈó®ÊéßËûçÂêàÊù•Á®≥ÂÆöÊâ©Â±ïrolloutÊúüÈó¥ÁöÑÂÜÖÂú®Â°ëÈÄ†„ÄÇÂú®Discoverse-LÂü∫ÂáÜÊµãËØï‰∏≠ÔºåEvoVLAÊØîÊúÄÂº∫ÁöÑÂü∫Á∫øÔºàOpenVLA-OFTÔºâÂπ≥Âùá‰ªªÂä°ÊàêÂäüÁéáÊèêÈ´ò‰∫Ü10.2‰∏™ÁôæÂàÜÁÇπÔºåËææÂà∞69.2%„ÄÇEvoVLAÁöÑÊ†∑Êú¨ÊïàÁéáÊèêÈ´ò‰∫Ü1.5ÂÄçÔºåÈò∂ÊÆµÂπªËßâ‰ªé38.5%Èôç‰ΩéÂà∞14.8%„ÄÇÂú®Áâ©ÁêÜÊú∫Âô®‰∫∫‰∏äÁöÑÁúüÂÆû‰∏ñÁïåÈÉ®ÁΩ≤‰∏≠ÔºåEvoVLAÂú®Âõõ‰∏™Êìç‰Ωú‰ªªÂä°‰∏äÁöÑÂπ≥ÂùáÊàêÂäüÁéá‰∏∫54.6%ÔºåË∂ÖËøáOpenVLA-OFT 11‰∏™ÁôæÂàÜÁÇπÔºåËØÅÊòé‰∫ÜÊúâÊïàÁöÑsim-to-realËøÅÁßªÂíåÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®ÈïøÊó∂Á®ãÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåÂÆπÊòìÂá∫Áé∞‚ÄúÈò∂ÊÆµÂπªËßâ‚ÄùÈóÆÈ¢ò„ÄÇËøôÊÑèÂë≥ÁùÄÊô∫ËÉΩ‰Ωì‰∏∫‰∫ÜËé∑ÂæóÊõ¥È´òÁöÑÂ•ñÂä±Ôºå‰ºöÂà©Áî®‰∏Ä‰∫õÁ≤óÁ≥ôÁöÑËßÜËßâ‰ø°Âè∑Êù•‚ÄúÊ¨∫È™ó‚ÄùÁ≥ªÁªüÔºå‰æãÂ¶ÇÔºåÂú®‰ªªÂä°Â∞öÊú™ÁúüÊ≠£ÂÆåÊàêÊó∂Â∞±Êä•ÂëäÂæàÈ´òÁöÑËøõÂ∫¶Ôºå‰ªéËÄåÂØºËá¥‰ªªÂä°Â§±Ë¥•„ÄÇËøôÁßçÁé∞Ë±°ÈòªÁ¢ç‰∫ÜVLAÊ®°ÂûãÂú®Â§çÊùÇÊìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEvoVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÂºèÔºåËÆ©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£‰ªªÂä°ÁöÑÁúüÂÆûËøõÂ±ïÔºå‰ªéËÄåÈÅøÂÖçÈò∂ÊÆµÂπªËßâ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÈÄöËøá‰∏â‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöÈò∂ÊÆµÂØπÈΩêÂ•ñÂä±ÔºàSARÔºâ„ÄÅÂü∫‰∫éÂßøÊÄÅÁöÑÁâ©‰ΩìÊé¢Á¥¢ÔºàPOEÔºâÂíåÈïøÊó∂Á®ãËÆ∞ÂøÜÔºåÊù•ÂàÜÂà´Ëß£ÂÜ≥ËßÜËßâÊç∑ÂæÑ„ÄÅÊé¢Á¥¢ÊïàÁéáÂíåÈïøÊúü‰æùËµñÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEvoVLAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜËßâÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÊèêÂèñÂú∫ÊôØÁöÑËßÜËßâÁâπÂæÅ„ÄÇ2) ËØ≠Ë®ÄÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÁêÜËß£‰ªªÂä°Êåá‰ª§„ÄÇ3) Âä®‰ΩúËß£Á†ÅÂô®ÔºöÊ†πÊçÆËßÜËßâÁâπÂæÅ„ÄÅËØ≠Ë®ÄÊåá‰ª§ÂíåÂéÜÂè≤ËÆ∞ÂøÜÁîüÊàêÂä®‰Ωú„ÄÇ4) Èò∂ÊÆµÂØπÈΩêÂ•ñÂä±Ê®°ÂùóÔºöÈÄöËøáÂØπÊØîÂ≠¶‰π†ÔºåÈºìÂä±Ê®°ÂûãÂÖ≥Ê≥®‰ªªÂä°ÁöÑÁúüÂÆûËøõÂ±ï„ÄÇ5) Âü∫‰∫éÂßøÊÄÅÁöÑÁâ©‰ΩìÊé¢Á¥¢Ê®°ÂùóÔºöÂºïÂØºÊ®°ÂûãÊé¢Á¥¢ÊúâÊÑè‰πâÁöÑÁâ©‰Ωì‰∫§‰∫í„ÄÇ6) ÈïøÊó∂Á®ãËÆ∞ÂøÜÊ®°ÂùóÔºöÂ≠òÂÇ®ÂíåÊ£ÄÁ¥¢ÂéÜÂè≤‰ø°ÊÅØÔºåÂ∏ÆÂä©Ê®°ÂûãÁêÜËß£ÈïøÊúü‰æùËµñÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEvoVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‰∏â‰∏™‰∫íË°•ÁöÑÁªÑ‰ª∂ÔºöSARÈÄöËøáGeminiÁîüÊàêÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ËøõË°åÂØπÊØîÂ≠¶‰π†ÔºåÊúâÊïàÈò≤Ê≠¢‰∫ÜËßÜËßâÊç∑ÂæÑÔºõPOEÂ∞ÜÂ•ΩÂ•áÂøÉÂª∫Á´ãÂú®Áõ∏ÂØπÁâ©‰Ωì-Â§πÁà™ÂßøÊÄÅ‰∏äÔºåÊèêÈ´ò‰∫ÜÊé¢Á¥¢ÊïàÁéáÔºõÈïøÊó∂Á®ãËÆ∞ÂøÜÂàôÈÄöËøáÈÄâÊã©ÊÄß‰∏ä‰∏ãÊñá‰øùÁïôÂíåÈó®ÊéßËûçÂêàÔºåÁ®≥ÂÆö‰∫ÜÈïøÊúürollout‰∏≠ÁöÑÂÜÖÂú®Â°ëÈÄ†„ÄÇËøô‰∫õÁªÑ‰ª∂ÂÖ±Âêå‰ΩúÁî®ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÈò∂ÊÆµÂπªËßâÔºåÊèêÂçá‰∫ÜÈïøÊó∂Á®ãÊìç‰ΩúÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSAR‰ΩøÁî®‰∏âÂÖÉÁªÑÊçüÂ§±ÔºåÂÖ∂‰∏≠Ê≠£Ê†∑Êú¨ÊòØ‰ªªÂä°ÁöÑÁúüÂÆûËøõÂ±ïÁä∂ÊÄÅÔºåË¥üÊ†∑Êú¨ÊòØGeminiÁîüÊàêÁöÑÂÖ∑ÊúâËø∑ÊÉëÊÄßÁöÑÁä∂ÊÄÅ„ÄÇPOE‰ΩøÁî®Áõ∏ÂØπÁâ©‰Ωì-Â§πÁà™ÂßøÊÄÅ‰Ωú‰∏∫Êé¢Á¥¢ÁöÑ‰æùÊçÆÔºåÈÅøÂÖç‰∫ÜÂéüÂßãÂÉèÁ¥†Â∏¶Êù•ÁöÑÂô™Â£∞„ÄÇÈïøÊó∂Á®ãËÆ∞ÂøÜ‰ΩøÁî®GRUÁªìÊûÑÔºåÂπ∂ÈÄöËøáÈó®ÊéßÊú∫Âà∂Êù•ÊéßÂà∂‰ø°ÊÅØÁöÑÊµÅÂÖ•ÂíåÊµÅÂá∫„ÄÇÊçüÂ§±ÂáΩÊï∞ÁªºÂêàËÄÉËôë‰∫ÜSAR„ÄÅPOEÂíå‰ªªÂä°Â•ñÂä±ÔºåÂπ∂ÈÄöËøáÂä†ÊùÉÁöÑÊñπÂºèËøõË°åÂπ≥Ë°°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EvoVLAÂú®Discoverse-LÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÁõ∏ÊØîÊúÄÂº∫Âü∫Á∫øOpenVLA-OFTÔºåÂπ≥Âùá‰ªªÂä°ÊàêÂäüÁéáÊèêÈ´ò‰∫Ü10.2‰∏™ÁôæÂàÜÁÇπÔºåËææÂà∞69.2%„ÄÇÂêåÊó∂ÔºåÊ†∑Êú¨ÊïàÁéáÊèêÈ´ò‰∫Ü1.5ÂÄçÔºåÈò∂ÊÆµÂπªËßâ‰ªé38.5%Èôç‰ΩéÂà∞14.8%„ÄÇÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåEvoVLAÂú®Âõõ‰∏™Êìç‰Ωú‰ªªÂä°‰∏äÁöÑÂπ≥ÂùáÊàêÂäüÁéá‰∏∫54.6%ÔºåË∂ÖËøáOpenVLA-OFT 11‰∏™ÁôæÂàÜÁÇπÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÁöÑsim-to-realËøÅÁßªËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EvoVLAÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫Á≠â„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÂÆåÊàêÂ§çÊùÇÁöÑÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°ÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇÊú™Êù•ÔºåEvoVLAÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÈ¢ÜÂüüÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

