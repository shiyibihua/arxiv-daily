---
layout: default
title: EvoVLA: Self-Evolving Vision-Language-Action Model
---

# EvoVLA: Self-Evolving Vision-Language-Action Model

**arXiv**: [2511.16166v1](https://arxiv.org/abs/2511.16166) | [PDF](https://arxiv.org/pdf/2511.16166.pdf)

**ä½œè€…**: Zeting Liu, Zida Yang, Zeyu Zhang, Hao Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvoVLAè‡ªè¿›åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ï¼Œè§£å†³é•¿ç¨‹æœºå™¨äººæ“ä½œä¸­çš„é˜¶æ®µå¹»è§‰é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `é•¿ç¨‹æœºå™¨äººæ“ä½œ` `é˜¶æ®µå¹»è§‰` `è‡ªç›‘ç£å­¦ä¹ ` `æ¨¡æ‹Ÿåˆ°çœŸå®žè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨é•¿ç¨‹æ“ä½œä¸­æ˜“å‡ºçŽ°é˜¶æ®µå¹»è§‰ï¼Œåˆ©ç”¨ç²—è¯„ä¼°ä¿¡å·èµ°æ·å¾„ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆé˜¶æ®µå¯¹é½å¥–åŠ±ã€å§¿æ€å¯¹è±¡æŽ¢ç´¢å’Œé•¿ç¨‹è®°å¿†ï¼Œæå‡ä»»åŠ¡å®ŒæˆçœŸå®žæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Discoverse-LåŸºå‡†ä¸Šï¼Œä»»åŠ¡æˆåŠŸçŽ‡æå‡10.2ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾69.2%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

