---
layout: default
title: EvoVLA: Self-Evolving Vision-Language-Action Model
---

# EvoVLA: Self-Evolving Vision-Language-Action Model

**arXiv**: [2511.16166v1](https://arxiv.org/abs/2511.16166) | [PDF](https://arxiv.org/pdf/2511.16166.pdf)

**ä½œè€…**: Zeting Liu, Zida Yang, Zeyu Zhang, Hao Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/AIGeeksGroup/EvoVLA) | [PROJECT_PAGE](https://aigeeksgroup.github.io/EvoVLA)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EvoVLAï¼šä¸€ç§è‡ªè¿›åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ï¼Œè§£å†³é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä¸­çš„é˜¶æ®µå¹»è§‰é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æœºå™¨äººæ“ä½œ` `é•¿æ—¶ç¨‹ä»»åŠ¡` `è‡ªç›‘ç£å­¦ä¹ ` `é˜¶æ®µå¹»è§‰` `å¯¹æ¯”å­¦ä¹ ` `sim-to-real`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä¸­å­˜åœ¨é˜¶æ®µå¹»è§‰é—®é¢˜ï¼Œå³åˆ©ç”¨ç²—ç³™çš„è¯„ä¼°ä¿¡å·æ¥è·³è¿‡å¤šæ­¥éª¤ä»»åŠ¡ã€‚
2. EvoVLAé€šè¿‡é˜¶æ®µå¯¹é½å¥–åŠ±ã€åŸºäºŽå§¿æ€çš„ç‰©ä½“æŽ¢ç´¢å’Œé•¿æ—¶ç¨‹è®°å¿†ä¸‰ä¸ªç»„ä»¶ï¼Œè§£å†³é˜¶æ®µå¹»è§‰é—®é¢˜ï¼Œæå‡é•¿æ—¶ç¨‹æ“ä½œæ€§èƒ½ã€‚
3. EvoVLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººå®žéªŒä¸­å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¯æ˜Žäº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºEvoVLAï¼Œä¸€ç§è‡ªç›‘ç£çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä¸­å­˜åœ¨çš„é˜¶æ®µå¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æž¶åŒ…å«ä¸‰ä¸ªäº’è¡¥ç»„ä»¶ï¼šé˜¶æ®µå¯¹é½å¥–åŠ±ï¼ˆSARï¼‰ï¼Œé€šè¿‡ä¸ŽGeminiç”Ÿæˆçš„å›°éš¾è´Ÿæ ·æœ¬è¿›è¡Œä¸‰å…ƒç»„å¯¹æ¯”å­¦ä¹ æ¥é˜²æ­¢è§†è§‰æ·å¾„ï¼›åŸºäºŽå§¿æ€çš„ç‰©ä½“æŽ¢ç´¢ï¼ˆPOEï¼‰ï¼Œå°†å¥½å¥‡å¿ƒå»ºç«‹åœ¨ç›¸å¯¹ç‰©ä½“-å¤¹çˆªå§¿æ€ä¸Šï¼Œè€ŒéžåŽŸå§‹åƒç´ ï¼›ä»¥åŠé•¿æ—¶ç¨‹è®°å¿†ï¼Œåˆ©ç”¨é€‰æ‹©æ€§ä¸Šä¸‹æ–‡ä¿ç•™å’Œé—¨æŽ§èžåˆæ¥ç¨³å®šæ‰©å±•rolloutæœŸé—´çš„å†…åœ¨å¡‘é€ ã€‚åœ¨Discoverse-LåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEvoVLAæ¯”æœ€å¼ºçš„åŸºçº¿ï¼ˆOpenVLA-OFTï¼‰å¹³å‡ä»»åŠ¡æˆåŠŸçŽ‡æé«˜äº†10.2ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾åˆ°69.2%ã€‚EvoVLAçš„æ ·æœ¬æ•ˆçŽ‡æé«˜äº†1.5å€ï¼Œé˜¶æ®µå¹»è§‰ä»Ž38.5%é™ä½Žåˆ°14.8%ã€‚åœ¨ç‰©ç†æœºå™¨äººä¸Šçš„çœŸå®žä¸–ç•Œéƒ¨ç½²ä¸­ï¼ŒEvoVLAåœ¨å››ä¸ªæ“ä½œä»»åŠ¡ä¸Šçš„å¹³å‡æˆåŠŸçŽ‡ä¸º54.6%ï¼Œè¶…è¿‡OpenVLA-OFT 11ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯æ˜Žäº†æœ‰æ•ˆçš„sim-to-realè¿ç§»å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå®¹æ˜“å‡ºçŽ°â€œé˜¶æ®µå¹»è§‰â€é—®é¢˜ã€‚è¿™æ„å‘³ç€æ™ºèƒ½ä½“ä¸ºäº†èŽ·å¾—æ›´é«˜çš„å¥–åŠ±ï¼Œä¼šåˆ©ç”¨ä¸€äº›ç²—ç³™çš„è§†è§‰ä¿¡å·æ¥â€œæ¬ºéª—â€ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼Œåœ¨ä»»åŠ¡å°šæœªçœŸæ­£å®Œæˆæ—¶å°±æŠ¥å‘Šå¾ˆé«˜çš„è¿›åº¦ï¼Œä»Žè€Œå¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚è¿™ç§çŽ°è±¡é˜»ç¢äº†VLAæ¨¡åž‹åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEvoVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œè®©æ¨¡åž‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡çš„çœŸå®žè¿›å±•ï¼Œä»Žè€Œé¿å…é˜¶æ®µå¹»è§‰ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šé˜¶æ®µå¯¹é½å¥–åŠ±ï¼ˆSARï¼‰ã€åŸºäºŽå§¿æ€çš„ç‰©ä½“æŽ¢ç´¢ï¼ˆPOEï¼‰å’Œé•¿æ—¶ç¨‹è®°å¿†ï¼Œæ¥åˆ†åˆ«è§£å†³è§†è§‰æ·å¾„ã€æŽ¢ç´¢æ•ˆçŽ‡å’Œé•¿æœŸä¾èµ–é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šEvoVLAçš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºŽæå–åœºæ™¯çš„è§†è§‰ç‰¹å¾ã€‚2) è¯­è¨€ç¼–ç å™¨ï¼šç”¨äºŽç†è§£ä»»åŠ¡æŒ‡ä»¤ã€‚3) åŠ¨ä½œè§£ç å™¨ï¼šæ ¹æ®è§†è§‰ç‰¹å¾ã€è¯­è¨€æŒ‡ä»¤å’ŒåŽ†å²è®°å¿†ç”ŸæˆåŠ¨ä½œã€‚4) é˜¶æ®µå¯¹é½å¥–åŠ±æ¨¡å—ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡åž‹å…³æ³¨ä»»åŠ¡çš„çœŸå®žè¿›å±•ã€‚5) åŸºäºŽå§¿æ€çš„ç‰©ä½“æŽ¢ç´¢æ¨¡å—ï¼šå¼•å¯¼æ¨¡åž‹æŽ¢ç´¢æœ‰æ„ä¹‰çš„ç‰©ä½“äº¤äº’ã€‚6) é•¿æ—¶ç¨‹è®°å¿†æ¨¡å—ï¼šå­˜å‚¨å’Œæ£€ç´¢åŽ†å²ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡åž‹ç†è§£é•¿æœŸä¾èµ–å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šEvoVLAçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ä¸‰ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šSARé€šè¿‡Geminiç”Ÿæˆçš„å›°éš¾è´Ÿæ ·æœ¬è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆé˜²æ­¢äº†è§†è§‰æ·å¾„ï¼›POEå°†å¥½å¥‡å¿ƒå»ºç«‹åœ¨ç›¸å¯¹ç‰©ä½“-å¤¹çˆªå§¿æ€ä¸Šï¼Œæé«˜äº†æŽ¢ç´¢æ•ˆçŽ‡ï¼›é•¿æ—¶ç¨‹è®°å¿†åˆ™é€šè¿‡é€‰æ‹©æ€§ä¸Šä¸‹æ–‡ä¿ç•™å’Œé—¨æŽ§èžåˆï¼Œç¨³å®šäº†é•¿æœŸrolloutä¸­çš„å†…åœ¨å¡‘é€ ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œæ˜¾è‘—é™ä½Žäº†é˜¶æ®µå¹»è§‰ï¼Œæå‡äº†é•¿æ—¶ç¨‹æ“ä½œçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šSARä½¿ç”¨ä¸‰å…ƒç»„æŸå¤±ï¼Œå…¶ä¸­æ­£æ ·æœ¬æ˜¯ä»»åŠ¡çš„çœŸå®žè¿›å±•çŠ¶æ€ï¼Œè´Ÿæ ·æœ¬æ˜¯Geminiç”Ÿæˆçš„å…·æœ‰è¿·æƒ‘æ€§çš„çŠ¶æ€ã€‚POEä½¿ç”¨ç›¸å¯¹ç‰©ä½“-å¤¹çˆªå§¿æ€ä½œä¸ºæŽ¢ç´¢çš„ä¾æ®ï¼Œé¿å…äº†åŽŸå§‹åƒç´ å¸¦æ¥çš„å™ªå£°ã€‚é•¿æ—¶ç¨‹è®°å¿†ä½¿ç”¨GRUç»“æž„ï¼Œå¹¶é€šè¿‡é—¨æŽ§æœºåˆ¶æ¥æŽ§åˆ¶ä¿¡æ¯çš„æµå…¥å’Œæµå‡ºã€‚æŸå¤±å‡½æ•°ç»¼åˆè€ƒè™‘äº†SARã€POEå’Œä»»åŠ¡å¥–åŠ±ï¼Œå¹¶é€šè¿‡åŠ æƒçš„æ–¹å¼è¿›è¡Œå¹³è¡¡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

EvoVLAåœ¨Discoverse-LåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”æœ€å¼ºåŸºçº¿OpenVLA-OFTï¼Œå¹³å‡ä»»åŠ¡æˆåŠŸçŽ‡æé«˜äº†10.2ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾åˆ°69.2%ã€‚åŒæ—¶ï¼Œæ ·æœ¬æ•ˆçŽ‡æé«˜äº†1.5å€ï¼Œé˜¶æ®µå¹»è§‰ä»Ž38.5%é™ä½Žåˆ°14.8%ã€‚åœ¨çœŸå®žæœºå™¨äººå®žéªŒä¸­ï¼ŒEvoVLAåœ¨å››ä¸ªæ“ä½œä»»åŠ¡ä¸Šçš„å¹³å‡æˆåŠŸçŽ‡ä¸º54.6%ï¼Œè¶…è¿‡OpenVLA-OFT 11ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆçš„sim-to-realè¿ç§»èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

EvoVLAåœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ï¼Œå®Œæˆå¤æ‚çš„é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆçŽ‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒEvoVLAå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰ï¼Œå®žçŽ°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„äººæœºäº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

