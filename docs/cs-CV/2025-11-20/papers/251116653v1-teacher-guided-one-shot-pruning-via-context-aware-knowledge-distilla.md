---
layout: default
title: Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation
---

# Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation

**arXiv**: [2511.16653v1](https://arxiv.org/abs/2511.16653) | [PDF](https://arxiv.org/pdf/2511.16653.pdf)

**ä½œè€…**: Md. Samiul Alim, Sharjil Khan, Amrijit Biswas, Fuad Rahman, Shafin Rahman, Nabeel Mohammed

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•™å¸ˆå¼•å¯¼çš„ä¸€æ¬¡æ€§å‰ªæžæ¡†æž¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çŸ¥è¯†è’¸é¦è§£å†³å‰ªæžè®¡ç®—å¼€é”€é—®é¢˜**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œå‰ªæž` `çŸ¥è¯†è’¸é¦` `ä¸€æ¬¡æ€§å‰ªæž` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `ç¨€ç–è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. éžç»“æž„åŒ–å‰ªæžéœ€è¿­ä»£è®­ç»ƒ-å‰ªæž-é‡è®­ç»ƒï¼Œè®¡ç®—å¼€é”€å¤§
2. æ–¹æ³•åœ¨é‡è¦æ€§è¯„åˆ†ä¸­é›†æˆæ•™å¸ˆæ¢¯åº¦ï¼Œä¸€æ¬¡æ€§å‰ªæžä¿ç•™å…³é”®å‚æ•°
3. å®žéªŒåœ¨CIFARç­‰æ•°æ®é›†å®žçŽ°é«˜ç¨€ç–åº¦ï¼Œæ€§èƒ½æŸå¤±å°ï¼Œä¼˜äºŽEPGç­‰åŸºçº¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.

