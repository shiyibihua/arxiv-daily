---
layout: default
title: Flow and Depth Assisted Video Prediction with Latent Transformer
---

# Flow and Depth Assisted Video Prediction with Latent Transformer

**arXiv**: [2511.16484v1](https://arxiv.org/abs/2511.16484) | [PDF](https://arxiv.org/pdf/2511.16484.pdf)

**ä½œè€…**: Eliyas Suleyman, Paul Henderson, Eksan Firkat, Nicolas Pugeault

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆç‚¹æµå’Œæ·±åº¦è¾…åŠ©çš„æ½œåœ¨å˜æ¢å™¨æ¨¡åž‹ï¼Œä»¥æå‡é®æŒ¡åœºæ™¯ä¸‹çš„è§†é¢‘é¢„æµ‹æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†é¢‘é¢„æµ‹` `é®æŒ¡å¤„ç†` `ç‚¹æµè¾…åŠ©` `æ·±åº¦å›¾` `æ½œåœ¨å˜æ¢å™¨` `è¿åŠ¨åˆ†å¸ƒè¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘é¢„æµ‹ä¸­é®æŒ¡å’ŒèƒŒæ™¯è¿åŠ¨å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ŒçŽ°æœ‰æ¨¡åž‹éš¾ä»¥å¤„ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æ½œåœ¨å˜æ¢å™¨æž¶æž„ä¸­æ•´åˆç‚¹æµå’Œæ·±åº¦å›¾ï¼Œæä¾›è¿åŠ¨å’Œå‡ ä½•ç»“æž„ä¿¡æ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒçœŸå®žæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œè¾…åŠ©æ¨¡åž‹åœ¨é®æŒ¡åœºæ™¯å’ŒèƒŒæ™¯è¿åŠ¨é¢„æµ‹æ›´å‡†ç¡®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.

