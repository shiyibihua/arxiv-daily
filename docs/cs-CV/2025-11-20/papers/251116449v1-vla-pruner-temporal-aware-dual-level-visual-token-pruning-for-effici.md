---
layout: default
title: VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference
---

# VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference

**arXiv**: [2511.16449v1](https://arxiv.org/abs/2511.16449) | [PDF](https://arxiv.org/pdf/2511.16449.pdf)

**ä½œè€…**: Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLA-Prunerä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æŽ¨ç†æ•ˆçŽ‡ä½Žçš„é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `åŒçº§é‡è¦æ€§` `æœºå™¨äººæ“ä½œ` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€æ¨¡åž‹å‰ªæžæ–¹æ³•å¿½ç•¥VLAæ¨¡åž‹çš„åŒç³»ç»Ÿç‰¹æ€§ï¼Œå¯¼è‡´åŠ¨ä½œç”Ÿæˆä¿¡æ¯ä¸¢å¤±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒçº§é‡è¦æ€§æ ‡å‡†ï¼Œç»“åˆè¯­ä¹‰çº§å’ŒåŠ¨ä½œçº§æ³¨æ„åŠ›ï¼Œè‡ªé€‚åº”ä¿ç•™å…³é”®è§†è§‰ä»¤ç‰Œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§VLAæž¶æž„å’Œæœºå™¨äººä»»åŠ¡ä¸­å®žçŽ°æœ€ä¼˜æ€§èƒ½ï¼Œæå‡æŽ¨ç†æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.

