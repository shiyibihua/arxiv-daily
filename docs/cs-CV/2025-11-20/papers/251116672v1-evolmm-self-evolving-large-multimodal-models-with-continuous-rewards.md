---
layout: default
title: EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards
---

# EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards

**arXiv**: [2511.16672v1](https://arxiv.org/abs/2511.16672) | [PDF](https://arxiv.org/pdf/2511.16672.pdf)

**ä½œè€…**: Omkat Thawakar, Shravan Venkatraman, Ritesh Thawkar, Abdelrahman Shaker, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvoLMMæ¡†æž¶ä»¥æ— ç›‘ç£æ–¹å¼æå‡å¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹` `æ— ç›‘ç£å­¦ä¹ ` `è‡ªæˆ‘è¿›åŒ–` `æŽ¨ç†èƒ½åŠ›` `è¿žç»­å¥–åŠ±` `å¤šæ¨¡æ€æ•°å­¦æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹è®­ç»ƒä¾èµ–äººå·¥æ•°æ®æˆ–å¤–éƒ¨å¥–åŠ±ï¼Œé™åˆ¶è‡ªä¸»æ€§å’Œå¯æ‰©å±•æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æè®®è€…å’Œæ±‚è§£è€…åŒä»£ç†ï¼Œé€šè¿‡å†…éƒ¨ä¸€è‡´æ€§å’Œè¿žç»­è‡ªå¥–åŠ±å®žçŽ°è‡ªæˆ‘è¿›åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ChartQAç­‰åŸºå‡†ä¸Šï¼ŒæŽ¨ç†å‡†ç¡®çŽ‡æå‡çº¦3%ï¼Œä»…ä½¿ç”¨åŽŸå§‹è®­ç»ƒå›¾åƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.

