---
layout: default
title: VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference
---

# VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference

**arXiv**: [2511.16449v2](https://arxiv.org/abs/2511.16449) | [PDF](https://arxiv.org/pdf/2511.16449.pdf)

**ä½œè€…**: Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-11-21)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLA-Prunerï¼šé¢å‘é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæŽ¨ç†çš„æ—¶åºæ„ŸçŸ¥åŒå±‚è§†è§‰Tokenå‰ªæž**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `Tokenå‰ªæž` `å…·èº«æ™ºèƒ½` `æœºå™¨äººæ“ä½œ` `æ—¶é—´è¿žç»­æ€§` `åŒå±‚é‡è¦æ€§` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹çš„Tokenå‰ªæžæ–¹æ³•å¿½ç•¥äº†VLAåŒç³»ç»Ÿç‰¹æ€§ï¼Œå¯¼è‡´åŠ¨ä½œç”Ÿæˆæ‰€éœ€å…³é”®ä¿¡æ¯ä¸¢å¤±ï¼Œæ€§èƒ½ä¸‹é™ã€‚
2. VLA-Pruneråˆ©ç”¨VLAæ¨¡åž‹çš„åŒç³»ç»Ÿç‰¹æ€§å’Œæœºå™¨äººæ“ä½œçš„æ—¶é—´è¿žç»­æ€§ï¼Œé‡‡ç”¨åŒå±‚é‡è¦æ€§å‡†åˆ™è¿›è¡ŒTokené€‰æ‹©ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒVLA-Pruneråœ¨å¤šç§VLAæž¶æž„å’Œæœºå™¨äººä»»åŠ¡ä¸­å®žçŽ°äº†SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸå±•çŽ°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¤„ç†è¿žç»­è§†è§‰æµçš„é«˜æ˜‚è®¡ç®—æˆæœ¬ä¸¥é‡é™åˆ¶äº†å®ƒä»¬çš„å®žæ—¶éƒ¨ç½²ã€‚Tokenå‰ªæžï¼ˆä¿ç•™æ˜¾è‘—çš„è§†è§‰tokenå¹¶ä¸¢å¼ƒå†—ä½™çš„tokenï¼‰å·²æˆä¸ºåŠ é€Ÿè§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºé«˜æ•ˆVLAæä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™äº›VLMç‰¹å®šçš„tokenå‰ªæžæ–¹æ³•ä»…åŸºäºŽè¯­ä¹‰æ˜¾è‘—æ€§æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œprefill attentionï¼‰é€‰æ‹©tokenï¼Œè€Œå¿½ç•¥äº†VLAå›ºæœ‰çš„é«˜å±‚è¯­ä¹‰ç†è§£å’Œä½Žå±‚åŠ¨ä½œæ‰§è¡Œçš„åŒç³»ç»Ÿç‰¹æ€§ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•å€¾å‘äºŽä¿ç•™è¯­ä¹‰çº¿ç´¢çš„tokenï¼Œä¸¢å¼ƒå¯¹åŠ¨ä½œç”Ÿæˆè‡³å…³é‡è¦çš„ä¿¡æ¯ï¼Œå¹¶æ˜¾è‘—é™ä½ŽVLAæ€§èƒ½ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†VLA-Prunerï¼Œä¸€ç§é€šç”¨çš„å³æ’å³ç”¨VLAç‰¹å®štokenå‰ªæžæ–¹æ³•ï¼Œå®ƒä¸ŽVLAæ¨¡åž‹çš„åŒç³»ç»Ÿç‰¹æ€§ç›¸ä¸€è‡´ï¼Œå¹¶åˆ©ç”¨äº†æœºå™¨äººæ“ä½œä¸­çš„æ—¶é—´è¿žç»­æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒVLA-Pruneré‡‡ç”¨åŒå±‚é‡è¦æ€§å‡†åˆ™è¿›è¡Œè§†è§‰tokenä¿ç•™ï¼šç”¨äºŽè¯­ä¹‰çº§åˆ«ç›¸å…³æ€§çš„è§†è§‰-è¯­è¨€prefill attentionï¼Œä»¥åŠé€šè¿‡æ—¶é—´å¹³æ»‘ä¼°è®¡çš„ç”¨äºŽåŠ¨ä½œçº§åˆ«é‡è¦æ€§çš„åŠ¨ä½œè§£ç attentionã€‚åŸºäºŽæ­¤å‡†åˆ™ï¼ŒVLA-Pruneræå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒå±‚tokené€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è‡ªé€‚åº”åœ°ä¿ç•™äº†ä¸€ç»„ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰tokenï¼Œç”¨äºŽåœ¨ç»™å®šçš„è®¡ç®—é¢„ç®—ä¸‹è¿›è¡Œè¯­ä¹‰ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œã€‚å®žéªŒè¡¨æ˜Žï¼ŒVLA-Pruneråœ¨å¤šç§VLAæž¶æž„å’Œä¸åŒçš„æœºå™¨äººä»»åŠ¡ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰VLAæ¨¡åž‹çš„tokenå‰ªæžæ–¹æ³•ä¸»è¦é’ˆå¯¹VLMè®¾è®¡ï¼Œä»…è€ƒè™‘è¯­ä¹‰æ˜¾è‘—æ€§ï¼Œå¿½ç•¥äº†VLAä»»åŠ¡ä¸­åŠ¨ä½œæ‰§è¡Œçš„é‡è¦æ€§ã€‚è¿™å¯¼è‡´å‰ªæžåŽçš„tokené›†åˆåå‘äºŽè¯­ä¹‰ä¿¡æ¯ï¼Œè€Œä¸¢å¤±äº†å¯¹åŠ¨ä½œç”Ÿæˆè‡³å…³é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œæœ€ç»ˆé™ä½Žäº†VLAæ¨¡åž‹çš„æ€§èƒ½ã€‚çŽ°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨VLAä»»åŠ¡çš„ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯é«˜å±‚è¯­ä¹‰ç†è§£å’Œä½Žå±‚åŠ¨ä½œæ‰§è¡Œçš„åŒç³»ç»Ÿç‰¹æ€§ï¼Œä»¥åŠæœºå™¨äººæ“ä½œä¸­çš„æ—¶é—´è¿žç»­æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLA-Prunerçš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§VLAä»»åŠ¡ç‰¹å®šçš„tokenå‰ªæžæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶è€ƒè™‘è¯­ä¹‰ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œçš„é‡è¦æ€§ã€‚é€šè¿‡å¼•å…¥åŒå±‚é‡è¦æ€§å‡†åˆ™ï¼Œåˆ†åˆ«è¯„ä¼°tokenåœ¨è¯­ä¹‰å±‚é¢çš„ç›¸å…³æ€§å’Œåœ¨åŠ¨ä½œå±‚é¢çš„é‡è¦æ€§ï¼Œä»Žè€Œè‡ªé€‚åº”åœ°ä¿ç•™å¯¹ä¸¤è€…éƒ½é‡è¦çš„tokenã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æœºå™¨äººæ“ä½œçš„æ—¶é—´è¿žç»­æ€§ï¼Œé€šè¿‡æ—¶é—´å¹³æ»‘æ¥æ›´å‡†ç¡®åœ°ä¼°è®¡åŠ¨ä½œè§£ç attentionï¼Œæé«˜åŠ¨ä½œçº§åˆ«é‡è¦æ€§çš„è¯„ä¼°ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVLA-Pruneræ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥é›†æˆåˆ°çŽ°æœ‰çš„VLAæ¨¡åž‹ä¸­ã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨è§†è§‰-è¯­è¨€prefill attentionè¯„ä¼°tokençš„è¯­ä¹‰çº§åˆ«ç›¸å…³æ€§ï¼›2) ä½¿ç”¨æ—¶é—´å¹³æ»‘çš„åŠ¨ä½œè§£ç attentionè¯„ä¼°tokençš„åŠ¨ä½œçº§åˆ«é‡è¦æ€§ï¼›3) åŸºäºŽåŒå±‚é‡è¦æ€§å‡†åˆ™ï¼Œé‡‡ç”¨åŒå±‚tokené€‰æ‹©ç­–ç•¥ï¼Œè‡ªé€‚åº”åœ°ä¿ç•™ä¸€ç»„ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰tokenã€‚

**å…³é”®åˆ›æ–°**ï¼šVLA-Prunerçš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†åŒå±‚é‡è¦æ€§å‡†åˆ™å’ŒåŒå±‚tokené€‰æ‹©ç­–ç•¥ã€‚åŒå±‚é‡è¦æ€§å‡†åˆ™èƒ½å¤ŸåŒæ—¶è€ƒè™‘è¯­ä¹‰ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œçš„é‡è¦æ€§ï¼Œé¿å…äº†çŽ°æœ‰æ–¹æ³•åå‘è¯­ä¹‰ä¿¡æ¯çš„ç¼ºé™·ã€‚åŒå±‚tokené€‰æ‹©ç­–ç•¥èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„è®¡ç®—é¢„ç®—ï¼Œè‡ªé€‚åº”åœ°å¹³è¡¡è¯­ä¹‰å’ŒåŠ¨ä½œä¿¡æ¯ï¼Œä»Žè€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶å®žçŽ°é«˜æ•ˆçš„tokenå‰ªæžã€‚

**å…³é”®è®¾è®¡**ï¼šVLA-Prunerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨è§†è§‰-è¯­è¨€prefill attentionä½œä¸ºè¯­ä¹‰çº§åˆ«ç›¸å…³æ€§çš„åº¦é‡ï¼›2) ä½¿ç”¨æ—¶é—´å¹³æ»‘çš„åŠ¨ä½œè§£ç attentionä½œä¸ºåŠ¨ä½œçº§åˆ«é‡è¦æ€§çš„åº¦é‡ï¼Œå…·ä½“çš„æ—¶é—´å¹³æ»‘æ–¹æ³•æœªçŸ¥ï¼›3) è®¾è®¡åŒå±‚tokené€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥çš„å…·ä½“å®žçŽ°æ–¹å¼æœªçŸ¥ï¼Œä½†ç›®æ ‡æ˜¯æ ¹æ®åŒå±‚é‡è¦æ€§å‡†åˆ™ï¼Œåœ¨è®¡ç®—é¢„ç®—çš„çº¦æŸä¸‹ï¼Œé€‰æ‹©æœ€ä½³çš„tokenå­é›†ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

VLA-Pruneråœ¨å¤šä¸ªVLAæž¶æž„å’Œä¸åŒçš„æœºå™¨äººä»»åŠ¡ä¸­å®žçŽ°äº†SOTAæ€§èƒ½ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒVLA-Prunerèƒ½å¤Ÿæ˜¾è‘—æé«˜VLAæ¨¡åž‹çš„æ•ˆçŽ‡ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡å…¶æ€§èƒ½ï¼Œå…‹æœäº†çŽ°æœ‰tokenå‰ªæžæ–¹æ³•åœ¨VLAä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

VLA-Prunerå¯åº”ç”¨äºŽå„ç§éœ€è¦å®žæ—¶è§†è§‰-è¯­è¨€-åŠ¨ä½œæŽ¨ç†çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ç‰©ä½“æ“ä½œã€äººæœºåä½œç­‰ã€‚é€šè¿‡é™ä½ŽVLAæ¨¡åž‹çš„è®¡ç®—æˆæœ¬ï¼ŒVLA-Prunerèƒ½å¤Ÿæé«˜æœºå™¨äººåœ¨èµ„æºå—é™çŽ¯å¢ƒä¸­çš„éƒ¨ç½²èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„æœºå™¨äººåº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.

