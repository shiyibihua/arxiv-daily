---
layout: default
title: TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing
---

# TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing

**arXiv**: [2511.16662v1](https://arxiv.org/abs/2511.16662) | [PDF](https://arxiv.org/pdf/2511.16662.pdf)

**ä½œè€…**: Eddie Pokming Sheung, Qihao Liu, Wufei Ma, Prakhar Kaushik, Jianwen Xie, Alan Yuille

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTriDiff-4Dä»¥è§£å†³æ–‡æœ¬åˆ°4Dç”Ÿæˆä¸­çš„ä¸€è‡´æ€§ä¸Žæ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `4Dç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `ä¸‰å¹³é¢è¡¨ç¤º` `éª¨æž¶é©±åŠ¨åŠ¨ç”»` `æ—¶é—´ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰4Dç”Ÿæˆæ–¹æ³•å­˜åœ¨æ—¶é—´ä¸ä¸€è‡´ã€è¿åŠ¨ä¸è§„åˆ™å’Œé«˜è®¡ç®—æˆæœ¬ç­‰é™åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ‰©æ•£æ¨¡åž‹å’Œè‡ªå›žå½’ç­–ç•¥ï¼Œé€šè¿‡ä¸‰å¹³é¢é‡å®šä½ç”Ÿæˆä»»æ„é•¿åº¦4Dåºåˆ—
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ˜¾è‘—å‡å°‘ç”Ÿæˆæ—¶é—´è‡³ç§’çº§ï¼Œæå‡è¿åŠ¨å‡†ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.

