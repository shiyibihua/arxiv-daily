---
layout: default
title: InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer
---

# InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer

**arXiv**: [2511.15967v1](https://arxiv.org/abs/2511.15967) | [PDF](https://arxiv.org/pdf/2511.15967.pdf)

**ä½œè€…**: Muyao Yuan, Yuanhong Zhang, Weizhan Zhang, Lan Ma, Yuan Gao, Jiangyong Ying, Yudeng Xin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInfoCLIPä»¥è§£å†³CLIPå¾®è°ƒä¸­æ¨¡æ€å¯¹é½é€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²` `è§†è§‰è¯­è¨€é¢„è®­ç»ƒ` `ä¿¡æ¯ç†è®ºå¯¹é½` `CLIPå¾®è°ƒ` `æ¨¡æ€å¯¹é½è½¬ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPå¾®è°ƒäºŽåˆ†å‰²ä»»åŠ¡æ˜“è¿‡æ‹Ÿåˆï¼Œç ´åé¢„è®­ç»ƒè§†è§‰è¯­è¨€å¯¹é½
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽäº’ä¿¡æ¯åŽ‹ç¼©å¯¹é½å™ªå£°å¹¶æœ€å¤§åŒ–çŸ¥è¯†è½¬ç§»
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šåŸºå‡†æµ‹è¯•éªŒè¯å…¶åœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­çš„ä¼˜è¶Šæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.

