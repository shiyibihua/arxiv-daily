---
layout: default
title: AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers
---

# AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers

**arXiv**: [2511.16047v1](https://arxiv.org/abs/2511.16047) | [PDF](https://arxiv.org/pdf/2511.16047.pdf)

**ä½œè€…**: Boxun Xu, Yu Wang, Zihu Wang, Peng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAMS-KVè‡ªé€‚åº”KVç¼“å­˜ç­–ç•¥ä»¥ä¼˜åŒ–å¤šå°ºåº¦è§†è§‰è‡ªå›žå½’æ¨¡åž‹çš„æ‰©å±•æ€§**

**å…³é”®è¯**: `è§†è§‰è‡ªå›žå½’å»ºæ¨¡` `KVç¼“å­˜ä¼˜åŒ–` `å¤šå°ºåº¦å›¾åƒç”Ÿæˆ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `è®¡ç®—æ•ˆçŽ‡æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šå°ºåº¦è§†è§‰è‡ªå›žå½’æ¨¡åž‹ä¸­KVç¼“å­˜å†…å­˜éšå°ºåº¦å¢žåŠ è€Œè¿‡åº¦å¢žé•¿ï¼Œé™åˆ¶æ‰©å±•æ€§
2. åŸºäºŽå±€éƒ¨å°ºåº¦å’Œå‡èšå°ºåº¦ä¼˜å…ˆå­˜å‚¨KVï¼Œå¹¶é€šè¿‡å±‚é—´ç›¸ä¼¼æ€§åˆ†æžä¼˜åŒ–ç¼“å­˜åˆ©ç”¨
3. å®žéªŒæ˜¾ç¤ºKVç¼“å­˜ä½¿ç”¨å‡å°‘84.83%ï¼Œè‡ªæ³¨æ„åŠ›å»¶è¿Ÿé™ä½Ž60.48ï¼Œæ”¯æŒæ›´å¤§æ‰¹æ¬¡ç”Ÿæˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.

