---
layout: default
title: An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs
---

# An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs

**arXiv**: [2511.16163v1](https://arxiv.org/abs/2511.16163) | [PDF](https://arxiv.org/pdf/2511.16163.pdf)

**ä½œè€…**: Zhi Luo, Zenghui Yuan, Wenqi Wei, Daizong Liu, Pan Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå†—é•¿æ–‡æœ¬è¯±å¯¼æ”»å‡»ä»¥ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡åž‹è¾“å‡ºé•¿åº¦ï¼Œæå‡æ”»å‡»ç¨³å®šæ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯¹æŠ—æ”»å‡»` `å†—é•¿æ–‡æœ¬ç”Ÿæˆ` `æ‰°åŠ¨ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å®‰å…¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•æ— æ³•ç›´æŽ¥æœ€å¤§åŒ–è§†è§‰è¯­è¨€æ¨¡åž‹è¾“å‡ºä»¤ç‰Œé•¿åº¦ï¼Œç¼ºä¹ç¨³å®šæ€§å’Œå¯æŽ§æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æž¶ï¼Œç»“åˆå¯¹æŠ—æç¤ºæœç´¢å’Œè§†è§‰å¯¹é½æ‰°åŠ¨ä¼˜åŒ–ï¼Œæ³¨å…¥ä¸å¯å¯Ÿè§‰æ‰°åŠ¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ç§æµè¡Œè§†è§‰è¯­è¨€æ¨¡åž‹ä¸ŠéªŒè¯ï¼Œæ–¹æ³•åœ¨æœ‰æ•ˆæ€§ã€æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜åŠ¿æ˜¾è‘—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.

