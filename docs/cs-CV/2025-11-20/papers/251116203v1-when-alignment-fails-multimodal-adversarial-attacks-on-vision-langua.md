---
layout: default
title: When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models
---

# When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models

**arXiv**: [2511.16203v1](https://arxiv.org/abs/2511.16203) | [PDF](https://arxiv.org/pdf/2511.16203.pdf)

**ä½œè€…**: Yuping Yan, Yuhan Xie, Yinxin Zhang, Lingjuan Lyu, Yaochu Jin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLA-Foolæ–¹æ³•ä»¥è¯„ä¼°å…·èº«è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„å¤šæ¨¡æ€å¯¹æŠ—é²æ£’æ€§**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `è·¨æ¨¡æ€é”™ä½` `é»‘ç›’è®¾ç½®` `è¯­ä¹‰å¼•å¯¼æç¤º` `å…·èº«çŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…·èº«VLAæ¨¡åž‹åœ¨å¤šæ¨¡æ€å’Œé»‘ç›’æ¡ä»¶ä¸‹çš„å¯¹æŠ—é²æ£’æ€§æœªçŸ¥ï¼ŒçŽ°æœ‰ç ”ç©¶å¿½è§†è·¨æ¨¡æ€é”™ä½å½±å“
2. æ–¹æ³•è¦ç‚¹ï¼šç»Ÿä¸€æ–‡æœ¬ã€è§†è§‰å’Œè·¨æ¨¡æ€æ”»å‡»ï¼Œå¹¶å¼•å…¥è¯­ä¹‰å¼•å¯¼æç¤ºæ¡†æž¶å¢žå¼ºæ”»å‡»æ•ˆæžœ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†ä¸Šï¼Œå¾®å°å¤šæ¨¡æ€æ‰°åŠ¨å¯å¯¼è‡´æ˜¾è‘—è¡Œä¸ºåå·®ï¼Œæ­ç¤ºæ¨¡åž‹è„†å¼±æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

