---
layout: default
title: Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution
---

# Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.16541" target="_blank" class="toolbar-btn">arXiv: 2511.16541v2</a>
    <a href="https://arxiv.org/pdf/2511.16541.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16541v2" 
            onclick="toggleFavorite(this, '2511.16541v2', 'Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Jaime Ãlvarez UrueÃ±a, David Camacho, Javier Huertas Tato

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-11-21)

**å¤‡æ³¨**: 17 pages, 6 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æ¡†æ¶ï¼Œç”¨äºå°‘æ ·æœ¬AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸æº¯æºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `å°‘æ ·æœ¬å­¦ä¹ ` `AIç”Ÿæˆå›¾åƒæ£€æµ‹` `å›¾åƒæº¯æº` `æ·±åº¦å­¦ä¹ ` `kè¿‘é‚»åˆ†ç±»å™¨` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•ä¾èµ–å®šæœŸé‡æ–°è®­ç»ƒï¼Œéš¾ä»¥è·Ÿä¸Šæ–°å‹ç”Ÿæˆæ¨¡å‹å¿«é€Ÿè¿­ä»£çš„æ­¥ä¼ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚
2. æå‡ºä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æå–å›¾åƒåµŒå…¥ï¼Œå¹¶ä½¿ç”¨å°‘æ ·æœ¬k-NNåˆ†ç±»å™¨è¿›è¡Œæ£€æµ‹å’Œæº¯æºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ç²¾åº¦å’Œæº¯æºæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ä½¿å¾—åˆæˆå›¾åƒä¸çœŸå®å›¾åƒéš¾ä»¥åŒºåˆ†ï¼Œç»™æ•°å­—åª’ä½“çš„å®Œæ•´æ€§å¸¦æ¥äº†ä¸¥å³»æŒ‘æˆ˜ã€‚æ–°å‹ç”Ÿæˆæ¨¡å‹åŠ é€Ÿå‘å¸ƒå‘¨æœŸåŠ å‰§äº†è¿™ä¸€é—®é¢˜ï¼Œä½¿å¾—ä¾èµ–å®šæœŸé‡æ–°è®­ç»ƒçš„ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•åœ¨è®¡ç®—ä¸Šä¸å¯è¡Œï¼Œåœ¨æ“ä½œä¸Šä¹Ÿä¸åˆ‡å®é™…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åˆæˆå›¾åƒæ£€æµ‹ä¸­å›ºæœ‰çš„æ³›åŒ–æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨é€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„è§†è§‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»è¾“å…¥å›¾åƒä¸­æå–åˆ¤åˆ«æ€§åµŒå…¥ã€‚å…³é”®åœ¨äºï¼Œè¯¥æ¨¡å‹åœ¨å¯ç”¨ç”Ÿæˆå™¨çš„ç­–ç•¥æ€§åˆ’åˆ†çš„å­é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¿ç•™ç‰¹å®šçš„æ¶æ„ä¸å‚ä¸è®­ç»ƒï¼Œä»¥ä¸¥æ ¼è¯„ä¼°è·¨ç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨åœ¨å­¦ä¹ åˆ°çš„åµŒå…¥ç©ºé—´ä¸Šè¿è¡Œçš„kè¿‘é‚»ï¼ˆk-NNï¼‰åˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨é‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªå…ˆå‰æœªè§è¿‡çš„æµ‹è¯•ç”Ÿæˆå™¨çš„æœ‰é™æ ·æœ¬ã€‚åœ¨å°‘æ ·æœ¬å­¦ä¹ æœºåˆ¶ä¸­ï¼Œæ¯ä¸ªç±»åˆ«ä»…éœ€150å¼ å›¾åƒï¼ˆå¾ˆå®¹æ˜“ä»å½“å‰ç”Ÿæˆæ¨¡å‹ä¸­è·å¾—ï¼‰ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å³å¯å®ç°91.3%çš„å¹³å‡æ£€æµ‹å‡†ç¡®ç‡ï¼Œæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†5.2ä¸ªç™¾åˆ†ç‚¹ã€‚å¯¹äºæºæº¯æºä»»åŠ¡ï¼Œåœ¨å¼€æ”¾é›†åˆ†ç±»ä¸Šä¸‹æ–‡ä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨AUCå’ŒOSCRæ–¹é¢åˆ†åˆ«è·å¾—äº†14.70%å’Œ4.27%çš„æå‡ï¼Œæ ‡å¿—ç€åœ¨ç¨³å¥ã€å¯æ‰©å±•çš„å–è¯æº¯æºç³»ç»Ÿæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”ä¸æ–­å‘å±•çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç¯å¢ƒï¼Œè€Œæ— éœ€è¿›è¡Œè¯¦å°½çš„é‡æ–°è®­ç»ƒåè®®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³AIç”Ÿæˆå›¾åƒæ£€æµ‹å’Œæº¯æºé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ–°å‹ç”Ÿæˆæ¨¡å‹ä¸æ–­æ¶Œç°ï¼Œä¼ ç»Ÿæ£€æµ‹æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéœ€è¦å¤§é‡é‡æ–°è®­ç»ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå˜åŒ–ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œæ•ˆç‡ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æå–å…·æœ‰åˆ¤åˆ«æ€§çš„å›¾åƒåµŒå…¥ï¼Œä½¿å¾—æ¥è‡ªåŒä¸€ç”Ÿæˆå™¨çš„å›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­æ›´æ¥è¿‘ï¼Œè€Œæ¥è‡ªä¸åŒç”Ÿæˆå™¨çš„å›¾åƒæ›´è¿œç¦»ã€‚ç„¶åï¼Œåˆ©ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œä»…éœ€å°‘é‡æ–°ç”Ÿæˆå™¨çš„æ ·æœ¬å³å¯è®­ç»ƒåˆ†ç±»å™¨ï¼Œå®ç°å¿«é€Ÿé€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) åµŒå…¥æå–é˜¶æ®µï¼šä½¿ç”¨è§†è§‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ResNetï¼‰åœ¨åŒ…å«å¤šä¸ªç”Ÿæˆå™¨å›¾åƒçš„æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼Œå­¦ä¹ å›¾åƒçš„åµŒå…¥è¡¨ç¤ºã€‚2) åˆ†ç±»é˜¶æ®µï¼šä½¿ç”¨k-NNåˆ†ç±»å™¨ï¼Œåœ¨å­¦ä¹ åˆ°çš„åµŒå…¥ç©ºé—´ä¸­ï¼Œåˆ©ç”¨å°‘é‡æ¥è‡ªæ–°ç”Ÿæˆå™¨çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå®ç°å¯¹AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹å’Œæº¯æºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹å¯¹ä¸åŒç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨è®­ç»ƒæ—¶å°†æ¥è‡ªåŒä¸€ç”Ÿæˆå™¨çš„å›¾åƒè§†ä¸ºæ­£æ ·æœ¬ï¼Œæ¥è‡ªä¸åŒç”Ÿæˆå™¨çš„å›¾åƒè§†ä¸ºè´Ÿæ ·æœ¬ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§çš„åµŒå…¥è¡¨ç¤ºï¼Œä»è€Œæé«˜å¯¹æœªçŸ¥ç”Ÿæˆå™¨çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œä½¿ç”¨äº†å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚InfoNCE lossï¼Œæ¥ä¼˜åŒ–åµŒå…¥è¡¨ç¤ºã€‚k-NNåˆ†ç±»å™¨çš„kå€¼æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†ç­–ç•¥æ€§åˆ’åˆ†çš„ç”Ÿæˆå™¨å­é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¿ç•™éƒ¨åˆ†ç”Ÿæˆå™¨ç”¨äºæµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å°‘æ ·æœ¬å­¦ä¹ ä¸­ï¼Œæ¯ä¸ªç±»åˆ«ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ï¼ˆä¾‹å¦‚150å¼ ï¼‰æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¡†æ¶åœ¨å°‘æ ·æœ¬å­¦ä¹ æœºåˆ¶ä¸‹ï¼Œæ¯ä¸ªç±»åˆ«ä»…éœ€150å¼ å›¾åƒï¼Œå¹³å‡æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°91.3%ï¼Œæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†5.2ä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨å¼€æ”¾é›†åˆ†ç±»çš„æºæº¯æºä»»åŠ¡ä¸­ï¼ŒAUCæŒ‡æ ‡æå‡äº†14.70%ï¼ŒOSCRæŒ‡æ ‡æå‡äº†4.27%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AIç”Ÿæˆå›¾åƒæ£€æµ‹å’Œæº¯æºæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ•°å­—åª’ä½“å†…å®¹å®‰å…¨é¢†åŸŸï¼Œä¾‹å¦‚æ£€æµ‹å’Œæº¯æºè™šå‡æ–°é—»ã€æ¶æ„Deepfakeå›¾åƒç­‰ã€‚æœ‰åŠ©äºç»´æŠ¤ç½‘ç»œä¿¡æ¯å®‰å…¨ï¼Œæ‰“å‡»ç½‘ç»œçŠ¯ç½ªï¼Œä¿æŠ¤çŸ¥è¯†äº§æƒï¼Œæå‡å…¬ä¼—å¯¹AIç”Ÿæˆå†…å®¹çš„è¾¨åˆ«èƒ½åŠ›ã€‚æœªæ¥å¯æ‰©å±•åˆ°è§†é¢‘ã€éŸ³é¢‘ç­‰å…¶ä»–æ¨¡æ€çš„AIç”Ÿæˆå†…å®¹æ£€æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
>   This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
>   With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70% and 4.27% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.

