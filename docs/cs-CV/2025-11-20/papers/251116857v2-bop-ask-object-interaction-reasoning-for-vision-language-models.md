---
layout: default
title: BOP-ASK: Object-Interaction Reasoning for Vision-Language Models
---

# BOP-ASK: Object-Interaction Reasoning for Vision-Language Models

**arXiv**: [2511.16857v2](https://arxiv.org/abs/2511.16857) | [PDF](https://arxiv.org/pdf/2511.16857.pdf)

**ä½œè€…**: Vineet Bhat, Sungsu Kim, Valts Blukis, Greg Heinrich, Prashanth Krishnamurthy, Ramesh Karri, Stan Birchfield, Farshad Khorrami, Jonathan Tremblay

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-12-04)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BOP-ASKï¼šç”¨äºŽè§†è§‰-è¯­è¨€æ¨¡åž‹çš„ç›®æ ‡äº¤äº’æŽ¨ç†æ•°æ®é›†ä¸ŽåŸºå‡†**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡åž‹` `å¯¹è±¡äº¤äº’æŽ¨ç†` `æ•°æ®é›†` `ç©ºé—´æŽ¨ç†` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨ç©ºé—´æŽ¨ç†ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯¹å¯¹è±¡äº¤äº’çš„ç»†ç²’åº¦ç†è§£ï¼Œé™åˆ¶äº†å…¶åœ¨å®žé™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. BOP-ASKæ•°æ®é›†é€šè¿‡åˆ©ç”¨6Då¯¹è±¡å§¿æ€ä¿¡æ¯ï¼Œç”ŸæˆåŒ…å«æŠ“å–å§¿æ€ã€è·¯å¾„è§„åˆ’ç­‰ç»†ç²’åº¦æ ‡æ³¨çš„å¤§è§„æ¨¡é—®ç­”å¯¹ï¼Œç”¨äºŽè®­ç»ƒå’Œè¯„ä¼°æ¨¡åž‹ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œåœ¨BOP-ASKä¸Šè®­ç»ƒçš„æ¨¡åž‹åœ¨å¯¹è±¡å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’å’Œç©ºé—´æŽ¨ç†æ–¹é¢è¡¨çŽ°å‡ºä¼˜äºŽåŸºçº¿æ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡åž‹(VLM)åœ¨ç©ºé—´æŽ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæžœï¼Œä½†è¿™äº›è¯„ä¼°æŽ©ç›–äº†å…¶åœ¨ç†è§£å¯¹è±¡äº¤äº’æ–¹é¢çš„å…³é”®å¼±ç‚¹ã€‚çŽ°æœ‰åŸºå‡†æµ‹è¯•ä¾§é‡äºŽé«˜çº§å…³ç³»ï¼ˆå¦‚â€œå·¦ä¾§â€ã€â€œåŽæ–¹â€ï¼‰ï¼Œå¿½ç•¥äº†å®žé™…åº”ç”¨æ‰€éœ€çš„ç²¾ç»†ç©ºé—´ç†è§£ï¼ŒåŒ…æ‹¬ç²¾ç¡®çš„3Då®šä½ã€å¯¹è±¡é—´çš„ç‰©ç†å…¼å®¹æ€§ã€å¯¹è±¡å¯ä¾›æ€§ä»¥åŠå¤šæ­¥ç©ºé—´è§„åˆ’ã€‚æœ¬æ–‡æå‡ºäº†BOP-ASKï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºŽå¯¹è±¡äº¤äº’æŽ¨ç†çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯ç”¨äºŽè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ã€‚æ•°æ®ç”Ÿæˆæµç¨‹åˆ©ç”¨äº†å¯¹è±¡å§¿æ€ä¼°è®¡åŸºå‡†(BOP)æ•°æ®é›†ä¸­çš„6Då¯¹è±¡å§¿æ€ï¼Œä»Žä¸­æå–ç²¾ç»†çš„æ ‡æ³¨ï¼Œå¦‚æŠ“å–å§¿æ€ã€å‚è€ƒå¯¹è±¡å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠå¯¹è±¡é—´å…³ç³»ã€‚BOP-ASKåŒ…å«è¶…è¿‡15ä¸‡å¼ å›¾åƒå’Œ3300ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–å…­ä¸ªä»»åŠ¡ï¼ˆå…¶ä¸­å››ä¸ªæ˜¯æ–°çš„ï¼‰ï¼Œä¸ºè®­ç»ƒå’Œè¯„ä¼°VLMæä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸“æœ‰å’Œå¼€æºçš„VLMï¼Œå¹¶å¯¹BOP-ASK-coreï¼ˆä¸€ä¸ªè´¡çŒ®çš„æµ‹è¯•åŸºå‡†ï¼‰è¿›è¡Œäº†äººå·¥è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†BOP-ASK-labï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¤–åŸºå‡†ï¼Œå…¶å›¾åƒå¹¶éžæ¥è‡ªBOPï¼Œç”¨äºŽæµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚å®žéªŒè¡¨æ˜Žï¼Œåœ¨BOP-ASKä¸Šè®­ç»ƒçš„æ¨¡åž‹ä¼˜äºŽåŸºçº¿æ¨¡åž‹ï¼Œå¹¶å±•çŽ°å‡ºç²¾ç¡®çš„å¯¹è±¡å’ŒæŠ“å–å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’ä»¥åŠåœ¨æ‚ä¹±çŽ¯å¢ƒä¸­è¿›è¡Œç²¾ç»†çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç©ºé—´æŽ¨ç†ç­‰æ–°å…´èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œæ•°æ®é›†ç”Ÿæˆæµç¨‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨ç©ºé—´æŽ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸»è¦å…³æ³¨é«˜çº§çš„ç©ºé—´å…³ç³»ï¼Œä¾‹å¦‚â€œå·¦è¾¹â€ã€â€œåŽé¢â€ç­‰ï¼Œç¼ºä¹å¯¹ç‰©ä½“ä¹‹é—´ç»†ç²’åº¦äº¤äº’å…³ç³»çš„ç†è§£ã€‚è¿™å¯¼è‡´æ¨¡åž‹åœ¨éœ€è¦ç²¾ç¡®3Då®šä½ã€ç‰©ç†å…¼å®¹æ€§åˆ¤æ–­ã€ç‰©ä½“å¯ä¾›æ€§åˆ†æžä»¥åŠå¤šæ­¥ç©ºé—´è§„åˆ’ç­‰å®žé™…åº”ç”¨ä¸­è¡¨çŽ°ä¸ä½³ã€‚çŽ°æœ‰æ•°æ®é›†æ— æ³•å……åˆ†è¯„ä¼°å’Œæå‡æ¨¡åž‹åœ¨è¿™äº›æ–¹é¢çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€ç»†ç²’åº¦çš„å¯¹è±¡äº¤äº’æŽ¨ç†æ•°æ®é›†BOP-ASKï¼Œè¯¥æ•°æ®é›†åŸºäºŽçŽ°æœ‰çš„å¯¹è±¡å§¿æ€ä¼°è®¡åŸºå‡†BOPæ•°æ®é›†ï¼Œåˆ©ç”¨å…¶ç²¾ç¡®çš„6Då¯¹è±¡å§¿æ€ä¿¡æ¯ï¼Œç”ŸæˆåŒ…å«æŠ“å–å§¿æ€ã€å‚è€ƒå¯¹è±¡å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠå¯¹è±¡é—´å…³ç³»ç­‰å¤šç§ç±»åž‹çš„æ ‡æ³¨ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼Œå¯ä»¥æå‡æ¨¡åž‹å¯¹å¯¹è±¡äº¤äº’çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šBOP-ASKçš„æ•°æ®é›†ç”Ÿæˆæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆ©ç”¨BOPæ•°æ®é›†ä¸­çš„6Då¯¹è±¡å§¿æ€ä¿¡æ¯ï¼›2) åŸºäºŽè¿™äº›å§¿æ€ä¿¡æ¯ï¼Œç”Ÿæˆå„ç§ç±»åž‹çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬æŠ“å–å§¿æ€ã€å‚è€ƒå¯¹è±¡å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠå¯¹è±¡é—´å…³ç³»ï¼›3) å°†å›¾åƒå’Œå¯¹åº”çš„æ ‡æ³¨è½¬åŒ–ä¸ºé—®ç­”å¯¹çš„å½¢å¼ï¼Œæž„å»ºæœ€ç»ˆçš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æž„å»ºäº†BOP-ASK-coreå’ŒBOP-ASK-labä¸¤ä¸ªæµ‹è¯•åŸºå‡†ï¼Œåˆ†åˆ«ç”¨äºŽè¯„ä¼°æ¨¡åž‹åœ¨åŒåˆ†å¸ƒå’Œåˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šBOP-ASKçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ç»†ç²’åº¦çš„å¯¹è±¡äº¤äº’æ ‡æ³¨ã€‚ä¸ŽçŽ°æœ‰æ•°æ®é›†åªå…³æ³¨é«˜çº§ç©ºé—´å…³ç³»ä¸åŒï¼ŒBOP-ASKæä¾›äº†åŒ…æ‹¬æŠ“å–å§¿æ€ã€è·¯å¾„è§„åˆ’ç­‰åœ¨å†…çš„å¤šç§ç±»åž‹çš„ç»†ç²’åº¦æ ‡æ³¨ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„å¯¹è±¡äº¤äº’çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒBOP-ASKè¿˜æž„å»ºäº†åˆ†å¸ƒå¤–æµ‹è¯•åŸºå‡†BOP-ASK-labï¼Œç”¨äºŽè¯„ä¼°æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šBOP-ASKæ•°æ®é›†åŒ…å«è¶…è¿‡15ä¸‡å¼ å›¾åƒå’Œ3300ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–å…­ä¸ªä»»åŠ¡ã€‚æ•°æ®é›†çš„æ ‡æ³¨ç”Ÿæˆè¿‡ç¨‹ä¾èµ–äºŽBOPæ•°æ®é›†æä¾›çš„ç²¾ç¡®6Då¯¹è±¡å§¿æ€ä¿¡æ¯ã€‚BOP-ASK-coreæ˜¯ä¸€ä¸ªä»ŽBOPæ•°æ®é›†ä¸­é‡‡æ ·çš„æµ‹è¯•é›†ï¼Œè€ŒBOP-ASK-labåˆ™åŒ…å«æ¥è‡ªå…¶ä»–æ¥æºçš„å›¾åƒï¼Œç”¨äºŽè¯„ä¼°æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡æ²¡æœ‰è¯¦ç»†è¯´æ˜Žå…·ä½“çš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æž„ï¼Œè€Œæ˜¯ä¾§é‡äºŽæ•°æ®é›†çš„æž„å»ºå’Œè¯„ä¼°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨BOP-ASKæ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨å¯¹è±¡å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’å’Œç©ºé—´æŽ¨ç†ç­‰ä»»åŠ¡ä¸Šå‡ä¼˜äºŽåŸºçº¿æ¨¡åž‹ã€‚æ¨¡åž‹å±•çŽ°å‡ºç²¾ç¡®çš„å¯¹è±¡å’ŒæŠ“å–å§¿æ€ä¼°è®¡èƒ½åŠ›ï¼Œä»¥åŠåœ¨æ‚ä¹±çŽ¯å¢ƒä¸­è¿›è¡Œç»†ç²’åº¦çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚BOP-ASK-labä¸Šçš„è¯„ä¼°ä¹ŸéªŒè¯äº†æ¨¡åž‹å…·æœ‰ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ¨¡åž‹ç†è§£ç‰©ä½“ä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œä»Žè€Œæ›´å¥½åœ°å®ŒæˆæŠ“å–ã€è£…é…ç­‰ä»»åŠ¡ã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥æ¨¡åž‹ç†è§£è½¦è¾†ä¸Žè¡Œäººã€è½¦è¾†ä¸Žè½¦è¾†ä¹‹é—´çš„å…³ç³»ï¼Œä»Žè€Œæé«˜è¡Œé©¶å®‰å…¨æ€§ã€‚å¢žå¼ºçŽ°å®žåº”ç”¨å¯ä»¥åˆ©ç”¨è¯¥æ¨¡åž‹å®žçŽ°æ›´è‡ªç„¶çš„ç‰©ä½“äº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.

