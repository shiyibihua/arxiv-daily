---
layout: default
title: Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation
---

# Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation

**arXiv**: [2511.16435v1](https://arxiv.org/abs/2511.16435) | [PDF](https://arxiv.org/pdf/2511.16435.pdf)

**ä½œè€…**: Jin Wang, Bingfeng Zhang, Jian Pang, Mengyu Liu, Honglong Chen, Weifeng Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€é©±åŠ¨å±žæ€§æ³›åŒ–æž¶æž„ä»¥è§£å†³å°‘æ ·æœ¬åˆ†å‰²ä¸­è§†è§‰æ”¯æŒåå·®é—®é¢˜**

**å…³é”®è¯**: `å°‘æ ·æœ¬åˆ†å‰²` `è¯­è¨€é©±åŠ¨å±žæ€§æ³›åŒ–` `å¤šæ¨¡æ€åŒ¹é…` `å¤§è¯­è¨€æ¨¡åž‹` `è·¨æ¨¡æ€å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå°‘æ ·æœ¬åˆ†å‰²ä¸­è§†è§‰æ”¯æŒæ ·æœ¬å› ç±»å†…å˜åŒ–å¯¼è‡´å…ƒæŒ‡å¯¼ä¸å‡†ç¡®
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹ç”Ÿæˆå¤šå±žæ€§æè¿°ï¼Œé€šè¿‡å¤šæ¨¡æ€åŒ¹é…æž„å»ºé²æ£’æ”¯æŒç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ–¹æ³•åœ¨å®žéªŒä¸­æ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æ–°æœ€ä¼˜æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.

