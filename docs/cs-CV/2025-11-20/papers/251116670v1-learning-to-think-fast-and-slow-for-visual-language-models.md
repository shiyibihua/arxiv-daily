---
layout: default
title: Learning to Think Fast and Slow for Visual Language Models
---

# Learning to Think Fast and Slow for Visual Language Models

**arXiv**: [2511.16670v1](https://arxiv.org/abs/2511.16670) | [PDF](https://arxiv.org/pdf/2511.16670.pdf)

**ä½œè€…**: Chenyu Lin, Cheng Chi, Jinlin Wu, Sharon Li, Kaiyang Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ¨¡å¼æ€è€ƒæ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†æ•ˆçŽ‡**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `åŒæ¨¡å¼æ€è€ƒ` `æŽ¨ç†æ•ˆçŽ‡` `ä»¤ç‰Œä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†é“¾å†—é•¿ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜
2. ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡åž‹æ ¹æ®é—®é¢˜éš¾åº¦è‡ªåŠ¨åˆ‡æ¢å¿«æ…¢æ€è€ƒæ¨¡å¼
3. æ¨¡åž‹æ€§èƒ½åª²ç¾Žå…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜ä»¤ç‰Œæ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.

