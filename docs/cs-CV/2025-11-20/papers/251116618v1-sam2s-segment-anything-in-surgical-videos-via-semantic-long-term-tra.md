---
layout: default
title: SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking
---

# SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking

**arXiv**: [2511.16618v1](https://arxiv.org/abs/2511.16618) | [PDF](https://arxiv.org/pdf/2511.16618.pdf)

**ä½œè€…**: Haofeng Liu, Ziyue Wang, Sudhanshu Mishra, Mingqi Gao, Guanyi Qin, Chang Han Low, Alex Y. W. Kong, Yueming Jin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAM2Sæ¨¡åž‹ä»¥è§£å†³æ‰‹æœ¯è§†é¢‘ä¸­é•¿æ—¶è·Ÿè¸ªä¸Žè¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜**

**å…³é”®è¯**: `æ‰‹æœ¯è§†é¢‘åˆ†å‰²` `é•¿æ—¶è·Ÿè¸ª` `äº¤äº’å¼åˆ†å‰²` `è¯­ä¹‰å­¦ä¹ ` `åŸºå‡†æ•°æ®é›†` `å®žæ—¶æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹æœ¯è§†é¢‘åˆ†å‰²å­˜åœ¨é¢†åŸŸå·®å¼‚å’Œé•¿æ—¶è·Ÿè¸ªä¸è¶³ï¼Œå½±å“äº¤äº’å¼åˆ†å‰²æ¨¡åž‹æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥DiveMemæœºåˆ¶ã€æ—¶é—´è¯­ä¹‰å­¦ä¹ å’ŒæŠ—æ¨¡ç³Šå­¦ä¹ ï¼Œå¢žå¼ºSAM2çš„é²æ£’æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SA-SVåŸºå‡†ä¸Šï¼ŒSAM2Sè¾¾åˆ°80.42å¹³å‡J&Fï¼Œä¼˜äºŽåŸºçº¿å¹¶ä¿æŒå®žæ—¶æŽ¨ç†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.

