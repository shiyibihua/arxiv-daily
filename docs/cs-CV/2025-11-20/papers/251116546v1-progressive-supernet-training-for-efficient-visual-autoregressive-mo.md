---
layout: default
title: Progressive Supernet Training for Efficient Visual Autoregressive Modeling
---

# Progressive Supernet Training for Efficient Visual Autoregressive Modeling

**arXiv**: [2511.16546v1](https://arxiv.org/abs/2511.16546) | [PDF](https://arxiv.org/pdf/2511.16546.pdf)

**ä½œè€…**: Xiaoyue Chen, Yuling Shi, Kaiyuan Li, Huandong Wang, Yong Li, Xiaodong Gu, Xinlei Chen, Mingbao Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVARiantæ¸è¿›è¶…ç½‘è®­ç»ƒä»¥è§£å†³è§†è§‰è‡ªå›žå½’æ¨¡åž‹å†…å­˜å¼€é”€é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è‡ªå›žå½’å»ºæ¨¡` `æ¸è¿›è®­ç»ƒ` `æƒé‡å…±äº«` `å†…å­˜ä¼˜åŒ–` `å¤šå°ºåº¦ç”Ÿæˆ` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è‡ªå›žå½’æ¨¡åž‹å¤šå°ºåº¦ç”Ÿæˆå› KVç¼“å­˜ç´¯ç§¯å¯¼è‡´é«˜å†…å­˜å¼€é”€ï¼Œé™åˆ¶éƒ¨ç½²
2. åˆ©ç”¨å°ºåº¦-æ·±åº¦éžå¯¹ç§°ä¾èµ–ï¼Œé€šè¿‡æƒé‡å…±äº«å­ç½‘å®žçŽ°çµæ´»æ·±åº¦è°ƒæ•´
3. å®žéªŒæ˜¾ç¤ºVARiantåœ¨ImageNetä¸Šæ˜¾è‘—é™ä½Žå†…å­˜å’ŒåŠ é€Ÿï¼Œä¿æŒç”Ÿæˆè´¨é‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.
>   We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.
>   However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.
>   Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.

