---
layout: default
title: Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions
---

# Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions

**arXiv**: [2511.16221v1](https://arxiv.org/abs/2511.16221) | [PDF](https://arxiv.org/pdf/2511.16221.pdf)

**ä½œè€…**: Caixin Kang, Yifei Huang, Liangyang Ouyang, Mingfang Zhang, Ruicong Liu, Yoichi Sato

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€äº¤äº’æ¬ºéª—è¯„ä¼°ä»»åŠ¡ä¸Žæ•°æ®é›†ï¼Œä»¥è§£å†³MLLMsåœ¨å¤æ‚ç¤¾äº¤ä¸­è¯†åˆ«æ¬ºéª—çš„ä¸è¶³ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç¤¾äº¤äº’åŠ¨åˆ†æž` `æ¬ºéª—æ£€æµ‹` `å¤šæ¨¡æ€åŸºå‡†` `æŽ¨ç†ç®¡é“` `åŠ¨æ€è®°å¿†æ¨¡å—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMsç¼ºä¹åœ¨å¤šäººç¤¾äº¤äº’åŠ¨ä¸­è¯„ä¼°æ¬ºéª—çš„èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆç†è§£å¤šæ¨¡æ€ç¤¾äº¤çº¿ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥MIDAä»»åŠ¡å’Œæ•°æ®é›†ï¼Œè®¾è®¡SoCoTæŽ¨ç†ç®¡é“å’ŒDSEMæ¨¡å—ä»¥æå‡æ¨¡åž‹æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°12ä¸ªMLLMsæ˜¾ç¤ºæ€§èƒ½å·®è·å¤§ï¼Œæ–°æ¡†æž¶åœ¨ä»»åŠ¡ä¸Šå¸¦æ¥æ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.

