---
layout: default
title: FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models
---

# FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models

**arXiv**: [2511.16233v1](https://arxiv.org/abs/2511.16233) | [PDF](https://arxiv.org/pdf/2511.16233.pdf)

**ä½œè€…**: Kewei Chen, Yayu Long, Shuai Li, Mingsheng Shang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFT-NCFMæ•°æ®è’¸é¦æ¡†æž¶ä»¥è§£å†³VLAæ¨¡åž‹æ•°æ®å†—ä½™é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æ•°æ®è’¸é¦` `å› æžœå½’å› ` `æ¨¡åž‹æ— å…³æ•°æ®` `é«˜æ•ˆè®­ç»ƒ` `æ ¸å¿ƒé›†åˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹ä¾èµ–å¤§è§„æ¨¡å†—ä½™æ•°æ®ï¼Œé˜»ç¢å¹¿æ³›åº”ç”¨
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå› æžœå½’å› å’Œç¨‹åºåŒ–å¯¹æ¯”éªŒè¯ï¼Œåˆæˆä¿¡æ¯å¯†é›†æ•°æ®
3. å®žéªŒæˆ–æ•ˆæžœï¼šè’¸é¦æ•°æ®é›†ä»…éœ€5%æ•°æ®ï¼Œæ€§èƒ½è¾¾85-90%ï¼Œè®­ç»ƒæ—¶é—´å‡80%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.

