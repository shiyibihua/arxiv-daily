---
layout: default
title: Contrastive vision-language learning with paraphrasing and negation
---

# Contrastive vision-language learning with paraphrasing and negation

**arXiv**: [2511.16527v1](https://arxiv.org/abs/2511.16527) | [PDF](https://arxiv.org/pdf/2511.16527.pdf)

**ä½œè€…**: Kwun Ho Ngan, Saman Sadeghi Afgeh, Joe Townsend, Artur d'Avila Garcez

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSemCLIPä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹å¯¹è¯­ä¹‰å˜æ¢çš„é²æ£’æ€§**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `è¯­ä¹‰é²æ£’æ€§` `å¦å®šå¤„ç†` `è½¬è¿°å¤„ç†` `é›¶æ ·æœ¬åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPæ¨¡åž‹åœ¨å¦å®šå’Œè½¬è¿°æ–‡æœ¬ä¸Šè¡¨çŽ°ä¸ç¨³å®šï¼Œå½±å“å›¾åƒæ£€ç´¢å‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ–°å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œç»“åˆLLMç”ŸæˆåŽŸå§‹ã€è½¬è¿°å’Œå¦å®šæ–‡æœ¬ä¸‰å…ƒç»„è¿›è¡Œè®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CC-NegåŸºå‡†ä¸Šï¼Œå›¾åƒæ£€ç´¢å‡†ç¡®çŽ‡ä»Ž68.1%æå‡è‡³78.1%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.

