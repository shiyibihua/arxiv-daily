---
layout: default
title: CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation
---

# CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation

**arXiv**: [2511.16428v1](https://arxiv.org/abs/2511.16428) | [PDF](https://arxiv.org/pdf/2511.16428.pdf)

**ä½œè€…**: Samer Abualhanud, Christian Grannemann, Max Mehltretter

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CylinderDepthï¼šåˆ©ç”¨æŸ±é¢ç©ºé—´æ³¨æ„åŠ›å®žçŽ°å¤šè§†è§’ä¸€è‡´çš„è‡ªç›‘ç£çŽ¯è§†æ·±åº¦ä¼°è®¡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `çŽ¯è§†æ·±åº¦ä¼°è®¡` `è‡ªç›‘ç£å­¦ä¹ ` `å¤šè§†è§’ä¸€è‡´æ€§` `ç©ºé—´æ³¨æ„åŠ›` `åœ†æŸ±åæ ‡ç³»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è‡ªç›‘ç£çŽ¯è§†æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨é‡å å›¾åƒåŒºåŸŸçš„æ·±åº¦ä¼°è®¡ä¸Šå­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œå½±å“äº†3Dåœºæ™¯ç†è§£çš„å‡†ç¡®æ€§ã€‚
2. è¯¥æ–¹æ³•å°†å¤šè§†è§’å›¾åƒçš„3Dç‚¹äº‘æŠ•å½±åˆ°å…±äº«åœ†æŸ±ä½“ä¸Šï¼Œåˆ©ç”¨åœ†æŸ±ä½“ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶èšåˆè·¨è§†è§’ç‰¹å¾ï¼Œæå‡æ·±åº¦ä¸€è‡´æ€§ã€‚
3. åœ¨DDADå’ŒnuScenesæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ·±åº¦ä¼°è®¡çš„è·¨è§†è§’ä¸€è‡´æ€§ï¼Œå¹¶æå‡äº†æ•´ä½“æ·±åº¦ä¼°è®¡çš„ç²¾åº¦ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‡ ä½•å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºŽæ ‡å®šçš„ã€æ—¶é—´åŒæ­¥çš„å¤šç›¸æœºç³»ç»Ÿï¼Œä»¥é¢„æµ‹ç¨ å¯†çš„ã€åº¦é‡çš„å’Œè·¨è§†è§’ä¸€è‡´çš„æ·±åº¦ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³çŽ°æœ‰è‡ªç›‘ç£çŽ¯è§†æ·±åº¦ä¼°è®¡æ–¹æ³•ä¸­ï¼Œé‡å å›¾åƒé—´æ·±åº¦ä¼°è®¡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œä¸ºæ¯ä¸ªå›¾åƒé¢„æµ‹ä¸€ä¸ªåˆå§‹æ·±åº¦å›¾ï¼Œå¹¶å°†æ‰€æœ‰å›¾åƒå¯¼å‡ºçš„3Dç‚¹æŠ•å½±åˆ°ä¸€ä¸ªå…±äº«çš„å•ä½åœ†æŸ±ä½“ä¸Šï¼Œä»Žè€Œå»ºç«‹ä¸åŒå›¾åƒä¹‹é—´çš„é‚»åŸŸå…³ç³»ã€‚è¿™ä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆä¸€ä¸ª2Dä½ç½®å›¾ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¢«åˆ†é…å…¶åœ¨åœ†æŸ±ä½“ä¸Šçš„æŠ•å½±ä½ç½®ã€‚åŸºäºŽè¿™äº›ä½ç½®å›¾ï¼Œåº”ç”¨æ˜¾å¼çš„ã€éžå­¦ä¹ çš„ç©ºé—´æ³¨æ„åŠ›ï¼Œæ ¹æ®åƒç´ åœ¨åœ†æŸ±ä½“ä¸Šçš„è·ç¦»èšåˆå›¾åƒé—´çš„ç‰¹å¾ï¼Œä»¥é¢„æµ‹æ¯ä¸ªå›¾åƒçš„æœ€ç»ˆæ·±åº¦å›¾ã€‚åœ¨DDADå’ŒnuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜Žï¼Œä¸Žæœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æé«˜äº†å›¾åƒé—´æ·±åº¦ä¼°è®¡çš„ä¸€è‡´æ€§å’Œæ•´ä½“æ·±åº¦ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è‡ªç›‘ç£çŽ¯è§†æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨å¤„ç†å¤šè§†è§’å›¾åƒæ—¶ï¼Œç”±äºŽç¼ºä¹æœ‰æ•ˆçš„è·¨è§†è§’ä¿¡æ¯èžåˆæœºåˆ¶ï¼Œå¯¼è‡´åœ¨å›¾åƒé‡å åŒºåŸŸçš„æ·±åº¦ä¼°è®¡ç»“æžœä¸ä¸€è‡´ã€‚è¿™ç§ä¸ä¸€è‡´æ€§ä¼šä¸¥é‡å½±å“åŽç»­çš„ä¸‰ç»´åœºæ™¯ç†è§£å’Œæ„ŸçŸ¥ä»»åŠ¡ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€è·¯å¾„è§„åˆ’ç­‰ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽå¤æ‚çš„åŽå¤„ç†æˆ–é¢å¤–çš„çº¦æŸæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†æ•ˆæžœæœ‰é™ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤šè§†è§’å›¾åƒçš„æ·±åº¦ä¿¡æ¯æŠ•å½±åˆ°ä¸€ä¸ªå…±äº«çš„åœ†æŸ±åæ ‡ç³»ä¸­ï¼Œä»Žè€Œå»ºç«‹è·¨è§†è§’åƒç´ ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚é€šè¿‡åœ¨åœ†æŸ±åæ ‡ç³»ä¸‹è¿›è¡Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°èšåˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ï¼Œä»Žè€Œæé«˜æ·±åº¦ä¼°è®¡çš„ä¸€è‡´æ€§ã€‚é€‰æ‹©åœ†æŸ±åæ ‡ç³»æ˜¯å› ä¸ºå…¶èƒ½å¤Ÿè¾ƒå¥½åœ°é€‚åº”çŽ¯è§†ç›¸æœºçš„è§†é‡ŽèŒƒå›´ï¼Œå¹¶ç®€åŒ–è·¨è§†è§’å‡ ä½•å…³ç³»çš„å»ºæ¨¡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆå§‹æ·±åº¦ä¼°è®¡ï¼šä½¿ç”¨çŽ°æœ‰çš„è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ç½‘ç»œä¸ºæ¯ä¸ªå›¾åƒé¢„æµ‹ä¸€ä¸ªåˆå§‹æ·±åº¦å›¾ã€‚2) 3Dç‚¹äº‘æŠ•å½±ï¼šåˆ©ç”¨ç›¸æœºå†…å¤–å‚å°†æ¯ä¸ªå›¾åƒçš„åƒç´ ç‚¹åæŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ï¼Œå¾—åˆ°3Dç‚¹äº‘ã€‚3) åœ†æŸ±åæ ‡è½¬æ¢ï¼šå°†3Dç‚¹äº‘æŠ•å½±åˆ°å…±äº«çš„å•ä½åœ†æŸ±ä½“ä¸Šï¼Œå¾—åˆ°æ¯ä¸ªåƒç´ åœ¨åœ†æŸ±ä½“ä¸Šçš„2Dä½ç½®ã€‚4) ç©ºé—´æ³¨æ„åŠ›èšåˆï¼šåŸºäºŽåœ†æŸ±ä½“ä¸Šçš„2Dä½ç½®ï¼Œè®¡ç®—åƒç´ ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶èšåˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ã€‚5) æœ€ç»ˆæ·±åº¦é¢„æµ‹ï¼šåˆ©ç”¨èšåˆåŽçš„ç‰¹å¾ï¼Œé¢„æµ‹æ¯ä¸ªå›¾åƒçš„æœ€ç»ˆæ·±åº¦å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†åŸºäºŽåœ†æŸ±åæ ‡çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸Žä¼ ç»Ÿçš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åœ†æŸ±åæ ‡ç³»æ¥å»ºç«‹è·¨è§†è§’åƒç´ ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»Žè€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èšåˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§éžå­¦ä¹ çš„æ–¹æ³•ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼Œå…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åœ†æŸ±åæ ‡è½¬æ¢é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„åœ†æŸ±åŠå¾„å’Œé«˜åº¦ã€‚åœ¨ç©ºé—´æ³¨æ„åŠ›èšåˆé˜¶æ®µï¼Œéœ€è¦è®¾è®¡åˆé€‚çš„æ³¨æ„åŠ›æƒé‡è®¡ç®—æ–¹å¼ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°æ¥è®¡ç®—åƒç´ ä¹‹é—´çš„è·ç¦»æƒé‡ã€‚æŸå¤±å‡½æ•°ä¸»è¦åŒ…æ‹¬æ·±åº¦ä¸€è‡´æ€§æŸå¤±å’Œå…‰åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œç”¨äºŽçº¦æŸæ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ç½‘ç»œç»“æž„æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨çŽ°æœ‰çš„è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ç½‘ç»œä½œä¸ºåˆå§‹æ·±åº¦ä¼°è®¡æ¨¡å—ï¼Œä¾‹å¦‚ResNetæˆ–DenseNetç­‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨DDADå’ŒnuScenesæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®žéªŒç»“æžœè¡¨æ˜Žï¼Œä¸ŽçŽ°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ·±åº¦ä¼°è®¡çš„è·¨è§†è§’ä¸€è‡´æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨DDADæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å°†æ·±åº¦ä¸€è‡´æ€§æŒ‡æ ‡æå‡äº†X%ï¼Œå¹¶å°†æ•´ä½“æ·±åº¦ä¼°è®¡ç²¾åº¦æå‡äº†Y%ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šï¼Œä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤šè§†è§’æ·±åº¦ä¼°è®¡ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹ŸçŽ°å®žç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥æä¾›æ›´å‡†ç¡®å’Œä¸€è‡´çš„çŽ¯è§†æ·±åº¦ä¿¡æ¯ï¼Œä»Žè€Œæé«˜è½¦è¾†å¯¹å‘¨å›´çŽ¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢žå¼ºè¡Œé©¶å®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¸®åŠ©æœºå™¨äººæž„å»ºæ›´ç²¾ç¡®çš„ä¸‰ç»´åœ°å›¾ï¼Œå®žçŽ°æ›´å¯é çš„è‡ªä¸»å¯¼èˆªã€‚åœ¨è™šæ‹ŸçŽ°å®žä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ä¸‰ç»´åœºæ™¯ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360Â° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.

