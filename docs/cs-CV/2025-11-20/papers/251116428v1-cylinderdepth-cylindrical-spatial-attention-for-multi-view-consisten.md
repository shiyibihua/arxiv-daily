---
layout: default
title: CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation
---

# CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.16428" target="_blank" class="toolbar-btn">arXiv: 2511.16428v1</a>
    <a href="https://arxiv.org/pdf/2511.16428.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16428v1" 
            onclick="toggleFavorite(this, '2511.16428v1', 'CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Samer Abualhanud, Christian Grannemann, Max Mehltretter

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CylinderDepthï¼šåˆ©ç”¨æŸ±é¢ç©ºé—´æ³¨æ„åŠ›å®ç°å¤šè§†è§’ä¸€è‡´çš„è‡ªç›‘ç£ç¯è§†æ·±åº¦ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç¯è§†æ·±åº¦ä¼°è®¡` `è‡ªç›‘ç£å­¦ä¹ ` `å¤šè§†è§’ä¸€è‡´æ€§` `ç©ºé—´æ³¨æ„åŠ›` `åœ†æŸ±åæ ‡ç³»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£ç¯è§†æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨é‡å å›¾åƒåŒºåŸŸçš„æ·±åº¦ä¼°è®¡ä¸Šå­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œå½±å“äº†3Dåœºæ™¯ç†è§£çš„å‡†ç¡®æ€§ã€‚
2. è¯¥æ–¹æ³•å°†å¤šè§†è§’å›¾åƒçš„3Dç‚¹äº‘æŠ•å½±åˆ°å…±äº«åœ†æŸ±ä½“ä¸Šï¼Œåˆ©ç”¨åœ†æŸ±ä½“ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶èšåˆè·¨è§†è§’ç‰¹å¾ï¼Œæå‡æ·±åº¦ä¸€è‡´æ€§ã€‚
3. åœ¨DDADå’ŒnuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ·±åº¦ä¼°è®¡çš„è·¨è§†è§’ä¸€è‡´æ€§ï¼Œå¹¶æå‡äº†æ•´ä½“æ·±åº¦ä¼°è®¡çš„ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‡ ä½•å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºæ ‡å®šçš„ã€æ—¶é—´åŒæ­¥çš„å¤šç›¸æœºç³»ç»Ÿï¼Œä»¥é¢„æµ‹ç¨ å¯†çš„ã€åº¦é‡çš„å’Œè·¨è§†è§’ä¸€è‡´çš„æ·±åº¦ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç°æœ‰è‡ªç›‘ç£ç¯è§†æ·±åº¦ä¼°è®¡æ–¹æ³•ä¸­ï¼Œé‡å å›¾åƒé—´æ·±åº¦ä¼°è®¡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œä¸ºæ¯ä¸ªå›¾åƒé¢„æµ‹ä¸€ä¸ªåˆå§‹æ·±åº¦å›¾ï¼Œå¹¶å°†æ‰€æœ‰å›¾åƒå¯¼å‡ºçš„3Dç‚¹æŠ•å½±åˆ°ä¸€ä¸ªå…±äº«çš„å•ä½åœ†æŸ±ä½“ä¸Šï¼Œä»è€Œå»ºç«‹ä¸åŒå›¾åƒä¹‹é—´çš„é‚»åŸŸå…³ç³»ã€‚è¿™ä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆä¸€ä¸ª2Dä½ç½®å›¾ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¢«åˆ†é…å…¶åœ¨åœ†æŸ±ä½“ä¸Šçš„æŠ•å½±ä½ç½®ã€‚åŸºäºè¿™äº›ä½ç½®å›¾ï¼Œåº”ç”¨æ˜¾å¼çš„ã€éå­¦ä¹ çš„ç©ºé—´æ³¨æ„åŠ›ï¼Œæ ¹æ®åƒç´ åœ¨åœ†æŸ±ä½“ä¸Šçš„è·ç¦»èšåˆå›¾åƒé—´çš„ç‰¹å¾ï¼Œä»¥é¢„æµ‹æ¯ä¸ªå›¾åƒçš„æœ€ç»ˆæ·±åº¦å›¾ã€‚åœ¨DDADå’ŒnuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æé«˜äº†å›¾åƒé—´æ·±åº¦ä¼°è®¡çš„ä¸€è‡´æ€§å’Œæ•´ä½“æ·±åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è‡ªç›‘ç£ç¯è§†æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨å¤„ç†å¤šè§†è§’å›¾åƒæ—¶ï¼Œç”±äºç¼ºä¹æœ‰æ•ˆçš„è·¨è§†è§’ä¿¡æ¯èåˆæœºåˆ¶ï¼Œå¯¼è‡´åœ¨å›¾åƒé‡å åŒºåŸŸçš„æ·±åº¦ä¼°è®¡ç»“æœä¸ä¸€è‡´ã€‚è¿™ç§ä¸ä¸€è‡´æ€§ä¼šä¸¥é‡å½±å“åç»­çš„ä¸‰ç»´åœºæ™¯ç†è§£å’Œæ„ŸçŸ¥ä»»åŠ¡ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€è·¯å¾„è§„åˆ’ç­‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„åå¤„ç†æˆ–é¢å¤–çš„çº¦æŸæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†æ•ˆæœæœ‰é™ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤šè§†è§’å›¾åƒçš„æ·±åº¦ä¿¡æ¯æŠ•å½±åˆ°ä¸€ä¸ªå…±äº«çš„åœ†æŸ±åæ ‡ç³»ä¸­ï¼Œä»è€Œå»ºç«‹è·¨è§†è§’åƒç´ ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚é€šè¿‡åœ¨åœ†æŸ±åæ ‡ç³»ä¸‹è¿›è¡Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°èšåˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡çš„ä¸€è‡´æ€§ã€‚é€‰æ‹©åœ†æŸ±åæ ‡ç³»æ˜¯å› ä¸ºå…¶èƒ½å¤Ÿè¾ƒå¥½åœ°é€‚åº”ç¯è§†ç›¸æœºçš„è§†é‡èŒƒå›´ï¼Œå¹¶ç®€åŒ–è·¨è§†è§’å‡ ä½•å…³ç³»çš„å»ºæ¨¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆå§‹æ·±åº¦ä¼°è®¡ï¼šä½¿ç”¨ç°æœ‰çš„è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ç½‘ç»œä¸ºæ¯ä¸ªå›¾åƒé¢„æµ‹ä¸€ä¸ªåˆå§‹æ·±åº¦å›¾ã€‚2) 3Dç‚¹äº‘æŠ•å½±ï¼šåˆ©ç”¨ç›¸æœºå†…å¤–å‚å°†æ¯ä¸ªå›¾åƒçš„åƒç´ ç‚¹åæŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ï¼Œå¾—åˆ°3Dç‚¹äº‘ã€‚3) åœ†æŸ±åæ ‡è½¬æ¢ï¼šå°†3Dç‚¹äº‘æŠ•å½±åˆ°å…±äº«çš„å•ä½åœ†æŸ±ä½“ä¸Šï¼Œå¾—åˆ°æ¯ä¸ªåƒç´ åœ¨åœ†æŸ±ä½“ä¸Šçš„2Dä½ç½®ã€‚4) ç©ºé—´æ³¨æ„åŠ›èšåˆï¼šåŸºäºåœ†æŸ±ä½“ä¸Šçš„2Dä½ç½®ï¼Œè®¡ç®—åƒç´ ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶èšåˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ã€‚5) æœ€ç»ˆæ·±åº¦é¢„æµ‹ï¼šåˆ©ç”¨èšåˆåçš„ç‰¹å¾ï¼Œé¢„æµ‹æ¯ä¸ªå›¾åƒçš„æœ€ç»ˆæ·±åº¦å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºåœ†æŸ±åæ ‡çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åœ†æŸ±åæ ‡ç³»æ¥å»ºç«‹è·¨è§†è§’åƒç´ ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èšåˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§éå­¦ä¹ çš„æ–¹æ³•ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼Œå…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åœ†æŸ±åæ ‡è½¬æ¢é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„åœ†æŸ±åŠå¾„å’Œé«˜åº¦ã€‚åœ¨ç©ºé—´æ³¨æ„åŠ›èšåˆé˜¶æ®µï¼Œéœ€è¦è®¾è®¡åˆé€‚çš„æ³¨æ„åŠ›æƒé‡è®¡ç®—æ–¹å¼ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°æ¥è®¡ç®—åƒç´ ä¹‹é—´çš„è·ç¦»æƒé‡ã€‚æŸå¤±å‡½æ•°ä¸»è¦åŒ…æ‹¬æ·±åº¦ä¸€è‡´æ€§æŸå¤±å’Œå…‰åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œç”¨äºçº¦æŸæ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨ç°æœ‰çš„è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ç½‘ç»œä½œä¸ºåˆå§‹æ·±åº¦ä¼°è®¡æ¨¡å—ï¼Œä¾‹å¦‚ResNetæˆ–DenseNetç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨DDADå’ŒnuScenesæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ·±åº¦ä¼°è®¡çš„è·¨è§†è§’ä¸€è‡´æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨DDADæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å°†æ·±åº¦ä¸€è‡´æ€§æŒ‡æ ‡æå‡äº†X%ï¼Œå¹¶å°†æ•´ä½“æ·±åº¦ä¼°è®¡ç²¾åº¦æå‡äº†Y%ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šï¼Œä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤šè§†è§’æ·±åº¦ä¼°è®¡ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥æä¾›æ›´å‡†ç¡®å’Œä¸€è‡´çš„ç¯è§†æ·±åº¦ä¿¡æ¯ï¼Œä»è€Œæé«˜è½¦è¾†å¯¹å‘¨å›´ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢å¼ºè¡Œé©¶å®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¸®åŠ©æœºå™¨äººæ„å»ºæ›´ç²¾ç¡®çš„ä¸‰ç»´åœ°å›¾ï¼Œå®ç°æ›´å¯é çš„è‡ªä¸»å¯¼èˆªã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ä¸‰ç»´åœºæ™¯ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360Â° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.

