---
layout: default
title: Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning
---

# Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning

**arXiv**: [2511.16160v1](https://arxiv.org/abs/2511.16160) | [PDF](https://arxiv.org/pdf/2511.16160.pdf)

**ä½œè€…**: Yibin Huang, Wang Xu, Wanyue Zhang, Helu Zhi, Jingjing Huang, Yangbin Xu, Yangang Sun, Conghui Zhu, Tiejun Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo2Layoutæ¡†æž¶ï¼Œé€šè¿‡è¿žç»­è¾¹ç•Œåæ ‡é‡å»ºç©ºé—´å¸ƒå±€ä»¥è§£å†³ç»†ç²’åº¦ç©ºé—´æŽ¨ç†é—®é¢˜**

**å…³é”®è¯**: `ç©ºé—´æŽ¨ç†` `è®¤çŸ¥åœ°å›¾` `è§†é¢‘ç†è§£` `è¿žç»­åæ ‡` `å¼ºåŒ–å¾®è°ƒ` `åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç½‘æ ¼è®¤çŸ¥åœ°å›¾æ–¹æ³•ä¾èµ–ç¦»æ•£æ …æ ¼è¡¨ç¤ºï¼Œé™åˆ¶ç»†ç²’åº¦ç©ºé—´æŽ¨ç†èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¿žç»­å¯¹è±¡è¾¹ç•Œåæ ‡é‡åŒ–ç‰©ç†è·ç¦»å’Œå¤§å°ï¼Œç»“åˆç›‘ç£å’Œå¼ºåŒ–å¾®è°ƒé˜¶æ®µ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨QVS-Benchç­‰åŸºå‡†ä¸Šï¼ŒV2LO-7Bæ¨¡åž‹å¹³å‡æå‡4.92%ï¼ŒéªŒè¯æ–¹æ³•ä¼˜è¶Šæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.

