---
layout: default
title: SAM 3: Segment Anything with Concepts
---

# SAM 3: Segment Anything with Concepts

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.16719" target="_blank" class="toolbar-btn">arXiv: 2511.16719v1</a>
    <a href="https://arxiv.org/pdf/2511.16719.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16719v1" 
            onclick="toggleFavorite(this, '2511.16719v1', 'SAM 3: Segment Anything with Concepts')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, Jie Lei, Tengyu Ma, Baishan Guo, Arpit Kalla, Markus Marks, Joseph Greer, Meng Wang, Peize Sun, Roman R√§dle, Triantafyllos Afouras, Effrosyni Mavroudi, Katherine Xu, Tsung-Han Wu, Yu Zhou, Liliane Momeni, Rishi Hazra, Shuangrui Ding, Sagar Vaze, Francois Porcher, Feng Li, Siyuan Li, Aishwarya Kamath, Ho Kei Cheng, Piotr Doll√°r, Nikhila Ravi, Kate Saenko, Pengchuan Zhang, Christoph Feichtenhofer

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SAM 3ÔºöÂü∫‰∫éÊ¶ÇÂøµÊèêÁ§∫ÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÈÄöÁî®ÂàÜÂâ≤Ê®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂõæÂÉèÂàÜÂâ≤` `ËßÜÈ¢ëÂàÜÂâ≤` `Ê¶ÇÂøµÊèêÁ§∫` `ÁõÆÊ†áÊ£ÄÊµã` `ÁõÆÊ†áË∑üË∏™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂàÜÂâ≤Ê®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÂú∫ÊôØÂíåÊ¶ÇÂøµÊèêÁ§∫Êó∂Â≠òÂú®Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ
2. SAM 3ÈÄöËøáÂºïÂÖ•Ê¶ÇÂøµÊèêÁ§∫ÔºåÂπ∂ÁªìÂêàÂõæÂÉèÁ∫ßÊ£ÄÊµãÂô®ÂíåËßÜÈ¢ëË∑üË∏™Âô®ÔºåÂÆûÁé∞‰∫ÜÊõ¥Á≤æÁ°ÆÁöÑÂØπË±°ÂàÜÂâ≤ÂíåË∑üË∏™„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSAM 3Âú®ÂõæÂÉèÂíåËßÜÈ¢ëPCS‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁ≤æÂ∫¶ÊèêÈ´ò‰∫Ü‰∏ÄÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜSegment Anything Model (SAM) 3Ôºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ®°ÂûãÔºåËÉΩÂ§üÂü∫‰∫éÊ¶ÇÂøµÊèêÁ§∫Âú®ÂõæÂÉèÂíåËßÜÈ¢ë‰∏≠Ê£ÄÊµã„ÄÅÂàÜÂâ≤ÂíåË∑üË∏™ÂØπË±°„ÄÇÊ¶ÇÂøµÊèêÁ§∫Ë¢´ÂÆö‰πâ‰∏∫ÁÆÄÁü≠ÁöÑÂêçËØçÁü≠ËØ≠Ôºà‰æãÂ¶ÇÔºå‚ÄúÈªÑËâ≤Ê†°ËΩ¶‚ÄùÔºâ„ÄÅÂõæÂÉèÁ§∫‰æãÊàñ‰∏§ËÄÖÁöÑÁªÑÂêà„ÄÇÂèØÊèêÁ§∫ÁöÑÊ¶ÇÂøµÂàÜÂâ≤ÔºàPCSÔºâÊé•ÂèóËøô‰∫õÊèêÁ§∫ÔºåÂπ∂ËøîÂõûÊâÄÊúâÂåπÈÖçÂØπË±°ÂÆû‰æãÁöÑÂàÜÂâ≤Êé©Á†ÅÂíåÂîØ‰∏ÄË∫´‰ªΩÊ†áËØÜ„ÄÇ‰∏∫‰∫ÜÊé®ËøõPCSÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÊï∞ÊçÆÂºïÊìéÔºåÁîüÊàê‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂõæÂÉèÂíåËßÜÈ¢ë‰∏≠400‰∏á‰∏™Áã¨ÁâπÁöÑÊ¶ÇÂøµÊ†áÁ≠æÔºåÂåÖÊã¨ÈöæË¥ü‰æã„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÁî±ÂõæÂÉèÁ∫ßÊ£ÄÊµãÂô®ÂíåÂü∫‰∫éÂÜÖÂ≠òÁöÑËßÜÈ¢ëË∑üË∏™Âô®ÁªÑÊàêÔºåÂÆÉ‰ª¨ÂÖ±‰∫´‰∏Ä‰∏™È™®Âπ≤ÁΩëÁªú„ÄÇËØÜÂà´ÂíåÂÆö‰ΩçÈÄöËøáÂ≠òÂú®Â§¥Ëß£ËÄ¶Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇSAM 3Âú®ÂõæÂÉèÂíåËßÜÈ¢ëPCS‰∏≠ÁöÑÁ≤æÂ∫¶ÊòØÁé∞ÊúâÁ≥ªÁªüÁöÑ‰∏§ÂÄçÔºåÂπ∂ÊèêÈ´ò‰∫ÜÂÖàÂâçSAMÂú®ËßÜËßâÂàÜÂâ≤‰ªªÂä°‰∏äÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºÄÊ∫ê‰∫ÜSAM 3‰ª•ÂèäÊàë‰ª¨Êñ∞ÁöÑÁî®‰∫éÂèØÊèêÁ§∫Ê¶ÇÂøµÂàÜÂâ≤ÁöÑSegment Anything with Concepts (SA-Co)Âü∫ÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂàÜÂâ≤Ê®°ÂûãÈöæ‰ª•Ê†πÊçÆÁî®Êà∑Êèê‰æõÁöÑÊ¶ÇÂøµÊÄßÊèêÁ§∫Ôºà‰æãÂ¶Ç‚ÄúÊüêÁßçÁ±ªÂûãÁöÑÁâ©‰Ωì‚ÄùÔºâËøõË°åÁ≤æÁ°ÆÂàÜÂâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜÈ¢ë‰∏≠ËøõË°åË∑üË∏™Êó∂ÔºåÊåëÊàòÊõ¥Â§ß„ÄÇ‰πãÂâçÁöÑSAMÊ®°ÂûãËôΩÁÑ∂Âº∫Â§ßÔºå‰ΩÜÂú®Â§ÑÁêÜÊ¶ÇÂøµÊèêÁ§∫ÂíåËßÜÈ¢ëÂàÜÂâ≤ÊñπÈù¢‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSAM 3ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê¶ÇÂøµÊèêÁ§∫ÔºàÊñáÊú¨ÊàñÂõæÂÉèÔºâÊù•ÂºïÂØºÂàÜÂâ≤ËøáÁ®ãÔºåÂπ∂ÁªìÂêàÂõæÂÉèÊ£ÄÊµãÂíåËßÜÈ¢ëË∑üË∏™ÊäÄÊúØÔºåÂÆûÁé∞ÂØπÂõæÂÉèÂíåËßÜÈ¢ë‰∏≠ÁâπÂÆöÊ¶ÇÂøµÂØπË±°ÁöÑÁ≤æÁ°ÆÂàÜÂâ≤ÂíåË∑üË∏™„ÄÇÈÄöËøáËß£ËÄ¶ËØÜÂà´ÂíåÂÆö‰ΩçÔºåÂπ∂ÂºïÂÖ•Â≠òÂú®Â§¥ÔºåÊèêÂçáÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSAM 3ÂåÖÂê´‰∏Ä‰∏™ÂõæÂÉèÁ∫ßÊ£ÄÊµãÂô®Âíå‰∏Ä‰∏™Âü∫‰∫éÂÜÖÂ≠òÁöÑËßÜÈ¢ëË∑üË∏™Âô®Ôºå‰∏§ËÄÖÂÖ±‰∫´‰∏Ä‰∏™È™®Âπ≤ÁΩëÁªú„ÄÇÂõæÂÉèÁ∫ßÊ£ÄÊµãÂô®Ë¥üË¥£Ê£ÄÊµãÂõæÂÉè‰∏≠ÁöÑÂØπË±°ÔºåÂπ∂Ê†πÊçÆÊ¶ÇÂøµÊèêÁ§∫ÁîüÊàêÂàÜÂâ≤Êé©Á†Å„ÄÇËßÜÈ¢ëË∑üË∏™Âô®ÂàôÂà©Áî®ÂÜÖÂ≠òÊú∫Âà∂ÔºåÂú®ËßÜÈ¢ëÂ∏ß‰πãÈó¥Ë∑üË∏™ÂØπË±°ÁöÑË∫´‰ªΩÂíå‰ΩçÁΩÆ„ÄÇÂ≠òÂú®Â§¥Áî®‰∫éÂà§Êñ≠ÂØπË±°ÊòØÂê¶Â≠òÂú®Ôºå‰ªéËÄåÊèêÈ´òÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSAM 3ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊ¶ÇÂøµÊèêÁ§∫‰Ωú‰∏∫ÂàÜÂâ≤ÁöÑÂºïÂØºÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ®°ÂûãÊ°ÜÊû∂ÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÂàÜÂâ≤‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåÈÄöËøáËß£ËÄ¶ËØÜÂà´ÂíåÂÆö‰ΩçÔºåÂπ∂ÂºïÂÖ•Â≠òÂú®Â§¥ÔºåÊèêÈ´ò‰∫ÜÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇÂ§ßËßÑÊ®°ÁöÑÊ¶ÇÂøµÊ†áÁ≠æÊï∞ÊçÆÈõÜ‰πüÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑË¥°ÁåÆ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSAM 3‰ΩøÁî®‰∫ÜTransformer‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉÊ®°Âûã„ÄÇÊ¶ÇÂøµÊèêÁ§∫Ë¢´ÁºñÁ†ÅÊàêÂêëÈáèÔºåÂπ∂Áî®‰∫éÊåáÂØºÂàÜÂâ≤ËøáÁ®ã„ÄÇËßÜÈ¢ëË∑üË∏™Âô®‰ΩøÁî®‰∫ÜÂÜÖÂ≠òÊú∫Âà∂Êù•Â≠òÂÇ®ÂíåÊõ¥Êñ∞ÂØπË±°ÁöÑÁâπÂæÅÔºå‰ªéËÄåÂÆûÁé∞ÂØπÂØπË±°ÁöÑÈïøÊúüË∑üË∏™„ÄÇÂ≠òÂú®Â§¥ÊòØ‰∏Ä‰∏™‰∫åÂàÜÁ±ªÂô®ÔºåÁî®‰∫éÂà§Êñ≠ÂØπË±°ÊòØÂê¶Â≠òÂú®„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SAM 3Âú®ÂõæÂÉèÂíåËßÜÈ¢ëPCS‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁ≤æÂ∫¶ÊòØÁé∞ÊúâÁ≥ªÁªüÁöÑ‰∏§ÂÄç„ÄÇËØ•Ê®°ÂûãÂú®SA-CoÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂèØÊèêÁ§∫Ê¶ÇÂøµÂàÜÂâ≤ÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇÊ≠§Â§ñÔºåSAM 3ËøòÊèêÈ´ò‰∫ÜÂÖàÂâçSAMÂú®ËßÜËßâÂàÜÂâ≤‰ªªÂä°‰∏äÁöÑËÉΩÂäõÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÂÖ∂ÈÄöÁî®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SAM 3Âú®Ëá™Âä®È©æÈ©∂„ÄÅËßÜÈ¢ëÁõëÊéß„ÄÅÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåÂèØ‰ª•Âà©Áî®SAM 3ËØÜÂà´ÂíåË∑üË∏™ÈÅìË∑Ø‰∏äÁöÑËΩ¶ËæÜ„ÄÅË°å‰∫∫Á≠âÁõÆÊ†á„ÄÇÂú®ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÂàÜÂâ≤ÂíåË∑üË∏™ËÇøÁò§Á≠âÁóÖÁÅ∂„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáËÆ°ÁÆóÊú∫ËßÜËßâÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÔºåÂÆûÁé∞Êõ¥Á≤æÁ°Æ„ÄÅÊõ¥È´òÊïàÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.

