---
layout: default
title: Sparse Autoencoders are Topic Models
---

# Sparse Autoencoders are Topic Models

**arXiv**: [2511.16309v1](https://arxiv.org/abs/2511.16309) | [PDF](https://arxiv.org/pdf/2511.16309.pdf)

**ä½œè€…**: Leander Girrbach, Zeynep Akata

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAE-TMæ¡†æž¶ï¼Œå°†ç¨€ç–è‡ªç¼–ç å™¨è§†ä¸ºä¸»é¢˜æ¨¡åž‹ï¼Œç”¨äºŽè·¨æ¨¡æ€å¤§è§„æ¨¡ä¸»é¢˜åˆ†æžã€‚**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `ä¸»é¢˜å»ºæ¨¡` `åµŒå…¥åˆ†æž` `è·¨æ¨¡æ€åˆ†æž` `æœ€å¤§åŽéªŒä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–è‡ªç¼–ç å™¨åœ¨åµŒå…¥åˆ†æžä¸­çš„è§’è‰²ä¸Žå®žç”¨ä»·å€¼å­˜åœ¨äº‰è®®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†LDAæ‰©å±•è‡³åµŒå…¥ç©ºé—´ï¼ŒæŽ¨å¯¼SAEç›®æ ‡ä¸ºæœ€å¤§åŽéªŒä¼°è®¡å™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šSAE-TMåœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ä¸Šç”Ÿæˆæ›´è¿žè´¯ä¸”å¤šæ ·çš„ä¸»é¢˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.

