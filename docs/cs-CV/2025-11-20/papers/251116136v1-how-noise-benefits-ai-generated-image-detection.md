---
layout: default
title: How Noise Benefits AI-generated Image Detection
---

# How Noise Benefits AI-generated Image Detection

**arXiv**: [2511.16136v1](https://arxiv.org/abs/2511.16136) | [PDF](https://arxiv.org/pdf/2511.16136.pdf)

**ä½œè€…**: Jiazhen Yan, Ziqiang Li, Fan Wang, Kai Zeng, Zhangjie Fu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPiN-CLIPæ–¹æ³•ä»¥è§£å†³AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ³›åŒ–é—®é¢˜**

**å…³é”®è¯**: `AIç”Ÿæˆå›¾åƒæ£€æµ‹` `ç‰¹å¾ç©ºé—´å™ªå£°` `æ³›åŒ–èƒ½åŠ›` `CLIPæ¨¡åž‹` `å˜åˆ†ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIç”Ÿæˆå›¾åƒæ£€æµ‹æ¨¡åž‹åœ¨åˆ†å¸ƒå¤–æ³›åŒ–ä¸Šå­˜åœ¨å›°éš¾ï¼ŒæºäºŽè®­ç»ƒä¸­çš„ä¼ªæ·å¾„ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å˜åˆ†æ­£æ¿€åŠ±åŽŸåˆ™è”åˆè®­ç»ƒå™ªå£°ç”Ÿæˆå™¨å’Œæ£€æµ‹ç½‘ç»œï¼Œæ³¨å…¥ç‰¹å¾ç©ºé—´å™ªå£°ä»¥æŠ‘åˆ¶æ·å¾„ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨42ç§ç”Ÿæˆæ¨¡åž‹çš„å¼€æ”¾ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œå¹³å‡å‡†ç¡®çŽ‡æå‡5.4ï¼Œè¾¾åˆ°æ–°SOTAã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.

