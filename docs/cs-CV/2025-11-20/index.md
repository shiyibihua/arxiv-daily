---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-11-20
---

# cs.CVï¼ˆ2025-11-20ï¼‰

ğŸ“Š å…± **40** ç¯‡è®ºæ–‡
 | ğŸ”— **9** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (21 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (21 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251116091v1-rad-gs-radar-vision-integration-for-3d-gaussian-splatting-slam-in-ou.html">Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments</a></td>
  <td>Rad-GSï¼šç”¨äºå®¤å¤–ç¯å¢ƒçš„é›·è¾¾-è§†è§‰èåˆ3Dé«˜æ–¯æº…å°„SLAM</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16091v1" onclick="toggleFavorite(this, '2511.16091v1', 'Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251116282v2-building-temporally-coherent-3d-maps-with-vggt-for-memory-efficient-.html">Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</a></td>
  <td>æå‡ºåŸºäºVGGTçš„æ—¶åºä¸€è‡´æ€§3Dåœ°å›¾æ„å»ºæ–¹æ³•ï¼Œç”¨äºå†…å­˜é«˜æ•ˆçš„è¯­ä¹‰SLAM</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16282v2" onclick="toggleFavorite(this, '2511.16282v2', 'Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251116030v1-curigs-curriculum-guided-gaussian-splatting-for-sparse-view-synthesi.html">CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis</a></td>
  <td>CuriGSï¼šé¢å‘ç¨€ç–è§†å›¾åˆæˆçš„è¯¾ç¨‹å¼•å¯¼é«˜æ–¯æº…å°„æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16030v1" onclick="toggleFavorite(this, '2511.16030v1', 'CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251116144v1-lego-slam-language-embedded-gaussian-optimization-slam.html">LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM</a></td>
  <td>LEGO-SLAMï¼šåŸºäºè¯­è¨€åµŒå…¥é«˜æ–¯ä¼˜åŒ–çš„å®æ—¶å¼€æ”¾è¯æ±‡SLAMç³»ç»Ÿ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16144v1" onclick="toggleFavorite(this, '2511.16144v1', 'LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251116349v1-cristal-real-time-camera-registration-in-static-lidar-scans-using-ne.html">CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</a></td>
  <td>CRISTALï¼šåˆ©ç”¨ç¥ç»æ¸²æŸ“åœ¨é™æ€æ¿€å…‰é›·è¾¾æ‰«æä¸­è¿›è¡Œå®æ—¶ç›¸æœºæ³¨å†Œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16349v1" onclick="toggleFavorite(this, '2511.16349v1', 'CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251116298v1-optimizing-3d-gaussian-splattering-for-mobile-gpus.html">Optimizing 3D Gaussian Splattering for Mobile GPUs</a></td>
  <td>Texture3dgsï¼šé’ˆå¯¹ç§»åŠ¨GPUä¼˜åŒ–çš„3Dé«˜æ–¯æº…å°„ç®—æ³•ï¼Œæå‡æ’åºæ•ˆç‡ä¸æ•´ä½“æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16298v1" onclick="toggleFavorite(this, '2511.16298v1', 'Optimizing 3D Gaussian Splattering for Mobile GPUs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251116535v1-investigating-optical-flow-computation-from-local-methods-to-a-multi.html">Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation</a></td>
  <td>ç ”ç©¶å…‰æµè®¡ç®—ï¼šä»å±€éƒ¨æ–¹æ³•åˆ°å¤šåˆ†è¾¨ç‡Horn-Schunckç®—æ³•ä¸åŒçº¿æ€§æ’å€¼</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16535v1" onclick="toggleFavorite(this, '2511.16535v1', 'Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251116524v1-boxingvi-a-multi-modal-benchmark-for-boxing-action-recognition-and-l.html">BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization</a></td>
  <td>BoxingVIï¼šä¸€ä¸ªç”¨äºæ‹³å‡»åŠ¨ä½œè¯†åˆ«ä¸å®šä½çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16524v1" onclick="toggleFavorite(this, '2511.16524v1', 'BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251116454v1-llava3-representing-3d-scenes-like-a-cubist-painter-to-boost-3d-scen.html">LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs</a></td>
  <td>LLaVA$^3$ï¼šå€Ÿé‰´ç«‹ä½“ç”»æ´¾ï¼Œæå‡VLMå¯¹3Dåœºæ™¯çš„ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16454v1" onclick="toggleFavorite(this, '2511.16454v1', 'LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251116428v1-cylinderdepth-cylindrical-spatial-attention-for-multi-view-consisten.html">CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation</a></td>
  <td>CylinderDepthï¼šåˆ©ç”¨æŸ±é¢ç©ºé—´æ³¨æ„åŠ›å®ç°å¤šè§†è§’ä¸€è‡´çš„è‡ªç›‘ç£ç¯è§†æ·±åº¦ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16428v1" onclick="toggleFavorite(this, '2511.16428v1', 'CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251116140v1-real-time-3d-object-detection-with-inference-aligned-learning.html">Real-Time 3D Object Detection with Inference-Aligned Learning</a></td>
  <td>æå‡ºSR3Dæ¡†æ¶ï¼Œé€šè¿‡æ¨ç†å¯¹é½å­¦ä¹ å®ç°å®¤å†…ç‚¹äº‘å®æ—¶3Dç›®æ ‡æ£€æµ‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16140v1" onclick="toggleFavorite(this, '2511.16140v1', 'Real-Time 3D Object Detection with Inference-Aligned Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251116112v1-clustered-error-correction-with-grouped-4d-gaussian-splatting.html">Clustered Error Correction with Grouped 4D Gaussian Splatting</a></td>
  <td>æå‡ºåŸºäºèšç±»è¯¯å·®æ ¡æ­£çš„åˆ†ç»„4Dé«˜æ–¯æº…å°„æ–¹æ³•ï¼Œæå‡åŠ¨æ€åœºæ™¯é‡å»ºè´¨é‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16112v1" onclick="toggleFavorite(this, '2511.16112v1', 'Clustered Error Correction with Grouped 4D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251116418v1-end-to-end-motion-capture-from-rigid-body-markers-with-geodesic-loss.html">End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss</a></td>
  <td>æå‡ºåŸºäºåˆšä½“æ ‡è®°å’Œæµ‹åœ°çº¿æŸå¤±çš„ç«¯åˆ°ç«¯äººä½“è¿åŠ¨æ•æ‰æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16418v1" onclick="toggleFavorite(this, '2511.16418v1', 'End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251116301v2-upsample-anything-a-simple-and-hard-to-beat-baseline-for-feature-ups.html">Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling</a></td>
  <td>æå‡ºUpsample Anythingï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„ç‰¹å¾ä¸Šé‡‡æ ·é€šç”¨åŸºçº¿æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16301v2" onclick="toggleFavorite(this, '2511.16301v2', 'Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251116807v1-mesh-rag-retrieval-augmentation-for-autoregressive-mesh-generation.html">Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation</a></td>
  <td>Mesh RAGï¼šç”¨äºè‡ªå›å½’ç½‘æ ¼ç”Ÿæˆçš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œæå‡è´¨é‡ä¸é€Ÿåº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16807v1" onclick="toggleFavorite(this, '2511.16807v1', 'Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251116719v1-sam-3-segment-anything-with-concepts.html">SAM 3: Segment Anything with Concepts</a></td>
  <td>SAM 3ï¼šåŸºäºæ¦‚å¿µæç¤ºçš„å›¾åƒå’Œè§†é¢‘é€šç”¨åˆ†å‰²æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16719v1" onclick="toggleFavorite(this, '2511.16719v1', 'SAM 3: Segment Anything with Concepts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251116650v1-late-decoupled-3d-hierarchical-semantic-segmentation-with-semantic-p.html">Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision</a></td>
  <td>æå‡ºåŸºäºè¯­ä¹‰åŸå‹åˆ¤åˆ«çš„è§£è€¦3Då±‚çº§è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œè§£å†³è·¨å±‚çº§å†²çªå’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16650v1" onclick="toggleFavorite(this, '2511.16650v1', 'Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251116521v1-yowo-you-only-walk-once-to-jointly-map-an-indoor-scene-and-register-.html">YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras</a></td>
  <td>æå‡ºYOWOï¼Œå•æ¬¡è¡Œèµ°å³å¯å®Œæˆå®¤å†…åœºæ™¯åœ°å›¾æ„å»ºä¸å¤©èŠ±æ¿ç›¸æœºæ³¨å†Œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16521v1" onclick="toggleFavorite(this, '2511.16521v1', 'YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251116317v1-natex-seamless-texture-generation-as-latent-color-diffusion.html">NaTex: Seamless Texture Generation as Latent Color Diffusion</a></td>
  <td>NaTexï¼šæå‡ºä¸€ç§åŸºäºæ½œåœ¨é¢œè‰²æ‰©æ•£çš„æ— ç¼çº¹ç†ç”Ÿæˆæ¡†æ¶ï¼Œç›´æ¥åœ¨3Dç©ºé—´é¢„æµ‹çº¹ç†é¢œè‰²ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16317v1" onclick="toggleFavorite(this, '2511.16317v1', 'NaTex: Seamless Texture Generation as Latent Color Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251116712v2-pairhuman-a-high-fidelity-photographic-dataset-for-customized-dual-p.html">PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation</a></td>
  <td>æå‡ºPairHumanæ•°æ®é›†ï¼Œç”¨äºé«˜è´¨é‡å®šåˆ¶åŒäººè‚–åƒç”Ÿæˆï¼Œå¹¶æå‡ºDHumanDiffåŸºçº¿æ¨¡å‹ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16712v2" onclick="toggleFavorite(this, '2511.16712v2', 'PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251115948v2-click2graph-interactive-panoptic-video-scene-graphs-from-a-single-cl.html">Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click</a></td>
  <td>æå‡ºClick2Graphï¼Œé€šè¿‡å•æ¬¡ç‚¹å‡»å®ç°äº¤äº’å¼å…¨æ™¯è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.15948v2" onclick="toggleFavorite(this, '2511.15948v2', 'Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/251116555v1-lite-any-stereo-efficient-zero-shot-stereo-matching.html">Lite Any Stereo: Efficient Zero-Shot Stereo Matching</a></td>
  <td>æå‡ºLite Any Stereoï¼Œå®ç°é«˜æ•ˆçš„é›¶æ ·æœ¬ç«‹ä½“åŒ¹é…æ·±åº¦ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16555v1" onclick="toggleFavorite(this, '2511.16555v1', 'Lite Any Stereo: Efficient Zero-Shot Stereo Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251116494v1-physics-informed-machine-learning-for-efficient-sim-to-real-data-aug.html">Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation</a></td>
  <td>æå‡ºç‰©ç†ä¿¡æ¯GANï¼Œç”¨äºå¾®å‹ç‰©ä½“ä½å§¿ä¼°è®¡çš„é«˜æ•ˆSim-to-Realæ•°æ®å¢å¼º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16494v1" onclick="toggleFavorite(this, '2511.16494v1', 'Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251116264v1-mem-mlp-real-time-3d-human-motion-generation-from-sparse-inputs.html">Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs</a></td>
  <td>Mem-MLPï¼šåŸºäºç¨€ç–è¾“å…¥çš„å®æ—¶3Däººä½“åŠ¨ä½œç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16264v1" onclick="toggleFavorite(this, '2511.16264v1', 'Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251116666v1-scenedesigner-controllable-multi-object-image-generation-with-9-dof-.html">SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</a></td>
  <td>SceneDesignerï¼šæå‡ºåŸºäºCNOCS Mapå’Œå¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå®ç°å¤šç‰©ä½“9è‡ªç”±åº¦å§¿æ€ç²¾ç¡®æ§åˆ¶çš„å›¾åƒç”Ÿæˆã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16666v1" onclick="toggleFavorite(this, '2511.16666v1', 'SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251116857v2-bop-ask-object-interaction-reasoning-for-vision-language-models.html">BOP-ASK: Object-Interaction Reasoning for Vision-Language Models</a></td>
  <td>BOP-ASKï¼šç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹çš„ç›®æ ‡äº¤äº’æ¨ç†æ•°æ®é›†ä¸åŸºå‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16857v2" onclick="toggleFavorite(this, '2511.16857v2', 'BOP-ASK: Object-Interaction Reasoning for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251116166v1-evovla-self-evolving-vision-language-action-model.html">EvoVLA: Self-Evolving Vision-Language-Action Model</a></td>
  <td>EvoVLAï¼šä¸€ç§è‡ªè¿›åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè§£å†³é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œä¸­çš„é˜¶æ®µå¹»è§‰é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16166v1" onclick="toggleFavorite(this, '2511.16166v1', 'EvoVLA: Self-Evolving Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251116020v2-physically-realistic-sequence-level-adversarial-clothing-for-robust-.html">Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</a></td>
  <td>æå‡ºåºåˆ—çº§å¯¹æŠ—æœè£…ç”Ÿæˆæ–¹æ³•ï¼Œæå‡äººä½“æ£€æµ‹è§„é¿åœ¨çœŸå®åœºæ™¯ä¸‹çš„é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16020v2" onclick="toggleFavorite(this, '2511.16020v2', 'Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251116449v2-vla-pruner-temporal-aware-dual-level-visual-token-pruning-for-effici.html">VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</a></td>
  <td>VLA-Prunerï¼šé¢å‘é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨ç†çš„æ—¶åºæ„ŸçŸ¥åŒå±‚è§†è§‰Tokenå‰ªæ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16449v2" onclick="toggleFavorite(this, '2511.16449v2', 'VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251116203v3-when-alignment-fails-multimodal-adversarial-attacks-on-vision-langua.html">When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</a></td>
  <td>VLA-Foolï¼šé’ˆå¯¹å…·èº«è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ç ”ç©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16203v3" onclick="toggleFavorite(this, '2511.16203v3', 'When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251116175v1-mantis-a-versatile-vision-language-action-model-with-disentangled-vi.html">Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</a></td>
  <td>Mantisï¼šä¸€ç§å…·æœ‰è§£è€¦è§†è§‰é¢„æµ‹çš„å¤šåŠŸèƒ½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16175v1" onclick="toggleFavorite(this, '2511.16175v1', 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/251116542v1-eogs-earth-observation-gaussian-splatting-with-internal-camera-refin.html">EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering</a></td>
  <td>EOGS++ï¼šç»“åˆå†…éƒ¨ç›¸æœºä¼˜åŒ–çš„åœ°çƒè§‚æµ‹é«˜æ–¯æº…å°„ï¼Œå®ç°ç›´æ¥å…¨è‰²æ¸²æŸ“</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16542v1" onclick="toggleFavorite(this, '2511.16542v1', 'EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251116567v2-poma-3d-the-point-map-way-to-3d-scene-understanding.html">POMA-3D: The Point Map Way to 3D Scene Understanding</a></td>
  <td>POMA-3Dï¼šæå‡ºåŸºäºç‚¹å›¾çš„è‡ªç›‘ç£3Dåœºæ™¯ç†è§£æ¨¡å‹ï¼Œæå‡å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16567v2" onclick="toggleFavorite(this, '2511.16567v2', 'POMA-3D: The Point Map Way to 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251116161v1-simba-towards-high-fidelity-and-geometrically-consistent-point-cloud.html">Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion</a></td>
  <td>Simbaï¼šåŸºäºå˜æ¢æ‰©æ•£çš„é«˜ä¿çœŸå‡ ä½•ä¸€è‡´æ€§ç‚¹äº‘è¡¥å…¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16161v1" onclick="toggleFavorite(this, '2511.16161v1', 'Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251116049v1-listar-ray-centric-world-models-for-4d-lidar-sequences-in-autonomous.html">LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving</a></td>
  <td>LiSTARï¼šé¢å‘è‡ªåŠ¨é©¾é©¶ï¼Œæå‡ºåŸºäºå°„çº¿ä¸­å¿ƒä¸–ç•Œæ¨¡å‹çš„4Dæ¿€å…‰é›·è¾¾åºåˆ—ç”Ÿæˆæ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16049v1" onclick="toggleFavorite(this, '2511.16049v1', 'LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/251116595v2-timeviper-a-hybrid-mamba-transformer-vision-language-model-for-effic.html">TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</a></td>
  <td>TimeViperï¼šä¸€ç§æ··åˆMamba-Transformerè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé«˜æ•ˆé•¿è§†é¢‘ç†è§£</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16595v2" onclick="toggleFavorite(this, '2511.16595v2', 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251116541v2-supervised-contrastive-learning-for-few-shot-ai-generated-image-dete.html">Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</a></td>
  <td>æå‡ºåŸºäºç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æ¡†æ¶ï¼Œç”¨äºå°‘æ ·æœ¬AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸æº¯æºã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16541v2" onclick="toggleFavorite(this, '2511.16541v2', 'Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251116077v1-videoseg-r1reasoning-video-object-segmentation-via-reinforcement-lea.html">VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning</a></td>
  <td>æå‡ºVideoSeg-R1ï¼Œé¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†é¢‘æ¨ç†åˆ†å‰²æ¡†æ¶ï¼Œæå‡å¤æ‚åœºæ™¯æ³›åŒ–æ€§ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16077v1" onclick="toggleFavorite(this, '2511.16077v1', 'VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/251116711v1-motion-transfer-enhanced-stylegan-for-generating-diverse-macaque-fac.html">Motion Transfer-Enhanced StyleGAN for Generating Diverse Macaque Facial Expressions</a></td>
  <td>æå‡ºåŸºäºè¿åŠ¨è¿ç§»å¢å¼ºçš„StyleGANï¼Œç”¨äºç”Ÿæˆå¤šæ ·åŒ–çš„çŒ•çŒ´é¢éƒ¨è¡¨æƒ…</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16711v1" onclick="toggleFavorite(this, '2511.16711v1', 'Motion Transfer-Enhanced StyleGAN for Generating Diverse Macaque Facial Expressions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/251116221v1-can-mllms-read-the-room-a-multimodal-benchmark-for-assessing-decepti.html">Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions</a></td>
  <td>æå‡ºMIDAåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šäººç¤¾äº¤äº’åŠ¨ä¸­è¯†åˆ«æ¬ºéª—çš„èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16221v1" onclick="toggleFavorite(this, '2511.16221v1', 'Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)