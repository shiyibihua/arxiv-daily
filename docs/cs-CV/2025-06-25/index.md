---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-25
---

# cs.CVï¼ˆ2025-06-25ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250620388v1-a-novel-large-vision-foundation-model-lvfm-based-approach-for-genera.html">A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management</a></td>
  <td>æå‡ºåŸºäºå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å† å±‚é«˜åº¦å›¾ç”Ÿæˆæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">height map</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20388v1" data-paper-url="./papers/250620388v1-a-novel-large-vision-foundation-model-lvfm-based-approach-for-genera.html" onclick="toggleFavorite(this, '2506.20388v1', 'A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250620877v1-thirdeye-cue-aware-monocular-depth-estimation-via-brain-inspired-mul.html">THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion</a></td>
  <td>æå‡ºTHIRDEYEä»¥è§£å†³å•ç›®æ·±åº¦ä¼°è®¡ä¸­çš„çº¿ç´¢åˆ©ç”¨ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20877v1" data-paper-url="./papers/250620877v1-thirdeye-cue-aware-monocular-depth-estimation-via-brain-inspired-mul.html" onclick="toggleFavorite(this, '2506.20877v1', 'THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250620601v1-video-perception-models-for-3d-scene-synthesis.html">Video Perception Models for 3D Scene Synthesis</a></td>
  <td>æå‡ºVIPSceneä»¥è§£å†³3Dåœºæ™¯åˆæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">first-person view</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20601v1" data-paper-url="./papers/250620601v1-video-perception-models-for-3d-scene-synthesis.html" onclick="toggleFavorite(this, '2506.20601v1', 'Video Perception Models for 3D Scene Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250620756v3-stereodiff-stereo-diffusion-synergy-for-video-depth-estimation.html">StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation</a></td>
  <td>æå‡ºStereoDiffä»¥è§£å†³è§†é¢‘æ·±åº¦ä¼°è®¡ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20756v3" data-paper-url="./papers/250620756v3-stereodiff-stereo-diffusion-synergy-for-video-depth-estimation.html" onclick="toggleFavorite(this, '2506.20756v3', 'StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250620342v1-feature-hallucination-for-self-supervised-action-recognition.html">Feature Hallucination for Self-supervised Action Recognition</a></td>
  <td>æå‡ºæ·±åº¦è½¬åŒ–åŠ¨ä½œè¯†åˆ«æ¡†æ¶ä»¥æå‡è§†é¢‘åŠ¨ä½œè¯†åˆ«å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20342v1" data-paper-url="./papers/250620342v1-feature-hallucination-for-self-supervised-action-recognition.html" onclick="toggleFavorite(this, '2506.20342v1', 'Feature Hallucination for Self-supervised Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250620638v1-joint-attitude-estimation-and-3d-neural-reconstruction-of-non-cooper.html">Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects</a></td>
  <td>åˆ©ç”¨NeRFå®ç°éåˆä½œç©ºé—´ç‰©ä½“çš„å§¿æ€ä¼°è®¡ä¸3Dé‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20638v1" data-paper-url="./papers/250620638v1-joint-attitude-estimation-and-3d-neural-reconstruction-of-non-cooper.html" onclick="toggleFavorite(this, '2506.20638v1', 'Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250620671v2-ipformer-visual-3d-panoptic-scene-completion-with-context-adaptive-i.html">IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals</a></td>
  <td>æå‡ºIPFormerä»¥è§£å†³è§†è§‰3Då…¨æ™¯åœºæ™¯è¡¥å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20671v2" data-paper-url="./papers/250620671v2-ipformer-visual-3d-panoptic-scene-completion-with-context-adaptive-i.html" onclick="toggleFavorite(this, '2506.20671v2', 'IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250622500v1-visual-semantic-knowledge-conflicts-in-operating-rooms-synthetic-dat.html">Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models</a></td>
  <td>æå‡ºåˆæˆæ•°æ®é›†ä»¥è§£å†³æ‰‹æœ¯å®¤ä¸­çš„è§†è§‰è¯­ä¹‰çŸ¥è¯†å†²çªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22500v1" data-paper-url="./papers/250622500v1-visual-semantic-knowledge-conflicts-in-operating-rooms-synthetic-dat.html" onclick="toggleFavorite(this, '2506.22500v1', 'Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250620795v1-how-do-foundation-models-compare-to-skeleton-based-approaches-for-ge.html">How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?</a></td>
  <td>æ¯”è¾ƒåŸºç¡€æ¨¡å‹ä¸éª¨æ¶æ–¹æ³•åœ¨æœºå™¨äººäº¤äº’æ‰‹åŠ¿è¯†åˆ«ä¸­çš„åº”ç”¨</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20795v1" data-paper-url="./papers/250620795v1-how-do-foundation-models-compare-to-skeleton-based-approaches-for-ge.html" onclick="toggleFavorite(this, '2506.20795v1', 'How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250620214v2-unicode2-cascaded-large-scale-codebooks-for-unified-multimodal-under.html">UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation</a></td>
  <td>æå‡ºUniCode$^2$ä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸­çš„è§†è§‰ç¼–ç é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20214v2" data-paper-url="./papers/250620214v2-unicode2-cascaded-large-scale-codebooks-for-unified-multimodal-under.html" onclick="toggleFavorite(this, '2506.20214v2', 'UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250622501v1-how-can-multimodal-remote-sensing-datasets-transform-classification-.html">How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?</a></td>
  <td>æå‡ºSpatialNet-ViTä»¥è§£å†³é¥æ„Ÿæ•°æ®åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22501v1" data-paper-url="./papers/250622501v1-how-can-multimodal-remote-sensing-datasets-transform-classification-.html" onclick="toggleFavorite(this, '2506.22501v1', 'How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250620174v2-towards-scalable-and-generalizable-earth-observation-data-mining-via.html">Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition</a></td>
  <td>é€šè¿‡åŸºç¡€æ¨¡å‹ç»„åˆå®ç°å¯æ‰©å±•çš„åœ°çƒè§‚æµ‹æ•°æ®æŒ–æ˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20174v2" data-paper-url="./papers/250620174v2-towards-scalable-and-generalizable-earth-observation-data-mining-via.html" onclick="toggleFavorite(this, '2506.20174v2', 'Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250620850v1-vector-contrastive-learning-for-pixel-wise-pretraining-in-medical-vi.html">Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision</a></td>
  <td>æå‡ºå‘é‡å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³åŒ»å­¦è§†è§‰ä¸­çš„åƒç´ çº§é¢„è®­ç»ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20850v1" data-paper-url="./papers/250620850v1-vector-contrastive-learning-for-pixel-wise-pretraining-in-medical-vi.html" onclick="toggleFavorite(this, '2506.20850v1', 'Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250620841v1-fixclr-negative-class-contrastive-learning-for-semi-supervised-domai.html">FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization</a></td>
  <td>æå‡ºFixCLRä»¥è§£å†³åŠç›‘ç£é¢†åŸŸæ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20841v1" data-paper-url="./papers/250620841v1-fixclr-negative-class-contrastive-learning-for-semi-supervised-domai.html" onclick="toggleFavorite(this, '2506.20841v1', 'FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250620670v1-mmsearch-r1-incentivizing-lmms-to-search.html">MMSearch-R1: Incentivizing LMMs to Search</a></td>
  <td>æå‡ºMMSearch-R1ä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹æœç´¢æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20670v1" data-paper-url="./papers/250620670v1-mmsearch-r1-incentivizing-lmms-to-search.html" onclick="toggleFavorite(this, '2506.20670v1', 'MMSearch-R1: Incentivizing LMMs to Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250620757v1-convitac-aligning-visual-tactile-fusion-with-contrastive-representat.html">ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations</a></td>
  <td>æå‡ºConViTacä»¥è§£å†³è§†è§‰è§¦è§‰èåˆç‰¹å¾å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20757v1" data-paper-url="./papers/250620757v1-convitac-aligning-visual-tactile-fusion-with-contrastive-representat.html" onclick="toggleFavorite(this, '2506.20757v1', 'ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250620590v1-wonderfree-enhancing-novel-view-quality-and-cross-view-consistency-f.html">WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration</a></td>
  <td>æå‡ºWonderFreeä»¥è§£å†³3Dåœºæ™¯æ¢ç´¢ä¸­çš„è§†è§’ä¸€è‡´æ€§å’Œå›¾åƒè´¨é‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20590v1" data-paper-url="./papers/250620590v1-wonderfree-enhancing-novel-view-quality-and-cross-view-consistency-f.html" onclick="toggleFavorite(this, '2506.20590v1', 'WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)