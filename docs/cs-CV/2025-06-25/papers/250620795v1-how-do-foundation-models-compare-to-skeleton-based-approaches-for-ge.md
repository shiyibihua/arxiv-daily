---
layout: default
title: How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?
---

# How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20795" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20795v1</a>
  <a href="https://arxiv.org/pdf/2506.20795.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20795v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20795v1', 'How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Stephanie KÃ¤s, Anton Burenko, Louis Markert, Onur Alp Culha, Dennis Mack, Timm Linder, Bastian Leibe

**åˆ†ç±»**: cs.CV, cs.HC, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒåŸºç¡€æ¨¡å‹ä¸éª¨æ¶æ–¹æ³•åœ¨æœºå™¨äººäº¤äº’æ‰‹åŠ¿è¯†åˆ«ä¸­çš„åº”ç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹åŠ¿è¯†åˆ«` `äººæœºäº¤äº’` `è§†è§‰åŸºç¡€æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `åŠ¨æ€è¯†åˆ«` `å¤šä»»åŠ¡å­¦ä¹ ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•å¤šä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç¼ºä¹çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è§†è§‰åŸºç¡€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ¥ç®€åŒ–æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿï¼Œæ¢ç´¢å…¶åœ¨åŠ¨æ€å…¨èº«æ‰‹åŠ¿è¯†åˆ«ä¸­çš„åº”ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHD-GCNåœ¨æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒV-JEPAæ˜¾ç¤ºå‡ºä½œä¸ºå¤šä»»åŠ¡æ¨¡å‹çš„æ½œåŠ›ï¼ŒGeminiåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å­˜åœ¨å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰‹åŠ¿æ˜¯äººæœºäº¤äº’ä¸­çš„é‡è¦éè¯­è¨€æ²Ÿé€šæ–¹å¼ï¼Œå°¤å…¶åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ã€‚ä¼ ç»Ÿçš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œä½¿ç”¨å›¾åƒã€è§†é¢‘æˆ–éª¨æ¶å§¿æ€ä¼°è®¡ä½œä¸ºè¾“å…¥ã€‚æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•å°†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åº”ç”¨äºåŠ¨æ€å…¨èº«æ‰‹åŠ¿è¯†åˆ«ï¼Œå¹¶ä¸HD-GCNè¿™ä¸€é¡¶å°–éª¨æ¶æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¼•å…¥äº†NUGGETæ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°äººæœºäº¤äº’ä¸­çš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHD-GCNè¡¨ç°æœ€ä½³ï¼Œè€ŒV-JEPAåœ¨ä½¿ç”¨ç®€å•çš„ä»»åŠ¡ç‰¹å®šåˆ†ç±»å¤´æ—¶ä¹Ÿå–å¾—äº†æ¥è¿‘çš„æ•ˆæœï¼Œè¡¨æ˜å…¶ä½œä¸ºå…±äº«å¤šä»»åŠ¡æ¨¡å‹çš„æ½œåŠ›ã€‚ç›¸å¯¹è€Œè¨€ï¼ŒGeminiåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä»…ä¾èµ–æ–‡æœ¬æè¿°è¿›è¡Œæ‰‹åŠ¿åŒºåˆ†æ—¶è¡¨ç°ä¸ä½³ï¼Œçªæ˜¾äº†å¯¹æ‰‹åŠ¿è¾“å…¥è¡¨ç¤ºçš„è¿›ä¸€æ­¥ç ”ç©¶éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ‰‹åŠ¿è¯†åˆ«æ–¹æ³•åœ¨åŠ¨æ€å…¨èº«æ‰‹åŠ¿è¯†åˆ«ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€‚åº”æ€§ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ¶æ„ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§é«˜ä¸”çµæ´»æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æ›¿ä»£ä¼ ç»Ÿçš„ä»»åŠ¡ç‰¹å®šæ¨¡å—ï¼Œä»è€Œç®€åŒ–æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨æ‰‹åŠ¿è¯†åˆ«ä¸­çš„è¡¨ç°ï¼Œæ¢ç´¢å…¶åœ¨åŠ¨æ€åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†NUGGETæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œä¸»è¦æ¯”è¾ƒäº†V-JEPAã€Gemini Flash 2.0å’ŒHD-GCNä¸‰ç§æ¨¡å‹ã€‚å®éªŒæµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ç­‰é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è§†è§‰åŸºç¡€æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨äºåŠ¨æ€æ‰‹åŠ¿è¯†åˆ«ï¼Œå±•ç¤ºäº†å…¶åœ¨ç³»ç»Ÿå¤æ‚æ€§é™ä½æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤šä»»åŠ¡åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼ŒV-JEPAé‡‡ç”¨äº†ç®€å•çš„ä»»åŠ¡ç‰¹å®šåˆ†ç±»å¤´ï¼Œä¼˜åŒ–äº†æ€§èƒ½ã€‚HD-GCNåˆ™ä½œä¸ºåŸºçº¿æ¨¡å‹ï¼Œè¡¨ç°å‡ºè‰²ï¼Œè€ŒGeminiåœ¨ä»…ä¾èµ–æ–‡æœ¬æè¿°è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«æ—¶å­˜åœ¨æ˜æ˜¾å±€é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHD-GCNåœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡é«˜äºå…¶ä»–æ¨¡å‹ã€‚è€ŒV-JEPAåœ¨ä½¿ç”¨ç®€å•åˆ†ç±»å¤´æ—¶ä¹Ÿå–å¾—äº†æ¥è¿‘çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºä½œä¸ºå…±äº«å¤šä»»åŠ¡æ¨¡å‹çš„æ½œåŠ›ã€‚Geminiåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°è¾ƒå·®ï¼Œå¼ºè°ƒäº†å¯¹æ‰‹åŠ¿è¾“å…¥è¡¨ç¤ºçš„è¿›ä¸€æ­¥ç ”ç©¶éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å·¥ä¸šè‡ªåŠ¨åŒ–ã€æœåŠ¡æœºå™¨äººå’Œäººæœºåä½œç­‰é¢†åŸŸã€‚é€šè¿‡æ”¹è¿›æ‰‹åŠ¿è¯†åˆ«æŠ€æœ¯ï¼Œå¯ä»¥æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„äº¤äº’èƒ½åŠ›ï¼Œå¢å¼ºäººæœºæ²Ÿé€šçš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Gestures enable non-verbal human-robot communication, especially in noisy environments like agile production. Traditional deep learning-based gesture recognition relies on task-specific architectures using images, videos, or skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs) and Vision Language Models (VLMs) with their strong generalization abilities offer potential to reduce system complexity by replacing dedicated task-specific modules. This study investigates adapting such models for dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing skeleton-based approach). We introduce NUGGET, a dataset tailored for human-robot communication in intralogistics environments, to evaluate the different gesture recognition approaches. In our experiments, HD-GCN achieves best performance, but V-JEPA comes close with a simple, task-specific classification head - thus paving a possible way towards reducing system complexity, by using it as a shared multi-task model. In contrast, Gemini struggles to differentiate gestures based solely on textual descriptions in the zero-shot setting, highlighting the need of further research on suitable input representations for gestures.

