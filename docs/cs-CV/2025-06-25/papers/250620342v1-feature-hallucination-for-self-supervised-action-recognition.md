---
layout: default
title: Feature Hallucination for Self-supervised Action Recognition
---

# Feature Hallucination for Self-supervised Action Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20342" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20342v1</a>
  <a href="https://arxiv.org/pdf/2506.20342.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20342v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20342v1', 'Feature Hallucination for Self-supervised Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lei Wang, Piotr Koniusz

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: Accepted for publication in International Journal of Computer Vision (IJCV)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ·±åº¦è½¬åŒ–åŠ¨ä½œè¯†åˆ«æ¡†æ¶ä»¥æå‡è§†é¢‘åŠ¨ä½œè¯†åˆ«å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘åŠ¨ä½œè¯†åˆ«` `æ·±åº¦å­¦ä¹ ` `å¤šæ¨¡æ€ç‰¹å¾` `å¯¹è±¡æ£€æµ‹` `æ˜¾è‘—æ€§æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†é¢‘åŠ¨ä½œè¯†åˆ«ä¸­å¾€å¾€ä¾èµ–äºåŸå§‹åƒç´ ï¼Œç¼ºä¹é«˜å±‚æ¬¡çš„è¯­ä¹‰ç†è§£å’Œå¤šæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆæ•´åˆã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦è½¬åŒ–åŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è”åˆé¢„æµ‹åŠ¨ä½œæ¦‚å¿µå’Œè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å¹»è§‰æµæ¨æ–­ç¼ºå¤±çº¿ç´¢ã€‚
3. è¯¥æ¡†æ¶åœ¨Kinetics-400ã€Kinetics-600å’ŒSomething-Something V2ç­‰å¤šä¸ªåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£è§†é¢‘ä¸­çš„äººç±»åŠ¨ä½œä¸ä»…ä¾èµ–äºåŸå§‹åƒç´ åˆ†æï¼Œè¿˜éœ€è¦é«˜å±‚æ¬¡çš„è¯­ä¹‰æ¨ç†å’Œå¤šæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆæ•´åˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦è½¬åŒ–åŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è”åˆé¢„æµ‹åŠ¨ä½œæ¦‚å¿µå’Œè¾…åŠ©ç‰¹å¾æ¥æå‡è¯†åˆ«å‡†ç¡®æ€§ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œå¹»è§‰æµæ¨æ–­ç¼ºå¤±çº¿ç´¢ï¼Œä¸°å¯Œç‰¹å¾è¡¨ç¤ºè€Œä¸å¢åŠ è®¡ç®—å¼€é”€ã€‚ä¸ºå…³æ³¨ä¸åŠ¨ä½œç›¸å…³çš„åŒºåŸŸï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸¤ç§æ–°é¢–çš„é¢†åŸŸç‰¹å®šæè¿°ç¬¦ï¼šå¯¹è±¡æ£€æµ‹ç‰¹å¾ï¼ˆODFï¼‰å’Œæ˜¾è‘—æ€§æ£€æµ‹ç‰¹å¾ï¼ˆSDFï¼‰ã€‚è¯¥æ¡†æ¶ä¸å¤šç§è¾…åŠ©æ¨¡æ€æ— ç¼é›†æˆï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•æ‰ç»†ç²’åº¦åŠ¨ä½œåŠ¨æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘åŠ¨ä½œè¯†åˆ«ä¸­å¯¹é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£å’Œå¤šæ¨¡æ€ç‰¹å¾æ•´åˆä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä»…ä¾èµ–äºåŸå§‹åƒç´ ï¼Œå¯¼è‡´è¯†åˆ«å‡†ç¡®æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ·±åº¦è½¬åŒ–åŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è”åˆé¢„æµ‹åŠ¨ä½œæ¦‚å¿µå’Œè¾…åŠ©ç‰¹å¾ï¼Œåˆ©ç”¨å¹»è§‰æµæ¨æ–­ç¼ºå¤±çº¿ç´¢ï¼Œä»è€Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨ä½œæ¦‚å¿µé¢„æµ‹å’Œè¾…åŠ©ç‰¹å¾é¢„æµ‹ã€‚é€šè¿‡å¼•å…¥å¯¹è±¡æ£€æµ‹ç‰¹å¾ï¼ˆODFï¼‰å’Œæ˜¾è‘—æ€§æ£€æµ‹ç‰¹å¾ï¼ˆSDFï¼‰ï¼Œæ¡†æ¶èƒ½å¤Ÿæ•æ‰ä¸Šä¸‹æ–‡çº¿ç´¢å’Œé‡è¦ç©ºé—´æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å¹»è§‰æµæœºåˆ¶å’Œé¢†åŸŸç‰¹å®šæè¿°ç¬¦ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨æ–­ç¼ºå¤±ä¿¡æ¯ï¼Œæå‡åŠ¨ä½œè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œæœ¬æ–‡å¼•å…¥äº†é²æ£’æŸå¤±å‡½æ•°ä»¥å‡è½»ç‰¹å¾å™ªå£°ï¼ŒåŒæ—¶ç»“åˆäº†éšæœºæ€§ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œä»¥å¤„ç†è¾…åŠ©ç‰¹å¾çš„ä¸ç¡®å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Kinetics-400ã€Kinetics-600å’ŒSomething-Something V2ç­‰å¤šä¸ªåŸºå‡†ä¸Šï¼Œè¯¥æ¡†æ¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•æ˜¾è‘—æé«˜äº†åŠ¨ä½œè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€ä½“è‚²åˆ†æå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡è§†é¢‘åŠ¨ä½œè¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºè‡ªåŠ¨åŒ–ç›‘æ§ç³»ç»Ÿæä¾›æ›´ä¸ºç²¾å‡†çš„è¡Œä¸ºè¯†åˆ«ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½å®‰é˜²å’Œäººæœºåä½œçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding human actions in videos requires more than raw pixel analysis; it relies on high-level semantic reasoning and effective integration of multimodal features. We propose a deep translational action recognition framework that enhances recognition accuracy by jointly predicting action concepts and auxiliary features from RGB video frames. At test time, hallucination streams infer missing cues, enriching feature representations without increasing computational overhead. To focus on action-relevant regions beyond raw pixels, we introduce two novel domain-specific descriptors. Object Detection Features (ODF) aggregate outputs from multiple object detectors to capture contextual cues, while Saliency Detection Features (SDF) highlight spatial and intensity patterns crucial for action recognition. Our framework seamlessly integrates these descriptors with auxiliary modalities such as optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It remains compatible with state-of-the-art architectures, including I3D, AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE V2 and InternVideo2. To handle uncertainty in auxiliary features, we incorporate aleatoric uncertainty modeling in the hallucination step and introduce a robust loss function to mitigate feature noise. Our multimodal self-supervised action recognition framework achieves state-of-the-art performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and Something-Something V2, demonstrating its effectiveness in capturing fine-grained action dynamics.

