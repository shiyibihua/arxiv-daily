---
layout: default
title: UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation
---

# UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20214" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20214v2</a>
  <a href="https://arxiv.org/pdf/2506.20214.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20214v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20214v2', 'UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanzhe Chen, Huasong Zhong, Yan Li, Zhenheng Yang

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25 (æ›´æ–°: 2025-07-08)

**å¤‡æ³¨**: 19 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniCode$^2$ä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸­çš„è§†è§‰ç¼–ç é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `è§†è§‰ç”Ÿæˆ` `ä»£ç æœ¬` `è¯­ä¹‰å¯¹é½` `æ¨¡å‹ç¨³å®šæ€§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»£ç æœ¬æ–¹æ³•é€šå¸¸ä¾èµ–äºå°è¯æ±‡é‡ï¼Œç¼ºä¹ç»†ç²’åº¦çš„è¯­ä¹‰è¡¨ç¤ºï¼Œæˆ–è€…ç®€å•æ‰©å±•å¯¼è‡´ä½ä»¤ç‰Œåˆ©ç”¨ç‡å’Œä¸ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ã€‚
2. æœ¬æ–‡æå‡ºçš„UniCode$^2$æ¡†æ¶é€šè¿‡çº§è”è®¾è®¡å®ç°äº†å¤§è§„æ¨¡ã€è¯­ä¹‰å¯¹é½çš„è§†è§‰ä»¤ç‰ŒåŒ–ï¼Œæå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œåˆ©ç”¨ç‡ã€‚
3. UniCode$^2$åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰åˆæˆä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸”ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å…±åŒæ¨è¿›å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ–¹é¢å±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œç°æœ‰çš„åŸºäºä»£ç æœ¬çš„æ–¹æ³•å­˜åœ¨å°è¯æ±‡é‡ç¼ºä¹ç»†ç²’åº¦è¯­ä¹‰æˆ–ç®€å•æ‰©å±•å¯¼è‡´ä½ä»¤ç‰Œåˆ©ç”¨ç‡å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†UniCode$^2$ï¼Œä¸€ä¸ªçº§è”ä»£ç æœ¬æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å¤§è§„æ¨¡ã€è¯­ä¹‰å¯¹é½å’Œç¨³å®šçš„è§†è§‰ä»¤ç‰ŒåŒ–ã€‚é€šè¿‡å¯¹æ•°ç™¾ä¸‡ä¸ªSigLIPåºåˆ—åµŒå…¥è¿›è¡Œèšç±»ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«50ä¸‡æ¡ç›®çš„ä»£ç æœ¬ï¼Œä¿æŒäº†è§†è§‰-è¯­è¨€å¯¹é½å¹¶æ‰©å±•äº†å®¹é‡ã€‚çº§è”è®¾è®¡ç¡®ä¿äº†ç¨³å®šæ€§ï¼šä¸€ä¸ªå†»ç»“çš„ä»£ç æœ¬é”šå®šäº†åµŒå…¥ç©ºé—´ï¼Œè€Œä¸€ä¸ªå¯è®­ç»ƒçš„ä»£ç æœ¬åˆ™ç»†åŒ–äº†ä»»åŠ¡ç‰¹å®šçš„è¯­ä¹‰ã€‚è¿™ç§è§£è€¦ä¿ƒè¿›äº†é«˜åˆ©ç”¨ç‡å’Œç¨³å¥å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè§†è§‰ä»¤ç‰Œä¸æ–‡æœ¬è¯­ä¹‰çš„å¯¹é½ä½¿å¾—ä¸é¢„è®­ç»ƒæ‰©æ•£è§£ç å™¨çš„æ— ç¼é›†æˆæˆä¸ºå¯èƒ½ï¼Œæ”¯æŒé«˜è´¨é‡çš„è§†è§‰åˆæˆï¼Œé€‚åº”æ€§æå°ã€‚UniCode$^2$åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†åœ¨ä¸ç‰ºç‰²ç¨³å®šæ€§ã€è¯­ä¹‰æˆ–æ¨¡å—åŒ–çš„æƒ…å†µä¸‹æ‰©å±•è§†è§‰ä»¤ç‰Œç©ºé—´çš„å¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰ç¼–ç æ–¹é¢å­˜åœ¨è¯æ±‡é‡å°ã€è¯­ä¹‰ä¸ç»†è‡´å’Œè®­ç»ƒä¸ç¨³å®šç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½å’Œåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniCode$^2$é€šè¿‡çº§è”ä»£ç æœ¬çš„è®¾è®¡ï¼Œç»“åˆå›ºå®šå’Œå¯è®­ç»ƒçš„ä»£ç æœ¬ï¼Œç¡®ä¿äº†è§†è§‰ä»¤ç‰Œçš„è¯­ä¹‰å¯¹é½å’Œé«˜æ•ˆåˆ©ç”¨ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªå†»ç»“çš„ä»£ç æœ¬ç”¨äºé”šå®šåµŒå…¥ç©ºé—´ï¼Œå¦ä¸€ä¸ªå¯è®­ç»ƒçš„ä»£ç æœ¬ç”¨äºç»†åŒ–ä»»åŠ¡ç‰¹å®šçš„è¯­ä¹‰ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å’Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªåŒ…å«50ä¸‡æ¡ç›®çš„å¤§è§„æ¨¡ä»£ç æœ¬ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å°è¯æ±‡é‡å’Œä½åˆ©ç”¨ç‡çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†è§†è§‰ä¸è¯­è¨€çš„å¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†èšç±»æŠ€æœ¯å¯¹SigLIPåºåˆ—åµŒå…¥è¿›è¡Œå¤„ç†ï¼Œç¡®ä¿äº†ä»£ç æœ¬çš„è¯­ä¹‰ä¸°å¯Œæ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniCode$^2$å±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨è§†è§‰åˆæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€è§†é¢‘ç†è§£å’Œå¤šæ¨¡æ€äº¤äº’ç­‰ã€‚UniCode$^2$çš„è®¾è®¡ç†å¿µèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€ä»»åŠ¡æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œæå‡ç”Ÿæˆæ¨¡å‹çš„è´¨é‡å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal large language models (MLLMs) have shown promise in jointly advancing multimodal understanding and generation, with visual codebooks discretizing images into tokens for autoregressive modeling. Existing codebook-based methods either rely on small vocabularies (~16K entries) that lack fine-grained semantics or naively scale up, resulting in low token utilization and unstable training. We propose UniCode$^2$, a cascaded codebook framework enabling large-scale, semantically aligned, and stable visual tokenization. By clustering millions of SigLIP sequence embeddings, we build a 500K-entry codebook that preserves vision-language alignment while expanding capacity. Stability is ensured via a cascaded design: a frozen codebook anchors the embedding space, and a trainable codebook refines task-specific semantics. This decoupling promotes high utilization and robust learning. Moreover, the alignment of our visual tokens with textual semantics enables seamless integration with pretrained diffusion decoders, supporting high-quality visual synthesis with minimal adaptation. UniCode^2 delivers strong performance across diverse benchmarks, demonstrating the viability of scaling visual token spaces without sacrificing stability, semantics, or modularity.

