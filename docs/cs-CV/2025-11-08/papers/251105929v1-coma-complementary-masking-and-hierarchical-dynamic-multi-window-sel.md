---
layout: default
title: CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework
---

# CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.05929" target="_blank" class="toolbar-btn">arXiv: 2511.05929v1</a>
    <a href="https://arxiv.org/pdf/2511.05929.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05929v1" 
            onclick="toggleFavorite(this, '2511.05929v1', 'CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiaxuan Li, Qing Xu, Xiangjian He, Ziyu Liu, Chang Xing, Zhen Chen, Daokun Zhang, Rong Qu, Chang Wen Chen

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-08

**Â§áÊ≥®**: 9 pages, 5 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**CoMAÔºö‰∫íË°•Êé©Á†Å‰∏éÂàÜÂ±ÇÂä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÔºåÊèêÂçáMAEÈ¢ÑËÆ≠ÁªÉÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ëá™ÁõëÁù£Â≠¶‰π†` `Êé©Á†ÅËá™ÁºñÁ†ÅÂô®` `ËßÜËßâTransformer` `‰∫íË°•Êé©Á†Å` `Âä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. MAEÂèäÂÖ∂Âèò‰Ωì‰æùËµñÈöèÊú∫Êé©Á†ÅÔºåÈúÄÊõ¥Â§öÈ¢ÑËÆ≠ÁªÉËΩÆÊ¨°‰ª•‰øùËØÅÈÄÇÂ∫îÊÄßÔºå‰∏îViTÂú®MAE‰∏≠ÂèÇÊï∞Âà©Áî®Áéá‰Ωé„ÄÇ
2. CoMAÈááÁî®‰∫íË°•Êé©Á†ÅÁ≠ñÁï•ÔºåÁ°Æ‰øùÂÉèÁ¥†ÂùáÂåÄÈááÊ†∑ÔºåÊèêÂçáÁâπÂæÅÂ≠¶‰π†ÊïàÁéáÂíåÊ®°ÂûãÈÄÇÂ∫îÊÄß„ÄÇ
3. DyViTÂºïÂÖ•Âä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÔºåÂáèÂ∞ëÂèÇÊï∞ÂíåËÆ°ÁÆóÈáèÔºåÂêåÊó∂ÊèêÂçáÁªÜÁ≤íÂ∫¶ÁâπÂæÅÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∫íË°•Êé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàCoMAÔºâÔºåÊó®Âú®ÊèêÂçáÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàMAEÔºâÁöÑÈ¢ÑËÆ≠ÁªÉÊïàÁéáÂíå‰∏ãÊ∏∏‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇCoMAÈááÁî®‰∫íË°•Êé©Á†ÅÁ≠ñÁï•ÔºåÁ°Æ‰øùÊâÄÊúâÂÉèÁ¥†ÁöÑÂùáÂåÄÈááÊ†∑Ôºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†ÊâÄÊúâÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•‰∫ÜDyViTÔºå‰∏ÄÁßçÂàÜÂ±ÇËßÜËßâTransformerÔºåÂÆÉÈááÁî®Âä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÔºàDM-MSAÔºâÔºåÊòæËëóÂáèÂ∞ëÂèÇÊï∞ÂíåFLOPsÔºåÂêåÊó∂ÊîπËøõ‰∫ÜÁªÜÁ≤íÂ∫¶ÁâπÂæÅÂ≠¶‰π†„ÄÇÂú®ImageNet-1K‰∏ä‰ΩøÁî®CoMAËøõË°åÈ¢ÑËÆ≠ÁªÉÂêéÔºåDyViT‰ªÖ‰ΩøÁî®MAEÈ¢ÑËÆ≠ÁªÉepochÁöÑ12%Âç≥ÂèØËææÂà∞‰∏éÂÖ∂Áõ∏ÂΩìÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÊõ¥ÊúâÊïàÁöÑÂ≠¶‰π†„ÄÇÊØè‰∏™epochÁöÑÈ¢ÑËÆ≠ÁªÉÊó∂Èó¥‰πüÂáèÂ∞ë‰∫Ü10%ÔºåËøõ‰∏ÄÊ≠•Á™ÅÂá∫‰∫ÜÂÖ∂ÂçìË∂äÁöÑÈ¢ÑËÆ≠ÁªÉÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑMAEÊñπÊ≥ï‰æùËµñ‰∫éÈöèÊú∫Êé©Á†ÅÁ≠ñÁï•ÔºåÂØºËá¥ËÆ≠ÁªÉÊïàÁéáËæÉ‰ΩéÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑÈ¢ÑËÆ≠ÁªÉepochÊâçËÉΩËé∑ÂæóËâØÂ•ΩÁöÑ‰∏ãÊ∏∏‰ªªÂä°ÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊ†áÂáÜÁöÑViTÁªìÊûÑÂú®MAE‰∏≠Â∫îÁî®Êó∂ÔºåÁî±‰∫éÂêÑÂ±ÇÁ©∫Èó¥ÂàÜËæ®ÁéáÂõ∫ÂÆöÔºåÂ≠òÂú®ÂèÇÊï∞Âà©Áî®Áéá‰∏çÈ´òÁöÑÈóÆÈ¢ò„ÄÇÂõ†Ê≠§ÔºåËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥MAEÈ¢ÑËÆ≠ÁªÉÊïàÁéá‰ΩéÂíåÂèÇÊï∞Âà©Áî®Áéá‰∏çÈ´òÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰∫íË°•Êé©Á†ÅÁ≠ñÁï•Êù•‰øùËØÅÊâÄÊúâÂÉèÁ¥†ÁöÑÂùáÂåÄÈááÊ†∑Ôºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†ÂõæÂÉèÁöÑÂêÑÁßçÁâπÂæÅÔºåÊèêÈ´òÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄß„ÄÇÂêåÊó∂ÔºåÈÄöËøáÂºïÂÖ•Âä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂáèÂ∞ëÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊèêÂçáÁªÜÁ≤íÂ∫¶ÁâπÂæÅÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoMAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºö‰∫íË°•Êé©Á†ÅÁ≠ñÁï•ÂíåDyViT„ÄÇ‰∫íË°•Êé©Á†ÅÁ≠ñÁï•Áî®‰∫éÁîüÊàêÊé©Á†ÅÔºåÁ°Æ‰øùÊØè‰∏™ÂÉèÁ¥†ÈÉΩÊúâÁõ∏ÂêåÁöÑÊ¶ÇÁéáË¢´Êé©Áõñ„ÄÇDyViTÊòØ‰∏Ä‰∏™ÂàÜÂ±ÇËßÜËßâTransformerÔºåÂÆÉ‰ΩøÁî®Âä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÊèêÂèñÂõæÂÉèÁâπÂæÅ„ÄÇÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåÊ®°ÂûãÈÄöËøáÈáçÂª∫Ë¢´Êé©ÁõñÁöÑÂõæÂÉèÂå∫ÂüüËøõË°åÂ≠¶‰π†„ÄÇÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®È¢ÑËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∫íË°•Êé©Á†ÅÁ≠ñÁï•ÂíåÂä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÇ‰∫íË°•Êé©Á†ÅÁ≠ñÁï•‰∏éÈöèÊú∫Êé©Á†ÅÁ≠ñÁï•‰∏çÂêåÔºåÂÆÉ‰øùËØÅ‰∫ÜÊâÄÊúâÂÉèÁ¥†ÁöÑÂùáÂåÄÈááÊ†∑Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜÊüê‰∫õÂÉèÁ¥†Ë¢´ËøáÂ∫¶ÈááÊ†∑ËÄåÂè¶‰∏Ä‰∫õÂÉèÁ¥†Ë¢´ÂøΩÁï•ÁöÑÈóÆÈ¢ò„ÄÇÂä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂÖÅËÆ∏Ê®°ÂûãÂú®‰∏çÂêåÁöÑÁ™óÂè£Â§ßÂ∞è‰∏äËøõË°åËá™Ê≥®ÊÑèÂäõËÆ°ÁÆóÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊçïÊçâÂõæÂÉèÁöÑÂ±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö‰∫íË°•Êé©Á†ÅÁ≠ñÁï•ÈÄöËøáÂ∞ÜÂõæÂÉèÂàíÂàÜ‰∏∫Â§ö‰∏™‰∫íË°•ÁöÑÊé©Á†ÅÈõÜÂêàÊù•ÂÆûÁé∞ÔºåÊØè‰∏™ÈõÜÂêà‰∏≠ÁöÑÊé©Á†ÅË¶ÜÁõñÂõæÂÉèÁöÑ‰∏çÂêåÂå∫ÂüüÔºåÁ°Æ‰øùÊâÄÊúâÂÉèÁ¥†Âú®ÊâÄÊúâÈõÜÂêà‰∏≠ÈÉΩË¢´Ë¶ÜÁõñ‰∏ÄÊ¨°„ÄÇÂä®ÊÄÅÂ§öÁ™óÂè£Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÈÄöËøáÂú®‰∏çÂêåÁöÑÁ™óÂè£Â§ßÂ∞è‰∏äÂπ∂Ë°åËÆ°ÁÆóËá™Ê≥®ÊÑèÂäõÔºåÂπ∂Â∞ÜÁªìÊûúËøõË°åËûçÂêàÊù•ÂÆûÁé∞„ÄÇÂÖ∑‰ΩìÁöÑÁ™óÂè£Â§ßÂ∞èËÆæÁΩÆÂíåËûçÂêàÊñπÂºèÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®CoMAÈ¢ÑËÆ≠ÁªÉÁöÑDyViT‰ªÖ‰ΩøÁî®MAEÈ¢ÑËÆ≠ÁªÉepochÁöÑ12%Âç≥ÂèØËææÂà∞‰∏éÂÖ∂Áõ∏ÂΩìÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊØè‰∏™epochÁöÑÈ¢ÑËÆ≠ÁªÉÊó∂Èó¥‰πüÂáèÂ∞ë‰∫Ü10%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåCoMAËÉΩÂ§üÊòæËëóÊèêÈ´òMAEÁöÑÈ¢ÑËÆ≠ÁªÉÊïàÁéáÔºåÂπ∂ÊèêÂçáÊ®°ÂûãÁöÑ‰∏ãÊ∏∏‰ªªÂä°ÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoMAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂõæÂÉèÂàÜÁ±ª„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅËØ≠‰πâÂàÜÂâ≤Á≠âÂ§öÁßçËÆ°ÁÆóÊú∫ËßÜËßâ‰ªªÂä°„ÄÇÂÖ∂È´òÊïàÁöÑÈ¢ÑËÆ≠ÁªÉËÉΩÂäõÂèØ‰ª•Èôç‰ΩéÊ®°ÂûãËÆ≠ÁªÉÊàêÊú¨ÔºåÂä†ÈÄüÊ®°ÂûãÂºÄÂèëÂë®Êúü„ÄÇËØ•Á†îÁ©∂ÂØπ‰∫éÊé®Âä®Ëá™ÁõëÁù£Â≠¶‰π†Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÁöÑÂ∫îÁî®ÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâÔºåÂπ∂ÂèØËÉΩ‰øÉËøõÁõ∏ÂÖ≥ÊäÄÊúØÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆâÈò≤Á≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.

