---
layout: default
title: CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework
---

# CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework

**arXiv**: [2511.05929v1](https://arxiv.org/abs/2511.05929) | [PDF](https://arxiv.org/pdf/2511.05929.pdf)

**ä½œè€…**: Jiaxuan Li, Qing Xu, Xiangjian He, Ziyu Liu, Chang Xing, Zhen Chen, Daokun Zhang, Rong Qu, Chang Wen Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-08

**å¤‡æ³¨**: 9 pages, 5 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CoMAï¼šäº’è¡¥æŽ©ç ä¸Žåˆ†å±‚åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›ï¼Œæå‡MAEé¢„è®­ç»ƒæ•ˆçŽ‡ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `æŽ©ç è‡ªç¼–ç å™¨` `è§†è§‰Transformer` `äº’è¡¥æŽ©ç ` `åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MAEåŠå…¶å˜ä½“ä¾èµ–éšæœºæŽ©ç ï¼Œéœ€æ›´å¤šé¢„è®­ç»ƒè½®æ¬¡ä»¥ä¿è¯é€‚åº”æ€§ï¼Œä¸”ViTåœ¨MAEä¸­å‚æ•°åˆ©ç”¨çŽ‡ä½Žã€‚
2. CoMAé‡‡ç”¨äº’è¡¥æŽ©ç ç­–ç•¥ï¼Œç¡®ä¿åƒç´ å‡åŒ€é‡‡æ ·ï¼Œæå‡ç‰¹å¾å­¦ä¹ æ•ˆçŽ‡å’Œæ¨¡åž‹é€‚åº”æ€§ã€‚
3. DyViTå¼•å…¥åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›ï¼Œå‡å°‘å‚æ•°å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶æå‡ç»†ç²’åº¦ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†äº’è¡¥æŽ©ç è‡ªç¼–ç å™¨ï¼ˆCoMAï¼‰ï¼Œæ—¨åœ¨æå‡æŽ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒæ•ˆçŽ‡å’Œä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚CoMAé‡‡ç”¨äº’è¡¥æŽ©ç ç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰åƒç´ çš„å‡åŒ€é‡‡æ ·ï¼Œä»Žè€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ æ‰€æœ‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†DyViTï¼Œä¸€ç§åˆ†å±‚è§†è§‰Transformerï¼Œå®ƒé‡‡ç”¨åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›ï¼ˆDM-MSAï¼‰ï¼Œæ˜¾è‘—å‡å°‘å‚æ•°å’ŒFLOPsï¼ŒåŒæ—¶æ”¹è¿›äº†ç»†ç²’åº¦ç‰¹å¾å­¦ä¹ ã€‚åœ¨ImageNet-1Kä¸Šä½¿ç”¨CoMAè¿›è¡Œé¢„è®­ç»ƒåŽï¼ŒDyViTä»…ä½¿ç”¨MAEé¢„è®­ç»ƒepochçš„12%å³å¯è¾¾åˆ°ä¸Žå…¶ç›¸å½“çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œå±•ç¤ºäº†æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚æ¯ä¸ªepochçš„é¢„è®­ç»ƒæ—¶é—´ä¹Ÿå‡å°‘äº†10%ï¼Œè¿›ä¸€æ­¥çªå‡ºäº†å…¶å“è¶Šçš„é¢„è®­ç»ƒæ•ˆçŽ‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„MAEæ–¹æ³•ä¾èµ–äºŽéšæœºæŽ©ç ç­–ç•¥ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆçŽ‡è¾ƒä½Žï¼Œéœ€è¦å¤§é‡çš„é¢„è®­ç»ƒepochæ‰èƒ½èŽ·å¾—è‰¯å¥½çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ ‡å‡†çš„ViTç»“æž„åœ¨MAEä¸­åº”ç”¨æ—¶ï¼Œç”±äºŽå„å±‚ç©ºé—´åˆ†è¾¨çŽ‡å›ºå®šï¼Œå­˜åœ¨å‚æ•°åˆ©ç”¨çŽ‡ä¸é«˜çš„é—®é¢˜ã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨è§£å†³MAEé¢„è®­ç»ƒæ•ˆçŽ‡ä½Žå’Œå‚æ•°åˆ©ç”¨çŽ‡ä¸é«˜çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡äº’è¡¥æŽ©ç ç­–ç•¥æ¥ä¿è¯æ‰€æœ‰åƒç´ çš„å‡åŒ€é‡‡æ ·ï¼Œä»Žè€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å›¾åƒçš„å„ç§ç‰¹å¾ï¼Œæé«˜æ¨¡åž‹çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡å°‘æ¨¡åž‹çš„å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œæå‡ç»†ç²’åº¦ç‰¹å¾çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCoMAçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šäº’è¡¥æŽ©ç ç­–ç•¥å’ŒDyViTã€‚äº’è¡¥æŽ©ç ç­–ç•¥ç”¨äºŽç”ŸæˆæŽ©ç ï¼Œç¡®ä¿æ¯ä¸ªåƒç´ éƒ½æœ‰ç›¸åŒçš„æ¦‚çŽ‡è¢«æŽ©ç›–ã€‚DyViTæ˜¯ä¸€ä¸ªåˆ†å±‚è§†è§‰Transformerï¼Œå®ƒä½¿ç”¨åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æå–å›¾åƒç‰¹å¾ã€‚é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡åž‹é€šè¿‡é‡å»ºè¢«æŽ©ç›–çš„å›¾åƒåŒºåŸŸè¿›è¡Œå­¦ä¹ ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡åž‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†äº’è¡¥æŽ©ç ç­–ç•¥å’ŒåŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚äº’è¡¥æŽ©ç ç­–ç•¥ä¸ŽéšæœºæŽ©ç ç­–ç•¥ä¸åŒï¼Œå®ƒä¿è¯äº†æ‰€æœ‰åƒç´ çš„å‡åŒ€é‡‡æ ·ï¼Œä»Žè€Œé¿å…äº†æŸäº›åƒç´ è¢«è¿‡åº¦é‡‡æ ·è€Œå¦ä¸€äº›åƒç´ è¢«å¿½ç•¥çš„é—®é¢˜ã€‚åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡åž‹åœ¨ä¸åŒçš„çª—å£å¤§å°ä¸Šè¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œä»Žè€Œæ›´å¥½åœ°æ•æ‰å›¾åƒçš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šäº’è¡¥æŽ©ç ç­–ç•¥é€šè¿‡å°†å›¾åƒåˆ’åˆ†ä¸ºå¤šä¸ªäº’è¡¥çš„æŽ©ç é›†åˆæ¥å®žçŽ°ï¼Œæ¯ä¸ªé›†åˆä¸­çš„æŽ©ç è¦†ç›–å›¾åƒçš„ä¸åŒåŒºåŸŸï¼Œç¡®ä¿æ‰€æœ‰åƒç´ åœ¨æ‰€æœ‰é›†åˆä¸­éƒ½è¢«è¦†ç›–ä¸€æ¬¡ã€‚åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡åœ¨ä¸åŒçš„çª—å£å¤§å°ä¸Šå¹¶è¡Œè®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œå¹¶å°†ç»“æžœè¿›è¡Œèžåˆæ¥å®žçŽ°ã€‚å…·ä½“çš„çª—å£å¤§å°è®¾ç½®å’Œèžåˆæ–¹å¼éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨CoMAé¢„è®­ç»ƒçš„DyViTä»…ä½¿ç”¨MAEé¢„è®­ç»ƒepochçš„12%å³å¯è¾¾åˆ°ä¸Žå…¶ç›¸å½“çš„ä¸‹æ¸¸æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªepochçš„é¢„è®­ç»ƒæ—¶é—´ä¹Ÿå‡å°‘äº†10%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒCoMAèƒ½å¤Ÿæ˜¾è‘—æé«˜MAEçš„é¢„è®­ç»ƒæ•ˆçŽ‡ï¼Œå¹¶æå‡æ¨¡åž‹çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CoMAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å…¶é«˜æ•ˆçš„é¢„è®­ç»ƒèƒ½åŠ›å¯ä»¥é™ä½Žæ¨¡åž‹è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡åž‹å¼€å‘å‘¨æœŸã€‚è¯¥ç ”ç©¶å¯¹äºŽæŽ¨åŠ¨è‡ªç›‘ç£å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶å¯èƒ½ä¿ƒè¿›ç›¸å…³æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®‰é˜²ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.

