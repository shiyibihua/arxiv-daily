---
layout: default
title: MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model
---

# MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model

**arXiv**: [2511.06019v1](https://arxiv.org/abs/2511.06019) | [PDF](https://arxiv.org/pdf/2511.06019.pdf)

**ä½œè€…**: Priyansh Srivastava, Romit Chatterjee, Abir Sen, Aradhana Behura, Ratnakar Dash

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-08

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MiVIDï¼šåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„å¤šç­–ç•¥è‡ªç›‘ç£è§†é¢‘å¸§æ’å€¼**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†é¢‘å¸§æ’å€¼` `æ‰©æ•£æ¨¡åž‹` `è‡ªç›‘ç£å­¦ä¹ ` `3D U-Net` `æ—¶é—´æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸVFIæ–¹æ³•åœ¨é®æŒ¡ã€é¢†åŸŸåç§»å’Œæ¨¡ç³Šè¿åŠ¨ä¸‹è¡¨çŽ°ä¸ä½³ï¼Œä¸”ä¾èµ–å…‰æµæˆ–å¯†é›†ground-truthã€‚
2. MiVIDåˆ©ç”¨æ‰©æ•£æ¨¡åž‹å’Œè‡ªç›‘ç£å­¦ä¹ ï¼Œç»“åˆ3D U-Netå’ŒTransformeræ³¨æ„åŠ›ï¼Œæ— éœ€æ˜¾å¼è¿åŠ¨ä¼°è®¡ã€‚
3. MiVIDåœ¨CPUä¸Šè®­ç»ƒï¼Œä»…ç”¨50ä¸ªepochå³åœ¨UCF101-7å’ŒDAVIS-7ä¸Šå–å¾—ä¸Žæœ‰ç›‘ç£æ–¹æ³•ç›¸å½“çš„ç»“æžœã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ˜¯è§†é¢‘å¢žå¼ºçš„å…³é”®æŠ€æœ¯ï¼Œå¯å®žçŽ°æ—¶é—´ç»´åº¦ä¸Šçš„åˆ†è¾¨çŽ‡æå‡ï¼Œåº”ç”¨äºŽæ…¢åŠ¨ä½œæ¸²æŸ“ã€å¸§çŽ‡è½¬æ¢å’Œè§†é¢‘ä¿®å¤ç­‰ä»»åŠ¡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å…‰æµï¼Œè€ŒåŸºäºŽå­¦ä¹ çš„æ¨¡åž‹åˆ™éœ€è¦å¯†é›†çš„ground-truthæ•°æ®ï¼Œä½†ä¸¤è€…åœ¨å¤„ç†é®æŒ¡ã€é¢†åŸŸåç§»å’Œæ¨¡ç³Šè¿åŠ¨æ—¶éƒ½å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†MiVIDï¼Œä¸€ä¸ªè½»é‡çº§çš„ã€è‡ªç›‘ç£çš„ã€åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„è§†é¢‘æ’å€¼æ¡†æž¶ã€‚è¯¥æ¨¡åž‹é€šè¿‡ç»“åˆ3D U-Netéª¨å¹²ç½‘ç»œå’ŒTransformeré£Žæ ¼çš„æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼è¿åŠ¨ä¼°è®¡çš„éœ€æ±‚ï¼Œå¹¶åœ¨æ··åˆæŽ©ç æœºåˆ¶ä¸‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æ¨¡æ‹Ÿé®æŒ¡å’Œè¿åŠ¨ä¸ç¡®å®šæ€§ã€‚é€šè¿‡ä½¿ç”¨åŸºäºŽä½™å¼¦çš„æ¸è¿›å¼æŽ©ç å’Œè‡ªé€‚åº”æŸå¤±è°ƒåº¦ï¼Œæˆ‘ä»¬çš„ç½‘ç»œèƒ½å¤Ÿåœ¨æ²¡æœ‰ä»»ä½•é«˜å¸§çŽ‡ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ é²æ£’çš„æ—¶ç©ºè¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¡†æž¶åœ¨UCF101-7å’ŒDAVIS-7æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚MiVIDå®Œå…¨åœ¨CPUä¸Šä½¿ç”¨è¿™äº›æ•°æ®é›†å’Œ9å¸§è§†é¢‘ç‰‡æ®µè¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªä½Žèµ„æºä½†é«˜æ•ˆçš„æµç¨‹ã€‚å°½ç®¡å­˜åœ¨è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡åž‹ä»…åœ¨50ä¸ªepochæ—¶å°±å–å¾—äº†æœ€ä½³ç»“æžœï¼Œä¸Žå¤šä¸ªæœ‰ç›‘ç£çš„åŸºçº¿æ¨¡åž‹ç›¸æ¯”å…·æœ‰ç«žäº‰åŠ›ã€‚è¿™é¡¹å·¥ä½œè¯æ˜Žäº†è‡ªç›‘ç£æ‰©æ•£å…ˆéªŒåœ¨æ—¶é—´è¿žè´¯çš„å¸§åˆæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œå¹¶ä¸ºå¯è®¿é—®å’Œå¯æ³›åŒ–çš„VFIç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘å¸§æ’å€¼æ—¨åœ¨ç”Ÿæˆè§†é¢‘åºåˆ—ä¸­ç¼ºå¤±çš„ä¸­é—´å¸§ï¼Œä»Žè€Œæé«˜è§†é¢‘çš„å¸§çŽ‡ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºŽå…‰æµçš„æ–¹æ³•ï¼Œåœ¨å¤„ç†å¤æ‚è¿åŠ¨å’Œé®æŒ¡æ—¶å®¹æ˜“å‡ºé”™ã€‚è€Œä¾èµ–ç›‘ç£å­¦ä¹ çš„æ–¹æ³•éœ€è¦å¤§é‡çš„ground-truthé«˜å¸§çŽ‡è§†é¢‘æ•°æ®ï¼ŒèŽ·å–æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ç¼ºä¹é«˜è´¨é‡ç›‘ç£ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå®žçŽ°é²æ£’ä¸”é«˜æ•ˆçš„è§†é¢‘å¸§æ’å€¼æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMiVIDçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡åž‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ç»“åˆè‡ªç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œé¿å…å¯¹æ˜¾å¼è¿åŠ¨ä¼°è®¡çš„ä¾èµ–ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé®æŒ¡å’Œè¿åŠ¨ä¸ç¡®å®šæ€§ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ åˆ°é²æ£’çš„æ—¶ç©ºè¡¨ç¤ºï¼Œä»Žè€Œåœ¨æŽ¨ç†é˜¶æ®µç”Ÿæˆé«˜è´¨é‡çš„ä¸­é—´å¸§ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºŽåˆ©ç”¨æ‰©æ•£æ¨¡åž‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„è‡ªç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œæ¥å¼¥è¡¥ç›‘ç£ä¿¡æ¯çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMiVIDçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) 3D U-Netéª¨å¹²ç½‘ç»œï¼šç”¨äºŽæå–è§†é¢‘å¸§çš„æ—¶ç©ºç‰¹å¾ã€‚2) Transformeré£Žæ ¼çš„æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼šç”¨äºŽå»ºæ¨¡è§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´ä¾èµ–å…³ç³»ã€‚3) æ··åˆæŽ©ç æœºåˆ¶ï¼šç”¨äºŽåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé®æŒ¡å’Œè¿åŠ¨ä¸ç¡®å®šæ€§ã€‚4) åŸºäºŽä½™å¼¦çš„æ¸è¿›å¼æŽ©ç ï¼šé€æ­¥å¢žåŠ æŽ©ç çš„æ¯”ä¾‹ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿé€æ­¥å­¦ä¹ åˆ°æ›´å¤æ‚çš„è¿åŠ¨æ¨¡å¼ã€‚5) è‡ªé€‚åº”æŸå¤±è°ƒåº¦ï¼šæ ¹æ®è®­ç»ƒçš„è¿›åº¦ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒæŸå¤±å‡½æ•°çš„æƒé‡ï¼Œä»¥ä¼˜åŒ–æ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMiVIDæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå…¶è‡ªç›‘ç£è®­ç»ƒç­–ç•¥å’Œæ‰©æ•£æ¨¡åž‹çš„ç»“åˆã€‚ä¸Žä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒMiVIDä¸éœ€è¦ground-truthé«˜å¸§çŽ‡è§†é¢‘æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡è‡ªç›‘ç£çš„æ–¹å¼ï¼Œåˆ©ç”¨è§†é¢‘è‡ªèº«çš„ä¿¡æ¯è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼ŒMiVIDè¿˜å¼•å…¥äº†æ··åˆæŽ©ç æœºåˆ¶å’ŒåŸºäºŽä½™å¼¦çš„æ¸è¿›å¼æŽ©ç ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡åž‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒMiVIDé‡‡ç”¨äº†ä»¥ä¸‹ç­–ç•¥ï¼š1) æ··åˆæŽ©ç æœºåˆ¶ï¼šç»“åˆäº†éšæœºæŽ©ç å’Œå—æŽ©ç ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒç±»åž‹çš„é®æŒ¡å’Œè¿åŠ¨ä¸ç¡®å®šæ€§ã€‚2) åŸºäºŽä½™å¼¦çš„æ¸è¿›å¼æŽ©ç ï¼šé€æ­¥å¢žåŠ æŽ©ç çš„æ¯”ä¾‹ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿé€æ­¥å­¦ä¹ åˆ°æ›´å¤æ‚çš„è¿åŠ¨æ¨¡å¼ã€‚3) è‡ªé€‚åº”æŸå¤±è°ƒåº¦ï¼šæ ¹æ®è®­ç»ƒçš„è¿›åº¦ï¼ŒåŠ¨æ€è°ƒæ•´L1æŸå¤±å’Œæ„ŸçŸ¥æŸå¤±çš„æƒé‡ï¼Œä»¥ä¼˜åŒ–æ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚4) è½»é‡çº§çš„3D U-Netéª¨å¹²ç½‘ç»œï¼šé™ä½Žäº†æ¨¡åž‹çš„è®¡ç®—å¤æ‚åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨CPUä¸Šè¿›è¡Œè®­ç»ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

MiVIDåœ¨UCF101-7å’ŒDAVIS-7æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æžœè¡¨æ˜Žï¼Œå³ä½¿åœ¨CPUä¸Šè®­ç»ƒï¼Œä»…ç”¨50ä¸ªepochï¼ŒMiVIDä¹Ÿèƒ½å–å¾—ä¸Žå¤šä¸ªæœ‰ç›‘ç£çš„åŸºçº¿æ¨¡åž‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„ç»“æžœã€‚è¿™è¯æ˜Žäº†MiVIDçš„è‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å’Œæ‰©æ•£æ¨¡åž‹çš„æœ‰æ•ˆæ€§ã€‚å°¤å…¶æ˜¯åœ¨ä½Žèµ„æºçŽ¯å¢ƒä¸‹ï¼ŒMiVIDå±•çŽ°äº†å¼ºå¤§çš„ç«žäº‰åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

MiVIDåœ¨è§†é¢‘ç¼–è¾‘ã€è§†é¢‘ç›‘æŽ§ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºŽå°†ä½Žå¸§çŽ‡è§†é¢‘è½¬æ¢ä¸ºé«˜å¸§çŽ‡è§†é¢‘ï¼Œæé«˜è§‚çœ‹ä½“éªŒï¼›å¯ä»¥ç”¨äºŽä¿®å¤è€æ—§è§†é¢‘ï¼Œæ¢å¤è§†é¢‘çš„æ¸…æ™°åº¦ï¼›è¿˜å¯ä»¥ç”¨äºŽç”Ÿæˆæ…¢åŠ¨ä½œè§†é¢‘ï¼Œåˆ›é€ æ›´å…·è‰ºæœ¯æ„Ÿçš„è§†è§‰æ•ˆæžœã€‚æ­¤å¤–ï¼ŒMiVIDçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è§†é¢‘åœºæ™¯ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Frame Interpolation (VFI) remains a cornerstone in video enhancement, enabling temporal upscaling for tasks like slow-motion rendering, frame rate conversion, and video restoration. While classical methods rely on optical flow and learning-based models assume access to dense ground-truth, both struggle with occlusions, domain shifts, and ambiguous motion. This article introduces MiVID, a lightweight, self-supervised, diffusion-based framework for video interpolation. Our model eliminates the need for explicit motion estimation by combining a 3D U-Net backbone with transformer-style temporal attention, trained under a hybrid masking regime that simulates occlusions and motion uncertainty. The use of cosine-based progressive masking and adaptive loss scheduling allows our network to learn robust spatiotemporal representations without any high-frame-rate supervision. Our framework is evaluated on UCF101-7 and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and 9-frame video segments, making it a low-resource yet highly effective pipeline. Despite these constraints, our model achieves optimal results at just 50 epochs, competitive with several supervised baselines.This work demonstrates the power of self-supervised diffusion priors for temporally coherent frame synthesis and provides a scalable path toward accessible and generalizable VFI systems.

