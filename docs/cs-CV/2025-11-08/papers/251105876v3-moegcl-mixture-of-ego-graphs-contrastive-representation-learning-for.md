---
layout: default
title: MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering
---

# MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.05876" target="_blank" class="toolbar-btn">arXiv: 2511.05876v3</a>
    <a href="https://arxiv.org/pdf/2511.05876.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05876v3" 
            onclick="toggleFavorite(this, '2511.05876v3', 'MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jian Zhu, Xin Zou, Jun Sun, Cheng Luo, Lei Liu, Lingfang Zeng, Ning Zhang, Bian Wu, Chang Tang, Lirong Dai

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-08 (Êõ¥Êñ∞: 2025-11-29)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/HackerHyper/MoEGCL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MoEGCLÔºåÈÄöËøáÊ∑∑ÂêàËá™ Ego ÂõæÂØπÊØîÂ≠¶‰π†ÊèêÂçáÂ§öËßÜÂõæËÅöÁ±ªÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öËßÜÂõæËÅöÁ±ª` `ÂõæÁ•ûÁªèÁΩëÁªú` `ÂØπÊØîÂ≠¶‰π†` `Ëá™ Ego Âõæ` `Ê∑∑Âêà‰∏ìÂÆ∂ÁΩëÁªú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öËßÜÂõæËÅöÁ±ª‰∏≠ÈááÁî®Á≤óÁ≤íÂ∫¶ÁöÑÂõæËûçÂêàÁ≠ñÁï•ÔºåÂøΩÁï•‰∫ÜÊ†∑Êú¨Á∫ßÂà´ÁöÑÁªÜÁ≤íÂ∫¶‰ø°ÊÅØ„ÄÇ
2. MoEGCL ÊèêÂá∫Ê∑∑ÂêàËá™ Ego ÂõæËûçÂêàÔºàMoEGFÔºâÂíå Ego ÂõæÂØπÊØîÂ≠¶‰π†ÔºàEGCLÔºâÔºåÂÆûÁé∞Ê†∑Êú¨Á∫ßÂà´ÁöÑÁªÜÁ≤íÂ∫¶ÂõæËûçÂêàÂíåË°®Á§∫ÂØπÈΩê„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoEGCL Âú®Ê∑±Â∫¶Â§öËßÜÂõæËÅöÁ±ª‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü state-of-the-art ÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNsÔºâÁöÑËøõÊ≠•ÊòæËëóÊé®Âä®‰∫ÜÂ§öËßÜÂõæËÅöÁ±ªÔºàMVCÔºâÁöÑÂèëÂ±ï„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥Á≤óÁ≤íÂ∫¶ÂõæËûçÂêàÁöÑÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂΩìÂâçÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰∏∫ÊØè‰∏™ËßÜÂõæÁîüÊàê‰∏Ä‰∏™ÂçïÁã¨ÁöÑÂõæÁªìÊûÑÔºåÁÑ∂ÂêéÂú®ËßÜÂõæÁ∫ßÂà´ÊâßË°åÂõæÁªìÊûÑÁöÑÂä†ÊùÉËûçÂêàÔºåËøôÊòØ‰∏ÄÁßçÁõ∏ÂØπÁ≤óÁï•ÁöÑÁ≠ñÁï•„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑∑ÂêàËá™ Ego ÂõæÂØπÊØîË°®Á§∫Â≠¶‰π†ÔºàMoEGCLÔºâ„ÄÇÂÆÉ‰∏ªË¶ÅÁî±‰∏§‰∏™Ê®°ÂùóÁªÑÊàê„ÄÇÁâπÂà´Âú∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑÊ∑∑ÂêàËá™ Ego ÂõæËûçÂêàÔºàMoEGFÔºâÔºåÂÆÉÊûÑÂª∫ Ego ÂõæÔºåÂπ∂Âà©Áî®Ê∑∑Âêà‰∏ìÂÆ∂ÁΩëÁªúÊù•ÂÆûÁé∞Ê†∑Êú¨Á∫ßÂà´‰∏äÁöÑÁªÜÁ≤íÂ∫¶ Ego ÂõæËûçÂêàÔºåËÄå‰∏çÊòØ‰º†ÁªüÁöÑËßÜÂõæÁ∫ßÂà´ËûçÂêà„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü Ego ÂõæÂØπÊØîÂ≠¶‰π†ÔºàEGCLÔºâÊ®°ÂùóÔºå‰ª•Â∞ÜËûçÂêàÁöÑË°®Á§∫‰∏éÁâπÂÆö‰∫éËßÜÂõæÁöÑË°®Á§∫ÂØπÈΩê„ÄÇEGCL Ê®°ÂùóÂ¢ûÂº∫‰∫ÜÊù•Ëá™Âêå‰∏ÄÁ∞áÁöÑÊ†∑Êú¨ÁöÑË°®Á§∫Áõ∏‰ººÊÄßÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÊù•Ëá™Âêå‰∏ÄÊ†∑Êú¨ÁöÑË°®Á§∫Áõ∏‰ººÊÄßÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÁªÜÁ≤íÂ∫¶ÂõæË°®Á§∫„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåMoEGCL Âú®Ê∑±Â∫¶Â§öËßÜÂõæËÅöÁ±ª‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/HackerHyper/MoEGCL ÂÖ¨ÂºÄËé∑Âæó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§öËßÜÂõæËÅöÁ±ªÊñπÊ≥ïÈÄöÂ∏∏Âú®ËßÜÂõæÁ∫ßÂà´ËøõË°åÂõæËûçÂêàÔºåÂøΩÁï•‰∫ÜÊ†∑Êú¨Á∫ßÂà´ÁöÑÁªÜÁ≤íÂ∫¶‰ø°ÊÅØ„ÄÇËøôÁßçÁ≤óÁ≤íÂ∫¶ÁöÑËûçÂêàÊñπÂºèÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®‰∏çÂêåËßÜÂõæ‰πãÈó¥ÁöÑ‰∫íË°•‰ø°ÊÅØÔºåÂØºËá¥ËÅöÁ±ªÊÄßËÉΩÂèóÈôê„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÊçïÊçâ‰∏çÂêåËßÜÂõæ‰∏≠Âêå‰∏ÄÊ†∑Êú¨ÁöÑÁªÜÂæÆÂ∑ÆÂºÇÂíåÂÖ≥ËÅîÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMoEGCL ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂú®Ê†∑Êú¨Á∫ßÂà´ËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÂõæËûçÂêàÔºåÂπ∂Âà©Áî®ÂØπÊØîÂ≠¶‰π†Êù•ÂØπÈΩêËûçÂêàÂêéÁöÑË°®Á§∫ÂíåËßÜÂõæÁâπÂÆöÁöÑË°®Á§∫„ÄÇÈÄöËøáÊûÑÂª∫ Ego ÂõæÂπ∂‰ΩøÁî®Ê∑∑Âêà‰∏ìÂÆ∂ÁΩëÁªúËøõË°åËûçÂêàÔºåÂèØ‰ª•Êõ¥Á≤æÁªÜÂú∞ÊçïÊçâÊ†∑Êú¨Âú®‰∏çÂêåËßÜÂõæ‰∏≠ÁöÑÁâπÂæÅ„ÄÇÂØπÊØîÂ≠¶‰π†ÂàôÁî®‰∫éÂ¢ûÂº∫Âêå‰∏ÄÁ∞áÊ†∑Êú¨ÁöÑË°®Á§∫Áõ∏‰ººÊÄßÔºå‰ªéËÄåÊèêÈ´òËÅöÁ±ªÊïàÊûú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMoEGCL ‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê®°ÂùóÔºöÊ∑∑ÂêàËá™ Ego ÂõæËûçÂêàÔºàMoEGFÔºâÂíå Ego ÂõæÂØπÊØîÂ≠¶‰π†ÔºàEGCLÔºâ„ÄÇÈ¶ñÂÖàÔºåMoEGF Ê®°Âùó‰∏∫ÊØè‰∏™Ê†∑Êú¨ÊûÑÂª∫ Ego ÂõæÔºåÁÑ∂ÂêéÂà©Áî®Ê∑∑Âêà‰∏ìÂÆ∂ÁΩëÁªúÂú®Ê†∑Êú¨Á∫ßÂà´ËûçÂêàËøô‰∫õ Ego Âõæ„ÄÇËûçÂêàÂêéÁöÑË°®Á§∫ÈöèÂêéË¢´ËæìÂÖ•Âà∞ EGCL Ê®°ÂùóÔºåËØ•Ê®°ÂùóÈÄöËøáÂØπÊØîÂ≠¶‰π†Â∞ÜËûçÂêàÁöÑË°®Á§∫‰∏éËßÜÂõæÁâπÂÆöÁöÑË°®Á§∫ÂØπÈΩê„ÄÇÊï¥‰∏™Ê°ÜÊû∂Êó®Âú®Â≠¶‰π†Êõ¥ÂÖ∑Âà§Âà´ÊÄßÁöÑÂ§öËßÜÂõæË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òËÅöÁ±ªÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMoEGCL ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊ∑∑ÂêàËá™ Ego ÂõæËûçÂêàÔºàMoEGFÔºâÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂÆûÁé∞‰∫ÜÊ†∑Êú¨Á∫ßÂà´ÁöÑÁªÜÁ≤íÂ∫¶ÂõæËûçÂêà„ÄÇ‰∏é‰º†ÁªüÁöÑËßÜÂõæÁ∫ßÂà´ËûçÂêàÁõ∏ÊØîÔºåMoEGF ËÉΩÂ§üÊõ¥Á≤æÁªÜÂú∞ÊçïÊçâÊ†∑Êú¨Âú®‰∏çÂêåËßÜÂõæ‰∏≠ÁöÑÁâπÂæÅÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥ÂÖ∑Âà§Âà´ÊÄßÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåEGCL Ê®°ÂùóÈÄöËøáÂØπÊØîÂ≠¶‰π†Â¢ûÂº∫‰∫ÜÂêå‰∏ÄÁ∞áÊ†∑Êú¨ÁöÑË°®Á§∫Áõ∏‰ººÊÄßÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜËÅöÁ±ªÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMoEGF Ê®°Âùó‰ΩøÁî®Ê∑∑Âêà‰∏ìÂÆ∂ÁΩëÁªúÊù•ËûçÂêà Ego Âõæ„ÄÇÊØè‰∏™‰∏ìÂÆ∂ÁΩëÁªúË¥üË¥£Â≠¶‰π†ÁâπÂÆöËßÜÂõæÁöÑÁâπÂæÅÔºåËÄåÊ∑∑ÂêàÊùÉÈáçÂàôÊ†πÊçÆÊ†∑Êú¨ÁöÑÁâπÂæÅÂä®ÊÄÅË∞ÉÊï¥„ÄÇEGCL Ê®°Âùó‰ΩøÁî® InfoNCE ÊçüÂ§±ÂáΩÊï∞ËøõË°åÂØπÊØîÂ≠¶‰π†ÔºåËØ•ÊçüÂ§±ÂáΩÊï∞Êó®Âú®ÊúÄÂ§ßÂåñÂêå‰∏ÄÁ∞áÊ†∑Êú¨ÁöÑË°®Á§∫Áõ∏‰ººÊÄßÔºåÂêåÊó∂ÊúÄÂ∞èÂåñ‰∏çÂêåÁ∞áÊ†∑Êú¨ÁöÑË°®Á§∫Áõ∏‰ººÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÊï∞ÊçÆÈõÜËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MoEGCL Âú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÁé∞ÊúâÁöÑÂ§öËßÜÂõæËÅöÁ±ªÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÔºåMoEGCL ÁöÑËÅöÁ±ªÂáÜÁ°ÆÁéáÔºàACCÔºâÂíåÂΩí‰∏ÄÂåñ‰∫í‰ø°ÊÅØÔºàNMIÔºâÊåáÊ†áÊèêÂçá‰∫Ü 5% ‰ª•‰∏ä„ÄÇÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫Ü MoEGCL Âú®ÁªÜÁ≤íÂ∫¶ÂõæËûçÂêàÂíåÂØπÊØîÂ≠¶‰π†ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MoEGCL ÂèØÂ∫îÁî®‰∫éÂ§öÁßçÈúÄË¶ÅÂ§öËßÜÂõæÊï∞ÊçÆËÅöÁ±ªÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁ§æ‰∫§ÁΩëÁªúÂàÜÊûêÔºàÂü∫‰∫éÁî®Êà∑Ë°å‰∏∫„ÄÅÂÖ¥Ë∂£Á≠âÂ§öËßÜÂõæÊï∞ÊçÆËøõË°åÁî®Êà∑ËÅöÁ±ªÔºâ„ÄÅÁîüÁâ©‰ø°ÊÅØÂ≠¶ÔºàÂü∫‰∫éÂü∫Âõ†Ë°®Ëææ„ÄÅËõãÁôΩË¥®Áõ∏‰∫í‰ΩúÁî®Á≠âÂ§öËßÜÂõæÊï∞ÊçÆËøõË°åÁñæÁóÖ‰∫öÂûãÂàÜÁ±ªÔºâ„ÄÅÂõæÂÉèËÅöÁ±ªÔºàÂü∫‰∫é‰∏çÂêåÁâπÂæÅÊèêÂèñÊñπÊ≥ïÂæóÂà∞ÁöÑÂ§öËßÜÂõæÂõæÂÉèÊï∞ÊçÆËøõË°åÂõæÂÉèËÅöÁ±ªÔºâÁ≠â„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÂ§öËßÜÂõæÊï∞ÊçÆÂàÜÊûêÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.

