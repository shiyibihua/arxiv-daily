---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-31
---

# cs.CVï¼ˆ2025-10-31ï¼‰

ğŸ“Š å…± **36** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (16 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251027350v1-rzenembed-towards-comprehensive-multimodal-retrieval.html">RzenEmbed: Towards Comprehensive Multimodal Retrieval</a></td>
  <td>RzenEmbedï¼šæå‡ºç»Ÿä¸€å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œæ˜¾è‘—æå‡è§†é¢‘å’Œæ–‡æ¡£æ£€ç´¢æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27350v1" onclick="toggleFavorite(this, '2510.27350v1', 'RzenEmbed: Towards Comprehensive Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251027584v2-image-hashing-via-cross-view-code-alignment-in-the-age-of-foundation.html">Image Hashing via Cross-View Code Alignment in the Age of Foundation Models</a></td>
  <td>æå‡ºCroVCAï¼Œé€šè¿‡è·¨è§†å›¾ç¼–ç å¯¹é½å®ç°é«˜æ•ˆå›¾åƒå“ˆå¸Œæ£€ç´¢</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27584v2" onclick="toggleFavorite(this, '2510.27584v2', 'Image Hashing via Cross-View Code Alignment in the Age of Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251027335v1-understanding-the-implicit-user-intention-via-reasoning-with-large-l.html">Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing</a></td>
  <td>æå‡ºCIELRï¼Œé€šè¿‡LLMæ¨ç†å°†å¤æ‚å›¾åƒç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•åŠ¨ä½œï¼Œæ— éœ€è”åˆå¾®è°ƒã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27335v1" onclick="toggleFavorite(this, '2510.27335v1', 'Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251027195v2-can-mllms-read-the-room-a-multimodal-benchmark-for-verifying-truthfu.html">Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions</a></td>
  <td>æå‡ºMIVAåŸºå‡†ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šäººç¤¾äº¤äº’åŠ¨ä¸­è¯†åˆ«è°è¨€çš„èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27195v2" onclick="toggleFavorite(this, '2510.27195v2', 'Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251027632v1-sketch-to-layout-sketch-guided-multimodal-layout-generation.html">Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation</a></td>
  <td>æå‡ºSketch-to-Layoutæ¡†æ¶ï¼Œåˆ©ç”¨è‰å›¾å¼•å¯¼å¤šæ¨¡æ€å¸ƒå±€ç”Ÿæˆï¼Œæå‡è®¾è®¡ä½“éªŒã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27632v1" onclick="toggleFavorite(this, '2510.27632v1', 'Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251027571v1-towards-universal-video-retrieval-generalizing-video-embedding-via-s.html">Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum</a></td>
  <td>æå‡ºé€šç”¨è§†é¢‘æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡åˆæˆå¤šæ¨¡æ€é‡‘å­—å¡”è¯¾ç¨‹æ³›åŒ–è§†é¢‘åµŒå…¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27571v1" onclick="toggleFavorite(this, '2510.27571v1', 'Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251027135v1-e-mmdit-revisiting-multimodal-diffusion-transformer-design-for-fast-.html">E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources</a></td>
  <td>æå‡ºE-MMDiTï¼Œä¸€ç§è½»é‡çº§å¤šæ¨¡æ€æ‰©æ•£Transformerï¼Œç”¨äºèµ„æºå—é™ä¸‹çš„å¿«é€Ÿå›¾åƒåˆæˆã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27135v1" onclick="toggleFavorite(this, '2510.27135v1', 'E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251100171v2-compagent-an-agentic-framework-for-visual-compliance-verification.html">CompAgent: An Agentic Framework for Visual Compliance Verification</a></td>
  <td>æå‡ºCompAgentï¼Œç”¨äºè§†è§‰åˆè§„æ€§éªŒè¯çš„Agentæ¡†æ¶ï¼Œæå‡ç»†ç²’åº¦æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00171v2" onclick="toggleFavorite(this, '2511.00171v2', 'CompAgent: An Agentic Framework for Visual Compliance Verification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251027280v2-focus-efficient-keyframe-selection-for-long-video-understanding.html">FOCUS: Efficient Keyframe Selection for Long Video Understanding</a></td>
  <td>æå‡ºFOCUSï¼Œä¸€ç§é«˜æ•ˆçš„å…³é”®å¸§é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºæå‡é•¿è§†é¢‘ç†è§£ä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27280v2" onclick="toggleFavorite(this, '2510.27280v2', 'FOCUS: Efficient Keyframe Selection for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251027164v1-generating-accurate-and-detailed-captions-for-high-resolution-images.html">Generating Accurate and Detailed Captions for High-Resolution Images</a></td>
  <td>æå‡ºä¸€ç§å¤šé˜¶æ®µæµç¨‹ï¼Œèåˆè§†è§‰-è¯­è¨€æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç›®æ ‡æ£€æµ‹ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæ›´å‡†ç¡®ã€è¯¦ç»†çš„æè¿°ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27164v1" onclick="toggleFavorite(this, '2510.27164v1', 'Generating Accurate and Detailed Captions for High-Resolution Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251100141v1-floc-facility-location-based-efficient-visual-token-compression-for-.html">FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</a></td>
  <td>FLoCï¼šåŸºäºè®¾æ–½é€‰å€çš„é•¿è§†é¢‘é«˜æ•ˆè§†è§‰Tokenå‹ç¼©æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00141v1" onclick="toggleFavorite(this, '2511.00141v1', 'FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251027647v1-negocollab-a-common-representation-negotiation-approach-for-heteroge.html">NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception</a></td>
  <td>NegoCollabï¼šä¸€ç§é¢å‘å¼‚æ„åä½œæ„ŸçŸ¥çš„åå•†å¼é€šç”¨è¡¨å¾æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27647v1" onclick="toggleFavorite(this, '2510.27647v1', 'NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251027547v1-mapsam2-adapting-sam2-for-automatic-segmentation-of-historical-map-i.html">MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map Images and Time Series</a></td>
  <td>MapSAM2ï¼šé€šè¿‡è‡ªé€‚åº”SAM2å®ç°å†å²åœ°å›¾å›¾åƒå’Œæ—¶é—´åºåˆ—çš„è‡ªåŠ¨åˆ†å‰²</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27547v1" onclick="toggleFavorite(this, '2510.27547v1', 'MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map Images and Time Series')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251027432v1-mitigating-semantic-collapse-in-partially-relevant-video-retrieval.html">Mitigating Semantic Collapse in Partially Relevant Video Retrieval</a></td>
  <td>æå‡ºæ–‡æœ¬ç›¸å…³æ€§ä¿æŒå­¦ä¹ ä¸è·¨åˆ†æ”¯è§†é¢‘å¯¹é½ï¼Œç¼“è§£éƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢ä¸­çš„è¯­ä¹‰åå¡Œé—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27432v1" onclick="toggleFavorite(this, '2510.27432v1', 'Mitigating Semantic Collapse in Partially Relevant Video Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251027234v1-more-3d-visual-geometry-reconstruction-meets-mixture-of-experts.html">MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts</a></td>
  <td>æå‡ºMoREï¼šåŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„3Dè§†è§‰å‡ ä½•é‡å»ºæ¡†æ¶ï¼Œæå‡å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27234v1" onclick="toggleFavorite(this, '2510.27234v1', 'MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251027208v1-multi-modal-feature-fusion-for-spatial-morphology-analysis-of-tradit.html">Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks</a></td>
  <td>æå‡ºåŸºäºåˆ†å±‚å›¾ç¥ç»ç½‘ç»œçš„å¤šæ¨¡æ€ç‰¹å¾èåˆæ–¹æ³•ï¼Œç”¨äºä¼ ç»Ÿæ‘è½ç©ºé—´å½¢æ€åˆ†æã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27208v1" onclick="toggleFavorite(this, '2510.27208v1', 'Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251027607v2-dual-stream-diffusion-for-world-model-augmented-vision-language-acti.html">Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model</a></td>
  <td>æå‡ºåŒæµæ‰©æ•£æ¨¡å‹DUSTï¼Œå¢å¼ºä¸–ç•Œæ¨¡å‹åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27607v2" onclick="toggleFavorite(this, '2510.27607v2', 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251100248v1-object-aware-4d-human-motion-generation.html">Object-Aware 4D Human Motion Generation</a></td>
  <td>æå‡ºMSDIæ¡†æ¶ï¼Œåˆ©ç”¨è¿åŠ¨æ‰©æ•£å…ˆéªŒç”Ÿæˆé€¼çœŸä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„4Däººä½“è¿åŠ¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00248v1" onclick="toggleFavorite(this, '2511.00248v1', 'Object-Aware 4D Human Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251027237v2-fusion-of-multi-scale-heterogeneous-pathology-foundation-models-for-.html">Fusion of Multi-scale Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis</a></td>
  <td>FuseCPathï¼šèåˆå¤šå°ºåº¦å¼‚æ„ç—…ç†å­¦åŸºç¡€æ¨¡å‹ç”¨äºå…¨åˆ‡ç‰‡å›¾åƒåˆ†æ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27237v2" onclick="toggleFavorite(this, '2510.27237v2', 'Fusion of Multi-scale Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251100114v1-end-to-end-framework-integrating-generative-ai-and-deep-reinforcemen.html">End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</a></td>
  <td>æå‡ºé›†æˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå®ç°è‡ªä¸»è¶…å£°æ‰«æã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00114v1" onclick="toggleFavorite(this, '2511.00114v1', 'End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251027599v1-anchor-integrating-adversarial-training-with-hard-mined-supervised-c.html">ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning</a></td>
  <td>æå‡ºANCHORæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—è®­ç»ƒä¸éš¾ä¾‹ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œæå‡è¡¨å¾å­¦ä¹ çš„é²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27599v1" onclick="toggleFavorite(this, '2510.27599v1', 'ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251100260v1-mambanetlk-enhancing-colonoscopy-point-cloud-registration-with-mamba.html">MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</a></td>
  <td>MambaNetLKï¼šåˆ©ç”¨Mamba SSMå¢å¼ºç»“è‚ é•œç‚¹äº‘é…å‡†ç²¾åº¦ä¸é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00260v1" onclick="toggleFavorite(this, '2511.00260v1', 'MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251027508v1-context-gated-cross-modal-perception-with-visual-mamba-for-pet-ct-lu.html">Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation</a></td>
  <td>æå‡ºvMambaXï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡é—¨æ§è·¨æ¨¡æ€æ„ŸçŸ¥å’Œè§†è§‰Mambaè¿›è¡ŒPET-CTè‚ºè‚¿ç˜¤åˆ†å‰²</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27508v1" onclick="toggleFavorite(this, '2510.27508v1', 'Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251027684v1-phased-dmd-few-step-distribution-matching-distillation-via-score-mat.html">Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals</a></td>
  <td>æå‡ºPhased DMDï¼Œé€šè¿‡å­åŒºé—´å†…çš„åˆ†æ•°åŒ¹é…è’¸é¦æå‡å¤šæ­¥ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½å’Œå¤šæ ·æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27684v1" onclick="toggleFavorite(this, '2510.27684v1', 'Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251027249v1-c-lead-contrastive-learning-for-enhanced-adversarial-defense.html">C-LEAD: Contrastive Learning for Enhanced Adversarial Defense</a></td>
  <td>C-LEADï¼šåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¢å¼ºå¯¹æŠ—é˜²å¾¡èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27249v1" onclick="toggleFavorite(this, '2510.27249v1', 'C-LEAD: Contrastive Learning for Enhanced Adversarial Defense')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251027318v1-sags-self-adaptive-alias-free-gaussian-splatting-for-dynamic-surgica.html">SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction</a></td>
  <td>æå‡ºSAGSï¼Œè§£å†³åŠ¨æ€æ‰‹æœ¯å†…çª¥é•œé‡å»ºä¸­çš„ä¼ªå½±å’Œæ··å é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27318v1" onclick="toggleFavorite(this, '2510.27318v1', 'SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251027481v1-nautilus-a-large-multimodal-model-for-underwater-scene-understanding.html">NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding</a></td>
  <td>NAUTILUSï¼šç”¨äºæ°´ä¸‹åœºæ™¯ç†è§£çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæå‡æ°´ä¸‹ä»»åŠ¡é²æ£’æ€§</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27481v1" onclick="toggleFavorite(this, '2510.27481v1', 'NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251027133v1-wildfirex-slam-a-large-scale-low-altitude-rgb-d-dataset-for-wildfire.html">WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond</a></td>
  <td>WildfireX-SLAMï¼šç”¨äºé‡ç«SLAMåŠå…¶ä»–åº”ç”¨çš„å¤§è§„æ¨¡ä½ç©ºRGB-Dæ•°æ®é›†</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27133v1" onclick="toggleFavorite(this, '2510.27133v1', 'WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251100255v1-beetleflow-an-integrative-deep-learning-pipeline-for-beetle-image-pr.html">BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing</a></td>
  <td>BeetleFlowï¼šç”¨äºç”²è™«å›¾åƒå¤„ç†çš„é›†æˆæ·±åº¦å­¦ä¹ æµæ°´çº¿</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00255v1" onclick="toggleFavorite(this, '2511.00255v1', 'BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251100191v1-a-retrospect-to-multi-prompt-learning-across-vision-and-language.html">A Retrospect to Multi-prompt Learning across Vision and Language</a></td>
  <td>æå‡ºèƒ½é‡é©±åŠ¨çš„å¤šæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæå‡è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00191v1" onclick="toggleFavorite(this, '2511.00191v1', 'A Retrospect to Multi-prompt Learning across Vision and Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/251100231v2-towards-1000-fold-electron-microscopy-image-compression-for-connecto.html">Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior</a></td>
  <td>æå‡ºåŸºäºVQ-VAEä¸Transformerå…ˆéªŒçš„ç”µé•œå›¾åƒå‹ç¼©æ–¹æ³•ï¼Œå®ç°é«˜è¾¾1000å€çš„å‹ç¼©æ¯”ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00231v2" onclick="toggleFavorite(this, '2511.00231v2', 'Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251027364v1-fine-tuning-open-video-generators-for-cinematic-scene-synthesis-a-sm.html">Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V</a></td>
  <td>æå‡ºLoRAå¾®è°ƒçš„è§†é¢‘ç”Ÿæˆç®¡çº¿ï¼Œç”¨äºç”µå½±åœºæ™¯åˆæˆï¼Œè§£å†³å°æ•°æ®é›†éš¾é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27364v1" onclick="toggleFavorite(this, '2510.27364v1', 'Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/251027179v1-silhouettetell-practical-video-identification-leveraging-blurred-rec.html">SilhouetteTell: Practical Video Identification Leveraging Blurred Recordings of Video Subtitles</a></td>
  <td>SilhouetteTellï¼šåˆ©ç”¨æ¨¡ç³Šè§†é¢‘å­—å¹•è®°å½•å®ç°è§†é¢‘è¯†åˆ«æ”»å‡»</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27179v1" onclick="toggleFavorite(this, '2510.27179v1', 'SilhouetteTell: Practical Video Identification Leveraging Blurred Recordings of Video Subtitles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251027166v1-m3detection-multi-frame-multi-level-feature-fusion-for-multi-modal-3.html">M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar</a></td>
  <td>M^3Detectionï¼šå¤šå¸§å¤šå±‚ç‰¹å¾èåˆçš„ç›¸æœº-4Dé›·è¾¾å¤šæ¨¡æ€3Dç›®æ ‡æ£€æµ‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27166v1" onclick="toggleFavorite(this, '2510.27166v1', 'M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>35</td>
  <td><a href="./papers/251027219v1-specaware-a-spectral-content-aware-foundation-model-for-unifying-mul.html">SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping</a></td>
  <td>SpecAwareï¼šä¸€ç§å…‰è°±å†…å®¹æ„ŸçŸ¥çš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºç»Ÿä¸€é«˜å…‰è°±é¥æ„Ÿå¤šä¼ æ„Ÿå™¨å­¦ä¹ ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27219v1" onclick="toggleFavorite(this, '2510.27219v1', 'SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/251027148v1-higs-hierarchical-generative-scene-framework-for-multi-step-associat.html">HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition</a></td>
  <td>HiGSï¼šç”¨äºå¤šæ­¥å…³è”è¯­ä¹‰ç©ºé—´ç»„åˆçš„åˆ†å±‚ç”Ÿæˆåœºæ™¯æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27148v1" onclick="toggleFavorite(this, '2510.27148v1', 'HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)