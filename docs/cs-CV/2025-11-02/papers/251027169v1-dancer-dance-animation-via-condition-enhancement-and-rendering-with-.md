---
layout: default
title: DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model
---

# DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model

**arXiv**: [2510.27169v1](https://arxiv.org/abs/2510.27169) | [PDF](https://arxiv.org/pdf/2510.27169.pdf)

**ä½œè€…**: Yucheng Xing, Jinxing Yin, Xiaodong Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDANCERæ¡†æž¶ä»¥å¢žå¼ºæ¡ä»¶ç”Ÿæˆå•äººç‰©èˆžè¹ˆè§†é¢‘**

**å…³é”®è¯**: `èˆžè¹ˆè§†é¢‘ç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `æ¡ä»¶å¢žå¼º` `å§¿æ€æ¸²æŸ“` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•äººç‰©èˆžè¹ˆè§†é¢‘ç”Ÿæˆå› äººä½“è¿åŠ¨è‡ªç”±åº¦å¤§è€Œå…·æŒ‘æˆ˜æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤–è§‚å¢žå¼ºæ¨¡å—å’Œå§¿æ€æ¸²æŸ“æ¨¡å—ï¼Œä¼˜åŒ–å‚è€ƒå›¾åƒå’Œè¿åŠ¨æ¡ä»¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰å…ˆè¿›æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, diffusion models have shown their impressive ability in visual
> generation tasks. Besides static images, more and more research attentions have
> been drawn to the generation of realistic videos. The video generation not only
> has a higher requirement for the quality, but also brings a challenge in
> ensuring the video continuity. Among all the video generation tasks,
> human-involved contents, such as human dancing, are even more difficult to
> generate due to the high degrees of freedom associated with human motions. In
> this paper, we propose a novel framework, named as DANCER (Dance ANimation via
> Condition Enhancement and Rendering with Diffusion Model), for realistic
> single-person dance synthesis based on the most recent stable video diffusion
> model. As the video generation is generally guided by a reference image and a
> video sequence, we introduce two important modules into our framework to fully
> benefit from the two inputs. More specifically, we design an Appearance
> Enhancement Module (AEM) to focus more on the details of the reference image
> during the generation, and extend the motion guidance through a Pose Rendering
> Module (PRM) to capture pose conditions from extra domains. To further improve
> the generation capability of our model, we also collect a large amount of video
> data from Internet, and generate a novel datasetTikTok-3K to enhance the model
> training. The effectiveness of the proposed model has been evaluated through
> extensive experiments on real-world datasets, where the performance of our
> model is superior to that of the state-of-the-art methods. All the data and
> codes will be released upon acceptance.

