---
layout: default
title: Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning
---

# Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning

**arXiv**: [2510.27606v1](https://arxiv.org/abs/2510.27606) | [PDF](https://arxiv.org/pdf/2510.27606.pdf)

**ä½œè€…**: Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpatial-SSRLä»¥å¢žå¼ºå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹çš„ç©ºé—´ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ ` `ç©ºé—´ç†è§£` `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯éªŒè¯ä¿¡å·` `RGB-Då›¾åƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç©ºé—´ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–é«˜æˆæœ¬ç›‘ç£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼Œä»Žæ™®é€šå›¾åƒè‡ªåŠ¨ç”Ÿæˆäº”ç§å¯éªŒè¯çš„ç©ºé—´ç»“æž„ä»»åŠ¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡å‡†ç¡®çŽ‡æå‡è¶…è¿‡3.89%ï¼Œä¿æŒé€šç”¨è§†è§‰èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial understanding remains a weakness of Large Vision-Language Models
> (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement
> learning with verifiable rewards (RLVR) pipelines depend on costly supervision,
> specialized tools, or constrained environments that limit scale. We introduce
> Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals
> directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically
> formulates five pretext tasks that capture 2D and 3D spatial structure:
> shuffled patch reordering, flipped patch recognition, cropped patch inpainting,
> regional depth ordering, and relative 3D position prediction. These tasks
> provide ground-truth answers that are easy to verify and require no human or
> LVLM annotation. Training on our tasks substantially improves spatial reasoning
> while preserving general visual capabilities. On seven spatial understanding
> benchmarks in both image and video settings, Spatial-SSRL delivers average
> accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our
> results show that simple, intrinsic supervision enables RLVR at scale and
> provides a practical route to stronger spatial intelligence in LVLMs.

