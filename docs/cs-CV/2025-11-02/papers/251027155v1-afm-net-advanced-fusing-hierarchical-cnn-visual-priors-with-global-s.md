---
layout: default
title: AFM-Net: Advanced Fusing Hierarchical CNN Visual Priors with Global Sequence Modeling for Remote Sensing Image Scene Classification
---

# AFM-Net: Advanced Fusing Hierarchical CNN Visual Priors with Global Sequence Modeling for Remote Sensing Image Scene Classification

**arXiv**: [2510.27155v1](https://arxiv.org/abs/2510.27155) | [PDF](https://arxiv.org/pdf/2510.27155.pdf)

**ä½œè€…**: Yuanhao Tang, Xuechao Zou, Zhengpei Hu, Junliang Xing, Chengkun Zhang, Jianqiang Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAFM-NetèžåˆCNNä¸ŽMambaä»¥è§£å†³é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»ä¸­çš„å¤šå°ºåº¦ä¸Žå…¨å±€å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»` `CNNä¸ŽMambaèžåˆ` `å±‚æ¬¡ç‰¹å¾èžåˆ` `å¤šå°ºåº¦å»ºæ¨¡` `å…¨å±€åºåˆ—å»ºæ¨¡` `æ··åˆä¸“å®¶åˆ†ç±»å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»å› å¤æ‚ç©ºé—´ç»“æž„å’Œå¤šå°ºåº¦ç‰¹å¾è€Œå…·æŒ‘æˆ˜æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡CNNåˆ†æ”¯æå–å±‚æ¬¡è§†è§‰å…ˆéªŒï¼ŒMambaåˆ†æ”¯è¿›è¡Œé«˜æ•ˆå…¨å±€åºåˆ—å»ºæ¨¡
3. å®žéªŒæ•ˆæžœï¼šåœ¨AIDç­‰æ•°æ®é›†ä¸Šå‡†ç¡®çŽ‡è¶…93%ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹³è¡¡æ€§èƒ½ä¸Žæ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Remote sensing image scene classification remains a challenging task,
> primarily due to the complex spatial structures and multi-scale characteristics
> of ground objects. Existing approaches see CNNs excel at modeling local
> textures, while Transformers excel at capturing global context. However,
> efficiently integrating them remains a bottleneck due to the high computational
> cost of Transformers. To tackle this, we propose AFM-Net, a novel Advanced
> Hierarchical Fusing framework that achieves effective local and global
> co-representation through two pathways: a CNN branch for extracting
> hierarchical visual priors, and a Mamba branch for efficient global sequence
> modeling. The core innovation of AFM-Net lies in its Hierarchical Fusion
> Mechanism, which progressively aggregates multi-scale features from both
> pathways, enabling dynamic cross-level feature interaction and contextual
> reconstruction to produce highly discriminative representations. These fused
> features are then adaptively routed through a Mixture-of-Experts classifier
> module, which dispatches them to the most suitable experts for fine-grained
> scene recognition. Experiments on AID, NWPU-RESISC45, and UC Merced show that
> AFM-Net obtains 93.72, 95.54, and 96.92 percent accuracy, surpassing
> state-of-the-art methods with balanced performance and efficiency. Code is
> available at https://github.com/tangyuanhao-qhu/AFM-Net.

