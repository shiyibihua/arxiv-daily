---
layout: default
title: Generating Accurate and Detailed Captions for High-Resolution Images
---

# Generating Accurate and Detailed Captions for High-Resolution Images

**arXiv**: [2510.27164v1](https://arxiv.org/abs/2510.27164) | [PDF](https://arxiv.org/pdf/2510.27164.pdf)

**ä½œè€…**: Hankyeol Lee, Gawon Seo, Kyounggyu Lee, Dogun Kim, Kyungwoo Song, Jiyoung Jung

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šé˜¶æ®µç®¡é“ä»¥å¢žå¼ºé«˜åˆ†è¾¨çŽ‡å›¾åƒçš„å‡†ç¡®è¯¦ç»†æè¿°**

**å…³é”®è¯**: `é«˜åˆ†è¾¨çŽ‡å›¾åƒæè¿°` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯¹è±¡æ£€æµ‹` `å¹»è§‰å‡å°‘` `å¤šæ¨¡æ€é›†æˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ä½Žåˆ†è¾¨çŽ‡é¢„è®­ç»ƒä¸‹ï¼Œå¯¹é«˜åˆ†è¾¨çŽ‡å›¾åƒç”Ÿæˆæè¿°æ—¶ä¸¢å¤±ç»†èŠ‚å’Œå¯¹è±¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè§†è§‰è¯­è¨€æ¨¡åž‹ã€å¤§è¯­è¨€æ¨¡åž‹å’Œå¯¹è±¡æ£€æµ‹ï¼Œé€šè¿‡å¯¹è±¡è¯†åˆ«ä¸ŽéªŒè¯ä¸°å¯Œæè¿°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å®šåˆ¶æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºæè¿°æ›´è¯¦ç»†å¯é ï¼Œå¹¶æœ‰æ•ˆå‡å°‘å¹»è§‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) often struggle to generate accurate and
> detailed captions for high-resolution images since they are typically
> pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels).
> Downscaling high-resolution images to these dimensions may result in the loss
> of visual details and the omission of important objects. To address this
> limitation, we propose a novel pipeline that integrates vision-language models,
> large language models (LLMs), and object detection systems to enhance caption
> quality. Our proposed pipeline refines captions through a novel, multi-stage
> process. Given a high-resolution image, an initial caption is first generated
> using a VLM, and key objects in the image are then identified by an LLM. The
> LLM predicts additional objects likely to co-occur with the identified key
> objects, and these predictions are verified by object detection systems. Newly
> detected objects not mentioned in the initial caption undergo focused,
> region-specific captioning to ensure they are incorporated. This process
> enriches caption detail while reducing hallucinations by removing references to
> undetected objects. We evaluate the enhanced captions using pairwise comparison
> and quantitative scoring from large multimodal models, along with a benchmark
> for hallucination detection. Experiments on a curated dataset of
> high-resolution images demonstrate that our pipeline produces more detailed and
> reliable image captions while effectively minimizing hallucinations.

