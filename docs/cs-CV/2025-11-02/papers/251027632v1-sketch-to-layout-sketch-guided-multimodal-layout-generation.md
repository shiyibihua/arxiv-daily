---
layout: default
title: Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation
---

# Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation

**arXiv**: [2510.27632v1](https://arxiv.org/abs/2510.27632) | [PDF](https://arxiv.org/pdf/2510.27632.pdf)

**ä½œè€…**: Riccardo Brioschi, Aleksandr Alekseev, Emanuele Nevali, Berkay DÃ¶ner, Omar El Malki, Blagoj Mitrevski, Leandro Kieliger, Mark Collier, Andrii Maksai, Jesse Berent, Claudiu Musat, Efi Kokiopoulou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè‰å›¾çš„å¸ƒå±€ç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³ç”¨æˆ·çº¦æŸå¤æ‚æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `è‰å›¾å¼•å¯¼å¸ƒå±€ç”Ÿæˆ` `å¤šæ¨¡æ€Transformer` `åˆæˆæ•°æ®ç”Ÿæˆ` `å›¾å½¢å¸ƒå±€` `çº¦æŸä¼˜åŒ–` `ç”¨æˆ·ä½“éªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç”¨æˆ·çº¦æŸåœ¨å¸ƒå±€ç”Ÿæˆä¸­å¤æ‚ï¼Œé™ä½Žå¯ç”¨æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤šæ¨¡æ€Transformerï¼Œç»“åˆè‰å›¾å’Œå†…å®¹èµ„äº§ç”Ÿæˆå¸ƒå±€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œæä¾›ç›´è§‚è®¾è®¡ä½“éªŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graphic layout generation is a growing research area focusing on generating
> aesthetically pleasing layouts ranging from poster designs to documents. While
> recent research has explored ways to incorporate user constraints to guide the
> layout generation, these constraints often require complex specifications which
> reduce usability. We introduce an innovative approach exploiting user-provided
> sketches as intuitive constraints and we demonstrate empirically the
> effectiveness of this new guidance method, establishing the sketch-to-layout
> problem as a promising research direction, which is currently under-explored.
> To tackle the sketch-to-layout problem, we propose a multimodal
> transformer-based solution using the sketch and the content assets as inputs to
> produce high quality layouts. Since collecting sketch training data from human
> annotators to train our model is very costly, we introduce a novel and
> efficient method to synthetically generate training sketches at scale. We train
> and evaluate our model on three publicly available datasets: PubLayNet,
> DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art
> constraint-based methods, while offering a more intuitive design experience. In
> order to facilitate future sketch-to-layout research, we release O(200k)
> synthetically-generated sketches for the public datasets above. The datasets
> are available at https://github.com/google-deepmind/sketch_to_layout.

