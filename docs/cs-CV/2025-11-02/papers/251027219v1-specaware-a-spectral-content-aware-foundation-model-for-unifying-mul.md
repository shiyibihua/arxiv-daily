---
layout: default
title: SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping
---

# SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping

**arXiv**: [2510.27219v1](https://arxiv.org/abs/2510.27219) | [PDF](https://arxiv.org/pdf/2510.27219.pdf)

**ä½œè€…**: Renjie Ji, Xue Wang, Chao Niu, Wen Zhang, Yong Mei, Kun Tan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpecAwareä»¥ç»Ÿä¸€å¤šä¼ æ„Ÿå™¨é«˜å…‰è°±é¥æ„Ÿæ˜ å°„ä¸­çš„å­¦ä¹ é—®é¢˜**

**å…³é”®è¯**: `é«˜å…‰è°±æˆåƒ` `å¤šä¼ æ„Ÿå™¨å­¦ä¹ ` `åŸºç¡€æ¨¡åž‹` `å…ƒå±žæ€§èžåˆ` `è¶…ç½‘ç»œç¼–ç ` `é¥æ„Ÿæ˜ å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é«˜å…‰è°±æ•°æ®å¼‚è´¨æ€§é˜»ç¢é€šç”¨æ¨¡åž‹å¼€å‘ï¼ŒçŽ°æœ‰æ–¹æ³•å¿½è§†ä¼ æ„Ÿå™¨å…ƒå±žæ€§æŒ‡å¯¼
2. è®¾è®¡å…ƒå†…å®¹æ„ŸçŸ¥æ¨¡å—å’Œè¶…åµŒå…¥æ¨¡å—ï¼ŒåŠ¨æ€ç”Ÿæˆæ¡ä»¶ç¼–ç ä»¥å¤„ç†å¯å˜å…‰è°±é€šé“
3. åœ¨å…­ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œåœ¨åœŸåœ°è¦†ç›–åˆ†å‰²ã€å˜åŒ–æ£€æµ‹å’Œåœºæ™¯åˆ†ç±»ä¸­è¡¨çŽ°ä¼˜å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use and
> land-cover (LULC) mapping. However, the inherent heterogeneity of HSI data has
> long posed a major barrier to developing generalized models via joint training.
> Although HSI foundation models have shown promise for different downstream
> tasks, the existing approaches typically overlook the critical guiding role of
> sensor meta-attributes, and struggle with multi-sensor training, limiting their
> transferability. To address these challenges, we propose SpecAware, which is a
> novel hyperspectral spectral-content aware foundation model for unifying
> multi-sensor learning for HSI mapping. We also constructed the Hyper-400K
> dataset to facilitate this research, which is a new large-scale, high-quality
> benchmark dataset with over 400k image patches from diverse airborne AVIRIS
> sensors. The core of SpecAware is a two-step hypernetwork-driven encoding
> process for HSI data. Firstly, we designed a meta-content aware module to
> generate a unique conditional input for each HSI patch, tailored to each
> spectral band of every sample by fusing the sensor meta-attributes and its own
> image content. Secondly, we designed the HyperEmbedding module, where a
> sample-conditioned hypernetwork dynamically generates a pair of matrix factors
> for channel-wise encoding, consisting of adaptive spatial pattern extraction
> and latent semantic feature re-projection. Thus, SpecAware gains the ability to
> perceive and interpret spatial-spectral features across diverse scenes and
> sensors. This, in turn, allows SpecAware to adaptively process a variable
> number of spectral channels, establishing a unified framework for joint
> pre-training. Extensive experiments on six datasets demonstrate that SpecAware
> can learn superior feature representations, excelling in land-cover semantic
> segmentation classification, change detection, and scene classification.

