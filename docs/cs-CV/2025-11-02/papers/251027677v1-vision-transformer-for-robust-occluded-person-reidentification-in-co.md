---
layout: default
title: Vision Transformer for Robust Occluded Person Reidentification in Complex Surveillance Scenes
---

# Vision Transformer for Robust Occluded Person Reidentification in Complex Surveillance Scenes

**arXiv**: [2510.27677v1](https://arxiv.org/abs/2510.27677) | [PDF](https://arxiv.org/pdf/2510.27677.pdf)

**ä½œè€…**: Bo Li, Duyuan Zheng, Xinyang Liu, Qingwen Li, Hong Li, Hongyan Cui, Ge Gao, Chen Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSh-ViTæ¨¡åž‹ä»¥è§£å†³å¤æ‚ç›‘æŽ§åœºæ™¯ä¸­é®æŒ¡è¡Œäººé‡è¯†åˆ«é—®é¢˜**

**å…³é”®è¯**: `è¡Œäººé‡è¯†åˆ«` `è§†è§‰Transformer` `é®æŒ¡é²æ£’æ€§` `çŸ¥è¯†è’¸é¦` `ç›‘æŽ§åœºæ™¯` `æ•°æ®å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›‘æŽ§è¡Œäººé‡è¯†åˆ«å—é®æŒ¡ã€è§†è§’æ‰­æ›²å’Œå›¾åƒè´¨é‡å·®å½±å“ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æ¨¡å—æˆ–ä»…é€‚ç”¨äºŽæ¸…æ™°æ­£é¢å›¾åƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽViT-Baseï¼Œå¼•å…¥Shuffleæ¨¡å—ã€åœºæ™¯é€‚åº”å¢žå¼ºå’ŒDeiTçŸ¥è¯†è’¸é¦ï¼Œæå‡å¯¹é®æŒ¡å’Œæ¨¡ç³Šçš„é²æ£’æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MyTTæ•°æ®é›†ä¸ŠRank-1è¾¾83.2%ï¼ŒmAPè¾¾80.1%ï¼Œä¼˜äºŽCNNå’ŒViTåŸºçº¿ï¼Œå¹¶åœ¨Market1501ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Person re-identification (ReID) in surveillance is challenged by occlusion,
> viewpoint distortion, and poor image quality. Most existing methods rely on
> complex modules or perform well only on clear frontal images. We propose Sh-ViT
> (Shuffling Vision Transformer), a lightweight and robust model for occluded
> person ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a
> Shuffle module in the final Transformer layer to break spatial correlations and
> enhance robustness to occlusion and blur; Second, scenario-adapted augmentation
> (geometric transforms, erasing, blur, and color adjustment) to simulate
> surveillance conditions; Third, DeiT-based knowledge distillation to improve
> learning with limited labels.To support real-world evaluation, we construct the
> MyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base
> station inspections, with frequent equipment occlusion and camera variations.
> Experiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,
> outperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on
> Market1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves
> robustness to occlusion and blur without external modules, offering a practical
> solution for surveillance-based personnel monitoring.

