---
layout: default
title: Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction
---

# Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.00858" target="_blank" class="toolbar-btn">arXiv: 2511.00858v1</a>
    <a href="https://arxiv.org/pdf/2511.00858.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00858v1" 
            onclick="toggleFavorite(this, '2511.00858v1', 'Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yu Liu, Zhijie Liu, Zedong Yang, You-Fu Li, He Kong

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-02

**Â§áÊ≥®**: This manuscript has been accepted to the IEEE Transactions on Intelligent Transportation Systems as a regular paper

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÈÅÆÊå°ÊÑüÁü•Êâ©Êï£Ê®°ÂûãÔºåËß£ÂÜ≥Ë°å‰∫∫ÊÑèÂõæÈ¢ÑÊµã‰∏≠ÈÅÆÊå°Âú∫ÊôØ‰∏ãÁöÑ‰∏çÂÆåÊï¥ËßÇÊµãÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ë°å‰∫∫ÊÑèÂõæÈ¢ÑÊµã` `ÈÅÆÊå°ÊÑüÁü•` `Êâ©Êï£Ê®°Âûã` `ËøêÂä®ËΩ®ËøπÈáçÂª∫` `Transformer`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâË°å‰∫∫ÊÑèÂõæÈ¢ÑÊµãÊ®°ÂûãÂú®ÈÅÆÊå°Âú∫ÊôØ‰∏ãÔºåÁî±‰∫éËßÇÊµã‰ø°ÊÅØ‰∏çÂÆåÊï¥ÔºåÈ¢ÑÊµãÁ≤æÂ∫¶ÊòæËëó‰∏ãÈôç„ÄÇ
2. ÊèêÂá∫ÈÅÆÊå°ÊÑüÁü•Êâ©Êï£Ê®°ÂûãÔºàODMÔºâÔºåÈÄöËøáÈáçÂª∫Ë¢´ÈÅÆÊå°ÁöÑËøêÂä®Ê®°ÂºèÊù•ÊèêÂçáÈ¢ÑÊµãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂú®PIEÂíåJAADÊï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ïÂú®ÂêÑÁßçÈÅÆÊå°Âú∫ÊôØ‰∏ãÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÅÆÊå°ÊÑüÁü•Êâ©Êï£Ê®°ÂûãÔºàODMÔºâÔºåÁî®‰∫éÈ¢ÑÊµãË°å‰∫∫ÁöÑËøáÈ©¨Ë∑ØÊÑèÂõæÔºåÂ∞§ÂÖ∂ÊòØÂú®Â≠òÂú®ÈÅÆÊå°ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇËØ•Ê®°ÂûãÊó®Âú®ÈáçÂª∫Ë¢´ÈÅÆÊå°ÁöÑËøêÂä®Ê®°ÂºèÔºåÂπ∂Âà©Áî®Ëøô‰∫õÊ®°ÂºèÊù•ÊåáÂØºÊú™Êù•ÁöÑÊÑèÂõæÈ¢ÑÊµã„ÄÇÂú®ÂéªÂô™Èò∂ÊÆµÔºåÂºïÂÖ•‰∫ÜÈÅÆÊå°ÊÑüÁü•Êâ©Êï£TransformerÊû∂ÊûÑÔºå‰ª•‰º∞ËÆ°‰∏éÈÅÆÊå°Ê®°ÂºèÁõ∏ÂÖ≥ÁöÑÂô™Â£∞ÁâπÂæÅÔºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÂú®ÈÅÆÊå°ËØ≠‰πâÂú∫ÊôØ‰∏≠ÊçïËé∑‰∏ä‰∏ãÊñáÂÖ≥Á≥ªÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÈÅÆÊå°Êé©Á†ÅÂºïÂØºÁöÑÂèçÂêëËøáÁ®ãÔºå‰ª•ÊúâÊïàÂà©Áî®ËßÇÊµã‰ø°ÊÅØÔºåÂáèÂ∞ëÈ¢ÑÊµãËØØÂ∑ÆÁöÑÁ¥ØÁßØÔºåÂπ∂ÊèêÈ´òÈáçÂª∫ËøêÂä®ÁâπÂæÅÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂú®PIEÂíåJAADÁ≠âÂ∏∏Áî®Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÔºåÂØπËØ•ÊñπÊ≥ïÂú®ÂêÑÁßçÈÅÆÊå°Âú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÂπ∂‰∏éÁé∞ÊúâÊñπÊ≥ïËøõË°å‰∫ÜÊØîËæÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊØîÁé∞ÊúâÊñπÊ≥ïÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöË°å‰∫∫ÊÑèÂõæÈ¢ÑÊµãÂØπ‰∫éÁßªÂä®Êú∫Âô®‰∫∫ÂíåÊô∫ËÉΩËΩ¶ËæÜÁöÑÂØºËà™Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈÅÆÊå°Âú∫ÊôØÊó∂ÔºåÁî±‰∫éË°å‰∫∫ËøêÂä®ËΩ®Ëøπ‰ø°ÊÅØ‰∏çÂÆåÊï¥ÔºåÂØºËá¥È¢ÑÊµãÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂà©Áî®‰∏ä‰∏ãÊñá‰ø°ÊÅØÊù•Êé®Êñ≠Ë¢´ÈÅÆÊå°ÁöÑËøêÂä®Ê®°ÂºèÔºå‰ªéËÄåÂΩ±Âìç‰∫ÜÊÑèÂõæÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Êâ©Êï£Ê®°ÂûãÂº∫Â§ßÁöÑÁîüÊàêËÉΩÂäõÔºåÈáçÂª∫Ë¢´ÈÅÆÊå°ÁöÑË°å‰∫∫ËøêÂä®ËΩ®Ëøπ„ÄÇÈÄöËøáÂ≠¶‰π†ËøêÂä®Ê®°ÂºèÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºåÊ®°ÂûãÂèØ‰ª•Ê†πÊçÆÂ∑≤ËßÇÊµãÂà∞ÁöÑ‰ø°ÊÅØÊé®Êñ≠Âá∫Ë¢´ÈÅÆÊå°ÁöÑÈÉ®ÂàÜÔºå‰ªéËÄåËé∑ÂæóÊõ¥ÂÆåÊï¥ÁöÑËøêÂä®ËΩ®ËøπË°®Á§∫ÔºåËøõËÄåÊèêÈ´òÊÑèÂõæÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÈÅÆÊå°ÊÑüÁü•Êú∫Âà∂ËÉΩÂ§ü‰ΩøÊ®°ÂûãÊõ¥Âä†ÂÖ≥Ê≥®Êú™Ë¢´ÈÅÆÊå°ÁöÑ‰ø°ÊÅØÔºåÂπ∂ÂáèÂ∞ëÈÅÆÊå°Âå∫ÂüüÂØπÈ¢ÑÊµãÁöÑÂΩ±Âìç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÊâ©Êï£Èò∂ÊÆµÂíåÂèçÂêëÊâ©Êï£Èò∂ÊÆµ„ÄÇÂú®Êâ©Êï£Èò∂ÊÆµÔºåÈÄêÊ≠•ÂêëËßÇÊµãÂà∞ÁöÑË°å‰∫∫ËøêÂä®ËΩ®ËøπÊ∑ªÂä†Âô™Â£∞ÔºåÁõ¥Âà∞ËΩ®ËøπÂÆåÂÖ®Ë¢´Âô™Â£∞Ê∑πÊ≤°„ÄÇÂú®ÂèçÂêëÊâ©Êï£Èò∂ÊÆµÔºå‰ªéÁ∫ØÂô™Â£∞ÂºÄÂßãÔºåÈÄêÊ≠•ÂéªÈô§Âô™Â£∞ÔºåÂπ∂Âà©Áî®ÈÅÆÊå°ÊÑüÁü•Êâ©Êï£TransformerÊû∂ÊûÑÈáçÂª∫Ë¢´ÈÅÆÊå°ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÊúÄÁªàÔºåÂà©Áî®ÈáçÂª∫ÂêéÁöÑÂÆåÊï¥ËΩ®ËøπËøõË°åÊÑèÂõæÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÈÅÆÊå°ÊÑüÁü•Êâ©Êï£TransformerÊû∂ÊûÑÂíåÈÅÆÊå°Êé©Á†ÅÂºïÂØºÁöÑÂèçÂêëËøáÁ®ã„ÄÇÈÅÆÊå°ÊÑüÁü•Êâ©Êï£TransformerÊû∂ÊûÑËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂèñ‰∏éÈÅÆÊå°Ê®°ÂºèÁõ∏ÂÖ≥ÁöÑÂô™Â£∞ÁâπÂæÅÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈáçÂª∫Ë¢´ÈÅÆÊå°ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÈÅÆÊå°Êé©Á†ÅÂºïÂØºÁöÑÂèçÂêëËøáÁ®ãËÉΩÂ§üÊúâÊïàÂà©Áî®ËßÇÊµã‰ø°ÊÅØÔºåÂáèÂ∞ëÈ¢ÑÊµãËØØÂ∑ÆÁöÑÁ¥ØÁßØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈÅÆÊå°ÊÑüÁü•Êâ©Êï£TransformerÊû∂ÊûÑÈááÁî®TransformerÁªìÊûÑÔºåÂπ∂ÂºïÂÖ•‰∫ÜÈÅÆÊå°Êé©Á†ÅÊú∫Âà∂Ôºå‰ª•Âå∫ÂàÜËßÇÊµãÂà∞ÁöÑ‰ø°ÊÅØÂíåË¢´ÈÅÆÊå°ÁöÑ‰ø°ÊÅØ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÈáçÂª∫ÊçüÂ§±ÂíåÊÑèÂõæÈ¢ÑÊµãÊçüÂ§±ÔºåÂÖ∂‰∏≠ÈáçÂª∫ÊçüÂ§±Áî®‰∫éÁ∫¶ÊùüÈáçÂª∫ËøêÂä®ËΩ®ËøπÁöÑÂáÜÁ°ÆÊÄßÔºåÊÑèÂõæÈ¢ÑÊµãÊçüÂ§±Áî®‰∫éÁ∫¶ÊùüÊÑèÂõæÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÈÅÆÊå°Êé©Á†ÅÁöÑËÆæËÆ°Ê†πÊçÆÂÆûÈôÖÁöÑÈÅÆÊå°ÊÉÖÂÜµËøõË°åË∞ÉÊï¥Ôºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑÈÅÆÊå°Âú∫ÊôØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®PIEÂíåJAADÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂêÑÁßçÈÅÆÊå°Âú∫ÊôØ‰∏ãÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®ÈÅÆÊå°ÊØî‰æãËæÉÈ´òÁöÑÊÉÖÂÜµ‰∏ãÔºåËØ•ÊñπÊ≥ïÁöÑÊÑèÂõæÈ¢ÑÊµãÂáÜÁ°ÆÁéáÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü5%-10%„ÄÇÂÆûÈ™åËøòË°®ÊòéÔºåÈÅÆÊå°ÊÑüÁü•Êâ©Êï£TransformerÊû∂ÊûÑÂíåÈÅÆÊå°Êé©Á†ÅÂºïÂØºÁöÑÂèçÂêëËøáÁ®ãËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òÈáçÂª∫ËøêÂä®ËΩ®ËøπÁöÑÂáÜÁ°ÆÊÄßÔºå‰ªéËÄåÊèêÈ´òÊÑèÂõæÈ¢ÑÊµãÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÁßªÂä®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂáÜÁ°ÆÈ¢ÑÊµãË°å‰∫∫ÁöÑËøáÈ©¨Ë∑ØÊÑèÂõæÔºåÂèØ‰ª•ÊèêÈ´òËá™Âä®È©æÈ©∂ËΩ¶ËæÜÁöÑÂÆâÂÖ®ÊÄßÔºåÂáèÂ∞ë‰∫§ÈÄö‰∫ãÊïÖÁöÑÂèëÁîü„ÄÇÂú®ÁßªÂä®Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÂπ∂ÂÅöÂá∫Êõ¥ÂêàÁêÜÁöÑÂÜ≥Á≠ñ„ÄÇÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµãÔºå‰æãÂ¶ÇË°å‰∫∫ÈóØÁ∫¢ÁÅØÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.

