---
layout: default
title: Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions
---

# Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions

**arXiv**: [2510.27195v1](https://arxiv.org/abs/2510.27195) | [PDF](https://arxiv.org/pdf/2510.27195.pdf)

**ä½œè€…**: Caixin Kang, Yifei Huang, Liangyang Ouyang, Mingfang Zhang, Yoichi Sato

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€äº¤äº’çœŸå®žæ€§è¯„ä¼°ä»»åŠ¡ä¸Žæ•°æ®é›†ï¼Œä»¥è¯„ä¼°MLLMsåœ¨å¤šå…šç¤¾äº¤äº’åŠ¨ä¸­çš„çœŸå®žæ€§æ£€æµ‹èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ¬ºéª—æ£€æµ‹` `å¤šå…šç¤¾äº¤äº’åŠ¨` `çœŸå®žæ€§è¯„ä¼°` `è§†è§‰è¯­è¨€ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šå…šåŠ¨æ€å¯¹è¯ä¸­è‡ªåŠ¨æ£€æµ‹æ¬ºéª—çš„æŒ‘æˆ˜ï¼Œæ¶‰åŠè¯­è¨€å’Œè§†è§‰çº¿ç´¢çš„å¤æ‚äº¤äº’
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç‹¼äººæ€æ¸¸æˆæž„å»ºå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥è§†é¢‘ã€æ–‡æœ¬å’ŒçœŸå®žæ€§æ ‡ç­¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºGPT-4oç­‰æ¨¡åž‹æ€§èƒ½ä¸è¶³ï¼Œæœªèƒ½æœ‰æ•ˆç»“åˆè§†è§‰ç¤¾äº¤çº¿ç´¢

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As AI systems become increasingly integrated into human lives, endowing them
> with robust social intelligence has emerged as a critical frontier. A key
> aspect of this intelligence is discerning truth from deception, a ubiquitous
> element of human interaction that is conveyed through a complex interplay of
> verbal language and non-verbal visual cues. However, automatic deception
> detection in dynamic, multi-party conversations remains a significant
> challenge. The recent rise of powerful Multimodal Large Language Models
> (MLLMs), with their impressive abilities in visual and textual understanding,
> makes them natural candidates for this task. Consequently, their capabilities
> in this crucial domain are mostly unquantified. To address this gap, we
> introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and
> present a novel multimodal dataset derived from the social deduction game
> Werewolf. This dataset provides synchronized video, text, with verifiable
> ground-truth labels for every statement. We establish a comprehensive benchmark
> evaluating state-of-the-art MLLMs, revealing a significant performance gap:
> even powerful models like GPT-4o struggle to distinguish truth from falsehood
> reliably. Our analysis of failure modes indicates that these models fail to
> ground language in visual social cues effectively and may be overly
> conservative in their alignment, highlighting the urgent need for novel
> approaches to building more perceptive and trustworthy AI systems.

