---
layout: default
title: Improving Cross-view Object Geo-localization: A Dual Attention Approach with Cross-view Interaction and Multi-Scale Spatial Features
---

# Improving Cross-view Object Geo-localization: A Dual Attention Approach with Cross-view Interaction and Multi-Scale Spatial Features

**arXiv**: [2510.27139v1](https://arxiv.org/abs/2510.27139) | [PDF](https://arxiv.org/pdf/2510.27139.pdf)

**ä½œè€…**: Xingtao Ling Yingying Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ³¨æ„åŠ›æ–¹æ³•ä»¥æ”¹è¿›è·¨è§†è§’ç‰©ä½“åœ°ç†å®šä½ï¼Œæå‡å®šä½ç²¾åº¦ã€‚**

**å…³é”®è¯**: `è·¨è§†è§’ç‰©ä½“åœ°ç†å®šä½` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šå°ºåº¦ç©ºé—´ç‰¹å¾` `æ•°æ®é›†æž„å»º` `æ— äººæœºå®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨è·¨è§†è§’ä¿¡æ¯ä¼ é€’å’Œç©ºé—´ç‰¹å¾ä¼˜åŒ–æ–¹é¢ä¸è¶³ï¼Œå¯¼è‡´æ¨¡åž‹å…³æ³¨æ— å…³å™ªå£°ã€‚
2. å¼•å…¥è·¨è§†è§’äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œå¤šå¤´ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼Œå¢žå¼ºç‰¹å¾è¡¨ç¤ºå’ŒæŠ‘åˆ¶å™ªå£°ã€‚
3. åœ¨CVOGLå’ŒG2Dæ•°æ®é›†ä¸Šå®žéªŒï¼Œå®šä½ç²¾åº¦è¶…è¶Šå½“å‰æœ€ä¼˜æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cross-view object geo-localization has recently gained attention due to
> potential applications. Existing methods aim to capture spatial dependencies of
> query objects between different views through attention mechanisms to obtain
> spatial relationship feature maps, which are then used to predict object
> locations. Although promising, these approaches fail to effectively transfer
> information between views and do not further refine the spatial relationship
> feature maps. This results in the model erroneously focusing on irrelevant edge
> noise, thereby affecting localization performance. To address these
> limitations, we introduce a Cross-view and Cross-attention Module (CVCAM),
> which performs multiple iterations of interaction between the two views,
> enabling continuous exchange and learning of contextual information about the
> query object from both perspectives. This facilitates a deeper understanding of
> cross-view relationships while suppressing the edge noise unrelated to the
> query object. Furthermore, we integrate a Multi-head Spatial Attention Module
> (MHSAM), which employs convolutional kernels of various sizes to extract
> multi-scale spatial features from the feature maps containing implicit
> correspondences, further enhancing the feature representation of the query
> object. Additionally, given the scarcity of datasets for cross-view object
> geo-localization, we created a new dataset called G2D for the "Ground-to-Drone"
> localization task, enriching existing datasets and filling the gap in
> "Ground-to-Drone" localization task. Extensive experiments on the CVOGL and G2D
> datasets demonstrate that our proposed method achieves high localization
> accuracy, surpassing the current state-of-the-art.

