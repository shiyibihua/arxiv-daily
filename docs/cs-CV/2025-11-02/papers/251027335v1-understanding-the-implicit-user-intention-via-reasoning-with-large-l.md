---
layout: default
title: Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing
---

# Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing

**arXiv**: [2510.27335v1](https://arxiv.org/abs/2510.27335) | [PDF](https://arxiv.org/pdf/2510.27335.pdf)

**ä½œè€…**: Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCIELRæ–¹æ³•ï¼Œé€šè¿‡LLMæŽ¨ç†å°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•ç¼–è¾‘åŠ¨ä½œï¼Œé¿å…è”åˆå¾®è°ƒLLMä¸Žæ‰©æ•£æ¨¡åž‹ã€‚**

**å…³é”®è¯**: `å¤æ‚å›¾åƒç¼–è¾‘` `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `æ‰©æ•£æ¨¡åž‹` `è¯­ä¹‰è¡¨ç¤º` `è¿­ä»£æ›´æ–°` `åŸºå‡†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¤„ç†å¤æ‚å›¾åƒç¼–è¾‘æŒ‡ä»¤éœ€è”åˆå¾®è°ƒLLMä¸Žæ‰©æ•£æ¨¡åž‹ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå›¾åƒç»“æž„åŒ–è¯­ä¹‰è¡¨ç¤ºï¼Œè¿­ä»£æ›´æ–°ä»¥ç»†åŒ–åœºæ™¯ï¼Œå®žçŽ°çµæ´»ç¼–è¾‘ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SmartEditæ•°æ®é›†ä¸ŠPSNRæå‡9.955 dBï¼Œå¹¶åœ¨è‡ªå»ºCIEBenchåŸºå‡†ä¸­è¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing image editing methods can handle simple editing instructions very
> well. To deal with complex editing instructions, they often need to jointly
> fine-tune the large language models (LLMs) and diffusion models (DMs), which
> involves very high computational complexity and training cost. To address this
> issue, we propose a new method, called \textbf{C}omplex \textbf{I}mage
> \textbf{E}diting via \textbf{L}LM \textbf{R}easoning (CIELR), which converts a
> complex user instruction into a set of simple and explicit editing actions,
> eliminating the need for jointly fine-tuning the large language models and
> diffusion models. Specifically, we first construct a structured semantic
> representation of the input image using foundation models. Then, we introduce
> an iterative update mechanism that can progressively refine this
> representation, obtaining a fine-grained visual representation of the image
> scene. This allows us to perform complex and flexible image editing tasks.
> Extensive experiments on the SmartEdit Reasoning Scenario Set show that our
> method surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating
> its superior preservation of regions that should remain consistent. Due to the
> limited number of samples of public datasets of complex image editing with
> reasoning, we construct a benchmark named CIEBench, containing 86 image
> samples, together with a metric specifically for reasoning-based image editing.
> CIELR also outperforms previous methods on this benchmark. The code and dataset
> are available at
> \href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.

