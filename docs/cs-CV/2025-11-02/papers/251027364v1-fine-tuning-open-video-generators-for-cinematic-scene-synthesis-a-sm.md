---
layout: default
title: Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V
---

# Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V

**arXiv**: [2510.27364v1](https://arxiv.org/abs/2510.27364) | [PDF](https://arxiv.org/pdf/2510.27364.pdf)

**ä½œè€…**: Meftun Akarsu, Kerem Catay, Sedat Bin Vedat, Enes Kutay Yarkan, Ilke Senturk, Arda Sar, Dafne Eksioglu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽLoRAå’ŒWan2.1 I2Vçš„å°æ•°æ®ç®¡é“ï¼Œç”¨äºŽå¾®è°ƒè§†é¢‘ç”Ÿæˆå™¨ä»¥åˆæˆå½±è§†åœºæ™¯ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆå¾®è°ƒ` `ä½Žç§©é€‚åº”` `å½±è§†åœºæ™¯åˆæˆ` `å°æ•°æ®é›†è®­ç»ƒ` `æ‰©æ•£å˜æ¢å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ä»Žå°æ•°æ®é›†é«˜æ•ˆå¾®è°ƒå¼€æºè§†é¢‘æ‰©æ•£æ¨¡åž‹ï¼Œç”Ÿæˆå½±è§†çº§åœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä¸¤é˜¶æ®µæµç¨‹ï¼Œå…ˆä½¿ç”¨LoRAå­¦ä¹ è§†è§‰é£Žæ ¼ï¼Œå†æ‰©å±•ä¸ºè¿žè´¯è§†é¢‘åºåˆ—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºåœ¨ç”µå½±ä¿çœŸåº¦å’Œæ—¶é—´ç¨³å®šæ€§ä¸Šä¼˜äºŽåŸºç¡€æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present a practical pipeline for fine-tuning open-source video diffusion
> transformers to synthesize cinematic scenes for television and film production
> from small datasets. The proposed two-stage process decouples visual style
> learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA)
> modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B
> model to adapt its visual representations using a compact dataset of short
> clips from Ay Yapim's historical television film El Turco. This enables
> efficient domain transfer within hours on a single GPU. In the second stage,
> the fine-tuned model produces stylistically consistent keyframes that preserve
> costume, lighting, and color grading, which are then temporally expanded into
> coherent 720p sequences through the model's video decoder. We further apply
> lightweight parallelization and sequence partitioning strategies to accelerate
> inference without quality degradation. Quantitative and qualitative evaluations
> using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study,
> demonstrate measurable improvements in cinematic fidelity and temporal
> stability over the base model. The complete training and inference pipeline is
> released to support reproducibility and adaptation across cinematic domains.

