---
layout: default
title: ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning
---

# ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning

**arXiv**: [2510.27599v1](https://arxiv.org/abs/2510.27599) | [PDF](https://arxiv.org/pdf/2510.27599.pdf)

**ä½œè€…**: Samarup Bhattacharya, Anubhab Bhattacharya, Abir Chakraborty

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºANCHORæ¡†æž¶ï¼Œç»“åˆå¯¹æŠ—è®­ç»ƒä¸Žç¡¬æŒ–æŽ˜ç›‘ç£å¯¹æ¯”å­¦ä¹ ä»¥æå‡å›¾åƒè¡¨ç¤ºé²æ£’æ€§**

**å…³é”®è¯**: `å¯¹æŠ—è®­ç»ƒ` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `ç¡¬æŒ–æŽ˜` `è¡¨ç¤ºå­¦ä¹ ` `é²æ£’æ€§` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¥žç»ç½‘ç»œæ˜“å—å¯¹æŠ—æ”»å‡»ï¼Œå¾®å°æ‰°åŠ¨å¯¼è‡´æ¨¡åž‹é”™è¯¯é¢„æµ‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆå¯¹æŠ—è®­ç»ƒä¸Žç¡¬æŒ–æŽ˜ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä½¿å›¾åƒåŠå…¶æ‰°åŠ¨ç‰ˆæœ¬åœ¨åµŒå…¥ç©ºé—´èšç±»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-10ä¸Šï¼ŒPGD-20æ”»å‡»ä¸‹æ¸…æ´ä¸Žé²æ£’å‡†ç¡®çŽ‡ä¼˜äºŽæ ‡å‡†å¯¹æŠ—è®­ç»ƒæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Neural networks have changed the way machines interpret the world. At their
> core, they learn by following gradients, adjusting their parameters step by
> step until they identify the most discriminant patterns in the data. This
> process gives them their strength, yet it also opens the door to a hidden flaw.
> The very gradients that help a model learn can also be used to produce small,
> imperceptible tweaks that cause the model to completely alter its decision.
> Such tweaks are called adversarial attacks. These attacks exploit this
> vulnerability by adding tiny, imperceptible changes to images that, while
> leaving them identical to the human eye, cause the model to make wrong
> predictions. In this work, we propose Adversarially-trained Contrastive
> Hard-mining for Optimized Robustness (ANCHOR), a framework that leverages the
> power of supervised contrastive learning with explicit hard positive mining to
> enable the model to learn representations for images such that the embeddings
> for the images, their augmentations, and their perturbed versions cluster
> together in the embedding space along with those for other images of the same
> class while being separated from images of other classes. This alignment helps
> the model focus on stable, meaningful patterns rather than fragile gradient
> cues. On CIFAR-10, our approach achieves impressive results for both clean and
> robust accuracy under PGD-20 (epsilon = 0.031), outperforming standard
> adversarial training methods. Our results indicate that combining adversarial
> guidance with hard-mined contrastive supervision helps models learn more
> structured and robust representations, narrowing the gap between accuracy and
> robustness.

