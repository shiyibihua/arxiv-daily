---
layout: default
title: M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar
---

# M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar

**arXiv**: [2510.27166v1](https://arxiv.org/abs/2510.27166) | [PDF](https://arxiv.org/pdf/2510.27166.pdf)

**ä½œè€…**: Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM^3Detectionæ¡†æž¶ï¼Œé€šè¿‡å¤šå¸§å¤šçº§ç‰¹å¾èžåˆè§£å†³ç›¸æœºä¸Ž4Dæˆåƒé›·è¾¾å¤šæ¨¡æ€3Dæ£€æµ‹é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€3Dæ£€æµ‹` `ç›¸æœº-é›·è¾¾èžåˆ` `å¤šå¸§ç‰¹å¾èžåˆ` `æ—¶ç©ºæŽ¨ç†` `4Dæˆåƒé›·è¾¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•å¸§è¾“å…¥å¯¼è‡´åœºæ™¯ä¿¡æ¯ä¸å®Œæ•´ï¼Œå›¾åƒé€€åŒ–ä¸Žé›·è¾¾ç¨€ç–æ€§å½±å“æ£€æµ‹æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è·Ÿè¸ªå™¨è½¨è¿¹å¼•å¯¼å…¨å±€ä¸Žå±€éƒ¨ç‰¹å¾èšåˆï¼Œå¢žå¼ºå¤šå¸§æ—¶ç©ºæŽ¨ç†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VoDå’ŒTJ4DRadSetæ•°æ®é›†ä¸Šå®žçŽ°å…ˆè¿›3Dæ£€æµ‹æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in 4D imaging radar have enabled robust perception in adverse
> weather, while camera sensors provide dense semantic information. Fusing the
> these complementary modalities has great potential for cost-effective 3D
> perception. However, most existing camera-radar fusion methods are limited to
> single-frame inputs, capturing only a partial view of the scene. The incomplete
> scene information, compounded by image degradation and 4D radar sparsity,
> hinders overall detection performance. In contrast, multi-frame fusion offers
> richer spatiotemporal information but faces two challenges: achieving robust
> and effective object feature fusion across frames and modalities, and
> mitigating the computational cost of redundant feature extraction.
> Consequently, we propose M^3Detection, a unified multi-frame 3D object
> detection framework that performs multi-level feature fusion on multi-modal
> data from camera and 4D imaging radar. Our framework leverages intermediate
> features from the baseline detector and employs the tracker to produce
> reference trajectories, improving computational efficiency and providing richer
> information for second-stage. In the second stage, we design a global-level
> inter-object feature aggregation module guided by radar information to align
> global features across candidate proposals and a local-level inter-grid feature
> aggregation module that expands local features along the reference trajectories
> to enhance fine-grained object representation. The aggregated features are then
> processed by a trajectory-level multi-frame spatiotemporal reasoning module to
> encode cross-frame interactions and enhance temporal representation. Extensive
> experiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detection
> achieves state-of-the-art 3D detection performance, validating its
> effectiveness in multi-frame detection with camera-4D imaging radar fusion.

