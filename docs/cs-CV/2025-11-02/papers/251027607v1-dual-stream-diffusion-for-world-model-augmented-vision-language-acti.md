---
layout: default
title: Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model
---

# Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model

**arXiv**: [2510.27607v1](https://arxiv.org/abs/2510.27607) | [PDF](https://arxiv.org/pdf/2510.27607.pdf)

**ä½œè€…**: John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæµæ‰©æ•£æ¨¡åž‹ä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ä¸­æ¨¡æ€å†²çªé—®é¢˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä¸–ç•Œå»ºæ¨¡` `æ‰©æ•£æ¨¡åž‹` `æ¨¡æ€è§£è€¦` `æœºå™¨äººç­–ç•¥å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰ä¸ŽåŠ¨ä½œæ¨¡æ€å·®å¼‚å¯¼è‡´è”åˆé¢„æµ‹å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒæµæ‰©æ•£æž¶æž„ï¼Œç‹¬ç«‹å™ªå£°æ‰°åŠ¨å’Œè§£è€¦æµåŒ¹é…æŸå¤±
3. å®žéªŒæ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, augmenting Vision-Language-Action models (VLAs) with world modeling
> has shown promise in improving robotic policy learning. However, it remains
> challenging to jointly predict next-state observations and action sequences
> because of the inherent difference between the two modalities. To address this,
> we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework
> that handles the modality conflict and enhances the performance of VLAs across
> diverse tasks. Specifically, we propose a multimodal diffusion transformer
> architecture that explicitly maintains separate modality streams while still
> enabling cross-modal knowledge sharing. In addition, we introduce independent
> noise perturbations for each modality and a decoupled flow-matching loss. This
> design enables the model to learn the joint distribution in a bidirectional
> manner while avoiding the need for a unified latent space. Based on the
> decoupling of modalities during training, we also introduce a joint sampling
> method that supports test-time scaling, where action and vision tokens evolve
> asynchronously at different rates. Through experiments on simulated benchmarks
> such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,
> while our test-time scaling approach provides an additional 2-5% boost. On
> real-world tasks with the Franka Research 3, DUST improves success rates by
> 13%, confirming its effectiveness beyond simulation. Furthermore, pre-training
> on action-free videos from BridgeV2 yields significant transfer gains on
> RoboCasa, underscoring DUST's potential for large-scale VLA pretraining.

