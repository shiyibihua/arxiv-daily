---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-19
---

# cs.CVï¼ˆ2025-08-19ï¼‰

ğŸ“Š å…± **31** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250814278v2-gala-guided-attention-with-language-alignment-for-open-vocabulary-ga.html">GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting</a></td>
  <td>æå‡ºGALAæ¡†æ¶ä»¥è§£å†³å¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14278v2" data-paper-url="./papers/250814278v2-gala-guided-attention-with-language-alignment-for-open-vocabulary-ga.html" onclick="toggleFavorite(this, '2508.14278v2', 'GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250814037v1-distilled-3dgsdistilled-3d-gaussian-splatting.html">Distilled-3DGS:Distilled 3D Gaussian Splatting</a></td>
  <td>æå‡ºè’¸é¦3Dé«˜æ–¯ç‚¹äº‘ä»¥è§£å†³é«˜ä¿çœŸæ¸²æŸ“çš„å­˜å‚¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14037v1" data-paper-url="./papers/250814037v1-distilled-3dgsdistilled-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.14037v1', 'Distilled-3DGS:Distilled 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250813911v2-physgm-large-physical-gaussian-model-for-feed-forward-4d-synthesis.html">PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</a></td>
  <td>æå‡ºPhysGMä»¥è§£å†³ç‰©ç†åŸºç¡€4Dåˆæˆä¸­çš„æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13911v2" data-paper-url="./papers/250813911v2-physgm-large-physical-gaussian-model-for-feed-forward-4d-synthesis.html" onclick="toggleFavorite(this, '2508.13911v2', 'PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250814295v1-pixels-to-play-a-foundation-model-for-3d-gameplay.html">Pixels to Play: A Foundation Model for 3D Gameplay</a></td>
  <td>æå‡ºPixels2Play-0.1ä»¥è§£å†³3Dæ¸¸æˆæ™ºèƒ½ä½“è¡Œä¸ºç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">behavior cloning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14295v1" data-paper-url="./papers/250814295v1-pixels-to-play-a-foundation-model-for-3d-gameplay.html" onclick="toggleFavorite(this, '2508.14295v1', 'Pixels to Play: A Foundation Model for 3D Gameplay')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250813439v1-structured-prompting-and-multi-agent-knowledge-distillation-for-traf.html">Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</a></td>
  <td>æå‡ºç»“æ„åŒ–æç¤ºä¸å¤šæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦ä»¥è§£å†³äº¤é€šè§†é¢‘ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13439v1" data-paper-url="./papers/250813439v1-structured-prompting-and-multi-agent-knowledge-distillation-for-traf.html" onclick="toggleFavorite(this, '2508.13439v1', 'Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250813712v1-diversity-enhanced-collaborative-mamba-for-semi-supervised-medical-i.html">Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation</a></td>
  <td>æå‡ºDiversity-enhanced Collaborative Mambaä»¥è§£å†³åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13712v1" data-paper-url="./papers/250813712v1-diversity-enhanced-collaborative-mamba-for-semi-supervised-medical-i.html" onclick="toggleFavorite(this, '2508.13712v1', 'Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250813599v1-towards-efficient-vision-state-space-models-via-token-merging.html">Towards Efficient Vision State Space Models via Token Merging</a></td>
  <td>æå‡ºMaMeä»¥è§£å†³SSMæ¨¡å‹è®¡ç®—æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13599v1" data-paper-url="./papers/250813599v1-towards-efficient-vision-state-space-models-via-token-merging.html" onclick="toggleFavorite(this, '2508.13599v1', 'Towards Efficient Vision State Space Models via Token Merging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250814153v2-lens-learning-to-segment-anything-with-unified-reinforced-reasoning.html">LENS: Learning to Segment Anything with Unified Reinforced Reasoning</a></td>
  <td>æå‡ºLENSæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬æç¤ºå›¾åƒåˆ†å‰²ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14153v2" data-paper-url="./papers/250814153v2-lens-learning-to-segment-anything-with-unified-reinforced-reasoning.html" onclick="toggleFavorite(this, '2508.14153v2', 'LENS: Learning to Segment Anything with Unified Reinforced Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250814015v1-backdooring-self-supervised-contrastive-learning-by-noisy-alignment.html">Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</a></td>
  <td>æå‡ºå™ªå£°å¯¹é½æ–¹æ³•ä»¥è§£å†³è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­çš„åé—¨æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14015v1" data-paper-url="./papers/250814015v1-backdooring-self-supervised-contrastive-learning-by-noisy-alignment.html" onclick="toggleFavorite(this, '2508.14015v1', 'Backdooring Self-Supervised Contrastive Learning by Noisy Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250813499v1-multi-view-clustering-via-bi-level-decoupling-and-consistency-learni.html">Multi-view Clustering via Bi-level Decoupling and Consistency Learning</a></td>
  <td>æå‡ºåŒå±‚è§£è€¦ä¸ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ä»¥æå‡å¤šè§†è§’èšç±»æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13499v1" data-paper-url="./papers/250813499v1-multi-view-clustering-via-bi-level-decoupling-and-consistency-learni.html" onclick="toggleFavorite(this, '2508.13499v1', 'Multi-view Clustering via Bi-level Decoupling and Consistency Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250813602v2-personavlog-personalized-multimodal-vlog-generation-with-multi-agent.html">PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction</a></td>
  <td>æå‡ºPersonaVlogä»¥è§£å†³ä¸ªæ€§åŒ–çŸ­è§†é¢‘ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13602v2" data-paper-url="./papers/250813602v2-personavlog-personalized-multimodal-vlog-generation-with-multi-agent.html" onclick="toggleFavorite(this, '2508.13602v2', 'PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250813796v1-a-fully-transformer-based-multimodal-framework-for-explainable-cance.html">A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports</a></td>
  <td>æå‡ºMed-CTXä»¥è§£å†³ä¹³è…ºç™Œè¶…å£°å›¾åƒåˆ†å‰²çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13796v1" data-paper-url="./papers/250813796v1-a-fully-transformer-based-multimodal-framework-for-explainable-cance.html" onclick="toggleFavorite(this, '2508.13796v1', 'A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250814264v2-directed-tokens-a-robust-multi-modality-alignment-approach-to-large-.html">Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models</a></td>
  <td>æå‡ºå®šå‘æ ‡è®°ä»¥è§£å†³å¤šæ¨¡æ€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14264v2" data-paper-url="./papers/250814264v2-directed-tokens-a-robust-multi-modality-alignment-approach-to-large-.html" onclick="toggleFavorite(this, '2508.14264v2', 'Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250813936v1-mmis-net-for-retinal-fluid-segmentation-and-detection.html">MMIS-Net for Retinal Fluid Segmentation and Detection</a></td>
  <td>æå‡ºMMIS-Netä»¥è§£å†³è§†ç½‘è†œæ¶²ä½“åˆ†å‰²ä¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13936v1" data-paper-url="./papers/250813936v1-mmis-net-for-retinal-fluid-segmentation-and-detection.html" onclick="toggleFavorite(this, '2508.13936v1', 'MMIS-Net for Retinal Fluid Segmentation and Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250813739v2-enhancing-targeted-adversarial-attacks-on-large-vision-language-mode.html">Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector</a></td>
  <td>æå‡ºä¸­é—´æŠ•å½±å™¨ä»¥å¢å¼ºé’ˆå¯¹å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13739v2" data-paper-url="./papers/250813739v2-enhancing-targeted-adversarial-attacks-on-large-vision-language-mode.html" onclick="toggleFavorite(this, '2508.13739v2', 'Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250813692v1-humanpcr-probing-mllm-capabilities-in-diverse-human-centric-scenes.html">HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes</a></td>
  <td>æå‡ºHumanPCRä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚äººç±»åœºæ™¯ä¸­çš„èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13692v1" data-paper-url="./papers/250813692v1-humanpcr-probing-mllm-capabilities-in-diverse-human-centric-scenes.html" onclick="toggleFavorite(this, '2508.13692v1', 'HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250813460v1-revisiting-mllm-token-technology-through-the-lens-of-classical-visua.html">Revisiting MLLM Token Technology through the Lens of Classical Visual Coding</a></td>
  <td>é€šè¿‡ç»å…¸è§†è§‰ç¼–ç é‡æ–°å®¡è§†MLLMä»¤ç‰ŒæŠ€æœ¯ä»¥æå‡ä¿¡æ¯ä¼ é€’æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13460v1" data-paper-url="./papers/250813460v1-revisiting-mllm-token-technology-through-the-lens-of-classical-visua.html" onclick="toggleFavorite(this, '2508.13460v1', 'Revisiting MLLM Token Technology through the Lens of Classical Visual Coding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250814014v2-online-3d-gaussian-splatting-modeling-with-novel-view-selection.html">Online 3D Gaussian Splatting Modeling with Novel View Selection</a></td>
  <td>æå‡ºåœ¨çº¿3Dé«˜æ–¯ç‚¹äº‘å»ºæ¨¡æ–¹æ³•ä»¥è§£å†³åœºæ™¯é‡å»ºä¸å®Œæ•´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14014v2" data-paper-url="./papers/250814014v2-online-3d-gaussian-splatting-modeling-with-novel-view-selection.html" onclick="toggleFavorite(this, '2508.14014v2', 'Online 3D Gaussian Splatting Modeling with Novel View Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250814041v1-longsplat-robust-unposed-3d-gaussian-splatting-for-casual-long-video.html">LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos</a></td>
  <td>æå‡ºLongSplatä»¥è§£å†³é•¿è§†é¢‘ä¸­çš„è§†è§’åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14041v1" data-paper-url="./papers/250814041v1-longsplat-robust-unposed-3d-gaussian-splatting-for-casual-long-video.html" onclick="toggleFavorite(this, '2508.14041v1', 'LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250813977v2-rovr-open-dataset-a-large-scale-depth-dataset-for-autonomous-driving.html">ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</a></td>
  <td>æå‡ºROVRæ•°æ®é›†ä»¥è§£å†³æ·±åº¦ä¼°è®¡å¤šæ ·æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13977v2" data-paper-url="./papers/250813977v2-rovr-open-dataset-a-large-scale-depth-dataset-for-autonomous-driving.html" onclick="toggleFavorite(this, '2508.13977v2', 'ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250813537v1-eavatar-expression-aware-head-avatar-reconstruction-with-generative-.html">EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors</a></td>
  <td>æå‡ºEAvatarä»¥è§£å†³é«˜ä¿çœŸå¤´éƒ¨è™šæ‹Ÿå½¢è±¡é‡å»ºä¸­çš„è¡¨æƒ…æ•æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13537v1" data-paper-url="./papers/250813537v1-eavatar-expression-aware-head-avatar-reconstruction-with-generative-.html" onclick="toggleFavorite(this, '2508.13537v1', 'EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250813775v1-mr6d-benchmarking-6d-pose-estimation-for-mobile-robots.html">MR6D: Benchmarking 6D Pose Estimation for Mobile Robots</a></td>
  <td>æå‡ºMR6Dæ•°æ®é›†ä»¥è§£å†³ç§»åŠ¨æœºå™¨äºº6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13775v1" data-paper-url="./papers/250813775v1-mr6d-benchmarking-6d-pose-estimation-for-mobile-robots.html" onclick="toggleFavorite(this, '2508.13775v1', 'MR6D: Benchmarking 6D Pose Estimation for Mobile Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250814797v1-mf-lpr2-multi-frame-license-plate-image-restoration-and-recognition-.html">MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow</a></td>
  <td>æå‡ºMF-LPR$^2$ä»¥è§£å†³ä½è´¨é‡è½¦ç‰Œå›¾åƒæ¢å¤ä¸è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14797v1" data-paper-url="./papers/250814797v1-mf-lpr2-multi-frame-license-plate-image-restoration-and-recognition-.html" onclick="toggleFavorite(this, '2508.14797v1', 'MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250813968v2-rotbench-evaluating-multimodal-large-language-models-on-identifying-.html">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</a></td>
  <td>æå‡ºRotBenchä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å›¾åƒæ—‹è½¬è¯†åˆ«èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13968v2" data-paper-url="./papers/250813968v2-rotbench-evaluating-multimodal-large-language-models-on-identifying-.html" onclick="toggleFavorite(this, '2508.13968v2', 'RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250813518v1-calibrating-biased-distribution-in-vfm-derived-latent-space-via-cros.html">Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</a></td>
  <td>æå‡ºå‡ ä½•çŸ¥è¯†å¼•å¯¼çš„åˆ†å¸ƒæ ¡å‡†æ–¹æ³•ä»¥è§£å†³æ ·æœ¬åå·®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13518v1" data-paper-url="./papers/250813518v1-calibrating-biased-distribution-in-vfm-derived-latent-space-via-cros.html" onclick="toggleFavorite(this, '2508.13518v1', 'Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250814160v2-rynnec-bringing-mllms-into-embodied-world.html">RynnEC: Bringing MLLMs into Embodied World</a></td>
  <td>æå‡ºRynnECä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«è®¤çŸ¥ä¸­çš„åº”ç”¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14160v2" data-paper-url="./papers/250814160v2-rynnec-bringing-mllms-into-embodied-world.html" onclick="toggleFavorite(this, '2508.14160v2', 'RynnEC: Bringing MLLMs into Embodied World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250813995v1-self-supervised-sparse-sensor-fusion-for-long-range-perception.html">Self-Supervised Sparse Sensor Fusion for Long Range Perception</a></td>
  <td>æå‡ºè‡ªç›‘ç£ç¨€ç–ä¼ æ„Ÿå™¨èåˆä»¥è§£å†³é•¿è·ç¦»æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sparse sensors</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13995v1" data-paper-url="./papers/250813995v1-self-supervised-sparse-sensor-fusion-for-long-range-perception.html" onclick="toggleFavorite(this, '2508.13995v1', 'Self-Supervised Sparse Sensor Fusion for Long Range Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250814024v1-unicon-unified-continual-learning-for-medical-foundational-models.html">UNICON: UNIfied CONtinual Learning for Medical Foundational Models</a></td>
  <td>æå‡ºUNICONæ¡†æ¶ä»¥è§£å†³åŒ»å­¦åŸºç¡€æ¨¡å‹çš„æŒç»­å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">UniCon</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14024v1" data-paper-url="./papers/250814024v1-unicon-unified-continual-learning-for-medical-foundational-models.html" onclick="toggleFavorite(this, '2508.14024v1', 'UNICON: UNIfied CONtinual Learning for Medical Foundational Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250813483v1-famnet-integrating-2d-and-3d-features-for-micro-expression-recogniti.html">FAMNet: Integrating 2D and 3D Features for Micro-expression Recognition via Multi-task Learning and Hierarchical Attention</a></td>
  <td>æå‡ºFAMNetä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­çš„ç‰¹å¾æå–æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13483v1" data-paper-url="./papers/250813483v1-famnet-integrating-2d-and-3d-features-for-micro-expression-recogniti.html" onclick="toggleFavorite(this, '2508.13483v1', 'FAMNet: Integrating 2D and 3D Features for Micro-expression Recognition via Multi-task Learning and Hierarchical Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250813792v1-visionlaw-inferring-interpretable-intrinsic-dynamics-from-visual-obs.html">VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization</a></td>
  <td>æå‡ºVisionLawä»¥è§£å†³ç‰©ä½“å†…åœ¨åŠ¨åŠ›å­¦æ¨æ–­é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13792v1" data-paper-url="./papers/250813792v1-visionlaw-inferring-interpretable-intrinsic-dynamics-from-visual-obs.html" onclick="toggleFavorite(this, '2508.13792v1', 'VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250813442v1-edtalk-full-disentanglement-for-controllable-talking-head-synthesis.html">EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis</a></td>
  <td>æå‡ºEDTalk++ä»¥è§£å†³å¯æ§äººå¤´åˆæˆä¸­çš„ç‰¹å¾è§£è€¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13442v1" data-paper-url="./papers/250813442v1-edtalk-full-disentanglement-for-controllable-talking-head-synthesis.html" onclick="toggleFavorite(this, '2508.13442v1', 'EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)