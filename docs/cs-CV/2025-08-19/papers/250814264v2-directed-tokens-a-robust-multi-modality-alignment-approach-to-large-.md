---
layout: default
title: Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models
---

# Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14264" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14264v2</a>
  <a href="https://arxiv.org/pdf/2508.14264.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14264v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14264v2', 'Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thanh-Dat Truong, Huu-Thien Tran, Tran Thai Son, Bhiksha Raj, Khoa Luu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: Accepted to NeurIPS'25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®šå‘æ ‡è®°ä»¥è§£å†³å¤šæ¨¡æ€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹é½` `è§†è§‰ç†è§£` `æ–‡æœ¬å¤„ç†` `å®šå‘æ ‡è®°` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹é²æ£’æ€§` `å›¾åƒé‡å»º` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½ä¸ç›¸å…³æ€§æ–¹é¢å­˜åœ¨é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡é‡å»ºå›¾åƒå’Œæ–‡æœ¬é¡ºåºçš„æ–°ä»»åŠ¡ï¼Œæ¥æ”¹å–„è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å¯¹é½å’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å„ç§ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»é¢ä¸´ä¸è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾å¯¹é½åŠç›¸å…³æ€§ç›¸å…³çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åŸºæœ¬é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡è§£å†³æ‰“ä¹±é—®é¢˜æ¥æ”¹å–„è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„é²æ£’å¯¹é½ã€‚å…·ä½“è€Œè¨€ï¼Œæ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡å¼•å…¥é‡å»ºå›¾åƒé¡ºåºå’Œæ–‡æœ¬é¡ºåºçš„ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€è§†è§‰ç†è§£å’Œè·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å®šå‘æ ‡è®°æ–¹æ³•ï¼Œä»¥æ•æ‰è§†è§‰å’Œæ–‡æœ¬çŸ¥è¯†ï¼Œå¢å¼ºé‡å»ºè§†è§‰è¾“å…¥æ­£ç¡®é¡ºåºçš„èƒ½åŠ›ã€‚æœ€åï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒåˆ°å“åº”å¼•å¯¼æŸå¤±ï¼Œè¿›ä¸€æ­¥æå‡LMMåœ¨å“åº”ä¸­çš„è§†è§‰ç†è§£ã€‚è¯¥æ–¹æ³•åœ¨å­¦æœ¯ä»»åŠ¡å¯¼å‘å’ŒæŒ‡ä»¤è·Ÿéšçš„LMMåŸºå‡†æµ‹è¯•ä¸­ï¼Œå§‹ç»ˆå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾å¯¹é½æ–¹é¢çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œå¸¸å¸¸å—åˆ°ç‰¹å¾æ‰“ä¹±çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡å¼•å…¥é‡å»ºå›¾åƒé¡ºåºå’Œæ–‡æœ¬é¡ºåºçš„ä»»åŠ¡ï¼Œæ¥å¢å¼ºè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å¯¹é½èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè§†è§‰ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡é‡å»ºä»»åŠ¡å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬çš„æ­£ç¡®é¡ºåºï¼›åœ¨å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨æ–°çš„å›¾åƒåˆ°å“åº”å¼•å¯¼æŸå¤±è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å®šå‘æ ‡è®°æ–¹æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰è§†è§‰å’Œæ–‡æœ¬çŸ¥è¯†ï¼Œå¢å¼ºæ¨¡å‹é‡å»ºè§†è§‰è¾“å…¥é¡ºåºçš„èƒ½åŠ›ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå®šå‘æ ‡è®°æä¾›äº†æ›´æ˜ç¡®çš„ç‰¹å¾å¯¹é½æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œæå‡ºäº†å›¾åƒåˆ°å“åº”å¼•å¯¼æŸå¤±ï¼Œæ—¨åœ¨é€šè¿‡å¼•å¯¼æ¨¡å‹å…³æ³¨é‡è¦çš„è§†è§‰ç‰¹å¾æ¥æå‡ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ¶æ„çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨å¤šæ¨¡æ€å¯¹é½ä»»åŠ¡ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä»¥å¾€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›å’Œè§†è§‰ç†è§£èƒ½åŠ›æå‡æ˜¾è‘—ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†æ•´ä½“æå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ç´§å¯†ç»“åˆçš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½åŠ©ç†ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç†è§£å’Œç¤¾äº¤åª’ä½“åˆ†æç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„äººæœºäº¤äº’ä½“éªŒå’Œä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large multimodal models (LMMs) have gained impressive performance due to their outstanding capability in various understanding tasks. However, these models still suffer from some fundamental limitations related to robustness and generalization due to the alignment and correlation between visual and textual features. In this paper, we introduce a simple but efficient learning mechanism for improving the robust alignment between visual and textual modalities by solving shuffling problems. In particular, the proposed approach can improve reasoning capability, visual understanding, and cross-modality alignment by introducing two new tasks: reconstructing the image order and the text order into the LMM's pre-training and fine-tuning phases. In addition, we propose a new directed-token approach to capture visual and textual knowledge, enabling the capability to reconstruct the correct order of visual inputs. Then, we introduce a new Image-to-Response Guided loss to further improve the visual understanding of the LMM in its responses. The proposed approach consistently achieves state-of-the-art (SoTA) performance compared with prior LMMs on academic task-oriented and instruction-following LMM benchmarks.

