---
layout: default
title: RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation
---

# RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13968" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13968v2</a>
  <a href="https://arxiv.org/pdf/2508.13968.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13968v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13968v2', 'RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-08-20)

**å¤‡æ³¨**: 20 pages. Code and data: https://github.com/tianyiniu/RotBench

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRotBenchä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å›¾åƒæ—‹è½¬è¯†åˆ«èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å›¾åƒæ—‹è½¬è¯†åˆ«` `è§†è§‰æ¨ç†` `åŸºå‡†æµ‹è¯•` `ç©ºé—´å…³ç³»ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ—‹è½¬è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨90Â°å’Œ270Â°çš„åŒºåˆ†ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºRotBenchåŸºå‡†ï¼Œé€šè¿‡350å¹…å›¾åƒè¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹å›¾åƒæ—‹è½¬çš„è¯†åˆ«èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æ¨¡å‹åœ¨è¯†åˆ«0Â°å’Œ180Â°å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨90Â°å’Œ270Â°çš„åŒºåˆ†ä¸Šå‡ ä¹æ²¡æœ‰æå‡ï¼Œè¡¨æ˜æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«æ—‹è½¬0Â°ã€90Â°ã€180Â°å’Œ270Â°çš„è¾“å…¥å›¾åƒæ–¹å‘ä¸Šçš„å‡†ç¡®æ€§ã€‚è¯¥ä»»åŠ¡è¦æ±‚æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä»¥æ£€æµ‹æ—‹è½¬çº¿ç´¢å¹¶åœ¨ä¸åŒæ–¹å‘ä¸Šç†è§£å›¾åƒçš„ç©ºé—´å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†RotBenchï¼Œä¸€ä¸ªåŒ…å«350å¹…ç»è¿‡äººå·¥ç­›é€‰çš„ç”Ÿæ´»æ–¹å¼ã€è‚–åƒå’Œé£æ™¯å›¾åƒçš„åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡è¿™ä¸€ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šä¸ªæœ€å…ˆè¿›çš„MLLMsï¼ŒåŒ…æ‹¬GPT-5ã€o3å’ŒGemini-2.5-Proï¼Œåœ¨è¯†åˆ«å›¾åƒæ—‹è½¬æ–¹é¢çš„è¡¨ç°å¹¶ä¸å¯é ã€‚æä¾›è¾…åŠ©ä¿¡æ¯æˆ–ä½¿ç”¨é“¾å¼æ€ç»´æç¤ºä»…å¸¦æ¥äº†å°å¹…ä¸”ä¸ä¸€è‡´çš„æå‡ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è¯†åˆ«æ­£å‘ï¼ˆ0Â°ï¼‰å›¾åƒï¼Œè€ŒæŸäº›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å€’ç½®ï¼ˆ180Â°ï¼‰å›¾åƒï¼Œä½†æ²¡æœ‰æ¨¡å‹èƒ½å¤Ÿå¯é åœ°åŒºåˆ†90Â°å’Œ270Â°çš„æ—‹è½¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ—‹è½¬è¯†åˆ«ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹90Â°å’Œ270Â°æ—‹è½¬çš„åŒºåˆ†èƒ½åŠ›è¾ƒå¼±ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥RotBenchåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°MLLMsåœ¨ä¸åŒæ—‹è½¬è§’åº¦ä¸‹çš„è¡¨ç°ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ä¸äººç±»çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRotBenchåŸºå‡†åŒ…å«350å¹…å›¾åƒï¼Œæ¶µç›–ç”Ÿæ´»æ–¹å¼ã€è‚–åƒå’Œé£æ™¯ç±»å‹ï¼Œæ¨¡å‹é€šè¿‡åˆ†æå›¾åƒçš„æ—‹è½¬çº¿ç´¢å’Œç©ºé—´å…³ç³»è¿›è¡Œè¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šRotBenchçš„å¼•å…¥ä¸ºè¯„ä¼°MLLMsæä¾›äº†ä¸€ä¸ªæ–°çš„æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—‹è½¬è¯†åˆ«ä»»åŠ¡ä¸Šï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å¤šç§è¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚å›¾åƒæ ‡é¢˜ã€æ·±åº¦å›¾ç­‰ï¼‰å’Œé“¾å¼æ€ç»´æç¤ºï¼Œä½†ç»“æœæ˜¾ç¤ºè¿™äº›æ–¹æ³•å¯¹90Â°å’Œ270Â°çš„åŒºåˆ†æå‡æœ‰é™ï¼Œè¡¨æ˜æ¨¡å‹çš„è®¾è®¡ä»éœ€æ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹èƒ½å¤Ÿå¯é è¯†åˆ«0Â°å›¾åƒï¼Œè€ŒæŸäº›æ¨¡å‹åœ¨180Â°å›¾åƒè¯†åˆ«ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨90Â°å’Œ270Â°çš„åŒºåˆ†ä¸Šå‡ ä¹æ²¡æœ‰æå‡ã€‚é€šè¿‡åŒæ—¶å±•ç¤ºä¸åŒæ–¹å‘çš„å›¾åƒï¼Œæ¨ç†æ¨¡å‹çš„è¡¨ç°æœ‰æ‰€æ”¹å–„ï¼Œè€ŒæŠ•ç¥¨æœºåˆ¶åˆ™æå‡äº†è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰éœ€è¦å›¾åƒç†è§£å’Œç©ºé—´æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ—‹è½¬è¯†åˆ«ä¸Šçš„èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0Â°, 90Â°, 180Â°, and 270Â°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0Â°) images, while certain models are able to identify upside-down (180Â°) images. None can reliably distinguish between 90Â° and 270Â°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90Â° and 270Â° rotations, despite substantially improving the identification of 180Â° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.

