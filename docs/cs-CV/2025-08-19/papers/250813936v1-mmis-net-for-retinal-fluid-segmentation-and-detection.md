---
layout: default
title: MMIS-Net for Retinal Fluid Segmentation and Detection
---

# MMIS-Net for Retinal Fluid Segmentation and Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13936" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13936v1</a>
  <a href="https://arxiv.org/pdf/2508.13936.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13936v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13936v1', 'MMIS-Net for Retinal Fluid Segmentation and Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nchongmaje Ndipenocha, Alina Mirona, Kezhi Wanga, Yongmin Li

**åˆ†ç±»**: eess.IV, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMIS-Netä»¥è§£å†³è§†ç½‘è†œæ¶²ä½“åˆ†å‰²ä¸æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒåˆ†å‰²` `æ·±åº¦å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `ç›¸ä¼¼æ€§èåˆ` `ä¸€çƒ­æ ‡ç­¾` `æ¶²ä½“æ£€æµ‹` `æ¨¡å‹æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²ä¸æ£€æµ‹æ–¹æ³•å¤šä¾èµ–å•ä¸€æ•°æ®æºï¼Œå¿½è§†äº†å¤šæ¨¡æ€æ•°æ®çš„ååŒæ•ˆåº”ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æå‡ºMMIS-Netç®—æ³•ï¼Œé€šè¿‡ç›¸ä¼¼æ€§èåˆæ¨¡å—å’Œä¸€çƒ­æ ‡ç­¾ç©ºé—´ï¼Œæå‡ä¸åŒæ•°æ®é›†é—´çš„ç‰¹å¾èåˆä¸ç±»ä¸€è‡´æ€§ã€‚
3. åœ¨RETOUCHå¤§æŒ‘æˆ˜éšè—æµ‹è¯•é›†ä¸Šï¼ŒMMIS-Netåœ¨æ¶²ä½“åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†0.83çš„Diceç³»æ•°ï¼Œæ¶²ä½“æ£€æµ‹ä»»åŠ¡çš„æ›²çº¿ä¸‹é¢ç§¯ä¸º1ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æå‡åŒ»å­¦å›¾åƒä¸­ç–¾ç—…çš„åˆ†å‰²ä¸æ£€æµ‹æ•ˆæœã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å•ä¸€æ•°æ®æºã€æ¨¡æ€ã€å™¨å®˜æˆ–ç–¾ç—…ç±»å‹ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šç§æ³¨é‡Šæ•°æ®çš„ååŒæ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•MMIS-Netï¼ˆå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œï¼‰ï¼Œé€šè¿‡ç›¸ä¼¼æ€§èåˆæ¨¡å—å®ç°ç‰¹å¾å›¾çš„èåˆï¼Œå¹¶åˆ›å»ºäº†ä¸€ç§ä¸€çƒ­æ ‡ç­¾ç©ºé—´ä»¥å¤„ç†ä¸åŒæ•°æ®é›†é—´çš„ç±»ä¸ä¸€è‡´æ€§ã€‚MMIS-Netåœ¨æ¶µç›–19ä¸ªå™¨å®˜å’Œ2ç§æ¨¡æ€çš„10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºå…¶åœ¨RETOUCHå¤§æŒ‘æˆ˜éšè—æµ‹è¯•é›†ä¸Šè¶…è¶Šäº†å¤§å‹åŸºç¡€æ¨¡å‹å’Œå…¶ä»–æœ€å…ˆè¿›ç®—æ³•ï¼Œæ¶²ä½“åˆ†å‰²ä»»åŠ¡çš„Diceç³»æ•°è¾¾åˆ°0.83ï¼Œæ¶²ä½“æ£€æµ‹ä»»åŠ¡çš„æ›²çº¿ä¸‹é¢ç§¯è¾¾åˆ°1ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒä¸­æ¶²ä½“åˆ†å‰²ä¸æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åœ¨å•ä¸€æ•°æ®æºä¸Šè®­ç»ƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®çš„æ½œåŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„MMIS-Neté€šè¿‡ç›¸ä¼¼æ€§èåˆæ¨¡å—æ¥æ•´åˆæ¥è‡ªä¸åŒæ•°æ®é›†çš„ç‰¹å¾ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€çƒ­æ ‡ç­¾ç©ºé—´æ¥å¤„ç†ç±»ä¸ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æœªè§æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMIS-Netçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ã€ç›¸ä¼¼æ€§èåˆæ¨¡å—ã€ç‰¹å¾æå–ç½‘ç»œå’Œè¾“å‡ºå±‚ã€‚ç›¸ä¼¼æ€§èåˆæ¨¡å—è´Ÿè´£ç‰¹å¾å›¾çš„èåˆï¼Œè€Œç‰¹å¾æå–ç½‘ç»œåˆ™åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯æå–åŒ»å­¦å›¾åƒä¸­çš„é‡è¦ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMIS-Netçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥ç›¸ä¼¼æ€§èåˆæ¨¡å—å’Œä¸€çƒ­æ ‡ç­¾ç©ºé—´ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒæ•°æ®é›†é—´çš„ç±»ä¸ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²ä¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å‰²æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ä¾¿æ›´å¥½åœ°èåˆå¤šæ¨¡æ€æ•°æ®çš„ç‰¹å¾ï¼Œç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨10ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…¨é¢çš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨RETOUCHå¤§æŒ‘æˆ˜çš„éšè—æµ‹è¯•é›†ä¸Šï¼ŒMMIS-Netåœ¨æ¶²ä½“åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†0.83çš„Diceç³»æ•°ï¼Œè¡¨ç°ä¼˜äºç°æœ‰å¤§å‹åŸºç¡€æ¨¡å‹å’Œå…¶ä»–æœ€å…ˆè¿›ç®—æ³•ã€‚æ­¤å¤–ï¼Œæ¶²ä½“æ£€æµ‹ä»»åŠ¡çš„æ›²çº¿ä¸‹é¢ç§¯è¾¾åˆ°äº†1ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹åœ¨æ£€æµ‹ç²¾åº¦ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨åŒ»å­¦å½±åƒåˆ†æé¢†åŸŸã€‚MMIS-Netå¯ä»¥ç”¨äºè‡ªåŠ¨åŒ–çš„ç–¾ç—…æ£€æµ‹ä¸è¯Šæ–­ï¼Œå¸®åŠ©åŒ»ç”Ÿæé«˜å·¥ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›æ¨å¹¿è‡³å…¶ä»–åŒ»å­¦å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œä¿ƒè¿›å¤šæ¨¡æ€æ•°æ®çš„æ•´åˆä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Purpose: Deep learning methods have shown promising results in the segmentation, and detection of diseases in medical images. However, most methods are trained and tested on data from a single source, modality, organ, or disease type, overlooking the combined potential of other available annotated data. Numerous small annotated medical image datasets from various modalities, organs, and diseases are publicly available. In this work, we aim to leverage the synergistic potential of these datasets to improve performance on unseen data. Approach: To this end, we propose a novel algorithm called MMIS-Net (MultiModal Medical Image Segmentation Network), which features Similarity Fusion blocks that utilize supervision and pixel-wise similarity knowledge selection for feature map fusion. Additionally, to address inconsistent class definitions and label contradictions, we created a one-hot label space to handle classes absent in one dataset but annotated in another. MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalities to build a single model. Results: The algorithm was evaluated on the RETOUCH grand challenge hidden test set, outperforming large foundation models for medical image segmentation and other state-of-the-art algorithms. We achieved the best mean Dice score of 0.83 and an absolute volume difference of 0.035 for the fluids segmentation task, as well as a perfect Area Under the Curve of 1 for the fluid detection task. Conclusion: The quantitative results highlight the effectiveness of our proposed model due to the incorporation of Similarity Fusion blocks into the network's backbone for supervision and similarity knowledge selection, and the use of a one-hot label space to address label class inconsistencies and contradictions.

