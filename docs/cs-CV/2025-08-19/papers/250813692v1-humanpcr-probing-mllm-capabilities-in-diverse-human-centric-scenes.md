---
layout: default
title: HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes
---

# HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13692" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13692v1</a>
  <a href="https://arxiv.org/pdf/2508.13692.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13692v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13692v1', 'HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keliang Li, Hongze Shen, Hao Shi, Ruibing Hou, Hong Chang, Jie Huang, Chenghao Jia, Wen Wang, Yiling Wu, Dongmei Jiang, Shiguang Shan, Xilin Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHumanPCRä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚äººç±»åœºæ™¯ä¸­çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `äººç±»ä¸­å¿ƒ` `è§†è§‰ç†è§£` `è§†é¢‘æ¨ç†` `èƒ½åŠ›è¯„ä¼°` `æ€ç»´é“¾æ¨ç†` `ç©ºé—´æ„ŸçŸ¥` `æ—¶é—´ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚äººç±»åœºæ™¯ä¸­çš„ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç©ºé—´å’Œæ—¶é—´æ„ŸçŸ¥æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºHumanPCRè¯„ä¼°å¥—ä»¶ï¼Œé€šè¿‡æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†ä¸‰ä¸ªå±‚æ¬¡ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸äººç±»ç›¸å…³çš„è§†è§‰ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨æå–å…³é”®è§†è§‰è¯æ®æ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒHumanPCRçš„è®¾è®¡æ­ç¤ºäº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œäººå·¥é€šç”¨æ™ºèƒ½çš„è¿½æ±‚éœ€è¦åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­å®ç°ä¸äººç±»ç›¸å½“çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†HumanPCRï¼Œä¸€ä¸ªè¯„ä¼°å¥—ä»¶ï¼Œç”¨äºæ¢æµ‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸äººç±»ç›¸å…³çš„è§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„èƒ½åŠ›ï¼Œæ¶µç›–æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†ä¸‰ä¸ªå±‚æ¬¡ã€‚Human-På’ŒHuman-CåŒ…å«è¶…è¿‡6000ä¸ªç»è¿‡äººç±»éªŒè¯çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œè¯„ä¼°9ä¸ªç»´åº¦çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç°æœ‰åŸºå‡†å¸¸å¸¸å¿½è§†çš„åŸºæœ¬æŠ€èƒ½ã€‚Human-Ræä¾›äº†ä¸€é¡¹æŒ‘æˆ˜æ€§çš„è§†é¢‘æ¨ç†æµ‹è¯•ï¼Œè¦æ±‚æ•´åˆå¤šä¸ªè§†è§‰è¯æ®ï¼Œä¸»åŠ¨æå–è¶…å‡ºé—®é¢˜æç¤ºçš„ä¸Šä¸‹æ–‡ï¼Œå¹¶åº”ç”¨ç±»äººä¸“ä¸šçŸ¥è¯†ã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…å«äººç±»æ³¨é‡Šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œæ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ã€‚å¯¹30å¤šç§æœ€å…ˆè¿›æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨äººç±»ä¸­å¿ƒçš„è§†è§‰ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠè¯¦ç»†ç©ºé—´æ„ŸçŸ¥ã€æ—¶é—´ç†è§£å’Œå¿ƒç†å»ºæ¨¡çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚äººç±»åœºæ™¯ä¸­ç†è§£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç©ºé—´æ„ŸçŸ¥ã€æ—¶é—´ç†è§£å’Œå¿ƒç†å»ºæ¨¡ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†è¿™äº›å…³é”®æŠ€èƒ½ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡HumanPCRè¯„ä¼°å¥—ä»¶ï¼Œç³»ç»Ÿæ€§åœ°æ¢æµ‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†ä¸‰ä¸ªå±‚æ¬¡çš„èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡å¤šç»´åº¦çš„ä»»åŠ¡ï¼Œç‰¹åˆ«å…³æ³¨äººç±»ä¸­å¿ƒçš„è§†è§‰ä¸Šä¸‹æ–‡ï¼Œæä¾›æ›´å…¨é¢çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHumanPCRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šHuman-Pï¼ˆæ„ŸçŸ¥ï¼‰ã€Human-Cï¼ˆç†è§£ï¼‰å’ŒHuman-Rï¼ˆæ¨ç†ï¼‰ã€‚æ¯ä¸ªæ¨¡å—åŒ…å«å¤§é‡ç»è¿‡äººç±»éªŒè¯çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯Human-Ræ¨¡å—è¿˜åŒ…æ‹¬è§†é¢‘æ¨ç†æµ‹è¯•ï¼Œè¦æ±‚æ¨¡å‹æ•´åˆå¤šç§è§†è§‰è¯æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºHumanPCRçš„è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯å…¶å¤šå±‚æ¬¡çš„è¯„ä¼°æ¡†æ¶å’Œäººç±»æ³¨é‡Šçš„æ€ç»´é“¾æ¨ç†ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€ç»´åº¦è¯„ä¼°å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„èƒ½åŠ›è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒHuman-På’ŒHuman-Cæ¨¡å—åŒ…å«è¶…è¿‡6000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–9ä¸ªç»´åº¦çš„ä»»åŠ¡ã€‚Human-Ræ¨¡å—åˆ™é€šè¿‡æ‰‹åŠ¨ç­–åˆ’çš„è§†é¢‘æ¨ç†æµ‹è¯•ï¼Œè¦æ±‚æ¨¡å‹ä¸»åŠ¨æå–è¶…å‡ºé—®é¢˜æç¤ºçš„ä¸Šä¸‹æ–‡ï¼Œä½“ç°äº†å¯¹äººç±»ç±»æ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡30ç§æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨HumanPCRè¯„ä¼°ä¸­é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç©ºé—´æ„ŸçŸ¥å’Œæ—¶é—´ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æ¨¡å‹åœ¨æå–å…³é”®è§†è§‰è¯æ®æ–¹é¢çš„ä¸è¶³ï¼Œè¡¨æ˜å…¶å¯¹æŸ¥è¯¢å¼•å¯¼æ£€ç´¢çš„ä¾èµ–æ€§è¿‡å¼ºï¼Œæå‡å¹…åº¦æœ‰é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æ•™è‚²å’Œå¨±ä¹ç­‰å¤šä¸ªè¡Œä¸šã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚äººç±»åœºæ™¯ä¸­çš„ç†è§£èƒ½åŠ›ï¼ŒHumanPCRå°†æ¨åŠ¨è¿™äº›é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼Œä¿ƒè¿›äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½åŒ–ã€‚æœªæ¥ï¼ŒHumanPCRçš„è®¾è®¡ç†å¿µå’Œè¯„ä¼°æ ‡å‡†å¯èƒ½æˆä¸ºå¤šæ¨¡æ€æ¨¡å‹å‘å±•çš„é‡è¦å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The aspiration for artificial general intelligence, fueled by the rapid progress of multimodal models, demands human-comparable performance across diverse environments. We propose HumanPCR, an evaluation suite for probing MLLMs' capacity about human-related visual contexts across three hierarchical levels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C, and Human-R, respectively). Human-P and Human-C feature over 6,000 human-verified multiple choice questions, assessing massive tasks of 9 dimensions, including but not limited to essential skills frequently overlooked by existing benchmarks. Human-R offers a challenging manually curated video reasoning test that requires integrating multiple visual evidences, proactively extracting context beyond question cues, and applying human-like expertise. Each question includes human-annotated Chain-of-Thought (CoT) rationales with key visual evidence to support further research. Extensive evaluations on over 30 state-of-the-art models exhibit significant challenges in human-centric visual understanding, particularly in tasks involving detailed space perception, temporal understanding, and mind modeling. Moreover, analysis of Human-R reveals the struggle of models in extracting essential proactive visual evidence from diverse human scenes and their faulty reliance on query-guided retrieval. Even with advanced techniques like scaling visual contexts and test-time thinking yield only limited benefits. We hope HumanPCR and our findings will advance the development, evaluation, and human-centric application of multimodal models.

