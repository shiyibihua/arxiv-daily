---
layout: default
title: GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting
---

# GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14278v2</a>
  <a href="https://arxiv.org/pdf/2508.14278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14278v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14278v2', 'GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Elena Alegret, Kunyi Li, Sen Wang, Siyun Liang, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-08-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGALAæ¡†æ¶ä»¥è§£å†³å¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¼€æ”¾è¯æ±‡` `è‡ªç›‘ç£å­¦ä¹ ` `é«˜æ–¯ç‚¹äº‘` `äº¤å‰æ³¨æ„åŠ›` `ç‰¹å¾å¯¹é½` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dåœºæ™¯ç†è§£æ–¹æ³•åœ¨ä»2Då›¾åƒä¸­æå–ç»†ç²’åº¦çš„è¯­è¨€æ„ŸçŸ¥3Dè¡¨ç¤ºæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æ»¡è¶³å¼€æ”¾è¯æ±‡çš„éœ€æ±‚ã€‚
2. GALAæ¡†æ¶é€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æç‚¼3Då®ä¾‹ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä»¥å®ç°è¯­è¨€ç‰¹å¾çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚
3. å®éªŒè¯æ˜ï¼ŒGALAåœ¨å¼€æ”¾è¯æ±‡æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒ2Då’Œ3DæŸ¥è¯¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dåœºæ™¯é‡å»ºä¸ç†è§£æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ä»2Då›¾åƒä¸­æ•æ‰ç»†ç²’åº¦ã€è¯­è¨€æ„ŸçŸ¥çš„3Dè¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºGALAï¼Œä¸€ä¸ªç”¨äºå¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£çš„æ–°æ¡†æ¶ï¼Œç»“åˆ3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ã€‚GALAé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æç‚¼åœºæ™¯ç‰¹å®šçš„3Då®ä¾‹ç‰¹å¾åœºã€‚ä¸ºæ‰©å±•åˆ°å¹¿ä¹‰è¯­è¨€ç‰¹å¾åœºï¼ŒGALAå¼•å…¥äº†æ ¸å¿ƒè´¡çŒ®â€”â€”ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªå¯å­¦ä¹ ä»£ç æœ¬çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºç¼–ç è§†è§’æ— å…³çš„è¯­ä¹‰åµŒå…¥ã€‚è¿™ä¸€è®¾è®¡ä¸ä»…ç¡®ä¿äº†å®ä¾‹å†…éƒ¨ç‰¹å¾çš„ç›¸ä¼¼æ€§ï¼Œè¿˜æ”¯æŒæ— ç¼çš„2Då’Œ3Då¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒGALAåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•ç°äº†å“è¶Šçš„å¼€æ”¾è¯æ±‡æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Dåœºæ™¯ç†è§£æ–¹æ³•åœ¨æ•æ‰ç»†ç²’åº¦ã€è¯­è¨€æ„ŸçŸ¥3Dè¡¨ç¤ºæ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸‹çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGALAé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æç‚¼åœºæ™¯ç‰¹å®šçš„3Då®ä¾‹ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨ä¸¤ä¸ªå¯å­¦ä¹ çš„ä»£ç æœ¬æ¥ç¼–ç è§†è§’æ— å…³çš„è¯­ä¹‰åµŒå…¥ï¼Œä»è€Œå¢å¼ºç‰¹å¾çš„è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGALAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å—ã€äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œç‰¹å¾æå–æ¨¡å—ã€‚è‡ªç›‘ç£å­¦ä¹ ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„3Dç‰¹å¾ï¼Œè€Œäº¤å‰æ³¨æ„åŠ›æ¨¡å—åˆ™å®ç°äº†è¯­è¨€ç‰¹å¾ä¸3Dç‰¹å¾çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šGALAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œä¸¤ä¸ªå¯å­¦ä¹ çš„ä»£ç æœ¬ï¼Œè¿™ä¸€è®¾è®¡ä¸ä»…æé«˜äº†ç‰¹å¾çš„ç›¸ä¼¼æ€§ï¼Œè¿˜æ”¯æŒäº†å¼€æ”¾è¯æ±‡æŸ¥è¯¢ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGALAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç‰¹å¾çš„å¯¹é½æ•ˆæœï¼Œå¹¶é€šè¿‡é«˜æ•ˆçš„ç½‘ç»œç»“æ„å‡å°‘äº†æ¯ä¸ªé«˜æ–¯çš„é«˜ç»´ç‰¹å¾å­¦ä¹ ï¼Œä»è€Œæå‡äº†æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GALAåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶å¼€æ”¾è¯æ±‡æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨2Då’Œ3DæŸ¥è¯¢ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GALAæ¡†æ¶åœ¨3Dåœºæ™¯ç†è§£ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶å¼€æ”¾è¯æ±‡ç‰¹æ€§ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†å¤šæ ·åŒ–çš„è¾“å…¥ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶åœ¨æœªæ¥çš„æ™ºèƒ½ç¯å¢ƒä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D scene reconstruction and understanding have gained increasing popularity, yet existing methods still struggle to capture fine-grained, language-aware 3D representations from 2D images. In this paper, we present GALA, a novel framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). GALA distills a scene-specific 3D instance feature field via self-supervised contrastive learning. To extend to generalized language feature fields, we introduce the core contribution of GALA, a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings. This design not only ensures intra-instance feature similarity but also supports seamless 2D and 3D open-vocabulary queries. It reduces memory consumption by avoiding per-Gaussian high-dimensional feature learning. Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D.

