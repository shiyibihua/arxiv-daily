---
layout: default
title: Self-Supervised Sparse Sensor Fusion for Long Range Perception
---

# Self-Supervised Sparse Sensor Fusion for Long Range Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13995" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13995v1</a>
  <a href="https://arxiv.org/pdf/2508.13995.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13995v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13995v1', 'Self-Supervised Sparse Sensor Fusion for Long Range Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Edoardo Palladin, Samuel Brucker, Filippo Ghilotti, Praveen Narayanan, Mario Bijelic, Felix Heide

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£ç¨€ç–ä¼ æ„Ÿå™¨èåˆä»¥è§£å†³é•¿è·ç¦»æ„ŸçŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `é•¿è·ç¦»æ„ŸçŸ¥` `è‡ªç›‘ç£å­¦ä¹ ` `ç¨€ç–è¡¨ç¤º` `å¤šæ¨¡æ€èåˆ` `è‡ªåŠ¨é©¾é©¶` `æ¿€å…‰é›·è¾¾` `ç›®æ ‡æ£€æµ‹` `æ•°æ®é¢„å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ„ŸçŸ¥æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨çŸ­è·ç¦»ï¼Œæ— æ³•æ»¡è¶³é•¿é€”é«˜é€Ÿé©¾é©¶çš„éœ€æ±‚ï¼Œå¯¼è‡´å®‰å…¨æ€§å’Œè§„åˆ’èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–è¡¨ç¤ºçš„3Dç¼–ç æ–¹æ³•ï¼Œå¹¶ç»“åˆè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€æ•°æ®ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç›®æ ‡æ£€æµ‹ä¸Šæå‡äº†26.6%çš„mAPï¼Œå¹¶åœ¨æ¿€å…‰é›·è¾¾é¢„æµ‹ä¸­å‡å°‘äº†30.5%çš„Chamferè·ç¦»ï¼Œæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŸå¸‚ä»¥å¤–ï¼Œè‡ªä¸»é©¾é©¶æ±½è½¦å’Œå¡è½¦éœ€è¦æŒæ¡åœ¨åŸé™…é«˜é€Ÿå…¬è·¯ä¸Šçš„é©¾é©¶ã€‚å®‰å…¨çš„é•¿é€”é«˜é€Ÿæ—…è¡Œè¦æ±‚è‡³å°‘250ç±³çš„æ„ŸçŸ¥è·ç¦»ï¼Œè¿™å¤§çº¦æ˜¯åŸå¸‚é©¾é©¶ä¸­é€šå¸¸å¤„ç†çš„50-100ç±³çš„äº”å€ã€‚ç°æœ‰æ„ŸçŸ¥æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¾ƒçŸ­çš„è·ç¦»ï¼Œå¹¶ä¾èµ–é¸Ÿç°å›¾ï¼ˆBEVï¼‰è¡¨ç¤ºï¼Œéšç€è·ç¦»çš„å¢åŠ ï¼Œå†…å­˜å’Œè®¡ç®—æˆæœ¬å‘ˆäºŒæ¬¡å¢é•¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡åŸºäºç¨€ç–è¡¨ç¤ºï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€å’Œæ—¶é—´ç‰¹å¾çš„3Dç¼–ç ï¼Œä»¥åŠä¸€ç§æ–°é¢–çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿å¾—èƒ½å¤Ÿä»æœªæ ‡è®°çš„ç›¸æœº-æ¿€å…‰é›·è¾¾æ•°æ®ä¸­è¿›è¡Œå¤§è§„æ¨¡å­¦ä¹ ã€‚è¯¥æ–¹æ³•å°†æ„ŸçŸ¥è·ç¦»æ‰©å±•è‡³250ç±³ï¼Œåœ¨ç›®æ ‡æ£€æµ‹ä¸­å®ç°äº†26.6%çš„mAPæå‡ï¼Œå¹¶åœ¨æ¿€å…‰é›·è¾¾é¢„æµ‹ä¸­å‡å°‘äº†30.5%çš„Chamferè·ç¦»ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä½³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªä¸»é©¾é©¶åœ¨é•¿è·ç¦»é«˜é€Ÿå…¬è·¯ä¸Šçš„æ„ŸçŸ¥é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºçŸ­è·ç¦»æ„ŸçŸ¥ï¼Œæ— æ³•æ»¡è¶³250ç±³ä»¥ä¸Šçš„æ„ŸçŸ¥éœ€æ±‚ï¼Œå¯¼è‡´è§„åˆ’å’Œå®‰å…¨æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–è¡¨ç¤ºçš„3Dç¼–ç æ–¹æ³•ï¼Œç»“åˆè‡ªç›‘ç£å­¦ä¹ ï¼Œä»æœªæ ‡è®°çš„ç›¸æœº-æ¿€å…‰é›·è¾¾æ•°æ®ä¸­æå–å¤šæ¨¡æ€ç‰¹å¾ï¼Œä»¥æé«˜é•¿è·ç¦»æ„ŸçŸ¥èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€3Dç¼–ç å’Œè‡ªç›‘ç£å­¦ä¹ æ¨¡å—ã€‚é€šè¿‡ç¨€ç–è¡¨ç¤ºï¼Œå‡å°‘è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼ŒåŒæ—¶ä¿æŒæ„ŸçŸ¥ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€ç‰¹å¾çš„èåˆï¼Œå¹¶é€šè¿‡ç¨€ç–ç¼–ç æŠ€æœ¯é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œæå‡äº†æ¨¡å‹çš„å®æ—¶æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†26.6%çš„mAPæå‡ï¼Œå¹¶åœ¨æ¿€å…‰é›·è¾¾é¢„æµ‹ä¸­å‡å°‘äº†30.5%çš„Chamferè·ç¦»ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒæˆåŠŸå°†æ„ŸçŸ¥è·ç¦»æ‰©å±•è‡³250ç±³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€ç‰©æµè¿è¾“å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿã€‚é€šè¿‡æå‡é•¿è·ç¦»æ„ŸçŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜é«˜é€Ÿå…¬è·¯è¡Œé©¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Outside of urban hubs, autonomous cars and trucks have to master driving on intercity highways. Safe, long-distance highway travel at speeds exceeding 100 km/h demands perception distances of at least 250 m, which is about five times the 50-100m typically addressed in city driving, to allow sufficient planning and braking margins. Increasing the perception ranges also allows to extend autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks, which need a longer planning horizon due to their high inertia. However, most existing perception approaches focus on shorter ranges and rely on Bird's Eye View (BEV) representations, which incur quadratic increases in memory and compute costs as distance grows. To overcome this limitation, we built on top of a sparse representation and introduced an efficient 3D encoding of multi-modal and temporal features, along with a novel self-supervised pre-training scheme that enables large-scale learning from unlabeled camera-LiDAR data. Our approach extends perception distances to 250 meters and achieves an 26.6% improvement in mAP in object detection and a decrease of 30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods, reaching distances up to 250 meters. Project Page: https://light.princeton.edu/lrs4fusion/

