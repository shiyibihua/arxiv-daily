---
layout: default
title: ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving
---

# ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13977" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13977v2</a>
  <a href="https://arxiv.org/pdf/2508.13977.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13977v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13977v2', 'ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xianda Guo, Ruijun Zhang, Yiqun Duan, Ruilin Wang, Matteo Poggi, Keyuan Zhou, Wenzhao Zheng, Wenke Huang, Gangwei Xu, Mike Horton, Yuan Si, Qin Zou, Hao Zhao, Long Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-09-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROVRæ•°æ®é›†ä»¥è§£å†³æ·±åº¦ä¼°è®¡å¤šæ ·æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦ä¼°è®¡` `æ•°æ®é›†` `è‡ªåŠ¨é©¾é©¶` `å¤šæ ·æ€§` `é²æ£’æ€§` `æ¨¡å‹è®­ç»ƒ` `è·¨æ•°æ®é›†æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦æ•°æ®é›†åœ¨å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. ROVRæ•°æ®é›†é€šè¿‡è½»é‡çº§é‡‡é›†ç®¡é“ï¼Œæä¾›äº†200Ké«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ¶µç›–å¤šç§åœºæ™¯å’Œæ¡ä»¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨ROVRä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†è¯¥æ•°æ®é›†åœ¨æ·±åº¦ä¼°è®¡é¢†åŸŸçš„æŒ‘æˆ˜æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¼°è®¡æ˜¯è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå’Œå¢å¼ºç°å®ä¸­çš„åŸºæœ¬ä»»åŠ¡ã€‚ç°æœ‰çš„æ•°æ®é›†å¦‚KITTIã€nuSceneså’ŒDDADè™½ç„¶æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†åœ¨å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚éšç€åŸºå‡†æ€§èƒ½çš„æ¥è¿‘é¥±å’Œï¼Œè¿«åˆ‡éœ€è¦æ–°ä¸€ä»£å¤§è§„æ¨¡ã€å¤šæ ·åŒ–ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ•°æ®é›†æ¥æ”¯æŒåŸºç¡€æ¨¡å‹å’Œå¤šæ¨¡æ€å­¦ä¹ çš„æ—¶ä»£ã€‚æœ¬æ–‡æå‡ºROVRï¼Œä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ·±åº¦æ•°æ®é›†ï¼Œæ—¨åœ¨æ•æ‰çœŸå®é©¾é©¶çš„å¤æ‚æ€§ã€‚ROVRåŒ…å«20ä¸‡å¸§é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ¶µç›–é«˜é€Ÿå…¬è·¯ã€ä¹¡æ‘å’ŒåŸå¸‚åœºæ™¯ï¼Œè·¨è¶Šæ˜¼å¤œå’Œæ¶åŠ£å¤©æ°”æ¡ä»¶ã€‚è½»é‡çº§é‡‡é›†ç®¡é“ç¡®ä¿äº†å¯æ‰©å±•çš„æ”¶é›†ï¼Œè€Œç¨€ç–ä½†ç»Ÿè®¡ä¸Šè¶³å¤Ÿçš„çœŸå®å€¼æ”¯æŒäº†ç¨³å¥çš„è®­ç»ƒã€‚å¯¹æœ€å…ˆè¿›çš„å•ç›®æ·±åº¦æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†ä¸¥é‡çš„è·¨æ•°æ®é›†æ³›åŒ–å¤±è´¥ï¼Œå¼ºè°ƒäº†ROVRåœ¨åœºæ™¯å¤šæ ·æ€§ã€åŠ¨æ€ç¯å¢ƒå’Œç¨€ç–çœŸå®å€¼æ–¹é¢æ‰€å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦ä¼°è®¡æ•°æ®é›†åœ¨å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€å’Œå¤æ‚ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨KITTIç­‰æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°çš„æ•°æ®é›†ä¸Šå´é¢ä¸´ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šROVRæ•°æ®é›†çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„åœºæ™¯é‡‡é›†ï¼Œæä¾›ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„è®­ç»ƒå¹³å°ï¼Œä»¥æé«˜æ·±åº¦ä¼°è®¡æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„é²æ£’æ€§ã€‚è®¾è®¡ä¸Šæ³¨é‡æ•æ‰ä¸åŒç¯å¢ƒå’Œå¤©æ°”æ¡ä»¶ä¸‹çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šROVRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ•°æ®æ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é‡‡é›†ä½¿ç”¨è½»é‡çº§çš„è®¾å¤‡å’Œæµç¨‹ï¼Œç¡®ä¿é«˜æ•ˆå’Œå¯æ‰©å±•æ€§ï¼›æ•°æ®æ ‡æ³¨åˆ™é‡‡ç”¨ç¨€ç–ä½†ç»Ÿè®¡ä¸Šæœ‰æ•ˆçš„çœŸå®å€¼ï¼Œä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šROVRçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„åœºæ™¯è®¾ç½®ï¼Œå°¤å…¶æ˜¯åœ¨æ˜¼å¤œå’Œä¸åŒå¤©æ°”æ¡ä»¶ä¸‹çš„è¦†ç›–ã€‚è¿™ä¸ç°æœ‰æ•°æ®é›†çš„å•ä¸€åœºæ™¯è®¾ç½®å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†é«˜åˆ†è¾¨ç‡çš„å›¾åƒé‡‡é›†è®¾å¤‡ï¼Œå¹¶ç»“åˆå¤šç§ç¯å¢ƒæ¡ä»¶è¿›è¡Œæ•°æ®è®°å½•ã€‚æ­¤å¤–ï¼Œç¨€ç–çš„çœŸå®å€¼è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒåœºæ™¯çš„å˜åŒ–ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒROVRèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒæ·±åº¦ä¼°è®¡æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„å•ç›®æ·±åº¦æ¨¡å‹åœ¨ROVRæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä½äºåœ¨KITTIä¸Šçš„è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºè·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›çš„ä¸¥é‡ä¸è¶³ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ROVRåœ¨æ·±åº¦ä¼°è®¡ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ROVRæ•°æ®é›†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡æä¾›å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼ŒROVRèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆå¼€å‘å‡ºæ›´å…·é²æ£’æ€§çš„æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œä»è€Œæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒROVRæœ‰æœ›æˆä¸ºæ·±åº¦å­¦ä¹ é¢†åŸŸçš„é‡è¦åŸºå‡†æ•°æ®é›†ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Depth estimation is a fundamental task for 3D scene understanding in autonomous driving, robotics, and augmented reality. Existing depth datasets, such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from limitations in diversity and scalability. As benchmark performance on these datasets approaches saturation, there is an increasing need for a new generation of large-scale, diverse, and cost-efficient datasets to support the era of foundation models and multi-modal learning. We present ROVR, a large-scale, diverse, and cost-efficient depth dataset designed to capture the complexity of real-world driving. ROVR comprises 200K high-resolution frames across highway, rural, and urban scenarios, spanning day/night and adverse weather conditions. A lightweight acquisition pipeline ensures scalable collection, while sparse but statistically sufficient ground truth supports robust training. Benchmarking with state-of-the-art monocular depth models reveals severe cross-dataset generalization failures: models achieving near-ceiling accuracy on KITTI degrade drastically on ROVR, and even when trained on ROVR, current methods fall short of saturation. These results highlight the unique challenges posed by ROVR-scene diversity, dynamic environments, and sparse ground truth, establishing it as a demanding new platform for advancing depth estimation and building models with stronger real-world robustness. Extensive ablation studies provide a more intuitive understanding of our dataset across different scenarios, lighting conditions, and generalized ability.

