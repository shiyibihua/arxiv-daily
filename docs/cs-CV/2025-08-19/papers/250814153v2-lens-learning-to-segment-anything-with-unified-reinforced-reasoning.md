---
layout: default
title: LENS: Learning to Segment Anything with Unified Reinforced Reasoning
---

# LENS: Learning to Segment Anything with Unified Reinforced Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14153" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14153v2</a>
  <a href="https://arxiv.org/pdf/2508.14153.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14153v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14153v2', 'LENS: Learning to Segment Anything with Unified Reinforced Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-11-18)

**å¤‡æ³¨**: Code is released at https://github.com/hustvl/LENS

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hustvl/LENS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLENSæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬æç¤ºå›¾åƒåˆ†å‰²ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒåˆ†å‰²` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†æœºåˆ¶` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒåˆ†å‰²æ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬æç¤ºæ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ¨ç†æœºåˆ¶ï¼Œå¯¼è‡´åœ¨æ–°åœºæ™¯ä¸­çš„è¡¨ç°ä¸ä½³ã€‚
2. LENSæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–æ¨ç†å’Œåˆ†å‰²è¿‡ç¨‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„å¥–åŠ±æœºåˆ¶æ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œåˆ†å‰²è´¨é‡ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLENSçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æ–‡æœ¬æç¤ºåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²å¯¹äºç»†ç²’åº¦è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨äººæœºäº¤äº’å’Œæœºå™¨äººé¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æµ‹è¯•æ—¶é€šå¸¸å¿½è§†æ˜¾å¼çš„æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†å…¶åœ¨æœªè§æç¤ºå’Œé¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LENSï¼Œä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°ä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œæ¶µç›–å¥å­ã€æ¡†å’Œåˆ†å‰²çº§åˆ«çš„çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„æ¨ç†ç†ç”±ï¼ŒåŒæ—¶æå‡æ©è†œè´¨é‡ã€‚ä½¿ç”¨ä¸€ä¸ªå…¬å¼€çš„30äº¿å‚æ•°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒLENSåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†ä¸Šå®ç°äº†81.2%çš„å¹³å‡cIoUï¼Œè¶…è¶Šäº†å¼ºå¤§çš„å¾®è°ƒæ–¹æ³•GLaMMï¼Œæå‡å¹…åº¦è¾¾åˆ°5.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬æç¤ºçš„åˆ†å‰²èƒ½åŠ›ï¼Œä¸ºæ›´å…·æ³›åŒ–èƒ½åŠ›çš„Segment Anythingæ¨¡å‹æä¾›äº†å®é™…è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬æç¤ºå›¾åƒåˆ†å‰²æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§æç¤ºå’Œé¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€çš„å¾®è°ƒç­–ç•¥ï¼Œç¼ºä¹åŠ¨æ€æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLENSæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–æ¨ç†å’Œåˆ†å‰²è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥ç»Ÿä¸€çš„å¥–åŠ±æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆåˆ†å‰²æ©è†œçš„åŒæ—¶ï¼Œè¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLENSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨ç†æ¨¡å—ã€åˆ†å‰²æ¨¡å—å’Œå¥–åŠ±æœºåˆ¶ã€‚æ¨ç†æ¨¡å—è´Ÿè´£ç”Ÿæˆæ¨ç†ç†ç”±ï¼Œåˆ†å‰²æ¨¡å—åˆ™ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²æ©è†œï¼Œè€Œå¥–åŠ±æœºåˆ¶åˆ™æ ¹æ®æ¨ç†å’Œåˆ†å‰²çš„è´¨é‡è¿›è¡Œåé¦ˆï¼ŒæŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šLENSçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œæ¶µç›–äº†å¥å­ã€æ¡†å’Œåˆ†å‰²çº§åˆ«çš„çº¿ç´¢ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆåˆ†å‰²æ©è†œçš„åŒæ—¶ï¼Œè¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œä»è€Œæ˜¾è‘—æå‡äº†åˆ†å‰²è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†30äº¿å‚æ•°çš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VL-3B-Instructï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†å’Œåˆ†å‰²ä»»åŠ¡ä¸­èƒ½å¤Ÿè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LENSåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†ä¸Šå®ç°äº†81.2%çš„å¹³å‡cIoUï¼Œç›¸è¾ƒäºå¾®è°ƒæ–¹æ³•GLaMMæå‡äº†5.6%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æœºåˆ¶æ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬æç¤ºåˆ†å‰²çš„æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LENSæ¡†æ¶åœ¨äººæœºäº¤äº’ã€æœºå™¨äººè§†è§‰å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡æ–‡æœ¬æç¤ºå›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒLENSèƒ½å¤Ÿä¸ºæ™ºèƒ½ç³»ç»Ÿæä¾›æ›´ä¸ºç²¾å‡†çš„è§†è§‰ç†è§£ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning significantly enhances text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models (SAM). Code is available at https://github.com/hustvl/LENS.

