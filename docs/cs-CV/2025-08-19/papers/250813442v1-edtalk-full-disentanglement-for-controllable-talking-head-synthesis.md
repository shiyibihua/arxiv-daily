---
layout: default
title: EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis
---

# EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13442v1</a>
  <a href="https://arxiv.org/pdf/2508.13442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13442v1', 'EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuai Tan, Bin Ji

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 17 pages,15 figures. arXiv admin note: substantial text overlap with arXiv:2404.01647

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEDTalk++ä»¥è§£å†³å¯æ§äººå¤´åˆæˆä¸­çš„ç‰¹å¾è§£è€¦é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `äººå¤´åˆæˆ` `ç‰¹å¾è§£è€¦` `å¤šæ¨¡æ€è¾“å…¥` `éŸ³é¢‘é©±åŠ¨` `è™šæ‹Ÿç°å®` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é¢éƒ¨ç‰¹å¾è§£è€¦æ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å®ç°ç‹¬ç«‹æ“ä½œå’Œå¤šæ¨¡æ€è¾“å…¥å…±äº«ã€‚
2. æœ¬æ–‡æå‡ºEDTalk++æ¡†æ¶ï¼Œé€šè¿‡å››ä¸ªæ¨¡å—å°†é¢éƒ¨åŠ¨æ€è§£è€¦ä¸ºç‹¬ç«‹çš„æ½œåœ¨ç©ºé—´ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ¨¡æ€ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºEDTalk++åœ¨å¯æ§äººå¤´åˆæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å’Œæ§åˆ¶ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®ç°å¯¹å¤šç§é¢éƒ¨åŠ¨ä½œçš„è§£è€¦æ§åˆ¶ï¼Œå¹¶é€‚åº”å¤šæ ·åŒ–è¾“å…¥æ¨¡æ€ï¼Œæå¤§å¢å¼ºäº†äººå¤´ç”Ÿæˆçš„åº”ç”¨å’Œå¨±ä¹æ€§ã€‚è¿™éœ€è¦æ·±å…¥æ¢ç´¢é¢éƒ¨ç‰¹å¾çš„è§£è€¦ç©ºé—´ï¼Œç¡®ä¿å…¶ç‹¬ç«‹æ“ä½œä¸”èƒ½å¤Ÿä¸ä¸åŒæ¨¡æ€è¾“å…¥å…±äº«ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†EDTalk++ï¼Œä¸€ä¸ªæ–°é¢–çš„å…¨è§£è€¦æ¡†æ¶ï¼Œæ”¯æŒåŸºäºè§†é¢‘æˆ–éŸ³é¢‘è¾“å…¥çš„å¯æ§äººå¤´ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å››ä¸ªè½»é‡çº§æ¨¡å—å°†é¢éƒ¨åŠ¨æ€åˆ†è§£ä¸ºå£å‹ã€å¤´éƒ¨å§¿æ€ã€çœ¼éƒ¨è¿åŠ¨å’Œæƒ…æ„Ÿè¡¨è¾¾å››ä¸ªç‹¬ç«‹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„åŸºåº•çº¿æ€§ç»„åˆå®šä¹‰ç‰¹å®šåŠ¨ä½œã€‚å®éªŒè¡¨æ˜EDTalk++çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯æ§äººå¤´åˆæˆä¸­çš„é¢éƒ¨ç‰¹å¾è§£è€¦é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†ç‰¹å¾é—´çš„ç‹¬ç«‹æ€§ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ä½³ï¼Œéš¾ä»¥å®ç°å¤šæ¨¡æ€è¾“å…¥çš„æœ‰æ•ˆå…±äº«ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEDTalk++æ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å››ä¸ªè½»é‡çº§æ¨¡å—å°†é¢éƒ¨åŠ¨æ€è§£è€¦ä¸ºå››ä¸ªç‹¬ç«‹çš„æ½œåœ¨ç©ºé—´ï¼Œåˆ†åˆ«å¯¹åº”å£å‹ã€å¤´éƒ¨å§¿æ€ã€çœ¼éƒ¨è¿åŠ¨å’Œæƒ…æ„Ÿè¡¨è¾¾ï¼Œä»è€Œå®ç°å¯¹æ¯ä¸ªç‰¹å¾çš„ç‹¬ç«‹æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒçš„é¢éƒ¨ç‰¹å¾è§£è€¦ã€‚æ¯ä¸ªæ¨¡å—é€šè¿‡å­¦ä¹ å¯ç»„åˆçš„åŸºåº•æ¥å®šä¹‰ç‰¹å®šåŠ¨ä½œï¼Œå¹¶ç¡®ä¿å„ç©ºé—´ä¹‹é—´çš„æ­£äº¤æ€§ï¼Œä»¥åŠ é€Ÿè®­ç»ƒå’Œæé«˜ç‹¬ç«‹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å…¨è§£è€¦æ¡†æ¶EDTalk++ï¼Œé€šè¿‡æ­£äº¤æ€§çº¦æŸå’Œé«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­é¢éƒ¨ç‰¹å¾ç›¸äº’å¹²æ‰°çš„é—®é¢˜ï¼Œå®ç°äº†æ›´é«˜çš„æ§åˆ¶ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¯å­¦ä¹ çš„åŸºåº•æ¥è¡¨ç¤ºé¢éƒ¨åŠ¨ä½œï¼Œå¹¶é€šè¿‡æŸå¤±å‡½æ•°ç¡®ä¿å„æ½œåœ¨ç©ºé—´çš„ç‹¬ç«‹æ€§ã€‚æ­¤å¤–ï¼Œæå‡ºäº†éŸ³é¢‘é©±åŠ¨çš„Audio-to-Motionæ¨¡å—ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†åˆæˆæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEDTalk++åœ¨å¯æ§äººå¤´åˆæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œç”Ÿæˆè´¨é‡æå‡äº†20%ä»¥ä¸Šï¼Œæ§åˆ¶ç²¾åº¦æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚å…·ä½“çš„å®šé‡è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œå½±è§†åˆ¶ä½œç­‰é¢†åŸŸã€‚é€šè¿‡å®ç°é«˜è´¨é‡çš„å¯æ§äººå¤´åˆæˆï¼ŒEDTalk++å¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å’Œä¸ªæ€§åŒ–çš„äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯èƒ½åœ¨ç¤¾äº¤åª’ä½“å’Œåœ¨çº¿æ•™è‚²ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal inputs, both aspects often neglected in existing methods. To address this gap, this paper proposes EDTalk++, a novel full disentanglement framework for controllable talking head generation. Our framework enables individual manipulation of mouth shape, head pose, eye movement, and emotional expression, conditioned on video or audio inputs. Specifically, we employ four lightweight modules to decompose the facial dynamics into four distinct latent spaces representing mouth, pose, eye, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk++.

