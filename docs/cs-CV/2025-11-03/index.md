---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-11-03
---

# cs.CVï¼ˆ2025-11-03ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251101501v1-se3-poseflow-estimating-6d-pose-distributions-for-uncertainty-aware-.html">SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation</a></td>
  <td>æå‡ºSE(3)-PoseFlowï¼Œç”¨äºä¼°è®¡6Dä½å§¿åˆ†å¸ƒï¼Œå®ç°ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æœºå™¨äººæ“ä½œ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01501v1" onclick="toggleFavorite(this, '2511.01501v1', 'SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251101210v2-omnivla-physically-grounded-multimodal-vla-with-unified-multi-sensor.html">OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation</a></td>
  <td>OmniVLAï¼šé¢å‘æœºå™¨äººæ“ä½œçš„ç‰©ç† grounding å¤šæ¨¡æ€ VLA æ¨¡å‹ï¼Œç»Ÿä¸€å¤šä¼ æ„Ÿå™¨æ„ŸçŸ¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01210v2" onclick="toggleFavorite(this, '2511.01210v2', 'OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251101833v2-tir-bench-a-comprehensive-benchmark-for-agentic-thinking-with-images.html">TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning</a></td>
  <td>æå‡ºTIR-Benchï¼Œç”¨äºè¯„ä¼°Agenticå›¾åƒæ¨ç†ä¸­æ¨¡å‹åˆ©ç”¨å·¥å…·è¿›è¡Œå›¾åƒå¤„ç†çš„èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01833v2" onclick="toggleFavorite(this, '2511.01833v2', 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251101571v1-pixelvla-advancing-pixel-level-understanding-in-vision-language-acti.html">PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</a></td>
  <td>PixelVLAï¼šé€šè¿‡åƒç´ çº§ç†è§£å’Œå¤šæ¨¡æ€æç¤ºï¼Œæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01571v1" onclick="toggleFavorite(this, '2511.01571v1', 'PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251101169v1-web-scale-collection-of-video-data-for-4d-animal-reconstruction.html">Web-Scale Collection of Video Data for 4D Animal Reconstruction</a></td>
  <td>æå‡ºAiMæ•°æ®é›†ä¸åŸºçº¿æ–¹æ³•ï¼Œç”¨äºé‡ç”Ÿç¯å¢ƒä¸‹çš„åŠ¨ç‰©4Dé‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01169v1" onclick="toggleFavorite(this, '2511.01169v1', 'Web-Scale Collection of Video Data for 4D Animal Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251105553v1-evlplearning-unified-embodied-vision-language-planner-with-reinforce.html">EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning</a></td>
  <td>æå‡ºEVLPï¼Œé€šè¿‡å¼ºåŒ–ç›‘ç£å¾®è°ƒå­¦ä¹ ç»Ÿä¸€å…·èº«è§†è§‰-è¯­è¨€è§„åˆ’å™¨ï¼Œè§£å†³é•¿ç¨‹æ“ä½œä»»åŠ¡ä¸­çš„å¤šæ¨¡æ€è§„åˆ’é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.05553v1" onclick="toggleFavorite(this, '2511.05553v1', 'EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251101317v2-a-generative-adversarial-approach-to-adversarial-attacks-guided-by-c.html">A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model</a></td>
  <td>æå‡ºåŸºäºå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹çš„ç”Ÿæˆå¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œæå‡æ”»å‡»æ•ˆæœä¸è§†è§‰ä¿çœŸåº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01317v2" onclick="toggleFavorite(this, '2511.01317v2', 'A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251101250v2-source-only-cross-weather-lidar-via-geometry-aware-point-drop.html">Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop</a></td>
  <td>æå‡ºå‡ ä½•æ„ŸçŸ¥ç‚¹ä¸¢å¼ƒé€‚é…å™¨ï¼Œæå‡LiDARåœ¨æ¶åŠ£å¤©æ°”ä¸‹çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01250v2" onclick="toggleFavorite(this, '2511.01250v2', 'Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/251101502v1-discriminately-treating-motion-components-evolves-joint-depth-and-eg.html">Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</a></td>
  <td>æå‡ºDiMoDEæ¡†æ¶ï¼Œé€šè¿‡åŒºåˆ†è¿åŠ¨åˆ†é‡æå‡æ·±åº¦å’Œè‡ªè¿åŠ¨è”åˆå­¦ä¹ æ•ˆæœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01502v1" onclick="toggleFavorite(this, '2511.01502v1', 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251101756v1-hgfrenet-hop-hybrid-graphfomer-for-3d-human-pose-estimation-with-tra.html">HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain</a></td>
  <td>æå‡ºHGFreNetï¼Œåˆ©ç”¨Hop-hybrid GraphFomerè§£å†³å•ç›®è§†é¢‘3Däººä½“å§¿æ€ä¼°è®¡ä¸­çš„è½¨è¿¹ä¸ä¸€è‡´é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01756v1" onclick="toggleFavorite(this, '2511.01756v1', 'HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251102065v1-opto-electronic-convolutional-neural-network-design-via-direct-kerne.html">Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization</a></td>
  <td>æå‡ºå…‰ç”µå·ç§¯ç¥ç»ç½‘ç»œä¸¤é˜¶æ®µè®¾è®¡ï¼Œé€šè¿‡ç›´æ¥æ ¸ä¼˜åŒ–æå‡å•ç›®æ·±åº¦ä¼°è®¡ç²¾åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.02065v1" onclick="toggleFavorite(this, '2511.02065v1', 'Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251101399v1-semantic-bim-enrichment-for-firefighting-assets-fire-art-dataset-and.html">Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction</a></td>
  <td>æå‡ºFire-ARTæ•°æ®é›†ï¼Œå¹¶è®¾è®¡åŸºäºå…¨æ™¯å›¾åƒçš„3Dé‡å»ºæ–¹æ³•ï¼Œç”¨äºæ¶ˆé˜²èµ„äº§çš„BIMè¯­ä¹‰å¢å¼ºã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01399v1" onclick="toggleFavorite(this, '2511.01399v1', 'Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251101237v1-eyes-on-target-gaze-aware-object-detection-in-egocentric-video.html">Eyes on Target: Gaze-Aware Object Detection in Egocentric Video</a></td>
  <td>Eyes on Targetï¼šæå‡ºæ·±åº¦æ„ŸçŸ¥å’Œæ³¨è§†å¼•å¯¼çš„ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘åˆ†æã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01237v1" onclick="toggleFavorite(this, '2511.01237v1', 'Eyes on Target: Gaze-Aware Object Detection in Egocentric Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251101315v1-mvsmamba-multi-view-stereo-with-state-space-model.html">MVSMamba: Multi-View Stereo with State Space Model</a></td>
  <td>MVSMambaï¼šåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹å®ç°é«˜æ•ˆå¤šè§†è§’ç«‹ä½“è§†è§‰é‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01315v1" onclick="toggleFavorite(this, '2511.01315v1', 'MVSMamba: Multi-View Stereo with State Space Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251101618v1-actial-activate-spatial-reasoning-ability-of-multimodal-large-langua.html">Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</a></td>
  <td>Actialï¼šé€šè¿‡è§†è§’å­¦ä¹ æ¿€æ´»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01618v1" onclick="toggleFavorite(this, '2511.01618v1', 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251101610v1-dino-mx-a-modular-flexible-framework-for-self-supervised-learning.html">DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning</a></td>
  <td>DINO-MXï¼šä¸€ä¸ªæ¨¡å—åŒ–è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶æå‡çµæ´»æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01610v1" onclick="toggleFavorite(this, '2511.01610v1', 'DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251101200v1-mosa-motion-generation-with-scalable-autoregressive-modeling.html">MoSa: Motion Generation with Scalable Autoregressive Modeling</a></td>
  <td>MoSaï¼šåŸºäºå¯æ‰©å±•è‡ªå›å½’å»ºæ¨¡çš„è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œæå‡æ–‡æœ¬é©±åŠ¨3Däººä½“è¿åŠ¨ç”Ÿæˆè´¨é‡ä¸æ•ˆç‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.01200v1" onclick="toggleFavorite(this, '2511.01200v1', 'MoSa: Motion Generation with Scalable Autoregressive Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)