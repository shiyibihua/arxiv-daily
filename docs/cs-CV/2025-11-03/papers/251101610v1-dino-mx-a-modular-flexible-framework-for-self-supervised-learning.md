---
layout: default
title: DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning
---

# DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning

**arXiv**: [2511.01610v1](https://arxiv.org/abs/2511.01610) | [PDF](https://arxiv.org/pdf/2511.01610.pdf)

**ä½œè€…**: Mahmut Selman Gokmen, Cody Bumgardner

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DINO-MXï¼šä¸€ä¸ªæ¨¡å—åŒ–è‡ªç›‘ç£å­¦ä¹ æ¡†æž¶ï¼Œé™ä½Žè®¡ç®—æˆæœ¬å¹¶æå‡çµæ´»æ€§ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è§†è§‰åŸºç¡€æ¨¡åž‹` `æ¨¡å—åŒ–æ¡†æž¶` `çŸ¥è¯†è’¸é¦` `åˆ†å¸ƒå¼è®­ç»ƒ` `Transformer` `è¡¨å¾å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæµç¨‹ç¼ºä¹çµæ´»æ€§ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸå’Œèµ„æºçŽ¯å¢ƒä¸‹çš„åº”ç”¨ã€‚
2. DINO-MXæ¡†æž¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œç»Ÿä¸€äº†DINOç³»åˆ—ç®—æ³•ï¼Œå¹¶æ”¯æŒå¤šç§è®­ç»ƒç­–ç•¥ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒDINO-MXåœ¨é™ä½Žè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒäº†å…·æœ‰ç«žäº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šæ€§å·¥å…·ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰åŸºç¡€æ¨¡åž‹(VFMs)é€šè¿‡è‡ªç›‘ç£æ–¹æ³•æ˜¾è‘—æå‡äº†è¡¨å¾å­¦ä¹ ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„è®­ç»ƒæµç¨‹é€šå¸¸ç¼ºä¹çµæ´»æ€§ï¼Œé¢†åŸŸé’ˆå¯¹æ€§å¼ºï¼Œæˆ–è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒé¢†åŸŸå’Œèµ„æºçŽ¯å¢ƒä¸‹çš„å¯ç”¨æ€§ã€‚DINO-MXæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„è®­ç»ƒæ¡†æž¶ï¼Œå®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€çš„é…ç½®é©±åŠ¨ç³»ç»Ÿä¸­ç»“åˆäº†DINOã€DINOv2å’ŒDINOv3çš„æ ¸å¿ƒåŽŸåˆ™ã€‚å®ƒæ”¯æŒå„ç§åŸºäºŽTransformerçš„æž¶æž„ï¼Œå¹¶ä¸”å®Œå…¨å…¼å®¹Hugging Faceç”Ÿæ€ç³»ç»Ÿã€‚è¯¥æ¡†æž¶åŒ…æ‹¬å¤šç§è®­ç»ƒç­–ç•¥ï¼Œå¦‚ä½Žç§©é€‚åº”(LoRA)ã€å±‚å†»ç»“å’ŒçŸ¥è¯†è’¸é¦ï¼Œä»¥åŠé€šè¿‡åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)å’Œå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ(FSDP)å¯¹åˆ†å¸ƒå¼è®­ç»ƒçš„æ”¯æŒã€‚DINO-MXæ—¨åœ¨å¤„ç†è‡ªç„¶å’Œä¸“é—¨çš„æ•°æ®ç±»åž‹ï¼ŒåŒ…æ‹¬å•é€šé“å’Œå¤šé€šé“å›¾åƒã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDINO-MXåœ¨æ˜¾è‘—é™ä½Žè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®žçŽ°äº†å…·æœ‰ç«žäº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†å¯è§£é‡Šæ€§å·¥å…·å’Œæ ‡ç­¾å¼•å¯¼çš„æ•°æ®å¢žå¼ºæ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–çš„æ£€æµ‹æˆ–åˆ†å‰²å¤´çš„æƒ…å†µä¸‹æ”¹è¿›åŸºäºŽæ³¨æ„åŠ›çš„å®šä½ã€‚DINO-MXä¸ºåœ¨å„ç§ç ”ç©¶å’Œå®žé™…åº”ç”¨ä¸­å¼€å‘ã€è°ƒæ•´å’ŒåŸºå‡†æµ‹è¯•è‡ªç›‘ç£è§†è§‰æ¨¡åž‹æä¾›äº†ä¸€ä¸ªå¯å¤çŽ°å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¦‚DINOç³»åˆ—ï¼Œè™½ç„¶æ•ˆæžœæ˜¾è‘—ï¼Œä½†è®­ç»ƒæµç¨‹é€šå¸¸è¾ƒä¸ºå›ºå®šï¼Œéš¾ä»¥çµæ´»è°ƒæ•´ä»¥é€‚åº”ä¸åŒé¢†åŸŸå’Œè®¡ç®—èµ„æºã€‚é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ä¹Ÿé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªæ›´åŠ çµæ´»ã€é«˜æ•ˆä¸”æ˜“äºŽæ‰©å±•çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æž¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDINO-MXçš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªæ¨¡å—åŒ–çš„è®­ç»ƒæ¡†æž¶ï¼Œå°†DINOã€DINOv2å’ŒDINOv3ç­‰ç®—æ³•çš„æ ¸å¿ƒç»„ä»¶è¿›è¡Œè§£è€¦ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„é…ç½®ç³»ç»Ÿè¿›è¡Œç®¡ç†ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªèº«éœ€æ±‚çµæ´»é€‰æ‹©å’Œç»„åˆä¸åŒçš„ç»„ä»¶ï¼Œä»Žè€Œå®šåˆ¶å‡ºæœ€é€‚åˆç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæµç¨‹ã€‚åŒæ—¶ï¼Œæ¡†æž¶æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥ï¼Œå¦‚LoRAã€å±‚å†»ç»“ç­‰ï¼Œä»¥é™ä½Žè®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDINO-MXçš„æ•´ä½“æž¶æž„æ˜¯ä¸€ä¸ªé…ç½®é©±åŠ¨çš„ç³»ç»Ÿï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡é…ç½®æ–‡ä»¶æŒ‡å®šè®­ç»ƒæµç¨‹çš„å„ä¸ªçŽ¯èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡åž‹é€‰æ‹©ã€ä¼˜åŒ–å™¨è®¾ç½®ã€è®­ç»ƒç­–ç•¥ç­‰ã€‚æ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼šæ•°æ®å¤„ç†æ¨¡å—ï¼ˆæ”¯æŒå•é€šé“å’Œå¤šé€šé“å›¾åƒï¼‰ã€æ¨¡åž‹æ¨¡å—ï¼ˆæ”¯æŒå¤šç§Transformeræž¶æž„ï¼‰ã€è®­ç»ƒç­–ç•¥æ¨¡å—ï¼ˆåŒ…æ‹¬LoRAã€å±‚å†»ç»“ã€çŸ¥è¯†è’¸é¦ç­‰ï¼‰ã€åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—ï¼ˆæ”¯æŒDDPå’ŒFSDPï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šDINO-MXçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„è®¾è®¡ã€‚å®ƒå°†DINOç³»åˆ—ç®—æ³•çš„æ ¸å¿ƒç»„ä»¶è¿›è¡Œè§£è€¦ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„é…ç½®ç³»ç»Ÿè¿›è¡Œç®¡ç†ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥çµæ´»å®šåˆ¶è®­ç»ƒæµç¨‹ã€‚æ­¤å¤–ï¼Œæ¡†æž¶è¿˜æä¾›äº†å¯è§£é‡Šæ€§å·¥å…·å’Œæ ‡ç­¾å¼•å¯¼çš„æ•°æ®å¢žå¼ºæ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–æ£€æµ‹æˆ–åˆ†å‰²å¤´çš„æƒ…å†µä¸‹æ”¹è¿›åŸºäºŽæ³¨æ„åŠ›çš„å®šä½ã€‚

**å…³é”®è®¾è®¡**ï¼šDINO-MXçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç»Ÿä¸€çš„é…ç½®æ–‡ä»¶ï¼Œç”¨äºŽç®¡ç†è®­ç»ƒæµç¨‹çš„å„ä¸ªçŽ¯èŠ‚ï¼›2) æ¨¡å—åŒ–çš„ç»„ä»¶è®¾è®¡ï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œå®šåˆ¶å’Œæ‰©å±•ï¼›3) å¯¹å¤šç§è®­ç»ƒç­–ç•¥çš„æ”¯æŒï¼Œå¦‚LoRAã€å±‚å†»ç»“ç­‰ï¼Œä»¥é™ä½Žè®¡ç®—æˆæœ¬ï¼›4) å¯¹åˆ†å¸ƒå¼è®­ç»ƒçš„æ”¯æŒï¼ŒåŒ…æ‹¬DDPå’ŒFSDPï¼›5) æ ‡ç­¾å¼•å¯¼çš„æ•°æ®å¢žå¼ºæ–¹æ³•ï¼Œç”¨äºŽæ”¹è¿›åŸºäºŽæ³¨æ„åŠ›çš„å®šä½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

DINO-MXåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«žäº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½Žäº†è®¡ç®—æˆæœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨DINO-MXè®­ç»ƒçš„æ¨¡åž‹åœ¨ä¿æŒç›¸ä¼¼æ€§èƒ½çš„åŒæ—¶ï¼Œå¯ä»¥å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³åŽŸæ¥çš„å‡ åˆ†ä¹‹ä¸€ã€‚æ­¤å¤–ï¼ŒDINO-MXæä¾›çš„å¯è§£é‡Šæ€§å·¥å…·å’Œæ ‡ç­¾å¼•å¯¼çš„æ•°æ®å¢žå¼ºæ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–æ£€æµ‹æˆ–åˆ†å‰²å¤´çš„æƒ…å†µä¸‹æ”¹è¿›åŸºäºŽæ³¨æ„åŠ›çš„å®šä½ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

DINO-MXæ¡†æž¶å¯å¹¿æ³›åº”ç”¨äºŽè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å„ç§è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ã€‚å®ƒå°¤å…¶é€‚ç”¨äºŽèµ„æºå—é™çš„çŽ¯å¢ƒï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼ç³»ç»Ÿã€‚è¯¥æ¡†æž¶çš„æ¨¡å—åŒ–è®¾è®¡å’Œå¯æ‰©å±•æ€§ä½¿å…¶èƒ½å¤Ÿè½»æ¾é€‚åº”æ–°çš„æ•°æ®é›†å’Œæ¨¡åž‹æž¶æž„ï¼Œä»Žè€ŒåŠ é€Ÿè‡ªç›‘ç£å­¦ä¹ ç®—æ³•çš„å¼€å‘å’Œéƒ¨ç½²ã€‚æœªæ¥ï¼ŒDINO-MXæœ‰æœ›æˆä¸ºè§†è§‰åŸºç¡€æ¨¡åž‹ç ”ç©¶çš„é‡è¦å·¥å…·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.

