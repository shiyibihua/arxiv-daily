---
layout: default
title: DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning
---

# DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01610" target="_blank" class="toolbar-btn">arXiv: 2511.01610v1</a>
    <a href="https://arxiv.org/pdf/2511.01610.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01610v1" 
            onclick="toggleFavorite(this, '2511.01610v1', 'DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Mahmut Selman Gokmen, Cody Bumgardner

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DINO-MXÔºö‰∏Ä‰∏™Ê®°ÂùóÂåñËá™ÁõëÁù£Â≠¶‰π†Ê°ÜÊû∂ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨Âπ∂ÊèêÂçáÁÅµÊ¥ªÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ëá™ÁõëÁù£Â≠¶‰π†` `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Ê®°ÂùóÂåñÊ°ÜÊû∂` `Áü•ËØÜËí∏È¶è` `ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ` `Transformer` `Ë°®ÂæÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËá™ÁõëÁù£Â≠¶‰π†ËÆ≠ÁªÉÊµÅÁ®ãÁº∫‰πèÁÅµÊ¥ªÊÄßÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏çÂêåÈ¢ÜÂüüÂíåËµÑÊ∫êÁéØÂ¢É‰∏ãÁöÑÂ∫îÁî®„ÄÇ
2. DINO-MXÊ°ÜÊû∂ÈÄöËøáÊ®°ÂùóÂåñËÆæËÆ°ÔºåÁªü‰∏Ä‰∫ÜDINOÁ≥ªÂàóÁÆóÊ≥ïÔºåÂπ∂ÊîØÊåÅÂ§öÁßçËÆ≠ÁªÉÁ≠ñÁï•ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDINO-MXÂú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåÂπ∂Êèê‰æõ‰∫ÜÂèØËß£ÈáäÊÄßÂ∑•ÂÖ∑„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÂü∫Á°ÄÊ®°Âûã(VFMs)ÈÄöËøáËá™ÁõëÁù£ÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜË°®ÂæÅÂ≠¶‰π†„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑËÆ≠ÁªÉÊµÅÁ®ãÈÄöÂ∏∏Áº∫‰πèÁÅµÊ¥ªÊÄßÔºåÈ¢ÜÂüüÈíàÂØπÊÄßÂº∫ÔºåÊàñËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰∏çÂêåÈ¢ÜÂüüÂíåËµÑÊ∫êÁéØÂ¢É‰∏ãÁöÑÂèØÁî®ÊÄß„ÄÇDINO-MXÊòØ‰∏Ä‰∏™Ê®°ÂùóÂåñÂíåÂèØÊâ©Â±ïÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂÆÉÂú®‰∏Ä‰∏™Áªü‰∏ÄÁöÑÈÖçÁΩÆÈ©±Âä®Á≥ªÁªü‰∏≠ÁªìÂêà‰∫ÜDINO„ÄÅDINOv2ÂíåDINOv3ÁöÑÊ†∏ÂøÉÂéüÂàô„ÄÇÂÆÉÊîØÊåÅÂêÑÁßçÂü∫‰∫éTransformerÁöÑÊû∂ÊûÑÔºåÂπ∂‰∏îÂÆåÂÖ®ÂÖºÂÆπHugging FaceÁîüÊÄÅÁ≥ªÁªü„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨Â§öÁßçËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂ¶Ç‰ΩéÁß©ÈÄÇÂ∫î(LoRA)„ÄÅÂ±ÇÂÜªÁªìÂíåÁü•ËØÜËí∏È¶èÔºå‰ª•ÂèäÈÄöËøáÂàÜÂ∏ÉÂºèÊï∞ÊçÆÂπ∂Ë°å(DDP)ÂíåÂÆåÂÖ®ÂàÜÁâáÊï∞ÊçÆÂπ∂Ë°å(FSDP)ÂØπÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁöÑÊîØÊåÅ„ÄÇDINO-MXÊó®Âú®Â§ÑÁêÜËá™ÁÑ∂Âíå‰∏ìÈó®ÁöÑÊï∞ÊçÆÁ±ªÂûãÔºåÂåÖÊã¨ÂçïÈÄöÈÅìÂíåÂ§öÈÄöÈÅìÂõæÂÉè„ÄÇÂú®‰∏çÂêåÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDINO-MXÂú®ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂÆÉËøòÊèê‰æõ‰∫ÜÂèØËß£ÈáäÊÄßÂ∑•ÂÖ∑ÂíåÊ†áÁ≠æÂºïÂØºÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÊ£ÄÊµãÊàñÂàÜÂâ≤Â§¥ÁöÑÊÉÖÂÜµ‰∏ãÊîπËøõÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂÆö‰Ωç„ÄÇDINO-MX‰∏∫Âú®ÂêÑÁßçÁ†îÁ©∂ÂíåÂÆûÈôÖÂ∫îÁî®‰∏≠ÂºÄÂèë„ÄÅË∞ÉÊï¥ÂíåÂü∫ÂáÜÊµãËØïËá™ÁõëÁù£ËßÜËßâÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØÂ§çÁé∞ÂíåÂèØÊâ©Â±ïÁöÑÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÂ¶ÇDINOÁ≥ªÂàóÔºåËôΩÁÑ∂ÊïàÊûúÊòæËëóÔºå‰ΩÜËÆ≠ÁªÉÊµÅÁ®ãÈÄöÂ∏∏ËæÉ‰∏∫Âõ∫ÂÆöÔºåÈöæ‰ª•ÁÅµÊ¥ªË∞ÉÊï¥‰ª•ÈÄÇÂ∫î‰∏çÂêåÈ¢ÜÂüüÂíåËÆ°ÁÆóËµÑÊ∫ê„ÄÇÈ´òÊòÇÁöÑËÆ°ÁÆóÊàêÊú¨‰πüÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏Ä‰∏™Êõ¥Âä†ÁÅµÊ¥ª„ÄÅÈ´òÊïà‰∏îÊòì‰∫éÊâ©Â±ïÁöÑËá™ÁõëÁù£Â≠¶‰π†Ê°ÜÊû∂„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDINO-MXÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Ê®°ÂùóÂåñÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂ∞ÜDINO„ÄÅDINOv2ÂíåDINOv3Á≠âÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÁªÑ‰ª∂ËøõË°åËß£ËÄ¶ÔºåÂπ∂ÈÄöËøáÁªü‰∏ÄÁöÑÈÖçÁΩÆÁ≥ªÁªüËøõË°åÁÆ°ÁêÜ„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆËá™Ë∫´ÈúÄÊ±ÇÁÅµÊ¥ªÈÄâÊã©ÂíåÁªÑÂêà‰∏çÂêåÁöÑÁªÑ‰ª∂Ôºå‰ªéËÄåÂÆöÂà∂Âá∫ÊúÄÈÄÇÂêàÁâπÂÆö‰ªªÂä°ÁöÑËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÂêåÊó∂ÔºåÊ°ÜÊû∂ÊîØÊåÅÂ§öÁßç‰ºòÂåñÁ≠ñÁï•ÔºåÂ¶ÇLoRA„ÄÅÂ±ÇÂÜªÁªìÁ≠âÔºå‰ª•Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDINO-MXÁöÑÊï¥‰ΩìÊû∂ÊûÑÊòØ‰∏Ä‰∏™ÈÖçÁΩÆÈ©±Âä®ÁöÑÁ≥ªÁªüÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáÈÖçÁΩÆÊñá‰ª∂ÊåáÂÆöËÆ≠ÁªÉÊµÅÁ®ãÁöÑÂêÑ‰∏™ÁéØËäÇÔºåÂåÖÊã¨Êï∞ÊçÆÂä†ËΩΩ„ÄÅÊ®°ÂûãÈÄâÊã©„ÄÅ‰ºòÂåñÂô®ËÆæÁΩÆ„ÄÅËÆ≠ÁªÉÁ≠ñÁï•Á≠â„ÄÇÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÊ®°ÂùóÔºöÊï∞ÊçÆÂ§ÑÁêÜÊ®°ÂùóÔºàÊîØÊåÅÂçïÈÄöÈÅìÂíåÂ§öÈÄöÈÅìÂõæÂÉèÔºâ„ÄÅÊ®°ÂûãÊ®°ÂùóÔºàÊîØÊåÅÂ§öÁßçTransformerÊû∂ÊûÑÔºâ„ÄÅËÆ≠ÁªÉÁ≠ñÁï•Ê®°ÂùóÔºàÂåÖÊã¨LoRA„ÄÅÂ±ÇÂÜªÁªì„ÄÅÁü•ËØÜËí∏È¶èÁ≠âÔºâ„ÄÅÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÊ®°ÂùóÔºàÊîØÊåÅDDPÂíåFSDPÔºâ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDINO-MXÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Ê®°ÂùóÂåñÂíåÂèØÊâ©Â±ïÁöÑËÆæËÆ°„ÄÇÂÆÉÂ∞ÜDINOÁ≥ªÂàóÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÁªÑ‰ª∂ËøõË°åËß£ËÄ¶ÔºåÂπ∂ÈÄöËøáÁªü‰∏ÄÁöÑÈÖçÁΩÆÁ≥ªÁªüËøõË°åÁÆ°ÁêÜÔºå‰ΩøÂæóÁî®Êà∑ÂèØ‰ª•ÁÅµÊ¥ªÂÆöÂà∂ËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊ≠§Â§ñÔºåÊ°ÜÊû∂ËøòÊèê‰æõ‰∫ÜÂèØËß£ÈáäÊÄßÂ∑•ÂÖ∑ÂíåÊ†áÁ≠æÂºïÂØºÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊ£ÄÊµãÊàñÂàÜÂâ≤Â§¥ÁöÑÊÉÖÂÜµ‰∏ãÊîπËøõÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂÆö‰Ωç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDINO-MXÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Áªü‰∏ÄÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÁî®‰∫éÁÆ°ÁêÜËÆ≠ÁªÉÊµÅÁ®ãÁöÑÂêÑ‰∏™ÁéØËäÇÔºõ2) Ê®°ÂùóÂåñÁöÑÁªÑ‰ª∂ËÆæËÆ°ÔºåÊñπ‰æøÁî®Êà∑ËøõË°åÂÆöÂà∂ÂíåÊâ©Â±ïÔºõ3) ÂØπÂ§öÁßçËÆ≠ÁªÉÁ≠ñÁï•ÁöÑÊîØÊåÅÔºåÂ¶ÇLoRA„ÄÅÂ±ÇÂÜªÁªìÁ≠âÔºå‰ª•Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨Ôºõ4) ÂØπÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁöÑÊîØÊåÅÔºåÂåÖÊã¨DDPÂíåFSDPÔºõ5) Ê†áÁ≠æÂºïÂØºÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÁî®‰∫éÊîπËøõÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂÆö‰Ωç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DINO-MXÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ‰æãÂ¶ÇÔºåÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏äÔºå‰ΩøÁî®DINO-MXËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®‰øùÊåÅÁõ∏‰ººÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂèØ‰ª•Â∞ÜËÆ≠ÁªÉÊó∂Èó¥Áº©Áü≠Ëá≥ÂéüÊù•ÁöÑÂá†ÂàÜ‰πã‰∏Ä„ÄÇÊ≠§Â§ñÔºåDINO-MXÊèê‰æõÁöÑÂèØËß£ÈáäÊÄßÂ∑•ÂÖ∑ÂíåÊ†áÁ≠æÂºïÂØºÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊ£ÄÊµãÊàñÂàÜÂâ≤Â§¥ÁöÑÊÉÖÂÜµ‰∏ãÊîπËøõÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂÆö‰ΩçÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DINO-MXÊ°ÜÊû∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÁöÑÂêÑÁßçËá™ÁõëÁù£Â≠¶‰π†‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèÂàÜÁ±ª„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅËØ≠‰πâÂàÜÂâ≤Á≠â„ÄÇÂÆÉÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢ÉÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§áÊàñÂµåÂÖ•ÂºèÁ≥ªÁªü„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ®°ÂùóÂåñËÆæËÆ°ÂíåÂèØÊâ©Â±ïÊÄß‰ΩøÂÖ∂ËÉΩÂ§üËΩªÊùæÈÄÇÂ∫îÊñ∞ÁöÑÊï∞ÊçÆÈõÜÂíåÊ®°ÂûãÊû∂ÊûÑÔºå‰ªéËÄåÂä†ÈÄüËá™ÁõëÁù£Â≠¶‰π†ÁÆóÊ≥ïÁöÑÂºÄÂèëÂíåÈÉ®ÁΩ≤„ÄÇÊú™Êù•ÔºåDINO-MXÊúâÊúõÊàê‰∏∫ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁ†îÁ©∂ÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.

