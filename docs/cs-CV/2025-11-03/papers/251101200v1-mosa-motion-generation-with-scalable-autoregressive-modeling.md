---
layout: default
title: MoSa: Motion Generation with Scalable Autoregressive Modeling
---

# MoSa: Motion Generation with Scalable Autoregressive Modeling

**arXiv**: [2511.01200v1](https://arxiv.org/abs/2511.01200) | [PDF](https://arxiv.org/pdf/2511.01200.pdf)

**ä½œè€…**: Mengyuan Liu, Sheng Yan, Yong Wang, Yingjie Li, Gui-Bin Bian, Hong Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://mosa-web.github.io/MoSa-web)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MoSaï¼šåŸºäºŽå¯æ‰©å±•è‡ªå›žå½’å»ºæ¨¡çš„è¿åŠ¨ç”Ÿæˆæ¡†æž¶ï¼Œæå‡æ–‡æœ¬é©±åŠ¨3Däººä½“è¿åŠ¨ç”Ÿæˆè´¨é‡ä¸Žæ•ˆçŽ‡ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ–‡æœ¬é©±åŠ¨è¿åŠ¨ç”Ÿæˆ` `3Däººä½“è¿åŠ¨` `å‘é‡é‡åŒ–` `è‡ªå›žå½’å»ºæ¨¡` `åˆ†å±‚ç”Ÿæˆ` `Transformer` `è¿åŠ¨ç¼–è¾‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–‡æœ¬é©±åŠ¨3Däººä½“è¿åŠ¨ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’ŒæŽ¨ç†æ•ˆçŽ‡ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥å…¼é¡¾ã€‚
2. MoSaé€šè¿‡åˆ†å±‚æ®‹å·®å‘é‡é‡åŒ–å’Œå¯æ‰©å±•è‡ªå›žå½’å»ºæ¨¡ï¼Œå®žçŽ°äº†ç²—åˆ°ç²¾çš„è¿åŠ¨ç”Ÿæˆï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆçŽ‡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMoSaåœ¨Motion-Xæ•°æ®é›†ä¸Šå–å¾—äº†SOTAç»“æžœï¼ŒFIDæ˜¾è‘—é™ä½Žï¼ŒæŽ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°è¿åŠ¨ç¼–è¾‘ä»»åŠ¡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMoSaçš„æ–°åž‹åˆ†å±‚è¿åŠ¨ç”Ÿæˆæ¡†æž¶ï¼Œç”¨äºŽæ–‡æœ¬é©±åŠ¨çš„3Däººä½“è¿åŠ¨ç”Ÿæˆã€‚MoSaé€šè¿‡ç²—åˆ°ç²¾çš„å¯æ‰©å±•ç”Ÿæˆè¿‡ç¨‹ï¼Œå¢žå¼ºäº†å‘é‡é‡åŒ–å¼•å¯¼çš„ç”ŸæˆTransformerï¼ˆVQ-GTï¼‰èŒƒå¼ã€‚MoSaé›†æˆäº†å¤šå°ºåº¦Tokenä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰åˆ°åˆ†å±‚æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆRQ-VAEï¼‰ä¸­ã€‚MTPSåœ¨æ¯ä¸ªåˆ†å±‚é‡åŒ–å±‚é‡‡ç”¨æ’å€¼ï¼Œæœ‰æ•ˆåœ°ä¿ç•™äº†ç²—åˆ°ç²¾çš„å¤šå°ºåº¦tokenã€‚ç”±æ­¤ï¼Œç”ŸæˆTransformeræ”¯æŒå¯æ‰©å±•è‡ªå›žå½’ï¼ˆSARï¼‰å»ºæ¨¡ï¼Œé¢„æµ‹å°ºåº¦tokenï¼Œè€Œéžä¼ ç»Ÿæ–¹æ³•ä¸­æ¯æ­¥ä»…é¢„æµ‹ä¸€ä¸ªtokenã€‚å› æ­¤ï¼ŒMoSaä»…éœ€10æ­¥æŽ¨ç†ï¼Œä¸ŽRQ-VAEé‡åŒ–å±‚æ•°åŒ¹é…ã€‚ä¸ºè§£å†³é¢‘ç¹æ’å€¼å¯èƒ½å¯¼è‡´çš„é‡å»ºé€€åŒ–é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºCAQ-VAEï¼Œä¸€ç§è½»é‡çº§ä½†å¯Œæœ‰è¡¨çŽ°åŠ›çš„å·ç§¯-æ³¨æ„åŠ›æ··åˆVQ-VAEã€‚CAQ-VAEå¢žå¼ºäº†æ®‹å·®å—è®¾è®¡ï¼Œå¹¶èžå…¥æ³¨æ„åŠ›æœºåˆ¶ä»¥æ›´å¥½åœ°æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒMoSaå®žçŽ°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆçŽ‡ï¼Œåœ¨ä¿çœŸåº¦å’Œé€Ÿåº¦æ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚åœ¨Motion-Xæ•°æ®é›†ä¸Šï¼ŒMoSaå®žçŽ°äº†0.06çš„FIDï¼ˆç›¸æ¯”MoMaskçš„0.20ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†27%çš„æŽ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼ŒMoSaå¯ä»¥å¾ˆå¥½åœ°æ³›åŒ–åˆ°è¿åŠ¨ç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ— éœ€é¢å¤–å¾®è°ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬é©±åŠ¨çš„3Däººä½“è¿åŠ¨ç”Ÿæˆé—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºŽVQ-GTçš„æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦è¾ƒå¤šçš„æŽ¨ç†æ­¥éª¤ï¼Œæ•ˆçŽ‡è¾ƒä½Žï¼Œå¹¶ä¸”éš¾ä»¥åœ¨ç”Ÿæˆè´¨é‡å’ŒæŽ¨ç†é€Ÿåº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚é¢‘ç¹çš„æ’å€¼æ“ä½œä¹Ÿå¯èƒ½å¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoSaçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†å±‚é‡åŒ–å’Œå¯æ‰©å±•è‡ªå›žå½’å»ºæ¨¡ï¼Œå®žçŽ°ç²—åˆ°ç²¾çš„è¿åŠ¨ç”Ÿæˆã€‚é€šè¿‡å¤šå°ºåº¦Tokenä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰ä¿ç•™ä¸åŒå°ºåº¦çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å¯æ‰©å±•è‡ªå›žå½’ï¼ˆSARï¼‰å»ºæ¨¡ä¸€æ¬¡æ€§é¢„æµ‹å¤šä¸ªå°ºåº¦çš„tokenï¼Œä»Žè€Œå‡å°‘æŽ¨ç†æ­¥éª¤ï¼Œæé«˜æ•ˆçŽ‡ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å·ç§¯-æ³¨æ„åŠ›æ··åˆVQ-VAEï¼ˆCAQ-VAEï¼‰æ¥æå‡é‡å»ºè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMoSaçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ï¼š1) åˆ†å±‚æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆRQ-VAEï¼‰ï¼Œç”¨äºŽå°†è¿åŠ¨æ•°æ®ç¼–ç ä¸ºç¦»æ•£çš„tokenåºåˆ—ï¼›2) å¤šå°ºåº¦Tokenä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰ï¼Œç”¨äºŽåœ¨åˆ†å±‚é‡åŒ–è¿‡ç¨‹ä¸­ä¿ç•™ä¸åŒå°ºåº¦çš„tokenï¼›3) å¯æ‰©å±•è‡ªå›žå½’ï¼ˆSARï¼‰å»ºæ¨¡çš„ç”ŸæˆTransformerï¼Œç”¨äºŽæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆè¿åŠ¨tokenåºåˆ—ï¼›4) å·ç§¯-æ³¨æ„åŠ›æ··åˆVQ-VAEï¼ˆCAQ-VAEï¼‰ï¼Œç”¨äºŽæå‡é‡å»ºè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoSaçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†å¤šå°ºåº¦Tokenä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰ï¼Œé€šè¿‡æ’å€¼ä¿ç•™ä¸åŒå°ºåº¦çš„è¿åŠ¨ä¿¡æ¯ï¼›2) æå‡ºäº†å¯æ‰©å±•è‡ªå›žå½’ï¼ˆSARï¼‰å»ºæ¨¡ï¼Œä¸€æ¬¡æ€§é¢„æµ‹å¤šä¸ªå°ºåº¦çš„tokenï¼Œå‡å°‘æŽ¨ç†æ­¥éª¤ï¼›3) æå‡ºäº†å·ç§¯-æ³¨æ„åŠ›æ··åˆVQ-VAEï¼ˆCAQ-VAEï¼‰ï¼Œé€šè¿‡å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶çš„ç»“åˆï¼Œæå‡é‡å»ºè´¨é‡ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMoSaèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šå°ºåº¦ä¿¡æ¯ï¼Œå¹¶å‡å°‘æŽ¨ç†æ­¥éª¤ï¼Œä»Žè€Œæé«˜ç”Ÿæˆè´¨é‡å’Œæ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMTPSåœ¨æ¯ä¸ªé‡åŒ–å±‚ä½¿ç”¨æ’å€¼æ¥ä¿ç•™tokenï¼Œæ’å€¼æƒé‡æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚SARå»ºæ¨¡çš„Transformerä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°é¢„æµ‹å¤šä¸ªå°ºåº¦çš„tokenã€‚CAQ-VAEåœ¨æ®‹å·®å—ä¸­å¼•å…¥äº†å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±ã€é‡åŒ–æŸå¤±å’ŒKLæ•£åº¦æŸå¤±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

MoSaåœ¨Motion-Xæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒFIDæŒ‡æ ‡ä»ŽMoMaskçš„0.20é™ä½Žåˆ°0.06ï¼ŒæŽ¨ç†æ—¶é—´å‡å°‘äº†27%ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMoSaåœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆçŽ‡æ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºŽè¿åŠ¨ç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

MoSaåœ¨è™šæ‹ŸçŽ°å®žã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€äººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°è‡ªåŠ¨ç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨ï¼Œä»Žè€Œé™ä½Žè¿åŠ¨ç”Ÿæˆæˆæœ¬ï¼Œæé«˜åˆ›ä½œæ•ˆçŽ‡ã€‚æ­¤å¤–ï¼ŒMoSaè¿˜å¯ä»¥ç”¨äºŽè¿åŠ¨ç¼–è¾‘ã€åŠ¨ä½œæ•æ‰æ•°æ®ä¿®å¤ç­‰ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at https://mosa-web.github.io/MoSa-web

