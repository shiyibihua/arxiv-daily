---
layout: default
title: OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation
---

# OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01210" target="_blank" class="toolbar-btn">arXiv: 2511.01210v2</a>
    <a href="https://arxiv.org/pdf/2511.01210.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01210v2" 
            onclick="toggleFavorite(this, '2511.01210v2', 'OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Heyu Guo, Shanmu Wang, Ruichun Ma, Shiqi Jiang, Yasaman Ghasempour, Omid Abari, Baining Guo, Lili Qiu

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03 (Êõ¥Êñ∞: 2025-11-06)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**OmniVLAÔºöÈù¢ÂêëÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁâ©ÁêÜ grounding Â§öÊ®°ÊÄÅ VLA Ê®°ÂûãÔºåÁªü‰∏ÄÂ§ö‰º†ÊÑüÂô®ÊÑüÁü•**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅËûçÂêà` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâËØ≠Ë®ÄÂä®‰Ωú` `‰º†ÊÑüÂô®ËûçÂêà` `Áâ©ÁêÜ grounding`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°Âûã‰∏ªË¶Å‰æùËµñRGBÁõ∏Êú∫ÔºåÊÑüÁü•ËÉΩÂäõÂèóÈôêÔºåÈöæ‰ª•Â∫îÂØπÂ§çÊùÇÊìç‰Ωú‰ªªÂä°„ÄÇ
2. OmniVLAÈÄöËøá‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉèÁªü‰∏ÄË°®Á§∫Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåÂÆûÁé∞Áâ©ÁêÜ grounding ÁöÑÁ©∫Èó¥Êô∫ËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåOmniVLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éRGB-onlyÂíåÂéüÂßã‰º†ÊÑüÂô®ËæìÂÖ•Ê®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú (VLA) Ê®°ÂûãÈÄöËøáÂ§ßËßÑÊ®°ËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÔºåÂú®Êú∫Âô®‰∫∫Âä®‰ΩúÈ¢ÑÊµãÊñπÈù¢Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°ÂûãÂ§ßÂ§ö‰ªÖ‰æùËµñ RGB Áõ∏Êú∫ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊÑüÁü•ÂíåÊìç‰ΩúËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü OmniVLAÔºå‰∏Ä‰∏™ÂÖ®Ê®°ÊÄÅ VLA Ê®°ÂûãÔºåÈõÜÊàê‰∫ÜÊñ∞ÁöÑ‰º†ÊÑüÊ®°ÊÄÅÔºå‰ª•ÂÆûÁé∞Ë∂ÖË∂ä RGB ÊÑüÁü•ÁöÑÁâ©ÁêÜ grounding Á©∫Èó¥Êô∫ËÉΩ„ÄÇÊàë‰ª¨ÁöÑÊ†∏ÂøÉÊñπÊ≥ïÊòØ‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉèÔºå‰∏ÄÁßçÁªü‰∏ÄÁöÑË°®Á§∫ÔºåÂ∞ÜÊù•Ëá™Á∫¢Â§ñÁõ∏Êú∫„ÄÅÊØ´Á±≥Ê≥¢Èõ∑ËææÂíåÈ∫¶ÂÖãÈ£éÈòµÂàóÁ≠â‰º†ÊÑüÂô®ÁöÑÁ©∫Èó¥ grounding ÂíåÁâ©ÁêÜÊÑè‰πâÁöÑÊé©Á†ÅÂè†Âä†Âà∞ RGB ÂõæÂÉè‰∏ä„ÄÇËøôÁßçÂõæÂÉèÂéüÁîüÁöÑÁªü‰∏Ä‰øùÊåÅ‰∫Ü‰º†ÊÑüÂô®ËæìÂÖ•Êé•Ëøë RGB ÁªüËÆ°Ôºå‰æø‰∫éËÆ≠ÁªÉÔºåÊèê‰æõ‰∫ÜË∑®‰º†ÊÑüÂô®Á°¨‰ª∂ÁöÑÁªü‰∏ÄÊé•Âè£ÔºåÂπ∂ÈÄöËøáËΩªÈáèÁ∫ßÁöÑÊØè‰º†ÊÑüÂô®ÊäïÂΩ±Âô®ÂÆûÁé∞Êï∞ÊçÆÈ´òÊïàÂ≠¶‰π†„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§ö‰º†ÊÑüÂô®ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÊû∂ÊûÑÔºåÂπ∂Âü∫‰∫é RGB È¢ÑËÆ≠ÁªÉÁöÑ VLA ‰∏ªÂπ≤ÁΩëÁªúËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Âú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ËØÑ‰º∞‰∫Ü OmniVLAÔºåÂÖ∂‰∏≠‰º†ÊÑüÂô®Ê®°ÊÄÅÊÑüÁü•ÊåáÂØºÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇOmniVLA ÂÆûÁé∞‰∫Ü 84% ÁöÑÂπ≥Âùá‰ªªÂä°ÊàêÂäüÁéáÔºåÊòæËëó‰ºò‰∫é‰ªÖ‰ΩøÁî® RGB ÂíåÂéüÂßã‰º†ÊÑüÂô®ËæìÂÖ•ÁöÑÂü∫Á∫øÊ®°ÂûãÔºåÂàÜÂà´ÊèêÈ´ò‰∫Ü 59% Âíå 28%ÔºåÂêåÊó∂ÊòæÁ§∫Âá∫Êõ¥È´òÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ôºå‰∏ªË¶Å‰æùËµñRGBÁõ∏Êú∫Ëé∑ÂèñËßÜËßâ‰ø°ÊÅØÔºåËøôÈôêÂà∂‰∫ÜÊ®°ÂûãÂØπÁéØÂ¢ÉÁöÑÊÑüÁü•ËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÖâÁÖßÂèòÂåñ„ÄÅÈÅÆÊå°Á≠âÊÉÖÂÜµ‰∏ã„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàËûçÂêàÂ§öÁßç‰º†ÊÑüÂô®‰ø°ÊÅØÔºåÂØºËá¥Ê®°ÂûãÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöOmniVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊù•Ëá™‰∏çÂêå‰º†ÊÑüÂô®ÁöÑ‰ø°ÊÅØÔºàÂ¶ÇÁ∫¢Â§ñÁõ∏Êú∫„ÄÅÊØ´Á±≥Ê≥¢Èõ∑Ëææ„ÄÅÈ∫¶ÂÖãÈ£éÈòµÂàóÔºâÁªü‰∏ÄË°®Á§∫‰∏∫‚Äú‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉè‚ÄùÔºåÂπ∂Âè†Âä†Âà∞RGBÂõæÂÉè‰∏ä„ÄÇËøôÁßçÊñπÊ≥ï‰øùÊåÅ‰∫Ü‰º†ÊÑüÂô®ËæìÂÖ•‰∏éRGBÂõæÂÉèÁöÑÁªüËÆ°Áõ∏‰ººÊÄßÔºå‰æø‰∫éÊ®°ÂûãËÆ≠ÁªÉÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑ‰º†ÊÑüÂô®Êé•Âè£„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOmniVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â§ö‰º†ÊÑüÂô®Êï∞ÊçÆÈááÈõÜÊ®°ÂùóÔºåË¥üË¥£ÈááÈõÜÊù•Ëá™‰∏çÂêå‰º†ÊÑüÂô®ÁöÑÂéüÂßãÊï∞ÊçÆÔºõ2) ‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉèÁîüÊàêÊ®°ÂùóÔºåÂ∞ÜÂéüÂßã‰º†ÊÑüÂô®Êï∞ÊçÆËΩ¨Êç¢‰∏∫Á©∫Èó¥ grounding ÁöÑÊé©Á†ÅÂõæÂÉèÔºõ3) Â§öÊ®°ÊÄÅËûçÂêàÊ®°ÂùóÔºåÂ∞Ü‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉèÂè†Âä†Âà∞RGBÂõæÂÉè‰∏äÔºåÂΩ¢ÊàêÁªü‰∏ÄÁöÑËæìÂÖ•Ë°®Á§∫Ôºõ4) VLAÊ®°ÂûãÔºåÂü∫‰∫éRGBÈ¢ÑËÆ≠ÁªÉÁöÑVLA‰∏ªÂπ≤ÁΩëÁªúÔºåÂØπÂ§öÊ®°ÊÄÅËæìÂÖ•ËøõË°åÂ§ÑÁêÜÔºåÈ¢ÑÊµãÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöOmniVLAÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫é‚Äú‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉè‚ÄùÁöÑÁªü‰∏ÄË°®Á§∫ÊñπÊ≥ï„ÄÇËøôÁßçÊñπÊ≥ïÂ∞Ü‰∏çÂêåÁ±ªÂûãÁöÑ‰º†ÊÑüÂô®Êï∞ÊçÆËΩ¨Êç¢‰∏∫ÂõæÂÉèÂΩ¢ÂºèÔºå‰ΩøÂÖ∂ËÉΩÂ§ü‰∏éRGBÂõæÂÉèËøõË°åÊúâÊïàËûçÂêàÔºåÂêåÊó∂‰øùÁïô‰∫Ü‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÁâ©ÁêÜÊÑè‰πâÂíåÁ©∫Èó¥‰ø°ÊÅØ„ÄÇ‰∏éÁõ¥Êé•‰ΩøÁî®ÂéüÂßã‰º†ÊÑüÂô®Êï∞ÊçÆÁõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÊõ¥Êòì‰∫éËÆ≠ÁªÉÔºåÂπ∂ÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöOmniVLAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËΩªÈáèÁ∫ßÁöÑÊØè‰º†ÊÑüÂô®ÊäïÂΩ±Âô®ÔºåÁî®‰∫éÂ∞ÜÂéüÂßã‰º†ÊÑüÂô®Êï∞ÊçÆËΩ¨Êç¢‰∏∫‰º†ÊÑüÂô®Êé©Á†ÅÂõæÂÉèÔºõ2) Âü∫‰∫éRGBÈ¢ÑËÆ≠ÁªÉÁöÑVLA‰∏ªÂπ≤ÁΩëÁªúÔºåÂà©Áî®Â§ßËßÑÊ®°RGBÂõæÂÉèÊï∞ÊçÆËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ3) ÈíàÂØπÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ËÆæËÆ°ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂä®‰ΩúÈ¢ÑÊµãÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

OmniVLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniVLAÁöÑÂπ≥Âùá‰ªªÂä°ÊàêÂäüÁéáËææÂà∞84%ÔºåÁõ∏ÊØî‰∫é‰ªÖ‰ΩøÁî®RGBÁöÑÂü∫Á∫øÊ®°ÂûãÊèêÈ´ò‰∫Ü59%ÔºåÁõ∏ÊØî‰∫é‰ΩøÁî®ÂéüÂßã‰º†ÊÑüÂô®ËæìÂÖ•ÁöÑÂü∫Á∫øÊ®°ÂûãÊèêÈ´ò‰∫Ü28%„ÄÇÂêåÊó∂ÔºåOmniVLAËøòË°®Áé∞Âá∫Êõ¥È´òÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

OmniVLAÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§öÊ®°ÊÄÅÊÑüÁü•ÂíåÁ≤æÁ°ÆÊìç‰ΩúÁöÑÊú∫Âô®‰∫∫Â∫îÁî®Âú∫ÊôØÔºåÂ¶ÇÊô∫ËÉΩÂà∂ÈÄ†„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫ÂíåËá™Âä®È©æÈ©∂Á≠â„ÄÇÈÄöËøáËûçÂêàÂ§öÁßç‰º†ÊÑüÂô®‰ø°ÊÅØÔºåOmniVLAËÉΩÂ§üÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÊìç‰ΩúÁ≤æÂ∫¶ÔºåÂÆûÁé∞Êõ¥ÂÆâÂÖ®„ÄÅÊõ¥È´òÊïàÁöÑËá™Âä®Âåñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.

