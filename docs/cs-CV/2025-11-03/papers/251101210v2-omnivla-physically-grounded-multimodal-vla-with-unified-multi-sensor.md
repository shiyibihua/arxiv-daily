---
layout: default
title: OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation
---

# OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation

**arXiv**: [2511.01210v2](https://arxiv.org/abs/2511.01210) | [PDF](https://arxiv.org/pdf/2511.01210.pdf)

**ä½œè€…**: Heyu Guo, Shanmu Wang, Ruichun Ma, Shiqi Jiang, Yasaman Ghasempour, Omid Abari, Baining Guo, Lili Qiu

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03 (æ›´æ–°: 2025-11-06)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniVLAï¼šé¢å‘æœºå™¨äººæ“ä½œçš„ç‰©ç† grounding å¤šæ¨¡æ€ VLA æ¨¡åž‹ï¼Œç»Ÿä¸€å¤šä¼ æ„Ÿå™¨æ„ŸçŸ¥**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èžåˆ` `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œ` `ä¼ æ„Ÿå™¨èžåˆ` `ç‰©ç† grounding`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹ä¸»è¦ä¾èµ–RGBç›¸æœºï¼Œæ„ŸçŸ¥èƒ½åŠ›å—é™ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚æ“ä½œä»»åŠ¡ã€‚
2. OmniVLAé€šè¿‡ä¼ æ„Ÿå™¨æŽ©ç å›¾åƒç»Ÿä¸€è¡¨ç¤ºå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå®žçŽ°ç‰©ç† grounding çš„ç©ºé—´æ™ºèƒ½ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒOmniVLAåœ¨çœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºŽRGB-onlyå’ŒåŽŸå§‹ä¼ æ„Ÿå™¨è¾“å…¥æ¨¡åž‹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡åž‹é€šè¿‡å¤§è§„æ¨¡è§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼Œåœ¨æœºå™¨äººåŠ¨ä½œé¢„æµ‹æ–¹é¢è¡¨çŽ°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰æ¨¡åž‹å¤§å¤šä»…ä¾èµ– RGB ç›¸æœºï¼Œé™åˆ¶äº†å…¶æ„ŸçŸ¥å’Œæ“ä½œèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº† OmniVLAï¼Œä¸€ä¸ªå…¨æ¨¡æ€ VLA æ¨¡åž‹ï¼Œé›†æˆäº†æ–°çš„ä¼ æ„Ÿæ¨¡æ€ï¼Œä»¥å®žçŽ°è¶…è¶Š RGB æ„ŸçŸ¥çš„ç‰©ç† grounding ç©ºé—´æ™ºèƒ½ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ–¹æ³•æ˜¯ä¼ æ„Ÿå™¨æŽ©ç å›¾åƒï¼Œä¸€ç§ç»Ÿä¸€çš„è¡¨ç¤ºï¼Œå°†æ¥è‡ªçº¢å¤–ç›¸æœºã€æ¯«ç±³æ³¢é›·è¾¾å’Œéº¦å…‹é£Žé˜µåˆ—ç­‰ä¼ æ„Ÿå™¨çš„ç©ºé—´ grounding å’Œç‰©ç†æ„ä¹‰çš„æŽ©ç å åŠ åˆ° RGB å›¾åƒä¸Šã€‚è¿™ç§å›¾åƒåŽŸç”Ÿçš„ç»Ÿä¸€ä¿æŒäº†ä¼ æ„Ÿå™¨è¾“å…¥æŽ¥è¿‘ RGB ç»Ÿè®¡ï¼Œä¾¿äºŽè®­ç»ƒï¼Œæä¾›äº†è·¨ä¼ æ„Ÿå™¨ç¡¬ä»¶çš„ç»Ÿä¸€æŽ¥å£ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„æ¯ä¼ æ„Ÿå™¨æŠ•å½±å™¨å®žçŽ°æ•°æ®é«˜æ•ˆå­¦ä¹ ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šä¼ æ„Ÿå™¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æž¶æž„ï¼Œå¹¶åŸºäºŽ RGB é¢„è®­ç»ƒçš„ VLA ä¸»å¹²ç½‘ç»œè®­ç»ƒæ¨¡åž‹ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®žä¸–ç•Œä»»åŠ¡ä¸­è¯„ä¼°äº† OmniVLAï¼Œå…¶ä¸­ä¼ æ„Ÿå™¨æ¨¡æ€æ„ŸçŸ¥æŒ‡å¯¼æœºå™¨äººæ“ä½œã€‚OmniVLA å®žçŽ°äº† 84% çš„å¹³å‡ä»»åŠ¡æˆåŠŸçŽ‡ï¼Œæ˜¾è‘—ä¼˜äºŽä»…ä½¿ç”¨ RGB å’ŒåŽŸå§‹ä¼ æ„Ÿå™¨è¾“å…¥çš„åŸºçº¿æ¨¡åž‹ï¼Œåˆ†åˆ«æé«˜äº† 59% å’Œ 28%ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ›´é«˜çš„å­¦ä¹ æ•ˆçŽ‡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¸»è¦ä¾èµ–RGBç›¸æœºèŽ·å–è§†è§‰ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†æ¨¡åž‹å¯¹çŽ¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å…‰ç…§å˜åŒ–ã€é®æŒ¡ç­‰æƒ…å†µä¸‹ã€‚çŽ°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèžåˆå¤šç§ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„ä¿¡æ¯ï¼ˆå¦‚çº¢å¤–ç›¸æœºã€æ¯«ç±³æ³¢é›·è¾¾ã€éº¦å…‹é£Žé˜µåˆ—ï¼‰ç»Ÿä¸€è¡¨ç¤ºä¸ºâ€œä¼ æ„Ÿå™¨æŽ©ç å›¾åƒâ€ï¼Œå¹¶å åŠ åˆ°RGBå›¾åƒä¸Šã€‚è¿™ç§æ–¹æ³•ä¿æŒäº†ä¼ æ„Ÿå™¨è¾“å…¥ä¸ŽRGBå›¾åƒçš„ç»Ÿè®¡ç›¸ä¼¼æ€§ï¼Œä¾¿äºŽæ¨¡åž‹è®­ç»ƒï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ä¼ æ„Ÿå™¨æŽ¥å£ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šOmniVLAçš„æ•´ä½“æž¶æž„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†æ¨¡å—ï¼Œè´Ÿè´£é‡‡é›†æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„åŽŸå§‹æ•°æ®ï¼›2) ä¼ æ„Ÿå™¨æŽ©ç å›¾åƒç”Ÿæˆæ¨¡å—ï¼Œå°†åŽŸå§‹ä¼ æ„Ÿå™¨æ•°æ®è½¬æ¢ä¸ºç©ºé—´ grounding çš„æŽ©ç å›¾åƒï¼›3) å¤šæ¨¡æ€èžåˆæ¨¡å—ï¼Œå°†ä¼ æ„Ÿå™¨æŽ©ç å›¾åƒå åŠ åˆ°RGBå›¾åƒä¸Šï¼Œå½¢æˆç»Ÿä¸€çš„è¾“å…¥è¡¨ç¤ºï¼›4) VLAæ¨¡åž‹ï¼ŒåŸºäºŽRGBé¢„è®­ç»ƒçš„VLAä¸»å¹²ç½‘ç»œï¼Œå¯¹å¤šæ¨¡æ€è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œé¢„æµ‹æœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniVLAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽâ€œä¼ æ„Ÿå™¨æŽ©ç å›¾åƒâ€çš„ç»Ÿä¸€è¡¨ç¤ºæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°†ä¸åŒç±»åž‹çš„ä¼ æ„Ÿå™¨æ•°æ®è½¬æ¢ä¸ºå›¾åƒå½¢å¼ï¼Œä½¿å…¶èƒ½å¤Ÿä¸ŽRGBå›¾åƒè¿›è¡Œæœ‰æ•ˆèžåˆï¼ŒåŒæ—¶ä¿ç•™äº†ä¼ æ„Ÿå™¨æ•°æ®çš„ç‰©ç†æ„ä¹‰å’Œç©ºé—´ä¿¡æ¯ã€‚ä¸Žç›´æŽ¥ä½¿ç”¨åŽŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ›´æ˜“äºŽè®­ç»ƒï¼Œå¹¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šOmniVLAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è½»é‡çº§çš„æ¯ä¼ æ„Ÿå™¨æŠ•å½±å™¨ï¼Œç”¨äºŽå°†åŽŸå§‹ä¼ æ„Ÿå™¨æ•°æ®è½¬æ¢ä¸ºä¼ æ„Ÿå™¨æŽ©ç å›¾åƒï¼›2) åŸºäºŽRGBé¢„è®­ç»ƒçš„VLAä¸»å¹²ç½‘ç»œï¼Œåˆ©ç”¨å¤§è§„æ¨¡RGBå›¾åƒæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼›3) é’ˆå¯¹æœºå™¨äººæ“ä½œä»»åŠ¡è®¾è®¡çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºŽä¼˜åŒ–æ¨¡åž‹çš„åŠ¨ä½œé¢„æµ‹æ€§èƒ½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

OmniVLAåœ¨çœŸå®žæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒOmniVLAçš„å¹³å‡ä»»åŠ¡æˆåŠŸçŽ‡è¾¾åˆ°84%ï¼Œç›¸æ¯”äºŽä»…ä½¿ç”¨RGBçš„åŸºçº¿æ¨¡åž‹æé«˜äº†59%ï¼Œç›¸æ¯”äºŽä½¿ç”¨åŽŸå§‹ä¼ æ„Ÿå™¨è¾“å…¥çš„åŸºçº¿æ¨¡åž‹æé«˜äº†28%ã€‚åŒæ—¶ï¼ŒOmniVLAè¿˜è¡¨çŽ°å‡ºæ›´é«˜çš„å­¦ä¹ æ•ˆçŽ‡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

OmniVLAå¯åº”ç”¨äºŽå„ç§éœ€è¦å¤šæ¨¡æ€æ„ŸçŸ¥å’Œç²¾ç¡®æ“ä½œçš„æœºå™¨äººåº”ç”¨åœºæ™¯ï¼Œå¦‚æ™ºèƒ½åˆ¶é€ ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡èžåˆå¤šç§ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼ŒOmniVLAèƒ½å¤Ÿæé«˜æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ“ä½œç²¾åº¦ï¼Œå®žçŽ°æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.

