---
layout: default
title: Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models
---

# Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01618" target="_blank" class="toolbar-btn">arXiv: 2511.01618v1</a>
    <a href="https://arxiv.org/pdf/2511.01618.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01618v1" 
            onclick="toggleFavorite(this, '2511.01618v1', 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ActialÔºöÈÄöËøáËßÜËßíÂ≠¶‰π†ÊøÄÊ¥ªÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Á©∫Èó¥Êé®ÁêÜ` `ËßÜËßíÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Êï∞ÊçÆÈõÜ` `Ë∑®ËßÜËßí‰∏ÄËá¥ÊÄß` `3DÂú∫ÊôØÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÂú®3DÊé®ÁêÜ‰∏≠Áº∫‰πèÊúâÊïàÁöÑÁ©∫Èó¥‰ø°ÊÅØÊçïÊçâÔºåÂ∞§ÂÖ∂Âú®Ë∑®ËßÜËßí‰∏ÄËá¥ÊÄßÊñπÈù¢Â≠òÂú®ÊåëÊàò„ÄÇ
2. ÊèêÂá∫ËßÜËßíÂ≠¶‰π†‰ªªÂä°ÂíåViewpoint-100KÊï∞ÊçÆÈõÜÔºåÈÄöËøá‰∏§Èò∂ÊÆµÂæÆË∞ÉÁ≠ñÁï•ÊèêÂçáÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëóÊøÄÊ¥ªMLLMÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÊèêÂçá‰∫ÜÂüüÂÜÖÂíåÂüüÂ§ñÊé®ÁêÜ‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)Âú®2DËßÜËßâÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÊøÄÂèë‰∫Ü‰∫∫‰ª¨Â∞ÜÂÖ∂Â∫îÁî®‰∫éÂ§çÊùÇ3DÊé®ÁêÜ‰ªªÂä°ÁöÑÂÖ¥Ë∂£„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÊòØÂê¶ËÉΩÊúâÊïàÊçïÊçâÂà∞Á®≥ÂÅ•ÁöÑÁúüÂÆû‰∏ñÁïåÊÄßËÉΩÊâÄÈúÄÁöÑËØ¶ÁªÜÁ©∫Èó¥‰ø°ÊÅØÔºåÂ∞§ÂÖ∂ÊòØÂú®Á≤æÁ°Æ3DÊé®ÁêÜÁöÑÂÖ≥ÈîÆË¶ÅÊ±Ç‚Äî‚ÄîË∑®ËßÜËßí‰∏ÄËá¥ÊÄßÊñπÈù¢Ôºå‰ªçÁÑ∂‰∏çÊ∏ÖÊ•ö„ÄÇËÄÉËôëÂà∞Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜËßÜËßíÂ≠¶‰π†ÔºåËøôÊòØ‰∏ÄÈ°πÊó®Âú®ËØÑ‰º∞ÂíåÊèêÈ´òMLLMÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜViewpoint-100KÊï∞ÊçÆÈõÜÔºåÂåÖÂê´10‰∏á‰∏™‰ª•ÂØπË±°‰∏∫‰∏≠ÂøÉÁöÑÂõæÂÉèÂØπÔºåÂÖ∑Êúâ‰∏çÂêåÁöÑËßÜËßíÂíåÁõ∏Â∫îÁöÑÈóÆÁ≠îÂØπ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈááÁî®‰∏§Èò∂ÊÆµÂæÆË∞ÉÁ≠ñÁï•ÔºöÈ¶ñÂÖàÔºåÈÄöËøáÂú®Viewpoint-100K‰∏äËøõË°åÁõëÁù£ÂæÆË∞É(SFT)ÔºåÂ∞ÜÂü∫Á°ÄÁü•ËØÜÊ≥®ÂÖ•Âà∞Âü∫Á∫øMLLM‰∏≠Ôºå‰ªéËÄåÂú®Â§ö‰∏™‰ªªÂä°‰∏≠Ëé∑ÂæóÊòæËëóÊîπËøõÔºõÂÖ∂Ê¨°ÔºåÈÄöËøáÂú®Êõ¥ÂπøÊ≥õÁöÑÈóÆÈ¢òÈõÜ‰∏ä‰ΩøÁî®Áæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñ(GRPO)ÁÆóÊ≥ïËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåÂ¢ûÂº∫Ê≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÂÜ∑ÂêØÂä®ÂàùÂßãÂåñÊñπÊ≥ïÔºåÊó®Âú®ÂêåÊó∂Â≠¶‰π†ËßÜËßíË°®Á§∫Âπ∂‰øùÊåÅËøûË¥ØÁöÑÊé®ÁêÜÊÄùÁª¥„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòæËëóÊøÄÊ¥ª‰∫ÜMLLMÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÊèêÈ´ò‰∫ÜÂú®ÂüüÂÜÖÂíåÂüüÂ§ñÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÂèëÁé∞Âº∫Ë∞É‰∫ÜÂú®MLLM‰∏≠ÂèëÂ±ïÂü∫Á°ÄÁ©∫Èó¥ÊäÄËÉΩÁöÑ‰ª∑ÂÄºÔºåÊîØÊåÅÊú∫Âô®‰∫∫„ÄÅËá™‰∏ªÁ≥ªÁªüÂíå3DÂú∫ÊôØÁêÜËß£ÊñπÈù¢ÁöÑÊú™Êù•ËøõÂ±ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈúÄË¶ÅÁ≤æÁªÜÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°Êó∂Ë°®Áé∞‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁêÜËß£‰∏çÂêåËßÜËßí‰∏ãÁöÑÁâ©‰ΩìÂÖ≥Á≥ªÂíå‰øùÊåÅË∑®ËßÜËßí‰∏ÄËá¥ÊÄßÊñπÈù¢Â≠òÂú®Âõ∞Èöæ„ÄÇËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂Á≠âÈúÄË¶ÅÁ≤æÁ°Æ3DÂú∫ÊôØÁêÜËß£È¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÊçïÊçâÂíåÂà©Áî®ÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥‰ø°ÊÅØÔºåÂØºËá¥Êé®ÁêÜÁªìÊûú‰∏çÂáÜÁ°Æ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‚ÄúËßÜËßíÂ≠¶‰π†‚ÄùÊù•ÊèêÂçáMLLMÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈÄöËøáÊûÑÂª∫ÂåÖÂê´Â§ßÈáè‰∏çÂêåËßÜËßíÂõæÂÉèÂØπÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ËÆæËÆ°Áõ∏Â∫îÁöÑÈóÆÁ≠î‰ªªÂä°ÔºåËÆ©Ê®°ÂûãÂ≠¶‰π†‰ªé‰∏çÂêåËßÜËßíËßÇÂØüÂêå‰∏ÄÁâ©‰ΩìÊó∂Â∫îËØ•ÂÖ∑Â§áÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®ËÆ©Ê®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊé®ÁêÜ3DÁ©∫Èó¥‰∏≠ÁöÑÁâ©‰ΩìÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) ÁõëÁù£ÂæÆË∞É(SFT)ÔºöÂú®Viewpoint-100KÊï∞ÊçÆÈõÜ‰∏äÔºå‰ΩøÁî®ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÂºèÂØπMLLMËøõË°åÂæÆË∞ÉÔºåÊ≥®ÂÖ•Âü∫Á°ÄÁöÑÁ©∫Èó¥Áü•ËØÜ„ÄÇ2) Âº∫ÂåñÂ≠¶‰π†Ôºö‰ΩøÁî®Áæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñ(GRPO)ÁÆóÊ≥ïÔºåÂú®Êõ¥ÂπøÊ≥õÁöÑÈóÆÈ¢òÈõÜ‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËøò‰ΩøÁî®‰∫ÜÊ∑∑ÂêàÂÜ∑ÂêØÂä®ÂàùÂßãÂåñÊñπÊ≥ïÔºå‰ª•ÂêåÊó∂Â≠¶‰π†ËßÜËßíË°®Á§∫Âπ∂‰øùÊåÅÊé®ÁêÜËøûË¥ØÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‚ÄúËßÜËßíÂ≠¶‰π†‚ÄùËøô‰∏ÄÊ¶ÇÂøµÔºåÂπ∂ËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑViewpoint-100KÊï∞ÊçÆÈõÜÂíå‰∏§Èò∂ÊÆµÂæÆË∞ÉÁ≠ñÁï•„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊúâÊïàÂú∞ÊøÄÊ¥ª‰∫ÜMLLMÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇÊ∑∑ÂêàÂÜ∑ÂêØÂä®ÂàùÂßãÂåñÊñπÊ≥ï‰πüÊòØ‰∏Ä‰∏™ÂàõÊñ∞ÁÇπÔºåÂÆÉËÉΩÂ§üÂêåÊó∂Â≠¶‰π†ËßÜËßíË°®Á§∫Âπ∂‰øùÊåÅÊé®ÁêÜÁöÑËøûË¥ØÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöViewpoint-100KÊï∞ÊçÆÈõÜÂåÖÂê´10‰∏á‰∏™‰ª•ÂØπË±°‰∏∫‰∏≠ÂøÉÁöÑÂõæÂÉèÂØπÔºåÊØè‰∏™ÂõæÂÉèÂØπÂåÖÂê´‰∏çÂêåËßÜËßíÁöÑÂêå‰∏ÄÁâ©‰ΩìÂõæÂÉèÔºåÂπ∂ÈÖçÊúâÁõ∏Â∫îÁöÑÈóÆÁ≠îÂØπ„ÄÇ‰∏§Èò∂ÊÆµÂæÆË∞ÉÁ≠ñÁï•‰∏≠ÔºåSFTÈò∂ÊÆµ‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÔºåGRPOÈò∂ÊÆµ‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†Â•ñÂä±ÂáΩÊï∞„ÄÇÊ∑∑ÂêàÂÜ∑ÂêØÂä®ÂàùÂßãÂåñÊñπÊ≥ïÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊèêÂá∫ÁöÑÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜMLLMÂú®Á©∫Èó¥Êé®ÁêÜ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇÂú®Viewpoint-100KÊï∞ÊçÆÈõÜ‰∏äÔºåÊÄßËÉΩÊèêÂçáÊòæËëó„ÄÇÂêåÊó∂ÔºåËØ•ÊñπÊ≥ïÂú®ÂüüÂ§ñÊé®ÁêÜ‰ªªÂä°‰∏ä‰πüË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Âú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÁöÑÂ±ïÁ§∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅ3DÂú∫ÊôØÁêÜËß£Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáMLLMÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩÁöÑÂØºËà™ÂíåÊìç‰Ωú„ÄÇÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÔºåÂèØ‰ª•ÊèêÈ´òËΩ¶ËæÜÂØπÂ§çÊùÇ‰∫§ÈÄöÂú∫ÊôØÁöÑÁêÜËß£ÂíåÂà§Êñ≠ËÉΩÂäõÔºå‰ªéËÄåÊèêÂçáÂÆâÂÖ®ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÊèê‰æõÊõ¥ÈÄºÁúüÁöÑÁî®Êà∑‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.

