---
layout: default
title: TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning
---

# TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01833" target="_blank" class="toolbar-btn">arXiv: 2511.01833v2</a>
    <a href="https://arxiv.org/pdf/2511.01833.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01833v2" 
            onclick="toggleFavorite(this, '2511.01833v2', 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ming Li, Jike Zhong, Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Yuxiang Lai, Chen Wei, Konstantinos Psounis, Kaipeng Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03 (Êõ¥Êñ∞: 2025-11-05)

**Â§áÊ≥®**: Preprint

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TIR-BenchÔºåÁî®‰∫éËØÑ‰º∞AgenticÂõæÂÉèÊé®ÁêÜ‰∏≠Ê®°ÂûãÂà©Áî®Â∑•ÂÖ∑ËøõË°åÂõæÂÉèÂ§ÑÁêÜÁöÑËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `AgenticÂõæÂÉèÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâÊé®ÁêÜ` `Âü∫ÂáÜÊµãËØï` `Â∑•ÂÖ∑‰ΩøÁî®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâÊé®ÁêÜÂü∫ÂáÜÊµãËØïÊó†Ê≥ïÂÖÖÂàÜËØÑ‰º∞Ê®°ÂûãÂà©Áî®Â∑•ÂÖ∑ËøõË°åÂ§çÊùÇÂõæÂÉèÂ§ÑÁêÜÂíåÊìç‰ΩúÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊÄùÁª¥Èìæ‰∏≠„ÄÇ
2. TIR-BenchÈÄöËøá13‰∏™ÈúÄË¶ÅÊñ∞ÂûãÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑÂõæÂÉèÂ§ÑÁêÜ‰ªªÂä°ÔºåÂÖ®Èù¢ËØÑ‰º∞AgenticÂõæÂÉèÊé®ÁêÜËÉΩÂäõÔºåÂº•Ë°•‰∫ÜÁé∞ÊúâÂü∫ÂáÜÁöÑ‰∏çË∂≥„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTIR-BenchÂØπÁé∞ÊúâÊ®°ÂûãÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÂπ∂ÂàùÊ≠•Á†îÁ©∂‰∫ÜÁõ¥Êé•ÂæÆË∞É‰∏éAgenticÂæÆË∞ÉÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÊé®ÁêÜÁöÑÂâçÊ≤øÊ≠£Âú®ËΩ¨ÂêëÂÉèOpenAI o3ËøôÊ†∑ÁöÑÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÂèØ‰ª•Êô∫ËÉΩÂú∞ÂàõÂª∫ÂíåÊìç‰ΩúÂ∑•ÂÖ∑Êù•ËΩ¨Êç¢ÂõæÂÉè‰ª•Ëß£ÂÜ≥ÈóÆÈ¢òÔºåÂç≥Âú®ÊÄùÁª¥Èìæ‰∏≠ËøõË°å‚ÄúÂõæÂÉèÊÄùËÄÉ‚Äù„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÊú™ËÉΩÂÆåÂÖ®ÊçïÊçâÂà∞ËøôÁßçÈ´òÁ∫ßËÉΩÂäõ„ÄÇÂç≥‰ΩøÊòØËßÜËßâÊêúÁ¥¢Ôºå‰Ωú‰∏∫ÂΩìÂâç‚ÄúÂõæÂÉèÊÄùËÄÉ‚ÄùÊñπÊ≥ïÊúÄÂ∏∏ËßÅÁöÑÂü∫ÂáÜÊµãËØïÔºå‰πüÂè™ÊµãËØï‰∫ÜËØ∏Â¶ÇÂÆö‰ΩçÂíåË£ÅÂâ™Á≠âÂü∫Êú¨Êìç‰ΩúÔºåÂØπ‰∫éÊõ¥Â§çÊùÇ„ÄÅÂä®ÊÄÅÂíå‰æùËµñ‰∫éÂ∑•ÂÖ∑ÁöÑÊé®ÁêÜÂá†‰πéÊ≤°ÊúâÊèê‰æõ‰ªª‰ΩïËßÅËß£„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜTIR-BenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÔºåÁî®‰∫éËØÑ‰º∞AgenticÂõæÂÉèÊé®ÁêÜÔºåÊ∂µÁõñ13‰∏™‰∏çÂêåÁöÑ‰ªªÂä°ÔºåÊØè‰∏™‰ªªÂä°ÈÉΩÈúÄË¶Å‰ΩøÁî®Êñ∞ÁöÑÂ∑•ÂÖ∑Âú®ÊÄùÁª¥Èìæ‰∏≠ËøõË°åÂõæÂÉèÂ§ÑÁêÜÂíåÊìç‰Ωú„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫Ü22‰∏™Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºå‰ªéÈ¢ÜÂÖàÁöÑÂºÄÊ∫êÂíå‰∏ìÊúâÊ®°ÂûãÂà∞ÈÇ£‰∫õÂÖ∑ÊúâÊòæÂºèÂ∑•ÂÖ∑‰ΩøÁî®Â¢ûÂº∫ÁöÑÊ®°Âûã„ÄÇÁªìÊûúË°®ÊòéÔºåTIR-BenchÂÖ∑ÊúâÊôÆÈÅçÁöÑÊåëÊàòÊÄßÔºåÂπ∂‰∏îÂº∫Â§ßÁöÑÊÄßËÉΩÈúÄË¶ÅÁúüÊ≠£ÁöÑÂõæÂÉèÊÄùËÄÉËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÊØîËæÉÁõ¥Êé•ÂæÆË∞É‰∏éAgenticÂæÆË∞ÉÁöÑÂàùÊ≠•Á†îÁ©∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâÊé®ÁêÜÂü∫ÂáÜÔºåÂ¶ÇËßÜËßâÊêúÁ¥¢Ôºå‰∏ªË¶ÅÊµãËØïÂÆö‰ΩçÂíåË£ÅÂâ™Á≠âÂü∫Êú¨Êìç‰ΩúÔºåÊó†Ê≥ïÂÖÖÂàÜËØÑ‰º∞Ê®°ÂûãÂú®Â§çÊùÇ„ÄÅÂä®ÊÄÅÂíåÂ∑•ÂÖ∑‰æùËµñÂú∫ÊôØ‰∏ãÁöÑ‚ÄúÂõæÂÉèÊÄùËÄÉ‚ÄùËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Ê®°Êãü‰∫∫Á±ªÂà©Áî®Â∑•ÂÖ∑ËøõË°åÂõæÂÉèÂ§ÑÁêÜÂíåÊìç‰Ωú‰ª•Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑËøáÁ®ãÔºåÁº∫‰πèÂØπAgenticÂõæÂÉèÊé®ÁêÜËÉΩÂäõÁöÑÊúâÊïàËØÑ‰º∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTIR-BenchÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´Â§öÊ†∑Âåñ‰ªªÂä°ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊØè‰∏™‰ªªÂä°ÈÉΩÈúÄË¶ÅÊ®°ÂûãÂà©Áî®ÁâπÂÆöÁöÑÂõæÂÉèÂ§ÑÁêÜÂ∑•ÂÖ∑ÔºåÂπ∂Âú®ÊÄùÁª¥Èìæ‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•Êõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞Ê®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÂà©Áî®Â∑•ÂÖ∑ËøõË°åÂõæÂÉèÊìç‰ΩúÂíåÊé®ÁêÜÁöÑËÉΩÂäõÔºå‰ªéËÄåÊé®Âä®AgenticÂõæÂÉèÊé®ÁêÜÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTIR-BenchÂåÖÂê´13‰∏™‰∏çÂêåÁöÑ‰ªªÂä°ÔºåÊØè‰∏™‰ªªÂä°ÈÉΩËÆæËÆ°‰∏∫ÈúÄË¶Å‰ΩøÁî®ÁâπÂÆöÁöÑÂõæÂÉèÂ§ÑÁêÜÂ∑•ÂÖ∑„ÄÇÊ®°ÂûãÈúÄË¶ÅÈ¶ñÂÖàÁêÜËß£‰ªªÂä°ÁõÆÊ†áÔºåÁÑ∂ÂêéÈÄâÊã©ÂêàÈÄÇÁöÑÂ∑•ÂÖ∑ÔºåÂπ∂ÊåâÁÖß‰∏ÄÂÆöÁöÑÊ≠•È™§ÊâßË°åÂõæÂÉèÂ§ÑÁêÜÊìç‰ΩúÔºåÊúÄÁªàÂæóÂà∞ÁªìÊûú„ÄÇÊï¥‰∏™ËøáÁ®ãÊ®°Êãü‰∫Ü‰∫∫Á±ªÂà©Áî®Â∑•ÂÖ∑Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑÊÄùÁª¥ËøáÁ®ã„ÄÇÂü∫ÂáÜÊµãËØïËøòÊèê‰æõ‰∫Ü‰∏ÄÂ•óËØÑ‰º∞ÊåáÊ†áÔºåÁî®‰∫éË°°ÈáèÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTIR-BenchÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‰ªªÂä°ÁöÑÂ§öÊ†∑ÊÄßÂíåÂØπÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑÂº∫Ë∞É„ÄÇ‰∏éÁé∞ÊúâÂü∫ÂáÜÊµãËØïÁõ∏ÊØîÔºåTIR-BenchÁöÑ‰ªªÂä°Êõ¥Âä†Â§çÊùÇÔºåÈúÄË¶ÅÊ®°ÂûãÂÖ∑Â§áÊõ¥Âº∫ÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåTIR-BenchËøòÂºïÂÖ•‰∫ÜAgenticÂæÆË∞ÉÁöÑÊ¶ÇÂøµÔºåÊé¢Á¥¢Â¶Ç‰ΩïÈÄöËøáÂæÆË∞ÉÊù•ÊèêÈ´òÊ®°ÂûãÂú®Â∑•ÂÖ∑‰ΩøÁî®ÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTIR-BenchÁöÑ‰ªªÂä°ËÆæËÆ°Ê∂µÁõñ‰∫ÜÂ§öÁßçÂõæÂÉèÂ§ÑÁêÜÊìç‰ΩúÔºå‰æãÂ¶ÇÂõæÂÉèÁºñËæë„ÄÅÂõæÂÉèÂ¢ûÂº∫„ÄÅÂõæÂÉè‰øÆÂ§çÁ≠â„ÄÇÊØè‰∏™‰ªªÂä°ÈÉΩÈÖçÂ§á‰∫ÜÁõ∏Â∫îÁöÑÂ∑•ÂÖ∑ÔºåÊ®°ÂûãÈúÄË¶ÅÂ≠¶‰π†Â¶Ç‰Ωï‰ΩøÁî®Ëøô‰∫õÂ∑•ÂÖ∑Êù•ÂÆåÊàê‰ªªÂä°„ÄÇÂü∫ÂáÜÊµãËØïËøòËÄÉËôë‰∫Ü‰ªªÂä°ÁöÑÈöæÂ∫¶Ôºå‰ªéÁÆÄÂçïÂà∞Â§çÊùÇÔºåÈÄêÊ≠•ÊèêÈ´òÂØπÊ®°ÂûãÁöÑË¶ÅÊ±Ç„ÄÇËØÑ‰º∞ÊåáÊ†áÂåÖÊã¨‰ªªÂä°ÂÆåÊàêÁéá„ÄÅÂáÜÁ°ÆÁéáÁ≠âÔºåÁî®‰∫éÂÖ®Èù¢ËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

TIR-BenchËØÑ‰º∞‰∫Ü22‰∏™Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÁªìÊûúË°®ÊòéËØ•Âü∫ÂáÜÊµãËØïÂÖ∑ÊúâÊôÆÈÅçÁöÑÊåëÊàòÊÄßÔºåÈúÄË¶ÅÊ®°ÂûãÂÖ∑Â§áÁúüÊ≠£ÁöÑÂõæÂÉèÊÄùËÄÉËÉΩÂäõ„ÄÇÂàùÊ≠•Á†îÁ©∂Ë°®ÊòéÔºåAgenticÂæÆË∞ÉÂèØËÉΩ‰ºò‰∫éÁõ¥Êé•ÂæÆË∞ÉÔºå‰∏∫Êú™Êù•ÁöÑÊ®°ÂûãËÆ≠ÁªÉÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊñπÂêë„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÊú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TIR-BenchÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÂõæÂÉèÁºñËæë„ÄÅËá™Âä®ÂåñÂõæÂÉè‰øÆÂ§ç„ÄÅËßÜËßâËæÖÂä©Â∑•ÂÖ∑Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂà©Áî®Â∑•ÂÖ∑ËøõË°åÂõæÂÉèÊé®ÁêÜÁöÑËÉΩÂäõÔºåÂèØ‰ª•ÂºÄÂèëÂá∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑÂõæÂÉèÂ§ÑÁêÜÁ≥ªÁªüÔºå‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Â•ΩÁöÑ‰ΩìÈ™åÔºåÂπ∂Êé®Âä®ËÆ°ÁÆóÊú∫ËßÜËßâÊäÄÊúØÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.

