---
layout: default
title: HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain
---

# HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.01756" target="_blank" class="toolbar-btn">arXiv: 2511.01756v1</a>
    <a href="https://arxiv.org/pdf/2511.01756.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01756v1" 
            onclick="toggleFavorite(this, '2511.01756v1', 'HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kai Zhai, Ziyan Huang, Qiang Nie, Xiang Li, Bo Ouyang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HGFreNetÔºåÂà©Áî®Hop-hybrid GraphFomerËß£ÂÜ≥ÂçïÁõÆËßÜÈ¢ë3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°‰∏≠ÁöÑËΩ®Ëøπ‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°` `ÂçïÁõÆËßÜÈ¢ë` `ÂõæÁ•ûÁªèÁΩëÁªú` `Transformer` `È¢ëÂüüÂàÜÊûê` `Êó∂Á©∫Âª∫Ê®°` `ËΩ®Ëøπ‰∏ÄËá¥ÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂçïÁõÆËßÜÈ¢ë3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊòìÂèóÊ∑±Â∫¶Ê≠ß‰πâÂíå2DËØØÂ∑ÆÂΩ±ÂìçÔºåÂØºËá¥3DËΩ®Ëøπ‰∏çËøûË¥ØÔºåÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπÂÖ®Â±ÄÊó∂Á©∫Áõ∏ÂÖ≥ÊÄßÁöÑÊúâÊïàÂª∫Ê®°„ÄÇ
2. HGFreNetÈÄöËøáHop-hybridÂõæÊ≥®ÊÑèÂäõÊ®°ÂùóÂíåTransformerÁºñÁ†ÅÂô®ÔºåÊçïÊçâÂÖ≥ËäÇÁöÑÂÖ®Â±ÄÊó∂Á©∫Áõ∏ÂÖ≥ÊÄßÔºåÂπ∂Âú®È¢ëÂüüÁ∫¶ÊùüËΩ®Ëøπ‰∏ÄËá¥ÊÄßÔºåÊèêÂçáÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåËøûË¥ØÊÄß„ÄÇ
3. Âú®Human3.6MÂíåMPI-INF-3DHPÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåHGFreNetÂú®‰ΩçÁΩÆÁ≤æÂ∫¶ÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄßÊñπÈù¢ÂùáË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈíàÂØπÂçïÁõÆËßÜÈ¢ë‰∏≠3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑ2D-to-3DÂßøÊÄÅÊèêÂçáÈóÆÈ¢òÔºåËØ•ÈóÆÈ¢òÈù¢‰∏¥Ê∑±Â∫¶Ê®°Á≥äÂíå2DÂßøÊÄÅ‰º∞ËÆ°ËØØÂ∑ÆÂØºËá¥ÁöÑ3DËΩ®Ëøπ‰∏ç‰∏ÄËá¥ÊÄßÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂú®Êó∂ÂüüÈôêÂà∂ÊäñÂä®ÔºåÂøΩÁï•‰∫ÜÈ™®È™ºÂÖ≥ËäÇËøêÂä®ÁöÑÂÖ®Â±ÄÊó∂Á©∫Áõ∏ÂÖ≥ÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑGraphFormerÊû∂ÊûÑHGFreNetÔºåÂÆÉÁªìÂêà‰∫ÜË∑≥Ë∑ÉÊ∑∑ÂêàÁâπÂæÅËÅöÂêàÂíåÈ¢ëÂüü‰∏≠ÁöÑ3DËΩ®Ëøπ‰∏ÄËá¥ÊÄß„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ë∑≥Ë∑ÉÊ∑∑ÂêàÂõæÊ≥®ÊÑèÂäõÔºàHGAÔºâÊ®°ÂùóÂíå‰∏Ä‰∏™TransformerÁºñÁ†ÅÂô®Êù•Âª∫Ê®°ÂÖ®Â±ÄÂÖ≥ËäÇÊó∂Á©∫Áõ∏ÂÖ≥ÊÄß„ÄÇHGAÊ®°ÂùóÂ∞ÜÈ™®È™ºÂÖ≥ËäÇÁöÑÊâÄÊúâkË∑≥ÈÇªÂ±ÖÂàÜÁªÑÂà∞‰∏Ä‰∏™Ê∑∑ÂêàÁªÑ‰∏≠Ôºå‰ª•Êâ©Â§ßÊÑüÂèóÈáéÔºåÂπ∂Â∫îÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÂèëÁé∞Ëøô‰∫õÁªÑÁöÑÊΩúÂú®Áõ∏ÂÖ≥ÊÄß„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÈÄöËøáÁ∫¶ÊùüÈ¢ëÂüü‰∏≠ÁöÑËΩ®Ëøπ‰∏ÄËá¥ÊÄßÊù•Âà©Áî®ÂÖ®Â±ÄÊó∂Èó¥Áõ∏ÂÖ≥ÊÄß„ÄÇ‰∏∫‰∫Ü‰∏∫Ë∑®Â∏ßÁöÑÊ∑±Â∫¶Êé®Êñ≠Êèê‰æõ3D‰ø°ÊÅØÂπ∂‰øùÊåÅÊó∂Èó¥‰∏äÁöÑ‰∏ÄËá¥ÊÄßÔºåÂ∫îÁî®ÂàùÊ≠•ÁΩëÁªúÊù•‰º∞ËÆ°3DÂßøÊÄÅ„ÄÇÂú®Human3.6MÂíåMPI-INF-3DHP‰∏§‰∏™Ê†áÂáÜÂü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑHGFreNetÂú®‰ΩçÁΩÆÁ≤æÂ∫¶ÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄßÊñπÈù¢Âùá‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂçïÁõÆËßÜÈ¢ë3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°‰∏≠ÔºåÁî±‰∫é2DÂßøÊÄÅ‰º∞ËÆ°ËØØÂ∑ÆÂíåÊ∑±Â∫¶Ê®°Á≥äÊÄßÂØºËá¥ÁöÑ3DËΩ®Ëøπ‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Êó∂Âüü‰∏äÁöÑÂπ≥ÊªëÔºå‰æãÂ¶ÇÈôêÂà∂Áõ∏ÈÇªÂ∏ß‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºå‰ΩÜÂøΩÁï•‰∫ÜÈ™®È™ºÂÖ≥ËäÇËøêÂä®ÁöÑÂÖ®Â±ÄÊó∂Á©∫Áõ∏ÂÖ≥ÊÄßÔºåÂØºËá¥‰º∞ËÆ°ÁöÑ3DÂßøÊÄÅÂ∫èÂàóÂ≠òÂú®ÊäñÂä®Âíå‰∏çËøûË¥ØÁé∞Ë±°„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®GraphFormerÊû∂ÊûÑÔºåÁªìÂêàË∑≥Ë∑ÉÊ∑∑ÂêàÂõæÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÈ¢ëÂüüÁ∫¶ÊùüÔºåÂêåÊó∂Âª∫Ê®°ÂÖ≥ËäÇÁöÑÂÖ®Â±ÄÊó∂Á©∫Áõ∏ÂÖ≥ÊÄßÂíåËΩ®ËøπÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÊâ©Â§ßÊÑüÂèóÈáéÂíåÂú®È¢ëÂüüËøõË°åÁ∫¶ÊùüÔºåÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞ÊäëÂà∂Âô™Â£∞ÂíåÊèêÈ´ò‰º∞ËÆ°ÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHGFreNetÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) 2DÂßøÊÄÅËæìÂÖ•Ôºõ2) ÂàùÂßã3DÂßøÊÄÅ‰º∞ËÆ°ÁΩëÁªúÔºà‰∏∫ÂêéÁª≠Ê®°ÂùóÊèê‰æõÂàùÊ≠•ÁöÑ3D‰ø°ÊÅØÔºâÔºõ3) Hop-hybridÂõæÊ≥®ÊÑèÂäõÔºàHGAÔºâÊ®°ÂùóÔºåÁî®‰∫éËÅöÂêàÂÖ≥ËäÇÁöÑkË∑≥ÈÇªÂ±Ö‰ø°ÊÅØÔºõ4) TransformerÁºñÁ†ÅÂô®ÔºåÁî®‰∫éÂª∫Ê®°ÂÖ®Â±ÄÊó∂Á©∫Áõ∏ÂÖ≥ÊÄßÔºõ5) È¢ëÂüüËΩ®Ëøπ‰∏ÄËá¥ÊÄßÁ∫¶ÊùüÊ®°ÂùóÔºåÁî®‰∫é‰øùËØÅ‰º∞ËÆ°ÁöÑ3DÂßøÊÄÅÂ∫èÂàóÂú®Êó∂Èó¥‰∏äÁöÑÂπ≥ÊªëÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ãÂá†ÁÇπÔºö1) ÊèêÂá∫‰∫ÜHop-hybridÂõæÊ≥®ÊÑèÂäõÔºàHGAÔºâÊ®°ÂùóÔºåÈÄöËøáÂ∞ÜkË∑≥ÈÇªÂ±ÖÂàÜÁªÑÂπ∂Â∫îÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊâ©Â§ß‰∫ÜÊÑüÂèóÈáéÔºåÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∫ÜÂÖ≥ËäÇ‰πãÈó¥ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÔºõ2) Âú®È¢ëÂüü‰∏äÂØπ3DËΩ®ËøπËøõË°å‰∏ÄËá¥ÊÄßÁ∫¶ÊùüÔºåÊúâÊïàÂú∞ÊäëÂà∂‰∫ÜÊó∂Èó¥‰∏äÁöÑÊäñÂä®ÔºåÊèêÈ´ò‰∫ÜÂßøÊÄÅ‰º∞ËÆ°ÁöÑÁ®≥ÂÆöÊÄßÔºõ3) Â∞ÜÂõæÁ•ûÁªèÁΩëÁªúÂíåTransformerÁªìÂêàÔºåÂÖÖÂàÜÂà©Áî®‰∫ÜÂÆÉ‰ª¨Âú®Âª∫Ê®°Á©∫Èó¥ÂíåÊó∂Èó¥Áõ∏ÂÖ≥ÊÄßÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHGAÊ®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ¶Ç‰ΩïÈÄâÊã©ÂêàÈÄÇÁöÑkÂÄº‰ª•ÂèäÂ¶Ç‰ΩïÂÆö‰πâÈÇªÂ±ÖÁªÑ„ÄÇÈ¢ëÂüüÁ∫¶ÊùüÁöÑÂÖ≥ÈîÆÂú®‰∫éÈÄâÊã©ÂêàÈÄÇÁöÑÂèòÊç¢ÊñπÊ≥ïÔºà‰æãÂ¶ÇÁ¶ªÊï£‰ΩôÂº¶ÂèòÊç¢DCTÔºâ‰ª•ÂèäÂ¶Ç‰ΩïËÆæËÆ°ÊçüÂ§±ÂáΩÊï∞Êù•Ë°°ÈáèËΩ®ËøπÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂàùÊ≠•3DÂßøÊÄÅ‰º∞ËÆ°ÁΩëÁªúÂèØ‰ª•‰ΩøÁî®Áé∞ÊúâÁöÑ2D-to-3D liftingÊñπÊ≥ïÔºå‰ΩÜÈúÄË¶Å‰øùËØÅÂÖ∂ËæìÂá∫ÁöÑ3DÂßøÊÄÅÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÁ≤æÂ∫¶Ôºå‰ª•‰æø‰∏∫ÂêéÁª≠Ê®°ÂùóÊèê‰æõÊúâÁî®ÁöÑ‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HGFreNetÂú®Human3.6MÂíåMPI-INF-3DHPÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®Human3.6MÊï∞ÊçÆÈõÜ‰∏äÔºåHGFreNetÂú®Â§ö‰∏™ÊåáÊ†á‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâSOTAÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®Êó∂Èó¥‰∏ÄËá¥ÊÄßÊñπÈù¢ÊúâÊòéÊòæÊîπÂñÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑHop-hybridÂõæÊ≥®ÊÑèÂäõÊ®°ÂùóÂíåÈ¢ëÂüüÁ∫¶ÊùüËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´ò3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰∫∫Êú∫‰∫§‰∫í„ÄÅÂä®‰ΩúÊçïÊçâ„ÄÅËôöÊãüÁé∞ÂÆû/Â¢ûÂº∫Áé∞ÂÆû„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËøêÂä®ÂàÜÊûêÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÈ´òÂçïÁõÆËßÜÈ¢ë3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁ®≥ÂÆöÊÄßÔºåÂèØ‰ª•‰∏∫Ëøô‰∫õÂ∫îÁî®Êèê‰æõÊõ¥ÂèØÈù†ÁöÑËæìÂÖ•Êï∞ÊçÆÔºå‰ªéËÄåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÊÄßËÉΩ„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂ§ö‰∫∫‰∫§‰∫í„ÄÅÈÅÆÊå°ÊÉÖÂÜµÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose estimation in monocular video, where graph convolutional networks (GCNs) and attention mechanisms have proven to be inherently suitable for encoding the spatial-temporal correlations of skeletal joints. However, depth ambiguity and errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous studies have attempted to restrict jitters in the time domain, for instance, by constraining the differences between adjacent frames while neglecting the global spatial-temporal correlations of skeletal joint motion. To tackle this problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid feature aggregation and 3D trajectory consistency in the frequency domain. Specifically, we propose a hop-hybrid graph attention (HGA) module and a Transformer encoder to model global joint spatial-temporal correlations. The HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group to enlarge the receptive field and applies the attention mechanism to discover the latent correlations of these groups globally. We then exploit global temporal correlations by constraining trajectory consistency in the frequency domain. To provide 3D information for depth inference across frames and maintain coherence over time, a preliminary network is applied to estimate the 3D pose. Extensive experiments were conducted on two standard benchmark datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional accuracy and temporal consistency.

