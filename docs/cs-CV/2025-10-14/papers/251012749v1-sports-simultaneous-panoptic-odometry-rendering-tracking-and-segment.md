---
layout: default
title: SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding
---

# SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding

**arXiv**: [2510.12749v1](https://arxiv.org/abs/2510.12749) | [PDF](https://arxiv.org/pdf/2510.12749.pdf)

**ä½œè€…**: Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPORTSæ¡†æž¶ä»¥è§£å†³åŸŽå¸‚åœºæ™¯ç†è§£ä¸­çš„åˆ†å‰²ç¼ºé™·å’ŒåŠ¨æ€å¹²æ‰°é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘å…¨æ™¯åˆ†å‰²` `è§†è§‰é‡Œç¨‹è®¡` `åœºæ™¯æ¸²æŸ“` `åŸŽå¸‚åœºæ™¯ç†è§£` `è‡ªé€‚åº”æ³¨æ„åŠ›èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å­˜åœ¨åˆ†å‰²ç¼ºé™·ã€åŠ¨æ€å¯¹è±¡å¹²æ‰°ã€æ•°æ®ç¨€ç–å’Œè§†è§’é™åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè§†é¢‘å…¨æ™¯åˆ†å‰²ã€è§†è§‰é‡Œç¨‹è®¡å’Œåœºæ™¯æ¸²æŸ“ï¼Œé‡‡ç”¨è‡ªé€‚åº”æ³¨æ„åŠ›ç‰¹å¾èžåˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼Œåœ¨é‡Œç¨‹è®¡ã€è·Ÿè¸ªã€åˆ†å‰²å’Œæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The scene perception, understanding, and simulation are fundamental
> techniques for embodied-AI agents, while existing solutions are still prone to
> segmentation deficiency, dynamic objects' interference, sensor data sparsity,
> and view-limitation problems. This paper proposes a novel framework, named
> SPORTS, for holistic scene understanding via tightly integrating Video Panoptic
> Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into
> an iterative and unified perspective. Firstly, VPS designs an adaptive
> attention-based geometric fusion mechanism to align cross-frame features via
> enrolling the pose, depth, and optical flow modality, which automatically
> adjust feature maps for different decoding stages. And a post-matching strategy
> is integrated to improve identities tracking. In VO, panoptic segmentation
> results from VPS are combined with the optical flow map to improve the
> confidence estimation of dynamic objects, which enhances the accuracy of the
> camera pose estimation and completeness of the depth map generation via the
> learning-based paradigm. Furthermore, the point-based rendering of SR is
> beneficial from VO, transforming sparse point clouds into neural fields to
> synthesize high-fidelity RGB views and twin panoptic views. Extensive
> experiments on three public datasets demonstrate that our attention-based
> feature fusion outperforms most existing state-of-the-art methods on the
> odometry, tracking, segmentation, and novel view synthesis tasks.

