---
layout: default
title: Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception
---

# Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception

**arXiv**: [2510.12528v1](https://arxiv.org/abs/2510.12528) | [PDF](https://arxiv.org/pdf/2510.12528.pdf)

**ä½œè€…**: Muxing Huang, Zibin Chen, Weiliang Xu, Zilan Li, Yuanzhi Zhou, Guoyuan Zhou, Wenjing Chen, Xinming Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæµç½‘ç»œç‰¹å¾æå–ä¸Žèžåˆç­–ç•¥ï¼Œä»¥æå‡è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨åœ¨ç‰©ä½“è¯†åˆ«ä¸­çš„å‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨` `åŒæµç½‘ç»œ` `ç‰¹å¾èžåˆ` `ç‰©ä½“è¯†åˆ«` `å·ç§¯ç¥žç»ç½‘ç»œ` `å¤šæ¨¡æ€æ„ŸçŸ¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨äº§ç”Ÿå†—ä½™ä¿¡æ¯ï¼Œå•ç»´ç‰¹å¾æå–ç¼ºä¹æœ‰æ•ˆèžåˆï¼Œé™åˆ¶è¯†åˆ«ç²¾åº¦æå‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒæµç½‘ç»œåˆ†åˆ«æå–ç‰©ä½“å†…å¤–ç‰¹å¾ï¼Œç»“åˆæ·±åº¦å›¾å’Œç¡¬åº¦ä¿¡æ¯ï¼Œç»CNNæå–åŽåŠ æƒèžåˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†æµ‹è¯•ä¸­ï¼Œç¡¬åº¦è¯†åˆ«å‡†ç¡®çŽ‡98.0%ï¼Œå½¢çŠ¶è¯†åˆ«93.75%ï¼Œå®žé™…æŠ“å–åœºæ™¯è¯†åˆ«è¶…98.5%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Tactile perception is crucial for embodied intelligent robots to recognize
> objects. Vision-based tactile sensors extract object physical attributes
> multidimensionally using high spatial resolution; however, this process
> generates abundant redundant information. Furthermore, single-dimensional
> extraction, lacking effective fusion, fails to fully characterize object
> attributes. These challenges hinder the improvement of recognition accuracy. To
> address this issue, this study introduces a two-stream network feature
> extraction and fusion perception strategy for vision-based tactile systems.
> This strategy employs a distributed approach to extract internal and external
> object features. It obtains depth map information through three-dimensional
> reconstruction while simultaneously acquiring hardness information by measuring
> contact force data. After extracting features with a convolutional neural
> network (CNN), weighted fusion is applied to create a more informative and
> effective feature representation. In standard tests on objects of varying
> shapes and hardness, the force prediction error is 0.06 N (within a 12 N
> range). Hardness recognition accuracy reaches 98.0%, and shape recognition
> accuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in
> actual grasping scenarios exceeds 98.5%. Focused on object physical attributes
> perception, this method enhances the artificial tactile system ability to
> transition from perception to cognition, enabling its use in embodied
> perception applications.

