---
layout: default
title: LayerSync: Self-aligning Intermediate Layers
---

# LayerSync: Self-aligning Intermediate Layers

**arXiv**: [2510.12581v1](https://arxiv.org/abs/2510.12581) | [PDF](https://arxiv.org/pdf/2510.12581.pdf)

**ä½œè€…**: Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLayerSyncè‡ªå¯¹é½ä¸­é—´å±‚æ–¹æ³•ï¼Œæå‡æ‰©æ•£æ¨¡åž‹ç”Ÿæˆè´¨é‡ä¸Žè®­ç»ƒæ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `ä¸­é—´å±‚æ­£åˆ™åŒ–` `è‡ªå¯¹é½å­¦ä¹ ` `å¤šæ¨¡æ€ç”Ÿæˆ` `è®­ç»ƒåŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹ä¸­é—´å±‚è¡¨ç¤ºè´¨é‡ä¸å‡ï¼Œå½±å“ç”Ÿæˆè´¨é‡ä¸Žè®­ç»ƒæ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è¯­ä¹‰ä¸°å¯Œå±‚æŒ‡å¯¼å¼±å±‚ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ï¼Œä½œä¸ºå³æ’å³ç”¨æ­£åˆ™é¡¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒç”Ÿæˆä¸­åŠ é€Ÿè®­ç»ƒ8.75å€ï¼Œæå‡è´¨é‡23.6%ï¼Œé€‚ç”¨äºŽå¤šæ¨¡æ€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose LayerSync, a domain-agnostic approach for improving the generation
> quality and the training efficiency of diffusion models. Prior studies have
> highlighted the connection between the quality of generation and the
> representations learned by diffusion models, showing that external guidance on
> model intermediate representations accelerates training. We reconceptualize
> this paradigm by regularizing diffusion models with their own intermediate
> representations. Building on the observation that representation quality varies
> across diffusion model layers, we show that the most semantically rich
> representations can act as an intrinsic guidance for weaker ones, reducing the
> need for external supervision. Our approach, LayerSync, is a self-sufficient,
> plug-and-play regularizer term with no overhead on diffusion model training and
> generalizes beyond the visual domain to other modalities. LayerSync requires no
> pretrained models nor additional data. We extensively evaluate the method on
> image generation and demonstrate its applicability to other domains such as
> audio, video, and motion generation. We show that it consistently improves the
> generation quality and the training efficiency. For example, we speed up the
> training of flow-based transformer by over 8.75x on ImageNet dataset and
> improved the generation quality by 23.6%. The code is available at
> https://github.com/vita-epfl/LayerSync.

