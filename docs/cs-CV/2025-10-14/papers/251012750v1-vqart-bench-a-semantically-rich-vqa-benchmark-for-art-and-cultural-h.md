---
layout: default
title: VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage
---

# VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage

**arXiv**: [2510.12750v1](https://arxiv.org/abs/2510.12750) | [PDF](https://arxiv.org/pdf/2510.12750.pdf)

**ä½œè€…**: A. Alfarano, L. Venturoli, D. Negueruela del Castillo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVQArt-BenchåŸºå‡†ä»¥è¯„ä¼°æ–‡åŒ–é—äº§é¢†åŸŸçš„è§†è§‰è¯­ä¹‰ç†è§£**

**å…³é”®è¯**: `è§†è§‰é—®ç­”åŸºå‡†` `æ–‡åŒ–é—äº§åˆ†æž` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è¯­ä¹‰ç†è§£è¯„ä¼°` `å¤šæ™ºèƒ½ä½“ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VQAåŸºå‡†ç¼ºä¹æ·±åº¦è¯­ä¹‰è¯„ä¼°ï¼Œå°¤å…¶åœ¨è‰ºæœ¯åˆ†æžä¸­ä¾èµ–è¡¨é¢å±žæ€§
2. é‡‡ç”¨å¤šæ™ºèƒ½ä½“ç®¡é“ç”Ÿæˆå¤šæ ·ã€éªŒè¯çš„é—®é¢˜ï¼Œè¦†ç›–ç¬¦å·æ„ä¹‰å’Œå¤æ‚å…³ç³»
3. è¯„ä¼°14ä¸ªMLLMæ˜¾ç¤ºæ¨¡åž‹åœ¨è®¡æ•°ä»»åŠ¡å’Œå¼€æºä¸Žä¸“æœ‰æ¨¡åž‹é—´å­˜åœ¨æ€§èƒ½å·®è·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have demonstrated significant
> capabilities in joint visual and linguistic tasks. However, existing Visual
> Question Answering (VQA) benchmarks often fail to evaluate deep semantic
> understanding, particularly in complex domains like visual art analysis.
> Confined to simple syntactic structures and surface-level attributes, these
> questions fail to capture the diversity and depth of human visual inquiry. This
> limitation incentivizes models to exploit statistical shortcuts rather than
> engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a
> new, large-scale VQA benchmark for the cultural heritage domain. This benchmark
> is constructed using a novel multi-agent pipeline where specialized agents
> collaborate to generate nuanced, validated, and linguistically diverse
> questions. The resulting benchmark is structured along relevant visual
> understanding dimensions that probe a model's ability to interpret symbolic
> meaning, narratives, and complex visual relationships. Our evaluation of 14
> state-of-the-art MLLMs on this benchmark reveals significant limitations in
> current models, including a surprising weakness in simple counting tasks and a
> clear performance gap between proprietary and open-source models.

