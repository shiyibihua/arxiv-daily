---
layout: default
title: ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution
---

# ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution

**arXiv**: [2510.12793v1](https://arxiv.org/abs/2510.12793) | [PDF](https://arxiv.org/pdf/2510.12793.pdf)

**ä½œè€…**: Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViCOè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥åŠ¨æ€è°ƒæ•´è§†è§‰ä»¤ç‰Œæ•°é‡ä»¥é™ä½MLLMæ¨ç†æˆæœ¬**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰ä»¤ç‰Œå‹ç¼©` `è¯­ä¹‰å¤æ‚åº¦` `åŠ¨æ€é«˜åˆ†è¾¨ç‡` `æ¨ç†æ•ˆç‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å› å›¾åƒè¾“å…¥å¢åŠ è§†è§‰ä»¤ç‰Œï¼Œå¯¼è‡´æ¨ç†æˆæœ¬ä¸Šå‡
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤šMLPè¿æ¥å™¨åŸºäºè¯­ä¹‰å¤æ‚åº¦å‹ç¼©è§†è§‰ä»¤ç‰Œï¼Œå¹¶æœ€å°åŒ–KLæ•£åº¦
3. å®éªŒæˆ–æ•ˆæœï¼šå‡å°‘è§†è§‰ä»¤ç‰Œè¾¾50%ï¼Œä¿æŒæ„ŸçŸ¥ã€æ¨ç†å’ŒOCRèƒ½åŠ›

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing Multimodal Large Language Models (MLLMs) suffer from increased
> inference costs due to the additional vision tokens introduced by image inputs.
> In this work, we propose Visual Consistency Learning (ViCO), a novel training
> algorithm that enables the model to represent images of varying semantic
> complexities using different numbers of vision tokens. The key idea behind our
> method is to employ multiple MLP connectors, each with a different image
> compression ratio, to downsample the vision tokens based on the semantic
> complexity of the image. During training, we minimize the KL divergence between
> the responses conditioned on different MLP connectors. At inference time, we
> introduce an image router, termed Visual Resolution Router (ViR), that
> automatically selects the appropriate compression rate for each image patch.
> Compared with existing dynamic high-resolution strategies, which adjust the
> number of visual tokens based on image resolutions, our method dynamically
> adapts the number of visual tokens according to semantic complexity.
> Experimental results demonstrate that our method can reduce the number of
> vision tokens by up to 50% while maintaining the model's perception, reasoning,
> and OCR capabilities. We hope this work will contribute to the development of
> more efficient MLLMs. The code and models will be released to facilitate future
> research.

