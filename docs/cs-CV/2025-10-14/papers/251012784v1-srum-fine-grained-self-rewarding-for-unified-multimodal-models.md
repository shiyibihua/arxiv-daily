---
layout: default
title: SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models
---

# SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models

**arXiv**: [2510.12784v1](https://arxiv.org/abs/2510.12784) | [PDF](https://arxiv.org/pdf/2510.12784.pdf)

**ä½œè€…**: Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, Xihui Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSRUMè‡ªå¥–åŠ±æ¡†æž¶ä»¥è§£å†³ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹ä¸­è§†è§‰ç†è§£ä¸Žç”Ÿæˆèƒ½åŠ›ä¸åŒ¹é…é—®é¢˜**

**å…³é”®è¯**: `ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹` `è‡ªå¥–åŠ±å­¦ä¹ ` `è§†è§‰ç”Ÿæˆ` `ç†è§£-ç”Ÿæˆå¯¹é½` `å¤šå°ºåº¦å¥–åŠ±` `åŽè®­ç»ƒæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹ä¸­è§†è§‰ç†è§£èƒ½åŠ›æ— æ³•æœ‰æ•ˆè¿ç§»åˆ°è§†è§‰ç”Ÿæˆï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒä¸å¿ å®ž
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å…¨å±€-å±€éƒ¨åŒå¥–åŠ±ç³»ç»Ÿï¼Œåˆ©ç”¨ç†è§£æ¨¡å—ä½œä¸ºå†…éƒ¨è¯„ä¼°å™¨æä¾›å¤šå°ºåº¦åé¦ˆä¿¡å·
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨T2I-CompBenchå’ŒT2I-ReasonBenchåŸºå‡†ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨æ•°æ®

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, remarkable progress has been made in Unified Multimodal Models
> (UMMs), which integrate vision-language generation and understanding
> capabilities within a single framework. However, a significant gap exists where
> a model's strong visual understanding often fails to transfer to its visual
> generation. A model might correctly understand an image based on user
> instructions, yet be unable to generate a faithful image from text prompts.
> This phenomenon directly raises a compelling question: Can a model achieve
> self-improvement by using its understanding module to reward its generation
> module? To bridge this gap and achieve self-improvement, we introduce SRUM, a
> self-rewarding post-training framework that can be directly applied to existing
> UMMs of various designs. SRUM creates a feedback loop where the model's own
> understanding module acts as an internal ``evaluator'', providing corrective
> signals to improve its generation module, without requiring additional
> human-labeled data. To ensure this feedback is comprehensive, we designed a
> global-local dual reward system. To tackle the inherent structural complexity
> of images, this system offers multi-scale guidance: a \textbf{global reward}
> ensures the correctness of the overall visual semantics and layout, while a
> \textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads
> to powerful capabilities and shows strong generalization, boosting performance
> on T2I-CompBench from 82.18 to \textbf{88.37} and on T2I-ReasonBench from 43.82
> to \textbf{46.75}. Overall, our work establishes a powerful new paradigm for
> enabling a UMMs' understanding module to guide and enhance its own generation
> via self-rewarding.

