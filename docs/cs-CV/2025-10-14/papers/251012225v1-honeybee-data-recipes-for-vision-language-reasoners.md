---
layout: default
title: HoneyBee: Data Recipes for Vision-Language Reasoners
---

# HoneyBee: Data Recipes for Vision-Language Reasoners

**arXiv**: [2510.12225v1](https://arxiv.org/abs/2510.12225) | [PDF](https://arxiv.org/pdf/2510.12225.pdf)

**ä½œè€…**: Hritik Bansal, Devandra Singh Sachan, Kai-Wei Chang, Aditya Grover, Gargi Ghosh, Wen-tau Yih, Ramakanth Pasunuru

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHoneyBeeæ•°æ®é›†ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†æ•°æ®é›†` `é“¾å¼æ€ç»´` `æ•°æ®å¹²é¢„` `æ¨¡åž‹ç¼©æ”¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æŽ¨ç†è®­ç»ƒæ•°æ®é›†æž„å»ºåŽŸåˆ™ä¸æ˜Žç¡®ï¼Œå½±å“æ¨¡åž‹æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ•°æ®å¹²é¢„å’Œè§„æ¨¡æ‰©å±•ï¼Œä¼˜åŒ–å›¾åƒ-é—®é¢˜å¯¹å’Œé“¾å¼æ€ç»´è§£å†³æ–¹æ¡ˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šHoneyBeeæ•°æ®é›†è®­ç»ƒæ¨¡åž‹åœ¨MathVerseç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in vision-language models (VLMs) have made them highly
> effective at reasoning tasks. However, the principles underlying the
> construction of performant VL reasoning training datasets remain poorly
> understood. In this work, we introduce several data curation approaches and
> study their impacts on VL reasoning capabilities by carefully controlling
> training and evaluation setups. We analyze the effects of context (image and
> question pair) sources, implement targeted data interventions, and explore
> scaling up images, questions, and chain-of-thought (CoT) solutions. Our
> findings reveal that (a) context source strategies significantly affect VLM
> performance, (b) interventions such as auxiliary signals from image captions
> and the inclusion of text-only reasoning yield substantial gains, and (c)
> scaling all data dimensions (e.g., unique questions per image and unique CoTs
> per image-question pair) consistently improves reasoning capability. Motivated
> by these insights, we introduce HoneyBee, a large-scale, high-quality CoT
> reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs
> trained with HoneyBee outperform state-of-the-art models across model sizes.
> For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA
> model and the base model by 7.8% and 24.8%, respectively, on MathVerse.
> Furthermore, we propose a test-time scaling strategy that reduces decoding cost
> by 73% without sacrificing accuracy. Overall, this work presents improved
> strategies for VL reasoning dataset curation research.

