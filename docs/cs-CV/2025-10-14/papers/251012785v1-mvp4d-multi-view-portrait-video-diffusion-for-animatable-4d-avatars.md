---
layout: default
title: MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars
---

# MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars

**arXiv**: [2510.12785v1](https://arxiv.org/abs/2510.12785) | [PDF](https://arxiv.org/pdf/2510.12785.pdf)

**ä½œè€…**: Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMVP4Dæ¨¡å‹ï¼ŒåŸºäºå•å‚è€ƒå›¾åƒç”Ÿæˆå¯åŠ¨ç”»4Dè™šæ‹Ÿäººï¼Œæå‡å¤šè§†è§’çœŸå®æ„Ÿ**

**å…³é”®è¯**: `4Dè™šæ‹Ÿäºº` `å¤šè§†è§’è§†é¢‘ç”Ÿæˆ` `è§†é¢‘æ‰©æ•£æ¨¡å‹` `å®æ—¶æ¸²æŸ“` `å•å›¾åƒåŠ¨ç”»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•å›¾åƒç”Ÿæˆè™šæ‹Ÿäººæ—¶ï¼Œè§†è§’åç¦»å¯¼è‡´è´¨é‡ä¸‹é™ï¼Œç¼ºä¹å¤šè§†è§’çº¦æŸ
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºé¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå¤šè§†è§’è§†é¢‘å¹¶è’¸é¦ä¸ºå®æ—¶æ¸²æŸ“4Dè™šæ‹Ÿäºº
3. å®éªŒæˆ–æ•ˆæœï¼šç›¸æ¯”å…ˆå‰æ–¹æ³•ï¼Œæ˜¾è‘—æ”¹è¿›çœŸå®æ„Ÿã€æ—¶é—´ä¸€è‡´æ€§å’Œ3Dä¸€è‡´æ€§

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Digital human avatars aim to simulate the dynamic appearance of humans in
> virtual environments, enabling immersive experiences across gaming, film,
> virtual reality, and more. However, the conventional process for creating and
> animating photorealistic human avatars is expensive and time-consuming,
> requiring large camera capture rigs and significant manual effort from
> professional 3D artists. With the advent of capable image and video generation
> models, recent methods enable automatic rendering of realistic animated avatars
> from a single casually captured reference image of a target subject. While
> these techniques significantly lower barriers to avatar creation and offer
> compelling realism, they lack constraints provided by multi-view information or
> an explicit 3D representation. So, image quality and realism degrade when
> rendered from viewpoints that deviate strongly from the reference image. Here,
> we build a video model that generates animatable multi-view videos of digital
> humans based on a single reference image and target expressions. Our model,
> MVP4D, is based on a state-of-the-art pre-trained video diffusion model and
> generates hundreds of frames simultaneously from viewpoints varying by up to
> 360 degrees around a target subject. We show how to distill the outputs of this
> model into a 4D avatar that can be rendered in real-time. Our approach
> significantly improves the realism, temporal consistency, and 3D consistency of
> generated avatars compared to previous methods.

