---
layout: default
title: VideoLucy: Deep Memory Backtracking for Long Video Understanding
---

# VideoLucy: Deep Memory Backtracking for Long Video Understanding

**arXiv**: [2510.12422v1](https://arxiv.org/abs/2510.12422) | [PDF](https://arxiv.org/pdf/2510.12422.pdf)

**ä½œè€…**: Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoLucyæ¡†æž¶ä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„æ—¶åºä¸Šä¸‹æ–‡å’Œå…³é”®ä¿¡æ¯ä¸¢å¤±é—®é¢˜**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `åˆ†å±‚è®°å¿†ç»“æž„` `è¿­ä»£å›žæº¯æœºåˆ¶` `EgoMemåŸºå‡†` `ä»£ç†ç³»ç»Ÿ` `æ—¶åºä¸Šä¸‹æ–‡å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºäºŽLLMçš„ä»£ç†ç³»ç»Ÿéš¾ä»¥æ•æ‰è¿žç»­å¸§çš„æ—¶åºä¸Šä¸‹æ–‡ï¼Œä¸”ç¨€ç–é‡‡æ ·æ˜“ä¸¢å¤±å…³é”®ä¿¡æ¯
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚è®°å¿†ç»“æž„å’Œè¿­ä»£å›žæº¯æœºåˆ¶ï¼Œä»Žç²—åˆ°ç»†æŒ–æŽ˜è§†é¢‘å…¨å±€çš„æ·±åº¦è®°å¿†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½è¶…è¶ŠGPT-4oç­‰ä¸“æœ‰æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent studies have shown that agent-based systems leveraging large language
> models (LLMs) for key information retrieval and integration have emerged as a
> promising approach for long video understanding. However, these systems face
> two major challenges. First, they typically perform modeling and reasoning on
> individual frames, struggling to capture the temporal context of consecutive
> frames. Second, to reduce the cost of dense frame-level captioning, they adopt
> sparse frame sampling, which risks discarding crucial information. To overcome
> these limitations, we propose VideoLucy, a deep memory backtracking framework
> for long video understanding. Inspired by the human recollection process from
> coarse to fine, VideoLucy employs a hierarchical memory structure with
> progressive granularity. This structure explicitly defines the detail level and
> temporal scope of memory at different hierarchical depths. Through an
> agent-based iterative backtracking mechanism, VideoLucy systematically mines
> video-wide, question-relevant deep memories until sufficient information is
> gathered to provide a confident answer. This design enables effective temporal
> understanding of consecutive frames while preserving critical details. In
> addition, we introduce EgoMem, a new benchmark for long video understanding.
> EgoMem is designed to comprehensively evaluate a model's ability to understand
> complex events that unfold over time and capture fine-grained details in
> extremely long videos. Extensive experiments demonstrate the superiority of
> VideoLucy. Built on open-source models, VideoLucy significantly outperforms
> state-of-the-art methods on multiple long video understanding benchmarks,
> achieving performance even surpassing the latest proprietary models such as
> GPT-4o. Our code and dataset will be made publicly at
> https://videolucy.github.io

