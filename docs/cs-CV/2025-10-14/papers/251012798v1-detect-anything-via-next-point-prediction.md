---
layout: default
title: Detect Anything via Next Point Prediction
---

# Detect Anything via Next Point Prediction

**arXiv**: [2510.12798v1](https://arxiv.org/abs/2510.12798) | [PDF](https://arxiv.org/pdf/2510.12798.pdf)

**ä½œè€…**: Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRex-Omniæ¨¡åž‹ï¼Œé€šè¿‡ä¸‹ä¸€ç‚¹é¢„æµ‹å®žçŽ°é›¶æ ·æœ¬ç‰©ä½“æ£€æµ‹ä¸Žå¤šä»»åŠ¡æ„ŸçŸ¥**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç‰©ä½“æ£€æµ‹` `é›¶æ ·æœ¬å­¦ä¹ ` `åæ ‡é‡åŒ–` `å¼ºåŒ–å­¦ä¹ è®­ç»ƒ` `è§†è§‰è¯­è¨€ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå›žå½’æ¨¡åž‹åœ¨ç‰©ä½“æ£€æµ‹ä¸­å­˜åœ¨å¬å›žçŽ‡ä½Žã€é‡å¤é¢„æµ‹ç­‰é—®é¢˜ï¼ŒMLLMsåº”ç”¨å—é™ã€‚
2. é‡‡ç”¨é‡åŒ–åæ ‡è¡¨ç¤ºã€å¤šæ•°æ®å¼•æ“Žç”Ÿæˆå’Œä¸¤é˜¶æ®µè®­ç»ƒï¼Œæå‡åæ ‡é¢„æµ‹ç²¾åº¦ä¸Žæ•ˆçŽ‡ã€‚
3. åœ¨COCOå’ŒLVISåŸºå‡†ä¸Šé›¶æ ·æœ¬æ€§èƒ½åª²ç¾Žå›žå½’æ¨¡åž‹ï¼Œå¹¶æ”¯æŒå¤šç§è¯­è¨€æ„ŸçŸ¥ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Object detection has long been dominated by traditional coordinate
> regression-based models, such as YOLO, DETR, and Grounding DINO. Although
> recent efforts have attempted to leverage MLLMs to tackle this task, they face
> challenges like low recall rate, duplicate predictions, coordinate
> misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a
> 3B-scale MLLM that achieves state-of-the-art object perception performance. On
> benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or
> exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot
> setting. This is enabled by three key designs: 1) Task Formulation: we use
> special tokens to represent quantized coordinates from 0 to 999, reducing the
> model's learning difficulty and improving token efficiency for coordinate
> prediction; 2) Data Engines: we construct multiple data engines to generate
> high-quality grounding, referring, and pointing data, providing semantically
> rich supervision for training; \3) Training Pipelines: we employ a two-stage
> training process, combining supervised fine-tuning on 22 million data with
> GRPO-based reinforcement post-training. This RL post-training leverages
> geometry-aware rewards to effectively bridge the discrete-to-continuous
> coordinate prediction gap, improve box accuracy, and mitigate undesirable
> behaviors like duplicate predictions that stem from the teacher-guided nature
> of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent
> language understanding enables versatile capabilities such as object referring,
> pointing, visual prompting, GUI grounding, spatial referring, OCR and
> key-pointing, all systematically evaluated on dedicated benchmarks. We believe
> that Rex-Omni paves the way for more versatile and language-aware visual
> perception systems.

