---
layout: default
title: AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion
---

# AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion

**arXiv**: [2510.12260v1](https://arxiv.org/abs/2510.12260) | [PDF](https://arxiv.org/pdf/2510.12260.pdf)

**ä½œè€…**: Xiaopeng Liu, Yupei Lin, Sen Zhang, Xiao Wang, Yukai Shi, Liang Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAngularFuseæ¡†æž¶ï¼Œé€šè¿‡è§’åº¦æ„ŸçŸ¥æŸå¤±è§£å†³å¯è§å…‰-çº¢å¤–å›¾åƒèžåˆä¸­çš„ç»†èŠ‚å’Œäº®åº¦é—®é¢˜ã€‚**

**å…³é”®è¯**: `å›¾åƒèžåˆ` `è§’åº¦æ„ŸçŸ¥æŸå¤±` `è·¨æ¨¡æ€äº’è¡¥` `æ— ç›‘ç£å­¦ä¹ ` `æ¢¯åº¦çº¦æŸ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ— ç›‘ç£èžåˆæ–¹æ³•ä¾èµ–æ‰‹å·¥æŸå¤±å‡½æ•°ï¼Œå¯¼è‡´ç»†èŠ‚ç¼ºå¤±å’Œäº®åº¦ä¸å‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è·¨æ¨¡æ€äº’è¡¥æŽ©ç æ¨¡å—å’Œè§’åº¦æ„ŸçŸ¥æŸå¤±ï¼ŒåŒæ—¶çº¦æŸæ¢¯åº¦å¹…å€¼å’Œæ–¹å‘ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MSRSç­‰æ•°æ®é›†ä¸Šä¼˜äºŽä¸»æµæ–¹æ³•ï¼Œç”Ÿæˆå›¾åƒæ›´é”åˆ©å’Œè¯¦ç»†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visible-infrared image fusion is crucial in key applications such as
> autonomous driving and nighttime surveillance. Its main goal is to integrate
> multimodal information to produce enhanced images that are better suited for
> downstream tasks. Although deep learning based fusion methods have made
> significant progress, mainstream unsupervised approaches still face serious
> challenges in practical applications. Existing methods mostly rely on manually
> designed loss functions to guide the fusion process. However, these loss
> functions have obvious limitations. On one hand, the reference images
> constructed by existing methods often lack details and have uneven brightness.
> On the other hand, the widely used gradient losses focus only on gradient
> magnitude. To address these challenges, this paper proposes an angle-based
> perception framework for spatial-sensitive image fusion (AngularFuse). At
> first, we design a cross-modal complementary mask module to force the network
> to learn complementary information between modalities. Then, a fine-grained
> reference image synthesis strategy is introduced. By combining Laplacian edge
> enhancement with adaptive histogram equalization, reference images with richer
> details and more balanced brightness are generated. Last but not least, we
> introduce an angle-aware loss, which for the first time constrains both
> gradient magnitude and direction simultaneously in the gradient domain.
> AngularFuse ensures that the fused images preserve both texture intensity and
> correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and
> M3FD public datasets show that AngularFuse outperforms existing mainstream
> methods with clear margin. Visual comparisons further confirm that our method
> produces sharper and more detailed results in challenging scenes, demonstrating
> superior fusion capability.

