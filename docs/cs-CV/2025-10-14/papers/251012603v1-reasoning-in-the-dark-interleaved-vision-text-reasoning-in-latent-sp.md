---
layout: default
title: Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space
---

# Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space

**arXiv**: [2510.12603v1](https://arxiv.org/abs/2510.12603) | [PDF](https://arxiv.org/pdf/2510.12603.pdf)

**ä½œè€…**: Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIVT-LRæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€æŽ¨ç†ä¸­æ˜¾å¼æ­¥éª¤ä¾èµ–å¯¼è‡´çš„æ ‡æ³¨æˆæœ¬é«˜å’ŒæŽ¨ç†å»¶è¿Ÿé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŽ¨ç†` `æ½œåœ¨ç©ºé—´æŽ¨ç†` `æŽ¨ç†æ•ˆçŽ‡ä¼˜åŒ–` `éšå¼è¡¨ç¤ºå­¦ä¹ ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰å¤šæ¨¡æ€æŽ¨ç†æ–¹æ³•ä¾èµ–æ˜¾å¼æŽ¨ç†æ­¥éª¤ï¼Œéœ€è¦å¤§é‡è§†è§‰-æ–‡æœ¬æ ‡æ³¨å¹¶å¼•å…¥é«˜æŽ¨ç†å»¶è¿Ÿ
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­æ³¨å…¥è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œç»“åˆéšå¼æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºè¿›è¡ŒæŽ¨ç†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨M3CoTå’ŒScienceQAæ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®çŽ‡æå‡5.45%ï¼ŒæŽ¨ç†é€Ÿåº¦æå‡5å€ä»¥ä¸Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal reasoning aims to enhance the capabilities of MLLMs by
> incorporating intermediate reasoning steps before reaching the final answer. It
> has evolved from text-only reasoning to the integration of visual information,
> enabling the thought process to be conveyed through both images and text.
> Despite its effectiveness, current multimodal reasoning methods depend on
> explicit reasoning steps that require labor-intensive vision-text annotations
> and inherently introduce significant inference latency. To address these
> issues, we introduce multimodal latent reasoning with the advantages of
> multimodal representation, reduced annotation, and inference efficiency. To
> facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
> which injects both visual and textual information in the reasoning process
> within the latent space. Specifically, IVT-LR represents each reasoning step by
> combining two implicit parts: latent text (the hidden states from the previous
> step) and latent vision (a set of selected image embeddings). We further
> introduce a progressive multi-stage training strategy to enable MLLMs to
> perform the above multimodal latent reasoning steps. Experiments on M3CoT and
> ScienceQA demonstrate that our IVT-LR method achieves an average performance
> increase of 5.45% in accuracy, while simultaneously achieving a speed increase
> of over 5 times compared to existing approaches. Code available at
> https://github.com/FYYDCC/IVT-LR.

