---
layout: default
title: On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation
---

# On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation

**arXiv**: [2510.12660v1](https://arxiv.org/abs/2510.12660) | [PDF](https://arxiv.org/pdf/2510.12660.pdf)

**ä½œè€…**: Shuhei Tarashima, Yushan Wang, Norio Tagawa

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨åˆ†å±‚è§†è§‰åŸºç¡€æ¨¡å‹æ—©æœŸé˜¶æ®µæ„å»ºé«˜æ•ˆäººä½“ç½‘æ ¼æ¢å¤ä¸å§¿æ€ä¼°è®¡æ¨¡å‹**

**å…³é”®è¯**: `äººä½“ç½‘æ ¼æ¢å¤` `äººä½“å§¿æ€ä¼°è®¡` `åˆ†å±‚è§†è§‰åŸºç¡€æ¨¡å‹` `è½»é‡åŒ–æ¨¡å‹` `è®¡ç®—æ•ˆç‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰HMRæ–¹æ³•ä¾èµ–å¤§å‹éåˆ†å±‚è§†è§‰å˜æ¢å™¨ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œéœ€è½»é‡åŒ–æ–¹æ¡ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨Swin Transformerç­‰åˆ†å±‚VFMçš„å‰ä¸¤æˆ–ä¸‰é˜¶æ®µä½œä¸ºç¼–ç å™¨ï¼Œä¿æŒç‰¹å¾å›¾é«˜åˆ†è¾¨ç‡ã€‚
3. å®éªŒæ•ˆæœï¼š27ä¸ªæ¨¡å‹è¯„ä¼°æ˜¾ç¤ºï¼Œæˆªæ–­æ¨¡å‹åœ¨ç²¾åº¦ä¸æ•ˆç‡é—´ä¼˜äºç°æœ‰è½»é‡æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we aim to develop simple and efficient models for human mesh
> recovery (HMR) and its predecessor task, human pose estimation (HPE).
> State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,
> non-hierarchical vision transformers as encoders, which are inherited from the
> corresponding HPE models like ViTPose. To establish baselines across varying
> computational budgets, we first construct three lightweight HMR2.0 variants by
> adapting the corresponding ViTPose models. In addition, we propose leveraging
> the early stages of hierarchical vision foundation models (VFMs), including
> Swin Transformer, GroupMixFormer, and VMamba, as encoders. This design is
> motivated by the observation that intermediate stages of hierarchical VFMs
> produce feature maps with resolutions comparable to or higher than those of
> non-hierarchical counterparts. We conduct a comprehensive evaluation of 27
> hierarchical-VFM-based HMR and HPE models, demonstrating that using only the
> first two or three stages achieves performance on par with full-stage models.
> Moreover, we show that the resulting truncated models exhibit better trade-offs
> between accuracy and computational efficiency compared to existing lightweight
> alternatives.

