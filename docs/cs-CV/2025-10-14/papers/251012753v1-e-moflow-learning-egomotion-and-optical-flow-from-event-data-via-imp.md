---
layout: default
title: E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization
---

# E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization

**arXiv**: [2510.12753v1](https://arxiv.org/abs/2510.12753) | [PDF](https://arxiv.org/pdf/2510.12753.pdf)

**ä½œè€…**: Wenpu Li, Bangyan Liao, Yi Zhou, Qi Xu, Pian Wan, Peidong Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºE-MoFlowæ¡†æž¶ï¼Œé€šè¿‡éšå¼æ­£åˆ™åŒ–ä»Žäº‹ä»¶æ•°æ®è”åˆå­¦ä¹ è‡ªè¿åŠ¨å’Œå…‰æµ**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `è‡ªè¿åŠ¨ä¼°è®¡` `å…‰æµä¼°è®¡` `éšå¼æ­£åˆ™åŒ–` `æ— ç›‘ç£å­¦ä¹ ` `ç¥žç»è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäº‹ä»¶ç›¸æœºä¸­è‡ªè¿åŠ¨å’Œå…‰æµç‹¬ç«‹ä¼°è®¡å› ç¼ºä¹æ•°æ®å…³è”è€Œç—…æ€ï¼ŒçŽ°æœ‰æ–¹æ³•æœ‰åå·®æˆ–æ˜“é™·å±€éƒ¨æœ€ä¼˜
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºæ¨¡è‡ªè¿åŠ¨ä¸ºè¿žç»­æ ·æ¡ã€å…‰æµä¸ºéšå¼ç¥žç»è¡¨ç¤ºï¼ŒåµŒå…¥æ—¶ç©ºä¸€è‡´æ€§å’Œå¾®åˆ†å‡ ä½•çº¦æŸ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ— ç›‘ç£æ–¹æ³•ä¸­è¾¾åˆ°SOTAï¼Œå¯¹ä¸€èˆ¬6-DoFè¿åŠ¨åœºæ™¯æœ‰æ•ˆï¼Œä¸Žæœ‰ç›‘ç£æ–¹æ³•ç«žäº‰

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in
> 3D vision, has typically been addressed independently. For neuromorphic vision
> (e.g., event cameras), however, the lack of robust data association makes
> solving the two problems separately an ill-posed challenge, especially in the
> absence of supervision via ground truth. Existing works mitigate this
> ill-posedness by either enforcing the smoothness of the flow field via an
> explicit variational regularizer or leveraging explicit structure-and-motion
> priors in the parametrization to improve event alignment. The former notably
> introduces bias in results and computational overhead, while the latter, which
> parametrizes the optical flow in terms of the scene depth and the camera
> motion, often converges to suboptimal local minima. To address these issues, we
> propose an unsupervised framework that jointly optimizes egomotion and optical
> flow via implicit spatial-temporal and geometric regularization. First, by
> modeling camera's egomotion as a continuous spline and optical flow as an
> implicit neural representation, our method inherently embeds spatial-temporal
> coherence through inductive biases. Second, we incorporate structure-and-motion
> priors through differential geometric constraints, bypassing explicit depth
> estimation while maintaining rigorous geometric consistency. As a result, our
> framework (called E-MoFlow) unifies egomotion and optical flow estimation via
> implicit regularization under a fully unsupervised paradigm. Experiments
> demonstrate its versatility to general 6-DoF motion scenarios, achieving
> state-of-the-art performance among unsupervised methods and competitive even
> with supervised approaches.

