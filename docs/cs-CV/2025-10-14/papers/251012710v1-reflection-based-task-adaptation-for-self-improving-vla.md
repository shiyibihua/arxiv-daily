---
layout: default
title: Reflection-Based Task Adaptation for Self-Improving VLA
---

# Reflection-Based Task Adaptation for Self-Improving VLA

**arXiv**: [2510.12710v1](https://arxiv.org/abs/2510.12710) | [PDF](https://arxiv.org/pdf/2510.12710.pdf)

**ä½œè€…**: Baicheng Li, Dong Wu, Zike Yan, Xinchen Liu, Zecui Zeng, Lusong Li, Hongbin Zha

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåå°„å¼è‡ªé€‚åº”æ¡†æž¶ä»¥è§£å†³VLAæ¨¡åž‹åœ¨æœºå™¨äººä»»åŠ¡ä¸­é€‚åº”æ•ˆçŽ‡ä½Žçš„é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `ä»»åŠ¡è‡ªé€‚åº”` `å¼ºåŒ–å­¦ä¹ ` `åæ€å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¢„è®­ç»ƒVLAæ¨¡åž‹åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶æ•ˆçŽ‡ä½Žï¼Œå¼ºåŒ–å­¦ä¹ æ”¶æ•›æ…¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒè·¯å¾„æž¶æž„ï¼Œç»“åˆå¤±è´¥é©±åŠ¨åæ€RLå’ŒæˆåŠŸé©±åŠ¨SFTï¼Œé˜²æ­¢å¥–åŠ±é»‘å®¢ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ“ä½œä»»åŠ¡ä¸­å®žçŽ°æ›´å¿«æ”¶æ•›å’Œæ›´é«˜æˆåŠŸçŽ‡ï¼Œä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pre-trained Vision-Language-Action (VLA) models represent a major leap
> towards general-purpose robots, yet efficiently adapting them to novel,
> specific tasks in-situ remains a significant hurdle. While reinforcement
> learning (RL) is a promising avenue for such adaptation, the process often
> suffers from low efficiency, hindering rapid task mastery. We introduce
> Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation
> without human intervention. Our framework establishes a self-improving loop
> where the agent learns from its own experience to enhance both strategy and
> execution.
>   The core of our framework is a dual-pathway architecture that addresses the
> full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway
> enables rapid learning by using the VLM's causal reasoning to automatically
> synthesize a targeted, dense reward function from failure analysis. This
> provides a focused learning signal that significantly accelerates policy
> exploration. However, optimizing such proxy rewards introduces a potential risk
> of "reward hacking," where the agent masters the reward function but fails the
> actual task. To counteract this, our second pathway, Success-Driven
> Quality-Guided SFT, grounds the policy in holistic success. It identifies and
> selectively imitates high-quality successful trajectories, ensuring the agent
> remains aligned with the ultimate task goal. This pathway is strengthened by a
> conditional curriculum mechanism to aid initial exploration.
>   We conduct experiments in challenging manipulation tasks. The results
> demonstrate that our framework achieves faster convergence and higher final
> success rates compared to representative baselines. Our work presents a robust
> solution for creating self-improving agents that can efficiently and reliably
> adapt to new environments.

