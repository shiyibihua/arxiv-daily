---
layout: default
title: Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control
---

# Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control

**arXiv**: [2510.12363v1](https://arxiv.org/abs/2510.12363) | [PDF](https://arxiv.org/pdf/2510.12363.pdf)

**ä½œè€…**: Jiale Fan, Andrei Cramariuc, Tifanny Portela, Marco Hutter

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé¢„è®­ç»ƒçš„æ¼”å‘˜-è¯„è®ºå®¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æå‡æœºå™¨äººè¿åŠ¨æŽ§åˆ¶çš„æ ·æœ¬æ•ˆçŽ‡å’Œæ€§èƒ½**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººè¿åŠ¨æŽ§åˆ¶` `é¢„è®­ç»ƒ` `æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•` `æ ·æœ¬æ•ˆçŽ‡` `PIDMæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººè¿åŠ¨æŽ§åˆ¶ä¸­å¼ºåŒ–å­¦ä¹ å¸¸ä»Žé›¶å¼€å§‹å­¦ä¹ ï¼Œç¼ºä¹è·¨ä»»åŠ¡é€šç”¨çŸ¥è¯†å…±äº«ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ä»»åŠ¡æ— å…³æŽ¢ç´¢æ”¶é›†æ•°æ®ï¼Œè®­ç»ƒPIDMæ¨¡åž‹ï¼Œé¢„è®­ç»ƒæƒé‡ç”¨äºŽåˆå§‹åŒ–æ¼”å‘˜-è¯„è®ºå®¶ç½‘ç»œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸ƒé¡¹ä»»åŠ¡ä¸­ï¼Œæ ·æœ¬æ•ˆçŽ‡å¹³å‡æå‡40.1%ï¼Œä»»åŠ¡æ€§èƒ½å¹³å‡æå‡7.5%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The pretraining-finetuning paradigm has facilitated numerous transformative
> advancements in artificial intelligence research in recent years. However, in
> the domain of reinforcement learning (RL) for robot motion control, individual
> skills are often learned from scratch despite the high likelihood that some
> generalizable knowledge is shared across all task-specific policies belonging
> to a single robot embodiment. This work aims to define a paradigm for
> pretraining neural network models that encapsulate such knowledge and can
> subsequently serve as a basis for warm-starting the RL process in classic
> actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin
> with a task-agnostic exploration-based data collection algorithm to gather
> diverse, dynamic transition data, which is then used to train a Proprioceptive
> Inverse Dynamics Model (PIDM) through supervised learning. The pretrained
> weights are loaded into both the actor and critic networks to warm-start the
> policy optimization of actual tasks. We systematically validated our proposed
> method on seven distinct robot motion control tasks, showing significant
> benefits to this initialization strategy. Our proposed approach on average
> improves sample efficiency by 40.1% and task performance by 7.5%, compared to
> random initialization. We further present key ablation studies and empirical
> analyses that shed light on the mechanisms behind the effectiveness of our
> method.

