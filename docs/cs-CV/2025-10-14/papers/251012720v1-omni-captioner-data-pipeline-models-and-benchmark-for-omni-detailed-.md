---
layout: default
title: Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception
---

# Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception

**arXiv**: [2510.12720v1](https://arxiv.org/abs/2510.12720) | [PDF](https://arxiv.org/pdf/2510.12720.pdf)

**ä½œè€…**: Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmni-Captioneræ•°æ®ç®¡é“ä¸Žæ¨¡åž‹ï¼Œä»¥è§£å†³å…¨æ¨¡æ€ç»†ç²’åº¦æ„ŸçŸ¥ä¸­çš„ç»†èŠ‚ä¸Žå¹»è§‰é—®é¢˜ã€‚**

**å…³é”®è¯**: `å…¨æ¨¡æ€æ„ŸçŸ¥` `ç»†ç²’åº¦æè¿°` `æ•°æ®ç”Ÿæˆç®¡é“` `éŸ³é¢‘-è§†è§‰æ¨¡åž‹` `åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰å…¨æ¨¡æ€è¯­è¨€æ¨¡åž‹åœ¨ç»†ç²’åº¦æ„ŸçŸ¥ä¸­å­˜åœ¨ç»†èŠ‚ä¸Žå¹»è§‰å…±å¢žé•¿é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘Omni-Detectiveæ•°æ®ç”Ÿæˆç®¡é“ï¼Œè‡ªåŠ¨äº§ç”Ÿé«˜ç»†èŠ‚ã€ä½Žå¹»è§‰çš„å¤šæ¨¡æ€æ•°æ®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šOmni-Captioneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä¼˜æˆ–æœ€ä½³å¹³è¡¡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained perception of multimodal information is critical for advancing
> human-AI interaction. With recent progress in audio-visual technologies, Omni
> Language Models (OLMs), capable of processing audio and video signals in
> parallel, have emerged as a promising paradigm for achieving richer
> understanding and reasoning. However, their capacity to capture and describe
> fine-grained details remains limited explored. In this work, we present a
> systematic and comprehensive investigation of omni detailed perception from the
> perspectives of the data pipeline, models, and benchmark. We first identify an
> inherent "co-growth" between detail and hallucination in current OLMs. To
> address this, we propose Omni-Detective, an agentic data generation pipeline
> integrating tool-calling, to autonomously produce highly detailed yet minimally
> hallucinatory multimodal data. Based on the data generated with Omni-Detective,
> we train two captioning models: Audio-Captioner for audio-only detailed
> perception, and Omni-Captioner for audio-visual detailed perception. Under the
> cascade evaluation protocol, Audio-Captioner achieves the best performance on
> MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
> delivering performance comparable to Gemini 2.5 Pro. On existing detailed
> captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
> achieves the best trade-off between detail and hallucination on the
> video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
> detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
> detailed audio, visual, and audio-visual captioning that ensures stable,
> efficient, and reliable assessment. Experimental results and analysis
> demonstrate the effectiveness of Omni-Detective in generating high-quality
> detailed captions, as well as the superiority of Omni-Cloze in evaluating such
> detailed captions.

