---
layout: default
title: Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training
---

# Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training

**arXiv**: [2510.12586v1](https://arxiv.org/abs/2510.12586) | [PDF](https://arxiv.org/pdf/2510.12586.pdf)

**ä½œè€…**: Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ¡†æž¶ä»¥æå‡åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡åž‹æ€§èƒ½**

**å…³é”®è¯**: `åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡åž‹` `è‡ªç›‘ç£é¢„è®­ç»ƒ` `æ‰©æ•£æ¨¡åž‹` `ä¸€è‡´æ€§æ¨¡åž‹` `ç«¯åˆ°ç«¯è®­ç»ƒ` `å›¾åƒç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡åž‹è®­ç»ƒå›°éš¾ä¸”æ€§èƒ½ä½ŽäºŽæ½œåœ¨ç©ºé—´æ¨¡åž‹
2. é‡‡ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒç¼–ç å™¨ï¼Œç»“åˆç«¯åˆ°ç«¯å¾®è°ƒè§£ç å™¨
3. åœ¨ImageNetä¸Šå®žçŽ°é«˜FIDåˆ†æ•°ï¼Œè¶…è¶ŠçŽ°æœ‰åƒç´ ç©ºé—´æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pixel-space generative models are often more difficult to train and generally
> underperform compared to their latent-space counterparts, leaving a persistent
> performance and efficiency gap. In this paper, we introduce a novel two-stage
> training framework that closes this gap for pixel-space diffusion and
> consistency models. In the first stage, we pre-train encoders to capture
> meaningful semantics from clean images while aligning them with points along
> the same deterministic sampling trajectory, which evolves points from the prior
> to the data distribution. In the second stage, we integrate the encoder with a
> randomly initialized decoder and fine-tune the complete model end-to-end for
> both diffusion and consistency models. Our training framework demonstrates
> strong empirical performance on ImageNet dataset. Specifically, our diffusion
> model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75
> number of function evaluations (NFE), surpassing prior pixel-space methods by a
> large margin in both generation quality and efficiency while rivaling leading
> VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our
> consistency model achieves an impressive FID of 8.82 in a single sampling step,
> significantly surpassing its latent-space counterpart. To the best of our
> knowledge, this marks the first successful training of a consistency model
> directly on high-resolution images without relying on pre-trained VAEs or
> diffusion models.

