---
layout: default
title: CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs
---

# CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs

**arXiv**: [2510.12184v1](https://arxiv.org/abs/2510.12184) | [PDF](https://arxiv.org/pdf/2510.12184.pdf)

**ä½œè€…**: Jiwan Kim, Kibum Kim, Sangwoo Seo, Chanyoung Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCompoDistillæ¡†æž¶ï¼Œé€šè¿‡æ³¨æ„åŠ›å¯¹é½è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­è§†è§‰æ„ŸçŸ¥è’¸é¦é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `çŸ¥è¯†è’¸é¦` `è§†è§‰æ³¨æ„åŠ›å¯¹é½` `ç»„åˆæŽ¨ç†` `è§†è§‰æ„ŸçŸ¥` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­éš¾ä»¥æœ‰æ•ˆä¼ é€’æ•™å¸ˆæ¨¡åž‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›
2. é€šè¿‡è§†è§‰æ³¨æ„åŠ›å¯¹é½æœºåˆ¶ï¼Œæ˜¾å¼ä¼˜åŒ–å­¦ç”Ÿæ¨¡åž‹çš„è§†è§‰æ³¨æ„åŠ›åˆ†å¸ƒ
3. å®žéªŒæ˜¾ç¤ºåœ¨ç»„åˆæŽ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶ä¿æŒè§†è§‰é—®ç­”ä»»åŠ¡è¡¨çŽ°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, efficient Multimodal Large Language Models (MLLMs) have gained
> significant attention as a solution to their high computational complexity,
> making them more practical for real-world applications. In this regard, the
> knowledge distillation (KD) approach has emerged as a promising alternative,
> which transfers the rich visual and linguistic knowledge from a larger model
> (teacher) to a smaller model (student). However, we observe that existing KD
> methods struggle to effectively distill the teacher MLLM's rich visual
> perception abilities to the student, a challenge that has been largely
> overlooked in previous studies. Through a systematic analysis, we identify
> visual attention misalignment between student and teacher as the main cause of
> this issue. Based on this insight, we propose CompoDistill, a novel KD
> framework that explicitly aligns the student's visual attention with that of
> the teacher to enhance the student's visual perception abilities. Our extensive
> experiments show that CompoDistill significantly improves performance on
> compositional reasoning tasks that require visual perception abilities while
> maintaining strong performance on visual question answering tasks, as done in
> existing studies. Furthermore, CompoDistill demonstrates effectiveness with a
> more advanced backbone, highlighting its generalizability.

