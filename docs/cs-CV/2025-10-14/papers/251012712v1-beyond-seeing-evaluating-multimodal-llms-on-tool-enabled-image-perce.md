---
layout: default
title: Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning
---

# Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning

**arXiv**: [2510.12712v1](https://arxiv.org/abs/2510.12712) | [PDF](https://arxiv.org/pdf/2510.12712.pdf)

**ä½œè€…**: Xingang Guo, Utkarsh Tyagi, Advait Gosai, Paula Vergara, Ernesto Gabriel HernÃ¡ndez Montoya, Chen Bo Calvin Zhang, Bin Hu, Yunzhong He, Bing Liu, Rakshith Sharma Srinivasa

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIRISåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å·¥å…·è¾…åŠ©å›¾åƒæ„ŸçŸ¥ã€å˜æ¢ä¸ŽæŽ¨ç†ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å›¾åƒäº¤äº’æŽ¨ç†` `å·¥å…·é›†æˆ` `åŸºå‡†è¯„ä¼°` `è§†è§‰å˜æ¢` `å¼€æ”¾ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†å¤šå°†å›¾åƒè§†ä¸ºé™æ€è¾“å…¥ï¼Œæœªå……åˆ†æŽ¢ç´¢MLLMsåœ¨åŠ¨æ€å›¾åƒå˜æ¢ä¸Žå·¥å…·é›†æˆä¸­çš„èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥IRISåŸºå‡†ï¼ŒåŒ…å«1,204ä¸ªå¼€æ”¾è§†è§‰ä»»åŠ¡ï¼Œæ¶µç›–æ„ŸçŸ¥ã€å˜æ¢å’ŒæŽ¨ç†ï¼Œæ”¯æŒå¤šè½®äº¤äº’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå½“å‰MLLMsè¡¨çŽ°ä¸ä½³ï¼Œæœ€å¼ºæ¨¡åž‹ä»…è¾¾18.68%é€šè¿‡çŽ‡ï¼Œå·¥å…·ä½¿ç”¨è¡Œä¸ºå­˜åœ¨å·®å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) are increasingly applied in
> real-world scenarios where user-provided images are often imperfect, requiring
> active image manipulations such as cropping, editing, or enhancement to uncover
> salient visual cues. Beyond static visual perception, MLLMs must also think
> with images: dynamically transforming visual content and integrating it with
> other tools to solve complex tasks. However, this shift from treating vision as
> passive context to a manipulable cognitive workspace remains underexplored.
> Most existing benchmarks still follow a think about images paradigm, where
> images are regarded as static inputs. To address this gap, we introduce IRIS,
> an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability
> to perceive, transform, and reason across complex visual-textual tasks under
> the think with images paradigm. IRIS comprises 1,204 challenging, open-ended
> vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse
> domains, each paired with detailed rubrics to enable systematic evaluation. Our
> evaluation shows that current MLLMs struggle with tasks requiring effective
> integration of vision and general-purpose tools. Even the strongest model
> (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent
> tool-use behaviors, with OpenAI models benefiting from diverse image
> manipulations while Gemini-2.5-pro shows no improvement. By introducing the
> first benchmark centered on think with images, IRIS offers critical insights
> for advancing visual intelligence in MLLMs.

