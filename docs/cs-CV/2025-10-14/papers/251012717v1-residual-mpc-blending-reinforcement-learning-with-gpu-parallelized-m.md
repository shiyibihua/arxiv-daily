---
layout: default
title: Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control
---

# Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control

**arXiv**: [2510.12717v1](https://arxiv.org/abs/2510.12717) | [PDF](https://arxiv.org/pdf/2510.12717.pdf)

**ä½œè€…**: Se Hwan Jeon, Ho Jae Lee, Seungwoo Hong, Sangbae Kim

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ®‹å·®MPCæ¶æ„ï¼Œèåˆå¼ºåŒ–å­¦ä¹ ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥æå‡æœºå™¨äººè¿åŠ¨æ§åˆ¶æ€§èƒ½ã€‚**

**å…³é”®è¯**: `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `æ®‹å·®æ¶æ„` `GPUå¹¶è¡ŒåŒ–` `æœºå™¨äººè¿åŠ¨æ§åˆ¶` `æ‰­çŸ©æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡å‹é¢„æµ‹æ§åˆ¶å—é™äºæ¨¡å‹å¤±é…å’Œå®æ—¶è®¡ç®—ï¼Œå¼ºåŒ–å­¦ä¹ ç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æ‰­çŸ©æ§åˆ¶å±‚èåˆMPCä¸RLè¾“å‡ºï¼Œåˆ©ç”¨GPUå¹¶è¡ŒåŒ–å®ç°é«˜æ•ˆè®­ç»ƒã€‚
3. å®éªŒæ•ˆæœï¼šç›¸æ¯”å•ç‹¬æ–¹æ³•ï¼Œæ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œé€‚åº”æœªè§æ­¥æ€å’Œåœ°å½¢ï¼Œæå‡å¥–åŠ±å’Œé€Ÿåº¦è·Ÿè¸ªèŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model Predictive Control (MPC) provides interpretable, tunable locomotion
> controllers grounded in physical models, but its robustness depends on frequent
> replanning and is limited by model mismatch and real-time computational
> constraints. Reinforcement Learning (RL), by contrast, can produce highly
> robust behaviors through stochastic training but often lacks interpretability,
> suffers from out-of-distribution failures, and requires intensive reward
> engineering. This work presents a GPU-parallelized residual architecture that
> tightly integrates MPC and RL by blending their outputs at the torque-control
> level. We develop a kinodynamic whole-body MPC formulation evaluated across
> thousands of agents in parallel at 100 Hz for RL training. The residual policy
> learns to make targeted corrections to the MPC outputs, combining the
> interpretability and constraint handling of model-based control with the
> adaptability of RL. The model-based control prior acts as a strong bias,
> initializing and guiding the policy towards desirable behavior with a simple
> set of rewards. Compared to standalone MPC or end-to-end RL, our approach
> achieves higher sample efficiency, converges to greater asymptotic rewards,
> expands the range of trackable velocity commands, and enables zero-shot
> adaptation to unseen gaits and uneven terrain.

