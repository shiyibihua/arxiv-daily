---
layout: default
title: UniFusion: Vision-Language Model as Unified Encoder in Image Generation
---

# UniFusion: Vision-Language Model as Unified Encoder in Image Generation

**arXiv**: [2510.12789v1](https://arxiv.org/abs/2510.12789) | [PDF](https://arxiv.org/pdf/2510.12789.pdf)

**ä½œè€…**: Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniFusionï¼Œåˆ©ç”¨å†»ç»“VLMä½œä¸ºç»Ÿä¸€ç¼–ç å™¨ï¼Œæå‡æ‰©æ•£æ¨¡åž‹çš„è·¨æ¨¡æ€ç”Ÿæˆä¸Žç¼–è¾‘èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ‰©æ•£æ¨¡åž‹` `è·¨æ¨¡æ€ç”Ÿæˆ` `å›¾åƒç¼–è¾‘` `ç»Ÿä¸€ç¼–ç å™¨` `é›¶æ ·æœ¬æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ‰©æ•£æ¨¡åž‹ä¾èµ–ç‹¬ç«‹å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œé™åˆ¶è·¨æ¨¡æ€æŽ¨ç†å’ŒçŸ¥è¯†è¿ç§»ã€‚
2. UniFusionå¼•å…¥Layerwise Attention Poolingæœºåˆ¶ï¼Œä»Žå†»ç»“VLMæå–å¤šå±‚çº§è¯­ä¹‰å’Œç»†èŠ‚ï¼Œæ¡ä»¶åŒ–æ‰©æ•£ç”Ÿæˆã€‚
3. å®žéªŒæ˜¾ç¤ºï¼ŒLAPåœ¨æ–‡æœ¬-å›¾åƒå¯¹é½å’Œè§†è§‰ä¿¡æ¯è¿ç§»ä¸Šä¼˜äºŽæµ…å±‚èžåˆï¼Œå¹¶é›¶æ ·æœ¬æ³›åŒ–åˆ°å¤šå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Although recent advances in visual generation have been remarkable, most
> existing architectures still depend on distinct encoders for images and text.
> This separation constrains diffusion models' ability to perform cross-modal
> reasoning and knowledge transfer. Prior attempts to bridge this gap often use
> the last layer information from VLM, employ multiple visual encoders, or train
> large unified models jointly for text and image generation, which demands
> substantial computational resources and large-scale data, limiting its
> accessibility.We present UniFusion, a diffusion-based generative model
> conditioned on a frozen large vision-language model (VLM) that serves as a
> unified multimodal encoder. At the core of UniFusion is the Layerwise Attention
> Pooling (LAP) mechanism that extracts both high level semantics and low level
> details from text and visual tokens of a frozen VLM to condition a diffusion
> generative model. We demonstrate that LAP outperforms other shallow fusion
> architectures on text-image alignment for generation and faithful transfer of
> visual information from VLM to the diffusion model which is key for editing. We
> propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),
> which conditions a diffusion transformer (DiT) only on the text tokens
> generated by the VLM during in-model prompt rewriting. VERIFI combines the
> alignment of the conditioning distribution with the VLM's reasoning
> capabilities for increased capabilities and flexibility at inference. In
> addition, finetuning on editing task not only improves text-image alignment for
> generation, indicative of cross-modality knowledge transfer, but also exhibits
> tremendous generalization capabilities. Our model when trained on single image
> editing, zero-shot generalizes to multiple image references further motivating
> the unified encoder design of UniFusion.

