---
layout: default
title: DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning
---

# DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning

**arXiv**: [2510.12107v1](https://arxiv.org/abs/2510.12107) | [PDF](https://arxiv.org/pdf/2510.12107.pdf)

**ä½œè€…**: Jiawei Zhan, Jun Liu, Jinlong Peng, Xiaochen Chen, Bin-Bin Gao, Yong Liu, Chengjie Wang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRLæ¡†æ¶ï¼Œé€šè¿‡å¹¶è¡Œé€‚é…å™¨å’Œè§£è€¦é”šç›‘ç£è§£å†³ç±»å¢é‡å­¦ä¹ ä¸­çš„æ¨¡å‹å¤æ‚åº¦å’Œè¡¨ç¤ºä¸ä¸€è‡´é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç±»å¢é‡å­¦ä¹ ` `é¢„è®­ç»ƒæ¨¡å‹` `å¹¶è¡Œé€‚é…å™¨` `è§£è€¦é”šç›‘ç£` `è¡¨ç¤ºå­¦ä¹ ` `è½»é‡çº§å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç±»å¢é‡å­¦ä¹ ä¸­æ¨¡å‹å¤æ‚åº¦å¢åŠ ã€è¡¨ç¤ºåç§»ä¸å¹³æ»‘åŠé˜¶æ®µä¼˜åŒ–ä¸å…¨å±€æ¨æ–­ä¸ä¸€è‡´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¢é‡å¹¶è¡Œé€‚é…å™¨ç½‘ç»œï¼Œè½»é‡çº§é€‚é…å™¨é€šè¿‡ä¼ è¾“é—¨ç»§æ‰¿è¡¨ç¤ºèƒ½åŠ›ï¼Œç¡®ä¿å¹³æ»‘è¡¨ç¤ºè½¬ç§»ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDRLæŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè®­ç»ƒå’Œæ¨æ–­æ•ˆç‡é«˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the excellent representation capabilities of Pre-Trained Models (PTMs),
> remarkable progress has been made in non-rehearsal Class-Incremental Learning
> (CIL) research. However, it remains an extremely challenging task due to three
> conundrums: increasingly large model complexity, non-smooth representation
> shift during incremental learning and inconsistency between stage-wise
> sub-problem optimization and global inference. In this work, we propose the
> Discriminative Representation Learning (DRL) framework to specifically address
> these challenges. To conduct incremental learning effectively and yet
> efficiently, the DRL's network, called Incremental Parallel Adapter (IPA)
> network, is built upon a PTM and increasingly augments the model by learning a
> lightweight adapter with a small amount of parameter learning overhead in each
> incremental stage. The adapter is responsible for adapting the model to new
> classes, it can inherit and propagate the representation capability from the
> current model through parallel connection between them by a transfer gate. As a
> result, this design guarantees a smooth representation shift between different
> incremental stages. Furthermore, to alleviate inconsistency and enable
> comparable feature representations across incremental stages, we design the
> Decoupled Anchor Supervision (DAS). It decouples constraints of positive and
> negative samples by respectively comparing them with the virtual anchor. This
> decoupling promotes discriminative representation learning and aligns the
> feature spaces learned at different stages, thereby narrowing the gap between
> stage-wise local optimization over a subset of data and global inference across
> all classes. Extensive experiments on six benchmarks reveal that our DRL
> consistently outperforms other state-of-the-art methods throughout the entire
> CIL period while maintaining high efficiency in both training and inference
> phases.

