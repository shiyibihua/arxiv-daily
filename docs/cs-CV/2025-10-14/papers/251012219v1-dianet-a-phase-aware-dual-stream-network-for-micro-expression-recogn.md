---
layout: default
title: DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images
---

# DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images

**arXiv**: [2510.12219v1](https://arxiv.org/abs/2510.12219) | [PDF](https://arxiv.org/pdf/2510.12219.pdf)

**ä½œè€…**: Vu Tram Anh Khuong, Luu Tu Nguyen, Thi Bich Phuong Man, Thanh Ha Le, Thi Duyen Ngo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDIANetåŒæµç½‘ç»œï¼Œé€šè¿‡ç›¸ä½æ„ŸçŸ¥åŠ¨æ€å›¾åƒè§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­å¿½ç•¥ä¸åŒæ—¶é—´ç›¸ä½çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¾®è¡¨æƒ…è¯†åˆ«` `åŠ¨æ€å›¾åƒ` `åŒæµç½‘ç»œ` `ç›¸ä½æ„ŸçŸ¥` `äº¤å‰æ³¨æ„åŠ›` `æ—¶é—´å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¾®è¡¨æƒ…è¯†åˆ«å› è¡¨æƒ…çŸ­æš‚ã€æ•°æ®ç¨€ç¼ºåŠä¼ ç»Ÿæ–¹æ³•å¿½ç•¥ä¸åŒæ—¶é—´ç›¸ä½ç‰¹å¾è€Œå›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŒæµç½‘ç»œåˆ†åˆ«å¤„ç†èµ·å§‹åˆ°é¡¶ç‚¹å’Œé¡¶ç‚¹åˆ°ç»“æŸç›¸ä½ï¼Œå¹¶é‡‡ç”¨äº¤å‰æ³¨æ„åŠ›èžåˆæ¨¡å—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºŽå•ç›¸ä½æ–¹æ³•ï¼ŒéªŒè¯äº†å»ºæ¨¡æ—¶é—´ç›¸ä½çš„é‡è¦æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Micro-expressions are brief, involuntary facial movements that typically last
> less than half a second and often reveal genuine emotions. Accurately
> recognizing these subtle expressions is critical for applications in
> psychology, security, and behavioral analysis. However, micro-expression
> recognition (MER) remains a challenging task due to the subtle and transient
> nature of facial cues and the limited availability of annotated data. While
> dynamic image (DI) representations have been introduced to summarize temporal
> motion into a single frame, conventional DI-based methods often overlook the
> distinct characteristics of different temporal phases within a
> micro-expression. To address this issue, this paper proposes a novel
> dual-stream framework, DIANet, which leverages phase-aware dynamic images - one
> encoding the onset-to-apex phase and the other capturing the apex-to-offset
> phase. Each stream is processed by a dedicated convolutional neural network,
> and a cross-attention fusion module is employed to adaptively integrate
> features from both streams based on their contextual relevance. Extensive
> experiments conducted on three benchmark MER datasets (CASME-II, SAMM, and
> MMEW) demonstrate that the proposed method consistently outperforms
> conventional single-phase DI-based approaches. The results highlight the
> importance of modeling temporal phase information explicitly and suggest a
> promising direction for advancing MER.

