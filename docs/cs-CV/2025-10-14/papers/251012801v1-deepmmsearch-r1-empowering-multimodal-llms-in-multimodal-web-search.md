---
layout: default
title: DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search
---

# DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search

**arXiv**: [2510.12801v1](https://arxiv.org/abs/2510.12801) | [PDF](https://arxiv.org/pdf/2510.12801.pdf)

**ä½œè€…**: Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepMMSearch-R1ä»¥è§£å†³å¤šæ¨¡æ€LLMåœ¨åŠ¨æ€ç½‘ç»œæœç´¢ä¸­çš„æ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `ç½‘ç»œæœç´¢` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€VQAæ•°æ®é›†` `å›¾åƒæœç´¢ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å­˜åœ¨ç®¡é“åƒµåŒ–ã€æœç´¢è°ƒç”¨è¿‡å¤šå’ŒæŸ¥è¯¢æž„å»ºä¸ä½³ç­‰é—®é¢˜
2. é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼šç›‘ç£å¾®è°ƒä¸Žåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ”¯æŒå¤šè½®å›¾åƒå’Œæ–‡æœ¬æœç´¢
3. åœ¨çŸ¥è¯†å¯†é›†åž‹åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜è¶Šï¼Œæä¾›å¤šæ¨¡æ€ç½‘ç»œæœç´¢æ´žè§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) in real-world applications require
> access to external knowledge sources and must remain responsive to the dynamic
> and ever-changing real-world information in order to address
> information-seeking and knowledge-intensive user queries. Existing approaches,
> such as retrieval augmented generation (RAG) methods, search agents, and search
> equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
> poorly constructed search queries, which result in inefficiencies and
> suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
> the first multimodal LLM capable of performing on-demand, multi-turn web
> searches and dynamically crafting queries for both image and text search tools.
> Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
> of the input image making the image search more effective, and can iteratively
> adapt text search queries based on retrieved information, thereby enabling
> self-reflection and self-correction. Our approach relies on a two-stage
> training pipeline: a cold start supervised finetuning phase followed by an
> online reinforcement learning optimization. For training, we introduce
> DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
> pipeline intermixed with real-world information from web search tools. This
> dataset contains diverse, multi-hop queries that integrate textual and visual
> information, teaching the model when to search, what to search for, which
> search tool to use and how to reason over the retrieved information. We conduct
> extensive experiments across a range of knowledge-intensive benchmarks to
> demonstrate the superiority of our approach. Finally, we analyze the results
> and provide insights that are valuable for advancing multimodal web-search.

