---
layout: default
title: Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback
---

# Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback

**arXiv**: [2510.12089v1](https://arxiv.org/abs/2510.12089) | [PDF](https://arxiv.org/pdf/2510.12089.pdf)

**ä½œè€…**: Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ‰©æ•£å˜æ¢å™¨çš„è®­ç»ƒå…è´¹å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»æ–¹æ³•ï¼Œä»¥æå‡å”‡åŒæ­¥å’Œé•¿è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚**

**å…³é”®è¯**: `æ‰©æ•£å˜æ¢å™¨` `éŸ³é¢‘é©±åŠ¨åŠ¨ç”»` `å¤šè§’è‰²ç”Ÿæˆ` `è®­ç»ƒå…è´¹æ–¹æ³•` `é•¿è§†é¢‘ç”Ÿæˆ` `å”‡åŒæ­¥å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨å”‡åŒæ­¥ç²¾åº¦ã€é•¿è§†é¢‘æ—¶åºä¸€è‡´æ€§å’Œå¤šè§’è‰²åŠ¨ç”»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨LoRAè®­ç»ƒä¸ä½ç½®åç§»æ¨ç†ï¼Œç»“åˆå¥–åŠ±åé¦ˆå¢å¼ºå”‡åŒæ­¥å’Œèº«ä½“è¿åŠ¨è‡ªç„¶æ€§ã€‚
3. å®éªŒæ•ˆæœï¼šåœ¨å®éªŒä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå®ç°é«˜è´¨é‡ã€æ—¶åºä¸€è‡´çš„å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in diffusion models have significantly improved audio-driven
> human video generation, surpassing traditional methods in both quality and
> controllability. However, existing approaches still face challenges in lip-sync
> accuracy, temporal coherence for long video generation, and multi-character
> animation. In this work, we propose a diffusion transformer (DiT)-based
> framework for generating lifelike talking videos of arbitrary length, and
> introduce a training-free method for multi-character audio-driven animation.
> First, we employ a LoRA-based training strategy combined with a position shift
> inference approach, which enables efficient long video generation while
> preserving the capabilities of the foundation model. Moreover, we combine
> partial parameter updates with reward feedback to enhance both lip
> synchronization and natural body motion. Finally, we propose a training-free
> approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character
> animation, which requires no specialized datasets or model modifications and
> supports audio-driven animation for three or more characters. Experimental
> results demonstrate that our method outperforms existing state-of-the-art
> approaches, achieving high-quality, temporally coherent, and multi-character
> audio-driven video generation in a simple, efficient, and cost-effective
> manner.

