---
layout: default
title: RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension
---

# RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.06276" target="_blank" class="toolbar-btn">arXiv: 2512.06276v2</a>
    <a href="https://arxiv.org/pdf/2512.06276.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06276v2" 
            onclick="toggleFavorite(this, '2512.06276v2', 'RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Tianyi Gao, Hao Li, Han Fang, Xin Wei, Xiaodong Dong, Hongbo Sun, Ye Yuan, Zhongjiang He, Jinglin Xu, Jingmin Xin, Hao Sun

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-06 (Êõ¥Êñ∞: 2025-12-13)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RefBench-PROÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÂú®Êåá‰ª£Ë°®ËææÁêÜËß£‰∏≠ÁöÑÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Êåá‰ª£Ë°®ËææÁêÜËß£` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ËßÜËßâËØ≠Ë®Ä` `Âü∫ÂáÜÊµãËØï` `Êé®ÁêÜËÉΩÂäõ` `ÊÑüÁü•ËÉΩÂäõ` `Âº∫ÂåñÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRECÂü∫ÂáÜ‰æßÈáçÊÑüÁü•ËÉΩÂäõËØÑ‰º∞ÔºåÁº∫‰πèÂØπÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÈíàÂØπÊÄßËØÑ‰º∞ÂíåÂèØËß£ÈáäÁöÑËØÑÂàÜÊú∫Âà∂„ÄÇ
2. RefBench-PROÂü∫ÂáÜÂ∞ÜÊåá‰ª£Ë°®ËææÁêÜËß£ÂàÜËß£‰∏∫ÊÑüÁü•ÂíåÊé®ÁêÜ‰∏§‰∏™Áª¥Â∫¶ÔºåÂπ∂ÁªÜÂàÜ‰∏∫ÂÖ≠‰∏™Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÂ≠ê‰ªªÂä°„ÄÇ
3. ÊèêÂá∫‰∫ÜRef-R1Â≠¶‰π†ÊñπÊ°àÔºåÈÄöËøáÁªìÂêàÂä®ÊÄÅIoUÁöÑGRPOÔºåÊèêÂçá‰∫ÜÂú®Â§çÊùÇÊé®ÁêÜÊù°‰ª∂‰∏ãÁöÑÂÆö‰ΩçÁ≤æÂ∫¶ÔºåÂπ∂Âª∫Á´ã‰∫ÜÊõ¥Âº∫ÁöÑÂü∫Á∫ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êåá‰ª£Ë°®ËææÁêÜËß£ÔºàRECÔºâÊòØ‰∏ÄÈ°πËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÔºåÊó®Âú®Ê†πÊçÆÊñáÊú¨ÊèèËø∞ÂÆö‰ΩçÁâπÂÆöÁöÑÂõæÂÉèÂå∫Âüü„ÄÇÁé∞ÊúâÁöÑRECÂü∫ÂáÜ‰∏ªË¶ÅËØÑ‰º∞ÊÑüÁü•ËÉΩÂäõÔºåÁº∫‰πèÂèØËß£ÈáäÁöÑËØÑÂàÜÊú∫Âà∂ÔºåÊó†Ê≥ïÊè≠Á§∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂú®‰∏çÂêåËÆ§Áü•ËÉΩÂäõ‰∏äÁöÑÂü∫Á°ÄËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÂ±ÄÈôêÊÄßÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜRefBench-PROÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑRECÂü∫ÂáÜÔºåÂÆÉÂ∞ÜÊåá‰ª£Ë°®ËææÂàÜËß£‰∏∫‰∏§‰∏™Ê†∏ÂøÉÁª¥Â∫¶ÔºåÂç≥ÊÑüÁü•ÂíåÊé®ÁêÜÔºåÂπ∂Ëøõ‰∏ÄÊ≠•ÁªÜÂàÜ‰∏∫ÂÖ≠‰∏™Ê∏êËøõÂºèÊåëÊàò‰ªªÂä°ÔºåÂ¶ÇÂ±ûÊÄß„ÄÅ‰ΩçÁΩÆ„ÄÅ‰∫§‰∫í„ÄÅÂ∏∏ËØÜ„ÄÅÂÖ≥Á≥ªÂíåÊãíÁªù„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂÖ®Ëá™Âä®ÁöÑÊï∞ÊçÆÁîüÊàêÁÆ°ÈÅìÔºåÁî®‰∫éÁîüÊàêË∑®ËøôÂÖ≠‰∏™Â≠êÁª¥Â∫¶ÁöÑÂ§öÊ†∑ÂåñÊåá‰ª£Ë°®Ëææ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜRef-R1Ôºå‰∏ÄÁßçÂü∫‰∫éRLÁöÑÂ≠¶‰π†ÊñπÊ°àÔºåÂÆÉÁªìÂêà‰∫ÜÂü∫‰∫éÂä®ÊÄÅIoUÁöÑGRPOÔºå‰ª•ÊèêÈ´òÂú®Êó•ÁõäÂ§çÊùÇÁöÑÊé®ÁêÜÊù°‰ª∂‰∏ãÁöÑÂÆö‰ΩçÁ≤æÂ∫¶Ôºå‰∏∫RECÂª∫Á´ãÊõ¥Âº∫ÁöÑÂü∫Á∫ø„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑRefBench-PROËÉΩÂ§üÂØπMLLMÂú®Êåá‰ª£Ë°®ËææÁêÜËß£ÊñπÈù¢ËøõË°åÂèØËß£ÈáäÁöÑËØÑ‰º∞ÔºåÂú®ÊÑüÁü•ÂíåÊé®ÁêÜÊñπÈù¢ÈÉΩÊèêÂá∫‰∫ÜÊõ¥Â§ßÁöÑÊåëÊàò„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊåá‰ª£Ë°®ËææÁêÜËß£ÔºàRECÔºâÂü∫ÂáÜÁöÑ‰∏çË∂≥ÔºåÂç≥‰∏ªË¶Å‰æßÈáç‰∫éÊÑüÁü•ËÉΩÂäõÁöÑËØÑ‰º∞ÔºåËÄåÂøΩÁï•‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂØπMLLMÂú®‰∏çÂêåËÆ§Áü•ËÉΩÂäõ‰∏äÁöÑÂü∫Á°ÄËÉΩÂäõËøõË°åÊúâÊïàËØÑ‰º∞ÔºåÁº∫‰πèÂèØËß£ÈáäÁöÑËØÑÂàÜÊú∫Âà∂„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊåá‰ª£Ë°®ËææÁêÜËß£‰ªªÂä°ÂàÜËß£‰∏∫ÊÑüÁü•ÂíåÊé®ÁêÜ‰∏§‰∏™Ê†∏ÂøÉÁª¥Â∫¶ÔºåÂπ∂Ëøõ‰∏ÄÊ≠•ÁªÜÂàÜ‰∏∫ÂÖ≠‰∏™ÂÖ∑ÊúâÈÄíËøõÈöæÂ∫¶ÁöÑÂ≠ê‰ªªÂä°„ÄÇÈÄöËøáËøôÁßçÂàÜËß£ÔºåÂèØ‰ª•Êõ¥Á≤æÁªÜÂú∞ËØÑ‰º∞MLLMÂú®‰∏çÂêåËÆ§Áü•ËÉΩÂäõ‰∏äÁöÑË°®Áé∞ÔºåÂπ∂Êèê‰æõÊõ¥ÂÖ∑Ëß£ÈáäÊÄßÁöÑËØÑ‰º∞ÁªìÊûú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRefBench-PROÂü∫ÂáÜÂåÖÂê´‰∏Ä‰∏™ÂÖ®Ëá™Âä®ÁöÑÊï∞ÊçÆÁîüÊàêÁÆ°ÈÅìÔºåÁî®‰∫éÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊåá‰ª£Ë°®ËææÔºåÊ∂µÁõñÂ±ûÊÄß„ÄÅ‰ΩçÁΩÆ„ÄÅ‰∫§‰∫í„ÄÅÂ∏∏ËØÜ„ÄÅÂÖ≥Á≥ªÂíåÊãíÁªùÂÖ≠‰∏™Â≠êÁª¥Â∫¶„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜRef-R1Â≠¶‰π†ÊñπÊ°àÔºåËØ•ÊñπÊ°àÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºåÂπ∂ÁªìÂêà‰∫ÜÂä®ÊÄÅIoUÁöÑGRPOÔºàÊú™Áü•ÂÖ∑‰ΩìÂê´‰πâÔºâÔºå‰ª•ÊèêÈ´òÂú®Â§çÊùÇÊé®ÁêÜÊù°‰ª∂‰∏ãÁöÑÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÁîüÊàê„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜRefBench-PROÂü∫ÂáÜÔºåËØ•Âü∫ÂáÜËÉΩÂ§üÂØπMLLMÂú®Êåá‰ª£Ë°®ËææÁêÜËß£‰∏≠ÁöÑÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõËøõË°åÊõ¥ÂÖ®Èù¢„ÄÅÊõ¥Á≤æÁªÜÁöÑËØÑ‰º∞„ÄÇ‰∏éÁé∞ÊúâÂü∫ÂáÜÁõ∏ÊØîÔºåRefBench-PROÊõ¥Ê≥®ÈáçÊé®ÁêÜËÉΩÂäõÁöÑËØÑ‰º∞ÔºåÂπ∂Êèê‰æõ‰∫ÜÊõ¥ÂÖ∑Ëß£ÈáäÊÄßÁöÑËØÑ‰º∞ÁªìÊûú„ÄÇÊ≠§Â§ñÔºåRef-R1Â≠¶‰π†ÊñπÊ°àÁöÑÂºïÂÖ•‰πü‰∏∫REC‰ªªÂä°Êèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËß£ÂÜ≥ÊÄùË∑Ø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥‰∫éÊï∞ÊçÆÁîüÊàêÁÆ°ÈÅìÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇ„ÄÅÂä®ÊÄÅIoU-based GRPOÁöÑÂÖ∑‰ΩìÁÆóÊ≥ïÁªÜËäÇ„ÄÅÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°„ÄÅ‰ª•ÂèäÁΩëÁªúÁªìÊûÑÁöÑÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÁ≠âÊäÄÊúØÁªÜËäÇÔºåËÆ∫ÊñáÊëòË¶Å‰∏≠Êú™Êèê‰æõËØ¶ÁªÜ‰ø°ÊÅØÔºåÂõ†Ê≠§Êó†Ê≥ïËøõË°åÊ∑±ÂÖ•ÊèèËø∞„ÄÇËøô‰∫õÁªÜËäÇÂèØËÉΩÂú®ËÆ∫ÊñáÊ≠£Êñá‰∏≠ÊúâÊâÄÈòêËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRefBench-PROÂü∫ÂáÜËÉΩÂ§üÊúâÊïàËØÑ‰º∞MLLMÂú®Êåá‰ª£Ë°®ËææÁêÜËß£‰∏≠ÁöÑÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõÔºåÂπ∂ÂØπÁé∞ÊúâÊ®°ÂûãÊèêÂá∫‰∫ÜÊõ¥Â§ßÁöÑÊåëÊàò„ÄÇRef-R1Â≠¶‰π†ÊñπÊ°àÂú®Â§çÊùÇÊé®ÁêÜÊù°‰ª∂‰∏ãÊòæËëóÊèêÈ´ò‰∫ÜÂÆö‰ΩçÁ≤æÂ∫¶Ôºå‰∏∫REC‰ªªÂä°Âª∫Á´ã‰∫Ü‰∏Ä‰∏™Êõ¥Âº∫ÁöÑÂü∫Á∫ø„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶ÈúÄË¶ÅÂú®ËÆ∫ÊñáÊ≠£Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂõæÂÉèÊêúÁ¥¢Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÂú®Êåá‰ª£Ë°®ËææÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÊú∫Âô®Êõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩÁöÑ‰∫∫Êú∫‰∫§‰∫íÂíåÊõ¥Á≤æÂáÜÁöÑÁõÆÊ†áÂÆö‰Ωç„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®ËßÜËßâ-ËØ≠Ë®ÄÊô∫ËÉΩÁöÑÂèëÂ±ïÔºåÂπ∂‰∏∫Áõ∏ÂÖ≥Â∫îÁî®Â∏¶Êù•Êõ¥ÂπøÈòîÁöÑÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

