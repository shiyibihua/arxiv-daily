---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-20
---

# cs.CVï¼ˆ2025-10-20ï¼‰

ğŸ“Š å…± **39** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251017722v1-mt-video-bench-a-holistic-video-understanding-benchmark-for-evaluati.html">MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues</a></td>
  <td>æå‡ºMT-Video-Benchï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€LLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„è§†é¢‘ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17722v1" onclick="toggleFavorite(this, '2510.17722v1', 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251017205v1-mathcalvisimathcalpruner-decoding-discontinuous-cross-modal-dynamics.html">$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs</a></td>
  <td>VisiPrunerï¼šè§£ç å¤šæ¨¡æ€LLMä¸­çš„éè¿ç»­è·¨æ¨¡æ€åŠ¨æ€ï¼Œå®ç°é«˜æ•ˆå‰ªæ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17205v1" onclick="toggleFavorite(this, '2510.17205v1', '$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251017078v1-towards-a-generalizable-fusion-architecture-for-multimodal-object-de.html">Towards a Generalizable Fusion Architecture for Multimodal Object Detection</a></td>
  <td>æå‡ºFMCAFæ¶æ„ï¼Œæå‡å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17078v1" onclick="toggleFavorite(this, '2510.17078v1', 'Towards a Generalizable Fusion Architecture for Multimodal Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251017800v2-glyph-scaling-context-windows-via-visual-text-compression.html">Glyph: Scaling Context Windows via Visual-Text Compression</a></td>
  <td>Glyphï¼šé€šè¿‡è§†è§‰-æ–‡æœ¬å‹ç¼©æ‰©å±•å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17800v2" onclick="toggleFavorite(this, '2510.17800v2', 'Glyph: Scaling Context Windows via Visual-Text Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251021795v1-xihe-scalable-zero-shot-time-series-learner-via-hierarchical-interle.html">Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention</a></td>
  <td>æå‡ºåŸºäºåˆ†å±‚äº¤é”™å—æ³¨æ„åŠ›ï¼ˆHIBAï¼‰çš„Xiheï¼Œç”¨äºå¯æ‰©å±•çš„é›¶æ ·æœ¬æ—¶é—´åºåˆ—å­¦ä¹ ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21795v1" onclick="toggleFavorite(this, '2510.21795v1', 'Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251017332v1-idetex-empowering-mllms-for-intelligent-detailed-explainable-iqa.html">iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA</a></td>
  <td>æå‡ºiDETEXï¼Œèµ‹èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°æ™ºèƒ½ã€è¯¦ç»†ã€å¯è§£é‡Šçš„å›¾åƒè´¨é‡è¯„ä¼°</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17332v1" onclick="toggleFavorite(this, '2510.17332v1', 'iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251017777v1-sparsevila-decoupling-visual-sparsity-for-efficient-vlm-inference.html">SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference</a></td>
  <td>SparseVILAï¼šè§£è€¦è§†è§‰ç¨€ç–æ€§ï¼ŒåŠ é€Ÿé«˜æ•ˆVLMæ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17777v1" onclick="toggleFavorite(this, '2510.17777v1', 'SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251017700v1-elastic-vits-from-pretrained-models-without-retraining.html">Elastic ViTs from Pretrained Models without Retraining</a></td>
  <td>æå‡ºSnapViTï¼Œæ— éœ€é‡è®­ç»ƒå³å¯ä»é¢„è®­ç»ƒViTæ¨¡å‹ä¸­è·å¾—å¼¹æ€§è®¡ç®—èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17700v1" onclick="toggleFavorite(this, '2510.17700v1', 'Elastic ViTs from Pretrained Models without Retraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251017617v1-imaggen-zero-shot-generation-of-co-speech-semantic-gestures-grounded.html">ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input</a></td>
  <td>ImaGGenï¼šåŸºäºè¯­è¨€å’Œå›¾åƒè¾“å…¥çš„é›¶æ ·æœ¬å…±è¯­è¯­ä¹‰æ‰‹åŠ¿ç”Ÿæˆ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17617v1" onclick="toggleFavorite(this, '2510.17617v1', 'ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251017501v3-context-aware-pseudo-label-scoring-for-zero-shot-video-summarization.html">Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization</a></td>
  <td>æå‡ºä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¼ªæ ‡ç­¾è¯„åˆ†çš„é›¶æ ·æœ¬è§†é¢‘æ‘˜è¦æ¡†æ¶ï¼Œæå‡LLMåœ¨è§†é¢‘æ‘˜è¦ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17501v3" onclick="toggleFavorite(this, '2510.17501v3', 'Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251017409v1-monitoring-horses-in-stalls-from-object-to-event-detection.html">Monitoring Horses in Stalls: From Object to Event Detection</a></td>
  <td>æå‡ºåŸºäºYOLOv11å’ŒBoT-SORTçš„é©¬å©é©¬åŒ¹è¡Œä¸ºç›‘æµ‹ç³»ç»Ÿï¼Œå®ç°äº‹ä»¶è‡ªåŠ¨æ£€æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17409v1" onclick="toggleFavorite(this, '2510.17409v1', 'Monitoring Horses in Stalls: From Object to Event Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251017364v1-recurrent-attention-based-token-selection-for-efficient-streaming-vi.html">Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs</a></td>
  <td>æå‡ºåŸºäºå¾ªç¯æ³¨æ„åŠ›çš„Tokené€‰æ‹©æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆçš„æµå¼è§†é¢‘-LLM</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17364v1" onclick="toggleFavorite(this, '2510.17364v1', 'Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251017347v1-exploring-the-missing-semantics-in-event-modality.html">Exploring The Missing Semantics In Event Modality</a></td>
  <td>æå‡ºSemantic-E2VIDï¼Œåˆ©ç”¨è§†è§‰è¯­ä¹‰çŸ¥è¯†å¢å¼ºäº‹ä»¶åˆ°è§†é¢‘çš„é‡å»ºæ•ˆæœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17347v1" onclick="toggleFavorite(this, '2510.17347v1', 'Exploring The Missing Semantics In Event Modality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251017790v2-ultracua-a-foundation-model-for-computer-use-agents-with-hybrid-acti.html">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a></td>
  <td>UltraCUAï¼šèåˆGUIæ“ä½œä¸é«˜çº§å·¥å…·çš„è®¡ç®—æœºä½¿ç”¨AgentåŸºç¡€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17790v2" onclick="toggleFavorite(this, '2510.17790v2', 'UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251017684v1-intelligent-communication-mixture-of-experts-boosted-medical-image-s.html">Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model</a></td>
  <td>æå‡ºIC-MoEæ¨¡å‹ï¼Œé€šè¿‡æ™ºèƒ½é€šä¿¡æ··åˆä¸“å®¶ç½‘ç»œæå‡åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17684v1" onclick="toggleFavorite(this, '2510.17684v1', 'Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251017384v1-closed-loop-transfer-for-weakly-supervised-affordance-grounding.html">Closed-Loop Transfer for Weakly-supervised Affordance Grounding</a></td>
  <td>æå‡ºLoopTransé—­ç¯æ¡†æ¶ï¼Œç”¨äºå¼±ç›‘ç£å¯ä¾›æ€§åŒºåŸŸå®šä½ï¼Œæå‡å¤æ‚äº¤äº’åœºæ™¯æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17384v1" onclick="toggleFavorite(this, '2510.17384v1', 'Closed-Loop Transfer for Weakly-supervised Affordance Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251017318v1-causalmamba-scalable-conditional-state-space-models-for-neural-causa.html">CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference</a></td>
  <td>CausalMambaï¼šç”¨äºç¥ç»å› æœæ¨æ–­çš„å¯æ‰©å±•æ¡ä»¶çŠ¶æ€ç©ºé—´æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17318v1" onclick="toggleFavorite(this, '2510.17318v1', 'CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251021794v1-token-level-inference-time-alignment-for-vision-language-models.html">Token-Level Inference-Time Alignment for Vision-Language Models</a></td>
  <td>æå‡ºTITAï¼šä¸€ç§ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹Tokençº§æ¨ç†æ—¶å¯¹é½çš„è½»é‡çº§æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21794v1" onclick="toggleFavorite(this, '2510.21794v1', 'Token-Level Inference-Time Alignment for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251018135v1-world-in-world-world-models-in-a-closed-loop-world.html">World-in-World: World Models in a Closed-Loop World</a></td>
  <td>World-in-Worldï¼šé¦–ä¸ªé—­ç¯ä¸–ç•Œæ¨¡å‹åŸºå‡†å¹³å°ï¼Œç”¨äºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„é¢„æµ‹æ„ŸçŸ¥èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18135v1" onclick="toggleFavorite(this, '2510.18135v1', 'World-in-World: World Models in a Closed-Loop World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251018117v1-online-in-context-distillation-for-low-resource-vision-language-mode.html">Online In-Context Distillation for Low-Resource Vision Language Models</a></td>
  <td>æå‡ºåœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œæå‡ä½èµ„æºè§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18117v1" onclick="toggleFavorite(this, '2510.18117v1', 'Online In-Context Distillation for Low-Resource Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251017482v3-sparseworld-a-flexible-adaptive-and-efficient-4d-occupancy-world-mod.html">SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries</a></td>
  <td>SparseWorldï¼šåŸºäºç¨€ç–åŠ¨æ€æŸ¥è¯¢çš„çµæ´»é«˜æ•ˆ4D Occupancyä¸–ç•Œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17482v3" onclick="toggleFavorite(this, '2510.17482v3', 'SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251017157v1-gaco-cad-geometry-augmented-and-conciseness-optimized-cad-model-gene.html">GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image</a></td>
  <td>GACO-CADï¼šé€šè¿‡å‡ ä½•å¢å¼ºä¸ç®€æ´æ€§ä¼˜åŒ–ï¼Œä»å•å¼ å›¾åƒç”ŸæˆCADæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17157v1" onclick="toggleFavorite(this, '2510.17157v1', 'GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251018101v1-from-volume-rendering-to-3d-gaussian-splatting-theory-and-applicatio.html">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a></td>
  <td>ç»¼è¿°3Dé«˜æ–¯æº…å°„ï¼šä»ä½“æ¸²æŸ“åˆ°åº”ç”¨ï¼Œè§£å†³å®æ—¶æ¸²æŸ“ä¸é«˜è´¨é‡é‡å»ºéš¾é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18101v1" onclick="toggleFavorite(this, '2510.18101v1', 'From Volume Rendering to 3D Gaussian Splatting: Theory and Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251017719v1-raindrop-gs-a-benchmark-for-3d-gaussian-splatting-under-raindrop-con.html">Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions</a></td>
  <td>Raindrop GSï¼šæå‡ºé›¨æ»´ç¯å¢ƒä¸‹3Dé«˜æ–¯æº…å°„é‡å»ºçš„ç»¼åˆè¯„æµ‹åŸºå‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17719v1" onclick="toggleFavorite(this, '2510.17719v1', 'Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251017274v1-enhanced-motion-forecasting-with-plug-and-play-multimodal-large-lang.html">Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models</a></td>
  <td>æå‡ºPnFï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¢å¼ºç°æœ‰è¿åŠ¨é¢„æµ‹æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17274v1" onclick="toggleFavorite(this, '2510.17274v1', 'Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251017479v1-initialize-to-generalize-a-stronger-initialization-pipeline-for-spar.html">Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS</a></td>
  <td>æå‡ºæ›´å¼ºçš„åˆå§‹åŒ–æµç¨‹ItG-GSï¼Œæ˜¾è‘—æå‡ç¨€ç–è§†è§’3DGSçš„æ¸²æŸ“è´¨é‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17479v1" onclick="toggleFavorite(this, '2510.17479v1', 'Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251017568v3-page-4d-disentangled-pose-and-geometry-estimation-for-vggt-4d-percep.html">PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception</a></td>
  <td>PAGE-4Dï¼šè§£è€¦å§¿æ€ä¸å‡ ä½•ä¿¡æ¯çš„åŠ¨æ€åœºæ™¯VGGT-4Dæ„ŸçŸ¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17568v3" onclick="toggleFavorite(this, '2510.17568v3', 'PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251017686v1-towards-3d-objectness-learning-in-an-open-world.html">Towards 3D Objectness Learning in an Open World</a></td>
  <td>æå‡ºOP3Detï¼Œè§£å†³å¼€æ”¾ä¸–ç•Œä¸­æ— æ–‡æœ¬æç¤ºçš„é€šç”¨3Dç›®æ ‡æ£€æµ‹é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17686v1" onclick="toggleFavorite(this, '2510.17686v1', 'Towards 3D Objectness Learning in an Open World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251018054v1-housetour-a-virtual-real-estate-aigent.html">HouseTour: A Virtual Real Estate A(I)gent</a></td>
  <td>HouseTourï¼šæå‡ºä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç©ºé—´æ„ŸçŸ¥ä¸‰ç»´ç›¸æœºè½¨è¿¹å’Œè‡ªç„¶è¯­è¨€æ‘˜è¦çš„æ–¹æ³•ï¼Œç”¨äºæˆ¿åœ°äº§åœºæ™¯ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18054v1" onclick="toggleFavorite(this, '2510.18054v1', 'HouseTour: A Virtual Real Estate A(I)gent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251017422v2-deepdetect-learning-all-in-one-dense-keypoints.html">DeepDetect: Learning All-in-One Dense Keypoints</a></td>
  <td>DeepDetectï¼šæå‡ºä¸€ç§èåˆç»å…¸æ£€æµ‹å™¨ä¼˜åŠ¿çš„ç«¯åˆ°ç«¯å¯†é›†å…³é”®ç‚¹æ£€æµ‹æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17422v2" onclick="toggleFavorite(this, '2510.17422v2', 'DeepDetect: Learning All-in-One Dense Keypoints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/251017095v1-gsplane-concise-and-accurate-planar-reconstruction-via-structured-re.html">GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation</a></td>
  <td>GSPlaneï¼šé€šè¿‡ç»“æ„åŒ–è¡¨ç¤ºå®ç°ç®€æ´è€Œç²¾ç¡®çš„å¹³é¢é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17095v1" onclick="toggleFavorite(this, '2510.17095v1', 'GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251018123v1-safecoop-unravelling-full-stack-safety-in-agentic-collaborative-driv.html">SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</a></td>
  <td>SafeCoopï¼šé’ˆå¯¹åŸºäºè‡ªç„¶è¯­è¨€ååŒé©¾é©¶çš„å…¨æ ˆå®‰å…¨é˜²å¾¡æ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18123v1" onclick="toggleFavorite(this, '2510.18123v1', 'SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251017803v1-consistedit-highly-consistent-and-precise-training-free-visual-editi.html">ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</a></td>
  <td>ConsistEditï¼šæå‡ºä¸€ç§é«˜ä¸€è‡´æ€§å’Œç²¾ç¡®æ€§çš„å…è®­ç»ƒè§†è§‰ç¼–è¾‘æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17803v1" onclick="toggleFavorite(this, '2510.17803v1', 'ConsistEdit: Highly Consistent and Precise Training-free Visual Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/251018014v1-manzaiset-a-multimodal-dataset-of-viewer-responses-to-japanese-manza.html">ManzaiSet: A Multimodal Dataset of Viewer Responses to Japanese Manzai Comedy</a></td>
  <td>ManzaiSetï¼šä¸€ä¸ªç”¨äºåˆ†æè§‚ä¼—å¯¹æ—¥æœ¬æ¼«æ‰ååº”çš„å¤šæ¨¡æ€æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18014v1" onclick="toggleFavorite(this, '2510.18014v1', 'ManzaiSet: A Multimodal Dataset of Viewer Responses to Japanese Manzai Comedy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251017434v2-leveraging-av1-motion-vectors-for-fast-and-dense-feature-matching.html">Leveraging AV1 motion vectors for Fast and Dense Feature Matching</a></td>
  <td>åˆ©ç”¨AV1è¿åŠ¨çŸ¢é‡å®ç°å¿«é€Ÿå¯†é›†ç‰¹å¾åŒ¹é…ï¼Œæå‡SfMæ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17434v2" onclick="toggleFavorite(this, '2510.17434v2', 'Leveraging AV1 motion vectors for Fast and Dense Feature Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/251018016v2-vibed-net-video-based-engagement-detection-network-using-face-aware-.html">ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</a></td>
  <td>ViBED-Netï¼šåˆ©ç”¨äººè„¸å’Œåœºæ™¯æ—¶ç©ºçº¿ç´¢è¿›è¡Œè§†é¢‘å‚ä¸åº¦æ£€æµ‹</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18016v2" onclick="toggleFavorite(this, '2510.18016v2', 'ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251017519v2-mug-v-10b-high-efficiency-training-pipeline-for-large-video-generati.html">MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models</a></td>
  <td>MUG-V 10Bï¼šé¢å‘å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒæ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17519v2" onclick="toggleFavorite(this, '2510.17519v2', 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>38</td>
  <td><a href="./papers/251017181v1-capturing-head-avatar-with-hand-contacts-from-a-monocular-video.html">Capturing Head Avatar with Hand Contacts from a Monocular Video</a></td>
  <td>æå‡ºä¸€ç§å•ç›®è§†é¢‘å¤´éƒ¨Avataré‡å»ºæ–¹æ³•ï¼Œè§£å†³æ‰‹éƒ¨äº¤äº’å½¢å˜å»ºæ¨¡é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17181v1" onclick="toggleFavorite(this, '2510.17181v1', 'Capturing Head Avatar with Hand Contacts from a Monocular Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/251017603v1-shapecraft-llm-agents-for-structured-textured-and-interactive-3d-mod.html">ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling</a></td>
  <td>ShapeCraftï¼šåˆ©ç”¨LLM Agentç”Ÿæˆç»“æ„åŒ–ã€çº¹ç†åŒ–å’Œäº¤äº’å¼3Dæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17603v1" onclick="toggleFavorite(this, '2510.17603v1', 'ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)