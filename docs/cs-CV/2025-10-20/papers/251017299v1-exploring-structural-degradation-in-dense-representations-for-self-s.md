---
layout: default
title: Exploring Structural Degradation in Dense Representations for Self-supervised Learning
---

# Exploring Structural Degradation in Dense Representations for Self-supervised Learning

**arXiv**: [2510.17299v1](https://arxiv.org/abs/2510.17299) | [PDF](https://arxiv.org/pdf/2510.17299.pdf)

**ä½œè€…**: Siran Dai, Qianqian Xu, Peisong Wen, Yang Liu, Qingming Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDSEæŒ‡æ ‡ä¸Žç­–ç•¥ä»¥ç¼“è§£è‡ªç›‘ç£å­¦ä¹ ä¸­çš„å¯†é›†é¢„æµ‹é€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å¯†é›†é¢„æµ‹` `è¡¨ç¤ºç»“æž„è¯„ä¼°` `æ¨¡åž‹é€‰æ‹©` `æ­£åˆ™åŒ–æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªç›‘ç£å­¦ä¹ ä¸­ï¼Œè®­ç»ƒæ—¶é—´å»¶é•¿å¯èƒ½æŸå®³å¯†é›†é¢„æµ‹ä»»åŠ¡æ€§èƒ½ï¼Œç§°ä¸ºSDDçŽ°è±¡
2. å¼•å…¥DSEæŒ‡æ ‡ï¼Œç»“åˆç±»ç›¸å…³æ€§å’Œæœ‰æ•ˆç»´åº¦ï¼Œæ— éœ€æ ‡æ³¨è¯„ä¼°å¯†é›†è¡¨ç¤ºç»“æž„
3. å®žéªŒéªŒè¯æ¨¡åž‹é€‰æ‹©å’Œæ­£åˆ™åŒ–ç­–ç•¥å¹³å‡æå‡mIoU 3.0%ï¼Œæœ‰æ•ˆç¼“è§£é€€åŒ–

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this work, we observe a counterintuitive phenomenon in self-supervised
> learning (SSL): longer training may impair the performance of dense prediction
> tasks (e.g., semantic segmentation). We refer to this phenomenon as
> Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
> across sixteen state-of-the-art SSL methods with various losses, architectures,
> and datasets. When the model performs suboptimally on dense tasks at the end of
> training, measuring the performance during training becomes essential. However,
> evaluating dense performance effectively without annotations remains an open
> challenge. To tackle this issue, we introduce a Dense representation Structure
> Estimator (DSE), composed of a class-relevance measure and an effective
> dimensionality measure. The proposed DSE is both theoretically grounded and
> empirically validated to be closely correlated with the downstream performance.
> Based on this metric, we introduce a straightforward yet effective model
> selection strategy and a DSE-based regularization method. Experiments on
> sixteen SSL methods across four benchmarks confirm that model selection
> improves mIoU by $3.0\%$ on average with negligible computational cost.
> Additionally, DSE regularization consistently mitigates the effects of dense
> degradation. Code is available at
> https://github.com/EldercatSAM/SSL-Degradation.

