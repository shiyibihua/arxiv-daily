---
layout: default
title: $\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs
---

# $\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs

**arXiv**: [2510.17205v1](https://arxiv.org/abs/2510.17205) | [PDF](https://arxiv.org/pdf/2510.17205.pdf)

**ä½œè€…**: Yingqi Fan, Anhao Zhao, Jinlan Fu, Junlong Tong, Hui Su, Yijie Pan, Wei Zhang, Xiaoyu Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisiPrunerä»¥é«˜æ•ˆå‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è®¡ç®—å¼€é”€**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `è·¨æ¨¡æ€äº¤äº’` `è®¡ç®—æ•ˆçŽ‡` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡åž‹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹è®¡ç®—å¼€é”€å¤§ï¼ŒæºäºŽæ³¨æ„åŠ›è®¡ç®—éšå¤šæ¨¡æ€ä»¤ç‰Œæ•°äºŒæ¬¡å¢žé•¿
2. åŸºäºŽä¸‰é˜¶æ®µè·¨æ¨¡æ€äº¤äº’åˆ†æžï¼Œè®¾è®¡æ— éœ€è®­ç»ƒçš„å‰ªæžæ¡†æž¶ï¼Œç§»é™¤å†—ä½™è§†è§‰ä»¤ç‰Œ
3. åœ¨LLaVA-v1.5 7Bä¸Šå‡å°‘99%è§†è§‰ç›¸å…³æ³¨æ„åŠ›è®¡ç®—å’Œ53.9% FLOPsï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have achieved strong performance
> across vision-language tasks, but suffer from significant computational
> overhead due to the quadratic growth of attention computations with the number
> of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
> \textit{they lack a fundamental understanding of how MLLMs process and fuse
> multimodal information.} Through systematic analysis, we uncover a
> \textbf{three-stage} cross-modal interaction process: (1) Shallow layers
> recognize task intent, with visual tokens acting as passive attention sinks;
> (2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
> critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
> on linguistic refinement. Based on these findings, we propose
> \emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
> vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
> significantly outperforms existing token pruning methods and generalizes across
> diverse MLLMs. Beyond pruning, our insights further provide actionable
> guidelines for training efficient MLLMs by aligning model architecture with its
> intrinsic layer-wise processing dynamics. Our code is available at:
> https://github.com/EIT-NLP/VisiPruner.

