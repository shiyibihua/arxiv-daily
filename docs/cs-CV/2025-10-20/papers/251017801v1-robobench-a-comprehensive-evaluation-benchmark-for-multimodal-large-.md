---
layout: default
title: Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain
---

# Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain

**arXiv**: [2510.17801v1](https://arxiv.org/abs/2510.17801) | [PDF](https://arxiv.org/pdf/2510.17801.pdf)

**ä½œè€…**: Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboBenchåŸºå‡†ä»¥ç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å…·èº«æœºå™¨äººä¸­çš„è®¤çŸ¥èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å…·èº«æ™ºèƒ½` `æœºå™¨äººåŸºå‡†` `è®¤çŸ¥è¯„ä¼°` `è§„åˆ’æ¨¡æ‹Ÿ` `å¤±è´¥åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†åœ¨è¯„ä¼°å…·èº«å¤§è„‘æ—¶å­˜åœ¨æ‰§è¡ŒæˆåŠŸåé‡å’Œä»»åŠ¡çœŸå®žæ€§ä¸è¶³çš„é—®é¢˜
2. å®šä¹‰äº”ä¸ªç»´åº¦è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œæ¶µç›–æŒ‡ä»¤ç†è§£ã€æ„ŸçŸ¥æŽ¨ç†ç­‰èƒ½åŠ›
3. å®žéªŒæ­ç¤ºæ¨¡åž‹åœ¨éšå¼æŒ‡ä»¤ç†è§£å’Œæ—¶ç©ºæŽ¨ç†ç­‰æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§å±€é™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Building robots that can perceive, reason, and act in dynamic, unstructured
> environments remains a core challenge. Recent embodied systems often adopt a
> dual-system paradigm, where System 2 handles high-level reasoning while System
> 1 executes low-level control. In this work, we refer to System 2 as the
> embodied brain, emphasizing its role as the cognitive core for reasoning and
> decision-making in manipulation tasks. Given this role, systematic evaluation
> of the embodied brain is essential. Yet existing benchmarks emphasize execution
> success, or when targeting high-level reasoning, suffer from incomplete
> dimensions and limited task realism, offering only a partial picture of
> cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
> that systematically evaluates multimodal large language models (MLLMs) as
> embodied brains. Motivated by the critical roles across the full manipulation
> pipeline, RoboBench defines five dimensions-instruction comprehension,
> perception reasoning, generalized planning, affordance prediction, and failure
> analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
> realism, we curate datasets across diverse embodiments, attribute-rich objects,
> and multi-view scenes, drawing from large-scale real robotic data. For
> planning, RoboBench introduces an evaluation framework,
> MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
> predicted plans can achieve critical object-state changes. Experiments on 14
> MLLMs reveal fundamental limitations: difficulties with implicit instruction
> comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
> affordance understanding, and execution failure diagnosis. RoboBench provides a
> comprehensive scaffold to quantify high-level cognition, and guide the
> development of next-generation embodied MLLMs. The project page is in
> https://robo-bench.github.io.

