---
layout: default
title: ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models
---

# ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models

**arXiv**: [2510.17197v1](https://arxiv.org/abs/2510.17197) | [PDF](https://arxiv.org/pdf/2510.17197.pdf)

**ä½œè€…**: Pu Zhang, Yuwei Li, Xingyuan Xian, Guoming Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé›¶æ ·æœ¬æç¤ºæ„ŸçŸ¥ä»¤ç‰Œå‰ªæžæ–¹æ³•ä»¥é™ä½Žè§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†æˆæœ¬**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `é›¶æ ·æœ¬å­¦ä¹ ` `æŽ¨ç†ä¼˜åŒ–` `æç¤ºæ„ŸçŸ¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹è¾“å…¥å¤§å¯¼è‡´è§†è§‰ä»¤ç‰Œå†—ä½™ï¼Œå¢žåŠ æŽ¨ç†å¼€é”€
2. åŸºäºŽæç¤ºæ„ŸçŸ¥å»ºæ¨¡å‰ªæžï¼Œå¹³è¡¡ä»»åŠ¡ç›¸å…³æ€§ä¸Žä¿¡æ¯å¤šæ ·æ€§
3. å®žéªŒæ˜¾ç¤ºå‰ªæž90%ä»¤ç‰Œæ—¶æ€§èƒ½æŽ¥è¿‘æœ€ä¼˜ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜å’Œå»¶è¿Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As the capabilities of Vision-Language Models (VLMs) advance, they can
> process increasingly large inputs, which, unlike in LLMs, generates significant
> visual token redundancy and leads to prohibitive inference costs. While many
> methods aim to reduce these costs by pruning visual tokens, existing
> approaches, whether based on attention or diversity, typically neglect the
> guidance of the text prompt and thus fail to prioritize task relevance. In this
> work, we propose a novel, zero-shot method that reframes the problem by
> introducing a prompt-aware perspective, explicitly modeling visual token
> pruning as a balance between task relevance and information diversity. Our
> hierarchical approach first selects a core set of task-relevant visual tokens
> and then supplements them with diversity tokens to preserve broader context.
> Experiments across multiple models and benchmarks show that our method achieves
> performance that matches or surpasses the state-of-the-art with only minimal
> accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
> gains are accompanied by significant reductions in GPU memory footprint and
> inference latency.

