---
layout: default
title: Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs
---

# Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs

**arXiv**: [2510.17771v1](https://arxiv.org/abs/2510.17771) | [PDF](https://arxiv.org/pdf/2510.17771.pdf)

**ä½œè€…**: Zhining Liu, Ziyi Chen, Hui Liu, Chen Luo, Xianfeng Tang, Suhang Wang, Joy Zeng, Zhenwei Dai, Zhan Shi, Tianxin Wei, Benoit Dumoulin, Hanghang Tong

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ·±åº¦å±‚æ³¨æ„åŠ›å¹²é¢„çš„æ–¹æ³•ï¼Œä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è§†è§‰è¯æ®æ„ŸçŸ¥ä¸ç­”æ¡ˆæ­£ç¡®æ€§è„±èŠ‚çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨ç†å¹²é¢„` `è§†è§‰é—®ç­”` `æ¨¡å‹è¯Šæ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯æ®å­˜åœ¨æ—¶ä»è¾“å‡ºé”™è¯¯ç­”æ¡ˆï¼Œå‡ºç°'çœ‹è§ä½†ä¸ç›¸ä¿¡'ç°è±¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†æå±‚é—´æ³¨æ„åŠ›åŠ¨æ€ï¼Œå¼•å…¥æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¹²é¢„ï¼Œçªå‡ºæ·±åº¦å±‚è¯æ®åŒºåŸŸã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå¹²é¢„æ–¹æ³•åœ¨LLaVAã€Qwenç­‰æ¨¡å‹ä¸Šä¸€è‡´æå‡å‡†ç¡®æ€§ï¼Œå¢å¼ºæ¨¡å‹å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
> as visual question answering, yet they can still fail even when the correct
> visual evidence is present. In this work, we systematically investigate whether
> these failures arise from not perceiving the evidence or from not leveraging it
> effectively. By examining layer-wise attention dynamics, we find that shallow
> layers focus primarily on text, while deeper layers sparsely but reliably
> attend to localized evidence regions. Surprisingly, VLMs often perceive the
> visual evidence when outputting incorrect answers, a phenomenon we term
> ``seeing but not believing'' that widely exists in major VLM families. Building
> on this, we introduce an inference-time intervention that highlights deep-layer
> evidence regions through selective attention-based masking. It requires no
> training and consistently improves accuracy across multiple families, including
> LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
> evidence internally but under-utilize it, making such signals explicit can
> bridge the gap between perception and reasoning, advancing the diagnostic
> understanding and reliability of VLMs.

