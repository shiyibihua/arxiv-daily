---
layout: default
title: An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning
---

# An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning

**arXiv**: [2510.17564v1](https://arxiv.org/abs/2510.17564) | [PDF](https://arxiv.org/pdf/2510.17564.pdf)

**ä½œè€…**: Lindsay Spoor, Ãlvaro Serra-GÃ³mez, Aske Plaat, Thomas Moerland

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æžæ‹‰æ ¼æœ—æ—¥ä¹˜å­åœ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ€ä¼˜æ€§ä¸Žç¨³å®šæ€§ï¼Œæå‡ºÎ»-å‰–é¢å¯è§†åŒ–æ–¹æ³•**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `æ‹‰æ ¼æœ—æ—¥æ–¹æ³•` `çº¦æŸä¼˜åŒ–` `ä¹˜å­æ›´æ–°` `ç¨³å®šæ€§åˆ†æž` `Î»-å‰–é¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‹‰æ ¼æœ—æ—¥ä¹˜å­Î»çš„é€‰æ‹©å¯¹å®‰å…¨å¼ºåŒ–å­¦ä¹ æ€§èƒ½ä¸Žçº¦æŸå¹³è¡¡é«˜åº¦æ•æ„Ÿï¼Œç¼ºä¹é€šç”¨ç›´è§‰
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Î»-å‰–é¢å¯è§†åŒ–æ€§èƒ½ä¸Žçº¦æŸæƒè¡¡ï¼Œå¹¶è¯„ä¼°è‡ªåŠ¨ä¹˜å­æ›´æ–°ä¸ŽPIDæŽ§åˆ¶æ–¹æ³•
3. å®žéªŒæˆ–æ•ˆæžœï¼šè‡ªåŠ¨æ›´æ–°å¯æ¢å¤æˆ–è¶…è¶Šæœ€ä¼˜æ€§èƒ½ï¼Œä½†å­˜åœ¨æŒ¯è¡ï¼Œéœ€è°ƒä¼˜ä»¥ç¨³å®š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In safety-critical domains such as robotics, navigation and power systems,
> constrained optimization problems arise where maximizing performance must be
> carefully balanced with associated constraints. Safe reinforcement learning
> provides a framework to address these challenges, with Lagrangian methods being
> a popular choice. However, the effectiveness of Lagrangian methods crucially
> depends on the choice of the Lagrange multiplier $\lambda$, which governs the
> trade-off between return and constraint cost. A common approach is to update
> the multiplier automatically during training. Although this is standard in
> practice, there remains limited empirical evidence on the robustness of an
> automated update and its influence on overall performance. Therefore, we
> analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
> reinforcement learning across a range of tasks. We provide $\lambda$-profiles
> that give a complete visualization of the trade-off between return and
> constraint cost of the optimization problem. These profiles show the highly
> sensitive nature of $\lambda$ and moreover confirm the lack of general
> intuition for choosing the optimal value $\lambda^*$. Our findings additionally
> show that automated multiplier updates are able to recover and sometimes even
> exceed the optimal performance found at $\lambda^*$ due to the vast difference
> in their learning trajectories. Furthermore, we show that automated multiplier
> updates exhibit oscillatory behavior during training, which can be mitigated
> through PID-controlled updates. However, this method requires careful tuning to
> achieve consistently better performance across tasks. This highlights the need
> for further research on stabilizing Lagrangian methods in safe reinforcement
> learning. The code used to reproduce our results can be found at
> https://github.com/lindsayspoor/Lagrangian_SafeRL.

