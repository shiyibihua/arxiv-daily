---
layout: default
title: Elastic ViTs from Pretrained Models without Retraining
---

# Elastic ViTs from Pretrained Models without Retraining

**arXiv**: [2510.17700v1](https://arxiv.org/abs/2510.17700) | [PDF](https://arxiv.org/pdf/2510.17700.pdf)

**ä½œè€…**: Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSnapViTæ–¹æ³•ï¼Œå®žçŽ°é¢„è®­ç»ƒè§†è§‰Transformerçš„å¼¹æ€§æŽ¨ç†ï¼Œæ— éœ€é‡è®­ç»ƒã€‚**

**å…³é”®è¯**: `è§†è§‰Transformer` `ç»“æž„åŒ–å‰ªæž` `å¼¹æ€§æŽ¨ç†` `è¿›åŒ–ç®—æ³•` `è‡ªç›‘ç£è¯„åˆ†` `é¢„è®­ç»ƒæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰åŸºç¡€æ¨¡åž‹å°ºå¯¸å›ºå®šï¼Œéš¾ä»¥é€‚åº”å®žé™…éƒ¨ç½²çš„è®¡ç®—çº¦æŸã€‚
2. ç»“åˆæ¢¯åº¦ä¸Žè·¨ç½‘ç»œç»“æž„ç›¸å…³æ€§ï¼Œä½¿ç”¨è¿›åŒ–ç®—æ³•è¿‘ä¼¼Hessianç»“æž„ï¼Œæ— éœ€æ ‡ç­¾æˆ–é‡è®­ç»ƒã€‚
3. åœ¨å¤šç§æ¨¡åž‹ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆå¼¹æ€§æ¨¡åž‹ä»…éœ€æ•°åˆ†é’Ÿï¼Œæ”¯æŒä»»æ„è®¡ç®—é¢„ç®—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision foundation models achieve remarkable performance but are only
> available in a limited set of pre-determined sizes, forcing sub-optimal
> deployment choices under real-world constraints. We introduce SnapViT:
> Single-shot network approximation for pruned Vision Transformers, a new
> post-pretraining structured pruning method that enables elastic inference
> across a continuum of compute budgets. Our approach efficiently combines
> gradient information with cross-network structure correlations, approximated
> via an evolutionary algorithm, does not require labeled data, generalizes to
> models without a classification head, and is retraining-free. Experiments on
> DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
> state-of-the-art methods across various sparsities, requiring less than five
> minutes on a single A100 GPU to generate elastic models that can be adjusted to
> any computational budget. Our key contributions include an efficient pruning
> strategy for pretrained Vision Transformers, a novel evolutionary approximation
> of Hessian off-diagonal structures, and a self-supervised importance scoring
> mechanism that maintains strong performance without requiring retraining or
> labels. Code and pruned models are available at: https://elastic.ashita.nl/

