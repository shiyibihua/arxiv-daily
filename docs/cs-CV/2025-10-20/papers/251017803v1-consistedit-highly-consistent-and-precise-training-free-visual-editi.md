---
layout: default
title: ConsistEdit: Highly Consistent and Precise Training-free Visual Editing
---

# ConsistEdit: Highly Consistent and Precise Training-free Visual Editing

**arXiv**: [2510.17803v1](https://arxiv.org/abs/2510.17803) | [PDF](https://arxiv.org/pdf/2510.17803.pdf)

**ä½œè€…**: Zixin Yin, Ling-Hao Chen, Lionel Ni, Xili Dai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConsistEditæ–¹æ³•ï¼ŒåŸºäºŽMM-DiTå®žçŽ°é«˜ä¸€è‡´æ€§å’Œç²¾ç¡®çš„æ— è®­ç»ƒè§†è§‰ç¼–è¾‘**

**å…³é”®è¯**: `æ— è®­ç»ƒè§†è§‰ç¼–è¾‘` `æ³¨æ„åŠ›æŽ§åˆ¶` `MM-DiTæ¨¡åž‹` `å¤šè½®ç¼–è¾‘` `è§†é¢‘ç¼–è¾‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰æ— è®­ç»ƒæ³¨æ„åŠ›æŽ§åˆ¶æ–¹æ³•åœ¨ç¼–è¾‘å¼ºåº¦ä¸Žæºä¸€è‡´æ€§é—´éš¾ä»¥å¹³è¡¡ï¼Œå½±å“å¤šè½®å’Œè§†é¢‘ç¼–è¾‘
2. é€šè¿‡è§†è§‰ä¸“ç”¨æ³¨æ„åŠ›æŽ§åˆ¶ã€æŽ©ç å¼•å¯¼é¢„èžåˆå’ŒQKVå·®å¼‚åŒ–æ“ä½œï¼Œæå‡ç¼–è¾‘ä¸€è‡´æ€§å’Œå¯¹é½æ€§
3. å®žéªŒæ˜¾ç¤ºåœ¨å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­è¾¾åˆ°SOTAï¼Œæ”¯æŒå¤šè½®ã€å¤šåŒºåŸŸå’Œæ¸è¿›ç»“æž„æŽ§åˆ¶

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in training-free attention control methods have enabled
> flexible and efficient text-guided editing capabilities for existing generation
> models. However, current approaches struggle to simultaneously deliver strong
> editing strength while preserving consistency with the source. This limitation
> becomes particularly critical in multi-round and video editing, where visual
> errors can accumulate over time. Moreover, most existing methods enforce global
> consistency, which limits their ability to modify individual attributes such as
> texture while preserving others, thereby hindering fine-grained editing.
> Recently, the architectural shift from U-Net to MM-DiT has brought significant
> improvements in generative performance and introduced a novel mechanism for
> integrating text and vision modalities. These advancements pave the way for
> overcoming challenges that previous methods failed to resolve. Through an
> in-depth analysis of MM-DiT, we identify three key insights into its attention
> mechanisms. Building on these, we propose ConsistEdit, a novel attention
> control method specifically tailored for MM-DiT. ConsistEdit incorporates
> vision-only attention control, mask-guided pre-attention fusion, and
> differentiated manipulation of the query, key, and value tokens to produce
> consistent, prompt-aligned edits. Extensive experiments demonstrate that
> ConsistEdit achieves state-of-the-art performance across a wide range of image
> and video editing tasks, including both structure-consistent and
> structure-inconsistent scenarios. Unlike prior methods, it is the first
> approach to perform editing across all inference steps and attention layers
> without handcraft, significantly enhancing reliability and consistency, which
> enables robust multi-round and multi-region editing. Furthermore, it supports
> progressive adjustment of structural consistency, enabling finer control.

