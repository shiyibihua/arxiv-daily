---
layout: default
title: From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors
---

# From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors

**arXiv**: [2510.17439v1](https://arxiv.org/abs/2510.17439) | [PDF](https://arxiv.org/pdf/2510.17439.pdf)

**ä½œè€…**: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFALCONèŒƒå¼ï¼Œé€šè¿‡æ³¨å…¥3Dç©ºé—´ä»¤ç‰Œè§£å†³VLAæ¨¡åž‹ç©ºé—´æŽ¨ç†ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `3Dç©ºé—´æŽ¨ç†` `ç©ºé—´åŸºç¡€æ¨¡åž‹` `æ¨¡æ€èžåˆ` `åŠ¨ä½œå¤´å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åŸºäºŽ2Dç¼–ç å™¨ï¼Œå­˜åœ¨ç©ºé—´æŽ¨ç†å·®è·ï¼Œé™åˆ¶æ³›åŒ–ä¸Žé€‚åº”æ€§
2. FALCONåˆ©ç”¨ç©ºé—´åŸºç¡€æ¨¡åž‹ä»ŽRGBæå–å‡ ä½•å…ˆéªŒï¼Œå¹¶å¯é€‰èžåˆæ·±åº¦æˆ–å§¿æ€
3. åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žä¸–ç•Œä»»åŠ¡ä¸­å®žçŽ°SOTAæ€§èƒ½ï¼Œä¿æŒé²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing vision-language-action (VLA) models act in 3D real-world but are
> typically built on 2D encoders, leaving a spatial reasoning gap that limits
> generalization and adaptability. Recent 3D integration techniques for VLAs
> either require specialized sensors and transfer poorly across modalities, or
> inject weak cues that lack geometry and degrade vision-language alignment. In
> this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
> injects rich 3D spatial tokens into the action head. FALCON leverages spatial
> foundation models to deliver strong geometric priors from RGB alone, and
> includes an Embodied Spatial Model that can optionally fuse depth, or pose for
> higher fidelity when available, without retraining or architectural changes. To
> preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
> Action Head rather than being concatenated into the vision-language backbone.
> These designs enable FALCON to address limitations in spatial representation,
> modality transferability, and alignment. In comprehensive evaluations across
> three simulation benchmarks and eleven real-world tasks, our proposed FALCON
> achieves state-of-the-art performance, consistently surpasses competitive
> baselines, and remains robust under clutter, spatial-prompt conditioning, and
> variations in object scale and height.

