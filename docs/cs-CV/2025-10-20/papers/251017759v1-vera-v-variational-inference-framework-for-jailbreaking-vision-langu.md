---
layout: default
title: VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models
---

# VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models

**arXiv**: [2510.17759v1](https://arxiv.org/abs/2510.17759) | [PDF](https://arxiv.org/pdf/2510.17759.pdf)

**ä½œè€…**: Qilin Liao, Anamika Lochab, Ruqi Zhang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVERA-Vå˜åˆ†æ¨ç†æ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹è¶Šç‹±æ”»å‡»é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹å®‰å…¨` `å˜åˆ†æ¨ç†` `å¯¹æŠ—æ”»å‡»` `è§†è§‰è¯­è¨€æ¨¡å‹` `è¶Šç‹±å‘ç°` `åéªŒåˆ†å¸ƒå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹å­˜åœ¨æœªå……åˆ†æ¢ç´¢çš„è„†å¼±æ€§ï¼Œç°æœ‰æ”»å‡»æ–¹æ³•ä¾èµ–è„†å¼±æ¨¡æ¿ä¸”è¦†ç›–èŒƒå›´çª„
2. å°†è¶Šç‹±å‘ç°å»ºæ¨¡ä¸ºå­¦ä¹ æ–‡æœ¬-å›¾åƒæç¤ºçš„è”åˆåéªŒåˆ†å¸ƒï¼Œç”Ÿæˆéšè”½å¯¹æŠ—è¾“å…¥å¹¶é›†æˆå¤šç§ç­–ç•¥
3. åœ¨HarmBenchå’ŒHADESåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ”»å‡»æˆåŠŸç‡æœ€é«˜æå‡53.75%ï¼Œä¼˜äºç°æœ‰æ–¹æ³•

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) extend large language models with visual
> reasoning, but their multimodal design also introduces new, underexplored
> vulnerabilities. Existing multimodal red-teaming methods largely rely on
> brittle templates, focus on single-attack settings, and expose only a narrow
> subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
> variational inference framework that recasts multimodal jailbreak discovery as
> learning a joint posterior distribution over paired text-image prompts. This
> probabilistic view enables the generation of stealthy, coupled adversarial
> inputs that bypass model guardrails. We train a lightweight attacker to
> approximate the posterior, allowing efficient sampling of diverse jailbreaks
> and providing distributional insights into vulnerabilities. VERA-V further
> integrates three complementary strategies: (i) typography-based text prompts
> that embed harmful cues, (ii) diffusion-based image synthesis that introduces
> adversarial signals, and (iii) structured distractors to fragment VLM
> attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
> consistently outperforms state-of-the-art baselines on both open-source and
> frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
> best baseline on GPT-4o.

