---
layout: default
title: SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference
---

# SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference

**arXiv**: [2510.17777v1](https://arxiv.org/abs/2510.17777) | [PDF](https://arxiv.org/pdf/2510.17777.pdf)

**ä½œè€…**: Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen, Konstantinos N. Plataniotis, Yao Lu, Song Han, Zhijian Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparseVILAä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†æ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†åŠ é€Ÿ` `ä»¤ç‰Œå‰ªæž` `å¤šæ¨¡æ€æŽ¨ç†` `é•¿è§†é¢‘åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰ä»¤ç‰Œæ•°é‡å¢žé•¿å¯¼è‡´VLMæŽ¨ç†å»¶è¿Ÿå¢žåŠ 
2. åœ¨é¢„å¡«å……é˜¶æ®µå‰ªæžå†—ä½™ä»¤ç‰Œï¼Œè§£ç é˜¶æ®µæ£€ç´¢æŸ¥è¯¢ç›¸å…³ä»¤ç‰Œ
3. å®žçŽ°ç«¯åˆ°ç«¯åŠ é€Ÿ2.6å€ï¼ŒåŒæ—¶æå‡æ–‡æ¡£ç†è§£å’ŒæŽ¨ç†ä»»åŠ¡å‡†ç¡®çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Language Models (VLMs) have rapidly advanced in integrating visual and
> textual reasoning, powering applications across high-resolution image
> understanding, long-video analysis, and multi-turn conversation. However, their
> scalability remains limited by the growing number of visual tokens that
> dominate inference latency. We present SparseVILA, a new paradigm for efficient
> VLM inference that decouples visual sparsity across the prefilling and decoding
> stages. SparseVILA distributes sparsity across stages by pruning redundant
> visual tokens during prefill and retrieving only query-relevant tokens during
> decoding. This decoupled design matches leading prefill pruning methods while
> preserving multi-turn fidelity by retaining most of the visual cache so that
> query-aware tokens can be retrieved at each conversation round. Built on an
> AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
> prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
> speedup on long-context video tasks -- while improving accuracy on
> document-understanding and reasoning tasks. By decoupling query-agnostic
> pruning and query-aware retrieval, SparseVILA establishes a new direction for
> efficient multimodal inference, offering a training-free, architecture-agnostic
> framework for accelerating large VLMs without sacrificing capability.

