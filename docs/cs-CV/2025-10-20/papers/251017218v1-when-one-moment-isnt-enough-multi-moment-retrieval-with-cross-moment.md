---
layout: default
title: When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions
---

# When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions

**arXiv**: [2510.17218v1](https://arxiv.org/abs/2510.17218) | [PDF](https://arxiv.org/pdf/2510.17218.pdf)

**ä½œè€…**: Zhuo Cao, Heming Du, Bingqing Zhang, Xin Yu, Xue Li, Sen Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlashMMRæ¡†æž¶ä¸ŽQV-MÂ²æ•°æ®é›†ä»¥è§£å†³è§†é¢‘å¤šæ—¶åˆ»æ£€ç´¢é—®é¢˜**

**å…³é”®è¯**: `å¤šæ—¶åˆ»æ£€ç´¢` `è§†é¢‘æ—¶åºå®šä½` `æ•°æ®é›†æž„å»º` `åŽéªŒè¯æ¨¡å—` `åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å•æ—¶åˆ»æ£€ç´¢æ–¹æ³•æ— æ³•å¤„ç†æŸ¥è¯¢å¯¹åº”å¤šä¸ªç›¸å…³æ—¶åˆ»çš„çŽ°å®žåœºæ™¯
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šæ—¶åˆ»åŽéªŒè¯æ¨¡å—ï¼Œé€šè¿‡çº¦æŸæ—¶é—´è°ƒæ•´å’ŒéªŒè¯ä¼˜åŒ–æ—¶åˆ»è¾¹ç•Œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨QV-MÂ²æ•°æ®é›†ä¸Šï¼ŒFlashMMRåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡è¾¾3.00%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
> (SMR). However, one query can correspond to multiple relevant moments in
> real-world applications. This makes the existing datasets and methods
> insufficient for video temporal grounding. By revisiting the gap between
> current MR tasks and real-world applications, we introduce a high-quality
> datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
> evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
> of 2,212 annotations covering 6,384 video segments. Building on existing
> efforts in MMR, we propose a framework called FlashMMR. Specifically, we
> propose a Multi-moment Post-verification module to refine the moment
> boundaries. We introduce constrained temporal adjustment and subsequently
> leverage a verification module to re-evaluate the candidate segments. Through
> this sophisticated filtering pipeline, low-confidence proposals are pruned, and
> robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
> MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
> Results show that QV-M$^2$ serves as an effective benchmark for training and
> evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
> on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
> 2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
> establish a foundation for advancing research in more realistic and challenging
> video temporal grounding scenarios. Code is released at
> https://github.com/Zhuo-Cao/QV-M2.

