---
layout: default
title: LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding
---

# LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding

**arXiv**: [2510.17305v1](https://arxiv.org/abs/2510.17305) | [PDF](https://arxiv.org/pdf/2510.17305.pdf)

**ä½œè€…**: ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLongInsightBenchåŸºå‡†ä»¥è¯„ä¼°å…¨æ¨¡æ€æ¨¡åž‹åœ¨äººç±»ä¸­å¿ƒé•¿è§†é¢‘ç†è§£ä¸­çš„è¡¨çŽ°**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å…¨æ¨¡æ€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `å¤šæ¨¡æ€èžåˆ` `æ—¶é—´å®šä½` `å› æžœæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨æ¨¡æ€æ¨¡åž‹åœ¨é•¿è§†é¢‘ç†è§£ä¸­é¢ä¸´æ—¶é—´å®šä½å’Œé•¿ç¨‹å› æžœæŽ¨ç†æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€é•¿è§†é¢‘åŸºå‡†ï¼Œæ¶µç›–å…­ç§ä»»åŠ¡åœºæ™¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºæ¨¡åž‹åœ¨T-Locå’ŒCE-Causä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œæ­ç¤ºå¤šæ¨¡æ€èžåˆåå·®

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce \textbf{LongInsightBench}, the first benchmark designed to
> assess models' ability to understand long videos, with a focus on human
> language, viewpoints, actions, and other contextual elements, while integrating
> \textbf{visual, audio, and text} modalities. Our benchmark excels in three key
> areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
> approximately 1,000 videos from open-source datasets FineVideo based on
> duration limit and the information density of both visual and audio modalities,
> focusing on content like lectures, interviews, and vlogs, which contain rich
> language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
> designed six challenging task scenarios, including both Intra-Event and
> Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
> Pipelines:} We have developed a three-step, semi-automated data quality
> assurance pipeline to ensure the difficulty and validity of the synthesized
> questions and answer options. Based on LongInsightBench, we designed a series
> of experiments. Experimental results shows that Omni-modal models(OLMs) still
> face challenge in tasks requiring precise temporal localization (T-Loc) and
> long-range causal inference (CE-Caus). Extended experiments reveal the
> information loss and processing bias in multi-modal fusion of OLMs. Our dataset
> and code is available at
> https://anonymous.4open.science/r/LongInsightBench-910F/.

