---
layout: default
title: Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition
---

# Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition

**arXiv**: [2510.17739v1](https://arxiv.org/abs/2510.17739) | [PDF](https://arxiv.org/pdf/2510.17739.pdf)

**ä½œè€…**: Timur Ismagilov, Shakaiba Majeed, Michael Milford, Tan Viet Tuyen Nguyen, Sarvapali D. Ramchurn, Shoaib Ehsan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçŸ©é˜µåˆ†è§£çš„å¤šæ¡ä»¶è”åˆå»ºæ¨¡æ–¹æ³•ä»¥æå‡è§†è§‰åœ°ç‚¹è¯†åˆ«æ€§èƒ½**

**å…³é”®è¯**: `è§†è§‰åœ°ç‚¹è¯†åˆ«` `çŸ©é˜µåˆ†è§£` `å¤šæ¡ä»¶å»ºæ¨¡` `æ— è®­ç»ƒæ–¹æ³•` `æ®‹å·®åŒ¹é…` `åŸºå‡†æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šå‚è€ƒè§†è§‰åœ°ç‚¹è¯†åˆ«ä¸­ï¼Œæ•°æ®å¤šæ ·æ€§å’Œæ¨¡åž‹å¤æ‚æ€§å¯¼è‡´é«˜è®¡ç®—æˆæœ¬
2. é‡‡ç”¨æ— è®­ç»ƒã€æè¿°ç¬¦æ— å…³çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ï¼Œåˆ†è§£ä¸ºåŸºè¡¨ç¤ºå¹¶å®žçŽ°æ®‹å·®åŒ¹é…
3. åœ¨SotonMVåŸºå‡†ä¸Šï¼ŒRecall@1æå‡è¾¾18%ï¼Œæ³›åŒ–æ€§å¼ºä¸”ä¿æŒè½»é‡çº§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We address multi-reference visual place recognition (VPR), where reference
> sets captured under varying conditions are used to improve localisation
> performance. While deep learning with large-scale training improves robustness,
> increasing data diversity and model complexity incur extensive computational
> cost during training and deployment. Descriptor-level fusion via voting or
> aggregation avoids training, but often targets multi-sensor setups or relies on
> heuristics with limited gains under appearance and viewpoint change. We propose
> a training-free, descriptor-agnostic approach that jointly models places using
> multiple reference descriptors via matrix decomposition into basis
> representations, enabling projection-based residual matching. We also introduce
> SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
> data, our method improves Recall@1 by up to ~18% over single-reference and
> outperforms multi-reference baselines across appearance and viewpoint changes,
> with gains of ~5% on unstructured data, demonstrating strong generalisation
> while remaining lightweight.

