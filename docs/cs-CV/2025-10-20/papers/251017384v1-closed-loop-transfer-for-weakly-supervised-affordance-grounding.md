---
layout: default
title: Closed-Loop Transfer for Weakly-supervised Affordance Grounding
---

# Closed-Loop Transfer for Weakly-supervised Affordance Grounding

**arXiv**: [2510.17384v1](https://arxiv.org/abs/2510.17384) | [PDF](https://arxiv.org/pdf/2510.17384.pdf)

**ä½œè€…**: Jiajin Tang, Zhengxuan Wei, Ge Zheng, Sibei Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoopTransé—­çŽ¯æ¡†æž¶ä»¥è§£å†³å¼±ç›‘ç£å¯æ‰¿å—æ€§æŽ¥åœ°åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„å±€é™æ€§**

**å…³é”®è¯**: `å¼±ç›‘ç£å­¦ä¹ ` `å¯æ‰¿å—æ€§æŽ¥åœ°` `é—­çŽ¯çŸ¥è¯†è½¬ç§»` `è·¨æ¨¡æ€å®šä½` `çŸ¥è¯†è’¸é¦` `é®æŒ¡å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä»…ä»Žå¤–ä¸­å¿ƒå›¾åƒå•å‘è½¬ç§»çŸ¥è¯†ï¼Œé™åˆ¶åœ¨å¤æ‚äº¤äº’åœºæ™¯çš„åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥é—­çŽ¯æ¡†æž¶ï¼ŒåŒå‘è½¬ç§»çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€è·¨æ¨¡æ€å®šä½å’ŒåŽ»å™ªçŸ¥è¯†è’¸é¦æœºåˆ¶ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å›¾åƒå’Œè§†é¢‘åŸºå‡†ä¸Šå®žçŽ°æŒ‡æ ‡ä¸€è‡´æå‡ï¼Œå¤„ç†å®Œå…¨é®æŒ¡åœºæ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humans can perform previously unexperienced interactions with novel objects
> simply by observing others engage with them. Weakly-supervised affordance
> grounding mimics this process by learning to locate object regions that enable
> actions on egocentric images, using exocentric interaction images with
> image-level annotations. However, extracting affordance knowledge solely from
> exocentric images and transferring it one-way to egocentric images limits the
> applicability of previous works in complex interaction scenarios. Instead, this
> study introduces LoopTrans, a novel closed-loop framework that not only
> transfers knowledge from exocentric to egocentric but also transfers back to
> enhance exocentric knowledge extraction. Within LoopTrans, several innovative
> mechanisms are introduced, including unified cross-modal localization and
> denoising knowledge distillation, to bridge domain gaps between object-centered
> egocentric and interaction-centered exocentric images while enhancing knowledge
> transfer. Experiments show that LoopTrans achieves consistent improvements
> across all metrics on image and video benchmarks, even handling challenging
> scenarios where object interaction regions are fully occluded by the human
> body.

