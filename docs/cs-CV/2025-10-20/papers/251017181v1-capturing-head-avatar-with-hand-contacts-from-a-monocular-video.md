---
layout: default
title: Capturing Head Avatar with Hand Contacts from a Monocular Video
---

# Capturing Head Avatar with Hand Contacts from a Monocular Video

**arXiv**: [2510.17181v1](https://arxiv.org/abs/2510.17181) | [PDF](https://arxiv.org/pdf/2510.17181.pdf)

**ä½œè€…**: Haonan He, Yufeng Zheng, Jie Song

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè”åˆå­¦ä¹ å¤´éƒ¨åŒ–èº«ä¸Žæ‰‹-è„¸äº¤äº’å˜å½¢æ¡†æž¶ï¼Œä»¥è§£å†³å•ç›®è§†é¢‘ä¸­è‡ªç„¶äº¤äº’å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `å¤´éƒ¨åŒ–èº«å»ºæ¨¡` `æ‰‹-è„¸äº¤äº’` `å•ç›®è§†é¢‘é‡å»º` `éžåˆšæ€§å˜å½¢` `PCAåŸºå­¦ä¹ ` `æŽ¥è§¦æŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¿½ç•¥æ‰‹-è„¸äº¤äº’ï¼Œå¯¼è‡´æ— æ³•æ•æ‰è®¤çŸ¥çŠ¶æ€å¦‚æ²‰æ€æ—¶çš„è‡ªç„¶å˜å½¢
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ·±åº¦é¡ºåºæŸå¤±ä¸ŽæŽ¥è§¦æ­£åˆ™åŒ–è¿›è¡Œå§¿æ€è·Ÿè¸ªï¼Œå¹¶å­¦ä¹ æ‰‹è¯±å¯¼é¢éƒ¨å˜å½¢çš„PCAåŸº
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨iPhoneè§†é¢‘å’Œåˆæˆæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œä¼˜äºŽSOTAæ–¹æ³•ï¼Œå‡å°‘ç©¿æ’ä¼ªå½±

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
> However, most methods focus solely on facial regions, ignoring natural
> hand-face interactions, such as a hand resting on the chin or fingers gently
> touching the cheek, which convey cognitive states like pondering. In this work,
> we present a novel framework that jointly learns detailed head avatars and the
> non-rigid deformations induced by hand-face interactions.
>   There are two principal challenges in this task. First, naively tracking hand
> and face separately fails to capture their relative poses. To overcome this, we
> propose to combine depth order loss with contact regularization during pose
> tracking, ensuring correct spatial relationships between the face and hand.
> Second, no publicly available priors exist for hand-induced deformations,
> making them non-trivial to learn from monocular videos. To address this, we
> learn a PCA basis specific to hand-induced facial deformations from a face-hand
> interaction dataset. This reduces the problem to estimating a compact set of
> PCA parameters rather than a full spatial deformation field. Furthermore,
> inspired by physics-based simulation, we incorporate a contact loss that
> provides additional supervision, significantly reducing interpenetration
> artifacts and enhancing the physical plausibility of the results.
>   We evaluate our approach on RGB(D) videos captured by an iPhone.
> Additionally, to better evaluate the reconstructed geometry, we construct a
> synthetic dataset of avatars with various types of hand interactions. We show
> that our method can capture better appearance and more accurate deforming
> geometry of the face than SOTA surface reconstruction methods.

