---
layout: default
title: Glyph: Scaling Context Windows via Visual-Text Compression
---

# Glyph: Scaling Context Windows via Visual-Text Compression

**arXiv**: [2510.17800v1](https://arxiv.org/abs/2510.17800) | [PDF](https://arxiv.org/pdf/2510.17800.pdf)

**ä½œè€…**: Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGlyphæ¡†æž¶ï¼Œé€šè¿‡è§†è§‰-æ–‡æœ¬åŽ‹ç¼©æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ä»¥é™ä½Žè®¡ç®—æˆæœ¬ã€‚**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡` `è§†è§‰-æ–‡æœ¬åŽ‹ç¼©` `è§†è§‰è¯­è¨€æ¨¡åž‹` `é—ä¼ æœç´¢ä¼˜åŒ–` `æ–‡æ¡£ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡LLMsè®¡ç®—å’Œå†…å­˜æˆæœ¬é«˜ï¼Œé™åˆ¶ç™¾ä¸‡çº§tokenä»»åŠ¡å®žç”¨æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†é•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹å¤„ç†ï¼Œå®žçŽ°è¯­ä¹‰ä¿ç•™åŽ‹ç¼©ã€‚
3. å®žéªŒæ•ˆæžœï¼šå®žçŽ°3-4å€tokenåŽ‹ç¼©ï¼Œå‡†ç¡®çŽ‡å¯æ¯”Qwen3-8Bï¼Œæå‡æŽ¨ç†å’Œè®­ç»ƒé€Ÿåº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) increasingly rely on long-context modeling for
> tasks such as document understanding, code analysis, and multi-step reasoning.
> However, scaling context windows to the million-token level brings prohibitive
> computational and memory costs, limiting the practicality of long-context LLMs.
> In this work, we take a different perspective-visual context scaling-to tackle
> this challenge. Instead of extending token-based sequences, we propose Glyph, a
> framework that renders long texts into images and processes them with
> vision-language models (VLMs). This approach substantially compresses textual
> input while preserving semantic information, and we further design an
> LLM-driven genetic search to identify optimal visual rendering configurations
> for balancing accuracy and compression. Through extensive experiments, we
> demonstrate that our method achieves 3-4x token compression while maintaining
> accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
> benchmarks. This compression also leads to around 4x faster prefilling and
> decoding, and approximately 2x faster SFT training. Furthermore, under extreme
> compression, a 128K-context VLM could scale to handle 1M-token-level text
> tasks. In addition, the rendered text data benefits real-world multimodal
> tasks, such as document understanding. Our code and model are released at
> https://github.com/thu-coai/Glyph.

