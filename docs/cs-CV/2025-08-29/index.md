---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-29
---

# cs.CVï¼ˆ2025-08-29ï¼‰

ğŸ“Š å…± **19** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250900213v2-multimodal-deep-learning-for-phyllodes-tumor-classification-from-ult.html">Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ä»¥æé«˜è…ºç˜¤åˆ†ç±»å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00213v2" data-paper-url="./papers/250900213v2-multimodal-deep-learning-for-phyllodes-tumor-classification-from-ult.html" onclick="toggleFavorite(this, '2509.00213v2', 'Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250902601v2-foundation-model-driven-classification-of-atypical-mitotic-figures-w.html">Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies</a></td>
  <td>æå‡ºåŸºäºåŸºç¡€æ¨¡å‹çš„åˆ†ç±»æ–¹æ³•ä»¥è§£å†³éå…¸å‹æœ‰ä¸åˆ†è£‚å›¾åƒè¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02601v2" data-paper-url="./papers/250902601v2-foundation-model-driven-classification-of-atypical-mitotic-figures-w.html" onclick="toggleFavorite(this, '2509.02601v2', 'Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250821738v2-from-drone-imagery-to-livability-mapping-ai-powered-environment-perc.html">From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China</a></td>
  <td>æå‡ºè§†è§‰-è¯­è¨€å¯¹æ¯”æ’åæ¡†æ¶ä»¥è§£å†³å†œæ‘ç¯å¢ƒæ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21738v2" data-paper-url="./papers/250821738v2-from-drone-imagery-to-livability-mapping-ai-powered-environment-perc.html" onclick="toggleFavorite(this, '2508.21738v2', 'From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250821451v4-mm-ser-multimodal-self-refinement-for-lightweight-image-captioning.html">MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning</a></td>
  <td>æå‡ºMM-SeRä»¥è§£å†³è½»é‡çº§å›¾åƒæè¿°çš„å¯é æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21451v4" data-paper-url="./papers/250821451v4-mm-ser-multimodal-self-refinement-for-lightweight-image-captioning.html" onclick="toggleFavorite(this, '2508.21451v4', 'MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250900192v2-safe-llava-a-privacy-preserving-vision-language-dataset-and-benchmar.html">Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety</a></td>
  <td>æå‡ºSafe-LLaVAä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿç‰©ç‰¹å¾æ³„éœ²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00192v2" data-paper-url="./papers/250900192v2-safe-llava-a-privacy-preserving-vision-language-dataset-and-benchmar.html" onclick="toggleFavorite(this, '2509.00192v2', 'Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250821824v1-driveqa-passing-the-driving-knowledge-test.html">DriveQA: Passing the Driving Knowledge Test</a></td>
  <td>æå‡ºDriveQAä»¥è§£å†³é©¾é©¶çŸ¥è¯†æµ‹è¯•çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21824v1" data-paper-url="./papers/250821824v1-driveqa-passing-the-driving-knowledge-test.html" onclick="toggleFavorite(this, '2508.21824v1', 'DriveQA: Passing the Driving Knowledge Test')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250821581v1-integrating-pathology-and-ct-imaging-for-personalized-recurrence-ris.html">Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer</a></td>
  <td>æå‡ºå¤šæ¨¡æ€èåˆæ–¹æ³•ä»¥æå‡è‚¾ç™Œå¤å‘é£é™©é¢„æµ‹ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21581v1" data-paper-url="./papers/250821581v1-integrating-pathology-and-ct-imaging-for-personalized-recurrence-ris.html" onclick="toggleFavorite(this, '2508.21581v1', 'Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250900284v1-generative-ai-for-industrial-contour-detection-a-language-guided-vis.html">Generative AI for Industrial Contour Detection: A Language-Guided Vision System</a></td>
  <td>æå‡ºè¯­è¨€å¼•å¯¼çš„ç”Ÿæˆè§†è§‰ç³»ç»Ÿä»¥è§£å†³å·¥ä¸šè½®å»“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00284v1" data-paper-url="./papers/250900284v1-generative-ai-for-industrial-contour-detection-a-language-guided-vis.html" onclick="toggleFavorite(this, '2509.00284v1', 'Generative AI for Industrial Contour Detection: A Language-Guided Vision System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250900176v1-waste-bench-a-comprehensive-benchmark-for-evaluating-vllms-in-clutte.html">Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments</a></td>
  <td>æå‡ºWaste-Benchä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹VLLMsè¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00176v1" data-paper-url="./papers/250900176v1-waste-bench-a-comprehensive-benchmark-for-evaluating-vllms-in-clutte.html" onclick="toggleFavorite(this, '2509.00176v1', 'Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250821769v2-domain-generalization-in-the-wild-disentangling-classification-from-.html">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></td>
  <td>æå‡ºCLIP-DCAä»¥è§£å†³é¢†åŸŸæ³›åŒ–è¯„ä¼°ä¸­çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21769v2" data-paper-url="./papers/250821769v2-domain-generalization-in-the-wild-disentangling-classification-from-.html" onclick="toggleFavorite(this, '2508.21769v2', 'Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250821693v1-why-stop-at-words-unveiling-the-bigger-picture-through-line-level-oc.html">Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</a></td>
  <td>æå‡ºè¡Œçº§OCRä»¥è§£å†³è¯çº§OCRçš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21693v1" data-paper-url="./papers/250821693v1-why-stop-at-words-unveiling-the-bigger-picture-through-line-level-oc.html" onclick="toggleFavorite(this, '2508.21693v1', 'Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250821565v1-how-well-do-vision-language-models-understand-cities-a-comparative-s.html">How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images</a></td>
  <td>æå‡ºåŸå¸‚ç©ºé—´æ¨ç†æ–°æŒ‘æˆ˜ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21565v1" data-paper-url="./papers/250821565v1-how-well-do-vision-language-models-understand-cities-a-comparative-s.html" onclick="toggleFavorite(this, '2508.21565v1', 'How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250821496v2-elv-halluc-benchmarking-semantic-aggregation-hallucinations-in-long-.html">ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</a></td>
  <td>æå‡ºELV-Hallucä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„è¯­ä¹‰èšåˆå¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21496v2" data-paper-url="./papers/250821496v2-elv-halluc-benchmarking-semantic-aggregation-hallucinations-in-long-.html" onclick="toggleFavorite(this, '2508.21496v2', 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250900210v1-beyond-pixels-introducing-geometric-semantic-world-priors-for-video-.html">Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</a></td>
  <td>æå‡ºVEMEä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„æ¨ç†ä¸è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00210v1" data-paper-url="./papers/250900210v1-beyond-pixels-introducing-geometric-semantic-world-priors-for-video-.html" onclick="toggleFavorite(this, '2509.00210v1', 'Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250821770v2-what-can-we-learn-from-harry-potter-an-exploratory-study-of-visual-r.html">What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</a></td>
  <td>æå‡ºåˆ©ç”¨éå…¸å‹è§†é¢‘æå‡å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21770v2" data-paper-url="./papers/250821770v2-what-can-we-learn-from-harry-potter-an-exploratory-study-of-visual-r.html" onclick="toggleFavorite(this, '2508.21770v2', 'What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250821767v1-uitron-foundational-gui-agent-with-advanced-perception-and-planning.html">UItron: Foundational GUI Agent with Advanced Perception and Planning</a></td>
  <td>æå‡ºUItronä»¥è§£å†³GUIä»£ç†è‡ªåŠ¨åŒ–æ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21767v1" data-paper-url="./papers/250821767v1-uitron-foundational-gui-agent-with-advanced-perception-and-planning.html" onclick="toggleFavorite(this, '2508.21767v1', 'UItron: Foundational GUI Agent with Advanced Perception and Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250821444v1-scale-gs-efficient-scalable-gaussian-splatting-via-redundancy-filter.html">Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content</a></td>
  <td>æå‡ºå¯æ‰©å±•é«˜æ•ˆçš„é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æ¡†æ¶ä»¥è§£å†³åŠ¨æ€åœºæ™¯è®­ç»ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21444v1" data-paper-url="./papers/250821444v1-scale-gs-efficient-scalable-gaussian-splatting-via-redundancy-filter.html" onclick="toggleFavorite(this, '2508.21444v1', 'Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250821542v1-complete-gaussian-splats-from-a-single-image-with-denoising-diffusio.html">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a></td>
  <td>æå‡ºåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å•å›¾åƒå®Œæ•´é«˜æ–¯ç‚¹äº‘é‡å»ºæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21542v1" data-paper-url="./papers/250821542v1-complete-gaussian-splats-from-a-single-image-with-denoising-diffusio.html" onclick="toggleFavorite(this, '2508.21542v1', 'Complete Gaussian Splats from a Single Image with Denoising Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250821556v1-echo-ego-centric-modeling-of-human-object-interactions.html">ECHO: Ego-Centric modeling of Human-Object interactions</a></td>
  <td>æå‡ºECHOä»¥è§£å†³äººæœºäº¤äº’å»ºæ¨¡çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21556v1" data-paper-url="./papers/250821556v1-echo-ego-centric-modeling-of-human-object-interactions.html" onclick="toggleFavorite(this, '2508.21556v1', 'ECHO: Ego-Centric modeling of Human-Object interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)