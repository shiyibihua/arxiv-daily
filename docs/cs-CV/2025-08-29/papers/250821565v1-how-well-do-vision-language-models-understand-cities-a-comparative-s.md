---
layout: default
title: How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images
---

# How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21565" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21565v1</a>
  <a href="https://arxiv.org/pdf/2508.21565.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21565v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21565v1', 'How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Juneyoung Ro, Namwoo Kim, Yoonjin Yoon

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

**å¤‡æ³¨**: Accepted to ICCV Workshop 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸå¸‚ç©ºé—´æ¨ç†æ–°æŒ‘æˆ˜ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `åŸå¸‚åœºæ™¯` `åˆæˆæ•°æ®é›†` `å¾®è°ƒç­–ç•¥` `æ·±åº¦å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚åœºæ™¯çš„ç©ºé—´æ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†éªŒè¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é—®é¢˜ç±»å‹ä¸Šè¡¨ç°ä¸è¶³ã€‚
2. æœ¬æ–‡é€šè¿‡æ„å»ºåˆæˆVQAæ•°æ®é›†ï¼Œç»“åˆé€æ­¥æ¨ç†çš„ç­”æ¡ˆï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹åŸå¸‚åœºæ™¯çš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥æå‡VLMsçš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹åœ¨å¤„ç†å¦å®šå’Œåäº‹å®é—®é¢˜æ—¶çš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†åˆæˆæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆç†è§£åŸå¸‚åœºæ™¯éœ€è¦å¯¹ç‰©ä½“ã€å¸ƒå±€å’Œæ·±åº¦çº¿ç´¢è¿›è¡Œç»†è‡´çš„ç©ºé—´æ¨ç†ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŸå¸‚é¢†åŸŸçš„èƒ½åŠ›è½¬ç§»å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¯¹ä¸‰ç§ç°æˆçš„VLMsï¼ˆBLIP-2ã€InstructBLIPå’ŒLLaVA-1.5ï¼‰è¿›è¡Œäº†æ¯”è¾ƒç ”ç©¶ï¼Œè¯„ä¼°äº†å®ƒä»¬åœ¨é›¶æ ·æœ¬æ€§èƒ½å’Œä½¿ç”¨ç‰¹å®šäºåŸå¸‚åœºæ™¯çš„åˆæˆVQAæ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„æ•ˆæœã€‚æˆ‘ä»¬æ„å»ºäº†è¯¥æ•°æ®é›†ï¼Œé€šè¿‡è¡—æ™¯å›¾åƒçš„åˆ†å‰²ã€æ·±åº¦å’Œç‰©ä½“æ£€æµ‹é¢„æµ‹ï¼Œé…å¯¹æ¯ä¸ªé—®é¢˜ä¸LLMç”Ÿæˆçš„é€æ­¥æ¨ç†ç­”æ¡ˆã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡VLMsåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ä½¿ç”¨æˆ‘ä»¬çš„åˆæˆCoTç›‘ç£æ•°æ®é›†è¿›è¡Œå¾®è°ƒæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¦å®šå’Œåäº‹å®ç­‰æŒ‘æˆ˜æ€§é—®é¢˜ç±»å‹ä¸Šã€‚æ­¤ç ”ç©¶å°†åŸå¸‚ç©ºé—´æ¨ç†å¼•å…¥VLMsçš„æ–°æŒ‘æˆ˜ï¼Œå¹¶å±•ç¤ºäº†åˆæˆæ•°æ®é›†æ„å»ºä½œä¸ºå°†é€šç”¨æ¨¡å‹é€‚åº”äºä¸“ä¸šé¢†åŸŸçš„å®ç”¨è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚åœºæ™¯ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é—®é¢˜ç±»å‹ï¼ˆå¦‚å¦å®šå’Œåäº‹å®ï¼‰ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªåˆæˆçš„VQAæ•°æ®é›†ï¼Œç»“åˆLLMç”Ÿæˆçš„é€æ­¥æ¨ç†ç­”æ¡ˆï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä»¥å¢å¼ºVLMsåœ¨åŸå¸‚åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†é€šè¿‡è¡—æ™¯å›¾åƒçš„åˆ†å‰²ã€æ·±åº¦å’Œç‰©ä½“æ£€æµ‹é¢„æµ‹ç”Ÿæˆï¼Œæ¨¡å‹åˆ™åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥åŸå¸‚ç©ºé—´æ¨ç†ä½œä¸ºVLMsçš„æ–°æŒ‘æˆ˜ï¼Œå¹¶å±•ç¤ºäº†åˆæˆæ•°æ®é›†æ„å»ºçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†åˆ†å‰²ã€æ·±åº¦å’Œç‰©ä½“æ£€æµ‹çš„å¤šç§é¢„æµ‹æŠ€æœ¯ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†é€æ­¥æ¨ç†çš„æŸå¤±å‡½æ•°ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåï¼Œæ¨¡å‹åœ¨å¤„ç†å¤æ‚é—®é¢˜ç±»å‹æ—¶çš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚ä¾‹å¦‚ï¼Œåœ¨å¦å®šå’Œåäº‹å®é—®é¢˜ä¸Šï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†åˆæˆæ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŸå¸‚è§„åˆ’ã€è‡ªåŠ¨é©¾é©¶ã€åŸå¸‚å®‰å…¨ç›‘æ§ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºç›¸å…³é¢†åŸŸæä¾›æ›´ç²¾å‡†çš„å†³ç­–æ”¯æŒï¼Œæ¨åŠ¨æ™ºèƒ½åŸå¸‚çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effectively understanding urban scenes requires fine-grained spatial reasoning about objects, layouts, and depth cues. However, how well current vision-language models (VLMs), pretrained on general scenes, transfer these abilities to urban domain remains underexplored. To address this gap, we conduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP, and LLaVA-1.5-evaluating both zero-shot performance and the effects of fine-tuning with a synthetic VQA dataset specific to urban scenes. We construct such dataset from segmentation, depth, and object detection predictions of street-view images, pairing each question with LLM-generated Chain-of-Thought (CoT) answers for step-by-step reasoning supervision. Results show that while VLMs perform reasonably well in zero-shot settings, fine-tuning with our synthetic CoT-supervised dataset substantially boosts performance, especially for challenging question types such as negation and counterfactuals. This study introduces urban spatial reasoning as a new challenge for VLMs and demonstrates synthetic dataset construction as a practical path for adapting general-purpose models to specialized domains.

