---
layout: default
title: MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning
---

# MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21451" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21451v4</a>
  <a href="https://arxiv.org/pdf/2508.21451.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21451v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21451v4', 'MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junha Song, Yongsik Jo, So Yeon Min, Quanting Xie, Taehwan Kim, Yonatan Bisk, Jaegul Choo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-12-12)

**å¤‡æ³¨**: Project page: https://sites.google.com/view/junha/mm-ser

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMM-SeRä»¥è§£å†³è½»é‡çº§å›¾åƒæè¿°çš„å¯é æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è½»é‡çº§æ¨¡å‹` `å›¾åƒæè¿°` `å¤šæ¨¡æ€è‡ªæˆ‘ç²¾ç‚¼` `è§†é¢‘é—®ç­”` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ ` `å®æ—¶åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å›¾åƒæè¿°æ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ¨¡æ€è‡ªæˆ‘ç²¾ç‚¼æ¡†æ¶æ¥æé«˜æè¿°çš„å¯é æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å•å¥å’Œè¯¦ç»†æè¿°ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘èŠå¤©æœºå™¨äººå’Œå¯¼èˆªæœºå™¨äººç­‰ç³»ç»Ÿå¸¸ä¾èµ–äºæµå¼å›¾åƒæè¿°æ¥è§£è¯»è§†è§‰è¾“å…¥ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è½»é‡çº§çš„æè¿°æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å°†MLLMsä¸­çš„å¤§å‹è¯­è¨€ç»„ä»¶æ›¿æ¢ä¸ºä¸€ä¸ª125Må‚æ•°çš„ç´§å‡‘æ¨¡å‹ï¼Œå‘ç°è¯¥æ¨¡å‹åœ¨ä½“ç§¯å‡å°‘93å€çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸MLLMsç›¸å½“ï¼Œè¡¨æ˜äº‹å®å›¾åƒæè¿°å¹¶ä¸éœ€è¦å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè½»é‡çº§æ¨¡å‹çš„å¯é æ€§ä»ç„¶ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è‡ªæˆ‘ç²¾ç‚¼æ¡†æ¶ï¼Œå€Ÿé‰´äººç±»è§†è§‰è¿‡ç¨‹ï¼Œé€šè¿‡å‚è€ƒå…ˆå‰çš„ç²—ç•¥æè¿°æ¥å¼•å¯¼æ¨¡å‹åˆ©ç”¨æ˜¾è‘—åŒºåŸŸçš„ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆæ›´ç²¾ç»†çš„æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å•å¥å’Œè¯¦ç»†æè¿°æ–¹é¢å‡è¡¨ç°ä¼˜è¶Šï¼Œç”šè‡³åœ¨é•¿è·ç¦»è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­ä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­çš„é«˜è®¡ç®—æˆæœ¬å’Œå¯é æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§å‹æ¨¡å‹ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å›¾åƒæè¿°æ¨¡å‹ï¼Œé€šè¿‡å°†å¤§å‹è¯­è¨€ç»„ä»¶æ›¿æ¢ä¸º125Må‚æ•°çš„ç´§å‡‘æ¨¡å‹ï¼Œç»“åˆå¤šæ¨¡æ€è‡ªæˆ‘ç²¾ç‚¼æ¡†æ¶ï¼Œå€Ÿé‰´äººç±»è§†è§‰å¤„ç†çš„æ–¹å¼æ¥æå‡æè¿°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç´§å‡‘çš„è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªè‡ªæˆ‘ç²¾ç‚¼æ¨¡å—ã€‚è‡ªæˆ‘ç²¾ç‚¼æ¨¡å—é€šè¿‡å‚è€ƒå…ˆå‰ç”Ÿæˆçš„ç²—ç•¥æè¿°ï¼Œè¯†åˆ«æ˜¾è‘—åŒºåŸŸçš„ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆæ›´ç²¾ç»†çš„æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å¤šæ¨¡æ€è‡ªæˆ‘ç²¾ç‚¼æ¡†æ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆæè¿°æ—¶æ›´å¥½åœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ˜¾è‘—ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†æè¿°çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å…³é”®è®¾è®¡åŒ…æ‹¬125Må‚æ•°çš„ç´§å‡‘è¯­è¨€æ¨¡å‹ã€æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ï¼Œä»¥åŠè‡ªæˆ‘ç²¾ç‚¼æ¨¡å—çš„å®ç°ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¿æŒè½»é‡çº§çš„åŒæ—¶ï¼Œä»èƒ½æä¾›é«˜è´¨é‡çš„æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„è½»é‡çº§æ¨¡å‹åœ¨å•å¥æè¿°å’Œè¯¦ç»†æè¿°ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œä¸”åœ¨é•¿è·ç¦»è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†93å€çš„å‚æ•°å‡å°‘ä¸æ€§èƒ½ç›¸å½“çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘èŠå¤©æœºå™¨äººã€å¯¼èˆªæœºå™¨äººä»¥åŠå…¶ä»–éœ€è¦å®æ—¶å›¾åƒç†è§£çš„ç³»ç»Ÿã€‚é€šè¿‡æä¾›é«˜æ•ˆä¸”å¯é çš„å›¾åƒæè¿°èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸­æå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.

