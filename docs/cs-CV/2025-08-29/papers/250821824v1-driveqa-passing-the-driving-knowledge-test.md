---
layout: default
title: DriveQA: Passing the Driving Knowledge Test
---

# DriveQA: Passing the Driving Knowledge Test

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21824" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21824v1</a>
  <a href="https://arxiv.org/pdf/2508.21824.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21824v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21824v1', 'DriveQA: Passing the Driving Knowledge Test')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

**å¤‡æ³¨**: Accepted by ICCV 2025. Project page: https://driveqaiccv.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDriveQAä»¥è§£å†³é©¾é©¶çŸ¥è¯†æµ‹è¯•çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é©¾é©¶çŸ¥è¯†æµ‹è¯•` `å¤šæ¨¡æ€åŸºå‡†` `äº¤é€šè§„åˆ™ç†è§£` `æ¨¡å‹å¾®è°ƒ` `ç¯å¢ƒå› ç´ æ•æ„Ÿæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨å¤„ç†å¤æ‚äº¤é€šè§„åˆ™å’Œè¾¹ç¼˜æ¡ˆä¾‹æ—¶è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å€¼æ¨ç†å’Œç©ºé—´å¸ƒå±€æ–¹é¢ã€‚
2. DriveQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œæ—¨åœ¨å…¨é¢è¦†ç›–äº¤é€šæ³•è§„å’Œå¤šæ ·åŒ–åœºæ™¯ï¼Œä»¥æå‡æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåæ¨¡å‹åœ¨ç›‘ç®¡æ ‡å¿—è¯†åˆ«å’Œäº¤å‰å£å†³ç­–ä¸Šæ˜¾è‘—æå‡ï¼Œä¸”åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¦‚æœä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»Šå¤©å‚åŠ é©¾é©¶çŸ¥è¯†æµ‹è¯•ï¼Œå®ƒèƒ½é€šè¿‡å—ï¼Ÿä¸å½“å‰è‡ªåŠ¨é©¾é©¶åŸºå‡†ä¸Šçš„æ ‡å‡†ç©ºé—´å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸åŒï¼Œé©¾é©¶çŸ¥è¯†æµ‹è¯•è¦æ±‚å¯¹æ‰€æœ‰äº¤é€šè§„åˆ™ã€æ ‡å¿—å’Œä¼˜å…ˆé€šè¡ŒåŸåˆ™æœ‰å…¨é¢ç†è§£ã€‚ä¸ºé€šè¿‡æ­¤æµ‹è¯•ï¼Œäººç±»é©¾é©¶å‘˜å¿…é¡»è¾¨åˆ«åœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸­å¾ˆå°‘å‡ºç°çš„å„ç§è¾¹ç¼˜æ¡ˆä¾‹ã€‚æœ¬æ–‡æå‡ºDriveQAï¼Œä¸€ä¸ªå…¨é¢çš„å¼€æºæ–‡æœ¬å’Œè§†è§‰åŸºå‡†ï¼Œè¯¦å°½è¦†ç›–äº¤é€šæ³•è§„å’Œåœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æœ€å…ˆè¿›çš„LLMå’Œå¤šæ¨¡æ€LLMåœ¨åŸºæœ¬äº¤é€šè§„åˆ™ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ•°å€¼æ¨ç†ã€å¤æ‚çš„ä¼˜å…ˆé€šè¡Œåœºæ™¯ã€äº¤é€šæ ‡å¿—å˜ä½“å’Œç©ºé—´å¸ƒå±€æ–¹é¢å­˜åœ¨æ˜¾è‘—å¼±ç‚¹ã€‚é€šè¿‡åœ¨DriveQAä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹åœ¨å¤šä¸ªç±»åˆ«çš„å‡†ç¡®æ€§å¾—åˆ°äº†æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ç›‘ç®¡æ ‡å¿—è¯†åˆ«å’Œäº¤å‰å£å†³ç­–æ–¹é¢ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨å¤æ‚äº¤é€šçŸ¥è¯†å’Œè¾¹ç¼˜æ¡ˆä¾‹è¯†åˆ«æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ•°å€¼æ¨ç†å’Œç©ºé—´å¸ƒå±€çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDriveQAé€šè¿‡æ„å»ºä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬å’Œè§†è§‰åŸºå‡†ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£äº¤é€šè§„åˆ™å’Œåœºæ™¯ï¼Œä»è€Œæå‡å…¶åœ¨é©¾é©¶çŸ¥è¯†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDriveQAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œæ¶µç›–äº¤é€šæ³•è§„ã€åœºæ™¯æ¨¡æ‹Ÿå’Œæ¨¡å‹è¯„ä¼°ï¼Œæ—¨åœ¨æä¾›å¤šæ ·åŒ–çš„æµ‹è¯•ç”¨ä¾‹å’Œç¯å¢ƒå˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šDriveQAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å…¨é¢è¦†ç›–çš„äº¤é€šæ³•è§„å’Œåœºæ™¯è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„ä¼˜å…ˆé€šè¡Œå’Œäº¤é€šæ ‡å¿—å˜ä½“æ–¹é¢ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„æµ‹è¯•å’Œè¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–å¯¹äº¤é€šæ ‡å¿—å’Œäº¤å‰å£å†³ç­–çš„è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶å¼•å…¥äº†ç¯å¢ƒå› ç´ çš„æ§åˆ¶å˜é‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ•æ„Ÿæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LLMåœ¨DriveQAä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„ä¼˜å…ˆé€šè¡Œå’Œäº¤é€šæ ‡å¿—å˜ä½“æ–¹é¢ä»æœ‰æ˜¾è‘—ä¸è¶³ã€‚é€šè¿‡å¾®è°ƒï¼Œæ¨¡å‹åœ¨ç›‘ç®¡æ ‡å¿—è¯†åˆ«å’Œäº¤å‰å£å†³ç­–çš„å‡†ç¡®æ€§æå‡äº†20%ä»¥ä¸Šï¼Œä¸”åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†å¦‚nuSceneså’ŒBDDä¸Šçš„è¡¨ç°ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DriveQAçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¼€å‘ä¸æµ‹è¯•ï¼Œå°¤å…¶æ˜¯åœ¨æå‡æ¨¡å‹å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„ç†è§£èƒ½åŠ›æ–¹é¢ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†æœ‰æœ›æ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤é€šç®¡ç†ç³»ç»Ÿå’Œå®‰å…¨é©¾é©¶æŠ€æœ¯çš„å‘å±•ï¼Œå‡å°‘äº¤é€šäº‹æ•…çš„å‘ç”Ÿç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks.

