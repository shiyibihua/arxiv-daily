---
layout: default
title: Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment
---

# Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00210" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00210v1</a>
  <a href="https://arxiv.org/pdf/2509.00210.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00210v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00210v1', 'Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVEMEä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„æ¨ç†ä¸è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ—¶ç©ºæ¨ç†` `è·¨æ¨¡æ€å¯¹é½` `åŠ¨æ€è®¤çŸ¥åœ°å›¾` `ä»»åŠ¡å¯¼å‘å¯¼èˆª` `å‡ ä½•-è¯­ä¹‰è®°å¿†` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ã€å¼€æ”¾ä»»åŠ¡ä¸­çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å…·èº«æ™ºèƒ½ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºVEMEï¼Œé€šè¿‡è·¨æ¨¡æ€å¯¹é½å’ŒåŠ¨æ€è®¤çŸ¥åœ°å›¾ï¼Œå¢å¼ºæ¨¡å‹åœ¨æœªçŸ¥åœºæ™¯ä¸­çš„æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ã€‚
3. åœ¨VSI-Benchå’ŒVLN-CEä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVEMEåœ¨å‡†ç¡®ç‡å’Œæ¢ç´¢æ•ˆç‡ä¸Šè¾ƒä¼ ç»Ÿæ–¹æ³•æå‡äº†1%-3%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤æ‚æœªçŸ¥ç¯å¢ƒä¸­å®ç°ç±»äººæ¨ç†ä»ç„¶æ˜¯å…·èº«æ™ºèƒ½çš„å…³é”®æŒ‘æˆ˜ã€‚å°½ç®¡å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é™æ€åœºæ™¯ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ—¶ç©ºæ¨ç†å’ŒåŠ¨æ€ä»»åŠ¡é€‚åº”æ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€å¯¹é½æ–¹æ³•VEMEï¼Œé€šè¿‡å­¦ä¹ ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¸–ç•Œæ¨¡å‹æ¥å¢å¼ºåœ¨æœªçŸ¥åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šè·¨æ¨¡æ€å¯¹é½æ¡†æ¶ã€åŠ¨æ€éšå¼è®¤çŸ¥åœ°å›¾å’ŒåŸºäºæŒ‡ä»¤çš„å¯¼èˆªæ¨ç†æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®ç‡å’Œæ¢ç´¢æ•ˆç‡æé«˜äº†1%-3%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ¨ç†å’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡å’Œå¼€æ”¾åœºæ™¯ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVEMEæ¡†æ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€å¯¹é½å’ŒåŠ¨æ€è®¤çŸ¥åœ°å›¾ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ—¶ç©ºçº¿ç´¢çš„ç†è§£å’Œè®°å¿†èƒ½åŠ›ï¼Œä»è€Œæå‡æ¨ç†å’Œè§„åˆ’çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVEMEæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè·¨æ¨¡æ€å¯¹é½æ¡†æ¶ã€åŠ¨æ€éšå¼è®¤çŸ¥åœ°å›¾å’ŒåŸºäºæŒ‡ä»¤çš„å¯¼èˆªæ¨ç†æ¡†æ¶ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šVEMEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥å‡ ä½•-è¯­ä¹‰ä¸–ç•Œå…ˆéªŒï¼Œé€šè¿‡æ—¶ç©ºå¯¹é½æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•åœ¨é™æ€åœºæ™¯ç†è§£ä¸Šçš„å±€é™æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€çš„éšå¼è®¤çŸ¥åœ°å›¾æ¥æ¿€æ´»ä¸–ç•ŒåµŒå…¥ï¼Œå¹¶é€šè¿‡æŒ‡ä»¤é©±åŠ¨çš„å¯¼èˆªæ¡†æ¶è¿›è¡Œé•¿è¿œè§„åˆ’ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œä»»åŠ¡å¯¼å‘çš„æ¢ç´¢ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVEMEåœ¨VSI-Benchå’ŒVLN-CEæ•°æ®é›†ä¸Šå®ç°äº†1%-3%çš„å‡†ç¡®ç‡å’Œæ¢ç´¢æ•ˆç‡æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸåŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„ä»»åŠ¡æ‰§è¡Œå’Œå†³ç­–ã€‚æœªæ¥ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½æ¨åŠ¨å…·èº«æ™ºèƒ½ç³»ç»Ÿåœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.

