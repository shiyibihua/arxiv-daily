---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-25
---

# cs.CVï¼ˆ2025-09-25ï¼‰

ğŸ“Š å…± **16** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250921486v3-reasoning-enhanced-domain-adaptive-pretraining-of-multimodal-large-l.html">Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance</a></td>
  <td>æå‡ºæ¨ç†å¢å¼ºçš„é¢†åŸŸè‡ªé€‚åº”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºçŸ­è§†é¢‘å†…å®¹æ²»ç†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21486v3" data-paper-url="./papers/250921486v3-reasoning-enhanced-domain-adaptive-pretraining-of-multimodal-large-l.html" onclick="toggleFavorite(this, '2509.21486v3', 'Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250921251v1-instruction-tuned-self-questioning-framework-for-multimodal-reasonin.html">Instruction-tuned Self-Questioning Framework for Multimodal Reasoning</a></td>
  <td>æå‡ºåŸºäºæŒ‡ä»¤è°ƒä¼˜çš„è‡ªé—®æ¡†æ¶SQ-InstructBLIPï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21251v1" data-paper-url="./papers/250921251v1-instruction-tuned-self-questioning-framework-for-multimodal-reasonin.html" onclick="toggleFavorite(this, '2509.21251v1', 'Instruction-tuned Self-Questioning Framework for Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250921559v1-x-cot-explainable-text-to-video-retrieval-via-llm-based-chain-of-tho.html">X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning</a></td>
  <td>æå‡ºX-CoTï¼Œåˆ©ç”¨LLMé“¾å¼æ€è€ƒæ¨ç†å®ç°å¯è§£é‡Šçš„æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21559v1" data-paper-url="./papers/250921559v1-x-cot-explainable-text-to-video-retrieval-via-llm-based-chain-of-tho.html" onclick="toggleFavorite(this, '2509.21559v1', 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250921451v1-videojudge-bootstrapping-enables-scalable-supervision-of-mllm-as-a-j.html">VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding</a></td>
  <td>VideoJudgeï¼šé€šè¿‡è‡ªä¸¾æ³•å®ç°MLLMä½œä¸ºè§†é¢‘ç†è§£è¯„åˆ¤å™¨çš„å¯æ‰©å±•ç›‘ç£</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21451v1" data-paper-url="./papers/250921451v1-videojudge-bootstrapping-enables-scalable-supervision-of-mllm-as-a-j.html" onclick="toggleFavorite(this, '2509.21451v1', 'VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250921273v1-a-sentinel-3-foundation-model-for-ocean-colour.html">A Sentinel-3 foundation model for ocean colour</a></td>
  <td>æå‡ºåŸºäºSentinel-3çš„æµ·æ´‹é¢œè‰²åŸºç¡€æ¨¡å‹ï¼Œæå‡æµ·æ´‹è§‚æµ‹ä»»åŠ¡æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21273v1" data-paper-url="./papers/250921273v1-a-sentinel-3-foundation-model-for-ocean-colour.html" onclick="toggleFavorite(this, '2509.21273v1', 'A Sentinel-3 foundation model for ocean colour')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250921249v1-decipher-mr-a-vision-language-foundation-model-for-3d-mri-representa.html">Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations</a></td>
  <td>Decipher-MRï¼šç”¨äº3D MRIè¡¨å¾çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21249v1" data-paper-url="./papers/250921249v1-decipher-mr-a-vision-language-foundation-model-for-3d-mri-representa.html" onclick="toggleFavorite(this, '2509.21249v1', 'Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250922737v1-comparebench-a-benchmark-for-visual-comparison-reasoning-in-vision-l.html">CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models</a></td>
  <td>æå‡ºCompareBenchï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¯”è¾ƒæ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22737v1" data-paper-url="./papers/250922737v1-comparebench-a-benchmark-for-visual-comparison-reasoning-in-vision-l.html" onclick="toggleFavorite(this, '2509.22737v1', 'CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250921268v1-mmr1-enhancing-multimodal-reasoning-with-variance-aware-sampling-and.html">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a></td>
  <td>MMR1ï¼šé€šè¿‡æ–¹å·®æ„ŸçŸ¥é‡‡æ ·å’Œå¼€æ”¾èµ„æºå¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21268v1" data-paper-url="./papers/250921268v1-mmr1-enhancing-multimodal-reasoning-with-variance-aware-sampling-and.html" onclick="toggleFavorite(this, '2509.21268v1', 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250921239v1-slidemamba-entropy-based-adaptive-fusion-of-gnn-and-mamba-for-enhanc.html">SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology</a></td>
  <td>SlideMambaï¼šç»“åˆGNNä¸Mambaçš„ç†µè‡ªé€‚åº”èåˆæ¡†æ¶ï¼Œæå‡æ•°å­—ç—…ç†å­¦è¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">predictive model</span> <span class="paper-tag">Mamba</span> <span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21239v1" data-paper-url="./papers/250921239v1-slidemamba-entropy-based-adaptive-fusion-of-gnn-and-mamba-for-enhanc.html" onclick="toggleFavorite(this, '2509.21239v1', 'SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250921657v2-fantasyworld-geometry-consistent-world-modeling-via-unified-video-an.html">FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction</a></td>
  <td>FantasyWorldï¼šé€šè¿‡ç»Ÿä¸€è§†é¢‘å’Œ3Dé¢„æµ‹å®ç°å‡ ä½•ä¸€è‡´çš„ä¸–ç•Œå»ºæ¨¡</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21657v2" data-paper-url="./papers/250921657v2-fantasyworld-geometry-consistent-world-modeling-via-unified-video-an.html" onclick="toggleFavorite(this, '2509.21657v2', 'FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250921574v1-x-streamer-unified-human-world-modeling-with-audiovisual-interaction.html">X-Streamer: Unified Human World Modeling with Audiovisual Interaction</a></td>
  <td>X-Streamerï¼šæå‡ºåŸºäºè§†å¬äº¤äº’çš„ç»Ÿä¸€äººç±»ä¸–ç•Œå»ºæ¨¡æ¡†æ¶ï¼Œå®ç°æ•°å­—äººå®æ—¶äº¤äº’ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21574v1" data-paper-url="./papers/250921574v1-x-streamer-unified-human-world-modeling-with-audiovisual-interaction.html" onclick="toggleFavorite(this, '2509.21574v1', 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250921573v1-enhancing-contrastive-learning-for-geolocalization-by-discovering-ha.html">Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms</a></td>
  <td>æå‡ºåŸºäºSemivariogramçš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæå‡å›¾åƒåœ°ç†å®šä½ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21573v1" data-paper-url="./papers/250921573v1-enhancing-contrastive-learning-for-geolocalization-by-discovering-ha.html" onclick="toggleFavorite(this, '2509.21573v1', 'Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250921263v1-dense-semantic-matching-with-vggt-prior.html">Dense Semantic Matching with VGGT Prior</a></td>
  <td>æå‡ºåŸºäºVGGTå…ˆéªŒçš„ç¨ å¯†è¯­ä¹‰åŒ¹é…æ–¹æ³•ï¼Œæå‡å‡ ä½•æ„ŸçŸ¥å’ŒåŒ¹é…å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">VGGT</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21263v1" data-paper-url="./papers/250921263v1-dense-semantic-matching-with-vggt-prior.html" onclick="toggleFavorite(this, '2509.21263v1', 'Dense Semantic Matching with VGGT Prior')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250921302v2-quantized-visual-geometry-grounded-transformer.html">Quantized Visual Geometry Grounded Transformer</a></td>
  <td>æå‡ºQuantVGGTï¼Œè§£å†³VGGTé‡åŒ–éš¾é¢˜ï¼Œå®ç°èµ„æºå—é™åœºæ™¯ä¸‹çš„é«˜æ•ˆ3Dé‡å»ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">VGGT</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21302v2" data-paper-url="./papers/250921302v2-quantized-visual-geometry-grounded-transformer.html" onclick="toggleFavorite(this, '2509.21302v2', 'Quantized Visual Geometry Grounded Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250921670v3-morph-pde-foundation-models-with-arbitrary-data-modality.html">MORPH: PDE Foundation Models with Arbitrary Data Modality</a></td>
  <td>æå‡ºMORPHæ¨¡å‹ä»¥å¤„ç†å¤šæ¨¡æ€åå¾®åˆ†æ–¹ç¨‹æ•°æ®</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21670v3" data-paper-url="./papers/250921670v3-morph-pde-foundation-models-with-arbitrary-data-modality.html" onclick="toggleFavorite(this, '2509.21670v3', 'MORPH: PDE Foundation Models with Arbitrary Data Modality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250921278v3-does-flux-already-know-how-to-perform-physically-plausible-image-com.html">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a></td>
  <td>æå‡ºSHINEæ¡†æ¶ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°ç‰©ç†ä¸Šåˆç†çš„å›¾åƒåˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21278v3" data-paper-url="./papers/250921278v3-does-flux-already-know-how-to-perform-physically-plausible-image-com.html" onclick="toggleFavorite(this, '2509.21278v3', 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)