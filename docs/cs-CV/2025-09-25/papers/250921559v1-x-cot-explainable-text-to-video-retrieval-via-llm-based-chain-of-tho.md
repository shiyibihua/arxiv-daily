---
layout: default
title: X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning
---

# X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21559" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21559v1</a>
  <a href="https://arxiv.org/pdf/2509.21559.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21559v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21559v1', 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prasanna Reddy Pulakurthi, Jiamian Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Zhiqiang Tao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: 12 pages, 7 figures. Accepted at EMNLP 2025 (Main Conference)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/PrasannaPulakurthi/X-CoT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºX-CoTï¼Œåˆ©ç”¨LLMé“¾å¼æ€è€ƒæ¨ç†å®ç°å¯è§£é‡Šçš„æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢` `å¯è§£é‡Šæ€§` `é“¾å¼æ€è€ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢æ–¹æ³•ä¾èµ–åµŒå…¥æ¨¡å‹å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”æ˜“å—ä½è´¨é‡æ•°æ®å½±å“ã€‚
2. X-CoTåˆ©ç”¨LLMçš„é“¾å¼æ€è€ƒèƒ½åŠ›ï¼Œé€šè¿‡æˆå¯¹æ¯”è¾ƒç”Ÿæˆè¯¦ç»†æ¨ç†ï¼Œå®ç°å¯è§£é‡Šçš„æ£€ç´¢æ’åºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒX-CoTæå‡äº†æ£€ç´¢æ€§èƒ½ï¼Œå¹¶èƒ½æä¾›æ’åºç†ç”±ï¼Œæœ‰åŠ©äºæ¨¡å‹è¡Œä¸ºåˆ†æå’Œæ•°æ®è´¨é‡è¯„ä¼°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢ç³»ç»Ÿä¸»è¦é‡‡ç”¨åµŒå…¥æ¨¡å‹æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œæ’åºã€‚ç„¶è€Œï¼Œè¿™ç§è®¾è®¡å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šä½è´¨é‡çš„æ–‡æœ¬-è§†é¢‘æ•°æ®å¯¹ä¼šå½±å“æ£€ç´¢æ•ˆæœï¼Œä¸”éš¾ä»¥è¯†åˆ«å’Œæ£€æŸ¥ï¼›ä»…ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ— æ³•è§£é‡Šæ’åºç»“æœï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶æ˜¯å¦å¯ä»¥è§£é‡Šæ’åºç»“æœï¼Œä»è€Œè¯„ä¼°æ£€ç´¢æ¨¡å‹å¹¶æ£€æŸ¥æ–‡æœ¬-è§†é¢‘æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†X-CoTï¼Œä¸€ä¸ªåŸºäºLLMé“¾å¼æ€è€ƒï¼ˆCoTï¼‰æ¨ç†çš„å¯è§£é‡Šæ£€ç´¢æ¡†æ¶ï¼Œæ›¿ä»£äº†åŸºäºåµŒå…¥æ¨¡å‹çš„ç›¸ä¼¼åº¦æ’åºã€‚é¦–å…ˆï¼Œé€šè¿‡å¢åŠ é¢å¤–çš„è§†é¢‘æ ‡æ³¨æ¥æ‰©å±•ç°æœ‰åŸºå‡†ï¼Œä»¥æ”¯æŒè¯­ä¹‰ç†è§£å¹¶å‡å°‘æ•°æ®åå·®ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§åŒ…å«æˆå¯¹æ¯”è¾ƒæ­¥éª¤çš„æ£€ç´¢CoTï¼Œä»è€Œäº§ç”Ÿè¯¦ç»†çš„æ¨ç†å’Œå®Œæ•´çš„æ’åºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-CoTåœ¨ç»éªŒä¸Šæé«˜äº†æ£€ç´¢æ€§èƒ½ï¼Œå¹¶äº§ç”Ÿäº†è¯¦ç»†çš„ç†ç”±ï¼ŒåŒæ—¶æœ‰åŠ©äºæ¨¡å‹è¡Œä¸ºå’Œæ•°æ®è´¨é‡åˆ†æã€‚ä»£ç å’Œæ•°æ®å¯åœ¨https://github.com/PrasannaPulakurthi/X-CoTè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢ç³»ç»Ÿä¾èµ–äºåµŒå…¥æ¨¡å‹æå–æ–‡æœ¬å’Œè§†é¢‘çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¿›è¡Œæ’åºã€‚è¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œç”¨æˆ·æ— æ³•ç†è§£æ£€ç´¢ç»“æœèƒŒåçš„åŸå› ï¼›äºŒæ˜¯å®¹æ˜“å—åˆ°ä½è´¨é‡æ–‡æœ¬-è§†é¢‘æ•°æ®å¯¹çš„å½±å“ï¼Œå¯¼è‡´æ£€ç´¢æ€§èƒ½ä¸‹é™ï¼Œè€Œä¸”è¿™äº›ä½è´¨é‡æ•°æ®éš¾ä»¥è¯†åˆ«å’Œæ’é™¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šX-CoTçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€è€ƒï¼ˆChain-of-Thought, CoTï¼‰èƒ½åŠ›ï¼Œå°†æ£€ç´¢è¿‡ç¨‹è½¬åŒ–ä¸ºä¸€ä¸ªæ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡è®©LLMé€æ­¥æ¯”è¾ƒä¸åŒçš„è§†é¢‘å’Œæ–‡æœ¬æè¿°ï¼Œå¹¶ç»™å‡ºæ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„æ£€ç´¢æ’åºã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä¾èµ–åµŒå…¥æ¨¡å‹å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè€Œæ˜¯é€šè¿‡è¯­ä¹‰ç†è§£å’Œæ¨ç†æ¥åˆ¤æ–­è§†é¢‘å’Œæ–‡æœ¬çš„ç›¸å…³æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šX-CoTçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®å¢å¼ºæ¨¡å—ï¼šé€šè¿‡å¢åŠ è§†é¢‘çš„æ ‡æ³¨ä¿¡æ¯ï¼Œæé«˜æ•°æ®çš„è´¨é‡å’Œè¯­ä¹‰ä¸°å¯Œåº¦ï¼Œå‡å°‘æ•°æ®åå·®ã€‚2) æ£€ç´¢CoTæ¨¡å—ï¼šè¯¥æ¨¡å—æ˜¯X-CoTçš„æ ¸å¿ƒï¼Œå®ƒå°†æ£€ç´¢è¿‡ç¨‹åˆ†è§£ä¸ºä¸€ç³»åˆ—æˆå¯¹æ¯”è¾ƒæ­¥éª¤ã€‚å¯¹äºç»™å®šçš„æ–‡æœ¬æŸ¥è¯¢ï¼ŒLLMä¼šé€æ­¥æ¯”è¾ƒä¸åŒçš„è§†é¢‘ï¼Œå¹¶ç»™å‡ºæ¨ç†è¿‡ç¨‹ï¼Œæœ€ç»ˆç”Ÿæˆå®Œæ•´çš„æ’åºã€‚3) è§£é‡Šç”Ÿæˆæ¨¡å—ï¼šè¯¥æ¨¡å—è´Ÿè´£å°†LLMçš„æ¨ç†è¿‡ç¨‹è½¬åŒ–ä¸ºå¯ç†è§£çš„è§£é‡Šï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ£€ç´¢ç»“æœèƒŒåçš„åŸå› ã€‚

**å…³é”®åˆ›æ–°**ï¼šX-CoTæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨LLMçš„é“¾å¼æ€è€ƒèƒ½åŠ›è¿›è¡Œæ–‡æœ¬åˆ°è§†é¢‘çš„æ£€ç´¢ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåµŒå…¥æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒX-CoTèƒ½å¤Ÿæä¾›å¯è§£é‡Šçš„æ£€ç´¢ç»“æœï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ä½è´¨é‡æ•°æ®ã€‚æ­¤å¤–ï¼ŒX-CoTè¿˜é€šè¿‡æ•°æ®å¢å¼ºå’Œæ£€ç´¢CoTçš„è®¾è®¡ï¼Œæé«˜äº†æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šX-CoTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è§†é¢‘æ ‡æ³¨ï¼šä¸ºäº†æé«˜æ•°æ®çš„è´¨é‡å’Œè¯­ä¹‰ä¸°å¯Œåº¦ï¼Œè®ºæ–‡å¢åŠ äº†è§†é¢‘çš„æ ‡æ³¨ä¿¡æ¯ï¼Œä¾‹å¦‚è§†é¢‘çš„å†…å®¹æè¿°ã€å…³é”®å¸§ç­‰ã€‚2) æ£€ç´¢CoTï¼šè®ºæ–‡è®¾è®¡äº†ä¸€ç§åŒ…å«æˆå¯¹æ¯”è¾ƒæ­¥éª¤çš„æ£€ç´¢CoTï¼ŒLLMä¼šé€æ­¥æ¯”è¾ƒä¸åŒçš„è§†é¢‘ï¼Œå¹¶ç»™å‡ºæ¨ç†è¿‡ç¨‹ã€‚3) æç¤ºå·¥ç¨‹ï¼šè®ºæ–‡è®¾è®¡äº†åˆé€‚çš„æç¤ºè¯­ï¼Œå¼•å¯¼LLMè¿›è¡Œé“¾å¼æ€è€ƒï¼Œå¹¶ç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

X-CoTåœ¨æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æ£€ç´¢ç†ç”±ã€‚é€šè¿‡ä¸ä¼ ç»ŸåµŒå…¥æ¨¡å‹æ–¹æ³•å¯¹æ¯”ï¼ŒX-CoTåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ›´å¥½çš„æ£€ç´¢å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒX-CoTç”Ÿæˆçš„æ£€ç´¢ç†ç”±èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ç†è§£æ£€ç´¢ç»“æœï¼Œå¹¶æœ‰åŠ©äºåˆ†ææ¨¡å‹è¡Œä¸ºå’Œæ•°æ®è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

X-CoTå¯åº”ç”¨äºè§†é¢‘æœç´¢å¼•æ“ã€æ™ºèƒ½æ¨èç³»ç»Ÿã€æ•™è‚²è§†é¢‘æ£€ç´¢ç­‰é¢†åŸŸã€‚å…¶å¯è§£é‡Šæ€§èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ç†è§£æ£€ç´¢ç»“æœï¼Œæé«˜ç”¨æˆ·ä¿¡ä»»åº¦ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ†ææ¨¡å‹è¡Œä¸ºå’Œæ•°æ®è´¨é‡ï¼Œæœ‰åŠ©äºæå‡æ£€ç´¢ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚æœªæ¥ï¼ŒX-CoTæœ‰æœ›åœ¨å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€å†…å®¹å®¡æ ¸ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute cosine similarities for ranking. However, this design presents two limitations. Low-quality text-video data pairs could compromise the retrieval, yet are hard to identify and examine. Cosine similarity alone provides no explanation for the ranking results, limiting the interpretability. We ask that can we interpret the ranking results, so as to assess the retrieval models and examine the text-video data? This work proposes X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of the embedding model-based similarity ranking. We first expand the existing benchmarks with additional video annotations to support semantic understanding and reduce data bias. We also devise a retrieval CoT consisting of pairwise comparison steps, yielding detailed reasoning and complete ranking. X-CoT empirically improves the retrieval performance and produces detailed rationales. It also facilitates the model behavior and data quality analysis. Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.

