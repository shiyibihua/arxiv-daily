---
layout: default
title: MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources
---

# MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21268" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21268v1</a>
  <a href="https://arxiv.org/pdf/2509.21268.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21268v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21268v1', 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, Deli Zhao, Wei Lu, Yu Rong, Aixin Sun, Shijian Lu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LengSicong/MMR1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MMR1ï¼šé€šè¿‡æ–¹å·®æ„ŸçŸ¥é‡‡æ ·å’Œå¼€æ”¾èµ„æºå¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ–¹å·®æ„ŸçŸ¥é‡‡æ ·` `é•¿é“¾æ€è€ƒ` `æ•°æ®å¢å¼º` `ç­–ç•¥ä¼˜åŒ–` `æ•°å­¦æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å—é™äºç¼ºä¹é«˜è´¨é‡é•¿é“¾æ€è€ƒæ•°æ®ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ å¾®è°ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚
2. è®ºæ–‡æå‡ºæ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰ç­–ç•¥ï¼Œé€šè¿‡æ–¹å·®æå‡åˆ†æ•°ï¼ˆVPSï¼‰æŒ‡å¯¼æ•°æ®é€‰æ‹©ï¼Œæå‡å¥–åŠ±æ–¹å·®å¹¶ç¨³å®šç­–ç•¥ä¼˜åŒ–ã€‚
3. è®ºæ–‡å‘å¸ƒäº†å¤§è§„æ¨¡é•¿é“¾æ€è€ƒæ•°æ®å’Œå¼ºåŒ–å­¦ä¹ QAå¯¹ï¼Œå¹¶å¼€æºäº†å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œå®éªŒéªŒè¯äº†æ•°æ®å’ŒVASçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å…¶å‘å±•å—åˆ°ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šç¼ºä¹å¼€æ”¾ã€å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é•¿é“¾æ€è€ƒï¼ˆCoTï¼‰æ•°æ®ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•åœ¨åè®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§ã€‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ˜¯RLå¾®è°ƒçš„æ ‡å‡†æ¡†æ¶ï¼Œå½“å¥–åŠ±æ–¹å·®è¾ƒä½æ—¶å®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±ï¼Œè¿™å‰Šå¼±äº†ä¼˜åŒ–ä¿¡å·å¹¶æŸå®³äº†æ”¶æ•›æ€§ã€‚æœ¬å·¥ä½œåšå‡ºäº†ä¸‰é¡¹è´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç”±æ–¹å·®æå‡åˆ†æ•°ï¼ˆVPSï¼‰æŒ‡å¯¼ï¼Œç»“åˆäº†ç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§ï¼Œä»¥ä¿ƒè¿›å¥–åŠ±æ–¹å·®å¹¶ç¨³å®šç­–ç•¥ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å‘å¸ƒäº†å¤§è§„æ¨¡ã€ç²¾å¿ƒç­–åˆ’çš„èµ„æºï¼ŒåŒ…å«çº¦160ä¸‡ä¸ªé•¿CoTå†·å¯åŠ¨æ•°æ®å’Œçº¦1.5ä¸‡ä¸ªRL QAå¯¹ï¼Œæ—¨åœ¨ç¡®ä¿è´¨é‡ã€éš¾åº¦å’Œå¤šæ ·æ€§ï¼Œä»¥åŠå®Œå…¨å¯å¤ç°çš„ç«¯åˆ°ç«¯è®­ç»ƒä»£ç åº“ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å¼€æºäº†ä¸€ç³»åˆ—å¤šå°ºåº¦å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œä¸ºç¤¾åŒºå»ºç«‹äº†æ ‡å‡†åŒ–åŸºçº¿ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¯æ˜äº†æ‰€ç­–åˆ’æ•°æ®å’Œæ‰€æå‡ºçš„VASçš„æœ‰æ•ˆæ€§ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œåˆ†æè¿›ä¸€æ­¥æ·±å…¥äº†è§£äº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç†è®ºä¸Šå»ºç«‹äº†å¥–åŠ±æ–¹å·®ä½œä¸ºé¢„æœŸç­–ç•¥æ¢¯åº¦å¹…åº¦çš„ä¸‹ç•Œï¼ŒVASä½œä¸ºå®ç°è¿™ä¸€ä¿è¯çš„å®ç”¨æœºåˆ¶ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ£€æŸ¥ç‚¹å¯åœ¨https://github.com/LengSicong/MMR1è·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é•¿é“¾æ€è€ƒï¼ˆCoTï¼‰æ•°æ®ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å­¦ä¹ å¤æ‚æ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚äºŒæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒè¿‡ç¨‹ä¸ç¨³å®šï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ—¶ï¼Œä½å¥–åŠ±æ–¹å·®ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œé˜»ç¢æ¨¡å‹æ”¶æ•›å’Œæ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æå‡å¥–åŠ±æ–¹å·®æ¥ç¨³å®šå¼ºåŒ–å­¦ä¹ å¾®è°ƒè¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨é«˜è´¨é‡çš„CoTæ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ—¨åœ¨é€‰æ‹©é‚£äº›èƒ½å¤Ÿæœ€å¤§åŒ–å¥–åŠ±æ–¹å·®çš„æ•°æ®æ ·æœ¬ï¼Œä»è€Œä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æ›´å¼ºçš„ä¿¡å·ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜å‘å¸ƒäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„CoTæ•°æ®å’ŒRL QAå¯¹ï¼Œä¸ºæ¨¡å‹çš„è®­ç»ƒæä¾›äº†å……è¶³çš„èµ„æºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®å‡†å¤‡é˜¶æ®µåŒ…æ‹¬æ”¶é›†å’Œæ¸…æ´—å¤§è§„æ¨¡çš„CoTæ•°æ®å’ŒRL QAå¯¹ï¼Œå¹¶ä½¿ç”¨æ–¹å·®æå‡åˆ†æ•°ï¼ˆVPSï¼‰å¯¹æ•°æ®è¿›è¡ŒåŠ æƒã€‚æ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œç„¶åä½¿ç”¨CoTæ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒï¼Œæœ€åä½¿ç”¨RL QAå¯¹å’ŒVASç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚è¯„ä¼°é˜¶æ®µä½¿ç”¨æ•°å­¦æ¨ç†åŸºå‡†æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯æ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·æˆ–é‡è¦æ€§é‡‡æ ·ä¸åŒï¼ŒVASç­–ç•¥æ ¹æ®æ•°æ®çš„æ–¹å·®æå‡åˆ†æ•°ï¼ˆVPSï¼‰è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œé€‰æ‹©é‚£äº›èƒ½å¤Ÿæœ€å¤§åŒ–å¥–åŠ±æ–¹å·®çš„æ•°æ®æ ·æœ¬ã€‚è¿™ç§ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³RLå¾®è°ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šVASç­–ç•¥çš„å…³é”®è®¾è®¡åœ¨äºæ–¹å·®æå‡åˆ†æ•°ï¼ˆVPSï¼‰çš„è®¡ç®—ã€‚VPSç»¼åˆè€ƒè™‘äº†ç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§ï¼Œæ—¨åœ¨é€‰æ‹©é‚£äº›æ—¢å…·æœ‰é«˜å¥–åŠ±æ–¹å·®ï¼Œåˆèƒ½å¤Ÿè¦†ç›–ä¸åŒæ¨ç†è·¯å¾„çš„æ•°æ®æ ·æœ¬ã€‚å…·ä½“çš„è®¡ç®—å…¬å¼æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å‘å¸ƒäº†å¤§è§„æ¨¡çš„CoTæ•°æ®å’ŒRL QAå¯¹ï¼Œè¿™äº›æ•°æ®ç»è¿‡ç²¾å¿ƒç­–åˆ’ï¼Œä»¥ç¡®ä¿è´¨é‡ã€éš¾åº¦å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„VASç­–ç•¥å’Œæ‰€å‘å¸ƒçš„æ•°æ®é›†èƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šï¼Œä½¿ç”¨VASç­–ç•¥çš„æ¨¡å‹ç›¸æ¯”äºåŸºçº¿æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒVASç­–ç•¥å’Œé«˜è´¨é‡æ•°æ®éƒ½å¯¹æ¨¡å‹çš„æ€§èƒ½æå‡èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœºæ™¯çš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰ï¼Œä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´å¼ºå¤§çš„å†³ç­–æ”¯æŒå·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two major limitations: the absence of open, large-scale, high-quality long chain-of-thought (CoT) data, and the instability of reinforcement learning (RL) algorithms in post-training. Group Relative Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone to gradient vanishing when reward variance is low, which weakens optimization signals and impairs convergence. This work makes three contributions: (1) We propose Variance-Aware Sampling (VAS), a data selection strategy guided by Variance Promotion Score (VPS) that combines outcome variance and trajectory diversity to promote reward variance and stabilize policy optimization. (2) We release large-scale, carefully curated resources containing ~1.6M long CoT cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty, and diversity, along with a fully reproducible end-to-end training codebase. (3) We open-source a family of multimodal reasoning models in multiple scales, establishing standardized baselines for the community. Experiments across mathematical reasoning benchmarks demonstrate the effectiveness of both the curated data and the proposed VAS. Comprehensive ablation studies and analyses provide further insight into the contributions of each component. In addition, we theoretically establish that reward variance lower-bounds the expected policy gradient magnitude, with VAS serving as a practical mechanism to realize this guarantee. Our code, data, and checkpoints are available at https://github.com/LengSicong/MMR1.

