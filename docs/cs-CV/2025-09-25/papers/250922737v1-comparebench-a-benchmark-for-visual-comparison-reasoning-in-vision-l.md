---
layout: default
title: CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models
---

# CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22737" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22737v1</a>
  <a href="https://arxiv.org/pdf/2509.22737.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22737v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22737v1', 'CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jie Cai, Kangning Yang, Lan Fu, Jiaming Ding, Jinlong Li, Huiming Sun, Daitao Xing, Jinglin Shen, Zibo Meng

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCompareBenchï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¯”è¾ƒæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è§†è§‰æ¯”è¾ƒæ¨ç†` `åŸºå‡†æµ‹è¯•` `å¤šæ¨¡æ€å­¦ä¹ ` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¯”è¾ƒæ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶é—´ã€ç©ºé—´å…³ç³»ç­‰å¤æ‚åœºæ™¯ä¸‹ã€‚
2. CompareBenchåŸºå‡†åŒ…å«æ•°é‡ã€æ—¶é—´ã€å‡ ä½•å’Œç©ºé—´å››ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹çš„è§†è§‰æ¯”è¾ƒèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è§†è§‰æ¯”è¾ƒä»»åŠ¡ä¸Šä»å­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼Œæœ‰å¾…è¿›ä¸€æ­¥æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†CompareBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¸­è§†è§‰æ¯”è¾ƒæ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œè¿™æ˜¯ä¸€é¡¹åŸºç¡€ä½†å°šæœªå……åˆ†ç ”ç©¶çš„æŠ€èƒ½ã€‚CompareBenchåŒ…å«1000ä¸ªQAå¯¹ï¼Œæ¶µç›–å››ä¸ªä»»åŠ¡ï¼šæ•°é‡(600)ã€æ—¶é—´(100)ã€å‡ ä½•(200)å’Œç©ºé—´(100)ã€‚å®ƒæºè‡ªæˆ‘ä»¬æ„å»ºçš„ä¸¤ä¸ªè¾…åŠ©æ•°æ®é›†ï¼šTallyBenchï¼ˆåŒ…å«2000ä¸ªå¸¦æœ‰QAçš„è®¡æ•°å›¾åƒï¼‰å’ŒHistCapsï¼ˆåŒ…å«515ä¸ªå¸¦æœ‰åŒè¯­å­—å¹•çš„å†å²å›¾åƒï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†é—­æºAPIï¼ˆOpenAIã€Geminiã€Claudeï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆQwen2.5-VLå’ŒQwen3-VLç³»åˆ—ï¼‰ã€‚ç»“æœæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ‰©å±•è¶‹åŠ¿ï¼Œä½†ä¹Ÿæ­ç¤ºäº†å…³é”®çš„å±€é™æ€§ï¼šå³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹ä¹Ÿå§‹ç»ˆæ— æ³•è¿›è¡Œæ—¶é—´æ’åºå’Œç©ºé—´å…³ç³»æ¨ç†ï¼Œå¹¶ä¸”å®ƒä»¬ç»å¸¸åœ¨äººç±»çœ‹æ¥å¾®ä¸è¶³é“çš„åŸºæœ¬è®¡æ•°å’Œå‡ ä½•æ¯”è¾ƒä¸­çŠ¯é”™ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè§†è§‰æ¯”è¾ƒä»ç„¶æ˜¯å½“å‰VLMçš„ä¸€ä¸ªç³»ç»Ÿæ€§ç›²ç‚¹ã€‚é€šè¿‡æä¾›å—æ§ã€å¤šæ ·åŒ–å’Œè¯Šæ–­æ€§è¯„ä¼°ï¼ŒCompareBenchä¸ºæ¨è¿›æ›´å¯é çš„å¤šæ¨¡æ€æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†éœ€è¦è§†è§‰æ¯”è¾ƒæ¨ç†çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å›¾åƒä¸­çš„æ•°é‡å…³ç³»ã€æ—¶é—´é¡ºåºã€å‡ ä½•å½¢çŠ¶å’Œç©ºé—´å…³ç³»ç­‰æ–¹é¢ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¯èƒ½æ— æ³•å……åˆ†è¯„ä¼°è¿™äº›èƒ½åŠ›ï¼Œæˆ–è€…ç¼ºä¹è¶³å¤Ÿçš„å¤šæ ·æ€§å’Œæ§åˆ¶æ€§ï¼Œéš¾ä»¥è¯Šæ–­æ¨¡å‹çš„å…·ä½“å¼±ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°VLMsè§†è§‰æ¯”è¾ƒæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå³CompareBenchã€‚é€šè¿‡è®¾è®¡åŒ…å«ä¸åŒç±»å‹æ¯”è¾ƒä»»åŠ¡çš„QAå¯¹ï¼Œå¹¶ç»“åˆè¾…åŠ©æ•°æ®é›†ï¼Œå¯ä»¥æ›´å…¨é¢ã€æ·±å…¥åœ°åˆ†æVLMsåœ¨è§†è§‰æ¯”è¾ƒæ–¹é¢çš„ä¼˜åŠ¿å’Œä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCompareBenchçš„æ„å»ºåŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1)å®šä¹‰æ¯”è¾ƒä»»åŠ¡ç±»å‹ï¼šç¡®å®šäº†æ•°é‡ã€æ—¶é—´ã€å‡ ä½•å’Œç©ºé—´å››ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚2)æ„å»ºè¾…åŠ©æ•°æ®é›†ï¼šåˆ›å»ºäº†TallyBenchï¼ˆç”¨äºè®¡æ•°ï¼‰å’ŒHistCapsï¼ˆç”¨äºå†å²å›¾åƒç†è§£ï¼‰ä¸¤ä¸ªæ•°æ®é›†ï¼Œä¸ºCompareBenchæä¾›æ•°æ®åŸºç¡€ã€‚3)ç”ŸæˆQAå¯¹ï¼šåŸºäºè¾…åŠ©æ•°æ®é›†ï¼Œè®¾è®¡å¹¶ç”Ÿæˆäº†åŒ…å«1000ä¸ªQAå¯¹çš„CompareBenchåŸºå‡†ã€‚4)æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨CompareBenchè¯„ä¼°äº†åŒ…æ‹¬é—­æºAPIå’Œå¼€æºæ¨¡å‹åœ¨å†…çš„å¤šç§VLMsã€‚

**å…³é”®åˆ›æ–°**ï¼šCompareBenchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸“æ³¨äºè§†è§‰æ¯”è¾ƒæ¨ç†è¿™ä¸€ç‰¹å®šèƒ½åŠ›ï¼Œå¹¶æä¾›äº†å¤šæ ·åŒ–å’Œæ§åˆ¶æ€§çš„è¯„ä¼°ã€‚ä¸ç°æœ‰çš„é€šç”¨VQAåŸºå‡†ç›¸æ¯”ï¼ŒCompareBenchæ›´å…·é’ˆå¯¹æ€§ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯Šæ–­VLMsåœ¨è§†è§‰æ¯”è¾ƒæ–¹é¢çš„å¼±ç‚¹ã€‚æ­¤å¤–ï¼Œè¾…åŠ©æ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºåŸºå‡†æµ‹è¯•æä¾›äº†æ›´ä¸°å¯Œçš„æ•°æ®æ¥æºã€‚

**å…³é”®è®¾è®¡**ï¼šCompareBenchåŒ…å«å››ä¸ªä»»åŠ¡ï¼šæ•°é‡æ¯”è¾ƒï¼ˆä¾‹å¦‚ï¼Œå“ªä¸ªå›¾åƒä¸­çš„ç‰©ä½“æ›´å¤šï¼Ÿï¼‰ã€æ—¶é—´æ¯”è¾ƒï¼ˆä¾‹å¦‚ï¼Œå“ªä¸ªäº‹ä»¶å‘ç”Ÿåœ¨å‰ï¼Ÿï¼‰ã€å‡ ä½•æ¯”è¾ƒï¼ˆä¾‹å¦‚ï¼Œå“ªä¸ªå½¢çŠ¶æ›´å¤§ï¼Ÿï¼‰ã€ç©ºé—´æ¯”è¾ƒï¼ˆä¾‹å¦‚ï¼Œå“ªä¸ªç‰©ä½“åœ¨å¦ä¸€ä¸ªç‰©ä½“çš„å·¦è¾¹ï¼Ÿï¼‰ã€‚æ¯ä¸ªä»»åŠ¡éƒ½è®¾è®¡äº†ç›¸åº”çš„QAå¯¹ï¼Œå¹¶æ§åˆ¶äº†å›¾åƒçš„å¤æ‚åº¦å’Œé—®é¢˜çš„éš¾åº¦ã€‚TallyBenchåŒ…å«2000ä¸ªè®¡æ•°å›¾åƒï¼ŒHistCapsåŒ…å«515ä¸ªå†å²å›¾åƒï¼Œè¿™äº›æ•°æ®é›†ç”¨äºç”Ÿæˆç›¸åº”çš„QAå¯¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨CompareBenchä¸Šçš„è¡¨ç°ä»ç„¶ä¸å°½å¦‚äººæ„ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶é—´æ’åºå’Œç©ºé—´å…³ç³»æ¨ç†æ–¹é¢ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹åœ¨æ—¶é—´æ¯”è¾ƒä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¿œä½äºäººç±»æ°´å¹³ï¼Œè¿™è¡¨æ˜å½“å‰æ¨¡å‹åœ¨ç†è§£æ—¶é—´åºåˆ—å’Œç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨è§†è§‰æ¯”è¾ƒæ¨ç†æ–¹é¢çš„ç³»ç»Ÿæ€§ç›²ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CompareBenchå¯ç”¨äºè¯„ä¼°å’Œæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å›¾åƒæ£€ç´¢å’Œè§†è§‰è¾…åŠ©ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨è§†è§‰æ¯”è¾ƒæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.

