---
layout: default
title: Instruction-tuned Self-Questioning Framework for Multimodal Reasoning
---

# Instruction-tuned Self-Questioning Framework for Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21251" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21251v1</a>
  <a href="https://arxiv.org/pdf/2509.21251.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21251v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21251v1', 'Instruction-tuned Self-Questioning Framework for Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: You-Won Jang, Yu-Jung Heo, Jaeseok Kim, Minsu Lee, Du-Seong Chang, Byoung-Tak Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**å¤‡æ³¨**: This paper was accepted to the "CLVL: 5th Workshop on Closing the Loop Between Vision and Language (ICCV 2023 CLVL workshop)."

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæŒ‡ä»¤è°ƒä¼˜çš„è‡ªé—®æ¡†æ¶SQ-InstructBLIPï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰-è¯­è¨€ç†è§£` `è‡ªé—®è‡ªç­”` `æŒ‡ä»¤è°ƒä¼˜` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å›¾åƒä¸­çš„ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ã€‚
2. æå‡ºSQ-InstructBLIPæ¡†æ¶ï¼Œé€šè¿‡è‡ªé—®è‡ªç­”çš„æ–¹å¼ï¼Œè¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆï¼Œè¾…åŠ©æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSQ-InstructBLIPåœ¨VQAä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œè§†è§‰-è¯­è¨€ç†è§£é¢†åŸŸçš„ç ”ç©¶éå¸¸æ´»è·ƒã€‚ç„¶è€Œï¼Œå³ä½¿å¯¹äºéå¸¸ç®€å•çš„é—®é¢˜ï¼Œè¯¥é¢†åŸŸåœ¨éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„é—®é¢˜ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚æœ€è¿‘çš„ç ”ç©¶é‡‡ç”¨LLMsé€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼Œä¾‹å¦‚ï¼š1) LLMsæ— æ³•è¯»å–è§†è§‰ä¿¡æ¯ï¼Œå› æ­¤æ— æ³•åˆ©ç”¨å›¾åƒçš„ç»†ç²’åº¦è§†è§‰å†…å®¹ï¼›2) ä½¿ç”¨é»‘ç›’LLMså¯¼è‡´å†…éƒ¨æœºåˆ¶ä¸å¯è®¿é—®ä¸”éš¾ä»¥é‡ç°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQï¼ˆSelf-Questioningï¼‰-InstructBLIPï¼Œå®ƒé€šè¿‡è¿­ä»£ç”Ÿæˆå›¾åƒæ„ŸçŸ¥çš„ã€ä¿¡æ¯ä¸°å¯Œçš„å­é—®é¢˜å’Œå­ç­”æ¡ˆæ¥æé«˜æ¨ç†æ€§èƒ½ã€‚SQ-InstructBLIPç”±Questionerã€Answererå’ŒReasonerç»„æˆï¼Œå®ƒä»¬å…±äº«ç›¸åŒçš„æ¶æ„ã€‚Questionerå’ŒAnswererç”Ÿæˆå­é—®é¢˜å’Œå­ç­”æ¡ˆæ¥å¸®åŠ©æ¨æ–­ä¸»è¦é—®é¢˜ï¼ŒReasoneråœ¨è€ƒè™‘ç”Ÿæˆçš„å­é—®é¢˜ä¿¡æ¯çš„æƒ…å†µä¸‹å¯¹ä¸»è¦é—®é¢˜æ‰§è¡Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•SQ-InstructBLIPåœ¨è§£å†³VQAä»»åŠ¡æ—¶ï¼Œä½¿ç”¨ç”Ÿæˆçš„å­é—®é¢˜ä½œä¸ºé™„åŠ ä¿¡æ¯ï¼Œæ¯”ä»¥å‰çš„æ–¹æ³•æ‰§è¡Œæ›´å‡†ç¡®çš„æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„ä»»åŠ¡æ—¶ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å›¾åƒä¸­çš„ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½å—é™ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é»‘ç›’LLMä½¿å¾—æ¨¡å‹å†…éƒ¨æœºåˆ¶ä¸å¯è§£é‡Šï¼Œéš¾ä»¥å¤ç°å’Œä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººç±»è§£å†³å¤æ‚é—®é¢˜çš„è¿‡ç¨‹ï¼Œé€šè¿‡è‡ªé—®è‡ªç­”çš„æ–¹å¼ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—å­é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å­é—®é¢˜çš„ç­”æ¡ˆæ¥è¾…åŠ©è§£å†³åŸå§‹é—®é¢˜ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåˆ©ç”¨å›¾åƒä¸­çš„ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ï¼Œå¹¶æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSQ-InstructBLIPæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šQuestionerã€Answererå’ŒReasonerã€‚Questionerè´Ÿè´£æ ¹æ®å½“å‰ä¿¡æ¯ï¼ˆåŒ…æ‹¬å›¾åƒå’ŒåŸå§‹é—®é¢˜ï¼‰ç”Ÿæˆå­é—®é¢˜ï¼›Answererè´Ÿè´£æ ¹æ®å›¾åƒå’Œå­é—®é¢˜ç”Ÿæˆå­ç­”æ¡ˆï¼›Reasonerè´Ÿè´£æ ¹æ®åŸå§‹é—®é¢˜ã€å­é—®é¢˜å’Œå­ç­”æ¡ˆè¿›è¡Œæ¨ç†ï¼Œæœ€ç»ˆç»™å‡ºç­”æ¡ˆã€‚è¿™ä¸‰ä¸ªæ¨¡å—å…±äº«ç›¸åŒçš„æ¶æ„ï¼Œå¹¶è¿›è¡Œè”åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå¯è®­ç»ƒçš„è‡ªé—®è‡ªç­”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå›¾åƒæ„ŸçŸ¥çš„ã€ä¿¡æ¯ä¸°å¯Œçš„å­é—®é¢˜å’Œå­ç­”æ¡ˆï¼Œä»è€Œæœ‰æ•ˆæé«˜å¤šæ¨¡æ€æ¨ç†çš„æ€§èƒ½ã€‚ä¸ä»¥å¾€ä½¿ç”¨é»‘ç›’LLMçš„æ–¹æ³•ä¸åŒï¼ŒSQ-InstructBLIPçš„å†…éƒ¨æœºåˆ¶æ˜¯å¯è§£é‡Šçš„ï¼Œå¹¶ä¸”æ˜“äºå¤ç°å’Œä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šQuestionerã€Answererå’ŒReasonerå…±äº«ç›¸åŒçš„InstructBLIPæ¶æ„ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜çš„æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œå„ç§ä»»åŠ¡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é—®é¢˜ç”ŸæˆæŸå¤±ã€ç­”æ¡ˆç”ŸæˆæŸå¤±å’Œæœ€ç»ˆç­”æ¡ˆé¢„æµ‹æŸå¤±ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„SQ-InstructBLIPæ–¹æ³•åœ¨VQAä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡ç”Ÿæˆå­é—®é¢˜ä½œä¸ºé™„åŠ ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¿›è¡Œæ›´å‡†ç¡®çš„æ¨ç†ï¼Œä¼˜äºä¹‹å‰çš„ç ”ç©¶å·¥ä½œã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ¨¡æ€æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€å›¾åƒç†è§£ã€è§†è§‰å¯¼èˆªç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œé¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The field of vision-language understanding has been actively researched in recent years, thanks to the development of Large Language Models~(LLMs). However, it still needs help with problems requiring multi-step reasoning, even for very simple questions. Recent studies adopt LLMs to tackle this problem by iteratively generating sub-questions and answers. However, there are disadvantages such as 1) the fine-grained visual contents of images are not available using LLMs that cannot read visual information, 2) internal mechanisms are inaccessible and difficult to reproduce by using black-box LLMs. To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP, which improves inference performance by generating image-aware informative sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists of a Questioner, Answerer, and Reasoner that share the same architecture. Questioner and Answerer generate sub-questions and sub-answers to help infer the main-question, and Reasoner performs reasoning on the main-question considering the generated sub-question information. Our experiments show that the proposed method SQ-InstructBLIP, which uses the generated sub-questions as additional information when solving the VQA task, performs more accurate reasoning than the previous works.

