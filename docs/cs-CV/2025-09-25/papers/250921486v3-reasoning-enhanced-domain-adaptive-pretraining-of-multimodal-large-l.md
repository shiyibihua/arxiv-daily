---
layout: default
title: Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance
---

# Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21486" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21486v3</a>
  <a href="https://arxiv.org/pdf/2509.21486.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21486v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21486v3', 'Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixuan Wang, Yu Sun, Hongwei Wang, Baoyu Jing, Xiang Shen, Xin Dong, Zhuolin Hao, Hongyu Xiong, Yang Song

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: Camera Ready for EMNLP 2025

**DOI**: [10.18653/v1/2025.emnlp-industry.77](https://doi.org/10.18653/v1/2025.emnlp-industry.77)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨ç†å¢å¼ºçš„é¢†åŸŸè‡ªé€‚åº”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºçŸ­è§†é¢‘å†…å®¹æ²»ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ­è§†é¢‘å†…å®¹æ²»ç†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒ` `æ¨ç†å¢å¼º` `è§†è§‰é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ­è§†é¢‘å†…å®¹å®¡æ ¸æ–¹æ³•ä¾èµ–äºé’ˆå¯¹ç‰¹å®šé—®é¢˜è®­ç»ƒçš„å°æ¨¡å‹ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ä¸”æ³›åŒ–æ€§å·®ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ¨ç†å¢å¼ºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é¢†åŸŸè‡ªé€‚åº”å’Œæ¨ç†èƒ½åŠ›æå‡ï¼Œå®ç°ç»Ÿä¸€çš„ä¸å½“å†…å®¹æ£€æµ‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒè®¾ç½®ä¸‹å‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¯¹æ–°é—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ­è§†é¢‘å¹³å°å¿«é€Ÿå‘å±•ï¼Œä¸å½“å†…å®¹çš„è¯†åˆ«å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸ºæ¯ç§é—®é¢˜ç±»å‹è®­ç»ƒç‹¬ç«‹çš„å°å‹åˆ†ç±»æ¨¡å‹ï¼Œè¿™éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œä¸”ç¼ºä¹è·¨é—®é¢˜æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨ç†å¢å¼ºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢„è®­ç»ƒèŒƒå¼ï¼Œç”¨äºç»Ÿä¸€çš„ä¸å½“å†…å®¹æ£€æµ‹ã€‚ä¸ºäº†è§£å†³çŸ­è§†é¢‘å†…å®¹ä¸MLLMåŸå§‹é¢„è®­ç»ƒæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®è·ä»¥åŠå¤æ‚çš„é—®é¢˜å®šä¹‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªæœ‰é’ˆå¯¹æ€§çš„é¢„è®­ç»ƒä»»åŠ¡ï¼šï¼ˆ1ï¼‰Captionï¼Œå¢å¼ºMLLMå¯¹è§†é¢‘ç»†èŠ‚çš„æ„ŸçŸ¥ï¼›ï¼ˆ2ï¼‰è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼ŒåŠ æ·±MLLMå¯¹é—®é¢˜å®šä¹‰å’Œæ ‡æ³¨æŒ‡å—çš„ç†è§£ï¼›ï¼ˆ3ï¼‰æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå¢å¼ºMLLMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMåœ¨é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®¾ç½®ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹è¿˜å±•ç¤ºäº†å¯¹æ–°å…´çš„ã€ä»¥å‰æœªè§è¿‡çš„é—®é¢˜çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰çŸ­è§†é¢‘å¹³å°çš„å†…å®¹å®¡æ ¸é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯éœ€è¦å¤„ç†å„ç§å„æ ·çš„ä¸å½“å†…å®¹ç±»å‹ï¼ŒäºŒæ˜¯ç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹æ¯ç§ç±»å‹è®­ç»ƒç‹¬ç«‹çš„åˆ†ç±»æ¨¡å‹ï¼Œå¯¼è‡´éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨æ•°æ®ï¼Œå¹¶ä¸”æ¨¡å‹ä¹‹é—´ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒçŸ­è§†é¢‘å†…å®¹ä¸ç°æœ‰MLLMçš„é¢„è®­ç»ƒæ•°æ®å­˜åœ¨åˆ†å¸ƒå·®å¼‚ï¼Œä½¿å¾—ç›´æ¥åº”ç”¨æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ¥æå‡MLLMåœ¨çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è®¾è®¡ä¸‰ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼Œä½¿MLLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è§†é¢‘å†…å®¹ã€é—®é¢˜å®šä¹‰å’Œæ ‡æ³¨æŒ‡å—ï¼Œå¹¶å…·å¤‡æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„ä¸å½“å†…å®¹æ£€æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œåˆ©ç”¨Captionä»»åŠ¡å¢å¼ºMLLMå¯¹è§†é¢‘ç»†èŠ‚çš„æ„ŸçŸ¥ï¼›å…¶æ¬¡ï¼Œé€šè¿‡VQAä»»åŠ¡åŠ æ·±MLLMå¯¹é—®é¢˜å®šä¹‰å’Œæ ‡æ³¨æŒ‡å—çš„ç†è§£ï¼›æœ€åï¼Œä½¿ç”¨CoTä»»åŠ¡å¢å¼ºMLLMçš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸‰ä¸ªä»»åŠ¡å…±åŒä½œç”¨ï¼Œä½¿å¾—MLLMèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªæ¨ç†å¢å¼ºçš„é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼é’ˆå¯¹çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†ä¸‰ä¸ªå®šåˆ¶åŒ–çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ•°æ®åˆ†å¸ƒå·®å¼‚å’Œé—®é¢˜å®šä¹‰å¤æ‚çš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šé—®é¢˜è®­ç»ƒå°æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ›´é«˜çš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šCaptionä»»åŠ¡ä½¿ç”¨è§†é¢‘å¸§å’Œå¯¹åº”çš„æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯è®©MLLMèƒ½å¤Ÿæ ¹æ®è§†é¢‘å†…å®¹ç”Ÿæˆå‡†ç¡®çš„æè¿°ã€‚VQAä»»åŠ¡ä½¿ç”¨åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„è§†é¢‘ç‰‡æ®µè¿›è¡Œè®­ç»ƒï¼Œé—®é¢˜å›´ç»•ä¸å½“å†…å®¹å®šä¹‰å’Œæ ‡æ³¨æŒ‡å—å±•å¼€ï¼Œç›®æ ‡æ˜¯è®©MLLMèƒ½å¤Ÿç†è§£é—®é¢˜å®šä¹‰å¹¶ç»™å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚CoTä»»åŠ¡åˆ™é€šè¿‡å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¼•å¯¼MLLMè¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»è€Œå¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é¢„è®­ç»ƒæ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMåœ¨çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒåœ¨é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒè®¾ç½®ä¸‹å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”æ¨¡å‹å±•ç°å‡ºå¯¹æ–°å…´é—®é¢˜çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œæ³›åŒ–æ€§æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§çŸ­è§†é¢‘å¹³å°çš„å†…å®¹å®¡æ ¸ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å’Œè¿‡æ»¤ä¸å½“å†…å®¹ï¼Œæé«˜å†…å®¹å®¡æ ¸æ•ˆç‡ï¼Œé™ä½äººå·¥å®¡æ ¸æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€å†…å®¹å®¡æ ¸åœºæ™¯ï¼Œä¾‹å¦‚å›¾åƒã€ç›´æ’­ç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Short video platforms are evolving rapidly, making the identification of inappropriate content increasingly critical. Existing approaches typically train separate and small classification models for each type of issue, which requires extensive human-labeled data and lacks cross-issue generalization. We propose a reasoning-enhanced multimodal large language model (MLLM) pretraining paradigm for unified inappropriate content detection. To address the distribution gap between short video content and the original pretraining data of MLLMs, as well as the complex issue definitions, we introduce three targeted pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the MLLM's understanding of issue definitions and annotation guidelines; (3) \textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability. Experimental results show that our pretraining approach significantly improves the MLLM's performance in both zero-shot and supervised fine-tuning (SFT) settings. In addition, our pretrained model demonstrates strong generalization capabilities to emergent, previously unseen issues.

