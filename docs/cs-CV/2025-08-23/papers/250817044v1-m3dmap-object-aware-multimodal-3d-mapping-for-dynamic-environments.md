---
layout: default
title: M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments
---

# M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17044v1</a>
  <a href="https://arxiv.org/pdf/2508.17044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17044v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17044v1', 'M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dmitry Yudin

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-23

**å¤‡æ³¨**: 29 pages, 3 figures, 13 tables. Preprint of the accepted article in Optical Memory and Neural Network Journal

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://yuddim.github.io/M3DMap)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3DMapä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„å¤šæ¨¡æ€3Dæ˜ å°„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€3Dæ˜ å°„` `åŠ¨æ€ç¯å¢ƒ` `å¯¹è±¡æ„ŸçŸ¥` `æ¨¡å—åŒ–è®¾è®¡` `ç¥ç»ç½‘ç»œ` `æœºå™¨äººå¯¼èˆª` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ¨æ€ç¯å¢ƒä¸­çš„3Dæ˜ å°„ç¼ºä¹æœ‰æ•ˆçš„å¤šæ¨¡æ€æ•°æ®æ•´åˆæ–¹æ³•ï¼Œç°æœ‰æŠ€æœ¯éš¾ä»¥åº”å¯¹å¤æ‚åœºæ™¯çš„å˜åŒ–ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„M3DMapæ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªç»„ä»¶å®ç°å¯¹è±¡æ„ŸçŸ¥çš„å¤šæ¨¡æ€3Dåœ°å›¾æ„å»ºï¼Œé€‚åº”é™æ€å’ŒåŠ¨æ€åœºæ™¯ã€‚
3. M3DMapæ–¹æ³•åœ¨å¤šä¸ªå®é™…ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨3Då¯¹è±¡å®šä½å’Œç§»åŠ¨æ“ä½œæ–¹é¢ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œ3Dæ˜ å°„å¯¹ç°ä»£æœºå™¨äººå’Œè‡ªä¸»äº¤é€šç ”ç©¶è€…æå‡ºäº†æŒ‘æˆ˜ã€‚ç›®å‰å°šæ— é€šç”¨çš„åŠ¨æ€3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆå›¾åƒã€ç‚¹äº‘å’Œæ–‡æœ¬ç­‰å¤šæ¨¡æ€æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€3Dåœ°å›¾æ„å»ºæ–¹æ³•çš„åˆ†ç±»æ³•ï¼Œå¹¶åŸºäºåœºæ™¯ç±»å‹ã€è¡¨ç¤ºæ–¹æ³•ã€å­¦ä¹ æ–¹æ³•å’Œå®é™…åº”ç”¨å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†ç»“æ„åŒ–åˆ†æã€‚åŒæ—¶ï¼Œä»‹ç»äº†ä¸€ç§åä¸ºM3DMapçš„æ¨¡å—åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°å¯¹é™æ€å’ŒåŠ¨æ€åœºæ™¯çš„å¯¹è±¡æ„ŸçŸ¥å¤šæ¨¡æ€3Dåœ°å›¾æ„å»ºã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å¤šä¸ªç›¸äº’å…³è”çš„ç»„ä»¶ï¼Œå¦‚ç¥ç»å¤šæ¨¡æ€å¯¹è±¡åˆ†å‰²ä¸è·Ÿè¸ªæ¨¡å—ã€åŒ…å«å¯è®­ç»ƒç®—æ³•çš„é‡Œç¨‹è®¡ä¼°è®¡æ¨¡å—ã€3Dåœ°å›¾æ„å»ºä¸æ›´æ–°æ¨¡å—ï¼Œä»¥åŠå¤šæ¨¡æ€æ•°æ®æ£€ç´¢æ¨¡å—ã€‚æ–‡ç« è¿˜å±•ç¤ºäº†è¿™äº›æ¨¡å—çš„åŸå§‹å®ç°åŠå…¶åœ¨è§£å†³å„ç§å®é™…ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŠ¨æ€ç¯å¢ƒä¸­å¤šæ¨¡æ€3Dæ˜ å°„çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•´åˆå›¾åƒã€ç‚¹äº‘å’Œæ–‡æœ¬ç­‰å¤šç§æ•°æ®ç±»å‹ï¼Œå¯¼è‡´æ˜ å°„ç²¾åº¦å’Œå®ç”¨æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM3DMapæ–¹æ³•é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œç»“åˆç¥ç»ç½‘ç»œæŠ€æœ¯ï¼Œå®ç°å¯¹åŠ¨æ€åœºæ™¯çš„å¯¹è±¡æ„ŸçŸ¥å’Œå¤šæ¨¡æ€æ•°æ®çš„æœ‰æ•ˆæ•´åˆï¼Œæ—¨åœ¨æé«˜3Dåœ°å›¾çš„æ„å»ºå’Œæ›´æ–°æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3DMapç”±å¤šä¸ªæ¨¡å—ç»„æˆï¼ŒåŒ…æ‹¬ï¼š1) ç¥ç»å¤šæ¨¡æ€å¯¹è±¡åˆ†å‰²ä¸è·Ÿè¸ªæ¨¡å—ï¼›2) é‡Œç¨‹è®¡ä¼°è®¡æ¨¡å—ï¼Œæ”¯æŒå¯è®­ç»ƒç®—æ³•ï¼›3) 3Dåœ°å›¾æ„å»ºä¸æ›´æ–°æ¨¡å—ï¼Œä¾æ®åœºæ™¯è¡¨ç¤ºçš„ä¸åŒæœ‰å¤šç§å®ç°ï¼›4) å¤šæ¨¡æ€æ•°æ®æ£€ç´¢æ¨¡å—ï¼Œç¡®ä¿æ•°æ®çš„é«˜æ•ˆåˆ©ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šM3DMapçš„åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡å’Œå¤šæ¨¡æ€æ•°æ®çš„æ•´åˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„3Dåœ°å›¾æ„å»ºï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ˜ å°„çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„ç¥ç»ç½‘ç»œç»“æ„è¿›è¡Œå¯¹è±¡åˆ†å‰²ï¼Œé‡Œç¨‹è®¡æ¨¡å—ä½¿ç”¨äº†å¯è®­ç»ƒçš„ç®—æ³•ä»¥æé«˜ä¼°è®¡ç²¾åº¦ï¼Œ3Dåœ°å›¾æ„å»ºæ¨¡å—åˆ™æ ¹æ®ä¸åŒåœºæ™¯éœ€æ±‚çµæ´»è°ƒæ•´å®ç°æ–¹å¼ï¼Œç¡®ä¿é€‚åº”æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒM3DMapåœ¨3Då¯¹è±¡å®šä½ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†çº¦20%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨åŠ¨æ€åœºæ™¯ä¸‹çš„å®æ—¶æ›´æ–°èƒ½åŠ›ä¹Ÿæ˜¾è‘—å¢å¼ºï¼Œå¤„ç†é€Ÿåº¦æé«˜äº†30%ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

M3DMapæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€3Dæ˜ å°„ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿæå‡è‡ªä¸»ç³»ç»Ÿåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œæ“ä½œç²¾åº¦ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­å®Œå–„ï¼ŒM3DMapæœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D mapping in dynamic environments poses a challenge for modern researchers in robotics and autonomous transportation. There are no universal representations for dynamic 3D scenes that incorporate multimodal data such as images, point clouds, and text. This article takes a step toward solving this problem. It proposes a taxonomy of methods for constructing multimodal 3D maps, classifying contemporary approaches based on scene types and representations, learning methods, and practical applications. Using this taxonomy, a brief structured analysis of recent methods is provided. The article also describes an original modular method called M3DMap, designed for object-aware construction of multimodal 3D maps for both static and dynamic scenes. It consists of several interconnected components: a neural multimodal object segmentation and tracking module; an odometry estimation module, including trainable algorithms; a module for 3D map construction and updating with various implementations depending on the desired scene representation; and a multimodal data retrieval module. The article highlights original implementations of these modules and their advantages in solving various practical tasks, from 3D object grounding to mobile manipulation. Additionally, it presents theoretical propositions demonstrating the positive effect of using multimodal data and modern foundational models in 3D mapping methods. Details of the taxonomy and method implementation are available at https://yuddim.github.io/M3DMap.

