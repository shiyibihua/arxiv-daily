---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-28
---

# cs.CVï¼ˆ2025-12-28ï¼‰

ğŸ“Š å…± **11** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251222009v1-ishift-lightweight-slow-fast-gui-agent-with-adaptive-perception.html">iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception</a></td>
  <td>iSHIFTï¼šè½»é‡çº§è‡ªé€‚åº”æ„ŸçŸ¥æ…¢-å¿«GUIä»£ç†ï¼Œæå‡äº¤äº’æ•ˆç‡ä¸ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.22009v1" data-paper-url="./papers/251222009v1-ishift-lightweight-slow-fast-gui-agent-with-adaptive-perception.html" onclick="toggleFavorite(this, '2512.22009v1', 'iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251222120v1-see-less-see-right-bi-directional-perceptual-shaping-for-multimodal-.html">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a></td>
  <td>æå‡ºåŒå‘æ„ŸçŸ¥å¡‘å½¢æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.22120v1" data-paper-url="./papers/251222120v1-see-less-see-right-bi-directional-perceptual-shaping-for-multimodal-.html" onclick="toggleFavorite(this, '2512.22120v1', 'See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251222046v1-backdoor-attacks-on-prompt-driven-video-segmentation-foundation-mode.html">Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</a></td>
  <td>æå‡ºBadVSFMï¼Œé’ˆå¯¹Prompté©±åŠ¨çš„è§†é¢‘åˆ†å‰²åŸºç¡€æ¨¡å‹çš„åé—¨æ”»å‡»æ¡†æ¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.22046v1" data-paper-url="./papers/251222046v1-backdoor-attacks-on-prompt-driven-video-segmentation-foundation-mode.html" onclick="toggleFavorite(this, '2512.22046v1', 'Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251221964v1-perceive-and-calibrate-analyzing-and-enhancing-robustness-of-medical.html">Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models</a></td>
  <td>æå‡ºInherent-enhanced Multi-modal Calibration (IMC)æ¡†æ¶ï¼Œæå‡åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21964v1" data-paper-url="./papers/251221964v1-perceive-and-calibrate-analyzing-and-enhancing-robustness-of-medical.html" onclick="toggleFavorite(this, '2512.21964v1', 'Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251221881v1-slim-brain-a-data-and-training-efficient-foundation-model-for-fmri-d.html">SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</a></td>
  <td>SLIM-Brainï¼šä¸€ç§æ•°æ®å’Œè®­ç»ƒé«˜æ•ˆçš„fMRIåˆ†æåŸºç¡€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21881v1" data-paper-url="./papers/251221881v1-slim-brain-a-data-and-training-efficient-foundation-model-for-fmri-d.html" onclick="toggleFavorite(this, '2512.21881v1', 'SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251221860v1-training-free-conditional-image-embedding-framework-leveraging-large.html">Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</a></td>
  <td>æå‡ºDIORï¼šä¸€ç§å…è®­ç»ƒçš„æ¡ä»¶å›¾åƒåµŒå…¥æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21860v1" data-paper-url="./papers/251221860v1-training-free-conditional-image-embedding-framework-leveraging-large.html" onclick="toggleFavorite(this, '2512.21860v1', 'Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/251221916v1-patch-as-node-human-centric-graph-representation-learning-for-multim.html">Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition</a></td>
  <td>æå‡ºPANæ¡†æ¶ï¼Œé€šè¿‡äººä½“ä¸­å¿ƒå›¾è¡¨ç¤ºå­¦ä¹ å®ç°æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ«ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21916v1" data-paper-url="./papers/251221916v1-patch-as-node-human-centric-graph-representation-learning-for-multim.html" onclick="toggleFavorite(this, '2512.21916v1', 'Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251222096v1-yume-15-a-text-controlled-interactive-world-generation-model.html">Yume-1.5: A Text-Controlled Interactive World Generation Model</a></td>
  <td>Yume-1.5ï¼šä¸€ç§æ–‡æœ¬æ§åˆ¶çš„äº¤äº’å¼ä¸–ç•Œç”Ÿæˆæ¨¡å‹ï¼Œæå‡å®æ—¶æ€§å’Œå¯æ§æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">linear attention</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.22096v1" data-paper-url="./papers/251222096v1-yume-15-a-text-controlled-interactive-world-generation-model.html" onclick="toggleFavorite(this, '2512.22096v1', 'Yume-1.5: A Text-Controlled Interactive World Generation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/251221831v1-end-to-end-3d-spatiotemporal-perception-with-multimodal-fusion-and-v.html">End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</a></td>
  <td>æå‡ºXET-V2Xï¼Œç”¨äºV2XååŒä¸­å¤šæ¨¡æ€èåˆçš„ç«¯åˆ°ç«¯3Dæ—¶ç©ºæ„ŸçŸ¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21831v1" data-paper-url="./papers/251221831v1-end-to-end-3d-spatiotemporal-perception-with-multimodal-fusion-and-v.html" onclick="toggleFavorite(this, '2512.21831v1', 'End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251222010v1-longfly-long-horizon-uav-vision-and-language-navigation-with-spatiot.html">LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</a></td>
  <td>LongFlyï¼šé’ˆå¯¹é•¿ç¨‹æ— äººæœºè§†è§‰-è¯­è¨€å¯¼èˆªï¼Œæå‡ºæ—¶ç©ºä¸Šä¸‹æ–‡èåˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">VLN</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.22010v1" data-paper-url="./papers/251222010v1-longfly-long-horizon-uav-vision-and-language-navigation-with-spatiot.html" onclick="toggleFavorite(this, '2512.22010v1', 'LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/251221883v1-reloc-vggt-visual-re-localization-with-geometry-grounded-transformer.html">Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer</a></td>
  <td>æå‡ºReloc-VGGTï¼Œåˆ©ç”¨å‡ ä½•çº¦æŸTransformerå®ç°é²æ£’é«˜æ•ˆçš„è§†è§‰é‡å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">VGGT</span> <span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21883v1" data-paper-url="./papers/251221883v1-reloc-vggt-visual-re-localization-with-geometry-grounded-transformer.html" onclick="toggleFavorite(this, '2512.21883v1', 'Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)