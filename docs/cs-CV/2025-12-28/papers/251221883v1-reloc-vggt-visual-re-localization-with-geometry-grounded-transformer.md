---
layout: default
title: "Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer"
---

# Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.21883" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.21883v1</a>
  <a href="https://arxiv.org/pdf/2512.21883.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.21883v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.21883v1', 'Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dtc111111/Reloc-VGGT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReloc-VGGTï¼Œåˆ©ç”¨å‡ ä½•çº¦æŸTransformerå®ç°é²æ£’é«˜æ•ˆçš„è§†è§‰é‡å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `è§†è§‰é‡å®šä½` `å¤šè§†è§’å‡ ä½•` `Transformerç½‘ç»œ` `æ—©æœŸèåˆ` `ç¨€ç–æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè§†è§‰å®šä½æ–¹æ³•ä¾èµ–æˆå¯¹ä½å§¿å›å½’ï¼ŒåæœŸèåˆç­–ç•¥éš¾ä»¥æœ‰æ•ˆæ•´åˆç©ºé—´ä¿¡æ¯ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­ç²¾åº¦ä¸‹é™ã€‚
2. Reloc-VGGTé€šè¿‡æ—©æœŸèåˆæœºåˆ¶è¿›è¡Œå¤šè§†è§’ç©ºé—´é›†æˆï¼Œåˆ©ç”¨VGGTéª¨å¹²ç½‘ç»œç¼–ç 3Då‡ ä½•ä¿¡æ¯ï¼Œå¹¶è®¾è®¡å§¿æ€æ ‡è®°å™¨å’ŒæŠ•å½±æ¨¡å—ã€‚
3. æå‡ºçš„ç¨€ç–æ©ç æ³¨æ„åŠ›é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œä½¿å¤§è§„æ¨¡å®æ—¶å®šä½æˆä¸ºå¯èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰é‡å®šä½æ¡†æ¶Reloc-VGGTï¼Œè¯¥æ¡†æ¶é€šè¿‡æ—©æœŸèåˆæœºåˆ¶æ‰§è¡Œå¤šè§†è§’ç©ºé—´é›†æˆï¼Œä»è€Œåœ¨ç»“æ„åŒ–å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°ç¨³å¥è¿è¡Œã€‚è¯¥æ¡†æ¶åŸºäºVGGTéª¨å¹²ç½‘ç»œï¼Œç¼–ç å¤šè§†è§’3Då‡ ä½•ä¿¡æ¯ï¼Œå¹¶å¼•å…¥å§¿æ€æ ‡è®°å™¨å’ŒæŠ•å½±æ¨¡å—ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ¥è‡ªå¤šä¸ªæ•°æ®åº“è§†è§’çš„ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–æ©ç æ³¨æ„åŠ›ç­–ç•¥ï¼Œé€šè¿‡é¿å…å…¨å±€æ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦æ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡çš„å®æ—¶æ€§èƒ½ã€‚Reloc-VGGTåœ¨çº¦800ä¸‡ä¸ªå¸¦å§¿æ€å›¾åƒå¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†å¼ºå¤§çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å„ç§å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒä¸€è‡´éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œåœ¨å®æ—¶æä¾›é«˜è´¨é‡ç›¸æœºå§¿æ€ä¼°è®¡çš„åŒæ—¶ï¼Œä¿æŒäº†å¯¹æœªè§ç¯å¢ƒçš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰é‡å®šä½æ—¨åœ¨ç¡®å®šç›¸æœºåœ¨å·²çŸ¥ç¯å¢ƒä¸­çš„ç²¾ç¡®ä½ç½®å’Œå§¿æ€ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤ä¸¤å›¾åƒä¹‹é—´çš„ä½å§¿å›å½’ï¼Œç„¶åé€šè¿‡åæœŸèåˆç­–ç•¥è·å¾—ç»å¯¹ä½å§¿ä¼°è®¡ã€‚ç„¶è€Œï¼Œè¿™ç§åæœŸèåˆæ–¹å¼æ— æ³•å……åˆ†åˆ©ç”¨å¤šè§†è§’ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œå¯¼è‡´åœ¨å¤æ‚æˆ–é®æŒ¡ç¯å¢ƒä¸­å®šä½ç²¾åº¦ä¸‹é™ï¼Œä¸”è®¡ç®—æ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReloc-VGGTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ—©æœŸèåˆæœºåˆ¶ï¼Œå°†å¤šä¸ªæ•°æ®åº“è§†è§’çš„å‡ ä½•ä¿¡æ¯é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾è¡¨ç¤ºä¸­ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç©ºé—´å…³ç³»ã€‚é€šè¿‡Transformeræ¶æ„å­¦ä¹ ä¸åŒè§†è§’ä¹‹é—´çš„å…³è”æ€§ï¼Œå¹¶åˆ©ç”¨å‡ ä½•çº¦æŸæ¥æé«˜å®šä½ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReloc-VGGTæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) VGGTéª¨å¹²ç½‘ç»œï¼šç”¨äºæå–å¤šè§†è§’å›¾åƒçš„æ·±åº¦ç‰¹å¾ï¼Œå¹¶ç¼–ç 3Då‡ ä½•ä¿¡æ¯ã€‚2) å§¿æ€æ ‡è®°å™¨ï¼šå°†æ¯ä¸ªè§†è§’çš„ä½å§¿ä¿¡æ¯è½¬æ¢ä¸ºå¯å­¦ä¹ çš„æ ‡è®°ã€‚3) æŠ•å½±æ¨¡å—ï¼šå°†ç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€çš„ç©ºé—´åæ ‡ç³»ä¸­ã€‚4) Transformerç½‘ç»œï¼šå­¦ä¹ ä¸åŒè§†è§’ä¹‹é—´çš„å…³è”æ€§ï¼Œå¹¶è¿›è¡Œå¤šè§†è§’ç‰¹å¾èåˆã€‚5) ä½å§¿å›å½’æ¨¡å—ï¼šæ ¹æ®èåˆåçš„ç‰¹å¾å›å½’ç›¸æœºçš„ä½å§¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šReloc-VGGTçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŸºäºVGGTéª¨å¹²ç½‘ç»œçš„æ—©æœŸèåˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šè§†è§’å‡ ä½•ä¿¡æ¯ã€‚2) å¼•å…¥äº†å§¿æ€æ ‡è®°å™¨å’ŒæŠ•å½±æ¨¡å—ï¼Œå°†ä½å§¿ä¿¡æ¯èå…¥åˆ°ç‰¹å¾è¡¨ç¤ºä¸­ã€‚3) æå‡ºäº†ç¨€ç–æ©ç æ³¨æ„åŠ›æœºåˆ¶ï¼Œé™ä½äº†Transformerç½‘ç»œçš„è®¡ç®—å¤æ‚åº¦ï¼Œå®ç°äº†å®æ—¶æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼š1) ç¨€ç–æ©ç æ³¨æ„åŠ›ï¼šé€šè¿‡åªå…³æ³¨ä¸å½“å‰è§†è§’ç›¸å…³çš„å…³é”®è§†è§’ï¼Œé¿å…äº†å…¨å±€æ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦ã€‚2) æŸå¤±å‡½æ•°ï¼šé‡‡ç”¨ä½å§¿å›å½’æŸå¤±å’Œå‡ ä½•ä¸€è‡´æ€§æŸå¤±ï¼Œå…±åŒä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚3) æ•°æ®å¢å¼ºï¼šé€šè¿‡éšæœºæ—‹è½¬ã€å¹³ç§»å’Œç¼©æ”¾ç­‰æ–¹å¼ï¼Œå¢åŠ æ•°æ®çš„å¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21883v1/teaser2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21883v1/framework.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21883v1/maskattention6.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

Reloc-VGGTåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜å…¶åœ¨å®šä½ç²¾åº¦å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸæ•°æ®é›†ä¸Šï¼ŒReloc-VGGTçš„å®šä½ç²¾åº¦æé«˜äº†15%ï¼ŒåŒæ—¶è®¡ç®—é€Ÿåº¦æå‡äº†2å€ã€‚è¯¥æ–¹æ³•è¿˜å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿æŒè¾ƒé«˜çš„å®šä½ç²¾åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Reloc-VGGTå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®ç°é«˜ç²¾åº¦å®šä½ï¼Œæé«˜è½¦è¾†çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»å¯¼èˆªã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼Œå¯ä»¥å®ç°è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯çš„ç²¾ç¡®å¯¹é½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.

