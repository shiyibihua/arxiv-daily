---
layout: default
title: "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"
---

# See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.22120" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.22120v1</a>
  <a href="https://arxiv.org/pdf/2512.22120.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.22120v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.22120v1', 'See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå‘æ„ŸçŸ¥å¡‘å½¢æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€æ¨ç†` `åŒå‘æ„ŸçŸ¥å¡‘å½¢` `KLä¸€è‡´æ€§` `ç»†ç²’åº¦è§†è§‰è¯æ®` `è·¨é¢†åŸŸæ³›åŒ–` `æ™ºèƒ½é—®ç­”` `å›¾åƒç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨ç»†ç²’åº¦çš„è§†è§‰è¯æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œæ¨ç†æ—¶é—´æˆæœ¬é«˜ã€‚
2. æœ¬æ–‡æå‡ºåŒå‘æ„ŸçŸ¥å¡‘å½¢ï¼ˆBiPSï¼‰ï¼Œé€šè¿‡å¼•å…¥KLä¸€è‡´æ€§å’ŒKLåˆ†ç¦»çº¦æŸï¼Œå¢å¼ºæ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ï¼Œæ”¹å–„æ¨ç†æ•ˆæœã€‚
3. åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBiPSå¹³å‡æå‡äº†Qwen2.5-VL-7Bæ¨¡å‹8.2%çš„æ€§èƒ½ï¼Œä¸”åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šå¸¸ä¾èµ–ä¸­é—´è§†è§‰çº¿ç´¢ï¼Œä½†ç°æœ‰æ–¹æ³•å¿½è§†äº†ç»†ç²’åº¦çš„è§†è§‰è¯æ®ï¼Œä¸”åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œæ¨ç†æ—¶é—´æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºäº†åŒå‘æ„ŸçŸ¥å¡‘å½¢ï¼ˆBiPSï¼‰ï¼Œé€šè¿‡å°†é—®é¢˜æ¡ä»¶ä¸‹çš„é®è”½è§†å›¾è½¬åŒ–ä¸ºåŒå‘çš„å…³æ³¨ä¿¡å·ï¼Œæ¥å¡‘é€ è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ„ŸçŸ¥ã€‚BiPSé¦–å…ˆåœ¨åŸå§‹å›¾åƒä¸ä¿ç•™è¯æ®çš„è§†å›¾ä¹‹é—´æ–½åŠ KLä¸€è‡´æ€§çº¦æŸï¼Œä»¥é¼“åŠ±å¯¹æ”¯æŒåƒç´ çš„ç²—ç•¥ä½†å®Œæ•´è¦†ç›–ã€‚ç„¶åï¼Œå®ƒåœ¨åŸå§‹å›¾åƒä¸è¯æ®æ¶ˆèè§†å›¾ä¹‹é—´æ–½åŠ KLåˆ†ç¦»çº¦æŸï¼Œä»¥é˜²æ­¢ä»…ä¾èµ–æ–‡æœ¬çš„å¿«æ·å›ç­”ï¼Œå¹¶å¼ºåŒ–å¯¹ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„ä¾èµ–ã€‚å®éªŒè¡¨æ˜ï¼ŒBiPSåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†Qwen2.5-VL-7Bæ¨¡å‹8.2%çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§æ•°æ®é›†å’Œå›¾åƒç±»å‹ä¸Šå±•ç°å‡ºå¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ç»†ç²’åº¦è§†è§‰è¯æ®çš„å¿½è§†ï¼Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œé«˜æ¨ç†æ—¶é—´æˆæœ¬çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„åŒå‘æ„ŸçŸ¥å¡‘å½¢ï¼ˆBiPSï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹é—®é¢˜æ¡ä»¶ä¸‹çš„é®è”½è§†å›¾è¿›è¡ŒåŒå‘ä¿¡å·å¡‘å½¢ï¼Œé¼“åŠ±æ¨¡å‹åœ¨è®­ç»ƒä¸­æ›´å¥½åœ°ä¾èµ–è§†è§‰ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBiPSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ–½åŠ KLä¸€è‡´æ€§çº¦æŸä»¥ä¿æŒé—®é¢˜ç›¸å…³åŒºåŸŸçš„å®Œæ•´è¦†ç›–ï¼Œå…¶æ¬¡æ–½åŠ KLåˆ†ç¦»çº¦æŸä»¥æ¶ˆé™¤å…³é”®åƒç´ ï¼Œé˜²æ­¢æ–‡æœ¬å¿«æ·å›ç­”ã€‚

**å…³é”®åˆ›æ–°**ï¼šBiPSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†åŒå‘çš„å…³æ³¨ä¿¡å·å¡‘å½¢æœºåˆ¶ï¼Œé€šè¿‡KLä¸€è‡´æ€§å’ŒKLåˆ†ç¦»çº¦æŸï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ç¨‹åº¦ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç»†ç²’åº¦è§†è§‰è¯æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†KLä¸€è‡´æ€§å’ŒKLåˆ†ç¦»ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ—¢èƒ½è¦†ç›–æ”¯æŒåƒç´ ï¼Œåˆèƒ½é¿å…ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.22120v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.22120v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.22120v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBiPSåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†Qwen2.5-VL-7Bæ¨¡å‹8.2%çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æœªè§æ•°æ®é›†å’Œä¸åŒå›¾åƒç±»å‹ä¸Šå±•ç°å‡ºå¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒç†è§£å’Œå¤šæ¨¡æ€äº¤äº’ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„ä¾èµ–ï¼ŒBiPSå¯ä»¥åœ¨æ›´å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.

