---
layout: default
title: MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution
---

# MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11768v1</a>
  <a href="https://arxiv.org/pdf/2506.11768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11768v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11768v1', 'MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linfeng He, Meiqin Liu, Qi Tang, Chao Yao, Yao Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMambaVSRä»¥è§£å†³è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­çš„éå±€éƒ¨ä¾èµ–å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è§†é¢‘è¶…åˆ†è¾¨ç‡` `å†…å®¹æ„ŸçŸ¥` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `å…±äº«æŒ‡å—æ„å»º` `åŠ¨æ€æ—¶ç©ºäº¤äº’` `é«˜é¢‘ç»†èŠ‚æ¢å¤` `ç¨€ç–æ³¨æ„åŠ›` `éå±€éƒ¨ä¾èµ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘è¶…åˆ†è¾¨ç‡æ–¹æ³•åœ¨å¤„ç†é”™ä½å¸§çš„éå±€éƒ¨ä¾èµ–æ€§æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶åœ¨å¤§è¿åŠ¨å’Œé•¿åºåˆ—ä¸­è¡¨ç°ä¸ä½³ã€‚
2. MambaVSRå¼•å…¥äº†å†…å®¹æ„ŸçŸ¥æ‰«ææœºåˆ¶ï¼Œé€šè¿‡å…±äº«æŒ‡å—æ„å»ºå’Œå†…å®¹æ„ŸçŸ¥åºåˆ—åŒ–å®ç°åŠ¨æ€æ—¶ç©ºäº¤äº’ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMambaVSRåœ¨REDSæ•°æ®é›†ä¸Šæ¯”ç°æœ‰å˜æ¢å™¨æ–¹æ³•æé«˜äº†0.58 dB PSNRï¼Œä¸”å‚æ•°é‡å‡å°‘äº†55%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVSRï¼‰åœ¨æœ‰æ•ˆå»ºæ¨¡é”™ä½å¸§ä¹‹é—´çš„éå±€éƒ¨ä¾èµ–æ€§ä»¥åŠä¿æŒè®¡ç®—æ•ˆç‡æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å…‰æµç­–ç•¥æˆ–å˜æ¢å™¨æ¶æ„ï¼Œä½†åœ¨å¤§è¿åŠ¨ä½ç§»å’Œé•¿è§†é¢‘åºåˆ—ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MambaVSRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†åˆ›æ–°çš„å†…å®¹æ„ŸçŸ¥æ‰«ææœºåˆ¶çº³å…¥VSRçš„çŠ¶æ€ç©ºé—´æ¨¡å‹æ¡†æ¶ã€‚MambaVSRé€šè¿‡å…±äº«æŒ‡å—æ„å»ºï¼ˆSCCï¼‰å’Œå†…å®¹æ„ŸçŸ¥åºåˆ—åŒ–ï¼ˆCASï¼‰æ¨¡å—ï¼Œå®ç°äº†åŠ¨æ€æ—¶ç©ºäº¤äº’ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMambaVSRåœ¨REDSæ•°æ®é›†ä¸Šæ¯”åŸºäºå˜æ¢å™¨çš„æ–¹æ³•æé«˜äº†0.58 dB PSNRï¼ŒåŒæ—¶å‚æ•°é‡å‡å°‘äº†55%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­çš„éå±€éƒ¨ä¾èµ–å»ºæ¨¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é”™ä½å¸§æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶åœ¨å¤§è¿åŠ¨å’Œé•¿è§†é¢‘åºåˆ—ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMambaVSRé€šè¿‡å¼•å…¥å†…å®¹æ„ŸçŸ¥æ‰«ææœºåˆ¶ï¼Œç»“åˆå…±äº«æŒ‡å—æ„å»ºï¼ˆSCCï¼‰å’Œå†…å®¹æ„ŸçŸ¥åºåˆ—åŒ–ï¼ˆCASï¼‰ï¼Œå®ç°åŠ¨æ€æ—¶ç©ºäº¤äº’ï¼Œä»è€Œæœ‰æ•ˆå¯¹é½å’Œèšåˆå¤šå¸§ä¸­çš„ç›¸ä¼¼å†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMambaVSRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬SCCæ¨¡å—å’ŒCASæ¨¡å—ã€‚SCCæ¨¡å—é€šè¿‡ç¨€ç–æ³¨æ„åŠ›æ„å»ºå¸§å†…è¯­ä¹‰è¿æ¥å›¾ï¼Œå¹¶åˆ©ç”¨è°±èšç±»ç”Ÿæˆè‡ªé€‚åº”ç©ºé—´æ‰«æåºåˆ—ï¼›CASæ¨¡å—åˆ™é€šè¿‡äº¤é”™æ—¶é—´ç‰¹å¾æ¥å¯¹é½å’Œèšåˆå¤šå¸§å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMambaVSRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å†…å®¹æ„ŸçŸ¥æ‰«ææœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€å¤„ç†æ—¶ç©ºäº¤äº’ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„åˆšæ€§1Dåºåˆ—å¤„ç†æ–¹å¼ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSCCæ¨¡å—é‡‡ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼ŒCASæ¨¡å—åˆ™é€šè¿‡å­¦ä¹ çš„ç©ºé—´é¡ºåºäº¤é”™æ—¶é—´ç‰¹å¾æ¥å®ç°éå±€éƒ¨å†…å®¹çš„èšåˆï¼Œç¡®ä¿äº†å…¨å±€ä¾èµ–ä¸å±€éƒ¨ç»†èŠ‚çš„æœ‰æ•ˆç»“åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MambaVSRåœ¨REDSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶PSNRæ¯”åŸºäºå˜æ¢å™¨çš„æ–¹æ³•æé«˜äº†0.58 dBï¼ŒåŒæ—¶å‚æ•°é‡å‡å°‘äº†55%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¯æ˜äº†å…¶åœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MambaVSRåœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜è´¨é‡è§†é¢‘é‡å»ºçš„åœºæ™¯ï¼Œå¦‚è§†é¢‘ç›‘æ§ã€å½±è§†åˆ¶ä½œå’Œè™šæ‹Ÿç°å®ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¨¡å‹è®¾è®¡å’Œä¼˜è¶Šçš„æ€§èƒ½å°†æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video super-resolution (VSR) faces critical challenges in effectively modeling non-local dependencies across misaligned frames while preserving computational efficiency. Existing VSR methods typically rely on optical flow strategies or transformer architectures, which struggle with large motion displacements and long video sequences. To address this, we propose MambaVSR, the first state-space model framework for VSR that incorporates an innovative content-aware scanning mechanism. Unlike rigid 1D sequential processing in conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal interactions through the Shared Compass Construction (SCC) and the Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs intra-frame semantic connectivity graphs via efficient sparse attention and generates adaptive spatial scanning sequences through spectral clustering. Building upon SCC, the CAS module effectively aligns and aggregates non-local similar content across multiple frames by interleaving temporal features along the learned spatial order. To bridge global dependencies with local details, the Global-Local State Space Block (GLSSB) synergistically integrates window self-attention operations with SSM-based feature propagation, enabling high-frequency detail recovery under global dependency guidance. Extensive experiments validate MambaVSR's superiority, outperforming the Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer parameters.

