---
layout: default
title: VGR: Visual Grounded Reasoning
---

# VGR: Visual Grounded Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11991" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11991v2</a>
  <a href="https://arxiv.org/pdf/2506.11991.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11991v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11991v2', 'VGR: Visual Grounded Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, Jun Xiao

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-06-16)

**å¤‡æ³¨**: 9 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGRä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯­è¨€åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰æ„ŸçŸ¥` `è¯­è¨€æ¨¡å‹` `å›¾åƒç†è§£` `æ·±åº¦å­¦ä¹ ` `æ¨ç†æœºåˆ¶` `åŒºåŸŸæ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–è¯­è¨€æ¨ç†ï¼Œå¯¼è‡´è¯­è¨€åè§ï¼Œä¸”éš¾ä»¥å¤„ç†å¤æ‚çš„è§†è§‰æ¨ç†ä»»åŠ¡ã€‚
2. VGRé€šè¿‡æ£€æµ‹å›¾åƒä¸­çš„ç›¸å…³åŒºåŸŸå¹¶ç»“åˆè§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œæå‡äº†æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚
3. åœ¨LLaVA-NeXT-7BåŸºçº¿æ¨¡å‹ä¸Šï¼ŒVGRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤šæ¨¡æ€é“¾å¼æ¨ç†é¢†åŸŸï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºçº¯è¯­è¨€ç©ºé—´çš„æ¨ç†ï¼Œå¯¼è‡´è¯­è¨€åè§ï¼Œå¹¶ä¸”å±€é™äºæ•°å­¦æˆ–ç§‘å­¦é¢†åŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹VGRï¼Œå…·å¤‡å¢å¼ºçš„ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚VGRé¦–å…ˆæ£€æµ‹ç›¸å…³åŒºåŸŸä»¥å¸®åŠ©è§£å†³é—®é¢˜ï¼Œç„¶ååŸºäºå›æ”¾çš„å›¾åƒåŒºåŸŸæä¾›ç²¾ç¡®ç­”æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒVGRåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•åœ¨è¯­è¨€ç©ºé—´æ¨ç†ä¸­å­˜åœ¨çš„åè§å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡æ—¶çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVGRçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ£€æµ‹å›¾åƒä¸­çš„ç›¸å…³åŒºåŸŸï¼Œç»“åˆè§†è§‰ä¿¡æ¯ä¸è¯­è¨€æ¨ç†ï¼Œæä¾›æ›´ä¸ºå‡†ç¡®çš„ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£å›¾åƒç»†èŠ‚ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯åŒºåŸŸæ£€æµ‹é˜¶æ®µï¼Œæ¨¡å‹é€‰æ‹©ä¸é—®é¢˜ç›¸å…³çš„è¾¹ç•Œæ¡†ï¼›å…¶æ¬¡æ˜¯å›æ”¾é˜¶æ®µï¼Œå°†é€‰å®šçš„å›¾åƒåŒºåŸŸæ•´åˆè¿›æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¢å¼ºå¤šæ¨¡æ€ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGRçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†è§†è§‰åŒºåŸŸçš„å›æ”¾æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸ä»…ä¾èµ–è¯­è¨€ä¿¡æ¯ï¼Œè¿˜èƒ½æœ‰æ•ˆåˆ©ç”¨å›¾åƒä¿¡æ¯ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¤šæ¨¡æ€èåˆçš„æ·±åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒVGRä½¿ç”¨äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„SFTæ•°æ®é›†ï¼ŒåŒ…å«è§†è§‰åŸºç¡€å’Œè¯­è¨€æ¨ç†æ··åˆçš„æ•°æ®ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å›¾åƒä»¤ç‰Œæ•°é‡ä¸Šä»…ä½¿ç”¨åŸºçº¿çš„30%ï¼Œä½†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VGRåœ¨LLaVA-NeXT-7BåŸºçº¿æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨MMStarã€AI2Då’ŒChartQAç­‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æé«˜äº†4.1ã€7.1å’Œ12.9çš„åˆ†æ•°ï¼Œä¸”ä»…ä½¿ç”¨äº†30%çš„å›¾åƒä»¤ç‰Œæ•°é‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VGRçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆã€ä»¥åŠå¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒVGRèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´ä¸ºå‡†ç¡®å’Œå…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨è§†è§‰ç†è§£æ–¹é¢çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.

