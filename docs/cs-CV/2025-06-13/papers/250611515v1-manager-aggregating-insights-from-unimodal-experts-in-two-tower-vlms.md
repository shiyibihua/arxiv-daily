---
layout: default
title: Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs
---

# Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11515" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11515v1</a>
  <a href="https://arxiv.org/pdf/2506.11515.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11515v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11515v1', 'Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Xu, Libo Qin, Wanxiang Che, Min-Yen Kan

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT). June 2025. DOI: https://doi.org/10.1109/TCSVT.2025.3578266

**DOI**: [10.1109/TCSVT.2025.3578266](https://doi.org/10.1109/TCSVT.2025.3578266)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LooperXX/ManagerTower)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºManageræ’ä»¶ä»¥è§£å†³ä¸¤å¡”VLMså’ŒMLLMsä¸­çš„å•æ¨¡æ€ä¸“å®¶èšåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€èåˆ` `å•æ¨¡æ€ä¸“å®¶` `è·¨æ¨¡æ€å¯¹é½` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸¤å¡”VLMsæ–¹æ³•åœ¨å•æ¨¡æ€è¡¨ç¤ºçš„åˆ©ç”¨å’Œè¯­ä¹‰çŸ¥è¯†çš„çµæ´»æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºManageræ’ä»¶ï¼Œé€šè¿‡åœ¨æ¯ä¸ªè·¨æ¨¡æ€å±‚å¼•å…¥ç®¡ç†å™¨ï¼Œé€‚åº”æ€§åœ°èšåˆä¸åŒå±‚æ¬¡çš„å•æ¨¡æ€ä¸“å®¶è§è§£ï¼Œæå‡è§†è§‰-è¯­è¨€å¯¹é½æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒManagerToweråœ¨å››ä¸ªä¸‹æ¸¸VLä»»åŠ¡ä¸­è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨20ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†LLaVA-OVçš„é›¶-shotæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸¤å¡”è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§ä¸‹æ¸¸è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç°æœ‰çš„BridgeToweræ–¹æ³•åœ¨å•æ¨¡æ€è¡¨ç¤ºçš„é€å±‚åˆ©ç”¨ã€ä¸åŒå±‚æ¬¡è¯­ä¹‰çŸ¥è¯†çš„çµæ´»åˆ©ç”¨ä»¥åŠä»…é™äºä½åˆ†è¾¨ç‡æ•°æ®é›†çš„è¯„ä¼°æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Managerï¼Œä¸€ä¸ªè½»é‡ã€é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ’ä»¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°èšåˆæ¥è‡ªä¸åŒå±‚æ¬¡çš„é¢„è®­ç»ƒå•æ¨¡æ€ä¸“å®¶çš„è§è§£ï¼Œä»è€Œä¿ƒè¿›æ›´å…¨é¢çš„è§†è§‰-è¯­è¨€å¯¹é½ä¸èåˆã€‚é€šè¿‡å¼•å…¥ManagerTowerï¼Œæœ¬æ–‡åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†ä¹‹å‰çš„å¼ºåŸºçº¿ï¼Œå¹¶åœ¨æœ€æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¶æ„ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„é›¶-shotæ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä¸¤å¡”VLMsåœ¨å•æ¨¡æ€è¡¨ç¤ºåˆ©ç”¨å’Œè¯­ä¹‰çŸ¥è¯†çµæ´»æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯BridgeToweræ–¹æ³•çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºManageræ’ä»¶ï¼Œé€šè¿‡åœ¨è·¨æ¨¡æ€å±‚å¼•å…¥ç®¡ç†å™¨ï¼Œèšåˆä¸åŒå±‚æ¬¡çš„å•æ¨¡æ€ä¸“å®¶è§è§£ï¼Œä»¥å®ç°æ›´å…¨é¢çš„è§†è§‰-è¯­è¨€å¯¹é½ä¸èåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ManagerTowerï¼Œä½œä¸ºä¸€ç§æ–°å‹çš„VLMï¼Œåœ¨æ¯ä¸ªè·¨æ¨¡æ€å±‚ä¸­å¼•å…¥ç®¡ç†å™¨æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´ä¿¡æ¯æµã€‚

**å…³é”®åˆ›æ–°**ï¼šManageræ’ä»¶çš„å¼•å…¥æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒé€šè¿‡å¤šå±‚æ¬¡çš„å•æ¨¡æ€ä¸“å®¶èšåˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒManageræ’ä»¶çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†å¤šæ¨¡æ€å¯¹é½çš„éœ€æ±‚ï¼Œç½‘ç»œç»“æ„åˆ™é‡‡ç”¨äº†çµæ´»çš„è·¨æ¨¡æ€å±‚ç®¡ç†æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒManagerToweråœ¨å››ä¸ªä¸‹æ¸¸VLä»»åŠ¡ä¸­è¶…è¶Šäº†ä¹‹å‰çš„å¼ºåŸºçº¿ï¼Œä¸”åœ¨20ä¸ªæ•°æ®é›†ä¸Šï¼ŒLLaVA-OV-Managerçš„é›¶-shotæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨ä¸åŒç±»åˆ«çš„èƒ½åŠ›ã€å›¾åƒå’Œåˆ†è¾¨ç‡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€è·¨æ¨¡æ€æ£€ç´¢ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿæä¾›æ›´å¼ºçš„æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¤æ‚çš„è§†è§‰-è¯­è¨€ä»»åŠ¡çš„å‘å±•ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, it \textit{(i)} suffers from ineffective layer-by-layer utilization of unimodal representations, \textit{(ii)} restricts the flexible exploitation of different levels of unimodal semantic knowledge, and \textit{(iii)} is limited to the evaluation on traditional low-resolution datasets only with the Two-Tower VLM architecture. In this work, we propose Manager, a lightweight, efficient and effective plugin that adaptively aggregates insights from different levels of pre-trained unimodal experts to facilitate more comprehensive VL alignment and fusion. First, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel VLM that introduces the manager in each cross-modal layer. Whether with or without VL pre-training, ManagerTower outperforms previous strong baselines and achieves superior performance on 4 downstream VL tasks. Moreover, we extend our exploration to the latest Multimodal Large Language Model (MLLM) architecture. We demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot performance of LLaVA-OV across different categories of capabilities, images, and resolutions on 20 downstream datasets, whether the multi-grid algorithm is enabled or not. In-depth analysis reveals that both our manager and the multi-grid algorithm can be viewed as a plugin that improves the visual representation by capturing more diverse visual details from two orthogonal perspectives (depth and width). Their synergy can mitigate the semantic ambiguity caused by the multi-grid algorithm and further improve performance. Code and models are available at https://github.com/LooperXX/ManagerTower.

