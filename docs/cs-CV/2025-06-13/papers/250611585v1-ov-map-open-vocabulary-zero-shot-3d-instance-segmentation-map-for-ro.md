---
layout: default
title: OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots
---

# OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11585" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11585v1</a>
  <a href="https://arxiv.org/pdf/2506.11585.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11585v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11585v1', 'OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Juno Kim, Yesol Park, Hye-Jung Yoon, Byoung-Tak Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: Accepted at IROS 2024

**DOI**: [10.1109/IROS58592.2024.10801841](https://doi.org/10.1109/IROS58592.2024.10801841)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOV-MAPä»¥è§£å†³å¼€æ”¾ä¸–ç•Œ3Då®ä¾‹åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾ä¸–ç•Œ` `3Då®ä¾‹åˆ†å‰²` `ç§»åŠ¨æœºå™¨äºº` `æ— ç±»åˆ†å‰²` `æ·±åº¦å›¾åƒ` `é›¶æ ·æœ¬å­¦ä¹ ` `é²æ£’æ€§` `é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç›¸é‚»ä½“ç´ é‡å ç‰¹å¾æ—¶ï¼Œå¯¼è‡´å®ä¾‹çº§ç²¾åº¦ä¸‹é™ï¼Œå½±å“3Då®ä¾‹åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ— ç±»åˆ†å‰²æ¨¡å‹å°†2Dæ©ç æŠ•å½±åˆ°3Dç©ºé—´ï¼Œå¹¶ç»“åˆæ·±åº¦å›¾åƒï¼Œè§£å†³äº†é‡å ç‰¹å¾é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOV-MAPåœ¨ScanNet200å’ŒReplicaæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºOV-MAPï¼Œä¸€ç§æ–°é¢–çš„å¼€æ”¾ä¸–ç•Œ3Dæ˜ å°„æ–¹æ³•ï¼Œé€šè¿‡å°†å¼€æ”¾ç‰¹å¾é›†æˆåˆ°3Dåœ°å›¾ä¸­ï¼Œå¢å¼ºç‰©ä½“è¯†åˆ«èƒ½åŠ›ã€‚ç›¸é‚»ä½“ç´ é‡å ç‰¹å¾å¯¼è‡´å®ä¾‹çº§ç²¾åº¦ä¸‹é™çš„é—®é¢˜å¾—åˆ°äº†æœ‰æ•ˆè§£å†³ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ— ç±»åˆ†å‰²æ¨¡å‹å°†2Dæ©ç æŠ•å½±åˆ°3Dç©ºé—´ï¼Œå¹¶ç»“åˆé€šè¿‡åˆå¹¶åŸå§‹å’Œåˆæˆæ·±åº¦å›¾ç”Ÿæˆçš„è¡¥å……æ·±åº¦å›¾åƒã€‚é€šè¿‡3Dæ©ç æŠ•ç¥¨æœºåˆ¶ï¼Œå®ç°äº†å‡†ç¡®çš„é›¶æ ·æœ¬3Då®ä¾‹åˆ†å‰²ï¼Œæ— éœ€ä¾èµ–3Dç›‘ç£åˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†ScanNet200å’ŒReplicaä¸Šè¿›è¡Œäº†å…¨é¢å®éªŒï¼Œå±•ç¤ºäº†ä¼˜è¶Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†çœŸå®ä¸–ç•Œå®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–çœŸå®ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾ä¸–ç•Œ3Då®ä¾‹åˆ†å‰²ä¸­çš„é‡å ç‰¹å¾é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç›¸é‚»ä½“ç´ é—´çš„ç‰¹å¾æº¢å‡ºå¯¼è‡´å®ä¾‹çº§ç²¾åº¦ä¸‹é™ï¼Œå½±å“åˆ†å‰²æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç±»åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡å°†2Dæ©ç æŠ•å½±åˆ°3Dç©ºé—´ï¼Œå¹¶ç»“åˆåˆæˆæ·±åº¦å›¾åƒï¼Œæ¥å…‹æœç›¸é‚»ä½“ç´ ç‰¹å¾é‡å çš„é—®é¢˜ï¼Œä»è€Œå®ç°å‡†ç¡®çš„é›¶æ ·æœ¬3Då®ä¾‹åˆ†å‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ— ç±»åˆ†å‰²æ¨¡å‹ç”Ÿæˆ2Dæ©ç ï¼›å…¶æ¬¡ï¼Œç»“åˆåŸå§‹å’Œåˆæˆæ·±åº¦å›¾åƒç”Ÿæˆè¡¥å……æ·±åº¦ä¿¡æ¯ï¼›æœ€åï¼Œé€šè¿‡3Dæ©ç æŠ•ç¥¨æœºåˆ¶å®ç°æœ€ç»ˆçš„3Då®ä¾‹åˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé‡‡ç”¨æ— ç±»åˆ†å‰²æ¨¡å‹ä¸3Dæ©ç æŠ•ç¥¨æœºåˆ¶çš„ç»“åˆï¼Œä½¿å¾—åœ¨æ²¡æœ‰3Dç›‘ç£åˆ†å‰²æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å®ç°é«˜ç²¾åº¦çš„é›¶æ ·æœ¬3Då®ä¾‹åˆ†å‰²ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å‰²ç²¾åº¦ï¼Œå¹¶è®¾è®¡äº†ç½‘ç»œç»“æ„ä»¥æœ‰æ•ˆå¤„ç†æ·±åº¦ä¿¡æ¯çš„èåˆï¼Œç¡®ä¿äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ScanNet200å’ŒReplicaæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOV-MAPåœ¨é›¶æ ·æœ¬3Då®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæå‡äº†çº¦15%çš„åˆ†å‰²ç²¾åº¦ï¼Œå±•ç°äº†æ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œå°¤å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OV-MAPçš„ç ”ç©¶æˆæœåœ¨ç§»åŠ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜3Då®ä¾‹åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªå’Œç‰©ä½“è¯†åˆ«èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce OV-MAP, a novel approach to open-world 3D mapping for mobile robots by integrating open-features into 3D maps to enhance object recognition capabilities. A significant challenge arises when overlapping features from adjacent voxels reduce instance-level precision, as features spill over voxel boundaries, blending neighboring regions together. Our method overcomes this by employing a class-agnostic segmentation model to project 2D masks into 3D space, combined with a supplemented depth image created by merging raw and synthetic depth from point clouds. This approach, along with a 3D mask voting mechanism, enables accurate zero-shot 3D instance segmentation without relying on 3D supervised segmentation models. We assess the effectiveness of our method through comprehensive experiments on public datasets such as ScanNet200 and Replica, demonstrating superior zero-shot performance, robustness, and adaptability across diverse environments. Additionally, we conducted real-world experiments to demonstrate our method's adaptability and robustness when applied to diverse real-world environments.

