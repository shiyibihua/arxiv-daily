---
layout: default
title: DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning
---

# DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14827" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.14827v1</a>
  <a href="https://arxiv.org/pdf/2506.14827.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14827v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14827v1', 'DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifeng Gao, Yifan Ding, Hongyu Su, Juncheng Li, Yunhan Zhao, Lin Luo, Zixing Chen, Li Wang, Xin Wang, Yixu Wang, Xingjun Ma, Yu-Gang Jiang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDAVID-XR1ä»¥è§£å†³AIç”Ÿæˆè§†é¢‘æ£€æµ‹çš„å¯è§£é‡Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `AIç”Ÿæˆè§†é¢‘` `å¯è§£é‡Šæ€§` `è§†é¢‘è¯­è¨€æ¨¡å‹` `ç¼ºé™·æ³¨é‡Š` `æ—¶ç©ºå®šä½` `è‡ªç„¶è¯­è¨€æ¨ç†` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å°†AIç”Ÿæˆè§†é¢‘æ£€æµ‹è§†ä¸ºäºŒåˆ†ç±»ä»»åŠ¡ï¼Œç¼ºä¹å¯¹æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æä¾›æœ‰æ•ˆçš„è¯æ®æ”¯æŒã€‚
2. æœ¬æ–‡æå‡ºDAVID-XR1ï¼Œé€šè¿‡ç»“åˆè¯¦ç»†çš„ç¼ºé™·æ³¨é‡Šå’Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œæä¾›å¯è§£é‡Šçš„è§†è§‰æ¨ç†é“¾ï¼Œå¢å¼ºæ£€æµ‹çš„é€æ˜æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨å¤šç§ç”Ÿæˆå™¨å’Œç”Ÿæˆæ¨¡å¼ä¸‹è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å¯è§£é‡Šæ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€AIç”Ÿæˆè§†é¢‘åœ¨åª’ä½“å¹³å°ä¸Šçš„æ™®åŠï¼Œå¯é åŒºåˆ†åˆæˆå†…å®¹ä¸çœŸå®è§†é¢‘çš„èƒ½åŠ›å˜å¾—æ„ˆå‘ç´§è¿«ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å°†æ­¤æŒ‘æˆ˜è§†ä¸ºäºŒåˆ†ç±»ä»»åŠ¡ï¼Œç¼ºä¹å¯¹æ¨¡å‹ä¸ºä½•å°†è§†é¢‘è¯†åˆ«ä¸ºAIç”Ÿæˆçš„æ·±å…¥ç†è§£ã€‚ä¸ºå¡«è¡¥è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºDAVID-Xï¼Œé¦–ä¸ªå°†AIç”Ÿæˆè§†é¢‘ä¸è¯¦ç»†ç¼ºé™·çº§ã€æ—¶ç©ºæ³¨é‡ŠåŠä¹¦é¢æ¨ç†ç›¸ç»“åˆçš„æ•°æ®é›†ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºDAVID-XR1ï¼Œä¸€ä¸ªæ—¨åœ¨æä¾›å¯è§£é‡Šè§†è§‰æ¨ç†é“¾çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ç¼ºé™·åˆ†ç±»ã€æ—¶ç©ºå®šä½å’Œè‡ªç„¶è¯­è¨€è§£é‡Šã€‚è¿™ä¸€æ–¹æ³•å°†AIç”Ÿæˆè§†é¢‘æ£€æµ‹ä»ä¸é€æ˜çš„é»‘ç®±å†³ç­–è½¬å˜ä¸ºé€æ˜ä¸”å¯éªŒè¯çš„è¯Šæ–­è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„é€šç”¨éª¨å¹²ç½‘ç»œåœ¨å¤šç§ç”Ÿæˆå™¨å’Œç”Ÿæˆæ¨¡å¼ä¸‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIç”Ÿæˆè§†é¢‘çš„æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æä¾›å¯è§£é‡Šæ€§å’Œç»†ç²’åº¦è¯æ®æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒå®¡è®¡å’Œç”¨æˆ·çš„ä¿¡ä»»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDAVID-XR1æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆæ—¶ç©ºæ³¨é‡Šå’Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œæä¾›é€æ˜çš„æ¨ç†è¿‡ç¨‹ï¼Œä½¿å¾—æ£€æµ‹ç»“æœæ›´å…·è¯´æœåŠ›å’Œå¯éªŒè¯æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†åŒ…å«AIç”Ÿæˆè§†é¢‘åŠå…¶ç¼ºé™·æ³¨é‡Šï¼Œæ¨¡å‹é€šè¿‡å¾®è°ƒå’Œè’¸é¦è®­ç»ƒæå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¼ºé™·çº§æ³¨é‡Šå’Œè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä½¿å¾—AIç”Ÿæˆè§†é¢‘æ£€æµ‹ä¸ä»…é™äºç»“æœï¼Œè¿˜èƒ½æä¾›æ¨ç†è¿‡ç¨‹çš„é€æ˜æ€§ï¼Œä¸ä¼ ç»Ÿé»‘ç®±æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨é€šç”¨éª¨å¹²ç½‘ç»œï¼Œç»“åˆé“¾å¼æ€ç»´è’¸é¦æŠ€æœ¯ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®ä¿åœ¨å¤šç§ç”Ÿæˆå™¨å’Œç”Ÿæˆæ¨¡å¼ä¸‹éƒ½èƒ½æœ‰æ•ˆå·¥ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„DAVID-XR1æ¨¡å‹åœ¨å¤šç§ç”Ÿæˆå™¨å’Œç”Ÿæˆæ¨¡å¼ä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†85%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†15%çš„æ£€æµ‹èƒ½åŠ›ï¼Œå±•ç¤ºäº†å¯è§£é‡Šæ£€æµ‹æ–¹æ³•çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€æ–°é—»çœŸå®æ€§éªŒè¯å’Œè§†é¢‘ç›‘æ§ç­‰ã€‚é€šè¿‡æä¾›å¯è§£é‡Šçš„æ£€æµ‹ç»“æœï¼Œèƒ½å¤Ÿå¢å¼ºç”¨æˆ·å¯¹AIç”Ÿæˆå†…å®¹çš„ä¿¡ä»»ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content.

