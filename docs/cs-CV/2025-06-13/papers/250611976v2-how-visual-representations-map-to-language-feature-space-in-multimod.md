---
layout: default
title: How Visual Representations Map to Language Feature Space in Multimodal LLMs
---

# How Visual Representations Map to Language Feature Space in Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11976" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11976v2</a>
  <a href="https://arxiv.org/pdf/2506.11976.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11976v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11976v2', 'How Visual Representations Map to Language Feature Space in Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-06-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå†»ç»“æ¨¡å‹ä¸çº¿æ€§é€‚é…å™¨ä»¥è§£å†³è§†è§‰ä¸è¯­è¨€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¯¹é½æœºåˆ¶` `çº¿æ€§é€‚é…å™¨` `ç¨€ç–è‡ªç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸è¯­è¨€è¡¨ç¤ºçš„å¯¹é½æœºåˆ¶ä¸Šå°šä¸æ˜ç¡®ï¼Œå¯¼è‡´å¤šæ¨¡æ€æ¨ç†æ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä¿æŒè¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹å†»ç»“ï¼Œä»…è®­ç»ƒçº¿æ€§é€‚é…å™¨æ¥å®ç°è§†è§‰ç‰¹å¾ä¸è¯­è¨€ç‰¹å¾çš„ç›´æ¥æ˜ å°„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰è¡¨ç¤ºä¸è¯­è¨€ç‰¹å¾è¡¨ç¤ºåœ¨ä¸­åå±‚é€æ¸å¯¹é½ï¼Œæ­ç¤ºäº†å½“å‰é€‚é…å™¨æ¶æ„çš„æ½œåœ¨ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ä¾èµ–äºè§†è§‰ä¸è¯­è¨€è¡¨ç¤ºçš„å¯¹é½ï¼Œä½†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®ç°è¿™ç§å¯¹é½çš„æœºåˆ¶ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡éµå¾ªLiMBeRæ¡†æ¶ï¼Œä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰å†»ç»“ï¼Œä»…é€šè¿‡è®­ç»ƒçº¿æ€§é€‚é…å™¨è¿›è¡Œè§†è§‰æŒ‡ä»¤è°ƒä¼˜ã€‚é€šè¿‡ä¿æŒè¯­è¨€æ¨¡å‹å†»ç»“ï¼Œç¡®ä¿å…¶åŸå§‹è¯­è¨€è¡¨ç¤ºä¸å—è§†è§‰æ•°æ®çš„é€‚åº”ï¼Œä»è€Œä½¿çº¿æ€§é€‚é…å™¨å¿…é¡»ç›´æ¥å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°LLMç°æœ‰çš„è¡¨ç¤ºç©ºé—´ã€‚å®éªŒè®¾è®¡ç‹¬ç‰¹åœ°åˆ©ç”¨äº†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ä½œä¸ºåˆ†ææ¢é’ˆï¼Œæ­ç¤ºäº†è§†è§‰è¡¨ç¤ºä¸è¯­è¨€ç‰¹å¾è¡¨ç¤ºé€å±‚å¯¹é½çš„è¿‡ç¨‹ï¼Œå°¤å…¶åœ¨ä¸­åå±‚æ”¶æ•›ã€‚è¿™è¡¨æ˜ViTè¾“å‡ºä¸æ—©æœŸLLMå±‚ä¹‹é—´å­˜åœ¨æ ¹æœ¬çš„ä¸å¯¹é½ï¼Œæå‡ºäº†å½“å‰åŸºäºé€‚é…å™¨çš„æ¶æ„æ˜¯å¦æœ€ä½³ä¿ƒè¿›è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„é‡è¦é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰ä¸è¯­è¨€è¡¨ç¤ºä¹‹é—´çš„å¯¹é½é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ­ç¤ºå…¶å¯¹é½æœºåˆ¶ï¼Œå¯¼è‡´å¤šæ¨¡æ€æ¨ç†çš„æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰å˜æ¢å™¨å†»ç»“ï¼Œä»…è®­ç»ƒçº¿æ€§é€‚é…å™¨ï¼Œä½¿å…¶ç›´æ¥å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ï¼Œä»è€Œé¿å…è¯­è¨€æ¨¡å‹å¯¹è§†è§‰æ•°æ®çš„é€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå†»ç»“çš„LLMå’Œä¸€ä¸ªå†»ç»“çš„ViTï¼ŒäºŒè€…é€šè¿‡çº¿æ€§é€‚é…å™¨è¿æ¥ã€‚å®éªŒä¸­ä½¿ç”¨é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ä½œä¸ºåˆ†æå·¥å…·ï¼Œè¯„ä¼°è§†è§‰ç‰¹å¾ä¸è¯­è¨€ç‰¹å¾çš„å¯¹é½æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å†»ç»“æ¨¡å‹ä¿æŒè¯­è¨€è¡¨ç¤ºä¸å˜ï¼Œç¡®ä¿çº¿æ€§é€‚é…å™¨çš„æ˜ å°„ç›´æ¥åæ˜ è§†è§‰ç‰¹å¾ä¸è¯­è¨€ç‰¹å¾çš„å…³ç³»ï¼Œæ­ç¤ºäº†å±‚çº§å¯¹é½çš„åŠ¨æ€è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†ç¨€ç–è‡ªç¼–ç å™¨ä½œä¸ºåˆ†ææ¢é’ˆï¼Œå…³æ³¨é‡å»ºè¯¯å·®ã€ç¨€ç–æ¨¡å¼å’Œç‰¹å¾æè¿°ï¼Œç³»ç»Ÿåˆ†æäº†è§†è§‰è¡¨ç¤ºä¸è¯­è¨€è¡¨ç¤ºçš„å¯¹é½è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œèƒ½å¤Ÿæ·±å…¥ç†è§£ä¸åŒå±‚æ¬¡çš„å¯¹é½æƒ…å†µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§†è§‰è¡¨ç¤ºä¸è¯­è¨€ç‰¹å¾è¡¨ç¤ºåœ¨ä¸­åå±‚é€æ¸å¯¹é½ï¼Œæ­ç¤ºäº†ViTè¾“å‡ºä¸æ—©æœŸLLMå±‚ä¹‹é—´çš„æ ¹æœ¬ä¸å¯¹é½ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œå‘ç°å½“å‰åŸºäºé€‚é…å™¨çš„æ¶æ„åœ¨ä¿ƒè¿›è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ–¹é¢å­˜åœ¨æ½œåœ¨ä¸è¶³ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€æ™ºèƒ½åŠ©ç†å’Œè‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æ”¹è¿›è§†è§‰ä¸è¯­è¨€çš„å¯¹é½æœºåˆ¶ï¼Œå¯ä»¥æå‡è¿™äº›ç³»ç»Ÿåœ¨ç†è§£å’Œç”Ÿæˆå¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly understood. Following the LiMBeR framework, we deliberately maintain a frozen large language model (LLM) and a frozen vision transformer (ViT), connected solely by training a linear adapter during visual instruction tuning. By keeping the language model frozen, we ensure it maintains its original language representations without adaptation to visual data. Consequently, the linear adapter must map visual features directly into the LLM's existing representational space rather than allowing the language model to develop specialized visual understanding through fine-tuning. Our experimental design uniquely enables the use of pre-trained sparse autoencoders (SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned with the unchanged language model and serve as a snapshot of the learned language feature-representations. Through systematic analysis of SAE reconstruction error, sparsity patterns, and feature SAE descriptions, we reveal the layer-wise progression through which visual representations gradually align with language feature representations, converging in middle-to-later layers. This suggests a fundamental misalignment between ViT outputs and early LLM layers, raising important questions about whether current adapter-based architectures optimally facilitate cross-modal representation learning.

