---
layout: default
title: VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?
---

# VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11571" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11571v2</a>
  <a href="https://arxiv.org/pdf/2506.11571.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11571v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11571v2', 'VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiachen Yu, Yufei Zhan, Ziheng Wu, Yousong Zhu, Jinqiao Wang, Minghui Qiu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-07-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVFaithä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡å‹` `è§†è§‰æ¨ç†` `è‡ªåŠ¨åŒ–ç¼–è¾‘` `GPT-Image-1` `VFaith-Bench` `è§†è§‰çœŸå®æ€§` `æ¨ç†èƒ½åŠ›è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ç¼ºä¹å®šé‡åˆ†æï¼Œéš¾ä»¥æ˜ç¡®è§†è§‰çº¿ç´¢æå–ä¸æ¨ç†è¿‡ç¨‹çš„è´¡çŒ®ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºGPT-Image-1çš„è‡ªåŠ¨åŒ–ç¼–è¾‘ç®¡é“ï¼Œèƒ½å¤Ÿç²¾ç¡®ç¼–è¾‘è§†è§‰çº¿ç´¢ï¼Œå¹¶å¼•å…¥VFaith-BenchåŸºå‡†è¯„ä¼°æ¨ç†èƒ½åŠ›ã€‚
3. é€šè¿‡å¯¹æ¯”å®éªŒï¼Œç ”ç©¶å‘ç°æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ä¸è§†è§‰æ„ŸçŸ¥ä¹‹é—´å­˜åœ¨æ˜¾è‘—å…³ç³»ï¼Œæä¾›äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¼•å…¥é•¿é“¾æ¨ç†ï¼ˆCoTï¼‰ï¼Œå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™ç§æå‡çš„åŸå› å°šä¸æ˜ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä¿¡æ¯æå–å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å…·ä½“è´¡çŒ®ã€‚å› æ­¤ï¼Œè¯„ä¼°MLLMsæ¨ç†çš„çœŸå®æ€§è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGPT-Image-1çš„è‡ªåŠ¨åŒ–ç¼–è¾‘ç®¡é“ï¼Œèƒ½å¤Ÿæ ¹æ®æŒ‡ä»¤è‡ªåŠ¨ç²¾ç¡®åœ°ç¼–è¾‘ç‰¹å®šè§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†VFaith-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°MLLMsè§†è§‰æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œé‡ç‚¹åˆ†æå…¶è§†è§‰çœŸå®æ€§ã€‚é€šè¿‡å¯¹å›¾åƒä¸­å…³é”®è§†è§‰çº¿ç´¢çš„ç¼–è¾‘ï¼Œæ„å»ºäº†æ¯”è¾ƒé—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œè¿›è€Œæµ‹è¯•æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚VFaith-BenchåŒ…å«755ä¸ªæ¡ç›®ï¼Œåˆ†ä¸ºäº”ä¸ªä¸åŒçš„å­é›†ï¼Œå¹¶è¿›è¡Œäº†æ·±å…¥çš„æµ‹è¯•å’Œåˆ†æã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹è§†è§‰ä¿¡æ¯çš„çœŸå®æ€§è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆé‡åŒ–è§†è§‰çº¿ç´¢æå–ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½æå‡åŸå› çš„ç†è§£ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–ç¼–è¾‘ç®¡é“ï¼Œç²¾ç¡®æ§åˆ¶è§†è§‰çº¿ç´¢çš„å˜åŒ–ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§†è§‰ä¿¡æ¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºæ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è§†è§‰æ„ŸçŸ¥ä¹‹é—´çš„å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªåŸºäºGPT-Image-1çš„ç¼–è¾‘ç®¡é“å’ŒVFaith-BenchåŸºå‡†ã€‚ç¼–è¾‘ç®¡é“ç”¨äºè‡ªåŠ¨åŒ–å¤„ç†è§†è§‰çº¿ç´¢ï¼ŒVFaith-Benchåˆ™æä¾›äº†å¤šæ ·åŒ–çš„æµ‹è¯•é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†VFaith-BenchåŸºå‡†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºè§†è§‰æ¨ç†èƒ½åŠ›çš„è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°åˆ†ææ¨¡å‹çš„è§†è§‰çœŸå®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŒ‡æ ‡æ¥é‡åŒ–è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ„å»ºäº†755ä¸ªæ¡ç›®çš„æµ‹è¯•é›†ï¼Œæ¶µç›–äº”ä¸ªå­é›†ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨VFaith-Benchè¯„ä¼°çš„å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨è§†è§‰æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒè§†è§‰çº¿ç´¢çš„å½±å“ï¼Œæ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†15%ï¼Œæœ‰æ•ˆæ­ç¤ºäº†è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹å’ŒåŒ»ç–—å½±åƒåˆ†æç­‰å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent extensive works have demonstrated that by introducing long CoT, the capabilities of MLLMs to solve complex problems can be effectively enhanced. However, the reasons for the effectiveness of such paradigms remain unclear. It is challenging to analysis with quantitative results how much the model's specific extraction of visual cues and its subsequent so-called reasoning during inference process contribute to the performance improvements. Therefore, evaluating the faithfulness of MLLMs' reasoning to visual information is crucial. To address this issue, we first present a cue-driven automatic and controllable editing pipeline with the help of GPT-Image-1. It enables the automatic and precise editing of specific visual cues based on the instruction. Furthermore, we introduce VFaith-Bench, the first benchmark to evaluate MLLMs' visual reasoning capabilities and analyze the source of such capabilities with an emphasis on the visual faithfulness. Using the designed pipeline, we constructed comparative question-answer pairs by altering the visual cues in images that are crucial for solving the original reasoning problem, thereby changing the question's answer. By testing similar questions with images that have different details, the average accuracy reflects the model's visual reasoning ability, while the difference in accuracy before and after editing the test set images effectively reveals the relationship between the model's reasoning ability and visual perception. We further designed specific metrics to expose this relationship. VFaith-Bench includes 755 entries divided into five distinct subsets, along with an additional human-labeled perception task. We conducted in-depth testing and analysis of existing mainstream flagship models and prominent open-source model series/reasoning models on VFaith-Bench, further investigating the underlying factors of their reasoning capabilities.

