---
layout: default
title: TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models
---

# TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11436" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11436v2</a>
  <a href="https://arxiv.org/pdf/2506.11436.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11436v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11436v2', 'TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyang Luo, Nian Liu, Xuguang Yang, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, Junwei Han

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-12-10)

**å¤‡æ³¨**: ICCV2025,code:https://github.com/Sssssuperior/TAViS

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTAViSä»¥è§£å†³éŸ³è§†é¢‘åˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³è§†é¢‘åˆ†å‰²` `è·¨æ¨¡æ€å¯¹é½` `å¤šæ¨¡æ€å­¦ä¹ ` `æ–‡æœ¬æ¡¥æ¥` `åŸºç¡€æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³è§†é¢‘åˆ†å‰²æ–¹æ³•åœ¨è·¨æ¨¡æ€å¯¹é½ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆç»“åˆéŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ã€‚
2. æœ¬æ–‡æå‡ºçš„TAViSæ¡†æ¶é€šè¿‡æ–‡æœ¬æ¡¥æ¥è®¾è®¡ï¼Œç»“åˆå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ï¼Œå®ç°éŸ³è§†é¢‘çš„ç²¾ç¡®åˆ†å‰²å’Œå¯¹é½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTAViSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³è§†é¢‘åˆ†å‰²ï¼ˆAVSï¼‰é¢ä¸´æœ‰æ•ˆå¯¹é½éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„åŸºæœ¬æŒ‘æˆ˜ã€‚å°½ç®¡è¿‘æœŸæ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹æ¥åº”å¯¹æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½†å¾€å¾€ä¾èµ–å•ä¸€æ¨¡æ€çŸ¥è¯†æˆ–ä»¥ç°æˆæ–¹å¼ç»“åˆåŸºç¡€æ¨¡å‹ï¼Œæœªèƒ½æœ‰æ•ˆè§£å†³è·¨æ¨¡æ€å¯¹é½é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†TAViSï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆImageBindï¼‰å’Œåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼ˆSAM2ï¼‰æ¥å®ç°è·¨æ¨¡æ€å¯¹é½å’Œç²¾ç¡®åˆ†å‰²ã€‚ä¸ºäº†è§£å†³æ¨¡å‹é—´çŸ¥è¯†è½¬ç§»çš„å›°éš¾å’Œä»…ä½¿ç”¨åˆ†å‰²æŸå¤±è¿›è¡Œç›‘ç£çš„ä¸è¶³ï¼Œæœ¬æ–‡å¼•å…¥äº†æ–‡æœ¬æ¡¥æ¥è®¾è®¡ï¼ŒåŒ…å«ä¼ªæ–‡æœ¬æä¾›ç±»åˆ«åŸå‹ä¿¡æ¯å’Œå¯¹é½ç›‘ç£ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨å•æºã€å¤šæºå’Œè¯­ä¹‰æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³è§†é¢‘åˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å¯¹é½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å•ä¸€æ¨¡æ€çŸ¥è¯†ï¼Œæœªèƒ½æœ‰æ•ˆæ•´åˆéŸ³é¢‘ä¸è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´åˆ†å‰²ç²¾åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„TAViSæ¡†æ¶é€šè¿‡æ–‡æœ¬æ¡¥æ¥è®¾è®¡ï¼Œåˆ©ç”¨ä¼ªæ–‡æœ¬æä¾›ç±»åˆ«åŸå‹ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™éŸ³é¢‘å’Œè§†è§‰è¾“å…¥çš„æ¨¡æ€ç‰¹å¾ï¼Œä»è€Œå®ç°æ›´å¥½çš„è·¨æ¨¡æ€å¯¹é½ä¸åˆ†å‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTAViSæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šæ–‡æœ¬æ¡¥æ¥æ··åˆæç¤ºæœºåˆ¶å’Œå¯¹é½ç›‘ç£ç­–ç•¥ã€‚å‰è€…é€šè¿‡ä¼ªæ–‡æœ¬æä¾›ç±»åˆ«ä¿¡æ¯ï¼Œåè€…åˆ©ç”¨æ–‡æœ¬å¯¹éŸ³è§†é¢‘æ¨¡æ€è¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTAViSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥æ–‡æœ¬ä½œä¸ºæ¡¥æ¢ï¼Œè§£å†³äº†ä¸åŒç‰¹å¾ç©ºé—´é—´çš„çŸ¥è¯†è½¬ç§»é—®é¢˜ï¼Œå¹¶é€šè¿‡å¯¹é½ç›‘ç£ç­–ç•¥æå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ··åˆæç¤ºæœºåˆ¶å’Œå¯¹é½ç›‘ç£ç­–ç•¥ï¼Œç¡®ä¿äº†éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„æœ‰æ•ˆç»“åˆã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¤šæ¨¡æ€ç‰¹å¾çš„å¯¹é½ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TAViSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚è¯¥æ–¹æ³•åœ¨å•æºå’Œå¤šæºæ•°æ®é›†ä¸Šå‡å®ç°äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„åˆ†å‰²ç²¾åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿå¯¹å¤æ‚ç¯å¢ƒä¸­éŸ³è§†é¢‘ä¿¡æ¯çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›ã€‚æœªæ¥ï¼ŒTAViSå¯èƒ½åœ¨å¤šæ¨¡æ€äº¤äº’å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Audio-Visual Segmentation (AVS) faces a fundamental challenge of effectively aligning audio and visual modalities. While recent approaches leverage foundation models to address data scarcity, they often rely on single-modality knowledge or combine foundation models in an off-the-shelf manner, failing to address the cross-modal alignment challenge. In this paper, we present TAViS, a novel framework that \textbf{couples} the knowledge of multimodal foundation models (ImageBind) for cross-modal alignment and a segmentation foundation model (SAM2) for precise segmentation. However, effectively combining these models poses two key challenges: the difficulty in transferring the knowledge between SAM2 and ImageBind due to their different feature spaces, and the insufficiency of using only segmentation loss for supervision. To address these challenges, we introduce a text-bridged design with two key components: (1) a text-bridged hybrid prompting mechanism where pseudo text provides class prototype information while retaining modality-specific details from both audio and visual inputs, and (2) an alignment supervision strategy that leverages text as a bridge to align shared semantic concepts within audio-visual modalities. Our approach achieves superior performance on single-source, multi-source, semantic datasets, and excels in zero-shot settings.

