---
layout: default
title: Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model
---

# Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11737" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11737v1</a>
  <a href="https://arxiv.org/pdf/2506.11737.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11737v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11737v1', 'Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dinh Viet Cuong, Hoang-Bao Le, An Pham Ngoc Nguyen, Liting Zhou, Cathal Gurrin

**åˆ†ç±»**: cs.CV, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dinhvietcuong1996/icme25-inova)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLaVA-NeXT-Interleaveä»¥è§£å†³å¤šå›¾åƒæ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šå›¾åƒæ¨ç†` `è¯­ä¹‰ç†è§£` `å³æ’å³ç”¨æŠ€æœ¯` `æ·±åº¦å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†ä»»åŠ¡ä¸­é¢ä¸´å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºLLaVA-NeXT-Interleaveæ¨¡å‹ï¼Œå¹¶å¼•å…¥DCIè¿æ¥å™¨ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ ‡å‡†æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒDCIç‰ˆæœ¬åœ¨éœ€è¦æ·±å±‚è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ä¸¤ä¸ªç›®æ ‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†LLaVA-NeXT-Interleaveåœ¨22ä¸ªæ•°æ®é›†ä¸Šé’ˆå¯¹å¤šå›¾åƒæ¨ç†ã€æ–‡æ¡£å’ŒçŸ¥è¯†ç†è§£ä»¥åŠäº¤äº’å¼å¤šæ¨¡æ€é€šä¿¡ä¸‰é¡¹ä»»åŠ¡çš„å‡ºè‰²è¡¨ç°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†Dense Channel Integration (DCI)è¿æ¥å™¨æ·»åŠ åˆ°LLaVA-NeXT-Interleaveä¸­ï¼Œå¹¶ä¸æ ‡å‡†æ¨¡å‹è¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæ ‡å‡†æ¨¡å‹åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚VISIONã€NLVR2å’ŒFashion200Kï¼‰ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒDCIå¢å¼ºç‰ˆæœ¬åœ¨éœ€è¦æ›´æ·±è¯­ä¹‰ä¸€è‡´æ€§æˆ–ç»“æ„å˜åŒ–ç†è§£çš„æ•°æ®é›†ï¼ˆå¦‚MIT-States_PropertyCoherenceå’ŒSlideVQAï¼‰ä¸Šè¡¨ç°çªå‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å°†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ä¸å³æ’å³ç”¨æŠ€æœ¯ç»“åˆåœ¨ä¸€èµ·çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå›¾åƒæ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶å¸¸å¸¸æ— æ³•æä¾›è¶³å¤Ÿçš„è¯­ä¹‰ä¸€è‡´æ€§å’Œç»“æ„å˜åŒ–ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LLaVA-NeXT-Interleaveæ¨¡å‹é€šè¿‡å¼•å…¥DCIè¿æ¥å™¨ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ·±å±‚è¯­ä¹‰ç†è§£çš„åœºæ™¯ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬LLaVA-NeXT-Interleaveæ¨¡å‹å’ŒDCIè¿æ¥å™¨ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®è¾“å…¥ã€ç‰¹å¾æå–ã€è¯­ä¹‰ç†è§£å’Œè¾“å‡ºç”Ÿæˆã€‚æ¨¡å‹é€šè¿‡å¤šå±‚æ¬¡çš„ç‰¹å¾èåˆæ¥å¢å¼ºä¿¡æ¯çš„ä¼ é€’å’Œç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºDCIè¿æ¥å™¨çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†å¤æ‚è¯­ä¹‰å…³ç³»æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæå‡äº†å¯¹ç»“æ„å˜åŒ–çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬DCIè¿æ¥å™¨çš„é…ç½®ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠç½‘ç»œç»“æ„çš„ä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°æ›´é«˜çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ ‡å‡†LLaVA-NeXT-Interleaveæ¨¡å‹åœ¨VISIONã€NLVR2å’ŒFashion200Kç­‰è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€é«˜çš„æ•´ä½“å‡†ç¡®æ€§ï¼Œè€ŒDCIå¢å¼ºç‰ˆæœ¬åœ¨MIT-States_PropertyCoherenceå’ŒSlideVQAç­‰éœ€è¦æ·±å±‚è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ¨¡æ€ç†è§£çš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰ã€‚é€šè¿‡æå‡å¤šå›¾åƒæ¨ç†çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´ä¸ºç²¾å‡†çš„å†³ç­–æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses two main objectives. Firstly, we demonstrate the impressive performance of the LLaVA-NeXT-interleave on 22 datasets across three different tasks: Multi-Image Reasoning, Documents and Knowledge-Based Understanding and Interactive Multi-Modal Communication. Secondly, we add the Dense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and compare its performance against the standard model. We find that the standard model achieves the highest overall accuracy, excelling in vision-heavy tasks like VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows particular strength on datasets requiring deeper semantic coherence or structured change understanding such as MIT-States_PropertyCoherence and SlideVQA. Our results highlight the potential of combining powerful foundation models with plug-and-play techniques for Interleave tasks. The code is available at https://github.com/dinhvietcuong1996/icme25-inova.

