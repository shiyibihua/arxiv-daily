---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-13
---

# cs.CVï¼ˆ2025-06-13ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250611672v1-dynamic-mixture-of-curriculum-lora-experts-for-continual-multimodal-.html">Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning</a></td>
  <td>æå‡ºåŠ¨æ€æ··åˆè¯¾ç¨‹LoRAä¸“å®¶ä»¥è§£å†³æŒç»­å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11672v1" data-paper-url="./papers/250611672v1-dynamic-mixture-of-curriculum-lora-experts-for-continual-multimodal-.html" onclick="toggleFavorite(this, '2506.11672v1', 'Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250611558v3-damo-a-data-efficient-multimodal-orchestrator-for-temporal-reasoning.html">DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs</a></td>
  <td>æå‡ºDaMOä»¥è§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­çš„æ—¶åºæ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11558v3" data-paper-url="./papers/250611558v3-damo-a-data-efficient-multimodal-orchestrator-for-temporal-reasoning.html" onclick="toggleFavorite(this, '2506.11558v3', 'DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250611436v2-tavis-text-bridged-audio-visual-segmentation-with-foundation-models.html">TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models</a></td>
  <td>æå‡ºTAViSä»¥è§£å†³éŸ³è§†é¢‘åˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11436v2" data-paper-url="./papers/250611436v2-tavis-text-bridged-audio-visual-segmentation-with-foundation-models.html" onclick="toggleFavorite(this, '2506.11436v2', 'TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250611991v2-vgr-visual-grounded-reasoning.html">VGR: Visual Grounded Reasoning</a></td>
  <td>æå‡ºVGRä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯­è¨€åè§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11991v2" data-paper-url="./papers/250611991v2-vgr-visual-grounded-reasoning.html" onclick="toggleFavorite(this, '2506.11991v2', 'VGR: Visual Grounded Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250611753v1-exploring-the-effectiveness-of-deep-features-from-domain-specific-fo.html">Exploring the Effectiveness of Deep Features from Domain-Specific Foundation Models in Retinal Image Synthesis</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦ç‰¹å¾çš„æŸå¤±å‡½æ•°ä»¥æ”¹è¿›è§†ç½‘è†œå›¾åƒåˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11753v1" data-paper-url="./papers/250611753v1-exploring-the-effectiveness-of-deep-features-from-domain-specific-fo.html" onclick="toggleFavorite(this, '2506.11753v1', 'Exploring the Effectiveness of Deep Features from Domain-Specific Foundation Models in Retinal Image Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250611571v2-vfaith-do-large-multimodal-models-really-reason-on-seen-images-rathe.html">VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?</a></td>
  <td>æå‡ºVFaithä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11571v2" data-paper-url="./papers/250611571v2-vfaith-do-large-multimodal-models-really-reason-on-seen-images-rathe.html" onclick="toggleFavorite(this, '2506.11571v2', 'VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250611380v1-enhance-multimodal-consistency-and-coherence-for-text-image-plan-gen.html">Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€ä¸€è‡´æ€§ä¸è¿è´¯æ€§å¢å¼ºæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬-å›¾åƒè®¡åˆ’ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11380v1" data-paper-url="./papers/250611380v1-enhance-multimodal-consistency-and-coherence-for-text-image-plan-gen.html" onclick="toggleFavorite(this, '2506.11380v1', 'Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250611515v1-manager-aggregating-insights-from-unimodal-experts-in-two-tower-vlms.html">Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs</a></td>
  <td>æå‡ºManageræ’ä»¶ä»¥è§£å†³ä¸¤å¡”VLMså’ŒMLLMsä¸­çš„å•æ¨¡æ€ä¸“å®¶èšåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11515v1" data-paper-url="./papers/250611515v1-manager-aggregating-insights-from-unimodal-experts-in-two-tower-vlms.html" onclick="toggleFavorite(this, '2506.11515v1', 'Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250611772v3-clip-meets-diffusion-a-synergistic-approach-to-anomaly-detection.html">CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection</a></td>
  <td>æå‡ºCLIPFUSIONä»¥è§£å†³å¼‚å¸¸æ£€æµ‹ä¸­çš„å¤šæ¨¡æ€èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11772v3" data-paper-url="./papers/250611772v3-clip-meets-diffusion-a-synergistic-approach-to-anomaly-detection.html" onclick="toggleFavorite(this, '2506.11772v3', 'CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250611737v1-quizzardinova-challenge-2025-track-a-plug-and-play-technique-in-inte.html">Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model</a></td>
  <td>æå‡ºLLaVA-NeXT-Interleaveä»¥è§£å†³å¤šå›¾åƒæ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11737v1" data-paper-url="./papers/250611737v1-quizzardinova-challenge-2025-track-a-plug-and-play-technique-in-inte.html" onclick="toggleFavorite(this, '2506.11737v1', 'Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250611599v2-a2lc-active-and-automated-label-correction-for-semantic-segmentation.html">A$^2$LC: Active and Automated Label Correction for Semantic Segmentation</a></td>
  <td>æå‡ºA$^2$LCæ¡†æ¶ä»¥è§£å†³è¯­ä¹‰åˆ†å‰²ä¸­çš„æ ‡ç­¾çº é”™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11599v2" data-paper-url="./papers/250611599v2-a2lc-active-and-automated-label-correction-for-semantic-segmentation.html" onclick="toggleFavorite(this, '2506.11599v2', 'A$^2$LC: Active and Automated Label Correction for Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250611768v1-mambavsr-content-aware-scanning-state-space-model-for-video-super-re.html">MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution</a></td>
  <td>æå‡ºMambaVSRä»¥è§£å†³è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­çš„éå±€éƒ¨ä¾èµ–å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11768v1" data-paper-url="./papers/250611768v1-mambavsr-content-aware-scanning-state-space-model-for-video-super-re.html" onclick="toggleFavorite(this, '2506.11768v1', 'MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250611976v2-how-visual-representations-map-to-language-feature-space-in-multimod.html">How Visual Representations Map to Language Feature Space in Multimodal LLMs</a></td>
  <td>æå‡ºå†»ç»“æ¨¡å‹ä¸çº¿æ€§é€‚é…å™¨ä»¥è§£å†³è§†è§‰ä¸è¯­è¨€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11976v2" data-paper-url="./papers/250611976v2-how-visual-representations-map-to-language-feature-space-in-multimod.html" onclick="toggleFavorite(this, '2506.11976v2', 'How Visual Representations Map to Language Feature Space in Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250612208v1-inceptionmamba-efficient-multi-stage-feature-enhancement-with-select.html">InceptionMamba: Efficient Multi-Stage Feature Enhancement with Selective State Space Model for Microscopic Medical Image Segmentation</a></td>
  <td>æå‡ºInceptionMambaä»¥è§£å†³æ˜¾å¾®åŒ»å­¦å›¾åƒåˆ†å‰²æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.12208v1" data-paper-url="./papers/250612208v1-inceptionmamba-efficient-multi-stage-feature-enhancement-with-select.html" onclick="toggleFavorite(this, '2506.12208v1', 'InceptionMamba: Efficient Multi-Stage Feature Enhancement with Selective State Space Model for Microscopic Medical Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250611773v4-agentsense-virtual-sensor-data-generation-using-llm-agents-in-simula.html">AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments</a></td>
  <td>æå‡ºAgentSenseä»¥è§£å†³æ™ºèƒ½å®¶å±…ä¸­ç¼ºä¹å¤šæ ·åŒ–æ ‡æ³¨æ•°æ®çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11773v4" data-paper-url="./papers/250611773v4-agentsense-virtual-sensor-data-generation-using-llm-agents-in-simula.html" onclick="toggleFavorite(this, '2506.11773v4', 'AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250611417v1-stop-learning-it-all-to-mitigate-visual-hallucination-focus-on-the-h.html">Stop learning it all to mitigate visual hallucination, Focus on the hallucination target</a></td>
  <td>æå‡ºåå¥½å­¦ä¹ æ–¹æ³•ä»¥ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">preference learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11417v1" data-paper-url="./papers/250611417v1-stop-learning-it-all-to-mitigate-visual-hallucination-focus-on-the-h.html" onclick="toggleFavorite(this, '2506.11417v1', 'Stop learning it all to mitigate visual hallucination, Focus on the hallucination target')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250611777v2-self-supervised-learning-of-echocardiographic-video-representations-.html">Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation</a></td>
  <td>æå‡ºDISCOVRä»¥è§£å†³å¿ƒè„è¶…å£°è§†é¢‘è¡¨ç¤ºå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11777v2" data-paper-url="./papers/250611777v2-self-supervised-learning-of-echocardiographic-video-representations-.html" onclick="toggleFavorite(this, '2506.11777v2', 'Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250614827v1-david-xr1-detecting-ai-generated-videos-with-explainable-reasoning.html">DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning</a></td>
  <td>æå‡ºDAVID-XR1ä»¥è§£å†³AIç”Ÿæˆè§†é¢‘æ£€æµ‹çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14827v1" data-paper-url="./papers/250614827v1-david-xr1-detecting-ai-generated-videos-with-explainable-reasoning.html" onclick="toggleFavorite(this, '2506.14827v1', 'DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250611595v1-easyarc-evaluating-vision-language-models-on-true-visual-reasoning.html">EasyARC: Evaluating Vision Language Models on True Visual Reasoning</a></td>
  <td>æå‡ºEasyARCä»¥è§£å†³å¤šæ¨¡æ€è§†è§‰æ¨ç†è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11595v1" data-paper-url="./papers/250611595v1-easyarc-evaluating-vision-language-models-on-true-visual-reasoning.html" onclick="toggleFavorite(this, '2506.11595v1', 'EasyARC: Evaluating Vision Language Models on True Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250611430v3-auto-connect-connectivity-preserving-rigformer-with-direct-preferenc.html">Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization</a></td>
  <td>æå‡ºAuto-Connectä»¥è§£å†³è‡ªåŠ¨ç»‘å®šä¸­éª¨éª¼è¿é€šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">direct preference optimization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11430v3" data-paper-url="./papers/250611430v3-auto-connect-connectivity-preserving-rigformer-with-direct-preferenc.html" onclick="toggleFavorite(this, '2506.11430v3', 'Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250614825v2-graphgsocc-semantic-geometric-graph-transformer-with-dynamic-static-.html">GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static Decoupling for 3D Gaussian Splatting-based Occupancy Prediction</a></td>
  <td>æå‡ºGraphGSOccä»¥è§£å†³3Dè¯­ä¹‰å ç”¨é¢„æµ‹ä¸­çš„åŠ¨æ€é™æ€è€¦åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.14825v2" data-paper-url="./papers/250614825v2-graphgsocc-semantic-geometric-graph-transformer-with-dynamic-static-.html" onclick="toggleFavorite(this, '2506.14825v2', 'GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static Decoupling for 3D Gaussian Splatting-based Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250612009v1-affogato-learning-open-vocabulary-affordance-grounding-with-automate.html">Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale</a></td>
  <td>æå‡ºAffogatoä»¥è§£å†³å¼€æ”¾è¯æ±‡çš„å¯ç”¨æ€§å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">affordance</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.12009v1" data-paper-url="./papers/250612009v1-affogato-learning-open-vocabulary-affordance-grounding-with-automate.html" onclick="toggleFavorite(this, '2506.12009v1', 'Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250611585v1-ov-map-open-vocabulary-zero-shot-3d-instance-segmentation-map-for-ro.html">OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots</a></td>
  <td>æå‡ºOV-MAPä»¥è§£å†³å¼€æ”¾ä¸–ç•Œ3Då®ä¾‹åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11585v1" data-paper-url="./papers/250611585v1-ov-map-open-vocabulary-zero-shot-3d-instance-segmentation-map-for-ro.html" onclick="toggleFavorite(this, '2506.11585v1', 'OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250611394v1-dynamic-double-space-tower.html">Dynamic Double Space Tower</a></td>
  <td>æå‡ºåŠ¨æ€åŒç©ºé—´å¡”ä»¥è§£å†³è§†è§‰é—®ç­”ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11394v1" data-paper-url="./papers/250611394v1-dynamic-double-space-tower.html" onclick="toggleFavorite(this, '2506.11394v1', 'Dynamic Double Space Tower')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250611863v2-spheredrag-spherical-geometry-aware-panoramic-image-editing.html">SphereDrag: Spherical Geometry-Aware Panoramic Image Editing</a></td>
  <td>æå‡ºSphereDragä»¥è§£å†³å…¨æ™¯å›¾åƒç¼–è¾‘ä¸­çš„å‡ ä½•é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11863v2" data-paper-url="./papers/250611863v2-spheredrag-spherical-geometry-aware-panoramic-image-editing.html" onclick="toggleFavorite(this, '2506.11863v2', 'SphereDrag: Spherical Geometry-Aware Panoramic Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250611621v1-signaligner-harmonizing-complementary-pose-modalities-for-coherent-s.html">SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation</a></td>
  <td>æå‡ºSignAlignerä»¥è§£å†³æ‰‹è¯­ç”Ÿæˆä¸­çš„å¤šæ¨¡æ€åè°ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11621v1" data-paper-url="./papers/250611621v1-signaligner-harmonizing-complementary-pose-modalities-for-coherent-s.html" onclick="toggleFavorite(this, '2506.11621v1', 'SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250611549v1-eyesim-vqa-a-free-energy-guided-eye-simulation-framework-for-video-q.html">EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment</a></td>
  <td>æå‡ºEyeSim-VQAä»¥è§£å†³è§†é¢‘è´¨é‡è¯„ä¼°ä¸­çš„è‡ªé€‚åº”ä¿®å¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11549v1" data-paper-url="./papers/250611549v1-eyesim-vqa-a-free-energy-guided-eye-simulation-framework-for-video-q.html" onclick="toggleFavorite(this, '2506.11549v1', 'EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250612258v1-egoprivacy-what-your-first-person-camera-says-about-you.html">EgoPrivacy: What Your First-Person Camera Says About You?</a></td>
  <td>æå‡ºEgoPrivacyä»¥è¯„ä¼°ç¬¬ä¸€äººç§°è§†é¢‘çš„éšç§é£é™©</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">first-person view</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.12258v1" data-paper-url="./papers/250612258v1-egoprivacy-what-your-first-person-camera-says-about-you.html" onclick="toggleFavorite(this, '2506.12258v1', 'EgoPrivacy: What Your First-Person Camera Says About You?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250612251v2-efficient-multi-camera-tokenization-with-triplanes-for-end-to-end-dr.html">Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving</a></td>
  <td>æå‡ºåŸºäºä¸‰å¹³é¢çš„å¤šæ‘„åƒå¤´é«˜æ•ˆæ ‡è®°æ–¹æ³•ä»¥æå‡è‡ªåŠ¨é©¾é©¶æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.12251v2" data-paper-url="./papers/250612251v2-efficient-multi-camera-tokenization-with-triplanes-for-end-to-end-dr.html" onclick="toggleFavorite(this, '2506.12251v2', 'Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)