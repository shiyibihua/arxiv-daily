---
layout: default
title: Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation
---

# Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.11654" target="_blank" class="toolbar-btn">arXiv: 2512.11654v1</a>
    <a href="https://arxiv.org/pdf/2512.11654.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11654v1" 
            onclick="toggleFavorite(this, '2512.11654v1', 'Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Luca Cazzola, Ahed Alboody

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-12

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://lucazzola.github.io/publications/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**KineMICÔºöÈÄöËøáÊñáÊú¨Âà∞Âä®‰ΩúËí∏È¶èÂÆûÁé∞Â∞ëÊ†∑Êú¨Âä®‰ΩúÂêàÊàêÔºåËß£ÂÜ≥HARÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `Â∞ëÊ†∑Êú¨Â≠¶‰π†` `Âä®‰ΩúÂêàÊàê` `ÊñáÊú¨Âà∞Âä®‰Ωú` `ËøÅÁßªÂ≠¶‰π†` `‰∫∫‰ΩìÊ¥ªÂä®ËØÜÂà´` `Êâ©Êï£Ê®°Âûã` `ËøêÂä®ÊåñÊéò`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâHARÊñπÊ≥ï‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåËÄåT2MÊ®°ÂûãËôΩÁÑ∂ËÉΩÁîüÊàêÂä®‰ΩúÔºå‰ΩÜ‰∏éHAR‰ªªÂä°ÈúÄÊ±ÇÂ≠òÂú®È¢ÜÂüüÂ∑ÆÂºÇ„ÄÇ
2. KineMICÂà©Áî®ÊñáÊú¨ÁºñÁ†ÅÁ©∫Èó¥ÁöÑËØ≠‰πâÂØπÂ∫îÂÖ≥Á≥ªÔºåÈÄöËøáËøêÂä®ÊåñÊéòÁ≠ñÁï•ÔºåÂ∞ÜÈÄöÁî®T2MÊ®°ÂûãËøÅÁßªÂà∞HARÈ¢ÜÂüü„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåKineMICÂú®Â∞ëÊ†∑Êú¨ÊÉÖÂÜµ‰∏ãÊòæËëóÊèêÂçá‰∫ÜÂä®‰ΩúÁîüÊàêË¥®ÈáèÔºåÂπ∂‰ΩøHARÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü23.1%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈíàÂØπÂü∫‰∫éÈ™®È™ºÁöÑ‰∫∫‰ΩìÊ¥ªÂä®ËØÜÂà´(HAR)‰∏≠Â∏¶Ê†áÊ≥®ÁöÑÂ§ßÂûãËøêÂä®Êï∞ÊçÆÈõÜËé∑ÂèñÊàêÊú¨È´òÊòÇËøô‰∏ÄÂÖ≥ÈîÆÁì∂È¢àÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫KineMICÔºàÊÉÖÂ¢É‰∏≠ÁöÑËøêÂä®ÊåñÊéòÔºâÁöÑËøÅÁßªÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁî®‰∫éÂ∞ëÊ†∑Êú¨Âä®‰ΩúÂêàÊàê„ÄÇKineMICÈÄöËøáÂÅáËÆæÊñáÊú¨ÁºñÁ†ÅÁ©∫Èó¥‰∏≠ÁöÑËØ≠‰πâÂØπÂ∫îÂÖ≥Á≥ªÂèØ‰ª•‰∏∫ËøêÂä®Â≠¶Ëí∏È¶èÊèê‰æõËΩØÁõëÁù£Ôºå‰ªéËÄåÂ∞ÜÊñáÊú¨Âà∞Âä®‰Ωú(T2M)Êâ©Êï£Ê®°ÂûãÈÄÇÈÖçÂà∞HARÈ¢ÜÂüü„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈÄöËøá‰∏ÄÁßçËøêÂä®ÊåñÊéòÁ≠ñÁï•ÔºåÂà©Áî®CLIPÊñáÊú¨ÂµåÂÖ•Êù•Âª∫Á´ãÁ®ÄÁñèHARÊ†áÁ≠æÂíåT2MÊ∫êÊï∞ÊçÆ‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇËØ•ËøáÁ®ãÊåáÂØºÂæÆË∞ÉÔºåÂ∞ÜÈÄöÁî®T2MÈ™®Âπ≤ÁΩëÁªúËΩ¨Êç¢‰∏∫‰∏ìÈó®ÁöÑÂ∞ëÊ†∑Êú¨Âä®‰ΩúÂà∞ËøêÂä®ÁîüÊàêÂô®„ÄÇÂú®HumanML3DÔºàÊ∫êT2MÊï∞ÊçÆÈõÜÔºâÂíåNTU RGB+D 120Â≠êÈõÜÔºàÁõÆÊ†áHARÈ¢ÜÂüüÔºâ‰∏äÈ™åËØÅ‰∫ÜKineMICÔºåÊØè‰∏™Âä®‰ΩúÁ±ªÂà´‰ªÖÈöèÊú∫ÈÄâÊã©10‰∏™Ê†∑Êú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÁîüÊàê‰∫ÜÊõ¥ËøûË¥ØÁöÑÂä®‰ΩúÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Âº∫Â§ßÁöÑÊï∞ÊçÆÂ¢ûÂº∫Êù•Ê∫êÔºåÂÆûÁé∞‰∫Ü+23.1%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÈ™®È™ºÁöÑHARÊñπÊ≥ï‰∏•Èáç‰æùËµñ‰∫éÂ§ßËßÑÊ®°„ÄÅÂ∏¶Ê†áÊ≥®ÁöÑËøêÂä®Êï∞ÊçÆÈõÜÔºåËÄåËøô‰∫õÊï∞ÊçÆÈõÜÁöÑËé∑ÂèñÊàêÊú¨ÈùûÂ∏∏È´òÊòÇ„ÄÇËôΩÁÑ∂ÊñáÊú¨Âà∞Âä®‰Ωú(T2M)ÁîüÊàêÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÂêàÊàêÊï∞ÊçÆÊù•Ê∫êÔºå‰ΩÜÂÖ∂ËÆ≠ÁªÉÁõÆÊ†á‰æßÈáç‰∫éÈÄöÁî®ÁöÑËâ∫ÊúØÊÄßËøêÂä®ÔºåÂπ∂‰∏îÊï∞ÊçÆÈõÜÁªìÊûÑ‰∏éHARÂØπËøêÂä®Â≠¶Á≤æÁ°Æ„ÄÅÁ±ªÂà´Âå∫ÂàÜÊÄßÂä®‰ΩúÁöÑË¶ÅÊ±ÇÂ≠òÂú®Ê†πÊú¨Â∑ÆÂºÇÔºåÂØºËá¥È¢ÜÂüüÈ∏øÊ≤ü„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöKineMICÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®T2MÊ®°Âûã‰∏≠ÊñáÊú¨ÁºñÁ†ÅÁ©∫Èó¥‰∏≠Ëï¥Âê´ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÈÄöËøáËøÅÁßªÂ≠¶‰π†ÁöÑÊñπÂºèÔºåÂ∞ÜÈÄöÁî®ÁöÑT2MÊ®°ÂûãÈÄÇÈÖçÂà∞ÁâπÂÆöÁöÑHARÈ¢ÜÂüü„ÄÇÂÆÉÂÅáËÆæÊñáÊú¨ÁºñÁ†ÅÁ©∫Èó¥‰∏≠ÁöÑËØ≠‰πâÂØπÂ∫îÂÖ≥Á≥ªÂèØ‰ª•‰∏∫ËøêÂä®Â≠¶Ëí∏È¶èÊèê‰æõËΩØÁõëÁù£Ôºå‰ªéËÄåÊåáÂØºT2MÊ®°ÂûãÁîüÊàêÊõ¥ÈÄÇÂêàHAR‰ªªÂä°ÁöÑÂä®‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöKineMICÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) Âà©Áî®CLIPÊ®°ÂûãÊèêÂèñHARÊ†áÁ≠æÂíåT2MÊï∞ÊçÆÁöÑÊñáÊú¨ÂµåÂÖ•Ôºõ2) ÈÄöËøáËøêÂä®ÊåñÊéòÁ≠ñÁï•ÔºåÂª∫Á´ãHARÊ†áÁ≠æÂíåT2MÊï∞ÊçÆ‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåÂç≥ÊâæÂà∞‰∏éHARÊ†áÁ≠æËØ≠‰πâÊúÄÁõ∏ÂÖ≥ÁöÑT2MÂä®‰ΩúÔºõ3) ‰ΩøÁî®Ëøô‰∫õÂØπÂ∫îÂÖ≥Á≥ª‰Ωú‰∏∫ËΩØÁõëÁù£ÔºåÂØπT2MÊâ©Êï£Ê®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üÁîüÊàêÁ¨¶ÂêàHAR‰ªªÂä°ÈúÄÊ±ÇÁöÑÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöKineMICÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜËøêÂä®ÊåñÊéòÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•Âà©Áî®CLIPÊñáÊú¨ÂµåÂÖ•Êù•Âª∫Á´ãÁ®ÄÁñèHARÊ†áÁ≠æÂíåT2MÊ∫êÊï∞ÊçÆ‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®T2MÊ®°Âûã‰∏≠Ëï¥Âê´ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂ∞ÜÂÖ∂ËøÅÁßªÂà∞HARÈ¢ÜÂüüÔºå‰ªéËÄåËß£ÂÜ≥‰∫ÜHARÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇ‰∏éÁõ¥Êé•‰ΩøÁî®T2MÊ®°ÂûãÁîüÊàêÂä®‰Ωú‰∏çÂêåÔºåKineMICÈÄöËøáËøêÂä®ÊåñÊéòÂíåÂæÆË∞ÉÔºå‰ΩøÂæóÁîüÊàêÁöÑÂä®‰ΩúÊõ¥ÂÖ∑ËøêÂä®Â≠¶Á≤æÁ°ÆÊÄßÂíåÁ±ªÂà´Âå∫ÂàÜÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöKineMICÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®CLIPÊ®°ÂûãÊèêÂèñÊñáÊú¨ÂµåÂÖ•Ôºå‰ª•ÊçïÊçâHARÊ†áÁ≠æÂíåT2MÊï∞ÊçÆ‰πãÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºõ2) ËÆæËÆ°ËøêÂä®ÊåñÊéòÁ≠ñÁï•ÔºåÈÄöËøáËÆ°ÁÆóÊñáÊú¨ÂµåÂÖ•‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºåÊâæÂà∞‰∏éHARÊ†áÁ≠æÊúÄÁõ∏ÂÖ≥ÁöÑT2MÂä®‰ΩúÔºõ3) ‰ΩøÁî®Êâ©Êï£Ê®°Âûã‰Ωú‰∏∫T2MÈ™®Âπ≤ÁΩëÁªúÔºåÂπ∂‰ΩøÁî®ËøêÂä®ÊåñÊéòÁöÑÁªìÊûú‰Ωú‰∏∫ËΩØÁõëÁù£‰ø°Âè∑ÔºåÂØπÊâ©Êï£Ê®°ÂûãËøõË°åÂæÆË∞É„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂèØËÉΩÂåÖÊã¨ÈáçÂª∫ÊçüÂ§±„ÄÅÂØπÊäóÊçüÂ§±‰ª•ÂèäÂü∫‰∫éËøêÂä®Â≠¶Á∫¶ÊùüÁöÑÊçüÂ§±È°π„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

KineMICÂú®NTU RGB+D 120Êï∞ÊçÆÈõÜÁöÑÂ≠êÈõÜ‰∏äËøõË°å‰∫ÜÈ™åËØÅÔºåÊØè‰∏™Âä®‰ΩúÁ±ªÂà´‰ªÖ‰ΩøÁî®10‰∏™Ê†∑Êú¨ËøõË°åËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKineMICËÉΩÂ§üÁîüÊàêÊõ¥ËøûË¥ØÁöÑÂä®‰ΩúÔºåÂπ∂ÊòæËëóÊèêÈ´ò‰∫ÜHARÁöÑÂáÜÁ°ÆÁéáÔºåÁõ∏ÊØî‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂÆûÁé∞‰∫Ü+23.1%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËøôË°®ÊòéKineMICÂú®Â∞ëÊ†∑Êú¨ÊÉÖÂÜµ‰∏ãÂÖ∑ÊúâÂº∫Â§ßÁöÑÂä®‰ΩúÂêàÊàêËÉΩÂäõÔºåËÉΩÂ§üÊúâÊïàËß£ÂÜ≥HARÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

KineMICÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂú®Êô∫ËÉΩÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂ∫∑Â§çËÆ≠ÁªÉÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁîüÊàêÂêÑÁßç‰∫∫‰ΩìÊ¥ªÂä®Ôºå‰ªéËÄåÊâ©ÂÖÖËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÊèêÈ´òHARÁ≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåKineMICËøòÂèØ‰ª•Áî®‰∫éÁîüÊàêÁâπÂÆöÂú∫ÊôØ‰∏ãÁöÑÂä®‰ΩúÔºå‰æãÂ¶ÇÊ®°ÊãüËÄÅÂπ¥‰∫∫Ë∑åÂÄíÔºåÂ∏ÆÂä©ËØÑ‰º∞ÂíåÊîπËøõÂÆâÂÖ®Êé™ÊñΩ„ÄÇËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®HARÊäÄÊúØÁöÑÂèëÂ±ïÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞Â∫îÁî®‰∫éÂÆûÈôÖÂú∫ÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).

