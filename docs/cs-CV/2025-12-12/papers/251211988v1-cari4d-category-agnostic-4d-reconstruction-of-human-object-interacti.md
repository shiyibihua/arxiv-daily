---
layout: default
title: CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction
---

# CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.11988" target="_blank" class="toolbar-btn">arXiv: 2512.11988v1</a>
    <a href="https://arxiv.org/pdf/2512.11988.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11988v1" 
            onclick="toggleFavorite(this, '2512.11988v1', 'CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xianghui Xie, Bowen Wen, Yan Chang, Hesam Rabeti, Jiefeng Li, Ye Yuan, Gerard Pons-Moll, Stan Birchfield

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-12

**Â§áÊ≥®**: 14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**CARI4DÔºöÊèêÂá∫‰∏ÄÁßçÁ±ªÂà´Êó†ÂÖ≥ÁöÑ4D‰∫∫-Áâ©‰∫§‰∫íÈáçÂª∫ÊñπÊ≥ïÔºåËß£ÂÜ≥ÂçïÁõÆRGBËßÜÈ¢ëÈáçÂª∫ÈöæÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `4DÈáçÂª∫` `‰∫∫-Áâ©‰∫§‰∫í` `Á±ªÂà´Êó†ÂÖ≥` `ÂçïÁõÆËßÜËßâ` `Ê∏≤Êüì-ÊØîËæÉ` `Áâ©ÁêÜÁ∫¶Êùü` `Âü∫Á°ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®4D‰∫∫-Áâ©‰∫§‰∫íÈáçÂª∫‰∏≠Ôºå‰æùËµñÁâ©‰ΩìÊ®°ÊùøÊàñÈôêÂà∂Áâ©‰ΩìÁ±ªÂà´ÔºåÈöæ‰ª•Â§ÑÁêÜÁúüÂÆûÂú∫ÊôØÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇ
2. CARI4DÈÄöËøáÊï¥ÂêàÂü∫Á°ÄÊ®°ÂûãÁöÑÈ¢ÑÊµãÔºåÂπ∂Âà©Áî®Ê∏≤Êüì-ÊØîËæÉËåÉ‰æãËøõË°åËÅîÂêà‰ºòÂåñÔºåÂÆûÁé∞Á©∫Èó¥„ÄÅÊó∂Èó¥ÂíåÂÉèÁ¥†Á∫ßÂà´ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCARI4DÂú®ÈáçÂª∫Á≤æÂ∫¶‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊäÄÊúØÔºåÂπ∂Âú®Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫CARI4DÔºå‰∏ÄÁßçÁ±ªÂà´Êó†ÂÖ≥ÁöÑÊñπÊ≥ïÔºåÁî®‰∫é‰ªéÂçïÁõÆRGBËßÜÈ¢ë‰∏≠‰ª•Â∫¶ÈáèÂ∞∫Â∫¶ÈáçÂª∫Á©∫Èó¥ÂíåÊó∂Èó¥‰∏ä‰∏ÄËá¥ÁöÑ4D‰∫∫-Áâ©‰∫§‰∫í„ÄÇÁî±‰∫éÊú™Áü•Áâ©‰ΩìÂíå‰∫∫‰Ωì‰ø°ÊÅØ„ÄÅÊ∑±Â∫¶Ê®°Á≥ä„ÄÅÈÅÆÊå°ÂíåÂ§çÊùÇËøêÂä®Ôºå‰ªéÂçï‰∏™RGBËßÜÂõæÊé®Êñ≠4D‰∫§‰∫íÊûÅÂÖ∑ÊåëÊàòÊÄßÔºåÈòªÁ¢ç‰∫Ü‰∏ÄËá¥ÁöÑ3DÂíåÊó∂Èó¥ÈáçÂª∫„ÄÇÂÖàÂâçÁöÑÊñπÊ≥ïÈÄöËøáÂÅáËÆæground truthÁâ©‰ΩìÊ®°ÊùøÊàñÈôêÂà∂‰∫éÊúâÈôêÁöÑÁâ©‰ΩìÁ±ªÂà´Êù•ÁÆÄÂåñËÆæÁΩÆ„ÄÇCARI4DÈÄöËøáÁ®≥ÂÅ•Âú∞Êï¥ÂêàÊù•Ëá™Âü∫Á°ÄÊ®°ÂûãÁöÑ‰∏™‰ΩìÈ¢ÑÊµãÔºåÂπ∂ÈÄöËøáÂ≠¶‰π†Âà∞ÁöÑÊ∏≤Êüì-ÊØîËæÉËåÉ‰æãËÅîÂêàÁªÜÂåñÂÆÉ‰ª¨Ôºå‰ª•Á°Æ‰øùÁ©∫Èó¥„ÄÅÊó∂Èó¥ÂíåÂÉèÁ¥†ÂØπÈΩêÔºåÊúÄÂêéÊé®ÁêÜÂ§çÊùÇÁöÑÊé•Ëß¶‰ª•Ëøõ‰∏ÄÊ≠•ÁªÜÂåñÔºå‰ªéËÄåÊª°Ë∂≥Áâ©ÁêÜÁ∫¶Êùü„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂêåÂàÜÂ∏ÉÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÊäÄÊúØ38%ÔºåÂú®Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÊäÄÊúØ36%„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂèØ‰ª•Ê≥õÂåñÂà∞ËÆ≠ÁªÉÁ±ªÂà´‰πãÂ§ñÔºåÂõ†Ê≠§ÂèØ‰ª•Èõ∂Ê†∑Êú¨Â∫îÁî®‰∫éÈáéÂ§ñ‰∫íËÅîÁΩëËßÜÈ¢ë„ÄÇ‰ª£Á†ÅÂíåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®ÂçïÁõÆRGBËßÜÈ¢ë‰∏≠ÈáçÂª∫4D‰∫∫-Áâ©‰∫§‰∫íÊó∂ÔºåÈù¢‰∏¥Áâ©‰ΩìÁ±ªÂà´Êú™Áü•„ÄÅÊ∑±Â∫¶Ê®°Á≥ä„ÄÅÈÅÆÊå°‰ª•ÂèäÂ§çÊùÇËøêÂä®Á≠âÊåëÊàòÔºåÂØºËá¥ÈáçÂª∫ÁªìÊûúÂú®Á©∫Èó¥ÂíåÊó∂Èó¥‰∏ä‰∏ç‰∏ÄËá¥„ÄÇ‰πãÂâçÁöÑÁ†îÁ©∂ÈÄöÂ∏∏‰æùËµñ‰∫éÂ∑≤Áü•ÁöÑÁâ©‰ΩìÊ®°ÊùøÊàñËÄÖÂ∞ÜÁâ©‰ΩìÁ±ªÂà´ÈôêÂà∂Âú®‰∏Ä‰∏™ËæÉÂ∞èÁöÑÈõÜÂêàÂÜÖÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®ÁúüÂÆû‰∏ñÁïåÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCARI4DÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂü∫Á°ÄÊ®°ÂûãÊèê‰æõÂàùÂßãÁöÑ‰∫∫‰ΩìÂíåÁâ©‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÔºåÁÑ∂ÂêéÈÄöËøá‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊ∏≤Êüì-ÊØîËæÉÊ°ÜÊû∂ÔºåÂØπËøô‰∫õ‰º∞ËÆ°ËøõË°åËÅîÂêà‰ºòÂåñÔºå‰ª•Á°Æ‰øùÈáçÂª∫ÁªìÊûúÂú®Á©∫Èó¥„ÄÅÊó∂Èó¥ÂíåÂÉèÁ¥†Á∫ßÂà´‰∏äÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãËøòÊòæÂºèÂú∞Êé®ÁêÜ‰∫∫‰∏éÁâ©‰Ωì‰πãÈó¥ÁöÑÊé•Ëß¶ÂÖ≥Á≥ªÔºåÂπ∂Âà©Áî®Áâ©ÁêÜÁ∫¶ÊùüËøõ‰∏ÄÊ≠•ÊèêÂçáÈáçÂª∫Ë¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCARI4DÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) **ÂßøÊÄÅÂÅáËÆæÁîüÊàê**ÔºöÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂü∫Á°ÄÊ®°ÂûãÔºàÂ¶Ç‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Âô®ÂíåÁâ©‰ΩìÊ£ÄÊµãÂô®ÔºâÁîüÊàêÂàùÂßãÁöÑ‰∫∫‰ΩìÂíåÁâ©‰ΩìÂßøÊÄÅÂÅáËÆæ„ÄÇ2) **ËÅîÂêà‰ºòÂåñ**ÔºöÈÄöËøá‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊ∏≤Êüì-ÊØîËæÉÊ°ÜÊû∂ÔºåÂØπ‰∫∫‰ΩìÂíåÁâ©‰ΩìÁöÑÂßøÊÄÅËøõË°åËÅîÂêà‰ºòÂåñ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊ∏≤ÊüìÈáçÂª∫ÁªìÊûúÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÂéüÂßãÂõæÂÉèËøõË°åÊØîËæÉÔºåËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞Ôºå‰ªéËÄåÈ©±Âä®ÂßøÊÄÅÁöÑ‰ºòÂåñ„ÄÇ3) **Êé•Ëß¶Êé®ÁêÜ**ÔºöÊòæÂºèÂú∞Êé®ÁêÜ‰∫∫‰∏éÁâ©‰Ωì‰πãÈó¥ÁöÑÊé•Ëß¶ÂÖ≥Á≥ªÔºåÂπ∂Âà©Áî®Áâ©ÁêÜÁ∫¶ÊùüËøõ‰∏ÄÊ≠•ÊèêÂçáÈáçÂª∫Ë¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCARI4DÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Á±ªÂà´Êó†ÂÖ≥ÁöÑÈáçÂª∫ËÉΩÂäõÂíåÁ´ØÂà∞Á´ØÁöÑ‰ºòÂåñÊ°ÜÊû∂„ÄÇ‰∏é‰ª•ÂæÄ‰æùËµñÁâ©‰ΩìÊ®°ÊùøÊàñÈôêÂà∂Áâ©‰ΩìÁ±ªÂà´ÁöÑÊñπÊ≥ï‰∏çÂêåÔºåCARI4DÂèØ‰ª•Â§ÑÁêÜ‰ªªÊÑèÁ±ªÂà´ÁöÑÁâ©‰ΩìÔºå‰ªéËÄåÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåCARI4DÈÄöËøáÁ´ØÂà∞Á´ØÁöÑ‰ºòÂåñÊ°ÜÊû∂ÔºåÂ∞Ü‰∫∫‰ΩìÂíåÁâ©‰ΩìÁöÑÂßøÊÄÅ‰º∞ËÆ°„ÄÅÊ∏≤ÊüìÂíåÊØîËæÉ‰ª•ÂèäÊé•Ëß¶Êé®ÁêÜÊï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÂíå‰∏ÄËá¥ÁöÑÈáçÂª∫ÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê∏≤Êüì-ÊØîËæÉÊ°ÜÊû∂‰∏≠Ôºå‰ΩøÁî®‰∫ÜÂèØÂæÆÂàÜÊ∏≤ÊüìÂô®ÔºåÂÖÅËÆ∏Ê¢ØÂ∫¶‰ªéÂÉèÁ¥†Á©∫Èó¥ÂèçÂêë‰º†Êí≠Âà∞ÂßøÊÄÅÂèÇÊï∞„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂÉèÁ¥†Á∫ßÂà´ÁöÑÂõæÂÉèÈáçÂª∫ÊçüÂ§±„ÄÅ3DÂá†‰Ωï‰∏ÄËá¥ÊÄßÊçüÂ§±ÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄßÊçüÂ§±„ÄÇÊ≠§Â§ñÔºåËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™Êé•Ëß¶ÊçüÂ§±ÔºåÁî®‰∫éÈºìÂä±Ê®°ÂûãÂ≠¶‰π†‰∫∫‰∏éÁâ©‰Ωì‰πãÈó¥ÁöÑÂêàÁêÜÊé•Ëß¶ÂÖ≥Á≥ª„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢Ôºå‰ΩøÁî®‰∫ÜTransformerÁΩëÁªúÊù•Âª∫Ê®°‰∫∫‰ΩìÂíåÁâ©‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂Âà©Áî®ÂõæÁ•ûÁªèÁΩëÁªúÊù•Êé®ÁêÜÊé•Ëß¶ÂÖ≥Á≥ª„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CARI4DÂú®ÂêåÂàÜÂ∏ÉÊï∞ÊçÆÈõÜ‰∏äÁõ∏ÊØîÁé∞ÊúâÊäÄÊúØÊèêÂçá‰∫Ü38%ÁöÑÈáçÂª∫Á≤æÂ∫¶ÔºåÂú®Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏äÊèêÂçá‰∫Ü36%„ÄÇËøôË°®ÊòéCARI4D‰∏ç‰ªÖÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËÄå‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÂ§ÑÁêÜÂêÑÁßçÁúüÂÆû‰∏ñÁïåÁöÑÂú∫ÊôØ„ÄÇËØ•Ê®°ÂûãËøòËÉΩÂ§üÈõ∂Ê†∑Êú¨Â∫îÁî®‰∫é‰∫íËÅîÁΩëËßÜÈ¢ëÔºåÊó†ÈúÄÈíàÂØπÁâπÂÆöÂú∫ÊôØËøõË°åËÆ≠ÁªÉ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CARI4DÂú®‰∫∫Êú∫‰∫§‰∫í„ÄÅÊ∏∏Êàè„ÄÅÊú∫Âô®‰∫∫Â≠¶‰π†Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Áî®‰∫éÂàõÂª∫Êõ¥ÈÄºÁúüÂíåËá™ÁÑ∂ÁöÑËôöÊãüÁé∞ÂÆû‰ΩìÈ™åÔºåËÆ≠ÁªÉÊú∫Âô®‰∫∫ËøõË°åÂ§çÊùÇÁöÑ‰∫∫-Áâ©‰∫§‰∫í‰ªªÂä°Ôºå‰ª•ÂèäÂàÜÊûê‰∫∫Á±ªË°å‰∏∫ÂíåÂßøÊÄÅ„ÄÇËØ•Á†îÁ©∂ÁöÑÁ™ÅÁ†¥‰∏∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂÖ∑ÈÄÇÂ∫îÊÄßÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÈì∫Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

