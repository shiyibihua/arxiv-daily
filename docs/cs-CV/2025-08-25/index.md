---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-25
---

# cs.CVï¼ˆ2025-08-25ï¼‰

ğŸ“Š å…± **34** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (12 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250817916v1-endoufm-utilizing-foundation-models-for-monocular-depth-estimation-o.html">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a></td>
  <td>æå‡ºEndoUFMä»¥è§£å†³å†…çª¥é•œå›¾åƒå•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17916v1" data-paper-url="./papers/250817916v1-endoufm-utilizing-foundation-models-for-monocular-depth-estimation-o.html" onclick="toggleFavorite(this, '2508.17916v1', 'EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250818389v2-fastavatar-instant-3d-gaussian-splatting-for-faces-from-single-uncon.html">FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses</a></td>
  <td>æå‡ºFastAvatarä»¥è§£å†³å•å›¾3Däººè„¸é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18389v2" data-paper-url="./papers/250818389v2-fastavatar-instant-3d-gaussian-splatting-for-faces-from-single-uncon.html" onclick="toggleFavorite(this, '2508.18389v2', 'FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250817876v1-camera-pose-refinement-via-3d-gaussian-splatting.html">Camera Pose Refinement via 3D Gaussian Splatting</a></td>
  <td>æå‡ºGS-SMCä»¥è§£å†³ç›¸æœºå§¿æ€ç²¾ç¡®åº¦ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17876v1" data-paper-url="./papers/250817876v1-camera-pose-refinement-via-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.17876v1', 'Camera Pose Refinement via 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250818242v1-gsvisloc-generalizable-visual-localization-for-gaussian-splatting-sc.html">GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations</a></td>
  <td>æå‡ºGSVisLocä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘åœºæ™¯å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18242v1" data-paper-url="./papers/250818242v1-gsvisloc-generalizable-visual-localization-for-gaussian-splatting-sc.html" onclick="toggleFavorite(this, '2508.18242v1', 'GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250818050v1-arguscogito-chain-of-thought-for-cross-modal-synergy-and-omnidirecti.html">ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation</a></td>
  <td>æå‡ºArgusCogitoä»¥è§£å†³ä¼ªè£…ç‰©ä½“åˆ†å‰²ä¸­çš„è®¤çŸ¥æ·±åº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">semantic map</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18050v1" data-paper-url="./papers/250818050v1-arguscogito-chain-of-thought-for-cross-modal-synergy-and-omnidirecti.html" onclick="toggleFavorite(this, '2508.18050v1', 'ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250817579v1-idu-incremental-dynamic-update-of-existing-3d-virtual-environments-w.html">IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data</a></td>
  <td>æå‡ºå¢é‡åŠ¨æ€æ›´æ–°æ–¹æ³•ä»¥é«˜æ•ˆç»´æŠ¤3Dè™šæ‹Ÿç¯å¢ƒ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17579v1" data-paper-url="./papers/250817579v1-idu-incremental-dynamic-update-of-existing-3d-virtual-environments-w.html" onclick="toggleFavorite(this, '2508.17579v1', 'IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250817832v2-hlg-comprehensive-3d-room-construction-via-hierarchical-layout-gener.html">HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</a></td>
  <td>æå‡ºå±‚æ¬¡å¸ƒå±€ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³ç»†ç²’åº¦3Dåœºæ™¯ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">physically plausible</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17832v2" data-paper-url="./papers/250817832v2-hlg-comprehensive-3d-room-construction-via-hierarchical-layout-gener.html" onclick="toggleFavorite(this, '2508.17832v2', 'HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250818506v1-dogflow-self-supervised-lidar-scene-flow-via-cross-modal-doppler-gui.html">DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance</a></td>
  <td>æå‡ºDoGFlowä»¥è§£å†³è‡ªç›‘ç£LiDARåœºæ™¯æµä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18506v1" data-paper-url="./papers/250818506v1-dogflow-self-supervised-lidar-scene-flow-via-cross-modal-doppler-gui.html" onclick="toggleFavorite(this, '2508.18506v1', 'DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250818539v2-adaptive-visual-navigation-assistant-in-3d-rpgs.html">Adaptive Visual Navigation Assistant in 3D RPGs</a></td>
  <td>æå‡ºè‡ªé€‚åº”è§†è§‰å¯¼èˆªåŠ©æ‰‹ä»¥è§£å†³3D RPGæ¸¸æˆä¸­çš„å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18539v2" data-paper-url="./papers/250818539v2-adaptive-visual-navigation-assistant-in-3d-rpgs.html" onclick="toggleFavorite(this, '2508.18539v2', 'Adaptive Visual Navigation Assistant in 3D RPGs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250817972v1-sail-recon-large-sfm-by-augmenting-scene-regression-with-localizatio.html">SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</a></td>
  <td>æå‡ºSAIL-Reconä»¥è§£å†³å¤§è§„æ¨¡SfMé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VGGT</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17972v1" data-paper-url="./papers/250817972v1-sail-recon-large-sfm-by-augmenting-scene-regression-with-localizatio.html" onclick="toggleFavorite(this, '2508.17972v1', 'SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250900056v2-mesti-meganet-micro-expression-spatio-temporal-image-and-micro-expre.html">MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition</a></td>
  <td>æå‡ºMESTI-MEGANetä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00056v2" data-paper-url="./papers/250900056v2-mesti-meganet-micro-expression-spatio-temporal-image-and-micro-expre.html" onclick="toggleFavorite(this, '2509.00056v2', 'MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250817712v1-ngd-neural-gradient-based-deformation-for-monocular-garment-reconstr.html">NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction</a></td>
  <td>æå‡ºNGDæ–¹æ³•ä»¥è§£å†³å•ç›®è§†é¢‘æœè£…é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">implicit representation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17712v1" data-paper-url="./papers/250817712v1-ngd-neural-gradient-based-deformation-for-monocular-garment-reconstr.html" onclick="toggleFavorite(this, '2508.17712v1', 'NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250817860v1-avam-universal-training-free-adaptive-visual-anchoring-embedded-into.html">AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering</a></td>
  <td>æå‡ºè‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥ä»¥è§£å†³å¤šå›¾åƒé—®ç­”ä¸­çš„è§†è§‰å†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17860v1" data-paper-url="./papers/250817860v1-avam-universal-training-free-adaptive-visual-anchoring-embedded-into.html" onclick="toggleFavorite(this, '2508.17860v1', 'AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250817890v1-uniapo-unified-multimodal-automated-prompt-optimization.html">UniAPO: Unified Multimodal Automated Prompt Optimization</a></td>
  <td>æå‡ºUniAPOä»¥è§£å†³å¤šæ¨¡æ€è‡ªåŠ¨æç¤ºä¼˜åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17890v1" data-paper-url="./papers/250817890v1-uniapo-unified-multimodal-automated-prompt-optimization.html" onclick="toggleFavorite(this, '2508.17890v1', 'UniAPO: Unified Multimodal Automated Prompt Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250819298v1-demobias-an-empirical-study-to-trace-demographic-biases-in-vision-fo.html">DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models</a></td>
  <td>æå‡ºDemoBiasä»¥è¿½è¸ªè§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„äººå£ç»Ÿè®¡åè§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19298v1" data-paper-url="./papers/250819298v1-demobias-an-empirical-study-to-trace-demographic-biases-in-vision-fo.html" onclick="toggleFavorite(this, '2508.19298v1', 'DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250818264v1-mmtok-multimodal-coverage-maximization-for-efficient-inference-of-vl.html">MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</a></td>
  <td>æå‡ºMMTokä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„å†—ä½™æ¨ç†æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18264v1" data-paper-url="./papers/250818264v1-mmtok-multimodal-coverage-maximization-for-efficient-inference-of-vl.html" onclick="toggleFavorite(this, '2508.18264v1', 'MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250819294v2-object-detection-with-multimodal-large-vision-language-models-an-in-.html">Object Detection with Multimodal Large Vision-Language Models: An In-depth Review</a></td>
  <td>ç»¼è¿°å¤šæ¨¡æ€å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“æ£€æµ‹ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19294v2" data-paper-url="./papers/250819294v2-object-detection-with-multimodal-large-vision-language-models-an-in-.html" onclick="toggleFavorite(this, '2508.19294v2', 'Object Detection with Multimodal Large Vision-Language Models: An In-depth Review')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250817760v1-ceidm-a-controlled-entity-and-interaction-diffusion-model-for-enhanc.html">CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</a></td>
  <td>æå‡ºCEIDMä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å®ä½“ä¸äº¤äº’æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17760v1" data-paper-url="./papers/250817760v1-ceidm-a-controlled-entity-and-interaction-diffusion-model-for-enhanc.html" onclick="toggleFavorite(this, '2508.17760v1', 'CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250817718v1-instant-preference-alignment-for-text-to-image-diffusion-models.html">Instant Preference Alignment for Text-to-Image Diffusion Models</a></td>
  <td>æå‡ºå³æ—¶åå¥½å¯¹é½æ¡†æ¶ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17718v1" data-paper-url="./papers/250817718v1-instant-preference-alignment-for-text-to-image-diffusion-models.html" onclick="toggleFavorite(this, '2508.17718v1', 'Instant Preference Alignment for Text-to-Image Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250818177v1-scene-aware-vectorized-memory-multi-agent-framework-with-cross-modal.html">Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance</a></td>
  <td>æå‡ºè·¨æ¨¡æ€å·®å¼‚åŒ–é‡åŒ–æ¡†æ¶ä»¥è§£å†³è§†è§‰éšœç¢è¾…åŠ©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18177v1" data-paper-url="./papers/250818177v1-scene-aware-vectorized-memory-multi-agent-framework-with-cross-modal.html" onclick="toggleFavorite(this, '2508.18177v1', 'Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250819289v1-seeing-like-a-designer-without-one-a-study-on-unsupervised-slide-qua.html">Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation</a></td>
  <td>æå‡ºæ— ç›‘ç£å¹»ç¯ç‰‡è´¨é‡è¯„ä¼°æ–¹æ³•ä»¥æå‡è®¾è®¡åé¦ˆ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19289v1" data-paper-url="./papers/250819289v1-seeing-like-a-designer-without-one-a-study-on-unsupervised-slide-qua.html" onclick="toggleFavorite(this, '2508.19289v1', 'Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250817857v1-visa-group-wise-visual-token-selection-and-aggregation-via-graph-sum.html">VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference</a></td>
  <td>æå‡ºVISAä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17857v1" data-paper-url="./papers/250817857v1-visa-group-wise-visual-token-selection-and-aggregation-via-graph-sum.html" onclick="toggleFavorite(this, '2508.17857v1', 'VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250817816v1-unisino-physics-driven-foundational-model-for-universal-ct-sinogram-.html">UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</a></td>
  <td>æå‡ºUniSinoä»¥è§£å†³CTæˆåƒä¸­æ ‡å‡†åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17816v1" data-paper-url="./papers/250817816v1-unisino-physics-driven-foundational-model-for-universal-ct-sinogram-.html" onclick="toggleFavorite(this, '2508.17816v1', 'UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250818067v1-annotation-free-open-vocabulary-segmentation-for-remote-sensing-imag.html">Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</a></td>
  <td>æå‡ºSegEarth-OVä»¥è§£å†³é¥æ„Ÿå›¾åƒçš„æ— æ³¨é‡Šå¼€æ”¾è¯æ±‡åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18067v1" data-paper-url="./papers/250818067v1-annotation-free-open-vocabulary-segmentation-for-remote-sensing-imag.html" onclick="toggleFavorite(this, '2508.18067v1', 'Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250818265v2-internvl35-advancing-open-source-multimodal-models-in-versatility-re.html">InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</a></td>
  <td>æå‡ºInternVL3.5ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline RL</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18265v2" data-paper-url="./papers/250818265v2-internvl35-advancing-open-source-multimodal-models-in-versatility-re.html" onclick="toggleFavorite(this, '2508.18265v2', 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250818463v2-context-aware-zero-shot-anomaly-detection-in-surveillance-using-cont.html">Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</a></td>
  <td>æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ¡†æ¶ä»¥è§£å†³ç›‘æ§è§†é¢‘ä¸­çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">predictive model</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18463v2" data-paper-url="./papers/250818463v2-context-aware-zero-shot-anomaly-detection-in-surveillance-using-cont.html" onclick="toggleFavorite(this, '2508.18463v2', 'Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250818032v2-visual-cog-stage-aware-reinforcement-learning-with-chain-of-guidance.html">Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation</a></td>
  <td>æå‡ºVisual-CoGä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18032v2" data-paper-url="./papers/250818032v2-visual-cog-stage-aware-reinforcement-learning-with-chain-of-guidance.html" onclick="toggleFavorite(this, '2508.18032v2', 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250817726v1-few-shot-human-action-anomaly-detection-via-a-unified-contrastive-le.html">Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework</a></td>
  <td>æå‡ºç»Ÿä¸€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å°‘æ ·æœ¬äººç±»åŠ¨ä½œå¼‚å¸¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17726v1" data-paper-url="./papers/250817726v1-few-shot-human-action-anomaly-detection-via-a-unified-contrastive-le.html" onclick="toggleFavorite(this, '2508.17726v1', 'Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250818007v1-fence-off-anomaly-interference-cross-domain-distillation-for-fully-u.html">Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection</a></td>
  <td>æå‡ºè·¨åŸŸè’¸é¦æ¡†æ¶ä»¥è§£å†³æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸­çš„å¹²æ‰°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18007v1" data-paper-url="./papers/250818007v1-fence-off-anomaly-interference-cross-domain-distillation-for-fully-u.html" onclick="toggleFavorite(this, '2508.18007v1', 'Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250817714v2-f2rvlm-boosting-fine-grained-fragment-retrieval-for-multi-modal-long.html">F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model</a></td>
  <td>æå‡ºF2RVLMä»¥è§£å†³å¤šæ¨¡æ€é•¿å¯¹è¯ä¸­çš„ç»†ç²’åº¦ç‰‡æ®µæ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17714v2" data-paper-url="./papers/250817714v2-f2rvlm-boosting-fine-grained-fragment-retrieval-for-multi-modal-long.html" onclick="toggleFavorite(this, '2508.17714v2', 'F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250817588v1-hero-hierarchical-extrapolation-and-refresh-for-efficient-world-mode.html">HERO: Hierarchical Extrapolation and Refresh for Efficient World Models</a></td>
  <td>æå‡ºHEROæ¡†æ¶ä»¥è§£å†³ä¸–ç•Œæ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17588v1" data-paper-url="./papers/250817588v1-hero-hierarchical-extrapolation-and-refresh-for-efficient-world-mode.html" onclick="toggleFavorite(this, '2508.17588v1', 'HERO: Hierarchical Extrapolation and Refresh for Efficient World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/250818421v1-why-relational-graphs-will-save-the-next-generation-of-vision-founda.html">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a></td>
  <td>æå‡ºåŠ¨æ€å…³ç³»å›¾ä»¥æå‡è§†è§‰åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">egocentric</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.18421v1" data-paper-url="./papers/250818421v1-why-relational-graphs-will-save-the-next-generation-of-vision-founda.html" onclick="toggleFavorite(this, '2508.18421v1', 'Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250817976v1-propose-and-rectify-a-forensics-driven-mllm-framework-for-image-mani.html">Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization</a></td>
  <td>æå‡ºPropose-Rectifyæ¡†æ¶ä»¥è§£å†³å›¾åƒç¯¡æ”¹å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17976v1" data-paper-url="./papers/250817976v1-propose-and-rectify-a-forensics-driven-mllm-framework-for-image-mani.html" onclick="toggleFavorite(this, '2508.17976v1', 'Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/250817595v1-tinygiantvlm-a-lightweight-vision-language-architecture-for-spatial-.html">TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints</a></td>
  <td>æå‡ºTinyGiantVLMä»¥è§£å†³å·¥ä¸šç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17595v1" data-paper-url="./papers/250817595v1-tinygiantvlm-a-lightweight-vision-language-architecture-for-spatial-.html" onclick="toggleFavorite(this, '2508.17595v1', 'TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)