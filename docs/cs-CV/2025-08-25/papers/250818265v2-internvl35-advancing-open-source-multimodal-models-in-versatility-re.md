---
layout: default
title: InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency
---

# InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18265" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18265v2</a>
  <a href="https://arxiv.org/pdf/2508.18265.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18265v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18265v2', 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-08-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInternVL3.5ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `çº§è”å¼ºåŒ–å­¦ä¹ ` `è§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨` `è§£è€¦éƒ¨ç½²` `å¼€æºæ¨¡å‹` `è®¡ç®—æ•ˆç‡` `æ™ºèƒ½äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºçº§è”å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œè§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ï¼Œä»¥æå‡æ¨ç†èƒ½åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInternVL3.5åœ¨æ¨ç†æ€§èƒ½ä¸Šæå‡16.0%ï¼Œæ¨ç†é€Ÿåº¦æå‡4.05å€ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†InternVL3.5ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¼€æºå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¤šæ ·æ€§ã€æ¨ç†èƒ½åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚å…³é”®åˆ›æ–°æ˜¯çº§è”å¼ºåŒ–å­¦ä¹ ï¼ˆCascade RLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè¿‡ç¨‹å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚è¿™ç§ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥åœ¨ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ï¼ˆå¦‚MMMUå’ŒMathVistaï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸ºä¼˜åŒ–æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ï¼ˆViRï¼‰ï¼ŒåŠ¨æ€è°ƒæ•´è§†è§‰æ ‡è®°çš„åˆ†è¾¨ç‡è€Œä¸å½±å“æ€§èƒ½ã€‚ç»“åˆViRï¼Œæˆ‘ä»¬çš„è§£è€¦è§†è§‰-è¯­è¨€éƒ¨ç½²ï¼ˆDvDï¼‰ç­–ç•¥å°†è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹åˆ†å¸ƒåœ¨ä¸åŒçš„GPUä¸Šï¼Œæœ‰æ•ˆå¹³è¡¡è®¡ç®—è´Ÿè½½ã€‚è¿™äº›è´¡çŒ®ä½¿InternVL3.5åœ¨æ¨ç†æ€§èƒ½ä¸Šæå‡äº†16.0%ï¼Œå¹¶å®ç°äº†4.05å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚æ­¤å¤–ï¼ŒInternVL3.5æ”¯æŒGUIäº¤äº’å’Œå…·èº«æ™ºèƒ½ç­‰æ–°èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æœ€å¤§æ¨¡å‹InternVL3.5-241B-A28Båœ¨å¼€æ”¾æºä»£ç çš„å¤šæ¨¡æ€ã€æ¨ç†ã€æ–‡æœ¬å’Œä»£ç†ä»»åŠ¡ä¸­å–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œç¼©å°äº†ä¸å•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPT-5ï¼‰çš„æ€§èƒ½å·®è·ã€‚æ‰€æœ‰æ¨¡å‹å’Œä»£ç å‡å·²å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¹³è¡¡æ¨ç†ç²¾åº¦ä¸è®¡ç®—æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡çº§è”å¼ºåŒ–å­¦ä¹ æ¡†æ¶æå‡æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°ç¨³å®šæ”¶æ•›å’Œç²¾ç»†å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šçº§è”å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œè§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ã€‚çº§è”å¼ºåŒ–å­¦ä¹ åˆ†ä¸ºç¦»çº¿å’Œåœ¨çº¿ä¸¤ä¸ªé˜¶æ®µï¼Œè§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨åˆ™åŠ¨æ€è°ƒæ•´è§†è§‰æ ‡è®°çš„åˆ†è¾¨ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯çº§è”å¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„å¼•å…¥ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´è§†è§‰æ ‡è®°åˆ†è¾¨ç‡çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡è§£è€¦è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹çš„æ–¹å¼æ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼Œç¡®ä¿åœ¨ä¸åŒGPUä¸Šé«˜æ•ˆè¿è¡Œã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒInternVL3.5åœ¨æ¨ç†æ€§èƒ½ä¸Šæå‡äº†16.0%ï¼Œå¹¶å®ç°äº†4.05å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œç›¸è¾ƒäºå‰ä¸€ç‰ˆæœ¬InternVL3ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å¤šæ¨¡æ€ã€æ¨ç†ã€æ–‡æœ¬å’Œä»£ç†ä»»åŠ¡ä¸­å‡å–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œç¼©å°äº†ä¸å•†ä¸šæ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœã€æ•™è‚²è¾…å¯¼ç­‰å¤šæ¨¡æ€äº¤äº’åœºæ™¯ã€‚é€šè¿‡æå‡æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ï¼ŒInternVL3.5èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„ç”¨æˆ·è¯·æ±‚ï¼Œæä¾›æ›´ä¸ºç²¾å‡†çš„åé¦ˆå’ŒæœåŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning performance and a 4.05$\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.

