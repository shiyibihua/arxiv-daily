---
layout: default
title: EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images
---

# EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17916" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17916v1</a>
  <a href="https://arxiv.org/pdf/2508.17916.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17916v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17916v1', 'EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinning Yao, Bo Liu, Bojian Li, Jingjing Wang, Jinghua Yue, Fugen Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: 12 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEndoUFMä»¥è§£å†³å†…çª¥é•œå›¾åƒå•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `å†…çª¥é•œå›¾åƒ` `åŸºç¡€æ¨¡å‹` `å¾®åˆ›æ‰‹æœ¯` `æ·±åº¦å­¦ä¹ ` `è‡ªé€‚åº”å¾®è°ƒ` `å¹³æ»‘æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨å¤æ‚çš„æ‰‹æœ¯ç¯å¢ƒä¸­å—é™äºå…‰ç…§å˜åŒ–å’Œçº¹ç†å¤æ‚æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºEndoUFMæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåŒé‡åŸºç¡€æ¨¡å‹å’Œè‡ªé€‚åº”å¾®è°ƒç­–ç•¥ï¼Œæå‡å†…çª¥é•œå›¾åƒçš„æ·±åº¦ä¼°è®¡èƒ½åŠ›ã€‚
3. åœ¨SCAREDã€Hamlynã€SERV-CTå’ŒEndoNeRFæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸”æ¨¡å‹è§„æ¨¡é«˜æ•ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¼°è®¡æ˜¯å¾®åˆ›å†…çª¥é•œæ‰‹æœ¯ä¸­3Dé‡å»ºçš„åŸºç¡€ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å•ç›®æ·±åº¦ä¼°è®¡æŠ€æœ¯åœ¨æ‰‹æœ¯ç¯å¢ƒä¸­ç”±äºå…‰ç…§å˜åŒ–å’Œå¤æ‚çº¹ç†çš„å½±å“ï¼Œè¡¨ç°å‡ºæœ‰é™çš„æ€§èƒ½ã€‚å°½ç®¡å¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶åœ¨è‡ªç„¶å›¾åƒä¸Šçš„è®­ç»ƒå¯¼è‡´åœ¨å†…çª¥é•œåº”ç”¨ä¸­å­˜åœ¨æ˜¾è‘—çš„é¢†åŸŸé€‚åº”æ€§é™åˆ¶å’Œè¯­ä¹‰æ„ŸçŸ¥ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºäº†EndoUFMï¼Œä¸€ä¸ªæ— ç›‘ç£çš„å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°æ•´åˆäº†åŒé‡åŸºç¡€æ¨¡å‹ä»¥å¢å¼ºæ‰‹æœ¯åœºæ™¯çš„æ·±åº¦ä¼°è®¡æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†éšæœºå‘é‡ä½ç§©é€‚åº”ï¼ˆRVLoRAï¼‰çš„è‡ªé€‚åº”å¾®è°ƒç­–ç•¥ï¼Œå¹¶åŸºäºæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„æ®‹å·®å—ï¼ˆRes-DSCï¼‰æ¥æ”¹å–„ç»†ç²’åº¦å±€éƒ¨ç‰¹å¾çš„æ•æ‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ©è†œå¼•å¯¼çš„å¹³æ»‘æŸå¤±ï¼Œä»¥å¢å¼ºè§£å‰–ç»„ç»‡ç»“æ„å†…çš„æ·±åº¦ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„æ¨¡å‹è§„æ¨¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å†…çª¥é•œå›¾åƒçš„å•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚çš„æ‰‹æœ¯ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºå…‰ç…§å˜åŒ–å’Œçº¹ç†å¤æ‚æ€§å¯¼è‡´çš„æ€§èƒ½é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEndoUFMæ¡†æ¶é€šè¿‡æ•´åˆåŒé‡åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨é¢„å­¦ä¹ çš„å…ˆéªŒçŸ¥è¯†æ¥å¢å¼ºæ·±åº¦ä¼°è®¡æ€§èƒ½ï¼ŒåŒæ—¶é‡‡ç”¨è‡ªé€‚åº”å¾®è°ƒç­–ç•¥ä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºåŒé‡åŸºç¡€æ¨¡å‹çš„æ·±åº¦ä¼°è®¡æ¨¡å—ï¼Œå…¶æ¬¡æ˜¯é‡‡ç”¨éšæœºå‘é‡ä½ç§©é€‚åº”ï¼ˆRVLoRAï¼‰è¿›è¡Œè‡ªé€‚åº”å¾®è°ƒçš„æ¨¡å—ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†æ©è†œå¼•å¯¼çš„å¹³æ»‘æŸå¤±ä»¥ç¡®ä¿æ·±åº¦ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†åŒé‡åŸºç¡€æ¨¡å‹ä¸è‡ªé€‚åº”å¾®è°ƒç­–ç•¥ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤æ‚æ‰‹æœ¯åœºæ™¯ä¸­çš„é€‚åº”æ€§å’Œæ·±åº¦ä¼°è®¡ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨äº†åŸºäºæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„æ®‹å·®å—ï¼ˆRes-DSCï¼‰æ¥æ•æ‰ç»†ç²’åº¦å±€éƒ¨ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†æ©è†œå¼•å¯¼çš„å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œä»¥å¢å¼ºè§£å‰–ç»“æ„å†…çš„æ·±åº¦ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨SCAREDã€Hamlynã€SERV-CTå’ŒEndoNeRFæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEndoUFMæ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¾®åˆ›æ‰‹æœ¯ä¸­çš„æ·±åº¦æ„ŸçŸ¥ã€å¢å¼ºç°å®å’Œå¯¼èˆªç³»ç»Ÿã€‚é€šè¿‡æé«˜å¤–ç§‘åŒ»ç”Ÿåœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼ŒEndoUFMæœ‰åŠ©äºæå‡æ‰‹æœ¯çš„ç²¾ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Depth estimation is a foundational component for 3D reconstruction in minimally invasive endoscopic surgeries. However, existing monocular depth estimation techniques often exhibit limited performance to the varying illumination and complex textures of the surgical environment. While powerful visual foundation models offer a promising solution, their training on natural images leads to significant domain adaptability limitations and semantic perception deficiencies when applied to endoscopy. In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Furthermore, we design a mask-guided smoothness loss to enforce depth consistency within anatomical tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. This work contributes to augmenting surgeons' spatial perception during minimally invasive procedures, thereby enhancing surgical precision and safety, with crucial implications for augmented reality and navigation systems.

