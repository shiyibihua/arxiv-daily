---
layout: default
title: Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?
---

# Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18421" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18421v1</a>
  <a href="https://arxiv.org/pdf/2508.18421.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18421v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18421v1', 'Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fatemeh Ziaeetabar

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å…³ç³»å›¾ä»¥æå‡è§†è§‰åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€å…³ç³»å›¾` `è§†è§‰åŸºç¡€æ¨¡å‹` `ç»†ç²’åº¦è¯†åˆ«` `å¤šæ¨¡æ€åˆ†æ` `å›¾æ¨ç†` `æ—¶ç©ºå…³ç³»` `è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æ˜ç¡®æ¨ç†çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦äººç±»æ´»åŠ¨è¯†åˆ«å’Œå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æä¸­ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥åŠ¨æ€å…³ç³»å›¾æ¥å¢å¼ºè§†è§‰åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æ—¶ç©ºå’Œè¯­ä¹‰ä¾èµ–å…³ç³»ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆåŠ¨æ€å…³ç³»å›¾çš„æ¨¡å‹åœ¨è¯­ä¹‰å‡†ç¡®æ€§ã€æŠ—å¹²æ‰°èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å·²æˆä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸»æµæ¶æ„ï¼Œèƒ½å¤Ÿä»å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®ä¸­å­¦ä¹ å¯è½¬ç§»çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨éœ€è¦æ˜ç¡®æ¨ç†å®ä½“ã€è§’è‰²åŠæ—¶ç©ºå…³ç³»çš„ä»»åŠ¡ä¸­ä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºä¸‹ä¸€ä»£FMsåº”å½“å¼•å…¥åŠ¨æ€å…³ç³»å›¾ï¼Œé€šè¿‡è½»é‡çº§çš„ä¸Šä¸‹æ–‡è‡ªé€‚åº”å›¾æ¨ç†æ¨¡å—ï¼Œæå‡ç»†ç²’åº¦è¯­ä¹‰å‡†ç¡®æ€§ã€æŠ—åˆ†å¸ƒåç§»èƒ½åŠ›ã€å¯è§£é‡Šæ€§åŠè®¡ç®—æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œç»“åˆåŠ¨æ€å…³ç³»å›¾çš„æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç»†ç²’åº¦äººç±»æ´»åŠ¨è¯†åˆ«å’ŒåŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ˜ç¡®çš„æ—¶ç©ºå’Œè¯­ä¹‰å…³ç³»æ¨ç†çš„åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå°†åŠ¨æ€å…³ç³»å›¾å¼•å…¥è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å›¾çš„æ‹“æ‰‘ç»“æ„å’Œè¾¹è¯­ä¹‰æ ¹æ®è¾“å…¥å’Œä»»åŠ¡ä¸Šä¸‹æ–‡åŠ¨æ€æ¨æ–­ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŸºç¡€è§†è§‰æ¨¡å‹å’ŒåŠ¨æ€å…³ç³»å›¾æ¨¡å—ï¼Œå‰è€…è´Ÿè´£ç‰¹å¾æå–ï¼Œåè€…åˆ™è¿›è¡Œä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„å…³ç³»æ¨ç†ã€‚æ¨¡å‹é€šè¿‡è½»é‡çº§çš„å›¾æ¨ç†æ¨¡å—å®ç°é«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥åŠ¨æ€å…³ç³»å›¾ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»åœ°è°ƒæ•´å›¾çš„ç»“æ„å’Œè¯­ä¹‰ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è½»é‡çº§çš„å›¾æ¨ç†æ¨¡å—ï¼Œä¼˜åŒ–äº†å›¾çš„æ„å»ºå’Œæ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿åœ¨å†…å­˜å’Œè®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒé«˜æ•ˆæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆåŠ¨æ€å…³ç³»å›¾çš„æ¨¡å‹åœ¨ç»†ç²’åº¦è¯­ä¹‰å‡†ç¡®æ€§ä¸Šæå‡äº†15%ï¼Œåœ¨æŠ—åˆ†å¸ƒåç§»èƒ½åŠ›ä¸Šæé«˜äº†20%ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šç›¸è¾ƒäºä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„åŸºçº¿æå‡äº†30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç»†ç²’åº¦äººç±»æ´»åŠ¨è¯†åˆ«ã€ä¸ªæ€§åŒ–åŒ»ç–—å½±åƒåˆ†æå’Œæ™ºèƒ½ç›‘æ§ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæ»¡è¶³æ—¥ç›Šå¤æ‚çš„è§†è§‰ä»»åŠ¡éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks.

