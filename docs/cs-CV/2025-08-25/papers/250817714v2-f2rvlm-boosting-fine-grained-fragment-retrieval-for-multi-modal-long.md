---
layout: default
title: F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model
---

# F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17714" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.17714v2</a>
  <a href="https://arxiv.org/pdf/2508.17714.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17714v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17714v2', 'F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hanbo Bi, Zhiqiang Yuan, Zexi Jia, Jiapei Zhang, Chongyang Li, Peixiang Luo, Ying Deng, Xiaoyue Duan, Jinchao Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-25 (Êõ¥Êñ∞: 2025-11-10)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫F2RVLM‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÈïøÂØπËØù‰∏≠ÁöÑÁªÜÁ≤íÂ∫¶ÁâáÊÆµÊ£ÄÁ¥¢ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁªÜÁ≤íÂ∫¶Ê£ÄÁ¥¢` `Â§öÊ®°ÊÄÅÂØπËØù` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ËØæÁ®ãÂ≠¶‰π†` `ËØ≠‰πâ‰∏ÄËá¥ÊÄß` `ÈïøÂØπËØùÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂØπËØùÊ£ÄÁ¥¢ÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜÈïøÂØπËØù‰∏≠ÂàÜÊï£ÁöÑËØ≠‰πâ‰∏ÄËá¥ÂÜÖÂÆπÔºåÂØºËá¥Áî®Êà∑‰ΩìÈ™å‰∏ç‰Ω≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫F2RVLMÊ®°ÂûãÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÂíåÂ§öÁõÆÊ†áÂ•ñÂä±Êú∫Âà∂ÔºåÂ¢ûÂº∫ÁâáÊÆµÊ£ÄÁ¥¢ÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄßÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇ
3. F2RVLMÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊµÅË°åÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫Ü‰ºòË∂äÁöÑÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰º†ÁªüÂØπËØùÊ£ÄÁ¥¢ÊñπÊ≥ïÂæÄÂæÄÊó†Ê≥ïÊª°Ë∂≥Áî®Êà∑Âú®ÈïøÂØπËØù‰∏≠ÂõûËÆøËØ≠‰πâ‰∏ÄËá¥ÂÜÖÂÆπÁöÑÈúÄÊ±Ç„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÂÆö‰πâ‰∫ÜÁªÜÁ≤íÂ∫¶ÁâáÊÆµÊ£ÄÁ¥¢ÔºàFFRÔºâ‰ªªÂä°ÔºåË¶ÅÊ±ÇÊ®°Âûã‰ªéÂ§öÊ®°ÊÄÅÈïøÂØπËØù‰∏≠ÂÆö‰Ωç‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÁâáÊÆµÔºåÂåÖÊã¨ÂèëË®ÄÂíåÂõæÂÉè„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫ÜÊúÄÈïøËΩÆÊ¨°ÁöÑÂ§öÊ®°ÊÄÅÂØπËØùÊ£ÄÁ¥¢Êï∞ÊçÆÈõÜMLDRÔºåÂπ∂ÊèêÂá∫F2RVLMÊ®°ÂûãÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÂíåÈöæÂ∫¶ÊÑüÁü•ËØæÁ®ãÈááÊ†∑ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÂØπËØùÊ£ÄÁ¥¢ÊñπÊ≥ïÂú®ÈïøÂØπËØù‰∏≠Êó†Ê≥ïÊúâÊïàÂÆö‰ΩçËØ≠‰πâ‰∏ÄËá¥ÁâáÊÆµÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂè™ÂÖ≥Ê≥®ÊúÄËøëÁöÑÂèëË®ÄÊàñÂõæÂÉèÔºåÂøΩËßÜ‰∫ÜÈïøÂØπËØù‰∏≠‰ø°ÊÅØÁöÑÂàÜÊï£ÊÄßÂíåÂ§çÊùÇÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁªÜÁ≤íÂ∫¶ÁâáÊÆµÊ£ÄÁ¥¢ÔºàFFRÔºâ‰ªªÂä°ÔºåË¶ÅÊ±ÇÊ®°Âûã‰ªéÂ§öÊ®°ÊÄÅÂØπËØù‰∏≠ÊèêÂèñÁõ∏ÂÖ≥ÁâáÊÆµ„ÄÇÈÄöËøáÂºïÂÖ•F2RVLMÊ®°ÂûãÔºåÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÈ¶ñÂÖàËøõË°åÊúâÁõëÁù£ÁöÑÂæÆË∞ÉÔºåÁÑ∂ÂêéÈÄöËøáÂü∫‰∫éGRPOÁöÑÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊ£ÄÁ¥¢Ë¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöF2RVLMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµ‰∏∫ÊúâÁõëÁù£ÂæÆË∞ÉÔºåÊ≥®ÂÖ•ÁâáÊÆµÁ∫ßÊ£ÄÁ¥¢Áü•ËØÜÔºõÁ¨¨‰∫åÈò∂ÊÆµ‰∏∫Âº∫ÂåñÂ≠¶‰π†Ôºå‰ΩøÁî®Â§öÁõÆÊ†áÂ•ñÂä±Êú∫Âà∂ÊèêÂçáËØ≠‰πâÁ≤æÁ°ÆÊÄßÂíå‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÈááÁî®ÈöæÂ∫¶ÊÑüÁü•ËØæÁ®ãÈááÊ†∑ÔºåÈÄêÊ≠•ÂºïÂØºÊ®°ÂûãÂ§ÑÁêÜÊõ¥Â§çÊùÇÁöÑÊ†∑Êú¨„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöF2RVLMÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊñπÊ≥ïÂíåÈöæÂ∫¶ÊÑüÁü•ËØæÁ®ãÈááÊ†∑Á≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®ÈïøÂØπËØù‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ£ÄÁ¥¢ÊïàÊûú„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÂçï‰∏ÄËÆ≠ÁªÉÊñπÂºèÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜGRPOÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêàÂ§öÁõÆÊ†áÂ•ñÂä±ÂáΩÊï∞Ôºå‰ºòÂåñËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÅÁõ∏ÂÖ≥ÊÄßÂíå‰∏ä‰∏ãÊñáËøûË¥ØÊÄß„ÄÇÂêåÊó∂ÔºåÈöæÂ∫¶ÊÑüÁü•ËØæÁ®ãÈááÊ†∑ÈÄöËøáÊ®°ÂûãÈ¢ÑÊµãÁöÑÈöæÂ∫¶ÂØπËÆ≠ÁªÉÂÆû‰æãËøõË°åÊéíÂ∫èÔºåÈÄêÊ≠•Â¢ûÂä†ËÆ≠ÁªÉÈöæÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

F2RVLMÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂú®È¢ÜÂüüÂÜÖÂíåÁúüÂÆûÂú∫ÊôØ‰∏≠ÂùáË∂ÖË∂ä‰∫ÜÊµÅË°åÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÊ£ÄÁ¥¢ÊÄßËÉΩÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Âú®MLDRÊï∞ÊçÆÈõÜ‰∏äÊ£ÄÁ¥¢ÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜXX%ÔºåÂú®WeChatÊµãËØïÈõÜ‰∏äË°®Áé∞‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂÆ¢Êúç„ÄÅÁ§æ‰∫§Â™í‰ΩìÂàÜÊûêÂíåÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøáÊèêÂçáÂØπËØùÁ≥ªÁªüÂú®ÈïøÂØπËØù‰∏≠ÁöÑÊ£ÄÁ¥¢ËÉΩÂäõÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Êª°Ë∂≥Áî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥Êô∫ËÉΩÁöÑÂØπËØùÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Traditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet users' actual needs for revisiting semantically coherent content scattered across long-form conversations. To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues. As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns. Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments. To this end, we propose F2RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards promoting semantic precision, relevance, and contextual coherence. To handle varying intra-fragment complexity, from locally dense to sparsely distributed, we introduce difficulty-aware curriculum sampling that ranks training instances by model-predicted difficulty and gradually exposes the model to harder samples. This boosts reasoning ability in long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain and real-domain settings, demonstrating superior retrieval performance.

