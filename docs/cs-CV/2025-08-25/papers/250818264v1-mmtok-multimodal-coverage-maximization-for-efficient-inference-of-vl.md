---
layout: default
title: MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs
---

# MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18264" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.18264v1</a>
  <a href="https://arxiv.org/pdf/2508.18264.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18264v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18264v1', 'MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sixun Dong, Juhua Hu, Mian Zhang, Ming Yin, Yanjie Fu, Qi Qian

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-25

**Â§áÊ≥®**: Project page: https://project.ironieser.cc/mmtok

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MMTok‰ª•Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜó‰ΩôÊé®ÁêÜÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Â§öÊ®°ÊÄÅËûçÂêà` `Êé®ÁêÜÊïàÁéá` `Ë¶ÜÁõñÁéá‰ºòÂåñ` `‰ø°ÊÅØÈÄâÊã©` `Ê∑±Â∫¶Â≠¶‰π†` `Ê®°ÂûãÂâ™Êûù`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÂæÄÂæÄ‰ªÖ‰æùËµñÂçï‰∏ÄÊ®°ÊÄÅ‰ø°ÊÅØÔºåÂØºËá¥ÂÜó‰ΩôËßÜËßâÊ†áËÆ∞ÂΩ±ÂìçÊé®ÁêÜÊïàÁéá„ÄÇ
2. Êú¨ÊñáÊèêÂá∫MMTokÊñπÊ≥ïÔºåÈÄöËøáÊúÄÂ§ßË¶ÜÁõñÁéáÊ†áÂáÜÂêåÊó∂‰ºòÂåñËßÜËßâÂíåÊñáÊú¨Ê†áËÆ∞ÁöÑÈÄâÊã©Ôºå‰ª•ÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMMTokÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®LLaVA-NeXT-13B‰∏äÂÆûÁé∞‰∫Ü1.87ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÈÄöËøáËØ≠Ë®ÄÊåá‰ª§ÁêÜËß£ËßÜËßâÂÜÖÂÆπÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËßÜËßâÊ†áËÆ∞ÁöÑÂÜó‰ΩôÊÄßÂØºËá¥Êé®ÁêÜÊïàÁéá‰∏ãÈôç„ÄÇÁé∞ÊúâÁöÑËÆ∏Â§öÁÆóÊ≥ï‰ªÖ‰æùËµñÂçï‰∏ÄÊ®°ÊÄÅ‰ø°ÊÅØËøõË°åÂâ™ÊûùÔºåÂøΩËßÜ‰∫ÜËßÜËßâËØ≠Ë®Ä‰ªªÂä°ÁöÑÂ§öÊ®°ÊÄÅÁâπÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂà©Áî®ËßÜËßâÂíåÊñáÊú¨Ê†áËÆ∞ÈÄâÊã©‰ø°ÊÅØ‰∏∞ÂØåÁöÑËßÜËßâÊ†áËÆ∞ÁöÑÊñπÊ≥ïÔºåÂü∫‰∫éË¶ÜÁõñÁéáÊ†áÂáÜ‰ºòÂåñËßÜËßâÊ†áËÆ∞ÁöÑÂ≠êÈõÜÈÄâÊã©ÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®‰∏çÂêåÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂπøÊ≥õËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÁªìÂêàÊòæËëó‰ºò‰∫éÂçïÊ®°ÊÄÅÂü∫Á∫øÔºå‰∏îÂú®POPEÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü1.87ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü98.7%ÁöÑÂéüÂßãÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËøáÁ®ã‰∏≠ËßÜËßâÊ†áËÆ∞ÁöÑÂÜó‰ΩôÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âè™Âà©Áî®Âçï‰∏ÄÊ®°ÊÄÅ‰ø°ÊÅØËøõË°åÂâ™ÊûùÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§öÊ®°ÊÄÅÁâπÊÄßÔºåÂØºËá¥Êé®ÁêÜÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÊúÄÂ§ßË¶ÜÁõñÁéáÊ†áÂáÜÔºåÁªìÂêàËßÜËßâÂíåÊñáÊú¨Ê†áËÆ∞ÁöÑ‰ø°ÊÅØÔºåÈÄâÊã©Âá∫ÊúÄÂÖ∑‰ø°ÊÅØÈáèÁöÑËßÜËßâÊ†áËÆ∞Ôºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°ÂÖÖÂàÜÂà©Áî®‰∫ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑ‰∫íË°•ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊñπÊ≥ïÂàÜ‰∏∫‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºåÂÆö‰πâÂ≠êÈõÜÈÄâÊã©ÈóÆÈ¢ò‰∏∫ÊúÄÂ§ßË¶ÜÁõñÈóÆÈ¢òÔºõÂÖ∂Ê¨°Ôºå‰ºòÂåñËßÜËßâÊ†áËÆ∞ÁöÑÂ≠êÈõÜ‰ª•Ë¶ÜÁõñÊñáÊú¨Ê†áËÆ∞ÂíåÂéüÂßãËßÜËßâÊ†áËÆ∞ÔºõÊúÄÂêéÔºåÈááÁî®VLM‰ª£ÁêÜËøõ‰∏ÄÊ≠•ÊèêÂçáÊñáÊú¨Ê†áËÆ∞ÁöÑË¥®ÈáèÔºå‰ª•ÊåáÂØºËßÜËßâÂâ™Êûù„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éË¶ÜÁõñÁéáÁöÑÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÈÄâÊã©Ê†áÂáÜÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰ªÖ‰æùËµñÂçï‰∏ÄÊ®°ÊÄÅÁöÑÂ±ÄÈôêÊÄßÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÈÄâÊã©Êú∫Âà∂Ôºå‰ª•Á°Æ‰øùÈÄâÊã©ÁöÑËßÜËßâÊ†áËÆ∞ËÉΩÂ§üÊúÄÂ§ßÁ®ãÂ∫¶‰∏äË¶ÜÁõñÊñáÊú¨‰ø°ÊÅØ„ÄÇÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÔºåÁªìÂêà‰∫ÜË¶ÜÁõñÁéáÂíå‰ø°ÊÅØÈáèÁöÑÊùÉË°°Ôºå‰ª•‰ºòÂåñÈÄâÊã©ËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMMTokÂú®POPEÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü1.87ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü98.7%ÁöÑÂéüÂßãÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂú®LLaVA-1.5-7BÊ®°Âûã‰∏≠Ôºå‰ªÖ‰ΩøÁî®Âõõ‰∏™ËßÜËßâÊ†áËÆ∞‰ªçËÉΩ‰øùÁïô87.7%ÁöÑÂéüÂßãÊÄßËÉΩÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ËßÜËßâÊ†áËÆ∞ÈÄâÊã©‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ËßÜËßâÁ≠âÈúÄË¶ÅÈ´òÊïàÂ§ÑÁêÜËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÈ´òËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéáÔºåMMTokÂèØ‰ª•Âú®ÂÆûÊó∂Â∫îÁî®‰∏≠ÊòæËëóÊèêÂçáÁ≥ªÁªüÁöÑÂìçÂ∫îÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual input to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of vision-language tasks. Moreover, it lacks a generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the criterion of coverage. We first formulate the subset selection problem as a maximum coverage problem. Afterward, a subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, a VLM agent can be adopted to further improve the quality of text tokens for guiding vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with a clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves a 1.87x speedup while maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, it still preserves 87.7% of the original performance on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection.

