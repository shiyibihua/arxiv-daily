---
layout: default
title: Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework
---

# Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17726" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17726v1</a>
  <a href="https://arxiv.org/pdf/2508.17726.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17726v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17726v1', 'Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Koichiro Kamide, Shunsuke Sakai, Shun Maeda, Chunzhi Gu, Chao Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å°‘æ ·æœ¬äººç±»åŠ¨ä½œå¼‚å¸¸æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»åŠ¨ä½œå¼‚å¸¸æ£€æµ‹` `å¯¹æ¯”å­¦ä¹ ` `å°‘æ ·æœ¬å­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹` `è¿åŠ¨å¢å¼º` `æ•°æ®ç¨€ç¼ºåœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰HAADæ–¹æ³•é€šå¸¸éœ€è¦ä¸ºæ¯ä¸ªåŠ¨ä½œç±»åˆ«å•ç‹¬è®­ç»ƒï¼Œä¸”ä¾èµ–å¤§é‡æ­£å¸¸æ ·æœ¬ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç±»åˆ«æ— å…³çš„è¡¨ç¤ºç©ºé—´ï¼Œæ”¯æŒå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ã€‚
3. åœ¨HumanAct12æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åŠ¨ä½œå¼‚å¸¸æ£€æµ‹ï¼ˆHAADï¼‰æ—¨åœ¨ä»…åˆ©ç”¨æ­£å¸¸åŠ¨ä½œæ•°æ®è¯†åˆ«å¼‚å¸¸åŠ¨ä½œã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ¯ç±»ä¸€ä¸ªæ¨¡å‹çš„èŒƒå¼ï¼Œéœ€ä¸ºæ¯ä¸ªåŠ¨ä½œç±»åˆ«å•ç‹¬è®­ç»ƒï¼Œå¹¶ä¸”éœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬ã€‚è¿™äº›é™åˆ¶å¦¨ç¢äº†å¯æ‰©å±•æ€§ï¼Œå¹¶é™åˆ¶äº†åœ¨æ•°æ®ç¨€ç¼ºæˆ–æ–°ç±»åˆ«é¢‘ç¹å‡ºç°çš„å®é™…åº”ç”¨ä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…¼å®¹å°‘æ ·æœ¬åœºæ™¯çš„HAADç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºç±»åˆ«æ— å…³çš„è¡¨ç¤ºç©ºé—´ï¼Œä½¿å¾—é€šè¿‡ä¸ç»™å®šçš„å°è§„æ¨¡æ­£å¸¸æ ·æœ¬é›†ï¼ˆæ”¯æŒé›†ï¼‰æ¯”è¾ƒæ¥å®ç°å¼‚å¸¸æ£€æµ‹ã€‚ä¸ºæé«˜ç±»åˆ«é—´çš„æ³›åŒ–èƒ½åŠ›å’Œç±»åˆ«å†…çš„é²æ£’æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿åŠ¨å¢å¼ºç­–ç•¥ï¼Œä»¥åˆ›å»ºå¤šæ ·ä¸”çœŸå®çš„è®­ç»ƒæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HumanAct12æ•°æ®é›†ä¸Šåœ¨å·²è§å’Œæœªè§ç±»åˆ«è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨ä»…æœ‰æ­£å¸¸åŠ¨ä½œæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆè¯†åˆ«å¼‚å¸¸åŠ¨ä½œçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºéœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬å’Œæ¯ç±»å•ç‹¬è®­ç»ƒï¼Œå¯¼è‡´å¯æ‰©å±•æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç±»åˆ«æ— å…³çš„è¡¨ç¤ºç©ºé—´ï¼Œé€šè¿‡ä¸å°‘é‡æ­£å¸¸æ ·æœ¬çš„æ¯”è¾ƒæ¥å®ç°å¼‚å¸¸æ£€æµ‹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å¯¹æ¯”å­¦ä¹ æ¨¡å—å’Œç”Ÿæˆè¿åŠ¨å¢å¼ºæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºè¡¨ç¤ºç©ºé—´ï¼Œç„¶ååˆ©ç”¨ç”Ÿæˆæ¨¡å‹å¢å¼ºè®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§ï¼Œæœ€åè¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡é¦–æ¬¡å¼•å…¥åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿åŠ¨å¢å¼ºç­–ç•¥ï¼Œä»¥å¢å¼ºå¯¹æ¯”å­¦ä¹ åœ¨åŠ¨ä½œå¼‚å¸¸æ£€æµ‹ä¸­çš„æ•ˆæœã€‚è¿™ä¸€åˆ›æ–°ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–æ ·æœ¬æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæœ¬æ–‡é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†æ‰©æ•£æ¨¡å‹çš„å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨HumanAct12æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å·²è§ç±»åˆ«å’Œæœªè§ç±»åˆ«è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼Œè®­ç»ƒæ•ˆç‡æ˜¾è‘—æé«˜ï¼Œæ¨¡å‹å¯æ‰©å±•æ€§å¾—åˆ°å¢å¼ºï¼Œå…·ä½“æ€§èƒ½æŒ‡æ ‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç›‘æ§ç³»ç»Ÿã€æ™ºèƒ½å®‰é˜²ã€è¿åŠ¨åˆ†æç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼Œæå‡å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨HAADæŠ€æœ¯çš„å®é™…è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human Action Anomaly Detection (HAAD) aims to identify anomalous actions given only normal action data during training. Existing methods typically follow a one-model-per-category paradigm, requiring separate training for each action category and a large number of normal samples. These constraints hinder scalability and limit applicability in real-world scenarios, where data is often scarce or novel categories frequently appear. To address these limitations, we propose a unified framework for HAAD that is compatible with few-shot scenarios. Our method constructs a category-agnostic representation space via contrastive learning, enabling AD by comparing test samples with a given small set of normal examples (referred to as the support set). To improve inter-category generalization and intra-category robustness, we introduce a generative motion augmentation strategy harnessing a diffusion-based foundation model for creating diverse and realistic training samples. Notably, to the best of our knowledge, our work is the first to introduce such a strategy specifically tailored to enhance contrastive learning for action AD. Extensive experiments on the HumanAct12 dataset demonstrate the state-of-the-art effectiveness of our approach under both seen and unseen category settings, regarding training efficiency and model scalability for few-shot HAAD.

