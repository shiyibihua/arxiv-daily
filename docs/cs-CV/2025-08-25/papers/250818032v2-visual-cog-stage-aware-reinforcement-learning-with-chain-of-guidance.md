---
layout: default
title: Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation
---

# Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18032" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18032v2</a>
  <a href="https://arxiv.org/pdf/2508.18032.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18032v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18032v2', 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaqi Li, Peng Chen, Mingyang Han, Pi Bu, Haoxiang Shi, Runzhou Zhao, Yang Yao, Xuan Zhang, Jun Song, Bo Zheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25 (æ›´æ–°: 2025-08-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisual-CoGä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `é“¾å¼æ€ç»´` `é˜¶æ®µæ„ŸçŸ¥` `è§†è§‰åˆæˆ` `è¯­ä¹‰æ¨ç†` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºæ—¶å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œéš¾ä»¥æœ‰æ•ˆè¯†åˆ«å„é˜¶æ®µçš„è´¡çŒ®ã€‚
2. æœ¬æ–‡æå‡ºVisual-CoGæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥é˜¶æ®µæ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¯ä¸ªé˜¶æ®µï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨GenEvalã€T2I-CompBenchå’ŒVisCog-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVisual-CoGåˆ†åˆ«æå‡äº†15%ã€5%å’Œ19%çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è¿‘å¹´æ¥è‡ªå›å½’æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¤„ç†å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºçš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œç°æœ‰ç ”ç©¶åº”ç”¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä»¥å®ç°é˜¶æ®µæ„ŸçŸ¥çš„è§†è§‰åˆæˆï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ä»…åœ¨ç”Ÿæˆé˜¶æ®µç»“æŸæ—¶æä¾›å¥–åŠ±ä¿¡å·ï¼Œè¿™ç§å•ä¸€çš„æœ€ç»ˆæŒ‡å¯¼ä½¿å¾—éš¾ä»¥è¯†åˆ«å“ªäº›é˜¶æ®µå¯¹æœ€ç»ˆç»“æœæœ‰ç§¯æè´¡çŒ®ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰æŒ‡å¯¼é“¾ï¼ˆVisual-CoGï¼‰èŒƒå¼ï¼ŒåŒ…å«è¯­ä¹‰æ¨ç†ã€è¿‡ç¨‹ä¼˜åŒ–å’Œç»“æœè¯„ä¼°ä¸‰ä¸ªé˜¶æ®µï¼Œé€šè¿‡é˜¶æ®µæ„ŸçŸ¥çš„å¥–åŠ±åœ¨æ•´ä¸ªå›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›å³æ—¶æŒ‡å¯¼ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒVisual-CoGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤šå±æ€§å’Œæ¨¡ç³Šæç¤ºæ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¥–åŠ±ä¿¡å·ä»…åœ¨ç”Ÿæˆç»“æŸæ—¶æä¾›å¯¼è‡´çš„æ¬¡ä¼˜ç­–ç•¥é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVisual-CoGæ¡†æ¶ï¼Œé€šè¿‡é˜¶æ®µæ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªé˜¶æ®µè¿›è¡Œå³æ—¶æŒ‡å¯¼ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVisual-CoGåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šè¯­ä¹‰æ¨ç†ã€è¿‡ç¨‹ä¼˜åŒ–å’Œç»“æœè¯„ä¼°ã€‚æ¯ä¸ªé˜¶æ®µéƒ½é…å¤‡ç›¸åº”çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹çš„æ¯ä¸€æ­¥éƒ½èƒ½å¾—åˆ°æœ‰æ•ˆåé¦ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é˜¶æ®µæ„ŸçŸ¥çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆçš„æ¯ä¸ªé˜¶æ®µè·å¾—åé¦ˆï¼Œä»è€Œä¼˜åŒ–æ¯ä¸ªé˜¶æ®µçš„å†³ç­–è¿‡ç¨‹ï¼Œä¸ä¼ ç»Ÿçš„æœ€ç»ˆå¥–åŠ±æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚æ¬¡çš„ç½‘ç»œç»“æ„ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†é˜¶æ®µæ€§å¥–åŠ±ï¼Œä»¥ç¡®ä¿æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºéƒ½èƒ½å¾—åˆ°æœ‰æ•ˆè¯„ä¼°å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVisual-CoGå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šåœ¨GenEvalä¸Šæå‡äº†15%ï¼Œåœ¨T2I-CompBenchä¸Šæå‡äº†5%ï¼Œåœ¨VisCog-Benchä¸Šæå‡äº†19%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVisual-CoGåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰ä¼˜è¶Šçš„è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‰ºæœ¯åˆ›ä½œã€å¹¿å‘Šè®¾è®¡å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç”Ÿæˆç¬¦åˆç‰¹å®šéœ€æ±‚çš„å›¾åƒã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒVisual-CoGæœ‰æœ›åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.

