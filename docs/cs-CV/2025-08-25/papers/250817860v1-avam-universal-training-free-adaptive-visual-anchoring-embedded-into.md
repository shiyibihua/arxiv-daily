---
layout: default
title: AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering
---

# AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17860" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17860v1</a>
  <a href="https://arxiv.org/pdf/2508.17860.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17860v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17860v1', 'AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kang Zeng, Guojin Zhong, Jintao Cheng, Jin Yuan, Zhiyong Li

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

**å¤‡æ³¨**: 14 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥ä»¥è§£å†³å¤šå›¾åƒé—®ç­”ä¸­çš„è§†è§‰å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰é—®ç­”` `è‡ªé€‚åº”è§†è§‰é”šå®š` `å¤šå›¾åƒé—®ç­”` `ååŒè§£ç æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šå›¾åƒé—®ç­”æ–¹æ³•åœ¨å¤„ç†è§†è§‰å†—ä½™æ—¶ç¼ºä¹çµæ´»æ€§ï¼Œå¯¼è‡´å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©è§†è§‰ä¿¡æ¯å¹¶æå‡é—®ç­”æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„å‘å±•ï¼Œå°¤å…¶æ˜¯å¤šå›¾åƒé—®ç­”ï¼ˆMVQAï¼‰ã€‚ç„¶è€Œï¼ŒMVQAä¸­å›¾åƒæ•°é‡çš„å¢åŠ ä¸å¯é¿å…åœ°å¼•å…¥äº†å¤§é‡ä¸é—®é¢˜å›ç­”æ— å…³çš„è§†è§‰å†—ä½™ï¼Œå½±å“äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶å‹ç¼©è§†è§‰æ ‡è®°æ•°é‡æ–¹é¢ç¼ºä¹çµæ´»æ€§ï¼Œä¸”å¾€å¾€äº§ç”Ÿç¦»æ•£çš„è§†è§‰ç‰‡æ®µï¼Œå¦¨ç¢äº†MLLMså¯¹å›¾åƒçš„æ•´ä½“ç†è§£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œé€šç”¨çš„è‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„MLLMsä¸­ï¼Œé€šè¿‡è‡ªé€‚åº”å‹ç¼©æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ååŒè§£ç æœºåˆ¶ï¼Œä»¥å¹³è¡¡æ¥è‡ªå…¨å±€å’Œå‹ç¼©è§†è§‰è¾“å…¥çš„ç»“æœã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šç§MLLMsä¸Šçš„ä¸€è‡´æ€§èƒ½æå‡ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå›¾åƒé—®ç­”ä¸­ç”±äºè§†è§‰å†—ä½™å¯¼è‡´çš„å‡†ç¡®æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å‹ç¼©è§†è§‰æ ‡è®°æ•°é‡æ—¶ç¼ºä¹çµæ´»æ€§ï¼Œäº§ç”Ÿçš„ç¦»æ•£è§†è§‰ç‰‡æ®µå½±å“äº†æ¨¡å‹å¯¹å›¾åƒçš„æ•´ä½“ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„è‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥é€šè¿‡åŠ¨æ€è°ƒæ•´è§†è§‰ä¿¡æ¯çš„å‹ç¼©ç¨‹åº¦ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹é‡è¦è§†è§‰ä¿¡æ¯çš„æ•æ‰èƒ½åŠ›ï¼Œä»è€Œæå‡é—®ç­”çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªé€‚åº”è§†è§‰é”šå®šæ¨¡å—å’ŒååŒè§£ç æœºåˆ¶ã€‚è‡ªé€‚åº”è§†è§‰é”šå®šæ¨¡å—è´Ÿè´£å‹ç¼©è§†è§‰è¾“å…¥ï¼Œè€ŒååŒè§£ç æœºåˆ¶åˆ™ç»“åˆå…¨å±€å’Œå‹ç¼©è§†è§‰ä¿¡æ¯è¿›è¡Œä¼˜åŒ–è§£ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥çš„æå‡ºï¼Œå®ƒå…è®¸æ¨¡å‹æ ¹æ®è¾“å…¥é—®é¢˜åŠ¨æ€è°ƒæ•´è§†è§‰ä¿¡æ¯çš„å¤„ç†æ–¹å¼ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å¯¹å›¾åƒçš„æ•´ä½“ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å…¨å±€å’Œå‹ç¼©è§†è§‰è¾“å…¥çš„è´¡çŒ®ï¼Œå¹¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”è‡ªé€‚åº”å‹ç¼©çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è‡ªé€‚åº”è§†è§‰é”šå®šç­–ç•¥åï¼Œæ¨¡å‹åœ¨å¤šç§åŸºå‡†æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æå‡äº†5%-10%ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨å¤„ç†å¤šå›¾åƒé—®ç­”æ—¶çš„æ•ˆç‡ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒæ£€ç´¢å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šå›¾åƒé—®ç­”çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advancement of Multimodal Large Language Models (MLLMs) has driven significant progress in Visual Question Answering (VQA), evolving from Single to Multi Image VQA (MVQA). However, the increased number of images in MVQA inevitably introduces substantial visual redundancy that is irrelevant to question answering, negatively impacting both accuracy and efficiency. To address this issue, existing methods lack flexibility in controlling the number of compressed visual tokens and tend to produce discrete visual fragments, which hinder MLLMs' ability to comprehend images holistically. In this paper, we propose a straightforward yet universal Adaptive Visual Anchoring strategy, which can be seamlessly integrated into existing MLLMs, offering significant accuracy improvements through adaptive compression. Meanwhile, to balance the results derived from both global and compressed visual input, we further introduce a novel collaborative decoding mechanism, enabling optimal performance. Extensive experiments validate the effectiveness of our method, demonstrating consistent performance improvements across various MLLMs. The code will be publicly available.

