---
layout: default
title: VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference
---

# VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17857" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.17857v1</a>
  <a href="https://arxiv.org/pdf/2508.17857.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17857v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17857v1', 'VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, Rongrong Ji

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-25

**Â§áÊ≥®**: Accepted by ACMMM 2025

**DOI**: [10.1145/3746027.3755792](https://doi.org/10.1145/3746027.3755792)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/mobiushy/VISA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VISA‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ‰ª§ÁâåËÅöÂêà` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜÊïàÁéá` `ÂõæÁªìÊûÑ` `‰ø°ÊÅØËÅöÂêà` `ÁªÑ-wiseÈÄâÊã©Á≠ñÁï•` `Ê®°ÂûãÂéãÁº©` `ËØ≠‰πâÁõ∏‰ººÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Èù¢‰∏¥ËßÜËßâ‰ª§ÁâåËøáÂ§öÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÂíå‰ø°ÊÅØÊçüÂ§±„ÄÇ
2. VISAÊñπÊ≥ïÈÄöËøáÂõæÁªìÊûÑËÅöÂêàÂíåÁªÑ-wiseÈÄâÊã©Á≠ñÁï•ÔºåÊúâÊïàÂéãÁº©ËßÜËßâ‰ª§ÁâåÔºåÂêåÊó∂‰øùÁïôÊõ¥Â§öËßÜËßâ‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVISAÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÊ®°ÂûãÊÄßËÉΩ‰∏éÊé®ÁêÜÈÄüÂ∫¶ÁöÑÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫ÁªÑ-wiseËßÜËßâ‰ª§ÁâåÈÄâÊã©‰∏éËÅöÂêàÔºàVISAÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏≠Áî±‰∫éËßÜËßâ‰ª§ÁâåËøáÂ§öÂØºËá¥ÁöÑÊé®ÁêÜÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢ò„ÄÇ‰∏é‰ª•ÂæÄÁöÑ‰ª§Áâå‰øÆÂâ™ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂéãÁº©ËßÜËßâ‰ª§ÁâåÁöÑÂêåÊó∂ËÉΩÂ§ü‰øùÁïôÊõ¥Â§öÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇÊàë‰ª¨È¶ñÂÖàÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂõæÁöÑËßÜËßâ‰ª§ÁâåËÅöÂêàÔºàVTAÔºâÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂ∞ÜÊØè‰∏™ËßÜËßâ‰ª§ÁâåËßÜ‰∏∫ËäÇÁÇπÔºåÂü∫‰∫éËßÜËßâ‰ª§Áâå‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏‰ººÊÄßÂΩ¢ÊàêÂõæÁªìÊûÑ„ÄÇÁÑ∂ÂêéÔºåVTAÊ†πÊçÆËØ•ÂõæÂ∞ÜË¢´ÁßªÈô§ÁöÑ‰ª§ÁâåÁöÑ‰ø°ÊÅØËÅöÂêàÂà∞‰øùÁïôÁöÑ‰ª§Áâå‰∏≠Ôºå‰ªéËÄåÁîüÊàêÊõ¥Á¥ßÂáëÁöÑËßÜËßâ‰ª§ÁâåË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁªÑ-wise‰ª§ÁâåÈÄâÊã©Á≠ñÁï•ÔºàGTSÔºâÔºåËØ•Á≠ñÁï•Ê†πÊçÆÊØèÁªÑÊúÄÁªàÂ±ÇÁöÑÊñáÊú¨‰ª§ÁâåÂ∞ÜËßÜËßâ‰ª§ÁâåÂàíÂàÜ‰∏∫‰øùÁïôÂíåÁßªÈô§‰∏§Á±ªÔºåÈÄêÊ≠•ËÅöÂêàËßÜËßâ‰ø°ÊÅØÔºåÂ¢ûÂº∫ËßÜËßâ‰ø°ÊÅØÊèêÂèñËøáÁ®ãÁöÑÁ®≥ÂÆöÊÄß„ÄÇÊàë‰ª¨Âú®LLaVA-1.5„ÄÅLLaVA-NeXTÂíåVideo-LLaVAÁ≠âÂ§ö‰∏™Âü∫ÂáÜ‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ÂÆûÈ™åÔºåÈ™åËØÅ‰∫ÜVISAÁöÑÊúâÊïàÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ°àÂú®Ê®°ÂûãÊÄßËÉΩ‰∏éÊé®ÁêÜÈÄüÂ∫¶‰πãÈó¥ÂÆûÁé∞‰∫ÜÊõ¥‰ºòÁöÑÂπ≥Ë°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËøáÁ®ã‰∏≠Áî±‰∫éËßÜËßâ‰ª§ÁâåÊï∞ÈáèËøáÂ§öËÄåÂØºËá¥ÁöÑÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑ‰ª§Áâå‰øÆÂâ™ÊñπÊ≥ïÂæÄÂæÄ‰ºöÂØºËá¥‰ø°ÊÅØÊçüÂ§±ÔºåÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVISAÈÄöËøáÂºïÂÖ•ÂõæÁªìÊûÑÊù•ËÅöÂêàËßÜËßâ‰ª§Áâå‰ø°ÊÅØÔºåÂπ∂ÈááÁî®ÁªÑ-wiseÈÄâÊã©Á≠ñÁï•Êù•‰ºòÂåñËßÜËßâ‰ª§ÁâåÁöÑ‰øùÁïô‰∏éÁßªÈô§Ôºå‰ªéËÄåÂú®ÂéãÁº©ËßÜËßâ‰ª§ÁâåÁöÑÂêåÊó∂‰øùÁïôÊõ¥Â§öÈáçË¶Å‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVISAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöËßÜËßâ‰ª§ÁâåËÅöÂêàÔºàVTAÔºâÊ®°ÂùóÂíåÁªÑ-wise‰ª§ÁâåÈÄâÊã©ÔºàGTSÔºâÁ≠ñÁï•„ÄÇVTAÊ®°ÂùóÈÄöËøáÊûÑÂª∫ËßÜËßâ‰ª§Áâå‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏‰ººÊÄßÂõæÊù•ÂÆûÁé∞‰ø°ÊÅØËÅöÂêàÔºåËÄåGTSÁ≠ñÁï•ÂàôÊ†πÊçÆÊñáÊú¨‰ª§ÁâåÁöÑÊåáÂØºÊù•ÂÜ≥ÂÆö‰øùÁïôÂíåÁßªÈô§ÁöÑËßÜËßâ‰ª§Áâå„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVISAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÂõæÁªìÊûÑËÅöÂêà‰∏éÁªÑ-wiseÈÄâÊã©Á≠ñÁï•ÔºåËøô‰∏ÄËÆæËÆ°‰ΩøÂæóÂú®ÂéãÁº©ËßÜËßâ‰ª§ÁâåÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÊúâÊïà‰øùÁïôÈáçË¶ÅÁöÑËßÜËßâ‰ø°ÊÅØÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®VTAÊ®°Âùó‰∏≠ÔºåËßÜËßâ‰ª§ÁâåË¢´ËßÜ‰∏∫ÂõæÁöÑËäÇÁÇπÔºåËæπÁöÑÊùÉÈáçÂü∫‰∫éËØ≠‰πâÁõ∏‰ººÊÄßËøõË°åËÆ°ÁÆó„ÄÇGTSÁ≠ñÁï•ÂàôÈÄöËøáÂàÜÊûêÊñáÊú¨‰ª§ÁâåÁöÑÁâπÂæÅÊù•ÊåáÂØºËßÜËßâ‰ª§ÁâåÁöÑÈÄâÊã©ÔºåÁ°Æ‰øù‰ø°ÊÅØÁöÑÊúâÊïàËÅöÂêà‰∏é‰øùÁïô„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Âú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåVISAÊñπÊ≥ïË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑ‰ª§Áâå‰øÆÂâ™ÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®LLaVA-1.5‰∏äÔºåVISAÂú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá‰∫ÜÁ∫¶30%„ÄÇËøô‰∏ÄÁªìÊûúË°®ÊòéVISAÂú®Ê®°ÂûãÊïàÁéá‰∏éÊÄßËÉΩ‰πãÈó¥ËææÊàê‰∫ÜÊõ¥‰ºòÁöÑÂπ≥Ë°°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VISAÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéáÊèêÂçáÊñπÈù¢ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÈúÄË¶ÅÂÆûÊó∂Â§ÑÁêÜËßÜËßâ‰ø°ÊÅØÁöÑÂú∫ÊôØÔºåÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅËßÜÈ¢ëÂàÜÊûêÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüü„ÄÇÂÖ∂È´òÊïàÁöÑ‰ø°ÊÅØËÅöÂêàËÉΩÂäõÂ∞ÜÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÊÄßËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In this study, we introduce a novel method called group-wise \textbf{VI}sual token \textbf{S}election and \textbf{A}ggregation (VISA) to address the issue of inefficient inference stemming from excessive visual tokens in multimoal large language models (MLLMs). Compared with previous token pruning approaches, our method can preserve more visual information while compressing visual tokens. We first propose a graph-based visual token aggregation (VTA) module. VTA treats each visual token as a node, forming a graph based on semantic similarity among visual tokens. It then aggregates information from removed tokens into kept tokens based on this graph, producing a more compact visual token representation. Additionally, we introduce a group-wise token selection strategy (GTS) to divide visual tokens into kept and removed ones, guided by text tokens from the final layers of each group. This strategy progressively aggregates visual information, enhancing the stability of the visual information extraction process. We conduct comprehensive experiments on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate the efficacy of VISA. Our method consistently outperforms previous methods, achieving a superior trade-off between model performance and inference speed. The code is available at https://github.com/mobiushy/VISA.

