---
layout: default
title: Interleaving Reasoning for Better Text-to-Image Generation
---

# Interleaving Reasoning for Better Text-to-Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06945" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06945v2</a>
  <a href="https://arxiv.org/pdf/2509.06945.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06945v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06945v2', 'Interleaving Reasoning for Better Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08 (æ›´æ–°: 2025-09-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Osilly/Interleaving-Reasoning-Generation)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤é”™æ¨ç†ç”Ÿæˆæ¡†æ¶IRGï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æŒ‡ä»¤éµå¾ªå’Œç»†èŠ‚ä¿æŒèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `äº¤é”™æ¨ç†` `å›¾åƒåˆæˆ` `æ–‡æœ¬ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’Œç»†èŠ‚ä¿æŒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸ç´§è€¦åˆç†è§£ä¸ç”Ÿæˆçš„ç³»ç»Ÿç›¸æ¯”ä»æœ‰å·®è·ã€‚
2. æå‡ºäº¤é”™æ¨ç†ç”Ÿæˆï¼ˆIRGï¼‰æ¡†æ¶ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œæ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆï¼Œé€æ­¥ä¼˜åŒ–å›¾åƒè´¨é‡å’Œç»†èŠ‚ã€‚
3. æ„å»ºIRGL-300Kæ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº¤é”™æ¨ç†ç”Ÿæˆï¼ˆIRGï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œç»†èŠ‚ä¿æŒèƒ½åŠ›ï¼Œç¼©å°ä¸GPT-4oç­‰ç´§è€¦åˆç†è§£ä¸ç”Ÿæˆç³»ç»Ÿä¹‹é—´çš„å·®è·ã€‚IRGæ¡†æ¶äº¤æ›¿è¿›è¡ŒåŸºäºæ–‡æœ¬çš„æ€è€ƒå’Œå›¾åƒåˆæˆï¼šæ¨¡å‹é¦–å…ˆç”Ÿæˆæ–‡æœ¬æ€è€ƒæŒ‡å¯¼åˆå§‹å›¾åƒï¼Œç„¶ååæ€ç»“æœä»¥æ”¹è¿›ç»†èŠ‚ã€è§†è§‰è´¨é‡å’Œç¾å­¦ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ºäº†æœ‰æ•ˆè®­ç»ƒIRGï¼Œæå‡ºäº†äº¤é”™æ¨ç†ç”Ÿæˆå­¦ä¹ ï¼ˆIRGLï¼‰æ–¹æ³•ï¼Œé’ˆå¯¹ä¸¤ä¸ªå­ç›®æ ‡ï¼šå¢å¼ºåˆå§‹æ€è€ƒå’Œç”Ÿæˆé˜¶æ®µä»¥å»ºç«‹æ ¸å¿ƒå†…å®¹å’ŒåŸºæœ¬è´¨é‡ï¼Œä»¥åŠå®ç°é«˜è´¨é‡çš„æ–‡æœ¬åæ€å¹¶åœ¨åç»­å›¾åƒä¸­å¿ å®åœ°å®æ–½è¿™äº›æ”¹è¿›ã€‚æ„å»ºäº†IRGL-300Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è¢«ç»„ç»‡æˆå…­ç§åˆ†è§£çš„å­¦ä¹ æ¨¡å¼ï¼Œå…±åŒæ¶µç›–äº†å­¦ä¹ åŸºäºæ–‡æœ¬çš„æ€è€ƒå’Œå®Œæ•´çš„æ€è€ƒ-å›¾åƒè½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†SoTAæ€§èƒ½ï¼Œåœ¨GenEvalã€WISEã€TIIFã€GenAI-Benchå’ŒOneIG-ENä¸Šè·å¾—äº†5-10åˆ†çš„ç»å¯¹æ”¶ç›Šï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†è§†è§‰è´¨é‡å’Œç²¾ç»†çš„ä¿çœŸåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°ç”Ÿæˆå¯¹åº”çš„å›¾åƒã€‚ç°æœ‰æ–¹æ³•åœ¨æŒ‡ä»¤éµå¾ªå’Œç»†èŠ‚ä¿æŒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆç”¨æˆ·æ„å›¾çš„å›¾åƒã€‚ç—›ç‚¹åœ¨äºæ¨¡å‹éš¾ä»¥ç†è§£æ–‡æœ¬æè¿°ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ï¼Œå¹¶ä¸”éš¾ä»¥å°†è¿™äº›ä¿¡æ¯å‡†ç¡®åœ°åæ˜ åˆ°ç”Ÿæˆçš„å›¾åƒä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥äº¤é”™æ¨ç†æœºåˆ¶ï¼Œè®©æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒçš„è¿‡ç¨‹ä¸­è¿›è¡Œæ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆçš„äº¤æ›¿è¿­ä»£ã€‚é€šè¿‡æ–‡æœ¬æ€è€ƒæ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆï¼Œå¹¶é€šè¿‡å›¾åƒåˆæˆçš„ç»“æœæ¥åæ€å’Œæ”¹è¿›æ–‡æœ¬æ€è€ƒï¼Œä»è€Œé€æ­¥ä¼˜åŒ–å›¾åƒçš„è´¨é‡å’Œç»†èŠ‚ã€‚è¿™ç§äº¤é”™æ¨ç†çš„æ–¹å¼å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£æ–‡æœ¬æè¿°ï¼Œå¹¶å°†å…¶å‡†ç¡®åœ°åæ˜ åˆ°ç”Ÿæˆçš„å›¾åƒä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIRGæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šåˆå§‹æ€è€ƒå’Œç”Ÿæˆé˜¶æ®µï¼Œä»¥åŠåæ€å’Œæ”¹è¿›é˜¶æ®µã€‚åœ¨åˆå§‹æ€è€ƒå’Œç”Ÿæˆé˜¶æ®µï¼Œæ¨¡å‹é¦–å…ˆæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆä¸€æ®µæ–‡æœ¬æ€è€ƒï¼Œç„¶åæ ¹æ®è¿™æ®µæ–‡æœ¬æ€è€ƒç”Ÿæˆåˆå§‹å›¾åƒã€‚åœ¨åæ€å’Œæ”¹è¿›é˜¶æ®µï¼Œæ¨¡å‹å¯¹åˆå§‹å›¾åƒè¿›è¡Œåˆ†æï¼Œå¹¶ç”Ÿæˆä¸€æ®µæ–‡æœ¬åæ€ï¼Œç„¶åæ ¹æ®è¿™æ®µæ–‡æœ¬åæ€å¯¹åˆå§‹å›¾åƒè¿›è¡Œæ”¹è¿›ï¼Œç”Ÿæˆæœ€ç»ˆçš„å›¾åƒã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥è¿­ä»£å¤šæ¬¡ï¼Œç›´åˆ°ç”Ÿæˆæ»¡æ„çš„å›¾åƒä¸ºæ­¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯å¼•å…¥äº†äº¤é”™æ¨ç†æœºåˆ¶ï¼Œå°†æ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆç»“åˆèµ·æ¥ï¼Œå®ç°äº†æ›´ç²¾ç»†çš„å›¾åƒç”Ÿæˆæ§åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIRGæ¡†æ¶å¯ä»¥æ›´å¥½åœ°ç†è§£æ–‡æœ¬æè¿°ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ï¼Œå¹¶å°†å…¶å‡†ç¡®åœ°åæ˜ åˆ°ç”Ÿæˆçš„å›¾åƒä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šIRGL-300Kæ•°æ®é›†åŒ…å«å…­ç§åˆ†è§£çš„å­¦ä¹ æ¨¡å¼ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹çš„æ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆèƒ½åŠ›ã€‚ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•é¦–å…ˆè®­ç»ƒæ¨¡å‹çš„æ–‡æœ¬æ€è€ƒå’Œåæ€èƒ½åŠ›ï¼Œç„¶åè®­ç»ƒæ¨¡å‹çš„å®Œæ•´äº¤é”™æ¨ç†ç”Ÿæˆèƒ½åŠ›ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ–‡æœ¬æ€è€ƒæŸå¤±ã€å›¾åƒåˆæˆæŸå¤±å’Œåæ€æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„å„ä¸ªæ¨¡å—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIRGæ¡†æ¶åœ¨GenEvalã€WISEã€TIIFã€GenAI-Benchå’ŒOneIG-ENç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œç»å¯¹æ”¶ç›Šè¾¾åˆ°5-10åˆ†ã€‚åŒæ—¶ï¼Œè§†è§‰è´¨é‡å’Œç²¾ç»†çš„ä¿çœŸåº¦ä¹Ÿå¾—åˆ°äº†å¤§å¹…æé«˜ã€‚è¿™äº›ç»“æœè¯æ˜äº†IRGæ¡†æ¶åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚è‰ºæœ¯åˆ›ä½œã€äº§å“è®¾è®¡ã€æ¸¸æˆå¼€å‘ã€å¹¿å‘Šåˆ¶ä½œç­‰ã€‚é€šè¿‡æä¾›æ›´ç²¾ç¡®çš„æ–‡æœ¬æ§åˆ¶å’Œæ›´é€¼çœŸçš„å›¾åƒæ•ˆæœï¼Œå¯ä»¥æå¤§åœ°æå‡åˆ›ä½œæ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆã€3Dæ¨¡å‹ç”Ÿæˆç­‰é¢†åŸŸï¼Œä¸ºå†…å®¹åˆ›ä½œå¸¦æ¥æ›´å¤šå¯èƒ½æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

