---
layout: default
title: Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction
---

# Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction

**arXiv**: [2511.07122v1](https://arxiv.org/abs/2511.07122) | [PDF](https://arxiv.org/pdf/2511.07122.pdf)

**ä½œè€…**: Changyue Shi, Chuxiao Yang, Xinyuan Hu, Minghao Chen, Wenwen Pan, Yan Yang, Jiajun Ding, Zhou Yu, Jun Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparse4DGSä»¥è§£å†³ç¨€ç–å¸§åŠ¨æ€åœºæ™¯é‡å»ºé—®é¢˜**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯é‡å»º` `é«˜æ–¯æº…å°„` `ç¨€ç–å¸§å¤„ç†` `çº¹ç†æ„ŸçŸ¥ä¼˜åŒ–` `4Dé‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŠ¨æ€é‡å»ºæ–¹æ³•ä¾èµ–å¯†é›†å¸§ï¼Œç¨€ç–å¸§ä¸‹åœ¨è§„èŒƒä¸Žå˜å½¢ç©ºé—´å¤±æ•ˆï¼Œå°¤å…¶åœ¨çº¹ç†ä¸°å¯ŒåŒºåŸŸã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥çº¹ç†æ„ŸçŸ¥å˜å½¢æ­£åˆ™åŒ–å’Œè§„èŒƒä¼˜åŒ–ï¼Œé€šè¿‡çº¹ç†å¼•å¯¼é«˜æ–¯å˜å½¢ä¸Žæ¢¯åº¦ä¸‹é™ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰åŠ¨æ€æˆ–å°‘é‡å¸§æŠ€æœ¯ï¼ŒéªŒè¯ç¨€ç–å¸§é‡å»ºæœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dynamic Gaussian Splatting approaches have achieved remarkable performance
> for 4D scene reconstruction. However, these approaches rely on dense-frame
> video sequences for photorealistic reconstruction. In real-world scenarios, due
> to equipment constraints, sometimes only sparse frames are accessible. In this
> paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene
> reconstruction. We observe that dynamic reconstruction methods fail in both
> canonical and deformed spaces under sparse-frame settings, especially in areas
> with high texture richness. Sparse4DGS tackles this challenge by focusing on
> texture-rich areas. For the deformation network, we propose Texture-Aware
> Deformation Regularization, which introduces a texture-based depth alignment
> loss to regulate Gaussian deformation. For the canonical Gaussian field, we
> introduce Texture-Aware Canonical Optimization, which incorporates
> texture-based noise into the gradient descent process of canonical Gaussians.
> Extensive experiments show that when taking sparse frames as inputs, our method
> outperforms existing dynamic or few-shot techniques on NeRF-Synthetic,
> HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

