---
layout: default
title: Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction
---

# Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.07122" target="_blank" class="toolbar-btn">arXiv: 2511.07122v1</a>
    <a href="https://arxiv.org/pdf/2511.07122.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07122v1" 
            onclick="toggleFavorite(this, '2511.07122v1', 'Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Changyue Shi, Chuxiao Yang, Xinyuan Hu, Minghao Chen, Wenwen Pan, Yan Yang, Jiajun Ding, Zhou Yu, Jun Yu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-10

**Â§áÊ≥®**: AAAI 2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Sparse4DGSÔºöÊèêÂá∫Á∫πÁêÜÊÑüÁü•Ê≠£ÂàôÂåñ‰∏é‰ºòÂåñÔºåËß£ÂÜ≥Á®ÄÁñèÂ∏ßÂä®ÊÄÅÂú∫ÊôØÁöÑ4DÈ´òÊñØÈáçÂª∫ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Âä®ÊÄÅÂú∫ÊôØÈáçÂª∫` `4DÈ´òÊñØÊ∫ÖÂ∞Ñ` `Á®ÄÁñèÂ∏ß` `Á∫πÁêÜÊÑüÁü•` `ÂΩ¢ÂèòÊ≠£ÂàôÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂä®ÊÄÅÈ´òÊñØÊ∫ÖÂ∞ÑÊñπÊ≥ï‰æùËµñÂØÜÈõÜÂ∏ßËßÜÈ¢ëÂ∫èÂàóÔºåÂú®Á®ÄÁñèÂ∏ßÂú∫ÊôØ‰∏ãÈáçÂª∫ÊïàÊûú‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂Âú®Á∫πÁêÜ‰∏∞ÂØåÂå∫Âüü„ÄÇ
2. Sparse4DGSÈÄöËøáÁ∫πÁêÜÊÑüÁü•ÂΩ¢ÂèòÊ≠£ÂàôÂåñÂíåÁ∫πÁêÜÊÑüÁü•ËßÑËåÉ‰ºòÂåñÔºåÊèêÂçáÁ∫πÁêÜ‰∏∞ÂØåÂå∫ÂüüÁöÑÈáçÂª∫Ë¥®Èáè„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSparse4DGSÂú®Á®ÄÁñèÂ∏ßËæìÂÖ•‰∏ãÔºå‰ºò‰∫éÁé∞ÊúâÂä®ÊÄÅÊàñÂ∞ëÊ†∑Êú¨NeRFÊñπÊ≥ïÔºåÂπ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæóSOTAÁªìÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫Sparse4DGSÔºå‰∏ÄÁßçÁî®‰∫éÁ®ÄÁñèÂ∏ßÂä®ÊÄÅÂú∫ÊôØÈáçÂª∫ÁöÑÈ¶ñÂàõÊñπÊ≥ï„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂú®Á®ÄÁñèÂ∏ßÊù°‰ª∂‰∏ãÔºåÂä®ÊÄÅÈáçÂª∫ÊñπÊ≥ïÂú®ËßÑËåÉÁ©∫Èó¥ÂíåÂΩ¢ÂèòÁ©∫Èó¥ÂùáË°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Á∫πÁêÜ‰∏∞ÂØåÁöÑÂå∫Âüü„ÄÇSparse4DGSÈÄöËøáÂÖ≥Ê≥®Á∫πÁêÜ‰∏∞ÂØåÁöÑÂå∫ÂüüÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÊåëÊàò„ÄÇÈíàÂØπÂΩ¢ÂèòÁΩëÁªúÔºåÊèêÂá∫‰∫ÜÁ∫πÁêÜÊÑüÁü•ÂΩ¢ÂèòÊ≠£ÂàôÂåñÔºåÂºïÂÖ•Âü∫‰∫éÁ∫πÁêÜÁöÑÊ∑±Â∫¶ÂØπÈΩêÊçüÂ§±Êù•Á∫¶ÊùüÈ´òÊñØÂΩ¢Âèò„ÄÇÈíàÂØπËßÑËåÉÈ´òÊñØÂú∫ÔºåÂºïÂÖ•‰∫ÜÁ∫πÁêÜÊÑüÁü•ËßÑËåÉ‰ºòÂåñÔºåÂ∞ÜÂü∫‰∫éÁ∫πÁêÜÁöÑÂô™Â£∞ËûçÂÖ•ËßÑËåÉÈ´òÊñØÁöÑÊ¢ØÂ∫¶‰∏ãÈôçËøáÁ®ã‰∏≠„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÂú®‰ª•Á®ÄÁñèÂ∏ß‰Ωú‰∏∫ËæìÂÖ•Êó∂ÔºåËØ•ÊñπÊ≥ïÂú®NeRF-Synthetic„ÄÅHyperNeRF„ÄÅNeRF-DS‰ª•ÂèäiPhone-4DÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂä®ÊÄÅÊàñÂ∞ëÊ†∑Êú¨ÊäÄÊúØ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂä®ÊÄÅÂú∫ÊôØÈáçÂª∫ÊñπÊ≥ï‰æùËµñ‰∫éÂØÜÈõÜÁöÑËßÜÈ¢ëÂ∏ßÂ∫èÂàóÔºå‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÁî±‰∫éËÆæÂ§áÈôêÂà∂Á≠âÂéüÂõ†ÔºåÂæÄÂæÄÂè™ËÉΩËé∑ÂèñÁ®ÄÁñèÁöÑÂ∏ß„ÄÇÁõ¥Êé•Â∞ÜÁé∞ÊúâÊñπÊ≥ïÂ∫îÁî®‰∫éÁ®ÄÁñèÂ∏ß‰ºöÂØºËá¥ÈáçÂª∫Ë¥®ÈáèÊòæËëó‰∏ãÈôçÔºåÂ∞§ÂÖ∂ÊòØÂú®Á∫πÁêÜ‰∏∞ÂØåÁöÑÂå∫ÂüüÔºåÂõ†‰∏∫Áº∫‰πèË∂≥Â§üÁöÑÁ∫¶Êùü‰ø°ÊÅØÊù•ÂáÜÁ°Æ‰º∞ËÆ°ÂΩ¢ÂèòÂíåËßÑËåÉÁ©∫Èó¥‰∏≠ÁöÑÈ´òÊñØÂèÇÊï∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSparse4DGSÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Á∫πÁêÜ‰ø°ÊÅØÊù•ÊåáÂØºÂΩ¢ÂèòÂíåËßÑËåÉÁ©∫Èó¥ÁöÑ‰ºòÂåñËøáÁ®ã„ÄÇÈÄöËøáÂÖ≥Ê≥®Á∫πÁêÜ‰∏∞ÂØåÁöÑÂå∫ÂüüÔºåÂπ∂ÂºïÂÖ•Á∫πÁêÜÊÑüÁü•ÁöÑÊ≠£ÂàôÂåñÂíå‰ºòÂåñÁ≠ñÁï•ÔºåÊù•Âº•Ë°•Á®ÄÁñèÂ∏ßÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÁº∫Â§±Ôºå‰ªéËÄåÊèêÂçáÈáçÂª∫Ë¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÂÅáËÆæÁ∫πÁêÜ‰∏∞ÂØåÁöÑÂå∫ÂüüÂåÖÂê´Êõ¥Â§öÁöÑÂá†‰Ωï‰ø°ÊÅØÔºåÂõ†Ê≠§Â∫îËØ•Êõ¥Âä†ÂÖ≥Ê≥®Ëøô‰∫õÂå∫ÂüüÁöÑ‰ºòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSparse4DGSÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂΩ¢ÂèòÁΩëÁªúÂíåËßÑËåÉÈ´òÊñØÂú∫„ÄÇÂΩ¢ÂèòÁΩëÁªúË¥üË¥£Â∞ÜËßÑËåÉÁ©∫Èó¥‰∏≠ÁöÑÈ´òÊñØÊò†Â∞ÑÂà∞ËßÇÂØüÁ©∫Èó¥ÔºåËßÑËåÉÈ´òÊñØÂú∫ÂàôË¥üË¥£Ë°®Á§∫Âú∫ÊôØÁöÑÈùôÊÄÅÂá†‰ΩïÂíåÂ§ñËßÇ‰ø°ÊÅØ„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂåÖÊã¨Ôºö1Ôºâ‰ΩøÁî®Á®ÄÁñèÂ∏ß‰Ωú‰∏∫ËæìÂÖ•ÔºåÈÄöËøáÂΩ¢ÂèòÁΩëÁªúÈ¢ÑÊµãÊØè‰∏™È´òÊñØÁöÑÂΩ¢ÂèòÔºõ2ÔºâÂú®ËßÇÂØüÁ©∫Èó¥‰∏≠Ê∏≤ÊüìÂõæÂÉèÔºåÂπ∂ËÆ°ÁÆóÊ∏≤ÊüìÊçüÂ§±Ôºõ3Ôºâ‰ΩøÁî®Á∫πÁêÜÊÑüÁü•ÂΩ¢ÂèòÊ≠£ÂàôÂåñÊù•Á∫¶ÊùüÂΩ¢ÂèòÁΩëÁªúÁöÑÂ≠¶‰π†Ôºõ4Ôºâ‰ΩøÁî®Á∫πÁêÜÊÑüÁü•ËßÑËåÉ‰ºòÂåñÊù•Êõ¥Êñ∞ËßÑËåÉÈ´òÊñØÂú∫ÁöÑÂèÇÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSparse4DGSÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÁ∫πÁêÜÊÑüÁü•ÂΩ¢ÂèòÊ≠£ÂàôÂåñÂíåÁ∫πÁêÜÊÑüÁü•ËßÑËåÉ‰ºòÂåñ„ÄÇÁ∫πÁêÜÊÑüÁü•ÂΩ¢ÂèòÊ≠£ÂàôÂåñÈÄöËøáÂºïÂÖ•Âü∫‰∫éÁ∫πÁêÜÁöÑÊ∑±Â∫¶ÂØπÈΩêÊçüÂ§±ÔºåÊù•Á∫¶ÊùüÈ´òÊñØÂΩ¢ÂèòÔºå‰ªéËÄåÈÅøÂÖçËøáÊãüÂêà„ÄÇÁ∫πÁêÜÊÑüÁü•ËßÑËåÉ‰ºòÂåñÈÄöËøáÂ∞ÜÂü∫‰∫éÁ∫πÁêÜÁöÑÂô™Â£∞ËûçÂÖ•ËßÑËåÉÈ´òÊñØÁöÑÊ¢ØÂ∫¶‰∏ãÈôçËøáÁ®ã‰∏≠ÔºåÊù•ÊèêÂçáËßÑËåÉÈ´òÊñØÂú∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSparse4DGSËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Á∫πÁêÜ‰ø°ÊÅØÊù•Âº•Ë°•Á®ÄÁñèÂ∏ßÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÁº∫Â§±„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÁ∫πÁêÜÊÑüÁü•ÂΩ¢ÂèòÊ≠£ÂàôÂåñ‰∏≠ÁöÑÊ∑±Â∫¶ÂØπÈΩêÊçüÂ§±Âü∫‰∫éÁ∫πÁêÜÊ¢ØÂ∫¶ËÆ°ÁÆóÔºåÈºìÂä±ÂΩ¢ÂèòÂêéÁöÑÈ´òÊñØÊ∑±Â∫¶‰∏éÁõ∏ÈÇªÂÉèÁ¥†ÁöÑÊ∑±Â∫¶‰øùÊåÅ‰∏ÄËá¥„ÄÇÁ∫πÁêÜÊÑüÁü•ËßÑËåÉ‰ºòÂåñ‰∏≠ÔºåÁ∫πÁêÜÂô™Â£∞ÁöÑÂº∫Â∫¶‰∏éÁ∫πÁêÜÊ¢ØÂ∫¶ÊàêÊ≠£ÊØîÔºå‰ΩøÂæóÁ∫πÁêÜ‰∏∞ÂØåÁöÑÂå∫ÂüüËÉΩÂ§üËé∑ÂæóÊõ¥Â§ßÁöÑ‰ºòÂåñÂäõÂ∫¶„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÈááÁî®‰∫ÜËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÁ≠ñÁï•ÔºåÊ†πÊçÆÁ∫πÁêÜ‰ø°ÊÅØÂä®ÊÄÅË∞ÉÊï¥Â≠¶‰π†Áéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Sparse4DGSÂú®NeRF-Synthetic„ÄÅHyperNeRF„ÄÅNeRF-DSÂíåiPhone-4DÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSparse4DGSÂú®Á®ÄÁñèÂ∏ßÊù°‰ª∂‰∏ãÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂä®ÊÄÅNeRFÊñπÊ≥ïÂíåÂ∞ëÊ†∑Êú¨NeRFÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®iPhone-4DÊï∞ÊçÆÈõÜ‰∏äÔºåSparse4DGSÁöÑPSNRÊåáÊ†áÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫Ü2-3dB„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Sparse4DGSÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Âà©Áî®Á®ÄÁñèÁöÑ‰º†ÊÑüÂô®Êï∞ÊçÆÈáçÂª∫Âä®ÊÄÅÂú∫ÊôØÔºå‰ªéËÄåÈôç‰ΩéÂØπÁ°¨‰ª∂ËÆæÂ§áÁöÑË¶ÅÊ±ÇÔºåÂπ∂ÊèêÈ´òÁ≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄßÂíåÂÆûÊó∂ÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®Sparse4DGSÊù•ÈáçÂª∫Âä®ÊÄÅÁéØÂ¢ÉÔºå‰ªéËÄåÂ∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÂπ∂ÂÅöÂá∫ÂÜ≥Á≠ñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

