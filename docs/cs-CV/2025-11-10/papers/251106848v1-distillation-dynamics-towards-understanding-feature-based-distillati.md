---
layout: default
title: Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers
---

# Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers

**arXiv**: [2511.06848v1](https://arxiv.org/abs/2511.06848) | [PDF](https://arxiv.org/pdf/2511.06848.pdf)

**ä½œè€…**: Huiyuan Tian, Bonan Xu Shijian Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè’¸é¦åŠ¨åŠ›å­¦æ¡†æž¶ä»¥è§£å†³ViTç‰¹å¾è’¸é¦è´Ÿè¿ç§»é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰Transformer` `çŸ¥è¯†è’¸é¦` `ç‰¹å¾è’¸é¦` `è´Ÿè¿ç§»` `æ¨¡åž‹åŽ‹ç¼©` `è¡¨ç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç‰¹å¾è’¸é¦åœ¨ViTä¸­å¤±æ•ˆï¼Œå¯¼è‡´è´Ÿè¿ç§»ï¼Œæ€§èƒ½ä¸å¦‚ç®€å•logitè’¸é¦
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆé¢‘è°±åˆ†æžã€ä¿¡æ¯ç†µå’Œæ¿€æ´»å¹…åº¦è¿½è¸ªï¼Œæ­ç¤ºViTçš„Uå½¢ä¿¡æ¯å¤„ç†æ¨¡å¼
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°å¸ˆç”Ÿæ¨¡åž‹è¡¨ç¤ºèŒƒå¼ä¸åŒ¹é…ï¼Œéœ€è¶…è¶Šç‰¹å¾æ¨¡ä»¿è®¾è®¡åŽ‹ç¼©ç­–ç•¥

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While feature-based knowledge distillation has proven highly effective for
> compressing CNNs, these techniques unexpectedly fail when applied to Vision
> Transformers (ViTs), often performing worse than simple logit-based
> distillation. We provide the first comprehensive analysis of this phenomenon
> through a novel analytical framework termed as ``distillation dynamics",
> combining frequency spectrum analysis, information entropy metrics, and
> activation magnitude tracking. Our investigation reveals that ViTs exhibit a
> distinctive U-shaped information processing pattern: initial compression
> followed by expansion. We identify the root cause of negative transfer in
> feature distillation: a fundamental representational paradigm mismatch between
> teacher and student models. Through frequency-domain analysis, we show that
> teacher models employ distributed, high-dimensional encoding strategies in
> later layers that smaller student models cannot replicate due to limited
> channel capacity. This mismatch causes late-layer feature alignment to actively
> harm student performance. Our findings reveal that successful knowledge
> transfer in ViTs requires moving beyond naive feature mimicry to methods that
> respect these fundamental representational constraints, providing essential
> theoretical guidance for designing effective ViTs compression strategies. All
> source code and experimental logs are provided in the supplementary material.

