---
layout: default
title: Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots
---

# Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots

**arXiv**: [2511.06801v1](https://arxiv.org/abs/2511.06801) | [PDF](https://arxiv.org/pdf/2511.06801.pdf)

**ä½œè€…**: Praveen Kumar, Tushar Sandhan

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰è¾…åŠ©åœ¨çº¿A*è·¯å¾„è§„åˆ’æ¡†æ¶ï¼Œå®ç°æœåŠ¡æœºå™¨äººåœ¨äººæœºç¯å¢ƒä¸­çš„é«˜æ•ˆå®‰å…¨å¯¼èˆª**

**å…³é”®è¯**: `æœåŠ¡æœºå™¨äººå¯¼èˆª` `è¯­ä¹‰åˆ†å‰²` `åœ¨çº¿A*è§„åˆ’` `è§†è§‰çº¦æŸ` `å®æ—¶è·¯å¾„è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¯¼èˆªä¾èµ–æ¿€å…‰é›·è¾¾ï¼Œè¯­ä¹‰æ„ŸçŸ¥ä¸è¶³ï¼Œæ— æ³•åŒºåˆ†å…³é”®ç‰©ä½“ä¸æ™®é€šéšœç¢ç‰©
2. é›†æˆè½»é‡çº§è¯­ä¹‰åˆ†å‰²ä¸åœ¨çº¿A*è§„åˆ’å™¨ï¼Œå°†è§†è§‰çº¦æŸæŠ•å½±ä¸ºåœ°å›¾éšœç¢ç‰©
3. åœ¨ä»¿çœŸå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­éªŒè¯äº†å®æ—¶æ€§èƒ½å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¼èˆªèƒ½åŠ›

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The deployment of autonomous service robots in human-centric environments is
> hindered by a critical gap in perception and planning. Traditional navigation
> systems rely on expensive LiDARs that, while geometrically precise, are seman-
> tically unaware, they cannot distinguish a important document on an office
> floor from a harmless piece of litter, treating both as physically traversable.
> While advanced semantic segmentation exists, no prior work has successfully
> integrated this visual intelligence into a real-time path planner that is
> efficient enough for low-cost, embedded hardware. This paper presents a frame-
> work to bridge this gap, delivering context-aware navigation on an affordable
> robotic platform. Our approach centers on a novel, tight integration of a
> lightweight perception module with an online A* planner. The perception system
> employs a semantic segmentation model to identify user-defined visual
> constraints, enabling the robot to navigate based on contextual importance
> rather than physical size alone. This adaptability allows an operator to define
> what is critical for a given task, be it sensitive papers in an office or
> safety lines in a factory, thus resolving the ambiguity of what to avoid. This
> semantic perception is seamlessly fused with geometric data. The identified
> visual constraints are projected as non-geometric obstacles onto a global map
> that is continuously updated from sensor data, enabling robust navigation
> through both partially known and unknown environments. We validate our
> framework through extensive experiments in high-fidelity simulations and on a
> real-world robotic platform. The results demonstrate robust, real-time
> performance, proving that a cost- effective robot can safely navigate complex
> environments while respecting critical visual cues invisible to traditional
> planners.

