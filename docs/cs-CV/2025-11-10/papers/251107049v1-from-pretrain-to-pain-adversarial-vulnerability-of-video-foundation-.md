---
layout: default
title: From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge
---

# From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge

**arXiv**: [2511.07049v1](https://arxiv.org/abs/2511.07049) | [PDF](https://arxiv.org/pdf/2511.07049.pdf)

**ä½œè€…**: Hui Lu, Yi Yu, Song Xia, Yiming Yang, Deepu Rajan, Boon Poh Ng, Alex Kot, Xudong Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTVAæ–¹æ³•ä»¥æ”»å‡»è§†é¢‘åŸºç¡€æ¨¡åž‹ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ— éœ€ä»»åŠ¡çŸ¥è¯†**

**å…³é”®è¯**: `è§†é¢‘åŸºç¡€æ¨¡åž‹` `å¯¹æŠ—æ”»å‡»` `æ—¶é—´ä¸€è‡´æ€§` `åŒå‘å¯¹æ¯”å­¦ä¹ ` `è¿ç§»æ”»å‡»` `å®‰å…¨æ¼æ´ž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘åŸºç¡€æ¨¡åž‹å¼€æ”¾å¯è®¿é—®æ€§å¸¦æ¥å®‰å…¨é£Žé™©ï¼Œæ”»å‡»è€…æ— éœ€ä»»åŠ¡æ•°æ®å³å¯åˆ©ç”¨æ¼æ´ž
2. æ–¹æ³•è¦ç‚¹ï¼šTVAåˆ©ç”¨æ—¶é—´è¡¨ç¤ºåŠ¨æ€ï¼Œç»“åˆåŒå‘å¯¹æ¯”å­¦ä¹ å’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±å¢žå¼ºæ‰°åŠ¨æ•ˆæžœ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨24ä¸ªè§†é¢‘ä»»åŠ¡ä¸­éªŒè¯TVAå¯¹ä¸‹æ¸¸æ¨¡åž‹å’ŒMLLMsçš„æœ‰æ•ˆæ”»å‡»

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large-scale Video Foundation Models (VFMs) has significantly advanced various
> video-related tasks, either through task-specific models or Multi-modal Large
> Language Models (MLLMs). However, the open accessibility of VFMs also
> introduces critical security risks, as adversaries can exploit full knowledge
> of the VFMs to launch potent attacks. This paper investigates a novel and
> practical adversarial threat scenario: attacking downstream models or MLLMs
> fine-tuned from open-source VFMs, without requiring access to the victim task,
> training data, model query, and architecture. In contrast to conventional
> transfer-based attacks that rely on task-aligned surrogate models, we
> demonstrate that adversarial vulnerabilities can be exploited directly from the
> VFMs. To this end, we propose the Transferable Video Attack (TVA), a
> temporal-aware adversarial attack method that leverages the temporal
> representation dynamics of VFMs to craft effective perturbations. TVA
> integrates a bidirectional contrastive learning mechanism to maximize the
> discrepancy between the clean and adversarial features, and introduces a
> temporal consistency loss that exploits motion cues to enhance the sequential
> impact of perturbations. TVA avoids the need to train expensive surrogate
> models or access to domain-specific data, thereby offering a more practical and
> efficient attack strategy. Extensive experiments across 24 video-related tasks
> demonstrate the efficacy of TVA against downstream models and MLLMs, revealing
> a previously underexplored security vulnerability in the deployment of video
> models.

