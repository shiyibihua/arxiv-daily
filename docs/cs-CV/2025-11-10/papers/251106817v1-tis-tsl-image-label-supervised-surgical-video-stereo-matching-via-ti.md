---
layout: default
title: TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning
---

# TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning

**arXiv**: [2511.06817v1](https://arxiv.org/abs/2511.06817) | [PDF](https://arxiv.org/pdf/2511.06817.pdf)

**ä½œè€…**: Rui Wang, Ying Zhou, Hao Wang, Wenwei Zhang, Qiang Li, Zhiwei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´å¯åˆ‡æ¢å¸ˆç”Ÿå­¦ä¹ æ¡†æž¶ä»¥è§£å†³å¾®åˆ›æ‰‹æœ¯è§†é¢‘ç«‹ä½“åŒ¹é…ä¸­çš„æ—¶ç©ºä¸ä¸€è‡´é—®é¢˜**

**å…³é”®è¯**: `ç«‹ä½“åŒ¹é…` `å¸ˆç”Ÿå­¦ä¹ ` `å¾®åˆ›æ‰‹æœ¯` `æ—¶ç©ºä¸€è‡´æ€§` `ä¼ªæ ‡ç­¾è¿‡æ»¤` `è§†é¢‘é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¾®åˆ›æ‰‹æœ¯ä¸­å¯†é›†è§†å·®ç›‘ç£ç¨€ç¼ºï¼ŒçŽ°æœ‰æ–¹æ³•ç¼ºä¹æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¯¼è‡´é¢„æµ‹ä¸ç¨³å®šå’Œé—ªçƒä¼ªå½±
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç»Ÿä¸€æ¨¡åž‹æ”¯æŒå›¾åƒå’Œè§†é¢‘é¢„æµ‹æ¨¡å¼ï¼Œé€šè¿‡åŒå‘æ—¶ç©ºä¸€è‡´æ€§è¿‡æ»¤ä¼ªæ ‡ç­¾å¹¶å¢žå¼ºæ—¶é—´è¿žè´¯æ€§
3. å®žéªŒæ•ˆæžœï¼šåœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼ŒTEPEå’ŒEPEæŒ‡æ ‡åˆ†åˆ«æå‡è‡³å°‘2.11%å’Œ4.54%ï¼Œä¼˜äºŽçŽ°æœ‰å›¾åƒçº§æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Stereo matching in minimally invasive surgery (MIS) is essential for
> next-generation navigation and augmented reality. Yet, dense disparity
> supervision is nearly impossible due to anatomical constraints, typically
> limiting annotations to only a few image-level labels acquired before the
> endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a
> promising solution by leveraging a teacher trained on sparse labels to generate
> pseudo labels and associated confidence maps from abundant unlabeled surgical
> videos. However, existing TSL methods are confined to image-level supervision,
> providing only spatial confidence and lacking temporal consistency estimation.
> This absence of spatio-temporal reliability results in unstable disparity
> predictions and severe flickering artifacts across video frames. To overcome
> these challenges, we propose TiS-TSL, a novel time-switchable teacher-student
> learning framework for video stereo matching under minimal supervision. At its
> core is a unified model that operates in three distinct modes: Image-Prediction
> (IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP),
> enabling flexible temporal modeling within a single architecture. Enabled by
> this unified model, TiS-TSL adopts a two-stage learning strategy. The
> Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize
> temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal
> disparity predictions by comparing forward and backward predictions to
> calculate bidirectional spatio-temporal consistency. This consistency
> identifies unreliable regions across frames, filters noisy video-level pseudo
> labels, and enforces temporal coherence. Experimental results on two public
> datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts
> by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..

