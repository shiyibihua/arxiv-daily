---
layout: default
title: CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video
---

# CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video

**arXiv**: [2511.07290v1](https://arxiv.org/abs/2511.07290) | [PDF](https://arxiv.org/pdf/2511.07290.pdf)

**ä½œè€…**: Xinyi Wang, Angeliki Katsenou, Junxiao Shen, David Bull

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAMP-VQAæ¡†æž¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ„ŸçŸ¥è¯„ä¼°åŽ‹ç¼©è§†é¢‘è´¨é‡ï¼Œä¼˜åŒ–ç”¨æˆ·ç”Ÿæˆå†…å®¹äº¤ä»˜ã€‚**

**å…³é”®è¯**: `æ— å‚è€ƒè§†é¢‘è´¨é‡è¯„ä¼°` `å¤šæ¨¡æ€æ„ŸçŸ¥` `ç”¨æˆ·ç”Ÿæˆå†…å®¹` `åŽ‹ç¼©è§†é¢‘` `è´¨é‡æ„ŸçŸ¥æç¤º` `BLIP-2æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç”¨æˆ·ç”Ÿæˆè§†é¢‘çš„éžä¸“ä¸šé‡‡é›†ä¸Žè½¬ç å¯¼è‡´æ— å‚è€ƒè´¨é‡è¯„ä¼°å›°éš¾ï¼Œç¼ºä¹ç»†ç²’åº¦ä¼ªå½±æ ‡æ³¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè§†é¢‘å…ƒæ•°æ®å’Œå¸§é—´å˜åŒ–ï¼Œé€šè¿‡è´¨é‡æ„ŸçŸ¥æç¤ºç”Ÿæˆç»†ç²’åº¦è´¨é‡æè¿°ï¼Œèžåˆå¤šæ¨¡æ€ç‰¹å¾å›žå½’è´¨é‡åˆ†æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒSRCCè¾¾0.928ï¼Œæ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The prevalence of user-generated content (UGC) on platforms such as YouTube
> and TikTok has rendered no-reference (NR) perceptual video quality assessment
> (VQA) vital for optimizing video delivery. Nonetheless, the characteristics of
> non-professional acquisition and the subsequent transcoding of UGC video on
> sharing platforms present significant challenges for NR-VQA. Although NR-VQA
> models attempt to infer mean opinion scores (MOS), their modeling of subjective
> scores for compressed content remains limited due to the absence of
> fine-grained perceptual annotations of artifact types. To address these
> challenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the
> semantic understanding capabilities of large vision-language models. Our
> approach introduces a quality-aware prompting mechanism that integrates video
> metadata (e.g., resolution, frame rate, bitrate) with key fragments extracted
> from inter-frame variations to guide the BLIP-2 pretraining approach in
> generating fine-grained quality captions. A unified architecture has been
> designed to model perceptual quality across three dimensions: semantic
> alignment, temporal characteristics, and spatial characteristics. These
> multimodal features are extracted and fused, then regressed to video quality
> scores. Extensive experiments on a wide variety of UGC datasets demonstrate
> that our model consistently outperforms existing NR-VQA methods, achieving
> improved accuracy without the need for costly manual fine-grained annotations.
> Our method achieves the best performance in terms of average rank and linear
> correlation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods.
> The source code and trained models, along with a user-friendly demo, are
> available at: https://github.com/xinyiW915/CAMP-VQA.

