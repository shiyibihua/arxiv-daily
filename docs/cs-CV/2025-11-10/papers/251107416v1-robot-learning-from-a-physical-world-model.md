---
layout: default
title: Robot Learning from a Physical World Model
---

# Robot Learning from a Physical World Model

**arXiv**: [2511.07416v1](https://arxiv.org/abs/2511.07416) | [PDF](https://arxiv.org/pdf/2511.07416.pdf)

**ä½œè€…**: Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, Yue Wang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPhysWorldæ¡†æ¶ï¼Œé€šè¿‡ç‰©ç†ä¸–ç•Œå»ºæ¨¡ä»ç”Ÿæˆè§†é¢‘ä¸­å­¦ä¹ æœºå™¨äººæ“ä½œã€‚**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `ç‰©ç†ä¸–ç•Œå»ºæ¨¡` `è§†é¢‘ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `é›¶æ ·æœ¬æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹ç›´æ¥ç”¨äºæœºå™¨äººå­¦ä¹ å¿½ç•¥ç‰©ç†çº¦æŸï¼Œå¯¼è‡´æ“ä½œä¸å‡†ç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè§†é¢‘ç”Ÿæˆä¸ç‰©ç†ä¸–ç•Œé‡å»ºï¼Œä½¿ç”¨å¯¹è±¡ä¸­å¿ƒæ®‹å·®å¼ºåŒ–å­¦ä¹ å°†è§†é¢‘è¿åŠ¨è½¬åŒ–ä¸ºç‰©ç†å‡†ç¡®åŠ¨ä½œã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å¤šæ ·çœŸå®ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ“ä½œç²¾åº¦ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce PhysWorld, a framework that enables robot learning from video
> generation through physical world modeling. Recent video generation models can
> synthesize photorealistic visual demonstrations from language commands and
> images, offering a powerful yet underexplored source of training signals for
> robotics. However, directly retargeting pixel motions from generated videos to
> robots neglects physics, often resulting in inaccurate manipulations. PhysWorld
> addresses this limitation by coupling video generation with physical world
> reconstruction. Given a single image and a task command, our method generates
> task-conditioned videos and reconstructs the underlying physical world from the
> videos, and the generated video motions are grounded into physically accurate
> actions through object-centric residual reinforcement learning with the
> physical world model. This synergy transforms implicit visual guidance into
> physically executable robotic trajectories, eliminating the need for real robot
> data collection and enabling zero-shot generalizable robotic manipulation.
> Experiments on diverse real-world tasks demonstrate that PhysWorld
> substantially improves manipulation accuracy compared to previous approaches.
> Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage}
> for details.

