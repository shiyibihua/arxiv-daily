---
layout: default
title: MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs
---

# MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs

**arXiv**: [2511.07250v1](https://arxiv.org/abs/2511.07250) | [PDF](https://arxiv.org/pdf/2511.07250.pdf)

**ä½œè€…**: Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMVU-EvalåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨å¤šè§†é¢‘ç†è§£ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `å¤šè§†é¢‘ç†è§£` `åŸºå‡†è¯„ä¼°` `é—®ç­”å¯¹` `æ€§èƒ½åˆ†æž` `çœŸå®žåœºæ™¯åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†å±€é™äºŽå•è§†é¢‘ç†è§£ï¼Œæ— æ³•æ»¡è¶³å¤šè§†é¢‘åœºæ™¯éœ€æ±‚
2. æž„å»ºåŒ…å«1,824ä¸ªé—®ç­”å¯¹å’Œ4,959ä¸ªè§†é¢‘çš„åŸºå‡†ï¼Œè¯„ä¼°å…«é¡¹æ ¸å¿ƒèƒ½åŠ›
3. å®žéªŒæ˜¾ç¤ºå½“å‰æ¨¡åž‹åœ¨å¤šè§†é¢‘ç†è§£ä¸­å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The advent of Multimodal Large Language Models (MLLMs) has expanded AI
> capabilities to visual modalities, yet existing evaluation benchmarks remain
> limited to single-video understanding, overlooking the critical need for
> multi-video understanding in real-world scenarios (e.g., sports analytics and
> autonomous driving). To address this significant gap, we introduce MVU-Eval,
> the first comprehensive benchmark for evaluating Multi-Video Understanding for
> MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies
> through 1,824 meticulously curated question-answer pairs spanning 4,959 videos
> from diverse domains, addressing both fundamental perception tasks and
> high-order reasoning tasks. These capabilities are rigorously aligned with
> real-world applications such as multi-sensor synthesis in autonomous systems
> and cross-angle sports analytics. Through extensive evaluation of
> state-of-the-art open-source and closed-source models, we reveal significant
> performance discrepancies and limitations in current MLLMs' ability to perform
> understanding across multiple videos. The benchmark will be made publicly
> available to foster future research.

