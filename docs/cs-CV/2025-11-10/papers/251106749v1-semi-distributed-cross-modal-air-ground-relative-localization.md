---
layout: default
title: Semi-distributed Cross-modal Air-Ground Relative Localization
---

# Semi-distributed Cross-modal Air-Ground Relative Localization

**arXiv**: [2511.06749v1](https://arxiv.org/abs/2511.06749) | [PDF](https://arxiv.org/pdf/2511.06749.pdf)

**ä½œè€…**: Weining Lu, Deer Bin, Lian Ma, Ming Ma, Zhihao Ma, Xiangyang Chen, Longfei Wang, Yixiao Feng, Zhouxian Jiang, Yongliang Shi, Bin Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠåˆ†å¸ƒå¼è·¨æ¨¡æ€ç©ºåœ°ç›¸å¯¹å®šä½æ¡†æž¶ï¼Œä»¥æå‡åä½œä»»åŠ¡ä¸­çš„çµæ´»æ€§ä¸Žå‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `ç›¸å¯¹å®šä½` `è·¨æ¨¡æ€SLAM` `åŠåˆ†å¸ƒå¼ç³»ç»Ÿ` `æ·±åº¦å­¦ä¹ å…³é”®ç‚¹` `é€šä¿¡å¸¦å®½ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå¤šæœºå™¨äººSLAMç³»ç»Ÿä¼ æ„Ÿå™¨é…ç½®ç›¸åŒï¼Œè€¦åˆçŠ¶æ€ä¼°è®¡ï¼Œé™åˆ¶çµæ´»æ€§ä¸Žå‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šUGVå’ŒUAVç‹¬ç«‹SLAMï¼Œæå–æ·±åº¦å­¦ä¹ å…³é”®ç‚¹ä¸Žæè¿°ç¬¦ï¼Œè§£è€¦ç›¸å¯¹å®šä½ä¸ŽçŠ¶æ€ä¼°è®¡ã€‚
3. å®žéªŒæ•ˆæžœï¼šæ–¹æ³•åœ¨ç²¾åº¦å’Œæ•ˆçŽ‡ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œé€šä¿¡å¸¦å®½æŽ§åˆ¶åœ¨0.3 Mbpsä»¥ä¸‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Efficient, accurate, and flexible relative localization is crucial in
> air-ground collaborative tasks. However, current approaches for robot relative
> localization are primarily realized in the form of distributed multi-robot SLAM
> systems with the same sensor configuration, which are tightly coupled with the
> state estimation of all robots, limiting both flexibility and accuracy. To this
> end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to
> integrate multiple sensors, enabling a semi-distributed cross-modal air-ground
> relative localization framework. In this work, both the UGV and the Unmanned
> Aerial Vehicle (UAV) independently perform SLAM while extracting deep
> learning-based keypoints and global descriptors, which decouples the relative
> localization from the state estimation of all agents. The UGV employs a local
> Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain
> accurate relative pose estimates. The BA process adopts sparse keypoint
> optimization and is divided into two stages: First, optimizing camera poses
> interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the
> relative camera poses between the UGV and UAV. Additionally, we implement an
> incremental loop closure detection algorithm using deep learning-based
> descriptors to maintain and retrieve keyframes efficiently. Experimental
> results demonstrate that our method achieves outstanding performance in both
> accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that
> transmit images or point clouds, our method only transmits keypoint pixels and
> their descriptors, effectively constraining the communication bandwidth under
> 0.3 Mbps. Codes and data will be publicly available on
> https://github.com/Ascbpiac/cross-model-relative-localization.git.

