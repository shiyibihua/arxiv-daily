---
layout: default
title: Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding
---

# Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding

**arXiv**: [2511.06908v1](https://arxiv.org/abs/2511.06908) | [PDF](https://arxiv.org/pdf/2511.06908.pdf)

**ä½œè€…**: Yuzhen Li, Min Liu, Zhaoyang Li, Yuan Bian, Xueping Wang, Erbo Zhai, Yaonan Wang

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: 10 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMono3DVG-EnSDæ¡†æž¶ï¼Œå¢žå¼ºå•ç›®3Dè§†è§‰å®šä½ä¸­ç©ºé—´æ„ŸçŸ¥å’Œç»´åº¦è§£è€¦çš„æ–‡æœ¬ç¼–ç ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å•ç›®3Dè§†è§‰å®šä½` `è§†è§‰å®šä½` `è‡ªç„¶è¯­è¨€å¤„ç†` `è·¨æ¨¡æ€èžåˆ` `ç©ºé—´å…³ç³»æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰Mono3DVGæ–¹æ³•è¿‡åº¦ä¾èµ–é«˜ç¡®å®šæ€§å…³é”®è¯ï¼Œå¿½ç•¥äº†æ–‡æœ¬ä¸­è•´å«çš„ç©ºé—´å…³ç³»æè¿°ã€‚
2. Mono3DVG-EnSDæ¡†æž¶é€šè¿‡CLIP-LCAå’ŒD2Mæ¨¡å—ï¼Œåˆ†åˆ«å¢žå¼ºç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å’Œè§£è€¦è·¨ç»´åº¦ç‰¹å¾ã€‚
3. åœ¨Mono3DReferæ•°æ®é›†ä¸Šï¼ŒMono3DVG-EnSDåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡å–å¾—SOTAï¼Œè¿œè·ç¦»å®šä½ç²¾åº¦æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•ç›®3Dè§†è§‰å®šä½(Mono3DVG)æ¡†æž¶Mono3DVG-EnSDï¼Œæ—¨åœ¨è§£å†³çŽ°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–é«˜ç¡®å®šæ€§å…³é”®è¯è€Œå¿½ç•¥ç©ºé—´æè¿°ï¼Œä»¥åŠé€šç”¨æ–‡æœ¬ç‰¹å¾ä¸­2D/3Dä¿¡æ¯æ··åˆå¯¼è‡´è·¨ç»´åº¦å¹²æ‰°çš„é—®é¢˜ã€‚è¯¥æ¡†æž¶é›†æˆäº†CLIPå¼•å¯¼çš„è¯æ±‡ç¡®å®šæ€§é€‚é…å™¨(CLIP-LCA)å’Œç»´åº¦è§£è€¦æ¨¡å—(D2M)ã€‚CLIP-LCAåŠ¨æ€å±è”½é«˜ç¡®å®šæ€§å…³é”®è¯ï¼Œä¿ç•™ä½Žç¡®å®šæ€§ç©ºé—´æè¿°ï¼Œä¿ƒä½¿æ¨¡åž‹æ›´æ·±å…¥ç†è§£æ–‡æœ¬ä¸­çš„ç©ºé—´å…³ç³»ã€‚D2Mä»Žé€šç”¨æ–‡æœ¬ç‰¹å¾ä¸­è§£è€¦ç»´åº¦ç‰¹å®šçš„(2D/3D)æ–‡æœ¬ç‰¹å¾ï¼Œä»¥æŒ‡å¯¼ç›¸åº”ç»´åº¦çš„è§†è§‰ç‰¹å¾ï¼Œä»Žè€Œå‡è½»è·¨ç»´åº¦å¹²æ‰°ã€‚åœ¨Mono3DReferæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è¿œè·ç¦»(Far(Acc@0.5))åœºæ™¯ä¸‹ï¼Œæ€§èƒ½æå‡äº†+13.54%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå•ç›®3Dè§†è§‰å®šä½æ—¨åœ¨åˆ©ç”¨æ–‡æœ¬æè¿°åœ¨RGBå›¾åƒä¸­å®šä½3Dç‰©ä½“ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽï¼š1)è¿‡åº¦ä¾èµ–æ–‡æœ¬ä¸­æ˜¾å¼çš„é«˜ç¡®å®šæ€§å…³é”®è¯ï¼Œå¿½ç•¥äº†éšå¼çš„ç©ºé—´å…³ç³»æè¿°ï¼›2)é€šç”¨æ–‡æœ¬ç‰¹å¾æ··åˆäº†2Då’Œ3Dä¿¡æ¯ï¼Œä¸Žå•ä¸€ç»´åº¦çš„è§†è§‰ç‰¹å¾èžåˆæ—¶ä¼šäº§ç”Ÿè·¨ç»´åº¦å¹²æ‰°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¢žå¼ºæ¨¡åž‹å¯¹æ–‡æœ¬ä¸­ç©ºé—´å…³ç³»çš„ç†è§£ï¼Œå¹¶è§£è€¦æ–‡æœ¬ç‰¹å¾ä¸­çš„2Då’Œ3Dä¿¡æ¯ï¼Œä»Žè€Œæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼è§†è§‰ç‰¹å¾çš„å­¦ä¹ ã€‚é€šè¿‡CLIP-LCAæ¨¡å—å…³æ³¨ç©ºé—´æè¿°ï¼Œé€šè¿‡D2Mæ¨¡å—é¿å…è·¨ç»´åº¦å¹²æ‰°ï¼Œæå‡å®šä½ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMono3DVG-EnSDæ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šCLIP-Guided Lexical Certainty Adapter (CLIP-LCA) å’Œ Dimension-Decoupled Module (D2M)ã€‚é¦–å…ˆï¼ŒCLIP-LCAç”¨äºŽåŠ¨æ€è°ƒæ•´æ–‡æœ¬ç‰¹å¾ï¼Œçªå‡ºç©ºé—´æè¿°ã€‚ç„¶åŽï¼ŒD2Må°†é€šç”¨æ–‡æœ¬ç‰¹å¾åˆ†è§£ä¸º2Då’Œ3Dç‰¹å®šç‰¹å¾ï¼Œåˆ†åˆ«ä¸Žå¯¹åº”çš„2Då’Œ3Dè§†è§‰ç‰¹å¾è¿›è¡Œèžåˆã€‚æœ€åŽï¼ŒèžåˆåŽçš„ç‰¹å¾ç”¨äºŽé¢„æµ‹3Dç‰©ä½“çš„ä½ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1)æå‡ºäº†CLIP-LCAæ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€å±è”½é«˜ç¡®å®šæ€§å…³é”®è¯ï¼Œè¿«ä½¿æ¨¡åž‹å­¦ä¹ æ–‡æœ¬ä¸­æ›´ä¸°å¯Œçš„ç©ºé—´å…³ç³»ï¼›2)æå‡ºäº†D2Mæ¨¡å—ï¼Œé€šè¿‡è§£è€¦æ–‡æœ¬ç‰¹å¾ä¸­çš„2Då’Œ3Dä¿¡æ¯ï¼Œé¿å…äº†è·¨ç»´åº¦å¹²æ‰°ï¼Œæå‡äº†ç‰¹å¾èžåˆçš„æ•ˆçŽ‡ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´å…³æ³¨æ–‡æœ¬ä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶è§£å†³äº†è·¨ç»´åº¦ç‰¹å¾èžåˆçš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šCLIP-LCAæ¨¡å—åˆ©ç”¨CLIPæ¨¡åž‹è®¡ç®—æ–‡æœ¬ä¸­æ¯ä¸ªè¯çš„ç¡®å®šæ€§å¾—åˆ†ï¼Œå¹¶æ ¹æ®å¾—åˆ†åŠ¨æ€è°ƒæ•´è¯çš„æƒé‡ã€‚D2Mæ¨¡å—ä½¿ç”¨çº¿æ€§å±‚å°†é€šç”¨æ–‡æœ¬ç‰¹å¾åˆ†è§£ä¸º2Då’Œ3Dç‰¹å®šç‰¹å¾ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å®šä½æŸå¤±å’Œåˆ†ç±»æŸå¤±ï¼Œç”¨äºŽä¼˜åŒ–æ¨¡åž‹çš„å®šä½å’Œè¯†åˆ«èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMono3DVG-EnSDåœ¨Mono3DReferæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¿œè·ç¦»(Far(Acc@0.5))åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”çŽ°æœ‰æœ€ä½³æ–¹æ³•æå‡äº†+13.54%ã€‚è¿™è¡¨æ˜Žè¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œè¿œè·ç¦»ç‰©ä½“å®šä½æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨å¤æ‚çŽ¯å¢ƒä¸­å®šä½ç›®æ ‡ç‰©ä½“ï¼›è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥æ ¹æ®ä¹˜å®¢çš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«å¹¶å®šä½è½¦è¾†å‘¨å›´çš„ç‰¹å®šç‰©ä½“ï¼›å¢žå¼ºçŽ°å®žåº”ç”¨å¯ä»¥æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°åœ¨çœŸå®žåœºæ™¯ä¸­å åŠ è™šæ‹Ÿç‰©ä½“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.

