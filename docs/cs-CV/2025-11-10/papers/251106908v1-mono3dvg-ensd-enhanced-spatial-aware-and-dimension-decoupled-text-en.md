---
layout: default
title: Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding
---

# Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.06908" target="_blank" class="toolbar-btn">arXiv: 2511.06908v1</a>
    <a href="https://arxiv.org/pdf/2511.06908.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06908v1" 
            onclick="toggleFavorite(this, '2511.06908v1', 'Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yuzhen Li, Min Liu, Zhaoyang Li, Yuan Bian, Xueping Wang, Erbo Zhai, Yaonan Wang

**ÂàÜÁ±ª**: cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-10

**Â§áÊ≥®**: 10 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Mono3DVG-EnSDÊ°ÜÊû∂ÔºåÂ¢ûÂº∫ÂçïÁõÆ3DËßÜËßâÂÆö‰Ωç‰∏≠Á©∫Èó¥ÊÑüÁü•ÂíåÁª¥Â∫¶Ëß£ËÄ¶ÁöÑÊñáÊú¨ÁºñÁ†Å„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂçïÁõÆ3DËßÜËßâÂÆö‰Ωç` `ËßÜËßâÂÆö‰Ωç` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Ë∑®Ê®°ÊÄÅËûçÂêà` `Á©∫Èó¥ÂÖ≥Á≥ªÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMono3DVGÊñπÊ≥ïËøáÂ∫¶‰æùËµñÈ´òÁ°ÆÂÆöÊÄßÂÖ≥ÈîÆËØçÔºåÂøΩÁï•‰∫ÜÊñáÊú¨‰∏≠Ëï¥Âê´ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÊèèËø∞„ÄÇ
2. Mono3DVG-EnSDÊ°ÜÊû∂ÈÄöËøáCLIP-LCAÂíåD2MÊ®°ÂùóÔºåÂàÜÂà´Â¢ûÂº∫Á©∫Èó¥ÊÑüÁü•ËÉΩÂäõÂíåËß£ËÄ¶Ë∑®Áª¥Â∫¶ÁâπÂæÅ„ÄÇ
3. Âú®Mono3DReferÊï∞ÊçÆÈõÜ‰∏äÔºåMono3DVG-EnSDÂú®ÂêÑÈ°πÊåáÊ†á‰∏äÂùáÂèñÂæóSOTAÔºåËøúË∑ùÁ¶ªÂÆö‰ΩçÁ≤æÂ∫¶ÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂçïÁõÆ3DËßÜËßâÂÆö‰Ωç(Mono3DVG)Ê°ÜÊû∂Mono3DVG-EnSDÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïËøáÂ∫¶‰æùËµñÈ´òÁ°ÆÂÆöÊÄßÂÖ≥ÈîÆËØçËÄåÂøΩÁï•Á©∫Èó¥ÊèèËø∞Ôºå‰ª•ÂèäÈÄöÁî®ÊñáÊú¨ÁâπÂæÅ‰∏≠2D/3D‰ø°ÊÅØÊ∑∑ÂêàÂØºËá¥Ë∑®Áª¥Â∫¶Âπ≤Êâ∞ÁöÑÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÈõÜÊàê‰∫ÜCLIPÂºïÂØºÁöÑËØçÊ±áÁ°ÆÂÆöÊÄßÈÄÇÈÖçÂô®(CLIP-LCA)ÂíåÁª¥Â∫¶Ëß£ËÄ¶Ê®°Âùó(D2M)„ÄÇCLIP-LCAÂä®ÊÄÅÂ±èËîΩÈ´òÁ°ÆÂÆöÊÄßÂÖ≥ÈîÆËØçÔºå‰øùÁïô‰ΩéÁ°ÆÂÆöÊÄßÁ©∫Èó¥ÊèèËø∞Ôºå‰øÉ‰ΩøÊ®°ÂûãÊõ¥Ê∑±ÂÖ•ÁêÜËß£ÊñáÊú¨‰∏≠ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇD2M‰ªéÈÄöÁî®ÊñáÊú¨ÁâπÂæÅ‰∏≠Ëß£ËÄ¶Áª¥Â∫¶ÁâπÂÆöÁöÑ(2D/3D)ÊñáÊú¨ÁâπÂæÅÔºå‰ª•ÊåáÂØºÁõ∏Â∫îÁª¥Â∫¶ÁöÑËßÜËßâÁâπÂæÅÔºå‰ªéËÄåÂáèËΩªË∑®Áª¥Â∫¶Âπ≤Êâ∞„ÄÇÂú®Mono3DReferÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÊâÄÊúâÊåáÊ†á‰∏äÂùáËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ËøúË∑ùÁ¶ª(Far(Acc@0.5))Âú∫ÊôØ‰∏ãÔºåÊÄßËÉΩÊèêÂçá‰∫Ü+13.54%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂçïÁõÆ3DËßÜËßâÂÆö‰ΩçÊó®Âú®Âà©Áî®ÊñáÊú¨ÊèèËø∞Âú®RGBÂõæÂÉè‰∏≠ÂÆö‰Ωç3DÁâ©‰Ωì„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÔºö1)ËøáÂ∫¶‰æùËµñÊñáÊú¨‰∏≠ÊòæÂºèÁöÑÈ´òÁ°ÆÂÆöÊÄßÂÖ≥ÈîÆËØçÔºåÂøΩÁï•‰∫ÜÈöêÂºèÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÊèèËø∞Ôºõ2)ÈÄöÁî®ÊñáÊú¨ÁâπÂæÅÊ∑∑Âêà‰∫Ü2DÂíå3D‰ø°ÊÅØÔºå‰∏éÂçï‰∏ÄÁª¥Â∫¶ÁöÑËßÜËßâÁâπÂæÅËûçÂêàÊó∂‰ºö‰∫ßÁîüË∑®Áª¥Â∫¶Âπ≤Êâ∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ¢ûÂº∫Ê®°ÂûãÂØπÊñáÊú¨‰∏≠Á©∫Èó¥ÂÖ≥Á≥ªÁöÑÁêÜËß£ÔºåÂπ∂Ëß£ËÄ¶ÊñáÊú¨ÁâπÂæÅ‰∏≠ÁöÑ2DÂíå3D‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ÊåáÂØºËßÜËßâÁâπÂæÅÁöÑÂ≠¶‰π†„ÄÇÈÄöËøáCLIP-LCAÊ®°ÂùóÂÖ≥Ê≥®Á©∫Èó¥ÊèèËø∞ÔºåÈÄöËøáD2MÊ®°ÂùóÈÅøÂÖçË∑®Áª¥Â∫¶Âπ≤Êâ∞ÔºåÊèêÂçáÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMono3DVG-EnSDÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºöCLIP-Guided Lexical Certainty Adapter (CLIP-LCA) Âíå Dimension-Decoupled Module (D2M)„ÄÇÈ¶ñÂÖàÔºåCLIP-LCAÁî®‰∫éÂä®ÊÄÅË∞ÉÊï¥ÊñáÊú¨ÁâπÂæÅÔºåÁ™ÅÂá∫Á©∫Èó¥ÊèèËø∞„ÄÇÁÑ∂ÂêéÔºåD2MÂ∞ÜÈÄöÁî®ÊñáÊú¨ÁâπÂæÅÂàÜËß£‰∏∫2DÂíå3DÁâπÂÆöÁâπÂæÅÔºåÂàÜÂà´‰∏éÂØπÂ∫îÁöÑ2DÂíå3DËßÜËßâÁâπÂæÅËøõË°åËûçÂêà„ÄÇÊúÄÂêéÔºåËûçÂêàÂêéÁöÑÁâπÂæÅÁî®‰∫éÈ¢ÑÊµã3DÁâ©‰ΩìÁöÑ‰ΩçÁΩÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1)ÊèêÂá∫‰∫ÜCLIP-LCAÊ®°ÂùóÔºåÈÄöËøáÂä®ÊÄÅÂ±èËîΩÈ´òÁ°ÆÂÆöÊÄßÂÖ≥ÈîÆËØçÔºåËø´‰ΩøÊ®°ÂûãÂ≠¶‰π†ÊñáÊú¨‰∏≠Êõ¥‰∏∞ÂØåÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÔºõ2)ÊèêÂá∫‰∫ÜD2MÊ®°ÂùóÔºåÈÄöËøáËß£ËÄ¶ÊñáÊú¨ÁâπÂæÅ‰∏≠ÁöÑ2DÂíå3D‰ø°ÊÅØÔºåÈÅøÂÖç‰∫ÜË∑®Áª¥Â∫¶Âπ≤Êâ∞ÔºåÊèêÂçá‰∫ÜÁâπÂæÅËûçÂêàÁöÑÊïàÁéá„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊõ¥ÂÖ≥Ê≥®ÊñáÊú¨‰∏≠ÁöÑÁ©∫Èó¥‰ø°ÊÅØÔºåÂπ∂Ëß£ÂÜ≥‰∫ÜË∑®Áª¥Â∫¶ÁâπÂæÅËûçÂêàÁöÑÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCLIP-LCAÊ®°ÂùóÂà©Áî®CLIPÊ®°ÂûãËÆ°ÁÆóÊñáÊú¨‰∏≠ÊØè‰∏™ËØçÁöÑÁ°ÆÂÆöÊÄßÂæóÂàÜÔºåÂπ∂Ê†πÊçÆÂæóÂàÜÂä®ÊÄÅË∞ÉÊï¥ËØçÁöÑÊùÉÈáç„ÄÇD2MÊ®°Âùó‰ΩøÁî®Á∫øÊÄßÂ±ÇÂ∞ÜÈÄöÁî®ÊñáÊú¨ÁâπÂæÅÂàÜËß£‰∏∫2DÂíå3DÁâπÂÆöÁâπÂæÅ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂÆö‰ΩçÊçüÂ§±ÂíåÂàÜÁ±ªÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂÆö‰ΩçÂíåËØÜÂà´ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMono3DVG-EnSDÂú®Mono3DReferÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇÂ∞§ÂÖ∂ÊòØÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËøúË∑ùÁ¶ª(Far(Acc@0.5))Âú∫ÊôØ‰∏ãÔºåËØ•ÊñπÊ≥ïÁõ∏ÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫Ü+13.54%„ÄÇËøôË°®ÊòéËØ•ÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÂú∫ÊôØÂíåËøúË∑ùÁ¶ªÁâ©‰ΩìÂÆö‰ΩçÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•ÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆö‰ΩçÁõÆÊ†áÁâ©‰ΩìÔºõËá™Âä®È©æÈ©∂Á≥ªÁªüÂèØ‰ª•Ê†πÊçÆ‰πòÂÆ¢ÁöÑËØ≠Èü≥Êåá‰ª§ËØÜÂà´Âπ∂ÂÆö‰ΩçËΩ¶ËæÜÂë®Âõ¥ÁöÑÁâπÂÆöÁâ©‰ΩìÔºõÂ¢ûÂº∫Áé∞ÂÆûÂ∫îÁî®ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑÊñáÊú¨ÊèèËø∞Âú®ÁúüÂÆûÂú∫ÊôØ‰∏≠Âè†Âä†ËôöÊãüÁâ©‰Ωì„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.

