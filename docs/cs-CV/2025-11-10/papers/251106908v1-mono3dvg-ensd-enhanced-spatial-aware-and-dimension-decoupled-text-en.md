---
layout: default
title: Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding
---

# Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding

**arXiv**: [2511.06908v1](https://arxiv.org/abs/2511.06908) | [PDF](https://arxiv.org/pdf/2511.06908.pdf)

**ä½œè€…**: Yuzhen Li, Min Liu, Zhaoyang Li, Yuan Bian, Xueping Wang, Erbo Zhai, Yaonan Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMono3DVG-EnSDæ¡†æž¶ï¼Œé€šè¿‡å¢žå¼ºç©ºé—´æ„ŸçŸ¥å’Œç»´åº¦è§£è€¦è§£å†³å•ç›®3Dè§†è§‰å®šä½ä¸­çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `å•ç›®3Dè§†è§‰å®šä½` `æ–‡æœ¬ç¼–ç å¢žå¼º` `ç©ºé—´æ„ŸçŸ¥` `ç»´åº¦è§£è€¦` `è·¨æ¨¡æ€äº¤äº’` `CLIPå¼•å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–é«˜ç¡®å®šæ€§å…³é”®è¯ï¼Œå¿½è§†ç©ºé—´æè¿°ï¼Œä¸”æ–‡æœ¬ç‰¹å¾å­˜åœ¨è·¨ç»´åº¦å¹²æ‰°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥CLIP-LCAåŠ¨æ€æŽ©ç å…³é”®è¯ä¿ç•™ç©ºé—´æè¿°ï¼ŒD2Mè§£è€¦ç»´åº¦ç‰¹å¾ä»¥æŒ‡å¯¼è§†è§‰ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Mono3DReferæ•°æ®é›†ä¸Šå®žçŽ°SOTAï¼ŒFar(Acc@0.5)åœºæ™¯æå‡13.54%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D
> objects in RGB images using text descriptions with geometric cues. However,
> existing methods face two key limitations. Firstly, they often over-rely on
> high-certainty keywords that explicitly identify the target object while
> neglecting critical spatial descriptions. Secondly, generalized textual
> features contain both 2D and 3D descriptive information, thereby capturing an
> additional dimension of details compared to singular 2D or 3D visual features.
> This characteristic leads to cross-dimensional interference when refining
> visual features under text guidance. To overcome these challenges, we propose
> Mono3DVG-EnSD, a novel framework that integrates two key components: the
> CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled
> Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while
> retaining low-certainty implicit spatial descriptions, thereby forcing the
> model to develop a deeper understanding of spatial relationships in captions
> for object localization. Meanwhile, the D2M decouples dimension-specific
> (2D/3D) textual features from generalized textual features to guide
> corresponding visual features at same dimension, which mitigates
> cross-dimensional interference by ensuring dimensionally-consistent cross-modal
> interactions. Through comprehensive comparisons and ablation studies on the
> Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance
> across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario
> by a significant +13.54%.

