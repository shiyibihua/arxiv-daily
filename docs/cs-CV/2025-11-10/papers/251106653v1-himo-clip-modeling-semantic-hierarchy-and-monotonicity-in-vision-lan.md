---
layout: default
title: HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment
---

# HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment

**arXiv**: [2511.06653v1](https://arxiv.org/abs/2511.06653) | [PDF](https://arxiv.org/pdf/2511.06653.pdf)

**ä½œè€…**: Ruijia Wu, Ping Chen, Fei Shen, Shaoan Zhao, Qiang Hui, Huanlin Gao, Ting Lu, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHiMo-CLIPä»¥å¢žå¼ºCLIPæ¨¡åž‹å¤„ç†å¤æ‚æ–‡æœ¬æè¿°çš„èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¹é½` `è¯­ä¹‰å±‚æ¬¡å»ºæ¨¡` `å¯¹æ¯”å­¦ä¹ ` `å›¾åƒæ–‡æœ¬æ£€ç´¢` `é•¿æ–‡æœ¬å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPæ¨¡åž‹å¤„ç†å¤æ‚æ–‡æœ¬æ—¶å¿½ç•¥è¯­ä¹‰å±‚æ¬¡å’Œå•è°ƒæ€§ï¼Œå¯¼è‡´å¯¹é½æ•ˆæžœä¸ä½³
2. å¼•å…¥HiDeæ¨¡å—åˆ†è§£è¯­ä¹‰å’ŒMoLoæŸå¤±å‡½æ•°ï¼Œå¼ºåŒ–å¤šç²’åº¦å¯¹é½å’Œè¯­ä¹‰é¡ºåº
3. åœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬å’Œç»„åˆæè¿°åœºæ™¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive vision-language models like CLIP have achieved impressive results
> in image-text retrieval by aligning image and text representations in a shared
> embedding space. However, these models often treat text as flat sequences,
> limiting their ability to handle complex, compositional, and long-form
> descriptions. In particular, they fail to capture two essential properties of
> language: semantic hierarchy, which reflects the multi-level compositional
> structure of text, and semantic monotonicity, where richer descriptions should
> result in stronger alignment with visual content.To address these limitations,
> we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style
> models without modifying the encoder architecture. HiMo-CLIP introduces two key
> components: a hierarchical decomposition (HiDe) module that extracts latent
> semantic components from long-form text via in-batch PCA, enabling flexible,
> batch-aware alignment across different semantic granularities, and a
> monotonicity-aware contrastive loss (MoLo) that jointly aligns global and
> component-level representations, encouraging the model to internalize semantic
> ordering and alignment strength as a function of textual completeness.These
> components work in concert to produce structured, cognitively-aligned
> cross-modal representations. Experiments on multiple image-text retrieval
> benchmarks show that HiMo-CLIP consistently outperforms strong baselines,
> particularly under long or compositional descriptions. The code is available at
> https://github.com/UnicomAI/HiMo-CLIP.

