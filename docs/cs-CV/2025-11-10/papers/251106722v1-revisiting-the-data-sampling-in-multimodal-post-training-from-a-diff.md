---
layout: default
title: Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View
---

# Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View

**arXiv**: [2511.06722v1](https://arxiv.org/abs/2511.06722) | [PDF](https://arxiv.org/pdf/2511.06722.pdf)

**ä½œè€…**: Jianyu Qi, Ding Zou, Wenrui Yan, Rui Ma, Jiaxu Li, Zhijie Zheng, Zhiguo Yang, Rongchang Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéš¾åº¦æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ä»¥ä¼˜åŒ–å¤šæ¨¡æ€åŽè®­ç»ƒä¸­çš„æ ·æœ¬é€‰æ‹©**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŽè®­ç»ƒä¼˜åŒ–` `éš¾åº¦æ„ŸçŸ¥é‡‡æ ·` `å¼ºåŒ–å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `å›¾åƒè¯­ä¹‰æŽ©ç `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŽè®­ç»ƒèŒƒå¼ç¼ºä¹é‡åŒ–éš¾åº¦æŒ‡æ ‡ï¼Œæ— æ³•ç­–ç•¥æ€§ç­›é€‰æ ·æœ¬
2. å¼•å…¥PISMå’ŒCMABæ–¹æ³•ï¼Œåˆ†åˆ«é€šè¿‡å›¾åƒé€€åŒ–å’Œæ³¨æ„åŠ›åˆ†æžè¯„ä¼°æ ·æœ¬éš¾åº¦
3. å®žéªŒæ˜¾ç¤ºGRPOåº”ç”¨äºŽéš¾åº¦åˆ†å±‚æ ·æœ¬ä¼˜äºŽä¼ ç»ŸSFT+GRPOï¼Œæå‡æ¨¡åž‹ç²¾åº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models (MLLMs) have spurred
> significant progress in Chain-of-Thought (CoT) reasoning. Building on the
> success of Deepseek-R1, researchers extended multimodal reasoning to
> post-training paradigms based on reinforcement learning (RL), focusing
> predominantly on mathematical datasets. However, existing post-training
> paradigms tend to neglect two critical aspects: (1) The lack of quantifiable
> difficulty metrics capable of strategically screening samples for post-training
> optimization. (2) Suboptimal post-training paradigms that fail to jointly
> optimize perception and reasoning capabilities. To address this gap, we propose
> two novel difficulty-aware sampling strategies: Progressive Image Semantic
> Masking (PISM) quantifies sample hardness through systematic image degradation,
> while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction
> complexity via attention distribution analysis. Leveraging these metrics, we
> design a hierarchical training framework that incorporates both GRPO-only and
> SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark
> datasets. Experiments demonstrate consistent superiority of GRPO applied to
> difficulty-stratified samples compared to conventional SFT+GRPO pipelines,
> indicating that strategic data sampling can obviate the need for supervised
> fine-tuning while improving model accuracy. Our code will be released at
> https://github.com/qijianyu277/DifficultySampling.

