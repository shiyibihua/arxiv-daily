---
layout: default
title: StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation
---

# StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation

**arXiv**: [2511.07399v1](https://arxiv.org/abs/2511.07399) | [PDF](https://arxiv.org/pdf/2511.07399.pdf)

**ä½œè€…**: Tianrui Feng, Zhi Li, Shuo Yang, Haocheng Xi, Muyang Li, Xiuyu Li, Lvmin Zhang, Keting Yang, Kelly Peng, Song Han, Maneesh Agrawala, Kurt Keutzer, Akio Kodaira, Chenfeng Xu

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStreamDiffusionV2æµå¼ç³»ç»Ÿï¼Œå®ç°åŠ¨æ€äº¤äº’è§†é¢‘ç”Ÿæˆçš„å®æ—¶æµåª’ä½“æœåŠ¡**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡å‹` `å®æ—¶æµåª’ä½“ç³»ç»Ÿ` `ä½å»¶è¿Ÿç”Ÿæˆ` `å¤šGPUå¹¶è¡Œ` `äº¤äº’å¼è§†é¢‘ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿æµåª’ä½“éœ€ä½å»¶è¿Ÿã€ä½æŠ–åŠ¨ï¼Œç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹éš¾ä»¥æ»¡è¶³å®æ—¶SLOè¦æ±‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆSLOæ„ŸçŸ¥æ‰¹å¤„ç†ã€å—è°ƒåº¦ã€æ»šåŠ¨KVç¼“å­˜å’Œè¿åŠ¨æ„ŸçŸ¥å™ªå£°æ§åˆ¶ç­‰ä¼˜åŒ–
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å››H100 GPUä¸Šï¼Œ14Bæ¨¡å‹è¾¾58.28 FPSï¼Œé¦–å¸§æ¸²æŸ“<0.5ç§’ï¼Œæ”¯æŒçµæ´»å»å™ªæ­¥æ•°

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generative models are reshaping the live-streaming industry by redefining how
> content is created, styled, and delivered. Previous image-based streaming
> diffusion models have powered efficient and creative live streaming products
> but have hit limits on temporal consistency due to the foundation of
> image-based designs. Recent advances in video diffusion have markedly improved
> temporal consistency and sampling efficiency for offline generation. However,
> offline generation systems primarily optimize throughput by batching large
> workloads. In contrast, live online streaming operates under strict
> service-level objectives (SLOs): time-to-first-frame must be minimal, and every
> frame must meet a per-frame deadline with low jitter. Besides, scalable
> multi-GPU serving for real-time streams remains largely unresolved so far. To
> address this, we present StreamDiffusionV2, a training-free pipeline for
> interactive live streaming with video diffusion models. StreamDiffusionV2
> integrates an SLO-aware batching scheduler and a block scheduler, together with
> a sink-token--guided rolling KV cache, a motion-aware noise controller, and
> other system-level optimizations. Moreover, we introduce a scalable pipeline
> orchestration that parallelizes the diffusion process across denoising steps
> and network layers, achieving near-linear FPS scaling without violating latency
> guarantees. The system scales seamlessly across heterogeneous GPU environments
> and supports flexible denoising steps (e.g., 1--4), enabling both
> ultra-low-latency and higher-quality modes. Without TensorRT or quantization,
> StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS
> with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four
> H100 GPUs, making state-of-the-art generative live streaming practical and
> accessible--from individual creators to enterprise-scale platforms.

