---
layout: default
title: FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection
---

# FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection

**arXiv**: [2511.06947v1](https://arxiv.org/abs/2511.06947) | [PDF](https://arxiv.org/pdf/2511.06947.pdf)

**ä½œè€…**: Yulin Chen, Zeyuan Wang, Tianyuan Yu, Yingmei Wei, Liang Bai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFoCLIPæ¡†æž¶ä»¥è¯¯å¯¼CLIPè¯„åˆ†å¹¶å®žçŽ°æ£€æµ‹é˜²å¾¡**

**å…³é”®è¯**: `ç‰¹å¾ç©ºé—´é”™ä½` `CLIPè¯„åˆ†è¯¯å¯¼` `å¤šæ¨¡æ€å¯¹é½` `å›¾åƒè´¨é‡è¯„ä¼°` `æ£€æµ‹é˜²å¾¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPè¯„åˆ†æ˜“å—ç‰¹å¾ç©ºé—´å¯¹é½å½±å“ï¼Œå¯¼è‡´å›¾åƒè´¨é‡è¯„ä¼°è„†å¼±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆç‰¹å¾å¯¹é½ã€åˆ†æ•°å¹³è¡¡å’Œåƒç´ ä¿æŠ¤ï¼Œä¼˜åŒ–å¤šæ¨¡æ€è¾“å‡ºã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨è‰ºæœ¯å’ŒImageNetæ•°æ®é›†ä¸Šæå‡CLIPè¯„åˆ†ï¼Œæ£€æµ‹å‡†ç¡®çŽ‡è¾¾91%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The well-aligned attribute of CLIP-based models enables its effective
> application like CLIPscore as a widely adopted image quality assessment metric.
> However, such a CLIP-based metric is vulnerable for its delicate multimodal
> alignment. In this work, we propose \textbf{FoCLIP}, a feature-space
> misalignment framework for fooling CLIP-based image quality metric. Based on
> the stochastic gradient descent technique, FoCLIP integrates three key
> components to construct fooling examples: feature alignment as the core module
> to reduce image-text modality gaps, the score distribution balance module and
> pixel-guard regularization, which collectively optimize multimodal output
> equilibrium between CLIPscore performance and image quality. Such a design can
> be engineered to maximize the CLIPscore predictions across diverse input
> prompts, despite exhibiting either visual unrecognizability or semantic
> incongruence with the corresponding adversarial prompts from human perceptual
> perspectives. Experiments on ten artistic masterpiece prompts and ImageNet
> subsets demonstrate that optimized images can achieve significant improvement
> in CLIPscore while preserving high visual fidelity. In addition, we found that
> grayscale conversion induces significant feature degradation in fooling images,
> exhibiting noticeable CLIPscore reduction while preserving statistical
> consistency with original images. Inspired by this phenomenon, we propose a
> color channel sensitivity-driven tampering detection mechanism that achieves
> 91% accuracy on standard benchmarks. In conclusion, this work establishes a
> practical pathway for feature misalignment in CLIP-based multimodal systems and
> the corresponding defense method.

