---
layout: default
title: LeCoT: revisiting network architecture for two-view correspondence pruning
---

# LeCoT: revisiting network architecture for two-view correspondence pruning

**arXiv**: [2511.07078v1](https://arxiv.org/abs/2511.07078) | [PDF](https://arxiv.org/pdf/2511.07078.pdf)

**ä½œè€…**: Luanyuan Dai, Xiaoyu Du, Jinhui Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: Just accepted at SCIENCE CHINA Information Sciences

**DOI**: [10.1007/s11432-024-4555-x](https://doi.org/10.1007/s11432-024-4555-x)

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LeCoTï¼šé€šè¿‡ç©ºé—´-é€šé“èžåˆTransformeræ”¹è¿›åŒè§†å›¾å¯¹åº”å…³ç³»å‰ªæž**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¯¹åº”å…³ç³»å‰ªæž` `åŒè§†å›¾å‡ ä½•` `Transformer` `å…¨å±€ä¸Šä¸‹æ–‡` `è§†è§‰å®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŒè§†å›¾å¯¹åº”å…³ç³»å‰ªæžæ–¹æ³•ä¾èµ–MLPï¼Œç¼ºä¹æœ‰æ•ˆåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚
2. LeCoTé€šè¿‡ç©ºé—´-é€šé“èžåˆTransformerå—ï¼Œåœ¨ä¸åŒé˜¶æ®µè‡ªç„¶åœ°åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒLeCoTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¶…è¶Šäº†çŽ°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œä¾‹å¦‚ç›¸å¯¹å§¿æ€ä¼°è®¡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒè§†å›¾å¯¹åº”å…³ç³»å‰ªæžæ—¨åœ¨ä»Žåˆå§‹å¯¹åº”å…³ç³»ä¸­å‡†ç¡®ç§»é™¤é”™è¯¯å¯¹åº”å…³ç³»ï¼ˆå¤–ç‚¹ï¼‰ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºŽå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚ç›®å‰æµè¡Œçš„ç­–ç•¥é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶è¾…ä»¥é¢å¤–çš„æ¨¡å—æ¥å¢žå¼ºç½‘ç»œå¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼Œè€Œè¿™æ­£æ˜¯MLPçš„å·²çŸ¥å±€é™æ€§ã€‚ä¸Žæ­¤ç›¸åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è§†è§’ï¼Œæ— éœ€é¢å¤–çš„è®¾è®¡æ¨¡å—å³å¯æ•èŽ·å¯¹åº”å…³ç³»ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåä¸ºLeCoTçš„åŒè§†å›¾å¯¹åº”å…³ç³»å‰ªæžç½‘ç»œï¼Œè¯¥ç½‘ç»œå¯ä»¥è‡ªç„¶åœ°åˆ©ç”¨ä¸åŒé˜¶æ®µçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒLeCoTçš„æ ¸å¿ƒè®¾è®¡æ˜¯ç©ºé—´-é€šé“èžåˆTransformerå—ï¼Œè¿™æ˜¯ä¸€ç§æ–°æå‡ºçš„ç»„ä»¶ï¼Œå¯æœ‰æ•ˆåˆ©ç”¨ç¨€ç–å¯¹åº”å…³ç³»ä¸­çš„ç©ºé—´å’Œé€šé“å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†æ‰€æå‡ºçš„é¢„æµ‹å—ï¼Œè¯¥é¢„æµ‹å—åˆ©ç”¨æ¥è‡ªä¸­é—´é˜¶æ®µçš„å¯¹åº”å…³ç³»ç‰¹å¾æ¥ç”Ÿæˆæ¦‚çŽ‡é›†ï¼Œè¯¥æ¦‚çŽ‡é›†å……å½“åŽç»­å­¦ä¹ é˜¶æ®µçš„æŒ‡å¯¼ä¿¡æ¯ï¼Œä»Žè€Œä½¿ç½‘ç»œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•èŽ·é²æ£’çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ­¤é¢„æµ‹å—ä¼šé€æ­¥ç»†åŒ–æ¦‚çŽ‡é›†ï¼Œä»Žè€Œç¼“è§£ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚å¤§é‡çš„å®žéªŒè¯æ˜Žï¼Œæ‰€æå‡ºçš„LeCoTåœ¨å¯¹åº”å…³ç³»å‰ªæžã€ç›¸å¯¹å§¿æ€ä¼°è®¡ã€å•åº”æ€§ä¼°è®¡ã€è§†è§‰å®šä½å’Œ3Dé‡å»ºä»»åŠ¡ä¸­å‡ä¼˜äºŽæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å·²åœ¨https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruningæä¾›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŒè§†å›¾å¯¹åº”å…³ç³»å‰ªæžæ—¨åœ¨ä»Žä¸¤å¹…å›¾åƒçš„åˆå§‹åŒ¹é…å…³ç³»ä¸­åŽ»é™¤é”™è¯¯åŒ¹é…ï¼ˆå¤–ç‚¹ï¼‰ï¼Œä»Žè€Œæé«˜åŽç»­ä»»åŠ¡çš„ç²¾åº¦ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽMLPçš„æ–¹æ³•ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°æ•æ‰å’Œåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´å‰ªæžæ•ˆæžœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLeCoTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ä¸€ç§æ–°çš„ç½‘ç»œæž¶æž„ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªç„¶åœ°ã€æœ‰æ•ˆåœ°åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯¥æž¶æž„çš„æ ¸å¿ƒæ˜¯ç©ºé—´-é€šé“èžåˆTransformerå—ï¼Œå®ƒèƒ½å¤ŸåŒæ—¶è€ƒè™‘ç©ºé—´å’Œé€šé“ç»´åº¦çš„å…¨å±€ä¿¡æ¯ï¼Œä»Žè€Œæ›´å¥½åœ°ç†è§£å¯¹åº”å…³ç³»ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šLeCoTçš„æ•´ä½“æž¶æž„åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½åŒ…å«ç©ºé—´-é€šé“èžåˆTransformerå—ã€‚ç½‘ç»œé¦–å…ˆæå–å¯¹åº”å…³ç³»çš„ç‰¹å¾ï¼Œç„¶åŽé€šè¿‡å¤šä¸ªTransformerå—è¿›è¡Œå¤„ç†ï¼Œé€æ­¥æå–å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒLeCoTè¿˜åŒ…å«ä¸€ä¸ªé¢„æµ‹å—ï¼Œè¯¥å—åˆ©ç”¨ä¸­é—´é˜¶æ®µçš„ç‰¹å¾æ¥ç”Ÿæˆæ¦‚çŽ‡é›†ï¼ŒæŒ‡å¯¼åŽç»­å­¦ä¹ ã€‚æ¦‚çŽ‡é›†ä¼šé€æ­¥ç»†åŒ–ï¼Œç¼“è§£ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šLeCoTçš„å…³é”®åˆ›æ–°åœ¨äºŽç©ºé—´-é€šé“èžåˆTransformerå—çš„è®¾è®¡ã€‚ä¸Žä¼ ç»Ÿçš„Transformeråªå…³æ³¨ç©ºé—´æˆ–é€šé“ç»´åº¦ä¸åŒï¼Œè¯¥æ¨¡å—åŒæ—¶è€ƒè™‘äº†ä¸¤ä¸ªç»´åº¦ï¼Œä»Žè€Œæ›´å…¨é¢åœ°æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€æ­¥ç»†åŒ–çš„é¢„æµ‹å—ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸­é—´é˜¶æ®µçš„ç‰¹å¾ï¼Œæé«˜å‰ªæžçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç©ºé—´-é€šé“èžåˆTransformerå—çš„å…·ä½“å®žçŽ°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æŽ¨æµ‹å…¶é‡‡ç”¨äº†æŸç§å½¢å¼çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¾¿åœ¨ç©ºé—´å’Œé€šé“ç»´åº¦ä¸Šé€‰æ‹©æ€§åœ°å…³æ³¨é‡è¦ä¿¡æ¯ã€‚é¢„æµ‹å—çš„å…·ä½“ç»“æž„å’ŒæŸå¤±å‡½æ•°ä¹ŸæœªçŸ¥ï¼Œä½†å¯ä»¥æŽ¨æµ‹å…¶ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹æ¦‚çŽ‡ä¸ŽçœŸå®žæ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

LeCoTåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å¯¹åº”å…³ç³»å‰ªæžä»»åŠ¡ä¸Šï¼ŒLeCoTä¼˜äºŽçŽ°æœ‰æœ€ä¼˜æ–¹æ³•ã€‚åœ¨ç›¸å¯¹å§¿æ€ä¼°è®¡ã€å•åº”æ€§ä¼°è®¡ã€è§†è§‰å®šä½å’Œ3Dé‡å»ºç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒLeCoTä¹Ÿå–å¾—äº†æ›´å¥½çš„ç»“æžœï¼Œè¯æ˜Žäº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

LeCoTåœ¨æœºå™¨äººå¯¼èˆªã€å¢žå¼ºçŽ°å®žã€ä¸‰ç»´é‡å»ºã€è§†è§‰å®šä½ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å‡†ç¡®çš„å¯¹åº”å…³ç³»å‰ªæžèƒ½å¤Ÿæé«˜è¿™äº›åº”ç”¨ä¸­ä½å§¿ä¼°è®¡ã€åœ°å›¾æž„å»ºç­‰å…³é”®ä»»åŠ¡çš„ç²¾åº¦å’Œé²æ£’æ€§ï¼Œä»Žè€Œæå‡æ•´ä½“ç³»ç»Ÿæ€§èƒ½ã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºŽæŽ¨åŠ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨çš„æ·±å…¥ç ”ç©¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks. Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs. In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules. To this end, we design a two-view correspondence pruning network called LeCoT, which can naturally leverage global context information at different stages. Specifically, the core design of LeCoT is the Spatial-Channel Fusion Transformer block, a newly proposed component that efficiently utilizes both spatial and channel global context information among sparse correspondences. In addition, we integrate the proposed prediction block that utilizes correspondence features from intermediate stages to generate a probability set, which acts as guiding information for subsequent learning phases, allowing the network to more effectively capture robust global context information. Notably, this prediction block progressively refines the probability set, thereby mitigating the issue of information loss that is common in the traditional one. Extensive experiments prove that the proposed LeCoT outperforms state-of-the-art methods in correspondence pruning, relative pose estimation, homography estimation, visual localization, and $3$D~reconstruction tasks. The code is provided in https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning.

