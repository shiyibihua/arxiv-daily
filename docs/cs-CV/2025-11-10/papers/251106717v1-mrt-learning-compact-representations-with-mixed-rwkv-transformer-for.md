---
layout: default
title: MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression
---

# MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression

**arXiv**: [2511.06717v1](https://arxiv.org/abs/2511.06717) | [PDF](https://arxiv.org/pdf/2511.06717.pdf)

**ä½œè€…**: Han Liu, Hengyu Man, Xingtao Wang, Wenrui Li, Debin Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMRTæž¶æž„ä»¥è§£å†³æžç«¯å›¾åƒåŽ‹ç¼©ä¸­ç©ºé—´å†—ä½™é—®é¢˜**

**å…³é”®è¯**: `æžç«¯å›¾åƒåŽ‹ç¼©` `æ··åˆRWKV-Transformer` `1-Dæ½œåœ¨è¡¨ç¤º` `æ³¨æ„åŠ›æœºåˆ¶` `æ¯”ç‰¹çŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä½¿ç”¨2-Dæ½œåœ¨ç©ºé—´ç¼–ç å›¾åƒï¼Œå­˜åœ¨ç©ºé—´å†—ä½™é™åˆ¶åŽ‹ç¼©æ€§èƒ½
2. ç»“åˆRWKVå’ŒTransformerï¼Œç¼–ç å›¾åƒä¸º1-Dæ½œåœ¨è¡¨ç¤ºï¼Œæå‡ç´§å‡‘æ€§
3. åœ¨ä½Žæ¯”ç‰¹çŽ‡ä¸‹å®žçŽ°ä¼˜è¶Šé‡å»ºè´¨é‡ï¼Œæ¯”ç‰¹çŽ‡èŠ‚çœè¶…30%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in extreme image compression have revealed that mapping pixel
> data into highly compact latent representations can significantly improve
> coding efficiency. However, most existing methods compress images into 2-D
> latent spaces via convolutional neural networks (CNNs) or Swin Transformers,
> which tend to retain substantial spatial redundancy, thereby limiting overall
> compression performance. In this paper, we propose a novel Mixed
> RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D
> latent representations by synergistically integrating the complementary
> strengths of linear-attention-based RWKV and self-attention-based Transformer
> models. Specifically, MRT partitions each image into fixed-size windows,
> utilizing RWKV modules to capture global dependencies across windows and
> Transformer blocks to model local redundancies within each window. The
> hierarchical attention mechanism enables more efficient and compact
> representation learning in the 1-D domain. To further enhance compression
> efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to
> the structure characteristics of the intermediate 1-D latent features in MRT.
> Extensive experiments on standard image compression benchmarks validate the
> effectiveness of our approach. The proposed MRT framework consistently achieves
> superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp).
> Quantitative results based on the DISTS metric show that MRT significantly
> outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate
> savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets,
> respectively.

