---
layout: default
title: HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and End-to-end Autonomous Driving
---

# HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and End-to-end Autonomous Driving

**arXiv**: [2511.07106v1](https://arxiv.org/abs/2511.07106) | [PDF](https://arxiv.org/pdf/2511.07106.pdf)

**ä½œè€…**: Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Ming-Hsuan Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHENet++æ¡†æž¶ï¼Œé€šè¿‡æ··åˆç¼–ç å’Œå¤šä»»åŠ¡å­¦ä¹ è§£å†³è‡ªåŠ¨é©¾é©¶3Dæ„ŸçŸ¥ä¸Žç«¯åˆ°ç«¯æŽ¨ç†é—®é¢˜**

**å…³é”®è¯**: `3Dæ„ŸçŸ¥` `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `æ··åˆç¼–ç ` `å¤šä»»åŠ¡å­¦ä¹ ` `é¸Ÿçž°å›¾åˆ†å‰²` `å ç”¨é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè®¡ç®—èµ„æºé™åˆ¶ä¸‹ï¼Œå¤§å›¾åƒç¼–ç å™¨å’Œé«˜åˆ†è¾¨çŽ‡è¾“å…¥éš¾ä»¥å…¼å®¹å¤šä»»åŠ¡3Dæ„ŸçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤§ç¼–ç å™¨å¤„ç†çŸ­æœŸå¸§ã€å°ç¼–ç å™¨å¤„ç†é•¿æœŸå¸§ï¼Œå¹¶æå–ç¨ å¯†ä¸Žç¨€ç–ç‰¹å¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨nuScenesåŸºå‡†ä¸Šå®žçŽ°SOTAå¤šä»»åŠ¡æ„ŸçŸ¥å’Œæœ€ä½Žç¢°æ’žçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Three-dimensional feature extraction is a critical component of autonomous
> driving systems, where perception tasks such as 3D object detection,
> bird's-eye-view (BEV) semantic segmentation, and occupancy prediction serve as
> important constraints on 3D features. While large image encoders,
> high-resolution images, and long-term temporal inputs can significantly enhance
> feature quality and deliver remarkable performance gains, these techniques are
> often incompatible in both training and inference due to computational resource
> constraints. Moreover, different tasks favor distinct feature representations,
> making it difficult for a single model to perform end-to-end inference across
> multiple tasks while maintaining accuracy comparable to that of single-task
> models. To alleviate these issues, we present the HENet and HENet++ framework
> for multi-task 3D perception and end-to-end autonomous driving. Specifically,
> we propose a hybrid image encoding network that uses a large image encoder for
> short-term frames and a small one for long-term frames. Furthermore, our
> framework simultaneously extracts both dense and sparse features, providing
> more suitable representations for different tasks, reducing cumulative errors,
> and delivering more comprehensive information to the planning module. The
> proposed architecture maintains compatibility with various existing 3D feature
> extraction methods and supports multimodal inputs. HENet++ achieves
> state-of-the-art end-to-end multi-task 3D perception results on the nuScenes
> benchmark, while also attaining the lowest collision rate on the nuScenes
> end-to-end autonomous driving benchmark.

