---
layout: default
title: ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search
---

# ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search

**arXiv**: [2511.06833v1](https://arxiv.org/abs/2511.06833) | [PDF](https://arxiv.org/pdf/2511.06833.pdf)

**ä½œè€…**: Zhenjie Liu, Jianzhang Lu, Renjie Lu, Cong Liang, Shangfei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConsistTalkæ¡†æž¶ï¼Œé€šè¿‡è§£è€¦å¤–è§‚-è¿åŠ¨è¡¨ç¤ºå’Œå™ªå£°æœç´¢æŽ¨ç†ï¼Œè§£å†³è¯´è¯å¤´ç”Ÿæˆä¸­çš„é—ªçƒå’Œèº«ä»½æ¼‚ç§»é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¯´è¯å¤´ç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `å…‰æµå¼•å¯¼` `çŸ¥è¯†è’¸é¦` `å™ªå£°æœç´¢` `éŸ³è§†é¢‘åŒæ­¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰æ–¹æ³•å­˜åœ¨è§†è§‰é—ªçƒã€èº«ä»½æ¼‚ç§»å’ŒéŸ³è§†é¢‘åŒæ­¥ä¸ä½³ï¼ŒæºäºŽå¤–è§‚-è¿åŠ¨è¡¨ç¤ºçº ç¼ å’ŒæŽ¨ç†ä¸ç¨³å®šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å…‰æµå¼•å¯¼æ—¶åºæ¨¡å—è§£è€¦è¿åŠ¨ç‰¹å¾ï¼ŒéŸ³é¢‘-å¼ºåº¦æ¨¡åž‹å®žçŽ°å¸§çº§å¼ºåº¦æŽ§åˆ¶ï¼Œæ‰©æ•£å™ªå£°åˆå§‹åŒ–ç­–ç•¥ä¼˜åŒ–æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºConsistTalkåœ¨å‡å°‘é—ªçƒã€ä¿æŒèº«ä»½å’Œæå‡æ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºŽå…ˆå‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in video diffusion models have significantly enhanced
> audio-driven portrait animation. However, current methods still suffer from
> flickering, identity drift, and poor audio-visual synchronization. These issues
> primarily stem from entangled appearance-motion representations and unstable
> inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel
> intensity-controllable and temporally consistent talking head generation
> framework with diffusion noise search inference. First, we propose \textbf{an
> optical flow-guided temporal module (OFT)} that decouples motion features from
> static appearance by leveraging facial optical flow, thereby reducing visual
> flicker and improving temporal consistency. Second, we present an
> \textbf{Audio-to-Intensity (A2I) model} obtained through multimodal
> teacher-student knowledge distillation. By transforming audio and facial
> velocity features into a frame-wise intensity sequence, the A2I model enables
> joint modeling of audio and visual motion, resulting in more natural dynamics.
> This further enables fine-grained, frame-wise control of motion dynamics while
> maintaining tight audio-visual synchronization. Third, we introduce a
> \textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing
> explicit constraints on background coherence and motion continuity during
> inference-time noise search, we achieve better identity preservation and refine
> motion dynamics compared to the current autoregressive strategy. Extensive
> experiments demonstrate that ConsistTalk significantly outperforms prior
> methods in reducing flicker, preserving identity, and delivering temporally
> stable, high-fidelity talking head videos.

