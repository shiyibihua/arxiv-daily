---
layout: default
title: SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation
---

# SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation

**arXiv**: [2511.06754v1](https://arxiv.org/abs/2511.06754) | [PDF](https://arxiv.org/pdf/2511.06754.pdf)

**ä½œè€…**: Taisei Hanyu, Nhat Chung, Huy Le, Toan Nguyen, Yuki Ikebe, Anthony Gunderman, Duy Nguyen Ho Minh, Khoa Vo, Tung Kieu, Kashu Yamazaki, Chase Rainwater, Anh Nguyen, Ngan Le

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSlotVLAæ¡†æž¶å’ŒLIBERO+æ•°æ®é›†ï¼Œä»¥æå‡æœºå™¨äººæ“ä½œä¸­çš„å¯¹è±¡å…³ç³»å»ºæ¨¡æ•ˆçŽ‡ä¸Žå¯è§£é‡Šæ€§ã€‚**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¯¹è±¡å…³ç³»å»ºæ¨¡` `æ§½æ³¨æ„åŠ›` `å¤šä»»åŠ¡å­¦ä¹ ` `å¯è§£é‡ŠAI` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æœºå™¨äººå¤šä»»åŠ¡æ¨¡åž‹ä¾èµ–å¯†é›†åµŒå…¥ï¼Œå¯¼è‡´å¯¹è±¡ä¸ŽèƒŒæ™¯ä¿¡æ¯çº ç¼ ï¼Œå½±å“æ•ˆçŽ‡å’Œå¯è§£é‡Šæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥SlotVLAæ¡†æž¶ï¼Œä½¿ç”¨æ§½æ³¨æ„åŠ›æ•èŽ·å¯¹è±¡åŠå…¶å…³ç³»ï¼Œç»“åˆLLMæ¨¡å—ç”Ÿæˆå¯æ‰§è¡ŒåŠ¨ä½œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBERO+æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºæ˜¾è‘—å‡å°‘è§†è§‰ä»¤ç‰Œæ•°é‡ï¼Œä¿æŒç«žäº‰æ€§æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Inspired by how humans reason over discrete objects and their relationships,
> we explore whether compact object-centric and object-relation representations
> can form a foundation for multitask robotic manipulation. Most existing robotic
> multitask models rely on dense embeddings that entangle both object and
> background cues, raising concerns about both efficiency and interpretability.
> In contrast, we study object-relation-centric representations as a pathway to
> more structured, efficient, and explainable visuomotor control. Our
> contributions are two-fold. First, we introduce LIBERO+, a fine-grained
> benchmark dataset designed to enable and evaluate object-relation reasoning in
> robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric
> annotations that enrich demonstrations with box- and mask-level labels as well
> as instance-level temporal tracking, supporting compact and interpretable
> visuomotor representations. Second, we propose SlotVLA, a slot-attention-based
> framework that captures both objects and their relations for action decoding.
> It uses a slot-based visual tokenizer to maintain consistent temporal object
> representations, a relation-centric decoder to produce task-relevant
> embeddings, and an LLM-driven module that translates these embeddings into
> executable actions. Experiments on LIBERO+ demonstrate that object-centric slot
> and object-relation slot representations drastically reduce the number of
> required visual tokens, while providing competitive generalization. Together,
> LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation
> for advancing object-relation-centric robotic manipulation.

