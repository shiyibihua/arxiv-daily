---
layout: default
title: Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models
---

# Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models

**arXiv**: [2511.07085v1](https://arxiv.org/abs/2511.07085) | [PDF](https://arxiv.org/pdf/2511.07085.pdf)

**ä½œè€…**: Xijie Zhang, Fengliang He, Hong-Ning Dai

**åˆ†ç±»**: cs.HC, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: 5 pages, 4 figures, 1 table, under review at ICASSP 2026

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ¡†æž¶ï¼Œç”¨äºŽé«˜æ•ˆè™šæ‹ŸçŽ°å®žäº¤äº’**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å£°å­¦æ‰‹åŠ¿è¯†åˆ«` `å¤§è¯­è¨€æ¨¡åž‹` `è™šæ‹ŸçŽ°å®ž` `å¢žå¼ºçŽ°å®ž` `ä¿¡é“å†²æ¿€å“åº”` `å°‘æ ·æœ¬å­¦ä¹ ` `äººæœºäº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰æ‰‹åŠ¿è¯†åˆ«åœ¨VR/ARä¸­å­˜åœ¨è®¡ç®—é‡å¤§ã€å…‰ç…§æ•æ„Ÿå’Œéšç§æ³„éœ²ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. åˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹å¤„ç†å£°å­¦ä¿¡å·ï¼ˆCIRï¼‰è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«ï¼Œæ— éœ€å¤§é‡è®­ç»ƒæ•°æ®ï¼Œé€‚ç”¨äºŽå°‘æ ·æœ¬åœºæ™¯ã€‚
3. é€šè¿‡æ”¶é›†å·®åˆ†CIRæ•°æ®å¹¶ç»“åˆLLMï¼Œå®žçŽ°äº†ä¸Žä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ç›¸å½“çš„è¯†åˆ«ç²¾åº¦ï¼Œæ— éœ€é¢†åŸŸç‰¹å®šé‡è®­ç»ƒã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç„¶é«˜æ•ˆçš„äº¤äº’ä»ç„¶æ˜¯è™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žï¼ˆVR/ARï¼‰ç³»ç»Ÿé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚åŸºäºŽè§†è§‰çš„æ‰‹åŠ¿è¯†åˆ«è®¡ç®—æˆæœ¬é«˜ï¼Œå¯¹å…‰ç…§æ¡ä»¶æ•æ„Ÿï¼Œå¹¶å­˜åœ¨éšç§æ³„éœ²çš„æ‹…å¿§ã€‚å£°å­¦ä¼ æ„Ÿæä¾›äº†ä¸€ç§æœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆï¼šé€šè¿‡å‘å°„ä¸å¯å¬çš„é«˜é¢‘ä¿¡å·å¹¶æ•èŽ·å…¶åå°„ï¼Œä¿¡é“å†²æ¿€å“åº”ï¼ˆCIRï¼‰ä»¥ä½Žæˆæœ¬å’Œç”¨æˆ·é€æ˜Žçš„æ–¹å¼ç¼–ç æ‰‹åŠ¿å¦‚ä½•æ‰°åŠ¨å£°åœºã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„åŸºäºŽCIRçš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•é€šå¸¸ä¾èµ–äºŽåœ¨å¤§åž‹æ ‡è®°æ•°æ®é›†ä¸Šå¯¹æ¨¡åž‹è¿›è¡Œå¤§é‡è®­ç»ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆå°‘æ ·æœ¬VRåœºæ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰è¿›è¡ŒVR/ARç³»ç»Ÿä¸­åŸºäºŽCIRçš„æ‰‹åŠ¿è¯†åˆ«çš„æ¡†æž¶ã€‚å°½ç®¡LLMå…·æœ‰ä¼˜åŠ¿ï¼Œä½†ç”±äºŽCIRæ‰‹åŠ¿çš„ä¸æ˜¾çœ¼ç‰¹å¾ï¼Œå®žçŽ°CIRæ‰‹åŠ¿çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ å¹¶éžæ˜“äº‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ”¶é›†å·®åˆ†CIRæ•°æ®è€Œä¸æ˜¯åŽŸå§‹CIRæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªçœŸå®žä¸–ç•Œçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±10åå‚ä¸Žè€…æ‰§è¡Œ15ä¸ªæ‰‹åŠ¿ï¼ˆè·¨è¶Šæ•°å­—ã€å­—æ¯å’Œå½¢çŠ¶ä¸‰ä¸ªç±»åˆ«ï¼‰ï¼Œæ¯ä¸ªæ‰‹åŠ¿é‡å¤10æ¬¡ã€‚ç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨é‡‡ç”¨LLMçš„åˆ†ç±»å™¨å¯¹è¯¥æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®žéªŒã€‚ç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬åŸºäºŽLLMçš„æ¡†æž¶å®žçŽ°äº†ä¸Žç»å…¸æœºå™¨å­¦ä¹ åŸºçº¿ç›¸å½“çš„å‡†ç¡®çŽ‡ï¼ŒåŒæ—¶ä¸éœ€è¦ç‰¹å®šé¢†åŸŸçš„é‡æ–°è®­ç»ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽè§†è§‰çš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•åœ¨VR/ARçŽ¯å¢ƒä¸­å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€å¯¹å…‰ç…§æ¡ä»¶æ•æ„Ÿä»¥åŠæ½œåœ¨çš„éšç§æ³„éœ²é—®é¢˜ã€‚è€ŒåŸºäºŽä¿¡é“å†²æ¿€å“åº”(CIR)çš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•è™½ç„¶å…·æœ‰ä½Žæˆæœ¬å’Œç”¨æˆ·é€æ˜Žçš„ä¼˜ç‚¹ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡åž‹è®­ç»ƒï¼Œè¿™åœ¨å®žé™…åº”ç”¨ä¸­éš¾ä»¥æ»¡è¶³ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å°‘é‡æ•°æ®å®žçŽ°é«˜æ•ˆçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹(LLM)å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’ŒçŸ¥è¯†è¿ç§»èƒ½åŠ›ï¼Œå°†LLMåº”ç”¨äºŽåŸºäºŽCIRçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­ã€‚é€šè¿‡å°†CIRæ•°æ®è½¬æ¢ä¸ºLLMå¯ä»¥ç†è§£çš„è¾“å…¥å½¢å¼ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»ï¼Œä»Žè€Œå®žçŽ°å°‘æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬çš„æ‰‹åŠ¿è¯†åˆ«ã€‚è¿™æ ·å¯ä»¥é¿å…ä¼ ç»Ÿæ–¹æ³•å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶ä¸»è¦åŒ…å«æ•°æ®é‡‡é›†ã€æ•°æ®é¢„å¤„ç†å’ŒLLMåˆ†ç±»å™¨ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å£°å­¦ä¼ æ„Ÿå™¨é‡‡é›†ç”¨æˆ·è¿›è¡Œæ‰‹åŠ¿æ“ä½œæ—¶çš„CIRæ•°æ®ã€‚ç„¶åŽï¼Œå¯¹CIRæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å·®åˆ†CIRè®¡ç®—ï¼Œä»¥å¢žå¼ºæ‰‹åŠ¿ç‰¹å¾ã€‚æœ€åŽï¼Œå°†é¢„å¤„ç†åŽçš„æ•°æ®è¾“å…¥åˆ°åŸºäºŽLLMçš„åˆ†ç±»å™¨ä¸­è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«ã€‚åˆ†ç±»å™¨åˆ©ç”¨LLMå¼ºå¤§çš„ç‰¹å¾æå–å’Œåˆ†ç±»èƒ½åŠ›ï¼Œå®žçŽ°é«˜æ•ˆçš„æ‰‹åŠ¿è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†å¤§è¯­è¨€æ¨¡åž‹åº”ç”¨äºŽåŸºäºŽCIRçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­ã€‚ä¸Žä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¶é›†å·®åˆ†CIRæ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¢žå¼ºæ‰‹åŠ¿ç‰¹å¾ï¼Œæé«˜è¯†åˆ«ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) é‡‡ç”¨å·®åˆ†CIRæ•°æ®ï¼Œè€ŒéžåŽŸå§‹CIRæ•°æ®ï¼Œä»¥çªå‡ºæ‰‹åŠ¿å˜åŒ–å¸¦æ¥çš„å½±å“ã€‚2) æž„å»ºäº†ä¸€ä¸ªåŒ…å«10åå‚ä¸Žè€…ã€15ç§æ‰‹åŠ¿çš„æ•°æ®é›†ï¼Œæ¶µç›–æ•°å­—ã€å­—æ¯å’Œå½¢çŠ¶ä¸‰ç§ç±»åˆ«ã€‚3) ä½¿ç”¨LLMä½œä¸ºåˆ†ç±»å™¨ï¼Œå¹¶é’ˆå¯¹CIRæ•°æ®çš„ç‰¹ç‚¹è¿›è¡Œäº†é€‚å½“çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚å…·ä½“çš„LLMå‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥åŸºäºŽLLMçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ¡†æž¶åœ¨è‡ªå»ºæ•°æ®é›†ä¸Šå–å¾—äº†ä¸Žä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ–¹æ³•ç›¸å½“çš„å‡†ç¡®çŽ‡ï¼ŒåŒæ—¶æ— éœ€é¢†åŸŸç‰¹å®šçš„é‡æ–°è®­ç»ƒã€‚è¿™è¡¨æ˜ŽLLMåœ¨å£°å­¦æ‰‹åŠ¿è¯†åˆ«é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥åœ¨å°‘æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬çš„æƒ…å†µä¸‹å®žçŽ°é«˜æ•ˆçš„æ‰‹åŠ¿è¯†åˆ«ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽè™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žã€æ™ºèƒ½å®¶å±…ã€å¯ç©¿æˆ´è®¾å¤‡ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ‰‹åŠ¿ä¸ŽVR/ARçŽ¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ— éœ€ä½©æˆ´å¤æ‚çš„æ‰‹å¥—æˆ–ä½¿ç”¨æŽ§åˆ¶å™¨ã€‚åœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ‰‹åŠ¿æŽ§åˆ¶å®¶ç”µè®¾å¤‡ã€‚åœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ‰‹åŠ¿è¿›è¡Œå¿«æ·æ“ä½œï¼Œæé«˜è®¾å¤‡çš„æ˜“ç”¨æ€§ã€‚è¯¥æŠ€æœ¯å…·æœ‰ä½Žæˆæœ¬ã€é«˜æ•ˆçŽ‡å’Œè‰¯å¥½çš„ç”¨æˆ·ä½“éªŒç­‰ä¼˜ç‚¹ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.

