---
layout: default
title: Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models
---

# Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models

**arXiv**: [2511.07085v1](https://arxiv.org/abs/2511.07085) | [PDF](https://arxiv.org/pdf/2511.07085.pdf)

**ä½œè€…**: Xijie Zhang, Fengliang He, Hong-Ning Dai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ¡†æž¶ï¼Œä»¥è§£å†³VR/ARä¸­å°‘æ ·æœ¬äº¤äº’é—®é¢˜**

**å…³é”®è¯**: `å£°å­¦æ‰‹åŠ¿è¯†åˆ«` `å¤§è¯­è¨€æ¨¡åž‹` `è™šæ‹ŸçŽ°å®žäº¤äº’` `å°‘æ ·æœ¬å­¦ä¹ ` `ä¿¡é“è„‰å†²å“åº”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVR/ARç³»ç»Ÿæ‰‹åŠ¿è¯†åˆ«ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œéš¾ä»¥é€‚åº”å°‘æ ·æœ¬åœºæ™¯
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹å¤„ç†å·®åˆ†ä¿¡é“è„‰å†²å“åº”æ•°æ®ï¼Œå®žçŽ°å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šï¼Œå‡†ç¡®çŽ‡ä¸Žç»å…¸æ–¹æ³•ç›¸å½“ï¼Œæ— éœ€é¢†åŸŸç‰¹å®šé‡è®­ç»ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Natural and efficient interaction remains a critical challenge for virtual
> reality and augmented reality (VR/AR) systems. Vision-based gesture recognition
> suffers from high computational cost, sensitivity to lighting conditions, and
> privacy leakage concerns. Acoustic sensing provides an attractive alternative:
> by emitting inaudible high-frequency signals and capturing their reflections,
> channel impulse response (CIR) encodes how gestures perturb the acoustic field
> in a low-cost and user-transparent manner. However, existing CIR-based gesture
> recognition methods often rely on extensive training of models on large labeled
> datasets, making them unsuitable for few-shot VR scenarios. In this work, we
> propose the first framework that leverages large language models (LLMs) for
> CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is
> non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to
> their inconspicuous features. To tackle this challenge, we collect differential
> CIR rather than original CIR data. Moreover, we construct a real-world dataset
> collected from 10 participants performing 15 gestures across three categories
> (digits, letters, and shapes), with 10 repetitions each. We then conduct
> extensive experiments on this dataset using an LLM-adopted classifier. Results
> show that our LLM-based framework achieves accuracy comparable to classical
> machine learning baselines, while requiring no domain-specific retraining.

