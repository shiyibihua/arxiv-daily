---
layout: default
title: How Do VLAs Effectively Inherit from VLMs?
---

# How Do VLAs Effectively Inherit from VLMs?

**arXiv**: [2511.06619v1](https://arxiv.org/abs/2511.06619) | [PDF](https://arxiv.org/pdf/2511.06619.pdf)

**ä½œè€…**: Chuheng Zhang, Rushuai Yang, Xiaoyu Chen, Kaixin Wang, Li Zhao, Yi Chen, Jiang Bian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGrinningFaceåŸºå‡†ä»¥è¯„ä¼°VLAå¦‚ä½•æœ‰æ•ˆç»§æ‰¿VLMå…ˆéªŒçŸ¥è¯†**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `å…ˆéªŒçŸ¥è¯†ç»§æ‰¿` `å…·èº«æŽ§åˆ¶` `è¯Šæ–­åŸºå‡†` `å‚æ•°é«˜æ•ˆå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹å¦‚ä½•æœ‰æ•ˆç»§æ‰¿VLMçš„è§†è§‰è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ä»¥å®žçŽ°å…·èº«æŽ§åˆ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡è¡¨æƒ…ç¬¦å·æ¡Œé¢æ“ä½œä»»åŠ¡ï¼Œæ¯”è¾ƒå‚æ•°é«˜æ•ˆå¾®è°ƒã€VLMå†»ç»“ç­‰æŠ€æœ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººä¸­éªŒè¯ï¼Œå¼ºè°ƒä¿ç•™VLMå…ˆéªŒå¯¹æ³›åŒ–çš„é‡è¦æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models hold the promise to attain generalizable
> embodied control. To achieve this, a pervasive paradigm is to leverage the rich
> vision-semantic priors of large vision-language models (VLMs). However, the
> fundamental question persists: How do VLAs effectively inherit the prior
> knowledge from VLMs? To address this critical question, we introduce a
> diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where
> the robot arm is asked to place objects onto printed emojis corresponding to
> language instructions. This task design is particularly revealing -- knowledge
> associated with emojis is ubiquitous in Internet-scale datasets used for VLM
> pre-training, yet emojis themselves are largely absent from standard robotics
> datasets. Consequently, they provide a clean proxy: successful task completion
> indicates effective transfer of VLM priors to embodied control. We implement
> this diagnostic task in both simulated environment and a real robot, and
> compare various promising techniques for knowledge transfer. Specifically, we
> investigate the effects of parameter-efficient fine-tuning, VLM freezing,
> co-training, predicting discretized actions, and predicting latent actions.
> Through systematic evaluation, our work not only demonstrates the critical
> importance of preserving VLM priors for the generalization of VLA but also
> establishes guidelines for future research in developing truly generalizable
> embodied AI systems.

