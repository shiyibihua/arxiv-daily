---
layout: default
title: Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models
---

# Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models

**arXiv**: [2511.07253v1](https://arxiv.org/abs/2511.07253) | [PDF](https://arxiv.org/pdf/2511.07253.pdf)

**ä½œè€…**: Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmni-AVSRç»Ÿä¸€æ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«ä¸­çš„ç‹¬ç«‹æ¨¡åž‹ä¸Žæ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«` `å¤§è¯­è¨€æ¨¡åž‹` `ç»Ÿä¸€æ¡†æž¶` `å‚æ•°é«˜æ•ˆé€‚åº”` `å¤šç²’åº¦è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰LLMæ–¹æ³•ç‹¬ç«‹å¤„ç†ASRã€VSRå’ŒAVSRä»»åŠ¡ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’ŒååŒç¼ºå¤±
2. é‡‡ç”¨å¤šç²’åº¦è®­ç»ƒå’ŒLoRAé€‚åº”ç­–ç•¥ï¼Œå®žçŽ°é«˜æ•ˆç»Ÿä¸€æ¨¡åž‹è®­ç»ƒä¸Žéƒ¨ç½²
3. åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šï¼Œç²¾åº¦å¯æ¯”æˆ–ä¼˜äºŽSOTAï¼Œèµ„æºä½¿ç”¨æ˜¾è‘—é™ä½Ž

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have recently achieved impressive results in
> speech recognition across multiple modalities, including Auditory Speech
> Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech
> Recognition (AVSR). Despite this progress, current LLM-based approaches
> typically address each task independently, training separate models that raise
> computational and deployment resource use while missing potential cross-task
> synergies. They also rely on fixed-rate token compression, which restricts
> flexibility in balancing accuracy with efficiency. These limitations highlight
> the need for a unified framework that can support ASR, VSR, and AVSR while
> enabling elastic inference. To this end, we present Omni-AVSR, a unified
> audio-visual LLM that combines efficient multi-granularity training with
> parameter-efficient adaptation. Specifically, we adapt the matryoshka
> representation learning paradigm to efficiently train across multiple audio and
> visual granularities, reducing its inherent training resource use. Furthermore,
> we explore three LoRA-based strategies for adapting the backbone LLM, balancing
> shared and task-specific specialization. Experiments on LRS2 and LRS3 show that
> Omni-AVSR achieves comparable or superior accuracy to state-of-the-art
> baselines while training a single model at substantially lower training and
> deployment resource use. The model also remains robust under acoustic noise,
> and we analyze its scaling behavior as LLM size increases, providing insights
> into the trade-off between performance and efficiency.

