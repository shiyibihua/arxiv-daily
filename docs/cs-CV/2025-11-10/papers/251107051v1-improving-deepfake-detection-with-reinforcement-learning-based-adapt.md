---
layout: default
title: Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation
---

# Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation

**arXiv**: [2511.07051v1](https://arxiv.org/abs/2511.07051) | [PDF](https://arxiv.org/pdf/2511.07051.pdf)

**ä½œè€…**: Yuxuan Zhou, Tao Yu, Wen Huang, Yuheng Zhang, Tao Dai, Shu-Tao Xia

**åˆ†ç±»**: cs.CV, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æ•°æ®å¢žå¼ºæ–¹æ³•CRDAï¼Œæå‡Deepfakeæ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `Deepfakeæ£€æµ‹` `æ•°æ®å¢žå¼º` `å¼ºåŒ–å­¦ä¹ ` `å› æžœæŽ¨æ–­` `æ³›åŒ–èƒ½åŠ›` `å¯¹æŠ—æ ·æœ¬` `è‡ªé€‚åº”å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰Deepfakeæ£€æµ‹å™¨ä¾èµ–å›ºå®šæ•°æ®å¢žå¼ºç­–ç•¥ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹çœŸå®žä¸–ç•Œä¸­ä¸æ–­æ¼”å˜çš„ä¼ªé€ æŠ€æœ¯å¤æ‚æ€§ã€‚
2. CRDAæ¡†æž¶ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå› æžœæŽ¨æ–­ï¼ŒåŠ¨æ€é€‰æ‹©å¢žå¼ºåŠ¨ä½œå¹¶ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¼•å¯¼æ£€æµ‹å™¨å­¦ä¹ å¤šé¢†åŸŸä¼ªé€ ç‰¹å¾ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒCRDAæ˜¾è‘—æå‡äº†æ£€æµ‹å™¨åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºŽå½“å‰æœ€ä¼˜æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›å¯¹äºŽå®žé™…åº”ç”¨è‡³å…³é‡è¦ã€‚é€šè¿‡åˆæˆä¼ªé€ äººè„¸ç”Ÿæˆæ•°æ®å¢žå¼ºå¯ä»¥æœ‰æ•ˆæå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†å½“å‰æœ€ä¼˜æ–¹æ³•ä¾èµ–äºŽå›ºå®šçš„ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå•ä¸€é™æ€å¢žå¼ºæ˜¯å¦è¶³å¤Ÿï¼Ÿæˆ–è€…ä¼ªé€ ç‰¹å¾çš„å¤šæ ·æ€§æ˜¯å¦éœ€è¦åŠ¨æ€æ–¹æ³•ï¼ŸçŽ°æœ‰æ–¹æ³•å¿½ç•¥äº†çœŸå®žä¼ªé€ æŠ€æœ¯çš„å¤æ‚æ€§æ¼”å˜ï¼ˆä¾‹å¦‚ï¼Œé¢éƒ¨æ‰­æ›²ã€è¡¨æƒ…æ“çºµï¼‰ï¼Œè€Œå›ºå®šç­–ç•¥æ— æ³•å®Œå…¨æ¨¡æ‹Ÿè¿™äº›å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CRDAï¼ˆCurriculum Reinforcement-Learning Data Augmentationï¼‰ï¼Œä¸€ä¸ªå¼•å¯¼æ£€æµ‹å™¨é€æ­¥æŽŒæ¡ä»Žç®€å•åˆ°å¤æ‚çš„å¤šé¢†åŸŸä¼ªé€ ç‰¹å¾çš„æ–°æ¡†æž¶ã€‚CRDAé€šè¿‡å¯é…ç½®çš„ä¼ªé€ æ“ä½œæ± åˆæˆå¢žå¼ºæ ·æœ¬ï¼Œå¹¶åŠ¨æ€ç”Ÿæˆé’ˆå¯¹æ£€æµ‹å™¨å½“å‰å­¦ä¹ çŠ¶æ€çš„å¯¹æŠ—æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ•´åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå› æžœæŽ¨æ–­ã€‚RLæ™ºèƒ½ä½“åŸºäºŽæ£€æµ‹å™¨æ€§èƒ½åŠ¨æ€é€‰æ‹©å¢žå¼ºåŠ¨ä½œï¼Œä»¥æœ‰æ•ˆæŽ¢ç´¢å¹¿é˜”çš„å¢žå¼ºç©ºé—´ï¼Œé€‚åº”æ—¥ç›Šå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼ªé€ ã€‚åŒæ—¶ï¼Œæ™ºèƒ½ä½“å¼•å…¥åŠ¨ä½œç©ºé—´å˜åŒ–ä»¥ç”Ÿæˆå¼‚æž„ä¼ªé€ æ¨¡å¼ï¼Œå¹¶ç”±å› æžœæŽ¨æ–­å¼•å¯¼ä»¥å‡è½»è™šå‡ç›¸å…³æ€§ï¼ŒæŠ‘åˆ¶ä¸Žä»»åŠ¡æ— å…³çš„åå·®ï¼Œå¹¶ä¸“æ³¨äºŽå› æžœä¸å˜ç‰¹å¾ã€‚è¿™ç§é›†æˆé€šè¿‡å°†åˆæˆå¢žå¼ºæ¨¡å¼ä¸Žæ¨¡åž‹å­¦ä¹ çš„è¡¨ç¤ºè§£è€¦ï¼Œç¡®ä¿äº†é²æ£’çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºŽå¤šä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„SOTAæ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰Deepfakeæ£€æµ‹å™¨åœ¨é¢å¯¹çœŸå®žåœºæ™¯æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ä¸»è¦åŽŸå› æ˜¯çŽ°æœ‰æ•°æ®å¢žå¼ºæ–¹æ³•é‡‡ç”¨å›ºå®šçš„ç­–ç•¥ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®žä¸–ç•Œä¸­ä¼ªé€ æŠ€æœ¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä¾‹å¦‚é¢éƒ¨æ‰­æ›²ã€è¡¨æƒ…æ“çºµç­‰ã€‚è¿™äº›å›ºå®šç­–ç•¥å®¹æ˜“å¯¼è‡´æ¨¡åž‹å­¦ä¹ åˆ°ä¸Žä»»åŠ¡æ— å…³çš„åå·®ï¼Œä»Žè€Œå½±å“å…¶åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨çŽ°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åŠ¨æ€åœ°é€‰æ‹©æ•°æ®å¢žå¼ºç­–ç•¥ï¼Œå¹¶ç»“åˆå› æžœæŽ¨æ–­æ¥å‡å°‘æ¨¡åž‹å­¦ä¹ åˆ°çš„è™šå‡ç›¸å…³æ€§ã€‚é€šè¿‡RLï¼Œå¯ä»¥æ ¹æ®æ£€æµ‹å™¨çš„å­¦ä¹ çŠ¶æ€ï¼Œè‡ªé€‚åº”åœ°ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„å¯¹æŠ—æ ·æœ¬ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„é²æ£’æ€§ã€‚å› æžœæŽ¨æ–­åˆ™ç”¨äºŽå¼•å¯¼æ¨¡åž‹å…³æ³¨å› æžœä¸å˜ç‰¹å¾ï¼ŒæŠ‘åˆ¶ä¸Žä»»åŠ¡æ— å…³çš„åå·®ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCRDAæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šä¼ªé€ æ“ä½œæ± ã€å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å’ŒDeepfakeæ£€æµ‹å™¨ã€‚ä¼ªé€ æ“ä½œæ± åŒ…å«å¤šç§ä¼ªé€ æŠ€æœ¯ï¼Œå¦‚é¢éƒ¨äº¤æ¢ã€è¡¨æƒ…æ“çºµç­‰ã€‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ ¹æ®æ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œä»Žä¼ªé€ æ“ä½œæ± ä¸­é€‰æ‹©åˆé€‚çš„å¢žå¼ºåŠ¨ä½œï¼Œç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚Deepfakeæ£€æµ‹å™¨åˆ™åˆ©ç”¨è¿™äº›å¢žå¼ºåŽçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ•´ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªå¾ªçŽ¯è¿­ä»£çš„è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“ä¸æ–­è°ƒæ•´å¢žå¼ºç­–ç•¥ï¼Œæ£€æµ‹å™¨ä¸æ–­æé«˜æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRDAçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†å¼ºåŒ–å­¦ä¹ å’Œå› æžœæŽ¨æ–­ç»“åˆèµ·æ¥ï¼Œç”¨äºŽè‡ªé€‚åº”åœ°ç”Ÿæˆæ•°æ®å¢žå¼ºæ ·æœ¬ã€‚ä¸Žä¼ ç»Ÿçš„å›ºå®šæ•°æ®å¢žå¼ºæ–¹æ³•ç›¸æ¯”ï¼ŒCRDAèƒ½å¤Ÿæ ¹æ®æ£€æµ‹å™¨çš„å­¦ä¹ çŠ¶æ€åŠ¨æ€åœ°è°ƒæ•´å¢žå¼ºç­–ç•¥ï¼Œä»Žè€Œæ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®žä¸–ç•Œä¸­ä¼ªé€ æŠ€æœ¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œå› æžœæŽ¨æ–­çš„å¼•å…¥å¯ä»¥å‡å°‘æ¨¡åž‹å­¦ä¹ åˆ°çš„è™šå‡ç›¸å…³æ€§ï¼Œæé«˜æ¨¡åž‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCRDAä½¿ç”¨ä¸€ä¸ªåŸºäºŽç­–ç•¥æ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥è®­ç»ƒæ™ºèƒ½ä½“ã€‚æ™ºèƒ½ä½“çš„çŠ¶æ€æ˜¯æ£€æµ‹å™¨çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠ¨ä½œæ˜¯ä»Žä¼ªé€ æ“ä½œæ± ä¸­é€‰æ‹©çš„å¢žå¼ºåŠ¨ä½œã€‚å¥–åŠ±å‡½æ•°æ ¹æ®æ£€æµ‹å™¨åœ¨å¢žå¼ºæ•°æ®ä¸Šçš„æ€§èƒ½æ¥è®¾è®¡ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚å› æžœæŽ¨æ–­é€šè¿‡å¼•å…¥å¹²é¢„å˜é‡æ¥å®žçŽ°ï¼Œç”¨äºŽè¯†åˆ«å’Œæ¶ˆé™¤ä¸Žä»»åŠ¡æ— å…³çš„åå·®ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®æ ¹æ®ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒCRDAåœ¨å¤šä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†Deepfakeæ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨FaceForensics++æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡åž‹ï¼Œåœ¨Celeb-DFæ•°æ®é›†ä¸Šçš„å‡†ç¡®çŽ‡æå‡äº†X%ï¼Œä¼˜äºŽå½“å‰æœ€ä¼˜æ–¹æ³•ã€‚æ¶ˆèžå®žéªŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ å’Œå› æžœæŽ¨æ–­åœ¨æå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæå‡Deepfakeæ£€æµ‹ç³»ç»Ÿåœ¨ç¤¾äº¤åª’ä½“ã€åœ¨çº¿è§†é¢‘å¹³å°ã€å®‰å…¨ç›‘æŽ§ç­‰é¢†åŸŸçš„æ€§èƒ½ï¼Œæœ‰æ•ˆè¯†åˆ«å’Œé˜²èŒƒæ¶æ„ä¼ªé€ å†…å®¹ï¼Œç»´æŠ¤ç½‘ç»œå®‰å…¨å’Œç¤¾ä¼šç¨³å®šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æŽ¨å¹¿åˆ°å…¶ä»–å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå’Œæ¨¡åž‹é²æ£’æ€§æå‡çš„ä»»åŠ¡ä¸­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.

