---
layout: default
title: MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression
---

# MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression

**arXiv**: [2511.06717v2](https://arxiv.org/abs/2511.06717) | [PDF](https://arxiv.org/pdf/2511.06717.pdf)

**ä½œè€…**: Han Liu, Hengyu Man, Xingtao Wang, Wenrui Li, Debin Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10 (æ›´æ–°: 2025-11-14)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆRWKV-Transformerçš„MRTæ¨¡åž‹ï¼Œç”¨äºŽæžä½Žç çŽ‡å›¾åƒåŽ‹ç¼©ï¼Œæ˜¾è‘—æå‡åŽ‹ç¼©æ€§èƒ½ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾åƒåŽ‹ç¼©` `æžä½Žç çŽ‡` `RWKV` `Transformer` `æ··åˆæž¶æž„` `è¡¨ç¤ºå­¦ä¹ ` `å…¨å±€ä¾èµ–` `å±€éƒ¨å†—ä½™`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æžä½Žç çŽ‡å›¾åƒåŽ‹ç¼©æ–¹æ³•ä¾èµ–CNNæˆ–Swin Transformerï¼Œåœ¨2Dæ½œåœ¨ç©ºé—´ä¸­åŽ‹ç¼©å›¾åƒï¼Œç©ºé—´å†—ä½™å¤§ï¼Œé™åˆ¶äº†åŽ‹ç¼©æ€§èƒ½ã€‚
2. æå‡ºæ··åˆRWKV-Transformer (MRT) æž¶æž„ï¼Œå°†å›¾åƒç¼–ç ä¸ºæ›´ç´§å‡‘çš„1ç»´æ½œåœ¨è¡¨ç¤ºï¼Œåˆ©ç”¨RWKVæ•èŽ·å…¨å±€ä¾èµ–ï¼ŒTransformerå»ºæ¨¡å±€éƒ¨å†—ä½™ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMRTåœ¨æžä½Žç çŽ‡ä¸‹å®žçŽ°äº†å“è¶Šçš„é‡å»ºè´¨é‡ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œåœ¨Kodakå’ŒCLIC2020æ•°æ®é›†ä¸Šå®žçŽ°äº†æ˜¾è‘—çš„æ¯”ç‰¹çŽ‡èŠ‚çœã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆRWKV-Transformer (MRT) æž¶æž„ï¼Œæ—¨åœ¨é€šè¿‡ååŒæ•´åˆçº¿æ€§æ³¨æ„åŠ›RWKVå’Œè‡ªæ³¨æ„åŠ›Transformeræ¨¡åž‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå°†å›¾åƒç¼–ç ä¸ºæ›´ç´§å‡‘çš„1ç»´æ½œåœ¨è¡¨ç¤ºï¼Œä»Žè€Œæå‡æžä½Žç çŽ‡å›¾åƒåŽ‹ç¼©çš„æ€§èƒ½ã€‚MRTå°†å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„çª—å£ï¼Œåˆ©ç”¨RWKVæ¨¡å—æ•èŽ·çª—å£é—´çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨Transformerå—å»ºæ¨¡æ¯ä¸ªçª—å£å†…çš„å±€éƒ¨å†—ä½™ã€‚è¿™ç§åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿåœ¨1ç»´åŸŸä¸­å®žçŽ°æ›´é«˜æ•ˆå’Œç´§å‡‘çš„è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åŽ‹ç¼©æ•ˆçŽ‡ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸“é—¨ä¸ºMRTä¸­é—´1ç»´æ½œåœ¨ç‰¹å¾çš„ç»“æž„ç‰¹æ€§é‡èº«å®šåˆ¶çš„RWKVåŽ‹ç¼©æ¨¡åž‹ (RCM)ã€‚åœ¨æ ‡å‡†å›¾åƒåŽ‹ç¼©åŸºå‡†ä¸Šçš„å¤§é‡å®žéªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ‰€æå‡ºçš„MRTæ¡†æž¶åœ¨ä½ŽäºŽ0.02 bppçš„æ¯”ç‰¹çŽ‡ä¸‹å§‹ç»ˆå¦‚ä¸€åœ°å®žçŽ°äº†å“è¶Šçš„é‡å»ºè´¨é‡ã€‚åŸºäºŽDISTSæŒ‡æ ‡çš„å®šé‡ç»“æžœè¡¨æ˜Žï¼ŒMRTæ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„2ç»´æž¶æž„GLCï¼Œåœ¨Kodakå’ŒCLIC2020æµ‹è¯•æ•°æ®é›†ä¸Šåˆ†åˆ«å®žçŽ°äº†43.75%å’Œ30.59%çš„æ¯”ç‰¹çŽ‡èŠ‚çœã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æžä½Žç çŽ‡å›¾åƒåŽ‹ç¼©æ–¹æ³•ï¼Œå¦‚åŸºäºŽCNNæˆ–Swin Transformerçš„æ–¹æ³•ï¼Œé€šå¸¸å°†å›¾åƒåŽ‹ç¼©åˆ°2Dæ½œåœ¨ç©ºé—´ä¸­ã€‚è¿™äº›æ–¹æ³•å€¾å‘äºŽä¿ç•™å¤§é‡çš„ç©ºé—´å†—ä½™ï¼Œä»Žè€Œé™åˆ¶äº†æ•´ä½“åŽ‹ç¼©æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŽ»é™¤ç©ºé—´å†—ä½™ï¼Œç”Ÿæˆæ›´ç´§å‡‘çš„å›¾åƒè¡¨ç¤ºçš„æ¨¡åž‹ï¼Œæ˜¯æžä½Žç çŽ‡å›¾åƒåŽ‹ç¼©çš„å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨RWKVå’ŒTransformerçš„äº’è¡¥ä¼˜åŠ¿ï¼Œè®¾è®¡ä¸€ç§æ··åˆæž¶æž„ï¼ˆMRTï¼‰ï¼Œå°†å›¾åƒç¼–ç ä¸ºæ›´ç´§å‡‘çš„1ç»´æ½œåœ¨è¡¨ç¤ºã€‚RWKVæ“…é•¿æ•èŽ·é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè€ŒTransformeræ“…é•¿å»ºæ¨¡å±€éƒ¨å†—ä½™ã€‚é€šè¿‡å°†ä¸¤è€…ç»“åˆï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åŽ»é™¤å›¾åƒä¸­çš„ç©ºé—´å†—ä½™ï¼Œä»Žè€Œæé«˜åŽ‹ç¼©æ€§èƒ½ã€‚å°†2Då›¾åƒè½¬åŒ–ä¸º1Dåºåˆ—ï¼Œæ›´åˆ©äºŽRWKVå»ºæ¨¡å…¨å±€ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMRTæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å°†è¾“å…¥å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„çª—å£ï¼›2) ä½¿ç”¨RWKVæ¨¡å—æ•èŽ·çª—å£ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ï¼›3) ä½¿ç”¨Transformerå—å»ºæ¨¡æ¯ä¸ªçª—å£å†…çš„å±€éƒ¨å†—ä½™ï¼›4) ä½¿ç”¨RWKVåŽ‹ç¼©æ¨¡åž‹ (RCM) å¯¹ä¸­é—´1ç»´æ½œåœ¨ç‰¹å¾è¿›è¡ŒåŽ‹ç¼©ã€‚æ•´ä½“æµç¨‹æ˜¯ä»Žå›¾åƒåƒç´ åˆ°1Dæ½œåœ¨è¡¨ç¤ºçš„ç¼–ç ï¼Œå†åˆ°åŽ‹ç¼©ï¼Œæœ€åŽè§£ç é‡å»ºå›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMRTçš„å…³é”®åˆ›æ–°åœ¨äºŽæ··åˆä½¿ç”¨äº†RWKVå’ŒTransformerï¼Œå¹¶å°†å…¶åº”ç”¨äºŽå›¾åƒåŽ‹ç¼©çš„1Dæ½œåœ¨è¡¨ç¤ºå­¦ä¹ ä¸­ã€‚ä¸Žä¼ ç»Ÿçš„2DåŽ‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼ŒMRTèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŽ»é™¤ç©ºé—´å†—ä½™ï¼Œç”Ÿæˆæ›´ç´§å‡‘çš„å›¾åƒè¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒRCMçš„è®¾è®¡ä¹Ÿé’ˆå¯¹MRTçš„ä¸­é—´ç‰¹å¾è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†åŽ‹ç¼©æ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMRTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) çª—å£å¤§å°çš„é€‰æ‹©ï¼Œéœ€è¦å¹³è¡¡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„å»ºæ¨¡ï¼›2) RWKVå’ŒTransformeræ¨¡å—çš„å±‚æ•°å’Œå‚æ•°è®¾ç½®ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼›3) RCMçš„ç»“æž„è®¾è®¡ï¼Œéœ€è¦ä¸ŽMRTçš„ä¸­é—´ç‰¹å¾ç›¸åŒ¹é…ï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦å¹³è¡¡é‡å»ºè´¨é‡å’ŒåŽ‹ç¼©çŽ‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæ‘˜è¦æœªæåŠï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMRTåœ¨æžä½Žç çŽ‡ä¸‹æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚åœ¨Kodakæ•°æ®é›†ä¸Šï¼ŒMRTç›¸æ¯”äºŽæœ€å…ˆè¿›çš„2Dæž¶æž„GLCï¼Œå®žçŽ°äº†43.75%çš„æ¯”ç‰¹çŽ‡èŠ‚çœã€‚åœ¨CLIC2020æ•°æ®é›†ä¸Šï¼ŒMRTå®žçŽ°äº†30.59%çš„æ¯”ç‰¹çŽ‡èŠ‚çœã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒMRTåœ¨æžä½Žç çŽ‡å›¾åƒåŽ‹ç¼©æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå¯¹å­˜å‚¨ç©ºé—´æˆ–ä¼ è¾“å¸¦å®½æœ‰ä¸¥æ ¼é™åˆ¶çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å›¾åƒå­˜å‚¨ã€ä½Žå¸¦å®½ç½‘ç»œçŽ¯å¢ƒä¸‹çš„å›¾åƒä¼ è¾“ã€ä»¥åŠé¥æ„Ÿå›¾åƒçš„åŽ‹ç¼©å­˜å‚¨ç­‰ã€‚é€šè¿‡æ›´é«˜æ•ˆçš„å›¾åƒåŽ‹ç¼©ï¼Œå¯ä»¥é™ä½Žå­˜å‚¨æˆæœ¬ã€æé«˜ä¼ è¾“æ•ˆçŽ‡ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„è§†è§‰ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨ç‰©è”ç½‘ã€è§†é¢‘ç›‘æŽ§ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in extreme image compression have revealed that mapping pixel data into highly compact latent representations can significantly improve coding efficiency. However, most existing methods compress images into 2-D latent spaces via convolutional neural networks (CNNs) or Swin Transformers, which tend to retain substantial spatial redundancy, thereby limiting overall compression performance. In this paper, we propose a novel Mixed RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D latent representations by synergistically integrating the complementary strengths of linear-attention-based RWKV and self-attention-based Transformer models. Specifically, MRT partitions each image into fixed-size windows, utilizing RWKV modules to capture global dependencies across windows and Transformer blocks to model local redundancies within each window. The hierarchical attention mechanism enables more efficient and compact representation learning in the 1-D domain. To further enhance compression efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to the structure characteristics of the intermediate 1-D latent features in MRT. Extensive experiments on standard image compression benchmarks validate the effectiveness of our approach. The proposed MRT framework consistently achieves superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp). Quantitative results based on the DISTS metric show that MRT significantly outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets, respectively.

