---
layout: default
title: MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression
---

# MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.06717" target="_blank" class="toolbar-btn">arXiv: 2511.06717v2</a>
    <a href="https://arxiv.org/pdf/2511.06717.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06717v2" 
            onclick="toggleFavorite(this, '2511.06717v2', 'MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Han Liu, Hengyu Man, Xingtao Wang, Wenrui Li, Debin Zhao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-10 (Êõ¥Êñ∞: 2025-11-14)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê∑∑ÂêàRWKV-TransformerÁöÑMRTÊ®°ÂûãÔºåÁî®‰∫éÊûÅ‰ΩéÁ†ÅÁéáÂõæÂÉèÂéãÁº©ÔºåÊòæËëóÊèêÂçáÂéãÁº©ÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÂõæÂÉèÂéãÁº©` `ÊûÅ‰ΩéÁ†ÅÁéá` `RWKV` `Transformer` `Ê∑∑ÂêàÊû∂ÊûÑ` `Ë°®Á§∫Â≠¶‰π†` `ÂÖ®Â±Ä‰æùËµñ` `Â±ÄÈÉ®ÂÜó‰Ωô`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊûÅ‰ΩéÁ†ÅÁéáÂõæÂÉèÂéãÁº©ÊñπÊ≥ï‰æùËµñCNNÊàñSwin TransformerÔºåÂú®2DÊΩúÂú®Á©∫Èó¥‰∏≠ÂéãÁº©ÂõæÂÉèÔºåÁ©∫Èó¥ÂÜó‰ΩôÂ§ßÔºåÈôêÂà∂‰∫ÜÂéãÁº©ÊÄßËÉΩ„ÄÇ
2. ÊèêÂá∫Ê∑∑ÂêàRWKV-Transformer (MRT) Êû∂ÊûÑÔºåÂ∞ÜÂõæÂÉèÁºñÁ†Å‰∏∫Êõ¥Á¥ßÂáëÁöÑ1Áª¥ÊΩúÂú®Ë°®Á§∫ÔºåÂà©Áî®RWKVÊçïËé∑ÂÖ®Â±Ä‰æùËµñÔºåTransformerÂª∫Ê®°Â±ÄÈÉ®ÂÜó‰Ωô„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMRTÂú®ÊûÅ‰ΩéÁ†ÅÁéá‰∏ãÂÆûÁé∞‰∫ÜÂçìË∂äÁöÑÈáçÂª∫Ë¥®ÈáèÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂú®KodakÂíåCLIC2020Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊØîÁâπÁéáËäÇÁúÅ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑∑ÂêàRWKV-Transformer (MRT) Êû∂ÊûÑÔºåÊó®Âú®ÈÄöËøáÂçèÂêåÊï¥ÂêàÁ∫øÊÄßÊ≥®ÊÑèÂäõRWKVÂíåËá™Ê≥®ÊÑèÂäõTransformerÊ®°ÂûãÁöÑ‰∫íË°•‰ºòÂäøÔºåÂ∞ÜÂõæÂÉèÁºñÁ†Å‰∏∫Êõ¥Á¥ßÂáëÁöÑ1Áª¥ÊΩúÂú®Ë°®Á§∫Ôºå‰ªéËÄåÊèêÂçáÊûÅ‰ΩéÁ†ÅÁéáÂõæÂÉèÂéãÁº©ÁöÑÊÄßËÉΩ„ÄÇMRTÂ∞ÜÂõæÂÉèÂàÜÂâ≤ÊàêÂõ∫ÂÆöÂ§ßÂ∞èÁöÑÁ™óÂè£ÔºåÂà©Áî®RWKVÊ®°ÂùóÊçïËé∑Á™óÂè£Èó¥ÁöÑÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂‰ΩøÁî®TransformerÂùóÂª∫Ê®°ÊØè‰∏™Á™óÂè£ÂÜÖÁöÑÂ±ÄÈÉ®ÂÜó‰Ωô„ÄÇËøôÁßçÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ËÉΩÂ§üÂú®1Áª¥Âüü‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÂíåÁ¥ßÂáëÁöÑË°®Á§∫Â≠¶‰π†„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÂéãÁº©ÊïàÁéáÔºåÊú¨ÊñáËøòÂºïÂÖ•‰∫Ü‰∏ìÈó®‰∏∫MRT‰∏≠Èó¥1Áª¥ÊΩúÂú®ÁâπÂæÅÁöÑÁªìÊûÑÁâπÊÄßÈáèË∫´ÂÆöÂà∂ÁöÑRWKVÂéãÁº©Ê®°Âûã (RCM)„ÄÇÂú®Ê†áÂáÜÂõæÂÉèÂéãÁº©Âü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊâÄÊèêÂá∫ÁöÑMRTÊ°ÜÊû∂Âú®‰Ωé‰∫é0.02 bppÁöÑÊØîÁâπÁéá‰∏ãÂßãÁªàÂ¶Ç‰∏ÄÂú∞ÂÆûÁé∞‰∫ÜÂçìË∂äÁöÑÈáçÂª∫Ë¥®Èáè„ÄÇÂü∫‰∫éDISTSÊåáÊ†áÁöÑÂÆöÈáèÁªìÊûúË°®ÊòéÔºåMRTÊòæËëó‰ºò‰∫éÊúÄÂÖàËøõÁöÑ2Áª¥Êû∂ÊûÑGLCÔºåÂú®KodakÂíåCLIC2020ÊµãËØïÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´ÂÆûÁé∞‰∫Ü43.75%Âíå30.59%ÁöÑÊØîÁâπÁéáËäÇÁúÅ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊûÅ‰ΩéÁ†ÅÁéáÂõæÂÉèÂéãÁº©ÊñπÊ≥ïÔºåÂ¶ÇÂü∫‰∫éCNNÊàñSwin TransformerÁöÑÊñπÊ≥ïÔºåÈÄöÂ∏∏Â∞ÜÂõæÂÉèÂéãÁº©Âà∞2DÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇËøô‰∫õÊñπÊ≥ïÂÄæÂêë‰∫é‰øùÁïôÂ§ßÈáèÁöÑÁ©∫Èó¥ÂÜó‰ΩôÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÊï¥‰ΩìÂéãÁº©ÊÄßËÉΩ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïËÆæËÆ°‰∏ÄÁßçËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÂéªÈô§Á©∫Èó¥ÂÜó‰ΩôÔºåÁîüÊàêÊõ¥Á¥ßÂáëÁöÑÂõæÂÉèË°®Á§∫ÁöÑÊ®°ÂûãÔºåÊòØÊûÅ‰ΩéÁ†ÅÁéáÂõæÂÉèÂéãÁº©ÁöÑÂÖ≥ÈîÆÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®RWKVÂíåTransformerÁöÑ‰∫íË°•‰ºòÂäøÔºåËÆæËÆ°‰∏ÄÁßçÊ∑∑ÂêàÊû∂ÊûÑÔºàMRTÔºâÔºåÂ∞ÜÂõæÂÉèÁºñÁ†Å‰∏∫Êõ¥Á¥ßÂáëÁöÑ1Áª¥ÊΩúÂú®Ë°®Á§∫„ÄÇRWKVÊìÖÈïøÊçïËé∑ÈïøË∑ùÁ¶ª‰æùËµñÂÖ≥Á≥ªÔºåËÄåTransformerÊìÖÈïøÂª∫Ê®°Â±ÄÈÉ®ÂÜó‰Ωô„ÄÇÈÄöËøáÂ∞Ü‰∏§ËÄÖÁªìÂêàÔºåÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞ÂéªÈô§ÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥ÂÜó‰ΩôÔºå‰ªéËÄåÊèêÈ´òÂéãÁº©ÊÄßËÉΩ„ÄÇÂ∞Ü2DÂõæÂÉèËΩ¨Âåñ‰∏∫1DÂ∫èÂàóÔºåÊõ¥Âà©‰∫éRWKVÂª∫Ê®°ÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMRTÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) Â∞ÜËæìÂÖ•ÂõæÂÉèÂàÜÂâ≤ÊàêÂõ∫ÂÆöÂ§ßÂ∞èÁöÑÁ™óÂè£Ôºõ2) ‰ΩøÁî®RWKVÊ®°ÂùóÊçïËé∑Á™óÂè£‰πãÈó¥ÁöÑÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ªÔºõ3) ‰ΩøÁî®TransformerÂùóÂª∫Ê®°ÊØè‰∏™Á™óÂè£ÂÜÖÁöÑÂ±ÄÈÉ®ÂÜó‰ΩôÔºõ4) ‰ΩøÁî®RWKVÂéãÁº©Ê®°Âûã (RCM) ÂØπ‰∏≠Èó¥1Áª¥ÊΩúÂú®ÁâπÂæÅËøõË°åÂéãÁº©„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØ‰ªéÂõæÂÉèÂÉèÁ¥†Âà∞1DÊΩúÂú®Ë°®Á§∫ÁöÑÁºñÁ†ÅÔºåÂÜçÂà∞ÂéãÁº©ÔºåÊúÄÂêéËß£Á†ÅÈáçÂª∫ÂõæÂÉè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMRTÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊ∑∑Âêà‰ΩøÁî®‰∫ÜRWKVÂíåTransformerÔºåÂπ∂Â∞ÜÂÖ∂Â∫îÁî®‰∫éÂõæÂÉèÂéãÁº©ÁöÑ1DÊΩúÂú®Ë°®Á§∫Â≠¶‰π†‰∏≠„ÄÇ‰∏é‰º†ÁªüÁöÑ2DÂéãÁº©ÊñπÊ≥ïÁõ∏ÊØîÔºåMRTËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÂéªÈô§Á©∫Èó¥ÂÜó‰ΩôÔºåÁîüÊàêÊõ¥Á¥ßÂáëÁöÑÂõæÂÉèË°®Á§∫„ÄÇÊ≠§Â§ñÔºåRCMÁöÑËÆæËÆ°‰πüÈíàÂØπMRTÁöÑ‰∏≠Èó¥ÁâπÂæÅËøõË°å‰∫Ü‰ºòÂåñÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÂéãÁº©ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMRTÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Á™óÂè£Â§ßÂ∞èÁöÑÈÄâÊã©ÔºåÈúÄË¶ÅÂπ≥Ë°°Â±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØÁöÑÂª∫Ê®°Ôºõ2) RWKVÂíåTransformerÊ®°ÂùóÁöÑÂ±ÇÊï∞ÂíåÂèÇÊï∞ËÆæÁΩÆÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥Ôºõ3) RCMÁöÑÁªìÊûÑËÆæËÆ°ÔºåÈúÄË¶Å‰∏éMRTÁöÑ‰∏≠Èó¥ÁâπÂæÅÁõ∏ÂåπÈÖçÔºõ4) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÈúÄË¶ÅÂπ≥Ë°°ÈáçÂª∫Ë¥®ÈáèÂíåÂéãÁº©Áéá„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞ÔºàÊëòË¶ÅÊú™ÊèêÂèäÔºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMRTÂú®ÊûÅ‰ΩéÁ†ÅÁéá‰∏ãÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂú®KodakÊï∞ÊçÆÈõÜ‰∏äÔºåMRTÁõ∏ÊØî‰∫éÊúÄÂÖàËøõÁöÑ2DÊû∂ÊûÑGLCÔºåÂÆûÁé∞‰∫Ü43.75%ÁöÑÊØîÁâπÁéáËäÇÁúÅ„ÄÇÂú®CLIC2020Êï∞ÊçÆÈõÜ‰∏äÔºåMRTÂÆûÁé∞‰∫Ü30.59%ÁöÑÊØîÁâπÁéáËäÇÁúÅ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåMRTÂú®ÊûÅ‰ΩéÁ†ÅÁéáÂõæÂÉèÂéãÁº©ÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂØπÂ≠òÂÇ®Á©∫Èó¥Êàñ‰º†ËæìÂ∏¶ÂÆΩÊúâ‰∏•Ê†ºÈôêÂà∂ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂõæÂÉèÂ≠òÂÇ®„ÄÅ‰ΩéÂ∏¶ÂÆΩÁΩëÁªúÁéØÂ¢É‰∏ãÁöÑÂõæÂÉè‰º†Ëæì„ÄÅ‰ª•ÂèäÈÅ•ÊÑüÂõæÂÉèÁöÑÂéãÁº©Â≠òÂÇ®Á≠â„ÄÇÈÄöËøáÊõ¥È´òÊïàÁöÑÂõæÂÉèÂéãÁº©ÔºåÂèØ‰ª•Èôç‰ΩéÂ≠òÂÇ®ÊàêÊú¨„ÄÅÊèêÈ´ò‰º†ËæìÊïàÁéáÔºåÂπ∂‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Â•ΩÁöÑËßÜËßâ‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®Áâ©ËÅîÁΩë„ÄÅËßÜÈ¢ëÁõëÊéßÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in extreme image compression have revealed that mapping pixel data into highly compact latent representations can significantly improve coding efficiency. However, most existing methods compress images into 2-D latent spaces via convolutional neural networks (CNNs) or Swin Transformers, which tend to retain substantial spatial redundancy, thereby limiting overall compression performance. In this paper, we propose a novel Mixed RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D latent representations by synergistically integrating the complementary strengths of linear-attention-based RWKV and self-attention-based Transformer models. Specifically, MRT partitions each image into fixed-size windows, utilizing RWKV modules to capture global dependencies across windows and Transformer blocks to model local redundancies within each window. The hierarchical attention mechanism enables more efficient and compact representation learning in the 1-D domain. To further enhance compression efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to the structure characteristics of the intermediate 1-D latent features in MRT. Extensive experiments on standard image compression benchmarks validate the effectiveness of our approach. The proposed MRT framework consistently achieves superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp). Quantitative results based on the DISTS metric show that MRT significantly outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets, respectively.

