---
layout: default
title: MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity
---

# MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.12061" target="_blank" class="toolbar-btn">arXiv: 2511.12061v1</a>
    <a href="https://arxiv.org/pdf/2511.12061.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12061v1" 
            onclick="toggleFavorite(this, '2511.12061v1', 'MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhichen Lai, Hua Lu, Huan Li, Jialiang Li, Christian S. Jensen

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.DB

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-15

**Â§áÊ≥®**: 8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MovSemCLÊ°ÜÊû∂ÔºåÈÄöËøáËøêÂä®ËØ≠‰πâÂØπÊØîÂ≠¶‰π†ÊèêÂçáËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËΩ®ËøπÁõ∏‰ººÂ∫¶` `ÂØπÊØîÂ≠¶‰π†` `ËøêÂä®ËØ≠‰πâ` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `Êï∞ÊçÆÂ¢ûÂº∫` `ËΩ®ËøπËÅöÁ±ª` `ÂºÇÂ∏∏Ê£ÄÊµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊñπÊ≥ïÂú®ËΩ®ËøπËØ≠‰πâÂª∫Ê®°„ÄÅËÆ°ÁÆóÊïàÁéáÂíåÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇ
2. MovSemCLÈÄöËøáËøêÂä®ËØ≠‰πâÁâπÂæÅÊèêÂèñ„ÄÅÂàÜÂ±ÇÊ≥®ÊÑèÂäõÁºñÁ†ÅÂíåÊõ≤ÁéáÂºïÂØºÂ¢ûÂº∫Êù•ÊèêÂçáËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆó„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMovSemCLÂú®Áõ∏‰ººÂ∫¶ÊêúÁ¥¢ÂíåÂêØÂèëÂºèËøë‰ºº‰ªªÂä°‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Èôç‰Ωé‰∫ÜÊé®ÁêÜÂª∂Ëøü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊòØËÅöÁ±ª„ÄÅÈ¢ÑÊµãÂíåÂºÇÂ∏∏Ê£ÄÊµãÁ≠â‰ªªÂä°ÁöÑÂü∫Á°Ä„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫‰∫éÂ≠¶‰π†ÁöÑÊñπÊ≥ïÂ≠òÂú®‰∏â‰∏™‰∏ªË¶ÅÈôêÂà∂Ôºö(1) ËΩ®ËøπËØ≠‰πâÂíåÂ±ÇÊ¨°ÁªìÊûÑÂª∫Ê®°‰∏çË∂≥ÔºåÁº∫‰πèËøêÂä®Âä®ÊÄÅÊèêÂèñÂíåÂ§öÂ∞∫Â∫¶ÁªìÊûÑË°®Á§∫Ôºõ(2) Áî±‰∫éÈÄêÁÇπÁºñÁ†ÅÂØºËá¥ËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºõ(3) ‰ΩøÁî®Áâ©ÁêÜ‰∏ä‰∏çÂêàÁêÜÁöÑÂ¢ûÂº∫ÊñπÂºèÔºåÊâ≠Êõ≤‰∫ÜËΩ®ËøπËØ≠‰πâ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMovSemCLÔºå‰∏Ä‰∏™Áî®‰∫éËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÁöÑËøêÂä®ËØ≠‰πâÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂„ÄÇMovSemCLÈ¶ñÂÖàÂ∞ÜÂéüÂßãGPSËΩ®ËøπËΩ¨Êç¢‰∏∫ËøêÂä®ËØ≠‰πâÁâπÂæÅÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ÂàÜÂâ≤Êàêpatches„ÄÇÊé•‰∏ãÊù•ÔºåMovSemCLÈááÁî®patchÂÜÖÂíåpatchÈó¥ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ÁºñÁ†ÅÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËΩ®ËøπÊ®°ÂºèÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÂàÜÂ±ÇË°®Á§∫Âπ∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇÊ≠§Â§ñÔºåMovSemCLÂåÖÂê´‰∏ÄÁßçÊõ≤ÁéáÂºïÂØºÁöÑÂ¢ûÂº∫Á≠ñÁï•ÔºåËØ•Á≠ñÁï•‰øùÁïô‰ø°ÊÅØ‰∏∞ÂØåÁöÑÁâáÊÆµÔºà‰æãÂ¶ÇÔºåËΩ¨ÂºØÂíå‰∫§ÂèâÂè£ÔºâÂπ∂Â±èËîΩÂÜó‰ΩôÁâáÊÆµÔºå‰ªéËÄåÁîüÊàêÁâ©ÁêÜ‰∏äÂêàÁêÜÁöÑÂ¢ûÂº∫ËßÜÂõæ„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåMovSemCLËÉΩÂ§ü‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂú®Áõ∏‰ººÂ∫¶ÊêúÁ¥¢‰ªªÂä°‰∏≠ÂÆûÁé∞Êé•ËøëÁêÜÊÉ≥ÂÄº1ÁöÑÂπ≥ÂùáÊéíÂêçÔºåÂπ∂Âú®ÂêØÂèëÂºèËøë‰ººÊñπÈù¢ÊèêÈ´òÈ´òËææ20.3%ÔºåÂêåÊó∂Â∞ÜÊé®ÁêÜÂª∂ËøüÈôç‰ΩéÈ´òËææ43.4%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂ≠¶‰π†ÁöÑËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊñπÊ≥ïÔºåÊó†Ê≥ïÂÖÖÂàÜÂª∫Ê®°ËΩ®ËøπÁöÑËøêÂä®ËØ≠‰πâÂíåÂ±ÇÊ¨°ÁªìÊûÑÔºåÂØºËá¥Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÁ≤æÂ∫¶‰∏çÈ´ò„ÄÇÂêåÊó∂ÔºåÈÄêÁÇπÁºñÁ†ÅÊñπÂºèËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ï‰∏çÂêàÁêÜÔºå‰ºöÊâ≠Êõ≤ËΩ®ËøπÁöÑÁúüÂÆûËØ≠‰πâ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÊúâÊïàÊèêÂèñËΩ®ËøπËØ≠‰πâ„ÄÅÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨Âπ∂ËøõË°åÂêàÁêÜÊï∞ÊçÆÂ¢ûÂº∫ÁöÑËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMovSemCLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËøêÂä®ËØ≠‰πâÁâπÂæÅÊèêÂèñÔºåÂ∞ÜÂéüÂßãGPSËΩ®ËøπËΩ¨Êç¢‰∏∫Êõ¥ÂÖ∑Ë°®ËææÂäõÁöÑÁâπÂæÅË°®Á§∫„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®ÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂØπËΩ®ËøπÁöÑÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÊ®°ÂºèËøõË°åÁºñÁ†ÅÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÂ±ÇÊ¨°ÂåñË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÈááÁî®Êõ≤ÁéáÂºïÂØºÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•Ôºå‰øùÁïôËΩ®Ëøπ‰∏≠ÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂπ∂Â±èËîΩÂÜó‰Ωô‰ø°ÊÅØÔºåÁîüÊàêÁâ©ÁêÜ‰∏äÂêàÁêÜÁöÑÂ¢ûÂº∫ËßÜÂõæ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMovSemCLÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ËøêÂä®ËØ≠‰πâÁâπÂæÅÊèêÂèñÔºöÂ∞ÜÂéüÂßãGPSËΩ®ËøπËΩ¨Êç¢‰∏∫ËøêÂä®ËØ≠‰πâÁâπÂæÅ„ÄÇ2) ËΩ®ËøπÂàÜÂùóÔºöÂ∞ÜËΩ®ËøπÂàÜÂâ≤Êàêpatches„ÄÇ3) ÂàÜÂ±ÇÊ≥®ÊÑèÂäõÁºñÁ†ÅÔºöÂà©Áî®patchÂÜÖÂíåpatchÈó¥ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂØπÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËΩ®ËøπÊ®°ÂºèËøõË°åÁºñÁ†Å„ÄÇ4) Êõ≤ÁéáÂºïÂØºÊï∞ÊçÆÂ¢ûÂº∫ÔºöÊ†πÊçÆËΩ®ËøπÁöÑÊõ≤ÁéáÔºå‰øùÁïô‰ø°ÊÅØ‰∏∞ÂØåÁöÑÁâáÊÆµÔºåÂπ∂Â±èËîΩÂÜó‰ΩôÁâáÊÆµ„ÄÇ5) ÂØπÊØîÂ≠¶‰π†ÔºöÈÄöËøáÂØπÊØîÂ≠¶‰π†ÔºåÂ≠¶‰π†ËΩ®ËøπÁöÑÁõ∏‰ººÂ∫¶Ë°®Á§∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMovSemCLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜËøêÂä®ËØ≠‰πâÁâπÂæÅÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Ë°®ËææËΩ®ËøπÁöÑËøêÂä®‰ø°ÊÅØ„ÄÇ2) ÈááÁî®‰∫ÜÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞ÁºñÁ†ÅËΩ®ËøπÁöÑÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÊ®°Âºè„ÄÇ3) ËÆæËÆ°‰∫ÜÊõ≤ÁéáÂºïÂØºÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÔºåËÉΩÂ§üÁîüÊàêÁâ©ÁêÜ‰∏äÂêàÁêÜÁöÑÂ¢ûÂº∫ËßÜÂõæ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMovSemCLÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËøêÂä®ËØ≠‰πâÁâπÂæÅÁöÑÂÖ∑‰ΩìËÆ°ÁÆóÊñπÊ≥ïÔºå‰æãÂ¶ÇÈÄüÂ∫¶„ÄÅÂä†ÈÄüÂ∫¶„ÄÅÊñπÂêëÁ≠â„ÄÇ2) patchÁöÑÂ§ßÂ∞èÂíåÊï∞Èáè„ÄÇ3) Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÔºå‰æãÂ¶ÇTransformer„ÄÇ4) Êõ≤ÁéáÁöÑËÆ°ÁÆóÊñπÊ≥ïÂíåÈòàÂÄºËÆæÂÆö„ÄÇ5) ÂØπÊØîÂ≠¶‰π†ÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰æãÂ¶ÇInfoNCE„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMovSemCLÂú®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂú®Áõ∏‰ººÂ∫¶ÊêúÁ¥¢‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊé•ËøëÁêÜÊÉ≥ÂÄº1ÁöÑÂπ≥ÂùáÊéíÂêçÔºåÂπ∂Âú®ÂêØÂèëÂºèËøë‰ººÊñπÈù¢ÊèêÈ´ò‰∫ÜÈ´òËææ20.3%ÔºåÂêåÊó∂Â∞ÜÊé®ÁêÜÂª∂ËøüÈôç‰Ωé‰∫ÜÈ´òËææ43.4%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåMovSemCLËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òËΩ®ËøπÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÁöÑÁ≤æÂ∫¶ÂíåÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MovSemCLÂèØÂ∫îÁî®‰∫éËΩ®ËøπËÅöÁ±ª„ÄÅËΩ®ËøπÈ¢ÑÊµã„ÄÅÂºÇÂ∏∏ËΩ®ËøπÊ£ÄÊµãÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®‰∫§ÈÄöÁÆ°ÁêÜ‰∏≠ÔºåÂèØ‰ª•Âà©Áî®MovSemCLÂØπËΩ¶ËæÜËΩ®ËøπËøõË°åËÅöÁ±ªÔºå‰ªéËÄåÂàÜÊûê‰∫§ÈÄöÊ®°ÂºèÔºõÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåÂèØ‰ª•Âà©Áî®MovSemCLÈ¢ÑÊµãËΩ¶ËæÜÁöÑÊú™Êù•ËΩ®ËøπÔºå‰ªéËÄåÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄßÔºõÂú®ÈáëËûçÈ£éÊéß‰∏≠ÔºåÂèØ‰ª•Âà©Áî®MovSemCLÊ£ÄÊµãÂºÇÂ∏∏‰∫§ÊòìËΩ®ËøπÔºå‰ªéËÄåÈò≤Ê≠¢Ê¨∫ËØàË°å‰∏∫„ÄÇËØ•Á†îÁ©∂ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

