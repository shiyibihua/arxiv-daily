---
layout: default
title: Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning
---

# Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21873" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21873v1</a>
  <a href="https://arxiv.org/pdf/2506.21873.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21873v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21873v1', 'Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tzu-Chun Chien, Chieh-Kai Lin, Shiang-Feng Tsai, Ruei-Chi Lai, Hung-Jen Chen, Min Sun

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå®šä½æ„ŸçŸ¥çš„æ ‡è®°å‰ªæä»¥è§£å†³è§†è§‰å®šä½æ€§èƒ½ä¸‹é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å®šä½` `å¤šæ¨¡æ€å­¦ä¹ ` `æ ‡è®°å‰ªæ` `ä½ç½®IDè°ƒæ•´` `æ€§èƒ½æ¢å¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ ‡è®°å‰ªææ–¹æ³•åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå¯¼è‡´æ¨¡å‹çš„è§†è§‰å®šä½èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œå½±å“äº†ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„åŸºäºå®šä½æ„ŸçŸ¥çš„æ ‡è®°å‰ªæï¼ˆGAPï¼‰æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´ä½ç½®IDæ¥æ¢å¤æ¨¡å‹çš„å®šä½æ€§èƒ½ï¼Œè§£å†³äº†å‰ªæå¸¦æ¥çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGAPæ–¹æ³•ä½¿å¾—RECçš„å‡†ç¡®ç‡ä»15.34%æ¢å¤è‡³51.42%ï¼Œåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡å®ç°äº†æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºå„ç§è§†è§‰-è¯­è¨€åº”ç”¨çš„é€šç”¨æ¥å£ã€‚ç„¶è€Œï¼Œæ ‡è®°å‰ªææ–¹æ³•çš„åº”ç”¨æ˜¾è‘—å‰Šå¼±äº†æ¨¡å‹çš„å®šä½èƒ½åŠ›ï¼Œå¯¼è‡´é¢„æµ‹é”™è¯¯å’Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚ä»¥Referring Expression Comprehensionï¼ˆRECï¼‰ä¸ºä¾‹ï¼Œå‰ªæä½¿å¾—LLaVAåœ¨RefCOCOéªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä»56.14%é™è‡³15.34%ã€‚åˆ†æè¡¨æ˜ï¼Œå‰ªæåä½ç½®IDçš„ä¸å¯¹é½æ˜¯æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå®šä½æ„ŸçŸ¥çš„æ ‡è®°å‰ªæï¼ˆGAPï¼‰ï¼Œé€šè¿‡ç®€å•æœ‰æ•ˆçš„è°ƒæ•´ä½ç½®IDï¼Œä½¿RECå‡†ç¡®ç‡æ¢å¤è‡³51.42%ï¼Œç›¸å½“äºåŸå§‹æ€§èƒ½çš„90%ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒã€å†…å­˜æˆ–è®¡ç®—å¼€é”€ã€‚è¯¥æ–¹æ³•åœ¨Shikraã€MiniGPTv2å’ŒLLaVAç³»åˆ—æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ ‡è®°å‰ªæå¯¼è‡´çš„è§†è§‰å®šä½æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰å‰ªææ–¹æ³•åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæ˜¾è‘—å‰Šå¼±äº†æ¨¡å‹çš„å®šä½èƒ½åŠ›ï¼Œå¯¼è‡´é¢„æµ‹é”™è¯¯å’Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯æå‡ºåŸºäºå®šä½æ„ŸçŸ¥çš„æ ‡è®°å‰ªæï¼ˆGAPï¼‰ï¼Œé€šè¿‡å¯¹ä½ç½®IDè¿›è¡Œç®€å•æœ‰æ•ˆçš„è°ƒæ•´ï¼Œæ¢å¤æ¨¡å‹åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨ä¿æŒä½ç½®IDçš„é¡ºåºå’Œæ•°å€¼ä¸€è‡´æ€§ï¼Œä»è€Œæå‡å‰ªæåçš„æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œæ ‡è®°å‰ªæï¼Œç„¶åé€šè¿‡GAPæ–¹æ³•è°ƒæ•´ä½ç½®IDï¼Œæœ€åè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å‰ªææ¨¡å—ã€ä½ç½®IDè°ƒæ•´æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†GAPæ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´ä½ç½®IDæ¥è§£å†³å‰ªæåæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒGAPå…³æ³¨äºä½ç½®IDçš„å¯¹é½ï¼Œè€Œä¸ä»…ä»…æ˜¯å‰ªæçš„æ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GAPæ–¹æ³•ä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹ä½ç½®IDçš„é‡æ–°æ’åºå’Œæ•°å€¼è°ƒæ•´ï¼Œç¡®ä¿åœ¨å‰ªæåæ¨¡å‹ä»èƒ½ä¿æŒå¯¹è¾“å…¥çš„æ­£ç¡®ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–è®¡ç®—å¼€é”€ï¼Œæå¤§åœ°æé«˜äº†å®ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGAPæ–¹æ³•ä½¿å¾—LLaVAåœ¨RefCOCOéªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä»15.34%æå‡è‡³51.42%ï¼Œæ¢å¤äº†90%çš„åŸå§‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨Shikraå’ŒMiniGPTv2ç­‰å¤šä¸ªæ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†è§‰é—®ç­”ã€å›¾åƒæ ‡æ³¨å’Œäººæœºäº¤äº’ç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡æé«˜å‰ªæåçš„æ¨¡å‹æ€§èƒ½ï¼ŒGAPæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„è§†è§‰-è¯­è¨€å¤„ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual grounding, establishing themselves as a general interface for various vision-language applications. This progress has driven the development of token pruning methods to mitigate the high computational costs associated with processing numerous visual tokens. However, we observe that pruning significantly weakens the model's grounding ability, leading to incorrect predictions and drastic performance degradation. In Referring Expression Comprehension (REC), for instance, pruning causes the accuracy of LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis identifies misaligned position IDs after pruning as the primary cause of this degradation, as both the order and value of these IDs are crucial for maintaining performance in grounding tasks. To address this issue, we propose Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs that recovers REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting, all while requiring no additional training, memory, or computational overhead. Applied to models such as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves performance across various token pruning strategies.

