---
layout: default
title: Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs
---

# Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22146" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22146v4</a>
  <a href="https://arxiv.org/pdf/2506.22146.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22146v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22146v4', 'Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amirmohammad Izadi, Mohammad Ali Banayeeanzade, Fatemeh Askari, Ali Rahimiakbar, Mohammad Mahdi Vahedi, Hosein Hasani, Mahdieh Soleymani Baghshah

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-11-10)

**å¤‡æ³¨**: Accepted to NeurIPS 2025 (Thirty-ninth Conference on Neural Information Processing Systems)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVISERä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç»‘å®šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ç©ºé—´ç»“æ„` `åºåˆ—è§£æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä¸­é¢ä¸´ç»‘å®šé—®é¢˜ï¼Œå¯¼è‡´åœ¨è®¡æ•°ã€è§†è§‰æœç´¢ç­‰ä»»åŠ¡ä¸­é¢‘ç¹å‡ºé”™ã€‚
2. æœ¬æ–‡æå‡ºçš„VISERæ–¹æ³•é€šè¿‡å¼•å…¥ä½çº§ç©ºé—´ç»“æ„ï¼Œå¢å¼ºè§†è§‰è¾“å…¥å¹¶ç»“åˆæ–‡æœ¬æç¤ºï¼Œå®ç°äº†ç©ºé—´æ„ŸçŸ¥çš„é¡ºåºè§£æã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVISERåœ¨å¤šä¸ªè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶åœ¨è§†è§‰æœç´¢å’Œè®¡æ•°ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å–å¾—äº†è¿›å±•ï¼Œä½†å…¶åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„èƒ½åŠ›å¸¸å¸¸å—åˆ°ç»‘å®šé—®é¢˜çš„é™åˆ¶ï¼Œå³æ— æ³•å¯é åœ°å°†æ„ŸçŸ¥ç‰¹å¾ä¸å…¶æ­£ç¡®çš„è§†è§‰æŒ‡ç§°å…³è”ã€‚å½“å‰LVLMsä¸»è¦å¹¶è¡Œå¤„ç†è§†è§‰ç‰¹å¾ï¼Œç¼ºä¹ç©ºé—´åŸºç¡€çš„åºåˆ—æ³¨æ„æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†å¢å¼ºæ¨ç†çš„è§†è§‰è¾“å…¥ç»“æ„ï¼ˆVISERï¼‰ï¼Œé€šè¿‡ä½çº§ç©ºé—´ç»“æ„å¢å¼ºè§†è§‰è¾“å…¥ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºé…å¯¹ï¼Œé¼“åŠ±é¡ºåºå’Œç©ºé—´æ„ŸçŸ¥è§£æã€‚å®éªŒè¯æ˜ï¼ŒVISERåœ¨æ ¸å¿ƒè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰æœç´¢ã€è®¡æ•°å’Œç©ºé—´å…³ç³»ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†25.0%ã€26.8%å’Œ9.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰æ¨ç†ä¸­é¢ä¸´çš„ç»‘å®šé—®é¢˜ï¼Œå³æ— æ³•æœ‰æ•ˆå…³è”æ„ŸçŸ¥ç‰¹å¾ä¸å…¶è§†è§‰æŒ‡ç§°ï¼Œå¯¼è‡´åœ¨è®¡æ•°ã€è§†è§‰æœç´¢ç­‰ä»»åŠ¡ä¸­çš„é”™è¯¯ç‡è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„VISERæ–¹æ³•é€šè¿‡å¼•å…¥ä½çº§ç©ºé—´ç»“æ„æ¥å¢å¼ºè§†è§‰è¾“å…¥ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºç»“åˆï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œé¡ºåºå’Œç©ºé—´æ„ŸçŸ¥çš„è§£æï¼Œä»è€Œæ”¹å–„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVISERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯è§†è§‰è¾“å…¥æ¨¡å—ï¼Œé€šè¿‡ä½çº§ç©ºé—´ç»“æ„å¢å¼ºè§†è§‰ç‰¹å¾ï¼›å…¶æ¬¡æ˜¯æ–‡æœ¬æç¤ºæ¨¡å—ï¼Œè®¾è®¡ç”¨äºå¼•å¯¼æ¨¡å‹è¿›è¡Œç©ºé—´æ„ŸçŸ¥çš„é¡ºåºè§£æã€‚

**å…³é”®åˆ›æ–°**ï¼šVISERçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è§†è§‰è¾“å…¥çš„è®¾è®¡ä¸æ–‡æœ¬æç¤ºç›¸ç»“åˆï¼Œå¼ºè°ƒè§†è§‰ç»“æ„çš„é‡è¦æ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„çº¯è¯­è¨€æ¨ç†ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVISERåœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒVISERé‡‡ç”¨äº†ç‰¹å®šçš„ç©ºé—´ç»“æ„å‚æ•°è®¾ç½®ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æç¤ºçš„ç»“åˆæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVISERåœ¨è§†è§‰æœç´¢ã€è®¡æ•°å’Œç©ºé—´å…³ç³»ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†25.0%ã€26.8%å’Œ9.5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨2Dæ•°æ®é›†çš„åœºæ™¯æè¿°ä¸­å°†ç¼–è¾‘è·ç¦»é”™è¯¯å‡å°‘äº†0.32ï¼Œæ˜¾è‘—ä¼˜äºçº¯æ–‡æœ¬ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è§†è§‰æœç´¢ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆä»¥åŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼ŒVISERå¯ä»¥åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œç†è§£ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite progress in Large Vision-Language Models (LVLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current LVLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces Visual Input Structure for Enhanced Reasoning (VISER), a simple, effective method that augments visual inputs with low-level spatial structures and pairs them with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks, using only a single-query inference. Specifically, VISER improves GPT-4o performance on visual search, counting, and spatial relationship tasks by 25.0%, 26.8%, and 9.5%, respectively, and reduces edit distance error in scene description by 0.32 on 2D datasets. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER underscores the importance of visual input design over purely linguistically based reasoning strategies and suggests that visual structuring is a powerful and general approach for enhancing compositional and spatial reasoning in LVLMs.

