---
layout: default
title: TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models
---

# TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21975v1</a>
  <a href="https://arxiv.org/pdf/2506.21975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21975v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21975v1', 'TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meng Yu, Te Cui, Qitong Chu, Wenjie Song, Yi Yang, Yufeng Yue

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: 6 pages, accepted for publication in lEEE/RSJ international Conference on Intelligent Robots and Systems (lROS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTASegæ¡†æ¶ä»¥è§£å†³RGB-Tè¯­ä¹‰åˆ†å‰²ä¸­çš„æ–‡æœ¬ä¿¡æ¯ç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `RGB-Tè¯­ä¹‰åˆ†å‰²` `æ–‡æœ¬ä¿¡æ¯èåˆ` `ä½ç§©é€‚åº”` `åŠ¨æ€ç‰¹å¾èåˆ` `CLIPåµŒå…¥` `æ™ºèƒ½ç³»ç»Ÿ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¸»è¦ä¾èµ–ä½çº§è§†è§‰ç‰¹å¾ï¼Œç¼ºä¹é«˜çº§æ–‡æœ¬ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨ç›¸ä¼¼è§†è§‰ç‰¹å¾ç±»åˆ«é—´çš„åˆ†å‰²å‡†ç¡®æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºTASegæ¡†æ¶ï¼Œé€šè¿‡ä½ç§©é€‚åº”æŠ€æœ¯å¾®è°ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼•å…¥åŠ¨æ€ç‰¹å¾èåˆæ¨¡å—ä»¥æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ç‰¹å¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTASegåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­ï¼Œå‚æ•°é‡æ˜¾è‘—å‡å°‘ï¼Œæ€§èƒ½æå‡æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­ï¼Œå¯é çš„å¼€æ”¾ç¯å¢ƒè¯­ä¹‰åˆ†å‰²è‡³å…³é‡è¦ï¼Œä½†ä»é¢ä¸´æ˜¾è‘—é—®é¢˜ï¼šç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¸»è¦ä¾èµ–ä½çº§è§†è§‰ç‰¹å¾ï¼Œç¼ºä¹é«˜çº§æ–‡æœ¬ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨ç±»åˆ«å…±äº«ç›¸ä¼¼è§†è§‰ç‰¹å¾æ—¶éš¾ä»¥å®ç°å‡†ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œå°½ç®¡SAMåœ¨å®ä¾‹çº§åˆ†å‰²ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸çƒ­æˆåƒå’Œæ–‡æœ¬çš„ç»“åˆå—åˆ°æ¨¡æ€å¼‚è´¨æ€§å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TASegæ¡†æ¶ï¼Œé€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯æ¥é€‚åº”è§†è§‰åŸºç¡€æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨å›¾åƒç¼–ç å™¨ä¸­æå‡ºäº†åŠ¨æ€ç‰¹å¾èåˆæ¨¡å—ï¼ˆDFFMï¼‰ï¼Œæœ‰æ•ˆèåˆå¤šç§è§†è§‰æ¨¡æ€çš„ç‰¹å¾ï¼ŒåŒæ—¶å†»ç»“SAMçš„åŸå§‹å˜æ¢å™¨å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ©è†œè§£ç å™¨ä¸­å¼•å…¥äº†CLIPç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ï¼Œä»¥å®ç°è¯­ä¹‰å¯¹é½ï¼Œä»è€Œè¿›ä¸€æ­¥çº æ­£åˆ†ç±»é”™è¯¯ï¼Œæé«˜è¯­ä¹‰ç†è§£çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶ä¸”è®­ç»ƒå‚æ•°æ›´å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨å¤„ç†ç›¸ä¼¼è§†è§‰ç‰¹å¾ç±»åˆ«æ—¶çš„å‡†ç¡®æ€§ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹æ–‡æœ¬ä¿¡æ¯çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨¡æ€å¼‚è´¨æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¹Ÿå­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTASegæ¡†æ¶é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¾®è°ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç»“åˆåŠ¨æ€ç‰¹å¾èåˆæ¨¡å—ï¼ˆDFFMï¼‰å’ŒCLIPç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ï¼Œå¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTASegçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€åŠ¨æ€ç‰¹å¾èåˆæ¨¡å—å’Œæ©è†œè§£ç å™¨ã€‚å›¾åƒç¼–ç å™¨è´Ÿè´£æå–å¤šæ¨¡æ€ç‰¹å¾ï¼ŒDFFMç”¨äºèåˆè¿™äº›ç‰¹å¾ï¼Œè€Œæ©è†œè§£ç å™¨åˆ™åˆ©ç”¨æ–‡æœ¬åµŒå…¥è¿›è¡Œè¯­ä¹‰å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥åŠ¨æ€ç‰¹å¾èåˆæ¨¡å—ï¼ˆDFFMï¼‰ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆæ•´åˆäº†æ¥è‡ªä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒäº†SAMçš„å˜æ¢å™¨ç»“æ„ä¸å˜ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä½ç§©é€‚åº”æŠ€æœ¯ä»¥å‡å°‘è®­ç»ƒå‚æ•°ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡è¯­ä¹‰å¯¹é½ï¼Œç½‘ç»œç»“æ„ä¸­ä¿æŒäº†SAMçš„åŸå§‹å˜æ¢å™¨å—ä¸å˜ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTASegåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹çš„åˆ†å‰²å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶è®­ç»ƒå‚æ•°å‡å°‘äº†XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TASegæ¡†æ¶åœ¨æ™ºèƒ½äº¤é€šã€æ— äººé©¾é©¶ã€å®‰é˜²ç›‘æ§ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒç¯å¢ƒç†è§£å’Œå†³ç­–åˆ¶å®šï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reliable semantic segmentation of open environments is essential for intelligent systems, yet significant problems remain: 1) Existing RGB-T semantic segmentation models mainly rely on low-level visual features and lack high-level textual information, which struggle with accurate segmentation when categories share similar visual characteristics. 2) While SAM excels in instance-level segmentation, integrating it with thermal images and text is hindered by modality heterogeneity and computational inefficiency. To address these, we propose TASeg, a text-aware RGB-T segmentation framework by using Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the image encoder, which effectively merges features from multiple visual modalities while freezing SAM's original transformer blocks. Additionally, we incorporate CLIP-generated text embeddings in the mask decoder to enable semantic alignment, which further rectifies the classification error and improves the semantic understanding accuracy. Experimental results across diverse datasets demonstrate that our method achieves superior performance in challenging scenarios with fewer trainable parameters.

