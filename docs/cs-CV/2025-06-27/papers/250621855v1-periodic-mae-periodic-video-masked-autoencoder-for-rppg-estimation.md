---
layout: default
title: Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation
---

# Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21855" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21855v1</a>
  <a href="https://arxiv.org/pdf/2506.21855.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21855v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21855v1', 'Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiho Choi, Sang Jun Lee

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ziiho08/Periodic-MAE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‘¨æœŸæ€§è§†é¢‘æ©ç è‡ªç¼–ç å™¨ä»¥è§£å†³rPPGä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è¿œç¨‹å…‰ç”µå®¹ç§¯æè®°æ³•` `å‘¨æœŸæ€§ä¿¡å·` `è‡ªç›‘ç£å­¦ä¹ ` `è§†é¢‘æ©ç è‡ªç¼–ç å™¨` `ç”Ÿç†ä¿¡å·æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»é¢éƒ¨è§†é¢‘ä¸­æå–ç”Ÿç†ä¿¡å·æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰ä¿¡å·çš„å‘¨æœŸæ€§å˜åŒ–ï¼Œå¯¼è‡´rPPGä¼°è®¡ç²¾åº¦ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„å‘¨æœŸæ€§è§†é¢‘æ©ç è‡ªç¼–ç å™¨é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨å¸§æ©ç æŠ€æœ¯æ•æ‰è§†é¢‘ä¸­çš„å‡†å‘¨æœŸä¿¡å·ï¼Œä»è€Œæ”¹è¿›ä¿¡å·è¡¨ç¤ºã€‚
3. åœ¨PUREã€UBFC-rPPGã€MMPDå’ŒV4Væ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­æ˜¾è‘—æé«˜äº†rPPGä¼°è®¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡æ•æ‰é¢éƒ¨è§†é¢‘ä¸­çš®è‚¤è‰²è°ƒçš„å¾®å°å˜åŒ–ï¼Œä»æœªæ ‡è®°çš„é¢éƒ¨è§†é¢‘ä¸­å­¦ä¹ å‘¨æœŸä¿¡å·çš„é€šç”¨è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é‡‡ç”¨è§†é¢‘æ©ç è‡ªç¼–ç å™¨ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ å­¦ä¹ é¢éƒ¨åŒºåŸŸçš„é«˜ç»´æ—¶ç©ºè¡¨ç¤ºã€‚æ•æ‰è§†é¢‘ä¸­çš„å‡†å‘¨æœŸä¿¡å·å¯¹äºè¿œç¨‹å…‰ç”µå®¹ç§¯æè®°æ³•ï¼ˆrPPGï¼‰ä¼°è®¡è‡³å…³é‡è¦ã€‚ä¸ºè€ƒè™‘ä¿¡å·çš„å‘¨æœŸæ€§ï¼Œæˆ‘ä»¬åœ¨è§†é¢‘é‡‡æ ·ä¸­åº”ç”¨å¸§æ©ç ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨é¢„è®­ç»ƒé˜¶æ®µæ•æ‰é‡é‡‡æ ·çš„å‡†å‘¨æœŸä¿¡å·ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“åˆç”Ÿç†å¸¦é™çº¦æŸï¼Œåˆ©ç”¨ç”Ÿç†ä¿¡å·åœ¨å…¶é¢‘å¸¦å†…ç¨€ç–çš„ç‰¹æ€§ï¼Œä¸ºæ¨¡å‹æä¾›è„‰æçº¿ç´¢ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»é¢éƒ¨è§†é¢‘ä¸­æå–ç”Ÿç†ä¿¡å·æ—¶ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰ä¿¡å·å‘¨æœŸæ€§å˜åŒ–çš„é—®é¢˜ã€‚è¿™å¯¼è‡´äº†è¿œç¨‹å…‰ç”µå®¹ç§¯æè®°æ³•ï¼ˆrPPGï¼‰ä¼°è®¡çš„ç²¾åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è§†é¢‘æ©ç è‡ªç¼–ç å™¨å­¦ä¹ å‘¨æœŸä¿¡å·çš„é«˜ç»´æ—¶ç©ºè¡¨ç¤ºï¼Œåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ æ•æ‰é¢éƒ¨è§†é¢‘ä¸­çš„å¾®å°è‰²è°ƒå˜åŒ–ï¼Œä»è€Œæé«˜rPPGä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µå’ŒrPPGä»»åŠ¡é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡å¸§æ©ç æŠ€æœ¯æ•æ‰å‡†å‘¨æœŸä¿¡å·ï¼›åœ¨rPPGä»»åŠ¡é˜¶æ®µï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨ä»é¢éƒ¨è§†é¢‘ä¸­æå–ç”Ÿç†ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å¸§æ©ç æŠ€æœ¯å’Œç”Ÿç†å¸¦é™çº¦æŸï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¿¡å·çš„å‘¨æœŸæ€§ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨ç”Ÿç†ä¿¡å·çš„ç¨€ç–æ€§æä¾›è„‰æçº¿ç´¢ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºæ›´å¥½åœ°è€ƒè™‘äº†ä¿¡å·çš„å‘¨æœŸæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ä¿¡å·çš„é‡å»ºè´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„ä»¥é€‚åº”é«˜ç»´æ—¶ç©ºæ•°æ®çš„å¤„ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ŒrPPGä¼°è®¡çš„å‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¥åº·ç›‘æµ‹ã€æƒ…æ„Ÿåˆ†æå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡å‡†ç¡®ä¼°è®¡ç”Ÿç†ä¿¡å·ï¼Œèƒ½å¤Ÿä¸ºè¿œç¨‹åŒ»ç–—ã€å¿ƒç†å¥åº·ç›‘æµ‹ç­‰æä¾›é‡è¦æ”¯æŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we propose a method that learns a general representation of periodic signals from unlabeled facial videos by capturing subtle changes in skin tone over time. The proposed framework employs the video masked autoencoder to learn a high-dimensional spatio-temporal representation of the facial region through self-supervised learning. Capturing quasi-periodic signals in the video is crucial for remote photoplethysmography (rPPG) estimation. To account for signal periodicity, we apply frame masking in terms of video sampling, which allows the model to capture resampled quasi-periodic signals during the pre-training stage. Moreover, the framework incorporates physiological bandlimit constraints, leveraging the property that physiological signals are sparse within their frequency bandwidth to provide pulse cues to the model. The pre-trained encoder is then transferred to the rPPG task, where it is used to extract physiological signals from facial videos. We evaluate the proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and V4V datasets. Our results demonstrate significant performance improvements, particularly in challenging cross-dataset evaluations. Our code is available at https://github.com/ziiho08/Periodic-MAE.

