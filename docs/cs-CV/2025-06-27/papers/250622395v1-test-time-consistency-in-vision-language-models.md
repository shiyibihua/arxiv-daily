---
layout: default
title: Test-Time Consistency in Vision Language Models
---

# Test-Time Consistency in Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22395" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22395v1</a>
  <a href="https://arxiv.org/pdf/2506.22395.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22395v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22395v1', 'Test-Time Consistency in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shih-Han Chou, Shivam Chandhok, James J. Little, Leonid Sigal

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶ä¸€è‡´æ€§æ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ä¸€è‡´æ€§æ¡†æ¶` `å¤šæ¨¡æ€å­¦ä¹ ` `åå¤„ç†æ–¹æ³•` `è¯­ä¹‰ç­‰ä»·è¾“å…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¯­ä¹‰ç­‰ä»·è¾“å…¥æ—¶è¡¨ç°å‡ºä¸ä¸€è‡´æ€§ï¼Œå½±å“äº†æ¨¡å‹çš„å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶ä¸€è‡´æ€§æ¡†æ¶ï¼Œé€šè¿‡åå¤„ç†æ–¹å¼å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚
3. åœ¨MM-R3åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€ææ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œå¼€è¾Ÿäº†å¤šæ¨¡æ€å­¦ä¹ æ¨ç†æ—¶é€‚åº”çš„æ–°æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹è¯­ä¹‰ç­‰ä»·è¾“å…¥æ—¶å¸¸å¸¸å‡ºç°ä¸ä¸€è‡´çš„è¡Œä¸ºï¼Œå½±å“å…¶å¯é æ€§å’Œç¨³å¥æ€§ã€‚è¿‘æœŸçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„VLMsåœ¨è¯­ä¹‰ç­‰ä»·è¾“å…¥ä¸Šä¹Ÿå¯èƒ½äº§ç”Ÿä¸åŒçš„é¢„æµ‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æµ‹è¯•æ—¶ä¸€è‡´æ€§æ¡†æ¶ï¼Œé€šè¿‡åå¤„ç†æ–¹å¼å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ï¼Œè€Œæ— éœ€ç›‘ç£å†è®­ç»ƒã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä»»ä½•å…·æœ‰æƒé‡çš„VLMï¼Œé€šè¿‡ä¸¤ä¸ªäº’è¡¥ç›®æ ‡æ¥å¼ºåˆ¶ä¸€è‡´é¢„æµ‹ï¼Œä»è€Œåœ¨MM-R3åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹è¯­ä¹‰ç­‰ä»·è¾“å…¥æ—¶äº§ç”Ÿä¸ä¸€è‡´é¢„æµ‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒï¼Œå­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åå¤„ç†çš„æµ‹è¯•æ—¶ä¸€è‡´æ€§æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„æŸå¤±å‡½æ•°æ¥å¢å¼ºæ¨¡å‹çš„é¢„æµ‹ä¸€è‡´æ€§ï¼Œé¿å…äº†é‡æ–°è®­ç»ƒçš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤å‰ç†µä¸€è‡´æ€§æŸå¤±ï¼ˆCross-Entropy Agreement Lossï¼‰å’Œä¼ªæ ‡ç­¾ä¸€è‡´æ€§æŸå¤±ï¼ˆPseudo-Label Consistency Lossï¼‰ã€‚å‰è€…å¯¹é½è¯­ä¹‰ç­‰ä»·è¾“å…¥çš„é¢„æµ‹åˆ†å¸ƒï¼Œåè€…åˆ™å°†è¾“å‡ºæ‹‰å‘è‡ªæˆ‘å¹³å‡çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§å®Œå…¨åå¤„ç†ã€æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„çš„æƒ…å†µä¸‹æå‡ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–¹æ³•æ›´ä¸ºçµæ´»å’Œé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±å’Œä¼ªæ ‡ç­¾æŸå¤±ä½œä¸ºä¸»è¦æŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨å•ä¸ªæµ‹è¯•è¾“å…¥ä¸Šè¿›è¡Œä¸€è‡´æ€§é¢„æµ‹ã€‚è¯¥æ–¹æ³•çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿç›´æ¥åˆ©ç”¨è¾“å…¥ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨MM-R3åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ä¸€è‡´æ€§ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¯­ä¹‰ç­‰ä»·è¾“å…¥æ—¶ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„é¢„æµ‹ç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ä¸è®¡ç®—æœºè§†è§‰çš„ç»“åˆç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿå¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜ç²¾åº¦é¢„æµ‹çš„åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šæ¨¡å‹åœ¨æ¨ç†æ—¶çš„é€‚åº”æ€§æ”¹è¿›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have achieved impressive performance across a wide range of multimodal tasks, yet they often exhibit inconsistent behavior when faced with semantically equivalent inputs, undermining their reliability and robustness. Recent benchmarks, such as MM-R3, highlight that even state-of-the-art VLMs can produce divergent predictions across semantically equivalent inputs, despite maintaining high average accuracy. Prior work addresses this issue by modifying model architectures or conducting large-scale fine-tuning on curated datasets. In contrast, we propose a simple and effective test-time consistency framework that enhances semantic consistency without supervised re-training. Our method is entirely post-hoc, model-agnostic, and applicable to any VLM with access to its weights. Given a single test point, we enforce consistent predictions via two complementary objectives: (i) a Cross-Entropy Agreement Loss that aligns predictive distributions across semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that draws outputs toward a self-averaged consensus. Our method is plug-and-play and leverages information from a single test input itself to improve consistency. Experiments on the MM-R3 benchmark show that our framework yields substantial gains in consistency across state-of-the-art models, establishing a new direction for inference-time adaptation in multimodal learning.

