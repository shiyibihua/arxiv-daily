---
layout: default
title: ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts
---

# ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21835" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21835v3</a>
  <a href="https://arxiv.org/pdf/2506.21835.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21835v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21835v3', 'ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoqi Wang, Clint Sebastian, Wenbin He, Liu Ren

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-08-03)

**å¤‡æ³¨**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProSAMä»¥è§£å†³SAMè§†è§‰å‚è€ƒåˆ†å‰²çš„ç¨³å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å‚è€ƒåˆ†å‰²` `å¼€æ”¾é›†åˆ†å‰²` `å˜åˆ†æç¤ºç¼–ç å™¨` `å›¾åƒåˆ†å‰²` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºSAMçš„è§†è§‰å‚è€ƒåˆ†å‰²æ–¹æ³•åœ¨æç¤ºç”Ÿæˆä¸Šå­˜åœ¨ä¸ç¨³å®šæ€§ï¼Œå¯¼è‡´åˆ†å‰²æ•ˆæœä¸ä½³ã€‚
2. ProSAMé€šè¿‡å¼•å…¥å˜åˆ†æç¤ºç¼–ç å™¨ï¼Œé¢„æµ‹å¤šå…ƒæç¤ºåˆ†å¸ƒï¼Œé¿å…åœ¨ä¸ç¨³å®šåŒºåŸŸç”Ÿæˆæç¤ºã€‚
3. åœ¨Pascal-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šï¼ŒProSAMçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹åŸºç¡€æ¨¡å‹çš„è¿›å±•æ¨åŠ¨äº†å¼€æ”¾é›†å›¾åƒåˆ†å‰²çš„æˆåŠŸï¼Œè¯¥ä»»åŠ¡ä¸“æ³¨äºåˆ†å‰²è¶…å‡ºé¢„å®šä¹‰ç±»åˆ«çš„å¯¹è±¡ã€‚åœ¨å„ç§æç¤ºç±»å‹ä¸­ï¼Œè§†è§‰å‚è€ƒåˆ†å‰²å› å…¶ç‹¬ç‰¹çš„çµæ´»æ€§å’Œå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›è€Œè„±é¢–è€Œå‡ºã€‚å°½ç®¡ä¸€äº›åŸºäºSAMçš„æ–¹æ³•åœ¨æ­¤ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¸¸å¸¸åœ¨ç›®æ ‡åŒºåŸŸçš„è¾¹ç•Œç”Ÿæˆæç¤ºï¼Œå¯¼è‡´ä¸ç¨³å®šæ€§å’Œé²æ£’æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ProSAMï¼Œé€šè¿‡å­¦ä¹ å˜åˆ†æç¤ºç¼–ç å™¨æ¥é¢„æµ‹å¤šå…ƒæç¤ºåˆ†å¸ƒï¼Œä»è€Œé¿å…åœ¨ä¸ç¨³å®šåŒºåŸŸç”Ÿæˆæç¤ºï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒProSAMåœ¨Pascal-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šå§‹ç»ˆè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´ä¸ºç¨³å¥çš„è§†è§‰å‚è€ƒåˆ†å‰²è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºSAMçš„è§†è§‰å‚è€ƒåˆ†å‰²æ–¹æ³•åœ¨æç¤ºç”Ÿæˆæ—¶çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç”±äºæç¤ºç¼–ç å™¨çš„ä¸è¶³ï¼Œå¸¸åœ¨ç›®æ ‡åŒºåŸŸçš„è¾¹ç•Œç”Ÿæˆæç¤ºï¼Œå¯¼è‡´åˆ†å‰²ç»“æœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šProSAMçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å­¦ä¹ å˜åˆ†æç¤ºç¼–ç å™¨ï¼Œé¢„æµ‹å¤šå…ƒæç¤ºåˆ†å¸ƒï¼Œä»è€Œé¿å…åœ¨ä¸ç¨³å®šåŒºåŸŸç”Ÿæˆæç¤ºã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æç¤ºçš„é²æ£’æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„æç¤ºæ›´ä¸ºå¯é ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProSAMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€å˜åˆ†æç¤ºç¼–ç å™¨ã€æç¤ºç”Ÿæˆæ¨¡å—å’Œåˆ†å‰²æ¨¡å‹ã€‚é¦–å…ˆï¼Œè¾“å…¥å›¾åƒå’Œå‚è€ƒä¿¡æ¯ï¼Œç„¶åé€šè¿‡å˜åˆ†æç¤ºç¼–ç å™¨ç”Ÿæˆå¤šå…ƒæç¤ºåˆ†å¸ƒï¼Œæœ€åå°†ç”Ÿæˆçš„æç¤ºè¾“å…¥åˆ°åˆ†å‰²æ¨¡å‹ä¸­è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šProSAMçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å˜åˆ†æç¤ºç¼–ç å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹å¤šå…ƒæç¤ºåˆ†å¸ƒã€‚è¿™ä¸€åˆ›æ–°ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒProSAMèƒ½å¤Ÿé¿å…ç”Ÿæˆä½äºä¸ç¨³å®šåŒºåŸŸçš„æç¤ºï¼Œä»è€Œæå‡åˆ†å‰²çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒProSAMä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æç¤ºç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒé€‰æ‹©ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°æœ€ä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒProSAMåœ¨Pascal-5$^i$æ•°æ®é›†ä¸Šå–å¾—äº†ç›¸è¾ƒäºæœ€å…ˆè¿›æ–¹æ³•çš„æ˜¾è‘—æå‡ï¼ŒmIoUï¼ˆå¹³å‡äº¤å¹¶æ¯”ï¼‰æé«˜äº†çº¦5%ã€‚åœ¨COCO-20$^i$æ•°æ®é›†ä¸Šï¼ŒProSAMåŒæ ·è¡¨ç°å‡ºè‰²ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨è§†è§‰å‚è€ƒåˆ†å‰²ä»»åŠ¡ä¸­çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æå’Œæœºå™¨äººè§†è§‰ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ†å‰²æœªçŸ¥ç±»åˆ«çš„å¯¹è±¡å¯¹äºæé«˜ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå†³ç­–èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœªæ¥ï¼ŒProSAMæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„è§†è§‰ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½è§†è§‰ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The recent advancements in large foundation models have driven the success of open-set image segmentation, a task focused on segmenting objects beyond predefined categories. Among various prompt types (such as points, boxes, texts, and visual references), visual reference segmentation stands out for its unique flexibility and strong zero-shot capabilities. Recently, several SAM-based methods have made notable progress in this task by automatically generating prompts to guide SAM. However, these methods often generate prompts at boundaries of target regions due to suboptimal prompt encoder, which results in instability and reduced robustness. In this work, we introduce ProSAM, a simple but effective method to address the stability challenges we identified in existing SAM-based visual reference segmentation approaches. By learning a variational prompt encoder to predict multivariate prompt distributions, ProSAM avoids generating prompts that lie in unstable regions, overcoming the instability caused by less robust prompts. Our approach consistently surpasses state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets, providing a more robust solution for visual reference segmentation.

