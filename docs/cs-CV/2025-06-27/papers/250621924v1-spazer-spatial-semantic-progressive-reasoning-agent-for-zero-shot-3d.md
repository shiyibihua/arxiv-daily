---
layout: default
title: SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding
---

# SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21924" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21924v1</a>
  <a href="https://arxiv.org/pdf/2506.21924.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21924v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21924v1', 'SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhao Jin, Rong-Cheng Tu, Jingyi Liao, Wenhao Sun, Xiao Luo, Shunyu Liu, Dacheng Tao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPAZERä»¥è§£å†³é›¶-shot 3Dè§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `é›¶-shotå­¦ä¹ ` `å¤šæ¨¡æ€æ¨ç†` `ç©ºé—´ç†è§£` `è¯­ä¹‰ç†è§£` `è¿›é˜¶æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶-shot 3Dè§†è§‰å®šä½æ–¹æ³•å¾€å¾€åªå…³æ³¨ç©ºé—´æˆ–è¯­ä¹‰ç†è§£ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ•ˆæœä¸ä½³ã€‚
2. SPAZERé€šè¿‡ç»“åˆç©ºé—´å’Œè¯­ä¹‰æ¨ç†ï¼Œé‡‡ç”¨è¿›é˜¶æ¨ç†æ¡†æ¶ï¼Œé¦–å…ˆè¿›è¡Œå…¨å±€åœºæ™¯åˆ†æå¹¶ç”Ÿæˆ3Dæ¸²æŸ“ã€‚
3. åœ¨ScanReferå’ŒNr3DåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPAZERçš„å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†9.0%å’Œ10.9%ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dè§†è§‰å®šä½ï¼ˆ3DVGï¼‰æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢åœ¨3Dåœºæ™¯ä¸­å®šä½ç›®æ ‡å¯¹è±¡ã€‚ä¸ºå‡å°‘å¯¹æ˜‚è´µ3Dè®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œè¿‘æœŸç ”ç©¶æ¢ç´¢äº†é€šè¿‡é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹å®ç°é›¶-shot 3DVGã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾§é‡äºç©ºé—´æˆ–è¯­ä¹‰ç†è§£ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç°å®åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡æå‡ºSPAZERï¼Œä¸€ä¸ªç»“åˆç©ºé—´å’Œè¯­ä¹‰ç†è§£çš„è¿›é˜¶æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æœªä½¿ç”¨3Dæ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°ç¨³å¥çš„é›¶-shotå®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAZERåœ¨ScanReferå’ŒNr3DåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é›¶-shotæ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡åˆ†åˆ«è¾¾åˆ°9.0%å’Œ10.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é›¶-shot 3Dè§†è§‰å®šä½ä¸­çš„ç©ºé—´ä¸è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆç»“åˆ3Då’Œ2Dä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPAZERé€šè¿‡ç»“åˆç©ºé—´å’Œè¯­ä¹‰æ¨ç†ï¼Œé‡‡ç”¨è¿›é˜¶æ¨ç†æ¡†æ¶ï¼Œé¦–å…ˆè¿›è¡Œå…¨å±€åœºæ™¯åˆ†æå¹¶ç”Ÿæˆ3Dæ¸²æŸ“ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„ç›®æ ‡å®šä½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPAZERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå…¨å±€åœºæ™¯åˆ†æã€é”šç‚¹å¼•å¯¼å€™é€‰ç­›é€‰å’Œ3D-2Dè”åˆå†³ç­–ã€‚é¦–å…ˆï¼Œç³»ç»Ÿä»æœ€ä½³è§†è§’ç”Ÿæˆ3Dæ¸²æŸ“ï¼Œç„¶åè¿›è¡Œç²—ç•¥å®šä½ï¼Œæœ€åç»“åˆç›¸å…³çš„2Då›¾åƒè¿›è¡Œç²¾ç¡®åŒ¹é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPAZERçš„åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿæ— ç¼ç»“åˆç©ºé—´å’Œè¯­ä¹‰æ¨ç†ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„æ¨ç†æ¡†æ¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSPAZERä½¿ç”¨äº†é”šç‚¹å¼•å¯¼çš„å€™é€‰ç­›é€‰æœºåˆ¶ï¼Œå¹¶é€šè¿‡æ£€ç´¢ç›¸å…³çš„2Då›¾åƒè¿›è¡Œ3D-2Dè”åˆå†³ç­–ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„ç›®æ ‡åŒ¹é…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SPAZERåœ¨ScanReferå’ŒNr3DåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡äº†9.0%å’Œ10.9%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é›¶-shotæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPAZERçš„ç ”ç©¶æˆæœåœ¨æ™ºèƒ½æœºå™¨äººã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å®ç°é«˜æ•ˆçš„3Dè§†è§‰å®šä½ï¼ŒSPAZERèƒ½å¤Ÿæå‡æœºå™¨å¯¹å¤æ‚ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene based on natural language queries. To alleviate the reliance on costly 3D training data, recent studies have explored zero-shot 3DVG by leveraging the extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and VLMs. However, existing paradigms tend to emphasize either spatial (3D-based) or semantic (2D-based) understanding, limiting their effectiveness in complex real-world applications. In this work, we introduce SPAZER - a VLM-driven agent that combines both modalities in a progressive reasoning framework. It first holistically analyzes the scene and produces a 3D rendering from the optimal viewpoint. Based on this, anchor-guided candidate screening is conducted to perform a coarse-level localization of potential objects. Furthermore, leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is efficiently performed to determine the best-matching object. By bridging spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot grounding without training on 3D-labeled data. Extensive experiments on ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms previous state-of-the-art zero-shot methods, achieving notable gains of 9.0% and 10.9% in accuracy.

