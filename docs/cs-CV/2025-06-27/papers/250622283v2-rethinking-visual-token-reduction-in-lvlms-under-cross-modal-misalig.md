---
layout: default
title: Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment
---

# Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22283" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22283v2</a>
  <a href="https://arxiv.org/pdf/2506.22283.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22283v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22283v2', 'Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Xu, Yunke Wang, Yong Luo, Bo Du

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-08-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisionDropä»¥è§£å†³LVLMä¸­è§†è§‰æ ‡è®°å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è§†è§‰æ ‡è®°å‰ªæ` `å¤šæ¨¡æ€å­¦ä¹ ` `å†…éƒ¨æ³¨æ„åŠ›` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æ ‡è®°å‡å°‘æ–¹æ³•ä¾èµ–æ–‡æœ¬ä¿¡å·ï¼Œå‡è®¾æ–‡æœ¬èƒ½æœ‰æ•ˆæ•æ‰è§†è§‰ä¿¡æ¯çš„é‡è¦æ€§ï¼Œä½†è¿™ç§å‡è®¾å­˜åœ¨ä¸å¯¹é½é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºVisionDropæ¡†æ¶ï¼Œé€šè¿‡è§†è§‰å†…éƒ¨æ³¨æ„åŠ›é€‰æ‹©è§†è§‰æ ‡è®°ï¼Œé¿å…äº†å¯¹æ–‡æœ¬ä¿¡å·çš„ä¾èµ–ï¼Œæå‡äº†å‰ªææ•ˆæœã€‚
3. åœ¨ä¸LLaVA-NeXT-7Bé›†æˆåï¼ŒVisionDropå®ç°äº†2.7å€çš„æ¨ç†å»¶è¿Ÿå‡å°‘å’Œ6å€çš„FLOPsé™ä½ï¼ŒåŒæ—¶ä¿ç•™äº†95.71%çš„åŸå§‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡å¯†é›†çš„è¡¥ä¸çº§æ ‡è®°åºåˆ—æ¥ç¼–ç è§†è§‰è¾“å…¥ï¼Œä»¥æ•æ‰ç»†ç²’åº¦è¯­ä¹‰ã€‚ç„¶è€Œï¼Œè§†è§‰æ ‡è®°çš„æ•°é‡é€šå¸¸è¿œè¶…æ–‡æœ¬æ ‡è®°ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ï¼Œé™åˆ¶äº†LVLMsçš„å¯æ‰©å±•æ€§ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°å‡å°‘æ–¹æ³•å¤šä¾èµ–æ–‡æœ¬æ¡ä»¶äº¤äº’ï¼Œå‡è®¾æ–‡æœ¬æ ‡è®°èƒ½å¯é æ•æ‰è§†è§‰æ ‡è®°çš„é‡è¦æ€§ã€‚æœ¬æ–‡é‡æ–°å®¡è§†è¿™ä¸€å‡è®¾ï¼Œæ­ç¤ºäº†è·¨æ¨¡æ€ä¸å¯¹é½çš„å› æœã€è¯­ä¹‰å’Œç©ºé—´å½¢å¼ï¼Œå½±å“äº†æ–‡æœ¬å¼•å¯¼çš„è§†è§‰æ ‡è®°å‡å°‘æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VisionDropï¼Œä¸€ä¸ªæ— è®­ç»ƒçš„è§†è§‰å‰ªææ¡†æ¶ï¼ŒåŸºäºè§†è§‰å†…éƒ¨æ³¨æ„åŠ›é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰æ ‡è®°ï¼Œæ— éœ€ä¾èµ–æ–‡æœ¬ä¿¡å·ã€‚é€šè¿‡å°†è§†è§‰ç¼–ç å™¨å’ŒLLMè§†ä¸ºç»Ÿä¸€ç³»ç»Ÿï¼Œæˆ‘ä»¬è®¾è®¡äº†æ¸è¿›å¼å‰ªæç®¡é“ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé˜¶æ®µè¿›è¡Œæ ‡è®°é€‰æ‹©å’Œè½»é‡çº§ä¸Šä¸‹æ–‡åˆå¹¶ï¼Œä¿ç•™ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒVisionDropåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚ä¿®æ”¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è§†è§‰æ ‡è®°å†—ä½™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–æ–‡æœ¬ä¿¡å·è¿›è¡Œè§†è§‰æ ‡è®°çš„é€‰æ‹©ï¼Œå¯¼è‡´åœ¨è·¨æ¨¡æ€ä¸å¯¹é½æƒ…å†µä¸‹æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVisionDropæ¡†æ¶ï¼Œé€šè¿‡è§†è§‰å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰æ ‡è®°ï¼Œé¿å…äº†å¯¹æ–‡æœ¬ä¿¡å·çš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†å‰ªæçš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„å°†è§†è§‰ç¼–ç å™¨å’ŒLLMè§†ä¸ºç»Ÿä¸€ç³»ç»Ÿï¼Œè®¾è®¡äº†æ¸è¿›å¼å‰ªæç®¡é“ï¼ŒåŒ…å«å¤šä¸ªé˜¶æ®µçš„æ ‡è®°é€‰æ‹©å’Œä¸Šä¸‹æ–‡åˆå¹¶æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šVisionDropçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„è§†è§‰å‰ªææ–¹æ³•ï¼ŒåŸºäºè§†è§‰å†…éƒ¨æ³¨æ„åŠ›è¿›è¡Œæ ‡è®°é€‰æ‹©ï¼Œä¸ä¼ ç»Ÿä¾èµ–æ–‡æœ¬çš„å‰ªææ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šé˜¶æ®µçš„å‰ªæç­–ç•¥ï¼Œç¡®ä¿åœ¨æ¿€è¿›çš„æ ‡è®°é¢„ç®—ä¸‹ä»èƒ½ä¿ç•™ç»†ç²’åº¦çš„è§†è§‰ä¿¡æ¯ï¼Œä¸”ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–å¤æ‚çš„æ¨¡å‹ä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionDropåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ä¸LLaVA-NeXT-7Bé›†æˆæ—¶ï¼Œå®ç°äº†2.7å€çš„æ¨ç†å»¶è¿Ÿå‡å°‘å’Œ6å€çš„FLOPsé™ä½ï¼ŒåŒæ—¶ä¿ç•™äº†95.71%çš„åŸå§‹æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡ä¼˜åŒ–è§†è§‰æ ‡è®°çš„å¤„ç†ï¼ŒVisionDropå¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æå‡LVLMçš„æ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶å’Œå›¾åƒç†è§£ç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences of patch-level tokens to capture fine-grained semantics. These visual tokens often outnumber their textual counterparts by a large margin, leading to substantial computational overhead and limiting the scalability of LVLMs in practice. Previous efforts have explored visual token reduction either prior to or within the large language models (LLMs). However, most in-LLM reduction approaches rely on text-conditioned interactions, implicitly assuming that textual tokens can reliably capture the importance of visual tokens. In this work, we revisit this assumption and reveal causal, semantic, and spatial forms of cross-modal misalignment. These misalignments undermine the effectiveness of text-guided visual token reduction. To address this, we introduce VisionDrop, a training-free, visual-only pruning framework that selects informative visual tokens based on intra-modal (visual-to-visual) attention, without relying on textual signals. To further suppress redundancy throughout the model hierarchy, we treat the visual encoder and the LLM as a unified system and design a progressive pruning pipeline. Our method performs dominant token selection and lightweight contextual merging at multiple stages, enabling fine-grained visual information to be retained even under aggressive token budgets. Extensive experiments across diverse benchmarks show that VisionDrop achieves consistent improvements over existing approaches, despite requiring no additional training or complex modifications. Notably, when integrated with LLaVA-NeXT-7B, VisionDrop achieves a 2.7x reduction in inference latency and 6x in FLOPs, while retaining 95.71% of the original performance.

