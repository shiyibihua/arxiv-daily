---
layout: default
title: Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset
---

# Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22554" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22554v2</a>
  <a href="https://arxiv.org/pdf/2506.22554.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22554v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22554v2', 'Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, Praveen Chowdary, Joe Chuang, Antony D'Avirro, Jon Daly, Ning Dong, Mark Duppenthaler, Cynthia Gao, Jeff Girard, Martin Gleize, Sahir Gomez, Hongyu Gong, Srivathsan Govindarajan, Brandon Han, Sen He, Denise Hernandez, Yordan Hristov, Rongjie Huang, Hirofumi Inaguma, Somya Jain, Raj Janardhan, Qingyao Jia, Christopher Klaiber, Dejan Kovachev, Moneish Kumar, Hang Li, Yilei Li, Pavel Litvin, Wei Liu, Guangyao Ma, Jing Ma, Martin Ma, Xutai Ma, Lucas Mantovani, Sagar Miglani, Sreyas Mohan, Louis-Philippe Morency, Evonne Ng, Kam-Woh Ng, Tu Anh Nguyen, Amia Oberai, Benjamin Peloquin, Juan Pino, Jovan Popovic, Omid Poursaeed, Fabian Prada, Alice Rakotoarison, Rakesh Ranjan, Alexander Richard, Christophe Ropers, Safiyyah Saleem, Vasu Sharma, Alex Shcherbyna, Jia Shen, Jie Shen, Anastasis Stathopoulos, Anna Sun, Paden Tomasello, Tuan Tran, Arina Turkatenko, Bo Wan, Chao Wang, Jeff Wang, Mary Williamson, Carleigh Wood, Tao Xiang, Yilin Yang, Julien Yao, Chen Zhang, Jiemin Zhang, Xinyue Zhang, Jason Zheng, Pavlo Zhyzheria, Jan Zikes, Michael Zollhoefer

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-07-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— ç¼äº¤äº’æ¨¡å‹ä»¥è§£å†³äººæœºäº¤äº’ä¸­çš„éè¯­è¨€ä¿¡å·ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— ç¼äº¤äº’` `å¤šæ¨¡æ€å­¦ä¹ ` `äººæœºäº¤äº’` `æƒ…æ„Ÿè®¡ç®—` `è™šæ‹Ÿä»£ç†` `æ·±åº¦å­¦ä¹ ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç†è§£å’Œç”ŸæˆäºŒäººäº¤äº’ä¸­çš„éè¯­è¨€ä¿¡å·æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºäº†æ— ç¼äº¤äº’æ•°æ®é›†å’Œä¸€ç³»åˆ—æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸äººç±»è¯­è¨€å’Œè§†è§‰è¡Œä¸ºç›¸ä¸€è‡´çš„äºŒäººè¿åŠ¨æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨ç”Ÿæˆæƒ…æ„Ÿå“åº”å’Œè¡¨è¾¾èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†äººæœºäº¤äº’çš„ç›´è§‚æ€§å’Œå“åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»æ²Ÿé€šæ¶‰åŠå¤æ‚çš„è¯­è¨€å’Œéè¯­è¨€ä¿¡å·çš„ç›¸äº’ä½œç”¨ï¼Œè¿™å¯¹äºä¼ è¾¾æ„ä¹‰å’Œå®ç°äººé™…ç›®æ ‡è‡³å…³é‡è¦ã€‚ä¸ºäº†å¼€å‘å…·æœ‰ç¤¾ä¼šæ™ºèƒ½çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå¿…é¡»å»ºç«‹èƒ½å¤Ÿç†è§£å’Œç”ŸæˆäºŒäººè¡Œä¸ºåŠ¨æ€çš„æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— ç¼äº¤äº’æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡4000å°æ—¶é¢å¯¹é¢äº’åŠ¨è§†é¢‘çš„å¤§è§„æ¨¡é›†åˆï¼Œæ¥è‡ª4000å¤šåå‚ä¸è€…ï¼Œæ¶µç›–å¤šç§æƒ…å¢ƒã€‚è¯¥æ•°æ®é›†ä½¿å¾—å¼€å‘ç†è§£äºŒäººèº«ä½“åŠ¨æ€çš„äººå·¥æ™ºèƒ½æŠ€æœ¯æˆä¸ºå¯èƒ½ï¼Œæ¨åŠ¨è™šæ‹Ÿä»£ç†ã€è¿œç¨‹ä½“éªŒå’Œå¤šæ¨¡æ€å†…å®¹åˆ†æå·¥å…·çš„çªç ´ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€å¥—åˆ©ç”¨è¯¥æ•°æ®é›†ç”Ÿæˆä¸äººç±»è¯­è¨€ç›¸ä¸€è‡´çš„äºŒäººè¿åŠ¨æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³äººæœºäº¤äº’ä¸­å¯¹äºŒäººäº¤äº’éè¯­è¨€ä¿¡å·ç†è§£çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰å’Œç”Ÿæˆè¿™äº›åŠ¨æ€è¡Œä¸ºï¼Œå¯¼è‡´äº¤äº’ä½“éªŒä¸å¤Ÿè‡ªç„¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºæ— ç¼äº¤äº’æ•°æ®é›†ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶ç†è§£å’Œç”Ÿæˆä¸äººç±»è¯­è¨€å’Œè§†è§‰è¡Œä¸ºç›¸ä¸€è‡´çš„äºŒäººè¿åŠ¨å’Œè¡¨æƒ…ï¼Œä»è€Œæå‡äº¤äº’çš„è‡ªç„¶æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µã€‚æ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€è¾“å…¥ï¼Œæ¨¡å‹åˆ™åˆ©ç”¨è¿™äº›è¾“å…¥ç”Ÿæˆç›¸åº”çš„è¿åŠ¨æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†è¯­è¨€å’Œè§†è§‰è¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸ä¹‹ç›¸åŒ¹é…çš„åŠ¨æ€è¡Œä¸ºï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œç»“åˆäº†æ·±åº¦å­¦ä¹ ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ŒæŸå¤±å‡½æ•°åˆ™è€ƒè™‘äº†ç”Ÿæˆè¡Œä¸ºçš„è¯­ä¹‰ç›¸å…³æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾çš„å¯æ§æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨ç”Ÿæˆä¸äººç±»è¯­è¨€å’Œè§†è§‰è¡Œä¸ºä¸€è‡´çš„åŠ¨æ€è¡Œä¸ºæ–¹é¢ï¼Œè¾ƒåŸºçº¿æ¨¡å‹æå‡äº†30%çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨æƒ…æ„Ÿè¡¨è¾¾çš„å¤šæ ·æ€§ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†æ›´é«˜çš„ç”¨æˆ·æ»¡æ„åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿä»£ç†ã€è¿œç¨‹ä¼šè®®ã€åœ¨çº¿æ•™è‚²ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹æœ‰æœ›åœ¨ç¤¾äº¤æœºå™¨äººå’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.

