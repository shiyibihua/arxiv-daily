---
layout: default
title: Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs
---

# Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22139" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22139v3</a>
  <a href="https://arxiv.org/pdf/2506.22139.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22139v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22139v3', 'Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaojie Zhang, Jiahui Yang, Jianqin Yin, Zhenbo Luo, Jian Luan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-07-22)

**å¤‡æ³¨**: Accepted at ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-Frameä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„å¸§é€‰æ‹©ä¸å¤šåˆ†è¾¨ç‡é€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `å¸§é€‰æ‹©` `å¤šåˆ†è¾¨ç‡é€‚åº”` `Gumbel-MaxæŠ€å·§` `æ–‡æœ¬-å›¾åƒåŒ¹é…` `æ—¶ç©ºä¿¡æ¯` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡è§†é¢‘æ•°æ®æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰ä¸æŸ¥è¯¢ç›¸å…³çš„æ—¶ç©ºä¿¡æ¯ã€‚
2. Q-Frameé€šè¿‡è‡ªé€‚åº”å¸§é€‰æ‹©å’Œå¤šåˆ†è¾¨ç‡ç¼©æ”¾ï¼Œåˆ©ç”¨æ–‡æœ¬-å›¾åƒåŒ¹é…ç½‘ç»œå®ç°é«˜æ•ˆçš„å¸§å¤„ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQ-Frameåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç†è§£çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨è§†é¢‘ç†è§£æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ•°æ®é‡å¤§å’Œæ—¶é—´å¤æ‚æ€§é«˜ã€‚ç°æœ‰çš„è§†é¢‘LLMsé€šå¸¸é‡‡ç”¨å‡åŒ€å¸§é‡‡æ ·ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®æ—¶ç©ºçº¿ç´¢ã€‚æœ¬æ–‡æå‡ºQ-Frameï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘å†…å®¹å’Œç‰¹å®šæŸ¥è¯¢çš„è‡ªé€‚åº”å¸§é€‰æ‹©å’Œå¤šåˆ†è¾¨ç‡ç¼©æ”¾çš„æ–°æ–¹æ³•ã€‚Q-Frameé‡‡ç”¨æ— è®­ç»ƒçš„å³æ’å³ç”¨ç­–ç•¥ï¼Œé€šè¿‡æ–‡æœ¬-å›¾åƒåŒ¹é…ç½‘ç»œï¼ˆå¦‚CLIPï¼‰ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨Gumbel-MaxæŠ€å·§è¿›è¡Œé«˜æ•ˆçš„å¸§é€‰æ‹©ã€‚Q-Frameä½¿è§†é¢‘LLMsèƒ½å¤Ÿå¤„ç†æ›´å¤šå¸§è€Œä¸è¶…å‡ºè®¡ç®—é™åˆ¶ï¼Œä»è€Œä¿ç•™å…³é”®çš„æ—¶é—´å’Œç©ºé—´ä¿¡æ¯ã€‚é€šè¿‡åœ¨MLVUã€LongVideoBenchå’ŒVideo-MMEç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†Q-Frameçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä¼˜äºç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿åŠåœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘ç†è§£ä¸­å¸§é€‰æ‹©çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”±äºå‡åŒ€é‡‡æ ·ï¼Œéš¾ä»¥æ•æ‰åˆ°ä¸æŸ¥è¯¢ç›¸å…³çš„é‡è¦æ—¶ç©ºçº¿ç´¢ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQ-Frameçš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®è§†é¢‘å†…å®¹å’Œç‰¹å®šæŸ¥è¯¢è‡ªé€‚åº”é€‰æ‹©å¸§ï¼Œå¹¶è¿›è¡Œå¤šåˆ†è¾¨ç‡ç¼©æ”¾ï¼Œä»¥æé«˜ä¿¡æ¯ä¿ç•™ç‡å’Œå¤„ç†æ•ˆç‡ã€‚é€šè¿‡æ— è®­ç»ƒçš„å³æ’å³ç”¨ç­–ç•¥ï¼Œç»“åˆæ–‡æœ¬-å›¾åƒåŒ¹é…ç½‘ç»œï¼Œä¼˜åŒ–äº†å¸§é€‰æ‹©è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQ-Frameçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬-å›¾åƒåŒ¹é…ç½‘ç»œç”¨äºç”ŸæˆæŸ¥è¯¢ç›¸å…³çš„å¸§é€‰æ‹©ç­–ç•¥ï¼ŒGumbel-MaxæŠ€å·§ç”¨äºé«˜æ•ˆé€‰æ‹©å¸§ï¼Œä»¥åŠå¤šåˆ†è¾¨ç‡é€‚åº”æ¨¡å—ä»¥å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„è§†é¢‘æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šQ-Frameçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„å¸§é€‰æ‹©ç­–ç•¥å’Œå¤šåˆ†è¾¨ç‡é€‚åº”èƒ½åŠ›ï¼Œä½¿å¾—è§†é¢‘ç†è§£æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œå¤„ç†æ›´å¤šå¸§å¹¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å‡åŒ€é‡‡æ ·ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒQ-Frameé‡‡ç”¨äº†Gumbel-MaxæŠ€å·§æ¥ä¼˜åŒ–å¸§é€‰æ‹©è¿‡ç¨‹ï¼Œç¡®ä¿é€‰æ‹©çš„å¸§èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦åœ°ä¿ç•™ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šç»“åˆäº†CLIPæ¨¡å‹çš„ç‰¹æ€§ï¼Œä»¥å¢å¼ºæ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„åŒ¹é…æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆå¦‚MLVUã€LongVideoBenchå’ŒVideo-MMEï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQ-Frameåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å‡†ç¡®ç‡å’Œå¤„ç†æ•ˆç‡ä¸Šå‡æœ‰è¶…è¿‡10%çš„æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Q-Frameçš„ç ”ç©¶æˆæœåœ¨è§†é¢‘ç†è§£é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘æ£€ç´¢ã€è§†é¢‘æ‘˜è¦ç”Ÿæˆå’Œå¤šæ¨¡æ€å†…å®¹åˆ†æç­‰åœºæ™¯ä¸­ã€‚é€šè¿‡æé«˜è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒQ-Frameèƒ½å¤Ÿä¸ºæ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶å’Œå¨±ä¹ç­‰è¡Œä¸šå¸¦æ¥æ˜¾è‘—çš„å®é™…ä»·å€¼ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have demonstrated significant success in visual understanding tasks. However, challenges persist in adapting these models for video comprehension due to the large volume of data and temporal complexity. Existing Video-LLMs using uniform frame sampling often struggle to capture the query-related crucial spatiotemporal clues of videos effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive frame selection and multi-resolution scaling tailored to the video's content and the specific query. Q-Frame employs a training-free, plug-and-play strategy generated by a text-image matching network like CLIP, utilizing the Gumbel-Max trick for efficient frame selection. Q-Frame allows Video-LLMs to process more frames without exceeding computational limits, thereby preserving critical temporal and spatial information. We demonstrate Q-Frame's effectiveness through extensive experiments on benchmark datasets, including MLVU, LongVideoBench, and Video-MME, illustrating its superiority over existing methods and its applicability across various video understanding tasks.

