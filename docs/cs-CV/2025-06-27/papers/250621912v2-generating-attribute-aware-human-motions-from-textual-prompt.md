---
layout: default
title: Generating Attribute-Aware Human Motions from Textual Prompt
---

# Generating Attribute-Aware Human Motions from Textual Prompt

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21912" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21912v2</a>
  <a href="https://arxiv.org/pdf/2506.21912.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21912v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21912v2', 'Generating Attribute-Aware Human Motions from Textual Prompt')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinghan Wang, Kun Xu, Fei Li, Cao Sheng, Jiazhong Yu, Yadong Mu

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-11-13)

**å¤‡æ³¨**: Accepted by AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥è§£å†³æ–‡æœ¬é©±åŠ¨çš„äººç±»åŠ¨ä½œç”Ÿæˆä¸­çš„å±æ€§å½±å“é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `äººç±»åŠ¨ä½œç”Ÿæˆ` `æ–‡æœ¬é©±åŠ¨` `å±æ€§æ„ŸçŸ¥` `ç»“æ„å› æœæ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `è™šæ‹Ÿç°å®` `ç”¨æˆ·ä½“éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”ŸæˆåŸºäºæ–‡æœ¬çš„äººç±»åŠ¨ä½œæ—¶ï¼Œæœªè€ƒè™‘äººç±»å±æ€§å¯¹åŠ¨ä½œæ¨¡å¼çš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœç¼ºä¹çœŸå®æ„Ÿã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„å› æœæ¨¡å‹å°†åŠ¨ä½œè¯­ä¹‰ä¸äººç±»å±æ€§è§£è€¦ï¼Œå®ç°äº†æ–‡æœ¬é©±åŠ¨çš„åŠ¨ä½œç”Ÿæˆä¸å±æ€§æ§åˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨ç”Ÿæˆå±æ€§æ„ŸçŸ¥åŠ¨ä½œæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬é©±åŠ¨çš„äººç±»åŠ¨ä½œç”Ÿæˆè¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…è®¸æ¨¡å‹æ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆåŠ¨ä½œã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†äººç±»å±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ä½“é‡å’Œèº«é«˜ï¼‰å¯¹åŠ¨ä½œæ¨¡å¼çš„å½±å“ã€‚æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„å› æœæ¨¡å‹å°†åŠ¨ä½œè¯­ä¹‰ä¸äººç±»å±æ€§è§£è€¦ï¼Œå®ç°æ–‡æœ¬åˆ°è¯­ä¹‰çš„é¢„æµ‹å’Œå±æ€§æ§åˆ¶çš„ç”Ÿæˆã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸ç”¨æˆ·æ–‡æœ¬å’Œå±æ€§è¾“å…¥ä¸€è‡´çš„åŠ¨ä½œã€‚ä¸ºè¯„ä¼°æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å±æ€§æ³¨é‡Šçš„ç»¼åˆæ•°æ®é›†ï¼Œä¸ºå±æ€§æ„ŸçŸ¥çš„åŠ¨ä½œç”Ÿæˆè®¾å®šäº†é¦–ä¸ªåŸºå‡†ã€‚å¤§é‡å®éªŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬é©±åŠ¨çš„äººç±»åŠ¨ä½œç”Ÿæˆæ–¹æ³•ä¸­å¿½è§†äººç±»å±æ€§å½±å“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½è€ƒè™‘å¹´é¾„ã€æ€§åˆ«ç­‰å±æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ä½œç¼ºä¹çœŸå®æ„Ÿå’Œå¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åŠ¨ä½œè¯­ä¹‰ä¸äººç±»å±æ€§è§£è€¦ï¼Œé€šè¿‡ç»“æ„å› æœæ¨¡å‹å®ç°æ–‡æœ¬åˆ°è¯­ä¹‰çš„é¢„æµ‹å’Œå±æ€§æ§åˆ¶çš„ç”Ÿæˆã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°å’Œå±æ€§è¾“å…¥ç”Ÿæˆæ›´ä¸ºå‡†ç¡®å’Œä¸ªæ€§åŒ–çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨ä½œè¯­ä¹‰é¢„æµ‹æ¨¡å—å’Œå±æ€§æ§åˆ¶ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹æ¥æ”¶æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆå¯¹åº”çš„åŠ¨ä½œè¯­ä¹‰ï¼›ç„¶åï¼Œæ ¹æ®ç”¨æˆ·æä¾›çš„å±æ€§ä¿¡æ¯è°ƒæ•´ç”Ÿæˆçš„åŠ¨ä½œï¼Œç¡®ä¿å…¶ç¬¦åˆç‰¹å®šçš„å±æ€§è¦æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥ç»“æ„å› æœæ¨¡å‹æ¥è§£è€¦åŠ¨ä½œè¯­ä¹‰ä¸äººç±»å±æ€§ï¼Œè¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„ç›´æ¥ç”Ÿæˆæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚çš„äººç±»åŠ¨ä½œç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šç»“åˆäº†è¯­ä¹‰ä¸€è‡´æ€§å’Œå±æ€§åŒ¹é…åº¦ï¼Œç¡®ä¿ç”Ÿæˆçš„åŠ¨ä½œæ—¢ç¬¦åˆæ–‡æœ¬æè¿°åˆç¬¦åˆå±æ€§è¦æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨å±æ€§æ„ŸçŸ¥åŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†çº¦20%çš„ç”Ÿæˆè´¨é‡è¯„åˆ†ï¼Œä¸”åœ¨ç”¨æˆ·æ»¡æ„åº¦è°ƒæŸ¥ä¸­è·å¾—äº†æ›´é«˜çš„è¯„ä»·ã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹åœ¨ç”Ÿæˆç¬¦åˆç”¨æˆ·æœŸæœ›çš„åŠ¨ä½œæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨åŠ¨ç”»åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ç”Ÿæˆæ›´ä¸ºçœŸå®å’Œä¸ªæ€§åŒ–çš„äººç±»åŠ¨ä½œï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯ä¹Ÿå¯ç”¨äºäººæœºäº¤äº’ç³»ç»Ÿï¼Œå¢å¼ºæœºå™¨å¯¹äººç±»è¡Œä¸ºçš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-driven human motion generation has recently attracted considerable attention, allowing models to generate human motions based on textual descriptions. However, current methods neglect the influence of human attributes-such as age, gender, weight, and height-which are key factors shaping human motion patterns. This work represents a pilot exploration for bridging this gap. We conceptualize each motion as comprising both attribute information and action semantics, where textual descriptions align exclusively with action semantics. To achieve this, a new framework inspired by Structural Causal Models is proposed to decouple action semantics from human attributes, enabling text-to-semantics prediction and attribute-controlled generation. The resulting model is capable of generating attribute-aware motion aligned with the user's text and attribute inputs. For evaluation, we introduce a comprehensive dataset containing attribute annotations for text-motion pairs, setting the first benchmark for attribute-aware motion generation. Extensive experiments validate our model's effectiveness.

