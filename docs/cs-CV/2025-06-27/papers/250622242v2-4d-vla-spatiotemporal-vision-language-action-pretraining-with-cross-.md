---
layout: default
title: 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration
---

# 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22242" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22242v2</a>
  <a href="https://arxiv.org/pdf/2506.22242.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22242v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22242v2', '4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-11-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4D-VLAä»¥è§£å†³æœºå™¨äººé¢„è®­ç»ƒä¸­çš„æ··ä¹±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººé¢„è®­ç»ƒ` `æ—¶ç©ºæ¨ç†` `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ ` `è®°å¿†åº“é‡‡æ ·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨æœºå™¨äººæ•°æ®è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œé¢ä¸´è¾“å…¥ä¸å®Œæ•´å¯¼è‡´çš„åŠ¨ä½œåˆ†å¸ƒæ··ä¹±é—®é¢˜ã€‚
2. è®ºæ–‡æå‡º4D-VLAï¼Œé€šè¿‡å¼•å…¥æ·±åº¦å’Œæ—¶é—´ä¿¡æ¯ï¼Œæ ¡å‡†æœºå™¨äººä¸åœºæ™¯çš„åæ ‡ç³»ç»Ÿï¼Œå¢å¼ºæ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ4D-VLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œç›¸è¾ƒäºOpenVLAæ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„ç©ºé—´ç†è§£å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ©ç”¨å¤šæ ·åŒ–çš„æœºå™¨äººæ•°æ®è¿›è¡Œé¢„è®­ç»ƒä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨ç®€å•è§‚å¯Ÿä½œä¸ºè¾“å…¥æ¥å»ºæ¨¡æ•°æ®é›†çš„åŠ¨ä½œåˆ†å¸ƒï¼Œä½†è¿™äº›è¾“å…¥å¾€å¾€ä¸å®Œæ•´ï¼Œå¯¼è‡´æ¡ä»¶åŠ¨ä½œåˆ†å¸ƒåˆ†æ•£ï¼Œç§°ä¸ºåæ ‡ç³»ç»Ÿæ··ä¹±å’ŒçŠ¶æ€æ··ä¹±ã€‚è¿™ç§ä¸ä¸€è‡´æ€§æ˜¾è‘—å½±å“äº†é¢„è®­ç»ƒæ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†4D-VLAï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†4Dä¿¡æ¯æ•´åˆåˆ°è¾“å…¥ä¸­ï¼Œä»¥å‡è½»è¿™äº›æ··ä¹±æºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡é¡ºåºRGB-Dè¾“å…¥å°†æ·±åº¦å’Œæ—¶é—´ä¿¡æ¯å¼•å…¥è§†è§‰ç‰¹å¾ï¼Œæ ¡å‡†æœºå™¨äººå’Œåœºæ™¯çš„åæ ‡ç³»ç»Ÿï¼Œä»è€Œèµ‹äºˆæ¨¡å‹å¼ºå¤§çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶æœ€å°åŒ–è®­ç»ƒå¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è®°å¿†åº“é‡‡æ ·ï¼Œè¿™æ˜¯ä¸€ç§ä»å†å²å›¾åƒä¸­æå–ä¿¡æ¯å¸§çš„å¸§é‡‡æ ·ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•å’Œæ¶æ„ç»„ä»¶æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººæ•°æ®é¢„è®­ç»ƒä¸­ï¼Œç”±äºè¾“å…¥çš„ä¸å®Œæ•´æ€§å¯¼è‡´çš„åæ ‡ç³»ç»Ÿæ··ä¹±å’ŒçŠ¶æ€æ··ä¹±ã€‚è¿™ç§æ··ä¹±ä½¿å¾—æ¡ä»¶åŠ¨ä½œåˆ†å¸ƒåˆ†æ•£ï¼Œå½±å“äº†æ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡å¼•å…¥4Dä¿¡æ¯ï¼ˆæ·±åº¦å’Œæ—¶é—´ï¼‰æ¥æ ¡å‡†æœºå™¨äººä¸åœºæ™¯çš„åæ ‡ç³»ç»Ÿï¼Œä»è€Œå‡è½»æ··ä¹±ç°è±¡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œæ—¶ç©ºæ¨ç†ï¼ŒåŒæ—¶é™ä½è®­ç»ƒå¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥çš„RGB-Dåºåˆ—ï¼Œç»è¿‡ç‰¹å¾æå–åè¿›è¡Œåæ ‡ç³»ç»Ÿçš„æ ¡å‡†ï¼Œæœ€åé€šè¿‡è®°å¿†åº“é‡‡æ ·ç­–ç•¥æå–ä¿¡æ¯å¸§ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¾“å…¥å¤„ç†ã€ç‰¹å¾æå–ã€åæ ‡æ ¡å‡†å’Œé‡‡æ ·ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äº4Dä¿¡æ¯çš„å¼•å…¥å’Œè®°å¿†åº“é‡‡æ ·ç­–ç•¥çš„è®¾è®¡ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åªä¾èµ–äºç®€å•çš„è§†è§‰è¾“å…¥ï¼Œè€Œ4D-VLAåˆ™é€šè¿‡æ·±åº¦ä¿¡æ¯å’Œæ—¶é—´åºåˆ—å¢å¼ºäº†æ¨¡å‹çš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ—¶ç©ºç‰¹å¾çš„å­¦ä¹ ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†æ·±åº¦å·ç§¯å±‚ä»¥å¤„ç†RGB-Dè¾“å…¥ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œ4D-VLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œç›¸è¾ƒäºOpenVLAï¼ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨ç©ºé—´ç†è§£å’Œé€‚åº”æ€§æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç¯å¢ƒçš„ç©ºé—´ç†è§£å’Œé€‚åº”èƒ½åŠ›ï¼Œ4D-VLAèƒ½å¤Ÿåœ¨å¤æ‚åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–å’Œè¡ŒåŠ¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Leveraging diverse robotic data for pretraining remains a critical challenge. Existing methods typically model the dataset's action distribution using simple observations as inputs. However, these inputs are often incomplete, resulting in a dispersed conditional action distribution-an issue we refer to as coordinate system chaos and state chaos. This inconsistency significantly hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel approach that effectively integrates 4D information into the input to mitigate these sources of chaos. Our model introduces depth and temporal information into visual features with sequential RGB-D inputs, aligning the coordinate systems of the robot and the scene. This alignment endows the model with strong spatiotemporal reasoning capabilities while minimizing training overhead. Additionally, we introduce memory bank sampling, a frame sampling strategy designed to extract informative frames from historical images, further improving effectiveness and efficiency. Experimental results demonstrate that our pretraining method and architectural components substantially enhance model performance. In both simulated and real-world experiments, our model achieves a significant increase in success rate over OpenVLA. To further assess spatial perception and generalization to novel views, we introduce MV-Bench, a multi-view simulation benchmark. Our model consistently outperforms existing methods, demonstrating stronger spatial understanding and adaptability.

