---
layout: default
title: MiCo: Multi-image Contrast for Reinforcement Visual Reasoning
---

# MiCo: Multi-image Contrast for Reinforcement Visual Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22434" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22434v1</a>
  <a href="https://arxiv.org/pdf/2506.22434.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22434v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22434v1', 'MiCo: Multi-image Contrast for Reinforcement Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMiCoä»¥è§£å†³å¤šå›¾åƒæ¨ç†ä¸­çš„é€»è¾‘å…³è”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šå›¾åƒæ¨ç†` `è‡ªç›‘ç£å­¦ä¹ ` `è§†è§‰æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰æ¯”è¾ƒ` `é€»è¾‘æ¨ç†` `å›¾åƒå¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ•´ç†çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„è§†è§‰ç»†èŠ‚å’Œé€»è¾‘å…³ç³»ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ„å»ºå›¾åƒä¸‰å…ƒç»„ï¼Œåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼è¿›è¡Œè§†è§‰æ¨ç†ï¼Œé¿å…äººå·¥æ ‡æ³¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡ï¼Œä¸”åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å¼ºåŠ²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢ç´¢äº†å¦‚ä½•é€šè¿‡Chain-of-Thought (CoT) æ¨ç†å°†è§†è§‰çº¿ç´¢è¿æ¥è·¨å¤šä¸ªå›¾åƒã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œè¿™åœ¨å¤„ç†ç»†è‡´çš„è§†è§‰ç»†èŠ‚å’Œå¤æ‚é€»è¾‘æ—¶å°¤ä¸ºå›°éš¾ã€‚æˆ‘ä»¬å€Ÿé‰´è‡ªç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„ç†å¿µï¼Œæ„å»ºäº†åŒ…å«ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒå›¾åƒçš„å›¾åƒä¸‰å…ƒç»„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«æç¤ºç”Ÿæˆæ¨ç†è¿‡ç¨‹ä»¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼Œå¹¶é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ä»…åœ¨è§†è§‰æ¯”è¾ƒä»»åŠ¡ä¸Šè®­ç»ƒï¼Œæ‰€å­¦çš„æ¨ç†èƒ½åŠ›åœ¨å¹¿æ³›é—®é¢˜ä¸Šæœ‰æ•ˆæ³›åŒ–ï¼Œä¸”æ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ˜¾è‘—æå‡äº†å¤šå›¾åƒæ¨ç†åŸºå‡†çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šå›¾åƒæ¨ç†ä¸­è§†è§‰çº¿ç´¢çš„é€»è¾‘å…³è”é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äººå·¥æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œéš¾ä»¥å¤„ç†ç»†è‡´çš„è§†è§‰ä¿¡æ¯å’Œå¤æ‚çš„é€»è¾‘å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºåŒ…å«ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒå›¾åƒçš„å›¾åƒä¸‰å…ƒç»„ï¼Œåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼è¿›è¡Œæ¨ç†è®­ç»ƒã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ç»†å¾®çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å›¾åƒä¸‰å…ƒç»„çš„æ„å»ºã€æ¨ç†è¿‡ç¨‹çš„ç”Ÿæˆå’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶éœ€è¦æ¯”è¾ƒå›¾åƒï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯ç›¸åŒè¿˜æ˜¯ä¸åŒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼æ„å»ºå›¾åƒä¸‰å…ƒç»„ï¼Œé¿å…äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œå¹¶ä¸”é€šè¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡è®­ç»ƒå‡ºæœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹å¯¹ç»†å¾®å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä¸»æµæ–¹æ³•ï¼Œä¸”åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†æ¨¡å‹çš„å¹¿æ³›é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç†è§£ã€è‡ªåŠ¨é—®ç­”ç³»ç»Ÿå’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œèƒ½å¤Ÿæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.

