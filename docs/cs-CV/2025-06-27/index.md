---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-27
---

# cs.CVï¼ˆ2025-06-27ï¼‰

ğŸ“Š å…± **41** ç¯‡è®ºæ–‡
 | ğŸ”— **9** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ğŸ”—4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (3)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250621873v1-grounding-aware-token-pruning-recovering-from-drastic-performance-dr.html">Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning</a></td>
  <td>æå‡ºåŸºäºå®šä½æ„ŸçŸ¥çš„æ ‡è®°å‰ªæä»¥è§£å†³è§†è§‰å®šä½æ€§èƒ½ä¸‹é™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21873v1" data-paper-url="./papers/250621873v1-grounding-aware-token-pruning-recovering-from-drastic-performance-dr.html" onclick="toggleFavorite(this, '2506.21873v1', 'Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250622385v2-can-video-large-multimodal-models-think-like-doubters-or-double-down.html">Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment</a></td>
  <td>æå‡ºå¯å¦å®šè§†é¢‘è•´å«ä»»åŠ¡ä»¥æå‡è§†é¢‘å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22385v2" data-paper-url="./papers/250622385v2-can-video-large-multimodal-models-think-like-doubters-or-double-down.html" onclick="toggleFavorite(this, '2506.22385v2', 'Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250621832v1-taleforge-interactive-multimodal-system-for-personalized-story-creat.html">TaleForge: Interactive Multimodal System for Personalized Story Creation</a></td>
  <td>æå‡ºTaleForgeä»¥è§£å†³ä¸ªæ€§åŒ–æ•…äº‹åˆ›ä½œçš„å‚ä¸åº¦ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21832v1" data-paper-url="./papers/250621832v1-taleforge-interactive-multimodal-system-for-personalized-story-creat.html" onclick="toggleFavorite(this, '2506.21832v1', 'TaleForge: Interactive Multimodal System for Personalized Story Creation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250622274v1-cooco-common-objects-out-of-context-semantic-violation-in-scenes-inv.html">COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication</a></td>
  <td>æå‡ºCOOCOæ•°æ®é›†ä»¥ç ”ç©¶å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åœ¨æŒ‡ç§°äº¤æµä¸­çš„ä½œç”¨</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22274v1" data-paper-url="./papers/250622274v1-cooco-common-objects-out-of-context-semantic-violation-in-scenes-inv.html" onclick="toggleFavorite(this, '2506.22274v1', 'COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250622149v1-retfiner-a-vision-language-refinement-scheme-for-retinal-foundation-.html">RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models</a></td>
  <td>æå‡ºRetFinerä»¥è§£å†³è§†ç½‘è†œåŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22149v1" data-paper-url="./papers/250622149v1-retfiner-a-vision-language-refinement-scheme-for-retinal-foundation-.html" onclick="toggleFavorite(this, '2506.22149v1', 'RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250622041v1-towards-scalable-and-robust-white-matter-lesion-localization-via-mul.html">Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ä»¥è§£å†³ç™½è´¨ç—…ç¶å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22041v1" data-paper-url="./papers/250622041v1-towards-scalable-and-robust-white-matter-lesion-localization-via-mul.html" onclick="toggleFavorite(this, '2506.22041v1', 'Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250621975v1-taseg-text-aware-rgb-t-semantic-segmentation-based-on-fine-tuning-vi.html">TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models</a></td>
  <td>æå‡ºTASegæ¡†æ¶ä»¥è§£å†³RGB-Tè¯­ä¹‰åˆ†å‰²ä¸­çš„æ–‡æœ¬ä¿¡æ¯ç¼ºå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21975v1" data-paper-url="./papers/250621975v1-taseg-text-aware-rgb-t-semantic-segmentation-based-on-fine-tuning-vi.html" onclick="toggleFavorite(this, '2506.21975v1', 'TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250621924v1-spazer-spatial-semantic-progressive-reasoning-agent-for-zero-shot-3d.html">SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding</a></td>
  <td>æå‡ºSPAZERä»¥è§£å†³é›¶-shot 3Dè§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21924v1" data-paper-url="./papers/250621924v1-spazer-spatial-semantic-progressive-reasoning-agent-for-zero-shot-3d.html" onclick="toggleFavorite(this, '2506.21924v1', 'SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250621826v1-few-shot-segmentation-of-historical-maps-via-linear-probing-of-visio.html">Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models</a></td>
  <td>æå‡ºåŸºäºçº¿æ€§æ¢æµ‹çš„å°‘æ ·æœ¬å†å²åœ°å›¾åˆ†å‰²æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21826v1" data-paper-url="./papers/250621826v1-few-shot-segmentation-of-historical-maps-via-linear-probing-of-visio.html" onclick="toggleFavorite(this, '2506.21826v1', 'Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250621934v1-cal-rag-retrieval-augmented-multi-agent-generation-for-content-aware.html">CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design</a></td>
  <td>æå‡ºCAL-RAGä»¥è§£å†³å†…å®¹æ„ŸçŸ¥å¸ƒå±€ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21934v1" data-paper-url="./papers/250621934v1-cal-rag-retrieval-augmented-multi-agent-generation-for-content-aware.html" onclick="toggleFavorite(this, '2506.21934v1', 'CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250621895v1-exploring-task-solving-paradigm-for-generalized-cross-domain-face-an.html">Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å¾®è°ƒçš„è·¨åŸŸäººè„¸åæ¬ºè¯ˆæ–¹æ³•ä»¥è§£å†³æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21895v1" data-paper-url="./papers/250621895v1-exploring-task-solving-paradigm-for-generalized-cross-domain-face-an.html" onclick="toggleFavorite(this, '2506.21895v1', 'Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250621862v1-llava-scissor-token-compression-with-semantic-connected-components-f.html">LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs</a></td>
  <td>æå‡ºLLaVA-Scissorä»¥è§£å†³è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„tokenå‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21862v1" data-paper-url="./papers/250621862v1-llava-scissor-token-compression-with-semantic-connected-components-f.html" onclick="toggleFavorite(this, '2506.21862v1', 'LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250702941v1-gametilenet-a-semantic-dataset-for-low-resolution-game-art-in-proced.html">GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation</a></td>
  <td>æå‡ºGameTileNetä»¥è§£å†³ä½åˆ†è¾¨ç‡æ¸¸æˆè‰ºæœ¯ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.02941v1" data-paper-url="./papers/250702941v1-gametilenet-a-semantic-dataset-for-low-resolution-game-art-in-proced.html" onclick="toggleFavorite(this, '2507.02941v1', 'GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250622554v2-seamless-interaction-dyadic-audiovisual-motion-modeling-and-large-sc.html">Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</a></td>
  <td>æå‡ºæ— ç¼äº¤äº’æ¨¡å‹ä»¥è§£å†³äººæœºäº¤äº’ä¸­çš„éè¯­è¨€ä¿¡å·ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22554v2" data-paper-url="./papers/250622554v2-seamless-interaction-dyadic-audiovisual-motion-modeling-and-large-sc.html" onclick="toggleFavorite(this, '2506.22554v2', 'Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250622395v1-test-time-consistency-in-vision-language-models.html">Test-Time Consistency in Vision Language Models</a></td>
  <td>æå‡ºæµ‹è¯•æ—¶ä¸€è‡´æ€§æ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22395v1" data-paper-url="./papers/250622395v1-test-time-consistency-in-vision-language-models.html" onclick="toggleFavorite(this, '2506.22395v1', 'Test-Time Consistency in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250622283v2-rethinking-visual-token-reduction-in-lvlms-under-cross-modal-misalig.html">Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment</a></td>
  <td>æå‡ºVisionDropä»¥è§£å†³LVLMä¸­è§†è§‰æ ‡è®°å†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22283v2" data-paper-url="./papers/250622283v2-rethinking-visual-token-reduction-in-lvlms-under-cross-modal-misalig.html" onclick="toggleFavorite(this, '2506.22283v2', 'Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250621835v3-prosam-enhancing-the-robustness-of-sam-based-visual-reference-segmen.html">ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts</a></td>
  <td>æå‡ºProSAMä»¥è§£å†³SAMè§†è§‰å‚è€ƒåˆ†å‰²çš„ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21835v3" data-paper-url="./papers/250621835v3-prosam-enhancing-the-robustness-of-sam-based-visual-reference-segmen.html" onclick="toggleFavorite(this, '2506.21835v3', 'ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250622567v1-unifying-biomedical-vision-language-expertise-towards-a-generalist-f.html">Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation</a></td>
  <td>æå‡ºMMKD-CLIPä»¥è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸæ¨¡å‹æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22567v1" data-paper-url="./papers/250622567v1-unifying-biomedical-vision-language-expertise-towards-a-generalist-f.html" onclick="toggleFavorite(this, '2506.22567v1', 'Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250621855v1-periodic-mae-periodic-video-masked-autoencoder-for-rppg-estimation.html">Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation</a></td>
  <td>æå‡ºå‘¨æœŸæ€§è§†é¢‘æ©ç è‡ªç¼–ç å™¨ä»¥è§£å†³rPPGä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">PULSE</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21855v1" data-paper-url="./papers/250621855v1-periodic-mae-periodic-video-masked-autoencoder-for-rppg-estimation.html" onclick="toggleFavorite(this, '2506.21855v1', 'Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250622591v1-brainmt-a-hybrid-mamba-transformer-architecture-for-modeling-long-ra.html">BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data</a></td>
  <td>æå‡ºBrainMTä»¥è§£å†³fMRIæ•°æ®é•¿ç¨‹ä¾èµ–å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22591v1" data-paper-url="./papers/250622591v1-brainmt-a-hybrid-mamba-transformer-architecture-for-modeling-long-ra.html" onclick="toggleFavorite(this, '2506.22591v1', 'BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250622246v1-eamamba-efficient-all-around-vision-state-space-model-for-image-rest.html">EAMamba: Efficient All-Around Vision State Space Model for Image Restoration</a></td>
  <td>æå‡ºEAMambaä»¥è§£å†³ä½çº§è§†è§‰ä»»åŠ¡ä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22246v1" data-paper-url="./papers/250622246v1-eamamba-efficient-all-around-vision-state-space-model-for-image-rest.html" onclick="toggleFavorite(this, '2506.22246v1', 'EAMamba: Efficient All-Around Vision State Space Model for Image Restoration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250622216v1-ref-lle-personalized-low-light-enhancement-via-reference-guided-deep.html">ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning</a></td>
  <td>æå‡ºReF-LLEä»¥è§£å†³ä½å…‰å›¾åƒå¢å¼ºçš„ä¸ªæ€§åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22216v1" data-paper-url="./papers/250622216v1-ref-lle-personalized-low-light-enhancement-via-reference-guided-deep.html" onclick="toggleFavorite(this, '2506.22216v1', 'ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250621857v2-spade-spatial-transcriptomics-and-pathology-alignment-using-a-mixtur.html">SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space</a></td>
  <td>æå‡ºSPADEä»¥è§£å†³ç—…ç†å›¾åƒä¸ç©ºé—´è½¬å½•ç»„æ•°æ®æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21857v2" data-paper-url="./papers/250621857v2-spade-spatial-transcriptomics-and-pathology-alignment-using-a-mixtur.html" onclick="toggleFavorite(this, '2506.21857v2', 'SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250622624v1-seg-r1-segmentation-can-be-surprisingly-simple-with-reinforcement-le.html">Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning</a></td>
  <td>æå‡ºSeg-R1ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„åƒç´ çº§ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22624v1" data-paper-url="./papers/250622624v1-seg-r1-segmentation-can-be-surprisingly-simple-with-reinforcement-le.html" onclick="toggleFavorite(this, '2506.22624v1', 'Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250621980v3-r1-track-direct-application-of-mllms-to-visual-object-tracking-via-r.html">R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning</a></td>
  <td>æå‡ºR1-Trackä»¥è§£å†³è§†è§‰ç›®æ ‡è·Ÿè¸ªä¸­çš„æ¨¡æ¿åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21980v3" data-paper-url="./papers/250621980v3-r1-track-direct-application-of-mllms-to-visual-object-tracking-via-r.html" onclick="toggleFavorite(this, '2506.21980v3', 'R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250622434v1-mico-multi-image-contrast-for-reinforcement-visual-reasoning.html">MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</a></td>
  <td>æå‡ºMiCoä»¥è§£å†³å¤šå›¾åƒæ¨ç†ä¸­çš„é€»è¾‘å…³è”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22434v1" data-paper-url="./papers/250622434v1-mico-multi-image-contrast-for-reinforcement-visual-reasoning.html" onclick="toggleFavorite(this, '2506.22434v1', 'MiCo: Multi-image Contrast for Reinforcement Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250622637v2-cao-2-rectifying-inconsistencies-in-diffusion-based-dataset-distilla.html">CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation</a></td>
  <td>æå‡ºCaO$_2$ä»¥è§£å†³æ‰©æ•£æ¨¡å‹æ•°æ®è’¸é¦ä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22637v2" data-paper-url="./papers/250622637v2-cao-2-rectifying-inconsistencies-in-diffusion-based-dataset-distilla.html" onclick="toggleFavorite(this, '2506.22637v2', 'CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250622298v1-outdreamer-video-outpainting-with-a-diffusion-transformer.html">OutDreamer: Video Outpainting with a Diffusion Transformer</a></td>
  <td>æå‡ºOutDreamerä»¥è§£å†³è§†é¢‘å¤–å»¶ç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dreamer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22298v1" data-paper-url="./papers/250622298v1-outdreamer-video-outpainting-with-a-diffusion-transformer.html" onclick="toggleFavorite(this, '2506.22298v1', 'OutDreamer: Video Outpainting with a Diffusion Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250621957v1-exploring-semantic-masked-autoencoder-for-self-supervised-point-clou.html">Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding</a></td>
  <td>æå‡ºè¯­ä¹‰æ©ç è‡ªç¼–ç å™¨ä»¥è§£å†³ç‚¹äº‘ç†è§£ä¸­çš„è¯­ä¹‰å…³ç³»æ•æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21957v1" data-paper-url="./papers/250621957v1-exploring-semantic-masked-autoencoder-for-self-supervised-point-clou.html" onclick="toggleFavorite(this, '2506.21957v1', 'Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250621905v1-raum-net-regional-attention-and-uncertainty-aware-mamba-network.html">RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network</a></td>
  <td>æå‡ºRAUM-Netä»¥è§£å†³ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21905v1" data-paper-url="./papers/250621905v1-raum-net-regional-attention-and-uncertainty-aware-mamba-network.html" onclick="toggleFavorite(this, '2506.21905v1', 'RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250622291v1-roomcraft-controllable-and-complete-3d-indoor-scene-generation.html">RoomCraft: Controllable and Complete 3D Indoor Scene Generation</a></td>
  <td>æå‡ºRoomCraftä»¥è§£å†³3Då®¤å†…åœºæ™¯ç”Ÿæˆä¸­çš„å¤šçº¦æŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22291v1" data-paper-url="./papers/250622291v1-roomcraft-controllable-and-complete-3d-indoor-scene-generation.html" onclick="toggleFavorite(this, '2506.22291v1', 'RoomCraft: Controllable and Complete 3D Indoor Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250622146v4-visual-structures-helps-visual-reasoning-addressing-the-binding-prob.html">Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</a></td>
  <td>æå‡ºVISERä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç»‘å®šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22146v4" data-paper-url="./papers/250622146v4-visual-structures-helps-visual-reasoning-addressing-the-binding-prob.html" onclick="toggleFavorite(this, '2506.22146v4', 'Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250622433v1-warprf-multi-view-consistency-for-training-free-uncertainty-quantifi.html">WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields</a></td>
  <td>æå‡ºWarpRFæ¡†æ¶ä»¥è§£å†³è¾å°„åœºä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22433v1" data-paper-url="./papers/250622433v1-warprf-multi-view-consistency-for-training-free-uncertainty-quantifi.html" onclick="toggleFavorite(this, '2506.22433v1', 'WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>34</td>
  <td><a href="./papers/250622242v2-4d-vla-spatiotemporal-vision-language-action-pretraining-with-cross-.html">4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration</a></td>
  <td>æå‡º4D-VLAä»¥è§£å†³æœºå™¨äººé¢„è®­ç»ƒä¸­çš„æ··ä¹±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22242v2" data-paper-url="./papers/250622242v2-4d-vla-spatiotemporal-vision-language-action-pretraining-with-cross-.html" onclick="toggleFavorite(this, '2506.22242v2', '4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250622139v3-q-frame-query-aware-frame-selection-and-multi-resolution-adaptation-.html">Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs</a></td>
  <td>æå‡ºQ-Frameä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„å¸§é€‰æ‹©ä¸å¤šåˆ†è¾¨ç‡é€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22139v3" data-paper-url="./papers/250622139v3-q-frame-query-aware-frame-selection-and-multi-resolution-adaptation-.html" onclick="toggleFavorite(this, '2506.22139v3', 'Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/250622099v3-bÃ©ziergs-dynamic-urban-scene-reconstruction-with-bÃ©zier-curve-gaussi.html">BÃ©zierGS: Dynamic Urban Scene Reconstruction with BÃ©zier Curve Gaussian Splatting</a></td>
  <td>æå‡ºBÃ©zierGSä»¥è§£å†³åŠ¨æ€åŸå¸‚åœºæ™¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22099v3" data-paper-url="./papers/250622099v3-bÃ©ziergs-dynamic-urban-scene-reconstruction-with-bÃ©zier-curve-gaussi.html" onclick="toggleFavorite(this, '2506.22099v3', 'BÃ©zierGS: Dynamic Urban Scene Reconstruction with BÃ©zier Curve Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250622280v1-digs-dynamic-cbct-reconstruction-using-deformation-informed-4d-gauss.html">DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model</a></td>
  <td>æå‡ºåŸºäºå˜å½¢ä¿¡æ¯çš„4Dé«˜æ–¯ç‚¹äº‘é‡å»ºæ–¹æ³•ä»¥è§£å†³åŠ¨æ€CBCTé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22280v1" data-paper-url="./papers/250622280v1-digs-dynamic-cbct-reconstruction-using-deformation-informed-4d-gauss.html" onclick="toggleFavorite(this, '2506.22280v1', 'DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>38</td>
  <td><a href="./papers/250622007v1-roboenvision-a-long-horizon-video-generation-model-for-multi-task-ro.html">RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation</a></td>
  <td>æå‡ºRoboEnvisionä»¥è§£å†³é•¿æ—¶é—´è§†é¢‘ç”Ÿæˆçš„æœºå™¨äººæ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22007v1" data-paper-url="./papers/250622007v1-roboenvision-a-long-horizon-video-generation-model-for-multi-task-ro.html" onclick="toggleFavorite(this, '2506.22007v1', 'RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250622432v2-shape-for-motion-precise-and-consistent-video-editing-with-3d-proxy.html">Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy</a></td>
  <td>æå‡ºShape-for-Motionä»¥è§£å†³è§†é¢‘ç¼–è¾‘ç²¾ç¡®æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22432v2" data-paper-url="./papers/250622432v2-shape-for-motion-precise-and-consistent-video-editing-with-3d-proxy.html" onclick="toggleFavorite(this, '2506.22432v2', 'Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/250621912v2-generating-attribute-aware-human-motions-from-textual-prompt.html">Generating Attribute-Aware Human Motions from Textual Prompt</a></td>
  <td>æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥è§£å†³æ–‡æœ¬é©±åŠ¨çš„äººç±»åŠ¨ä½œç”Ÿæˆä¸­çš„å±æ€§å½±å“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21912v2" data-paper-url="./papers/250621912v2-generating-attribute-aware-human-motions-from-textual-prompt.html" onclick="toggleFavorite(this, '2506.21912v2', 'Generating Attribute-Aware Human Motions from Textual Prompt')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/250622336v1-matcha-cross-algorithm-matching-with-feature-augmentation.html">MatChA: Cross-Algorithm Matching with Feature Augmentation</a></td>
  <td>æå‡ºMatChAä»¥è§£å†³è·¨ç®—æ³•ç‰¹å¾åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.22336v1" data-paper-url="./papers/250622336v1-matcha-cross-algorithm-matching-with-feature-augmentation.html" onclick="toggleFavorite(this, '2506.22336v1', 'MatChA: Cross-Algorithm Matching with Feature Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)