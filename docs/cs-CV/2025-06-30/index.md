---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-30
---

# cs.CVï¼ˆ2025-06-30ï¼‰

ğŸ“Š å…± **43** ç¯‡è®ºæ–‡
 | ğŸ”— **16** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (12 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250700243v1-vocal-visual-odometry-via-contrastive-learning.html">VOCAL: Visual Odometry via ContrAstive Learning</a></td>
  <td>æå‡ºVOCALæ¡†æ¶ä»¥è§£å†³è§†è§‰é‡Œç¨‹è®¡çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">visual odometry</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.00243v1" data-paper-url="./papers/250700243v1-vocal-visual-odometry-via-contrastive-learning.html" onclick="toggleFavorite(this, '2507.00243v1', 'VOCAL: Visual Odometry via ContrAstive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250623783v1-mamba-fetrack-v2-revisiting-state-space-model-for-frame-event-based-.html">Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking</a></td>
  <td>æå‡ºMamba-FETrack V2ä»¥è§£å†³å¤šæ¨¡æ€è§†è§‰ç›®æ ‡è·Ÿè¸ªæ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23783v1" data-paper-url="./papers/250623783v1-mamba-fetrack-v2-revisiting-state-space-model-for-frame-event-based-.html" onclick="toggleFavorite(this, '2506.23783v1', 'Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250623552v1-jam-flow-joint-audio-motion-synthesis-with-flow-matching.html">JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching</a></td>
  <td>æå‡ºJAM-Flowä»¥è§£å†³éŸ³é¢‘ä¸é¢éƒ¨åŠ¨ä½œåˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">motion synthesis</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23552v1" data-paper-url="./papers/250623552v1-jam-flow-joint-audio-motion-synthesis-with-flow-matching.html" onclick="toggleFavorite(this, '2506.23552v1', 'JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250701066v1-embedding-based-retrieval-in-multimodal-content-moderation.html">Embedding-based Retrieval in Multimodal Content Moderation</a></td>
  <td>æå‡ºåµŒå…¥å¼æ£€ç´¢æ–¹æ³•ä»¥è§£å†³çŸ­è§†é¢‘å†…å®¹å®¡æ ¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.01066v1" data-paper-url="./papers/250701066v1-embedding-based-retrieval-in-multimodal-content-moderation.html" onclick="toggleFavorite(this, '2507.01066v1', 'Embedding-based Retrieval in Multimodal Content Moderation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250623434v2-towards-foundational-lidar-world-models-with-efficient-latent-flow-m.html">Towards foundational LiDAR world models with efficient latent flow matching</a></td>
  <td>æå‡ºåŸºäºæ½œåœ¨æ¡ä»¶æµåŒ¹é…çš„LiDARä¸–ç•Œæ¨¡å‹ä»¥è§£å†³é¢†åŸŸè¿ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23434v2" data-paper-url="./papers/250623434v2-towards-foundational-lidar-world-models-with-efficient-latent-flow-m.html" onclick="toggleFavorite(this, '2506.23434v2', 'Towards foundational LiDAR world models with efficient latent flow matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250623580v1-dataset-distillation-via-vision-language-category-prototype.html">Dataset Distillation via Vision-Language Category Prototype</a></td>
  <td>æå‡ºè§†è§‰-è¯­è¨€ç±»åˆ«åŸå‹çš„è’¸é¦æ–¹æ³•ä»¥æå‡æ•°æ®é›†è’¸é¦æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23580v1" data-paper-url="./papers/250623580v1-dataset-distillation-via-vision-language-category-prototype.html" onclick="toggleFavorite(this, '2506.23580v1', 'Dataset Distillation via Vision-Language Category Prototype')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250623502v2-llm-enhanced-action-aware-multi-modal-prompt-tuning-for-image-text-m.html">LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching</a></td>
  <td>æå‡ºLLMå¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€æç¤ºè°ƒä¼˜ä»¥è§£å†³å›¾åƒ-æ–‡æœ¬åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23502v2" data-paper-url="./papers/250623502v2-llm-enhanced-action-aware-multi-modal-prompt-tuning-for-image-text-m.html" onclick="toggleFavorite(this, '2506.23502v2', 'LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250623468v2-navmorph-a-self-evolving-world-model-for-vision-and-language-navigat.html">NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments</a></td>
  <td>æå‡ºNavMorphä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„ç¯å¢ƒé€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">VLN</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23468v2" data-paper-url="./papers/250623468v2-navmorph-a-self-evolving-world-model-for-vision-and-language-navigat.html" onclick="toggleFavorite(this, '2506.23468v2', 'NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250702957v1-cs-vlm-compressed-sensing-attention-for-efficient-vision-language-re.html">CS-VLM: Compressed Sensing Attention for Efficient Vision-Language Representation Learning</a></td>
  <td>æå‡ºå‹ç¼©æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡ç®—ç“¶é¢ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.02957v1" data-paper-url="./papers/250702957v1-cs-vlm-compressed-sensing-attention-for-efficient-vision-language-re.html" onclick="toggleFavorite(this, '2507.02957v1', 'CS-VLM: Compressed Sensing Attention for Efficient Vision-Language Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250700263v1-room-scene-discovery-and-grouping-in-unstructured-vacation-rental-im.html">Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections</a></td>
  <td>æå‡ºæˆ¿é—´åœºæ™¯å‘ç°ä¸åˆ†ç»„æ–¹æ³•ä»¥è§£å†³åº¦å‡ç§Ÿèµå›¾åƒæ— ç»“æ„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.00263v1" data-paper-url="./papers/250700263v1-room-scene-discovery-and-grouping-in-unstructured-vacation-rental-im.html" onclick="toggleFavorite(this, '2507.00263v1', 'Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250624125v1-fadrm-fast-and-accurate-data-residual-matching-for-dataset-distillat.html">FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation</a></td>
  <td>æå‡ºFADRMä»¥è§£å†³æ•°æ®è’¸é¦ä¸­çš„ä¿¡æ¯æ¶ˆå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24125v1" data-paper-url="./papers/250624125v1-fadrm-fast-and-accurate-data-residual-matching-for-dataset-distillat.html" onclick="toggleFavorite(this, '2506.24125v1', 'FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250623519v1-from-sight-to-insight-unleashing-eye-tracking-in-weakly-supervised-v.html">From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection</a></td>
  <td>æå‡ºåŸºäºçœ¼åŠ¨è¿½è¸ªçš„å¼±ç›‘ç£è§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23519v1" data-paper-url="./papers/250623519v1-from-sight-to-insight-unleashing-eye-tracking-in-weakly-supervised-v.html" onclick="toggleFavorite(this, '2506.23519v1', 'From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250623529v1-when-test-time-adaptation-meets-self-supervised-models.html">When Test-Time Adaptation Meets Self-Supervised Models</a></td>
  <td>æå‡ºè‡ªç›‘ç£æµ‹è¯•æ—¶é€‚åº”åè®®ä»¥æå‡æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23529v1" data-paper-url="./papers/250623529v1-when-test-time-adaptation-meets-self-supervised-models.html" onclick="toggleFavorite(this, '2506.23529v1', 'When Test-Time Adaptation Meets Self-Supervised Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250623611v1-attentiongs-towards-initialization-free-3d-gaussian-splatting-via-st.html">AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</a></td>
  <td>æå‡ºAttentionGSä»¥è§£å†³3Dé‡å»ºä¸­å¯¹é«˜è´¨é‡ç‚¹äº‘çš„ä¾èµ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23611v1" data-paper-url="./papers/250623611v1-attentiongs-towards-initialization-free-3d-gaussian-splatting-via-st.html" onclick="toggleFavorite(this, '2506.23611v1', 'AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250623607v1-pgov3d-open-vocabulary-3d-semantic-segmentation-with-partial-to-glob.html">PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum</a></td>
  <td>æå‡ºPGOV3Dä»¥è§£å†³å¼€æ”¾è¯æ±‡3Dè¯­ä¹‰åˆ†å‰²ä¸­çš„ä¿¡æ¯è½¬ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23607v1" data-paper-url="./papers/250623607v1-pgov3d-open-vocabulary-3d-semantic-segmentation-with-partial-to-glob.html" onclick="toggleFavorite(this, '2506.23607v1', 'PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250624096v2-milo-mesh-in-the-loop-gaussian-splatting-for-detailed-and-efficient-.html">MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction</a></td>
  <td>æå‡ºMILoæ¡†æ¶ä»¥è§£å†³é«˜è´¨é‡3Dè¡¨é¢é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24096v2" data-paper-url="./papers/250624096v2-milo-mesh-in-the-loop-gaussian-splatting-for-detailed-and-efficient-.html" onclick="toggleFavorite(this, '2506.24096v2', 'MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250623751v1-can-we-challenge-open-vocabulary-object-detectors-with-generated-con.html">Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?</a></td>
  <td>åˆ©ç”¨ç”Ÿæˆå†…å®¹æŒ‘æˆ˜å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹å™¨çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23751v1" data-paper-url="./papers/250623751v1-can-we-challenge-open-vocabulary-object-detectors-with-generated-con.html" onclick="toggleFavorite(this, '2506.23751v1', 'Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250623479v1-instant-gaussianimage-a-generalizable-and-self-adaptive-image-repres.html">Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting</a></td>
  <td>æå‡ºè‡ªé€‚åº”é«˜æ–¯å›¾åƒè¡¨ç¤ºæ¡†æ¶ä»¥è§£å†³è®­ç»ƒæ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23479v1" data-paper-url="./papers/250623479v1-instant-gaussianimage-a-generalizable-and-self-adaptive-image-repres.html" onclick="toggleFavorite(this, '2506.23479v1', 'Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250700153v1-diffusion-based-image-augmentation-for-semantic-segmentation-in-outd.html">Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics</a></td>
  <td>æå‡ºåŸºäºæ‰©æ•£çš„å›¾åƒå¢å¼ºæ–¹æ³•ä»¥è§£å†³æˆ·å¤–æœºå™¨äººè¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.00153v1" data-paper-url="./papers/250700153v1-diffusion-based-image-augmentation-for-semantic-segmentation-in-outd.html" onclick="toggleFavorite(this, '2507.00153v1', 'Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250624121v2-textmesh4d-text-to-4d-mesh-generation-via-jacobian-deformation-field.html">TextMesh4D: Text-to-4D Mesh Generation via Jacobian Deformation Field</a></td>
  <td>æå‡ºTextMesh4Dä»¥è§£å†³åŠ¨æ€3Dç½‘æ ¼ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24121v2" data-paper-url="./papers/250624121v2-textmesh4d-text-to-4d-mesh-generation-via-jacobian-deformation-field.html" onclick="toggleFavorite(this, '2506.24121v2', 'TextMesh4D: Text-to-4D Mesh Generation via Jacobian Deformation Field')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250623897v3-prior-flow-enhancing-primitive-panoramic-optical-flow-with-orthogona.html">PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View</a></td>
  <td>æå‡ºPriOr-Flowä»¥è§£å†³å…¨æ™¯å…‰æµä¼°è®¡ä¸­çš„æåŒºå¤±çœŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23897v3" data-paper-url="./papers/250623897v3-prior-flow-enhancing-primitive-panoramic-optical-flow-with-orthogona.html" onclick="toggleFavorite(this, '2506.23897v3', 'PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250623729v1-proteus-id-id-consistent-and-motion-coherent-video-customization.html">Proteus-ID: ID-Consistent and Motion-Coherent Video Customization</a></td>
  <td>æå‡ºProteus-IDä»¥è§£å†³è§†é¢‘èº«ä»½ä¸€è‡´æ€§ä¸è¿åŠ¨è¿è´¯æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23729v1" data-paper-url="./papers/250623729v1-proteus-id-id-consistent-and-motion-coherent-video-customization.html" onclick="toggleFavorite(this, '2506.23729v1', 'Proteus-ID: ID-Consistent and Motion-Coherent Video Customization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250700224v1-computer-vision-for-objects-used-in-group-work-challenges-and-opport.html">Computer Vision for Objects used in Group Work: Challenges and Opportunities</a></td>
  <td>æå‡ºFiboSBæ•°æ®é›†ä»¥è§£å†³åä½œä»»åŠ¡ä¸­çš„6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.00224v1" data-paper-url="./papers/250700224v1-computer-vision-for-objects-used-in-group-work-challenges-and-opport.html" onclick="toggleFavorite(this, '2507.00224v1', 'Computer Vision for Objects used in Group Work: Challenges and Opportunities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250624074v2-c3vdv2-colonoscopy-3d-video-dataset-with-enhanced-realism.html">C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism</a></td>
  <td>æå‡ºC3VDv2æ•°æ®é›†ä»¥è§£å†³3Dç»“è‚ é•œé‡å»ºç®—æ³•è®­ç»ƒä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24074v2" data-paper-url="./papers/250624074v2-c3vdv2-colonoscopy-3d-video-dataset-with-enhanced-realism.html" onclick="toggleFavorite(this, '2506.24074v2', 'C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250623835v2-scorp-scene-consistent-object-refinement-via-proxy-generation-and-tu.html">SCORP: Scene-Consistent Object Refinement via Proxy Generation and Tuning</a></td>
  <td>æå‡ºSCORPä»¥è§£å†³åœºæ™¯é‡å»ºä¸­å¯¹è±¡è§†è§’ç¼ºå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23835v2" data-paper-url="./papers/250623835v2-scorp-scene-consistent-object-refinement-via-proxy-generation-and-tu.html" onclick="toggleFavorite(this, '2506.23835v2', 'SCORP: Scene-Consistent Object Refinement via Proxy Generation and Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250624044v1-a-survey-on-vision-language-action-models-for-autonomous-driving.html">A Survey on Vision-Language-Action Models for Autonomous Driving</a></td>
  <td>ç»¼è¿°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä»¥æ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å‘å±•</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24044v1" data-paper-url="./papers/250624044v1-a-survey-on-vision-language-action-models-for-autonomous-driving.html" onclick="toggleFavorite(this, '2506.24044v1', 'A Survey on Vision-Language-Action Models for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250623481v1-evaluation-of-geolocation-capabilities-of-multimodal-large-language-.html">Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks</a></td>
  <td>è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åœ°ç†å®šä½èƒ½åŠ›ä»¥åº”å¯¹éšç§é£é™©</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23481v1" data-paper-url="./papers/250623481v1-evaluation-of-geolocation-capabilities-of-multimodal-large-language-.html" onclick="toggleFavorite(this, '2506.23481v1', 'Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250623639v1-unified-multimodal-understanding-via-byte-pair-visual-encoding.html">Unified Multimodal Understanding via Byte-Pair Visual Encoding</a></td>
  <td>æå‡ºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£æ¡†æ¶ä»¥è§£å†³æ¨¡æ€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23639v1" data-paper-url="./papers/250623639v1-unified-multimodal-understanding-via-byte-pair-visual-encoding.html" onclick="toggleFavorite(this, '2506.23639v1', 'Unified Multimodal Understanding via Byte-Pair Visual Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250624039v2-foundation-models-for-zero-shot-segmentation-of-scientific-images-wi.html">Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data</a></td>
  <td>æå‡ºZenesisä»¥è§£å†³ç§‘å­¦å›¾åƒé›¶-shotåˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24039v2" data-paper-url="./papers/250624039v2-foundation-models-for-zero-shot-segmentation-of-scientific-images-wi.html" onclick="toggleFavorite(this, '2506.24039v2', 'Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250624102v1-denseworld-1m-towards-detailed-dense-grounded-caption-in-the-real-wo.html">DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World</a></td>
  <td>æå‡ºDenseWorld-1Mä»¥è§£å†³ç°æœ‰å›¾åƒæè¿°æ•°æ®é›†ç¼ºä¹ç»†èŠ‚çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24102v1" data-paper-url="./papers/250624102v1-denseworld-1m-towards-detailed-dense-grounded-caption-in-the-real-wo.html" onclick="toggleFavorite(this, '2506.24102v1', 'DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250623714v1-towards-an-automated-multimodal-approach-for-video-summarization-bui.html">Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization</a></td>
  <td>æå‡ºä¸€ç§å¤šæ¨¡æ€è§†é¢‘æ‘˜è¦æ–¹æ³•ä»¥æå‡è§†é¢‘å†…å®¹ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23714v1" data-paper-url="./papers/250623714v1-towards-an-automated-multimodal-approach-for-video-summarization-bui.html" onclick="toggleFavorite(this, '2506.23714v1', 'Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250623825v2-flash-vstream-efficient-real-time-understanding-for-long-video-strea.html">Flash-VStream: Efficient Real-Time Understanding for Long Video Streams</a></td>
  <td>æå‡ºFlash-VStreamä»¥è§£å†³é•¿è§†é¢‘ç†è§£çš„æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23825v2" data-paper-url="./papers/250623825v2-flash-vstream-efficient-real-time-understanding-for-long-video-strea.html" onclick="toggleFavorite(this, '2506.23825v2', 'Flash-VStream: Efficient Real-Time Understanding for Long Video Streams')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250623663v1-on-the-domain-robustness-of-contrastive-vision-language-models.html">On the Domain Robustness of Contrastive Vision-Language Models</a></td>
  <td>æå‡ºDeepbenchæ¡†æ¶ä»¥è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„é¢†åŸŸé²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23663v1" data-paper-url="./papers/250623663v1-on-the-domain-robustness-of-contrastive-vision-language-models.html" onclick="toggleFavorite(this, '2506.23663v1', 'On the Domain Robustness of Contrastive Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250623641v1-vap-diffusion-enriching-descriptions-with-mllms-for-enhanced-medical.html">VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation</a></td>
  <td>æå‡ºVAP-Diffusionä»¥è§£å†³åŒ»å­¦å›¾åƒç”Ÿæˆä¸­çš„æè¿°ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23641v1" data-paper-url="./papers/250623641v1-vap-diffusion-enriching-descriptions-with-mllms-for-enhanced-medical.html" onclick="toggleFavorite(this, '2506.23641v1', 'VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250623972v2-learning-frequency-and-memory-aware-prompts-for-multi-modal-object-t.html">Learning Frequency and Memory-Aware Prompts for Multi-Modal Object Tracking</a></td>
  <td>æå‡ºé¢‘ç‡ä¸è®°å¿†æ„ŸçŸ¥æç¤ºä»¥è§£å†³å¤šæ¨¡æ€ç›®æ ‡è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23972v2" data-paper-url="./papers/250623972v2-learning-frequency-and-memory-aware-prompts-for-multi-modal-object-t.html" onclick="toggleFavorite(this, '2506.23972v2', 'Learning Frequency and Memory-Aware Prompts for Multi-Modal Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250623605v1-ai-generated-lecture-slides-for-improving-slide-element-detection-an.html">AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval</a></td>
  <td>æå‡ºSynLecSlideGenä»¥è§£å†³è®²ä¹‰å¹»ç¯ç‰‡å…ƒç´ æ£€æµ‹ä¸æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23605v1" data-paper-url="./papers/250623605v1-ai-generated-lecture-slides-for-improving-slide-element-detection-an.html" onclick="toggleFavorite(this, '2506.23605v1', 'AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/250623918v3-thinking-with-images-for-multimodal-reasoning-foundations-methods-an.html">Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers</a></td>
  <td>æå‡ºä»¥å›¾åƒæ€ç»´æ¨åŠ¨å¤šæ¨¡æ€æ¨ç†çš„æ¡†æ¶ä»¥è§£å†³è¯­ä¹‰å·®è·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23918v3" data-paper-url="./papers/250623918v3-thinking-with-images-for-multimodal-reasoning-foundations-methods-an.html" onclick="toggleFavorite(this, '2506.23918v3', 'Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250624113v1-epona-autoregressive-diffusion-world-model-for-autonomous-driving.html">Epona: Autoregressive Diffusion World Model for Autonomous Driving</a></td>
  <td>æå‡ºEponaä»¥è§£å†³è‡ªä¸»é©¾é©¶ä¸­çš„é•¿æ—¶åºé¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">world model</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24113v1" data-paper-url="./papers/250624113v1-epona-autoregressive-diffusion-world-model-for-autonomous-driving.html" onclick="toggleFavorite(this, '2506.24113v1', 'Epona: Autoregressive Diffusion World Model for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250623606v1-sg-ldm-semantic-guided-lidar-generation-via-latent-aligned-diffusion.html">SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion</a></td>
  <td>æå‡ºSG-LDMä»¥è§£å†³æ¿€å…‰é›·è¾¾ç‚¹äº‘ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23606v1" data-paper-url="./papers/250623606v1-sg-ldm-semantic-guided-lidar-generation-via-latent-aligned-diffusion.html" onclick="toggleFavorite(this, '2506.23606v1', 'SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/250624086v3-motiongpt3-human-motion-as-a-second-modality.html">MotionGPT3: Human Motion as a Second Modality</a></td>
  <td>æå‡ºMotionGPT3ä»¥è§£å†³å¤šæ¨¡æ€è¿åŠ¨ç†è§£ä¸ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span> <span class="paper-tag">MotionGPT</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24086v3" data-paper-url="./papers/250624086v3-motiongpt3-human-motion-as-a-second-modality.html" onclick="toggleFavorite(this, '2506.24086v3', 'MotionGPT3: Human Motion as a Second Modality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/250623676v1-a-unified-framework-for-stealthy-adversarial-generation-via-latent-o.html">A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement</a></td>
  <td>æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥è§£å†³æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ ·æœ¬ç”Ÿæˆçš„è½¬ç§»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">latent optimization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23676v1" data-paper-url="./papers/250623676v1-a-unified-framework-for-stealthy-adversarial-generation-via-latent-o.html" onclick="toggleFavorite(this, '2506.23676v1', 'A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/250624019v1-ella-embodied-social-agents-with-lifelong-memory.html">Ella: Embodied Social Agents with Lifelong Memory</a></td>
  <td>æå‡ºEllaä»¥è§£å†³ç¤¾äº¤æ™ºèƒ½ä½“çš„ç»ˆèº«å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24019v1" data-paper-url="./papers/250624019v1-ella-embodied-social-agents-with-lifelong-memory.html" onclick="toggleFavorite(this, '2506.24019v1', 'Ella: Embodied Social Agents with Lifelong Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>43</td>
  <td><a href="./papers/250623852v2-rgc-vqa-an-exploration-database-for-robotic-generated-video-quality-.html">RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment</a></td>
  <td>æå‡ºRGC-VQAä»¥è§£å†³æœºå™¨äººç”Ÿæˆè§†é¢‘è´¨é‡è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23852v2" data-paper-url="./papers/250623852v2-rgc-vqa-an-exploration-database-for-robotic-generated-video-quality-.html" onclick="toggleFavorite(this, '2506.23852v2', 'RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)