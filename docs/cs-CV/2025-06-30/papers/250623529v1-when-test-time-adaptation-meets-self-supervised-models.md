---
layout: default
title: When Test-Time Adaptation Meets Self-Supervised Models
---

# When Test-Time Adaptation Meets Self-Supervised Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23529" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23529v1</a>
  <a href="https://arxiv.org/pdf/2506.23529.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23529v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23529v1', 'When Test-Time Adaptation Meets Self-Supervised Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jisu Han, Jihee Park, Dongyoon Han, Wonjun Hwang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 15 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£æµ‹è¯•æ—¶é€‚åº”åè®®ä»¥æå‡æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `æµ‹è¯•æ—¶é€‚åº”` `å¯¹æ¯”å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `æ·±åº¦å­¦ä¹ ` `åŠ¨æ€ç¯å¢ƒ` `æ¨¡å‹é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•åœ¨ç›´æ¥åº”ç”¨äºè‡ªç›‘ç£æ¨¡å‹æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æºé¢†åŸŸå‡†ç¡®ç‡è¾ƒä½çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£æµ‹è¯•æ—¶é€‚åº”åè®®ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåä½œå­¦ä¹ æ¡†æ¶ï¼Œç»“åˆè‡ªç›‘ç£å­¦ä¹ å’Œæµ‹è¯•æ—¶é€‚åº”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè‡ªç›‘ç£æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨æ²¡æœ‰æºé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ä¾ç„¶èƒ½å¤Ÿå®ç°ç«äº‰æ€§æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æµ‹è¯•æ—¶æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒä½¿æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿé€‚åº”åŠ¨æ€ç¯å¢ƒå˜åŒ–ï¼Œä»è€Œå¢å¼ºå…¶å®ç”¨æ€§ã€‚å°½ç®¡æºåˆ°ç›®æ ‡é¢†åŸŸçš„åœ¨çº¿é€‚åº”å…·æœ‰æ½œåŠ›ï¼Œä½†ä»ç„¶é«˜åº¦ä¾èµ–äºæºé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•æ˜¯å¦èƒ½å¤Ÿåœ¨ä¸ä¾èµ–æºé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒæŒç»­æ”¹è¿›é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨è§‚å¯Ÿåˆ°ç°æœ‰TTAæ–¹æ³•åœ¨ç›´æ¥åº”ç”¨äºæºé¢†åŸŸå‡†ç¡®ç‡ä½çš„è‡ªç›‘ç£æ¨¡å‹æ—¶é‡åˆ°å›°éš¾åï¼Œæå‡ºäº†ä¸€ç§è‡ªç›‘ç£TTAåè®®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œå­¦ä¹ æ¡†æ¶ï¼Œæ•´åˆäº†SSLå’ŒTTAæ¨¡å‹ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦è¿›è¡Œé€æ­¥è¡¨ç¤ºç²¾ç‚¼ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚DINOã€MoCoå’ŒiBOTï¼‰ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œåœ¨TTAåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œè¯æ˜å³ä½¿æ²¡æœ‰æºé¢„è®­ç»ƒï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½å®ç°ç«äº‰æ€§æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨æµ‹è¯•æ—¶é€‚åº”è¿‡ç¨‹ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æºé¢†åŸŸå‡†ç¡®ç‡ä½æ—¶éš¾ä»¥æœ‰æ•ˆåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£æµ‹è¯•æ—¶é€‚åº”åè®®ï¼Œæ—¨åœ¨é€šè¿‡ä¸ä¾èµ–æºé¢„è®­ç»ƒçš„æ–¹å¼ï¼ŒæŒç»­æ”¹è¿›è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ä¸æµ‹è¯•æ—¶é€‚åº”æ¨¡å—çš„åä½œï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦è¿›è¡Œé€æ­¥çš„ç‰¹å¾è¡¨ç¤ºç²¾ç‚¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªç›‘ç£TTAåè®®ï¼Œè§£å†³äº†ç°æœ‰TTAæ–¹æ³•åœ¨è‡ªç›‘ç£æ¨¡å‹åº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºç‰¹å¾è¡¨ç¤ºçš„åŒºåˆ†æ€§ï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦æ¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªè‡ªç›‘ç£æ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨DINOæ¨¡å‹ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼Œåœ¨MoCoå’ŒiBOTæ¨¡å‹ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œå‡è¶…è¿‡äº†ç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½ç›‘æ§ç­‰åŠ¨æ€ç¯å¢ƒä¸‹çš„ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å˜åŒ–ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜è¿™äº›é¢†åŸŸçš„å®é™…åº”ç”¨æ•ˆæœå’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training on test-time data enables deep learning models to adapt to dynamic environmental changes, enhancing their practical applicability. Online adaptation from source to target domains is promising but it remains highly reliant on the performance of source pretrained model. In this paper, we investigate whether test-time adaptation (TTA) methods can continuously improve models trained via self-supervised learning (SSL) without relying on source pretraining. We introduce a self-supervised TTA protocol after observing that existing TTA approaches struggle when directly applied to self-supervised models with low accuracy on the source domain. Furthermore, we propose a collaborative learning framework that integrates SSL and TTA models, leveraging contrastive learning and knowledge distillation for stepwise representation refinement. We validate our method on diverse self-supervised models, including DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the effectiveness of our approach in SSL, showing that it achieves competitive performance even without source pretraining.

