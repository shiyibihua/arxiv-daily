---
layout: default
title: From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection
---

# From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23519" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23519v1</a>
  <a href="https://arxiv.org/pdf/2506.23519.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23519v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23519v1', 'From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Qin, Runmin Cong, Gen Zhan, Yiting Liao, Sam Kwong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 15 Pages, 9 Figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçœ¼åŠ¨è¿½è¸ªçš„å¼±ç›‘ç£è§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è§†é¢‘æ˜¾è‘—æ€§æ£€æµ‹` `çœ¼åŠ¨è¿½è¸ª` `å¼±ç›‘ç£å­¦ä¹ ` `ç‰¹å¾å­¦ä¹ ` `æ—¶ç©ºå»ºæ¨¡` `å¯¹æ¯”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¼±ç›‘ç£æ¡ä»¶ä¸‹å¯¹è§†é¢‘æ˜¾è‘—ç›®æ ‡çš„æ£€æµ‹æ•ˆæœæœ‰é™ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ æœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä½ç½®å’Œè¯­ä¹‰åµŒå…¥æ¨¡å—ï¼ˆPSEï¼‰ä»¥åŠè¯­ä¹‰å’Œå±€éƒ¨æŸ¥è¯¢ï¼ˆSLQï¼‰ç«äº‰è€…ï¼Œä»¥å¢å¼ºç‰¹å¾å­¦ä¹ å’Œæ—¶ç©ºå»ºæ¨¡èƒ½åŠ›ã€‚
3. åœ¨äº”ä¸ªVSODåŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çœ¼åŠ¨è¿½è¸ªè§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹ï¼ˆVSPï¼‰ä»»åŠ¡ä¸è§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆVSODï¼‰ä»»åŠ¡å‡å…³æ³¨è§†é¢‘ä¸­æœ€å¸å¼•äººçš„å¯¹è±¡ï¼Œå¹¶ä»¥é¢„æµ‹çƒ­å›¾å’Œåƒç´ çº§æ˜¾è‘—æ€§æ©ç çš„å½¢å¼å‘ˆç°ç»“æœã€‚ç”±äºçœ¼åŠ¨è¿½è¸ªæ ‡æ³¨æ›´æ˜“è·å–ä¸”ä¸äººçœ¼çš„çœŸå®è§†è§‰æ¨¡å¼ç´§å¯†å¯¹é½ï¼Œæœ¬æ–‡æ—¨åœ¨å¼•å…¥æ³¨è§†ä¿¡æ¯ä»¥è¾…åŠ©åœ¨å¼±ç›‘ç£ä¸‹æ£€æµ‹è§†é¢‘æ˜¾è‘—ç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†ä½ç½®å’Œè¯­ä¹‰åµŒå…¥ï¼ˆPSEï¼‰æ¨¡å—ï¼Œä»¥åœ¨ç‰¹å¾å­¦ä¹ è¿‡ç¨‹ä¸­æä¾›ä½ç½®å’Œè¯­ä¹‰æŒ‡å¯¼ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†è¯­ä¹‰å’Œå±€éƒ¨æŸ¥è¯¢ï¼ˆSLQï¼‰ç«äº‰è€…ï¼Œä»¥æœ‰æ•ˆé€‰æ‹©æœ€åŒ¹é…çš„å¯¹è±¡æŸ¥è¯¢è¿›è¡Œæ—¶ç©ºå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨äº†å†…å¤–æ··åˆå¯¹æ¯”ï¼ˆIIMCï¼‰æ¨¡å‹ï¼Œé€šè¿‡å½¢æˆå†…è§†é¢‘å’Œå¤–è§†é¢‘çš„å¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œæå‡äº†å¼±ç›‘ç£ä¸‹çš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨äº”ä¸ªæµè¡Œçš„VSODåŸºå‡†ä¸Šè¶…è¶Šäº†å…¶ä»–ç«äº‰è€…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼±ç›‘ç£æ¡ä»¶ä¸‹è¿›è¡Œè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥çœ¼åŠ¨è¿½è¸ªçš„æ³¨è§†ä¿¡æ¯ï¼Œåˆ©ç”¨ä½ç½®å’Œè¯­ä¹‰åµŒå…¥æ¨¡å—ï¼ˆPSEï¼‰æ¥æŒ‡å¯¼ç‰¹å¾å­¦ä¹ ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹æ˜¾è‘—ç›®æ ‡çš„æ£€æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä½ç½®å’Œè¯­ä¹‰åµŒå…¥æ¨¡å—ï¼ˆPSEï¼‰ç”¨äºç‰¹å¾å­¦ä¹ ï¼Œè¯­ä¹‰å’Œå±€éƒ¨æŸ¥è¯¢ï¼ˆSLQï¼‰ç«äº‰è€…ç”¨äºæ—¶ç©ºç‰¹å¾å»ºæ¨¡ã€‚æ¨¡å‹é€šè¿‡å†…å¤–æ··åˆå¯¹æ¯”ï¼ˆIIMCï¼‰æœºåˆ¶è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆçœ¼åŠ¨è¿½è¸ªä¿¡æ¯ä¸å¼±ç›‘ç£å­¦ä¹ ï¼Œæå‡ºäº†PSEå’ŒSLQæ¨¡å—ï¼Œä½¿å¾—æ¨¡å‹åœ¨ç‰¹å¾é€‰æ‹©å’Œå¯¹æ¯”å­¦ä¹ ä¸Šæ›´å…·é’ˆå¯¹æ€§ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä½ç½®å’Œè¯­ä¹‰ä¿¡æ¯çš„å½±å“ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”æ—¶ç©ºç‰¹å¾çš„å­¦ä¹ éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨äº”ä¸ªä¸»æµVSODåŸºå‡†ä¸Šå‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå…·ä½“åœ¨æŸäº›æŒ‡æ ‡ä¸Šæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€å¹¿å‘Šåˆ†æå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆè¯†åˆ«è§†é¢‘ä¸­çš„æ˜¾è‘—ç›®æ ‡ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The eye-tracking video saliency prediction (VSP) task and video salient object detection (VSOD) task both focus on the most attractive objects in video and show the result in the form of predictive heatmaps and pixel-level saliency masks, respectively. In practical applications, eye tracker annotations are more readily obtainable and align closely with the authentic visual patterns of human eyes. Therefore, this paper aims to introduce fixation information to assist the detection of video salient objects under weak supervision. On the one hand, we ponder how to better explore and utilize the information provided by fixation, and then propose a Position and Semantic Embedding (PSE) module to provide location and semantic guidance during the feature learning process. On the other hand, we achieve spatiotemporal feature modeling under weak supervision from the aspects of feature selection and feature contrast. A Semantics and Locality Query (SLQ) Competitor with semantic and locality constraints is designed to effectively select the most matching and accurate object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed Contrastive (IIMC) model improves the spatiotemporal modeling capabilities under weak supervision by forming an intra-video and inter-video contrastive learning paradigm. Experimental results on five popular VSOD benchmarks indicate that our model outperforms other competitors on various evaluation metrics.

