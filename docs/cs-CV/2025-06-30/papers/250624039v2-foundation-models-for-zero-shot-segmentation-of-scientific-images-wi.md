---
layout: default
title: Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data
---

# Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.24039" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.24039v2</a>
  <a href="https://arxiv.org/pdf/2506.24039.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.24039v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.24039v2', 'Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shubhabrata Mukherjee, Jack Lang, Obeen Kwon, Iryna Zenyuk, Valerie Brogden, Adam Weber, Daniela Ushizima

**åˆ†ç±»**: cs.CV, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-08-17)

**å¤‡æ³¨**: This paper has been accepted for presentation at the 59th International Conference on Parallel Processing (ICPP 2025), DRAI workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZenesisä»¥è§£å†³ç§‘å­¦å›¾åƒé›¶-shotåˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é›¶-shotåˆ†å‰²` `ç§‘å­¦å›¾åƒ` `è®¡ç®—æœºè§†è§‰` `å¤šæ¨¡æ€é€‚åº”` `äººæœºåä½œ` `å›¾åƒåˆ†æ` `æ•°æ®å‡†å¤‡` `FIB-SEM`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶-shotæ¨¡å‹åœ¨å¤„ç†ç§‘å­¦å›¾åƒæ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸç‰¹å®šæŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. Zenesisé€šè¿‡æ— ä»£ç å¹³å°å’Œå¤šæ¨¡æ€é€‚åº”æŠ€æœ¯ï¼Œå®ç°äº†å¯¹åŸå§‹ç§‘å­¦æ•°æ®çš„é›¶-shotæ¨ç†ï¼Œæå‡äº†å›¾åƒåˆ†å‰²æ•ˆç‡ã€‚
3. åœ¨FIB-SEMæ•°æ®é›†ä¸Šï¼ŒZenesisçš„å‡†ç¡®ç‡è¾¾åˆ°0.987ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é›¶-shotå’ŒåŸºäºæç¤ºçš„æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç¨€ç–å’Œç‰¹å®šé¢†åŸŸçš„ç§‘å­¦å›¾åƒæ•°æ®ä¸Šå¸¸å¸¸å¤±è´¥ã€‚æˆ‘ä»¬æå‡ºäº†Zenesisï¼Œä¸€ä¸ªæ— ä»£ç çš„äº¤äº’å¼è®¡ç®—æœºè§†è§‰å¹³å°ï¼Œæ—¨åœ¨å‡å°‘ç§‘å­¦æˆåƒå·¥ä½œæµç¨‹ä¸­çš„æ•°æ®å‡†å¤‡ç“¶é¢ˆã€‚Zenesisé›†æˆäº†è½»é‡çº§çš„å¤šæ¨¡æ€é€‚åº”ï¼Œç”¨äºåŸå§‹ç§‘å­¦æ•°æ®çš„é›¶-shotæ¨ç†ã€äººæœºåä½œçš„ç²¾ç»†åŒ–å¤„ç†å’ŒåŸºäºå¯å‘å¼çš„æ—¶é—´å¢å¼ºã€‚æˆ‘ä»¬åœ¨å‚¬åŒ–å‰‚è´Ÿè½½è†œçš„èšç„¦ç¦»å­æŸæ‰«æç”µå­æ˜¾å¾®é•œï¼ˆFIB-SEMï¼‰æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒZenesisçš„è¡¨ç°è¶…è¶Šäº†åŸºçº¿ï¼Œå–å¾—äº†0.947çš„å¹³å‡å‡†ç¡®ç‡ã€0.858çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’Œ0.923çš„Diceç³»æ•°ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç§‘å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ•°æ®å‡†å¤‡ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¨€ç–å’Œç‰¹å®šé¢†åŸŸæ•°æ®æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆè¿›è¡Œå›¾åƒåˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºZenesiså¹³å°ï¼Œé€šè¿‡æ— ä»£ç äº¤äº’å’Œå¤šæ¨¡æ€é€‚åº”æŠ€æœ¯ï¼Œå®ç°å¯¹åŸå§‹ç§‘å­¦æ•°æ®çš„é›¶-shotæ¨ç†ï¼Œé™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šZenesisçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€é›¶-shotæ¨ç†æ¨¡å—ã€äººæœºåä½œç²¾ç»†åŒ–æ¨¡å—å’Œæ—¶é—´å¢å¼ºæ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„ç§‘å­¦æˆåƒå·¥ä½œæµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šZenesisçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½»é‡çº§çš„å¤šæ¨¡æ€é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢„å…ˆæ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œæœ‰æ•ˆçš„å›¾åƒåˆ†å‰²ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å¤„ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒZenesisé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶ç»“åˆå¯å‘å¼æ–¹æ³•è¿›è¡Œæ—¶é—´å¢å¼ºï¼Œç¡®ä¿åœ¨ä¸åŒç±»å‹çš„ç§‘å­¦å›¾åƒä¸Šå‡èƒ½ä¿æŒé«˜æ•ˆè¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Zenesisåœ¨FIB-SEMæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„å®éªŒç»“æœï¼Œå‚¬åŒ–å‰‚æ ·æœ¬çš„å¹³å‡å‡†ç¡®ç‡ä¸º0.947ï¼ŒIoUä¸º0.858ï¼ŒDiceç³»æ•°ä¸º0.923ï¼Œè€Œæ™¶ä½“æ ·æœ¬çš„å‡†ç¡®ç‡æ›´æ˜¯è¾¾åˆ°0.987ã€‚è¿™äº›ç»“æœè¿œè¶…ä¼ ç»Ÿçš„Otsué˜ˆå€¼æ³•å’Œç°æœ‰çš„Segment Anything Model (SAM)ï¼Œå±•ç¤ºäº†å…¶åœ¨ç§‘å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Zenesisçš„ç ”ç©¶æˆæœåœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ææ–™ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’ŒåŒ–å­¦ç­‰é¢†åŸŸã€‚å®ƒèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿè¿›è¡Œå›¾åƒåˆ†æå’Œæ•°æ®æŒ–æ˜ï¼Œä»è€ŒåŠ é€Ÿç§‘å­¦å‘ç°çš„è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Zero-shot and prompt-based models have excelled at visual reasoning tasks by leveraging large-scale natural image corpora, but they often fail on sparse and domain-specific scientific image data. We introduce Zenesis, a no-code interactive computer vision platform designed to reduce data readiness bottlenecks in scientific imaging workflows. Zenesis integrates lightweight multimodal adaptation for zero-shot inference on raw scientific data, human-in-the-loop refinement, and heuristic-based temporal enhancement. We validate our approach on Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) datasets of catalyst-loaded membranes. Zenesis outperforms baselines, achieving an average accuracy of 0.947, Intersection over Union (IoU) of 0.858, and Dice score of 0.923 on amorphous catalyst samples; and 0.987 accuracy, 0.857 IoU, and 0.923 Dice on crystalline samples. These results represent a significant performance gain over conventional methods such as Otsu thresholding and standalone models like the Segment Anything Model (SAM). Zenesis enables effective image segmentation in domains where annotated datasets are limited, offering a scalable solution for scientific discovery.

