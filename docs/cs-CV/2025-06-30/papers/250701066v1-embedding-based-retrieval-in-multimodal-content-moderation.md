---
layout: default
title: Embedding-based Retrieval in Multimodal Content Moderation
---

# Embedding-based Retrieval in Multimodal Content Moderation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.01066" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.01066v1</a>
  <a href="https://arxiv.org/pdf/2507.01066.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.01066v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.01066v1', 'Embedding-based Retrieval in Multimodal Content Moderation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanzhong Liang, Jinghao Shi, Xiang Shen, Zixuan Wang, Vera Wen, Ardalan Mehrani, Zhiqian Chen, Yifan Wu, Zhixin Zhang

**åˆ†ç±»**: cs.IR, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: Camera ready for SIGIR 2025

**DOI**: [10.1145/3726302.3731945](https://doi.org/10.1145/3726302.3731945)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåµŒå…¥å¼æ£€ç´¢æ–¹æ³•ä»¥è§£å†³çŸ­è§†é¢‘å†…å®¹å®¡æ ¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ­è§†é¢‘å®¡æ ¸` `åµŒå…¥å¼æ£€ç´¢` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€æ¨¡å‹` `å†…å®¹ç®¡ç†` `æ•ˆç‡æå‡` `æˆæœ¬é™ä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å†…å®¹å®¡æ ¸æ–¹æ³•ä¸»è¦ä¾èµ–åˆ†ç±»ï¼Œä½†åœ¨å¿«é€Ÿå“åº”å’Œæˆæœ¬æ§åˆ¶æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶‹åŠ¿é€‚åº”å’Œç´§æ€¥å‡çº§åœºæ™¯ä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åµŒå…¥å¼æ£€ç´¢ï¼ˆEBRï¼‰æ–¹æ³•ï¼Œç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒå¤šç§åµŒå…¥æ¨¡å‹ï¼Œä»¥æé«˜å†…å®¹å®¡æ ¸çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEBRåœ¨25ä¸ªæ–°å…´è¶‹åŠ¿çš„ç¦»çº¿å®éªŒä¸­ï¼ŒROC-AUCä»0.85æå‡è‡³0.99ï¼ŒPR-AUCä»0.35æå‡è‡³0.95ï¼Œåœ¨çº¿å®éªŒä¸­è¡ŒåŠ¨ç‡æé«˜10.32%ï¼Œè¿è¥æˆæœ¬é™ä½è¶…è¿‡80%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘ç†è§£åœ¨çŸ­è§†é¢‘å¹³å°çš„å†…å®¹å®¡æ ¸ä¸­èµ·ç€åŸºç¡€æ€§ä½œç”¨ï¼Œèƒ½å¤Ÿæ£€æµ‹ä¸å½“å†…å®¹ã€‚å°½ç®¡åˆ†ç±»æ–¹æ³•ä»æ˜¯ä¸»æµï¼Œä½†åœ¨å¿«é€Ÿå“åº”å’Œæˆæœ¬æ•ˆç›Šè¦æ±‚é«˜çš„åœºæ™¯ä¸­å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åµŒå…¥å¼æ£€ç´¢ï¼ˆEBRï¼‰æ–¹æ³•ï¼Œä»¥è¡¥å……ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶è®­ç»ƒäº†ä¸€ç³»åˆ—åŸºç¡€åµŒå…¥æ¨¡å‹ï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ¶æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒEBRåœ¨å¤„ç†æ–°å…´è¶‹åŠ¿æ—¶æ˜¾è‘—æå‡äº†ROC-AUCå’ŒPR-AUCï¼Œå¹¶åœ¨åœ¨çº¿å®éªŒä¸­æé«˜äº†è¡ŒåŠ¨ç‡å’Œé™ä½äº†è¿è¥æˆæœ¬ï¼ŒåŒæ—¶å¢å¼ºäº†è§£é‡Šæ€§å’Œçµæ´»æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä¸­ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•åœ¨å¿«é€Ÿå“åº”å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ–°å…´è¶‹åŠ¿æ—¶çš„æ•ˆç‡é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåµŒå…¥å¼æ£€ç´¢ï¼ˆEBRï¼‰æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒåµŒå…¥æ¨¡å‹æ¥å®ç°é«˜æ•ˆçš„è§†é¢‘æ£€ç´¢ï¼Œæ—¨åœ¨è¡¥å……ä¼ ç»Ÿçš„åˆ†ç±»æ–¹æ³•ï¼Œæå‡å®¡æ ¸çš„çµæ´»æ€§å’Œå“åº”é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºç›‘ç£å¯¹æ¯”å­¦ä¹ çš„åµŒå…¥æ¨¡å‹è®­ç»ƒï¼Œå…¶æ¬¡æ˜¯åµŒå…¥ç”Ÿæˆä¸è§†é¢‘æ£€ç´¢çš„æ•´åˆï¼Œå½¢æˆé«˜æ•ˆçš„æ£€ç´¢ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åµŒå…¥å¼æ£€ç´¢æ–¹æ³•ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶è®­ç»ƒçš„åµŒå…¥æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚CLIPå’ŒMoCoï¼‰ï¼Œåœ¨å¤„ç†æ–°å…´è¶‹åŠ¿æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–åµŒå…¥çš„ç”Ÿæˆæ•ˆæœï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEBRæ–¹æ³•åœ¨ç¦»çº¿å®éªŒä¸­å°†ROC-AUCä»0.85æå‡è‡³0.99ï¼ŒPR-AUCä»0.35æå‡è‡³0.95ã€‚åœ¨åœ¨çº¿å®éªŒä¸­ï¼Œè¡ŒåŠ¨ç‡æé«˜äº†10.32%ï¼Œè¿è¥æˆæœ¬é™ä½è¶…è¿‡80%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡å’Œç»æµæ•ˆç›Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬çŸ­è§†é¢‘å¹³å°çš„å†…å®¹å®¡æ ¸ã€ç¤¾äº¤åª’ä½“å†…å®¹ç›‘æ§ä»¥åŠåœ¨çº¿æ•™è‚²è§†é¢‘çš„å†…å®¹ç®¡ç†ç­‰ã€‚é€šè¿‡æé«˜å®¡æ ¸æ•ˆç‡å’Œé™ä½æˆæœ¬ï¼ŒEBRæ–¹æ³•èƒ½å¤Ÿä¸ºå†…å®¹å¹³å°æä¾›æ›´çµæ´»çš„åº”å¯¹ç­–ç•¥ï¼Œæœªæ¥å¯èƒ½åœ¨æ›´å¹¿æ³›çš„å¤šæ¨¡æ€å†…å®¹ç®¡ç†ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video understanding plays a fundamental role for content moderation on short video platforms, enabling the detection of inappropriate content. While classification remains the dominant approach for content moderation, it often struggles in scenarios requiring rapid and cost-efficient responses, such as trend adaptation and urgent escalations. To address this issue, we introduce an Embedding-Based Retrieval (EBR) method designed to complement traditional classification approaches. We first leverage a Supervised Contrastive Learning (SCL) framework to train a suite of foundation embedding models, including both single-modal and multi-modal architectures. Our models demonstrate superior performance over established contrastive learning methods such as CLIP and MoCo. Building on these embedding models, we design and implement the embedding-based retrieval system that integrates embedding generation and video retrieval to enable efficient and effective trend handling. Comprehensive offline experiments on 25 diverse emerging trends show that EBR improves ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online experiments reveal that EBR increases action rates by 10.32% and reduces operational costs by over 80%, while also enhancing interpretability and flexibility compared to classification-based solutions.

