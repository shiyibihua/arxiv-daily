---
layout: default
title: Computer Vision for Objects used in Group Work: Challenges and Opportunities
---

# Computer Vision for Objects used in Group Work: Challenges and Opportunities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00224" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00224v1</a>
  <a href="https://arxiv.org/pdf/2507.00224.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00224v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00224v1', 'Computer Vision for Objects used in Group Work: Challenges and Opportunities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changsoo Jung, Sheikh Mannan, Jack Fitzgerald, Nathaniel Blanchard

**åˆ†ç±»**: cs.CV, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: Accepted to AIED 2025 Late Breaking Results Track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFiboSBæ•°æ®é›†ä»¥è§£å†³åä½œä»»åŠ¡ä¸­çš„6Då§¿æ€ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `6Då§¿æ€ä¼°è®¡` `åä½œä»»åŠ¡` `æ•°æ®é›†æ„å»º` `YOLO11-x` `æ•™è‚²æŠ€æœ¯` `ç‰©ä½“æ£€æµ‹` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç³»ç»Ÿåœ¨åä½œä»»åŠ¡ä¸­æ— æ³•å‡†ç¡®æ•æ‰å­¦ç”Ÿä¸ç‰©ç†å¯¹è±¡çš„äº’åŠ¨ï¼Œå¯¼è‡´6Då§¿æ€ä¼°è®¡çš„å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºFiboSBæ•°æ®é›†ï¼Œä¸“æ³¨äºä¸‰äººå°ç»„åœ¨äº’åŠ¨ä»»åŠ¡ä¸­çš„6Då§¿æ€ä¼°è®¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. é€šè¿‡å¯¹å››ç§6Då§¿æ€ä¼°è®¡æ–¹æ³•çš„è¯„ä¼°ï¼Œå‘ç°YOLO11-xçš„å¾®è°ƒæ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ï¼ŒmAP_50è¾¾åˆ°äº†0.898ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº¤äº’å¼å’Œç©ºé—´æ„ŸçŸ¥æŠ€æœ¯æ­£åœ¨æ”¹å˜æ•™è‚²æ¡†æ¶ï¼Œå°¤å…¶æ˜¯åœ¨K-12ç¯å¢ƒä¸­ï¼ŒåŠ¨æ‰‹æ¢ç´¢ä¿ƒè¿›äº†æ›´æ·±çš„æ¦‚å¿µç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰ç³»ç»Ÿåœ¨æ•æ‰å­¦ç”Ÿä¸ç‰©ç†å¯¹è±¡ä¹‹é—´çš„çœŸå®äº’åŠ¨æ—¶å¸¸å¸¸å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨6Då§¿æ€ä¼°è®¡çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»RGBå›¾åƒæˆ–è§†é¢‘ä¸­ä¼°è®¡ç‰©ä½“åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„ä½ç½®å’Œæ–¹å‘ã€‚æˆ‘ä»¬å¼•å…¥äº†FiboSBï¼Œä¸€ä¸ªæ–°çš„6Då§¿æ€è§†é¢‘æ•°æ®é›†ï¼Œè®°å½•äº†ä¸‰åå‚ä¸è€…åœ¨è§£å†³äº’åŠ¨ä»»åŠ¡æ—¶çš„è¡¨ç°ï¼Œé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹å››ç§æœ€å…ˆè¿›çš„6Då§¿æ€ä¼°è®¡æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œæ­ç¤ºäº†å½“å‰ç®—æ³•åœ¨åä½œä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶é€šè¿‡å¾®è°ƒYOLO11-xæ¨¡å‹ï¼Œè¾¾åˆ°äº†0.898çš„mAP_50ï¼Œå¥ å®šäº†åœ¨å¤æ‚åä½œç¯å¢ƒä¸­åˆ©ç”¨6Då§¿æ€ä¼°è®¡çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨åä½œä»»åŠ¡ä¸­å‡†ç¡®ä¼°è®¡ç‰©ä½“çš„6Då§¿æ€çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å°å‹ç‰©ä½“ä¸å¤šä¸ªå‚ä¸è€…çš„äº’åŠ¨æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨è¿œè·ç¦»è®°å½•æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥FiboSBæ•°æ®é›†ï¼Œè®ºæ–‡æä¾›äº†ä¸€ä¸ªæ–°çš„æŒ‘æˆ˜åœºæ™¯ï¼Œåˆ©ç”¨è‡ªåŠ¨6Då§¿æ€ä¼°è®¡æ¥æ”¹å–„ç‰©ä½“ä¸å®ä½“ä¹‹é—´çš„å…³ç³»ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€6Då§¿æ€ä¼°è®¡æ–¹æ³•çš„è¯„ä¼°ä»¥åŠYOLO11-xæ¨¡å‹çš„å¾®è°ƒã€‚æ•°æ®é›†è®°å½•äº†ä¸‰åå‚ä¸è€…çš„äº’åŠ¨ï¼Œæ¶µç›–äº†å°å‹æ‰‹æŒç«‹æ–¹ä½“å’Œç§°é‡è®¾å¤‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFiboSBæ•°æ®é›†çš„å¼•å…¥æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ä»¥è¯„ä¼°6Då§¿æ€ä¼°è®¡åœ¨å¤æ‚åä½œç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFiboSBçš„è®¾è®¡è€ƒè™‘äº†å°ç‰©ä½“å’Œå¤šå‚ä¸è€…çš„äº’åŠ¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨YOLO11-xçš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–å…¶åœ¨FiboSBæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œæœ€ç»ˆå®ç°äº†0.898çš„mAP_50ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„YOLO11-xåœ¨FiboSBæ•°æ®é›†ä¸Šçš„mAP_50è¾¾åˆ°äº†0.898ï¼Œæ˜¾è‘—é«˜äºæœªå¾®è°ƒæ¨¡å‹çš„è¡¨ç°ã€‚è¿™ä¸€ç»“æœä¸ä»…æ­ç¤ºäº†å½“å‰6Då§¿æ€ä¼°è®¡æ–¹æ³•çš„å±€é™æ€§ï¼Œä¹Ÿä¸ºæœªæ¥åœ¨å¤æ‚åä½œç¯å¢ƒä¸­åº”ç”¨6Då§¿æ€ä¼°è®¡å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æœºå™¨äººåä½œå’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡å‡†ç¡®çš„6Då§¿æ€ä¼°è®¡ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ”¯æŒå­¦ç”Ÿåœ¨åä½œä»»åŠ¡ä¸­çš„äº’åŠ¨ï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯ä¹Ÿå¯åº”ç”¨äºå…¶ä»–éœ€è¦ç²¾ç¡®ç‰©ä½“è¯†åˆ«å’Œå®šä½çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¶å±…å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Interactive and spatially aware technologies are transforming educational frameworks, particularly in K-12 settings where hands-on exploration fosters deeper conceptual understanding. However, during collaborative tasks, existing systems often lack the ability to accurately capture real-world interactions between students and physical objects. This issue could be addressed with automatic 6D pose estimation, i.e., estimation of an object's position and orientation in 3D space from RGB images or videos. For collaborative groups that interact with physical objects, 6D pose estimates allow AI systems to relate objects and entities. As part of this work, we introduce FiboSB, a novel and challenging 6D pose video dataset featuring groups of three participants solving an interactive task featuring small hand-held cubes and a weight scale. This setup poses unique challenges for 6D pose because groups are holistically recorded from a distance in order to capture all participants -- this, coupled with the small size of the cubes, makes 6D pose estimation inherently non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. An error analysis of these methods reveals that the 6D pose methods' object detection modules fail. We address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results, and analysis of YOLO11-x errors presented here lay the groundwork for leveraging the estimation of 6D poses in difficult collaborative contexts.

