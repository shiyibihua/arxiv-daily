---
layout: default
title: LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching
---

# LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23502" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23502v2</a>
  <a href="https://arxiv.org/pdf/2506.23502.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23502v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23502v2', 'LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengxiao Tian, Xinxiao Wu, Shuo Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-07-12)

**å¤‡æ³¨**: accepted by ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMå¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€æç¤ºè°ƒä¼˜ä»¥è§£å†³å›¾åƒ-æ–‡æœ¬åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒ-æ–‡æœ¬åŒ¹é…` `å¤šæ¨¡æ€å­¦ä¹ ` `åŠ¨ä½œæ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `æç¤ºè°ƒä¼˜` `è§†è§‰è¡¨ç¤º` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬åŒ¹é…æ–¹æ³•åœ¨ç†è§£ç‰©ä½“å±æ€§å’Œç©ºé—´å…³ç³»ç­‰ç»†ç²’åº¦ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹å¯¹åŠ¨ä½œçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§LLMå¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸åŠ¨ä½œç›¸å…³çš„çŸ¥è¯†æ¥æ”¹å–„CLIPçš„è¡¨ç°ã€‚
3. åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å›¾åƒ-æ–‡æœ¬åŒ¹é…çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è§„æ¨¡å¯¹æ¯”è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å‘å±•ï¼Œå›¾åƒ-æ–‡æœ¬åŒ¹é…ä»»åŠ¡åœ¨è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼ŒCLIPåœ¨ç†è§£ç»†ç²’åº¦ç»†èŠ‚ï¼ˆå¦‚ç‰©ä½“å±æ€§å’Œç‰©ä½“é—´ç©ºé—´å…³ç³»ï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§LLMå¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸åŠ¨ä½œç›¸å…³çš„å¤–éƒ¨çŸ¥è¯†ï¼Œèµ‹äºˆCLIPç»†ç²’åº¦çš„åŠ¨ä½œçº§ç†è§£ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºï¼Œä»¥åˆ©ç”¨LLMä¸­éšå«çš„ç»„åˆè¯­ä¹‰çŸ¥è¯†å’ŒçŠ¶æ€ç›¸å…³å› æœçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæœ‰æ•ˆæå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒ-æ–‡æœ¬åŒ¹é…æ–¹æ³•åœ¨ç»†ç²’åº¦ç†è§£å’ŒåŠ¨ä½œæ„ŸçŸ¥æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯CLIPåœ¨å¤„ç†ç‰©ä½“å±æ€§å’Œç©ºé—´å…³ç³»æ—¶çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ä¸åŠ¨ä½œç›¸å…³çš„çŸ¥è¯†ï¼Œè®¾è®¡åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºï¼Œä»¥å¢å¼ºCLIPçš„åŠ¨ä½œçº§ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨ä½œä¸‰å…ƒç»„æç¤ºã€åŠ¨ä½œçŠ¶æ€æç¤ºå’Œè‡ªé€‚åº”äº¤äº’æ¨¡å—ã€‚è‡ªé€‚åº”äº¤äº’æ¨¡å—ç”¨äºèšåˆåŸºäºåŠ¨ä½œæ„ŸçŸ¥æç¤ºçš„è§†è§‰ç‰¹å¾ï¼Œä»¥å»ºç«‹æ›´å…·è¾¨åˆ«åŠ›çš„è§†è§‰è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LLMç”Ÿæˆçš„çŸ¥è¯†ä¸è§†è§‰ç‰¹å¾ç»“åˆï¼Œå½¢æˆåŠ¨ä½œæ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºï¼Œè¿™ä¸€æ–¹æ³•åœ¨ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬åŒ¹é…æŠ€æœ¯ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åŠ¨ä½œæ„ŸçŸ¥çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”äº¤äº’æ¨¡å—å®ç°è§†è§‰ç‰¹å¾çš„åŠ¨æ€èšåˆï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åŠ¨ä½œä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†å›¾åƒ-æ–‡æœ¬åŒ¹é…çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿå¯¹å¤æ‚åœºæ™¯ä¸­ç‰©ä½“é—´å…³ç³»å’ŒåŠ¨ä½œçš„ç†è§£èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Driven by large-scale contrastive vision-language pre-trained models such as CLIP, recent advancements in the image-text matching task have achieved remarkable success in representation learning. Due to image-level visual-language alignment, CLIP falls short in understanding fine-grained details such as object attributes and spatial relationships between objects. Recent efforts have attempted to compel CLIP to acquire structured visual representations by introducing prompt learning to achieve object-level alignment. While achieving promising results, they still lack the capability to perceive actions, which are crucial for describing the states or relationships between objects. Therefore, we propose to endow CLIP with fine-grained action-level understanding by introducing an LLM-enhanced action-aware multi-modal prompt-tuning method, incorporating the action-related external knowledge generated by large language models (LLMs). Specifically, we design an action triplet prompt and an action state prompt to exploit compositional semantic knowledge and state-related causal knowledge implicitly stored in LLMs. Subsequently, we propose an adaptive interaction module to aggregate attentive visual features conditioned on action-aware prompted knowledge for establishing discriminative and action-aware visual representations, which further improves the performance. Comprehensive experimental results on two benchmark datasets demonstrate the effectiveness of our method.

