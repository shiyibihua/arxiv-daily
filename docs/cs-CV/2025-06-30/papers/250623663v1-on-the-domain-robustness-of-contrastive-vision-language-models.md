---
layout: default
title: On the Domain Robustness of Contrastive Vision-Language Models
---

# On the Domain Robustness of Contrastive Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23663" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23663v1</a>
  <a href="https://arxiv.org/pdf/2506.23663.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23663v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23663v1', 'On the Domain Robustness of Contrastive Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mario Koddenbrock, Rudolf Hoffmann, David Brodmann, Erik Rodner

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: Deepbench is available at https://github.com/ml-lab-htw/deepbench

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepbenchæ¡†æ¶ä»¥è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„é¢†åŸŸé²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `é¢†åŸŸé²æ£’æ€§` `å›¾åƒæŸåç”Ÿæˆ` `æ— ç›‘ç£è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè½¬ç§»æ—¶è¡¨ç°ä¸ç¨³å®šï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°å·¥å…·ã€‚
2. Deepbenchæ¡†æ¶é€šè¿‡ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„å›¾åƒæŸåï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„é¢†åŸŸé²æ£’æ€§ã€‚
3. åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œé¢†åŸŸä¸­è¯„ä¼°å¤šç§æ¨¡å‹ï¼Œå‘ç°é²æ£’æ€§å·®å¼‚æ˜¾è‘—ï¼Œå¼ºè°ƒäº†é¢†åŸŸç‰¹å®šè¯„ä¼°çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°å®ä¸–ç•Œçš„è§†è§‰-è¯­è¨€åº”ç”¨ä¸­ï¼Œå®è·µè€…è¶Šæ¥è¶Šä¾èµ–å¤§å‹é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œè€Œéå®šåˆ¶è§£å†³æ–¹æ¡ˆï¼Œå°½ç®¡å¯¹å…¶è®­ç»ƒæ•°æ®å’Œè¿‡ç¨‹çš„é€æ˜åº¦æœ‰é™ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨é€šç”¨åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸè½¬ç§»æ—¶ï¼Œå…¶æœ‰æ•ˆæ€§å¯èƒ½æ˜¾è‘—ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†Deepbenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹é¢†åŸŸç‰¹å®šé²æ£’æ€§çš„æ¡†æ¶ã€‚Deepbenchåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ç‰¹å®šéƒ¨ç½²é¢†åŸŸçš„çœŸå®ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åƒæŸåï¼Œè€Œæ— éœ€æ ‡è®°æ•°æ®ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å¯¹æ¯”è§†è§‰-è¯­è¨€æ¶æ„åŠå…¶å˜ä½“ï¼Œå‘ç°é²æ£’æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°éœ€æ±‚ã€‚Deepbenchä½œä¸ºå¼€æºè½¯ä»¶å‘å¸ƒï¼Œä»¥æ”¯æŒé¢†åŸŸé²æ£’æ€§è¯„ä¼°çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè½¬ç§»æ—¶é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„é¢†åŸŸè¯„ä¼°å·¥å…·ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºDeepbenchæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆçœŸå®çš„å›¾åƒæŸåï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œè¯„ä¼°ï¼Œé¿å…äº†å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeepbenchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ã€æ¨¡å‹è¯„ä¼°æ¨¡å—å’Œç»“æœåˆ†ææ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åƒæŸåï¼Œæ¨¡å‹è¯„ä¼°æ¨¡å—åˆ™å¯¹ä¸åŒè§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œé²æ£’æ€§æµ‹è¯•ï¼Œæœ€åç»“æœåˆ†ææ¨¡å—å¯¹è¯„ä¼°ç»“æœè¿›è¡Œæ€»ç»“å’Œå¯è§†åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepbenchçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— ç›‘ç£çš„å›¾åƒæŸåç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®ç‰¹å®šé¢†åŸŸçš„éœ€æ±‚ç”Ÿæˆé€‚åº”æ€§å¼ºçš„æµ‹è¯•æ ·æœ¬ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„ä¾èµ–æ ‡è®°æ•°æ®çš„è¯„ä¼°æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Deepbenchä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå›¾åƒæŸåçš„ç­–ç•¥ï¼Œä»¥åŠå¯¹ä¸åŒè§†è§‰-è¯­è¨€æ¶æ„çš„ç³»ç»Ÿè¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œé¢†åŸŸä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸åŒå¯¹æ¯”è§†è§‰-è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§å·®å¼‚æ˜¾è‘—ï¼ŒæŸäº›æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†é¢†åŸŸç‰¹å®šè¯„ä¼°çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æå’Œç¤¾äº¤åª’ä½“å†…å®¹ç†è§£ç­‰ã€‚é€šè¿‡æä¾›æœ‰æ•ˆçš„é¢†åŸŸé²æ£’æ€§è¯„ä¼°å·¥å…·ï¼ŒDeepbenchèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…ä¼˜åŒ–æ¨¡å‹åœ¨ç‰¹å®šç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œæå‡å®é™…åº”ç”¨çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In real-world vision-language applications, practitioners increasingly rely on large, pretrained foundation models rather than custom-built solutions, despite limited transparency regarding their training data and processes. While these models achieve impressive performance on general benchmarks, their effectiveness can decline notably under specialized domain shifts, such as unique imaging conditions or environmental variations. In this work, we introduce Deepbench, a framework designed to assess domain-specific robustness of vision-language models (VLMs). Deepbench leverages a large language model (LLM) to generate realistic, context-aware image corruptions tailored to specific deployment domains without requiring labeled data. We evaluate a range of contrastive vision-language architectures and architectural variants across six real-world domains and observe substantial variability in robustness, highlighting the need for targeted, domain-aware evaluation. Deepbench is released as open-source software to support further research into domain-aware robustness assessment.

