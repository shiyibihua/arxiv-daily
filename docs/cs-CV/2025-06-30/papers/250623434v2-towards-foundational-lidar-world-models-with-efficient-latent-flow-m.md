---
layout: default
title: Towards foundational LiDAR world models with efficient latent flow matching
---

# Towards foundational LiDAR world models with efficient latent flow matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23434" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23434v2</a>
  <a href="https://arxiv.org/pdf/2506.23434.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23434v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23434v2', 'Towards foundational LiDAR world models with efficient latent flow matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianran Liu, Shengwen Zhao, Nicholas Rhinehart

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Accepted to the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 25 pages, 13 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ½œåœ¨æ¡ä»¶æµåŒ¹é…çš„LiDARä¸–ç•Œæ¨¡å‹ä»¥è§£å†³é¢†åŸŸè¿ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `LiDARä¸–ç•Œæ¨¡å‹` `é¢†åŸŸè¿ç§»` `æ½œåœ¨æ¡ä»¶æµåŒ¹é…` `è¯­ä¹‰å ç”¨é¢„æµ‹` `è®¡ç®—æ•ˆç‡` `è‡ªåŠ¨é©¾é©¶` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LiDARä¸–ç•Œæ¨¡å‹é€šå¸¸åªèƒ½åœ¨ç‰¹å®šé¢†åŸŸå†…æœ‰æ•ˆï¼Œç¼ºä¹è·¨é¢†åŸŸçš„è¿ç§»èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ¡ä»¶æµåŒ¹é…çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LiDARä¸–ç•Œæ¨¡å‹çš„è¿ç§»èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„è¿ç§»ä¸­å®ç°äº†æœ€é«˜11%çš„ç»å¯¹æå‡ï¼Œå¹¶åœ¨è¯­ä¹‰å ç”¨é¢„æµ‹ä¸­å‡å°‘äº†95%çš„æ ‡æ³¨æ•°æ®éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºLiDARçš„ä¸–ç•Œæ¨¡å‹æä¾›äº†æ¯”å›¾åƒæ¨¡å‹æ›´ç»“æ„åŒ–å’Œå‡ ä½•æ„ŸçŸ¥çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„LiDARä¸–ç•Œæ¨¡å‹è®­ç»ƒèŒƒå›´ç‹­çª„ï¼Œé€šå¸¸åªèƒ½åœ¨ç‰¹å®šé¢†åŸŸå†…è¡¨ç°è‰¯å¥½ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†è·¨å¤šä¸ªé¢†åŸŸçš„è¿ç§»èƒ½åŠ›ï¼Œå±•ç¤ºäº†å•ä¸€é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ¯”è¾ƒä¸­è¶…è¶Šäº†ä»å¤´è®­ç»ƒçš„æ•ˆæœï¼Œå¹¶åœ¨è¯­ä¹‰å ç”¨é¢„æµ‹ä¸­æ˜¾è‘—å‡å°‘äº†å¯¹æ‰‹åŠ¨æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æˆ‘ä»¬æå‡ºçš„æ½œåœ¨æ¡ä»¶æµåŒ¹é…æ¡†æ¶åœ¨æ•°æ®å‹ç¼©å’Œè®­ç»ƒæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„é‡å»ºç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LiDARä¸–ç•Œæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸé—´è¿ç§»èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®åˆ©ç”¨å’Œè®­ç»ƒæ•ˆç‡ä¸Šå­˜åœ¨ä½æ•ˆç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ½œåœ¨æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œå°‘é‡æ ‡æ³¨æ•°æ®ï¼Œæå‡æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹é¢„è®­ç»ƒã€é¢†åŸŸé€‚åº”å’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„è¿ç§»å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥æ½œåœ¨æ¡ä»¶æµåŒ¹é…æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå‹ç¼©æ¯”æé«˜äº†6å€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®åˆ©ç”¨ç‡ï¼Œç¡®ä¿åœ¨ä»…ä½¿ç”¨ä¸€åŠè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä»èƒ½è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„è¿ç§»ä¸­å®ç°äº†æœ€é«˜11%çš„ç»å¯¹æå‡ï¼Œå¹¶åœ¨30/36çš„æ¯”è¾ƒä¸­è¶…è¶Šäº†ä»å¤´è®­ç»ƒçš„æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è¯­ä¹‰å ç”¨é¢„æµ‹ä¸­ä»…éœ€5%çš„æ ‡æ³¨æ•°æ®ï¼Œä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šå®ç°äº†23å€çš„æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½åŸå¸‚å»ºè®¾ç­‰ã€‚é€šè¿‡æé«˜LiDARæ¨¡å‹çš„è¿ç§»èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ï¼Œå¯ä»¥åœ¨ä¸åŒç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„ç©ºé—´ç†è§£å’Œå†³ç­–æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> LiDAR-based world models offer more structured and geometry-aware representations than their image-based counterparts. However, existing LiDAR world models are narrowly trained; each model excels only in the domain for which it was built. Can we develop LiDAR world models that exhibit strong transferability across multiple domains? We conduct the first systematic domain transfer study across three demanding scenarios: (i) outdoor to indoor generalization, (ii) sparse-beam & dense-beam adaptation, and (iii) non-semantic to semantic transfer. Given different amounts of fine-tuning data, our experiments show that a single pre-trained model can achieve up to 11% absolute improvement (83% relative) over training from scratch and outperforms training from scratch in 30/36 of our comparisons. This transferability of dynamic learning significantly reduces the reliance on manually annotated data for semantic occupancy forecasting: our method exceed the previous semantic occupancy forecasting models with only 5% of the labeled training data required by prior models. We also observed inefficiencies of current LiDAR world models, mainly through their under-compression of LiDAR data and inefficient training objectives. To address this, we propose a latent conditional flow matching (CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy using only half the training data and a compression ratio 6 times higher than that of prior methods. Our model achieves SOTA performance on future-trajectory-conditioned semantic occupancy forecasting while being 23x more computationally efficient (a 28x FPS speedup); and achieves SOTA performance on semantic occupancy forecasting while being 2x more computationally efficient (a 1.1x FPS speedup).

