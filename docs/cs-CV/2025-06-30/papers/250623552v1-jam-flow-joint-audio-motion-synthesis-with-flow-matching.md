---
layout: default
title: JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching
---

# JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23552" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23552v1</a>
  <a href="https://arxiv.org/pdf/2506.23552.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23552v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23552v1', 'JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh

**åˆ†ç±»**: cs.CV, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: project page: https://joonghyuk.com/jamflow-web Under review. Preprint published on arXiv

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJAM-Flowä»¥è§£å†³éŸ³é¢‘ä¸é¢éƒ¨åŠ¨ä½œåˆæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç”Ÿæˆ` `éŸ³é¢‘åˆæˆ` `é¢éƒ¨åŠ¨ä½œåˆæˆ` `æµåŒ¹é…` `æ‰©æ•£å˜æ¢å™¨` `è”åˆæ³¨æ„åŠ›` `è™šæ‹Ÿè§’è‰²äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸å°†é¢éƒ¨åŠ¨ä½œåˆæˆä¸è¯­éŸ³åˆæˆè§†ä¸ºç‹¬ç«‹ä»»åŠ¡ï¼Œç¼ºä¹æœ‰æ•ˆçš„è·¨æ¨¡æ€æ•´åˆã€‚
2. JAM-Flowé€šè¿‡æµåŒ¹é…å’Œå¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥åŒæ—¶å¤„ç†éŸ³é¢‘ä¸é¢éƒ¨åŠ¨ä½œåˆæˆã€‚
3. è¯¥æ–¹æ³•åœ¨å¤šç§æ¡ä»¶è¾“å…¥ä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„åŒæ­¥è¯´è¯å¤´ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆå»ºæ¨¡çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢éƒ¨åŠ¨ä½œä¸è¯­éŸ³ä¹‹é—´çš„å†…åœ¨è”ç³»åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­å¸¸è¢«å¿½è§†ï¼Œé€šå¸¸å°†è¯´è¯å¤´åˆæˆä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰è§†ä¸ºç‹¬ç«‹ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†JAM-Flowï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶åˆæˆå’Œæ¡ä»¶åŒ–é¢éƒ¨åŠ¨ä½œä¸è¯­éŸ³ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æµåŒ¹é…å’Œæ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼ˆMM-DiTï¼‰æ¶æ„ï¼Œé›†æˆäº†ä¸“é—¨çš„Motion-DiTå’ŒAudio-DiTæ¨¡å—ï¼Œé€šè¿‡é€‰æ‹©æ€§è”åˆæ³¨æ„åŠ›å±‚è€¦åˆï¼Œç»“åˆæ—¶é—´å¯¹é½çš„ä½ç½®åµŒå…¥å’Œå±€éƒ¨è”åˆæ³¨æ„åŠ›æ©è”½ç­‰å…³é”®æ¶æ„é€‰æ‹©ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ï¼ŒåŒæ—¶ä¿ç•™ç‰¹å®šæ¨¡æ€çš„ä¼˜åŠ¿ã€‚JAM-Flowæ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å‚è€ƒéŸ³é¢‘å’Œå‚è€ƒåŠ¨ä½œï¼Œä¿ƒè¿›äº†ä»æ–‡æœ¬ç”ŸæˆåŒæ­¥è¯´è¯å¤´ã€éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ç­‰ä»»åŠ¡ï¼Œæä¾›äº†ä¸€ä¸ªæ•´ä½“çš„éŸ³è§†é¢‘åˆæˆè§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é¢éƒ¨åŠ¨ä½œä¸è¯­éŸ³åˆæˆä¹‹é—´çš„æ•´åˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äºŒè€…çš„å†…åœ¨è”ç³»ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šJAM-Flowé€šè¿‡å¼•å…¥æµåŒ¹é…æŠ€æœ¯å’Œå¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œè®¾è®¡äº†ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†éŸ³é¢‘å’Œé¢éƒ¨åŠ¨ä½œçš„ç»Ÿä¸€æ¡†æ¶ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬Motion-DiTå’ŒAudio-DiTä¸¤ä¸ªæ¨¡å—ï¼Œé€šè¿‡é€‰æ‹©æ€§è”åˆæ³¨æ„åŠ›å±‚è¿›è¡Œè€¦åˆï¼Œé‡‡ç”¨æ—¶é—´å¯¹é½çš„ä½ç½®åµŒå…¥å’Œå±€éƒ¨è”åˆæ³¨æ„åŠ›æ©è”½ï¼Œç¡®ä¿ä¸åŒæ¨¡æ€é—´çš„æœ‰æ•ˆä¿¡æ¯ä¼ é€’ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæµåŒ¹é…å’Œå¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨çš„ç»“åˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†å¤šç§è¾“å…¥æ—¶ä¿æŒé«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†å¤šç§å…³é”®è®¾è®¡ï¼ŒåŒ…æ‹¬æŸå¤±å‡½æ•°çš„è®¾ç½®ã€ç½‘ç»œç»“æ„çš„ä¼˜åŒ–ä»¥åŠå‚æ•°çš„é€‰æ‹©ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒJAM-Flowåœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œç”Ÿæˆçš„åŒæ­¥è¯´è¯å¤´åœ¨è§†è§‰å’ŒéŸ³é¢‘ä¸€è‡´æ€§æ–¹é¢æå‡äº†çº¦30%ã€‚è¯¥æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„è¾“å…¥æ¡ä»¶æ—¶ï¼Œå±•ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§å’Œç”Ÿæˆè´¨é‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€ç”Ÿæˆé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

JAM-Flowçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€åŠ¨ç”»åˆ¶ä½œå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡å®ç°é«˜è´¨é‡çš„éŸ³é¢‘ä¸é¢éƒ¨åŠ¨ä½œåŒæ­¥åˆæˆï¼Œè¯¥ç ”ç©¶èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶åœ¨å¨±ä¹ã€æ•™è‚²ç­‰å¤šä¸ªè¡Œä¸šä¸­å…·æœ‰å®é™…ä»·å€¼ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´è‡ªç„¶çš„è™šæ‹Ÿè§’è‰²äº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web

