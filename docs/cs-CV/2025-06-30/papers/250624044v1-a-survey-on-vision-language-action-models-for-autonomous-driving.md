---
layout: default
title: A Survey on Vision-Language-Action Models for Autonomous Driving
---

# A Survey on Vision-Language-Action Models for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.24044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.24044v1</a>
  <a href="https://arxiv.org/pdf/2506.24044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.24044v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.24044v1', 'A Survey on Vision-Language-Action Models for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sicong Jiang, Zilin Huang, Kangan Qian, Ziang Luo, Tianze Zhu, Yang Zhong, Yihong Tang, Menglin Kong, Yunlong Wang, Siwen Jiao, Hao Ye, Zihao Sheng, Xin Zhao, Tuopu Wen, Zheng Fu, Sikai Chen, Kun Jiang, Diange Yang, Seongjin Choi, Lijun Sun

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/JohnsonJiang1996/Awesome-VLA4AD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä»¥æ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å‘å±•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `è‡ªåŠ¨é©¾é©¶` `å¤šæ¨¡æ€å­¦ä¹ ` `æ™ºèƒ½å†³ç­–` `äº¤é€šå®‰å…¨` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹æ¯”è¾ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯åœ¨å¤„ç†å¤æ‚äº¤é€šåœºæ™¯å’Œé«˜å±‚æŒ‡ä»¤æ—¶å­˜åœ¨ç†è§£å’Œå†³ç­–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç»¼åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æ™ºèƒ½å†³ç­–èƒ½åŠ›ã€‚
3. é€šè¿‡æ¯”è¾ƒ20å¤šä¸ªæ¨¡å‹ï¼Œè®ºæ–‡å±•ç¤ºäº†VLAåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„è¿›å±•ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿè¿›å±•ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰èŒƒå¼é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›èŒƒå¼å°†è§†è§‰æ„ŸçŸ¥ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œæ§åˆ¶æ•´åˆåœ¨å•ä¸€ç­–ç•¥ä¸­ã€‚è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„ç ”ç©¶è€…æ­£åœ¨ç§¯æé€‚åº”è¿™äº›æ–¹æ³•ï¼Œä»¥å®ç°èƒ½å¤Ÿè§£è¯»é«˜å±‚æŒ‡ä»¤ã€æ¨ç†å¤æ‚äº¤é€šåœºæ™¯å¹¶è‡ªä¸»å†³ç­–çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚ç„¶è€Œï¼Œç›¸å…³æ–‡çŒ®ä»ç„¶é›¶æ•£ä¸”å¿«é€Ÿæ‰©å±•ã€‚æœ¬ç»¼è¿°é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†è‡ªåŠ¨é©¾é©¶ä¸­çš„VLAï¼ˆVLA4ADï¼‰ï¼ŒåŒ…æ‹¬å¯¹è¿‘æœŸå·¥ä½œçš„æ¶æ„æ„å»ºå—è¿›è¡Œå½¢å¼åŒ–ã€è¿½è¸ªä»æ—©æœŸè§£é‡Šå™¨åˆ°ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„VLAæ¨¡å‹çš„æ¼”å˜ï¼Œå¹¶æ¯”è¾ƒ20å¤šä¸ªä»£è¡¨æ€§æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ•´åˆç°æœ‰æ•°æ®é›†å’ŒåŸºå‡†ï¼Œå¼ºè°ƒå…±åŒæµ‹é‡é©¾é©¶å®‰å…¨æ€§ã€å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡çš„åè®®ï¼Œæœ€åè¯¦ç»†é˜è¿°äº†å¼€æ”¾æŒ‘æˆ˜åŠæœªæ¥æ–¹å‘ï¼Œä¸ºæ¨åŠ¨å¯è§£é‡Šçš„ç¤¾ä¼šå¯¹é½è‡ªåŠ¨é©¾é©¶è½¦è¾†æä¾›äº†ç®€æ˜è€Œå®Œæ•´çš„å‚è€ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨ç†è§£å¤æ‚äº¤é€šåœºæ™¯å’Œé«˜å±‚æŒ‡ä»¤æ—¶çš„ä¸è¶³ï¼Œå¯¼è‡´å†³ç­–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯æ„å»ºä¸€ä¸ªè§†è§‰-è¯­è¨€-åŠ¨ä½œçš„ç»¼åˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€ç­–ç•¥ä¸­æ•´åˆè§†è§‰æ„ŸçŸ¥ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œæ§åˆ¶ï¼Œä»è€Œæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰æ„ŸçŸ¥æ¨¡å—è´Ÿè´£å¤„ç†è¾“å…¥çš„è§†è§‰ä¿¡æ¯ï¼Œè¯­è¨€ç†è§£æ¨¡å—è§£æé«˜å±‚æŒ‡ä»¤ï¼Œæ§åˆ¶æ¨¡å—åˆ™åŸºäºå‰ä¸¤è€…çš„è¾“å‡ºè¿›è¡Œå†³ç­–å’Œè¡ŒåŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ•´åˆäº†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„å†³ç­–æ¡†æ¶ï¼Œä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œç»“åˆäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°åˆ™è€ƒè™‘äº†å®‰å…¨æ€§ã€å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡çš„ç»¼åˆè¯„ä¼°ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå®æ—¶å†³ç­–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ¯”è¾ƒäº†20å¤šä¸ªä»£è¡¨æ€§æ¨¡å‹ï¼Œå±•ç¤ºäº†VLAåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„æ˜¾è‘—è¿›å±•ã€‚é€šè¿‡æ•´åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¿¡æ¯ï¼Œæ¨¡å‹åœ¨å¤æ‚äº¤é€šåœºæ™¯ä¸­çš„å†³ç­–å‡†ç¡®æ€§å’Œå®‰å…¨æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªæŠ«éœ²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç†è§£å’Œå†³ç­–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜äº¤é€šå®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½å¯¹æ™ºèƒ½åŸå¸‚çš„å‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

