---
layout: default
title: MotionGPT3: Human Motion as a Second Modality
---

# MotionGPT3: Human Motion as a Second Modality

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.24086" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.24086v3</a>
  <a href="https://arxiv.org/pdf/2506.24086.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.24086v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.24086v3', 'MotionGPT3: Human Motion as a Second Modality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, Xin Chen

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-11-03)

**å¤‡æ³¨**: 26 pages, 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMotionGPT3ä»¥è§£å†³å¤šæ¨¡æ€è¿åŠ¨ç†è§£ä¸ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è¿åŠ¨ç”Ÿæˆ` `è¯­è¨€æ¨¡å‹` `å˜åˆ†è‡ªç¼–ç å™¨` `Transformeræ¶æ„` `æ¨¡å‹æ”¶æ•›` `è¿åŠ¨ç†è§£` `ä¿¡æ¯æµåŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¡†æ¶åœ¨å¤„ç†è¿åŠ¨ä¸è¯­è¨€çš„ç»“åˆæ—¶ï¼Œé¢ä¸´ç€é‡åŒ–è¯¯å·®å’Œè·¨æ¨¡æ€å¹²æ‰°çš„é—®é¢˜ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½ã€‚
2. MotionGPT3é€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨å°†è¿åŠ¨ç¼–ç ä¸ºè¿ç»­æ½œåœ¨ç©ºé—´ï¼Œé‡‡ç”¨åŒæµTransformeræ¶æ„ä»¥å‡å°‘æ¨¡æ€é—´å¹²æ‰°ï¼Œæå‡ä¿¡æ¯æµåŠ¨æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMotionGPT3åœ¨è®­ç»ƒå’ŒéªŒè¯é˜¶æ®µçš„æ”¶æ•›é€Ÿåº¦åˆ†åˆ«æå‡äº†2å€å’Œ4å€ï¼ŒåŒæ—¶åœ¨è¿åŠ¨ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆçš„å¤šæ¨¡æ€æ¡†æ¶å˜å¾—è¶Šæ¥è¶Šæœ‰å‰æ™¯ï¼Œä½†éšç€æ¨¡æ€å’Œä»»åŠ¡æ•°é‡çš„å¢åŠ ï¼Œå¤æ‚æ€§ä¹Ÿåœ¨å¢åŠ ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°è¿åŠ¨é‡åŒ–å¼•å…¥çš„è¿‘ä¼¼è¯¯å·®é™åˆ¶äº†è¿åŠ¨è´¨é‡ï¼Œè€Œå°†ç¦»æ•£æ–‡æœ¬å’Œè¿ç»­è¿åŠ¨ç»Ÿä¸€åœ¨å•ä¸€æµéª¨å¹²ä¸­åˆ™åŠ å‰§äº†è·¨æ¨¡æ€å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MotionGPT3ï¼Œä¸€ä¸ªç”¨äºç†è§£å’Œç”Ÿæˆçš„åŒæ¨¡æ€è¿åŠ¨-è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†åŸå§‹è¿åŠ¨ç¼–ç ä¸ºè¿ç»­æ½œåœ¨ç©ºé—´ï¼Œé¿å…äº†é‡åŒ–å¼•èµ·çš„ä¼ªå½±ï¼ŒåŒæ—¶åˆ©ç”¨äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å…ˆéªŒã€‚åŒæµTransformerä¸å…±äº«æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œä¿ç•™äº†æ¨¡æ€ç‰¹å®šçš„è·¯å¾„ï¼ŒåŒæ—¶å®ç°äº†å—æ§çš„åŒå‘ä¿¡æ¯æµï¼Œå‡å°‘äº†å¹²æ‰°ï¼Œç¨³å®šäº†ä¼˜åŒ–ï¼Œå¹¶åœ¨ä¸é™ä½ä¿çœŸåº¦çš„æƒ…å†µä¸‹åŠ é€Ÿäº†æ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼ŒMotionGPT3åœ¨è®­ç»ƒæŸå¤±ä¸Šå®ç°äº†2å€çš„æ”¶æ•›é€Ÿåº¦æå‡ï¼Œåœ¨éªŒè¯ä¸Šå®ç°äº†é«˜è¾¾4å€çš„æ”¶æ•›é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨æ ‡å‡†è¿åŠ¨ç†è§£å’Œç”ŸæˆåŸºå‡†ä¸Šä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è¿åŠ¨ç†è§£ä¸ç”Ÿæˆä¸­çš„é‡åŒ–è¯¯å·®å’Œè·¨æ¨¡æ€å¹²æ‰°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å°†ç¦»æ•£æ–‡æœ¬ä¸è¿ç»­è¿åŠ¨ç»“åˆæ—¶ï¼Œå¸¸å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMotionGPT3æ¨¡å‹ï¼Œé€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è¿åŠ¨æ•°æ®ç¼–ç ä¸ºè¿ç»­æ½œåœ¨ç©ºé—´ï¼Œé¿å…é‡åŒ–ä¼ªå½±ï¼ŒåŒæ—¶ç»“åˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæå‡æ¨¡å‹çš„ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMotionGPT3é‡‡ç”¨åŒæµTransformeræ¶æ„ï¼Œåˆ†åˆ«å¤„ç†è¿åŠ¨å’Œè¯­è¨€æ¨¡æ€ã€‚å…±äº«æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡æ€é—´çš„ä¿¡æ¯æµåŠ¨ï¼ŒåŒæ—¶ä¿æŒæ¨¡æ€ç‰¹å®šçš„ç‰¹å¾ï¼Œå‡å°‘å¹²æ‰°ã€‚æ¨¡å‹è®­ç»ƒé‡‡ç”¨ç”Ÿæˆ-å¯¹é½çš„ä¸‰é˜¶æ®µè°ƒåº¦ï¼Œè¿›ä¸€æ­¥æé«˜ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨åŒæµç»“æ„ä¸VAEç›¸ç»“åˆï¼Œæ˜¾è‘—é™ä½äº†æ¨¡æ€é—´çš„å¹²æ‰°ï¼Œå¹¶åŠ é€Ÿäº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å•æµæ¨¡å‹å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬VAEçš„æ½œåœ¨ç©ºé—´ç»´åº¦ã€åŒæµTransformerçš„å±‚æ•°å’Œæ³¨æ„åŠ›å¤´æ•°ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†é‡æ„æŸå¤±ä¸å¯¹æŠ—æŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆè¿åŠ¨çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚æ•´ä½“æ¶æ„ç»è¿‡å¤šè½®å®éªŒä¼˜åŒ–ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„è®­ç»ƒä¸æ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMotionGPT3åœ¨è®­ç»ƒæŸå¤±ä¸Šå®ç°äº†2å€çš„æ”¶æ•›é€Ÿåº¦æå‡ï¼Œåœ¨éªŒè¯é˜¶æ®µçš„æ”¶æ•›é€Ÿåº¦é«˜è¾¾4å€ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¸ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MotionGPT3çš„ç ”ç©¶æˆæœåœ¨è™šæ‹Ÿç°å®ã€åŠ¨ç”»åˆ¶ä½œã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è¿åŠ¨ç†è§£ä¸ç”Ÿæˆçš„è´¨é‡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä¸ºäººæœºäº¤äº’ã€è‡ªåŠ¨åŠ¨ç”»ç”Ÿæˆç­‰åº”ç”¨æä¾›æ›´è‡ªç„¶çš„ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid progress of large language models (LLMs), multimodal frameworks that unify understanding and generation have become promising, yet they face increasing complexity as the number of modalities and tasks grows. We observe that motion quantization introduces approximation errors that cap motion quality, and that unifying discrete text and continuous motion within a single-stream backbone amplifies cross-modal interference. Motivated by recent multi-branch Transformer designs that separate signals from different modalities, we propose MotionGPT3, a bimodal motion-language model for both understanding and generation. MotionGPT3 encodes raw motion into a continuous latent space using a variational autoencoder (VAE), thereby avoiding quantization-induced artifacts, while leveraging the semantic prior of pretrained language models. A dual-stream Transformer with shared attention preserves modality-specific routes while enabling controlled, bidirectional information flow, which reduces interference, stabilizing optimization, and empirically accelerates convergence without degrading fidelity. For multimodal joint training, a generate-then-align three-stage schedule further improves stability and limits cross-task interference. Experiments show that MotionGPT3 achieves 2x faster convergence in training loss and up to 4x faster convergence in validation, while maintaining state-of-the-art performance on standard motion understanding and motion generation benchmarks.

