---
layout: default
title: Dataset Distillation via Vision-Language Category Prototype
---

# Dataset Distillation via Vision-Language Category Prototype

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23580" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23580v1</a>
  <a href="https://arxiv.org/pdf/2506.23580.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23580v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23580v1', 'Dataset Distillation via Vision-Language Category Prototype')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yawen Zou, Guang Li, Duo Su, Zi Wang, Jun Yu, Chao Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: accepted by ICCV2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰-è¯­è¨€ç±»åˆ«åŸå‹çš„è’¸é¦æ–¹æ³•ä»¥æå‡æ•°æ®é›†è’¸é¦æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `è§†è§‰-è¯­è¨€èåˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­ä¹‰ä¿¡æ¯` `å›¾åƒç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒä¿¡æ¯ï¼Œå¿½è§†äº†è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ–‡æœ¬åŸå‹ï¼Œç»“åˆè§†è§‰-è¯­è¨€æ–¹æ³•ï¼Œå¢å¼ºäº†æ•°æ®é›†è’¸é¦çš„æ•ˆæœå’Œé€»è¾‘ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨éªŒè¯æ€§èƒ½ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†è’¸é¦ï¼ˆDDï¼‰æ—¨åœ¨å°†å¤§å‹æ•°æ®é›†æµ“ç¼©ä¸ºç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ›¿ä»£å“ï¼Œä¿æŒä¸åŸå§‹æ•°æ®é›†ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½å­˜å‚¨ã€ä¼ è¾“æˆæœ¬å’Œè®¡ç®—æ¶ˆè€—ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DDæ–¹æ³•ä¸»è¦é›†ä¸­äºä»å›¾åƒä¸­æå–ä¿¡æ¯ï¼Œå¾€å¾€å¿½è§†æ•°æ®ä¸­å›ºæœ‰çš„è¯­ä¹‰ä¿¡æ¯ã€‚è¿™ç§å¯¹ä¸Šä¸‹æ–‡çš„å¿½è§†é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤æ‚æ•°æ®é›†çš„ä»»åŠ¡ä¸­ï¼Œå¯èƒ½å¯¼è‡´ä¸åˆé€»è¾‘çš„è¾“å‡ºæˆ–å…³é”®å¯¹è±¡çš„é—æ¼ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ–‡æœ¬åŸå‹ï¼Œå°†è§†è§‰-è¯­è¨€æ–¹æ³•æ•´åˆåˆ°DDä¸­ï¼Œä»¥è’¸é¦è¯­è¨€ä¿¡æ¯å¹¶ä¸å›¾åƒåŸå‹ååŒåˆæˆæ•°æ®ï¼Œä»è€Œæå‡æ•°æ®é›†è’¸é¦æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„æ–‡æœ¬åŸå‹æºè‡ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æè¿°æ€§æ–‡æœ¬ä¿¡æ¯ã€‚è¯¥æ¡†æ¶åœ¨æ²¡æœ‰é¢„å…ˆå­˜åœ¨æ–‡æœ¬æè¿°çš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œæ‰©å±•äº†æ•°æ®é›†è’¸é¦çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•å¿½è§†è¯­ä¹‰ä¿¡æ¯çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ–‡æœ¬åŸå‹ï¼Œç»“åˆå›¾åƒåŸå‹è¿›è¡Œæ•°æ®åˆæˆï¼Œåˆ©ç”¨è¯­è¨€ä¿¡æ¯æå‡æ•°æ®é›†è’¸é¦çš„æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç”Ÿæˆçš„æ•°æ®åœ¨é€»è¾‘ä¸Šæ›´åŠ ä¸€è‡´ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ ç›®æ ‡å¯¹è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æœ¬åŸå‹ç”Ÿæˆæ¨¡å—å’Œå›¾åƒåŸå‹åˆæˆæ¨¡å—ã€‚æ–‡æœ¬åŸå‹é€šè¿‡å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆï¼Œè€Œå›¾åƒåŸå‹åˆ™é€šè¿‡ä¼ ç»Ÿçš„å›¾åƒå¤„ç†æŠ€æœ¯è¿›è¡Œæå–å’Œåˆæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è§†è§‰-è¯­è¨€æ–¹æ³•å¼•å…¥æ•°æ®é›†è’¸é¦ï¼Œåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯æ¥å¢å¼ºå›¾åƒç”Ÿæˆçš„é€»è¾‘ä¸€è‡´æ€§ï¼Œè¿™ä¸ä»¥å¾€ä»…ä¾èµ–å›¾åƒä¿¡æ¯çš„è’¸é¦æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯çš„è´¡çŒ®ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œè¯­è¨€æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ç”Ÿæˆé€»è¾‘ä¸€è‡´çš„å›¾åƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯æ€§èƒ½è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡å¹…åº¦æ˜¾è‘—ï¼Œå…·ä½“æ•°æ®æœªè¯¦è¿°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡æ•°æ®é›†è’¸é¦çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source large language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/

