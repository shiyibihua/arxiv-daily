---
layout: default
title: PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum
---

# PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23607" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.23607v1</a>
  <a href="https://arxiv.org/pdf/2506.23607.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23607v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23607v1', 'PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shiqi Zhang, Sha Zhang, Jiajun Deng, Yedong Shen, Mingxiao MA, Yanyong Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PGOV3D‰ª•Ëß£ÂÜ≥ÂºÄÊîæËØçÊ±á3DËØ≠‰πâÂàÜÂâ≤‰∏≠ÁöÑ‰ø°ÊÅØËΩ¨ÁßªÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂºÄÊîæËØçÊ±á` `3DËØ≠‰πâÂàÜÂâ≤` `ËØæÁ®ãÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂºÄÊîæËØçÊ±á3DËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ïÂøΩËßÜ‰∫ÜÂ§öËßÜÂõæÂõæÂÉèÁöÑ‰∏∞ÂØåËØ≠‰πâÂÜÖÂÆπÔºåÂØºËá¥Ê®°ÂûãÊïàÊûúÂèóÈôê„ÄÇ
2. PGOV3DÈÄöËøáÂºïÂÖ•ÈÉ®ÂàÜÂà∞ÂÖ®Â±ÄÁöÑËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•ÔºåÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•Êù•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. Âú®ScanNet„ÄÅScanNet200ÂíåS3DISÂü∫ÂáÜÊµãËØï‰∏≠ÔºåPGOV3DÂ±ïÁé∞Âá∫‰ºòÂºÇÁöÑÊÄßËÉΩÔºåÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÁªìÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÂºÄÊîæËØçÊ±á3DËØ≠‰πâÂàÜÂâ≤ÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂ∞ÜÂ§öËßÜÂõæÂõæÂÉè‰∏≠ÊèêÂèñÁöÑÊñáÊú¨ÂØπÈΩêÁâπÂæÅÂêàÂπ∂Âà∞3DÁÇπ‰∏äÊù•ÁõëÁù£Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ï‰ªÖÂ∞ÜÂ§öËßÜÂõæÂõæÂÉèËßÜ‰∏∫‰º†ÈÄíÂºÄÊîæËØçÊ±á‰ø°ÊÅØÁöÑ‰∏≠‰ªãÔºåÂøΩËßÜ‰∫ÜÂÖ∂‰∏∞ÂØåÁöÑËØ≠‰πâÂÜÖÂÆπÂíåË∑®ËßÜÂõæÂØπÂ∫îÂÖ≥Á≥ªÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜPGOV3DÔºå‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ÔºåÂºïÂÖ•‰∫ÜÈÉ®ÂàÜÂà∞ÂÖ®Â±ÄÁöÑËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•Ôºå‰ª•ÊîπÂñÑÂºÄÊîæËØçÊ±á3DËØ≠‰πâÂàÜÂâ≤„ÄÇÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫é‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÈ¶ñÂÖàÂú®Êèê‰æõÂØÜÈõÜËØ≠‰πâ‰ø°ÊÅØÁöÑÈÉ®ÂàÜÂú∫ÊôØ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂú®ÂÆåÊï¥Âú∫ÊôØ‰∏äËøõË°åÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPGOV3DÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂºÄÊîæËØçÊ±á3DËØ≠‰πâÂàÜÂâ≤‰∏≠‰ø°ÊÅØËΩ¨Áßª‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§öËßÜÂõæÂõæÂÉèÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPGOV3DÈááÁî®ÈÉ®ÂàÜÂà∞ÂÖ®Â±ÄÁöÑËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•ÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊù•ÊèêÂçáÊ®°ÂûãÁöÑÂºÄÊîæËØçÊ±áÂ≠¶‰π†ËÉΩÂäõÔºåÂÖÖÂàÜÂà©Áî®Â§öËßÜÂõæÊï∞ÊçÆÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÂú®ÈÉ®ÂàÜÂú∫ÊôØ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁ¨¨‰∫åÈò∂ÊÆµÂú®ÂÆåÊï¥Âú∫ÊôØ‰∏äËøõË°åÂæÆË∞É„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÅ2DÂàÜÂâ≤Âü∫Á°ÄÊ®°ÂûãÂíåËæÖÂä©ÁöÑÂ∏ßÈó¥‰∏ÄËá¥ÊÄßÊ®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÈÉ®ÂàÜÂà∞ÂÖ®Â±ÄÁöÑËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•ÔºåÈÄöËøáÈÄêÊ≠•ÂºïÂØºÊ®°Âûã‰ªéÁÆÄÂçïÂà∞Â§çÊùÇÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑËØ≠‰πâÁêÜËß£ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãÈááÁî®ÂÉèÁ¥†Á∫ßÊ∑±Â∫¶ÊäïÂΩ±ÁîüÊàêÈÉ®ÂàÜÁÇπ‰∫ëÔºåÂπ∂ÈÄöËøáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂºÄÊîæËØçÊ±áÊ†áÁ≠æÔºåËÆæËÆ°‰∫ÜËæÖÂä©Ê®°Âùó‰ª•Â¢ûÂº∫ÁâπÂæÅ‰∏ÄËá¥ÊÄßÔºåÁ°Æ‰øùË∑®ËßÜÂõæÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ScanNet„ÄÅScanNet200ÂíåS3DISÂü∫ÂáÜÊµãËØï‰∏≠ÔºåPGOV3DÁöÑÊÄßËÉΩË°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂºÄÊîæËØçÊ±á3DËØ≠‰πâÂàÜÂâ≤‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÁ´û‰∫âÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PGOV3DÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÊèêÂçáËøô‰∫õÈ¢ÜÂüü‰∏≠3DÁéØÂ¢ÉÁêÜËß£ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®ÂºÄÊîæËØçÊ±áÂ≠¶‰π†Âú®Êõ¥Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®Ôºå‰øÉËøõÊô∫ËÉΩÁ≥ªÁªüÁöÑËá™‰∏ªÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Existing open-vocabulary 3D semantic segmentation methods typically supervise 3D segmentation models by merging text-aligned features (e.g., CLIP) extracted from multi-view images onto 3D points. However, such approaches treat multi-view images merely as intermediaries for transferring open-vocabulary information, overlooking their rich semantic content and cross-view correspondences, which limits model effectiveness. To address this, we propose PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for improving open-vocabulary 3D semantic segmentation. The key innovation lies in a two-stage training strategy. In the first stage, we pre-train the model on partial scenes that provide dense semantic information but relatively simple geometry. These partial point clouds are derived from multi-view RGB-D inputs via pixel-wise depth projection. To enable open-vocabulary learning, we leverage a multi-modal large language model (MLLM) and a 2D segmentation foundation model to generate open-vocabulary labels for each viewpoint, offering rich and aligned supervision. An auxiliary inter-frame consistency module is introduced to enforce feature consistency across varying viewpoints and enhance spatial understanding. In the second stage, we fine-tune the model on complete scene-level point clouds, which are sparser and structurally more complex. We aggregate the partial vocabularies associated with each scene and generate pseudo labels using the pre-trained model, effectively bridging the semantic gap between dense partial observations and large-scale 3D environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS benchmarks demonstrate that PGOV3D achieves competitive performance in open-vocabulary 3D semantic segmentation.

