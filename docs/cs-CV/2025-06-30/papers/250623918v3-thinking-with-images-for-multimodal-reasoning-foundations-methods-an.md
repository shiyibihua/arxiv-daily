---
layout: default
title: Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers
---

# Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23918" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23918v3</a>
  <a href="https://arxiv.org/pdf/2506.23918.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23918v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23918v3', 'Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, Yi R. Fung

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-07-03)

**å¤‡æ³¨**: Preprint in progress. We maintain a real-time GitHub repository tracking progress at: https://github.com/zhaochen0110/Awesome_Think_With_Images

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»¥å›¾åƒæ€ç»´æ¨åŠ¨å¤šæ¨¡æ€æ¨ç†çš„æ¡†æ¶ä»¥è§£å†³è¯­ä¹‰å·®è·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `å›¾åƒæ€ç»´` `è®¤çŸ¥è‡ªä¸»æ€§` `è§†è§‰ä¿¡æ¯` `æ™ºèƒ½åŠ©æ‰‹` `è‡ªåŠ¨é©¾é©¶` `åŒ»ç–—å½±åƒåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è¿›è¡Œæ¨ç†ï¼Œå¿½è§†äº†è§†è§‰ä¿¡æ¯çš„åŠ¨æ€ä½œç”¨ï¼Œå¯¼è‡´è¯­ä¹‰å·®è·ã€‚
2. è®ºæ–‡æå‡ºä»¥å›¾åƒæ€ç»´ä¸ºæ ¸å¿ƒçš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œå¼ºè°ƒè§†è§‰ä¿¡æ¯åœ¨æ€ç»´è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚
3. é€šè¿‡ç³»ç»Ÿçš„æ–‡çŒ®å›é¡¾å’Œæ¡†æ¶æ„å»ºï¼Œè®ºæ–‡ä¸ºæœªæ¥çš„å¤šæ¨¡æ€AIç ”ç©¶æä¾›äº†æ¸…æ™°çš„æ–¹å‘å’ŒæŒ‘æˆ˜åˆ†æã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€æ¨ç†çš„è¿›å±•å¾—ç›Šäºæ–‡æœ¬é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•ï¼Œä½†è¿™ç§ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ–¹æ³•å°†è§†è§‰è§†ä¸ºé™æ€åˆå§‹ä¸Šä¸‹æ–‡ï¼Œé€ æˆäº†ä¸°å¯Œæ„ŸçŸ¥æ•°æ®ä¸ç¦»æ•£ç¬¦å·æ€ç»´ä¹‹é—´çš„åŸºæœ¬â€œè¯­ä¹‰å·®è·â€ã€‚äººç±»è®¤çŸ¥å¸¸å¸¸è¶…è¶Šè¯­è¨€ï¼Œåˆ©ç”¨è§†è§‰ä½œä¸ºåŠ¨æ€çš„å¿ƒç†è‰å›¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å…´çš„ä»¥å›¾åƒæ€ç»´ä¸ºæ ¸å¿ƒçš„èŒƒå¼ï¼Œå¼ºè°ƒè§†è§‰ä¿¡æ¯åœ¨æ€ç»´è¿‡ç¨‹ä¸­çš„åŠ¨æ€ä½œç”¨ã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œå›é¡¾æ ¸å¿ƒæ–¹æ³•ï¼Œåˆ†æè¯„ä¼°åŸºå‡†å’Œåº”ç”¨ï¼Œè¯†åˆ«æŒ‘æˆ˜å¹¶å±•æœ›æœªæ¥ï¼Œä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„ç ”ç©¶æä¾›æ¸…æ™°çš„è·¯çº¿å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ä¸­è§†è§‰ä¿¡æ¯è¢«è§†ä¸ºé™æ€è¾“å…¥çš„é—®é¢˜ï¼Œå¯¼è‡´è¯­ä¹‰å·®è·å’Œè®¤çŸ¥èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä»¥å›¾åƒæ€ç»´ä¸ºæ ¸å¿ƒçš„æ¨ç†èŒƒå¼ï¼Œå¼ºè°ƒè§†è§‰ä¿¡æ¯åœ¨æ€ç»´è¿‡ç¨‹ä¸­çš„åŠ¨æ€ä½œç”¨ï¼Œæ¨åŠ¨ä»è¢«åŠ¨è¾“å…¥åˆ°ä¸»åŠ¨æ€ç»´çš„è½¬å˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šå¤–éƒ¨å·¥å…·æ¢ç´¢ã€ç¨‹åºåŒ–æ“ä½œå’Œå†…åœ¨æƒ³è±¡ã€‚æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‰¹å®šçš„æ ¸å¿ƒæ–¹æ³•å’Œåº”ç”¨åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è§†è§‰ä¿¡æ¯è½¬å˜ä¸ºæ€ç»´è¿‡ç¨‹ä¸­çš„åŠ¨æ€è®¤çŸ¥å·¥ä½œç©ºé—´ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è®¤çŸ¥è‡ªä¸»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æ”¯æŒä¸åŒé˜¶æ®µçš„æ¨ç†éœ€æ±‚ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºå›¾åƒæ€ç»´çš„æ¨¡å‹åœ¨å¤šä¸ªå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–‡æœ¬ä¸­å¿ƒæ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿçš„è®¤çŸ¥èƒ½åŠ›å’Œå†³ç­–è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½åœ¨å¤šæ¨¡æ€äº¤äº’å’Œäººæœºåä½œä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ›´è‡ªç„¶çš„AIåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.

