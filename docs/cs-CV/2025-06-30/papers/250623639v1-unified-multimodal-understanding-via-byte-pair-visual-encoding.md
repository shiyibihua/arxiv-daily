---
layout: default
title: Unified Multimodal Understanding via Byte-Pair Visual Encoding
---

# Unified Multimodal Understanding via Byte-Pair Visual Encoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23639" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23639v1</a>
  <a href="https://arxiv.org/pdf/2506.23639.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23639v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23639v1', 'Unified Multimodal Understanding via Byte-Pair Visual Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, Sipeng Zheng, Zongqing Lu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£æ¡†æ¶ä»¥è§£å†³æ¨¡æ€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `å­—èŠ‚å¯¹ç¼–ç ` `è§†è§‰-è¯­è¨€ä»»åŠ¡` `æ¨¡æ€å¯¹é½` `å˜æ¢å™¨æ¨¡å‹` `è·¨æ¨¡æ€å…³ç³»` `è¯¾ç¨‹é©±åŠ¨è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€ç†è§£æ–¹æ³•åœ¨æ¨¡æ€å¯¹é½ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰è§†è§‰ä¸æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡å­—èŠ‚å¯¹ç¼–ç ç›´æ¥å°†ç»“æ„ä¿¡æ¯èå…¥è§†è§‰æ ‡è®°ï¼Œå¢å¼ºæ¨¡æ€é—´çš„å¯¹é½èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰-è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸åŒæ¨¡æ€çš„æœ‰æ•ˆå¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¯¹è§†è§‰æ ‡è®°åº”ç”¨å­—èŠ‚å¯¹ç¼–ç æ¥ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£çš„æ¡†æ¶ã€‚ä¸ä¾èµ–äºç‰¹å®šæ¨¡æ€ç¼–ç å™¨çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å°†ç»“æ„ä¿¡æ¯èå…¥è§†è§‰æ ‡è®°ä¸­ï¼Œç±»ä¼¼äºæ–‡æœ¬è¯­è¨€æ¨¡å‹ä¸­çš„æˆåŠŸæ ‡è®°åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¼˜å…ˆå¼•å¯¼ç¼–ç æ–¹æ¡ˆï¼Œè€ƒè™‘äº†é¢‘ç‡å’Œç©ºé—´ä¸€è‡´æ€§ï¼Œå¹¶ç»“åˆåŸºäºè¯¾ç¨‹é©±åŠ¨çš„æ•°æ®ç»„åˆçš„å¤šé˜¶æ®µè®­ç»ƒç¨‹åºã€‚è¿™äº›å¢å¼ºä½¿å˜æ¢å™¨æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è·¨æ¨¡æ€å…³ç³»å¹¶æ¨ç†è§†è§‰ä¿¡æ¯ã€‚å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚é€šè¿‡å¼¥åˆè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ›´å¼ºå¤§å’Œé«˜æ•ˆçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç†è§£ä¸­ä¸åŒæ¨¡æ€å¯¹é½çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç‰¹å®šæ¨¡æ€çš„ç¼–ç å™¨ï¼Œå¯¼è‡´ä¿¡æ¯æ•´åˆä¸å¤Ÿæœ‰æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡å­—èŠ‚å¯¹ç¼–ç å°†ç»“æ„ä¿¡æ¯ç›´æ¥èå…¥è§†è§‰æ ‡è®°ï¼Œå€Ÿé‰´æ–‡æœ¬è¯­è¨€æ¨¡å‹çš„æˆåŠŸç»éªŒï¼Œä»è€Œæå‡æ¨¡æ€é—´çš„å¯¹é½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¼˜å…ˆå¼•å¯¼ç¼–ç æ–¹æ¡ˆå’Œå¤šé˜¶æ®µè®­ç»ƒç¨‹åºï¼Œå‰è€…è€ƒè™‘é¢‘ç‡å’Œç©ºé—´ä¸€è‡´æ€§ï¼Œåè€…é€šè¿‡è¯¾ç¨‹é©±åŠ¨çš„æ•°æ®ç»„åˆé€æ­¥æå‡æ¨¡å‹èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å­—èŠ‚å¯¹ç¼–ç åº”ç”¨äºè§†è§‰æ ‡è®°ï¼Œç›´æ¥æ•´åˆç»“æ„ä¿¡æ¯ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æ¨¡æ€ç‰¹å®šç¼–ç æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¼˜å…ˆå¼•å¯¼ç¼–ç çš„ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†æ¨¡æ€é—´çš„äº¤äº’ä¿¡æ¯ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒæè¿°ç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è§†è§‰åŠ©æ‰‹ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆå’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è§†è§‰ä¸æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€äº¤äº’æä¾›æ›´ä¸ºç²¾å‡†çš„æ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—å’Œå¨±ä¹ç­‰è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.

