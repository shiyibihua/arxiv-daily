---
layout: default
title: EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models
---

# EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11886" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11886v1</a>
  <a href="https://arxiv.org/pdf/2508.11886.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11886v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11886v1', 'EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Shao Tang, Sayan Ghosh, Xuanzhao Dong, Rajat Koner, Yalin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEVTP-IVSä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰åˆ†å‰²æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»¤è§†è§‰åˆ†å‰²` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ ‡è®°ä¿®å‰ª` `æ¨ç†æ•ˆç‡` `ç©ºé—´ä¿¡æ¯æ•´åˆ` `ä¿¡æ¯è®ºåˆ†æ` `è§†é¢‘å¤„ç†` `å›¾åƒå¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è§†è§‰åˆ†å‰²ä»»åŠ¡ä¸­æ¨ç†æˆæœ¬é«˜ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘å¤„ç†æ—¶ï¼Œæˆä¸ºä¸»è¦ç“¶é¢ˆã€‚
2. æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•EVTP-IVï¼Œé€šè¿‡æ•´åˆç©ºé—´ä¿¡æ¯é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æ ‡è®°å­é›†ï¼Œä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEVTP-IVåœ¨è§†é¢‘ä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜5å€çš„é€Ÿåº¦æå‡ï¼Œåœ¨å›¾åƒä»»åŠ¡ä¸Šå®ç°äº†3.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬è®ºæ–‡é’ˆå¯¹æŒ‡ä»¤è§†è§‰åˆ†å‰²ï¼ˆIVSï¼‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•EVTP-IVï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒå’Œè§†é¢‘å¤„ç†ä¸­çš„æ¨ç†æ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰æ ‡è®°çš„å­é›†è¦†ç›–ç‡ä¸åˆ†å‰²æ€§èƒ½ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ï¼Œå› æ­¤è®¾è®¡äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œé€‰æ‹©ä¸€ä¸ªç´§å‡‘ä¸”ç©ºé—´ä¸Šå…·æœ‰ä»£è¡¨æ€§çš„æ ‡è®°å­é›†ä»¥åŠ é€Ÿæ¨ç†ã€‚é€šè¿‡ä¿¡æ¯è®ºåˆ†ææ”¯æŒè®¾è®¡æ€è·¯ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜5å€çš„é€Ÿåº¦æå‡ï¼Œåœ¨å›¾åƒä»»åŠ¡ä¸Šå®ç°äº†3.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä»…ä½¿ç”¨20%çš„æ ‡è®°ä¿æŒäº†ç›¸å½“çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒçš„ä¿®å‰ªæ¯”ä¾‹ä¸‹å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ä¿®å‰ªåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æŒ‡ä»¤è§†è§‰åˆ†å‰²ï¼ˆIVSï¼‰ä»»åŠ¡ä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†é¢‘æ—¶æ¨ç†æˆæœ¬é«˜ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºEVTP-IVçš„è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œé€šè¿‡åˆ†æè§†è§‰æ ‡è®°çš„å­é›†è¦†ç›–ç‡ä¸åˆ†å‰²æ€§èƒ½çš„å…³ç³»ï¼Œè®¾è®¡äº†ä¸€ç§é€‰æ‹©ç´§å‡‘ä¸”ç©ºé—´ä¸Šå…·æœ‰ä»£è¡¨æ€§çš„æ ‡è®°å­é›†çš„ç­–ç•¥ï¼Œä»¥åŠ é€Ÿæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯è§†è§‰æ ‡è®°çš„é‡‡æ ·ä¸åˆ†æï¼Œå…¶æ¬¡æ˜¯åŸºäºkä¸­å¿ƒç®—æ³•çš„æ ‡è®°ä¿®å‰ªï¼Œæœ€åæ˜¯ä¿¡æ¯è®ºåˆ†æä»¥æ”¯æŒè®¾è®¡å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç©ºé—´ä¿¡æ¯æ•´åˆåˆ°æ ‡è®°é€‰æ‹©è¿‡ç¨‹ä¸­ï¼Œç¡®ä¿æ‰€é€‰æ ‡è®°èƒ½å¤Ÿæ›´å¥½åœ°è¦†ç›–è¾“å…¥æ•°æ®çš„ç‰¹å¾ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¼ºè°ƒç©ºé—´ä»£è¡¨æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯æ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡ä½¿ç”¨äº†ä¿¡æ¯è®ºä¸­çš„è¦†ç›–ç‡æŒ‡æ ‡æ¥è¯„ä¼°æ ‡è®°çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†ä¸æ ‡è®°é€‰æ‹©ç›¸å…³çš„æƒé‡ï¼Œä»¥ä¼˜åŒ–æ ‡è®°çš„é€‰æ‹©è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEVTP-IVåœ¨è§†é¢‘ä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜5å€çš„é€Ÿåº¦æå‡ï¼Œåœ¨å›¾åƒä»»åŠ¡ä¸Šå®ç°äº†3.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä»…ä½¿ç”¨20%çš„æ ‡è®°ä¿æŒäº†ç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„ä¿®å‰ªæ¯”ä¾‹ä¸‹å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ä¿®å‰ªåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘åˆ†æç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å®æ—¶è§†è§‰ç†è§£å’Œäº¤äº’å¼åº”ç”¨ä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instructed Visual Segmentation (IVS) tasks require segmenting objects in images or videos based on natural language instructions. While recent multimodal large language models (MLLMs) have achieved strong performance on IVS, their inference cost remains a major bottleneck, particularly in video. We empirically analyze visual token sampling in MLLMs and observe a strong correlation between subset token coverage and segmentation performance. This motivates our design of a simple and effective token pruning method that selects a compact yet spatially representative subset of tokens to accelerate inference. In this paper, we introduce a novel visual token pruning method for IVS, called EVTP-IV, which builds upon the k-center by integrating spatial information to ensure better coverage. We further provide an information-theoretic analysis to support our design. Experiments on standard IVS benchmarks show that our method achieves up to 5X speed-up on video tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens. Our method also consistently outperforms state-of-the-art pruning baselines under varying pruning ratios.

