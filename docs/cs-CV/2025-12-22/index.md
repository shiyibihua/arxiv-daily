---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-22
---

# cs.CVï¼ˆ2025-12-22ï¼‰

ğŸ“Š å…± **26** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251219115v1-generative-giants-retrieval-weaklings-why-do-multimodal-large-langua.html">Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</a></td>
  <td>æ­ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸­è¡¨ç°ä¸ä½³çš„åŸå› </td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19115v1" data-paper-url="./papers/251219115v1-generative-giants-retrieval-weaklings-why-do-multimodal-large-langua.html" onclick="toggleFavorite(this, '2512.19115v1', 'Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251219675v1-multimodal-llms-for-historical-dataset-construction-from-archival-im.html">Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</a></td>
  <td>åˆ©ç”¨å¤šæ¨¡æ€LLMä»æ¡£æ¡ˆå›¾åƒæ‰«æä»¶æ„å»ºå¾·å›½ä¸“åˆ©å†å²æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19675v1" data-paper-url="./papers/251219675v1-multimodal-llms-for-historical-dataset-construction-from-archival-im.html" onclick="toggleFavorite(this, '2512.19675v1', 'Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251219354v1-reasoncd-a-multimodal-reasoning-large-model-for-implicit-change-of-i.html">ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining</a></td>
  <td>ReasonCDï¼šæå‡ºå¤šæ¨¡æ€æ¨ç†å¤§æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå›¾åƒéšå¼å…´è¶£å˜æ›´è¯­ä¹‰æŒ–æ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19354v1" data-paper-url="./papers/251219354v1-reasoncd-a-multimodal-reasoning-large-model-for-implicit-change-of-i.html" onclick="toggleFavorite(this, '2512.19354v1', 'ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251219663v1-beyond-clip-knowledge-enhanced-multimodal-transformers-for-cross-mod.html">Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</a></td>
  <td>æå‡ºçŸ¥è¯†å¢å¼ºå¤šæ¨¡æ€Transformerï¼Œç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜è¯Šæ–­ä¸­çš„è·¨æ¨¡æ€å¯¹é½</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19663v1" data-paper-url="./papers/251219663v1-beyond-clip-knowledge-enhanced-multimodal-transformers-for-cross-mod.html" onclick="toggleFavorite(this, '2512.19663v1', 'Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251219433v1-dmllm-tts-self-verified-and-efficient-test-time-scaling-for-diffusio.html">dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</a></td>
  <td>æå‡ºdMLLM-TTSï¼Œé€šè¿‡è‡ªéªŒè¯å’Œé«˜æ•ˆæµ‹è¯•æ—¶ç¼©æ”¾æå‡æ‰©æ•£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19433v1" data-paper-url="./papers/251219433v1-dmllm-tts-self-verified-and-efficient-test-time-scaling-for-diffusio.html" onclick="toggleFavorite(this, '2512.19433v1', 'dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251219683v1-from-indoor-to-open-world-revealing-the-spatial-reasoning-gap-in-mll.html">From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</a></td>
  <td>æ­ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç©ºé—´æ¨ç†æ–¹é¢çš„å·®è·ï¼Œå¹¶æå‡ºç›¸åº”è¯„æµ‹åŸºå‡†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19683v1" data-paper-url="./papers/251219683v1-from-indoor-to-open-world-revealing-the-spatial-reasoning-gap-in-mll.html" onclick="toggleFavorite(this, '2512.19683v1', 'From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251219609v1-maptrace-scalable-data-generation-for-route-tracing-on-maps.html">MapTrace: Scalable Data Generation for Route Tracing on Maps</a></td>
  <td>MapTraceï¼šæå‡ºå¯æ‰©å±•æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œæå‡MLLMåœ°å›¾è·¯å¾„è¿½è¸ªèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19609v1" data-paper-url="./papers/251219609v1-maptrace-scalable-data-generation-for-route-tracing-on-maps.html" onclick="toggleFavorite(this, '2512.19609v1', 'MapTrace: Scalable Data Generation for Route Tracing on Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251219443v1-d2pruner-debiased-importance-and-structural-diversity-for-mllm-token.html">D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</a></td>
  <td>D2Prunerï¼šé€šè¿‡è§£åé‡è¦æ€§å’Œç»“æ„å¤šæ ·æ€§å®ç°MLLM Tokenå‰ªæ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19443v1" data-paper-url="./papers/251219443v1-d2pruner-debiased-importance-and-structural-diversity-for-mllm-token.html" onclick="toggleFavorite(this, '2512.19443v1', 'D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251219686v1-visual-aware-cot-achieving-high-fidelity-visual-consistency-in-unifi.html">Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</a></td>
  <td>æå‡ºVisual-Aware CoTï¼Œæå‡ç»Ÿä¸€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç”Ÿæˆä¸­è§†è§‰ä¸€è‡´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19686v1" data-paper-url="./papers/251219686v1-visual-aware-cot-achieving-high-fidelity-visual-consistency-in-unifi.html" onclick="toggleFavorite(this, '2512.19686v1', 'Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251219535v1-casa-cross-attention-via-self-attention-for-efficient-vision-languag.html">CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</a></td>
  <td>æå‡ºCASAï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›å®ç°çš„è·¨æ³¨æ„åŠ›ï¼Œç”¨äºé«˜æ•ˆçš„è§†è§‰-è¯­è¨€èåˆ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19535v1" data-paper-url="./papers/251219535v1-casa-cross-attention-via-self-attention-for-efficient-vision-languag.html" onclick="toggleFavorite(this, '2512.19535v1', 'CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/251219648v1-4d-gaussian-splatting-as-a-learned-dynamical-system.html">4D Gaussian Splatting as a Learned Dynamical System</a></td>
  <td>EvoGSï¼šå°†4Dé«˜æ–¯æº…å°„é‡æ„ä¸ºå¯å­¦ä¹ çš„åŠ¨æ€ç³»ç»Ÿï¼Œå®ç°æ—¶åºä¸€è‡´æ€§åŠ¨æ€åœºæ™¯å»ºæ¨¡</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19648v1" data-paper-url="./papers/251219648v1-4d-gaussian-splatting-as-a-learned-dynamical-system.html" onclick="toggleFavorite(this, '2512.19648v1', '4D Gaussian Splatting as a Learned Dynamical System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251219108v1-gaussianimage-boosted-image-representation-and-compression-with-2d-g.html">GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting</a></td>
  <td>GaussianImage++ï¼šåˆ©ç”¨2Dé«˜æ–¯æº…å°„å¢å¼ºå›¾åƒè¡¨ç¤ºä¸å‹ç¼©æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19108v1" data-paper-url="./papers/251219108v1-gaussianimage-boosted-image-representation-and-compression-with-2d-g.html" onclick="toggleFavorite(this, '2512.19108v1', 'GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251219088v1-retrieving-objects-from-3d-scenes-with-box-guided-open-vocabulary-in.html">Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation</a></td>
  <td>æå‡ºåŸºäº2Dæ¡†å¼•å¯¼çš„å¼€æ”¾è¯æ±‡å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºä»3Dåœºæ™¯ä¸­æ£€ç´¢ç›®æ ‡</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19088v1" data-paper-url="./papers/251219088v1-retrieving-objects-from-3d-scenes-with-box-guided-open-vocabulary-in.html" onclick="toggleFavorite(this, '2512.19088v1', 'Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251219020v1-cetcam-camera-controllable-video-generation-via-consistent-and-exten.html">CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</a></td>
  <td>CETCAMï¼šé€šè¿‡ä¸€è‡´ä¸”å¯æ‰©å±•çš„TokenåŒ–å®ç°ç›¸æœºå¯æ§çš„è§†é¢‘ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">VGGT</span> <span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19020v1" data-paper-url="./papers/251219020v1-cetcam-camera-controllable-video-generation-via-consistent-and-exten.html" onclick="toggleFavorite(this, '2512.19020v1', 'CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251219678v1-worldwarp-propagating-3d-geometry-with-asynchronous-video-diffusion.html">WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</a></td>
  <td>WorldWarpï¼šåˆ©ç”¨å¼‚æ­¥è§†é¢‘æ‰©æ•£ä¼ æ’­3Då‡ ä½•ä¿¡æ¯ï¼Œç”Ÿæˆé•¿æ—¶å‡ ä½•ä¸€è‡´æ€§è§†é¢‘ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19678v1" data-paper-url="./papers/251219678v1-worldwarp-propagating-3d-geometry-with-asynchronous-video-diffusion.html" onclick="toggleFavorite(this, '2512.19678v1', 'WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/251219512v1-anatomy-r1-enhancing-anatomy-reasoning-in-multimodal-large-language-.html">Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</a></td>
  <td>Anatomy-R1ï¼šé€šè¿‡è§£å‰–ç›¸ä¼¼æ€§è¯¾ç¨‹å­¦ä¹ å’Œç¾¤ä½“å¤šæ ·æ€§å¢å¼ºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§£å‰–æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">curriculum learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19512v1" data-paper-url="./papers/251219512v1-anatomy-r1-enhancing-anatomy-reasoning-in-multimodal-large-language-.html" onclick="toggleFavorite(this, '2512.19512v1', 'Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251219687v1-pushing-the-frontier-of-audiovisual-perception-with-large-scale-mult.html">Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</a></td>
  <td>æå‡ºPE-AVï¼šåŸºäºå¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ çš„éŸ³è§†é¢‘æ„ŸçŸ¥ç»Ÿä¸€ç¼–ç å™¨ï¼Œå®ç°è·¨æ¨¡æ€å¯¹é½ä¸æ£€ç´¢ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19687v1" data-paper-url="./papers/251219687v1-pushing-the-frontier-of-audiovisual-perception-with-large-scale-mult.html" onclick="toggleFavorite(this, '2512.19687v1', 'Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251219504v1-fusionnet-physics-aware-representation-learning-for-multi-spectral-a.html">FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</a></td>
  <td>FusionNetï¼šé€šè¿‡å¯è®­ç»ƒä¿¡å·å¤„ç†å…ˆéªŒå®ç°å¤šå…‰è°±ä¸çƒ­æ•°æ®çš„ç‰©ç†æ„ŸçŸ¥è¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19504v1" data-paper-url="./papers/251219504v1-fusionnet-physics-aware-representation-learning-for-multi-spectral-a.html" onclick="toggleFavorite(this, '2512.19504v1', 'FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251219048v1-waterflow-watermark-temporal-robustness-via-flow-consistency.html">WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</a></td>
  <td>æå‡ºWaTeRFlowæ¡†æ¶ï¼Œå¢å¼ºæ°´å°åœ¨å›¾åƒè½¬è§†é¢‘ä¸­çš„æ—¶é—´é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19048v1" data-paper-url="./papers/251219048v1-waterflow-watermark-temporal-robustness-via-flow-consistency.html" onclick="toggleFavorite(this, '2512.19048v1', 'WaTeRFlow: Watermark Temporal Robustness via Flow Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/251219049v1-decoupled-generative-modeling-for-human-object-interaction-synthesis.html">Decoupled Generative Modeling for Human-Object Interaction Synthesis</a></td>
  <td>æå‡ºDecHOIï¼Œè§£è€¦è·¯å¾„è§„åˆ’ä¸åŠ¨ä½œç”Ÿæˆï¼Œå®ç°é€¼çœŸçš„äºº-ç‰©äº¤äº’åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">penetration</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19049v1" data-paper-url="./papers/251219049v1-decoupled-generative-modeling-for-human-object-interaction-synthesis.html" onclick="toggleFavorite(this, '2512.19049v1', 'Decoupled Generative Modeling for Human-Object Interaction Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251219684v1-zero-shot-reconstruction-of-in-scene-object-manipulation-from-video.html">Zero-shot Reconstruction of In-Scene Object Manipulation from Video</a></td>
  <td>æå‡ºé¦–ä¸ªç³»ç»Ÿï¼Œä»å•ç›®è§†é¢‘é›¶æ ·æœ¬é‡å»ºåœºæ™¯å†…ç‰©ä½“æ“ä½œè¿‡ç¨‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene reconstruction</span> <span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19684v1" data-paper-url="./papers/251219684v1-zero-shot-reconstruction-of-in-scene-object-manipulation-from-video.html" onclick="toggleFavorite(this, '2512.19684v1', 'Zero-shot Reconstruction of In-Scene Object Manipulation from Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251219021v1-vlnverse-a-benchmark-for-vision-language-navigation-with-versatile-e.html">VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation</a></td>
  <td>VLNVerseï¼šç”¨äºè§†è§‰-è¯­è¨€å¯¼èˆªçš„å¤šåŠŸèƒ½ã€å…·èº«ã€é€¼çœŸæ¨¡æ‹Ÿä¸è¯„ä¼°åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19021v1" data-paper-url="./papers/251219021v1-vlnverse-a-benchmark-for-vision-language-navigation-with-versatile-e.html" onclick="toggleFavorite(this, '2512.19021v1', 'VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251219190v1-pedestrian-an-egocentric-vision-dataset-for-obstacle-detection-on-pa.html">PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</a></td>
  <td>æå‡ºè¡Œäººè§†è§’éšœç¢ç‰©æ£€æµ‹æ•°æ®é›†PEDESTRIANï¼Œç”¨äºæå‡åŸå¸‚äººè¡Œé“å®‰å…¨ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19190v1" data-paper-url="./papers/251219190v1-pedestrian-an-egocentric-vision-dataset-for-obstacle-detection-on-pa.html" onclick="toggleFavorite(this, '2512.19190v1', 'PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251219283v1-hand-aware-egocentric-motion-reconstruction-with-sequence-level-cont.html">Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</a></td>
  <td>æå‡ºHaMoSï¼šä¸€ç§æ‰‹éƒ¨æ„ŸçŸ¥çš„åºåˆ—çº§è‡ªä¸­å¿ƒè¿åŠ¨é‡å»ºæ‰©æ•£æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19283v1" data-paper-url="./papers/251219283v1-hand-aware-egocentric-motion-reconstruction-with-sequence-level-cont.html" onclick="toggleFavorite(this, '2512.19283v1', 'Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/251219159v1-omnimogen-unifying-human-motion-generation-via-learning-from-interle.html">OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</a></td>
  <td>OmniMoGenï¼šé€šè¿‡å­¦ä¹ äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ï¼Œç»Ÿä¸€äº†äººä½“è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span> <span class="paper-tag">VQ-VAE</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.19159v1" data-paper-url="./papers/251219159v1-omnimogen-unifying-human-motion-generation-via-learning-from-interle.html" onclick="toggleFavorite(this, '2512.19159v1', 'OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251218994v1-towards-ai-guided-open-world-ecological-taxonomic-classification.html">Towards AI-Guided Open-World Ecological Taxonomic Classification</a></td>
  <td>æå‡ºTaxoNetï¼Œè§£å†³å¼€æ”¾ä¸–ç•Œç”Ÿæ€åˆ†ç±»ä¸­çš„é•¿å°¾åˆ†å¸ƒå’Œé¢†åŸŸåç§»é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.18994v1" data-paper-url="./papers/251218994v1-towards-ai-guided-open-world-ecological-taxonomic-classification.html" onclick="toggleFavorite(this, '2512.18994v1', 'Towards AI-Guided Open-World Ecological Taxonomic Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)