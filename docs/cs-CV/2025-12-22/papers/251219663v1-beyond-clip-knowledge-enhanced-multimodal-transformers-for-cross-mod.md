---
layout: default
title: Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis
---

# Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19663" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19663v1</a>
  <a href="https://arxiv.org/pdf/2512.19663.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19663v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19663v1', 'Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

**å¤‡æ³¨**: 14 pages, 14 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†å¢å¼ºå¤šæ¨¡æ€Transformerï¼Œç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜è¯Šæ–­ä¸­çš„è·¨æ¨¡æ€å¯¹é½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç³–å°¿ç—…è§†ç½‘è†œç—…å˜` `å¤šæ¨¡æ€å­¦ä¹ ` `è·¨æ¨¡æ€å¯¹é½` `åŒ»å­¦å›¾åƒåˆ†æ` `çŸ¥è¯†å¢å¼º` `Transformer` `å¯¹æ¯”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CLIPç­‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆå¯¹é½å›¾åƒå’Œä¸´åºŠæ–‡æœ¬ã€‚
2. æå‡ºä¸€ç§çŸ¥è¯†å¢å¼ºçš„å¤šæ¨¡æ€Transformeræ¡†æ¶ï¼Œèåˆè§†ç½‘è†œå›¾åƒã€ä¸´åºŠæ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®ï¼Œå®ç°è·¨æ¨¡æ€å¯¹é½ã€‚
3. åœ¨BRSETæ•°æ®é›†ä¸Šï¼Œæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢Recall@1è¾¾åˆ°99.94%ï¼Œæ˜¾è‘—ä¼˜äºå¾®è°ƒçš„CLIPï¼Œå¹¶åœ¨DeepEyeNetæ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç³–å°¿ç—…è§†ç½‘è†œç—…å˜(DR)æ˜¯å…¨çƒå¯é¢„é˜²æ€§å¤±æ˜çš„ä¸»è¦åŸå› ï¼Œéœ€è¦ç²¾ç¡®çš„è‡ªåŠ¨åŒ–è¯Šæ–­ç³»ç»Ÿã€‚é€šç”¨é¢†åŸŸçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨è‡ªç„¶å›¾åƒä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸåº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨çœ¼ç§‘å›¾åƒçš„è·¨æ¨¡æ€æ£€ç´¢æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„çŸ¥è¯†å¢å¼ºè”åˆåµŒå…¥æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€Transformeræ¶æ„æ•´åˆè§†ç½‘è†œçœ¼åº•å›¾åƒã€ä¸´åºŠæ–‡æœ¬å’Œç»“æ„åŒ–æ‚£è€…æ•°æ®ï¼Œä»¥è§£å†³åŒ»å­¦å›¾åƒ-æ–‡æœ¬å¯¹é½çš„å…³é”®å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¯ä¸ªæ¨¡æ€é‡‡ç”¨å•ç‹¬çš„ç¼–ç å™¨ï¼šç”¨äºè§†ç½‘è†œå›¾åƒçš„Vision Transformer (ViT-B/16)ã€ç”¨äºä¸´åºŠå™è¿°çš„Bio-ClinicalBERTä»¥åŠç”¨äºç»“æ„åŒ–äººå£ç»Ÿè®¡å­¦å’Œä¸´åºŠç‰¹å¾çš„å¤šå±‚æ„ŸçŸ¥å™¨ã€‚è¿™äº›æ¨¡æ€é€šè¿‡å…·æœ‰æ¨¡æ€ç‰¹å®šåµŒå…¥çš„è”åˆTransformerèåˆï¼Œå¹¶ä½¿ç”¨åŒ…æ‹¬æ¨¡æ€å¯¹ä¹‹é—´çš„å¯¹æ¯”æŸå¤±ã€å›¾åƒå’Œæ–‡æœ¬çš„é‡å»ºæŸå¤±ä»¥åŠæ ¹æ®ICDRå’ŒSDRGæ–¹æ¡ˆè¿›è¡ŒDRä¸¥é‡ç¨‹åº¦åˆ†çº§çš„åˆ†ç±»æŸå¤±åœ¨å†…çš„å¤šä¸ªç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚åœ¨å·´è¥¿å¤šæ ‡ç­¾çœ¼ç§‘æ•°æ®é›†(BRSET)ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†è¿‘ä¹å®Œç¾çš„æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½ï¼ŒRecall@1ä¸º99.94%ï¼Œè€Œå¾®è°ƒçš„CLIPä¸º1.29%ï¼ŒåŒæ—¶ä¿æŒäº†æœ€å…ˆè¿›çš„SDRGåˆ†ç±»ç²¾åº¦97.05%å’ŒICDRåˆ†ç±»ç²¾åº¦97.97%ã€‚æ­¤å¤–ï¼Œåœ¨æœªè§è¿‡çš„DeepEyeNetæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è¯„ä¼°éªŒè¯äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒRecall@1ä¸º93.95%ï¼Œè€Œå¾®è°ƒçš„CLIPä¸º0.22%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ³•æœ‰æ•ˆåœ°æ•æ‰äº†åŒ»å­¦é¢†åŸŸçš„è·¨æ¨¡æ€å…³ç³»ï¼Œä»è€Œå»ºç«‹äº†å“è¶Šçš„æ£€ç´¢èƒ½åŠ›å’Œå¼ºå¤§çš„è¯Šæ–­æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDRï¼‰è¯Šæ–­ä¸­ï¼ŒåŒ»å­¦å›¾åƒï¼ˆçœ¼åº•å›¾åƒï¼‰ä¸ä¸´åºŠæ–‡æœ¬ä¿¡æ¯ä¹‹é—´è·¨æ¨¡æ€å¯¹é½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨é€šç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¿›è¡Œå¾®è°ƒï¼Œåœ¨åŒ»å­¦é¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¯¼è‡´æ£€ç´¢å’Œè¯Šæ–­æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çŸ¥è¯†å¢å¼ºçš„å¤šæ¨¡æ€Transformeræ¶æ„ï¼Œæ˜¾å¼åœ°èåˆæ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆå›¾åƒã€æ–‡æœ¬ã€ç»“æ„åŒ–æ•°æ®ï¼‰çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¤šç›®æ ‡å­¦ä¹ ç­–ç•¥ï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´é²æ£’å’Œå‡†ç¡®çš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚é€šè¿‡å¼•å…¥åŒ»å­¦é¢†åŸŸçš„å…ˆéªŒçŸ¥è¯†ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ¨¡æ€ç¼–ç å™¨ï¼šåˆ†åˆ«ä½¿ç”¨ViT-B/16ç¼–ç çœ¼åº•å›¾åƒï¼ŒBio-ClinicalBERTç¼–ç ä¸´åºŠæ–‡æœ¬ï¼Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç¼–ç ç»“æ„åŒ–æ•°æ®ã€‚2) è”åˆTransformerï¼šå°†ä¸åŒæ¨¡æ€çš„åµŒå…¥å‘é‡è¾“å…¥åˆ°è”åˆTransformerä¸­ï¼Œè¿›è¡Œè·¨æ¨¡æ€ç‰¹å¾èåˆã€‚3) å¤šç›®æ ‡å­¦ä¹ ï¼šä½¿ç”¨å¯¹æ¯”æŸå¤±ï¼ˆæ¨¡æ€å¯¹ä¹‹é—´ï¼‰ã€é‡å»ºæŸå¤±ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆDRä¸¥é‡ç¨‹åº¦åˆ†çº§ï¼‰è¿›è¡Œè”åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºçŸ¥è¯†å¢å¼ºçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ã€‚ä¸ç›´æ¥å¾®è°ƒé€šç”¨æ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•é’ˆå¯¹åŒ»å­¦é¢†åŸŸç‰¹ç‚¹ï¼Œè®¾è®¡äº†ä¸“é—¨çš„æ¨¡æ€ç¼–ç å™¨ï¼ˆå¦‚Bio-ClinicalBERTï¼‰å’Œå¤šç›®æ ‡å­¦ä¹ ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰åŒ»å­¦å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³è”ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Bio-ClinicalBERTä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œåˆ©ç”¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„é¢„è®­ç»ƒçŸ¥è¯†ã€‚2) è®¾è®¡äº†å¤šç›®æ ‡æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬å¯¹æ¯”æŸå¤±ã€é‡å»ºæŸå¤±å’Œåˆ†ç±»æŸå¤±ï¼Œä»¥ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½å’Œè¯Šæ–­æ€§èƒ½ã€‚3) ä½¿ç”¨æ¨¡æ€ç‰¹å®šçš„åµŒå…¥ï¼Œæ›´å¥½åœ°è¡¨ç¤ºä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19663v1/dr_grades_distribution_log.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19663v1/anatomical_status_distribution.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19663v1/other_pathological_features_prevalence.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨BRSETæ•°æ®é›†ä¸Šå®ç°äº†è¿‘ä¹å®Œç¾çš„æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½ï¼ŒRecall@1è¾¾åˆ°99.94%ï¼Œæ˜¾è‘—ä¼˜äºå¾®è°ƒçš„CLIPï¼ˆ1.29%ï¼‰ã€‚åŒæ—¶ï¼Œåœ¨SDRGå’ŒICDRåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œåˆ†åˆ«å–å¾—äº†97.05%å’Œ97.97%çš„åˆ†ç±»ç²¾åº¦ã€‚åœ¨æœªè§è¿‡çš„DeepEyeNetæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒRecall@1ä¸º93.95%ï¼Œè¿œè¶…å¾®è°ƒçš„CLIPï¼ˆ0.22%ï¼‰ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„è‡ªåŠ¨åŒ–è¯Šæ–­ç³»ç»Ÿï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œå¿«é€Ÿå‡†ç¡®çš„è¯Šæ–­ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—èµ„æºåŒ®ä¹çš„åœ°åŒºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–åŒ»å­¦å›¾åƒ-æ–‡æœ¬å¯¹é½ä»»åŠ¡ï¼Œä¾‹å¦‚æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€ç—…ç†å›¾åƒåˆ†æç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.

