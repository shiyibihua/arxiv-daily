---
layout: default
title: OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions
---

# OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19159" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19159v1</a>
  <a href="https://arxiv.org/pdf/2512.19159.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19159v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19159v1', 'OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wendong Bu, Kaihang Pan, Yuze Lin, Jiacheng Li, Kai Shen, Wenqiao Zhang, Juncheng Li, Jun Xiao, Siliang Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://OmniMoGen.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniMoGenï¼šé€šè¿‡å­¦ä¹ äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ï¼Œç»Ÿä¸€äº†äººä½“è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººä½“è¿åŠ¨ç”Ÿæˆ` `æ–‡æœ¬åˆ°è¿åŠ¨` `è¿åŠ¨ç¼–è¾‘` `äº¤é”™æŒ‡ä»¤å­¦ä¹ ` `ç»Ÿä¸€æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿åŠ¨ç”Ÿæˆæ–¹æ³•å±€é™äºå­¤ç«‹ä»»åŠ¡ï¼Œç¼ºä¹è‡ªç”±å½¢å¼å’Œå…¨ç›®æ ‡ç”Ÿæˆçš„çµæ´»æ€§ã€‚
2. OmniMoGené€šè¿‡å­¦ä¹ äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ï¼Œç»Ÿä¸€äº†å¤šç§è¿åŠ¨ç”Ÿæˆä»»åŠ¡ï¼Œå®ç°é€šç”¨è¿åŠ¨ç”Ÿæˆã€‚
3. OmniMoGenåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€è¿åŠ¨ç¼–è¾‘å’ŒAnyContextåŸºå‡†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶å±•ç°äº†æ–°å…´èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²åœ¨å•ä¸ªæ¡†æ¶å†…ç»Ÿä¸€äº†å„ç§è¯­è¨€ä»»åŠ¡ï¼Œä½†è¿™ç§ç»Ÿä¸€æ€§åœ¨äººä½“è¿åŠ¨ç”Ÿæˆä¸­ä»æœªè¢«æ¢ç´¢ã€‚ç°æœ‰æ–¹æ³•ä»…é™äºå­¤ç«‹çš„ä»»åŠ¡ï¼Œé™åˆ¶äº†è‡ªç”±å½¢å¼å’Œå…¨ç›®æ ‡ç”Ÿæˆçš„çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniMoGenï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤å®ç°å¤šåŠŸèƒ½çš„è¿åŠ¨ç”Ÿæˆã€‚OmniMoGenå»ºç«‹åœ¨ä¸€ä¸ªç®€æ´çš„RVQ-VAEå’ŒTransformeræ¶æ„ä¹‹ä¸Šï¼Œæ”¯æŒç«¯åˆ°ç«¯çš„æŒ‡ä»¤é©±åŠ¨è¿åŠ¨ç”Ÿæˆã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„X2Moæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡137Kçš„äº¤é”™æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ï¼Œå¹¶å¼•å…¥äº†AnyContextï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°äº¤é”™è¿åŠ¨ç”Ÿæˆçš„åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniMoGenåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€è¿åŠ¨ç¼–è¾‘å’ŒAnyContextä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç°äº†è¯¸å¦‚ç»„åˆç¼–è¾‘ã€è‡ªæˆ‘åæ€ç”Ÿæˆå’ŒçŸ¥è¯†é©±åŠ¨ç”Ÿæˆç­‰æ–°å…´èƒ½åŠ›ã€‚è¿™äº›ç»“æœæ ‡å¿—ç€æœç€ä¸‹ä¸€ä»£æ™ºèƒ½è¿åŠ¨ç”Ÿæˆè¿ˆå‡ºäº†ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººä½“è¿åŠ¨ç”Ÿæˆæ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°è¿åŠ¨ã€è¿åŠ¨ç¼–è¾‘ç­‰ï¼Œç¼ºä¹é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚ç”¨æˆ·éš¾ä»¥é€šè¿‡è‡ªç”±ç»„åˆæ–‡æœ¬å’ŒåŠ¨ä½œæŒ‡ä»¤æ¥æ§åˆ¶è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ï¼Œé™åˆ¶äº†åº”ç”¨åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚çš„ã€å¤šç›®æ ‡çš„è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å„ç§è¿åŠ¨ç”Ÿæˆä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸‹ï¼Œé€šè¿‡å­¦ä¹ äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå¤æ‚çš„è¿åŠ¨åºåˆ—ã€‚é€šè¿‡å°†æ–‡æœ¬å’ŒåŠ¨ä½œè§†ä¸ºç»Ÿä¸€çš„è¾“å…¥æ¨¡æ€ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆæ›´è‡ªç„¶ã€æ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„è¿åŠ¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniMoGenæ¡†æ¶åŸºäºRVQ-VAEï¼ˆResidual Vector Quantized Variational Autoencoderï¼‰å’ŒTransformeræ¶æ„ã€‚RVQ-VAEç”¨äºå°†è¿ç»­çš„è¿åŠ¨æ•°æ®ç¦»æ•£åŒ–ä¸ºç æœ¬ï¼ŒTransformeråˆ™ç”¨äºå­¦ä¹ æ–‡æœ¬å’Œç¦»æ•£åŒ–è¿åŠ¨ç ä¹‹é—´çš„å…³ç³»ã€‚æ•´ä¸ªæ¡†æ¶æ”¯æŒç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥æ ¹æ®äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ç”Ÿæˆè¿åŠ¨åºåˆ—ã€‚æ¡†æ¶åŒ…å«ç¼–ç å™¨ã€è§£ç å™¨å’Œé‡åŒ–æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniMoGençš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„æ¡†æ¶è®¾è®¡ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å­¦ä¹ äº¤é”™çš„æ–‡æœ¬-åŠ¨ä½œæŒ‡ä»¤ï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£å¤æ‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„è¿åŠ¨ã€‚æ­¤å¤–ï¼ŒX2Moæ•°æ®é›†çš„æ„å»ºå’ŒAnyContextåŸºå‡†çš„æå‡ºï¼Œä¸ºäº¤é”™è¿åŠ¨ç”Ÿæˆçš„ç ”ç©¶æä¾›äº†æ•°æ®å’Œè¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šRVQ-VAEçš„ç æœ¬å¤§å°ã€Transformerçš„ç½‘ç»œç»“æ„ï¼ˆå±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ï¼‰ã€æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼ˆåŒ…æ‹¬é‡æ„æŸå¤±ã€é‡åŒ–æŸå¤±ç­‰ï¼‰æ˜¯å…³é”®çš„è®¾è®¡ç»†èŠ‚ã€‚è®ºæ–‡è¿˜å¯èƒ½é‡‡ç”¨äº†ç‰¹å®šçš„æ•°æ®å¢å¼ºæ–¹æ³•æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19159v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19159v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19159v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

OmniMoGenåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€è¿åŠ¨ç¼–è¾‘å’ŒAnyContextåŸºå‡†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚åœ¨AnyContextåŸºå‡†ä¸Šï¼ŒOmniMoGenæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å¼ºå¤§çš„äº¤é”™è¿åŠ¨ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒOmniMoGenè¿˜å±•ç°äº†ç»„åˆç¼–è¾‘ã€è‡ªæˆ‘åæ€ç”Ÿæˆå’ŒçŸ¥è¯†é©±åŠ¨ç”Ÿæˆç­‰æ–°å…´èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniMoGenå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆå„ç§è‡ªç„¶çš„äººä½“è¿åŠ¨ï¼Œä¾‹å¦‚è¡Œèµ°ã€è·‘æ­¥ã€è·³è·ƒã€èˆè¹ˆç­‰ã€‚é€šè¿‡ç»“åˆæ–‡æœ¬æŒ‡ä»¤ï¼Œç”¨æˆ·å¯ä»¥ç²¾ç¡®åœ°æ§åˆ¶è¿åŠ¨çš„é£æ ¼ã€é€Ÿåº¦ã€æ–¹å‘ç­‰ã€‚æ­¤å¤–ï¼ŒOmniMoGenè¿˜å¯ä»¥ç”¨äºè¿åŠ¨ç¼–è¾‘ï¼Œä¾‹å¦‚ä¿®æ”¹è¿åŠ¨çš„å§¿åŠ¿ã€èŠ‚å¥ç­‰ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½åº·å¤ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.

