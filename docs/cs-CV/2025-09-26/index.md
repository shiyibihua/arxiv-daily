---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-26
---

# cs.CVï¼ˆ2025-09-26ï¼‰

ğŸ“Š å…± **62** ç¯‡è®ºæ–‡
 | ğŸ”— **14** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (22 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (16 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ğŸ”—6)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (22 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250922415v2-explaining-multimodal-llms-via-intra-modal-token-interactions.html">Explaining multimodal LLMs via intra-modal token interactions</a></td>
  <td>é€šè¿‡æ¨¡æ€å†…tokenäº¤äº’å¢å¼ºå¤šæ¨¡æ€LLMçš„å¯è§£é‡Šæ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22415v2" data-paper-url="./papers/250922415v2-explaining-multimodal-llms-via-intra-modal-token-interactions.html" onclick="toggleFavorite(this, '2509.22415v2', 'Explaining multimodal LLMs via intra-modal token interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250921990v1-wave-learning-unified-versatile-audio-visual-embeddings-with-multimo.html">WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM</a></td>
  <td>WAVEï¼šåˆ©ç”¨å¤šæ¨¡æ€LLMå­¦ä¹ ç»Ÿä¸€ä¸”é€šç”¨çš„éŸ³è§†é¢‘åµŒå…¥</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21990v1" data-paper-url="./papers/250921990v1-wave-learning-unified-versatile-audio-visual-embeddings-with-multimo.html" onclick="toggleFavorite(this, '2509.21990v1', 'WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250922548v1-janusvln-decoupling-semantics-and-spatiality-with-dual-implicit-memo.html">JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</a></td>
  <td>JanusVLNï¼šåˆ©ç”¨åŒé‡éšå¼è®°å¿†è§£è€¦è¯­ä¹‰ä¸ç©ºé—´ä¿¡æ¯ï¼Œæå‡è§†è§‰è¯­è¨€å¯¼èˆªæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22548v1" data-paper-url="./papers/250922548v1-janusvln-decoupling-semantics-and-spatiality-with-dual-implicit-memo.html" onclick="toggleFavorite(this, '2509.22548v1', 'JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250922810v1-introducing-multimodal-paradigm-for-learning-sleep-staging-psg-via-g.html">Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-Purpose Model</a></td>
  <td>æå‡ºåŸºäºé€šç”¨å¤šæ¨¡æ€æ¨¡å‹çš„ç¡çœ åˆ†æœŸæ–°èŒƒå¼ï¼Œæå‡PSGåˆ†æçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22810v1" data-paper-url="./papers/250922810v1-introducing-multimodal-paradigm-for-learning-sleep-staging-psg-via-g.html" onclick="toggleFavorite(this, '2509.22810v1', 'Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-Purpose Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250922377v1-effectiveness-of-large-multimodal-models-in-detecting-disinformation.html">Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results</a></td>
  <td>åˆ©ç”¨GPT-4oæ¨¡å‹ï¼Œç»“åˆä¼˜åŒ–Promptå·¥ç¨‹ï¼Œè§£å†³å¤šæ¨¡æ€ä¿¡æ¯ä¼ªé€ æ£€æµ‹éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22377v1" data-paper-url="./papers/250922377v1-effectiveness-of-large-multimodal-models-in-detecting-disinformation.html" onclick="toggleFavorite(this, '2509.22377v1', 'Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250922761v2-milr-improving-multimodal-image-generation-via-test-time-latent-reas.html">MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning</a></td>
  <td>æå‡ºMILRï¼Œé€šè¿‡æµ‹è¯•æ—¶æ½œåœ¨æ¨ç†æå‡å¤šæ¨¡æ€å›¾åƒç”Ÿæˆè´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22761v2" data-paper-url="./papers/250922761v2-milr-improving-multimodal-image-generation-via-test-time-latent-reas.html" onclick="toggleFavorite(this, '2509.22761v2', 'MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250922221v1-towards-faithful-reasoning-in-remote-sensing-a-perceptually-grounded.html">Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models</a></td>
  <td>æå‡ºåŸºäºæ„ŸçŸ¥çš„åœ°ç†ç©ºé—´æ€ç»´é“¾Geo-CoTï¼Œæå‡é¥æ„Ÿè§†è§‰-è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22221v1" data-paper-url="./papers/250922221v1-towards-faithful-reasoning-in-remote-sensing-a-perceptually-grounded.html" onclick="toggleFavorite(this, '2509.22221v1', 'Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250922151v1-multimat-multimodal-program-synthesis-for-procedural-materials-using.html">MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models</a></td>
  <td>MultiMatï¼šåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œç¨‹åºåŒ–æè´¨çš„å¤šæ¨¡æ€ç¨‹åºåˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22151v1" data-paper-url="./papers/250922151v1-multimat-multimodal-program-synthesis-for-procedural-materials-using.html" onclick="toggleFavorite(this, '2509.22151v1', 'MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250921787v2-dehate-a-stable-diffusion-based-multimodal-approach-to-mitigate-hate.html">DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images</a></td>
  <td>æå‡ºåŸºäºStable Diffusionçš„å¤šæ¨¡æ€æ–¹æ³•DeHateï¼Œä»¥ç¼“è§£å›¾åƒä¸­çš„ä»‡æ¨è¨€è®º</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21787v2" data-paper-url="./papers/250921787v2-dehate-a-stable-diffusion-based-multimodal-approach-to-mitigate-hate.html" onclick="toggleFavorite(this, '2509.21787v2', 'DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250921722v1-on-the-status-of-foundation-models-for-sar-imagery.html">On the Status of Foundation Models for SAR Imagery</a></td>
  <td>æ¢ç´¢SARå›¾åƒçš„Foundation Modelï¼šè‡ªç›‘ç£å¾®è°ƒDINOv2å®ç°ç›®æ ‡è¯†åˆ«æ–°SOTA</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21722v1" data-paper-url="./papers/250921722v1-on-the-status-of-foundation-models-for-sar-imagery.html" onclick="toggleFavorite(this, '2509.21722v1', 'On the Status of Foundation Models for SAR Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250921930v1-dynanav-dynamic-feature-and-layer-selection-for-efficient-visual-nav.html">DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation</a></td>
  <td>DynaNavï¼šé’ˆå¯¹é«˜æ•ˆè§†è§‰å¯¼èˆªçš„åŠ¨æ€ç‰¹å¾ä¸å±‚é€‰æ‹©æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21930v1" data-paper-url="./papers/250921930v1-dynanav-dynamic-feature-and-layer-selection-for-efficient-visual-nav.html" onclick="toggleFavorite(this, '2509.21930v1', 'DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250922930v1-fishai-20-marine-fish-image-classification-with-multi-modal-few-shot.html">FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning</a></td>
  <td>FishAI 2.0ï¼šèåˆå¤šæ¨¡æ€å°‘æ ·æœ¬å­¦ä¹ çš„æµ·æ´‹é±¼ç±»å›¾åƒåˆ†ç±»æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22930v1" data-paper-url="./papers/250922930v1-fishai-20-marine-fish-image-classification-with-multi-modal-few-shot.html" onclick="toggleFavorite(this, '2509.22930v1', 'FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250922631v1-labeling-copilot-a-deep-research-agent-for-automated-data-curation-i.html">LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</a></td>
  <td>æå‡ºLabeling Copilotï¼Œç”¨äºè®¡ç®—æœºè§†è§‰ä¸­è‡ªåŠ¨åŒ–æ•°æ®æ ‡æ³¨çš„æ·±åº¦ç ”ç©¶Agentã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22631v1" data-paper-url="./papers/250922631v1-labeling-copilot-a-deep-research-agent-for-automated-data-curation-i.html" onclick="toggleFavorite(this, '2509.22631v1', 'LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250922628v2-uml-cot-structured-reasoning-and-planning-with-unified-modeling-lang.html">UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning</a></td>
  <td>æå‡ºUML-CoTæ¡†æ¶ï¼Œåˆ©ç”¨UMLè¿›è¡Œæœºå™¨äººæˆ¿é—´æ¸…æ´ä»»åŠ¡çš„ç»“æ„åŒ–æ¨ç†ä¸è§„åˆ’</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22628v2" data-paper-url="./papers/250922628v2-uml-cot-structured-reasoning-and-planning-with-unified-modeling-lang.html" onclick="toggleFavorite(this, '2509.22628v2', 'UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250922496v2-where-mllms-attend-and-what-they-rely-on-explaining-autoregressive-t.html">Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</a></td>
  <td>EAGLEï¼šä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œç”¨äºè§£é‡Šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è‡ªå›å½’tokenç”Ÿæˆè¿‡ç¨‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22496v2" data-paper-url="./papers/250922496v2-where-mllms-attend-and-what-they-rely-on-explaining-autoregressive-t.html" onclick="toggleFavorite(this, '2509.22496v2', 'Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250921997v1-exposing-hallucinations-to-suppress-them-vlms-representation-editing.html">Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors</a></td>
  <td>æå‡ºåŸºäºç”Ÿæˆé”šç‚¹çš„VLMè¡¨å¾ç¼–è¾‘æ–¹æ³•ï¼ŒæŠ‘åˆ¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21997v1" data-paper-url="./papers/250921997v1-exposing-hallucinations-to-suppress-them-vlms-representation-editing.html" onclick="toggleFavorite(this, '2509.21997v1', 'Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250921976v2-geo-r1-improving-few-shot-geospatial-referring-expression-understand.html">Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning</a></td>
  <td>Geo-R1ï¼šé€šè¿‡å¼ºåŒ–å¾®è°ƒæå‡å°‘æ ·æœ¬åœ°ç†ç©ºé—´æŒ‡ä»£è¡¨è¾¾ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21976v2" data-paper-url="./papers/250921976v2-geo-r1-improving-few-shot-geospatial-referring-expression-understand.html" onclick="toggleFavorite(this, '2509.21976v2', 'Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250922339v1-circuitsense-a-hierarchical-circuit-system-benchmark-bridging-visual.html">CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process</a></td>
  <td>CircuitSenseï¼šæå‡ºç”µè·¯ç³»ç»ŸåŸºå‡†ï¼Œæ¡¥æ¥å·¥ç¨‹è®¾è®¡ä¸­çš„è§†è§‰ç†è§£ä¸ç¬¦å·æ¨ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22339v1" data-paper-url="./papers/250922339v1-circuitsense-a-hierarchical-circuit-system-benchmark-bridging-visual.html" onclick="toggleFavorite(this, '2509.22339v1', 'CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250922229v2-a-tale-of-two-experts-cooperative-learning-for-source-free-unsupervi.html">A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation</a></td>
  <td>æå‡ºä¸“å®¶ååŒå­¦ä¹ æ¡†æ¶EXCLï¼Œè§£å†³æ— æºåŸŸæ— ç›‘ç£åŸŸè‡ªé€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22229v2" data-paper-url="./papers/250922229v2-a-tale-of-two-experts-cooperative-learning-for-source-free-unsupervi.html" onclick="toggleFavorite(this, '2509.22229v2', 'A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250921984v1-from-bias-to-balance-exploring-and-mitigating-spatial-bias-in-lvlms.html">From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs</a></td>
  <td>æå‡ºBaPAå¹³è¡¡ä½ç½®ç¼–ç æ–¹æ³•ï¼Œæå‡LVLMçš„ç©ºé—´é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21984v1" data-paper-url="./papers/250921984v1-from-bias-to-balance-exploring-and-mitigating-spatial-bias-in-lvlms.html" onclick="toggleFavorite(this, '2509.21984v1', 'From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250921839v2-ditraj-training-free-trajectory-control-for-video-diffusion-transfor.html">DiTraj: training-free trajectory control for video diffusion transformer</a></td>
  <td>æå‡ºDiTrajï¼Œä¸€ç§é¢å‘è§†é¢‘æ‰©æ•£Transformerçš„å…è®­ç»ƒè½¨è¿¹æ§åˆ¶æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21839v2" data-paper-url="./papers/250921839v2-ditraj-training-free-trajectory-control-for-video-diffusion-transfor.html" onclick="toggleFavorite(this, '2509.21839v2', 'DiTraj: training-free trajectory control for video diffusion transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250921760v1-univid-unifying-vision-tasks-with-pre-trained-video-generation-model.html">UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models</a></td>
  <td>UniVidï¼šåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ç»Ÿä¸€è§†è§‰ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21760v1" data-paper-url="./papers/250921760v1-univid-unifying-vision-tasks-with-pre-trained-video-generation-model.html" onclick="toggleFavorite(this, '2509.21760v1', 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250922917v1-learning-unified-representation-of-3d-gaussian-splatting.html">Learning Unified Representation of 3D Gaussian Splatting</a></td>
  <td>æå‡ºåŸºäºè¿ç»­å­æµå½¢åœºçš„3Dé«˜æ–¯æº…å°„ç»Ÿä¸€è¡¨å¾æ–¹æ³•ï¼Œæå‡ç¥ç»ç½‘ç»œå­¦ä¹ æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22917v1" data-paper-url="./papers/250922917v1-learning-unified-representation-of-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2509.22917v1', 'Learning Unified Representation of 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250922225v1-polysemous-language-gaussian-splatting-via-matching-based-mask-lifti.html">Polysemous Language Gaussian Splatting via Matching-based Mask Lifting</a></td>
  <td>æå‡ºMUSplatï¼Œé€šè¿‡åŒ¹é…çš„æ©ç æå‡å®ç°å¤šä¹‰è¯­è¨€é«˜æ–¯æº…å°„ï¼Œæ— éœ€åœºæ™¯é‡è®­ç»ƒã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22225v1" data-paper-url="./papers/250922225v1-polysemous-language-gaussian-splatting-via-matching-based-mask-lifti.html" onclick="toggleFavorite(this, '2509.22225v1', 'Polysemous Language Gaussian Splatting via Matching-based Mask Lifting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250922014v1-lightweight-structured-multimodal-reasoning-for-clinical-scene-under.html">Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics</a></td>
  <td>æå‡ºè½»é‡çº§ç»“æ„åŒ–å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œç”¨äºæœºå™¨äººä¸´åºŠåœºæ™¯ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22014v1" data-paper-url="./papers/250922014v1-lightweight-structured-multimodal-reasoning-for-clinical-scene-under.html" onclick="toggleFavorite(this, '2509.22014v1', 'Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250921950v1-customizing-visual-emotion-evaluation-for-mllms-an-open-vocabulary-m.html">Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach</a></td>
  <td>æå‡ºä¸€ç§å¼€æ”¾è¯æ±‡ã€å¤šæ–¹é¢ã€å¯æ‰©å±•çš„è§†è§‰æƒ…æ„Ÿè¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21950v1" data-paper-url="./papers/250921950v1-customizing-visual-emotion-evaluation-for-mllms-an-open-vocabulary-m.html" onclick="toggleFavorite(this, '2509.21950v1', 'Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250922615v1-vision-language-alignment-from-compressed-image-representations-usin.html">Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting</a></td>
  <td>åˆ©ç”¨2Dé«˜æ–¯æº…å°„å‹ç¼©å›¾åƒè¡¨ç¤ºå®ç°è§†è§‰-è¯­è¨€å¯¹é½</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22615v1" data-paper-url="./papers/250922615v1-vision-language-alignment-from-compressed-image-representations-usin.html" onclick="toggleFavorite(this, '2509.22615v1', 'Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250922527v1-efficientdepth-a-fast-and-detail-preserving-monocular-depth-estimati.html">EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model</a></td>
  <td>EfficientDepthï¼šä¸€ç§å¿«é€Ÿä¸”ä¿ç•™ç»†èŠ‚çš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22527v1" data-paper-url="./papers/250922527v1-efficientdepth-a-fast-and-detail-preserving-monocular-depth-estimati.html" onclick="toggleFavorite(this, '2509.22527v1', 'EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250922276v1-gs-2m-gaussian-splatting-for-joint-mesh-reconstruction-and-material-.html">GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition</a></td>
  <td>GS-2Mï¼šåŸºäºé«˜æ–¯æº…å°„çš„è”åˆç½‘æ ¼é‡å»ºä¸æè´¨åˆ†è§£æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22276v1" data-paper-url="./papers/250922276v1-gs-2m-gaussian-splatting-for-joint-mesh-reconstruction-and-material-.html" onclick="toggleFavorite(this, '2509.22276v1', 'GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250922627v1-ccnext-an-effective-self-supervised-stereo-depth-estimation-approach.html">CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach</a></td>
  <td>æå‡ºCCNeXtï¼Œä¸€ç§é«˜æ•ˆçš„è‡ªç›‘ç£ç«‹ä½“æ·±åº¦ä¼°è®¡æ–¹æ³•ï¼Œåœ¨è®¡ç®—æˆæœ¬å’Œç²¾åº¦é—´å–å¾—å¹³è¡¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">stereo depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22627v1" data-paper-url="./papers/250922627v1-ccnext-an-effective-self-supervised-stereo-depth-estimation-approach.html" onclick="toggleFavorite(this, '2509.22627v1', 'CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250921922v1-spatial-reasoning-in-foundation-models-benchmarking-object-centric-s.html">Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding</a></td>
  <td>æå‡ºç³»ç»ŸåŸºå‡†ä»¥è§£å†³è§†è§‰æ¨¡å‹ç©ºé—´ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21922v1" data-paper-url="./papers/250921922v1-spatial-reasoning-in-foundation-models-benchmarking-object-centric-s.html" onclick="toggleFavorite(this, '2509.21922v1', 'Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250922228v1-urbanfeel-a-comprehensive-benchmark-for-temporal-and-perceptual-unde.html">UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective</a></td>
  <td>UrbanFeelï¼šæå‡ºä¸€ä¸ªç»¼åˆæ€§åŸå¸‚è¡—æ™¯ç†è§£benchmarkï¼Œå…³æ³¨æ—¶åºå˜åŒ–å’Œäººç±»æ„ŸçŸ¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22228v1" data-paper-url="./papers/250922228v1-urbanfeel-a-comprehensive-benchmark-for-temporal-and-perceptual-unde.html" onclick="toggleFavorite(this, '2509.22228v1', 'UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250921719v1-delivr-differential-spatiotemporal-lie-bias-for-efficient-video-dera.html">DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining</a></td>
  <td>DeLiVRï¼šåˆ©ç”¨æ—¶ç©ºLieç¾¤å¾®åˆ†åç½®å®ç°é«˜æ•ˆè§†é¢‘å»é›¨</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21719v1" data-paper-url="./papers/250921719v1-delivr-differential-spatiotemporal-lie-bias-for-efficient-video-dera.html" onclick="toggleFavorite(this, '2509.21719v1', 'DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250921927v1-singref6d-monocular-novel-object-pose-estimation-with-a-single-rgb-r.html">SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference</a></td>
  <td>SingRef6Dï¼šåŸºäºå•å¼ RGBå‚è€ƒå›¾åƒçš„æ–°ç‰©ä½“å•ç›®6Dä½å§¿ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">Depth Anything</span> <span class="paper-tag">6D pose estimation</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21927v1" data-paper-url="./papers/250921927v1-singref6d-monocular-novel-object-pose-estimation-with-a-single-rgb-r.html" onclick="toggleFavorite(this, '2509.21927v1', 'SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250922112v1-large-material-gaussian-model-for-relightable-3d-generation.html">Large Material Gaussian Model for Relightable 3D Generation</a></td>
  <td>æå‡ºLarge Material Gaussian Modelï¼Œå®ç°å¯åŠ¨æ€å…‰ç…§çš„3Då†…å®¹ç”Ÿæˆï¼Œè§£å†³ç°æœ‰æ–¹æ³•æè´¨å±æ€§ç¼ºå¤±é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22112v1" data-paper-url="./papers/250922112v1-large-material-gaussian-model-for-relightable-3d-generation.html" onclick="toggleFavorite(this, '2509.22112v1', 'Large Material Gaussian Model for Relightable 3D Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250921888v1-drag4d-align-your-motion-with-text-driven-3d-scene-generation.html">Drag4D: Align Your Motion with Text-Driven 3D Scene Generation</a></td>
  <td>Drag4Dï¼šæå‡ºæ–‡æœ¬é©±åŠ¨çš„3Dåœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°äº¤äº’å¼ç‰©ä½“è¿åŠ¨æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21888v1" data-paper-url="./papers/250921888v1-drag4d-align-your-motion-with-text-driven-3d-scene-generation.html" onclick="toggleFavorite(this, '2509.21888v1', 'Drag4D: Align Your Motion with Text-Driven 3D Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250921853v2-dynamic-novel-view-synthesis-in-high-dynamic-range.html">Dynamic Novel View Synthesis in High Dynamic Range</a></td>
  <td>æå‡ºHDR-4DGSï¼Œè§£å†³é«˜åŠ¨æ€èŒƒå›´åŠ¨æ€åœºæ™¯çš„æ–°è§†è§’åˆæˆé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21853v2" data-paper-url="./papers/250921853v2-dynamic-novel-view-synthesis-in-high-dynamic-range.html" onclick="toggleFavorite(this, '2509.21853v2', 'Dynamic Novel View Synthesis in High Dynamic Range')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250921992v1-dualfocus-depth-from-focus-with-spatio-focal-dual-variational-constr.html">DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints</a></td>
  <td>DualFocusï¼šåˆ©ç”¨ç©ºåŸŸ-ç„¦åŸŸåŒé‡å˜åˆ†çº¦æŸçš„æ™¯æ·±ä¼°è®¡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21992v1" data-paper-url="./papers/250921992v1-dualfocus-depth-from-focus-with-spatio-focal-dual-variational-constr.html" onclick="toggleFavorite(this, '2509.21992v1', 'DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250921854v1-perception-consistency-multimodal-large-language-models-reasoning-vi.html">Perception-Consistency Multimodal Large Language Models Reasoning via Caption-Regularized Policy Optimization</a></td>
  <td>æå‡ºCapPOï¼Œé€šè¿‡Captionæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ„ŸçŸ¥ä¸€è‡´æ€§æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21854v1" data-paper-url="./papers/250921854v1-perception-consistency-multimodal-large-language-models-reasoning-vi.html" onclick="toggleFavorite(this, '2509.21854v1', 'Perception-Consistency Multimodal Large Language Models Reasoning via Caption-Regularized Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/251000037v3-on-robustness-of-vision-language-action-model-against-multi-modal-pe.html">On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations</a></td>
  <td>æå‡ºRobustVLAï¼Œå¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¤šæ¨¡æ€æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00037v3" data-paper-url="./papers/251000037v3-on-robustness-of-vision-language-action-model-against-multi-modal-pe.html" onclick="toggleFavorite(this, '2510.00037v3', 'On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250922841v1-multimodal-slice-interaction-network-enhanced-by-transfer-learning-f.html">Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging</a></td>
  <td>æå‡ºåŸºäºè¿ç§»å­¦ä¹ å’Œå¤šæ¨¡æ€äº¤äº’ç½‘ç»œçš„è‚ºç™ŒIGTVç²¾ç¡®åˆ†å‰²æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22841v1" data-paper-url="./papers/250922841v1-multimodal-slice-interaction-network-enhanced-by-transfer-learning-f.html" onclick="toggleFavorite(this, '2509.22841v1', 'Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250921871v1-unlocking-the-essence-of-beauty-advanced-aesthetic-reasoning-with-re.html">Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization</a></td>
  <td>æå‡ºåŸºäºç›¸å¯¹-ç»å¯¹ç­–ç•¥ä¼˜åŒ–çš„Aes-R1æ¡†æ¶ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç¾å­¦æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21871v1" data-paper-url="./papers/250921871v1-unlocking-the-essence-of-beauty-advanced-aesthetic-reasoning-with-re.html" onclick="toggleFavorite(this, '2509.21871v1', 'Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250922813v1-trust-test-time-refinement-using-uncertainty-guided-ssm-traverses.html">TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses</a></td>
  <td>æå‡ºTRUSTï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§å¼•å¯¼çš„SSMéå†è¿›è¡Œæµ‹è¯•æ—¶ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22813v1" data-paper-url="./papers/250922813v1-trust-test-time-refinement-using-uncertainty-guided-ssm-traverses.html" onclick="toggleFavorite(this, '2509.22813v1', 'TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250922624v1-spark-synergistic-policy-and-reward-co-evolving-framework.html">SPARK: Synergistic Policy And Reward Co-Evolving Framework</a></td>
  <td>æå‡ºSPARKæ¡†æ¶ä»¥è§£å†³RLHFä¸RLVRçš„æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22624v1" data-paper-url="./papers/250922624v1-spark-synergistic-policy-and-reward-co-evolving-framework.html" onclick="toggleFavorite(this, '2509.22624v1', 'SPARK: Synergistic Policy And Reward Co-Evolving Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250922481v1-pstts-a-plug-and-play-token-selector-for-efficient-event-based-spati.html">PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning</a></td>
  <td>æå‡ºPSTTSå³æ’å³ç”¨æ¨¡å—ï¼Œæœ‰æ•ˆæå‡äº‹ä»¶æ•°æ®æ—¶ç©ºè¡¨å¾å­¦ä¹ çš„æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22481v1" data-paper-url="./papers/250922481v1-pstts-a-plug-and-play-token-selector-for-efficient-event-based-spati.html" onclick="toggleFavorite(this, '2509.22481v1', 'PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/250922799v1-videoscore2-think-before-you-score-in-generative-video-evaluation.html">VideoScore2: Think before You Score in Generative Video Evaluation</a></td>
  <td>VideoScore2ï¼šæå‡ºå¤šç»´åº¦ã€å¯è§£é‡Šçš„è§†é¢‘ç”Ÿæˆè¯„ä¼°æ¡†æ¶ï¼Œæå‡è¯„ä¼°å‡†ç¡®æ€§å’Œå¯æ§æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22799v1" data-paper-url="./papers/250922799v1-videoscore2-think-before-you-score-in-generative-video-evaluation.html" onclick="toggleFavorite(this, '2509.22799v1', 'VideoScore2: Think before You Score in Generative Video Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/250922647v1-caprl-stimulating-dense-image-caption-capabilities-via-reinforcement.html">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a></td>
  <td>æå‡ºCapRLï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å›¾åƒæè¿°çš„ç¨ å¯†æ€§å’Œè´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22647v1" data-paper-url="./papers/250922647v1-caprl-stimulating-dense-image-caption-capabilities-via-reinforcement.html" onclick="toggleFavorite(this, '2509.22647v1', 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250922318v1-nifty-a-non-local-image-flow-matching-for-texture-synthesis.html">NIFTY: a Non-Local Image Flow Matching for Texture Synthesis</a></td>
  <td>NIFTYï¼šä¸€ç§ç”¨äºçº¹ç†åˆæˆçš„éå±€éƒ¨å›¾åƒæµåŒ¹é…æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22318v1" data-paper-url="./papers/250922318v1-nifty-a-non-local-image-flow-matching-for-texture-synthesis.html" onclick="toggleFavorite(this, '2509.22318v1', 'NIFTY: a Non-Local Image Flow Matching for Texture Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250922283v1-rule-based-reinforcement-learning-for-document-image-classification-.html">Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models</a></td>
  <td>æå‡ºåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22283v1" data-paper-url="./papers/250922283v1-rule-based-reinforcement-learning-for-document-image-classification-.html" onclick="toggleFavorite(this, '2509.22283v1', 'Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250922150v1-joint-graph-entropy-knowledge-distillation-for-point-cloud-classific.html">Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions</a></td>
  <td>æå‡ºè”åˆå›¾ç†µçŸ¥è¯†è’¸é¦ä»¥è§£å†³3Dç‚¹äº‘åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22150v1" data-paper-url="./papers/250922150v1-joint-graph-entropy-knowledge-distillation-for-point-cloud-classific.html" onclick="toggleFavorite(this, '2509.22150v1', 'Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>51</td>
  <td><a href="./papers/250921991v1-ergo-efficient-high-resolution-visual-understanding-for-vision-langu.html">ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models</a></td>
  <td>æå‡ºERGOï¼Œé€šè¿‡ç²—åˆ°ç²¾æ¨ç†æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ä¸­çš„æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21991v1" data-paper-url="./papers/250921991v1-ergo-efficient-high-resolution-visual-understanding-for-vision-langu.html" onclick="toggleFavorite(this, '2509.21991v1', 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>52</td>
  <td><a href="./papers/250921965v2-partsam-a-scalable-promptable-part-segmentation-model-trained-on-nat.html">PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data</a></td>
  <td>æå‡ºPartSAMä»¥è§£å†³3Dç‰©ä½“åˆ†å‰²ä¸­çš„å‡ ä½•ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21965v2" data-paper-url="./papers/250921965v2-partsam-a-scalable-promptable-part-segmentation-model-trained-on-nat.html" onclick="toggleFavorite(this, '2509.21965v2', 'PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>53</td>
  <td><a href="./papers/250921788v1-mirg-rl-multi-image-reasoning-and-grounding-with-reinforcement-learn.html">MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning</a></td>
  <td>æå‡ºMIRG-RLæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤šå›¾æ¨ç†å’Œå®šä½èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21788v1" data-paper-url="./papers/250921788v1-mirg-rl-multi-image-reasoning-and-grounding-with-reinforcement-learn.html" onclick="toggleFavorite(this, '2509.21788v1', 'MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>54</td>
  <td><a href="./papers/250921774v1-training-free-multimodal-deepfake-detection-via-graph-reasoning.html">Training-Free Multimodal Deepfake Detection via Graph Reasoning</a></td>
  <td>æå‡ºGASP-ICLæ¡†æ¶ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°å¤šæ¨¡æ€Deepfakeæ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21774v1" data-paper-url="./papers/250921774v1-training-free-multimodal-deepfake-detection-via-graph-reasoning.html" onclick="toggleFavorite(this, '2509.21774v1', 'Training-Free Multimodal Deepfake Detection via Graph Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>55</td>
  <td><a href="./papers/250921797v2-mowm-mixture-of-world-models-for-embodied-planning-via-latent-to-pix.html">MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation</a></td>
  <td>æå‡ºMoWMï¼šä¸€ç§æ··åˆä¸–ç•Œæ¨¡å‹çš„å…·èº«è§„åˆ’æ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨åˆ°åƒç´ ç‰¹å¾è°ƒåˆ¶æå‡æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21797v2" data-paper-url="./papers/250921797v2-mowm-mixture-of-world-models-for-embodied-planning-via-latent-to-pix.html" onclick="toggleFavorite(this, '2509.21797v2', 'MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/250921790v1-longscape-advancing-long-horizon-embodied-world-models-with-context-.html">LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE</a></td>
  <td>LongScapeï¼šæå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥MoEçš„é•¿æ—¶ç¨‹å…·èº«ä¸–ç•Œæ¨¡å‹ï¼Œè§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶åºä¸ä¸€è‡´é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21790v1" data-paper-url="./papers/250921790v1-longscape-advancing-long-horizon-embodied-world-models-with-context-.html" onclick="toggleFavorite(this, '2509.21790v1', 'LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>57</td>
  <td><a href="./papers/250922281v1-mesatask-towards-task-driven-tabletop-scene-generation-via-3d-spatia.html">MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning</a></td>
  <td>MesaTaskï¼šæå‡ºåŸºäº3Dç©ºé—´æ¨ç†çš„ä»»åŠ¡é©±åŠ¨å‹æ¡Œé¢åœºæ™¯ç”Ÿæˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">DPO</span> <span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22281v1" data-paper-url="./papers/250922281v1-mesatask-towards-task-driven-tabletop-scene-generation-via-3d-spatia.html" onclick="toggleFavorite(this, '2509.22281v1', 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>58</td>
  <td><a href="./papers/250921905v1-tdedit-a-unified-diffusion-framework-for-text-drag-guided-image-mani.html">TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation</a></td>
  <td>æå‡ºTDEditæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬ä¸æ‹–æ‹½äº¤äº’çš„å›¾åƒç¼–è¾‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21905v1" data-paper-url="./papers/250921905v1-tdedit-a-unified-diffusion-framework-for-text-drag-guided-image-mani.html" onclick="toggleFavorite(this, '2509.21905v1', 'TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>59</td>
  <td><a href="./papers/250922169v1-dragganspace-latent-space-exploration-and-control-for-gans.html">DragGANSpace: Latent Space Exploration and Control for GANs</a></td>
  <td>DragGANSpaceï¼šèåˆPCAçš„GANæ½œåœ¨ç©ºé—´æ¢ç´¢ä¸æ§åˆ¶æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22169v1" data-paper-url="./papers/250922169v1-dragganspace-latent-space-exploration-and-control-for-gans.html" onclick="toggleFavorite(this, '2509.22169v1', 'DragGANSpace: Latent Space Exploration and Control for GANs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>60</td>
  <td><a href="./papers/250922646v2-learning-human-perceived-fakeness-in-ai-generated-videos-via-multimo.html">Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs</a></td>
  <td>æå‡ºDeeptraceRewardä»¥è§£å†³AIç”Ÿæˆè§†é¢‘çš„ä¼ªé€ æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22646v2" data-paper-url="./papers/250922646v2-learning-human-perceived-fakeness-in-ai-generated-videos-via-multimo.html" onclick="toggleFavorite(this, '2509.22646v2', 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>61</td>
  <td><a href="./papers/250921980v1-resolving-ambiguity-in-gaze-facilitated-visual-assistant-interaction.html">Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm</a></td>
  <td>GLARIFYï¼šåˆ©ç”¨æ—¶ç©ºæ³¨è§†ä¿¡æ¯è§£å†³è§†è§‰åŠ©æ‰‹äº¤äº’ä¸­çš„æ­§ä¹‰æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21980v1" data-paper-url="./papers/250921980v1-resolving-ambiguity-in-gaze-facilitated-visual-assistant-interaction.html" onclick="toggleFavorite(this, '2509.21980v1', 'Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>62</td>
  <td><a href="./papers/250922019v1-egoinstruct-an-egocentric-video-dataset-of-face-to-face-instructiona.html">EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking</a></td>
  <td>EgoInstructï¼šç”¨äºäººé™…æ•™å­¦äº¤äº’çš„è‡ªä¸­å¿ƒè§†é¢‘æ•°æ®é›†ä¸å¤šæ¨¡æ€LLMåŸºå‡†æµ‹è¯•</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22019v1" data-paper-url="./papers/250922019v1-egoinstruct-an-egocentric-video-dataset-of-face-to-face-instructiona.html" onclick="toggleFavorite(this, '2509.22019v1', 'EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)