---
layout: default
title: Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding
---

# Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21922v1</a>
  <a href="https://arxiv.org/pdf/2509.21922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21922v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21922v1', 'Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vahid Mirjalili, Ramin Giahi, Sriram Kollipara, Akshay Kekuda, Kehui Yao, Kai Zhao, Jianpeng Xu, Kaushiki Nag, Sinduja Subramaniam, Topojoy Biswas, Evren Korpeoglu, Kannan Achan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 4 pages, NeurIPS Workshop SpaVLE

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç³»ç»ŸåŸºå‡†ä»¥è§£å†³è§†è§‰æ¨¡å‹ç©ºé—´ç†è§£ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´ç†è§£` `è§†è§‰æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `ç‰©ä½“æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `åˆæˆæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æ¨¡å‹åœ¨ç©ºé—´ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç‰©ä½“å…³ç³»å’Œç›¸å¯¹ä½ç½®çš„æ¨ç†ä¸Šã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†ï¼Œä¸“æ³¨äºç‰©ä½“ä¸­å¿ƒçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œåˆ©ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ£€æµ‹å™¨åœ¨å®šä½ç²¾åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç©ºé—´æ¨ç†ä¸Šå­˜åœ¨å±€é™ï¼Œå¼ºè°ƒäº†å¯¹ç©ºé—´æ„ŸçŸ¥æ¨¡å‹çš„éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´ç†è§£æ˜¯è§†è§‰åŸºç¡€æ¨¡å‹çš„é‡è¦èƒ½åŠ›ã€‚å°½ç®¡è¿‘å¹´æ¥å¤§å‹è§†è§‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä½†å¤§å¤šæ•°åŸºå‡†æµ‹è¯•å¼ºè°ƒå®šä½ç²¾åº¦ï¼Œè€Œå¿½è§†äº†æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ•æ‰åœºæ™¯ä¸­ç‰©ä½“çš„æ’åˆ—å’Œå…³ç³»ã€‚æœ‰æ•ˆçš„åœºæ™¯ç†è§£ä¸ä»…éœ€è¦è¯†åˆ«ç‰©ä½“ï¼Œè¿˜éœ€è¦æ¨ç†å®ƒä»¬çš„ç›¸å¯¹ä½ç½®ã€åˆ†ç»„å’Œæ·±åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹çš„ç‰©ä½“ä¸­å¿ƒç©ºé—´æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨å—æ§çš„åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å®šä½ã€ç©ºé—´æ¨ç†å’Œä¸‹æ¸¸æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæ£€æµ‹å™¨å¦‚GroundingDINOå’ŒOWLv2æä¾›äº†ç²¾ç¡®çš„è¾¹ç•Œæ¡†ï¼Œä½†åœ¨å…³ç³»æ¨ç†æ–¹é¢æœ‰é™ï¼Œè€ŒåƒSmolVLMå’ŒGPT-4oè¿™æ ·çš„è§†è§‰è¯­è¨€æ¨¡å‹åˆ™æä¾›äº†ç²—ç•¥çš„å¸ƒå±€çº¿ç´¢å’Œæµç•…çš„æè¿°ï¼Œä½†åœ¨ç»†ç²’åº¦ç©ºé—´ä¸Šä¸‹æ–‡æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªæ˜¾äº†å®šä½ä¸çœŸå®ç©ºé—´ç†è§£ä¹‹é—´çš„å·®è·ï¼Œå¹¶æŒ‡å‘äº†ç¤¾åŒºå¯¹ç©ºé—´æ„ŸçŸ¥åŸºç¡€æ¨¡å‹çš„éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰æ¨¡å‹åœ¨ç©ºé—´ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç‰©ä½“ä¹‹é—´å…³ç³»çš„æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºå®šä½ç²¾åº¦ï¼Œå¿½è§†äº†ç‰©ä½“çš„ç›¸å¯¹ä½ç½®å’Œæ·±åº¦å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡å»ºç«‹ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†æ¥è¯„ä¼°åŸºç¡€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒç‰©ä½“ä¸­å¿ƒçš„ç©ºé—´å…³ç³»ï¼Œè€Œä¸ä»…ä»…æ˜¯ç‰©ä½“çš„è¯†åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªå—æ§çš„åˆæˆæ•°æ®é›†ï¼Œè¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»»åŠ¡åŒ…æ‹¬ç©ºé—´å®šä½ã€ç©ºé—´æ¨ç†å’Œä¸‹æ¸¸æ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨ç©ºé—´ç†è§£æ–¹é¢çš„ç©ºç™½ï¼Œå¼ºè°ƒäº†ç©ºé—´æ¨ç†åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œè®¾ç½®äº†ä¸åŒçš„ä»»åŠ¡å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ç©ºé—´ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGroundingDINOå’ŒOWLv2åœ¨ç©ºé—´å®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæä¾›äº†é«˜ç²¾åº¦çš„è¾¹ç•Œæ¡†ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSmolVLMå’ŒGPT-4oåœ¨ç”Ÿæˆæµç•…æè¿°æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»†ç²’åº¦ç©ºé—´ä¸Šä¸‹æ–‡çš„ç†è§£ä¸Šå­˜åœ¨å›°éš¾ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç©ºé—´ç†è§£èƒ½åŠ›çš„æå‡éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰éœ€è¦æ·±åº¦ç©ºé—´ç†è§£çš„åœºæ™¯ã€‚é€šè¿‡æå‡æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„è¿™äº›é¢†åŸŸçš„æ™ºèƒ½ç³»ç»Ÿè¡¨ç°ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial understanding is a critical capability for vision foundation models. While recent advances in large vision models or vision-language models (VLMs) have expanded recognition capabilities, most benchmarks emphasize localization accuracy rather than whether models capture how objects are arranged and related within a scene. This gap is consequential; effective scene understanding requires not only identifying objects, but reasoning about their relative positions, groupings, and depth. In this paper, we present a systematic benchmark for object-centric spatial reasoning in foundation models. Using a controlled synthetic dataset, we evaluate state-of-the-art vision models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL, LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and downstream retrieval tasks. We find a stable trade-off: detectors such as GroundingDINO and OWLv2 deliver precise boxes with limited relational reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and fluent captions but struggle with fine-grained spatial context. Our study highlights the gap between localization and true spatial understanding, and pointing toward the need for spatially-aware foundation models in the community.

