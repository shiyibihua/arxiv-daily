---
layout: default
title: UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models
---

# UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21760" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21760v1</a>
  <a href="https://arxiv.org/pdf/2509.21760.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21760v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21760v1', 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lan Chen, Yuchao Gu, Qi Mao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CUC-MIPG/UniVid)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniVidï¼šåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ç»Ÿä¸€è§†è§‰ä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆæ¨¡å‹` `è§†è§‰ä»»åŠ¡ç»Ÿä¸€` `è·¨æ¨¡æ€å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `è§†è§‰å¥å­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æ¨¡å‹é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•åˆ°æ–°ä»»åŠ¡ã€‚
2. UniVidåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è§†è§‰å¥å­çš„å½¢å¼ç»Ÿä¸€è¡¨ç¤ºå„ç§è§†è§‰ä»»åŠ¡ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šä¿®æ”¹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniVidåœ¨è·¨æ¨¡æ€æ¨ç†å’Œè·¨æºä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½çµæ´»åˆ‡æ¢ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åœ¨å¹¿æ³›è¯­æ–™åº“ä¸Šè®­ç»ƒï¼ŒæˆåŠŸåœ°å°†å„ç§è¯­è¨€ä»»åŠ¡ç»Ÿä¸€åœ¨ä¸€ä¸ªç”Ÿæˆæ¡†æ¶ä¸­ã€‚å—æ­¤å¯å‘ï¼Œæœ€è¿‘çš„å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰é€šè¿‡å°†ä»»åŠ¡ç»„ç»‡æˆé¡ºåºè§†è§‰å¥å­ï¼Œå°†è¿™ç§èŒƒå¼æ‰©å±•åˆ°è§†è§‰é¢†åŸŸï¼Œå…¶ä¸­è§†è§‰æç¤ºä½œä¸ºæŒ‡å¯¼è¾“å‡ºçš„ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè¿™ç§å»ºæ¨¡éœ€è¦åœ¨è·¨æ¨¡æ€å’Œæ¥æºçš„ä»»åŠ¡ç‰¹å®šé¢„è®­ç»ƒï¼Œè¿™æ—¢æ˜‚è´µåˆé™åˆ¶äº†å¯¹æœªè§ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚é‰´äºé¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å›ºæœ‰åœ°æ•è·äº†æ—¶é—´åºåˆ—ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§æ›´ç»Ÿä¸€å’Œå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼šé¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½å¦é€‚åº”å„ç§å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniVidï¼Œä¸€ä¸ªå¾®è°ƒè§†é¢‘æ‰©æ•£Transformerä»¥å¤„ç†å„ç§è§†è§‰ä»»åŠ¡çš„æ¡†æ¶ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šçš„ä¿®æ”¹ã€‚ä»»åŠ¡è¢«è¡¨ç¤ºä¸ºè§†è§‰å¥å­ï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡åºåˆ—å®šä¹‰äº†ä»»åŠ¡å’Œé¢„æœŸçš„è¾“å‡ºæ¨¡æ€ã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è¯„ä¼°UniVidçš„æ³›åŒ–èƒ½åŠ›ï¼šï¼ˆ1ï¼‰å…·æœ‰ç”±å›¾åƒå’Œè§†é¢‘ç»„æˆçš„ä¸Šä¸‹æ–‡çš„è·¨æ¨¡æ€æ¨ç†ï¼Œè¶…è¶Šäº†LVMçš„å•æ¨¡æ€è®¾ç½®ï¼›ï¼ˆ2ï¼‰ä»è‡ªç„¶æ•°æ®åˆ°æ ‡æ³¨æ•°æ®çš„è·¨æºä»»åŠ¡ï¼Œæ— éœ€å¤šæºé¢„è®­ç»ƒã€‚å°½ç®¡ä»…åœ¨è‡ªç„¶è§†é¢‘æ•°æ®ä¸Šè®­ç»ƒï¼ŒUniVidåœ¨è¿™ä¸¤ç§è®¾ç½®ä¸­éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡ç®€å•åœ°åè½¬è¿™ç§èŒƒå¼ä¸­çš„è§†è§‰å¥å­é¡ºåºï¼Œå¯ä»¥è½»æ¾åˆ‡æ¢ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚è¿™äº›å‘ç°çªå‡ºäº†é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸ºè§†è§‰å»ºæ¨¡çš„å¯æ‰©å±•å’Œç»Ÿä¸€åŸºç¡€çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨https://github.com/CUC-MIPG/UniVidå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰è™½ç„¶èƒ½å¤Ÿç»Ÿä¸€å¤šç§è§†è§‰ä»»åŠ¡ï¼Œä½†éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å±€é™äºå•æ¨¡æ€è¾“å…¥ï¼Œéš¾ä»¥å¤„ç†å›¾åƒå’Œè§†é¢‘æ··åˆçš„å¤æ‚åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniVidçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æ‰€å…·å¤‡çš„æ—¶é—´åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œå°†å„ç§è§†è§‰ä»»åŠ¡è½¬åŒ–ä¸ºè§†è§‰å¥å­çš„å½¢å¼ã€‚é€šè¿‡å¾®è°ƒè§†é¢‘æ‰©æ•£Transformerï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å¹¶ç”Ÿæˆä¸åŒæ¨¡æ€çš„è§†è§‰å†…å®¹ï¼Œä»è€Œå®ç°ä»»åŠ¡çš„ç»Ÿä¸€è¡¨ç¤ºå’Œå¤„ç†ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä»»åŠ¡ç‰¹å®šçš„é¢„è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniVidçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å°†è§†è§‰ä»»åŠ¡è¡¨ç¤ºä¸ºè§†è§‰å¥å­ï¼Œå…¶ä¸­åŒ…å«ä¸Šä¸‹æ–‡åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œè¾“å…¥å›¾åƒæˆ–è§†é¢‘ï¼‰å’Œç›®æ ‡åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œä»»åŠ¡çš„è¾“å‡ºï¼‰ã€‚2) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£Transformerä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚3) å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„è§†è§‰å¥å­ç”Ÿæˆç›¸åº”çš„è¾“å‡ºã€‚4) é€šè¿‡åè½¬è§†è§‰å¥å­çš„é¡ºåºï¼Œå¯ä»¥è½»æ¾åˆ‡æ¢ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniVidæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¥ç»Ÿä¸€è§†è§‰ä»»åŠ¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒUniVidæ— éœ€ä»»åŠ¡ç‰¹å®šçš„é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†è·¨æ¨¡æ€è¾“å…¥ï¼Œå¹¶ä¸”å¯ä»¥çµæ´»åˆ‡æ¢ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å…·æœ‰æ›´é«˜çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šUniVidçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨è§†é¢‘æ‰©æ•£Transformerä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨å…¶æ—¶é—´åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚2) å°†è§†è§‰ä»»åŠ¡è¡¨ç¤ºä¸ºè§†è§‰å¥å­ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿç†è§£ä»»åŠ¡çš„ä¸Šä¸‹æ–‡å’Œç›®æ ‡ã€‚3) é€šè¿‡å¾®è°ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”å„ç§è§†è§‰ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¿®æ”¹å…¶ç½‘ç»œç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniVidåœ¨è·¨æ¨¡æ€æ¨ç†å’Œè·¨æºä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»…ä½¿ç”¨è‡ªç„¶è§†é¢‘æ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒUniVidèƒ½å¤ŸæˆåŠŸå¤„ç†æ¥è‡ªæ ‡æ³¨æ•°æ®çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç®€å•åœ°åè½¬è§†è§‰å¥å­çš„é¡ºåºï¼ŒUniVidå¯ä»¥è½»æ¾åˆ‡æ¢ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„çµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniVidå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†é¢‘ç¼–è¾‘ã€å›¾åƒä¿®å¤ã€è§†é¢‘é¢„æµ‹ã€è§†è§‰é—®ç­”ç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæä¾›äº†ä¸€ç§æ›´é€šç”¨ã€æ›´å¯æ‰©å±•çš„è§†è§‰å»ºæ¨¡æ–¹æ³•ï¼Œå¯ä»¥é™ä½å¼€å‘æˆæœ¬ï¼Œå¹¶ä¿ƒè¿›è§†è§‰æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ã€‚æœªæ¥ï¼ŒUniVidå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€å’Œä»»åŠ¡ï¼Œä¾‹å¦‚ç»“åˆæ–‡æœ¬ä¿¡æ¯ï¼Œå®ç°æ›´å¤æ‚çš„è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.

