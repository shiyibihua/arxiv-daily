---
layout: default
title: A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation
---

# A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22229" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22229v2</a>
  <a href="https://arxiv.org/pdf/2509.22229.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22229v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22229v2', 'A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaping Yu, Muli Yang, Jiapeng Ji, Jiexi Yan, Cheng Deng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸“å®¶ååŒå­¦ä¹ æ¡†æ¶EXCLï¼Œè§£å†³æ— æºåŸŸæ— ç›‘ç£åŸŸè‡ªé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— æºåŸŸè‡ªé€‚åº”` `é¢†åŸŸè‡ªé€‚åº”` `ååŒå­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SFUDAæ–¹æ³•å¿½ç•¥äº†æºæ¨¡å‹å’Œé¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹æä¾›çš„äº’è¡¥ä¿¡æ¯ï¼Œä¸”æœªå……åˆ†æŒ–æ˜ç›®æ ‡åŸŸæ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚
2. æå‡ºä¸“å®¶ååŒå­¦ä¹ æ¡†æ¶EXCLï¼Œåˆ©ç”¨åŒä¸“å®¶ï¼ˆæºåŸŸæ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰æŒ–æ˜ç›®æ ‡åŸŸçš„å…±è¯†çŸ¥è¯†ã€‚
3. å¼•å…¥æ£€ç´¢å¢å¼ºäº¤äº’(RAIN)æµç¨‹ï¼ŒååŒæ£€ç´¢æ ·æœ¬å¹¶åˆ†åˆ«å¾®è°ƒä¸“å®¶ï¼Œé€šè¿‡å…±äº«å­¦ä¹ ç»“æœä¿è¯ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“å®¶ååŒå­¦ä¹ (EXCL)æ–¹æ³•ï¼Œç”¨äºè§£å†³æ— æºåŸŸæ— ç›‘ç£åŸŸè‡ªé€‚åº”(SFUDA)é—®é¢˜ã€‚SFUDAæ—¨åœ¨å°†æºåŸŸè®­ç»ƒçš„æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡åŸŸï¼Œä¸”æ— æ³•è®¿é—®æºæ•°æ®ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä»…åˆ©ç”¨æºæ¨¡å‹çš„é¢„æµ‹ï¼Œè¦ä¹ˆå¾®è°ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¿½ç•¥äº†äº’è¡¥ä¿¡æ¯å’Œç›®æ ‡æ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚EXCLåŒ…å«åŒä¸“å®¶æ¡†æ¶å’Œæ£€ç´¢å¢å¼ºäº¤äº’(RAIN)ä¼˜åŒ–æµç¨‹ã€‚åŒä¸“å®¶æ¡†æ¶å°†å†»ç»“çš„æºåŸŸæ¨¡å‹ï¼ˆé€šè¿‡Conv-Adapterå¢å¼ºï¼‰å’Œé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¸¦æœ‰å¯è®­ç»ƒçš„æ–‡æœ¬æç¤ºï¼‰ç½®äºåŒç­‰åœ°ä½ï¼Œä»¥æŒ–æ˜æ¥è‡ªæœªæ ‡è®°ç›®æ ‡æ ·æœ¬çš„å…±è¯†çŸ¥è¯†ã€‚RAINæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæµç¨‹ï¼Œç”¨äºåœ¨çº¯æ— ç›‘ç£æ¡ä»¶ä¸‹æœ‰æ•ˆè®­ç»ƒè¿™äº›æ’ä»¶æ¨¡å—ï¼Œå®ƒ(1)ååŒæ£€ç´¢ä¼ªæºæ ·æœ¬å’Œå¤æ‚ç›®æ ‡æ ·æœ¬ï¼Œ(2)åˆ†åˆ«åœ¨å„è‡ªçš„æ ·æœ¬é›†ä¸Šå¾®è°ƒæ¯ä¸ªä¸“å®¶ï¼Œä»¥åŠ(3)é€šè¿‡å…±äº«å­¦ä¹ ç»“æœæ¥å¼ºåˆ¶å­¦ä¹ å¯¹è±¡ä¸€è‡´æ€§ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ— æºåŸŸæ— ç›‘ç£åŸŸè‡ªé€‚åº”(SFUDA)é—®é¢˜ï¼Œå³åœ¨æ— æ³•è®¿é—®æºæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°†æºåŸŸè®­ç»ƒçš„æ¨¡å‹è¿ç§»åˆ°ç›®æ ‡åŸŸã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯ä»…ä¾èµ–æºæ¨¡å‹çš„é¢„æµ‹ï¼Œå¿½ç•¥äº†ç›®æ ‡åŸŸçš„è‡ªèº«ä¿¡æ¯ï¼›äºŒæ˜¯ç›´æ¥å¾®è°ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å¯èƒ½å¼•å…¥å™ªå£°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸¤ä¸ªâ€œä¸“å®¶â€â€”â€”æºåŸŸæ¨¡å‹å’Œé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ååŒå­¦ä¹ çš„æ–¹å¼ï¼Œäº’ç›¸è¡¥å……ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ç»“åˆæºæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é¿å…ç›´æ¥å¾®è°ƒå¤§å‹æ¨¡å‹å¸¦æ¥çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEXCLæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šåŒä¸“å®¶æ¡†æ¶å’Œæ£€ç´¢å¢å¼ºäº¤äº’(RAIN)ä¼˜åŒ–æµç¨‹ã€‚åŒä¸“å®¶æ¡†æ¶ç”±ä¸€ä¸ªå†»ç»“çš„æºåŸŸæ¨¡å‹ï¼ˆé€šè¿‡Conv-Adapterå¢å¼ºï¼‰å’Œä¸€ä¸ªé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¸¦æœ‰å¯è®­ç»ƒçš„æ–‡æœ¬æç¤ºï¼‰ç»„æˆã€‚RAINæµç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š(1)ååŒæ£€ç´¢ä¼ªæºæ ·æœ¬å’Œå¤æ‚ç›®æ ‡æ ·æœ¬ï¼›(2)åˆ†åˆ«åœ¨å„è‡ªçš„æ ·æœ¬é›†ä¸Šå¾®è°ƒæ¯ä¸ªä¸“å®¶ï¼›(3)é€šè¿‡å…±äº«å­¦ä¹ ç»“æœæ¥å¼ºåˆ¶å­¦ä¹ å¯¹è±¡ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒä¸“å®¶ååŒå­¦ä¹ çš„æ¡†æ¶ï¼Œå°†æºåŸŸæ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ç½®äºåŒç­‰é‡è¦çš„åœ°ä½ï¼Œå¹¶é€šè¿‡RAINæµç¨‹å®ç°æœ‰æ•ˆçš„æ— ç›‘ç£è®­ç»ƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEXCLèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼ŒæŒ–æ˜ç›®æ ‡åŸŸçš„æ½œåœ¨ç»“æ„ï¼Œä»è€Œæé«˜è‡ªé€‚åº”æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šConv-Adapterç”¨äºå¢å¼ºæºåŸŸæ¨¡å‹ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸçš„ç‰¹å¾ã€‚å¯è®­ç»ƒçš„æ–‡æœ¬æç¤ºç”¨äºå¼•å¯¼è§†è§‰-è¯­è¨€æ¨¡å‹å­¦ä¹ ç›®æ ‡åŸŸçš„çŸ¥è¯†ã€‚RAINæµç¨‹ä¸­çš„æ ·æœ¬æ£€ç´¢ç­–ç•¥æ—¨åœ¨é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æ ·æœ¬ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚å…±äº«å­¦ä¹ ç»“æœé€šè¿‡ä¸€è‡´æ€§æŸå¤±å‡½æ•°æ¥å®ç°ï¼Œç¡®ä¿ä¸¤ä¸ªä¸“å®¶å­¦ä¹ åˆ°ä¸€è‡´çš„è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEXCLæ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒEXCLåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„SFUDAæ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœéªŒè¯äº†åŒä¸“å®¶ååŒå­¦ä¹ å’ŒRAINæµç¨‹çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œç”±äºæ•°æ®éšç§æˆ–è·å–æˆæœ¬çš„é™åˆ¶ï¼Œæ— æ³•ç›´æ¥è®¿é—®æºåŸŸæ•°æ®ï¼Œå› æ­¤SFUDAæŠ€æœ¯å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚EXCLæ¡†æ¶çš„æå‡ºï¼Œä¸ºè§£å†³æ­¤ç±»é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic challenge of adapting a source-trained model to a target domain without access to the source data, driven by concerns over privacy and cost. Existing SFUDA methods either exploit only the source model's predictions or fine-tune large multimodal models, yet both neglect complementary insights and the latent structure of target data. In this paper, we propose the Experts Cooperative Learning (EXCL). EXCL contains the Dual Experts framework and Retrieval-Augmentation-Interaction optimization pipeline. The Dual Experts framework places a frozen source-domain model (augmented with Conv-Adapter) and a pretrained vision-language model (with a trainable text prompt) on equal footing to mine consensus knowledge from unlabeled target samples. To effectively train these plug-in modules under purely unsupervised conditions, we introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that (1) collaboratively retrieves pseudo-source and complex target samples, (2) separately fine-tunes each expert on its respective sample set, and (3) enforces learning object consistency via a shared learning result. Extensive experiments on four benchmark datasets demonstrate that our approach matches state-of-the-art performance.

