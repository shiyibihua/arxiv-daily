---
layout: default
title: EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking
---

# EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22019" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22019v1</a>
  <a href="https://arxiv.org/pdf/2509.22019.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22019v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22019v1', 'EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuki Sakai, Ryosuke Furuta, Juichun Yen, Yoichi Sato

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: Accepted to the I-HFM Workshop at ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EgoInstructï¼šç”¨äºäººé™…æ•™å­¦äº¤äº’çš„è‡ªä¸­å¿ƒè§†é¢‘æ•°æ®é›†ä¸å¤šæ¨¡æ€LLMåŸºå‡†æµ‹è¯•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸­å¿ƒè§†é¢‘` `äººé™…æ•™å­¦` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†` `ç¨‹åºæ­¥éª¤åˆ†å‰²` `ä¼šè¯çŠ¶æ€åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹äººé™…æ•™å­¦åœºæ™¯çš„ç³»ç»Ÿç ”ç©¶ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºå°‘åˆé€‚çš„æ•°æ®é›†å’Œæœ‰æ•ˆçš„åˆ†ææŠ€æœ¯ã€‚
2. è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªæ–°çš„è‡ªä¸­å¿ƒè§†é¢‘æ•°æ®é›†EgoInstructï¼Œå¹¶æ ‡æ³¨äº†ç¨‹åºæ­¥éª¤åˆ†å‰²å’Œä¼šè¯çŠ¶æ€åˆ†ç±»ä¸¤ä¸ªä»»åŠ¡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç†è§£äººé™…æ•™å­¦åœºæ™¯æ–¹é¢ä¼˜äºä¸“é—¨çš„åŸºçº¿æ¨¡å‹ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ†ææ•™å¸ˆå’Œå­¦ä¹ è€…åœ¨åŒä¸€ç‰©ç†ç©ºé—´ä¸­è¿›è¡Œçš„äººé™…æ•™å­¦äº’åŠ¨ï¼Œå¯¹äºæ•™è‚²æ”¯æŒå’ŒæŠ€èƒ½è½¬ç§»è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå°šæœªç³»ç»Ÿåœ°ç ”ç©¶è¿™ç§é¢å¯¹é¢çš„æ•™å­¦åœºæ™¯ã€‚æˆ‘ä»¬è®¤ä¸ºå­˜åœ¨ä¸¤ä¸ªä¸»è¦åŸå› ï¼ši) ç¼ºä¹åˆé€‚çš„æ•°æ®é›†ï¼›ii) åˆ†ææŠ€æœ¯æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„è‡ªä¸­å¿ƒè§†é¢‘æ•°æ®é›†ï¼Œç”¨äºäººé™…æ•™å­¦ï¼Œå¹¶ä¸ºä¸¤ä¸ªåŸºæœ¬ä»»åŠ¡æä¾›äº†çœŸå®æ ‡æ³¨ï¼Œä½œä¸ºå…¨é¢ç†è§£æ•™å­¦äº’åŠ¨çš„ç¬¬ä¸€æ­¥ï¼šç¨‹åºæ­¥éª¤åˆ†å‰²å’Œä¼šè¯çŠ¶æ€åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œä»¥ä¼ ç»Ÿä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¸ºåŸºå‡†ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚ç”±äºäººé™…æ•™å­¦æ¶‰åŠå¤šç§æ¨¡æ€ï¼ˆè¯­éŸ³å†…å®¹å’ŒéŸµå¾‹ã€æ³¨è§†å’Œèº«ä½“è¿åŠ¨ä»¥åŠè§†è§‰ä¸Šä¸‹æ–‡ï¼‰ï¼Œå› æ­¤æœ‰æ•ˆçš„ç†è§£éœ€è¦ä»¥é›†æˆæ–¹å¼å¤„ç†å£å¤´å’Œéå£å¤´äº¤æµçš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€è¿‘å¼•å…¥çš„è”åˆå¤„ç†å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬çš„MLLMã€‚è¯¥è¯„ä¼°é‡åŒ–äº†å½“å‰æœºå™¨å­¦ä¹ æ¨¡å‹ç†è§£äººé™…æ•™å­¦åœºæ™¯çš„ç¨‹åº¦ã€‚åœ¨å®éªŒä¸­ï¼ŒMLLMå³ä½¿æ²¡æœ‰ç»è¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œä¹Ÿä¼˜äºä¸“é—¨çš„åŸºçº¿æ¨¡å‹ï¼Œè¡¨æ˜å®ƒä»¬åœ¨å…¨é¢ç†è§£æ•™å­¦äº’åŠ¨æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰é¢†åŸŸå¯¹äººé™…æ•™å­¦åœºæ™¯ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ•™å¸ˆå’Œå­¦ä¹ è€…ä¹‹é—´å¤æ‚äº¤äº’çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶ä¸”ç¼ºå°‘é«˜è´¨é‡çš„æ•°æ®é›†æ¥æ”¯æŒç›¸å…³ç ”ç©¶ã€‚å› æ­¤ï¼Œç°æœ‰çš„æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°è¿›è¡Œç¨‹åºæ­¥éª¤åˆ†å‰²å’Œä¼šè¯çŠ¶æ€åˆ†ç±»ç­‰ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«å¤šç§æ¨¡æ€ä¿¡æ¯ï¼ˆè§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬ï¼‰çš„è‡ªä¸­å¿ƒè§†è§’æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥å­¦ä¹ å’Œç†è§£äººé™…æ•™å­¦åœºæ™¯ä¸­çš„å¤æ‚äº¤äº’ã€‚é€šè¿‡è”åˆå¤„ç†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼ŒMLLMèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•™å¸ˆå’Œå­¦ä¹ è€…ä¹‹é—´çš„å£å¤´å’Œéå£å¤´äº¤æµã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æ•°æ®é›†æ„å»ºï¼šæ”¶é›†å¹¶æ ‡æ³¨è‡ªä¸­å¿ƒè§†è§’çš„æ•™å­¦è§†é¢‘ï¼ŒåŒ…å«ç¨‹åºæ­¥éª¤åˆ†å‰²å’Œä¼šè¯çŠ¶æ€åˆ†ç±»çš„æ ‡æ³¨ä¿¡æ¯ã€‚2) åŸºçº¿æ¨¡å‹ï¼šé€‰æ‹©ä¼ ç»Ÿçš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹ä½œä¸ºåŸºçº¿ï¼Œä¾‹å¦‚ç”¨äºç¨‹åºæ­¥éª¤åˆ†å‰²çš„åºåˆ—æ¨¡å‹å’Œç”¨äºä¼šè¯çŠ¶æ€åˆ†ç±»çš„åˆ†ç±»å™¨ã€‚3) å¤šæ¨¡æ€LLMï¼šä½¿ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€LLMï¼Œä¾‹å¦‚èƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬çš„æ¨¡å‹ã€‚4) è¯„ä¼°ï¼šåœ¨æ„å»ºçš„æ•°æ®é›†ä¸Šè¯„ä¼°MLLMå’ŒåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†ä¸€ä¸ªæ–°çš„è‡ªä¸­å¿ƒè§†è§’çš„äººé™…æ•™å­¦æ•°æ®é›†EgoInstructï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„æ•°æ®ç©ºç™½ã€‚2) è¯æ˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç†è§£äººé™…æ•™å­¦åœºæ™¯æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå³ä½¿æ²¡æœ‰ç»è¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œä¹Ÿèƒ½å–å¾—ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹çš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®é›†çš„æ ‡æ³¨æ–¹æ¡ˆï¼Œé’ˆå¯¹ç¨‹åºæ­¥éª¤åˆ†å‰²å’Œä¼šè¯çŠ¶æ€åˆ†ç±»ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œæ ‡æ³¨ï¼Œä¿è¯äº†æ•°æ®çš„è´¨é‡å’Œå¯ç”¨æ€§ã€‚2) å¤šæ¨¡æ€LLMçš„é€‰æ‹©ï¼Œé€‰æ‹©äº†èƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬çš„æ¨¡å‹ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚3) å®éªŒè¯„ä¼°æ–¹æ¡ˆï¼Œé€šè¿‡ä¸ä¼ ç»ŸåŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼ŒéªŒè¯äº†MLLMçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨EgoInstructæ•°æ®é›†ä¸Šï¼Œå³ä½¿æ²¡æœ‰ç»è¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œä¹Ÿä¼˜äºä¸“é—¨çš„åŸºçº¿æ¨¡å‹ã€‚è¿™è¡¨æ˜MLLMåœ¨ç†è§£äººé™…æ•™å­¦åœºæ™¯æ–¹é¢å…·æœ‰å¼ºå¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å’Œåˆ©ç”¨ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåœ¨çº¿æ•™è‚²ã€æŠ€èƒ½åŸ¹è®­ã€è¿œç¨‹åä½œç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£äººé™…æ•™å­¦äº’åŠ¨ï¼Œå¯ä»¥å¼€å‘æ›´æ™ºèƒ½çš„æ•™å­¦è¾…åŠ©ç³»ç»Ÿï¼Œæå‡æ•™å­¦æ•ˆæœå’Œå­¦ä¹ ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„æ•™å­¦åœºæ™¯ï¼Œä¾‹å¦‚å¤šäººåä½œæ•™å­¦ã€ä¸ªæ€§åŒ–æ•™å­¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Analyzing instructional interactions between an instructor and a learner who are co-present in the same physical space is a critical problem for educational support and skill transfer. Yet such face-to-face instructional scenes have not been systematically studied in computer vision. We identify two key reasons: i) the lack of suitable datasets and ii) limited analytical techniques. To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification. Using this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models. Since face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner. Accordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text. This evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes. In experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.

