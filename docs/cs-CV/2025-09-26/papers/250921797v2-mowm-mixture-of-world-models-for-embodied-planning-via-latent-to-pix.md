---
layout: default
title: MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation
---

# MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21797" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21797v2</a>
  <a href="https://arxiv.org/pdf/2509.21797.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21797v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21797v2', 'MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: 11 pages, 4 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/tsinghua-fib-lab/MoWM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoWMï¼šä¸€ç§æ··åˆä¸–ç•Œæ¨¡å‹çš„å…·èº«è§„åˆ’æ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨åˆ°åƒç´ ç‰¹å¾è°ƒåˆ¶æå‡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `ä¸–ç•Œæ¨¡å‹` `åŠ¨ä½œè§„åˆ’` `æ··åˆæ¨¡å‹` `ç‰¹å¾è°ƒåˆ¶` `æœºå™¨äººå­¦ä¹ ` `CALVINåŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºåƒç´ é‡å»ºçš„ä¸–ç•Œæ¨¡å‹å­˜åœ¨è§†è§‰å†—ä½™ï¼Œå½±å“åŠ¨ä½œè§£ç å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. MoWMèåˆæ½œåœ¨ç©ºé—´å’Œåƒç´ ç©ºé—´ä¸–ç•Œæ¨¡å‹ï¼Œåˆ©ç”¨æ½œåœ¨æ¨¡å‹å¼•å¯¼åƒç´ ç‰¹å¾æå–ï¼Œçªå‡ºå…³é”®è§†è§‰ç»†èŠ‚ã€‚
3. åœ¨CALVINåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoWMå–å¾—äº†SOTAçš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«åŠ¨ä½œè§„åˆ’æ˜¯æœºå™¨äººé¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå®ƒè¦æ±‚æ¨¡å‹èƒ½å¤Ÿä»è§†è§‰è§‚å¯Ÿå’Œè¯­è¨€æŒ‡ä»¤ä¸­ç”Ÿæˆç²¾ç¡®çš„åŠ¨ä½œã€‚è§†é¢‘ç”Ÿæˆä¸–ç•Œæ¨¡å‹å¾ˆæœ‰å‰æ™¯ï¼Œä½†å®ƒä»¬å¯¹åƒç´ çº§é‡å»ºçš„ä¾èµ–å¼•å…¥äº†è§†è§‰å†—ä½™ï¼Œé˜»ç¢äº†åŠ¨ä½œè§£ç å’Œæ³›åŒ–ã€‚æ½œåœ¨ä¸–ç•Œæ¨¡å‹æä¾›äº†ä¸€ç§ç´§å‡‘çš„ã€è¿åŠ¨æ„ŸçŸ¥çš„è¡¨ç¤ºï¼Œä½†å¿½ç•¥äº†ç²¾ç¡®æ“ä½œæ‰€éœ€çš„ç»†ç²’åº¦ç»†èŠ‚ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†MoWMï¼Œä¸€ä¸ªæ··åˆä¸–ç•Œæ¨¡å‹æ¡†æ¶ï¼Œèåˆäº†æ¥è‡ªæ··åˆä¸–ç•Œæ¨¡å‹çš„è¡¨ç¤ºï¼Œç”¨äºå…·èº«åŠ¨ä½œè§„åˆ’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ¥è‡ªæ½œåœ¨æ¨¡å‹çš„è¿åŠ¨æ„ŸçŸ¥è¡¨ç¤ºä½œä¸ºé«˜å±‚å…ˆéªŒï¼ŒæŒ‡å¯¼ä»åƒç´ ç©ºé—´æ¨¡å‹ä¸­æå–ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ã€‚è¿™ç§è®¾è®¡å…è®¸MoWMçªå‡ºæ˜¾ç¤ºåŠ¨ä½œè§£ç æ‰€éœ€çš„æœ‰ç”¨è§†è§‰ç»†èŠ‚ã€‚åœ¨CALVINåŸºå‡†ä¸Šçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„ä»»åŠ¡æˆåŠŸç‡å’Œå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¯¹æ¯ä¸ªç‰¹å¾ç©ºé—´çš„ä¼˜åŠ¿è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œä¸ºå…·èº«è§„åˆ’çš„æœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚ä»£ç å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå…·èº«åŠ¨ä½œè§„åˆ’æ—¨åœ¨è®©æœºå™¨äººæ ¹æ®è§†è§‰è¾“å…¥å’Œè¯­è¨€æŒ‡ä»¤æ‰§è¡Œç²¾ç¡®åŠ¨ä½œã€‚ç°æœ‰åŸºäºåƒç´ é‡å»ºçš„ä¸–ç•Œæ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†ç”±äºè§†è§‰å†—ä½™ï¼Œéš¾ä»¥æå–æœ‰æ•ˆçš„åŠ¨ä½œä¿¡æ¯ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚è€Œæ½œåœ¨ä¸–ç•Œæ¨¡å‹è™½ç„¶ç´§å‡‘ï¼Œå´ä¸¢å¤±äº†åƒç´ çº§åˆ«çš„ç»†èŠ‚ä¿¡æ¯ï¼Œå½±å“æ“ä½œç²¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoWMçš„æ ¸å¿ƒæ€æƒ³æ˜¯ç»“åˆæ½œåœ¨ä¸–ç•Œæ¨¡å‹å’Œåƒç´ ä¸–ç•Œæ¨¡å‹çš„ä¼˜åŠ¿ã€‚åˆ©ç”¨æ½œåœ¨ä¸–ç•Œæ¨¡å‹å­¦ä¹ åˆ°çš„è¿åŠ¨æ„ŸçŸ¥çš„é«˜å±‚æŠ½è±¡è¡¨ç¤ºï¼Œä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼åƒç´ ä¸–ç•Œæ¨¡å‹æå–ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ—¢èƒ½ä¿ç•™å…³é”®çš„åƒç´ ç»†èŠ‚ï¼Œåˆèƒ½é¿å…è§†è§‰å†—ä½™ï¼Œä»è€Œæå‡åŠ¨ä½œè§„åˆ’çš„ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoWMæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„ä¸–ç•Œæ¨¡å‹åˆ†æ”¯ï¼šä¸€ä¸ªæ½œåœ¨ä¸–ç•Œæ¨¡å‹å’Œä¸€ä¸ªåƒç´ ä¸–ç•Œæ¨¡å‹ã€‚æ½œåœ¨ä¸–ç•Œæ¨¡å‹è´Ÿè´£å­¦ä¹ ç¯å¢ƒçš„æŠ½è±¡è¡¨ç¤ºå’Œè¿åŠ¨è§„å¾‹ï¼Œç”Ÿæˆè¿åŠ¨æ„ŸçŸ¥çš„æ½œåœ¨ç‰¹å¾ã€‚åƒç´ ä¸–ç•Œæ¨¡å‹è´Ÿè´£ä»åŸå§‹åƒç´ è¾“å…¥ä¸­æå–ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œåˆ©ç”¨æ½œåœ¨ç‰¹å¾å¯¹åƒç´ ç‰¹å¾è¿›è¡Œè°ƒåˆ¶ï¼Œçªå‡ºä¸åŠ¨ä½œç›¸å…³çš„å…³é”®è§†è§‰ä¿¡æ¯ã€‚æœ€åï¼Œå°†è°ƒåˆ¶åçš„ç‰¹å¾è¾“å…¥åˆ°åŠ¨ä½œè§£ç å™¨ä¸­ï¼Œç”Ÿæˆæœ€ç»ˆçš„åŠ¨ä½œæŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoWMçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ··åˆä¸–ç•Œæ¨¡å‹çš„æ¶æ„ï¼Œå¹¶è®¾è®¡äº†æ½œåœ¨åˆ°åƒç´ çš„ç‰¹å¾è°ƒåˆ¶æœºåˆ¶ã€‚è¿™ç§æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆæ¥è‡ªä¸åŒç‰¹å¾ç©ºé—´çš„è¡¨ç¤ºï¼Œæ—¢ä¿ç•™äº†é«˜å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œåˆé¿å…äº†åƒç´ çº§åˆ«çš„å†—ä½™ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMoWMèƒ½å¤Ÿæ›´å¥½åœ°æå–ä¸åŠ¨ä½œç›¸å…³çš„å…³é”®è§†è§‰ç‰¹å¾ï¼Œä»è€Œæå‡åŠ¨ä½œè§„åˆ’çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šMoWMä½¿ç”¨äº†å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä½œä¸ºæ½œåœ¨ä¸–ç•Œæ¨¡å‹ï¼Œå­¦ä¹ ç¯å¢ƒçš„æ½œåœ¨è¡¨ç¤ºã€‚åƒç´ ä¸–ç•Œæ¨¡å‹ä½¿ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–è§†è§‰ç‰¹å¾ã€‚ç‰¹å¾è°ƒåˆ¶æœºåˆ¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œåˆ©ç”¨æ½œåœ¨ç‰¹å¾ä½œä¸ºqueryï¼Œåƒç´ ç‰¹å¾ä½œä¸ºkeyå’Œvalueï¼Œè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œçªå‡ºä¸æ½œåœ¨ç‰¹å¾ç›¸å…³çš„åƒç´ ç‰¹å¾ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±ã€KLæ•£åº¦å’ŒåŠ¨ä½œé¢„æµ‹æŸå¤±ï¼Œç”¨äºè®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MoWMåœ¨CALVINåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„SOTAæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒMoWMåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoWMèƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆæ¥è‡ªä¸åŒç‰¹å¾ç©ºé—´çš„è¡¨ç¤ºï¼Œæå–ä¸åŠ¨ä½œç›¸å…³çš„å…³é”®è§†è§‰ç‰¹å¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoWMåœ¨æœºå™¨äººå…·èº«æ™ºèƒ½é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£ç¯å¢ƒï¼Œæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒMoWMè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.

