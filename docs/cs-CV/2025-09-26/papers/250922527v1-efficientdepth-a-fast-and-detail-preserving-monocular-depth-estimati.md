---
layout: default
title: EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model
---

# EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22527" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22527v1</a>
  <a href="https://arxiv.org/pdf/2509.22527.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22527v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22527v1', 'EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrii Litvynchuk, Ivan Livinsky, Anand Ravi, Nima Kalantari, Andrii Tsarov

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 12 pages, 7 figures, 5 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EfficientDepthï¼šä¸€ç§å¿«é€Ÿä¸”ä¿ç•™ç»†èŠ‚çš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `Transformer` `å·ç§¯ç¥ç»ç½‘ç»œ` `æ·±åº¦å­¦ä¹ ` `å‡ ä½•ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨å‡ ä½•ä¸€è‡´æ€§ã€ç»†èŠ‚ä¿ç•™ã€çœŸå®åœºæ™¯é²æ£’æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
2. EfficientDepthç»“åˆTransformeræ¶æ„å’Œè½»é‡çº§å·ç§¯è§£ç å™¨ï¼Œå¹¶å¼•å…¥åŒå³°å¯†åº¦å¤´ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç²¾ç»†çš„æ·±åº¦å›¾ä¼°è®¡ã€‚
3. é€šè¿‡åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥å’ŒåŸºäºLPIPSçš„æŸå¤±å‡½æ•°ï¼ŒEfficientDepthåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡æœ‰æ‰€æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰åœ¨æœºå™¨äººã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰å¤šç§è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸æ— æ³•æ»¡è¶³3Dé‡å»ºå’Œè§†å›¾åˆæˆçš„å…³é”®è¦æ±‚ï¼ŒåŒ…æ‹¬å‡ ä½•ä¸€è‡´æ€§ã€ç²¾ç»†ç»†èŠ‚ã€å¯¹åå°„è¡¨é¢ç­‰ç°å®ä¸–ç•ŒæŒ‘æˆ˜çš„é²æ£’æ€§ä»¥åŠè¾¹ç¼˜è®¾å¤‡çš„æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„MDEç³»ç»Ÿï¼Œç§°ä¸ºEfficientDepthï¼Œå®ƒç»“åˆäº†Transformeræ¶æ„ä¸è½»é‡çº§å·ç§¯è§£ç å™¨ï¼Œä»¥åŠå…è®¸ç½‘ç»œä¼°è®¡è¯¦ç»†æ·±åº¦å›¾çš„åŒå³°å¯†åº¦å¤´ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡è®°çš„åˆæˆå’ŒçœŸå®å›¾åƒä»¥åŠä½¿ç”¨é«˜æ€§èƒ½MDEæ–¹æ³•ç”Ÿæˆçš„ä¼ªæ ‡è®°çœŸå®å›¾åƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥æ¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶ç”Ÿæˆå¼ºè°ƒå‡ ä½•ä¸€è‡´æ€§å’Œç²¾ç»†ç»†èŠ‚çš„æ¨¡å‹ã€‚æœ€åï¼Œé™¤äº†å¸¸ç”¨çš„ç›®æ ‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºLPIPSçš„æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±ç½‘ç»œç”Ÿæˆè¯¦ç»†çš„æ·±åº¦å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficientDepthå®ç°äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰ä¸­ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•ä¸€è‡´æ€§ã€ç»†èŠ‚ä¿ç•™ã€çœŸå®åœºæ™¯é²æ£’æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œå¹¶ä¸”åœ¨å¤„ç†åå°„è¡¨é¢ç­‰å¤æ‚åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆTransformeræ¶æ„çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›å’Œå·ç§¯è§£ç å™¨çš„å±€éƒ¨ç»†èŠ‚æå–èƒ½åŠ›ï¼Œå¹¶å¼•å…¥åŒå³°å¯†åº¦å¤´ä»¥æ›´å¥½åœ°è¡¨ç¤ºæ·±åº¦åˆ†å¸ƒã€‚é€šè¿‡è¿™ç§æ··åˆæ¶æ„ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´ç²¾ç»†çš„æ·±åº¦å›¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEfficientDepthç³»ç»Ÿä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šTransformerç¼–ç å™¨ã€è½»é‡çº§å·ç§¯è§£ç å™¨å’ŒåŒå³°å¯†åº¦å¤´ã€‚Transformerç¼–ç å™¨è´Ÿè´£æå–å›¾åƒçš„å…¨å±€ç‰¹å¾ï¼Œå·ç§¯è§£ç å™¨è´Ÿè´£ä»ç¼–ç åçš„ç‰¹å¾ä¸­é‡å»ºæ·±åº¦å›¾ï¼ŒåŒå³°å¯†åº¦å¤´ç”¨äºé¢„æµ‹æ¯ä¸ªåƒç´ çš„æ·±åº¦åˆ†å¸ƒã€‚æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ··åˆæ¶æ„çš„è®¾è®¡ï¼Œå®ƒæœ‰æ•ˆåœ°ç»“åˆäº†Transformerå’Œå·ç§¯ç½‘ç»œçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒåŒå³°å¯†åº¦å¤´çš„è®¾è®¡å…è®¸æ¨¡å‹æ›´å¥½åœ°å¤„ç†æ·±åº¦ä¸ç¡®å®šæ€§ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„æ·±åº¦å›¾ã€‚åŸºäºLPIPSçš„æŸå¤±å‡½æ•°ä¹Ÿé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç²¾ç»†çš„æ·±åº¦å›¾ã€‚

**å…³é”®è®¾è®¡**ï¼šTransformerç¼–ç å™¨é‡‡ç”¨æ ‡å‡†çš„Transformerç»“æ„ï¼Œå·ç§¯è§£ç å™¨é‡‡ç”¨è½»é‡çº§è®¾è®¡ä»¥å‡å°‘è®¡ç®—é‡ã€‚åŒå³°å¯†åº¦å¤´é¢„æµ‹ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼Œç”¨äºè¡¨ç¤ºæ¯ä¸ªåƒç´ çš„æ·±åº¦åˆ†å¸ƒã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ·±åº¦å›å½’æŸå¤±ã€æ¢¯åº¦æŸå¤±å’ŒåŸºäºLPIPSçš„æŸå¤±ã€‚å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥åŒ…æ‹¬é¦–å…ˆåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficientDepthåœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¡¨æ˜EfficientDepthåœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EfficientDepthåœ¨æœºå™¨äººã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äº3Dé‡å»ºã€è§†å›¾åˆæˆã€åœºæ™¯ç†è§£å’Œå¯¼èˆªç­‰ä»»åŠ¡ã€‚ç”±äºå…¶é«˜æ•ˆæ€§ï¼ŒEfficientDepthç‰¹åˆ«é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä»è€Œå®ç°å®æ—¶çš„æ·±åº¦æ„ŸçŸ¥å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Monocular depth estimation (MDE) plays a pivotal role in various computer vision applications, such as robotics, augmented reality, and autonomous driving. Despite recent advancements, existing methods often fail to meet key requirements for 3D reconstruction and view synthesis, including geometric consistency, fine details, robustness to real-world challenges like reflective surfaces, and efficiency for edge devices. To address these challenges, we introduce a novel MDE system, called EfficientDepth, which combines a transformer architecture with a lightweight convolutional decoder, as well as a bimodal density head that allows the network to estimate detailed depth maps. We train our model on a combination of labeled synthetic and real images, as well as pseudo-labeled real images, generated using a high-performing MDE method. Furthermore, we employ a multi-stage optimization strategy to improve training efficiency and produce models that emphasize geometric consistency and fine detail. Finally, in addition to commonly used objectives, we introduce a loss function based on LPIPS to encourage the network to produce detailed depth maps. Experimental results demonstrate that EfficientDepth achieves performance comparable to or better than existing state-of-the-art models, with significantly reduced computational resources.

