---
layout: default
title: Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting
---

# Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22615" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22615v1</a>
  <a href="https://arxiv.org/pdf/2509.22615.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22615v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22615v1', 'Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yasmine Omri, Connor Ding, Tsachy Weissman, Thierry Tambe

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨2Dé«˜æ–¯æº…å°„å‹ç¼©å›¾åƒè¡¨ç¤ºå®ç°è§†è§‰-è¯­è¨€å¯¹é½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `2Dé«˜æ–¯æº…å°„` `è§†è§‰-è¯­è¨€å¯¹é½` `å›¾åƒå‹ç¼©` `è¾¹ç¼˜è®¡ç®—` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ä¾èµ–RGBå›¾åƒï¼Œå­˜åœ¨ä¼ è¾“æˆæœ¬é«˜å’Œtokenåºåˆ—è¿‡é•¿çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨2Dé«˜æ–¯æº…å°„(2DGS)ä½œä¸ºè§†è§‰è¡¨ç¤ºï¼Œå®ç°å›¾åƒå‹ç¼©å’Œé«˜æ•ˆçš„è§†è§‰-è¯­è¨€å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäº2DGSçš„ç¼–ç å™¨åœ¨å‹ç¼©å›¾åƒçš„åŒæ—¶ï¼Œå®ç°äº†å¯è§‚çš„é›¶æ ·æœ¬ImageNet-1Kæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£è§†è§‰è¯­è¨€æµç¨‹ä¾èµ–äºåœ¨æµ·é‡å›¾åƒæ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒçš„RGBè§†è§‰ç¼–ç å™¨ã€‚å°½ç®¡è¿™äº›æµç¨‹å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬èƒ½åŠ›å’Œå¼ºå¤§çš„è·¨ä»»åŠ¡è¿ç§»ï¼Œä½†å®ƒä»¬ä»ç„¶ç»§æ‰¿äº†åƒç´ åŸŸçš„ä¸¤ä¸ªç»“æ„æ€§ä½æ•ˆé—®é¢˜ï¼š(i) ä»è¾¹ç¼˜è®¾å¤‡å‘äº‘ç«¯ä¼ è¾“å¯†é›†çš„RGBå›¾åƒæ—¢è€—èƒ½åˆæ˜‚è´µï¼Œ(ii) åŸºäºpatchçš„tokenizationä¼šä½¿åºåˆ—é•¿åº¦çˆ†ç‚¸ï¼Œç»™æ³¨æ„åŠ›æœºåˆ¶å’Œä¸Šä¸‹æ–‡é™åˆ¶å¸¦æ¥å‹åŠ›ã€‚æˆ‘ä»¬æ¢ç´¢ä½¿ç”¨2Dé«˜æ–¯æº…å°„(2DGS)ä½œä¸ºè§†è§‰å¯¹é½çš„æ›¿ä»£åŸºåº•ï¼šä¸€ç§ç´§å‡‘çš„ã€ç©ºé—´è‡ªé€‚åº”çš„è¡¨ç¤ºï¼Œé€šè¿‡ä¸€ç»„å½©è‰²å„å‘å¼‚æ€§é«˜æ–¯å‡½æ•°æ¥å‚æ•°åŒ–å›¾åƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„2DGSæµç¨‹ï¼Œå…·æœ‰ç»“æ„åŒ–åˆå§‹åŒ–ã€äº®åº¦æ„ŸçŸ¥å‰ªæå’Œæ‰¹å¤„ç†CUDAå†…æ ¸ï¼Œä¸ä¹‹å‰çš„å®ç°ç›¸æ¯”ï¼Œå®ç°äº†è¶…è¿‡90å€çš„æ‹Ÿåˆé€Ÿåº¦æå‡å’Œçº¦97%çš„GPUåˆ©ç”¨ç‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡é‡ç”¨ä¸€ä¸ªå†»ç»“çš„åŸºäºRGBçš„Transformeréª¨å¹²ç½‘ç»œï¼Œä»¥åŠä¸€ä¸ªè½»é‡çº§çš„splatæ„ŸçŸ¥è¾“å…¥stemå’Œä¸€ä¸ªperceiver resamplerï¼Œå°†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒ(CLIP)é€‚é…åˆ°2DGSï¼Œä»…è®­ç»ƒçº¦7%çš„æ€»å‚æ•°ã€‚åœ¨å¤§å‹DataCompå­é›†ä¸Šï¼ŒGSç¼–ç å™¨åœ¨ç›¸å¯¹äºåƒç´ å‹ç¼©3åˆ°20å€è¾“å…¥çš„åŒæ—¶ï¼Œäº§ç”Ÿäº†æœ‰æ„ä¹‰çš„é›¶æ ·æœ¬ImageNet-1Kæ€§èƒ½ã€‚è™½ç„¶å‡†ç¡®ç‡ç›®å‰è½åäºRGBç¼–ç å™¨ï¼Œä½†æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†2DGSä½œä¸ºä¸€ç§å¯è¡Œçš„å¤šæ¨¡æ€åŸºåº•ï¼ŒæŒ‡å‡ºäº†æ¶æ„ç“¶é¢ˆï¼Œå¹¶ä¸ºè¯­ä¹‰å¼ºå¤§ä¸”ä¼ è¾“é«˜æ•ˆçš„è¾¹ç¼˜äº‘å­¦ä¹ è¡¨ç¤ºå¼€è¾Ÿäº†ä¸€æ¡é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ä¾èµ–äºRGBå›¾åƒï¼Œè¿™å¯¼è‡´äº†ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯å°†å¯†é›†çš„RGBå›¾åƒä»è¾¹ç¼˜è®¾å¤‡ä¼ è¾“åˆ°äº‘ç«¯éœ€è¦å¤§é‡çš„èƒ½é‡å’Œæˆæœ¬ï¼›äºŒæ˜¯åŸºäºpatchçš„tokenizationæ–¹æ³•ä¼šæ˜¾è‘—å¢åŠ åºåˆ—é•¿åº¦ï¼Œç»™æ¨¡å‹çš„è®¡ç®—èµ„æºå¸¦æ¥å‹åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´ç´§å‡‘ã€æ›´é«˜æ•ˆçš„å›¾åƒè¡¨ç¤ºæ–¹æ³•ï¼Œä»¥é™ä½ä¼ è¾“æˆæœ¬å’Œè®¡ç®—å¤æ‚åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä½¿ç”¨2Dé«˜æ–¯æº…å°„(2DGS)æ¥è¡¨ç¤ºå›¾åƒã€‚2DGSæ˜¯ä¸€ç§åŸºäºä¸€ç»„å½©è‰²å„å‘å¼‚æ€§é«˜æ–¯å‡½æ•°çš„å›¾åƒå‚æ•°åŒ–æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿä»¥ç´§å‡‘ä¸”ç©ºé—´è‡ªé€‚åº”çš„æ–¹å¼è¡¨ç¤ºå›¾åƒã€‚é€šè¿‡å°†å›¾åƒè¡¨ç¤ºä¸ºä¸€ç»„é«˜æ–¯å‡½æ•°ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘éœ€è¦ä¼ è¾“å’Œå¤„ç†çš„æ•°æ®é‡ï¼Œä»è€Œé™ä½ä¼ è¾“æˆæœ¬å’Œè®¡ç®—å¤æ‚åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è®ºæ–‡æå‡ºçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) 2DGS pipelineï¼šåŒ…æ‹¬ç»“æ„åŒ–åˆå§‹åŒ–ã€äº®åº¦æ„ŸçŸ¥å‰ªæå’Œæ‰¹å¤„ç†CUDAå†…æ ¸ï¼Œç”¨äºé«˜æ•ˆåœ°æ‹Ÿåˆ2DGSè¡¨ç¤ºã€‚2) CLIPé€‚é…ï¼šé‡ç”¨å†»ç»“çš„RGB-based Transformeréª¨å¹²ç½‘ç»œï¼Œå¹¶æ·»åŠ ä¸€ä¸ªè½»é‡çº§çš„splatæ„ŸçŸ¥è¾“å…¥stemå’Œä¸€ä¸ªperceiver resamplerï¼Œå°†CLIPæ¨¡å‹é€‚é…åˆ°2DGSè¡¨ç¤ºã€‚3) è®­ç»ƒï¼šä»…è®­ç»ƒsplatæ„ŸçŸ¥è¾“å…¥stemå’Œperceiver resamplerï¼Œä¿æŒTransformeréª¨å¹²ç½‘ç»œä¸å˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) å°†2DGSå¼•å…¥è§†è§‰-è¯­è¨€å¯¹é½ä»»åŠ¡ï¼Œæ¢ç´¢äº†ä¸€ç§æ–°çš„å›¾åƒè¡¨ç¤ºæ–¹æ³•ã€‚2) å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„2DGS pipelineï¼Œæ˜¾è‘—æé«˜äº†æ‹Ÿåˆé€Ÿåº¦å’ŒGPUåˆ©ç”¨ç‡ã€‚3) é€šè¿‡é‡ç”¨ç°æœ‰çš„RGB-based Transformeréª¨å¹²ç½‘ç»œï¼Œå¹¶æ·»åŠ è½»é‡çº§çš„é€‚é…æ¨¡å—ï¼Œå®ç°äº†é«˜æ•ˆçš„CLIPæ¨¡å‹è¿ç§»ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè¯¥æ–¹æ³•ä¸å†ä¾èµ–äºåƒç´ çº§åˆ«çš„å›¾åƒè¡¨ç¤ºï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç§æ›´ç´§å‡‘ã€æ›´é«˜æ•ˆçš„é«˜æ–¯å‡½æ•°è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨2DGS pipelineä¸­ï¼Œé‡‡ç”¨äº†ç»“æ„åŒ–åˆå§‹åŒ–æ–¹æ³•ï¼Œä»¥åŠ é€Ÿé«˜æ–¯å‡½æ•°çš„æ”¶æ•›ã€‚äº®åº¦æ„ŸçŸ¥å‰ªæç”¨äºå»é™¤ä¸é‡è¦çš„é«˜æ–¯å‡½æ•°ï¼Œè¿›ä¸€æ­¥å‹ç¼©å›¾åƒè¡¨ç¤ºã€‚æ‰¹å¤„ç†CUDAå†…æ ¸ç”¨äºåŠ é€Ÿè®¡ç®—è¿‡ç¨‹ï¼Œæé«˜GPUåˆ©ç”¨ç‡ã€‚åœ¨CLIPé€‚é…ä¸­ï¼Œsplatæ„ŸçŸ¥è¾“å…¥stemç”¨äºå°†2DGSè¡¨ç¤ºè½¬æ¢ä¸ºTransformerå¯ä»¥å¤„ç†çš„è¾“å…¥æ ¼å¼ã€‚Perceiver resamplerç”¨äºå°†ä¸åŒæ•°é‡çš„é«˜æ–¯å‡½æ•°è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹DataCompå­é›†ä¸Šï¼Œèƒ½å¤Ÿåœ¨ç›¸å¯¹äºåƒç´ å‹ç¼©3åˆ°20å€è¾“å…¥çš„åŒæ—¶ï¼Œäº§ç”Ÿæœ‰æ„ä¹‰çš„é›¶æ ·æœ¬ImageNet-1Kæ€§èƒ½ã€‚ä¸ä¹‹å‰çš„å®ç°ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¶…è¿‡90å€çš„æ‹Ÿåˆé€Ÿåº¦æå‡å’Œçº¦97%çš„GPUåˆ©ç”¨ç‡ã€‚è™½ç„¶å‡†ç¡®ç‡ç›®å‰è½åäºRGBç¼–ç å™¨ï¼Œä½†è¯¥ç ”ç©¶ç¡®ç«‹äº†2DGSä½œä¸ºä¸€ç§å¯è¡Œçš„å¤šæ¨¡æ€åŸºåº•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡è§†è§‰ã€äº‘ç«¯å›¾åƒå¤„ç†ç­‰é¢†åŸŸã€‚é€šè¿‡ä½¿ç”¨2DGSå‹ç¼©å›¾åƒï¼Œå¯ä»¥é™ä½æ•°æ®ä¼ è¾“å¸¦å®½éœ€æ±‚ï¼Œå‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆã€æ›´ç»æµçš„è§†è§‰-è¯­è¨€åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.

