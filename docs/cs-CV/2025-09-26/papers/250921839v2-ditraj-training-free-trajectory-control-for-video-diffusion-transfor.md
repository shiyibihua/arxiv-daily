---
layout: default
title: DiTraj: training-free trajectory control for video diffusion transformer
---

# DiTraj: training-free trajectory control for video diffusion transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21839" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21839v2</a>
  <a href="https://arxiv.org/pdf/2509.21839.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21839v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21839v2', 'DiTraj: training-free trajectory control for video diffusion transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang, Yiqiang Yan, Fei Su, Zhicheng Zhao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiTrajï¼Œä¸€ç§é¢å‘è§†é¢‘æ‰©æ•£Transformerçš„å…è®­ç»ƒè½¨è¿¹æ§åˆ¶æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `è½¨è¿¹æ§åˆ¶` `æ‰©æ•£æ¨¡å‹` `Transformer` `ä½ç½®ç¼–ç ` `å…è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è½¨è¿¹æ§åˆ¶æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒèµ„æºæˆ–ä¸“ä¸ºU-Netè®¾è®¡ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨DiTçš„å“è¶Šæ€§èƒ½ã€‚
2. DiTrajé€šè¿‡å‰æ™¯-èƒŒæ™¯åˆ†ç¦»å¼•å¯¼å’Œå¸§é—´æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼Œå®ç°å…è®­ç»ƒçš„è½¨è¿¹æ§åˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDiTrajåœ¨è§†é¢‘è´¨é‡å’Œè½¨è¿¹å¯æ§æ€§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºDiTrajï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å…è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„è½¨è¿¹æ§åˆ¶ï¼Œä¸“é—¨ä¸ºDiffusion Transformers (DiT)è®¾è®¡ã€‚é¦–å…ˆï¼Œä¸ºäº†æ³¨å…¥å¯¹è±¡çš„è½¨è¿¹ï¼Œæˆ‘ä»¬æå‡ºäº†å‰æ™¯-èƒŒæ™¯åˆ†ç¦»å¼•å¯¼ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†ç”¨æˆ·æä¾›çš„æç¤ºè½¬æ¢ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æç¤ºï¼Œåˆ†åˆ«æŒ‡å¯¼è§†é¢‘ä¸­å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„ç”Ÿæˆã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ†æäº†3Då…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶æ¢ç´¢äº†tokené—´æ³¨æ„åŠ›åˆ†æ•°ä¸ä½ç½®åµŒå…¥ä¹‹é—´çš„ç´§å¯†ç›¸å…³æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¸§é—´æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰ã€‚é€šè¿‡ä»…ä¿®æ”¹å‰æ™¯tokençš„ä½ç½®åµŒå…¥ï¼ŒSTD-RoPEæ¶ˆé™¤äº†å®ƒä»¬ä¹‹é—´çš„è·¨å¸§ç©ºé—´å·®å¼‚ï¼ŒåŠ å¼ºäº†å®ƒä»¬ä¹‹é—´çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œä»è€Œå¢å¼ºäº†è½¨è¿¹æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒèŠ‚ä½ç½®åµŒå…¥çš„å¯†åº¦æ¥å®ç°3Dæ„ŸçŸ¥çš„è½¨è¿¹æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘è´¨é‡å’Œè½¨è¿¹å¯æ§æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºDiffusion Transformer (DiT) çš„æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æ§åˆ¶è§†é¢‘ä¸­ç‰¹å®šç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„è½¨è¿¹æ§åˆ¶æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œæˆ–è€…ä¸“é—¨ä¸ºU-Netæ¶æ„è®¾è®¡ï¼Œæ— æ³•ç›´æ¥åº”ç”¨äºDiTæ¨¡å‹ï¼Œé™åˆ¶äº†DiTåœ¨å¯æ§è§†é¢‘ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiTrajçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è§£è€¦å‰æ™¯å’ŒèƒŒæ™¯çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨æ”¹è¿›çš„æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰æ¥ç²¾ç¡®æ§åˆ¶å‰æ™¯ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†ç”¨æˆ·æç¤ºåˆ†è§£ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æç¤ºï¼Œåˆ†åˆ«æŒ‡å¯¼ç›¸åº”åŒºåŸŸçš„ç”Ÿæˆã€‚ç„¶åï¼Œé€šè¿‡ä¿®æ”¹å‰æ™¯tokençš„ä½ç½®åµŒå…¥ï¼Œå¢å¼ºå®ƒä»¬ä¹‹é—´çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œä»è€Œå®ç°å¯¹è½¨è¿¹çš„ç²¾ç¡®æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiTrajæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼š1) å‰æ™¯-èƒŒæ™¯åˆ†ç¦»å¼•å¯¼æ¨¡å—ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºå‰æ™¯æç¤ºå’ŒèƒŒæ™¯æç¤ºï¼Œåˆ†åˆ«ç”¨äºæŒ‡å¯¼è§†é¢‘ä¸­å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„ç”Ÿæˆã€‚2) å¸§é—´æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰æ¨¡å—ï¼šé€šè¿‡ä¿®æ”¹å‰æ™¯tokençš„ä½ç½®åµŒå…¥ï¼Œæ¶ˆé™¤å®ƒä»¬ä¹‹é—´çš„è·¨å¸§ç©ºé—´å·®å¼‚ï¼Œå¢å¼ºè·¨å¸§æ³¨æ„åŠ›ï¼Œä»è€Œå®ç°å¯¹è½¨è¿¹çš„ç²¾ç¡®æ§åˆ¶ã€‚æ•´ä¸ªæµç¨‹æ— éœ€é¢å¤–çš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDiTrajçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¸§é—´æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰ã€‚ä¸ä¼ ç»Ÿçš„RoPEç›¸æ¯”ï¼ŒSTD-RoPEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ§åˆ¶å‰æ™¯tokençš„è¿åŠ¨è½¨è¿¹ï¼Œå› ä¸ºå®ƒåªä¿®æ”¹å‰æ™¯tokençš„ä½ç½®åµŒå…¥ï¼Œä»è€Œé¿å…äº†å¯¹èƒŒæ™¯åŒºåŸŸçš„å½±å“ã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒèŠ‚ä½ç½®åµŒå…¥çš„å¯†åº¦ï¼ŒDiTrajè¿˜å¯ä»¥å®ç°3Dæ„ŸçŸ¥çš„è½¨è¿¹æ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‰æ™¯-èƒŒæ™¯åˆ†ç¦»å¼•å¯¼æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„promptå·¥ç¨‹æŠ€æœ¯ï¼Œä»¥ç¡®ä¿LLMèƒ½å¤Ÿå‡†ç¡®åœ°æå–å‰æ™¯å’ŒèƒŒæ™¯ä¿¡æ¯ã€‚åœ¨STD-RoPEæ¨¡å—ä¸­ï¼Œå…³é”®åœ¨äºå¦‚ä½•ç¡®å®šå“ªäº›tokenå±äºå‰æ™¯ï¼Œè¿™å¯ä»¥é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æˆ–è¯­ä¹‰åˆ†å‰²ç­‰æ–¹æ³•æ¥å®ç°ã€‚æ­¤å¤–ï¼Œä½ç½®åµŒå…¥å¯†åº¦çš„è°ƒèŠ‚ç­–ç•¥ä¹Ÿéœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDiTrajåœ¨è§†é¢‘è´¨é‡å’Œè½¨è¿¹å¯æ§æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒDiTrajèƒ½å¤Ÿç”Ÿæˆæ›´æ¸…æ™°ã€æ›´é€¼çœŸçš„è§†é¢‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ§åˆ¶å‰æ™¯ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ã€‚å®šé‡è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºï¼ŒDiTrajåœ¨è½¨è¿¹è¯¯å·®æ–¹é¢æ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è§†é¢‘è´¨é‡è¯„åˆ†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiTrajåœ¨è§†é¢‘ç¼–è¾‘ã€æ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨DiTrajè½»æ¾åœ°æ§åˆ¶è§†é¢‘ä¸­è§’è‰²çš„è¿åŠ¨è½¨è¿¹ï¼Œæˆ–è€…åˆ›å»ºå…·æœ‰ç‰¹å®šè¿åŠ¨æ¨¡å¼çš„ç‰¹æ•ˆã€‚æ­¤å¤–ï¼ŒDiTrajè¿˜å¯ä»¥ç”¨äºç”Ÿæˆå…·æœ‰ç‰¹å®šæ•…äº‹æƒ…èŠ‚çš„åŠ¨ç”»è§†é¢‘ï¼Œä»è€Œé™ä½åŠ¨ç”»åˆ¶ä½œçš„æˆæœ¬å’Œéš¾åº¦ã€‚è¯¥ç ”ç©¶ä¸ºå¯æ§è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œæœ‰æœ›æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.

