---
layout: default
title: ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models
---

# ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21991" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.21991v1</a>
  <a href="https://arxiv.org/pdf/2509.21991.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21991v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21991v1', 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/nota-github/ERGO)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ERGOÔºåÈÄöËøáÁ≤óÂà∞Á≤æÊé®ÁêÜÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®È´òÂàÜËæ®ÁéáÂõæÂÉèÁêÜËß£‰∏≠ÁöÑÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `È´òÂàÜËæ®ÁéáÂõæÂÉè` `Á≤óÂà∞Á≤æÊé®ÁêÜ` `Âº∫ÂåñÂ≠¶‰π†` `Êé®ÁêÜÈ©±Âä®ÊÑüÁü•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊó∂ÔºåÁî±‰∫éËßÜËßâtokensÊï∞ÈáèÂ∫ûÂ§ßÔºåÂØºËá¥ËÆ°ÁÆóÂºÄÈîÄÊòæËëóÂ¢ûÂä†„ÄÇ
2. ERGOÈááÁî®‰∏§Èò∂ÊÆµÁ≤óÂà∞Á≤æÊé®ÁêÜÔºåÂÖàËØÜÂà´‰ªªÂä°Áõ∏ÂÖ≥Âå∫ÂüüÔºåÂÜçÂØπËøô‰∫õÂå∫ÂüüËøõË°åÂÖ®ÂàÜËæ®ÁéáÂ§ÑÁêÜÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ
3. ERGOÂú®V*Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äQwen2.5-VL-7B 4.7‰∏™ÁÇπÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®23%ÁöÑËßÜËßâtokensÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá3ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜÂú®ÂÆûÈôÖËßÜËßâËØ≠Ë®ÄÂ∫îÁî®‰∏≠È´òÊïàÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ERGOÔºàEfficient Reasoning & Guided ObservationÔºâÁöÑ‰∏§Èò∂ÊÆµ‚ÄúÁ≤óÂà∞Á≤æ‚ÄùÊé®ÁêÜÊµÅÁ®ã„ÄÇËØ•ÊµÅÁ®ãÈ¶ñÂÖàÂàÜÊûêÈôçÈááÊ†∑ÂõæÂÉè‰ª•ËØÜÂà´‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂå∫ÂüüÔºåÁÑ∂Âêé‰ªÖË£ÅÂâ™Ëøô‰∫õÂå∫ÂüüÂπ∂‰ª•ÂÖ®ÂàÜËæ®ÁéáËøõË°åÂêéÁª≠Êé®ÁêÜ„ÄÇËøôÁßçÊñπÊ≥ïÂú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂Ôºå‰øùÁïô‰∫ÜÂøÖË¶ÅÁöÑÁ≤æÁªÜËßÜËßâÁªÜËäÇ„ÄÇERGOÈÄöËøáÂà©Áî®Â§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÊâßË°åÊé®ÁêÜÈ©±Âä®ÁöÑÊÑüÁü•Ôºå‰ªéËÄåÁ°ÆÂÆöÂÖ≥Ê≥®Âå∫ÂüüÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®ËæìÂÖ•ÂõæÂÉèÈôçÈááÊ†∑ÂêéÁ¨¨‰∏ÄÈò∂ÊÆµÁöÑÂ§±ÊïàÈóÆÈ¢ò„ÄÇËØ•Ê®°ÂûãÂèØ‰ª•ËÄÉËôëÊÑüÁü•‰∏çÁ°ÆÂÆöÊÄßÔºåÊâ©Â§ßË£ÅÂâ™Âå∫Âüü‰ª•Ë¶ÜÁõñËßÜËßâÊ®°Á≥äÂå∫ÂüüÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ÂõûÁ≠îÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂‰∏≠ÂºÄÂèëÁÆÄÂçïËÄåÊúâÊïàÁöÑÂ•ñÂä±ÁªÑ‰ª∂ÔºåÂÆûÁé∞‰∫ÜÁ≤óÂà∞Á≤æÁöÑÊÑüÁü•„ÄÇÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåERGOÂú®ÊïàÁéáÊõ¥È´òÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØîÂéüÂßãÊ®°ÂûãÂíåÁ´û‰∫âÊñπÊ≥ïÂÖ∑ÊúâÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰æãÂ¶ÇÔºåERGOÂú®V*Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖËøáQwen2.5-VL-7B 4.7‰∏™ÁÇπÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®23%ÁöÑËßÜËßâtokensÔºåÂÆûÁé∞‰∫Ü3ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊó∂Èù¢‰∏¥Â∑®Â§ßÁöÑËÆ°ÁÆóË¥üÊãÖÔºåÂõ†‰∏∫ËßÜËßâtokensÁöÑÊï∞ÈáèÈùûÂ∏∏Â∫ûÂ§ß„ÄÇÂ∞§ÂÖ∂ÊòØÂú®‚ÄúÁî®ÂõæÂÉèÊÄùËÄÉ‚ÄùÁöÑÊ®°Âûã‰∏≠ÔºåÊé®ÁêÜ‰∏ç‰ªÖÈôê‰∫éÊñáÊú¨ÔºåËøòÊâ©Â±ïÂà∞ËßÜËßâÈ¢ÜÂüüÔºåËøô‰ΩøÂæóÈ´òÊïàÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÂèòÂæóÊõ¥Âä†ÈáçË¶Å„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÔºåÁâπÂà´ÊòØÈÇ£‰∫õ‰æùËµñ‰∫éÊÑüÁü•È©±Âä®Êé®ÁêÜÁöÑÊñπÊ≥ïÔºåÂú®ÂõæÂÉèÈôçÈááÊ†∑ÂêéÂæÄÂæÄ‰ºöÂ§±ÊïàÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈúÄË¶ÅÊ∏ÖÊô∞ÁöÑËßÜËßâ‰ø°ÊÅØÊâçËÉΩËøõË°åÊúâÊïàÁöÑÊé®ÁêÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöERGOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®‰∏ÄÁßç‰∏§Èò∂ÊÆµÁöÑ‚ÄúÁ≤óÂà∞Á≤æ‚ÄùÊé®ÁêÜÊµÅÁ®ã„ÄÇÈ¶ñÂÖàÔºåÂØπÈôçÈááÊ†∑ÂêéÁöÑÂõæÂÉèËøõË°åÂàÜÊûêÔºå‰ª•ËØÜÂà´‰∏éÁªôÂÆöÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÂå∫Âüü„ÄÇÁÑ∂ÂêéÔºå‰ªÖÂ∞ÜËøô‰∫õÂå∫ÂüüË£ÅÂâ™Âá∫Êù•ÔºåÂπ∂‰ª•ÂÖ®ÂàÜËæ®ÁéáËøõË°åÂ§ÑÁêÜÔºåÁî®‰∫éÂêéÁª≠ÁöÑÊé®ÁêÜÈò∂ÊÆµ„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®ÂáèÂ∞ëËÆ°ÁÆóÊàêÊú¨ÔºåÂêåÊó∂‰øùÁïôÂøÖË¶ÅÁöÑÁ≤æÁªÜËßÜËßâÁªÜËäÇ„ÄÇERGOÈÄöËøáÊé®ÁêÜÈ©±Âä®ÁöÑÊÑüÁü•ÔºåÂà©Áî®Â§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÊù•Á°ÆÂÆöÂ∫îËØ•ÂÖ≥Ê≥®Âì™‰∫õÂå∫ÂüüÔºå‰ªéËÄåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®ÈôçÈááÊ†∑ÂõæÂÉè‰∏äÁöÑÂ§±ÊïàÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöERGOÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ≤óÁï•ÊÑüÁü•Èò∂ÊÆµÂíåÁ≤æÁªÜÊé®ÁêÜÈò∂ÊÆµ„ÄÇÂú®Á≤óÁï•ÊÑüÁü•Èò∂ÊÆµÔºåÊ®°ÂûãÊé•Êî∂ÈôçÈááÊ†∑ÂêéÁöÑÂõæÂÉèÂíåÊñáÊú¨Êü•ËØ¢‰Ωú‰∏∫ËæìÂÖ•ÔºåÂπ∂‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†Êù•Â≠¶‰π†Â¶Ç‰ΩïÈÄâÊã©‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÂå∫Âüü„ÄÇÂú®Á≤æÁªÜÊé®ÁêÜÈò∂ÊÆµÔºåÊ®°ÂûãÂ∞ÜË£ÅÂâ™Âá∫ÁöÑÈ´òÂàÜËæ®ÁéáÂå∫Âüü‰∏éÂéüÂßãÊñáÊú¨Êü•ËØ¢ÁªìÂêàËµ∑Êù•ÔºåËøõË°åÊúÄÁªàÁöÑÊé®ÁêÜÂíåÁ≠îÊ°àÁîüÊàê„ÄÇÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Áî®‰∫é‰ºòÂåñÂå∫ÂüüÈÄâÊã©Á≠ñÁï•ÔºåÂ•ñÂä±Ê®°ÂûãÈÄâÊã©ËÉΩÂ§ü‰∫ßÁîüÂáÜÁ°ÆÁ≠îÊ°àÁöÑÂå∫Âüü„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöERGOÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Êé®ÁêÜÈ©±Âä®ÁöÑÊÑüÁü•Êú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÊÑüÁü•È©±Âä®Êé®ÁêÜ‰∏çÂêåÔºåERGOÂà©Áî®Â§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÔºàÂåÖÊã¨ÊñáÊú¨Êü•ËØ¢ÔºâÊù•ÊåáÂØºËßÜËßâÂå∫ÂüüÁöÑÈÄâÊã©„ÄÇËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏Ê®°ÂûãÂú®ÊÑüÁü•‰∏çÁ°ÆÂÆöÊÄßÂ≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâ©Â§ßË£ÅÂâ™Âå∫Âüü‰ª•Ë¶ÜÁõñËßÜËßâÊ®°Á≥äÁöÑÂå∫ÂüüÔºå‰ªéËÄåÊèêÈ´òÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåERGO‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†Êù•‰ºòÂåñÂå∫ÂüüÈÄâÊã©Á≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Â≠¶‰π†Â¶Ç‰ΩïÈÄâÊã©ÊúÄÁõ∏ÂÖ≥ÁöÑÂå∫Âüü„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöERGOÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÂåÖÂê´Â§ö‰∏™Â•ñÂä±ÁªÑ‰ª∂ÔºåÊó®Âú®ÈºìÂä±Ê®°ÂûãÈÄâÊã©ËÉΩÂ§ü‰∫ßÁîüÂáÜÁ°ÆÁ≠îÊ°àÁöÑÂå∫Âüü„ÄÇËøô‰∫õÂ•ñÂä±ÁªÑ‰ª∂ÂåÖÊã¨ÔºöÂáÜÁ°ÆÊÄßÂ•ñÂä±ÔºåÁî®‰∫éÂ•ñÂä±Ê®°ÂûãÁîüÊàêÊ≠£Á°ÆÁ≠îÊ°àÔºõÊïàÁéáÂ•ñÂä±ÔºåÁî®‰∫éÊÉ©ÁΩöÊ®°ÂûãÈÄâÊã©ËøáÂ§ßÁöÑÂå∫ÂüüÔºõ‰ª•ÂèäË¶ÜÁõñÁéáÂ•ñÂä±ÔºåÁî®‰∫éÈºìÂä±Ê®°ÂûãË¶ÜÁõñËßÜËßâÊ®°Á≥äÁöÑÂå∫Âüü„ÄÇÊ®°Âûã‰ΩøÁî®Actor-CriticÁÆóÊ≥ïËøõË°åËÆ≠ÁªÉÔºåActorÁΩëÁªúË¥üË¥£ÈÄâÊã©Âå∫ÂüüÔºåCriticÁΩëÁªúË¥üË¥£ËØÑ‰º∞ÊâÄÈÄâÂå∫ÂüüÁöÑË¥®Èáè„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ERGOÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®V*Âü∫ÂáÜÊµãËØï‰∏≠ÔºåERGOË∂ÖË∂ä‰∫ÜQwen2.5-VL-7B 4.7‰∏™ÁÇπÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®‰∫Ü23%ÁöÑËßÜËßâtokensÔºåÂÆûÁé∞‰∫Ü3ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåERGOËÉΩÂ§üÂú®‰øùÊåÅÁîöËá≥ÊèêÈ´òÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩøÂÖ∂Êàê‰∏∫È´òÂàÜËæ®ÁéáËßÜËßâËØ≠Ë®Ä‰ªªÂä°ÁöÑÊúâÊïàËß£ÂÜ≥ÊñπÊ°à„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂ∑≤ÂºÄÊ∫ê„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ERGOÈÄÇÁî®‰∫éÈúÄË¶ÅÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁöÑÂêÑÁßçËßÜËßâËØ≠Ë®Ä‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞„ÄÅÁõÆÊ†áÊ£ÄÊµãÁ≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•Â∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÂåªÁñóÂΩ±ÂÉèÂàÜÊûê„ÄÅÊô∫ËÉΩÈõ∂ÂîÆÁ≠âÈ¢ÜÂüüÔºåÊèêÈ´òÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩÂíåÊïàÁéá„ÄÇÊú™Êù•ÔºåERGOÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Â§ÑÁêÜËßÜÈ¢ëÁ≠âÂ§öÊ®°ÊÄÅÊï∞ÊçÆÔºåÂπ∂‰∏éÂÖ∂‰ªñÈ´òÊïàÊé®ÁêÜÊäÄÊúØÁõ∏ÁªìÂêàÔºåÂÆûÁé∞Êõ¥Âº∫Â§ßÁöÑËßÜËßâËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of "thinking with images" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.

