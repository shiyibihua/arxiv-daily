---
layout: default
title: MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning
---

# MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22761" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22761v2</a>
  <a href="https://arxiv.org/pdf/2509.22761.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22761v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22761v2', 'MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yapeng Mi, Hengli Li, Yanpeng Zhao, Chenxi Li, Huimin Wu, Xiaojian Ma, Song-Chun Zhu, Ying Nian Wu, Qing Li

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-12-04)

**å¤‡æ³¨**: 21 pages,13 figures,9 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMILRï¼Œé€šè¿‡æµ‹è¯•æ—¶æ½œåœ¨æ¨ç†æå‡å¤šæ¨¡æ€å›¾åƒç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å›¾åƒç”Ÿæˆ` `æµ‹è¯•æ—¶æ¨ç†` `æ½œåœ¨ç©ºé—´æ¨ç†` `ç­–ç•¥æ¢¯åº¦` `è·¨æ¨¡æ€ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ¨ç†çš„å›¾åƒç”Ÿæˆæ–¹æ³•é€šå¸¸å±€é™äºå•æ¨¡æ€æ¨ç†ï¼Œæˆ–ä¾èµ–é«˜è´¨é‡çš„æ¨ç†æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå­˜åœ¨å±€é™æ€§ã€‚
2. MILRçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµ‹è¯•æ—¶ï¼Œé€šè¿‡åœ¨ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­è”åˆæ¨ç†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMILRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMILRçš„æµ‹è¯•æ—¶æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å›¾åƒç”Ÿæˆæ•ˆæœã€‚MILRåœ¨ç»Ÿä¸€çš„æ½œåœ¨å‘é‡ç©ºé—´ä¸­è”åˆæ¨ç†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ã€‚æ¨ç†è¿‡ç¨‹é€šè¿‡æœç´¢ç¦»æ•£å›¾åƒå’Œæ–‡æœ¬tokençš„å‘é‡è¡¨ç¤ºæ¥å®ç°ï¼Œå…·ä½“é‡‡ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå¹¶ç”±å›¾åƒè´¨é‡è¯„ä»·å™¨æŒ‡å¯¼ã€‚MILRåœ¨ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼ˆMUGï¼‰æ¡†æ¶å†…å®ç°ï¼Œè¯¥æ¡†æ¶åŸç”Ÿæ”¯æŒå›¾åƒåˆæˆå‰çš„è¯­è¨€æ¨ç†ï¼Œä»è€Œä¿ƒè¿›è·¨æ¨¡æ€æ¨ç†ã€‚å¾…ä¼˜åŒ–çš„ä¸­é—´æ¨¡å‹è¾“å‡ºä½œä¸ºç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œä½¿MILRèƒ½å¤Ÿåœ¨å®Œå…¨æµ‹è¯•æ—¶è¿è¡Œã€‚åœ¨GenEvalã€T2I-CompBenchå’ŒWISEä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMILRåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨çŸ¥è¯†å¯†é›†å‹WISEä¸Šï¼ŒMILRçš„æ€»åˆ†è¾¾åˆ°0.63ï¼Œæ¯”åŸºçº¿æé«˜äº†80%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œç»Ÿä¸€æ½œåœ¨ç©ºé—´ä¸­çš„è”åˆæ¨ç†æ˜¯å…¶å¼ºå¤§æ€§èƒ½çš„å…³é”®ã€‚æ­¤å¤–ï¼Œå®šæ€§ç ”ç©¶æ­ç¤ºäº†MILRåœ¨æ—¶é—´å’Œæ–‡åŒ–æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œçªå‡ºäº†è¯¥æ¨ç†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæ¨ç†çš„å›¾åƒç”Ÿæˆæ–¹æ³•è¦ä¹ˆä»…é™äºå¯¹å›¾åƒæˆ–æ–‡æœ¬è¿›è¡Œå•æ¨¡æ€æ¨ç†ï¼Œè¦ä¹ˆéœ€è¦é«˜è´¨é‡çš„æ¨ç†æ•°æ®è¿›è¡Œå¾®è°ƒã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è·¨æ¨¡æ€çŸ¥è¯†æ¨ç†çš„åœºæ™¯ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMILRçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æµ‹è¯•é˜¶æ®µï¼Œé€šè¿‡åœ¨ç»Ÿä¸€çš„æ½œåœ¨å‘é‡ç©ºé—´ä¸­è”åˆæ¨ç†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯æ¥æå‡å›¾åƒç”Ÿæˆè´¨é‡ã€‚è¿™ç§è”åˆæ¨ç†å…è®¸æ¨¡å‹åŒæ—¶è€ƒè™‘å›¾åƒå’Œæ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆè¦æ±‚çš„å›¾åƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMILRæ„å»ºäºç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼ˆMUGï¼‰æ¡†æ¶ä¹‹ä¸Šã€‚MUGæ¡†æ¶é¦–å…ˆè¿›è¡Œè¯­è¨€æ¨ç†ï¼Œç„¶åè¿›è¡Œå›¾åƒåˆæˆï¼Œä¸ºè·¨æ¨¡æ€æ¨ç†æä¾›äº†å¤©ç„¶çš„ä¼˜åŠ¿ã€‚MILRåˆ©ç”¨MUGæ¡†æ¶çš„ä¸­é—´è¾“å‡ºä½œä¸ºç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œåœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼Œå›¾åƒå’Œæ–‡æœ¬çš„ç¦»æ•£tokenè¢«è¡¨ç¤ºä¸ºå‘é‡ã€‚ç„¶åï¼Œé€šè¿‡ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨è¿™ä¸ªæ½œåœ¨ç©ºé—´ä¸­æœç´¢æœ€ä¼˜çš„å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œä»¥æå‡å›¾åƒè´¨é‡ã€‚å›¾åƒè´¨é‡ç”±ä¸€ä¸ªå›¾åƒè´¨é‡è¯„ä»·å™¨è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä½œä¸ºç­–ç•¥æ¢¯åº¦çš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šMILRçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æµ‹è¯•æ—¶è”åˆæ¨ç†æœºåˆ¶ï¼Œå®ƒä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–å¾®è°ƒã€‚é€šè¿‡åœ¨ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼ŒMILRèƒ½å¤ŸåŒæ—¶è€ƒè™‘å›¾åƒå’Œæ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆè¦æ±‚çš„å›¾åƒã€‚æ­¤å¤–ï¼ŒMILRåˆ©ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¥ä¼˜åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å¦‚ä½•è¿›è¡Œæ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šMILRä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¥ä¼˜åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹é€šè¿‡é‡‡æ ·ä¸€ç³»åˆ—çš„å›¾åƒå’Œæ–‡æœ¬tokenï¼Œå¹¶æ ¹æ®å›¾åƒè´¨é‡è¯„ä»·å™¨çš„è¾“å‡ºè®¡ç®—å¥–åŠ±ä¿¡å·ã€‚ç„¶åï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»¥æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ã€‚å›¾åƒè´¨é‡è¯„ä»·å™¨å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œä¾‹å¦‚CLIPã€‚æ­¤å¤–ï¼ŒMILRè¿˜ä½¿ç”¨äº†æ¸©åº¦å‚æ•°æ¥æ§åˆ¶é‡‡æ ·è¿‡ç¨‹çš„æ¢ç´¢ç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MILRåœ¨GenEvalã€T2I-CompBenchå’ŒWISEç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨çŸ¥è¯†å¯†é›†å‹WISEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMILRçš„æ€»åˆ†è¾¾åˆ°äº†0.63ï¼Œæ¯”åŸºçº¿æé«˜äº†80%ã€‚è¿™è¡¨æ˜MILRåœ¨å¤„ç†éœ€è¦è·¨æ¨¡æ€çŸ¥è¯†æ¨ç†çš„ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MILRå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒç¼–è¾‘ã€å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„å›¾åƒï¼Œæé«˜å›¾åƒç”Ÿæˆç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æ­¤å¤–ï¼ŒMILRè¿˜å¯ä»¥åº”ç”¨äºæ•™è‚²ã€å¨±ä¹ç­‰é¢†åŸŸï¼Œä¾‹å¦‚ç”Ÿæˆä¸ªæ€§åŒ–çš„å­¦ä¹ ææ–™ã€åˆ›å»ºè™šæ‹Ÿç°å®åœºæ™¯ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.

