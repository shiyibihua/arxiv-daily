---
layout: default
title: Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs
---

# Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22646" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22646v2</a>
  <a href="https://arxiv.org/pdf/2509.22646.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22646v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22646v2', 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-01)

**å¤‡æ³¨**: Project Page: https://deeptracereward.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeeptraceRewardä»¥è§£å†³AIç”Ÿæˆè§†é¢‘çš„ä¼ªé€ æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦ä¼ªé€ æ£€æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†é¢‘ç”Ÿæˆ` `äººç±»æ„ŸçŸ¥` `å¥–åŠ±æ¨¡å‹` `æ•°æ®é›†æ„å»º` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ—¶ç©ºæ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ä¼ªé€ æ£€æµ‹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæœªèƒ½æœ‰æ•ˆè¯†åˆ«æ·±åº¦ä¼ªé€ ç—•è¿¹ã€‚
2. è®ºæ–‡æå‡ºDeeptraceRewardåŸºå‡†ï¼Œé€šè¿‡ç»†ç²’åº¦æ³¨é‡Šå¸®åŠ©æ¨¡å‹å­¦ä¹ äººç±»å¦‚ä½•è¯†åˆ«ä¼ªé€ è§†é¢‘ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æ7Bå¥–åŠ±æ¨¡å‹åœ¨ä¼ªé€ çº¿ç´¢è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°34.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»èƒ½å¦è¯†åˆ«AIç”Ÿæˆï¼ˆä¼ªé€ ï¼‰è§†é¢‘å¹¶æä¾›åˆç†è§£é‡Šï¼Ÿå°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å¿«é€Ÿå‘å±•ï¼Œä½†äººç±»èƒ½å¦æ£€æµ‹ç”Ÿæˆè§†é¢‘ä¸­çš„æ·±åº¦ä¼ªé€ ç—•è¿¹è¿™ä¸€å…³é”®ç»´åº¦å´è¢«å¿½è§†ã€‚æˆ‘ä»¬å¼•å…¥DeeptraceRewardï¼Œè¿™æ˜¯é¦–ä¸ªç»†ç²’åº¦ã€æ—¶ç©ºæ„ŸçŸ¥çš„åŸºå‡†ï¼Œæ³¨é‡Šäº†äººç±»æ„ŸçŸ¥çš„ä¼ªé€ ç—•è¿¹ä»¥ç”¨äºè§†é¢‘ç”Ÿæˆå¥–åŠ±ã€‚è¯¥æ•°æ®é›†åŒ…å«4300ä¸ªè¯¦ç»†æ³¨é‡Šï¼Œè¦†ç›–3300ä¸ªé«˜è´¨é‡ç”Ÿæˆè§†é¢‘ã€‚æ¯ä¸ªæ³¨é‡Šæä¾›è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæ ‡è®°åŒ…å«ä¼ªé€ ç—•è¿¹çš„è¾¹ç•Œæ¡†åŒºåŸŸï¼Œå¹¶ç²¾ç¡®æ ‡è®°èµ·æ­¢æ—¶é—´ã€‚æˆ‘ä»¬å°†è¿™äº›æ³¨é‡Šæ•´åˆä¸º9ä¸ªä¸»è¦ç±»åˆ«ï¼Œè®­ç»ƒå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ¨¡æ‹Ÿäººç±»åˆ¤æ–­å’Œå®šä½ã€‚åœ¨DeeptraceRewardä¸Šï¼Œæˆ‘ä»¬çš„7Bå¥–åŠ±æ¨¡å‹åœ¨ä¼ªé€ çº¿ç´¢è¯†åˆ«ã€å®šä½å’Œè§£é‡Šæ–¹é¢å¹³å‡è¶…è¶ŠGPT-5 34.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³äººç±»å¦‚ä½•è¯†åˆ«AIç”Ÿæˆè§†é¢‘ä¸­çš„ä¼ªé€ ç—•è¿¹è¿™ä¸€å…·ä½“é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘äººç±»çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´ä¼ªé€ æ£€æµ‹æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥DeeptraceRewardåŸºå‡†ï¼Œé€šè¿‡è¯¦ç»†çš„æ³¨é‡Šå’Œè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œå¸®åŠ©å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å­¦ä¹ äººç±»çš„åˆ¤æ–­æ ‡å‡†å’Œå®šä½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ³¨é‡Šæ•´åˆã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†åŒ…å«ä¼ªé€ ç—•è¿¹çš„è¯¦ç»†æ³¨é‡Šï¼Œæ¨¡å‹åˆ™é€šè¿‡è¿™äº›æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºDeeptraceRewardåŸºå‡†çš„æå‡ºï¼Œå®ƒæä¾›äº†æ—¶ç©ºæ„ŸçŸ¥çš„ä¼ªé€ ç—•è¿¹æ³¨é‡Šï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†7Bå‚æ•°çš„å¥–åŠ±æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨äº†å¤šæ¨¡æ€èåˆçš„ç­–ç•¥ï¼Œç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰ä¿¡æ¯ï¼Œä»¥æé«˜ä¼ªé€ çº¿ç´¢çš„è¯†åˆ«èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿé’ˆå¯¹ä¼ªé€ ç—•è¿¹çš„ç‰¹å¾è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æ7Bå¥–åŠ±æ¨¡å‹åœ¨ä¼ªé€ çº¿ç´¢è¯†åˆ«ã€å®šä½å’Œè§£é‡Šæ–¹é¢çš„å¹³å‡æ€§èƒ½è¶…è¶ŠGPT-5 34.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä¼ªé€ ä¸çœŸå®è§†é¢‘çš„äºŒåˆ†ç±»ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œè€Œç»†ç²’åº¦çš„ä¼ªé€ ç—•è¿¹æ£€æµ‹åˆ™è¡¨ç°å‡ºæ˜æ˜¾çš„éš¾åº¦æ¢¯åº¦ï¼Œåæ˜ äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€è§†é¢‘ç›‘æ§ç³»ç»Ÿä»¥åŠè™šå‡ä¿¡æ¯æ£€æµ‹ç­‰ã€‚é€šè¿‡æé«˜AIç”Ÿæˆè§†é¢‘çš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œæå‡ç”¨æˆ·å¯¹è§†é¢‘å†…å®¹çš„ä¿¡ä»»åº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.

