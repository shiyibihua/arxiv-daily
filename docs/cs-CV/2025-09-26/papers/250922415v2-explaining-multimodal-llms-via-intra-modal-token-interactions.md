---
layout: default
title: Explaining multimodal LLMs via intra-modal token interactions
---

# Explaining multimodal LLMs via intra-modal token interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22415" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22415v2</a>
  <a href="https://arxiv.org/pdf/2509.22415.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22415v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22415v2', 'Explaining multimodal LLMs via intra-modal token interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawei Liang, Ruoyu Chen, Xianghao Jiao, Siyuan Liang, Shiming Liu, Qunli Zhang, Zheng Hu, Xiaochun Cao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ¨¡æ€å†…tokenäº¤äº’å¢å¼ºå¤šæ¨¡æ€LLMçš„å¯è§£é‡Šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¯è§£é‡Šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ¨¡æ€å†…äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMå¯è§£é‡Šæ€§æ–¹æ³•å¿½ç•¥äº†æ¨¡æ€å†…éƒ¨tokençš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´è§†è§‰è§£é‡Šåˆ†æ•£ï¼Œæ–‡æœ¬è§£é‡Šå¼•å…¥è™šå‡æ¿€æ´»ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨æ¨¡æ€å†…äº¤äº’å¢å¼ºå¯è§£é‡Šæ€§ï¼Œè§†è§‰æ¨¡æ€é‡‡ç”¨å¤šå°ºåº¦è§£é‡Šèšåˆï¼Œæ–‡æœ¬æ¨¡æ€é‡‡ç”¨æ¿€æ´»æ’åºç›¸å…³æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªMLLMå’Œæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›æ›´å¿ å®å’Œç»†ç²’åº¦çš„æ¨¡å‹è¡Œä¸ºè§£é‡Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶å†…éƒ¨å†³ç­–æœºåˆ¶ä»æœªå¾—åˆ°å……åˆ†ç†è§£ã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦é›†ä¸­äºè·¨æ¨¡æ€å½’å› ï¼Œå³è¯†åˆ«æ¨¡å‹åœ¨è¾“å‡ºç”Ÿæˆè¿‡ç¨‹ä¸­å…³æ³¨çš„å›¾åƒåŒºåŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å¿½ç•¥äº†æ¨¡æ€å†…çš„ä¾èµ–å…³ç³»ã€‚åœ¨è§†è§‰æ¨¡æ€ä¸­ï¼Œç”±äºæ„Ÿå—é‡æœ‰é™ï¼Œå°†é‡è¦æ€§å½’å› äºå­¤ç«‹çš„å›¾åƒå—ä¼šå¿½ç•¥ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è§£é‡Šåˆ†æ•£ä¸”å˜ˆæ‚ã€‚åœ¨æ–‡æœ¬æ¨¡æ€ä¸­ï¼Œä¾èµ–äºå‰é¢çš„tokenä¼šå¼•å…¥è™šå‡çš„æ¿€æ´»ã€‚æœªèƒ½æœ‰æ•ˆç¼“è§£è¿™äº›å¹²æ‰°ä¼šæŸå®³å½’å› çš„ä¿çœŸåº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆ©ç”¨æ¨¡æ€å†…äº¤äº’æ¥å¢å¼ºå¯è§£é‡Šæ€§ã€‚å¯¹äºè§†è§‰åˆ†æ”¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦è§£é‡Šèšåˆ(MSEA)ï¼Œå®ƒèšåˆå¤šå°ºåº¦è¾“å…¥çš„å½’å› ï¼Œä»¥åŠ¨æ€è°ƒæ•´æ„Ÿå—é‡ï¼Œä»è€Œäº§ç”Ÿæ›´æ•´ä½“å’Œç©ºé—´è¿è´¯çš„è§†è§‰è§£é‡Šã€‚å¯¹äºæ–‡æœ¬åˆ†æ”¯ï¼Œæˆ‘ä»¬æå‡ºäº†æ¿€æ´»æ’åºç›¸å…³æ€§(ARC)ï¼Œå®ƒé€šè¿‡å¯¹é½ä¸Šä¸‹æ–‡tokençš„top-ké¢„æµ‹æ’åæ¥è¡¡é‡ä¸Šä¸‹æ–‡tokenä¸å½“å‰tokençš„ç›¸å…³æ€§ã€‚ARCåˆ©ç”¨è¿™ç§ç›¸å…³æ€§æ¥æŠ‘åˆ¶æ¥è‡ªä¸ç›¸å…³ä¸Šä¸‹æ–‡çš„è™šå‡æ¿€æ´»ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ä¸Šè¿è´¯çš„æ¿€æ´»ã€‚åœ¨æœ€å…ˆè¿›çš„MLLMå’ŒåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œä»è€Œäº§ç”Ÿæ›´å¿ å®å’Œç»†ç²’åº¦çš„æ¨¡å‹è¡Œä¸ºè§£é‡Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMå¯è§£é‡Šæ€§æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€çš„è§†è§‰åŒºåŸŸå®šä½ï¼Œå¿½ç•¥äº†æ¨¡æ€å†…éƒ¨tokenä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚åœ¨è§†è§‰æ¨¡æ€ä¸­ï¼Œç”±äºæ„Ÿå—é‡çš„é™åˆ¶ï¼Œå¯¹å­¤ç«‹å›¾åƒå—çš„å½’å› å¿½ç•¥äº†ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è§£é‡Šç»“æœåˆ†æ•£ä¸”å™ªå£°å¤§ã€‚åœ¨æ–‡æœ¬æ¨¡æ€ä¸­ï¼Œå¯¹å‰åºtokençš„ä¾èµ–ä¼šå¼•å…¥è™šå‡æ¿€æ´»ï¼Œå½±å“å½’å› çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æŒ–æ˜å’Œåˆ©ç”¨æ¨¡æ€å†…éƒ¨tokenä¹‹é—´çš„äº¤äº’å…³ç³»æ¥æå‡å¯è§£é‡Šæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºè§†è§‰æ¨¡æ€ï¼Œé€šè¿‡èšåˆå¤šå°ºåº¦ä¿¡æ¯æ¥åŠ¨æ€è°ƒæ•´æ„Ÿå—é‡ï¼Œä»è€Œè·å¾—æ›´å…¨é¢çš„ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¯¹äºæ–‡æœ¬æ¨¡æ€ï¼Œé€šè¿‡è¯„ä¼°ä¸Šä¸‹æ–‡tokenä¸å½“å‰tokençš„ç›¸å…³æ€§æ¥æŠ‘åˆ¶ä¸ç›¸å…³çš„æ¿€æ´»ï¼Œä¿ç•™è¯­ä¹‰è¿è´¯çš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰è§£é‡Šå¢å¼ºæ¨¡å—å’Œæ–‡æœ¬è§£é‡Šå¢å¼ºæ¨¡å—ã€‚è§†è§‰è§£é‡Šå¢å¼ºæ¨¡å—é‡‡ç”¨å¤šå°ºåº¦è§£é‡Šèšåˆ(MSEA)ï¼Œæ–‡æœ¬è§£é‡Šå¢å¼ºæ¨¡å—é‡‡ç”¨æ¿€æ´»æ’åºç›¸å…³æ€§(ARC)ã€‚MSEAé¦–å…ˆå¯¹è¾“å…¥å›¾åƒè¿›è¡Œå¤šå°ºåº¦å¤„ç†ï¼Œç„¶åå¯¹æ¯ä¸ªå°ºåº¦ä¸‹çš„å½’å› å›¾è¿›è¡Œèšåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„è§†è§‰è§£é‡Šã€‚ARCé¦–å…ˆè®¡ç®—ä¸Šä¸‹æ–‡tokenä¸å½“å‰tokençš„é¢„æµ‹æ’åç›¸å…³æ€§ï¼Œç„¶ååˆ©ç”¨è¯¥ç›¸å…³æ€§å¯¹ä¸Šä¸‹æ–‡tokençš„æ¿€æ´»è¿›è¡ŒåŠ æƒï¼Œä»è€ŒæŠ‘åˆ¶ä¸ç›¸å…³çš„æ¿€æ´»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åˆ©ç”¨æ¨¡æ€å†…tokenäº¤äº’æ¥å¢å¼ºMLLMå¯è§£é‡Šæ€§çš„æ€æƒ³ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ¨¡æ€å†…éƒ¨çš„ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿæ›´å‡†ç¡®ã€æ›´ç»†ç²’åº¦çš„è§£é‡Šã€‚MSEAé€šè¿‡å¤šå°ºåº¦èšåˆåŠ¨æ€è°ƒæ•´æ„Ÿå—é‡ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•æ„Ÿå—é‡å›ºå®šçš„å±€é™æ€§ã€‚ARCé€šè¿‡é¢„æµ‹æ’åç›¸å…³æ€§æ¥è¯„ä¼°tokenç›¸å…³æ€§ï¼Œé¿å…äº†ç›´æ¥ä¾èµ–å‰åºtokenå¸¦æ¥çš„è™šå‡æ¿€æ´»é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MSEAä¸­ï¼Œå¤šå°ºåº¦çš„é€‰æ‹©å’Œèšåˆæ–¹å¼æ˜¯å…³é”®ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨äº†å“ªäº›å°ºåº¦ï¼Œä»¥åŠå¦‚ä½•å¯¹ä¸åŒå°ºåº¦çš„å½’å› å›¾è¿›è¡ŒåŠ æƒèšåˆï¼Œè¿™äº›ç»†èŠ‚å†³å®šäº†æœ€ç»ˆçš„è§£é‡Šæ•ˆæœã€‚åœ¨ARCä¸­ï¼Œtop-kå€¼çš„é€‰æ‹©ä»¥åŠç›¸å…³æ€§åº¦é‡æ–¹å¼çš„é€‰æ‹©æ˜¯å…³é”®ã€‚ä¸åŒçš„kå€¼å’Œç›¸å…³æ€§åº¦é‡æ–¹å¼ä¼šå½±å“tokenç›¸å…³æ€§çš„è¯„ä¼°ç»“æœï¼Œä»è€Œå½±å“è™šå‡æ¿€æ´»çš„æŠ‘åˆ¶æ•ˆæœã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨äº†ä»€ä¹ˆkå€¼å’Œç›¸å…³æ€§åº¦é‡æ–¹å¼ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„MSEAå’ŒARCæ–¹æ³•åœ¨å¤šä¸ªMLLMå’Œæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿæ›´å¿ å®ã€æ›´ç»†ç²’åº¦çš„æ¨¡å‹è¡Œä¸ºè§£é‡Šï¼Œä¼˜äºç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦å’Œé€æ˜åº¦ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨æ”¸å…³é¢†åŸŸï¼Œæ¸…æ™°çš„è§£é‡Šèƒ½å¤Ÿå¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»ï¼Œå¹¶ä¿ƒè¿›æ¨¡å‹çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ¨¡å‹è°ƒè¯•å’Œä¼˜åŒ–ï¼Œå‘ç°æ¨¡å‹æ½œåœ¨çš„é—®é¢˜å’Œæ”¹è¿›æ–¹å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.

