---
layout: default
title: SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference
---

# SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21927" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.21927v1</a>
  <a href="https://arxiv.org/pdf/2509.21927.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21927v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21927v1', 'SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-26

**Â§áÊ≥®**: Accepted as a poster in NeurIPS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SingRef6DÔºöÂü∫‰∫éÂçïÂº†RGBÂèÇËÄÉÂõæÂÉèÁöÑÊñ∞Áâ©‰ΩìÂçïÁõÆ6D‰ΩçÂßø‰º∞ËÆ°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `6D‰ΩçÂßø‰º∞ËÆ°` `ÂçïÁõÆËßÜËßâ` `Ê∑±Â∫¶È¢ÑÊµã` `ÁâπÂæÅÂåπÈÖç` `Êú∫Âô®‰∫∫ÊäìÂèñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ6D‰ΩçÂßø‰º∞ËÆ°ÊñπÊ≥ï‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÔºåÂú®ÈÄèÊòéÊàñÈ´òÂèçÂ∞ÑÊùêË¥®‰∏äÂ§±ÊïàÔºõRGBÊñπÊ≥ïÂú®Âº±ÂÖâÂíåÊó†Á∫πÁêÜÂú∫ÊôØ‰∏≠ÂåπÈÖçÊÄßËÉΩËæÉÂ∑Æ„ÄÇ
2. SingRef6D‰ªÖ‰ΩøÁî®ÂçïÂº†RGBÂõæÂÉè‰Ωú‰∏∫ÂèÇËÄÉÔºåÈÄöËøáÊ∑±Â∫¶È¢ÑÊµãÂíåÊ∑±Â∫¶ÊÑüÁü•ÂåπÈÖçÔºåÊèêÂçáÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑ‰ΩçÂßø‰º∞ËÆ°È≤ÅÊ£íÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSingRef6DÂú®Ê∑±Â∫¶È¢ÑÊµãÂíå‰ΩçÂßø‰º∞ËÆ°ÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫SingRef6DÔºå‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÂçïÁõÆ6D‰ΩçÂßø‰º∞ËÆ°ÊµÅÁ®ãÔºå‰ªÖÈúÄÂçïÂº†RGBÂèÇËÄÉÂõæÂÉèÔºåÊó†ÈúÄÊ∑±Â∫¶‰º†ÊÑüÂô®„ÄÅÂ§öËßÜËßíÂõæÂÉèÊàñËÆ≠ÁªÉËßÜËßíÂêàÊàêÊ®°Âûã‰∏éÁ•ûÁªèÂú∫„ÄÇËøô‰ΩøÂæóSingRef6DÂú®ËµÑÊ∫êÂèóÈôê„ÄÅÊ∑±Â∫¶ÊàñÂØÜÈõÜÊ®°Êùø‰∏çÂèØÁî®ÁöÑÊÉÖÂÜµ‰∏ã‰æùÁÑ∂Á®≥ÂÅ•„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏§È°πÂÖ≥ÈîÆÂàõÊñ∞Ôºö‰∏ÄÊòØÂü∫‰∫étoken-scalerÁöÑÂæÆË∞ÉÊú∫Âà∂ÔºåÁªìÂêàÊñ∞ÁöÑ‰ºòÂåñÊçüÂ§±ÔºåÂ¢ûÂº∫Depth-Anything v2È¢ÑÊµãÁ≤æÁ°ÆÊ∑±Â∫¶ÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÈíàÂØπÂ§çÊùÇË°®Èù¢„ÄÇÂú®REAL275Êï∞ÊçÆÈõÜ‰∏äÔºåÊ∑±Â∫¶È¢ÑÊµãÁ≤æÂ∫¶($Œ¥_{1.05}$)Áõ∏ÊØîÂæÆË∞ÉÂêéÁöÑDepth-Anything v2ÊèêÂçá14.41%„ÄÇ‰∫åÊòØÂºïÂÖ•Ê∑±Â∫¶ÊÑüÁü•ÁöÑÂåπÈÖçËøáÁ®ãÔºåÊúâÊïàÊï¥ÂêàLoFTR‰∏≠ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÔºå‰ªéËÄåÂ§ÑÁêÜÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊùêË¥®ÂíåÂÖâÁÖßÊù°‰ª∂‰∏ãÁöÑÂåπÈÖç„ÄÇÂú®REAL275„ÄÅClearPoseÂíåToyota-LightÊï∞ÊçÆÈõÜ‰∏äÁöÑ‰ΩçÂßø‰º∞ËÆ°ËØÑ‰º∞Ë°®ÊòéÔºåËØ•ÊñπÊ≥ïË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊäÄÊúØÔºåÂπ≥ÂùáÂè¨ÂõûÁéáÊèêÂçá6.1%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ6D‰ΩçÂßø‰º∞ËÆ°ÊñπÊ≥ïÈù¢‰∏¥ÂÆûÈôÖÂ∫îÁî®ÈôêÂà∂„ÄÇ‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÁöÑÊñπÊ≥ïÂú®Â§ÑÁêÜÈÄèÊòé„ÄÅÈ´òÂèçÂ∞ÑÁ≠âÊùêË¥®Êó∂ÊÄßËÉΩ‰∏ãÈôç„ÄÇÁ∫ØRGBÊñπÊ≥ïÂú®Âº±ÂÖâÁÖß„ÄÅÊó†Á∫πÁêÜÂú∫ÊôØ‰∏ãÔºåÁî±‰∫éÁº∫‰πèÂá†‰Ωï‰ø°ÊÅØÔºåÂåπÈÖçÁ≤æÂ∫¶‰∏çÈ´ò„ÄÇËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫Ü6D‰ΩçÂßø‰º∞ËÆ°Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSingRef6DÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂçïÂº†RGBÂõæÂÉèÔºåÈÄöËøáÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóËé∑ÂæóÂú∫ÊôØÁöÑÊ∑±Â∫¶‰ø°ÊÅØÔºåÁÑ∂ÂêéÂà©Áî®Ê∑±Â∫¶‰ø°ÊÅØËæÖÂä©ÁâπÂæÅÂåπÈÖçÔºå‰ªéËÄåÊèêÂçá‰ΩçÂßø‰º∞ËÆ°ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπÊ∑±Â∫¶‰º†ÊÑüÂô®ÁöÑ‰æùËµñÔºå‰πüÊó†ÈúÄÂ§çÊùÇÁöÑËßÜËßíÂêàÊàêÊàñÁ•ûÁªèÂú∫Âª∫Ê®°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSingRef6D‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºö1) Ê∑±Â∫¶È¢ÑÊµãÈò∂ÊÆµÔºö‰ΩøÁî®ÊîπËøõÁöÑDepth-Anything v2Ê®°ÂûãÈ¢ÑÊµãÂú∫ÊôØÊ∑±Â∫¶Âõæ„ÄÇËØ•Ê®°ÂûãÈÄöËøátoken-scalerÂæÆË∞ÉÊú∫Âà∂ÂíåÊñ∞ÁöÑ‰ºòÂåñÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉÔºå‰ª•ÊèêÈ´òÊ∑±Â∫¶È¢ÑÊµãÁ≤æÂ∫¶„ÄÇ2) ‰ΩçÂßø‰º∞ËÆ°Èò∂ÊÆµÔºöÂà©Áî®È¢ÑÊµãÁöÑÊ∑±Â∫¶ÂõæÔºåÁªìÂêàLoFTRËøõË°åÊ∑±Â∫¶ÊÑüÁü•ÁöÑÁâπÂæÅÂåπÈÖç„ÄÇÂåπÈÖçÁªìÊûúÁî®‰∫é‰º∞ËÆ°Áâ©‰ΩìÁöÑ6D‰ΩçÂßø„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰∏§‰∏™ÊñπÈù¢Ôºö‰∏ÄÊòØÊèêÂá∫‰∫ÜÂü∫‰∫étoken-scalerÁöÑÊ∑±Â∫¶È¢ÑÊµãÂæÆË∞ÉÊú∫Âà∂ÔºåÊúâÊïàÊèêÂçá‰∫ÜDepth-Anything v2Âú®Â§çÊùÇÊùêË¥®‰∏äÁöÑÊ∑±Â∫¶È¢ÑÊµãÁ≤æÂ∫¶„ÄÇ‰∫åÊòØÂºïÂÖ•‰∫ÜÊ∑±Â∫¶ÊÑüÁü•ÁöÑÂåπÈÖçËøáÁ®ãÔºåÂ∞ÜÊ∑±Â∫¶‰ø°ÊÅØËûçÂÖ•Âà∞LoFTRÁöÑÁâπÂæÅÂåπÈÖç‰∏≠Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂåπÈÖçÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSingRef6DÊó†ÈúÄÊ∑±Â∫¶‰º†ÊÑüÂô®ÊàñÂ§öËßÜËßíÂõæÂÉèÔºåÊõ¥Âä†ËΩªÈáèÁ∫ßÂíåÂÆûÁî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê∑±Â∫¶È¢ÑÊµãÈò∂ÊÆµÔºåÈááÁî®‰∫Ütoken-scalerÂæÆË∞ÉÊú∫Âà∂ÔºåÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•„ÄÇ‰ºòÂåñÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüÊú™Áü•„ÄÇÂú®Ê∑±Â∫¶ÊÑüÁü•ÁöÑÂåπÈÖçËøáÁ®ã‰∏≠ÔºåÂ¶Ç‰ΩïÂ∞ÜÊ∑±Â∫¶‰ø°ÊÅØÊúâÊïàÂú∞ËûçÂÖ•Âà∞LoFTRÁöÑÁâπÂæÅÂåπÈÖç‰∏≠ÔºåÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•„ÄÇËøô‰∫õÁªÜËäÇÂØπÊúÄÁªàÁöÑÊÄßËÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SingRef6DÂú®REAL275Êï∞ÊçÆÈõÜ‰∏äÔºåÊ∑±Â∫¶È¢ÑÊµãÁ≤æÂ∫¶($Œ¥_{1.05}$)Áõ∏ÊØîÂæÆË∞ÉÂêéÁöÑDepth-Anything v2ÊèêÂçá14.41%„ÄÇÂú®REAL275„ÄÅClearPoseÂíåToyota-LightÊï∞ÊçÆÈõÜ‰∏äÁöÑ‰ΩçÂßø‰º∞ËÆ°ËØÑ‰º∞Ë°®ÊòéÔºåËØ•ÊñπÊ≥ïË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊäÄÊúØÔºåÂπ≥ÂùáÂè¨ÂõûÁéáÊèêÂçá6.1%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéSingRef6DÂú®Ê∑±Â∫¶È¢ÑÊµãÂíå‰ΩçÂßø‰º∞ËÆ°ÊñπÈù¢ÂùáÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SingRef6DÈÄÇÁî®‰∫éÊú∫Âô®‰∫∫ÊäìÂèñ„ÄÅÂ¢ûÂº∫Áé∞ÂÆû„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü„ÄÇÂú®Ëøô‰∫õÂú∫ÊôØ‰∏≠ÔºåËé∑ÂèñÁ≤æÁ°ÆÁöÑÁâ©‰Ωì‰ΩçÂßøËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁî±‰∫éSingRef6D‰ªÖÈúÄÂçïÂº†RGBÂõæÂÉèÔºåÂõ†Ê≠§ÂèØ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÈÉ®ÁΩ≤Ôºå‰æãÂ¶ÇÁßªÂä®Êú∫Âô®‰∫∫ÊàñÂµåÂÖ•ÂºèÁ≥ªÁªü„ÄÇËØ•Á†îÁ©∂ÁöÑÊú™Êù•ÂΩ±ÂìçÂú®‰∫éÊé®Âä®6D‰ΩçÂßø‰º∞ËÆ°Âú®Êõ¥ÂπøÊ≥õÁöÑÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent 6D pose estimation methods demonstrate notable performance but still face some practical limitations. For instance, many of them rely heavily on sensor depth, which may fail with challenging surface conditions, such as transparent or highly reflective materials. In the meantime, RGB-based solutions provide less robust matching performance in low-light and texture-less scenes due to the lack of geometry information. Motivated by these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB image as a reference, eliminating the need for costly depth sensors, multi-view image acquisition, or training view synthesis models and neural fields. This enables SingRef6D to remain robust and capable even under resource-limited settings where depth or dense templates are unavailable. Our framework incorporates two key innovations. First, we propose a token-scaler-based fine-tuning mechanism with a novel optimization loss on top of Depth-Anything v2 to enhance its ability to predict accurate depth, even for challenging surfaces. Our results show a 14.41% improvement (in $Œ¥_{1.05}$) on REAL275 depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second, benefiting from depth availability, we introduce a depth-aware matching process that effectively integrates spatial relationships within LoFTR, enabling our system to handle matching for challenging materials and lighting conditions. Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light datasets show that our approach surpasses state-of-the-art methods, achieving a 6.1% improvement in average recall.

