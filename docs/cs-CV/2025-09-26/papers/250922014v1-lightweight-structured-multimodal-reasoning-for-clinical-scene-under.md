---
layout: default
title: Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics
---

# Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22014" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22014v1</a>
  <a href="https://arxiv.org/pdf/2509.22014.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22014v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22014v1', 'Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Saurav Jha, Stefan K. Ehrlich

**åˆ†ç±»**: cs.CV, cs.AI, cs.HC, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 11 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§ç»“æ„åŒ–å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œç”¨äºæœºå™¨äººä¸´åºŠåœºæ™¯ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `ä¸´åºŠåœºæ™¯ç†è§£` `åŒ»ç–—æœºå™¨äºº` `è§†è§‰-è¯­è¨€æ¨¡å‹` `åœºæ™¯å›¾` `æ€ç»´é“¾æ¨ç†` `è½»é‡çº§æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´æ¨ç†ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œç»“æ„åŒ–è¾“å‡ºæ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥æ»¡è¶³åŒ»ç–—æœºå™¨äººä¸´åºŠåœºæ™¯ç†è§£çš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§è½»é‡çº§å¤šæ¨¡æ€æ¡†æ¶ï¼Œç»“åˆQwen2.5-VL-3B-Instructæ¨¡å‹å’ŒSmolAgentç¼–æ’å±‚ï¼Œå®ç°æ€ç»´é“¾æ¨ç†å’ŒåŠ¨æ€å·¥å…·è°ƒç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰VLMï¼ŒéªŒè¯äº†å…¶åœ¨åŒ»ç–—æœºå™¨äººé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒ»ç–—æœºå™¨äººéœ€è¦åœ¨åŠ¨æ€ä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œé²æ£’çš„å¤šæ¨¡æ€æ„ŸçŸ¥å’Œæ¨ç†ä»¥ç¡®ä¿å®‰å…¨ã€‚ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)è™½ç„¶å±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨èƒ½åŠ›ï¼Œä½†åœ¨æ—¶é—´æ¨ç†ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œæœºå™¨äººè§„åˆ’æ‰€éœ€çš„ç»“æ„åŒ–è¾“å‡ºæ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ã€åŸºäºä»£ç†çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºåŸºäºè§†é¢‘çš„åœºæ™¯ç†è§£ã€‚è¯¥æ¡†æ¶ç»“åˆäº†Qwen2.5-VL-3B-Instructæ¨¡å‹ä¸åŸºäºSmolAgentçš„ç¼–æ’å±‚ï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†ã€è¯­éŸ³-è§†è§‰èåˆå’ŒåŠ¨æ€å·¥å…·è°ƒç”¨ã€‚è¯¥æ¡†æ¶ç”Ÿæˆç»“æ„åŒ–çš„åœºæ™¯å›¾ï¼Œå¹¶åˆ©ç”¨æ··åˆæ£€ç´¢æ¨¡å—è¿›è¡Œå¯è§£é‡Šå’Œè‡ªé€‚åº”çš„æ¨ç†ã€‚åœ¨Video-MMEåŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªè‡ªå®šä¹‰ä¸´åºŠæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„VLMsç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§å’Œæ”¹è¿›çš„é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨æœºå™¨äººè¾…åŠ©æ‰‹æœ¯ã€æ‚£è€…ç›‘æµ‹å’Œå†³ç­–æ”¯æŒæ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—æœºå™¨äººä¸´åºŠåœºæ™¯ç†è§£ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šä¸€æ˜¯æ—¶é—´æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥ç†è§£è§†é¢‘ä¸­çš„åŠ¨æ€å˜åŒ–ï¼›äºŒæ˜¯ä¸ç¡®å®šæ€§ä¼°è®¡èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•å¯é åœ°å¤„ç†ä¸´åºŠç¯å¢ƒä¸­çš„å™ªå£°å’Œå¼‚å¸¸ï¼›ä¸‰æ˜¯ç¼ºä¹ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„èƒ½åŠ›ï¼Œéš¾ä»¥ç›´æ¥ç”¨äºæœºå™¨äººè§„åˆ’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªè½»é‡çº§çš„ã€åŸºäºä»£ç†çš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼ŒåŒæ—¶å…‹æœå…¶åœ¨æ—¶é—´æ¨ç†ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œç»“æ„åŒ–è¾“å‡ºæ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å¼•å…¥SmolAgentç¼–æ’å±‚ï¼Œå®ç°æ€ç»´é“¾æ¨ç†ã€è¯­éŸ³-è§†è§‰èåˆå’ŒåŠ¨æ€å·¥å…·è°ƒç”¨ï¼Œä»è€Œæé«˜åœºæ™¯ç†è§£çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) Qwen2.5-VL-3B-Instructæ¨¡å‹ï¼šä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼Œè´Ÿè´£æå–å›¾åƒå’Œè¯­éŸ³ç‰¹å¾ï¼Œå¹¶è¿›è¡Œåˆæ­¥çš„æ¨ç†ï¼›2) SmolAgentç¼–æ’å±‚ï¼šè´Ÿè´£åè°ƒå„ä¸ªæ¨¡å—ä¹‹é—´çš„äº¤äº’ï¼Œå®ç°æ€ç»´é“¾æ¨ç†å’ŒåŠ¨æ€å·¥å…·è°ƒç”¨ï¼›3) æ··åˆæ£€ç´¢æ¨¡å—ï¼šç”¨äºæ£€ç´¢ç›¸å…³çš„çŸ¥è¯†å’Œä¿¡æ¯ï¼Œæ”¯æŒå¯è§£é‡Šå’Œè‡ªé€‚åº”çš„æ¨ç†ï¼›4) åœºæ™¯å›¾ç”Ÿæˆæ¨¡å—ï¼šç”¨äºç”Ÿæˆç»“æ„åŒ–çš„åœºæ™¯å›¾ï¼Œä¸ºæœºå™¨äººè§„åˆ’æä¾›åŸºç¡€ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŸºäºä»£ç†çš„ç¼–æ’å±‚ç›¸ç»“åˆï¼Œå®ç°äº†ä¸€ç§è½»é‡çº§çš„ã€ç»“æ„åŒ–çš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ã€‚è¿™ç§æ¡†æ¶ä¸ä»…èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œè¿˜èƒ½å¤Ÿå…‹æœå…¶åœ¨æ—¶é—´æ¨ç†ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œç»“æ„åŒ–è¾“å‡ºæ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæ··åˆæ£€ç´¢æ¨¡å—å’Œåœºæ™¯å›¾ç”Ÿæˆæ¨¡å—ä¹Ÿä¸ºå¯è§£é‡Šæ€§å’Œæœºå™¨äººè§„åˆ’æä¾›äº†æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚æ–¹é¢ï¼ŒSmolAgentç¼–æ’å±‚é‡‡ç”¨äº†æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œå¯ä»¥çµæ´»åœ°æ·»åŠ å’Œåˆ é™¤å·¥å…·ã€‚æ··åˆæ£€ç´¢æ¨¡å—é‡‡ç”¨äº†åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿåœ°æ‰¾åˆ°ç›¸å…³çš„çŸ¥è¯†å’Œä¿¡æ¯ã€‚åœºæ™¯å›¾ç”Ÿæˆæ¨¡å—é‡‡ç”¨äº†åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆç»“æ„åŒ–çš„åœºæ™¯å›¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Video-MMEåŸºå‡†æµ‹è¯•å’Œè‡ªå®šä¹‰ä¸´åºŠæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰VLMã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨è‡ªå®šä¹‰ä¸´åºŠæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶çš„å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„VLMæé«˜äº†çº¦5%-10%ï¼Œå¹¶ä¸”åœ¨å¤„ç†å™ªå£°å’Œå¼‚å¸¸æƒ…å†µæ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯ï¼Œé€šè¿‡ç†è§£æ‰‹æœ¯è§†é¢‘ï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ‰‹æœ¯æ“ä½œï¼›ä¹Ÿå¯ç”¨äºæ‚£è€…ç›‘æµ‹ï¼Œå®æ—¶åˆ†ææ‚£è€…çŠ¶æ€ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸æƒ…å†µï¼›è¿˜å¯ç”¨äºä¸´åºŠå†³ç­–æ”¯æŒï¼Œä¸ºåŒ»ç”Ÿæä¾›è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚è¯¥ç ”ç©¶æœ‰æœ›æé«˜åŒ»ç–—æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæ”¹å–„åŒ»ç–—æœåŠ¡è´¨é‡ï¼Œå¹¶é™ä½åŒ»ç–—æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.

