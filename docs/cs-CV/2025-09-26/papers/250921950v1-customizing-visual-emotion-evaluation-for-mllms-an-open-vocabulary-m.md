---
layout: default
title: Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach
---

# Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21950" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21950v1</a>
  <a href="https://arxiv.org/pdf/2509.21950.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21950v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21950v1', 'Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, Yu Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wdqqdw/MVEI)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¼€æ”¾è¯æ±‡ã€å¤šæ–¹é¢ã€å¯æ‰©å±•çš„è§†è§‰æƒ…æ„Ÿè¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æƒ…æ„Ÿè¯„ä¼°` `æƒ…æ„Ÿé™ˆè¿°åˆ¤æ–­` `å¼€æ”¾è¯æ±‡` `è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æƒ…æ„Ÿè¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼ŒåŒ…æ‹¬å¿½ç•¥åˆç†å“åº”ã€æƒ…æ„Ÿåˆ†ç±»ä½“ç³»æœ‰é™ã€å¿½è§†ä¸Šä¸‹æ–‡å› ç´ å’Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚
2. æå‡ºæƒ…æ„Ÿé™ˆè¿°åˆ¤æ–­ä»»åŠ¡å’Œè‡ªåŠ¨åŒ–æµç¨‹ï¼Œä»¥å¼€æ”¾è¯æ±‡ã€å¤šæ–¹é¢ã€å¯æ‰©å±•çš„æ–¹å¼æ„å»ºæƒ…æ„Ÿè¯„ä¼°æ•°æ®é›†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMLLMåœ¨æƒ…æ„Ÿè§£é‡Šå’Œä¸Šä¸‹æ–‡æƒ…æ„Ÿåˆ¤æ–­æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨ç†è§£æ„ŸçŸ¥ä¸»è§‚æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸äººç±»å­˜åœ¨å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶èƒ½åŠ›ä¸æ–­è¶…å‡ºé¢„æœŸã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»å›¾åƒä¸­æ„ŸçŸ¥æƒ…æ„Ÿçš„èƒ½åŠ›ä»ç„¶å­˜åœ¨äº‰è®®ï¼Œåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„ç ”ç©¶ç»“æœå„ä¸ç›¸åŒã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§ä¸ä¸€è‡´éƒ¨åˆ†æºäºç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å¿½ç•¥äº†åˆç†çš„å“åº”ã€æƒ…æ„Ÿåˆ†ç±»ä½“ç³»æœ‰é™ã€å¿½è§†äº†ä¸Šä¸‹æ–‡å› ç´ ä»¥åŠæ ‡æ³¨å·¥ä½œé‡å¤§ã€‚ä¸ºäº†ä¿ƒè¿›MLLMçš„å®šåˆ¶åŒ–è§†è§‰æƒ…æ„Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æƒ…æ„Ÿé™ˆè¿°åˆ¤æ–­ä»»åŠ¡ï¼Œä»¥å…‹æœè¿™äº›é™åˆ¶ã€‚ä½œä¸ºå¯¹è¯¥ä»»åŠ¡çš„è¡¥å……ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œä»¥æœ€å°çš„äººå·¥æˆæœ¬é«˜æ•ˆåœ°æ„å»ºä»¥æƒ…æ„Ÿä¸ºä¸­å¿ƒçš„é™ˆè¿°ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ä¸»æµMLLMï¼Œæˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†å®ƒä»¬åœ¨æƒ…æ„Ÿè§£é‡Šå’ŒåŸºäºä¸Šä¸‹æ–‡çš„æƒ…æ„Ÿåˆ¤æ–­æ–¹é¢çš„æ›´å¼ºæ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†åœ¨ç†è§£æ„ŸçŸ¥ä¸»è§‚æ€§æ–¹é¢çš„ç›¸å¯¹å±€é™æ€§ã€‚ä¸äººç±»ç›¸æ¯”ï¼Œå³ä½¿æ˜¯åƒGPT4oè¿™æ ·çš„é¡¶çº§MLLMä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œçªå‡ºäº†æœªæ¥æ”¹è¿›çš„å…³é”®é¢†åŸŸã€‚é€šè¿‡å¼€å‘ä¸€ä¸ªåŸºç¡€çš„è¯„ä¼°æ¡†æ¶å¹¶è¿›è¡Œå…¨é¢çš„MLLMè¯„ä¼°ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œæœ‰åŠ©äºæé«˜MLLMçš„æƒ…æ„Ÿæ™ºèƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æƒ…æ„Ÿç†è§£æ–¹é¢è¯„ä¼°ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼š1) å¿½ç•¥äº†æ¨¡å‹å¯èƒ½äº§ç”Ÿçš„åˆç†æƒ…æ„Ÿååº”ï¼›2) ä½¿ç”¨çš„æƒ…æ„Ÿåˆ†ç±»ä½“ç³»è¿‡äºæœ‰é™ï¼Œæ— æ³•æ•æ‰æƒ…æ„Ÿçš„ç»†å¾®å·®åˆ«ï¼›3) å¿½è§†äº†å›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹æƒ…æ„Ÿç†è§£çš„å½±å“ï¼›4) éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ã€‚è¿™äº›é—®é¢˜å¯¼è‡´ç°æœ‰è¯„ä¼°ç»“æœæ— æ³•å‡†ç¡®åæ˜ MLLMsçš„çœŸå®æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ä¸€ç§æ–°çš„è¯„ä¼°ä»»åŠ¡â€”â€”æƒ…æ„Ÿé™ˆè¿°åˆ¤æ–­ï¼ˆEmotion Statement Judgmentï¼‰ï¼Œå¹¶ç»“åˆè‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œæ¥å…‹æœç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•å…è®¸ä½¿ç”¨å¼€æ”¾è¯æ±‡æ¥æè¿°æƒ…æ„Ÿï¼Œè€ƒè™‘äº†æƒ…æ„Ÿçš„å¤šæ–¹é¢æ€§ï¼Œå¹¶èƒ½é«˜æ•ˆåœ°ç”Ÿæˆå¤§è§„æ¨¡çš„è¯„ä¼°æ•°æ®é›†ã€‚é€šè¿‡è®©MLLMsåˆ¤æ–­ç»™å®šçš„å›¾åƒå’Œæƒ…æ„Ÿé™ˆè¿°æ˜¯å¦åŒ¹é…ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°å…¶è§†è§‰æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) æƒ…æ„Ÿé™ˆè¿°åˆ¤æ–­ä»»åŠ¡çš„è®¾è®¡ï¼šç»™å®šä¸€å¼ å›¾åƒå’Œä¸€ä¸ªæƒ…æ„Ÿé™ˆè¿°ï¼ŒMLLMéœ€è¦åˆ¤æ–­è¯¥é™ˆè¿°æ˜¯å¦å‡†ç¡®æè¿°äº†å›¾åƒæ‰€è¡¨è¾¾çš„æƒ…æ„Ÿã€‚2) è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆæµç¨‹ï¼šè¯¥æµç¨‹åˆ©ç”¨ç®—æ³•è‡ªåŠ¨ç”Ÿæˆå¤§é‡ä»¥æƒ…æ„Ÿä¸ºä¸­å¿ƒçš„é™ˆè¿°ï¼Œå‡å°‘äººå·¥æ ‡æ³¨çš„éœ€æ±‚ã€‚å…·ä½“æµç¨‹ç»†èŠ‚æœªçŸ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰æƒ…æ„Ÿè¯„ä¼°ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ›´å…¨é¢åœ°è€ƒè™‘äº†æƒ…æ„Ÿçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼›2) è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œå¤§å¤§é™ä½äº†è¯„ä¼°æ•°æ®é›†çš„æ„å»ºæˆæœ¬ï¼Œä½¿å…¶æ›´å…·å¯æ‰©å±•æ€§ï¼›3) é‡‡ç”¨å¼€æ”¾è¯æ±‡è¿›è¡Œæƒ…æ„Ÿæè¿°ï¼Œé¿å…äº†ä¼ ç»Ÿæƒ…æ„Ÿåˆ†ç±»ä½“ç³»çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³äºæƒ…æ„Ÿé™ˆè¿°åˆ¤æ–­ä»»åŠ¡çš„å…³é”®è®¾è®¡åœ¨äºå…è®¸å¼€æ”¾è¯æ±‡çš„æƒ…æ„Ÿæè¿°ï¼Œè¿™ä½¿å¾—è¯„ä¼°èƒ½å¤Ÿæ•æ‰åˆ°æ›´ç»†å¾®å’Œå¤šæ ·çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆæµç¨‹çš„å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶è¡¨æ˜ï¼Œä¸»æµMLLMåœ¨æƒ…æ„Ÿè§£é‡Šå’ŒåŸºäºä¸Šä¸‹æ–‡çš„æƒ…æ„Ÿåˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºè¾ƒå¼ºçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨ç†è§£æ„ŸçŸ¥ä¸»è§‚æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸äººç±»çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå³ä½¿æ˜¯GPT4oè¿™æ ·çš„é¡¶çº§MLLMä¹Ÿæœªèƒ½è¾¾åˆ°äººç±»æ°´å¹³ã€‚è¿™çªå‡ºäº†æœªæ¥ç ”ç©¶éœ€è¦é‡ç‚¹å…³æ³¨çš„æ–¹å‘ï¼Œå³æå‡MLLMå¯¹æƒ…æ„Ÿä¸»è§‚æ€§çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿè®¡ç®—ã€äººæœºäº¤äº’ã€æ™ºèƒ½å®¢æœã€æƒ…æ„Ÿåˆ†æç­‰é¢†åŸŸçš„æ€§èƒ½ã€‚æ›´å‡†ç¡®çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›æœ‰åŠ©äºå¼€å‘æ›´å…·åŒç†å¿ƒå’Œäººæ€§åŒ–çš„AIç³»ç»Ÿï¼Œä¾‹å¦‚ï¼Œåœ¨å¿ƒç†å¥åº·å’¨è¯¢ã€æƒ…æ„Ÿé™ªä¼´æœºå™¨äººç­‰åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚æ­¤å¤–ï¼Œè¯¥è¯„ä¼°æ¡†æ¶å¯ä»¥ä½œä¸ºMLLMæƒ…æ„Ÿæ™ºèƒ½å‘å±•çš„é‡è¦åŸºå‡†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional performance across diverse tasks, continually surpassing previous expectations regarding their capabilities. Nevertheless, their proficiency in perceiving emotions from images remains debated, with studies yielding divergent results in zero-shot scenarios. We argue that this inconsistency stems partly from constraints in existing evaluation methods, including the oversight of plausible responses, limited emotional taxonomies, neglect of contextual factors, and labor-intensive annotations. To facilitate customized visual emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task that overcomes these constraints. Complementing this task, we devise an automated pipeline that efficiently constructs emotion-centric statements with minimal human effort. Through systematically evaluating prevailing MLLMs, our study showcases their stronger performance in emotion interpretation and context-based emotion judgment, while revealing relative limitations in comprehending perception subjectivity. When compared to humans, even top-performing MLLMs like GPT4o demonstrate remarkable performance gaps, underscoring key areas for future improvement. By developing a fundamental evaluation framework and conducting a comprehensive MLLM assessment, we hope this work contributes to advancing emotional intelligence in MLLMs. Project page: https://github.com/wdqqdw/MVEI.

