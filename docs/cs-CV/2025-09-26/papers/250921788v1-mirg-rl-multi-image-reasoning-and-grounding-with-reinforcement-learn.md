---
layout: default
title: MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning
---

# MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21788" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21788v1</a>
  <a href="https://arxiv.org/pdf/2509.21788.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21788v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21788v1', 'MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lihao Zheng, Jiawei Chen, Xintian Shen, Hao Ma, Tao Wei

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ZEUS2035/MIRG-RL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIRG-RLæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤šå›¾æ¨ç†å’Œå®šä½èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šå›¾æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å›¾åƒå®šä½` `è·¨å›¾åƒå…³ç³»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç¼ºä¹æœ‰æ•ˆçš„è·¨å›¾åƒæ¨ç†èƒ½åŠ›ï¼Œä¸”åœ¨è·¨å›¾åƒå‚è€ƒå¥–åŠ±å»ºæ¨¡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. MIRG-RLæ¡†æ¶é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒå’Œå›¾åƒæ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œé€æ­¥æå‡æ¨¡å‹çš„å¤šå›¾æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMIRG-RLåœ¨å¤šå›¾å®šä½ä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨è·¨å›¾åƒæ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶â€”â€”åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šå›¾æ¨ç†å’Œå®šä½ï¼ˆMIRG-RLï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šå›¾æ¨ç†å’Œå®šä½ä¸­é¢ä¸´çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¼ºä¹è·¨å›¾åƒæ¨ç†èƒ½åŠ›å’Œè·¨å›¾åƒå‚è€ƒå¥–åŠ±å»ºæ¨¡ä¸è¶³ã€‚MIRG-RLé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†å¸¦æ ‡æ³¨è½¨è¿¹çš„ç›‘ç£å¾®è°ƒå’Œå›¾åƒæ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œé€æ­¥å‘å±•å¤šå›¾æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ç§æ„å»ºè½¨è¿¹æ•°æ®çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é›†æˆäº†å¯¹è±¡çº§åˆ«å’Œå›¾åƒçº§åˆ«çš„æ ‡æ³¨ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨è¯¥æ–¹æ³•ç”Ÿæˆäº†ä¸€ä¸ªè½»é‡çº§çš„æ¨ç†å¢å¼ºæ•°æ®é›†ã€‚ä¸ºäº†æœ‰æ•ˆè§£å†³è·¨å›¾åƒæ­§ä¹‰ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§å…·æœ‰å¯¹è±¡å’Œå›¾åƒåŒé‡å¥–åŠ±å‡½æ•°çš„å›¾åƒæ„ŸçŸ¥RLç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIRG-RLåœ¨å¤šå›¾å®šä½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œåœ¨è·¨å›¾åƒæ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†64.82%ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å¥½çš„æ–¹æ³•1%ã€‚ä»£ç å’Œæ•°æ®é›†å·²å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†å¤šå›¾æ¨ç†å’Œå®šä½ä»»åŠ¡æ—¶ï¼Œé¢ä¸´ç€ä¸¤ä¸ªä¸»è¦çš„æŒ‘æˆ˜ã€‚ä¸€æ˜¯ç¼ºä¹æœ‰æ•ˆçš„è·¨å›¾åƒæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥ç†è§£å›¾åƒä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚äºŒæ˜¯è·¨å›¾åƒå‚è€ƒå¥–åŠ±å»ºæ¨¡ä¸è¶³ï¼Œæ— æ³•å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤šå›¾ç¯å¢ƒä¸‹çš„æ¨ç†å’Œå®šä½æ€§èƒ½ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†LVLMsåœ¨éœ€è¦ç»¼åˆå¤šå¼ å›¾åƒä¿¡æ¯æ‰èƒ½åšå‡ºå†³ç­–çš„åº”ç”¨åœºæ™¯ä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMIRG-RLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æå‡æ¨¡å‹çš„å¤šå›¾æ¨ç†å’Œå®šä½èƒ½åŠ›ã€‚é€šè¿‡å°†å¤šå›¾æ¨ç†å’Œå®šä½ä»»åŠ¡å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ å¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¤šå¼ å›¾åƒçš„ä¿¡æ¯è¿›è¡Œæ¨ç†å’Œå®šä½ã€‚åŒæ—¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒï¼Œåˆ©ç”¨æ ‡æ³¨æ•°æ®é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡åˆæ­¥çš„æ¨ç†èƒ½åŠ›ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMIRG-RLæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„è®­ç»ƒé˜¶æ®µï¼šç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚åœ¨ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨å¸¦æœ‰æ ‡æ³¨è½¨è¿¹çš„æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶å­¦ä¹ å¦‚ä½•æ ¹æ®å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¨ç†å’Œå®šä½ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–é˜¶æ®µï¼Œä½¿ç”¨å›¾åƒæ„ŸçŸ¥çš„RLç­–ç•¥ï¼Œæ ¹æ®ç¯å¢ƒåé¦ˆçš„å¥–åŠ±ä¿¡å·ï¼Œä¸æ–­è°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šå›¾æ¨ç†å’Œå®šä½ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬ä¸€ä¸ªè½¨è¿¹æ•°æ®æ„å»ºæ¨¡å—ï¼Œç”¨äºç”ŸæˆåŒ…å«å¯¹è±¡çº§åˆ«å’Œå›¾åƒçº§åˆ«æ ‡æ³¨ä¿¡æ¯çš„æ¨ç†å¢å¼ºæ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šMIRG-RLçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼šä¸€æ˜¯æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†å¤šå›¾æ¨ç†å’Œå®šä½ä»»åŠ¡å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚äºŒæ˜¯è®¾è®¡äº†ä¸€ç§å›¾åƒæ„ŸçŸ¥çš„RLç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è·¨å›¾åƒæ­§ä¹‰ã€‚ä¸‰æ˜¯æå‡ºäº†ä¸€ç§æ„å»ºè½¨è¿¹æ•°æ®çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é›†æˆäº†å¯¹è±¡çº§åˆ«å’Œå›¾åƒçº§åˆ«çš„æ ‡æ³¨ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆäº†ä¸€ä¸ªè½»é‡çº§çš„æ¨ç†å¢å¼ºæ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–é˜¶æ®µï¼ŒMIRG-RLé‡‡ç”¨äº†åŒé‡å¥–åŠ±å‡½æ•°ï¼Œåˆ†åˆ«é’ˆå¯¹å¯¹è±¡å’Œå›¾åƒã€‚å¯¹è±¡çº§åˆ«çš„å¥–åŠ±ç”¨äºé¼“åŠ±æ¨¡å‹å‡†ç¡®åœ°å®šä½ç›®æ ‡å¯¹è±¡ï¼Œå›¾åƒçº§åˆ«çš„å¥–åŠ±ç”¨äºé¼“åŠ±æ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨å¤šå¼ å›¾åƒçš„ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼ŒMIRG-RLè¿˜è®¾è®¡äº†ä¸€ç§å›¾åƒæ„ŸçŸ¥çš„RLç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæ ¹æ®å½“å‰å›¾åƒçš„çŠ¶æ€ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æœªåœ¨æ‘˜è¦ä¸­ä½“ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MIRG-RLåœ¨å¤šå›¾å®šä½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œåœ¨è·¨å›¾åƒæ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†64.82%ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å¥½çš„æ–¹æ³•1%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒMIRG-RLæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„å¤šå›¾æ¨ç†å’Œå®šä½èƒ½åŠ›ï¼Œå¹¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MIRG-RLåœ¨éœ€è¦ç»¼åˆå¤šå¼ å›¾åƒä¿¡æ¯è¿›è¡Œæ¨ç†å’Œå®šä½çš„åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å®‰é˜²ï¼ˆé€šè¿‡ç›‘æ§æ‘„åƒå¤´è¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼‰ã€è‡ªåŠ¨é©¾é©¶ï¼ˆé€šè¿‡å¤šä¸ªæ‘„åƒå¤´æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒï¼‰ã€åŒ»å­¦å½±åƒåˆ†æï¼ˆç»“åˆå¤šå¼ åŒ»å­¦å›¾åƒè¿›è¡Œç–¾ç—…è¯Šæ–­ï¼‰ç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-image reasoning and grounding require understanding complex cross-image relationships at both object levels and image levels. Current Large Visual Language Models (LVLMs) face two critical challenges: the lack of cross-image reasoning capabilities and insufficient cross-image reference reward modeling. To address these issues, we propose a unified framework - Multi-Image Reasoning and Grounding with Reinforcement Learning (MIRG-RL). Specifically, our two-stage training paradigm combines supervised fine-tuning with annotated trajectories and image-aware reinforcement learning optimization, progressively developing multi-image reasoning capabilities. Furthermore, we innovatively propose a method for constructing the trajectory data, which integrates object-level and image-level annotation information, and use this method to generate a lightweight reasoning-enhanced dataset. To effectively resolve cross-image ambiguities, we design an image-aware RL policy with dual reward functions for objects and images. Experiments demonstrate that MIRG-RL achieves state-of-the-art (SOTA) performance in multi-image grounding benchmarks, attaining 64.82% on cross-image reasoning tasks - exceeding the previous best method by 1%. The code and dataset have been released at https://github.com/ZEUS2035/MIRG-RL.

