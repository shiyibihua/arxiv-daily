---
layout: default
title: Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm
---

# Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21980" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21980v1</a>
  <a href="https://arxiv.org/pdf/2509.21980.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21980v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21980v1', 'Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyu Wang, Baiyu Chen, Kun Yan, Hongjing Piao, Hao Xue, Flora D. Salim, Yuanchun Shi, Yuntao Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GLARIFYï¼šåˆ©ç”¨æ—¶ç©ºæ³¨è§†ä¿¡æ¯è§£å†³è§†è§‰åŠ©æ‰‹äº¤äº’ä¸­çš„æ­§ä¹‰æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰åŠ©æ‰‹` `æ³¨è§†è¿½è¸ª` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ­§ä¹‰æ¶ˆè§£` `æ•°æ®åˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†è§‰åŠ©æ‰‹äº¤äº’ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†ç”¨æˆ·å£è¯­æé—®çš„æ¨¡ç³Šæ€§ä»¥åŠæ³¨è§†æ•°æ®çš„å™ªå£°é—®é¢˜ã€‚
2. GLARIFYé€šè¿‡åˆ†ææ³¨è§†æ•°æ®ï¼Œç»“åˆGPT-4oç”Ÿæˆæ•°æ®é›†ï¼Œå¹¶è®¾è®¡çƒ­å›¾æ¨¡å—å°†æ³¨è§†ä¿¡æ¯èå…¥VLMsï¼Œä»è€Œè§£å†³æ­§ä¹‰æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGLARIFYåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œæå‡äº†è§†è§‰åŠ©æ‰‹äº¤äº’çš„å¯ç”¨æ€§å’Œç›´è§‚æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ™ºèƒ½çœ¼é•œçš„æ™®åŠï¼Œç”¨æˆ·çš„æ³¨æ„åŠ›è¢«æ•´åˆåˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ï¼Œä»¥ç®€åŒ–æ—¥å¸¸åœºæ™¯ä¸­çš„å¤šæ¨¡æ€æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œåˆ©ç”¨æ³¨è§†æ•°æ®æ¥å»ºæ¨¡ç”¨æˆ·çš„æ³¨æ„åŠ›å¯èƒ½ä¼šå¼•å…¥æ­§ä¹‰æ€§æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç”¨æˆ·çš„å£å¤´é—®é¢˜å› ä½¿ç”¨ä»£è¯æˆ–è·³è¿‡ä¸Šä¸‹æ–‡è€Œå˜å¾—æ¨¡ç³Šï¼›ï¼ˆ2ï¼‰äººç±»çš„æ³¨è§†æ¨¡å¼å¯èƒ½å­˜åœ¨å™ªå£°ï¼Œå¹¶ä¸å…¶å£å¤´é—®é¢˜è¡¨ç°å‡ºå¤æ‚çš„æ—¶ç©ºå…³ç³»ã€‚å…ˆå‰çš„å·¥ä½œä»…è€ƒè™‘å•å¼ å›¾åƒä½œä¸ºè§†è§‰æ¨¡æ€è¾“å…¥ï¼Œæ— æ³•æ•æ‰ç”¨æˆ·æ³¨æ„åŠ›çš„åŠ¨æ€ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGLARIFYçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ—¶ç©ºæ³¨è§†ä¿¡æ¯æ¥æé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†æäº†æ•°ç™¾ä¸ªå¸¦æœ‰æ³¨è§†æ¨¡æ€çš„æŸ¥è¯¢æ ·æœ¬ï¼Œä»¥è¯æ˜ç”¨æˆ·æ³¨è§†æ¨¡å¼çš„å™ªå£°æ€§è´¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨GPT-4oè®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®åˆæˆæµç¨‹ï¼Œç”Ÿæˆäº†GLARIFY-Ambiæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªä¸“é—¨çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è¿‡ç¨‹æ¥å¤„ç†å™ªå£°æ³¨è§†æ¨¡å¼ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªçƒ­å›¾æ¨¡å—ï¼Œå°†æ³¨è§†ä¿¡æ¯æ•´åˆåˆ°æœ€å…ˆè¿›çš„VLMsä¸­ï¼ŒåŒæ—¶ä¿ç•™å…¶é¢„è®­ç»ƒçŸ¥è¯†ã€‚æˆ‘ä»¬ä½¿ç”¨ä¿ç•™æµ‹è¯•é›†è¯„ä¼°äº†GLARIFYã€‚å®éªŒè¡¨æ˜ï¼ŒGLARIFYæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡å°†VLMsä¸äººç±»æ³¨æ„åŠ›è¿›è¡Œé²æ£’å¯¹é½ï¼ŒGLARIFYä¸ºä¸è§†è§‰åŠ©æ‰‹è¿›è¡Œå¯ç”¨ä¸”ç›´è§‚çš„äº¤äº’èŒƒå¼é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰åŠ©æ‰‹äº¤äº’ç³»ç»Ÿåœ¨å¤„ç†ç”¨æˆ·æé—®æ—¶ï¼Œé¢ä¸´ç€ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚ä¸€æ˜¯ç”¨æˆ·å£å¤´æé—®å¸¸å¸¸åŒ…å«ä»£è¯æˆ–çœç•¥ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´é—®é¢˜æœ¬èº«å­˜åœ¨æ­§ä¹‰ã€‚äºŒæ˜¯ç”¨æˆ·çš„æ³¨è§†æ•°æ®å…·æœ‰å™ªå£°ï¼Œä¸å£å¤´æé—®ä¹‹é—´å­˜åœ¨å¤æ‚çš„æ—¶ç©ºå…³ç³»ï¼Œéš¾ä»¥ç›´æ¥åˆ©ç”¨ã€‚å…ˆå‰çš„æ–¹æ³•é€šå¸¸åªè€ƒè™‘å•å¼ å›¾åƒä½œä¸ºè§†è§‰è¾“å…¥ï¼Œå¿½ç•¥äº†ç”¨æˆ·æ³¨æ„åŠ›çš„åŠ¨æ€å˜åŒ–ï¼Œæ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGLARIFYçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ—¶ç©ºæ³¨è§†ä¿¡æ¯æ¥æ¶ˆé™¤è§†è§‰åŠ©æ‰‹äº¤äº’ä¸­çš„æ­§ä¹‰æ€§ã€‚é€šè¿‡åˆ†æç”¨æˆ·çš„æ³¨è§†æ¨¡å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œä»è€Œæ›´å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚ä¸ºäº†è§£å†³æ³¨è§†æ•°æ®çš„å™ªå£°é—®é¢˜ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®åˆæˆæµç¨‹ï¼Œç”ŸæˆåŒ…å«å™ªå£°æ³¨è§†æ¨¡å¼çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰è¿‡ç¨‹è¿›è¡Œå¤„ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGLARIFYçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°æ®åˆ†ææ¨¡å—ã€æ•°æ®åˆæˆæ¨¡å—å’Œçƒ­å›¾èåˆæ¨¡å—ã€‚é¦–å…ˆï¼Œæ•°æ®åˆ†ææ¨¡å—ç”¨äºåˆ†æçœŸå®ç”¨æˆ·äº¤äº’æ•°æ®ï¼Œæ­ç¤ºæ³¨è§†æ¨¡å¼çš„å™ªå£°ç‰¹æ€§ã€‚ç„¶åï¼Œæ•°æ®åˆæˆæ¨¡å—åˆ©ç”¨GPT-4oç”ŸæˆGLARIFY-Ambiæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¸¦æœ‰å™ªå£°æ³¨è§†æ¨¡å¼çš„æŸ¥è¯¢æ ·æœ¬ã€‚æœ€åï¼Œçƒ­å›¾èåˆæ¨¡å—å°†æ³¨è§†ä¿¡æ¯ä»¥çƒ­å›¾çš„å½¢å¼æ•´åˆåˆ°ç°æœ‰çš„VLMsä¸­ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šGLARIFYçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š(1) æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®åˆæˆæµç¨‹ï¼Œç”¨äºç”ŸæˆåŒ…å«å™ªå£°æ³¨è§†æ¨¡å¼çš„æ•°æ®é›†ï¼Œè§£å†³äº†çœŸå®æ•°æ®éš¾ä»¥è·å–çš„é—®é¢˜ã€‚(2) è®¾è®¡äº†ä¸€ä¸ªçƒ­å›¾èåˆæ¨¡å—ï¼Œå¯ä»¥å°†æ³¨è§†ä¿¡æ¯æœ‰æ•ˆåœ°æ•´åˆåˆ°ç°æœ‰çš„VLMsä¸­ï¼Œè€Œä¸ä¼šç ´åæ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†ã€‚(3) è€ƒè™‘äº†ç”¨æˆ·æ³¨æ„åŠ›çš„åŠ¨æ€å˜åŒ–ï¼Œåˆ©ç”¨æ—¶ç©ºæ³¨è§†ä¿¡æ¯æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGLARIFYèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ç”¨æˆ·æé—®ä¸­çš„æ­§ä¹‰æ€§ï¼Œå¹¶æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®åˆæˆæ¨¡å—ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†GPT-4oæ¥ç”ŸæˆæŸ¥è¯¢æ ·æœ¬ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ€ç»´é“¾ï¼ˆCoTï¼‰è¿‡ç¨‹æ¥æ¨¡æ‹Ÿå™ªå£°æ³¨è§†æ¨¡å¼ã€‚åœ¨çƒ­å›¾èåˆæ¨¡å—ä¸­ï¼Œè®ºæ–‡å°†æ³¨è§†ä¿¡æ¯è½¬æ¢ä¸ºçƒ­å›¾ï¼Œå¹¶å°†å…¶ä¸è§†è§‰ç‰¹å¾è¿›è¡Œèåˆã€‚å…·ä½“çš„èåˆæ–¹å¼æœªçŸ¥ï¼Œä½†ç›®æ ‡æ˜¯ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶ï¼Œèå…¥æ³¨è§†ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGLARIFYåœ¨å¤„ç†æ­§ä¹‰æ€§é—®é¢˜ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºGLARIFYåœ¨ä¿ç•™æµ‹è¯•é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚é€šè¿‡é²æ£’åœ°å¯¹é½VLMsä¸äººç±»æ³¨æ„åŠ›ï¼ŒGLARIFYä¸ºæ›´ç›´è§‚å’Œå¯ç”¨çš„è§†è§‰åŠ©æ‰‹äº¤äº’é“ºå¹³äº†é“è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GLARIFYæŠ€æœ¯å¯åº”ç”¨äºæ™ºèƒ½çœ¼é•œã€è¾…åŠ©é©¾é©¶ã€è¿œç¨‹åä½œç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£ç”¨æˆ·çš„æ³¨è§†æ„å›¾ï¼Œè§†è§‰åŠ©æ‰‹å¯ä»¥æä¾›æ›´ç²¾å‡†ã€æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ï¼Œä¾‹å¦‚åœ¨æ™ºèƒ½çœ¼é•œä¸­å¿«é€Ÿè¯†åˆ«ç”¨æˆ·å…³æ³¨çš„ç‰©ä½“å¹¶æä¾›ç›¸å…³ä¿¡æ¯ï¼Œåœ¨è¾…åŠ©é©¾é©¶ä¸­é¢„æµ‹é©¾é©¶å‘˜çš„æ½œåœ¨é£é™©ï¼Œåœ¨è¿œç¨‹åä½œä¸­å¸®åŠ©å‚ä¸è€…æ›´å¥½åœ°ç†è§£å¯¹æ–¹çš„æ„å›¾ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rise in popularity of smart glasses, users' attention has been integrated into Vision-Language Models (VLMs) to streamline multi-modal querying in daily scenarios. However, leveraging gaze data to model users' attention may introduce ambiguity challenges: (1) users' verbal questions become ambiguous by using pronouns or skipping context, (2) humans' gaze patterns can be noisy and exhibit complex spatiotemporal relationships with their spoken questions. Previous works only consider single image as visual modality input, failing to capture the dynamic nature of the user's attention. In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal gaze information to enhance the model's effectiveness in real-world applications. Initially, we analyzed hundreds of querying samples with the gaze modality to demonstrate the noisy nature of users' gaze patterns. We then utilized GPT-4o to design an automatic data synthesis pipeline to generate the GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process to handle noisy gaze patterns. Finally, we designed a heatmap module to incorporate gaze information into cutting-edge VLMs while preserving their pretrained knowledge. We evaluated GLARIFY using a hold-out test set. Experiments demonstrate that GLARIFY significantly outperforms baselines. By robustly aligning VLMs with human attention, GLARIFY paves the way for a usable and intuitive interaction paradigm with a visual assistant.

