---
layout: default
title: PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data
---

# PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21965" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21965v2</a>
  <a href="https://arxiv.org/pdf/2509.21965.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21965v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21965v2', 'PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Zhu, Le Wan, Rui Xu, Yiheng Zhang, Honghua Chen, Zhiyang Dou, Cheng Lin, Yuan Liu, Mingqiang Wei

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: Project Page: https://czvvd.github.io/PartSAMPage/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPartSAMä»¥è§£å†³3Dç‰©ä½“åˆ†å‰²ä¸­çš„å‡ ä½•ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dç‰©ä½“åˆ†å‰²` `éƒ¨ä»¶è¯†åˆ«` `å‡ ä½•ç†è§£` `å¼€æ”¾ä¸–ç•Œ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Déƒ¨ä»¶åˆ†å‰²æ–¹æ³•é€šå¸¸ä¾èµ–äº2Dæ¨¡å‹çš„ç›‘ç£ï¼Œå¯¼è‡´å‡ ä½•ç†è§£ä¸è¶³å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚
2. PartSAMé€šè¿‡åœ¨å¤§è§„æ¨¡3Dæ•°æ®ä¸ŠåŸç”Ÿè®­ç»ƒï¼Œé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç”Ÿæˆç©ºé—´ç»“æ„åŒ–çš„æ ‡è®°ä»¥å®ç°éƒ¨ä»¶æ„ŸçŸ¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPartSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾ä¸–ç•Œèƒ½åŠ›ä¸Šçš„çªç ´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dç‰©ä½“çš„éƒ¨ä»¶åˆ†å‰²ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœåˆ†ç±»æ³•çš„é™åˆ¶å¹¶æ¨å¹¿åˆ°æœªè§è¿‡çš„3Dç‰©ä½“ï¼Œè¿‘æœŸçš„ç ”ç©¶è½¬å‘å¼€æ”¾ä¸–ç•Œçš„éƒ¨ä»¶åˆ†å‰²ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šè¿‡å°†2DåŸºç¡€æ¨¡å‹çš„ç›‘ç£è½¬ç§»åˆ°3Dä¸Šï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰å†…åœ¨å‡ ä½•ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºPartSAMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨å¤§è§„æ¨¡3Dæ•°æ®ä¸ŠåŸç”Ÿè®­ç»ƒçš„å¯æç¤ºéƒ¨ä»¶åˆ†å‰²æ¨¡å‹ã€‚PartSAMé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç»“åˆä¸‰å¹³é¢åŒåˆ†æ”¯ç¼–ç å™¨ç”Ÿæˆç©ºé—´ç»“æ„åŒ–çš„æ ‡è®°ï¼Œä»¥å®ç°å¯æ‰©å±•çš„éƒ¨ä»¶æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å¼•å…¥æ¨¡å‹å¾ªç¯æ³¨é‡Šç®¡é“ï¼ŒPartSAMä»åœ¨çº¿èµ„æºä¸­ç­–åˆ’äº†è¶…è¿‡äº”ç™¾ä¸‡ä¸ª3Då½¢çŠ¶-éƒ¨ä»¶å¯¹ï¼Œæä¾›äº†å¤šæ ·åŒ–å’Œç»†ç²’åº¦çš„æ ‡ç­¾ã€‚å®éªŒè¡¨æ˜ï¼ŒPartSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ ‡å¿—ç€3Déƒ¨ä»¶ç†è§£åŸºç¡€æ¨¡å‹çš„å…³é”®è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³3Dç‰©ä½“åˆ†å‰²ä¸­å¯¹å‡ ä½•ç‰¹å¾çš„ç†è§£ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡2Dæ¨¡å‹è½¬ç§»ç›‘ç£ï¼Œå¯¼è‡´è¡¨é¢ç†è§£å’Œåˆ†è§£æ§åˆ¶ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPartSAMçš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åœ¨å¤§è§„æ¨¡3Dæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨å¯æç¤ºçš„éƒ¨ä»¶åˆ†å‰²æ¨¡å‹ï¼Œæ—¨åœ¨æ•æ‰ç‰©ä½“çš„å†…åœ¨å‡ ä½•ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPartSAMé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œä½¿ç”¨ä¸‰å¹³é¢åŒåˆ†æ”¯ç¼–ç å™¨ç”Ÿæˆç©ºé—´ç»“æ„åŒ–çš„æ ‡è®°ï¼Œæ”¯æŒå¯æ‰©å±•çš„éƒ¨ä»¶æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šPartSAMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŸç”Ÿè®­ç»ƒäº3Dæ•°æ®ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„å‡ ä½•ç†è§£ä¸è¶³ï¼Œèƒ½å¤Ÿå®ç°å¯¹è¡¨é¢å’Œå†…éƒ¨ç»“æ„çš„è‡ªåŠ¨åˆ†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒPartSAMå¼•å…¥äº†æ¨¡å‹å¾ªç¯æ³¨é‡Šç®¡é“ï¼Œç­–åˆ’äº†è¶…è¿‡äº”ç™¾ä¸‡ä¸ª3Då½¢çŠ¶-éƒ¨ä»¶å¯¹ï¼Œç¡®ä¿äº†å¤šæ ·åŒ–å’Œç»†ç²’åº¦çš„æ ‡ç­¾ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥æ”¯æŒå¤§è§„æ¨¡ç›‘ç£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PartSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾ä¸–ç•Œéƒ¨ä»¶è¯†åˆ«ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartSAMåœ¨å•ä¸€æç¤ºä¸‹å®ç°äº†é«˜ç²¾åº¦çš„éƒ¨ä»¶è¯†åˆ«ï¼Œå¹¶åœ¨Segment-Every-Partæ¨¡å¼ä¸‹è‡ªåŠ¨åˆ†è§£å½¢çŠ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PartSAMåœ¨3Dç‰©ä½“è¯†åˆ«ã€æœºå™¨äººæŠ“å–ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„éƒ¨ä»¶åˆ†å‰²èƒ½åŠ›å¯ä»¥æå‡è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„ç†è§£èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½åˆ¶é€ å’Œäººæœºäº¤äº’çš„å‘å±•ã€‚æœªæ¥ï¼ŒPartSAMçš„æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥åº”ç”¨äºæ›´å¤æ‚çš„3Dåœºæ™¯åˆ†æå’Œç†è§£ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Segmenting 3D objects into parts is a long-standing challenge in computer vision. To overcome taxonomy constraints and generalize to unseen 3D objects, recent works turn to open-world part segmentation. These approaches typically transfer supervision from 2D foundation models, such as SAM, by lifting multi-view masks into 3D. However, this indirect paradigm fails to capture intrinsic geometry, leading to surface-only understanding, uncontrolled decomposition, and limited generalization. We present PartSAM, the first promptable part segmentation model trained natively on large-scale 3D data. Following the design philosophy of SAM, PartSAM employs an encoder-decoder architecture in which a triplane-based dual-branch encoder produces spatially structured tokens for scalable part-aware representation learning. To enable large-scale supervision, we further introduce a model-in-the-loop annotation pipeline that curates over five million 3D shape-part pairs from online assets, providing diverse and fine-grained labels. This combination of scalable architecture and diverse 3D data yields emergent open-world capabilities: with a single prompt, PartSAM achieves highly accurate part identification, and in a Segment-Every-Part mode, it automatically decomposes shapes into both surface and internal structures. Extensive experiments show that PartSAM outperforms state-of-the-art methods by large margins across multiple benchmarks, marking a decisive step toward foundation models for 3D part understanding.

