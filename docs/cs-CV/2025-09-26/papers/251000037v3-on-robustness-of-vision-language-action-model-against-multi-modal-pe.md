---
layout: default
title: On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations
---

# On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00037" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00037v3</a>
  <a href="https://arxiv.org/pdf/2510.00037.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00037v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00037v3', 'On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianing Guo, Zhenhong Wu, Chang Tu, Yiyao Ma, Xiangqi Kong, Zhiqian Liu, Jiaming Ji, Shuning Zhang, Yuanpei Chen, Kai Chen, Qi Dou, Yaodong Yang, Xianglong Liu, Huijie Zhao, Weifeng Lv, Simin Li

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRobustVLAï¼Œå¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¤šæ¨¡æ€æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¤šæ¨¡æ€é²æ£’æ€§` `å¯¹æŠ—è®­ç»ƒ` `å¤šè‡‚è€è™æœº` `æœºå™¨äººå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­é¢ä¸´å¤šæ¨¡æ€æ‰°åŠ¨æŒ‘æˆ˜ï¼Œä¾‹å¦‚åŠ¨ä½œã€æŒ‡ä»¤å’Œç¯å¢ƒå˜åŒ–ï¼Œè€Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰æ‰°åŠ¨ã€‚
2. RobustVLAé€šè¿‡ç¦»çº¿é²æ£’ä¼˜åŒ–å¢å¼ºè¾“å‡ºåŠ¨ä½œçš„é²æ£’æ€§ï¼Œå¹¶å¼ºåˆ¶è¾“å…¥åŠ¨ä½œä¸€è‡´æ€§ï¼ŒåŒæ—¶åˆ©ç”¨å¤šè‡‚è€è™æœºç®—æ³•è¯†åˆ«æœ€æœ‰å®³çš„å™ªå£°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRobustVLAåœ¨LIBEROå’ŒçœŸå®æœºå™¨äººFR5ä¸Šï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤šæ¨¡æ€æ‰°åŠ¨ä¸‹æ˜¾è‘—æå‡äº†æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸­ï¼Œå¯¹çœŸå®ä¸–ç•Œæ‰°åŠ¨çš„é²æ£’æ€§å¯¹äºéƒ¨ç½²è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç®€å•çš„è§†è§‰å¹²æ‰°ï¼Œå¿½ç•¥äº†åŠ¨ä½œã€æŒ‡ä»¤ã€ç¯å¢ƒå’Œè§‚å¯Ÿä¸­å‡ºç°çš„æ›´å¹¿æ³›çš„å¤šæ¨¡æ€æ‰°åŠ¨ã€‚æœ¬æ–‡é¦–å…ˆè¯„ä¼°äº†ä¸»æµVLAåœ¨å››ç§æ¨¡æ€çš„17ç§æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰åŠ¨ä½œæ˜¯æœ€è„†å¼±çš„æ¨¡æ€ï¼›ï¼ˆ2ï¼‰ç°æœ‰çš„è§†è§‰é²æ£’VLAåœ¨å…¶ä»–æ¨¡æ€ä¸­æ²¡æœ‰è·å¾—é²æ£’æ€§ï¼›ï¼ˆ3ï¼‰pi0é€šè¿‡åŸºäºæ‰©æ•£çš„åŠ¨ä½œå¤´è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚ä¸ºäº†æ„å»ºå¤šæ¨¡æ€é²æ£’çš„VLAï¼Œæœ¬æ–‡æå‡ºäº†RobustVLAï¼Œä»¥åº”å¯¹VLAè¾“å…¥å’Œè¾“å‡ºä¸­çš„æ‰°åŠ¨ã€‚å¯¹äºè¾“å‡ºé²æ£’æ€§ï¼Œæˆ‘ä»¬æ‰§è¡Œç¦»çº¿é²æ£’ä¼˜åŒ–ï¼Œä»¥å¯¹æŠ—æœ€åæƒ…å†µä¸‹çš„åŠ¨ä½œå™ªå£°ï¼Œä»è€Œæœ€å¤§åŒ–æµåŒ¹é…ç›®æ ‡ä¸­çš„ä¸åŒ¹é…ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯å¯¹æŠ—è®­ç»ƒã€æ ‡ç­¾å¹³æ»‘å’Œå¼‚å¸¸å€¼æƒ©ç½šã€‚å¯¹äºè¾“å…¥é²æ£’æ€§ï¼Œæˆ‘ä»¬å¼ºåˆ¶æ‰§è¡Œè·¨è¾“å…¥å˜åŒ–çš„åŠ¨ä½œä¸€è‡´æ€§ï¼Œä»¥ä¿ç•™ä»»åŠ¡è¯­ä¹‰ã€‚ä¸ºäº†è§£å†³å¤šä¸ªæ‰°åŠ¨é—®é¢˜ï¼Œæˆ‘ä»¬å°†é²æ£’æ€§å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œå¹¶åº”ç”¨ä¸Šé™ç½®ä¿¡åŒºé—´ç®—æ³•æ¥è‡ªåŠ¨è¯†åˆ«æœ€æœ‰å®³çš„å™ªå£°ã€‚åœ¨LIBEROä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RobustVLAåœ¨æ‰€æœ‰17ç§æ‰°åŠ¨ä¸‹ï¼Œåœ¨pi0éª¨å¹²ç½‘ç»œä¸Šå®ç°äº†12.6%çš„ç»å¯¹å¢ç›Šï¼Œåœ¨OpenVLAéª¨å¹²ç½‘ç»œä¸Šå®ç°äº†10.4%çš„ç»å¯¹å¢ç›Šï¼Œæ¯”ç°æœ‰çš„è§†è§‰é²æ£’VLAå®ç°äº†å¿«50.6å€çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨æ··åˆæ‰°åŠ¨ä¸‹å®ç°äº†10.4%çš„å¢ç›Šã€‚æˆ‘ä»¬çš„RobustVLAåœ¨çœŸå®ä¸–ç•Œçš„FR5æœºå™¨äººä¸Šå°¤å…¶æœ‰æ•ˆï¼Œåœ¨æœ‰é™çš„æ¼”ç¤ºä¸‹ï¼Œåœ¨å››ç§æ¨¡æ€çš„æ‰°åŠ¨ä¸‹å®ç°äº†65.6%çš„ç»å¯¹å¢ç›Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²æ—¶ï¼Œå®¹æ˜“å—åˆ°å¤šç§æ¨¡æ€çš„æ‰°åŠ¨å½±å“ï¼Œä¾‹å¦‚åŠ¨ä½œæ‰§è¡Œå™ªå£°ã€æŒ‡ä»¤æ¨¡ç³Šã€ç¯å¢ƒå˜åŒ–ä»¥åŠè§‚æµ‹è¯¯å·®ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰æ‰°åŠ¨ï¼Œå¿½ç•¥äº†å…¶ä»–æ¨¡æ€çš„è„†å¼±æ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRobustVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åŒæ—¶å¢å¼ºVLAæ¨¡å‹åœ¨è¾“å…¥å’Œè¾“å‡ºä¸¤ä¸ªå±‚é¢çš„é²æ£’æ€§ã€‚å¯¹äºè¾“å‡ºï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒçš„æ–¹å¼ï¼Œä½¿æ¨¡å‹å¯¹åŠ¨ä½œå™ªå£°å…·æœ‰æŠµæŠ—åŠ›ã€‚å¯¹äºè¾“å…¥ï¼Œé€šè¿‡ä¿æŒè¾“å…¥å˜åŒ–æ—¶åŠ¨ä½œçš„ä¸€è‡´æ€§ï¼Œæé«˜æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å¤šè‡‚è€è™æœºç®—æ³•è‡ªåŠ¨è¯†åˆ«å¹¶åº”å¯¹æœ€æœ‰å®³çš„å™ªå£°ç±»å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRobustVLAåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¾“å‡ºé²æ£’æ€§æ¨¡å—å’Œè¾“å…¥é²æ£’æ€§æ¨¡å—ã€‚è¾“å‡ºé²æ£’æ€§æ¨¡å—é€šè¿‡ç¦»çº¿é²æ£’ä¼˜åŒ–ï¼Œå¯¹æŠ—æœ€åæƒ…å†µä¸‹çš„åŠ¨ä½œå™ªå£°ï¼Œé‡‡ç”¨æµåŒ¹é…ç›®æ ‡å‡½æ•°æ¥è¡¡é‡åŠ¨ä½œçš„åŒ¹é…ç¨‹åº¦ï¼Œå¹¶æœ€å¤§åŒ–ä¸åŒ¹é…ç¨‹åº¦ã€‚è¾“å…¥é²æ£’æ€§æ¨¡å—é€šè¿‡å¼ºåˆ¶æ‰§è¡Œè·¨è¾“å…¥å˜åŒ–çš„åŠ¨ä½œä¸€è‡´æ€§æ¥ä¿æŒä»»åŠ¡è¯­ä¹‰ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¤šè‡‚è€è™æœºç®—æ³•æ¥è‡ªåŠ¨è¯†åˆ«æœ€æœ‰å®³çš„å™ªå£°ï¼Œå¹¶æ ¹æ®å™ªå£°çš„å±å®³ç¨‹åº¦è°ƒæ•´ä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šRobustVLAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€é²æ£’æ€§å¢å¼ºç­–ç•¥ï¼Œå®ƒä¸ä»…è€ƒè™‘äº†è§†è§‰æ‰°åŠ¨ï¼Œè¿˜åŒæ—¶å…³æ³¨åŠ¨ä½œã€æŒ‡ä»¤å’Œç¯å¢ƒç­‰å¤šç§æ¨¡æ€çš„æ‰°åŠ¨ã€‚æ­¤å¤–ï¼ŒRobustVLAé‡‡ç”¨å¤šè‡‚è€è™æœºç®—æ³•è‡ªåŠ¨è¯†åˆ«æœ€æœ‰å®³çš„å™ªå£°ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¾“å‡ºé²æ£’æ€§æ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†å¯¹æŠ—è®­ç»ƒçš„æ€æƒ³ï¼Œé€šè¿‡æœ€å¤§åŒ–æµåŒ¹é…ç›®æ ‡å‡½æ•°ä¸­çš„ä¸åŒ¹é…ç¨‹åº¦æ¥æ¨¡æ‹Ÿæœ€åæƒ…å†µä¸‹çš„åŠ¨ä½œå™ªå£°ã€‚åœ¨è¾“å…¥é²æ£’æ€§æ¨¡å—ä¸­ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œè·¨è¾“å…¥å˜åŒ–çš„åŠ¨ä½œä¸€è‡´æ€§æ¥ä¿æŒä»»åŠ¡è¯­ä¹‰ã€‚å¤šè‡‚è€è™æœºç®—æ³•é‡‡ç”¨ä¸Šé™ç½®ä¿¡åŒºé—´ï¼ˆUCBï¼‰ç®—æ³•æ¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»è€Œæœ‰æ•ˆåœ°è¯†åˆ«æœ€æœ‰å®³çš„å™ªå£°ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡åŒ…æ‹¬å¯¹æŠ—æŸå¤±ã€ä¸€è‡´æ€§æŸå¤±ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RobustVLAåœ¨LIBEROæ•°æ®é›†ä¸Šï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹pi0å’ŒOpenVLAï¼Œåœ¨æ‰€æœ‰17ç§æ‰°åŠ¨ä¸‹åˆ†åˆ«å–å¾—äº†12.6%å’Œ10.4%çš„ç»å¯¹æ€§èƒ½æå‡ã€‚åœ¨çœŸå®ä¸–ç•Œçš„FR5æœºå™¨äººå®éªŒä¸­ï¼ŒRobustVLAåœ¨å››ç§æ¨¡æ€çš„æ‰°åŠ¨ä¸‹å®ç°äº†65.6%çš„ç»å¯¹å¢ç›Šï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RobustVLAå¯åº”ç”¨äºå„ç§éœ€è¦ä¸ç¯å¢ƒäº¤äº’çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨å¤æ‚å’Œä¸ç¡®å®šç¯å¢ƒä¸­çš„é²æ£’æ€§ï¼Œå¯ä»¥æ˜¾è‘—æå‡æœºå™¨äººçš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”çœŸå®ä¸–ç•Œçš„æŒ‘æˆ˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.

