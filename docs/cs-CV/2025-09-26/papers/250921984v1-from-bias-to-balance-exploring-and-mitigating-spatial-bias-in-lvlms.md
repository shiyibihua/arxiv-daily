---
layout: default
title: From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs
---

# From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21984" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21984v1</a>
  <a href="https://arxiv.org/pdf/2509.21984.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21984v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21984v1', 'From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Weili Guan, Jun Yu, Min Zhang

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBaPAå¹³è¡¡ä½ç½®ç¼–ç æ–¹æ³•ï¼Œæå‡LVLMçš„ç©ºé—´é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç©ºé—´åè§` `ä½ç½®ç¼–ç ` `é²æ£’æ€§` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LVLMåœ¨ç©ºé—´ä½ç½®å˜åŒ–æ—¶è¡¨ç°å‡ºä¸ä¸€è‡´æ€§ï¼Œè¡¨æ˜å…¶ç©ºé—´è¯­ä¹‰ç†è§£å­˜åœ¨å±€é™ã€‚
2. è®ºæ–‡æå‡ºå¹³è¡¡ä½ç½®åˆ†é…ï¼ˆBaPAï¼‰æ–¹æ³•ï¼Œä¸ºæ‰€æœ‰å›¾åƒtokenåˆ†é…ç›¸åŒçš„ä½ç½®åµŒå…¥ï¼Œä¿ƒè¿›è§†è§‰ä¿¡æ¯å¹³è¡¡æ•´åˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBaPAæ— éœ€é‡æ–°è®­ç»ƒå³å¯æå‡LVLMçš„ç©ºé—´é²æ£’æ€§ï¼Œç»“åˆå¾®è°ƒå¯è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶å¯¹ç©ºé—´å˜åŒ–çš„é²æ£’æ€§ä»æœªå¾—åˆ°å……åˆ†ç†è§£ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†LVLMçš„ç©ºé—´åè§ï¼Œé‡ç‚¹å…³æ³¨å½“ç›¸åŒçš„å…³é”®è§†è§‰ä¿¡æ¯æ”¾ç½®åœ¨å›¾åƒä¸­çš„ä¸åŒä½ç½®æ—¶ï¼Œæ¨¡å‹å¦‚ä½•å“åº”ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¢æµ‹æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“å‰çš„LVLMåœ¨ç©ºé—´ç§»åŠ¨ä¸‹ç»å¸¸äº§ç”Ÿä¸ä¸€è‡´çš„è¾“å‡ºï¼Œæ­ç¤ºäº†å…¶ç©ºé—´è¯­ä¹‰ç†è§£çš„æ ¹æœ¬å±€é™æ€§ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§ç°è±¡å¹¶éæºäºè§†è§‰ç¼–ç å™¨ï¼Œè€Œæ˜¯æºäºè¯­è¨€æ¨¡å‹ç»„ä»¶ä¸­ä½ç½®åµŒå…¥çš„ä¸å¹³è¡¡è®¾è®¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¹¿æ³›é‡‡ç”¨çš„ä½ç½®åµŒå…¥ç­–ç•¥ï¼ˆå¦‚RoPEï¼‰åœ¨è·¨æ¨¡æ€äº¤äº’æœŸé—´å¼•å…¥äº†ä¸å¹³è¡¡ï¼Œå¯¼è‡´ä¸åŒä½ç½®çš„å›¾åƒtokenå¯¹è¯­ä¹‰ç†è§£äº§ç”Ÿä¸å¹³ç­‰çš„å½±å“ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¹³è¡¡ä½ç½®åˆ†é…ï¼ˆBaPAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æœºåˆ¶ï¼Œå®ƒä¸ºæ‰€æœ‰å›¾åƒtokenåˆ†é…ç›¸åŒçš„ä½ç½®åµŒå…¥ï¼Œä»è€Œä¿ƒè¿›äº†è§†è§‰ä¿¡æ¯çš„æ›´å¹³è¡¡çš„æ•´åˆã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒBaPAå¢å¼ºäº†LVLMçš„ç©ºé—´é²æ£’æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”åœ¨ä¸è½»é‡çº§å¾®è°ƒç›¸ç»“åˆæ—¶ï¼Œè¿›ä¸€æ­¥æé«˜äº†å…¶åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚å¯¹ä¿¡æ¯æµçš„è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒBaPAäº§ç”Ÿäº†å¹³è¡¡çš„æ³¨æ„åŠ›ï¼Œä»è€Œå®ç°äº†æ›´å…¨é¢çš„è§†è§‰ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å¤„ç†ç©ºé—´ä½ç½®å˜åŒ–æ—¶ï¼Œè¡¨ç°å‡ºä¸ä¸€è‡´çš„è¾“å‡ºï¼Œå³ç›¸åŒçš„è§†è§‰ä¿¡æ¯åœ¨å›¾åƒçš„ä¸åŒä½ç½®ä¼šå¯¼è‡´æ¨¡å‹äº§ç”Ÿä¸åŒçš„ç†è§£ã€‚è¿™ç§ç©ºé—´åè§æºäºæ¨¡å‹å¯¹å›¾åƒä¸­ä¸åŒä½ç½®çš„è§†è§‰ç‰¹å¾èµ‹äºˆäº†ä¸ç›¸ç­‰çš„æƒé‡ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å‡†ç¡®ç†è§£å›¾åƒçš„æ•´ä½“è¯­ä¹‰ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚RoPEç­‰ä½ç½®ç¼–ç ç­–ç•¥ï¼Œåœ¨è·¨æ¨¡æ€äº¤äº’æ—¶å¼•å…¥äº†ä¸å¹³è¡¡æ€§ï¼ŒåŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¶ˆé™¤å›¾åƒtokençš„ä½ç½®å·®å¼‚ï¼Œä½¿æ¨¡å‹å¹³ç­‰åœ°å¯¹å¾…å›¾åƒä¸­çš„æ‰€æœ‰è§†è§‰ä¿¡æ¯ã€‚é€šè¿‡ä¸ºæ‰€æœ‰å›¾åƒtokenåˆ†é…ç›¸åŒçš„ä½ç½®åµŒå…¥ï¼Œå¯ä»¥é¿å…æ¨¡å‹è¿‡åº¦å…³æ³¨æŸäº›ç‰¹å®šä½ç½®çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹ç©ºé—´å˜åŒ–çš„é²æ£’æ€§ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¹³è¡¡è§†è§‰ä¿¡æ¯åœ¨è¯­ä¹‰ç†è§£è¿‡ç¨‹ä¸­çš„è´¡çŒ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£å›¾åƒå†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„æ–¹æ³•ä¸»è¦é’ˆå¯¹LVLMä¸­çš„è¯­è¨€æ¨¡å‹éƒ¨åˆ†è¿›è¡Œæ”¹è¿›ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨è§†è§‰ç¼–ç å™¨æå–å›¾åƒç‰¹å¾åï¼Œä¼ ç»Ÿçš„åšæ³•æ˜¯ä¸ºæ¯ä¸ªå›¾åƒtokenåˆ†é…ä¸åŒçš„ä½ç½®åµŒå…¥ï¼Œç„¶åå°†è¿™äº›å¸¦æœ‰ä½ç½®ä¿¡æ¯çš„è§†è§‰ç‰¹å¾è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚è€ŒBaPAæ–¹æ³•åˆ™ç›´æ¥å°†æ‰€æœ‰å›¾åƒtokençš„ä½ç½®åµŒå…¥è®¾ç½®ä¸ºç›¸åŒçš„å€¼ï¼Œç„¶åå†è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ã€‚æ•´ä¸ªæµç¨‹ä¿æŒäº†LVLMåŸæœ‰çš„æ¶æ„ï¼Œåªæ˜¯åœ¨ä½ç½®åµŒå…¥çš„åˆ†é…æ–¹å¼ä¸Šè¿›è¡Œäº†ä¿®æ”¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šBaPAæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä¸å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–ä½ç½®ç¼–ç æ–¹æ³•ç›¸æ¯”ï¼ŒBaPAé€šè¿‡ä¸€ç§æå…¶ç®€æ´çš„æ–¹å¼æ¶ˆé™¤äº†ä½ç½®åè§ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„ç©ºé—´é²æ£’æ€§ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¤§é‡çš„ä¿®æ”¹æˆ–é‡æ–°è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„LVLMä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šBaPAæ–¹æ³•çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•é€‰æ‹©åˆé€‚çš„ä½ç½®åµŒå…¥å€¼ã€‚è®ºæ–‡ä¸­æåˆ°ï¼Œå¯ä»¥å°†æ‰€æœ‰å›¾åƒtokençš„ä½ç½®åµŒå…¥è®¾ç½®ä¸ºä¸€ä¸ªå›ºå®šçš„å¸¸æ•°å‘é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹å¼ç”Ÿæˆä¸€ä¸ªå…±äº«çš„ä½ç½®åµŒå…¥ã€‚å…·ä½“å®ç°æ—¶ï¼Œéœ€è¦æ ¹æ®ä¸åŒçš„LVLMæ¶æ„å’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†å°†BaPAä¸è½»é‡çº§å¾®è°ƒç›¸ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥è°ƒæ•´è¯­è¨€æ¨¡å‹çš„å‚æ•°ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”BaPAå¸¦æ¥çš„ä½ç½®ä¿¡æ¯å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBaPAæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜LVLMçš„ç©ºé—´é²æ£’æ€§ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBaPAä¸è½»é‡çº§å¾®è°ƒç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å‡†ç¡®ç‡æå‡äº†è¶…è¿‡5%ã€‚ä¿¡æ¯æµåˆ†æè¡¨æ˜ï¼ŒBaPAèƒ½å¤Ÿäº§ç”Ÿæ›´å¹³è¡¡çš„æ³¨æ„åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£å›¾åƒå†…å®¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç©ºé—´é²æ£’æ€§çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹ç©ºé—´å˜åŒ–çš„é²æ£’æ€§ï¼Œå¯ä»¥ä½¿å…¶åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­è¡¨ç°æ›´å¥½ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have achieved remarkable success across a wide range of multimodal tasks, yet their robustness to spatial variations remains insufficiently understood. In this work, we present a systematic study of the spatial bias of LVLMs, focusing on how models respond when identical key visual information is placed at different locations within an image. Through a carefully designed probing dataset, we demonstrate that current LVLMs often produce inconsistent outputs under such spatial shifts, revealing a fundamental limitation in their spatial-semantic understanding. Further analysis shows that this phenomenon originates not from the vision encoder, which reliably perceives and interprets visual content across positions, but from the unbalanced design of position embeddings in the language model component. In particular, the widely adopted position embedding strategies, such as RoPE, introduce imbalance during cross-modal interaction, leading image tokens at different positions to exert unequal influence on semantic understanding. To mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple yet effective mechanism that assigns identical position embeddings to all image tokens, promoting a more balanced integration of visual information. Extensive experiments show that BaPA enhances the spatial robustness of LVLMs without retraining and further boosts their performance across diverse multimodal benchmarks when combined with lightweight fine-tuning. Further analysis of information flow reveals that BaPA yields balanced attention, enabling more holistic visual understanding.

