---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-24
---

# cs.CVï¼ˆ2025-08-24ï¼‰

ğŸ“Š å…± **7** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250817524v1-omnimri-a-unified-vision-language-foundation-model-for-generalist-mr.html">OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation</a></td>
  <td>æå‡ºOmniMRIä»¥è§£å†³MRIè§£è¯»æµç¨‹ç¢ç‰‡åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17524v1" data-paper-url="./papers/250817524v1-omnimri-a-unified-vision-language-foundation-model-for-generalist-mr.html" onclick="toggleFavorite(this, '2508.17524v1', 'OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250817243v2-covipal-layer-wise-contextualized-visual-token-pruning-for-large-vis.html">CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</a></td>
  <td>æå‡ºCoViPALä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ ‡è®°å†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17243v2" data-paper-url="./papers/250817243v2-covipal-layer-wise-contextualized-visual-token-pruning-for-large-vis.html" onclick="toggleFavorite(this, '2508.17243v2', 'CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>3</td>
  <td><a href="./papers/250817502v1-social-mae-a-transformer-based-multimodal-autoencoder-for-face-and-v.html">Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice</a></td>
  <td>æå‡ºSocial-MAEä»¥è§£å†³å¤šæ¨¡æ€ç¤¾äº¤è¡Œä¸ºç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17502v1" data-paper-url="./papers/250817502v1-social-mae-a-transformer-based-multimodal-autoencoder-for-face-and-v.html" onclick="toggleFavorite(this, '2508.17502v1', 'Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250817509v1-dinotwins-combining-dino-and-barlow-twins-for-robust-label-efficient.html">DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers</a></td>
  <td>æå‡ºDinoTwinsä»¥è§£å†³æ ‡ç­¾æ•ˆç‡ä½ä¸‹çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17509v1" data-paper-url="./papers/250817509v1-dinotwins-combining-dino-and-barlow-twins-for-robust-label-efficient.html" onclick="toggleFavorite(this, '2508.17509v1', 'DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250817205v1-multi-agent-visual-language-reasoning-for-comprehensive-highway-scen.html">Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</a></td>
  <td>æå‡ºå¤šæ™ºèƒ½ä½“è§†è§‰è¯­è¨€æ¨ç†æ¡†æ¶ä»¥è§£å†³é«˜é€Ÿå…¬è·¯åœºæ™¯ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17205v1" data-paper-url="./papers/250817205v1-multi-agent-visual-language-reasoning-for-comprehensive-highway-scen.html" onclick="toggleFavorite(this, '2508.17205v1', 'Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250817255v1-seer-var-semantic-egocentric-environment-reasoner-for-vehicle-augmen.html">SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</a></td>
  <td>æå‡ºSEER-VARä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸‹è½¦è¾†å¢å¼ºç°å®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17255v1" data-paper-url="./papers/250817255v1-seer-var-semantic-egocentric-environment-reasoner-for-vehicle-augmen.html" onclick="toggleFavorite(this, '2508.17255v1', 'SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/250817468v2-a-synthetic-dataset-for-manometry-recognition-in-robotic-application.html">A Synthetic Dataset for Manometry Recognition in Robotic Applications</a></td>
  <td>æå‡ºæ··åˆæ•°æ®åˆæˆæ–¹æ³•ä»¥è§£å†³å·¥ä¸šç¯å¢ƒæ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">domain randomization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17468v2" data-paper-url="./papers/250817468v2-a-synthetic-dataset-for-manometry-recognition-in-robotic-application.html" onclick="toggleFavorite(this, '2508.17468v2', 'A Synthetic Dataset for Manometry Recognition in Robotic Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)