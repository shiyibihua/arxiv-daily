---
layout: default
title: Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice
---

# Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17502" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17502v1</a>
  <a href="https://arxiv.org/pdf/2508.17502.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17502v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17502v1', 'Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hugo Bohy, Minh Tran, Kevin El Haddad, Thierry Dutoit, Mohammad Soleymani

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24

**å¤‡æ³¨**: 5 pages, 3 figures, IEEE FG 2024 conference

**æœŸåˆŠ**: 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HuBohy/SocialMAE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSocial-MAEä»¥è§£å†³å¤šæ¨¡æ€ç¤¾äº¤è¡Œä¸ºç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `æƒ…æ„Ÿè¯†åˆ«` `ç¤¾äº¤è¡Œä¸ºåˆ†æ` `éŸ³é¢‘-è§†è§‰èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç†è§£å¤šæ¨¡æ€ç¤¾äº¤è¡Œä¸ºæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯çš„èåˆæ–¹é¢ã€‚
2. Social-MAEé€šè¿‡ä¿®æ”¹CAV-MAEæ¶æ„ï¼Œå¢å¼ºäº†è¾“å…¥å¸§æ•°é‡ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ç¤¾äº¤æ•°æ®é›†ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæå‡äº†æ¨¡å‹çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSocial-MAEåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œç¬‘å£°è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæ˜¾æ€§ä¸ªæ€§ä¼°è®¡ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»ç¤¾äº¤è¡Œä¸ºæœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œå› æ­¤éœ€è¦å¼€å‘å¼ºå¤§çš„è§†å¬æ¨¡å‹æ¥è¿›è¡Œæ„ŸçŸ¥ã€‚æœ¬æ–‡æå‡ºäº†Social-MAEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©å±•ç‰ˆå¯¹æ¯”éŸ³é¢‘-è§†è§‰æ©ç è‡ªç¼–ç å™¨ï¼ˆCAV-MAEï¼‰çš„é¢„è®­ç»ƒè§†å¬æ©ç è‡ªç¼–ç å™¨ï¼Œä¸“é—¨é’ˆå¯¹ç¤¾äº¤æ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬å¯¹CAV-MAEè¿›è¡Œäº†ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥æ”¶æ›´å¤šå¸§ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨å¤§è§„æ¨¡äººç±»ç¤¾äº¤äº’åŠ¨æ•°æ®é›†ï¼ˆVoxCeleb2ï¼‰ä¸Šä»¥è‡ªç›‘ç£æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒã€‚é€šè¿‡å¯¹ä¸åŒç¤¾äº¤å’Œæƒ…æ„Ÿä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿè¯†åˆ«ã€ç¬‘å£°æ£€æµ‹å’Œæ˜¾æ€§ä¸ªæ€§ä¼°è®¡ï¼‰è¿›è¡Œå¾®è°ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œç¬‘å£°è¯†åˆ«ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨æ˜¾æ€§ä¸ªæ€§ä¼°è®¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè¯æ˜äº†é¢†åŸŸå†…è‡ªç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç¤¾äº¤è¡Œä¸ºç†è§£ä¸­çš„éŸ³é¢‘ä¸è§†è§‰ä¿¡æ¯èåˆä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç¤¾äº¤åœºæ™¯æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰å¤šæ¨¡æ€ä¿¡æ¯çš„å…³è”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Social-MAEé€šè¿‡æ‰©å±•CAV-MAEçš„è¾“å…¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†æ›´å¤šçš„å¸§æ•°æ®ï¼Œä»è€Œæ›´å…¨é¢åœ°æ•æ‰ç¤¾äº¤è¡Œä¸ºä¸­çš„éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ã€‚è‡ªç›‘ç£é¢„è®­ç»ƒä½¿å¾—æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå†…è·å¾—æ›´å¥½çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSocial-MAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯çš„è¾“å…¥æ¨¡å—ã€æ©ç è‡ªç¼–ç å™¨æ ¸å¿ƒä»¥åŠè‡ªç›‘ç£é¢„è®­ç»ƒé˜¶æ®µã€‚æ¨¡å‹é¦–å…ˆæ¥æ”¶å¤šå¸§éŸ³é¢‘å’Œè§†è§‰æ•°æ®ï¼Œç„¶åé€šè¿‡æ©ç æœºåˆ¶è¿›è¡Œç‰¹å¾å­¦ä¹ ï¼Œæœ€ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¯¹CAV-MAEçš„æ‰©å±•ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤šçš„è¾“å…¥å¸§ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ç¤¾äº¤æ•°æ®é›†ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒã€‚è¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è¾“å…¥å¸§çš„æ•°é‡ã€æ©ç æ¯”ä¾‹å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ã€‚ç½‘ç»œç»“æ„é‡‡ç”¨äº†Transformeræ¶æ„ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSocial-MAEåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¶…è¿‡äº†ç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œç¬‘å£°è¯†åˆ«ä»»åŠ¡ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºè‡ªç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤æœºå™¨äººã€æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒSocial-MAEèƒ½å¤Ÿåœ¨æƒ…æ„Ÿè¯†åˆ«ã€ç¤¾äº¤è¡Œä¸ºåˆ†æç­‰æ–¹é¢æä¾›æ›´ç²¾å‡†çš„æ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œç¤¾äº¤åª’ä½“åˆ†æä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human social behaviors are inherently multimodal necessitating the development of powerful audiovisual models for their perception. In this paper, we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE), which is pre-trained on audiovisual social data. Specifically, we modify CAV-MAE to receive a larger number of frames as input and pre-train it on a large dataset of human social interaction (VoxCeleb2) in a self-supervised manner. We demonstrate the effectiveness of this model by finetuning and evaluating the model on different social and affective downstream tasks, namely, emotion recognition, laughter detection and apparent personality estimation. The model achieves state-of-the-art results on multimodal emotion recognition and laughter recognition and competitive results for apparent personality estimation, demonstrating the effectiveness of in-domain self-supervised pre-training. Code and model weight are available here https://github.com/HuBohy/SocialMAE.

