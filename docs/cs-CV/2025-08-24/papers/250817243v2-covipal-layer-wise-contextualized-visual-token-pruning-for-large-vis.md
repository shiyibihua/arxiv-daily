---
layout: default
title: CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models
---

# CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17243" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17243v2</a>
  <a href="https://arxiv.org/pdf/2508.17243.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17243v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17243v2', 'CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zicong Tang, Ziyang Ma, Suqing Wang, Zuchao Li, Lefei Zhang, Hai Zhao, Yun Li, Qianren Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24 (æ›´æ–°: 2025-08-30)

**å¤‡æ³¨**: Accepted by EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoViPALä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ ‡è®°å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰æ ‡è®°ä¿®å‰ª` `ä¸Šä¸‹æ–‡åŒ–` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æµ…å±‚ä¸­ç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´å†—ä½™è§†è§‰æ ‡è®°çš„ä¿®å‰ªæ•ˆæœä¸ä½³ã€‚
2. CoViPALé€šè¿‡å±‚çº§ä¸Šä¸‹æ–‡åŒ–çš„ä¿®å‰ªæ–¹æ³•ï¼Œåˆ©ç”¨æ’ä»¶å¼ä¿®å‰ªæ¨¡å—æœ‰æ•ˆé¢„æµ‹å’Œç§»é™¤å†—ä½™è§†è§‰æ ‡è®°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoViPALåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ— è®­ç»ƒå’Œæœ‰è®­ç»ƒä¿®å‰ªæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å¤„ç†ç”±æ–‡æœ¬æ ‡è®°å’Œä»å›¾åƒæˆ–è§†é¢‘ä¸­æå–çš„è§†è§‰æ ‡è®°ç»„æˆçš„å¤šæ¨¡æ€è¾“å…¥ã€‚ç”±äºä¸°å¯Œçš„è§†è§‰ä¿¡æ¯ï¼Œå•å¼ å›¾åƒå¯ä»¥ç”Ÿæˆæˆåƒä¸Šä¸‡çš„è§†è§‰æ ‡è®°ï¼Œå¯¼è‡´åœ¨é¢„å¡«å……é˜¶æ®µçš„é«˜è®¡ç®—æˆæœ¬å’Œè§£ç æ—¶çš„æ˜¾è‘—å†…å­˜å¼€é”€ã€‚ç°æœ‰æ–¹æ³•å°è¯•ä¿®å‰ªå†—ä½™è§†è§‰æ ‡è®°ï¼Œä½†åœ¨æµ…å±‚ä¸­ç”±äºç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºCoViPALï¼Œä¸€ç§å±‚çº§ä¸Šä¸‹æ–‡åŒ–è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œé‡‡ç”¨è½»é‡çº§çš„æ’ä»¶å¼ä¿®å‰ªæ¨¡å—ï¼ˆPPMï¼‰åœ¨è§†è§‰æ ‡è®°è¢«LVLMå¤„ç†ä¹‹å‰é¢„æµ‹å¹¶ç§»é™¤å†—ä½™æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼ŒCoViPALåœ¨ç›¸åŒæ ‡è®°é¢„ç®—ä¸‹è¶…è¶Šäº†æ— è®­ç»ƒä¿®å‰ªæ–¹æ³•ï¼Œå¹¶åœ¨å¯æ¯”ç›‘ç£ä¸‹è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æé«˜LVLMçš„æ¨ç†æ•ˆç‡è€Œä¸å½±å“å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è§†è§‰æ ‡è®°çš„å†—ä½™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æµ…å±‚ä¸­ç”±äºä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆä¿®å‰ªå†—ä½™æ ‡è®°ï¼Œå¯¼è‡´è®¡ç®—å’Œå†…å­˜å¼€é”€å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoViPALæå‡ºäº†ä¸€ç§å±‚çº§ä¸Šä¸‹æ–‡åŒ–çš„è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡å·æ¥è¯†åˆ«å’Œç§»é™¤å†—ä½™è§†è§‰æ ‡è®°ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå³ä½¿åœ¨æµ…å±‚ä¹Ÿèƒ½æœ‰æ•ˆä¿®å‰ªå†—ä½™æ ‡è®°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ’ä»¶å¼ä¿®å‰ªæ¨¡å—ï¼ˆPPMï¼‰ï¼Œè¯¥æ¨¡å—åœ¨è§†è§‰æ ‡è®°è¢«LVLMå¤„ç†ä¹‹å‰è¿›è¡Œå†—ä½™æ ‡è®°çš„é¢„æµ‹å’Œç§»é™¤ã€‚PPMæ˜¯è½»é‡çº§çš„ã€æ¨¡å‹æ— å…³çš„ï¼Œèƒ½å¤Ÿä¸å¤šç§LVLMæ¶æ„æ— ç¼é›†æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šCoViPALçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å±‚çº§ä¸Šä¸‹æ–‡åŒ–çš„ä¿®å‰ªç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æµ…å±‚ä¸­æœ‰æ•ˆè¯†åˆ«å†—ä½™è§†è§‰æ ‡è®°ï¼Œè€Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šPPMçš„è®¾è®¡ç¡®ä¿å…¶è½»é‡æ€§å’Œæ¨¡å‹æ— å…³æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„LVLMæ¶æ„ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†è¯´æ˜ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoViPALåœ¨ç›¸åŒçš„æ ‡è®°é¢„ç®—ä¸‹è¶…è¶Šäº†æ— è®­ç»ƒä¿®å‰ªæ–¹æ³•ï¼Œå¹¶åœ¨å¯æ¯”ç›‘ç£ä¸‹è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ•°æ®å°šæœªæä¾›ï¼Œéœ€å‚è€ƒåŸæ–‡è¿›è¡Œè¯¦ç»†äº†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoViPALçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨å®æ—¶å›¾åƒå’Œè§†é¢‘åˆ†æã€æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.

