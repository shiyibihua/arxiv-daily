---
layout: default
title: Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving
---

# Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.00589" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.00589v2</a>
  <a href="https://arxiv.org/pdf/2508.00589.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.00589v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.00589v2', 'Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Stefan Englmeier, Max A. BÃ¼ttner, Katharina Winter, Fabian B. Flohr

**åˆ†ç±»**: cs.CV, cs.CL, cs.IR, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-01 (æ›´æ–°: 2025-08-12)

**å¤‡æ³¨**: Project page: https://iv.ee.hm.edu/contextmotionclip/; This work has been submitted to the IEEE for possible publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸Šä¸‹æ–‡çš„è¿åŠ¨æ£€ç´¢æ¡†æ¶ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„è¾¹ç¼˜æ¡ˆä¾‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è¿åŠ¨æ£€ç´¢` `å¤šæ¨¡æ€å­¦ä¹ ` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `äººç±»è¡Œä¸ºè¯†åˆ«` `æ•°æ®é›†æ‰©å±•` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸­æ£€ç´¢ç¨€æœ‰çš„äººç±»è¡Œä¸ºåœºæ™¯é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¿åŠ¨æ£€ç´¢æ¡†æ¶ï¼Œç»“åˆSMPLè¿åŠ¨åºåˆ—ä¸è§†é¢‘å¸§ï¼Œç¼–ç ä¸ºå¤šæ¨¡æ€åµŒå…¥ç©ºé—´ã€‚
3. åœ¨WayMoCoæ•°æ®é›†ä¸Šï¼Œæå‡ºçš„æ–¹æ³•åœ¨è¿åŠ¨ä¸Šä¸‹æ–‡æ£€ç´¢ä¸­æ¯”ç°æœ‰æ¨¡å‹æé«˜äº†27.5%çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¿…é¡»åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­å¯é è¿è¡Œï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠè„†å¼±é“è·¯ä½¿ç”¨è€…ï¼ˆVRUsï¼‰å¤æ‚è¡Œä¸ºçš„æƒ…å†µä¸‹ã€‚è¯†åˆ«è¿™äº›è¾¹ç¼˜æ¡ˆä¾‹å¯¹äºç¨³å¥è¯„ä¼°å’Œæ³›åŒ–è‡³å…³é‡è¦ï¼Œä½†åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸­æ£€ç´¢è¿™äº›ç¨€æœ‰çš„äººç±»è¡Œä¸ºåœºæ™¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ”¯æŒå¯¹å¤šæ ·åŒ–äººæœ¬åœºæ™¯çš„ç›®æ ‡è¯„ä¼°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¿åŠ¨æ£€ç´¢æ¡†æ¶ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºSkinned Multi-Person Linearï¼ˆSMPLï¼‰çš„è¿åŠ¨åºåˆ—å’Œç›¸åº”çš„è§†é¢‘å¸§ï¼Œå¹¶å°†å…¶ç¼–ç åˆ°ä¸è‡ªç„¶è¯­è¨€å¯¹é½çš„å…±äº«å¤šæ¨¡æ€åµŒå…¥ç©ºé—´ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡æ–‡æœ¬æŸ¥è¯¢å®ç°äº†äººç±»è¡Œä¸ºåŠå…¶ä¸Šä¸‹æ–‡çš„å¯æ‰©å±•æ£€ç´¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†WayMoCoæ•°æ®é›†ï¼Œè¿™æ˜¯Waymoå¼€æ”¾æ•°æ®é›†çš„æ‰©å±•ï¼ŒåŒ…å«ä»ç”Ÿæˆçš„ä¼ªçœŸå®SMPLåºåˆ—å’Œç›¸åº”å›¾åƒæ•°æ®ä¸­æ´¾ç”Ÿçš„è‡ªåŠ¨æ ‡æ³¨è¿åŠ¨å’Œåœºæ™¯ä¸Šä¸‹æ–‡æè¿°ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨WayMoCoæ•°æ®é›†ä¸Šåœ¨è¿åŠ¨ä¸Šä¸‹æ–‡æ£€ç´¢ä¸­æ¯”æœ€å…ˆè¿›çš„æ¨¡å‹æé«˜äº†å¤šè¾¾27.5%çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆæ£€ç´¢å¤æ‚å’Œç¨€æœ‰çš„äººç±»è¡Œä¸ºï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›è¾¹ç¼˜æ¡ˆä¾‹æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´è¯„ä¼°ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡ç»“åˆSMPLæ¨¡å‹ç”Ÿæˆçš„è¿åŠ¨åºåˆ—å’Œè§†é¢‘å¸§ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œæ£€ç´¢ï¼Œä»è€Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¿åŠ¨æ£€ç´¢ï¼Œå¢å¼ºäº†å¯¹äººç±»è¡Œä¸ºçš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è¿åŠ¨åºåˆ—ä¸è§†é¢‘å¸§çš„ç»“åˆã€ç”Ÿæˆå¤šæ¨¡æ€åµŒå…¥ç©ºé—´ä»¥åŠåŸºäºæ–‡æœ¬çš„æŸ¥è¯¢æ£€ç´¢ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¿åŠ¨åºåˆ—ç”Ÿæˆã€è§†é¢‘å¸§æå–å’Œå¤šæ¨¡æ€åµŒå…¥ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†SMPLæ¨¡å‹ä¸è§†é¢‘æ•°æ®ç»“åˆï¼Œå½¢æˆä¸€ä¸ªå…±äº«çš„å¤šæ¨¡æ€åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—æ£€ç´¢è¿‡ç¨‹æ›´åŠ çµæ´»å’Œé«˜æ•ˆï¼Œæ˜¾è‘—æå‡äº†å¯¹å¤æ‚è¡Œä¸ºçš„è¯†åˆ«èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†å¤šå±‚ç¥ç»ç½‘ç»œè¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ£€ç´¢ç²¾åº¦ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨WayMoCoæ•°æ®é›†ä¸Šï¼Œæå‡ºçš„æ–¹æ³•åœ¨è¿åŠ¨ä¸Šä¸‹æ–‡æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†27.5%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨è¯„ä¼°ã€äº¤é€šè¡Œä¸ºåˆ†æä»¥åŠäººæœºäº¤äº’è®¾è®¡ã€‚é€šè¿‡æé«˜å¯¹å¤æ‚äººç±»è¡Œä¸ºçš„æ£€ç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¼€å‘ä¸ä¼˜åŒ–ï¼Œæå‡è¡Œè½¦å®‰å…¨æ€§å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset.

