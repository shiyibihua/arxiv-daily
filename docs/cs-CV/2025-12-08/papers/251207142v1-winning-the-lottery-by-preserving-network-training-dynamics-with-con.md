---
layout: default
title: Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search
---

# Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search

**arXiv**: [2512.07142v1](https://arxiv.org/abs/2512.07142) | [PDF](https://arxiv.org/pdf/2512.07142.pdf)

**ä½œè€…**: Tanay Arora, Christof Teuscher

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConcrete Ticket Searchç®—æ³•ï¼Œé€šè¿‡æ•´ä½“ä¼˜åŒ–è§£å†³åˆå§‹åŒ–å‰ªæžçš„æ€§èƒ½å·®è·é—®é¢˜ã€‚**

**å…³é”®è¯**: `å½©ç¥¨ç¥¨å‡è®¾` `åˆå§‹åŒ–å‰ªæž` `ç»„åˆä¼˜åŒ–` `çŸ¥è¯†è’¸é¦` `ç¨€ç–ç½‘ç»œ` `è®­ç»ƒåŠ¨æ€`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåˆå§‹åŒ–å‰ªæžæ–¹æ³•ä¾èµ–ä¸€é˜¶æ˜¾è‘—æ€§æŒ‡æ ‡ï¼Œå¿½ç•¥æƒé‡é—´ä¾èµ–ï¼Œå¯¼è‡´ç²¾åº¦-ç¨€ç–åº¦æƒè¡¡ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Concreteæ¾å¼›å’Œæ¢¯åº¦å¹³è¡¡æ–¹æ¡ˆï¼Œå°†å­ç½‘ç»œå‘çŽ°å»ºæ¨¡ä¸ºç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œæ— éœ€æ•æ„Ÿè¶…å‚æ•°è°ƒä¼˜ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒCTSç”Ÿæˆå­ç½‘ç»œé€šè¿‡å¥å…¨æ€§æ£€æŸ¥ï¼Œç²¾åº¦æŽ¥è¿‘æˆ–è¶…è¿‡LTRï¼Œè®¡ç®—æˆæœ¬å¤§å¹…é™ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

