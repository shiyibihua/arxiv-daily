---
layout: default
title: GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory
---

# GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory

**arXiv**: [2512.07782v1](https://arxiv.org/abs/2512.07782) | [PDF](https://arxiv.org/pdf/2512.07782.pdf)

**ä½œè€…**: Jiaxu Liu, Yuhe Bai, Christos-Savvas Bouganis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGatedFWAä»¥è§£å†³æ»‘åŠ¨çª—å£æ³¨æ„åŠ›åœ¨è‡ªå›žå½’æ¨¡åž‹ä¸­å†…å­˜æ›´æ–°ä¸ç¨³å®šå’Œæ¢¯åº¦é—®é¢˜ï¼Œä¿æŒçº¿æ€§æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `è‡ªå›žå½’æ¨¡åž‹` `æ³¨æ„åŠ›æœºåˆ¶` `çº¿æ€§å¤æ‚åº¦` `å†…å­˜é—¨æŽ§` `æ¢¯åº¦ç¨³å®š` `è¯­è¨€å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ»‘åŠ¨çª—å£æ³¨æ„åŠ›åœ¨å…³è”å†…å­˜è§£é‡Šä¸‹æ›´æ–°æ— ç•Œï¼ŒSoftmaxæ³¨æ„åŠ›å¯¼è‡´å†…å­˜æ”¶ç¼©å’Œæ¢¯åº¦æ¶ˆå¤±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¯å­¦ä¹ çš„é—¨æŽ§æœºåˆ¶ï¼Œé€šè¿‡è¡°å‡åç½®ç¨³å®šå†…å­˜æ›´æ–°ï¼Œå®žçŽ°å¯æŽ§æ¢¯åº¦æµã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šï¼ŒGatedFWAä¿æŒé«˜åžåé‡ï¼Œé›†æˆåŽ‹ç¼©æ–¹æ³•ï¼Œæ³›åŒ–è‡³å¤šç§è‡ªå›žå½’é¢†åŸŸã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.

