---
layout: default
title: DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection
---

# DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection

**arXiv**: [2512.07351v1](https://arxiv.org/abs/2512.07351) | [PDF](https://arxiv.org/pdf/2512.07351.pdf)

**ä½œè€…**: Sayeem Been Zaman, Wasimul Karim, Arefin Ittesafun Abian, Reem E. Mohamed, Md Rafiqul Islam, Asif Karim, Sami Azam

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepAgentå¤šæ™ºèƒ½ä½“èžåˆæ¡†æž¶ï¼Œé€šè¿‡åŒæµåä½œæå‡å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹çš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹` `å¤šæ™ºèƒ½ä½“åä½œ` `è§†å¬ä¸ä¸€è‡´æ€§æ£€æµ‹` `éšæœºæ£®æž—èžåˆ` `è·¨æ•°æ®é›†éªŒè¯` `é²æ£’æ€§å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å•æ¨¡åž‹å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æ˜“å—æ¨¡æ€ä¸åŒ¹é…ã€å™ªå£°å’Œæ“çºµå½±å“ï¼Œé²æ£’æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡åŒæ™ºèƒ½ä½“åä½œæ¡†æž¶ï¼ŒAgent-1åŸºäºŽAlexNetæ£€æµ‹è§†è§‰ä¼ªé€ ç—•è¿¹ï¼ŒAgent-2ç»“åˆéŸ³é¢‘ç‰¹å¾å’ŒOCRæ£€æµ‹è§†å¬ä¸ä¸€è‡´æ€§ï¼Œé€šè¿‡éšæœºæ£®æž—å…ƒåˆ†ç±»å™¨èžåˆå†³ç­–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå…ƒåˆ†ç±»å™¨åœ¨DeepFakeTIMITä¸Šè¾¾åˆ°97.49%å‡†ç¡®çŽ‡ï¼Œæ˜¾ç¤ºè·¨æ•°æ®é›†é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

