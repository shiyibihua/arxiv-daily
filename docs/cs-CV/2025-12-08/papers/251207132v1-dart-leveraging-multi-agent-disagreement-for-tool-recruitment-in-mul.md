---
layout: default
title: DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning
---

# DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning

**arXiv**: [2512.07132v1](https://arxiv.org/abs/2512.07132) | [PDF](https://arxiv.org/pdf/2512.07132.pdf)

**ä½œè€…**: Nithin Sivakumaran, Justin Chih-Yao Chen, David Wan, Yue Zhang, Jaehong Yoon, Elias Stengel-Eskin, Mohit Bansal

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDARTå¤šæ™ºèƒ½ä½“æ¡†æž¶ï¼Œåˆ©ç”¨è§†è§‰æ™ºèƒ½ä½“é—´çš„åˆ†æ­§æ¥æ‹›å‹Ÿå·¥å…·ä»¥å¢žå¼ºå¤šæ¨¡æ€æŽ¨ç†**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“è¾©è®º` `è§†è§‰å·¥å…·è°ƒç”¨` `å¤šæ¨¡æ€æŽ¨ç†` `åˆ†æ­§è§£å†³` `ä¸“å®¶çŸ¥è¯†å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æŽ¨ç†ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆé€‰æ‹©å’Œè°ƒç”¨è§†è§‰å·¥å…·ä»¥è§£å†³æ™ºèƒ½ä½“é—´çš„åˆ†æ­§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºè¯†åˆ«åˆ†æ­§ï¼Œå¼•å…¥å·¥å…·æä¾›æ–°ä¿¡æ¯å’Œä¸€è‡´æ€§è¯„åˆ†ï¼Œèšåˆå™¨é€‰æ‹©æœ€ä½³ç­”æ¡ˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽåŸºçº¿ï¼Œå¦‚A-OKVQAå’ŒMMMUåˆ†åˆ«æå‡3.4%å’Œ2.4%ï¼Œå¹¶é€‚åº”æ–°å·¥å…·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

