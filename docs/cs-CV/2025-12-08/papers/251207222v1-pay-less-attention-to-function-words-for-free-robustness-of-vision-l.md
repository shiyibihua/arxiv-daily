---
layout: default
title: Pay Less Attention to Function Words for Free Robustness of Vision-Language Models
---

# Pay Less Attention to Function Words for Free Robustness of Vision-Language Models

**arXiv**: [2512.07222v1](https://arxiv.org/abs/2512.07222) | [PDF](https://arxiv.org/pdf/2512.07222.pdf)

**ä½œè€…**: Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Chao Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡½æ•°è¯åŽ»æ³¨æ„åŠ›æ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹å¯¹æŠ—è·¨æ¨¡æ€æ”»å‡»çš„é²æ£’æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯¹æŠ—é²æ£’æ€§` `è·¨æ¨¡æ€æ”»å‡»` `æ³¨æ„åŠ›æœºåˆ¶` `å‡½æ•°è¯å¤„ç†` `é›¶æ ·æœ¬æ€§èƒ½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå‡½æ•°è¯å¯¼è‡´è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è·¨æ¨¡æ€å¯¹æŠ—æ”»å‡»ä¸‹è„†å¼±ï¼Œéœ€å¹³è¡¡é²æ£’æ€§ä¸Žæ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å‡½æ•°è¯åŽ»æ³¨æ„åŠ›ï¼Œåœ¨æ³¨æ„åŠ›å¤´ä¸­è®¡ç®—åŽŸå§‹ä¸Žå‡½æ•°è¯äº¤å‰æ³¨æ„åŠ›å¹¶å·®åˆ†ç›¸å‡ï¼Œå¢žå¼ºå¯¹é½ä¸Žé²æ£’æ€§ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨æ£€ç´¢å’Œè§†è§‰å®šä½ä»»åŠ¡ä¸Šæ˜¾è‘—é™ä½Žæ”»å‡»æˆåŠŸçŽ‡ï¼Œæ€§èƒ½ä¸‹é™æžå°æˆ–ç•¥æœ‰æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§ä¸Žæ³›åŒ–æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

