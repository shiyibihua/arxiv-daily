---
layout: default
title: Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models
---

# Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models

**arXiv**: [2512.07234v1](https://arxiv.org/abs/2512.07234) | [PDF](https://arxiv.org/pdf/2512.07234.pdf)

**ä½œè€…**: Biao Chen, Lin Zuo, Mengmeng Jing, Kunbin He, Yuchen Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDropout Prompt Learningä»¥æå‡è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨ä½Žæ ·æœ¬å­¦ä¹ å’Œåˆ†å¸ƒå¤–æ³›åŒ–ç­‰åœºæ™¯çš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡åž‹` `æç¤ºå­¦ä¹ ` `é²æ£’æ€§å¢žå¼º` `ä½Žæ ·æœ¬å­¦ä¹ ` `åˆ†å¸ƒå¤–æ³›åŒ–` `æ­£åˆ™åŒ–æŠ€æœ¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨ä½Žæ ·æœ¬å­¦ä¹ ã€é•¿å°¾åˆ†ç±»å’Œåˆ†å¸ƒå¤–æ³›åŒ–ç­‰æŒ‘æˆ˜æ€§åœºæ™¯ä¸­é²æ£’æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽtokené‡è¦æ€§è¯„ä¼°ï¼Œåœ¨æ–‡æœ¬å’Œè§†è§‰åˆ†æ”¯åº”ç”¨è‡ªé€‚åº”dropoutï¼Œå¹¶å¼•å…¥æ®‹å·®ç†µæ­£åˆ™åŒ–ä»¥å¹³è¡¡è¯­ä¹‰å¯¹é½å’Œå¤šæ ·æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆï¼Œåœ¨åŸºç¡€åˆ°æ–°ç±»åˆ«æ³›åŒ–ä¸Šä¼˜äºŽKgCoOpå’ŒPromptSRCç­‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

