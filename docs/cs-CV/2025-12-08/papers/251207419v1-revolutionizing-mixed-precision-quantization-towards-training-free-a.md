---
layout: default
title: Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models
---

# Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models

**arXiv**: [2512.07419v1](https://arxiv.org/abs/2512.07419) | [PDF](https://arxiv.org/pdf/2512.07419.pdf)

**ä½œè€…**: Haidong Kang, Jun Du, Lihong Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„è®­ç»ƒæ— å…³è‡ªåŠ¨ä»£ç†å‘çŽ°æ¡†æž¶ï¼Œä»¥é©æ–°æ··åˆç²¾åº¦é‡åŒ–è®¾è®¡èŒƒå¼ã€‚**

**å…³é”®è¯**: `æ··åˆç²¾åº¦é‡åŒ–` `å¤§è¯­è¨€æ¨¡åž‹` `è®­ç»ƒæ— å…³ä¼˜åŒ–` `è‡ªåŠ¨ä»£ç†å‘çŽ°` `å¼ºåŒ–å­¦ä¹ ` `æ·±åº¦å­¦ä¹ åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ··åˆç²¾åº¦é‡åŒ–ä¾èµ–äººå·¥è®¾è®¡ä»£ç†æˆ–é«˜æˆæœ¬ä¼˜åŒ–ï¼Œæ•ˆçŽ‡ä½Žä¸”ä¸çµæ´»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹è‡ªåŠ¨å‘çŽ°ä»£ç†ï¼Œé€šè¿‡ç›´æŽ¥ç­–ç•¥ä¼˜åŒ–å¢žå¼ºæŽ¨ç†ï¼Œå½¢æˆæ­£åé¦ˆå¾ªçŽ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œä¸ºæ··åˆç²¾åº¦é‡åŒ–ç¤¾åŒºæä¾›æ–°è§†è§’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

