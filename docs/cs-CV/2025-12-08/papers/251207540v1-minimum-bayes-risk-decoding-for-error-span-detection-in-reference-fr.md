---
layout: default
title: Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation
---

# Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation

**arXiv**: [2512.07540v1](https://arxiv.org/abs/2512.07540) | [PDF](https://arxiv.org/pdf/2512.07540.pdf)

**ä½œè€…**: Boxuan Lyu, Haiyue Song, Hidetaka Kamigaito, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Kotaro Funakoshi, Manabu Okumura

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ€å°è´å¶æ–¯é£Žé™©è§£ç ä»¥è§£å†³ç”Ÿæˆå¼é”™è¯¯è·¨åº¦æ£€æµ‹ä¸­æ¨¡åž‹ä¼¼ç„¶ä¸Žäººå·¥æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜**

**å…³é”®è¯**: `é”™è¯¯è·¨åº¦æ£€æµ‹` `æœ€å°è´å¶æ–¯é£Žé™©è§£ç ` `æœºå™¨ç¿»è¯‘è¯„ä¼°` `ç”Ÿæˆå¼æ¨¡åž‹` `è’¸é¦è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç”Ÿæˆå¼é”™è¯¯è·¨åº¦æ£€æµ‹æ–¹æ³•ä¸­ï¼Œæœ€å¤§åŽéªŒè§£ç å‡è®¾æ¨¡åž‹æ¦‚çŽ‡ä¸Žäººå·¥æ ‡æ³¨ç›¸ä¼¼åº¦å®Œç¾Žç›¸å…³ï¼Œä½†å®žé™…å­˜åœ¨ä¸ä¸€è‡´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåº”ç”¨æœ€å°è´å¶æ–¯é£Žé™©è§£ç ï¼Œä½¿ç”¨å¥å­å’Œè·¨åº¦çº§ç›¸ä¼¼åº¦åº¦é‡ä½œä¸ºæ•ˆç”¨å‡½æ•°ï¼Œé€‰æ‹©æ›´æŽ¥è¿‘äººå·¥æ ‡æ³¨çš„å€™é€‰å‡è®¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºæœ€å°è´å¶æ–¯é£Žé™©è§£ç åœ¨ç³»ç»Ÿã€å¥å­å’Œè·¨åº¦çº§åˆ«ä¼˜äºŽåŸºçº¿ï¼Œå¹¶é€šè¿‡è’¸é¦æ¶ˆé™¤æŽ¨ç†å»¶è¿Ÿç“¶é¢ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

