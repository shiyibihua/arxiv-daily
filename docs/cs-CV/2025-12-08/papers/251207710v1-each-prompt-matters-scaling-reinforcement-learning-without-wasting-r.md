---
layout: default
title: Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE
---

# Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE

**arXiv**: [2512.07710v1](https://arxiv.org/abs/2512.07710) | [PDF](https://arxiv.org/pdf/2512.07710.pdf)

**ä½œè€…**: Anxiang Zeng, Haibo Zhang, Hailing Zhang, Kaixiang Mo, Liang Yao, Ling Hu, Long Zhang, Shuman Liu, Shuyi Xie, Yanshi Li, Yizhang Chen, Yuepeng Sheng, Yuwei Huang, Zhaochen Xu, Zhiqiang Zhou, Ziqin Liew

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ¯ä¸ªæç¤ºéƒ½é‡è¦çš„RLæ¡†æž¶ï¼Œä»¥é«˜æ•ˆè®­ç»ƒç™¾äº¿è§„æ¨¡MoEæŽ¨ç†æ¨¡åž‹**

**å…³é”®è¯**: `ç™¾äº¿è§„æ¨¡MoEæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ æ¡†æž¶` `é›¶æ–¹å·®æç¤ºæ¶ˆé™¤` `ç†µè‡ªé€‚åº”ä¼˜åŒ–` `è·¯ç”±å™¨é‡æ”¾` `é«˜åžåç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç™¾äº¿è§„æ¨¡MoEæ¨¡åž‹RLè®­ç»ƒå­˜åœ¨é›¶æ–¹å·®æç¤ºæµªè´¹rolloutsã€é‡è¦æ€§é‡‡æ ·ä¸ç¨³å®šã€ä¼˜åŠ¿åè½¬å’Œç³»ç»Ÿç“¶é¢ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šé˜¶æ®µé›¶æ–¹å·®æ¶ˆé™¤ã€ç†µè‡ªé€‚åº”ä¼˜åŒ–ESPOã€è·¯ç”±å™¨é‡æ”¾ç­–ç•¥å’ŒFP8é«˜åžåç³»ç»Ÿã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹åœ¨å†…éƒ¨å’Œå…¬å¼€è¯„ä¼°ä¸­è¡¨çŽ°å¼ºåŠ²ï¼Œå®žçŽ°ç¨³å®šé«˜æ•ˆè®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

