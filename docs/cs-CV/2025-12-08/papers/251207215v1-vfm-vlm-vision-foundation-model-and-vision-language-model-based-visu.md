---
layout: default
title: VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation
---

# VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation

**arXiv**: [2512.07215v1](https://arxiv.org/abs/2512.07215) | [PDF](https://arxiv.org/pdf/2512.07215.pdf)

**ä½œè€…**: Md Selim Sarowar, Sungho Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒCLIPä¸ŽDINOv2åœ¨æŠ“å–åœºæ™¯3Då§¿æ€ä¼°è®¡ä¸­çš„è§†è§‰åŸºç¡€æ¨¡åž‹ä¸Žè§†è§‰è¯­è¨€æ¨¡åž‹æ–¹æ³•**

**å…³é”®è¯**: `3Då§¿æ€ä¼°è®¡` `è§†è§‰åŸºç¡€æ¨¡åž‹` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŠ“å–åœºæ™¯` `6Då§¿æ€ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯„ä¼°è§†è§‰åŸºç¡€æ¨¡åž‹å’Œè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æŠ“å–åœºæ™¯6Dç‰©ä½“å§¿æ€ä¼°è®¡ä¸­çš„æ€§èƒ½å·®å¼‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¯¹æ¯”CLIPåŸºäºŽè¯­è¨€æŽ¥åœ°çš„è¯­ä¹‰ç†è§£å’ŒDINOv2çš„å¯†é›†å‡ ä½•ç‰¹å¾æå–
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºCLIPåœ¨è¯­ä¹‰ä¸€è‡´æ€§ä¸Šæ›´ä¼˜ï¼ŒDINOv2åœ¨å‡ ä½•ç²¾åº¦ä¸Šè¡¨çŽ°ç«žäº‰æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

