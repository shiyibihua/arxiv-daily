---
layout: default
title: Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach
---

# Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach

**arXiv**: [2512.07170v1](https://arxiv.org/abs/2512.07170) | [PDF](https://arxiv.org/pdf/2512.07170.pdf)

**ä½œè€…**: Jiayang Li, Chengjie Jiang, Junjun Jiang, Pengwei Liang, Jiayi Ma, Liqiang Nie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiTFuseæ‰©æ•£Transformeræ¡†æž¶ï¼Œå®žçŽ°è¯­ä¹‰å¯æŽ§çš„å›¾åƒèžåˆ**

**å…³é”®è¯**: `å›¾åƒèžåˆ` `æ‰©æ•£Transformer` `è¯­ä¹‰æŽ§åˆ¶` `å¤šæ¨¡æ€å¯¹é½` `é›¶æ ·æœ¬æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å›¾åƒèžåˆæ–¹æ³•åœ¨é²æ£’æ€§ã€é€‚åº”æ€§å’Œå¯æŽ§æ€§æ–¹é¢å—é™ï¼Œç¼ºä¹ç”¨æˆ·æ„å›¾æ•´åˆèƒ½åŠ›
2. DiTFuseé€šè¿‡è”åˆç¼–ç å›¾åƒä¸Žè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œåœ¨å•ä¸€æ¨¡åž‹ä¸­å®žçŽ°ç«¯åˆ°ç«¯è¯­ä¹‰æ„ŸçŸ¥èžåˆ
3. å®žéªŒåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾ç¤ºä¼˜å¼‚æ€§èƒ½ï¼Œæ”¯æŒå¤šçº§æŽ§åˆ¶å’Œé›¶æ ·æœ¬æ³›åŒ–

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

