---
layout: default
title: Geometric Prior-Guided Federated Prompt Calibration
---

# Geometric Prior-Guided Federated Prompt Calibration

**arXiv**: [2512.07208v1](https://arxiv.org/abs/2512.07208) | [PDF](https://arxiv.org/pdf/2512.07208.pdf)

**ä½œè€…**: Fei Luo, Ziwei Zhao, Mingxuan Wang, Duoyang Li, Zhe Qian, Jiayi Tuo, Chenyue Zhou, Yanbiao Ma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡ ä½•å…ˆéªŒå¼•å¯¼çš„è”é‚¦æç¤ºæ ¡å‡†ä»¥è§£å†³æ•°æ®å¼‚æž„æ€§å¯¼è‡´çš„æœ¬åœ°è®­ç»ƒåå·®é—®é¢˜**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æç¤ºå­¦ä¹ ` `æ•°æ®å¼‚æž„æ€§` `å‡ ä½•å…ˆéªŒ` `ç‰¹å¾æ ¡å‡†` `éšç§ä¿æŠ¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè”é‚¦æç¤ºå­¦ä¹ ä¸­æ•°æ®å¼‚æž„æ€§å¯¼è‡´æœ¬åœ°æç¤ºè®­ç»ƒåå·®ï¼ŒçŽ°æœ‰æ–¹æ³•æœªèƒ½æ ¹æ²»æ­¤é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æœåŠ¡å™¨é‡æž„å…¨å±€å‡ ä½•å…ˆéªŒï¼Œå®¢æˆ·ç«¯ä½¿ç”¨å‡ ä½•å…ˆéªŒæ ¡å‡†å±‚å¯¹é½æœ¬åœ°ç‰¹å¾åˆ†å¸ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡ç­¾åæ–œCIFAR-100ä¸Šè¶…è¶ŠSOTA 2.15%ï¼Œæžç«¯åæ–œä¸‹æå‡9.17%ï¼Œä½œä¸ºæ’ä»¶æ¨¡å—æå‡FedAvgæ€§èƒ½4.60%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($Î²$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($Î²$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.

