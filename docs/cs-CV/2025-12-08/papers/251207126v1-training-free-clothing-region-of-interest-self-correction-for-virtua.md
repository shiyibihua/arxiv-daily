---
layout: default
title: Training-free Clothing Region of Interest Self-correction for Virtual Try-On
---

# Training-free Clothing Region of Interest Self-correction for Virtual Try-On

**arXiv**: [2512.07126v1](https://arxiv.org/abs/2512.07126) | [PDF](https://arxiv.org/pdf/2512.07126.pdf)

**ä½œè€…**: Shengjie Lu, Zhibin Wan, Jiejie Liu, Quan Zhang, Mingjie Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— è®­ç»ƒæœè£…æ„Ÿå…´è¶£åŒºåŸŸè‡ªæ ¡æ­£æ–¹æ³•ä»¥æå‡è™šæ‹Ÿè¯•ç©¿æ•ˆæžœ**

**å…³é”®è¯**: `è™šæ‹Ÿè¯•ç©¿` `æ³¨æ„åŠ›æœºåˆ¶` `èƒ½é‡å‡½æ•°` `è¯„ä¼°æŒ‡æ ‡` `æœè£…åŒºåŸŸæ ¡æ­£` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è™šæ‹Ÿè¯•ç©¿æ–¹æ³•åœ¨å›¾æ¡ˆã€çº¹ç†å’Œè¾¹ç•Œä¸Šå­˜åœ¨ç”Ÿæˆæœè£…ä¸Žç›®æ ‡æœè£…çš„å·®å¼‚é—®é¢˜
2. é€šè¿‡èƒ½é‡å‡½æ•°çº¦æŸç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›å›¾ï¼Œä½¿æ³¨æ„åŠ›æ›´é›†ä¸­äºŽæœè£…åŒºåŸŸï¼Œæ”¹å–„ç»†èŠ‚ä¸€è‡´æ€§
3. åœ¨VITON-HDå’ŒDressCodeæ•°æ®é›†ä¸Šï¼Œä¼ ç»ŸæŒ‡æ ‡å’Œæ–°VTIDæŒ‡æ ‡å‡ä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼Œå¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

