---
layout: default
title: A Geometric Unification of Concept Learning with Concept Cones
---

# A Geometric Unification of Concept Learning with Concept Cones

**arXiv**: [2512.07355v1](https://arxiv.org/abs/2512.07355) | [PDF](https://arxiv.org/pdf/2512.07355.pdf)

**ä½œè€…**: Alexandre Rocchi--Henry, Thomas Fel, Gianni Franchi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¦‚å¿µé”¥å‡ ä½•æ¡†æž¶ï¼Œç»Ÿä¸€ç›‘ç£ä¸Žéžç›‘ç£æ¦‚å¿µå­¦ä¹ ï¼Œå¹¶å»ºç«‹é‡åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚**

**å…³é”®è¯**: `æ¦‚å¿µå­¦ä¹ ` `å‡ ä½•æ¡†æž¶` `ç¨€ç–è‡ªç¼–ç å™¨` `æ¦‚å¿µç“¶é¢ˆæ¨¡åž‹` `å¯è§£é‡Šæ€§` `é‡åŒ–è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›‘ç£ä¸Žéžç›‘ç£æ¦‚å¿µå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚CBMä¸ŽSAEï¼‰ç¼ºä¹ç»Ÿä¸€æ¡†æž¶ä¸Žé‡åŒ–è¯„ä¼°æ ‡å‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†CBMä¸ŽSAEç»Ÿä¸€ä¸ºå­¦ä¹ æ¿€æ´»ç©ºé—´ä¸­çš„çº¿æ€§æ–¹å‘ï¼Œå…¶éžè´Ÿç»„åˆå½¢æˆæ¦‚å¿µé”¥ï¼Œå¹¶åŸºäºŽé”¥åŒ…å«å…³ç³»å»ºç«‹è¯„ä¼°æŒ‡æ ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°ç¨€ç–æ€§ä¸Žæ‰©å±•å› å­çš„â€œç”œç‚¹â€ï¼Œæœ€å¤§åŒ–ä¸ŽCBMæ¦‚å¿µçš„å‡ ä½•å’Œè¯­ä¹‰å¯¹é½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

