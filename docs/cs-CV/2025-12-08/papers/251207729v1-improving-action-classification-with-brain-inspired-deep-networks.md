---
layout: default
title: Improving action classification with brain-inspired deep networks
---

# Improving action classification with brain-inspired deep networks

**arXiv**: [2512.07729v1](https://arxiv.org/abs/2512.07729) | [PDF](https://arxiv.org/pdf/2512.07729.pdf)

**ä½œè€…**: Aidas Aglinskas, Stefano Anzellotti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè„‘å¯å‘çš„åŒæµæ·±åº¦ç½‘ç»œä»¥æå‡åŠ¨ä½œåˆ†ç±»æ€§èƒ½ï¼Œæ¨¡æ‹Ÿäººç±»å¯¹èº¯ä½“å’ŒèƒŒæ™¯çš„æ„ŸçŸ¥åˆ†ç¦»ã€‚**

**å…³é”®è¯**: `åŠ¨ä½œè¯†åˆ«` `è„‘å¯å‘ç½‘ç»œ` `é¢†åŸŸç‰¹å¼‚æ€§` `æ·±åº¦ç¥žç»ç½‘ç»œ` `èº¯ä½“æ„ŸçŸ¥` `èƒŒæ™¯æ„ŸçŸ¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦ç¥žç»ç½‘ç»œåœ¨åŠ¨ä½œè¯†åˆ«ä¸­å¯èƒ½è¿‡åº¦ä¾èµ–èƒŒæ™¯ä¿¡æ¯ï¼Œå¿½è§†èº¯ä½“ä¿¡æ¯ï¼Œä¸Žäººç±»æ„ŸçŸ¥æ¨¡å¼ä¸åŒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡è„‘å¯å‘çš„æž¶æž„ï¼ŒåŒ…å«ç‹¬ç«‹çš„èº¯ä½“æµå’ŒèƒŒæ™¯æµï¼Œæ¨¡æ‹Ÿå¤§è„‘çš„é¢†åŸŸç‰¹å¼‚æ€§å¤„ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯¥æž¶æž„åœ¨HAA500æ•°æ®é›†ä¸Šæå‡æ€§èƒ½ï¼Œä¸”å‡†ç¡®çŽ‡æ¨¡å¼æ›´æŽ¥è¿‘äººç±»å‚ä¸Žè€…ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

