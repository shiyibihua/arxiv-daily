---
layout: default
title: The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers
---

# The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers

**arXiv**: [2512.07331v1](https://arxiv.org/abs/2512.07331) | [PDF](https://arxiv.org/pdf/2512.07331.pdf)

**ä½œè€…**: Kanishk Awadhiya

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè§†è§‰Transformerä¸­æ•°æ®é©±åŠ¨çš„è¡¨ç¤ºç¨€ç–æ€§ï¼Œå³å½’çº³ç“¶é¢ˆä¸Žä»»åŠ¡è¯­ä¹‰æŠ½è±¡ç›¸å…³**

**å…³é”®è¯**: `è§†è§‰Transformer` `è¡¨ç¤ºç¨€ç–æ€§` `å½’çº³ç“¶é¢ˆ` `æœ‰æ•ˆç¼–ç ç»´åº¦` `æ•°æ®é©±åŠ¨é€‚åº”` `è¯­ä¹‰æŠ½è±¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰Transformerç¼ºä¹å·ç§¯ç½‘ç»œçš„å±‚æ¬¡å½’çº³åç½®ï¼Œä½†å¸¸è‡ªå‘å½¢æˆUå½¢ç†µåˆ†å¸ƒï¼Œä¸­é—´å±‚åŽ‹ç¼©ä¿¡æ¯
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†æžDINOè®­ç»ƒViTçš„å±‚é—´æœ‰æ•ˆç¼–ç ç»´åº¦ï¼ŒæŽ¢ç©¶æ•°æ®é›†ç»„æˆå¤æ‚åº¦å¯¹è¡¨ç¤ºç¨€ç–æ€§çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°çº¹ç†ä¸°å¯Œæ•°æ®é›†ä¿æŒé«˜ç§©è¡¨ç¤ºï¼Œè€Œå¯¹è±¡ä¸­å¿ƒæ•°æ®é›†é©±åŠ¨ç½‘ç»œåœ¨ä¸­é—´å±‚æŠ‘åˆ¶é«˜é¢‘ä¿¡æ¯ï¼Œå­¦ä¹ è¯­ä¹‰ç‰¹å¾éš”ç¦»

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

