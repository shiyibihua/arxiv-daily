---
layout: default
title: Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation
---

# Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation

**arXiv**: [2512.07747v1](https://arxiv.org/abs/2512.07747) | [PDF](https://arxiv.org/pdf/2512.07747.pdf)

**ä½œè€…**: Shihao Zhao, Yitong Chen, Zeyinzi Jiang, Bojia Zi, Shaozhe Hao, Yu Liu, Chaojie Mao, Kwan-Yee K. Wong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUnisonæ¡†æž¶ï¼Œä»¥ä½Žæˆæœ¬å®žçŽ°å¤šæ¨¡æ€ç†è§£ä¸Žç”Ÿæˆçš„ç»Ÿä¸€è‡ªåŠ¨åŒ–å¤„ç†**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç»Ÿä¸€ç†è§£ä¸Žç”Ÿæˆ` `è‡ªåŠ¨åŒ–ä»»åŠ¡è§£æž` `ä½Žæˆæœ¬è®­ç»ƒ` `ä¸¤é˜¶æ®µæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ–¹æ³•æˆæœ¬é«˜ã€ä»»åŠ¡è¦†ç›–æœ‰é™ä¸”ä¾èµ–æ‰‹åŠ¨é…ç½®å‚æ•°
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ¡ˆï¼Œç»“åˆé¢„è®­ç»ƒæ¨¡åž‹ï¼Œè‡ªåŠ¨è§£æžç”¨æˆ·æ„å›¾å’Œä»»åŠ¡å…ƒä¿¡æ¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»…ç”¨500kæ ·æœ¬å’Œ50 GPUå°æ—¶ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸Šå®žçŽ°å‡†ç¡®è‡ªåŠ¨è¯†åˆ«å’Œä¼˜è¶Šæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

