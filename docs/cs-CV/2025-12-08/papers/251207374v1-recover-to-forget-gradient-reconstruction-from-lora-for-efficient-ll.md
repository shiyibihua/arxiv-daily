---
layout: default
title: Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning
---

# Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning

**arXiv**: [2512.07374v1](https://arxiv.org/abs/2512.07374) | [PDF](https://arxiv.org/pdf/2512.07374.pdf)

**ä½œè€…**: Yezi Liu, Hanning Chen, Wenjun Huang, Yang Ni, Mohsen Imani

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR2Fæ¡†æž¶ï¼Œé€šè¿‡ä»ŽLoRAé‡å»ºæ¢¯åº¦å®žçŽ°é«˜æ•ˆLLMé—å¿˜å­¦ä¹ **

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `é—å¿˜å­¦ä¹ ` `LoRA` `æ¢¯åº¦é‡å»º` `æ¨¡åž‹è¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é—å¿˜å­¦ä¹ æ–¹æ³•éœ€å…¨æ¨¡åž‹å¾®è°ƒæˆ–åŽŸå§‹æ•°æ®ï¼Œé™åˆ¶å¯æ‰©å±•æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽLoRAå‚æ•°æ¢¯åº¦ï¼Œè®­ç»ƒè§£ç å™¨è¿‘ä¼¼å…¨æ¨¡åž‹æ¢¯åº¦æ–¹å‘
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»£ç†æ¨¡åž‹ä¸Šè®­ç»ƒè§£ç å™¨ï¼Œå¯è¿ç§»è‡³ç›®æ ‡æ¨¡åž‹ï¼Œä¿æŒæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

