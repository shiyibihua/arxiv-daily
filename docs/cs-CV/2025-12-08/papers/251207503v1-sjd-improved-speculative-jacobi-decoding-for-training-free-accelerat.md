---
layout: default
title: SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation
---

# SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation

**arXiv**: [2512.07503v1](https://arxiv.org/abs/2512.07503) | [PDF](https://arxiv.org/pdf/2512.07503.pdf)

**ä½œè€…**: Yao Teng, Zhihuan Jiang, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSJD++ä»¥åŠ é€Ÿè‡ªå›žå½’æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæ— éœ€è®­ç»ƒå³å¯å‡å°‘æŽ¨ç†å»¶è¿Ÿå’Œæ­¥éª¤ã€‚**

**å…³é”®è¯**: `è‡ªå›žå½’æ¨¡åž‹` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¹¶è¡Œè§£ç ` `æŽ¨æµ‹é‡‡æ ·` `Jacobiè§£ç ` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªå›žå½’æ¨¡åž‹ç”Ÿæˆå›¾åƒæ…¢ï¼Œéœ€å¤§é‡é¡ºåºå‰å‘ä¼ é€’é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚
2. SJD++ç»“åˆJacobiè§£ç çš„å¤šæ ‡è®°é¢„æµ‹å’ŒæŽ¨æµ‹é‡‡æ ·çš„è‰ç¨¿éªŒè¯æœºåˆ¶ï¼Œå®žçŽ°å¹¶è¡Œè§£ç ã€‚
3. å®žéªŒæ˜¾ç¤ºSJD++åœ¨å¤šä¸ªæ¨¡åž‹ä¸Šå®žçŽ°2-3å€å»¶è¿Ÿé™ä½Žå’Œ2-7å€æ­¥éª¤åŽ‹ç¼©ï¼Œè§†è§‰è´¨é‡æ— ä¸‹é™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

