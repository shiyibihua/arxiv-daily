---
layout: default
title: DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement
---

# DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement

**arXiv**: [2512.07253v1](https://arxiv.org/abs/2512.07253) | [PDF](https://arxiv.org/pdf/2512.07253.pdf)

**ä½œè€…**: Handing Xu, Zhenguo Nie, Tairan Peng, Huimin Pan, Xin-Jun Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDGGANï¼Œé€šè¿‡é€€åŒ–æ„ŸçŸ¥å»ºæ¨¡å®žçŽ°å®žæ—¶å†…çª¥é•œè§†é¢‘å¢žå¼º**

**å…³é”®è¯**: `å†…çª¥é•œè§†é¢‘å¢žå¼º` `é€€åŒ–æ„ŸçŸ¥å»ºæ¨¡` `å®žæ—¶å¤„ç†` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ` `å¯¹æ¯”å­¦ä¹ ` `å¾ªçŽ¯ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå†…çª¥é•œè§†é¢‘å› å…‰ç…§ä¸å‡ã€ç»„ç»‡æ•£å°„ç­‰é€€åŒ–å½±å“æ‰‹æœ¯å®‰å…¨ï¼ŒçŽ°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥å®žæ—¶åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æå–é€€åŒ–è¡¨ç¤ºï¼Œé€šè¿‡èžåˆæœºåˆ¶è°ƒåˆ¶å›¾åƒç‰¹å¾ï¼Œç»“åˆå¾ªçŽ¯ä¸€è‡´æ€§çº¦æŸè®­ç»ƒå•å¸§å¢žå¼ºæ¨¡åž‹ï¼Œå®žçŽ°è·¨å¸§é€€åŒ–ä¼ æ’­ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ€§èƒ½ä¸Žæ•ˆçŽ‡é—´å–å¾—ä¼˜è¶Šå¹³è¡¡ï¼ŒéªŒè¯äº†é€€åŒ–æ„ŸçŸ¥å»ºæ¨¡å¯¹å®žæ—¶å¢žå¼ºçš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸´åºŠåº”ç”¨æä¾›å¯è¡Œè·¯å¾„ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

