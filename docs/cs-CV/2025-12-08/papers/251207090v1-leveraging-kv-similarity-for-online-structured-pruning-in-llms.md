---
layout: default
title: Leveraging KV Similarity for Online Structured Pruning in LLMs
---

# Leveraging KV Similarity for Online Structured Pruning in LLMs

**arXiv**: [2512.07090v1](https://arxiv.org/abs/2512.07090) | [PDF](https://arxiv.org/pdf/2512.07090.pdf)

**ä½œè€…**: Jungmin Lee, Gwangeun Byeon, Yulhwa Kim, Seokin Hong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToken Filteringåœ¨çº¿ç»“æž„åŒ–å‰ªæžæ–¹æ³•ï¼Œåˆ©ç”¨KVç›¸ä¼¼æ€§åœ¨LLMæŽ¨ç†ä¸­è·³è¿‡å†—ä½™è®¡ç®—ä»¥åŠ é€Ÿã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å‰ªæž` `åœ¨çº¿æŽ¨ç†åŠ é€Ÿ` `é”®å€¼ç›¸ä¼¼æ€§` `ç»“æž„åŒ–å‰ªæž` `æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰LLMå‰ªæžæ–¹æ³•ä¾èµ–ç¦»çº¿æ ¡å‡†æ•°æ®ï¼Œå¯¼è‡´è·¨è¾“å…¥ä¸ç¨³å®šã€‚
2. åŸºäºŽé”®å€¼ç›¸ä¼¼æ€§åœ¨çº¿è¯„ä¼°tokenå†—ä½™ï¼Œè‡ªé€‚åº”èžåˆç­–ç•¥å¢žå¼ºç¨³å®šæ€§ï¼Œæ— é¢å¤–å†…å­˜å¼€é”€ã€‚
3. åœ¨LLaMAå’ŒMistralæ¨¡åž‹ä¸Šå®žéªŒï¼Œ50%å‰ªæžä¸‹ä¿æŒMMLUç­‰ä»»åŠ¡æ€§èƒ½ï¼Œä¼˜äºŽå…ˆå‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

