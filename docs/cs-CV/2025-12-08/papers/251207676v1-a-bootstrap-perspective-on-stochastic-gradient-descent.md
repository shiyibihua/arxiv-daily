---
layout: default
title: A Bootstrap Perspective on Stochastic Gradient Descent
---

# A Bootstrap Perspective on Stochastic Gradient Descent

**arXiv**: [2512.07676v1](https://arxiv.org/abs/2512.07676) | [PDF](https://arxiv.org/pdf/2512.07676.pdf)

**ä½œè€…**: Hongjian Lan, Yucong Liu, Florian SchÃ¤fer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç»Ÿè®¡è‡ªåŠ©æ³•çš„è§†è§’è§£é‡ŠSGDæ³›åŒ–ä¼˜åŠ¿ï¼Œé€šè¿‡æ¢¯åº¦åæ–¹å·®çŸ©é˜µæ­£åˆ™åŒ–æŽ§åˆ¶ç®—æ³•å˜å¼‚æ€§ã€‚**

**å…³é”®è¯**: `éšæœºæ¢¯åº¦ä¸‹é™` `æ³›åŒ–èƒ½åŠ›` `ç»Ÿè®¡è‡ªåŠ©æ³•` `æ¢¯åº¦åæ–¹å·®` `ç®—æ³•å˜å¼‚æ€§` `æ­£åˆ™åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSGDç›¸æ¯”ç¡®å®šæ€§æ¢¯åº¦ä¸‹é™ä¸ºä½•èƒ½æå‡æœºå™¨å­¦ä¹ æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†SGDè§†ä¸ºæ•°æ®æ”¶é›†éšæœºæ€§çš„è‡ªåŠ©æ³•ä»£ç†ï¼Œæ­£åˆ™åŒ–æ¢¯åº¦åæ–¹å·®çŸ©é˜µè¿¹ä»¥é™ä½Žé‡‡æ ·å™ªå£°æ•æ„Ÿæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç»éªŒé£Žé™©æœ€å°åŒ–ä¸­éªŒè¯SGDé¿å…ä¼ªè§£ï¼Œç¥žç»ç½‘ç»œè®­ç»ƒä¸­æ˜¾å¼æ­£åˆ™åŒ–æå‡æµ‹è¯•æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.

