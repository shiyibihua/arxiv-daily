---
layout: default
title: Provable Long-Range Benefits of Next-Token Prediction
---

# Provable Long-Range Benefits of Next-Token Prediction

**arXiv**: [2512.07818v1](https://arxiv.org/abs/2512.07818) | [PDF](https://arxiv.org/pdf/2512.07818.pdf)

**ä½œè€…**: Xinyuan Cao, Santosh S. Vempala

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯æ˜ŽRNNä¸­ä¸‹ä¸€è¯é¢„æµ‹èƒ½å­¦ä¹ é•¿ç¨‹ç»“æž„ï¼Œå®žçŽ°kè¯ä¸å¯åŒºåˆ†æ€§ã€‚**

**å…³é”®è¯**: `ä¸‹ä¸€è¯é¢„æµ‹` `é•¿ç¨‹ç»“æž„å­¦ä¹ ` `RNNç†è®ºåˆ†æž` `kè¯ä¸å¯åŒºåˆ†æ€§` `è¯­è¨€æ¨¡åž‹ç†è®º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§£é‡Šè¯­è¨€æ¨¡åž‹é€šè¿‡ä¸‹ä¸€è¯é¢„æµ‹å¦‚ä½•ç”Ÿæˆè¿žè´¯æ–‡æ¡£å¹¶æ•èŽ·é•¿ç¨‹ç»“æž„ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç†è®ºè¯æ˜ŽRNNä¼˜åŒ–ä¸‹ä¸€è¯é¢„æµ‹å¯è¿‘ä¼¼è®­ç»ƒåˆ†å¸ƒï¼Œå®žçŽ°kè¯ä¸å¯åŒºåˆ†æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæä¾›å¤šé¡¹å¼æ¨¡åž‹å¤§å°ç•Œé™ï¼Œè§£é‡Šå®žè·µä¸­è§‚å¯Ÿåˆ°çš„é•¿ç¨‹ä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.

