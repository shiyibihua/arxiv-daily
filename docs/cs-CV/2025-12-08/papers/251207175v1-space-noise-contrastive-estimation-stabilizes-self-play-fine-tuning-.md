---
layout: default
title: SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models
---

# SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models

**arXiv**: [2512.07175v1](https://arxiv.org/abs/2512.07175) | [PDF](https://arxiv.org/pdf/2512.07175.pdf)

**ä½œè€…**: Yibo Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Lijun Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPACEæ–¹æ³•ï¼Œé€šè¿‡å™ªå£°å¯¹æ¯”ä¼°è®¡ç¨³å®šè‡ªåšå¼ˆå¾®è°ƒï¼Œè§£å†³ç›®æ ‡é€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `è‡ªåšå¼ˆå¾®è°ƒ` `å™ªå£°å¯¹æ¯”ä¼°è®¡` `å¤§è¯­è¨€æ¨¡åž‹` `ç¨³å®šæ”¶æ•›` `åˆ†å¸ƒå¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è‡ªåšå¼ˆå¾®è°ƒæ–¹æ³•åŸºäºŽå¥–åŠ±å·®è·ï¼Œå¿½ç•¥ç»å¯¹å€¼ï¼Œå¯¼è‡´ç›®æ ‡é€€åŒ–ä¸Žä¸ç¨³å®šæ¼”åŒ–
2. SPACEå°†åˆæˆæ ·æœ¬è§†ä¸ºè¾…åŠ©ï¼Œä»¥äºŒå…ƒåˆ†ç±»åŒºåˆ†çœŸå®žæ ·æœ¬ï¼Œç‹¬ç«‹ä¼˜åŒ–ç»å¯¹å¥–åŠ±å€¼
3. å®žéªŒæ˜¾ç¤ºSPACEåœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒä¸Žå·®è·æ–¹æ³•ï¼Œç¡®ä¿ç¨³å®šæ”¶æ•›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.

