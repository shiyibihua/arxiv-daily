---
layout: default
title: In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models
---

# In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models

**arXiv**: [2512.07705v1](https://arxiv.org/abs/2512.07705) | [PDF](https://arxiv.org/pdf/2512.07705.pdf)

**ä½œè€…**: Saroj Gopali, Bipin Chhetri, Deepika Giri, Sima Siami-Namini, Akbar Siami Namin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§è¯­è¨€æ¨¡åž‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ä¸Šä¸‹æ–‡ä¸Žå°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `å¤§è¯­è¨€æ¨¡åž‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å°‘æ ·æœ¬å­¦ä¹ ` `åŸºç¡€æ¨¡åž‹` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¯”è¾ƒå¤§è¯­è¨€æ¨¡åž‹ä¸Žä¼ ç»Ÿæ–¹æ³•åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ è®­ç»ƒLLMï¼Œå¹¶å¯¹æ¯”TimesFMã€TCNå’ŒLSTM
3. å®žéªŒæˆ–æ•ˆæžœï¼šTimesFMè¡¨çŽ°æœ€ä½³ï¼ŒRMSEæœ€ä½Žä¸º0.3023ï¼ŒæŽ¨ç†æ—¶é—´266ç§’

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
>   This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
>   These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.

