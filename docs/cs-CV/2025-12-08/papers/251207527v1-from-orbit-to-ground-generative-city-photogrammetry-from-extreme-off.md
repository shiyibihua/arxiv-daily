---
layout: default
title: From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images
---

# From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images

**arXiv**: [2512.07527v1](https://arxiv.org/abs/2512.07527) | [PDF](https://arxiv.org/pdf/2512.07527.pdf)

**ä½œè€…**: Fei Yu, Yu Liu, Luyang Tang, Mingchao Sun, Zengye Ge, Rui Bu, Yuchao Jin, Haisen Zhao, He Sun, Yangyan Li, Mu Xu, Wenzheng Chen, Baoquan Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽ2.5Dé«˜åº¦å›¾å’Œç”Ÿæˆçº¹ç†æ¢å¤çš„æ–¹æ³•ï¼Œä»¥è§£å†³ä»Žæžç«¯ç¦»å¤©åº•å«æ˜Ÿå›¾åƒåˆæˆåœ°é¢çº§åŸŽå¸‚è§†å›¾çš„æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `åŸŽå¸‚3Dé‡å»º` `å«æ˜Ÿå›¾åƒå¤„ç†` `ç”Ÿæˆçº¹ç†æ¢å¤` `2.5Dé«˜åº¦å›¾` `å¯å¾®åˆ†æ¸²æŸ“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žç¨€ç–ã€è§†è§’æžç«¯çš„å«æ˜Ÿå›¾åƒè¿›è¡ŒåŸŽå¸‚å°ºåº¦3Dé‡å»ºï¼Œéœ€æŽ¨æ–­è¿‘90åº¦è§†è§’å·®è·ï¼Œå¯¼è‡´çŽ°æœ‰æ–¹æ³•å¦‚NeRFå’Œ3DGSå¤±æ•ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºæ¨¡åŸŽå¸‚å‡ ä½•ä¸º2.5Dé«˜åº¦å›¾ï¼Œä½¿ç”¨Zå•è°ƒç¬¦å·è·ç¦»åœºç¨³å®šä¼˜åŒ–ï¼›é€šè¿‡å¯å¾®åˆ†æ¸²æŸ“å’Œç”Ÿæˆç½‘ç»œæ¢å¤é«˜é¢‘çŽ‡çº¹ç†ç»†èŠ‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤§è§„æ¨¡åŸŽå¸‚é‡å»ºå®žéªŒä¸­ï¼Œä»Žå°‘é‡å«æ˜Ÿå›¾åƒé‡å»º4å¹³æ–¹å…¬é‡ŒåŒºåŸŸï¼Œåˆæˆé€¼çœŸåœ°é¢è§†å›¾ï¼Œæ€§èƒ½è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
>   To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
>   Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

