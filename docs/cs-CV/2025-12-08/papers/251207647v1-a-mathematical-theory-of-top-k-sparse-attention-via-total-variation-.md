---
layout: default
title: A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance
---

# A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance

**arXiv**: [2512.07647v1](https://arxiv.org/abs/2512.07647) | [PDF](https://arxiv.org/pdf/2512.07647.pdf)

**ä½œè€…**: Georgios Tzachristas, Lei Deng, Ioannis Tzachristas, Gong Zhang, Renhai Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ€»å˜å·®è·ç¦»çš„Top-kç¨€ç–æ³¨æ„åŠ›æ•°å­¦ç†è®ºï¼Œé‡åŒ–æˆªæ–­è¯¯å·®å¹¶æä¾›ç¡®å®šæ€§è¾¹ç•Œã€‚**

**å…³é”®è¯**: `æ³¨æ„åŠ›æœºåˆ¶` `ç¨€ç–æ³¨æ„åŠ›` `æ€»å˜å·®è·ç¦»` `æ•°å­¦ç†è®º` `è¯¯å·®åˆ†æž` `Top-kæˆªæ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTop-kæ³¨æ„åŠ›æˆªæ–­çš„è¿‘ä¼¼è¯¯å·®ç¼ºä¹ç»Ÿä¸€æ•°å­¦æ¡†æž¶ä¸Žç¡®å®šæ€§è¾¹ç•Œã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ€»å˜å·®è·ç¦»é‡åŒ–åˆ†å¸ƒä¸Žè¾“å‡ºè¯¯å·®ï¼ŒæŽ¨å¯¼éžæ¸è¿‘è¾¹ç•ŒåŠå¤´å°¾åˆ†è§£å…¬å¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨BERTå’Œåˆæˆæ•°æ®ä¸ŠéªŒè¯ç†è®ºï¼Œå¹³å‡å‡å°‘2-4å€é”®å€¼è®¡ç®—å¹¶æ»¡è¶³è¯¯å·®é¢„ç®—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\\|_2=Ï„\\|Î¼_{\mathrm{tail}}-Î¼_{\mathrm{head}}\\|_2$ with $Ï„=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\\|_2\leÏ„\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(Î¼,Ïƒ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxÎ¦_c(Ïƒ+Î¦^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.

