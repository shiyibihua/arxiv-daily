---
layout: default
title: ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation
---

# ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation

**arXiv**: [2512.07328v1](https://arxiv.org/abs/2512.07328) | [PDF](https://arxiv.org/pdf/2512.07328.pdf)

**ä½œè€…**: Ziyang Mai, Yu-Wing Tai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºContextAnyoneæ¡†æž¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰©æ•£å®žçŽ°åŸºäºŽæ–‡æœ¬å’Œå•å‚è€ƒå›¾åƒçš„å­—ç¬¦ä¸€è‡´è§†é¢‘ç”Ÿæˆã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ` `å­—ç¬¦ä¸€è‡´æ€§` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰©æ•£` `å‚è€ƒå›¾åƒé‡å»º` `Emphasize-Attentionæ¨¡å—` `Gap-RoPEä½ç½®åµŒå…¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•éš¾ä»¥ä¿æŒè·¨åœºæ™¯çš„å­—ç¬¦èº«ä»½ä¸€è‡´æ€§ï¼Œå¦‚å‘åž‹ã€æœè£…å’Œä½“åž‹ç­‰ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå‚è€ƒå›¾åƒé‡å»ºå’Œæ–°å¸§ç”Ÿæˆï¼Œé‡‡ç”¨Emphasize-Attentionæ¨¡å—å’ŒGap-RoPEä½ç½®åµŒå…¥ï¼Œå¢žå¼ºå‚è€ƒæ„ŸçŸ¥å¹¶ç¨³å®šæ—¶åºå»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨èº«ä»½ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ä¸Šä¼˜äºŽçŽ°æœ‰å‚è€ƒåˆ°è§†é¢‘æ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·åŠ¨ä½œå’Œåœºæ™¯ä¸‹çš„è¿žè´¯å­—ç¬¦è§†é¢‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

