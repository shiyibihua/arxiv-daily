---
layout: default
title: ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation
---

# ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation

**arXiv**: [2512.07720v1](https://arxiv.org/abs/2512.07720) | [PDF](https://arxiv.org/pdf/2512.07720.pdf)

**ä½œè€…**: Fan Yang, Heyuan Li, Peihao Li, Weihao Yuan, Lingteng Qiu, Chaoyue Song, Cheng Chen, Yisheng He, Shifeng Zhang, Xiaoguang Han, Steven Hoi, Guosheng Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViSAæ¡†æž¶ï¼Œç»“åˆ3Dé‡å»ºä¸Žè§†é¢‘ç”Ÿæˆï¼Œå®žçŽ°å®žæ—¶é«˜ä¿çœŸä¸Šèº«è™šæ‹Ÿäººåˆ›å»ºã€‚**

**å…³é”®è¯**: `3Dè™šæ‹Ÿäººç”Ÿæˆ` `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `å®žæ—¶æ¸²æŸ“` `ä¸Šèº«è™šæ‹Ÿäºº` `ç»“æž„å…ˆéªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•å›¾åƒç”Ÿæˆä¸Šèº«3Dè™šæ‹Ÿäººå­˜åœ¨çº¹ç†æ¨¡ç³Šã€è¿åŠ¨åƒµç¡¬æˆ–ç»“æž„ä¸ç¨³å®šé—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨3Dé‡å»ºæ¨¡åž‹æä¾›ç»“æž„å…ˆéªŒï¼Œå¼•å¯¼å®žæ—¶è‡ªå›žå½’è§†é¢‘æ‰©æ•£æ¨¡åž‹æ¸²æŸ“ç»†èŠ‚ä¸ŽåŠ¨æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ˜¾è‘—å‡å°‘ä¼ªå½±ï¼Œæå‡è§†è§‰è´¨é‡ï¼Œé€‚ç”¨äºŽæ¸¸æˆå’Œè™šæ‹ŸçŽ°å®žç­‰å®žæ—¶åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

