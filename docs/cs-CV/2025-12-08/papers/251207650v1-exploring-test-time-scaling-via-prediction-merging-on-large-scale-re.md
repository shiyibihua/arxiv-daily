---
layout: default
title: Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation
---

# Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation

**arXiv**: [2512.07650v1](https://arxiv.org/abs/2512.07650) | [PDF](https://arxiv.org/pdf/2512.07650.pdf)

**ä½œè€…**: Fuyuan Lyu, Zhentai Chen, Jingyan Jiang, Lingjie Li, Xing Tang, Xiuqiang He, Xue Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹åˆå¹¶æå‡å¤§è§„æ¨¡æŽ¨èç³»ç»Ÿæ•ˆçŽ‡**

**å…³é”®è¯**: `æµ‹è¯•æ—¶ç¼©æ”¾` `é¢„æµ‹åˆå¹¶` `å¤§è§„æ¨¡æŽ¨èç³»ç»Ÿ` `æ¨¡åž‹å¼‚è´¨æ€§` `è®¡ç®—æ•ˆçŽ‡` `åœ¨çº¿æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•åœ¨æµ‹è¯•æ—¶é«˜æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºï¼Œæ›¿ä»£ä¼ ç»Ÿè®­ç»ƒæ—¶å‚æ•°ç¼©æ”¾ï¼Œä»¥æå‡æŽ¨èç³»ç»Ÿæ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ¨¡åž‹æž¶æž„å¼‚è´¨æ€§æˆ–åŒæž„åˆå§‹åŒ–éšæœºæ€§ç”Ÿæˆå¤šæ ·åŒ–é¢„æµ‹ï¼Œå¹¶é€šè¿‡åˆå¹¶å¢žå¼ºè¾“å‡º
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°å…«ç§æ¨¡åž‹ï¼Œè¯æ˜Žæ–¹æ³•æœ‰æ•ˆä¸”ä¼˜äºŽå‚æ•°ç¼©æ”¾ï¼Œæ”¯æŒåœ¨çº¿å¹¶è¡ŒåŠ é€Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.

