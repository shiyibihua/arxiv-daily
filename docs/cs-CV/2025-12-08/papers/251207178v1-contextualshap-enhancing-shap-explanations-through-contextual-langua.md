---
layout: default
title: ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation
---

# ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation

**arXiv**: [2512.07178v1](https://arxiv.org/abs/2512.07178) | [PDF](https://arxiv.org/pdf/2512.07178.pdf)

**ä½œè€…**: Latifa Dwiyanti, Sergio Ryan Wibisono, Hidetaka Nambo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºContextualSHAPåŒ…ï¼Œé€šè¿‡é›†æˆGPTç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬è§£é‡Šï¼Œä»¥å¢žå¼ºSHAPåœ¨éžæŠ€æœ¯ç”¨æˆ·ä¸­çš„å¯ç†è§£æ€§ã€‚**

**å…³é”®è¯**: `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `SHAPè§£é‡Š` `å¤§è¯­è¨€æ¨¡åž‹é›†æˆ` `ä¸Šä¸‹æ–‡ç”Ÿæˆ` `ç”¨æˆ·è¯„ä¼°` `åŒ»ç–—åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. SHAPè§£é‡Šç¼ºä¹å¯¹éžæŠ€æœ¯ç”¨æˆ·çš„ä¸Šä¸‹æ–‡æ„ä¹‰ï¼Œå¯¼è‡´ç†è§£å›°éš¾ã€‚
2. æ–¹æ³•ç»“åˆSHAPä¸ŽGPTï¼ŒåŸºäºŽç”¨æˆ·å‚æ•°ç”Ÿæˆå®šåˆ¶åŒ–æ–‡æœ¬è§£é‡Šã€‚
3. åœ¨åŒ»ç–—æ¡ˆä¾‹ä¸­ï¼Œç”¨æˆ·è¯„ä¼°æ˜¾ç¤ºç”Ÿæˆè§£é‡Šæ¯”çº¯è§†è§‰è¾“å‡ºæ›´æ˜“ç†è§£å’Œåˆé€‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

