---
layout: default
title: Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation
---

# Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation

**arXiv**: [2512.07472v1](https://arxiv.org/abs/2512.07472) | [PDF](https://arxiv.org/pdf/2512.07472.pdf)

**ä½œè€…**: Siyu Xu, Zijian Wang, Yunke Wang, Chenghao Xia, Tao Huang, Chang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAffordance Field Interventionï¼Œé€šè¿‡3Dç©ºé—´å¯æ“ä½œåœºå¼•å¯¼VLAæ¨¡åž‹ï¼Œä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„è®°å¿†é™·é˜±é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æœºå™¨äººæ“ä½œ` `3Dç©ºé—´å¯æ“ä½œåœº` `åˆ†å¸ƒåç§»` `è®°å¿†é™·é˜±` `è½»é‡çº§æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨åˆ†å¸ƒåç§»ä¸‹æ˜“é™·å…¥è®°å¿†é™·é˜±ï¼Œé‡å¤è®°å¿†è½¨è¿¹è€Œéžé€‚åº”æ–°åœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨3Dç©ºé—´å¯æ“ä½œåœºä½œä¸ºè½»é‡çº§æ’ä»¶ï¼Œæ£€æµ‹è®°å¿†é™·é˜±å¹¶ç”Ÿæˆå¯æ“ä½œé©±åŠ¨çš„è·¯å¾„ç‚¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæœºå™¨äººå¹³å°å’ŒLIBERO-ProåŸºå‡†ä¸Šï¼Œå¹³å‡æ€§èƒ½æå‡23.5%å’Œ20.2%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($Ï€_{0}$ and $Ï€_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.

