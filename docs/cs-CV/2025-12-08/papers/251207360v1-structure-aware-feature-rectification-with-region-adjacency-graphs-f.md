---
layout: default
title: Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation
---

# Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation

**arXiv**: [2512.07360v1](https://arxiv.org/abs/2512.07360) | [PDF](https://arxiv.org/pdf/2512.07360.pdf)

**ä½œè€…**: Qiming Huang, Hao Ai, Jianbo Jiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŒºåŸŸé‚»æŽ¥å›¾çš„ç»“æž„æ„ŸçŸ¥ç‰¹å¾æ ¡æ­£æ–¹æ³•ï¼Œä»¥æå‡æ— è®­ç»ƒå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å±€éƒ¨ä¸€è‡´æ€§ã€‚**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²` `ç»“æž„æ„ŸçŸ¥ç‰¹å¾æ ¡æ­£` `åŒºåŸŸé‚»æŽ¥å›¾` `æ— è®­ç»ƒæ–¹æ³•` `å±€éƒ¨ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPæ¨¡åž‹åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­å› å…¨å±€è¯­ä¹‰å¯¹é½å¯¼è‡´å±€éƒ¨åŒºåŸŸé¢„æµ‹å™ªå£°å’Œä¸ä¸€è‡´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨ä½Žå±‚ç‰¹å¾æž„å»ºåŒºåŸŸé‚»æŽ¥å›¾ï¼Œæ•èŽ·å±€éƒ¨ç»“æž„å…³ç³»ï¼Œæ ¡æ­£CLIPç‰¹å¾ä»¥å¢žå¼ºå±€éƒ¨åˆ¤åˆ«åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå¼€æ”¾è¯æ±‡åˆ†å‰²åŸºå‡†ä¸Šæœ‰æ•ˆæŠ‘åˆ¶å™ªå£°ï¼Œæå‡åŒºåŸŸä¸€è‡´æ€§ï¼Œå®žçŽ°å¼ºæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

