---
layout: default
title: ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning
---

# ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning

**arXiv**: [2512.07371v1](https://arxiv.org/abs/2512.07371) | [PDF](https://arxiv.org/pdf/2512.07371.pdf)

**ä½œè€…**: Byungju Kim, Jinu Pahk, Chungwoo Lee, Jaejoon Kim, Jangha Lee, Theo Taeyeong Kim, Kyuhwan Shim, Jun Ki Lee, Byoung-Tak Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºESPADAæ¡†æž¶ï¼Œé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥æ¼”ç¤ºæ•°æ®ä¸‹é‡‡æ ·åŠ é€Ÿæ¨¡ä»¿å­¦ä¹ ä¸­çš„æœºå™¨äººæŽ§åˆ¶ã€‚**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `è¡Œä¸ºå…‹éš†` `æ•°æ®ä¸‹é‡‡æ ·` `è¯­ä¹‰åˆ†å‰²` `æœºå™¨äººæŽ§åˆ¶` `åŠ¨æ€æ—¶é—´è§„æ•´`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¡Œä¸ºå…‹éš†ç­–ç•¥ç»§æ‰¿äººç±»æ¼”ç¤ºçš„ç¼“æ…¢èŠ‚å¥ï¼Œé™åˆ¶å®žé™…éƒ¨ç½²ã€‚
2. ESPADAåˆ©ç”¨VLM-LLMç®¡é“å’Œ3Då¤¹çˆª-ç‰©ä½“å…³ç³»è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œä»…åœ¨éžå…³é”®æ®µè¿›è¡Œæ¿€è¿›ä¸‹é‡‡æ ·ã€‚
3. åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žå®žéªŒä¸­ï¼ŒESPADAå®žçŽ°çº¦2å€åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒæˆåŠŸçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.

