---
layout: default
title: Forget and Explain: Transparent Verification of GNN Unlearning
---

# Forget and Explain: Transparent Verification of GNN Unlearning

**arXiv**: [2512.07450v1](https://arxiv.org/abs/2512.07450) | [PDF](https://arxiv.org/pdf/2512.07450.pdf)

**ä½œè€…**: Imran Ahsan, Hyunwook Yu, Jinsung Kim, Mucheol Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¯è§£é‡Šæ€§çš„éªŒè¯å™¨ä»¥é€æ˜ŽéªŒè¯å›¾ç¥žç»ç½‘ç»œé—å¿˜æ•ˆæžœ**

**å…³é”®è¯**: `å›¾ç¥žç»ç½‘ç»œé—å¿˜` `å¯è§£é‡Šæ€§éªŒè¯` `éšç§ä¿æŠ¤` `å½’å› åˆ†æž` `å›¾ç¼–è¾‘è·ç¦»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾ç¥žç»ç½‘ç»œé—å¿˜ç¼ºä¹é€æ˜Žåº¦ï¼Œéš¾ä»¥éªŒè¯ä¿¡æ¯æ˜¯å¦çœŸæ­£åˆ é™¤
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å½’å› åç§»å’Œå±€éƒ¨ç»“æž„å˜åŒ–ä½œä¸ºé€æ˜Žè¯æ®ï¼Œå®šä¹‰äº”ç§å¯è§£é‡Šæ€§æŒ‡æ ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°å¤šç§é—å¿˜ç­–ç•¥ï¼Œç»“æžœæ˜¾ç¤ºRetrainå’ŒGNNDeleteæŽ¥è¿‘å®Œå…¨é—å¿˜ï¼Œè§£é‡Šå·®å¼‚æä¾›ä¸»è¦è¯æ®

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.

