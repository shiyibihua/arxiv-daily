---
layout: default
title: Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models
---

# Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models

**arXiv**: [2512.07141v1](https://arxiv.org/abs/2512.07141) | [PDF](https://arxiv.org/pdf/2512.07141.pdf)

**ä½œè€…**: Fenghua Weng, Chaochao Lu, Xia Hu, Wenqi Shao, Wenjie Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThink-Reflect-Reviseæ¡†æž¶ä»¥å¢žå¼ºå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹çš„å®‰å…¨å¯¹é½èƒ½åŠ›**

**å…³é”®è¯**: `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `å®‰å…¨å¯¹é½` `è‡ªæˆ‘åæ€` `å¼ºåŒ–å­¦ä¹ ` `è¶Šç‹±æ”»å‡»` `å¤šæ¨¡æ€æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•æ¬¡æŽ¨ç†èŒƒå¼æ˜“å—ä¸Šä¸‹æ–‡æˆ–è§†è§‰è¶Šç‹±æ”»å‡»ï¼Œå¯èƒ½å¿½ç•¥è‡ªèº«è¾“å‡ºä¸­çš„æœ‰å®³å†…å®¹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«5000ä¸ªç¤ºä¾‹çš„ReSafeæ•°æ®é›†ï¼Œé€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒï¼ˆæ•°æ®é›†å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ï¼‰å¼•å¯¼æ¨¡åž‹è¿›è¡Œç­–ç•¥æŒ‡å¯¼çš„è‡ªæˆ‘åæ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Qwen2.5-VL-7Bä¸Šï¼Œå®‰å…¨å“åº”çŽ‡ä»Ž42.8%æå‡è‡³87.7%ï¼ŒåŒæ—¶ä¿æŒé€šç”¨åŸºå‡†æ€§èƒ½ç¨³å®šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

