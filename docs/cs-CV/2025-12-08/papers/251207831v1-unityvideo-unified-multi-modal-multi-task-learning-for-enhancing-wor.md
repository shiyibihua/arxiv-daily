---
layout: default
title: UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation
---

# UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation

**arXiv**: [2512.07831v1](https://arxiv.org/abs/2512.07831) | [PDF](https://arxiv.org/pdf/2512.07831.pdf)

**ä½œè€…**: Jiehui Huang, Yuechen Zhang, Xu He, Yuan Gao, Zhi Cen, Bin Xia, Yan Zhou, Xin Tao, Pengfei Wan, Jiaya Jia

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUnityVideoç»Ÿä¸€å¤šæ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ æ¡†æž¶ï¼Œä»¥å¢žå¼ºä¸–ç•Œæ„ŸçŸ¥è§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆ` `ä¸–ç•Œæ„ŸçŸ¥å­¦ä¹ ` `ç»Ÿä¸€è®­ç»ƒæ¡†æž¶` `é›¶æ ·æœ¬æ³›åŒ–` `åŠ¨æ€å™ªå£°` `ä¸Šä¸‹æ–‡å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡åž‹å—é™äºŽå•æ¨¡æ€æ¡ä»¶ï¼Œç¼ºä¹è·¨æ¨¡æ€äº¤äº’å’Œæ¨¡æ€å¤šæ ·æ€§ï¼Œå½±å“ä¸–ç•Œç†è§£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŠ¨æ€å™ªå£°ç»Ÿä¸€å¼‚æž„è®­ç»ƒèŒƒå¼ï¼Œç»“åˆæ¨¡æ€åˆ‡æ¢å™¨å’Œä¸Šä¸‹æ–‡å­¦ä¹ å™¨å®žçŽ°å¤šæ¨¡æ€ç»Ÿä¸€å¤„ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€šè¿‡è”åˆä¼˜åŒ–åŠ é€Ÿæ”¶æ•›ï¼Œæå‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ”¹å–„è§†é¢‘è´¨é‡å’Œç‰©ç†ä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

