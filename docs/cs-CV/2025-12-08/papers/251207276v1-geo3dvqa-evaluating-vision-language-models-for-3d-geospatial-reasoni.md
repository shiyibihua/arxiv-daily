---
layout: default
title: Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery
---

# Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery

**arXiv**: [2512.07276v1](https://arxiv.org/abs/2512.07276) | [PDF](https://arxiv.org/pdf/2512.07276.pdf)

**ä½œè€…**: Mai Tsujimoto, Junjue Wang, Weihao Xuan, Naoto Yokoya

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGeo3DVQAåŸºå‡†ï¼Œè¯„ä¼°ä»…ç”¨RGBé¥æ„Ÿå›¾åƒçš„è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ä¸‰ç»´åœ°ç†ç©ºé—´æŽ¨ç†ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `ä¸‰ç»´åœ°ç†ç©ºé—´æŽ¨ç†` `è§†è§‰è¯­è¨€æ¨¡åž‹è¯„ä¼°` `é¥æ„Ÿå›¾åƒåˆ†æž` `åŸºå‡†æ•°æ®é›†` `é¢†åŸŸé€‚åº”` `RGBå›¾åƒå¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µä¼ æ„Ÿå™¨ï¼Œéš¾ä»¥æ•´åˆå¤š3Dçº¿ç´¢å¤„ç†å¤šæ ·åŒ–æŸ¥è¯¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«11ä¸‡é—®ç­”å¯¹çš„åŸºå‡†ï¼Œæ¶µç›–16ä¸ªä»»åŠ¡ç±»åˆ«å’Œä¸‰ä¸ªå¤æ‚åº¦çº§åˆ«ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°10ä¸ªå…ˆè¿›VLMï¼ŒGPT-4oå’ŒGemini-2.5-Flashå‡†ç¡®çŽ‡ä½Žï¼Œé¢†åŸŸå¾®è°ƒQwen2.5-VL-7Bæå‡è‡³49.6%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

