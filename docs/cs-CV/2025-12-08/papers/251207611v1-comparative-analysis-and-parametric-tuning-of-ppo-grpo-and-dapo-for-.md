---
layout: default
title: Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement
---

# Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement

**arXiv**: [2512.07611v1](https://arxiv.org/abs/2512.07611) | [PDF](https://arxiv.org/pdf/2512.07611.pdf)

**ä½œè€…**: Yongsheng Lian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿæ¯”è¾ƒPPOã€GRPOå’ŒDAPOä¸‰ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥å¢žå¼ºå¤§è¯­è¨€æ¨¡åž‹çš„å¤æ‚æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹` `æŽ¨ç†å¢žå¼º` `å‚æ•°è°ƒä¼˜` `è½¬ç§»å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡åž‹åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æŽ§åˆ¶è½¬ç§»å­¦ä¹ è¯„ä¼°ï¼Œå…ˆåœ¨Countdown Gameä¸Šå¾®è°ƒï¼Œå†åœ¨é€šç”¨æŽ¨ç†åŸºå‡†ä¸Šæµ‹è¯•ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šRLè®­ç»ƒæ¨¡åž‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šä¼˜äºŽåŸºç¡€æ¨¡åž‹ï¼Œä½†æ”¹è¿›ç¨‹åº¦å› åŸºå‡†è€Œå¼‚ï¼›å‚æ•°åˆ†æžæä¾›å®žç”¨æŒ‡å¯¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
>   Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

