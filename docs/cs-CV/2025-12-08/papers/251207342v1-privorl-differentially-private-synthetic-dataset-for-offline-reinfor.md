---
layout: default
title: PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning
---

# PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning

**arXiv**: [2512.07342v1](https://arxiv.org/abs/2512.07342) | [PDF](https://arxiv.org/pdf/2512.07342.pdf)

**ä½œè€…**: Chen Gong, Zheng Liu, Kecen Li, Tianhao Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPrivORLæ–¹æ³•ï¼Œåˆ©ç”¨å·®åˆ†éšç§åˆæˆç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®é›†ä»¥ä¿æŠ¤éšç§ã€‚**

**å…³é”®è¯**: `å·®åˆ†éšç§` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é›†åˆæˆ` `æ‰©æ•£æ¨¡åž‹` `éšç§ä¿æŠ¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®é›†å­˜åœ¨éšç§æ³„éœ²é£Žé™©ï¼Œéœ€ä¿æŠ¤æ•æ„Ÿä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæ‰©æ•£æ¨¡åž‹å’Œæ‰©æ•£å˜æ¢å™¨ï¼Œåœ¨å·®åˆ†éšç§ä¸‹åˆæˆè¿‡æ¸¡å’Œè½¨è¿¹ï¼Œå¹¶å¼•å…¥å¥½å¥‡å¿ƒé©±åŠ¨é¢„è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªæ•æ„Ÿæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œç›¸æ¯”åŸºçº¿åœ¨æ•ˆç”¨å’Œä¿çœŸåº¦æ–¹é¢è¡¨çŽ°æ›´ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
>   To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.

