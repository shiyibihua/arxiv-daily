---
layout: default
title: PVeRA: Probabilistic Vector-Based Random Matrix Adaptation
---

# PVeRA: Probabilistic Vector-Based Random Matrix Adaptation

**arXiv**: [2512.07703v1](https://arxiv.org/abs/2512.07703) | [PDF](https://arxiv.org/pdf/2512.07703.pdf)

**ä½œè€…**: Leo Fillioux, Enzo Ferrante, Paul-Henry CournÃ¨de, Maria Vakalopoulou, Stergios Christodoulidis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPVeRAæ¦‚çŽ‡å‘é‡é€‚é…å™¨ï¼Œä»¥å¢žå¼ºåŸºç¡€æ¨¡åž‹åœ¨å°æ•°æ®é«˜æ•ˆé€‚åº”ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆé€‚åº”` `æ¦‚çŽ‡é€‚é…å™¨` `åŸºç¡€æ¨¡åž‹` `ä½Žç§©çŸ©é˜µ` `VTAB-1kåŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºç¡€æ¨¡åž‹é€‚åº”éœ€å¤§é‡æ•°æ®ä¸Žè®¡ç®—ï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚VeRAé€‚é…å™¨åœ¨å‚æ•°æ•ˆçŽ‡ä¸Šä»æœ‰å±€é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†VeRAçš„ä½Žç§©çŸ©é˜µä¿®æ”¹ä¸ºæ¦‚çŽ‡ç‰ˆæœ¬ï¼Œå¤„ç†è¾“å…¥æ¨¡ç³Šæ€§å¹¶æ”¯æŒè®­ç»ƒä¸Žæµ‹è¯•æ—¶çš„ä¸åŒé‡‡æ ·é…ç½®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VTAB-1kåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPVeRAä¼˜äºŽVeRAåŠå…¶ä»–é€‚é…å™¨ï¼Œä»£ç å·²å¼€æºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

