---
layout: default
title: PCMind-2.1-Kaiyuan-2B Technical Report
---

# PCMind-2.1-Kaiyuan-2B Technical Report

**arXiv**: [2512.07612v1](https://arxiv.org/abs/2512.07612) | [PDF](https://arxiv.org/pdf/2512.07612.pdf)

**ä½œè€…**: Kairong Luo, Zhenbo Sun, Xinyu Shi, Shengqi Chen, Bowen Yu, Yunyi Chen, Chenyi Dang, Hengtao Tao, Hui Wang, Fangming Liu, Kaifeng Lyu, Wenguang Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPCMind-2.1-Kaiyuan-2Bï¼Œé€šè¿‡æ•°æ®åŸºå‡†ã€é€‰æ‹©æ€§é‡å¤å’Œå¤šé¢†åŸŸè¯¾ç¨‹è®­ç»ƒï¼Œåœ¨èµ„æºå—é™ä¸‹æå‡å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡ä¸Žæ•ˆæžœã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `æ•°æ®åŸºå‡†` `é€‰æ‹©æ€§é‡å¤` `è¯¾ç¨‹è®­ç»ƒ` `å¼€æºæ¨¡åž‹` `èµ„æºå—é™è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€æºç¤¾åŒºä¸Žäº§ä¸šé—´å› é—­æºé«˜è´¨é‡æ•°æ®å’Œè®­ç»ƒæ–¹æ³•å­˜åœ¨çŸ¥è¯†é¸¿æ²Ÿï¼Œèµ„æºå—é™ä¸‹è®­ç»ƒæ•ˆçŽ‡ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†ä½æ•°æ•°æ®åŸºå‡†æ³•æ¯”è¾ƒå¼‚æž„æ•°æ®é›†ï¼Œå¤šé˜¶æ®µç­–ç•¥é€‰æ‹©æ€§é‡å¤åˆ©ç”¨ç¨€ç–é«˜è´¨é‡æ•°æ®ï¼Œå¤šé¢†åŸŸè¯¾ç¨‹è®­ç»ƒæŒ‰è´¨é‡æŽ’åºæ ·æœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹æ€§èƒ½ä¸Žé¡¶å°–å¼€æºæ¨¡åž‹ç«žäº‰ï¼Œæä¾›å¯æ‰©å±•çš„é¢„è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œæ‰€æœ‰èµ„äº§åœ¨Apache 2.0è®¸å¯ä¸‹å¼€æºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

