---
layout: default
title: OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing
---

# OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing

**arXiv**: [2512.07826v1](https://arxiv.org/abs/2512.07826) | [PDF](https://arxiv.org/pdf/2512.07826.pdf)

**ä½œè€…**: Haoyang He, Jie Wang, Jiangning Zhang, Zhucun Xue, Xingyuan Bu, Qiangpeng Yang, Shilei Wen, Lei Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenVE-3Mæ•°æ®é›†ä»¥è§£å†³æŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®çš„é—®é¢˜**

**å…³é”®è¯**: `æŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘` `å¤§è§„æ¨¡æ•°æ®é›†` `è§†é¢‘ç¼–è¾‘åŸºå‡†` `ç©ºé—´å¯¹é½ç¼–è¾‘` `éžç©ºé—´å¯¹é½ç¼–è¾‘` `è´¨é‡è¿‡æ»¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡æ•°æ®é›†ï¼Œé˜»ç¢æ¨¡åž‹å‘å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºOpenVE-3Mæ•°æ®é›†ï¼ŒåŒ…å«ç©ºé—´å¯¹é½å’Œéžç©ºé—´å¯¹é½ç¼–è¾‘ç±»åž‹ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç®¡é“å’Œè´¨é‡è¿‡æ»¤ç¡®ä¿è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºäºŽæ•°æ®é›†è®­ç»ƒOpenVE-Editæ¨¡åž‹ï¼Œåœ¨OpenVE-BenchåŸºå‡†ä¸Šè¶…è¶Šæ‰€æœ‰å¼€æºæ¨¡åž‹ï¼ŒåŒ…æ‹¬14BåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

