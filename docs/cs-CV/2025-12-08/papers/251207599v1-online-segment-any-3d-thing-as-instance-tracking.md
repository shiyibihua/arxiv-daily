---
layout: default
title: Online Segment Any 3D Thing as Instance Tracking
---

# Online Segment Any 3D Thing as Instance Tracking

**arXiv**: [2512.07599v1](https://arxiv.org/abs/2512.07599) | [PDF](https://arxiv.org/pdf/2512.07599.pdf)

**ä½œè€…**: Hanshi Wang, Zijian Cai, Jin Gao, Yiwei Zhang, Weiming Hu, Ke Wang, Zhipeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoSeg3Dæ–¹æ³•ï¼Œå°†åœ¨çº¿3Dåˆ†å‰²é‡æž„ä¸ºå®žä¾‹è·Ÿè¸ªé—®é¢˜ï¼Œä»¥å¢žå¼ºå…·èº«æ™ºèƒ½ä½“çš„åŠ¨æ€çŽ¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `åœ¨çº¿3Dåˆ†å‰²` `å®žä¾‹è·Ÿè¸ª` `æ—¶é—´ä¿¡æ¯ä¼ æ’­` `ç©ºé—´ä¸€è‡´æ€§å­¦ä¹ ` `å…·èº«æ™ºèƒ½ä½“` `ç‚¹äº‘å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºäºŽæŸ¥è¯¢çš„3Dåˆ†å‰²æ–¹æ³•å¿½è§†æ—¶é—´ç»´åº¦ï¼Œéš¾ä»¥å¤„ç†åŠ¨æ€çŽ¯å¢ƒä¸­çš„éƒ¨åˆ†å¯è§å¯¹è±¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¯¹è±¡æŸ¥è¯¢è¿›è¡Œæ—¶é—´ä¿¡æ¯ä¼ æ’­ï¼Œç»“åˆé•¿æœŸå…³è”å’ŒçŸ­æœŸæ›´æ–°ï¼Œå¹¶å¼•å…¥ç©ºé—´ä¸€è‡´æ€§å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ScanNet200ç­‰æ•°æ®é›†ä¸Šè¶…è¶ŠESAMï¼Œè¾¾åˆ°æ–°SOTAï¼Œæå‡åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

