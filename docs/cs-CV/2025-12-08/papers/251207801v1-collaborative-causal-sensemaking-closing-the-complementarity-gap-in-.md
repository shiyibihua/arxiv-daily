---
layout: default
title: Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support
---

# Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support

**arXiv**: [2512.07801v1](https://arxiv.org/abs/2512.07801) | [PDF](https://arxiv.org/pdf/2512.07801.pdf)

**ä½œè€…**: Raunak Jain, Mudita Khurana

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåä½œå› æžœæ„ŸçŸ¥æ¡†æž¶ä»¥è§£å†³äººæœºå†³ç­–æ”¯æŒä¸­çš„äº’è¡¥æ€§å·®è·é—®é¢˜**

**å…³é”®è¯**: `äººæœºåä½œå†³ç­–` `å› æžœæ„ŸçŸ¥` `AIä¼™ä¼´ç³»ç»Ÿ` `è®¤çŸ¥æ¨¡åž‹` `äº’è¡¥æ€§è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäººæœºå›¢é˜Ÿåœ¨å¤æ‚é«˜é£Žé™©å†³ç­–ä¸­å¸¸è¡¨çŽ°ä¸ä½³ï¼Œäº’è¡¥æ€§æœªå®žçŽ°ï¼Œä¸“å®¶åœ¨éªŒè¯ä¸Žè¿‡åº¦ä¾èµ–é—´æ‘‡æ‘†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†AIè®¾è®¡ä¸ºè®¤çŸ¥å·¥ä½œä¼™ä¼´ï¼Œå…±åŒæž„å»ºå’Œæµ‹è¯•å› æžœå‡è®¾ï¼Œå­¦ä¹ è”åˆå†³ç­–ç»“æžœä»¥ä¿ƒè¿›åŒæ–¹æ”¹è¿›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥ï¼Œä½†æå‡ºå›´ç»•è®­ç»ƒç”Ÿæ€ã€æ¨¡åž‹è¡¨ç¤ºå’Œä»¥ä¿¡ä»»ä¸Žäº’è¡¥æ€§ä¸ºä¸­å¿ƒçš„è¯„ä»·ç­‰æŒ‘æˆ˜æ–¹å‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

