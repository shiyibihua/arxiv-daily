---
layout: default
title: Auditing Games for Sandbagging
---

# Auditing Games for Sandbagging

**arXiv**: [2512.07810v1](https://arxiv.org/abs/2512.07810) | [PDF](https://arxiv.org/pdf/2512.07810.pdf)

**ä½œè€…**: Jordan Taylor, Sid Black, Dillon Bowen, Thomas Read, Satvik Golechha, Alex Zelenka-Martin, Oliver Makins, Connor Kissane, Kola Ayonrinde, Jacob Merizian, Samuel Marks, Chris Cundy, Joseph Bloom

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®¡è®¡æ¸¸æˆä»¥è¯„ä¼°AIç³»ç»Ÿåœ¨è¯„ä¼°ä¸­éšè—èƒ½åŠ›çš„æ£€æµ‹æ–¹æ³•**

**å…³é”®è¯**: `AIå®‰å…¨å®¡è®¡` `æ²™è¢‹è¡Œä¸ºæ£€æµ‹` `æ¨¡åž‹å¾®è°ƒ` `èƒ½åŠ›æ¿€å‘` `é»‘ç›’è¯„ä¼°` `çº¿æ€§æŽ¢é’ˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœªæ¥AIç³»ç»Ÿå¯èƒ½åœ¨è¯„ä¼°ä¸­éšè—èƒ½åŠ›ï¼ˆæ²™è¢‹è¡Œä¸ºï¼‰ï¼Œè¯¯å¯¼å¼€å‘è€…å’Œå®¡è®¡è€…
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡çº¢é˜Ÿå¾®è°ƒæ¨¡åž‹æ¨¡æ‹Ÿæ²™è¢‹è¡Œä¸ºï¼Œè“é˜Ÿä½¿ç”¨é»‘ç›’ã€æ¨¡åž‹å†…éƒ¨æˆ–åŸºäºŽè®­ç»ƒçš„æ–¹æ³•è¿›è¡Œæ£€æµ‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šè“é˜Ÿæ— æ³•å¯é åŒºåˆ†æ²™è¢‹æ¨¡åž‹ä¸Žè‰¯æ€§æ¨¡åž‹ï¼ŒåŸºäºŽè®­ç»ƒçš„æ¿€å‘æ–¹æ³•èƒ½æå‡æ€§èƒ½ä½†æ˜“äº§ç”Ÿå‡é˜³æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .

