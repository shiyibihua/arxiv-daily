---
layout: default
title: One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation
---

# One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation

**arXiv**: [2512.07829v1](https://arxiv.org/abs/2512.07829) | [PDF](https://arxiv.org/pdf/2512.07829.pdf)

**ä½œè€…**: Yuan Gao, Chen Chen, Tianrong Chen, Jiatao Gu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFAEæ¡†æž¶ï¼Œé€šè¿‡å•å±‚æ³¨æ„åŠ›é€‚é…é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ç”¨äºŽå›¾åƒç”Ÿæˆ**

**å…³é”®è¯**: `å›¾åƒç”Ÿæˆ` `é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨` `ç‰¹å¾é€‚é…` `æ‰©æ•£æ¨¡åž‹` `å½’ä¸€åŒ–æµ` `è‡ªç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¢„è®­ç»ƒè§†è§‰ç‰¹å¾ä¸Žç”Ÿæˆæ¨¡åž‹æ½œåœ¨ç©ºé—´ä¸åŒ¹é…ï¼Œå¯¼è‡´é€‚é…å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹è§£ç å™¨ï¼Œä¸€ä¸ªé‡æž„ç‰¹å¾ç©ºé—´ï¼Œå¦ä¸€ä¸ªç”¨äºŽå›¾åƒç”Ÿæˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNetç­‰åŸºå‡†ä¸Šè¾¾åˆ°æŽ¥è¿‘SOTAçš„FIDï¼Œæ”¯æŒæ‰©æ•£æ¨¡åž‹å’Œå½’ä¸€åŒ–æµ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

