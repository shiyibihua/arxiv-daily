---
layout: default
title: All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs
---

# All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs

**arXiv**: [2512.07580v1](https://arxiv.org/abs/2512.07580) | [PDF](https://arxiv.org/pdf/2512.07580.pdf)

**ä½œè€…**: Yahong Wang, Juncheng Wu, Zhangkai Ni, Longzhen Yang, Yihang Liu, Chengmei Yang, Ying Wen, Xianfeng Tang, Hui Liu, Yuyin Zhou, Lianghua He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¿¡æ¯åœ°å¹³çº¿æ¦‚å¿µï¼Œæ­ç¤ºè§†è§‰ä»¤ç‰Œåœ¨æ·±å±‚å†—ä½™ï¼Œé€šè¿‡éšæœºå‰ªæžæå‡VLLMæ•ˆçŽ‡**

**å…³é”®è¯**: `è§†è§‰å¤§è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `ä¿¡æ¯åœ°å¹³çº¿` `éšæœºå‰ªæž` `è®¡ç®—æ•ˆçŽ‡` `è§†è§‰ä»¤ç‰Œå†—ä½™`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å‘çŽ°çŽ°æœ‰è®­ç»ƒæ— å…³å‰ªæžæ–¹æ³•åœ¨æ·±å±‚è¡¨çŽ°ä¸ä¼˜äºŽéšæœºå‰ªæžï¼Œå½’å› äºŽä»¤ç‰Œä¿¡æ¯æ¶ˆå¤±
2. æå‡ºä¿¡æ¯åº¦é‡åˆ†æžè§†è§‰ä»¤ç‰Œä¿¡æ¯å˜åŒ–ï¼Œè¯†åˆ«ä¿¡æ¯åœ°å¹³çº¿ï¼Œå…¶ä½ç½®éšä»»åŠ¡å’Œæ¨¡åž‹èƒ½åŠ›å˜åŒ–
3. å®žéªŒè¡¨æ˜Žæ·±å±‚éšæœºå‰ªæžæœ‰æ•ˆå¹³è¡¡æ€§èƒ½ä¸Žæ•ˆçŽ‡ï¼Œç»“åˆDivPruneåœ¨Qwen2.5-VLä¸Šå‰ªæž50%ä¿æŒ96.9%æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

