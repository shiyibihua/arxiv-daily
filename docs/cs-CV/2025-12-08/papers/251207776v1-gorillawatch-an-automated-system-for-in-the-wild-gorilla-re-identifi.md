---
layout: default
title: GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring
---

# GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring

**arXiv**: [2512.07776v1](https://arxiv.org/abs/2512.07776) | [PDF](https://arxiv.org/pdf/2512.07776.pdf)

**ä½œè€…**: Maximilian Schall, Felix Leonard KnÃ¶fel, Noah Elias KÃ¶nig, Jan Jonas Kubeler, Maximilian von Klinski, Joan Wilhelm Linnemann, Xiaoshi Liu, Iven Jelle Schlegelmilch, Ole Woyciniuk, Alexandra Schild, Dante Wasmuht, Magdalena Bermejo Espinet, German Illera Basas, Gerard de Melo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGorillaWatchç³»ç»Ÿä»¥è§£å†³é‡Žå¤–å¤§çŒ©çŒ©é‡è¯†åˆ«ä¸Žç§ç¾¤ç›‘æµ‹çš„è‡ªåŠ¨åŒ–éš¾é¢˜**

**å…³é”®è¯**: `é‡Žç”ŸåŠ¨ç‰©é‡è¯†åˆ«` `å¤šç›®æ ‡è·Ÿè¸ª` `è‡ªç›‘ç£å­¦ä¹ ` `ç§ç¾¤ç›‘æµ‹` `ç›¸æœºé™·é˜±è§†é¢‘` `è·¨åŸŸæ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹å¤§è§„æ¨¡é‡Žå¤–è§†é¢‘æ•°æ®é›†ï¼Œé˜»ç¢è‡ªåŠ¨åŒ–é‡è¯†åˆ«æ¨¡åž‹è®­ç»ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šæ•°æ®é›†åŸºå‡†ï¼Œé›†æˆæ£€æµ‹ã€è·Ÿè¸ªä¸Žé‡è¯†åˆ«ï¼Œé‡‡ç”¨å¤šå¸§è‡ªç›‘ç£é¢„è®­ç»ƒç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡AttnLRPéªŒè¯æ¨¡åž‹ä¾èµ–ç”Ÿç‰©ç‰¹å¾ï¼Œå¤§è§„æ¨¡å›¾åƒéª¨å¹²ä¼˜äºŽè§†é¢‘æž¶æž„ï¼Œå®žçŽ°æ— ç›‘ç£ç§ç¾¤è®¡æ•°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

