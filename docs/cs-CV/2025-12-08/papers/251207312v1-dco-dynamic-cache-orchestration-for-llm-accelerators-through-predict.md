---
layout: default
title: DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management
---

# DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management

**arXiv**: [2512.07312v1](https://arxiv.org/abs/2512.07312) | [PDF](https://arxiv.org/pdf/2512.07312.pdf)

**ä½œè€…**: Zhongchun Zhou, Chengtao Lai, Yuhang Gu, Wei Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€ç¼“å­˜ç¼–æŽ’æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹ç®¡ç†ä¼˜åŒ–LLMåŠ é€Ÿå™¨æ€§èƒ½**

**å…³é”®è¯**: `AIåŠ é€Ÿå™¨` `ç¼“å­˜ç®¡ç†` `æ•°æ®æµé¢„æµ‹` `æ€§èƒ½ä¼˜åŒ–` `RTLå®žçŽ°` `å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é’ˆå¯¹AIåŠ é€Ÿå™¨ä¸­ç¼“å­˜å±‚æ¬¡å¤æ‚åŒ–é—®é¢˜ï¼Œç ”ç©¶å…±äº«ç³»ç»Ÿçº§ç¼“å­˜ä¸Žè½¯ä»¶æ ˆæ•°æ®æµå¼•å¯¼çš„ç®¡ç†ç­–ç•¥
2. ç»“åˆæ­»å—é¢„æµ‹ã€æ—è·¯å†³ç­–å’Œç¼“å­˜é¢ ç°¸ç¼“è§£æœºåˆ¶ï¼Œåœ¨å‘¨æœŸç²¾ç¡®æ¨¡æ‹Ÿä¸­å®žçŽ°æœ€é«˜1.80å€åŠ é€Ÿ
3. é€šè¿‡RTLå®žçŽ°éªŒè¯è®¾è®¡å¯è¡Œæ€§ï¼Œé¢ç§¯0.064mmÂ²ï¼Œæ—¶é’Ÿé¢‘çŽ‡2GHzï¼Œæ”¯æŒå¤§è§„æ¨¡å·¥ä½œè´Ÿè½½æ‰©å±•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
>   We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
>   Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

