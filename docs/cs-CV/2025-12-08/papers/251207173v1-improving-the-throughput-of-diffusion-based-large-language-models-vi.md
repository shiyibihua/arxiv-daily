---
layout: default
title: Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration
---

# Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration

**arXiv**: [2512.07173v1](https://arxiv.org/abs/2512.07173) | [PDF](https://arxiv.org/pdf/2512.07173.pdf)

**ä½œè€…**: Jucheng Shen, Gaurav Sarkar, Yeonju Ro, Sharath Nittur Sridhar, Zhangyang Wang, Aditya Akella, Souvik Kundu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCadLLMä»¥åŠ é€ŸåŸºäºŽæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†åžåé‡**

**å…³é”®è¯**: `æ‰©æ•£å¤§è¯­è¨€æ¨¡åž‹` `æŽ¨ç†åŠ é€Ÿ` `è®­ç»ƒæ— å…³æ–¹æ³•` `ç½®ä¿¡åº¦æ ¡å‡†` `KVç¼“å­˜` `åžåé‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†åžåé‡ä½Žï¼Œéœ€æå‡æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼Œæ— éœ€è®­ç»ƒï¼Œå…¼å®¹KVç¼“å­˜æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªä»»åŠ¡ä¸Šå®žçŽ°æœ€é«˜2.28å€åžåé‡æå‡ï¼Œä¿æŒå‡†ç¡®çŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.

