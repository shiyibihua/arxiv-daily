---
layout: default
title: Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting
---

# Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting

**arXiv**: [2512.07345v1](https://arxiv.org/abs/2512.07345) | [PDF](https://arxiv.org/pdf/2512.07345.pdf)

**ä½œè€…**: Shilong Jin, Haoran Duan, Litao Hua, Wentao Huang, Yuan Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTD-Attnæ¡†æž¶ï¼Œé€šè¿‡3Dæ³¨æ„åŠ›æœºåˆ¶è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹åœ¨3Dä»»åŠ¡ä¸­çš„å¤šè§†è§’ä¸ä¸€è‡´é—®é¢˜ã€‚**

**å…³é”®è¯**: `3Dç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `å¤šè§†è§’ä¸€è‡´æ€§` `æ³¨æ„åŠ›æœºåˆ¶` `æ–‡æœ¬åˆ°å›¾åƒ` `é«˜æ–¯æº…å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹å­˜åœ¨å…ˆéªŒè§†è§’åå·®ï¼Œå¯¼è‡´3Dä»»åŠ¡ä¸­ä¸åŒè§†è§’å¤–è§‚å†²çªã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥3D-AAGæ¨¡å—æž„å»ºè§†å›¾ä¸€è‡´çš„3Dæ³¨æ„åŠ›é«˜æ–¯ï¼ŒHAMæ¨¡å—é€šè¿‡è¯­ä¹‰å¼•å¯¼æ ‘è°ƒåˆ¶è·¨æ³¨æ„åŠ›å±‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šTD-Attnä½œä¸ºé€šç”¨æ’ä»¶ï¼Œæ˜¾è‘—æå‡3Dä»»åŠ¡çš„å¤šè§†è§’ä¸€è‡´æ€§ï¼Œæ”¯æŒå¯æŽ§ç¼–è¾‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

