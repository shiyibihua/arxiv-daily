---
layout: default
title: Unified Video Editing with Temporal Reasoner
---

# Unified Video Editing with Temporal Reasoner

**arXiv**: [2512.07469v1](https://arxiv.org/abs/2512.07469) | [PDF](https://arxiv.org/pdf/2512.07469.pdf)

**ä½œè€…**: Xiangpeng Yang, Ji Xie, Yiyuan Yang, Yan Huang, Min Xu, Qiang Wu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoCoFæ–¹æ³•ï¼Œé€šè¿‡é“¾å¼å¸§æŽ¨ç†è§£å†³è§†é¢‘ç¼–è¾‘ä¸­ç²¾åº¦ä¸Žç»Ÿä¸€æ€§çš„å†²çªã€‚**

**å…³é”®è¯**: `è§†é¢‘ç¼–è¾‘` `é“¾å¼å¸§æŽ¨ç†` `æ‰©æ•£æ¨¡åž‹` `æ— æŽ©ç ç¼–è¾‘` `è¿åŠ¨å¯¹é½` `æ—¶é•¿å¤–æŽ¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘ç¼–è¾‘æ–¹æ³•åœ¨ä¸“å®¶æ¨¡åž‹ç²¾åº¦ä¸Žç»Ÿä¸€æ¨¡åž‹é€šç”¨æ€§é—´å­˜åœ¨æƒè¡¡ï¼Œç¼ºä¹ç²¾ç¡®æŒ‡ä»¤åˆ°åŒºåŸŸæ˜ å°„ã€‚
2. VideoCoFå¼•å…¥é“¾å¼å¸§æŽ¨ç†ï¼Œå¼ºåˆ¶æ¨¡åž‹å…ˆé¢„æµ‹ç¼–è¾‘åŒºåŸŸæ½œåœ¨è¡¨ç¤ºï¼Œå†ç”Ÿæˆç›®æ ‡è§†é¢‘ï¼Œå®žçŽ°æ— æŽ©ç ç²¾ç¡®ç¼–è¾‘ã€‚
3. ä»…ç”¨5ä¸‡è§†é¢‘å¯¹è®­ç»ƒï¼Œåœ¨VideoCoF-Benchä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œå¹¶æ”¯æŒè¿åŠ¨å¯¹é½å’Œæ—¶é•¿å¤–æŽ¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

