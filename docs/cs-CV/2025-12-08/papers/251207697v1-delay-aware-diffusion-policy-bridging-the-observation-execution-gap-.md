---
layout: default
title: Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks
---

# Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks

**arXiv**: [2512.07697v1](https://arxiv.org/abs/2512.07697) | [PDF](https://arxiv.org/pdf/2512.07697.pdf)

**ä½œè€…**: Aileen Liao, Dong-Ki Kim, Max Olan Smith, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå»¶è¿Ÿæ„ŸçŸ¥æ‰©æ•£ç­–ç•¥ä»¥è§£å†³æœºå™¨äººåŠ¨æ€ä»»åŠ¡ä¸­çš„è§‚æµ‹-æ‰§è¡Œå»¶è¿Ÿé—®é¢˜**

**å…³é”®è¯**: `å»¶è¿Ÿæ„ŸçŸ¥ç­–ç•¥` `æ‰©æ•£æ¨¡åž‹` `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äººæŽ§åˆ¶` `åŠ¨æ€ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººæ„ŸçŸ¥ä¸ŽåŠ¨ä½œé€‰æ‹©é—´çš„å»¶è¿Ÿå¯¼è‡´è§‚æµ‹çŠ¶æ€ä¸Žæ‰§è¡ŒçŠ¶æ€ä¸åŒ¹é…ï¼Œå½±å“ä»»åŠ¡æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å»¶è¿Ÿè¡¥å¿è½¨è¿¹å’Œå»¶è¿Ÿæ¡ä»¶å¢žå¼ºï¼Œå°†æŽ¨ç†å»¶è¿Ÿæ˜¾å¼çº³å…¥ç­–ç•¥å­¦ä¹ æ¡†æž¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§ä»»åŠ¡ã€æœºå™¨äººå’Œå»¶è¿Ÿä¸‹éªŒè¯ï¼Œç›¸æ¯”æ— å»¶è¿Ÿæ–¹æ³•ï¼ŒæˆåŠŸçŽ‡å¯¹å»¶è¿Ÿæ›´é²æ£’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.

