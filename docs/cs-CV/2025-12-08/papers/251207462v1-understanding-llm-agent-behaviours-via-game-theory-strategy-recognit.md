---
layout: default
title: Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics
---

# Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics

**arXiv**: [2512.07462v1](https://arxiv.org/abs/2512.07462) | [PDF](https://arxiv.org/pdf/2512.07462.pdf)

**ä½œè€…**: Trung-Kiet Huynh, Duy-Minh Dao-Sy, Thanh-Bang Cao, Phong-Hao Le, Hong-Dan Nguyen, Phu-Quy Nguyen-Lam, Minh-Luan Nguyen-Vo, Hong-Phat Pham, Phu-Hoa Pham, Thien-Kim Than, Chi-Nguyen Tran, Huy Tran, Gia-Thoai Tran-Le, Alessio Buscemi, Le Hong Trang, The Anh Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ‰©å±•FAIRGAMEæ¡†æž¶ä»¥è¯„ä¼°LLMåœ¨é‡å¤ç¤¾ä¼šå›°å¢ƒä¸­çš„æˆ˜ç•¥è¡Œä¸ºï¼Œæ­ç¤ºåˆä½œåå·®ä¸Žè¯­è¨€å½±å“**

**å…³é”®è¯**: `å¤§åž‹è¯­è¨€æ¨¡åž‹` `åšå¼ˆè®º` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `ç¤¾ä¼šå›°å¢ƒ` `è¡Œä¸ºè¯„ä¼°` `AIæ²»ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç†è§£LLMä½œä¸ºè‡ªä¸»å†³ç­–è€…åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æˆ˜ç•¥æ„å›¾ï¼Œå¯¹AIå®‰å…¨ä¸Žåè°ƒè‡³å…³é‡è¦
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ”¶ç›Šç¼©æ”¾å›šå¾’å›°å¢ƒå’ŒåŠ¨æ€æ”¶ç›Šå…¬å…±ç‰©å“æ¸¸æˆï¼Œç³»ç»Ÿè¯„ä¼°LLMè¡Œä¸ºæ¨¡å¼
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°LLMè¡¨çŽ°å‡ºæ¿€åŠ±æ•æ„Ÿåˆä½œã€è·¨è¯­è¨€å·®å¼‚å’ŒèƒŒå›å€¾å‘ï¼Œè¡Œä¸ºæ„å›¾å—æ¨¡åž‹å’Œè¯­è¨€å½±å“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.

