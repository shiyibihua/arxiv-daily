---
layout: default
title: Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent
---

# Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent

**arXiv**: [2512.07490v1](https://arxiv.org/abs/2512.07490) | [PDF](https://arxiv.org/pdf/2512.07490.pdf)

**ä½œè€…**: Zhiyu Liu, Zhi Han, Yandong Tang, Jun Fan, Yao Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤æ›¿é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™ç®—æ³•ä»¥è§£å†³ä½Žç®¡ç§©å¼ é‡ä¼°è®¡ä¸­çš„è¿‡å‚æ•°åŒ–æ”¶æ•›é—®é¢˜**

**å…³é”®è¯**: `ä½Žç®¡ç§©å¼ é‡ä¼°è®¡` `äº¤æ›¿é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™` `è¿‡å‚æ•°åŒ–æ”¶æ•›` `å¼ é‡åˆ†è§£` `å¼ é‡æ¢å¤` `çº¿æ€§æ”¶æ•›ä¿è¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä½Žç®¡ç§©å¼ é‡ä¼°è®¡ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œæ¢¯åº¦ä¸‹é™åœ¨ç§©è¿‡ä¼°è®¡æ—¶æ”¶æ•›æ…¢æˆ–å‘æ•£
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ·»åŠ é¢„æ¡ä»¶é¡¹å’Œäº¤æ›¿æ›´æ–°å› å­ï¼ŒåŠ é€Ÿè¿‡å‚æ•°åŒ–è®¾ç½®ä¸‹çš„æ”¶æ•›ï¼Œå¹¶å»ºç«‹çº¿æ€§æ”¶æ•›ä¿è¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šç†è®ºåˆ†æžæ˜¾ç¤ºç®—æ³•åœ¨çº¿æ€§æ”¶æ•›ä¸”æ”¶æ•›çŽ‡ç‹¬ç«‹äºŽå¼ é‡æ¡ä»¶æ•°ï¼Œåˆæˆæ•°æ®æ¨¡æ‹ŸéªŒè¯äº†ç†è®ºæ–­è¨€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.

