---
layout: default
title: Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots
---

# Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots

**arXiv**: [2512.07673v1](https://arxiv.org/abs/2512.07673) | [PDF](https://arxiv.org/pdf/2512.07673.pdf)

**ä½œè€…**: Matthias Heyrman, Chenhao Li, Victor Klemm, Dongho Kang, Stelian Coros, Marco Hutter

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šåŸŸè¿åŠ¨åµŒå…¥ä»¥è§£å†³è…¿å¼æœºå™¨äººå®žæ—¶æ¨¡ä»¿ä¸­è¿åŠ¨è¡¨ç¤ºä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `è¿åŠ¨è¡¨ç¤ºå­¦ä¹ ` `å®žæ—¶æœºå™¨äººæ¨¡ä»¿` `å¤šåŸŸåµŒå…¥` `å°æ³¢ç¼–ç ` `æ¦‚çŽ‡åµŒå…¥` `é›¶æ ·æœ¬éƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è¿åŠ¨æŽ§åˆ¶å™¨å¿½ç•¥è¿åŠ¨å›ºæœ‰æ¨¡å¼ï¼Œéš¾ä»¥è”åˆæ•æ‰ç»“æž„åŒ–å‘¨æœŸæ¨¡å¼å’Œéžè§„åˆ™å˜åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŸºäºŽå°æ³¢çš„ç¼–ç å™¨å’Œæ¦‚çŽ‡åµŒå…¥å¹¶è¡Œç»Ÿä¸€åµŒå…¥ç»“æž„åŒ–å’Œéžç»“æž„åŒ–ç‰¹å¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç±»äººå½¢å’Œå››è¶³æœºå™¨äººä¸Šå®žçŽ°æ— é‡å®šå‘å®žæ—¶æ¨¡ä»¿ï¼Œä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼Œæ”¯æŒé›¶æ ·æœ¬éƒ¨ç½²

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.

