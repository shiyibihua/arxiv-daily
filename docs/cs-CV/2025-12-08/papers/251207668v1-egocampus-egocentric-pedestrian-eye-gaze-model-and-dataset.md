---
layout: default
title: EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset
---

# EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset

**arXiv**: [2512.07668v1](https://arxiv.org/abs/2512.07668) | [PDF](https://arxiv.org/pdf/2512.07668.pdf)

**ä½œè€…**: Ronan John, Aditya Kesari, Vincenzo DiMatteo, Kristin Dana

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEgoCampusæ•°æ®é›†å’ŒEgoCampusNetæ–¹æ³•ï¼Œä»¥é¢„æµ‹æˆ·å¤–æ ¡å›­çŽ¯å¢ƒä¸­è¡Œäººå¯¼èˆªæ—¶çš„è§†è§‰æ³¨æ„åŠ›ã€‚**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒè§†è§‰` `çœ¼åŠ¨æ³¨è§†é¢„æµ‹` `æˆ·å¤–å¯¼èˆª` `æ•°æ®é›†æž„å»º` `è¡Œäººè¡Œä¸ºåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¢„æµ‹çœŸå®žä¸–ç•Œå¯¼èˆªä¸­çš„äººç±»è§†è§‰æ³¨æ„åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æˆ·å¤–æ ¡å›­çŽ¯å¢ƒä¸‹çš„è¡Œäººçœ¼åŠ¨æ³¨è§†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Meta's Project Ariaçœ¼é•œæ”¶é›†æ•°æ®ï¼Œå¼€å‘EgoCampusNetæ¨¡åž‹é¢„æµ‹è¡Œäººçœ¼åŠ¨æ³¨è§†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ•°æ®é›†åŒ…å«è¶…è¿‡80åè¡Œäººçš„6å…¬é‡Œæˆ·å¤–è·¯å¾„è§†é¢‘ï¼Œæä¾›çœ¼åŠ¨æ³¨é‡Šï¼Œæ¨¡åž‹æ•ˆæžœæœªçŸ¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

