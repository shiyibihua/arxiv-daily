---
layout: default
title: When normalization hallucinates: unseen risks in AI-powered whole slide image processing
---

# When normalization hallucinates: unseen risks in AI-powered whole slide image processing

**arXiv**: [2512.07426v1](https://arxiv.org/abs/2512.07426) | [PDF](https://arxiv.org/pdf/2512.07426.pdf)

**ä½œè€…**: Karel Moens, Matthew B. Blaschko, Tinne Tuytelaars, Bart Diricx, Jonas De Vylder, Mustafa Yousif

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾åƒæ¯”è¾ƒåº¦é‡ä»¥æ£€æµ‹å…¨åˆ‡ç‰‡å›¾åƒå½’ä¸€åŒ–ä¸­çš„å¹»è§‰é£Žé™©**

**å…³é”®è¯**: `å…¨åˆ‡ç‰‡å›¾åƒå½’ä¸€åŒ–` `å¹»è§‰æ£€æµ‹` `è®¡ç®—ç—…ç†å­¦` `æ·±åº¦å­¦ä¹ ` `å›¾åƒæ¯”è¾ƒåº¦é‡` `ä¸´åºŠéªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦å­¦ä¹ é©±åŠ¨çš„å…¨åˆ‡ç‰‡å›¾åƒå½’ä¸€åŒ–å¯èƒ½å¼•å…¥å¹»è§‰å†…å®¹ï¼Œå¨èƒä¸‹æ¸¸åˆ†æžã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ–°é¢–å›¾åƒæ¯”è¾ƒåº¦é‡ï¼Œè‡ªåŠ¨æ£€æµ‹å½’ä¸€åŒ–è¾“å‡ºä¸­çš„å¹»è§‰ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žä¸´åºŠæ•°æ®ä¸Šè¯„ä¼°æ–¹æ³•ï¼Œæ­ç¤ºä¼ ç»ŸæŒ‡æ ‡æœªæ•æ‰çš„æ˜¾è‘—ä¸ä¸€è‡´å’Œå¤±è´¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

