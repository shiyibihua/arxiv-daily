---
layout: default
title: A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning
---

# A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning

**arXiv**: [2512.07136v1](https://arxiv.org/abs/2512.07136) | [PDF](https://arxiv.org/pdf/2512.07136.pdf)

**ä½œè€…**: Siyang Jiang, Mu Yuan, Xiang Ji, Bufang Yang, Zeyu Liu, Lilin Xu, Yang Li, Yuting He, Liran Dong, Wenrui Lu, Zhenyu Yan, Xiaofan Jiang, Wei Gao, Hongkai Chen, Guoliang Xing

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†CUHK-Xä¸ŽåŸºå‡†å¥—ä»¶ï¼Œä»¥æ”¯æŒäººç±»æ´»åŠ¨ç†è§£ä¸ŽæŽ¨ç†ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ•°æ®é›†` `äººç±»æ´»åŠ¨ç†è§£` `äººç±»æ´»åŠ¨æŽ¨ç†` `å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `éžRGBæ¨¡æ€`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ•°æ®é›†ç¼ºä¹éžRGBæ¨¡æ€çš„å¤§è§„æ¨¡æ–‡æœ¬æè¿°èµ„æºï¼Œé™åˆ¶LVLMsåœ¨äººç±»æ´»åŠ¨ç†è§£ä¸ŽæŽ¨ç†ä¸­çš„åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŸºäºŽæç¤ºçš„åœºæ™¯åˆ›å»ºæ–¹æ³•ï¼Œåˆ©ç”¨LLMsç”Ÿæˆé€»è¾‘è¿žè´¯çš„æ´»åŠ¨åºåˆ—ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯æå‡æè¿°ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HARã€HAUå’ŒHARnä»»åŠ¡ä¸Šå¹³å‡å‡†ç¡®çŽ‡åˆ†åˆ«ä¸º76.52%ã€40.76%å’Œ70.25%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

