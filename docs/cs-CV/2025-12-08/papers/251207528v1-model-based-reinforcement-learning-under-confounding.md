---
layout: default
title: Model-Based Reinforcement Learning Under Confounding
---

# Model-Based Reinforcement Learning Under Confounding

**arXiv**: [2512.07528v1](https://arxiv.org/abs/2512.07528) | [PDF](https://arxiv.org/pdf/2512.07528.pdf)

**ä½œè€…**: Nishanth Venkatesh, Andreas A. Malikopoulos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽä»£ç†å˜é‡çš„è¿‘ç«¯ç¦»ç­–ç•¥è¯„ä¼°æ–¹æ³•ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡æœªè§‚æµ‹ä¸‹çš„æ··æ·†æ¨¡åž‹å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ` `æ··æ·†çŽ¯å¢ƒ` `ç¦»ç­–ç•¥è¯„ä¼°` `ä»£ç†å˜é‡` `æœ€å¤§å› æžœç†µ` `ä¸Šä¸‹æ–‡MDPs`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶ä¸Šä¸‹æ–‡æœªè§‚æµ‹çš„C-MDPsä¸­ï¼Œç¦»çº¿æ•°æ®å­˜åœ¨æ··æ·†å¯¼è‡´ä¼ ç»Ÿæ¨¡åž‹å­¦ä¹ ä¸ä¸€è‡´ã€‚
2. åˆ©ç”¨ä»£ç†å˜é‡å¯é€†æ€§ï¼Œè¯†åˆ«æ··æ·†å¥–åŠ±æœŸæœ›ï¼Œç»“åˆè¡Œä¸ºå¹³å‡è½¬ç§»æ¨¡åž‹æž„å»ºæ›¿ä»£MDPã€‚
3. è¯¥æ–¹æ³•ä¸Žæœ€å¤§å› æžœç†µæ¡†æž¶å…¼å®¹ï¼Œæ”¯æŒåœ¨æ··æ·†çŽ¯å¢ƒä¸­è¿›è¡ŒåŽŸåˆ™æ€§æ¨¡åž‹å­¦ä¹ å’Œè§„åˆ’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.

