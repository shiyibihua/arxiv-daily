---
layout: default
title: See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations
---

# See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations

**arXiv**: [2512.07582v1](https://arxiv.org/abs/2512.07582) | [PDF](https://arxiv.org/pdf/2512.07582.pdf)

**ä½œè€…**: Guangyan Chen, Meiling Wang, Qi Shao, Zichen Zhou, Weixin Mao, Te Cui, Minzhao Zhu, Yinan Deng, Luojie Yang, Zhanqi Zhang, Yi Yang, Hua Chen, Yufeng Yue

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViVLAæ¨¡åž‹ï¼Œé€šè¿‡å•æ¬¡è§†é¢‘æ¼”ç¤ºå®žçŽ°æœºå™¨äººä»»åŠ¡å­¦ä¹ ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å•æ¬¡å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `çŸ¥è¯†è’¸é¦` `æ³›åŒ–èƒ½åŠ›` `è§†é¢‘æ¼”ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚
2. ViVLAè”åˆå¤„ç†æ¼”ç¤ºè§†é¢‘ä¸Žæœºå™¨äººè§‚æµ‹ï¼Œé¢„æµ‹åŠ¨ä½œåºåˆ—ä»¥è’¸é¦çŸ¥è¯†ã€‚
3. å®žéªŒæ˜¾ç¤ºåœ¨æœªè§ä»»åŠ¡ä¸Šæå‡è¶…30%ï¼Œè·¨å…·èº«è§†é¢‘ä¿æŒè¶…35%å¢žç›Šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.

