---
layout: default
title: Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse
---

# Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse

**arXiv**: [2512.07400v1](https://arxiv.org/abs/2512.07400) | [PDF](https://arxiv.org/pdf/2512.07400.pdf)

**ä½œè€…**: Giulia Lanzillotta, Damiano Meier, Thomas Hofmann

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ‰©å±•ç¥žç»åç¼©æ¡†æž¶åˆ†æžç»éªŒå›žæ”¾ä¸­æµ…å±‚ä¸Žæ·±å±‚é—å¿˜çš„ä¸å¯¹ç§°æ€§**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `ç»éªŒå›žæ”¾` `ç¥žç»åç¼©` `é—å¿˜åˆ†æž` `ç‰¹å¾å‡ ä½•` `ç»Ÿè®¡ä¼ªå½±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æŒç»­å­¦ä¹ ä¸­ç¥žç»ç½‘ç»œå¸¸ä¿ç•™è¿‡åŽ»ä»»åŠ¡çš„çº¿æ€§å¯åˆ†è¡¨ç¤ºï¼Œä½†è¾“å‡ºé¢„æµ‹å¤±è´¥ï¼Œå®šä¹‰ä¸ºæµ…å±‚ä¸Žæ·±å±‚é—å¿˜çš„å·®è·ã€‚
2. æ­ç¤ºç»éªŒå›žæ”¾ä¸­ä¸å¯¹ç§°æ€§ï¼šå°ç¼“å†²åŒºå¯é”šå®šç‰¹å¾å‡ ä½•é˜²æ­¢æ·±å±‚é—å¿˜ï¼Œä½†ç¼“è§£æµ…å±‚é—å¿˜éœ€æ›´å¤§å®¹é‡ã€‚
3. æ‰©å±•ç¥žç»åç¼©æ¡†æž¶è‡³åºåˆ—è®¾ç½®ï¼Œè¯æ˜Žéžé›¶å›žæ”¾åˆ†æ•°æ¸è¿‘ä¿è¯çº¿æ€§å¯åˆ†æ€§ï¼Œè€Œå°ç¼“å†²åŒºå¯¼è‡´ç»Ÿè®¡ä¼ªå½±ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.

