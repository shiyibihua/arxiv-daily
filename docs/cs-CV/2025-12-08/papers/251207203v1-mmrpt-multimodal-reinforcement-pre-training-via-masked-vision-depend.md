---
layout: default
title: MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning
---

# MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning

**arXiv**: [2512.07203v1](https://arxiv.org/abs/2512.07203) | [PDF](https://arxiv.org/pdf/2512.07203.pdf)

**ä½œè€…**: Xuhui Zheng, Kang An, Ziliang Wang, Yuhang Wang, Faqiang Qian, Yichao Wu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMRPTæ¡†æž¶ï¼Œé€šè¿‡æŽ©ç å¤šæ¨¡æ€å¼ºåŒ–é¢„è®­ç»ƒå¢žå¼ºè§†è§‰æŽ¨ç†ï¼Œè§£å†³å¤šæ¨¡æ€é¢„è®­ç»ƒä¸­çš„æè¿°æ€§åå·®é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€é¢„è®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰æŽ¨ç†` `æŽ©ç å­¦ä¹ ` `é›¶æ ·æœ¬å­¦ä¹ ` `é²æ£’æ€§å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒå—å›¾åƒ-æ–‡æœ¬å¯¹æè¿°æ€§åå·®é™åˆ¶ï¼Œæ¨¡åž‹ä¾èµ–è¡¨é¢è¯­è¨€çº¿ç´¢è€Œéžè§†è§‰ç†è§£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ç›´æŽ¥èžå…¥é¢„è®­ç»ƒï¼Œé€šè¿‡æŽ©ç è§†è§‰ä¾èµ–ç‰‡æ®µå¹¶åŸºäºŽè¯­ä¹‰-è§†è§‰å¥–åŠ±é‡æž„ï¼Œå¥–åŠ±è§†è§‰åŸºç¡€è€Œéžæ–‡æœ¬æ¨¡ä»¿ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´æå‡ï¼Œç›‘ç£å¾®è°ƒä¸‹æ˜¾è‘—å¢žå¼ºé²æ£’æ€§ï¼Œè¯æ˜Žå¼ºåŒ–é©±åŠ¨çš„æŽ©ç æŽ¨ç†æä¾›æ›´å¯é é¢„è®­ç»ƒç›®æ ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

