---
layout: default
title: ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning
---

# ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning

**arXiv**: [2512.07795v1](https://arxiv.org/abs/2512.07795) | [PDF](https://arxiv.org/pdf/2512.07795.pdf)

**ä½œè€…**: Nearchos Potamitis, Lars Klein, Akhil Arora

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReasonBENCHåŸºå‡†ä»¥é‡åŒ–å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†çš„ä¸ç¨³å®šæ€§**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `åŸºå‡†æµ‹è¯•` `ä¸ç¡®å®šæ€§é‡åŒ–` `å¯å¤çŽ°æ€§è¯„ä¼°` `å¤šè½®åè®®` `æ€§èƒ½ç¨³å®šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰è¯„ä¼°å¿½è§†éšæœºè§£ç å¯¼è‡´çš„ä¸ç¨³å®šæ€§ï¼Œå½±å“æ€§èƒ½å¯é æ€§å’Œå¯å¤çŽ°æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šæä¾›æ¨¡å—åŒ–è¯„ä¼°åº“ã€å¤šè½®åè®®å’Œå…¬å¼€æŽ’è¡Œæ¦œï¼Œæ ‡å‡†åŒ–æŽ¨ç†æ¡†æž¶ä¸Žä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°å¤šæ•°æŽ¨ç†ç­–ç•¥ä¸ç¨³å®šï¼Œæ€§èƒ½ç›¸è¿‘æ–¹æ³•ç½®ä¿¡åŒºé—´å·®å¼‚å¯è¾¾å››å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

