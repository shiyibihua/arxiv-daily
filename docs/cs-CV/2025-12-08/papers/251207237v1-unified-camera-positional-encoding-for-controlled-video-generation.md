---
layout: default
title: Unified Camera Positional Encoding for Controlled Video Generation
---

# Unified Camera Positional Encoding for Controlled Video Generation

**arXiv**: [2512.07237v1](https://arxiv.org/abs/2512.07237) | [PDF](https://arxiv.org/pdf/2512.07237.pdf)

**ä½œè€…**: Cheng Zhang, Boying Li, Meng Wei, Yan-Pei Cao, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€ç›¸æœºä½ç½®ç¼–ç UCPEï¼Œé€šè¿‡ç›¸å¯¹å°„çº¿ç¼–ç å’Œç»å¯¹æ–¹å‘ç¼–ç å¢žå¼ºç›¸æœºå¯æŽ§è§†é¢‘ç”Ÿæˆã€‚**

**å…³é”®è¯**: `ç›¸æœºå¯æŽ§è§†é¢‘ç”Ÿæˆ` `ç»Ÿä¸€ç›¸æœºç¼–ç ` `ç›¸å¯¹å°„çº¿ç¼–ç ` `æ‰©æ•£å˜æ¢å™¨` `å¤šè§†å›¾ä»»åŠ¡` `é•œå¤´ç•¸å˜å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç›¸æœºç¼–ç ä¾èµ–ç®€åŒ–é’ˆå­”æ¨¡åž‹ï¼Œé™åˆ¶äº†å¯¹çœŸå®žç›¸æœºå¤šæ ·å†…å‚å’Œé•œå¤´ç•¸å˜çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. å¼•å…¥ç›¸å¯¹å°„çº¿ç¼–ç ç»Ÿä¸€ç›¸æœº6è‡ªç”±åº¦ä½å§¿ã€å†…å‚å’Œé•œå¤´ç•¸å˜ï¼Œå¹¶è¯†åˆ«ä¿¯ä»°å’Œæ¨ªæ»šä½œä¸ºç»å¯¹æ–¹å‘ç¼–ç çš„å…³é”®ç»„ä»¶ã€‚
3. åœ¨ç›¸æœºå¯æŽ§æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒUCPEä»¥å°‘äºŽ1%å¯è®­ç»ƒå‚æ•°å®žçŽ°æœ€å…ˆè¿›çš„ç›¸æœºæŽ§åˆ¶æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

