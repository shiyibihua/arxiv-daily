---
layout: default
title: Reconstructing Objects along Hand Interaction Timelines in Egocentric Video
---

# Reconstructing Objects along Hand Interaction Timelines in Egocentric Video

**arXiv**: [2512.07394v1](https://arxiv.org/abs/2512.07394) | [PDF](https://arxiv.org/pdf/2512.07394.pdf)

**ä½œè€…**: Zhifan Zhu, Siddhant Bansal, Shashank Tripathi, Dima Damen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROHITä»»åŠ¡ä¸ŽCOPæ¡†æž¶ï¼Œé€šè¿‡æ‰‹äº¤äº’æ—¶é—´çº¿çº¦æŸä¼˜åŒ–ç‰©ä½“å§¿æ€ä¼ æ’­ï¼Œæå‡ç¬¬ä¸€äººç§°è§†é¢‘ä¸­ç‰©ä½“é‡å»ºç²¾åº¦ã€‚**

**å…³é”®è¯**: `ç¬¬ä¸€äººç§°è§†é¢‘` `ç‰©ä½“é‡å»º` `æ‰‹äº¤äº’æ—¶é—´çº¿` `å§¿æ€ä¼ æ’­` `çº¦æŸä¼˜åŒ–` `ç¨³å®šæŠ“å–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨ç¬¬ä¸€äººç§°è§†é¢‘ä¸­ï¼Œå¦‚ä½•åŸºäºŽæ‰‹äº¤äº’æ—¶é—´çº¿ï¼ˆHITï¼‰é‡å»ºç‰©ä½“å§¿æ€ï¼Œå°¤å…¶å…³æ³¨ç¨³å®šæŠ“å–é˜¶æ®µã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå®šä¹‰HITå¹¶å»ºæ¨¡å§¿æ€çº¦æŸï¼Œæå‡ºCOPæ¡†æž¶è¿›è¡Œçº¦æŸä¼˜åŒ–ä¸Žå§¿æ€ä¼ æ’­ï¼Œæ— éœ€3DçœŸå€¼æ ‡æ³¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HOT3Då’ŒEPIC-Kitchensæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒCOPæå‡ç¨³å®šæŠ“å–é‡å»º6.2-11.3%ï¼ŒHITé‡å»ºæœ€é«˜è¾¾24.5%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

