---
layout: default
title: How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline
---

# How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline

**arXiv**: [2512.07385v1](https://arxiv.org/abs/2512.07385) | [PDF](https://arxiv.org/pdf/2512.07385.pdf)

**ä½œè€…**: Chunhui Zhang, Li Liu, Zhipeng Zhang, Yong Wang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUAV-Anti-UAVä»»åŠ¡ä¸ŽMambaSTSåŸºçº¿ï¼Œä»¥è§£å†³ç§»åŠ¨æ— äººæœºå¹³å°è¿½è¸ªç›®æ ‡æ— äººæœºçš„æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `æ— äººæœºè¿½è¸ª` `å¤šæ¨¡æ€è§†è§‰è·Ÿè¸ª` `æ—¶ç©ºè¯­ä¹‰å­¦ä¹ ` `Mambaæ¨¡åž‹` `é•¿åºåˆ—å»ºæ¨¡` `åŸºå‡†æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åæ— äººæœºç ”ç©¶å¿½è§†ç§»åŠ¨å¹³å°è¿½è¸ªï¼ŒUAV-Anti-UAVä»»åŠ¡é¢ä¸´åŒåŠ¨æ€å¹²æ‰°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºç™¾ä¸‡è§„æ¨¡æ•°æ®é›†ï¼Œæå‡ºMambaSTSæ–¹æ³•ï¼Œé›†æˆæ—¶ç©ºè¯­ä¹‰å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°50ç§çŽ°ä»£è·Ÿè¸ªç®—æ³•ï¼Œæ˜¾ç¤ºè¯¥é¢†åŸŸæœ‰æ˜¾è‘—æ”¹è¿›ç©ºé—´ï¼ŒéªŒè¯MambaSTSæœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

