---
layout: default
title: Grounding Everything in Tokens for Multimodal Large Language Models
---

# Grounding Everything in Tokens for Multimodal Large Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.10554" target="_blank" class="toolbar-btn">arXiv: 2512.10554v1</a>
    <a href="https://arxiv.org/pdf/2512.10554.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10554v1" 
            onclick="toggleFavorite(this, '2512.10554v1', 'Grounding Everything in Tokens for Multimodal Large Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiangxuan Ren, Zhongdao Wang, Liping Hou, Pin Tang, Guoqing Wang, Chao Ma

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-11

**Â§áÊ≥®**: 19 pages, 16 figures, 12 Tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**GETokÔºöÈÄöËøátokenÂåñÂÆûÁé∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁ≤æÁ°Æ2DÁ©∫Èó¥ÂÆö‰Ωç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `2DÁ©∫Èó¥ÂÆö‰Ωç` `ÂõæÂÉètokenÂåñ` `Á©∫Èó¥ÂÖ≥Á≥ªÊé®ÁêÜ` `ËßÜËßâÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. MLLM‰æùËµñÂõæÂÉètokenÂåñÔºå‰ΩÜÁé∞ÊúâtokenÂåñÊñπÊ≥ïÈöæ‰ª•Á≤æÁ°ÆÂú∞Âú®2DÁ©∫Èó¥‰∏≠ÂÆö‰ΩçÁâ©‰Ωì„ÄÇ
2. GETokÈÄöËøáÂºïÂÖ•ÁΩëÊ†ºtokenÂíåÂÅèÁßªtokenÔºåÂ∞ÜÁ©∫Èó¥ÂÖ≥Á≥ªÂµåÂÖ•Âà∞token‰∏≠ÔºåÂÆûÁé∞Á≤æÁ°ÆÂÆö‰Ωç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGETokÂú®Êåá‰ª£‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®2DÁ©∫Èó¥Êé®ÁêÜ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®ËßÜËßâÁêÜËß£ÂíåÊé®ÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåMLLMs‰ΩøÁî®ÁöÑËá™ÂõûÂΩíTransformerÊû∂ÊûÑÈúÄË¶ÅÂØπËæìÂÖ•ÂõæÂÉèËøõË°åtokenÂåñÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®2DÂõæÂÉèÁ©∫Èó¥ÂÜÖÁ≤æÁ°ÆÂÆö‰ΩçÂØπË±°ÁöÑËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áî®‰∫éÂØπË±°ÂÆö‰ΩçÁöÑÁ©∫Èó¥Ë°®Á§∫ÊñπÊ≥ïÔºåÂêç‰∏∫GETokÔºåÂÆÉÂ∞Ü‰∏Ä‰∏™‰∏ìÈó®ÁöÑÂèØÂ≠¶‰π†tokenËØçÊ±áË°®ÈõÜÊàêÂà∞MLLMs‰∏≠„ÄÇGETokÈ¶ñÂÖà‰ΩøÁî®ÁΩëÊ†ºtokenÂ∞ÜÂõæÂÉèÂπ≥Èù¢ÂàíÂàÜ‰∏∫ÁªìÊûÑÂåñÁöÑÁ©∫Èó¥ÈîöÁÇπÔºåÁÑ∂ÂêéÂà©Áî®ÂÅèÁßªtokenÊù•ÂÆûÁé∞ÂØπÂÆö‰ΩçÈ¢ÑÊµãÁöÑÁ≤æÁ°ÆÂíåËø≠‰ª£ÁªÜÂåñ„ÄÇÈÄöËøáÂ∞ÜÁ©∫Èó¥ÂÖ≥Á≥ªÁõ¥Êé•ÂµåÂÖ•Âà∞token‰∏≠ÔºåGETokÊòæËëóÊèêÂçá‰∫ÜMLLMsÂú®ÂéüÁîü2DÁ©∫Èó¥Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºåËÄåÊó†ÈúÄ‰øÆÊîπËá™ÂõûÂΩíÊû∂ÊûÑ„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÂú®ÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ËÆæÁΩÆ‰∏≠ÔºåGETokÂú®ÂêÑÁßçÊåá‰ª£‰ªªÂä°‰∏äÈÉΩ‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜËßÜËßâ‰ø°ÊÅØÊó∂ÔºåÈúÄË¶ÅÂ∞ÜÂõæÂÉèËΩ¨Êç¢‰∏∫tokenÂ∫èÂàó„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑtokenÂåñÊñπÊ≥ïÂú®Â∞ÜÂõæÂÉèÁâπÂæÅÊò†Â∞ÑÂà∞Á¶ªÊï£tokenÊó∂Ôºå‰ºö‰∏¢Â§±Á≤æÁ°ÆÁöÑÁ©∫Èó¥‰ø°ÊÅØÔºåÂØºËá¥Ê®°ÂûãÈöæ‰ª•Âú®2DÂõæÂÉèÁ©∫Èó¥‰∏≠Á≤æÁ°ÆÂÆö‰ΩçÂíåÁêÜËß£Áâ©‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇËøôÈôêÂà∂‰∫ÜMLLMsÂú®ÈúÄË¶ÅÁ≤æÁ°ÆÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGETokÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁ©∫Èó¥‰ø°ÊÅØÁõ¥Êé•ÁºñÁ†ÅÂà∞token‰∏≠Ôºå‰ªéËÄå‰ΩøMLLMsËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂà©Áî®ÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåGETokÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ÁöÑtokenÔºöÁΩëÊ†ºtokenÂíåÂÅèÁßªtoken„ÄÇÁΩëÊ†ºtokenÁî®‰∫éÂ∞ÜÂõæÂÉèÂàíÂàÜ‰∏∫ËßÑÂàôÁöÑÁΩëÊ†ºÔºåÊèê‰æõÁ≤óÁï•ÁöÑÁ©∫Èó¥ÈîöÁÇπÔºõÂÅèÁßªtokenÂàôÁî®‰∫éÂú®ÁΩëÊ†ºÁöÑÂü∫Á°Ä‰∏äËøõË°åÁ≤æÁªÜÁöÑ‰ΩçÁΩÆË∞ÉÊï¥ÔºåÂÆûÁé∞Á≤æÁ°ÆÁöÑÁâ©‰ΩìÂÆö‰Ωç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGETokÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÊòØÂú®Áé∞ÊúâÁöÑMLLMÊû∂ÊûÑ‰∏≠Âä†ÂÖ•‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑtokenËØçÊ±áË°®„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®ÁΩëÊ†ºtokenÂ∞ÜËæìÂÖ•ÂõæÂÉèÂàÜÂâ≤ÊàêÂ§ö‰∏™ÁΩëÊ†ºÂå∫ÂüüÔºåÊØè‰∏™ÁΩëÊ†ºÂå∫ÂüüÂØπÂ∫î‰∏Ä‰∏™token„ÄÇÁÑ∂ÂêéÔºå‰ΩøÁî®ÂÅèÁßªtokenÂØπÊØè‰∏™ÁΩëÊ†ºÂå∫ÂüüÂÜÖÁöÑÁâ©‰Ωì‰ΩçÁΩÆËøõË°åÂæÆË∞É„ÄÇËøô‰∫õtoken‰∏éÂõæÂÉèÁöÑÂÖ∂‰ªñËßÜËßâtoken‰∏ÄËµ∑ËæìÂÖ•Âà∞MLLM‰∏≠ËøõË°åÂ§ÑÁêÜ„ÄÇMLLMÂà©Áî®Ëøô‰∫õÁ©∫Èó¥tokenËøõË°åÊé®ÁêÜÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑ2DÁ©∫Èó¥ÂÆö‰Ωç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGETokÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÁ©∫Èó¥‰ø°ÊÅØÊòæÂºèÂú∞ÁºñÁ†ÅÂà∞token‰∏≠„ÄÇ‰∏é‰º†ÁªüÁöÑtokenÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåGETok‰∏ç‰ªÖ‰øùÁïô‰∫ÜÂõæÂÉèÁöÑËßÜËßâÁâπÂæÅÔºåËøò‰øùÁïô‰∫ÜÁâ©‰ΩìÂú®ÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇËøôÁßçÊòæÂºèÁöÑÁ©∫Èó¥ÁºñÁ†ÅÊñπÂºè‰ΩøÂæóMLLMsËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂà©Áî®ÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÔºå‰ªéËÄåÊèêÈ´òÂÖ∂Âú®ÈúÄË¶ÅÁ≤æÁ°ÆÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöGETokÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÁΩëÊ†ºtokenÁöÑÊï∞ÈáèÂíåÂ§ßÂ∞èÔºåÈúÄË¶ÅÊ†πÊçÆÂõæÂÉèÁöÑÂàÜËæ®ÁéáÂíåÁâ©‰ΩìÁöÑÂ∞∫ÂØ∏ËøõË°åË∞ÉÊï¥Ôºõ2) ÂÅèÁßªtokenÁöÑË°®Á§∫ÊñπÂºèÔºåÂèØ‰ª•‰ΩøÁî®Áõ∏ÂØπÂùêÊ†áÊàñÁªùÂØπÂùêÊ†áÔºõ3) Â¶Ç‰ΩïÂ∞ÜÁΩëÊ†ºtokenÂíåÂÅèÁßªtoken‰∏éÂõæÂÉèÁöÑÂÖ∂‰ªñËßÜËßâtokenËøõË°åËûçÂêàÔºåÂèØ‰ª•‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÊàñÂÖ∂‰ªñËûçÂêàÊñπÊ≥ïÔºõ4) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÈúÄË¶ÅËÄÉËôëÂÆö‰ΩçÁöÑÁ≤æÂ∫¶ÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGETokÂú®ÂêÑÁßçÊåá‰ª£‰ªªÂä°‰∏äÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®RefCOCOgÊï∞ÊçÆÈõÜ‰∏äÔºåGETokÁöÑÂáÜÁ°ÆÁéáÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜË∂ÖËøá5%„ÄÇÊ≠§Â§ñÔºåGETokÂú®Âº∫ÂåñÂ≠¶‰π†ËÆæÁΩÆ‰∏ã‰πüË°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GETokÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÁ≤æÁ°Æ2DÁ©∫Èó¥ÂÆö‰ΩçÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°ÔºåÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞„ÄÅÁõÆÊ†áÊ£ÄÊµãÂíåÊú∫Âô®‰∫∫ÂØºËà™„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáÊ®°ÂûãÂØπÂõæÂÉè‰∏≠Áâ©‰ΩìÁ©∫Èó¥ÂÖ≥Á≥ªÁöÑÁêÜËß£Ôºå‰ªéËÄåÊèêÈ´ò‰ªªÂä°ÊÄßËÉΩ„ÄÇÊú™Êù•ÔºåGETokÊúâÊúõÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéßÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.

