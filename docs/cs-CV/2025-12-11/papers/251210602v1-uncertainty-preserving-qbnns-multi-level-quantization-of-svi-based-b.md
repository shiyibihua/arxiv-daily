---
layout: default
title: Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification
---

# Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification

**arXiv**: [2512.10602v1](https://arxiv.org/abs/2512.10602) | [PDF](https://arxiv.org/pdf/2512.10602.pdf)

**ä½œè€…**: Hendrik Borras, Yong Wu, Bernhard Klein, Holger FrÃ¶ning

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šçº§é‡åŒ–æ¡†æž¶ä»¥åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²è´å¶æ–¯ç¥žç»ç½‘ç»œï¼Œä¿æŒä¸ç¡®å®šæ€§ä¼°è®¡**

**å…³é”®è¯**: `è´å¶æ–¯ç¥žç»ç½‘ç»œ` `ä¸ç¡®å®šæ€§é‡åŒ–` `å¤šçº§é‡åŒ–` `éšæœºå˜åˆ†æŽ¨æ–­` `è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²` `ä½Žç²¾åº¦è®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è´å¶æ–¯ç¥žç»ç½‘ç»œæä¾›ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä½†è®¡ç®—å’Œå†…å­˜å¼€é”€å¤§ï¼Œé‡åŒ–åº”ç”¨ä¸è¶³
2. å¼•å…¥ä¸‰ç§é‡åŒ–ç­–ç•¥ï¼šå˜åˆ†å‚æ•°é‡åŒ–ã€é‡‡æ ·å‚æ•°é‡åŒ–å’Œè”åˆé‡åŒ–ï¼Œä½¿ç”¨å¯¹æ•°é‡åŒ–ä¿æŠ¤åˆ†å¸ƒç»“æž„
3. åœ¨Dirty-MNISTä¸Šå®žéªŒï¼Œ4ä½é‡åŒ–å®žçŽ°8å€å†…å­˜å‡å°‘ï¼Œåˆ†ç±»å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§ä¼°è®¡ä¿æŒè‰¯å¥½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog "Bayesian Machines" operating at inherently low precision.

