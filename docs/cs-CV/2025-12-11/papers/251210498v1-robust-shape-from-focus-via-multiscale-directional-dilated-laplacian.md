---
layout: default
title: Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network
---

# Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network

**arXiv**: [2512.10498v1](https://arxiv.org/abs/2512.10498) | [PDF](https://arxiv.org/pdf/2512.10498.pdf)

**ä½œè€…**: Khurram Ashfaq, Muhammad Tariq Mahmood

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šå°ºåº¦æ–¹å‘æ€§æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯å’Œå¾ªçŽ¯ç½‘ç»œçš„é²æ£’èšç„¦å½¢çŠ¶æ¢å¤æ–¹æ³•**

**å…³é”®è¯**: `èšç„¦å½¢çŠ¶æ¢å¤` `å¤šå°ºåº¦ç‰¹å¾æå–` `å¾ªçŽ¯ç¥žç»ç½‘ç»œ` `æ·±åº¦ä¼°è®¡` `é²æ£’æ€§ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. èšç„¦å½¢çŠ¶æ¢å¤ä¸­æ·±åº¦å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¸¤é˜¶æ®µå¤„ç†å¯¼è‡´ä¼ªå½±å’Œå™ªå£°æ”¾å¤§çš„é—®é¢˜
2. é‡‡ç”¨ä¼ ç»Ÿå¤šå°ºåº¦æ–¹å‘æ€§æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯æ ¸æå–é²æ£’èšç„¦ä½“ç§¯ï¼Œç»“åˆè½»é‡çº§GRUå¾ªçŽ¯ç½‘ç»œè¿­ä»£ä¼˜åŒ–æ·±åº¦ä¼°è®¡
3. åœ¨åˆæˆå’ŒçœŸå®žæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•åœ¨ç²¾åº¦å’Œæ³›åŒ–æ€§ä¸Šä¼˜äºŽçŽ°æœ‰æŠ€æœ¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.

