---
layout: default
title: Latent Chain-of-Thought World Modeling for End-to-End Driving
---

# Latent Chain-of-Thought World Modeling for End-to-End Driving

**arXiv**: [2512.10226v1](https://arxiv.org/abs/2512.10226) | [PDF](https://arxiv.org/pdf/2512.10226.pdf)

**ä½œè€…**: Shuhan Tan, Kashyap Chitta, Yuxiao Chen, Ran Tian, Yurong You, Yan Wang, Wenjie Luo, Yulong Cao, Philipp Krahenbuhl, Marco Pavone, Boris Ivanovic

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLatent-CoT-Driveæ¨¡åž‹ï¼Œé€šè¿‡æ½œåœ¨è¯­è¨€é“¾å¼æ€ç»´æŽ¨ç†æå‡ç«¯åˆ°ç«¯é©¾é©¶æ€§èƒ½**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `æ½œåœ¨è¯­è¨€æŽ¨ç†` `ç«¯åˆ°ç«¯é©¾é©¶` `é“¾å¼æ€ç»´` `ä¸–ç•Œå»ºæ¨¡` `å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ–‡æœ¬é“¾å¼æ€ç»´æŽ¨ç†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¯èƒ½æ•ˆçŽ‡ä¸è¶³ï¼Œå½±å“å†³ç­–è´¨é‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŠ¨ä½œå¯¹é½æ½œåœ¨ç©ºé—´ç»Ÿä¸€æŽ¨ç†ä¸Žå†³ç­–ï¼Œé€šè¿‡åŠ¨ä½œæè®®å’Œä¸–ç•Œæ¨¡åž‹ä»¤ç‰Œè¡¨è¾¾æœªæ¥ç»“æžœã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°æ›´å¿«æŽ¨ç†ã€æ›´å¥½è½¨è¿¹è´¨é‡ï¼Œå¼ºåŒ–å­¦ä¹ åŽæ”¹è¿›æ˜¾è‘—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.

