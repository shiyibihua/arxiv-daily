---
layout: default
title: Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap
---

# Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap

**arXiv**: [2512.10236v1](https://arxiv.org/abs/2512.10236) | [PDF](https://arxiv.org/pdf/2512.10236.pdf)

**ä½œè€…**: Shagnik Pal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam, Lizy K. John

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFiCCOç»†ç²’åº¦è®¡ç®—é€šä¿¡é‡å æ–¹æ³•ï¼Œåˆ©ç”¨DMAä¼˜åŒ–åˆ†å¸ƒå¼MLæ€§èƒ½**

**å…³é”®è¯**: `åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ` `è®¡ç®—é€šä¿¡é‡å ` `ç»†ç²’åº¦è°ƒåº¦` `DMAå¸è½½` `æ€§èƒ½ä¼˜åŒ–` `è®¾è®¡ç©ºé—´æŽ¢ç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é’ˆå¯¹åˆ†å¸ƒå¼MLä¸­æ•°æ®ä¾èµ–é€šä¿¡ä¸Žè®¡ç®—æš´éœ²å¯¼è‡´çš„æ€§èƒ½æŸå¤±é—®é¢˜
2. æå‡ºç»†ç²’åº¦é‡å FiCCOï¼Œæ‰©å±•è®¾è®¡ç©ºé—´å¹¶åˆ†æžæ“ä½œåˆ†è§£æ•ˆçŽ‡æŸå¤±
3. åŸºäºŽDMAå¸è½½é€šä¿¡ï¼Œå®šåˆ¶åŒ–è°ƒåº¦å®žçŽ°æœ€é«˜1.6å€åŠ é€Ÿï¼Œå¯å‘å¼å‡†ç¡®çŽ‡è¾¾81%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.

