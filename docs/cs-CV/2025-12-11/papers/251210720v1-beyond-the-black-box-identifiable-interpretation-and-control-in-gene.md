---
layout: default
title: Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality
---

# Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality

**arXiv**: [2512.10720v1](https://arxiv.org/abs/2512.10720) | [PDF](https://arxiv.org/pdf/2512.10720.pdf)

**ä½œè€…**: Lingjing Kong, Shaoan Xie, Guangyi Chen, Yuewen Sun, Xiangchen Song, Eric P. Xing, Kun Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå› æžœæœ€å°æ€§åŽŸåˆ™çš„ç”Ÿæˆæ¨¡åž‹å¯è§£é‡Šæ¡†æž¶ï¼Œå®žçŽ°å¯è¯†åˆ«æŽ§åˆ¶ä¸Žæ¦‚å¿µæå–**

**å…³é”®è¯**: `ç”Ÿæˆæ¨¡åž‹å¯è§£é‡Šæ€§` `å› æžœæœ€å°æ€§` `æ½œåœ¨å˜é‡è¯†åˆ«` `å±‚æ¬¡æ¦‚å¿µæå–` `æ¨¡åž‹æŽ§åˆ¶` `æ‰©æ•£æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç”Ÿæˆæ¨¡åž‹ä½œä¸ºé»‘ç›’é˜»ç¢ç†è§£ä¸ŽæŽ§åˆ¶ï¼ŒçŽ°æœ‰æ–¹æ³•ç¼ºä¹ç†è®ºä¿è¯
2. æ–¹æ³•è¦ç‚¹ï¼šåº”ç”¨å› æžœæœ€å°æ€§åŽŸåˆ™ï¼Œé€šè¿‡ç¨€ç–æˆ–åŽ‹ç¼©çº¦æŸå­¦ä¹ å¯è§£é‡Šçš„æ½œåœ¨è¡¨ç¤º
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ‰©æ•£è§†è§‰å’Œè‡ªå›žå½’è¯­è¨€æ¨¡åž‹ä¸­æå–å±‚æ¬¡æ¦‚å¿µå›¾ï¼Œå®žçŽ°ç»†ç²’åº¦æ¨¡åž‹å¼•å¯¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.

