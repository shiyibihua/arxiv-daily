---
layout: default
title: Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework
---

# Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework

**arXiv**: [2512.10758v1](https://arxiv.org/abs/2512.10758) | [PDF](https://arxiv.org/pdf/2512.10758.pdf)

**ä½œè€…**: Kaihua Ding

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽäº’è”é—®é¢˜çš„AIå¼¹æ€§è¯„ä¼°æ¡†æž¶ï¼Œä»¥åº”å¯¹ç”Ÿæˆå¼AIå¯¹è®¡ç®—æ•™è‚²è¯„ä¼°çš„æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `AIå¼¹æ€§è¯„ä¼°` `äº’è”é—®é¢˜è®¾è®¡` `è®¡ç®—æ•™è‚²` `å­¦æœ¯è¯šä¿¡` `ç”Ÿæˆå¼AI` `å®žè¯éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç”Ÿæˆå¼AIå‰Šå¼±ä¼ ç»Ÿæ¨¡å—åŒ–è¯„ä¼°ï¼Œå¯¼è‡´å­¦æœ¯è¯„ä¼°ä¸Žè¡Œä¸šå®žè·µè„±èŠ‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡äº’è”é—®é¢˜å’ŒåŠç»“æž„åŒ–é—®é¢˜ï¼Œå¢žå¼ºAIå¼¹æ€§ï¼ŒåŸºäºŽç†è®ºåˆ†æžå’Œå®žè¯éªŒè¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››é—¨å¤§å­¦æ•°æ®ç§‘å­¦è¯¾ç¨‹ä¸­éªŒè¯ï¼Œäº’è”é¡¹ç›®ä¸Žæ¨¡å—åŒ–è¯„ä¼°å¼ºç›¸å…³ï¼Œå‡å°‘AIè¯¯ç”¨å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.
>   We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.
>   Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.
>   Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.

