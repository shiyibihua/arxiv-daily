---
layout: default
title: Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos
---

# Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos

**arXiv**: [2512.10607v1](https://arxiv.org/abs/2512.10607) | [PDF](https://arxiv.org/pdf/2512.10607.pdf)

**ä½œè€…**: Bishoy Galoaa, Sarah Ostadabbas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTCAMæ¡†æž¶ï¼Œé€šè¿‡æ— æŸ¥è¯¢æ–¹å¼å‘çŽ°å’Œæè¿°è§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡å¼ï¼Œä»¥è§£å†³é®æŒ¡ã€ä¼ªè£…æˆ–å¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„è§†é¢‘ç†è§£é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `è¿åŠ¨æ¨¡å¼å‘çŽ°` `æ— æŸ¥è¯¢æè¿°` `ç©ºé—´å®šä½` `å¯¹æ¯”è§†è§‰-è¯­è¨€è¡¨ç¤º` `è·¨ä»»åŠ¡æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨é®æŒ¡ã€ä¼ªè£…æˆ–å¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹ï¼Œè§†é¢‘ç†è§£æ›´ä¾èµ–è¿åŠ¨åŠ¨æ€è€Œéžé™æ€å¤–è§‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è¿åŠ¨åœºæ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆå¯¹æ¯”è§†è§‰-è¯­è¨€è¡¨ç¤ºï¼Œå®žçŽ°æ— æŸ¥è¯¢çš„å¤šè¿åŠ¨æ´»åŠ¨å‘çŽ°å’Œç©ºé—´å®šä½æè¿°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MeViSåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTCAMåœ¨è§†é¢‘åˆ°æ–‡æœ¬æ£€ç´¢ã€ç©ºé—´å®šä½ç²¾åº¦å’Œå‘çŽ°ç›¸å…³è¡¨è¾¾æ–¹é¢è¡¨çŽ°ä¼˜å¼‚ï¼Œå±•ç¤ºå¼ºè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.

