---
layout: default
title: Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching
---

# Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching

**arXiv**: [2512.10379v1](https://arxiv.org/abs/2512.10379) | [PDF](https://arxiv.org/pdf/2512.10379.pdf)

**ä½œè€…**: Alberto Rota, Elena De Momi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£å¯¹æ¯”åµŒå…¥é€‚åº”æ–¹æ³•ï¼Œä»¥è§£å†³å†…çª¥é•œå›¾åƒåŒ¹é…ä¸­çš„ç‰¹å¾å¯¹åº”é—®é¢˜ã€‚**

**å…³é”®è¯**: `å†…çª¥é•œå›¾åƒåŒ¹é…` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `ç‰¹å¾åµŒå…¥é€‚åº”` `æ–°è§†å›¾åˆæˆ` `Transformerä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå†…çª¥é•œå›¾åƒåŒ¹é…å› å¼±é€è§†ã€éžæœ—ä¼¯åå°„å’Œå¯å˜å½¢è§£å‰–è€Œå›°éš¾ï¼Œä¼ ç»Ÿæ–¹æ³•æ€§èƒ½å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ–°è§†å›¾åˆæˆç”ŸæˆçœŸå®žå¯¹åº”ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–Transformerå±‚ï¼Œå¢žå¼ºDINOv2éª¨å¹²ä»¥äº§ç”Ÿç›´æŽ¥åŒ¹é…çš„åµŒå…¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SCAREDæ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒåŒ¹é…ç²¾åº¦æ›´é«˜ï¼Œæžçº¿è¯¯å·®æ›´ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness. In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation. However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques. While Deep Learning models have shown strong performance in natural scenes, their features are not inherently suited for fine-grained matching in surgical images and require targeted adaptation to meet the demands of this domain. This research presents a novel Deep Learning pipeline for establishing feature correspondences in endoscopic image pairs, alongside a self-supervised optimization framework for model training. The proposed methodology leverages a novel-view synthesis pipeline to generate ground-truth inlier correspondences, subsequently utilized for mining triplets within a contrastive learning paradigm. Through this self-supervised approach, we augment the DINOv2 backbone with an additional Transformer layer, specifically optimized to produce embeddings that facilitate direct matching through cosine similarity thresholding. Experimental evaluation demonstrates that our pipeline surpasses state-of-the-art methodologies on the SCARED datasets improved matching precision and lower epipolar error compared to the related work. The proposed framework constitutes a valuable contribution toward enabling more accurate high-level computer vision applications in surgical endoscopy.

