---
layout: default
title: Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval
---

# Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval

**arXiv**: [2512.10596v1](https://arxiv.org/abs/2512.10596) | [PDF](https://arxiv.org/pdf/2512.10596.pdf)

**ä½œè€…**: J. Xiao, Y. Guo, X. Zi, K. Thiyagarajan, C. Moreira, M. Prasad

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè®­ç»ƒå…è´¹ã€çº¯æ–‡æœ¬çš„TRSLLaVAæ¡†æž¶ï¼Œé€šè¿‡æ–‡æœ¬åˆ°æ–‡æœ¬åŒ¹é…è§£å†³é¥æ„Ÿå›¾åƒè¯­ä¹‰æ£€ç´¢ä¸­çš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ã€‚**

**å…³é”®è¯**: `é¥æ„Ÿå›¾åƒæ£€ç´¢` `è¯­ä¹‰é¸¿æ²Ÿ` `è®­ç»ƒå…è´¹æ–¹æ³•` `æ–‡æœ¬åˆ°æ–‡æœ¬åŒ¹é…` `VLMç”Ÿæˆæè¿°` `RSRTæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¥æ„Ÿå›¾åƒæ£€ç´¢å­˜åœ¨è¯­ä¹‰é¸¿æ²Ÿï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µé¢†åŸŸè®­ç»ƒä¸”ç¼ºä¹é›¶æ ·æœ¬åŸºå‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥RSRTæ•°æ®é›†ï¼Œå°†è·¨æ¨¡æ€æ£€ç´¢é‡æž„ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬åŒ¹é…ï¼Œåˆ©ç”¨VLMç”Ÿæˆæè¿°åœ¨ç»Ÿä¸€æ–‡æœ¬åµŒå…¥ç©ºé—´è¿›è¡Œæ£€ç´¢ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨RSITMDå’ŒRSICDåŸºå‡†ä¸Šï¼Œè®­ç»ƒå…è´¹æ–¹æ³•è¾¾åˆ°42.62%å¹³å‡å¬å›žçŽ‡ï¼Œè¶…è¶Šæ ‡å‡†é›¶æ ·æœ¬åŸºçº¿åŠå¤šä¸ªç›‘ç£æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.

