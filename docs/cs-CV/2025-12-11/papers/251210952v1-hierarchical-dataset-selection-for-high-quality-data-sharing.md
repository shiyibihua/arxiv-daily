---
layout: default
title: Hierarchical Dataset Selection for High-Quality Data Sharing
---

# Hierarchical Dataset Selection for High-Quality Data Sharing

**arXiv**: [2512.10952v1](https://arxiv.org/abs/2512.10952) | [PDF](https://arxiv.org/pdf/2512.10952.pdf)

**ä½œè€…**: Xiaona Zhou, Yingyan Zeng, Ran Jin, Ismini Lourentzou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDaSHæ–¹æ³•ä»¥è§£å†³å¼‚æž„æ•°æ®æ± ä¸­é«˜æ•ˆé€‰æ‹©é«˜è´¨é‡æ•°æ®é›†çš„é—®é¢˜**

**å…³é”®è¯**: `æ•°æ®é›†é€‰æ‹©` `å±‚æ¬¡åŒ–å»ºæ¨¡` `å¤šæºå­¦ä¹ ` `èµ„æºçº¦æŸä¼˜åŒ–` `å¼‚æž„æ•°æ®æ± `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¿½ç•¥æ•°æ®é›†é—´å·®å¼‚ï¼Œéš¾ä»¥åœ¨èµ„æºçº¦æŸä¸‹é€‰æ‹©æ•´ä½“æ•°æ®é›†æå‡ä¸‹æ¸¸æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šDaSHé€šè¿‡å±‚æ¬¡åŒ–å»ºæ¨¡æ•°æ®é›†å’Œç»„çº§æ•ˆç”¨ï¼Œä»Žæœ‰é™è§‚æµ‹ä¸­å®žçŽ°é«˜æ•ˆæ³›åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Digit-Fiveå’ŒDomainNetåŸºå‡†ä¸Šï¼ŒDaSHå‡†ç¡®çŽ‡æå‡è¾¾26.2%ï¼Œä¸”æŽ¢ç´¢æ­¥éª¤æ˜¾è‘—å‡å°‘

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

