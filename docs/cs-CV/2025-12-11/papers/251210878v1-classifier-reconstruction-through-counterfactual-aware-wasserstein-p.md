---
layout: default
title: Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes
---

# Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes

**arXiv**: [2512.10878v1](https://arxiv.org/abs/2512.10878) | [PDF](https://arxiv.org/pdf/2512.10878.pdf)

**ä½œè€…**: Xuan Zhao, Zhuo Cao, Arya Bangun, Hanno Scharr, Ira Assent

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåäº‹å®žæ„ŸçŸ¥WassersteinåŽŸåž‹çš„åˆ†ç±»å™¨é‡å»ºæ–¹æ³•ï¼Œä»¥æå‡æœ‰é™æ•°æ®ä¸‹çš„æ¨¡åž‹å¤åˆ¶æ•ˆæžœ**

**å…³é”®è¯**: `åäº‹å®žè§£é‡Š` `æ¨¡åž‹é‡å»º` `WassersteinåŽŸåž‹` `å†³ç­–è¾¹ç•Œ` `æœ‰é™æ•°æ®å­¦ä¹ ` `ä»£ç†æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹ï¼Œåäº‹å®žæ ·æœ¬ç”¨äºŽæ¨¡åž‹é‡å»ºæ—¶æ˜“å¯¼è‡´å†³ç­–è¾¹ç•Œåç§»ï¼Œå½±å“ä»£ç†æ¨¡åž‹ä¿çœŸåº¦
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆåŽŸå§‹æ•°æ®ä¸Žåäº‹å®žæ ·æœ¬ï¼Œé€šè¿‡Wassersteiné‡å¿ƒè¿‘ä¼¼ç±»åŽŸåž‹ï¼Œä¿ç•™ç±»åˆ†å¸ƒç»“æž„
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šæ•°æ®é›†å®žéªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æé«˜ä»£ç†ä¸Žç›®æ ‡æ¨¡åž‹é—´ä¿çœŸåº¦ï¼ŒéªŒè¯å…¶æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.

