---
layout: default
title: PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction
---

# PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction

**arXiv**: [2512.10888v1](https://arxiv.org/abs/2512.10888) | [PDF](https://arxiv.org/pdf/2512.10888.pdf)

**ä½œè€…**: Brandon Smock, Valerie Faucon-Morin, Max Sokolov, Libin Liang, Tayyibah Khanam, Maury Courtland

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPubTables-v2æ•°æ®é›†ä»¥è§£å†³è§†è§‰æ–‡æ¡£ç†è§£ä¸­è¡¨æ ¼æå–çš„æ•°æ®ç¼ºä¹é—®é¢˜**

**å…³é”®è¯**: `è¡¨æ ¼æå–` `è§†è§‰æ–‡æ¡£ç†è§£` `å¤šé¡µè¡¨æ ¼è¯†åˆ«` `æ•°æ®é›†æž„å»º` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¡¨æ ¼æå–ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œé˜»ç¢å…¨é¡µå’Œå¤šé¡µè¡¨æ ¼æå–æ–¹æ³•çš„å‘å±•
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ›å»ºPubTables-v2æ•°æ®é›†ï¼Œæ”¯æŒå…¨é¡µå’Œå¤šé¡µè¡¨æ ¼ç»“æž„è¯†åˆ«ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œå¹¶åŸºäºŽæ•°æ®é›†å¼€å‘Page-Object Table Transformeræ‰©å±•æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.

