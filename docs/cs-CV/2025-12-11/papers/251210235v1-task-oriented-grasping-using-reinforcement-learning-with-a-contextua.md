---
layout: default
title: Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine
---

# Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine

**arXiv**: [2512.10235v1](https://arxiv.org/abs/2512.10235) | [PDF](https://arxiv.org/pdf/2512.10235.pdf)

**ä½œè€…**: Hui Li, Akhlak Uz Zaman, Fujian Yan, Hongsheng He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆä¸Šä¸‹æ–‡å¥–åŠ±æœºå™¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æž¶ä»¥è§£å†³ä»»åŠ¡å¯¼å‘æŠ“å–é—®é¢˜**

**å…³é”®è¯**: `ä»»åŠ¡å¯¼å‘æŠ“å–` `å¼ºåŒ–å­¦ä¹ ` `ä¸Šä¸‹æ–‡å¥–åŠ±æœºå™¨` `çŠ¶æ€æŠ½è±¡` `è½¬ç§»å¥–åŠ±` `æœºå™¨äººæŠ“å–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»»åŠ¡å¯¼å‘æŠ“å–ä»»åŠ¡å¤æ‚ï¼ŒçŠ¶æ€-åŠ¨ä½œç©ºé—´å¤§ï¼Œå­¦ä¹ æ•ˆçŽ‡ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡å¥–åŠ±æœºå™¨åˆ†è§£ä»»åŠ¡ä¸ºå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡å…³è”ç‰¹å®šä¸Šä¸‹æ–‡ï¼ˆå¥–åŠ±å‡½æ•°ã€åŠ¨ä½œç©ºé—´ã€çŠ¶æ€æŠ½è±¡ï¼‰ï¼Œå¹¶å¼•å…¥è½¬ç§»å¥–åŠ±å¼•å¯¼é˜¶æ®µåºåˆ—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­è¾¾åˆ°95%æˆåŠŸçŽ‡ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼›åœ¨çœŸå®žæœºå™¨äººä¸Šå®žçŽ°83.3%æˆåŠŸçŽ‡ï¼Œå±•ç¤ºé«˜å‡†ç¡®æ€§å’Œå­¦ä¹ æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.

