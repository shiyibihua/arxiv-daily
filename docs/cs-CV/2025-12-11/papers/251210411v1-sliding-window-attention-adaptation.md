---
layout: default
title: Sliding Window Attention Adaptation
---

# Sliding Window Attention Adaptation

**arXiv**: [2512.10411v1](https://arxiv.org/abs/2512.10411) | [PDF](https://arxiv.org/pdf/2512.10411.pdf)

**ä½œè€…**: Yijiong Yu, Jiale Liu, Qingyun Wu, Huazheng Wang, Ji Pei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚åº”æ–¹æ³•ï¼Œä»¥ä½Žæˆæœ¬é€‚é…å…¨æ³¨æ„åŠ›é¢„è®­ç»ƒæ¨¡åž‹è‡³é•¿ä¸Šä¸‹æ–‡æŽ¨ç†ã€‚**

**å…³é”®è¯**: `æ»‘åŠ¨çª—å£æ³¨æ„åŠ›` `é•¿ä¸Šä¸‹æ–‡æŽ¨ç†` `æ³¨æ„åŠ›æœºåˆ¶é€‚é…` `Transformeræ¨¡åž‹` `æ€§èƒ½æ•ˆçŽ‡æƒè¡¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨æ³¨æ„åŠ›é¢„è®­ç»ƒæ¨¡åž‹ç›´æŽ¥åº”ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å¯¼è‡´é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆäº”ç§ç­–ç•¥ï¼Œå¦‚ä»…é¢„å¡«å……æ—¶åº”ç”¨æ»‘åŠ¨çª—å£ã€ä¿ç•™sinkä»¤ç‰Œã€å±‚é—´äº¤é”™ç­‰ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç‰¹å®šç»„åˆèƒ½æœ‰æ•ˆæ¢å¤æ€§èƒ½ï¼Œæä¾›ä¸åŒåœºæ™¯çš„æŽ¨èé…ç½®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

