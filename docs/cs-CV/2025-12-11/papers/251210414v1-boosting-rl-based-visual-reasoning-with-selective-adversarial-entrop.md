---
layout: default
title: Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention
---

# Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention

**arXiv**: [2512.10414v1](https://arxiv.org/abs/2512.10414) | [PDF](https://arxiv.org/pdf/2512.10414.pdf)

**ä½œè€…**: Yang Yu, Zhuangzhuang Chen, Siqi Wang, Lanqing Li, Xiaomeng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€‰æ‹©æ€§å¯¹æŠ—ç†µå¹²é¢„ä»¥å¢žå¼ºåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„è§†è§‰æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç†µå¹²é¢„` `å¯¹æŠ—æ”»å‡»` `è§†è§‰æŽ¨ç†` `ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡åž‹å¾®è°ƒæ–¹æ³•å¿½è§†é‡‡æ ·é˜¶æ®µçš„ç†µå¹²é¢„ï¼Œå½±å“å“åº”å¤šæ ·æ€§å’Œæ€§èƒ½
2. æå‡ºSaEIæ–¹æ³•ï¼Œé€šè¿‡ç†µå¼•å¯¼å¯¹æŠ—é‡‡æ ·å’Œä»¤ç‰Œé€‰æ‹©æ€§ç†µè®¡ç®—ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ é‡‡æ ·ä¸­å¢žå¼ºç­–ç•¥ç†µ
3. åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–æ•°æ®é›†ä¸Šå®žéªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æå‡ç­–ç•¥æŽ¢ç´¢å’ŒæŽ¨ç†èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL-based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing studies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ignore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the diversity of responses. In this paper, we propose Selective-adversarial Entropy Intervention, namely SaEI, which enhances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the entropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formulates the entropy of sampled responses as an adversarial objective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger answer space during RL sampling. Then, we propose token-selective entropy computation (TsEC) to maximize the effectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

