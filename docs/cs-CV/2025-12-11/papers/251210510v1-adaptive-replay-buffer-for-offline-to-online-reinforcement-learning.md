---
layout: default
title: Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning
---

# Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning

**arXiv**: [2512.10510v1](https://arxiv.org/abs/2512.10510) | [PDF](https://arxiv.org/pdf/2512.10510.pdf)

**ä½œè€…**: Chihyeon Song, Jaewoo Lee, Jinkyoo Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”å›žæ”¾ç¼“å†²åŒºä»¥è§£å†³ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æ•°æ®å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”å›žæ”¾ç¼“å†²åŒº` `ç­–ç•¥ä¸€è‡´æ€§` `æ•°æ®é‡‡æ ·` `D4RLåŸºå‡†æµ‹è¯•` `å­¦ä¹ ç¨³å®šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­å›ºå®šæ•°æ®æ··åˆæ¯”éš¾ä»¥å¹³è¡¡æ—©æœŸå­¦ä¹ ç¨³å®šæ€§ä¸Žæ¸è¿›æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽè½»é‡çº§'ç­–ç•¥ä¸€è‡´æ€§'æŒ‡æ ‡åŠ¨æ€ä¼˜å…ˆé‡‡æ ·æ•°æ®ï¼Œæ— éœ€å¤æ‚å­¦ä¹ è¿‡ç¨‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆç¼“è§£æ—©æœŸæ€§èƒ½ä¸‹é™å¹¶æ˜¾è‘—æå‡æœ€ç»ˆæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

