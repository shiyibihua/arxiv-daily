---
layout: default
title: Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation
---

# Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation

**arXiv**: [2512.10734v1](https://arxiv.org/abs/2512.10734) | [PDF](https://arxiv.org/pdf/2512.10734.pdf)

**ä½œè€…**: Rebekka GÃ¶rge, Sujan Sai Gannamaneni, Tabea Naeven, Hammam Abdelwahab, HÃ©ctor Allende-Cid, Armin B. Cremers, Lennard Helmer, Michael Mock, Anna Schmitz, Songkai Xue, Elif Yildirir, Maximilian Poretschkin, Stefan Wrobel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯æ‰©å±•çš„æ–‡æœ¬æ•°æ®åè§æ£€æµ‹ä¸Žç¼“è§£ç®¡é“ï¼Œç”¨äºŽå‡å°‘å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒæ•°æ®ä¸­çš„åè§ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬æ•°æ®åè§æ£€æµ‹` `è¡¨ç¤ºåè§é‡åŒ–` `åˆ»æ¿å°è±¡è¿‡æ»¤` `åäº‹å®žæ•°æ®å¢žå¼º` `å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒ` `åè§åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬æ•°æ®å­˜åœ¨è¡¨ç¤ºåè§å’Œåˆ»æ¿å°è±¡ï¼Œç¼ºä¹å®žè·µæŒ‡å¯¼ä»¥ç¬¦åˆæ³•è§„è¦æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽLLMç”Ÿæˆè¯è¡¨æ£€æµ‹ç¾¤ä½“æ ‡ç­¾ï¼Œé‡åŒ–è¡¨ç¤ºåè§ï¼Œè¿‡æ»¤åˆ»æ¿å°è±¡ï¼Œå¹¶é€šè¿‡æ•°æ®å¢žå¼ºè¡¥å¿åè§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ€§åˆ«ã€å®—æ•™å’Œå¹´é¾„ç¤ºä¾‹ä¸­éªŒè¯äº†æ•°æ®åŽ»åè§æ•ˆæžœï¼Œä½†æ¨¡åž‹å¾®è°ƒåŽåè§åŸºå‡†è¡¨çŽ°ä¸ä¸€è‡´ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

