---
layout: default
title: Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks
---

# Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks

**arXiv**: [2512.10562v1](https://arxiv.org/abs/2512.10562) | [PDF](https://arxiv.org/pdf/2512.10562.pdf)

**ä½œè€…**: Meher Md Saad

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽéª¨æž¶ç¼–ç å™¨çš„å°‘æ ·æœ¬åŽŸåž‹ç½‘ç»œï¼Œä»¥è§£å†³å­¤ç«‹æ‰‹è¯­è¯†åˆ«ä¸­çš„æ•°æ®ç¨€ç¼ºå’Œé•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚**

**å…³é”®è¯**: `å­¤ç«‹æ‰‹è¯­è¯†åˆ«` `å°‘æ ·æœ¬å­¦ä¹ ` `åŽŸåž‹ç½‘ç»œ` `éª¨æž¶ç¼–ç ` `åº¦é‡å­¦ä¹ ` `é›¶æ ·æœ¬æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå­¤ç«‹æ‰‹è¯­è¯†åˆ«å—é™äºŽæ•°æ®ç¨€ç¼ºå’Œè¯æ±‡é•¿å°¾åˆ†å¸ƒï¼Œä¼ ç»Ÿåˆ†ç±»æ–¹æ³•æ˜“è¿‡æ‹Ÿåˆä¸”æ³›åŒ–å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å°‘æ ·æœ¬åŽŸåž‹ç½‘ç»œï¼Œç»“åˆST-GCNå’Œå¤šå°ºåº¦æ—¶é—´èšåˆæ¨¡å—ï¼Œå­¦ä¹ è¯­ä¹‰åº¦é‡ç©ºé—´è¿›è¡ŒåŠ¨æ€åˆ†ç±»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨WLASLæ•°æ®é›†ä¸ŠTop-1å‡†ç¡®çŽ‡43.75%ï¼Œä¼˜äºŽåŸºçº¿è¶…13%ï¼Œå¹¶åœ¨SignASLä¸Šå®žçŽ°è¿‘30%é›¶æ ·æœ¬æ³›åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.

