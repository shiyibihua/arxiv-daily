---
layout: default
title: Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots
---

# Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots

**arXiv**: [2512.10477v1](https://arxiv.org/abs/2512.10477) | [PDF](https://arxiv.org/pdf/2512.10477.pdf)

**ä½œè€…**: Timur Ishuov, Michele Folgheraiter, Madi Nurmanov, Goncalo Gordo, RichÃ¡rd Farkas, JÃ³zsef Dombi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSymphonyç®—æ³•ï¼Œé€šè¿‡å½’ä¸€åŒ–æ ¡å‡†ä¼˜åŠ¿ä¸Žç¡®å®šæ€§ç­–ç•¥ï¼Œå®žçŽ°äººå½¢æœºå™¨äººå®‰å…¨é«˜æ•ˆè®­ç»ƒã€‚**

**å…³é”®è¯**: `äººå½¢æœºå™¨äººæŽ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ç®—æ³•` `æ ·æœ¬æ•ˆçŽ‡ä¼˜åŒ–` `åŠ¨ä½œå®‰å…¨æ€§` `ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦` `ç»éªŒå›žæ”¾ç¼“å†²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººä»Žé›¶è®­ç»ƒéœ€é«˜æ ·æœ¬æ•ˆçŽ‡ä¸Žå®‰å…¨æ€§ï¼Œé¿å…æœºæ¢°æŸä¼¤ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆSwaddlingæ­£åˆ™åŒ–é™åˆ¶åŠ¨ä½œå¼ºåº¦ï¼Œä½¿ç”¨Fading Replay Bufferä¼˜åŒ–æ ·æœ¬é‡‡æ ·ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”éšæœºç®—æ³•ï¼Œå‡å°‘å™ªå£°å¹¶æå‡åŠ¨ä½œå®‰å…¨æ€§ï¼Œå®žçŽ°å•æ¬¡æ›´æ–°Actorä¸ŽCriticã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.

