---
layout: default
title: Guided Transfer Learning for Discrete Diffusion Models
---

# Guided Transfer Learning for Discrete Diffusion Models

**arXiv**: [2512.10877v1](https://arxiv.org/abs/2512.10877) | [PDF](https://arxiv.org/pdf/2512.10877.pdf)

**ä½œè€…**: Julian Kleutgens, Claudio Battiloro, Lingkai Kong, Benjamin Grewe, Francesca Dominici, Mauricio Tec

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼•å¯¼å¼è¿ç§»å­¦ä¹ ä»¥è§£å†³ç¦»æ•£æ‰©æ•£æ¨¡åž‹åœ¨æ–°é¢†åŸŸé€‚åº”ä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜**

**å…³é”®è¯**: `ç¦»æ•£æ‰©æ•£æ¨¡åž‹` `è¿ç§»å­¦ä¹ ` `å¼•å¯¼é‡‡æ ·` `è¯­è¨€å»ºæ¨¡` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»æ•£æ‰©æ•£æ¨¡åž‹ä¾èµ–å¤§æ•°æ®è®­ç»ƒï¼Œè¿ç§»å­¦ä¹ éœ€å¾®è°ƒæ¨¡åž‹ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”ä¸å®žç”¨
2. åŸºäºŽè¿žç»­æ‰©æ•£çš„æ¯”çŽ‡æ–¹æ³•ï¼Œæå‡ºå¼•å¯¼å¼è¿ç§»å­¦ä¹ ï¼Œæ— éœ€ä¿®æ”¹é¢„è®­ç»ƒåŽ»å™ªå™¨å³å¯é‡‡æ ·ç›®æ ‡åˆ†å¸ƒ
3. è¯„ä¼°æ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨åºåˆ—æ•°æ®å’Œè¯­è¨€å»ºæ¨¡ä¸­æœ‰æ•ˆï¼Œå¹¶é€šè¿‡é«˜æ•ˆé‡‡æ ·å™¨é™ä½Žè®¡ç®—å¼€é”€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

