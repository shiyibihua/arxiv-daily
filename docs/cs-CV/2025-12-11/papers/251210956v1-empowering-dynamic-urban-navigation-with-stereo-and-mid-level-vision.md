---
layout: default
title: Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision
---

# Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision

**arXiv**: [2512.10956v1](https://arxiv.org/abs/2512.10956) | [PDF](https://arxiv.org/pdf/2512.10956.pdf)

**ä½œè€…**: Wentao Zhou, Xuweiyi Chen, Vignesh Rajagopal, Jeffrey Chen, Rohan Chandra, Zezhou Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStereoWalkerï¼Œç»“åˆç«‹ä½“è§†è§‰ä¸Žä¸­å±‚è§†è§‰æ¨¡å—ä»¥æå‡åŠ¨æ€åŸŽå¸‚å¯¼èˆªæ€§èƒ½**

**å…³é”®è¯**: `ç«‹ä½“è§†è§‰å¯¼èˆª` `ä¸­å±‚è§†è§‰æ¨¡å—` `åŠ¨æ€åœºæ™¯ç†è§£` `æ·±åº¦ä¼°è®¡` `åƒç´ è·Ÿè¸ª` `æœºå™¨äººå¯¼èˆªåŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šçŽ°æœ‰å¯¼èˆªåŸºç¡€æ¨¡åž‹ä¾èµ–å•ç›®è§†è§‰ï¼Œå¿½ç•¥ä¸­å±‚è§†è§‰å…ˆéªŒï¼Œåœ¨åŠ¨æ€åœºæ™¯ä¸­æ•ˆçŽ‡ä½Žä¸‹ä¸”å—æ·±åº¦å°ºåº¦æ¨¡ç³Šé™åˆ¶
2. æ–¹æ³•ï¼šå¼•å…¥ç«‹ä½“è¾“å…¥è§£å†³æ·±åº¦æ¨¡ç³Šï¼Œå¹¶é›†æˆæ·±åº¦ä¼°è®¡å’Œå¯†é›†åƒç´ è·Ÿè¸ªç­‰ä¸­å±‚è§†è§‰æ¨¡å—ä»¥å¢žå¼ºå‡ ä½•ä¸ŽåŠ¨æ€ç†è§£
3. æ•ˆæžœï¼šä»…éœ€1.5%è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œå…¨æ•°æ®ä¸‹è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œç«‹ä½“è§†è§‰ä¼˜äºŽå•ç›®è¾“å…¥

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.
>   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.

