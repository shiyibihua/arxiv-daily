---
layout: default
title: MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos
---

# MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos

**arXiv**: [2512.10408v1](https://arxiv.org/abs/2512.10408) | [PDF](https://arxiv.org/pdf/2512.10408.pdf)

**ä½œè€…**: Qiyue Sun, Tailin Chen, Yinghui Zhang, Yuchen Zhang, Jiangbei Yue, Jianbo Jiao, Zeyu Fu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMultiHateLocæ¡†æž¶ä»¥è§£å†³å¼±ç›‘ç£ä¸‹åœ¨çº¿è§†é¢‘ä¸­å¤šæ¨¡æ€ä»‡æ¨å†…å®¹çš„æ—¶é—´å®šä½é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹` `å¼±ç›‘ç£æ—¶é—´å®šä½` `è·¨æ¨¡æ€èžåˆ` `æ—¶åºå»ºæ¨¡` `è§†é¢‘å†…å®¹åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç ”ç©¶å¤šå…³æ³¨è§†é¢‘çº§åˆ†ç±»ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€ä»‡æ¨å†…å®¹æ—¶é—´å®šä½çš„å¼±ç›‘ç£æ–¹æ³•ï¼Œéš¾ä»¥æ•æ‰è·¨æ¨¡æ€å’Œæ—¶åºåŠ¨æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ¨¡æ€æ„ŸçŸ¥æ—¶åºç¼–ç å™¨ã€åŠ¨æ€è·¨æ¨¡æ€èžåˆä¸Žå¯¹æ¯”å¯¹é½ç­–ç•¥ï¼Œä»¥åŠæ¨¡æ€æ„ŸçŸ¥MILç›®æ ‡ï¼Œå®žçŽ°ç»†ç²’åº¦å¸§çº§é¢„æµ‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HateMMå’ŒMultiHateClipæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œåœ¨å®šä½ä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œä»…ä¾èµ–ç²—æ ‡ç­¾ç”Ÿæˆå¯è§£é‡Šé¢„æµ‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.

