---
layout: default
title: RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds
---

# RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds

**arXiv**: [2512.10376v1](https://arxiv.org/abs/2512.10376) | [PDF](https://arxiv.org/pdf/2512.10376.pdf)

**ä½œè€…**: Jingyun Fu, Zhiyu Xiang, Na Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRaLiFlowæ¡†æž¶ï¼Œé€šè¿‡4Dé›·è¾¾ä¸ŽLiDARèžåˆè§£å†³åœºæ™¯æµä¼°è®¡é—®é¢˜**

**å…³é”®è¯**: `åœºæ™¯æµä¼°è®¡` `å¤šæ¨¡æ€èžåˆ` `4Dé›·è¾¾` `LiDARç‚¹äº‘` `è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š4Dé›·è¾¾ä¸ŽLiDARèžåˆåœ¨åœºæ™¯æµä¼°è®¡ä¸­æœªæŽ¢ç´¢ï¼Œé›·è¾¾æ•°æ®å™ªå£°å¤§ã€åˆ†è¾¨çŽ‡ä½Žä¸”ç¨€ç–
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡åŠ¨æ€æ„ŸçŸ¥åŒå‘è·¨æ¨¡æ€èžåˆæ¨¡å—å’ŒæŸå¤±å‡½æ•°ï¼Œå®žçŽ°é›·è¾¾ä¸ŽLiDARçš„æœ‰æ•ˆèžåˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æž„å»ºçš„æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽå•æ¨¡æ€æ–¹æ³•ï¼Œæå‡åŠ¨æ€å‰æ™¯åŒºåŸŸçš„ä¼°è®¡ç²¾åº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.

