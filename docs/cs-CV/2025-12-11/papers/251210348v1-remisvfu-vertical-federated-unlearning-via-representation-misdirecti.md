---
layout: default
title: REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature
---

# REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature

**arXiv**: [2512.10348v1](https://arxiv.org/abs/2512.10348) | [PDF](https://arxiv.org/pdf/2512.10348.pdf)

**ä½œè€…**: Wenhan Wu, Zhili He, Huanghuang Liang, Yili Gong, Jiawei Jiang, Chuang Hu, Dazhao Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREMISVFUæ¡†æž¶ï¼Œé€šè¿‡è¡¨ç¤ºè¯¯å¯¼å®žçŽ°åž‚ç›´è”é‚¦å­¦ä¹ ä¸­çš„å¿«é€Ÿå®¢æˆ·ç«¯çº§é—å¿˜**

**å…³é”®è¯**: `åž‚ç›´è”é‚¦å­¦ä¹ ` `è”é‚¦é—å¿˜` `è¡¨ç¤ºè¯¯å¯¼` `å®¢æˆ·ç«¯çº§é—å¿˜` `éšç§ä¿æŠ¤` `æ¨¡åž‹æ•ˆç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é’ˆå¯¹åž‚ç›´è”é‚¦å­¦ä¹ ï¼ˆVFLï¼‰ä¸­æ•°æ®æŒ‰ç‰¹å¾åˆ’åˆ†ï¼ŒçŽ°æœ‰é—å¿˜æ–¹æ³•æ— æ•ˆçš„é—®é¢˜
2. é‡‡ç”¨è¡¨ç¤ºè¯¯å¯¼æŠ€æœ¯ï¼Œå°†é—å¿˜æ–¹ç¼–ç å™¨è¾“å‡ºåç¼©è‡³å•ä½çƒé¢éšæœºé”šç‚¹ï¼Œåˆ‡æ–­ç‰¹å¾ä¸Žå…¨å±€æ¨¡åž‹çš„ç»Ÿè®¡å…³è”
3. å®žéªŒæ˜¾ç¤ºï¼ŒREMISVFUèƒ½æŠ‘åˆ¶åŽé—¨æ”»å‡»è‡³è‡ªç„¶ç±»å…ˆéªŒæ°´å¹³ï¼Œä»…ç‰ºç‰²çº¦2.5%çš„å¹²å‡€å‡†ç¡®çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

