---
layout: default
title: TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning
---

# TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning

**arXiv**: [2512.10419v1](https://arxiv.org/abs/2512.10419) | [PDF](https://arxiv.org/pdf/2512.10419.pdf)

**ä½œè€…**: Phu Pham, Damon Conover, Aniket Bera

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTransLocNetï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›ä¸Žå¯¹æ¯”å­¦ä¹ è§£å†³åœ°é¢LiDARä¸Žç©ºä¸­å›¾åƒé—´çš„å®šä½éš¾é¢˜ã€‚**

**å…³é”®è¯**: `è·¨æ¨¡æ€å®šä½` `æ³¨æ„åŠ›æœºåˆ¶` `å¯¹æ¯”å­¦ä¹ ` `LiDAR-å›¾åƒèžåˆ` `ç©ºä¸­-åœ°é¢å®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ°é¢LiDARä¸Žç©ºä¸­å›¾åƒé—´å­˜åœ¨å¤§è§†è§’å’Œæ¨¡æ€å·®å¼‚ï¼Œå¯¼è‡´ç©ºä¸­-åœ°é¢å®šä½å›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›èžåˆLiDARå‡ ä½•ä¸Žç©ºä¸­è¯­ä¹‰ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ ä¼˜åŒ–å…±äº«åµŒå…¥ç©ºé—´ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CARLAå’ŒKITTIä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå®šä½è¯¯å·®é™ä½Žè¾¾63%ï¼Œå®žçŽ°äºšç±³ã€äºšåº¦ç²¾åº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.

