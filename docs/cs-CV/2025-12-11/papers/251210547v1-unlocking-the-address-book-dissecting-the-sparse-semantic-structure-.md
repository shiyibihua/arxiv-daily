---
layout: default
title: Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders
---

# Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders

**arXiv**: [2512.10547v1](https://arxiv.org/abs/2512.10547) | [PDF](https://arxiv.org/pdf/2512.10547.pdf)

**ä½œè€…**: Qingsen Ma, Dianyun Wang, Jiaming Lyu, Yaoye Wang, Lechen Ning, Sujie Zhu, Zhenbo Xu, Liuyu Xiang, Huining Li, Huijia Wu, Zhaofeng He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTA-Attentionæ¡†æž¶ï¼Œåˆ©ç”¨Top-Kç¨€ç–è‡ªç¼–ç å™¨åˆ†è§£LLMé”®å€¼ç¼“å­˜ä¸ºå¯è§£é‡Šè¯­ä¹‰åŽŸå­ï¼Œä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `é”®å€¼ç¼“å­˜` `ç¨€ç–è‡ªç¼–ç å™¨` `é•¿ä¸Šä¸‹æ–‡æ¨¡åž‹` `è¯­ä¹‰åˆ†è§£` `æ³¨æ„åŠ›æœºåˆ¶` `æœºåˆ¶å¯è§£é‡Šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé”®å€¼ç¼“å­˜æ˜¯é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡åž‹çš„ä¸»è¦å†…å­˜ç“¶é¢ˆï¼Œé€šå¸¸è¢«è§†ä¸ºä¸é€æ˜Žçš„æ•°å€¼å¼ é‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨Top-Kç¨€ç–è‡ªç¼–ç å™¨åˆ†è§£é”®å€¼ç¼“å­˜ï¼Œæ­ç¤ºé”®å€¼ä¸å¯¹ç§°æ€§ï¼Œå¹¶å¼•å…¥åŒé¢„ç®—ç­–ç•¥é€‰æ‹©æ€§ä¿ç•™è¯­ä¹‰ç»„ä»¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ¨¡åž‹ä¸ŠéªŒè¯ï¼Œè¯­ä¹‰é‡å»ºä¿æŒå›°æƒ‘åº¦å’Œé›¶æ ·æœ¬æ€§èƒ½ï¼Œå¼¥åˆæœºåˆ¶å¯è§£é‡Šæ€§ä¸Žæ³¨æ„åŠ›å»ºæ¨¡çš„å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

