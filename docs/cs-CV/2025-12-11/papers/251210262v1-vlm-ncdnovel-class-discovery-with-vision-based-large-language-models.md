---
layout: default
title: VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models
---

# VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models

**arXiv**: [2512.10262v1](https://arxiv.org/abs/2512.10262) | [PDF](https://arxiv.org/pdf/2512.10262.pdf)

**ä½œè€…**: Yuetong Su, Baoguo Wei, Xinyu Wang, Xu Li, Lixin Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLM-NCDæ¡†æž¶ï¼Œèžåˆè§†è§‰-æ–‡æœ¬è¯­ä¹‰ä¸ŽåŽŸåž‹èšç±»ï¼Œä»¥è§£å†³å›¾åƒæ–°ç±»å‘çŽ°ä¸­ç‰¹å¾åˆ¤åˆ«æ€§ä¸è¶³å’Œé•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚**

**å…³é”®è¯**: `æ–°ç±»å‘çŽ°` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `åŽŸåž‹èšç±»` `é•¿å°¾åˆ†å¸ƒ` `è¯­ä¹‰èžåˆ` `åŒé˜¶æ®µå‘çŽ°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒæ–°ç±»å‘çŽ°ä¾èµ–è§†è§‰ç‰¹å¾ï¼Œå­˜åœ¨ç‰¹å¾åˆ¤åˆ«æ€§ä¸è¶³å’Œæ•°æ®é•¿å°¾åˆ†å¸ƒé™åˆ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è”åˆä¼˜åŒ–å·²çŸ¥ç±»å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾å»ºæ¨¡èšç±»ä¸­å¿ƒä¸Žè¯­ä¹‰åŽŸåž‹ï¼Œé‡‡ç”¨åŒé˜¶æ®µå‘çŽ°æœºåˆ¶åŠ¨æ€åˆ†ç¦»æ ·æœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-100æ•°æ®é›†ä¸Šï¼ŒæœªçŸ¥ç±»å‡†ç¡®çŽ‡æå‡è¾¾25.3%ï¼Œé¦–æ¬¡å±•ç¤ºå¯¹é•¿å°¾åˆ†å¸ƒçš„é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Novel Class Discovery aims to utilise prior knowledge of known classes to classify and discover unknown classes from unlabelled data. Existing NCD methods for images primarily rely on visual features, which suffer from limitations such as insufficient feature discriminability and the long-tail distribution of data. We propose LLM-NCD, a multimodal framework that breaks this bottleneck by fusing visual-textual semantics and prototype guided clustering. Our key innovation lies in modelling cluster centres and semantic prototypes of known classes by jointly optimising known class image and text features, and a dualphase discovery mechanism that dynamically separates known or novel samples via semantic affinity thresholds and adaptive clustering. Experiments on the CIFAR-100 dataset show that compared to the current methods, this method achieves up to 25.3% improvement in accuracy for unknown classes. Notably, our method shows unique resilience to long tail distributions, a first in NCD literature.

