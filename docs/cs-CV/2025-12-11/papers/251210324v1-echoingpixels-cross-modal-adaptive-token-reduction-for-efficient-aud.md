---
layout: default
title: EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs
---

# EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs

**arXiv**: [2512.10324v1](https://arxiv.org/abs/2512.10324) | [PDF](https://arxiv.org/pdf/2512.10324.pdf)

**ä½œè€…**: Chao Gong, Depeng Wang, Zhipeng Wei, Ya Guo, Huijia Zhu, Jingjing Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEchoingPixelsæ¡†æž¶ï¼Œé€šè¿‡è·¨æ¨¡æ€è‡ªé€‚åº”ä»¤ç‰Œå‡å°‘è§£å†³éŸ³é¢‘-è§†è§‰å¤§è¯­è¨€æ¨¡åž‹çš„è®¡ç®—æ•ˆçŽ‡ç“¶é¢ˆã€‚**

**å…³é”®è¯**: `éŸ³é¢‘-è§†è§‰å¤§è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‡å°‘` `è·¨æ¨¡æ€äº¤äº’` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–` `è‡ªé€‚åº”é¢„ç®—åˆ†é…` `æ—¶é—´å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. éŸ³é¢‘-è§†è§‰å¤§è¯­è¨€æ¨¡åž‹é¢ä¸´å¤§é‡éŸ³é¢‘å’Œè§†é¢‘ä»¤ç‰Œå¯¼è‡´çš„è®¡ç®—å¼€é”€è¿‡é«˜é—®é¢˜ï¼ŒçŽ°æœ‰å•æ¨¡æ€ä»¤ç‰Œå‡å°‘æ–¹æ³•æ— æ³•åˆ©ç”¨è·¨æ¨¡æ€ååŒã€‚
2. æ ¸å¿ƒæ¨¡å—Cross-Modal Semantic Sieveï¼ˆCS2ï¼‰å®žçŽ°æ—©æœŸéŸ³é¢‘-è§†è§‰äº¤äº’ï¼Œä»Žè”åˆä»¤ç‰Œæ± ä¸­è‡ªé€‚åº”å‡å°‘ä»¤ç‰Œï¼Œå¹¶è®¾è®¡Sync-RoPEä¿æŒç¨€ç–ä»¤ç‰Œçš„æ—¶é—´å…³ç³»ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œä»…ä½¿ç”¨5-20%åŽŸå§‹ä»¤ç‰Œå³å¯è¾¾åˆ°åŸºçº¿æ€§èƒ½ï¼Œå®žçŽ°2-3å€åŠ é€Ÿå’Œå†…å­˜å‡å°‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.

