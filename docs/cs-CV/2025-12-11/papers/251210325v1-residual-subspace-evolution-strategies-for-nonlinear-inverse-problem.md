---
layout: default
title: Residual subspace evolution strategies for nonlinear inverse problems
---

# Residual subspace evolution strategies for nonlinear inverse problems

**arXiv**: [2512.10325v1](https://arxiv.org/abs/2512.10325) | [PDF](https://arxiv.org/pdf/2512.10325.pdf)

**ä½œè€…**: Francesco Alemanno

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ®‹å·®å­ç©ºé—´è¿›åŒ–ç­–ç•¥ä»¥è§£å†³éžçº¿æ€§é€†é—®é¢˜ä¸­å™ªå£°ã€ä¸å¯å¾®æˆ–æ˜‚è´µæ®‹å·®è¯„ä¼°çš„æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `éžçº¿æ€§é€†é—®é¢˜` `æ— å¯¼æ•°ä¼˜åŒ–` `è¿›åŒ–ç­–ç•¥` `æ®‹å·®ä»£ç†` `æœ€å°äºŒä¹˜æ±‚è§£` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šéžçº¿æ€§é€†é—®é¢˜å¸¸æ¶‰åŠå™ªå£°ã€ä¸å¯å¾®æˆ–è®¡ç®—æ˜‚è´µçš„æ®‹å·®è¯„ä¼°ï¼Œå¯¼è‡´åŸºäºŽé›…å¯æ¯”çŸ©é˜µçš„æ±‚è§£å™¨ä¸å¯é ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šRSESé€šè¿‡é‡‡æ ·é«˜æ–¯æŽ¢é’ˆæž„å»ºæ®‹å·®ä»£ç†ï¼Œåˆ©ç”¨æœ€å°äºŒä¹˜æ±‚è§£æ›´æ–°ï¼Œé¿å…é›…å¯æ¯”çŸ©é˜µæˆ–åæ–¹å·®è®¡ç®—ï¼Œæ¯æ¬¡è¿­ä»£æˆæœ¬ä½Žã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å®šã€å›žå½’å’ŒåŽ»å·ç§¯é—®é¢˜ä¸­ï¼ŒRSESåœ¨ç¡®å®šæ€§å’Œéšæœºè®¾ç½®ä¸‹å‡èƒ½æŒç»­é™ä½Žå¤±é…ï¼Œæ€§èƒ½ä¼˜äºŽæˆ–åŒ¹é…xNESå’ŒNEWUOAï¼Œä¸ŽEKIç«žäº‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Nonlinear inverse problems often feature noisy, non-differentiable, or expensive residual evaluations that make Jacobian-based solvers unreliable. Popular derivative-free optimizers such as natural evolution strategies (NES) or Powell's NEWUOA still assume smoothness or expend many evaluations to maintain stability. Ensemble Kalman inversion (EKI) relies on empirical covariances that require preconditioning and scale poorly with residual dimension.
>   We introduce residual subspace evolution strategies (RSES), a derivative-free solver that samples Gaussian probes around the current iterate, builds a residual-only surrogate from their differences, and recombines the probes through a least-squares solve yielding an optimal update without forming Jacobians or covariances. Each iteration costs $k+1$ residual evaluations, where $k \ll n$ for $n$-dimensional problems, with $O(k^3)$ linear algebra overhead.
>   Benchmarks on calibration, regression, and deconvolution problems demonstrate consistent misfit reduction in both deterministic and stochastic settings. RSES matches or surpasses xNES and NEWUOA while staying competitive with EKI under matched evaluation budgets, particularly when smoothness or covariance assumptions fail.

