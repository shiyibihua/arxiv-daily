---
layout: default
title: Clustered Federated Learning with Hierarchical Knowledge Distillation
---

# Clustered Federated Learning with Hierarchical Knowledge Distillation

**arXiv**: [2512.10443v1](https://arxiv.org/abs/2512.10443) | [PDF](https://arxiv.org/pdf/2512.10443.pdf)

**ä½œè€…**: Sabtain Ahmad, Meerzhan Kanatbekova, Ivona Brandic, Atakan Aral

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCFLHKDæ–¹æ¡ˆï¼Œé€šè¿‡åˆ†å±‚çŸ¥è¯†è’¸é¦è§£å†³èšç±»è”é‚¦å­¦ä¹ ä¸­çš„çŸ¥è¯†å…±äº«ä¸Žä¸ªæ€§åŒ–å¹³è¡¡é—®é¢˜ã€‚**

**å…³é”®è¯**: `èšç±»è”é‚¦å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `åˆ†å±‚èšåˆ` `ä¸ªæ€§åŒ–æ¨¡åž‹` `ç‰©è”ç½‘çŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿèšç±»è”é‚¦å­¦ä¹ æ¨¡åž‹ç‹¬ç«‹è®­ç»ƒï¼Œç¼ºä¹é›†ç¾¤é—´çŸ¥è¯†å…±äº«ï¼Œå¯¼è‡´å­¦ä¹ ç¢Žç‰‡åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚èšåˆä¸Žå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ï¼Œå®žçŽ°é›†ç¾¤é—´çŸ¥è¯†ä¼ é€’ï¼ŒåŒæ—¶ä¿æŒä¸ªæ€§åŒ–æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šï¼ŒCFLHKDåœ¨é›†ç¾¤ç‰¹å®šå’Œå…¨å±€æ¨¡åž‹å‡†ç¡®çŽ‡ä¸Šä¼˜äºŽåŸºçº¿ï¼Œæå‡3.32-7.57%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

