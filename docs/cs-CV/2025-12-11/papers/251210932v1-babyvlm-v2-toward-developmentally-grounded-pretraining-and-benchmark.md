---
layout: default
title: BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models
---

# BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models

**arXiv**: [2512.10932v1](https://arxiv.org/abs/2512.10932) | [PDF](https://arxiv.org/pdf/2512.10932.pdf)

**ä½œè€…**: Shengao Wang, Wenqi Wang, Zecheng Wang, Max Whitton, Michael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu, Helen Chen, David Li, Jeffrey Li, Shawn Li, Andrew Zagula, Amy Zhao, Andrew Zhu, Sayaka Nakamura, Yuki Yamamoto, Jerry Jun Yokono, Aaron Mueller, Bryan A. Plummer, Kate Saenko, Venkatesh Saligrama, Boqing Gong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBabyVLM-V2æ¡†æž¶ï¼ŒåŸºäºŽå©´å„¿å‘å±•è½¨è¿¹è¿›è¡Œè§†è§‰åŸºç¡€æ¨¡åž‹é¢„è®­ç»ƒä¸Žè¯„ä¼°**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å‘å±•æ€§é¢„è®­ç»ƒ` `å©´å„¿ä¸­å¿ƒæ•°æ®` `è®¤çŸ¥è¯„ä¼°åŸºå‡†` `å¤šæ¨¡æ€ä»»åŠ¡` `æ ·æœ¬æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ—©æœŸå„¿ç«¥å‘å±•è½¨è¿¹ä¸ºæ ·æœ¬é«˜æ•ˆé¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡åž‹æä¾›è‡ªç„¶ç›®æ ‡
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡çºµå‘å¤šæ¨¡æ€é¢„è®­ç»ƒé›†å’ŒDevCVå·¥å…·ç®±ï¼Œæ¨¡æ‹Ÿå©´å„¿ç»éªŒå¹¶è¯„ä¼°è®¤çŸ¥èƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šç´§å‡‘æ¨¡åž‹åœ¨DevCVå·¥å…·ç®±ä¸Šè¡¨çŽ°ç«žäº‰æ€§ï¼Œéƒ¨åˆ†ä»»åŠ¡è¶…è¶ŠGPT-4o

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

