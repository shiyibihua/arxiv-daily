---
layout: default
title: AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding
---

# AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding

**arXiv**: [2512.10195v1](https://arxiv.org/abs/2512.10195) | [PDF](https://arxiv.org/pdf/2512.10195.pdf)

**ä½œè€…**: Gyutaek Oh, Sangjoon Park, Byung-Hoon Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoMedicæ¡†æž¶ä»¥è‡ªåŠ¨åŒ–è¯„ä¼°ä¸´åºŠå¯¹è¯ä»£ç†ï¼ŒåŸºäºŽåŒ»ç–—æ•°æ®é›†æ¨¡æ‹Ÿå¤šè½®å¯¹è¯**

**å…³é”®è¯**: `ä¸´åºŠå¯¹è¯ä»£ç†` `è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æž¶` `å¤šä»£ç†æ¨¡æ‹Ÿ` `åŒ»ç–—æ•°æ®é›†` `å¤šè½®å¯¹è¯è¯„ä¼°` `CAREæŒ‡æ ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é™æ€åŒ»ç–—é—®ç­”åŸºå‡†éš¾ä»¥è¯„ä¼°LLMåœ¨åŠ¨æ€äº¤äº’ä¸´åºŠå¯¹è¯ä¸­çš„è¡¨çŽ°ï¼Œç¼ºä¹æ ‡å‡†åŒ–æ–¹æ³•
2. æ–¹æ³•è¦ç‚¹ï¼šå°†é™æ€QAæ•°æ®é›†è½¬åŒ–ä¸ºè™šæ‹Ÿæ‚£è€…æ¡£æ¡ˆï¼Œé€šè¿‡å¤šä»£ç†æ¨¡æ‹Ÿå®žçŽ°è‡ªåŠ¨åŒ–å¤šè½®ä¸´åºŠå¯¹è¯è¯„ä¼°
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¼•å…¥CAREå¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œç»ä¸“å®¶éªŒè¯æ¡†æž¶æœ‰æ•ˆæ€§ï¼Œä¸ºåŒ»ç–—å¯¹è¯åº”ç”¨æä¾›å¼€å‘æŒ‡å—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

