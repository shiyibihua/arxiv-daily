---
layout: default
title: Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization
---

# Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization

**arXiv**: [2512.10955v1](https://arxiv.org/abs/2512.10955) | [PDF](https://arxiv.org/pdf/2512.10955.pdf)

**ä½œè€…**: Tsai-Shien Chen, Aliaksandr Siarohin, Guocheng Gordon Qian, Kuan-Chieh Jackson Wang, Egor Nemchinov, Moayed Haji-Ali, Riza Alp Guler, Willi Menapace, Ivan Skorokhodov, Anil Kag, Jun-Yan Zhu, Sergey Tulyakov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmni-Attributeå¼€æ”¾è¯æ±‡å±žæ€§ç¼–ç å™¨ï¼Œä»¥è§£å†³è§†è§‰æ¦‚å¿µä¸ªæ€§åŒ–ä¸­å±žæ€§çº ç¼ å’Œä¿¡æ¯æ³„æ¼é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰æ¦‚å¿µä¸ªæ€§åŒ–` `å¼€æ”¾è¯æ±‡å±žæ€§ç¼–ç ` `å±žæ€§è§£ç¼ ` `å¯¹æ¯”å­¦ä¹ ` `ç”Ÿæˆæ¨¡åž‹` `å›¾åƒåˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä½¿ç”¨é€šç”¨å›¾åƒç¼–ç å™¨ï¼Œå¯¼è‡´å¤šä¸ªè§†è§‰å±žæ€§çº ç¼ ï¼Œéš¾ä»¥éš”ç¦»å•ä¸€å±žæ€§ï¼Œå¼•å‘ä¿¡æ¯æ³„æ¼å’Œä¸ä¸€è‡´åˆæˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè”åˆè®¾è®¡æ•°æ®å’Œæ¨¡åž‹ï¼Œé€šè¿‡è¯­ä¹‰é“¾æŽ¥å›¾åƒå¯¹å’ŒåŒç›®æ ‡è®­ç»ƒï¼Œå­¦ä¹ é«˜ä¿çœŸã€å±žæ€§ç‰¹å®šçš„è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¼€æ”¾è¯æ±‡å±žæ€§æ£€ç´¢ã€ä¸ªæ€§åŒ–å’Œç»„åˆç”Ÿæˆä»»åŠ¡ä¸­å®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†ç¼–ç å™¨çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.

