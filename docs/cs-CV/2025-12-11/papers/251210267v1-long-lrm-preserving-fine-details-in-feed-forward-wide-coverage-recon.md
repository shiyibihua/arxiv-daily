---
layout: default
title: Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction
---

# Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction

**arXiv**: [2512.10267v1](https://arxiv.org/abs/2512.10267) | [PDF](https://arxiv.org/pdf/2512.10267.pdf)

**ä½œè€…**: Chen Ziwen, Hao Tan, Peng Wang, Zexiang Xu, Li Fuxin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLong-LRM++æ¨¡åž‹ï¼Œç»“åˆåŠæ˜¾å¼åœºæ™¯è¡¨ç¤ºä¸Žè½»é‡è§£ç å™¨ï¼Œå®žçŽ°å®žæ—¶é«˜è´¨é‡åœºæ™¯é‡å»ºã€‚**

**å…³é”®è¯**: `åœºæ™¯é‡å»º` `å®žæ—¶æ¸²æŸ“` `é«˜æ–¯æº…å°„` `éšå¼è¡¨ç¤º` `è½»é‡è§£ç å™¨` `æ·±åº¦é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨å®žæ—¶æ¸²æŸ“ä¸Žç»†èŠ‚ä¿ç•™é—´å­˜åœ¨æƒè¡¡ï¼Œå¦‚é«˜æ–¯æº…å°„æ˜“æ¨¡ç³Šç»†èŠ‚ï¼Œéšå¼è¡¨ç¤ºè®¡ç®—å¼€é”€å¤§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŠæ˜¾å¼åœºæ™¯è¡¨ç¤ºï¼Œç»“åˆè½»é‡è§£ç å™¨ï¼Œå‡å°‘è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒé«˜æ¸²æŸ“è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨DL3DVä¸ŠåŒ¹é…LaCTæ¸²æŸ“è´¨é‡ï¼ŒA100 GPUä¸Šå®žçŽ°14 FPSå®žæ—¶æ¸²æŸ“ï¼Œæ”¯æŒ64è¾“å…¥è§†å›¾ï¼Œæå‡æ·±åº¦é¢„æµ‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\times540$ resolution, achieving 360Â° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential "decompression" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.

