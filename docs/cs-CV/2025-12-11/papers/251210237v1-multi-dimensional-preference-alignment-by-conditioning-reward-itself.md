---
layout: default
title: Multi-dimensional Preference Alignment by Conditioning Reward Itself
---

# Multi-dimensional Preference Alignment by Conditioning Reward Itself

**arXiv**: [2512.10237v1](https://arxiv.org/abs/2512.10237) | [PDF](https://arxiv.org/pdf/2512.10237.pdf)

**ä½œè€…**: Jiho Jang, Jinyoung Kim, Kyungjune Baek, Nojun Kwak

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMCDPOä»¥è§£å†³æ‰©æ•£æ¨¡åž‹å¯¹é½ä¸­çš„å¥–åŠ±å†²çªé—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹å¯¹é½` `å¥–åŠ±å†²çª` `å¤šç»´åº¦åå¥½` `æ¡ä»¶åŒ–è®­ç»ƒ` `å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ‡å‡†DPOä¾èµ–Bradley-Terryæ¨¡åž‹èšåˆå¤šç»´åº¦å¥–åŠ±ï¼Œå¯¼è‡´å¥–åŠ±å†²çªå’Œç‰¹å¾é—å¿˜
2. MCDPOå¼•å…¥è§£è€¦Bradley-Terryç›®æ ‡ï¼Œé€šè¿‡åå¥½ç»“æžœå‘é‡æ¡ä»¶åŒ–è®­ç»ƒç‹¬ç«‹ä¼˜åŒ–å„å¥–åŠ±ç»´åº¦
3. å®žéªŒåœ¨Stable Diffusion 1.5å’ŒSDXLä¸ŠéªŒè¯MCDPOæ€§èƒ½ä¼˜è¶Šï¼Œæ”¯æŒæŽ¨ç†æ—¶åŠ¨æ€å¤šè½´æŽ§åˆ¶

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement Learning from Human Feedback has emerged as a standard for aligning diffusion models. However, we identify a fundamental limitation in the standard DPO formulation because it relies on the Bradley-Terry model to aggregate diverse evaluation axes like aesthetic quality and semantic alignment into a single scalar reward. This aggregation creates a reward conflict where the model is forced to unlearn desirable features of a specific dimension if they appear in a globally non-preferred sample. To address this issue, we propose Multi Reward Conditional DPO (MCDPO). This method resolves reward conflicts by introducing a disentangled Bradley-Terry objective. MCDPO explicitly injects a preference outcome vector as a condition during training, which allows the model to learn the correct optimization direction for each reward axis independently within a single network. We further introduce dimensional reward dropout to ensure balanced optimization across dimensions. Extensive experiments on Stable Diffusion 1.5 and SDXL demonstrate that MCDPO achieves superior performance on benchmarks. Notably, our conditional framework enables dynamic and multiple-axis control at inference time using Classifier Free Guidance to amplify specific reward dimensions without additional training or external reward models.

