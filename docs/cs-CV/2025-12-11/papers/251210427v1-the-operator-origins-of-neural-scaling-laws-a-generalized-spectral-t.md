---
layout: default
title: The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning
---

# The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning

**arXiv**: [2512.10427v1](https://arxiv.org/abs/2512.10427) | [PDF](https://arxiv.org/pdf/2512.10427.pdf)

**ä½œè€…**: Yizhou Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç®—å­ç†è®ºçš„è°±è¾“è¿åŠ¨åŠ›å­¦æ¡†æž¶ï¼Œç»Ÿä¸€è§£é‡Šæ·±åº¦å­¦ä¹ çš„ç¥žç»ç¼©æ”¾å®šå¾‹ä¸Žè®­ç»ƒåŠ¨æ€ã€‚**

**å…³é”®è¯**: `ç¥žç»ç¼©æ”¾å®šå¾‹` `ç®—å­ç†è®º` `è°±è¾“è¿åŠ¨åŠ›å­¦` `è®­ç»ƒåŠ¨æ€` `æ·±åº¦å­¦ä¹ ç†è®º` `æ¢¯åº¦ä¸‹é™åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°ä»£æ·±åº¦ç½‘ç»œåœ¨ç²—ç³™æœ‰é™æ­£åˆ™æ€§ä¸‹ï¼Œé›…å¯æ¯”ç®—å­è°±é‡å°¾ä¸”åŸºæ¼‚ç§»ï¼Œéœ€ç»Ÿä¸€ç†è®ºæè¿°è®­ç»ƒåŠ¨æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä»Žæ¢¯åº¦ä¸‹é™æŽ¨å¯¼å‡½æ•°ç©ºé—´æ¼”åŒ–ï¼Œåº”ç”¨Katoæ‰°åŠ¨ç†è®ºå¾—åˆ°è°±è¾“è¿-è€—æ•£PDEï¼Œåˆ†æžè‡ªç›¸ä¼¼è§£ä¸Žç¼©æ”¾æŒ‡æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç†è®ºé¢„æµ‹ç¼©æ”¾å®šå¾‹æŒ‡æ•°ã€åŒä¸‹é™å‡ ä½•ï¼Œå¹¶ç»Ÿä¸€NTKè®­ç»ƒä¸Žç‰¹å¾å­¦ä¹ ä¸ºPDEæžé™ï¼Œæä¾›è°±æ¡†æž¶è¿žæŽ¥ç®—å­å‡ ä½•ä¸Žä¼˜åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern deep networks operate in a rough, finite-regularity regime where Jacobian-induced operators exhibit heavy-tailed spectra and strong basis drift. In this work, we derive a unified operator-theoretoretic description of neural training dynamics directly from gradient descent. Starting from the exact evolution $\dot e_t = -M(t)e_t$ in function space, we apply Kato perturbation theory to obtain a rigorous system of coupled mode ODEs and show that, after coarse-graining, these dynamics converge to a spectral transport-dissipation PDE \[ \partial_t g + \partial_Î»(v g) = -Î»g + S, \] where $v$ captures eigenbasis drift and $S$ encodes nonlocal spectral coupling.
>   We prove that neural training preserves functional regularity, forcing the drift to take an asymptotic power-law form $v(Î»,t)\sim -c(t)Î»^b$. In the weak-coupling regime -- naturally induced by spectral locality and SGD noise -- the PDE admits self-similar solutions with a resolution frontier, polynomial amplitude growth, and power-law dissipation. This structure yields explicit scaling-law exponents, explains the geometry of double descent, and shows that the effective training time satisfies $Ï„(t)=t^Î±L(t)$ for slowly varying $L$.
>   Finally, we show that NTK training and feature learning arise as two limits of the same PDE: $v\equiv 0$ recovers lazy dynamics, while $v\neq 0$ produces representation drift. Our results provide a unified spectral framework connecting operator geometry, optimization dynamics, and the universal scaling behavior of modern deep networks.

