---
layout: default
title: LLMs Can Assist with Proposal Selection at Large User Facilities
---

# LLMs Can Assist with Proposal Selection at Large User Facilities

**arXiv**: [2512.10895v1](https://arxiv.org/abs/2512.10895) | [PDF](https://arxiv.org/pdf/2512.10895.pdf)

**‰ΩúËÄÖ**: Lijie Ding, Janell Thomson, Jon Taylor, Changwoo Do

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊèêÊ°àÈÄâÊã©ÊñπÊ≥ïÔºå‰ª•ÊèêÂçáÂ§ßÂûãÁî®Êà∑ËÆæÊñΩ‰∏≠ÊèêÊ°àËØÑÂÆ°ÁöÑÊïàÁéáÂíå‰∏ÄËá¥ÊÄß„ÄÇ**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÊèêÊ°àÈÄâÊã©` `ÊàêÂØπÂÅèÂ•Ω` `ËØÑÂÆ°Ëá™Âä®Âåñ` `ÊàêÊú¨ÊïàÁõä` `ÊèêÊ°àÁõ∏‰ººÊÄßÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºö‰º†Áªü‰∫∫Â∑•ËØÑÂÆ°Â≠òÂú®ÊèêÊ°àÈó¥Áõ∏ÂÖ≥ÊÄßÂº±„ÄÅËØÑÂÆ°ËÄÖÂÅèËßÅÂíå‰∏ç‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÈááÁî®Âü∫‰∫éÊàêÂØπÂÅèÂ•ΩÁöÑLLMÊñπÊ≥ïÔºåÊõø‰ª£‰∫∫Â∑•ËøõË°åÊèêÊ°àÊéíÂ∫èÔºåÂÆûÁé∞Êõ¥‰∏•Ë∞®ÁöÑÂÜÖÈÉ®‰∏ÄËá¥ÊÄß„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöLLMÊéíÂ∫è‰∏é‰∫∫Â∑•ÊéíÂ∫èÂº∫Áõ∏ÂÖ≥ÔºàSpearman œÅÁ∫¶0.2-0.8ÔºâÔºåÊàêÊú¨Èôç‰Ωé‰∏§‰∏™Êï∞ÈáèÁ∫ß‰ª•‰∏äÔºå‰∏îËÉΩËøõË°åÊèêÊ°àÁõ∏‰ººÊÄßÁ≠âÈ´òÁ∫ßÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $œÅ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

