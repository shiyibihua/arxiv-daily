---
layout: default
title: Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views
---

# Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views

**arXiv**: [2512.10369v1](https://arxiv.org/abs/2512.10369) | [PDF](https://arxiv.org/pdf/2512.10369.pdf)

**ä½œè€…**: Zhankuo Xu, Chaoran Feng, Yingtao Li, Jianbin Zhao, Jiashu Yang, Wangbo Yu, Li Yuan, Yonghong Tian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoherentGSæ¡†æž¶ï¼Œåˆ©ç”¨åŒå…ˆéªŒç­–ç•¥ä»Žç¨€ç–å’Œè¿åŠ¨æ¨¡ç³Šå›¾åƒå®žçŽ°é«˜ä¿çœŸ3Dé‡å»ºã€‚**

**å…³é”®è¯**: `3Dé«˜æ–¯æ³¼æº…` `ç¨€ç–è§†å›¾é‡å»º` `è¿åŠ¨æ¨¡ç³Šå¤„ç†` `åŒå…ˆéªŒç­–ç•¥` `æ‰©æ•£æ¨¡åž‹` `åŽ»æ¨¡ç³Šç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–å’Œè¿åŠ¨æ¨¡ç³Šå›¾åƒå¯¼è‡´3Dé«˜æ–¯æ³¼æº…é‡å»ºå¤±è´¥ï¼Œå½¢æˆæ¶æ€§å¾ªçŽ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆåŽ»æ¨¡ç³Šç½‘ç»œå’Œæ‰©æ•£æ¨¡åž‹çš„åŒå…ˆéªŒç­–ç•¥ï¼Œè¾…ä»¥ä¸€è‡´æ€§å¼•å¯¼ç›¸æœºæŽ¢ç´¢å’Œæ·±åº¦æ­£åˆ™åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒçœŸå®žåœºæ™¯ä¸Šï¼Œä½¿ç”¨å°‘è‡³3ã€6ã€9ä¸ªè¾“å…¥è§†å›¾ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.

