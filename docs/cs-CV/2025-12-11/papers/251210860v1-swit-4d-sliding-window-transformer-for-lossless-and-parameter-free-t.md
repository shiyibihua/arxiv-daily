---
layout: default
title: SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation
---

# SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation

**arXiv**: [2512.10860v1](https://arxiv.org/abs/2512.10860) | [PDF](https://arxiv.org/pdf/2512.10860.pdf)

**ä½œè€…**: Kehong Gong, Zhengyu Wen, Mingxi Xu, Weixia He, Qi Wang, Ning Zhang, Zhengyu Li, Chenbin Li, Dongze Lian, Wei Zhao, Xiaoyu He, Mingyuan Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSWiT-4Dæ»‘åŠ¨çª—å£Transformerï¼Œå®žçŽ°æ— æŸå¤±ã€æ— å‚æ•°çš„å•ç›®è§†é¢‘åˆ°4Dç½‘æ ¼ç”Ÿæˆ**

**å…³é”®è¯**: `4Då†…å®¹ç”Ÿæˆ` `å•ç›®è§†é¢‘é‡å»º` `æ‰©æ•£Transformer` `æ—¶ç©ºå»ºæ¨¡` `æ— å‚æ•°å¾®è°ƒ` `è½¨è¿¹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•ç›®è§†é¢‘è½¬é«˜è´¨é‡4Dç½‘æ ¼å›°éš¾ï¼Œç¼ºä¹å¤§è§„æ¨¡4Dæ•°æ®é›†é™åˆ¶æ•°æ®é©±åŠ¨è®­ç»ƒ
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæ‰©æ•£Transformerå›¾åƒåˆ°3Dç”Ÿæˆå™¨ï¼Œæ·»åŠ æ—¶ç©ºå»ºæ¨¡ï¼Œæ”¯æŒä»»æ„é•¿åº¦è§†é¢‘ï¼Œå¼•å…¥è½¨è¿¹æ¨¡å—æ¢å¤å…¨å±€å¹³ç§»
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»…éœ€å•ä¸ªçŸ­è§†é¢‘å¾®è°ƒï¼Œåœ¨å‡ ä½•ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šä¼˜äºŽåŸºçº¿ï¼ŒéªŒè¯æ•°æ®æ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/

