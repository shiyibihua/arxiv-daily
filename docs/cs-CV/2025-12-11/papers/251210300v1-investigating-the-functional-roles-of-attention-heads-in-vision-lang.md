---
layout: default
title: Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules
---

# Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules

**arXiv**: [2512.10300v1](https://arxiv.org/abs/2512.10300) | [PDF](https://arxiv.org/pdf/2512.10300.pdf)

**ä½œè€…**: Yanbei Jiang, Xueqi Ma, Shu Liu, Sarah Monazam Erfani, Tongliang Liu, James Bailey, Jey Han Lau, Krista A. Ehinger

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCogVisionæ•°æ®é›†ä¸ŽæŽ¢æµ‹æ–¹æ³•ï¼Œåˆ†æžè§†è§‰è¯­è¨€æ¨¡åž‹ä¸­æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½è§’è‰²ï¼Œæ­ç¤ºå…¶åœ¨å¤šæ¨¡æ€æŽ¨ç†ä¸­çš„æ¨¡å—åŒ–ç»„ç»‡ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ³¨æ„åŠ›å¤´åˆ†æž` `å¤šæ¨¡æ€æŽ¨ç†` `å¯è§£é‡Šæ€§æ¡†æž¶` `CogVisionæ•°æ®é›†` `åŠŸèƒ½å¤´æŽ¢æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹å†…éƒ¨æœºåˆ¶ä¸é€æ˜Žï¼Œç¼ºä¹å¯¹æ³¨æ„åŠ›å¤´åœ¨å¤šæ¨¡æ€æŽ¨ç†ä¸­åŠŸèƒ½è§’è‰²çš„ç†è§£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥CogVisionæ•°æ®é›†ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œé€šè¿‡æŽ¢æµ‹æ–¹æ³•è¯†åˆ«ä¸Žç‰¹å®šè®¤çŸ¥åŠŸèƒ½ç›¸å…³çš„åŠŸèƒ½å¤´ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°åŠŸèƒ½å¤´ç¨€ç–ä¸”åˆ†å¸ƒä¸å‡ï¼Œå¹²é¢„å®žéªŒæ˜¾ç¤ºå…¶ç§»é™¤å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå¼ºè°ƒåˆ™æå‡å‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

