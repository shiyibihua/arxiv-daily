---
layout: default
title: DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation
---

# DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation

**arXiv**: [2512.10314v1](https://arxiv.org/abs/2512.10314) | [PDF](https://arxiv.org/pdf/2512.10314.pdf)

**ä½œè€…**: Anh M. Vu, Khang P. Le, Trang T. K. Vo, Ha Thach, Huy Hung Nguyen, David Yang, Han H. Huynh, Quynh Nguyen, Tuan M. Pham, Tuan-Anh Le, Minh H. N. Le, Thanh-Huy Nguyen, Akash Awasthi, Chandra Mohan, Zhu Han, Hien Van Nguyen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDualProtoSegæ¡†æž¶ï¼Œé€šè¿‡æ–‡æœ¬å’Œå›¾åƒå¼•å¯¼çš„åŽŸåž‹å­¦ä¹ è§£å†³å¼±ç›‘ç£ç»„ç»‡ç—…ç†å›¾åƒåˆ†å‰²é—®é¢˜**

**å…³é”®è¯**: `å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²` `ç»„ç»‡ç—…ç†å›¾åƒ` `åŽŸåž‹å­¦ä¹ ` `è§†è§‰è¯­è¨€å¯¹é½` `å¤šå°ºåº¦é‡‘å­—å¡”æ¨¡å—` `æ•°å­—ç—…ç†å­¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼±ç›‘ç£ç»„ç»‡ç—…ç†å›¾åƒåˆ†å‰²é¢ä¸´ç±»é—´åŒè´¨ã€ç±»å†…å¼‚è´¨å’ŒCAMç›‘ç£åŒºåŸŸæ”¶ç¼©æ•ˆåº”
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¯å­¦ä¹ æç¤ºè°ƒä¼˜ç”Ÿæˆæ–‡æœ¬åŽŸåž‹ï¼Œå¹¶ä¸Žå›¾åƒåŽŸåž‹å½¢æˆåŒæ¨¡æ€åŽŸåž‹åº“ï¼Œå¢žå¼ºè¯­ä¹‰å’Œå¤–è§‚çº¿ç´¢
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨BCSS-WSSSåŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯æ–‡æœ¬æè¿°å¤šæ ·æ€§å’Œå¤šæ¨¡æ€åŽŸåž‹çš„äº’è¡¥æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.

