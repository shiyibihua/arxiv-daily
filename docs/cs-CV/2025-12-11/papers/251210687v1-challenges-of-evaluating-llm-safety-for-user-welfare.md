---
layout: default
title: Challenges of Evaluating LLM Safety for User Welfare
---

# Challenges of Evaluating LLM Safety for User Welfare

**arXiv**: [2512.10687v1](https://arxiv.org/abs/2512.10687) | [PDF](https://arxiv.org/pdf/2512.10687.pdf)

**ä½œè€…**: Manon Kempermann, Sai Suresh Macharla Vasu, Mahalakshmi Raveenthiran, Theo Farrell, Ingmar Weber

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç”¨æˆ·æƒ…å¢ƒçš„å®‰å…¨è¯„ä¼°æ–¹æ³•ï¼Œä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹åœ¨ä¸ªäººé«˜é£Žé™©å»ºè®®ä¸­çš„å®‰å…¨è¯„ä¼°æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨è¯„ä¼°` `ç”¨æˆ·ç¦åˆ©é£Žé™©` `æƒ…å¢ƒæ„ŸçŸ¥è¯„ä¼°` `é«˜é£Žé™©å»ºè®®` `è„†å¼±ç”¨æˆ·ç¾¤ä½“` `è¯„ä¼°æ–¹æ³•å­¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰LLMå®‰å…¨è¯„ä¼°èšç„¦é€šç”¨é£Žé™©ï¼Œå¿½ç•¥ç”¨æˆ·æƒ…å¢ƒä¾èµ–çš„ä¸ªäººç¦åˆ©é£Žé™©ï¼Œè¯„ä¼°æ–¹æ³•ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è®¾è®¡åŒ…å«ä¸åŒè„†å¼±æ€§ç”¨æˆ·æƒ…å¢ƒçš„è¯„ä¼°æ¡†æž¶ï¼Œæ¯”è¾ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸Žç›²è¯„çš„å®‰å…¨è¯„åˆ†å·®å¼‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºä¸Šä¸‹æ–‡ç›²è¯„é«˜ä¼°å®‰å…¨æ€§ï¼Œç”¨æˆ·æƒ…å¢ƒæŠ«éœ²æ— æ³•æ˜¾è‘—æ”¹å–„è¯„ä¼°ï¼Œéœ€è¯„ä¼°è€…ç›´æŽ¥è¯„ä¼°å¤šæ ·åŒ–ç”¨æˆ·æ¡£æ¡ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

