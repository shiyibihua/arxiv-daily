---
layout: default
title: Efficient-VLN: A Training-Efficient Vision-Language Navigation Model
---

# Efficient-VLN: A Training-Efficient Vision-Language Navigation Model

**arXiv**: [2512.10310v1](https://arxiv.org/abs/2512.10310) | [PDF](https://arxiv.org/pdf/2512.10310.pdf)

**ä½œè€…**: Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEfficient-VLNä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­è®­ç»ƒå¼€é”€å¤§çš„é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `è®­ç»ƒæ•ˆçŽ‡` `è®°å¿†æœºåˆ¶` `åŠ¨æ€ç­–ç•¥` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMsåœ¨VLNä¸­é¢ä¸´é•¿åŽ†å²è§‚å¯Ÿçš„äºŒæ¬¡è®¡ç®—è´Ÿæ‹…å’ŒDAggerä¸­æŽ¢ç´¢ä¸Žæ•ˆçŽ‡çš„æƒè¡¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ¸è¿›å¼è®°å¿†å’Œå¯å­¦ä¹ é€’å½’è®°å¿†æœºåˆ¶ï¼Œå¹¶å¼•å…¥åŠ¨æ€æ··åˆç­–ç•¥ä»¥å¹³è¡¡æŽ¢ç´¢ä¸Žæ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨R2R-CEå’ŒRxR-CEä¸Šè¾¾åˆ°SOTAæ€§èƒ½ï¼Œè®­ç»ƒå¼€é”€æ˜¾è‘—é™ä½Žè‡³282 H800 GPUå°æ—¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.

