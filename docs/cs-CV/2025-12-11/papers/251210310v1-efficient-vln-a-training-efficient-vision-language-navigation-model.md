---
layout: default
title: Efficient-VLN: A Training-Efficient Vision-Language Navigation Model
---

# Efficient-VLN: A Training-Efficient Vision-Language Navigation Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.10310" target="_blank" class="toolbar-btn">arXiv: 2512.10310v1</a>
    <a href="https://arxiv.org/pdf/2512.10310.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10310v1" 
            onclick="toggleFavorite(this, '2512.10310v1', 'Efficient-VLN: A Training-Efficient Vision-Language Navigation Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Efficient-VLNÔºö‰∏ÄÁßçËÆ≠ÁªÉÈ´òÊïàÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Ê®°ÂûãÔºåÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÂºÄÈîÄ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂØºËà™` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `È´òÊïàËÆ≠ÁªÉ` `ËÆ∞ÂøÜÊú∫Âà∂` `Êé¢Á¥¢Á≠ñÁï•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLNÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÂéÜÂè≤ËßÇÊµãÊó∂ËÆ°ÁÆóÂºÄÈîÄÂ§ßÔºå‰∏îDAggerËÆ≠ÁªÉ‰∏≠Êé¢Á¥¢ÊïàÁéá‰∏éËΩ®ËøπÈïøÂ∫¶Â≠òÂú®ÊùÉË°°„ÄÇ
2. Efficient-VLNÈÄöËøáÊ∏êËøõÂºèËÆ∞ÂøÜÂíåÂèØÂ≠¶‰π†ÈÄíÂΩíËÆ∞ÂøÜÂáèÂ∞ëtokenÂ§ÑÁêÜË¥üÊãÖÔºåÂπ∂‰ΩøÁî®Âä®ÊÄÅÊ∑∑ÂêàÁ≠ñÁï•Âπ≥Ë°°Êé¢Á¥¢ÊïàÁéá„ÄÇ
3. Efficient-VLNÂú®R2R-CEÂíåRxR-CE‰∏äÂèñÂæóSOTAÊÄßËÉΩÔºå‰∏îËÆ≠ÁªÉÊó∂Èó¥Â§ßÂπÖÁº©Áü≠Ëá≥282 H800 GPUÂ∞èÊó∂„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®ËßÜËßâ-ËØ≠Ë®ÄÂØºËà™(VLN)‰∏≠Â±ïÁé∞Âá∫Â∑®Â§ßÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Â∑®Â§ßÁöÑËÆ≠ÁªÉÂºÄÈîÄ‰∏•ÈáçÈòªÁ¢ç‰∫ÜÂÆûÈôÖÂ∫îÁî®„ÄÇÊàë‰ª¨ÂèëÁé∞ÂØºËá¥ÂºÄÈîÄÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÈóÆÈ¢òÔºö(1)Â§ÑÁêÜÈïøÊó∂Á®ãÂéÜÂè≤ËßÇÊµã‰Ωú‰∏∫Â§ßÈáètokenÂ∫èÂàóÂ∏¶Êù•ÁöÑ‰∫åÊ¨°ËÆ°ÁÆóË¥üÊãÖÔºå‰ª•Âèä(2)DAgger‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéáÊùÉË°°ÔºåÂç≥Êî∂ÈõÜagentÊé¢Á¥¢ËΩ®ËøπÁöÑÊï∞ÊçÆËÅöÂêàËøáÁ®ã„ÄÇÊõ¥Â§öÁöÑÊé¢Á¥¢ËôΩÁÑ∂ËÉΩ‰∫ßÁîüÊúâÊïàÁöÑÈîôËØØÊÅ¢Â§çËΩ®Ëøπ‰ª•Â§ÑÁêÜÊµãËØïÊó∂ÂàÜÂ∏ÉÂÅèÁßªÔºå‰ΩÜ‰ª£‰ª∑ÊòØËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑËΩ®ËøπÈïøÂ∫¶Êõ¥Èïø„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜEfficient-VLNÔºå‰∏ÄÁßçËÆ≠ÁªÉÈ´òÊïàÁöÑVLNÊ®°Âûã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰∏∫‰∫ÜÂáèËΩªtokenÂ§ÑÁêÜË¥üÊãÖÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏§ÁßçÈ´òÊïàÁöÑËÆ∞ÂøÜÊú∫Âà∂Ôºö‰∏ÄÁßçÂä®ÊÄÅÂú∞‰∏∫ÊúÄËøëÁöÑËßÇÊµãÂàÜÈÖçÊõ¥Â§ötokenÁöÑÊ∏êËøõÂºèËÆ∞ÂøÜÔºå‰ª•Âèä‰∏ÄÁßçÂà©Áî®ÂèØÂ≠¶‰π†tokenÁöÑÈîÆÂÄºÁºìÂ≠ò‰Ωú‰∏∫ËÆ∞ÂøÜÁä∂ÊÄÅÁöÑÂèØÂ≠¶‰π†ÈÄíÂΩíËÆ∞ÂøÜ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂä®ÊÄÅÊ∑∑ÂêàÁ≠ñÁï•Êù•Âπ≥Ë°°Êé¢Á¥¢ÊïàÁéáÁöÑÊùÉË°°„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåEfficient-VLNÂú®R2R-CEÔºà64.2% SRÔºâÂíåRxR-CEÔºà67.0% SRÔºâ‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÂÖ≥ÈîÆÁöÑÊòØÔºåÊàë‰ª¨ÁöÑÊ®°Âûã‰ªÖÊ∂àËÄó282 H800 GPUÂ∞èÊó∂Ôºå‰∏éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåËÆ≠ÁªÉÂºÄÈîÄÊòæËëóÈôç‰Ωé„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØºËà™ÔºàVLNÔºâÊ®°ÂûãÔºåÁâπÂà´ÊòØÂü∫‰∫éÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÊ®°ÂûãÔºåÂú®ËÆ≠ÁªÉÊó∂Èù¢‰∏¥ÁùÄÂ∑®Â§ßÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÂ§ÑÁêÜÈïøÊó∂Á®ãÁöÑÂéÜÂè≤ËßÇÊµãÊï∞ÊçÆÊó∂ÔºåÈúÄË¶ÅÂ§ÑÁêÜÂ§ßÈáèÁöÑtokenÂ∫èÂàóÔºåÂØºËá¥ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Âëà‰∫åÊ¨°ÊñπÂ¢ûÈïø„ÄÇÊ≠§Â§ñÔºåÂú®Âà©Áî®DAggerÁÆóÊ≥ïËøõË°åËÆ≠ÁªÉÊó∂ÔºåÈúÄË¶ÅÂπ≥Ë°°Êé¢Á¥¢ÁöÑÂÖÖÂàÜÊÄßÂíåËÆ≠ÁªÉÊïàÁéáÔºåÂç≥Êõ¥Â§öÁöÑÊé¢Á¥¢ËôΩÁÑ∂ËÉΩÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩÜ‰ºöÊòæËëóÂ¢ûÂä†ËÆ≠ÁªÉËΩ®ËøπÁöÑÈïøÂ∫¶Ôºå‰ªéËÄåÂ¢ûÂä†ËÆ°ÁÆóË¥üÊãÖ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEfficient-VLNÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËÆæËÆ°È´òÊïàÁöÑËÆ∞ÂøÜÊú∫Âà∂ÂíåÂä®ÊÄÅÁöÑÊé¢Á¥¢Á≠ñÁï•Êù•Èôç‰ΩéËÆ≠ÁªÉÂºÄÈîÄ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÊó®Âú®ÂáèÂ∞ëÈúÄË¶ÅÂ§ÑÁêÜÁöÑtokenÊï∞ÈáèÔºåÂπ∂‰ºòÂåñDAggerËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÊé¢Á¥¢Á≠ñÁï•Ôºå‰ªéËÄåÂú®‰øùËØÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊâÄÈúÄÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEfficient-VLNÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨ËßÜËßâÁºñÁ†ÅÂô®„ÄÅËØ≠Ë®ÄÁºñÁ†ÅÂô®„ÄÅËÆ∞ÂøÜÊ®°ÂùóÂíåÂä®‰ΩúÈ¢ÑÊµãÊ®°Âùó„ÄÇËßÜËßâÁºñÁ†ÅÂô®Ë¥üË¥£ÊèêÂèñÁéØÂ¢ÉÂõæÂÉèÁöÑËßÜËßâÁâπÂæÅÔºåËØ≠Ë®ÄÁºñÁ†ÅÂô®Ë¥üË¥£Â§ÑÁêÜÂØºËà™Êåá‰ª§„ÄÇËÆ∞ÂøÜÊ®°ÂùóÁî®‰∫éÂ≠òÂÇ®ÂíåÊõ¥Êñ∞ÂéÜÂè≤ËßÇÊµã‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÂΩìÂâçËßÇÊµã‰ø°ÊÅØËûçÂêà„ÄÇÂä®‰ΩúÈ¢ÑÊµãÊ®°ÂùóÊ†πÊçÆËûçÂêàÂêéÁöÑ‰ø°ÊÅØÈ¢ÑÊµã‰∏ã‰∏ÄÊ≠•ÁöÑÂØºËà™Âä®‰Ωú„ÄÇËØ•Ê°ÜÊû∂ÁöÑÂÖ≥ÈîÆÂú®‰∫éËÆ∞ÂøÜÊ®°ÂùóÁöÑËÆæËÆ°ÔºåÂÆÉÈááÁî®‰∫ÜÊ∏êËøõÂºèËÆ∞ÂøÜÂíåÂèØÂ≠¶‰π†ÈÄíÂΩíËÆ∞ÂøÜ‰∏§ÁßçÊú∫Âà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEfficient-VLNÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂÖ∂È´òÊïàÁöÑËÆ∞ÂøÜÊú∫Âà∂„ÄÇÊ∏êËøõÂºèËÆ∞ÂøÜÂä®ÊÄÅÂú∞‰∏∫ÊúÄËøëÁöÑËßÇÊµãÂàÜÈÖçÊõ¥Â§öÁöÑtokenÔºå‰ªéËÄåÊõ¥ÂÖ≥Ê≥®ÂΩìÂâçÁéØÂ¢É‰ø°ÊÅØ„ÄÇÂèØÂ≠¶‰π†ÈÄíÂΩíËÆ∞ÂøÜÂàôÂà©Áî®ÂèØÂ≠¶‰π†ÁöÑtoken‰Ωú‰∏∫ËÆ∞ÂøÜÁä∂ÊÄÅÔºåÈÄöËøáÈîÆÂÄºÁºìÂ≠òÁöÑÊñπÂºèÂ≠òÂÇ®ÂéÜÂè≤‰ø°ÊÅØÔºåÈÅøÂÖç‰∫ÜÂØπÊâÄÊúâÂéÜÂè≤ËßÇÊµãËøõË°åÈáçÂ§çÂ§ÑÁêÜ„ÄÇÊ≠§Â§ñÔºåÂä®ÊÄÅÊ∑∑ÂêàÁ≠ñÁï•ËÉΩÂ§üÊ†πÊçÆËÆ≠ÁªÉÁöÑËøõÂ±ïËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Êé¢Á¥¢ÁöÑÁ®ãÂ∫¶Ôºå‰ªéËÄåÂπ≥Ë°°Êé¢Á¥¢ÊïàÁéáÂíåÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ∏êËøõÂºèËÆ∞ÂøÜÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥tokenÂàÜÈÖçÊØî‰æãÊù•ÂÖ≥Ê≥®ÊúÄËøëÁöÑËßÇÊµã„ÄÇÂèØÂ≠¶‰π†ÈÄíÂΩíËÆ∞ÂøÜ‰ΩøÁî®Â∞ëÈáèÂèØÂ≠¶‰π†ÁöÑtokenÊù•Ë°®Á§∫ÂéÜÂè≤Áä∂ÊÄÅÔºåÂπ∂ÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂Â∞ÜÂΩìÂâçËßÇÊµã‰ø°ÊÅØ‰∏éÂéÜÂè≤Áä∂ÊÄÅËûçÂêà„ÄÇÂä®ÊÄÅÊ∑∑ÂêàÁ≠ñÁï•‰ΩøÁî®‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊùÉÈáçÊù•Âπ≥Ë°°‰∏ìÂÆ∂Á≠ñÁï•ÂíåÊé¢Á¥¢Á≠ñÁï•ÔºåËØ•ÊùÉÈáçÊ†πÊçÆËÆ≠ÁªÉÁöÑËøõÂ±ïËøõË°åË∞ÉÊï¥„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂØºËà™ÊçüÂ§±ÂíåËæÖÂä©ÊçüÂ§±ÔºåÂØºËà™ÊçüÂ§±Áî®‰∫é‰ºòÂåñÂä®‰ΩúÈ¢ÑÊµãÔºåËæÖÂä©ÊçüÂ§±Áî®‰∫é‰ºòÂåñËÆ∞ÂøÜÊ®°ÂùóÁöÑÂ≠¶‰π†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Efficient-VLNÂú®R2R-CE‰∏äÂèñÂæó‰∫Ü64.2%ÁöÑSRÔºåÂú®RxR-CE‰∏äÂèñÂæó‰∫Ü67.0%ÁöÑSRÔºåËææÂà∞‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåËØ•Ê®°Âûã‰ªÖÊ∂àËÄó282 H800 GPUÂ∞èÊó∂ËøõË°åËÆ≠ÁªÉÔºåÁõ∏ÊØî‰∫éÂÖ∂‰ªñSOTAÊñπÊ≥ïÔºåËÆ≠ÁªÉÂºÄÈîÄÊòæËëóÈôç‰ΩéÔºå‰ΩìÁé∞‰∫ÜÂÖ∂È´òÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Efficient-VLNÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨ÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊõ¥ÂÆπÊòìÂú∞ÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÂèóÈôêÁöÑÂπ≥Âè∞‰∏äÔºåÂπ∂Âä†ÈÄüÁõ∏ÂÖ≥ÊäÄÊúØÁöÑÁ†îÂèëÂíåÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑÂØºËà™Á≥ªÁªüÂèëÂ±ïÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.

