---
layout: default
title: Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds
---

# Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds

**arXiv**: [2512.10407v1](https://arxiv.org/abs/2512.10407) | [PDF](https://arxiv.org/pdf/2512.10407.pdf)

**ä½œè€…**: Christian Soize

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç´§è‡´æ— è¾¹å¤šè¿žé€šæµå½¢ä¸Šæ½œåœ¨éšæœºåœºçš„éšæœºç¥žç»æž¶æž„ç›‘ç£å­¦ä¹ æ¡†æž¶ï¼Œä»¥å»ºæ¨¡éžé«˜æ–¯éšæœºè¾“å‡ºç³»ç»Ÿã€‚**

**å…³é”®è¯**: `éšæœºç¥žç»æž¶æž„` `æ½œåœ¨éšæœºåœº` `ç´§è‡´æ— è¾¹å¤šè¿žé€šæµå½¢` `ç›‘ç£å­¦ä¹ ` `éžé«˜æ–¯ç³»ç»Ÿ` `å‡ ä½•æ„ŸçŸ¥å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé’ˆå¯¹ç¡®å®šæ€§è¾“å…¥ä¸‹è¾“å‡ºå¼ºéžé«˜æ–¯çš„å¤æ‚ä¸ç¡®å®šç³»ç»Ÿï¼Œç¼ºä¹å‡ ä½•æ„ŸçŸ¥çš„éšæœºå»ºæ¨¡æ¡†æž¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ½œåœ¨å„å‘å¼‚æ€§é«˜æ–¯éšæœºåœºåœ¨æµå½¢ä¸Šç”Ÿæˆéšæœºç¥žç»æž¶æž„ï¼Œè”åˆå†³å®šæ‹“æ‰‘å’Œæƒé‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå»ºç«‹æ•°å­¦æ¨¡åž‹ï¼Œåˆ†æžé€‚å®šæ€§ã€å¯æµ‹æ€§å’Œè¡¨è¾¾å˜å¼‚æ€§ï¼Œä¸ºå‡ ä½•é©±åŠ¨éšæœºå­¦ä¹ å¥ å®šåŸºç¡€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning.

