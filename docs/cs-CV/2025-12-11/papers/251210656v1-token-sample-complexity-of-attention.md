---
layout: default
title: Token Sample Complexity of Attention
---

# Token Sample Complexity of Attention

**arXiv**: [2512.10656v1](https://arxiv.org/abs/2512.10656) | [PDF](https://arxiv.org/pdf/2512.10656.pdf)

**ä½œè€…**: LÃ©a Bohbot, Cyril Letrouit, Gabriel PeyrÃ©, FranÃ§ois-Xavier Vialard

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»¤ç‰Œæ ·æœ¬å¤æ‚åº¦ä»¥åˆ»ç”»æ³¨æ„åŠ›åœ¨é•¿åºåˆ—ä¸­çš„æ”¶æ•›è¡Œä¸º**

**å…³é”®è¯**: `æ³¨æ„åŠ›æœºåˆ¶` `æ ·æœ¬å¤æ‚åº¦` `æ”¶æ•›åˆ†æž` `é•¿åºåˆ—å»ºæ¨¡` `è½¯æœ€å¤§å‡½æ•°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ³¨æ„åŠ›æœºåˆ¶åœ¨æžç«¯åºåˆ—é•¿åº¦ä¸‹çš„æ”¶æ•›é€ŸçŽ‡æœªçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†æžæ³¨æ„åŠ›å›¾ç‚¹ä¸€è‡´æ”¶æ•›å’Œå˜æ¢ä»¤ç‰Œåˆ†å¸ƒçŸ©çš„æ”¶æ•›ç•Œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆé«˜æ–¯æ•°æ®å’ŒçœŸå®žBERTæ¨¡åž‹ä¸ŠéªŒè¯ç†è®ºé¢„æµ‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^Î²$ with $Î²<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $Î²$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.

