---
layout: default
title: On Learning-Curve Monotonicity for Maximum Likelihood Estimators
---

# On Learning-Curve Monotonicity for Maximum Likelihood Estimators

**arXiv**: [2512.10220v1](https://arxiv.org/abs/2512.10220) | [PDF](https://arxiv.org/pdf/2512.10220.pdf)

**ä½œè€…**: Mark Sellke, Steven Yin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯æ˜Žæœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨åœ¨å¤šç§å‚æ•°è®¾ç½®ä¸‹çš„å­¦ä¹ æ›²çº¿å•è°ƒæ€§ï¼ŒåŒ…æ‹¬é«˜æ–¯å’ŒGammaåˆ†å¸ƒ**

**å…³é”®è¯**: `å­¦ä¹ æ›²çº¿å•è°ƒæ€§` `æœ€å¤§ä¼¼ç„¶ä¼°è®¡` `KLæ•£åº¦` `é«˜æ–¯åˆ†å¸ƒ` `Gammaåˆ†å¸ƒ` `å‚æ•°ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶å­¦ä¹ æ›²çº¿å•è°ƒæ€§ï¼Œå³ç®—æ³•éšæ•°æ®å¢žåŠ å¹³å‡æ€§èƒ½æå‡çš„æ€§è´¨
2. é’ˆå¯¹é«˜æ–¯å‘é‡å’ŒGammaå˜é‡ï¼Œè¯æ˜Žå‰å‘KLæ•£åº¦çš„å•è°ƒæ€§ï¼Œè§£å†³å¼€æ”¾é—®é¢˜
3. ä½¿ç”¨GPT-5.2 Proç”Ÿæˆæ‰€æœ‰ç»“æžœï¼Œäººç±»ä»…éªŒè¯å’Œè½¬å½•è¯æ˜Ž

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The property of learning-curve monotonicity, highlighted in a recent series of work by Loog, Mey and Viering, describes algorithms which only improve in average performance given more data, for any underlying data distribution within a given family. We establish the first nontrivial monotonicity guarantees for the maximum likelihood estimator in a variety of well-specified parametric settings. For sequential prediction with log loss, we show monotonicity (in fact complete monotonicity) of the forward KL divergence for Gaussian vectors with unknown covariance and either known or unknown mean, as well as for Gamma variables with unknown scale parameter. The Gaussian setting was explicitly highlighted as open in the aforementioned works, even in dimension 1. Finally we observe that for reverse KL divergence, a folklore trick yields monotonicity for very general exponential families.
>   All results in this paper were derived by variants of GPT-5.2 Pro. Humans did not provide any proof strategies or intermediate arguments, but only prompted the model to continue developing additional results, and verified and transcribed its proofs.

