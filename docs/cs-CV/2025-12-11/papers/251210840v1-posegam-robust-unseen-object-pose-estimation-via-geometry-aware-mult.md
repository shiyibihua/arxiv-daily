---
layout: default
title: PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning
---

# PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.10840" target="_blank" class="toolbar-btn">arXiv: 2512.10840v1</a>
    <a href="https://arxiv.org/pdf/2512.10840.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10840v1" 
            onclick="toggleFavorite(this, '2512.10840v1', 'PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jianqi Chen, Biao Zhang, Xiangjun Tang, Peter Wonka

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-11

**Â§áÊ≥®**: Project page: https://windvchen.github.io/PoseGAM/

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://windvchen.github.io/PoseGAM/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**PoseGAMÔºöÂü∫‰∫éÂá†‰ΩïÊÑüÁü•Â§öËßÜËßíÊé®ÁêÜÁöÑÈ≤ÅÊ£íÊú™Áü•Áâ©‰ΩìÂßøÊÄÅ‰º∞ËÆ°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `6DÂßøÊÄÅ‰º∞ËÆ°` `Êú™Áü•Áâ©‰Ωì` `Â§öËßÜËßíÊé®ÁêÜ` `Âá†‰ΩïÊÑüÁü•` `ÁÇπ‰∫ë` `Ê∑±Â∫¶Â≠¶‰π†` `Êú∫Âô®‰∫∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÊü•ËØ¢ÂõæÂÉè‰∏éÁâ©‰ΩìÊ®°ÂûãÊàñÊ®°ÊùøÂõæÂÉè‰πãÈó¥ÁöÑÊòæÂºèÁâπÂæÅÂØπÂ∫îÔºåËøôÂú®Êú™Áü•Áâ©‰ΩìÂßøÊÄÅ‰º∞ËÆ°‰∏≠Èù¢‰∏¥ÊåëÊàò„ÄÇ
2. PoseGAMÈÄöËøáÂá†‰ΩïÊÑüÁü•Â§öËßÜËßíÊé®ÁêÜÔºåÁõ¥Êé•‰ªéÊü•ËØ¢ÂõæÂÉèÂíåÊ®°ÊùøÂõæÂÉèÈ¢ÑÊµãÁâ©‰ΩìÂßøÊÄÅÔºåÈÅøÂÖç‰∫ÜÊòæÂºèÂåπÈÖç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPoseGAMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ≥ÂùáARÊèêÂçá5.1%ÔºåÊúÄÈ´òÊèêÂçáËææ17.6%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫PoseGAMÔºå‰∏ÄÁßçÂá†‰ΩïÊÑüÁü•ÁöÑÂ§öËßÜËßíÊ°ÜÊû∂ÔºåÁî®‰∫éÁõ¥Êé•‰ªéÊü•ËØ¢ÂõæÂÉèÂíåÂ§ö‰∏™Ê®°ÊùøÂõæÂÉèÈ¢ÑÊµãÁâ©‰ΩìÂßøÊÄÅÔºåÊó†ÈúÄÊòæÂºèÁâπÂæÅÂåπÈÖçÔºå‰ªéËÄåËß£ÂÜ≥Êú™Áü•Áâ©‰ΩìÁöÑ6DÂßøÊÄÅ‰º∞ËÆ°ÈöæÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éÂ§öËßÜËßíÂü∫Á°ÄÊ®°ÂûãÊû∂ÊûÑÔºåÈÄöËøáÊòæÂºèÁöÑÂü∫‰∫éÁÇπÁöÑÂá†‰Ωï‰ø°ÊÅØÂíåÂá†‰ΩïË°®Á§∫ÁΩëÁªúÂ≠¶‰π†ÁöÑÁâπÂæÅÊù•Êï¥ÂêàÁâ©‰ΩìÂá†‰Ωï‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´Ë∂ÖËøá19‰∏á‰∏™ÂØπË±°ÁöÑÂ§ßËßÑÊ®°ÂêàÊàêÊï∞ÊçÆÈõÜÔºå‰ª•Â¢ûÂº∫È≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏äÁöÑÂ§ßÈáèËØÑ‰º∞Ë°®ÊòéÔºåPoseGAMËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂπ≥ÂùáARÊåáÊ†áÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü5.1%ÔºåÂú®Âçï‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ17.6%ÁöÑÂ¢ûÁõäÔºåË°®ÊòéÂÖ∂ÂØπÊú™Áü•Áâ©‰ΩìÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú™Áü•Áâ©‰ΩìÁöÑ6DÂßøÊÄÅ‰º∞ËÆ°ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂú®Êü•ËØ¢ÂõæÂÉèÂíåÁâ©‰ΩìÊ®°ÂûãÊàñÊ®°ÊùøÂõæÂÉè‰πãÈó¥Âª∫Á´ãÊòæÂºèÁöÑÁâπÂæÅÂØπÂ∫îÂÖ≥Á≥ªÔºåËøôÁßçÊñπÊ≥ïÂú®Â§ÑÁêÜÊú™Áü•Áâ©‰ΩìÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂõ†‰∏∫Áº∫‰πèÈ¢ÑÂÖàÂ≠òÂú®ÁöÑÊ®°ÂûãÊàñÊ®°Êùø„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®Âá†‰Ωï‰ø°ÊÅØÔºåÂÆûÁé∞ÂØπÊú™Áü•Áâ©‰ΩìÁöÑÈ≤ÅÊ£íÂßøÊÄÅ‰º∞ËÆ°ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPoseGAMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öËßÜËßí‰ø°ÊÅØÂíåÂá†‰ΩïÊÑüÁü•ËÉΩÂäõÔºåÁõ¥Êé•‰ªéÊü•ËØ¢ÂõæÂÉèÂíåÂ§ö‰∏™Ê®°ÊùøÂõæÂÉèÈ¢ÑÊµãÁâ©‰ΩìÂßøÊÄÅÔºåËÄåÊó†ÈúÄÊòæÂºèÂú∞Âª∫Á´ãÁâπÂæÅÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÈÄöËøáÊï¥ÂêàÊòæÂºèÁöÑÁÇπ‰∫ëÂá†‰Ωï‰ø°ÊÅØÂíå‰ªéÂá†‰ΩïË°®Á§∫ÁΩëÁªúÂ≠¶‰π†Âà∞ÁöÑÂá†‰ΩïÁâπÂæÅÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£Áâ©‰ΩìÁöÑ‰∏âÁª¥ÁªìÊûÑÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑÂßøÊÄÅ‰º∞ËÆ°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPoseGAMÁöÑÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éÂ§öËßÜËßíÂü∫Á°ÄÊ®°ÂûãÊû∂ÊûÑ„ÄÇÂÆÉÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºåÁî®‰∫é‰ªéÊü•ËØ¢ÂõæÂÉèÂíåÊ®°ÊùøÂõæÂÉè‰∏≠ÊèêÂèñËßÜËßâÁâπÂæÅÔºõ2) Âá†‰ΩïË°®Á§∫Ê®°ÂùóÔºåÁî®‰∫éÁºñÁ†ÅÁâ©‰ΩìÁöÑÂá†‰Ωï‰ø°ÊÅØÔºåÂåÖÊã¨ÊòæÂºèÁöÑÁÇπ‰∫ëË°®Á§∫ÂíåÂ≠¶‰π†Âà∞ÁöÑÂá†‰ΩïÁâπÂæÅÔºõ3) Â§öËßÜËßíÊé®ÁêÜÊ®°ÂùóÔºåÁî®‰∫éÊï¥ÂêàÊù•Ëá™‰∏çÂêåËßÜËßíÁöÑÁâπÂæÅÂíåÂá†‰Ωï‰ø°ÊÅØÔºåÈ¢ÑÊµãÁâ©‰ΩìÁöÑÂßøÊÄÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPoseGAMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âá†‰ΩïÊÑüÁü•ÁöÑÂ§öËßÜËßíÊé®ÁêÜÊñπÊ≥ï„ÄÇÂÆÉÈÄöËøáÊòæÂºèÁöÑÁÇπ‰∫ëÂá†‰Ωï‰ø°ÊÅØÂíå‰ªéÂá†‰ΩïË°®Á§∫ÁΩëÁªúÂ≠¶‰π†Âà∞ÁöÑÂá†‰ΩïÁâπÂæÅÔºåÊúâÊïàÂú∞Êï¥Âêà‰∫ÜÁâ©‰ΩìÁöÑÂá†‰Ωï‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÈÅøÂÖç‰∫ÜÊòæÂºèÁöÑÁâπÂæÅÂåπÈÖçÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÊú™Áü•Áâ©‰Ωì„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPoseGAMÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®ÁÇπ‰∫ë‰Ωú‰∏∫ÊòæÂºèÁöÑÂá†‰ΩïË°®Á§∫ÔºåÁõ¥Êé•ÁºñÁ†ÅÁâ©‰ΩìÁöÑ‰∏âÁª¥ÁªìÊûÑÔºõ2) ËÆæËÆ°Âá†‰ΩïË°®Á§∫ÁΩëÁªúÔºåÂ≠¶‰π†Áâ©‰ΩìÁöÑÂá†‰ΩïÁâπÂæÅÔºåË°•ÂÖÖÁÇπ‰∫ëË°®Á§∫ÁöÑ‰∏çË∂≥Ôºõ3) ‰ΩøÁî®Â§öËßÜËßíÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊï¥ÂêàÊù•Ëá™‰∏çÂêåËßÜËßíÁöÑÁâπÂæÅÂíåÂá†‰Ωï‰ø°ÊÅØÔºõ4) ÊûÑÂª∫Â§ßËßÑÊ®°ÂêàÊàêÊï∞ÊçÆÈõÜÔºåÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PoseGAMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®Âπ≥ÂùáARÊåáÊ†á‰∏äÔºåPoseGAMÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü5.1%ÔºåÂú®Âçï‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ17.6%ÁöÑÂ¢ûÁõä„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåPoseGAMÂØπÊú™Áü•Áâ©‰ΩìÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏îËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Âá†‰Ωï‰ø°ÊÅØËøõË°åÂßøÊÄÅ‰º∞ËÆ°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PoseGAMÂú®Êú∫Âô®‰∫∫ÊäìÂèñ„ÄÅÂ¢ûÂº∫Áé∞ÂÆû„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊìç‰ΩúÊú™Áü•Áâ©‰ΩìÔºåÊèêÈ´òÂ¢ûÂº∫Áé∞ÂÆûÂ∫îÁî®ÁöÑÁúüÂÆûÊÑüÔºåÂπ∂‰∏∫Ëá™Âä®È©æÈ©∂Á≥ªÁªüÊèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõ„ÄÇËØ•Á†îÁ©∂ÁöÑÊú™Êù•ÂΩ±ÂìçÂú®‰∫éÊé®Âä®ËÆ°ÁÆóÊú∫ËßÜËßâÊäÄÊúØÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®ÔºåÂπ∂‰øÉËøõÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> 6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .

