---
layout: default
title: When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection
---

# When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection

**arXiv**: [2512.10449v1](https://arxiv.org/abs/2512.10449) | [PDF](https://arxiv.org/pdf/2512.10449.pdf)

**ä½œè€…**: Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Jahnvi Singh, Vinay Chamola, Yash Sinha, Murari Mandal, Dhruv Kumar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWAVSæŒ‡æ ‡è¯„ä¼°LLMç§‘å­¦è¯„å®¡ç³»ç»Ÿå¯¹PDFå¯¹æŠ—æ€§æ“çºµçš„è„†å¼±æ€§**

**å…³é”®è¯**: `LLMè¯„å®¡ç³»ç»Ÿ` `å¯¹æŠ—æ€§æ”»å‡»` `PDFæ“çºµ` `ç§‘å­¦åŒè¡Œè¯„å®¡` `è„†å¼±æ€§è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶LLMç§‘å­¦è¯„å®¡ç³»ç»Ÿåœ¨å¯¹æŠ—æ€§PDFæ“çºµä¸‹çš„é²æ£’æ€§ï¼Œèšç„¦å°†æ‹’ç¨¿å†³ç­–ç¿»è½¬ä¸ºæŽ¥æ”¶çš„æ¿€åŠ±
2. å¼€å‘WAVSæŒ‡æ ‡é‡åŒ–è„†å¼±æ€§ï¼Œå¹¶åŸºäºŽ200ç¯‡è®ºæ–‡æ•°æ®é›†åº”ç”¨15ç§é¢†åŸŸç‰¹å®šæ”»å‡»ç­–ç•¥
3. å®žéªŒæ˜¾ç¤ºå¦‚Maximum Mark Magykç­‰æ··æ·†ç­–ç•¥èƒ½æœ‰æ•ˆæ“çºµè¯„åˆ†ï¼Œåœ¨åŒ…æ‹¬GPT-5çš„13ä¸ªæ¨¡åž‹ä¸­å®žçŽ°é«˜å†³ç­–ç¿»è½¬çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

