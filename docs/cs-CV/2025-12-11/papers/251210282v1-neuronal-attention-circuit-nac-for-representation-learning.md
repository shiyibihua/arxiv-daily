---
layout: default
title: Neuronal Attention Circuit (NAC) for Representation Learning
---

# Neuronal Attention Circuit (NAC) for Representation Learning

**arXiv**: [2512.10282v1](https://arxiv.org/abs/2512.10282) | [PDF](https://arxiv.org/pdf/2512.10282.pdf)

**ä½œè€…**: Waleed Razzaq, Izis Kankaraway, Yun-Bo Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNeuronal Attention Circuitä»¥è§£å†³è¿žç»­æ—¶é—´å»ºæ¨¡ä¸­æ³¨æ„åŠ›æœºåˆ¶ç¦»æ•£æ€§çš„é™åˆ¶**

**å…³é”®è¯**: `è¿žç»­æ—¶é—´æ³¨æ„åŠ›` `ç”Ÿç‰©å¯å‘æ¨¡åž‹` `ç¨€ç–é—¨æŽ§ç½‘ç»œ` `ODEæ±‚è§£` `è¡¨ç¤ºå­¦ä¹ ` `æ—¶é—´åºåˆ—åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ³¨æ„åŠ›æœºåˆ¶åœ¨è¡¨ç¤ºå­¦ä¹ ä¸­ä¼˜äºŽRNNï¼Œä½†å…¶ç¦»æ•£ç‰¹æ€§é™åˆ¶äº†è¿žç»­æ—¶é—´å»ºæ¨¡èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ç”Ÿç‰©å¯å‘çš„è¿žç»­æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡çº¿æ€§ä¸€é˜¶ODEå’Œç¨€ç–é—¨æŽ§ç½‘ç»œè®¡ç®—æ³¨æ„åŠ›å¯¹æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªé¢†åŸŸéªŒè¯ï¼ŒNACåœ¨ç²¾åº¦ä¸ŠåŒ¹é…æˆ–ä¼˜äºŽåŸºçº¿ï¼Œåœ¨è¿è¡Œæ—¶å’Œå†…å­˜æ•ˆçŽ‡ä¸Šå¤„äºŽä¸­ç­‰æ°´å¹³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.

