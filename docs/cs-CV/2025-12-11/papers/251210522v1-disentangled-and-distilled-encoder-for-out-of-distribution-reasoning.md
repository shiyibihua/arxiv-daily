---
layout: default
title: Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees
---

# Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees

**arXiv**: [2512.10522v1](https://arxiv.org/abs/2512.10522) | [PDF](https://arxiv.org/pdf/2512.10522.pdf)

**ä½œè€…**: Zahra Rahiminasab, Michael Yuhas, Arvind Easwaran

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦è’¸é¦ç¼–ç å™¨æ¡†æž¶ï¼Œä»¥åŽ‹ç¼©æ¨¡åž‹ç”¨äºŽèµ„æºå—é™è®¾å¤‡ä¸Šçš„åˆ†å¸ƒå¤–æŽ¨ç†ã€‚**

**å…³é”®è¯**: `è§£è€¦æ½œåœ¨ç©ºé—´` `æ¨¡åž‹åŽ‹ç¼©` `åˆ†å¸ƒå¤–æŽ¨ç†` `å¸ˆç”Ÿè’¸é¦` `Rademacherå¤æ‚åº¦` `èµ„æºå—é™è®¾å¤‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå˜åˆ†è‡ªç¼–ç å™¨çš„è§£è€¦æ½œåœ¨ç©ºé—´ç”¨äºŽå¤šæ ‡ç­¾åˆ†å¸ƒå¤–æŽ¨ç†ï¼Œä½†æ¨¡åž‹å¤§å°ä¸é€‚åˆèµ„æºå—é™è®¾å¤‡éƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¸ˆç”Ÿè’¸é¦å°†æ¨¡åž‹åŽ‹ç¼©å½¢å¼åŒ–ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œç»“åˆè§£è€¦çº¦æŸä¿æŒæ½œåœ¨ç©ºé—´è§£è€¦æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºäºŽRademacherå¤æ‚åº¦æä¾›ç†è®ºä¿è¯ï¼Œå¹¶åœ¨NVIDIAè®¾å¤‡ä¸Šå®žè¯è¯„ä¼°åŽ‹ç¼©æ¨¡åž‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA

