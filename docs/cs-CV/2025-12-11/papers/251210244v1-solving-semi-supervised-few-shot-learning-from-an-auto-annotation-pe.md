---
layout: default
title: Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective
---

# Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective

**arXiv**: [2512.10244v1](https://arxiv.org/abs/2512.10244) | [PDF](https://arxiv.org/pdf/2512.10244.pdf)

**ä½œè€…**: Tian Liu, Anwesha Basu, James Caverlee, Shu Kong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSWIFTæ–¹æ³•ä»¥è§£å†³åŠç›‘ç£å°‘æ ·æœ¬å­¦ä¹ ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹å¾®è°ƒæ•ˆæžœä¸ä½³çš„é—®é¢˜**

**å…³é”®è¯**: `åŠç›‘ç£å°‘æ ·æœ¬å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡åž‹å¾®è°ƒ` `ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦æå‡` `æ¸©åº¦è°ƒä¼˜` `é˜¶æ®µå¼å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŠç›‘ç£å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•å¿½è§†å¼€æºè§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œå¯¼è‡´å¾®è°ƒæ—¶æœªæ ‡è®°æ•°æ®åˆ©ç”¨çŽ‡å’Œç›‘ç£ä¿¡å·å¼±
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†ç±»å™¨åˆå§‹åŒ–å’Œæ¸©åº¦è°ƒä¼˜æå‡ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦ï¼Œå¹¶è®¾è®¡é˜¶æ®µå¼å¾®è°ƒç­–ç•¥SWIFTä»¥æœ‰æ•ˆåˆ©ç”¨ä»»åŠ¡ç›¸å…³æ•°æ®
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSWIFTè¶…è¶ŠçŽ°æœ‰æ–¹æ³•çº¦5ä¸ªå‡†ç¡®ç‚¹ï¼Œç”šè‡³åª²ç¾Žç›‘ç£å­¦ä¹ 

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!

