---
layout: default
title: Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning
---

# Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning

**arXiv**: [2512.10691v1](https://arxiv.org/abs/2512.10691) | [PDF](https://arxiv.org/pdf/2512.10691.pdf)

**ä½œè€…**: Benjamin Gundersen, Nicolas Deperrois, Samuel Ruiperez-Campillo, Thomas M. Sutter, Julia E. Vogt, Michael Moor, Farhad Nooralahzadeh, Michael Krauthammer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡åž‹ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆä¸Žè§†è§‰å®šä½æ€§èƒ½**

**å…³é”®è¯**: `åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆ` `è§†è§‰å®šä½` `GRPOä¼˜åŒ–` `ä¸´åºŠä»»åŠ¡å¥–åŠ±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡åž‹ä¾èµ–ç›‘ç£å¾®è°ƒï¼Œç¼ºä¹ä»»åŠ¡è´¨é‡åé¦ˆï¼Œå½±å“æŠ¥å‘Šç”Ÿæˆä¸Žè§†è§‰å®šä½æ•ˆæžœ
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç»“åˆä¸´åºŠä»»åŠ¡å¥–åŠ±ï¼Œé€šè¿‡GRPOä¼˜åŒ–æ¨¡åž‹ï¼Œå¹¶æŽ¢ç´¢æ˜¾å¼æŽ¨ç†èƒ½åŠ›çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¼ºåŒ–å­¦ä¹ åœ¨æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰å®šä½ä»»åŠ¡ä¸Šå¸¦æ¥é¢å¤–å¢žç›Šï¼Œæ¨¡åž‹è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œæ˜¾å¼æŽ¨ç†æœªæ˜¾è‘—æå‡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

