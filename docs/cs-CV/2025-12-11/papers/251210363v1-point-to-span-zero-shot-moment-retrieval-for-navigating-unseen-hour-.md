---
layout: default
title: Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos
---

# Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos

**arXiv**: [2512.10363v1](https://arxiv.org/abs/2512.10363) | [PDF](https://arxiv.org/pdf/2512.10363.pdf)

**ä½œè€…**: Mingyu Jeon, Jisoo Yang, Sungjin Han, Jinkwon Hwang, Sunjae Yoon, Jonghee Kim, Junyeoung Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoint-to-Spanæ¡†æž¶ä»¥è§£å†³é›¶æ ·æœ¬é•¿è§†é¢‘æ—¶åˆ»æ£€ç´¢ä¸­çš„æœç´¢çˆ†ç‚¸å’ŒéªŒè¯æˆæœ¬é«˜é—®é¢˜**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `é•¿è§†é¢‘ç†è§£` `æ—¶åˆ»æ£€ç´¢` `è‡ªé€‚åº”è·¨åº¦ç”Ÿæˆ` `æŸ¥è¯¢åˆ†è§£` `è®­ç»ƒå…è´¹æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›¶æ ·æœ¬é•¿è§†é¢‘æ—¶åˆ»æ£€ç´¢é¢ä¸´æœç´¢é˜¶æ®µå€™é€‰çˆ†ç‚¸å’ŒéªŒè¯é˜¶æ®µé«˜æˆæœ¬VLMä¾èµ–çš„æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è‡ªé€‚åº”è·¨åº¦ç”Ÿæˆå™¨é˜²æ­¢å€™é€‰çˆ†ç‚¸ï¼Œå¹¶åˆ©ç”¨æŸ¥è¯¢åˆ†è§£è¿›è¡Œä½Žæˆæœ¬å€™é€‰ç²¾ç‚¼
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MADæ•°æ®é›†ä¸Šè¶…è¶Šç›‘ç£æ–¹æ³•ï¼Œå¦‚R5@0.1æŒ‡æ ‡æå‡3.7%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

