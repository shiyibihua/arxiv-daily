---
layout: default
title: MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence
---

# MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence

**arXiv**: [2512.10863v1](https://arxiv.org/abs/2512.10863) | [PDF](https://arxiv.org/pdf/2512.10863.pdf)

**ä½œè€…**: Jingli Lin, Runsen Xu, Shaohao Zhu, Sihan Yang, Peizhou Cao, Yunlong Ran, Miao Hu, Chenming Zhu, Yiman Xie, Yilin Long, Wenbo Hu, Dahua Lin, Tai Wang, Jiangmiao Pang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMSI-Video-BenchåŸºå‡†ï¼Œå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘ç©ºé—´æ™ºèƒ½ä¸Šçš„è¡¨çŽ°ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç©ºé—´æ™ºèƒ½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `å‡ ä½•æŽ¨ç†` `è·¨è§†é¢‘æŽ¨ç†` `äººç±»-AIå·®è·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹å…¨é¢è¯„ä¼°è§†é¢‘ç©ºé—´æ™ºèƒ½çš„åŸºå‡†ï¼Œé˜»ç¢MLLMsåœ¨ç‰©ç†çŽ¯å¢ƒä¸­çš„å‘å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå››å±‚æ¡†æž¶ï¼ˆæ„ŸçŸ¥ã€è§„åˆ’ã€é¢„æµ‹ã€è·¨è§†é¢‘æŽ¨ç†ï¼‰ï¼Œæž„å»ºåŒ…å«1,106ä¸ªé—®é¢˜çš„åŸºå‡†ï¼Œæ•°æ®æ¥è‡ª25ä¸ªæ•°æ®é›†å’Œå†…éƒ¨è§†é¢‘ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°25ä¸ªMLLMsï¼Œå‘çŽ°äººç±»ä¸ŽAIå·®è·æ˜¾è‘—ï¼Œæœ€ä½³æ¨¡åž‹è½åŽäººç±»è¿‘60%ï¼Œå¹¶æ­ç¤ºæ¨¡åž‹åœ¨å‡ ä½•æŽ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„ç³»ç»Ÿå¤±è´¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

