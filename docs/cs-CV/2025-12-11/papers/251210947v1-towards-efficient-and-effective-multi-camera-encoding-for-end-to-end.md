---
layout: default
title: Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving
---

# Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving

**arXiv**: [2512.10947v1](https://arxiv.org/abs/2512.10947) | [PDF](https://arxiv.org/pdf/2512.10947.pdf)

**ä½œè€…**: Jiawei Yang, Ziyu Chen, Yurong You, Yan Wang, Yiming Li, Yuxiao Chen, Boyi Li, Boris Ivanovic, Marco Pavone, Yue Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlexåœºæ™¯ç¼–ç å™¨ï¼Œä»¥é«˜æ•ˆå¤„ç†ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­çš„å¤šæ‘„åƒå¤´æ•°æ®ç“¶é¢ˆã€‚**

**å…³é”®è¯**: `å¤šæ‘„åƒå¤´ç¼–ç ` `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `åœºæ™¯è¡¨ç¤ºå­¦ä¹ ` `é«˜æ•ˆæŽ¨ç†` `æ•°æ®é©±åŠ¨æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ‘„åƒå¤´æ•°æ®é‡å¤§ï¼Œå¤„ç†æ•ˆçŽ‡ä½Žï¼Œæ˜¯ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„è®¡ç®—ç“¶é¢ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¯å­¦ä¹ åœºæ™¯ä»¤ç‰Œè”åˆç¼–ç å¤šæ‘„åƒå¤´å’Œæ—¶åºå›¾åƒä¿¡æ¯ï¼Œæ— éœ€ä¾èµ–3Då…ˆéªŒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨2ä¸‡å°æ—¶æ•°æ®é›†ä¸Šï¼ŒæŽ¨ç†åžåé‡æå‡2.2å€ï¼Œé©¾é©¶æ€§èƒ½å¤§å¹…è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.

