---
layout: default
title: Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces
---

# Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces

**arXiv**: [2512.10617v1](https://arxiv.org/abs/2512.10617) | [PDF](https://arxiv.org/pdf/2512.10617.pdf)

**ä½œè€…**: Bishoy Galoaa, Xiangyu Bai, Sarah Ostadabbas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLang2Motionæ¡†æž¶ï¼Œé€šè¿‡è”åˆåµŒå…¥ç©ºé—´å¯¹é½è¿åŠ¨æµå½¢ï¼Œå®žçŽ°è¯­è¨€å¼•å¯¼çš„ä»»æ„ç‰©ä½“ç‚¹è½¨è¿¹ç”Ÿæˆã€‚**

**å…³é”®è¯**: `è¯­è¨€å¼•å¯¼è¿åŠ¨ç”Ÿæˆ` `ç‚¹è½¨è¿¹ç”Ÿæˆ` `è”åˆåµŒå…¥ç©ºé—´` `CLIPå¯¹é½` `Transformerè‡ªç¼–ç å™¨` `è¿åŠ¨åŸŸè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¤šå…³æ³¨äººä½“è¿åŠ¨æˆ–è§†é¢‘åˆæˆï¼Œç¼ºä¹ä»Žè¯­è¨€ç”Ÿæˆä»»æ„ç‰©ä½“æ˜¾å¼è½¨è¿¹çš„èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽTransformerçš„è‡ªç¼–ç å™¨ï¼Œé€šè¿‡CLIPç¼–ç å™¨å¯¹æ–‡æœ¬æè¿°å’Œè½¨è¿¹å¯è§†åŒ–è¿›è¡ŒåŒé‡ç›‘ç£ï¼Œå­¦ä¹ è½¨è¿¹è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–‡æœ¬åˆ°è½¨è¿¹æ£€ç´¢ä¸­Recall@1è¾¾34.2%ï¼Œä¼˜äºŽè§†é¢‘æ–¹æ³•12.5ç‚¹ï¼›è¿åŠ¨ç²¾åº¦æå‡33-52%ï¼Œæ”¯æŒé£Žæ ¼è¿ç§»å’Œè¯­ä¹‰æ’å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.

