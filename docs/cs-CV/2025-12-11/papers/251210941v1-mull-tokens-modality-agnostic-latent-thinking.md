---
layout: default
title: Mull-Tokens: Modality-Agnostic Latent Thinking
---

# Mull-Tokens: Modality-Agnostic Latent Thinking

**arXiv**: [2512.10941v1](https://arxiv.org/abs/2512.10941) | [PDF](https://arxiv.org/pdf/2512.10941.pdf)

**ä½œè€…**: Arijit Ray, Ahmed Abdelkader, Chengzhi Mao, Bryan A. Plummer, Kate Saenko, Ranjay Krishna, Leonidas Guibas, Wen-Sheng Chu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMull-Tokensæ¨¡æ€æ— å…³æ½œåœ¨ä»¤ç‰Œï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€è‡ªç”±æŽ¨ç†ï¼Œæå‡ç©ºé—´æŽ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŽ¨ç†` `æ½œåœ¨ä»¤ç‰Œ` `ç©ºé—´æŽ¨ç†` `æ¨¡æ€æ— å…³` `é¢„è®­ç»ƒ` `éžç›‘ç£å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šæ¨¡æ€æ¨¡åž‹ä¾èµ–å·¥å…·æˆ–ç”Ÿæˆå›¾åƒï¼Œéš¾ä»¥å®žçŽ°ç¨³å¥ä¸”å¯æ‰©å±•çš„è·¨æ¨¡æ€æŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé¢„è®­ç»ƒæ¨¡æ€æ— å…³æ½œåœ¨ä»¤ç‰Œï¼Œé€šè¿‡ç›‘ç£å’Œéžç›‘ç£å¾®è°ƒï¼Œæ”¯æŒå›¾åƒæˆ–æ–‡æœ¬ä¸­é—´ä¿¡æ¯çš„è‡ªç”±æ€è€ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªç©ºé—´æŽ¨ç†åŸºå‡†ä¸Šï¼Œå¹³å‡æå‡3%ï¼ŒæŽ¨ç†å¯†é›†åž‹ä»»åŠ¡æœ€é«˜æå‡16%ï¼Œä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

