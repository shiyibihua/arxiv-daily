---
layout: default
title: VL-JEPA: Joint Embedding Predictive Architecture for Vision-language
---

# VL-JEPA: Joint Embedding Predictive Architecture for Vision-language

**arXiv**: [2512.10942v1](https://arxiv.org/abs/2512.10942) | [PDF](https://arxiv.org/pdf/2512.10942.pdf)

**ä½œè€…**: Delong Chen, Mustafa Shukor, Theo Moutakanni, Willy Chung, Jade Yu, Tejaswi Kasarla, Allen Bolourchi, Yann LeCun, Pascale Fung

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVL-JEPAï¼ŒåŸºäºŽè”åˆåµŒå…¥é¢„æµ‹æž¶æž„ï¼Œåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­é¢„æµ‹è¿žç»­åµŒå…¥ä»¥æå‡æ€§èƒ½å¹¶å‡å°‘å‚æ•°ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `è”åˆåµŒå…¥é¢„æµ‹` `è¿žç»­åµŒå…¥é¢„æµ‹` `é€‰æ‹©æ€§è§£ç ` `å¤šä»»åŠ¡æ”¯æŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨tokenç©ºé—´è‡ªå›žå½’ç”Ÿæˆï¼Œå¯èƒ½å¿½ç•¥è¯­ä¹‰æŠ½è±¡å’Œæ•ˆçŽ‡ä¼˜åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è”åˆåµŒå…¥é¢„æµ‹æž¶æž„ï¼Œåœ¨æŠ½è±¡è¡¨ç¤ºç©ºé—´é¢„æµ‹ç›®æ ‡æ–‡æœ¬åµŒå…¥ï¼Œå‡å°‘è¡¨é¢è¯­è¨€å˜å¼‚æ€§å½±å“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç›¸åŒè§†è§‰ç¼–ç å™¨å’Œè®­ç»ƒæ•°æ®ä¸‹ï¼Œæ¯”æ ‡å‡†æ¨¡åž‹æ€§èƒ½æ›´å¼ºï¼Œå‚æ•°å‡å°‘50%ï¼Œæ”¯æŒé€‰æ‹©æ€§è§£ç å’Œå¤šç§ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.

