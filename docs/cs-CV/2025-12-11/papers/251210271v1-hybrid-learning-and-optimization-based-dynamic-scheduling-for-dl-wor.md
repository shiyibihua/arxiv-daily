---
layout: default
title: Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters
---

# Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters

**arXiv**: [2512.10271v1](https://arxiv.org/abs/2512.10271) | [PDF](https://arxiv.org/pdf/2512.10271.pdf)

**ä½œè€…**: Shruti Dongare, Redwan Ibne Seraj Khan, Hadeel Albahar, Nannan Zhao, Diego Melendez Maita, Ali R. Butt

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLTuneæ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸Žä¼˜åŒ–ç»“åˆï¼ŒåŠ¨æ€è°ƒåº¦å¼‚æž„GPUé›†ç¾¤ä¸Šçš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `å¼‚æž„GPUè°ƒåº¦` `å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½` `åŠ¨æ€ä¼˜å…ˆçº§åˆ†é…` `æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šå¼‚æž„GPUé›†ç¾¤å’Œä»»åŠ¡ç‰¹æ€§æœªçŸ¥å¯¼è‡´çŽ°æœ‰è°ƒåº¦å™¨æ€§èƒ½å—é™ï¼Œä¾èµ–ç¦»çº¿åˆ†æžæˆ–å‡è®¾ã€‚
2. æ–¹æ³•ï¼šç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜å…ˆçº§æŽ’åºå’Œæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’èŠ‚ç‚¹æ˜ å°„ï¼Œå®žçŽ°æ— ä»»åŠ¡ç‰¹å®šåˆ†æžçš„åŠ¨æ€è°ƒåº¦ã€‚
3. æ•ˆæžœï¼šåœ¨çœŸå®žç”Ÿäº§æ•°æ®ä¸Šè®­ç»ƒï¼Œæå‡GPUåˆ©ç”¨çŽ‡è¾¾20%ï¼Œå‡å°‘æŽ’é˜Ÿå»¶è¿Ÿ81%ï¼Œç¼©çŸ­ä»»åŠ¡å®Œæˆæ—¶é—´70%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

