---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-27
---

# cs.CVï¼ˆ2025-08-27ï¼‰

ğŸ“Š å…± **31** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ğŸ”—4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250819786v2-mapo-motion-aware-partitioning-of-deformable-3d-gaussian-splatting-f.html">MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction</a></td>
  <td>æå‡ºMAPoä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„æ¨¡ç³Šæ¸²æŸ“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19786v2" data-paper-url="./papers/250819786v2-mapo-motion-aware-partitioning-of-deformable-3d-gaussian-splatting-f.html" onclick="toggleFavorite(this, '2508.19786v2', 'MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250819699v1-labelgs-label-aware-3d-gaussian-splatting-for-3d-scene-segmentation.html">LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation</a></td>
  <td>æå‡ºLabelGSä»¥è§£å†³3Dåœºæ™¯åˆ†å‰²èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19699v1" data-paper-url="./papers/250819699v1-labelgs-label-aware-3d-gaussian-splatting-for-3d-scene-segmentation.html" onclick="toggleFavorite(this, '2508.19699v1', 'LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250820080v1-seam360gs-seamless-360-gaussian-splatting-from-real-world-omnidirect.html">Seam360GS: Seamless 360Â° Gaussian Splatting from Real-World Omnidirectional Images</a></td>
  <td>æå‡ºSeam360GSä»¥è§£å†³360åº¦å›¾åƒæ¸²æŸ“ä¸å®Œç¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20080v1" data-paper-url="./papers/250820080v1-seam360gs-seamless-360-gaussian-splatting-from-real-world-omnidirect.html" onclick="toggleFavorite(this, '2508.20080v1', 'Seam360GS: Seamless 360Â° Gaussian Splatting from Real-World Omnidirectional Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250820063v1-openm3d-open-vocabulary-multi-view-indoor-3d-object-detection-withou.html">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</a></td>
  <td>æå‡ºOpenM3Dä»¥è§£å†³æ— äººå·¥æ³¨é‡Šçš„å¤šè§†è§’å®¤å†…3Dç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20063v1" data-paper-url="./papers/250820063v1-openm3d-open-vocabulary-multi-view-indoor-3d-object-detection-withou.html" onclick="toggleFavorite(this, '2508.20063v1', 'OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250819651v1-scalable-object-detection-in-the-car-interior-with-vision-foundation.html">Scalable Object Detection in the Car Interior With Vision Foundation Models</a></td>
  <td>æå‡ºODALæ¡†æ¶ä»¥è§£å†³è½¦å†…ç‰©ä½“æ£€æµ‹ä¸å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19651v1" data-paper-url="./papers/250819651v1-scalable-object-detection-in-the-car-interior-with-vision-foundation.html" onclick="toggleFavorite(this, '2508.19651v1', 'Scalable Object Detection in the Car Interior With Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250819754v1-fastavatar-towards-unified-fast-high-fidelity-3d-avatar-reconstructi.html">FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers</a></td>
  <td>æå‡ºFastAvatarä»¥è§£å†³é«˜æ—¶é—´å¤æ‚åº¦å’Œæ•°æ®åˆ©ç”¨ç‡ä½çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19754v1" data-paper-url="./papers/250819754v1-fastavatar-towards-unified-fast-high-fidelity-3d-avatar-reconstructi.html" onclick="toggleFavorite(this, '2508.19754v1', 'FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250819806v1-context-aware-sparse-spatiotemporal-learning-for-event-based-vision.html">Context-aware Sparse Spatiotemporal Learning for Event-based Vision</a></td>
  <td>æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨€ç–æ—¶ç©ºå­¦ä¹ ä»¥è§£å†³äº‹ä»¶è§†è§‰å¤„ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19806v1" data-paper-url="./papers/250819806v1-context-aware-sparse-spatiotemporal-learning-for-event-based-vision.html" onclick="toggleFavorite(this, '2508.19806v1', 'Context-aware Sparse Spatiotemporal Learning for Event-based Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250821090v1-q-align-alleviating-attention-leakage-in-zero-shot-appearance-transf.html">Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment</a></td>
  <td>æå‡ºQ-Alignä»¥è§£å†³é›¶æ ·æœ¬å¤–è§‚è½¬ç§»ä¸­çš„æ³¨æ„åŠ›æ³„æ¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">semantic mapping</span> <span class="paper-tag">semantic map</span> <span class="paper-tag">structure preservation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.21090v1" data-paper-url="./papers/250821090v1-q-align-alleviating-attention-leakage-in-zero-shot-appearance-transf.html" onclick="toggleFavorite(this, '2508.21090v1', 'Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250819808v1-autoq-vis-improving-unsupervised-video-instance-segmentation-via-aut.html">AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment</a></td>
  <td>æå‡ºAutoQ-VISä»¥è§£å†³æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²ä¸­çš„è´¨é‡è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19808v1" data-paper-url="./papers/250819808v1-autoq-vis-improving-unsupervised-video-instance-segmentation-via-aut.html" onclick="toggleFavorite(this, '2508.19808v1', 'AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250820279v1-how-multimodal-llms-solve-image-tasks-a-lens-on-visual-grounding-tas.html">How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding</a></td>
  <td>æå‡ºå¤šæ¨¡æ€LLMåˆ†ææ¡†æ¶ä»¥æ­ç¤ºè§†è§‰ä»»åŠ¡å¤„ç†æœºåˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20279v1" data-paper-url="./papers/250820279v1-how-multimodal-llms-solve-image-tasks-a-lens-on-visual-grounding-tas.html" onclick="toggleFavorite(this, '2508.20279v1', 'How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250820188v1-grounding-multimodal-large-language-models-with-quantitative-skin-at.html">Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study</a></td>
  <td>ç»“åˆå®šé‡çš®è‚¤å±æ€§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥æå‡çš®è‚¤ç—…è¯Šæ–­è§£é‡Šæ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20188v1" data-paper-url="./papers/250820188v1-grounding-multimodal-large-language-models-with-quantitative-skin-at.html" onclick="toggleFavorite(this, '2508.20188v1', 'Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250819542v2-cvbench-evaluating-cross-video-synergies-for-complex-multimodal-unde.html">CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning</a></td>
  <td>æå‡ºCVBenchä»¥è§£å†³å¤šè§†é¢‘å…³ç³»æ¨ç†è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19542v2" data-paper-url="./papers/250819542v2-cvbench-evaluating-cross-video-synergies-for-complex-multimodal-unde.html" onclick="toggleFavorite(this, '2508.19542v2', 'CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250820088v2-audiostory-generating-long-form-narrative-audio-with-large-language-.html">AudioStory: Generating Long-Form Narrative Audio with Large Language Models</a></td>
  <td>æå‡ºAudioStoryä»¥è§£å†³é•¿ç¯‡å™äº‹éŸ³é¢‘ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">instruction following</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20088v2" data-paper-url="./papers/250820088v2-audiostory-generating-long-form-narrative-audio-with-large-language-.html" onclick="toggleFavorite(this, '2508.20088v2', 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250819862v1-multimodal-conditional-meshgan-for-personalized-aneurysm-growth-pred.html">Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction</a></td>
  <td>æå‡ºMCMeshGANä»¥è§£å†³ä¸ªæ€§åŒ–åŠ¨è„‰ç˜¤ç”Ÿé•¿é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19862v1" data-paper-url="./papers/250819862v1-multimodal-conditional-meshgan-for-personalized-aneurysm-growth-pred.html" onclick="toggleFavorite(this, '2508.19862v1', 'Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250819769v3-aim-adaptive-intra-network-modulation-for-balanced-multimodal-learni.html">AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning</a></td>
  <td>æå‡ºè‡ªé€‚åº”ç½‘ç»œå†…è°ƒåˆ¶ä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸å¹³è¡¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19769v3" data-paper-url="./papers/250819769v3-aim-adaptive-intra-network-modulation-for-balanced-multimodal-learni.html" onclick="toggleFavorite(this, '2508.19769v3', 'AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250819493v2-mind-the-third-eye-benchmarking-privacy-awareness-in-mllm-powered-sm.html">Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents</a></td>
  <td>æå‡ºå¤§è§„æ¨¡åŸºå‡†ä»¥è¯„ä¼°æ™ºèƒ½æ‰‹æœºä»£ç†çš„éšç§æ„è¯†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19493v2" data-paper-url="./papers/250819493v2-mind-the-third-eye-benchmarking-privacy-awareness-in-mllm-powered-sm.html" onclick="toggleFavorite(this, '2508.19493v2', 'Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250819909v1-integrating-sam-supervision-for-3d-weakly-supervised-point-cloud-seg.html">Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</a></td>
  <td>æå‡ºç»“åˆSAMç›‘ç£ä»¥è§£å†³3Då¼±ç›‘ç£ç‚¹äº‘åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19909v1" data-paper-url="./papers/250819909v1-integrating-sam-supervision-for-3d-weakly-supervised-point-cloud-seg.html" onclick="toggleFavorite(this, '2508.19909v1', 'Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250819773v1-the-return-of-structural-handwritten-mathematical-expression-recogni.html">The Return of Structural Handwritten Mathematical Expression Recognition</a></td>
  <td>æå‡ºç»“æ„åŒ–æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«æ–¹æ³•ä»¥è§£å†³ç¬¦å·å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19773v1" data-paper-url="./papers/250819773v1-the-return-of-structural-handwritten-mathematical-expression-recogni.html" onclick="toggleFavorite(this, '2508.19773v1', 'The Return of Structural Handwritten Mathematical Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250820072v3-discrete-diffusion-vla-bringing-discrete-diffusion-to-action-decodin.html">Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</a></td>
  <td>æå‡ºç¦»æ•£æ‰©æ•£VLAä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç»Ÿä¸€æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">transformer policy</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20072v3" data-paper-url="./papers/250820072v3-discrete-diffusion-vla-bringing-discrete-diffusion-to-action-decodin.html" onclick="toggleFavorite(this, '2508.20072v3', 'Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250820181v1-mitigating-hallucinations-in-multimodal-llms-via-object-aware-prefer.html">Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization</a></td>
  <td>æå‡ºCHAIR-DPOä»¥å‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20181v1" data-paper-url="./papers/250820181v1-mitigating-hallucinations-in-multimodal-llms-via-object-aware-prefer.html" onclick="toggleFavorite(this, '2508.20181v1', 'Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250820265v1-plug-in-feedback-self-adaptive-attention-in-clip-for-training-free-o.html">Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation</a></td>
  <td>æå‡ºåé¦ˆè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ä»¥è§£å†³CLIPçš„å¼€æ”¾è¯æ±‡åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20265v1" data-paper-url="./papers/250820265v1-plug-in-feedback-self-adaptive-attention-in-clip-for-training-free-o.html" onclick="toggleFavorite(this, '2508.20265v1', 'Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250819574v1-multimodal-prototype-alignment-for-semi-supervised-pathology-image-s.html">Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</a></td>
  <td>æå‡ºMPAMatchä»¥è§£å†³ç—…ç†å›¾åƒåˆ†å‰²ä¸­çš„æ¨¡ç³Šè¾¹ç•Œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19574v1" data-paper-url="./papers/250819574v1-multimodal-prototype-alignment-for-semi-supervised-pathology-image-s.html" onclick="toggleFavorite(this, '2508.19574v1', 'Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250819527v1-motionflux-efficient-text-guided-motion-generation-through-rectified.html">MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment</a></td>
  <td>æå‡ºMotionFluxä»¥è§£å†³æ–‡æœ¬é©±åŠ¨è¿åŠ¨ç”Ÿæˆçš„æ•ˆç‡ä¸ç²¾åº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19527v1" data-paper-url="./papers/250819527v1-motionflux-efficient-text-guided-motion-generation-through-rectified.html" onclick="toggleFavorite(this, '2508.19527v1', 'MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250820089v1-bridging-domain-gaps-for-fine-grained-moth-classification-through-ex.html">Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</a></td>
  <td>æå‡ºè½»é‡çº§åˆ†ç±»æ–¹æ³•ä»¥è§£å†³è›¾ç±»ç»†ç²’åº¦è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20089v1" data-paper-url="./papers/250820089v1-bridging-domain-gaps-for-fine-grained-moth-classification-through-ex.html" onclick="toggleFavorite(this, '2508.20089v1', 'Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250820096v1-coda-coordinating-the-cerebrum-and-cerebellum-for-a-dual-brain-compu.html">CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</a></td>
  <td>æå‡ºCODAæ¡†æ¶ä»¥è§£å†³ç§‘å­¦è®¡ç®—ä¸­çš„è‡ªä¸»ä»£ç†æ‰§è¡Œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">generalist agent</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20096v1" data-paper-url="./papers/250820096v1-coda-coordinating-the-cerebrum-and-cerebellum-for-a-dual-brain-compu.html" onclick="toggleFavorite(this, '2508.20096v1', 'CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250820232v1-atms-kd-adaptive-temperature-and-mixed-sample-knowledge-distillation.html">ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems</a></td>
  <td>æå‡ºATMS-KDæ¡†æ¶ä»¥æå‡å†œä¸šåµŒå…¥å¼ç³»ç»Ÿä¸­çš„è½»é‡çº§CNNæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.20232v1" data-paper-url="./papers/250820232v1-atms-kd-adaptive-temperature-and-mixed-sample-knowledge-distillation.html" onclick="toggleFavorite(this, '2508.20232v1', 'ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250819864v1-self-supervised-structured-object-representation-learning.html">Self-supervised structured object representation learning</a></td>
  <td>æå‡ºè‡ªç›‘ç£ç»“æ„åŒ–ç‰©ä½“è¡¨ç¤ºå­¦ä¹ ä»¥æå‡è§†è§‰ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19864v1" data-paper-url="./papers/250819864v1-self-supervised-structured-object-representation-learning.html" onclick="toggleFavorite(this, '2508.19864v1', 'Self-supervised structured object representation learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250819852v2-ego-centric-predictive-model-conditioned-on-hand-trajectories.html">Ego-centric Predictive Model Conditioned on Hand Trajectories</a></td>
  <td>æå‡ºç»Ÿä¸€çš„é¢„æµ‹æ¨¡å‹ä»¥è§£å†³äººæœºäº¤äº’ä¸­çš„åŠ¨ä½œä¸è§†è§‰ç»“æœå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">predictive model</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19852v2" data-paper-url="./papers/250819852v2-ego-centric-predictive-model-conditioned-on-hand-trajectories.html" onclick="toggleFavorite(this, '2508.19852v2', 'Ego-centric Predictive Model Conditioned on Hand Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250819730v2-improving-generalization-in-deepfake-detection-with-face-foundation-.html">Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning</a></td>
  <td>æå‡ºåŸºäºäººè„¸åŸºç¡€æ¨¡å‹ä¸åº¦é‡å­¦ä¹ çš„æ·±ä¼ªæ£€æµ‹æ¡†æ¶ä»¥æå‡æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19730v2" data-paper-url="./papers/250819730v2-improving-generalization-in-deepfake-detection-with-face-foundation-.html" onclick="toggleFavorite(this, '2508.19730v2', 'Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250819575v2-interact-custom-customized-human-object-interaction-image-generation.html">Interact-Custom: Customized Human Object Interaction Image Generation</a></td>
  <td>æå‡ºInteract-Customä»¥è§£å†³äººæœºäº¤äº’å›¾åƒç”Ÿæˆä¸­çš„èº«ä»½ä¸äº¤äº’æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span> <span class="paper-tag">CHOIS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19575v2" data-paper-url="./papers/250819575v2-interact-custom-customized-human-object-interaction-image-generation.html" onclick="toggleFavorite(this, '2508.19575v2', 'Interact-Custom: Customized Human Object Interaction Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250819688v1-sat-supervisor-regularization-and-animation-augmentation-for-two-pro.html">SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction</a></td>
  <td>æå‡ºSATæ¡†æ¶ä»¥è§£å†³å•ç›®çº¹ç†3Däººç±»é‡å»ºä¸­çš„å‡ ä½•æ¨¡ç³Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SMPL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.19688v1" data-paper-url="./papers/250819688v1-sat-supervisor-regularization-and-animation-augmentation-for-two-pro.html" onclick="toggleFavorite(this, '2508.19688v1', 'SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)