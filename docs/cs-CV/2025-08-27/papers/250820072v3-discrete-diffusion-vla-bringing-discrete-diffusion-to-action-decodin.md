---
layout: default
title: Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies
---

# Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20072" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.20072v3</a>
  <a href="https://arxiv.org/pdf/2508.20072.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20072v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20072v3', 'Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Tian Nian, Liuao Pei, Shunbo Zhou, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo

**ÂàÜÁ±ª**: cs.CV, cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-27 (Êõ¥Êñ∞: 2025-12-22)

**Â§áÊ≥®**: New experiments on VL retention and new ablations. 18 pages

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Á¶ªÊï£Êâ©Êï£VLA‰ª•Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÁªü‰∏ÄÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Á¶ªÊï£Êâ©Êï£` `Ëá™ÈÄÇÂ∫îËß£Á†Å` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ÁîüÊàêÂä®‰ΩúÊó∂Â≠òÂú®Âõ∫ÂÆöÈ°∫Â∫èÂíå‰ø°ÊÅØË∑ØÂæÑÁ¢éÁâáÂåñÁöÑÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÁªü‰∏ÄÊÄßÂíåÊâ©Â±ïÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Á¶ªÊï£Êâ©Êï£VLAÔºåÈÄöËøáÁ¶ªÊï£Êâ©Êï£Âª∫Ê®°Âä®‰ΩúÂùóÔºåÊîØÊåÅËá™ÈÄÇÂ∫îËß£Á†ÅÈ°∫Â∫èÂíå‰∫åÊ¨°ÈáçÊé©ËîΩÔºåÊèêÂçá‰∫ÜÂä®‰ΩúÂª∫Ê®°ÁöÑÁ≤æÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁ¶ªÊï£Êâ©Êï£VLAÂú®LIBERO‰∏äËææÂà∞‰∫Ü96.3%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºå‰ºò‰∫éËá™ÂõûÂΩíÂíåMLPËß£Á†ÅÂô®ÔºåÂ±ïÁ§∫‰∫ÜÊõ¥Âº∫ÁöÑËßÜËßâ-ËØ≠Ë®ÄËÉΩÂäõ‰øùÁïô„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂ∞ÜÂ§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÈ™®Âπ≤ÁΩëÁªúÈÄÇÈÖç‰∫éÂ∞ÜÂõæÂÉèÂíåÊåá‰ª§Êò†Â∞Ñ‰∏∫Êú∫Âô®‰∫∫Âä®‰Ωú„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑVLAÊ®°ÂûãÈÄöÂ∏∏ÈááÁî®Âõ∫ÂÆöÁöÑËá™ÂõûÂΩíÁîüÊàêÈ°∫Â∫èÊàñÂú®È™®Âπ≤ÁΩëÁªúÂ§ñÈôÑÂä†Áã¨Á´ãÁöÑMLPÊàñÊâ©Êï£Â§¥ÔºåÂØºËá¥‰ø°ÊÅØË∑ØÂæÑÁ¢éÁâáÂåñÂíåËÆ≠ÁªÉË¶ÅÊ±Ç‰∏ìÈó®ÂåñÔºå‰ªéËÄåÈòªÁ¢ç‰∫ÜÁªü‰∏Ä‰∏îÂèØÊâ©Â±ïÁöÑÊû∂ÊûÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÁ¶ªÊï£Êâ©Êï£VLAÔºå‰∏ÄÁßçÁªü‰∏ÄÁöÑÂèòÊç¢Âô®Á≠ñÁï•ÔºåÈÄöËøáÁ¶ªÊï£Êâ©Êï£Âª∫Ê®°Á¶ªÊï£ÂåñÁöÑÂä®‰ΩúÂùó„ÄÇËØ•ËÆæËÆ°‰øùÁïô‰∫ÜÊâ©Êï£ÁöÑÊ∏êËøõÁ≤æÁÇºËåÉÂºèÔºåÂêåÊó∂‰∏éVLMÁöÑÁ¶ªÊï£‰ª§ÁâåÊé•Âè£ÂéüÁîüÂÖºÂÆπ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ°àÂÆûÁé∞‰∫ÜËá™ÈÄÇÂ∫îËß£Á†ÅÈ°∫Â∫èÔºåÂÖàËß£ÂÜ≥ÁÆÄÂçïÂä®‰ΩúÂÖÉÁ¥†ÔºåÂÜçÂ§ÑÁêÜÂ§çÊùÇÂÖÉÁ¥†ÔºåÂπ∂ÈÄöËøá‰∫åÊ¨°ÈáçÊé©ËîΩÂú®Á≤æÁÇºËΩÆÊ¨°‰∏≠ÈáçÊñ∞ÂÆ°ËßÜ‰∏çÁ°ÆÂÆöÁöÑÈ¢ÑÊµãÔºå‰ªéËÄåÊèêÈ´ò‰∏ÄËá¥ÊÄßÂπ∂ÂÆûÁé∞Á®≥ÂÅ•ÁöÑÈîôËØØ‰øÆÊ≠£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®Âä®‰ΩúÁîüÊàêÊó∂ÁöÑÂõ∫ÂÆöÈ°∫Â∫èÂíå‰ø°ÊÅØÁ¢éÁâáÂåñÈóÆÈ¢òÔºåËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÁªü‰∏ÄÊÄßÂíåÊâ©Â±ïÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫Á¶ªÊï£Êâ©Êï£VLAÔºåÈÄöËøáÁ¶ªÊï£ÂåñÁöÑÂä®‰ΩúÂùóÂª∫Ê®°ÔºåÈááÁî®Ëá™ÈÄÇÂ∫îËß£Á†ÅÈ°∫Â∫èÂíå‰∫åÊ¨°ÈáçÊé©ËîΩÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òÂä®‰ΩúÁîüÊàêÁöÑÁ≤æÁ°ÆÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑ‰∏∫Áªü‰∏ÄÁöÑÂèòÊç¢Âô®Á≠ñÁï•ÔºåÂåÖÂê´Á¶ªÊï£Êâ©Êï£Ê®°ÂùóÂíåËá™ÈÄÇÂ∫îËß£Á†ÅÊú∫Âà∂ÔºåÊîØÊåÅÂπ∂Ë°åËß£Á†ÅÔºåÊâìÁ†¥Ëá™ÂõûÂΩíÁì∂È¢à„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂ∞ÜÁ¶ªÊï£Êâ©Êï£ÂºïÂÖ•VLAÊ®°ÂûãÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®Ëß£Á†ÅËøáÁ®ã‰∏≠Âä®ÊÄÅË∞ÉÊï¥È°∫Â∫èÔºåÂπ∂ÈÄöËøáÈáçÊé©ËîΩÊú∫Âà∂Â¢ûÂº∫È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁ¶ªÊï£‰ª§ÁâåÊé•Âè£ÔºåËÆæÁΩÆ‰∫ÜÈÄÇÂ∫îÊÄßÊçüÂ§±ÂáΩÊï∞ÔºåÂπ∂‰ºòÂåñ‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•ÊîØÊåÅÈ´òÊïàÁöÑÂπ∂Ë°åÂ§ÑÁêÜ„ÄÇÂÆûÈ™å‰∏≠ËøòËøõË°å‰∫ÜÊ∂àËûçÁ†îÁ©∂Ôºå‰ª•È™åËØÅËßÜËßâ-ËØ≠Ë®ÄËÉΩÂäõÁöÑ‰øùÁïô„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Á¶ªÊï£Êâ©Êï£VLAÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü96.3%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåÂú®SimplerEnv-Fractal‰∏äÂÆûÁé∞‰∫Ü71.2%ÁöÑËßÜËßâÂåπÈÖçÁéáÔºåÊï¥‰ΩìÂú®SimplerEnv-Bridge‰∏äËææÂà∞‰∫Ü54.2%„ÄÇËøô‰∫õÁªìÊûúÊòæËëó‰ºò‰∫éËá™ÂõûÂΩí„ÄÅMLPËß£Á†ÅÂô®ÂíåËøûÁª≠Êâ©Êï£Âü∫Á∫øÔºåÂ±ïÁ§∫‰∫ÜÊ®°ÂûãÂú®Âä®‰ΩúÂª∫Ê®°ÂíåËÆ≠ÁªÉ‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®ÂåñÁ≥ªÁªüÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇÈÄöËøáÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÁªü‰∏ÄÊÄßÂíåÊâ©Â±ïÊÄßÔºåÊú™Êù•ÂèØÂú®Êõ¥Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜÂíåÊ®°Âûã‰∏≠ÂÆûÁé∞Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.

