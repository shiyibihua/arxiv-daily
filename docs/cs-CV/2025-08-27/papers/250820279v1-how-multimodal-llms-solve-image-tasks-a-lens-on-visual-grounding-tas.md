---
layout: default
title: How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding
---

# How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20279" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20279v1</a>
  <a href="https://arxiv.org/pdf/2508.20279.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20279v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20279v1', 'How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuoran Yu, Yong Jae Lee

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: Accepted by COLM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€LLMåˆ†ææ¡†æ¶ä»¥æ­ç¤ºè§†è§‰ä»»åŠ¡å¤„ç†æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰ä»»åŠ¡` `å±‚çº§åˆ†æ` `è§†è§‰å®šä½` `è¯­ä¹‰æ¨ç†` `æ¨¡å‹æ— å…³åˆ†æ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥æ—¶ï¼Œå…¶å†…éƒ¨å¤„ç†æœºåˆ¶å°šä¸æ¸…æ™°ï¼Œç¼ºä¹ç³»ç»Ÿåˆ†æã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¢æµ‹æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨åˆ†æä¸åŒå±‚æ¬¡çš„è§†è§‰å’Œæ–‡æœ¬è¾“å…¥å¤„ç†ï¼Œæ­ç¤ºå±‚çš„åŠŸèƒ½è§’è‰²ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMsçš„å±‚çº§ç»“æ„åœ¨è§†è§‰å®šä½ã€è¯æ±‡æ•´åˆå’Œè¯­ä¹‰æ¨ç†ç­‰æ–¹é¢å…·æœ‰ä¸€è‡´æ€§ï¼Œä¸”ä¸åŒæ¶æ„ä¸‹å±‚åˆ†é…å­˜åœ¨æ˜¾è‘—å˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†…éƒ¨å¤„ç†åŠ¨æ€å°šæœªæ·±å…¥æ¢è®¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¢æµ‹æ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æMLLMså¦‚ä½•åœ¨ä¸åŒå±‚æ¬¡å¤„ç†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ã€‚é€šè¿‡è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨é¢„æµ‹ç»†ç²’åº¦è§†è§‰ç±»åˆ«ï¼Œç»“åˆä¸‰ç§å—æ§æç¤ºå˜ä½“ï¼Œæ­ç¤ºä¸åŒå±‚çš„åŠŸèƒ½è§’è‰²ã€‚ç ”ç©¶å‘ç°ï¼Œæ—©æœŸå±‚è¿›è¡Œè§†è§‰å®šä½ï¼Œä¸­é—´å±‚æ”¯æŒè¯æ±‡æ•´åˆå’Œè¯­ä¹‰æ¨ç†ï¼Œæœ€åå±‚å‡†å¤‡ä»»åŠ¡ç‰¹å®šè¾“å‡ºã€‚å°½ç®¡æ•´ä½“ç»“æ„åœ¨ä¸åŒæ¡ä»¶ä¸‹ä¿æŒç¨³å®šï¼Œç‰¹å®šå±‚çš„åˆ†é…éšåŸºç¡€LLMæ¶æ„å˜åŒ–æ˜¾è‘—ã€‚æ­¤ç ”ç©¶ä¸ºMLLMsçš„å±‚çº§ç»„ç»‡æä¾›äº†ç»Ÿä¸€è§†è§’ï¼Œå¹¶æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„å¤šæ¨¡æ€è¡¨ç¤ºåŠ¨æ€åˆ†ææ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡å¤„ç†ä¸­çš„å†…éƒ¨æœºåˆ¶ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å±‚çº§å¤„ç†åŠ¨æ€çš„ç³»ç»Ÿåˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ¢æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨çº¿æ€§åˆ†ç±»å™¨ä»ä¸åŒå±‚æå–çš„tokenåµŒå…¥ä¸­é¢„æµ‹ç»†ç²’åº¦è§†è§‰ç±»åˆ«ï¼Œåˆ†æå„å±‚çš„åŠŸèƒ½è§’è‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰è¾“å…¥å¤„ç†ã€æ–‡æœ¬è¾“å…¥å¤„ç†å’Œå±‚çº§åˆ†æã€‚é€šè¿‡æ ‡å‡†åŒ–çš„é”šç‚¹é—®é¢˜ï¼Œç»“åˆä¸‰ç§å—æ§æç¤ºå˜ä½“ï¼Œç³»ç»Ÿè¯„ä¼°ä¸åŒå±‚çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„åˆ†ææ–¹æ³•ï¼Œèƒ½å¤Ÿæ­ç¤ºå¤šæ¨¡æ€LLMsçš„å±‚çº§ç»„ç»‡ç»“æ„ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†çº¿æ€§åˆ†ç±»å™¨è¿›è¡Œç»†ç²’åº¦åˆ†ç±»ï¼Œè®¾è®¡äº†ä¸‰ç§æç¤ºå˜ä½“ï¼ˆè¯æ±‡å˜ä½“ã€è¯­ä¹‰å¦å®šå˜ä½“å’Œè¾“å‡ºæ ¼å¼å˜ä½“ï¼‰ï¼Œä»¥æµ‹è¯•ä¸åŒå±‚å¯¹è¾“å…¥å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMsåœ¨ä¸åŒå±‚æ¬¡çš„å¤„ç†æœºåˆ¶å…·æœ‰ä¸€è‡´æ€§ï¼Œæ—©æœŸå±‚ä¸“æ³¨äºè§†è§‰å®šä½ï¼Œä¸­é—´å±‚è¿›è¡Œè¯­ä¹‰æ¨ç†ï¼Œæœ€ç»ˆå±‚ç”Ÿæˆä»»åŠ¡ç‰¹å®šè¾“å‡ºã€‚å°½ç®¡æ•´ä½“ç»“æ„ç¨³å®šï¼Œç‰¹å®šå±‚çš„åŠŸèƒ½åˆ†é…åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸­å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£å¤šæ¨¡æ€LLMsçš„å¤„ç†æœºåˆ¶ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹è®¾è®¡ï¼Œæé«˜è§†è§‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have demonstrated strong performance across a wide range of vision-language tasks, yet their internal processing dynamics remain underexplored. In this work, we introduce a probing framework to systematically analyze how MLLMs process visual and textual inputs across layers. We train linear classifiers to predict fine-grained visual categories (e.g., dog breeds) from token embeddings extracted at each layer, using a standardized anchor question. To uncover the functional roles of different layers, we evaluate these probes under three types of controlled prompt variations: (1) lexical variants that test sensitivity to surface-level changes, (2) semantic negation variants that flip the expected answer by modifying the visual concept in the prompt, and (3) output format variants that preserve reasoning but alter the answer format. Applying our framework to LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent stage-wise structure in which early layers perform visual grounding, middle layers support lexical integration and semantic reasoning, and final layers prepare task-specific outputs. We further show that while the overall stage-wise structure remains stable across variations in visual tokenization, instruction tuning data, and pretraining corpus, the specific layer allocation to each stage shifts notably with changes in the base LLM architecture. Our findings provide a unified perspective on the layer-wise organization of MLLMs and offer a lightweight, model-agnostic approach for analyzing multimodal representation dynamics.

