---
layout: default
title: Ego-centric Predictive Model Conditioned on Hand Trajectories
---

# Ego-centric Predictive Model Conditioned on Hand Trajectories

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19852" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19852v2</a>
  <a href="https://arxiv.org/pdf/2508.19852.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19852v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19852v2', 'Ego-centric Predictive Model Conditioned on Hand Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Binjie Zhang, Mike Zheng Shou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-08-28)

**å¤‡æ³¨**: Code: github.com/showlab/Ego-PM

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€çš„é¢„æµ‹æ¨¡å‹ä»¥è§£å†³äººæœºäº¤äº’ä¸­çš„åŠ¨ä½œä¸è§†è§‰ç»“æœå»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `åŠ¨ä½œé¢„æµ‹` `è§†è§‰ç”Ÿæˆ` `å¤šæ¨¡æ€èåˆ` `æ½œåœ¨æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨åŠ¨ä½œé¢„æµ‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯¹åŠ¨ä½œå¦‚ä½•å½±å“è§†è§‰åœºæ™¯çš„æ˜ç¡®å»ºæ¨¡ã€‚
2. æœ¬æ–‡æå‡ºçš„ä¸¤é˜¶æ®µé¢„æµ‹æ¡†æ¶é€šè¿‡æ‰‹éƒ¨è½¨è¿¹è”åˆå»ºæ¨¡åŠ¨ä½œå’Œè§†è§‰æœªæ¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. åœ¨Ego4Dã€BridgeDataå’ŒRLBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨ä½œé¢„æµ‹å’Œè§†é¢‘åˆæˆæ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œé¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œåŠå…¶è§†è§‰ç»“æœå¯¹äºç†è§£äººæœºäº¤äº’å’Œæœºå™¨äººè§„åˆ’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåœ°è”åˆå»ºæ¨¡è¿™ä¸¤ä¸ªæ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ä¸¤é˜¶æ®µé¢„æµ‹æ¡†æ¶ï¼ŒåŸºäºæ‰‹éƒ¨è½¨è¿¹å…±åŒå»ºæ¨¡åŠ¨ä½œå’Œè§†è§‰æœªæ¥ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¤„ç†å¼‚æ„è¾“å…¥å¹¶æ˜ç¡®é¢„æµ‹æœªæ¥æ‰‹éƒ¨è½¨è¿¹ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨å› æœäº¤å‰æ³¨æ„åŠ›èåˆå¤šæ¨¡æ€çº¿ç´¢ï¼Œå¼•å¯¼å›¾åƒåŸºç¡€çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€å¸§è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹é¦–æ¬¡è®¾è®¡ä¸ºåŒæ—¶å¤„ç†äººç±»æ´»åŠ¨ç†è§£å’Œæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œæä¾›å³å°†å‘ç”Ÿçš„åŠ¨ä½œåŠå…¶è§†è§‰åæœçš„æ˜ç¡®é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨ä½œé¢„æµ‹å’Œæœªæ¥è§†é¢‘åˆæˆæ–¹é¢è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆé¢„æµ‹äººç±»åŠ¨ä½œåŠå…¶è§†è§‰ç»“æœçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åŒæ—¶è€ƒè™‘è¿™ä¸¤ä¸ªæ–¹é¢ï¼Œå¯¼è‡´ç”Ÿæˆçš„ç»“æœä¸å¤Ÿåˆç†æˆ–ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µçš„æ–¹å¼ï¼Œé¦–å…ˆé¢„æµ‹æ‰‹éƒ¨è½¨è¿¹ï¼Œç„¶ååˆ©ç”¨è¿™äº›è½¨è¿¹ä¿¡æ¯æŒ‡å¯¼è§†é¢‘ç”Ÿæˆï¼Œç¡®ä¿ç”Ÿæˆçš„è§†è§‰ç»“æœä¸åŠ¨ä½œä¸€è‡´ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººæœºäº¤äº’çš„åŠ¨æ€è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µè¿›è¡ŒçŠ¶æ€å»ºæ¨¡ï¼Œå¤„ç†è§†è§‰è§‚å¯Ÿã€è¯­è¨€å’ŒåŠ¨ä½œå†å²ç­‰å¼‚æ„è¾“å…¥ï¼Œæ˜ç¡®é¢„æµ‹æœªæ¥æ‰‹éƒ¨è½¨è¿¹ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥å› æœäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œåˆ©ç”¨æ¨æ–­çš„åŠ¨ä½œä¿¡å·æŒ‡å¯¼å›¾åƒåŸºç¡€çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€å¸§è§†é¢‘ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æœ€å¤§åˆ›æ–°åœ¨äºé¦–æ¬¡æå‡ºä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†äººç±»æ´»åŠ¨ç†è§£å’Œæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œæä¾›å³å°†å‘ç”Ÿçš„åŠ¨ä½œåŠå…¶è§†è§‰åæœçš„æ˜ç¡®é¢„æµ‹ã€‚è¿™ä¸€æ–¹æ³•åœ¨ç†è®ºå’Œå®è·µä¸Šéƒ½å¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€è¾“å…¥çš„å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆèåˆï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œè€ƒè™‘äº†åŠ¨ä½œé¢„æµ‹ä¸è§†è§‰ç”Ÿæˆçš„ååŒä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Ego4Dã€BridgeDataå’ŒRLBenchæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨åŠ¨ä½œé¢„æµ‹å’Œæœªæ¥è§†é¢‘åˆæˆä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ™ºèƒ½æœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡å‡†ç¡®é¢„æµ‹äººç±»åŠ¨ä½œåŠå…¶è§†è§‰ç»“æœï¼Œèƒ½å¤Ÿæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.

