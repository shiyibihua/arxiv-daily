---
layout: default
title: Context-aware Sparse Spatiotemporal Learning for Event-based Vision
---

# Context-aware Sparse Spatiotemporal Learning for Event-based Vision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19806" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19806v1</a>
  <a href="https://arxiv.org/pdf/2508.19806.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19806v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19806v1', 'Context-aware Sparse Spatiotemporal Learning for Event-based Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shenqi Wang, Guangzhi Tang

**åˆ†ç±»**: cs.CV, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: Accepted at IROS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨€ç–æ—¶ç©ºå­¦ä¹ ä»¥è§£å†³äº‹ä»¶è§†è§‰å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `äº‹ä»¶è§†è§‰` `ç¨€ç–å­¦ä¹ ` `ç¥ç»å½¢æ€è®¡ç®—` `ç‰©ä½“æ£€æµ‹` `å…‰æµä¼°è®¡` `æ·±åº¦å­¦ä¹ ` `æœºå™¨äººæ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚
2. æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨€ç–æ—¶ç©ºå­¦ä¹ ï¼ˆCSSLï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒèŠ‚ç¥ç»å…ƒæ¿€æ´»æ¥å‡å°‘æ¿€æ´»å¯†åº¦ï¼Œé¿å…äº†æ‰‹åŠ¨è°ƒèŠ‚ç¨€ç–æŸå¤±é¡¹çš„å¤æ‚æ€§ã€‚
3. CSSLåœ¨äº‹ä»¶åŸºç¡€çš„ç‰©ä½“æ£€æµ‹å’Œå…‰æµä¼°è®¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæé«˜çš„ç¥ç»å…ƒç¨€ç–æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶æ‘„åƒå¤´ä½œä¸ºæœºå™¨äººæ„ŸçŸ¥çš„æ–°å…´èŒƒå¼ï¼Œå…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡ã€é«˜åŠ¨æ€èŒƒå›´å’ŒæŠ—è¿åŠ¨æ¨¡ç³Šçš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„äº‹ä»¶å¤„ç†æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨äº‹ä»¶æ•°æ®çš„ç¨€ç–ç‰¹æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜åº”ç”¨ä¸­çš„é›†æˆã€‚å°½ç®¡ç¥ç»å½¢æ€è®¡ç®—æä¾›äº†ä¸€ç§èŠ‚èƒ½çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†è„‰å†²ç¥ç»ç½‘ç»œåœ¨å¤æ‚çš„äº‹ä»¶è§†è§‰ä»»åŠ¡ï¼ˆå¦‚ç‰©ä½“æ£€æµ‹å’Œå…‰æµä¼°è®¡ï¼‰ä¸­éš¾ä»¥ä¸æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨€ç–æ—¶ç©ºå­¦ä¹ ï¼ˆCSSLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥é˜ˆå€¼åŠ¨æ€è°ƒèŠ‚ç¥ç»å…ƒæ¿€æ´»ï¼Œå‡å°‘æ¿€æ´»å¯†åº¦è€Œæ— éœ€æ˜¾å¼ç¨€ç–çº¦æŸã€‚åœ¨äº‹ä»¶åŸºç¡€çš„ç‰©ä½“æ£€æµ‹å’Œå…‰æµä¼°è®¡ä¸­ï¼ŒCSSLå®ç°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæé«˜çš„ç¥ç»å…ƒç¨€ç–æ€§ã€‚å®éªŒç»“æœçªæ˜¾äº†CSSLåœ¨å®ç°é«˜æ•ˆäº‹ä»¶è§†è§‰å¤„ç†ä¸­çš„é‡è¦ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰äº‹ä»¶è§†è§‰å¤„ç†æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨äº‹ä»¶æ•°æ®ç¨€ç–æ€§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜åº”ç”¨ä¸­ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³å’Œèƒ½è€—é«˜çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„CSSLæ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é˜ˆå€¼åŠ¨æ€è°ƒèŠ‚ç¥ç»å…ƒæ¿€æ´»ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥åˆ†å¸ƒè‡ªç„¶å‡å°‘æ¿€æ´»å¯†åº¦ï¼Œä»è€Œé¿å…äº†å¯¹ç¨€ç–çº¦æŸçš„æ˜¾å¼éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCSSLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥äº‹ä»¶æ•°æ®çš„å¤„ç†æ¨¡å—ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥é˜ˆå€¼è°ƒèŠ‚æ¨¡å—å’Œè¾“å‡ºç»“æœç”Ÿæˆæ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼Œå®ç°äº†é«˜æ•ˆçš„äº‹ä»¶å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCSSLçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŠ¨æ€é˜ˆå€¼è°ƒèŠ‚æœºåˆ¶ï¼Œä½¿å¾—ç¥ç»å…ƒæ¿€æ´»çš„ç¨€ç–æ€§å¾—ä»¥è‡ªç„¶å®ç°ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è°ƒèŠ‚ç¨€ç–æŸå¤±é¡¹ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CSSLä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åŠ¨æ€é˜ˆå€¼çš„è®¾å®šã€æ¿€æ´»å‡½æ•°çš„é€‰æ‹©ä»¥åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¿æŒé«˜ç¨€ç–æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œäº‹ä»¶å¤„ç†ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”äº‹ä»¶æ•°æ®çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCSSLåœ¨äº‹ä»¶åŸºç¡€çš„ç‰©ä½“æ£€æµ‹å’Œå…‰æµä¼°è®¡ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œä¸”ç¥ç»å…ƒç¨€ç–æ€§æ˜¾è‘—æé«˜ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚è¿™ä¸€æˆæœä¸ºé«˜æ•ˆäº‹ä»¶è§†è§‰å¤„ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›é¢†åŸŸä¸­äº‹ä»¶åŸºç¡€è§†è§‰å¤„ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒCSSLå¯èƒ½æ¨åŠ¨æ›´å¤šä½åŠŸè€—ã€é«˜æ€§èƒ½çš„ç¥ç»å½¢æ€è®¡ç®—è®¾å¤‡çš„å¼€å‘ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event-based camera has emerged as a promising paradigm for robot perception, offering advantages with high temporal resolution, high dynamic range, and robustness to motion blur. However, existing deep learning-based event processing methods often fail to fully leverage the sparse nature of event data, complicating their integration into resource-constrained edge applications. While neuromorphic computing provides an energy-efficient alternative, spiking neural networks struggle to match of performance of state-of-the-art models in complex event-based vision tasks, like object detection and optical flow. Moreover, achieving high activation sparsity in neural networks is still difficult and often demands careful manual tuning of sparsity-inducing loss terms. Here, we propose Context-aware Sparse Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware thresholding to dynamically regulate neuron activations based on the input distribution, naturally reducing activation density without explicit sparsity constraints. Applied to event-based object detection and optical flow estimation, CSSL achieves comparable or superior performance to state-of-the-art methods while maintaining extremely high neuronal sparsity. Our experimental results highlight CSSL's crucial role in enabling efficient event-based vision for neuromorphic processing.

