---
layout: default
title: AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning
---

# AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19769" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.19769v3</a>
  <a href="https://arxiv.org/pdf/2508.19769.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19769v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19769v3', 'AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shu Shen, C. L. Philip Chen, Tong Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-27 (Êõ¥Êñ∞: 2025-11-03)

**Â§áÊ≥®**: 13pages,7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ëá™ÈÄÇÂ∫îÁΩëÁªúÂÜÖË∞ÉÂà∂‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ≠¶‰π†‰∏çÂπ≥Ë°°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ëá™ÈÄÇÂ∫îË∞ÉÂà∂` `‰ºòÂåñÂÅèÂ∑Æ` `ËæÖÂä©Ê®°Âùó` `ËÅîÂêàËÆ≠ÁªÉ` `Ê∑±Â∫¶Â≠¶‰π†` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÊñπÊ≥ïÂú®Â§ÑÁêÜÊ®°ÊÄÅ‰∏çÂπ≥Ë°°Êó∂ÔºåÂæÄÂæÄÊäëÂà∂‰∏ªÂØºÊ®°ÊÄÅÁöÑÂ≠¶‰π†ÔºåÂØºËá¥Êï¥‰ΩìÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Ëá™ÈÄÇÂ∫îÁΩëÁªúÂÜÖË∞ÉÂà∂ÔºàAIMÔºâÔºåÈÄöËøáËß£ËÄ¶‰∏ªÂØºÊ®°ÊÄÅÁöÑÊ¨†‰ºòÂåñÂèÇÊï∞Ôºå‰øÉËøõ‰∏éÂº±Ê®°ÊÄÅÁöÑËÅîÂêàËÆ≠ÁªÉ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåAIMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ≠¶‰π†ÊòæËëóÊèêÂçá‰∫ÜÊú∫Âô®Â≠¶‰π†ÊÄßËÉΩÔºå‰ΩÜ‰ªçÈù¢‰∏¥ËØ∏Â§öÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂ§öÊ®°ÊÄÅ‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáË∞ÉÂà∂ÊØèÁßçÊ®°ÊÄÅÁöÑÂ≠¶‰π†Êù•ÁºìËß£Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩÜÂæÄÂæÄÊäëÂà∂‰∫Ü‰∏ªÂØºÊ®°ÊÄÅÁöÑÂ≠¶‰π†ÔºåÂΩ±ÂìçÊï¥‰ΩìÊÄßËÉΩ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫Ëá™ÈÄÇÂ∫îÁΩëÁªúÂÜÖË∞ÉÂà∂ÔºàAIMÔºâÔºåÈ¶ñÊ¨°Âú®‰∏çÊäëÂà∂‰∏ªÂØºÊàñÂº±Ê®°ÊÄÅÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞Âπ≥Ë°°ÁöÑÂ§öÊ®°ÊÄÅÂ≠¶‰π†„ÄÇAIMÈÄöËøáÂ∞Ü‰∏ªÂØºÊ®°ÊÄÅÁöÑÊ¨†‰ºòÂåñÂèÇÊï∞Ëß£ËÄ¶‰∏∫ËæÖÂä©Ê®°ÂùóÔºåÂπ∂‰∏éÂº±Ê®°ÊÄÅÂÖ±ÂêåËÆ≠ÁªÉÔºåÈÅøÂÖç‰∫ÜÂØπÂº±Ê®°ÊÄÅÁöÑÂéãÂà∂ÔºåÂêåÊó∂ÈíàÂØπÊ¨†‰ºòÂåñÂèÇÊï∞ËøõË°åÊúâÈíàÂØπÊÄßÁöÑ‰ºòÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAIMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅ‰∏çÂπ≥Ë°°Â≠¶‰π†ÊñπÊ≥ïÔºåÂπ∂Âú®‰∏çÂêåÁöÑÁΩëÁªúÁªìÊûÑ„ÄÅËûçÂêàÁ≠ñÁï•Âíå‰ºòÂåñÂô®‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ≠¶‰π†‰∏≠ÁöÑ‰∏çÂπ≥Ë°°ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÈÄöËøáÊäëÂà∂‰∏ªÂØºÊ®°ÊÄÅÁöÑÂ≠¶‰π†Êù•‰øÉËøõÂº±Ê®°ÊÄÅÔºåÂØºËá¥Êï¥‰ΩìÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAIMÈÄöËøáËØÜÂà´ÁΩëÁªúÂÜÖÈÉ®ÁöÑ‰ºòÂåñÂÅèÂ∑ÆÔºåËß£ËÄ¶‰∏ªÂØºÊ®°ÊÄÅÁöÑÊ¨†‰ºòÂåñÂèÇÊï∞ÔºåÂΩ¢ÊàêËæÖÂä©Ê®°ÂùóÔºå‰∏éÂº±Ê®°ÊÄÅÂÖ±ÂêåËÆ≠ÁªÉÔºå‰ªéËÄåÂÆûÁé∞Âπ≥Ë°°ÁöÑÂ§öÊ®°ÊÄÅÂ≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAIMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰∏ÄÊòØÂØπ‰∏ªÂØºÊ®°ÊÄÅÁöÑÊ¨†‰ºòÂåñÂèÇÊï∞ËøõË°åËß£ËÄ¶ÔºåÂΩ¢ÊàêËæÖÂä©Ê®°ÂùóÔºõ‰∫åÊòØÊ†πÊçÆÁΩëÁªúÊ∑±Â∫¶Ëá™ÈÄÇÂ∫îË∞ÉÊï¥Ë∞ÉÂà∂Âº∫Â∫¶Ôºå‰ª•‰ºòÂåñÂêÑÊ®°ÊÄÅÁöÑÂ≠¶‰π†ÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAIMÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°ÂÆûÁé∞‰∫ÜÂú®‰∏çÊäëÂà∂‰∏ªÂØºÊ®°ÊÄÅÁöÑÊÉÖÂÜµ‰∏ãÔºå‰øÉËøõÂº±Ê®°ÊÄÅÁöÑÂ≠¶‰π†ÔºåËß£ÂÜ≥‰∫Ü‰ª•ÂæÄÊñπÊ≥ï‰∏≠ÁöÑ‰ºòÂåñÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåAIMÈááÁî®‰∫ÜËá™ÈÄÇÂ∫îË∞ÉÂà∂Êú∫Âà∂ÔºåÊ†πÊçÆ‰∏çÂêåÊ∑±Â∫¶ÁöÑÊ®°ÊÄÅ‰∏çÂπ≥Ë°°Á®ãÂ∫¶Âä®ÊÄÅË∞ÉÊï¥Ë∞ÉÂà∂Âº∫Â∫¶ÔºåÁ°Æ‰øùÂêÑÊ®°ÊÄÅÁöÑÂ≠¶‰π†ÊïàÊûúÊúÄÂ§ßÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAIMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÊúÄÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅ‰∏çÂπ≥Ë°°Â≠¶‰π†ÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞X%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâÔºåÂπ∂Âú®‰∏çÂêåÁöÑÁΩëÁªúÁªìÊûÑÂíå‰ºòÂåñÂô®‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§öÊ®°ÊÄÅÊï∞ÊçÆÂàÜÊûê„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅÂåªÁñóÂΩ±ÂÉèÂ§ÑÁêÜÁ≠â„ÄÇÈÄöËøáÂÆûÁé∞Âπ≥Ë°°ÁöÑÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÔºåAIMÂèØ‰ª•ÊèêÂçáÁ≥ªÁªüÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal learning has significantly enhanced machine learning performance but still faces numerous challenges and limitations. Imbalanced multimodal learning is one of the problems extensively studied in recent works and is typically mitigated by modulating the learning of each modality. However, we find that these methods typically hinder the dominant modality's learning to promote weaker modalities, which affects overall multimodal performance. We analyze the cause of this issue and highlight a commonly overlooked problem: optimization bias within networks. To address this, we propose Adaptive Intra-Network Modulation (AIM) to improve balanced modality learning. AIM accounts for differences in optimization state across parameters and depths within the network during modulation, achieving balanced multimodal learning without hindering either dominant or weak modalities for the first time. Specifically, AIM decouples the dominant modality's under-optimized parameters into Auxiliary Blocks and encourages reliance on these performance-degraded blocks for joint training with weaker modalities. This approach effectively prevents suppression of weaker modalities while enabling targeted optimization of under-optimized parameters to improve the dominant modality. Additionally, AIM assesses modality imbalance level across network depths and adaptively adjusts modulation strength at each depth. Experimental results demonstrate that AIM outperforms state-of-the-art imbalanced modality learning methods across multiple benchmarks and exhibits strong generalizability across different backbones, fusion strategies, and optimizers.

