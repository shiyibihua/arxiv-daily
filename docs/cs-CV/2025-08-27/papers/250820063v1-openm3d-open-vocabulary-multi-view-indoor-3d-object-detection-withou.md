---
layout: default
title: OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations
---

# OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20063" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20063v1</a>
  <a href="https://arxiv.org/pdf/2508.20063.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20063v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20063v1', 'OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: ICCV2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenM3Dä»¥è§£å†³æ— äººå·¥æ³¨é‡Šçš„å¤šè§†è§’å®¤å†…3Dç‰©ä½“æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡` `3Dç‰©ä½“æ£€æµ‹` `æ— äººå·¥æ³¨é‡Š` `å¤šè§†è§’å›¾åƒ` `ä¼ªæ¡†ç”Ÿæˆ` `ä½“ç´ ç‰¹å¾` `å›¾åµŒå…¥æŠ€æœ¯` `å®¤å†…åœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dç‰©ä½“æ£€æµ‹æ–¹æ³•å¤šä¾èµ–äº3Dç‚¹äº‘ï¼Œå›¾åƒåŸºç¡€çš„å¼€æ”¾è¯æ±‡æ£€æµ‹æ¢ç´¢è¾ƒå°‘ï¼Œé™åˆ¶äº†åº”ç”¨åœºæ™¯ã€‚
2. OpenM3Dé€šè¿‡æ— äººå·¥æ³¨é‡Šçš„æ–¹å¼ï¼Œç»“åˆ2Dè¯±å¯¼ä½“ç´ ç‰¹å¾å’Œå¤šè§†è§’å›¾åƒï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å•é˜¶æ®µæ£€æµ‹å™¨ã€‚
3. åœ¨ScanNet200å’ŒARKitScenesåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOpenM3Dåœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰3Dç‰©ä½“æ£€æµ‹æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œä½†åŸºäºå›¾åƒçš„æ–¹æ³•æ¢ç´¢ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºOpenM3Dï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¼€æ”¾è¯æ±‡å¤šè§†è§’å®¤å†…3Dç‰©ä½“æ£€æµ‹å™¨ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€äººå·¥æ³¨é‡Šã€‚OpenM3Dæ˜¯ä¸€ä¸ªå•é˜¶æ®µæ£€æµ‹å™¨ï¼Œé‡‡ç”¨ImGeoNetæ¨¡å‹çš„2Dè¯±å¯¼ä½“ç´ ç‰¹å¾ã€‚ä¸ºæ”¯æŒOVï¼Œå®ƒä¸æ— ç±»åˆ«çš„3Då®šä½æŸå¤±å’Œä½“ç´ è¯­ä¹‰å¯¹é½æŸå¤±å…±åŒè®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§3Dä¼ªæ¡†ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨å›¾åµŒå…¥æŠ€æœ¯å°†2Dåˆ†æ®µç»„åˆæˆä¸€è‡´çš„3Dç»“æ„ã€‚OpenM3Dåœ¨ScanNet200å’ŒARKitScenesåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ä¼˜è¶Šçš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼Œä¸”åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„ä¸¤é˜¶æ®µæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡çš„å¤šè§†è§’å®¤å†…3Dç‰©ä½“æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–äººå·¥æ³¨é‡Šå’Œ3Dç‚¹äº‘ï¼Œé™åˆ¶äº†å…¶åº”ç”¨å’Œæ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOpenM3Dé€šè¿‡æ— äººå·¥æ³¨é‡Šçš„è®­ç»ƒæ–¹å¼ï¼Œç»“åˆ2Dè¯±å¯¼ä½“ç´ ç‰¹å¾å’Œå¤šè§†è§’å›¾åƒï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å•é˜¶æ®µæ£€æµ‹å™¨ï¼Œæ—¨åœ¨æé«˜æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¼ªæ¡†ç”Ÿæˆæ¨¡å—å’Œç‰¹å¾å¯¹é½æ¨¡å—ã€‚ä¼ªæ¡†ç”Ÿæˆæ¨¡å—åˆ©ç”¨å›¾åµŒå…¥æŠ€æœ¯å°†2Dåˆ†æ®µç»„åˆæˆä¸€è‡´çš„3Dç»“æ„ï¼Œç‰¹å¾å¯¹é½æ¨¡å—åˆ™ä»2Dåˆ†æ®µä¸­é‡‡æ ·å¤šæ ·çš„CLIPç‰¹å¾ä»¥å¯¹é½ä½“ç´ ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ— äººå·¥æ³¨é‡Šçš„3Dä¼ªæ¡†ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆäº†å›¾åµŒå…¥æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†ä¼ªæ¡†çš„ç²¾åº¦å’Œå¬å›ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡è®¾è®¡äº†æ— ç±»åˆ«çš„3Då®šä½æŸå¤±å’Œä½“ç´ è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡ç›®æ ‡çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨äº†ImGeoNetæ¨¡å‹çš„2Dè¯±å¯¼ä½“ç´ ç‰¹å¾ï¼Œæå‡äº†æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OpenM3Dåœ¨ScanNet200å’ŒARKitScenesåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œæ£€æµ‹é€Ÿåº¦ä¸ºæ¯åœºæ™¯0.3ç§’ï¼Œå‡†ç¡®æ€§è¶…è¶Šäº†å¼ºå¤§çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºåœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœºå™¨äººå¯¼èˆªå’Œå¢å¼ºç°å®ç­‰åœºæ™¯ã€‚é€šè¿‡å®ç°æ— äººå·¥æ³¨é‡Šçš„3Dç‰©ä½“æ£€æµ‹ï¼ŒOpenM3Dèƒ½å¤Ÿåœ¨å¿«é€Ÿå˜åŒ–çš„ç¯å¢ƒä¸­å®æ—¶è¯†åˆ«å’Œå®šä½ç‰©ä½“ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.

