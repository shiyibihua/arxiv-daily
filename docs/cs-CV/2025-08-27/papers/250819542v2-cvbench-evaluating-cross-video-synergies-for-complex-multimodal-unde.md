---
layout: default
title: CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning
---

# CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19542" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19542v2</a>
  <a href="https://arxiv.org/pdf/2508.19542.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19542v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19542v2', 'CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, Shuaiwei Jiao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27 (æ›´æ–°: 2025-08-28)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Hokhim2/CVBench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCVBenchä»¥è§£å†³å¤šè§†é¢‘å…³ç³»æ¨ç†è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè§†é¢‘æ¨ç†` `è·¨è§†é¢‘å…³è”` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šè§†é¢‘ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶åœ¨è·¨è§†é¢‘å…³ç³»æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚
2. æœ¬æ–‡æå‡ºCVBenchåŸºå‡†ï¼Œé€šè¿‡1000å¯¹é—®ç­”è¯„ä¼°æ¨¡å‹åœ¨è·¨è§†é¢‘å¯¹è±¡å’Œäº‹ä»¶å…³è”åŠå¤æ‚æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¡¶å°–æ¨¡å‹åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ä»…ä¸º60%ï¼Œè€Œäººç±»è¡¨ç°è¾¾åˆ°91%ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å…³é”®ç“¶é¢ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å•è§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶åœ¨å¤šè§†é¢‘åœºæ™¯ä¸‹çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†CVBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°è·¨è§†é¢‘å…³ç³»æ¨ç†ã€‚CVBenchåŒ…å«1000å¯¹é—®ç­”ï¼Œæ¶µç›–è·¨è§†é¢‘å¯¹è±¡å…³è”ã€äº‹ä»¶å…³è”å’Œå¤æ‚æ¨ç†ä¸‰ä¸ªå±‚æ¬¡ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰ä¸Šä¸‹æ–‡ä¸­ç»¼åˆä¿¡æ¯ã€‚å¯¹10å¤šç§é¢†å…ˆçš„MLLMsè¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯é¡¶å°–æ¨¡å‹åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º60%ï¼Œè¿œä½äºäººç±»çš„91%ã€‚åˆ†ææ­ç¤ºäº†å½“å‰MLLMæ¶æ„çš„æ ¹æœ¬ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è§†é¢‘ä¸Šä¸‹æ–‡ä¿ç•™å’Œé‡å å®ä½“æ¶ˆæ­§æ–¹é¢çš„ä¸è¶³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨è§†é¢‘å…³ç³»æ¨ç†ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šè§†é¢‘åœºæ™¯ä¸‹çš„èƒ½åŠ›è¯„ä¼°å°šä¸å……åˆ†ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºCVBenchåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨è·¨è§†é¢‘å¯¹è±¡å…³è”ã€äº‹ä»¶å…³è”å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæä¾›å…¨é¢çš„æ€§èƒ½åˆ†æã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCVBenchç”±1000å¯¹é—®ç­”ç»„æˆï¼Œåˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡ï¼šè·¨è§†é¢‘å¯¹è±¡å…³è”ã€è·¨è§†é¢‘äº‹ä»¶å…³è”å’Œè·¨è§†é¢‘å¤æ‚æ¨ç†ï¼Œæ¶µç›–å¤šç§é¢†åŸŸçš„è§†é¢‘æ•°æ®ã€‚è¯„ä¼°è¿‡ç¨‹åŒ…æ‹¬å¯¹10å¤šç§MLLMsçš„é›¶-shotå’Œé“¾å¼æ€ç»´æç¤ºçš„æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šCVBenchæ˜¯é¦–ä¸ªé’ˆå¯¹è·¨è§†é¢‘æ¨ç†çš„ç»¼åˆåŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰MLLMåœ¨å¤„ç†å¤šè§†é¢‘ä¿¡æ¯æ—¶çš„æ ¹æœ¬ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡ä¿ç•™å’Œå®ä½“æ¶ˆæ­§æ–¹é¢çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æç¤ºç­–ç•¥ï¼Œå¹¶å¯¹æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹æ¯”äººç±»è¡¨ç°ï¼Œæ­ç¤ºäº†æ¨¡å‹çš„æ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡é¡¶å°–æ¨¡å‹å¦‚GPT-4oåœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä»…ä¸º60%çš„å‡†ç¡®ç‡ï¼Œä½†äººç±»çš„è¡¨ç°è¾¾åˆ°äº†91%ã€‚è¿™ä¸€å‘ç°çªæ˜¾äº†å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨è§†é¢‘æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡ä¿ç•™å’Œå®ä½“æ¶ˆæ­§æ–¹é¢çš„ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CVBenchçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬å¤šæ‘„åƒå¤´ç›‘æ§ã€è·¨è§†é¢‘ç¨‹åºå­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡å¤šè§†é¢‘ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ¨åŠ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿçš„è¿›æ­¥ï¼Œå¢å¼ºäººæœºäº¤äº’ä½“éªŒï¼Œå¹¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€AIç³»ç»Ÿæä¾›é‡è¦çš„ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs. The data and evaluation code are available at https://github.com/Hokhim2/CVBench.

