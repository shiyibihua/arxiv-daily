---
layout: default
title: Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation
---

# Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20265" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20265v1</a>
  <a href="https://arxiv.org/pdf/2508.20265.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20265v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20265v1', 'Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhixiang Chi, Yanan Wu, Li Gu, Huan Liu, Ziqiang Wang, Yang Zhang, Yang Wang, Konstantinos N. Plataniotis

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: ICCV 2025, code:https://github.com/chi-chi-zx/FSA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåé¦ˆè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ä»¥è§£å†³CLIPçš„å¼€æ”¾è¯æ±‡åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡åˆ†å‰²` `è‡ªé€‚åº”æ³¨æ„åŠ›` `è§†è§‰æ–‡æœ¬å¯¹é½` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹ä¼˜åŒ–` `åé¦ˆæœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•åœ¨ç©ºé—´ä¸€è‡´æ€§å’Œæœ€ç»ˆè¾“å‡ºä¹‹é—´çš„ä¼ é€’å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´å®šä½æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åé¦ˆé©±åŠ¨çš„è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡å°†è¾“å‡ºä¿¡æ¯åé¦ˆåˆ°ä¸­é—´æ³¨æ„åŠ›ï¼Œå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

CLIPåœ¨è§†è§‰ä¸æ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­ç”±äºå®šä½ä¸ä½³è€Œé¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ä¿®æ”¹ä¸­é—´æ³¨æ„åŠ›æ¥å¢å¼ºç©ºé—´ä¸€è‡´æ€§ï¼Œä½†è¿™ç§ä¸€è‡´æ€§æœªèƒ½æœ‰æ•ˆä¼ é€’åˆ°æœ€ç»ˆè¾“å‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„åé¦ˆé©±åŠ¨è‡ªé€‚åº”æ¡†æ¶ï¼Œå°†åŸºäºè¾“å‡ºçš„è¡¥ä¸çº§å¯¹åº”å…³ç³»åé¦ˆåˆ°ä¸­é—´æ³¨æ„åŠ›ä¸­ï¼Œä»è€Œå¢å¼ºå†…éƒ¨è¡¨ç¤ºä¸æœ€ç»ˆé¢„æµ‹ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†å¤šä¸ªå…³é”®æ¨¡å—ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›éš”ç¦»ã€åŸºäºç½®ä¿¡åº¦çš„ç¨€ç–é€‚åº”ä¿®å‰ªå’Œé€‚åº”é›†æˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåé¦ˆè¾“å‡ºä¸€è‡´æ€§çº¿ç´¢ã€‚è¯¥æ–¹æ³•ä½œä¸ºæ’ä»¶æ¨¡å—ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å››ç§æœ€å…ˆè¿›çš„æ–¹æ³•ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³CLIPåœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­çš„å®šä½ä¸ä½³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ä¿®æ”¹ä¸­é—´æ³¨æ„åŠ›æ¥å¢å¼ºç©ºé—´ä¸€è‡´æ€§ï¼Œä½†è¿™ç§ä¸€è‡´æ€§æœªèƒ½æœ‰æ•ˆä¼ é€’åˆ°æœ€ç»ˆè¾“å‡ºï¼Œå¯¼è‡´è¯­ä¹‰å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„åé¦ˆé©±åŠ¨è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡å°†è¾“å‡ºçš„è¡¥ä¸çº§å¯¹åº”å…³ç³»åé¦ˆåˆ°ä¸­é—´æ³¨æ„åŠ›ä¸­ï¼Œåˆ©ç”¨æ¨¡å‹è¾“å‡ºä½œä¸ºæ›´å¼ºçš„ç©ºé—´ä¸€è‡´æ€§å…ˆéªŒï¼Œä»è€Œå¢å¼ºå†…éƒ¨è¡¨ç¤ºä¸æœ€ç»ˆé¢„æµ‹ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªå…³é”®æ¨¡å—ï¼šæ³¨æ„åŠ›éš”ç¦»ã€åŸºäºç½®ä¿¡åº¦çš„ç¨€ç–é€‚åº”ä¿®å‰ªå’Œé€‚åº”é›†æˆã€‚æ•´ä½“æµç¨‹æ˜¯é€šè¿‡è¾“å‡ºé¢„æµ‹ä¿¡æ¯æ¥è°ƒæ•´ä¸­é—´æ³¨æ„åŠ›ï¼Œä»è€Œå®ç°è‡ªé€‚åº”è°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¾“å‡ºä¿¡æ¯åé¦ˆåˆ°ä¸­é—´æ³¨æ„åŠ›ä¸­ï¼Œå½¢æˆé—­ç¯æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å•å‘è°ƒæ•´æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæˆ‘ä»¬åœ¨è®¾è®¡ä¸­é‡‡ç”¨äº†æ³¨æ„åŠ›éš”ç¦»æœºåˆ¶ï¼Œä»¥ç¡®ä¿ä¸­é—´è¡¨ç¤ºçš„ç‹¬ç«‹æ€§ï¼›åŒæ—¶ï¼ŒåŸºäºç½®ä¿¡åº¦çš„ç¨€ç–é€‚åº”ä¿®å‰ªç­–ç•¥ç”¨äºä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œé€‚åº”é›†æˆåˆ™ç”¨äºæ•´åˆå¤šç§æ³¨æ„åŠ›ç±»å‹çš„åé¦ˆä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„å¯¹æ¯”ä¸­ï¼Œæå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒåˆ†å‰²ã€ç‰©ä½“æ£€æµ‹å’Œåœºæ™¯ç†è§£ç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡å¼€æ”¾è¯æ±‡åˆ†å‰²çš„æ€§èƒ½ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„è§†è§‰ä¿¡æ¯å¤„ç†ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> CLIP exhibits strong visual-textual alignment but struggle with open-vocabulary segmentation due to poor localization. Prior methods enhance spatial coherence by modifying intermediate attention. But, this coherence isn't consistently propagated to the final output due to subsequent operations such as projections. Additionally, intermediate attention lacks direct interaction with text representations, such semantic discrepancy limits the full potential of CLIP.
>   In this work, we propose a training-free, feedback-driven self-adaptive framework that adapts output-based patch-level correspondences back to the intermediate attention. The output predictions, being the culmination of the model's processing, encapsulate the most comprehensive visual and textual semantics about each patch. Our approach enhances semantic consistency between internal representations and final predictions by leveraging the model's outputs as a stronger spatial coherence prior. We design key modules, including attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble, to effectively feedback the output coherence cues. Our method functions as a plug-in module, seamlessly integrating into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We further validate our framework across multiple attention types (Q-K, self-self, and Proxy augmented with MAE, SAM, and DINO). Our approach consistently improves their performance across eight benchmarks.

