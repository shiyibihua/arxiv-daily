---
layout: default
title: Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation
---

# Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19909" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19909v1</a>
  <a href="https://arxiv.org/pdf/2508.19909.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19909v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19909v1', 'Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆSAMç›‘ç£ä»¥è§£å†³3Då¼±ç›‘ç£ç‚¹äº‘åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè¯­ä¹‰åˆ†å‰²` `å¼±ç›‘ç£å­¦ä¹ ` `ç‚¹äº‘å¤„ç†` `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨æ ‡æ³¨ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨2Då’Œ3Dæ•°æ®çš„äº’è¡¥æ€§ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¼•å…¥2DåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„åˆ†å‰²æ©ç ï¼Œå¢å¼º3Dç‚¹äº‘çš„æ ‡æ³¨ï¼Œåˆ©ç”¨å‡ ä½•å¯¹åº”å…³ç³»å°†2Dä¿¡æ¯ä¼ æ’­è‡³3Dã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨3Då¼±ç›‘ç£åˆ†å‰²ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒéªŒè¯äº†2Dä¸3Dæ•°æ®ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•é€šå¸¸ä¾èµ–æœ‰é™çš„æ ‡æ³¨ï¼Œéš¾ä»¥å¤„ç†å¤§è§„æ¨¡ã€ä¸è§„åˆ™å’Œæ— åºçš„3Dç‚¹äº‘æ•°æ®ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äº3Dé¢†åŸŸï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨2Då’Œ3Dæ•°æ®çš„äº’è¡¥æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç”±2DåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„åˆ†å‰²æ©ç ï¼Œæœ€å¤§åŒ–ç¨€ç–3Dæ ‡æ³¨çš„æ•ˆç”¨ï¼Œå¹¶é€šè¿‡å»ºç«‹3Dåœºæ™¯ä¸2Dè§†å›¾ä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œå°†2Dåˆ†å‰²æ©ç ä¼ æ’­åˆ°3Dç©ºé—´ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åº”ç”¨åŸºäºç½®ä¿¡åº¦å’Œä¸ç¡®å®šæ€§çš„è¿ç»­æ€§æ­£åˆ™åŒ–ï¼Œé€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå¯ç”¨æ ‡ç­¾æ± ï¼Œæœ€ç»ˆæå‡3Då¼±ç›‘ç£åˆ†å‰²çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³3Då¼±ç›‘ç£ç‚¹äº‘åˆ†å‰²ä¸­æ ‡æ³¨ç¨€ç¼ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªä¾èµ–æœ‰é™çš„3Dæ ‡æ³¨ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨2Dæ•°æ®çš„ä¼˜åŠ¿ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆ2DåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„åˆ†å‰²æ©ç ï¼Œé€šè¿‡å‡ ä½•å¯¹åº”å…³ç³»å°†2Dä¿¡æ¯æœ‰æ•ˆä¼ æ’­åˆ°3Dç©ºé—´ï¼Œä»è€Œå¢å¼º3Dæ ‡æ³¨çš„ç¨€ç–æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨2DåŸºç¡€æ¨¡å‹ç”Ÿæˆåˆ†å‰²æ©ç ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å‡ ä½•å¯¹åº”å…³ç³»å°†è¿™äº›æ©ç æ˜ å°„åˆ°3Dç‚¹äº‘ä¸­ï¼Œå½¢æˆæ›´ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†2Dåˆ†å‰²æ©ç ä¸3Dç‚¹äº‘ç›¸ç»“åˆï¼Œåˆ©ç”¨å‡ ä½•å¯¹åº”å…³ç³»æœ‰æ•ˆæ‰©å±•äº†ç¨€ç–çš„3Dæ ‡æ³¨ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºç½®ä¿¡åº¦å’Œä¸ç¡®å®šæ€§çš„è¿ç»­æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼Œç¡®ä¿é€‰æ‹©çš„ä¼ªæ ‡ç­¾å…·æœ‰è¾ƒé«˜çš„å¯é æ€§ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¯¹æ ‡ç­¾å™ªå£°çš„å¤„ç†æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†3Då¼±ç›‘ç£åˆ†å‰²çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œåˆ†å‰²ç²¾åº¦æé«˜äº†çº¦15%ï¼ŒéªŒè¯äº†2Dä¸3Dæ•°æ®ç»“åˆçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡3Dç‚¹äº‘åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥æœ‰æ•ˆæ”¹å–„ç¯å¢ƒæ„ŸçŸ¥å’Œç‰©ä½“è¯†åˆ«çš„æ€§èƒ½ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.

