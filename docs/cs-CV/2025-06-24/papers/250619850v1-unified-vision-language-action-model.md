---
layout: default
title: Unified Vision-Language-Action Model
---

# Unified Vision-Language-Action Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19850" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19850v1</a>
  <a href="https://arxiv.org/pdf/2506.19850.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19850v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19850v1', 'Unified Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**å¤‡æ³¨**: technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniVLAæ¨¡å‹ä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `å¤šæ¨¡æ€å­¦ä¹ ` `è‡ªå›å½’å»ºæ¨¡` `å› æœåŠ¨æ€` `æœºå™¨äººæ“ä½œ` `é•¿æ—¶é—´ä»»åŠ¡` `è§†é¢‘ç†è§£` `ç­–ç•¥å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨ç”ŸæˆåŠ¨ä½œä¿¡å·æ—¶ï¼Œå¾€å¾€å¿½è§†äº†è§†è§‰æ•°æ®ä¸­çš„æ—¶åºå’Œå› æœå…³ç³»ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„UniVLAæ¨¡å‹é€šè¿‡è‡ªå›å½’å»ºæ¨¡è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¿¡å·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œç‰¹åˆ«é€‚åˆå¤§è§„æ¨¡è§†é¢‘æ•°æ®çš„å­¦ä¹ ã€‚
3. UniVLAåœ¨å¤šä¸ªä»¿çœŸåŸºå‡†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œä¾‹å¦‚åœ¨LIBEROåŸºå‡†ä¸Šè¾¾åˆ°95.5%çš„å¹³å‡æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„85.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰å› å…¶åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ½œåŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é€šç”¨ç†è§£èƒ½åŠ›ç”ŸæˆåŠ¨ä½œä¿¡å·ï¼Œå¸¸å¸¸å¿½è§†è§†è§‰è§‚å¯Ÿä¸­è•´å«çš„ä¸°å¯Œæ—¶åºå’Œå› æœç»“æ„ã€‚æœ¬æ–‡æå‡ºäº†UniVLAï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€VLAæ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªå›å½’åœ°å°†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¿¡å·å»ºæ¨¡ä¸ºç¦»æ•£çš„æ ‡è®°åºåˆ—ã€‚è¿™ç§è¡¨è¿°æ–¹å¼ä½¿å¾—ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®ä¸­å­¦ä¹ çµæ´»çš„å¤šæ¨¡æ€ä»»åŠ¡æˆä¸ºå¯èƒ½ã€‚é€šè¿‡åœ¨åæœŸè®­ç»ƒä¸­å¼•å…¥ä¸–ç•Œå»ºæ¨¡ï¼ŒUniVLAæ•æ‰è§†é¢‘ä¸­çš„å› æœåŠ¨æ€ï¼Œä»è€Œæœ‰æ•ˆåœ°è½¬ç§»åˆ°ä¸‹æ¸¸ç­–ç•¥å­¦ä¹ ï¼Œå°¤å…¶é€‚ç”¨äºé•¿æ—¶é—´ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»¿çœŸåŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨ç”ŸæˆåŠ¨ä½œä¿¡å·æ—¶å¯¹æ—¶åºå’Œå› æœç»“æ„çš„å¿½è§†ï¼Œå¯¼è‡´çš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniVLAæ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼å°†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¿¡å·å»ºæ¨¡ä¸ºç¦»æ•£çš„æ ‡è®°åºåˆ—ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€ä¿¡æ¯å’Œå› æœå…³ç³»ï¼Œæå‡ä»»åŠ¡å­¦ä¹ çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰ç¼–ç å™¨ã€è¯­è¨€ç¼–ç å™¨å’ŒåŠ¨ä½œç”Ÿæˆå™¨ã€‚è§†è§‰ç¼–ç å™¨è´Ÿè´£æå–è§†é¢‘ä¸­çš„è§†è§‰ç‰¹å¾ï¼Œè¯­è¨€ç¼–ç å™¨å¤„ç†æ–‡æœ¬ä¿¡æ¯ï¼ŒåŠ¨ä½œç”Ÿæˆå™¨åˆ™æ ¹æ®å‰ä¸¤è€…ç”Ÿæˆç›¸åº”çš„åŠ¨ä½œä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniVLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªå›å½’å»ºæ¨¡æ–¹å¼å’ŒåæœŸè®­ç»ƒä¸­çš„ä¸–ç•Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘ä¸­çš„å› æœåŠ¨æ€ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚Transformerç»“æ„ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆäº†é¢„æµ‹å‡†ç¡®æ€§å’Œå› æœä¸€è‡´æ€§çš„å¤åˆæŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„åŠ¨ä½œä¿¡å·ä¸è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniVLAåœ¨å¤šä¸ªä»¿çœŸåŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œä¾‹å¦‚åœ¨LIBEROåŸºå‡†ä¸Šå®ç°äº†95.5%çš„å¹³å‡æˆåŠŸç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†pi0-FASTçš„85.5%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å®é™…åº”ç”¨åœºæ™¯å¦‚ALOHAæ“ä½œå’Œè‡ªåŠ¨é©¾é©¶ä¸­ä¹Ÿå±•ç°äº†è‰¯å¥½çš„é€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰ã€‚UniVLAæ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚çš„åŠ¨æ€ç¯å¢ƒä¸­ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æ—¶é—´è§„åˆ’å’Œå†³ç­–çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.

