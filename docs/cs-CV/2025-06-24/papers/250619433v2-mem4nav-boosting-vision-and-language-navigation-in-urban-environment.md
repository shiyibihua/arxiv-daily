---
layout: default
title: Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System
---

# Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19433" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19433v2</a>
  <a href="https://arxiv.org/pdf/2506.19433.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19433v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19433v2', 'Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24 (æ›´æ–°: 2025-10-10)

**å¤‡æ³¨**: The paper is currently under investigation regarding concerns of potential academic misconduct. While the investigation is ongoing, the authors have voluntarily requested to withdraw the manuscript

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/tsinghua-fib-lab/Mem4Nav)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMem4Navä»¥è§£å†³åŸå¸‚ç¯å¢ƒä¸­çš„è§†è§‰-è¯­è¨€å¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€å¯¼èˆª` `é•¿çŸ­æœŸè®°å¿†` `ç©ºé—´è®¤çŸ¥` `ç¨€ç–å…«å‰æ ‘` `è¯­ä¹‰æ‹“æ‰‘å›¾` `å¤šæ¨¡æ€èåˆ` `æ™ºèƒ½ä½“å¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€å¯¼èˆªæ–¹æ³•åœ¨å¤„ç†å¤æ‚åŸå¸‚ç¯å¢ƒæ—¶ï¼Œç¼ºä¹ç»Ÿä¸€çš„è®°å¿†æœºåˆ¶ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºçš„Mem4Navé€šè¿‡å±‚æ¬¡åŒ–çš„ç©ºé—´è®¤çŸ¥é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿï¼Œç»“åˆç¨€ç–å…«å‰æ ‘å’Œè¯­ä¹‰æ‹“æ‰‘å›¾ï¼Œå¢å¼ºäº†å¯¼èˆªèƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMem4Navåœ¨ä»»åŠ¡å®Œæˆç‡ã€SPDå’ŒnDTWç­‰æŒ‡æ ‡ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§è§„æ¨¡åŸå¸‚ç¯å¢ƒä¸­ï¼Œè§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰è¦æ±‚å…·èº«æ™ºèƒ½ä½“å°†è¯­è¨€æŒ‡ä»¤ä¸å¤æ‚åœºæ™¯ç›¸ç»“åˆï¼Œå¹¶åœ¨è¾ƒé•¿æ—¶é—´èŒƒå›´å†…å›å¿†ç›¸å…³ç»éªŒã€‚ç°æœ‰çš„æ¨¡å—åŒ–ç®¡é“è™½ç„¶æä¾›äº†è§£é‡Šæ€§ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„è®°å¿†ï¼Œè€Œç«¯åˆ°ç«¯çš„ï¼ˆMï¼‰LLMæ™ºèƒ½ä½“åœ¨èåˆè§†è§‰å’Œè¯­è¨€æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†å—åˆ°å›ºå®šä¸Šä¸‹æ–‡çª—å£å’Œéšå¼ç©ºé—´æ¨ç†çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†Mem4Navï¼Œä¸€ä¸ªå±‚æ¬¡åŒ–ç©ºé—´è®¤çŸ¥é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿï¼Œå¯ä»¥å¢å¼ºä»»ä½•VLNéª¨å¹²ç½‘ç»œã€‚Mem4Navç»“åˆç¨€ç–å…«å‰æ ‘è¿›è¡Œç»†ç²’åº¦ä½“ç´ ç´¢å¼•ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰æ‹“æ‰‘å›¾å®ç°é«˜å±‚æ¬¡åœ°æ ‡è¿æ¥ï¼Œå°†ä¸¤è€…å­˜å‚¨åœ¨é€šè¿‡å¯é€†TransformeråµŒå…¥çš„å¯è®­ç»ƒè®°å¿†æ ‡è®°ä¸­ã€‚é•¿æ—¶è®°å¿†ï¼ˆLTMï¼‰å‹ç¼©å¹¶ä¿ç•™å†å²è§‚å¯Ÿï¼Œè€ŒçŸ­æ—¶è®°å¿†ï¼ˆSTMï¼‰ç¼“å­˜ç›¸å¯¹åæ ‡ä¸­çš„æœ€è¿‘å¤šæ¨¡æ€æ¡ç›®ï¼Œä»¥å®ç°å®æ—¶éšœç¢ç‰©è§„é¿å’Œå±€éƒ¨è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMem4Navåœ¨Touchdownå’ŒMap2Seqä¸Šç›¸è¾ƒäºä¸‰ç§éª¨å¹²ç½‘ç»œï¼ˆæ¨¡å—åŒ–ã€åŸºäºæç¤ºçš„æœ€å…ˆè¿›VLNå’ŒåŸºäºè·¨æ­¥æ³¨æ„åŠ›çš„æœ€å…ˆè¿›VLNï¼‰åœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šæå‡äº†7-13ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”æœ‰æ•ˆå‡å°‘äº†SPDï¼ŒnDTWæå‡è¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­ï¼Œè§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ™ºèƒ½ä½“å¦‚ä½•æœ‰æ•ˆåœ°å°†è¯­è¨€æŒ‡ä»¤ä¸è§†è§‰ä¿¡æ¯ç»“åˆçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®°å¿†ç®¡ç†å’Œç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMem4Navçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–çš„é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿï¼Œç»“åˆç¨€ç–å…«å‰æ ‘å’Œè¯­ä¹‰æ‹“æ‰‘å›¾ï¼Œæ¥å¢å¼ºæ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›å’Œç©ºé—´è®¤çŸ¥ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤æ‚åœºæ™¯ä¸­çš„å¯¼èˆªä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMem4Navçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é•¿æ—¶è®°å¿†ï¼ˆLTMï¼‰å’ŒçŸ­æ—¶è®°å¿†ï¼ˆSTMï¼‰ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚LTMç”¨äºå‹ç¼©å’Œä¿ç•™å†å²è§‚å¯Ÿï¼Œè€ŒSTMåˆ™ç¼“å­˜æœ€è¿‘çš„å¤šæ¨¡æ€æ¡ç›®ï¼Œä»¥æ”¯æŒå®æ—¶å†³ç­–ã€‚ç³»ç»Ÿé€šè¿‡å¯é€†Transformerå°†ä¿¡æ¯åµŒå…¥å¯è®­ç»ƒçš„è®°å¿†æ ‡è®°ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šMem4Navçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å±‚æ¬¡åŒ–çš„è®°å¿†ç»“æ„ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æœ‰æ•ˆåœ°æ£€ç´¢å’Œåˆ©ç”¨å†å²ä¿¡æ¯ï¼Œä¸ä¼ ç»Ÿçš„å›ºå®šä¸Šä¸‹æ–‡çª—å£æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ™ºèƒ½ä½“çš„å¯¼èˆªèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLTMå’ŒSTMçš„äº¤äº’æœºåˆ¶è‡³å…³é‡è¦ï¼ŒSTMé€šè¿‡åŠ¨æ€ä¸Šä¸‹æ–‡æ£€ç´¢æ¥ä¼˜åŒ–å®æ—¶å†³ç­–ï¼Œè€ŒLTMåˆ™æä¾›æ·±å±‚å†å²ä¿¡æ¯çš„æ— æŸè§£ç ï¼Œç¡®ä¿äº†ä¿¡æ¯çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Touchdownå’ŒMap2SeqåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMem4Navåœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šæå‡äº†7-13ä¸ªç™¾åˆ†ç‚¹ï¼ŒSPDæ˜¾è‘—é™ä½ï¼ŒnDTWæŒ‡æ ‡æå‡è¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹ï¼ŒéªŒè¯äº†å…¶åœ¨è§†è§‰-è¯­è¨€å¯¼èˆªä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶è½¦è¾†å’Œå¢å¼ºç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ã€‚æœªæ¥ï¼ŒMem4Navçš„è®¾è®¡ç†å¿µä¹Ÿå¯èƒ½è¢«åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨æ›´å¹¿æ³›çš„åº”ç”¨ä¸­å®ç°æ›´é«˜çš„æ™ºèƒ½æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.

