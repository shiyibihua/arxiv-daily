---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-24
---

# cs.CVï¼ˆ2025-06-24ï¼‰

ğŸ“Š å…± **5** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250619257v2-msr-align-policy-grounded-multimodal-alignment-for-safety-aware-reas.html">MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models</a></td>
  <td>æå‡ºMSR-Alignä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹å®‰å…¨å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19257v2" data-paper-url="./papers/250619257v2-msr-align-policy-grounded-multimodal-alignment-for-safety-aware-reas.html" onclick="toggleFavorite(this, '2506.19257v2', 'MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250619433v2-mem4nav-boosting-vision-and-language-navigation-in-urban-environment.html">Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System</a></td>
  <td>æå‡ºMem4Navä»¥è§£å†³åŸå¸‚ç¯å¢ƒä¸­çš„è§†è§‰-è¯­è¨€å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19433v2" data-paper-url="./papers/250619433v2-mem4nav-boosting-vision-and-language-navigation-in-urban-environment.html" onclick="toggleFavorite(this, '2506.19433v2', 'Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250619593v1-implementing-blind-navigation-through-multi-modal-sensing-and-gait-g.html">Implementing blind navigation through multi-modal sensing and gait guidance</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ„ŸçŸ¥ä¸æ­¥æ€å¼•å¯¼çš„ç›²äººå¯¼èˆªç³»ç»Ÿä»¥è§£å†³è§†éšœäººå£«å¯¼èˆªå›°éš¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19593v1" data-paper-url="./papers/250619593v1-implementing-blind-navigation-through-multi-modal-sensing-and-gait-g.html" onclick="toggleFavorite(this, '2506.19593v1', 'Implementing blind navigation through multi-modal sensing and gait guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>4</td>
  <td><a href="./papers/250619850v1-unified-vision-language-action-model.html">Unified Vision-Language-Action Model</a></td>
  <td>æå‡ºUniVLAæ¨¡å‹ä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19850v1" data-paper-url="./papers/250619850v1-unified-vision-language-action-model.html" onclick="toggleFavorite(this, '2506.19850v1', 'Unified Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250619288v2-da-yu-towards-usv-based-image-captioning-for-waterway-surveillance-a.html">Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding</a></td>
  <td>æå‡ºDa Yuä»¥è§£å†³æ°´é“ç›‘æµ‹ä¸­çš„å›¾åƒæè¿°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.19288v2" data-paper-url="./papers/250619288v2-da-yu-towards-usv-based-image-captioning-for-waterway-surveillance-a.html" onclick="toggleFavorite(this, '2506.19288v2', 'Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)