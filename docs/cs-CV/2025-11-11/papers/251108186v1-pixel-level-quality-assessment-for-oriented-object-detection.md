---
layout: default
title: Pixel-level Quality Assessment for Oriented Object Detection
---

# Pixel-level Quality Assessment for Oriented Object Detection

**arXiv**: [2511.08186v1](https://arxiv.org/abs/2511.08186) | [PDF](https://arxiv.org/pdf/2511.08186.pdf)

**ä½œè€…**: Yunhui Zhu, Buliao Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåƒç´ çº§è´¨é‡è¯„ä¼°PQAï¼Œè§£å†³æœ‰å‘ç›®æ ‡æ£€æµ‹ä¸­IoUé¢„æµ‹çš„ç»“æž„è€¦åˆé—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœ‰å‘ç›®æ ‡æ£€æµ‹` `è´¨é‡è¯„ä¼°` `åƒç´ çº§` `ç©ºé—´ä¸€è‡´æ€§` `é¥æ„Ÿå›¾åƒ` `å®šä½ç²¾åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æœ‰å‘ç›®æ ‡æ£€æµ‹å™¨ä¾èµ–æ¡†çº§IoUé¢„æµ‹ä½œä¸ºå®šä½è´¨é‡çš„ä»£ç†ï¼Œä½†å­˜åœ¨ç»“æž„è€¦åˆé—®é¢˜ï¼Œå¯¼è‡´å®šä½è´¨é‡è¢«é«˜ä¼°ã€‚
2. è®ºæ–‡æå‡ºåƒç´ çº§è´¨é‡è¯„ä¼°ï¼ˆPQAï¼‰æ¡†æž¶ï¼Œé€šè¿‡åƒç´ çº§ç©ºé—´ä¸€è‡´æ€§ç§¯åˆ†æ¥é¿å…ç›´æŽ¥æ¯”è¾ƒé¢„æµ‹æ¡†å’ŒçœŸå®žæ¡†ï¼Œæ¶ˆé™¤ç›¸ä¼¼æ€§åå·®ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒPQAå¯ä»¥æ— ç¼é›†æˆåˆ°å¤šç§æœ‰å‘ç›®æ ‡æ£€æµ‹å™¨ä¸­ï¼Œå¹¶åœ¨HRSC2016å’ŒDOTAæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŽ°ä»£æœ‰å‘ç›®æ ‡æ£€æµ‹å™¨é€šå¸¸é¢„æµ‹ä¸€ç³»åˆ—è¾¹ç•Œæ¡†ï¼Œå¹¶æ ¹æ®ä¼°è®¡çš„å®šä½è´¨é‡é€‰æ‹©æŽ’åé å‰çš„æ¡†ã€‚å®žçŽ°é«˜æ£€æµ‹æ€§èƒ½éœ€è¦ä¼°è®¡çš„è´¨é‡ä¸Žå®žé™…å®šä½ç²¾åº¦ç´§å¯†å¯¹é½ã€‚ä¸ºæ­¤ï¼ŒçŽ°æœ‰æ–¹æ³•é¢„æµ‹é¢„æµ‹æ¡†å’ŒçœŸå®žæ¡†ä¹‹é—´çš„IoUä½œä¸ºå®šä½è´¨é‡çš„ä»£ç†ã€‚ç„¶è€Œï¼Œæ¡†çº§IoUé¢„æµ‹å­˜åœ¨ç»“æž„è€¦åˆé—®é¢˜ï¼šç”±äºŽé¢„æµ‹æ¡†æºäºŽæ£€æµ‹å™¨å¯¹çœŸå®žæ¡†çš„å†…éƒ¨ä¼°è®¡ï¼Œå› æ­¤åŸºäºŽå®ƒä»¬ç›¸ä¼¼æ€§çš„é¢„æµ‹IoUå¯¹äºŽå®šä½ä¸è‰¯çš„æ¡†å¯èƒ½è¢«é«˜ä¼°ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åƒç´ çº§è´¨é‡è¯„ä¼°ï¼ˆPQAï¼‰æ¡†æž¶ï¼Œè¯¥æ¡†æž¶ç”¨åƒç´ çº§ç©ºé—´ä¸€è‡´æ€§çš„ç§¯åˆ†ä»£æ›¿äº†æ¡†çº§IoUé¢„æµ‹ã€‚PQAæµ‹é‡æ¯ä¸ªåƒç´ ç›¸å¯¹äºŽé¢„æµ‹æ¡†çš„ä½ç½®ä¸Žå…¶ç›¸å¯¹äºŽçœŸå®žæ¡†çš„å¯¹åº”ä½ç½®ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚é€šè¿‡åœ¨åƒç´ çº§åˆ«æ“ä½œï¼ŒPQAé¿å…äº†ç›´æŽ¥æ¯”è¾ƒé¢„æµ‹æ¡†ä¸Žä¼°è®¡çš„çœŸå®žæ¡†ï¼Œä»Žè€Œæ¶ˆé™¤äº†æ¡†çº§IoUé¢„æµ‹ä¸­å›ºæœ‰çš„ç›¸ä¼¼æ€§åå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ç§¯åˆ†åº¦é‡ï¼Œå°†åƒç´ çº§ç©ºé—´ä¸€è‡´æ€§èšåˆä¸ºç»Ÿä¸€çš„è´¨é‡åˆ†æ•°ï¼Œä»Žè€Œæ›´å‡†ç¡®åœ°è¿‘ä¼¼å®žé™…å®šä½è´¨é‡ã€‚åœ¨HRSC2016å’ŒDOTAä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒPQAå¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§æœ‰å‘ç›®æ ‡æ£€æµ‹å™¨ä¸­ï¼ŒæŒç»­æé«˜æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œåœ¨Rotated RetinaNetä¸Š+5.96% AP$_{50:95}$ï¼Œåœ¨STDä¸Š+2.32%ï¼‰ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æœ‰å‘ç›®æ ‡æ£€æµ‹å™¨ä½¿ç”¨é¢„æµ‹æ¡†å’ŒçœŸå®žæ¡†çš„IoUä½œä¸ºå®šä½è´¨é‡çš„æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œç”±äºŽé¢„æµ‹æ¡†æœ¬èº«å°±æ˜¯ç”±æ£€æµ‹å™¨é¢„æµ‹å¾—åˆ°çš„ï¼Œå› æ­¤é¢„æµ‹æ¡†ä¸ŽçœŸå®žæ¡†çš„IoUä¼šå—åˆ°é¢„æµ‹æ¡†è´¨é‡çš„å½±å“ï¼Œå¯¼è‡´å¯¹å®šä½è´¨é‡çš„è¯„ä¼°äº§ç”Ÿåå·®ï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹æ¡†å®šä½ä¸å‡†ç¡®æ—¶ï¼ŒIoUå¯èƒ½ä¼šè¢«é«˜ä¼°ã€‚è¿™ç§ç»“æž„è€¦åˆé—®é¢˜é™åˆ¶äº†æ£€æµ‹å™¨çš„æ€§èƒ½ä¸Šé™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è´¨é‡è¯„ä¼°ä»Žæ¡†çº§åˆ«é™åˆ°åƒç´ çº§åˆ«ã€‚ä¸å†ç›´æŽ¥æ¯”è¾ƒé¢„æµ‹æ¡†å’ŒçœŸå®žæ¡†çš„æ•´ä½“ç›¸ä¼¼åº¦ï¼ˆIoUï¼‰ï¼Œè€Œæ˜¯è¯„ä¼°æ¯ä¸ªåƒç´ ç›¸å¯¹äºŽé¢„æµ‹æ¡†çš„ä½ç½®å’Œç›¸å¯¹äºŽçœŸå®žæ¡†çš„ä½ç½®çš„ä¸€è‡´æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé¿å…äº†ç›´æŽ¥æ¯”è¾ƒé¢„æµ‹æ¡†å’ŒçœŸå®žæ¡†ï¼Œä»Žè€Œæ¶ˆé™¤äº†ç»“æž„è€¦åˆå¸¦æ¥çš„åå·®ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šPQAæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) å¯¹äºŽæ¯ä¸ªé¢„æµ‹æ¡†ï¼Œç¡®å®šå…¶å¯¹åº”çš„å›¾åƒåŒºåŸŸã€‚2) å¯¹äºŽè¯¥åŒºåŸŸå†…çš„æ¯ä¸ªåƒç´ ï¼Œè®¡ç®—å…¶ç›¸å¯¹äºŽé¢„æµ‹æ¡†çš„ä½ç½®å’Œç›¸å¯¹äºŽçœŸå®žæ¡†çš„ä½ç½®ã€‚3) ä½¿ç”¨æŸç§åº¦é‡æ–¹å¼ï¼ˆè®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„ç§¯åˆ†åº¦é‡ï¼‰æ¥è¡¡é‡è¿™ä¸¤ä¸ªä½ç½®çš„ä¸€è‡´æ€§ï¼Œå¾—åˆ°ä¸€ä¸ªåƒç´ çº§åˆ«çš„è´¨é‡å¾—åˆ†ã€‚4) å°†æ‰€æœ‰åƒç´ çš„è´¨é‡å¾—åˆ†è¿›è¡Œèšåˆï¼Œå¾—åˆ°ä¸€ä¸ªç»Ÿä¸€çš„è´¨é‡åˆ†æ•°ï¼Œä½œä¸ºè¯¥é¢„æµ‹æ¡†çš„æœ€ç»ˆè´¨é‡è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†è´¨é‡è¯„ä¼°çš„ç²’åº¦ä»Žæ¡†çº§åˆ«é™ä½Žåˆ°åƒç´ çº§åˆ«ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›´æŽ¥æ¯”è¾ƒé¢„æµ‹æ¡†å’ŒçœŸå®žæ¡†çš„IoUä¸åŒï¼ŒPQAé€šè¿‡è¯„ä¼°åƒç´ çº§åˆ«çš„ç©ºé—´ä¸€è‡´æ€§æ¥é—´æŽ¥è¡¡é‡å®šä½è´¨é‡ï¼Œä»Žè€Œé¿å…äº†ç»“æž„è€¦åˆé—®é¢˜ã€‚è¿™ç§åƒç´ çº§åˆ«çš„è¯„ä¼°æ–¹å¼èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ å®šä½çš„çœŸå®žè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç§¯åˆ†åº¦é‡æ¥èšåˆåƒç´ çº§åˆ«çš„ç©ºé—´ä¸€è‡´æ€§ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æž„ç­‰ï¼‰å¯èƒ½å› é›†æˆçš„æ£€æµ‹å™¨è€Œå¼‚ï¼Œä½†æ ¸å¿ƒæ€æƒ³éƒ½æ˜¯åˆ©ç”¨åƒç´ çº§åˆ«çš„ç©ºé—´ä¸€è‡´æ€§æ¥è¯„ä¼°å®šä½è´¨é‡ã€‚å…³é”®åœ¨äºŽå¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„åƒç´ çº§ä¸€è‡´æ€§åº¦é‡å’Œèšåˆæ–¹æ³•ï¼Œä»¥èŽ·å¾—æ›´å‡†ç¡®çš„è´¨é‡è¯„ä¼°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒPQAå¯ä»¥æ˜¾è‘—æå‡çŽ°æœ‰æœ‰å‘ç›®æ ‡æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨HRSC2016æ•°æ®é›†ä¸Šï¼ŒPQAå°†Rotated RetinaNetçš„AP$_{50:95}$æé«˜äº†5.96%ï¼Œåœ¨DOTAæ•°æ®é›†ä¸Šï¼Œå°†STDçš„APæé«˜äº†2.32%ã€‚è¿™äº›ç»“æžœè¯æ˜Žäº†PQAåœ¨æé«˜æœ‰å‘ç›®æ ‡æ£€æµ‹ç²¾åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽéœ€è¦é«˜ç²¾åº¦æœ‰å‘ç›®æ ‡æ£€æµ‹çš„åœºæ™¯ï¼Œä¾‹å¦‚é¥æ„Ÿå›¾åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šã€åŒ»å­¦å›¾åƒåˆ†æžç­‰ã€‚é€šè¿‡æé«˜æœ‰å‘ç›®æ ‡æ£€æµ‹çš„ç²¾åº¦ï¼Œå¯ä»¥æå‡è¿™äº›åº”ç”¨åœºæ™¯çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä¾‹å¦‚æ›´å‡†ç¡®åœ°è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„å»ºç­‘ç‰©å’Œè½¦è¾†ï¼Œä»Žè€Œæ”¯æŒåŸŽå¸‚è§„åˆ’å’ŒçŽ¯å¢ƒç›‘æµ‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).

