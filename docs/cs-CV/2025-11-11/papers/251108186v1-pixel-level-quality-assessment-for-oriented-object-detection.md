---
layout: default
title: Pixel-level Quality Assessment for Oriented Object Detection
---

# Pixel-level Quality Assessment for Oriented Object Detection

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08186" target="_blank" class="toolbar-btn">arXiv: 2511.08186v1</a>
    <a href="https://arxiv.org/pdf/2511.08186.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08186v1" 
            onclick="toggleFavorite(this, '2511.08186v1', 'Pixel-level Quality Assessment for Oriented Object Detection')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yunhui Zhu, Buliao Huang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂÉèÁ¥†Á∫ßË¥®ÈáèËØÑ‰º∞PQAÔºåËß£ÂÜ≥ÊúâÂêëÁõÆÊ†áÊ£ÄÊµã‰∏≠IoUÈ¢ÑÊµãÁöÑÁªìÊûÑËÄ¶ÂêàÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÊúâÂêëÁõÆÊ†áÊ£ÄÊµã` `Ë¥®ÈáèËØÑ‰º∞` `ÂÉèÁ¥†Á∫ß` `Á©∫Èó¥‰∏ÄËá¥ÊÄß` `ÈÅ•ÊÑüÂõæÂÉè` `ÂÆö‰ΩçÁ≤æÂ∫¶`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÂô®‰æùËµñÊ°ÜÁ∫ßIoUÈ¢ÑÊµã‰Ωú‰∏∫ÂÆö‰ΩçË¥®ÈáèÁöÑ‰ª£ÁêÜÔºå‰ΩÜÂ≠òÂú®ÁªìÊûÑËÄ¶ÂêàÈóÆÈ¢òÔºåÂØºËá¥ÂÆö‰ΩçË¥®ÈáèË¢´È´ò‰º∞„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÂÉèÁ¥†Á∫ßË¥®ÈáèËØÑ‰º∞ÔºàPQAÔºâÊ°ÜÊû∂ÔºåÈÄöËøáÂÉèÁ¥†Á∫ßÁ©∫Èó¥‰∏ÄËá¥ÊÄßÁßØÂàÜÊù•ÈÅøÂÖçÁõ¥Êé•ÊØîËæÉÈ¢ÑÊµãÊ°ÜÂíåÁúüÂÆûÊ°ÜÔºåÊ∂àÈô§Áõ∏‰ººÊÄßÂÅèÂ∑Æ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPQAÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Â§öÁßçÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÂô®‰∏≠ÔºåÂπ∂Âú®HRSC2016ÂíåDOTAÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçáÊ£ÄÊµãÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞‰ª£ÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÂô®ÈÄöÂ∏∏È¢ÑÊµã‰∏ÄÁ≥ªÂàóËæπÁïåÊ°ÜÔºåÂπ∂Ê†πÊçÆ‰º∞ËÆ°ÁöÑÂÆö‰ΩçË¥®ÈáèÈÄâÊã©ÊéíÂêçÈù†ÂâçÁöÑÊ°Ü„ÄÇÂÆûÁé∞È´òÊ£ÄÊµãÊÄßËÉΩÈúÄË¶Å‰º∞ËÆ°ÁöÑË¥®Èáè‰∏éÂÆûÈôÖÂÆö‰ΩçÁ≤æÂ∫¶Á¥ßÂØÜÂØπÈΩê„ÄÇ‰∏∫Ê≠§ÔºåÁé∞ÊúâÊñπÊ≥ïÈ¢ÑÊµãÈ¢ÑÊµãÊ°ÜÂíåÁúüÂÆûÊ°Ü‰πãÈó¥ÁöÑIoU‰Ωú‰∏∫ÂÆö‰ΩçË¥®ÈáèÁöÑ‰ª£ÁêÜ„ÄÇÁÑ∂ËÄåÔºåÊ°ÜÁ∫ßIoUÈ¢ÑÊµãÂ≠òÂú®ÁªìÊûÑËÄ¶ÂêàÈóÆÈ¢òÔºöÁî±‰∫éÈ¢ÑÊµãÊ°ÜÊ∫ê‰∫éÊ£ÄÊµãÂô®ÂØπÁúüÂÆûÊ°ÜÁöÑÂÜÖÈÉ®‰º∞ËÆ°ÔºåÂõ†Ê≠§Âü∫‰∫éÂÆÉ‰ª¨Áõ∏‰ººÊÄßÁöÑÈ¢ÑÊµãIoUÂØπ‰∫éÂÆö‰Ωç‰∏çËâØÁöÑÊ°ÜÂèØËÉΩË¢´È´ò‰º∞„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏™ÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÉèÁ¥†Á∫ßË¥®ÈáèËØÑ‰º∞ÔºàPQAÔºâÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Áî®ÂÉèÁ¥†Á∫ßÁ©∫Èó¥‰∏ÄËá¥ÊÄßÁöÑÁßØÂàÜ‰ª£Êõø‰∫ÜÊ°ÜÁ∫ßIoUÈ¢ÑÊµã„ÄÇPQAÊµãÈáèÊØè‰∏™ÂÉèÁ¥†Áõ∏ÂØπ‰∫éÈ¢ÑÊµãÊ°ÜÁöÑ‰ΩçÁΩÆ‰∏éÂÖ∂Áõ∏ÂØπ‰∫éÁúüÂÆûÊ°ÜÁöÑÂØπÂ∫î‰ΩçÁΩÆ‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáÂú®ÂÉèÁ¥†Á∫ßÂà´Êìç‰ΩúÔºåPQAÈÅøÂÖç‰∫ÜÁõ¥Êé•ÊØîËæÉÈ¢ÑÊµãÊ°Ü‰∏é‰º∞ËÆ°ÁöÑÁúüÂÆûÊ°ÜÔºå‰ªéËÄåÊ∂àÈô§‰∫ÜÊ°ÜÁ∫ßIoUÈ¢ÑÊµã‰∏≠Âõ∫ÊúâÁöÑÁõ∏‰ººÊÄßÂÅèÂ∑Æ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁßØÂàÜÂ∫¶ÈáèÔºåÂ∞ÜÂÉèÁ¥†Á∫ßÁ©∫Èó¥‰∏ÄËá¥ÊÄßËÅöÂêà‰∏∫Áªü‰∏ÄÁöÑË¥®ÈáèÂàÜÊï∞Ôºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞Ëøë‰ººÂÆûÈôÖÂÆö‰ΩçË¥®Èáè„ÄÇÂú®HRSC2016ÂíåDOTA‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåPQAÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞ÂêÑÁßçÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÂô®‰∏≠ÔºåÊåÅÁª≠ÊèêÈ´òÊÄßËÉΩÔºà‰æãÂ¶ÇÔºåÂú®Rotated RetinaNet‰∏ä+5.96% AP$_{50:95}$ÔºåÂú®STD‰∏ä+2.32%Ôºâ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÂô®‰ΩøÁî®È¢ÑÊµãÊ°ÜÂíåÁúüÂÆûÊ°ÜÁöÑIoU‰Ωú‰∏∫ÂÆö‰ΩçË¥®ÈáèÁöÑÊåáÊ†á„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÈ¢ÑÊµãÊ°ÜÊú¨Ë∫´Â∞±ÊòØÁî±Ê£ÄÊµãÂô®È¢ÑÊµãÂæóÂà∞ÁöÑÔºåÂõ†Ê≠§È¢ÑÊµãÊ°Ü‰∏éÁúüÂÆûÊ°ÜÁöÑIoU‰ºöÂèóÂà∞È¢ÑÊµãÊ°ÜË¥®ÈáèÁöÑÂΩ±ÂìçÔºåÂØºËá¥ÂØπÂÆö‰ΩçË¥®ÈáèÁöÑËØÑ‰º∞‰∫ßÁîüÂÅèÂ∑ÆÔºåÂ∞§ÂÖ∂ÊòØÂú®È¢ÑÊµãÊ°ÜÂÆö‰Ωç‰∏çÂáÜÁ°ÆÊó∂ÔºåIoUÂèØËÉΩ‰ºöË¢´È´ò‰º∞„ÄÇËøôÁßçÁªìÊûÑËÄ¶ÂêàÈóÆÈ¢òÈôêÂà∂‰∫ÜÊ£ÄÊµãÂô®ÁöÑÊÄßËÉΩ‰∏äÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜË¥®ÈáèËØÑ‰º∞‰ªéÊ°ÜÁ∫ßÂà´ÈôçÂà∞ÂÉèÁ¥†Á∫ßÂà´„ÄÇ‰∏çÂÜçÁõ¥Êé•ÊØîËæÉÈ¢ÑÊµãÊ°ÜÂíåÁúüÂÆûÊ°ÜÁöÑÊï¥‰ΩìÁõ∏‰ººÂ∫¶ÔºàIoUÔºâÔºåËÄåÊòØËØÑ‰º∞ÊØè‰∏™ÂÉèÁ¥†Áõ∏ÂØπ‰∫éÈ¢ÑÊµãÊ°ÜÁöÑ‰ΩçÁΩÆÂíåÁõ∏ÂØπ‰∫éÁúüÂÆûÊ°ÜÁöÑ‰ΩçÁΩÆÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÈÅøÂÖç‰∫ÜÁõ¥Êé•ÊØîËæÉÈ¢ÑÊµãÊ°ÜÂíåÁúüÂÆûÊ°ÜÔºå‰ªéËÄåÊ∂àÈô§‰∫ÜÁªìÊûÑËÄ¶ÂêàÂ∏¶Êù•ÁöÑÂÅèÂ∑Æ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPQAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÂØπ‰∫éÊØè‰∏™È¢ÑÊµãÊ°ÜÔºåÁ°ÆÂÆöÂÖ∂ÂØπÂ∫îÁöÑÂõæÂÉèÂå∫Âüü„ÄÇ2) ÂØπ‰∫éËØ•Âå∫ÂüüÂÜÖÁöÑÊØè‰∏™ÂÉèÁ¥†ÔºåËÆ°ÁÆóÂÖ∂Áõ∏ÂØπ‰∫éÈ¢ÑÊµãÊ°ÜÁöÑ‰ΩçÁΩÆÂíåÁõ∏ÂØπ‰∫éÁúüÂÆûÊ°ÜÁöÑ‰ΩçÁΩÆ„ÄÇ3) ‰ΩøÁî®ÊüêÁßçÂ∫¶ÈáèÊñπÂºèÔºàËÆ∫Êñá‰∏≠ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁßØÂàÜÂ∫¶ÈáèÔºâÊù•Ë°°ÈáèËøô‰∏§‰∏™‰ΩçÁΩÆÁöÑ‰∏ÄËá¥ÊÄßÔºåÂæóÂà∞‰∏Ä‰∏™ÂÉèÁ¥†Á∫ßÂà´ÁöÑË¥®ÈáèÂæóÂàÜ„ÄÇ4) Â∞ÜÊâÄÊúâÂÉèÁ¥†ÁöÑË¥®ÈáèÂæóÂàÜËøõË°åËÅöÂêàÔºåÂæóÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑË¥®ÈáèÂàÜÊï∞Ôºå‰Ωú‰∏∫ËØ•È¢ÑÊµãÊ°ÜÁöÑÊúÄÁªàË¥®ÈáèËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜË¥®ÈáèËØÑ‰º∞ÁöÑÁ≤íÂ∫¶‰ªéÊ°ÜÁ∫ßÂà´Èôç‰ΩéÂà∞ÂÉèÁ¥†Á∫ßÂà´„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ¥Êé•ÊØîËæÉÈ¢ÑÊµãÊ°ÜÂíåÁúüÂÆûÊ°ÜÁöÑIoU‰∏çÂêåÔºåPQAÈÄöËøáËØÑ‰º∞ÂÉèÁ¥†Á∫ßÂà´ÁöÑÁ©∫Èó¥‰∏ÄËá¥ÊÄßÊù•Èó¥Êé•Ë°°ÈáèÂÆö‰ΩçË¥®ÈáèÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÁªìÊûÑËÄ¶ÂêàÈóÆÈ¢ò„ÄÇËøôÁßçÂÉèÁ¥†Á∫ßÂà´ÁöÑËØÑ‰º∞ÊñπÂºèËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÂèçÊò†ÂÆö‰ΩçÁöÑÁúüÂÆûË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÁßØÂàÜÂ∫¶ÈáèÊù•ËÅöÂêàÂÉèÁ¥†Á∫ßÂà´ÁöÑÁ©∫Èó¥‰∏ÄËá¥ÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊäÄÊúØÁªÜËäÇÔºàÂ¶ÇÊçüÂ§±ÂáΩÊï∞„ÄÅÁΩëÁªúÁªìÊûÑÁ≠âÔºâÂèØËÉΩÂõ†ÈõÜÊàêÁöÑÊ£ÄÊµãÂô®ËÄåÂºÇÔºå‰ΩÜÊ†∏ÂøÉÊÄùÊÉ≥ÈÉΩÊòØÂà©Áî®ÂÉèÁ¥†Á∫ßÂà´ÁöÑÁ©∫Èó¥‰∏ÄËá¥ÊÄßÊù•ËØÑ‰º∞ÂÆö‰ΩçË¥®Èáè„ÄÇÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïËÆæËÆ°ÊúâÊïàÁöÑÂÉèÁ¥†Á∫ß‰∏ÄËá¥ÊÄßÂ∫¶ÈáèÂíåËÅöÂêàÊñπÊ≥ïÔºå‰ª•Ëé∑ÂæóÊõ¥ÂáÜÁ°ÆÁöÑË¥®ÈáèËØÑ‰º∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPQAÂèØ‰ª•ÊòæËëóÊèêÂçáÁé∞ÊúâÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÂô®ÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®HRSC2016Êï∞ÊçÆÈõÜ‰∏äÔºåPQAÂ∞ÜRotated RetinaNetÁöÑAP$_{50:95}$ÊèêÈ´ò‰∫Ü5.96%ÔºåÂú®DOTAÊï∞ÊçÆÈõÜ‰∏äÔºåÂ∞ÜSTDÁöÑAPÊèêÈ´ò‰∫Ü2.32%„ÄÇËøô‰∫õÁªìÊûúËØÅÊòé‰∫ÜPQAÂú®ÊèêÈ´òÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÁ≤æÂ∫¶ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÈ´òÁ≤æÂ∫¶ÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÈÅ•ÊÑüÂõæÂÉèÂàÜÊûê„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩ‰∫§ÈÄö„ÄÅÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊúâÂêëÁõÆÊ†áÊ£ÄÊµãÁöÑÁ≤æÂ∫¶ÔºåÂèØ‰ª•ÊèêÂçáËøô‰∫õÂ∫îÁî®Âú∫ÊôØÁöÑÊÄßËÉΩÂíåÂèØÈù†ÊÄßÔºå‰æãÂ¶ÇÊõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´ÈÅ•ÊÑüÂõæÂÉè‰∏≠ÁöÑÂª∫Á≠ëÁâ©ÂíåËΩ¶ËæÜÔºå‰ªéËÄåÊîØÊåÅÂüéÂ∏ÇËßÑÂàíÂíåÁéØÂ¢ÉÁõëÊµã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).

