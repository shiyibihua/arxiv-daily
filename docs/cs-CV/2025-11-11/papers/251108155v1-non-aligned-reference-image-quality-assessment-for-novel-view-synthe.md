---
layout: default
title: Non-Aligned Reference Image Quality Assessment for Novel View Synthesis
---

# Non-Aligned Reference Image Quality Assessment for Novel View Synthesis

**arXiv**: [2511.08155v1](https://arxiv.org/abs/2511.08155) | [PDF](https://arxiv.org/pdf/2511.08155.pdf)

**ä½œè€…**: Abhijay Ghildyal, Rajesh Sureddi, Nabajeet Barman, Saman Zadtootaghaj, Alan Bovik

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://stootaghaj.github.io/nova-project/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNAR-IQAæ¡†æž¶ï¼Œç”¨äºŽè§£å†³æ–°è§†è§’åˆæˆä¸­éžå¯¹é½å‚è€ƒå›¾åƒçš„è´¨é‡è¯„ä¼°é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ–°è§†è§’åˆæˆ` `å›¾åƒè´¨é‡è¯„ä¼°` `éžå¯¹é½å‚è€ƒ` `å¯¹æ¯”å­¦ä¹ ` `DINOv2` `LoRA` `æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å…¨å‚è€ƒIQAæ–¹æ³•åœ¨å‚è€ƒå›¾åƒæœªå¯¹é½æ—¶å¤±æ•ˆï¼Œæ— å‚è€ƒIQAæ–¹æ³•æ³›åŒ–æ€§ä¸è¶³ï¼Œéš¾ä»¥è¯„ä¼°æ–°è§†è§’åˆæˆå›¾åƒè´¨é‡ã€‚
2. æå‡ºNAR-IQAæ¡†æž¶ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’ŒLoRAå¢žå¼ºçš„DINOv2åµŒå…¥ï¼Œå¹¶ç»“åˆçŽ°æœ‰IQAæ–¹æ³•çš„ç›‘ç£ä¿¡æ¯ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ¨¡åž‹åœ¨å¯¹é½å’Œéžå¯¹é½å‚è€ƒå›¾åƒä¸Šå‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä¸”ä¸Žäººç±»ä¸»è§‚è¯„ä»·é«˜åº¦ç›¸å…³ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§éžå¯¹é½å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(NAR-IQA)æ¡†æž¶ï¼Œä¸“é—¨ç”¨äºŽæ–°è§†è§’åˆæˆ(NVS)å›¾åƒçš„è´¨é‡è¯„ä¼°ã€‚è¯¥æ¡†æž¶æ—¨åœ¨è§£å†³åœ¨ç¼ºä¹åƒç´ çº§å¯¹é½çš„ground truthå‚è€ƒå›¾åƒæ—¶ï¼Œä¼ ç»Ÿå…¨å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(FR-IQA)æ–¹æ³•å¤±æ•ˆä»¥åŠæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(NR-IQA)æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä½œè€…æž„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸ(TROI)çš„åˆæˆå¤±çœŸï¼Œç”¨äºŽè®­ç»ƒNAR-IQAæ¨¡åž‹ã€‚è¯¥æ¨¡åž‹åŸºäºŽå¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œç»“åˆäº†LoRAå¢žå¼ºçš„DINOv2åµŒå…¥ï¼Œå¹¶åˆ©ç”¨çŽ°æœ‰IQAæ–¹æ³•çš„ç›‘ç£ä¿¡æ¯è¿›è¡ŒæŒ‡å¯¼ã€‚æ¨¡åž‹ä»…åœ¨åˆæˆå¤±çœŸæ•°æ®ä¸Šè®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆç‰¹å®šçš„çœŸå®žNVSæ ·æœ¬ï¼Œä»Žè€Œå¢žå¼ºæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¨¡åž‹ä¼˜äºŽå½“å‰æœ€å…ˆè¿›çš„FR-IQAã€NR-IQAå’ŒNAR-IQAæ–¹æ³•ï¼Œåœ¨å¯¹é½å’Œéžå¯¹é½å‚è€ƒå›¾åƒä¸Šå‡è¡¨çŽ°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œæ”¶é›†äº†åœ¨NVSä¸­è§‚å¯Ÿéžå¯¹é½å‚è€ƒå›¾åƒæ—¶çš„äººç±»åå¥½æ•°æ®ï¼Œå‘çŽ°æ‰€æå‡ºçš„è´¨é‡é¢„æµ‹æ¨¡åž‹ä¸Žæ”¶é›†çš„ä¸»è§‚è¯„åˆ†ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–°è§†è§’åˆæˆ(NVS)å›¾åƒè´¨é‡è¯„ä¼°é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹åƒç´ å¯¹é½çš„å‚è€ƒå›¾åƒæ—¶ã€‚ä¼ ç»Ÿå…¨å‚è€ƒIQAæ–¹æ³•ä¾èµ–äºŽåƒç´ çº§åˆ«çš„å¯¹åº”å…³ç³»ï¼Œå½“å‚è€ƒå›¾åƒä¸Žåˆæˆå›¾åƒæœªå¯¹é½æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è€Œæ— å‚è€ƒIQAæ–¹æ³•è™½ç„¶ä¸éœ€è¦å‚è€ƒå›¾åƒï¼Œä½†åœ¨NVSåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥å‡†ç¡®è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éžå¯¹é½çš„å‚è€ƒå›¾åƒï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œå­¦ä¹ å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚æ ¸å¿ƒåœ¨äºŽæå–å‚è€ƒå›¾åƒå’Œåˆæˆå›¾åƒçš„ç‰¹å¾ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå®¹å¿ä¸€å®šç¨‹åº¦ä¸å¯¹é½çš„è´¨é‡è¯„ä¼°æ¨¡åž‹ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¢žå¼ºæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„NVSåœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®é›†æž„å»ºï¼šæž„å»ºåŒ…å«åˆæˆå¤±çœŸçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¨¡æ‹ŸNVSä¸­å¯èƒ½å‡ºçŽ°çš„å„ç§è´¨é‡é—®é¢˜ã€‚2) ç‰¹å¾æå–ï¼šä½¿ç”¨LoRAå¢žå¼ºçš„DINOv2æ¨¡åž‹æå–å‚è€ƒå›¾åƒå’Œåˆæˆå›¾åƒçš„ç‰¹å¾åµŒå…¥ã€‚3) å¯¹æ¯”å­¦ä¹ ï¼šåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œå­¦ä¹ å›¾åƒè´¨é‡çš„è¡¨ç¤ºï¼Œä½¿å¾—é«˜è´¨é‡çš„åˆæˆå›¾åƒä¸Žå‚è€ƒå›¾åƒçš„ç‰¹å¾åµŒå…¥æ›´åŠ æŽ¥è¿‘ã€‚4) è´¨é‡é¢„æµ‹ï¼šåŸºäºŽå­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºï¼Œé¢„æµ‹åˆæˆå›¾åƒçš„è´¨é‡å¾—åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹NVSåœºæ™¯çš„NAR-IQAæ¡†æž¶ï¼Œè¯¥æ¡†æž¶èƒ½å¤Ÿåˆ©ç”¨éžå¯¹é½çš„å‚è€ƒå›¾åƒè¿›è¡Œè´¨é‡è¯„ä¼°ã€‚ä¸Žä¼ ç»Ÿçš„FR-IQAæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æž¶ä¸éœ€è¦åƒç´ çº§åˆ«çš„å¯¹é½ï¼Œå› æ­¤æ›´åŠ é€‚ç”¨äºŽå®žé™…çš„NVSåº”ç”¨åœºæ™¯ã€‚ä¸ŽNR-IQAæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æž¶åˆ©ç”¨äº†å‚è€ƒå›¾åƒçš„ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LoRAå¯¹DINOv2æ¨¡åž‹è¿›è¡Œå¢žå¼ºï¼Œæé«˜ç‰¹å¾æå–çš„æ•ˆçŽ‡å’Œå‡†ç¡®æ€§ã€‚2) æž„å»ºåŒ…å«æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸ(TROI)å¤±çœŸçš„åˆæˆæ•°æ®é›†ï¼Œæ¨¡æ‹ŸNVSä¸­å¯èƒ½å‡ºçŽ°çš„å„ç§è´¨é‡é—®é¢˜ã€‚3) åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œå­¦ä¹ å›¾åƒè´¨é‡çš„è¡¨ç¤ºï¼Œå¹¶ç»“åˆçŽ°æœ‰IQAæ–¹æ³•çš„ç›‘ç£ä¿¡æ¯ï¼Œæé«˜æ¨¡åž‹çš„æ€§èƒ½ã€‚4) ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆç‰¹å®šçš„çœŸå®žNVSæ ·æœ¬ï¼Œä»Žè€Œå¢žå¼ºæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ¨¡åž‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨çœŸå®žNVSæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æžœè¡¨æ˜Žè¯¥æ¨¡åž‹ä¼˜äºŽå½“å‰æœ€å…ˆè¿›çš„FR-IQAã€NR-IQAå’ŒNAR-IQAæ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜Žï¼Œè¯¥æ¨¡åž‹é¢„æµ‹çš„è´¨é‡å¾—åˆ†ä¸Žäººç±»ä¸»è§‚è¯„ä»·å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ŒéªŒè¯äº†è¯¥æ¨¡åž‹çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®ä¸»é¡µæä¾›äº†æ•°æ®é›†å’Œä»£ç ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§æ–°è§†è§’åˆæˆç³»ç»Ÿï¼Œä¾‹å¦‚è™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žã€è‡ªç”±è§†ç‚¹è§†é¢‘ç­‰ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡ï¼Œå¯ä»¥ä¼˜åŒ–NVSç®—æ³•ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽè¯„ä¼°ä¸åŒNVSç®—æ³•çš„æ€§èƒ½ï¼Œä¸ºç®—æ³•é€‰æ‹©æä¾›ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–å›¾åƒç”Ÿæˆä»»åŠ¡çš„è´¨é‡è¯„ä¼°ä¸­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/

