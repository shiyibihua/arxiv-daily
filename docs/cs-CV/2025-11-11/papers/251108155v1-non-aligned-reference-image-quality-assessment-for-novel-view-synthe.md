---
layout: default
title: Non-Aligned Reference Image Quality Assessment for Novel View Synthesis
---

# Non-Aligned Reference Image Quality Assessment for Novel View Synthesis

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08155" target="_blank" class="toolbar-btn">arXiv: 2511.08155v1</a>
    <a href="https://arxiv.org/pdf/2511.08155.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08155v1" 
            onclick="toggleFavorite(this, '2511.08155v1', 'Non-Aligned Reference Image Quality Assessment for Novel View Synthesis')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Abhijay Ghildyal, Rajesh Sureddi, Nabajeet Barman, Saman Zadtootaghaj, Alan Bovik

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://stootaghaj.github.io/nova-project/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫NAR-IQAÊ°ÜÊû∂ÔºåÁî®‰∫éËß£ÂÜ≥Êñ∞ËßÜËßíÂêàÊàê‰∏≠ÈùûÂØπÈΩêÂèÇËÄÉÂõæÂÉèÁöÑË¥®ÈáèËØÑ‰º∞ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Êñ∞ËßÜËßíÂêàÊàê` `ÂõæÂÉèË¥®ÈáèËØÑ‰º∞` `ÈùûÂØπÈΩêÂèÇËÄÉ` `ÂØπÊØîÂ≠¶‰π†` `DINOv2` `LoRA` `Êó∂Èó¥ÊÑüÂÖ¥Ë∂£Âå∫Âüü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÖ®ÂèÇËÄÉIQAÊñπÊ≥ïÂú®ÂèÇËÄÉÂõæÂÉèÊú™ÂØπÈΩêÊó∂Â§±ÊïàÔºåÊó†ÂèÇËÄÉIQAÊñπÊ≥ïÊ≥õÂåñÊÄß‰∏çË∂≥ÔºåÈöæ‰ª•ËØÑ‰º∞Êñ∞ËßÜËßíÂêàÊàêÂõæÂÉèË¥®Èáè„ÄÇ
2. ÊèêÂá∫NAR-IQAÊ°ÜÊû∂ÔºåÂà©Áî®ÂØπÊØîÂ≠¶‰π†ÂíåLoRAÂ¢ûÂº∫ÁöÑDINOv2ÂµåÂÖ•ÔºåÂπ∂ÁªìÂêàÁé∞ÊúâIQAÊñπÊ≥ïÁöÑÁõëÁù£‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®ÂØπÈΩêÂíåÈùûÂØπÈΩêÂèÇËÄÉÂõæÂÉè‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰∏î‰∏é‰∫∫Á±ª‰∏ªËßÇËØÑ‰ª∑È´òÂ∫¶Áõ∏ÂÖ≥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈùûÂØπÈΩêÂèÇËÄÉÂõæÂÉèË¥®ÈáèËØÑ‰º∞(NAR-IQA)Ê°ÜÊû∂Ôºå‰∏ìÈó®Áî®‰∫éÊñ∞ËßÜËßíÂêàÊàê(NVS)ÂõæÂÉèÁöÑË¥®ÈáèËØÑ‰º∞„ÄÇËØ•Ê°ÜÊû∂Êó®Âú®Ëß£ÂÜ≥Âú®Áº∫‰πèÂÉèÁ¥†Á∫ßÂØπÈΩêÁöÑground truthÂèÇËÄÉÂõæÂÉèÊó∂Ôºå‰º†ÁªüÂÖ®ÂèÇËÄÉÂõæÂÉèË¥®ÈáèËØÑ‰º∞(FR-IQA)ÊñπÊ≥ïÂ§±Êïà‰ª•ÂèäÊó†ÂèÇËÄÉÂõæÂÉèË¥®ÈáèËØÑ‰º∞(NR-IQA)ÊñπÊ≥ïÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÂõæÂÉèÊï∞ÊçÆÈõÜÔºåÂåÖÂê´ÈíàÂØπÊó∂Èó¥ÊÑüÂÖ¥Ë∂£Âå∫Âüü(TROI)ÁöÑÂêàÊàêÂ§±ÁúüÔºåÁî®‰∫éËÆ≠ÁªÉNAR-IQAÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÂü∫‰∫éÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜLoRAÂ¢ûÂº∫ÁöÑDINOv2ÂµåÂÖ•ÔºåÂπ∂Âà©Áî®Áé∞ÊúâIQAÊñπÊ≥ïÁöÑÁõëÁù£‰ø°ÊÅØËøõË°åÊåáÂØº„ÄÇÊ®°Âûã‰ªÖÂú®ÂêàÊàêÂ§±ÁúüÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÔºåÈÅøÂÖçËøáÊãüÂêàÁâπÂÆöÁöÑÁúüÂÆûNVSÊ†∑Êú¨Ôºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°Âûã‰ºò‰∫éÂΩìÂâçÊúÄÂÖàËøõÁöÑFR-IQA„ÄÅNR-IQAÂíåNAR-IQAÊñπÊ≥ïÔºåÂú®ÂØπÈΩêÂíåÈùûÂØπÈΩêÂèÇËÄÉÂõæÂÉè‰∏äÂùáË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºå‰ΩúËÄÖËøòËøõË°å‰∫Ü‰∏ÄÈ°πÁî®Êà∑Á†îÁ©∂ÔºåÊî∂ÈõÜ‰∫ÜÂú®NVS‰∏≠ËßÇÂØüÈùûÂØπÈΩêÂèÇËÄÉÂõæÂÉèÊó∂ÁöÑ‰∫∫Á±ªÂÅèÂ•ΩÊï∞ÊçÆÔºåÂèëÁé∞ÊâÄÊèêÂá∫ÁöÑË¥®ÈáèÈ¢ÑÊµãÊ®°Âûã‰∏éÊî∂ÈõÜÁöÑ‰∏ªËßÇËØÑÂàÜ‰πãÈó¥Â≠òÂú®ÂæàÂº∫ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êñ∞ËßÜËßíÂêàÊàê(NVS)ÂõæÂÉèË¥®ÈáèËØÑ‰º∞ÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèÂÉèÁ¥†ÂØπÈΩêÁöÑÂèÇËÄÉÂõæÂÉèÊó∂„ÄÇ‰º†ÁªüÂÖ®ÂèÇËÄÉIQAÊñπÊ≥ï‰æùËµñ‰∫éÂÉèÁ¥†Á∫ßÂà´ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåÂΩìÂèÇËÄÉÂõæÂÉè‰∏éÂêàÊàêÂõæÂÉèÊú™ÂØπÈΩêÊó∂ÔºåÊÄßËÉΩ‰ºöÊòæËëó‰∏ãÈôç„ÄÇËÄåÊó†ÂèÇËÄÉIQAÊñπÊ≥ïËôΩÁÑ∂‰∏çÈúÄË¶ÅÂèÇËÄÉÂõæÂÉèÔºå‰ΩÜÂú®NVSÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÊúâÈôêÔºåÈöæ‰ª•ÂáÜÁ°ÆËØÑ‰º∞ÂêàÊàêÂõæÂÉèÁöÑË¥®Èáè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÈùûÂØπÈΩêÁöÑÂèÇËÄÉÂõæÂÉèÔºåÈÄöËøáÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÂºèÔºåÂ≠¶‰π†ÂõæÂÉèÁöÑÊÑüÁü•Ë¥®Èáè„ÄÇÊ†∏ÂøÉÂú®‰∫éÊèêÂèñÂèÇËÄÉÂõæÂÉèÂíåÂêàÊàêÂõæÂÉèÁöÑÁâπÂæÅÔºåÂπ∂Â≠¶‰π†‰∏Ä‰∏™ËÉΩÂ§üÂÆπÂøç‰∏ÄÂÆöÁ®ãÂ∫¶‰∏çÂØπÈΩêÁöÑË¥®ÈáèËØÑ‰º∞Ê®°Âûã„ÄÇÈÄöËøáÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑNVSÂú∫ÊôØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Êï∞ÊçÆÈõÜÊûÑÂª∫ÔºöÊûÑÂª∫ÂåÖÂê´ÂêàÊàêÂ§±ÁúüÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÊ®°ÊãüNVS‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂêÑÁßçË¥®ÈáèÈóÆÈ¢ò„ÄÇ2) ÁâπÂæÅÊèêÂèñÔºö‰ΩøÁî®LoRAÂ¢ûÂº∫ÁöÑDINOv2Ê®°ÂûãÊèêÂèñÂèÇËÄÉÂõæÂÉèÂíåÂêàÊàêÂõæÂÉèÁöÑÁâπÂæÅÂµåÂÖ•„ÄÇ3) ÂØπÊØîÂ≠¶‰π†ÔºöÂà©Áî®ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂ≠¶‰π†ÂõæÂÉèË¥®ÈáèÁöÑË°®Á§∫Ôºå‰ΩøÂæóÈ´òË¥®ÈáèÁöÑÂêàÊàêÂõæÂÉè‰∏éÂèÇËÄÉÂõæÂÉèÁöÑÁâπÂæÅÂµåÂÖ•Êõ¥Âä†Êé•Ëøë„ÄÇ4) Ë¥®ÈáèÈ¢ÑÊµãÔºöÂü∫‰∫éÂ≠¶‰π†Âà∞ÁöÑÁâπÂæÅË°®Á§∫ÔºåÈ¢ÑÊµãÂêàÊàêÂõæÂÉèÁöÑË¥®ÈáèÂæóÂàÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπNVSÂú∫ÊôØÁöÑNAR-IQAÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÂà©Áî®ÈùûÂØπÈΩêÁöÑÂèÇËÄÉÂõæÂÉèËøõË°åË¥®ÈáèËØÑ‰º∞„ÄÇ‰∏é‰º†ÁªüÁöÑFR-IQAÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•Ê°ÜÊû∂‰∏çÈúÄË¶ÅÂÉèÁ¥†Á∫ßÂà´ÁöÑÂØπÈΩêÔºåÂõ†Ê≠§Êõ¥Âä†ÈÄÇÁî®‰∫éÂÆûÈôÖÁöÑNVSÂ∫îÁî®Âú∫ÊôØ„ÄÇ‰∏éNR-IQAÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•Ê°ÜÊû∂Âà©Áî®‰∫ÜÂèÇËÄÉÂõæÂÉèÁöÑ‰ø°ÊÅØÔºåËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞ÂêàÊàêÂõæÂÉèÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®LoRAÂØπDINOv2Ê®°ÂûãËøõË°åÂ¢ûÂº∫ÔºåÊèêÈ´òÁâπÂæÅÊèêÂèñÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ2) ÊûÑÂª∫ÂåÖÂê´Êó∂Èó¥ÊÑüÂÖ¥Ë∂£Âå∫Âüü(TROI)Â§±ÁúüÁöÑÂêàÊàêÊï∞ÊçÆÈõÜÔºåÊ®°ÊãüNVS‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂêÑÁßçË¥®ÈáèÈóÆÈ¢ò„ÄÇ3) Âà©Áî®ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂ≠¶‰π†ÂõæÂÉèË¥®ÈáèÁöÑË°®Á§∫ÔºåÂπ∂ÁªìÂêàÁé∞ÊúâIQAÊñπÊ≥ïÁöÑÁõëÁù£‰ø°ÊÅØÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ4) ‰ªÖÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÈÅøÂÖçËøáÊãüÂêàÁâπÂÆöÁöÑÁúüÂÆûNVSÊ†∑Êú¨Ôºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Ê®°ÂûãÂú®ÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÂπ∂Âú®ÁúüÂÆûNVSÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÊµãËØïÔºåÁªìÊûúË°®ÊòéËØ•Ê®°Âûã‰ºò‰∫éÂΩìÂâçÊúÄÂÖàËøõÁöÑFR-IQA„ÄÅNR-IQAÂíåNAR-IQAÊñπÊ≥ï„ÄÇÁî®Êà∑Á†îÁ©∂Ë°®ÊòéÔºåËØ•Ê®°ÂûãÈ¢ÑÊµãÁöÑË¥®ÈáèÂæóÂàÜ‰∏é‰∫∫Á±ª‰∏ªËßÇËØÑ‰ª∑ÂÖ∑ÊúâÂæàÂº∫ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÈ™åËØÅ‰∫ÜËØ•Ê®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇÈ°πÁõÆ‰∏ªÈ°µÊèê‰æõ‰∫ÜÊï∞ÊçÆÈõÜÂíå‰ª£Á†Å„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊñ∞ËßÜËßíÂêàÊàêÁ≥ªÁªüÔºå‰æãÂ¶ÇËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆû„ÄÅËá™Áî±ËßÜÁÇπËßÜÈ¢ëÁ≠â„ÄÇÈÄöËøáËá™Âä®ËØÑ‰º∞ÂêàÊàêÂõæÂÉèÁöÑË¥®ÈáèÔºåÂèØ‰ª•‰ºòÂåñNVSÁÆóÊ≥ïÔºåÊèêÈ´òÁî®Êà∑‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éËØÑ‰º∞‰∏çÂêåNVSÁÆóÊ≥ïÁöÑÊÄßËÉΩÔºå‰∏∫ÁÆóÊ≥ïÈÄâÊã©Êèê‰æõ‰æùÊçÆ„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊâ©Â±ïÂà∞ÂÖ∂‰ªñÂõæÂÉèÁîüÊàê‰ªªÂä°ÁöÑË¥®ÈáèËØÑ‰º∞‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/

