---
layout: default
title: Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification
---

# Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification

**arXiv**: [2511.07929v1](https://arxiv.org/abs/2511.07929) | [PDF](https://arxiv.org/pdf/2511.07929.pdf)

**ä½œè€…**: Yihang Wu, Ahmad Chaddad

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedMedCLIPä»¥è§£å†³åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­è”é‚¦å­¦ä¹ çš„å¼‚æž„æ•°æ®ä¸Žèµ„æºæˆæœ¬é—®é¢˜**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `åŒ»ç–—å›¾åƒåˆ†ç±»` `CLIPæ¨¡åž‹` `ç‰¹å¾é€‚åº”` `æ¨¡åž‹åŽ‹ç¼©` `KLè’¸é¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒ»ç–—å›¾åƒåˆ†ç±»ä¸­æ•°æ®éšç§ã€å¼‚æž„æ€§å’Œé«˜èµ„æºæˆæœ¬é™åˆ¶è”é‚¦å­¦ä¹ éƒ¨ç½²
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æŽ©ç ç‰¹å¾é€‚åº”æ¨¡å—å’Œç§æœ‰åˆ†ç±»å™¨ï¼Œç»“åˆKLè’¸é¦æ­£åˆ™åŒ–ä¸Žæ¨¡åž‹åŽ‹ç¼©
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šæ€§èƒ½æå‡8%ï¼Œé€šä¿¡é€Ÿåº¦æå‡120å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).

