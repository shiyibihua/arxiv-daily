---
layout: default
title: VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion
---

# VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08173" target="_blank" class="toolbar-btn">arXiv: 2511.08173v1</a>
    <a href="https://arxiv.org/pdf/2511.08173.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08173v1" 
            onclick="toggleFavorite(this, '2511.08173v1', 'VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11

**Â§áÊ≥®**: WACV 2026

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/giddyyupp/VLMDiff)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VLMDiffÔºöÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíåÊâ©Êï£Ê®°ÂûãËøõË°åÂ§öÁ±ªÂà´ÂºÇÂ∏∏Ê£ÄÊµã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂºÇÂ∏∏Ê£ÄÊµã` `Êâ©Êï£Ê®°Âûã` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Êó†ÁõëÁù£Â≠¶‰π†` `Â§öÁ±ªÂà´ÂàÜÁ±ª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊâ©Êï£ÁöÑÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ï‰æùËµñÂêàÊàêÂô™Â£∞ÔºåÊ≥õÂåñÊÄßÂèóÈôêÔºå‰∏îÈúÄÈíàÂØπÊØè‰∏™Á±ªÂà´ÂçïÁã¨ËÆ≠ÁªÉÊ®°ÂûãÔºåÊâ©Â±ïÊÄßÂ∑Æ„ÄÇ
2. VLMDiffÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊèêÂèñÂõæÂÉèÊèèËø∞Ôºå‰Ωú‰∏∫Êâ©Êï£Ê®°ÂûãÁöÑÊù°‰ª∂ÔºåÂ≠¶‰π†Ê≠£Â∏∏ÂõæÂÉèÁöÑÈ≤ÅÊ£íÁâπÂæÅË°®Á§∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVLMDiffÂú®Real-IADÂíåCOCO-ADÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÂÉèÁ¥†Á∫ßÂºÇÂ∏∏Ê£ÄÊµãÊÄßËÉΩÔºå‰ºò‰∫éÁé∞ÊúâÊâ©Êï£ÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫VLMDiffÁöÑÊñ∞ÂûãÊó†ÁõëÁù£Â§öÁ±ªÂà´ËßÜËßâÂºÇÂ∏∏Ê£ÄÊµãÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ÈõÜÊàê‰∫ÜÊΩúÂú®Êâ©Êï£Ê®°ÂûãÔºàLDMÔºâÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÔºå‰ª•Â¢ûÂº∫ÂºÇÂ∏∏ÂÆö‰ΩçÂíåÊ£ÄÊµãËÉΩÂäõ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑVLMÈÄöËøáÁÆÄÂçïÁöÑÊèêÁ§∫ÊèêÂèñËØ¶ÁªÜÁöÑÂõæÂÉèÊèèËø∞Ôºå‰Ωú‰∏∫LDMËÆ≠ÁªÉÁöÑÈ¢ùÂ§ñÊù°‰ª∂„ÄÇ‰∏éÂΩìÂâç‰æùËµñÂêàÊàêÂô™Â£∞ÁîüÊàêÁöÑÊâ©Êï£ÊñπÊ≥ï‰∏çÂêåÔºåVLMDiffÂà©Áî®VLMËé∑ÂèñÊ≠£Â∏∏ÂõæÂÉèÁöÑÊèèËø∞ÔºåÊó†ÈúÄÊâãÂä®Ê†áÊ≥®ÊàñÈ¢ùÂ§ñËÆ≠ÁªÉÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÊ≥õÂåñÊÄßÈóÆÈ¢òÂíåÊØè‰∏™Á±ªÂà´ÂçïÁã¨ËÆ≠ÁªÉÁöÑÈúÄÊ±Ç„ÄÇËøô‰∫õÊèèËø∞Áî®‰∫éË∞ÉËäÇÊâ©Êï£Ê®°ÂûãÔºåÂ≠¶‰π†È≤ÅÊ£íÁöÑÊ≠£Â∏∏ÂõæÂÉèÁâπÂæÅË°®Á§∫Ôºå‰ª•ËøõË°åÂ§öÁ±ªÂà´ÂºÇÂ∏∏Ê£ÄÊµã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåÂú®Real-IADÊï∞ÊçÆÈõÜ‰∏äÔºåÂÉèÁ¥†Á∫ßÂå∫ÂüüÈáçÂè†Â∫¶ÔºàPROÔºâÊåáÊ†áÊèêÂçáÈ´òËææ25‰∏™ÁÇπÔºåÂú®COCO-ADÊï∞ÊçÆÈõÜ‰∏äÊèêÂçá8‰∏™ÁÇπÔºå‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÁ±ªÂà´ÁúüÂÆû‰∏ñÁïåÂõæÂÉè‰∏≠ÁöÑËßÜËßâÂºÇÂ∏∏Ê£ÄÊµãÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂêàÊàêÂô™Â£∞ÁöÑÁîüÊàêÔºåËøôÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈíàÂØπÊØè‰∏™Á±ªÂà´ÂçïÁã¨ËÆ≠ÁªÉÊ®°ÂûãÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨Âú®Â§ÑÁêÜÂ§öÁ±ªÂà´ÈóÆÈ¢òÊó∂Áº∫‰πèÂèØÊâ©Â±ïÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVLMDiffÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÊù•Ëé∑ÂèñÊ≠£Â∏∏ÂõæÂÉèÁöÑÊñáÊú¨ÊèèËø∞ÔºåÂπ∂Â∞ÜËøô‰∫õÊèèËø∞‰Ωú‰∏∫ÊΩúÂú®Êâ©Êï£Ê®°ÂûãÔºàLDMÔºâÁöÑÈ¢ùÂ§ñÊù°‰ª∂„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞Êõ¥È≤ÅÊ£íÁöÑÊ≠£Â∏∏ÂõæÂÉèÁâπÂæÅË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òÂºÇÂ∏∏Ê£ÄÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLMDiffÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑVLMÔºà‰æãÂ¶ÇCLIPÔºâÊèêÂèñËæìÂÖ•ÂõæÂÉèÁöÑÊñáÊú¨ÊèèËø∞Ôºõ2) Â∞ÜËøô‰∫õÊñáÊú¨ÊèèËø∞‰Ωú‰∏∫Êù°‰ª∂ËæìÂÖ•Âà∞LDM‰∏≠Ôºõ3) LDMÂ≠¶‰π†Ê≠£Â∏∏ÂõæÂÉèÁöÑÂàÜÂ∏ÉÔºõ4) Âú®Êé®ÁêÜÈò∂ÊÆµÔºåÈÄöËøáÊØîËæÉËæìÂÖ•ÂõæÂÉè‰∏éLDMÈáçÂª∫ÂõæÂÉè‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊù•Ê£ÄÊµãÂºÇÂ∏∏„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLMDiffÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®VLMÊù•Ëé∑ÂèñÂõæÂÉèÁöÑÊñáÊú¨ÊèèËø∞Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜÊâãÂä®Ê†áÊ≥®ÊàñÈ¢ùÂ§ñËÆ≠ÁªÉÁöÑÈúÄË¶Å„ÄÇËøô‰ΩøÂæóËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜÂ§öÁ±ªÂà´ÂºÇÂ∏∏Ê£ÄÊµãÈóÆÈ¢òÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåVLMDiff‰∏çÈúÄË¶ÅÁîüÊàêÂêàÊàêÂô™Â£∞ÔºåËÄåÊòØÁõ¥Êé•Âà©Áî®ÁúüÂÆûÂõæÂÉèÁöÑÊñáÊú¨ÊèèËø∞Êù•ÊåáÂØºÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVLMDiffÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑCLIPÊ®°Âûã‰Ωú‰∏∫VLMÔºå‰ª•ÊèêÂèñÈ´òË¥®ÈáèÁöÑÂõæÂÉèÊèèËø∞Ôºõ2) Â∞ÜÊñáÊú¨ÊèèËø∞ÂµåÂÖ•Âà∞LDMÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠Ôºå‰Ωú‰∏∫È¢ùÂ§ñÁöÑÊù°‰ª∂Ôºõ3) ‰ΩøÁî®ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâ‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•Ë°°ÈáèËæìÂÖ•ÂõæÂÉè‰∏éLDMÈáçÂª∫ÂõæÂÉè‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VLMDiffÂú®Real-IADÂíåCOCO-ADÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®Real-IADÊï∞ÊçÆÈõÜ‰∏äÔºåÂÉèÁ¥†Á∫ßÂå∫ÂüüÈáçÂè†Â∫¶ÔºàPROÔºâÊåáÊ†áÊèêÂçáÈ´òËææ25‰∏™ÁÇπÔºåÂú®COCO-ADÊï∞ÊçÆÈõÜ‰∏äÊèêÂçá8‰∏™ÁÇπ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVLMDiff‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÔºåÂÖ∑ÊúâÂæàÂº∫ÁöÑÁ´û‰∫âÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VLMDiffÂèØÂ∫îÁî®‰∫éÂ∑•‰∏öË¥®Ê£Ä„ÄÅÂåªÁñóÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü„ÄÇÂú®Â∑•‰∏öË¥®Ê£Ä‰∏≠ÔºåÂèØÁî®‰∫éÊ£ÄÊµã‰∫ßÂìÅË°®Èù¢ÁöÑÁº∫Èô∑ÔºõÂú®ÂåªÁñóÂΩ±ÂÉèÂàÜÊûê‰∏≠ÔºåÂèØÁî®‰∫éËæÖÂä©ÂåªÁîüËØäÊñ≠ÁñæÁóÖÔºõÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåÂèØÁî®‰∫éÊ£ÄÊµãÈÅìË∑Ø‰∏äÁöÑÂºÇÂ∏∏Áâ©‰Ωì„ÄÇËØ•Á†îÁ©∂ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÔºåÊúâÊúõÊèêÈ´òÁõ∏ÂÖ≥È¢ÜÂüüÁöÑËá™Âä®ÂåñÊ∞¥Âπ≥ÂíåÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.

