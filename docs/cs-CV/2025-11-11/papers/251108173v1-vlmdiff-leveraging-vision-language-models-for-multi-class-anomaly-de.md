---
layout: default
title: VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion
---

# VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion

**arXiv**: [2511.08173v1](https://arxiv.org/abs/2511.08173) | [PDF](https://arxiv.org/pdf/2511.08173.pdf)

**ä½œè€…**: Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: WACV 2026

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/giddyyupp/VLMDiff)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLMDiffï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡åž‹å’Œæ‰©æ•£æ¨¡åž‹è¿›è¡Œå¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰å¼‚å¸¸æ£€æµ‹` `æ‰©æ•£æ¨¡åž‹` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `æ— ç›‘ç£å­¦ä¹ ` `å¤šç±»åˆ«åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽæ‰©æ•£çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–åˆæˆå™ªå£°ï¼Œæ³›åŒ–æ€§å—é™ï¼Œä¸”éœ€é’ˆå¯¹æ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒæ¨¡åž‹ï¼Œæ‰©å±•æ€§å·®ã€‚
2. VLMDiffåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡åž‹æå–å›¾åƒæè¿°ï¼Œä½œä¸ºæ‰©æ•£æ¨¡åž‹çš„æ¡ä»¶ï¼Œå­¦ä¹ æ­£å¸¸å›¾åƒçš„é²æ£’ç‰¹å¾è¡¨ç¤ºã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒVLMDiffåœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åƒç´ çº§å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œä¼˜äºŽçŽ°æœ‰æ‰©æ•£æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVLMDiffçš„æ–°åž‹æ— ç›‘ç£å¤šç±»åˆ«è§†è§‰å¼‚å¸¸æ£€æµ‹æ¡†æž¶ã€‚è¯¥æ¡†æž¶é›†æˆäº†æ½œåœ¨æ‰©æ•£æ¨¡åž‹ï¼ˆLDMï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰ï¼Œä»¥å¢žå¼ºå¼‚å¸¸å®šä½å’Œæ£€æµ‹èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„VLMé€šè¿‡ç®€å•çš„æç¤ºæå–è¯¦ç»†çš„å›¾åƒæè¿°ï¼Œä½œä¸ºLDMè®­ç»ƒçš„é¢å¤–æ¡ä»¶ã€‚ä¸Žå½“å‰ä¾èµ–åˆæˆå™ªå£°ç”Ÿæˆçš„æ‰©æ•£æ–¹æ³•ä¸åŒï¼ŒVLMDiffåˆ©ç”¨VLMèŽ·å–æ­£å¸¸å›¾åƒçš„æè¿°ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨æˆ–é¢å¤–è®­ç»ƒï¼Œä»Žè€Œé¿å…äº†æ³›åŒ–æ€§é—®é¢˜å’Œæ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒçš„éœ€æ±‚ã€‚è¿™äº›æè¿°ç”¨äºŽè°ƒèŠ‚æ‰©æ•£æ¨¡åž‹ï¼Œå­¦ä¹ é²æ£’çš„æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºï¼Œä»¥è¿›è¡Œå¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•å…·æœ‰ç«žäº‰åŠ›çš„æ€§èƒ½ï¼Œåœ¨Real-IADæ•°æ®é›†ä¸Šï¼Œåƒç´ çº§åŒºåŸŸé‡å åº¦ï¼ˆPROï¼‰æŒ‡æ ‡æå‡é«˜è¾¾25ä¸ªç‚¹ï¼Œåœ¨COCO-ADæ•°æ®é›†ä¸Šæå‡8ä¸ªç‚¹ï¼Œä¼˜äºŽæœ€å…ˆè¿›çš„åŸºäºŽæ‰©æ•£çš„æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šç±»åˆ«çœŸå®žä¸–ç•Œå›¾åƒä¸­çš„è§†è§‰å¼‚å¸¸æ£€æµ‹é—®é¢˜ã€‚çŽ°æœ‰åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºŽåˆæˆå™ªå£°çš„ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹æ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒæ¨¡åž‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å¤„ç†å¤šç±»åˆ«é—®é¢˜æ—¶ç¼ºä¹å¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLMDiffçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰æ¥èŽ·å–æ­£å¸¸å›¾åƒçš„æ–‡æœ¬æè¿°ï¼Œå¹¶å°†è¿™äº›æè¿°ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡åž‹ï¼ˆLDMï¼‰çš„é¢å¤–æ¡ä»¶ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡åž‹å¯ä»¥å­¦ä¹ åˆ°æ›´é²æ£’çš„æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºï¼Œä»Žè€Œæé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVLMDiffçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„VLMï¼ˆä¾‹å¦‚CLIPï¼‰æå–è¾“å…¥å›¾åƒçš„æ–‡æœ¬æè¿°ï¼›2) å°†è¿™äº›æ–‡æœ¬æè¿°ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°LDMä¸­ï¼›3) LDMå­¦ä¹ æ­£å¸¸å›¾åƒçš„åˆ†å¸ƒï¼›4) åœ¨æŽ¨ç†é˜¶æ®µï¼Œé€šè¿‡æ¯”è¾ƒè¾“å…¥å›¾åƒä¸ŽLDMé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLMDiffçš„å…³é”®åˆ›æ–°åœ¨äºŽåˆ©ç”¨VLMæ¥èŽ·å–å›¾åƒçš„æ–‡æœ¬æè¿°ï¼Œä»Žè€Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨æˆ–é¢å¤–è®­ç»ƒçš„éœ€è¦ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹é—®é¢˜ï¼Œå¹¶æé«˜äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸Žä¼ ç»Ÿçš„åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒVLMDiffä¸éœ€è¦ç”Ÿæˆåˆæˆå™ªå£°ï¼Œè€Œæ˜¯ç›´æŽ¥åˆ©ç”¨çœŸå®žå›¾åƒçš„æ–‡æœ¬æè¿°æ¥æŒ‡å¯¼æ¨¡åž‹çš„è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šVLMDiffçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡åž‹ä½œä¸ºVLMï¼Œä»¥æå–é«˜è´¨é‡çš„å›¾åƒæè¿°ï¼›2) å°†æ–‡æœ¬æè¿°åµŒå…¥åˆ°LDMçš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½œä¸ºé¢å¤–çš„æ¡ä»¶ï¼›3) ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä»¥è¡¡é‡è¾“å…¥å›¾åƒä¸ŽLDMé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

VLMDiffåœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨Real-IADæ•°æ®é›†ä¸Šï¼Œåƒç´ çº§åŒºåŸŸé‡å åº¦ï¼ˆPROï¼‰æŒ‡æ ‡æå‡é«˜è¾¾25ä¸ªç‚¹ï¼Œåœ¨COCO-ADæ•°æ®é›†ä¸Šæå‡8ä¸ªç‚¹ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒVLMDiffä¼˜äºŽçŽ°æœ‰çš„åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå…·æœ‰å¾ˆå¼ºçš„ç«žäº‰åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

VLMDiffå¯åº”ç”¨äºŽå·¥ä¸šè´¨æ£€ã€åŒ»ç–—å½±åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚åœ¨å·¥ä¸šè´¨æ£€ä¸­ï¼Œå¯ç”¨äºŽæ£€æµ‹äº§å“è¡¨é¢çš„ç¼ºé™·ï¼›åœ¨åŒ»ç–—å½±åƒåˆ†æžä¸­ï¼Œå¯ç”¨äºŽè¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ç–¾ç—…ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ç”¨äºŽæ£€æµ‹é“è·¯ä¸Šçš„å¼‚å¸¸ç‰©ä½“ã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼ï¼Œæœ‰æœ›æé«˜ç›¸å…³é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ°´å¹³å’Œæ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.

