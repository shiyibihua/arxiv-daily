---
layout: default
title: VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion
---

# VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion

**arXiv**: [2511.08173v1](https://arxiv.org/abs/2511.08173) | [PDF](https://arxiv.org/pdf/2511.08173.pdf)

**ä½œè€…**: Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLMDiffæ¡†æž¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹å’Œæ‰©æ•£æ¨¡åž‹è§£å†³å¤šç±»è§†è§‰å¼‚å¸¸æ£€æµ‹é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰å¼‚å¸¸æ£€æµ‹` `æ‰©æ•£æ¨¡åž‹` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šç±»æ£€æµ‹` `æ— ç›‘ç£å­¦ä¹ ` `å¼‚å¸¸å®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šç±»çœŸå®žå›¾åƒä¸­çš„è§†è§‰å¼‚å¸¸æ£€æµ‹æ˜¯é‡å¤§æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•æ³›åŒ–æ€§å·®ä¸”éœ€é€ç±»è®­ç»ƒ
2. é›†æˆé¢„è®­ç»ƒVLMå’ŒLDMï¼Œé€šè¿‡æç¤ºç”Ÿæˆå›¾åƒæè¿°ä½œä¸ºæ‰©æ•£æ¨¡åž‹æ¡ä»¶ï¼Œæ— éœ€äººå·¥æ ‡æ³¨
3. åœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šï¼Œåƒç´ çº§PROæŒ‡æ ‡æå‡é«˜è¾¾25ç‚¹å’Œ8ç‚¹ï¼Œä¼˜äºŽå…ˆè¿›æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.

