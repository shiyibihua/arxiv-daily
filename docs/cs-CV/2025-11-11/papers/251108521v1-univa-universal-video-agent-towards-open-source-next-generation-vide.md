---
layout: default
title: UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist
---

# UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist

**arXiv**: [2511.08521v1](https://arxiv.org/abs/2511.08521) | [PDF](https://arxiv.org/pdf/2511.08521.pdf)

**ä½œè€…**: Zhengyang Liang, Daoan Zhang, Huichi Zhou, Rui Huang, Bobo Li, Yuechen Zhang, Shengqiong Wu, Xiaohan Wang, Jiebo Luo, Lizi Liao, Hao Fei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniVAé€šç”¨è§†é¢‘ä»£ç†æ¡†æž¶ï¼Œä»¥ç»Ÿä¸€è§†é¢‘ä»»åŠ¡è§£å†³å¤æ‚å·¥ä½œæµéœ€æ±‚ã€‚**

**å…³é”®è¯**: `è§†é¢‘é€šç”¨ä»£ç†` `å¤šä»£ç†æ¡†æž¶` `è§†é¢‘å·¥ä½œæµ` `åˆ†å±‚å†…å­˜` `å¼€æºåŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸“ä¸šAIæ¨¡åž‹éš¾ä»¥å¤„ç†è§†é¢‘ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆçš„å¤æ‚è¿­ä»£å·¥ä½œæµã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è®¡åˆ’-æ‰§è¡ŒåŒä»£ç†æž¶æž„ï¼Œç»“åˆæ¨¡å—åŒ–å·¥å…·å’Œåˆ†å±‚å†…å­˜å®žçŽ°è‡ªåŠ¨åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¼•å…¥UniVA-BenchåŸºå‡†è¯„ä¼°ï¼Œå¹¶å¼€æºæ¡†æž¶ä¿ƒè¿›å¤šæ¨¡æ€AIç ”ç©¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\rightarrow$ multi-round editing $\rightarrow$ object segmentation $\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)

