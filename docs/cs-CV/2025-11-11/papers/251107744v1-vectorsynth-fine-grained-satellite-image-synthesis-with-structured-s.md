---
layout: default
title: VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics
---

# VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics

**arXiv**: [2511.07744v1](https://arxiv.org/abs/2511.07744) | [PDF](https://arxiv.org/pdf/2511.07744.pdf)

**ä½œè€…**: Daniel Cher, Brian Wei, Srikumar Sastry, Nathan Jacobs

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVectorSynthæ¡†æž¶ï¼ŒåŸºäºŽæ‰©æ•£æ¨¡åž‹å®žçŽ°å¤šè¾¹å½¢æ ‡æ³¨ä¸‹çš„ç²¾ç»†å«æ˜Ÿå›¾åƒåˆæˆã€‚**

**å…³é”®è¯**: `å«æ˜Ÿå›¾åƒåˆæˆ` `æ‰©æ•£æ¨¡åž‹` `è¯­ä¹‰å‘é‡å‡ ä½•` `è§†è§‰è¯­è¨€å¯¹é½` `ç©ºé—´ç¼–è¾‘` `åƒç´ çº§åµŒå…¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–‡æœ¬æˆ–å¸ƒå±€æ¡ä»¶æ¨¡åž‹éš¾ä»¥å®žçŽ°åƒç´ çº§ç²¾ç¡®çš„å«æ˜Ÿå›¾åƒåˆæˆä¸Žç¼–è¾‘ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§†è§‰è¯­è¨€å¯¹é½æ¨¡å—ç”Ÿæˆåƒç´ çº§åµŒå…¥ï¼Œç»“åˆè¯­ä¹‰å‘é‡å‡ ä½•æŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è¯­ä¹‰ä¿çœŸåº¦å’Œç»“æž„çœŸå®žæ„Ÿä¸Šä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼Œæ”¯æŒäº¤äº’å¼ç©ºé—´ç¼–è¾‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at https://github.com/mvrl/VectorSynth.

