---
layout: default
title: LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models
---

# LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models

**arXiv**: [2511.07727v1](https://arxiv.org/abs/2511.07727) | [PDF](https://arxiv.org/pdf/2511.07727.pdf)

**ä½œè€…**: Xiaohan Zhang, Yan Ding, Yohei Hayamizu, Zainab Altaweel, Yifeng Zhu, Yuke Zhu, Peter Stone, Chris Paxton, Shiqi Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLM-GROPæ¡†æž¶ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡åž‹ä¸Žè§†è§‰æ–¹æ³•è§£å†³ç§»åŠ¨æ“ä½œä»»åŠ¡è§„åˆ’é—®é¢˜**

**å…³é”®è¯**: `ä»»åŠ¡ä¸Žè¿åŠ¨è§„åˆ’` `å¤§è¯­è¨€æ¨¡åž‹` `ç§»åŠ¨æ“ä½œ` `ç‰©ä½“é‡æŽ’` `è§†è§‰åŸºç¡€` `æœºå™¨äººå¯¼èˆª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç§»åŠ¨æ“ä½œä¸­ä»»åŠ¡ä¸Žè¿åŠ¨è§„åˆ’çš„ç»“åˆï¼Œå¤„ç†å¤šç‰©ä½“æ”¾ç½®çš„æ¨¡ç³Šç›®æ ‡
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹å¸¸è¯†çŸ¥è¯†æŒ‡å¯¼ä»»åŠ¡è§„åˆ’ï¼Œè§†è§‰æ–¹æ³•é€‰æ‹©æœºå™¨äººåŸºä½ç½®
3. å®žéªŒæˆ–æ•ˆæžœï¼šçœŸå®žä¸–ç•Œå®žéªŒæˆåŠŸçŽ‡84.4%ï¼Œä½†æ€§èƒ½ä½ŽäºŽäººç±»æœåŠ¡å‘˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.
>   In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.

