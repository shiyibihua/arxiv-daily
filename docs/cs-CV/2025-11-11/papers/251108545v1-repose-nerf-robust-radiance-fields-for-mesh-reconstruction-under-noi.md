---
layout: default
title: RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses
---

# RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses

**arXiv**: [2511.08545v1](https://arxiv.org/abs/2511.08545) | [PDF](https://arxiv.org/pdf/2511.08545.pdf)

**ä½œè€…**: Sriram Srinivasan, Gautam Ramachandra

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRePose-NeRFä»¥ä»Žå™ªå£°ç›¸æœºä½å§¿çš„å¤šè§†è§’å›¾åƒä¸­é‡å»ºå¯ç¼–è¾‘3Dç½‘æ ¼**

**å…³é”®è¯**: `ç¥žç»è¾å°„åœº` `3Dç½‘æ ¼é‡å»º` `ç›¸æœºä½å§¿ä¼˜åŒ–` `å¤šè§†è§’å›¾åƒ` `æœºå™¨äººè§†è§‰` `éšå¼è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçœŸå®žåœºæ™¯ä¸­ç›¸æœºä½å§¿ä¸ç²¾ç¡®ï¼Œé™åˆ¶NeRFæ–¹æ³•å®žç”¨æ€§ï¼Œä¸”éšå¼ä½“ç§¯è¡¨ç¤ºä¸Žå¤šè¾¹å½¢ç½‘æ ¼ä¸å…¼å®¹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè”åˆä¼˜åŒ–ç›¸æœºä½å§¿å¹¶å­¦ä¹ éšå¼åœºæ™¯è¡¨ç¤ºï¼Œæ•èŽ·å‡ ä½•ç»†èŠ‚å’ŒçœŸå®žæ„Ÿå¤–è§‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®žçŽ°ä½å§¿ä¸ç¡®å®šæ€§ä¸‹çš„å‡†ç¡®é²æ£’3Dé‡å»ºï¼Œæå‡æœºå™¨äººåº”ç”¨å…¼å®¹æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.

