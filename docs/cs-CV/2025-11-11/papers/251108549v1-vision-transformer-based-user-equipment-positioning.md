---
layout: default
title: Vision Transformer Based User Equipment Positioning
---

# Vision Transformer Based User Equipment Positioning

**arXiv**: [2511.08549v1](https://arxiv.org/abs/2511.08549) | [PDF](https://arxiv.org/pdf/2511.08549.pdf)

**ä½œè€…**: Parshwa Shah, Dhaval K. Patel, Brijesh Soni, Miguel LÃ³pez-BenÃ­tez, Siddhartan Govindasamy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè§†è§‰å˜æ¢å™¨çš„ç”¨æˆ·è®¾å¤‡å®šä½æ–¹æ³•ï¼Œåˆ©ç”¨è§’åº¦å»¶è¿Ÿé…ç½®æ–‡ä»¶æå‡ç²¾åº¦ã€‚**

**å…³é”®è¯**: `ç”¨æˆ·è®¾å¤‡å®šä½` `è§†è§‰å˜æ¢å™¨` `ä¿¡é“çŠ¶æ€ä¿¡æ¯` `è§’åº¦å»¶è¿Ÿé…ç½®æ–‡ä»¶` `æ·±åº¦å­¦ä¹ ` `å®šä½è¯¯å·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ·±åº¦å­¦ä¹ æ¨¡åž‹å¯¹è¾“å…¥æ•°æ®åˆ†é…ç›¸åŒæ³¨æ„åŠ›ï¼Œä¸”ä¸é€‚ç”¨äºŽéžåºåˆ—æ•°æ®å¦‚çž¬æ—¶ä¿¡é“çŠ¶æ€ä¿¡æ¯ã€‚
2. é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰å˜æ¢å™¨æž¶æž„ï¼Œèšç„¦ä¿¡é“çŠ¶æ€ä¿¡æ¯çŸ©é˜µä¸­çš„è§’åº¦å»¶è¿Ÿé…ç½®æ–‡ä»¶ã€‚
3. åœ¨DeepMIMOå’ŒViWiæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå®¤å†…å¤–å®šä½è¯¯å·®æ˜¾è‘—é™ä½Žï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•çº¦38%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Deep Learning (DL) techniques have been used for User Equipment (UE) positioning. However, the key shortcomings of such models is that: i) they weigh the same attention to the entire input; ii) they are not well suited for the non-sequential data e.g., when only instantaneous Channel State Information (CSI) is available. In this context, we propose an attention-based Vision Transformer (ViT) architecture that focuses on the Angle Delay Profile (ADP) from CSI matrix. Our approach, validated on the `DeepMIMO' and `ViWi' ray-tracing datasets, achieves an Root Mean Squared Error (RMSE) of 0.55m indoors, 13.59m outdoors in DeepMIMO, and 3.45m in ViWi's outdoor blockage scenario. The proposed scheme outperforms state-of-the-art schemes by $\sim$ 38\%. It also performs substantially better than other approaches that we have considered in terms of the distribution of error distance.

