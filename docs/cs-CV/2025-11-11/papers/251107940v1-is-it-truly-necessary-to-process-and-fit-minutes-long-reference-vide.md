---
layout: default
title: Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?
---

# Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?

**arXiv**: [2511.07940v1](https://arxiv.org/abs/2511.07940) | [PDF](https://arxiv.org/pdf/2511.07940.pdf)

**ä½œè€…**: Rui-Qing Sun, Ang Li, Zhijing Wu, Tian Lan, Qianyu Lu, Xingshan Yao, Chen Xu, Xian-Ling Mao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºISExploreç­–ç•¥ï¼Œé€šè¿‡é€‰æ‹©ä¿¡æ¯ä¸°å¯ŒçŸ­è§†é¢‘æ®µï¼Œæå‡ä¸ªæ€§åŒ–è¯´è¯äººè„¸ç”Ÿæˆæ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `è¯´è¯äººè„¸ç”Ÿæˆ` `å‚è€ƒè§†é¢‘é€‰æ‹©` `ä¿¡æ¯ä¸°å¯Œæ®µ` `NeRFæ–¹æ³•` `3Dé«˜æ–¯æº…å°„` `æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•éœ€å¤„ç†æ•°åˆ†é’Ÿå‚è€ƒè§†é¢‘ï¼Œè®¡ç®—è´Ÿæ‹…å¤§ï¼Œé™åˆ¶å®žé™…åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽéŸ³é¢‘å¤šæ ·æ€§ã€å”‡åŠ¨å¹…åº¦å’Œç›¸æœºè§†è§’ï¼Œè‡ªåŠ¨é€‰æ‹©5ç§’ä¿¡æ¯ä¸°å¯Œè§†é¢‘æ®µã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨NeRFå’Œ3DGSæ–¹æ³•ä¸­ï¼Œæ•°æ®å¤„ç†å’Œè®­ç»ƒé€Ÿåº¦æå‡5å€ä»¥ä¸Šï¼Œä¿æŒé«˜ä¿çœŸè¾“å‡ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.

