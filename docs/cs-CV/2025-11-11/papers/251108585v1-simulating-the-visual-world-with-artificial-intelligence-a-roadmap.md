---
layout: default
title: Simulating the Visual World with Artificial Intelligence: A Roadmap
---

# Simulating the Visual World with Artificial Intelligence: A Roadmap

**arXiv**: [2511.08585v1](https://arxiv.org/abs/2511.08585) | [PDF](https://arxiv.org/pdf/2511.08585.pdf)

**ä½œè€…**: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†é¢‘åŸºç¡€æ¨¡åž‹ä½œä¸ºéšå¼ä¸–ç•Œæ¨¡åž‹ï¼Œä»¥æž„å»ºäº¤äº’å¼è™šæ‹ŸçŽ¯å¢ƒã€‚**

**å…³é”®è¯**: `è§†é¢‘åŸºç¡€æ¨¡åž‹` `éšå¼ä¸–ç•Œæ¨¡åž‹` `è§†é¢‘æ¸²æŸ“å™¨` `ç‰©ç†åˆç†æ€§` `äº¤äº’æ¨¡æ‹Ÿ` `å¤šå°ºåº¦è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘ç”Ÿæˆä»Žè§†è§‰å¸å¼•åŠ›è½¬å‘ç‰©ç†åˆç†æ€§å’Œäº¤äº’æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆéšå¼ä¸–ç•Œæ¨¡åž‹å’Œè§†é¢‘æ¸²æŸ“å™¨ï¼Œç¼–ç ç‰©ç†çŸ¥è¯†ä¸ŽåŠ¨æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåº”ç”¨äºŽæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œæ¸¸æˆç­‰é¢†åŸŸã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.

