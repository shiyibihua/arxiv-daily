---
layout: default
title: The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression
---

# The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression

**arXiv**: [2511.08226v1](https://arxiv.org/abs/2511.08226) | [PDF](https://arxiv.org/pdf/2511.08226.pdf)

**ä½œè€…**: RaphaÃ«l Bayle, Martial Mermillod, Robert M. French

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOPREåœ¨çº¿æ•°æ®é›†åŽ‹ç¼©æ–¹æ³•ï¼Œå®žçŽ°æ— å…ˆéªŒä¿¡æ¯çš„åœ¨çº¿æŒç»­å­¦ä¹ ã€‚**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `æ•°æ®é›†åŽ‹ç¼©` `åœ¨çº¿å­¦ä¹ ` `æ— å…ˆéªŒå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŒç»­å­¦ä¹ ä¸­ç¾éš¾æ€§é—å¿˜ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å…ˆéªŒä¿¡æ¯ï¼Œç¼ºä¹æ— å…ˆéªŒé€šç”¨æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šOPREåœ¨çº¿åŽ‹ç¼©æ•°æ®é›†ï¼Œæµ‹è¯•æ—¶è®­ç»ƒåˆ†ç±»å™¨ï¼Œå‡å°‘æ•°æ®å†—ä½™ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-10å’ŒCIFAR-100ä¸Šæ€§èƒ½ä¼˜äºŽå…¶ä»–åœ¨çº¿æŒç»­å­¦ä¹ æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In order to achieve Continual Learning (CL), the problem of catastrophic forgetting, one that has plagued neural networks since their inception, must be overcome. The evaluation of continual learning methods relies on splitting a known homogeneous dataset and learning the associated tasks one after the other. We argue that most CL methods introduce a priori information about the data to come and cannot be considered agnostic. We exemplify this point with the case of methods relying on pretrained feature extractors, which are still used in CL. After showing that pretrained feature extractors imply a loss of generality with respect to the data that can be learned by the model, we then discuss other kinds of a priori information introduced in other CL methods. We then present the Online Patch Redundancy Eliminator (OPRE), an online dataset compression algorithm, which, along with the training of a classifier at test time, yields performance on CIFAR-10 and CIFAR-100 superior to a number of other state-of-the-art online continual learning methods. Additionally, OPRE requires only minimal and interpretable hypothesis on the data to come. We suggest that online dataset compression could well be necessary to achieve fully agnostic CL.

