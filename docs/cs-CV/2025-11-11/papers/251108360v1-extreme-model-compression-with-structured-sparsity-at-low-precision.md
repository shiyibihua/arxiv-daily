---
layout: default
title: Extreme Model Compression with Structured Sparsity at Low Precision
---

# Extreme Model Compression with Structured Sparsity at Low Precision

**arXiv**: [2511.08360v1](https://arxiv.org/abs/2511.08360) | [PDF](https://arxiv.org/pdf/2511.08360.pdf)

**ä½œè€…**: Dan Liu, Nikita Dvornik, Xue Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSLOPEæ¡†æž¶ï¼Œç»“åˆç»“æž„åŒ–ç¨€ç–ä¸Žä½Žç²¾åº¦é‡åŒ–ä»¥åŽ‹ç¼©æ¨¡åž‹**

**å…³é”®è¯**: `æ¨¡åž‹åŽ‹ç¼©` `ç»“æž„åŒ–ç¨€ç–` `ä½Žç²¾åº¦é‡åŒ–` `è®­ç»ƒæ­£åˆ™åŒ–` `è§’åº¦å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦ç¥žç»ç½‘ç»œåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ï¼Œç¨€ç–ä¸Žé‡åŒ–ç»“åˆä¼šä¸¥é‡æŸå®³ç²¾åº¦
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è®­ç»ƒæ—¶æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä¿ƒè¿›æƒé‡è§’åº¦å¯¹é½è€Œéžç›´æŽ¥åŒ¹é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ResNet-18ä¸Šå®žçŽ°çº¦20å€æ¨¡åž‹åŽ‹ç¼©ï¼Œä¿æŒçº¦99%åŽŸå§‹ç²¾åº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.

