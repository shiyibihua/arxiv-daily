---
layout: default
title: OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition
---

# OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition

**arXiv**: [2511.08133v1](https://arxiv.org/abs/2511.08133) | [PDF](https://arxiv.org/pdf/2511.08133.pdf)

**ä½œè€…**: Lixu Sun, Nurmemet Yolwas, Wushour Silamu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOTSNetä»¥è§£å†³åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­è§†è§‰-è¯­è¨€è·¨æ¨¡æ€å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `åœºæ™¯æ–‡æœ¬è¯†åˆ«` `è·¨æ¨¡æ€å¯¹é½` `ç¥žç»è®¤çŸ¥å¯å‘` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šæ¨¡æ€èžåˆ` `ä¸è§„åˆ™æ–‡æœ¬å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•è§†è§‰-è¯­è¨€è§£è€¦ä¼˜åŒ–å¯¼è‡´é”™è¯¯ä¼ æ’­ï¼Œè§†è§‰ç¼–ç å™¨æ³¨æ„åŠ›åå‘èƒŒæ™¯å¹²æ‰°ï¼Œè§£ç å™¨ç©ºé—´å¯¹é½å·®
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è§‚å¯Ÿ-æ€è€ƒ-æ‹¼å†™ä¸‰é˜¶æ®µç®¡é“ï¼Œé›†æˆåŒæ³¨æ„åŠ›ç¼–ç å™¨ã€ä½ç½®æ„ŸçŸ¥æ¨¡å—å’Œå¤šæ¨¡æ€éªŒè¯å™¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Union14M-Lå’ŒOSTæ•°æ®é›†ä¸Šè¾¾åˆ°83.5%å’Œ79.1%å‡†ç¡®çŽ‡ï¼Œ14ä¸ªåœºæ™¯ä¸­9ä¸ªåˆ›çºªå½•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.

