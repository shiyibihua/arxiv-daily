---
layout: default
title: CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion
---

# CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion

**arXiv**: [2511.08075v1](https://arxiv.org/abs/2511.08075) | [PDF](https://arxiv.org/pdf/2511.08075.pdf)

**ä½œè€…**: Cameron Braunstein, Mariya Toneva, Eddy Ilg

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºCLIPåœ¨Stable Diffusionä¸­ä¸»å¯¼äººç±»è¯­ä¹‰è¡¨ç¤ºï¼Œè€Œéžæ‰©æ•£è¿‡ç¨‹**

**å…³é”®è¯**: `è¯­ä¹‰è¡¨ç¤º` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `CLIPæ¨¡åž‹` `æ‰©æ•£æ¨¡åž‹` `æ¨¡åž‹æŽ¢æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶Stable Diffusionåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­æ˜¯å¦å…·æœ‰äººç±»å¯ç†è§£çš„è¯­ä¹‰è¡¨ç¤º
2. ä½¿ç”¨å›žå½’å±‚æŽ¢æµ‹æ¨¡åž‹å†…éƒ¨è¡¨ç¤ºï¼Œé¢„æµ‹è¯­ä¹‰å±žæ€§å¹¶ä¸Žäººç±»æ ‡æ³¨æ¯”è¾ƒ
3. å‘çŽ°CLIPæ–‡æœ¬ç¼–ç å†³å®šè¯­ä¹‰è¡¨ç¤ºï¼Œæ‰©æ•£è¿‡ç¨‹ä»…ä½œä¸ºè§†è§‰è§£ç å™¨

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.

