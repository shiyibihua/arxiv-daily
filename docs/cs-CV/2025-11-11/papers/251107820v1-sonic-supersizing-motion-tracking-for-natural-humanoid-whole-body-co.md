---
layout: default
title: SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control
---

# SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control

**arXiv**: [2511.07820v1](https://arxiv.org/abs/2511.07820) | [PDF](https://arxiv.org/pdf/2511.07820.pdf)

**ä½œè€…**: Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando CastaÃ±eda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, Umar Iqbal, Linxi "Jim" Fan, Yuke Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤§è§„æ¨¡è¿åŠ¨è·Ÿè¸ªæ¨¡åž‹ä»¥è§£å†³äººå½¢æœºå™¨äººå…¨èº«æŽ§åˆ¶é—®é¢˜**

**å…³é”®è¯**: `äººå½¢æœºå™¨äººæŽ§åˆ¶` `è¿åŠ¨è·Ÿè¸ª` `åŸºç¡€æ¨¡åž‹` `å¤§è§„æ¨¡è®­ç»ƒ` `å…¨èº«è¿åŠ¨` `å¤šæ¨¡æ€è¾“å…¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰äººå½¢æœºå™¨äººæŽ§åˆ¶å™¨è§„æ¨¡å°ã€è¡Œä¸ºæœ‰é™ï¼Œè®­ç»ƒèµ„æºä¸è¶³
2. é€šè¿‡æ‰©å±•æ¨¡åž‹å‚æ•°ã€æ•°æ®é›†å’Œè®¡ç®—é‡ï¼Œæž„å»ºé€šç”¨è¿åŠ¨è·Ÿè¸ªåŸºç¡€æ¨¡åž‹
3. æ¨¡åž‹æ”¯æŒå®žæ—¶è¿åŠ¨è§„åˆ’å’Œå¤šè¾“å…¥æŽ¥å£ï¼Œæå‡æŽ§åˆ¶è‡ªç„¶æ€§å’Œé²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.

