---
layout: default
title: Visual Bridge: Universal Visual Perception Representations Generating
---

# Visual Bridge: Universal Visual Perception Representations Generating

**arXiv**: [2511.07877v1](https://arxiv.org/abs/2511.07877) | [PDF](https://arxiv.org/pdf/2511.07877.pdf)

**ä½œè€…**: Yilin Gao, Shuguang Dou, Junzhou Li, Zhiheng Yu, Yin Li, Dongsheng Jiang, Shugong Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæµåŒ¹é…çš„é€šç”¨è§†è§‰æ„ŸçŸ¥æ¡†æž¶ï¼Œä»¥è§£å†³å¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æ³›åŒ–ä¸Žæ‰©å±•æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `é€šç”¨è§†è§‰æ„ŸçŸ¥` `æµåŒ¹é…` `å¤šä»»åŠ¡å­¦ä¹ ` `è§†è§‰è¡¨ç¤ºç”Ÿæˆ` `é›¶æ ·æœ¬å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ‰©æ•£æ¨¡åž‹å±€é™äºŽå•ä»»åŠ¡å•æ¨¡åž‹èŒƒå¼ï¼Œæ³›åŒ–ä¸Žæ‰©å±•æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æµåŒ¹é…ä»Žå›¾åƒå—ä»¤ç‰Œç”Ÿæˆä»»åŠ¡ç‰¹å®šè¡¨ç¤ºï¼Œå¼•å…¥å¤šå°ºåº¦å¾ªçŽ¯ä»»åŠ¡åµŒå…¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆ†ç±»ã€æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ï¼Œé›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹æ€§èƒ½ä¼˜äºŽå…ˆå‰æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.

