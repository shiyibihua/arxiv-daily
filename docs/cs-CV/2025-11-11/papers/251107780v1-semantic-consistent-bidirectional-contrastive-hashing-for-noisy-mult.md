---
layout: default
title: Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval
---

# Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval

**arXiv**: [2511.07780v1](https://arxiv.org/abs/2511.07780) | [PDF](https://arxiv.org/pdf/2511.07780.pdf)

**ä½œè€…**: Likang Peng, Chao Su, Wenyuan Wu, Yuan Sun, Dezhong Peng, Xi Peng, Xu Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰ä¸€è‡´åŒå‘å¯¹æ¯”å“ˆå¸Œä»¥è§£å†³å™ªå£°å¤šæ ‡ç­¾è·¨æ¨¡æ€æ£€ç´¢é—®é¢˜**

**å…³é”®è¯**: `è·¨æ¨¡æ€å“ˆå¸Œ` `å¤šæ ‡ç­¾æ£€ç´¢` `å™ªå£°æ ‡ç­¾å¤„ç†` `å¯¹æ¯”å­¦ä¹ ` `è¯­ä¹‰ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ ‡ç­¾æ•°æ®ä¸­æ ‡ç­¾å™ªå£°å’Œè¯­ä¹‰é‡å æœªè¢«å……åˆ†å¤„ç†ï¼Œå½±å“æ£€ç´¢æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´åˆ†ç±»å’ŒåŒå‘è½¯å¯¹æ¯”å“ˆå¸Œï¼ŒåŠ¨æ€ç”Ÿæˆæ ·æœ¬å¯¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå™ªå£°æ¡ä»¶ä¸‹ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.

