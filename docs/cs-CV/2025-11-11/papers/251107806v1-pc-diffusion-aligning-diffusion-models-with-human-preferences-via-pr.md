---
layout: default
title: PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier
---

# PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier

**arXiv**: [2511.07806v1](https://arxiv.org/abs/2511.07806) | [PDF](https://arxiv.org/pdf/2511.07806.pdf)

**ä½œè€…**: Shaomeng Wang, He Wang, Xiaolu Wei, Longquan Dai, Jinhui Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPC-Diffusionæ¡†æž¶ï¼Œé€šè¿‡åå¥½åˆ†ç±»å™¨å¯¹é½æ‰©æ•£æ¨¡åž‹ä¸Žäººç±»åå¥½**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `åå¥½å¯¹é½` `è½»é‡åˆ†ç±»å™¨` `ç›´æŽ¥åå¥½ä¼˜åŒ–` `å›¾åƒç”Ÿæˆ` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡åž‹è¾“å‡ºå¸¸ä¸Žäººç±»åå¥½ä¸ä¸€è‡´ï¼ŒçŽ°æœ‰DPOæ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”ä¾èµ–å‚è€ƒæ¨¡åž‹
2. PC-Diffusionä½¿ç”¨è½»é‡åå¥½åˆ†ç±»å™¨ç›´æŽ¥å»ºæ¨¡æ ·æœ¬åå¥½ï¼Œæ— éœ€å…¨æ¨¡åž‹å¾®è°ƒæˆ–å‚è€ƒæ¨¡åž‹
3. å®žéªŒæ˜¾ç¤ºPC-Diffusionåœ¨ä¿æŒåå¥½ä¸€è‡´æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½Žè®­ç»ƒæˆæœ¬ï¼Œå®žçŽ°é«˜æ•ˆç¨³å®šç”Ÿæˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.

