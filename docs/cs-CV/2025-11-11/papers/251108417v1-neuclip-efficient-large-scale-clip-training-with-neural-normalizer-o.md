---
layout: default
title: NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization
---

# NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization

**arXiv**: [2511.08417v1](https://arxiv.org/abs/2511.08417) | [PDF](https://arxiv.org/pdf/2511.08417.pdf)

**ä½œè€…**: Xiyuan Wei, Chih-Jen Lin, Tianbao Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNeuCLIPä»¥ä¼˜åŒ–CLIPè®­ç»ƒä¸­çš„å½’ä¸€åŒ–é¡¹ä¼°è®¡ï¼Œæå‡å¤§è§„æ¨¡æ•°æ®é›†æ•ˆçŽ‡**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `å½’ä¸€åŒ–ä¼˜åŒ–` `CLIPè®­ç»ƒ` `å¤§è§„æ¨¡æ•°æ®é›†` `ç¥žç»ç½‘ç»œä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPå¯¹æ¯”æŸå¤±ä¸­å½’ä¸€åŒ–é¡¹ä¼°è®¡ä¾èµ–å¤§æ‰¹æ¬¡ï¼Œè®¡ç®—èµ„æºéœ€æ±‚é«˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å‡¸åˆ†æžå’Œå˜åˆ†åˆ†æžï¼Œå°†å½’ä¸€åŒ–é¡¹ä¼°è®¡è½¬åŒ–ä¸ºç¥žç»ç½‘ç»œä¼˜åŒ–é—®é¢˜
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼Œæå‡è®­ç»ƒæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, which are updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches. To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) $\textbf{reformulating}$ the contrastive loss for each sample $\textbf{via convex analysis}$ into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) $\textbf{transforming}$ the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via $\textbf{variational analysis}$ into the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.

