---
layout: default
title: RAPTR: Radar-based 3D Pose Estimation using Transformer
---

# RAPTR: Radar-based 3D Pose Estimation using Transformer

**arXiv**: [2511.08387v1](https://arxiv.org/abs/2511.08387) | [PDF](https://arxiv.org/pdf/2511.08387.pdf)

**ä½œè€…**: Sorachi Kato, Ryoma Yataka, Pu Perry Wang, Pedro Miraldo, Takuya Fujihashi, Petros Boufounos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAPTRä»¥åœ¨å¼±ç›‘ç£ä¸‹ä½¿ç”¨é›·è¾¾è¿›è¡Œå®¤å†…3Däººä½“å§¿æ€ä¼°è®¡**

**å…³é”®è¯**: `é›·è¾¾å§¿æ€ä¼°è®¡` `å¼±ç›‘ç£å­¦ä¹ ` `3Däººä½“å§¿æ€` `Transformeræž¶æž„` `å®¤å†…åœºæ™¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›·è¾¾å®¤å†…3Då§¿æ€ä¼°è®¡ä¾èµ–æ˜‚è´µ3Då…³é”®ç‚¹æ ‡æ³¨ï¼Œéš¾ä»¥åœ¨å¤æ‚åœºæ™¯ä¸­æ‰©å±•
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè§£ç å™¨ï¼Œç»“åˆä¼ª3Då¯å˜å½¢æ³¨æ„åŠ›ï¼Œåˆ©ç”¨3Dè¾¹ç•Œæ¡†å’Œ2Då…³é”®ç‚¹æ ‡ç­¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå…³èŠ‚ä½ç½®è¯¯å·®æ˜¾è‘—é™ä½Ž

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

