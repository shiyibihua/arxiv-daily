---
layout: default
title: RAPTR: Radar-based 3D Pose Estimation using Transformer
---

# RAPTR: Radar-based 3D Pose Estimation using Transformer

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08387" target="_blank" class="toolbar-btn">arXiv: 2511.08387v1</a>
    <a href="https://arxiv.org/pdf/2511.08387.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08387v1" 
            onclick="toggleFavorite(this, '2511.08387v1', 'RAPTR: Radar-based 3D Pose Estimation using Transformer')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sorachi Kato, Ryoma Yataka, Pu Perry Wang, Pedro Miraldo, Takuya Fujihashi, Petros Boufounos

**ÂàÜÁ±ª**: cs.CV, cs.AI, eess.SP

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11

**Â§áÊ≥®**: 26 pages, Accepted to NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/merlresearch/radar-pose-transformer)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RAPTRÔºöÂà©Áî®TransformerÁöÑÈõ∑Ëææ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Ôºå‰ΩøÁî®Âº±ÁõëÁù£Â≠¶‰π†„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction & Matching)**

**ÂÖ≥ÈîÆËØç**: `Èõ∑Ëææ` `3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°` `Transformer` `Âº±ÁõëÁù£Â≠¶‰π†` `ÂÆ§ÂÜÖÂú∫ÊôØ` `ÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÈõ∑ËææÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°‰æùËµñ‰∫éÊòÇË¥µÁöÑ3DÂÖ≥ÈîÆÁÇπÊ†áÊ≥®ÔºåÈöæ‰ª•Âú®Â§çÊùÇÂÆ§ÂÜÖÁéØÂ¢É‰∏≠Ëé∑Âèñ„ÄÇ
2. RAPTRÈÄöËøá‰∏§Èò∂ÊÆµTransformerËß£Á†ÅÂô®ÔºåÂà©Áî®Êòì‰∫éËé∑ÂèñÁöÑ3D BBoxÂíå2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æËøõË°åÂº±ÁõëÁù£Â≠¶‰π†„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRAPTRÂú®HIBERÂíåMMVRÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂÖ≥ËäÇ‰ΩçÁΩÆËØØÂ∑ÆÂàÜÂà´Èôç‰Ωé34.3%Âíå76.9%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈõ∑ËææÁöÑÂÆ§ÂÜÖ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïRAPTRÔºåËØ•ÊñπÊ≥ï‰ΩøÁî®TransformerÔºåÂπ∂Âú®Âº±ÁõëÁù£‰∏ãËøõË°åËÆ≠ÁªÉ„ÄÇ‰∏éÈúÄË¶ÅÁ≤æÁªÜ3DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æÁöÑ‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåRAPTR‰ªÖ‰ΩøÁî®3D BBoxÂíå2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æÔºåËøô‰∫õÊ†áÁ≠æÊõ¥ÂÆπÊòì‰∏îÊõ¥ÂÖ∑ÂèØÊâ©Â±ïÊÄß„ÄÇRAPTRÁöÑÁâπÁÇπÊòØÈááÁî®‰∏§Èò∂ÊÆµÂßøÊÄÅËß£Á†ÅÂô®Êû∂ÊûÑÔºåÂπ∂‰ΩøÁî®‰º™3DÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÊù•Â¢ûÂº∫Â§öËßÜËßíÈõ∑ËææÁâπÂæÅÁöÑÂßøÊÄÅ/ÂÖ≥ËäÇÊü•ËØ¢„ÄÇÂßøÊÄÅËß£Á†ÅÂô®‰ΩøÁî®3DÊ®°ÊùøÊçüÂ§±‰º∞ËÆ°ÂàùÂßã3DÂßøÊÄÅÔºå‰ª•Âà©Áî®3D BBoxÊ†áÁ≠æÂπ∂ÂáèËΩªÊ∑±Â∫¶Ê®°Á≥äÔºõÂÖ≥ËäÇËß£Á†ÅÂô®‰ΩøÁî®2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æÂíå3DÈáçÂäõÊçüÂ§±Êù•ÁªÜÂåñÂàùÂßãÂßøÊÄÅ„ÄÇÂú®‰∏§‰∏™ÂÆ§ÂÜÖÈõ∑ËææÊï∞ÊçÆÈõÜ‰∏äÁöÑËØÑ‰º∞Ë°®ÊòéÔºåRAPTR‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂú®HIBER‰∏äÂ∞ÜÂÖ≥ËäÇ‰ΩçÁΩÆËØØÂ∑ÆÈôç‰Ωé‰∫Ü34.3ÔºÖÔºåÂú®MMVR‰∏äÈôç‰Ωé‰∫Ü76.9ÔºÖ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÈõ∑ËææÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÁöÑÁ≤æÁªÜ3DÂÖ≥ÈîÆÁÇπÊ†áÊ≥®ÔºåËøôÂú®Â§çÊùÇÁöÑÂÆ§ÂÜÖÁéØÂ¢É‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â≠òÂú®ÈÅÆÊå°ÊàñÂ§ö‰∫∫Âú∫ÊôØ‰∏ãÔºåÊ†áÊ≥®ÊàêÊú¨ÈùûÂ∏∏È´òÊòÇ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂà©Áî®Êõ¥ÂÆπÊòìËé∑ÂèñÁöÑÂº±ÁõëÁù£‰ø°ÊÅØÔºàÂ¶Ç3D BBoxÂíå2DÂÖ≥ÈîÆÁÇπÔºâÊù•ÂÆûÁé∞Á≤æÁ°ÆÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRAPTRÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®TransformerÊû∂ÊûÑÔºåÈÄöËøá‰∏§Èò∂ÊÆµËß£Á†ÅÂô®ÔºåÁªìÂêà3D BBoxÂíå2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æËøõË°åÂº±ÁõëÁù£Â≠¶‰π†„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÂà©Áî®3D BBox‰ø°ÊÅØ‰º∞ËÆ°ÂàùÂßãÂßøÊÄÅÔºåÁ¨¨‰∫åÈò∂ÊÆµÂà©Áî®2DÂÖ≥ÈîÆÁÇπ‰ø°ÊÅØÁªÜÂåñÂßøÊÄÅ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®Âà©Áî®‰∏çÂêåÁ±ªÂûãÊ†áÁ≠æÁöÑ‰ºòÂäøÔºåÂêåÊó∂ÂáèËΩªÊ∑±Â∫¶Ê®°Á≥äÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRAPTRÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) Èõ∑ËææÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºàÂÖ∑‰ΩìÂÆûÁé∞Êú™Áü•ÔºâÔºõ2) ÂßøÊÄÅËß£Á†ÅÂô®ÔºöÂà©Áî®3D BBoxÊ†áÁ≠æÂíå3DÊ®°ÊùøÊçüÂ§±‰º∞ËÆ°ÂàùÂßã3DÂßøÊÄÅÔºõ3) ÂÖ≥ËäÇËß£Á†ÅÂô®ÔºöÂà©Áî®2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æÂíå3DÈáçÂäõÊçüÂ§±ÁªÜÂåñÂàùÂßãÂßøÊÄÅ„ÄÇÂßøÊÄÅËß£Á†ÅÂô®ÂíåÂÖ≥ËäÇËß£Á†ÅÂô®ÈÉΩÂü∫‰∫éTransformerÊû∂ÊûÑÔºåÂπ∂‰ΩøÁî®‰º™3DÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â¢ûÂº∫ÁâπÂæÅË°®Á§∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRAPTRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âº±ÁõëÁù£Â≠¶‰π†Ê°ÜÊû∂Âíå‰∏§Èò∂ÊÆµËß£Á†ÅÂô®Êû∂ÊûÑ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåRAPTR‰∏çÈúÄË¶ÅÊòÇË¥µÁöÑ3DÂÖ≥ÈîÆÁÇπÊ†áÊ≥®ÔºåËÄåÊòØÂà©Áî®Êõ¥ÂÆπÊòìËé∑ÂèñÁöÑ3D BBoxÂíå2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æËøõË°åËÆ≠ÁªÉ„ÄÇÊ≠§Â§ñÔºå‰∏§Èò∂ÊÆµËß£Á†ÅÂô®ËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®‰∏çÂêåÁ±ªÂûãÊ†áÁ≠æÁöÑ‰ø°ÊÅØÔºåÂπ∂ÂáèËΩªÊ∑±Â∫¶Ê®°Á≥äÈóÆÈ¢ò„ÄÇ‰º™3DÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÊú∫Âà∂‰πüÊòØ‰∏Ä‰∏™ÂàõÊñ∞ÁÇπÔºåÂÆÉËÉΩÂ§üÊúâÊïàÂú∞ËûçÂêàÂ§öËßÜËßíÈõ∑ËææÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRAPTRÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) 3DÊ®°ÊùøÊçüÂ§±ÔºöÁî®‰∫éÂà©Áî®3D BBoxÊ†áÁ≠æÔºåÈºìÂä±È¢ÑÊµãÁöÑ3DÂßøÊÄÅ‰∏éBBoxÂØπÈΩêÔºõ2) 3DÈáçÂäõÊçüÂ§±ÔºöÁî®‰∫éÂà©Áî®2DÂÖ≥ÈîÆÁÇπÊ†áÁ≠æÔºåÈºìÂä±È¢ÑÊµãÁöÑ3DÂßøÊÄÅÁ¨¶ÂêàÈáçÂäõÊñπÂêëÔºõ3) ‰º™3DÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÔºöÁî®‰∫éËûçÂêàÂ§öËßÜËßíÈõ∑ËææÁâπÂæÅÔºåÂ¢ûÂº∫ÁâπÂæÅË°®Á§∫„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠Â∫îËØ•ÊúâËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

RAPTRÂú®HIBERÂíåMMVR‰∏§‰∏™ÂÆ§ÂÜÖÈõ∑ËææÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRAPTRÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂú®HIBERÊï∞ÊçÆÈõÜ‰∏äÔºåRAPTRÂ∞ÜÂÖ≥ËäÇ‰ΩçÁΩÆËØØÂ∑ÆÈôç‰Ωé‰∫Ü34.3ÔºÖÔºåÂú®MMVRÊï∞ÊçÆÈõÜ‰∏äÔºåÂÖ≥ËäÇ‰ΩçÁΩÆËØØÂ∑ÆÈôç‰Ωé‰∫Ü76.9ÔºÖ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåRAPTRÂú®Âü∫‰∫éÈõ∑ËææÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RAPTRÂú®ÂÆ§ÂÜÖÁéØÂ¢É‰∏≠ÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅËÄÅ‰∫∫ÁúãÊä§„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂÆâÈò≤ÁõëÊéßÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÂà©Áî®‰ΩéÊàêÊú¨ÁöÑÈõ∑Ëææ‰º†ÊÑüÂô®ÔºåÂú®ÂÖâÁ∫ø‰∏çË∂≥ÊàñÂ≠òÂú®ÈÅÆÊå°ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂáÜÁ°ÆÂú∞‰º∞ËÆ°‰∫∫‰ΩìÂßøÊÄÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂÆâÂÖ®ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

