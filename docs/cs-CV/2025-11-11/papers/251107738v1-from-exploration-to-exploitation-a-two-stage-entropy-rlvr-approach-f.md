---
layout: default
title: From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training
---

# From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training

**arXiv**: [2511.07738v1](https://arxiv.org/abs/2511.07738) | [PDF](https://arxiv.org/pdf/2511.07738.pdf)

**ä½œè€…**: Donglai Xu, Hongzheng Yang, Yuzhi Zhao, Pingping Zhang, Jinpeng Chen, Wenao Ma, Zhijian Hou, Mengyang Wu, Xiaolei Li, Senkang Hu, Ziyi Guan, Jason Chun Lok Li, Lai Man Po

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤é˜¶æ®µç†µä¼˜åŒ–æ–¹æ³•ä»¥å¢žå¼ºå™ªå£°å®¹å¿çš„MLLMå¼ºåŒ–å­¦ä¹ è®­ç»ƒ**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å™ªå£°å®¹å¿è®­ç»ƒ` `ç†µä¼˜åŒ–` `ä¸¤é˜¶æ®µç­–ç•¥` `GRPO`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMå¼ºåŒ–å­¦ä¹ ä¾èµ–é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œä½†çŽ°å®žæ•°æ®å™ªå£°å¤§æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆ
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†æŽ¢ç´¢ä¸Žåˆ©ç”¨é˜¶æ®µï¼ŒåŠ¨æ€è°ƒæ•´è¯çº§ç†µä»¥å¹³è¡¡å¤šæ ·æ€§ä¸Žç¡®å®šæ€§è¾“å‡º
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªMLLMéª¨å¹²å’Œå™ªå£°è®¾ç½®ä¸‹ï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡é²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.

