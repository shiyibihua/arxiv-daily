---
layout: default
title: Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis
---

# Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis

**arXiv**: [2511.08087v1](https://arxiv.org/abs/2511.08087) | [PDF](https://arxiv.org/pdf/2511.08087.pdf)

**ä½œè€…**: Aditi Singhania, Krutik Malani, Riddhi Dhawan, Arushi Jain, Garv Tandon, Nippun Sharma, Souymodip Chakraborty, Vineet Batra, Ankit Phogat

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽVLMçš„åˆ†å±‚è¯„ä¼°æ¡†æž¶ä»¥è§£å†³ç”Ÿæˆæ¨¡åž‹ä¸­èº«ä»½ä¿æŒçš„ç»†ç²’åº¦è¯„ä¼°é—®é¢˜**

**å…³é”®è¯**: `èº«ä»½ä¿æŒè¯„ä¼°` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åˆ†å±‚è¯„ä¼°æ¡†æž¶` `ç”Ÿæˆæ¨¡åž‹åŸºå‡†` `ç»†ç²’åº¦åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æŒ‡æ ‡ä¾èµ–å…¨å±€åµŒå…¥æˆ–ç²—ç•¥æç¤ºï¼Œæ— æ³•æ•æ‰ç»†ç²’åº¦èº«ä»½å˜åŒ–ä¸”è¯Šæ–­èƒ½åŠ›æœ‰é™
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†å±‚åˆ†è§£ä¸»ä½“ä¸ºå†³ç­–æ ‘ï¼Œå¼•å¯¼VLMè¿›è¡Œç»“æž„åŒ–æŽ¨ç†ä»¥è¯„ä¼°ç‰¹å¾çº§å˜æ¢
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªå…ˆè¿›ç”Ÿæˆæ¨¡åž‹ä¸ŠéªŒè¯ï¼Œä¸Žäººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œå¹¶å¼•å…¥æ–°åŸºå‡†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.

