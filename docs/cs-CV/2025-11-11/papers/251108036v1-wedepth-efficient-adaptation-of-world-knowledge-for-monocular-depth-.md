---
layout: default
title: WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation
---

# WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08036" target="_blank" class="toolbar-btn">arXiv: 2511.08036v1</a>
    <a href="https://arxiv.org/pdf/2511.08036.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08036v1" 
            onclick="toggleFavorite(this, '2511.08036v1', 'WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Gongshu Wang, Zhirui Wang, Kan Yang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**WEDepthÔºöÈ´òÊïàÂà©Áî®‰∏ñÁïåÁü•ËØÜËá™ÈÄÇÂ∫îÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°` `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Áü•ËØÜËøÅÁßª` `Â§öÂ±ÇÊ¨°ÁâπÂæÅ` `Èõ∂Ê†∑Êú¨Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Èù¢‰∏¥‰ªé2DÂõæÂÉèÊé®Êñ≠3D‰ø°ÊÅØÁöÑÂõ∫ÊúâÈöæÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂÖÖÂàÜÂà©Áî®ÂõæÂÉè‰∏≠ÁöÑ‰∏ñÁïåÁü•ËØÜ„ÄÇ
2. WEDepthÈÄöËøáÂ∞ÜËßÜËßâÂü∫Á°ÄÊ®°Âûã(VFM)‰Ωú‰∏∫Â§öÂ±ÇÊ¨°ÁâπÂæÅÂ¢ûÂº∫Âô®ÔºåÂú®‰∏çÂêåË°®Á§∫Â±ÇÁ∫ßÊ≥®ÂÖ•ÂÖàÈ™åÁü•ËØÜÔºåÂÆûÁé∞È´òÊïàÁöÑÁü•ËØÜËøÅÁßª„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåWEDepthÂú®NYU-Depth v2ÂíåKITTIÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°(MDE)Â∫îÁî®ÂπøÊ≥õÔºå‰ΩÜÁî±‰∫é‰ªé2DÂõæÂÉèÈáçÂª∫3DÂú∫ÊôØÁöÑÂõ∫Êúâ‰∏çÈÄÇÂÆöÊÄßËÄåÊûÅÂÖ∑ÊåëÊàò„ÄÇÁé∞‰ª£ËßÜËßâÂü∫Á°ÄÊ®°Âûã(VFMs)Âú®Â§ßÂûãÂ§öÊ†∑ÂåñÊï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÔºåË°®Áé∞Âá∫ÂçìË∂äÁöÑ‰∏ñÁïåÁêÜËß£ËÉΩÂäõÔºåËøôÊúâÂà©‰∫éÂêÑÁßçËßÜËßâ‰ªªÂä°„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøáÂæÆË∞ÉËøô‰∫õVFMsÔºåMDEÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπËøõ„ÄÇÂèóËøô‰∫õËøõÂ±ïÁöÑÂêØÂèëÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜWEDepthÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÁî®‰∫éË∞ÉÊï¥VFMs‰ª•ËøõË°åMDEÔºåËÄåÊó†ÈúÄ‰øÆÊîπÂÖ∂ÁªìÊûÑÂíåÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåÂêåÊó∂ÊúâÊïàÂú∞ÂºïÂá∫ÂíåÂà©Áî®ÂÖ∂Âõ∫ÊúâÁöÑÂÖàÈ™åÁü•ËØÜ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈááÁî®VFM‰Ωú‰∏∫Â§öÂ±ÇÊ¨°ÁâπÂæÅÂ¢ûÂº∫Âô®ÔºåÁ≥ªÁªüÂú∞Âú®‰∏çÂêåÁöÑË°®Á§∫Â±ÇÊ¨°‰∏äÊ≥®ÂÖ•ÂÖàÈ™åÁü•ËØÜ„ÄÇÂú®NYU-Depth v2ÂíåKITTIÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåWEDepthÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõ(SOTA)ÊÄßËÉΩÔºå‰∏éÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ï(ÈúÄË¶ÅÂ§öÊ¨°ÂâçÂêë‰º†ÈÄí)ÂíåÂú®Áõ∏ÂØπÊ∑±Â∫¶‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÂèñÂæó‰∫ÜÊúâÁ´û‰∫âÂäõÁöÑÁªìÊûú„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂêÑÁßçÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Êó®Âú®‰ªéÂçïÂº†2DÂõæÂÉè‰∏≠È¢ÑÊµãÂú∫ÊôØÁöÑÊ∑±Â∫¶‰ø°ÊÅØ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•ÊúâÊïàÂà©Áî®ÂõæÂÉè‰∏≠Ëï¥Âê´ÁöÑ‰∏ñÁïåÁü•ËØÜÔºåÂØºËá¥Ê∑±Â∫¶‰º∞ËÆ°Á≤æÂ∫¶ÂèóÈôê„ÄÇÊ≠§Â§ñÔºåÁõ¥Êé•ÂæÆË∞ÉÂ§ßÂûãËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºàVFMsÔºâËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÂèØËÉΩÁ†¥ÂùèÂÖ∂È¢ÑËÆ≠ÁªÉÁöÑÈÄöÁî®Áü•ËØÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöWEDepthÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºàVFMsÔºâ‰Ωú‰∏∫Áü•ËØÜÊù•Ê∫êÔºåÈÄöËøá‰∏ÄÁßçÈ´òÊïàÁöÑËá™ÈÄÇÂ∫îÊñπÂºèÔºåÂ∞ÜVFMs‰∏≠Ëï¥Âê´ÁöÑ‰∏ñÁïåÁü•ËØÜËøÅÁßªÂà∞ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰ªªÂä°‰∏≠„ÄÇËØ•ÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπVFMÁªìÊûÑÁöÑ‰øÆÊîπÂíåÊùÉÈáçÁöÑÂæÆË∞ÉÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂‰øùÁïô‰∫ÜVFMÁöÑÈÄöÁî®ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöWEDepthÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºàVFMÔºâÔºö‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô®ÔºåÊèêÂèñËæìÂÖ•ÂõæÂÉèÁöÑÂ§öÂ±ÇÊ¨°ÁâπÂæÅË°®Á§∫„ÄÇ2) Â§öÂ±ÇÊ¨°ÁâπÂæÅÊ≥®ÂÖ•ÔºöÂ∞ÜVFMÊèêÂèñÁöÑÁâπÂæÅÊ≥®ÂÖ•Âà∞Ê∑±Â∫¶‰º∞ËÆ°ÁΩëÁªúÁöÑÂêÑ‰∏™Â±ÇÁ∫ßÔºå‰ªéËÄåÂ∞ÜVFMÁöÑÂÖàÈ™åÁü•ËØÜ‰º†ÈÄíÁªôÊ∑±Â∫¶‰º∞ËÆ°ÁΩëÁªú„ÄÇ3) Ê∑±Â∫¶‰º∞ËÆ°ÁΩëÁªúÔºöË¥üË¥£‰ªéËûçÂêà‰∫ÜVFMÁâπÂæÅÁöÑÂõæÂÉèË°®Á§∫‰∏≠È¢ÑÊµãÊ∑±Â∫¶Âõæ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöWEDepthÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂È´òÊïàÁöÑÁü•ËØÜËøÅÁßªÊú∫Âà∂„ÄÇÂÆÉÈÄöËøáÂ§öÂ±ÇÊ¨°ÁâπÂæÅÊ≥®ÂÖ•ÁöÑÊñπÂºèÔºåÂ∞ÜVFMÁöÑÁü•ËØÜËûçÂÖ•Âà∞Ê∑±Â∫¶‰º∞ËÆ°ÁΩëÁªú‰∏≠ÔºåËÄåÊó†ÈúÄÂØπVFMËøõË°åÂæÆË∞É„ÄÇËøôÁßçÊñπÊ≥ïÊó¢Èôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂèà‰øùÁïô‰∫ÜVFMÁöÑÈÄöÁî®ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂú®‰∏çÂêåÂ±ÇÁ∫ßÊ≥®ÂÖ•Áü•ËØÜÔºå‰ΩøÂæóÊ∑±Â∫¶‰º∞ËÆ°ÁΩëÁªúËÉΩÂ§üÂ≠¶‰π†Âà∞‰∏çÂêåÁ≤íÂ∫¶ÁöÑÂÖàÈ™å‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöWEDepthÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) VFMÁöÑÈÄâÊã©ÔºöËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâTransformerÊ®°Âûã‰Ωú‰∏∫VFM„ÄÇ2) ÁâπÂæÅÊ≥®ÂÖ•ÊñπÂºèÔºöÈááÁî®‰∫ÜÊÆãÂ∑ÆËøûÊé•ÁöÑÊñπÂºèÂ∞ÜVFMÁöÑÁâπÂæÅÊ≥®ÂÖ•Âà∞Ê∑±Â∫¶‰º∞ËÆ°ÁΩëÁªú‰∏≠ÔºåÈÅøÂÖç‰∫ÜÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò„ÄÇ3) ÊçüÂ§±ÂáΩÊï∞Ôºö‰ΩøÁî®‰∫ÜÊ∑±Â∫¶ÂõûÂΩíÂ∏∏Áî®ÁöÑL1ÊçüÂ§±ÂáΩÊï∞ÂíåÂ∞∫Â∫¶‰∏çÂèòÊ¢ØÂ∫¶ÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•ÊèêÈ´òÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

WEDepthÂú®NYU-Depth v2ÂíåKITTIÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇÂú®NYU-Depth v2Êï∞ÊçÆÈõÜ‰∏äÔºåWEDepthÁöÑÁªùÂØπÁõ∏ÂØπËØØÂ∑Æ(Abs Rel)‰∏∫0.068ÔºåÂùáÊñπÊ†πËØØÂ∑Æ(RMSE)‰∏∫0.265Ôºå‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂíåÂú®Áõ∏ÂØπÊ∑±Â∫¶‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåWEDepthËøòÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõÔºåÂú®Êú™ËßÅËøáÁöÑÂú∫ÊôØ‰∏≠‰πüËÉΩÂèñÂæóËæÉÂ•ΩÁöÑÊ∑±Â∫¶‰º∞ËÆ°ÁªìÊûú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

WEDepthÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÁ≤æÁ°ÆÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÂèØ‰ª•Â∏ÆÂä©Ëá™Âä®È©æÈ©∂Á≥ªÁªüÊõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÊèêÈ´òÂØºËà™ÁöÑÂÆâÂÖ®ÊÄß„ÄÇÂú®Êú∫Âô®‰∫∫È¢ÜÂüüÔºåÊ∑±Â∫¶‰ø°ÊÅØÂèØ‰ª•Áî®‰∫éÁâ©‰ΩìËØÜÂà´„ÄÅÊäìÂèñÂíåÂú∫ÊôØÈáçÂª∫„ÄÇÂ¢ûÂº∫Áé∞ÂÆûÂ∫îÁî®ÂàôÂèØ‰ª•Âà©Áî®Ê∑±Â∫¶‰ø°ÊÅØÂÆûÁé∞Êõ¥ÈÄºÁúüÁöÑËôöÊãüÁâ©‰ΩìÂè†Âä†ÊïàÊûú„ÄÇËØ•Á†îÁ©∂Èôç‰Ωé‰∫ÜÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÂØπËÆ°ÁÆóËµÑÊ∫êÁöÑÈúÄÊ±ÇÔºå‰ΩøÂÖ∂Êõ¥ÂÆπÊòìÈÉ®ÁΩ≤Âú®ÁßªÂä®ËÆæÂ§áÂíåÂµåÂÖ•ÂºèÁ≥ªÁªü‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.

