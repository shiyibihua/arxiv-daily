---
layout: default
title: WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation
---

# WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation

**arXiv**: [2511.08036v1](https://arxiv.org/abs/2511.08036) | [PDF](https://arxiv.org/pdf/2511.08036.pdf)

**ä½œè€…**: Gongshu Wang, Zhirui Wang, Kan Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWEDepthä»¥é«˜æ•ˆé€‚é…ä¸–ç•ŒçŸ¥è¯†ç”¨äºŽå•ç›®æ·±åº¦ä¼°è®¡**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `è§†è§‰åŸºç¡€æ¨¡åž‹` `ç‰¹å¾å¢žå¼º` `é›¶æ ·æœ¬è¿ç§»` `ä¸–ç•ŒçŸ¥è¯†é€‚é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å•ç›®æ·±åº¦ä¼°è®¡å› ä»Žå•å¼ 2Då›¾åƒé‡å»º3Dåœºæ™¯è€Œå…·æœ‰æŒ‘æˆ˜æ€§
2. æ–¹æ³•åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡åž‹ä½œä¸ºå¤šçº§ç‰¹å¾å¢žå¼ºå™¨ï¼Œä¸ä¿®æ”¹å…¶ç»“æž„æˆ–æƒé‡
3. åœ¨NYU-Depth v2å’ŒKITTIæ•°æ®é›†ä¸Šå®žçŽ°SOTAï¼Œå¹¶å±•ç¤ºå¼ºé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.

