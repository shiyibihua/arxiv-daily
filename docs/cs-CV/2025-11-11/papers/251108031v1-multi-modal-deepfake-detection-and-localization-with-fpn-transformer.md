---
layout: default
title: Multi-modal Deepfake Detection and Localization with FPN-Transformer
---

# Multi-modal Deepfake Detection and Localization with FPN-Transformer

**arXiv**: [2511.08031v1](https://arxiv.org/abs/2511.08031) | [PDF](https://arxiv.org/pdf/2511.08031.pdf)

**ä½œè€…**: Chende Zheng, Ruiqi Suo, Zhoulin Ji, Jingyi Deng, Fangbin Yi, Chenhao Lin, Chao Shen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Zig-HS/MM-DDL)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽFPN-Transformerçš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸Žå®šä½æ¡†æž¶ï¼Œæå‡è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›å’Œæ—¶åºè¾¹ç•Œå›žå½’ç²¾åº¦ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ·±åº¦ä¼ªé€ æ£€æµ‹` `å¤šæ¨¡æ€èžåˆ` `ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ` `Transformer` `æ—¶åºå®šä½` `è‡ªç›‘ç£å­¦ä¹ ` `è·¨æ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å•æ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯ï¼Œä¸”å®šä½ç²¾åº¦ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ä¼ªé€ åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºåŸºäºŽFPN-Transformerçš„å¤šæ¨¡æ€æ£€æµ‹æ¡†æž¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹æå–ç‰¹å¾ï¼Œæž„å»ºå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œå®žçŽ°è·¨æ¨¡æ€ä¿¡æ¯èžåˆã€‚
3. åœ¨IJCAI'25 DDL-AVåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨è·¨æ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œå®šä½ä»»åŠ¡ä¸Šå–å¾—äº†0.7535çš„è‰¯å¥½æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡åž‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—æ·±åº¦ä¼ªé€ å†…å®¹é«˜åº¦é€¼çœŸï¼Œå¯¹è§†å¬é¢†åŸŸçš„æ•°å­—ä¿¡ä»»æž„æˆé‡å¤§å¨èƒã€‚å•æ¨¡æ€æ£€æµ‹æ–¹æ³•åœ¨è¯†åˆ«åˆæˆåª’ä½“æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†å…¶æ— æ³•åˆ©ç”¨è·¨æ¨¡æ€ç›¸å…³æ€§ä»¥åŠç²¾ç¡®å®šä½ä¼ªé€ ç‰‡æ®µçš„èƒ½åŠ›é™åˆ¶äº†å…¶åœ¨å¤æ‚ã€ç»†ç²’åº¦æ“ä½œä¸­çš„å®žç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºŽç‰¹å¾é‡‘å­—å¡”-Transformer (FPN-Transformer)çš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸Žå®šä½æ¡†æž¶ï¼Œè§£å†³äº†è·¨æ¨¡æ€æ³›åŒ–å’Œæ—¶é—´è¾¹ç•Œå›žå½’çš„å…³é”®é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è‡ªç›‘ç£æ¨¡åž‹ï¼ˆéŸ³é¢‘ä½¿ç”¨WavLMï¼Œè§†é¢‘ä½¿ç”¨CLIPï¼‰æå–åˆ†å±‚æ—¶é—´ç‰¹å¾ã€‚é€šè¿‡å…·æœ‰å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„R-TLMå—æž„å»ºå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œå®žçŽ°è·¨ä¸Šä¸‹æ–‡æ—¶é—´ä¾èµ–æ€§çš„è”åˆåˆ†æžã€‚åŒåˆ†æ”¯é¢„æµ‹å¤´åŒæ—¶é¢„æµ‹ä¼ªé€ æ¦‚çŽ‡å¹¶ç»†åŒ–è¢«æ“çºµç‰‡æ®µçš„æ—¶é—´åç§»ï¼Œå®žçŽ°å¸§çº§å®šä½ç²¾åº¦ã€‚æˆ‘ä»¬åœ¨IJCAI'25 DDL-AVåŸºå‡†æµ‹è¯•é›†çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çŽ¯å¢ƒä¸­ï¼Œè·¨æ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œå®šä½çš„æœ€ç»ˆå¾—åˆ†ä¸º0.7535ï¼Œè¡¨çŽ°è‰¯å¥½ã€‚å®žéªŒç»“æžœè¯å®žäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå¹¿ä¹‰æ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ã€‚ä»£ç å¯åœ¨https://github.com/Zig-HS/MM-DDLèŽ·å–ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çŽ°æœ‰æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•åœ¨è·¨æ¨¡æ€ä¿¡æ¯åˆ©ç”¨ä¸è¶³å’Œæ—¶åºå®šä½ç²¾åº¦ä¸é«˜çš„é—®é¢˜ã€‚çŽ°æœ‰çš„å•æ¨¡æ€æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨éŸ³é¢‘å’Œè§†é¢‘ä¹‹é—´çš„å…³è”æ€§ï¼Œå¯¼è‡´åœ¨å¤æ‚ä¼ªé€ åœºæ™¯ä¸‹çš„æ£€æµ‹æ•ˆæžœä¸ä½³ã€‚åŒæ—¶ï¼ŒçŽ°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®å®šä½ä¼ªé€ ç‰‡æ®µçš„æ—¶é—´è¾¹ç•Œæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯èžåˆï¼Œé€šè¿‡æž„å»ºç‰¹å¾é‡‘å­—å¡”å’ŒTransformerç»“æž„ï¼Œå®žçŽ°å¯¹éŸ³é¢‘å’Œè§†é¢‘æ—¶åºç‰¹å¾çš„è”åˆåˆ†æžã€‚é€šè¿‡é¢„è®­ç»ƒçš„è‡ªç›‘ç£æ¨¡åž‹æå–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨å…³é”®çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä»Žè€Œæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„WavLMï¼ˆéŸ³é¢‘ï¼‰å’ŒCLIPï¼ˆè§†é¢‘ï¼‰æ¨¡åž‹æå–åˆ†å±‚æ—¶é—´ç‰¹å¾ã€‚2) ç‰¹å¾é‡‘å­—å¡”æž„å»ºæ¨¡å—ï¼šé€šè¿‡R-TLMå—æž„å»ºå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œå®žçŽ°è·¨ä¸Šä¸‹æ–‡æ—¶é—´ä¾èµ–æ€§çš„è”åˆåˆ†æžã€‚3) é¢„æµ‹æ¨¡å—ï¼šé‡‡ç”¨åŒåˆ†æ”¯é¢„æµ‹å¤´ï¼ŒåŒæ—¶é¢„æµ‹ä¼ªé€ æ¦‚çŽ‡å’Œç»†åŒ–è¢«æ“çºµç‰‡æ®µçš„æ—¶é—´åç§»ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆæå–å¤šæ¨¡æ€ç‰¹å¾ï¼Œç„¶åŽè¿›è¡Œç‰¹å¾èžåˆå’Œåˆ†æžï¼Œæœ€åŽè¿›è¡Œä¼ªé€ æ£€æµ‹å’Œæ—¶åºå®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†åŸºäºŽFPN-Transformerçš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æž¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèžåˆéŸ³é¢‘å’Œè§†é¢‘ä¿¡æ¯ã€‚2) åˆ©ç”¨R-TLMå—æž„å»ºå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œå¢žå¼ºäº†æ¨¡åž‹å¯¹ä¸åŒå°ºåº¦æ—¶é—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚3) é‡‡ç”¨åŒåˆ†æ”¯é¢„æµ‹å¤´ï¼ŒåŒæ—¶è¿›è¡Œä¼ªé€ æ¦‚çŽ‡é¢„æµ‹å’Œæ—¶åºåç§»å›žå½’ï¼Œæé«˜äº†å®šä½ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šR-TLMå—çš„å…·ä½“ç»“æž„æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨ï¼Œç”¨äºŽå…³æ³¨å…³é”®çš„æ—¶é—´ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…å«åˆ†ç±»æŸå¤±ï¼ˆä¼ªé€ /éžä¼ªé€ ï¼‰å’Œå›žå½’æŸå¤±ï¼ˆæ—¶åºåç§»ï¼‰ã€‚é¢„è®­ç»ƒæ¨¡åž‹WavLMå’ŒCLIPçš„é€‰æ‹©æ˜¯åŸºäºŽå…¶åœ¨éŸ³é¢‘å’Œè§†é¢‘ç‰¹å¾æå–æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„ç»†èŠ‚å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨IJCAI'25 DDL-AVåŸºå‡†æµ‹è¯•é›†çš„æµ‹è¯•é›†ä¸Šå–å¾—äº†0.7535çš„æœ€ç»ˆå¾—åˆ†ï¼Œè¡¨æ˜Žå…¶åœ¨è·¨æ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œå®šä½æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚è¯¥ç»“æžœéªŒè¯äº†æ‰€æå‡ºçš„FPN-Transformeræ¡†æž¶åœ¨èžåˆå¤šæ¨¡æ€ä¿¡æ¯å’Œç²¾ç¡®å®šä½ä¼ªé€ ç‰‡æ®µæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æå‡å¹…åº¦éœ€è¦å‚è€ƒåŸºçº¿æ–¹æ³•çš„æ€§èƒ½æ•°æ®ï¼Œä½†æ‘˜è¦ä¸­æœªæä¾›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽç¤¾äº¤åª’ä½“å¹³å°ã€æ–°é—»åª’ä½“æœºæž„ç­‰ï¼Œç”¨äºŽæ£€æµ‹å’Œè¯†åˆ«æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œç»´æŠ¤æ•°å­—ä¿¡æ¯çš„çœŸå®žæ€§å’Œå¯ä¿¡åº¦ã€‚é€šè¿‡è‡ªåŠ¨æ£€æµ‹å’Œå®šä½ä¼ªé€ ç‰‡æ®µï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œä¿æŠ¤å…¬ä¼—åˆ©ç›Šï¼Œå¹¶ä¸ºå¸æ³•é‰´å®šæä¾›æŠ€æœ¯æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šåª’ä½“é¢†åŸŸï¼Œå¦‚å›¾åƒã€æ–‡æœ¬ç­‰ï¼Œæž„å»ºæ›´å…¨é¢çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL

