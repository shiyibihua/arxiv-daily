---
layout: default
title: Multi-modal Deepfake Detection and Localization with FPN-Transformer
---

# Multi-modal Deepfake Detection and Localization with FPN-Transformer

**arXiv**: [2511.08031v1](https://arxiv.org/abs/2511.08031) | [PDF](https://arxiv.org/pdf/2511.08031.pdf)

**ä½œè€…**: Chende Zheng, Ruiqi Suo, Zhoulin Ji, Jingyi Deng, Fangbin Yi, Chenhao Lin, Chao Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽFPN-Transformerçš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸Žå®šä½æ¡†æž¶ï¼Œä»¥åº”å¯¹è·¨æ¨¡æ€ä¼ªé€ å¨èƒã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹` `ç‰¹å¾é‡‘å­—å¡”-Transformer` `è·¨æ¨¡æ€å®šä½` `è‡ªç›‘ç£ç‰¹å¾æå–` `æ—¶é—´è¾¹ç•Œå›žå½’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•æ¨¡æ€æ–¹æ³•éš¾ä»¥åˆ©ç”¨è·¨æ¨¡æ€å…³è”å¹¶ç²¾ç¡®å®šä½ä¼ªé€ ç‰‡æ®µï¼Œé™åˆ¶äº†å¯¹ç²¾ç»†ä¼ªé€ çš„æ£€æµ‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨WavLMå’ŒCLIPæå–ç‰¹å¾ï¼Œæž„å»ºå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œé€šè¿‡åŒåˆ†æ”¯é¢„æµ‹å¤´å®žçŽ°æ£€æµ‹ä¸Žå®šä½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨IJCAI'25 DDL-AVåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†0.7535ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL

