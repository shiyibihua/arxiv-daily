---
layout: default
title: Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling
---

# Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.07710" target="_blank" class="toolbar-btn">arXiv: 2511.07710v3</a>
    <a href="https://arxiv.org/pdf/2511.07710.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07710v3" 
            onclick="toggleFavorite(this, '2511.07710v3', 'Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiale Liu, Haoming Zhou, Yishu Liu, Bingzhi Chen, Yuncheng Jiang

**ÂàÜÁ±ª**: cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-11 (Êõ¥Êñ∞: 2025-11-29)

**Â§áÊ≥®**: 10 pages, 6 figures, accepted by AAAI 2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Á≤íÂ∫¶ÊÑüÁü•ÂíåÂå∫Âüü‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ÁöÑË∑®Ê®°ÊÄÅÁªÜÁ≤íÂ∫¶ÂØπÈΩêÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÁªÜÁ≤íÂ∫¶ÂØπÈΩê` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `ÂõæÂÉèÊñáÊú¨ÂØπÈΩê` `‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°` `È´òÊñØÊ∑∑ÂêàÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁªÜÁ≤íÂ∫¶ÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÊñπÊ≥ïÁº∫‰πèÂØπËßÜËßâÂíåÊñáÊú¨tokenÈáçË¶ÅÊÄßÁöÑÊúâÊïàËØÑ‰º∞ÔºåÂØºËá¥Â§çÊùÇÂú∫ÊôØÊ≥õÂåñÊÄßÂ∑Æ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÁªìÂêàÊòæËëóÊÄßÊÑüÁü•ÂíåÁ≤íÂ∫¶ÊÑüÁü•Âª∫Ê®°Ôºå‰ª•ÂèäÂå∫ÂüüÁ∫ß‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ÁöÑÁªü‰∏ÄÊñπÊ≥ïÔºåÊèêÂçáÂØπÈΩêÁ≤æÂ∫¶„ÄÇ
3. Âú®Flickr30KÂíåMS-COCOÊï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ïÂú®Â§öÁßçÈ™®Âπ≤ÁΩëÁªú‰∏äÂèñÂæóSOTAÊÄßËÉΩÔºåÊèêÂçá‰∫ÜÈ≤ÅÊ£íÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁªÜÁ≤íÂ∫¶ÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÊòØÂ§öÊ®°ÊÄÅÂ≠¶‰π†‰∏≠ÁöÑÂÖ≥ÈîÆÊåëÊàòÔºåÊîØÊíëÁùÄËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞ÂíåËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≠âÈáçË¶ÅÂ∫îÁî®„ÄÇ‰∏éÂÖ®Â±ÄÂØπÈΩê‰∏çÂêåÔºåÁªÜÁ≤íÂ∫¶ÂØπÈΩêÈúÄË¶ÅÂú®Â±ÄÈÉ®ËßÜËßâÂå∫ÂüüÂíåÊñáÊú¨Ê†áËÆ∞‰πãÈó¥Âª∫Á´ãÁ≤æÁ°ÆÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºå‰ΩÜÂ∏∏Â∏∏ÂèóÂà∞Âô™Â£∞Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåËøáÂ∫¶ÁÆÄÂåñÁöÑË∑®Ê®°ÊÄÅÂÖ≥Á≥ªÂª∫Ê®°ÁöÑÈòªÁ¢ç„ÄÇÊú¨ÊñáÊåáÂá∫Áé∞ÊúâÊñπÊ≥ïÁöÑ‰∏§‰∏™Ê†πÊú¨Â±ÄÈôêÊÄßÔºöÁº∫‰πèÈ≤ÅÊ£íÁöÑÊ®°ÊÄÅÂÜÖÊú∫Âà∂Êù•ËØÑ‰º∞ËßÜËßâÂíåÊñáÊú¨Ê†áËÆ∞ÁöÑÈáçË¶ÅÊÄßÔºåÂØºËá¥Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠Ê≥õÂåñËÉΩÂäõÂ∑ÆÔºõ‰ª•ÂèäÁº∫‰πèÁªÜÁ≤íÂ∫¶ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ÔºåÊó†Ê≥ïÊçïÊçâÂå∫Âüü-ËØçÂØπÂ∫îÂÖ≥Á≥ªÁöÑ‰∏ÄÂØπÂ§öÂíåÂ§öÂØπ‰∏ÄÊÄßË¥®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜÊòæËëóÊÄßÊÑüÁü•ÂíåÁ≤íÂ∫¶ÊÑüÁü•Âª∫Ê®°‰ª•ÂèäÂå∫ÂüüÁ∫ß‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Ê®°ÊÄÅÁâπÂÆöÁöÑÂÅèÂ∑ÆÊù•ËØÜÂà´ÊòæËëóÁâπÂæÅÔºåËÄåÊó†ÈúÄ‰æùËµñËÑÜÂº±ÁöÑË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõÔºåÂπ∂Â∞ÜÂå∫ÂüüÁâπÂæÅË°®Á§∫‰∏∫È´òÊñØÊ∑∑ÂêàÂàÜÂ∏ÉÔºå‰ª•ÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÂú®Flickr30KÂíåMS-COCO‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂêÑÁßçÈ™®Âπ≤ÁΩëÁªúÊû∂ÊûÑ‰∏äÈÉΩÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁªÜÁ≤íÂ∫¶ÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÁöÑÈ≤ÅÊ£íÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁªÜÁ≤íÂ∫¶ÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÊñπÊ≥ïÈöæ‰ª•ÂáÜÁ°ÆÂª∫Á´ãÂ±ÄÈÉ®ËßÜËßâÂå∫ÂüüÂíåÊñáÊú¨Ê†áËÆ∞‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÔºö‰∏ÄÊòØÁº∫‰πèÈ≤ÅÊ£íÁöÑÊ®°ÊÄÅÂÜÖÊú∫Âà∂Êù•ËØÑ‰º∞ËßÜËßâÂíåÊñáÊú¨tokenÁöÑÈáçË¶ÅÊÄßÔºåÂØºËá¥Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠Ê≥õÂåñËÉΩÂäõÂ∑ÆÔºõ‰∫åÊòØÁº∫‰πèÁªÜÁ≤íÂ∫¶ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ÔºåÊó†Ê≥ïÊçïÊçâÂå∫Âüü-ËØçÂØπÂ∫îÂÖ≥Á≥ªÁöÑ‰∏ÄÂØπÂ§öÂíåÂ§öÂØπ‰∏ÄÊÄßË¥®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂêåÊó∂ÂÖ≥Ê≥®Ê®°ÊÄÅÂÜÖÁöÑÊòæËëóÊÄß‰ª•ÂèäÊ®°ÊÄÅÈó¥ÂØπÂ∫îÂÖ≥Á≥ªÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÈÄöËøáÊ®°ÊÄÅÁâπÂÆöÁöÑÂÅèÂ∑ÆÊù•ËØÜÂà´ÊòæËëóÁâπÂæÅÔºåÈÅøÂÖç‰æùËµñËÑÜÂº±ÁöÑË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõ„ÄÇÂêåÊó∂Ôºå‰ΩøÁî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãÊù•Ë°®Á§∫Âå∫ÂüüÁâπÂæÅÔºåÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞Âª∫Ê®°Âå∫Âüü-ËØçÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºöÂàÜÂà´ÊèêÂèñÂõæÂÉèÂå∫ÂüüÂíåÊñáÊú¨tokenÁöÑÁâπÂæÅ„ÄÇ2) ÊòæËëóÊÄßÊÑüÁü•ÂíåÁ≤íÂ∫¶ÊÑüÁü•Âª∫Ê®°Ê®°ÂùóÔºöÂà©Áî®Ê®°ÊÄÅÁâπÂÆöÁöÑÂÅèÂ∑ÆÊù•ËØÜÂà´ÊòæËëóÁâπÂæÅ„ÄÇ3) Âå∫ÂüüÁ∫ß‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°Ê®°ÂùóÔºöÂ∞ÜÂå∫ÂüüÁâπÂæÅË°®Á§∫‰∏∫È´òÊñØÊ∑∑ÂêàÂàÜÂ∏É„ÄÇ4) ÂØπÈΩêÊ®°ÂùóÔºöÂü∫‰∫é‰∏äËø∞ÁâπÂæÅË°®Á§∫ÔºåËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂêåÊó∂ËÄÉËôë‰∫ÜÊ®°ÊÄÅÂÜÖÁöÑÊòæËëóÊÄßÂíåÊ®°ÊÄÅÈó¥ÂØπÂ∫îÂÖ≥Á≥ªÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ï‰∏çÂÜç‰æùËµñ‰∫éËÑÜÂº±ÁöÑË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõÔºåËÄåÊòØÈÄöËøáÊ®°ÊÄÅÁâπÂÆöÁöÑÂÅèÂ∑ÆÊù•ËØÜÂà´ÊòæËëóÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãÊù•Ë°®Á§∫Âå∫ÂüüÁâπÂæÅÔºåËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞Âª∫Ê®°Âå∫Âüü-ËØçÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Âå∫ÂüüÁ∫ß‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°Ê®°Âùó‰∏≠ÔºåÂå∫ÂüüÁâπÂæÅË¢´Ë°®Á§∫‰∏∫È´òÊñØÊ∑∑ÂêàÂàÜÂ∏ÉÔºåÊØè‰∏™È´òÊñØÂàÜÈáè‰ª£Ë°®‰∏ÄÁßçÂèØËÉΩÁöÑÂå∫ÂüüÁâπÂæÅ„ÄÇÊ∑∑ÂêàÊ®°ÂûãÁöÑÂèÇÊï∞ÔºàÂùáÂÄº„ÄÅÊñπÂ∑Æ„ÄÅÊ∑∑ÂêàÁ≥ªÊï∞ÔºâÈÄöËøáÂ≠¶‰π†ÂæóÂà∞„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈúÄË¶ÅÂêåÊó∂ËÄÉËôëÂØπÈΩêÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÁöÑÂª∫Ê®°„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®Flickr30KÂíåMS-COCOÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂêÑÁßçÈ™®Âπ≤ÁΩëÁªúÊû∂ÊûÑ‰∏äÈÉΩËÉΩÂ§üÊòæËëóÊèêÈ´òÁªÜÁ≤íÂ∫¶ÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÂØπÈΩêÂáÜÁ°ÆÁéá‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂπ∂‰∏îÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØËß£ÈáäÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÁªÜÁ≤íÂ∫¶ÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÔºåÂèØ‰ª•ÊèêÈ´òËøô‰∫õÂ∫îÁî®Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ÊèêÂá∫ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ÊñπÊ≥ï‰πü‰∏∫ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅÂ≠¶‰π†‰ªªÂä°Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊΩúÂú®ÁöÑÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.

