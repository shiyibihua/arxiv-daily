---
layout: default
title: Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling
---

# Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling

**arXiv**: [2511.07710v3](https://arxiv.org/abs/2511.07710) | [PDF](https://arxiv.org/pdf/2511.07710.pdf)

**ä½œè€…**: Jiale Liu, Haoming Zhou, Yishu Liu, Bingzhi Chen, Yuncheng Jiang

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11 (æ›´æ–°: 2025-11-29)

**å¤‡æ³¨**: 10 pages, 6 figures, accepted by AAAI 2026

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç²’åº¦æ„ŸçŸ¥å’ŒåŒºåŸŸä¸ç¡®å®šæ€§å»ºæ¨¡çš„è·¨æ¨¡æ€ç»†ç²’åº¦å¯¹é½æ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç»†ç²’åº¦å¯¹é½` `è·¨æ¨¡æ€å­¦ä¹ ` `å›¾åƒæ–‡æœ¬å¯¹é½` `ä¸ç¡®å®šæ€§å»ºæ¨¡` `é«˜æ–¯æ··åˆæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹æ³•ç¼ºä¹å¯¹è§†è§‰å’Œæ–‡æœ¬tokené‡è¦æ€§çš„æœ‰æ•ˆè¯„ä¼°ï¼Œå¯¼è‡´å¤æ‚åœºæ™¯æ³›åŒ–æ€§å·®ã€‚
2. è®ºæ–‡æå‡ºç»“åˆæ˜¾è‘—æ€§æ„ŸçŸ¥å’Œç²’åº¦æ„ŸçŸ¥å»ºæ¨¡ï¼Œä»¥åŠåŒºåŸŸçº§ä¸ç¡®å®šæ€§å»ºæ¨¡çš„ç»Ÿä¸€æ–¹æ³•ï¼Œæå‡å¯¹é½ç²¾åº¦ã€‚
3. åœ¨Flickr30Kå’ŒMS-COCOæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§éª¨å¹²ç½‘ç»œä¸Šå–å¾—SOTAæ€§èƒ½ï¼Œæå‡äº†é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ˜¯å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œæ”¯æ’‘ç€è§†è§‰é—®ç­”ã€å›¾åƒæè¿°å’Œè§†è§‰-è¯­è¨€å¯¼èˆªç­‰é‡è¦åº”ç”¨ã€‚ä¸Žå…¨å±€å¯¹é½ä¸åŒï¼Œç»†ç²’åº¦å¯¹é½éœ€è¦åœ¨å±€éƒ¨è§†è§‰åŒºåŸŸå’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´å»ºç«‹ç²¾ç¡®çš„å¯¹åº”å…³ç³»ï¼Œä½†å¸¸å¸¸å—åˆ°å™ªå£°æ³¨æ„åŠ›æœºåˆ¶å’Œè¿‡åº¦ç®€åŒ–çš„è·¨æ¨¡æ€å…³ç³»å»ºæ¨¡çš„é˜»ç¢ã€‚æœ¬æ–‡æŒ‡å‡ºçŽ°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªæ ¹æœ¬å±€é™æ€§ï¼šç¼ºä¹é²æ£’çš„æ¨¡æ€å†…æœºåˆ¶æ¥è¯„ä¼°è§†è§‰å’Œæ–‡æœ¬æ ‡è®°çš„é‡è¦æ€§ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›å·®ï¼›ä»¥åŠç¼ºä¹ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œæ— æ³•æ•æ‰åŒºåŸŸ-è¯å¯¹åº”å…³ç³»çš„ä¸€å¯¹å¤šå’Œå¤šå¯¹ä¸€æ€§è´¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œç»“åˆäº†æ˜¾è‘—æ€§æ„ŸçŸ¥å’Œç²’åº¦æ„ŸçŸ¥å»ºæ¨¡ä»¥åŠåŒºåŸŸçº§ä¸ç¡®å®šæ€§å»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ¨¡æ€ç‰¹å®šçš„åå·®æ¥è¯†åˆ«æ˜¾è‘—ç‰¹å¾ï¼Œè€Œæ— éœ€ä¾èµ–è„†å¼±çš„è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼Œå¹¶å°†åŒºåŸŸç‰¹å¾è¡¨ç¤ºä¸ºé«˜æ–¯æ··åˆåˆ†å¸ƒï¼Œä»¥æ•æ‰ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§ã€‚åœ¨Flickr30Kå’ŒMS-COCOä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§éª¨å¹²ç½‘ç»œæž¶æž„ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½çš„é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹æ³•éš¾ä»¥å‡†ç¡®å»ºç«‹å±€éƒ¨è§†è§‰åŒºåŸŸå’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºŽï¼šä¸€æ˜¯ç¼ºä¹é²æ£’çš„æ¨¡æ€å†…æœºåˆ¶æ¥è¯„ä¼°è§†è§‰å’Œæ–‡æœ¬tokençš„é‡è¦æ€§ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›å·®ï¼›äºŒæ˜¯ç¼ºä¹ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œæ— æ³•æ•æ‰åŒºåŸŸ-è¯å¯¹åº”å…³ç³»çš„ä¸€å¯¹å¤šå’Œå¤šå¯¹ä¸€æ€§è´¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åŒæ—¶å…³æ³¨æ¨¡æ€å†…çš„æ˜¾è‘—æ€§ä»¥åŠæ¨¡æ€é—´å¯¹åº”å…³ç³»çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡æ¨¡æ€ç‰¹å®šçš„åå·®æ¥è¯†åˆ«æ˜¾è‘—ç‰¹å¾ï¼Œé¿å…ä¾èµ–è„†å¼±çš„è·¨æ¨¡æ€æ³¨æ„åŠ›ã€‚åŒæ—¶ï¼Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡åž‹æ¥è¡¨ç¤ºåŒºåŸŸç‰¹å¾ï¼Œæ•æ‰ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§ï¼Œä»Žè€Œæ›´å‡†ç¡®åœ°å»ºæ¨¡åŒºåŸŸ-è¯çš„å¯¹åº”å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šåˆ†åˆ«æå–å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬tokençš„ç‰¹å¾ã€‚2) æ˜¾è‘—æ€§æ„ŸçŸ¥å’Œç²’åº¦æ„ŸçŸ¥å»ºæ¨¡æ¨¡å—ï¼šåˆ©ç”¨æ¨¡æ€ç‰¹å®šçš„åå·®æ¥è¯†åˆ«æ˜¾è‘—ç‰¹å¾ã€‚3) åŒºåŸŸçº§ä¸ç¡®å®šæ€§å»ºæ¨¡æ¨¡å—ï¼šå°†åŒºåŸŸç‰¹å¾è¡¨ç¤ºä¸ºé«˜æ–¯æ··åˆåˆ†å¸ƒã€‚4) å¯¹é½æ¨¡å—ï¼šåŸºäºŽä¸Šè¿°ç‰¹å¾è¡¨ç¤ºï¼Œè¿›è¡Œç»†ç²’åº¦çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽåŒæ—¶è€ƒè™‘äº†æ¨¡æ€å†…çš„æ˜¾è‘—æ€§å’Œæ¨¡æ€é—´å¯¹åº”å…³ç³»çš„ä¸ç¡®å®šæ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸å†ä¾èµ–äºŽè„†å¼±çš„è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡æ€ç‰¹å®šçš„åå·®æ¥è¯†åˆ«æ˜¾è‘—ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡åž‹æ¥è¡¨ç¤ºåŒºåŸŸç‰¹å¾ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å»ºæ¨¡åŒºåŸŸ-è¯çš„å¯¹åº”å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŒºåŸŸçº§ä¸ç¡®å®šæ€§å»ºæ¨¡æ¨¡å—ä¸­ï¼ŒåŒºåŸŸç‰¹å¾è¢«è¡¨ç¤ºä¸ºé«˜æ–¯æ··åˆåˆ†å¸ƒï¼Œæ¯ä¸ªé«˜æ–¯åˆ†é‡ä»£è¡¨ä¸€ç§å¯èƒ½çš„åŒºåŸŸç‰¹å¾ã€‚æ··åˆæ¨¡åž‹çš„å‚æ•°ï¼ˆå‡å€¼ã€æ–¹å·®ã€æ··åˆç³»æ•°ï¼‰é€šè¿‡å­¦ä¹ å¾—åˆ°ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦åŒæ—¶è€ƒè™‘å¯¹é½çš„å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§çš„å»ºæ¨¡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨Flickr30Kå’ŒMS-COCOæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å„ç§éª¨å¹²ç½‘ç»œæž¶æž„ä¸Šéƒ½èƒ½å¤Ÿæ˜¾è‘—æé«˜ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹é½å‡†ç¡®çŽ‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽè§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€è§†è§‰-è¯­è¨€å¯¼èˆªç­‰é¢†åŸŸã€‚é€šè¿‡æå‡ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½çš„ç²¾åº¦å’Œé²æ£’æ€§ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºçš„ä¸ç¡®å®šæ€§å»ºæ¨¡æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œæ½œåœ¨çš„æœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.

