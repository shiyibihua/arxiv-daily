---
layout: default
title: DT-NVS: Diffusion Transformers for Novel View Synthesis
---

# DT-NVS: Diffusion Transformers for Novel View Synthesis

**arXiv**: [2511.08823v1](https://arxiv.org/abs/2511.08823) | [PDF](https://arxiv.org/pdf/2511.08823.pdf)

**ä½œè€…**: Wonbong Jang, Jonathan Tremblay, Lourdes Agapito

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: 14 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDT-NVSï¼Œåˆ©ç”¨Transformerçš„3Dæ‰©æ•£æ¨¡åž‹å®žçŽ°çœŸå®žåœºæ™¯çš„æ–°è§†è§’åˆæˆ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ–°è§†è§’åˆæˆ` `æ‰©æ•£æ¨¡åž‹` `Transformer` `3Dæ„ŸçŸ¥` `å•ç›®è§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ–°è§†è§’åˆæˆæ–¹æ³•ä¸»è¦é›†ä¸­äºŽå°èŒƒå›´ç›¸æœºè¿åŠ¨æˆ–éžè‡ªç„¶ç‰©ä½“ä¸­å¿ƒåœºæ™¯ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®žåœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. DT-NVSåˆ©ç”¨Transformeræž¶æž„ï¼Œæž„å»º3Dæ„ŸçŸ¥çš„æ‰©æ•£æ¨¡åž‹ï¼Œå¹¶è®¾è®¡äº†ç›¸æœºæ¡ä»¶ç­–ç•¥ï¼Œä»¥é€‚åº”çœŸå®žä¸–ç•Œæœªå¯¹é½çš„æ•°æ®é›†ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDT-NVSåœ¨å•è§†è§’å›¾åƒç”Ÿæˆæ–°è§†è§’ä»»åŠ¡ä¸Šï¼Œè¶…è¶Šäº†çŽ°æœ‰3Dæ„ŸçŸ¥æ‰©æ•£æ¨¡åž‹å’Œç¡®å®šæ€§æ–¹æ³•ï¼Œå¹¶èƒ½ç”Ÿæˆå¤šæ ·åŒ–ç»“æžœã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹ä»Žå•è§†è§’å›¾åƒç”Ÿæˆè‡ªç„¶åœºæ™¯æ–°è§†è§’è¿™ä¸€æœªè¢«å……åˆ†æŽ¢ç´¢çš„é—®é¢˜ï¼Œæå‡ºäº†DT-NVSï¼Œä¸€ç§åŸºäºŽTransformeræž¶æž„çš„3Dæ„ŸçŸ¥æ‰©æ•£æ¨¡åž‹ï¼Œç”¨äºŽå¹¿ä¹‰æ–°è§†è§’åˆæˆã€‚è¯¥æ¨¡åž‹åœ¨åŒ…å«çœŸå®žä¸–ç•Œã€å¤šç±»åˆ«ã€æœªå¯¹é½ä¸”éšæ„æ‹æ‘„çš„æ—¥å¸¸åœºæ™¯è§†é¢‘çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œä»…ä½¿ç”¨å›¾åƒæŸå¤±è¿›è¡Œè®­ç»ƒã€‚è®ºæ–‡å¯¹Transformerå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº†é‡è¦æ”¹è¿›ï¼Œä»¥å°†å›¾åƒè½¬æ¢ä¸º3Dè¡¨ç¤ºï¼Œå¹¶æå‡ºäº†æ–°çš„ç›¸æœºæ¡ä»¶ç­–ç•¥ï¼Œä»Žè€Œå¯ä»¥åœ¨çœŸå®žä¸–ç•Œæœªå¯¹é½çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼ï¼Œå³åœ¨æ¡ä»¶å›¾åƒå’Œé‡‡æ ·çš„å™ªå£°è¾“å…¥ä¹‹é—´äº¤æ¢å‚è€ƒå¸§çš„è§’è‰²ã€‚åœ¨å¹¿ä¹‰æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜Žï¼Œè¯¥æ–¹æ³•ä¼˜äºŽæœ€å…ˆè¿›çš„3Dæ„ŸçŸ¥æ‰©æ•£æ¨¡åž‹å’Œç¡®å®šæ€§æ–¹æ³•ï¼Œå¹¶èƒ½ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å‡ºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»Žå•å¼ å›¾åƒç”ŸæˆçœŸå®žä¸–ç•Œå¤æ‚åœºæ™¯çš„æ–°è§†è§’å›¾åƒçš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ–¹æ³•ï¼Œé€šå¸¸å—é™äºŽå°èŒƒå›´çš„ç›¸æœºè¿åŠ¨æˆ–è€…ä»…é€‚ç”¨äºŽç‰©ä½“ä¸­å¿ƒåœºæ™¯ï¼Œæ— æ³•å¾ˆå¥½åœ°å¤„ç†çœŸå®žä¸–ç•Œä¸­å¤šç±»åˆ«ã€æœªå¯¹é½çš„æ—¥å¸¸åœºæ™¯è§†é¢‘ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†æ–°è§†è§’åˆæˆæŠ€æœ¯åœ¨æ›´å¹¿æ³›çš„å®žé™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨3Dæ„ŸçŸ¥çš„æ‰©æ•£æ¨¡åž‹ï¼Œç»“åˆTransformeræž¶æž„çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ï¼Œå­¦ä¹ ä»Žå•å¼ å›¾åƒåˆ°3Dåœºæ™¯è¡¨ç¤ºçš„æ˜ å°„ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šç”Ÿæˆæ–°çš„è§†è§’ã€‚é€šè¿‡å¼•å…¥ç›¸æœºæ¡ä»¶ç­–ç•¥å’Œåˆ›æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œæ¨¡åž‹èƒ½å¤Ÿé€‚åº”çœŸå®žä¸–ç•Œæœªå¯¹é½çš„æ•°æ®é›†ï¼Œä»Žè€Œå®žçŽ°æ›´å¹¿ä¹‰çš„æ–°è§†è§’åˆæˆã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDT-NVSçš„æ•´ä½“æ¡†æž¶æ˜¯ä¸€ä¸ªåŸºäºŽTransformerçš„3Dæ‰©æ•£æ¨¡åž‹ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å›¾åƒç¼–ç å™¨ï¼šå°†è¾“å…¥å›¾åƒç¼–ç æˆç‰¹å¾è¡¨ç¤ºã€‚2) 3Dè¡¨ç¤ºæ¨¡å—ï¼šåˆ©ç”¨æ”¹è¿›çš„Transformerå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å›¾åƒç‰¹å¾è½¬æ¢ä¸º3Dåœºæ™¯è¡¨ç¤ºã€‚3) ç›¸æœºæ¡ä»¶æ¨¡å—ï¼šå°†ç›®æ ‡ç›¸æœºçš„ä½å§¿ä¿¡æ¯èžå…¥åˆ°æ¨¡åž‹ä¸­ï¼ŒæŒ‡å¯¼æ–°è§†è§’çš„ç”Ÿæˆã€‚4) æ‰©æ•£æ¨¡åž‹ï¼šé€šè¿‡é€æ­¥åŽ»å™ªçš„è¿‡ç¨‹ï¼Œä»Žå™ªå£°ä¸­ç”Ÿæˆæ–°è§†è§’çš„å›¾åƒã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å›¾åƒæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡åž‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) é’ˆå¯¹Transformerå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›ï¼Œä½¿å…¶æ›´é€‚åˆäºŽå›¾åƒåˆ°3Dè¡¨ç¤ºçš„è½¬æ¢ã€‚2) æ–°çš„ç›¸æœºæ¡ä»¶ç­–ç•¥ï¼Œå…è®¸æ¨¡åž‹åœ¨çœŸå®žä¸–ç•Œæœªå¯¹é½çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚3) åˆ›æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œå³åœ¨æ¡ä»¶å›¾åƒå’Œé‡‡æ ·çš„å™ªå£°è¾“å…¥ä¹‹é—´äº¤æ¢å‚è€ƒå¸§çš„è§’è‰²ï¼Œå¢žå¼ºäº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æž„æ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†æŸç§å½¢å¼çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¾‹å¦‚Sparse Attentionæˆ–è€…Axial Attentionï¼Œä»¥é™ä½Žè®¡ç®—å¤æ‚åº¦ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé™¤äº†å¸¸è§çš„L1æˆ–L2æŸå¤±å¤–ï¼Œå¯èƒ½è¿˜ä½¿ç”¨äº†æ„ŸçŸ¥æŸå¤±æˆ–å¯¹æŠ—æŸå¤±ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚ç›¸æœºæ¡ä»¶æ¨¡å—çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦æœ‰æ•ˆåœ°å°†ç›¸æœºä½å§¿ä¿¡æ¯èžå…¥åˆ°æ¨¡åž‹ä¸­ï¼Œä¾‹å¦‚é€šè¿‡ç‰¹å¾èžåˆæˆ–æ¡ä»¶å½’ä¸€åŒ–ç­‰æ–¹å¼ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

DT-NVSåœ¨å¹¿ä¹‰æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†çŽ°æœ‰çš„3Dæ„ŸçŸ¥æ‰©æ•£æ¨¡åž‹å’Œç¡®å®šæ€§æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†å…¶åœ¨ç”Ÿæˆå¤šæ ·åŒ–è¾“å‡ºæ–¹é¢çš„ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†çœŸå®žä¸–ç•Œã€å¤šç±»åˆ«ã€æœªå¯¹é½çš„æ—¥å¸¸åœºæ™¯è§†é¢‘æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œè¯æ˜Žäº†å…¶åœ¨å®žé™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨å•ç›®æ‘„åƒå¤´æ‹æ‘„çš„å›¾åƒï¼Œç”Ÿæˆå‘¨å›´çŽ¯å¢ƒçš„æ–°è§†è§’å›¾åƒï¼Œä»Žè€Œæ›´å¥½åœ°ç†è§£çŽ¯å¢ƒå¹¶è§„åˆ’è·¯å¾„ã€‚åœ¨è™šæ‹ŸçŽ°å®žä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å•å¼ ç…§ç‰‡ç”Ÿæˆé€¼çœŸçš„3Dåœºæ™¯ï¼Œä»Žè€ŒèŽ·å¾—æ›´æ²‰æµ¸å¼çš„ä½“éªŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

