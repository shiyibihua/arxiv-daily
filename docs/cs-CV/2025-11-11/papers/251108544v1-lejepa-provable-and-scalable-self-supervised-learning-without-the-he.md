---
layout: default
title: LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics
---

# LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics

**arXiv**: [2511.08544v1](https://arxiv.org/abs/2511.08544) | [PDF](https://arxiv.org/pdf/2511.08544.pdf)

**ä½œè€…**: Randall Balestriero, Yann LeCun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLeJEPAä»¥æä¾›å¯è¯æ˜Žä¸”å¯æ‰©å±•çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæ— éœ€å¯å‘å¼æ–¹æ³•ã€‚**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è”åˆåµŒå…¥é¢„æµ‹æž¶æž„` `å„å‘åŒæ€§é«˜æ–¯åˆ†å¸ƒ` `å¯æ‰©å±•è®­ç»ƒ` `ç†è®ºæŒ‡å¯¼` `å›¾åƒè¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šJEPAç¼ºä¹ç†è®ºæŒ‡å¯¼ï¼Œå¯¼è‡´ç ”å‘ä¾èµ–å¯å‘å¼æ–¹æ³•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆé¢„æµ‹æŸå¤±ä¸ŽSIGRegï¼Œçº¦æŸåµŒå…¥åˆ†å¸ƒä¸ºå„å‘åŒæ€§é«˜æ–¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet-1kä¸Šï¼ŒViT-H/14çº¿æ€§è¯„ä¼°å‡†ç¡®çŽ‡è¾¾79%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).

