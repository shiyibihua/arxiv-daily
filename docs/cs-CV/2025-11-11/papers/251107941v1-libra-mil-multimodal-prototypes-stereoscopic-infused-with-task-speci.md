---
layout: default
title: Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification
---

# Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification

**arXiv**: [2511.07941v1](https://arxiv.org/abs/2511.07941) | [PDF](https://arxiv.org/pdf/2511.07941.pdf)

**ä½œè€…**: Zhenfeng Zhuang, Fangyu Zhou, Liansheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€åŽŸåž‹å¤šç¤ºä¾‹å­¦ä¹ æ–¹æ³•ï¼Œä»¥è§£å†³å°‘æ ·æœ¬å…¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»ä¸­çš„è·¨æ¨¡æ€äº¤äº’é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šç¤ºä¾‹å­¦ä¹ ` `å¤šæ¨¡æ€åŽŸåž‹` `ç«‹ä½“æœ€ä¼˜ä¼ è¾“` `å°‘æ ·æœ¬åˆ†ç±»` `å…¨åˆ‡ç‰‡å›¾åƒ` `ç—…ç†å®žä½“æè¿°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨åˆ‡ç‰‡å›¾åƒè®¡ç®—æˆæœ¬é«˜ï¼Œä¸”ç—…ç†ä»»åŠ¡ä»…æœ‰è¢‹çº§æ ‡ç­¾ï¼Œè¯­è¨€æ¨¡åž‹ç”Ÿæˆå®žä¾‹æè¿°å­˜åœ¨åå·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºä»»åŠ¡ç‰¹å®šç—…ç†å®žä½“åŽŸåž‹ï¼Œé€šè¿‡ç«‹ä½“æœ€ä¼˜ä¼ è¾“ç®—æ³•ä¿ƒè¿›è§†è§‰ä¸Žæ–‡æœ¬åŒå‘äº¤äº’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ–¹æ³•åœ¨å°‘æ ·æœ¬åˆ†ç±»å’Œå¯è§£é‡Šæ€§æ–¹é¢è¡¨çŽ°ä¼˜è¶Šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.

