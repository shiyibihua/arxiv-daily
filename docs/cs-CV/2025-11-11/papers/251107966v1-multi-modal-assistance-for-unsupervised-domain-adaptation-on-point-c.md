---
layout: default
title: Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection
---

# Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection

**arXiv**: [2511.07966v1](https://arxiv.org/abs/2511.07966) | [PDF](https://arxiv.org/pdf/2511.07966.pdf)

**ä½œè€…**: Shenao Zhao, Pengpeng Liang, Zhoufan Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: Accepted to AAAI-26

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/liangp/MMAssist)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMAssistï¼Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è¾…åŠ©LiDARç‚¹äº‘3Dç›®æ ‡æ£€æµ‹çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dç›®æ ‡æ£€æµ‹` `æ— ç›‘ç£åŸŸè‡ªé€‚åº”` `å¤šæ¨¡æ€å­¦ä¹ ` `ç‚¹äº‘` `å›¾åƒç‰¹å¾` `æ–‡æœ¬ç‰¹å¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽä¼ªæ ‡ç­¾çš„3D UDAæ–¹æ³•å¿½ç•¥äº†åŒæ­¥é‡‡é›†çš„å›¾åƒæ•°æ®ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚
2. MMAssiståˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸçš„3Dç‰¹å¾ï¼Œå®žçŽ°è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMMAssiståœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œè¯æ˜Žäº†å¤šæ¨¡æ€è¾…åŠ©åœ¨3D UDAä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMMAssistçš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€è¾…åŠ©æå‡åŸºäºŽLiDARçš„3Dç›®æ ‡æ£€æµ‹çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆ3D UDAï¼‰æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŸºäºŽæ•™å¸ˆ-å­¦ç”Ÿæž¶æž„å’Œä¼ªæ ‡ç­¾ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„3Dç‰¹å¾ã€‚å…·ä½“è€Œè¨€ï¼Œå°†çœŸå€¼æ ‡ç­¾æˆ–ä¼ªæ ‡ç­¾æŠ•å½±åˆ°å›¾åƒä¸Šï¼ŒèŽ·å¾—ä¸€ç»„2Dè¾¹ç•Œæ¡†ã€‚å¯¹äºŽæ¯ä¸ª2Dæ¡†ï¼Œä»Žé¢„è®­ç»ƒçš„è§†è§‰éª¨å¹²ç½‘ç»œä¸­æå–å…¶å›¾åƒç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMï¼‰æå–æ¡†çš„æ–‡æœ¬æè¿°ï¼Œç„¶åŽä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨èŽ·å¾—å…¶æ–‡æœ¬ç‰¹å¾ã€‚åœ¨æºåŸŸæ¨¡åž‹å’Œç›®æ ‡åŸŸå­¦ç”Ÿæ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†é¢„æµ‹æ¡†çš„3Dç‰¹å¾ä¸Žå…¶å¯¹åº”çš„å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œå¹¶å°†3Dç‰¹å¾ä¸Žå¯¹é½çš„ç‰¹å¾èžåˆï¼Œé€šè¿‡å­¦ä¹ æƒé‡è¿›è¡Œæœ€ç»ˆé¢„æµ‹ã€‚åŒæ—¶ï¼Œå¯¹é½ç›®æ ‡åŸŸä¸­å­¦ç”Ÿåˆ†æ”¯å’Œæ•™å¸ˆåˆ†æ”¯ä¹‹é—´çš„ç‰¹å¾ã€‚ä¸ºäº†å¢žå¼ºä¼ªæ ‡ç­¾ï¼Œä½¿ç”¨çŽ°æˆçš„2Dç›®æ ‡æ£€æµ‹å™¨ä»Žå›¾åƒç”Ÿæˆ2Dè¾¹ç•Œæ¡†ï¼Œå¹¶åœ¨ç‚¹äº‘çš„å¸®åŠ©ä¸‹ä¼°è®¡å…¶å¯¹åº”çš„3Dæ¡†ï¼Œå¹¶å°†è¿™äº›3Dæ¡†ä¸Žæ•™å¸ˆæ¨¡åž‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ç›¸ç»“åˆã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨ä¸‰ä¸ªæµè¡Œçš„3Dç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šçš„ä¸‰ä¸ªåŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä¸Žæœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽLiDARçš„3Dç›®æ ‡æ£€æµ‹æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œé€šå¸¸åªå…³æ³¨ç‚¹äº‘æ•°æ®æœ¬èº«ï¼Œå¿½ç•¥äº†åœ¨å®žé™…åº”ç”¨ä¸­ç»å¸¸åŒæ—¶å­˜åœ¨çš„å›¾åƒæ•°æ®ã€‚å¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›å›¾åƒæ•°æ®æ¥æå‡åŸŸè‡ªé€‚åº”çš„æ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å›¾åƒä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡åž‹åœ¨ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå°†æºåŸŸå’Œç›®æ ‡åŸŸçš„3Dç‰¹å¾è¿›è¡Œå¯¹é½ã€‚é€šè¿‡å°†3Dç›®æ ‡æŠ•å½±åˆ°å›¾åƒä¸Šï¼Œæå–å¯¹åº”çš„å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œç„¶åŽå°†è¿™äº›ç‰¹å¾ä¸Ž3Dç‰¹å¾è¿›è¡Œèžåˆï¼Œä»Žè€Œå®žçŽ°è·¨æ¨¡æ€çš„çŸ¥è¯†è¿ç§»ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å›¾åƒä¿¡æ¯ï¼Œæå‡æ¨¡åž‹åœ¨ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMMAssistçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) 2Dç›®æ ‡æ£€æµ‹æ¨¡å—ï¼šç”¨äºŽåœ¨å›¾åƒä¸Šæ£€æµ‹2Dç›®æ ‡æ¡†ï¼Œå¯ä»¥ä½¿ç”¨ground truthæˆ–è€…é¢„è®­ç»ƒçš„2Dæ£€æµ‹å™¨ã€‚2) å¤šæ¨¡æ€ç‰¹å¾æå–æ¨¡å—ï¼šç”¨äºŽæå–2Dç›®æ ‡æ¡†çš„å›¾åƒç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼Œå›¾åƒç‰¹å¾é€šè¿‡é¢„è®­ç»ƒçš„è§†è§‰éª¨å¹²ç½‘ç»œæå–ï¼Œæ–‡æœ¬ç‰¹å¾é€šè¿‡å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹å’Œé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨æå–ã€‚3) 3Dç‰¹å¾å¯¹é½æ¨¡å—ï¼šå°†3Dç‰¹å¾ä¸Žå¯¹åº”çš„å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ æƒé‡è¿›è¡Œèžåˆã€‚4) ä¼ªæ ‡ç­¾å¢žå¼ºæ¨¡å—ï¼šä½¿ç”¨2Dæ£€æµ‹å™¨ç”Ÿæˆçš„2Dæ¡†ä¼°è®¡3Dæ¡†ï¼Œå¹¶ä¸Žæ•™å¸ˆæ¨¡åž‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ç»“åˆï¼Œå¢žå¼ºä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå®žçŽ°äº†3Dç‰¹å¾çš„è·¨æ¨¡æ€å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å›¾åƒä¿¡æ¯ï¼Œæå‡æ¨¡åž‹åœ¨ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¼ªæ ‡ç­¾å¢žå¼ºæ¨¡å—ä¹Ÿè¿›ä¸€æ­¥æå‡äº†æ¨¡åž‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨3Dç‰¹å¾å¯¹é½æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¯å­¦ä¹ çš„æƒé‡æ¥èžåˆ3Dç‰¹å¾å’Œå›¾åƒ/æ–‡æœ¬ç‰¹å¾ï¼Œå…è®¸æ¨¡åž‹è‡ªé€‚åº”åœ°å­¦ä¹ ä¸åŒæ¨¡æ€ç‰¹å¾çš„é‡è¦æ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬3Dç›®æ ‡æ£€æµ‹æŸå¤±ã€ç‰¹å¾å¯¹é½æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±ã€‚ç‰¹å¾å¯¹é½æŸå¤±ç”¨äºŽçº¦æŸ3Dç‰¹å¾ä¸Žå›¾åƒ/æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„è·ç¦»ï¼Œä¸€è‡´æ€§æŸå¤±ç”¨äºŽçº¦æŸæ•™å¸ˆæ¨¡åž‹å’Œå­¦ç”Ÿæ¨¡åž‹ä¹‹é—´çš„è¾“å‡ºä¸€è‡´æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMMAssiståœ¨ä¸‰ä¸ªæµè¡Œçš„3Dç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼ŒKITTI, Waymo, nuScenesï¼‰ä¸Šçš„ä¸‰ä¸ªåŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸­ï¼Œä¸Žæœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒMMAssistçš„æ€§èƒ½æå‡è¶…è¿‡äº†5%ï¼Œè¯æ˜Žäº†è¯¥æ–¹æ³•åœ¨3D UDAä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®‰é˜²ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œé€šå¸¸éœ€è¦åˆ©ç”¨LiDARç‚¹äº‘è¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ï¼Œè€ŒåŸŸè‡ªé€‚åº”æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡åž‹é€‚åº”ä¸åŒçš„çŽ¯å¢ƒå’Œä¼ æ„Ÿå™¨ï¼Œæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€3Dæ„ŸçŸ¥ä»»åŠ¡ä¸­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.

