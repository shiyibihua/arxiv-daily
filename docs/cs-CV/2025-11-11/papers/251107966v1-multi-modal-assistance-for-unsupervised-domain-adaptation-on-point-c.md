---
layout: default
title: Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection
---

# Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection

**arXiv**: [2511.07966v1](https://arxiv.org/abs/2511.07966) | [PDF](https://arxiv.org/pdf/2511.07966.pdf)

**ä½œè€…**: Shenao Zhao, Pengpeng Liang, Zhoufan Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMAssistæ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€è¾…åŠ©æå‡ç‚¹äº‘3Dç›®æ ‡æ£€æµ‹çš„æ— ç›‘ç£åŸŸé€‚åº”æ€§èƒ½**

**å…³é”®è¯**: `æ— ç›‘ç£åŸŸé€‚åº”` `ç‚¹äº‘3Dç›®æ ‡æ£€æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ç‰¹å¾å¯¹é½` `ä¼ªæ ‡ç­¾å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç‚¹äº‘3Dç›®æ ‡æ£€æµ‹åœ¨æ— ç›‘ç£åŸŸé€‚åº”ä¸­ï¼Œå›¾åƒæ•°æ®æœªè¢«å……åˆ†åˆ©ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸçš„3Dç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªåŸŸé€‚åº”ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰å…ˆè¿›æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.

