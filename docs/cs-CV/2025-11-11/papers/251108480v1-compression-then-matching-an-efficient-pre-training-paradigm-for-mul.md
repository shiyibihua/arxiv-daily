---
layout: default
title: Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding
---

# Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.08480" target="_blank" class="toolbar-btn">arXiv: 2511.08480v1</a>
    <a href="https://arxiv.org/pdf/2511.08480.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08480v1" 
            onclick="toggleFavorite(this, '2511.08480v1', 'Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Da Li, Yuxiao Luo, Keping Bi, Jiafeng Guo, Wei Yuan, Biao Yang, Yan Wang, Fan Yang, Tingting Gao, Guorui Zhou

**åˆ†ç±»**: cs.CV, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: Multimodal Embedding

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoMaï¼šä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€åµŒå…¥é¢„è®­ç»ƒèŒƒå¼ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åµŒå…¥` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `é¢„è®­ç»ƒ` `è·¨æ¨¡æ€æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„åµŒå…¥æ–¹æ³•æ¥åŒæ—¶ä¿ç•™è¯­ä¹‰ä¿¡æ¯å’ŒåŒºåˆ†æ€§ç‰¹å¾ã€‚
2. CoMaé€šè¿‡è§£è€¦è¯­ä¹‰ç†è§£å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæå‡ºå‹ç¼©é¢„è®­ç»ƒé˜¶æ®µï¼Œä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„é¢„çƒ­ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMaä»…ä½¿ç”¨å°‘é‡é¢„è®­ç»ƒæ•°æ®å³å¯å°†è§†è§‰-è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºæœ‰ç«äº‰åŠ›çš„åµŒå…¥æ¨¡å‹ï¼Œå¹¶åœ¨MMEBä¸Šå–å¾—SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡è·å–å¯è¿ç§»çš„è¯­ä¹‰åµŒå…¥æ¥æ¨è¿›å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œæ˜¾è‘—æå‡è·¨æ¨¡æ€æ£€ç´¢ã€èšç±»å’Œåˆ†ç±»ç­‰è§†è§‰-è¯­è¨€ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ‰æ•ˆçš„åµŒå…¥éœ€è¦å…¨é¢ä¿ç•™è¾“å…¥çš„è¯­ä¹‰å†…å®¹ï¼ŒåŒæ—¶å¼ºè°ƒå¯¹ä¸‹æ¸¸ä»»åŠ¡å…·æœ‰åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚æœ€è¿‘çš„æ–¹æ³•è¡¨æ˜ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ è½¬åŒ–ä¸ºæœ‰ç«äº‰åŠ›çš„åµŒå…¥æ¨¡å‹ï¼Œä»è€ŒåŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªäº’è¡¥çš„ç›®æ ‡ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸¤ä¸ªç›®æ ‡å¯ä»¥è§£è€¦ï¼šå¯¹è¾“å…¥çš„å…¨é¢ç†è§£æœ‰åŠ©äºåµŒå…¥æ¨¡å‹é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è·å¾—å“è¶Šçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CoMaï¼Œä¸€ä¸ªå‹ç¼©çš„é¢„è®­ç»ƒé˜¶æ®µï¼Œä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„é¢„çƒ­é˜¶æ®µã€‚å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨å°‘é‡é¢„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†è§†è§‰-è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºæœ‰ç«äº‰åŠ›çš„åµŒå…¥æ¨¡å‹ã€‚CoMaåœ¨MMEBä¸Šå®ç°äº†åŒç­‰è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„æœ€æ–°ç»“æœï¼Œå®ç°äº†æ•ˆç‡å’Œæ•ˆæœçš„ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å­¦ä¹ å¤šæ¨¡æ€åµŒå…¥æ—¶ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„å¯¹æ¯”å­¦ä¹ æ¥åŒæ—¶ä¼˜åŒ–è¯­ä¹‰ç†è§£å’Œç‰¹å¾åŒºåˆ†ä¸¤ä¸ªç›®æ ‡ã€‚è¿™ç§è”åˆä¼˜åŒ–æ–¹å¼æ•ˆç‡è¾ƒä½ï¼Œä¸”å¯èƒ½å¯¼è‡´æ¨¡å‹éš¾ä»¥å¹³è¡¡ä¸¤ä¸ªç›®æ ‡ã€‚å› æ­¤ï¼Œå¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨VLMè¿›è¡Œå¤šæ¨¡æ€åµŒå…¥å­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†VLMçš„å¤šæ¨¡æ€åµŒå…¥å­¦ä¹ è¿‡ç¨‹è§£è€¦ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œä¸€ä¸ªå‹ç¼©çš„é¢„è®­ç»ƒé˜¶æ®µï¼ˆCoMaï¼‰ï¼Œä¸“æ³¨äºå­¦ä¹ å¯¹è¾“å…¥æ•°æ®çš„å…¨é¢è¯­ä¹‰ç†è§£ï¼›ç„¶åï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¥è¿›ä¸€æ­¥ä¼˜åŒ–åµŒå…¥ï¼Œä½¿å…¶æ›´å…·åŒºåˆ†æ€§ã€‚è¿™ç§è§£è€¦çš„æ–¹å¼å…è®¸æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸“æ³¨äºå­¦ä¹ é€šç”¨çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œä¸ºåç»­çš„å¯¹æ¯”å­¦ä¹ æä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoMaçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šå‹ç¼©é¢„è®­ç»ƒé˜¶æ®µå’Œå¯¹æ¯”å­¦ä¹ é˜¶æ®µã€‚åœ¨å‹ç¼©é¢„è®­ç»ƒé˜¶æ®µï¼ŒVLMé€šè¿‡ä¸€ä¸ªè‡ªç›‘ç£çš„å­¦ä¹ ç›®æ ‡ï¼ˆå…·ä½“ç›®æ ‡æœªçŸ¥ï¼‰è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨å­¦ä¹ è¾“å…¥æ•°æ®çš„å…¨é¢è¯­ä¹‰è¡¨ç¤ºã€‚åœ¨å¯¹æ¯”å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨å¤§è§„æ¨¡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆå…·ä½“æ–¹æ³•æœªçŸ¥ï¼‰æ¥è¿›ä¸€æ­¥ä¼˜åŒ–åµŒå…¥ï¼Œä½¿å…¶æ›´å…·åŒºåˆ†æ€§ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µå¯ä»¥é¡ºåºæ‰§è¡Œï¼Œä¹Ÿå¯ä»¥è¿­ä»£æ‰§è¡Œï¼ˆå…·ä½“ç­–ç•¥æœªçŸ¥ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªè§£è€¦çš„å¤šæ¨¡æ€åµŒå…¥å­¦ä¹ èŒƒå¼ï¼Œå°†è¯­ä¹‰ç†è§£å’Œç‰¹å¾åŒºåˆ†ä¸¤ä¸ªç›®æ ‡åˆ†ç¦»åˆ°ä¸åŒçš„é˜¶æ®µè¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§è§£è€¦çš„æ–¹å¼å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨VLMçš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå¹¶å‡å°‘å¯¹æ¯”å­¦ä¹ æ‰€éœ€çš„æ•°æ®é‡ã€‚æ­¤å¤–ï¼ŒCoMaå¼•å…¥çš„å‹ç¼©é¢„è®­ç»ƒé˜¶æ®µï¼Œå¯ä»¥ä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„æœ‰æ•ˆé¢„çƒ­ï¼Œä»è€ŒåŠ é€Ÿæ¨¡å‹çš„æ”¶æ•›å¹¶æå‡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³äºCoMaçš„å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œè®ºæ–‡æ‘˜è¦ä¸­å¹¶æœªè¯¦ç»†è¯´æ˜ã€‚ä¾‹å¦‚ï¼Œå‹ç¼©é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨çš„è‡ªç›‘ç£å­¦ä¹ ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿå¯¹æ¯”å­¦ä¹ é˜¶æ®µé‡‡ç”¨çš„å…·ä½“æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•å¹³è¡¡ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Ÿè¿™äº›ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚æ­¤å¤–ï¼Œå…³äºç½‘ç»œç»“æ„ã€æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ç­‰æ–¹é¢çš„å…·ä½“è®¾è®¡ï¼Œä¹Ÿéœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥äº†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoMaåœ¨MMEBåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†åŒç­‰è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„æœ€ä½³ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šçš„ä¼˜è¶Šæ€§ã€‚è¯¥æ–¹æ³•ä»…éœ€å°‘é‡é¢„è®­ç»ƒæ•°æ®ï¼Œå³å¯å°†VLMè½¬åŒ–ä¸ºå…·æœ‰ç«äº‰åŠ›çš„åµŒå…¥æ¨¡å‹ï¼Œè¡¨æ˜å…¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€å›¾åƒ/è§†é¢‘åˆ†ç±»ã€å¤šæ¨¡æ€æ•°æ®èšç±»ç­‰é¢†åŸŸã€‚é€šè¿‡æ›´é«˜æ•ˆåœ°å­¦ä¹ å¤šæ¨¡æ€åµŒå…¥ï¼Œå¯ä»¥æå‡ç›¸å…³åº”ç”¨çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¾‹å¦‚ï¼Œåœ¨ç”µå•†é¢†åŸŸï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¿›è¡Œå•†å“æ¨èï¼›åœ¨æ™ºèƒ½å®‰é˜²é¢†åŸŸï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œè§†é¢‘ç›‘æ§å’Œåˆ†æã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºé™ä½å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒçš„æˆæœ¬ï¼Œä¿ƒè¿›å…¶åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.

