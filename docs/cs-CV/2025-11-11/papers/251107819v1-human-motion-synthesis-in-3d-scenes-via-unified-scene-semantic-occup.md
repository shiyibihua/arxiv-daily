---
layout: default
title: Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy
---

# Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy

**arXiv**: [2511.07819v1](https://arxiv.org/abs/2511.07819) | [PDF](https://arxiv.org/pdf/2511.07819.pdf)

**ä½œè€…**: Gong Jingyu, Tong Kunkun, Chen Zhuoran, Yuan Chuanhan, Chen Mingang, Zhang Zhizhong, Tan Xin, Xie Yuan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSSOMotionæ¡†æž¶ï¼Œåˆ©ç”¨ç»Ÿä¸€åœºæ™¯è¯­ä¹‰å æ®å®žçŽ°3Dåœºæ™¯ä¸­äººä½“è¿åŠ¨åˆæˆã€‚**

**å…³é”®è¯**: `äººä½“è¿åŠ¨åˆæˆ` `3Dåœºæ™¯ç†è§£` `åœºæ™¯è¯­ä¹‰å æ®` `CLIPç¼–ç ` `åŒå‘ä¸‰å¹³é¢åˆ†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–åœºæ™¯ç»“æž„ï¼Œä½†å¿½ç•¥è¯­ä¹‰ç†è§£ï¼Œå½±å“è¿åŠ¨åˆæˆå‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒå‘ä¸‰å¹³é¢åˆ†è§£åŽ‹ç¼©åœºæ™¯è¯­ä¹‰å æ®ï¼Œç»“åˆCLIPç¼–ç æ˜ å°„åˆ°ç»Ÿä¸€ç‰¹å¾ç©ºé—´ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ShapeNetã€PROXå’ŒReplicaæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ€§èƒ½é¢†å…ˆä¸”æ³›åŒ–èƒ½åŠ›å¼ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.

