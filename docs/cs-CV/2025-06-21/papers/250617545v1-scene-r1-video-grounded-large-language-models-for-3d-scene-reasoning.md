---
layout: default
title: Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations
---

# Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17545" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17545v1</a>
  <a href="https://arxiv.org/pdf/2506.17545.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17545v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17545v1', 'Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihao Yuan, Shuyi Jiang, Chun-Mei Feng, Yaolun Zhang, Shuguang Cui, Zhen Li, Na Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºScene-R1ä»¥è§£å†³æ— 3Dæ ‡æ³¨çš„3Dåœºæ™¯æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯æ¨ç†` `è§†é¢‘ç†è§£` `å¼ºåŒ–å­¦ä¹ ` `æ— ç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `è§†è§‰é—®ç­”` `æ¨¡å‹é€æ˜æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹é€æ˜æ€§ï¼Œæ— æ³•æ­ç¤ºå†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä¾èµ–äºé¢„è®­ç»ƒçš„3Dæ£€æµ‹å™¨æä¾›ç‰©ä½“æè®®ã€‚
2. æœ¬æ–‡æå‡ºScene-R1æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æ¨ç†ä¸ä¸¤é˜¶æ®µçš„åŸºç¡€å®šä½ç®¡é“ï¼Œå­¦ä¹ åœ¨æ— 3Dæ ‡æ³¨çš„æƒ…å†µä¸‹è¿›è¡Œ3Dåœºæ™¯æ¨ç†ã€‚
3. Scene-R1åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰åŸºçº¿ï¼Œæä¾›é€æ˜çš„æ¨ç†è¿‡ç¨‹ï¼Œå±•ç¤ºäº†å…¶åœ¨3Dè§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›®å‰ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£3Dä¸–ç•Œçš„ç ”ç©¶é€æ¸å¢å¤šã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€ä½œä¸ºé»‘ç®±è¿ä½œï¼Œè¾“å‡ºè¾¹ç•Œæ¡†æˆ–æ–‡æœ¬ç­”æ¡ˆï¼Œå´æ— æ³•æ­ç¤ºå†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä¸”ä¾èµ–äºé¢„è®­ç»ƒçš„3Dæ£€æµ‹å™¨æä¾›ç‰©ä½“æè®®ã€‚æœ¬æ–‡æå‡ºäº†Scene-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªè§†é¢‘é©±åŠ¨çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æ¨ç†ä¸ä¸¤é˜¶æ®µçš„åŸºç¡€å®šä½ç®¡é“ï¼Œå­¦ä¹ åœ¨æ²¡æœ‰ç‚¹ä½3Då®ä¾‹ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œ3Dåœºæ™¯æ¨ç†ã€‚åœ¨æ—¶é—´åŸºç¡€å®šä½é˜¶æ®µï¼Œæ˜ç¡®æ¨ç†è§†é¢‘å¹¶é€‰æ‹©ä¸å¼€æ”¾å¼æŸ¥è¯¢æœ€ç›¸å…³çš„è§†é¢‘ç‰‡æ®µã€‚åœ¨åç»­çš„å›¾åƒåŸºç¡€å®šä½é˜¶æ®µï¼Œåˆ†æå›¾åƒå¹¶é¢„æµ‹2Dè¾¹ç•Œæ¡†ã€‚æœ€ç»ˆï¼Œé€šè¿‡SAM2è·Ÿè¸ªå¯¹è±¡ï¼Œç”ŸæˆRGBå¸§ä¸­çš„åƒç´ ç²¾ç¡®æ©ç ï¼Œå¹¶å°†å…¶æŠ•å½±å›3Dï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹åŸºäº3Dæ£€æµ‹å™¨çš„æè®®çš„éœ€æ±‚ï¼ŒåŒæ—¶æ•æ‰ç»†è‡´çš„å‡ ä½•å’Œææ–™çº¿ç´¢ã€‚Scene-R1è¿˜å¯ä»¥é€‚åº”3Dè§†è§‰é—®ç­”ä»»åŠ¡ï¼Œç›´æ¥å›ç­”æ¥è‡ªè§†é¢‘çš„è‡ªç”±å½¢å¼é—®é¢˜ã€‚æˆ‘ä»¬çš„è®­ç»ƒç®¡é“åªéœ€è¦ä»»åŠ¡çº§çš„2Dæ¡†æˆ–æ–‡æœ¬æ ‡ç­¾ï¼Œè€Œæ— éœ€å¯†é›†çš„3Dç‚¹ä½æ ‡ç­¾ã€‚Scene-R1åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰çš„å¼€æ”¾è¯æ±‡åŸºçº¿ï¼ŒåŒæ—¶æä¾›é€æ˜çš„é€æ­¥æ¨ç†ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†ç»“åˆRGB-Dè§†é¢‘æä¾›äº†ä¸€æ¡å®ç”¨çš„ã€æ³¨é‡Šé«˜æ•ˆçš„å¯ä¿¡3Dåœºæ™¯ç†è§£è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Dæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸é€æ˜æ€§åŠå¯¹3Dæ£€æµ‹å™¨çš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æä¾›æ¸…æ™°çš„å†³ç­–ä¾æ®ï¼Œå¹¶ä¸”éœ€è¦å¯†é›†çš„3Dæ ‡æ³¨æ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šScene-R1é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ä¸¤é˜¶æ®µçš„åŸºç¡€å®šä½ç®¡é“ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç‚¹ä½3Då®ä¾‹ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œ3Dåœºæ™¯æ¨ç†ã€‚è¯¥è®¾è®¡æ—¨åœ¨æé«˜æ¨ç†çš„é€æ˜åº¦å’Œæ•ˆç‡ï¼ŒåŒæ—¶å‡å°‘å¯¹å¤æ‚æ ‡æ³¨çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šScene-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ—¶é—´åŸºç¡€å®šä½é˜¶æ®µå’Œå›¾åƒåŸºç¡€å®šä½é˜¶æ®µã€‚åœ¨æ—¶é—´åŸºç¡€å®šä½é˜¶æ®µï¼Œç³»ç»Ÿåˆ†æè§†é¢‘å¹¶é€‰æ‹©ä¸æŸ¥è¯¢ç›¸å…³çš„ç‰‡æ®µï¼›åœ¨å›¾åƒåŸºç¡€å®šä½é˜¶æ®µï¼Œç³»ç»Ÿåˆ†æé€‰å®šçš„å›¾åƒå¹¶é¢„æµ‹2Dè¾¹ç•Œæ¡†ã€‚

**å…³é”®åˆ›æ–°**ï¼šScene-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— é¡»ä¾èµ–3Dæ£€æµ‹å™¨çš„ç‰©ä½“æè®®ï¼Œèƒ½å¤Ÿç›´æ¥ä»RGB-Dè§†é¢‘ä¸­æå–ä¿¡æ¯ï¼Œæ•æ‰ç»†è‡´çš„å‡ ä½•å’Œææ–™çº¿ç´¢ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒScene-R1ä»…éœ€ä»»åŠ¡çº§çš„2Dæ¡†æˆ–æ–‡æœ¬æ ‡ç­¾ï¼Œè€Œä¸éœ€è¦å¯†é›†çš„3Dç‚¹ä½æ ‡ç­¾ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œè¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒScene-R1è¶…è¶Šäº†ç°æœ‰çš„å¼€æ”¾è¯æ±‡åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨3Dåœºæ™¯æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†é€æ˜åº¦å’Œå‡†ç¡®æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œæä¾›äº†é€æ­¥çš„æ¨ç†è¿‡ç¨‹ï¼Œå¢å¼ºäº†ç”¨æˆ·å¯¹æ¨¡å‹å†³ç­–çš„ä¿¡ä»»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Scene-R1çš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰åœºæ™¯ç†è§£ä»»åŠ¡ã€‚é€šè¿‡æä¾›é«˜æ•ˆä¸”é€æ˜çš„3Dåœºæ™¯æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºæ™ºèƒ½ç³»ç»Ÿæä¾›æ›´å¯é çš„ç¯å¢ƒç†è§£ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Currently, utilizing large language models to understand the 3D world is becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output bounding boxes or textual answers without revealing how those decisions are made, and they still rely on pre-trained 3D detectors to supply object proposals. We introduce Scene-R1, a video-grounded framework that learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline. In the temporal grounding stage, we explicitly reason about the video and select the video snippets most relevant to an open-ended query. In the subsequent image grounding stage, we analyze the image and predict the 2D bounding box. After that, we track the object using SAM2 to produce pixel-accurate masks in RGB frames, and project them back into 3D, thereby eliminating the need for 3D detector-based proposals while capturing fine geometry and material cues. Scene-R1 can also adapt to the 3D visual question answering task to answer free-form questions directly from video. Our training pipeline only needs task-level 2D boxes or textual labels without dense 3D point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales. These results show that reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding.

