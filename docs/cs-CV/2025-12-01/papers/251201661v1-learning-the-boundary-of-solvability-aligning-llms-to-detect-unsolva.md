---
layout: default
title: Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems
---

# Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems

**arXiv**: [2512.01661v1](https://arxiv.org/abs/2512.01661) | [PDF](https://arxiv.org/pdf/2512.01661.pdf)

**ä½œè€…**: Dengyun Peng, Qiguang Chen, Bofei Liu, Jiannan Guan, Libo Qin, Zheng Yan, Jinhao Liu, Jianshu Zhang, Wanxiang Che

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUnsolvableQAå’ŒUnsolvableRLä»¥è§£å†³LLMåœ¨æ£€æµ‹ä¸å¯è§£é—®é¢˜æ—¶çš„å¯é æ€§é—®é¢˜**

**å…³é”®è¯**: `ä¸å¯è§£é—®é¢˜æ£€æµ‹` `LLMå¯é æ€§` `å¼ºåŒ–å­¦ä¹ æ¡†æž¶` `æ•°æ®é›†æž„å»º` `èƒ½åŠ›å´©æºƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMéš¾ä»¥åŒºåˆ†å®¢è§‚ä¸å¯è§£ï¼ˆé—®é¢˜å†…åœ¨çŸ›ç›¾ï¼‰ä¸Žä¸»è§‚èƒ½åŠ›é™åˆ¶ï¼ˆæ¨¡åž‹èƒ½åŠ›ä¸è¶³ï¼‰ï¼Œå¯¼è‡´å¹»è§‰å’Œè¿‡åº¦è‡ªä¿¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºUnsolvableQAæ•°æ®é›†ï¼ŒåŒ…å«å¯è§£å’Œä¸å¯è§£å®žä¾‹ï¼Œå¹¶åŸºäºŽæ­¤è®¾è®¡UnsolvableRLå¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œç»“åˆå‡†ç¡®æ€§ã€ä¸å¯è§£æ€§å’Œéš¾åº¦å¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ–¹æ³•å®žçŽ°è¿‘ä¹Žå®Œç¾Žçš„ä¸å¯è§£æ£€æµ‹ï¼ŒåŒæ—¶æå‡å¯è§£ä»»åŠ¡å‡†ç¡®æ€§ï¼Œå¹¶å‘çŽ°èƒ½åŠ›å´©æºƒçŽ°è±¡ï¼Œå¼ºè°ƒæš´éœ²ä¸å¯è§£æ•°æ®çš„å¿…è¦æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

