---
layout: default
title: Benchmarking Overton Pluralism in LLMs
---

# Benchmarking Overton Pluralism in LLMs

**arXiv**: [2512.01351v1](https://arxiv.org/abs/2512.01351) | [PDF](https://arxiv.org/pdf/2512.01351.pdf)

**‰ΩúËÄÖ**: Elinor Poole-Dayan, Jiayi Wu, Taylor Sorensen, Jiaxin Pei, Michiel A. Bakker

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫OvertonScoreÊ°ÜÊû∂‰ª•ËØÑ‰º∞Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËßÇÁÇπÂ§öÊ†∑ÊÄß**

**ÂÖ≥ÈîÆËØç**: `ËßÇÁÇπÂ§öÊ†∑ÊÄßËØÑ‰º∞` `Â§ßËØ≠Ë®ÄÊ®°ÂûãÂü∫ÂáÜ` `ÈõÜÂêàË¶ÜÁõñÂ∫¶Èáè` `‰∫∫Á±ªÁ†îÁ©∂` `Ëá™Âä®ÂåñËØÑ‰º∞` `Ê®°ÂûãÂØπÈΩê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÂ¶Ç‰ΩïÈáèÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãËæìÂá∫‰∏≠ÁöÑËßÇÁÇπÂ§öÊ†∑ÊÄßÔºàOverton pluralismÔºâ„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÂÆö‰πâOvertonScore‰Ωú‰∏∫ÈõÜÂêàË¶ÜÁõñÂ∫¶ÈáèÔºåÂπ∂ÂºÄÂèëËá™Âä®ÂåñÂü∫ÂáÜ‰ª•Â§çÁé∞‰∫∫Á±ªÂà§Êñ≠„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®8‰∏™LLMs‰∏äËøõË°åÂ§ßËßÑÊ®°‰∫∫Á±ªÁ†îÁ©∂ÔºåÊ®°ÂûãÂπ≥ÂùáÂæóÂàÜ0.35-0.41ÔºåËá™Âä®ÂåñÂü∫ÂáÜ‰∏é‰∫∫Á±ªÂà§Êñ≠È´òÂ∫¶Áõ∏ÂÖ≥ÔºàœÅ=0.88Ôºâ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($œÅ=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.

