---
layout: default
title: Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators
---

# Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators

**arXiv**: [2512.01370v1](https://arxiv.org/abs/2512.01370) | [PDF](https://arxiv.org/pdf/2512.01370.pdf)

**ä½œè€…**: Medha Sawhney, Abhilash Neog, Mridul Khurana, Anuj Karpatne

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRISMAæ–¹æ³•ï¼Œé€šè¿‡è°±åŸŸæ³¨æ„åŠ›åµŒå…¥PDEæ®‹å·®ï¼Œå®žçŽ°æ— æ¢¯åº¦ä¼˜åŒ–çš„å¿«é€Ÿæ‰©æ•£ç¥žç»ç®—å­æ±‚è§£åå¾®åˆ†æ–¹ç¨‹ã€‚**

**å…³é”®è¯**: `åå¾®åˆ†æ–¹ç¨‹æ±‚è§£` `æ‰©æ•£æ¨¡åž‹` `ç¥žç»ç®—å­` `è°±åŸŸæ³¨æ„åŠ›` `æ— æ¢¯åº¦ä¼˜åŒ–` `å™ªå£°é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸåŸºäºŽæ‰©æ•£çš„PDEæ±‚è§£å™¨ä¾èµ–æ¢¯åº¦ä¼˜åŒ–ï¼Œå¯¼è‡´æŽ¨ç†æ…¢ã€ä¸ç¨³å®šä¸”å¯¹å™ªå£°æ•æ„Ÿã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šPRISMAå°†PDEæ®‹å·®ä½œä¸ºè°±åŸŸæ³¨æ„åŠ›æœºåˆ¶ç›´æŽ¥é›†æˆåˆ°æ¨¡åž‹æž¶æž„ä¸­ï¼Œé¿å…å¤–éƒ¨æŸå¤±å¼•å¯¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªåŸºå‡†PDEä¸Šï¼ŒPRISMAä»¥æ›´å°‘åŽ»å™ªæ­¥éª¤å®žçŽ°ç«žäº‰æ€§ç²¾åº¦ï¼ŒæŽ¨ç†é€Ÿåº¦æå‡15å€è‡³250å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. To address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-descent free inference. In contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA has competitive accuracy, at substantially lower inference costs, compared to previous methods across five benchmark PDEs, especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.

