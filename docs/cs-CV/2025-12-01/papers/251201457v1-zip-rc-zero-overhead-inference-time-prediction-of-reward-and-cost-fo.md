---
layout: default
title: ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation
---

# ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation

**arXiv**: [2512.01457v1](https://arxiv.org/abs/2512.01457) | [PDF](https://arxiv.org/pdf/2512.01457.pdf)

**ä½œè€…**: Rohin Manvi, Joey Hong, Tim Seyde, Maxime Labonne, Mathias Lechner, Sergey Levine

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZIP-RCæ–¹æ³•ï¼Œé€šè¿‡é›¶å¼€é”€æŽ¨ç†æ—¶é¢„æµ‹å¥–åŠ±ä¸Žæˆæœ¬ï¼Œå®žçŽ°è‡ªé€‚åº”é«˜æ•ˆç”Ÿæˆ**

**å…³é”®è¯**: `è‡ªé€‚åº”æŽ¨ç†` `é›¶å¼€é”€é¢„æµ‹` `å¥–åŠ±æˆæœ¬å»ºæ¨¡` `å¤§è¯­è¨€æ¨¡åž‹è‡ªçœ` `æŽ¨ç†æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹ç¼ºä¹å®žæ—¶è‡ªçœèƒ½åŠ›ï¼Œæ— æ³•é¢„æµ‹è‡ªèº«æˆåŠŸæ¦‚çŽ‡ä¸Žè®¡ç®—éœ€æ±‚ï¼Œå¯¼è‡´æŽ¨ç†æ•ˆçŽ‡ä½Žä¸‹å’Œä¿¡ä»»é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æŽ¨ç†æ—¶å¤ç”¨æœªä½¿ç”¨çš„logitsï¼Œåœ¨åŒä¸€å‰å‘ä¼ é€’ä¸­é¢„æµ‹æœ€ç»ˆå¥–åŠ±å’Œå‰©ä½™é•¿åº¦ï¼Œæ— éœ€é¢å¤–æ¨¡åž‹æˆ–å¼€é”€ï¼Œå®žçŽ°è‡ªé€‚åº”é‡‡æ ·å†³ç­–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ··åˆéš¾åº¦æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”å¤šæ•°æŠ•ç¥¨ï¼ŒZIP-RCåœ¨ç›¸åŒæˆ–æ›´ä½Žå¹³å‡æˆæœ¬ä¸‹æå‡å‡†ç¡®çŽ‡é«˜è¾¾12%ï¼Œå¹¶ä¼˜åŒ–è´¨é‡ã€è®¡ç®—å’Œå»¶è¿Ÿçš„æƒè¡¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.

