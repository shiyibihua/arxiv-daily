---
layout: default
title: On the Unreasonable Effectiveness of Last-layer Retraining
---

# On the Unreasonable Effectiveness of Last-layer Retraining

**arXiv**: [2512.01766v1](https://arxiv.org/abs/2512.01766) | [PDF](https://arxiv.org/pdf/2512.01766.pdf)

**ä½œè€…**: John C. Hill, Tyler LaBonte, Xinchen Zhang, Vidya Muthukumar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºæœ€åŽä¸€å±‚é‡è®­ç»ƒæœ‰æ•ˆæ€§çš„åŽŸå› ï¼Œå¼ºè°ƒä¿ç•™é›†ç»„å¹³è¡¡çš„å…³é”®ä½œç”¨**

**å…³é”®è¯**: `æœ€åŽä¸€å±‚é‡è®­ç»ƒ` `è™šå‡ç›¸å…³æ€§` `ç»„å¹³è¡¡` `é²æ£’æ€§` `æ¢¯åº¦ä¸‹é™éšå¼åå·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶æœ€åŽä¸€å±‚é‡è®­ç»ƒåœ¨ç¼“è§£è™šå‡ç›¸å…³æ€§å’Œæå‡å°‘æ•°ç¾¤ä½“æ€§èƒ½ä¸­çš„é«˜æ•ˆæ€§
2. é€šè¿‡å®žéªŒå¦å®šç¥žç»å´©æºƒç¼“è§£å‡è¯´ï¼Œè¯æ˜Žä¿ç•™é›†ç»„å¹³è¡¡æ˜¯ä¸»è¦é©±åŠ¨å› ç´ 
3. å±•ç¤ºCB-LLRå’ŒAFRç®—æ³•é€šè¿‡éšå¼ç»„å¹³è¡¡å®žçŽ°é²æ£’æ€§æ”¹è¿›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Last-layer retraining (LLR) methods -- wherein the last layer of a neural network is reinitialized and retrained on a held-out set following ERM training -- have garnered interest as an efficient approach to rectify dependence on spurious correlations and improve performance on minority groups. Surprisingly, LLR has been found to improve worst-group accuracy even when the held-out set is an imbalanced subset of the training set. We initially hypothesize that this ``unreasonable effectiveness'' of LLR is explained by its ability to mitigate neural collapse through the held-out set, resulting in the implicit bias of gradient descent benefiting robustness. Our empirical investigation does not support this hypothesis. Instead, we present strong evidence for an alternative hypothesis: that the success of LLR is primarily due to better group balance in the held-out set. We conclude by showing how the recent algorithms CB-LLR and AFR perform implicit group-balancing to elicit a robustness improvement.

