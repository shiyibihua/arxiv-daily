---
layout: default
title: Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe
---

# Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe

**arXiv**: [2512.01252v1](https://arxiv.org/abs/2512.01252) | [PDF](https://arxiv.org/pdf/2512.01252.pdf)

**ä½œè€…**: Yahui Liu, Yang Yue, Jingyuan Zhang, Chenxi Sun, Yang Zhou, Wencong Zeng, Ruiming Tang, Guorui Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆè®­ç»ƒæ‰©æ•£ä¸“å®¶æ··åˆæ¨¡åž‹çš„æž¶æž„é…ç½®æ–¹æ¡ˆï¼Œä»¥æå‡æ€§èƒ½å¹¶å‡å°‘æ¿€æ´»å‚æ•°ã€‚**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `ä¸“å®¶æ··åˆ` `æž¶æž„ä¼˜åŒ–` `é«˜æ•ˆè®­ç»ƒ` `å‚æ•°æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£ä¸“å®¶æ··åˆæ¨¡åž‹çš„æž¶æž„é…ç½®ç©ºé—´æœªå……åˆ†æŽ¢ç´¢ï¼Œå½±å“æ€§èƒ½ä¼˜åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå€Ÿé‰´å¤§è¯­è¨€æ¨¡åž‹è®¾è®¡ï¼Œç³»ç»Ÿç ”ç©¶ä¸“å®¶æ¨¡å—ã€å®½åº¦ã€æ•°é‡å’Œä½ç½®ç¼–ç ç­‰å…³é”®å› ç´ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ–°æž¶æž„åœ¨æ½œç©ºé—´å’Œåƒç´ ç©ºé—´æ‰©æ•£æ¡†æž¶ä¸­é«˜æ•ˆåº”ç”¨ï¼Œè¶…è¶ŠåŸºçº¿ä¸”å‚æ•°æ›´å°‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: https://github.com/yhlleo/EfficientMoE.

