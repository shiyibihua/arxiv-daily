---
layout: default
title: Provably Safe Model Updates
---

# Provably Safe Model Updates

**arXiv**: [2512.01899v1](https://arxiv.org/abs/2512.01899) | [PDF](https://arxiv.org/pdf/2512.01899.pdf)

**ä½œè€…**: Leo Elmecker-Plakolm, Pierre Fasterling, Philip Sosnin, Calvin Tsay, Matthew Wicker

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è¯æ˜Žå®‰å…¨æ¨¡åž‹æ›´æ–°æ¡†æž¶ï¼Œä»¥è§£å†³åŠ¨æ€çŽ¯å¢ƒä¸­æ¨¡åž‹æ›´æ–°å¯èƒ½è¿åæ€§èƒ½è§„èŒƒçš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ¨¡åž‹æ›´æ–°å®‰å…¨` `å±€éƒ¨ä¸å˜åŸŸ` `å½¢å¼åŒ–è®¤è¯` `æŒç»­å­¦ä¹ ` `åŸºç¡€æ¨¡åž‹å¾®è°ƒ` `å‚æ•°åŒ–æŠ½è±¡åŸŸ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡åž‹æ›´æ–°å¯èƒ½å¯¼è‡´ç¾éš¾æ€§é—å¿˜æˆ–å¯¹é½æ¼‚ç§»ï¼ŒçŽ°æœ‰å¯å‘å¼æ–¹æ³•æ— æ³•æä¾›å½¢å¼åŒ–å®‰å…¨ä¿è¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†é—®é¢˜å½¢å¼åŒ–ä¸ºè®¡ç®—æœ€å¤§å±€éƒ¨ä¸å˜åŸŸï¼Œé€šè¿‡å‚æ•°åŒ–æŠ½è±¡åŸŸå®žçŽ°é«˜æ•ˆè®¤è¯ï¼Œæ”¯æŒæŠ•å½±æ›´æ–°åˆ°å®‰å…¨åŸŸã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æŒç»­å­¦ä¹ å’ŒåŸºç¡€æ¨¡åž‹å¾®è°ƒåŸºå‡†ä¸Šï¼ŒåŒ¹é…æˆ–è¶…è¶Šå¯å‘å¼åŸºçº¿ï¼ŒåŒæ—¶æä¾›å½¢å¼åŒ–å®‰å…¨ä¿è¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Safety-critical environments are inherently dynamic. Distribution shifts, emerging vulnerabilities, and evolving requirements demand continuous updates to machine learning models. Yet even benign parameter updates can have unintended consequences, such as catastrophic forgetting in classical models or alignment drift in foundation models. Existing heuristic approaches (e.g., regularization, parameter isolation) can mitigate these effects but cannot certify that updated models continue to satisfy required performance specifications. We address this problem by introducing a framework for provably safe model updates. Our approach first formalizes the problem as computing the largest locally invariant domain (LID): a connected region in parameter space where all points are certified to satisfy a given specification. While exact maximal LID computation is intractable, we show that relaxing the problem to parameterized abstract domains (orthotopes, zonotopes) yields a tractable primal-dual formulation. This enables efficient certification of updates - independent of the data or algorithm used - by projecting them onto the safe domain. Our formulation further allows computation of multiple approximately optimal LIDs, incorporation of regularization-inspired biases, and use of lookahead data buffers. Across continual learning and foundation model fine-tuning benchmarks, our method matches or exceeds heuristic baselines for avoiding forgetting while providing formal safety guarantees.

