---
layout: default
title: Know Thyself by Knowing Others: Learning Neuron Identity from Population Context
---

# Know Thyself by Knowing Others: Learning Neuron Identity from Population Context

**arXiv**: [2512.01199v1](https://arxiv.org/abs/2512.01199) | [PDF](https://arxiv.org/pdf/2512.01199.pdf)

**ä½œè€…**: Vinam Arora, Divyansha Lachi, Ian J. Knight, Mehdi Azabou, Blake Richards, Cole L. Hurwitz, Josh Siegle, Eva L. Dyer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNuCLRæ¡†æž¶ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä»Žç¥žç»æ´»åŠ¨ä¸­æŽ¨æ–­ç¥žç»å…ƒèº«ä»½ä¿¡æ¯ã€‚**

**å…³é”®è¯**: `ç¥žç»å…ƒèº«ä»½è§£ç ` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `æ—¶ç©ºTransformer` `é›¶æ ·æœ¬æ³›åŒ–` `ç¥žç»è¡¨ç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žç¥žç»æ´»åŠ¨æŽ¨æ–­ç¥žç»å…ƒç±»åž‹ã€è¿žæŽ¥å’Œè„‘åŒºç­‰èº«ä»½ä¿¡æ¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ•´åˆä¸åŒæ—¶é—´å’Œåˆºæ¿€ä¸‹çš„ç¥žç»å…ƒè§†å›¾ï¼Œå¹¶æž„å»ºç½®æ¢ç­‰å˜æ—¶ç©ºTransformerã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®žçŽ°ç»†èƒžç±»åž‹å’Œè„‘åŒºè§£ç çš„æ–°SOTAï¼Œå¹¶å±•ç¤ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Neurons process information in ways that depend on their cell type, connectivity, and the brain region in which they are embedded. However, inferring these factors from neural activity remains a significant challenge. To build general-purpose representations that allow for resolving information about a neuron's identity, we introduce NuCLR, a self-supervised framework that aims to learn representations of neural activity that allow for differentiating one neuron from the rest. NuCLR brings together views of the same neuron observed at different times and across different stimuli and uses a contrastive objective to pull these representations together. To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner. Across multiple electrophysiology and calcium imaging datasets, a linear decoding evaluation on top of NuCLR representations achieves a new state-of-the-art for both cell type and brain region decoding tasks, and demonstrates strong zero-shot generalization to unseen animals. We present the first systematic scaling analysis for neuron-level representation learning, showing that increasing the number of animals used during pretraining consistently improves downstream performance. The learned representations are also label-efficient, requiring only a small fraction of labeled samples to achieve competitive performance. These results highlight how large, diverse neural datasets enable models to recover information about neuron identity that generalize across animals. Code is available at https://github.com/nerdslab/nuclr.

