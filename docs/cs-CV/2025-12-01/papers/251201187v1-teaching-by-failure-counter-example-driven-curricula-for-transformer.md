---
layout: default
title: Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement
---

# Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement

**arXiv**: [2512.01187v1](https://arxiv.org/abs/2512.01187) | [PDF](https://arxiv.org/pdf/2512.01187.pdf)

**ä½œè€…**: Harshil Vejendla

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåä¾‹é©±åŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æž¶ï¼Œä»¥æå‡Transformeræ¨¡åž‹åœ¨å¤æ‚è¾“å…¥ä¸Šçš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `Transformeræ¨¡åž‹` `è¯¾ç¨‹å­¦ä¹ ` `åä¾‹é©±åŠ¨` `é²æ£’æ€§æå‡` `é•¿åº¦å¤–æŽ¨` `è‡ªåŠ¨éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeræ¨¡åž‹åœ¨è®­ç»ƒæ•°æ®å¤–çš„é•¿æˆ–å¤æ‚è¾“å…¥ä¸Šæ³›åŒ–èƒ½åŠ›å¼±ã€‚
2. CEDCé€šè¿‡è¿­ä»£ç”Ÿæˆåä¾‹å¹¶å¾®è°ƒæ¨¡åž‹ï¼Œè‡ªåŠ¨èšç„¦äºŽå¤±è´¥æ¡ˆä¾‹ã€‚
3. å®žéªŒæ˜¾ç¤ºCEDCåœ¨ç®—æ³•å’Œè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡é•¿åº¦å¤–æŽ¨å’Œè®¡ç®—æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.

