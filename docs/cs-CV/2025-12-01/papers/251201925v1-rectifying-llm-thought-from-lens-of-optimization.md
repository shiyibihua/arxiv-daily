---
layout: default
title: Rectifying LLM Thought from Lens of Optimization
---

# Rectifying LLM Thought from Lens of Optimization

**arXiv**: [2512.01925v1](https://arxiv.org/abs/2512.01925) | [PDF](https://arxiv.org/pdf/2512.01925.pdf)

**ä½œè€…**: Junnan Liu, Hongwei Liu, Songyang Zhang, Kai Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReProæ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–è§†è§’ä¿®æ­£å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†è¿‡ç¨‹ï¼Œæå‡æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `é“¾å¼æ€ç»´æŽ¨ç†` `è¿‡ç¨‹çº§å¥–åŠ±` `å¼ºåŒ–å­¦ä¹ ` `ä¼˜åŒ–è§†è§’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿é“¾æ€ç»´æŽ¨ç†æ¨¡åž‹å­˜åœ¨è¿‡åº¦æ€è€ƒå’Œå†—é•¿æŽ¨ç†é“¾ç­‰æ¬¡ä¼˜è¡Œä¸ºï¼Œå½±å“æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†é“¾å¼æ€ç»´è§†ä¸ºæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ï¼Œå®šä¹‰è¿‡ç¨‹çº§å¥–åŠ±å‡½æ•°ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReProèƒ½ä¸€è‡´æå‡æŽ¨ç†æ€§èƒ½å¹¶ç¼“è§£æ¬¡ä¼˜è¡Œä¸ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.

