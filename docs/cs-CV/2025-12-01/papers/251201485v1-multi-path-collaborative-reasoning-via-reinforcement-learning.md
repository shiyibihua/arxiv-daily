---
layout: default
title: Multi-Path Collaborative Reasoning via Reinforcement Learning
---

# Multi-Path Collaborative Reasoning via Reinforcement Learning

**arXiv**: [2512.01485v1](https://arxiv.org/abs/2512.01485) | [PDF](https://arxiv.org/pdf/2512.01485.pdf)

**ä½œè€…**: Jindi Lv, Yuhao Zhou, Zheng Zhu, Xiaofeng Wang, Guan Huang, Jiancheng Lv

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3POå¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œé€šè¿‡å¤šè·¯å¾„åä½œæŽ¨ç†è§£å†³é“¾å¼æ€ç»´è§£ç çš„ç¡®å®šæ€§é™åˆ¶ã€‚**

**å…³é”®è¯**: `é“¾å¼æ€ç»´æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šè·¯å¾„åä½œ` `è§£ç ä¼˜åŒ–` `æŽ¨ç†å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿé“¾å¼æ€ç»´æŽ¨ç†åœ¨è§£ç æ—¶å­˜åœ¨å†…éƒ¨ç¡®å®šæ€§ï¼Œé™åˆ¶äº†æ›¿ä»£æŽ¨ç†è·¯å¾„çš„æŽ¢ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¹¶è¡Œç­–ç•¥ä½œä¸ºå¤šæ ·æŽ¨ç†æºï¼Œé€šè¿‡è½»é‡åä½œæœºåˆ¶æ•´åˆè·¨è·¯å¾„äº¤äº’ä»¥ä¼˜åŒ–æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çŸ¥è¯†å’ŒæŽ¨ç†å¯†é›†åž‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œä¿æŒå¯è§£é‡Šæ€§å’ŒæŽ¨ç†æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.

