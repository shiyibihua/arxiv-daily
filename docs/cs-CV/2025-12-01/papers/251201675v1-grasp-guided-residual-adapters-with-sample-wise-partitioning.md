---
layout: default
title: GRASP: Guided Residual Adapters with Sample-wise Partitioning
---

# GRASP: Guided Residual Adapters with Sample-wise Partitioning

**arXiv**: [2512.01675v1](https://arxiv.org/abs/2512.01675) | [PDF](https://arxiv.org/pdf/2512.01675.pdf)

**ä½œè€…**: Felix NÃ¼tzel, Mischa Dombrowski, Bernhard Kainz

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRASPæ–¹æ³•ï¼Œé€šè¿‡æ ·æœ¬åˆ†åŒºå’Œæ®‹å·®é€‚é…å™¨è§£å†³é•¿å°¾æ•°æ®ä¸‹æ‰©æ•£æ¨¡åž‹çš„æ¢¯åº¦å†²çªé—®é¢˜ã€‚**

**å…³é”®è¯**: `é•¿å°¾å­¦ä¹ ` `æ‰©æ•£æ¨¡åž‹` `æ®‹å·®é€‚é…å™¨` `æ¢¯åº¦å†²çª` `åŒ»å­¦å›¾åƒç”Ÿæˆ` `æ ·æœ¬åˆ†åŒº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿å°¾æ•°æ®ä¸­ç½•è§ç±»åˆ«å¯¼è‡´æ‰©æ•£æ¨¡åž‹æ¢¯åº¦å†²çªï¼Œè¾“å‡ºè´¨é‡ä¸Žå¤šæ ·æ€§ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¤–éƒ¨å…ˆéªŒé™æ€åˆ†åŒºæ ·æœ¬ï¼Œæ³¨å…¥é›†ç¾¤ç‰¹å®šæ®‹å·®é€‚é…å™¨è¿›è¡Œå¾®è°ƒï¼Œé¿å…æ¢¯åº¦å†²çªã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MIMIC-CXR-LTç­‰æ•°æ®é›†ä¸Šï¼ŒGRASPåœ¨FIDå’Œå¤šæ ·æ€§æŒ‡æ ‡ä¸Šä¼˜äºŽåŸºçº¿ï¼Œæå‡ç½•è§ç±»åˆ«æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.

