---
layout: default
title: AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation
---

# AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation

**arXiv**: [2512.01334v1](https://arxiv.org/abs/2512.01334) | [PDF](https://arxiv.org/pdf/2512.01334.pdf)

**ä½œè€…**: Yexin Liu, Wen-Jie Shu, Zile Huang, Haoze Zheng, Yueze Wang, Manyuan Zhang, Ser-Nam Lim, Harry Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAlignVidæ¡†æž¶ä»¥è§£å†³æ–‡æœ¬å¼•å¯¼å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„è¯­ä¹‰ç–å¿½é—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬å¼•å¯¼å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ` `è¯­ä¹‰ç–å¿½` `æ³¨æ„åŠ›ç¼©æ”¾` `æ— è®­ç»ƒæ¡†æž¶` `è¯­ä¹‰ä¿çœŸåº¦è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨è¾“å…¥å›¾åƒéœ€å¤§å¹…å˜æ¢æ—¶ï¼Œéš¾ä»¥éµå¾ªç»†ç²’åº¦æç¤ºè¯­ä¹‰ï¼Œç§°ä¸ºè¯­ä¹‰ç–å¿½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ— è®­ç»ƒæ¡†æž¶ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›ç¼©æ”¾è°ƒåˆ¶å’Œå¼•å¯¼è°ƒåº¦ï¼Œç›´æŽ¥é‡åŠ æƒæ³¨æ„åŠ›ä»¥æå‡è¯­ä¹‰ä¿çœŸåº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¼•å…¥OmitI2Vè¯„ä¼°é›†ï¼Œå®žéªŒæ˜¾ç¤ºAlignVidèƒ½å¢žå¼ºè¯­ä¹‰ä¿çœŸåº¦ï¼ŒåŒæ—¶é™åˆ¶è§†è§‰è´¨é‡ä¸‹é™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.

