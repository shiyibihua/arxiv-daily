---
layout: default
title: Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling
---

# Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling

**arXiv**: [2512.02010v1](https://arxiv.org/abs/2512.02010) | [PDF](https://arxiv.org/pdf/2512.02010.pdf)

**ä½œè€…**: Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4/6è‡ªé€‚åº”å—ç¼©æ”¾æ–¹æ³•ä»¥æå‡NVFP4é‡åŒ–ç²¾åº¦ï¼Œè§£å†³è®­ç»ƒå‘æ•£ä¸ŽæŽ¨ç†æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚**

**å…³é”®è¯**: `NVFP4é‡åŒ–` `è‡ªé€‚åº”å—ç¼©æ”¾` `ä½Žç²¾åº¦è®­ç»ƒ` `å¤§è¯­è¨€æ¨¡åž‹` `æµ®ç‚¹æ ¼å¼ä¼˜åŒ–` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. NVFP4é‡åŒ–åœ¨è®­ç»ƒå’ŒæŽ¨ç†ä¸­æ˜“å¯¼è‡´å‘æ•£ä¸Žæ€§èƒ½ä¸‹é™ï¼Œä¸»è¦æºäºŽå—å†…è¿‘æœ€å¤§å€¼é‡åŒ–è¯¯å·®ã€‚
2. 4/6æ–¹æ³•ä¸ºæ¯ä¸ªå€¼å—è¯„ä¼°ä¸¤ä¸ªç¼©æ”¾å› å­ï¼Œä½¿å¯è¡¨ç¤ºå€¼åˆ†å¸ƒæ›´å‡åŒ€ï¼Œæ”¹å–„è¿‘æœ€å¤§å€¼è¡¨ç¤ºã€‚
3. å®žéªŒè¡¨æ˜Ž4/6åœ¨é¢„è®­ç»ƒä¸­é˜²æ­¢å‘æ•£ï¼Œæå‡ä¸‹æ¸¸ç²¾åº¦ï¼Œå¹¶å…¼å®¹NVIDIA Blackwell GPUé«˜æ•ˆå®žçŽ°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.

