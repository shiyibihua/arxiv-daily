---
layout: default
title: RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving
---

# RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving

**arXiv**: [2512.01300v1](https://arxiv.org/abs/2512.01300) | [PDF](https://arxiv.org/pdf/2512.01300.pdf)

**ä½œè€…**: Dacheng Liao, Mengshi Qi, Peng Shu, Zhining Zhang, Yuxin Lin, Liang Liu, Huadong Ma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboDriveVLMæ¡†æž¶ä¸ŽRoboDriveBenchåŸºå‡†ï¼Œä»¥å¢žå¼ºè‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹çš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `é²æ£’æ€§åŸºå‡†` `å¤šæ¨¡æ€èžåˆ` `æµ‹è¯•æ—¶é€‚åº”` `è½¨è¿¹é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLMè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨çœŸå®žåœºæ™¯ä¸­é¢ä¸´ä¼ æ„Ÿå™¨å’Œæç¤ºæŸåç­‰é£Žé™©ï¼Œé²æ£’æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šæ¨¡æ€æ•°æ®æ˜ å°„å’Œè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ï¼Œæå‡ç³»ç»Ÿé²æ£’æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŒ…å«11ç§æ¨¡æ‹Ÿåœºæ™¯çš„åŸºå‡†ä¸Šè¯„ä¼°ï¼Œæä¾›æ›´å¯é çš„è‡ªåŠ¨é©¾é©¶è§£å†³æ–¹æ¡ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

