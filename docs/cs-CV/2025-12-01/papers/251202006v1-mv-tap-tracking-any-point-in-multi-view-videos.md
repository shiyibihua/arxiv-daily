---
layout: default
title: MV-TAP: Tracking Any Point in Multi-View Videos
---

# MV-TAP: Tracking Any Point in Multi-View Videos

**arXiv**: [2512.02006v1](https://arxiv.org/abs/2512.02006) | [PDF](https://arxiv.org/pdf/2512.02006.pdf)

**ä½œè€…**: Jahyeok Koo, InÃ¨s Hyeonsu Kim, Mungyeom Kim, Junghyun Park, Seohyun Park, Jaeyeong Kim, Jung Yi, Seokju Cho, Seungryong Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMV-TAPä»¥è§£å†³å¤šè§†è§’è§†é¢‘ä¸­åŠ¨æ€åœºæ™¯ç‚¹è·Ÿè¸ªé—®é¢˜**

**å…³é”®è¯**: `å¤šè§†è§’è§†é¢‘` `ç‚¹è·Ÿè¸ª` `è·¨è§†è§’æ³¨æ„åŠ›` `ç›¸æœºå‡ ä½•` `åˆæˆæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šè§†è§’è§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡çš„ç‚¹è·Ÿè¸ªï¼Œéœ€è·¨è§†è§’æ•´åˆä¿¡æ¯ä»¥æé«˜è½¨è¿¹å®Œæ•´æ€§å’Œå¯é æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨ç›¸æœºå‡ ä½•å’Œè·¨è§†è§’æ³¨æ„åŠ›æœºåˆ¶èšåˆæ—¶ç©ºä¿¡æ¯ï¼Œå¢žå¼ºè·Ÿè¸ªæ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºå¤§è§„æ¨¡åˆæˆè®­ç»ƒæ•°æ®é›†å’ŒçœŸå®žè¯„ä¼°é›†ï¼Œåœ¨æŒ‘æˆ˜æ€§åŸºå‡†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.

