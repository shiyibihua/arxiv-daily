---
layout: default
title: S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
---

# S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance

**arXiv**: [2512.01223v1](https://arxiv.org/abs/2512.01223) | [PDF](https://arxiv.org/pdf/2512.01223.pdf)

**ä½œè€…**: Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS^2-MLLMæ¡†æž¶ï¼Œé€šè¿‡éšå¼ç©ºé—´æŽ¨ç†å¢žå¼ºMLLMsåœ¨3Dè§†è§‰å®šä½ä¸­çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç©ºé—´æŽ¨ç†` `ç»“æž„å¼•å¯¼` `æ³¨æ„åŠ›æœºåˆ¶` `ä½ç½®ç¼–ç `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMså¤„ç†2Dè§†è§‰è¾“å…¥ï¼Œéš¾ä»¥ä»Žæœ‰é™è§†è§’ç†è§£3Dåœºæ™¯ç©ºé—´ç»“æž„ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–ç‚¹äº‘é‡å»ºå¯¼è‡´æ•ˆçŽ‡ä½Žä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ç©ºé—´å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æž„æ„ŸçŸ¥ï¼Œé€šè¿‡ç»“æž„å¢žå¼ºæ¨¡å—ç»“åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šçº§ä½ç½®ç¼–ç æå‡ç©ºé—´æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒS^2-MLLMåœ¨æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆçŽ‡æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

