---
layout: default
title: S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
---

# S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.01223" target="_blank" class="toolbar-btn">arXiv: 2512.01223v1</a>
    <a href="https://arxiv.org/pdf/2512.01223.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.01223v1" 
            onclick="toggleFavorite(this, '2512.01223v1', 'S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-01

**Â§áÊ≥®**: 18 pages, 9 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**S$^2$-MLLMÔºöÈÄöËøáÁªìÊûÑÂºïÂØºÂ¢ûÂº∫MLLMÂú®3DËßÜËßâÂÆö‰Ωç‰∏≠ÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `3DËßÜËßâÂÆö‰Ωç` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Á©∫Èó¥Êé®ÁêÜ` `ÁªìÊûÑÂºïÂØº` `ÂÖ∑Ë∫´Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DËßÜËßâÂÆö‰ΩçÊñπÊ≥ï‰æùËµñ‰∫éÁÇπ‰∫ëÈáçÂª∫ÂíåËßÜËßíÊ∏≤ÊüìÔºåÊïàÁéá‰Ωé‰∏îÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÊúâÈôêÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®MLLMÁöÑÊΩúÂäõ„ÄÇ
2. S$^2$-MLLMÈÄöËøáÂºïÂÖ•Á©∫Èó¥ÂºïÂØºÁ≠ñÁï•ÔºåÂà©Áî®ÂâçÈ¶à3DÈáçÂª∫ÁöÑÁªìÊûÑÊÑüÁü•ËÉΩÂäõÔºå‰ΩøMLLMËÉΩÂ§üÈöêÂºèÂú∞ËøõË°å3DÁ©∫Èó¥Êé®ÁêÜ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåS$^2$-MLLMÂú®ScanRefer„ÄÅNr3DÂíåSr3DÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÅÊ≥õÂåñÊÄßÂíåÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

3DËßÜËßâÂÆö‰Ωç(3DVG)Êó®Âú®Ê†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Âú®3DÂú∫ÊôØ‰∏≠ÂÆö‰ΩçÁâ©‰ΩìÔºåÊòØÂÖ∑Ë∫´Êô∫ËÉΩÂíåÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂü∫Á°Ä‰ªªÂä°„ÄÇÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)ÁöÑÊúÄÊñ∞ËøõÂ±ïÊé®Âä®‰∫ÜÂ∞ÜÂÖ∂Êâ©Â±ïÂà∞3DVGÁöÑÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåMLLM‰∏ªË¶ÅÂ§ÑÁêÜ2DËßÜËßâËæìÂÖ•ÔºåÈöæ‰ª•‰ªÖ‰ªéËøô‰∫õÊúâÈôêÁöÑËßÜËßíÁêÜËß£Âú∫ÊôØÁöÑ3DÁ©∫Èó¥ÁªìÊûÑ„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂà©Áî®ÈáçÂª∫ÁÇπ‰∫ëÁöÑËßÜËßíÁõ∏ÂÖ≥Ê∏≤ÊüìÔºå‰∏∫MLLMÂú®3DVG‰ªªÂä°‰∏≠Êèê‰æõÊòæÂºèÁªìÊûÑÂºïÂØºÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÂíåÁ©∫Èó¥Êé®ÁêÜÂèóÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜS$^2$-MLLMÔºå‰∏Ä‰∏™ÈÄöËøáÈöêÂºèÁ©∫Èó¥Êé®ÁêÜÂ¢ûÂº∫MLLMÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÈ´òÊïàÊ°ÜÊû∂„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁ©∫Èó¥ÂºïÂØºÁ≠ñÁï•ÔºåÂà©Áî®ÂâçÈ¶à3DÈáçÂª∫ÁöÑÁªìÊûÑÊÑüÁü•ËÉΩÂäõ„ÄÇÈÄöËøáÂú®ËÆ≠ÁªÉÊúüÈó¥Ëé∑Âæó3DÁªìÊûÑÁêÜËß£ÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂèØ‰ª•ÈöêÂºèÂú∞Êé®ÁêÜ3DÂú∫ÊôØÔºåËÄåÊó†ÈúÄ‰æùËµñ‰ΩéÊïàÁöÑÁÇπ‰∫ëÈáçÂª∫„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂ¢ûÂº∫Ê®°Âùó(SE)ÔºåËØ•Ê®°ÂùóÈ¶ñÂÖàÈááÁî®ËßÜÂõæÂÜÖÂíåËßÜÂõæÈó¥Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÊçïËé∑ËßÜÂõæÂÜÖÁöÑ‰æùËµñÂÖ≥Á≥ªÂíåËßÜÂõæÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇËØ•Ê®°ÂùóËøõ‰∏ÄÊ≠•ÈõÜÊàê‰∫ÜÂ§öÁ∫ß‰ΩçÁΩÆÁºñÁ†ÅÔºåÂ∞ÜËßÜËßâË°®Á§∫‰∏éÁ©∫Èó¥‰ΩçÁΩÆÂíåËßÜÁÇπ‰ø°ÊÅØÁõ∏ÂÖ≥ËÅîÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑÁªìÊûÑÁêÜËß£„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåS$^2$-MLLMÁªü‰∏Ä‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÅÊ≥õÂåñÊÄßÂíåÊïàÁéáÔºåÂú®ScanRefer„ÄÅNr3DÂíåSr3DÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÁöÑÊòæËëóÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**Ôºö3DËßÜËßâÂÆö‰Ωç‰ªªÂä°Êó®Âú®Ê†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Âú®3DÂú∫ÊôØ‰∏≠ÂÆö‰ΩçÁõÆÊ†áÁâ©‰Ωì„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫é‰ªéÂ§ö‰∏™ËßÜËßíÊ∏≤ÊüìÁöÑ3DÁÇπ‰∫ëÔºå‰∏∫Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)Êèê‰æõÊòæÂºèÁöÑÁªìÊûÑ‰ø°ÊÅØ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÂèóÈôê‰∫éÁÇπ‰∫ëÈáçÂª∫ÁöÑË¥®ÈáèÔºåÂØºËá¥Á©∫Èó¥Êé®ÁêÜËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöS$^2$-MLLMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÈöêÂºèÁöÑÊñπÂºèÂºïÂØºMLLMÂ≠¶‰π†3DÂú∫ÊôØÁöÑÁªìÊûÑ‰ø°ÊÅØÔºåÈÅøÂÖçÊòæÂºèÁöÑÁÇπ‰∫ëÈáçÂª∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂà©Áî®ÂâçÈ¶à3DÈáçÂª∫ÁöÑÁªìÊûÑÊÑüÁü•ËÉΩÂäõÔºå‰ΩøÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠¶‰π†Âà∞3DÂú∫ÊôØÁöÑÁªìÊûÑ‰ø°ÊÅØÔºå‰ªéËÄåÂú®Êé®ÁêÜÈò∂ÊÆµËÉΩÂ§üÈöêÂºèÂú∞ËøõË°åÁ©∫Èó¥Êé®ÁêÜ„ÄÇËøôÊ†∑ÂèØ‰ª•ÊèêÈ´òÊïàÁéáÔºåÂπ∂Â¢ûÂº∫Ê®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöS$^2$-MLLMÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ËßÜËßâÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºöÁî®‰∫é‰ªéÂ§öËßÜËßíÂõæÂÉè‰∏≠ÊèêÂèñËßÜËßâÁâπÂæÅ„ÄÇ2) Á©∫Èó¥ÂºïÂØºÊ®°ÂùóÔºöÂà©Áî®ÂâçÈ¶à3DÈáçÂª∫ÁöÑÁªìÊûÑÊÑüÁü•ËÉΩÂäõÔºå‰∏∫MLLMÊèê‰æõÁ©∫Èó¥ÂºïÂØº„ÄÇ3) ÁªìÊûÑÂ¢ûÂº∫Ê®°Âùó(SE)ÔºöÈÄöËøáËßÜÂõæÂÜÖÂíåËßÜÂõæÈó¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊçïËé∑ËßÜÂõæÂÜÖÁöÑ‰æùËµñÂÖ≥Á≥ªÂíåËßÜÂõæÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåÂπ∂ÁªìÂêàÂ§öÁ∫ß‰ΩçÁΩÆÁºñÁ†ÅÔºåÂ∞ÜËßÜËßâË°®Á§∫‰∏éÁ©∫Èó¥‰ΩçÁΩÆÂíåËßÜÁÇπ‰ø°ÊÅØÁõ∏ÂÖ≥ËÅî„ÄÇ4) Â§öÊ®°ÊÄÅËûçÂêàÊ®°ÂùóÔºöÂ∞ÜËßÜËßâÁâπÂæÅ„ÄÅÁ©∫Èó¥ÂºïÂØºÂíåËØ≠Ë®ÄÁâπÂæÅËøõË°åËûçÂêàÔºåÁî®‰∫éÊúÄÁªàÁöÑ3DÁõÆÊ†áÂÆö‰Ωç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöS$^2$-MLLMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÈöêÂºèÁöÑÁ©∫Èó¥Êé®ÁêÜÊñπÊ≥ïÔºåÈÅøÂÖç‰∫ÜÊòæÂºèÁöÑÁÇπ‰∫ëÈáçÂª∫Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊïàÁéáÂíåÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÁªìÊûÑÂ¢ûÂº∫Ê®°Âùó(SE)ÈÄöËøáËßÜÂõæÂÜÖÂíåËßÜÂõæÈó¥Ê≥®ÊÑèÂäõÊú∫Âà∂‰ª•ÂèäÂ§öÁ∫ß‰ΩçÁΩÆÁºñÁ†ÅÔºåÊõ¥ÊúâÊïàÂú∞Âà©Áî®‰∫ÜÂ§öËßÜËßí‰ø°ÊÅØÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÁªìÊûÑÂ¢ûÂº∫Ê®°Âùó(SE)ÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏Ä„ÄÇÂÆÉÂåÖÂê´intra-view attentionÂíåinter-view attention‰∏§‰∏™ÈÉ®ÂàÜÔºåÂàÜÂà´Áî®‰∫éÊçïËé∑ËßÜÂõæÂÜÖÁöÑ‰æùËµñÂÖ≥Á≥ªÂíåËßÜÂõæÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÂ§öÁ∫ß‰ΩçÁΩÆÁºñÁ†ÅÂ∞ÜËßÜËßâÁâπÂæÅ‰∏éÁ©∫Èó¥‰ΩçÁΩÆÂíåËßÜÁÇπ‰ø°ÊÅØÂÖ≥ËÅîËµ∑Êù•Ôºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÁªìÊûÑÊÑüÁü•ËÉΩÂäõ„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢ÔºåÂèØËÉΩÈááÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÊàñÁ±ª‰ººÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éËÆ≠ÁªÉÊ®°ÂûãËøõË°å3DÁõÆÊ†áÂÆö‰Ωç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

S$^2$-MLLMÂú®ScanRefer„ÄÅNr3DÂíåSr3DÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ScanReferÊï∞ÊçÆÈõÜ‰∏äÔºåS$^2$-MLLMÁöÑÊÄßËÉΩË∂ÖËøá‰∫ÜÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåS$^2$-MLLMÂÖ∑ÊúâÂçìË∂äÁöÑÊÄßËÉΩ„ÄÅÊ≥õÂåñÊÄßÂíåÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

S$^2$-MLLMÂú®ÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁöÑ‰∏âÁª¥ÁéØÂ¢É‰∏≠ÁêÜËß£Ëá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåÂπ∂ÂáÜÁ°ÆÂú∞ÂÆö‰ΩçÁõÆÊ†áÁâ©‰ΩìÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

