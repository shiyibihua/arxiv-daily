---
layout: default
title: S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
---

# S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance

**arXiv**: [2512.01223v1](https://arxiv.org/abs/2512.01223) | [PDF](https://arxiv.org/pdf/2512.01223.pdf)

**ä½œè€…**: Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-01

**å¤‡æ³¨**: 18 pages, 9 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**S$^2$-MLLMï¼šé€šè¿‡ç»“æž„å¼•å¯¼å¢žå¼ºMLLMåœ¨3Dè§†è§‰å®šä½ä¸­çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç©ºé—´æŽ¨ç†` `ç»“æž„å¼•å¯¼` `å…·èº«æ™ºèƒ½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•ä¾èµ–äºŽç‚¹äº‘é‡å»ºå’Œè§†è§’æ¸²æŸ“ï¼Œæ•ˆçŽ‡ä½Žä¸”ç©ºé—´æŽ¨ç†èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨MLLMçš„æ½œåŠ›ã€‚
2. S$^2$-MLLMé€šè¿‡å¼•å…¥ç©ºé—´å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æž„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿MLLMèƒ½å¤Ÿéšå¼åœ°è¿›è¡Œ3Dç©ºé—´æŽ¨ç†ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒS$^2$-MLLMåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå±•çŽ°äº†å“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆçŽ‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dè§†è§‰å®šä½(3DVG)æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°åœ¨3Dåœºæ™¯ä¸­å®šä½ç‰©ä½“ï¼Œæ˜¯å…·èº«æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„åŸºç¡€ä»»åŠ¡ã€‚å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹(MLLM)çš„æœ€æ–°è¿›å±•æŽ¨åŠ¨äº†å°†å…¶æ‰©å±•åˆ°3DVGçš„ç ”ç©¶ã€‚ç„¶è€Œï¼ŒMLLMä¸»è¦å¤„ç†2Dè§†è§‰è¾“å…¥ï¼Œéš¾ä»¥ä»…ä»Žè¿™äº›æœ‰é™çš„è§†è§’ç†è§£åœºæ™¯çš„3Dç©ºé—´ç»“æž„ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨é‡å»ºç‚¹äº‘çš„è§†è§’ç›¸å…³æ¸²æŸ“ï¼Œä¸ºMLLMåœ¨3DVGä»»åŠ¡ä¸­æä¾›æ˜¾å¼ç»“æž„å¼•å¯¼ï¼Œå¯¼è‡´æ•ˆçŽ‡ä½Žä¸‹å’Œç©ºé—´æŽ¨ç†å—é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†S$^2$-MLLMï¼Œä¸€ä¸ªé€šè¿‡éšå¼ç©ºé—´æŽ¨ç†å¢žå¼ºMLLMç©ºé—´æŽ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ¡†æž¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç©ºé—´å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æž„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡åœ¨è®­ç»ƒæœŸé—´èŽ·å¾—3Dç»“æž„ç†è§£ï¼Œæˆ‘ä»¬çš„æ¨¡åž‹å¯ä»¥éšå¼åœ°æŽ¨ç†3Dåœºæ™¯ï¼Œè€Œæ— éœ€ä¾èµ–ä½Žæ•ˆçš„ç‚¹äº‘é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“æž„å¢žå¼ºæ¨¡å—(SE)ï¼Œè¯¥æ¨¡å—é¦–å…ˆé‡‡ç”¨è§†å›¾å†…å’Œè§†å›¾é—´æ³¨æ„åŠ›æœºåˆ¶æ¥æ•èŽ·è§†å›¾å†…çš„ä¾èµ–å…³ç³»å’Œè§†å›¾é—´çš„å¯¹åº”å…³ç³»ã€‚è¯¥æ¨¡å—è¿›ä¸€æ­¥é›†æˆäº†å¤šçº§ä½ç½®ç¼–ç ï¼Œå°†è§†è§‰è¡¨ç¤ºä¸Žç©ºé—´ä½ç½®å’Œè§†ç‚¹ä¿¡æ¯ç›¸å…³è”ï¼Œä»Žè€Œå®žçŽ°æ›´å‡†ç¡®çš„ç»“æž„ç†è§£ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒS$^2$-MLLMç»Ÿä¸€äº†å“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆçŽ‡ï¼Œåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸Šå®žçŽ°äº†ä¼˜äºŽçŽ°æœ‰æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼š3Dè§†è§‰å®šä½ä»»åŠ¡æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°åœ¨3Dåœºæ™¯ä¸­å®šä½ç›®æ ‡ç‰©ä½“ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºŽä»Žå¤šä¸ªè§†è§’æ¸²æŸ“çš„3Dç‚¹äº‘ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹(MLLM)æä¾›æ˜¾å¼çš„ç»“æž„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å—é™äºŽç‚¹äº‘é‡å»ºçš„è´¨é‡ï¼Œå¯¼è‡´ç©ºé—´æŽ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šS$^2$-MLLMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡éšå¼çš„æ–¹å¼å¼•å¯¼MLLMå­¦ä¹ 3Dåœºæ™¯çš„ç»“æž„ä¿¡æ¯ï¼Œé¿å…æ˜¾å¼çš„ç‚¹äº‘é‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æž„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åˆ°3Dåœºæ™¯çš„ç»“æž„ä¿¡æ¯ï¼Œä»Žè€Œåœ¨æŽ¨ç†é˜¶æ®µèƒ½å¤Ÿéšå¼åœ°è¿›è¡Œç©ºé—´æŽ¨ç†ã€‚è¿™æ ·å¯ä»¥æé«˜æ•ˆçŽ‡ï¼Œå¹¶å¢žå¼ºæ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šS$^2$-MLLMæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰ç‰¹å¾æå–æ¨¡å—ï¼šç”¨äºŽä»Žå¤šè§†è§’å›¾åƒä¸­æå–è§†è§‰ç‰¹å¾ã€‚2) ç©ºé—´å¼•å¯¼æ¨¡å—ï¼šåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æž„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºMLLMæä¾›ç©ºé—´å¼•å¯¼ã€‚3) ç»“æž„å¢žå¼ºæ¨¡å—(SE)ï¼šé€šè¿‡è§†å›¾å†…å’Œè§†å›¾é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•èŽ·è§†å›¾å†…çš„ä¾èµ–å…³ç³»å’Œè§†å›¾é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶ç»“åˆå¤šçº§ä½ç½®ç¼–ç ï¼Œå°†è§†è§‰è¡¨ç¤ºä¸Žç©ºé—´ä½ç½®å’Œè§†ç‚¹ä¿¡æ¯ç›¸å…³è”ã€‚4) å¤šæ¨¡æ€èžåˆæ¨¡å—ï¼šå°†è§†è§‰ç‰¹å¾ã€ç©ºé—´å¼•å¯¼å’Œè¯­è¨€ç‰¹å¾è¿›è¡Œèžåˆï¼Œç”¨äºŽæœ€ç»ˆçš„3Dç›®æ ‡å®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šS$^2$-MLLMçš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†éšå¼çš„ç©ºé—´æŽ¨ç†æ–¹æ³•ï¼Œé¿å…äº†æ˜¾å¼çš„ç‚¹äº‘é‡å»ºï¼Œä»Žè€Œæé«˜äº†æ•ˆçŽ‡å’Œç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç»“æž„å¢žå¼ºæ¨¡å—(SE)é€šè¿‡è§†å›¾å†…å’Œè§†å›¾é—´æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠå¤šçº§ä½ç½®ç¼–ç ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨äº†å¤šè§†è§’ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡åž‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šç»“æž„å¢žå¼ºæ¨¡å—(SE)æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚å®ƒåŒ…å«intra-view attentionå’Œinter-view attentionä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«ç”¨äºŽæ•èŽ·è§†å›¾å†…çš„ä¾èµ–å…³ç³»å’Œè§†å›¾é—´çš„å¯¹åº”å…³ç³»ã€‚å¤šçº§ä½ç½®ç¼–ç å°†è§†è§‰ç‰¹å¾ä¸Žç©ºé—´ä½ç½®å’Œè§†ç‚¹ä¿¡æ¯å…³è”èµ·æ¥ï¼Œä»Žè€Œå¢žå¼ºäº†æ¨¡åž‹çš„ç»“æž„æ„ŸçŸ¥èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯èƒ½é‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±æˆ–ç±»ä¼¼çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºŽè®­ç»ƒæ¨¡åž‹è¿›è¡Œ3Dç›®æ ‡å®šä½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

S$^2$-MLLMåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ScanReferæ•°æ®é›†ä¸Šï¼ŒS$^2$-MLLMçš„æ€§èƒ½è¶…è¿‡äº†çŽ°æœ‰æœ€ä½³æ–¹æ³•ï¼Œå®žçŽ°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒS$^2$-MLLMå…·æœ‰å“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

S$^2$-MLLMåœ¨å…·èº«æ™ºèƒ½ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚çš„ä¸‰ç»´çŽ¯å¢ƒä¸­ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶å‡†ç¡®åœ°å®šä½ç›®æ ‡ç‰©ä½“ï¼Œä»Žè€Œå®žçŽ°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

