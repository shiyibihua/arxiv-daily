---
layout: default
title: A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns
---

# A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns

**arXiv**: [2512.01412v1](https://arxiv.org/abs/2512.01412) | [PDF](https://arxiv.org/pdf/2512.01412.pdf)

**ä½œè€…**: Ziqian Wang, Yuxiao Cheng, Jinli Suo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEXCAPæ¡†æž¶ä»¥è§£å†³é•¿æ—¶åºå»ºæ¨¡ä¸­è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡æå–ç»“æž„åŒ–å› æžœæ¨¡å¼å¢žå¼ºå¯è§£é‡Šæ€§ã€‚**

**å…³é”®è¯**: `é•¿æ—¶åºå»ºæ¨¡` `å¯è§£é‡ŠAI` `å› æžœæ¨¡å¼æå–` `æ³¨æ„åŠ›æœºåˆ¶` `é¢„æµ‹å‡†ç¡®æ€§` `é«˜é£Žé™©åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¯è§£é‡ŠAIæ–¹æ³•ä»…ç”Ÿæˆç‚¹çŠ¶é‡è¦æ€§åˆ†æ•°ï¼Œæ— æ³•æ•æ‰è¶‹åŠ¿ã€å‘¨æœŸç­‰æ—¶åºç»“æž„ï¼Œå‰Šå¼±é•¿æ—¶åºæ¨¡åž‹çš„ä¿¡ä»»åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ³¨æ„åŠ›åˆ†å‰²å™¨æå–è¿žè´¯æ—¶åºæ¨¡å¼ã€å› æžœå›¾å¼•å¯¼çš„è§£ç å™¨å’Œæ½œåœ¨èšåˆæœºåˆ¶ï¼Œæ»¡è¶³è¿žç»­æ€§ã€æ¨¡å¼ä¸­å¿ƒã€å› æžœè§£è€¦å’Œå¿ å®žæ€§è¦æ±‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆ†ç±»å’Œé¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEXCAPä¿æŒé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼ŒåŒæ—¶ç”Ÿæˆè¿žè´¯ä¸”å› æžœåŸºç¡€çš„è§£é‡Šï¼Œé€‚ç”¨äºŽåŒ»ç–—å’Œé‡‘èžç­‰é«˜é£Žé™©é¢†åŸŸã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Explainability is essential for neural networks that model long time series, yet most existing explainable AI methods only produce point-wise importance scores and fail to capture temporal structures such as trends, cycles, and regime changes. This limitation weakens human interpretability and trust in long-horizon models. To address these issues, we identify four key requirements for interpretable time-series modeling: temporal continuity, pattern-centric explanation, causal disentanglement, and faithfulness to the model's inference process. We propose EXCAP, a unified framework that satisfies all four requirements. EXCAP combines an attention-based segmenter that extracts coherent temporal patterns, a causally structured decoder guided by a pre-trained causal graph, and a latent aggregation mechanism that enforces representation stability. Our theoretical analysis shows that EXCAP provides smooth and stable explanations over time and is robust to perturbations in causal masks. Extensive experiments on classification and forecasting benchmarks demonstrate that EXCAP achieves strong predictive accuracy while generating coherent and causally grounded explanations. These results show that EXCAP offers a principled and scalable approach to interpretable modeling of long time series with relevance to high-stakes domains such as healthcare and finance.

