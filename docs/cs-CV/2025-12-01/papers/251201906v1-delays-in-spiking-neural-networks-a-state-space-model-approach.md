---
layout: default
title: Delays in Spiking Neural Networks: A State Space Model Approach
---

# Delays in Spiking Neural Networks: A State Space Model Approach

**arXiv**: [2512.01906v1](https://arxiv.org/abs/2512.01906) | [PDF](https://arxiv.org/pdf/2512.01906.pdf)

**ä½œè€…**: Sanja Karilanova, Subhrakanti Dey, AyÃ§a Ã–zÃ§elikkale

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçŠ¶æ€ç©ºé—´æ¨¡åž‹çš„å»¶è¿Ÿæœºåˆ¶ï¼Œä»¥å¢žå¼ºè„‰å†²ç¥žç»ç½‘ç»œå¤„ç†æ—¶åºæ•°æ®çš„èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è„‰å†²ç¥žç»ç½‘ç»œ` `å»¶è¿Ÿæœºåˆ¶` `çŠ¶æ€ç©ºé—´æ¨¡åž‹` `æ—¶åºæ•°æ®å¤„ç†` `ç¥žç»å½¢æ€ç¡¬ä»¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè„‰å†²ç¥žç»ç½‘ç»œä¸­å»¶è¿Ÿæœºåˆ¶å¯¹æ•èŽ·å¤æ‚æ—¶åºä¾èµ–è‡³å…³é‡è¦ï¼Œä½†çŽ°æœ‰æ–¹æ³•å¯èƒ½è®¡ç®—æ•ˆçŽ‡ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é¢å¤–çŠ¶æ€å˜é‡å¼•å…¥å»¶è¿Ÿï¼Œä½¿ç¥žç»å…ƒèƒ½è®¿é—®æœ‰é™è¾“å…¥åŽ†å²ï¼Œå…¼å®¹LIFç­‰æ ‡å‡†æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SHDæ•°æ®é›†ä¸ŠåŒ¹é…çŽ°æœ‰å»¶è¿ŸSNNæ€§èƒ½ï¼Œè®¡ç®—é«˜æ•ˆï¼Œä¸”åœ¨å°ç½‘ç»œä¸­æ˜¾è‘—æå‡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spiking neural networks (SNNs) are biologically inspired, event-driven models that are suitable for processing temporal data and offer energy-efficient computation when implemented on neuromorphic hardware. In SNNs, richer neuronal dynamic allows capturing more complex temporal dependencies, with delays playing a crucial role by allowing past inputs to directly influence present spiking behavior. We propose a general framework for incorporating delays into SNNs through additional state variables. The proposed mechanism enables each neuron to access a finite temporal input history. The framework is agnostic to neuron models and hence can be seamlessly integrated into standard spiking neuron models such as LIF and adLIF. We analyze how the duration of the delays and the learnable parameters associated with them affect the performance. We investigate the trade-offs in the network architecture due to additional state variables introduced by the delay mechanism. Experiments on the Spiking Heidelberg Digits (SHD) dataset show that the proposed mechanism matches the performance of existing delay-based SNNs while remaining computationally efficient. Moreover, the results illustrate that the incorporation of delays may substantially improve performance in smaller networks.

