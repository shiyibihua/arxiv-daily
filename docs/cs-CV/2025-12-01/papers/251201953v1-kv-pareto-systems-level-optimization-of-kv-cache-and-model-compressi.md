---
layout: default
title: KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference
---

# KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference

**arXiv**: [2512.01953v1](https://arxiv.org/abs/2512.01953) | [PDF](https://arxiv.org/pdf/2512.01953.pdf)

**ä½œè€…**: Sai Gokhale, Devleena Das, Rajeev Patwari, Ashish Sirasao, Elliott Delaye

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKV Paretoæ¡†æž¶ï¼Œé€šè¿‡è”åˆä¼˜åŒ–KVç¼“å­˜å’Œæ¨¡åž‹åŽ‹ç¼©ä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæŽ¨ç†ä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡æŽ¨ç†` `KVç¼“å­˜ä¼˜åŒ–` `æ¨¡åž‹åŽ‹ç¼©` `å¸•ç´¯æ‰˜å‰æ²¿` `è¾¹ç¼˜éƒ¨ç½²` `å†…å­˜æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡LLMæŽ¨ç†ä¸­KVç¼“å­˜çº¿æ€§å¢žé•¿å¯¼è‡´å†…å­˜ç“¶é¢ˆï¼ŒçŽ°æœ‰æŠ€æœ¯è”åˆä¼˜åŒ–ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç³»ç»Ÿè¯„ä¼°KVé‡åŒ–ã€åˆ†å—é¢„å¡«å……å’Œæƒé‡é‡åŒ–ï¼Œæž„å»ºå†…å­˜ä¸Žç²¾åº¦çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ¨¡åž‹ä¸Šå®žçŽ°68-78%å†…å­˜å‡å°‘ï¼Œç²¾åº¦æŸå¤±ä»…1-3%ï¼ŒéªŒè¯äºŽé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.

