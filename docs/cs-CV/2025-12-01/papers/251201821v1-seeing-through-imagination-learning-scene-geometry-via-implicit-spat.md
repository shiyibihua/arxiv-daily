---
layout: default
title: Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling
---

# Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling

**arXiv**: [2512.01821v1](https://arxiv.org/abs/2512.01821) | [PDF](https://arxiv.org/pdf/2512.01821.pdf)

**ä½œè€…**: Meng Cao, Haokun Lin, Haoyuan Li, Haoran Tang, Rongtao Xu, Dong An, Xue Liu, Ian Reid, Xiaodan Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMILOéšå¼ç©ºé—´ä¸–ç•Œå»ºæ¨¡èŒƒå¼ï¼Œé€šè¿‡è§†è§‰ç”Ÿæˆå™¨å¢žå¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `ç©ºé—´æŽ¨ç†` `éšå¼å»ºæ¨¡` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å‡ ä½•æ„ŸçŸ¥` `ç›¸å¯¹ä½ç½®ç¼–ç ` `ç”Ÿæˆæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ä¾èµ–æ–‡æœ¬æè¿°å­¦ä¹ ç©ºé—´æ¦‚å¿µï¼Œç¼ºä¹è§†è§‰è¿žæŽ¥ï¼Œå¯¼è‡´ç©ºé—´æŽ¨ç†èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥MILOèŒƒå¼ï¼Œé›†æˆè§†è§‰ç”Ÿæˆå™¨æä¾›å‡ ä½•æ„ŸçŸ¥åé¦ˆï¼Œå¹¶è®¾è®¡RePEç¼–ç æ–¹æ¡ˆæ•æ‰ç›¸å¯¹ç›¸æœºä½å§¿å˜æ¢ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºGeoGenæ•°æ®é›†ï¼Œå®žéªŒæ˜¾ç¤ºæ–¹æ³•æ˜¾è‘—æå‡å¤šä¸ªåŸºçº¿å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºé—´æŽ¨ç†æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.

