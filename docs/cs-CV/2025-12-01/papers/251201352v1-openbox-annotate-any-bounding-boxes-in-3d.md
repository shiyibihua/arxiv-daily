---
layout: default
title: OpenBox: Annotate Any Bounding Boxes in 3D
---

# OpenBox: Annotate Any Bounding Boxes in 3D

**arXiv**: [2512.01352v1](https://arxiv.org/abs/2512.01352) | [PDF](https://arxiv.org/pdf/2512.01352.pdf)

**ä½œè€…**: In-Jae Lee, Mungyeom Kim, Kwonyoung Ryu, Pierre Musacchio, Jaesik Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenBoxä¸¤é˜¶æ®µè‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œåˆ©ç”¨2Dè§†è§‰åŸºç¡€æ¨¡åž‹é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dè¾¹ç•Œæ¡†æ ‡æ³¨ã€‚**

**å…³é”®è¯**: `3Dç›®æ ‡æ£€æµ‹` `è‡ªåŠ¨æ ‡æ³¨` `è·¨æ¨¡æ€å¯¹é½` `å¼€æ”¾è¯æ±‡æ£€æµ‹` `è‡ªé€‚åº”è¾¹ç•Œæ¡†` `ç‚¹äº‘å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰3Dç›®æ ‡æ£€æµ‹æ–¹æ³•æ ‡æ³¨æˆæœ¬é«˜ã€å¿½ç•¥ç‰©ä½“ç‰©ç†çŠ¶æ€ã€ä¾èµ–è‡ªè®­ç»ƒå¯¼è‡´è´¨é‡ä½Žå’Œè®¡ç®—å¼€é”€å¤§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è·¨æ¨¡æ€å®žä¾‹å¯¹é½å…³è”2Då›¾åƒä¸Ž3Dç‚¹äº‘ï¼ŒåŸºäºŽåˆšæ€§å’Œè¿åŠ¨çŠ¶æ€åˆ†ç±»ç”Ÿæˆè‡ªé€‚åº”è¾¹ç•Œæ¡†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Waymoã€Lyftã€nuScenesæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œç›¸æ¯”åŸºçº¿æå‡å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ï¼Œæ— éœ€è‡ªè®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.

