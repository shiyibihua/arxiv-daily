---
layout: default
title: Securing Large Language Models (LLMs) from Prompt Injection Attacks
---

# Securing Large Language Models (LLMs) from Prompt Injection Attacks

**arXiv**: [2512.01326v1](https://arxiv.org/abs/2512.01326) | [PDF](https://arxiv.org/pdf/2512.01326.pdf)

**ä½œè€…**: Omar Farooq Khan Suri, John McCrae

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°JATMOé˜²å¾¡æ–¹æ³•å¯¹HOUYIæ”»å‡»çš„é²æ£’æ€§ï¼Œæ­ç¤ºå¾®è°ƒé˜²å¾¡çš„å±€é™ä¸Žæƒè¡¡**

**å…³é”®è¯**: `æç¤ºæ³¨å…¥æ”»å‡»` `å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨` `å¾®è°ƒé˜²å¾¡` `é—ä¼ æ”»å‡»æ¡†æž¶` `é²æ£’æ€§è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMsæ˜“å—æç¤ºæ³¨å…¥æ”»å‡»ï¼Œåˆ©ç”¨æŒ‡ä»¤è·Ÿéšèƒ½åŠ›æ‰§è¡Œæ¶æ„ä»»åŠ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨JATMOä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œå¹¶åŸºäºŽHOUYIæ”»å‡»æ¡†æž¶è¯„ä¼°å…¶é˜²å¾¡æ•ˆæžœã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šJATMOé™ä½Žæ”»å‡»æˆåŠŸçŽ‡ä½†æœªå®Œå…¨é˜»æ­¢ï¼Œå­˜åœ¨ç”Ÿæˆè´¨é‡ä¸Žæ¼æ´žçš„æƒè¡¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.

