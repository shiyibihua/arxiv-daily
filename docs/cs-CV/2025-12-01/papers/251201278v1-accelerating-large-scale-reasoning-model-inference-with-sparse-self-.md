---
layout: default
title: Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding
---

# Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding

**arXiv**: [2512.01278v1](https://arxiv.org/abs/2512.01278) | [PDF](https://arxiv.org/pdf/2512.01278.pdf)

**ä½œè€…**: Yilong Zhao, Jiaming Tang, Kan Zhu, Zihao Ye, Chi-Chih Chang, Chaofan Lin, Jongseok Park, Guangxuan Xiao, Mohamed S. Abdelfattah, Mingyu Gao, Baris Kasikci, Song Han, Ion Stoica

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparseSpecç¨€ç–è‡ªæŽ¨æµ‹è§£ç æ¡†æž¶ï¼Œä»¥åŠ é€Ÿå¤§è§„æ¨¡æŽ¨ç†æ¨¡åž‹ç”Ÿæˆå¹¶ç¼“è§£å†…å­˜å¸¦å®½åŽ‹åŠ›**

**å…³é”®è¯**: `æŽ¨æµ‹è§£ç ` `ç¨€ç–æ³¨æ„åŠ›` `KV-Cacheä¼˜åŒ–` `æŽ¨ç†åŠ é€Ÿ` `å†…å­˜å¸¦å®½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æŽ¨ç†æ¨¡åž‹é•¿ç”Ÿæˆå¯¼è‡´å†…å­˜è®¿é—®ç“¶é¢ˆï¼Œä»Žè®¡ç®—å¯†é›†åž‹è½¬ä¸ºå†…å­˜å¯†é›†åž‹
2. SparseSpecé‡‡ç”¨PillarAttnç¨€ç–æ³¨æ„åŠ›ä½œä¸ºè‰ç¨¿æ¨¡åž‹ï¼Œå¹¶ååŒè®¾è®¡è°ƒåº¦ã€å»¶è¿ŸéªŒè¯å’ŒåŠ¨æ€KV-Cacheç®¡ç†
3. å®žéªŒæ˜¾ç¤ºåœ¨å¤šç§æ¨¡åž‹å’Œæ•°æ®é›†ä¸Šï¼Œåžåé‡æœ€é«˜æå‡2.13å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.
>   To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.

