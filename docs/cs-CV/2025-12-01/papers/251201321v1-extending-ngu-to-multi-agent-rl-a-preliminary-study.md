---
layout: default
title: Extending NGU to Multi-Agent RL: A Preliminary Study
---

# Extending NGU to Multi-Agent RL: A Preliminary Study

**arXiv**: [2512.01321v1](https://arxiv.org/abs/2512.01321) | [PDF](https://arxiv.org/pdf/2512.01321.pdf)

**ä½œè€…**: Juan Hernandez, Diego FernÃ¡ndez, Manuel Cifuentes, Denis Parra, Rodrigo Toro Icarte

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å°†NGUç®—æ³•æ‰©å±•è‡³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨PettingZooçš„simple_tagçŽ¯å¢ƒä¸­è¯„ä¼°æ€§èƒ½**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `NGUç®—æ³•` `å†…åœ¨åŠ¨æœº` `ç¨€ç–å¥–åŠ±` `PettingZoo` `ç»éªŒå…±äº«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ™ºèƒ½ä½“çŽ¯å¢ƒä¸­ç¨€ç–å¥–åŠ±ä»»åŠ¡çš„å­¦ä¹ æ•ˆçŽ‡ä¸Žç¨³å®šæ€§é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šå°†NGUçš„episodicæ–°å¥‡æ€§ä¸Žå†…åœ¨åŠ¨æœºç»“åˆï¼ŒæŽ¢ç´¢å…±äº«å›žæ”¾ç¼“å†²åŒºå’Œå‚æ•°è°ƒä¼˜è®¾è®¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”å¤šæ™ºèƒ½ä½“DQNåŸºçº¿ï¼ŒNGUèŽ·å¾—ä¸­ç­‰æ›´é«˜å›žæŠ¥å’Œæ›´ç¨³å®šå­¦ä¹ åŠ¨æ€ï¼Œå…±äº«å›žæ”¾ç¼“å†²åŒºè¡¨çŽ°æœ€ä½³

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

