---
layout: default
title: Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework
---

# Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework

**arXiv**: [2512.01452v1](https://arxiv.org/abs/2512.01452) | [PDF](https://arxiv.org/pdf/2512.01452.pdf)

**ä½œè€…**: Lingbo Li, Anuradha Mathrani, Teo Susnjak

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽGEPAçš„ç¨‹åºåŒ–æç¤ºæ¡†æž¶ä»¥è‡ªåŠ¨åŒ–éšæœºå¯¹ç…§è¯•éªŒåå€šé£Žé™©è¯„ä¼°**

**å…³é”®è¯**: `åå€šé£Žé™©è¯„ä¼°` `ç¨‹åºåŒ–æç¤º` `GEPAæ¨¡å—` `éšæœºå¯¹ç…§è¯•éªŒ` `è¯æ®åˆæˆ` `å¤§åž‹è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹åŠ¨åå€šé£Žé™©è¯„ä¼°èµ„æºå¯†é›†ä¸”å­˜åœ¨å˜å¼‚æ€§ï¼ŒçŽ°æœ‰LLMæ–¹æ³•ä¾èµ–éš¾ä»¥å¤çŽ°çš„æ‰‹å·¥æç¤º
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨DSPyå’ŒGEPAæ¨¡å—ï¼Œé€šè¿‡å¸•ç´¯æ‰˜å¼•å¯¼æœç´¢ä¼˜åŒ–ä»£ç åŒ–æç¤ºï¼Œç”Ÿæˆå¯æ£€æŸ¥çš„æ‰§è¡Œè½¨è¿¹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨100ä¸ªRCTä¸Šè¯„ä¼°ï¼ŒGEPAæç¤ºåœ¨æ¸…æ™°æŠ¥å‘Šé¢†åŸŸè¡¨çŽ°æœ€ä½³ï¼Œæ•´ä½“å‡†ç¡®çŽ‡æœ€é«˜ï¼Œç›¸æ¯”æ‰‹åŠ¨æç¤ºæå‡30%-40%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.

