---
layout: default
title: nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis
---

# nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis

**arXiv**: [2512.01273v1](https://arxiv.org/abs/2512.01273) | [PDF](https://arxiv.org/pdf/2512.01273.pdf)

**ä½œè€…**: Xin Li, Wenhui Zhu, Xuanzhao Dong, Hao Wang, Yujian Xiong, Oana Dumitrascu, Yalin Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºnnMobileNet++æ··åˆç½‘ç»œä»¥æå‡è§†ç½‘è†œå›¾åƒåˆ†æžæ•ˆçŽ‡ä¸Žå‡†ç¡®æ€§**

**å…³é”®è¯**: `è§†ç½‘è†œå›¾åƒåˆ†æž` `æ··åˆç½‘ç»œæž¶æž„` `åŠ¨æ€è›‡å½¢å·ç§¯` `Transformerå—` `è½»é‡çº§ç½‘ç»œ` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå·ç§¯ç½‘ç»œéš¾ä»¥æ•èŽ·è§†ç½‘è†œå›¾åƒä¸­çš„é•¿è·ç¦»ä¾èµ–å’Œä¸è§„åˆ™ç—…å˜æ¨¡å¼ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆåŠ¨æ€è›‡å½¢å·ç§¯å’Œé˜¶æ®µç‰¹å®šTransformerå—ï¼Œå®žçŽ°è¾¹ç•Œæ„ŸçŸ¥ä¸Žå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¾¾åˆ°å…ˆè¿›ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒä½Žè®¡ç®—æˆæœ¬ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.

