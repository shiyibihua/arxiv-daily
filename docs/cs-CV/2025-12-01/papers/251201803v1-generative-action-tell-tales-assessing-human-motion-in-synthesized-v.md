---
layout: default
title: Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos
---

# Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos

**arXiv**: [2512.01803v1](https://arxiv.org/abs/2512.01803) | [PDF](https://arxiv.org/pdf/2512.01803.pdf)

**ä½œè€…**: Xavier Thomas, Youngsun Lim, Ananya Srinivasan, Audrey Zheng, Deepti Ghadiyaram

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçœŸå®žåŠ¨ä½œæ½œåœ¨ç©ºé—´çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥è§£å†³è§†é¢‘ç”Ÿæˆä¸­å¤æ‚äººç±»åŠ¨ä½œçš„è§†è§‰ä¸Žæ—¶é—´æ­£ç¡®æ€§è¯„ä¼°éš¾é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆè¯„ä¼°` `äººç±»åŠ¨ä½œåˆ†æž` `æ½œåœ¨ç©ºé—´å­¦ä¹ ` `æ—¶é—´åŠ¨æ€ç†è§£` `éª¨éª¼å‡ ä½•ç‰¹å¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡å­˜åœ¨å¤–è§‚åè§ï¼Œç¼ºä¹æ—¶é—´ç†è§£ï¼Œéš¾ä»¥æ£€æµ‹åŠ¨ä½œåŠ¨æ€å’Œè§£å‰–å­¦ä¸åˆç†æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆå¤–è§‚æ— å…³çš„éª¨éª¼å‡ ä½•ç‰¹å¾ä¸Žå¤–è§‚ç‰¹å¾ï¼Œå­¦ä¹ çœŸå®žåŠ¨ä½œçš„æ½œåœ¨ç©ºé—´ï¼Œé‡åŒ–ç”Ÿæˆè§†é¢‘ä¸ŽçœŸå®žåˆ†å¸ƒçš„è·ç¦»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–°åŸºå‡†ä¸Šæ¯”çŽ°æœ‰æ–¹æ³•æå‡è¶…è¿‡68%ï¼Œåœ¨å¤–éƒ¨åŸºå‡†ä¸Šè¡¨çŽ°ç«žäº‰æ€§ï¼Œä¸Žäººç±»æ„ŸçŸ¥ç›¸å…³æ€§æ›´å¼ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.

