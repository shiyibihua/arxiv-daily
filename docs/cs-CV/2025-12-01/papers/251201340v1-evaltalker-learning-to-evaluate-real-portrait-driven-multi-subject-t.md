---
layout: default
title: EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans
---

# EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans

**arXiv**: [2512.01340v1](https://arxiv.org/abs/2512.01340) | [PDF](https://arxiv.org/pdf/2512.01340.pdf)

**ä½œè€…**: Yingjie Zhou, Xilei Zhu, Siyu Ren, Ziyi Zhao, Ziwen Wang, Farong Wen, Yu Zhou, Jiezhang Cao, Xiongkuo Min, Fengjiao Chen, Xiaoyu Li, Xuezhi Cao, Guangtao Zhai, Xiaohong Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvalTalkeræ¡†æž¶ä»¥è¯„ä¼°å¤šä¸»ä½“è¯´è¯äººè§†é¢‘è´¨é‡ï¼Œå¹¶æž„å»ºé¦–ä¸ªå¤§è§„æ¨¡è¯„ä¼°æ•°æ®é›†THQA-MTã€‚**

**å…³é”®è¯**: `å¤šä¸»ä½“è¯´è¯äººç”Ÿæˆ` `è§†é¢‘è´¨é‡è¯„ä¼°` `å¤šæ¨¡æ€åŒæ­¥` `æ•°æ®é›†æž„å»º` `èº«ä»½ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šä¸»ä½“è¯´è¯äººè§†é¢‘ç”Ÿæˆå­˜åœ¨è´¨é‡ä¸‹é™ï¼Œç¼ºä¹è¯„ä¼°æ ‡å‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºTHQA-MTæ•°æ®é›†ï¼Œè®¾è®¡EvalTalkeræ¡†æž¶æ„ŸçŸ¥å…¨å±€è´¨é‡ã€èº«ä»½ä¸€è‡´æ€§å’Œå¤šæ¨¡æ€åŒæ­¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šEvalTalkerä¸Žä¸»è§‚è¯„åˆ†ç›¸å…³æ€§é«˜ï¼Œä¸ºé«˜è´¨é‡å¤šä¸»ä½“è¯´è¯äººç”Ÿæˆæä¾›è¯„ä¼°åŸºç¡€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.

