---
layout: default
title: Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval
---

# Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval

**arXiv**: [2512.01636v1](https://arxiv.org/abs/2512.01636) | [PDF](https://arxiv.org/pdf/2512.01636.pdf)

**ä½œè€…**: Xin Wang, Haipeng Zhang, Mang Li, Zhaohui Xia, Yueguo Chen, Yu Zhang, Chunyu Wei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFusion-Diffæ¡†æž¶ï¼Œé€šè¿‡è”åˆè§†è§‰-è¯­è¨€ç©ºé—´ç¼–è¾‘è§£å†³é›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢çš„æ¨¡æ€é¸¿æ²Ÿé—®é¢˜ã€‚**

**å…³é”®è¯**: `é›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢` `å¤šæ¨¡æ€å¯¹é½` `ç”Ÿæˆç¼–è¾‘` `è§†è§‰-è¯­è¨€ç©ºé—´` `æ•°æ®æ•ˆçŽ‡` `ç‰¹å¾èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢ä¸­ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¼¥åˆè§†è§‰ä¸Žè¯­è¨€æ¨¡æ€ä¹‹é—´çš„é¸¿æ²Ÿã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨è”åˆè§†è§‰-è¯­è¨€ç©ºé—´å¼•å…¥å¤šæ¨¡æ€èžåˆç‰¹å¾ç¼–è¾‘ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§Control-Adapteræå‡æ•°æ®æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIRRã€FashionIQå’ŒCIRCOåŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šå…ˆå‰é›¶æ ·æœ¬æ–¹æ³•ï¼Œä»…éœ€20ä¸‡åˆæˆæ ·æœ¬å¾®è°ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.

