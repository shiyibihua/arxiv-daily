---
layout: default
title: SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation
---

# SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation

**arXiv**: [2512.01960v1](https://arxiv.org/abs/2512.01960) | [PDF](https://arxiv.org/pdf/2512.01960.pdf)

**ä½œè€…**: Zisu Li, Hengye Lyu, Jiaxin Shi, Yufeng Zeng, Mingming Fan, Hanwang Zhang, Chen Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpriteHandè‡ªå›žå½’è§†é¢‘ç”Ÿæˆæ¡†æž¶ï¼Œå®žçŽ°å®žæ—¶å¤šæ ·åŒ–æ‰‹-ç‰©äº¤äº’è§†é¢‘åˆæˆ**

**å…³é”®è¯**: `è‡ªå›žå½’è§†é¢‘ç”Ÿæˆ` `æ‰‹-ç‰©äº¤äº’` `å®žæ—¶åˆæˆ` `å› æžœæŽ¨ç†` `æ··åˆåŽè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ¨¡æ‹Ÿæ–¹æ³•éš¾ä»¥å¤„ç†éžåˆšæ€§æˆ–é“°æŽ¥ç‰©ä½“çš„åŠ¨æ€æ‰‹-ç‰©äº¤äº’
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å› æžœæŽ¨ç†æž¶æž„è¿›è¡Œè‡ªå›žå½’ç”Ÿæˆï¼Œç»“åˆæ··åˆåŽè®­ç»ƒæå‡è§†è§‰çœŸå®žæ€§å’Œæ—¶åºä¸€è‡´æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹æ”¯æŒå®žæ—¶æµå¼ç”Ÿæˆï¼Œåœ¨è§†è§‰è´¨é‡ã€ç‰©ç†åˆç†æ€§å’Œäº¤äº’ä¿çœŸåº¦ä¸Šä¼˜äºŽåŸºçº¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.

