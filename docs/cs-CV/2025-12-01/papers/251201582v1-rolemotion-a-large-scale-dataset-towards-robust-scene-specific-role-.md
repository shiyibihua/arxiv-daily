---
layout: default
title: RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions
---

# RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions

**arXiv**: [2512.01582v1](https://arxiv.org/abs/2512.01582) | [PDF](https://arxiv.org/pdf/2512.01582.pdf)

**ä½œè€…**: Junran Peng, Yiheng Huang, Silei Shen, Zeji Wei, Jingwei Yang, Baojie Wang, Yonghao He, Chuanchen Luo, Man Zhang, Xucheng Yin, Wei Sui

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoleMotionæ•°æ®é›†ä»¥è§£å†³åœºæ™¯ç‰¹å®šè§’è‰²æ‰®æ¼”è¿åŠ¨åˆæˆä¸­æ•°æ®åŠŸèƒ½æ€§å’Œæ–‡æœ¬ç»†ç²’åº¦ä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `è§’è‰²æ‰®æ¼”è¿åŠ¨åˆæˆ` `ç»†ç²’åº¦æ–‡æœ¬æ ‡æ³¨` `å…¨èº«è¿åŠ¨ç”Ÿæˆ` `åœºæ™¯ç‰¹å®šæ•°æ®é›†` `è¿åŠ¨è¯„ä¼°å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–‡æœ¬é©±åŠ¨è¿åŠ¨æ•°æ®é›†æ•°æ®éžåŠŸèƒ½åŒ–ã€å­¤ç«‹ä¸”æ ‡æ³¨ç²—ç³™ï¼Œéš¾ä»¥è¦†ç›–å¤šæ ·åœºæ™¯ç¤¾äº¤æ´»åŠ¨
2. RoleMotionåŒ…å«25ä¸ªåœºæ™¯ã€110ä¸ªè§’è‰²ã€500+è¡Œä¸ºã€10296ä¸ªé«˜è´¨é‡å…¨èº«è¿åŠ¨åºåˆ—ï¼Œæ ‡æ³¨27831æ¡ç»†ç²’åº¦æ–‡æœ¬
3. æž„å»ºå¼ºè¯„ä¼°å™¨éªŒè¯æ•°æ®é›†è´¨é‡ï¼Œå®žéªŒè¯æ˜Žå…¶åœ¨æ–‡æœ¬é©±åŠ¨å…¨èº«ç”Ÿæˆä¸­çš„é«˜åŠŸèƒ½æ€§å’Œæœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.

