---
layout: default
title: A Diffusion Model Framework for Maximum Entropy Reinforcement Learning
---

# A Diffusion Model Framework for Maximum Entropy Reinforcement Learning

**arXiv**: [2512.02019v1](https://arxiv.org/abs/2512.02019) | [PDF](https://arxiv.org/pdf/2512.02019.pdf)

**ä½œè€…**: Sebastian Sanokowski, Kaustubh Patil, Alois Knoll

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ¡†æž¶ä»¥æ”¹è¿›æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼Œæå‡è¿žç»­æŽ§åˆ¶ä»»åŠ¡çš„æ€§èƒ½ä¸Žæ ·æœ¬æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `è¿žç»­æŽ§åˆ¶` `æ ·æœ¬æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå°†æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ é‡æ–°è§£é‡Šä¸ºæ‰©æ•£æ¨¡åž‹é‡‡æ ·é—®é¢˜ï¼Œä»¥å¤„ç†å¤æ‚ç­–ç•¥åˆ†å¸ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æœ€å°åŒ–åå‘KLæ•£åº¦çš„ä¸Šç•Œï¼Œç»“åˆæ‰©æ•£åŠ¨æ€æŽ¨å¯¼æ”¹è¿›çš„ä»£ç†ç›®æ ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†åŸºå‡†ä¸Šï¼ŒDiffSACã€DiffPPOå’ŒDiffWPOä¼˜äºŽSACå’ŒPPOï¼Œå®žçŽ°æ›´é«˜å›žæŠ¥å’Œæ ·æœ¬æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.

