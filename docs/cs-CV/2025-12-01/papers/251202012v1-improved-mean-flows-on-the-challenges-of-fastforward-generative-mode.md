---
layout: default
title: Improved Mean Flows: On the Challenges of Fastforward Generative Models
---

# Improved Mean Flows: On the Challenges of Fastforward Generative Models

**arXiv**: [2512.02012v1](https://arxiv.org/abs/2512.02012) | [PDF](https://arxiv.org/pdf/2512.02012.pdf)

**ä½œè€…**: Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman, J. Zico Kolter, Kaiming He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ”¹è¿›MeanFlowä»¥è§£å†³å•æ­¥ç”Ÿæˆæ¨¡åž‹ä¸­çš„è®­ç»ƒä¸ç¨³å®šå’Œå¼•å¯¼çµæ´»æ€§ä¸è¶³é—®é¢˜ã€‚**

**å…³é”®è¯**: `å•æ­¥ç”Ÿæˆæ¨¡åž‹` `è®­ç»ƒç¨³å®šæ€§` `å¼•å¯¼æœºåˆ¶` `ä¸Šä¸‹æ–‡æ¡ä»¶` `å›¾åƒç”Ÿæˆ` `å¹³å‡é€Ÿåº¦é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŽŸMeanFlowè®­ç»ƒç›®æ ‡ä¾èµ–ç½‘ç»œè‡ªèº«ï¼Œå¯¼è‡´ä¸ç¨³å®šï¼›æœ¬æ–‡é‡å‚æ•°åŒ–ä¸ºå¹³å‡é€Ÿåº¦é¢„æµ‹ï¼Œæå‡å›žå½’ç¨³å®šæ€§ã€‚
2. åŽŸæ–¹æ³•å›ºå®šè®­ç»ƒæ—¶å¼•å¯¼å°ºåº¦ï¼Œç‰ºç‰²çµæ´»æ€§ï¼›æœ¬æ–‡é€šè¿‡æ˜¾å¼æ¡ä»¶å˜é‡å®žçŽ°æµ‹è¯•æ—¶çµæ´»å¼•å¯¼ï¼Œå¹¶é‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶å¤„ç†ã€‚
3. æ”¹è¿›æ–¹æ³•åœ¨ImageNet 256Ã—256ä¸Šå®žçŽ°1.72 FIDï¼ˆ1-NFEï¼‰ï¼Œä¼˜äºŽåŒç±»æ–¹æ³•ï¼Œç¼©å°ä¸Žå¤šæ­¥æ–¹æ³•çš„å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.

