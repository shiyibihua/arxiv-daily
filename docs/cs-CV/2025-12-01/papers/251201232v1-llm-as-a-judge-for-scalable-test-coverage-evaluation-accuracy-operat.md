---
layout: default
title: LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost
---

# LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost

**arXiv**: [2512.01232v1](https://arxiv.org/abs/2512.01232) | [PDF](https://arxiv.org/pdf/2512.01232.pdf)

**ä½œè€…**: Donghao Huang, Shila Chew, Anna Dutkiewicz, Zhaoxia Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLM-as-a-Judgeæ¡†æž¶ï¼Œä»¥å¯æ‰©å±•æ–¹å¼è¯„ä¼°GherkinéªŒæ”¶æµ‹è¯•çš„è¦†ç›–åº¦ã€‚**

**å…³é”®è¯**: `æµ‹è¯•è¦†ç›–åº¦è¯„ä¼°` `LLMè¯„ä¼°æ¡†æž¶` `GherkinéªŒæ”¶æµ‹è¯•` `æ“ä½œå¯é æ€§` `æˆæœ¬åˆ†æž` `æ¨¡åž‹æ¯”è¾ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§„æ¨¡è½¯ä»¶æµ‹è¯•è¦†ç›–åº¦è¯„ä¼°åœ¨QAæµç¨‹ä¸­ä»æ˜¯ç“¶é¢ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽè¯„åˆ†å‡†åˆ™çš„æ¡†æž¶ï¼Œä½¿ç”¨LLMç”Ÿæˆç»“æž„åŒ–JSONè¾“å‡ºè¯„ä¼°æµ‹è¯•ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨20ç§æ¨¡åž‹é…ç½®ä¸Šåˆ†æžå‡†ç¡®æ€§ã€æ“ä½œå¯é æ€§å’Œæˆæœ¬ï¼Œå‘çŽ°å°æ¨¡åž‹å¯ä¼˜äºŽå¤§æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Assessing software test coverage at scale remains a bottleneck in QA pipelines. We present LLM-as-a-Judge (LAJ), a production-ready, rubric-driven framework for evaluating Gherkin acceptance tests with structured JSON outputs. Across 20 model configurations (GPT-4, GPT-5 with varying reasoning effort, and open-weight models) on 100 expert-annotated scripts over 5 runs (500 evaluations), we provide the first comprehensive analysis spanning accuracy, operational reliability, and cost. We introduce the Evaluation Completion Rate (ECR@1) to quantify first-attempt success, revealing reliability from 85.4% to 100.0% with material cost implications via retries. Results show that smaller models can outperform larger ones: GPT-4o Mini attains the best accuracy (6.07 MAAE), high reliability (96.6% ECR@1), and low cost ($1.01 per 1K), yielding a 78x cost reduction vs. GPT-5 (high reasoning) while improving accuracy. Reasoning effort is model-family dependent: GPT-5 benefits from increased reasoning (with predictable accuracy-cost tradeoffs), whereas open-weight models degrade across all dimensions as reasoning increases. Overall, cost spans 175x ($0.45-$78.96 per 1K). We release the dataset, framework, and code to support reproducibility and deployment.

