---
layout: default
title: H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons
---

# H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons

**arXiv**: [2512.01797v1](https://arxiv.org/abs/2512.01797) | [PDF](https://arxiv.org/pdf/2512.01797.pdf)

**ä½œè€…**: Cheng Gao, Huimin Chen, Chaojun Xiao, Zhiyi Chen, Zhiyuan Liu, Maosong Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹»è§‰å…³è”ç¥žç»å…ƒï¼ˆH-Neuronsï¼‰ä»¥æ­ç¤ºå¤§è¯­è¨€æ¨¡åž‹å¹»è§‰çš„å¾®è§‚æœºåˆ¶**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `å¹»è§‰æ£€æµ‹` `ç¥žç»å…ƒåˆ†æž` `å› æžœå¹²é¢„` `é¢„è®­ç»ƒæœºåˆ¶` `å¯é æ€§æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹å¸¸äº§ç”Ÿå¹»è§‰ï¼Œä½†ç¥žç»å…ƒçº§æœºåˆ¶æœªçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šè¯†åˆ«ç¨€ç–ç¥žç»å…ƒå­é›†é¢„æµ‹å¹»è§‰ï¼Œå¹¶æŽ¢ç©¶å…¶å› æžœå½±å“ä¸Žèµ·æº
3. å®žéªŒæˆ–æ•ˆæžœï¼šç¥žç»å…ƒé¢„æµ‹æ³›åŒ–å¼ºï¼Œå¹²é¢„æ˜¾ç¤ºå› æžœå…³è”ï¼Œèµ·æºå¯è¿½æº¯è‡³é¢„è®­ç»ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

