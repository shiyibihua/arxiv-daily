---
layout: default
title: Low-Rank Prehab: Preparing Neural Networks for SVD Compression
---

# Low-Rank Prehab: Preparing Neural Networks for SVD Compression

**arXiv**: [2512.01980v1](https://arxiv.org/abs/2512.01980) | [PDF](https://arxiv.org/pdf/2512.01980.pdf)

**ä½œè€…**: Haoran Qin, Shansita Sharma, Ali Abbasi, Chayne Thrash, Soheil Kolouri

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½Žç§©é¢„é€‚åº”æ–¹æ³•ï¼Œåœ¨SVDåŽ‹ç¼©å‰ä¼˜åŒ–ç¥žç»ç½‘ç»œæƒé‡ç»“æž„ä»¥æå‡åŽ‹ç¼©æ•ˆæžœã€‚**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œåŽ‹ç¼©` `ä½Žç§©è¿‘ä¼¼` `å¥‡å¼‚å€¼åˆ†è§£` `é¢„é€‚åº”å¾®è°ƒ` `Transformeræž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSVDåŽ‹ç¼©ç¥žç»ç½‘ç»œåŽéœ€å¾®è°ƒæ¢å¤ç²¾åº¦ï¼Œä½†åŽ‹ç¼©æ—¶ç²¾åº¦ä¸‹é™è¾ƒå¤§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥é¢„åŽ‹ç¼©å¾®è°ƒé˜¶æ®µï¼Œé¼“åŠ±æƒé‡çŸ©é˜µä½Žç§©åŒ–ï¼Œä¸ºSVDåŽ‹ç¼©åšå‡†å¤‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLMså’ŒViTsä¸ŠéªŒè¯ï¼Œå‡å°‘åŽ‹ç¼©åŽç²¾åº¦ä¸‹é™ï¼Œä¼˜äºŽçŽ°æœ‰SVDæŠ€æœ¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Low-rank approximation methods such as singular value decomposition (SVD) and its variants (e.g., Fisher-weighted SVD, Activation SVD) have recently emerged as effective tools for neural network compression. In this setting, decomposition acts as a "surgical" intervention, followed by fine-tuning that serves as "rehab" to recover accuracy. Inspired by prehabilitation in surgery, we introduce a pre-compression fine-tuning stage, Low-Rank Prehab, that explicitly encourages low-rank structure in weight matrices while preserving task performance. By conditioning the model before SVD, Prehab steers weights toward spectrally compact regions of the parameter space, enabling smoother low-rank approximation and improved recovery. Experiments on large language models (LLMs) and other Transformer-based architectures, including Vision Transformers (ViTs), show that Prehab substantially reduces the immediate accuracy drop after compression and consistently improves post-finetuning performance. Across a wide range of compression ratios, our method outperforms state-of-the-art SVD-based techniques such as SVD-LLM, highlighting the importance of preparing models for compression rather than only improving the compression and recovery stages. Source code is available at https://github.com/niqretnuh/PREHAB-SVD

