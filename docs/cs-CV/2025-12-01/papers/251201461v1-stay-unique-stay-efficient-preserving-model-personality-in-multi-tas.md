---
layout: default
title: Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging
---

# Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging

**arXiv**: [2512.01461v1](https://arxiv.org/abs/2512.01461) | [PDF](https://arxiv.org/pdf/2512.01461.pdf)

**ä½œè€…**: Kuangpu Guo, Yuhe Ding, Jian Liang, Zilei Wang, Ran He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDTSæ¡†æž¶ä»¥åœ¨æ¨¡åž‹åˆå¹¶ä¸­é«˜æ•ˆä¿ç•™ä»»åŠ¡ç‰¹å®šä¿¡æ¯**

**å…³é”®è¯**: `æ¨¡åž‹åˆå¹¶` `å¤šä»»åŠ¡å­¦ä¹ ` `å¥‡å¼‚å€¼åˆ†è§£` `ä¸ªæ€§åŒ–æ¡†æž¶` `å­˜å‚¨æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¨¡åž‹åˆå¹¶æ–¹æ³•åœ¨ç›¸ä¼¼ä»»åŠ¡ä¸Šæ€§èƒ½ä¸‹é™æ˜¾è‘—ï¼Œéœ€ä¿ç•™ä»»åŠ¡ç‰¹å®šä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¥‡å¼‚å€¼åˆ†è§£ã€é˜ˆå€¼åˆ†ç»„å’Œç¼©æ”¾å› å­ï¼Œå®žçŽ°ä¸ªæ€§åŒ–åˆå¹¶ï¼Œå­˜å‚¨å¼€é”€ä½Žã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šDTSåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šä¼˜äºŽåŸºçº¿ï¼Œæ¯ä»»åŠ¡ä»…éœ€1%é¢å¤–å­˜å‚¨ï¼Œå˜ä½“åœ¨æœªè§ä»»åŠ¡ä¸Šæ³›åŒ–æ€§èƒ½å¥½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at https://github.com/krumpguo/DTS.

