---
layout: default
title: Decision Tree Embedding by Leaf-Means
---

# Decision Tree Embedding by Leaf-Means

**arXiv**: [2512.01819v1](https://arxiv.org/abs/2512.01819) | [PDF](https://arxiv.org/pdf/2512.01819.pdf)

**ä½œè€…**: Cencheng Shen, Yuexiao Dong, Carey E. Priebe

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå†³ç­–æ ‘åµŒå…¥æ–¹æ³•ï¼Œé€šè¿‡å¶èŠ‚ç‚¹å‡å€¼æž„å»ºç‰¹å¾è¡¨ç¤ºä»¥é™ä½Žæ–¹å·®å¹¶æå‡åˆ†ç±»æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `å†³ç­–æ ‘åµŒå…¥` `ç‰¹å¾è¡¨ç¤º` `åˆ†ç±»ç®—æ³•` `è®¡ç®—æ•ˆçŽ‡` `é›†æˆå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å†³ç­–æ ‘å’Œéšæœºæ£®æž—åœ¨åˆ†ç±»ä¸­é¢ä¸´é«˜æ–¹å·®æˆ–è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚
2. DTEåˆ©ç”¨è®­ç»ƒæ ‘çš„åˆ†åŒºç»“æž„ï¼Œä»¥å¶å†…æ ·æœ¬å‡å€¼ä¸ºé”šç‚¹æ˜ å°„è¾“å…¥åˆ°åµŒå…¥ç©ºé—´ã€‚
3. å®žéªŒæ˜¾ç¤ºDTEåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆçŽ‡ä¸Šä¼˜äºŽæˆ–åŒ¹é…éšæœºæ£®æž—å’Œæµ…å±‚ç¥žç»ç½‘ç»œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms.

