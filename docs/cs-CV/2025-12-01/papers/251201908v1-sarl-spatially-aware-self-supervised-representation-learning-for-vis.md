---
layout: default
title: SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception
---

# SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception

**arXiv**: [2512.01908v1](https://arxiv.org/abs/2512.01908) | [PDF](https://arxiv.org/pdf/2512.01908.pdf)

**ä½œè€…**: Gurmeher Khurana, Lan Wei, Dandan Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSARLæ¡†æž¶ï¼Œé€šè¿‡ç©ºé—´æ„ŸçŸ¥è‡ªç›‘ç£å­¦ä¹ æå‡èžåˆè§†è§‰-è§¦è§‰æ„ŸçŸ¥èƒ½åŠ›**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è§†è§‰-è§¦è§‰æ„ŸçŸ¥` `ç©ºé—´æ„ŸçŸ¥è¡¨ç¤º` `æœºå™¨äººæ“ä½œ` `ç‰¹å¾å›¾å¯¹é½` `å‡ ä½•ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è‡ªç›‘ç£å­¦ä¹ æ¡†æž¶åŽ‹ç¼©ç‰¹å¾å›¾ä¸ºå…¨å±€å‘é‡ï¼Œä¸¢å¼ƒç©ºé—´ç»“æž„ï¼Œä¸é€‚ç”¨äºŽéœ€è¦å±€éƒ¨å‡ ä½•ä¿¡æ¯çš„æœºå™¨äººæ“ä½œä»»åŠ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽBYOLæž¶æž„ï¼Œå¼•å…¥ä¸‰ä¸ªåœ°å›¾çº§ç›®æ ‡ï¼ˆSALã€PPDAã€RAMï¼‰ï¼Œä¿æŒè·¨è§†å›¾çš„æ³¨æ„åŠ›ç„¦ç‚¹ã€éƒ¨ä»¶ç»„åˆå’Œå‡ ä½•å…³ç³»ä¸€è‡´æ€§ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å…­ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºŽä¹ä¸ªåŸºçº¿ï¼Œåœ¨å‡ ä½•æ•æ„Ÿçš„è¾¹ç¼˜å§¿æ€å›žå½’ä»»åŠ¡ä¸ŠMAEä¸º0.3955ï¼Œç›¸å¯¹æå‡30%ï¼ŒæŽ¥è¿‘ç›‘ç£ä¸Šé™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.

