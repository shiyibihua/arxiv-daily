---
layout: default
title: Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages
---

# Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages

**arXiv**: [2512.01460v1](https://arxiv.org/abs/2512.01460) | [PDF](https://arxiv.org/pdf/2512.01460.pdf)

**ä½œè€…**: Jozef KubÃ­k, Marek Å uppa, Martin TakÃ¡Ä

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆä¸»åŠ¨å­¦ä¹ ä¸Žèšç±»çš„å¾®è°ƒç®¡é“ï¼Œä»¥æå‡ä½Žèµ„æºè¯­è¨€æƒ…æ„Ÿåˆ†æžæ€§èƒ½å¹¶å‡å°‘æ ‡æ³¨æˆæœ¬ã€‚**

**å…³é”®è¯**: `ä½Žèµ„æºè¯­è¨€å¤„ç†` `ä¸»åŠ¨å­¦ä¹ ` `æ•°æ®èšç±»` `å¾®è°ƒä¼˜åŒ–` `æƒ…æ„Ÿåˆ†æž` `æ ‡æ³¨æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä½Žèµ„æºè¯­è¨€æ•°æ®æœ‰é™å¯¼è‡´è¯­è¨€æ¨¡åž‹å¾®è°ƒæ•ˆæžœä¸ä½³ï¼Œéœ€åœ¨æœ‰é™æ•°æ®ä¸‹ä¼˜åŒ–æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆä¸»åŠ¨å­¦ä¹ ã€æ•°æ®èšç±»å’ŒåŠ¨æ€é€‰æ‹©è°ƒåº¦å™¨ï¼Œæž„å»ºç³»ç»ŸåŒ–å¾®è°ƒæµç¨‹ä»¥é«˜æ•ˆåˆ©ç”¨æ ‡æ³¨æ•°æ®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–¯æ´›ä¼å…‹è¯­ç­‰è¯­è¨€ä¸Šæµ‹è¯•ï¼Œå®žçŽ°é«˜è¾¾30%çš„æ ‡æ³¨èŠ‚çœå’Œæœ€å¤š4ä¸ªF1åˆ†æ•°çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶å¢žå¼ºå¾®è°ƒç¨³å®šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.

