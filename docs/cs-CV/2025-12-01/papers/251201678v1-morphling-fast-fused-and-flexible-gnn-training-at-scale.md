---
layout: default
title: Morphling: Fast, Fused, and Flexible GNN Training at Scale
---

# Morphling: Fast, Fused, and Flexible GNN Training at Scale

**arXiv**: [2512.01678v1](https://arxiv.org/abs/2512.01678) | [PDF](https://arxiv.org/pdf/2512.01678.pdf)

**ä½œè€…**: Anubhab, Rupesh Nasre

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMorphlingä»£ç åˆæˆå™¨ä»¥è§£å†³GNNè®­ç»ƒä¸­å›¾éåŽ†ä¸ŽçŸ©é˜µè¿ç®—èžåˆçš„æ€§èƒ½ç“¶é¢ˆ**

**å…³é”®è¯**: `å›¾ç¥žç»ç½‘ç»œè®­ç»ƒ` `ä»£ç åˆæˆ` `å¼‚æž„è®¡ç®—ä¼˜åŒ–` `ç¨€ç–æ„ŸçŸ¥æ‰§è¡Œ` `é«˜æ€§èƒ½è®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. GNNè®­ç»ƒé¢ä¸´å›¾éåŽ†ä¸ŽçŸ©é˜µè¿ç®—çš„å¼‚æž„æ‰§è¡ŒæŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ¡†æž¶ä¾èµ–é€šç”¨å†…æ ¸å¯¼è‡´æ€§èƒ½ä½Žä¸‹
2. Morphlingé€šè¿‡ç¼–è¯‘GNNè§„èŒƒä¸ºåŽç«¯ä¸“ç”¨å®žçŽ°ï¼Œé›†æˆä¼˜åŒ–åŽŸè¯­å’Œè¿è¡Œæ—¶ç¨€ç–æ„ŸçŸ¥å¼•æ“Ž
3. å®žéªŒæ˜¾ç¤ºåœ¨CPUå’ŒGPUä¸Šå¹³å‡æé€Ÿ20å€å’Œ19å€ï¼Œå†…å­˜æ¶ˆè€—é™ä½Žè¾¾15å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.

