---
layout: default
title: SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models
---

# SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models

**arXiv**: [2512.01748v1](https://arxiv.org/abs/2512.01748) | [PDF](https://arxiv.org/pdf/2512.01748.pdf)

**ä½œè€…**: Stella Etuk, Ashraf Matrawy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSA-ADPæ–¹æ³•ï¼ŒåŸºäºŽPIIæ•æ„Ÿåº¦åˆ†é…å™ªå£°ä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒä¸­çš„éšç§-æ•ˆç”¨æƒè¡¡é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `å·®åˆ†éšç§` `éšç§ä¿æŠ¤` `æ•æ„Ÿåº¦æ„ŸçŸ¥` `è‡ªé€‚åº”å™ªå£°åˆ†é…` `æ¨¡åž‹æ•ˆç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒä¸­PIIè®°å¿†å¼•å‘éšç§æ‹…å¿§ï¼Œä¼ ç»ŸDP-SGDæ–¹æ³•å› å‡åŒ€åŠ å™ªå¯¼è‡´æ¨¡åž‹æ•ˆç”¨ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šSA-ADPæ ¹æ®ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯çš„æ•æ„Ÿåº¦è‡ªé€‚åº”åˆ†é…å™ªå£°ï¼Œè€Œéžç»Ÿä¸€å¤„ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒSA-ADPåœ¨ä¿æŒå¼ºéšç§ä¿æŠ¤çš„åŒæ—¶ï¼Œæ¨¡åž‹æ•ˆç”¨æœªé™ä½Žï¼Œä¸Žæ— éšç§ä¿æŠ¤å’ŒDP-SGDåŸºçº¿ç›¸å½“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.

