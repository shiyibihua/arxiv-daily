---
layout: default
title: Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models
---

# Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models

**arXiv**: [2512.01343v1](https://arxiv.org/abs/2512.01343) | [PDF](https://arxiv.org/pdf/2512.01343.pdf)

**ä½œè€…**: Shashank Landge, Abhishek Patil, Tejas kamble, Bhushan Buddhivant, Priyanka Joshi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽSVDçš„ç»“æž„æ„ŸçŸ¥æƒé‡é€‰æ‹©æ–¹æ³•ï¼Œä»¥è§£å†³æ— æ ¡å‡†æ•°æ®ä¸‹å¤§è¯­è¨€æ¨¡åž‹æ··åˆç²¾åº¦é‡åŒ–é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–` `æ··åˆç²¾åº¦é‡åŒ–` `å¥‡å¼‚å€¼åˆ†è§£` `æƒé‡é€‰æ‹©` `æ•°æ®è‡ªç”±æ–¹æ³•` `ç»“æž„é‡è¦æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹é‡åŒ–ä¸­ï¼Œå‡åŒ€é‡åŒ–å› å…³é”®æƒé‡ï¼ˆå¼‚å¸¸ç‰¹å¾ï¼‰å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸”çŽ°æœ‰æ–¹æ³•ä¾èµ–æ ¡å‡†æ•°æ®ï¼Œåœ¨æ•°æ®éšç§æˆ–ç¼ºå¤±åœºæ™¯ä¸‹ä¸é€‚ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå‡è®¾å¥‡å¼‚å€¼åˆ†è§£è¯†åˆ«çš„ä¸»æˆåˆ†æƒé‡å¯¹æ¨¡åž‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œæå‡ºæ•°æ®è‡ªç”±é€‰æ‹©å¯å‘å¼ï¼Œä¿ç•™FP32ç²¾åº¦ä¸»æˆåˆ†æƒé‡ï¼Œå…¶ä½™æƒé‡è¿›è¡Œæ¿€è¿›é‡åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºŽSVDçš„æ–¹æ³•åœ¨RTEä»»åŠ¡ä¸Šè¾¾åˆ°66.06%å‡†ç¡®çŽ‡ï¼Œä¼˜äºŽAWQå’ŒSpQRï¼ŒéªŒè¯ç»“æž„é‡è¦æ€§å¯ä½œä¸ºæƒé‡æ˜¾è‘—æ€§çš„ç¨³å¥ä»£ç†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As Large Language Models (LLMs) continue to scale in parameter count, deploying them on commodity hardware has become increasingly challenging. Post-Training Quantization (PTQ) addresses this by reducing the precision of model weights, typically to 4-bit or lower. However, uniform quantization often leads to significant performance degradation due to the presence of ``outlier features'' -- weights that, while few in number, are critical for maintaining model accuracy. Current state-of-the-art methods such as AWQ (Activation-aware Weight Quantization) and SpQR (Sparse Quantization Representations) rely on calibration data to identify these salient weights via activation magnitudes or Hessian sensitivity. In scenarios where data privacy is paramount or calibration data is unavailable, these methods are inapplicable.
>   In this work, we propose a data-free, structure-aware hypothesis: that the weights identified as Principal Components via Singular Value Decomposition (SVD) are intrinsically important to the model's downstream performance. We introduce a novel selection heuristic that preserves the top-$k$ weights aligned with the principal components in FP32, while aggressively quantizing the residual weights. We compare our method against activation-aware (AWQ) and second-order (SpQR) methods across GLUE benchmarks (MRPC, RTE, QNLI) using a DistilBERT backbone. Our experiments reveal that structural importance is highly correlated with functional importance. On the challenging RTE task, our SVD-based method achieves an accuracy of 66.06\%, outperforming both AWQ (65.34\%) and SpQR (65.34\%) at high protection budgets, validating that intrinsic matrix structure can serve as a robust proxy for weight saliency without the need for forward passes or calibration data.

