---
layout: default
title: PSA-MF: Personality-Sentiment Aligned Multi-Level Fusion for Multimodal Sentiment Analysis
---

# PSA-MF: Personality-Sentiment Aligned Multi-Level Fusion for Multimodal Sentiment Analysis

**arXiv**: [2512.01442v1](https://arxiv.org/abs/2512.01442) | [PDF](https://arxiv.org/pdf/2512.01442.pdf)

**ä½œè€…**: Heng Xie, Kang Zhu, Zhengqi Wen, Jianhua Tao, Xuefei Liu, Ruibo Fu, Changsheng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPSA-MFæ¡†æž¶ï¼Œé€šè¿‡ä¸ªæ€§-æƒ…æ„Ÿå¯¹é½å’Œå¤šçº§èžåˆè§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžä¸­çš„ç‰¹å¾æå–ä¸Žèžåˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æž` `ä¸ªæ€§-æƒ…æ„Ÿå¯¹é½` `å¤šçº§èžåˆ` `ç‰¹å¾æå–` `æƒ…æ„Ÿè¯†åˆ«` `å¤šæ¨¡æ€èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨å•æ¨¡æ€ç‰¹å¾æå–ä¸­å¿½ç•¥ä¸ªæ€§å·®å¼‚ï¼Œå¤šæ¨¡æ€èžåˆä¸­æœªè€ƒè™‘ç‰¹å¾å±‚çº§å·®å¼‚ï¼Œå½±å“è¯†åˆ«æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ä¸ªæ€§ç‰¹å¾ï¼Œæå‡ºä¸ªæ€§-æƒ…æ„Ÿå¯¹é½æ–¹æ³•èŽ·å–ä¸ªæ€§åŒ–æƒ…æ„ŸåµŒå…¥ï¼›é‡‡ç”¨å¤šçº§èžåˆç­–ç•¥ï¼Œé€šè¿‡é¢„èžåˆå’Œå¢žå¼ºèžåˆé€æ­¥æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸¤ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šå®žéªŒï¼Œè¾¾åˆ°æœ€å…ˆè¿›ç»“æžœï¼ŒéªŒè¯äº†æ¡†æž¶çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal sentiment analysis (MSA) is a research field that recognizes human sentiments by combining textual, visual, and audio modalities. The main challenge lies in integrating sentiment-related information from different modalities, which typically arises during the unimodal feature extraction phase and the multimodal feature fusion phase. Existing methods extract only shallow information from unimodal features during the extraction phase, neglecting sentimental differences across different personalities. During the fusion phase, they directly merge the feature information from each modality without considering differences at the feature level. This ultimately affects the model's recognition performance. To address this problem, we propose a personality-sentiment aligned multi-level fusion framework. We introduce personality traits during the feature extraction phase and propose a novel personality-sentiment alignment method to obtain personalized sentiment embeddings from the textual modality for the first time. In the fusion phase, we introduce a novel multi-level fusion method. This method gradually integrates sentimental information from textual, visual, and audio modalities through multimodal pre-fusion and a multi-level enhanced fusion strategy. Our method has been evaluated through multiple experiments on two commonly used datasets, achieving state-of-the-art results.

