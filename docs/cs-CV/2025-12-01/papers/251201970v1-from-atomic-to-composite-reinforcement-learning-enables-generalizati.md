---
layout: default
title: From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning
---

# From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning

**arXiv**: [2512.01970v1](https://arxiv.org/abs/2512.01970) | [PDF](https://arxiv.org/pdf/2512.01970.pdf)

**ä½œè€…**: Sitao Cheng, Xunjian Yin, Ruiwen Zhou, Yuxuan Li, Xinyi Wang, Liangming Pan, William Yang Wang, Victor Zhong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŽŸå­æŠ€èƒ½åˆ†è§£ä¸Žå¼ºåŒ–å­¦ä¹ ç»“åˆæ–¹æ³•ï¼Œä»¥æå‡äº’è¡¥æŽ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `äº’è¡¥æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `åŽŸå­æŠ€èƒ½åˆ†è§£` `ç›‘ç£å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç©¶å¼ºåŒ–å­¦ä¹ åœ¨æŽ¨ç†èƒ½åŠ›ä¸­çš„ä½œç”¨ï¼Œæ˜¯åˆæˆæ–°æŠ€èƒ½è¿˜æ˜¯ä»…æ”¾å¤§çŽ°æœ‰è¡Œä¸º
2. æ–¹æ³•è¦ç‚¹ï¼šå°†äº’è¡¥æŽ¨ç†åˆ†è§£ä¸ºå‚æ•°æŽ¨ç†å’Œä¸Šä¸‹æ–‡æŽ¨ç†åŽŸå­æŠ€èƒ½ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸Žå¼ºåŒ–å­¦ä¹ ç»“åˆè®­ç»ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°å¼ºåŒ–å­¦ä¹ èƒ½åˆæˆå¤æ‚æŽ¨ç†ç­–ç•¥ï¼Œä½†éœ€åŽŸå­æŠ€èƒ½ä½œä¸ºåŸºç¡€ï¼Œæ˜¾è‘—æå‡é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

