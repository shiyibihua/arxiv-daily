---
layout: default
title: FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention
---

# FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention

**arXiv**: [2512.01540v1](https://arxiv.org/abs/2512.01540) | [PDF](https://arxiv.org/pdf/2512.01540.pdf)

**ä½œè€…**: Zipeng Wang, Dan Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlashVGGTä»¥è§£å†³å¤šè§†å›¾3Dé‡å»ºä¸­è‡ªæ³¨æ„åŠ›äºŒæ¬¡å¤æ‚åº¦å¯¼è‡´çš„æ‰©å±•æ€§é—®é¢˜**

**å…³é”®è¯**: `å¤šè§†å›¾3Dé‡å»º` `æè¿°ç¬¦æ³¨æ„åŠ›` `é•¿åºåˆ—å¤„ç†` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–` `è§†è§‰å‡ ä½•å˜æ¢å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVGGTç­‰æ¨¡åž‹å› å…¨è‡ªæ³¨æ„åŠ›äºŒæ¬¡å¤æ‚åº¦ï¼Œåœ¨å¤„ç†é•¿å›¾åƒåºåˆ—æ—¶æ‰©å±•æ€§å·®
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åŽ‹ç¼©ç©ºé—´ä¿¡æ¯ä¸ºæè¿°ç¬¦ä»¤ç‰Œï¼Œé‡‡ç”¨æè¿°ç¬¦æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£å¯†é›†å…¨å±€æ³¨æ„åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨1000å¼ å›¾åƒä¸ŠæŽ¨ç†æ—¶é—´é™è‡³VGGTçš„9.3%ï¼Œå¯æ‰©å±•è‡³è¶…è¿‡3000å¼ å›¾åƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.

