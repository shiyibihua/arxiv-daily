---
layout: default
title: Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity
---

# Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity

**arXiv**: [2512.01357v1](https://arxiv.org/abs/2512.01357) | [PDF](https://arxiv.org/pdf/2512.01357.pdf)

**ä½œè€…**: Wenbin Zhu, Zhaoyan Shen, Zili Shao, Hongjun Dai, Feng Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTangramç³»ç»Ÿï¼Œé€šè¿‡GPUå†…å­˜é‡ç”¨å’Œäº²å’Œæ€§è°ƒåº¦åŠ é€ŸServerless LLMåŠ è½½**

**å…³é”®è¯**: `Serverless LLM` `GPUå†…å­˜é‡ç”¨` `å†·å¯åŠ¨ä¼˜åŒ–` `KVç¼“å­˜ç®¡ç†` `GPUäº²å’Œæ€§è°ƒåº¦` `æ¨¡åž‹åŠ è½½åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šServerless LLMå†·å¯åŠ¨å»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯æ¨¡åž‹åŠ è½½é˜¶æ®µï¼Œéšæ¨¡åž‹å¤§å°çº¿æ€§å¢žé•¿ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æœªä½¿ç”¨çš„GPUå†…å­˜ä¿ç•™æ¨¡åž‹å‚æ•°ï¼ŒåŒ…æ‹¬ç»Ÿä¸€GPUå†…å­˜æ± ã€æŒ‰éœ€KVç¼“å­˜åˆ†é…å’ŒGPUäº²å’Œæ€§è°ƒåº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŽŸåž‹å®žçŽ°æ˜¾ç¤ºï¼ŒåŠ è½½é€Ÿåº¦æå‡æœ€é«˜è¾¾6.2å€ï¼Œå†·å¯åŠ¨æ—¶é¦–ä»¤ç‰Œæ—¶é—´å‡å°‘23-55%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.

