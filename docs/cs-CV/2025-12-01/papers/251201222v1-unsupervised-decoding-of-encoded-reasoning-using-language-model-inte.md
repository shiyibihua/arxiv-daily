---
layout: default
title: Unsupervised decoding of encoded reasoning using language model interpretability
---

# Unsupervised decoding of encoded reasoning using language model interpretability

**arXiv**: [2512.01222v1](https://arxiv.org/abs/2512.01222) | [PDF](https://arxiv.org/pdf/2512.01222.pdf)

**ä½œè€…**: Ching Fang, Samuel Marks

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽlogit lensçš„æ— ç›‘ç£è§£ç æ–¹æ³•ï¼Œä»¥è¯„ä¼°æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯å¯¹ç¼–ç æŽ¨ç†çš„è§£æžèƒ½åŠ›ã€‚**

**å…³é”®è¯**: `æœºåˆ¶å¯è§£é‡Šæ€§` `ç¼–ç æŽ¨ç†è§£ç ` `logit lensåˆ†æž` `æ— ç›‘ç£è§£ç ` `è¯­è¨€æ¨¡åž‹è¯„ä¼°` `ROT-13åŠ å¯†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†è¿‡ç¨‹å¯èƒ½è¢«ç¼–ç éšè—ï¼Œéœ€è¯„ä¼°çŽ°æœ‰å¯è§£é‡Šæ€§æŠ€æœ¯èƒ½å¦è§£æžæ­¤ç±»ç¼–ç æŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¾®è°ƒæ¨¡åž‹åœ¨ROT-13åŠ å¯†ä¸‹è¿›è¡Œé“¾å¼æŽ¨ç†ï¼Œå¹¶åˆ©ç”¨logit lensåˆ†æžå†…éƒ¨æ¿€æ´»æ¥è§£ç æŽ¨ç†è¿‡ç¨‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šlogit lensåœ¨ä¸­é—´è‡³æ·±å±‚æœ‰æ•ˆè§£ç ï¼Œç»“åˆè‡ªåŠ¨é‡Šä¹‰å®žçŽ°é«˜ç²¾åº¦é‡å»ºæŽ¨ç†æ–‡æœ¬ï¼Œè¡¨æ˜ŽæŠ€æœ¯å¯¹ç®€å•ç¼–ç æŽ¨ç†å…·æœ‰é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

