---
layout: default
title: IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval
---

# IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval

**arXiv**: [2512.01312v1](https://arxiv.org/abs/2512.01312) | [PDF](https://arxiv.org/pdf/2512.01312.pdf)

**ä½œè€…**: Ning Han, Yawen Zeng, Shaohua Long, Chengqing Li, Sijie Yang, Dun Tan, Jianfeng Dong, Jingjing Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIVCR-200Kæ•°æ®é›†ä¸Žå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹æ¡†æž¶ï¼Œä»¥æ”¯æŒäº¤äº’å¼è§†é¢‘è¯­æ–™æ£€ç´¢ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `äº¤äº’å¼è§†é¢‘æ£€ç´¢` `å¤šè½®å¯¹è¯æ•°æ®é›†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†é¢‘è¯­æ–™åº“` `è§†é¢‘æ—¶åˆ»æ£€ç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘æ£€ç´¢ç³»ç»Ÿç¼ºä¹å¤šè½®äº¤äº’ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·ä¸ªæ€§åŒ–éœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé«˜è´¨é‡åŒè¯­å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œå¹¶åŸºäºŽå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹è®¾è®¡äº¤äº’æ¡†æž¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¹¿æ³›å®žéªŒéªŒè¯äº†æ•°æ®é›†å’Œæ¡†æž¶çš„æœ‰æ•ˆæ€§ï¼Œæå‡æ£€ç´¢äº¤äº’ä½“éªŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.

