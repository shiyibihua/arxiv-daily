---
layout: default
title: QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions
---

# QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions

**arXiv**: [2512.01519v1](https://arxiv.org/abs/2512.01519) | [PDF](https://arxiv.org/pdf/2512.01519.pdf)

**ä½œè€…**: Can Polat, Erchin Serpedin, Mustafa Kurban, Hasan Kurban

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQuantumCanvaså¤šæ¨¡æ€åŸºå‡†ï¼Œé€šè¿‡è§†è§‰è¡¨ç¤ºå­¦ä¹ åŽŸå­é—´é‡å­ç›¸äº’ä½œç”¨ä»¥æå‡ç‰©ç†å¯è¿ç§»æ€§ã€‚**

**å…³é”®è¯**: `é‡å­ç›¸äº’ä½œç”¨å­¦ä¹ ` `å¤šæ¨¡æ€åŸºå‡†` `è§†è§‰è¡¨ç¤ºå­¦ä¹ ` `åŽŸå­å¯¹å»ºæ¨¡` `ç‰©ç†å¯è¿ç§»æ€§` `è½¨é“å¯†åº¦å›¾åƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åˆ†å­ä¸Žææ–™æœºå™¨å­¦ä¹ æ¨¡åž‹ç¼ºä¹ç‰©ç†å¯è¿ç§»æ€§ï¼Œæœªå­¦ä¹ åŽŸå­å¯¹é—´çš„é‡å­ç›¸äº’ä½œç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å«å…ƒç´ å¯¹çš„å¤šå±žæ€§æ ‡æ³¨å’ŒåŸºäºŽè½¨é“å¯†åº¦ç­‰çš„åé€šé“å›¾åƒè¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºå‡†æµ‹è¯•æ˜¾ç¤ºæ¨¡åž‹åœ¨èƒ½é‡ç›¸å…³é‡ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œé¢„è®­ç»ƒæå‡åœ¨QM9ç­‰æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.

