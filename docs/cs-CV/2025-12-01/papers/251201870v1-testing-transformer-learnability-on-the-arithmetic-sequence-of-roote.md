---
layout: default
title: Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees
---

# Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees

**arXiv**: [2512.01870v1](https://arxiv.org/abs/2512.01870) | [PDF](https://arxiv.org/pdf/2512.01870.pdf)

**ä½œè€…**: Alessandro Breccia, Federica Gerace, Marco Lippi, Gabriele Sicuro, Pierluigi Contucci

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æµ‹è¯•Transformeråœ¨è‡ªç„¶æ•°è´¨å› æ•°åˆ†è§£æ ‘åºåˆ—ä¸Šçš„å­¦ä¹ èƒ½åŠ›ï¼Œæ­ç¤ºå…¶æ•èŽ·ç®—æœ¯ç»“æž„è§„å¾‹**

**å…³é”®è¯**: `Transformerå­¦ä¹ èƒ½åŠ›` `ç®—æœ¯åºåˆ—` `è´¨å› æ•°åˆ†è§£æ ‘` `GPT-2æž¶æž„` `é¢„æµ‹ä»»åŠ¡` `ç»“æž„è§„å¾‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç©¶å¤§è¯­è¨€æ¨¡åž‹èƒ½å¦å­¦ä¹ ç”±è‡ªç„¶æ•°è¿­ä»£è´¨å› æ•°åˆ†è§£ç”Ÿæˆçš„ç¡®å®šæ€§æ ‘åºåˆ—ï¼Œå³ç®—æœ¯æ–‡æœ¬çš„ç»“æž„è§„å¾‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨GPT-2æž¶æž„çš„Transformerç½‘ç»œï¼Œä»Žå¤´è®­ç»ƒäºŽå‰10^11ä¸ªå…ƒç´ ï¼Œæµ‹è¯•å…¶åœ¨ä¸‹ä¸€ä¸ªè¯å’ŒæŽ©ç è¯é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹éƒ¨åˆ†å­¦ä¹ åˆ°åºåˆ—çš„å†…éƒ¨è¯­æ³•ï¼Œæ•èŽ·éžå¹³å‡¡è§„å¾‹å’Œç›¸å…³æ€§ï¼Œè¡¨æ˜Žå­¦ä¹ èƒ½åŠ›å¯èƒ½æ‰©å±•åˆ°ç®—æœ¯ç»“æž„æœ¬èº«ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \mathbb{N}\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\mathbb{N}\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.

