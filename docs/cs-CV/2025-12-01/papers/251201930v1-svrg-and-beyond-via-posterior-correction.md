---
layout: default
title: SVRG and Beyond via Posterior Correction
---

# SVRG and Beyond via Posterior Correction

**arXiv**: [2512.01930v1](https://arxiv.org/abs/2512.01930) | [PDF](https://arxiv.org/pdf/2512.01930.pdf)

**ä½œè€…**: Nico Daheim, Thomas MÃ¶llenhoff, Ming Liang Ang, Mohammad Emtiyaz Khan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡åŽéªŒæ ¡æ­£è¿žæŽ¥SVRGä¸Žè´å¶æ–¯æ–¹æ³•ï¼Œæå‡ºæ–°å˜ä½“ä»¥æå‡æ·±åº¦ç½‘ç»œè®­ç»ƒ**

**å…³é”®è¯**: `éšæœºæ¢¯åº¦ä¸‹é™` `è´å¶æ–¯æ–¹æ³•` `å˜åˆ†è®­ç»ƒ` `æ·±åº¦ç½‘ç»œä¼˜åŒ–` `åŽéªŒæ ¡æ­£` `Transformeræ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ­ç¤ºSVRGä¸ŽåŽéªŒæ ¡æ­£çš„è´å¶æ–¯è”ç³»ï¼Œå°†å…¶è§†ä¸ºå„å‘åŒæ€§é«˜æ–¯æ—çš„ç‰¹ä¾‹
2. åŸºäºŽé«˜æ–¯æ—æŽ¨å¯¼ç‰›é¡¿å¼å’ŒAdamå¼SVRGå˜ä½“ï¼Œå¼•å…¥Hessianæ ¡æ­£
3. æ–°å˜ä½“åœ¨Transformerè¯­è¨€æ¨¡åž‹çš„é¢„è®­ç»ƒå’Œå¾®è°ƒä¸­è¡¨çŽ°æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.

