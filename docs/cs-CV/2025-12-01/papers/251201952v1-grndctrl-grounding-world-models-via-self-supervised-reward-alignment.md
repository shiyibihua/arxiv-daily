---
layout: default
title: GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment
---

# GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment

**arXiv**: [2512.01952v1](https://arxiv.org/abs/2512.01952) | [PDF](https://arxiv.org/pdf/2512.01952.pdf)

**ä½œè€…**: Haoyang He, Jay Patrikar, Dong-Ki Kim, Max Smith, Daniel McGann, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei, Sebastian Scherer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGrndCtrlæ¡†æž¶ï¼Œé€šè¿‡è‡ªç›‘ç£å¥–åŠ±å¯¹é½å¢žå¼ºä¸–ç•Œæ¨¡åž‹çš„å‡ ä½•åŸºç¡€ï¼Œä»¥æå‡å¯¼èˆªä»»åŠ¡çš„ç©ºé—´ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡åž‹` `è‡ªç›‘ç£å­¦ä¹ ` `å¥–åŠ±å¯¹é½` `å‡ ä½•åŸºç¡€` `å¯¼èˆªä»»åŠ¡` `å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†é¢‘ä¸–ç•Œæ¨¡åž‹ç¼ºä¹å‡ ä½•åŸºç¡€ï¼Œé™åˆ¶å…¶åœ¨å¯¼èˆªä»»åŠ¡ä¸­çš„ç©ºé—´ä¸€è‡´æ€§å’Œé•¿æœŸç¨³å®šæ€§ã€‚
2. å¼•å…¥RLWGæ¡†æž¶ï¼Œåˆ©ç”¨å‡ ä½•å’Œæ„ŸçŸ¥å¥–åŠ±è‡ªç›‘ç£å¯¹é½é¢„è®­ç»ƒä¸–ç•Œæ¨¡åž‹ï¼ŒåŸºäºŽGRPOå®žçŽ°å¥–åŠ±å¯¹é½ã€‚
3. åœ¨æˆ·å¤–çŽ¯å¢ƒä¸­ï¼ŒGrndCtrlä¼˜äºŽç›‘ç£å¾®è°ƒï¼Œå®žçŽ°æ›´ä¼˜çš„ç©ºé—´ä¸€è‡´æ€§å’Œå¯¼èˆªç¨³å®šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.

