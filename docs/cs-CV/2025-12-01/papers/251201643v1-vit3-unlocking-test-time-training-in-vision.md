---
layout: default
title: ViT$^3$: Unlocking Test-Time Training in Vision
---

# ViT$^3$: Unlocking Test-Time Training in Vision

**arXiv**: [2512.01643v1](https://arxiv.org/abs/2512.01643) | [PDF](https://arxiv.org/pdf/2512.01643.pdf)

**ä½œè€…**: Dongchen Han, Yining Li, Tianyu Li, Zixuan Cao, Ziming Wang, Jun Song, Yu Cheng, Bo Zheng, Gao Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViT^3æ¨¡åž‹ï¼Œé€šè¿‡ç³»ç»Ÿç ”ç©¶æµ‹è¯•æ—¶è®­ç»ƒè®¾è®¡ï¼Œå®žçŽ°è§†è§‰åºåˆ—å»ºæ¨¡çš„çº¿æ€§å¤æ‚åº¦ä¸Žå¹¶è¡Œè®¡ç®—ã€‚**

**å…³é”®è¯**: `æµ‹è¯•æ—¶è®­ç»ƒ` `è§†è§‰åºåˆ—å»ºæ¨¡` `çº¿æ€§å¤æ‚åº¦` `æ³¨æ„åŠ›æœºåˆ¶` `åœ¨çº¿å­¦ä¹ ` `è§†è§‰Transformer`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰æµ‹è¯•æ—¶è®­ç»ƒç¼ºä¹è®¾è®¡åŽŸåˆ™ä¸Žå®žç”¨æŒ‡å—ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ³¨æ„åŠ›æ“ä½œé‡æž„ä¸ºåœ¨çº¿å­¦ä¹ é—®é¢˜ï¼Œæž„å»ºç´§å‡‘å†…éƒ¨æ¨¡åž‹ï¼Œå®žçŽ°çº¿æ€§è®¡ç®—å¤æ‚åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒåˆ†ç±»ã€ç”Ÿæˆã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒViT^3åŒ¹é…æˆ–è¶…è¶Šå…ˆè¿›çº¿æ€§æ¨¡åž‹ï¼Œç¼©å°ä¸Žä¼˜åŒ–Transformerçš„å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.

