---
layout: default
title: End-to-end Deep Reinforcement Learning for Stochastic Multi-objective Optimization in C-VRPTW
---

# End-to-end Deep Reinforcement Learning for Stochastic Multi-objective Optimization in C-VRPTW

**arXiv**: [2512.01518v1](https://arxiv.org/abs/2512.01518) | [PDF](https://arxiv.org/pdf/2512.01518.pdf)

**ä½œè€…**: Abdo Abouelrous, Laurens Bliek, Yaoxin Wu, Yingqian Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡åž‹ï¼Œè§£å†³å¸¦æ—¶é—´çª—çš„éšæœºå¤šç›®æ ‡è½¦è¾†è·¯å¾„é—®é¢˜**

**å…³é”®è¯**: `è½¦è¾†è·¯å¾„é—®é¢˜` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¤šç›®æ ‡ä¼˜åŒ–` `éšæœºä¼˜åŒ–` `æ³¨æ„åŠ›æœºåˆ¶` `å¸•ç´¯æ‰˜å‰æ²¿`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè½¦è¾†è·¯å¾„é—®é¢˜ä¸­æ—…è¡Œæ—¶é—´ä¸ç¡®å®šæ€§ä¸Žå¤šç›®æ ‡ï¼ˆæ€»æ—…è¡Œæ—¶é—´å’Œè·¯çº¿å®Œå·¥æ—¶é—´ï¼‰å†²çªçš„è”åˆä¼˜åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæ³¨æ„åŠ›æœºåˆ¶å’Œç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡åœºæ™¯èšç±»è®­ç»ƒæœºåˆ¶å¤„ç†éšæœºæ€§å’Œå¤šç›®æ ‡æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹èƒ½åœ¨å¯æŽ¥å—è¿è¡Œæ—¶é—´å†…æž„å»ºé«˜è´¨é‡å¸•ç´¯æ‰˜å‰æ²¿ï¼Œä¼˜äºŽä¸‰ä¸ªåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this work, we consider learning-based applications in routing to solve a Vehicle Routing variant characterized by stochasticity and multiple objectives. Such problems are representative of practical settings where decision-makers have to deal with uncertainty in the operational environment as well as multiple conflicting objectives due to different stakeholders. We specifically consider travel time uncertainty. We also consider two objectives, total travel time and route makespan, that jointly target operational efficiency and labor regulations on shift length, although different objectives could be incorporated. Learning-based methods offer earnest computational advantages as they can repeatedly solve problems with limited interference from the decision-maker. We specifically focus on end-to-end deep learning models that leverage the attention mechanism and multiple solution trajectories. These models have seen several successful applications in routing problems. However, since travel times are not a direct input to these models due to the large dimensions of the travel time matrix, accounting for uncertainty is a challenge, especially in the presence of multiple objectives. In turn, we propose a model that simultaneously addresses stochasticity and multi-objectivity and provide a refined training mechanism for this model through scenario clustering to reduce training time. Our results show that our model is capable of constructing a Pareto Front of good quality within acceptable run times compared to three baselines.

