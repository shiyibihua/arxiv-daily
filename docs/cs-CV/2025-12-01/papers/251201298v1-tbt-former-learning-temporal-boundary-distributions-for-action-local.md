---
layout: default
title: TBT-Former: Learning Temporal Boundary Distributions for Action Localization
---

# TBT-Former: Learning Temporal Boundary Distributions for Action Localization

**arXiv**: [2512.01298v1](https://arxiv.org/abs/2512.01298) | [PDF](https://arxiv.org/pdf/2512.01298.pdf)

**ä½œè€…**: Thisara Rathnayaka, Uthayasanker Thayasivam

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTBT-Formerï¼Œé€šè¿‡å¢žå¼ºTransformeréª¨å¹²ã€è·¨å°ºåº¦ç‰¹å¾èžåˆå’Œè¾¹ç•Œåˆ†å¸ƒå›žå½’ï¼Œè§£å†³åŠ¨ä½œå®šä½ä¸­è¾¹ç•Œæ¨¡ç³Šå’Œå¤šå°ºåº¦ä¿¡æ¯èžåˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `æ—¶åºåŠ¨ä½œå®šä½` `Transformeræž¶æž„` `ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ` `è¾¹ç•Œåˆ†å¸ƒå›žå½’` `è§†é¢‘ç†è§£` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŠ¨ä½œå®šä½æ¨¡åž‹åœ¨æ¨¡ç³Šè¾¹ç•Œå’Œå¤šå°ºåº¦ä¿¡æ¯èžåˆä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“å®šä½ç²¾åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨é«˜å®¹é‡Transformeréª¨å¹²ã€è·¨å°ºåº¦ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œå’ŒåŸºäºŽå¹¿ä¹‰ç„¦ç‚¹æŸå¤±çš„è¾¹ç•Œåˆ†å¸ƒå›žå½’å¤´ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨THUMOS14å’ŒEPIC-Kitchens 100æ•°æ®é›†ä¸Šè¾¾åˆ°æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨ActivityNet-1.3ä¸Šä¿æŒç«žäº‰åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or "fuzzy" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding

