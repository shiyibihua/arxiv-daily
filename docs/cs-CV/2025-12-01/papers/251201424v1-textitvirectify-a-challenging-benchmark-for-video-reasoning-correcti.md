---
layout: default
title: \textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models
---

# \textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models

**arXiv**: [2512.01424v1](https://arxiv.org/abs/2512.01424) | [PDF](https://arxiv.org/pdf/2512.01424.pdf)

**ä½œè€…**: Xusen Hei, Jiali Chen, Jinyu Yang, Mengchen Zhao, Yi Cai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViRectifyåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘æŽ¨ç†é”™è¯¯çº æ­£ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘æŽ¨ç†çº æ­£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `é”™è¯¯è¯†åˆ«` `è½¨è¿¹è¯æ®` `åŸºå‡†è¯„ä¼°` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å¤æ‚è§†é¢‘æŽ¨ç†ä¸­å¸¸å‡ºé”™ï¼Œç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°å…¶é”™è¯¯è¯†åˆ«ä¸Žçº æ­£èƒ½åŠ›çš„åŸºå‡†
2. é€šè¿‡AIè¾…åŠ©æ ‡æ³¨æž„å»ºè¶…3ä¸‡å®žä¾‹æ•°æ®é›†ï¼Œæ¶µç›–åŠ¨æ€æ„ŸçŸ¥ã€ç§‘å­¦æŽ¨ç†å’Œå…·èº«å†³ç­–é¢†åŸŸï¼Œè¦æ±‚æ¨¡åž‹è¿›è¡Œé€æ­¥é”™è¯¯è¯†åˆ«å’ŒåŸºäºŽè§†é¢‘è¯æ®çš„çº æ­£
3. æå‡ºè½¨è¿¹è¯æ®é©±åŠ¨çš„çº æ­£æ¡†æž¶ï¼Œè¯„ä¼°16ä¸ªå…ˆè¿›æ¨¡åž‹ï¼ŒGPT-5çº æ­£å‡†ç¡®çŽ‡ä»…31.94%ï¼Œæ¡†æž¶ä½¿Qwen2.5-VL-7Bä¼˜äºŽ72Bå˜ä½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.

