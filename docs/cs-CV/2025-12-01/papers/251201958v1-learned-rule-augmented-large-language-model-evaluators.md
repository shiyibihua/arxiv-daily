---
layout: default
title: Learned-Rule-Augmented Large Language Model Evaluators
---

# Learned-Rule-Augmented Large Language Model Evaluators

**arXiv**: [2512.01958v1](https://arxiv.org/abs/2512.01958) | [PDF](https://arxiv.org/pdf/2512.01958.pdf)

**ä½œè€…**: Jie Meng, Jin Mao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§„åˆ™å¢žå¼ºè¯„ä¼°èŒƒå¼ï¼Œé€šè¿‡è§„åˆ™è’¸é¦ä¸Žç­–ç•¥åº”ç”¨æå‡å¤§è¯­è¨€æ¨¡åž‹ä½œä¸ºé€šç”¨è¯„ä¼°å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°` `è§„åˆ™è’¸é¦` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `é“¾å¼è§„åˆ™` `å¼ºåŒ–å­¦ä¹ ` `é€šç”¨è¯„ä¼°å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°å™¨ä¾èµ–äººå·¥è®¾è®¡åŽŸåˆ™ï¼Œæ³›åŒ–å—é™ä¸”ä¸Žæ•°æ®å’Œæ¨¡åž‹ç†è§£ä¸åŒ¹é…ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢è‡ªåŠ¨ä»Žæ•°æ®ä¸­è’¸é¦è¯„åˆ†è§„åˆ™ï¼Œå¹¶è®¾è®¡é“¾å¼è§„åˆ™å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ä»¥å¢žå¼ºæ¨¡åž‹åº”ç”¨è§„åˆ™çš„èƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ï¼Œé€‚ç”¨äºŽå¹¿æ³›è¯„ä¼°åœºæ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

