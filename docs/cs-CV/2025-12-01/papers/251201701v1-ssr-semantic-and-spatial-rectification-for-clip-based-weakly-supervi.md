---
layout: default
title: SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation
---

# SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation

**arXiv**: [2512.01701v1](https://arxiv.org/abs/2512.01701) | [PDF](https://arxiv.org/pdf/2512.01701.pdf)

**ä½œè€…**: Xiuli Bi, Die Xiao, Junchao Fan, Bin Xiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰ä¸Žç©ºé—´æ ¡æ­£æ–¹æ³•ä»¥è§£å†³CLIPå¼±ç›‘ç£åˆ†å‰²ä¸­çš„è¿‡æ¿€æ´»é—®é¢˜**

**å…³é”®è¯**: `å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²` `CLIPæ¨¡åž‹` `è·¨æ¨¡æ€å¯¹é½` `è¶…åƒç´ å¼•å¯¼` `è¯­ä¹‰æ ¡æ­£` `ç©ºé—´æ ¡æ­£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é’ˆå¯¹CLIPå¼±ç›‘ç£åˆ†å‰²ä¸­éžç›®æ ‡å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„è¿‡æ¿€æ´»é—®é¢˜
2. é€šè¿‡è·¨æ¨¡æ€åŽŸåž‹å¯¹é½å’Œè¶…åƒç´ å¼•å¯¼æ ¡æ­£è¿›è¡Œè¯­ä¹‰ä¸Žç©ºé—´æ ¡æ­£
3. åœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå–å¾—é¢†å…ˆçš„mIoUåˆ†æ•°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.

