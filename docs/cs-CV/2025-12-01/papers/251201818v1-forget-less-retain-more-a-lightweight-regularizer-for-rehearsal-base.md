---
layout: default
title: Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning
---

# Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning

**arXiv**: [2512.01818v1](https://arxiv.org/abs/2512.01818) | [PDF](https://arxiv.org/pdf/2512.01818.pdf)

**ä½œè€…**: Lama Alssum, Hasan Abed Al Kader Hammoud, Motasem Alfarra, Juan C Leon Alcazar, Bernard Ghanem

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¿¡æ¯æœ€å¤§åŒ–æ­£åˆ™å™¨ä»¥è§£å†³åŸºäºŽå›žæ”¾çš„æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `æ­£åˆ™åŒ–æ–¹æ³•` `å›žæ”¾ç­–ç•¥` `ä¿¡æ¯æœ€å¤§åŒ–` `è§†é¢‘å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦ç¥žç»ç½‘ç»œåœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ–°ä»»åŠ¡è®­ç»ƒå¯¼è‡´æ—§ä»»åŠ¡æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé¢„æœŸæ ‡ç­¾åˆ†å¸ƒè®¾è®¡ç±»æ— å…³æ­£åˆ™å™¨ï¼Œè½»é‡é›†æˆåˆ°å›žæ”¾æ–¹æ³•ä¸­å‡å°‘é—å¿˜å¹¶åŠ é€Ÿæ”¶æ•›
3. å®žéªŒæˆ–æ•ˆæžœï¼šè·¨æ•°æ®é›†å’Œä»»åŠ¡æ•°éªŒè¯æ€§èƒ½æå‡ï¼Œè®¡ç®—å¼€é”€å°ï¼Œé€‚ç”¨äºŽè§†é¢‘æ•°æ®ç­‰å®žé™…åœºæ™¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.

