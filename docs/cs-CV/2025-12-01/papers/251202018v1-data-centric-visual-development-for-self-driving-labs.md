---
layout: default
title: Data-Centric Visual Development for Self-Driving Labs
---

# Data-Centric Visual Development for Self-Driving Labs

**arXiv**: [2512.02018v1](https://arxiv.org/abs/2512.02018) | [PDF](https://arxiv.org/pdf/2512.02018.pdf)

**ä½œè€…**: Anbang Liu, Guanzhong Hu, Jiayi Wang, Ping Guo, Han Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆçœŸå®žä¸Žè™šæ‹Ÿæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³è‡ªé©±åŠ¨å®žéªŒå®¤ä¸­ç§»æ¶²æ°”æ³¡æ£€æµ‹çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚**

**å…³é”®è¯**: `è‡ªé©±åŠ¨å®žéªŒå®¤` `ç§»æ¶²æ£€æµ‹` `æ•°æ®å¢žå¼º` `æ··åˆæ•°æ®ç”Ÿæˆ` `æ°”æ³¡æ£€æµ‹` `è§†è§‰åé¦ˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªé©±åŠ¨å®žéªŒå®¤ä¸­ç§»æ¶²æ“ä½œç²¾åº¦è¦æ±‚é«˜ï¼Œä½†è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼Œå°¤å…¶æ˜¯è´Ÿæ ·æœ¬éš¾ä»¥èŽ·å–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºæ··åˆç®¡é“ï¼Œç»“åˆäººæœºååŒçš„çœŸå®žæ•°æ®é‡‡é›†ä¸ŽåŸºäºŽå‚è€ƒæ¡ä»¶çš„è™šæ‹Ÿå›¾åƒç”Ÿæˆï¼Œä»¥å¢žå¼ºæ•°æ®é›†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæµ‹è¯•é›†ä¸Šï¼Œä»…ç”¨çœŸå®žæ•°æ®è®­ç»ƒçš„æ¨¡åž‹å‡†ç¡®çŽ‡è¾¾99.6%ï¼Œæ··åˆæ•°æ®è®­ç»ƒç»´æŒ99.4%å‡†ç¡®çŽ‡ï¼Œé™ä½Žæ•°æ®æ”¶é›†è´Ÿæ‹…ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.

