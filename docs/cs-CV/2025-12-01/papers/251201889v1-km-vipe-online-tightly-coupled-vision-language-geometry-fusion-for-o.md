---
layout: default
title: KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM
---

# KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM

**arXiv**: [2512.01889v1](https://arxiv.org/abs/2512.01889) | [PDF](https://arxiv.org/pdf/2512.01889.pdf)

**ä½œè€…**: Zaid Nasser, Mikhail Iumanov, Tianhao Li, Maxim Popov, Jaafar Mahmoud, Malik Mohrat, Ilya Obrubov, Ekaterina Derevyanka, Ivan Sosin, Sergey Kolyubin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKM-ViPEæ¡†æž¶ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€-å‡ ä½•ç´§è€¦åˆå®žçŽ°åŠ¨æ€çŽ¯å¢ƒä¸­æœªæ ‡å®šå•ç›®ç›¸æœºçš„å®žæ—¶å¼€æ”¾è¯æ±‡è¯­ä¹‰SLAMã€‚**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡è¯­ä¹‰SLAM` `è§†è§‰-è¯­è¨€-å‡ ä½•èžåˆ` `åŠ¨æ€åœºæ™¯é²æ£’æ€§` `æœªæ ‡å®šå•ç›®ç›¸æœº` `åœ¨çº¿å®žæ—¶æ“ä½œ` `è‡ªé€‚åº”é²æ£’æ ¸`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰SLAMç³»ç»Ÿä¾èµ–æ·±åº¦ä¼ æ„Ÿå™¨ã€ç¦»çº¿æ ¡å‡†æˆ–ç¼ºä¹åŠ¨æ€åœºæ™¯é²æ£’æ€§ï¼Œé™åˆ¶äº†åœ¨æœªæ ‡å®šå•ç›®ç›¸æœºå’ŒåŠ¨æ€çŽ¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç´§è€¦åˆDINOè§†è§‰ç‰¹å¾ä¸Žå‡ ä½•çº¦æŸï¼Œä½¿ç”¨åŸºäºŽé«˜çº§ç‰¹å¾çš„è‡ªé€‚åº”é²æ£’æ ¸å¤„ç†ç§»åŠ¨å’Œå¯ç§»åŠ¨é™æ€ç‰©ä½“ï¼Œåœ¨çº¿èžåˆå‡ ä½•ä¸Žè¯­è¨€å¯¹é½çš„è§†è§‰ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å®žæ—¶æ“ä½œä¸­å®žçŽ°ç«žäº‰æ€§æ€§èƒ½ï¼Œé€‚ç”¨äºŽè‡ªä¸»æœºå™¨äººå’ŒAR/VRï¼Œæå‡å…·èº«AIçš„ç©ºé—´æ™ºèƒ½èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.

