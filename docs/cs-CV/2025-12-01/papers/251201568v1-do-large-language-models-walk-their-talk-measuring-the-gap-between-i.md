---
layout: default
title: Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism
---

# Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism

**arXiv**: [2512.01568v1](https://arxiv.org/abs/2512.01568) | [PDF](https://arxiv.org/pdf/2512.01568.pdf)

**ä½œè€…**: Sandro Andric

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ ¡å‡†å·®è·ä½œä¸ºæ ‡å‡†åŒ–å¯¹é½æŒ‡æ ‡ï¼Œä»¥è¡¡é‡å¤§è¯­è¨€æ¨¡åž‹åœ¨åˆ©ä»–ä¸»ä¹‰ä¸­çš„è¨€è¡Œå·®å¼‚ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å¯¹é½` `éšå«å…³è”æµ‹è¯•` `è¡Œä¸ºåˆ©ä»–ä¸»ä¹‰` `è‡ªæˆ‘æŠ¥å‘Šæ ¡å‡†` `æ¨¡åž‹ä¸€è‡´æ€§è¯„ä¼°` `ç¤¾ä¼šå¿ƒç†å­¦æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹æ˜¯å¦è¡¨çŽ°å‡ºåˆ©ä»–å€¾å‘ï¼Œå…¶éšå«å…³è”å’Œè‡ªæˆ‘æŠ¥å‘Šèƒ½å¦é¢„æµ‹å®žé™…è¡Œä¸ºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šæ–¹æ³•æµ‹è¯•24ä¸ªå‰æ²¿æ¨¡åž‹ï¼ŒåŒ…æ‹¬éšå«å…³è”æµ‹è¯•ã€è¡Œä¸ºä»»åŠ¡å’Œè‡ªæˆ‘è¯„ä¼°é‡è¡¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹æ™®éé«˜ä¼°è‡ªèº«åˆ©ä»–è¡Œä¸ºï¼Œéšå«å…³è”ä¸é¢„æµ‹è¡Œä¸ºï¼Œæ ¡å‡†å·®è·æ˜¾è‘—å½±å“æ¨¡åž‹ä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.

