---
layout: default
title: On the Tension Between Optimality and Adversarial Robustness in Policy Optimization
---

# On the Tension Between Optimality and Adversarial Robustness in Policy Optimization

**arXiv**: [2512.01228v1](https://arxiv.org/abs/2512.01228) | [PDF](https://arxiv.org/pdf/2512.01228.pdf)

**ä½œè€…**: Haoran Li, Jiayu Lv, Congying Han, Zicheng Zhang, Anqi Li, Yan Liu, Tiande Guo, Nan Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBARPOåŒå±‚æ¡†æž¶ä»¥è°ƒå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥ä¼˜åŒ–çš„æœ€ä¼˜æ€§ä¸Žå¯¹æŠ—é²æ£’æ€§**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—é²æ£’æ€§` `ç­–ç•¥ä¼˜åŒ–` `åŒå±‚ä¼˜åŒ–` `å…¨å±€æ™¯è§‚åˆ†æž` `ç†è®ºå®žè·µå·®è·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ ‡å‡†ç­–ç•¥ä¼˜åŒ–ä¸Žå¯¹æŠ—é²æ£’ç­–ç•¥ä¼˜åŒ–åœ¨ç†è®ºä¸€è‡´ä½†å®žè·µä¸­å­˜åœ¨æœ€ä¼˜æ€§ä¸Žé²æ£’æ€§å†²çª
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è°ƒåˆ¶å¯¹æŠ—è€…å¼ºåº¦ç»Ÿä¸€ä¸¤ç§ä¼˜åŒ–ï¼Œç¼“è§£å…¨å±€æ™¯è§‚å¤æ‚æ€§ï¼Œæå‡å¯¼èˆªæ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šBARPOåœ¨å¹¿æ³›å®žéªŒä¸­ä¼˜äºŽåŸºçº¿ARPOï¼Œå®žçŽ°ç†è®ºä¸Žå®žè¯æ€§èƒ½çš„è°ƒå’Œ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving optimality and adversarial robustness in deep reinforcement learning has long been regarded as conflicting goals. Nonetheless, recent theoretical insights presented in CAR suggest a potential alignment, raising the important question of how to realize this in practice. This paper first identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). Although they share theoretical consistency, a fundamental tension between robustness and optimality arises in practical policy gradient methods. SPO tends toward convergence to vulnerable first-order stationary policies (FOSPs) with strong natural performance, whereas ARPO typically favors more robust FOSPs at the expense of reduced returns. Furthermore, we attribute this tradeoff to the reshaping effect of the strongest adversary in ARPO, which significantly complicates the global landscape by inducing deceptive sticky FOSPs. This improves robustness but makes navigation more challenging. To alleviate this, we develop the BARPO, a bilevel framework unifying SPO and ARPO by modulating adversary strength, thereby facilitating navigability while preserving global optima. Extensive empirical results demonstrate that BARPO consistently outperforms vanilla ARPO, providing a practical approach to reconcile theoretical and empirical performance.

