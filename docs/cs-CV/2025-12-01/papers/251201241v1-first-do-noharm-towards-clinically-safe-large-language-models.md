---
layout: default
title: First, do NOHARM: towards clinically safe large language models
---

# First, do NOHARM: towards clinically safe large language models

**arXiv**: [2512.01241v1](https://arxiv.org/abs/2512.01241) | [PDF](https://arxiv.org/pdf/2512.01241.pdf)

**ä½œè€…**: David Wu, Fateme Nateghi Haredasht, Saloni Kumar Maharaj, Priyank Jain, Jessica Tran, Matthew Gwiazdon, Arjun Rustagi, Jenelle Jindal, Jacob M. Koshy, Vinay Kadiyala, Anup Agarwal, Bassman Tappuni, Brianna French, Sirus Jesudasen, Christopher V. Cosgriff, Rebanta Chakraborty, Jillian Caldwell, Susan Ziolkowski, David J. Iberri, Robert Diep, Rahul S. Dalal, Kira L. Newman, Kristin Galetta, J. Carl Pallais, Nancy Wei, Kathleen M. Buchheit, David I. Hong, Ernest Y. Lee, Allen Shih, Vartan Pahalyants, Tamara B. Kaplan, Vishnu Ravi, Sarita Khemani, April S. Liang, Daniel Shirvani, Advait Patil, Nicholas Marshall, Kanav Chopra, Joel Koh, Adi Badhwar, Liam G. McCoy, David J. H. Wu, Yingjie Weng, Sumant Ranji, Kevin Schulman, Nigam H. Shah, Jason Hom, Arnold Milstein, Adam Rodman, Jonathan H. Chen, Ethan Goh

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNOHARMåŸºå‡†ä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡åž‹åœ¨åŒ»ç–—å’¨è¯¢ä¸­çš„ä¸´åºŠå®‰å…¨æ€§**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `ä¸´åºŠå®‰å…¨æ€§è¯„ä¼°` `åŒ»ç–—å’¨è¯¢åŸºå‡†` `å±å®³åˆ†æž` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨åŒ»ç–—å»ºè®®ä¸­çš„ä¸´åºŠå®‰å…¨æ€§æœªå……åˆ†è¯„ä¼°ï¼Œå­˜åœ¨æ½œåœ¨å±å®³é£Žé™©ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽ100ä¸ªçœŸå®žåˆçº§åˆ°ä¸“ç§‘å’¨è¯¢æ¡ˆä¾‹ï¼Œæž„å»ºNOHARMåŸºå‡†ï¼Œæ¶µç›–10ä¸ªä¸“ç§‘ï¼Œé€šè¿‡ä¸“å®¶æ ‡æ³¨é‡åŒ–å±å®³é¢‘çŽ‡ä¸Žä¸¥é‡æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨31ä¸ªæ¨¡åž‹ä¸­ï¼Œä¸¥é‡å±å®³çŽ‡é«˜è¾¾22.2%ï¼Œé—æ¼å±å®³å é”™è¯¯76.6%ï¼Œå®‰å…¨æ€§èƒ½ä¸ŽçŽ°æœ‰åŸºå‡†ç›¸å…³æ€§ä¸­ç­‰ï¼Œå¤šæ™ºèƒ½ä½“æ–¹æ³•å¯é™ä½Žå±å®³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) are routinely used by physicians and patients for medical advice, yet their clinical safety profiles remain poorly characterized. We present NOHARM (Numerous Options Harm Assessment for Risk in Medicine), a benchmark using 100 real primary-care-to-specialist consultation cases to measure harm frequency and severity from LLM-generated medical recommendations. NOHARM covers 10 specialties, with 12,747 expert annotations for 4,249 clinical management options. Across 31 LLMs, severe harm occurs in up to 22.2% (95% CI 21.6-22.8%) of cases, with harms of omission accounting for 76.6% (95% CI 76.4-76.8%) of errors. Safety performance is only moderately correlated (r = 0.61-0.64) with existing AI and medical knowledge benchmarks. The best models outperform generalist physicians on safety (mean difference 9.7%, 95% CI 7.0-12.5%), and a diverse multi-agent approach reduces harm compared to solo models (mean difference 8.0%, 95% CI 4.0-12.1%). Therefore, despite strong performance on existing evaluations, widely used AI models can produce severely harmful medical advice at nontrivial rates, underscoring clinical safety as a distinct performance dimension necessitating explicit measurement.

