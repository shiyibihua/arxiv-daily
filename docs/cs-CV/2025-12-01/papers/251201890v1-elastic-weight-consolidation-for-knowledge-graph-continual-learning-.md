---
layout: default
title: Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical Evaluation
---

# Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical Evaluation

**arXiv**: [2512.01890v1](https://arxiv.org/abs/2512.01890) | [PDF](https://arxiv.org/pdf/2512.01890.pdf)

**ä½œè€…**: Gaganpreet Jhajj, Fuhua Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¼¹æ€§æƒé‡å·©å›ºåœ¨çŸ¥è¯†å›¾è°±æŒç»­å­¦ä¹ ä¸­ç¼“è§£ç¾éš¾æ€§é—å¿˜çš„æ•ˆæžœ**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `å¼¹æ€§æƒé‡å·©å›º` `é“¾æŽ¥é¢„æµ‹` `TransEåµŒå…¥` `FB15k-237`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŸ¥è¯†å›¾è°±æŒç»­æ›´æ–°æ—¶ï¼Œç¥žç»åµŒå…¥æ¨¡åž‹é¢ä¸´ç¾éš¾æ€§é—å¿˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¼¹æ€§æƒé‡å·©å›ºæ­£åˆ™åŒ–æ–¹æ³•ï¼ŒåŸºäºŽTransEåµŒå…¥åœ¨FB15k-237æ•°æ®é›†ä¸Šè¯„ä¼°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šEWCå°†é—å¿˜çŽ‡ä»Ž12.62%é™è‡³6.85%ï¼Œä»»åŠ¡åˆ’åˆ†ç­–ç•¥å½±å“é—å¿˜ç¨‹åº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge graphs (KGs) require continual updates as new information emerges, but neural embedding models suffer from catastrophic forgetting when learning new tasks sequentially. We evaluate Elastic Weight Consolidation (EWC), a regularization-based continual learning method, on KG link prediction using TransE embeddings on FB15k-237. Across multiple experiments with five random seeds, we find that EWC reduces catastrophic forgetting from 12.62% to 6.85%, a 45.7% reduction compared to naive sequential training. We observe that the task partitioning strategy affects the magnitude of forgetting: relation-based partitioning (grouping triples by relation type) exhibits 9.8 percentage points higher forgetting than randomly partitioned tasks (12.62% vs 2.81%), suggesting that task construction influences evaluation outcomes. While focused on a single embedding model and dataset, our results demonstrate that EWC effectively mitigates catastrophic forgetting in KG continual learning and highlight the importance of evaluation protocol design.

