---
layout: default
title: NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction
---

# NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction

**arXiv**: [2512.01550v1](https://arxiv.org/abs/2512.01550) | [PDF](https://arxiv.org/pdf/2512.01550.pdf)

**ä½œè€…**: Fei Liu, Shichao Xie, Minghua Luo, Zedong Chu, Junjun Hu, Xiaolong Wu, Mu Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNavForeseeç»Ÿä¸€è§†è§‰è¯­è¨€ä¸–ç•Œæ¨¡åž‹ï¼Œä»¥è§£å†³é•¿è§†é‡Žä»»åŠ¡ä¸­åŸºäºŽè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¯¼èˆªè§„åˆ’ä¸Žé¢„æµ‹æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `é•¿è§†é‡Žå¯¼èˆª` `ä¸–ç•Œæ¨¡åž‹é¢„æµ‹` `ä»»åŠ¡è§„åˆ’` `è‡ªç„¶è¯­è¨€æŒ‡ä»¤` `æ„ŸçŸ¥-è§„åˆ’-è¡ŒåŠ¨å¾ªçŽ¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ™ºèƒ½ä½“åœ¨æœªçŸ¥çŽ¯å¢ƒä¸­è¿›è¡Œé•¿è§†é‡Žå¯¼èˆªæ—¶ï¼Œè§„åˆ’èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´é«˜å¤±è´¥çŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šNavForeseeå°†é«˜å±‚è¯­è¨€è§„åˆ’ä¸Žç”Ÿæˆå¼ä¸–ç•Œæ¨¡åž‹é¢„æµ‹ç»Ÿä¸€äºŽå•ä¸€æ¡†æž¶ï¼Œå®žçŽ°ä»»åŠ¡åˆ†è§£ã€è¿›åº¦è·Ÿè¸ªå’ŒåŒè§†é‡Žé¢„æµ‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨R2R-CEå’ŒRxR-CEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒNavForeseeåœ¨å¤æ‚åœºæ™¯ä¸‹è¡¨çŽ°å‡ºé«˜åº¦ç«žäº‰åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.

