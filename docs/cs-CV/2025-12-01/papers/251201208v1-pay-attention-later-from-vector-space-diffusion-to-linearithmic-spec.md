---
layout: default
title: Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking
---

# Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking

**arXiv**: [2512.01208v1](https://arxiv.org/abs/2512.01208) | [PDF](https://arxiv.org/pdf/2512.01208.pdf)

**ä½œè€…**: Alper YÄ±ldÄ±rÄ±m, Ä°brahim YÃ¼cedaÄŸ

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRISMæ¨¡åž‹ä»¥è§£å†³Transformeråœ¨å®žæ—¶çŸ¥è¯†é€‚åº”ä¸­çš„å¯å¡‘æ€§-ç¨³å®šæ€§å›°å¢ƒ**

**å…³é”®è¯**: `Transformerä¼˜åŒ–` `å¯å¡‘æ€§-ç¨³å®šæ€§å›°å¢ƒ` `è°æ³¢è¡¨ç¤º` `é—¨æŽ§è°æ³¢å·ç§¯` `å®žæ—¶çŸ¥è¯†é€‚åº”` `WMT14ç¿»è¯‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ‡å‡†Transformerå­˜åœ¨è¯­ä¹‰å¯¹é½ç¨Žï¼Œå¯¼è‡´ä¼˜åŒ–æˆæœ¬é«˜ä¸”éš¾ä»¥é€‚åº”æ–°æ¦‚å¿µ
2. PRISMä½¿ç”¨å¤åŸŸè°æŒ¯é¢‘çŽ‡ç¼–ç è¯­ä¹‰ï¼Œä»¥é—¨æŽ§è°æ³¢å·ç§¯æ›¿ä»£äºŒæ¬¡è‡ªæ³¨æ„åŠ›
3. åœ¨WMT14ç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒPRISMå±•ç¤ºæ— æŸå¯å¡‘æ€§ï¼Œè€ŒTransformerå‡ºçŽ°ç¾éš¾æ€§é—å¿˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Standard Transformers suffer from a "Semantic Alignment Tax", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates "Catastrophic Rigidity", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the "Plasticity-Stability" stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.

