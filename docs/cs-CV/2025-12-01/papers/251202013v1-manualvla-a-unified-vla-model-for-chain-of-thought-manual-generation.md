---
layout: default
title: ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation
---

# ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation

**arXiv**: [2512.02013v1](https://arxiv.org/abs/2512.02013) | [PDF](https://arxiv.org/pdf/2512.02013.pdf)

**ä½œè€…**: Chenyang Gu, Jiaming Liu, Hao Chen, Runzhong Huang, Qingpo Wuwu, Zhuoyang Liu, Xiaoqi Li, Ying Li, Renrui Zhang, Peng Jia, Pheng-Ann Heng, Shanghang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºManualVLAç»Ÿä¸€æ¡†æž¶ï¼Œé€šè¿‡ç”Ÿæˆå¤šæ¨¡æ€æ‰‹å†Œè§£å†³é•¿æ—¶ç¨‹æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸Žæ‰§è¡Œé—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `é•¿æ—¶ç¨‹ä»»åŠ¡è§„åˆ’` `å¤šæ¨¡æ€æ‰‹å†Œç”Ÿæˆ` `æœºå™¨äººæ“ä½œ` `3Dé«˜æ–¯æº…å°„` `æ•°å­—å­ªç”Ÿå·¥å…·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLAæ¨¡åž‹åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­éš¾ä»¥åè°ƒé«˜å±‚è§„åˆ’ä¸Žç²¾ç¡®æ“ä½œã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽMixture-of-Transformersæž¶æž„ï¼Œé›†æˆè§„åˆ’ä¸“å®¶ç”Ÿæˆå¤šæ¨¡æ€æ‰‹å†Œï¼Œå¹¶é€šè¿‡ManualCoTæŽ¨ç†æŒ‡å¯¼åŠ¨ä½œæ‰§è¡Œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LEGOç»„è£…å’Œç‰©ä½“é‡æŽ’ä»»åŠ¡ä¸­ï¼Œå¹³å‡æˆåŠŸçŽ‡æ¯”åŸºçº¿é«˜32%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the "how" process from the "what" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.

