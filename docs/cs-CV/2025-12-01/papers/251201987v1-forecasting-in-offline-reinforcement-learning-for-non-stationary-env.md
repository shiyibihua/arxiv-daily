---
layout: default
title: Forecasting in Offline Reinforcement Learning for Non-stationary Environments
---

# Forecasting in Offline Reinforcement Learning for Non-stationary Environments

**arXiv**: [2512.01987v1](https://arxiv.org/abs/2512.01987) | [PDF](https://arxiv.org/pdf/2512.01987.pdf)

**ä½œè€…**: Suzan Ece Ada, Georg Martius, Emre Ugur, Erhan Oztop

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFORLæ¡†æž¶ä»¥è§£å†³éžå¹³ç¨³çŽ¯å¢ƒä¸­ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `éžå¹³ç¨³çŽ¯å¢ƒ` `æ¡ä»¶æ‰©æ•£æ¨¡åž‹` `é›¶æ ·æœ¬é¢„æµ‹` `æ—¶é—´åºåˆ—åˆ†æž` `éƒ¨åˆ†å¯è§‚æµ‹æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨éžå¹³ç¨³çŽ¯å¢ƒä¸­å› åç§»å¯¼è‡´éƒ¨åˆ†å¯è§‚æµ‹æ€§å’Œæ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ¡ä»¶æ‰©æ•£å€™é€‰çŠ¶æ€ç”Ÿæˆå’Œé›¶æ ·æœ¬æ—¶é—´åºåˆ—åŸºç¡€æ¨¡åž‹è¿›è¡Œé¢„æµ‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹ŸçœŸå®žéžå¹³ç¨³æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFORLç›¸æ¯”åŸºçº¿æ–¹æ³•æ€§èƒ½æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.

