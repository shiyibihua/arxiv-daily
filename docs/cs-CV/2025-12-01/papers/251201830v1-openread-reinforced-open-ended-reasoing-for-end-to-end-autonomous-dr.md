---
layout: default
title: OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic
---

# OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic

**arXiv**: [2512.01830v1](https://arxiv.org/abs/2512.01830) | [PDF](https://arxiv.org/pdf/2512.01830.pdf)

**ä½œè€…**: Songyan Zhang, Wenhui Huang, Zhan Chen, Chua Jiahao Collister, Qihang Huang, Chen Lv

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenREADæ¡†æž¶ï¼Œé€šè¿‡LLMä½œä¸ºè¯„åˆ¤è€…å®žçŽ°ç«¯åˆ°ç«¯å¼ºåŒ–å¾®è°ƒï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­å¼€æ”¾ç«¯æŽ¨ç†çš„é‡åŒ–éš¾é¢˜ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å¾®è°ƒ` `å¼€æ”¾ç«¯æŽ¨ç†` `å¤§è¯­è¨€æ¨¡åž‹` `ç«¯åˆ°ç«¯å­¦ä¹ ` `æ€ç»´é“¾æ ‡æ³¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œç›‘ç£å¾®è°ƒæ³›åŒ–æœ‰é™ï¼Œå¼ºåŒ–å¾®è°ƒéš¾ä»¥é‡åŒ–å¼€æ”¾ç«¯æŽ¨ç†çš„å¥–åŠ±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡æ€ç»´é“¾æ ‡æ³¨ï¼Œåˆ©ç”¨Qwen3å¤§è¯­è¨€æ¨¡åž‹ä½œä¸ºè¯„åˆ¤è€…ï¼Œé‡åŒ–å¼€æ”¾ç«¯é—®é¢˜çš„æŽ¨ç†è´¨é‡è¿›è¡Œå¥–åŠ±å»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç«¯åˆ°ç«¯å¼ºåŒ–å¾®è°ƒåœ¨ä¸Šä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æå‡ï¼Œåœ¨æŽ¨ç†å’Œè§„åˆ’åŸºå‡†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.

