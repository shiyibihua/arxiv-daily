---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-31
---

# cs.CVï¼ˆ2025-12-31ï¼‰

ğŸ“Š å…± **14** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251224851v1-vln-mme-diagnosing-mllms-as-language-guided-visual-navigation-agents.html">VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents</a></td>
  <td>VLN-MMEï¼šè¯Šæ–­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€å¼•å¯¼è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24851v1" data-paper-url="./papers/251224851v1-vln-mme-diagnosing-mllms-as-language-guided-visual-navigation-agents.html" onclick="toggleFavorite(this, '2512.24851v1', 'VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251224903v1-finmmdocr-benchmarking-financial-multimodal-reasoning-with-scenario-.html">FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</a></td>
  <td>FinMMDocRï¼šæå‡ºé‡‘èå¤šæ¨¡æ€æ¨ç†åŸºå‡†ï¼Œå…³æ³¨åœºæ™¯æ„ŸçŸ¥ã€æ–‡æ¡£ç†è§£å’Œå¤šæ­¥è®¡ç®—ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24903v1" data-paper-url="./papers/251224903v1-finmmdocr-benchmarking-financial-multimodal-reasoning-with-scenario-.html" onclick="toggleFavorite(this, '2512.24903v1', 'FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251224605v1-monirefer-a-real-world-large-scale-multi-modal-dataset-based-on-road.html">MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</a></td>
  <td>æå‡ºMoniReferæ•°æ®é›†ï¼Œç”¨äºè·¯ä¾§åŸºç¡€è®¾æ–½çš„3Dè§†è§‰å®šä½ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24605v1" data-paper-url="./papers/251224605v1-monirefer-a-real-world-large-scale-multi-modal-dataset-based-on-road.html" onclick="toggleFavorite(this, '2512.24605v1', 'MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251224561v1-rgbt-ground-benchmark-visual-grounding-beyond-rgb-in-complex-real-wo.html">RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios</a></td>
  <td>æå‡ºRGBT-GroundåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤æ‚åœºæ™¯ä¸‹RGB-Tå›¾åƒçš„è§†è§‰å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24561v1" data-paper-url="./papers/251224561v1-rgbt-ground-benchmark-visual-grounding-beyond-rgb-in-complex-real-wo.html" onclick="toggleFavorite(this, '2512.24561v1', 'RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251224731v1-echofoley-event-centric-hierarchical-control-for-video-grounded-crea.html">EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation</a></td>
  <td>æå‡ºEchoFoleyï¼Œé€šè¿‡äº‹ä»¶ä¸­å¿ƒçš„åˆ†å±‚æ§åˆ¶å®ç°è§†é¢‘ç›¸å…³çš„åˆ›æ„å£°éŸ³ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24731v1" data-paper-url="./papers/251224731v1-echofoley-event-centric-hierarchical-control-for-video-grounded-crea.html" onclick="toggleFavorite(this, '2512.24731v1', 'EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>6</td>
  <td><a href="./papers/251224742v1-splatwizard-a-benchmark-toolkit-for-3d-gaussian-splatting-compressio.html">Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression</a></td>
  <td>Splatwizardï¼šç”¨äº3Dé«˜æ–¯æº…å°„å‹ç¼©çš„ç»¼åˆåŸºå‡†æµ‹è¯•å·¥å…·åŒ…</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24742v1" data-paper-url="./papers/251224742v1-splatwizard-a-benchmark-toolkit-for-3d-gaussian-splatting-compressio.html" onclick="toggleFavorite(this, '2512.24742v1', 'Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251225008v1-foundationslam-unleashing-the-power-of-depth-foundation-models-for-e.html">FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</a></td>
  <td>FoundationSLAMï¼šåˆ©ç”¨æ·±åº¦åŸºç¡€æ¨¡å‹å®ç°ç«¯åˆ°ç«¯ç¨ å¯†è§†è§‰SLAM</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">geometric consistency</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.25008v1" data-paper-url="./papers/251225008v1-foundationslam-unleashing-the-power-of-depth-foundation-models-for-e.html" onclick="toggleFavorite(this, '2512.25008v1', 'FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251224792v1-projection-based-adversarial-attack-using-physics-in-the-loop-optimi.html">Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation</a></td>
  <td>æå‡ºåŸºäºç‰©ç†ç¯è·¯ä¼˜åŒ–çš„æŠ•å½±å¯¹æŠ—æ”»å‡»ï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24792v1" data-paper-url="./papers/251224792v1-projection-based-adversarial-attack-using-physics-in-the-loop-optimi.html" onclick="toggleFavorite(this, '2512.24792v1', 'Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251224946v1-haineifrdm-explore-diffusion-to-restore-defects-in-fast-movement-fil.html">HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films</a></td>
  <td>æå‡ºHaineiFRDMï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¿®å¤å¿«é€Ÿç§»åŠ¨å½±ç‰‡ä¸­çš„ç¼ºé™·</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24946v1" data-paper-url="./papers/251224946v1-haineifrdm-explore-diffusion-to-restore-defects-in-fast-movement-fil.html" onclick="toggleFavorite(this, '2512.24946v1', 'HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/251224763v1-unic-lift-unified-3d-instance-segmentation-via-contrastive-learning.html">UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning</a></td>
  <td>UniC-Liftï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ å®ç°ç»Ÿä¸€çš„3Då®ä¾‹åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24763v1" data-paper-url="./papers/251224763v1-unic-lift-unified-3d-instance-segmentation-via-contrastive-learning.html" onclick="toggleFavorite(this, '2512.24763v1', 'UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251224551v1-phygdpo-physics-aware-groupwise-direct-preference-optimization-for-p.html">PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation</a></td>
  <td>æå‡ºPhyGDPOæ¡†æ¶ï¼Œé€šè¿‡ç‰©ç†æ„ŸçŸ¥çš„ç¾¤ä½“åå¥½ä¼˜åŒ–å®ç°ç‰©ç†ä¸€è‡´çš„æ–‡æœ¬ç”Ÿæˆè§†é¢‘ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">direct preference optimization</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24551v1" data-paper-url="./papers/251224551v1-phygdpo-physics-aware-groupwise-direct-preference-optimization-for-p.html" onclick="toggleFavorite(this, '2512.24551v1', 'PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/251224965v1-showui-Ï€-flow-based-generative-models-as-gui-dexterous-hands.html">ShowUI-$Ï€$: Flow-based Generative Models as GUI Dexterous Hands</a></td>
  <td>ShowUI-$Ï€$ï¼šæå‡ºåŸºäºFlowçš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°GUIç•Œé¢çš„çµå·§æ“ä½œã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24965v1" data-paper-url="./papers/251224965v1-showui-Ï€-flow-based-generative-models-as-gui-dexterous-hands.html" onclick="toggleFavorite(this, '2512.24965v1', 'ShowUI-$Ï€$: Flow-based Generative Models as GUI Dexterous Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251224547v1-hierarchical-vector-quantized-latents-for-perceptual-low-resolution-.html">Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression</a></td>
  <td>æå‡ºä¸€ç§åˆ†å±‚çŸ¢é‡é‡åŒ–éšå˜é‡çš„æ„ŸçŸ¥ä½åˆ†è¾¨ç‡è§†é¢‘å‹ç¼©æ–¹æ³•ï¼Œé€‚ç”¨äºå¸¦å®½å—é™åœºæ™¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">VQ-VAE</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.24547v1" data-paper-url="./papers/251224547v1-hierarchical-vector-quantized-latents-for-perceptual-low-resolution-.html" onclick="toggleFavorite(this, '2512.24547v1', 'Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251225073v1-gamo-geometry-aware-multi-view-diffusion-outpainting-for-sparse-view.html">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</a></td>
  <td>GaMOï¼šåŸºäºå‡ ä½•æ„ŸçŸ¥çš„å¤šè§†è§’æ‰©æ•£å¤–ç»˜ç”¨äºç¨€ç–è§†è§’3Dé‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.25073v1" data-paper-url="./papers/251225073v1-gamo-geometry-aware-multi-view-diffusion-outpainting-for-sparse-view.html" onclick="toggleFavorite(this, '2512.25073v1', 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)