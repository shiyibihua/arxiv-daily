---
layout: default
title: "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction"
---

# GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.25073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.25073v1</a>
  <a href="https://arxiv.org/pdf/2512.25073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.25073v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.25073v1', 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: Project page: https://yichuanh.github.io/GaMO/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://yichuanh.github.io/GaMO/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GaMOï¼šåŸºäºå‡ ä½•æ„ŸçŸ¥çš„å¤šè§†è§’æ‰©æ•£å¤–ç»˜ç”¨äºç¨€ç–è§†è§’3Dé‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `3Dé‡å»º` `ç¨€ç–è§†è§’` `å¤šè§†è§’å¤–ç»˜` `æ‰©æ•£æ¨¡å‹` `å‡ ä½•æ„ŸçŸ¥` `é›¶æ ·æœ¬å­¦ä¹ ` `å›¾åƒç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dé‡å»ºæ–¹æ³•åœ¨å¯†é›†å¤šè§†è§’å›¾åƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¾“å…¥è§†è§’æœ‰é™æ—¶æ•ˆæœä¸ä½³ï¼Œç¼ºä¹å¯¹å·²çŸ¥è§†è§’å¤–å›´çš„å……åˆ†è¦†ç›–ã€‚
2. GaMOé€šè¿‡å¤šè§†è§’å¤–ç»˜æ‰©å±•ç°æœ‰ç›¸æœºå§¿æ€çš„è§†é‡ï¼Œè€Œéç”Ÿæˆæ–°è§†ç‚¹ï¼Œä»è€Œåœ¨æœ¬è´¨ä¸Šä¿æŒäº†å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶æ‰©å¤§äº†åœºæ™¯è¦†ç›–èŒƒå›´ã€‚
3. GaMOåœ¨Replicaå’ŒScanNet++æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼Œä¸”é€Ÿåº¦æ¯”ç°æœ‰æ‰©æ•£æ–¹æ³•å¿«25å€ï¼Œå¤„ç†æ—¶é—´ç¼©çŸ­è‡³10åˆ†é’Ÿä»¥å†…ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºGaMOï¼ˆGeometry-aware Multi-view Outpainterï¼‰ï¼Œä¸€ä¸ªé€šè¿‡å¤šè§†è§’å¤–ç»˜é‡æ–°æ„å»ºç¨€ç–è§†è§’3Dé‡å»ºçš„æ¡†æ¶ã€‚ä¸ç”Ÿæˆæ–°è§†ç‚¹ä¸åŒï¼ŒGaMOä»ç°æœ‰ç›¸æœºå§¿æ€æ‰©å±•è§†é‡ï¼Œä»è€Œå›ºæœ‰åœ°ä¿æŒäº†å‡ ä½•ä¸€è‡´æ€§ï¼ŒåŒæ—¶æä¾›äº†æ›´å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šè§†è§’æ¡ä»¶å’Œå‡ ä½•æ„ŸçŸ¥å»å™ªç­–ç•¥ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è¿è¡Œï¼Œæ— éœ€è®­ç»ƒã€‚åœ¨Replicaå’ŒScanNet++ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨3ã€6å’Œ9ä¸ªè¾“å…¥è§†è§’ä¸‹ï¼ŒGaMOå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼Œåœ¨PSNRå’ŒLPIPSæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ¯”æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å®ç°äº†25å€çš„åŠ é€Ÿï¼Œå¤„ç†æ—¶é—´ä½äº10åˆ†é’Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¨€ç–è§†è§’ä¸‹çš„3Dé‡å»ºé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è§†è§’ç¨€ç–æ—¶ï¼Œé‡å»ºè´¨é‡æ˜¾è‘—ä¸‹é™ï¼Œä¸»è¦ç—›ç‚¹åŒ…æ‹¬ï¼šè¦†ç›–èŒƒå›´ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ¨æ–­å·²çŸ¥è§†è§’ä¹‹å¤–çš„åŒºåŸŸï¼›å‡ ä½•ä¸ä¸€è‡´æ€§ï¼Œç”Ÿæˆçš„è§†å›¾ä¹‹é—´ç¼ºä¹ç©ºé—´ä¸€è‡´æ€§ï¼›è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGaMOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¨€ç–è§†è§’3Dé‡å»ºé—®é¢˜è½¬åŒ–ä¸ºå¤šè§†è§’å¤–ç»˜é—®é¢˜ã€‚é€šè¿‡ä»ç°æœ‰ç›¸æœºå§¿æ€å‘å¤–æ‰©å±•è§†é‡ï¼Œè€Œä¸æ˜¯ç”Ÿæˆå…¨æ–°çš„ç›¸æœºå§¿æ€ï¼Œå¯ä»¥è‡ªç„¶åœ°ä¿æŒå‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶æä¾›æ›´å¹¿é˜”çš„åœºæ™¯è¦†ç›–èŒƒå›´ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç”Ÿæˆæ–°è§†ç‚¹å¸¦æ¥çš„å‡ ä½•æ ¡æ­£é—®é¢˜ï¼Œç®€åŒ–äº†é‡å»ºæµç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGaMOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) å¤šè§†è§’æ¡ä»¶è¾“å…¥ï¼šåˆ©ç”¨å¤šä¸ªå·²çŸ¥è§†è§’çš„å›¾åƒä½œä¸ºæ¡ä»¶ä¿¡æ¯ã€‚2) å‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒå¤–ç»˜ï¼Œå¹¶èå…¥å‡ ä½•ä¿¡æ¯ä»¥ä¿è¯ç”Ÿæˆå›¾åƒçš„å‡ ä½•ä¸€è‡´æ€§ã€‚3) é›¶æ ·æœ¬æ¨ç†ï¼šæ•´ä¸ªè¿‡ç¨‹æ— éœ€è®­ç»ƒï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚æ¡†æ¶é€šè¿‡è¿­ä»£çš„å»å™ªè¿‡ç¨‹ï¼Œé€æ­¥ç”Ÿæˆæ‰©å±•è§†é‡çš„å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šGaMOçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤šè§†è§’å¤–ç»˜åº”ç”¨äºç¨€ç–è§†è§’3Dé‡å»ºï¼Œå¹¶è®¾è®¡äº†å‡ ä½•æ„ŸçŸ¥çš„æ‰©æ•£æ¨¡å‹ã€‚ä¸ä»¥å¾€ç”Ÿæˆæ–°è§†è§’çš„æ‰©æ•£æ–¹æ³•ä¸åŒï¼ŒGaMOé€šè¿‡æ‰©å±•ç°æœ‰è§†è§’çš„è§†é‡ï¼Œé¿å…äº†å¤æ‚çš„å‡ ä½•æ ¡æ­£ï¼Œå¹¶æ˜¾è‘—æé«˜äº†é‡å»ºé€Ÿåº¦ã€‚æ­¤å¤–ï¼Œé›¶æ ·æœ¬æ¨ç†æ–¹å¼ä¹Ÿé¿å…äº†é’ˆå¯¹ç‰¹å®šåœºæ™¯çš„è®­ç»ƒéœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šGaMOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šè§†è§’æ¡ä»¶èåˆç­–ç•¥ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†å¤šä¸ªè§†è§’çš„å›¾åƒä¿¡æ¯èå…¥åˆ°æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ä¸­ã€‚2) å‡ ä½•æ„ŸçŸ¥å»å™ªç­–ç•¥ï¼Œå…·ä½“å¦‚ä½•å°†å‡ ä½•ä¿¡æ¯ï¼ˆä¾‹å¦‚æ·±åº¦ä¿¡æ¯æˆ–ç›¸æœºå‚æ•°ï¼‰èå…¥åˆ°æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ä¸­ï¼Œä»¥ä¿è¯ç”Ÿæˆå›¾åƒçš„å‡ ä½•ä¸€è‡´æ€§ã€‚3) æ‰©æ•£æ¨¡å‹çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚ä½¿ç”¨ä½•ç§æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œä»¥åŠå¦‚ä½•è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„å‚æ•°ä»¥è·å¾—æœ€ä½³çš„é‡å»ºæ•ˆæœã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒï¼ˆå¦‚æœä½¿ç”¨äº†å¾®è°ƒï¼‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.25073v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.25073v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.25073v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

GaMOåœ¨Replicaå’ŒScanNet++æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨3ã€6å’Œ9ä¸ªè¾“å…¥è§†è§’ä¸‹ï¼ŒGaMOåœ¨PSNRå’ŒLPIPSæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒGaMOæ¯”æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å®ç°äº†25å€çš„åŠ é€Ÿï¼Œå¤„ç†æ—¶é—´ç¼©çŸ­è‡³10åˆ†é’Ÿä»¥å†…ï¼Œå¤§å¤§æé«˜äº†é‡å»ºæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GaMOåœ¨æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚è¯¥æŠ€æœ¯å¯ä»¥åˆ©ç”¨å°‘é‡å›¾åƒå¿«é€Ÿé‡å»ºåœºæ™¯ï¼Œé™ä½äº†å¯¹ä¼ æ„Ÿå™¨æ•°é‡å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚åœ¨æ–‡ç‰©ä¿æŠ¤é¢†åŸŸï¼ŒGaMOå¯ä»¥ç”¨äºå¿«é€Ÿé‡å»ºæ–‡ç‰©çš„ä¸‰ç»´æ¨¡å‹ï¼Œæ–¹ä¾¿ç ”ç©¶å’Œå±•ç¤ºã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºå®æ—¶ä¸‰ç»´é‡å»ºï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å¼çš„ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/

