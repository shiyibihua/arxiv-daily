---
layout: default
title: "FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation"
---

# FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24903v1</a>
  <a href="https://arxiv.org/pdf/2512.24903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24903v1', 'FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao

**åˆ†ç±»**: cs.CV, cs.CE

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: Accepted by AAAI-26 Main Track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FinMMDocRï¼šæå‡ºé‡‘èå¤šæ¨¡æ€æ¨ç†åŸºå‡†ï¼Œå…³æ³¨åœºæ™¯æ„ŸçŸ¥ã€æ–‡æ¡£ç†è§£å’Œå¤šæ­¥è®¡ç®—ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡‘èå¤šæ¨¡æ€æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `åœºæ™¯æ„ŸçŸ¥` `æ–‡æ¡£ç†è§£` `å¤šæ­¥è®¡ç®—` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†åœ¨é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹å¯¹å¤æ‚åœºæ™¯ã€æ·±åº¦æ–‡æ¡£ç†è§£å’Œå¤šæ­¥è®¡ç®—çš„æœ‰æ•ˆè¯„ä¼°ã€‚
2. FinMMDocRé€šè¿‡å¼•å…¥åœºæ™¯æ„ŸçŸ¥ã€æ·±åº¦é‡‘èæ–‡æ¡£å’Œå¤šæ­¥è®¡ç®—ï¼Œæ„å»ºæ›´è´´è¿‘çœŸå®é‡‘èåœºæ™¯çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰æœ€ä½³MLLMåœ¨FinMMDocRä¸Šè¡¨ç°ä»æœ‰æå‡ç©ºé—´ï¼Œä¸”RAGæ–¹æ³•æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œçªæ˜¾äº†è¯¥åŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†FinMMDocRï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŒè¯­å¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨çœŸå®é‡‘èæ•°å€¼æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼ŒFinMMDocRæœ‰ä¸‰ä¸ªä¸»è¦è¿›å±•ï¼š(1) åœºæ™¯æ„ŸçŸ¥ï¼š1200ä¸ªä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ä¸­ï¼Œ57.9%çš„é—®é¢˜èå…¥äº†12ç§éšå¼é‡‘èåœºæ™¯ï¼ˆä¾‹å¦‚ï¼ŒæŠ•èµ„ç»„åˆç®¡ç†ï¼‰ï¼ŒæŒ‘æˆ˜æ¨¡å‹åŸºäºå‡è®¾æ‰§è¡Œä¸“å®¶çº§æ¨ç†ï¼›(2) æ–‡æ¡£ç†è§£ï¼š837ç¯‡ä¸­æ–‡/è‹±æ–‡æ–‡æ¡£æ¶µç›–9ç§ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œå…¬å¸ç ”ç©¶ï¼‰ï¼Œå¹³å‡50.8é¡µï¼ŒåŒ…å«ä¸°å¯Œçš„è§†è§‰å…ƒç´ ï¼Œåœ¨é‡‘èæ–‡æ¡£çš„å¹¿åº¦å’Œæ·±åº¦ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰åŸºå‡†ï¼›(3) å¤šæ­¥è®¡ç®—ï¼šé—®é¢˜å¹³å‡éœ€è¦11æ­¥æ¨ç†ï¼ˆ5.3æ­¥æå–+5.7æ­¥è®¡ç®—ï¼‰ï¼Œå…¶ä¸­65.0%éœ€è¦è·¨é¡µè¯æ®ï¼ˆå¹³å‡2.4é¡µï¼‰ã€‚æ€§èƒ½æœ€ä½³çš„MLLMä»…è¾¾åˆ°58.0%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”ä¸åŒçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚æˆ‘ä»¬æœŸæœ›FinMMDocRèƒ½å¤Ÿæ¨åŠ¨MLLMå’Œæ¨ç†å¢å¼ºæ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨çœŸå®é‡‘èåœºæ™¯ä¸‹è¿›è¡Œæ•°å€¼æ¨ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯æ„ŸçŸ¥ã€æ–‡æ¡£ç†è§£æ·±åº¦å’Œå¤šæ­¥è®¡ç®—èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚çš„é‡‘èæ–‡æ¡£å’Œæ¨ç†ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§å’ŒçœŸå®æ€§çš„é‡‘èå¤šæ¨¡æ€æ¨ç†åŸºå‡†ï¼Œå³FinMMDocRã€‚è¯¥åŸºå‡†é€šè¿‡å¼•å…¥éšå¼é‡‘èåœºæ™¯ã€æ·±åº¦é‡‘èæ–‡æ¡£å’Œå¤šæ­¥è®¡ç®—ï¼Œè¿«ä½¿æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„ç†è§£å’Œæ¨ç†ï¼Œä»è€Œæ›´å¥½åœ°è¯„ä¼°å’Œæå‡MLLMåœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFinMMDocRåŸºå‡†åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1) å¤§é‡çš„é‡‘èæ–‡æ¡£ï¼Œæ¶µç›–å¤šç§ç±»å‹å’Œæ ¼å¼ï¼›2) ä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ï¼ŒåŒ…å«éšå¼é‡‘èåœºæ™¯å’Œå¤šæ­¥è®¡ç®—è¦æ±‚ï¼›3) è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨åœºæ™¯æ„ŸçŸ¥ã€æ–‡æ¡£ç†è§£å’Œå¤šæ­¥è®¡ç®—æ–¹é¢çš„æ€§èƒ½ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šç»™å®šé‡‘èæ–‡æ¡£å’Œé—®é¢˜ï¼Œæ¨¡å‹éœ€è¦æå–ç›¸å…³ä¿¡æ¯ã€è¿›è¡Œè®¡ç®—å’Œæ¨ç†ï¼Œæœ€ç»ˆç»™å‡ºç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šFinMMDocRçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹çœŸå®é‡‘èåœºæ™¯çš„æ¨¡æ‹Ÿå’Œå¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ·±åº¦è¯„ä¼°ã€‚å…·ä½“ä½“ç°åœ¨ï¼š1) å¼•å…¥äº†éšå¼é‡‘èåœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡ä¸“å®¶çº§çš„æ¨ç†èƒ½åŠ›ï¼›2) é‡‡ç”¨äº†æ·±åº¦é‡‘èæ–‡æ¡£ï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„æ–‡æ¡£ç†è§£èƒ½åŠ›ï¼›3) è®¾è®¡äº†å¤šæ­¥è®¡ç®—é—®é¢˜ï¼Œè€ƒå¯Ÿæ¨¡å‹çš„æ¨ç†å’Œè®¡ç®—èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šFinMMDocRåŸºå‡†åŒ…å«1200ä¸ªä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ï¼Œå…¶ä¸­57.9%çš„é—®é¢˜åŒ…å«12ç§éšå¼é‡‘èåœºæ™¯ã€‚æ–‡æ¡£æ–¹é¢ï¼ŒåŒ…å«837ç¯‡ä¸­æ–‡/è‹±æ–‡æ–‡æ¡£ï¼Œæ¶µç›–9ç§ç±»å‹ï¼Œå¹³å‡50.8é¡µã€‚é—®é¢˜å¹³å‡éœ€è¦11æ­¥æ¨ç†ï¼ˆ5.3æ­¥æå–+5.7æ­¥è®¡ç®—ï¼‰ï¼Œå…¶ä¸­65.0%éœ€è¦è·¨é¡µè¯æ®ï¼ˆå¹³å‡2.4é¡µï¼‰ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®ç‡ç­‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24903v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24903v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24903v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æœ€ä½³MLLMåœ¨FinMMDocRä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º58.0%ï¼Œè¡¨æ˜è¯¥åŸºå‡†å…·æœ‰å¾ˆé«˜çš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œä¸åŒçš„RAGæ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼Œçªæ˜¾äº†æ£€ç´¢å’Œæ¨ç†ç­–ç•¥çš„é‡è¦æ€§ã€‚è¿™äº›ç»“æœä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ–¹å‘ï¼Œå³éœ€è¦è¿›ä¸€æ­¥æå‡MLLMåœ¨åœºæ™¯æ„ŸçŸ¥ã€æ–‡æ¡£ç†è§£å’Œå¤šæ­¥è®¡ç®—æ–¹é¢çš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FinMMDocRçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºé‡‘èé¢†åŸŸçš„æ™ºèƒ½æŠ•é¡¾ã€é£é™©è¯„ä¼°ã€è´¢åŠ¡åˆ†æç­‰åœºæ™¯ã€‚é€šè¿‡æå‡MLLMåœ¨é‡‘èå¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©é‡‘èä»ä¸šè€…æ›´é«˜æ•ˆåœ°å¤„ç†æµ·é‡é‡‘èæ•°æ®ï¼Œåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ï¼Œå¹¶ä¸ºæŠ•èµ„è€…æä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯ä»¥ä¿ƒè¿›é‡‘èé¢†åŸŸAIåº”ç”¨çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.

