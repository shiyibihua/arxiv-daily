---
layout: default
title: "MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding"
---

# MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24605v1</a>
  <a href="https://arxiv.org/pdf/2512.24605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24605v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24605v1', 'MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: 14 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoniReferæ•°æ®é›†ï¼Œç”¨äºè·¯ä¾§åŸºç¡€è®¾æ–½çš„3Dè§†è§‰å®šä½ä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šæ¨¡æ€èåˆ` `è·¯ä¾§åŸºç¡€è®¾æ–½` `æ™ºèƒ½äº¤é€š` `ç‚¹äº‘å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè§†è§‰å®šä½æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å®¤å†…å’Œè‡ªåŠ¨é©¾é©¶åœºæ™¯ï¼Œç¼ºä¹è·¯ä¾§åŸºç¡€è®¾æ–½è§†è§’çš„æˆ·å¤–ç›‘æ§åœºæ™¯æ•°æ®ã€‚
2. æå‡ºMoniReferæ•°æ®é›†å’ŒMoni3DVGæ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒå¤–è§‚ä¿¡æ¯å’Œç‚¹äº‘å‡ ä½•ã€å…‰å­¦ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ å’Œ3Då¯¹è±¡å®šä½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMoni3DVGæ–¹æ³•åœ¨MoniReferæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºè·¯ä¾§3Dè§†è§‰å®šä½æä¾›æ–°åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šé¢å‘æˆ·å¤–ç›‘æ§åœºæ™¯çš„3Dè§†è§‰å®šä½ï¼Œæ—¨åœ¨å®ç°åŸºç¡€è®¾æ–½çº§åˆ«çš„äº¤é€šåœºæ™¯ç†è§£ï¼Œè¶…è¶Šäº†è‡ªè½¦è§†è§’ã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†MoniReferï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºè·¯ä¾§çº§åˆ«çš„3Dè§†è§‰å®šä½ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦136,018ä¸ªå¯¹è±¡ï¼Œä»¥åŠä»çœŸå®ç¯å¢ƒä¸­çš„å¤šä¸ªå¤æ‚äº¤é€šè·¯å£æ”¶é›†çš„411,128ä¸ªè‡ªç„¶è¯­è¨€è¡¨è¾¾å¼ã€‚ä¸ºäº†ç¡®ä¿æ•°æ®é›†çš„è´¨é‡å’Œå‡†ç¡®æ€§ï¼Œæˆ‘ä»¬æ‰‹åŠ¨éªŒè¯äº†æ‰€æœ‰è¯­è¨€æè¿°å’Œå¯¹è±¡çš„3Dæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯æ–¹æ³•Moni3DVGï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒæä¾›çš„ä¸°å¯Œå¤–è§‚ä¿¡æ¯ä»¥åŠç‚¹äº‘æä¾›çš„å‡ ä½•å’Œå…‰å­¦ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ å’Œ3Då¯¹è±¡å®šä½ã€‚åœ¨æå‡ºçš„åŸºå‡†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†ä¼šå¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è·¯ä¾§åŸºç¡€è®¾æ–½è§†è§’ä¸‹çš„3Dè§†è§‰å®šä½é—®é¢˜ï¼Œå³æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°åœ¨3Dç‚¹äº‘åœºæ™¯ä¸­å®šä½ç›®æ ‡å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•å’Œæ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å®¤å†…å’Œè‡ªåŠ¨é©¾é©¶åœºæ™¯ï¼Œç¼ºä¹å¯¹è·¯ä¾§ç›‘æ§åœºæ™¯çš„æœ‰æ•ˆæ”¯æŒï¼Œæ— æ³•æ»¡è¶³æ™ºèƒ½äº¤é€šç³»ç»Ÿå¯¹åŸºç¡€è®¾æ–½çº§åˆ«åœºæ™¯ç†è§£çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è·¯ä¾§3Dè§†è§‰å®šä½æ•°æ®é›†ï¼ˆMoniReferï¼‰ï¼Œå¹¶æå‡ºä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼ˆMoni3DVGï¼‰ã€‚é€šè¿‡ç»“åˆå›¾åƒæä¾›çš„ä¸°å¯Œå¤–è§‚ä¿¡æ¯å’Œç‚¹äº‘æä¾›çš„å‡ ä½•ã€å…‰å­¦ä¿¡æ¯ï¼Œå®ç°æ›´å‡†ç¡®çš„3Då¯¹è±¡å®šä½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoni3DVGæ–¹æ³•æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š1) å¤šæ¨¡æ€ç‰¹å¾æå–æ¨¡å—ï¼Œåˆ†åˆ«ä»å›¾åƒå’Œç‚¹äº‘ä¸­æå–ç‰¹å¾ï¼›2) ç‰¹å¾èåˆæ¨¡å—ï¼Œå°†å›¾åƒå’Œç‚¹äº‘ç‰¹å¾è¿›è¡Œèåˆï¼›3) 3Då¯¹è±¡å®šä½æ¨¡å—ï¼Œæ ¹æ®èåˆåçš„ç‰¹å¾é¢„æµ‹3D bounding boxã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡è·¯ä¾§3Dè§†è§‰å®šä½æ•°æ®é›†MoniReferï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„æ•°æ®ç©ºç™½ï¼›2) æå‡ºäº†Moni3DVGæ–¹æ³•ï¼Œæœ‰æ•ˆèåˆäº†å›¾åƒå’Œç‚¹äº‘çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæé«˜äº†3Då¯¹è±¡å®šä½çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMoni3DVGæ›´å…³æ³¨è·¯ä¾§åœºæ™¯çš„ç‰¹ç‚¹ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è®¾è®¡äº†å¤šæ¨¡æ€èåˆç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šMoni3DVGæ–¹æ³•åœ¨ç‰¹å¾æå–æ–¹é¢ï¼Œå›¾åƒåˆ†æ”¯å¯èƒ½é‡‡ç”¨é¢„è®­ç»ƒçš„CNNæ¨¡å‹ï¼ˆå¦‚ResNetï¼‰ï¼Œç‚¹äº‘åˆ†æ”¯å¯èƒ½é‡‡ç”¨PointNet++ç­‰ç‚¹äº‘å¤„ç†ç½‘ç»œã€‚åœ¨ç‰¹å¾èåˆæ–¹é¢ï¼Œå¯èƒ½é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–è·¨æ¨¡æ€Transformerç­‰æ–¹æ³•ï¼Œå­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬å®šä½æŸå¤±ï¼ˆå¦‚IoU lossï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µæŸå¤±ï¼‰ã€‚å…·ä½“ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ ¹æ®æ‘˜è¦ä¿¡æ¯æ— æ³•å¾—çŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24605v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24605v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24605v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„Moni3DVGæ–¹æ³•åœ¨MoniReferæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Då¯¹è±¡å®šä½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼ˆå¦‚å®šä½ç²¾åº¦ã€å¬å›ç‡ç­‰ï¼‰ä»¥åŠä¸ç°æœ‰åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚æ‘˜è¦ä¸­æåˆ°â€œextensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our methodâ€ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½äº¤é€šç³»ç»Ÿã€æ™ºæ…§åŸå¸‚ç­‰é¢†åŸŸã€‚é€šè¿‡è·¯ä¾§åŸºç¡€è®¾æ–½å¯¹äº¤é€šåœºæ™¯è¿›è¡Œ3Dè§†è§‰å®šä½ï¼Œå¯ä»¥å®ç°æ›´ç²¾ç¡®çš„äº¤é€šç›‘æ§ã€äº‹ä»¶æ£€æµ‹å’Œè‡ªåŠ¨é©¾é©¶è¾…åŠ©ï¼Œæå‡äº¤é€šæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„æˆ·å¤–ç›‘æ§åœºæ™¯ï¼Œä¾‹å¦‚å®‰é˜²ç›‘æ§ã€ç¯å¢ƒç›‘æµ‹ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.

