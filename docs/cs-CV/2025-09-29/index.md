---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-29
---

# cs.CVï¼ˆ2025-09-29ï¼‰

ğŸ“Š å…± **60** ç¯‡è®ºæ–‡
 | ğŸ”— **23** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (24 ğŸ”—8)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (14 ğŸ”—6)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (13 ğŸ”—6)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (24 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250925564v1-fishnet-analyzing-the-capabilities-of-multimodal-large-language-mode.html">FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology</a></td>
  <td>FishNet++ï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æµ·æ´‹ç”Ÿç‰©å­¦ä¸­çš„èƒ½åŠ›ï¼Œå¹¶æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25564v1" data-paper-url="./papers/250925564v1-fishnet-analyzing-the-capabilities-of-multimodal-large-language-mode.html" onclick="toggleFavorite(this, '2509.25564v1', 'FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250924888v1-mmrqa-signal-enhanced-multimodal-large-language-models-for-mri-quali.html">MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment</a></td>
  <td>æå‡ºMMRQAæ¡†æ¶ï¼Œèåˆä¿¡å·å¤„ç†ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºMRIè´¨é‡è¯„ä¼°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24888v1" data-paper-url="./papers/250924888v1-mmrqa-signal-enhanced-multimodal-large-language-models-for-mri-quali.html" onclick="toggleFavorite(this, '2509.24888v1', 'MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250924739v2-toward-a-vision-language-foundation-model-for-medical-data-multimoda.html">Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation</a></td>
  <td>æå‡ºViPET-ReportGenæ•°æ®é›†ä¸åŸºå‡†ï¼Œç”¨äºæå‡è¶Šå—è¯­PET/CTæŠ¥å‘Šç”Ÿæˆçš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24739v2" data-paper-url="./papers/250924739v2-toward-a-vision-language-foundation-model-for-medical-data-multimoda.html" onclick="toggleFavorite(this, '2509.24739v2', 'Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250925528v2-llm-rg-referential-grounding-in-outdoor-scenarios-using-large-langua.html">LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models</a></td>
  <td>LLM-RGï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°æˆ·å¤–åœºæ™¯ä¸‹çš„æŒ‡ç§°å¯¹è±¡å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25528v2" data-paper-url="./papers/250925528v2-llm-rg-referential-grounding-in-outdoor-scenarios-using-large-langua.html" onclick="toggleFavorite(this, '2509.25528v2', 'LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250925178v1-ghost-hallucination-inducing-image-generation-for-multimodal-llms.html">GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</a></td>
  <td>GHOSTï¼šé€šè¿‡è¯±å¯¼å¹»è§‰çš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºå‹åŠ›æµ‹è¯•å¤šæ¨¡æ€LLM</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25178v1" data-paper-url="./papers/250925178v1-ghost-hallucination-inducing-image-generation-for-multimodal-llms.html" onclick="toggleFavorite(this, '2509.25178v1', 'GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250925177v1-mitigating-hallucination-in-multimodal-llms-with-layer-contrastive-d.html">Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding</a></td>
  <td>æå‡ºå±‚å¯¹æ¯”è§£ç (LayerCD)æ–¹æ³•ï¼Œç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25177v1" data-paper-url="./papers/250925177v1-mitigating-hallucination-in-multimodal-llms-with-layer-contrastive-d.html" onclick="toggleFavorite(this, '2509.25177v1', 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251000069v1-oig-bench-a-multi-agent-annotated-benchmark-for-multimodal-one-image.html">OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding</a></td>
  <td>æå‡ºOIG-BenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹å•å›¾å¼•å¯¼çš„ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00069v1" data-paper-url="./papers/251000069v1-oig-bench-a-multi-agent-annotated-benchmark-for-multimodal-one-image.html" onclick="toggleFavorite(this, '2510.00069v1', 'OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250924791v1-vision-function-layer-in-multimodal-llms.html">Vision Function Layer in Multimodal LLMs</a></td>
  <td>æ­ç¤ºå¤šæ¨¡æ€LLMè§†è§‰åŠŸèƒ½å±‚ï¼Œå®ç°é«˜æ•ˆå¯å®šåˆ¶çš„è§†è§‰èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24791v1" data-paper-url="./papers/250924791v1-vision-function-layer-in-multimodal-llms.html" onclick="toggleFavorite(this, '2509.24791v1', 'Vision Function Layer in Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251003295v1-multimodal-arabic-captioning-with-interpretable-visual-concept-integ.html">Multimodal Arabic Captioning with Interpretable Visual Concept Integration</a></td>
  <td>VLCAPï¼šä¸€ç§ç»“åˆå¯è§£é‡Šè§†è§‰æ¦‚å¿µé›†æˆçš„å¤šæ¨¡æ€é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03295v1" data-paper-url="./papers/251003295v1-multimodal-arabic-captioning-with-interpretable-visual-concept-integ.html" onclick="toggleFavorite(this, '2510.03295v1', 'Multimodal Arabic Captioning with Interpretable Visual Concept Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250925151v1-videoanchor-reinforcing-subspace-structured-visual-cues-for-coherent.html">VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning</a></td>
  <td>VideoAnchorï¼šé€šè¿‡å¼ºåŒ–å­ç©ºé—´ç»“æ„è§†è§‰çº¿ç´¢å®ç°è¿è´¯çš„è§†è§‰-ç©ºé—´æ¨ç†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25151v1" data-paper-url="./papers/250925151v1-videoanchor-reinforcing-subspace-structured-visual-cues-for-coherent.html" onclick="toggleFavorite(this, '2509.25151v1', 'VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250925044v1-a-scalable-distributed-framework-for-multimodal-gigavoxel-image-regi.html">A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration</a></td>
  <td>æå‡ºFFDPæ¡†æ¶ï¼Œå®ç°å‰æ‰€æœªæœ‰çš„åäº¿ä½“ç´ å¤šæ¨¡æ€å›¾åƒé…å‡†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25044v1" data-paper-url="./papers/250925044v1-a-scalable-distributed-framework-for-multimodal-gigavoxel-image-regi.html" onclick="toggleFavorite(this, '2509.25044v1', 'A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250924505v1-robust-multimodal-semantic-segmentation-with-balanced-modality-contr.html">Robust Multimodal Semantic Segmentation with Balanced Modality Contributions</a></td>
  <td>æå‡ºEQUISegï¼Œé€šè¿‡å¹³è¡¡æ¨¡æ€è´¡çŒ®æå‡å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24505v1" data-paper-url="./papers/250924505v1-robust-multimodal-semantic-segmentation-with-balanced-modality-contr.html" onclick="toggleFavorite(this, '2509.24505v1', 'Robust Multimodal Semantic Segmentation with Balanced Modality Contributions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250924365v2-uni-x-mitigating-modality-conflict-with-a-two-end-separated-architec.html">Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models</a></td>
  <td>æå‡ºUni-Xæ¨¡å‹ï¼Œé€šè¿‡ä¸¤ç«¯åˆ†ç¦»æ¶æ„ç¼“è§£å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ä¸­çš„æ¨¡æ€å†²çªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24365v2" data-paper-url="./papers/250924365v2-uni-x-mitigating-modality-conflict-with-a-two-end-separated-architec.html" onclick="toggleFavorite(this, '2509.24365v2', 'Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250925502v1-seeing-before-reasoning-a-unified-framework-for-generalizable-and-ex.html">Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection</a></td>
  <td>æå‡ºForensic-Chatæ¡†æ¶ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¼ªé€ å›¾åƒæ£€æµ‹ä¸­çš„æ³›åŒ–æ€§å’Œå¯è§£é‡Šæ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25502v1" data-paper-url="./papers/250925502v1-seeing-before-reasoning-a-unified-framework-for-generalizable-and-ex.html" onclick="toggleFavorite(this, '2509.25502v1', 'Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250925185v1-pixelcraft-a-multi-agent-system-for-high-fidelity-visual-reasoning-o.html">PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images</a></td>
  <td>PixelCraftï¼šç”¨äºç»“æ„åŒ–å›¾åƒé«˜ä¿çœŸè§†è§‰æ¨ç†çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25185v1" data-paper-url="./papers/250925185v1-pixelcraft-a-multi-agent-system-for-high-fidelity-visual-reasoning-o.html" onclick="toggleFavorite(this, '2509.25185v1', 'PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250925033v3-vt-fsl-bridging-vision-and-text-with-llms-for-few-shot-learning.html">VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</a></td>
  <td>æå‡ºVT-FSLæ¡†æ¶ï¼Œåˆ©ç”¨LLMæ¡¥æ¥è§†è§‰ä¸æ–‡æœ¬ï¼Œæå‡å°æ ·æœ¬å­¦ä¹ æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25033v3" data-paper-url="./papers/250925033v3-vt-fsl-bridging-vision-and-text-with-llms-for-few-shot-learning.html" onclick="toggleFavorite(this, '2509.25033v3', 'VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250924875v1-environment-aware-satellite-image-generation-with-diffusion-models.html">Environment-Aware Satellite Image Generation with Diffusion Models</a></td>
  <td>æå‡ºç¯å¢ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€ç¯å¢ƒç›¸å…³çš„å«æ˜Ÿå›¾åƒã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24875v1" data-paper-url="./papers/250924875v1-environment-aware-satellite-image-generation-with-diffusion-models.html" onclick="toggleFavorite(this, '2509.24875v1', 'Environment-Aware Satellite Image Generation with Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250924621v1-freeret-mllms-as-training-free-retrievers.html">FreeRet: MLLMs as Training-Free Retrievers</a></td>
  <td>FreeRetï¼šæ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨MLLMå®ç°å¼ºå¤§çš„å¤šæ¨¡æ€æ£€ç´¢</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24621v1" data-paper-url="./papers/250924621v1-freeret-mllms-as-training-free-retrievers.html" onclick="toggleFavorite(this, '2509.24621v1', 'FreeRet: MLLMs as Training-Free Retrievers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250924473v3-euclids-gift-enhancing-spatial-perception-and-reasoning-in-vision-la.html">Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</a></td>
  <td>æå‡ºEuclid30Kæ•°æ®é›†å¹¶å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡å…¶ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24473v3" data-paper-url="./papers/250924473v3-euclids-gift-enhancing-spatial-perception-and-reasoning-in-vision-la.html" onclick="toggleFavorite(this, '2509.24473v3', 'Euclid&#39;s Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250924427v1-ui2v-bench-an-understanding-based-image-to-video-generation-benchmar.html">UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark</a></td>
  <td>UI2V-Benchï¼šæå‡ºä¸€ä¸ªåŸºäºç†è§£çš„å›¾ç”Ÿè§†é¢‘ç”Ÿæˆè¯„æµ‹åŸºå‡†ï¼Œå…³æ³¨è¯­ä¹‰ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24427v1" data-paper-url="./papers/250924427v1-ui2v-bench-an-understanding-based-image-to-video-generation-benchmar.html" onclick="toggleFavorite(this, '2509.24427v1', 'UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250925533v1-visor-universal-visual-inputs-based-steering-for-large-vision-langua.html">VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models</a></td>
  <td>VISOR++ï¼šåŸºäºé€šç”¨è§†è§‰è¾“å…¥çš„è§†è§‰è¯­è¨€æ¨¡å‹è¡Œä¸ºå¼•å¯¼æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25533v1" data-paper-url="./papers/250925533v1-visor-universal-visual-inputs-based-steering-for-large-vision-langua.html" onclick="toggleFavorite(this, '2509.25533v1', 'VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250924943v1-perceive-reflect-and-understand-long-video-progressive-multi-granula.html">Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents</a></td>
  <td>CogniGPTï¼šäº¤äº’å¼å¤šç²’åº¦çº¿ç´¢æ¢ç´¢ï¼Œæå‡é•¿è§†é¢‘ç†è§£çš„æ•ˆç‡ä¸å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24943v1" data-paper-url="./papers/250924943v1-perceive-reflect-and-understand-long-video-progressive-multi-granula.html" onclick="toggleFavorite(this, '2509.24943v1', 'Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250924837v1-training-free-token-pruning-via-zeroth-order-gradient-estimation-in-.html">Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models</a></td>
  <td>æå‡ºè®­ç»ƒæ— å…³çš„ä»¤ç‰Œä¿®å‰ªæ–¹æ³•ä»¥é™ä½è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24837v1" data-paper-url="./papers/250924837v1-training-free-token-pruning-via-zeroth-order-gradient-estimation-in-.html" onclick="toggleFavorite(this, '2509.24837v1', 'Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250924514v1-instruction-guided-multi-object-image-editing-with-quantity-and-layo.html">Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency</a></td>
  <td>æå‡ºQL-Adapterï¼Œè§£å†³å¤šå¯¹è±¡å›¾åƒç¼–è¾‘ä¸­æ•°é‡å’Œå¸ƒå±€ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24514v1" data-paper-url="./papers/250924514v1-instruction-guided-multi-object-image-editing-with-quantity-and-layo.html" onclick="toggleFavorite(this, '2509.24514v1', 'Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250925304v1-luma-low-dimension-unified-motion-alignment-with-dual-path-anchoring.html">LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model</a></td>
  <td>LUMAï¼šåŸºäºåŒè·¯é”šå®šçš„ä½ç»´ç»Ÿä¸€è¿åŠ¨å¯¹é½æ–‡æœ¬åˆ°åŠ¨ä½œæ‰©æ•£æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">motion diffusion model</span> <span class="paper-tag">motion diffusion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25304v1" data-paper-url="./papers/250925304v1-luma-low-dimension-unified-motion-alignment-with-dual-path-anchoring.html" onclick="toggleFavorite(this, '2509.25304v1', 'LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250924385v1-vid-llm-a-compact-video-based-3d-multimodal-llm-with-reconstruction-.html">Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy</a></td>
  <td>Vid-LLMï¼šæå‡ºä¸€ç§åŸºäºè§†é¢‘çš„ç´§å‡‘å‹3Då¤šæ¨¡æ€LLMï¼Œå®ç°é‡å»º-æ¨ç†ååŒ</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">metric depth</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24385v1" data-paper-url="./papers/250924385v1-vid-llm-a-compact-video-based-3d-multimodal-llm-with-reconstruction-.html" onclick="toggleFavorite(this, '2509.24385v1', 'Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250924896v1-dam-dual-active-learning-with-multimodal-foundation-model-for-source.html">DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation</a></td>
  <td>æå‡ºDAMï¼Œåˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œæ— æºåŸŸè‡ªé€‚åº”åŒé‡ä¸»åŠ¨å­¦ä¹ ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24896v1" data-paper-url="./papers/250924896v1-dam-dual-active-learning-with-multimodal-foundation-model-for-source.html" onclick="toggleFavorite(this, '2509.24896v1', 'DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250925077v2-bridge-building-reinforcement-learning-depth-to-image-data-generatio.html">BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ·±åº¦å›¾åˆ°å›¾åƒç”Ÿæˆå¼•æ“BRIDGEï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25077v2" data-paper-url="./papers/250925077v2-bridge-building-reinforcement-learning-depth-to-image-data-generatio.html" onclick="toggleFavorite(this, '2509.25077v2', 'BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250924776v1-vtperception-r1-enhancing-multimodal-reasoning-via-explicit-visual-a.html">VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding</a></td>
  <td>VTPerception-R1ï¼šé€šè¿‡æ˜¾å¼è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24776v1" data-paper-url="./papers/250924776v1-vtperception-r1-enhancing-multimodal-reasoning-via-explicit-visual-a.html" onclick="toggleFavorite(this, '2509.24776v1', 'VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250925127v2-score-distillation-of-flow-matching-models.html">Score Distillation of Flow Matching Models</a></td>
  <td>å°†Score DistillationæˆåŠŸåº”ç”¨äºFlow Matchingæ¨¡å‹ï¼Œå®ç°å¿«é€Ÿé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25127v2" data-paper-url="./papers/250925127v2-score-distillation-of-flow-matching-models.html" onclick="toggleFavorite(this, '2509.25127v2', 'Score Distillation of Flow Matching Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250924491v1-mitigating-visual-hallucinations-via-semantic-curriculum-preference-.html">Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs</a></td>
  <td>æå‡ºSCPOæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰è¯¾ç¨‹åå¥½ä¼˜åŒ–ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24491v1" data-paper-url="./papers/250924491v1-mitigating-visual-hallucinations-via-semantic-curriculum-preference-.html" onclick="toggleFavorite(this, '2509.24491v1', 'Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250924361v2-ui-ug-a-unified-mllm-for-ui-understanding-and-generation.html">UI-UG: A Unified MLLM for UI Understanding and Generation</a></td>
  <td>UI-UGï¼šç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç”¨æˆ·ç•Œé¢ç†è§£ä¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24361v2" data-paper-url="./papers/250924361v2-ui-ug-a-unified-mllm-for-ui-understanding-and-generation.html" onclick="toggleFavorite(this, '2509.24361v2', 'UI-UG: A Unified MLLM for UI Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251000072v1-geo-r1-unlocking-vlm-geospatial-reasoning-with-cross-view-reinforcem.html">Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning</a></td>
  <td>Geo-R1ï¼šé€šè¿‡è·¨è§†è§’å¼ºåŒ–å­¦ä¹ è§£é”è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00072v1" data-paper-url="./papers/251000072v1-geo-r1-unlocking-vlm-geospatial-reasoning-with-cross-view-reinforcem.html" onclick="toggleFavorite(this, '2510.00072v1', 'Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250925190v1-visual-jigsaw-post-training-improves-mllms.html">Visual Jigsaw Post-Training Improves MLLMs</a></td>
  <td>Visual Jigsawï¼šé€šè¿‡è§†è§‰æ‹¼å›¾åè®­ç»ƒæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25190v1" data-paper-url="./papers/250925190v1-visual-jigsaw-post-training-improves-mllms.html" onclick="toggleFavorite(this, '2509.25190v1', 'Visual Jigsaw Post-Training Improves MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250924382v1-realign-regularized-procedure-alignment-with-matching-video-embeddin.html">REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport</a></td>
  <td>REALIGNï¼šåŸºäºæ­£åˆ™åŒ–èåˆåGromov-Wassersteinæœ€ä¼˜ä¼ è¾“çš„ç¨‹åºè§†é¢‘å¯¹é½æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24382v1" data-paper-url="./papers/250924382v1-realign-regularized-procedure-alignment-with-matching-video-embeddin.html" onclick="toggleFavorite(this, '2509.24382v1', 'REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250924968v1-event-based-facial-keypoint-alignment-via-cross-modal-fusion-attenti.html">Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning</a></td>
  <td>æå‡ºåŸºäºè·¨æ¨¡æ€èåˆæ³¨æ„åŠ›å’Œè‡ªç›‘ç£å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ çš„äº‹ä»¶ç›¸æœºäººè„¸å…³é”®ç‚¹å¯¹é½æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24968v1" data-paper-url="./papers/250924968v1-event-based-facial-keypoint-alignment-via-cross-modal-fusion-attenti.html" onclick="toggleFavorite(this, '2509.24968v1', 'Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250924448v1-generalist-multi-class-anomaly-detection-via-distillation-to-two-het.html">Generalist Multi-Class Anomaly Detection via Distillation to Two Heterogeneous Student Networks</a></td>
  <td>æå‡ºåŸºäºçŸ¥è¯†è’¸é¦çš„åŒå¼‚æ„å­¦ç”Ÿç½‘ç»œï¼Œç”¨äºé€šç”¨å¤šç±»å¼‚å¸¸æ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24448v1" data-paper-url="./papers/250924448v1-generalist-multi-class-anomaly-detection-via-distillation-to-two-het.html" onclick="toggleFavorite(this, '2509.24448v1', 'Generalist Multi-Class Anomaly Detection via Distillation to Two Heterogeneous Student Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250925161v1-rolling-forcing-autoregressive-long-video-diffusion-in-real-time.html">Rolling Forcing: Autoregressive Long Video Diffusion in Real Time</a></td>
  <td>æå‡ºRolling Forcingï¼Œå®ç°å®æ—¶è‡ªå›å½’é•¿è§†é¢‘æ‰©æ•£ç”Ÿæˆï¼Œæ˜¾è‘—é™ä½è¯¯å·®ç´¯ç§¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25161v1" data-paper-url="./papers/250925161v1-rolling-forcing-autoregressive-long-video-diffusion-in-real-time.html" onclick="toggleFavorite(this, '2509.25161v1', 'Rolling Forcing: Autoregressive Long Video Diffusion in Real Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250925075v2-gem-3d-gaussian-splatting-for-efficient-and-accurate-cryo-em-reconst.html">GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction</a></td>
  <td>GEMï¼šåŸºäº3Dé«˜æ–¯æº…å°„çš„å†·å†»ç”µé•œé«˜æ•ˆç²¾ç¡®é‡å»ºæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25075v2" data-paper-url="./papers/250925075v2-gem-3d-gaussian-splatting-for-efficient-and-accurate-cryo-em-reconst.html" onclick="toggleFavorite(this, '2509.25075v2', 'GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250924528v3-core-3d-context-aware-open-vocabulary-retrieval-by-embeddings-in-3d.html">CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D</a></td>
  <td>CORE-3Dï¼šé€šè¿‡3DåµŒå…¥å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œå®ç°å¼€æ”¾è¯æ±‡çš„3Dåœºæ™¯æ£€ç´¢</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">semantic mapping</span> <span class="paper-tag">semantic map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24528v3" data-paper-url="./papers/250924528v3-core-3d-context-aware-open-vocabulary-retrieval-by-embeddings-in-3d.html" onclick="toggleFavorite(this, '2509.24528v3', 'CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250924421v2-proxy-gs-efficient-3d-gaussian-splatting-via-proxy-mesh.html">Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh</a></td>
  <td>Proxy-GSï¼šåˆ©ç”¨ä»£ç†ç½‘æ ¼å®ç°é«˜æ•ˆçš„3Dé«˜æ–¯æº…å°„ï¼Œæå‡æ¸²æŸ“é€Ÿåº¦ä¸è´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24421v2" data-paper-url="./papers/250924421v2-proxy-gs-efficient-3d-gaussian-splatting-via-proxy-mesh.html" onclick="toggleFavorite(this, '2509.24421v2', 'Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250925122v1-triangle-splatting-differentiable-rendering-with-opaque-triangles.html">Triangle Splatting+: Differentiable Rendering with Opaque Triangles</a></td>
  <td>Triangle Splatting+ï¼šæå‡ºåŸºäºä¸é€æ˜ä¸‰è§’å½¢çš„å¯å¾®æ¸²æŸ“æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆç½‘æ ¼é‡å»ºä¸æ–°è§†è§’åˆæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25122v1" data-paper-url="./papers/250925122v1-triangle-splatting-differentiable-rendering-with-opaque-triangles.html" onclick="toggleFavorite(this, '2509.25122v1', 'Triangle Splatting+: Differentiable Rendering with Opaque Triangles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250925191v2-vggt-x-when-vggt-meets-dense-novel-view-synthesis.html">VGGT-X: When VGGT Meets Dense Novel View Synthesis</a></td>
  <td>VGGT-Xï¼šé’ˆå¯¹å¯†é›†åœºæ™¯çš„æ–°è§†è§’åˆæˆï¼Œæå‡3DåŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">VGGT</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25191v2" data-paper-url="./papers/250925191v2-vggt-x-when-vggt-meets-dense-novel-view-synthesis.html" onclick="toggleFavorite(this, '2509.25191v2', 'VGGT-X: When VGGT Meets Dense Novel View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250924681v1-classifier-centric-adaptive-framework-for-open-vocabulary-camouflage.html">Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation</a></td>
  <td>æå‡ºåˆ†ç±»å™¨ä¸ºä¸­å¿ƒçš„è‡ªé€‚åº”æ¡†æ¶ï¼Œæå‡å¼€æ”¾è¯æ±‡ä¼ªè£…ç›®æ ‡åˆ†å‰²æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24681v1" data-paper-url="./papers/250924681v1-classifier-centric-adaptive-framework-for-open-vocabulary-camouflage.html" onclick="toggleFavorite(this, '2509.24681v1', 'Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250925603v1-gaussianlens-localized-high-resolution-reconstruction-via-on-demand-.html">GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification</a></td>
  <td>GaussianLensï¼šåŸºäºæŒ‰éœ€é«˜æ–¯è‡´å¯†åŒ–çš„å±€éƒ¨é«˜åˆ†è¾¨ç‡é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25603v1" data-paper-url="./papers/250925603v1-gaussianlens-localized-high-resolution-reconstruction-via-on-demand-.html" onclick="toggleFavorite(this, '2509.25603v1', 'GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/250924893v3-hbsplat-robust-sparse-view-gaussian-reconstruction-with-hybrid-loss-.html">HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss Guided Depth and Bidirectional Warping</a></td>
  <td>HBSplatï¼šåŸºäºæ··åˆæŸå¤±å¼•å¯¼æ·±åº¦å’ŒåŒå‘æ‰­æ›²çš„é²æ£’ç¨€ç–è§†è§’é«˜æ–¯é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24893v3" data-paper-url="./papers/250924893v3-hbsplat-robust-sparse-view-gaussian-reconstruction-with-hybrid-loss-.html" onclick="toggleFavorite(this, '2509.24893v3', 'HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss Guided Depth and Bidirectional Warping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/250925413v2-depthlm-metric-depth-from-vision-language-models.html">DepthLM: Metric Depth From Vision Language Models</a></td>
  <td>DepthLMï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å®ç°åº¦é‡æ·±åº¦ä¼°è®¡ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æŸå¤±å‡½æ•°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25413v2" data-paper-url="./papers/250925413v2-depthlm-metric-depth-from-vision-language-models.html" onclick="toggleFavorite(this, '2509.25413v2', 'DepthLM: Metric Depth From Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250924758v4-exgs-extreme-3d-gaussian-compression-with-diffusion-priors.html">ExGS: Extreme 3D Gaussian Compression with Diffusion Priors</a></td>
  <td>ExGSï¼šåˆ©ç”¨æ‰©æ•£å…ˆéªŒå®ç°æç«¯3Dé«˜æ–¯å‹ç¼©ï¼Œå…¼é¡¾é«˜è´¨é‡æ¸²æŸ“</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24758v4" data-paper-url="./papers/250924758v4-exgs-extreme-3d-gaussian-compression-with-diffusion-priors.html" onclick="toggleFavorite(this, '2509.24758v4', 'ExGS: Extreme 3D Gaussian Compression with Diffusion Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250925001v1-lvt-large-scale-scene-reconstruction-via-local-view-transformers.html">LVT: Large-Scale Scene Reconstruction via Local View Transformers</a></td>
  <td>æå‡ºå±€éƒ¨è§†å›¾Transformer(LVT)ï¼Œç”¨äºå¤§è§„æ¨¡åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25001v1" data-paper-url="./papers/250925001v1-lvt-large-scale-scene-reconstruction-via-local-view-transformers.html" onclick="toggleFavorite(this, '2509.25001v1', 'LVT: Large-Scale Scene Reconstruction via Local View Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250924966v1-social-3d-scene-graphs-modeling-human-actions-and-relations-for-inte.html">Social 3D Scene Graphs: Modeling Human Actions and Relations for Interactive Service Robots</a></td>
  <td>æå‡ºSocial 3D Scene Graphsï¼Œç”¨äºäº¤äº’å¼æœåŠ¡æœºå™¨äººç†è§£äººç±»è¡Œä¸ºä¸å…³ç³»</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24966v1" data-paper-url="./papers/250924966v1-social-3d-scene-graphs-modeling-human-actions-and-relations-for-inte.html" onclick="toggleFavorite(this, '2509.24966v1', 'Social 3D Scene Graphs: Modeling Human Actions and Relations for Interactive Service Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>51</td>
  <td><a href="./papers/250925183v1-pad3r-pose-aware-dynamic-3d-reconstruction-from-casual-videos.html">PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos</a></td>
  <td>PAD3Rï¼šä»å•ç›®è§†é¢‘ä¸­è¿›è¡Œå§¿æ€æ„ŸçŸ¥çš„åŠ¨æ€3Dé‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25183v1" data-paper-url="./papers/250925183v1-pad3r-pose-aware-dynamic-3d-reconstruction-from-casual-videos.html" onclick="toggleFavorite(this, '2509.25183v1', 'PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>52</td>
  <td><a href="./papers/250925146v1-fast-feature-field-textf3-a-predictive-representation-of-events.html">Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events</a></td>
  <td>æå‡ºå¿«é€Ÿç‰¹å¾åœºï¼ˆFÂ³ï¼‰ï¼Œç”¨äºäº‹ä»¶ç›¸æœºæ•°æ®çš„é¢„æµ‹æ€§è¡¨å¾å­¦ä¹ ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25146v1" data-paper-url="./papers/250925146v1-fast-feature-field-textf3-a-predictive-representation-of-events.html" onclick="toggleFavorite(this, '2509.25146v1', 'Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>53</td>
  <td><a href="./papers/250924572v1-scope-semantic-conditioning-for-sim2real-category-level-object-pose-.html">SCOPE: Semantic Conditioning for Sim2Real Category-Level Object Pose Estimation in Robotics</a></td>
  <td>SCOPEï¼šåŸºäºè¯­ä¹‰æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æœºå™¨äººSim2Realç±»åˆ«çº§ç‰©ä½“å§¿æ€ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim2real</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24572v1" data-paper-url="./papers/250924572v1-scope-semantic-conditioning-for-sim2real-category-level-object-pose-.html" onclick="toggleFavorite(this, '2509.24572v1', 'SCOPE: Semantic Conditioning for Sim2Real Category-Level Object Pose Estimation in Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>54</td>
  <td><a href="./papers/250924441v1-neoworld-neural-simulation-of-explorable-virtual-worlds-via-progress.html">NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding</a></td>
  <td>NeoWorldï¼šé€šè¿‡æ¸è¿›å¼3Då±•å¼€å®ç°å¯æ¢ç´¢è™šæ‹Ÿä¸–ç•Œçš„ç¥ç»æ¨¡æ‹Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span> <span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24441v1" data-paper-url="./papers/250924441v1-neoworld-neural-simulation-of-explorable-virtual-worlds-via-progress.html" onclick="toggleFavorite(this, '2509.24441v1', 'NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>55</td>
  <td><a href="./papers/250924871v1-streamforest-efficient-online-video-understanding-with-persistent-ev.html">StreamForest: Efficient Online Video Understanding with Persistent Event Memory</a></td>
  <td>æå‡ºStreamForestï¼Œåˆ©ç”¨æŒä¹…äº‹ä»¶è®°å¿†å®ç°é«˜æ•ˆçš„åœ¨çº¿è§†é¢‘ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24871v1" data-paper-url="./papers/250924871v1-streamforest-efficient-online-video-understanding-with-persistent-ev.html" onclick="toggleFavorite(this, '2509.24871v1', 'StreamForest: Efficient Online Video Understanding with Persistent Event Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/250924997v1-panoworld-x-generating-explorable-panoramic-worlds-via-sphere-aware-.html">PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion</a></td>
  <td>PanoWorld-Xï¼šåŸºäºçƒé¢æ„ŸçŸ¥è§†é¢‘æ‰©æ•£ç”Ÿæˆå¯æ¢ç´¢å…¨æ™¯ä¸–ç•Œ</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24997v1" data-paper-url="./papers/250924997v1-panoworld-x-generating-explorable-panoramic-worlds-via-sphere-aware-.html" onclick="toggleFavorite(this, '2509.24997v1', 'PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>57</td>
  <td><a href="./papers/250924850v2-phase-net-physics-grounded-harmonic-attention-system-for-efficient-r.html">PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement</a></td>
  <td>æå‡ºPHASE-Netï¼Œé€šè¿‡ç‰©ç†é©±åŠ¨çš„è°æ³¢æ³¨æ„åŠ›æœºåˆ¶å®ç°é«˜æ•ˆçš„è¿œç¨‹å…‰ç”µå®¹ç§¯è„‰ææ³¢æµ‹é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">PULSE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24850v2" data-paper-url="./papers/250924850v2-phase-net-physics-grounded-harmonic-attention-system-for-efficient-r.html" onclick="toggleFavorite(this, '2509.24850v2', 'PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>58</td>
  <td><a href="./papers/250924469v2-lamogen-laban-movement-guided-diffusion-for-text-to-motion-generatio.html">LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation</a></td>
  <td>æå‡ºLaMoGenä»¥è§£å†³æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆä¸­çš„è¡¨è¾¾æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion synthesis</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24469v2" data-paper-url="./papers/250924469v2-lamogen-laban-movement-guided-diffusion-for-text-to-motion-generatio.html" onclick="toggleFavorite(this, '2509.24469v2', 'LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>59</td>
  <td><a href="./papers/250924370v1-dinoreg-strong-point-cloud-registration-with-vision-foundation-model.html">DINOReg: Strong Point Cloud Registration with Vision Foundation Model</a></td>
  <td>DINORegï¼šåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å®ç°å¼ºå¤§çš„ç‚¹äº‘é…å‡†</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.24370v1" data-paper-url="./papers/250924370v1-dinoreg-strong-point-cloud-registration-with-vision-foundation-model.html" onclick="toggleFavorite(this, '2509.24370v1', 'DINOReg: Strong Point Cloud Registration with Vision Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>60</td>
  <td><a href="./papers/250925390v1-spinbench-perspective-and-rotation-as-a-lens-on-spatial-reasoning-in.html">SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs</a></td>
  <td>æå‡ºSpinBenchä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25390v1" data-paper-url="./papers/250925390v1-spinbench-perspective-and-rotation-as-a-lens-on-spatial-reasoning-in.html" onclick="toggleFavorite(this, '2509.25390v1', 'SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)