---
layout: default
title: Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models
---

# Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24837" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24837v1</a>
  <a href="https://arxiv.org/pdf/2509.24837.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24837v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24837v1', 'Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youngeun Kim, Youjia Zhang, Huiling Liu, Aecheon Jung, Sunwoo Lee, Sungeun Hong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè®­ç»ƒæ— å…³çš„ä»¤ç‰Œä¿®å‰ªæ–¹æ³•ä»¥é™ä½è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ä»¤ç‰Œä¿®å‰ª` `å¤šæ¨¡æ€æ¨ç†` `é›¶é˜¶æ¢¯åº¦ä¼°è®¡` `æ¨ç†æ•ˆç‡` `æ•æ„Ÿæ€§è¯„ä¼°` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»¤ç‰Œä¿®å‰ªæ–¹æ³•åœ¨å¤„ç†å†—ä½™è§†è§‰ä»¤ç‰Œæ—¶é¢ä¸´ç¨³å®šæ€§å’Œå‡†ç¡®æ€§çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé›¶é˜¶æ¢¯åº¦ä¼°è®¡çš„è®­ç»ƒæ— å…³ä»¤ç‰Œä¿®å‰ªæ–¹æ³•ï¼Œåˆ©ç”¨ä»¤ç‰Œçš„æ•æ„Ÿæ€§æ¥é€‰æ‹©å¯¹æ¨¡å‹è¾“å‡ºå½±å“è¾ƒå¤§çš„ä»¤ç‰Œã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä¿®å‰ªé«˜è¾¾94.4%çš„ä»¤ç‰Œï¼ŒåŒæ—¶æå‡æ¨ç†é€Ÿåº¦è‡³2.30å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå†—ä½™è§†è§‰ä»¤ç‰Œå¯¼è‡´çš„æ¨ç†æˆæœ¬è¾ƒé«˜ï¼Œæˆä¸ºäº†ä¸€ä¸ªä¸»è¦é—®é¢˜ã€‚ç°æœ‰çš„ä»¤ç‰Œä¿®å‰ªæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼ŒåŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ä¾èµ–äºä¸ç¨³å®šçš„åŸå§‹æ³¨æ„åŠ›åˆ†æ•°ï¼Œè€ŒåŸºäºå¤šæ ·æ€§çš„æ–¹æ³•åˆ™å¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³çš„æ¡†æ¶ï¼ŒåŸºäºç®€å•çš„ç›´è§‰ï¼šæ•æ„Ÿæ€§é«˜çš„ä»¤ç‰Œæ›´å¯èƒ½å½±å“æ¨¡å‹è¾“å‡ºï¼Œå¹¶åº”æ•è·äº’è¡¥çš„è§†è§‰çº¿ç´¢ã€‚é€šè¿‡åœ¨æŠ•å½±å±‚ä½¿ç”¨é›¶é˜¶æ‰°åŠ¨ä¼°è®¡ä»¤ç‰Œæ•æ„Ÿæ€§ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œåå‘ä¼ æ’­çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è½»é‡çº§çš„å‰å‘ä¼ é€’æ¥è¿‘ä¼¼æ¯ä¸ªä»¤ç‰Œçš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªVLMå’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæœ€å¤šå¯ä¿®å‰ª94.4%çš„ä»¤ç‰Œï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—æé«˜æ•ˆç‡ï¼Œæ¨ç†é€Ÿåº¦æ¯”åŸºçº¿å¿«2.30å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç”±äºå†—ä½™è§†è§‰ä»¤ç‰Œå¯¼è‡´çš„é«˜æ¨ç†æˆæœ¬é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©ä»¤ç‰Œæ—¶å­˜åœ¨ä¸ç¨³å®šæ€§å’Œä¿¡æ¯ä¸¢å¤±çš„é£é™©ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„æ¡†æ¶åŸºäºä»¤ç‰Œæ•æ„Ÿæ€§è¿›è¡Œä¿®å‰ªï¼Œè®¤ä¸ºæ•æ„Ÿæ€§é«˜çš„ä»¤ç‰Œæ›´èƒ½å½±å“æ¨¡å‹è¾“å‡ºï¼Œå¹¶ä¸”åº”å½“æ•è·äº’è¡¥çš„è§†è§‰ä¿¡æ¯è€Œéé‡å ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆåœ¨æŠ•å½±å±‚è¿›è¡Œé›¶é˜¶æ‰°åŠ¨ä¼°è®¡ï¼Œä»¥è¯„ä¼°æ¯ä¸ªä»¤ç‰Œçš„æ•æ„Ÿæ€§ï¼›å…¶æ¬¡æ ¹æ®æ•æ„Ÿæ€§é€‰æ‹©ä»¤ç‰Œè¿›è¡Œä¿®å‰ªï¼Œæœ€ç»ˆå½¢æˆä¼˜åŒ–çš„ä»¤ç‰Œé›†åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨é›¶é˜¶æ‰°åŠ¨ä¼°è®¡æ¥è¯„ä¼°ä»¤ç‰Œçš„æ•æ„Ÿæ€§ï¼Œè¿™ä¸€æ–¹æ³•é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹æ³¨æ„åŠ›åˆ†æ•°çš„ä¾èµ–ï¼Œæä¾›äº†æ›´ä¸ºç¨³å®šå’Œæœ‰æ•ˆçš„ä»¤ç‰Œé€‰æ‹©æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæœ¬æ–‡è®¾è®¡äº†è½»é‡çº§çš„å‰å‘ä¼ é€’è¿‡ç¨‹ï¼Œä»¥å®ç°å¯¹ä»¤ç‰Œæ•æ„Ÿæ€§çš„å¿«é€Ÿä¼°è®¡ï¼Œé¿å…äº†å¤æ‚çš„åå‘ä¼ æ’­è¿‡ç¨‹ï¼Œç¡®ä¿äº†é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¿®å‰ªé«˜è¾¾94.4%çš„ä»¤ç‰Œï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¹¶å®ç°æ¨ç†é€Ÿåº¦æå‡è‡³åŸºçº¿çš„2.30å€ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬çš„è”åˆç†è§£ã€æ™ºèƒ½æœç´¢å¼•æ“ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­é™ä½è®¡ç®—æˆæœ¬ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (VLMs) enable strong multimodal reasoning but incur heavy inference costs from redundant visual tokens. Token pruning alleviates this issue, yet existing approaches face limitations. Attention-based methods rely on raw attention scores, which are often unstable across layers and heads and can lead to redundant selections. Diversity-based methods improve robustness by selecting tokens far apart in feature space but risk dropping regions needed for accurate prediction. We propose \ours, a training-free framework built on a simple intuition: tokens with higher sensitivity are more likely to influence the model's output, and they should also capture complementary visual cues rather than overlapping information. To achieve this, we estimate token sensitivity using zeroth-order perturbations at the projection layer, a shallow and computationally light component of the model. This approach measures how small random perturbations affect the projection outputs, allowing us to approximate each token's influence through lightweight forward passes without backpropagation. Extensive experiments across multiple VLMs and benchmarks show that \ours consistently outperforms prior methods, pruning up to 94.4\% of tokens while maintaining accuracy and significantly improving efficiency, achieving up to 2.30x faster end-to-end inference over the baseline.

