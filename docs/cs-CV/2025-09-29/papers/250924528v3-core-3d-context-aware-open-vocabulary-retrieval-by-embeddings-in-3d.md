---
layout: default
title: CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D
---

# CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24528" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24528v3</a>
  <a href="https://arxiv.org/pdf/2509.24528.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24528v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24528v3', 'CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamad Amin Mirzaei, Pantea Amoie, Ali Ekhterachian, Matin Mirzababaei, Babak Khalaj

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-12-07)

**å¤‡æ³¨**: Submitted for ICLR 2026 conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CORE-3Dï¼šé€šè¿‡3DåµŒå…¥å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œå®ç°å¼€æ”¾è¯æ±‡çš„3Dåœºæ™¯æ£€ç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¼€æ”¾è¯æ±‡æ£€ç´¢` `è§†è§‰-è¯­è¨€æ¨¡å‹` `è¯­ä¹‰åˆ†å‰²` `å¯¹è±¡æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç›´æ¥ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ©ç è¿›è¡Œ3Dè¯­ä¹‰æ˜ å°„ï¼Œå¯¼è‡´æ©ç è´¨é‡å·®å’Œè¯­ä¹‰åˆ†é…ä¸å‡†ç¡®ã€‚
2. CORE-3Dåˆ©ç”¨SemanticSAMç”Ÿæˆé«˜è´¨é‡å¯¹è±¡çº§æ©ç ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ï¼Œæå‡è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCORE-3Dåœ¨3Dè¯­ä¹‰åˆ†å‰²å’ŒåŸºäºè¯­è¨€æŸ¥è¯¢çš„å¯¹è±¡æ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº3DåµŒå…¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ£€ç´¢æ–¹æ³•CORE-3Dï¼Œæ—¨åœ¨æå‡å…·èº«AIå’Œæœºå™¨äººé¢†åŸŸä¸­3Dåœºæ™¯ç†è§£çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”Ÿæˆçš„2Dç±»åˆ«æ— å…³æ©ç ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°3Dç©ºé—´ï¼Œå¯¼è‡´æ©ç ç¢ç‰‡åŒ–å’Œè¯­ä¹‰åˆ†é…ä¸å‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨SemanticSAMè¿›è¡Œæ¸è¿›å¼ç²’åº¦ç»†åŒ–ï¼Œç”Ÿæˆæ›´å‡†ç¡®å’Œä¸°å¯Œçš„å¯¹è±¡çº§æ©ç ï¼Œä»è€Œå‡è½»äº†ä¼ ç»ŸSAMç­‰æ©ç ç”Ÿæˆæ¨¡å‹ä¸­å¸¸è§çš„è¿‡åº¦åˆ†å‰²é—®é¢˜ï¼Œå¹¶æ”¹å–„äº†ä¸‹æ¸¸çš„3Dè¯­ä¹‰åˆ†å‰²æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é›†æˆäº†æ¯ä¸ªæ©ç çš„å¤šä¸ªä¸Šä¸‹æ–‡è§†å›¾ï¼Œå¹¶ä½¿ç”¨ç»éªŒç¡®å®šçš„æƒé‡è¿›è¡ŒåŠ æƒï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„è§†è§‰ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ª3Dåœºæ™¯ç†è§£ä»»åŠ¡ï¼ˆåŒ…æ‹¬3Dè¯­ä¹‰åˆ†å‰²å’ŒåŸºäºè¯­è¨€æŸ¥è¯¢çš„å¯¹è±¡æ£€ç´¢ï¼‰ä»¥åŠå¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„3Dåœºæ™¯ç†è§£æ–¹æ³•ï¼Œä¾èµ–äºå°†2Dæ©ç æŠ•å½±åˆ°3Dç©ºé—´ã€‚è¿™äº›æ–¹æ³•ç›´æ¥ä½¿ç”¨åŸå§‹æ©ç ï¼Œå®¹æ˜“äº§ç”Ÿç¢ç‰‡åŒ–å’Œä¸å‡†ç¡®çš„è¯­ä¹‰åˆ†é…ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºç¼ºä¹å¯¹æ©ç è´¨é‡çš„ä¼˜åŒ–å’Œå¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å……åˆ†åˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCORE-3Dçš„æ ¸å¿ƒæ€è·¯æ˜¯é¦–å…ˆé€šè¿‡SemanticSAMç”Ÿæˆé«˜è´¨é‡çš„å¯¹è±¡çº§æ©ç ï¼Œç„¶ååˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ç­–ç•¥ï¼Œå°†æ¯ä¸ªæ©ç çš„å¤šä¸ªä¸Šä¸‹æ–‡è§†å›¾è¿›è¡Œæ•´åˆï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„è§†è§‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†æå‡æ©ç çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰çš„å®Œæ•´æ€§ï¼Œä»è€Œæ”¹å–„3Dåœºæ™¯ç†è§£çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCORE-3Dçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨SemanticSAMè¿›è¡Œæ©ç ç”Ÿæˆï¼Œé€šè¿‡æ¸è¿›å¼ç²’åº¦ç»†åŒ–ï¼Œå¾—åˆ°æ›´å‡†ç¡®çš„å¯¹è±¡çº§æ©ç ï¼›2) å¯¹æ¯ä¸ªæ©ç æå–å¤šä¸ªä¸Šä¸‹æ–‡è§†å›¾ï¼›3) ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ç­–ç•¥ï¼Œå¯¹è¿™äº›è§†å›¾è¿›è¡Œç¼–ç ï¼Œå¹¶ä½¿ç”¨ç»éªŒç¡®å®šçš„æƒé‡è¿›è¡ŒåŠ æƒï¼›4) å°†ç¼–ç åçš„ç‰¹å¾æŠ•å½±åˆ°3Dç©ºé—´ï¼Œç”¨äº3Dè¯­ä¹‰åˆ†å‰²å’Œå¯¹è±¡æ£€ç´¢ç­‰ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCORE-3Dçš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) åˆ©ç”¨SemanticSAMç”Ÿæˆé«˜è´¨é‡å¯¹è±¡çº§æ©ç ï¼Œæœ‰æ•ˆç¼“è§£äº†ä¼ ç»ŸSAMç­‰æ©ç ç”Ÿæˆæ¨¡å‹ä¸­å¸¸è§çš„è¿‡åº¦åˆ†å‰²é—®é¢˜ï¼›2) æå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ç­–ç•¥ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªä¸Šä¸‹æ–‡è§†å›¾ï¼Œæ˜¾è‘—æå‡äº†è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ç­–ç•¥ä¸­ï¼Œå…³é”®çš„è®¾è®¡åœ¨äºå¦‚ä½•é€‰æ‹©ä¸Šä¸‹æ–‡è§†å›¾ä»¥åŠå¦‚ä½•ç¡®å®šæ¯ä¸ªè§†å›¾çš„æƒé‡ã€‚è®ºæ–‡ä¸­æåˆ°ï¼Œæƒé‡çš„ç¡®å®šæ˜¯åŸºäºç»éªŒçš„ï¼Œå…·ä½“æ–¹æ³•æœªçŸ¥ã€‚æ­¤å¤–ï¼ŒSemanticSAMçš„å‚æ•°è®¾ç½®å’Œè®­ç»ƒç»†èŠ‚ï¼Œä»¥åŠ3DæŠ•å½±çš„å…·ä½“æ–¹æ³•ï¼Œä¹Ÿæ˜¯å½±å“æœ€ç»ˆæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œä½†è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCORE-3Dåœ¨3Dè¯­ä¹‰åˆ†å‰²å’ŒåŸºäºè¯­è¨€æŸ¥è¯¢çš„å¯¹è±¡æ£€ç´¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†CORE-3Dä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æå‡3Dåœºæ™¯ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ©ç”¨SemanticSAMå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ï¼ŒCORE-3Dèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®çš„æ©ç å’Œæ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CORE-3Dåœ¨å…·èº«AIå’Œæœºå™¨äººé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€åœºæ™¯ç†è§£ã€ç‰©ä½“äº¤äº’ç­‰ã€‚è¯¥æŠ€æœ¯å¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„äº¤äº’å’Œå¯¼èˆªã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸï¼Œæå‡ç”¨æˆ·åœ¨3Dç¯å¢ƒä¸­çš„ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.

