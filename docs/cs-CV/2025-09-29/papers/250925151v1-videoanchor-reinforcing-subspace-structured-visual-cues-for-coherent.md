---
layout: default
title: VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning
---

# VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25151" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25151v1</a>
  <a href="https://arxiv.org/pdf/2509.25151.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25151v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25151v1', 'VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaozhi Wang, Tong Zhang, Mingyue Guo, Yaowei Wang, Qixiang Ye

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 16 pages, 6 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/feufhd/VideoAnchor)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VideoAnchorï¼šé€šè¿‡å¼ºåŒ–å­ç©ºé—´ç»“æ„è§†è§‰çº¿ç´¢å®ç°è¿è´¯çš„è§†è§‰-ç©ºé—´æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰ç©ºé—´æ¨ç†` `å­ç©ºé—´èšç±»` `æ³¨æ„åŠ›æœºåˆ¶` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç©ºé—´æ¨ç†ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸»è¦åŸå› æ˜¯è§†è§‰tokenå®¹æ˜“è¢«è¯­è¨€tokenæ©ç›–ï¼Œå¯¼è‡´è·¨å¸§è§†è§‰çº¿ç´¢è¯†åˆ«ä¸ä¸€è‡´ã€‚
2. è®ºæ–‡æå‡ºVideoAnchoræ¨¡å—ï¼Œåˆ©ç”¨ç¨€ç–å­ç©ºé—´èšç±»çš„è‡ªè¡¨è¾¾å±æ€§ï¼Œé€šè¿‡å­ç©ºé—´äº²å’Œæ€§å¼ºåŒ–è·¨å¸§è§†è§‰çº¿ç´¢ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoAnchoråœ¨VSI-Benchå’ŒVideo-MMEç­‰ç©ºé—´ç›¸å…³ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨ä¸åŒéª¨å¹²æ¨¡å‹å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è§†è§‰-è¯­è¨€å¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è§†è§‰-ç©ºé—´æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬é¦–å…ˆæŒ‡å‡ºï¼Œè¿™ç§å±€é™æ€§æºäºæ³¨æ„åŠ›æœºåˆ¶ï¼šè§†è§‰tokenè¢«è¯­è¨€tokenæ‰€æ©ç›–ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•è·¨å¸§ä¸€è‡´åœ°è¯†åˆ«ç›¸åŒçš„è§†è§‰çº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–å­ç©ºé—´èšç±»ä¸­çš„è‡ªè¡¨è¾¾å±æ€§ä¸Transformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¹‹é—´çš„æ–°é¢–è”ç³»ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºVideoAnchorï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼Œå®ƒåˆ©ç”¨å­ç©ºé—´äº²å’Œæ€§æ¥å¼ºåŒ–è·¨å¸§çš„è§†è§‰çº¿ç´¢ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œä»è€Œæœ‰æ•ˆåœ°å°†æ³¨æ„åŠ›é”šå®šåˆ°å…±äº«çš„è§†è§‰ç»“æ„ä¸Šã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œéª¨å¹²æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ€§èƒ½å¾—åˆ°äº†æŒç»­æå‡ï¼Œä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨InternVL2-8Bå’ŒQwen2.5VL-72Bæ—¶ï¼Œåœ¨VSI-Benchå’ŒVideo-MMEï¼ˆç©ºé—´ç›¸å…³ä»»åŠ¡ï¼‰ä¸Šåˆ†åˆ«æé«˜äº†3.2%å’Œ4.6%ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œæ›´è¿è´¯çš„å­ç©ºé—´åˆ’åˆ†å’Œæ›´å¼ºçš„è§†è§‰åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨https://github.com/feufhd/VideoAnchorä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†é¢‘æ—¶ï¼Œç”±äºè§†è§‰tokenå®¹æ˜“è¢«è¯­è¨€tokenä¸»å¯¼ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥åœ¨ä¸åŒå¸§ä¹‹é—´ä¿æŒå¯¹åŒä¸€è§†è§‰çº¿ç´¢çš„ç¨³å®šå…³æ³¨ï¼Œä»è€Œå½±å“äº†è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆæœºåˆ¶æ¥å¼ºåŒ–å’Œä¿æŒè§†è§‰ä¿¡æ¯çš„è¿è´¯æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘å¸§ä¸­çš„è§†è§‰tokenè§†ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„æ•°æ®ç‚¹ï¼Œå¹¶åˆ©ç”¨ç¨€ç–å­ç©ºé—´èšç±»çš„æ€æƒ³ï¼Œå‡è®¾è¿™äº›æ•°æ®ç‚¹ä½äºè‹¥å¹²ä¸ªä½ç»´å­ç©ºé—´ä¸­ã€‚é€šè¿‡è®¡ç®—tokenä¹‹é—´çš„å­ç©ºé—´äº²å’Œæ€§ï¼Œå¯ä»¥è¯†åˆ«å‡ºå±äºåŒä¸€è§†è§‰ç»“æ„çš„tokenï¼Œå¹¶ä»¥æ­¤æ¥å¼ºåŒ–è¿™äº›tokençš„æ³¨æ„åŠ›æƒé‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVideoAnchoræ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ·»åŠ åˆ°ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) ä»è§†é¢‘å¸§ä¸­æå–è§†è§‰ç‰¹å¾ï¼›2) è®¡ç®—è§†è§‰tokenä¹‹é—´çš„å­ç©ºé—´äº²å’Œæ€§çŸ©é˜µï¼›3) åˆ©ç”¨äº²å’Œæ€§çŸ©é˜µè°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œå¼ºåŒ–å±äºåŒä¸€å­ç©ºé—´çš„tokenï¼›4) å°†è°ƒæ•´åçš„è§†è§‰ç‰¹å¾è¾“å…¥åˆ°åç»­çš„è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç¨€ç–å­ç©ºé—´èšç±»çš„è‡ªè¡¨è¾¾å±æ€§ä¸Transformerçš„æ³¨æ„åŠ›æœºåˆ¶è”ç³»èµ·æ¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å­ç©ºé—´äº²å’Œæ€§æ¥å¼ºåŒ–è§†è§‰çº¿ç´¢çš„æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVideoAnchoræ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå³å¯æœ‰æ•ˆåœ°æå‡è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå­ç©ºé—´äº²å’Œæ€§çŸ©é˜µçš„è®¡ç®—æ˜¯å…³é”®ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºç¨€ç–è¡¨ç¤ºçš„æ–¹æ³•ï¼Œé€šè¿‡æ±‚è§£ä¸€ä¸ªç¨€ç–ç¼–ç é—®é¢˜æ¥ä¼°è®¡tokenä¹‹é—´çš„äº²å’Œæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªtokenï¼Œæ‰¾åˆ°å…¶ä»–tokençš„ä¸€ä¸ªç¨€ç–çº¿æ€§ç»„åˆæ¥è¡¨ç¤ºå®ƒï¼Œç»„åˆç³»æ•°å³ä¸ºäº²å’Œæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ç§æ³¨æ„åŠ›æƒé‡è°ƒæ•´æœºåˆ¶ï¼Œæ ¹æ®äº²å’Œæ€§çŸ©é˜µæ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œå¼ºåŒ–å±äºåŒä¸€å­ç©ºé—´çš„tokenã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoAnchoråœ¨VSI-Benchå’ŒVideo-MMEç­‰ç©ºé—´ç›¸å…³ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨InternVL2-8Bæ¨¡å‹æ—¶ï¼ŒVSI-Benchçš„æ€§èƒ½æå‡äº†3.2%ï¼ŒVideo-MMEçš„æ€§èƒ½æå‡äº†4.6%ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¡¨æ˜ï¼ŒVideoAnchorèƒ½å¤Ÿäº§ç”Ÿæ›´è¿è´¯çš„å­ç©ºé—´åˆ’åˆ†å’Œæ›´å¼ºçš„è§†è§‰åŸºç¡€ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VideoAnchorå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæå‡è§†é¢‘ç†è§£ã€è§†é¢‘é—®ç­”ã€è§†é¢‘ç›®æ ‡è·Ÿè¸ªç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¢å¼ºæ¨¡å‹å¯¹è§†é¢‘ä¸­ç©ºé—´ä¿¡æ¯çš„ç†è§£ï¼Œæé«˜æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language alignment, yet they remain limited in visual-spatial reasoning. We first identify that this limitation arises from the attention mechanism: visual tokens are overshadowed by language tokens, preventing the model from consistently recognizing the same visual cues across frames. To address this challenge, we draw a novel connection between the self-expressiveness property in sparse subspace clustering and the attention mechanism in Transformers. Building on this insight, we propose VideoAnchor, a plug-and-play module that leverages subspace affinities to reinforce visual cues across frames without retraining, effectively anchoring attention to shared visual structures. Extensive experiments across benchmarks and backbone models show consistent performance gains -- $e.g.$, 3.2% and 4.6% improvements on VSI-Bench and Video-MME (spatial-related tasks) with InternVL2-8B and Qwen2.5VL-72B -- while qualitative analyses demonstrate more coherent subspace partitions and stronger visual grounding. Our codes will be made public available at https://github.com/feufhd/VideoAnchor.

