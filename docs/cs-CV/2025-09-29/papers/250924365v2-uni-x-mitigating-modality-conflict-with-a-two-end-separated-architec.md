---
layout: default
title: Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models
---

# Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24365" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24365v2</a>
  <a href="https://arxiv.org/pdf/2509.24365.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24365v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24365v2', 'Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jitai Hao, Hao Liu, Xinyan Xiao, Qiang Huang, Jun Yu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-11-29)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CURRENTF/Uni-X)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni-Xæ¨¡å‹ï¼Œé€šè¿‡ä¸¤ç«¯åˆ†ç¦»æ¶æ„ç¼“è§£å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ä¸­çš„æ¨¡æ€å†²çªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç»Ÿä¸€æ¨¡å‹` `æ¢¯åº¦å†²çª` `Transformer` `æ¨¡å‹æ¶æ„` `å›¾åƒç”Ÿæˆ` `è§†è§‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨è®­ç»ƒæ—¶ï¼Œæµ…å±‚å’Œæ·±å±‚Transformerå­˜åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€é—´çš„æ¢¯åº¦å†²çªï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. Uni-Xé‡‡ç”¨ä¸¤ç«¯åˆ†ç¦»ã€ä¸­é—´å…±äº«çš„Xå½¢æ¶æ„ï¼Œç¼“è§£äº†æ¨¡æ€é—´çš„æ¢¯åº¦å†²çªï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-Xåœ¨è®­ç»ƒæ•ˆç‡ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ‰©å±•åˆ°3Bå‚æ•°æ—¶ï¼Œæ€§èƒ½å¯åª²ç¾7Bå‚æ•°çš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹(UMMs)å› å…¶æ¶æ„çš„ç®€æ´æ€§è€Œå¤‡å—å…³æ³¨ï¼Œå®ƒä»¬é€šå¸¸åŸºäºå…±äº«çš„è‡ªå›å½’(AR) Transformerã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šå½“åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸Šè®­ç»ƒæ—¶ï¼Œæ¨¡æ€å…±äº«çš„Transformerä¼šé­å—è§†è§‰å’Œæ–‡æœ¬ä¹‹é—´ä¸¥é‡çš„æ¢¯åº¦å†²çªï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚å’Œæ·±å±‚ã€‚æˆ‘ä»¬å°†æ­¤é—®é¢˜è¿½æº¯åˆ°å›¾åƒå’Œæ–‡æœ¬çš„ä½çº§ç»Ÿè®¡å±æ€§çš„æ ¹æœ¬å·®å¼‚ï¼ŒåŒæ—¶æ³¨æ„åˆ°å†²çªåœ¨è¡¨ç¤ºå˜å¾—æ›´æŠ½è±¡å’Œè¯­ä¹‰å¯¹é½çš„ä¸­é—´å±‚æœ‰æ‰€å‡å°‘ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Uni-Xï¼Œä¸€ç§ä¸¤ç«¯åˆ†ç¦»ã€ä¸­é—´å…±äº«çš„æ¶æ„ã€‚Uni-Xå°†å…¶åˆå§‹å±‚å’Œæœ€ç»ˆå±‚ä¸“ç”¨äºæ¨¡æ€ç‰¹å®šçš„å¤„ç†ï¼ŒåŒæ—¶åœ¨ä¸­é—´å±‚ä¿æŒå…±äº«å‚æ•°ä»¥è¿›è¡Œé«˜çº§è¯­ä¹‰èåˆã€‚è¿™ç§Xå½¢è®¾è®¡ä¸ä»…æ¶ˆé™¤äº†ä¸¤ç«¯çš„æ¢¯åº¦å†²çªï¼Œè€Œä¸”è¿›ä¸€æ­¥ç¼“è§£äº†å…±äº«å±‚ä¸­çš„æ®‹ä½™å†²çªã€‚å¤§é‡çš„å®éªŒéªŒè¯äº†Uni-Xçš„æœ‰æ•ˆæ€§ã€‚åœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹ï¼ŒUni-Xå®ç°äº†ä¼˜äºå¼ºåŸºçº¿çš„è®­ç»ƒæ•ˆç‡ã€‚å½“ä½¿ç”¨æ›´å¤§çš„è®­ç»ƒæ•°æ®æ‰©å±•åˆ°3Bå‚æ•°æ—¶ï¼ŒUni-XåŒ¹é…æˆ–è¶…è¿‡äº†7BåŸºäºARçš„UMMï¼Œåœ¨å›¾åƒç”Ÿæˆæ–¹é¢å®ç°äº†82çš„GenEvalåˆ†æ•°ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬å’Œè§†è§‰ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†Uni-Xä½œä¸ºæœªæ¥ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡çš„å‚æ•°é«˜æ•ˆä¸”å¯æ‰©å±•çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰ä¸­ï¼Œç”±äºè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ä½çº§ç»Ÿè®¡å±æ€§å·®å¼‚ï¼Œå¯¼è‡´åœ¨å…±äº«Transformerçš„æµ…å±‚å’Œæ·±å±‚å‡ºç°ä¸¥é‡çš„æ¢¯åº¦å†²çªé—®é¢˜ã€‚è¿™ç§å†²çªé˜»ç¢äº†æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒå’Œæ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸€ç§ä¸¤ç«¯åˆ†ç¦»ã€ä¸­é—´å…±äº«çš„Xå½¢æ¶æ„ï¼ˆUni-Xï¼‰ã€‚é€šè¿‡å°†æ¨¡å‹çš„åˆå§‹å±‚å’Œæœ€ç»ˆå±‚è®¾è®¡ä¸ºæ¨¡æ€ç‰¹å®šçš„å¤„ç†å±‚ï¼Œä¸“é—¨å¤„ç†å„è‡ªæ¨¡æ€çš„ä½çº§ç‰¹å¾ï¼Œä»è€Œæ¶ˆé™¤ä¸¤ç«¯çš„æ¢¯åº¦å†²çªã€‚ä¸­é—´å±‚åˆ™ä¿æŒå…±äº«ï¼Œç”¨äºè¿›è¡Œé«˜çº§è¯­ä¹‰èåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUni-Xæ¨¡å‹çš„æ•´ä½“æ¶æ„ç”±ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šæ¨¡æ€ç‰¹å®šçš„åˆå§‹å±‚ã€å…±äº«çš„ä¸­é—´å±‚å’Œæ¨¡æ€ç‰¹å®šçš„æœ€ç»ˆå±‚ã€‚è¾“å…¥æ•°æ®é¦–å…ˆé€šè¿‡å„è‡ªæ¨¡æ€çš„åˆå§‹å±‚è¿›è¡Œå¤„ç†ï¼Œæå–æ¨¡æ€ç‰¹å®šçš„ä½çº§ç‰¹å¾ã€‚ç„¶åï¼Œè¿™äº›ç‰¹å¾è¢«ä¼ é€’åˆ°å…±äº«çš„ä¸­é—´å±‚è¿›è¡Œé«˜çº§è¯­ä¹‰èåˆã€‚æœ€åï¼Œèåˆåçš„ç‰¹å¾é€šè¿‡å„è‡ªæ¨¡æ€çš„æœ€ç»ˆå±‚è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šUni-Xçš„å…³é”®åˆ›æ–°åœ¨äºå…¶Xå½¢æ¶æ„ï¼Œè¿™ç§æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ä¸­çš„æ¨¡æ€å†²çªé—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å…±äº«Transformeræ¶æ„ç›¸æ¯”ï¼ŒUni-Xé€šè¿‡ä¸¤ç«¯åˆ†ç¦»çš„è®¾è®¡ï¼Œé¿å…äº†åœ¨æµ…å±‚å’Œæ·±å±‚å‡ºç°æ¢¯åº¦å†²çªï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šUni-Xçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ¨¡æ€ç‰¹å®šå±‚çš„å…·ä½“ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œå·ç§¯å±‚ç”¨äºè§†è§‰æ¨¡æ€ï¼ŒåµŒå…¥å±‚ç”¨äºæ–‡æœ¬æ¨¡æ€ï¼‰ï¼›2) å…±äº«ä¸­é—´å±‚çš„Transformerå—æ•°é‡å’Œå‚æ•°è®¾ç½®ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œå¯èƒ½åŒ…æ‹¬æ¨¡æ€ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œè·¨æ¨¡æ€çš„å¯¹é½æŸå¤±å‡½æ•°ï¼›4) æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–ç­–ç•¥ï¼Œä¾‹å¦‚å­¦ä¹ ç‡è°ƒåº¦å’Œæ­£åˆ™åŒ–æ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Uni-Xåœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹ï¼Œè®­ç»ƒæ•ˆç‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ã€‚å½“æ‰©å±•åˆ°3Bå‚æ•°å¹¶ä½¿ç”¨æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†æ—¶ï¼ŒUni-Xçš„æ€§èƒ½ä¸7Bå‚æ•°çš„è‡ªå›å½’UMMæ¨¡å‹ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­GenEvalå¾—åˆ†è¾¾åˆ°82ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬å’Œè§†è§‰ç†è§£ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Uni-Xæ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šè·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€å›¾åƒ/è§†é¢‘æè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿç­‰ã€‚è¯¥æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºèµ„æºå—é™çš„è®¾å¤‡ï¼Œå¹¶ä¸ºæœªæ¥çš„ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡æä¾›äº†ä¸€ä¸ªæœ‰åŠ›çš„åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X

