---
layout: default
title: LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model
---

# LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25304" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.25304v1</a>
  <a href="https://arxiv.org/pdf/2509.25304.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25304v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25304v1', 'LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-29

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**LUMAÔºöÂü∫‰∫éÂèåË∑ØÈîöÂÆöÁöÑ‰ΩéÁª¥Áªü‰∏ÄËøêÂä®ÂØπÈΩêÊñáÊú¨Âà∞Âä®‰ΩúÊâ©Êï£Ê®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàê` `Êâ©Êï£Ê®°Âûã` `ËøêÂä®ÂØπÈΩê` `ÂØπÊØîÂ≠¶‰π†` `Êó∂ÂüüÈîöÂÆö` `È¢ëÂüüÈîöÂÆö` `ËØ≠‰πâÁõëÁù£` `Âä®‰ΩúÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ïÂ≠òÂú®ËØ≠‰πâ‰∏çÂØπÈΩêÂíåËøêÂä®‰º™ÂΩ±ÈóÆÈ¢òÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØÁΩëÁªúÊ∑±Â±ÇÊ¢ØÂ∫¶Ë°∞ÂáèÂØºËá¥È´òÂ±ÇÁâπÂæÅÂ≠¶‰π†‰∏çË∂≥„ÄÇ
2. LUMAÈÄöËøáÂèåË∑ØÈîöÂÆöÂ¢ûÂº∫ËØ≠‰πâÂØπÈΩêÔºå‰∏ÄË∑ØÂà©Áî®MoCLIPÂú®Êó∂ÂüüËøõË°åËØ≠‰πâÁõëÁù£ÔºåÂè¶‰∏ÄË∑ØÂà©Áî®‰ΩéÈ¢ëDCTÂàÜÈáèÂú®È¢ëÂüüÊèê‰æõ‰∫íË°•ÂØπÈΩê‰ø°Âè∑„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLUMAÂú®HumanML3DÂíåKIT-MLÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåFIDÂàÜÊï∞ÂàÜÂà´‰∏∫0.035Âíå0.123ÔºåÂπ∂‰∏îÊî∂ÊïõÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü1.4ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫LUMAÔºà‰ΩéÁª¥Áªü‰∏ÄËøêÂä®ÂØπÈΩêÔºâÔºå‰∏ÄÁßçÊñáÊú¨Âà∞Âä®‰ΩúÁöÑÊâ©Êï£Ê®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂü∫‰∫éU-NetÊû∂ÊûÑÁöÑÊ®°ÂûãÂú®ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàê‰ªªÂä°‰∏≠Â≠òÂú®ÁöÑËØ≠‰πâ‰∏çÂØπÈΩêÂíåËøêÂä®‰º™ÂΩ±ÈóÆÈ¢ò„ÄÇÈÄöËøáÂàÜÊûêÂèëÁé∞ÔºåÁΩëÁªúÊ∑±Â±ÇÁöÑÊ¢ØÂ∫¶Ë°∞ÂáèÊòØÂØºËá¥È´òÂ±ÇÁâπÂæÅÂ≠¶‰π†‰∏çË∂≥ÁöÑÂÖ≥ÈîÆÁì∂È¢à„ÄÇLUMAÈÄöËøáÂºïÂÖ•ÂèåË∑ØÈîöÂÆöÊù•Â¢ûÂº∫ËØ≠‰πâÂØπÈΩê„ÄÇÁ¨¨‰∏ÄÊù°Ë∑ØÂæÑÈááÁî®ËΩªÈáèÁ∫ßÁöÑMoCLIPÊ®°ÂûãÔºåÈÄöËøáÂØπÊØîÂ≠¶‰π†ËøõË°åËÆ≠ÁªÉÔºåÊó†ÈúÄÂ§ñÈÉ®Êï∞ÊçÆÔºå‰ªéËÄåÂú®Êó∂Èó¥ÂüüÊèê‰æõËØ≠‰πâÁõëÁù£„ÄÇÁ¨¨‰∫åÊù°Ë∑ØÂæÑÂºïÂÖ•È¢ëÂüü‰∏≠ÁöÑ‰∫íË°•ÂØπÈΩê‰ø°Âè∑ÔºåËØ•‰ø°Âè∑‰ªé‰ª•ÂÖ∂‰∏∞ÂØåÁöÑËØ≠‰πâÂÜÖÂÆπËÄåÈóªÂêçÁöÑ‰ΩéÈ¢ëDCTÂàÜÈáè‰∏≠ÊèêÂèñ„ÄÇËøô‰∏§‰∏™ÈîöÈÄöËøáÊó∂Èó¥Ë∞ÉÂà∂Êú∫Âà∂Ëá™ÈÄÇÂ∫îÂú∞ËûçÂêàÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®Êï¥‰∏™ÂéªÂô™ËøáÁ®ã‰∏≠‰ªéÁ≤óÁï•ÂØπÈΩêÈÄêÊ≠•ËøáÊ∏°Âà∞Á≤æÁªÜÁöÑËØ≠‰πâÁªÜÂåñ„ÄÇÂú®HumanML3DÂíåKIT-ML‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLUMAÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåFIDÂàÜÊï∞ÂàÜÂà´‰∏∫0.035Âíå0.123„ÄÇÊ≠§Â§ñÔºå‰∏éÂü∫Á∫øÁõ∏ÊØîÔºåLUMAÂ∞ÜÊî∂ÊïõÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü1.4ÂÄçÔºå‰ΩøÂÖ∂Êàê‰∏∫È´ò‰øùÁúüÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÁöÑÈ´òÊïà‰∏îÂèØÊâ©Â±ïÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éU-NetÊû∂ÊûÑÁöÑÊ®°ÂûãÔºåÂú®ÁîüÊàêÂä®‰ΩúÊó∂Â≠òÂú®ËØ≠‰πâ‰∏çÂØπÈΩêÂíåËøêÂä®‰º™ÂΩ±ÁöÑÈóÆÈ¢ò„ÄÇËøô‰∫õÈóÆÈ¢òÊ∫ê‰∫éÁΩëÁªúÊ∑±Â±ÇÊ¢ØÂ∫¶Ë°∞ÂáèÔºåÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÂÖÖÂàÜÂ≠¶‰π†È´òÂ±ÇËØ≠‰πâÁâπÂæÅÔºå‰ªéËÄåÂΩ±ÂìçÁîüÊàêÂä®‰ΩúÁöÑË¥®ÈáèÂíåÁúüÂÆûÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLUMAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•ÂèåË∑ØÈîöÂÆöÊú∫Âà∂Ôºå‰ªéÊó∂ÂüüÂíåÈ¢ëÂüü‰∏§‰∏™ËßíÂ∫¶ÂØπËøêÂä®Êï∞ÊçÆËøõË°åËØ≠‰πâÂØπÈΩêÔºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÂØπÈ´òÂ±ÇËØ≠‰πâÁâπÂæÅÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇÊó∂ÂüüÈîöÂÆöÂà©Áî®ÂØπÊØîÂ≠¶‰π†ÂæóÂà∞ÁöÑMoCLIPÊ®°ÂûãÊèê‰æõËØ≠‰πâÁõëÁù£ÔºåÈ¢ëÂüüÈîöÂÆöÂàôÂà©Áî®‰ΩéÈ¢ëDCTÂàÜÈáèÊèêÂèñËØ≠‰πâ‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLUMAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÊòØ‰∏Ä‰∏™Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊµÅÁ®ãÔºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ÊñáÊú¨ÁºñÁ†ÅÂô®ÔºöÂ∞ÜËæìÂÖ•ÁöÑÊñáÊú¨ÊèèËø∞ËΩ¨Êç¢‰∏∫ÊñáÊú¨ÁâπÂæÅÂêëÈáè„ÄÇ2) ËøêÂä®Êâ©Êï£Ê®°ÂûãÔºöÂü∫‰∫éU-NetÊû∂ÊûÑÔºåË¥üË¥£‰ªéÂô™Â£∞‰∏≠ÈÄêÊ≠•ÁîüÊàêËøêÂä®Êï∞ÊçÆ„ÄÇ3) Êó∂ÂüüÈîöÂÆöÊ®°ÂùóÔºöÂà©Áî®MoCLIPÊ®°ÂûãÔºåÂú®Êó∂Èó¥ÂüüÂØπËøêÂä®Êï∞ÊçÆËøõË°åËØ≠‰πâÁõëÁù£„ÄÇ4) È¢ëÂüüÈîöÂÆöÊ®°ÂùóÔºöÊèêÂèñ‰ΩéÈ¢ëDCTÂàÜÈáèÔºåÂú®È¢ëÁéáÂüüÊèê‰æõ‰∫íË°•ÁöÑÂØπÈΩê‰ø°Âè∑„ÄÇ5) Êó∂Èó¥Ë∞ÉÂà∂Ê®°ÂùóÔºöËá™ÈÄÇÂ∫îÂú∞ËûçÂêàÊó∂ÂüüÂíåÈ¢ëÂüüÁöÑÈîöÂÆö‰ø°Âè∑ÔºåÂÆûÁé∞‰ªéÁ≤óÁï•ÂØπÈΩêÂà∞Á≤æÁªÜËØ≠‰πâÁªÜÂåñÁöÑËøáÊ∏°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLUMAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂèåË∑ØÈîöÂÆöÊú∫Âà∂ÔºåÂÆÉÂêåÊó∂Âà©Áî®Êó∂ÂüüÂíåÈ¢ëÂüüÁöÑ‰ø°ÊÅØÊù•Â¢ûÂº∫ËØ≠‰πâÂØπÈΩê„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåLUMAÊó†ÈúÄ‰æùËµñÂ§ñÈÉ®Êï∞ÊçÆËøõË°åÂØπÊØîÂ≠¶‰π†ÔºåÂπ∂‰∏îËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞ËûçÂêà‰∏çÂêåÂüüÁöÑÂØπÈΩê‰ø°Âè∑Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑËØ≠‰πâÊéßÂà∂ÂíåÊõ¥È´òË¥®ÈáèÁöÑÂä®‰ΩúÁîüÊàê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLUMAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËΩªÈáèÁ∫ßÁöÑMoCLIPÊ®°ÂûãÔºåÈÄöËøáÂØπÊØîÂ≠¶‰π†Âú®Êó∂Èó¥ÂüüÊèê‰æõËØ≠‰πâÁõëÁù£„ÄÇ2) ‰ΩéÈ¢ëDCTÂàÜÈáèÁöÑÊèêÂèñÔºåÂà©Áî®ÂÖ∂‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØÂú®È¢ëÁéáÂüüÊèê‰æõ‰∫íË°•ÁöÑÂØπÈΩê‰ø°Âè∑„ÄÇ3) Êó∂Èó¥Ë∞ÉÂà∂Êú∫Âà∂ÔºåËá™ÈÄÇÂ∫îÂú∞ËûçÂêàÊó∂ÂüüÂíåÈ¢ëÂüüÁöÑÈîöÂÆö‰ø°Âè∑ÔºåÂÆûÁé∞‰ªéÁ≤óÁï•ÂØπÈΩêÂà∞Á≤æÁªÜËØ≠‰πâÁªÜÂåñÁöÑËøáÊ∏°„ÄÇ4) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÁªºÂêàËÄÉËôë‰∫ÜÊâ©Êï£Ê®°ÂûãÁöÑÈáçÂª∫ÊçüÂ§±„ÄÅÂØπÊØîÂ≠¶‰π†ÊçüÂ§±‰ª•ÂèäÂØπÈΩêÊçüÂ§±Ôºå‰ªéËÄå‰ºòÂåñÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LUMAÂú®HumanML3DÂíåKIT-MLÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåFIDÂàÜÊï∞ÂàÜÂà´ËææÂà∞‰∫Ü0.035Âíå0.123ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑSOTAÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåLUMAËøòÊòæËëóÊèêÈ´ò‰∫ÜÊî∂ÊïõÈÄüÂ∫¶Ôºå‰∏éÂü∫Á∫øÁõ∏ÊØîÔºåÊî∂ÊïõÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü1.4ÂÄçÔºåËøôË°®ÊòéLUMAÂÖ∑ÊúâÊõ¥È´òÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LUMAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂåÖÊã¨ËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÂä®ÁîªÂà∂‰Ωú„ÄÅ‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Ê†πÊçÆÊñáÊú¨ÊèèËø∞Ëá™Âä®ÁîüÊàêÈÄºÁúüËá™ÁÑ∂ÁöÑÂä®‰ΩúÔºå‰ªéËÄåÈôç‰ΩéÂä®‰ΩúÁîüÊàêÊàêÊú¨ÔºåÊèêÈ´òÂàõ‰ΩúÊïàÁéá„ÄÇÊ≠§Â§ñÔºåLUMAËøòÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÊõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÁêÜËß£‰∫∫Á±ªÁöÑÊåá‰ª§Âπ∂ÊâßË°åÁõ∏Â∫îÁöÑÂä®‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.

