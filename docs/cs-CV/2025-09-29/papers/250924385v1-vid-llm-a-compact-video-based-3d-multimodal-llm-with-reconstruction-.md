---
layout: default
title: Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy
---

# Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24385" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24385v1</a>
  <a href="https://arxiv.org/pdf/2509.24385.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24385v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24385v1', 'Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haijier Chen, Bo Xu, Shoujian Zhang, Haoze Liu, Jiaxuan Lin, Jingrong Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Vid-LLMï¼šæå‡ºä¸€ç§åŸºäºè§†é¢‘çš„ç´§å‡‘å‹3Då¤šæ¨¡æ€LLMï¼Œå®ç°é‡å»º-æ¨ç†ååŒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†é¢‘è¾“å…¥` `å‡ ä½•é‡å»º` `è·¨ä»»åŠ¡é€‚é…å™¨` `è’¸é¦è®­ç»ƒ` `åº¦é‡æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3D-MLLMä¾èµ–3Dæ•°æ®è¾“å…¥ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥å®é™…éƒ¨ç½²ã€‚
2. Vid-LLMç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ï¼Œæ— éœ€å¤–éƒ¨3Dæ•°æ®ï¼Œå¹¶åˆ©ç”¨å‡ ä½•å…ˆéªŒæå‡åœºæ™¯æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒè¯æ˜Vid-LLMåœ¨3Dé—®ç­”ã€3Då¯†é›†å­—å¹•å’Œ3Dè§†è§‰å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡å“è¶Šçš„å¤šä»»åŠ¡èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)çš„æœ€æ–°å‘å±•æ˜¾è‘—æé«˜äº†2Dé¢†åŸŸçš„è§†è§‰-è¯­è¨€(VL)æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›èƒ½åŠ›æ‰©å±•åˆ°3Dåœºæ™¯ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„3Då¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(3D-MLLM)é€šå¸¸ä¾èµ–äº3Dæ•°æ®è¾“å…¥ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Vid-LLMï¼Œä¸€ä¸ªåŸºäºè§†é¢‘çš„3D-MLLMï¼Œå®ƒç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ï¼Œè€Œä¸éœ€è¦å¤–éƒ¨3Dæ•°æ®ï¼Œä½¿å…¶é€‚ç”¨äºå®é™…éƒ¨ç½²ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œå‡ ä½•å…ˆéªŒè¢«ç›´æ¥ç”¨äºæé«˜åœºæ™¯æ„ŸçŸ¥æ€§èƒ½ã€‚ä¸ºäº†å°†å‡ ä½•çº¿ç´¢ç´§å‡‘åœ°é›†æˆåˆ°MLLMä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè·¨ä»»åŠ¡é€‚é…å™¨(CTA)æ¨¡å—ï¼Œä»¥å°†3Då‡ ä½•å…ˆéªŒä¸è§†è§‰-è¯­è¨€è¡¨ç¤ºå¯¹é½ã€‚ä¸ºäº†ç¡®ä¿å‡ ä½•ä¸€è‡´æ€§å’Œå®Œæ•´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåº¦é‡æ·±åº¦æ¨¡å‹ï¼Œä»é‡å»ºè¾“å‡ºä¸­æ¢å¤çœŸå®å°ºåº¦çš„å‡ ä½•ä¿¡æ¯ã€‚æœ€åï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè’¸é¦ä¼˜åŒ–ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œå®ç°å¿«é€Ÿæ”¶æ•›å¹¶ç¨³å®šè®­ç»ƒã€‚åœ¨å„ç§åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨3Dé—®ç­”ã€3Då¯†é›†å­—å¹•å’Œ3Dè§†è§‰å®šä½ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„å¤šä»»åŠ¡èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Då¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¾èµ–äº3Dæ•°æ®è¾“å…¥ï¼Œä¾‹å¦‚ç‚¹äº‘æˆ–ä½“ç´ ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è·å–é«˜è´¨é‡çš„3Dæ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œä¸”çœŸå®åœºæ™¯ä¸­å¾€å¾€ç¼ºä¹å®Œç¾çš„3Dä¿¡æ¯ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨æ›´æ˜“è·å–çš„è§†é¢‘æ•°æ®è¿›è¡Œæœ‰æ•ˆçš„3Dåœºæ™¯ç†è§£æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVid-LLMçš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥ä»è§†é¢‘è¾“å…¥ä¸­å­¦ä¹ 3Dåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸è§†è§‰-è¯­è¨€è¡¨ç¤ºå¯¹é½ï¼Œä»è€Œå®ç°åŸºäºè§†é¢‘çš„3Dåœºæ™¯ç†è§£ã€‚é€šè¿‡å¼•å…¥å‡ ä½•å…ˆéªŒå’Œåº¦é‡æ·±åº¦æ¨¡å‹ï¼Œå¢å¼ºæ¨¡å‹å¯¹3Dåœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶ç¡®ä¿å‡ ä½•ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVid-LLMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘ç¼–ç å™¨ï¼šç”¨äºæå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ã€‚2) å‡ ä½•é‡å»ºæ¨¡å—ï¼šä»è§†é¢‘ä¸­é‡å»º3Dåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œä¾‹å¦‚æ·±åº¦å›¾ã€‚3) è·¨ä»»åŠ¡é€‚é…å™¨(CTA)ï¼šå°†3Då‡ ä½•å…ˆéªŒä¸è§†è§‰-è¯­è¨€è¡¨ç¤ºå¯¹é½ã€‚4) åº¦é‡æ·±åº¦æ¨¡å‹ï¼šä»é‡å»ºè¾“å‡ºä¸­æ¢å¤çœŸå®å°ºåº¦çš„å‡ ä½•ä¿¡æ¯ã€‚5) å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ï¼šç”¨äºè¿›è¡Œ3Dé—®ç­”ã€3Då¯†é›†å­—å¹•å’Œ3Dè§†è§‰å®šä½ç­‰ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVid-LLMçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç›´æ¥ä»è§†é¢‘è¾“å…¥ä¸­å­¦ä¹ 3Dåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œæ— éœ€å¤–éƒ¨3Dæ•°æ®ã€‚2) æå‡ºè·¨ä»»åŠ¡é€‚é…å™¨(CTA)æ¨¡å—ï¼Œæœ‰æ•ˆåœ°å°†3Då‡ ä½•å…ˆéªŒä¸è§†è§‰-è¯­è¨€è¡¨ç¤ºå¯¹é½ã€‚3) å¼•å…¥åº¦é‡æ·±åº¦æ¨¡å‹ï¼Œæ¢å¤çœŸå®å°ºåº¦çš„å‡ ä½•ä¿¡æ¯ï¼Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§ã€‚4) é‡‡ç”¨ä¸¤é˜¶æ®µè’¸é¦ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°å¿«é€Ÿæ”¶æ•›å’Œç¨³å®šè®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šCTAæ¨¡å—çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒè´Ÿè´£å°†å‡ ä½•ç‰¹å¾ï¼ˆä¾‹å¦‚æ·±åº¦å›¾ï¼‰ä¸è§†è§‰ç‰¹å¾è¿›è¡Œèåˆï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ã€‚åº¦é‡æ·±åº¦æ¨¡å‹é€šè¿‡ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå­¦ä¹ ä»é‡å»ºçš„æ·±åº¦å›¾ä¸­æ¢å¤çœŸå®å°ºåº¦çš„æ·±åº¦ä¿¡æ¯ã€‚ä¸¤é˜¶æ®µè’¸é¦ä¼˜åŒ–ç­–ç•¥åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§è§„æ¨¡çš„2Dè§†è§‰-è¯­è¨€æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼›ç„¶åï¼Œä½¿ç”¨3Dç›¸å…³çš„ä»»åŠ¡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¹¶åˆ©ç”¨è’¸é¦æŠ€æœ¯åŠ é€Ÿæ”¶æ•›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVid-LLMåœ¨3Dé—®ç­”ã€3Då¯†é›†å­—å¹•å’Œ3Dè§†è§‰å®šä½ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨3Dé—®ç­”ä»»åŠ¡ä¸­ï¼ŒVid-LLMçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†XX%ã€‚æ­¤å¤–ï¼Œä¸¤é˜¶æ®µè’¸é¦ä¼˜åŒ–ç­–ç•¥æœ‰æ•ˆåœ°åŠ é€Ÿäº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Vid-LLMåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„å¯¼èˆªå’Œäº¤äº’ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼ŒVid-LLMå¯ä»¥æé«˜è½¦è¾†å¯¹3Dåœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨AR/VRé¢†åŸŸï¼ŒVid-LLMå¯ä»¥å®ç°æ›´é€¼çœŸçš„3Dåœºæ™¯é‡å»ºå’Œäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks, demonstrating the superior multi-task capabilities.

