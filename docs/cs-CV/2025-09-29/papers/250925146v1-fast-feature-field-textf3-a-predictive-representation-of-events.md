---
layout: default
title: Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events
---

# Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25146" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25146v1</a>
  <a href="https://arxiv.org/pdf/2509.25146.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25146v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25146v1', 'Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Richeek Das, Kostas Daniilidis, Pratik Chaudhari

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 39 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¿«é€Ÿç‰¹å¾åœºï¼ˆFÂ³ï¼‰ï¼Œç”¨äºäº‹ä»¶ç›¸æœºæ•°æ®çš„é¢„æµ‹æ€§è¡¨å¾å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `ç‰¹å¾åœº` `é¢„æµ‹å­¦ä¹ ` `å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç ` `æ·±åº¦é›†` `å…‰æµä¼°è®¡` `è¯­ä¹‰åˆ†å‰²` `å•ç›®æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿäº‹ä»¶ç›¸æœºæ•°æ®å¤„ç†æ–¹æ³•éš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸ç²¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯å’Œé«˜åˆ†è¾¨ç‡ä¸‹ã€‚
2. FÂ³é€šè¿‡é¢„æµ‹æœªæ¥äº‹ä»¶æ¥å­¦ä¹ åœºæ™¯è¡¨å¾ï¼Œåˆ©ç”¨äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§ï¼Œå®ç°é«˜æ•ˆä¸”é²æ£’çš„ç‰¹å¾æå–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFÂ³åœ¨å…‰æµä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼Œä¸”é€Ÿåº¦å¿«ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¿«é€Ÿç‰¹å¾åœºï¼ˆFÂ³ï¼‰çš„äº‹ä»¶ç›¸æœºæ•°æ®è¡¨å¾æ„å»ºæ–¹æ³•ï¼Œå¹¶ç»™å‡ºäº†ç›¸åº”çš„æ•°å­¦è®ºè¯å’Œç®—æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹æœªæ¥äº‹ä»¶æ¥å­¦ä¹ è¡¨å¾ï¼Œå¹¶è¯æ˜å…¶èƒ½å¤Ÿä¿ç•™åœºæ™¯ç»“æ„å’Œè¿åŠ¨ä¿¡æ¯ã€‚FÂ³åˆ©ç”¨äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§ï¼Œå¯¹å™ªå£°å’Œäº‹ä»¶é€Ÿç‡çš„å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚å®ƒå¯ä»¥é€šè¿‡å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç å’Œæ·±åº¦é›†ä¸­çš„æ€æƒ³é«˜æ•ˆè®¡ç®—ï¼Œåœ¨é«˜æ¸…åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°120 Hzï¼Œåœ¨VGAåˆ†è¾¨ç‡ä¸‹è¾¾åˆ°440 Hzã€‚FÂ³å°†è¿ç»­æ—¶ç©ºä½“ç§¯å†…çš„äº‹ä»¶è¡¨ç¤ºä¸ºå¤šé€šé“å›¾åƒï¼Œä»è€Œæ”¯æŒå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨æ¥è‡ªä¸‰ä¸ªæœºå™¨äººå¹³å°ï¼ˆæ±½è½¦ã€å››è¶³æœºå™¨äººå’Œé£è¡Œå¹³å°ï¼‰çš„æ•°æ®ä¸Šï¼Œè·¨è¶Šä¸åŒçš„å…‰ç…§æ¡ä»¶ï¼ˆç™½å¤©ã€å¤œæ™šï¼‰ã€ç¯å¢ƒï¼ˆå®¤å†…ã€å®¤å¤–ã€åŸå¸‚ä»¥åŠè¶Šé‡ï¼‰å’ŒåŠ¨æ€è§†è§‰ä¼ æ„Ÿå™¨ï¼ˆåˆ†è¾¨ç‡å’Œäº‹ä»¶é€Ÿç‡ï¼‰ï¼Œæˆ‘ä»¬åœ¨å…‰æµä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œå•ç›®åº¦é‡æ·±åº¦ä¼°è®¡æ–¹é¢è·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°èƒ½å¤Ÿåœ¨é«˜æ¸…åˆ†è¾¨ç‡ä¸‹ä»¥25-75 Hzçš„é€Ÿåº¦é¢„æµ‹è¿™äº›ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šäº‹ä»¶ç›¸æœºäº§ç”Ÿçš„æ•°æ®å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œä½å»¶è¿Ÿçš„ç‰¹ç‚¹ï¼Œä½†å…¶å¼‚æ­¥å’Œç¨€ç–çš„ç‰¹æ€§ç»™ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ç®—æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å’Œå¤æ‚åœºæ™¯æ—¶ï¼Œå¾€å¾€é¢ä¸´è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿé¢„æµ‹æœªæ¥äº‹ä»¶çš„ç‰¹å¾åœºï¼Œè¯¥ç‰¹å¾åœºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰åœºæ™¯çš„ç»“æ„å’Œè¿åŠ¨ä¿¡æ¯ã€‚é€šè¿‡é¢„æµ‹æœªæ¥äº‹ä»¶ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¯¹å™ªå£°å’Œäº‹ä»¶é€Ÿç‡å˜åŒ–å…·æœ‰é²æ£’æ€§çš„è¡¨å¾ã€‚è¿™ç§é¢„æµ‹æ€§çš„å­¦ä¹ æ–¹å¼èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨äº‹ä»¶æ•°æ®çš„æ—¶åºä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFÂ³çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬äº‹ä»¶æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾åœºæ„å»ºå’Œä¸‹æ¸¸ä»»åŠ¡é¢„æµ‹ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œå¯¹åŸå§‹äº‹ä»¶æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚æ»¤æ³¢å’Œå»å™ªã€‚ç„¶åï¼Œåˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç å’Œæ·±åº¦é›†æ„å»ºç‰¹å¾åœºï¼Œè¯¥ç‰¹å¾åœºå°†æ—¶ç©ºä½“ç§¯å†…çš„äº‹ä»¶è¡¨ç¤ºä¸ºå¤šé€šé“å›¾åƒã€‚æœ€åï¼Œå°†ç‰¹å¾åœºè¾“å…¥åˆ°ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹ä¸­ï¼Œä¾‹å¦‚å…‰æµä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFÂ³çš„å…³é”®åˆ›æ–°åœ¨äºå…¶é¢„æµ‹æ€§çš„è¡¨å¾å­¦ä¹ æ–¹æ³•å’Œé«˜æ•ˆçš„ç‰¹å¾åœºæ„å»ºæ–¹å¼ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºäº‹ä»¶çš„ç‰¹å¾æå–æ–¹æ³•ä¸åŒï¼ŒFÂ³é€šè¿‡é¢„æµ‹æœªæ¥äº‹ä»¶æ¥å­¦ä¹ è¡¨å¾ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰åœºæ™¯çš„åŠ¨æ€ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒFÂ³åˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç å’Œæ·±åº¦é›†ï¼Œå®ç°äº†é«˜æ•ˆçš„ç‰¹å¾åœºæ„å»ºï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡ä¸‹å®ç°å®æ—¶å¤„ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šFÂ³çš„å…³é”®è®¾è®¡åŒ…æ‹¬å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç ã€æ·±åº¦é›†å’ŒæŸå¤±å‡½æ•°ã€‚å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç ç”¨äºå°†äº‹ä»¶æ•°æ®æ˜ å°„åˆ°ç‰¹å¾å‘é‡ï¼Œæ·±åº¦é›†ç”¨äºèšåˆæ¥è‡ªä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾å‘é‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬äº‹ä»¶é¢„æµ‹æŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡æŸå¤±ï¼Œç”¨äºä¼˜åŒ–ç‰¹å¾åœºçš„å‚æ•°ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ ¹æ®ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FÂ³åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨å…‰æµä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼ŒFÂ³åœ¨é«˜æ¸…åˆ†è¾¨ç‡ä¸‹èƒ½å¤Ÿè¾¾åˆ°120 Hzçš„å¤„ç†é€Ÿåº¦ï¼Œåœ¨VGAåˆ†è¾¨ç‡ä¸‹èƒ½å¤Ÿè¾¾åˆ°440 Hzçš„å¤„ç†é€Ÿåº¦ï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFÂ³å¯¹å™ªå£°å’Œäº‹ä»¶é€Ÿç‡å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„å…‰ç…§æ¡ä»¶å’Œç¯å¢ƒä¸‹ç¨³å®šè¿è¡Œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FÂ³å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„äº‹ä»¶ç›¸æœºæ•°æ®å¤„ç†èƒ½åŠ›å¯ä»¥æ”¯æŒå®æ—¶æ„ŸçŸ¥å’Œå†³ç­–ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ç”¨äºéšœç¢ç‰©æ£€æµ‹å’Œè·Ÿè¸ªï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ç”¨äºç¯å¢ƒå»ºæ¨¡å’Œè·¯å¾„è§„åˆ’ã€‚æ­¤å¤–ï¼ŒFÂ³è¿˜å¯ä»¥åº”ç”¨äºè¿åŠ¨æ•æ‰ã€æ‰‹åŠ¿è¯†åˆ«å’Œç”Ÿç‰©åŒ»å­¦æˆåƒç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper develops a mathematical argument and algorithms for building representations of data from event-based cameras, that we call Fast Feature Field ($\text{F}^3$). We learn this representation by predicting future events from past events and show that it preserves scene structure and motion information. $\text{F}^3$ exploits the sparsity of event data and is robust to noise and variations in event rates. It can be computed efficiently using ideas from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and 440 Hz at VGA resolutions. $\text{F}^3$ represents events within a contiguous spatiotemporal volume as a multi-channel image, enabling a range of downstream tasks. We obtain state-of-the-art performance on optical flow estimation, semantic segmentation, and monocular metric depth estimation, on data from three robotic platforms (a car, a quadruped robot and a flying platform), across different lighting conditions (daytime, nighttime), environments (indoors, outdoors, urban, as well as off-road) and dynamic vision sensors (resolutions and event rates). Our implementations can predict these tasks at 25-75 Hz at HD resolution.

