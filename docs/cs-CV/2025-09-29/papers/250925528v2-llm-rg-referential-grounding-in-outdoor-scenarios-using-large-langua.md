---
layout: default
title: LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models
---

# LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25528" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25528v2</a>
  <a href="https://arxiv.org/pdf/2509.25528.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25528v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25528v2', 'LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pranav Saxena, Avigyan Bhattacharya, Ji Zhang, Wenshan Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Human-aware Embodied AI Workshop @ IROS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LLM-RGï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°æˆ·å¤–åœºæ™¯ä¸‹çš„æŒ‡ç§°å¯¹è±¡å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ç§°å¯¹è±¡å®šä½` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `æˆ·å¤–åœºæ™¯` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æˆ·å¤–åœºæ™¯æŒ‡ç§°å¯¹è±¡å®šä½é¢ä¸´åœºæ™¯å¤æ‚ã€å¯¹è±¡ç›¸ä¼¼ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£æè‡ªç„¶è¯­è¨€æŒ‡ç§°ã€‚
2. LLM-RGç»“åˆVLMæå–è§†è§‰ç‰¹å¾ï¼Œåˆ©ç”¨LLMè¿›è¡Œç¬¦å·æ¨ç†ï¼Œå°†è§†è§‰ä¿¡æ¯å’Œç©ºé—´ä¿¡æ¯èå…¥è‡ªç„¶è¯­è¨€æç¤ºã€‚
3. åœ¨Talk2Caræ•°æ®é›†ä¸Šï¼ŒLLM-RGæ˜¾è‘—ä¼˜äºLLMå’ŒVLMåŸºçº¿ï¼Œ3Dç©ºé—´çº¿ç´¢çš„åŠ å…¥è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”±äºåœºæ™¯å˜åŒ–å¤§ã€è§†è§‰ç›¸ä¼¼å¯¹è±¡å¤šä»¥åŠåŠ¨æ€å…ƒç´ å¤æ‚åŒ–äº†è‡ªç„¶è¯­è¨€æŒ‡ç§°çš„è§£æï¼ˆä¾‹å¦‚ï¼Œâ€œå³è¾¹çš„é»‘è‰²æ±½è½¦â€ï¼‰ï¼Œæˆ·å¤–é©¾é©¶åœºæ™¯ä¸­çš„æŒ‡ç§°å¯¹è±¡å®šä½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†LLM-RGï¼Œä¸€ç§æ··åˆç®¡é“ï¼Œå®ƒç»“åˆäº†ç°æˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œç»†ç²’åº¦å±æ€§æå–ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¬¦å·æ¨ç†ã€‚LLM-RGå¤„ç†å›¾åƒå’Œè‡ªç”±å½¢å¼çš„æŒ‡ç§°è¡¨è¾¾å¼ï¼Œé¦–å…ˆä½¿ç”¨LLMæå–ç›¸å…³çš„å¯¹è±¡ç±»å‹å’Œå±æ€§ï¼Œç„¶åæ£€æµ‹å€™é€‰åŒºåŸŸï¼Œä½¿ç”¨VLMç”Ÿæˆä¸°å¯Œçš„è§†è§‰æè¿°ç¬¦ï¼Œå¹¶å°†è¿™äº›æè¿°ç¬¦ä¸ç©ºé—´å…ƒæ•°æ®ç»„åˆæˆè‡ªç„¶è¯­è¨€æç¤ºï¼Œè¾“å…¥åˆ°LLMä¸­è¿›è¡Œæ€ç»´é“¾æ¨ç†ï¼Œä»¥è¯†åˆ«æŒ‡ç§°å¯¹è±¡çš„è¾¹ç•Œæ¡†ã€‚åœ¨Talk2CaråŸºå‡†æµ‹è¯•ä¸­ï¼ŒLLM-RGç›¸å¯¹äºåŸºäºLLMå’ŒVLMçš„åŸºçº¿éƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ·»åŠ 3Dç©ºé—´çº¿ç´¢å¯ä»¥è¿›ä¸€æ­¥æ”¹å–„å®šä½æ•ˆæœã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†VLMå’ŒLLMçš„äº’è¡¥ä¼˜åŠ¿ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼åº”ç”¨äºé²æ£’çš„æˆ·å¤–æŒ‡ç§°å¯¹è±¡å®šä½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æˆ·å¤–é©¾é©¶åœºæ™¯ä¸­ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç²¾ç¡®å®šä½ç‰¹å®šå¯¹è±¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚åœºæ™¯ã€è§†è§‰ç›¸ä¼¼å¯¹è±¡ä»¥åŠåŠ¨æ€ç¯å¢ƒå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æŒ‡ç§°å¯¹è±¡å®šä½çš„å‡†ç¡®ç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŠ¿ã€‚VLMæ“…é•¿æå–ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ï¼Œè€ŒLLMæ“…é•¿è¿›è¡Œç¬¦å·æ¨ç†å’Œè‡ªç„¶è¯­è¨€ç†è§£ã€‚é€šè¿‡å°†ä¸¤è€…ç»“åˆï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è§£æè‡ªç„¶è¯­è¨€æŒ‡ç§°ï¼Œå¹¶å°†å…¶ä¸è§†è§‰ä¿¡æ¯å…³è”èµ·æ¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLM-RGçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä½¿ç”¨LLMæå–å¯¹è±¡ç±»å‹å’Œå±æ€§ï¼›2) æ£€æµ‹å€™é€‰åŒºåŸŸï¼›3) ä½¿ç”¨VLMç”Ÿæˆè§†è§‰æè¿°ç¬¦ï¼›4) å°†è§†è§‰æè¿°ç¬¦å’Œç©ºé—´å…ƒæ•°æ®ç»„åˆæˆè‡ªç„¶è¯­è¨€æç¤ºï¼›5) ä½¿ç”¨LLMè¿›è¡Œæ€ç»´é“¾æ¨ç†ï¼Œæœ€ç»ˆç¡®å®šæŒ‡ç§°å¯¹è±¡çš„è¾¹ç•Œæ¡†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†VLMå’ŒLLMä»¥ä¸€ç§äº’è¡¥çš„æ–¹å¼ç»“åˆèµ·æ¥ï¼Œåˆ©ç”¨VLMæå–è§†è§‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œæ¨ç†å’Œå†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜è€ƒè™‘äº†3Dç©ºé—´ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æå‡äº†å®šä½çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„VLMå’ŒLLMï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼›2) å°†è§†è§‰æè¿°ç¬¦å’Œç©ºé—´å…ƒæ•°æ®ä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼è¾“å…¥åˆ°LLMä¸­ï¼Œæ–¹ä¾¿LLMè¿›è¡Œæ¨ç†ï¼›3) ä½¿ç”¨æ€ç»´é“¾æ¨ç†ï¼Œé€æ­¥ç¼©å°æœç´¢èŒƒå›´ï¼Œæœ€ç»ˆç¡®å®šæŒ‡ç§°å¯¹è±¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LLM-RGåœ¨Talk2CaråŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†åŸºäºLLMå’ŒVLMçš„åŸºçº¿æ–¹æ³•ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ·»åŠ 3Dç©ºé—´çº¿ç´¢å¯ä»¥è¿›ä¸€æ­¥æé«˜å®šä½ç²¾åº¦ã€‚è¿™äº›ç»“æœéªŒè¯äº†VLMå’ŒLLMç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠ3Dç©ºé—´ä¿¡æ¯çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯ç†è§£é©¾é©¶å‘˜æˆ–ä¹˜å®¢çš„æŒ‡ä»¤ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è€Œæ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚åœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å¿«é€Ÿå®šä½ç›®æ ‡å¯¹è±¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Referential grounding in outdoor driving scenes is challenging due to large scene variability, many visually similar objects, and dynamic elements that complicate resolving natural-language references (e.g., "the black car on the right"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf vision-language models for fine-grained attribute extraction with large language models for symbolic reasoning. LLM-RG processes an image and a free-form referring expression by using an LLM to extract relevant object types and attributes, detecting candidate regions, generating rich visual descriptors with a VLM, and then combining these descriptors with spatial metadata into natural-language prompts that are input to an LLM for chain-of-thought reasoning to identify the referent's bounding box. Evaluated on the Talk2Car benchmark, LLM-RG yields substantial gains over both LLM and VLM-based baselines. Additionally, our ablations show that adding 3D spatial cues further improves grounding. Our results demonstrate the complementary strengths of VLMs and LLMs, applied in a zero-shot manner, for robust outdoor referential grounding.

