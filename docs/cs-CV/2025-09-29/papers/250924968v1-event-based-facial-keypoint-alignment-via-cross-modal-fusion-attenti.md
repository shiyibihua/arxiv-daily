---
layout: default
title: Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning
---

# Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24968" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24968v1</a>
  <a href="https://arxiv.org/pdf/2509.24968.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24968v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24968v1', 'Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Donghwa Kang, Junho Kim, Dongwoo Kang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: 11 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè·¨æ¨¡æ€èåˆæ³¨æ„åŠ›å’Œè‡ªç›‘ç£å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ çš„äº‹ä»¶ç›¸æœºäººè„¸å…³é”®ç‚¹å¯¹é½æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `äººè„¸å…³é”®ç‚¹å¯¹é½` `è·¨æ¨¡æ€èåˆ` `è‡ªç›‘ç£å­¦ä¹ ` `å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RGBæ–¹æ³•åœ¨äº‹ä»¶æ•°æ®ä¸Šè¡¨ç°å·®ï¼Œå•ç‹¬è®­ç»ƒäº‹ä»¶æ•°æ®å› ç©ºé—´ä¿¡æ¯ä¸è¶³æ•ˆæœä¸ä½³ï¼Œä¸”ç¼ºä¹æ ‡è®°äº‹ä»¶æ•°æ®é›†ã€‚
2. åˆ©ç”¨è·¨æ¨¡æ€èåˆæ³¨æ„åŠ›(CMFA)æ•´åˆRGBä¿¡æ¯ï¼ŒæŒ‡å¯¼äº‹ä»¶ç‰¹å¾æå–ï¼›è‡ªç›‘ç£å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ (SSMER)ä»æ— æ ‡è®°æ•°æ®å­¦ä¹ ç‰¹å¾ã€‚
3. åœ¨E-SIEå’ŒWFLW-Væ•°æ®é›†ä¸Šå®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè·¨æ¨¡æ€èåˆæ³¨æ„åŠ›(CMFA)å’Œè‡ªç›‘ç£å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ (SSMER)çš„äº‹ä»¶ç›¸æœºäººè„¸å…³é”®ç‚¹å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä½å…‰ç…§ã€å¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„äººè„¸å…³é”®ç‚¹å¯¹é½é—®é¢˜ã€‚ç”±äºäº‹ä»¶ç›¸æœºå…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œå¯¹å…‰ç…§å˜åŒ–çš„é²æ£’æ€§ï¼Œå› æ­¤åœ¨è¿™äº›æ¡ä»¶ä¸‹å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RGBäººè„¸å…³é”®ç‚¹å¯¹é½æ–¹æ³•åœ¨äº‹ä»¶æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”ä»…åœ¨äº‹ä»¶æ•°æ®ä¸Šè®­ç»ƒé€šå¸¸ä¼šå¯¼è‡´æ¬¡ä¼˜æ€§èƒ½ï¼Œå› ä¸ºå…¶ç©ºé—´ä¿¡æ¯æœ‰é™ã€‚æ­¤å¤–ï¼Œç¼ºä¹å…¨é¢çš„æ ‡è®°äº‹ä»¶æ•°æ®é›†è¿›ä¸€æ­¥é˜»ç¢äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚CMFAç”¨äºæ•´åˆç›¸åº”çš„RGBæ•°æ®ï¼Œå¼•å¯¼æ¨¡å‹ä»äº‹ä»¶è¾“å…¥å›¾åƒä¸­æå–é²æ£’çš„äººè„¸ç‰¹å¾ã€‚SSMERèƒ½å¤Ÿä»æ— æ ‡è®°äº‹ä»¶æ•°æ®ä¸­è¿›è¡Œæœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ï¼Œå…‹æœç©ºé—´é™åˆ¶ã€‚åœ¨çœŸå®äº‹ä»¶E-SIEæ•°æ®é›†å’Œå…¬å…±WFLW-VåŸºå‡†çš„åˆæˆäº‹ä»¶ç‰ˆæœ¬ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³äº‹ä»¶ç›¸æœºåœ¨äººè„¸å…³é”®ç‚¹å¯¹é½ä»»åŠ¡ä¸­ï¼Œç”±äºç©ºé—´ä¿¡æ¯ä¸è¶³å’Œç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹RGBå›¾åƒè®¾è®¡çš„æ–¹æ³•ï¼Œæ— æ³•ç›´æ¥åº”ç”¨äºäº‹ä»¶æ•°æ®ï¼Œå¹¶ä¸”ç›´æ¥åœ¨äº‹ä»¶æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯èåˆå’Œè‡ªç›‘ç£å­¦ä¹ æ¥å¼¥è¡¥äº‹ä»¶æ•°æ®çš„ä¸è¶³ã€‚é€šè¿‡èåˆRGBå›¾åƒæä¾›çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ä»å¤§é‡æ— æ ‡æ³¨äº‹ä»¶æ•°æ®ä¸­æå–æœ‰æ•ˆç‰¹å¾ï¼Œä»è€Œæå‡äº‹ä»¶ç›¸æœºäººè„¸å…³é”®ç‚¹å¯¹é½çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè·¨æ¨¡æ€èåˆæ³¨æ„åŠ›(CMFA)å’Œè‡ªç›‘ç£å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ (SSMER)ã€‚CMFAæ¨¡å—å°†RGBå›¾åƒå’Œäº‹ä»¶æ•°æ®è¿›è¡Œèåˆï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼æ¨¡å‹å…³æ³¨äº‹ä»¶æ•°æ®ä¸­ä¸äººè„¸ç›¸å…³çš„åŒºåŸŸã€‚SSMERæ¨¡å—åˆ™åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä»æœªæ ‡æ³¨çš„äº‹ä»¶æ•°æ®ä¸­å­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆé€šè¿‡CMFAèåˆRGBå’Œäº‹ä»¶æ•°æ®ï¼Œç„¶ååˆ©ç”¨SSMERè¿›è¡Œç‰¹å¾å¢å¼ºï¼Œæœ€åè¿›è¡Œå…³é”®ç‚¹é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è·¨æ¨¡æ€èåˆå’Œè‡ªç›‘ç£å­¦ä¹ ç»“åˆèµ·æ¥ï¼Œç”¨äºäº‹ä»¶ç›¸æœºçš„äººè„¸å…³é”®ç‚¹å¯¹é½ã€‚CMFAæ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨RGBå›¾åƒæä¾›çš„ç©ºé—´ä¿¡æ¯ï¼Œè€ŒSSMERæ¨¡å—åˆ™èƒ½å¤Ÿä»æœªæ ‡æ³¨çš„äº‹ä»¶æ•°æ®ä¸­å­¦ä¹ åˆ°æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§ç»“åˆå…‹æœäº†äº‹ä»¶æ•°æ®ç©ºé—´ä¿¡æ¯ä¸è¶³å’Œç¼ºä¹æ ‡æ³¨æ•°æ®çš„éš¾é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šCMFAæ¨¡å—ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆRGBå’Œäº‹ä»¶æ•°æ®ï¼Œå…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ã€‚SSMERæ¨¡å—é‡‡ç”¨äº†å¤šäº‹ä»¶è¡¨å¾å­¦ä¹ ï¼Œå¯èƒ½æ¶‰åŠåˆ°å¯¹æ¯”å­¦ä¹ æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰æŠ€æœ¯ï¼Œå…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬å…³é”®ç‚¹é¢„æµ‹æŸå¤±ã€è·¨æ¨¡æ€ä¸€è‡´æ€§æŸå¤±å’Œè‡ªç›‘ç£å­¦ä¹ æŸå¤±ï¼Œå…·ä½“å½¢å¼æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨E-SIEçœŸå®äº‹ä»¶æ•°æ®é›†å’ŒWFLW-Våˆæˆäº‹ä»¶æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºâ€œconsistently surpasses state-of-the-art methods across multiple evaluation metricsâ€ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä½å…‰ç…§ã€å¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹çš„äººè„¸è¯†åˆ«ã€äººæœºäº¤äº’ã€å®‰å…¨ç›‘æ§ç­‰é¢†åŸŸã€‚äº‹ä»¶ç›¸æœºåœ¨è¿™äº›åœºæ™¯ä¸‹å…·æœ‰ä¼ ç»Ÿç›¸æœºæ— æ³•æ¯”æ‹Ÿçš„ä¼˜åŠ¿ï¼Œè¯¥ç ”ç©¶æœ‰åŠ©äºå……åˆ†å‘æŒ¥äº‹ä»¶ç›¸æœºçš„æ½œåŠ›ï¼Œæå‡ç›¸å…³åº”ç”¨ç³»ç»Ÿçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯æ‰©å±•åˆ°å…¶ä»–åŸºäºäº‹ä»¶ç›¸æœºçš„è§†è§‰ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event cameras offer unique advantages for facial keypoint alignment under challenging conditions, such as low light and rapid motion, due to their high temporal resolution and robustness to varying illumination. However, existing RGB facial keypoint alignment methods do not perform well on event data, and training solely on event data often leads to suboptimal performance because of its limited spatial information. Moreover, the lack of comprehensive labeled event datasets further hinders progress in this area. To address these issues, we propose a novel framework based on cross-modal fusion attention (CMFA) and self-supervised multi-event representation learning (SSMER) for event-based facial keypoint alignment. Our framework employs CMFA to integrate corresponding RGB data, guiding the model to extract robust facial features from event input images. In parallel, SSMER enables effective feature learning from unlabeled event data, overcoming spatial limitations. Extensive experiments on our real-event E-SIE dataset and a synthetic-event version of the public WFLW-V benchmark show that our approach consistently surpasses state-of-the-art methods across multiple evaluation metrics.

