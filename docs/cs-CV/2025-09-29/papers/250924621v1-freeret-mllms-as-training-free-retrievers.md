---
layout: default
title: FreeRet: MLLMs as Training-Free Retrievers
---

# FreeRet: MLLMs as Training-Free Retrievers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24621" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24621v1</a>
  <a href="https://arxiv.org/pdf/2509.24621.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24621v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24621v1', 'FreeRet: MLLMs as Training-Free Retrievers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhan Zhu, Xiangyu Zeng, Chenting Wang, Xinhao Li, Yicheng Xu, Ziang Yan, Yi Wang, Limin Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FreeRetï¼šæ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨MLLMå®ç°å¼ºå¤§çš„å¤šæ¨¡æ€æ£€ç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ ` `è¯­ä¹‰åµŒå…¥` `é‡æ’åº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMæ£€ç´¢æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒï¼Œå°†å…¶è½¬åŒ–ä¸ºå¯¹æ¯”ç¼–ç å™¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. FreeRetæ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œé€šè¿‡è¯­ä¹‰åµŒå…¥å’Œæ¨ç†é‡æ’åºï¼Œå°†MLLMè½¬åŒ–ä¸ºå¼ºå¤§çš„æ£€ç´¢å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFreeRetåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºè®­ç»ƒæ¨¡å‹ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æ­£æˆä¸ºæ··åˆæ¨¡æ€æ£€ç´¢çš„å¤šåŠŸèƒ½åŸºç¡€ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„åè®­ç»ƒæ‰èƒ½è½¬åŒ–ä¸ºç”¨äºæ£€ç´¢çš„å¯¹æ¯”ç¼–ç å™¨ã€‚æœ¬æ–‡æ¢è®¨äº†ï¼šç°æˆçš„MLLMæ˜¯å¦å¯ä»¥åœ¨æ²¡æœ‰é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ä½œä¸ºå¼ºå¤§çš„æ£€ç´¢å™¨ï¼Ÿæˆ‘ä»¬æå‡ºäº†FreeRetï¼Œä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥å°†ä»»ä½•MLLMè½¬åŒ–ä¸ºä¸¤é˜¶æ®µæ£€ç´¢å™¨ã€‚FreeReté¦–å…ˆç›´æ¥ä»æ¨¡å‹ä¸­å¯¼å‡ºè¯­ä¹‰ç›¸å…³çš„åµŒå…¥ï¼Œç”¨äºå¿«é€Ÿå€™é€‰æœç´¢ï¼Œç„¶ååˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›è¿›è¡Œç²¾ç¡®çš„é‡æ’åºã€‚è¯¥æ¡†æ¶è´¡çŒ®äº†ä¸‰ä¸ªæ–¹é¢çš„è¿›å±•ï¼šç»•è¿‡è¯æ±‡å¯¹é½å±‚ä»¥è·å¾—è¯­ä¹‰å¿ å®çš„åµŒå…¥ï¼Œä½¿ç”¨æ˜¾å¼å…ˆéªŒè°ƒèŠ‚è¡¨ç¤ºç”Ÿæˆï¼Œä»¥åŠé€šè¿‡ä¸­æ€§é€‰æ‹©æ¡†æ¶å‡è½»é‡æ’åºä¸­çš„æ¡†æ¶æ•ˆåº”ã€‚åœ¨æ¶µç›–46ä¸ªæ•°æ®é›†çš„MMEBå’ŒMMEB-V2åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFreeRetæ˜¾è‘—ä¼˜äºåœ¨æ•°ç™¾ä¸‡ä¸ªpairä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚é™¤äº†åŸºå‡†æµ‹è¯•ä¹‹å¤–ï¼ŒFreeRetæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå¯ä»¥åœ¨MLLMç³»åˆ—å’Œå¤§å°ä¹‹é—´æ— ç¼æ‰©å±•ï¼Œä¿ç•™å…¶ç”Ÿæˆèƒ½åŠ›ï¼Œæ”¯æŒä»»æ„æ¨¡æ€ç»„åˆï¼Œå¹¶å°†æ£€ç´¢ã€é‡æ’åºå’Œç”Ÿæˆç»Ÿä¸€åˆ°å•ä¸ªæ¨¡å‹ä¸­çš„ç«¯åˆ°ç«¯RAGä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒåˆ©ç”¨çš„é¢„è®­ç»ƒMLLMå¯ä»¥åœ¨æ²¡æœ‰è®­ç»ƒçš„æƒ…å†µä¸‹ä½œä¸ºå¼ºå¤§çš„æ£€ç´¢å¼•æ“ï¼Œä»è€Œå¼¥è¡¥äº†å®ƒä»¬ä½œä¸ºé€šç”¨æ¨¡å‹çš„ä¸€ä¸ªå…³é”®å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯¹æ¯”å­¦ä¹ ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºæ¥å°†MLLMè½¬åŒ–ä¸ºå¯¹æ¯”ç¼–ç å™¨ã€‚è¿™äº›æ–¹æ³•ä¸ä»…è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œè€Œä¸”å¯èƒ½é™åˆ¶äº†MLLMçš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œè¯æ±‡å¯¹é½å±‚å¯èƒ½ä¼šå¼•å…¥å™ªå£°ï¼Œå½±å“æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFreeRetçš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒMLLMçš„è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚é€šè¿‡ç»•è¿‡è¯æ±‡å¯¹é½å±‚ï¼Œè·å¾—æ›´å‡†ç¡®çš„è¯­ä¹‰åµŒå…¥ï¼Œå¹¶åˆ©ç”¨MLLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œé‡æ’åºï¼Œä»è€Œæé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFreeRetæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ£€ç´¢æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯å€™é€‰æ£€ç´¢ï¼Œåˆ©ç”¨ä»MLLMä¸­æå–çš„è¯­ä¹‰åµŒå…¥è¿›è¡Œå¿«é€Ÿæœç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç‰¹å®šçš„promptï¼Œå¼•å¯¼MLLMç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œç„¶åæå–è¯¥æ–‡æœ¬æè¿°çš„åµŒå…¥å‘é‡ä½œä¸ºå›¾åƒ/è§†é¢‘çš„è¡¨ç¤ºã€‚ç¬¬äºŒé˜¶æ®µæ˜¯é‡æ’åºï¼Œåˆ©ç”¨MLLMçš„æ¨ç†èƒ½åŠ›å¯¹å€™é€‰ç»“æœè¿›è¡Œæ’åºï¼Œé€šè¿‡è®¾è®¡ä¸­æ€§çš„promptï¼Œå‡è½»æ¡†æ¶æ•ˆåº”çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šFreeRetçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ— éœ€è®­ç»ƒï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒMLLMçš„æ£€ç´¢èƒ½åŠ›ï¼›2) ç»•è¿‡è¯æ±‡å¯¹é½å±‚ï¼Œè·å¾—æ›´å‡†ç¡®çš„è¯­ä¹‰åµŒå…¥ï¼›3) ä½¿ç”¨æ˜¾å¼å…ˆéªŒè°ƒèŠ‚è¡¨ç¤ºç”Ÿæˆï¼Œæé«˜åµŒå…¥çš„è´¨é‡ï¼›4) é€šè¿‡ä¸­æ€§é€‰æ‹©æ¡†æ¶å‡è½»é‡æ’åºä¸­çš„æ¡†æ¶æ•ˆåº”ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFreeRetæ›´åŠ é«˜æ•ˆã€çµæ´»å’Œé€šç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šFreeRetçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç‰¹å®šçš„promptå¼•å¯¼MLLMç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œä»è€Œæå–è¯­ä¹‰åµŒå…¥ï¼›2) è®¾è®¡ä¸­æ€§çš„promptï¼Œå‡è½»é‡æ’åºä¸­çš„æ¡†æ¶æ•ˆåº”ï¼›3) ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºç›¸ä¼¼æ€§åº¦é‡ï¼›4) åœ¨é‡æ’åºé˜¶æ®µï¼Œä½¿ç”¨MLLMå¯¹å€™é€‰ç»“æœè¿›è¡Œæ‰“åˆ†ï¼Œå¹¶æ ¹æ®å¾—åˆ†è¿›è¡Œæ’åºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FreeRetåœ¨MMEBå’ŒMMEB-V2åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†åœ¨æ•°ç™¾ä¸‡ä¸ªpairä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒFreeRetçš„æ€§èƒ½æå‡è¶…è¿‡äº†10%ã€‚æ­¤å¤–ï¼ŒFreeRetå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„MLLMæ¨¡å‹å’Œä¸åŒçš„æ¨¡æ€ç»„åˆï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ£€ç´¢èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FreeRetå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€å›¾åƒ/è§†é¢‘æœç´¢ã€é—®ç­”ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚å®ƒèƒ½å¤Ÿé™ä½å¤šæ¨¡æ€æ£€ç´¢çš„è®­ç»ƒæˆæœ¬ï¼Œæé«˜æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¹¶æ”¯æŒä»»æ„æ¨¡æ€ç»„åˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„æœªæ¥å‘å±•å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºç”µå•†å¹³å°çš„å•†å“æœç´¢ï¼Œæ–°é—»åª’ä½“çš„è§†é¢‘æ£€ç´¢ï¼Œä»¥åŠæ™ºèƒ½å®¢æœçš„çŸ¥è¯†åº“æ£€ç´¢ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) are emerging as versatile foundations for mixed-modality retrieval. Yet, they often require heavy post-hoc training to convert them into contrastive encoders for retrieval. This work asks: Can off-the-shelf MLLMs serve as powerful retrievers without additional training? We present FreeRet, a plug-and-play framework that turns any MLLM into a two-stage retriever. FreeRet first derives semantically grounded embeddings directly from the model for fast candidate search, and then exploits its reasoning ability for precise reranking. The framework contributes three advances: bypassing lexical alignment layers to obtain semantically faithful embeddings, conditioning representation generation with explicit priors, and mitigating framing effect in reranking via neutral choice framing. On the MMEB and MMEB-V2 benchmarks spanning 46 datasets, FreeRet substantially outperforms models trained on millions of pairs. Beyond benchmarks, FreeRet is model-agnostic and scales seamlessly across MLLM families and sizes, preserves their generative abilities, supports arbitrary modality combinations, and unifies retrieval, reranking, and generation into end-to-end RAG within a single model. Our findings demonstrate that pretrained MLLMs, when carefully harnessed, can serve as strong retrieval engines without training, closing a critical gap in their role as generalists.

