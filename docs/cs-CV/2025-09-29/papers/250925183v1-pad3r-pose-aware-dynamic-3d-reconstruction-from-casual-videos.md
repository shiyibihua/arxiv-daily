---
layout: default
title: PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos
---

# PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25183" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25183v1</a>
  <a href="https://arxiv.org/pdf/2509.25183.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25183v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25183v1', 'PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ting-Hsuan Liao, Haowen Liu, Yiran Xu, Songwei Ge, Gengshan Yang, Jia-Bin Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: SIGGRAPH Asia 2025. Project page:https://pad3r.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PAD3Rï¼šä»å•ç›®è§†é¢‘ä¸­è¿›è¡Œå§¿æ€æ„ŸçŸ¥çš„åŠ¨æ€3Dé‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `åŠ¨æ€3Dé‡å»º` `å•ç›®è§†é¢‘` `å§¿æ€ä¼°è®¡` `å¯å˜å½¢ç‰©ä½“` `3Dé«˜æ–¯è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†é•¿è§†é¢‘ä¸­ç‰©ä½“çš„å¤§å¹…åº¦å½¢å˜ã€å‰§çƒˆç›¸æœºè¿åŠ¨å’Œæœ‰é™è§†è§’è¦†ç›–ç­‰é—®é¢˜ã€‚
2. PAD3Ré€šè¿‡è®­ç»ƒä¸ªæ€§åŒ–çš„å§¿æ€ä¼°è®¡å™¨ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„å›¾åƒåˆ°3Dæ¨¡å‹å’Œé•¿æœŸ2Dç‚¹è·Ÿè¸ªï¼Œä¼˜åŒ–å¯å˜å½¢3Dé«˜æ–¯è¡¨ç¤ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPAD3Råœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé‡å»ºé«˜ä¿çœŸåº¦çš„3Dæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

PAD3Ræ˜¯ä¸€ç§ä»éšæ„æ‹æ‘„çš„ã€æ— å§¿æ€å•ç›®è§†é¢‘ä¸­é‡å»ºå¯å˜å½¢3Dç‰©ä½“çš„æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒPAD3Rèƒ½å¤Ÿå¤„ç†åŒ…å«æ˜¾è‘—ç‰©ä½“å˜å½¢ã€å¤§è§„æ¨¡ç›¸æœºè¿åŠ¨å’Œæœ‰é™è§†è§’è¦†ç›–çš„é•¿è§†é¢‘åºåˆ—ï¼Œè¿™äº›é€šå¸¸å¯¹ä¼ ç»Ÿç³»ç»Ÿæ„æˆæŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯è®­ç»ƒä¸€ä¸ªä¸ªæ€§åŒ–çš„ã€ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„å§¿æ€ä¼°è®¡å™¨ï¼Œå¹¶ç”±é¢„è®­ç»ƒçš„å›¾åƒåˆ°3Dæ¨¡å‹è¿›è¡Œç›‘ç£ã€‚è¿™æŒ‡å¯¼äº†å¯å˜å½¢3Dé«˜æ–¯è¡¨ç¤ºçš„ä¼˜åŒ–ã€‚è¯¥ä¼˜åŒ–è¿‡ç¨‹è¿˜å—åˆ°æ•´ä¸ªè¾“å…¥è§†é¢‘ä¸Šçš„é•¿æœŸ2Dç‚¹è·Ÿè¸ªçš„çº¦æŸã€‚é€šè¿‡ç»“åˆç”Ÿæˆå…ˆéªŒå’Œå¯å¾®æ¸²æŸ“ï¼ŒPAD3Rä»¥ç±»åˆ«æ— å…³çš„æ–¹å¼é‡å»ºç‰©ä½“çš„é«˜ä¿çœŸã€é“°æ¥å¼3Dè¡¨ç¤ºã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡ç»“æœè¡¨æ˜ï¼ŒPAD3Ræ˜¯ç¨³å¥çš„ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªå‡ºäº†å…¶åœ¨åŠ¨æ€åœºæ™¯ç†è§£å’Œ3Då†…å®¹åˆ›å»ºæ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä»éšæ„æ‹æ‘„çš„å•ç›®è§†é¢‘ä¸­é‡å»ºå¯å˜å½¢3Dç‰©ä½“æ—¶ï¼Œé¢ä¸´ç€ç‰©ä½“å¤§å¹…åº¦å½¢å˜ã€å‰§çƒˆç›¸æœºè¿åŠ¨ä»¥åŠè§†è§’è¦†ç›–æœ‰é™ç­‰æŒ‘æˆ˜ã€‚è¿™äº›å› ç´ ä¼šå¯¼è‡´é‡å»ºç»“æœä¸å‡†ç¡®ï¼Œç”šè‡³å¤±è´¥ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥å¤„ç†é•¿è§†é¢‘åºåˆ—ï¼Œå¹¶ä¸”å¯¹åˆå§‹åŒ–å§¿æ€æœ‰è¾ƒå¼ºçš„ä¾èµ–æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPAD3Rçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªä¸ªæ€§åŒ–çš„ã€ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„å§¿æ€ä¼°è®¡å™¨æ¥å¼•å¯¼å¯å˜å½¢3Dé«˜æ–¯è¡¨ç¤ºçš„ä¼˜åŒ–ã€‚é€šè¿‡é¢„è®­ç»ƒçš„å›¾åƒåˆ°3Dæ¨¡å‹æä¾›ç›‘ç£ä¿¡å·ï¼Œå¹¶ç»“åˆé•¿æœŸ2Dç‚¹è·Ÿè¸ªä½œä¸ºçº¦æŸï¼Œä»è€Œå®ç°å¯¹ç‰©ä½“å½¢å˜å’Œç›¸æœºè¿åŠ¨çš„é²æ£’ä¼°è®¡ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†ç”Ÿæˆå…ˆéªŒå’Œå¯å¾®æ¸²æŸ“ï¼Œèƒ½å¤Ÿåœ¨ç±»åˆ«æ— å…³çš„æƒ…å†µä¸‹é‡å»ºé«˜ä¿çœŸåº¦çš„3Dæ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPAD3Rçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å§¿æ€ä¼°è®¡å™¨è®­ç»ƒï¼šä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒåˆ°3Dæ¨¡å‹ç›‘ç£è®­ç»ƒä¸€ä¸ªä¸ªæ€§åŒ–çš„å§¿æ€ä¼°è®¡å™¨ã€‚2) 3Dé«˜æ–¯è¡¨ç¤ºä¼˜åŒ–ï¼šåˆ©ç”¨å§¿æ€ä¼°è®¡å™¨æä¾›çš„å§¿æ€ä¿¡æ¯ï¼Œä¼˜åŒ–å¯å˜å½¢3Dé«˜æ–¯è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®åœ°è¡¨ç¤ºç‰©ä½“çš„å½¢çŠ¶å’Œå¤–è§‚ã€‚3) é•¿æœŸ2Dç‚¹è·Ÿè¸ªï¼šåœ¨æ•´ä¸ªè§†é¢‘åºåˆ—ä¸Šè¿›è¡Œ2Dç‚¹è·Ÿè¸ªï¼Œä¸º3Dé‡å»ºæä¾›é¢å¤–çš„çº¦æŸã€‚4) å¯å¾®æ¸²æŸ“ï¼šä½¿ç”¨å¯å¾®æ¸²æŸ“æŠ€æœ¯ï¼Œå°†3Dé«˜æ–¯è¡¨ç¤ºæ¸²æŸ“æˆ2Då›¾åƒï¼Œå¹¶ä¸è¾“å…¥è§†é¢‘è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œä¼˜åŒ–3Dè¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šPAD3Rçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å§¿æ€æ„ŸçŸ¥çš„åŠ¨æ€3Dé‡å»ºæ–¹æ³•ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªä¸ªæ€§åŒ–çš„å§¿æ€ä¼°è®¡å™¨ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„å›¾åƒåˆ°3Dæ¨¡å‹å’Œé•¿æœŸ2Dç‚¹è·Ÿè¸ªï¼ŒPAD3Rèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ç‰©ä½“å½¢å˜å’Œç›¸æœºè¿åŠ¨ï¼Œä»è€Œå®ç°å¯¹å¯å˜å½¢ç‰©ä½“çš„å‡†ç¡®3Dé‡å»ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPAD3Rä¸éœ€è¦é¢„å…ˆçŸ¥é“ç‰©ä½“çš„å§¿æ€ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†é•¿è§†é¢‘åºåˆ—ã€‚

**å…³é”®è®¾è®¡**ï¼šPAD3Rçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¸ªæ€§åŒ–çš„å§¿æ€ä¼°è®¡å™¨ï¼šé’ˆå¯¹æ¯ä¸ªç‰©ä½“è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„å§¿æ€ä¼°è®¡å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç‰©ä½“çš„ç‰¹å®šå½¢çŠ¶å’Œè¿åŠ¨æ¨¡å¼ã€‚2) 3Dé«˜æ–¯è¡¨ç¤ºï¼šä½¿ç”¨3Dé«˜æ–¯è¡¨ç¤ºæ¥è¡¨ç¤ºç‰©ä½“çš„å½¢çŠ¶å’Œå¤–è§‚ï¼Œè¿™ç§è¡¨ç¤ºæ–¹æ³•å…·æœ‰å¯å¾®æ€§ï¼Œä¾¿äºä¼˜åŒ–ã€‚3) é•¿æœŸ2Dç‚¹è·Ÿè¸ªï¼šä½¿ç”¨å…‰æµç­‰æŠ€æœ¯åœ¨æ•´ä¸ªè§†é¢‘åºåˆ—ä¸Šè¿›è¡Œ2Dç‚¹è·Ÿè¸ªï¼Œä¸º3Dé‡å»ºæä¾›é¢å¤–çš„çº¦æŸã€‚4) æŸå¤±å‡½æ•°ï¼šä½¿ç”¨å¤šç§æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–3Dé«˜æ–¯è¡¨ç¤ºï¼ŒåŒ…æ‹¬å›¾åƒé‡å»ºæŸå¤±ã€å§¿æ€æŸå¤±å’Œ2Dç‚¹è·Ÿè¸ªæŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PAD3Råœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ…å«å¤§å¹…åº¦å½¢å˜å’Œå‰§çƒˆç›¸æœºè¿åŠ¨çš„è§†é¢‘åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPAD3Rèƒ½å¤Ÿé‡å»ºé«˜ä¿çœŸåº¦çš„3Dæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨é‡å»ºç²¾åº¦å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PAD3Rå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬åŠ¨æ€åœºæ™¯ç†è§£ã€3Då†…å®¹åˆ›å»ºã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚è¯¥æŠ€æœ¯å¯ä»¥ç”¨äºé‡å»ºäººä½“ã€åŠ¨ç‰©å’Œæœè£…ç­‰å¯å˜å½¢ç‰©ä½“çš„3Dæ¨¡å‹ï¼Œä»è€Œä¸ºåŠ¨ç”»åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿè¯•ç©¿ç­‰åº”ç”¨æä¾›æ”¯æŒã€‚æ­¤å¤–ï¼ŒPAD3Rè¿˜å¯ä»¥ç”¨äºæœºå™¨äººå¯¼èˆªå’Œç‰©ä½“è¯†åˆ«ç­‰ä»»åŠ¡ï¼Œæé«˜æœºå™¨äººçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present PAD3R, a method for reconstructing deformable 3D objects from casually captured, unposed monocular videos. Unlike existing approaches, PAD3R handles long video sequences featuring substantial object deformation, large-scale camera movement, and limited view coverage that typically challenge conventional systems. At its core, our approach trains a personalized, object-centric pose estimator, supervised by a pre-trained image-to-3D model. This guides the optimization of deformable 3D Gaussian representation. The optimization is further regularized by long-term 2D point tracking over the entire input video. By combining generative priors and differentiable rendering, PAD3R reconstructs high-fidelity, articulated 3D representations of objects in a category-agnostic way. Extensive qualitative and quantitative results show that PAD3R is robust and generalizes well across challenging scenarios, highlighting its potential for dynamic scene understanding and 3D content creation.

