---
layout: default
title: OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding
---

# OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00069" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00069v1</a>
  <a href="https://arxiv.org/pdf/2510.00069.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00069v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00069v1', 'OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiancong Xie, Wenjin Wang, Zhuomeng Zhang, Zihan Liu, Qi Liu, Ke Feng, Zixun Sun, Yuedong Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/XiejcSYSU/OIG-Bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOIG-BenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹å•å›¾å¼•å¯¼çš„ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å•å›¾å¼•å¯¼ç†è§£` `åŸºå‡†æ•°æ®é›†` `åŠè‡ªåŠ¨æ ‡æ³¨` `å¤šæ™ºèƒ½ä½“åä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹å•å›¾å¼•å¯¼çš„ç†è§£èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨çš„åŸºå‡†ã€‚
2. æå‡ºä¸€ç§åŠè‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œç”Ÿæˆå›¾åƒæè¿°ï¼Œè¾…åŠ©äººå·¥æ„å»ºé«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹ã€‚
3. æ„å»ºOIG-BenchåŸºå‡†å¹¶è¯„ä¼°äº†29ä¸ªMLLMï¼Œå‘ç°æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾å¼±ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºOIG-Benchï¼Œä¸€ä¸ªç»¼åˆæ€§çš„åŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£å•å›¾å¼•å¯¼æ–¹é¢çš„èƒ½åŠ›ã€‚å•å›¾å¼•å¯¼æ˜¯ä¸€ç§ç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œç¬¦å·çš„è§†è§‰å½¢å¼ï¼Œæ—¨åœ¨ä»¥é‡ç»„å’Œç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°ä¿¡æ¯ï¼Œä¾¿äºäººç±»ç†è§£ï¼Œå¹¶ä½“ç°äº†äººç±»æ„ŸçŸ¥å’Œç†è§£çš„ç‰¹æ€§ã€‚ä¸ºäº†é™ä½æ‰‹åŠ¨æ ‡æ³¨çš„æˆæœ¬ï¼Œå¼€å‘äº†ä¸€ç§åŠè‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œå…¶ä¸­å¤šä¸ªæ™ºèƒ½ä½“ååŒç”Ÿæˆåˆæ­¥çš„å›¾åƒæè¿°ï¼Œè¾…åŠ©äººå·¥æ„å»ºå›¾åƒ-æ–‡æœ¬å¯¹ã€‚ä½¿ç”¨OIG-Benchå¯¹29ä¸ªæœ€å…ˆè¿›çš„MLLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒQwen2.5-VL-72Båœ¨è¯„ä¼°çš„æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º77%ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†æ–¹é¢éƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„å¼±ç‚¹ï¼Œè¡¨æ˜å½“å‰çš„MLLMä»ç„¶éš¾ä»¥å‡†ç¡®åœ°è§£é‡Šå¤æ‚çš„è§†è§‰-æ–‡æœ¬å…³ç³»ã€‚æ­¤å¤–ï¼Œè¿˜è¯æ˜äº†æ‰€æå‡ºçš„å¤šæ™ºèƒ½ä½“æ ‡æ³¨ç³»ç»Ÿåœ¨å›¾åƒæè¿°æ–¹é¢ä¼˜äºæ‰€æœ‰MLLMï¼Œçªæ˜¾äº†å…¶ä½œä¸ºé«˜è´¨é‡å›¾åƒæè¿°ç”Ÿæˆå™¨å’Œæœªæ¥æ•°æ®é›†æ„å»ºå·¥å…·çš„æ½œåŠ›ã€‚æ•°æ®é›†å¯åœ¨https://github.com/XiejcSYSU/OIG-Benchè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£å•å›¾å¼•å¯¼ï¼ˆOne-Image Guides, OIGsï¼‰æ–¹é¢çš„èƒ½åŠ›è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•é€šå¸¸ä¾§é‡äºé€šç”¨å›¾åƒç†è§£æˆ–è§†è§‰é—®ç­”ï¼Œç¼ºä¹é’ˆå¯¹OIGsè¿™ç§ç‰¹æ®Šè§†è§‰å½¢å¼çš„ä¸“é—¨åŸºå‡†ã€‚OIGsç»“åˆäº†æ–‡æœ¬ã€å›¾åƒå’Œç¬¦å·ï¼Œéœ€è¦æ¨¡å‹å…·å¤‡æ›´å¼ºçš„è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œè€Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°MLLMså¯¹OIGsç†è§£èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†OIG-Benchã€‚ä¸ºäº†é™ä½æ ‡æ³¨æˆæœ¬ï¼Œé‡‡ç”¨äº†åŠè‡ªåŠ¨åŒ–çš„æ ‡æ³¨æµç¨‹ï¼Œåˆ©ç”¨å¤šä¸ªæ™ºèƒ½ä½“åä½œç”Ÿæˆåˆæ­¥çš„å›¾åƒæè¿°ï¼Œè¾…åŠ©äººå·¥è¿›è¡Œæ ‡æ³¨ï¼Œä»è€Œæé«˜æ ‡æ³¨æ•ˆç‡å’Œè´¨é‡ã€‚é€šè¿‡å¯¹å¤šä¸ªMLLMsåœ¨OIG-Benchä¸Šçš„è¯„ä¼°ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°äº†è§£å®ƒä»¬åœ¨ç†è§£å¤æ‚è§†è§‰-æ–‡æœ¬å…³ç³»æ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOIG-Benchçš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†æ¥è‡ªä¸åŒé¢†åŸŸçš„OIGså›¾åƒã€‚2) å¤šæ™ºèƒ½ä½“è¾…åŠ©æ ‡æ³¨ï¼šåˆ©ç”¨å¤šä¸ªæ™ºèƒ½ä½“ç”Ÿæˆåˆæ­¥çš„å›¾åƒæè¿°ï¼Œä¾‹å¦‚å›¾åƒæ ‡é¢˜ã€å¯¹è±¡æ£€æµ‹ç»“æœç­‰ã€‚3) äººå·¥æ ¡å¯¹ä¸æ ‡æ³¨ï¼šäººå·¥å¯¹æ™ºèƒ½ä½“ç”Ÿæˆçš„æè¿°è¿›è¡Œæ ¡å¯¹å’Œè¡¥å……ï¼Œæ„å»ºé«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹ã€‚4) åŸºå‡†è¯„ä¼°ï¼šä½¿ç”¨æ„å»ºå¥½çš„OIG-Benchå¯¹å¤šä¸ªMLLMsè¿›è¡Œè¯„ä¼°ï¼Œåˆ†æå®ƒä»¬çš„æ€§èƒ½è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“è¾…åŠ©æ ‡æ³¨æµç¨‹ï¼Œç”¨äºæ„å»ºé«˜è´¨é‡çš„OIG-Benchæ•°æ®é›†ã€‚è¯¥æµç¨‹é€šè¿‡åˆ©ç”¨å¤šä¸ªæ™ºèƒ½ä½“çš„åä½œï¼Œå¯ä»¥æ˜¾è‘—é™ä½äººå·¥æ ‡æ³¨çš„æˆæœ¬ï¼Œå¹¶æé«˜æ ‡æ³¨æ•ˆç‡å’Œè´¨é‡ã€‚æ­¤å¤–ï¼ŒOIG-Benchæ•°æ®é›†æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ï¼Œå®ƒå¡«è¡¥äº†ç°æœ‰MLLMè¯„ä¼°åŸºå‡†åœ¨OIGsç†è§£èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤šæ™ºèƒ½ä½“æ ‡æ³¨ç³»ç»ŸåŒ…å«å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ã€ç›®æ ‡æ£€æµ‹æ¨¡å‹ç­‰ã€‚è¿™äº›æ¨¡å‹å¹¶è¡Œå·¥ä½œï¼Œç”Ÿæˆä¸åŒçš„å›¾åƒæè¿°ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯æ•´åˆèµ·æ¥ï¼Œä½œä¸ºäººå·¥æ ‡æ³¨çš„å‚è€ƒã€‚åœ¨åŸºå‡†è¯„ä¼°æ–¹é¢ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€å¬å›ç‡ç­‰ï¼Œä»¥å…¨é¢è¯„ä¼°MLLMsåœ¨OIGsç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen2.5-VL-72Båœ¨OIG-Benchä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°77%ã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¢«è¯„ä¼°çš„æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†æ–¹é¢éƒ½å­˜åœ¨æ˜æ˜¾çš„ä¸è¶³ï¼Œè¡¨æ˜ç°æœ‰MLLMåœ¨ç†è§£å¤æ‚è§†è§‰-æ–‡æœ¬å…³ç³»æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚æ­¤å¤–ï¼Œå¤šæ™ºèƒ½ä½“æ ‡æ³¨ç³»ç»Ÿåœ¨å›¾åƒæè¿°ç”Ÿæˆæ–¹é¢ä¼˜äºæ‰€æœ‰MLLMï¼ŒéªŒè¯äº†å…¶ä½œä¸ºé«˜è´¨é‡å›¾åƒæè¿°ç”Ÿæˆå™¨çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„è§†è§‰åŠ©æ‰‹ã€æ•™è‚²å·¥å…·å’Œä¿¡æ¯æ£€ç´¢ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨MLLMç†è§£OIGsï¼Œä¸ºç”¨æˆ·æä¾›æ›´ä¾¿æ·çš„å¯¼èˆªã€æ“ä½œæŒ‡å—å’ŒçŸ¥è¯†å­¦ä¹ æœåŠ¡ã€‚æ­¤å¤–ï¼ŒOIG-Benchæ•°æ®é›†å¯ä»¥ä¿ƒè¿›MLLMåœ¨è§†è§‰-æ–‡æœ¬ç†è§£æ–¹é¢çš„ç ”ç©¶ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities. However, evaluating their capacity for human-like understanding in One-Image Guides remains insufficiently explored. One-Image Guides are a visual format combining text, imagery, and symbols to present reorganized and structured information for easier comprehension, which are specifically designed for human viewing and inherently embody the characteristics of human perception and understanding. Here, we present OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding across diverse domains. To reduce the cost of manual annotation, we developed a semi-automated annotation pipeline in which multiple intelligent agents collaborate to generate preliminary image descriptions, assisting humans in constructing image-text pairs. With OIG-Bench, we have conducted a comprehensive evaluation of 29 state-of-the-art MLLMs, including both proprietary and open-source models. The results show that Qwen2.5-VL-72B performs the best among the evaluated models, with an overall accuracy of 77%. Nevertheless, all models exhibit notable weaknesses in semantic understanding and logical reasoning, indicating that current MLLMs still struggle to accurately interpret complex visual-text relationships. In addition, we also demonstrate that the proposed multi-agent annotation system outperforms all MLLMs in image captioning, highlighting its potential as both a high-quality image description generator and a valuable tool for future dataset construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.

