---
layout: default
title: Multimodal Arabic Captioning with Interpretable Visual Concept Integration
---

# Multimodal Arabic Captioning with Interpretable Visual Concept Integration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03295" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03295v1</a>
  <a href="https://arxiv.org/pdf/2510.03295.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03295v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03295v1', 'Multimodal Arabic Captioning with Interpretable Visual Concept Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Passant Elchafei, Amany Fashwan

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLCAPï¼šä¸€ç§ç»“åˆå¯è§£é‡Šè§†è§‰æ¦‚å¿µé›†æˆçš„å¤šæ¨¡æ€é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰æ¦‚å¿µé›†æˆ` `å¯è§£é‡Šæ€§` `CLIP` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ–‡åŒ–è¿è´¯æ€§` `ä¸Šä¸‹æ–‡å‡†ç¡®æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥ä¿è¯æ–‡åŒ–ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚
2. VLCAPé€šè¿‡æ£€ç´¢å¹¶èåˆè§†è§‰æ¦‚å¿µï¼Œå°†å›¾åƒæè¿°ç”Ÿæˆå»ºç«‹åœ¨å¯è§£é‡Šçš„é˜¿æ‹‰ä¼¯è¯­è§†è§‰æ¦‚å¿µä¹‹ä¸Šã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVLCAPåœ¨BLEU-1ã€ä½™å¼¦ç›¸ä¼¼åº¦å’ŒLLM-judgeè¯„åˆ†ç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVLCAPçš„é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†åŸºäºCLIPçš„è§†è§‰æ ‡ç­¾æ£€ç´¢å’Œå¤šæ¨¡æ€æ–‡æœ¬ç”Ÿæˆã€‚VLCAPå¹¶éå®Œå…¨ä¾èµ–ç«¯åˆ°ç«¯æè¿°ï¼Œè€Œæ˜¯å°†ç”Ÿæˆè¿‡ç¨‹å»ºç«‹åœ¨å¯è§£é‡Šçš„é˜¿æ‹‰ä¼¯è¯­è§†è§‰æ¦‚å¿µä¹‹ä¸Šï¼Œè¿™äº›æ¦‚å¿µé€šè¿‡ä¸‰ä¸ªå¤šè¯­è¨€ç¼–ç å™¨ï¼ˆmCLIPã€AraCLIPå’ŒJina V4ï¼‰æå–ï¼Œå¹¶åˆ†åˆ«è¯„ä¼°å…¶æ ‡ç­¾æ£€ç´¢æ€§èƒ½ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªæ··åˆè¯æ±‡è¡¨ï¼Œè¯¥è¯æ±‡è¡¨ç”±è®­ç»ƒæè¿°è¯­å¥æ„æˆï¼Œå¹¶ä½¿ç”¨ä»Visual Genomeæ•°æ®é›†ä¸­ç¿»è¯‘çš„çº¦2.1ä¸‡ä¸ªé€šç”¨é¢†åŸŸæ ‡ç­¾ï¼ˆæ¶µç›–å¯¹è±¡ã€å±æ€§å’Œåœºæ™¯ï¼‰è¿›è¡Œä¸°å¯Œã€‚æ£€ç´¢åˆ°çš„å‰kä¸ªæ ‡ç­¾è¢«è½¬æ¢ä¸ºæµç•…çš„é˜¿æ‹‰ä¼¯è¯­æç¤ºï¼Œå¹¶ä¸åŸå§‹å›¾åƒä¸€èµ·ä¼ é€’ç»™è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæµ‹è¯•äº†Qwen-VLå’ŒGemini Pro Visionç”¨äºæè¿°ç”Ÿæˆï¼Œäº§ç”Ÿäº†å…­ç§ç¼–ç å™¨-è§£ç å™¨é…ç½®ã€‚ç»“æœè¡¨æ˜ï¼ŒmCLIP + Gemini Pro Visionå®ç°äº†æœ€ä½³çš„BLEU-1ï¼ˆ5.34%ï¼‰å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ60.01%ï¼‰ï¼Œè€ŒAraCLIP + Qwen-VLè·å¾—äº†æœ€é«˜çš„LLM-judgeè¯„åˆ†ï¼ˆ36.33%ï¼‰ã€‚è¿™ç§å¯è§£é‡Šçš„æµç¨‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ–‡åŒ–è¿è´¯æ€§å’Œä¸Šä¸‹æ–‡å‡†ç¡®æ€§çš„é˜¿æ‹‰ä¼¯è¯­æè¿°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å›¾åƒæè¿°æ¨¡å‹ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„å›¾åƒæè¿°æ¨¡å‹ï¼Œé€šå¸¸ç¼ºä¹å¯è§£é‡Šæ€§ã€‚å®ƒä»¬é€šå¸¸æ˜¯ç«¯åˆ°ç«¯çš„é»‘ç›’æ¨¡å‹ï¼Œéš¾ä»¥ç†è§£æ¨¡å‹ç”Ÿæˆæè¿°çš„åŸå› ï¼Œä¹Ÿéš¾ä»¥ä¿è¯ç”Ÿæˆçš„æè¿°åœ¨æ–‡åŒ–ä¸Šæ˜¯è¿è´¯çš„ï¼Œå¹¶ä¸”åœ¨ä¸Šä¸‹æ–‡ä¸­æ˜¯å‡†ç¡®çš„ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæä¾›å¯è§£é‡Šæ€§ï¼Œå¹¶èƒ½ç”Ÿæˆæ›´ç¬¦åˆé˜¿æ‹‰ä¼¯è¯­æ–‡åŒ–å’Œè¯­å¢ƒçš„å›¾åƒæè¿°æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLCAPçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å›¾åƒæè¿°ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤šè¯­è¨€ç¼–ç å™¨ä»å›¾åƒä¸­æå–å¯è§£é‡Šçš„è§†è§‰æ¦‚å¿µï¼ˆå³æ ‡ç­¾ï¼‰ï¼›ç„¶åï¼Œå°†è¿™äº›è§†è§‰æ¦‚å¿µè½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯è¯­æç¤ºï¼Œå¹¶ä¸åŸå§‹å›¾åƒä¸€èµ·è¾“å…¥åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„å›¾åƒæè¿°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ˜¾å¼åœ°åˆ©ç”¨è§†è§‰æ¦‚å¿µï¼Œæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å…è®¸æ¨¡å‹æ›´å¥½åœ°ç†è§£å›¾åƒçš„å†…å®¹å’Œä¸Šä¸‹æ–‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLCAPæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) è§†è§‰æ¦‚å¿µæ£€ç´¢ï¼šä½¿ç”¨mCLIPã€AraCLIPå’ŒJina V4ä¸‰ä¸ªå¤šè¯­è¨€ç¼–ç å™¨ä»å›¾åƒä¸­æ£€ç´¢ç›¸å…³çš„è§†è§‰æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾æ¥è‡ªä¸€ä¸ªæ··åˆè¯æ±‡è¡¨ï¼Œè¯¥è¯æ±‡è¡¨ç”±è®­ç»ƒæè¿°è¯­å¥å’Œä»Visual Genomeæ•°æ®é›†ä¸­ç¿»è¯‘çš„é€šç”¨é¢†åŸŸæ ‡ç­¾ç»„æˆã€‚2) å›¾åƒæè¿°ç”Ÿæˆï¼šå°†æ£€ç´¢åˆ°çš„å‰kä¸ªæ ‡ç­¾è½¬æ¢ä¸ºæµç•…çš„é˜¿æ‹‰ä¼¯è¯­æç¤ºï¼Œå¹¶ä¸åŸå§‹å›¾åƒä¸€èµ·è¾“å…¥åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆQwen-VLæˆ–Gemini Pro Visionï¼‰ä¸­ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„å›¾åƒæè¿°ã€‚æ¡†æ¶é€šè¿‡ç»„åˆä¸åŒçš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå½¢æˆå¤šç§é…ç½®ï¼Œå¹¶è¿›è¡Œå®éªŒè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLCAPçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯è§£é‡Šçš„è§†è§‰æ¦‚å¿µé›†æˆæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å›¾åƒæè¿°æ¨¡å‹ä¸åŒï¼ŒVLCAPæ˜¾å¼åœ°åˆ©ç”¨è§†è§‰æ¦‚å¿µæ¥æŒ‡å¯¼æè¿°ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒçš„å†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆé˜¿æ‹‰ä¼¯è¯­æ–‡åŒ–å’Œè¯­å¢ƒçš„æè¿°ã€‚æ­¤å¤–ï¼ŒVLCAPè¿˜ä½¿ç”¨äº†å¤šç§å¤šè¯­è¨€ç¼–ç å™¨å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œä»è€Œæ‰¾åˆ°äº†æœ€ä½³çš„é…ç½®ã€‚

**å…³é”®è®¾è®¡**ï¼šVLCAPçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ··åˆè¯æ±‡è¡¨ï¼šè¯¥è¯æ±‡è¡¨åŒ…å«è®­ç»ƒæè¿°è¯­å¥å’Œä»Visual Genomeæ•°æ®é›†ä¸­ç¿»è¯‘çš„é€šç”¨é¢†åŸŸæ ‡ç­¾ï¼Œä»è€Œè¦†ç›–äº†æ›´å¹¿æ³›çš„è§†è§‰æ¦‚å¿µã€‚2) é˜¿æ‹‰ä¼¯è¯­æç¤ºè½¬æ¢ï¼šå°†æ£€ç´¢åˆ°çš„è§†è§‰æ ‡ç­¾è½¬æ¢ä¸ºæµç•…çš„é˜¿æ‹‰ä¼¯è¯­æç¤ºï¼Œä»è€Œæ›´å¥½åœ°å¼•å¯¼è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆæè¿°ã€‚3) å¤šç§ç¼–ç å™¨-è§£ç å™¨é…ç½®ï¼šé€šè¿‡ç»„åˆä¸åŒçš„ç¼–ç å™¨ï¼ˆmCLIPã€AraCLIPã€Jina V4ï¼‰å’Œè§£ç å™¨ï¼ˆQwen-VLã€Gemini Pro Visionï¼‰ï¼Œå½¢æˆå¤šç§é…ç½®ï¼Œå¹¶è¿›è¡Œå®éªŒè¯„ä¼°ï¼Œä»è€Œæ‰¾åˆ°æœ€ä½³çš„é…ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVLCAPæ¡†æ¶åœ¨é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…¶ä¸­ï¼ŒmCLIP + Gemini Pro Visioné…ç½®å®ç°äº†æœ€ä½³çš„BLEU-1ï¼ˆ5.34%ï¼‰å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ60.01%ï¼‰ï¼Œè€ŒAraCLIP + Qwen-VLé…ç½®è·å¾—äº†æœ€é«˜çš„LLM-judgeè¯„åˆ†ï¼ˆ36.33%ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLCAPæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€æ›´æµç•…ã€æ›´ç¬¦åˆé˜¿æ‹‰ä¼¯è¯­æ–‡åŒ–å’Œè¯­å¢ƒçš„å›¾åƒæè¿°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLCAPæ¡†æ¶å¯åº”ç”¨äºå¤šç§åœºæ™¯ï¼Œä¾‹å¦‚ï¼šé˜¿æ‹‰ä¼¯è¯­å›¾åƒæœç´¢å¼•æ“ã€é˜¿æ‹‰ä¼¯è¯­ç¤¾äº¤åª’ä½“å†…å®¹ç†è§£ã€é˜¿æ‹‰ä¼¯è¯­æ•™è‚²èµ„æºåˆ›å»ºç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡é˜¿æ‹‰ä¼¯è¯­å›¾åƒæè¿°çš„è´¨é‡å’Œå¯è§£é‡Šæ€§ï¼Œä¿ƒè¿›é˜¿æ‹‰ä¼¯è¯­æ–‡åŒ–å†…å®¹çš„ä¼ æ’­å’Œç†è§£ï¼Œå¹¶ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€çš„å›¾åƒæè¿°ç ”ç©¶æä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present VLCAP, an Arabic image captioning framework that integrates CLIP-based visual label retrieval with multimodal text generation. Rather than relying solely on end-to-end captioning, VLCAP grounds generation in interpretable Arabic visual concepts extracted with three multilingual encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label retrieval. A hybrid vocabulary is built from training captions and enriched with about 21K general domain labels translated from the Visual Genome dataset, covering objects, attributes, and scenes. The top-k retrieved labels are transformed into fluent Arabic prompts and passed along with the original image to vision-language models. In the second stage, we tested Qwen-VL and Gemini Pro Vision for caption generation, resulting in six encoder-decoder configurations. The results show that mCLIP + Gemini Pro Vision achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%). This interpretable pipeline enables culturally coherent and contextually accurate Arabic captions.

