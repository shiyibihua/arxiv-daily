---
layout: default
title: UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark
---

# UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24427" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24427v1</a>
  <a href="https://arxiv.org/pdf/2509.24427.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24427v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24427v1', 'UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ailing Zhang, Lina Lei, Dehong Kong, Zhixin Wang, Jiaqi Xu, Fenglong Song, Chun-Le Guo, Chang Liu, Fan Li, Jie Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UI2V-Benchï¼šæå‡ºä¸€ä¸ªåŸºäºç†è§£çš„å›¾ç”Ÿè§†é¢‘ç”Ÿæˆè¯„æµ‹åŸºå‡†ï¼Œå…³æ³¨è¯­ä¹‰ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾ç”Ÿè§†é¢‘` `è§†é¢‘ç”Ÿæˆ` `è¯­ä¹‰ç†è§£` `å¸¸è¯†æ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰I2Vè¯„ä¼°åŸºå‡†ä¾§é‡è§†é¢‘è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå¿½ç•¥äº†æ¨¡å‹å¯¹å›¾åƒè¯­ä¹‰çš„ç†è§£å’Œå¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚
2. UI2V-Benché€šè¿‡å¼•å…¥ç©ºé—´ç†è§£ã€å±æ€§ç»‘å®šã€ç±»åˆ«ç†è§£å’Œæ¨ç†å››ä¸ªç»´åº¦ï¼Œè¯„ä¼°I2Væ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚
3. è¯¥åŸºå‡†åŒ…å«500ä¸ªæ–‡æœ¬-å›¾åƒå¯¹ï¼Œå¹¶ä½¿ç”¨åŸºäºMLLMçš„è¯„ä¼°æ–¹æ³•ï¼Œä¸äººå·¥è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆæ‰©æ•£æ¨¡å‹å‘å±•è¿…é€Ÿï¼Œå¹¶åœ¨å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆé¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨è§†é¢‘è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹ç†è§£è¾“å…¥å›¾åƒä¸­ç‰¹å®šå¯¹è±¡è¯­ä¹‰ä»¥åŠç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†è§„å¾‹å’Œäººç±»å¸¸è¯†çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UI2V-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°I2Væ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨è¯­ä¹‰ç†è§£å’Œæ¨ç†ã€‚å®ƒå¼•å…¥äº†å››ä¸ªä¸»è¦çš„è¯„ä¼°ç»´åº¦ï¼šç©ºé—´ç†è§£ã€å±æ€§ç»‘å®šã€ç±»åˆ«ç†è§£å’Œæ¨ç†ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›ç»´åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¯„ä¼°æ–¹æ³•ï¼šç”¨äºç»†ç²’åº¦è¯­ä¹‰ç†è§£çš„å®ä¾‹çº§pipelineï¼Œä»¥åŠç”¨äºé€æ­¥å› æœè¯„ä¼°çš„åŸºäºåé¦ˆçš„æ¨ç†pipelineï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„è¯„ä¼°ã€‚UI2V-BenchåŒ…æ‹¬å¤§çº¦500ä¸ªç²¾å¿ƒæ„å»ºçš„æ–‡æœ¬-å›¾åƒå¯¹ï¼Œå¹¶è¯„ä¼°äº†ä¸€ç³»åˆ—å¼€æºå’Œé—­æºI2Væ¨¡å‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥çº³å…¥äº†äººå·¥è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ä¸æˆ‘ä»¬æå‡ºçš„åŸºäºMLLMçš„æŒ‡æ ‡é«˜åº¦ä¸€è‡´ã€‚æ€»çš„æ¥è¯´ï¼ŒUI2V-Benché€šè¿‡å¼ºè°ƒè¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¡«è¡¥äº†I2Vè¯„ä¼°ä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶å’Œæ•°æ®é›†ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶å’Œæ¨¡å‹å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è§†é¢‘è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šï¼Œç¼ºä¹å¯¹æ¨¡å‹ç†è§£è¾“å…¥å›¾åƒè¯­ä¹‰ä»¥åŠè¿›è¡Œå¸¸è¯†æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè¯„ä¼°ã€‚è¿™å¯¼è‡´æ¨¡å‹å¯èƒ½ç”Ÿæˆè§†è§‰ä¸Šæµç•…ä½†è¯­ä¹‰ä¸Šä¸åˆç†æˆ–è¿åç‰©ç†è§„å¾‹çš„è§†é¢‘ï¼Œé™åˆ¶äº†I2VæŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUI2V-Benchçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå…¨é¢è¯„ä¼°I2Væ¨¡å‹è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚é€šè¿‡å®šä¹‰å››ä¸ªå…³é”®ç»´åº¦ï¼ˆç©ºé—´ç†è§£ã€å±æ€§ç»‘å®šã€ç±»åˆ«ç†è§£å’Œæ¨ç†ï¼‰ï¼Œå¹¶è®¾è®¡åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥åŸºå‡†æ—¨åœ¨æ›´å‡†ç¡®åœ°è¡¡é‡æ¨¡å‹å¯¹å›¾åƒå†…å®¹çš„ç†è§£ç¨‹åº¦ä»¥åŠç”Ÿæˆç¬¦åˆå¸¸è¯†å’Œç‰©ç†è§„å¾‹è§†é¢‘çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUI2V-Benchçš„è¯„ä¼°æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é›†æ„å»ºï¼šåŒ…å«500ä¸ªç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬-å›¾åƒå¯¹ï¼Œæ¶µç›–å„ç§åœºæ™¯å’Œå¯¹è±¡ã€‚2) ç‰¹å¾æå–ï¼šä½¿ç”¨I2Væ¨¡å‹ç”Ÿæˆè§†é¢‘ã€‚3) ç»´åº¦è¯„ä¼°ï¼šé’ˆå¯¹ç©ºé—´ç†è§£ã€å±æ€§ç»‘å®šã€ç±»åˆ«ç†è§£å’Œæ¨ç†å››ä¸ªç»´åº¦ï¼Œåˆ†åˆ«è®¾è®¡è¯„ä¼°æ–¹æ³•ã€‚4) MLLMè¯„ä¼°ï¼šåˆ©ç”¨MLLMè¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼ŒåŒ…æ‹¬å®ä¾‹çº§pipelineå’Œåé¦ˆå¼æ¨ç†pipelineã€‚5) äººå·¥è¯„ä¼°ï¼šè¿›è¡Œäººå·¥è¯„ä¼°ï¼ŒéªŒè¯MLLMè¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šUI2V-Benchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¯„ä¼°ç»´åº¦å’Œè¯„ä¼°æ–¹æ³•ã€‚å®ƒé¦–æ¬¡å°†è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ä½œä¸ºI2Væ¨¡å‹è¯„ä¼°çš„é‡è¦æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†åŸºäºMLLMçš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´ç»†ç²’åº¦åœ°è¯„ä¼°æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åé¦ˆå¼æ¨ç†pipelineé€šè¿‡é€æ­¥å› æœè¯„ä¼°ï¼Œæé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šUI2V-Benchçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å››ä¸ªè¯„ä¼°ç»´åº¦çš„å®šä¹‰ï¼Œç¡®ä¿å…¨é¢è¦†ç›–è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚2) åŸºäºMLLMçš„å®ä¾‹çº§pipelineï¼Œç”¨äºç»†ç²’åº¦è¯­ä¹‰ç†è§£ã€‚3) åŸºäºMLLMçš„åé¦ˆå¼æ¨ç†pipelineï¼Œç”¨äºé€æ­¥å› æœè¯„ä¼°ã€‚4) æ•°æ®é›†çš„æ„å»ºï¼ŒåŒ…å«å¤šæ ·åŒ–çš„åœºæ™¯å’Œå¯¹è±¡ï¼Œä»¥æé«˜è¯„ä¼°çš„æ³›åŒ–èƒ½åŠ›ã€‚5) MLLMçš„é€‰æ‹©å’Œpromptçš„è®¾è®¡ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UI2V-Benchå¯¹å¤šä¸ªå¼€æºå’Œé—­æºI2Væ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œæ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚äººå·¥è¯„ä¼°ç»“æœä¸MLLMè¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ï¼ŒéªŒè¯äº†UI2V-Benchçš„æœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†ä¸ºæœªæ¥çš„I2Væ¨¡å‹ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯é çš„è¯„ä¼°å¹³å°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UI2V-Benchå¯åº”ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜åº¦è¯­ä¹‰ä¸€è‡´æ€§å’Œå¸¸è¯†æ¨ç†çš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚ï¼šè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿã€æ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®å†…å®¹ç”Ÿæˆã€æ•™è‚²è§†é¢‘åˆ¶ä½œç­‰ã€‚è¯¥åŸºå‡†èƒ½å¤Ÿæ¨åŠ¨I2VæŠ€æœ¯çš„å‘å±•ï¼Œä½¿å…¶ç”Ÿæˆçš„è§†é¢‘æ›´ç¬¦åˆäººç±»çš„é¢„æœŸå’Œç†è§£ï¼Œä»è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œåº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generative diffusion models are developing rapidly and attracting increasing attention due to their wide range of applications. Image-to-Video (I2V) generation has become a major focus in the field of video synthesis. However, existing evaluation benchmarks primarily focus on aspects such as video quality and temporal consistency, while largely overlooking the model's ability to understand the semantics of specific subjects in the input image or to ensure that the generated video aligns with physical laws and human commonsense. To address this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V models with a focus on semantic understanding and reasoning. It introduces four primary evaluation dimensions: spatial understanding, attribute binding, category understanding, and reasoning. To assess these dimensions, we design two evaluation methods based on Multimodal Large Language Models (MLLMs): an instance-level pipeline for fine-grained semantic understanding, and a feedback-based reasoning pipeline that enables step-by-step causal assessment for more accurate evaluation. UI2V-Bench includes approximately 500 carefully constructed text-image pairs and evaluates a range of both open source and closed-source I2V models across all defined dimensions. We further incorporate human evaluations, which show strong alignment with the proposed MLLM-based metrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by emphasizing semantic comprehension and reasoning ability, offering a robust framework and dataset to support future research and model development in the field.

