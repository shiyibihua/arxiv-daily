---
layout: default
title: VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning
---

# VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25033" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.25033v3</a>
  <a href="https://arxiv.org/pdf/2509.25033.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25033v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25033v3', 'VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-29 (Êõ¥Êñ∞: 2025-10-23)

**Â§áÊ≥®**: Accepted by NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/peacelwh/VT-FSL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VT-FSLÊ°ÜÊû∂ÔºåÂà©Áî®LLMÊ°•Êé•ËßÜËßâ‰∏éÊñáÊú¨ÔºåÊèêÂçáÂ∞èÊ†∑Êú¨Â≠¶‰π†ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â∞èÊ†∑Êú¨Â≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `ËßÜËßâÊñáÊú¨ËûçÂêà` `Âá†‰ΩïÂØπÈΩê` `Ëø≠‰ª£ÊèêÁ§∫` `ÂõæÂÉèÂêàÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ∞èÊ†∑Êú¨Â≠¶‰π†ÊñπÊ≥ï‰æùËµñËØ≠‰πâ‰ø°ÊÅØÂ¢ûÂº∫ÊîØÊåÅÁâπÂæÅÔºå‰ΩÜÊòì‰∫ßÁîü‰∏éËßÜËßâËØÅÊçÆÁüõÁõæÁöÑËØ≠‰πâÂπªËßâÔºåÂØºËá¥Âô™Â£∞ÊåáÂØºÂíåÈ´òÊòÇÁöÑ‰øÆÊ≠£ÊàêÊú¨„ÄÇ
2. VT-FSLÊ°ÜÊû∂Âà©Áî®LLMÁîüÊàêÁ≤æÁ°ÆÁöÑÁ±ªÊèèËø∞ÂíåÂêàÊàêÂõæÂÉèÔºåÂàÜÂà´‰Ωú‰∏∫ÊñáÊú¨ÂíåËßÜËßâÊèêÁ§∫ÔºåÂº•Ë°•ÊúâÈôêÊîØÊåÅÊï∞ÊçÆÂ∏¶Êù•ÁöÑ‰∏çË∂≥„ÄÇ
3. ÈÄöËøáË∑®Ê®°ÊÄÅÂá†‰ΩïÂØπÈΩêÔºåVT-FSLÊ°ÜÊû∂ËÉΩÂ§üÊçïËé∑ÂÖ®Â±ÄÂíåÈùûÁ∫øÊÄßÂÖ≥Á≥ªÔºåÂÆûÁé∞ÁªìÊûÑÂåñÂíå‰∏ÄËá¥ÁöÑÂ§öÊ®°ÊÄÅ‰ø°ÊÅØËûçÂêàÔºåÊòæËëóÊèêÂçáÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂç≥Âà©Áî®LLMÊ°•Êé•ËßÜËßâ‰∏éÊñáÊú¨ÁöÑÂ∞èÊ†∑Êú¨Â≠¶‰π†ÔºàVT-FSLÔºâ„ÄÇËØ•Ê°ÜÊû∂ÊûÑÂª∫‰∫ÜÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåÊîØÊåÅÂõæÂÉèÁöÑÁ≤æÁ°ÆË∑®Ê®°ÊÄÅÊèêÁ§∫ÔºåÂπ∂ÈÄöËøáÂá†‰ΩïÊÑüÁü•ÂØπÈΩêÊó†ÁºùÈõÜÊàêÂÆÉ‰ª¨„ÄÇVT-FSL‰∏ªË¶ÅÁî±Ë∑®Ê®°ÊÄÅËø≠‰ª£ÊèêÁ§∫ÔºàCIPÔºâÂíåË∑®Ê®°ÊÄÅÂá†‰ΩïÂØπÈΩêÔºàCGAÔºâÁªÑÊàê„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåCIP‰ª•Á±ªÂêçÂíåÊîØÊåÅÂõæÂÉè‰∏∫Êù°‰ª∂Ôºå‰ΩøLLMÂú®Âçï‰∏™ÁªìÊûÑÂåñÊé®ÁêÜËøáÁ®ã‰∏≠Ëø≠‰ª£ÁîüÊàêÁ≤æÁ°ÆÁöÑÁ±ªÊèèËø∞„ÄÇËøô‰∫õÊèèËø∞‰∏ç‰ªÖ‰∏∞ÂØå‰∫ÜÂØπÊñ∞Á±ªÁöÑËØ≠‰πâÁêÜËß£ÔºåËøòÂÆûÁé∞‰∫ÜËØ≠‰πâ‰∏ÄËá¥ÂõæÂÉèÁöÑÈõ∂Ê†∑Êú¨ÂêàÊàê„ÄÇÊèèËø∞ÂíåÂêàÊàêÂõæÂÉèÂàÜÂà´‰Ωú‰∏∫‰∫íË°•ÁöÑÊñáÊú¨ÂíåËßÜËßâÊèêÁ§∫ÔºåÊèê‰æõÈ´òÂ±ÇÊ¨°ÁöÑÁ±ªËØ≠‰πâÂíå‰ΩéÂ±ÇÊ¨°ÁöÑÁ±ªÂÜÖÂ§öÊ†∑ÊÄßÔºå‰ª•Âº•Ë°•ÊúâÈôêÁöÑÊîØÊåÅÊï∞ÊçÆ„ÄÇÊ≠§Â§ñÔºåCGAÈÄöËøáÊúÄÂ∞èÂåñÂÆÉ‰ª¨ÊâÄË∑®Ë∂äÁöÑ‰∏âÁª¥Âπ≥Ë°åÂ§öÈù¢‰ΩìÁöÑÊ†∏Âåñ‰ΩìÁßØÔºåËÅîÂêàÂØπÈΩêËûçÂêàÁöÑÊñáÊú¨„ÄÅÊîØÊåÅÂíåÂêàÊàêËßÜËßâË°®Á§∫„ÄÇÂÆÉÊçïËé∑ÊâÄÊúâË°®Á§∫‰πãÈó¥ÁöÑÂÖ®Â±ÄÂíåÈùûÁ∫øÊÄßÂÖ≥Á≥ªÔºå‰ªéËÄåÂÆûÁé∞ÁªìÊûÑÂåñÂíå‰∏ÄËá¥ÁöÑÂ§öÊ®°ÊÄÅÈõÜÊàê„ÄÇÊâÄÊèêÂá∫ÁöÑVT-FSLÊñπÊ≥ïÂú®ÂåÖÊã¨Ê†áÂáÜ„ÄÅË∑®ÂüüÂíåÁªÜÁ≤íÂ∫¶Â∞èÊ†∑Êú¨Â≠¶‰π†Âú∫ÊôØÂú®ÂÜÖÁöÑÂçÅ‰∏™‰∏çÂêåÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂ∞èÊ†∑Êú¨Â≠¶‰π†ÊñπÊ≥ïÂú®Âà©Áî®ËØ≠‰πâ‰ø°ÊÅØÂ¢ûÂº∫ÊîØÊåÅÁâπÂæÅÊó∂ÔºåÂÆπÊòì‰∫ßÁîü‰∏éÂÆûÈôÖËßÜËßâËØÅÊçÆÁõ∏ÊÇñÁöÑËØ≠‰πâÂπªËßâÔºå‰ªéËÄåÂØºËá¥Âô™Â£∞ÊåáÂØºÔºåÈúÄË¶Å‰ªòÂá∫È´òÊòÇÁöÑ‰øÆÊ≠£ÊàêÊú¨„ÄÇËøô‰∫õÊñπÊ≥ïÁº∫‰πèÂØπÂÆûÈôÖÂÆû‰æãÁöÑÊúâÊïà groundingÔºåÈöæ‰ª•ÂáÜÁ°ÆÊçïÊçâÊñ∞Á±ªÂà´ÁöÑÊú¨Ë¥®ÁâπÂæÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVT-FSLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂº∫Â§ßËØ≠‰πâÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÔºåÁîüÊàêÊõ¥Á≤æÁ°ÆÁöÑÁ±ªÊèèËø∞ÂíåÂêàÊàêÂõæÂÉèÔºå‰ªéËÄåÂº•Ë°•Â∞èÊ†∑Êú¨Â≠¶‰π†‰∏≠ÊîØÊåÅÊï∞ÊçÆ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂ∞ÜËßÜËßâ‰ø°ÊÅØÂíåÊñáÊú¨‰ø°ÊÅØÊúâÊïàÁªìÂêàÔºåÊèê‰æõÊõ¥ÂÖ®Èù¢ÂíåÂáÜÁ°ÆÁöÑÁ±ªÂà´‰ø°ÊÅØÔºåÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVT-FSLÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöË∑®Ê®°ÊÄÅËø≠‰ª£ÊèêÁ§∫ÔºàCIPÔºâÂíåË∑®Ê®°ÊÄÅÂá†‰ΩïÂØπÈΩêÔºàCGAÔºâ„ÄÇCIPÊ®°ÂùóÈ¶ñÂÖàÂà©Áî®LLMÔºå‰ª•Á±ªÂêçÂíåÊîØÊåÅÂõæÂÉè‰∏∫Êù°‰ª∂ÔºåËø≠‰ª£ÁîüÊàêÁ≤æÁ°ÆÁöÑÁ±ªÊèèËø∞ÂíåÂêàÊàêÂõæÂÉè„ÄÇËøô‰∫õÊèèËø∞ÂíåÂõæÂÉèÂàÜÂà´‰Ωú‰∏∫ÊñáÊú¨ÂíåËßÜËßâÊèêÁ§∫ÔºåÊèê‰æõÈ´òÂ±ÇÊ¨°ÁöÑÁ±ªËØ≠‰πâÂíå‰ΩéÂ±ÇÊ¨°ÁöÑÁ±ªÂÜÖÂ§öÊ†∑ÊÄß„ÄÇCGAÊ®°ÂùóÂàôË¥üË¥£Â∞ÜËûçÂêàÁöÑÊñáÊú¨„ÄÅÊîØÊåÅÂõæÂÉèÂíåÂêàÊàêÂõæÂÉèË°®Á§∫ËøõË°åÂØπÈΩêÔºåÈÄöËøáÊúÄÂ∞èÂåñÂÆÉ‰ª¨ÊâÄË∑®Ë∂äÁöÑ‰∏âÁª¥Âπ≥Ë°åÂ§öÈù¢‰ΩìÁöÑÊ†∏Âåñ‰ΩìÁßØÔºåÂÆûÁé∞ÁªìÊûÑÂåñÂíå‰∏ÄËá¥ÁöÑÂ§öÊ®°ÊÄÅÈõÜÊàê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVT-FSLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®LLMËøõË°åË∑®Ê®°ÊÄÅËø≠‰ª£ÊèêÁ§∫ÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑÊñáÊú¨ÊèèËø∞ÂíåÂêàÊàêÂõæÂÉèÔºå‰ªéËÄåÊúâÊïàÂ¢ûÂº∫‰∫ÜÂ∞èÊ†∑Êú¨Â≠¶‰π†‰∏≠ÁöÑÁ±ªÂà´Ë°®Á§∫„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåVT-FSLËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®LLMÁöÑËØ≠‰πâÁêÜËß£ËÉΩÂäõÔºåÁîüÊàêÊõ¥Á≤æÁ°ÆÂíå‰∏ÄËá¥ÁöÑÁ±ªÂà´‰ø°ÊÅØÔºåÈÅøÂÖç‰∫ÜËØ≠‰πâÂπªËßâÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåCGAÊ®°ÂùóÈÄöËøáÂá†‰ΩïÂØπÈΩêÁöÑÊñπÂºèÔºåÂÆûÁé∞‰∫ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊúâÊïàËûçÂêàÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCIPÊ®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éËø≠‰ª£ÊèêÁ§∫Á≠ñÁï•ÔºåÈÄöËøáÂ§öËΩÆËø≠‰ª£ÔºåÈÄêÊ≠•ÂÆåÂñÑÁ±ªÊèèËø∞ÂíåÂêàÊàêÂõæÂÉèÁöÑË¥®Èáè„ÄÇCGAÊ®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂà©Áî®Ê†∏Âåñ‰ΩìÁßØÊúÄÂ∞èÂåñÊù•ÂÆûÁé∞Â§öÊ®°ÊÄÅÂØπÈΩêÔºåËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊçïËé∑ÂÖ®Â±ÄÂíåÈùûÁ∫øÊÄßÂÖ≥Á≥ªÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÂ§öÊ®°ÊÄÅËûçÂêà„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°ÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞Ôºå‰ΩÜÊëòË¶Å‰∏≠Êú™ÊòéÁ°ÆÊèêÂèäÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VT-FSLÊñπÊ≥ïÂú®ÂçÅ‰∏™‰∏çÂêåÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂåÖÊã¨Ê†áÂáÜ„ÄÅË∑®ÂüüÂíåÁªÜÁ≤íÂ∫¶Â∞èÊ†∑Êú¨Â≠¶‰π†Âú∫ÊôØ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÂú®ËÆ∫Êñá‰∏≠ËØ¶ÁªÜÁªôÂá∫Ôºå‰ΩÜÊëòË¶Å‰∏≠Êú™Êèê‰æõÂÖ∑‰ΩìÊï∞ÂÄº„ÄÇ‰ª£Á†ÅÂ∑≤ÂºÄÊ∫êÔºåÊñπ‰æøÁ†îÁ©∂ËÄÖÂ§çÁé∞ÂíåËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VT-FSLÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂõæÂÉèËØÜÂà´„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅÂõæÂÉèÂàÜÁ±ªÁ≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÂú∫ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂåªÁñóÂΩ±ÂÉèÂàÜÊûê‰∏≠ÔºåÁΩïËßÅÁñæÁóÖÁöÑÁóÖ‰æãÊï∞ÊçÆÈÄöÂ∏∏ÈùûÂ∏∏ÊúâÈôêÔºåVT-FSLÂèØ‰ª•Â∏ÆÂä©ÂåªÁîüÊõ¥ÂáÜÁ°ÆÂú∞ËØäÊñ≠Ëøô‰∫õÁñæÁóÖ„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ËøòÂèØ‰ª•Â∫îÁî®‰∫éÊñ∞‰∫ßÂìÅËØÜÂà´„ÄÅÈáéÁîüÂä®Áâ©‰øùÊä§Á≠âÈ¢ÜÂüüÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at https://github.com/peacelwh/VT-FSL.

