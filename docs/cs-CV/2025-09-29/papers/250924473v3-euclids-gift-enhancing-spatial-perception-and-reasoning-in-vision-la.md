---
layout: default
title: Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks
---

# Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24473" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24473v3</a>
  <a href="https://arxiv.org/pdf/2509.24473.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24473v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24473v3', 'Euclid\'s Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-11-19)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://zgca-ai4edu.github.io/Euclids_Gift)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEuclid30Kæ•°æ®é›†å¹¶å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡å…¶ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `å‡ ä½•å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `ä»£ç†ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥è¿›è¡Œå½¢çŠ¶å˜æ¢ã€å…³ç³»æ¨ç†ç­‰ã€‚
2. å°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜æ±‚è§£ä½œä¸ºä»£ç†ä»»åŠ¡ï¼Œé€šè¿‡æ„å»ºEuclid30Kæ•°æ®é›†å¹¶è¿›è¡Œå¾®è°ƒï¼Œæå‡æ¨¡å‹ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„é›¶æ ·æœ¬æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´æ™ºèƒ½ï¼ŒåŒ…æ‹¬å½¢çŠ¶å¯è§†åŒ–ã€ç‰©ä½“æ—‹è½¬ã€å…³ç³»ä½ç½®åˆ¤æ–­å’Œæ•°é‡ä¼°è®¡ç­‰èƒ½åŠ›ï¼Œå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è€Œè¨€ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºå°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜æ±‚è§£ä½œä¸ºä»£ç†ä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåä¸ºEuclid30Kçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«çº¦3ä¸‡ä¸ªå¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å’Œåº”ç”¨æ¬§å‡ é‡Œå¾—åŸç†ï¼Œä½œè€…ä½¿ç”¨Group Relative Policy Optimization (GRPO) å¯¹æ¥è‡ªQwen2.5VLã€Qwen3VLå’ŒRoboBrain2.0ç³»åˆ—çš„ä¸ƒä¸ªæ¨¡å‹å˜ä½“ï¼ˆå‚æ•°èŒƒå›´3-72Bï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œä»è€Œæ¿€å‘æ¨¡å‹è¯†åˆ«å½¢çŠ¶ã€è®¡æ•°ã€å…³è”å®ä½“ï¼Œå¹¶ä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸç†æ‰§è¡Œå¤šæ­¥æ¼”ç»æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹åœ¨å››ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆSuper-CLEVRã€Omni3DBenchã€VSI-Benchå’ŒMindCubeï¼‰ä¸Šå®ç°äº†æ˜¾è‘—çš„é›¶æ ·æœ¬å¢ç›Šï¼Œè€Œæ— éœ€ä»»ä½•ç‰¹å®šäºä»»åŠ¡çš„è°ƒæ•´ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨Euclid30Kä¸Šè®­ç»ƒåï¼ŒVSI-Benchçš„å¹³å‡å‡†ç¡®ç‡ä»36.6%æé«˜åˆ°41.8%ï¼ˆ+5.2%ï¼‰ï¼ŒMindCubeçš„å¹³å‡å‡†ç¡®ç‡ä»31.4%æé«˜åˆ°38.1%ï¼ˆ+6.7%ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿæ€§ç ”ç©¶è¡¨æ˜ï¼Œä»¥å‡ ä½•ä¸ºä¸­å¿ƒçš„å¾®è°ƒå¯ä»¥èµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹å¹¿æ³›å¯è¿ç§»çš„ç©ºé—´æŠ€èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆè§£å†³æ¶‰åŠå‡ ä½•å½¢çŠ¶ã€ç©ºé—´å…³ç³»ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å‡ ä½•çŸ¥è¯†çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†ç©ºé—´ç›¸å…³ä»»åŠ¡æ—¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜æ±‚è§£ä½œä¸ºä¸€ç§ä»£ç†ä»»åŠ¡ï¼Œé€šè¿‡è®©æ¨¡å‹å­¦ä¹ è§£å†³å‡ ä½•é—®é¢˜æ¥æå‡å…¶ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•åŸºäºå‡ ä½•çŸ¥è¯†æ˜¯ç©ºé—´æ™ºèƒ½çš„åŸºç¡€ï¼Œé€šè¿‡å­¦ä¹ å‡ ä½•åŸç†å¯ä»¥æå‡æ¨¡å‹å¯¹ç©ºé—´å…³ç³»çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®æ„å»ºå’Œæ¨¡å‹å¾®è°ƒä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»ºåŒ…å«3ä¸‡ä¸ªå¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜çš„Euclid30Kæ•°æ®é›†ã€‚ç„¶åï¼Œä½¿ç”¨Group Relative Policy Optimization (GRPO) å¯¹Qwen2.5VLã€Qwen3VLå’ŒRoboBrain2.0ç­‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å­¦ä¹ å‡ ä½•çŸ¥è¯†å¹¶æå‡ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å‡ ä½•é—®é¢˜æ±‚è§£ä½œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä»£ç†ä»»åŠ¡ï¼Œé€šè¿‡å­¦ä¹ å‡ ä½•çŸ¥è¯†æ¥æå‡æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚ä¸ä»¥å¾€ç›´æ¥åœ¨ç‰¹å®šç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡çš„æ–¹æ³•æ›´æ³¨é‡å­¦ä¹ é€šç”¨çš„å‡ ä½•åŸç†ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šEuclid30Kæ•°æ®é›†åŒ…å«å¤šç§ç±»å‹çš„å‡ ä½•é—®é¢˜ï¼Œæ¶µç›–å¹³é¢å’Œç«‹ä½“å‡ ä½•ï¼Œæ—¨åœ¨å…¨é¢æå‡æ¨¡å‹çš„å‡ ä½•çŸ¥è¯†ã€‚GRPOç®—æ³•ç”¨äºå¾®è°ƒæ¨¡å‹ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ è¯†åˆ«å½¢çŠ¶ã€è®¡æ•°å’Œå…³è”å®ä½“ï¼Œå¹¶ä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸç†è¿›è¡Œå¤šæ­¥æ¼”ç»æ¨ç†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡åœ¨Euclid30Kæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹åœ¨VSI-Benchä¸Šçš„å¹³å‡å‡†ç¡®ç‡æå‡äº†5.2%ï¼ˆä»36.6%åˆ°41.8%ï¼‰ï¼Œåœ¨MindCubeä¸Šçš„å¹³å‡å‡†ç¡®ç‡æå‡äº†6.7%ï¼ˆä»31.4%åˆ°38.1%ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä¸”æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé¢å¤–è°ƒæ•´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€ä¸‰ç»´åœºæ™¯ç†è§£ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£å’Œäº¤äº’ï¼Œå®ç°æ›´æ™ºèƒ½åŒ–çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\% to 41.8\% (+5.2\%), and the mean MindCube accuracy rose from 31.4\% to 38.1\% (+6.7\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.

