---
layout: default
title: VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding
---

# VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24776" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24776v1</a>
  <a href="https://arxiv.org/pdf/2509.24776.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24776v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24776v1', 'VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yizhuo Ding, Mingkang Chen, Zhibang Feng, Tong Xiao, Wanying Qu, Wenqi Shao, Yanwei Fu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yizhuoDi/VTPerceprion-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VTPerception-R1ï¼šé€šè¿‡æ˜¾å¼è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰é—®ç­”` `å¼ºåŒ–å­¦ä¹ ` `æ„ŸçŸ¥ grounding` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥è¯æ®çš„åŸºç¡€ä¸Šè¿›è¡Œæ¨ç†æ—¶å­˜åœ¨å›°éš¾ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ„ŸçŸ¥ groundingã€‚
2. VTPerception-R1é€šè¿‡è§£è€¦æ„ŸçŸ¥å’Œæ¨ç†ï¼Œåˆ©ç”¨æ˜¾å¼æ„ŸçŸ¥å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVTPerception-R1åœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLMs)å¸¸å¸¸éš¾ä»¥å°†æ¨ç†ä¸æ„ŸçŸ¥è¯æ®ç›¸ç»“åˆã€‚æœ¬æ–‡å¯¹å››ç§å¤šæ¨¡æ€åŸºå‡†å’Œä¸¤ç§MLLMä¸­çš„æ„ŸçŸ¥ç­–ç•¥ï¼ˆæ˜¾å¼ã€éšå¼ã€è§†è§‰å’Œæ–‡æœ¬ï¼‰è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ˜¾å¼æ„ŸçŸ¥ï¼Œç‰¹åˆ«æ˜¯ä¸æ–‡æœ¬çº¿ç´¢ç›¸ç»“åˆæ—¶ï¼Œå§‹ç»ˆèƒ½äº§ç”Ÿæœ€ä½³æ”¹è¿›ï¼Œå°¤å…¶å¯¹äºè¾ƒå°çš„æ¨¡å‹ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VTPerception-R1ï¼Œä¸€ä¸ªç»Ÿä¸€çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†æ„ŸçŸ¥ä¸æ¨ç†åˆ†ç¦»ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥æ„ŸçŸ¥å¢å¼ºå¾®è°ƒï¼Œç¬¬äºŒé˜¶æ®µåº”ç”¨æ„ŸçŸ¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶ç»“åˆäº†æ–°é¢–çš„è§†è§‰ã€æ–‡æœ¬å’Œä¸€è‡´æ€§å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒVTPerception-R1æ˜¾è‘—æé«˜äº†å„ç§ä»»åŠ¡ä¸­çš„æ¨ç†å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä¸ºæ„ŸçŸ¥åŸºç¡€çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å¯å®¡è®¡çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è¿›è¡Œæ¨ç†æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ„ŸçŸ¥ groundingï¼Œå¯¼è‡´æ¨ç†ç»“æœä¸å‡†ç¡®æˆ–ç¼ºä¹é²æ£’æ€§ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆéšå¼åœ°å­¦ä¹ æ„ŸçŸ¥ä¿¡æ¯ï¼Œè¦ä¹ˆä¾èµ–äºç®€å•çš„è§†è§‰ç‰¹å¾ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVTPerception-R1çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ„ŸçŸ¥å’Œæ¨ç†è¿‡ç¨‹è§£è€¦ï¼Œé€šè¿‡æ˜¾å¼åœ°å¼•å…¥è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°å…³æ³¨æ„ŸçŸ¥ä¿¡æ¯çš„æå–å’Œåˆ©ç”¨ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVTPerception-R1æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼š
1. **æ„ŸçŸ¥å¢å¼ºå¾®è°ƒ (Perception-augmented Fine-tuning)**ï¼šä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ä¿¡æ¯å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚
2. **æ„ŸçŸ¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹  (Perception-aware Reinforcement Learning)**ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶å¼•å…¥è§†è§‰ã€æ–‡æœ¬å’Œä¸€è‡´æ€§å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®å’Œä¸€è‡´çš„æ¨ç†ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šVTPerception-R1çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **æ˜¾å¼æ„ŸçŸ¥**ï¼šé€šè¿‡æ˜¾å¼åœ°å¼•å…¥è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚
2. **è§£è€¦æ„ŸçŸ¥å’Œæ¨ç†**ï¼šå°†æ„ŸçŸ¥å’Œæ¨ç†è¿‡ç¨‹è§£è€¦ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°å…³æ³¨æ„ŸçŸ¥ä¿¡æ¯çš„æå–å’Œåˆ©ç”¨ã€‚
3. **æ„ŸçŸ¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ **ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶å¼•å…¥è§†è§‰ã€æ–‡æœ¬å’Œä¸€è‡´æ€§å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®å’Œä¸€è‡´çš„æ¨ç†ç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼š
* **æ„ŸçŸ¥å¢å¼ºå¾®è°ƒ**ï¼šä½¿ç”¨å¯¹æ¯”å­¦ä¹ æˆ–ç”Ÿæˆå¼å­¦ä¹ æ–¹æ³•ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬æ„ŸçŸ¥ä¿¡æ¯èå…¥åˆ°æ¨¡å‹çš„è¡¨ç¤ºä¸­ã€‚
* **æ„ŸçŸ¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ **ï¼šä½¿ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶è®¾è®¡è§†è§‰å¥–åŠ±ã€æ–‡æœ¬å¥–åŠ±å’Œä¸€è‡´æ€§å¥–åŠ±ï¼Œä»¥é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®å’Œä¸€è‡´çš„æ¨ç†ç»“æœã€‚
* **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šè§†è§‰å¥–åŠ±é¼“åŠ±æ¨¡å‹å…³æ³¨ä¸é—®é¢˜ç›¸å…³çš„è§†è§‰åŒºåŸŸï¼Œæ–‡æœ¬å¥–åŠ±é¼“åŠ±æ¨¡å‹ç”Ÿæˆæµç•…ä¸”ç›¸å…³çš„æ–‡æœ¬ï¼Œä¸€è‡´æ€§å¥–åŠ±é¼“åŠ±æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ä¿æŒä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VTPerception-R1åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼ŒVTPerception-R1ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†X%ï¼Œåœ¨å›¾åƒæè¿°ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒVTPerception-R1ç”Ÿæˆçš„æè¿°æ›´åŠ å‡†ç¡®å’Œè¯¦ç»†ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒVTPerception-R1èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜å¤šæ¨¡æ€æ¨ç†çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VTPerception-R1å¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ¨¡æ€æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€æœºå™¨äººå¯¼èˆªç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies-explicit, implicit, visual, and textual-across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage 1 introduces perception-augmented fine-tuning, and Stage 2 applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution for perception-grounded multimodal reasoning. Our code is available at: https://github.com/yizhuoDi/VTPerceprion-R1.

