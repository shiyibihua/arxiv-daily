---
layout: default
title: Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation
---

# Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24739" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.24739v2</a>
  <a href="https://arxiv.org/pdf/2509.24739.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24739v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24739v2', 'Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Huu Tien Nguyen, Dac Thai Nguyen, The Minh Duc Nguyen, Trung Thanh Nguyen, Thao Nguyen Truong, Huy Hieu Pham, Johan Barthelemy, Minh Quan Tran, Thanh Tam Nguyen, Quoc Viet Hung Nguyen, Quynh Anh Chau, Hong Son Mai, Thanh Trung Nguyen, Phi Le Nguyen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-29 (Êõ¥Êñ∞: 2025-10-23)

**Â§áÊ≥®**: 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/AIoT-Lab-BKAI/ViPET-ReportGen)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ViPET-ReportGenÊï∞ÊçÆÈõÜ‰∏éÂü∫ÂáÜÔºåÁî®‰∫éÊèêÂçáË∂äÂçóËØ≠PET/CTÊä•ÂëäÁîüÊàêÁöÑËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°ÂûãÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `ÂåªÂ≠¶ÂΩ±ÂÉè` `PET/CT` `Êä•ÂëäÁîüÊàê` `Ë∂äÂçóËØ≠` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `‰ΩéËµÑÊ∫êËØ≠Ë®Ä` `Êï∞ÊçÆÈõÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂåªÂ≠¶ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁº∫‰πèÂØπPET/CTÁ≠âÂäüËÉΩÊàêÂÉèÊ®°ÊÄÅÁöÑÊîØÊåÅÔºå‰∏î‰∏ªË¶ÅÈõÜ‰∏≠Âú®È´òËµÑÊ∫êËØ≠Ë®Ä‰∏äÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏¥Â∫ä‰∏äÁöÑÂ∫îÁî®„ÄÇ
2. ËÆ∫ÊñáÊûÑÂª∫‰∫ÜÂåÖÂê´2757‰∏™Ë∂äÂçóËØ≠PET/CTÂõæÂÉèÂèäÂÖ∂ÂØπÂ∫îÊä•ÂëäÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â¢ûÂº∫VLMsÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂåÖÊã¨Êï∞ÊçÆÂ¢ûÂº∫Âíå‰∏ìÂÆ∂È™åËØÅ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ÜËØ•Êï∞ÊçÆÈõÜÊï¥ÂêàÂà∞Áé∞ÊúâVLMs‰∏≠ÔºåËÉΩÂ§üÊòæËëóÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºå‰∏∫‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûêÊèê‰æõÊúâÂäõÊîØÊåÅ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°Âûã(VLMs)ÈÄöËøáÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÔºåÂú®‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÂÆûÁé∞‰∫Ü‰∏∞ÂØåÁöÑË∑®Ê®°ÊÄÅÊé®ÁêÜ„ÄÇÂ∞ΩÁÆ°ÂÆÉ‰ª¨Âú®ÈÄöÁî®È¢ÜÂüüÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÁî±‰∫éÁº∫‰πèÂ§öÊ†∑ÂåñÁöÑÊàêÂÉèÊ®°ÂºèÂíåÂ§öËØ≠Ë®Ä‰∏¥Â∫äÊï∞ÊçÆÔºåÂ∞ÜËøô‰∫õÊ®°ÂûãÂ∫îÁî®‰∫éÂåªÂ≠¶ÊàêÂÉè‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁé∞ÊúâÁöÑÂ§ßÂ§öÊï∞ÂåªÂ≠¶VLMsÈÉΩÂú®ÊàêÂÉèÊ®°ÂºèÁöÑÂ≠êÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂‰∏î‰∏ªË¶ÅÂÖ≥Ê≥®È´òËµÑÊ∫êËØ≠Ë®ÄÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÊ≥õÂåñÊÄßÂíå‰∏¥Â∫äÂÆûÁî®ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑË∂äÂçóËØ≠Â§öÊ®°ÊÄÅÂåªÂ≠¶Êï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Êù•Ëá™Áã¨Á´ãÊÇ£ËÄÖÁöÑ2,757‰∏™ÂÖ®Ë∫´PET/CTÂÆπÁßØÂèäÂÖ∂Áõ∏Â∫îÁöÑÂÆåÊï¥‰∏¥Â∫äÊä•Âëä„ÄÇËØ•Êï∞ÊçÆÈõÜÊó®Âú®Â°´Ë°•ÂåªÂ≠¶AIÂèëÂ±ï‰∏≠ÁöÑ‰∏§‰∏™Á¥ßËø´Áº∫Âè£Ôºö(1)Áé∞ÊúâVLMsËÆ≠ÁªÉËØ≠ÊñôÂ∫ì‰∏≠Áº∫‰πèPET/CTÊàêÂÉèÊï∞ÊçÆÔºåËøôÈòªÁ¢ç‰∫ÜËÉΩÂ§üÂ§ÑÁêÜÂäüËÉΩÊàêÂÉè‰ªªÂä°ÁöÑÊ®°ÂûãÁöÑÂºÄÂèëÔºõ(2)‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºåÁâπÂà´ÊòØË∂äÂçóËØ≠ÔºåÂú®ÂåªÂ≠¶ËßÜËßâ-ËØ≠Ë®ÄÁ†îÁ©∂‰∏≠‰ª£Ë°®ÊÄß‰∏çË∂≥„ÄÇÊçÆÊàë‰ª¨ÊâÄÁü•ÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Êèê‰æõÂÖ®Èù¢ÁöÑË∂äÂçóËØ≠PET/CT-Êä•ÂëäÂØπÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ËÆ≠ÁªÉÊ°ÜÊû∂Êù•Â¢ûÂº∫VLMsÁöÑÂ≠¶‰π†ÔºåÂåÖÊã¨Êï∞ÊçÆÂ¢ûÂº∫Âíå‰∏ìÂÆ∂È™åËØÅÁöÑÊµãËØïÈõÜ„ÄÇÊàë‰ª¨ËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûÈ™åÔºåÂØπÊúÄÂÖàËøõÁöÑVLMsÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏äËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁªìÂêàÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÂèØ‰ª•ÊòæËëóÊèêÈ´òÁé∞ÊúâVLMsÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨Áõ∏‰ø°Ëøô‰∏™Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÂ∞ÜÊàê‰∏∫Êé®Âä®ÂåªÂ≠¶ÊàêÂÉèÈ¢ÜÂüüÊõ¥Âº∫Â§ßÁöÑVLMsÂèëÂ±ïÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÊ≠•È™§ÔºåÁâπÂà´ÊòØÂØπ‰∫é‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂíåË∂äÂçóÂåªÁñó‰øùÂÅ•ÁöÑ‰∏¥Â∫äÂ∫îÁî®„ÄÇÊ∫ê‰ª£Á†ÅÂèØÂú®https://github.com/AIoT-Lab-BKAI/ViPET-ReportGenËé∑Âæó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂåªÂ≠¶ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§ÑÁêÜPET/CTÂõæÂÉèÔºåÁâπÂà´ÊòØÁªìÂêàË∂äÂçóËØ≠Êä•ÂëäÁîüÊàêÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÁº∫‰πèÂ§ßËßÑÊ®°ÁöÑPET/CTÂõæÂÉè-Ë∂äÂçóËØ≠Êä•ÂëäÈÖçÂØπÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäÈíàÂØπ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑVLMËÆ≠ÁªÉ‰ºòÂåñÁ≠ñÁï•„ÄÇËøôÈôêÂà∂‰∫ÜVLMsÂú®Ë∂äÂçóÂåªÁñóÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®ÔºåÈòªÁ¢ç‰∫ÜÂØπPET/CTÂõæÂÉèÁöÑÊ∑±ÂÖ•ÁêÜËß£ÂíåÊä•ÂëäÁîüÊàêËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™È´òË¥®ÈáèÁöÑË∂äÂçóËØ≠PET/CTÂõæÂÉè-Êä•ÂëäÊï∞ÊçÆÈõÜÔºåÂπ∂ËÆæËÆ°Áõ∏Â∫îÁöÑËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰ª•ÊèêÂçáVLMsÂú®PET/CTÊä•ÂëäÁîüÊàê‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÊèê‰æõÂÖÖË∂≥ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂ÁªìÂêàÊï∞ÊçÆÂ¢ûÂº∫Á≠âÊäÄÊúØÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Â≠¶‰π†PET/CTÂõæÂÉè‰∏éË∂äÂçóËØ≠Êä•Âëä‰πãÈó¥ÁöÑÂÖ≥ËÅîÔºå‰ªéËÄåÊèêÈ´òÊä•ÂëäÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåÊµÅÁïÖÊÄß„ÄÇÈÄâÊã©Ë∂äÂçóËØ≠ÊòØÂõ†‰∏∫ÂÖ∂‰Ωú‰∏∫‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºåÂú®ÂåªÂ≠¶VLMÈ¢ÜÂüüÁöÑÁ†îÁ©∂Áõ∏ÂØπÂåÆ‰πè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´Êï∞ÊçÆÊî∂ÈõÜ‰∏éÊ†áÊ≥®„ÄÅÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ‰∏éÂ¢ûÂº∫„ÄÅÊ®°ÂûãËÆ≠ÁªÉ‰∏éËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÈ¶ñÂÖàÔºåÊî∂ÈõÜ2757‰∏™PET/CTÂõæÂÉèÂèäÂÖ∂ÂØπÂ∫îÁöÑË∂äÂçóËØ≠Êä•ÂëäÔºåÂπ∂ËøõË°åÊ∏ÖÊ¥óÂíåÊ†áÊ≥®„ÄÇÁÑ∂ÂêéÔºåÈááÁî®Êï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÔºåÂ¶ÇÂõæÂÉèÊóãËΩ¨„ÄÅÁº©Êîæ„ÄÅË£ÅÂâ™Á≠âÔºåÂ¢ûÂä†Êï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß„ÄÇÊúÄÂêéÔºåÈÄâÊã©ÂêàÈÄÇÁöÑVLMÊû∂ÊûÑÔºåÂ¶ÇCLIPÊàñÁ±ª‰ººÊ®°ÂûãÔºåÂπ∂Âú®ÊûÑÂª∫ÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºå‰ΩøÁî®‰∏ìÂÆ∂È™åËØÅÁöÑÊµãËØïÈõÜËøõË°åËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫ÜÈ¶ñ‰∏™Â§ßËßÑÊ®°ÁöÑË∂äÂçóËØ≠PET/CTÂõæÂÉè-Êä•ÂëäÈÖçÂØπÊï∞ÊçÆÈõÜViPET-ReportGen„ÄÇËØ•Êï∞ÊçÆÈõÜÂ°´Ë°•‰∫ÜÂåªÂ≠¶VLMÈ¢ÜÂüüÂú®PET/CTÊàêÂÉèÂíå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÊñπÈù¢ÁöÑÁ©∫ÁôΩ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈíàÂØπËØ•Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂåÖÊã¨Êï∞ÊçÆÂ¢ûÂº∫Âíå‰∏ìÂÆ∂È™åËØÅÁöÑÊµãËØïÈõÜÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÂåÖÊã¨ÂõæÂÉèÁöÑÈöèÊú∫ÊóãËΩ¨Ôºà-15¬∞Âà∞15¬∞Ôºâ„ÄÅÁº©ÊîæÔºà0.8Âà∞1.2ÂÄçÔºâ„ÄÅÂπ≥ÁßªÔºà-10%Âà∞10%Ôºâ‰ª•ÂèäÈöèÊú∫Ë£ÅÂâ™„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÊàñÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÔºåÂÖ∑‰ΩìÂèñÂÜ≥‰∫éÊâÄÈÄâÊã©ÁöÑVLMÊû∂ÊûÑ„ÄÇÊ®°ÂûãËÆ≠ÁªÉÈááÁî®AdamW‰ºòÂåñÂô®ÔºåÂ≠¶‰π†ÁéáËÆæÁΩÆ‰∏∫1e-4ÔºåÂπ∂‰ΩøÁî®‰ΩôÂº¶ÈÄÄÁÅ´Á≠ñÁï•ËøõË°åÂ≠¶‰π†ÁéáË°∞Âáè„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåÂõæÂÉèÁºñÁ†ÅÂô®ÂèØ‰ª•‰ΩøÁî®ResNetÊàñVision TransformerÔºåÊñáÊú¨ÁºñÁ†ÅÂô®ÂèØ‰ª•‰ΩøÁî®BERTÊàñÁ±ª‰ººTransformerÊ®°Âûã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®ViPET-ReportGenÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑVLMsÔºåÂú®PET/CTÊä•ÂëäÁîüÊàê‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºå‰∏éÂú®ÈÄöÁî®Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÂü∫Á∫øÊ®°ÂûãÁõ∏ÊØîÔºå‰ΩøÁî®ËØ•Êï∞ÊçÆÈõÜËøõË°åÂæÆË∞ÉÁöÑÊ®°ÂûãÂú®BLEU„ÄÅROUGEÁ≠âÊåáÊ†á‰∏äÂùáÊúâÊòéÊòæÊèêÈ´òÔºå‰æãÂ¶ÇBLEU-4ÊèêÂçá‰∫ÜÁ∫¶10%„ÄÇËøôËØÅÊòé‰∫ÜËØ•Êï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄßÂíå‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éË∂äÂçóÂåªÁñóÂΩ±ÂÉèËØäÊñ≠È¢ÜÂüüÔºåËæÖÂä©ÂåªÁîüËøõË°åPET/CTÂõæÂÉèÁöÑÂàÜÊûêÂíåÊä•ÂëäÁîüÊàêÔºåÊèêÈ´òËØäÊñ≠ÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇÊú™Êù•ÔºåËØ•Êï∞ÊçÆÈõÜÂíåÊ®°ÂûãÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê‰ªªÂä°‰∏≠Ôºå‰øÉËøõÂÖ®ÁêÉÂåªÁñóAIÁöÑÂèëÂ±ïÔºåÂπ∂‰∏∫ÊÇ£ËÄÖÊèê‰æõÊõ¥‰ºòË¥®ÁöÑÂåªÁñóÊúçÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Foundation Models (VLMs), trained on large-scale multimodal datasets, have driven significant advances in Artificial Intelligence (AI) by enabling rich cross-modal reasoning. Despite their success in general domains, applying these models to medical imaging remains challenging due to the limited availability of diverse imaging modalities and multilingual clinical data. Most existing medical VLMs are trained on a subset of imaging modalities and focus primarily on high-resource languages, thus limiting their generalizability and clinical utility. To address these limitations, we introduce a novel Vietnamese-language multimodal medical dataset consisting of 2,757 whole-body PET/CT volumes from independent patients and their corresponding full-length clinical reports. This dataset is designed to fill two pressing gaps in medical AI development: (1) the lack of PET/CT imaging data in existing VLMs training corpora, which hinders the development of models capable of handling functional imaging tasks; and (2) the underrepresentation of low-resource languages, particularly the Vietnamese language, in medical vision-language research. To the best of our knowledge, this is the first dataset to provide comprehensive PET/CT-report pairs in Vietnamese. We further introduce a training framework to enhance VLMs' learning, including data augmentation and expert-validated test sets. We conduct comprehensive experiments benchmarking state-of-the-art VLMs on downstream tasks. The experimental results show that incorporating our dataset significantly improves the performance of existing VLMs. We believe this dataset and benchmark will serve as a pivotal step in advancing the development of more robust VLMs for medical imaging, especially for low-resource languages and clinical use in Vietnamese healthcare. The source code is available at https://github.com/AIoT-Lab-BKAI/ViPET-ReportGen.

