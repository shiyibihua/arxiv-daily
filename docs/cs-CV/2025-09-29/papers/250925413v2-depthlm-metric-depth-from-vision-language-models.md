---
layout: default
title: DepthLM: Metric Depth From Vision Language Models
---

# DepthLM: Metric Depth From Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25413" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25413v2</a>
  <a href="https://arxiv.org/pdf/2509.25413.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25413v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25413v2', 'DepthLM: Metric Depth From Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhipeng Cai, Ching-Feng Yeh, Hu Xu, Zhuang Liu, Gregory Meyer, Xinjie Lei, Changsheng Zhao, Shang-Wen Li, Vikas Chandra, Yangyang Shi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DepthLMï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å®ç°åº¦é‡æ·±åº¦ä¼°è®¡ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æŸå¤±å‡½æ•°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åº¦é‡æ·±åº¦ä¼°è®¡` `æ–‡æœ¬ç›‘ç£å¾®è°ƒ` `è§†è§‰æç¤º` `å†…åœ¨æ¡ä»¶å¢å¼º` `ä¸‰ç»´ç†è§£` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3Dç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åº¦é‡æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼Œéœ€è¦ä¸“é—¨çš„æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚
2. DepthLMé€šè¿‡æ–‡æœ¬ç›‘ç£å¾®è°ƒVLMï¼Œåˆ©ç”¨è§†è§‰æç¤ºå’Œå†…åœ¨æ¡ä»¶å¢å¼ºè§£å†³åƒç´ å‚è€ƒå’Œç›¸æœºæ­§ä¹‰é—®é¢˜ï¼Œæå‡æ·±åº¦ä¼°è®¡ç²¾åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDepthLMåœ¨åº¦é‡æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰VLMï¼Œå¹¶ä¸çº¯è§†è§‰æ¨¡å‹ç›¸å½“ï¼Œä¸”é¿å…äº†è¿‡åº¦å¹³æ»‘é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)å¯ä»¥é€šè¿‡æ–‡æœ¬äº¤äº’çµæ´»åœ°å¤„ç†å„ç§è§†è§‰ä»»åŠ¡ã€‚å°½ç®¡åœ¨è¯­ä¹‰ç†è§£æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åŒ…æ‹¬GPT-5åœ¨å†…çš„æœ€å…ˆè¿›çš„VLMåœ¨ç†è§£2Dè¾“å…¥çš„3Dä¿¡æ¯æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œçº¯è§†è§‰æ¨¡å‹åœ¨åº¦é‡æ·±åº¦ä¼°è®¡è¿™ä¸€å…³é”®çš„3Dç†è§£ä»»åŠ¡ä¸­è¾¾åˆ°äº†è¶…äººçš„ç²¾åº¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦ç‰¹å®šäºä»»åŠ¡çš„æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚è¿™ç§å·®å¼‚ä¿ƒä½¿æˆ‘ä»¬æ€è€ƒï¼šVLMæ˜¯å¦å¯ä»¥åœ¨ä¸æ”¹å˜æ¶æ„æˆ–æŸå¤±å‡½æ•°çš„æƒ…å†µä¸‹è¾¾åˆ°ä¸“å®¶çº§çš„ç²¾åº¦ï¼Ÿæˆ‘ä»¬ä»¥é€åƒç´ åº¦é‡æ·±åº¦ä¼°è®¡ä½œä¸ºä»£è¡¨æ€§ä»»åŠ¡ï¼Œå¹¶è¡¨æ˜ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå…¨é¢çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ–‡æœ¬çš„ç¨€ç–æ ‡ç­¾ç›‘ç£å¾®è°ƒè¶³ä»¥è®©VLMé‡Šæ”¾å¼ºå¤§çš„3Dç†è§£èƒ½åŠ›ï¼Œä¸éœ€è¦å¯†é›†çš„é¢„æµ‹å¤´æˆ–å¤æ‚çš„å›å½’/æ­£åˆ™åŒ–æŸå¤±ã€‚VLMçš„ç“¶é¢ˆå®é™…ä¸Šåœ¨äºåƒç´ å‚è€ƒå’Œè·¨æ•°æ®é›†ç›¸æœºæ­§ä¹‰ï¼Œæˆ‘ä»¬é€šè¿‡è§†è§‰æç¤ºå’Œå†…åœ¨æ¡ä»¶å¢å¼ºæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„DepthLMæ–¹æ³•ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼Œå…¶ç²¾åº¦è¶…è¿‡äº†å¤§å¤šæ•°å…ˆè¿›çš„VLM 2å€ä»¥ä¸Šï¼Œé¦–æ¬¡ä½¿VLMä¸çº¯è§†è§‰æ¨¡å‹ç›¸åª²ç¾ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰æ˜ç¡®å¼ºåˆ¶æ‰§è¡Œçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨DepthLMè®­ç»ƒçš„VLMè‡ªç„¶åœ°é¿å…äº†è¿‡åº¦å¹³æ»‘ï¼Œä¸çº¯è§†è§‰æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è¾¹ç•ŒåŒºåŸŸçš„æ‚¬æµ®ç‚¹è¦å°‘å¾—å¤šã€‚DepthLMçš„ç®€å•æ€§è¿˜ä½¿å•ä¸ªVLMèƒ½å¤Ÿè¦†ç›–åº¦é‡æ·±åº¦ä¹‹å¤–çš„å„ç§3Dä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†åœ¨ä»¥ä¸‹é“¾æ¥å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨åº¦é‡æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚ç°æœ‰VLMè™½ç„¶åœ¨è¯­ä¹‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£2Då›¾åƒçš„3Dä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ç²¾ç¡®çš„åº¦é‡æ·±åº¦æ–¹é¢ï¼Œä»ç„¶è½åäºä¸“é—¨çš„çº¯è§†è§‰æ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡å¤æ‚çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ï¼Œç¼ºä¹é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œé€šè¿‡ç®€å•çš„æ–‡æœ¬ç›‘ç£å¾®è°ƒï¼Œå³å¯ä½¿VLMå…·å¤‡å¼ºå¤§çš„3Dç†è§£èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¿®æ”¹VLMçš„æ¶æ„æˆ–å¼•å…¥å¤æ‚çš„æŸå¤±å‡½æ•°ã€‚å…³é”®åœ¨äºè§£å†³VLMåœ¨åƒç´ å‚è€ƒå’Œè·¨æ•°æ®é›†ç›¸æœºå‚æ•°ä¸Šçš„æ­§ä¹‰æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDepthLMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„VLMä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚2) åˆ©ç”¨ç¨€ç–çš„æ·±åº¦æ ‡ç­¾ï¼Œé€šè¿‡æ–‡æœ¬ç›‘ç£å¾®è°ƒVLMã€‚3) å¼•å…¥è§†è§‰æç¤ºï¼ˆVisual Promptingï¼‰æ¥å¸®åŠ©VLMæ›´å¥½åœ°ç†è§£åƒç´ çº§åˆ«çš„æ·±åº¦ä¿¡æ¯ã€‚4) ä½¿ç”¨å†…åœ¨æ¡ä»¶å¢å¼ºï¼ˆIntrinsic-Conditioned Augmentationï¼‰æ¥è§£å†³è·¨æ•°æ®é›†çš„ç›¸æœºå‚æ•°æ­§ä¹‰é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šDepthLMæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºï¼Œå®ƒè¯æ˜äº†é€šè¿‡ç®€å•çš„æ–‡æœ¬ç›‘ç£å¾®è°ƒï¼Œå³å¯è§£é”VLMåœ¨åº¦é‡æ·±åº¦ä¼°è®¡æ–¹é¢çš„æ½œåŠ›ï¼Œè€Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹æˆ–æŸå¤±å‡½æ•°è®¾è®¡ã€‚æ­¤å¤–ï¼Œè§†è§‰æç¤ºå’Œå†…åœ¨æ¡ä»¶å¢å¼ºæœ‰æ•ˆåœ°è§£å†³äº†VLMåœ¨åƒç´ å‚è€ƒå’Œç›¸æœºå‚æ•°æ–¹é¢çš„å›ºæœ‰é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§†è§‰æç¤ºæ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†å¯å­¦ä¹ çš„prompt tokensï¼Œæ·»åŠ åˆ°è¾“å…¥å›¾åƒçš„embeddingä¸­ï¼Œå¼•å¯¼VLMå…³æ³¨ä¸æ·±åº¦ç›¸å…³çš„ç‰¹å¾ã€‚åœ¨å†…åœ¨æ¡ä»¶å¢å¼ºæ–¹é¢ï¼Œå¯èƒ½é€šè¿‡å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œç›¸æœºå‚æ•°çš„å½’ä¸€åŒ–æˆ–æ•°æ®å¢å¼ºï¼Œæ¥å‡å°‘è·¨æ•°æ®é›†çš„ç›¸æœºå‚æ•°å·®å¼‚ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯èƒ½ä½¿ç”¨äº†ç®€å•çš„L1æˆ–L2æŸå¤±ï¼Œç›´æ¥å›å½’é¢„æµ‹çš„æ·±åº¦å€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DepthLMåœ¨åº¦é‡æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„VLM 2å€ä»¥ä¸Šï¼Œå¹¶ä¸”ä¸çº¯è§†è§‰æ¨¡å‹è¾¾åˆ°äº†å¯æ¯”çš„ç²¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒDepthLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰æ˜ç¡®å¼ºåˆ¶æ‰§è¡Œçš„æƒ…å†µä¸‹ï¼Œè‡ªç„¶åœ°é¿å…äº†è¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œåœ¨è¾¹ç•ŒåŒºåŸŸçš„æ‚¬æµ®ç‚¹æ˜æ˜¾å°‘äºçº¯è§†è§‰æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DepthLMçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€ä¸‰ç»´é‡å»ºã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºï¼Œå®ƒæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½¿VLMèƒ½å¤Ÿç†è§£3Dåœºæ™¯ï¼Œä»è€Œä¿ƒè¿›äº†VLMåœ¨3Dè§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼ŒDepthLMå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–3Dä»»åŠ¡ï¼Œä¾‹å¦‚åœºæ™¯ç†è§£ã€ç‰©ä½“è¯†åˆ«å’Œå§¿æ€ä¼°è®¡ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision language models (VLMs) can flexibly address various vision tasks through text interactions. Although successful in semantic understanding, state-of-the-art VLMs including GPT-5 still struggle in understanding 3D from 2D inputs. On the other hand, expert pure vision models achieve super-human accuracy in metric depth estimation, a key 3D understanding task. However, they require task-specific architectures and losses. Such difference motivates us to ask: Can VLMs reach expert-level accuracy without architecture or loss change? We take per-pixel metric depth estimation as the representative task and show that the answer is yes! Surprisingly, comprehensive analysis shows that text-based supervised-finetuning with sparse labels is sufficient for VLMs to unlock strong 3D understanding, no dense prediction head or complex regression/regularization loss is needed. The bottleneck for VLMs lies actually in pixel reference and cross-dataset camera ambiguity, which we address through visual prompting and intrinsic-conditioned augmentation. With much smaller models, our method DepthLM surpasses the accuracy of most advanced VLMs by over 2x, making VLMs for the first time comparable with pure vision models. Interestingly, without explicit enforcement during training, VLMs trained with DepthLM naturally avoids over-smoothing, having much fewer flying points at boundary regions than pure vision models. The simplicity of DepthLM also enables a single VLM to cover various 3D tasks beyond metric depth. Our code and model will be released at the link below.

