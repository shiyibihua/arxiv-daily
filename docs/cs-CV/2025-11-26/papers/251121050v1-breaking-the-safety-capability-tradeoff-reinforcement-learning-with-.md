---
layout: default
title: Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
---

# Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs

**arXiv**: [2511.21050v1](https://arxiv.org/abs/2511.21050) | [PDF](https://arxiv.org/pdf/2511.21050.pdf)

**ä½œè€…**: Dongkyu Derek Cho, Huan Song, Arijit Ghosh Chowdhury, Haotian An, Yawei Wang, Rohit Thekkanal, Negin Sokhandan, Sharlina Keshava, Hannah Marlowe

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨ä¸Žèƒ½åŠ›æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨å¯¹é½` `å¯éªŒè¯å¥–åŠ±` `KLçº¦æŸä¼˜åŒ–` `å®‰å…¨åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¾®è°ƒå¤§è¯­è¨€æ¨¡åž‹æ—¶å­˜åœ¨å®‰å…¨ä¸Žèƒ½åŠ›æƒè¡¡ï¼Œæ ‡å‡†æ–¹æ³•å¦‚SFTå’ŒRLHFä¼šé™ä½Žå®‰å…¨å¯¹é½
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨KLçº¦æŸä¼˜åŒ–ä¸‹ç†è®ºä¸Šæ¶ˆé™¤å®‰å…¨é€€åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªå¯¹æŠ—å®‰å…¨åŸºå‡†ä¸ŠéªŒè¯ï¼ŒRLVRå¯åŒæ—¶æå‡æŽ¨ç†èƒ½åŠ›å¹¶ç»´æŒå®‰å…¨æŠ¤æ 

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.

