---
layout: default
title: SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition
---

# SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition

**arXiv**: [2511.21471v1](https://arxiv.org/abs/2511.21471) | [PDF](https://arxiv.org/pdf/2511.21471.pdf)

**ä½œè€…**: Peiran Xu, Sudong Wang, Yao Zhu, Jianing Li, Yunjian Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpatialBenchåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›**

**å…³é”®è¯**: `ç©ºé—´è®¤çŸ¥` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†æµ‹è¯•` `åˆ†å±‚æ¡†æž¶` `ç»Ÿä¸€åº¦é‡` `ç¬¦å·æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†è¿‡åº¦ç®€åŒ–ç©ºé—´è®¤çŸ¥ï¼Œç¼ºä¹å±‚æ¬¡åŒ–è¯„ä¼°æ¡†æž¶
2. æž„å»ºåˆ†å±‚ç©ºé—´è®¤çŸ¥æ¡†æž¶å’Œç»Ÿä¸€åº¦é‡ï¼Œè¦†ç›–15ä¸ªä»»åŠ¡
3. å®žéªŒæ˜¾ç¤ºæ¨¡åž‹æ„ŸçŸ¥å¼ºä½†æŽ¨ç†å¼±ï¼Œäººç±»æµ‹è¯•æ­ç¤ºæ¨¡åž‹ç¼ºä¹ç©ºé—´æ„å›¾

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.

