---
layout: default
title: GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision
---

# GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision

**arXiv**: [2511.20994v1](https://arxiv.org/abs/2511.20994) | [PDF](https://arxiv.org/pdf/2511.20994.pdf)

**ä½œè€…**: Yuxiao Xiang, Junchi Chen, Zhenchao Jin, Changtao Miao, Haojie Yuan, Qi Chu, Tao Gong, Nenghai Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGuardTrace-VLä»¥æ£€æµ‹å¤šæ¨¡æ€æŽ¨ç†ä¸­çš„ä¸å®‰å…¨å†…å®¹**

**å…³é”®è¯**: `å¤šæ¨¡æ€å®‰å…¨æ£€æµ‹` `æŽ¨ç†è¿‡ç¨‹ç›‘æŽ§` `æ¸è¿›å¼è®­ç»ƒ` `å›¾åƒ-æ–‡æœ¬åˆ†æž` `å®‰å…¨æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§æŽ¨ç†æ¨¡åž‹åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿä¸å®‰å…¨å†…å®¹ï¼ŒçŽ°æœ‰æ–¹æ³•ä»…è¯„ä¼°è¾“å…¥å’Œæœ€ç»ˆç­”æ¡ˆ
2. é€šè¿‡è”åˆå›¾åƒ-æ–‡æœ¬åˆ†æžç›‘æŽ§å®Œæ•´æŽ¨ç†ç®¡é“ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆå­¦ä¹ å®‰å…¨åå¥½
3. åœ¨æµ‹è¯•é›†ä¸ŠF1åˆ†æ•°è¾¾93.1%ï¼Œæ¯”å…ˆå‰æ–¹æ³•æå‡13.5%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.

