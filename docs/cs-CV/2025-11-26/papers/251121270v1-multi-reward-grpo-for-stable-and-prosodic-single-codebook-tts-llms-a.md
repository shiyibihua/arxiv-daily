---
layout: default
title: Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale
---

# Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale

**arXiv**: [2511.21270v1](https://arxiv.org/abs/2511.21270) | [PDF](https://arxiv.org/pdf/2511.21270.pdf)

**ä½œè€…**: Yicheng Zhong, Peiji Yang, Zhisheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå¥–åŠ±GRPOæ¡†æž¶ä»¥è§£å†³å•ç æœ¬TTS LLMçš„éŸµå¾‹ä¸ç¨³å®šé—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆ` `å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–` `å•ç æœ¬æ¨¡åž‹` `éŸµå¾‹å¯¹é½` `å¯æ‰©å±•æ€§åˆ†æž` `æµå¼æž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å•ç æœ¬TTS LLMå­˜åœ¨éŸµå¾‹ä¸ç¨³å®šã€è¯´è¯äººæ¼‚ç§»å’Œè‡ªç„¶åº¦ä¸‹é™é—®é¢˜
2. é‡‡ç”¨å¤šå¥–åŠ±GRPOä¼˜åŒ–ç­–ç•¥ï¼Œé›†æˆé•¿åº¦æƒ©ç½šã€ç†µæ­£åˆ™åŒ–å’ŒLLMæ ‡æ³¨éŸµå¾‹å¯¹é½å¥–åŠ±
3. å®žéªŒæ˜¾ç¤ºæ–¹æ³•æå‡éŸµå¾‹ç¨³å®šæ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œæ•´ä½“è‡ªç„¶åº¦ï¼Œå¹¶éªŒè¯å¯æ‰©å±•æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.

