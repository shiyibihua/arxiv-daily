---
layout: default
title: Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model
---

# Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model

**arXiv**: [2511.21399v1](https://arxiv.org/abs/2511.21399) | [PDF](https://arxiv.org/pdf/2511.21399.pdf)

**ä½œè€…**: Joshua Fonseca Rivera

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¾®è°ƒä½¿7Bæ¨¡åž‹å¯é æ£€æµ‹å•ä»¤ç‰Œæ³¨å…¥çš„å†…éƒ¨çŠ¶æ€ï¼Œæå‡AIé€æ˜Žåº¦**

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹å¾®è°ƒ` `å†…éƒ¨çŠ¶æ€æ£€æµ‹` `AIé€æ˜Žåº¦` `æ¿€æ´»æ¨¡å¼æ³¨å…¥` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯­è¨€æ¨¡åž‹èƒ½å¦é€šè¿‡è®­ç»ƒå¯é æ£€æµ‹æ³¨å…¥çš„æ¿€æ´»æ¨¡å¼ï¼Œè€Œéžä¾èµ–è‡ªå‘æ¶ŒçŽ°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¯¹7Bæ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œé’ˆå¯¹å•ä»¤ç‰Œæ³¨å…¥çš„çž¬æ€æ¿€æ´»æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚
3. å®žéªŒæ•ˆæžœï¼šå‡†ç¡®çŽ‡ä»Ž0.4%æå‡è‡³85%ï¼Œå‡é˜³æ€§çŽ‡ä¸º0%ï¼Œå¹¶æ³›åŒ–åˆ°æœªè§æ¦‚å¿µã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected "thoughts" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at Î±=40, 0% false positives). Our model detects fleeting "thoughts" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether "training for introspection would help eliminate cross-model differences." We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.

