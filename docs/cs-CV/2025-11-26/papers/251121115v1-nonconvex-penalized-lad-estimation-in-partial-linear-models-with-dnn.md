---
layout: default
title: Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms
---

# Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms

**arXiv**: [2511.21115v1](https://arxiv.org/abs/2511.21115) | [PDF](https://arxiv.org/pdf/2511.21115.pdf)

**ä½œè€…**: Lechen Feng, Haoran Li, Lucky Li, Xingqiu Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéžå‡¸æƒ©ç½šLADä¼°è®¡ç»“åˆDNNsï¼Œè§£å†³éƒ¨åˆ†çº¿æ€§æ¨¡åž‹ä¸­çš„æ¸è¿‘åˆ†æžä¸Žè®¡ç®—é—®é¢˜**

**å…³é”®è¯**: `éƒ¨åˆ†çº¿æ€§æ¨¡åž‹` `LADå›žå½’` `æ·±åº¦ç¥žç»ç½‘ç»œ` `éžå‡¸ä¼˜åŒ–` `æ¸è¿‘åˆ†æž` `è¿‘ç«¯ç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šéžå‡¸éžå…‰æ»‘æ­£åˆ™åŒ–ä¸Žé«˜ç»´ä¸è¿žç»­ä¼˜åŒ–ï¼ŒæŒ‘æˆ˜æ¸è¿‘ç†è®ºä¸Žè®¡ç®—
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨DNNså‚æ•°åŒ–éžå‚æ•°é¡¹ï¼Œå»ºç«‹ä¸€è‡´æ€§ã€æ”¶æ•›çŽ‡å’Œæ¸è¿‘æ­£æ€æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåˆ†æžoracleé—®é¢˜ä¸Žè¿žç»­æ¾å¼›ï¼Œæ¯”è¾ƒè¿‘ç«¯æ¬¡æ¢¯åº¦æ–¹æ³•çš„è®¡ç®—æ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.

