---
layout: default
title: Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning
---

# Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning

**arXiv**: [2511.21075v1](https://arxiv.org/abs/2511.21075) | [PDF](https://arxiv.org/pdf/2511.21075.pdf)

**ä½œè€…**: Zhenchao Tang, Fang Wang, Haohuai He, Jiale Zhou, Tianxu Lv, Jun Zhu, Shouzhi Chen, Minghao Yang, Yu Wang, Jiayang Wu, Yidong Song, Jianhua Yao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹³è¡¡å¾®è°ƒæ–¹æ³•ä»¥è§£å†³ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å¯¹é½ä¸­çš„è¿‡æ‹Ÿåˆå’Œå¥–åŠ±å®šä¹‰éš¾é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å¯¹é½` `ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†` `å¹³è¡¡å¾®è°ƒ` `ä»¤ç‰Œçº§åŠ æƒ` `æ ·æœ¬çº§åŠ æƒ` `ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç”Ÿç‰©åŒ»å­¦æŽ¨ç†ä¾èµ–ç¨€ç–æ–‡æœ¬æ•°æ®ï¼Œæ ‡å‡†å¾®è°ƒæ˜“è¿‡æ‹Ÿåˆè¡¨é¢æ¨¡å¼
2. æ–¹æ³•é‡‡ç”¨åŒå±‚åŠ æƒæœºåˆ¶ï¼šä»¤ç‰Œçº§æ¦‚çŽ‡ç¼©æ”¾æŸå¤±å’Œæ ·æœ¬çº§æœ€å°ç»„ç½®ä¿¡åº¦
3. å®žéªŒæ˜¾ç¤ºåœ¨åŒ»å­¦å’Œç”Ÿç‰©ä»»åŠ¡ä¸­è¶…è¶Šæ ‡å‡†å¾®è°ƒï¼Œå¹¶æ”¯æŒä¸‹æ¸¸åº”ç”¨å¦‚åŸºå› äº¤äº’é¢„æµ‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.

