---
layout: default
title: When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models
---

# When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models

**arXiv**: [2511.21192v1](https://arxiv.org/abs/2511.21192) | [PDF](https://arxiv.org/pdf/2511.21192.pdf)

**ä½œè€…**: Hui Lu, Yi Yu, Yiming Yang, Chenyu Yi, Qixin Zhang, Bingquan Shen, Alex C. Kot, Xudong Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUPA-RFASæ¡†æž¶ä»¥æž„å»ºé€šç”¨å¯è¿ç§»å¯¹æŠ—è¡¥ä¸æ”»å‡»è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹**

**å…³é”®è¯**: `å¯¹æŠ—è¡¥ä¸æ”»å‡»` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `é€šç”¨å¯è¿ç§»æ€§` `é²æ£’æ€§ä¼˜åŒ–` `ç‰¹å¾ç©ºé—´å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹æ˜“å—æ”»å‡»ï¼Œä½†çŽ°æœ‰è¡¥ä¸æ”»å‡»åœ¨æœªçŸ¥æ¨¡åž‹å’Œé»‘ç›’è®¾ç½®ä¸‹éš¾ä»¥è¿ç§»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆç‰¹å¾ç©ºé—´ç›®æ ‡ã€é²æ£’æ€§ä¼˜åŒ–å’ŒVLAç‰¹å®šæŸå¤±ï¼Œå­¦ä¹ å…±äº«ç‰¹å¾ç©ºé—´ä¸­çš„é€šç”¨è¡¥ä¸ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šç§VLAæ¨¡åž‹å’Œç‰©ç†æ‰§è¡Œä¸­ï¼Œæ”»å‡»å¯è·¨æ¨¡åž‹ã€ä»»åŠ¡å’Œè§†è§’è¿ç§»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.

