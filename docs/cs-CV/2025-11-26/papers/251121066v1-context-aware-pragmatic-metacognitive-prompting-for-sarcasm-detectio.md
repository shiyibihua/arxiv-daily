---
layout: default
title: Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection
---

# Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection

**arXiv**: [2511.21066v1](https://arxiv.org/abs/2511.21066) | [PDF](https://arxiv.org/pdf/2511.21066.pdf)

**ä½œè€…**: Michael Iskandardinata, William Christian, Derwin Suhartono

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å®žç”¨å…ƒè®¤çŸ¥æç¤ºæ–¹æ³•ï¼Œä»¥æå‡å¤§è¯­è¨€æ¨¡åž‹åœ¨è®½åˆºæ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è®½åˆºæ£€æµ‹` `å¤§è¯­è¨€æ¨¡åž‹` `ä¸Šä¸‹æ–‡æ£€ç´¢` `å®žç”¨å…ƒè®¤çŸ¥æç¤º` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è®½åˆºæ£€æµ‹å› è¯­è¨€å¤šæ ·æ€§å’Œæ–‡åŒ–å·®å¼‚è€Œå…·æŒ‘æˆ˜æ€§ï¼ŒçŽ°æœ‰æ¨¡åž‹å¯¹æœªçŸ¥è¯æ±‡æ£€æµ‹ä¸å¯é ã€‚
2. æ–¹æ³•ç»“åˆæ£€ç´¢ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç½‘ç»œæ£€ç´¢å’Œæ¨¡åž‹è‡ªçŸ¥è¯†ç­–ç•¥ï¼Œå¢žå¼ºèƒŒæ™¯ç†è§£ã€‚
3. å®žéªŒåœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œéžå‚æ•°æ£€ç´¢æ˜¾è‘—æå‡æ€§èƒ½ï¼Œæœ€é«˜è¾¾9.87%å®F1æ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.

