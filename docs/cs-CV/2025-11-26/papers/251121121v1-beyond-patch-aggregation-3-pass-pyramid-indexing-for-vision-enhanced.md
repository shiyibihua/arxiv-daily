---
layout: default
title: Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval
---

# Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval

**arXiv**: [2511.21121v1](https://arxiv.org/abs/2511.21121) | [PDF](https://arxiv.org/pdf/2511.21121.pdf)

**ä½œè€…**: Anup Roy, Rishabh Gyanendra Upadhyay, Animesh Rameshbhai Panara, Robin Mills

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisionRAGç³»ç»Ÿä»¥è§£å†³è§†è§‰å¢žå¼ºæ–‡æ¡£æ£€ç´¢ä¸­çš„å†…å­˜å¼€é”€å’Œæ¨¡åž‹ä¾èµ–é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰å¢žå¼ºæ£€ç´¢` `é‡‘å­—å¡”ç´¢å¼•` `å¤šæ¨¡æ€ç³»ç»Ÿ` `æ–‡æ¡£å›¾åƒå¤„ç†` `OCRè‡ªç”±æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæ–‡æ¡£æ£€ç´¢ä¾èµ–OCRå’Œå¯å‘å¼åˆ†å—ï¼Œæ˜“å—å¸ƒå±€å˜åŒ–å½±å“ä¸”ä¸¢å¤±ç©ºé—´çº¿ç´¢
2. é‡‡ç”¨ä¸‰éé‡‘å­—å¡”ç´¢å¼•æ¡†æž¶ï¼Œç”Ÿæˆå…¨å±€é¡µé¢æ‘˜è¦ã€èŠ‚æ ‡é¢˜ç­‰è½»é‡çº§å‘é‡è¿›è¡Œæ£€ç´¢
3. åœ¨é‡‘èžæ–‡æ¡£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFinanceBenchå‡†ç¡®çŽ‡è¾¾0.8051ï¼ŒTAT DQAå¬å›žçŽ‡è¾¾0.9629

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.
>   We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.
>   VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.

