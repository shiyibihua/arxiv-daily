---
layout: default
title: Predictive Safety Shield for Dyna-Q Reinforcement Learning
---

# Predictive Safety Shield for Dyna-Q Reinforcement Learning

**arXiv**: [2511.21531v1](https://arxiv.org/abs/2511.21531) | [PDF](https://arxiv.org/pdf/2511.21531.pdf)

**ä½œè€…**: Jin Pin, Krasowski Hanna, Vanneaux Elena

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢„æµ‹æ€§å®‰å…¨æŠ¤ç›¾ä»¥æå‡ç¦»æ•£ç©ºé—´æ¨¡åž‹å¼ºåŒ–å­¦ä¹ çš„å®‰å…¨æ€§ä¸Žæ€§èƒ½**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ å®‰å…¨` `æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶` `ç¦»æ•£ç©ºé—´å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨æŠ¤ç›¾` `Qå‡½æ•°æ›´æ–°` `åˆ†å¸ƒåç§»é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ åœ¨çŽ°å®žä»»åŠ¡ä¸­éš¾ä»¥èŽ·å¾—ç¡¬å®‰å…¨ä¿è¯ï¼ŒçŽ°æœ‰å®‰å…¨æŠ¤ç›¾å¿½ç•¥ä¸åŒå®‰å…¨åŠ¨ä½œçš„æœªæ¥æ€§èƒ½å½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽçŽ¯å¢ƒæ¨¡åž‹çš„å®‰å…¨æ¨¡æ‹Ÿè¿›è¡Œå±€éƒ¨Qå‡½æ•°æ›´æ–°ï¼Œå®žçŽ°é¢„æµ‹æ€§å®‰å…¨æŠ¤ç›¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç½‘æ ¼ä¸–ç•ŒçŽ¯å¢ƒä¸­ï¼ŒçŸ­é¢„æµ‹èŒƒå›´å³å¯è¯†åˆ«æœ€ä¼˜è·¯å¾„ï¼Œä¸”å¯¹åˆ†å¸ƒåç§»å…·æœ‰é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.

