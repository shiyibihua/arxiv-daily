---
layout: default
title: Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining
---

# Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining

**arXiv**: [2511.21613v1](https://arxiv.org/abs/2511.21613) | [PDF](https://arxiv.org/pdf/2511.21613.pdf)

**ä½œè€…**: Dongyang Fan, Diba Hashemi, Sai Praneeth Karimireddy, Martin Jaggi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ ·åŒ–å…ƒæ•°æ®ä¸Žä½ç½®ç­–ç•¥ä»¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡åž‹é¢„è®­ç»ƒ**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹é¢„è®­ç»ƒ` `å…ƒæ•°æ®å¤šæ ·æ€§` `è®­ç»ƒæ•ˆçŽ‡ä¼˜åŒ–` `è¾…åŠ©ä»»åŠ¡å­¦ä¹ ` `æ½œåœ¨è¡¨ç¤ºåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä»…åˆ©ç”¨URLå…ƒæ•°æ®ï¼Œå¿½ç•¥å…¶ä»–ç±»åž‹å…ƒæ•°æ®çš„æ½œåœ¨åŠ é€Ÿæ•ˆæžœ
2. æ–¹æ³•è¦ç‚¹ï¼šæŽ¢ç´¢ç»†ç²’åº¦å…ƒæ•°æ®å¹¶å¼•å…¥å…ƒæ•°æ®å‰ç½®ä¸Žé™„åŠ ä½œä¸ºè¾…åŠ©ä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šå…ƒæ•°æ®å¯æå‡è®­ç»ƒæ•ˆçŽ‡ï¼Œå¹¶é€šè¿‡æ½œåœ¨è¡¨ç¤ºåˆ†æžæ­ç¤ºå­¦ä¹ æœºåˆ¶

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

