---
layout: default
title: $\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion
---

# $\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion

**arXiv**: [2511.21542v1](https://arxiv.org/abs/2511.21542) | [PDF](https://arxiv.org/pdf/2511.21542.pdf)

**ä½œè€…**: Zhihao Zhan, Jiaying Zhou, Likui Zhang, Qinhan Lv, Hao Liu, Jusheng Zhang, Weizheng Li, Ziliang Chen, Tianshui Chen, Keze Wang, Liang Lin, Guangrun Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºE0è¿žç»­åŒ–ç¦»æ•£æ‰©æ•£æ¡†æž¶ä»¥å¢žå¼ºVLAæ¨¡åž‹çš„æ³›åŒ–æ€§å’Œç²¾ç»†æŽ§åˆ¶**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `ç¦»æ•£æ‰©æ•£` `æœºå™¨äººæŽ§åˆ¶` `æ³›åŒ–æ€§` `ç²¾ç»†åŠ¨ä½œæŽ§åˆ¶` `åŽ»å™ªç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹æ³›åŒ–èƒ½åŠ›å·®ï¼ŒåŠ¨ä½œç”Ÿæˆç²—ç³™æˆ–ä¸ç¨³å®š
2. E0å°†åŠ¨ä½œç”Ÿæˆå»ºæ¨¡ä¸ºé‡åŒ–åŠ¨ä½œä»¤ç‰Œçš„è¿­ä»£åŽ»å™ªï¼ŒåŒ¹é…æœºå™¨äººæŽ§åˆ¶çš„ç¦»æ•£ç‰¹æ€§
3. å®žéªŒåœ¨å¤šä¸ªåŸºå‡†ä¸Šå®žçŽ°SOTAï¼Œå¹³å‡æå‡10.7%ï¼ŒçœŸå®žä¸–ç•ŒéªŒè¯ç²¾ç¡®æ“æŽ§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.

