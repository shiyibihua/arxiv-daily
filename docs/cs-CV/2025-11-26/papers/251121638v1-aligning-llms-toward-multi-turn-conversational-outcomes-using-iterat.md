---
layout: default
title: Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO
---

# Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO

**arXiv**: [2511.21638v1](https://arxiv.org/abs/2511.21638) | [PDF](https://arxiv.org/pdf/2511.21638.pdf)

**ä½œè€…**: Daniel R. Jiang, Jalaj Bhandari, Yukai Yang, RÃ©mi Munos, Tyler Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿­ä»£PPOæ–¹æ³•ä»¥ä¼˜åŒ–å¤šè½®å¯¹è¯LLMsï¼Œé€‚ç”¨äºŽç›®æ ‡å¯¼å‘åœºæ™¯ã€‚**

**å…³é”®è¯**: `å¤šè½®å¯¹è¯ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ å¯¹é½` `è¿­ä»£PPOç®—æ³•` `Qå‡½æ•°å­¦ä¹ ` `RLHFåº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šè½®å¯¹è¯ä¸­å¥–åŠ±ç¨€ç–ã€é•¿è§†é‡Žï¼Œå“åº”è§„åˆ’ä¸Žä»¤ç‰Œç”Ÿæˆä¸ä¸€è‡´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†å¤šè½®RLé—®é¢˜ç®€åŒ–ä¸ºå•è½®RLHFï¼Œä½¿ç”¨å­¦ä¹ Qå‡½æ•°ä½œä¸ºå¥–åŠ±æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¿­ä»£PPOç»“åˆåœ¨çº¿æ›´æ–°ä¸Žç¦»çº¿è®­ç»ƒï¼Œæå‡ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

