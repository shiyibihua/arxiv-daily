---
layout: default
title: Generative Early Stage Ranking
---

# Generative Early Stage Ranking

**arXiv**: [2511.21095v1](https://arxiv.org/abs/2511.21095) | [PDF](https://arxiv.org/pdf/2511.21095.pdf)

**ä½œè€…**: Juhee Hong, Meng Liu, Shengzhi Wang, Xiaoheng Mao, Huihui Cheng, Leon Gao, Christopher Leung, Jin Zhou, Chandra Mouli Sekar, Zhao Zhu, Ruochen Liu, Tuan Trieu, Dawei Sun, Jeet Kanjani, Rui Li, Jing Qian, Xuan Cao, Minjie Fan, Mingze Gao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆå¼æ—©æœŸæŽ’åºèŒƒå¼ä»¥è§£å†³ç”¨æˆ·-ç‰©å“è§£è€¦æ–¹æ³•åœ¨æ•ˆæžœä¸Šçš„é™åˆ¶**

**å…³é”®è¯**: `æŽ¨èç³»ç»Ÿ` `æ—©æœŸæŽ’åº` `æ³¨æ„åŠ›æœºåˆ¶` `ç”¨æˆ·-ç‰©å“äº¤äº’` `å¤šé˜¶æ®µæŽ’åº` `æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ—©æœŸæŽ’åºç³»ç»Ÿé‡‡ç”¨ç”¨æˆ·-ç‰©å“è§£è€¦æ–¹æ³•ï¼Œéš¾ä»¥æ•æ‰ç»†ç²’åº¦äº²å’Œæ€§ä¸Žè·¨ä¿¡å·äº¤äº’
2. å¼•å…¥æ··åˆæ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒ…æ‹¬ç¡¬åŒ¹é…ã€ç›®æ ‡æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ï¼Œå¢žå¼ºç”¨æˆ·-ç‰©å“äº¤äº’
3. é€šè¿‡ç¦»çº¿ä¸Žåœ¨çº¿å®žéªŒéªŒè¯ï¼Œåœ¨å…³é”®æŒ‡æ ‡ã€å‚ä¸Žåº¦å’Œæ¶ˆè´¹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the "user-item decoupling" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.

