---
layout: default
title: Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning
---

# Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning

**arXiv**: [2511.21011v1](https://arxiv.org/abs/2511.21011) | [PDF](https://arxiv.org/pdf/2511.21011.pdf)

**ä½œè€…**: Sid Bharthulwar, Stone Tao, Hao Su

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤é”™é‡ç½®ä»¥è§£å†³å¤§è§„æ¨¡å¹¶è¡Œå¼ºåŒ–å­¦ä¹ ä¸­çš„éžå¹³ç¨³æ€§é—®é¢˜**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸ` `éžå¹³ç¨³æ€§` `äº¤é”™é‡ç½®` `æ ·æœ¬æ•ˆçŽ‡` `æœºå™¨äººçŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒæ­¥é‡ç½®åœ¨é«˜UTDæ¯”è®¾ç½®ä¸‹å¼•å…¥æœ‰å®³éžå¹³ç¨³æ€§ï¼Œæ‰­æ›²å­¦ä¹ ä¿¡å·
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨äº¤é”™é‡ç½®ï¼ŒçŽ¯å¢ƒåœ¨ä»»åŠ¡å‘¨æœŸå†…ä¸åŒæ—¶é—´ç‚¹åˆå§‹åŒ–ï¼Œå¢žåŠ æ—¶é—´å¤šæ ·æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æœºå™¨äººçŽ¯å¢ƒä¸­æå‡æ ·æœ¬æ•ˆçŽ‡ã€æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.

