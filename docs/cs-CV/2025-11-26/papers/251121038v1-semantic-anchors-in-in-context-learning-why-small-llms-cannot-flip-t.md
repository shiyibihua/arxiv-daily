---
layout: default
title: Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels
---

# Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels

**arXiv**: [2511.21038v1](https://arxiv.org/abs/2511.21038) | [PDF](https://arxiv.org/pdf/2511.21038.pdf)

**ä½œè€…**: Anantha Padmanaban Krishna Kumar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­ä¹‰é”šç‚¹ç†è®ºè§£é‡Šå°æ¨¡åž‹æ— æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç¿»è½¬æ ‡ç­¾è¯­ä¹‰**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `è¯­ä¹‰é”šç‚¹` `æ ‡ç­¾è¯­ä¹‰` `æ¨¡åž‹å¯¹é½` `å°‘æ ·æœ¬æç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸Šä¸‹æ–‡å­¦ä¹ èƒ½å¦è¦†ç›–é¢„è®­ç»ƒæ ‡ç­¾è¯­ä¹‰ï¼Œæˆ–ä»…ä¼˜åŒ–çŽ°æœ‰è¯­ä¹‰éª¨å¹²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¯¹æ¯”è‡ªç„¶ä¸Žç¿»è½¬æ ‡ç­¾æ¼”ç¤ºï¼Œå®šä¹‰è¯­ä¹‰å¯¹é½æŒ‡æ ‡å’Œè¯­ä¹‰è¦†ç›–çŽ‡ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨1-12Bå‚æ•°æ¨¡åž‹ä¸­ï¼Œç¿»è½¬è¯­ä¹‰ä¸‹æ­£ç¡®çŽ‡ä¿æŒé›¶ï¼Œæ”¯æŒè¯­ä¹‰é”šç‚¹è§‚ç‚¹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.

