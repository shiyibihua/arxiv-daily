---
layout: default
title: Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning
---

# Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning

**arXiv**: [2511.21375v1](https://arxiv.org/abs/2511.21375) | [PDF](https://arxiv.org/pdf/2511.21375.pdf)

**ä½œè€…**: Xin Gu, Haoji Zhang, Qihang Fan, Jingxuan Niu, Zhipeng Zhang, Libo Zhang, Guang Chen, Fan Chen, Longyin Wen, Sijie Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTVG-o1æ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨æ—¶ç©ºè§†é¢‘å®šä½ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `æ—¶ç©ºè§†é¢‘å®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `è¾¹ç•Œæ¡†æ€ç»´é“¾` `å‡ ä½•æ„ŸçŸ¥å¥–åŠ±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨æ—¶ç©ºè§†é¢‘å®šä½ä¸­å› è®­ç»ƒç›®æ ‡ä¸åŒ¹é…å’Œç»†ç²’åº¦å¯¹é½å¼±è€Œè¡¨çŽ°ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è¾¹ç•Œæ¡†æ€ç»´é“¾æœºåˆ¶å’Œå‡ ä½•æ„ŸçŸ¥çš„å¤šç»´å¼ºåŒ–å¥–åŠ±å‡½æ•°è¿›è¡Œå¾®è°ƒã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨HCSTVGæ•°æ®é›†ä¸Šè¶…è¶Šæœ€ä½³ä»»åŠ¡ç‰¹å®šæ–¹æ³•ï¼Œå¹¶åœ¨å¤šæ•°æ®é›†ä¸Šå±•ç¤ºå¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\% m\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.

