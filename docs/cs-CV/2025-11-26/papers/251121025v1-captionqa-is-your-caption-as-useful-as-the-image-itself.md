---
layout: default
title: CaptionQA: Is Your Caption as Useful as the Image Itself?
---

# CaptionQA: Is Your Caption as Useful as the Image Itself?

**arXiv**: [2511.21025v1](https://arxiv.org/abs/2511.21025) | [PDF](https://arxiv.org/pdf/2511.21025.pdf)

**ä½œè€…**: Shijia Yang, Yunong Liu, Bohan Zhai, Ximeng Sun, Zicheng Liu, Emad Barsoum, Manling Li, Chenfeng Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCaptionQAåŸºå‡†ä»¥è¯„ä¼°å›¾åƒæè¿°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®žç”¨æ€§**

**å…³é”®è¯**: `å›¾åƒæè¿°è¯„ä¼°` `å¤šæ¨¡æ€åŸºå‡†` `ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨` `LLMé—®ç­”` `é¢†åŸŸç‰¹å®šåˆ†ç±»` `å¼€æºåŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒæè¿°èƒ½å¦æ›¿ä»£å›¾åƒæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼ŒçŽ°æœ‰è¯„ä¼°æ–¹æ³•å¿½ç•¥æ­¤é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤šé¢†åŸŸåŸºå‡†ï¼Œé€šè¿‡LLMä½¿ç”¨æè¿°å›žç­”å¤šé€‰é¢˜ï¼Œç›´æŽ¥æµ‹é‡æè¿°æ•ˆç”¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºæè¿°ä¸Žå›¾åƒæ•ˆç”¨å·®è·å¤§ï¼Œæ¨¡åž‹åœ¨ä¼ ç»ŸåŸºå‡†ç›¸ä¼¼ä½†æè¿°æ•ˆç”¨é™32%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.

