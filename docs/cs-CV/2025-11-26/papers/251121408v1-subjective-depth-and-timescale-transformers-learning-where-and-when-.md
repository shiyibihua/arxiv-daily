---
layout: default
title: Subjective Depth and Timescale Transformers: Learning Where and When to Compute
---

# Subjective Depth and Timescale Transformers: Learning Where and When to Compute

**arXiv**: [2511.21408v1](https://arxiv.org/abs/2511.21408) | [PDF](https://arxiv.org/pdf/2511.21408.pdf)

**ä½œè€…**: Frederico Wieser, Martin Benfeghoul, Haitham Bou Ammar, Jun Wang, Zafeirios Fountas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸»è§‚æ·±åº¦ä¸Žæ—¶é—´å°ºåº¦Transformerï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±è®¡ç®—æå‡Transformeræ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `Transformeræž¶æž„` `åŠ¨æ€è®¡ç®—è·¯ç”±` `è´å¶æ–¯æƒŠå–œ` `KVç¼“å­˜ä¼˜åŒ–` `æ¡ä»¶è®¡ç®—` `æ•ˆçŽ‡æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ‡å‡†Transformerè®¡ç®—åˆ†é…åƒµåŒ–ï¼Œé™åˆ¶å¤§æ¨¡åž‹å’Œé•¿åºåˆ—æ•ˆçŽ‡ã€‚
2. SDTå’ŒSTTåˆ©ç”¨è´å¶æ–¯æƒŠå–œä¿¡å·åŠ¨æ€è·¯ç”±ï¼Œå­¦ä¹ è®¡ç®—ä½ç½®å’Œæ—¶é—´ã€‚
3. å®žéªŒæ˜¾ç¤ºè‡ªæ³¨æ„åŠ›è®¡ç®—å‡å°‘75%ï¼ŒKVç¼“å­˜éœ€æ±‚é™ä½Ž50%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.

