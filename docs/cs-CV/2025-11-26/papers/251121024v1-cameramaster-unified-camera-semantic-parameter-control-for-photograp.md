---
layout: default
title: CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching
---

# CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching

**arXiv**: [2511.21024v1](https://arxiv.org/abs/2511.21024) | [PDF](https://arxiv.org/pdf/2511.21024.pdf)

**ä½œè€…**: Qirui Yang, Yang Yang, Ying Zeng, Xiaobin Hu, Bo Li, Huanjing Yue, Jingyu Yang, Peng-Tao Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCameraMasterç»Ÿä¸€æ¡†æž¶ï¼Œå®žçŽ°æ‘„å½±åŽæœŸå¤„ç†ä¸­çš„ç²¾ç¡®ç›¸æœºè¯­ä¹‰å‚æ•°æŽ§åˆ¶ã€‚**

**å…³é”®è¯**: `å›¾åƒç¼–è¾‘` `æ‰©æ•£æ¨¡åž‹` `ç›¸æœºå‚æ•°æŽ§åˆ¶` `è¯­ä¹‰å‚æ•°å¯¹é½` `æ‘„å½±åŽæœŸå¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–æ¨¡ç³Šæ–‡æœ¬æç¤ºæˆ–ç‹¬ç«‹å‚æ•°è°ƒæ•´ï¼Œéš¾ä»¥å®žçŽ°ç‰©ç†ä¸€è‡´çš„å¤šå‚æ•°æŽ§åˆ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§£è€¦ç›¸æœºæŒ‡ä»¤ä¸Žå‚æ•°åµŒå…¥ï¼Œå¹¶æ³¨å…¥å†…å®¹ç‰¹å¾ï¼Œå®žçŽ°è¯­ä¹‰ä¸Žå‚æ•°çš„ç´§å¯†å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨78Kæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ”¯æŒå¤šå‚æ•°ç»„åˆï¼Œå“åº”å•è°ƒçº¿æ€§ï¼Œæ€§èƒ½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-guided diffusion models have greatly advanced image editing and generation. However, achieving physically consistent image retouching with precise parameter control (e.g., exposure, white balance, zoom) remains challenging. Existing methods either rely solely on ambiguous and entangled text prompts, which hinders precise camera control, or train separate heads/weights for parameter adjustment, which compromises scalability, multi-parameter composition, and sensitivity to subtle variations. To address these limitations, we propose CameraMaster, a unified camera-aware framework for image retouching. The key idea is to explicitly decouple the camera directive and then coherently integrate two critical information streams: a directive representation that captures the photographer's intent, and a parameter embedding that encodes precise camera settings. CameraMaster first uses the camera parameter embedding to modulate both the camera directive and the content semantics. The modulated directive is then injected into the content features via cross-attention, yielding a strongly camera-sensitive semantic context. In addition, the directive and camera embeddings are injected as conditioning and gating signals into the time embedding, enabling unified, layer-wise modulation throughout the denoising process and enforcing tight semantic-parameter alignment. To train and evaluate CameraMaster, we construct a large-scale dataset of 78K image-prompt pairs annotated with camera parameters. Extensive experiments show that CameraMaster produces monotonic and near-linear responses to parameter variations, supports seamless multi-parameter composition, and significantly outperforms existing methods.

