---
layout: default
title: Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit
---

# Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit

**arXiv**: [2511.21569v1](https://arxiv.org/abs/2511.21569) | [PDF](https://arxiv.org/pdf/2511.21569.pdf)

**ä½œè€…**: Alex Diep

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºä¸“å®¶è§’è‰²LLMè‡ªæˆ‘é€æ˜Žåº¦å¤±è´¥ï¼Œéœ€è¡Œä¸ºè®¾è®¡ä¸ŽéªŒè¯**

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹é€æ˜Žåº¦` `ä¸“å®¶è§’è‰²å®¡è®¡` `è¡Œä¸ºä¸ä¸€è‡´` `æŽ¨ç†ä¼˜åŒ–å½±å“` `è´å¶æ–¯éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMåœ¨ä¸“å®¶è§’è‰²ä¸­æ— æ³•å¯é æŠ«éœ²AIèº«ä»½ï¼Œå±å®³ç”¨æˆ·ä¿¡ä»»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å…¬å…±èŠ±å›­è®¾è®¡ï¼Œå®¡è®¡16ä¸ªæ¨¡åž‹åœ¨19200æ¬¡è¯•éªŒä¸­çš„è¡Œä¸ºã€‚
3. å®žéªŒæ•ˆæžœï¼šæŠ«éœ²çŽ‡2.8%-73.6%ï¼Œæ¨¡åž‹èº«ä»½æ¯”å‚æ•°æ•°æ›´èƒ½é¢„æµ‹è¡Œä¸ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a "Reverse Gell-Mann Amnesia" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($Î”R_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($Îº= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.

