---
layout: default
title: MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training
---

# MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training

**arXiv**: [2511.21592v1](https://arxiv.org/abs/2511.21592) | [PDF](https://arxiv.org/pdf/2511.21592.pdf)

**ä½œè€…**: Haotian Xue, Qi Chen, Zhonghao Wang, Xun Huang, Eli Shechtman, Jinrong Xie, Yongxin Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://xavihart.github.io/mogan)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MoGANï¼šé€šè¿‡å°‘é‡æ­¥æ•°çš„è¿åŠ¨å¯¹æŠ—åŽè®­ç»ƒæå‡è§†é¢‘æ‰©æ•£æ¨¡åž‹çš„è¿åŠ¨è´¨é‡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `è¿åŠ¨è´¨é‡` `å¯¹æŠ—è®­ç»ƒ` `å…‰æµåˆ¤åˆ«å™¨` `åŽè®­ç»ƒ` `è§†é¢‘ç”Ÿæˆ` `æ—¶é—´ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨è¿åŠ¨è¿žè´¯æ€§å’ŒçœŸå®žæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“äº§ç”ŸæŠ–åŠ¨ç­‰é—®é¢˜ï¼ŒåŽŸå› æ˜¯ç¼ºä¹å¯¹æ—¶é—´ä¸€è‡´æ€§çš„ç›´æŽ¥ç›‘ç£ã€‚
2. MoGANé€šè¿‡è¿åŠ¨å¯¹æŠ—åŽè®­ç»ƒï¼Œåˆ©ç”¨å…‰æµåˆ¤åˆ«å™¨åŒºåˆ†çœŸå®žå’Œç”Ÿæˆè¿åŠ¨ï¼Œå¹¶ç»“åˆåˆ†å¸ƒåŒ¹é…æ­£åˆ™åŒ–å™¨ï¼Œæå‡è¿åŠ¨è´¨é‡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMoGANåœ¨VBenchå’ŒVideoJAM-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—æé«˜äº†è¿åŠ¨è´¨é‡ï¼Œä¸”ä¿æŒäº†è‰¯å¥½çš„è§†è§‰æ•ˆæžœã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨å¸§çº§åˆ«ä¸Šå®žçŽ°äº†å¾ˆé«˜çš„é€¼çœŸåº¦ï¼Œä½†ä»ç„¶éš¾ä»¥ä¿è¯è¿åŠ¨çš„è¿žè´¯æ€§ã€åŠ¨æ€æ€§å’ŒçœŸå®žæ„Ÿï¼Œç»å¸¸äº§ç”ŸæŠ–åŠ¨ã€é‡å½±æˆ–ä¸åˆç†çš„åŠ¨æ€æ•ˆæžœã€‚ä¸€ä¸ªå…³é”®çš„é™åˆ¶æ˜¯ï¼Œæ ‡å‡†åŽ»å™ªMSEç›®æ ‡æ²¡æœ‰æä¾›å¯¹æ—¶é—´ä¸€è‡´æ€§çš„ç›´æŽ¥ç›‘ç£ï¼Œä½¿å¾—æ¨¡åž‹åœ¨äº§ç”Ÿè¾ƒå·®è¿åŠ¨çš„åŒæ—¶ä»èƒ½å®žçŽ°è¾ƒä½Žçš„æŸå¤±ã€‚æˆ‘ä»¬æå‡ºäº†MoGANï¼Œä¸€ä¸ªä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„åŽè®­ç»ƒæ¡†æž¶ï¼Œå¯ä»¥åœ¨æ²¡æœ‰å¥–åŠ±æ¨¡åž‹æˆ–äººç±»åå¥½æ•°æ®çš„æƒ…å†µä¸‹æé«˜è¿åŠ¨çš„çœŸå®žæ„Ÿã€‚MoGANå»ºç«‹åœ¨3æ­¥è’¸é¦è§†é¢‘æ‰©æ•£æ¨¡åž‹ä¹‹ä¸Šï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºŽDiTçš„å…‰æµåˆ¤åˆ«å™¨æ¥åŒºåˆ†çœŸå®žè¿åŠ¨å’Œç”Ÿæˆè¿åŠ¨ï¼Œå¹¶ç»“åˆä¸€ä¸ªåˆ†å¸ƒåŒ¹é…æ­£åˆ™åŒ–å™¨æ¥ä¿æŒè§†è§‰é€¼çœŸåº¦ã€‚åœ¨Wan2.1-T2V-1.3Bä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒMoGANåœ¨å„ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è¿åŠ¨è´¨é‡ã€‚åœ¨VBenchä¸Šï¼ŒMoGANçš„è¿åŠ¨å¾—åˆ†æ¯”50æ­¥æ•™å¸ˆæ¨¡åž‹æé«˜äº†+7.3%ï¼Œæ¯”3æ­¥DMDæ¨¡åž‹æé«˜äº†+13.3%ã€‚åœ¨VideoJAM-Benchä¸Šï¼ŒMoGANçš„è¿åŠ¨å¾—åˆ†æ¯”æ•™å¸ˆæ¨¡åž‹æé«˜äº†+7.4%ï¼Œæ¯”DMDæé«˜äº†+8.8%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“ç”šè‡³æ›´å¥½çš„ç¾Žå­¦å’Œå›¾åƒè´¨é‡å¾—åˆ†ã€‚ä¸€é¡¹äººç±»ç ”ç©¶è¿›ä¸€æ­¥è¯å®žï¼ŒMoGANåœ¨è¿åŠ¨è´¨é‡æ–¹é¢æ›´å—æ¬¢è¿Žï¼ˆ52% vs. 38% for the teacher; 56% vs. 29% for DMDï¼‰ã€‚æ€»çš„æ¥è¯´ï¼ŒMoGANåœ¨ä¸ç‰ºç‰²è§†è§‰é€¼çœŸåº¦æˆ–æ•ˆçŽ‡çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†æ˜¾è‘—æ›´çœŸå®žçš„è¿åŠ¨ï¼Œä¸ºå¿«é€Ÿã€é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€æ¡å®žç”¨çš„é€”å¾„ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨ç”Ÿæˆè§†é¢‘æ—¶ï¼Œè™½ç„¶åœ¨å•å¸§å›¾åƒè´¨é‡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œä½†éš¾ä»¥ä¿è¯è§†é¢‘ä¸­è¿åŠ¨çš„è¿žè´¯æ€§å’ŒçœŸå®žæ€§ï¼Œå®¹æ˜“å‡ºçŽ°æŠ–åŠ¨ã€é‡å½±ç­‰é—®é¢˜ã€‚çŽ°æœ‰çš„åŸºäºŽMSEçš„è®­ç»ƒç›®æ ‡æ²¡æœ‰ç›´æŽ¥å¯¹æ—¶é—´ä¸€è‡´æ€§è¿›è¡Œçº¦æŸï¼Œå¯¼è‡´æ¨¡åž‹ä¼˜åŒ–æ–¹å‘ä¸Žè¿åŠ¨è´¨é‡æå‡ä¸å®Œå…¨ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoGANçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æŠ—è®­ç»ƒçš„æ–¹å¼ï¼Œè®©æ¨¡åž‹å­¦ä¹ åˆ°æ›´çœŸå®žçš„è¿åŠ¨æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥ä¸€ä¸ªå…‰æµåˆ¤åˆ«å™¨ï¼Œç”¨äºŽåŒºåˆ†çœŸå®žè§†é¢‘å’Œç”Ÿæˆè§†é¢‘çš„è¿åŠ¨æ¨¡å¼ï¼Œä»Žè€Œå¼•å¯¼ç”Ÿæˆå™¨ç”Ÿæˆæ›´é€¼çœŸçš„è¿åŠ¨ã€‚åŒæ—¶ï¼Œä¸ºäº†é¿å…å¯¹æŠ—è®­ç»ƒå¯èƒ½å¸¦æ¥çš„è§†è§‰è´¨é‡ä¸‹é™ï¼Œå¼•å…¥äº†åˆ†å¸ƒåŒ¹é…æ­£åˆ™åŒ–å™¨ï¼Œä»¥ä¿æŒç”Ÿæˆè§†é¢‘çš„è§†è§‰é€¼çœŸåº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMoGANçš„æ•´ä½“æ¡†æž¶æ˜¯ä¸€ä¸ªåŽè®­ç»ƒæµç¨‹ï¼Œå»ºç«‹åœ¨ä¸€ä¸ªé¢„è®­ç»ƒçš„3æ­¥è’¸é¦è§†é¢‘æ‰©æ•£æ¨¡åž‹ä¹‹ä¸Šã€‚ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼š1) åŸºäºŽDiTçš„å…‰æµåˆ¤åˆ«å™¨ï¼Œç”¨äºŽåŒºåˆ†çœŸå®žå’Œç”Ÿæˆè§†é¢‘çš„è¿åŠ¨æ¨¡å¼ï¼›2) åˆ†å¸ƒåŒ¹é…æ­£åˆ™åŒ–å™¨ï¼Œç”¨äºŽä¿æŒç”Ÿæˆè§†é¢‘çš„è§†è§‰é€¼çœŸåº¦ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨è¿›è¡Œå¯¹æŠ—è®­ç»ƒï¼ŒåŒæ—¶ä½¿ç”¨æ­£åˆ™åŒ–å™¨çº¦æŸç”Ÿæˆå™¨çš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šMoGANçš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†ä¸€ç§ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„åŽè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå’Œåˆ†å¸ƒåŒ¹é…æ­£åˆ™åŒ–ï¼Œåœ¨ä¸ä¾èµ–äººå·¥æ ‡æ³¨æˆ–å¥–åŠ±æ¨¡åž‹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æ‰©æ•£æ¨¡åž‹çš„è¿åŠ¨è´¨é‡ã€‚ä¸Žä¼ ç»Ÿçš„åŸºäºŽMSEçš„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒMoGANèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ åˆ°çœŸå®žçš„è¿åŠ¨æ¨¡å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šå…‰æµåˆ¤åˆ«å™¨é‡‡ç”¨DiTæž¶æž„ï¼Œè¾“å…¥ä¸ºçœŸå®žè§†é¢‘å’Œç”Ÿæˆè§†é¢‘çš„å…‰æµä¿¡æ¯ï¼Œè¾“å‡ºä¸ºåˆ¤åˆ«ç»“æžœã€‚åˆ†å¸ƒåŒ¹é…æ­£åˆ™åŒ–å™¨é‡‡ç”¨L1æŸå¤±ï¼Œç”¨äºŽçº¦æŸç”Ÿæˆè§†é¢‘çš„åƒç´ åˆ†å¸ƒä¸ŽåŽŸå§‹è§†é¢‘çš„åƒç´ åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚å¯¹æŠ—è®­ç»ƒé‡‡ç”¨æ ‡å‡†çš„GANæŸå¤±å‡½æ•°ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„å­¦ä¹ çŽ‡ï¼Œä»¥åŠæ­£åˆ™åŒ–ç³»æ•°ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„è®­ç»ƒæ•ˆæžœã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

MoGANåœ¨å¤šä¸ªè§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨VBenchä¸Šï¼ŒMoGANçš„è¿åŠ¨å¾—åˆ†æ¯”50æ­¥æ•™å¸ˆæ¨¡åž‹æé«˜äº†+7.3%ï¼Œæ¯”3æ­¥DMDæ¨¡åž‹æé«˜äº†+13.3%ã€‚åœ¨VideoJAM-Benchä¸Šï¼ŒMoGANçš„è¿åŠ¨å¾—åˆ†æ¯”æ•™å¸ˆæ¨¡åž‹æé«˜äº†+7.4%ï¼Œæ¯”DMDæé«˜äº†+8.8%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“ç”šè‡³æ›´å¥½çš„ç¾Žå­¦å’Œå›¾åƒè´¨é‡å¾—åˆ†ã€‚äººç±»è¯„ä¼°ä¹Ÿè¡¨æ˜Žï¼ŒMoGANåœ¨è¿åŠ¨è´¨é‡æ–¹é¢æ›´å—æ¬¢è¿Žã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

MoGANæŠ€æœ¯å¯å¹¿æ³›åº”ç”¨äºŽè§†é¢‘ç”Ÿæˆã€è§†é¢‘ç¼–è¾‘ã€æ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥æå‡ç”Ÿæˆè§†é¢‘çš„çœŸå®žæ„Ÿå’Œæµç•…åº¦ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ä¾‹å¦‚ï¼Œåœ¨æ¸¸æˆå¼€å‘ä¸­ï¼Œå¯ä»¥åˆ©ç”¨MoGANç”Ÿæˆæ›´é€¼çœŸçš„è§’è‰²åŠ¨ç”»ï¼›åœ¨ç”µå½±åˆ¶ä½œä¸­ï¼Œå¯ä»¥ç”¨äºŽç”Ÿæˆç‰¹æ•ˆåœºæ™¯ï¼Œé™ä½Žåˆ¶ä½œæˆæœ¬ã€‚æœªæ¥ï¼ŒMoGANæœ‰æœ›æˆä¸ºè§†é¢‘å†…å®¹åˆ›ä½œçš„é‡è¦å·¥å…·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

