---
layout: default
title: From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings
---

# From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.21428" target="_blank" class="toolbar-btn">arXiv: 2511.21428v1</a>
    <a href="https://arxiv.org/pdf/2511.21428.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21428v1" 
            onclick="toggleFavorite(this, '2511.21428v1', 'From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Jiajie Zhang, SÃ¶ren Schwertfeger, Alexander Kleiner

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

**å¤‡æ³¨**: 10 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéšå¼åŠ¨ä½œåŸè¯­åˆ†å‰²çš„VLAé¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå·¥ä¸šåœºæ™¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `VLAé¢„è®­ç»ƒ` `æ— ç›‘ç£å­¦ä¹ ` `åŠ¨ä½œåˆ†å‰²` `å·¥ä¸šæœºå™¨äºº` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å·¥ä¸šè§†é¢‘ä¸­å¤§é‡æœªæ ‡æ³¨çš„äººå·¥æ“ä½œæ•°æ®ï¼Œé˜»ç¢äº†VLAæ¨¡å‹åœ¨å·¥ä¸šé¢†åŸŸçš„åº”ç”¨ã€‚
2. æå‡ºä¸€ç§æ— ç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è¿åŠ¨æ ‡è®°å™¨å’Œéšå¼åŠ¨ä½œèƒ½é‡åº¦é‡ï¼Œè‡ªåŠ¨å‘ç°å¹¶åˆ†å‰²è§†é¢‘ä¸­çš„åŠ¨ä½œåŸè¯­ã€‚
3. åœ¨å…¬å…±æ•°æ®é›†å’Œå·¥ä¸šæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿæå–è¯­ä¹‰è¿è´¯çš„åŠ¨ä½œåŸè¯­ï¼Œé€‚ç”¨äºVLAé¢„è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— ç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨ä»è¿ç»­çš„å·¥ä¸šè§†é¢‘æµä¸­æŒ–æ˜å¤§é‡æœªæ ‡æ³¨çš„äººå·¥æ¼”ç¤ºæ•°æ®ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„é¢„è®­ç»ƒã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„è¿åŠ¨æ ‡è®°å™¨æ¥ç¼–ç è¿åŠ¨åŠ¨æ€ï¼Œç„¶ååˆ©ç”¨ä¸€ä¸ªæ— ç›‘ç£çš„åŠ¨ä½œåˆ†å‰²å™¨ï¼Œè¯¥åˆ†å‰²å™¨åˆ©ç”¨äº†ä¸€ç§æ–°çš„â€œéšå¼åŠ¨ä½œèƒ½é‡â€åº¦é‡æ¥å‘ç°å’Œåˆ†å‰²è¯­ä¹‰è¿è´¯çš„åŠ¨ä½œåŸè¯­ã€‚è¯¥æµç¨‹è¾“å‡ºåˆ†å‰²åçš„è§†é¢‘ç‰‡æ®µåŠå…¶å¯¹åº”çš„éšå¼åŠ¨ä½œåºåˆ—ï¼Œä¸ºVLAé¢„è®­ç»ƒæä¾›ç›´æ¥é€‚ç”¨çš„ç»“æ„åŒ–æ•°æ®ã€‚åœ¨å…¬å…±åŸºå‡†å’Œä¸€ä¸ªä¸“æœ‰çš„ç”µæœºè£…é…æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†å‰²äººç±»åœ¨å·¥ä½œç«™æ‰§è¡Œçš„å…³é”®ä»»åŠ¡ã€‚é€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œçš„è¿›ä¸€æ­¥èšç±»å’Œå®šé‡è¯„ä¼°è¯å®äº†æ‰€å‘ç°çš„åŠ¨ä½œåŸè¯­çš„è¯­ä¹‰è¿è´¯æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨è‡ªåŠ¨ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œç”¨äºä»éç»“æ„åŒ–å·¥ä¸šè§†é¢‘ä¸­æå–å’Œç»„ç»‡VLAé¢„è®­ç»ƒæ•°æ®ï¼Œä¸ºåˆ¶é€ ä¸šä¸­å…·èº«äººå·¥æ™ºèƒ½çš„é›†æˆæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•ä»æµ·é‡æœªæ ‡æ³¨çš„å·¥ä¸šè§†é¢‘æ•°æ®ä¸­æå–æœ‰ç”¨çš„ä¿¡æ¯ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„é¢„è®­ç»ƒã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦äººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚æ­¤å¤–ï¼Œå·¥ä¸šè§†é¢‘é€šå¸¸æ˜¯è¿ç»­çš„ï¼Œç¼ºä¹æ˜ç¡®çš„åŠ¨ä½œè¾¹ç•Œï¼Œä½¿å¾—è‡ªåŠ¨åˆ†å‰²å’Œç†è§£å˜å¾—å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œè‡ªåŠ¨å‘ç°å’Œåˆ†å‰²è§†é¢‘ä¸­çš„åŠ¨ä½œåŸè¯­ã€‚é¦–å…ˆï¼Œåˆ©ç”¨è¿åŠ¨æ ‡è®°å™¨å­¦ä¹ è§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡å¼ï¼Œç„¶ååŸºäºè¿™äº›è¿åŠ¨æ¨¡å¼ï¼Œä½¿ç”¨ä¸€ç§æ–°çš„â€œéšå¼åŠ¨ä½œèƒ½é‡â€åº¦é‡æ¥ç¡®å®šåŠ¨ä½œçš„è¾¹ç•Œã€‚è¿™ç§æ–¹æ³•é¿å…äº†äººå·¥æ ‡æ³¨çš„éœ€è¦ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†è¿ç»­çš„è§†é¢‘æµã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä¸ªæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¿åŠ¨æ ‡è®°å™¨å’ŒåŠ¨ä½œåˆ†å‰²å™¨ã€‚è¿åŠ¨æ ‡è®°å™¨è´Ÿè´£å°†è§†é¢‘å¸§ç¼–ç æˆè¿åŠ¨tokenï¼Œæ•æ‰è§†é¢‘ä¸­çš„è¿åŠ¨ä¿¡æ¯ã€‚åŠ¨ä½œåˆ†å‰²å™¨åˆ™åˆ©ç”¨è¿™äº›è¿åŠ¨tokenï¼Œé€šè¿‡è®¡ç®—â€œéšå¼åŠ¨ä½œèƒ½é‡â€æ¥ç¡®å®šåŠ¨ä½œçš„è¾¹ç•Œï¼Œå¹¶å°†è§†é¢‘åˆ†å‰²æˆä¸€ç³»åˆ—åŠ¨ä½œåŸè¯­ã€‚æœ€åï¼Œå°†åˆ†å‰²åçš„è§†é¢‘ç‰‡æ®µå’Œå¯¹åº”çš„éšå¼åŠ¨ä½œåºåˆ—ä½œä¸ºVLAæ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†â€œéšå¼åŠ¨ä½œèƒ½é‡â€è¿™ä¸€æ¦‚å¿µï¼Œå¹¶å°†å…¶ç”¨äºæ— ç›‘ç£çš„åŠ¨ä½œåˆ†å‰²ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰‹å·¥ç‰¹å¾æˆ–ç›‘ç£å­¦ä¹ çš„åŠ¨ä½œåˆ†å‰²æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ è§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡å¼ï¼Œå¹¶æ ¹æ®è¿™äº›æ¨¡å¼æ¥ç¡®å®šåŠ¨ä½œçš„è¾¹ç•Œã€‚è¿™ç§æ–¹æ³•æ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„å·¥ä¸šåœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè¿åŠ¨æ ‡è®°å™¨å¯ä»¥ä½¿ç”¨å„ç§ç°æœ‰çš„è§†é¢‘ç¼–ç å™¨ï¼Œä¾‹å¦‚TimeSformerã€‚éšå¼åŠ¨ä½œèƒ½é‡çš„è®¡ç®—æ–¹å¼æ˜¯åŸºäºè¿åŠ¨tokenä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼Œè¡¨ç¤ºè¯¥ç‰‡æ®µå±äºåŒä¸€ä¸ªåŠ¨ä½œçš„å¯èƒ½æ€§è¶Šå¤§ã€‚åŠ¨ä½œåˆ†å‰²å™¨å¯ä»¥ä½¿ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•æ¥å¯»æ‰¾æœ€ä¼˜çš„åˆ†å‰²æ–¹æ¡ˆã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘åˆ†å‰²çš„å‡†ç¡®æ€§å’ŒåŠ¨ä½œåŸè¯­çš„è¯­ä¹‰è¿è´¯æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨å…¬å…±åŸºå‡†å’Œä¸€ä¸ªä¸“æœ‰çš„ç”µæœºè£…é…æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†å‰²äººç±»åœ¨å·¥ä½œç«™æ‰§è¡Œçš„å…³é”®ä»»åŠ¡ï¼Œå¹¶ä¸”æ‰€å‘ç°çš„åŠ¨ä½œåŸè¯­å…·æœ‰è‰¯å¥½çš„è¯­ä¹‰è¿è´¯æ€§ã€‚é€šè¿‡ä¸è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å®šé‡è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•æå–çš„åŠ¨ä½œåŸè¯­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºVLAæ¨¡å‹çš„é¢„è®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è´¨é‡æ£€æµ‹ã€è®¾å¤‡ç»´æŠ¤ç­‰ã€‚é€šè¿‡VLAæ¨¡å‹ï¼Œæœºå™¨äººå¯ä»¥ç†è§£äººç±»çš„æ“ä½œæŒ‡ä»¤ï¼Œå¹¶è‡ªä¸»å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåˆ†æå·¥äººçš„æ“ä½œè¡Œä¸ºï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨åˆ¶é€ ä¸šå‘æ™ºèƒ½åŒ–ã€æŸ”æ€§åŒ–æ–¹å‘å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.

