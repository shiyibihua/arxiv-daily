---
layout: default
title: G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning
---

# G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning

**arXiv**: [2511.21688v1](https://arxiv.org/abs/2511.21688) | [PDF](https://arxiv.org/pdf/2511.21688.pdf)

**ä½œè€…**: Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran, Lihan Jiang, Yifan Wang, Chenming Zhu, Runsen Xu, Tai Wang, Jiangmiao Pang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGÂ²VLMæ¨¡åž‹ï¼Œé€šè¿‡ç»Ÿä¸€3Dé‡å»ºä¸Žç©ºé—´æŽ¨ç†è§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹ç©ºé—´æ™ºèƒ½ä¸è¶³é—®é¢˜ã€‚**

**å…³é”®è¯**: `å‡ ä½•åŸºç¡€è§†è§‰è¯­è¨€æ¨¡åž‹` `3Dé‡å»º` `ç©ºé—´æŽ¨ç†` `å¤šè§†å›¾å›¾åƒè®­ç»ƒ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ç»Ÿä¸€è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç©ºé—´ç†è§£å’ŒæŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œç¼ºä¹ä»Ž2Då›¾åƒé‡å»º3Dç©ºé—´çš„å‡ ä½•å­¦ä¹ è¿‡ç¨‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šGÂ²VLMåˆ©ç”¨å­¦ä¹ åˆ°çš„3Dè§†è§‰å‡ ä½•ç‰¹å¾ï¼Œç›´æŽ¥é¢„æµ‹3Då±žæ€§å¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œäº¤é”™æŽ¨ç†å¢žå¼ºç©ºé—´æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨3Dé‡å»ºä»»åŠ¡ä¸­ä¸Žå…ˆè¿›å‰é¦ˆæ¨¡åž‹ç›¸å½“ï¼Œåœ¨ç©ºé—´ç†è§£ä¸ŽæŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜äºŽæˆ–ç«žäº‰äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

