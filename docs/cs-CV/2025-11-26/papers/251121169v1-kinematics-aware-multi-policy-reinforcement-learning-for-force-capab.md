---
layout: default
title: Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation
---

# Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation

**arXiv**: [2511.21169v1](https://arxiv.org/abs/2511.21169) | [PDF](https://arxiv.org/pdf/2511.21169.pdf)

**ä½œè€…**: Kaiyan Xiao, Zihan Xu, Cheng Zhe, Chengju Liu, Qijun Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„è§£è€¦ä¸‰é˜¶æ®µæ¡†æž¶ï¼Œä»¥è§£å†³äººå½¢æœºå™¨äººåœ¨é«˜è´Ÿè½½å·¥ä¸šåœºæ™¯ä¸­çš„çµå·§ä¸Žä¸»åŠ¨åŠ›äº¤äº’é—®é¢˜ã€‚**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `è¿åŠ¨å­¦å…ˆéªŒ` `åŠ›äº¤äº’` `è¯¾ç¨‹å­¦ä¹ ` `è§£è€¦ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨çµå·§æ“ä½œä¸Šä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³é«˜è´Ÿè½½å·¥ä¸šå¯¹çµå·§æ€§å’Œä¸»åŠ¨åŠ›äº¤äº’çš„å¤åˆéœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è§£è€¦ä¸‰é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬ä¸Šä½“ç­–ç•¥ã€ä¸‹ä½“ç­–ç•¥å’Œå¢žé‡å‘½ä»¤ç­–ç•¥ï¼ŒåµŒå…¥è¿åŠ¨å­¦å…ˆéªŒåŠ é€Ÿæ”¶æ•›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä¸Šä½“ç­–ç•¥é€šè¿‡å¯å‘å¼å¥–åŠ±å‡½æ•°å¿«é€Ÿæ”¶æ•›ï¼Œä¸‹ä½“ç­–ç•¥åŸºäºŽåŠ›è¯¾ç¨‹å­¦ä¹ ä¸»åŠ¨è°ƒæŽ§çŽ¯å¢ƒåŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humanoid robots, with their human-like morphology, hold great potential for industrial applications. However, existing loco-manipulation methods primarily focus on dexterous manipulation, falling short of the combined requirements for dexterity and proactive force interaction in high-load industrial scenarios. To bridge this gap, we propose a reinforcement learning-based framework with a decoupled three-stage training pipeline, consisting of an upper-body policy, a lower-body policy, and a delta-command policy. To accelerate upper-body training, a heuristic reward function is designed. By implicitly embedding forward kinematics priors, it enables the policy to converge faster and achieve superior performance. For the lower body, a force-based curriculum learning strategy is developed, enabling the robot to actively exert and regulate interaction forces with the environment.

