---
layout: default
title: Estimating Ising Models in Total Variation Distance
---

# Estimating Ising Models in Total Variation Distance

**arXiv**: [2511.21008v1](https://arxiv.org/abs/2511.21008) | [PDF](https://arxiv.org/pdf/2511.21008.pdf)

**ä½œè€…**: Constantinos Daskalakis, Vardis Kandiros, Rui Yao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ€å¤§ä¼ªä¼¼ç„¶ä¼°è®¡å™¨ç»Ÿä¸€åˆ†æžï¼Œç”¨äºŽæ€»å˜å·®è·ç¦»ä¸‹çš„ä¼Šè¾›æ¨¡åž‹ä¼°è®¡**

**å…³é”®è¯**: `ä¼Šè¾›æ¨¡åž‹ä¼°è®¡` `æ€»å˜å·®è·ç¦»` `æœ€å¤§ä¼ªä¼¼ç„¶ä¼°è®¡å™¨` `ç®—å­èŒƒæ•°æœ‰ç•Œ` `æ— ç©·èŒƒæ•°æœ‰ç•Œ` `æ ·æœ¬å¤æ‚åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼°è®¡ä¼Šè¾›æ¨¡åž‹åœ¨æ€»å˜å·®è·ç¦»ä¸‹çš„è®¡ç®—ä¸Žç»Ÿè®¡æ•ˆçŽ‡æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†æžMPLEåœ¨ç®—å­èŒƒæ•°æœ‰ç•Œå’Œæ— ç©·èŒƒæ•°æœ‰ç•Œæ¨¡åž‹ç±»ä¸­çš„æ€§èƒ½
3. å®žéªŒæˆ–æ•ˆæžœï¼šèŽ·å¾—å¤šé¡¹å¼æ—¶é—´ç®—æ³•å’Œæœ€ä¼˜æˆ–è¿‘æœ€ä¼˜æ ·æœ¬å¤æ‚åº¦ä¿è¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We consider the problem of estimating Ising models over $n$ variables in Total Variation (TV) distance, given $l$ independent samples from the model. While the statistical complexity of the problem is well-understood [DMR20], identifying computationally and statistically efficient algorithms has been challenging. In particular, remarkable progress has occurred in several settings, such as when the underlying graph is a tree [DP21, BGPV21], when the entries of the interaction matrix follow a Gaussian distribution [GM24, CK24], or when the bulk of its eigenvalues lie in a small interval [AJK+24, KLV24], but no unified framework for polynomial-time estimation in TV exists so far. Our main contribution is a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) for two general classes of Ising models. The first class includes models that have bounded operator norm and satisfy the Modified Log-Sobolev Inequality (MLSI), a functional inequality that was introduced to study the convergence of the associated Glauber dynamics to stationarity. In the second class of models, the interaction matrix has bounded infinity norm (or bounded width), which is the most common assumption in the literature for structure learning of Ising models. We show how our general results for these classes yield polynomial-time algorithms and optimal or near-optimal sample complexity guarantees in a variety of settings. Our proofs employ a variety of tools from tensorization inequalities to measure decompositions and concentration bounds.

