---
layout: default
title: Seeing without Pixels: Perception from Camera Trajectories
---

# Seeing without Pixels: Perception from Camera Trajectories

**arXiv**: [2511.21681v1](https://arxiv.org/abs/2511.21681) | [PDF](https://arxiv.org/pdf/2511.21681.pdf)

**ä½œè€…**: Zihui Xue, Kristen Grauman, Dima Damen, Andrew Zisserman, Tengda Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCamFormeræ¡†æž¶ï¼Œåˆ©ç”¨ç›¸æœºè½¨è¿¹æ„ŸçŸ¥è§†é¢‘å†…å®¹ï¼Œæ— éœ€åƒç´ ä¿¡æ¯ã€‚**

**å…³é”®è¯**: `ç›¸æœºè½¨è¿¹æ„ŸçŸ¥` `å¯¹æ¯”å­¦ä¹ ` `è§†é¢‘å†…å®¹ç†è§£` `è·¨æ¨¡æ€å¯¹é½` `é²æ£’è¡¨ç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»…ä»Žç›¸æœºè½¨è¿¹èƒ½å¦æ„ŸçŸ¥è§†é¢‘å†…å®¹ï¼ŒæŒ‘æˆ˜ä¼ ç»Ÿåƒç´ ä¾èµ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå°†ç›¸æœºè½¨è¿¹ä¸Žè‡ªç„¶è¯­è¨€å¯¹é½äºŽåµŒå…¥ç©ºé—´ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è·¨æ¨¡æ€å¯¹é½ã€åˆ†ç±»ç­‰ä»»åŠ¡ä¸­éªŒè¯è½¨è¿¹ä¿¡æ¯çš„é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

