---
layout: default
title: DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving
---

# DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving

**arXiv**: [2511.21669v1](https://arxiv.org/abs/2511.21669) | [PDF](https://arxiv.org/pdf/2511.21669.pdf)

**ä½œè€…**: Fengze Yu, Leshu Li, Brad McDanel, Saiqian Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒå¼æŽ¨æµ‹è§£ç æ¡†æž¶DSDä»¥è§£å†³è¾¹ç¼˜äº‘å¼‚æž„çŽ¯å¢ƒä¸­å¤§æ¨¡åž‹æŽ¨ç†å»¶è¿Ÿé«˜å’Œå¯æ‰©å±•æ€§å·®çš„é—®é¢˜**

**å…³é”®è¯**: `åˆ†å¸ƒå¼æŽ¨æµ‹è§£ç ` `è¾¹ç¼˜äº‘æŽ¨ç†` `å¤§è¯­è¨€æ¨¡åž‹æœåŠ¡` `è‡ªé€‚åº”çª—å£æŽ§åˆ¶` `ç¦»æ•£äº‹ä»¶æ¨¡æ‹Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†åœ¨å¼‚æž„è¾¹ç¼˜äº‘çŽ¯å¢ƒä¸­è§£ç å»¶è¿Ÿé«˜ã€å¯æ‰©å±•æ€§å—é™
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åè°ƒè‰ç¨¿-ç›®æ ‡æ‰§è¡Œå°†æŽ¨æµ‹è§£ç æ‰©å±•åˆ°å¤šè®¾å¤‡éƒ¨ç½²ï¼Œå¹¶è®¾è®¡è‡ªé€‚åº”çª—å£æŽ§åˆ¶ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºDSDç›¸æ¯”çŽ°æœ‰åŸºçº¿æœ€é«˜åŠ é€Ÿ1.1å€ï¼Œåžåé‡æå‡9.7%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.

