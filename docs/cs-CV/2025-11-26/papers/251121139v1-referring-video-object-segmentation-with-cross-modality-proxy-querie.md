---
layout: default
title: Referring Video Object Segmentation with Cross-Modality Proxy Queries
---

# Referring Video Object Segmentation with Cross-Modality Proxy Queries

**arXiv**: [2511.21139v1](https://arxiv.org/abs/2511.21139) | [PDF](https://arxiv.org/pdf/2511.21139.pdf)

**ä½œè€…**: Baoli Sun, Xinzhu Ma, Ning Wang, Zhihui Wang, Zhiyong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProxyFormerä»¥è§£å†³å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å¯¹é½ä¸Žè·Ÿè¸ªé—®é¢˜**

**å…³é”®è¯**: `å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²` `è·¨æ¨¡æ€å¯¹é½` `ä»£ç†æŸ¥è¯¢` `Transformerç»“æž„` `è¯­ä¹‰ä¸€è‡´æ€§è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ç¼ºä¹å¸§é—´ä¾èµ–å»ºæ¨¡å’Œæ–‡æœ¬çº¦æŸå»¶è¿Ÿé›†æˆï¼Œå¯¼è‡´ç›®æ ‡è·Ÿè¸ªä¸å‡†ç¡®
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ä»£ç†æŸ¥è¯¢é›†æˆè§†è§‰ä¸Žæ–‡æœ¬è¯­ä¹‰ï¼Œé€šè¿‡å¤šé˜¶æ®µæ›´æ–°å¢žå¼ºè·¨æ¨¡æ€å¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡åˆ†å‰²å‡†ç¡®æ€§å’Œè·Ÿè¸ªè¿žè´¯æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.

