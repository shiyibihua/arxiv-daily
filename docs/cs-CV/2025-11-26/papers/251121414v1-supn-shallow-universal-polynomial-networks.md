---
layout: default
title: SUPN: Shallow Universal Polynomial Networks
---

# SUPN: Shallow Universal Polynomial Networks

**arXiv**: [2511.21414v1](https://arxiv.org/abs/2511.21414) | [PDF](https://arxiv.org/pdf/2511.21414.pdf)

**ä½œè€…**: Zachary Morrow, Michael Penwarden, Brian Chen, Aurya Javeed, Akil Narayan, John D. Jakeman

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ…å±‚é€šç”¨å¤šé¡¹å¼ç½‘ç»œä»¥é™ä½Žå‡½æ•°é€¼è¿‘çš„å‚æ•°éœ€æ±‚**

**å…³é”®è¯**: `å‡½æ•°é€¼è¿‘` `å¤šé¡¹å¼ç½‘ç»œ` `å‚æ•°ä¼˜åŒ–` `æµ…å±‚ç½‘ç»œ` `æ•°å€¼å®žéªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦ç½‘ç»œå’ŒKANså‚æ•°è¿‡å¤šï¼Œå¯¼è‡´ä¼˜åŒ–å›°éš¾å’Œæ³›åŒ–è¯¯å·®ä¸ç¨³å®š
2. ç”¨å•å±‚å¯å­¦ä¹ å¤šé¡¹å¼æ›¿æ¢éšè—å±‚ï¼Œç»“åˆæ·±åº¦ç½‘ç»œä¸Žå¤šé¡¹å¼ä¼˜åŠ¿
3. å®žéªŒæ˜¾ç¤ºSUPNsåœ¨å‚æ•°ç›¸åŒä¸‹è¯¯å·®å’Œå˜å¼‚æ€§ä½ŽäºŽDNNså’ŒKANs

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.

