---
layout: default
title: Escaping the Verifier: Learning to Reason via Demonstrations
---

# Escaping the Verifier: Learning to Reason via Demonstrations

**arXiv**: [2511.21667v1](https://arxiv.org/abs/2511.21667) | [PDF](https://arxiv.org/pdf/2511.21667.pdf)

**ä½œè€…**: Locke Cai, Ivan Provilkov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAROæ–¹æ³•ï¼Œä»Žä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ æŽ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šéªŒè¯å™¨ã€‚**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—è®­ç»ƒ` `æŽ¨ç†ä¼˜åŒ–` `ä¸“å®¶æ¼”ç¤º` `è¯­è¨€æ¨¡åž‹è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè®¸å¤šæŽ¨ç†ä»»åŠ¡ç¼ºä¹éªŒè¯å™¨ï¼Œä½†ä¸“å®¶æ¼”ç¤ºæœªè¢«å……åˆ†åˆ©ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¯¹æŠ—æ€§äº¤äº’ï¼Œç­–ç•¥æ¨¡ä»¿ä¸“å®¶ç­”æ¡ˆï¼Œæ‰¹è¯„è€…åŒºåˆ†ç­–ç•¥ä¸Žä¸“å®¶ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨Countdownç­‰ä»»åŠ¡ä¸Šè¶…è¶ŠåŸºçº¿ï¼Œå±•ç¤ºç¨³å¥æ‰©å±•è¶‹åŠ¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.

