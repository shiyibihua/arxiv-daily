---
layout: default
title: Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models
---

# Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models

**arXiv**: [2511.21122v1](https://arxiv.org/abs/2511.21122) | [PDF](https://arxiv.org/pdf/2511.21122.pdf)

**ä½œè€…**: Changlin Li, Jiawei Zhang, Zeyi Shi, Zongxin Yang, Zhihui Li, Xiaojun Chang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEntPruneræ¡†æž¶ä»¥è§£å†³æ‰©æ•£å’Œæµæ¨¡åž‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å‚æ•°å†—ä½™é—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `æµæ¨¡åž‹` `æ¨¡åž‹å‰ªæž` `æ¡ä»¶ç†µåå·®` `è‡ªé€‚åº”å‰ªæž` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¢„è®­ç»ƒæ‰©æ•£å’Œæµæ¨¡åž‹è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—å‚æ•°å†—ä½™
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ¡ä»¶ç†µåå·®æŒ‡å¯¼å—çº§é‡è¦æ€§è¯„ä¼°ï¼Œå®žçŽ°é›¶-shotè‡ªé€‚åº”å‰ªæž
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨DiTå’ŒSiTæ¨¡åž‹ä¸Šå®žçŽ°æœ€é«˜2.22å€æŽ¨ç†åŠ é€Ÿï¼Œä¿æŒç”Ÿæˆè´¨é‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy specifically designed for generative models. Unlike discriminative models, generative models require preserving the diversity and condition-fidelity of the output distribution. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent Conditional Entropy Deviation (CED) as a guiding metric. CED quantifies how much the distribution diverges from the learned conditional data distribution after removing a block. Second, we propose a zero-shot adaptive pruning framework to automatically determine when and how much to prune during training. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22$\times$ inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.

