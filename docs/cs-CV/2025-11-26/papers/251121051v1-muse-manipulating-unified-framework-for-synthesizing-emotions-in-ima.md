---
layout: default
title: MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization
---

# MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization

**arXiv**: [2511.21051v1](https://arxiv.org/abs/2511.21051) | [PDF](https://arxiv.org/pdf/2511.21051.pdf)

**ä½œè€…**: Yingjie Xia, Xi Wang, Jinglei Shi, Vicky Kalogeiton, Jian Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMUSEç»Ÿä¸€æ¡†æž¶ï¼Œé€šè¿‡æµ‹è¯•æ—¶ä¼˜åŒ–å®žçŽ°å›¾åƒæƒ…æ„Ÿç”Ÿæˆä¸Žç¼–è¾‘ï¼Œæå‡æƒ…æ„Ÿå‡†ç¡®æ€§å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚**

**å…³é”®è¯**: `å›¾åƒæƒ…æ„Ÿåˆæˆ` `æµ‹è¯•æ—¶ä¼˜åŒ–` `æ‰©æ•£æ¨¡åž‹` `æƒ…æ„Ÿåˆ†ç±»å™¨` `å¤šæƒ…æ„ŸæŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åˆ†ç¦»ç”Ÿæˆä¸Žç¼–è¾‘ä»»åŠ¡ï¼Œå¯¼è‡´æ•ˆçŽ‡ä½Žä¸‹ï¼Œé™åˆ¶å¦‚æ²»ç–—å¹²é¢„ç­‰åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨çŽ°æˆæƒ…æ„Ÿåˆ†ç±»å™¨ï¼ŒåŸºäºŽæ¢¯åº¦ä¼˜åŒ–æƒ…æ„Ÿä»¤ç‰Œï¼Œå¹¶å¼•å…¥è¯­ä¹‰ç›¸ä¼¼æ€§æŒ‡å¯¼æ—¶æœºã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­ä¼˜äºŽæ‰€æœ‰æ–¹æ³•ï¼Œå¹³è¡¡å†…å®¹ã€æ–‡æœ¬æç¤ºå’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.

