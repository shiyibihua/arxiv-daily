---
layout: default
title: Mechanistic Interpretability for Transformer-based Time Series Classification
---

# Mechanistic Interpretability for Transformer-based Time Series Classification

**arXiv**: [2511.21514v1](https://arxiv.org/abs/2511.21514) | [PDF](https://arxiv.org/pdf/2511.21514.pdf)

**ä½œè€…**: MatÄ«ss KalnÄre, Sofoklis Kitharidis, Thomas BÃ¤ck, Niki van Stein

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•ä»¥è§£å†³æ—¶é—´åºåˆ—åˆ†ç±»ä¸­Transformeræ¨¡åž‹å†…éƒ¨æœºåˆ¶ä¸é€æ˜Žé—®é¢˜**

**å…³é”®è¯**: `æœºåˆ¶å¯è§£é‡Šæ€§` `æ—¶é—´åºåˆ—åˆ†ç±»` `Transformeræ¨¡åž‹` `æ³¨æ„åŠ›æœºåˆ¶` `å› æžœå›¾` `ç¨€ç–è‡ªç¼–ç å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformeræ¨¡åž‹åœ¨æ—¶é—´åºåˆ—åˆ†ç±»ä¸­å†…éƒ¨å†³ç­–æœºåˆ¶å¤æ‚ä¸”éš¾ä»¥ç†è§£
2. æ–¹æ³•è¦ç‚¹ï¼šä»ŽNLPå¼•å…¥æ¿€æ´»ä¿®è¡¥ã€æ³¨æ„åŠ›æ˜¾è‘—æ€§å’Œç¨€ç–è‡ªç¼–ç å™¨è¿›è¡Œæœºåˆ¶å¯è§£é‡Šæ€§åˆ†æž
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šæž„å»ºå› æžœå›¾ï¼Œæ­ç¤ºå…³é”®æ³¨æ„åŠ›å¤´å’Œæ—¶åºä½ç½®çš„ä½œç”¨

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.

