---
layout: default
title: Controlling changes to attention logits
---

# Controlling changes to attention logits

**arXiv**: [2511.21377v1](https://arxiv.org/abs/2511.21377) | [PDF](https://arxiv.org/pdf/2511.21377.pdf)

**ä½œè€…**: Ben Anson, Laurence Aitchison

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‚æ•°ä¾èµ–å­¦ä¹ çŽ‡æ–¹æ³•ä»¥è§£å†³Transformeræ³¨æ„åŠ›æƒé‡ä¸ç¨³å®šé—®é¢˜**

**å…³é”®è¯**: `Transformerç¨³å®šæ€§` `æ³¨æ„åŠ›æœºåˆ¶` `å­¦ä¹ çŽ‡è°ƒæ•´` `æƒé‡æŽ§åˆ¶` `MLAå…¼å®¹æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformerä¸­æŸ¥è¯¢å’Œé”®æƒé‡æ˜“å¢žé•¿è¿‡å¤§ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®š
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å‚æ•°ä¾èµ–å­¦ä¹ çŽ‡æŽ§åˆ¶æ³¨æ„åŠ›logitså˜åŒ–ï¼Œæ— éœ€QKå½’ä¸€åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MLAè®¾ç½®ä¸­ä¼˜äºŽå…¶ä»–æ–¹æ³•ï¼Œæ€§èƒ½ä¸ŽQKå½’ä¸€åŒ–ç«žäº‰

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as `QK norm', fixes stability issues in practice, but is not always applicable. For example, QK norm is not compatible with Multi Latent Attention (MLA) because QK norm requires full materialization of queries and keys during inference, which is not done in MLA. In this paper we suggest that controlling the changes to logits is important for stability. We show that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights. We find that our cheap intervention allows us to increase the base learning rate of the network, outperform other methods in the MLA setting, and achieve performance competitive with QK norm when using Multi-head Attention.

