---
layout: default
title: MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts
---

# MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts

**arXiv**: [2511.21089v1](https://arxiv.org/abs/2511.21089) | [PDF](https://arxiv.org/pdf/2511.21089.pdf)

**ä½œè€…**: Ivan Novikov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMLPMoEæ–¹æ³•ï¼Œå°†ç¨ å¯†LLM MLPsé›¶æ ·æœ¬è½¬æ¢ä¸ºé™æ€MoEä»¥æå‡è®¡ç®—æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `é›¶æ ·æœ¬è½¬æ¢` `æ··åˆä¸“å®¶` `ç»“æž„åŒ–ç¨€ç–` `æŽ¨ç†ä¼˜åŒ–` `å¼ é‡å¹¶è¡Œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨ å¯†LLMæŽ¨ç†è®¡ç®—æˆæœ¬é«˜ï¼Œå‚æ•°æ¿€æ´»å†—ä½™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¼ é‡åˆ‡ç‰‡å’Œæ±‚å’Œï¼Œæ— éœ€è®­ç»ƒæˆ–æ ¡å‡†æ•°æ®ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨8Bæ¨¡åž‹ä¸Šï¼Œç¨€ç–åŒ–ç§»é™¤20%å‚æ•°ï¼Œå›°æƒ‘åº¦å˜åŒ–å°äºŽ2%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1

