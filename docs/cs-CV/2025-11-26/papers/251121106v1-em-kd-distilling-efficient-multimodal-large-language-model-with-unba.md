---
layout: default
title: EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens
---

# EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens

**arXiv**: [2511.21106v1](https://arxiv.org/abs/2511.21106) | [PDF](https://arxiv.org/pdf/2511.21106.pdf)

**ä½œè€…**: Ze Feng, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEM-KDä»¥è§£å†³é«˜æ•ˆå¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­è§†è§‰ä»¤ç‰Œä¸å¹³è¡¡å¯¼è‡´çš„ç»†ç²’åº¦ç†è§£ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰ä»¤ç‰Œå¯¹é½` `é«˜æ•ˆè®¡ç®—` `è¯­ä¹‰ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé«˜æ•ˆå¤šæ¨¡æ€å¤§æ¨¡åž‹åŽ‹ç¼©è§†è§‰ä»¤ç‰Œæ—¶ï¼Œä»¤ç‰Œä¸å¹³è¡¡å¯¼è‡´ç»†ç²’åº¦è§†è§‰ç†è§£èƒ½åŠ›ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•å¯¹é½å¸ˆç”Ÿè§†è§‰ä»¤ç‰Œï¼Œå¹¶å¼•å…¥äº²å’ŒåŠ›ä¸Žè¯­ä¹‰è’¸é¦ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEM-KDåœ¨å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ä¸Šæ˜¾è‘—ä¼˜äºŽå…ˆå‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.

