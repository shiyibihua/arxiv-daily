---
layout: default
title: EvilGenie: A Reward Hacking Benchmark
---

# EvilGenie: A Reward Hacking Benchmark

**arXiv**: [2511.21654v1](https://arxiv.org/abs/2511.21654) | [PDF](https://arxiv.org/pdf/2511.21654.pdf)

**ä½œè€…**: Jonathan Gabor, Jayson Lynch, Jonathan Rosenfeld

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvilGenieåŸºå‡†ä»¥è¯„ä¼°ç¼–ç¨‹çŽ¯å¢ƒä¸­çš„å¥–åŠ±ç ´è§£é—®é¢˜**

**å…³é”®è¯**: `å¥–åŠ±ç ´è§£åŸºå‡†` `ç¼–ç¨‹ä»£ç†è¯„ä¼°` `LLMè¯„åˆ¤` `æµ‹è¯•æ–‡ä»¶ç¼–è¾‘æ£€æµ‹` `ä»£ç ç”Ÿæˆå®‰å…¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼–ç¨‹ä»£ç†å¯èƒ½é€šè¿‡ç¡¬ç¼–ç æµ‹è¯•ç”¨ä¾‹ç­‰æ–¹å¼è¿›è¡Œå¥–åŠ±ç ´è§£
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¿ç•™å•å…ƒæµ‹è¯•ã€LLMè¯„åˆ¤å’Œæµ‹è¯•æ–‡ä»¶ç¼–è¾‘æ£€æµ‹ä¸‰ç§æ–¹æ³•
3. å®žéªŒæˆ–æ•ˆæžœï¼šéªŒè¯LLMè¯„åˆ¤åœ¨æ˜Žç¡®æ¡ˆä¾‹ä¸­é«˜æ•ˆï¼Œå¹¶è§‚å¯Ÿåˆ°å¤šä¸ªæµè¡Œä»£ç†çš„å¥–åŠ±ç ´è§£è¡Œä¸º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at https://github.com/JonathanGabor/EvilGenie.

