---
layout: default
title: TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos
---

# TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos

**arXiv**: [2511.21690v1](https://arxiv.org/abs/2511.21690) | [PDF](https://arxiv.org/pdf/2511.21690.pdf)

**ä½œè€…**: Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTraceGenåœ¨3Dè½¨è¿¹ç©ºé—´å»ºæ¨¡ä¸–ç•Œï¼Œä»¥ä»Žè·¨å…·èº«è§†é¢‘ä¸­å­¦ä¹ æœºå™¨äººä»»åŠ¡**

**å…³é”®è¯**: `3Dè½¨è¿¹ç©ºé—´` `è·¨å…·èº«å­¦ä¹ ` `ä¸–ç•Œå»ºæ¨¡` `æœºå™¨äººä»»åŠ¡å­¦ä¹ ` `è§†é¢‘æ•°æ®è½¬æ¢` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žå°‘é‡æ¼”ç¤ºå­¦ä¹ æ–°æœºå™¨äººä»»åŠ¡æ—¶ï¼Œè·¨å…·èº«ã€çŽ¯å¢ƒå’Œä»»åŠ¡çš„è§†é¢‘å·®å¼‚é˜»ç¢ç›´æŽ¥åˆ©ç”¨
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥3Dè½¨è¿¹ç©ºé—´ä½œä¸ºç¬¦å·è¡¨ç¤ºï¼ŒæŠ½è±¡å¤–è§‚ä¿ç•™å‡ ä½•ç»“æž„ï¼Œé¢„æµ‹æœªæ¥è¿åŠ¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»…ç”¨äº”ä¸ªç›®æ ‡è§†é¢‘ï¼Œåœ¨çœŸå®žæœºå™¨äººä¸Šè¾¾åˆ°80%æˆåŠŸçŽ‡ï¼ŒæŽ¨ç†é€Ÿåº¦æå‡50-600å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

