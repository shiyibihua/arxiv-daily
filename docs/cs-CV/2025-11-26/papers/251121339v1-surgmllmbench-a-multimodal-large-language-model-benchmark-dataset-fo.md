---
layout: default
title: SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding
---

# SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.21339" target="_blank" class="toolbar-btn">arXiv: 2511.21339v1</a>
    <a href="https://arxiv.org/pdf/2511.21339.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21339v1" 
            onclick="toggleFavorite(this, '2511.21339v1', 'SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Tae-Min Choi, Tae Kyeong Jeong, Garam Kim, Jaemin Lee, Yeongyoon Koh, In Cheul Choi, Jae-Ho Chung, Jong Woong Park, Juyoun Park

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-26

**Â§áÊ≥®**: 10 pages, 5 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SurgMLLMBenchÔºöÁî®‰∫éÊâãÊúØÂú∫ÊôØÁêÜËß£ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂü∫ÂáÜÊï∞ÊçÆÈõÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÊâãÊúØÂú∫ÊôØÁêÜËß£` `ËßÜËßâÈóÆÁ≠î` `ÂÉèÁ¥†Á∫ßÂàÜÂâ≤`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊâãÊúØÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÈááÁî®VQAÊ†ºÂºèÔºåÂàÜÁ±ª‰ΩìÁ≥ª‰∏çÁªü‰∏ÄÔºåÁº∫‰πèÂÉèÁ¥†Á∫ßÂàÜÂâ≤ÊîØÊåÅÔºåÈôêÂà∂‰∫ÜÂ§öÊ®°ÊÄÅLLMÁöÑËØÑ‰º∞ÂíåÂ∫îÁî®„ÄÇ
2. SurgMLLMBenchÈÄöËøáÁªü‰∏ÄÁöÑÂàÜÁ±ª‰ΩìÁ≥ªÊï¥Âêà‰∫ÜÂ§öÁßçÊâãÊúØÊñπÂºèÁöÑÂÉèÁ¥†Á∫ßÂô®Ê¢∞ÂàÜÂâ≤Êé©Á†ÅÂíåÁªìÊûÑÂåñVQAÊ≥®ÈáäÔºåÊîØÊåÅÊõ¥ÂÖ®Èù¢ÁöÑËØÑ‰º∞Âíå‰∫§‰∫í„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂú®SurgMLLMBench‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®‰∏çÂêåÊâãÊúØÈ¢ÜÂüüË°®Áé∞‰∏ÄËá¥ÔºåÂπ∂ËÉΩÊúâÊïàÊ≥õÂåñÂà∞Êñ∞ÁöÑÊï∞ÊçÆÈõÜ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊúÄÊñ∞ËøõÂ±ïÂá∏Êòæ‰∫ÜÂÖ∂Âú®ÂåªÁñóÂíåÂ§ñÁßëÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÊâãÊúØÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÈááÁî®ËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÊ†ºÂºèÔºåÂÖ∑ÊúâÂºÇÊûÑÁöÑÂàÜÁ±ª‰ΩìÁ≥ªÔºåÂπ∂‰∏îÁº∫‰πèÂØπÂÉèÁ¥†Á∫ßÂàÜÂâ≤ÁöÑÊîØÊåÅÔºåÈôêÂà∂‰∫Ü‰∏ÄËá¥ÁöÑËØÑ‰º∞ÂíåÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSurgMLLMBenchÔºåËøôÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÔºå‰∏ìÈó®Áî®‰∫éÂºÄÂèëÂíåËØÑ‰º∞Áî®‰∫éÊâãÊúØÂú∫ÊôØÁêÜËß£ÁöÑ‰∫§‰∫íÂºèÂ§öÊ®°ÊÄÅLLMÔºåÂåÖÊã¨Êñ∞Êî∂ÈõÜÁöÑÊòæÂæÆÂ§ñÁßë‰∫∫Â∑•Ë°ÄÁÆ°ÂêªÂêàÔºàMAVISÔºâÊï∞ÊçÆÈõÜ„ÄÇÂÆÉÂú®Áªü‰∏ÄÁöÑÂàÜÁ±ª‰ΩìÁ≥ª‰∏ãÊï¥Âêà‰∫ÜËÖπËÖîÈïú„ÄÅÊú∫Âô®‰∫∫ËæÖÂä©ÂíåÊòæÂæÆÂ§ñÁßëÈ¢ÜÂüüÁöÑÂÉèÁ¥†Á∫ßÂô®Ê¢∞ÂàÜÂâ≤Êé©Á†ÅÂíåÁªìÊûÑÂåñVQAÊ≥®ÈáäÔºå‰ªéËÄåËÉΩÂ§üËøõË°åË∂ÖË∂ä‰º†ÁªüVQA‰ªªÂä°ÁöÑÂÖ®Èù¢ËØÑ‰º∞ÔºåÂπ∂ÂÆûÁé∞Êõ¥‰∏∞ÂØåÁöÑËßÜËßâÂØπËØù‰∫§‰∫í„ÄÇÂπøÊ≥õÁöÑÂü∫Á∫øÂÆûÈ™åË°®ÊòéÔºåÂú®SurgMLLMBench‰∏äËÆ≠ÁªÉÁöÑÂçï‰∏™Ê®°ÂûãÂèØ‰ª•Âú®‰∏çÂêåÈ¢ÜÂüüÂÆûÁé∞‰∏ÄËá¥ÁöÑÊÄßËÉΩÔºåÂπ∂ÊúâÊïàÂú∞Êé®ÂπøÂà∞Êú™ËßÅËøáÁöÑÊï∞ÊçÆÈõÜ„ÄÇSurgMLLMBenchÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏ÉÔºå‰Ωú‰∏∫‰∏Ä‰∏™Âº∫Â§ßÁöÑËµÑÊ∫êÔºå‰ª•Êé®ËøõÂ§öÊ®°ÊÄÅÊâãÊúØAIÁ†îÁ©∂ÔºåÊîØÊåÅÂèØÈáçÂ§çÁöÑËØÑ‰º∞Âíå‰∫§‰∫íÂºèÊâãÊúØÊé®ÁêÜÊ®°ÂûãÁöÑÂºÄÂèë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊâãÊúØÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÈááÁî®ËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÊ†ºÂºèÔºå‰∏î‰∏çÂêåÊï∞ÊçÆÈõÜÁöÑÊ†áÊ≥®‰ΩìÁ≥ª‰∏çÁªü‰∏ÄÔºåÁº∫‰πèÂÉèÁ¥†Á∫ßÂà´ÁöÑÂàÜÂâ≤‰ø°ÊÅØÔºåËøô‰ΩøÂæóËÆ≠ÁªÉÂíåËØÑ‰º∞Áî®‰∫éÊâãÊúØÂú∫ÊôØÁêÜËß£ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂèòÂæóÂõ∞Èöæ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ËøõË°å‰∏ÄËá¥ÊÄßËØÑ‰º∞ÔºåÂπ∂‰∏îÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®‰∏çÂêåÊâãÊúØÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÊï∞ÊçÆÈõÜSurgMLLMBenchÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´Â§öÁßçÊâãÊúØÊñπÂºèÔºàËÖπËÖîÈïú„ÄÅÊú∫Âô®‰∫∫ËæÖÂä©„ÄÅÊòæÂæÆÂ§ñÁßëÔºâÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÊï∞ÊçÆÔºåÂπ∂Êèê‰æõÁªü‰∏ÄÁöÑÊ†áÊ≥®‰ΩìÁ≥ªÔºåÂåÖÊã¨ÂÉèÁ¥†Á∫ßÂà´ÁöÑÂô®Ê¢∞ÂàÜÂâ≤Êé©Á†ÅÂíåÁªìÊûÑÂåñÁöÑVQAÊ≥®Èáä„ÄÇÈÄöËøáÁªü‰∏ÄÁöÑÊï∞ÊçÆÊ†ºÂºèÂíåÊ†áÊ≥®ÔºåÂèØ‰ª•Êõ¥Êñπ‰æøÂú∞ËÆ≠ÁªÉÂíåËØÑ‰º∞Â§öÊ®°ÊÄÅLLMÂú®ÊâãÊúØÂú∫ÊôØÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSurgMLLMBenchÊï∞ÊçÆÈõÜÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöËÖπËÖîÈïúÊâãÊúØÊï∞ÊçÆ„ÄÅÊú∫Âô®‰∫∫ËæÖÂä©ÊâãÊúØÊï∞ÊçÆÂíåÊòæÂæÆÂ§ñÁßëÊâãÊúØÊï∞ÊçÆÔºàMAVISÔºâ„ÄÇÂØπ‰∫éÊØèÁßçÊâãÊúØÊï∞ÊçÆÔºåÈÉΩÊèê‰æõ‰∫ÜÂÉèÁ¥†Á∫ßÂà´ÁöÑÂô®Ê¢∞ÂàÜÂâ≤Êé©Á†ÅÂíåÁªìÊûÑÂåñÁöÑVQAÊ≥®Èáä„ÄÇÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÊ†áÊ≥®ÂíåÈ™åËØÅ‰∏â‰∏™Èò∂ÊÆµ„ÄÇÊ†áÊ≥®ËøáÁ®ãÈááÁî®Áªü‰∏ÄÁöÑÂàÜÁ±ª‰ΩìÁ≥ªÔºåÁ°Æ‰øù‰∏çÂêåÊâãÊúØÊñπÂºèÁöÑÊï∞ÊçÆÂÖ∑ÊúâÂèØÊØîÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSurgMLLMBenchÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÂÆÉÊï¥Âêà‰∫ÜÂ§öÁßçÊâãÊúØÊñπÂºèÁöÑÊï∞ÊçÆÔºåÂπ∂Êèê‰æõ‰∫ÜÂÉèÁ¥†Á∫ßÂà´ÁöÑÂô®Ê¢∞ÂàÜÂâ≤Êé©Á†ÅÂíåÁªìÊûÑÂåñÁöÑVQAÊ≥®Èáä„ÄÇËøôÁßçÁªü‰∏ÄÁöÑÊï∞ÊçÆÊ†ºÂºèÂíåÊ†áÊ≥®‰ΩìÁ≥ª‰ΩøÂæóÂèØ‰ª•Êõ¥Êñπ‰æøÂú∞ËÆ≠ÁªÉÂíåËØÑ‰º∞Â§öÊ®°ÊÄÅLLMÂú®ÊâãÊúØÂú∫ÊôØÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõÔºåÂπ∂‰øÉËøõ‰∫Ü‰∏çÂêåÊ®°Âûã‰πãÈó¥ÁöÑÊØîËæÉÂíåÂàÜÊûê„ÄÇÊ≠§Â§ñÔºåÊñ∞Êî∂ÈõÜÁöÑMAVISÊï∞ÊçÆÈõÜ‰πü‰∏∫ÊòæÂæÆÂ§ñÁßëÊâãÊúØÂú∫ÊôØÁêÜËß£Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊï∞ÊçÆËµÑÊ∫ê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSurgMLLMBenchÊï∞ÊçÆÈõÜÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Áªü‰∏ÄÁöÑÂàÜÁ±ª‰ΩìÁ≥ªÔºåÁî®‰∫éÊ†áÊ≥®‰∏çÂêåÊâãÊúØÊñπÂºèÁöÑÊï∞ÊçÆÔºõ2) ÂÉèÁ¥†Á∫ßÂà´ÁöÑÂô®Ê¢∞ÂàÜÂâ≤Êé©Á†ÅÔºåÁî®‰∫éÊèê‰æõÊõ¥Á≤æÁªÜÁöÑËßÜËßâ‰ø°ÊÅØÔºõ3) ÁªìÊûÑÂåñÁöÑVQAÊ≥®ÈáäÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂØπÊâãÊúØÂú∫ÊôØÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÂèñÂÜ≥‰∫é‰ΩøÁî®ÁöÑÂ§öÊ®°ÊÄÅLLMÊ®°ÂûãÔºåËÆ∫Êñá‰∏ªË¶ÅÂÖ≥Ê≥®Êï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ÂíåËØÑ‰º∞ÔºåËÄåÈùûÁâπÂÆöÊ®°ÂûãÁöÑ‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®SurgMLLMBench‰∏äËÆ≠ÁªÉÁöÑÂçï‰∏™Ê®°ÂûãÂèØ‰ª•Âú®‰∏çÂêåÊâãÊúØÈ¢ÜÂüüÂÆûÁé∞‰∏ÄËá¥ÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îËÉΩÂ§üÊúâÊïàÂú∞Ê≥õÂåñÂà∞Êú™ËßÅËøáÁöÑÊï∞ÊçÆÈõÜ„ÄÇËøôË°®ÊòéSurgMLLMBenchÊòØ‰∏Ä‰∏™ÊúâÊïàÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞Â§öÊ®°ÊÄÅLLMÂú®ÊâãÊúØÂú∫ÊôØÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶ÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂºÄÂèëÊô∫ËÉΩÊâãÊúØËæÖÂä©Á≥ªÁªüÔºå‰æãÂ¶ÇÔºåÈÄöËøáÁêÜËß£ÊâãÊúØÂú∫ÊôØÔºå‰∏∫ÂåªÁîüÊèê‰æõÂÆûÊó∂ÁöÑÂô®Ê¢∞ÂÆö‰Ωç„ÄÅÊìç‰ΩúÂª∫ËÆÆÂíåÈ£éÈô©È¢ÑË≠¶„ÄÇÊ≠§Â§ñÔºåËØ•Êï∞ÊçÆÈõÜËøòÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÊú∫Âô®‰∫∫ÊâãÊúØÁ≥ªÁªüÔºåÊèêÈ´òÊâãÊúØÁöÑÁ≤æÂáÜÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®ÊâãÊúØAIÁöÑÊô∫ËÉΩÂåñÂèëÂ±ïÔºåÊèêÂçáÂåªÁñóÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.

