---
layout: default
title: PG-ControlNet: A Physics-Guided ControlNet for Generative Spatially Varying Image Deblurring
---

# PG-ControlNet: A Physics-Guided ControlNet for Generative Spatially Varying Image Deblurring

**arXiv**: [2511.21043v1](https://arxiv.org/abs/2511.21043) | [PDF](https://arxiv.org/pdf/2511.21043.pdf)

**ä½œè€…**: Hakki Motorcu, Mujdat Cetin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPG-ControlNetä»¥è§£å†³ç©ºé—´å˜åŒ–å›¾åƒåŽ»æ¨¡ç³Šé—®é¢˜ï¼Œç»“åˆç‰©ç†çº¦æŸä¸Žç”Ÿæˆæ¨¡åž‹**

**å…³é”®è¯**: `å›¾åƒåŽ»æ¨¡ç³Š` `ç©ºé—´å˜åŒ–æ¨¡ç³Š` `ç‰©ç†å¼•å¯¼ç”Ÿæˆ` `ControlNet` `æ‰©æ•£æ¨¡åž‹` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç©ºé—´å˜åŒ–å›¾åƒåŽ»æ¨¡ç³Šæ˜¯ç—…æ€é—®é¢˜ï¼Œå°¤å…¶åœ¨å™ªå£°å’Œå¤æ‚æ¨¡ç³Šä¸‹ï¼ŒçŽ°æœ‰æ–¹æ³•æ˜“äº§ç”Ÿä¼ªå½±æˆ–å¹»è§‰ç»†èŠ‚
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºæ¨¡å¯†é›†è¿žç»­æ¨¡ç³Šæ ¸ï¼Œé€šè¿‡ControlNetæž¶æž„å¼ºå¼•å¯¼æ‰©æ•£é‡‡æ ·ï¼Œèžåˆç‰©ç†çº¦æŸä¸Žç”Ÿæˆå…ˆéªŒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸¥é‡æ¨¡ç³Šåœºæ™¯ä¸­ï¼Œä¼˜äºŽåŸºäºŽæ¨¡åž‹å’Œç”ŸæˆåŸºçº¿æ–¹æ³•ï¼Œå¹³è¡¡ç‰©ç†å‡†ç¡®æ€§ä¸Žæ„ŸçŸ¥çœŸå®žæ„Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatially varying image deblurring remains a fundamentally ill-posed problem, especially when degradations arise from complex mixtures of motion and other forms of blur under significant noise. State-of-the-art learning-based approaches generally fall into two paradigms: model-based deep unrolling methods that enforce physical constraints by modeling the degradations, but often produce over-smoothed, artifact-laden textures, and generative models that achieve superior perceptual quality yet hallucinate details due to weak physical constraints. In this paper, we propose a novel framework that uniquely reconciles these paradigms by taming a powerful generative prior with explicit, dense physical constraints. Rather than oversimplifying the degradation field, we model it as a dense continuum of high-dimensional compressed kernels, ensuring that minute variations in motion and other degradation patterns are captured. We leverage this rich descriptor field to condition a ControlNet architecture, strongly guiding the diffusion sampling process. Extensive experiments demonstrate that our method effectively bridges the gap between physical accuracy and perceptual realism, outperforming state-of-the-art model-based methods as well as generative baselines in challenging, severely blurred scenarios.

