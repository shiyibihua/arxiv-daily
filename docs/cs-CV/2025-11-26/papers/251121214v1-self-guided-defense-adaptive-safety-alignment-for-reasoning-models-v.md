---
layout: default
title: Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines
---

# Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines

**arXiv**: [2511.21214v1](https://arxiv.org/abs/2511.21214) | [PDF](https://arxiv.org/pdf/2511.21214.pdf)

**ä½œè€…**: Yuhang Wang, Yanxu Zhu, Dongyuan Lu, Jitao Sang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSGASAæ¡†æž¶ä»¥å¢žå¼ºæŽ¨ç†æ¨¡åž‹å¯¹å¯¹æŠ—æ€§è¶Šç‹±æç¤ºçš„å®‰å…¨é˜²å¾¡**

**å…³é”®è¯**: `æŽ¨ç†æ¨¡åž‹å®‰å…¨` `è‡ªé€‚åº”å¯¹é½` `å¯¹æŠ—æ€§é˜²å¾¡` `æŒ‡å—åˆæˆ` `å¾®è°ƒä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹æŠ—æ€§è¶Šç‹±æç¤ºéšè”½æ¬ºéª—ï¼Œæ˜“ç»•è¿‡å®‰å…¨æœºåˆ¶ç”Ÿæˆæœ‰å®³å†…å®¹
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ•°æ®é¢„åˆæˆç”Ÿæˆå®‰å…¨æŒ‡å—ï¼Œç»“åˆSFTå’ŒDPOè¿›è¡Œå¯¹é½å¾®è°ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šæ•°æ®é›†å®žéªŒæ˜¾ç¤ºSGASAæ˜¾è‘—æå‡æ¨¡åž‹å®‰å…¨æ€§å’Œé€‚åº”æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.

