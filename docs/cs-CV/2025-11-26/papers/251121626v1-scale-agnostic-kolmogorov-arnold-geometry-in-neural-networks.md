---
layout: default
title: Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks
---

# Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks

**arXiv**: [2511.21626v1](https://arxiv.org/abs/2511.21626) | [PDF](https://arxiv.org/pdf/2511.21626.pdf)

**ä½œè€…**: Mathew Vanherreweghe, Michael H. Freedman, Keith M. Adams

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å‘çŽ°å¤šå±‚æ„ŸçŸ¥å™¨åœ¨MNISTåˆ†ç±»ä¸­è‡ªå‘å½¢æˆå°ºåº¦æ— å…³çš„Kolmogorov-Arnoldå‡ ä½•ç»“æž„**

**å…³é”®è¯**: `Kolmogorov-Arnoldå‡ ä½•` `å¤šå±‚æ„ŸçŸ¥å™¨` `MNISTåˆ†ç±»` `å°ºåº¦æ— å…³æ€§` `ç©ºé—´åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šKolmogorov-Arnoldå‡ ä½•æ˜¯å¦åœ¨çœŸå®žé«˜ç»´æ•°æ®ä¸­æŒç»­å­˜åœ¨åŠå…¶ç©ºé—´ç‰¹æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨2å±‚MLPå¯¹MNISTè¿›è¡Œå¤šå°ºåº¦ç©ºé—´åˆ†æžï¼Œæ¶µç›–å±€éƒ¨åˆ°å…¨å±€
3. å®žéªŒæˆ–æ•ˆæžœï¼šKAGåœ¨è®­ç»ƒä¸­è‡ªå‘å‡ºçŽ°ï¼Œå°ºåº¦æ— å…³æ€§åœ¨ä¸åŒè®­ç»ƒæ–¹æ³•ä¸­ä¸€è‡´

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
>   We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.

