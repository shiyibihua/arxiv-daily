---
layout: default
title: BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model
---

# BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model

**arXiv**: [2511.20956v1](https://arxiv.org/abs/2511.20956) | [PDF](https://arxiv.org/pdf/2511.20956.pdf)

**ä½œè€…**: Rawa Mohammed, Mina Attin, Bryar Shareef

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBUSTRæ¡†æž¶ï¼Œé€šè¿‡æè¿°ç¬¦æ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡åž‹ç”Ÿæˆä¹³è…ºè¶…å£°æŠ¥å‘Šï¼Œæ— éœ€é…å¯¹å›¾åƒ-æŠ¥å‘Šæ•°æ®ã€‚**

**å…³é”®è¯**: `ä¹³è…ºè¶…å£°æŠ¥å‘Šç”Ÿæˆ` `æè¿°ç¬¦æ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šä»»åŠ¡å­¦ä¹ ` `è§†è§‰æ–‡æœ¬å¯¹é½` `æ— é…å¯¹æ•°æ®è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¹³è…ºè¶…å£°è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆç¼ºä¹é…å¯¹æ•°æ®é›†ï¼Œä¸”å¤§è¯­è¨€æ¨¡åž‹æ˜“äº§ç”Ÿå¹»è§‰ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤šä»»åŠ¡Swinç¼–ç å™¨å­¦ä¹ æè¿°ç¬¦æ„ŸçŸ¥è§†è§‰è¡¨ç¤ºï¼Œç»“åˆåŒçº§ç›®æ ‡å¯¹é½è§†è§‰ä¸Žæ–‡æœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨BrEaSTå’ŒBUS-BRAæ•°æ®é›†ä¸Šæå‡è‡ªç„¶è¯­è¨€ç”Ÿæˆå’Œä¸´åºŠç–—æ•ˆæŒ‡æ ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR

