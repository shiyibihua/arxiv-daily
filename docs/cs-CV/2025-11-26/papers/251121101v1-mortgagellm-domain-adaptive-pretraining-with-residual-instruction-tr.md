---
layout: default
title: MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing
---

# MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing

**arXiv**: [2511.21101v1](https://arxiv.org/abs/2511.21101) | [PDF](https://arxiv.org/pdf/2511.21101.pdf)

**ä½œè€…**: Manish Jain, Satheesh Kumar Ponnambalam, Salman Faroz, Chandrakanth Lns, Vinay Sharma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMortgageLLMä»¥è§£å†³æŠµæŠ¼é‡‘èžé¢†åŸŸå¤§è¯­è¨€æ¨¡åž‹ä¸“ä¸šæ€§ä¸ŽæŒ‡ä»¤éµå¾ªçš„å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒ` `æ®‹å·®æŒ‡ä»¤æŠ€æœ¯` `åŒä¸“å®¶æž¶æž„` `ä»»åŠ¡è·¯ç”±æœºåˆ¶` `æŠµæŠ¼é‡‘èž` `å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨æŠµæŠ¼é‡‘èžç­‰ä¸“ä¸šé¢†åŸŸåº”ç”¨æ—¶ï¼Œéœ€å¢žå¼ºä¸“ä¸šçŸ¥è¯†åŒæ—¶ä¿æŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒä¸“å®¶æž¶æž„ï¼Œç»“åˆæ®‹å·®æŒ‡ä»¤æŠ€æœ¯ï¼Œå®žçŽ°é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒä¸Žä»»åŠ¡è·¯ç”±
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡åž‹åœ¨æ‘˜è¦ã€é—®ç­”å’Œåˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºŽåŸºçº¿æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.

