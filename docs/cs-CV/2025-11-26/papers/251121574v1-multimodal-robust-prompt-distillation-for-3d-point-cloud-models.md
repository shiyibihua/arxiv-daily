---
layout: default
title: Multimodal Robust Prompt Distillation for 3D Point Cloud Models
---

# Multimodal Robust Prompt Distillation for 3D Point Cloud Models

**arXiv**: [2511.21574v1](https://arxiv.org/abs/2511.21574) | [PDF](https://arxiv.org/pdf/2511.21574.pdf)

**ä½œè€…**: Xiang Gu, Liming Lu, Xu Zheng, Anan Du, Yongbin Zhou, Shuchao Pang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€é²æ£’æç¤ºè’¸é¦æ¡†æž¶ä»¥å¢žå¼º3Dç‚¹äº‘æ¨¡åž‹å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§**

**å…³é”®è¯**: `3Dç‚¹äº‘æ¨¡åž‹` `å¯¹æŠ—æ”»å‡»é˜²å¾¡` `å¤šæ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `é²æ£’æ€§å¢žå¼º` `æç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹æŠ—æ”»å‡»å¨èƒ3Dç‚¹äº‘æ¨¡åž‹å¯é æ€§ï¼ŒçŽ°æœ‰é˜²å¾¡æ–¹æ³•è®¡ç®—å¼€é”€é«˜ä¸”æ³›åŒ–èƒ½åŠ›å·®
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¡†æž¶ï¼Œé€šè¿‡å¤šæ¨¡æ€ç‰¹å¾å¯¹é½å’Œç½®ä¿¡é—¨æŽ§æœºåˆ¶è’¸é¦é²æ£’æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§ç™½ç›’å’Œé»‘ç›’æ”»å‡»ä¸‹ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒæŽ¨ç†æ—¶æ— é¢å¤–è®¡ç®—æˆæœ¬

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.

