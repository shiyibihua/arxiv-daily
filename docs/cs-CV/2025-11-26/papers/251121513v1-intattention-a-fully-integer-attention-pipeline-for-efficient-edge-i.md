---
layout: default
title: IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference
---

# IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference

**arXiv**: [2511.21513v1](https://arxiv.org/abs/2511.21513) | [PDF](https://arxiv.org/pdf/2511.21513.pdf)

**ä½œè€…**: Wanli Zhong, Haibo Feng, Zirui Zhou, Hanyang Peng, Shiqi Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIntAttentionå…¨æ•´æ•°æ³¨æ„åŠ›ç®¡é“ï¼Œè§£å†³è¾¹ç¼˜è®¾å¤‡TransformeræŽ¨ç†ä¸­softmaxç“¶é¢ˆé—®é¢˜**

**å…³é”®è¯**: `æ•´æ•°æ³¨æ„åŠ›` `è¾¹ç¼˜æŽ¨ç†` `Transformerä¼˜åŒ–` `è½¯æœ€å¤§é‡åŒ–` `ç¡¬ä»¶åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šINT8é‡åŒ–ä¸­softmaxé˜¶æ®µéœ€æµ®ç‚¹è®¡ç®—ï¼Œå¯¼è‡´å»¶è¿Ÿå’Œèƒ½è€—ç“¶é¢ˆï¼Œå æ³¨æ„åŠ›å»¶è¿Ÿé«˜è¾¾65%
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨IndexSoftmaxç¡¬ä»¶å‹å¥½ç®—å­ï¼Œç»“åˆæŸ¥æ‰¾è¡¨è¿‘ä¼¼å’Œæ•´æ•°å½’ä¸€åŒ–ï¼Œå®žçŽ°å…¨æ•´æ•°å¤„ç†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Armv8 CPUä¸Šï¼Œç›¸æ¯”FP16åŸºçº¿æé€Ÿ3.7å€ã€èƒ½è€—é™61%ï¼Œç²¾åº¦ä¸ŽåŸºçº¿ç›¸å½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can account for up to 65% of total attention latency and disrupts the end-to-end integer dataflow critical for edge hardware efficiency. To address this limitation, we present IntAttention, the first fully integer, plug-and-play attention pipeline without retraining. At the core of our approach lies IndexSoftmax, a hardware-friendly operator that replaces floating-point exponentials entirely within the integer domain. IntAttention integrates sparsity-aware clipping, a 32-entry lookup-table approximation, and direct integer normalization, thereby eliminating all datatype conversion overhead. We evaluate IntAttention and demonstrate consistent and substantial gains. Our method achieves up to 3.7x speedup and 61% energy reduction over FP16 baselines and 2.0x faster than conventional INT8 attention pipelines on Armv8 CPUs. These gains are achieved with high-fidelity accuracy comparable to baselines across diverse language and vision models, enabling practical and efficient Transformer inference on commodity edge devices. Code will be released in later version of this work.

