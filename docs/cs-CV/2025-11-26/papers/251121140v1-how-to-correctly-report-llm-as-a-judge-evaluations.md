---
layout: default
title: How to Correctly Report LLM-as-a-Judge Evaluations
---

# How to Correctly Report LLM-as-a-Judge Evaluations

**arXiv**: [2511.21140v1](https://arxiv.org/abs/2511.21140) | [PDF](https://arxiv.org/pdf/2511.21140.pdf)

**ä½œè€…**: Chungpa Lee, Thomas Zeng, Jongwon Jeong, Jy-yong Sohn, Kangwook Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ’ä»¶æ¡†æž¶ä»¥è§£å†³LLMä½œä¸ºè¯„ä¼°è€…æ—¶çš„åå·®å’Œç½®ä¿¡åŒºé—´æž„å»ºé—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°` `åå·®æ ¡æ­£` `ç½®ä¿¡åŒºé—´æž„å»º` `æ ¡å‡†æ ·æœ¬åˆ†é…` `ç»Ÿè®¡æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMä½œä¸ºè¯„ä¼°è€…æ—¶ï¼Œå› ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§ä¸å®Œç¾Žå¯¼è‡´å‡†ç¡®åº¦ä¼°è®¡åå·®å’Œä¸ç¡®å®šæ€§
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘æ’ä»¶æ¡†æž¶è¿›è¡Œåå·®æ ¡æ­£ï¼Œå¹¶æž„å»ºåæ˜ æµ‹è¯•å’Œæ ¡å‡†æ•°æ®é›†ä¸ç¡®å®šæ€§çš„ç½®ä¿¡åŒºé—´
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¼•å…¥è‡ªé€‚åº”ç®—æ³•ä¼˜åŒ–æ ¡å‡†æ ·æœ¬åˆ†é…ï¼Œé™ä½Žå‡†ç¡®åº¦ä¼°è®¡çš„ä¸ç¡®å®šæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.

