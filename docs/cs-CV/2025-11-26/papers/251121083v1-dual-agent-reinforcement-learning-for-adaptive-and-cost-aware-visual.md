---
layout: default
title: Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry
---

# Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry

**arXiv**: [2511.21083v1](https://arxiv.org/abs/2511.21083) | [PDF](https://arxiv.org/pdf/2511.21083.pdf)

**ä½œè€…**: Feiyang Pan, Shenghe Zheng, Chunyan Yin, Guangbin Dou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œè‡ªé€‚åº”ä¼˜åŒ–è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ç²¾åº¦ä¸Žæ•ˆçŽ‡**

**å…³é”®è¯**: `è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡` `å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”æŽ§åˆ¶` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–` `æœºå™¨äººå¯¼èˆª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡åœ¨ç²¾åº¦ä¸Žè®¡ç®—æ•ˆçŽ‡é—´å­˜åœ¨æƒè¡¡ï¼Œä¼˜åŒ–æ–¹æ³•è®¡ç®—æˆæœ¬é«˜
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è½»é‡çº§å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ™ºèƒ½æŽ§åˆ¶è§†è§‰å‰ç«¯æ‰§è¡Œä¸ŽçŠ¶æ€èžåˆ
3. å®žéªŒæ•ˆæžœï¼šåœ¨EuRoCå’ŒTUM-VIæ•°æ®é›†ä¸Šï¼Œå®žçŽ°æ›´é«˜ç²¾åº¦ã€æ›´å¿«é€Ÿåº¦å’Œæ›´ä½Žå†…å­˜å ç”¨

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality. Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms. Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked. To this end, we cast two key design choices in modern VIO, when to run the visual frontend and how strongly to trust its output, as sequential decision problems, and solve them with lightweight reinforcement learning (RL) agents. Our framework introduces a lightweight, dual-pronged RL policy that serves as our core contribution: (1) a Select Agent intelligently gates the entire VO pipeline based only on high-frequency IMU data; and (2) a composite Fusion Agent that first estimates a robust velocity state via a supervised network, before an RL policy adaptively fuses the full (p, v, q) state. Experiments on the EuRoC MAV and TUM-VI datasets show that, in our unified evaluation, the proposed method achieves a more favorable accuracy-efficiency-memory trade-off than prior GPU-based VO/VIO systems: it attains the best average ATE while running up to 1.77 times faster and using less GPU memory. Compared to classical optimization-based VIO systems, our approach maintains competitive trajectory accuracy while substantially reducing computational load.

