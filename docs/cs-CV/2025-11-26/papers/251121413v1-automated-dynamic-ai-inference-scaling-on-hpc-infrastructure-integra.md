---
layout: default
title: Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM
---

# Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM

**arXiv**: [2511.21413v1](https://arxiv.org/abs/2511.21413) | [PDF](https://arxiv.org/pdf/2511.21413.pdf)

**ä½œè€…**: Tim Trappen, Robert KeÃŸler, Roland Pabel, Viktor Achter, Stefan Wesner

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé›†æˆvLLMã€Slurmå’ŒKubernetesçš„è§£å†³æ–¹æ¡ˆï¼Œä»¥åœ¨HPCä¸Šé«˜æ•ˆæœåŠ¡åŠ¨æ€AIæŽ¨ç†ã€‚**

**å…³é”®è¯**: `AIæŽ¨ç†` `é«˜æ€§èƒ½è®¡ç®—` `Kubernetesé›†æˆ` `vLLM` `Slurm` `åŠ¨æ€æ‰©å±•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šHPCä¼ ç»Ÿæ¨¡å¼ä¸é€‚åº”åŒæ­¥ã€ç”¨æˆ·é¢åŠ¨æ€AIæŽ¨ç†çš„é«˜éœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨è¶…çº§è®¡ç®—æœºRAMSESä¸Šæ•´åˆvLLMã€Slurmå’ŒKubernetesã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œ100è‡³1000å¹¶å‘è¯·æ±‚ä¸‹å»¶è¿Ÿä»…å¢žçº¦500æ¯«ç§’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

