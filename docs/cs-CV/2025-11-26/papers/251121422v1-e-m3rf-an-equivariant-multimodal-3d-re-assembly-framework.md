---
layout: default
title: E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework
---

# E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework

**arXiv**: [2511.21422v1](https://arxiv.org/abs/2511.21422) | [PDF](https://arxiv.org/pdf/2511.21422.pdf)

**ä½œè€…**: Adeela Islam, Stefano Fiorini, Manuel Lecha, Theodore Tsesmelis, Stuart James, Pietro Morerio, Alessio Del Bue

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºE-M3RFæ¡†æž¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ç‰¹å¾è§£å†³3Dç¢Žç‰‡é‡ç»„çš„å‡ ä½•æ¨¡ç³Šé—®é¢˜**

**å…³é”®è¯**: `3Dé‡ç»„` `å¤šæ¨¡æ€ç‰¹å¾` `SE(3)æµåŒ¹é…` `æ—‹è½¬ç­‰å˜ç¼–ç ` `æ–‡åŒ–é—äº§ä¿®å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š3Dé‡ç»„ä¸­ä»…ä¾èµ–å‡ ä½•ç‰¹å¾æ˜“å—ç¢Žç‰‡å°ã€ä¾µèš€æˆ–å¯¹ç§°æ€§å½±å“ï¼Œä¸”ç¼ºä¹ç‰©ç†çº¦æŸé˜²æ­¢é‡å 
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå‡ ä½•å’Œé¢œè‰²ç‰¹å¾ï¼Œä½¿ç”¨SE(3)æµåŒ¹é…é¢„æµ‹å˜æ¢ï¼Œå®žçŽ°æ—‹è½¬ç­‰å˜ç¼–ç 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨RePAIRæ•°æ®é›†ä¸Šï¼Œæ—‹è½¬è¯¯å·®é™ä½Ž23.1%ï¼Œå¹³ç§»è¯¯å·®é™ä½Ž13.2%ï¼ŒChamferè·ç¦»å‡å°‘18.4%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

