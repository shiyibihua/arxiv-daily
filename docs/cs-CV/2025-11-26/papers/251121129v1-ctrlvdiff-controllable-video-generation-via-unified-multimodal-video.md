---
layout: default
title: CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion
---

# CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion

**arXiv**: [2511.21129v1](https://arxiv.org/abs/2511.21129) | [PDF](https://arxiv.org/pdf/2511.21129.pdf)

**ä½œè€…**: Dianbing Xi, Jiepeng Wang, Yuanzhi Liang, Xi Qiu, Jialun Liu, Hao Pan, Yuchi Huo, Rui Wang, Haibin Huang, Chi Zhang, Xuelong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCtrlVDiffç»Ÿä¸€æ‰©æ•£æ¨¡åž‹ä»¥è§£å†³å¯æŽ§è§†é¢‘ç”Ÿæˆä¸­çš„å¤šæ¨¡æ€èžåˆæŒ‘æˆ˜**

**å…³é”®è¯**: `å¯æŽ§è§†é¢‘ç”Ÿæˆ` `å¤šæ¨¡æ€èžåˆ` `æ‰©æ•£æ¨¡åž‹` `æ—¶é—´ä¸€è‡´æ€§` `å›¾å½¢æ¨¡æ€`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå‡ ä½•çº¿ç´¢ä¸è¶³ä»¥çº¦æŸå¤–è§‚å’Œå…‰ç…§ï¼Œå¯¼è‡´æ—¶é—´æ¼‚ç§»å’Œç¼–è¾‘é™åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ··åˆæ¨¡æ€æŽ§åˆ¶ç­–ç•¥èžåˆæ·±åº¦ã€æ³•çº¿ã€åˆ†å‰²ç­‰å›¾å½¢æ¨¡æ€
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°é«˜å¯æŽ§æ€§å’Œä¿çœŸåº¦ï¼Œæ”¯æŒåˆ†å±‚ç¼–è¾‘

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.
>   However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.
>   We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.

