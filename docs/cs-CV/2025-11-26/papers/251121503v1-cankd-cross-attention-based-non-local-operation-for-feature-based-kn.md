---
layout: default
title: CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation
---

# CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation

**arXiv**: [2511.21503v1](https://arxiv.org/abs/2511.21503) | [PDF](https://arxiv.org/pdf/2511.21503.pdf)

**ä½œè€…**: Shizhe Sun, Wataru Ohyama

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCanKDä»¥æ”¹è¿›ç‰¹å¾çŸ¥è¯†è’¸é¦ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶å¢žå¼ºåƒç´ çº§å…³ç³»æ•èŽ·**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è·¨æ³¨æ„åŠ›æœºåˆ¶` `ç‰¹å¾å¯¹é½` `ç›®æ ‡æ£€æµ‹` `å›¾åƒåˆ†å‰²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›è’¸é¦ç‹¬ç«‹å¯¹é½å¸ˆç”Ÿç‰¹å¾ï¼Œéš¾ä»¥å……åˆ†æ•æ‰åƒç´ é—´å…³ç³»
2. CanKDä½¿ç”¨è·¨æ³¨æ„åŠ›ï¼Œä½¿å­¦ç”Ÿç‰¹å¾åŠ¨æ€è€ƒè™‘æ•™å¸ˆç‰¹å¾çš„æ‰€æœ‰åƒç´ 
3. åœ¨ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒCanKDä¼˜äºŽçŽ°æœ‰è’¸é¦æ–¹æ³•ï¼Œä»…éœ€é¢å¤–æŸå¤±å‡½æ•°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD

