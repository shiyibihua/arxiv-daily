---
layout: default
title: Privacy in Federated Learning with Spiking Neural Networks
---

# Privacy in Federated Learning with Spiking Neural Networks

**arXiv**: [2511.21181v1](https://arxiv.org/abs/2511.21181) | [PDF](https://arxiv.org/pdf/2511.21181.pdf)

**ä½œè€…**: Dogukan Aksu, Jesus Martinez del Rincon, Ihsen Alouani

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å®žè¯ç ”ç©¶è„‰å†²ç¥žç»ç½‘ç»œåœ¨è”é‚¦å­¦ä¹ ä¸­çš„æ¢¯åº¦æ³„éœ²éšç§é£Žé™©**

**å…³é”®è¯**: `è„‰å†²ç¥žç»ç½‘ç»œ` `è”é‚¦å­¦ä¹ ` `æ¢¯åº¦æ³„éœ²æ”»å‡»` `éšç§ä¿æŠ¤` `æ›¿ä»£æ¢¯åº¦è®­ç»ƒ` `ç¥žç»å½¢æ€è®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè”é‚¦å­¦ä¹ ä¸­æ¢¯åº¦åè½¬æ”»å‡»å¨èƒéšç§ï¼Œè„‰å†²ç¥žç»ç½‘ç»œé£Žé™©æœªçŸ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ¢¯åº¦æ³„éœ²æ”»å‡»é€‚é…åˆ°è„‰å†²åŸŸï¼Œä½¿ç”¨æ›¿ä»£æ¢¯åº¦è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šSNNæ¢¯åº¦é‡å»ºå™ªå£°å¤§ï¼Œæ— æ³•æ¢å¤æ—¶ç©ºç»“æž„ï¼Œéšç§é£Žé™©ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.

