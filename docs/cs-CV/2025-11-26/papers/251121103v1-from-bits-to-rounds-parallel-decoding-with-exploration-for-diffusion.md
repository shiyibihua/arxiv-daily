---
layout: default
title: From Bits to Rounds: Parallel Decoding with Exploration for Diffusion Language Models
---

# From Bits to Rounds: Parallel Decoding with Exploration for Diffusion Language Models

**arXiv**: [2511.21103v1](https://arxiv.org/abs/2511.21103) | [PDF](https://arxiv.org/pdf/2511.21103.pdf)

**ä½œè€…**: Hengyu Fu, Baihe Huang, Virginia Adams, Charles Wang, Venkat Srinivasan, Jiantao Jiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŽ¢ç´¢-åˆ©ç”¨ç­–ç•¥ä»¥è§£å†³æ‰©æ•£è¯­è¨€æ¨¡åž‹å¹¶è¡Œè§£ç æ•ˆçŽ‡ç“¶é¢ˆ**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡åž‹` `å¹¶è¡Œè§£ç ` `ä¿¡æ¯ç“¶é¢ˆ` `æŽ¢ç´¢-åˆ©ç”¨ç­–ç•¥` `è§£ç æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ‡å‡†è§£ç ä¾èµ–é«˜ç½®ä¿¡åº¦è¯å…ƒï¼Œå­˜åœ¨ä¿¡æ¯ç“¶é¢ˆé™åˆ¶è§£ç è¿›åº¦
2. å¼•å…¥æŽ¢ç´¢-åˆ©ç”¨ç­–ç•¥ï¼Œç»“åˆè·¨å—è§£ç å’Œé«˜ä¸ç¡®å®šæ€§è¯å…ƒæŽ¢ç´¢
3. å®žéªŒæ˜¾ç¤ºå‡å°‘è§£ç è½®æ¬¡ï¼Œä¿æŒç”Ÿæˆè´¨é‡ï¼ŒéªŒè¯ç†è®ºç•Œé™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion Language Models (DLMs) have recently emerged as a strong alternative to autoregressive language models (LMs). DLMs offer comparable accuracy with faster inference speed via parallel decoding. However, standard DLM decoding strategies relying on high-confidence tokens encounter an inherent information-theoretic bottleneck that restricts decoding progress and ultimately slows generation. We demonstrate both theoretically and empirically that prioritizing high-confidence tokens is inherently inefficient. High-probability tokens carry negligible information and strictly relying on them limits the effective progress made in each decoding round. We prove that the number of decoding rounds must grow linearly with the sample's total information (negative log-likelihood) and inversely with the per-round information budget, establishing a bits-to-rounds principle. We also propose Explore-Then-Exploit (ETE), a training-free decoding strategy that maximizes information throughput and decoding efficiency. ETE combines cross-block decoding with targeted exploration of high-uncertainty tokens to reshape the conditional distribution and trigger cascades of confident predictions. Experiments verify our theoretical bounds and demonstrate that ETE consistently reduces the required number of decoding rounds compared to confidence-only baselines without compromising generation quality.

