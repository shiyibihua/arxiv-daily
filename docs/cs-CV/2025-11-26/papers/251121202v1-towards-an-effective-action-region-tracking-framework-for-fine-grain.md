---
layout: default
title: Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition
---

# Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition

**arXiv**: [2511.21202v1](https://arxiv.org/abs/2511.21202) | [PDF](https://arxiv.org/pdf/2511.21202.pdf)

**ä½œè€…**: Baoli Sun, Yihan Wang, Xinzhu Ma, Zhihui Wang, Kun Lu, Zhiyong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨ä½œåŒºåŸŸè·Ÿè¸ªæ¡†æž¶ä»¥è§£å†³ç»†ç²’åº¦è§†é¢‘åŠ¨ä½œè¯†åˆ«ä¸­å±€éƒ¨ç»†èŠ‚åŠ¨æ€è¿½è¸ªé—®é¢˜**

**å…³é”®è¯**: `ç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«` `åŠ¨ä½œåŒºåŸŸè·Ÿè¸ª` `æŸ¥è¯¢-å“åº”æœºåˆ¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `è½¨è¿¹å¯¹æ¯”çº¦æŸ` `è§†é¢‘ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰ç»†ç²’åº¦åŠ¨ä½œç±»åˆ«é—´å±€éƒ¨åŒºåŸŸéšæ—¶é—´æ¼”å˜çš„ç»†å¾®å·®å¼‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æŸ¥è¯¢-å“åº”æœºåˆ¶å’Œæ–‡æœ¬çº¦æŸè¯­ä¹‰æ¥å‘çŽ°å¹¶è·Ÿè¸ªåŠ¨ä½œç›¸å…³åŒºåŸŸï¼Œå½¢æˆåŠ¨ä½œè½¨è¿¹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽå…ˆå‰æœ€å…ˆè¿›åŸºçº¿ï¼ŒéªŒè¯äº†æ¡†æž¶æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories. However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time. In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions. Specifically, we propose a region-specific semantic activation module that employs discriminative and text-constrained semantics as queries to capture the most action-related region responses in each video frame, facilitating interaction among spatial and temporal dimensions with corresponding video features. The captured region responses are organized into action tracklets, which characterize region-based action dynamics by linking related responses across video frames in a coherent sequence. The text-constrained queries encode nuanced semantic representations derived from textual descriptions of action labels extracted by language branches within Visual Language Models (VLMs). To optimize the action tracklets, we design a multi-level tracklet contrastive constraint among region responses at spatial and temporal levels, enabling effective discrimination within each frame and correlation between adjacent frames. Additionally, a task-specific fine-tuning mechanism refines textual semantics such that semantic representations encoded by VLMs are preserved while optimized for task preferences. Comprehensive experiments on widely used action recognition benchmarks demonstrate the superiority to previous state-of-the-art baselines.

