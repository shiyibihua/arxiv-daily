---
layout: default
title: Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning
---

# Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning

**arXiv**: [2511.21081v1](https://arxiv.org/abs/2511.21081) | [PDF](https://arxiv.org/pdf/2511.21081.pdf)

**ä½œè€…**: Thura Aung, Eaint Kay Khaing Kyaw, Ye Kyaw Thu, Thazin Myint Oo, Thepchai Supnithi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKANåˆ†ç±»å¤´å¾®è°ƒæ–¹æ³•ä»¥æå‡ç¼…ç”¸è¯­æ–°é—»åˆ†ç±»æ€§èƒ½**

**å…³é”®è¯**: `ä½Žèµ„æºè¯­è¨€åˆ†ç±»` `Kolmogorov-Arnoldç½‘ç»œ` `åˆ†ç±»å¤´å¾®è°ƒ` `ç¼…ç”¸è¯­æ–°é—»` `å¤šè¯­è¨€åµŒå…¥` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¼…ç”¸è¯­ç­‰ä½Žèµ„æºè¯­è¨€åˆ†ç±»å¸¸ä»…å¾®è°ƒæœ€ç»ˆå±‚ï¼Œå›ºå®šé¢„è®­ç»ƒç¼–ç å™¨æƒé‡
2. ä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œä½œä¸ºåˆ†ç±»å¤´ï¼Œè¯„ä¼°FourierKANã€EfficientKANå’ŒFasterKANå˜ä½“
3. å®žéªŒæ˜¾ç¤ºKANå¤´åœ¨F1åˆ†æ•°å’Œé€Ÿåº¦ä¸Šä¼˜äºŽæˆ–åŒ¹é…ä¼ ç»ŸMLPï¼ŒEfficientKANè¾¾æœ€é«˜0.928 F1

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.

