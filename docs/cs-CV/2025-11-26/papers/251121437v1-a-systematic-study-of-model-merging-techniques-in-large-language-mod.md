---
layout: default
title: A Systematic Study of Model Merging Techniques in Large Language Models
---

# A Systematic Study of Model Merging Techniques in Large Language Models

**arXiv**: [2511.21437v1](https://arxiv.org/abs/2511.21437) | [PDF](https://arxiv.org/pdf/2511.21437.pdf)

**ä½œè€…**: OÄŸuz KaÄŸan Hitit, Leander Girrbach, Zeynep Akata

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿè¯„ä¼°æ¨¡åž‹åˆå¹¶æ–¹æ³•ï¼Œå‘çŽ°Task Arithmeticåœ¨å¤§è¯­è¨€æ¨¡åž‹ä¸­è¡¨çŽ°æœ€ä½³**

**å…³é”®è¯**: `æ¨¡åž‹åˆå¹¶` `å¤§è¯­è¨€æ¨¡åž‹` `Task Arithmetic` `æ€§èƒ½è¯„ä¼°` `å­ç©ºé—´æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡åž‹åˆå¹¶æŠ€æœ¯æ˜¯å¦é€‚ç”¨äºŽå¤§è¯­è¨€æ¨¡åž‹ï¼Œä»¥æå‡æ€§èƒ½è€Œä¸éœ€é¢å¤–è®­ç»ƒ
2. æ–¹æ³•è¦ç‚¹ï¼šè¯„ä¼°å…­ç§å…ˆè¿›åˆå¹¶æ–¹æ³•ï¼ŒåŒ…æ‹¬å­ç©ºé—´æ–¹æ³•ï¼Œä½¿ç”¨æ ‡å‡†åŸºå‡†æµ‹è¯•
3. å®žéªŒæˆ–æ•ˆæžœï¼šTask Arithmeticå¯é æå‡æ€§èƒ½ï¼Œå…¶ä»–æ–¹æ³•å¸¸å¯¼è‡´æ˜¾è‘—æ€§èƒ½ä¸‹é™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.

