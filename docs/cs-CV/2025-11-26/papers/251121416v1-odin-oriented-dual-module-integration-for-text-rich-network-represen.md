---
layout: default
title: Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning
---

# Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning

**arXiv**: [2511.21416v1](https://arxiv.org/abs/2511.21416) | [PDF](https://arxiv.org/pdf/2511.21416.pdf)

**ä½œè€…**: Kaifeng Hong, Yinglong Zhang, Xiaoying Hong, Xuewen Xia, Xing Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOdinæž¶æž„ä»¥è§£å†³æ–‡æœ¬å±žæ€§å›¾ä¸­ç»“æž„ä¸Žæ–‡æœ¬èžåˆé—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬å±žæ€§å›¾` `å›¾ç¥žç»ç½‘ç»œ` `Transformeræž¶æž„` `ç»“æž„æ³¨å…¥` `è½»é‡æ¨¡åž‹` `å¤šè·³ç»“æž„æŠ½è±¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥ç»“åˆæ–‡æœ¬ç†è§£ä¸Žå›¾ç»“æž„ï¼ŒGNNæ˜“è¿‡å¹³æ»‘ï¼ŒTransformerå¿½ç•¥æ‹“æ‰‘
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å®šå‘åŒæ¨¡å—åœ¨Transformerç‰¹å®šå±‚æ³¨å…¥å›¾ç»“æž„ï¼Œé¿å…å¤šè·³æ‰©æ•£
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šå®žçŽ°SOTAï¼Œè½»é‡ç‰ˆä¿æŒæ€§èƒ½å¹¶é™ä½Žè®¡ç®—æˆæœ¬

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.

