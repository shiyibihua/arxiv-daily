---
layout: default
title: MetaRank: Task-Aware Metric Selection for Model Transferability Estimation
---

# MetaRank: Task-Aware Metric Selection for Model Transferability Estimation

**arXiv**: [2511.21007v1](https://arxiv.org/abs/2511.21007) | [PDF](https://arxiv.org/pdf/2511.21007.pdf)

**ä½œè€…**: Yuhang Liu, Wenjie Zhao, Yunhui Guo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMetaRankæ¡†æž¶ä»¥è§£å†³è¿ç§»å­¦ä¹ ä¸­æ¨¡åž‹å¯è¿ç§»æ€§ä¼°è®¡çš„åº¦é‡é€‰æ‹©é—®é¢˜**

**å…³é”®è¯**: `æ¨¡åž‹å¯è¿ç§»æ€§ä¼°è®¡` `å…ƒå­¦ä¹ ` `åº¦é‡é€‰æ‹©` `è¿ç§»å­¦ä¹ ` `æ–‡æœ¬åµŒå…¥` `å­¦ä¹ æŽ’åº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMTEåº¦é‡é€‰æ‹©ä¾èµ–ä»»åŠ¡ï¼Œæ— é€šç”¨æœ€ä¼˜åº¦é‡ï¼Œå¯¼è‡´é€‰æ‹©æ•ˆçŽ‡ä½Ž
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å…ƒå­¦ä¹ æ¡†æž¶ï¼Œç¼–ç æ•°æ®é›†å’Œåº¦é‡æ–‡æœ¬æè¿°ï¼Œå­¦ä¹ åº¦é‡æŽ’å
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨11ä¸ªæ¨¡åž‹å’Œ11ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒMetaRankæ˜¾è‘—æå‡åº¦é‡é€‰æ‹©æ•ˆæžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Selecting an appropriate pre-trained source model is a critical, yet computationally expensive, task in transfer learning. Model Transferability Estimation (MTE) methods address this by providing efficient proxy metrics to rank models without full fine-tuning. In practice, the choice of which MTE metric to use is often ad hoc or guided simply by a metric's average historical performance. However, we observe that the effectiveness of MTE metrics is highly task-dependent and no single metric is universally optimal across all target datasets. To address this gap, we introduce MetaRank, a meta-learning framework for automatic, task-aware MTE metric selection. We formulate metric selection as a learning-to-rank problem. Rather than relying on conventional meta-features, MetaRank encodes textual descriptions of both datasets and MTE metrics using a pretrained language model, embedding them into a shared semantic space. A meta-predictor is then trained offline on diverse meta-tasks to learn the intricate relationship between dataset characteristics and metric mechanisms, optimized with a listwise objective that prioritizes correctly ranking the top-performing metrics. During the subsequent online phase, MetaRank efficiently ranks the candidate MTE metrics for a new, unseen target dataset based on its textual description, enabling practitioners to select the most appropriate metric a priori. Extensive experiments across 11 pretrained models and 11 target datasets demonstrate the strong effectiveness of our approach.

