---
layout: default
title: ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning
---

# ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning

**arXiv**: [2511.21005v1](https://arxiv.org/abs/2511.21005) | [PDF](https://arxiv.org/pdf/2511.21005.pdf)

**ä½œè€…**: Jinpeng Wang, Chao Li, Ting Ye, Mengyuan Zhang, Wei Liu, Jian Luan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºICPOæ–¹æ³•ä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±ç²—ç³™ã€å™ªå£°å’Œä¸ç¨³å®šè®­ç»ƒé—®é¢˜ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `åå¥½ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡åž‹` `æŽ¨ç†å¢žå¼º` `å¥–åŠ±å»ºæ¨¡` `æŽ¢ç´¢ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰RLVRæ–¹æ³•å­˜åœ¨å¥–åŠ±ç²—ç³™ã€å™ªå£°å’ŒæŽ¢ç´¢æ•ˆçŽ‡ä½Žï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œç†µå´©æºƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨LLMç”Ÿæˆæ¦‚çŽ‡è®¡ç®—åå¥½ä¼˜åŠ¿åˆ†ï¼Œç»“åˆå¯éªŒè¯å¥–åŠ±æŒ‡å¯¼æŽ¢ç´¢ï¼Œæå‡æŽ¨ç†è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒICPOç¨³å®šæå‡æŽ¨ç†èƒ½åŠ›ï¼Œä¼˜äºŽGRPOæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

