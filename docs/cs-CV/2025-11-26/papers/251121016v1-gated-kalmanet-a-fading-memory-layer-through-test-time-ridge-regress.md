---
layout: default
title: Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression
---

# Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression

**arXiv**: [2511.21016v1](https://arxiv.org/abs/2511.21016) | [PDF](https://arxiv.org/pdf/2511.21016.pdf)

**ä½œè€…**: Liangzu Peng, Aditya Chattopadhyay, Luca Zancato, Elvis Nunez, Wei Xia, Stefano Soatto

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGated KalmaNetå±‚ï¼Œé€šè¿‡åœ¨çº¿å²­å›žå½’è§£å†³çº¿æ€§çŠ¶æ€ç©ºé—´æ¨¡åž‹è®°å¿†æŸå¤±é—®é¢˜ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ€§èƒ½ã€‚**

**å…³é”®è¯**: `çº¿æ€§çŠ¶æ€ç©ºé—´æ¨¡åž‹` `åœ¨çº¿å²­å›žå½’` `æ•°å€¼ç¨³å®šæ€§` `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `è‡ªé€‚åº”é—¨æŽ§` `ç¡¬ä»¶ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çº¿æ€§çŠ¶æ€ç©ºé—´æ¨¡åž‹åœ¨å¬å›žä»»åŠ¡ä¸­å› è®°å¿†æŸå¤±å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚
2. é‡‡ç”¨åœ¨çº¿å²­å›žå½’å’Œè‡ªé€‚åº”é—¨æŽ§ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§å’Œé«˜æ•ˆè®¡ç®—ã€‚
3. åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ï¼Œç›¸å¯¹åŸºçº¿æ¨¡åž‹æå‡è¶…è¿‡10%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.

