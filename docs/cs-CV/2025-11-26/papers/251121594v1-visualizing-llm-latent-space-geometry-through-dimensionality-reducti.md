---
layout: default
title: Visualizing LLM Latent Space Geometry Through Dimensionality Reduction
---

# Visualizing LLM Latent Space Geometry Through Dimensionality Reduction

**arXiv**: [2511.21594v1](https://arxiv.org/abs/2511.21594) | [PDF](https://arxiv.org/pdf/2511.21594.pdf)

**ä½œè€…**: Alex Ning, Vainateya Rangaraju

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé™ç»´çš„å¯è§†åŒ–æ–¹æ³•ä»¥åˆ†æžTransformerè¯­è¨€æ¨¡åž‹çš„æ½œåœ¨ç©ºé—´å‡ ä½•**

**å…³é”®è¯**: `æ½œåœ¨ç©ºé—´å¯è§†åŒ–` `é™ç»´åˆ†æž` `Transformeræ¨¡åž‹` `å‡ ä½•æ¨¡å¼` `å¯è§£é‡Šæ€§ç ”ç©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹å†…éƒ¨æœºåˆ¶éš¾ä»¥è§£é‡Šï¼Œæ½œåœ¨ç©ºé—´å‡ ä½•ç‰¹æ€§æœªçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šæå–Transformerå±‚æ¿€æ´»ï¼Œåº”ç”¨PCAå’ŒUMAPè¿›è¡Œé™ç»´å¯è§†åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨GPT-2å’ŒLLaMaä¸­å‘çŽ°æ³¨æ„åŠ›ä¸ŽMLPè¾“å‡ºåˆ†ç¦»ç­‰æ–°å‡ ä½•æ¨¡å¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

