---
layout: default
title: LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs
---

# LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs

**arXiv**: [2511.21150v1](https://arxiv.org/abs/2511.21150) | [PDF](https://arxiv.org/pdf/2511.21150.pdf)

**ä½œè€…**: Shichu Sun, Yichen Zhang, Haolin Song, Zonghao Guo, Chi Chen, Yidan Zhang, Yuan Yao, Zhiyuan Liu, Maosong Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¸è¿›è§†è§‰åŽ‹ç¼©æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­å…¨å±€åŽŸç”Ÿåˆ†è¾¨çŽ‡ç¼–ç çš„é«˜è®¡ç®—å¼€é”€é—®é¢˜**

**å…³é”®è¯**: `æ¸è¿›è§†è§‰åŽ‹ç¼©` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰Transformer` `ä»¤ç‰ŒåŽ‹ç¼©` `é«˜æ•ˆç¼–ç ` `åŽŸç”Ÿåˆ†è¾¨çŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨å±€åŽŸç”Ÿåˆ†è¾¨çŽ‡è§†è§‰ç¼–ç å¢žå¼ºå¤šæ¨¡æ€å¤§æ¨¡åž‹èƒ½åŠ›ï¼Œä½†è®¡ç®—å¼€é”€å¤§
2. æ–¹æ³•è¦ç‚¹ï¼šæ¸è¿›è§†è§‰åŽ‹ç¼©åŒ…æ‹¬ç²¾ç‚¼è¡¥ä¸åµŒå…¥å’Œçª—å£åŒ–ä»¤ç‰ŒåŽ‹ç¼©æ¨¡å—
3. å®žéªŒæˆ–æ•ˆæžœï¼šViT-UHDåœ¨æ€§èƒ½ç«žäº‰ä¸‹ï¼ŒTTFTé™ä½Ž2.4å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.

