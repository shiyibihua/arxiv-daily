---
layout: default
title: Agentic Learner with Grow-and-Refine Multimodal Semantic Memory
---

# Agentic Learner with Grow-and-Refine Multimodal Semantic Memory

**arXiv**: [2511.21678v1](https://arxiv.org/abs/2511.21678) | [PDF](https://arxiv.org/pdf/2511.21678.pdf)

**ä½œè€…**: Weihao Bo, Shan Zhang, Yanpeng Sun, Jingjing Wu, Qunyi Xie, Xiao Tan, Kunbin Chen, Wei He, Xiaofan Li, Na Zhao, Jingdong Wang, Zechao Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViLoMemåŒæµè®°å¿†æ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€ä»£ç†é‡å¤é”™è¯¯é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­ä¹‰è®°å¿†` `åŒæµè®°å¿†æ¡†æž¶` `ä»£ç†å­¦ä¹ ` `è§†è§‰åˆ†å¿ƒæ¨¡å¼` `é€»è¾‘æŽ¨ç†é”™è¯¯` `å¢žé•¿å¼æ›´æ–°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§æ¨¡åž‹ç‹¬ç«‹å¤„ç†æŸ¥è¯¢ï¼Œé‡å¤è§†è§‰å’Œé€»è¾‘é”™è¯¯ï¼Œç¼ºä¹å¤šæ¨¡æ€è¯­ä¹‰è®°å¿†
2. é‡‡ç”¨åŒæµè®°å¿†åˆ†åˆ«ç¼–ç è§†è§‰åˆ†å¿ƒæ¨¡å¼å’Œé€»è¾‘æŽ¨ç†é”™è¯¯ï¼Œæ”¯æŒå¢žé•¿å¼æ›´æ–°
3. åœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸Šæå‡å‡†ç¡®çŽ‡ï¼Œå‡å°‘é‡å¤é”™è¯¯ï¼ŒéªŒè¯åŒæµå¿…è¦æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

