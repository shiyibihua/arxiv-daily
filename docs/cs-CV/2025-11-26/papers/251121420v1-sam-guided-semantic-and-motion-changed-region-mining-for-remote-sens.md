---
layout: default
title: SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning
---

# SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning

**arXiv**: [2511.21420v1](https://arxiv.org/abs/2511.21420) | [PDF](https://arxiv.org/pdf/2511.21420.pdf)

**ä½œè€…**: Futian Wang, Mengqi Wang, Xiao Wang, Haowen Wang, Jin Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAMå¼•å¯¼çš„è¯­ä¹‰ä¸Žè¿åŠ¨å˜åŒ–åŒºåŸŸæŒ–æŽ˜æ–¹æ³•ï¼Œä»¥æå‡é¥æ„Ÿå˜åŒ–æè¿°æ€§èƒ½**

**å…³é”®è¯**: `é¥æ„Ÿå˜åŒ–æè¿°` `SAMæ¨¡åž‹` `åŒºåŸŸçº§è¡¨ç¤º` `çŸ¥è¯†å›¾è°±` `äº¤å‰æ³¨æ„åŠ›` `Transformerè§£ç å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é¥æ„Ÿå˜åŒ–æè¿°æ–¹æ³•åŒºåŸŸæ„ŸçŸ¥å¼±ä¸”æ—¶é—´å¯¹é½æœ‰é™
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆSAMæå–åŒºåŸŸç‰¹å¾ã€çŸ¥è¯†å›¾è°±å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›ç”Ÿæˆæè¿°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œä»£ç å·²å¼€æº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

