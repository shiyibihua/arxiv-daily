---
layout: default
title: FITRep: Attention-Guided Item Representation via MLLMs
---

# FITRep: Attention-Guided Item Representation via MLLMs

**arXiv**: [2511.21389v1](https://arxiv.org/abs/2511.21389) | [PDF](https://arxiv.org/pdf/2511.21389.pdf)

**ä½œè€…**: Guoxiao Zhang, Ao Li, Tan Qu, Qianlong Xie, Xingxing Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFITRepæ¡†æž¶ä»¥è§£å†³åœ¨çº¿å¹³å°è¿‘é‡å¤ç‰©å“å¯¼è‡´çš„ç”¨æˆ·ä½“éªŒä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç‰©å“åŽ»é‡` `ç‰¹å¾æ•´åˆç†è®º` `æ³¨æ„åŠ›å¼•å¯¼` `FAISSèšç±»` `UMAPé™ç»´`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¿‘é‡å¤ç‰©å“è§†è§‰å’Œæ–‡æœ¬ç›¸ä¼¼ï¼ŒçŽ°æœ‰æ–¹æ³•å¿½ç•¥ç»“æž„å…³ç³»ï¼Œå¯¼è‡´å±€éƒ¨ç»“æž„å´©æºƒ
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç‰¹å¾æ•´åˆç†è®ºï¼Œä½¿ç”¨MLLMsæå–å±‚æ¬¡æ¦‚å¿µï¼Œå¹¶é€šè¿‡UMAPå’ŒFAISSè¿›è¡Œé™ç»´èšç±»
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç¾Žå›¢å¹¿å‘Šç³»ç»ŸA/Bæµ‹è¯•ä¸­ï¼Œç‚¹å‡»çŽ‡å’Œåƒæ¬¡å±•ç¤ºæ”¶ç›Šåˆ†åˆ«æå‡3.60%å’Œ4.25%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.

