---
layout: default
title: Towards Audio Token Compression in Large Audio Language Models
---

# Towards Audio Token Compression in Large Audio Language Models

**arXiv**: [2511.20973v1](https://arxiv.org/abs/2511.20973) | [PDF](https://arxiv.org/pdf/2511.20973.pdf)

**ä½œè€…**: Saurabhchand Bhati, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéŸ³é¢‘ä»¤ç‰ŒåŽ‹ç¼©æ–¹æ³•ä»¥è§£å†³å¤§åž‹éŸ³é¢‘è¯­è¨€æ¨¡åž‹åœ¨é•¿éŸ³é¢‘å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¯æ‰©å±•æ€§é—®é¢˜**

**å…³é”®è¯**: `éŸ³é¢‘ä»¤ç‰ŒåŽ‹ç¼©` `å¤§åž‹éŸ³é¢‘è¯­è¨€æ¨¡åž‹` `ä½Žç§©é€‚é…å™¨` `æ— ç›‘ç£åˆ†å‰²` `è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²` `è¯­éŸ³è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹éŸ³é¢‘è¯­è¨€æ¨¡åž‹å› æ³¨æ„åŠ›äºŒæ¬¡å¤æ‚æ€§å’Œé«˜éŸ³é¢‘ä»¤ç‰ŒçŽ‡ï¼Œéš¾ä»¥æ‰©å±•è‡³é•¿éŸ³é¢‘å’Œèµ„æºå—é™å¹³å°
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ— ç›‘ç£åˆ†å‰²å’Œå‡åŒ€å¹³å‡æ± åŒ–åŽ‹ç¼©éŸ³é¢‘ä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨ä½Žç§©é€‚é…å™¨å¾®è°ƒä»¥ç¼“è§£æ€§èƒ½ä¸‹é™
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒåŽ‹ç¼©æ¨¡åž‹æ€§èƒ½æŽ¥è¿‘å¸§çº§æ¨¡åž‹ï¼Œä»¤ç‰Œæ•°å‡å°‘è¾¾ä¸‰å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.
>   In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.

