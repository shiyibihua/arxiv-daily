---
layout: default
title: Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning
---

# Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning

**arXiv**: [2511.20993v1](https://arxiv.org/abs/2511.20993) | [PDF](https://arxiv.org/pdf/2511.20993.pdf)

**ä½œè€…**: Shanwei Fan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSGA-ACRæ¡†æž¶ä»¥è§£å†³LLMåœ¨å¼€æ”¾ä¸–ç•Œå¼ºåŒ–å­¦ä¹ ä¸­çš„è§„åˆ’-æ‰§è¡Œå¯¹é½é—®é¢˜**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹` `å­ç›®æ ‡è§„åˆ’` `è§„åˆ’-æ‰§è¡Œå¯¹é½` `å¼€æ”¾ä¸–ç•ŒçŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMç”Ÿæˆçš„å­ç›®æ ‡è¯­ä¹‰åˆç†ä½†çŽ¯å¢ƒä¸å¯è¡Œï¼Œä¸”è§„åˆ’è¿‡ç¨‹ç¼ºä¹è‡ªéªŒè¯å¯¼è‡´ä¸å¯é 
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆçŽ¯å¢ƒç‰¹å®šå­ç›®æ ‡å›¾å’Œç»“æž„åŒ–çŸ¥è¯†ï¼Œé‡‡ç”¨å¤šLLMç®¡é“åˆ†ç¦»ç”Ÿæˆã€æ‰¹åˆ¤å’Œç²¾ç‚¼
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨22ä¸ªå¼€æ”¾ä¸–ç•Œæ¸¸æˆä»»åŠ¡ä¸­éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.

