---
layout: default
title: Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance
---

# Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance

**arXiv**: [2511.21356v1](https://arxiv.org/abs/2511.21356) | [PDF](https://arxiv.org/pdf/2511.21356.pdf)

**ä½œè€…**: Bram Silue, Santiago Amaya-Corredor, Patrick Mannion, Lander Willem, Pieter Libin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHybrid-AIRLä»¥å¢žå¼ºé€†å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ç¨€ç–å¥–åŠ±åœºæ™¯ä¸­çš„æ€§èƒ½**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—å­¦ä¹ ` `ç¨€ç–å¥–åŠ±` `ä¸“å®¶æŒ‡å¯¼` `ç­–ç•¥å­¦ä¹ ` `å¥–åŠ±å‡½æ•°æŽ¨æ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. AIRLåœ¨å¤æ‚ä¸ç¡®å®šçŽ¯å¢ƒä¸­éš¾ä»¥æŽ¨æ–­æœ‰æ•ˆå¥–åŠ±å‡½æ•°ï¼Œå¦‚HULHEæ‰‘å…‹
2. H-AIRLå¼•å…¥ç›‘ç£æŸå¤±å’Œéšæœºæ­£åˆ™åŒ–æœºåˆ¶ï¼Œæ”¹è¿›å¥–åŠ±æŽ¨æ–­å’Œç­–ç•¥å­¦ä¹ 
3. å®žéªŒæ˜¾ç¤ºH-AIRLåœ¨æ ·æœ¬æ•ˆçŽ‡å’Œç¨³å®šæ€§ä¸Šä¼˜äºŽAIRLï¼Œé€‚ç”¨äºŽçŽ°å®žæŒ‘æˆ˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adversarial Inverse Reinforcement Learning (AIRL) has shown promise in addressing the sparse reward problem in reinforcement learning (RL) by inferring dense reward functions from expert demonstrations. However, its performance in highly complex, imperfect-information settings remains largely unexplored. To explore this gap, we evaluate AIRL in the context of Heads-Up Limit Hold'em (HULHE) poker, a domain characterized by sparse, delayed rewards and significant uncertainty. In this setting, we find that AIRL struggles to infer a sufficiently informative reward function. To overcome this limitation, we contribute Hybrid-AIRL (H-AIRL), an extension that enhances reward inference and policy learning by incorporating a supervised loss derived from expert data and a stochastic regularization mechanism. We evaluate H-AIRL on a carefully selected set of Gymnasium benchmarks and the HULHE poker setting. Additionally, we analyze the learned reward function through visualization to gain deeper insights into the learning process. Our experimental results show that H-AIRL achieves higher sample efficiency and more stable learning compared to AIRL. This highlights the benefits of incorporating supervised signals into inverse RL and establishes H-AIRL as a promising framework for tackling challenging, real-world settings.

