---
layout: default
title: Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning
---

# Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning

**arXiv**: [2511.21581v1](https://arxiv.org/abs/2511.21581) | [PDF](https://arxiv.org/pdf/2511.21581.pdf)

**ä½œè€…**: Alex Ning, Yen-Ling Kuo, Gabe Gomes

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”æ½œåœ¨æŽ¨ç†æ–¹æ³•ä»¥å‡å°‘æŽ¨ç†é•¿åº¦å¹¶ä¿æŒå‡†ç¡®æ€§**

**å…³é”®è¯**: `æ½œåœ¨æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æŽ¨ç†é•¿åº¦ä¼˜åŒ–` `Transformeræ¨¡åž‹` `çŸ¥è¯†è’¸é¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ½œåœ¨æŽ¨ç†æ¨¡åž‹éœ€ä¼˜åŒ–æŽ¨ç†é•¿åº¦ä»¥é™ä½Žè®¡ç®—æˆæœ¬
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¼ºåŒ–å­¦ä¹ è‡ªé€‚åº”è°ƒæ•´æŽ¨ç†é•¿åº¦ï¼Œæœ€å°åŒ–é•¿åº¦å¹¶ç»´æŒç²¾åº¦
3. å®žéªŒæ•ˆæžœï¼šåœ¨GSM8K-Augæ•°æ®é›†ä¸ŠæŽ¨ç†é•¿åº¦å‡å°‘52%ï¼Œå‡†ç¡®æ€§æ— æŸå¤±

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.

