---
layout: default
title: The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment
---

# The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment

**arXiv**: [2511.21331v1](https://arxiv.org/abs/2511.21331) | [PDF](https://arxiv.org/pdf/2511.21331.pdf)

**ä½œè€…**: Stefanos Koutoupis, Michaela Areti Zervou, Konstantinos Kontras, Maarten De Vos, Panagiotis Tsakalides, Grigorios Tsagatakis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConFuæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€é«˜é˜¶å¯¹é½ä¸­å¿½è§†æˆå¯¹å…³ç³»çš„é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹é½` `å¯¹æ¯”å­¦ä¹ ` `é«˜é˜¶ä¾èµ–` `èžåˆè¡¨ç¤º` `æ£€ç´¢ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å­¦ä¹ å¤šé™äºŽæˆå¯¹å¯¹é½ï¼Œé«˜é˜¶æ–¹æ³•å¸¸å¿½ç•¥æˆå¯¹å…³ç³»ï¼Œå½±å“å•æ¨¡æ€ä»»åŠ¡æ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥èžåˆæ¨¡æ€å¯¹æ¯”é¡¹ï¼Œè”åˆåµŒå…¥å•æ¨¡æ€ä¸Žèžåˆæ¨¡æ€ï¼Œæ•èŽ·é«˜é˜¶ä¾èµ–å¦‚XORå…³ç³»
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒçœŸå®žåŸºå‡†ä¸Šï¼ŒConFuåœ¨æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¡¨çŽ°ç«žäº‰æ€§ï¼Œæ”¯æŒç»Ÿä¸€æ£€ç´¢

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.

