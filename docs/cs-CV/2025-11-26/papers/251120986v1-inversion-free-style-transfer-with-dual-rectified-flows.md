---
layout: default
title: Inversion-Free Style Transfer with Dual Rectified Flows
---

# Inversion-Free Style Transfer with Dual Rectified Flows

**arXiv**: [2511.20986v1](https://arxiv.org/abs/2511.20986) | [PDF](https://arxiv.org/pdf/2511.20986.pdf)

**ä½œè€…**: Yingying Deng, Xiangyu He, Fan Tang, Weiming Dong, Xucheng Yin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŒæ•´æµæµçš„å…åæ¼”é£Žæ ¼è¿ç§»æ¡†æž¶ï¼Œä»¥æå‡å›¾åƒé£Žæ ¼è¿ç§»çš„æ•ˆçŽ‡ä¸Žè§†è§‰è´¨é‡ã€‚**

**å…³é”®è¯**: `é£Žæ ¼è¿ç§»` `æ•´æµæµ` `å…åæ¼”æ–¹æ³•` `å›¾åƒåˆæˆ` `åŠ¨æ€æ’å€¼` `æ³¨æ„åŠ›æ³¨å…¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¸»æµå…è®­ç»ƒæ‰©æ•£æ–¹æ³•ä¾èµ–è®¡ç®—å¯†é›†åž‹åæ¼”è¿‡ç¨‹ï¼Œå¯¼è‡´æ•ˆçŽ‡ä½Žä¸‹å’Œè§†è§‰å¤±çœŸã€‚
2. é‡‡ç”¨åŒæ•´æµæµå¹¶è¡Œé¢„æµ‹å†…å®¹å’Œé£Žæ ¼è½¨è¿¹ï¼Œé€šè¿‡åŠ¨æ€ä¸­ç‚¹æ’å€¼èžåˆï¼Œä»…éœ€å‰å‘ä¼ æ’­ã€‚
3. å®žéªŒéªŒè¯äº†æ–¹æ³•åœ¨å¤šæ ·é£Žæ ¼å’Œå†…å®¹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå®žçŽ°é«˜æ•ˆä¸”é«˜è´¨é‡çš„è¿ç§»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Style transfer, a pivotal task in image processing, synthesizes visually compelling images by seamlessly blending realistic content with artistic styles, enabling applications in photo editing and creative design. While mainstream training-free diffusion-based methods have greatly advanced style transfer in recent years, their reliance on computationally inversion processes compromises efficiency and introduces visual distortions when inversion is inaccurate. To address these limitations, we propose a novel \textit{inversion-free} style transfer framework based on dual rectified flows, which tackles the challenge of finding an unknown stylized distribution from two distinct inputs (content and style images), \textit{only with forward pass}. Our approach predicts content and style trajectories in parallel, then fuses them through a dynamic midpoint interpolation that integrates velocities from both paths while adapting to the evolving stylized image. By jointly modeling the content, style, and stylized distributions, our velocity field design achieves robust fusion and avoids the shortcomings of naive overlays. Attention injection further guides style integration, enhancing visual fidelity, content preservation, and computational efficiency. Extensive experiments demonstrate generalization across diverse styles and content, providing an effective and efficient pipeline for style transfer.

