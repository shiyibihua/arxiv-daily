---
layout: default
title: Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution
---

# Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution

**arXiv**: [2511.03232v1](https://arxiv.org/abs/2511.03232) | [PDF](https://arxiv.org/pdf/2511.03232.pdf)

**ä½œè€…**: Sichen Guo, Wenjie Li, Yuanyang Liu, Guangwei Gao, Jian Yang, Chia-Wen Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºT-PMambaSRè½»é‡çº§è¶…åˆ†æ¡†æž¶ï¼Œç»“åˆçª—å£è‡ªæ³¨æ„åŠ›å’Œæ¸è¿›Mambaä»¥æå‡ç‰¹å¾è¡¨ç¤ºæ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `å›¾åƒè¶…åˆ†è¾¨çŽ‡` `è½»é‡çº§æ¨¡åž‹` `æ¸è¿›Mamba` `çª—å£è‡ªæ³¨æ„åŠ›` `é«˜é¢‘ç»†èŠ‚æ¢å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰Mambaè¶…åˆ†æ–¹æ³•ç¼ºä¹è·¨å°ºåº¦ç»†ç²’åº¦è¿‡æ¸¡ï¼Œé™åˆ¶ç‰¹å¾è¡¨ç¤ºæ•ˆçŽ‡ã€‚
2. é›†æˆçª—å£è‡ªæ³¨æ„åŠ›ä¸Žæ¸è¿›Mambaï¼Œå®žçŽ°çº¿æ€§å¤æ‚åº¦ä¸‹çš„å¤šå°ºåº¦äº¤äº’å’Œæ¸è¿›å¢žå¼ºã€‚
3. å®žéªŒæ˜¾ç¤ºT-PMambaSRæ€§èƒ½ä¼˜äºŽTransformeræˆ–Mambaæ–¹æ³•ï¼Œè®¡ç®—æˆæœ¬æ›´ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Mamba-based super-resolution (SR) methods have demonstrated the
> ability to capture global receptive fields with linear complexity, addressing
> the quadratic computational cost of Transformer-based SR approaches. However,
> existing Mamba-based methods lack fine-grained transitions across different
> modeling scales, which limits the efficiency of feature representation. In this
> paper, we propose T-PMambaSR, a lightweight SR framework that integrates
> window-based self-attention with Progressive Mamba. By enabling interactions
> among receptive fields of different scales, our method establishes a
> fine-grained modeling paradigm that progressively enhances feature
> representation with linear complexity. Furthermore, we introduce an Adaptive
> High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost
> during Transformer and Mamba processing. Extensive experiments demonstrate that
> T-PMambaSR progressively enhances the model's receptive field and
> expressiveness, yielding better performance than recent Transformer- or
> Mamba-based methods while incurring lower computational cost. Our codes will be
> released after acceptance.

