---
layout: default
title: SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention
---

# SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention

**arXiv**: [2511.03178v1](https://arxiv.org/abs/2511.03178) | [PDF](https://arxiv.org/pdf/2511.03178.pdf)

**ä½œè€…**: Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarak I. Hoque

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSurgAnt-ViVQAæ¨¡åž‹ï¼Œé€šè¿‡GRUé©±åŠ¨çš„æ—¶é—´äº¤å‰æ³¨æ„åŠ›é¢„æµ‹æ‰‹æœ¯äº‹ä»¶ï¼Œä»¥æ”¯æŒå†…çª¥é•œæ‰‹æœ¯å®žæ—¶è¾…åŠ©ã€‚**

**å…³é”®è¯**: `æ‰‹æœ¯è§†è§‰é—®ç­”` `æ—¶é—´å»ºæ¨¡` `é—¨æŽ§äº¤å‰æ³¨æ„åŠ›` `GRUç¼–ç ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å‰çž»é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰é—®ç­”ç³»ç»Ÿåœ¨æ‰‹æœ¯ä¸­ä»…å…³æ³¨å½“å‰åœºæ™¯ï¼Œæ— æ³•é¢„æµ‹æœªæ¥äº‹ä»¶ï¼Œå¦‚æ‰‹æœ¯é˜¶æ®µæˆ–å™¨æ¢°éœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŒå‘GRUç¼–ç è§†é¢‘åŠ¨æ€ï¼Œå¹¶é€šè¿‡é—¨æŽ§äº¤å‰æ³¨æ„åŠ›å°†è§†è§‰ä¸Šä¸‹æ–‡æ³¨å…¥è¯­è¨€æ¨¡åž‹ï¼Œå®žçŽ°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PitVQA-Anticipationå’ŒEndoVisæ•°æ®é›†ä¸Šè¶…è¶ŠåŸºçº¿ï¼Œæ—¶é—´å»ºæ¨¡å’Œé—¨æŽ§èžåˆæ˜¾è‘—æå‡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Anticipating forthcoming surgical events is vital for real-time assistance in
> endonasal transsphenoidal pituitary surgery, where visibility is limited and
> workflow changes rapidly. Most visual question answering (VQA) systems reason
> on isolated frames with static vision language alignment, providing little
> support for forecasting next steps or instrument needs. Existing surgical VQA
> datasets likewise center on the current scene rather than the near future. We
> introduce PitVQA-Anticipation, the first VQA dataset designed for forward
> looking surgical reasoning. It comprises 33.5 hours of operative video and
> 734,769 question answer pairs built from temporally grouped clips and expert
> annotations across four tasks: predicting the future phase, next step, upcoming
> instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
> language model that adapts a large language model using a GRU Gated Temporal
> Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
> while an adaptive gate injects visual context into the language stream at the
> token level. Parameter efficient fine tuning customizes the language backbone
> to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
> EndoVis datasets, surpassing strong image and video based baselines. Ablations
> show that temporal recurrence and gated fusion drive most of the gains. A frame
> budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
> frames slightly reduce BLEU but improve numeric time estimation. By pairing a
> temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
> advances surgical VQA from retrospective description to proactive anticipation.
> PitVQA-Anticipation offers a comprehensive benchmark for this setting and
> highlights the importance of targeted temporal modeling for reliable, future
> aware surgical assistance.

