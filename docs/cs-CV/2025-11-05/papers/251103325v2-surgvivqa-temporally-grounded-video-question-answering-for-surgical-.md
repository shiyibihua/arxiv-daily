---
layout: default
title: SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding
---

# SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding

**arXiv**: [2511.03325v2](https://arxiv.org/abs/2511.03325) | [PDF](https://arxiv.org/pdf/2511.03325.pdf)

**ä½œè€…**: Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-05 (æ›´æ–°: 2025-11-06)

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/madratak/SurgViVQA)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SurgViVQAï¼šé¢å‘æ‰‹æœ¯åœºæ™¯ç†è§£çš„æ—¶åºè§†é¢‘é—®ç­”æ¨¡åž‹**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ‰‹æœ¯è§†é¢‘é—®ç­”` `æ—¶åºå»ºæ¨¡` `è§†é¢‘ç†è§£` `å¤šæ¨¡æ€èžåˆ` `Transformer` `åŒ»å­¦å½±åƒ` `å†…çª¥é•œè§†é¢‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ‰‹æœ¯è§†é¢‘é—®ç­”æ–¹æ³•ä¾èµ–é™æ€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„æ—¶åºåŠ¨æ€ä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨¡åž‹å¯¹æ‰‹æœ¯è¿‡ç¨‹çš„ç†è§£ã€‚
2. SurgViVQAé€šè¿‡Masked Video-Text Encoderèžåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰è¿åŠ¨å’Œå·¥å…·-ç»„ç»‡äº¤äº’ç­‰æ—¶åºä¿¡æ¯ï¼Œæå‡æ¨¡åž‹å¯¹åŠ¨æ€æ‰‹æœ¯åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚
3. åœ¨REAL-Colon-VQAå’ŒEndoVis18-VQAæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒSurgViVQAåœ¨å…³é”®è¯å‡†ç¡®çŽ‡ä¸Šä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ï¼Œåˆ†åˆ«æå‡äº†11%å’Œ9%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰‹æœ¯é¢†åŸŸçš„è§†é¢‘é—®ç­”(VideoQA)æ—¨åœ¨é€šè¿‡ä½¿AIæ¨¡åž‹èƒ½å¤ŸæŽ¨ç†æ—¶é—´ä¸Šè¿žè´¯çš„äº‹ä»¶ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„å¸§ï¼Œæ¥å¢žå¼ºæœ¯ä¸­ç†è§£ã€‚ç›®å‰çš„æ–¹æ³•ä»…é™äºŽé™æ€å›¾åƒç‰¹å¾ï¼Œå¹¶ä¸”å¯ç”¨çš„æ•°æ®é›†é€šå¸¸ç¼ºä¹æ—¶é—´æ³¨é‡Šï¼Œå¿½ç•¥äº†å¯¹äºŽå‡†ç¡®ç¨‹åºè§£é‡Šè‡³å…³é‡è¦çš„åŠ¨æ€ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†SurgViVQAï¼Œä¸€ç§æ‰‹æœ¯VideoQAæ¨¡åž‹ï¼Œå®ƒå°†è§†è§‰æŽ¨ç†ä»Žé™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚å®ƒä½¿ç”¨Masked Video-Text Encoderæ¥èžåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰è¿åŠ¨å’Œå·¥å…·-ç»„ç»‡äº¤äº’ç­‰æ—¶é—´çº¿ç´¢ï¼Œç„¶åŽç”±å¾®è°ƒçš„å¤§åž‹è¯­è¨€æ¨¡åž‹(LLM)å°†å…¶è§£ç ä¸ºè¿žè´¯çš„ç­”æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬æ•´ç†äº†REAL-Colon-VQAï¼Œä¸€ä¸ªç»“è‚ é•œè§†é¢‘æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸Žè¿åŠ¨ç›¸å…³çš„é—®é¢˜å’Œè¯Šæ–­å±žæ€§ï¼Œä»¥åŠå…·æœ‰æ”¹å†™æˆ–è¯­ä¹‰æ”¹å˜çš„å…¬å¼çš„æ¨¡æ¿å¤–é—®é¢˜ï¼Œä»¥è¯„ä¼°æ¨¡åž‹çš„é²æ£’æ€§ã€‚åœ¨REAL-Colon-VQAå’Œå…¬å…±EndoVis18-VQAæ•°æ®é›†ä¸Šçš„å®žéªŒéªŒè¯è¡¨æ˜Žï¼ŒSurgViVQAä¼˜äºŽçŽ°æœ‰çš„åŸºäºŽå›¾åƒçš„VQAåŸºå‡†æ¨¡åž‹ï¼Œå°¤å…¶æ˜¯åœ¨å…³é”®è¯å‡†ç¡®çŽ‡æ–¹é¢ï¼Œåœ¨REAL-Colon-VQAä¸Šæ¯”PitVQAæé«˜äº†+11%ï¼Œåœ¨EndoVis18-VQAä¸Šæé«˜äº†+9%ã€‚å¯¹é—®é¢˜çš„æ‰°åŠ¨ç ”ç©¶è¿›ä¸€æ­¥è¯å®žäº†æ”¹è¿›çš„æ³›åŒ–æ€§å’Œå¯¹é—®é¢˜æŽªè¾žå˜åŒ–çš„é²æ£’æ€§ã€‚SurgViVQAå’ŒREAL-Colon-VQAæ•°æ®é›†ä¸ºæ‰‹æœ¯VideoQAä¸­å…·æœ‰æ—¶é—´æ„è¯†çš„ç†è§£æä¾›äº†ä¸€ä¸ªæ¡†æž¶ï¼Œä½¿AIæ¨¡åž‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£é‡ŠåŠ¨æ€ç¨‹åºä¸Šä¸‹æ–‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ‰‹æœ¯è§†é¢‘é—®ç­”æ–¹æ³•ä¸»è¦åŸºäºŽé™æ€å›¾åƒï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„æ—¶åºä¿¡æ¯ï¼Œä¾‹å¦‚å·¥å…·è¿åŠ¨ã€ç»„ç»‡å½¢å˜ç­‰ã€‚è¿™å¯¼è‡´æ¨¡åž‹éš¾ä»¥å‡†ç¡®ç†è§£æ‰‹æœ¯æ­¥éª¤å’Œå…³é”®äº‹ä»¶ï¼Œé™åˆ¶äº†å…¶åœ¨æœ¯ä¸­è¾…åŠ©å†³ç­–ä¸­çš„åº”ç”¨ã€‚çŽ°æœ‰æ•°æ®é›†ä¹Ÿç¼ºä¹è¶³å¤Ÿçš„æ—¶é—´æ ‡æ³¨ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸ªé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSurgViVQAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘å’Œé—®é¢˜è¿›è¡Œè”åˆç¼–ç ï¼Œåˆ©ç”¨Transformeræž¶æž„æ•æ‰è§†é¢‘ä¸­çš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå¹¶å°†å…¶ä¸Žé—®é¢˜è¯­ä¹‰è¿›è¡Œå¯¹é½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡åž‹å¯ä»¥æ›´å¥½åœ°ç†è§£æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ï¼Œå¹¶æ ¹æ®é—®é¢˜ç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSurgViVQAä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†é¢‘ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡åž‹ï¼ˆä¾‹å¦‚ResNetæˆ–TimeSformerï¼‰æå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ã€‚2) é—®é¢˜ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬æ¨¡åž‹ï¼ˆä¾‹å¦‚BERTï¼‰æå–é—®é¢˜çš„æ–‡æœ¬ç‰¹å¾ã€‚3) Masked Video-Text Encoderï¼šä½¿ç”¨Transformeræž¶æž„ï¼Œå°†è§†é¢‘ç‰¹å¾å’Œé—®é¢˜ç‰¹å¾è¿›è¡Œèžåˆï¼Œå­¦ä¹ è§†é¢‘å’Œé—®é¢˜ä¹‹é—´çš„æ—¶åºä¾èµ–å…³ç³»ã€‚è¯¥æ¨¡å—é‡‡ç”¨Masked Language Modeling (MLM) é¢„è®­ç»ƒæ–¹å¼ï¼Œæå‡æ¨¡åž‹å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚4) è§£ç å™¨ï¼šä½¿ç”¨å¾®è°ƒçš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰å°†èžåˆåŽçš„ç‰¹å¾è§£ç ä¸ºç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šSurgViVQAçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶Masked Video-Text Encoderï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸Žé—®é¢˜è¯­ä¹‰è¿›è¡Œå¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSurgViVQAèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ï¼Œä»Žè€Œç»™å‡ºæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼ŒREAL-Colon-VQAæ•°æ®é›†çš„æž„å»ºä¹Ÿä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šMasked Video-Text Encoderé‡‡ç”¨å¤šå±‚Transformerç»“æž„ï¼Œæ¯ä¸€å±‚åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥žç»ç½‘ç»œã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶ç”¨äºŽæ•æ‰è§†é¢‘å¸§ä¹‹é—´çš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå‰é¦ˆç¥žç»ç½‘ç»œç”¨äºŽå­¦ä¹ ç‰¹å¾çš„éžçº¿æ€§å˜æ¢ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨Masked Language Modeling (MLM) é¢„è®­ç»ƒæ–¹å¼ï¼ŒéšæœºmaskæŽ‰ä¸€éƒ¨åˆ†è§†é¢‘å¸§æˆ–é—®é¢˜ä¸­çš„è¯è¯­ï¼Œç„¶åŽè®©æ¨¡åž‹é¢„æµ‹è¢«maskæŽ‰çš„å†…å®¹ã€‚è¿™ç§é¢„è®­ç»ƒæ–¹å¼å¯ä»¥æå‡æ¨¡åž‹å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚è§£ç å™¨éƒ¨åˆ†ï¼Œé€‰æ‹©åˆé€‚çš„LLMå¹¶è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ‰‹æœ¯è§†é¢‘é—®ç­”ä»»åŠ¡çš„ç‰¹ç‚¹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SurgViVQAåœ¨REAL-Colon-VQAå’ŒEndoVis18-VQAæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨REAL-Colon-VQAæ•°æ®é›†ä¸Šï¼ŒSurgViVQAçš„å…³é”®è¯å‡†ç¡®çŽ‡æ¯”PitVQAæé«˜äº†11%ã€‚åœ¨EndoVis18-VQAæ•°æ®é›†ä¸Šï¼ŒSurgViVQAçš„å…³é”®è¯å‡†ç¡®çŽ‡æ¯”PitVQAæé«˜äº†9%ã€‚æ­¤å¤–ï¼Œå¯¹é—®é¢˜çš„æ‰°åŠ¨ç ”ç©¶è¡¨æ˜Žï¼ŒSurgViVQAå…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§å’Œå¯¹é—®é¢˜æŽªè¾žå˜åŒ–çš„é²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SurgViVQAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºŽæœ¯ä¸­å¯¼èˆªã€æ‰‹æœ¯æŠ€èƒ½è¯„ä¼°ã€æ‰‹æœ¯æœºå™¨äººæŽ§åˆ¶ç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£æ‰‹æœ¯è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯ï¼ŒSurgViVQAå¯ä»¥ä¸ºåŒ»ç”Ÿæä¾›å®žæ—¶çš„æ‰‹æœ¯æŒ‡å¯¼ï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°å®Œæˆæ‰‹æœ¯ã€‚æ­¤å¤–ï¼ŒSurgViVQAè¿˜å¯ä»¥ç”¨äºŽè¯„ä¼°åŒ»ç”Ÿçš„æ‰‹æœ¯æŠ€èƒ½ï¼Œä¸ºæ‰‹æœ¯æœºå™¨äººçš„æŽ§åˆ¶æä¾›æ›´å‡†ç¡®çš„æŒ‡ä»¤ã€‚æœªæ¥ï¼ŒSurgViVQAæœ‰æœ›æˆä¸ºæ™ºèƒ½æ‰‹æœ¯å®¤çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\% on REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at https://github.com/madratak/SurgViVQA.

