---
layout: default
title: Indicating Robot Vision Capabilities with Augmented Reality
---

# Indicating Robot Vision Capabilities with Augmented Reality

**arXiv**: [2511.03550v1](https://arxiv.org/abs/2511.03550) | [PDF](https://arxiv.org/pdf/2511.03550.pdf)

**ä½œè€…**: Hong Wang, Ridhima Phatak, James Ocampo, Zhao Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¢žå¼ºçŽ°å®žè§†åœºæŒ‡ç¤ºå™¨ä»¥è§£å†³äººæœºåä½œä¸­äººç±»å¯¹æœºå™¨äººè§†è§‰èƒ½åŠ›çš„è¯¯åˆ¤é—®é¢˜**

**å…³é”®è¯**: `å¢žå¼ºçŽ°å®ž` `æœºå™¨äººè§†è§‰` `äººæœºåä½œ` `è§†åœºæŒ‡ç¤º` `ç”¨æˆ·å®žéªŒ` `è®¤çŸ¥è´Ÿè·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäººç±»å¸¸è¯¯ä»¥ä¸ºæœºå™¨äººè§†åœºä¸Žäººç±»ç›¸åŒï¼Œå¯¼è‡´åä½œå¤±è´¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å››ç§å¢žå¼ºçŽ°å®žè§†åœºæŒ‡ç¤ºå™¨ï¼Œæ¶µç›–è‡ªæˆ‘ä¸­å¿ƒåˆ°ä»»åŠ¡ä¸­å¿ƒè§†è§’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç”¨æˆ·å®žéªŒæ˜¾ç¤ºä»»åŠ¡ä¸­å¿ƒæŒ‡ç¤ºå™¨å‡†ç¡®çŽ‡æœ€é«˜ï¼Œå‚ä¸Žè€…ä¿¡å¿ƒé«˜ä¸”è®¤çŸ¥è´Ÿè·ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Research indicates that humans can mistakenly assume that robots and humans
> have the same field of view (FoV), possessing an inaccurate mental model of
> robots. This misperception may lead to failures during human-robot
> collaboration tasks where robots might be asked to complete impossible tasks
> about out-of-view objects. The issue is more severe when robots do not have a
> chance to scan the scene to update their world model while focusing on assigned
> tasks. To help align humans' mental models of robots' vision capabilities, we
> propose four FoV indicators in augmented reality (AR) and conducted a user
> human-subjects experiment (N=41) to evaluate them in terms of accuracy,
> confidence, task efficiency, and workload. These indicators span a spectrum
> from egocentric (robot's eye and head space) to allocentric (task space).
> Results showed that the allocentric blocks at the task space had the highest
> accuracy with a delay in interpreting the robot's FoV. The egocentric indicator
> of deeper eye sockets, possible for physical alteration, also increased
> accuracy. In all indicators, participants' confidence was high while cognitive
> load remained low. Finally, we contribute six guidelines for practitioners to
> apply our AR indicators or physical alterations to align humans' mental models
> with robots' vision capabilities.

