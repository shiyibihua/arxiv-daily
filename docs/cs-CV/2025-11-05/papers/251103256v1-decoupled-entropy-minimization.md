---
layout: default
title: Decoupled Entropy Minimization
---

# Decoupled Entropy Minimization

**arXiv**: [2511.03256v1](https://arxiv.org/abs/2511.03256) | [PDF](https://arxiv.org/pdf/2511.03256.pdf)

**ä½œè€…**: Jing Ma, Hanlin Li, Xiang Xiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è§£è€¦ç†µæœ€å°åŒ–ä»¥è§£å†³ç»å…¸ç†µæœ€å°åŒ–åœ¨å™ªå£°å’ŒåŠ¨æ€çŽ¯å¢ƒä¸­çš„å±€é™æ€§**

**å…³é”®è¯**: `ç†µæœ€å°åŒ–` `è§£è€¦å­¦ä¹ ` `è‡ªé€‚åº”æ ¡å‡†` `å™ªå£°çŽ¯å¢ƒå­¦ä¹ ` `ä¸å®Œç¾Žç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»å…¸ç†µæœ€å°åŒ–å­˜åœ¨å¥–åŠ±å´©æºƒå’Œæ˜“ç±»åå·®ï¼Œé™åˆ¶å…¶åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­çš„æ½œåŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šå°†ç†µæœ€å°åŒ–è§£è€¦ä¸ºèšç±»èšåˆé©±åŠ¨å› å­å’Œæ¢¯åº¦ç¼“è§£æ ¡å‡†å™¨ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”å½’ä¸€åŒ–å’Œè¾¹é™…ç†µæ ¡å‡†å™¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å™ªå£°å’ŒåŠ¨æ€çŽ¯å¢ƒçš„ä¸å®Œç¾Žç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºŽç»å…¸ç†µæœ€å°åŒ–ä¸Šç•Œå˜ä½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
> domain gap, and restricting uncertainty for various tasks in machine learning,
> yet its potential is limited. To study the internal mechanism of EM, we
> reformulate and decouple the classical EM into two parts with opposite effects:
> cluster aggregation driving factor (CADF) rewards dominant classes and prompts
> a peaked output distribution, while gradient mitigation calibrator (GMC)
> penalizes high-confidence classes based on predicted probabilities.
> Furthermore, we reveal the limitations of classical EM caused by its coupled
> formulation: 1) reward collapse impedes the contribution of high-certainty
> samples in the learning process, and 2) easy-class bias induces misalignment
> between output distribution and label distribution. To address these issues, we
> propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
> reward brought from CADF and employs a marginal entropy calibrator (MEC) to
> replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
> and achieves superior performance across various imperfectly supervised
> learning tasks in noisy and dynamic environments.

