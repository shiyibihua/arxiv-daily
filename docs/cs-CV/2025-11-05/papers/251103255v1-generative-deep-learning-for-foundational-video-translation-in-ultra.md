---
layout: default
title: Generative deep learning for foundational video translation in ultrasound
---

# Generative deep learning for foundational video translation in ultrasound

**arXiv**: [2511.03255v1](https://arxiv.org/abs/2511.03255) | [PDF](https://arxiv.org/pdf/2511.03255.pdf)

**ä½œè€…**: Nikolina Tomic Roshni Bhatnagar, Sarthak Jain, Connor Lau, Tien-Yu Liu, Laura Gambini, Rima Arnaout

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ–¹æ³•ä»¥è§£å†³è¶…å£°å­æ¨¡æ€è§†é¢‘æ•°æ®ä¸å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `ç”Ÿæˆå¼æ·±åº¦å­¦ä¹ ` `è¶…å£°è§†é¢‘ç¿»è¯‘` `æ•°æ®ä¸å¹³è¡¡` `å¯¹æŠ—è®­ç»ƒ` `åŒ»å­¦å½±åƒå¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¶…å£°æ•°æ®ä¸­ç°åº¦ä¸Žå½©è‰²å¤šæ™®å‹’å­æ¨¡æ€å¸¸ä¸å¹³è¡¡ï¼Œå½±å“æ·±åº¦å­¦ä¹ åº”ç”¨
2. ä½¿ç”¨åƒç´ ã€å¯¹æŠ—å’Œæ„ŸçŸ¥æŸå¤±ï¼Œç»“åˆç»“æž„é‡å»ºä¸ŽåŽ»å™ªç½‘ç»œç”Ÿæˆè§†é¢‘
3. åˆæˆè§†é¢‘åœ¨åˆ†ç±»ã€åˆ†å‰²ä»»åŠ¡ä¸­ä¸ŽçœŸå®žè§†é¢‘æ— æ˜¾è‘—å·®å¼‚ï¼Œä¸´åºŠä¸“å®¶éš¾ä»¥åŒºåˆ†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep learning (DL) has the potential to revolutionize image acquisition and
> interpretation across medicine, however, attention to data imbalance and
> missingness is required. Ultrasound data presents a particular challenge
> because in addition to different views and structures, it includes several
> sub-modalities-such as greyscale and color flow doppler (CFD)-that are often
> imbalanced in clinical studies. Image translation can help balance datasets but
> is challenging for ultrasound sub-modalities to date. Here, we present a
> generative method for ultrasound CFD-greyscale video translation, trained on
> 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise,
> adversarial, and perceptual loses and utilized two networks: one for
> reconstructing anatomic structures and one for denoising to achieve realistic
> ultrasound imaging. Average pairwise SSIM between synthetic videos and ground
> truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real
> ones in DL classification and segmentation tasks and when evaluated by blinded
> clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice
> score between real and synthetic segmentation was 0.97. Overall clinician
> accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%),
> indicating realistic synthetic videos. Although trained only on heart videos,
> the model worked well on ultrasound spanning several clinical domains (average
> SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data
> expand the utility of retrospectively collected imaging and augment the dataset
> design toolbox for medical imaging.

