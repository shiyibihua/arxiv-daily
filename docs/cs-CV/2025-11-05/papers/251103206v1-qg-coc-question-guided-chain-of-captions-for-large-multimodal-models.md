---
layout: default
title: QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models
---

# QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models

**arXiv**: [2511.03206v1](https://arxiv.org/abs/2511.03206) | [PDF](https://arxiv.org/pdf/2511.03206.pdf)

**ä½œè€…**: Kuei-Chun Kao, Hsu Tzu-Yin, Yunqi Hong, Ruochen Wang, Cho-Jui Hsieh

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQG-CoCé›¶æ ·æœ¬æç¤ºæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨å¤šå›¾åƒæŽ¨ç†ä¸­çš„æ„ŸçŸ¥ä¸Žæ•´åˆé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¤šå›¾åƒæŽ¨ç†` `é›¶æ ·æœ¬æç¤º` `é“¾å¼æè¿°` `è§†è§‰æ„ŸçŸ¥` `åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨å¤šå›¾åƒåœºæ™¯ä¸­ç¼ºä¹ç»†ç²’åº¦æ„ŸçŸ¥å’Œæœ‰æ•ˆæŽ¨ç†èƒ½åŠ›
2. QG-CoCé€šè¿‡é—®é¢˜å¼•å¯¼çš„é“¾å¼æè¿°å®žçŽ°ä»»æ„æ•°é‡å›¾åƒçš„å¤„ç†
3. å®žéªŒæ˜¾ç¤ºQG-CoCåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹æ”¹è¿›æ˜¾è‘—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
> in multi-image contexts: (1) a lack of fine-grained perception across disparate
> images, and (2) a diminished capability to effectively reason over and
> synthesize information from multiple visual inputs. However, while various
> prompting methods aim to describe visual content, many existing studies focus
> primarily on single-image settings or specific, constrained scenarios. This
> leaves a critical gap in understanding and addressing how MLLMs tackle more
> general and complex multi-image reasoning tasks. Thus, we first extensively
> investigate how current prompting methods perceive fine-grained visual details
> and process visual information when dealing with multiple images. Our findings
> reveal that existing prompting methods fall short in attending to needed clues
> and seamlessly integrating perception and reasoning. Inspired by the findings,
> we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
> (QG-CoC), a generalized prompting approach that effectively handles problems
> with an arbitrary number of images. We evaluate our method on various
> open-source and closed-source MLLMs for multi-image and single-image
> benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
> performance across tasks and exhibits robust improvements in the challenging
> scenarios where existing prompting methods fail.

