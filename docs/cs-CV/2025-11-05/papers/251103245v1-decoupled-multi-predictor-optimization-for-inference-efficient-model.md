---
layout: default
title: Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning
---

# Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning

**arXiv**: [2511.03245v1](https://arxiv.org/abs/2511.03245) | [PDF](https://arxiv.org/pdf/2511.03245.pdf)

**ä½œè€…**: Liwei Luo, Shuaitengyuan Li, Dongwei Ren, Qilong Wang, Pengfei Zhu, Qinghua Hu

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦å¤šé¢„æµ‹å™¨ä¼˜åŒ–æ–¹æ³•ä»¥æå‡é¢„è®­ç»ƒæ¨¡å‹è°ƒä¼˜çš„æ¨ç†æ•ˆç‡**

**å…³é”®è¯**: `æ¨¡å‹è°ƒä¼˜` `æ¨ç†æ•ˆç‡` `æ—©æœŸé€€å‡º` `å¤šé¢„æµ‹å™¨` `è§£è€¦ä¼˜åŒ–` `ç‰¹å¾è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ—©æœŸé˜¶æ®µå¦‚ä½•åŒæ—¶æä¾›ä½å±‚åŸºç¡€ç‰¹å¾ç»™æ·±å±‚é˜¶æ®µå’Œé«˜å±‚åˆ¤åˆ«ç‰¹å¾ç»™æ—©æœŸé¢„æµ‹å™¨
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è½»é‡æ—è·¯æ¨¡å—å’Œé«˜é˜¶ç»Ÿè®¡é¢„æµ‹å™¨ï¼Œå¹¶é‡‡ç”¨è§£è€¦ä¼˜åŒ–åˆ†é…ä¸¤é˜¶æ®µæŸå¤±æƒé‡
3. å®éªŒæ•ˆæœï¼šåœ¨å¤šç§æ•°æ®é›†å’Œé¢„è®­ç»ƒéª¨å¹²ä¸Šï¼ŒDMPOåœ¨é™ä½è®¡ç®—æˆæœ¬æ—¶æ˜æ˜¾ä¼˜äºå¯¹æ¯”æ–¹æ³•

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, remarkable progress has been made in large-scale pre-trained model
> tuning, and inference efficiency is becoming more crucial for practical
> deployment. Early exiting in conjunction with multi-stage predictors, when
> cooperated with a parameter-efficient fine-tuning strategy, offers a
> straightforward way to achieve an inference-efficient model. However, a key
> challenge remains unresolved: How can early stages provide low-level
> fundamental features to deep stages while simultaneously supplying high-level
> discriminative features to early-stage predictors? To address this problem, we
> propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively
> decouple the low-level representative ability and high-level discriminative
> ability in early stages. First, in terms of architecture, we introduce a
> lightweight bypass module into multi-stage predictors for functional
> decomposition of shallow features from early stages, while a high-order
> statistics-based predictor is developed for early stages to effectively enhance
> their discriminative ability. To reasonably train our multi-predictor
> architecture, a decoupled optimization is proposed to allocate two-phase loss
> weights for multi-stage predictors during model tuning, where the initial
> training phase enables the model to prioritize the acquisition of
> discriminative ability of deep stages via emphasizing representative ability of
> early stages, and the latter training phase drives discriminative ability
> towards earlier stages as much as possible. As such, our DMPO can effectively
> decouple representative and discriminative abilities in early stages in terms
> of architecture design and model optimization. Experiments across various
> datasets and pre-trained backbones demonstrate that DMPO clearly outperforms
> its counterparts when reducing computational cost.

