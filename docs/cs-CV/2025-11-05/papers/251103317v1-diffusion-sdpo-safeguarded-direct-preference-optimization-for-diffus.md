---
layout: default
title: Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models
---

# Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models

**arXiv**: [2511.03317v1](https://arxiv.org/abs/2511.03317) | [PDF](https://arxiv.org/pdf/2511.03317.pdf)

**ä½œè€…**: Minghao Fu, Guo-Hua Wang, Tianyu Cui, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiffusion-SDPOä»¥è§£å†³æ‰©æ•£æ¨¡åž‹åå¥½ä¼˜åŒ–ä¸­çš„ç”Ÿæˆè´¨é‡é€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `åå¥½ä¼˜åŒ–` `æ¢¯åº¦ç¼©æ”¾` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¯¹é½æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ ‡å‡†Diffusion-DPOæ‰©å¤§åå¥½é—´éš”å¯èƒ½å¯¼è‡´èƒœè´¥åˆ†æ”¯é‡å»ºè¯¯å·®å¢žåŠ ï¼Œå½±å“ç”Ÿæˆè´¨é‡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è‡ªé€‚åº”ç¼©æ”¾è´¥è€…æ¢¯åº¦çš„ä¿æŠ¤æ›´æ–°è§„åˆ™ï¼Œç¡®ä¿èƒœè€…è¾“å‡ºè¯¯å·®éžå¢ž
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼Œåå¥½ã€ç¾Žå­¦å’Œæç¤ºå¯¹é½æŒ‡æ ‡ä¼˜äºŽåŸºçº¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-image diffusion models deliver high-quality images, yet aligning them
> with human preferences remains challenging. We revisit diffusion-based Direct
> Preference Optimization (DPO) for these models and identify a critical
> pathology: enlarging the preference margin does not necessarily improve
> generation quality. In particular, the standard Diffusion-DPO objective can
> increase the reconstruction error of both winner and loser branches.
> Consequently, degradation of the less-preferred outputs can become sufficiently
> severe that the preferred branch is also adversely affected even as the margin
> grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
> that preserves the winner by adaptively scaling the loser gradient according to
> its alignment with the winner gradient. A first-order analysis yields a
> closed-form scaling coefficient that guarantees the error of the preferred
> output is non-increasing at each optimization step. Our method is simple,
> model-agnostic, broadly compatible with existing DPO-style alignment frameworks
> and adds only marginal computational overhead. Across standard text-to-image
> benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
> baselines on automated preference, aesthetic, and prompt alignment metrics.
> Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.

