---
layout: default
title: Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks
---

# Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks

**arXiv**: [2511.03328v1](https://arxiv.org/abs/2511.03328) | [PDF](https://arxiv.org/pdf/2511.03328.pdf)

**ä½œè€…**: Jindong Hong, Tianjie Chen, Lingjie Luo, Chuanyang Zheng, Ting Xu, Haibao Yu, Jianing Qiu, Qianzhong Chen, Suning Huang, Yan Xu, Yong Gui, Yijun He, Jiankai Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨ä¸´åºŠä»»åŠ¡ä¸­æ€ç»´æ¨¡å¼å¯¹æ€§èƒ½çš„å½±å“**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ä¸´åºŠä»»åŠ¡è¯„ä¼°` `æ€ç»´æ¨¡å¼` `åŒ»å­¦å›¾åƒè§£é‡Š` `è§†è§‰é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯„ä¼°MLLMsçš„æ€ç»´æ¨¡å¼åœ¨ä¸´åºŠä»»åŠ¡ä¸­æ˜¯å¦æ˜¾è‘—æå‡æ¨¡åž‹æ€§èƒ½ä¸Žå¯é æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¯”è¾ƒSeed1.5-VLå’ŒGemini-2.5-Flashåœ¨æ€ç»´æ¨¡å¼ä¸Žéžæ€ç»´æ¨¡å¼ä¸‹çš„è¡¨çŽ°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ€ç»´æ¨¡å¼å¯¹å¤šæ•°ä»»åŠ¡æ”¹è¿›æœ‰é™ï¼Œå¤æ‚åŒ»ç–—ä»»åŠ¡è¡¨çŽ°ä»ä¸ç†æƒ³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A recent advancement in Multimodal Large Language Models (MLLMs) research is
> the emergence of "reasoning MLLMs" that offer explicit control over their
> internal thinking processes (normally referred as the "thinking mode")
> alongside the standard "non-thinking mode". This capability allows these models
> to engage in a step-by-step process of internal deliberation before generating
> a final response. With the rapid transition to and adoption of these
> "dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
> processes of these MLLMs impact model performance and reliability in clinical
> tasks. This paper evaluates the active "thinking mode" capabilities of two
> leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
> assessed their performance on four visual medical tasks using VQA-RAD and
> ROCOv2 datasets. Our findings reveal that the improvement from activating the
> thinking mode remains marginal compared to the standard non-thinking mode for
> the majority of the tasks. Their performance on complex medical tasks such as
> open-ended VQA and medical image interpretation remains suboptimal,
> highlighting the need for domain-specific medical data and more advanced
> methods for medical knowledge integration.

