---
layout: default
title: SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding
---

# SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding

**arXiv**: [2511.03325v1](https://arxiv.org/abs/2511.03325) | [PDF](https://arxiv.org/pdf/2511.03325.pdf)

**ä½œè€…**: Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSurgViVQAæ¨¡åž‹ä»¥è§£å†³æ‰‹æœ¯è§†é¢‘é—®ç­”ä¸­çš„åŠ¨æ€åœºæ™¯ç†è§£é—®é¢˜**

**å…³é”®è¯**: `æ‰‹æœ¯è§†é¢‘é—®ç­”` `æ—¶é—´åŸºç¡€ç†è§£` `æŽ©ç è§†é¢‘ç¼–ç ` `å¤§è¯­è¨€æ¨¡åž‹` `åŠ¨æ€åœºæ™¯åˆ†æž` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ‰‹æœ¯VideoQAæ–¹æ³•ä¾èµ–é™æ€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥åŠ¨æ€äº‹ä»¶ï¼Œå¯¼è‡´ç†è§£ä¸å‡†ç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æŽ©ç è§†é¢‘-æ–‡æœ¬ç¼–ç å™¨èžåˆè§†é¢‘ä¸Žé—®é¢˜ç‰¹å¾ï¼Œç»“åˆLLMè§£ç ï¼Œæ•æ‰è¿åŠ¨ä¸Žå·¥å…·-ç»„ç»‡äº¤äº’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨REAL-Colon-VQAå’ŒEndoVis18-VQAæ•°æ®é›†ä¸Šï¼Œå…³é”®è¯å‡†ç¡®çŽ‡åˆ†åˆ«æå‡11%å’Œ9%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Question Answering (VideoQA) in the surgical domain aims to enhance
> intraoperative understanding by enabling AI models to reason over temporally
> coherent events rather than isolated frames. Current approaches are limited to
> static image features, and available datasets often lack temporal annotations,
> ignoring the dynamics critical for accurate procedural interpretation. We
> propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
> static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
> to fuse video and question features, capturing temporal cues such as motion and
> tool--tissue interactions, which a fine-tuned large language model (LLM) then
> decodes into coherent answers. To evaluate its performance, we curated
> REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
> questions and diagnostic attributes, as well as out-of-template questions with
> rephrased or semantically altered formulations to assess model robustness.
> Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
> shows that SurgViVQA outperforms existing image-based VQA benchmark models,
> particularly in keyword accuracy, improving over PitVQA by +11\% on
> REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
> further confirms improved generalizability and robustness to variations in
> question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
> for temporally-aware understanding in surgical VideoQA, enabling AI models to
> interpret dynamic procedural contexts more effectively. Code and dataset
> available at https://github.com/madratak/SurgViVQA.

