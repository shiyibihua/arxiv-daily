---
layout: default
title: Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control
---

# Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control

**arXiv**: [2511.03181v1](https://arxiv.org/abs/2511.03181) | [PDF](https://arxiv.org/pdf/2511.03181.pdf)

**ä½œè€…**: Rewida Ali, Cristian C. Beltran-Hernandez, Weiwei Wan, Kensuke Harada

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå­¦ä¹ çš„ç»Ÿä¸€æŽ§åˆ¶ç­–ç•¥ï¼Œç»“åˆæ®‹å·®åŠ›æŽ§åˆ¶ï¼Œè§£å†³äººæœºåä½œåŒ…è£…å˜å½¢ç‰©ä½“é—®é¢˜ã€‚**

**å…³é”®è¯**: `äººæœºåä½œ` `å˜å½¢ç‰©ä½“æ“ä½œ` `ç»Ÿä¸€æŽ§åˆ¶ç­–ç•¥` `æ®‹å·®åŠ›æŽ§åˆ¶` `é•¿æ—¶ç¨‹ä»»åŠ¡` `åŸºäºŽå­¦ä¹ æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå˜å½¢ç‰©ä½“åŠ¨æ€ä¸å¯é¢„æµ‹ï¼Œéœ€è‡ªé€‚åº”åŠ›æŽ§åˆ¶ï¼Œå®žçŽ°é•¿æ—¶ç¨‹åŒ…è£…ä»»åŠ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆLLMä»»åŠ¡è§„åˆ’ä¸ŽIL/RLç­–ç•¥ï¼Œä½¿ç”¨STARTæ¨¡åž‹æ•èŽ·é•¿ç¨‹æ—¶åºä¾èµ–ã€‚
3. å®žéªŒæ•ˆæžœï¼šçœŸå®žä¸–ç•ŒåŒ…è£…ä»»åŠ¡æˆåŠŸçŽ‡97%ï¼Œç»Ÿä¸€ç­–ç•¥å‡å°‘ä¸“ç”¨æ¨¡åž‹éœ€æ±‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Human-robot cooperation is essential in environments such as warehouses and
> retail stores, where workers frequently handle deformable objects like paper,
> bags, and fabrics. Coordinating robotic actions with human assistance remains
> difficult due to the unpredictable dynamics of deformable materials and the
> need for adaptive force control. To explore this challenge, we focus on the
> task of gift wrapping, which exemplifies a long-horizon manipulation problem
> involving precise folding, controlled creasing, and secure fixation of paper.
> Success is achieved when the robot completes the sequence to produce a neatly
> wrapped package with clean folds and no tears.
>   We propose a learning-based framework that integrates a high-level task
> planner powered by a large language model (LLM) with a low-level hybrid
> imitation learning (IL) and reinforcement learning (RL) policy. At its core is
> a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
> human demonstrations. The key novelty lies in capturing long-range temporal
> dependencies across the full wrapping sequence within a single model. Unlike
> vanilla Action Chunking with Transformer (ACT), typically applied to short
> tasks, our method introduces sub-task IDs that provide explicit temporal
> grounding. This enables robust performance across the entire wrapping process
> and supports flexible execution, as the policy learns sub-goals rather than
> merely replicating motion sequences.
>   Our framework achieves a 97% success rate on real-world wrapping tasks. We
> show that the unified transformer-based policy reduces the need for specialized
> models, allows controlled human supervision, and effectively bridges high-level
> intent with the fine-grained force control required for deformable object
> manipulation.

