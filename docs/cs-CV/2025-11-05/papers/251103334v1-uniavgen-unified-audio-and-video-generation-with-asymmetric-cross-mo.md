---
layout: default
title: UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions
---

# UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions

**arXiv**: [2511.03334v1](https://arxiv.org/abs/2511.03334) | [PDF](https://arxiv.org/pdf/2511.03334.pdf)

**ä½œè€…**: Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, Limin Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniAVGenæ¡†æž¶ä»¥è§£å†³éŸ³è§†é¢‘ç”Ÿæˆä¸­çš„è·¨æ¨¡æ€åŒæ­¥ä¸Žè¯­ä¹‰ä¸€è‡´æ€§é—®é¢˜**

**å…³é”®è¯**: `éŸ³è§†é¢‘ç”Ÿæˆ` `è·¨æ¨¡æ€äº¤äº’` `æ‰©æ•£å˜æ¢å™¨` `å”‡éƒ¨åŒæ­¥` `è¯­ä¹‰ä¸€è‡´æ€§` `è”åˆåˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å› è·¨æ¨¡æ€å»ºæ¨¡ä¸è¶³ï¼Œå¯¼è‡´å”‡éƒ¨åŒæ­¥å’Œè¯­ä¹‰ä¸€è‡´æ€§å·®
2. é‡‡ç”¨åŒåˆ†æ”¯æ‰©æ•£å˜æ¢å™¨ä¸Žä¸å¯¹ç§°è·¨æ¨¡æ€äº¤äº’æœºåˆ¶ï¼Œå¢žå¼ºæ—¶ç©ºå¯¹é½
3. å®žéªŒæ˜¾ç¤ºï¼Œè®­ç»ƒæ ·æœ¬å°‘ä½†éŸ³è§†é¢‘åŒæ­¥ã€éŸ³è‰²å’Œæƒ…æ„Ÿä¸€è‡´æ€§æ›´ä¼˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Due to the lack of effective cross-modal modeling, existing open-source
> audio-video generation methods often exhibit compromised lip synchronization
> and insufficient semantic consistency. To mitigate these drawbacks, we propose
> UniAVGen, a unified framework for joint audio and video generation. UniAVGen is
> anchored in a dual-branch joint synthesis architecture, incorporating two
> parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent
> space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which
> enables bidirectional, temporally aligned cross-attention, thus ensuring
> precise spatiotemporal synchronization and semantic consistency. Furthermore,
> this cross-modal interaction is augmented by a Face-Aware Modulation module,
> which dynamically prioritizes salient regions in the interaction process. To
> enhance generative fidelity during inference, we additionally introduce
> Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly
> amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint
> synthesis design enables seamless unification of pivotal audio-video tasks
> within a single model, such as joint audio-video generation and continuation,
> video-to-audio dubbing, and audio-driven video synthesis. Comprehensive
> experiments validate that, with far fewer training samples (1.3M vs. 30.1M),
> UniAVGen delivers overall advantages in audio-video synchronization, timbre
> consistency, and emotion consistency.

