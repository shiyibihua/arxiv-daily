---
layout: default
title: Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps
---

# Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.05590" target="_blank" class="toolbar-btn">arXiv: 2511.05590v1</a>
    <a href="https://arxiv.org/pdf/2511.05590.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05590v1" 
            onclick="toggleFavorite(this, '2511.05590v1', 'Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yoojin Oh, Junhyug Noh

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-05

**Â§áÊ≥®**: Accepted at BMVC 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/finallyupper/beyond-softmax)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèåÂàÜÊîØSigmoidÊû∂ÊûÑÔºåËß£ÂÜ≥CAM‰∏≠logitÂÅèÁßªÂíåÁ¨¶Âè∑ÂùçÂ°åÈóÆÈ¢òÔºåÊèêÂçáÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Á±ªÊøÄÊ¥ªÊò†Â∞Ñ` `ÂèØËß£ÈáäÊÄß` `Âº±ÁõëÁù£ÂÆö‰Ωç` `ÂèåÂàÜÊîØÁΩëÁªú` `SigmoidÂàÜÁ±ªÂô®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüCAMÊñπÊ≥ï‰æùËµñSoftmaxÂàÜÁ±ªÂô®ÔºåÂ≠òÂú®logitÂÅèÁßªÂíåÁ¨¶Âè∑ÂùçÂ°åÈóÆÈ¢òÔºåÂΩ±ÂìçÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ
2. ÊèêÂá∫ÂèåÂàÜÊîØSigmoidÊû∂ÊûÑÔºåËß£ËÄ¶ÂàÜÁ±ªÂíåÂÆö‰ΩçÔºåÂà©Áî®SigmoidÂàÜÊîØÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÁ±ªÊøÄÊ¥ªÂõæ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊèêÂçá‰∫ÜËß£Èáä‰øùÁúüÂ∫¶ÂíåTop-1ÂÆö‰ΩçÁ≤æÂ∫¶Ôºå‰∏î‰∏çÊçüÂ§±ÂàÜÁ±ªÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á±ªÊøÄÊ¥ªÊò†Â∞Ñ(CAM)ÂèäÂÖ∂Êâ©Â±ïÂ∑≤Êàê‰∏∫ÂèØËßÜÂåñÊ∑±Â∫¶ÁΩëÁªúÈ¢ÑÊµã‰æùÊçÆÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é‰æùËµñÊúÄÁªàÁöÑsoftmaxÂàÜÁ±ªÂô®ÔºåËøô‰∫õÊñπÊ≥ïÂ≠òÂú®‰∏§‰∏™Ê†πÊú¨ÊÄßÁöÑÂ§±ÁúüÔºö‰ªªÊÑèÂÅèÁΩÆÈáçË¶ÅÊÄßÂàÜÊï∞ÁöÑÂä†ÊÄßlogitÂÅèÁßªÔºå‰ª•ÂèäÊ∑∑Ê∑Ü‰∫ÜÂÖ¥Â•ãÂíåÊäëÂà∂ÁâπÂæÅÁöÑÁ¨¶Âè∑ÂùçÂ°å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑ„ÄÅ‰∏éÊû∂ÊûÑÊó†ÂÖ≥ÁöÑÂèåÂàÜÊîØsigmoidÂ§¥ÔºåÂ∞ÜÂÆö‰Ωç‰∏éÂàÜÁ±ªËß£ËÄ¶„ÄÇÁªôÂÆö‰ªª‰ΩïÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÊàë‰ª¨Â∞ÜÂÖ∂ÂàÜÁ±ªÂ§¥ÂÖãÈöÜÂà∞‰∏Ä‰∏™‰ª•per-class sigmoidËæìÂá∫ÁªìÊùüÁöÑÂπ∂Ë°åÂàÜÊîØ‰∏≠ÔºåÂÜªÁªìÂéüÂßãÁöÑsoftmaxÂ§¥ÔºåÂπ∂‰ªÖ‰ΩøÁî®Á±ªÂà´Âπ≥Ë°°ÁöÑ‰∫åÂÖÉÁõëÁù£ÂæÆË∞ÉsigmoidÂàÜÊîØ„ÄÇÂú®Êé®ÁêÜÊó∂Ôºåsoftmax‰øùÁïôËØÜÂà´Á≤æÂ∫¶ÔºåËÄåÁ±ªËØÅÊçÆÂõæ‰ªésigmoidÂàÜÊîØÁîüÊàê‚Äî‚Äî‰øùÁïô‰∫ÜÁâπÂæÅË¥°ÁåÆÁöÑÂ§ßÂ∞èÂíåÁ¨¶Âè∑„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏éÂ§ßÂ§öÊï∞CAMÂèò‰ΩìÊó†ÁºùÈõÜÊàêÔºåÂπ∂‰∏îÂºÄÈîÄÂèØÂøΩÁï•‰∏çËÆ°„ÄÇÂú®ÁªÜÁ≤íÂ∫¶‰ªªÂä°(CUB-200-2011, Stanford Cars)ÂíåWSOLÂü∫ÂáÜ(ImageNet-1K, OpenImages30K)‰∏äÁöÑÂπøÊ≥õËØÑ‰º∞Ë°®ÊòéÔºåËß£Èáä‰øùÁúüÂ∫¶ÂæóÂà∞ÊîπÂñÑÔºåÂπ∂‰∏îTop-1ÂÆö‰ΩçÂßãÁªàËé∑ÂæóÊèêÂçá‚Äî‚ÄîËÄåÂàÜÁ±ªÁ≤æÂ∫¶Ê≤°Êúâ‰ªª‰Ωï‰∏ãÈôç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÁ±ªÊøÄÊ¥ªÊò†Â∞ÑÔºàCAMÔºâÊñπÊ≥ï‰æùËµñ‰∫éSoftmaxÂàÜÁ±ªÂô®ËøõË°åÈ¢ÑÊµãÔºåËøôÂØºËá¥‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇ‰∏ÄÊòØSoftmaxÁöÑlogitÂÅèÁßª‰ºö‰ªªÊÑèÂÅèÁΩÆÈáçË¶ÅÊÄßÂàÜÊï∞Ôºå‰ΩøÂæóÁîüÊàêÁöÑÊøÄÊ¥ªÂõæ‰∏çÂáÜÁ°Æ„ÄÇ‰∫åÊòØSoftmaxÂ∞ÜÂÖ¥Â•ãÂíåÊäëÂà∂ÁâπÂæÅÊ∑∑Ê∑ÜÔºåÂØºËá¥Á¨¶Âè∑ÂùçÂ°åÔºåÊó†Ê≥ïÂå∫ÂàÜÊ≠£Ë¥üË¥°ÁåÆÁöÑÁâπÂæÅ„ÄÇËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫ÜCAMÊñπÊ≥ïÂú®ÂÆö‰ΩçÂíåËß£ÈáäÊ®°ÂûãÈ¢ÑÊµãÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂàÜÁ±ªÂíåÂÆö‰ΩçËß£ËÄ¶„ÄÇÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™Âπ∂Ë°åÁöÑSigmoidÂàÜÊîØÔºå‰∏ìÈó®Ë¥üË¥£ÁîüÊàêÁ±ªÊøÄÊ¥ªÂõæ„ÄÇSoftmaxÂàÜÊîØ‰øùÊåÅÂéüÊúâÁöÑÂàÜÁ±ªÂäüËÉΩÔºåËÄåSigmoidÂàÜÊîØÂàôÈÄöËøá‰∫åÂÖÉ‰∫§ÂèâÁÜµÊçüÂ§±ËøõË°åËÆ≠ÁªÉÔºåÂ≠¶‰π†ÊØè‰∏™Á±ªÂà´ÁöÑÁã¨Á´ãÊøÄÊ¥ªÂõæ„ÄÇËøôÊ†∑ÂèØ‰ª•ÈÅøÂÖçSoftmaxÁöÑlogitÂÅèÁßªÂíåÁ¨¶Âè∑ÂùçÂ°åÈóÆÈ¢òÔºå‰ªéËÄåÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÁ±ªÊøÄÊ¥ªÂõæ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØÂèåÂàÜÊîØÊû∂ÊûÑ„ÄÇÈ¶ñÂÖàÔºåÁªôÂÆö‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑÂàÜÁ±ªÊ®°ÂûãÔºåÂÖãÈöÜÂÖ∂ÂàÜÁ±ªÂ§¥ÔºåÂàõÂª∫‰∏Ä‰∏™Âπ∂Ë°åÁöÑSigmoidÂàÜÊîØ„ÄÇÂéüÂßãÁöÑSoftmaxÂàÜÊîØ‰øùÊåÅÂÜªÁªìÔºåÂè™ËÆ≠ÁªÉSigmoidÂàÜÊîØ„ÄÇSigmoidÂàÜÊîØÁöÑËæìÂá∫ÊòØÊØè‰∏™Á±ªÂà´ÁöÑÁã¨Á´ãÊ¶ÇÁéáÔºå‰ΩøÁî®Á±ªÂà´Âπ≥Ë°°ÁöÑ‰∫åÂÖÉ‰∫§ÂèâÁÜµÊçüÂ§±ËøõË°åËÆ≠ÁªÉ„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåSoftmaxÂàÜÊîØÁî®‰∫éÂàÜÁ±ªÔºåËÄåSigmoidÂàÜÊîØÁî®‰∫éÁîüÊàêÁ±ªÊøÄÊ¥ªÂõæ„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•‰∏éÂ§ßÂ§öÊï∞CAMÂèò‰ΩìÊó†ÁºùÈõÜÊàê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éËß£ËÄ¶‰∫ÜÂàÜÁ±ªÂíåÂÆö‰Ωç„ÄÇÈÄöËøáÂºïÂÖ•Áã¨Á´ãÁöÑSigmoidÂàÜÊîØÔºåÈÅøÂÖç‰∫ÜSoftmaxÁöÑÂõ∫ÊúâÁº∫Èô∑ÂØπÁ±ªÊøÄÊ¥ªÂõæÁöÑÂΩ±Âìç„ÄÇËøôÁßçËß£ËÄ¶ÁöÑÊÄùÊÉ≥ÂèØ‰ª•Â∫îÁî®‰∫éÂÖ∂‰ªñÈúÄË¶ÅËß£ÈáäÊÄßÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°Âûã‰∏≠ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆÁöÑËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Á±ªÂà´Âπ≥Ë°°ÁöÑ‰∫åÂÖÉ‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉSigmoidÂàÜÊîØÔºå‰ª•Ëß£ÂÜ≥Á±ªÂà´‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇ2) ÂÜªÁªìSoftmaxÂàÜÊîØÔºåÂè™ËÆ≠ÁªÉSigmoidÂàÜÊîØÔºå‰ª•‰øùÊåÅÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ3) SigmoidÂàÜÊîØÁöÑËæìÂá∫ÊòØÊØè‰∏™Á±ªÂà´ÁöÑÁã¨Á´ãÊ¶ÇÁéáÔºåËÄå‰∏çÊòØSoftmaxÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ4) ËØ•ÊñπÊ≥ï‰∏éÂ§ßÂ§öÊï∞CAMÂèò‰ΩìÂÖºÂÆπÔºåÂèØ‰ª•Êñπ‰æøÂú∞Â∫îÁî®‰∫é‰∏çÂêåÁöÑÊ®°ÂûãÂíå‰ªªÂä°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®CUB-200-2011„ÄÅStanford Cars„ÄÅImageNet-1KÂíåOpenImages30KÁ≠âÊï∞ÊçÆÈõÜ‰∏äÔºåÊòæËëóÊèêÂçá‰∫ÜËß£Èáä‰øùÁúüÂ∫¶ÂíåTop-1ÂÆö‰ΩçÁ≤æÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºåÂú®ImageNet-1KÊï∞ÊçÆÈõÜ‰∏äÔºåTop-1ÂÆö‰ΩçÁ≤æÂ∫¶ÊèêÂçá‰∫ÜÂ§ö‰∏™ÁôæÂàÜÁÇπ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Ëß£ÂÜ≥SoftmaxÂ∏¶Êù•ÁöÑÈóÆÈ¢òÔºåÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÁ±ªÊøÄÊ¥ªÂõæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÊ®°ÂûãÂèØËß£ÈáäÊÄßÁöÑÈ¢ÜÂüüÔºå‰æãÂ¶ÇÂåªÂ≠¶ÂõæÂÉèËØäÊñ≠„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂÆâÂÖ®ÁõëÊéßÁ≠â„ÄÇÈÄöËøáÊèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÁ±ªÊøÄÊ¥ªÂõæÔºåÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑ÁêÜËß£Ê®°ÂûãÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂèØ‰ø°Â∫¶„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÂº±ÁõëÁù£ÁõÆÊ†áÂÆö‰ΩçÔºåÊèêÈ´òÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.

