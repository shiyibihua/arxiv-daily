---
layout: default
title: Populate-A-Scene: Affordance-Aware Human Video Generation
---

# Populate-A-Scene: Affordance-Aware Human Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00334" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00334v1</a>
  <a href="https://arxiv.org/pdf/2507.00334.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00334v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00334v1', 'Populate-A-Scene: Affordance-Aware Human Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengyi Shan, Zecheng He, Haoyu Ma, Felix Juefei-Xu, Peizhao Zhang, Tingbo Hou, Ching-Yao Chuang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-07-01

**å¤‡æ³¨**: Project page: https://shanmy.github.io/Populate-A-Scene

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåœºæ™¯å›¾åƒçš„äººç±»è§†é¢‘ç”Ÿæˆæ¨¡å‹ä»¥è§£å†³äº¤äº’æ¨¡æ‹Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `äººæœºäº¤äº’` `åœºæ™¯ç†è§£` `å¯ç”¨æ€§æ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†äººç±»ä¸ç¯å¢ƒçš„äº¤äº’æ—¶ï¼Œé€šå¸¸ä¾èµ–äºæ˜ç¡®çš„æ¡ä»¶ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–å§¿åŠ¿ï¼Œè¿™é™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å¯¹åœºæ™¯å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ä¾èµ–æ˜ç¡®æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨æ¨æ–­äººç±»çš„å¯ç”¨æ€§å¹¶ç”Ÿæˆç›¸åº”çš„è§†é¢‘ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè§†é¢‘æ—¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿æŒäººç‰©è¡Œä¸ºçš„è¿è´¯æ€§å’Œåœºæ™¯çš„å’Œè°æ€§ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸ºäº¤äº’ä¸–ç•Œæ¨¡æ‹Ÿå™¨çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹åœºæ™¯å›¾åƒå’Œæè¿°äººç±»è¡Œä¸ºçš„æç¤ºè¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨åœºæ™¯ä¸­æ’å…¥äººç‰©ï¼Œå¹¶ç¡®ä¿å…¶è¡Œä¸ºã€å¤–è§‚å’Œåœºæ™¯çš„å’Œè°æ€§ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡ä»å•ä¸€åœºæ™¯å›¾åƒä¸­æ¨æ–­äººç±»çš„å¯ç”¨æ€§ï¼Œè€Œæ— éœ€æ˜ç¡®çš„æ¡ä»¶å¦‚è¾¹ç•Œæ¡†æˆ–èº«ä½“å§¿åŠ¿ã€‚é€šè¿‡å¯¹äº¤å‰æ³¨æ„åŠ›çƒ­å›¾çš„æ·±å…¥ç ”ç©¶ï¼Œæ­ç¤ºäº†é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹çš„å†…åœ¨å¯ç”¨æ€§æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€Œæ— éœ€æ ‡è®°çš„å¯ç”¨æ€§æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†äººç±»ä¸ç¯å¢ƒäº¤äº’æ—¶çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹æ˜ç¡®æ¡ä»¶çš„ä¾èµ–æ€§ï¼Œè¿™ä½¿å¾—æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä»å•ä¸€åœºæ™¯å›¾åƒä¸­æ¨æ–­äººç±»çš„å¯ç”¨æ€§ï¼Œè¿›è€Œç”Ÿæˆè‡ªç„¶çš„äº¤äº’è§†é¢‘ï¼Œè€Œæ— éœ€ä¾èµ–è¾¹ç•Œæ¡†æˆ–å§¿åŠ¿ç­‰æ˜¾å¼æ¡ä»¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥åœºæ™¯å›¾åƒå’Œæè¿°äººç±»è¡Œä¸ºçš„æç¤ºï¼Œæ¨¡å‹é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿¡æ¯èåˆï¼Œç”ŸæˆåŒ…å«äººç‰©çš„åŠ¨æ€è§†é¢‘ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬åœºæ™¯ç†è§£ã€è¡Œä¸ºæ¨æ–­å’Œè§†é¢‘ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºèƒ½å¤Ÿä»å•ä¸€åœºæ™¯å›¾åƒä¸­æ¨æ–­äººç±»çš„å¯ç”¨æ€§ï¼Œè€Œä¸éœ€è¦æ ‡è®°æ•°æ®é›†ï¼Œè¿™ä¸€æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºåœºæ™¯ä¸äººç‰©ä¹‹é—´çš„å…³è”ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç”Ÿæˆè§†é¢‘çš„è¿è´¯æ€§å’Œå’Œè°æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç”Ÿæˆè§†é¢‘çš„è¿è´¯æ€§å’Œåœºæ™¯å’Œè°æ€§æ–¹é¢ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æŒ‡æ ‡æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨æ— æ ‡è®°æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡å®ç°æ›´è‡ªç„¶çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’çš„çœŸå®æ„Ÿï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å¨±ä¹å’Œè®­ç»ƒç­‰å¤šä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Can a video generation model be repurposed as an interactive world simulator? We explore the affordance perception potential of text-to-video models by teaching them to predict human-environment interaction. Given a scene image and a prompt describing human actions, we fine-tune the model to insert a person into the scene, while ensuring coherent behavior, appearance, harmonization, and scene affordance. Unlike prior work, we infer human affordance for video generation (i.e., where to insert a person and how they should behave) from a single scene image, without explicit conditions like bounding boxes or body poses. An in-depth study of cross-attention heatmaps demonstrates that we can uncover the inherent affordance perception of a pre-trained video model without labeled affordance datasets.

