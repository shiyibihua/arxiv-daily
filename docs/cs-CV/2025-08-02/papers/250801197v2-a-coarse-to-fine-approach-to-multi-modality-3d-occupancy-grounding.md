---
layout: default
title: A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding
---

# A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01197" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01197v2</a>
  <a href="https://arxiv.org/pdf/2508.01197.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01197v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01197v2', 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-02 (æ›´æ–°: 2025-09-03)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/RONINGOD/GroundingOcc)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGroundingOccä»¥è§£å†³3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šæ¨¡æ€å­¦ä¹ ` `å ç”¨åŸºç¡€` `è‡ªç„¶è¯­è¨€å¤„ç†` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰å®šä½æ–¹æ³•ä¾èµ–è¾¹ç•Œæ¡†ï¼Œæ— æ³•å‡†ç¡®æ•æ‰ç‰©ä½“çš„ç»†èŠ‚ï¼Œå¯¼è‡´ç‰©ä½“è¡¨ç¤ºä¸å‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºGroundingOccæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€å­¦ä¹ æ•´åˆè§†è§‰ã€æ–‡æœ¬å’Œç‚¹äº‘ç‰¹å¾ï¼Œå®ç°3Då ç”¨åŸºç¡€çš„ç²¾ç¡®å®šä½ã€‚
3. åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGroundingOccåœ¨3Då ç”¨åŸºç¡€å®šä½ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å®šä½æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°è¯†åˆ«åœºæ™¯ä¸­çš„ç‰©ä½“æˆ–åŒºåŸŸï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´æ„ŸçŸ¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰å®šä½ä»»åŠ¡é€šå¸¸ä¾èµ–äºè¾¹ç•Œæ¡†ï¼Œæ— æ³•æ•æ‰ç»†ç²’åº¦çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹å¤æ‚æˆ·å¤–åœºæ™¯çš„3Då ç”¨åŸºç¡€åŸºå‡†ï¼ŒåŸºäºnuScenesæ•°æ®é›†ï¼Œç»“åˆè‡ªç„¶è¯­è¨€ä¸ä½“ç´ çº§å ç”¨æ³¨é‡Šï¼Œæä¾›æ¯”ä¼ ç»Ÿå®šä½ä»»åŠ¡æ›´ç²¾ç¡®çš„ç‰©ä½“æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†GroundingOccï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€å­¦ä¹ å®ç°3Då ç”¨åŸºç¡€çš„å®šä½ï¼Œç»“åˆè§†è§‰ã€æ–‡æœ¬å’Œç‚¹äº‘ç‰¹å¾ï¼Œä»ç²—åˆ°ç»†é¢„æµ‹ç‰©ä½“ä½ç½®å’Œå ç”¨ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨3Då ç”¨åŸºç¡€å®šä½ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰å®šä½æ–¹æ³•åœ¨3Dåœºæ™¯ä¸­å¯¹ç‰©ä½“ç»†èŠ‚æ•æ‰ä¸è¶³çš„é—®é¢˜ï¼Œä¼ ç»Ÿçš„è¾¹ç•Œæ¡†æ–¹æ³•å¸¸å¸¸å¯¼è‡´ä¸å‡†ç¡®çš„ç‰©ä½“è¡¨ç¤ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†GroundingOccæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€å­¦ä¹ ï¼Œç»“åˆè§†è§‰ã€æ–‡æœ¬å’Œç‚¹äº‘ç‰¹å¾ï¼Œä»ç²—åˆ°ç»†åœ°é¢„æµ‹ç‰©ä½“çš„ä½ç½®å’Œå ç”¨ä¿¡æ¯ï¼Œä»¥æé«˜å®šä½çš„ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGroundingOccçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šæ¨¡æ€ç¼–ç å™¨ç”¨äºç‰¹å¾æå–ã€å ç”¨å¤´ç”¨äºä½“ç´ çº§é¢„æµ‹ã€ä»¥åŠå®šä½å¤´ç”¨äºç²¾ç»†åŒ–å®šä½ã€‚æ­¤å¤–ï¼Œ2Då®šä½æ¨¡å—å’Œæ·±åº¦ä¼°è®¡æ¨¡å—å¢å¼ºäº†å‡ ä½•ç†è§£ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ä½“ç´ çº§å ç”¨æ³¨é‡Šï¼Œç»“åˆå¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œ3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ï¼Œè¿™ä¸€æ–¹æ³•åœ¨ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è¾¹ç•Œæ¡†æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€ç¼–ç å™¨æ¥å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥æ•°æ®ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡ä½“ç´ çº§çš„å‡†ç¡®æ€§ï¼Œç½‘ç»œç»“æ„åˆ™é€šè¿‡å±‚æ¬¡åŒ–å¤„ç†å®ç°ä»ç²—åˆ°ç»†çš„é¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGroundingOccæ¨¡å‹åœ¨3Då ç”¨åŸºç¡€å®šä½ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚æˆ·å¤–åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›æ›´ç²¾ç¡®çš„ç‰©ä½“å®šä½å’Œç†è§£èƒ½åŠ›ï¼ŒGroundingOccèƒ½å¤Ÿæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼ŒåŒæ—¶ä¹Ÿä¸ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–æä¾›æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ™ºèƒ½åŸå¸‚å’Œæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.

