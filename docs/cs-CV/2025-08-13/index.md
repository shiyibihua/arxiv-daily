---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-13
---

# cs.CVï¼ˆ2025-08-13ï¼‰

ğŸ“Š å…± **44** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (12 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250809789v1-describe-what-you-see-with-multimodal-large-language-models-to-enhan.html">Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥å¢å¼ºè§†é¢‘æ¨èç³»ç»Ÿçš„è¯­ä¹‰ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09789v1" data-paper-url="./papers/250809789v1-describe-what-you-see-with-multimodal-large-language-models-to-enhan.html" onclick="toggleFavorite(this, '2508.09789v1', 'Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250810110v1-empowering-morphing-attack-detection-using-interpretable-image-text-.html">Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ä»¥å¢å¼ºäººè„¸å˜å½¢æ”»å‡»æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10110v1" data-paper-url="./papers/250810110v1-empowering-morphing-attack-detection-using-interpretable-image-text-.html" onclick="toggleFavorite(this, '2508.10110v1', 'Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250809818v2-vimonet-a-multimodal-vision-language-framework-for-human-behavior-un.html">ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</a></td>
  <td>æå‡ºViMoNetä»¥è§£å†³äººç±»è¡Œä¸ºç†è§£ä¸­çš„å¤šæ¨¡æ€æ•°æ®èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09818v2" data-paper-url="./papers/250809818v2-vimonet-a-multimodal-vision-language-framework-for-human-behavior-un.html" onclick="toggleFavorite(this, '2508.09818v2', 'ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250809456v3-iag-input-aware-backdoor-attack-on-vlm-based-visual-grounding.html">IAG: Input-aware Backdoor Attack on VLM-based Visual Grounding</a></td>
  <td>æå‡ºIAGä»¥è§£å†³VLMåŸºç¡€è§†è§‰å®šä½ç³»ç»Ÿçš„åé—¨æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09456v3" data-paper-url="./papers/250809456v3-iag-input-aware-backdoor-attack-on-vlm-based-visual-grounding.html" onclick="toggleFavorite(this, '2508.09456v3', 'IAG: Input-aware Backdoor Attack on VLM-based Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250810133v2-mango-multimodal-attention-based-normalizing-flow-approach-to-fusion.html">MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning</a></td>
  <td>æå‡ºMANGOæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€èåˆå­¦ä¹ çš„ç‰¹å¾æ•æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10133v2" data-paper-url="./papers/250810133v2-mango-multimodal-attention-based-normalizing-flow-approach-to-fusion.html" onclick="toggleFavorite(this, '2508.10133v2', 'MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250809966v1-january-food-benchmark-jfb-a-public-benchmark-dataset-and-evaluation.html">January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis</a></td>
  <td>æå‡ºJanuary Food Benchmarkä»¥è§£å†³è¥å…»åˆ†ææ ‡å‡†åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09966v1" data-paper-url="./papers/250809966v1-january-food-benchmark-jfb-a-public-benchmark-dataset-and-evaluation.html" onclick="toggleFavorite(this, '2508.09966v1', 'January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250809717v1-multimodal-sheaf-based-network-for-glioblastoma-molecular-subtype-pr.html">Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction</a></td>
  <td>æå‡ºåŸºäºsheafçš„å¤šæ¨¡æ€ç½‘ç»œä»¥è§£å†³èƒ¶è´¨æ¯ç»†èƒç˜¤åˆ†å­äºšå‹é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09717v1" data-paper-url="./papers/250809717v1-multimodal-sheaf-based-network-for-glioblastoma-molecular-subtype-pr.html" onclick="toggleFavorite(this, '2508.09717v1', 'Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250809715v1-neural-attention-guided-pruning-for-unified-multimodal-resource-cons.html">NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation</a></td>
  <td>æå‡ºNEURALä»¥è§£å†³èµ„æºå—é™ä¸´åºŠç¯å¢ƒä¸­çš„å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09715v1" data-paper-url="./papers/250809715v1-neural-attention-guided-pruning-for-unified-multimodal-resource-cons.html" onclick="toggleFavorite(this, '2508.09715v1', 'NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250809649v2-the-brain-resection-multimodal-image-registration-remind2reg-2025-ch.html">The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge</a></td>
  <td>æå‡ºReMIND2RegæŒ‘æˆ˜ä»¥è§£å†³è„‘è‚¿ç˜¤æ‰‹æœ¯ä¸­çš„å›¾åƒé…å‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09649v2" data-paper-url="./papers/250809649v2-the-brain-resection-multimodal-image-registration-remind2reg-2025-ch.html" onclick="toggleFavorite(this, '2508.09649v2', 'The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250810232v1-cellsymphony-deciphering-the-molecular-and-phenotypic-orchestration-.html">CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics</a></td>
  <td>æå‡ºCellSymphonyä»¥è§£å†³ç»†èƒç‰¹å¾æå–ä¸ç©ºé—´è½¬å½•ç»„æ•°æ®æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10232v1" data-paper-url="./papers/250810232v1-cellsymphony-deciphering-the-molecular-and-phenotypic-orchestration-.html" onclick="toggleFavorite(this, '2508.10232v1', 'CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250809987v1-echo-4o-harnessing-the-power-of-gpt-4o-synthetic-images-for-improved.html">Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</a></td>
  <td>æå‡ºEcho-4oä»¥è§£å†³å›¾åƒç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09987v1" data-paper-url="./papers/250809987v1-echo-4o-harnessing-the-power-of-gpt-4o-synthetic-images-for-improved.html" onclick="toggleFavorite(this, '2508.09987v1', 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250810104v1-dinov3.html">DINOv3</a></td>
  <td>æå‡ºDINOv3ä»¥è§£å†³è‡ªç›‘ç£å­¦ä¹ ä¸­çš„ç‰¹å¾å›¾é€€åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10104v1" data-paper-url="./papers/250810104v1-dinov3.html" onclick="toggleFavorite(this, '2508.10104v1', 'DINOv3')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250810945v1-iwatchroad-scalable-detection-and-geospatial-visualization-of-pothol.html">iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities</a></td>
  <td>æå‡ºiWatchRoadä»¥è§£å†³å°åº¦é“è·¯å‘æ´¼æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10945v1" data-paper-url="./papers/250810945v1-iwatchroad-scalable-detection-and-geospatial-visualization-of-pothol.html" onclick="toggleFavorite(this, '2508.10945v1', 'iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250809814v2-on-the-dynamic-evolution-of-clip-texture-shape-bias-and-its-relation.html">On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness</a></td>
  <td>åˆ†æCLIPæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„çº¹ç†-å½¢çŠ¶åå·®åŠå…¶ä¸äººç±»æ„ŸçŸ¥çš„å…³ç³»</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09814v2" data-paper-url="./papers/250809814v2-on-the-dynamic-evolution-of-clip-texture-shape-bias-and-its-relation.html" onclick="toggleFavorite(this, '2508.09814v2', 'On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250809632v6-preacher-paper-to-video-agentic-system.html">Preacher: Paper-to-Video Agentic System</a></td>
  <td>æå‡ºPreacherä»¥è§£å†³è®ºæ–‡è½¬è§†é¢‘ç”Ÿæˆçš„å¤šé‡é™åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09632v6" data-paper-url="./papers/250809632v6-preacher-paper-to-video-agentic-system.html" onclick="toggleFavorite(this, '2508.09632v6', 'Preacher: Paper-to-Video Agentic System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250809525v1-learning-spatial-decay-for-vision-transformers.html">Learning Spatial Decay for Vision Transformers</a></td>
  <td>æå‡ºç©ºé—´è¡°å‡å˜æ¢å™¨ä»¥æå‡è§†è§‰å˜æ¢å™¨çš„ç©ºé—´æ³¨æ„åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09525v1" data-paper-url="./papers/250809525v1-learning-spatial-decay-for-vision-transformers.html" onclick="toggleFavorite(this, '2508.09525v1', 'Learning Spatial Decay for Vision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250809461v1-gen-affect-generation-of-avatar-fine-grained-facial-expressions-with.html">Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></td>
  <td>æå‡ºGEN-AFFECTä»¥è§£å†³ä¸ªæ€§åŒ–å¤´åƒç”Ÿæˆä¸­çš„è¡¨æƒ…ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09461v1" data-paper-url="./papers/250809461v1-gen-affect-generation-of-avatar-fine-grained-facial-expressions-with.html" onclick="toggleFavorite(this, '2508.09461v1', 'Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250809977v2-a-survey-on-3d-gaussian-splatting-applications-segmentation-editing-.html">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></td>
  <td>ç»¼è¿°3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯åœ¨åˆ†å‰²ã€ç¼–è¾‘ä¸ç”Ÿæˆä¸­çš„åº”ç”¨</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09977v2" data-paper-url="./papers/250809977v2-a-survey-on-3d-gaussian-splatting-applications-segmentation-editing-.html" onclick="toggleFavorite(this, '2508.09977v2', 'A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250809667v1-gsfixer-improving-3d-gaussian-splatting-with-reference-guided-video-.html">GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</a></td>
  <td>æå‡ºGSFixerä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘é‡å»ºä¸­çš„ä¼ªå½±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09667v1" data-paper-url="./papers/250809667v1-gsfixer-improving-3d-gaussian-splatting-with-reference-guided-video-.html" onclick="toggleFavorite(this, '2508.09667v1', 'GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250809470v1-cityseg-a-3d-open-vocabulary-semantic-segmentation-foundation-model-.html">CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios</a></td>
  <td>æå‡ºCitySegä»¥è§£å†³åŸå¸‚è§„æ¨¡ç‚¹äº‘è¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09470v1" data-paper-url="./papers/250809470v1-cityseg-a-3d-open-vocabulary-semantic-segmentation-foundation-model-.html" onclick="toggleFavorite(this, '2508.09470v1', 'CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250810227v1-entropygs-an-efficient-entropy-coding-on-3d-gaussian-splatting.html">EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting</a></td>
  <td>æå‡ºEntropyGSä»¥é«˜æ•ˆç¼–ç 3D Gaussian Splattingæ•°æ®</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10227v1" data-paper-url="./papers/250810227v1-entropygs-an-efficient-entropy-coding-on-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.10227v1', 'EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250809681v1-surg-invnerf-invertible-nerf-for-3d-tracking-and-reconstruction-in-s.html">Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision</a></td>
  <td>æå‡ºInvertible NeRFä»¥è§£å†³å¤–ç§‘è§†è§‰ä¸­çš„3Dè·Ÿè¸ªä¸é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09681v1" data-paper-url="./papers/250809681v1-surg-invnerf-invertible-nerf-for-3d-tracking-and-reconstruction-in-s.html" onclick="toggleFavorite(this, '2508.09681v1', 'Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250809858v1-humangenesis-agent-based-geometric-and-generative-modeling-for-synth.html">HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</a></td>
  <td>æå‡ºHumanGenesisä»¥è§£å†³åˆæˆäººä½“åŠ¨æ€ä¸­çš„å‡ ä½•ä¸ä¸€è‡´æ€§å’Œè¿åŠ¨æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09858v1" data-paper-url="./papers/250809858v1-humangenesis-agent-based-geometric-and-generative-modeling-for-synth.html" onclick="toggleFavorite(this, '2508.09858v1', 'HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250809423v2-distilling-llm-prior-to-flow-model-for-generalizable-agents-imaginat.html">Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation</a></td>
  <td>æå‡ºGOALæ¡†æ¶ä»¥è§£å†³å®¤å†…ç›®æ ‡å¯¼èˆªä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">semantic map</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09423v2" data-paper-url="./papers/250809423v2-distilling-llm-prior-to-flow-model-for-generalizable-agents-imaginat.html" onclick="toggleFavorite(this, '2508.09423v2', 'Distilling LLM Prior to Flow Model for Generalizable Agent&#39;s Imagination in Object Goal Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250809626v2-semantic-aware-dropsplat-adaptive-pruning-of-redundant-gaussians-for.html">Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</a></td>
  <td>æå‡ºSAD-Splatä»¥è§£å†³3Dèˆªç©ºå›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09626v2" data-paper-url="./papers/250809626v2-semantic-aware-dropsplat-adaptive-pruning-of-redundant-gaussians-for.html" onclick="toggleFavorite(this, '2508.09626v2', 'Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250809973v1-persona-personalized-whole-body-3d-avatar-with-pose-driven-deformati.html">PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image</a></td>
  <td>æå‡ºPERSONAæ¡†æ¶ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆä¸ªæ€§åŒ–3Däººç±»å¤´åƒ</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09973v1" data-paper-url="./papers/250809973v1-persona-personalized-whole-body-3d-avatar-with-pose-driven-deformati.html" onclick="toggleFavorite(this, '2508.09973v1', 'PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250809830v1-rayletdf-raylet-distance-fields-for-generalizable-3d-surface-reconst.html">RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</a></td>
  <td>æå‡ºRayletDFä»¥è§£å†³3Dè¡¨é¢é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09830v1" data-paper-url="./papers/250809830v1-rayletdf-raylet-distance-fields-for-generalizable-3d-surface-reconst.html" onclick="toggleFavorite(this, '2508.09830v1', 'RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250809912v2-e-4dgs-high-fidelity-dynamic-reconstruction-from-the-multi-view-even.html">E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras</a></td>
  <td>æå‡ºE-4DGSä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„å…‰ç…§ä¸æ¨¡ç³Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09912v2" data-paper-url="./papers/250809912v2-e-4dgs-high-fidelity-dynamic-reconstruction-from-the-multi-view-even.html" onclick="toggleFavorite(this, '2508.09912v2', 'E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250809597v2-svg-head-hybrid-surface-volumetric-gaussians-for-high-fidelity-head-.html">SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing</a></td>
  <td>æå‡ºSVG-Headä»¥è§£å†³é«˜ä¿çœŸå¤´éƒ¨é‡å»ºä¸å®æ—¶ç¼–è¾‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">implicit representation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09597v2" data-paper-url="./papers/250809597v2-svg-head-hybrid-surface-volumetric-gaussians-for-high-fidelity-head-.html" onclick="toggleFavorite(this, '2508.09597v2', 'SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250809479v1-skysplat-generalizable-3d-gaussian-splatting-from-multi-temporal-spa.html">SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images</a></td>
  <td>æå‡ºSkySplatä»¥è§£å†³å¤šæ—¶ç›¸ç¨€ç–å«æ˜Ÿå›¾åƒçš„3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09479v1" data-paper-url="./papers/250809479v1-skysplat-generalizable-3d-gaussian-splatting-from-multi-temporal-spa.html" onclick="toggleFavorite(this, '2508.09479v1', 'SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250809453v1-hyperkd-distilling-cross-spectral-knowledge-in-masked-autoencoders-v.html">HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</a></td>
  <td>æå‡ºHyperKDä»¥è§£å†³é«˜å…‰è°±é¥æ„Ÿä¸­çš„çŸ¥è¯†è’¸é¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09453v1" data-paper-url="./papers/250809453v1-hyperkd-distilling-cross-spectral-knowledge-in-masked-autoencoders-v.html" onclick="toggleFavorite(this, '2508.09453v1', 'HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250809736v4-seeing-listening-remembering-and-reasoning-a-multimodal-agent-with-l.html">Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</a></td>
  <td>æå‡ºM3-Agentä»¥è§£å†³å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„é•¿æœŸè®°å¿†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09736v4" data-paper-url="./papers/250809736v4-seeing-listening-remembering-and-reasoning-a-multimodal-agent-with-l.html" onclick="toggleFavorite(this, '2508.09736v4', 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250809560v3-weatherprompt-multi-modality-representation-learning-for-all-weather.html">WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</a></td>
  <td>æå‡ºWeatherPromptä»¥è§£å†³æ— äººæœºè§†è§‰åœ°ç†å®šä½ä¸­çš„å¤©æ°”å¹²æ‰°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09560v3" data-paper-url="./papers/250809560v3-weatherprompt-multi-modality-representation-learning-for-all-weather.html" onclick="toggleFavorite(this, '2508.09560v3', 'WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250809599v1-bridgeta-bridging-the-representation-gap-in-knowledge-distillation-v.html">BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation</a></td>
  <td>æå‡ºBridgeTAä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„è¡¨ç¤ºå·®è·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09599v1" data-paper-url="./papers/250809599v1-bridgeta-bridging-the-representation-gap-in-knowledge-distillation-v.html" onclick="toggleFavorite(this, '2508.09599v1', 'BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird&#39;s Eye View Map Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250809913v1-speechforensics-audio-visual-speech-representation-learning-for-face.html">SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</a></td>
  <td>æå‡ºéŸ³è§†é¢‘è”åˆå­¦ä¹ æ–¹æ³•ä»¥è§£å†³äººè„¸ä¼ªé€ æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09913v1" data-paper-url="./papers/250809913v1-speechforensics-audio-visual-speech-representation-learning-for-face.html" onclick="toggleFavorite(this, '2508.09913v1', 'SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/250809822v4-physical-autoregressive-model-for-robotic-manipulation-without-actio.html">Physical Autoregressive Model for Robotic Manipulation without Action Pretraining</a></td>
  <td>æå‡ºç‰©ç†è‡ªå›å½’æ¨¡å‹ä»¥è§£å†³æœºå™¨äººæ“ä½œæ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09822v4" data-paper-url="./papers/250809822v4-physical-autoregressive-model-for-robotic-manipulation-without-actio.html" onclick="toggleFavorite(this, '2508.09822v4', 'Physical Autoregressive Model for Robotic Manipulation without Action Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250809459v2-relayformer-a-unified-local-global-attention-framework-for-scalable-.html">RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization</a></td>
  <td>æå‡ºRelayFormerä»¥è§£å†³å›¾åƒå’Œè§†é¢‘ç¯¡æ”¹åŒºåŸŸå®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09459v2" data-paper-url="./papers/250809459v2-relayformer-a-unified-local-global-attention-framework-for-scalable-.html" onclick="toggleFavorite(this, '2508.09459v2', 'RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250809959v1-lia-x-interpretable-latent-portrait-animator.html">LIA-X: Interpretable Latent Portrait Animator</a></td>
  <td>æå‡ºLIA-Xä»¥è§£å†³å¯è§£é‡Šæ€§å’Œæ§åˆ¶æ€§ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09959v1" data-paper-url="./papers/250809959v1-lia-x-interpretable-latent-portrait-animator.html" onclick="toggleFavorite(this, '2508.09959v1', 'LIA-X: Interpretable Latent Portrait Animator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250809857v1-onevae-joint-discrete-and-continuous-optimization-helps-discrete-vid.html">OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better</a></td>
  <td>æå‡ºOneVAEä»¥è§£å†³ç¦»æ•£è§†é¢‘VAEè®­ç»ƒä¸ç¨³å®šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09857v1" data-paper-url="./papers/250809857v1-onevae-joint-discrete-and-continuous-optimization-helps-discrete-vid.html" onclick="toggleFavorite(this, '2508.09857v1', 'OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250809655v1-noise-adapted-neural-operator-for-robust-non-line-of-sight-imaging.html">Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging</a></td>
  <td>æå‡ºå™ªå£°é€‚åº”ç¥ç»ç®—å­ä»¥è§£å†³éè§†çº¿æˆåƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09655v1" data-paper-url="./papers/250809655v1-noise-adapted-neural-operator-for-robust-non-line-of-sight-imaging.html" onclick="toggleFavorite(this, '2508.09655v1', 'Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250809454v1-animate-x-universal-character-image-animation-with-dynamic-backgroun.html">Animate-X++: Universal Character Image Animation with Dynamic Backgrounds</a></td>
  <td>æå‡ºAnimate-X++ä»¥è§£å†³è§’è‰²åŠ¨ç”»ä¸åŠ¨æ€èƒŒæ™¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">character animation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09454v1" data-paper-url="./papers/250809454v1-animate-x-universal-character-image-animation-with-dynamic-backgroun.html" onclick="toggleFavorite(this, '2508.09454v1', 'Animate-X++: Universal Character Image Animation with Dynamic Backgrounds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/250809547v1-govig-goal-conditioned-visual-navigation-instruction-generation.html">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></td>
  <td>æå‡ºGoViGä»¥è§£å†³åŸºäºè§†è§‰çš„å¯¼èˆªæŒ‡ä»¤ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09547v1" data-paper-url="./papers/250809547v1-govig-goal-conditioned-visual-navigation-instruction-generation.html" onclick="toggleFavorite(this, '2508.09547v1', 'GoViG: Goal-Conditioned Visual Navigation Instruction Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250809629v1-enhancing-monocular-3d-hand-reconstruction-with-learned-texture-prio.html">Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors</a></td>
  <td>æå‡ºè½»é‡çº§çº¹ç†æ¨¡å—ä»¥æå‡å•ç›®3Dæ‰‹é‡å»ºç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">hand reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09629v1" data-paper-url="./papers/250809629v1-enhancing-monocular-3d-hand-reconstruction-with-learned-texture-prio.html" onclick="toggleFavorite(this, '2508.09629v1', 'Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250809486v1-episodic-memory-representation-for-long-form-video-understanding.html">Episodic Memory Representation for Long-form Video Understanding</a></td>
  <td>æå‡ºVideo-EMä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09486v1" data-paper-url="./papers/250809486v1-episodic-memory-representation-for-long-form-video-understanding.html" onclick="toggleFavorite(this, '2508.09486v1', 'Episodic Memory Representation for Long-form Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)