---
layout: default
title: Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory
---

# Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09736" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09736v4</a>
  <a href="https://arxiv.org/pdf/2508.09736.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09736v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09736v4', 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-10-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/bytedance-seed/m3-agent)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3-Agentä»¥è§£å†³å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„é•¿æœŸè®°å¿†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ™ºèƒ½ä½“` `é•¿æœŸè®°å¿†` `æ¨ç†èƒ½åŠ›` `è§†è§‰å¬è§‰èåˆ` `å¼ºåŒ–å­¦ä¹ ` `é•¿è§†é¢‘é—®ç­”` `æœºå™¨äººæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨é•¿æœŸè®°å¿†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚ä»»åŠ¡ã€‚
2. M3-Agenté€šè¿‡å®æ—¶å¤„ç†è§†è§‰å’Œå¬è§‰è¾“å…¥ï¼Œæ„å»ºå¤šæ¨¡æ€çš„é•¿æœŸè®°å¿†ï¼Œæ”¯æŒè‡ªä¸»æ¨ç†å’Œä»»åŠ¡å®Œæˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3-Agentåœ¨M3-BenchåŸºå‡†ä¸Šåˆ†åˆ«æ¯”åŸºçº¿æé«˜äº†6.7%ã€7.7%å’Œ5.3%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†M3-Agentï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå…·å¤‡é•¿æœŸè®°å¿†åŠŸèƒ½ã€‚M3-Agentèƒ½å¤Ÿå¤„ç†å®æ—¶çš„è§†è§‰å’Œå¬è§‰è¾“å…¥ï¼Œæ„å»ºå’Œæ›´æ–°æƒ…èŠ‚è®°å¿†å’Œè¯­ä¹‰è®°å¿†ï¼Œé€æ­¥ç§¯ç´¯ä¸–ç•ŒçŸ¥è¯†ã€‚å…¶è®°å¿†ä»¥å®ä½“ä¸ºä¸­å¿ƒï¼Œä»¥å¤šæ¨¡æ€æ–¹å¼ç»„ç»‡ï¼Œä»è€Œå®ç°å¯¹ç¯å¢ƒçš„æ›´æ·±åˆ»å’Œä¸€è‡´çš„ç†è§£ã€‚åœ¨æ¥æ”¶åˆ°æŒ‡ä»¤åï¼ŒM3-Agentèƒ½å¤Ÿè‡ªä¸»è¿›è¡Œå¤šè½®æ¨ç†ï¼Œå¹¶æ£€ç´¢ç›¸å…³è®°å¿†ä»¥å®Œæˆä»»åŠ¡ã€‚ä¸ºè¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„è®°å¿†æœ‰æ•ˆæ€§å’ŒåŸºäºè®°å¿†çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†M3-Benchï¼Œä¸€ä¸ªåŒ…å«100ä¸ªæ–°å½•åˆ¶çš„æœºå™¨äººè§†è§’è§†é¢‘å’Œ920ä¸ªå¤šæ ·åŒ–ç½‘ç»œæ¥æºè§†é¢‘çš„é•¿è§†é¢‘é—®ç­”åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„M3-Agentåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†æœ€å¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨é•¿æœŸè®°å¿†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•´åˆå¤šç§è¾“å…¥å¹¶è¿›è¡Œå¤æ‚æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM3-Agenté€šè¿‡æ„å»ºå®ä½“ä¸­å¿ƒçš„å¤šæ¨¡æ€è®°å¿†ï¼Œæ¨¡æ‹Ÿäººç±»çš„è®°å¿†æœºåˆ¶ï¼Œèƒ½å¤Ÿå®æ—¶æ›´æ–°å’Œæ£€ç´¢ä¿¡æ¯ï¼Œä»è€Œæå‡æ™ºèƒ½ä½“çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3-Agentçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†æ¨¡å—ã€è®°å¿†ç®¡ç†æ¨¡å—å’Œæ¨ç†æ¨¡å—ã€‚è¾“å…¥å¤„ç†æ¨¡å—è´Ÿè´£æ¥æ”¶è§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œè®°å¿†ç®¡ç†æ¨¡å—ç”¨äºå­˜å‚¨å’Œæ›´æ–°è®°å¿†ï¼Œè€Œæ¨ç†æ¨¡å—åˆ™æ‰§è¡Œä»»åŠ¡ç›¸å…³çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šM3-Agentçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€è®°å¿†çš„ç»„ç»‡æ–¹å¼å’Œè‡ªä¸»æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡æ€æˆ–é™æ€è®°å¿†æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥å¹³è¡¡è®°å¿†æ›´æ–°å’Œä»»åŠ¡æ‰§è¡Œçš„æ•ˆç‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒM3-Agentåœ¨M3-Bench-robotã€M3-Bench-webå’ŒVideoMME-longåŸºå‡†ä¸Šåˆ†åˆ«æ¯”æœ€å¼ºåŸºçº¿æé«˜äº†6.7%ã€7.7%å’Œ5.3%çš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€è®°å¿†å’Œæ¨ç†æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

M3-Agentçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœºå™¨äººåŠ©æ‰‹å’Œæ•™è‚²é¢†åŸŸç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒM3-Agentèƒ½å¤Ÿæä¾›æ›´è‡ªç„¶çš„äººæœºäº¤äº’ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½ä½“å‘æ›´é«˜å±‚æ¬¡çš„æ™ºèƒ½å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update episodic and semantic memories, gradually accumulating world knowledge. Its memory is organized in an entity-centric, multimodal manner, enabling deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn reasoning and retrieves relevant memories to complete tasks. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a long-video question answering benchmark comprising 100 newly recorded robot-perspective videos (M3-Bench-robot) and 920 diverse web-sourced videos (M3-Bench-web). We annotate QA pairs designed to test capabilities essential for agent applications, such as person understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances multimodal agents toward more human-like long-term memory and provides insights for their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent.

