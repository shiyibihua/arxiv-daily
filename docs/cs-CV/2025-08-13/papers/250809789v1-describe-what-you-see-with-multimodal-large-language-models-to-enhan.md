---
layout: default
title: Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations
---

# Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09789" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09789v1</a>
  <a href="https://arxiv.org/pdf/2508.09789.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09789v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09789v1', 'Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marco De Nadai, Andreas Damianou, Mounia Lalmas

**åˆ†ç±»**: cs.IR, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**DOI**: [10.1145/3705328.3759303](https://doi.org/10.1145/3705328.3759303)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥å¢å¼ºè§†é¢‘æ¨èç³»ç»Ÿçš„è¯­ä¹‰ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¨è` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¸ªæ€§åŒ–æ¨è` `æ·±å±‚è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ¨èç³»ç»Ÿä¾èµ–ä½çº§ç‰¹å¾ï¼Œç¼ºä¹å¯¹å†…å®¹æ·±å±‚è¯­ä¹‰çš„ç†è§£ï¼Œå¯¼è‡´ä¸ªæ€§åŒ–æ¨èæ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œå¢å¼ºæ¨èç³»ç»Ÿå¯¹è§†é¢‘å†…å®¹çš„ç†è§£ã€‚
3. åœ¨MicroLens-100Kæ•°æ®é›†ä¸Šï¼Œæ‰€ææ¡†æ¶åœ¨äº”ä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸­å‡ä¼˜äºä¼ ç»Ÿç‰¹å¾ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†é¢‘æ¨èç³»ç»Ÿä¸»è¦ä¾èµ–ç”¨æˆ·å®šä¹‰çš„å…ƒæ•°æ®æˆ–ç”±ä¸“é—¨ç¼–ç å™¨æå–çš„ä½çº§è§†è§‰å’Œå£°å­¦ä¿¡å·ã€‚è¿™äº›ä½çº§ç‰¹å¾è™½ç„¶æè¿°äº†å±å¹•ä¸Šå‡ºç°çš„å†…å®¹ï¼Œä½†ç¼ºä¹æ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¦‚æ„å›¾ã€å¹½é»˜æ„Ÿå’Œä¸–ç•ŒçŸ¥è¯†ï¼Œè¿™äº›å› ç´ ä½¿å¾—è§†é¢‘ç‰‡æ®µèƒ½å¤Ÿå¼•èµ·è§‚ä¼—çš„å…±é¸£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„ã€ä¸æ¨èç³»ç»Ÿæ— å…³çš„é›¶å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡æç¤ºç°æˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸ºæ¯ä¸ªè§†é¢‘ç‰‡æ®µç”Ÿæˆä¸°å¯Œçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä»è€Œå°†é«˜å±‚è¯­ä¹‰æ³¨å…¥æ¨èæµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨MicroLens-100Kæ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„è§†é¢‘ã€éŸ³é¢‘å’Œå…ƒæ•°æ®ç‰¹å¾ï¼Œå±•ç¤ºäº†MLLMä½œä¸ºå³æ—¶çŸ¥è¯†æå–å™¨çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘æ¨èç³»ç»Ÿä¸»è¦ä¾èµ–ä½çº§è§†è§‰å’Œå£°å­¦ä¿¡å·ï¼Œæ— æ³•æ•æ‰è§†é¢‘å†…å®¹çš„æ·±å±‚è¯­ä¹‰ï¼Œå¦‚æ„å›¾å’Œå¹½é»˜æ„Ÿï¼Œå¯¼è‡´æ¨èæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆè§†é¢‘ç‰‡æ®µçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä»è€Œå°†é«˜å±‚è¯­ä¹‰ä¿¡æ¯æ³¨å…¥æ¨èæµç¨‹ï¼Œæå‡ä¸ªæ€§åŒ–æ¨èçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨MLLMå¯¹è§†é¢‘ç‰‡æ®µè¿›è¡Œæè¿°ç”Ÿæˆï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨å…ˆè¿›çš„æ–‡æœ¬ç¼–ç å™¨å¤„ç†MLLMè¾“å‡ºï¼›æœ€åï¼Œå°†å¤„ç†åçš„ä¿¡æ¯è¾“å…¥åˆ°æ ‡å‡†çš„ååŒè¿‡æ»¤ã€åŸºäºå†…å®¹å’Œç”Ÿæˆæ¨èæ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥MLLMä½œä¸ºå³æ—¶çŸ¥è¯†æå–å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç›´æ¥ä¸ºæ¨èç³»ç»Ÿæä¾›ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•æˆªç„¶ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMLLMçš„è¾“å‡ºè¢«ç”¨ä½œæ–‡æœ¬ç‰¹å¾ï¼Œç»“åˆäº†å…ˆè¿›çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’å’Œå¤„ç†ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨MicroLens-100Kæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè§†é¢‘ã€éŸ³é¢‘å’Œå…ƒæ•°æ®ç‰¹å¾ï¼Œæå‡å¹…åº¦åœ¨å¤šä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸­å‡æ˜¾è‘—ï¼Œå±•ç¤ºäº†MLLMåœ¨è§†é¢‘æ¨èä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å¹³å°ã€è§†é¢‘æµæœåŠ¡å’Œåœ¨çº¿æ•™è‚²ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œå†…å®¹æ¨èçš„ç›¸å…³æ€§ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾å’Œè§†é¢‘å†…å®¹ï¼Œæœªæ¥çš„æ¨èç³»ç»Ÿå°†æ›´åŠ æ™ºèƒ½åŒ–å’Œä¸ªæ€§åŒ–ï¼Œæ¨åŠ¨å†…å®¹æ¶ˆè´¹çš„å˜é©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing video recommender systems rely primarily on user-defined metadata or on low-level visual and acoustic signals extracted by specialised encoders. These low-level features describe what appears on the screen but miss deeper semantics such as intent, humour, and world knowledge that make clips resonate with viewers. For example, is a 30-second clip simply a singer on a rooftop, or an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such distinctions are critical to personalised recommendations yet remain invisible to traditional encoding pipelines. In this paper, we introduce a simple, recommendation system-agnostic zero-finetuning framework that injects high-level semantics into the recommendation pipeline by prompting an off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip into a rich natural-language description (e.g. "a superhero parody with slapstick fights and orchestral stabs"), bridging the gap between raw content and user intent. We use MLLM output with a state-of-the-art text encoder and feed it into standard collaborative, content-based, and generative recommenders. On the MicroLens-100K dataset, which emulates user interactions with TikTok-style videos, our framework consistently surpasses conventional video, audio, and metadata features in five representative models. Our findings highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders.

