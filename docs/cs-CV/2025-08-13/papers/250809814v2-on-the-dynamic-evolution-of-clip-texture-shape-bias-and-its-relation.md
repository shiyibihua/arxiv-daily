---
layout: default
title: On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness
---

# On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09814" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09814v2</a>
  <a href="https://arxiv.org/pdf/2508.09814.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09814v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09814v2', 'On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pablo HernÃ¡ndez-CÃ¡mara, Jose Manuel JaÃ©n-Lorites, Alexandra GÃ³mez-Villa, Jorge Vila-TomÃ¡s, Valero Laparra, Jesus Malo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-12-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æCLIPæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„çº¹ç†-å½¢çŠ¶åå·®åŠå…¶ä¸äººç±»æ„ŸçŸ¥çš„å…³ç³»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€æ¨¡å‹` `è§†è§‰è¡¨å¾` `äººç±»æ„ŸçŸ¥` `æ¨¡å‹é²æ£’æ€§` `çº¹ç†åå·®` `å½¢çŠ¶è¡¨å¾` `è®­ç»ƒåŠ¨æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å®Œå…¨è®­ç»ƒåçš„æ¨¡å‹åˆ†æï¼Œç¼ºä¹å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨å¾åå·®å’Œæ„ŸçŸ¥å¯¹é½åŠ¨æ€çš„æ¢è®¨ã€‚
2. æœ¬æ–‡é€šè¿‡é€è½®åˆ†æCLIPæ¨¡å‹ï¼Œæ­ç¤ºäº†çº¹ç†-å½¢çŠ¶åå·®çš„æ¼”å˜åŠå…¶ä¸äººç±»æ„ŸçŸ¥çš„å…³ç³»ï¼Œæä¾›äº†æ–°çš„è§†è§’ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ—©æœŸé˜¶æ®µçš„ä½çº§æ„ŸçŸ¥å¯¹é½ä¸åæœŸçš„é²æ£’æ€§ä¹‹é—´å­˜åœ¨ç³»ç»Ÿæ€§çš„æƒè¡¡ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºä¸å®è·µæ„ä¹‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”è¯­è¨€-å›¾åƒæ¨¡å‹å¦‚CLIPå±•ç°äº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å…¶å†…éƒ¨è§†è§‰è¡¨å¾åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜åŠä¸äººç±»æ„ŸçŸ¥çš„å…³ç³»ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡é€šè¿‡é€è½®åˆ†æCLIPæ¨¡å‹ï¼Œé‡ç‚¹ç ”ç©¶çº¹ç†-å½¢çŠ¶åå·®ã€ä¸äººç±»æ„ŸçŸ¥åˆ¤æ–­çš„å¯¹é½åŠå¯¹å›¾åƒå™ªå£°çš„æ•æ„Ÿæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ—©æœŸè®­ç»ƒé˜¶æ®µè¡¨ç°å‡ºå¼ºçƒˆçš„çº¹ç†åå·®å’Œå¯¹ä½çº§äººç±»æ„ŸçŸ¥çš„é«˜åº¦å¯¹é½ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œçº¹ç†åå·®é€æ¸å‡å¼±ï¼Œå½¢çŠ¶è¡¨å¾å¢å¼ºï¼ŒåŒæ—¶å¯¹å™ªå£°çš„é²æ£’æ€§æé«˜ã€‚è¿™äº›åŠ¨æ€å˜åŒ–åœ¨ä¸åŒè§„æ¨¡çš„CLIPæ¨¡å‹ä¸­ä¸€è‡´å­˜åœ¨ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒä¸­æ„ŸçŸ¥å¯¹é½ã€ç‰¹å¾åå·®ä¸é²æ£’æ€§çš„å…±åŒæ¼”åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯¹æ¯”è¯­è¨€-å›¾åƒæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†…éƒ¨è¡¨å¾æ¼”å˜çš„ç†è§£ä¸è¶³ï¼Œå°¤å…¶æ˜¯çº¹ç†-å½¢çŠ¶åå·®ä¸äººç±»æ„ŸçŸ¥å¯¹é½çš„åŠ¨æ€å…³ç³»ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºæ¨¡å‹è®­ç»ƒå®Œæˆåçš„é™æ€åˆ†æï¼Œç¼ºä¹å¯¹è®­ç»ƒåŠ¨æ€çš„æ·±å…¥æ¢è®¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é€è½®åˆ†æCLIPæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼Œç ”ç©¶å…¶çº¹ç†åå·®å¦‚ä½•éšè®­ç»ƒé˜¶æ®µå˜åŒ–ï¼Œä»¥åŠè¿™ç§å˜åŒ–å¦‚ä½•å½±å“æ¨¡å‹å¯¹äººç±»æ„ŸçŸ¥çš„å¯¹é½å’Œå¯¹å™ªå£°çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å¤šç§æ„ŸçŸ¥åŸºå‡†è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ä½çº§å›¾åƒè´¨é‡è¯„ä¼°ã€ä¸­çº§æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€æ˜¾è‘—æ€§å¯¹åº”å’Œå™ªå£°é²æ£’æ€§ã€‚æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„è¡¨ç°è¢«ç³»ç»Ÿè®°å½•å’Œåˆ†æï¼Œä»¥è¯†åˆ«è¡¨å¾è½¬å˜çš„è§„å¾‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæä¾›äº†å¯¹CLIPæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨å¾åŠ¨æ€çš„å®è¯åˆ†æï¼Œæ­ç¤ºäº†æ—©æœŸä½çº§æ„ŸçŸ¥å¯¹é½ä¸åæœŸé²æ£’æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™ä¸€ç°è±¡åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­æ™®éå­˜åœ¨ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†å¤šç§æ„ŸçŸ¥åŸºå‡†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿äº†å¯¹æ¨¡å‹è¡¨ç°çš„å…¨é¢åˆ†æã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šæŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLIPæ¨¡å‹åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µè¡¨ç°å‡ºå¼ºçƒˆçš„çº¹ç†åå·®å’Œå¯¹ä½çº§æ„ŸçŸ¥çš„é«˜åº¦å¯¹é½ï¼Œéšç€è®­ç»ƒè¿›å±•ï¼Œçº¹ç†åå·®æ˜¾è‘—å‡å¼±ï¼Œå½¢çŠ¶è¡¨å¾å¢å¼ºï¼Œå™ªå£°é²æ£’æ€§æé«˜ã€‚è¿™ä¸€åŠ¨æ€å˜åŒ–åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­å‡æœ‰ä½“ç°ï¼Œè¡¨æ˜è¯¥ç°è±¡å…·æœ‰æ™®éæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†åŠå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£æ¨¡å‹çš„è¡¨å¾åŠ¨æ€ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹è®¾è®¡ï¼Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§å’Œå¯¹äººç±»æ„ŸçŸ¥çš„å¯¹é½ç¨‹åº¦ï¼Œè¿›è€Œæå‡å¤šæ¨¡æ€ç³»ç»Ÿçš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Contrastive language-image models such as CLIP have demonstrated remarkable generalization capabilities. However, how their internal visual representations evolve during training and how this evolution relates to human perception remains poorly understood. Most existing analysis characterize fully trained models, leaving the dynamics of representational biases and perceptual alignment largely unexplored. In this work, we present an epoch-by-epoch analysis of CLIP models throughout training, focusing on the evolution of texture-shape bias, alignment with human perceptual judgements, and sensitivity to image noise. Using multiple perceptual benchmarks spanning low-level image quality assessment, mid-level perceptual similarity, saliency correspondence, and noisy robustness, we identify a consistent, training-stage-dependent representational transition. Early training stages exhibit strong texture bias, elevated alignment with low-level human perceptual measures, and increased sensitivity to Gaussian noise perturbations. As training progresses, this texture bias gradually diminishes in favor of more shape-based representations, coinciding with improved robustness to noise and a decline in low-level perceptual alignment. Importantly, these dynamics are consistently observed across multiple CLIP model scales, indicating that the phenomenon is not specific to a particular architecture size. Our findings provide an empirical characterization of how perceptual alignment, feature bias, and robustness co-evolve during multimodal model training. This work reveals a systematic trade-off between early low-level perceptual alignment and later robustness, offering new insights into the representational dynamics of vision-language models and their relationship to human visual processing.

