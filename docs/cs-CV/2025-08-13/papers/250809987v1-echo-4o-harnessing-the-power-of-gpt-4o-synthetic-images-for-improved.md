---
layout: default
title: Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation
---

# Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09987" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09987v1</a>
  <a href="https://arxiv.org/pdf/2508.09987.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09987v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09987v1', 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: 19 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEcho-4oä»¥è§£å†³å›¾åƒç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆæˆå›¾åƒ` `å›¾åƒç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®é›†` `è¯„ä¼°åŸºå‡†` `GPT-4o` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†ç¨€æœ‰åœºæ™¯æ—¶è¡¨ç°ä¸è¶³ï¼Œä¸”çœŸå®æ•°æ®é›†å¸¸å¸¸åŒ…å«å¤æ‚çš„èƒŒæ™¯å™ªå£°ã€‚
2. æœ¬æ–‡æå‡ºäº†Echo-4o-Imageåˆæˆæ•°æ®é›†ï¼Œåˆ©ç”¨GPT-4oç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä»¥å¼¥è¡¥çœŸå®æ•°æ®é›†çš„ç›²ç‚¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEcho-4oåœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå¯¹å…¶ä»–åŸºç¡€æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ€è¿‘ï¼ŒGPT-4oå› å…¶åœ¨å›¾åƒç”Ÿæˆä¸­çš„å¼ºå¤§è¡¨ç°è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å¼€æºæ¨¡å‹ä»ç„¶æ»åã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨GPT-4oç”Ÿæˆçš„åˆæˆå›¾åƒçš„ä¼˜åŠ¿ï¼ŒæŒ‡å‡ºå…¶èƒ½å¤Ÿè¡¥å……çœŸå®æ•°æ®é›†ä¸­ç¨€æœ‰åœºæ™¯ï¼Œå¹¶æä¾›å¹²å‡€å¯æ§çš„ç›‘ç£ä¿¡å·ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†Echo-4o-Imageï¼Œä¸€ä¸ªç”±GPT-4oç”Ÿæˆçš„180Kè§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¾®è°ƒç»Ÿä¸€çš„å¤šæ¨¡æ€ç”ŸæˆåŸºçº¿Bagelï¼Œè·å¾—äº†Echo-4oã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸¤ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†GenEval++å’ŒImagine-Benchï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEcho-4oåœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå¯¹å…¶ä»–åŸºç¡€æ¨¡å‹ï¼ˆå¦‚OmniGen2ã€BLIP3-oï¼‰å®ç°äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆä¸­çœŸå®æ•°æ®é›†ç¨€ç¼ºå’Œå™ªå£°å¹²æ‰°çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¨€æœ‰åœºæ™¯æ—¶æ•ˆæœä¸ä½³ï¼Œä¸”çœŸå®æ•°æ®å¸¸å­˜åœ¨æ–‡æœ¬ä¸å›¾åƒå†…å®¹çš„ä¸å¯¹é½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥è¡¥å……çœŸå®æ•°æ®é›†ä¸­çš„ç¨€æœ‰åœºæ™¯ï¼Œå¹¶æä¾›æ›´å¹²å‡€çš„ç›‘ç£ä¿¡å·ï¼Œä»¥æé«˜æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åˆæˆæ•°æ®é›†çš„ç”Ÿæˆã€åŸºäºè¯¥æ•°æ®é›†çš„æ¨¡å‹å¾®è°ƒï¼Œä»¥åŠæ–°çš„è¯„ä¼°åŸºå‡†çš„è®¾è®¡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ã€æ¨¡å‹è®­ç»ƒæ¨¡å—å’Œè¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†Echo-4o-Imageåˆæˆæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†GenEval++å’ŒImagine-Benchä¸¤ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆçš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥ç¡®ä¿åˆæˆå›¾åƒçš„è´¨é‡å’Œç”Ÿæˆçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†è®­ç»ƒå‚æ•°ä»¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEcho-4oåœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨GenEval++å’ŒImagine-Benchä¸Šï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œåº”ç”¨Echo-4o-Imageäºå…¶ä»–åŸºç¡€æ¨¡å‹ï¼ˆå¦‚OmniGen2ã€BLIP3-oï¼‰æ—¶ï¼Œå‡å®ç°äº†æ€§èƒ½çš„ä¸€è‡´æå‡ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„å¼ºè½¬ç§»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€æ¸¸æˆè®¾è®¡ã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜è´¨é‡çš„åˆæˆå›¾åƒï¼Œå¸®åŠ©è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æœªæ¥ï¼Œéšç€åˆæˆå›¾åƒæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.

