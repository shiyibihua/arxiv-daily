---
layout: default
title: MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning
---

# MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10133" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.10133v2</a>
  <a href="https://arxiv.org/pdf/2508.10133.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10133v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10133v2', 'MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Thanh-Dat Truong, Christophe Bobda, Nitin Agarwal, Khoa Luu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-13 (Êõ¥Êñ∞: 2025-11-25)

**Â§áÊ≥®**: Accepted to NeurIPS'25

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MANGOÊñπÊ≥ï‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅËûçÂêàÂ≠¶‰π†ÁöÑÁâπÂæÅÊçïÊçâÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÂΩí‰∏ÄÂåñÊµÅ` `ÁâπÂæÅÊçïÊçâ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅËûçÂêàÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÊçïÊçâÂêÑÊ®°ÊÄÅÁöÑÊ†∏ÂøÉÁâπÂæÅÔºåÂØºËá¥ÂØπÂ§çÊùÇÁªìÊûÑÂíåÊ®°ÊÄÅÈó¥ÂÖ≥ËÅîÁöÑÁêÜËß£Âõ∞Èöæ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÄÅÊ≥®ÊÑèÂäõÂΩí‰∏ÄÂåñÊµÅÔºàMANGOÔºâÊñπÊ≥ïÔºåÂà©Áî®ÂèØÈÄÜ‰∫§ÂèâÊ≥®ÊÑèÂäõÂ±ÇÂíåÊñ∞Âûã‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•Â¢ûÂº∫ÁâπÂæÅÊçïÊçâËÉΩÂäõ„ÄÇ
3. Âú®ËØ≠‰πâÂàÜÂâ≤„ÄÅÂõæÂÉèÂà∞ÂõæÂÉèËΩ¨Êç¢ÂíåÁîµÂΩ±Á±ªÂûãÂàÜÁ±ªÁ≠â‰ªªÂä°‰∏≠ÔºåMANGOÂ±ïÁ§∫‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëóÊèêÂçá‰∫ÜËûçÂêàÂ≠¶‰π†ÁöÑÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ≠¶‰π†ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÁé∞ÊúâÁöÑËûçÂêàÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éTransformerÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâÂêÑÊ®°ÊÄÅÁöÑÊ†∏ÂøÉÁâπÂæÅÔºåÂØºËá¥ÂØπÂ§çÊùÇÁªìÊûÑÂíåÊ®°ÊÄÅÈó¥ÂÖ≥ËÅîÁöÑÁêÜËß£Âõ∞Èöæ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÊ®°ÊÄÅÊ≥®ÊÑèÂäõÂΩí‰∏ÄÂåñÊµÅÔºàMANGOÔºâÊñπÊ≥ïÔºåÊó®Âú®ÂÆûÁé∞ÊòéÁ°Æ„ÄÅÂèØËß£ÈáäÂíåÂèØÂ§ÑÁêÜÁöÑÂ§öÊ®°ÊÄÅËûçÂêàÂ≠¶‰π†„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂèØÈÄÜ‰∫§ÂèâÊ≥®ÊÑèÂäõÔºàICAÔºâÂ±ÇÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏âÁßçÊñ∞ÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•È´òÊïàÊçïÊçâÂ§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏≠ÁöÑÂ§çÊùÇÂÖ≥ËÅî„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMANGOÂú®ËØ≠‰πâÂàÜÂâ≤„ÄÅÂõæÂÉèÂà∞ÂõæÂÉèËΩ¨Êç¢ÂíåÁîµÂΩ±Á±ªÂûãÂàÜÁ±ªÁ≠â‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅËûçÂêàÂ≠¶‰π†ÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÊçïÊçâÂêÑÊ®°ÊÄÅÊ†∏ÂøÉÁâπÂæÅÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÂØπÂ§çÊùÇÁªìÊûÑÂíåÊ®°ÊÄÅÈó¥ÂÖ≥ËÅîÁöÑÁêÜËß£‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫Â§öÊ®°ÊÄÅÊ≥®ÊÑèÂäõÂΩí‰∏ÄÂåñÊµÅÔºàMANGOÔºâÊñπÊ≥ïÔºåÈÄöËøáÂºïÂÖ•ÂèØÈÄÜ‰∫§ÂèâÊ≥®ÊÑèÂäõÂ±ÇÂíå‰∏âÁßçÊñ∞Âûã‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊòæËëóÊèêÂçáÂ§öÊ®°ÊÄÅÊï∞ÊçÆÁöÑÁâπÂæÅÊçïÊçâËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMANGOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂèØÈÄÜ‰∫§ÂèâÊ≥®ÊÑèÂäõÂ±Ç„ÄÅÊ®°ÊÄÅÈó¥‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºàMMCA„ÄÅIMCA„ÄÅLICAÔºâÂíåÂΩí‰∏ÄÂåñÊµÅÊ®°ÂûãÔºåÊó®Âú®ÂÆûÁé∞È´òÊïàÁöÑÂ§öÊ®°ÊÄÅËûçÂêà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫ÜÂèØÈÄÜ‰∫§ÂèâÊ≥®ÊÑèÂäõÂ±ÇÂíå‰∏âÁßçÊñ∞Âûã‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊòéÁ°ÆÊçïÊçâÊ®°ÊÄÅÈó¥ÁöÑÂ§çÊùÇÂÖ≥ËÅîÔºåÂå∫Âà´‰∫é‰º†ÁªüÁöÑÈöêÂºèÂ≠¶‰π†ÊñπÊ≥ï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÂÖ≥Ê≥®‰∫Ü‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ÁöÑÈÄâÊã©Ôºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®È´òÁª¥Â§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏äÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMANGOÂú®ËØ≠‰πâÂàÜÂâ≤„ÄÅÂõæÂÉèÂà∞ÂõæÂÉèËΩ¨Êç¢ÂíåÁîµÂΩ±Á±ªÂûãÂàÜÁ±ª‰ªªÂä°‰∏äÂùáËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ÊòæËëóÔºåÂÖ∑‰ΩìÊèêÂçáÊï∞ÊçÆÊú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËÆ°ÁÆóÊú∫ËßÜËßâ„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÂ§öÊ®°ÊÄÅÊï∞ÊçÆÂàÜÊûêÁ≠â„ÄÇMANGOÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÊèêÂçáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÊÄßËÉΩÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéßÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.

