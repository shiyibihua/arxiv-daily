---
layout: default
title: NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation
---

# NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09715" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09715v1</a>
  <a href="https://arxiv.org/pdf/2508.09715.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09715v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09715v1', 'NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Devvrat Joshi, Islem Rekik

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/basiralab/NEURAL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNEURALä»¥è§£å†³èµ„æºå—é™ä¸´åºŠç¯å¢ƒä¸­çš„å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŒ»å­¦å½±åƒ` `æ•°æ®å‹ç¼©` `ä¸´åºŠè¯„ä¼°` `äº¤å‰æ³¨æ„åŠ›` `çŸ¥è¯†å›¾` `æ·±åº¦å­¦ä¹ ` `æ”¾å°„å­¦` `å›¾å½¢è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®çš„å¿«é€Ÿå¢é•¿å¯¼è‡´å­˜å‚¨å’Œä¼ è¾“çš„æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚
2. NEURALæ¡†æ¶é€šè¿‡è¯­ä¹‰å¼•å¯¼çš„æ•°æ®å‹ç¼©ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›åˆ†æ•°å¯¹å½±åƒè¿›è¡Œç»“æ„æ€§ä¿®å‰ªï¼Œä¿ç•™å…³é”®è¯Šæ–­åŒºåŸŸã€‚
3. åœ¨MIMIC-CXRå’ŒCheXpert Plusæ•°æ®é›†ä¸Šï¼ŒNEURALå®ç°äº†93.4%-97.7%çš„æ•°æ®å¤§å°å‡å°‘ï¼ŒåŒæ—¶ä¿æŒé«˜è¾¾0.95çš„AUCæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®çš„å¿«é€Ÿå¢é•¿ï¼Œå­˜å‚¨å’Œä¼ è¾“é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚æœ¬æ–‡æå‡ºäº†NEURALï¼Œä¸€ä¸ªé€šè¿‡è¯­ä¹‰å¼•å¯¼çš„æ•°æ®å‹ç¼©æ¡†æ¶ï¼Œåˆ©ç”¨ç»è¿‡å¾®è°ƒçš„ç”Ÿæˆè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¯¹èƒ¸éƒ¨Xå…‰å½±åƒè¿›è¡Œç»“æ„æ€§ä¿®å‰ªï¼Œä»…ä¿ç•™è¯Šæ–­å…³é”®åŒºåŸŸã€‚è¯¥æ–¹æ³•å°†å½±åƒè½¬åŒ–ä¸ºé«˜åº¦å‹ç¼©çš„å›¾å½¢è¡¨ç¤ºï¼Œå¹¶å°†ä¿®å‰ªåçš„è§†è§‰å›¾ä¸æ¥è‡ªä¸´åºŠæŠ¥å‘Šçš„çŸ¥è¯†å›¾èåˆï¼Œåˆ›å»ºä¸€ä¸ªé€šç”¨æ•°æ®ç»“æ„ï¼Œç®€åŒ–åç»­å»ºæ¨¡ã€‚NEURALåœ¨MIMIC-CXRå’ŒCheXpert Plusæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå›¾åƒæ•°æ®å¤§å°å‡å°‘93.4%-97.7%ï¼ŒåŒæ—¶ä¿æŒ0.88-0.95çš„é«˜è¯Šæ–­æ€§èƒ½ï¼Œè¶…è¶Šäº†ä½¿ç”¨æœªå‹ç¼©æ•°æ®çš„åŸºçº¿æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œå¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®å­˜å‚¨å’Œä¼ è¾“çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå‹ç¼©æ•°æ®ï¼ŒåŒæ—¶ä¿æŒè¯Šæ–­æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNEURALé€šè¿‡åˆ©ç”¨ç»è¿‡å¾®è°ƒçš„ç”Ÿæˆè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿›è¡Œå½±åƒçš„ç»“æ„æ€§ä¿®å‰ªï¼Œä¿ç•™è¯Šæ–­å…³é”®åŒºåŸŸï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ•°æ®å‹ç¼©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNEURALçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å½±åƒçš„ç»“æ„æ€§ä¿®å‰ªï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›åˆ†æ•°è¯†åˆ«å…³é”®åŒºåŸŸï¼›å…¶æ¬¡æ˜¯å°†ä¿®å‰ªåçš„è§†è§‰å›¾ä¸çŸ¥è¯†å›¾èåˆï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„å›¾å½¢è¡¨ç¤ºï¼Œç®€åŒ–åç»­çš„å»ºæ¨¡è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šNEURALçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è¯­ä¹‰å¼•å¯¼çš„å‹ç¼©ä¸å›¾å½¢è¡¨ç¤ºç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªé€šç”¨çš„æ•°æ®ç»“æ„ï¼Œè§£å†³äº†æ•°æ®å¤§å°ä¸ä¸´åºŠå®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNEURALåœ¨ä¿æŒé«˜è¯Šæ–­æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ•°æ®å¤§å°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒNEURALé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å½±åƒçš„å‹ç¼©æ•ˆæœï¼Œå¹¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº¤å‰æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—ï¼Œç¡®ä¿ä¿ç•™å…³é”®çš„è¯Šæ–­ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NEURALåœ¨MIMIC-CXRå’ŒCheXpert Plusæ•°æ®é›†ä¸Šå®ç°äº†93.4%-97.7%çš„å½±åƒæ•°æ®å¤§å°å‡å°‘ï¼ŒåŒæ—¶ç»´æŒäº†0.88-0.95çš„AUCæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä½¿ç”¨æœªå‹ç¼©æ•°æ®çš„åŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€åŒ»å­¦å½±åƒå¤„ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NEURALçš„ç ”ç©¶æˆæœåœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è¿œç¨‹æ”¾å°„å­¦å’Œé«˜æ•ˆå·¥ä½œæµç¨‹ä¸­ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©å½±åƒæ•°æ®ï¼ŒNEURALèƒ½å¤Ÿæé«˜æ•°æ®ä¼ è¾“æ•ˆç‡ï¼Œé™ä½å­˜å‚¨æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒé«˜æ°´å¹³çš„è¯Šæ–­æ€§èƒ½ï¼Œä¿ƒè¿›åŒ»ç–—æœåŠ¡çš„å¯åŠæ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid growth of multimodal medical imaging data presents significant storage and transmission challenges, particularly in resource-constrained clinical settings. We propose NEURAL, a novel framework that addresses this by using semantics-guided data compression. Our approach repurposes cross-attention scores between the image and its radiological report from a fine-tuned generative vision-language model to structurally prune chest X-rays, preserving only diagnostically critical regions. This process transforms the image into a highly compressed, graph representation. This unified graph-based representation fuses the pruned visual graph with a knowledge graph derived from the clinical report, creating a universal data structure that simplifies downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming other baseline models that use uncompressed data. By creating a persistent, task-agnostic data asset, NEURAL resolves the trade-off between data size and clinical utility, enabling efficient workflows and teleradiology without sacrificing performance. Our NEURAL code is available at https://github.com/basiralab/NEURAL.

