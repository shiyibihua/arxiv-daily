---
layout: default
title: Learning Spatial Decay for Vision Transformers
---

# Learning Spatial Decay for Vision Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09525" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09525v1</a>
  <a href="https://arxiv.org/pdf/2508.09525.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09525v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09525v1', 'Learning Spatial Decay for Vision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxin Mao, Zhen Qin, Jinxing Zhou, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´è¡°å‡å˜æ¢å™¨ä»¥æå‡è§†è§‰å˜æ¢å™¨çš„ç©ºé—´æ³¨æ„åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å˜æ¢å™¨` `ç©ºé—´æ³¨æ„åŠ›` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `æ•°æ®ä¾èµ–` `å›¾åƒåˆ†ç±»` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å˜æ¢å™¨åœ¨å¤„ç†ç©ºé—´ç»“æ„ä»»åŠ¡æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç©ºé—´å½’çº³åç½®ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç©ºé—´è¡°å‡å˜æ¢å™¨ï¼ˆSDTï¼‰ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§æœºåˆ¶åŠ¨æ€è°ƒèŠ‚ç©ºé—´æ³¨æ„åŠ›ã€‚
3. åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼ŒSDTåœ¨åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰å¼ºåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œä½†å…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹æ˜ç¡®çš„ç©ºé—´å½’çº³åç½®ï¼Œå¯¼è‡´åœ¨ç©ºé—´ç»“æ„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•åŸºäºå›ºå®šè·ç¦»åº¦é‡å¼•å…¥æ•°æ®æ— å…³çš„ç©ºé—´è¡°å‡ï¼Œå¯¼è‡´æ³¨æ„åŠ›æƒé‡å‡åŒ€åˆ†é…ï¼Œé™åˆ¶äº†å¯¹å¤šæ ·åŒ–è§†è§‰åœºæ™¯çš„é€‚åº”æ€§ã€‚æœ¬æ–‡é¦–æ¬¡æˆåŠŸå°†æ•°æ®ä¾èµ–çš„ç©ºé—´è¡°å‡é€‚é…äºäºŒç»´è§†è§‰å˜æ¢å™¨ï¼Œæå‡ºäº†ç©ºé—´è¡°å‡å˜æ¢å™¨ï¼ˆSDTï¼‰ï¼Œå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§æœºåˆ¶ï¼ˆCAGï¼‰ï¼Œç”ŸæˆåŠ¨æ€çš„æ•°æ®ä¾èµ–è¡°å‡ä»¥è°ƒèŠ‚è¡¥ä¸é—´çš„äº¤äº’ã€‚é€šè¿‡ç»Ÿä¸€çš„ç©ºé—´-å†…å®¹èåˆæ¡†æ¶ï¼Œæ•´åˆåŸºäºæ›¼å“ˆé¡¿è·ç¦»çš„ç©ºé—´å…ˆéªŒä¸å­¦ä¹ çš„å†…å®¹è¡¨ç¤ºï¼Œè§£å†³äº†ä»ä¸€ç»´åˆ°äºŒç»´é€‚é…çš„åŸºæœ¬æŒ‘æˆ˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ImageNet-1Kåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿ï¼ŒSDTè¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰å˜æ¢å™¨åœ¨ç©ºé—´ç»“æ„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”±äºé‡‡ç”¨å›ºå®šçš„ç©ºé—´è¡°å‡ç­–ç•¥ï¼Œæ— æ³•æ ¹æ®å›¾åƒå†…å®¹åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œå¯¼è‡´é€‚åº”æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„ç©ºé—´è¡°å‡å˜æ¢å™¨ï¼ˆSDTï¼‰é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§æœºåˆ¶ï¼ˆCAGï¼‰å®ç°æ•°æ®ä¾èµ–çš„ç©ºé—´è¡°å‡ï¼Œèƒ½å¤Ÿæ ¹æ®å†…å®¹ç›¸å…³æ€§å’Œç©ºé—´æ¥è¿‘æ€§åŠ¨æ€è°ƒèŠ‚æ³¨æ„åŠ›ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨ç©ºé—´ç»“æ„ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSDTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥è¡¥ä¸çš„ç‰¹å¾æå–ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§æœºåˆ¶çš„åº”ç”¨ä»¥åŠç©ºé—´-å†…å®¹èåˆæ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ çš„å†…å®¹è¡¨ç¤ºä¸åŸºäºæ›¼å“ˆé¡¿è·ç¦»çš„ç©ºé—´å…ˆéªŒç›¸ç»“åˆï¼Œå½¢æˆç»Ÿä¸€çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å°†æ•°æ®ä¾èµ–çš„ç©ºé—´è¡°å‡æœºåˆ¶å¼•å…¥åˆ°äºŒç»´è§†è§‰å˜æ¢å™¨ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç©ºé—´æ³¨æ„åŠ›èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„é™æ€æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§æœºåˆ¶çš„ç»“æ„å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ç©ºé—´å’Œå†…å®¹çš„å…³ç³»ï¼Œä¼˜åŒ–æ³¨æ„åŠ›åˆ†é…ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œç©ºé—´è¡°å‡å˜æ¢å™¨ï¼ˆSDTï¼‰åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹æå‡äº†çº¦3.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒéªŒè¯äº†æ•°æ®ä¾èµ–ç©ºé—´è¡°å‡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒç”Ÿæˆç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚é€šè¿‡æå‡è§†è§‰å˜æ¢å™¨çš„ç©ºé—´æ³¨æ„åŠ›èƒ½åŠ›ï¼ŒSDTèƒ½å¤Ÿåœ¨å¤šç§è§†è§‰åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Transformers (ViTs) have revolutionized computer vision, yet their self-attention mechanism lacks explicit spatial inductive biases, leading to suboptimal performance on spatially-structured tasks. Existing approaches introduce data-independent spatial decay based on fixed distance metrics, applying uniform attention weighting regardless of image content and limiting adaptability to diverse visual scenarios. Inspired by recent advances in large language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX) significantly outperform static alternatives, we present the first successful adaptation of data-dependent spatial decay to 2D vision transformers. We introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent decay for patch interactions. Our approach learns to modulate spatial attention based on both content relevance and spatial proximity. We address the fundamental challenge of 1D-to-2D adaptation through a unified spatial-content fusion framework that integrates manhattan distance-based spatial priors with learned content representations. Extensive experiments on ImageNet-1K classification and generation tasks demonstrate consistent improvements over strong baselines. Our work establishes data-dependent spatial decay as a new paradigm for enhancing spatial attention in vision transformers.

