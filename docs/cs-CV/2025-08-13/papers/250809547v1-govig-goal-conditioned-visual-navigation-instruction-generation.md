---
layout: default
title: GoViG: Goal-Conditioned Visual Navigation Instruction Generation
---

# GoViG: Goal-Conditioned Visual Navigation Instruction Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09547" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09547v1</a>
  <a href="https://arxiv.org/pdf/2508.09547.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09547v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09547v1', 'GoViG: Goal-Conditioned Visual Navigation Instruction Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: Under review. Code: https://github.com/F1y1113/GoViG

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGoViGä»¥è§£å†³åŸºäºè§†è§‰çš„å¯¼èˆªæŒ‡ä»¤ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å¯¼èˆª` `æŒ‡ä»¤ç”Ÿæˆ` `è‡ªæˆ‘ä¸­å¿ƒè§†è§‰` `å¤šæ¨¡æ€å­¦ä¹ ` `æœºå™¨äººæŠ€æœ¯` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ç»“æ„åŒ–è¾“å…¥ï¼Œéš¾ä»¥é€‚åº”æœªçŸ¥å’Œéç»“æ„åŒ–ç¯å¢ƒï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. GoViGé€šè¿‡è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ•°æ®ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ï¼Œåˆ†è§£ä¸ºè§†è§‰é¢„æµ‹å’ŒæŒ‡ä»¤ç”Ÿæˆä¸¤ä¸ªå­ä»»åŠ¡ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„çµæ´»æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGoViGåœ¨BLEU-4å’ŒCIDErè¯„åˆ†ä¸Šæ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ç›®æ ‡æ¡ä»¶è§†è§‰å¯¼èˆªæŒ‡ä»¤ç”Ÿæˆï¼ˆGoViGï¼‰ï¼Œæ—¨åœ¨ä»…åŸºäºè‡ªæˆ‘ä¸­å¿ƒçš„è§†è§‰è§‚å¯Ÿè‡ªåŠ¨ç”Ÿæˆç²¾ç¡®ä¸”ä¸Šä¸‹æ–‡ä¸€è‡´çš„å¯¼èˆªæŒ‡ä»¤ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ç»“æ„åŒ–è¾“å…¥ï¼ˆå¦‚è¯­ä¹‰æ³¨é‡Šæˆ–ç¯å¢ƒåœ°å›¾ï¼‰ä¸åŒï¼ŒGoViGå®Œå…¨åˆ©ç”¨åŸå§‹çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†å¯¹æœªçŸ¥å’Œéç»“æ„åŒ–ç¯å¢ƒçš„é€‚åº”æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªç›¸äº’å…³è”çš„å­ä»»åŠ¡æ¥è§£å†³ï¼šè§†è§‰é¢„æµ‹å’ŒæŒ‡ä»¤ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGoViGåœ¨BLEU-4å’ŒCIDErè¯„åˆ†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä»è‡ªæˆ‘ä¸­å¿ƒè§†è§‰è§‚å¯Ÿä¸­ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤çš„ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–ç»“æ„åŒ–è¾“å…¥ï¼Œéš¾ä»¥å¤„ç†æœªçŸ¥ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGoViGé€šè¿‡è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ•°æ®ç”ŸæˆæŒ‡ä»¤ï¼Œé‡‡ç”¨è§†è§‰é¢„æµ‹å’ŒæŒ‡ä»¤ç”Ÿæˆçš„åˆ†è§£ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜ç³»ç»Ÿçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰é¢„æµ‹æ¨¡å—ç”¨äºé¢„æµ‹åˆå§‹è§†å›¾ä¸ç›®æ ‡è§†å›¾ä¹‹é—´çš„ä¸­é—´è§†è§‰çŠ¶æ€ï¼ŒæŒ‡ä»¤ç”Ÿæˆæ¨¡å—åˆ™åŸºäºè§‚å¯Ÿåˆ°çš„è§†è§‰ä¿¡æ¯å’Œé¢„æµ‹çš„è§†è§‰çŠ¶æ€ç”Ÿæˆè¯­è¨€æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ•°æ®çš„ä½¿ç”¨ï¼Œé¿å…äº†å¯¹ç»“æ„åŒ–è¾“å…¥çš„ä¾èµ–ï¼Œä¸”é‡‡ç”¨äº†è‡ªå›å½’çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿ç©ºé—´å‡†ç¡®æ€§å’Œè¯­è¨€æ¸…æ™°åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®­ç»ƒä¸­ä½¿ç”¨äº†å®šåˆ¶çš„ç›®æ ‡å‡½æ•°ï¼Œç»“åˆäº†ä¸€æ¬¡æ€§æ¨ç†å’Œäº¤é”™æ¨ç†çš„å¤šæ¨¡æ€æ¨ç†ç­–ç•¥ï¼Œä»¥æ¨¡æ‹Ÿäººç±»åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­çš„è®¤çŸ¥è¿‡ç¨‹ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªè¯¦ç»†æŠ«éœ²ï¼Œæ ‡è®°ä¸ºæœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGoViGåœ¨BLEU-4å’ŒCIDErè¯„åˆ†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦æœªè¯¦ç»†æŠ«éœ²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´ä¸ºçµæ´»å’Œæ™ºèƒ½çš„å¯¼èˆªè§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼ŒGoViGæœ‰æœ›åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„è‡ªä¸»å¯¼èˆªï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.

