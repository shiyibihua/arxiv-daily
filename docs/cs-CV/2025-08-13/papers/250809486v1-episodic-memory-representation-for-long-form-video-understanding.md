---
layout: default
title: Episodic Memory Representation for Long-form Video Understanding
---

# Episodic Memory Representation for Long-form Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09486" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09486v1</a>
  <a href="https://arxiv.org/pdf/2508.09486.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09486v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09486v1', 'Episodic Memory Representation for Long-form Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yun Wang, Long Zhang, Jingren Liu, Jiaqi Yan, Zhanjie Zhang, Jiahao Zheng, Xun Yang, Dapeng Wu, Xiangyu Chen, Xuelong Li

**åˆ†ç±»**: cs.CV, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: 10 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo-EMä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `æƒ…èŠ‚è®°å¿†` `è§†é¢‘é—®ç­”` `æ—¶ç©ºå…³ç³»` `é“¾å¼æ€ç»´` `å¤šæ¨¡æ€å­¦ä¹ ` `ä¿¡æ¯æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œå› ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰æ—¶ç©ºå…³ç³»ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚
2. è®ºæ–‡æå‡ºVideo-EMæ¡†æ¶ï¼Œå°†å…³é”®å¸§è§†ä¸ºæ—¶é—´åºåˆ—çš„æƒ…èŠ‚äº‹ä»¶ï¼Œå¢å¼ºäº†å¯¹ç©ºé—´å’Œæ—¶é—´åŠ¨æ€çš„å»ºæ¨¡èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVideo-EMç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å®ç°äº†4-9%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„å¸§æ•°æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨ä¸€èˆ¬è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é•¿è§†é¢‘å¤„ç†ä¸Šå—åˆ°ä¸Šä¸‹æ–‡çª—å£é™åˆ¶çš„å›°æ‰°ã€‚ä¸ºæ­¤ï¼Œè¿‘æœŸçš„æ–¹æ³•é›†ä¸­äºå…³é”®å¸§æ£€ç´¢ï¼Œå°†å†—é•¿è§†é¢‘å‹ç¼©ä¸ºä¸€å°ç»„ä¿¡æ¯ä¸°å¯Œçš„å¸§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç®€åŒ–äº†é—®é¢˜ï¼Œå¿½è§†äº†æ•æ‰åœºæ™¯è½¬å˜å’Œä¸Šä¸‹æ–‡è¿ç»­æ€§æ‰€éœ€çš„æ—¶ç©ºå…³ç³»ï¼Œå¯èƒ½å¯¼è‡´å†—ä½™å…³é”®å¸§çš„äº§ç”Ÿï¼Œé™åˆ¶äº†ä¿¡æ¯é‡ï¼Œä»è€Œå½±å“è§†é¢‘é—®ç­”çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Video-EMï¼Œä¸€ä¸ªä¸éœ€è¦è®­ç»ƒçš„æ¡†æ¶ï¼Œçµæ„Ÿæ¥è‡ªäººç±»çš„æƒ…èŠ‚è®°å¿†ï¼Œæ—¨åœ¨ä¿ƒè¿›ç¨³å¥ä¸”åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç†ã€‚Video-EMå°†å…³é”®å¸§è§†ä¸ºæ—¶é—´é¡ºåºçš„æƒ…èŠ‚äº‹ä»¶ï¼Œæ•æ‰å¿…è¦çš„ç©ºé—´å…³ç³»å’Œæ—¶é—´åŠ¨æ€ï¼Œä»è€Œå‡†ç¡®é‡å»ºæ½œåœ¨å™äº‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä¸LLMsç»“åˆï¼Œè¿­ä»£è¯†åˆ«å‡ºæœ€å°ä½†ä¿¡æ¯ä¸°å¯Œçš„æƒ…èŠ‚è®°å¿†å­é›†ï¼Œä»è€Œå®ç°é«˜æ•ˆå‡†ç¡®çš„è§†é¢‘é—®ç­”ã€‚å¯¹Video-MMEã€EgoSchemaã€HourVideoå’ŒLVBenchåŸºå‡†çš„å¹¿æ³›è¯„ä¼°è¯å®äº†Video-EMçš„ä¼˜è¶Šæ€§ï¼Œå…¶åœ¨ä½¿ç”¨æ›´å°‘å¸§çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æå‡è¾¾4-9ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å°†å…³é”®å¸§è§†ä¸ºå­¤ç«‹çš„è§†è§‰å®ä½“ï¼Œå¿½è§†äº†æ—¶ç©ºå…³ç³»ï¼Œå¯¼è‡´ä¿¡æ¯å†—ä½™å’Œé—®ç­”å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å…³é”®å¸§å»ºæ¨¡ä¸ºæ—¶é—´é¡ºåºçš„æƒ…èŠ‚äº‹ä»¶ï¼Œå¼ºè°ƒæ—¶ç©ºå…³ç³»çš„æ•æ‰ï¼Œä»¥ä¾¿æ›´å¥½åœ°é‡å»ºè§†é¢‘å™äº‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒVideo-EMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVideo-EMæ¡†æ¶åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å…³é”®å¸§çš„æå–ä¸æ’åºï¼Œå…¶æ¬¡æ˜¯æƒ…èŠ‚è®°å¿†çš„æ„å»ºï¼Œæœ€åæ˜¯åŸºäºé“¾å¼æ€ç»´çš„é—®ç­”æ¨¡å—ã€‚è¿™ä¸€æµç¨‹ç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨å’Œä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å…³é”®å¸§è§†ä¸ºæ—¶é—´åºåˆ—çš„æƒ…èŠ‚äº‹ä»¶ï¼Œè€Œéé™æ€å›¾åƒï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰æ—¶ç©ºåŠ¨æ€ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹æ—¶ç©ºå…³ç³»çš„é‡è§†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒVideo-EMé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–å…³é”®å¸§çš„é€‰æ‹©ï¼Œå¹¶è®¾è®¡äº†é€‚åˆæƒ…èŠ‚è®°å¿†çš„æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å’Œåˆ©ç”¨ä¿¡æ¯ä¸°å¯Œçš„å¸§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒVideo-EMåœ¨Video-MMEã€EgoSchemaã€HourVideoå’ŒLVBenchåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å®ç°äº†4-9%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„å¸§æ•°æ˜¾è‘—å‡å°‘ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è§†é¢‘é—®ç­”ã€è§†é¢‘æ‘˜è¦ç”Ÿæˆå’Œé•¿è§†é¢‘å†…å®¹åˆ†æç­‰é¢†åŸŸã€‚é€šè¿‡æå‡é•¿è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§ï¼ŒVideo-EMèƒ½å¤Ÿä¸ºæ•™è‚²ã€å¨±ä¹å’Œå®‰å…¨ç›‘æ§ç­‰è¡Œä¸šå¸¦æ¥æ˜¾è‘—çš„å®é™…ä»·å€¼ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video Large Language Models (Video-LLMs) excel at general video understanding but struggle with long-form videos due to context window limits. Consequently, recent approaches focus on keyframe retrieval, condensing lengthy videos into a small set of informative frames. Despite their practicality, these methods simplify the problem to static text image matching, overlooking spatio temporal relationships crucial for capturing scene transitions and contextual continuity, and may yield redundant keyframes with limited information, diluting salient cues essential for accurate video question answering. To address these limitations, we introduce Video-EM, a training free framework inspired by the principles of human episodic memory, designed to facilitate robust and contextually grounded reasoning. Rather than treating keyframes as isolated visual entities, Video-EM explicitly models them as temporally ordered episodic events, capturing both spatial relationships and temporal dynamics necessary for accurately reconstructing the underlying narrative. Furthermore, the framework leverages chain of thought (CoT) thinking with LLMs to iteratively identify a minimal yet highly informative subset of episodic memories, enabling efficient and accurate question answering by Video-LLMs. Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench benchmarks confirm the superiority of Video-EM, which achieves highly competitive results with performance gains of 4-9 percent over respective baselines while utilizing fewer frames.

