---
layout: default
title: Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts
---

# Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts

**arXiv**: [2512.02486v1](https://arxiv.org/abs/2512.02486) | [PDF](https://arxiv.org/pdf/2512.02486.pdf)

**ä½œè€…**: Zhongjian Qiao, Rui Yang, Jiafei Lyu, Xiu Li, Zhongxiang Dai, Zhuoran Yang, Siyang Gao, Shuang Qiu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDROCOç®—æ³•ä»¥å¢žå¼ºè·¨åŸŸç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨è®­ç»ƒå’Œæµ‹è¯•æ—¶å¯¹åŠ¨æ€åç§»çš„åŒé‡é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `è·¨åŸŸç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€åç§»é²æ£’æ€§` `é²æ£’Bellmanç®—å­` `åŒé‡é²æ£’ç®—æ³•` `ç¦»çº¿ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè·¨åŸŸç¦»çº¿å¼ºåŒ–å­¦ä¹ çŽ°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è®­ç»ƒæ—¶é²æ£’æ€§ï¼Œå¿½ç•¥æµ‹è¯•æ—¶åŠ¨æ€æ‰°åŠ¨å¯¼è‡´çš„ç­–ç•¥è„†å¼±æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥é²æ£’è·¨åŸŸBellmanç®—å­ï¼Œç»“åˆåŠ¨æ€å€¼æƒ©ç½šå’ŒHuberæŸå¤±ï¼Œç¡®ä¿åŒé‡é²æ£’æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§åŠ¨æ€åç§»åœºæ™¯ä¸‹ï¼ŒDROCOä¼˜äºŽåŸºçº¿æ–¹æ³•ï¼Œå±•çŽ°å‡ºå¢žå¼ºçš„é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \textbf{D}ual-\textbf{RO}bust \textbf{C}ross-domain \textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.

