---
layout: default
title: RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning
---

# RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning

**arXiv**: [2512.02729v1](https://arxiv.org/abs/2512.02729) | [PDF](https://arxiv.org/pdf/2512.02729.pdf)

**ä½œè€…**: Yuhong Zhang, Zihan Gao, Shengpeng Li, Ling-Hao Chen, Kaisheng Liu, Runqing Cheng, Xiao Lin, Junjia Liu, Zhuoheng Li, Jingyi Feng, Ziyan He, Jintian Lin, Zheyan Huang, Zhifang Liu, Haoqian Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboWheelæ•°æ®å¼•æ“Žï¼Œä»ŽçœŸå®žäººæ‰‹äº¤äº’è§†é¢‘ç”Ÿæˆè·¨å½¢æ€æœºå™¨äººå­¦ä¹ çš„è®­ç»ƒç›‘ç£**

**å…³é”®è¯**: `äººæ‰‹äº¤äº’é‡å»º` `è·¨å½¢æ€é‡å®šå‘` `ä»¿çœŸæ•°æ®å¢žå¼º` `æœºå™¨äººå­¦ä¹ ç›‘ç£` `å¤šæ¨¡æ€æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•åˆ©ç”¨äººæ‰‹äº¤äº’è§†é¢‘ä¸ºä¸åŒå½¢æ€æœºå™¨äººæä¾›æœ‰æ•ˆç›‘ç£æ•°æ®
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é«˜ç²¾åº¦é‡å»ºä¸Žå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œå®žçŽ°ç‰©ç†åˆç†çš„è½¨è¿¹é‡å®šå‘ä¸Žä»¿çœŸå¢žå¼º
3. å®žéªŒæˆ–æ•ˆæžœï¼šéªŒè¯æ•°æ®åœ¨ä¸»æµæ¨¡åž‹ä¸Šæ€§èƒ½ç¨³å®šï¼Œåª²ç¾Žé¥æ“ä½œï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®é›†æž„å»º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.

