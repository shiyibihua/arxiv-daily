---
layout: default
title: VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm
---

# VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm

**arXiv**: [2512.02700v1](https://arxiv.org/abs/2512.02700) | [PDF](https://arxiv.org/pdf/2512.02700.pdf)

**ä½œè€…**: Zhenkai Wu, Xiaowen Ma, Zhenliang Ni, Dengming Zhang, Han Shu, Xin Jiang, Xinghao Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLM-Prunerä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç§»åŠ¨è®¾å¤‡éƒ¨ç½²ä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `ç©ºé—´ç¨€ç–æ€§` `è®¡ç®—æ•ˆçŽ‡` `ç§»åŠ¨éƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹è§†è§‰ä»¤ç‰Œæ•°é‡å¤§å¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼ŒçŽ°æœ‰å‰ªæžæ–¹æ³•å¿½è§†ä»¤ç‰Œé—´å†—ä½™å’Œç©ºé—´å…³ç³»ï¼Œå¯¼è‡´ä¿ç•™ä»¤ç‰Œç¨€ç–æˆ–é‡å¤
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç¦»å¿ƒä»¤ç‰Œå‰ªæžèŒƒå¼å¹³è¡¡å†—ä½™å’Œç©ºé—´ç¨€ç–æ€§ï¼Œè®¾è®¡ç¼“å†²ç©ºé—´ç¨€ç–æ€§å‡†åˆ™å»¶è¿Ÿé€‰æ‹©è¿œè·ç¦»ä»¤ç‰Œï¼Œå¹¶è¡Œè´ªå©ªç­–ç•¥é«˜æ•ˆé€‰æ‹©ä»¤ç‰Œ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ç§è§†è§‰è¯­è¨€æ¨¡åž‹ä¸Šä»¥88.9%å‰ªæžçŽ‡ä¼˜äºŽåŸºçº¿ï¼Œå®žçŽ°ç«¯åˆ°ç«¯æŽ¨ç†åŠ é€Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.

