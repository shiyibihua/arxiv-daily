---
layout: default
title: Hear What Matters! Text-conditioned Selective Video-to-Audio Generation
---

# Hear What Matters! Text-conditioned Selective Video-to-Audio Generation

**arXiv**: [2512.02650v1](https://arxiv.org/abs/2512.02650) | [PDF](https://arxiv.org/pdf/2512.02650.pdf)

**ä½œè€…**: Junwon Lee, Juhan Nam, Jiyoung Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSelVAæ¨¡åž‹ä»¥è§£å†³å¤šå¯¹è±¡è§†é¢‘ä¸­æ–‡æœ¬æ¡ä»¶é€‰æ‹©æ€§éŸ³é¢‘ç”Ÿæˆé—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆ` `æ–‡æœ¬æ¡ä»¶ç”Ÿæˆ` `é€‰æ‹©æ€§éŸ³é¢‘åˆ†ç¦»` `è·¨æ³¨æ„åŠ›è°ƒåˆ¶` `è‡ªå¢žå¼ºè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ç”Ÿæˆæ··åˆéŸ³é¢‘ï¼Œéš¾ä»¥ä»Žå¤šå¯¹è±¡è§†é¢‘ä¸­åˆ†ç¦»ç”¨æˆ·æŒ‡å®šå£°æºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ–‡æœ¬æç¤ºä½œä¸ºç›®æ ‡æºé€‰æ‹©å™¨ï¼Œè°ƒåˆ¶è§†é¢‘ç¼–ç å™¨æå–ç›¸å…³ç‰¹å¾ï¼Œå¹¶å¼•å…¥è¡¥å……ä»¤ç‰Œä¼˜åŒ–è·¨æ³¨æ„åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VGG-MONOAUDIOåŸºå‡†ä¸ŠéªŒè¯ï¼ŒéŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ—¶é—´åŒæ­¥è¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.

