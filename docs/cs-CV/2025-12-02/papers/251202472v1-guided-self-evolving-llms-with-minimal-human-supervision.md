---
layout: default
title: Guided Self-Evolving LLMs with Minimal Human Supervision
---

# Guided Self-Evolving LLMs with Minimal Human Supervision

**arXiv**: [2512.02472v1](https://arxiv.org/abs/2512.02472) | [PDF](https://arxiv.org/pdf/2512.02472.pdf)

**ä½œè€…**: Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR-Fewæ¡†æž¶ä»¥è§£å†³LLMè‡ªè¿›åŒ–ä¸­çš„ä¸ç¨³å®šé—®é¢˜ï¼Œé€šè¿‡è½»é‡äººç±»ç›‘ç£å®žçŽ°å¯æŽ§æ”¹è¿›ã€‚**

**å…³é”®è¯**: `è‡ªè¿›åŒ–å­¦ä¹ ` `è½»é‡ç›‘ç£` `æŒ‘æˆ˜è€…-æ±‚è§£å™¨æ¡†æž¶` `ä¸Šä¸‹æ–‡æŽ¥åœ°` `æ··åˆè®­ç»ƒ` `ç¨³å®šæ€§æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ— æŒ‡å¯¼è‡ªè¿›åŒ–æ˜“å¯¼è‡´æ¦‚å¿µæ¼‚ç§»ã€å¤šæ ·æ€§å´©æºƒå’Œè¯¯è¿›åŒ–ï¼Œæ¨¡åž‹æ€§èƒ½åœæ»žæˆ–ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æŒ‘æˆ˜è€…-æ±‚è§£å™¨è‡ªåšå¼ˆæ¡†æž¶ï¼Œç»“åˆä¸Šä¸‹æ–‡æŽ¥åœ°å’Œæ··åˆè®­ç»ƒï¼Œå®žçŽ°è½»é‡äººç±»ç›‘ç£ä¸‹çš„ç¨³å®šè¿›åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°å­¦å’Œé€šç”¨æŽ¨ç†åŸºå‡†ä¸Šå®žçŽ°è¿­ä»£æå‡ï¼ŒQwen3-8B-Baseåœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¶…è¶ŠR-Zeroï¼Œæ€§èƒ½åª²ç¾Žä½¿ç”¨20å€äººç±»æ•°æ®çš„æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

