---
layout: default
title: ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation
---

# ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation

**arXiv**: [2512.02453v1](https://arxiv.org/abs/2512.02453) | [PDF](https://arxiv.org/pdf/2512.02453.pdf)

**ä½œè€…**: Kerui Chen, Jianrong Zhang, Ming Li, Zhonglong Zheng, Hehe Fan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºClusterStyleæ¡†æž¶ï¼Œé€šè¿‡åŽŸåž‹èšç±»å»ºæ¨¡é£Žæ ¼å†…å¤šæ ·æ€§ï¼Œä»¥æå‡é£Žæ ¼åŒ–è¿åŠ¨ç”Ÿæˆæ•ˆæžœã€‚**

**å…³é”®è¯**: `é£Žæ ¼åŒ–è¿åŠ¨ç”Ÿæˆ` `åŽŸåž‹èšç±»` `é£Žæ ¼å†…å¤šæ ·æ€§` `è¿åŠ¨é£Žæ ¼è¿ç§»` `Stylistic Modulation Adapter`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¨¡åž‹éš¾ä»¥æ•æ‰é£Žæ ¼å†…å¤šæ ·æ€§ï¼Œå³åŒä¸€é£Žæ ¼å¯¹åº”å¤šç§è¿åŠ¨å˜åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨åŽŸåž‹é›†å»ºæ¨¡å…¨å±€å’Œå±€éƒ¨é£Žæ ¼å¤šæ ·æ€§ï¼Œé€šè¿‡Stylistic Modulation Adapteré›†æˆé£Žæ ¼ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é£Žæ ¼åŒ–è¿åŠ¨ç”Ÿæˆå’Œè¿åŠ¨é£Žæ ¼è¿ç§»ä»»åŠ¡ä¸­ä¼˜äºŽçŽ°æœ‰å…ˆè¿›æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.

