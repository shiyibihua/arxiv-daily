---
layout: default
title: MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding
---

# MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding

**arXiv**: [2512.02906v1](https://arxiv.org/abs/2512.02906) | [PDF](https://arxiv.org/pdf/2512.02906.pdf)

**ä½œè€…**: Fan Yang, Kaihao Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šåˆ†è¾¨çŽ‡æ£€ç´¢-æ£€æµ‹èžåˆæ¡†æž¶ä»¥è§£å†³é«˜åˆ†è¾¨çŽ‡å›¾åƒç†è§£ä¸­è¯­ä¹‰ç›¸ä¼¼æ€§åå·®é—®é¢˜**

**å…³é”®è¯**: `é«˜åˆ†è¾¨çŽ‡å›¾åƒç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è¯­ä¹‰ç›¸ä¼¼æ€§èžåˆ` `å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹` `è®­ç»ƒå…è´¹æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé«˜åˆ†è¾¨çŽ‡å›¾åƒä¸­ç›®æ ‡å¯¹è±¡è¢«åˆ†å‰²åˆ°å¤šä¸ªå›¾åƒå—ï¼Œå¯¼è‡´è¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—åå·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¤šåˆ†è¾¨çŽ‡è¯­ä¹‰èžåˆå’Œå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼Œæ— éœ€è®­ç»ƒå³å¯æ•´åˆä¸åŒåˆ†è¾¨çŽ‡ä¸‹çš„è¯­ä¹‰ä¿¡æ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é«˜åˆ†è¾¨çŽ‡å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæå‡äº†ç›®æ ‡å®šä½å‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.

