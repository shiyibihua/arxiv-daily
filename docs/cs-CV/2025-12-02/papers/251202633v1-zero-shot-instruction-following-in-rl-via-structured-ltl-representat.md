---
layout: default
title: Zero-Shot Instruction Following in RL via Structured LTL Representations
---

# Zero-Shot Instruction Following in RL via Structured LTL Representations

**arXiv**: [2512.02633v1](https://arxiv.org/abs/2512.02633) | [PDF](https://arxiv.org/pdf/2512.02633.pdf)

**ä½œè€…**: Mattia Giuri, Mathias Jackermeier, Alessandro Abate

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå›¾ç¥žç»ç½‘ç»œçš„å¸ƒå°”å…¬å¼åºåˆ—æ–¹æ³•ï¼Œä»¥è§£å†³å¤šäº‹ä»¶äº¤äº’ä¸‹å¼ºåŒ–å­¦ä¹ çš„é›¶æ ·æœ¬æŒ‡ä»¤è·Ÿéšé—®é¢˜ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `çº¿æ€§æ—¶åºé€»è¾‘` `å›¾ç¥žç»ç½‘ç»œ` `é›¶æ ·æœ¬å­¦ä¹ ` `å¤šä»»åŠ¡ç­–ç•¥` `ç»“æž„åŒ–è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨å¤šä¸ªé«˜ç»´äº‹ä»¶åŒæ—¶å‘ç”Ÿä¸”å¤æ‚äº¤äº’çš„çŽ¯å¢ƒä¸­è¡¨çŽ°ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¸ƒå°”å…¬å¼åºåˆ—å¯¹é½è‡ªåŠ¨æœºè½¬æ¢ï¼Œå¹¶é€šè¿‡å›¾ç¥žç»ç½‘ç»œç¼–ç ç»“æž„åŒ–ä»»åŠ¡è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤æ‚æ£‹ç±»çŽ¯å¢ƒä¸­éªŒè¯äº†æ–¹æ³•çš„ä¼˜åŠ¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.

