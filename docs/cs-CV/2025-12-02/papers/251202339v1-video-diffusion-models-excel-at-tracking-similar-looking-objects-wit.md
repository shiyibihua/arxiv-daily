---
layout: default
title: Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision
---

# Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision

**arXiv**: [2512.02339v1](https://arxiv.org/abs/2512.02339) | [PDF](https://arxiv.org/pdf/2512.02339.pdf)

**ä½œè€…**: Chenshuang Zhang, Kang Zhang, Joon Son Chung, In So Kweon, Junmo Kim, Chengzhi Mao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡åž‹å®žçŽ°æ— ç›‘ç£è·Ÿè¸ªè§†è§‰ç›¸ä¼¼ç‰©ä½“**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `æ— ç›‘ç£è·Ÿè¸ª` `è§†è§‰ç›¸ä¼¼ç‰©ä½“` `è¿åŠ¨è¡¨ç¤º` `è‡ªç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰ç›¸ä¼¼ç‰©ä½“åœ¨è¿åŠ¨åŒºåˆ†ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼ŒçŽ°æœ‰è‡ªç›‘ç£è·Ÿè¸ªå™¨åœ¨è§†è§‰çº¿ç´¢æ¨¡ç³Šæ—¶æ€§èƒ½å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå‘çŽ°è§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨åŽ»å™ªæ—©æœŸé˜¶æ®µè‡ªç„¶å­¦ä¹ åˆ°è¿åŠ¨è¡¨ç¤ºï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒå³å¯ç”¨äºŽè·Ÿè¸ªã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŸºå‡†æµ‹è¯•å’Œæ–°å¼•å…¥çš„è§†è§‰ç›¸ä¼¼ç‰©ä½“è·Ÿè¸ªæµ‹è¯•ä¸­ï¼Œæ€§èƒ½æå‡é«˜è¾¾6ç‚¹ï¼Œå¯è§†åŒ–éªŒè¯äº†é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.

