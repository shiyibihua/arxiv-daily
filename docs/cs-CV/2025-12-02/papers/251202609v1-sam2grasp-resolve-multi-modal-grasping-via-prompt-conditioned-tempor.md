---
layout: default
title: SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction
---

# SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction

**arXiv**: [2512.02609v1](https://arxiv.org/abs/2512.02609) | [PDF](https://arxiv.org/pdf/2512.02609.pdf)

**ä½œè€…**: Shengkai Wu, Jinrong Yang, Wenqiu Luo, Linfeng Gao, Chaohui Shang, Meiyu Zhi, Mingshan Sun, Fangping Yang, Liangliang Ren, Yong Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAM2Graspæ¡†æž¶ï¼Œé€šè¿‡æç¤ºæ¡ä»¶åŒ–æ—¶åºåŠ¨ä½œé¢„æµ‹è§£å†³å¤šæ¨¡æ€æŠ“å–é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `æ¨¡ä»¿å­¦ä¹ ` `å¤šæ¨¡æ€é—®é¢˜` `æ—¶åºåŠ¨ä½œé¢„æµ‹` `æç¤ºæ¡ä»¶åŒ–` `è§†è§‰è·Ÿè¸ª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡ä»¿å­¦ä¹ åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­å› è®­ç»ƒä¿¡å·å†²çªå¯¼è‡´åŠ¨ä½œå¹³å‡åŒ–å¤±æ•ˆ
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å†»ç»“SAM2æ¨¡åž‹è¿›è¡Œæ—¶åºè·Ÿè¸ªï¼Œç»“åˆè½»é‡åŠ¨ä½œå¤´å®žçŽ°æç¤ºæ¡ä»¶åŒ–æŠ“å–è½¨è¿¹é¢„æµ‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ‚ä¹±å¤šå¯¹è±¡æŠ“å–ä»»åŠ¡ä¸­è¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œæœ‰æ•ˆæ¶ˆé™¤ç­–ç•¥æ¨¡ç³Šæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.

