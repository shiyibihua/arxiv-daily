---
layout: default
title: FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization
---

# FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization

**arXiv**: [2512.02901v1](https://arxiv.org/abs/2512.02901) | [PDF](https://arxiv.org/pdf/2512.02901.pdf)

**ä½œè€…**: Feiyu Wang, Xinyu Tan, Bokai Huang, Yihao Zhang, Guoan Wang, Peizhuang Cong, Tong Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFairy2iæ¡†æž¶ï¼Œé€šè¿‡å¹¿æ³›çº¿æ€§è¡¨ç¤ºå’Œç›¸ä½æ„ŸçŸ¥é‡åŒ–ï¼Œå®žçŽ°é¢„è®­ç»ƒå®žå€¼æ¨¡åž‹çš„æžä½Žæ¯”ç‰¹é‡åŒ–ã€‚**

**å…³é”®è¯**: `æžä½Žæ¯”ç‰¹é‡åŒ–` `å¹¿æ³›çº¿æ€§è¡¨ç¤º` `ç›¸ä½æ„ŸçŸ¥é‡åŒ–` `å¤å€¼ç¥žç»ç½‘ç»œ` `é¢„è®­ç»ƒæ¨¡åž‹è½¬æ¢` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹é‡åŒ–è‡³å•æ¯”ç‰¹æžé™æ—¶ï¼Œå¤å€¼æ¨¡åž‹éœ€ä»Žå¤´è®­ç»ƒï¼Œæ— æ³•åˆ©ç”¨é¢„è®­ç»ƒå®žå€¼æ¨¡åž‹ç”Ÿæ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè¯æ˜Žå®žå€¼ä¸Žå¹¿æ³›çº¿æ€§æ˜ å°„çš„æ•°å­¦ç­‰ä»·æ€§ï¼Œè½¬æ¢Transformerè‡³å¤åŸŸï¼Œé‡‡ç”¨åŸºäºŽå•ä½å››æ¬¡æ ¹çš„ç›¸ä½æ„ŸçŸ¥é‡åŒ–æ–¹æ¡ˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLaMA-2 7Bä¸Šï¼Œ2æ¯”ç‰¹ç²¾åº¦æ¢å¤æ€§èƒ½æŽ¥è¿‘å…¨ç²¾åº¦åŸºçº¿ï¼Œä¼˜äºŽçŽ°æœ‰å®žå€¼äºŒå€¼åŒ–å’Œä¸‰å€¼åŒ–æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.

