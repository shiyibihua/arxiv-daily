---
layout: default
title: SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys
---

# SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys

**arXiv**: [2512.02763v1](https://arxiv.org/abs/2512.02763) | [PDF](https://arxiv.org/pdf/2512.02763.pdf)

**ä½œè€…**: Jiahao Zhao, Shuaixing Zhang, Nan Xu, Lei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSurveyEvalåŸºå‡†ä»¥å…¨é¢è¯„ä¼°LLMç”Ÿæˆçš„å­¦æœ¯ç»¼è¿°ç³»ç»Ÿ**

**å…³é”®è¯**: `LLMè¯„ä¼°` `å­¦æœ¯ç»¼è¿°ç”Ÿæˆ` `åŸºå‡†æµ‹è¯•` `å¤šç»´åº¦è¯„ä¼°` `äººæœºå¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMè‡ªåŠ¨ç”Ÿæˆç»¼è¿°ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•å°šä¸å®Œå–„ï¼Œç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºSurveyEvalåŸºå‡†ï¼Œä»Žæ•´ä½“è´¨é‡ã€å¤§çº²è¿žè´¯æ€§å’Œå¼•ç”¨å‡†ç¡®æ€§ä¸‰ä¸ªç»´åº¦è¯„ä¼°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨7ä¸ªå­¦ç§‘ä¸Šæµ‹è¯•ï¼Œæ˜¾ç¤ºä¸“ç”¨ç³»ç»Ÿä¼˜äºŽé€šç”¨é•¿æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿï¼Œè¯„ä¼°ç»“æžœä¸Žäººç±»å¯¹é½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

