---
layout: default
title: Quantum feature encoding optimization
---

# Quantum feature encoding optimization

**arXiv**: [2512.02422v1](https://arxiv.org/abs/2512.02422) | [PDF](https://arxiv.org/pdf/2512.02422.pdf)

**ä½œè€…**: Tommaso Fioravanti, Brian Quanz, Gabriele Agliardi, Edgar Andres Ruiz Guzman, GinÃ©s Carrascal, Jae-Eun Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé‡å­ç‰¹å¾ç¼–ç ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç»å…¸æ•°æ®é¢„å¤„ç†æå‡é‡å­æœºå™¨å­¦ä¹ æ¨¡åž‹æ€§èƒ½**

**å…³é”®è¯**: `é‡å­æœºå™¨å­¦ä¹ ` `ç‰¹å¾ç¼–ç ä¼˜åŒ–` `æ•°æ®é¢„å¤„ç†` `é‡å­ç”µè·¯` `æ¨¡åž‹æ€§èƒ½æå‡` `ç¡¬ä»¶å®žéªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé‡å­æœºå™¨å­¦ä¹ ä¸­æ•°æ®ç¼–ç å¯¹æ¨¡åž‹æ€§èƒ½æœ‰å†³å®šæ€§å½±å“ï¼Œä½†ç¼–ç æ–¹å¼ä¼˜åŒ–å¸¸è¢«å¿½è§†
2. æ–¹æ³•è¦ç‚¹ï¼šè°ƒæ•´æ•°æ®è¾“å…¥æ–¹å¼ï¼Œå¦‚æŽ’åºã€é€‰æ‹©å’ŒåŠ æƒç‰¹å¾ï¼Œä½œä¸ºç¼–ç å‰çš„é¢„å¤„ç†æ­¥éª¤
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§æ•°æ®é›†å’Œç”µè·¯è§„æ¨¡ä¸ŠéªŒè¯ï¼Œä¼˜åŒ–ç¼–ç èƒ½æ˜¾è‘—ä¸”ä¸€è‡´åœ°æå‡æ¨¡åž‹æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®žé‡å­ç¡¬ä»¶ä¸Šå®žçŽ°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Quantum Machine Learning (QML) holds the promise of enhancing machine learning modeling in terms of both complexity and accuracy. A key challenge in this domain is the encoding of input data, which plays a pivotal role in determining the performance of QML models. In this work, we tackle a largely unaddressed aspect of encoding that is unique to QML modeling -- rather than adjusting the ansatz used for encoding, we consider adjusting how data is conveyed to the ansatz. We specifically implement QML pipelines that leverage classical data manipulation (i.e., ordering, selecting, and weighting features) as a preprocessing step, and evaluate if these aspects of encoding can have a significant impact on QML model performance, and if they can be effectively optimized to improve performance. Our experimental results, applied across a wide variety of data sets, ansatz, and circuit sizes, with a representative QML approach, demonstrate that by optimizing how features are encoded in an ansatz we can substantially and consistently improve the performance of QML models, making a compelling case for integrating these techniques in future QML applications. Finally we demonstrate the practical feasibility of this approach by running it using real quantum hardware with 100 qubit circuits and successfully achieving improved QML modeling performance in this case as well.

