---
layout: default
title: Synthetic Error Injection Fails to Elicit Self-Correction In Language Models
---

# Synthetic Error Injection Fails to Elicit Self-Correction In Language Models

**arXiv**: [2512.02389v1](https://arxiv.org/abs/2512.02389) | [PDF](https://arxiv.org/pdf/2512.02389.pdf)

**ä½œè€…**: David X. Wu, Shreyas Kapur, Anant Sahai, Stuart Russell

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆæˆé”™è¯¯æ³¨å…¥æ–¹æ³•ä»¥è¯±å¯¼è¯­è¨€æ¨¡åž‹è‡ªæˆ‘çº æ­£ï¼Œä½†å®žéªŒè¡¨æ˜Žå…¶æ•ˆæžœæœ‰é™ã€‚**

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹` `è‡ªæˆ‘çº æ­£` `åˆæˆé”™è¯¯æ³¨å…¥` `ç›‘ç£å­¦ä¹ ` `åˆ†å¸ƒåç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ æˆæœ¬é«˜ï¼ŒæŽ¢ç´¢åˆæˆé”™è¯¯æ³¨å…¥ä½œä¸ºæ›¿ä»£æ–¹æ³•ä»¥æ¿€å‘è¯­è¨€æ¨¡åž‹è‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æŽ¨ç†é“¾ä¸­æ’å…¥äººå·¥é”™è¯¯å¹¶æŽ©ç ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ è®­ç»ƒæ¨¡åž‹è¯†åˆ«å’Œçº æ­£è¿™äº›é”™è¯¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ–¹æ³•åœ¨ç®€å•åˆæˆä»»åŠ¡ä¸Šæœªèƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¸”æ¨¡åž‹å¸¸é‡å¤åŽŸå§‹é”™è¯¯ï¼Œåˆæˆé”™è¯¯ä¸Žç­–ç•¥é”™è¯¯åˆ†å¸ƒåç§»å¯¼è‡´èƒ½åŠ›ä¸‹é™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

