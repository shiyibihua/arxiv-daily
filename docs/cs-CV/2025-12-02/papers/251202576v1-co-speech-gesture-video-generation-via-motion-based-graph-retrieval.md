---
layout: default
title: Co-speech Gesture Video Generation via Motion-Based Graph Retrieval
---

# Co-speech Gesture Video Generation via Motion-Based Graph Retrieval

**arXiv**: [2512.02576v1](https://arxiv.org/abs/2512.02576) | [PDF](https://arxiv.org/pdf/2512.02576.pdf)

**ä½œè€…**: Yafei Song, Peng Zhang, Bang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ‰©æ•£æ¨¡åž‹å’Œè¿åŠ¨å›¾æ£€ç´¢çš„æ¡†æž¶ä»¥ç”ŸæˆåŒæ­¥è‡ªç„¶çš„è¯­éŸ³æ‰‹åŠ¿è§†é¢‘**

**å…³é”®è¯**: `è¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `è¿åŠ¨å›¾æ£€ç´¢` `è§†é¢‘åˆæˆ` `å¤šå¯¹å¤šæ˜ å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯­éŸ³ä¸Žæ‰‹åŠ¿é—´å¤šå¯¹å¤šæ˜ å°„å¯¼è‡´çŽ°æœ‰åŸºäºŽä¸€å¯¹ä¸€æ˜ å°„çš„æ£€ç´¢æ–¹æ³•æ•ˆæžœä¸ä½³
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ‰©æ•£æ¨¡åž‹å­¦ä¹ éŸ³é¢‘ä¸Žè¿åŠ¨çš„è”åˆåˆ†å¸ƒç”Ÿæˆæ‰‹åŠ¿ï¼Œå†é€šè¿‡è¿åŠ¨ç›¸ä¼¼æ€§æ£€ç´¢å›¾è·¯å¾„å¹¶æ‹¼æŽ¥è§†é¢‘
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒéªŒè¯æ–¹æ³•åœ¨åŒæ­¥å‡†ç¡®æ€§å’Œæ‰‹åŠ¿è‡ªç„¶åº¦ä¸Šæ˜¾è‘—ä¼˜äºŽå…ˆå‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.

