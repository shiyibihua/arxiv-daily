---
layout: default
title: Revisiting Theory of Contrastive Learning for Domain Generalization
---

# Revisiting Theory of Contrastive Learning for Domain Generalization

**arXiv**: [2512.02831v1](https://arxiv.org/abs/2512.02831) | [PDF](https://arxiv.org/pdf/2512.02831.pdf)

**ä½œè€…**: Ali Alvandi, Mina Rezaei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æ¯”å­¦ä¹ æ³›åŒ–ç†è®ºï¼Œåˆ†æžåŸŸåç§»ä¸Žæ–°æ ‡ç­¾ç©ºé—´ä¸‹çš„è¡¨ç¤ºæ€§èƒ½ä¿è¯**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `åŸŸæ³›åŒ–` `è¡¨ç¤ºå­¦ä¹ ` `æ³›åŒ–ç†è®º` `è‡ªç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸‹æ¸¸ä»»åŠ¡å¯èƒ½æ¶‰åŠåŸŸåç§»æˆ–æ–°æ ‡ç­¾ç©ºé—´ï¼ŒçŽ°æœ‰ç†è®ºå‡è®¾ä¸é€‚ç”¨
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ³›åŒ–ç•Œï¼ŒåŒæ—¶è€ƒè™‘åŸŸåç§»å’ŒåŸŸæ³›åŒ–ä¸¤ç§ä¸åŒ¹é…ç±»åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šç†è®ºåˆ†æžæ­ç¤ºè¡¨ç¤ºæ€§èƒ½ä¾èµ–äºŽé¢„è®­ç»ƒä¸Žä¸‹æ¸¸åˆ†å¸ƒçš„ç»Ÿè®¡å·®å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.

