---
layout: default
title: AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning
---

# AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning

**arXiv**: [2512.02535v1](https://arxiv.org/abs/2512.02535) | [PDF](https://arxiv.org/pdf/2512.02535.pdf)

**ä½œè€…**: Jeric Lew, Yuhong Cao, Derek Ming Siang Tan, Guillaume Sartoretti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAIDæ¡†æž¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡åž‹è§£å†³å¤šæ™ºèƒ½ä½“ä¿¡æ¯è·¯å¾„è§„åˆ’ä¸­çš„åè°ƒé—®é¢˜**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ä¿¡æ¯è·¯å¾„è§„åˆ’` `æ‰©æ•£æ¨¡åž‹` `éžè‡ªå›žå½’è½¨è¿¹ç”Ÿæˆ` `è¡Œä¸ºå…‹éš†` `å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–` `åŽ»ä¸­å¿ƒåŒ–åè°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ™ºèƒ½ä½“ä¿¡æ¯è·¯å¾„è§„åˆ’ä¸­ï¼Œåè°ƒå›°éš¾ä¸”çŽ°æœ‰æ„å›¾é¢„æµ‹æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€æ˜“å‡ºé”™
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ‰©æ•£æ¨¡åž‹éžè‡ªå›žå½’ç”Ÿæˆé•¿æœŸè½¨è¿¹ï¼Œç»“åˆè¡Œä¸ºå…‹éš†ä¸Žå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”åŸºçº¿ï¼Œæ‰§è¡Œé€Ÿåº¦æå‡é«˜è¾¾4å€ï¼Œä¿¡æ¯å¢žç›Šå¢žåŠ 17%ï¼Œå¯æ‰©å±•è‡³æ›´å¤šæ™ºèƒ½ä½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as "intent" to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.

