---
layout: default
title: An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation
---

# An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation

**arXiv**: [2512.02689v1](https://arxiv.org/abs/2512.02689) | [PDF](https://arxiv.org/pdf/2512.02689.pdf)

**ä½œè€…**: Daiki Shirafuji, Tatsuhiko Saito, Yasutomo Kimura

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å®žè¯æ¯”è¾ƒä¸ƒç§æ¨¡åž‹åˆå¹¶ç®—æ³•åœ¨ç¼“è§£å¤§è¯­è¨€æ¨¡åž‹ç¤¾ä¼šåè§ä¸­çš„æ•ˆæžœä¸Žæƒè¡¡**

**å…³é”®è¯**: `æ¨¡åž‹åˆå¹¶ç®—æ³•` `ç¤¾ä¼šåè§ç¼“è§£` `å¤§è¯­è¨€æ¨¡åž‹` `å®žè¯è°ƒæŸ¥` `æ€§èƒ½æƒè¡¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹ç»§æ‰¿å¹¶æ”¾å¤§ç¤¾ä¼šåè§ï¼Œå¨èƒå…¬å¹³æ€§ï¼Œéœ€å‚æ•°ç¼–è¾‘ç¼“è§£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå®žè¯è°ƒæŸ¥Linearã€SLERPç­‰ä¸ƒç§åˆå¹¶ç®—æ³•ï¼Œåº”ç”¨äºŽGPTã€LLaMAç­‰13ä¸ªå¼€æºæƒé‡æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°åè§å‡å°‘ä¸Žä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒè¡¡ï¼Œå‘çŽ°SLERPåœ¨ä¸­ç­‰æ’å€¼æƒé‡ä¸‹è¡¨çŽ°æœ€å¹³è¡¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

