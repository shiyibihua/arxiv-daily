---
layout: default
title: Laplace Approximation For Tensor Train Kernel Machines In System Identification
---

# Laplace Approximation For Tensor Train Kernel Machines In System Identification

**arXiv**: [2512.02532v1](https://arxiv.org/abs/2512.02532) | [PDF](https://arxiv.org/pdf/2512.02532.pdf)

**ä½œè€…**: Albert Saiapin, Kim Batselier

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè´å¶æ–¯å¼ é‡åˆ—è½¦æ ¸æœºå™¨ï¼Œç»“åˆæ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ä¸Žå˜åˆ†æŽ¨ç†ï¼Œç”¨äºŽç³»ç»Ÿè¯†åˆ«ä¸­çš„å¯æ‰©å±•é«˜æ–¯è¿‡ç¨‹å›žå½’ã€‚**

**å…³é”®è¯**: `å¼ é‡åˆ—è½¦æ ¸æœºå™¨` `æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼` `å˜åˆ†æŽ¨ç†` `ç³»ç»Ÿè¯†åˆ«` `é«˜æ–¯è¿‡ç¨‹å›žå½’` `è´å¶æ–¯å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ é‡åˆ—è½¦æ¨¡åž‹åœ¨è´å¶æ–¯æ‰©å±•ä¸­ï¼Œä¸ç¡®å®šå“ªä¸ªæ ¸å¿ƒåº”è¿›è¡Œè´å¶æ–¯å¤„ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¯¹é€‰å®šå¼ é‡åˆ—è½¦æ ¸å¿ƒåº”ç”¨æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ä¼°è®¡åŽéªŒåˆ†å¸ƒï¼Œå¹¶ç”¨å˜åˆ†æŽ¨ç†å¤„ç†ç²¾åº¦è¶…å‚æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ ¸å¿ƒé€‰æ‹©ä¸Žå¼ é‡åˆ—è½¦ç§©å’Œç‰¹å¾ç»“æž„æ— å…³ï¼Œå˜åˆ†æŽ¨ç†æ›¿ä»£äº¤å‰éªŒè¯ï¼Œè®­ç»ƒé€Ÿåº¦æå‡é«˜è¾¾65å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To address the scalability limitations of Gaussian process (GP) regression, several approximation techniques have been proposed. One such method is based on tensor networks, which utilizes an exponential number of basis functions without incurring exponential computational cost. However, extending this model to a fully probabilistic formulation introduces several design challenges. In particular, for tensor train (TT) models, it is unclear which TT-core should be treated in a Bayesian manner. We introduce a Bayesian tensor train kernel machine that applies Laplace approximation to estimate the posterior distribution over a selected TT-core and employs variational inference (VI) for precision hyperparameters. Experiments show that core selection is largely independent of TT-ranks and feature structure, and that VI replaces cross-validation while offering up to 65x faster training. The method's effectiveness is demonstrated on an inverse dynamics problem.

