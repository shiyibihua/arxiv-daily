---
layout: default
title: Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs
---

# Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs

**arXiv**: [2512.02719v1](https://arxiv.org/abs/2512.02719) | [PDF](https://arxiv.org/pdf/2512.02719.pdf)

**ä½œè€…**: Julian Ma, Jun Wang, Zafeirios Fountas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBayesBenchåŸºå‡†ä¸Žè´å¶æ–¯ä¸€è‡´æ€§è¯„åˆ†ï¼Œè¯„ä¼°LLMsåœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„éšå¼è´å¶æ–¯è¡Œä¸ºä¸Žå¤šæ¨¡æ€çº¿ç´¢æ•´åˆã€‚**

**å…³é”®è¯**: `è´å¶æ–¯è¡Œä¸ºè¯„ä¼°` `å¤šæ¨¡æ€çº¿ç´¢æ•´åˆ` `å¿ƒç†ç‰©ç†å­¦åŸºå‡†` `LLMséšå¼è®¡ç®—` `ä¸ç¡®å®šæ€§å¤„ç†` `è¡Œä¸ºä¸€è‡´æ€§è¯„åˆ†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç©¶LLMsæ˜¯å¦åœ¨æ— æ˜¾å¼è®­ç»ƒä¸‹è¡¨çŽ°å‡ºç±»ä¼¼äººç±»çš„è¿‘æœ€ä¼˜è´å¶æ–¯ç­–ç•¥è¿›è¡Œå¤šæ¨¡æ€çº¿ç´¢æ•´åˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¿ƒç†ç‰©ç†å­¦èŒƒå¼ï¼Œè®¾è®¡æ–‡æœ¬å’Œå›¾åƒçš„å››ç§å¹…åº¦ä¼°è®¡ä»»åŠ¡ï¼Œé€šè¿‡å™ªå£°ã€ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»¤æç¤ºçš„å—æŽ§æ¶ˆèžå®žéªŒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°èƒ½åŠ›å¼ºçš„æ¨¡åž‹å¸¸ä»¥è´å¶æ–¯ä¸€è‡´æ–¹å¼é€‚åº”ï¼Œä½†å‡†ç¡®æ€§ä¸ä¿è¯é²æ£’æ€§ï¼Œå¦‚GPT-5 Miniæ–‡æœ¬å‡†ç¡®ä½†è§†è§‰æ•´åˆæ•ˆçŽ‡ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

