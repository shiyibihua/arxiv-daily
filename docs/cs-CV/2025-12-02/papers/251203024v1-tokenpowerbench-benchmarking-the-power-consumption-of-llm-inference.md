---
layout: default
title: TokenPowerBench: Benchmarking the Power Consumption of LLM Inference
---

# TokenPowerBench: Benchmarking the Power Consumption of LLM Inference

**arXiv**: [2512.03024v1](https://arxiv.org/abs/2512.03024) | [PDF](https://arxiv.org/pdf/2512.03024.pdf)

**ä½œè€…**: Chenxu Niu, Wei Zhang, Jie Li, Yongjian Zhao, Tongyang Wang, Xi Wang, Yong Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTokenPowerBenchä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†åŠŸè€—æµ‹é‡ä¸Žåˆ†æžä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `åŠŸè€—åŸºå‡†` `èƒ½æ•ˆåˆ†æž` `å¼€æºå·¥å…·` `é˜¶æ®µå¯¹é½æµ‹é‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†ç¼ºä¹å¯¹LLMæŽ¨ç†åŠŸè€—çš„æµ‹é‡æ”¯æŒï¼Œè€ŒæŽ¨ç†å åŠŸè€—90%ä»¥ä¸Š
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå£°æ˜Žå¼é…ç½®ã€æ— ä¸“ç”¨ç”µè¡¨çš„åŠŸè€—æµ‹é‡å’Œé˜¶æ®µå¯¹é½çš„æŒ‡æ ‡ç®¡é“
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°äº†Llamaç­‰å››ä¸ªæ¨¡åž‹ç³»åˆ—ï¼Œè¦†ç›–1Bè‡³405Bå‚æ•°ï¼Œå¹¶å¼€æºå·¥å…·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.

