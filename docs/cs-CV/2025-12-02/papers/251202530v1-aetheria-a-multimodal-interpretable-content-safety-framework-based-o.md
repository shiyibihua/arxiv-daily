---
layout: default
title: Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration
---

# Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration

**arXiv**: [2512.02530v1](https://arxiv.org/abs/2512.02530) | [PDF](https://arxiv.org/pdf/2512.02530.pdf)

**ä½œè€…**: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAetheriaå¤šæ¨¡æ€å¯è§£é‡Šå†…å®¹å®‰å…¨æ¡†æž¶ï¼ŒåŸºäºŽå¤šæ™ºèƒ½ä½“è¾©è®ºä¸Žåä½œè§£å†³æ•°å­—å†…å®¹å®‰å…¨æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å†…å®¹å®‰å…¨` `å¤šæ™ºèƒ½ä½“è¾©è®º` `å¯è§£é‡ŠAI` `RAGçŸ¥è¯†æ£€ç´¢` `å†…å®¹å®¡æ ¸æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•°å­—å†…å®¹æ¿€å¢žï¼ŒçŽ°æœ‰å•æ¨¡åž‹æˆ–å›ºå®šæµæ°´çº¿ç³»ç»Ÿåœ¨è¯†åˆ«éšå«é£Žé™©å’Œæä¾›å¯è§£é‡Šåˆ¤æ–­è¿‡ç¨‹æ–¹é¢å­˜åœ¨å±€é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨äº”æ ¸å¿ƒæ™ºèƒ½ä½“åä½œæž¶æž„ï¼Œé€šè¿‡åŸºäºŽRAGçŸ¥è¯†æ£€ç´¢çš„åŠ¨æ€è¾©è®ºæœºåˆ¶ï¼Œå¯¹å¤šæ¨¡æ€å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æžå’Œè£å†³ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨AIR-BenchåŸºå‡†ä¸ŠéªŒè¯ï¼ŒAetheriaç”Ÿæˆè¯¦ç»†å¯è¿½æº¯å®¡è®¡æŠ¥å‘Šï¼Œåœ¨æ•´ä½“å†…å®¹å®‰å…¨å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯éšå«é£Žé™©è¯†åˆ«ä¸Šä¼˜äºŽåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.

