---
layout: default
title: Retrieval-Augmented Memory for Online Learning
---

# Retrieval-Augmented Memory for Online Learning

**arXiv**: [2512.02333v1](https://arxiv.org/abs/2512.02333) | [PDF](https://arxiv.org/pdf/2512.02333.pdf)

**ä½œè€…**: Wenzhang Du

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAM-OLä»¥å¢žå¼ºåœ¨çº¿å­¦ä¹ åœ¨æ¦‚å¿µæ¼‚ç§»çŽ¯å¢ƒä¸‹çš„æ€§èƒ½**

**å…³é”®è¯**: `åœ¨çº¿å­¦ä¹ ` `æ¦‚å¿µæ¼‚ç§»` `æ£€ç´¢å¢žå¼ºè®°å¿†` `æœ€è¿‘é‚»æ£€ç´¢` `æ¢¯åº¦ä¸‹é™æ‰©å±•` `éžå¹³ç¨³çŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶åœ¨çº¿åˆ†ç±»åœ¨éžå¹³ç¨³çŽ¯å¢ƒä¸­çš„é—®é¢˜ï¼Œæ¦‚å¿µæ¼‚ç§»å½±å“æ¨¡åž‹é€‚åº”æ€§
2. æ‰©å±•SGDï¼Œç»´æŠ¤å°ç¼“å†²åŒºï¼Œé€šè¿‡æ£€ç´¢æœ€è¿‘é‚»å¹¶è”åˆæ›´æ–°æ¨¡åž‹æ¥å¢žå¼ºè®°å¿†
3. åœ¨å¼ºæ¼‚ç§»æ•°æ®æµä¸Šæå‡é¢„åºå‡†ç¡®çŽ‡çº¦7ä¸ªç™¾åˆ†ç‚¹ï¼Œå‡å°‘æ–¹å·®ï¼Œé—¨æŽ§å˜ä½“åœ¨å™ªå£°æµä¸­åŒ¹é…åŸºçº¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Retrieval-augmented models couple parametric predictors with non-parametric memories, but their use in streaming supervised learning with concept drift is not well understood. We study online classification in non-stationary environments and propose Retrieval-Augmented Memory for Online Learning (RAM-OL), a simple extension of stochastic gradient descent that maintains a small buffer of past examples. At each time step, RAM-OL retrieves a few nearest neighbours of the current input in the hidden representation space and updates the model jointly on the current example and the retrieved neighbours. We compare a naive replay variant with a gated replay variant that constrains neighbours using a time window, similarity thresholds, and gradient reweighting, in order to balance fast reuse of relevant past data against robustness to outdated regimes. From a theoretical perspective, we interpret RAM-OL under a bounded drift model and discuss how retrieval can reduce adaptation cost and improve regret constants when patterns recur over time. Empirically, we instantiate RAM-OL on a simple online multilayer perceptron and evaluate it on three real-world data streams derived from electricity pricing, electricity load, and airline delay data. On strongly and periodically drifting streams, RAM-OL improves prequential accuracy by up to about seven percentage points and greatly reduces variance across random seeds, while on a noisy airline stream the gated variant closely matches the purely online baseline. These results show that retrieval-augmented memory is a practical and robust tool for online learning under concept drift.

