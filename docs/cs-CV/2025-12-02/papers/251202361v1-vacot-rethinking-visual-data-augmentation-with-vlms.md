---
layout: default
title: VACoT: Rethinking Visual Data Augmentation with VLMs
---

# VACoT: Rethinking Visual Data Augmentation with VLMs

**arXiv**: [2512.02361v1](https://arxiv.org/abs/2512.02361) | [PDF](https://arxiv.org/pdf/2512.02361.pdf)

**ä½œè€…**: Zhengzhuo Xu, Chong Sun, SiNan Du, Chen Li, Jing Lyu, Chun Yuan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVACoTæ¡†æž¶ï¼Œåœ¨æŽ¨ç†æ—¶åŠ¨æ€è°ƒç”¨å›¾åƒå¢žå¼ºä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¯¹æŠ—åœºæ™¯ä¸­çš„é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å›¾åƒå¢žå¼º` `æŽ¨ç†æ—¶å¢žå¼º` `å¯¹æŠ—é²æ£’æ€§` `OCR` `åŽå¤„ç†å˜æ¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹ä¾èµ–å¤§è§„æ¨¡çœŸå®žæ•°æ®ï¼Œä¼ ç»Ÿå¢žå¼ºæ–¹æ³•åœ¨è®­ç»ƒä¸­æ•ˆæžœæœ‰é™ï¼Œå¯¼è‡´åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡æ€§èƒ½ä¸è¶³ã€‚
2. VACoTåœ¨æ¨¡åž‹æŽ¨ç†é˜¶æ®µé›†æˆé€šç”¨è§†è§‰å¢žå¼ºï¼Œé€šè¿‡åŽ»å™ªç­‰åŽå¤„ç†å˜æ¢ï¼Œå‡å°‘è®­ç»ƒå¤æ‚åº¦å’Œè®¡ç®—å¼€é”€ã€‚
3. åœ¨13ä¸ªæ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†VACoTçš„ä¼˜è¶Šæ€§ï¼Œå°¤å…¶åœ¨OCRç›¸å…³å¯¹æŠ—åœºæ™¯ä¸­æ˜¾è‘—æå‡é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.

