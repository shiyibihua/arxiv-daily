---
layout: default
title: Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation
---

# Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation

**arXiv**: [2512.02400v1](https://arxiv.org/abs/2512.02400) | [PDF](https://arxiv.org/pdf/2512.02400.pdf)

**ä½œè€…**: Wentao Xiang, Haokang Zhang, Tianhang Yang, Zedong Chu, Ruihang Chu, Shichao Xie, Yujian Yuan, Jian Sun, Zhining Gu, Junjie Wang, Xiaolong Wu, Mu Xu, Yujiu Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNav-R^2æ¡†æž¶ï¼Œé€šè¿‡åŒå…³ç³»æŽ¨ç†è§£å†³å¼€æ”¾è¯æ±‡å¯¹è±¡ç›®æ ‡å¯¼èˆªä¸­æœªè§å¯¹è±¡å®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡å¯¼èˆª` `å¯¹è±¡ç›®æ ‡å¯¼èˆª` `åŒå…³ç³»æŽ¨ç†` `æ€ç»´é“¾æŽ¨ç†` `ç›¸ä¼¼æ€§æ„ŸçŸ¥è®°å¿†` `æœªè§å¯¹è±¡å®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€æ”¾è¯æ±‡å¯¹è±¡ç›®æ ‡å¯¼èˆªä¸­ï¼ŒçŽ°æœ‰æ–¹æ³•å†³ç­–ä¸é€æ˜Žä¸”å¯¹æœªè§å¯¹è±¡å®šä½æˆåŠŸçŽ‡ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ˜¾å¼å»ºæ¨¡ç›®æ ‡-çŽ¯å¢ƒå’ŒçŽ¯å¢ƒ-åŠ¨ä½œåŒå…³ç³»ï¼Œç»“åˆç»“æž„åŒ–æ€ç»´é“¾æŽ¨ç†ä¸Žç›¸ä¼¼æ€§æ„ŸçŸ¥è®°å¿†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æœªè§å¯¹è±¡å®šä½ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼Œä¿æŒå®žæ—¶æŽ¨ç†é€Ÿåº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \href{https://github.com/AMAP-EAI/Nav-R2}{github link}.

