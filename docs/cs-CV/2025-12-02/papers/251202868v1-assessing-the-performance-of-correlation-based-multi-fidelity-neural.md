---
layout: default
title: Assessing the performance of correlation-based multi-fidelity neural emulators
---

# Assessing the performance of correlation-based multi-fidelity neural emulators

**arXiv**: [2512.02868v1](https://arxiv.org/abs/2512.02868) | [PDF](https://arxiv.org/pdf/2512.02868.pdf)

**ä½œè€…**: Cristian J. Villatoro, Gianluca Geraci, Daniele E. Schiavazzi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°åŸºäºŽç›¸å…³æ€§çš„å¤šä¿çœŸåº¦ç¥žç»æ¨¡æ‹Ÿå™¨æ€§èƒ½ï¼Œä»¥é™ä½Žé«˜ä¿çœŸæ¨¡åž‹è®¡ç®—æˆæœ¬**

**å…³é”®è¯**: `å¤šä¿çœŸåº¦æ¨¡æ‹Ÿå™¨` `ç¥žç»ç½‘ç»œæ€§èƒ½è¯„ä¼°` `è®¡ç®—æˆæœ¬é™ä½Ž` `æ•°æ®èžåˆ` `è°±åç½®ç½‘ç»œ` `ä¸ç¡®å®šæ€§é‡åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶å¤šä¿çœŸåº¦ç¥žç»æ¨¡æ‹Ÿå™¨ï¼Œç»“åˆä½Žä¿çœŸåº¦æ•°æ®ä¸Žç¨€ç¼ºé«˜ä¿çœŸåº¦æ•°æ®ï¼Œæå‡é¢„æµ‹æ•ˆçŽ‡
2. æµ‹è¯•å¤šç§ç½‘ç»œæž¶æž„å’Œæ•°æ®é›†é…ç½®ï¼ŒåŒ…æ‹¬ä¸åŒè°±åç½®ç½‘ç»œå’Œåæ ‡ç¼–ç æœºåˆ¶
3. é€šè¿‡å•ä¿çœŸåº¦å¯¹æ¯”å®žéªŒï¼Œé‡åŒ–å¤šä¿çœŸåº¦æ–¹æ³•åœ¨æ€§èƒ½å¢žç›Šä¸Šçš„ä¼˜åŠ¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Outer loop tasks such as optimization, uncertainty quantification or inference can easily become intractable when the underlying high-fidelity model is computationally expensive. Similarly, data-driven architectures typically require large datasets to perform predictive tasks with sufficient accuracy. A possible approach to mitigate these challenges is the development of multi-fidelity emulators, leveraging potentially biased, inexpensive low-fidelity information while correcting and refining predictions using scarce, accurate high-fidelity data. This study investigates the performance of multi-fidelity neural emulators, neural networks designed to learn the input-to-output mapping by integrating limited high-fidelity data with abundant low-fidelity model solutions. We investigate the performance of such emulators for low and high-dimensional functions, with oscillatory character, in the presence of discontinuities, for collections of models with equal and dissimilar parametrization, and for a possibly large number of potentially corrupted low-fidelity sources. In doing so, we consider a large number of architectural, hyperparameter, and dataset configurations including networks with a different amount of spectral bias (Multi-Layered Perceptron, Siren and Kolmogorov Arnold Network), various mechanisms for coordinate encoding, exact or learnable low-fidelity information, and for varying training dataset size. We further analyze the added value of the multi-fidelity approach by conducting equivalent single-fidelity tests for each case, quantifying the performance gains achieved through fusing multiple sources of information.

