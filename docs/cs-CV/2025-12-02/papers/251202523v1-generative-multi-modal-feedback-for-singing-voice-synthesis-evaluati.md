---
layout: default
title: Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation
---

# Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation

**arXiv**: [2512.02523v1](https://arxiv.org/abs/2512.02523) | [PDF](https://arxiv.org/pdf/2512.02523.pdf)

**ä½œè€…**: Xueyan Li, Yuxin Wang, Mengjie Jiang, Qingzi Zhu, Jiang Zhang, Zoey Kim, Yazhe Niu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆå¼å¤šæ¨¡æ€åé¦ˆæ¡†æž¶ä»¥è§£å†³æ­Œå”±è¯­éŸ³åˆæˆè¯„ä¼°ä¸­å•ç»´è¯„åˆ†å’Œæ ‡æ³¨æˆæœ¬é«˜çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ­Œå”±è¯­éŸ³åˆæˆè¯„ä¼°` `å¤šæ¨¡æ€åé¦ˆ` `éŸ³é¢‘-è¯­è¨€æ¨¡åž‹` `ç”Ÿæˆå¼å¥–åŠ±æ¨¡åž‹` `æ··åˆæ•°æ®é›†å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ­Œå”±è¯­éŸ³åˆæˆè¯„ä¼°æ–¹æ³•ä¾èµ–å•ç»´æ•°å€¼è¯„åˆ†ï¼Œéš¾ä»¥æ•æ‰è¡¨è¾¾åŠ›ç­‰å¤šç»´åº¦ï¼Œä¸”æ ‡æ³¨æˆæœ¬é«˜ã€å¯è§£é‡Šæ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨éŸ³é¢‘-è¯­è¨€æ¨¡åž‹ç”Ÿæˆæ¶µç›–æ—‹å¾‹ã€å†…å®¹å’Œå¬è§‰è´¨é‡çš„å¤šç»´è¯­è¨€å’ŒéŸ³é¢‘åé¦ˆï¼Œé€šè¿‡æ··åˆæ•°æ®é›†å¾®è°ƒå¢žå¼ºå¤šæ ·æ€§å’Œè¯­è¨€ä¸°å¯Œæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®šé‡å®žéªŒéªŒè¯äº†æ•°æ®é›†å’Œè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæ¡†æž¶èƒ½äº§ç”ŸéŸ³ä¹å‡†ç¡®ä¸”å¯è§£é‡Šçš„è¯„ä¼°ï¼Œé€‚ç”¨äºŽæŒ‡å¯¼ç”Ÿæˆæ¨¡åž‹ä¼˜åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Singing voice synthesis (SVS) has advanced significantly, enabling models to generate vocals with accurate pitch and consistent style. As these capabilities improve, the need for reliable evaluation and optimization becomes increasingly critical. However, current methods like reward systems often rely on single numerical scores, struggle to capture various dimensions such as phrasing or expressiveness, and require costly annotations, limiting interpretability and generalization. To address these issues, we propose a generative feedback (i.e., reward model) framework that provides multi-dimensional language and audio feedback for SVS assessment. Our approach leverages an audio-language model to generate text and audio critiques-covering aspects such as melody, content, and auditory quality. The model is fine-tuned on a hybrid dataset combining human music reactions and synthetic critiques from a MLLMs, enhancing diversity and linguistic richness. Quantitative experiments validate the effectiveness of the proposed dataset and training strategy, demonstrating that the framework produces musically accurate and interpretable evaluations suitable for guiding generative model improvement. The code is at [https://github.com/opendilab/VocalCritic](https://github.com/opendilab/VocalCritic)

