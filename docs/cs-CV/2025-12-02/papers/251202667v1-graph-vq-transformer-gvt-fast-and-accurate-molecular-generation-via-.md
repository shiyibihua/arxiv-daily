---
layout: default
title: Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents
---

# Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents

**arXiv**: [2512.02667v1](https://arxiv.org/abs/2512.02667) | [PDF](https://arxiv.org/pdf/2512.02667.pdf)

**ä½œè€…**: Haozhuo Zheng, Cheng Wang, Yang Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraph VQ-Transformerä»¥é«˜æ•ˆå‡†ç¡®ç”Ÿæˆåˆ†å­ï¼Œé€šè¿‡é«˜ä¿çœŸç¦»æ•£æ½œåœ¨åºåˆ—è§£å†³æ‰©æ•£æ¨¡åž‹è®¡ç®—é‡å¤§å’Œè‡ªå›žå½’æ¨¡åž‹è¯¯å·®ä¼ æ’­é—®é¢˜ã€‚**

**å…³é”®è¯**: `åˆ†å­ç”Ÿæˆ` `å›¾ç¥žç»ç½‘ç»œ` `ç¦»æ•£æ½œåœ¨ç©ºé—´` `è‡ªå›žå½’Transformer` `å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåˆ†å­ç”Ÿæˆä¸­æ‰©æ•£æ¨¡åž‹è®¡ç®—é‡å¤§ï¼Œè‡ªå›žå½’æ¨¡åž‹æ˜“è¯¯å·®ä¼ æ’­ï¼Œéœ€å…¼é¡¾å‡†ç¡®æ€§ä¸Žæ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆGraph VQ-VAEå’Œè‡ªå›žå½’Transformerï¼Œå°†åˆ†å­å›¾åŽ‹ç¼©ä¸ºé«˜ä¿çœŸç¦»æ•£åºåˆ—è¿›è¡Œåºåˆ—å»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ZINC250kç­‰åŸºå‡†ä¸Šè¾¾åˆ°SOTAæˆ–ç«žäº‰æ€§èƒ½ï¼Œå…³é”®æŒ‡æ ‡ä¼˜äºŽæ‰©æ•£æ¨¡åž‹ï¼Œæ•ˆçŽ‡é«˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.

