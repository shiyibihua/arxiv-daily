---
layout: default
title: UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making
---

# UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making

**arXiv**: [2512.02485v1](https://arxiv.org/abs/2512.02485) | [PDF](https://arxiv.org/pdf/2512.02485.pdf)

**ä½œè€…**: Qianhan Feng, Zhongzhen Huang, Yakun Zhu, Xiaofan Zhang, Qi Dou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUCAgentsæ¡†æž¶ä»¥è§£å†³åŒ»å­¦è§†è§‰é—®ç­”ä¸­è§†è§‰è¯æ®ä¸Žè¯­è¨€æŽ¨ç†è„±èŠ‚çš„é—®é¢˜**

**å…³é”®è¯**: `åŒ»å­¦è§†è§‰é—®ç­”` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `è§†è§‰è¯æ®é”šå®š` `ä¿¡æ¯ç†è®ºåˆ†æž` `ä¸´åºŠå†³ç­–æ”¯æŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨åŒ»å­¦è¯Šæ–­ä¸­å­˜åœ¨æŽ¨ç†è„±èŠ‚ï¼Œå¤šæ™ºèƒ½ä½“æ¡†æž¶æ˜“äº§ç”Ÿæ–‡æœ¬å™ªå£°ä¸”æœªé”šå®šè§†è§‰è¯æ®
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚å¤šæ™ºèƒ½ä½“ç»“æž„ï¼Œé€šè¿‡å•å‘æ”¶æ•›å’Œè¯æ®å®¡è®¡é™åˆ¶äº¤äº’ï¼ŒæŠ‘åˆ¶ä¿®è¾žæ¼‚ç§»å¹¶å¢žå¼ºè§†è§‰ä¿¡å·æå–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªåŒ»å­¦VQAåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°æ›´é«˜å‡†ç¡®çŽ‡ï¼ˆå¦‚PathVQAä¸Š71.3%ï¼‰å’Œæ›´ä½Žè®¡ç®—æˆæœ¬ï¼ˆä»¤ç‰Œæˆæœ¬é™ä½Ž87.7%ï¼‰

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.

