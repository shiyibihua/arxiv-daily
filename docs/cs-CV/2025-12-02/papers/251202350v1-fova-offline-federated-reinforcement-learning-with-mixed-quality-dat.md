---
layout: default
title: FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data
---

# FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data

**arXiv**: [2512.02350v1](https://arxiv.org/abs/2512.02350) | [PDF](https://arxiv.org/pdf/2512.02350.pdf)

**ä½œè€…**: Nan Qiao, Sheng Yue, Ju Ren, Yaoxue Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFOVAæ¡†æž¶ä»¥è§£å†³ç¦»çº¿è”é‚¦å¼ºåŒ–å­¦ä¹ ä¸­æ··åˆè´¨é‡æ•°æ®å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `ç¦»çº¿è”é‚¦å¼ºåŒ–å­¦ä¹ ` `æ··åˆè´¨é‡æ•°æ®` `æŠ•ç¥¨æœºåˆ¶` `ä¼˜åŠ¿åŠ æƒå›žå½’` `ç­–ç•¥æ”¹è¿›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç¦»çº¿è”é‚¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ··åˆè´¨é‡æ•°æ®ï¼ˆå³å®¢æˆ·ç«¯ç­–ç•¥è´¨é‡ä¸ä¸€ï¼‰ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æŠ•ç¥¨æœºåˆ¶è¯†åˆ«é«˜å›žæŠ¥åŠ¨ä½œï¼ŒåŸºäºŽä¼˜åŠ¿åŠ æƒå›žå½’æž„å»ºä¸€è‡´è®­ç»ƒç›®æ ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šç†è®ºåˆ†æžè¯æ˜Žç­–ç•¥æ”¹è¿›ï¼Œå®žéªŒæ˜¾ç¤ºåœ¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰åŸºçº¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.

