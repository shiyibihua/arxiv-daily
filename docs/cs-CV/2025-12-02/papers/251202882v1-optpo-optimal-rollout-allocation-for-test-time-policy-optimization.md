---
layout: default
title: OptPO: Optimal Rollout Allocation for Test-time Policy Optimization
---

# OptPO: Optimal Rollout Allocation for Test-time Policy Optimization

**arXiv**: [2512.02882v1](https://arxiv.org/abs/2512.02882) | [PDF](https://arxiv.org/pdf/2512.02882.pdf)

**ä½œè€…**: Youkang Wang, Jian Wang, Rubing Chen, Tianyi Zeng, Xiao-Yong Wei, Qing Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOptPOæ¡†æž¶ï¼Œé€šè¿‡è‡ªé€‚åº”åˆ†é…æŽ¨ç†é¢„ç®—ä»¥ä¼˜åŒ–æµ‹è¯•æ—¶ç­–ç•¥å­¦ä¹ æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `æµ‹è¯•æ—¶ç­–ç•¥ä¼˜åŒ–` `è´å¶æ–¯åºè´¯æ£€éªŒ` `è‡ªé€‚åº”æŽ¨ç†é¢„ç®—` `å¤§è¯­è¨€æ¨¡åž‹` `æ— ç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æµ‹è¯•æ—¶ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ä¾èµ–å›ºå®šé¢„ç®—å¤šæ•°æŠ•ç¥¨ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æŠ•ç¥¨è¿‡ç¨‹å»ºæ¨¡ä¸ºè´å¶æ–¯åºè´¯æ¦‚çŽ‡æ¯”æ£€éªŒï¼ŒåŠ¨æ€åœæ­¢é‡‡æ ·ä»¥æå‡æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ ·æŽ¨ç†åŸºå‡†ä¸Šï¼Œæ˜¾è‘—å‡å°‘rolloutå¼€é”€ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.

