---
layout: default
title: Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration
---

# Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration

**arXiv**: [2512.02458v1](https://arxiv.org/abs/2512.02458) | [PDF](https://arxiv.org/pdf/2512.02458.pdf)

**ä½œè€…**: Zhongyi Cai, Yi Du, Chen Wang, Yu Kong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º3DSPMRæ–¹æ³•ï¼Œåˆ©ç”¨3Dç©ºé—´è®°å¿†å¢žå¼ºMLLMåœ¨é¡ºåºå…·èº«ä»»åŠ¡ä¸­çš„æŽ¨ç†ä¸ŽæŽ¢ç´¢èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `é¡ºåºå…·èº«ä»»åŠ¡` `3Dç©ºé—´è®°å¿†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å…·èº«é—®ç­”` `å…·èº«å¤šæ¨¡æ€å¯¼èˆª` `å‡ ä½•ä¿¡æ¯é›†æˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶é¡ºåºå…·èº«ä»»åŠ¡ä¸­ç©ºé—´çŸ¥è¯†å¤ç”¨çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¦‚æœç´¢ä¸å­˜åœ¨ç‰©ä½“å¯¼è‡´å­ä»»åŠ¡ä¸å¯è¡Œã€‚
2. æå‡º3DSPMRæ–¹æ³•ï¼Œæ•´åˆå…³ç³»ã€è§†è§‰å’Œå‡ ä½•çº¿ç´¢æž„å»º3Dç©ºé—´è®°å¿†ä»¥å¢žå¼ºMLLMã€‚
3. åœ¨SEER-BenchåŸºå‡†ä¸ŠéªŒè¯ï¼Œ3DSPMRåœ¨é¡ºåºEQAå’ŒEMNä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.

