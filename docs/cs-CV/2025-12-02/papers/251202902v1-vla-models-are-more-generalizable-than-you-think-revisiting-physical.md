---
layout: default
title: VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling
---

# VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling

**arXiv**: [2512.02902v1](https://arxiv.org/abs/2512.02902) | [PDF](https://arxiv.org/pdf/2512.02902.pdf)

**ä½œè€…**: Weiqi Li, Quande Zhang, Ruifeng Zhai, Liang Lin, Guangrun Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§è§†è§‰é€‚åº”æ¡†æž¶ï¼Œé€šè¿‡ç‰¹å¾æ ¡å‡†è§£å†³VLAæ¨¡åž‹è§†è§’æ³›åŒ–é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `ç©ºé—´å»ºæ¨¡` `è§†è§’æ³›åŒ–` `è½»é‡çº§é€‚åº”` `ç‰¹å¾æ ¡å‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨è§†è§’å˜åŒ–ä¸‹æ€§èƒ½éª¤é™ï¼ŒæºäºŽç©ºé—´å»ºæ¨¡è€Œéžç‰©ç†å»ºæ¨¡çš„é”™ä½
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºFTMå…¨å±€ä»¿å°„å˜æ¢å’ŒFLAä½Žç§©æ›´æ–°ï¼Œå®žçŽ°è½»é‡çº§è§†è§‰è¡¨å¾é‡æ ¡å‡†
3. å®žéªŒæ•ˆæžœï¼šFTMä»…ç”¨4Kå‚æ•°å°†Liberoè§†è§’å‡†ç¡®çŽ‡ä»Ž48.5%æå‡è‡³87.1%ï¼ŒFLAè¾¾90.8%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.

