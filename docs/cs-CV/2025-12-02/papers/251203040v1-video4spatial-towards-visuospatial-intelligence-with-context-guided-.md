---
layout: default
title: Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation
---

# Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation

**arXiv**: [2512.03040v1](https://arxiv.org/abs/2512.03040) | [PDF](https://arxiv.org/pdf/2512.03040.pdf)

**ä½œè€…**: Zeqi Xiao, Yiwei Zhao, Lingxiao Li, Yushi Lan, Yu Ning, Rahul Garg, Roshni Cooper, Mohammad H. Taghavi, Xingang Pan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo4Spatialæ¡†æž¶ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡åž‹å®žçŽ°ä»…åŸºäºŽè§†è§‰æ•°æ®çš„å¤æ‚ç©ºé—´ä»»åŠ¡å¤„ç†ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆæ¨¡åž‹` `è§†è§‰ç©ºé—´æ™ºèƒ½` `åœºæ™¯å¯¼èˆª` `å¯¹è±¡å®šä½` `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `ä¸Šä¸‹æ–‡å¼•å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç´¢è§†é¢‘ç”Ÿæˆæ¨¡åž‹æ˜¯å¦ä»…å‡­è§†è§‰æ•°æ®å±•çŽ°ç±»ä¼¼äººç±»çš„è§†è§‰ç©ºé—´æ™ºèƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è§†é¢‘åœºæ™¯ä¸Šä¸‹æ–‡å¼•å¯¼çš„è§†é¢‘æ‰©æ•£æ¨¡åž‹ï¼Œæ‰§è¡Œåœºæ™¯å¯¼èˆªå’Œå¯¹è±¡å®šä½ä»»åŠ¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¯¼èˆªå’Œå®šä½ä»»åŠ¡ä¸­å±•ç¤ºå¼ºç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¹¶æ³›åŒ–è‡³é•¿ä¸Šä¸‹æ–‡å’ŒåŸŸå¤–çŽ¯å¢ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.

