---
layout: default
title: Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation
---

# Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation

**arXiv**: [2512.02323v1](https://arxiv.org/abs/2512.02323) | [PDF](https://arxiv.org/pdf/2512.02323.pdf)

**ä½œè€…**: Kentaro Kubo, Hayato Goto

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ—ä¹‹ä¸‡æ¨¡æ‹Ÿåˆ†å²”é‡‡æ ·å™¨å’Œæ¡ä»¶æœŸæœ›åŒ¹é…æ–¹æ³•ï¼Œä»¥è§£å†³çŽ»å°”å…¹æ›¼æœºè®­ç»ƒä¸­é‡‡æ ·å¹¶è¡ŒåŒ–å’Œæ¸©åº¦æŽ§åˆ¶é—®é¢˜ã€‚**

**å…³é”®è¯**: `çŽ»å°”å…¹æ›¼æœº` `å¹¶è¡Œé‡‡æ ·` `æ¸©åº¦ä¼°è®¡` `èƒ½é‡åŸºç”Ÿæˆæ¨¡åž‹` `é‡‡æ ·å™¨è‡ªé€‚åº”å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ»å°”å…¹æ›¼æœºè®­ç»ƒæˆæœ¬é«˜ï¼Œé‡‡æ ·éš¾ä»¥å¹¶è¡ŒåŒ–ï¼Œé™åˆ¶äº†å…¶åº”ç”¨è¶…è¶Šå—é™çŽ»å°”å…¹æ›¼æœºã€‚
2. åŸºäºŽæ¨¡æ‹Ÿåˆ†å²”æå‡ºæœ—ä¹‹ä¸‡æ¨¡æ‹Ÿåˆ†å²”é‡‡æ ·å™¨ï¼Œå®žçŽ°å¹¶è¡Œé‡‡æ ·ï¼Œé€‚ç”¨äºŽä¸€èˆ¬è€¦åˆçŽ»å°”å…¹æ›¼æœºã€‚
3. ç»“åˆæ¡ä»¶æœŸæœ›åŒ¹é…ä¼°è®¡é€†æ¸©åº¦ï¼Œå»ºç«‹é‡‡æ ·å™¨è‡ªé€‚åº”å­¦ä¹ æ¡†æž¶ï¼Œæå‡ç”Ÿæˆå»ºæ¨¡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Boltzmann machines (BMs) are powerful energy-based generative models, but their heavy training cost has largely confined practical use to Restricted BMs (RBMs) trained with an efficient learning method called contrastive divergence. More accurate learning typically requires Markov chain Monte Carlo (MCMC) Boltzmann sampling, but it is time-consuming due to the difficulty of parallelization for more expressive models. To address this limitation, we first propose a new Boltzmann sampler inspired by a quantum-inspired combinatorial optimization called simulated bifurcation (SB). This SB-inspired approach, which we name Langevin SB (LSB), enables parallelized sampling while maintaining accuracy comparable to MCMC. Furthermore, this is applicable not only to RBMs but also to BMs with general couplings. However, LSB cannot control the inverse temperature of the output Boltzmann distribution, which hinders learning and degrades performance. To overcome this limitation, we also developed an efficient method for estimating the inverse temperature during the learning process, which we call conditional expectation matching (CEM). By combining LSB and CEM, we establish an efficient learning framework for BMs with greater expressive power than RBMs. We refer to this framework as sampler-adaptive learning (SAL). SAL opens new avenues for energy-based generative modeling beyond RBMs.

