---
layout: default
title: Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities
---

# Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities

**arXiv**: [2512.02973v1](https://arxiv.org/abs/2512.02973) | [PDF](https://arxiv.org/pdf/2512.02973.pdf)

**ä½œè€…**: Yuan Xiong, Ziqi Miao, Lijun Li, Chen Qian, Jie Li, Jing Shao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºContextual Image Attackï¼Œé€šè¿‡è§†è§‰ä¸Šä¸‹æ–‡åµŒå…¥æœ‰å®³æŸ¥è¯¢ä»¥æ”»å‡»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨å¯¹é½ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å®‰å…¨å¯¹é½` `è§†è§‰ä¸Šä¸‹æ–‡æ”»å‡»` `å¤šä»£ç†ç³»ç»Ÿ` `æ¯’æ€§è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ”»å‡»æ–¹æ³•æœªå……åˆ†åˆ©ç”¨å›¾åƒæºå¸¦å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ½œåŠ›ï¼Œè§†è§‰æ¨¡æ€å®‰å…¨æ¼æ´žæ˜¾è‘—ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿï¼Œé€šè¿‡å››ç§å¯è§†åŒ–ç­–ç•¥å°†æœ‰å®³æŸ¥è¯¢åµŒå…¥è‰¯æ€§è§†è§‰ä¸Šä¸‹æ–‡ï¼Œç»“åˆå…ƒç´ å¢žå¼ºå’Œæ¯’æ€§æ··æ·†æŠ€æœ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MMSafetyBench-tinyæ•°æ®é›†ä¸Šï¼Œå¯¹GPT-4oå’ŒQwen2.5-VL-72Bæ¨¡åž‹æ¯’æ€§å¾—åˆ†é«˜ï¼Œæ”»å‡»æˆåŠŸçŽ‡è¶…è¿‡86%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.

