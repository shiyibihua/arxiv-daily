---
layout: default
title: Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education
---

# Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education

**arXiv**: [2512.02651v1](https://arxiv.org/abs/2512.02651) | [PDF](https://arxiv.org/pdf/2512.02651.pdf)

**ä½œè€…**: Alvaro Becerra, Pablo Villegas, Ruth Cobos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWatch-DMLTå’ŒViSeDOPSå·¥å…·ï¼Œä»¥è§£å†³æ•™è‚²ä¸­å¤šæ¨¡æ€æ•°æ®é‡‡é›†ä¸Žå¯è§†åŒ–çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ åˆ†æž` `æ™ºèƒ½æ‰‹è¡¨æ•°æ®é‡‡é›†` `å®žæ—¶ç›‘æŽ§` `æ•™è‚²å¯è§†åŒ–` `ç”Ÿç†ä¿¡å·åˆ†æž` `è¯¾å ‚éƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•™è‚²ä¸­ç¼ºä¹å¯æ‰©å±•ã€åŒæ­¥ã€é«˜åˆ†è¾¨çŽ‡çš„å¤šæ¨¡æ€æ•°æ®é‡‡é›†å·¥å…·ï¼Œé˜»ç¢å¤šæ¨¡æ€å­¦ä¹ åˆ†æžçš„å®žé™…åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘Watch-DMLTç”¨äºŽæ™ºèƒ½æ‰‹è¡¨å®žæ—¶é‡‡é›†ç”Ÿç†å’Œè¿åŠ¨æ•°æ®ï¼ŒViSeDOPSç”¨äºŽä»ªè¡¨ç›˜å¯è§†åŒ–åŒæ­¥å¤šæ¨¡æ€æ•°æ®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨65åå­¦ç”Ÿè¯¾å ‚éƒ¨ç½²ä¸­ï¼ŒæˆåŠŸé‡‡é›†å¿ƒçŽ‡ã€è¿åŠ¨ã€æ³¨è§†ç­‰å¤šæ¨¡æ€æ•°æ®ï¼ŒéªŒè¯äº†ç³»ç»Ÿçš„å¯è¡Œæ€§å’Œå®žç”¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Wearable sensors, such as smartwatches, have become increasingly prevalent across domains like healthcare, sports, and education, enabling continuous monitoring of physiological and behavioral data. In the context of education, these technologies offer new opportunities to study cognitive and affective processes such as engagement, attention, and performance. However, the lack of scalable, synchronized, and high-resolution tools for multimodal data acquisition continues to be a significant barrier to the widespread adoption of Multimodal Learning Analytics in real-world educational settings. This paper presents two complementary tools developed to address these challenges: Watch-DMLT, a data acquisition application for Fitbit Sense 2 smartwatches that enables real-time, multi-user monitoring of physiological and motion signals; and ViSeDOPS, a dashboard-based visualization system for analyzing synchronized multimodal data collected during oral presentations. We report on a classroom deployment involving 65 students and up to 16 smartwatches, where data streams including heart rate, motion, gaze, video, and contextual annotations were captured and analyzed. Results demonstrate the feasibility and utility of the proposed system for supporting fine-grained, scalable, and interpretable Multimodal Learning Analytics in real learning environments.

