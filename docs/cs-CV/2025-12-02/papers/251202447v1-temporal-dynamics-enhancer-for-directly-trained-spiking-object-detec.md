---
layout: default
title: Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors
---

# Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors

**arXiv**: [2512.02447v1](https://arxiv.org/abs/2512.02447) | [PDF](https://arxiv.org/pdf/2512.02447.pdf)

**ä½œè€…**: Fan Luo, Zeyu Gao, Xinhao Luo, Kai Zhao, Yanfeng Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTemporal Dynamics Enhancerä»¥å¢žå¼ºè„‰å†²ç¥žç»ç½‘ç»œåœ¨ç›®æ ‡æ£€æµ‹ä¸­çš„æ—¶åºå»ºæ¨¡èƒ½åŠ›**

**å…³é”®è¯**: `è„‰å†²ç¥žç»ç½‘ç»œ` `ç›®æ ‡æ£€æµ‹` `æ—¶åºå»ºæ¨¡` `èƒ½è€—ä¼˜åŒ–` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰SNNè¾“å…¥ç­–ç•¥å¯¼è‡´ç¥žç»å…ƒæŽ¥æ”¶ç›¸ä¼¼åˆºæ¿€ï¼Œé™åˆ¶æ¨¡åž‹è¡¨è¾¾èƒ½åŠ›
2. TDEåŒ…å«Spiking Encoderå’ŒAttention Gating Moduleï¼Œå¢žå¼ºæ—¶åºä¿¡æ¯å»ºæ¨¡
3. å®žéªŒæ˜¾ç¤ºTDEåœ¨PASCAL VOCå’ŒEvDET200Kæ•°æ®é›†ä¸Šæ€§èƒ½æå‡ï¼ŒSDAé™ä½Žèƒ½è€—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.

