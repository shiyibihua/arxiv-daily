---
layout: default
title: When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents
---

# When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents

**arXiv**: [2512.02445v1](https://arxiv.org/abs/2512.02445) | [PDF](https://arxiv.org/pdf/2512.02445.pdf)

**ä½œè€…**: Tsimur Hadeliya, Mohammad Ali Jauhar, Nidhi Sakpal, Diogo Cruz

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºé•¿ä¸Šä¸‹æ–‡LLMä»£ç†ä¸­å®‰å…¨æœºåˆ¶çš„ä¸ç¨³å®šæ€§åŠå…¶å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡LLMä»£ç†` `å®‰å…¨æœºåˆ¶ä¸ç¨³å®šæ€§` `ä»»åŠ¡æ€§èƒ½è¯„ä¼°` `æ‹’ç»çŽ‡åˆ†æž` `å¤šæ­¥ä»»åŠ¡å®‰å…¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡LLMä»£ç†åœ¨ä»»åŠ¡æ€§èƒ½å’Œå®‰å…¨æ‹’ç»çŽ‡ä¸Šè¡¨çŽ°å‡ºä¸å¯é¢„æµ‹çš„æ³¢åŠ¨ï¼ŒçŽ°æœ‰è¯„ä¼°èŒƒå¼å­˜åœ¨å±€é™
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å®žéªŒåˆ†æžä¸Šä¸‹æ–‡é•¿åº¦ã€ç±»åž‹å’Œä½ç½®å¯¹ä»£ç†è¡Œä¸ºå’Œæ‹’ç»çŽ‡çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨100K-200Kä»¤ç‰Œä¸‹ï¼Œæ€§èƒ½ä¸‹é™è¶…50%ï¼Œæ‹’ç»çŽ‡å˜åŒ–æ˜¾è‘—ï¼Œå¦‚GPT-4.1-nanoä»Ž5%å¢žè‡³40%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\sim$5\% to $\sim$40\% while Grok 4 Fast decreases from $\sim$80\% to $\sim$10\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.

