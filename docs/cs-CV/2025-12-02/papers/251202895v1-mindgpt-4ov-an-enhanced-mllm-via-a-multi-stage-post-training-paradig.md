---
layout: default
title: MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm
---

# MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm

**arXiv**: [2512.02895v1](https://arxiv.org/abs/2512.02895) | [PDF](https://arxiv.org/pdf/2512.02895.pdf)

**ä½œè€…**: Wei Chen, Chaoqun Du, Feng Gu, Wei He, Qizhen Li, Zide Liu, Xuhao Pan, Chang Ren, Xudong Rao, Chenfeng Wang, Tao Wei, Chengjun Yu, Pengfei Yu, Yufei Zheng, Chunpeng Zhou, Pan Zhou, Xuhan Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šé˜¶æ®µåŽè®­ç»ƒèŒƒå¼ä»¥å¢žå¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹èƒ½åŠ›ä¸Žéƒ¨ç½²æ•ˆçŽ‡**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŽè®­ç»ƒèŒƒå¼` `æ•°æ®ç”Ÿæˆ` `ç›‘ç£å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ ` `é«˜æ•ˆéƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ä½Žæˆæœ¬æå‡MLLMåŸºç¡€èƒ½åŠ›ä¸Žæ³›åŒ–æ€§ï¼Œå®žçŽ°å­¦æœ¯åˆ°å·¥ä¸šçš„æ— ç¼è¿‡æ¸¡
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä¿¡æ¯å¯†åº¦çš„æ•°æ®ç”Ÿæˆã€åä½œè¯¾ç¨‹ç›‘ç£å¾®è°ƒã€æ··åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æŽ¨ç†ä¸Žå¤šç›®æ ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MMBenchç­‰åŸºå‡†ä¸Šè¶…è¶ŠSOTAï¼Œç”¨æˆ·ä½“éªŒä¼˜å¼‚ï¼Œå¼€æºæ¨¡åž‹æƒé‡ä¸Žä»£ç 

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.

