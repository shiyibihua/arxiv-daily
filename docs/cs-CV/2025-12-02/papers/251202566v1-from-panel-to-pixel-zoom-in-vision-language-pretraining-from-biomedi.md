---
layout: default
title: From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature
---

# From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature

**arXiv**: [2512.02566v1](https://arxiv.org/abs/2512.02566) | [PDF](https://arxiv.org/pdf/2512.02566.pdf)

**ä½œè€…**: Kun Yuan, Min Woo Sun, Zhen Chen, Alejandro Lozano, Xiangteng He, Shi Li, Nassir Navab, Xiaoxiao Sun, Nicolas Padoy, Serena Yeung-Levy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPanel2Patchæ•°æ®ç®¡é“ï¼Œé€šè¿‡æŒ–æŽ˜ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­çš„å±‚æ¬¡ç»“æž„ï¼Œè§£å†³è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸­å¿½ç•¥å±€éƒ¨å¯¹åº”å…³ç³»çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡åž‹` `å±‚æ¬¡ç»“æž„æŒ–æŽ˜` `å¤šç²’åº¦ç›‘ç£` `æ•°æ®ç®¡é“` `ç§‘å­¦æ–‡çŒ®åˆ†æž` `å±€éƒ¨è¯­ä¹‰å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒå°†ç§‘å­¦å›¾è¡¨åŽ‹ç¼©ä¸ºç²—ç²’åº¦å›¾çº§é…å¯¹ï¼Œä¸¢å¼ƒäº†ä¸´åºŠåŒ»ç”Ÿä¾èµ–çš„å±€éƒ¨è¯­ä¹‰å¯¹åº”ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šPanel2Pipelineè§£æžå›¾è¡¨å¸ƒå±€ã€é¢æ¿å’Œè§†è§‰æ ‡è®°ï¼Œæž„å»ºå›¾ã€é¢æ¿å’Œè¡¥ä¸çº§åˆ«çš„å±‚æ¬¡å¯¹é½è§†è§‰è¯­è¨€å¯¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä½¿ç”¨å°‘é‡æ–‡çŒ®å›¾è¡¨æå–æ›´æœ‰æ•ˆç›‘ç£ï¼Œåœ¨è¾ƒå°‘é¢„è®­ç»ƒæ•°æ®ä¸‹å®žçŽ°æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.

