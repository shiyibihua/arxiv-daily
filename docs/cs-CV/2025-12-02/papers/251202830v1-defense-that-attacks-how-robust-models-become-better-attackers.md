---
layout: default
title: Defense That Attacks: How Robust Models Become Better Attackers
---

# Defense That Attacks: How Robust Models Become Better Attackers

**arXiv**: [2512.02830v1](https://arxiv.org/abs/2512.02830) | [PDF](https://arxiv.org/pdf/2512.02830.pdf)

**ä½œè€…**: Mohamed Awad, Mahmoud Akrm, Walid Gomaa

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºå¯¹æŠ—è®­ç»ƒæ„å¤–å¢žå¼ºå¯¹æŠ—æ ·æœ¬å¯è¿ç§»æ€§ï¼Œæå‡ºæ–°é£Žé™©è¯„ä¼°æ¡†æž¶**

**å…³é”®è¯**: `å¯¹æŠ—è®­ç»ƒ` `å¯¹æŠ—æ ·æœ¬å¯è¿ç§»æ€§` `æ¨¡åž‹é²æ£’æ€§è¯„ä¼°` `è®¡ç®—æœºè§†è§‰å®‰å…¨` `æ·±åº¦å­¦ä¹ æ¼æ´ž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹æŠ—è®­ç»ƒå¯¹å¯¹æŠ—æ ·æœ¬å¯è¿ç§»æ€§çš„å½±å“æœªçŸ¥ï¼Œå¯èƒ½å¼•å…¥æ–°é£Žé™©
2. æ–¹æ³•è¦ç‚¹ï¼šè®­ç»ƒ36ä¸ªå¤šæ ·åŒ–æ¨¡åž‹ï¼ŒåŒ…æ‹¬CNNå’ŒViTï¼Œè¿›è¡Œç»¼åˆå¯è¿ç§»æ€§å®žéªŒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°å¯¹æŠ—è®­ç»ƒæ¨¡åž‹äº§ç”Ÿçš„æ‰°åŠ¨æ›´æ˜“è¿ç§»ï¼Œå½¢æˆæ‚–è®ºï¼Œå¹¶å‘å¸ƒä»£ç ä¿ƒè¿›ç ”ç©¶

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.

