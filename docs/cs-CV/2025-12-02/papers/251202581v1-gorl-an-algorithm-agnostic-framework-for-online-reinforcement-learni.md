---
layout: default
title: GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies
---

# GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies

**arXiv**: [2512.02581v1](https://arxiv.org/abs/2512.02581) | [PDF](https://arxiv.org/pdf/2512.02581.pdf)

**ä½œè€…**: Chubin Zhang, Zhenglin Wan, Feng Chen, Xingrui Yu, Ivor Tsang, Bo An

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGoRLæ¡†æž¶ï¼Œé€šè¿‡è§£è€¦ä¼˜åŒ–ä¸Žç”Ÿæˆï¼Œè§£å†³åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç”Ÿæˆç­–ç•¥çš„ç¨³å®šæ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `åœ¨çº¿å¼ºåŒ–å­¦ä¹ ` `ç”Ÿæˆç­–ç•¥` `ç¨³å®šæ€§ä¼˜åŒ–` `è¿žç»­æŽ§åˆ¶` `åŒæ—¶é—´å°ºåº¦æ›´æ–°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œé«˜æ–¯ç­–ç•¥è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œè€Œç”Ÿæˆç­–ç•¥ï¼ˆå¦‚æ‰©æ•£æ¨¡åž‹ï¼‰å› ä¼¼ç„¶éš¾å¤„ç†å¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šGoRLæ¡†æž¶ä¼˜åŒ–æ˜“å¤„ç†çš„æ½œåœ¨ç­–ç•¥ï¼Œä½¿ç”¨æ¡ä»¶ç”Ÿæˆè§£ç å™¨åˆæˆåŠ¨ä½œï¼Œé€šè¿‡åŒæ—¶é—´å°ºåº¦æ›´æ–°å®žçŽ°ç¨³å®šå­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è¿žç»­æŽ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒGoRLä¼˜äºŽé«˜æ–¯ç­–ç•¥å’Œç”Ÿæˆç­–ç•¥åŸºçº¿ï¼Œåœ¨HopperStandä»»åŠ¡ä¸Šå½’ä¸€åŒ–å›žæŠ¥è¶…870ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.

