---
layout: default
title: AVGGT: Rethinking Global Attention for Accelerating VGGT
---

# AVGGT: Rethinking Global Attention for Accelerating VGGT

**arXiv**: [2512.02541v1](https://arxiv.org/abs/2512.02541) | [PDF](https://arxiv.org/pdf/2512.02541.pdf)

**ä½œè€…**: Xianbing Sun, Zhikai Zhu, Zhengyu Lou, Bo Yang, Jinyang Tang, Liqing Zhang, He Wang, Jianfu Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAVGGTè®­ç»ƒæ— å…³åŠ é€Ÿæ–¹æ¡ˆï¼Œé€šè¿‡åˆ†æžå…¨å±€æ³¨æ„åŠ›è§’è‰²å¹¶é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡VGGT/Ï€Â³æŽ¨ç†é€Ÿåº¦**

**å…³é”®è¯**: `å¤šè§†å›¾ä¸‰ç»´é‡å»º` `æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–` `æŽ¨ç†åŠ é€Ÿ` `å…¨å±€æ³¨æ„åŠ›åˆ†æž` `è®­ç»ƒæ— å…³åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åˆ†æžVGGT/Ï€Â³å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‘çŽ°æ—©æœŸå±‚æ— æœ‰æ•ˆå¯¹åº”ã€ä¸­é—´å±‚è´Ÿè´£è·¨è§†å›¾å¯¹é½ã€åŽæœŸå±‚ä»…å¾®è°ƒ
2. æå‡ºä¸¤æ­¥åŠ é€Ÿç­–ç•¥ï¼šæ—©æœŸå±‚è½¬ä¸ºå¸§æ³¨æ„åŠ›ï¼Œä¸­é—´å±‚é€šè¿‡ä»¤ç‰Œå­é‡‡æ ·ä¿ç•™å¯¹è§’çº¿å¹¶è¡¥å……å‡å€¼
3. åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°8-10å€æŽ¨ç†åŠ é€Ÿï¼Œç²¾åº¦æŒå¹³æˆ–ç•¥å‡ï¼Œåœ¨å¯†é›†å¤šè§†å›¾åœºæ™¯ä¿æŒé²æ£’

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Since DUSt3R, models such as VGGT and $Ï€^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $Ï€^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $Ï€^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.

