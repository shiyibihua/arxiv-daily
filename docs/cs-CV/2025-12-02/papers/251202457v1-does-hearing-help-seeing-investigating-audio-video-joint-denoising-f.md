---
layout: default
title: Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation
---

# Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation

**arXiv**: [2512.02457v1](https://arxiv.org/abs/2512.02457) | [PDF](https://arxiv.org/pdf/2512.02457.pdf)

**ä½œè€…**: Jianzong Wu, Hao Lian, Dachao Hao, Ye Tian, Qingyu Shi, Biaolong Chen, Hao Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéŸ³é¢‘-è§†é¢‘è”åˆåŽ»å™ªè®­ç»ƒæ–¹æ³•ï¼Œä»¥æå‡è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œå°¤å…¶åœ¨å¤æ‚è¿åŠ¨åœºæ™¯ä¸­ã€‚**

**å…³é”®è¯**: `éŸ³é¢‘-è§†é¢‘è”åˆåŽ»å™ª` `è§†é¢‘ç”Ÿæˆ` `è·¨æ¨¡æ€è®­ç»ƒ` `å‚æ•°é«˜æ•ˆæž¶æž„` `ç‰©ç†ä¸–ç•Œå»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šéŸ³é¢‘-è§†é¢‘è”åˆåŽ»å™ªè®­ç»ƒæ˜¯å¦èƒ½æå‡ä»…å…³æ³¨è§†é¢‘è´¨é‡çš„ç”Ÿæˆæ•ˆæžœã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å‚æ•°é«˜æ•ˆçš„AVFullDiTæž¶æž„ï¼Œç»“åˆé¢„è®­ç»ƒT2Vå’ŒT2Aæ¨¡å—è¿›è¡Œè”åˆåŽ»å™ªã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤§åž‹å’Œç‰©ä½“æŽ¥è§¦è¿åŠ¨å­é›†ä¸Šï¼ŒéŸ³é¢‘-è§†é¢‘è”åˆè®­ç»ƒå¸¦æ¥è§†é¢‘è´¨é‡ä¸€è‡´æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.

