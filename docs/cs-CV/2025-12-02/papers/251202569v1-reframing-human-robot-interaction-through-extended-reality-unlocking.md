---
layout: default
title: Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models
---

# Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models

**arXiv**: [2512.02569v1](https://arxiv.org/abs/2512.02569) | [PDF](https://arxiv.org/pdf/2512.02569.pdf)

**ä½œè€…**: Yuchong Zhang, Yong Ma, Danica Kragic

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ‰©å±•çŽ°å®žå’ŒåŸºç¡€æ¨¡åž‹çš„è™šæ‹Ÿæœºå™¨äººï¼Œä»¥é‡å¡‘äººæœºäº¤äº’ä¸ºæ›´å®‰å…¨ã€æ™ºèƒ½å’Œå…±æƒ…çš„èŒƒå¼**

**å…³é”®è¯**: `æ‰©å±•çŽ°å®ž` `äººæœºäº¤äº’` `åŸºç¡€æ¨¡åž‹` `è™šæ‹Ÿæœºå™¨äºº` `å…±æƒ…äº¤äº’` `ä¼¦ç†è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç‰©ç†æœºå™¨äººå—ç¡¬ä»¶é™åˆ¶ï¼Œéš¾ä»¥å®žçŽ°çµæ´»ã€å…±æƒ…çš„äººæœºäº¤äº’
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ‰©å±•çŽ°å®žå’ŒåŸºç¡€æ¨¡åž‹æž„å»ºè™šæ‹Ÿæœºå™¨äººï¼Œæ”¯æŒæƒ…å¢ƒæ„ŸçŸ¥æŽ¨ç†å’Œé•¿æœŸé€‚åº”
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥ï¼Œä½†è®¨è®ºå…¶åœ¨å®‰å…¨å…³é”®åœºæ™¯ã€è·¨é¢†åŸŸå…±æƒ…äº¤äº’å’Œæ‰©å±•ç‰©ç†èƒ½åŠ›ä¸­çš„åº”ç”¨æ½œåŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.

