---
layout: default
title: CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography
---

# CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography

**arXiv**: [2512.02625v1](https://arxiv.org/abs/2512.02625) | [PDF](https://arxiv.org/pdf/2512.02625.pdf)

**ä½œè€…**: Mayar Elfares, Pascal Reisert, Tilman Dietz, Manpa Barman, Ahmed Zaki, Ralf KÃ¼sters, Andreas Bulling

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCryptoQAæ•°æ®é›†ä»¥è¯„ä¼°å’Œè®­ç»ƒå¤§è¯­è¨€æ¨¡åž‹åœ¨å¯†ç å­¦ä»»åŠ¡ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¯†ç å­¦é—®ç­”æ•°æ®é›†` `å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°` `æ•°å­¦æŽ¨ç†èƒ½åŠ›` `æ¨¡åž‹å¾®è°ƒ` `å¯¹æŠ—æ ·æœ¬é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨å¯†ç å­¦ç­‰éœ€è¦æ·±åº¦æŽ¨ç†å’Œæ•°å­¦åˆ†æžçš„ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œç¼ºä¹åˆé€‚çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°å’Œè®­ç»ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé¦–ä¸ªå¤§è§„æ¨¡å¯†ç å­¦é—®ç­”æ•°æ®é›†CryptoQAï¼ŒåŒ…å«è¶…è¿‡ä¸¤ç™¾ä¸‡å¯¹é—®ç­”å’Œä¸Šä¸‹æ–‡å…ƒæ•°æ®ï¼Œç”¨äºŽæµ‹è¯•å’Œè®­ç»ƒæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºå‡†æµ‹è¯•15ä¸ªå…ˆè¿›å¤§è¯­è¨€æ¨¡åž‹ï¼Œæ­ç¤ºå…¶åœ¨å½¢å¼æŽ¨ç†å’Œç²¾ç¡®æ•°å­¦çŸ¥è¯†æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ï¼Œå¹¶é€šè¿‡å¾®è°ƒå±•ç¤ºæ€§èƒ½æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.

