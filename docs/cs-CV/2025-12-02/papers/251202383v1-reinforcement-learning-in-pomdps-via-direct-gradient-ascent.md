---
layout: default
title: Reinforcement Learning in POMDP's via Direct Gradient Ascent
---

# Reinforcement Learning in POMDP's via Direct Gradient Ascent

**arXiv**: [2512.02383v1](https://arxiv.org/abs/2512.02383) | [PDF](https://arxiv.org/pdf/2512.02383.pdf)

**ä½œè€…**: Jonathan Baxter, Peter L. Bartlett

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPOMDPç®—æ³•ï¼Œç”¨äºŽåœ¨éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ç›´æŽ¥ä¼˜åŒ–ç­–ç•¥æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ¢¯åº¦ä¸Šå‡` `ç­–ç•¥ä¼˜åŒ–` `å¹³å‡å¥–åŠ±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ç›´æŽ¥ä¼˜åŒ–ç­–ç•¥æ€§èƒ½çš„æ¢¯åº¦æ–¹æ³•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥GPOMDPç®—æ³•ï¼ŒåŸºäºŽå•æ ·æœ¬è·¯å¾„ä¼°è®¡å¹³å‡å¥–åŠ±æ¢¯åº¦ï¼Œå‚æ•°å°‘ä¸”æ— éœ€çŠ¶æ€çŸ¥è¯†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯æ˜Žç®—æ³•æ”¶æ•›æ€§ï¼Œå¹¶å±•ç¤ºæ¢¯åº¦ä¼°è®¡å¯ç”¨äºŽå…±è½­æ¢¯åº¦æ³•å¯»æ‰¾å±€éƒ¨æœ€ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper discusses theoretical and experimental aspects of gradient-based approaches to the direct optimization of policy performance in controlled POMDPs. We introduce GPOMDP, a REINFORCE-like algorithm for estimating an approximation to the gradient of the average reward as a function of the parameters of a stochastic policy. The algorithm's chief advantages are that it requires only a single sample path of the underlying Markov chain, it uses only one free parameter $Î²\in [0,1)$, which has a natural interpretation in terms of bias-variance trade-off, and it requires no knowledge of the underlying state. We prove convergence of GPOMDP and show how the gradient estimates produced by GPOMDP can be used in a conjugate-gradient procedure to find local optima of the average reward.

