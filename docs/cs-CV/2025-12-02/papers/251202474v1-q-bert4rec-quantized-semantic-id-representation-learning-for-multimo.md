---
layout: default
title: Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation
---

# Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation

**arXiv**: [2512.02474v1](https://arxiv.org/abs/2512.02474) | [PDF](https://arxiv.org/pdf/2512.02474.pdf)

**ä½œè€…**: Haofeng Huang, Ling Gai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-BERT4Recï¼Œé€šè¿‡é‡åŒ–è¯­ä¹‰IDè¡¨ç¤ºå­¦ä¹ è§£å†³å¤šæ¨¡æ€åºåˆ—æŽ¨èä¸­çš„æ³›åŒ–ä¸Žå¯è§£é‡Šæ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `åºåˆ—æŽ¨è` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­ä¹‰é‡åŒ–` `Transformeræ¨¡åž‹` `é¢„è®­ç»ƒç­–ç•¥` `æŽ¨èç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿåºåˆ—æŽ¨èæ–¹æ³•ä¾èµ–ç¦»æ•£ç‰©å“IDï¼Œç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œå¿½ç•¥å¤šæ¨¡æ€æ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–å¼±å’Œå¯è§£é‡Šæ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æž¶ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€è¯­ä¹‰æ³¨å…¥ã€è¯­ä¹‰é‡åŒ–å’Œå¤šæŽ©ç é¢„è®­ç»ƒï¼Œèžåˆæ–‡æœ¬ã€è§†è§‰å’Œç»“æž„ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨AmazonåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è¯­ä¹‰é‡åŒ–åœ¨å¤šæ¨¡æ€åºåˆ—æŽ¨èä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing.

