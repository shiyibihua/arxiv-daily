---
layout: default
title: PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding
---

# PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding

**arXiv**: [2512.02624v1](https://arxiv.org/abs/2512.02624) | [PDF](https://arxiv.org/pdf/2512.02624.pdf)

**ä½œè€…**: Zheng Huang, Xukai Liu, Tianyu Hu, Kai Zhang, Ye Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPPTBenchä»¥å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡åž‹åœ¨PowerPointå¸ƒå±€ä¸Žè®¾è®¡ç†è§£ä¸­çš„å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºå‡†` `å¸ƒå±€ç†è§£` `è§†è§‰ç»“æž„æŽ¨ç†` `å¹»ç¯ç‰‡ç”Ÿæˆ` `å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†å¿½è§†å¸ƒå±€ä¸­å¿ƒæŒ‘æˆ˜ï¼ŒPPTBenchå¡«è¡¥æ­¤ç©ºç™½ï¼Œæ¶µç›–æ£€æµ‹ã€ç†è§£ã€ä¿®æ”¹å’Œç”Ÿæˆå››ç±»ä»»åŠ¡
2. åŸºäºŽ958ä¸ªPPTXæ–‡ä»¶æž„å»º4,439ä¸ªæ ·æœ¬ï¼Œå®žéªŒæ­ç¤ºå½“å‰MLLMsåœ¨è¯­ä¹‰ç†è§£ä¸Žè§†è§‰å¸ƒå±€æŽ¨ç†é—´å­˜åœ¨æ˜¾è‘—å·®è·
3. æ¨¡åž‹èƒ½è§£é‡Šå†…å®¹ä½†æ— æ³•ç”Ÿæˆè¿žè´¯ç©ºé—´æŽ’åˆ—ï¼Œæ¡ˆä¾‹ç ”ç©¶æš´éœ²å¯¹é½é”™è¯¯å’Œå…ƒç´ é‡å ç­‰ç³»ç»Ÿå¸ƒå±€é—®é¢˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.

