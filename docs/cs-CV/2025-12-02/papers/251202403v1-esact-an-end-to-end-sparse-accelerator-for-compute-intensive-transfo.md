---
layout: default
title: ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity
---

# ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity

**arXiv**: [2512.02403v1](https://arxiv.org/abs/2512.02403) | [PDF](https://arxiv.org/pdf/2512.02403.pdf)

**ä½œè€…**: Hongxiang Liu, Zhifang Deng, Tong Pu, Shengli Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºESACTï¼Œä¸€ç§åŸºäºŽå±€éƒ¨ç›¸ä¼¼æ€§çš„ç«¯åˆ°ç«¯ç¨€ç–åŠ é€Ÿå™¨ï¼Œç”¨äºŽè®¡ç®—å¯†é›†åž‹Transformerã€‚**

**å…³é”®è¯**: `TransformeråŠ é€Ÿ` `ç¨€ç–è®¡ç®—` `å±€éƒ¨ç›¸ä¼¼æ€§` `ç¡¬ä»¶æž¶æž„` `èƒ½æ•ˆä¼˜åŒ–` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformerè®¡ç®—æˆæœ¬é«˜ï¼ŒçŽ°æœ‰åŠ é€Ÿå™¨å¤šä»…åˆ©ç”¨æ³¨æ„åŠ›å†…è¡Œç¨€ç–ï¼Œå¿½ç•¥è¡Œé—´ç¨€ç–æˆ–ä¾èµ–é«˜å¼€é”€å…¨å±€ç›¸ä¼¼æ€§ä¼°è®¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡SPLSæœºåˆ¶ï¼Œåˆ©ç”¨HLogé‡åŒ–é¢„æµ‹å±€éƒ¨æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œå®žçŽ°æ‰€æœ‰Transformerç»„ä»¶çš„ç«¯åˆ°ç«¯ç¨€ç–åŠ é€Ÿã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨26ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPLSå‡å°‘æ€»è®¡ç®—52.03%ï¼Œç²¾åº¦æŸå¤±å°äºŽ1%ï¼›ESACTç«¯åˆ°ç«¯èƒ½æ•ˆè¾¾3.29 TOPS/Wï¼Œæ³¨æ„åŠ›çº§èƒ½æ•ˆä¼˜äºŽSOTAã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformers, composed of QKV generation, attention computation, and FFNs,
>   have become the dominant model across various domains due to their outstanding performance.
>   However, their high computational cost hinders efficient hardware deployment.
>   Sparsity offers a promising solution,
>   yet most existing accelerators exploit only intra-row sparsity in attention,
>   while few consider inter-row sparsity.
>   Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
>   which diminishes the acceleration benefits of sparsity,
>   and typically apply sparsity to only one or two transformer components.
>   Through careful analysis of the attention distribution and computation flow,
>   we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
>   Motivated by this observation, we propose ESACT,
>   an end-to-end sparse accelerator for compute-intensive Transformers.
>   ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
>   which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
>   achieving efficient sparsity across all transformer components.
>   To support efficient hardware realization, we introduce three architectural innovations.
>   Experimental results on 26 benchmarks demonstrate that
>   SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
>   ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
>   and improves attention-level energy efficiency by 2.95x and 2.26x over
>   SOTA attention accelerators SpAtten and Sanger, respectively.

