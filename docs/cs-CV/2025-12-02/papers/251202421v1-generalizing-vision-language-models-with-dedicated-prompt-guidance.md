---
layout: default
title: Generalizing Vision-Language Models with Dedicated Prompt Guidance
---

# Generalizing Vision-Language Models with Dedicated Prompt Guidance

**arXiv**: [2512.02421v1](https://arxiv.org/abs/2512.02421) | [PDF](https://arxiv.org/pdf/2512.02421.pdf)

**ä½œè€…**: Xinyao Li, Yinjie Min, Hongbo Chen, Zhekai Du, Fengling Li, Jingjing Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGuiDGæ¡†æž¶ï¼Œé€šè¿‡ä¸“å®¶å¼•å¯¼æå‡è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é¢†åŸŸæ³›åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `é¢†åŸŸæ³›åŒ–` `æç¤ºè°ƒä¼˜` `è·¨æ¨¡æ€æ³¨æ„åŠ›` `ä¸“å®¶æ¨¡åž‹` `å¾®è°ƒä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹å¾®è°ƒåœ¨é¢†åŸŸç‰¹å¼‚æ€§å’Œæ³›åŒ–èƒ½åŠ›é—´å­˜åœ¨æƒè¡¡ï¼Œå½“å‰æ–¹æ³•å¯èƒ½æŸå®³æœªè§åŸŸçš„æ³›åŒ–èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç†è®ºåˆ†æžï¼Œé‡‡ç”¨ä¸¤æ­¥éª¤æ¡†æž¶ï¼Œå…ˆé€šè¿‡æç¤ºè°ƒä¼˜èŽ·å–æºåŸŸä¸“å®¶ï¼Œå†é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—è‡ªé€‚åº”é›†æˆä¸“å®¶æŒ‡å¯¼è§†è§‰ç¼–ç å™¨å¾®è°ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†é¢†åŸŸæ³›åŒ–åŸºå‡†å’Œæž„å»ºçš„ImageNet-DGæ•°æ®é›†ä¸Šï¼ŒGuiDGä¼˜äºŽçŽ°æœ‰å¾®è°ƒæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.

