---
layout: default
title: Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening
---

# Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening

**arXiv**: [2512.02643v1](https://arxiv.org/abs/2512.02643) | [PDF](https://arxiv.org/pdf/2512.02643.pdf)

**ä½œè€…**: Yongchuan Cui, Peng Liu, Yi Zeng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®é¢„è®­ç»ƒçš„ç­–ç•¥ï¼Œä»¥æå‡é¥æ„Ÿå›¾åƒèžåˆçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `é¥æ„Ÿå›¾åƒèžåˆ` `é›¶æ ·æœ¬æ³›åŒ–` `é¢„è®­ç»ƒç­–ç•¥` `ç©ºé—´-å…‰è°±å…ˆéªŒ` `è·¨åŸŸé€‚åº”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•å› çœŸå®žæ•°æ®æœ‰é™å’Œä¼ æ„Ÿå™¨å·®å¼‚ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›å·®
2. é€šè¿‡æ¨¡æ‹Ÿé€€åŒ–ä¸Žå¢žå¼ºæ“ä½œæž„å»ºæ•°æ®é›†ï¼Œé¢„è®­ç»ƒå­¦ä¹ é²æ£’çš„ç©ºé—´-å…‰è°±å…ˆéªŒ
3. åœ¨å¤šä¸ªå«æ˜Ÿæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œé¢„è®­ç»ƒæ¨¡åž‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­è¡¨çŽ°ä¼˜å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.

