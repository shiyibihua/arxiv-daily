---
layout: default
title: Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models
---

# Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models

**arXiv**: [2512.02636v1](https://arxiv.org/abs/2512.02636) | [PDF](https://arxiv.org/pdf/2512.02636.pdf)

**ä½œè€…**: Xinyue Ai, Yutong He, Albert Gu, Ruslan Salakhutdinov, J Zico Kolter, Nicholas Matthew Boffi, Max Simchowitz

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºF2D2æ¡†æž¶ï¼Œé€šè¿‡è”åˆè’¸é¦åŒæ—¶åŠ é€Ÿæµæ¨¡åž‹ä¸­çš„é‡‡æ ·å’Œä¼¼ç„¶è¯„ä¼°**

**å…³é”®è¯**: `æµæ¨¡åž‹` `ä¼¼ç„¶è¯„ä¼°` `è”åˆè’¸é¦` `è¿žç»­å½’ä¸€åŒ–æµ` `å°‘æ­¥é‡‡æ ·` `è‡ªå¼•å¯¼æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæµæ¨¡åž‹å’Œæ‰©æ•£æ¨¡åž‹éœ€æ•°ç™¾è‡³æ•°åƒæ¬¡è¯„ä¼°è®¡ç®—ä¼¼ç„¶ï¼ŒçŽ°æœ‰è’¸é¦æ–¹æ³•ç‰ºç‰²ä¼¼ç„¶å¯è®¡ç®—æ€§æˆ–ä»ä¾èµ–æ˜‚è´µç§¯åˆ†
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽè¿žç»­å½’ä¸€åŒ–æµä¸­é‡‡æ ·å’Œä¼¼ç„¶ODEå…±äº«é€Ÿåº¦åœºï¼Œç”¨å•ä¸€æ¨¡åž‹è”åˆè’¸é¦è½¨è¿¹å’Œç´¯ç§¯æ•£åº¦ï¼Œæ¨¡å—åŒ–ä¸”å…¼å®¹çŽ°æœ‰å°‘æ­¥é‡‡æ ·æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šF2D2å°†é‡‡æ ·å’Œä¼¼ç„¶è¯„ä¼°æ‰€éœ€è¯„ä¼°æ¬¡æ•°å‡å°‘ä¸¤ä¸ªæ•°é‡çº§ï¼Œä¿æŒé«˜æ ·æœ¬è´¨é‡ï¼Œå¹¶åº”ç”¨äºŽè½»é‡è‡ªå¼•å¯¼æ–¹æ³•æå‡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Log-likelihood evaluation enables important capabilities in generative models, including model comparison, certain fine-tuning objectives, and many downstream applications. Yet paradoxically, some of today's best generative models -- diffusion and flow-based models -- still require hundreds to thousands of neural function evaluations (NFEs) to compute a single likelihood. While recent distillation methods have successfully accelerated sampling to just a few steps, they achieve this at the cost of likelihood tractability: existing approaches either abandon likelihood computation entirely or still require expensive integration over full trajectories. We present fast flow joint distillation (F2D2), a framework that simultaneously reduces the number of NFEs required for both sampling and likelihood evaluation by two orders of magnitude. Our key insight is that in continuous normalizing flows, the coupled ODEs for sampling and likelihood are computed from a shared underlying velocity field, allowing us to jointly distill both the sampling trajectory and cumulative divergence using a single model. F2D2 is modular, compatible with existing flow-based few-step sampling models, and requires only an additional divergence prediction head. Experiments demonstrate F2D2's capability of achieving accurate log-likelihood with few-step evaluations while maintaining high sample quality, solving a long-standing computational bottleneck in flow-based generative models. As an application of our approach, we propose a lightweight self-guidance method that enables a 2-step MeanFlow model to outperform a 1024 step teacher model with only a single additional backward NFE.

