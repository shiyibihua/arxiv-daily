---
layout: default
title: SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots
---

# SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots

**arXiv**: [2512.02851v1](https://arxiv.org/abs/2512.02851) | [PDF](https://arxiv.org/pdf/2512.02851.pdf)

**ä½œè€…**: Iana Zhura, Sausar Karaf, Faryal Batool, Nipun Dhananjaya Weerakkodi Mudalige, Valerii Serpiva, Ali Alridha Abdulkarim, Aleksey Fedoseev, Didar Seyidov, Amjad Hajira, Dzmitry Tsetserukou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSwarmDiffusionä»¥è§£å†³å¼‚æž„æœºå™¨äººå¯¼èˆªä¸­è§†è§‰å¯é€šè¡Œæ€§ä¼°è®¡ä¸Žè½¨è¿¹ç”Ÿæˆçš„ç«¯åˆ°ç«¯ç»Ÿä¸€é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰å¯é€šè¡Œæ€§ä¼°è®¡` `ç«¯åˆ°ç«¯æ‰©æ•£æ¨¡åž‹` `å¼‚æž„æœºå™¨äººå¯¼èˆª` `è½¨è¿¹ç”Ÿæˆ` `æ— æç¤ºå­¦ä¹ ` `è½»é‡çº§æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLMæ–¹æ³•ä¾èµ–æ‰‹å·¥æç¤ºï¼Œæ³›åŒ–æ€§å·®ï¼Œä¸”ä»…è¾“å‡ºå¯é€šè¡Œå›¾ï¼Œè½¨è¿¹ç”Ÿæˆéœ€å¤–éƒ¨è§„åˆ’å™¨
2. SwarmDiffusionæ˜¯è½»é‡çº§æ‰©æ•£æ¨¡åž‹ï¼Œä»Žå•RGBå›¾åƒè”åˆé¢„æµ‹å¯é€šè¡Œæ€§å¹¶ç”Ÿæˆå¯è¡Œè½¨è¿¹ï¼Œæ— éœ€æ ‡æ³¨æˆ–è§„åˆ’å™¨è·¯å¾„
3. åœ¨å®¤å†…çŽ¯å¢ƒå’Œä¸¤ç§æœºå™¨äººå¹³å°ä¸Šå®žçŽ°80-100%å¯¼èˆªæˆåŠŸçŽ‡ï¼ŒæŽ¨ç†æ—¶é—´0.09ç§’ï¼Œä»…éœ€500æ ·æœ¬é€‚åº”æ–°æœºå™¨äºº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100\% navigation success and 0.09 s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.

