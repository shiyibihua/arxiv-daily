---
layout: default
title: See, Think, Learn: A Self-Taught Multimodal Reasoner
---

# See, Think, Learn: A Self-Taught Multimodal Reasoner

**arXiv**: [2512.02456v1](https://arxiv.org/abs/2512.02456) | [PDF](https://arxiv.org/pdf/2512.02456.pdf)

**ä½œè€…**: Sourabh Sharma, Sonam Gupta, Sadbhawna

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSee-Think-Learnè‡ªè®­ç»ƒæ¡†æž¶ä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹çš„å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `è‡ªè®­ç»ƒ` `å¤šæ¨¡æ€æŽ¨ç†` `ç»“æž„åŒ–æŽ¨ç†` `è´Ÿæ ·æœ¬å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æ„ŸçŸ¥ä¸ŽæŽ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”çŽ°æœ‰æ–¹æ³•ä¾èµ–é«˜æˆæœ¬æ ‡æ³¨æˆ–å¿½ç•¥æ„ŸçŸ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ç»“æž„åŒ–æŽ¨ç†æ¨¡æ¿ï¼Œå…ˆæå–è§†è§‰å±žæ€§å†æŒ‡å¯¼æŽ¨ç†ï¼Œé€šè¿‡è‡ªè®­ç»ƒå¾ªçŽ¯è”åˆä¼˜åŒ–æ„ŸçŸ¥ä¸ŽæŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªé¢†åŸŸå®žéªŒä¸­ä¼˜äºŽåŸºçº¿ï¼Œå®šæ€§åˆ†æžæ˜¾ç¤ºå…¶ç”Ÿæˆé«˜è´¨é‡æŽ¨ç†ä¾æ®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.

