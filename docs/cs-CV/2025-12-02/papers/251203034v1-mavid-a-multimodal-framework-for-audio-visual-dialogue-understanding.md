---
layout: default
title: MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation
---

# MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation

**arXiv**: [2512.03034v1](https://arxiv.org/abs/2512.03034) | [PDF](https://arxiv.org/pdf/2512.03034.pdf)

**ä½œè€…**: Youxin Pang, Jiajun Liu, Lingfeng Tan, Yong Zhang, Feng Gao, Xiang Deng, Zhuoliang Kang, Xiaoming Wei, Yebin Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAViDæ¡†æž¶ä»¥è§£å†³éŸ³é¢‘-è§†è§‰å¯¹è¯ç†è§£ä¸Žç”Ÿæˆä¸­çš„å¤šæ¨¡æ€èžåˆå’Œé•¿è§†é¢‘ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿ` `éŸ³é¢‘-è§†è§‰èžåˆ` `é•¿è§†é¢‘ç”Ÿæˆ` `è‡ªå›žå½’æ¨¡åž‹` `æ‰©æ•£æ¨¡åž‹` `å¯¹è¯ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥ç”Ÿæˆè‡ªç„¶é•¿è§†é¢‘å¯¹è¯ï¼Œéœ€æ•´åˆç†è§£ä¸Žç”Ÿæˆèƒ½åŠ›åŠéŸ³é¢‘-è§†é¢‘èžåˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨Conductor-Creatoræž¶æž„ï¼Œç»“åˆè‡ªå›žå½’å’Œæ‰©æ•£æ¨¡åž‹ï¼Œå¹¶å¼•å…¥æ–°èžåˆæ¨¡å—å¢žå¼ºå¤šæ¨¡æ€åŒæ­¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¡¨æ˜Žæ¡†æž¶èƒ½ç”Ÿæˆç”ŸåŠ¨ã€ä¸Šä¸‹æ–‡è¿žè´¯çš„é•¿å¯¹è¯äº¤äº’ï¼Œå‡†ç¡®ç†è§£ç”¨æˆ·å¤šæ¨¡æ€æŸ¥è¯¢ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.

