---
layout: default
title: CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning
---

# CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning

**arXiv**: [2512.02551v1](https://arxiv.org/abs/2512.02551) | [PDF](https://arxiv.org/pdf/2512.02551.pdf)

**ä½œè€…**: Songqiao Su, Xiaofei Sun, Xiaoya Li, Albert Wang, Jiwei Li, Chris Shum

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCUDA-L2ç³»ç»Ÿï¼Œç»“åˆå¤§è¯­è¨€æ¨¡åž‹ä¸Žå¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ä¼˜åŒ–åŠç²¾åº¦çŸ©é˜µä¹˜æ³•CUDAå†…æ ¸ï¼Œè¶…è¶Šä¸»æµåº“æ€§èƒ½ã€‚**

**å…³é”®è¯**: `çŸ©é˜µä¹˜æ³•ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹` `CUDAå†…æ ¸` `åŠç²¾åº¦è®¡ç®—` `è‡ªåŠ¨åŒ–è°ƒä¼˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨ä¼˜åŒ–åŠç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼ˆHGEMMï¼‰CUDAå†…æ ¸ä»¥æå‡æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡åž‹å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥CUDAæ‰§è¡Œé€Ÿåº¦ä¸ºå¥–åŠ±ï¼Œè‡ªåŠ¨æŽ¢ç´¢é…ç½®ç©ºé—´ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ç¦»çº¿ä¸ŽæœåŠ¡å™¨æ¨¡å¼ä¸‹ï¼Œå¹³å‡æ€§èƒ½è¶…è¶Štorch.matmulã€cuBLASå’ŒcuBLASLtç­‰åŸºå‡†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\it cuBLAS}, {\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\% over {\it torch.matmul} on average; +19.2\% over {\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\% over {\it cuBLASLt-heuristic}, which queries {\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\% over the most competitive {\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\%, +26.0\%, +22.4\%, and +15.9\% for {\it torch.matmul}, {\it cuBLAS}, {\it cuBLASLt-heuristic}, and {\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2

