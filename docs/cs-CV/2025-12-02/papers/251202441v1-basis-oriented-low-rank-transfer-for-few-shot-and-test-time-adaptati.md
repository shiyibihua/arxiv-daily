---
layout: default
title: Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation
---

# Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation

**arXiv**: [2512.02441v1](https://arxiv.org/abs/2512.02441) | [PDF](https://arxiv.org/pdf/2512.02441.pdf)

**ä½œè€…**: Junghwan Park, Woojin Cho, Junhyuk Heo, Darongsae Kwon, Kookjin Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBOLTæ¡†æž¶ï¼Œé€šè¿‡æå–æ­£äº¤ä»»åŠ¡è°±åŸºå®žçŽ°å°‘æ ·æœ¬å’Œæµ‹è¯•æ—¶é€‚åº”**

**å…³é”®è¯**: `å°‘æ ·æœ¬é€‚åº”` `æµ‹è¯•æ—¶é€‚åº”` `ä½Žç§©è¿ç§»` `æ­£äº¤è°±åŸº` `å‚æ•°é«˜æ•ˆå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨æ•°æ®ä¸Žè®¡ç®—å—é™ä¸‹ï¼Œå¦‚ä½•é«˜æ•ˆè¿ç§»é¢„è®­ç»ƒæ¨¡åž‹åˆ°æ–°ä»»åŠ¡ï¼Œé¿å…å…ƒå­¦ä¹ çš„é«˜æˆæœ¬ä¸Žä¸ç¨³å®šæ€§
2. æ–¹æ³•è¦ç‚¹ï¼šç¦»çº¿é˜¶æ®µä»Žå¤šä»»åŠ¡å‘é‡æå–æ­£äº¤è°±åŸºï¼Œåœ¨çº¿é˜¶æ®µå†»ç»“åŸºå¹¶è®­ç»ƒå°‘é‡å¯¹è§’ç³»æ•°è¿›è¡Œä½Žç§©æ›´æ–°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å®žéªŒä¸­ï¼ŒBOLTæä¾›å¼ºåˆå§‹åŒ–ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ€§èƒ½ä¼˜äºŽå¸¸è§PEFTåŸºçº¿å’Œå…ƒå­¦ä¹ åˆå§‹åŒ–

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.

