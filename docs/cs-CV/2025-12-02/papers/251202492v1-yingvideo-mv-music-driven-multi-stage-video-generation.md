---
layout: default
title: YingVideo-MV: Music-Driven Multi-Stage Video Generation
---

# YingVideo-MV: Music-Driven Multi-Stage Video Generation

**arXiv**: [2512.02492v1](https://arxiv.org/abs/2512.02492) | [PDF](https://arxiv.org/pdf/2512.02492.pdf)

**ä½œè€…**: Jiahui Chen, Weida Wang, Runhua Shi, Huan Yang, Chaofan Ding, Zihao Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYingVideo-MVæ¡†æž¶ä»¥è§£å†³éŸ³ä¹é©±åŠ¨é•¿è§†é¢‘ç”Ÿæˆä¸­ç›¸æœºè¿åŠ¨æŽ§åˆ¶ä¸Žè¿žè´¯æ€§é—®é¢˜**

**å…³é”®è¯**: `éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆ` `é•¿è§†é¢‘ç”Ÿæˆ` `ç›¸æœºè¿åŠ¨æŽ§åˆ¶` `æ‰©æ•£æ¨¡åž‹` `éŸ³é¢‘-è§†è§‰åŒæ­¥` `å¤šé˜¶æ®µæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é•¿è§†é¢‘ç”Ÿæˆæ–¹æ³•ç¼ºä¹æ˜¾å¼ç›¸æœºè¿åŠ¨æŽ§åˆ¶ï¼ŒéŸ³ä¹è¡¨æ¼”è§†é¢‘ç”Ÿæˆæœªå……åˆ†æŽ¢ç´¢
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆéŸ³é¢‘è¯­ä¹‰åˆ†æžã€å¯è§£é‡Šé•œå¤´è§„åˆ’æ¨¡å—ã€ç›¸æœºé€‚é…å™¨æ¨¡å—å’Œæ—¶é—´æ„ŸçŸ¥åŠ¨æ€çª—å£ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨éŸ³ä¹è§†é¢‘ç”Ÿæˆä¸­å®žçŽ°é«˜è´¨é‡ã€è¿žè´¯ä¸”éŸ³ä¹-åŠ¨ä½œ-ç›¸æœºåŒæ­¥çš„ç»“æžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .

