---
layout: default
title: DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
---

# DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images

**arXiv**: [2512.03004v1](https://arxiv.org/abs/2512.03004) | [PDF](https://arxiv.org/pdf/2512.03004.pdf)

**ä½œè€…**: Xiaoxue Chen, Ziyi Xiong, Yuantao Chen, Gen Li, Nan Wang, Hongcheng Luo, Long Chen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Hongyang Li, Ya-Qin Zhang, Hao Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDGGTæ¡†æž¶ï¼Œä»Žæ— ä½å§¿å›¾åƒå®žçŽ°å‰é¦ˆå¼åŠ¨æ€é©¾é©¶åœºæ™¯4Dé‡å»ºã€‚**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯é‡å»º` `æ— ä½å§¿é‡å»º` `4Dé‡å»º` `é©¾é©¶åœºæ™¯` `å‰é¦ˆæ¨¡åž‹` `é«˜æ–¯å›¾é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŠ¨æ€é©¾é©¶åœºæ™¯é‡å»ºæ–¹æ³•ä¾èµ–ä½å§¿è¾“å…¥æˆ–é€åœºæ™¯ä¼˜åŒ–ï¼Œå¯¼è‡´é€Ÿåº¦æ…¢ã€çµæ´»æ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè”åˆé¢„æµ‹3Dé«˜æ–¯å›¾å’Œç›¸æœºå‚æ•°ï¼Œé€šè¿‡åŠ¨æ€å¤´å’Œå¯¿å‘½å¤´è§£è€¦åŠ¨æ€å¹¶ä¿æŒæ—¶åºä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå¤§è§„æ¨¡é©¾é©¶æ•°æ®é›†ä¸Šå®žçŽ°SOTAæ€§èƒ½å’Œé€Ÿåº¦ï¼Œæ”¯æŒé›¶æ ·æœ¬è·¨æ•°æ®é›†è¿ç§»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.

