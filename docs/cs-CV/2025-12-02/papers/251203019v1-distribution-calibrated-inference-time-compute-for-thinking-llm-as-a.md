---
layout: default
title: Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge
---

# Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge

**arXiv**: [2512.03019v1](https://arxiv.org/abs/2512.03019) | [PDF](https://arxiv.org/pdf/2512.03019.pdf)

**ä½œè€…**: Hamid Dadkhahi, Firas Trabelsi, Parker Riley, Juraj Juraska, Mehdi Mirzazadeh

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒæ ¡å‡†çš„æŽ¨ç†æ—¶è®¡ç®—èšåˆæ–¹æ³•ï¼Œä»¥æå‡å¤§è¯­è¨€æ¨¡åž‹ä½œä¸ºè¯„åˆ¤è€…çš„å¯é æ€§**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹è¯„åˆ¤` `æŽ¨ç†æ—¶è®¡ç®—` `åˆ†å¸ƒæ ¡å‡†` `æˆå¯¹åå¥½è¯„ä¼°` `Bradley-Terryæ¨¡åž‹` `å™ªå£°èšåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹ä½œä¸ºæˆå¯¹åå¥½è¯„åˆ¤è€…æ—¶ï¼Œå•æ ·æœ¬å™ªå£°å¤§ä¸”å¸¸è§èšåˆè§„åˆ™åœ¨å…è®¸å¹³å±€æ—¶ä¸ä¸€è‡´
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽBradley-Terry-Davidsonæ¨¡åž‹ï¼Œåˆ©ç”¨æžæ€§ï¼ˆéžå¹³å±€è¾¹é™…ï¼‰å’Œå†³æ–­æ€§ï¼ˆéžå¹³å±€çŽ‡ï¼‰æ ¡å‡†è¯„åˆ†åˆ†å¸ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­é™ä½ŽMAEã€æé«˜æˆå¯¹å‡†ç¡®çŽ‡ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäººç±»è¯„åˆ¤è€…å…±è¯†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.

