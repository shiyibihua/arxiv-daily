---
layout: default
title: Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training
---

# Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training

**arXiv**: [2512.02652v1](https://arxiv.org/abs/2512.02652) | [PDF](https://arxiv.org/pdf/2512.02652.pdf)

**ä½œè€…**: Hong-Jie You, Jie-Jing Shao, Xiao-Wen Yang, Lin-Han Jia, Lan-Zhe Guo, Yu-Feng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPianist Transformerï¼Œé€šè¿‡å¯æ‰©å±•çš„è‡ªç›‘ç£é¢„è®­ç»ƒè§£å†³é’¢ç´è¡¨çŽ°åŠ›æ¸²æŸ“çš„æ•°æ®å’Œæ¨¡åž‹è§„æ¨¡é™åˆ¶ã€‚**

**å…³é”®è¯**: `é’¢ç´è¡¨çŽ°åŠ›æ¸²æŸ“` `è‡ªç›‘ç£é¢„è®­ç»ƒ` `MIDIæ•°æ®è¡¨ç¤º` `ä¸å¯¹ç§°æž¶æž„` `å¯æ‰©å±•æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–å°è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†æ•°æ®å’Œæ¨¡åž‹è§„æ¨¡æ‰©å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç»Ÿä¸€MIDIè¡¨ç¤ºå’Œä¸å¯¹ç§°æž¶æž„ï¼Œå®žçŽ°è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæå‡æ¸²æŸ“æ•ˆçŽ‡å’Œè´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨10B tokené¢„è®­ç»ƒåŽï¼Œæ¨¡åž‹è¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œå®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è¯„åˆ†æŽ¥è¿‘äººç±»æ°´å¹³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in the music domain.

