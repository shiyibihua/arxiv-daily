---
layout: default
title: Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks
---

# Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks

**arXiv**: [2512.03014v1](https://arxiv.org/abs/2512.03014) | [PDF](https://arxiv.org/pdf/2512.03014.pdf)

**ä½œè€…**: Matthew Dutson, Nathan Labiosa, Yin Li, Mohit Gupta

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨ç¨³å®šæ€§é€‚é…å™¨ä»¥è§£å†³å›¾åƒç½‘ç»œåœ¨è§†é¢‘å¤„ç†ä¸­çš„æ—¶åºä¸ä¸€è‡´é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘ç¨³å®šæ€§` `é€šç”¨é€‚é…å™¨` `æ—¶åºä¸€è‡´æ€§` `å›¾åƒå™ªå£°é²æ£’æ€§` `å†»ç»“ç½‘ç»œè®­ç»ƒ` `ç²¾åº¦-ç¨³å®šæ€§-é²æ£’æ€§æŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¸§åŸºç½‘ç»œåœ¨è§†é¢‘åºåˆ—ä¸­è¾“å‡ºæ—¶åºä¸ä¸€è‡´ï¼Œå¦‚é—ªçƒï¼Œè¾“å…¥å«æ—¶å˜å™ªå£°æ—¶åŠ å‰§
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å¯æ’å…¥ä»»æ„æž¶æž„çš„ç¨³å®šæ€§é€‚é…å™¨ï¼ŒåŸºäºŽå†»ç»“åŸºç½‘ç»œè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œæå‡ºç²¾åº¦-ç¨³å®šæ€§-é²æ£’æ€§æŸå¤±ç»Ÿä¸€æ¡†æž¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŽ»å™ªã€å›¾åƒå¢žå¼ºã€æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­éªŒè¯ï¼Œæå‡æ—¶åºç¨³å®šæ€§å’ŒæŠ—å›¾åƒå™ªå£°é²æ£’æ€§ï¼Œä¿æŒæˆ–æ”¹è¿›é¢„æµ‹è´¨é‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.

