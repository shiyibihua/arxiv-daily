---
layout: default
title: InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration
---

# InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration

**arXiv**: [2512.02981v1](https://arxiv.org/abs/2512.02981) | [PDF](https://arxiv.org/pdf/2512.02981.pdf)

**ä½œè€…**: Zhongyu Yang, Yingfang Yuan, Xuanming Jiang, Baoyi An, Wei Pang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInExæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ä¸­çš„å¹»è§‰é—®é¢˜**

**å…³é”®è¯**: `å¹»è§‰ç¼“è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¤šæ™ºèƒ½ä½“åä½œ` `å†…çœæŽ¨ç†` `è·¨æ¨¡æ€éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¹»è§‰æ˜¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å¯é æ€§çš„å…³é”®æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æˆ–æœªå……åˆ†åˆ©ç”¨è‡ªä¸»ç¼“è§£èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽäººç±»è®¤çŸ¥èŒƒå¼ï¼Œé€šè¿‡å†…çœæŽ¨ç†å’Œè·¨æ¨¡æ€å¤šæ™ºèƒ½ä½“åä½œï¼Œå®žçŽ°æ— è®­ç»ƒè‡ªä¸»å¹»è§‰ç¼“è§£ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é€šç”¨å’Œå¹»è§‰åŸºå‡†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡4%-27%ï¼Œå±•çŽ°å¼ºé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.

