---
layout: default
title: Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling
---

# Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling

**arXiv**: [2512.03044v1](https://arxiv.org/abs/2512.03044) | [PDF](https://arxiv.org/pdf/2512.03044.pdf)

**ä½œè€…**: Yueru Jia, Jiaming Liu, Shengbang Liu, Rui Zhou, Wanhe Yu, Yuyang Yan, Xiaowei Chi, Yandong Guo, Boxin Shi, Shanghang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo2Actæ¡†æž¶ï¼Œé€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡åž‹ä¸Žæ‰©æ•£å˜æ¢å™¨çš„åŒç³»ç»Ÿè®¾è®¡ï¼Œå¢žå¼ºæœºå™¨äººåŠ¨ä½œå­¦ä¹ çš„ç©ºé—´ä¸Žè¿åŠ¨æ„ŸçŸ¥èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `æœºå™¨äººç­–ç•¥å­¦ä¹ ` `è¿åŠ¨æ„ŸçŸ¥è¡¨ç¤º` `æ‰©æ•£å˜æ¢å™¨` `åŒç³»ç»Ÿè®¾è®¡` `åŠ¨ä½œç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å¿½è§†è§†é¢‘æ‰©æ•£æ¨¡åž‹ä¸­è·¨å¸§çš„è¿žè´¯è¿åŠ¨è¡¨ç¤ºï¼Œå¯¼è‡´æœºå™¨äººç­–ç•¥å­¦ä¹ å—é™ã€‚
2. Video2Actæå–å‰æ™¯è¾¹ç•Œå’Œå¸§é—´è¿åŠ¨å˜åŒ–ï¼Œä½œä¸ºæ‰©æ•£å˜æ¢å™¨åŠ¨ä½œå¤´çš„æ¡ä»¶è¾“å…¥ï¼Œå®žçŽ°å¼‚æ­¥åŒç³»ç»Ÿåä½œã€‚
3. åœ¨ä»¿çœŸå’ŒçœŸå®žä»»åŠ¡ä¸­ï¼Œå¹³å‡æˆåŠŸçŽ‡åˆ†åˆ«æå‡7.7%å’Œ21.7%ï¼Œå±•çŽ°å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.

