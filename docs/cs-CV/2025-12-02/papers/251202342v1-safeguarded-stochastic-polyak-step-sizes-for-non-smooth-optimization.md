---
layout: default
title: Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients
---

# Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients

**arXiv**: [2512.02342v1](https://arxiv.org/abs/2512.02342) | [PDF](https://arxiv.org/pdf/2512.02342.pdf)

**ä½œè€…**: Dimitris Oikonomou, Nicolas Loizou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSafeguarded SPSä»¥è§£å†³éžå…‰æ»‘ä¼˜åŒ–ä¸­éšæœºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜**

**å…³é”®è¯**: `éžå…‰æ»‘ä¼˜åŒ–` `éšæœºæ¢¯åº¦ä¸‹é™` `è‡ªé€‚åº”æ­¥é•¿` `æ·±åº¦ç¥žç»ç½‘ç»œè®­ç»ƒ` `æ”¶æ•›åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šéšæœºPolyakæ­¥é•¿åœ¨éžå…‰æ»‘ä¼˜åŒ–ä¸­ä¾èµ–å¼ºå‡è®¾æˆ–æœ€ä¼˜è§£çŸ¥è¯†ï¼Œé™åˆ¶åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Safeguarded SPSï¼Œç»“åˆåŠ¨é‡ï¼Œæä¾›æ— éœ€å¼ºå‡è®¾çš„æ”¶æ•›ä¿è¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å‡¸åŸºå‡†å’Œæ·±åº¦ç¥žç»ç½‘ç»œä¸ŠåŠ é€Ÿæ”¶æ•›ã€é™ä½Žæ–¹å·®ï¼Œä¼˜äºŽçŽ°æœ‰è‡ªé€‚åº”æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. Comprehensive experiments on convex benchmarks and deep neural networks corroborate our theory: the proposed step size accelerates convergence, reduces variance, and consistently outperforms existing adaptive baselines. Finally, in the context of deep neural network training, our method demonstrates robust performance by addressing the vanishing gradient problem.

