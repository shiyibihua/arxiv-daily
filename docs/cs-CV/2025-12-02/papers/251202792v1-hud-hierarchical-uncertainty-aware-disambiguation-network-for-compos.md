---
layout: default
title: HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval
---

# HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval

**arXiv**: [2512.02792v1](https://arxiv.org/abs/2512.02792) | [PDF](https://arxiv.org/pdf/2512.02792.pdf)

**ä½œè€…**: Zhiwei Chen, Yupeng Hu, Zixu Li, Zhiheng Fu, Haokun Wen, Weili Guan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHUDç½‘ç»œä»¥è§£å†³ç»„åˆè§†é¢‘æ£€ç´¢ä¸­çš„æ¨¡æ€ä¿¡æ¯å¯†åº¦å·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `ç»„åˆè§†é¢‘æ£€ç´¢` `å¤šæ¨¡æ€æŸ¥è¯¢` `ä¸ç¡®å®šæ€§å»ºæ¨¡` `è¯­ä¹‰å¯¹é½` `è·¨æ¨¡æ€äº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘ä¸Žæ–‡æœ¬æ¨¡æ€ä¿¡æ¯å¯†åº¦å·®å¼‚å¯¼è‡´ä¿®æ”¹ä¸»é¢˜æŒ‡ä»£æ¨¡ç³Šå’Œè¯­ä¹‰ç»†èŠ‚å…³æ³¨ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ•´ä½“ä»£è¯æ¶ˆæ­§ã€åŽŸå­ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œæ•´ä½“åˆ°åŽŸå­å¯¹é½å¢žå¼ºå¤šæ¨¡æ€æŸ¥è¯¢ç†è§£
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CVRå’ŒCIRä»»åŠ¡ä¸­å®žçŽ°SOTAæ€§èƒ½ï¼Œä»£ç å·²å¼€æº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.

