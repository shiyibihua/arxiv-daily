---
layout: default
title: QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine
---

# QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.07046" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.07046v1</a>
  <a href="https://arxiv.org/pdf/2506.07046.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.07046v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.07046v1', 'QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anushka Jha, Tanushree Dewangan, Mukul Lokhande, Santosh Kumar Vishvakarma

**åˆ†ç±»**: cs.AR, cs.CV, cs.RO, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQForce-RLä»¥è§£å†³FPGAä¸Šå¼ºåŒ–å­¦ä¹ è®¡ç®—èµ„æºæ¶ˆè€—é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `FPGA` `é‡åŒ–æŠ€æœ¯` `ç¡¬ä»¶åŠ é€Ÿ` `èµ„æºä¼˜åŒ–` `è½»é‡çº§æ¶æ„` `æ™ºèƒ½å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰FPGAä¸Šå¼ºåŒ–å­¦ä¹ çš„éƒ¨ç½²æˆæœ¬é«˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é«˜è´¨é‡å›¾åƒæ—¶ï¼Œè®¡ç®—èµ„æºæ¶ˆè€—æ˜¾è‘—ã€‚
2. QForce-RLé€šè¿‡é‡åŒ–æŠ€æœ¯å’Œè½»é‡çº§æ¶æ„ï¼Œæ—¨åœ¨æå‡ååé‡å¹¶é™ä½èƒ½è€—ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ç¨³å®šã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQForce-RLåœ¨æ€§èƒ½ä¸Šæå‡äº†2.3å€ï¼ŒFPSæå‡äº†2.6å€ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åºåˆ—å†³ç­–å’ŒåŠ¨æ€ç¯å¢ƒæ§åˆ¶ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨FPGAéƒ¨ç½²ä¸­é¢ä¸´é«˜èµ„æºæ¶ˆè€—å’Œè®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºQForce-RLï¼Œé€šè¿‡é‡åŒ–æŠ€æœ¯æå‡ååé‡å¹¶é™ä½èƒ½è€—ï¼Œæ„å»ºè½»é‡çº§çš„RLæ¶æ„ï¼Œä¸”æ€§èƒ½æŸå¤±ä¸æ˜¾è‘—ã€‚QForce-RLåˆ©ç”¨E2HRLå‡å°‘RLåŠ¨ä½œä»¥å­¦ä¹ æ‰€éœ€ç­–ç•¥ï¼Œå¹¶ç»“åˆQuaRLå®ç°åŸºäºé‡åŒ–çš„SIMDç¡¬ä»¶åŠ é€Ÿã€‚æˆ‘ä»¬å¯¹ä¸åŒRLç¯å¢ƒè¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œå¼ºè°ƒæ¨¡å‹å¤§å°ã€å‚æ•°å’ŒåŠ é€Ÿè®¡ç®—æ“ä½œã€‚è¯¥æ¶æ„å¯æ‰©å±•è‡³èµ„æºå—é™è®¾å¤‡ï¼Œæä¾›å‚æ•°åŒ–çš„é«˜æ•ˆéƒ¨ç½²ï¼Œçµæ´»è°ƒæ•´å»¶è¿Ÿã€ååé‡ã€åŠŸè€—å’Œèƒ½æ•ˆã€‚QForce-RLåœ¨æ€§èƒ½ä¸Šæå‡äº†2.3å€ï¼ŒFPSæå‡äº†2.6å€ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯è¡¨ç°æ›´ä½³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³FPGAä¸Šå¼ºåŒ–å­¦ä¹ è®¡ç®—èµ„æºæ¶ˆè€—é«˜çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒé«˜è´¨é‡å›¾åƒçš„ä»£ç†æ—¶é¢ä¸´å¤§é‡è®¡ç®—å’Œèµ„æºæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQForce-RLé€šè¿‡é‡åŒ–æŠ€æœ¯ä¼˜åŒ–è®¡ç®—ï¼Œæå‡ååé‡å¹¶é™ä½èƒ½è€—ï¼Œæ„å»ºè½»é‡çº§çš„å¼ºåŒ–å­¦ä¹ æ¶æ„ï¼Œç¡®ä¿æ€§èƒ½æŸå¤±æœ€å°åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQForce-RLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é‡åŒ–æ¨¡å—ã€E2HRLç­–ç•¥å­¦ä¹ æ¨¡å—å’ŒQuaRLç¡¬ä»¶åŠ é€Ÿæ¨¡å—ï¼ŒååŒå·¥ä½œä»¥å®ç°é«˜æ•ˆçš„RLè®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šQForce-RLçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†E2HRLå’ŒQuaRLï¼Œåˆ©ç”¨é‡åŒ–æŠ€æœ¯å®ç°åŸºäºSIMDçš„ç¡¬ä»¶åŠ é€Ÿï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ¨¡å‹å‚æ•°ç»è¿‡ç²¾ç»†è°ƒæ•´ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œä¼˜åŒ–çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„é«˜æ•ˆè¿è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQForce-RLåœ¨æ€§èƒ½ä¸Šå®ç°äº†2.3å€çš„æå‡ï¼Œä¸”åœ¨å¸§ç‡ï¼ˆFPSï¼‰æ–¹é¢æå‡äº†2.6å€ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨FPGAä¼˜åŒ–å¼ºåŒ–å­¦ä¹ è®¡ç®—ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

QForce-RLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰éœ€è¦å®æ—¶å†³ç­–çš„åœºæ™¯ã€‚å…¶é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›å’Œçµæ´»çš„éƒ¨ç½²æ–¹å¼ä½¿å…¶é€‚ç”¨äºèµ„æºå—é™çš„åµŒå…¥å¼è®¾å¤‡ï¼Œæ¨åŠ¨äº†è¾¹ç¼˜è®¡ç®—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has outperformed other counterparts in sequential decision-making and dynamic environment control. However, FPGA deployment is significantly resource-expensive, as associated with large number of computations in training agents with high-quality images and possess new challenges. In this work, we propose QForce-RL takes benefits of quantization to enhance throughput and reduce energy footprint with light-weight RL architecture, without significant performance degradation. QForce-RL takes advantages from E2HRL to reduce overall RL actions to learn desired policy and QuaRL for quantization based SIMD for hardware acceleration. We have also provided detailed analysis for different RL environments, with emphasis on model size, parameters, and accelerated compute ops. The architecture is scalable for resource-constrained devices and provide parametrized efficient deployment with flexibility in latency, throughput, power, and energy efficiency. The proposed QForce-RL provides performance enhancement up to 2.3x and better FPS - 2.6x compared to SoTA works.

