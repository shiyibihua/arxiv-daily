---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-10
---

# cs.CVï¼ˆ2025-09-10ï¼‰

ğŸ“Š å…± **27** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250909014v1-coco-urdu-a-large-scale-urdu-image-caption-dataset-with-multimodal-q.html">COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation</a></td>
  <td>COCO-Urduï¼šæ„å»ºå¤§è§„æ¨¡ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°æ•°æ®é›†ï¼Œå¹¶æå‡ºå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09014v1" data-paper-url="./papers/250909014v1-coco-urdu-a-large-scale-urdu-image-caption-dataset-with-multimodal-q.html" onclick="toggleFavorite(this, '2509.09014v1', 'COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250908777v1-calibrating-mllm-as-a-judge-via-multimodal-bayesian-prompt-ensembles.html">Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles</a></td>
  <td>æå‡ºMMBæ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€è´å¶æ–¯æç¤ºé›†æˆæ ¡å‡†MLLMåœ¨æ–‡å›¾ç”Ÿæˆè¯„åˆ¤ä¸­çš„åå·®ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08777v1" data-paper-url="./papers/250908777v1-calibrating-mllm-as-a-judge-via-multimodal-bayesian-prompt-ensembles.html" onclick="toggleFavorite(this, '2509.08777v1', 'Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250908570v1-vision-language-semantic-aggregation-leveraging-foundation-model-for.html">Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</a></td>
  <td>æå‡ºåŸºäºEMèšåˆå’Œæ–‡æœ¬å¼•å¯¼è§£ç çš„è§†è§‰-è¯­è¨€è¯­ä¹‰èšåˆæ–¹æ³•ï¼Œæå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ³›åŒ–æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08570v1" data-paper-url="./papers/250908570v1-vision-language-semantic-aggregation-leveraging-foundation-model-for.html" onclick="toggleFavorite(this, '2509.08570v1', 'Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250909730v1-mits-a-large-scale-multimodal-benchmark-dataset-for-intelligent-traf.html">MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance</a></td>
  <td>æå‡ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ™ºèƒ½äº¤é€šç›‘æ§æ•°æ®é›†MITSï¼Œæå‡LMMåœ¨ITSé¢†åŸŸçš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09730v1" data-paper-url="./papers/250909730v1-mits-a-large-scale-multimodal-benchmark-dataset-for-intelligent-traf.html" onclick="toggleFavorite(this, '2509.09730v1', 'MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250908303v1-an-open-benchmark-dataset-for-geoai-foundation-models-for-oil-palm-m.html">An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia</a></td>
  <td>å‘å¸ƒå°å°¼æ²¹æ£•æ¦ˆæµ‹ç»˜GeoAIåŸºç¡€æ¨¡å‹å¼€æ”¾åŸºå‡†æ•°æ®é›†ï¼ŒåŠ©åŠ›å¯æŒç»­å‘å±•ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">PaLM-E</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08303v1" data-paper-url="./papers/250908303v1-an-open-benchmark-dataset-for-geoai-foundation-models-for-oil-palm-m.html" onclick="toggleFavorite(this, '2509.08303v1', 'An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250908897v1-recurrence-meets-transformers-for-universal-multimodal-retrieval.html">Recurrence Meets Transformers for Universal Multimodal Retrieval</a></td>
  <td>æå‡ºReT-2ï¼Œä¸€ç§æ”¯æŒå¤šæ¨¡æ€æŸ¥è¯¢çš„é€šç”¨å¤šæ¨¡æ€æ£€ç´¢æ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08897v1" data-paper-url="./papers/250908897v1-recurrence-meets-transformers-for-universal-multimodal-retrieval.html" onclick="toggleFavorite(this, '2509.08897v1', 'Recurrence Meets Transformers for Universal Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250908338v1-retrieval-augmented-vlms-for-multimodal-melanoma-diagnosis.html">Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis</a></td>
  <td>æå‡ºæ£€ç´¢å¢å¼ºçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæå‡å¤šæ¨¡æ€é»‘è‰²ç´ ç˜¤è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08338v1" data-paper-url="./papers/250908338v1-retrieval-augmented-vlms-for-multimodal-melanoma-diagnosis.html" onclick="toggleFavorite(this, '2509.08338v1', 'Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250909721v1-a-multimodal-rag-framework-for-housing-damage-assessment-collaborati.html">A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval</a></td>
  <td>æå‡ºå¤šæ¨¡æ€RAGæ¡†æ¶ï¼Œç”¨äºç¾åæˆ¿å±‹æŸä¼¤è¯„ä¼°ï¼ŒååŒä¼˜åŒ–å›¾åƒç¼–ç å’Œç­–ç•¥å‘é‡æ£€ç´¢ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09721v1" data-paper-url="./papers/250909721v1-a-multimodal-rag-framework-for-housing-damage-assessment-collaborati.html" onclick="toggleFavorite(this, '2509.09721v1', 'A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250908715v1-bcqlm-efficient-vision-language-understanding-with-distilled-q-gated.html">BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion</a></td>
  <td>æå‡ºåŸºäºBreezeCLIPçš„BcQLMè½»é‡çº§MLLMæ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆè§†è§‰è¯­è¨€ç†è§£ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08715v1" data-paper-url="./papers/250908715v1-bcqlm-efficient-vision-language-understanding-with-distilled-q-gated.html" onclick="toggleFavorite(this, '2509.08715v1', 'BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250908621v1-adsqa-towards-advertisement-video-understanding.html">AdsQA: Towards Advertisement Video Understanding</a></td>
  <td>æå‡ºAdsQAå¹¿å‘Šè§†é¢‘ç†è§£åŸºå‡†ï¼Œå¹¶è®¾è®¡ReAd-Ræ¨¡å‹æå‡LLMåœ¨å¹¿å‘Šé¢†åŸŸçš„åº”ç”¨èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08621v1" data-paper-url="./papers/250908621v1-adsqa-towards-advertisement-video-understanding.html" onclick="toggleFavorite(this, '2509.08621v1', 'AdsQA: Towards Advertisement Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250908910v1-promptguard-an-orchestrated-prompting-framework-for-principled-synth.html">PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability</a></td>
  <td>PromptGuardï¼šé’ˆå¯¹å¼±åŠ¿ç¾¤ä½“ï¼Œé€šè¿‡ç¼–æ’å¼Promptingæ¡†æ¶æå‡LLMç”Ÿæˆæ–‡æœ¬çš„å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œå¯æ§æ€§</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08910v1" data-paper-url="./papers/250908910v1-promptguard-an-orchestrated-prompting-framework-for-principled-synth.html" onclick="toggleFavorite(this, '2509.08910v1', 'PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250908458v2-first-order-state-space-model-for-lightweight-image-super-resolution.html">First-order State Space Model for Lightweight Image Super-resolution</a></td>
  <td>æå‡ºä¸€é˜¶çŠ¶æ€ç©ºé—´æ¨¡å‹FSSMï¼Œç”¨äºè½»é‡çº§å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ï¼Œæ— éœ€é¢å¤–å‚æ•°å³å¯æå‡æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08458v2" data-paper-url="./papers/250908458v2-first-order-state-space-model-for-lightweight-image-super-resolution.html" onclick="toggleFavorite(this, '2509.08458v2', 'First-order State Space Model for Lightweight Image Super-resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250908311v1-simcrop-radiograph-representation-learning-with-similarity-driven-cr.html">SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training</a></td>
  <td>SimCroPï¼šåŸºäºç›¸ä¼¼æ€§é©±åŠ¨çš„è·¨ç²’åº¦é¢„è®­ç»ƒæå‡èƒ¸éƒ¨CTå½±åƒè¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08311v1" data-paper-url="./papers/250908311v1-simcrop-radiograph-representation-learning-with-similarity-driven-cr.html" onclick="toggleFavorite(this, '2509.08311v1', 'SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250909737v1-world-modeling-with-probabilistic-structure-integration.html">World Modeling with Probabilistic Structure Integration</a></td>
  <td>æå‡ºæ¦‚ç‡ç»“æ„é›†æˆï¼ˆPSIï¼‰ï¼Œç”¨äºå­¦ä¹ å¯æ§ä¸”çµæ´»æç¤ºçš„ä¸–ç•Œæ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09737v1" data-paper-url="./papers/250909737v1-world-modeling-with-probabilistic-structure-integration.html" onclick="toggleFavorite(this, '2509.09737v1', 'World Modeling with Probabilistic Structure Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250908826v1-rewarddance-reward-scaling-in-visual-generation.html">RewardDance: Reward Scaling in Visual Generation</a></td>
  <td>RewardDanceï¼šé€šè¿‡ç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡è§£å†³è§†è§‰ç”Ÿæˆä¸­çš„å¥–åŠ±ç¼©æ”¾ä¸å¥–åŠ±åˆ©ç”¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08826v1" data-paper-url="./papers/250908826v1-rewarddance-reward-scaling-in-visual-generation.html" onclick="toggleFavorite(this, '2509.08826v1', 'RewardDance: Reward Scaling in Visual Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250908265v1-hyperspectral-mamba-for-hyperspectral-object-tracking.html">Hyperspectral Mamba for Hyperspectral Object Tracking</a></td>
  <td>æå‡ºåŸºäºMambaçš„HyMambaç½‘ç»œï¼Œç”¨äºé«˜å…‰è°±ç›®æ ‡è·Ÿè¸ªï¼Œæå‡å¤æ‚åœºæ™¯ä¸‹çš„è·Ÿè¸ªç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08265v1" data-paper-url="./papers/250908265v1-hyperspectral-mamba-for-hyperspectral-object-tracking.html" onclick="toggleFavorite(this, '2509.08265v1', 'Hyperspectral Mamba for Hyperspectral Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250908502v2-chirality-in-action-time-aware-video-representation-learning-by-late.html">Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening</a></td>
  <td>æå‡ºåŸºäºæ½œåœ¨ç©ºé—´çŸ«æ­£çš„æ—¶é—´æ„ŸçŸ¥è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ‰‹æ€§åŠ¨ä½œè¯†åˆ«ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08502v2" data-paper-url="./papers/250908502v2-chirality-in-action-time-aware-video-representation-learning-by-late.html" onclick="toggleFavorite(this, '2509.08502v2', 'Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250908376v1-bitrate-controlled-diffusion-for-disentangling-motion-and-content-in.html">Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</a></td>
  <td>æå‡ºåŸºäºç ç‡æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„è§†é¢‘è§£è€¦æ¡†æ¶ï¼Œç”¨äºåˆ†ç¦»è§†é¢‘ä¸­çš„è¿åŠ¨å’Œå†…å®¹</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08376v1" data-paper-url="./papers/250908376v1-bitrate-controlled-diffusion-for-disentangling-motion-and-content-in.html" onclick="toggleFavorite(this, '2509.08376v1', 'Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250908489v1-prompt-driven-image-analysis-with-multimodal-generative-ai-detection.html">Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation</a></td>
  <td>æå‡ºåŸºäºæç¤ºçš„å¤šæ¨¡æ€ç”ŸæˆAIå›¾åƒåˆ†ææµç¨‹ï¼Œå®ç°æ£€æµ‹ã€åˆ†å‰²ã€ä¿®å¤ä¸æè¿°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08489v1" data-paper-url="./papers/250908489v1-prompt-driven-image-analysis-with-multimodal-generative-ai-detection.html" onclick="toggleFavorite(this, '2509.08489v1', 'Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250908670v1-fractalpinn-flow-a-fractal-inspired-network-for-unsupervised-optical.html">FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization</a></td>
  <td>æå‡ºFractalPINN-Flowï¼Œä¸€ç§åŸºäºåˆ†å½¢ç½‘ç»œçš„æ— ç›‘ç£å…‰æµä¼°è®¡æ–¹æ³•ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08670v1" data-paper-url="./papers/250908670v1-fractalpinn-flow-a-fractal-inspired-network-for-unsupervised-optical.html" onclick="toggleFavorite(this, '2509.08670v1', 'FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250908991v1-ultron-ultrasound-occupancy-networks.html">UltrON: Ultrasound Occupancy Networks</a></td>
  <td>UltrONï¼šåˆ©ç”¨å£°å­¦ç‰¹å¾çš„è¶…å£°å›¾åƒå æ®ç½‘ç»œï¼Œè§£å†³å¼±ç›‘ç£ä¸‹çš„ä¸‰ç»´é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">implicit representation</span> <span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08991v1" data-paper-url="./papers/250908991v1-ultron-ultrasound-occupancy-networks.html" onclick="toggleFavorite(this, '2509.08991v1', 'UltrON: Ultrasound Occupancy Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250908388v1-semantic-causality-aware-vision-based-3d-occupancy-prediction.html">Semantic Causality-Aware Vision-Based 3D Occupancy Prediction</a></td>
  <td>æå‡ºè¯­ä¹‰å› æœæ„ŸçŸ¥çš„3D Occupancyé¢„æµ‹æ–¹æ³•ï¼Œè§£å†³2Dåˆ°3Dè½¬æ¢ä¸­çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">semantic mapping</span> <span class="paper-tag">semantic map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08388v1" data-paper-url="./papers/250908388v1-semantic-causality-aware-vision-based-3d-occupancy-prediction.html" onclick="toggleFavorite(this, '2509.08388v1', 'Semantic Causality-Aware Vision-Based 3D Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250908982v1-imatcher-improve-matching-in-point-cloud-registration-via-local-to-g.html">iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning</a></td>
  <td>iMatcherï¼šé€šè¿‡å±€éƒ¨åˆ°å…¨å±€å‡ ä½•ä¸€è‡´æ€§å­¦ä¹ æ”¹è¿›ç‚¹äº‘é…å‡†ä¸­çš„ç‰¹å¾åŒ¹é…</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08982v1" data-paper-url="./papers/250908982v1-imatcher-improve-matching-in-point-cloud-registration-via-local-to-g.html" onclick="toggleFavorite(this, '2509.08982v1', 'iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250908908v3-diffusion-based-action-recognition-generalizes-to-untrained-domains.html">Diffusion-Based Action Recognition Generalizes to Untrained Domains</a></td>
  <td>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨æœªè®­ç»ƒåŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08908v3" data-paper-url="./papers/250908908v3-diffusion-based-action-recognition-generalizes-to-untrained-domains.html" onclick="toggleFavorite(this, '2509.08908v3', 'Diffusion-Based Action Recognition Generalizes to Untrained Domains')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250908583v1-efficientiml-efficient-high-resolution-image-manipulation-localizati.html">EfficientIML: Efficient High-Resolution Image Manipulation Localization</a></td>
  <td>æå‡ºEfficientIMLæ¨¡å‹ï¼Œé«˜æ•ˆå®šä½é«˜åˆ†è¾¨ç‡å›¾åƒä¸­åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¯¡æ”¹åŒºåŸŸã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08583v1" data-paper-url="./papers/250908583v1-efficientiml-efficient-high-resolution-image-manipulation-localizati.html" onclick="toggleFavorite(this, '2509.08583v1', 'EfficientIML: Efficient High-Resolution Image Manipulation Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250908764v1-argotweak-towards-self-updating-hd-maps-through-structured-priors.html">ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</a></td>
  <td>ArgoTweakï¼šé€šè¿‡ç»“æ„åŒ–å…ˆéªŒå®ç°é«˜ç²¾åœ°å›¾çš„è‡ªæ›´æ–°</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08764v1" data-paper-url="./papers/250908764v1-argotweak-towards-self-updating-hd-maps-through-structured-priors.html" onclick="toggleFavorite(this, '2509.08764v1', 'ArgoTweak: Towards Self-Updating HD Maps through Structured Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/250908519v1-humo-human-centric-video-generation-via-collaborative-multi-modal-co.html">HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</a></td>
  <td>HuMoï¼šé€šè¿‡ååŒå¤šæ¨¡æ€æ¡ä»¶æ§åˆ¶å®ç°ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.08519v1" data-paper-url="./papers/250908519v1-humo-human-centric-video-generation-via-collaborative-multi-modal-co.html" onclick="toggleFavorite(this, '2509.08519v1', 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)