---
layout: default
title: Recurrence Meets Transformers for Universal Multimodal Retrieval
---

# Recurrence Meets Transformers for Universal Multimodal Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08897" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08897v1</a>
  <a href="https://arxiv.org/pdf/2509.08897.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08897v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08897v1', 'Recurrence Meets Transformers for Universal Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/aimagelab/ReT-2)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReT-2ï¼Œä¸€ç§æ”¯æŒå¤šæ¨¡æ€æŸ¥è¯¢çš„é€šç”¨å¤šæ¨¡æ€æ£€ç´¢æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `å¾ªç¯Transformer` `LSTMé—¨æ§æœºåˆ¶` `è§†è§‰è¯­è¨€æ¨¡å‹` `è·¨æ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡æ—¶ï¼Œä¾èµ–ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œä¸”ä»…æ”¯æŒå•æ¨¡æ€æŸ¥è¯¢ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. ReT-2é€šè¿‡å¾ªç¯Transformeræ¶æ„å’ŒLSTMé—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€æ•´åˆè·¨å±‚å’Œè·¨æ¨¡æ€ä¿¡æ¯ï¼Œå®ç°ç»†ç²’åº¦ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒReT-2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAï¼ŒåŒæ—¶é™ä½äº†æ¨ç†æ—¶é—´å’Œå†…å­˜å ç”¨ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€æ£€ç´¢åŠå…¶åœ¨LLMå’Œå¤šæ¨¡æ€LLMä¸­çš„åº”ç”¨å¿«é€Ÿå‘å±•ï¼Œæ¶Œç°å‡ºæ—¥ç›Šå¤æ‚çš„æ£€ç´¢ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè§†è§‰-è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œå¹¶ä¸”ä»…é™äºå•æ¨¡æ€æŸ¥è¯¢æˆ–æ–‡æ¡£ã€‚æœ¬æ–‡æå‡ºReT-2ï¼Œä¸€ç§ç»Ÿä¸€çš„æ£€ç´¢æ¨¡å‹ï¼Œæ”¯æŒç”±å›¾åƒå’Œæ–‡æœ¬ç»„æˆçš„å¤šæ¨¡æ€æŸ¥è¯¢ï¼Œå¹¶åœ¨æ–‡æœ¬å’Œå›¾åƒå…±å­˜çš„å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­è¿›è¡Œæœç´¢ã€‚ReT-2åˆ©ç”¨å¤šå±‚è¡¨ç¤ºå’Œä¸€ä¸ªå¸¦æœ‰LSTMå¯å‘å¼é—¨æ§æœºåˆ¶çš„å¾ªç¯Transformeræ¶æ„ï¼Œä»¥åŠ¨æ€åœ°æ•´åˆè·¨å±‚å’Œè·¨æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œæ•è·ç»†ç²’åº¦çš„è§†è§‰å’Œæ–‡æœ¬ç»†èŠ‚ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„M2KRå’ŒM-BEIRåŸºå‡†ä¸Šï¼Œé’ˆå¯¹ä¸åŒçš„æ£€ç´¢é…ç½®è¯„ä¼°äº†ReT-2ã€‚ç»“æœè¡¨æ˜ï¼ŒReT-2åœ¨å„ç§è®¾ç½®ä¸‹å§‹ç»ˆå¦‚ä¸€åœ°å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ç‡ã€‚å½“é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ä¸­æ—¶ï¼ŒReT-2è¿˜åœ¨ç™¾ç§‘é—®ç­”ï¼ˆEncyclopedic-VQAï¼‰å’Œä¿¡æ¯æœç´¢ï¼ˆInfoSeekï¼‰æ•°æ®é›†ä¸Šæé«˜äº†ä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å­˜åœ¨çš„å±€é™æ€§ï¼Œå³æ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰ï¼Œå¹¶ä¸”åœ¨å¤„ç†åŒ…å«å¤šç§æ¨¡æ€æ–‡æ¡£çš„é›†åˆæ—¶æ€§èƒ½ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç¼ºä¹é€šç”¨æ€§å’Œçµæ´»æ€§ï¼Œéš¾ä»¥é€‚åº”å¤æ‚çš„å¤šæ¨¡æ€æ£€ç´¢åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReT-2çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¾ªç¯Transformeræ¶æ„ï¼Œç»“åˆLSTMé£æ ¼çš„é—¨æ§æœºåˆ¶ï¼Œå®ç°è·¨å±‚å’Œè·¨æ¨¡æ€ä¿¡æ¯çš„åŠ¨æ€æ•´åˆã€‚é€šè¿‡å¾ªç¯æœºåˆ¶ï¼Œæ¨¡å‹å¯ä»¥è¿­ä»£åœ°å¤„ç†ä¸åŒå±‚çº§çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæ•è·ç»†ç²’åº¦çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚é—¨æ§æœºåˆ¶åˆ™ç”¨äºæ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ï¼Œé€‰æ‹©æ€§åœ°ä¿ç•™æˆ–ä¸¢å¼ƒä¸åŒæ¨¡æ€å’Œå±‚çº§çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReT-2çš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥ç¼–ç å™¨ï¼šç”¨äºå°†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ç¼–ç æˆå‘é‡è¡¨ç¤ºã€‚å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚CLIPã€‚2) å¾ªç¯Transformerå±‚ï¼šæ ¸å¿ƒæ¨¡å—ï¼ŒåŒ…å«å¤šä¸ªTransformerå±‚ï¼Œæ¯ä¸€å±‚éƒ½æ¥æ”¶ä¸Šä¸€å±‚çš„è¾“å‡ºä»¥åŠåŸå§‹çš„è¾“å…¥ç‰¹å¾ã€‚LSTMé—¨æ§æœºåˆ¶è¢«é›†æˆåˆ°Transformerå±‚ä¸­ï¼Œç”¨äºæ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ã€‚3) è¾“å‡ºå±‚ï¼šç”¨äºç”Ÿæˆæœ€ç»ˆçš„æ£€ç´¢ç»“æœã€‚å¯ä»¥é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ç­‰æ–¹æ³•æ¥è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šReT-2çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¾ªç¯Transformeræ¶æ„å’ŒLSTMé—¨æ§æœºåˆ¶çš„ç»“åˆã€‚å¾ªç¯æœºåˆ¶å…è®¸æ¨¡å‹è¿­ä»£åœ°å¤„ç†ä¸åŒå±‚çº§çš„ç‰¹å¾ï¼Œä»è€Œæ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚LSTMé—¨æ§æœºåˆ¶åˆ™å¯ä»¥åŠ¨æ€åœ°æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ReT-2èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œå¹¶åœ¨å„ç§æ£€ç´¢ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒReT-2ä¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œå…·æœ‰æ›´å¥½çš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šReT-2çš„å…³é”®è®¾è®¡ç»†èŠ‚åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ä½œä¸ºè§†è§‰å’Œè¯­è¨€ç¼–ç å™¨ï¼Œä»¥è·å¾—é«˜è´¨é‡çš„ç‰¹å¾è¡¨ç¤ºã€‚2) åœ¨å¾ªç¯Transformerå±‚ä¸­ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ•è·ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚3) ä½¿ç”¨LSTMé—¨æ§æœºåˆ¶æ¥æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ï¼ŒåŒ…æ‹¬è¾“å…¥é—¨ã€é—å¿˜é—¨å’Œè¾“å‡ºé—¨ã€‚4) ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡å‡½æ•°ï¼Œä»¥è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚5) é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ReT-2åœ¨M2KRå’ŒM-BEIRåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨å„ç§æ£€ç´¢é…ç½®ä¸‹å‡è¾¾åˆ°äº†SOTAã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒReT-2åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜é™ä½äº†æ¨ç†æ—¶é—´å’Œå†…å­˜å ç”¨ã€‚æ­¤å¤–ï¼ŒReT-2é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“åï¼Œåœ¨ç™¾ç§‘é—®ç­”ï¼ˆEncyclopedic-VQAï¼‰å’Œä¿¡æ¯æœç´¢ï¼ˆInfoSeekï¼‰æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReT-2å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå¤šæ¨¡æ€æœç´¢å¼•æ“ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ç”µå•†å•†å“æ£€ç´¢ã€ç¤¾äº¤åª’ä½“å†…å®¹æ¨èç­‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†åŒ…å«å›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡æ€æŸ¥è¯¢ï¼Œå¹¶åœ¨å¤šæ¨¡æ€æ–‡æ¡£é›†åˆä¸­è¿›è¡Œé«˜æ•ˆæ£€ç´¢ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡æ¯è·å–æ•ˆç‡ã€‚æœªæ¥ï¼ŒReT-2æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„åœºæ™¯ï¼Œä¾‹å¦‚è·¨æ¨¡æ€æ¨ç†å’Œç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2

