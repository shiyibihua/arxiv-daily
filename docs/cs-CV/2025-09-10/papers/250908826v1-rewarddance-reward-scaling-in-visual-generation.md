---
layout: default
title: RewardDance: Reward Scaling in Visual Generation
---

# RewardDance: Reward Scaling in Visual Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08826" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.08826v1</a>
  <a href="https://arxiv.org/pdf/2509.08826.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08826v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08826v1', 'RewardDance: Reward Scaling in Visual Generation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-10

**Â§áÊ≥®**: Bytedance Seed Technical Report

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RewardDanceÔºöÈÄöËøáÁîüÊàêÂºèÂ•ñÂä±Âª∫Ê®°Ëß£ÂÜ≥ËßÜËßâÁîüÊàê‰∏≠ÁöÑÂ•ñÂä±Áº©Êîæ‰∏éÂ•ñÂä±Âà©Áî®ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â•ñÂä±Ê®°Âûã` `ËßÜËßâÁîüÊàê` `Âº∫ÂåñÂ≠¶‰π†` `Â•ñÂä±Áº©Êîæ` `Â•ñÂä±Âà©Áî®` `ÁîüÊàêÂºèÂ•ñÂä±Âª∫Ê®°` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éCLIPÁöÑÂ•ñÂä±Ê®°ÂûãÂèóÈôê‰∫éÊû∂ÊûÑÂíåËæìÂÖ•Ê®°ÊÄÅÔºåBradley-TerryÊçüÂ§±‰∏éVLMÁöÑtokenÈ¢ÑÊµãÊú∫Âà∂‰∏çÁ¨¶ÔºåÈòªÁ¢ç‰∫ÜÊúâÊïàÁº©Êîæ„ÄÇ
2. RewardDanceÂ∞ÜÂ•ñÂä±ÂàÜÊï∞ÂÆö‰πâ‰∏∫Ê®°ÂûãÈ¢ÑÊµã‚ÄúÊòØ‚ÄùtokenÁöÑÊ¶ÇÁéáÔºå‰ªéËÄåÂ∞ÜÂ•ñÂä±ÁõÆÊ†á‰∏éVLMÊû∂ÊûÑÂØπÈΩêÔºåÂÆûÁé∞Ê®°ÂûãÂíå‰∏ä‰∏ãÊñáÁöÑÁº©Êîæ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåRewardDanceÂú®Â§öÁßçËßÜËßâÁîüÊàê‰ªªÂä°‰∏äË∂ÖË∂äSOTAÊñπÊ≥ïÔºåÂπ∂ÊúâÊïàËß£ÂÜ≥‰∫ÜÂ•ñÂä±Âà©Áî®ÈóÆÈ¢òÔºåÁºìËß£‰∫ÜÊ®°ÂºèÂ¥©Ê∫É„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â•ñÂä±Ê®°ÂûãÔºàRMsÔºâÂØπ‰∫éÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊîπËøõÁîüÊàêÊ®°ÂûãËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜËßÜËßâÁîüÊàê‰∏≠ÁöÑRMÁº©ÊîæËåÉÂºè‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇËøô‰∏ªË¶ÅÊòØÁî±‰∫éÁé∞ÊúâÊñπÊ≥ïÁöÑÊ†πÊú¨Â±ÄÈôêÊÄßÔºöÂü∫‰∫éCLIPÁöÑRMÂèóÂà∞Êû∂ÊûÑÂíåËæìÂÖ•Ê®°ÊÄÅÁöÑÁ∫¶ÊùüÔºåËÄåÊµÅË°åÁöÑBradley-TerryÊçüÂ§±‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑ‰∏ã‰∏Ä‰∏™tokenÈ¢ÑÊµãÊú∫Âà∂Â≠òÂú®Ê†πÊú¨‰∏äÁöÑ‰∏ç‰∏ÄËá¥ÔºåÈòªÁ¢ç‰∫ÜÊúâÊïàÁöÑÁº©Êîæ„ÄÇÊõ¥ÂÖ≥ÈîÆÁöÑÊòØÔºåRLHF‰ºòÂåñËøáÁ®ãÂèóÂà∞Â•ñÂä±Âà©Áî®ÈóÆÈ¢òÁöÑÂõ∞Êâ∞ÔºåÊ®°ÂûãÂà©Áî®Â•ñÂä±‰ø°Âè∑‰∏≠ÁöÑÁº∫Èô∑ËÄåÊ≤°ÊúâÊèêÈ´òÁúüÊ≠£ÁöÑË¥®Èáè„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜRewardDanceÔºåËøôÊòØ‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÂ•ñÂä±Âª∫Ê®°Ê°ÜÊû∂ÔºåÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑÁîüÊàêÂºèÂ•ñÂä±ËåÉÂºèÂÖãÊúç‰∫ÜËøô‰∫õÈöúÁ¢ç„ÄÇÈÄöËøáÂ∞ÜÂ•ñÂä±ÂàÜÊï∞ÈáçÊñ∞ÂÆö‰πâ‰∏∫Ê®°ÂûãÈ¢ÑÊµã‚ÄúÊòØ‚ÄùtokenÁöÑÊ¶ÇÁéáÔºåË°®ÊòéÁîüÊàêÁöÑÂõæÂÉèÂú®ÁâπÂÆöÊ†áÂáÜ‰∏ã‰ºò‰∫éÂèÇËÄÉÂõæÂÉèÔºåRewardDance‰ªéÊú¨Ë¥®‰∏äÂ∞ÜÂ•ñÂä±ÁõÆÊ†á‰∏éVLMÊû∂ÊûÑÂØπÈΩê„ÄÇËøôÁßçÂØπÈΩêËß£ÈîÅ‰∫Ü‰∏§‰∏™Áª¥Â∫¶ÁöÑÁº©ÊîæÔºöÔºà1ÔºâÊ®°ÂûãÁº©ÊîæÔºöÁ≥ªÁªüÂú∞Â∞ÜRMÊâ©Â±ïÂà∞È´òËææ260‰∫ø‰∏™ÂèÇÊï∞ÔºõÔºà2Ôºâ‰∏ä‰∏ãÊñáÁº©ÊîæÔºöÈõÜÊàêÁâπÂÆö‰ªªÂä°ÁöÑÊåá‰ª§„ÄÅÂèÇËÄÉÁ§∫‰æãÂíåÊÄùÁª¥ÈìæÔºàCoTÔºâÊé®ÁêÜ„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåRewardDanceÂú®ÊñáÊú¨Âà∞ÂõæÂÉè„ÄÅÊñáÊú¨Âà∞ËßÜÈ¢ëÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÊòæËëóË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇËá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜÈïøÊúüÂ≠òÂú®ÁöÑ‚ÄúÂ•ñÂä±Âà©Áî®‚ÄùÊåëÊàòÔºöÊàë‰ª¨ÁöÑÂ§ßËßÑÊ®°RMÂú®RLÂæÆË∞ÉÊúüÈó¥Ë°®Áé∞Âá∫Âπ∂‰øùÊåÅ‰∫ÜÈ´òÂ•ñÂä±ÊñπÂ∑ÆÔºåËØÅÊòé‰∫ÜÂÆÉ‰ª¨ÂØπÂà©Áî®ÁöÑÊäµÊäóËÉΩÂäõ‰ª•Âèä‰∫ßÁîüÂ§öÊ†∑Âåñ„ÄÅÈ´òË¥®ÈáèËæìÂá∫ÁöÑËÉΩÂäõ„ÄÇËøôÊûÅÂ§ßÂú∞ÁºìËß£‰∫ÜÂõ∞Êâ∞ËæÉÂ∞èÊ®°ÂûãÁöÑÊ®°ÂºèÂ¥©Ê∫ÉÈóÆÈ¢ò„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâÁîüÊàê‰ªªÂä°‰∏≠ÔºåÂà©Áî®Â•ñÂä±Ê®°ÂûãËøõË°åÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊó∂ÔºåÂ≠òÂú®Â•ñÂä±Ê®°ÂûãÈöæ‰ª•ÊúâÊïàÁº©ÊîæÁöÑÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂü∫‰∫éCLIPÁöÑÂ•ñÂä±Ê®°ÂûãÂ≠òÂú®Êû∂ÊûÑÂíåÊ®°ÊÄÅÈôêÂà∂ÔºåËÄåBradley-TerryÊçüÂ§±‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑtokenÈ¢ÑÊµãÊú∫Âà∂‰∏çÂÖºÂÆπ„ÄÇÊ≠§Â§ñÔºåÂ•ñÂä±Âà©Áî®ÔºàReward HackingÔºâÈóÆÈ¢ò‰ºöÂØºËá¥Ê®°ÂûãÂà©Áî®Â•ñÂä±‰ø°Âè∑ÁöÑÊºèÊ¥ûÔºåËÄåÈùûÁúüÊ≠£ÊèêÂçáÁîüÊàêË¥®Èáè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRewardDanceÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂ•ñÂä±Âª∫Ê®°ÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ÁîüÊàêÂºè‰ªªÂä°ÔºåÂç≥ËÆ©Ê®°ÂûãÈ¢ÑÊµãÁîüÊàêÁöÑÂõæÂÉèÊòØÂê¶‰ºò‰∫éÂèÇËÄÉÂõæÂÉè„ÄÇÈÄöËøáÂ∞ÜÂ•ñÂä±ÂàÜÊï∞ÂÆö‰πâ‰∏∫Ê®°ÂûãÈ¢ÑÊµã‚ÄúÊòØ‚ÄùtokenÁöÑÊ¶ÇÁéáÔºåÂ∞ÜÂ•ñÂä±ÁõÆÊ†á‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÂØπÈΩêÔºå‰ªéËÄåÂÆûÁé∞Â•ñÂä±Ê®°ÂûãÁöÑÊúâÊïàÁº©ÊîæÔºåÂπ∂ÁºìËß£Â•ñÂä±Âà©Áî®ÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRewardDanceÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ≠•È™§Ôºö1) ÊûÑÂª∫ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂåÖÂê´ÁîüÊàêÁöÑÂõæÂÉè„ÄÅÂèÇËÄÉÂõæÂÉè‰ª•ÂèäÊåáÁ§∫ÁîüÊàêÂõæÂÉèÊòØÂê¶‰ºò‰∫éÂèÇËÄÉÂõæÂÉèÁöÑÊ†áÁ≠æÔºõ2) ‰ΩøÁî®ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Â•ñÂä±Ê®°ÂûãÔºåÂπ∂Â∞ÜÂÖ∂ËÆ≠ÁªÉÊàê‰∏Ä‰∏™ÁîüÊàêÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈ¢ÑÊµã‚ÄúÊòØ‚ÄùÊàñ‚ÄúÂê¶‚ÄùtokenÔºõ3) ‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊ†πÊçÆÂ•ñÂä±Ê®°ÂûãÊèê‰æõÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåÂØπÁîüÊàêÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•ÊèêÈ´òÁîüÊàêÂõæÂÉèÁöÑË¥®Èáè„ÄÇÊ°ÜÊû∂ÊîØÊåÅÊ®°ÂûãÁº©ÊîæÔºàÊâ©Â±ïÊ®°ÂûãÂèÇÊï∞ÈáèÔºâÂíå‰∏ä‰∏ãÊñáÁº©ÊîæÔºàÈõÜÊàê‰ªªÂä°Êåá‰ª§„ÄÅÂèÇËÄÉÁ§∫‰æãÂíåÊÄùÁª¥ÈìæÊé®ÁêÜÔºâ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRewardDanceÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÁîüÊàêÂºèÂ•ñÂä±Âª∫Ê®°ËåÉÂºè„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éCLIPÊàñBradley-TerryÊçüÂ§±ÁöÑÂ•ñÂä±Ê®°Âûã‰∏çÂêåÔºåRewardDanceÂ∞ÜÂ•ñÂä±Âª∫Ê®°ÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ÁîüÊàêÂºè‰ªªÂä°Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÂØπÈΩêÔºåÂπ∂ÊúâÊïàÁºìËß£‰∫ÜÂ•ñÂä±Âà©Áî®ÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåRewardDanceËøòÊîØÊåÅÊ®°ÂûãÂíå‰∏ä‰∏ãÊñáÁöÑÁº©ÊîæÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÂ•ñÂä±Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRewardDanceÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â∞ÜÂ•ñÂä±ÂàÜÊï∞ÂÆö‰πâ‰∏∫Ê®°ÂûãÈ¢ÑÊµã‚ÄúÊòØ‚ÄùtokenÁöÑÊ¶ÇÁéáÔºå‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉÔºõ2) ÊîØÊåÅÈõÜÊàê‰ªªÂä°Êåá‰ª§„ÄÅÂèÇËÄÉÁ§∫‰æãÂíåÊÄùÁª¥ÈìæÊé®ÁêÜÔºå‰ª•ÊèêÈ´òÂ•ñÂä±Ê®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõÔºõ3) ‰ΩøÁî®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºå‰ª•ÊèêÈ´òÂ•ñÂä±Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ4) Âú®Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉËøáÁ®ã‰∏≠ÔºåÁõëÊéßÂ•ñÂä±ÊñπÂ∑ÆÔºå‰ª•Ê£ÄÊµãÂíåÁºìËß£Â•ñÂä±Âà©Áî®ÈóÆÈ¢ò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRewardDanceÂú®ÊñáÊú¨Âà∞ÂõæÂÉè„ÄÅÊñáÊú¨Âà∞ËßÜÈ¢ëÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜSOTAÊñπÊ≥ï„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåRewardDanceÊúâÊïàËß£ÂÜ≥‰∫ÜÂ•ñÂä±Âà©Áî®ÈóÆÈ¢òÔºåÂÖ∂Â§ßËßÑÊ®°RMÂú®RLÂæÆË∞ÉÊúüÈó¥Ë°®Áé∞Âá∫Âπ∂‰øùÊåÅ‰∫ÜÈ´òÂ•ñÂä±ÊñπÂ∑ÆÔºåËØÅÊòé‰∫ÜÂÖ∂ÂØπÂà©Áî®ÁöÑÊäµÊäóËÉΩÂäõ‰ª•Âèä‰∫ßÁîüÂ§öÊ†∑Âåñ„ÄÅÈ´òË¥®ÈáèËæìÂá∫ÁöÑËÉΩÂäõ„ÄÇËøôÊûÅÂ§ßÂú∞ÁºìËß£‰∫ÜÂõ∞Êâ∞ËæÉÂ∞èÊ®°ÂûãÁöÑÊ®°ÂºèÂ¥©Ê∫ÉÈóÆÈ¢ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RewardDanceÂèØÂ∫îÁî®‰∫éÂêÑÁßçËßÜËßâÁîüÊàê‰ªªÂä°ÔºåÂ¶ÇÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê„ÄÅÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàê„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáÁîüÊàêÂõæÂÉèÂíåËßÜÈ¢ëÁöÑË¥®Èáè„ÄÅÂ§öÊ†∑ÊÄßÂíå‰∏ÄËá¥ÊÄßÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂÜÖÂÆπÂàõ‰Ωú„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèëÁ≠âÈ¢ÜÂüü„ÄÇÊú™Êù•ÔºåRewardDanceÊúâÊúõËøõ‰∏ÄÊ≠•Êé®Âä®ËßÜËßâÁîüÊàêÊäÄÊúØÁöÑÂèëÂ±ïÔºåÂπ∂‰∏∫Áõ∏ÂÖ≥Â∫îÁî®Â∏¶Êù•Êõ¥Â§öÂèØËÉΩÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.

