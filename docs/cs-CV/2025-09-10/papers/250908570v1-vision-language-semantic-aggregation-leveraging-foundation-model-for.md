---
layout: default
title: Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation
---

# Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08570" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08570v1</a>
  <a href="https://arxiv.org/pdf/2509.08570.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08570v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08570v1', 'Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenjun Yu, Yinchen Zhou, Jia-Xuan Jiang, Shubin Zeng, Yuee Li, Zhong Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

**å¤‡æ³¨**: 29 pages and 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºEMèšåˆå’Œæ–‡æœ¬å¼•å¯¼è§£ç çš„è§†è§‰-è¯­è¨€è¯­ä¹‰èšåˆæ–¹æ³•ï¼Œæå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ³›åŒ–æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒåˆ†å‰²` `å¤šæ¨¡æ€èåˆ` `è¯­ä¹‰èšåˆ` `é¢†åŸŸæ³›åŒ–` `æœŸæœ›æœ€å¤§åŒ–` `æ–‡æœ¬å¼•å¯¼` `è§†è§‰è¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œç”±äºæ–‡æœ¬æç¤ºå’Œè§†è§‰ç‰¹å¾çš„è¯­ä¹‰å·®è·ä»¥åŠç‰¹å¾åˆ†æ•£ï¼Œå¤šæ¨¡æ€èåˆæ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºEMèšåˆæœºåˆ¶å’Œæ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ï¼Œåˆ†åˆ«ç”¨äºå‡å°‘ç‰¹å¾åˆ†æ•£å’Œå¼¥åˆè¯­ä¹‰å·®è·ï¼Œæå‡è·¨æ¨¡æ€å¯¹åº”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¿ƒè„å’Œçœ¼åº•æ•°æ®é›†ä¸Šï¼Œç›¸æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œåœ¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸåº”ç”¨æ—¶æ€§èƒ½å¾€å¾€ä¸ä½³ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†è¿™ç§æ€§èƒ½å·®è·å½’å› äºå¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯æŠ½è±¡æ–‡æœ¬æç¤ºå’Œç»†ç²’åº¦åŒ»å­¦è§†è§‰ç‰¹å¾ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è¯­ä¹‰å·®è·ï¼Œä»¥åŠç”±æ­¤äº§ç”Ÿçš„ç‰¹å¾åˆ†æ•£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»è¯­ä¹‰èšåˆçš„è§’åº¦é‡æ–°å®¡è§†è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰èšåˆæœºåˆ¶å’Œä¸€ç§æ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ã€‚å‰è€…é€šè¿‡å°†ç‰¹å¾åŠ¨æ€èšç±»æˆç´§å‡‘çš„è¯­ä¹‰ä¸­å¿ƒæ¥å‡è½»ç‰¹å¾åˆ†æ•£ï¼Œä»è€Œå¢å¼ºè·¨æ¨¡æ€å¯¹åº”å…³ç³»ã€‚åè€…æ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢†åŸŸä¸å˜çš„æ–‡æœ¬çŸ¥è¯†æ¥æœ‰æ•ˆåœ°å¼•å¯¼æ·±åº¦è§†è§‰è¡¨ç¤ºï¼Œä»è€Œå¼¥åˆè¯­ä¹‰å·®è·ã€‚è¿™ä¸¤ä¸ªæœºåˆ¶ä¹‹é—´çš„ååŒä½œç”¨æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å…±å¿ƒè„å’Œçœ¼åº•æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒä¸Šæ€§èƒ½ä¸‹é™ã€‚ä¸»è¦åŸå› æ˜¯åŒ»å­¦å›¾åƒçš„ç»†ç²’åº¦ç‰¹å¾ä¸æŠ½è±¡æ–‡æœ¬æç¤ºä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„è¯­ä¹‰é¸¿æ²Ÿï¼Œå¯¼è‡´ç‰¹å¾åˆ†æ•£ï¼Œéš¾ä»¥æœ‰æ•ˆèåˆã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯æŒ‡å¯¼è§†è§‰ç‰¹å¾çš„å­¦ä¹ ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»è¯­ä¹‰èšåˆçš„è§’åº¦å‡ºå‘ï¼Œé€šè¿‡å¢å¼ºè·¨æ¨¡æ€çš„è¯­ä¹‰å¯¹åº”å…³ç³»æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡EMèšåˆæœºåˆ¶å°†åˆ†æ•£çš„è§†è§‰ç‰¹å¾èšé›†æˆç´§å‡‘çš„è¯­ä¹‰ä¸­å¿ƒï¼Œå‡å°‘ç‰¹å¾åˆ†æ•£ï¼›åŒæ—¶ï¼Œåˆ©ç”¨æ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ï¼Œå°†é¢†åŸŸä¸å˜çš„æ–‡æœ¬çŸ¥è¯†èå…¥è§†è§‰ç‰¹å¾çš„å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¼¥åˆè¯­ä¹‰é¸¿æ²Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«è§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€EMèšåˆæ¨¡å—å’Œæ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ã€‚é¦–å…ˆï¼Œè§†è§‰ç¼–ç å™¨æå–åŒ»å­¦å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼Œæ–‡æœ¬ç¼–ç å™¨æå–æ–‡æœ¬æç¤ºçš„è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶åï¼ŒEMèšåˆæ¨¡å—å¯¹è§†è§‰ç‰¹å¾è¿›è¡Œèšç±»ï¼Œå½¢æˆè¯­ä¹‰ä¸­å¿ƒã€‚æœ€åï¼Œæ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯æŒ‡å¯¼è§†è§‰ç‰¹å¾çš„è§£ç ï¼Œç”Ÿæˆåˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†EMèšåˆæœºåˆ¶å’Œæ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ã€‚EMèšåˆæœºåˆ¶é€šè¿‡åŠ¨æ€èšç±»çš„æ–¹å¼ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†ç‰¹å¾åˆ†æ•£ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€çš„å¯¹åº”å…³ç³»ã€‚æ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨åˆ™åˆ©ç”¨é¢†åŸŸä¸å˜çš„æ–‡æœ¬çŸ¥è¯†ï¼Œå¼¥åˆäº†è¯­ä¹‰é¸¿æ²Ÿï¼Œæå‡äº†æ¨¡å‹å¯¹åŒ»å­¦å›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šEMèšåˆæœºåˆ¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ç®—æ³•è¿›è¡Œç‰¹å¾èšç±»ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–èšç±»ä¸­å¿ƒå’Œç‰¹å¾åˆ†é…ï¼Œæœ€ç»ˆå¾—åˆ°ç´§å‡‘çš„è¯­ä¹‰è¡¨ç¤ºã€‚æ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ–‡æœ¬ç‰¹å¾ä¸è§†è§‰ç‰¹å¾è¿›è¡Œèåˆï¼Œä»è€ŒæŒ‡å¯¼åƒç´ çº§åˆ«çš„åˆ†å‰²ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åˆ†å‰²æŸå¤±å’Œè·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¿ƒè„å’Œçœ¼åº•æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨é¢†åŸŸæ³›åŒ–å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨å¿ƒè„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼ŒDiceç³»æ•°æå‡äº†3%-5%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œä¾‹å¦‚å¿ƒè„ã€çœ¼åº•ã€è‚¿ç˜¤ç­‰ç—…ç¶çš„è‡ªåŠ¨åˆ†å‰²ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†å’Œä¸åŒåŒ»é™¢æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼å’Œå•†ä¸šæ½œåŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–åŒ»å­¦å½±åƒæ¨¡æ€ï¼Œä¾‹å¦‚CTã€MRIç­‰ï¼Œå®ç°æ›´å…¨é¢çš„åŒ»å­¦å›¾åƒåˆ†æã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the model's generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks.

