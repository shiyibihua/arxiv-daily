---
layout: default
title: Diffusion-Based Action Recognition Generalizes to Untrained Domains
---

# Diffusion-Based Action Recognition Generalizes to Untrained Domains

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08908" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08908v3</a>
  <a href="https://arxiv.org/pdf/2509.08908.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08908v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08908v3', 'Diffusion-Based Action Recognition Generalizes to Untrained Domains')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rogerio Guimaraes, Frank Xiao, Pietro Perona, Markus Marks

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: Project page: https://www.vision.caltech.edu/actiondiff. Code: https://github.com/frankyaoxiao/ActionDiff

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/frankyaoxiao/ActionDiff)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨æœªè®­ç»ƒåŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `åŠ¨ä½œè¯†åˆ«` `æ‰©æ•£æ¨¡å‹` `é¢†åŸŸæ³›åŒ–` `Transformer` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŠ¨ä½œè¯†åˆ«ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹ç‰©ç§ã€è§†è§’å’Œä¸Šä¸‹æ–‡ç­‰å˜åŒ–ã€‚
2. åˆ©ç”¨è§†è§‰æ‰©æ•£æ¨¡å‹æå–è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨Transformerè¿›è¡Œèšåˆï¼Œä»¥æå‡æ¨¡å‹åœ¨æœªè®­ç»ƒåŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨ç‰©ç§ã€è·¨è§†è§’å’Œè·¨è®°å½•ä¸Šä¸‹æ–‡çš„åŠ¨ä½œè¯†åˆ«ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»å¯ä»¥åœ¨ä¸Šä¸‹æ–‡å’Œè§†è§’å‘ç”Ÿå·¨å¤§å˜åŒ–çš„æƒ…å†µä¸‹è¯†åˆ«ç›¸åŒçš„åŠ¨ä½œï¼Œä¾‹å¦‚ç‰©ç§å·®å¼‚ï¼ˆèœ˜è››ä¸é©¬çš„è¡Œèµ°æ–¹å¼ï¼‰ã€è§†è§’å·®å¼‚ï¼ˆç¬¬ä¸€äººç§°ä¸ç¬¬ä¸‰äººç§°ï¼‰ä»¥åŠä¸Šä¸‹æ–‡å·®å¼‚ï¼ˆç°å®ç”Ÿæ´»ä¸ç”µå½±ï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰ç”Ÿæˆçš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡Transformerè¿›è¡Œèšåˆçš„æ–¹æ³•ï¼Œä»¥å®ç°ç±»ä¼¼äººç±»çš„è·¨è¶Šè¿™äº›æŒ‘æˆ˜æ€§æ¡ä»¶çš„åŠ¨ä½œè¯†åˆ«ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ä»¥æ‰©æ•£è¿‡ç¨‹çš„æ—©æœŸæ—¶é—´æ­¥ä¸ºæ¡ä»¶çš„æ¨¡å‹ï¼Œå¯ä»¥çªå‡ºæå–ç‰¹å¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯è€Œéåƒç´ çº§ç»†èŠ‚ï¼Œä»è€Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡æ¢è®¨äº†è¯¥æ–¹æ³•åœ¨è·¨ç‰©ç§ã€è·¨è§†è§’å’Œè·¨è®°å½•ä¸Šä¸‹æ–‡çš„åŠ¨ä½œåˆ†ç±»ä¸­çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶åœ¨æ‰€æœ‰ä¸‰ä¸ªæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„state-of-the-artç»“æœï¼Œä½¿æœºå™¨åŠ¨ä½œè¯†åˆ«æ›´æ¥è¿‘äººç±»çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠ¨ä½œè¯†åˆ«æ¨¡å‹åœ¨é¢å¯¹æœªè®­ç»ƒè¿‡çš„é¢†åŸŸï¼ˆä¾‹å¦‚ä¸åŒçš„ç‰©ç§ã€è§†è§’æˆ–æ‹æ‘„ç¯å¢ƒï¼‰æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œå¹¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆåˆ°è®­ç»ƒæ•°æ®ä¸­çš„ç‰¹å®šæ¨¡å¼ï¼Œå¯¼è‡´åœ¨æ–°çš„é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰æå–å¯¹é¢†åŸŸå˜åŒ–æ›´é²æ£’çš„è¯­ä¹‰ç‰¹å¾ã€‚VDMé€šè¿‡å­¦ä¹ ä»å™ªå£°åˆ°å›¾åƒçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°å›¾åƒä¸­æ›´æŠ½è±¡å’Œæœ¬è´¨çš„ç‰¹å¾ï¼Œä»è€Œå‡å°‘å¯¹åƒç´ çº§åˆ«ç»†èŠ‚çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œä½¿ç”¨Transformeræ¥èšåˆè¿™äº›ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å¯¹ä¸åŒåŠ¨ä½œä¹‹é—´å…³ç³»çš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ‰©æ•£æ¨¡å‹æå–è§†é¢‘å¸§çš„ç‰¹å¾ï¼›2) å¯¹æ‰©æ•£æ¨¡å‹çš„ä¸åŒæ—¶é—´æ­¥çš„ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆï¼Œçªå‡ºè¯­ä¹‰ä¿¡æ¯ï¼›3) ä½¿ç”¨Transformerç½‘ç»œå¯¹æå–çš„ç‰¹å¾è¿›è¡Œæ—¶åºå»ºæ¨¡å’Œèšåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„åŠ¨ä½œç±»åˆ«é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨äº†æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ä»¥æ‰©æ•£è¿‡ç¨‹çš„æ—©æœŸæ—¶é—´æ­¥ä¸ºæ¡ä»¶çš„æ¨¡å‹æ¥æå–ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åˆ°å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œå¿½ç•¥æ‰ä¸€äº›ä¸ç›¸å…³çš„åƒç´ çº§åˆ«ç»†èŠ‚ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç›´æ¥ä½¿ç”¨åŸå§‹åƒç´ æˆ–æ·±åº¦ç½‘ç»œæå–çš„ç‰¹å¾ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹æå–çš„ç‰¹å¾æ›´å…·æœ‰é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ‰©æ•£æ¨¡å‹ï¼Œé¿å…äº†ä»å¤´è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å·¨å¤§è®¡ç®—æˆæœ¬ï¼›2) é€šè¿‡å®éªŒç¡®å®šäº†æœ€ä½³çš„æ‰©æ•£æ—¶é—´æ­¥èŒƒå›´ï¼Œä»¥å¹³è¡¡è¯­ä¹‰ä¿¡æ¯å’Œç»†èŠ‚ä¿¡æ¯ï¼›3) ä½¿ç”¨æ ‡å‡†çš„Transformerç½‘ç»œç»“æ„è¿›è¡Œæ—¶åºå»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¨¡å‹åœ¨è·¨ç‰©ç§ã€è·¨è§†è§’å’Œè·¨è®°å½•ä¸Šä¸‹æ–‡çš„ä¸‰ä¸ªæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†state-of-the-artçš„ç»“æœã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¯¦ç»†ç»™å‡ºï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æå‡åŠ¨ä½œè¯†åˆ«æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚é¡¹ç›®ä¸»é¡µå’Œä»£ç å·²å¼€æºï¼Œæ–¹ä¾¿ç ”ç©¶è€…å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é²æ£’åŠ¨ä½œè¯†åˆ«çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€è§†é¢‘ç›‘æ§ã€äººæœºäº¤äº’ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒå’Œè§†è§’ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿è¿™äº›åº”ç”¨æ›´åŠ å¯é å’Œå®ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦è·¨é¢†åŸŸæ³›åŒ–çš„è§†è§‰ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: https://www.vision.caltech.edu/actiondiff. Code: https://github.com/frankyaoxiao/ActionDiff

