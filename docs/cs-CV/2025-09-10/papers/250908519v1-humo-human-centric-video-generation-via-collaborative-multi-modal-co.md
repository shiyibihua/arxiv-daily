---
layout: default
title: HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning
---

# HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08519" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08519v1</a>
  <a href="https://arxiv.org/pdf/2509.08519.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08519v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08519v1', 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://phantom-video.github.io/HuMo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HuMoï¼šé€šè¿‡ååŒå¤šæ¨¡æ€æ¡ä»¶æ§åˆ¶å®ç°ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `å¤šæ¨¡æ€èåˆ` `ä»¥äººä¸ºä¸­å¿ƒ` `è§†å¬åŒæ­¥` `æ·±åº¦å­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹` `æ¡ä»¶ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰HCVGæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåè°ƒæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œé¢ä¸´è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œå­ä»»åŠ¡ååŒå›°éš¾çš„æŒ‘æˆ˜ã€‚
2. HuMoæ¡†æ¶é€šè¿‡æ„å»ºé«˜è´¨é‡æ•°æ®é›†å’Œè®¾è®¡ä¸¤é˜¶æ®µæ¸è¿›å¼è®­ç»ƒèŒƒå¼ï¼Œå®ç°äº†å¤šæ¨¡æ€è¾“å…¥çš„ååŒæ§åˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHuMoåœ¨ä¸»ä½“ä¿æŒå’Œè§†å¬åŒæ­¥ç­‰å­ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€HCVGã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆ(HCVG)æ—¨åœ¨ä»å¤šæ¨¡æ€è¾“å…¥ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ï¼‰åˆæˆäººç‰©è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåè°ƒè¿™äº›å¼‚æ„æ¨¡æ€ï¼ŒåŸå› åœ¨äºä¸¤ä¸ªæŒ‘æˆ˜ï¼šé…å¯¹çš„ä¸‰å…ƒç»„æ¡ä»¶è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼Œä»¥åŠåœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹ï¼Œä¸»ä½“ä¿æŒå’Œè§†å¬åŒæ­¥ç­‰å­ä»»åŠ¡éš¾ä»¥ååŒã€‚æœ¬æ–‡æå‡ºäº†HuMoï¼Œä¸€ä¸ªç”¨äºååŒå¤šæ¨¡æ€æ§åˆ¶çš„ç»Ÿä¸€HCVGæ¡†æ¶ã€‚é’ˆå¯¹ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·ä¸”é…å¯¹çš„æ–‡æœ¬ã€å‚è€ƒå›¾åƒå’ŒéŸ³é¢‘ã€‚é’ˆå¯¹ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰ä»»åŠ¡ç‰¹å®šç­–ç•¥çš„ä¸¤é˜¶æ®µæ¸è¿›å¼å¤šæ¨¡æ€è®­ç»ƒèŒƒå¼ã€‚å¯¹äºä¸»ä½“ä¿æŒä»»åŠ¡ï¼Œä¸ºäº†ä¿æŒåŸºç¡€æ¨¡å‹çš„æç¤ºéµå¾ªå’Œè§†è§‰ç”Ÿæˆèƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€å°ä¾µå…¥å¼å›¾åƒæ³¨å…¥ç­–ç•¥ã€‚å¯¹äºè§†å¬åŒæ­¥ä»»åŠ¡ï¼Œé™¤äº†å¸¸ç”¨çš„éŸ³é¢‘äº¤å‰æ³¨æ„åŠ›å±‚å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡é¢„æµ‹è¿›è¡Œèšç„¦çš„ç­–ç•¥ï¼Œéšå¼åœ°å¼•å¯¼æ¨¡å‹å°†éŸ³é¢‘ä¸é¢éƒ¨åŒºåŸŸç›¸å…³è”ã€‚ä¸ºäº†è”åˆå­¦ä¹ è·¨å¤šæ¨¡æ€è¾“å…¥çš„å¯æ§æ€§ï¼Œæˆ‘ä»¬åœ¨å…ˆå‰è·å¾—çš„èƒ½åŠ›çš„åŸºç¡€ä¸Šï¼Œé€æ­¥æ•´åˆè§†å¬åŒæ­¥ä»»åŠ¡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†çµæ´»å’Œç»†ç²’åº¦çš„å¤šæ¨¡æ€æ§åˆ¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ—¶é—´è‡ªé€‚åº”çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´å»å™ªæ­¥éª¤ä¸­çš„å¼•å¯¼æƒé‡ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒHuMoåœ¨å­ä»»åŠ¡ä¸­è¶…è¶Šäº†ä¸“é—¨çš„state-of-the-artæ–¹æ³•ï¼Œä¸ºååŒå¤šæ¨¡æ€æ¡ä»¶HCVGå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆæ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä¸»è¦ç—›ç‚¹åœ¨äºç¼ºä¹é«˜è´¨é‡çš„é…å¯¹è®­ç»ƒæ•°æ®ï¼Œä»¥åŠéš¾ä»¥åŒæ—¶ä¿è¯ç”Ÿæˆè§†é¢‘çš„ä¸»ä½“ä¸€è‡´æ€§å’Œè§†å¬åŒæ­¥æ€§ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œå¯æ§æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHuMoçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µæ¸è¿›å¼è®­ç»ƒèŒƒå¼ï¼Œé€æ­¥æå‡æ¨¡å‹å¯¹ä¸åŒæ¨¡æ€ä¿¡æ¯çš„ç†è§£å’Œèåˆèƒ½åŠ›ã€‚é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ç­–ç•¥ï¼Œåˆ†åˆ«ä¼˜åŒ–ä¸»ä½“ä¿æŒå’Œè§†å¬åŒæ­¥ï¼Œæœ€ç»ˆå®ç°ååŒçš„å¤šæ¨¡æ€æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHuMoæ¡†æ¶åŒ…å«æ•°æ®æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»ºåŒ…å«æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘é…å¯¹çš„é«˜è´¨é‡æ•°æ®é›†ã€‚ç„¶åï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¸è¿›å¼è®­ç»ƒèŒƒå¼ï¼Œç¬¬ä¸€é˜¶æ®µä¾§é‡äºä¸»ä½“ä¿æŒï¼Œç¬¬äºŒé˜¶æ®µé€æ­¥æ•´åˆè§†å¬åŒæ­¥ä»»åŠ¡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œä½¿ç”¨æ—¶é—´è‡ªé€‚åº”çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œå®ç°çµæ´»çš„å¤šæ¨¡æ€æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šHuMoçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†å……è¶³çš„æ•°æ®æ”¯æŒï¼›2) æå‡ºäº†ä¸¤é˜¶æ®µæ¸è¿›å¼è®­ç»ƒèŒƒå¼ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€ä¿¡æ¯èåˆçš„éš¾é¢˜ï¼›3) æå‡ºäº†æœ€å°ä¾µå…¥å¼å›¾åƒæ³¨å…¥ç­–ç•¥å’ŒåŸºäºé¢„æµ‹çš„èšç„¦ç­–ç•¥ï¼Œåˆ†åˆ«ä¼˜åŒ–äº†ä¸»ä½“ä¿æŒå’Œè§†å¬åŒæ­¥æ•ˆæœï¼›4) è®¾è®¡äº†æ—¶é—´è‡ªé€‚åº”çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸»ä½“ä¿æŒé˜¶æ®µï¼Œé‡‡ç”¨æœ€å°ä¾µå…¥å¼å›¾åƒæ³¨å…¥ç­–ç•¥ï¼Œé¿å…è¿‡åº¦ä¿®æ”¹åŸºç¡€æ¨¡å‹ï¼Œä¿æŒå…¶ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨è§†å¬åŒæ­¥é˜¶æ®µï¼Œé™¤äº†éŸ³é¢‘äº¤å‰æ³¨æ„åŠ›å±‚å¤–ï¼Œè¿˜å¼•å…¥äº†åŸºäºé¢„æµ‹çš„èšç„¦ç­–ç•¥ï¼Œé€šè¿‡é¢„æµ‹é¢éƒ¨åŒºåŸŸæ¥å¼•å¯¼æ¨¡å‹å…³æ³¨éŸ³é¢‘ç›¸å…³åŒºåŸŸã€‚æ—¶é—´è‡ªé€‚åº”çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥æ ¹æ®å»å™ªæ­¥éª¤åŠ¨æ€è°ƒæ•´å¼•å¯¼æƒé‡ï¼Œå®ç°æ›´ç²¾ç»†çš„æ§åˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HuMoåœ¨å¤šä¸ªå®éªŒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸»ä½“ä¿æŒæ–¹é¢ï¼ŒHuMoèƒ½å¤Ÿç”Ÿæˆä¸å‚è€ƒå›¾åƒé«˜åº¦ä¸€è‡´çš„äººç‰©è§†é¢‘ã€‚åœ¨è§†å¬åŒæ­¥æ–¹é¢ï¼ŒHuMoç”Ÿæˆçš„è§†é¢‘èƒ½å¤Ÿå‡†ç¡®åœ°å°†éŸ³é¢‘ä¸äººç‰©çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œåŒæ­¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHuMoåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„state-of-the-artæ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HuMoæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è™šæ‹Ÿå½¢è±¡å®šåˆ¶ã€ç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆç­‰ã€‚è¯¥æŠ€æœ¯å¯ä»¥æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°ã€å‚è€ƒå›¾åƒå’ŒéŸ³é¢‘ï¼Œç”Ÿæˆé«˜åº¦é€¼çœŸä¸”å¯æ§çš„äººç‰©è§†é¢‘ï¼Œæå¤§åœ°æå‡äº†å†…å®¹åˆ›ä½œçš„æ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼ŒHuMoæœ‰æœ›æˆä¸ºå¤šåª’ä½“å†…å®¹ç”Ÿæˆé¢†åŸŸçš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.

