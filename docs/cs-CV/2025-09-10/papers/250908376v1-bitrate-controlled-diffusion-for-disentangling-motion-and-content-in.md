---
layout: default
title: Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video
---

# Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08376" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08376v1</a>
  <a href="https://arxiv.org/pdf/2509.08376.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08376v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08376v1', 'Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Li, Qi Chen, Xiulian Peng, Kai Yu, Xie Chen, Yan Lu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç ç‡æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„è§†é¢‘è§£è€¦æ¡†æ¶ï¼Œç”¨äºåˆ†ç¦»è§†é¢‘ä¸­çš„è¿åŠ¨å’Œå†…å®¹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `è§†é¢‘è§£è€¦` `æ‰©æ•£æ¨¡å‹` `è‡ªç›‘ç£å­¦ä¹ ` `ä¿¡æ¯ç“¶é¢ˆ` `è¿åŠ¨è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘è§£è€¦æ–¹æ³•ä¾èµ–è¾ƒå¼ºçš„å‡è®¾å’Œå½’çº³åç½®ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œçµæ´»æ€§ã€‚
2. æå‡ºä¸€ç§åŸºäºç ç‡æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡ä¿¡æ¯ç“¶é¢ˆä¿ƒè¿›è¿åŠ¨å’Œå†…å®¹çš„è§£è€¦ã€‚
3. åœ¨è¯´è¯äººå¤´éƒ¨è§†é¢‘å’Œ2Då¡é€šè§†é¢‘ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†è¿åŠ¨è¿ç§»å’Œè‡ªå›å½’è¿åŠ¨ç”Ÿæˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºå°†è§†é¢‘æ•°æ®è§£è€¦ä¸ºåŠ¨æ€è¿åŠ¨å’Œé™æ€å†…å®¹ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚è¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªè‡ªç›‘ç£æµç¨‹ï¼Œä¸ä»¥å¾€å·¥ä½œç›¸æ¯”ï¼Œå…·æœ‰æ›´å°‘çš„å‡è®¾å’Œå½’çº³åç½®ã€‚å®ƒåˆ©ç”¨åŸºäºTransformerçš„æ¶æ„ï¼Œä¸ºé€å¸§è¿åŠ¨å’Œç‰‡æ®µå†…å®¹è”åˆç”Ÿæˆçµæ´»çš„éšå¼ç‰¹å¾ï¼Œå¹¶ç»“åˆä½ç ç‡çŸ¢é‡é‡åŒ–ä½œä¸ºä¿¡æ¯ç“¶é¢ˆï¼Œä»¥ä¿ƒè¿›è§£è€¦å¹¶å½¢æˆæœ‰æ„ä¹‰çš„ç¦»æ•£è¿åŠ¨ç©ºé—´ã€‚ç ç‡æ§åˆ¶çš„æ½œåœ¨è¿åŠ¨å’Œå†…å®¹è¢«ç”¨ä½œå»å™ªæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶è¾“å…¥ï¼Œä»¥ä¿ƒè¿›è‡ªç›‘ç£è¡¨å¾å­¦ä¹ ã€‚åœ¨çœŸå®ä¸–ç•Œçš„è¯´è¯äººå¤´éƒ¨è§†é¢‘ä¸Šï¼Œé€šè¿‡è¿åŠ¨è¿ç§»å’Œè‡ªå›å½’è¿åŠ¨ç”Ÿæˆä»»åŠ¡éªŒè¯äº†è§£è€¦è¡¨å¾å­¦ä¹ æ¡†æ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„è§†é¢‘æ•°æ®ï¼Œä¾‹å¦‚2Då¡é€šäººç‰©çš„åƒç´ ç²¾çµã€‚è¿™é¡¹å·¥ä½œä¸ºè§£è€¦è§†é¢‘è¡¨å¾çš„è‡ªç›‘ç£å­¦ä¹ æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œä¸ºæ›´å¹¿æ³›çš„è§†é¢‘åˆ†æå’Œç”Ÿæˆé¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘è§£è€¦æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šçš„ç½‘ç»œç»“æ„æˆ–æŸå¤±å‡½æ•°è®¾è®¡ï¼Œå¯¹è§†é¢‘å†…å®¹å’Œè¿åŠ¨çš„å…ˆéªŒçŸ¥è¯†è¦æ±‚è¾ƒé«˜ï¼Œç¼ºä¹é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ†ç¦»è§†é¢‘ä¸­çš„è¿åŠ¨å’Œå†…å®¹ï¼Œå¹¶å­¦ä¹ åˆ°å¯è§£é‡Šçš„è¿åŠ¨è¡¨å¾ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¿¡æ¯ç“¶é¢ˆåŸç†ï¼Œé€šè¿‡ä½ç ç‡çŸ¢é‡é‡åŒ–æ¥é™åˆ¶è¿åŠ¨ä¿¡æ¯çš„å®¹é‡ï¼Œä»è€Œè¿«ä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ ç®€æ´å’Œå¯è§£é‡Šçš„è¿åŠ¨è¡¨å¾ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå°†è§£è€¦åçš„è¿åŠ¨å’Œå†…å®¹ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå®ç°é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šè¿åŠ¨å’Œå†…å®¹ç¼–ç å™¨ã€çŸ¢é‡é‡åŒ–æ¨¡å—å’Œæ‰©æ•£æ¨¡å‹ã€‚é¦–å…ˆï¼Œä½¿ç”¨åŸºäºTransformerçš„ç¼–ç å™¨åˆ†åˆ«æå–é€å¸§è¿åŠ¨ç‰¹å¾å’Œç‰‡æ®µå†…å®¹ç‰¹å¾ã€‚ç„¶åï¼Œé€šè¿‡çŸ¢é‡é‡åŒ–æ¨¡å—å¯¹è¿åŠ¨ç‰¹å¾è¿›è¡Œç¦»æ•£åŒ–ï¼Œå½¢æˆä½ç ç‡çš„è¿åŠ¨ç¼–ç ã€‚æœ€åï¼Œå°†é‡åŒ–åçš„è¿åŠ¨ç¼–ç å’Œå†…å®¹ç‰¹å¾ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”Ÿæˆç›®æ ‡è§†é¢‘å¸§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åŸºäºç ç‡æ§åˆ¶çš„è§£è€¦æ¡†æ¶ï¼Œé€šè¿‡ä¿¡æ¯ç“¶é¢ˆä¿ƒè¿›è¿åŠ¨å’Œå†…å®¹çš„è§£è€¦ï¼›2) åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°äº†é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ï¼›3) è¯¥æ¡†æ¶å…·æœ‰è¾ƒå¼ºçš„é€šç”¨æ€§ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒç±»å‹çš„è§†é¢‘æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¿åŠ¨å’Œå†…å®¹ç¼–ç å™¨ä¸­ï¼Œä½¿ç”¨äº†åŸºäºTransformerçš„æ¶æ„ï¼Œä»¥æ•æ‰è§†é¢‘ä¸­çš„æ—¶åºä¾èµ–å…³ç³»ã€‚çŸ¢é‡é‡åŒ–æ¨¡å—é‡‡ç”¨Gumbel-SoftmaxæŠ€å·§ï¼Œå®ç°å¯å¾®çš„ç¦»æ•£åŒ–è¿‡ç¨‹ã€‚æ‰©æ•£æ¨¡å‹é‡‡ç”¨U-Netç»“æ„ï¼Œå¹¶å¼•å…¥äº†æ¡ä»¶è¾“å…¥æœºåˆ¶ï¼Œä»¥æ§åˆ¶ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨å’Œå†…å®¹ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±ã€é‡åŒ–æŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œä»¥ä¿è¯ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œè§£è€¦æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡åœ¨è¯´è¯äººå¤´éƒ¨è§†é¢‘å’Œ2Då¡é€šè§†é¢‘ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚åœ¨è¿åŠ¨è¿ç§»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡è§†é¢‘ï¼Œå¹¶ä¿æŒäººç‰©çš„èº«ä»½ä¿¡æ¯ã€‚åœ¨è‡ªå›å½’è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰è‡ªç„¶è¿åŠ¨æ¨¡å¼çš„è§†é¢‘åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£è€¦è§†é¢‘ä¸­çš„è¿åŠ¨å’Œå†…å®¹ï¼Œå¹¶å­¦ä¹ åˆ°å¯è§£é‡Šçš„è¿åŠ¨è¡¨å¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç¼–è¾‘ã€è§†é¢‘ç”Ÿæˆã€è§†é¢‘å‹ç¼©ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å®ç°å¯¹è§†é¢‘ä¸­äººç‰©çš„è¿åŠ¨è¿›è¡Œç¼–è¾‘å’Œè¿ç§»ï¼Œç”Ÿæˆå…·æœ‰ç‰¹å®šè¿åŠ¨é£æ ¼çš„æ–°è§†é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè§†é¢‘å‹ç¼©ï¼Œé€šè¿‡å¯¹è¿åŠ¨ä¿¡æ¯è¿›è¡Œé«˜æ•ˆç¼–ç ï¼Œé™ä½è§†é¢‘çš„å­˜å‚¨å’Œä¼ è¾“æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å¼çš„è§†é¢‘ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.

