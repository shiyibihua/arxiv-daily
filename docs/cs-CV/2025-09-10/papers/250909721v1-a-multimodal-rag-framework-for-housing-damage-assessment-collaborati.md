---
layout: default
title: A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval
---

# A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09721" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09721v1</a>
  <a href="https://arxiv.org/pdf/2509.09721.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09721v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09721v1', 'A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayi Miao, Dingxin Lu, Zhuqi Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€RAGæ¡†æ¶ï¼Œç”¨äºç¾åæˆ¿å±‹æŸä¼¤è¯„ä¼°ï¼ŒååŒä¼˜åŒ–å›¾åƒç¼–ç å’Œç­–ç•¥å‘é‡æ£€ç´¢ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æˆ¿å±‹æŸä¼¤è¯„ä¼°` `ç¾åé‡å»º` `è·¨æ¨¡æ€äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¾åæˆ¿å±‹æŸä¼¤è¯„ä¼°ä¸­ï¼Œç¼ºä¹å¯¹å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯æœ‰æ•ˆèåˆï¼Œå¯¼è‡´è¯„ä¼°ç²¾åº¦ä¸è¶³ã€‚
2. æå‡ºä¸€ç§å¤šæ¨¡æ€RAGæ¡†æ¶ï¼Œé€šè¿‡åŒåˆ†æ”¯ç¼–ç å™¨å’Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œå®ç°å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯çš„ååŒç†è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æŸä¼¤ä¸¥é‡ç¨‹åº¦çš„æ£€ç´¢å‡†ç¡®ç‡ä¸Šæ˜¾è‘—æå‡ï¼ŒTop-1æ£€ç´¢å‡†ç¡®ç‡æé«˜äº†9.6%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰æ¡†æ¶ï¼Œç”¨äºè‡ªç„¶ç¾å®³åæˆ¿å±‹æŸä¼¤çš„ç²¾ç¡®è¯„ä¼°ï¼Œè¿™å¯¹äºä¿é™©ç†èµ”å’Œèµ„æºè§„åˆ’è‡³å…³é‡è¦ã€‚è¯¥æ¡†æ¶åœ¨ç»å…¸RAGæ¶æ„çš„åŸºç¡€ä¸Šï¼Œè®¾è®¡äº†ä¸€ä¸ªåŒåˆ†æ”¯å¤šæ¨¡æ€ç¼–ç å™¨ç»“æ„ï¼šå›¾åƒåˆ†æ”¯é‡‡ç”¨ç”±ResNetå’ŒTransformerç»„æˆçš„è§†è§‰ç¼–ç å™¨ï¼Œæå–ç¾åå»ºç­‘ç‰©æŸä¼¤ç‰¹å¾ï¼›æ–‡æœ¬åˆ†æ”¯åˆ©ç”¨BERTæ£€ç´¢å™¨å¯¹å¸–å­å’Œä¿é™©ç­–ç•¥è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ï¼Œå¹¶æ„å»ºå¯æ£€ç´¢çš„ä¿®å¤ç´¢å¼•ã€‚ä¸ºäº†å®ç°è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ï¼Œæ¨¡å‹é›†æˆäº†è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œé€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¡¥æ¥å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰è¡¨ç¤ºã€‚åŒæ—¶ï¼Œåœ¨ç”Ÿæˆæ¨¡å—ä¸­ï¼Œå¼•å…¥çš„æ¨¡æ€æ³¨æ„åŠ›é—¨æ§æœºåˆ¶åŠ¨æ€æ§åˆ¶è§†è§‰è¯æ®å’Œæ–‡æœ¬å…ˆéªŒä¿¡æ¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒï¼Œç»“åˆå¯¹æ¯”æŸå¤±ã€æ£€ç´¢æŸå¤±å’Œç”ŸæˆæŸå¤±å½¢æˆå¤šä»»åŠ¡ä¼˜åŒ–ç›®æ ‡ï¼Œåœ¨ååŒå­¦ä¹ ä¸­å®ç°å›¾åƒç†è§£å’Œç­–ç•¥åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æŸä¼¤ä¸¥é‡ç¨‹åº¦çš„æ£€ç´¢å‡†ç¡®ç‡å’Œåˆ†ç±»æŒ‡æ ‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­Top-1æ£€ç´¢å‡†ç¡®ç‡æé«˜äº†9.6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªç„¶ç¾å®³åæˆ¿å±‹æŸä¼¤è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äººå·¥è¯„ä¼°æˆ–å•ä¸€æ¨¡æ€çš„ä¿¡æ¯ï¼Œæ•ˆç‡ä½ä¸”å®¹æ˜“å‡ºé”™ã€‚ç¼ºä¹æœ‰æ•ˆèåˆå›¾åƒï¼ˆæˆ¿å±‹å—æŸè§†è§‰ä¿¡æ¯ï¼‰å’Œæ–‡æœ¬ï¼ˆä¿é™©æ”¿ç­–ã€ç¾æƒ…æè¿°ï¼‰çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œå¯¼è‡´è¯„ä¼°ç²¾åº¦å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯äº’è¡¥çš„ä¼˜åŠ¿ï¼Œæå‡æˆ¿å±‹æŸä¼¤è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚é€šè¿‡ååŒä¼˜åŒ–å›¾åƒç¼–ç å’Œç­–ç•¥å‘é‡æ£€ç´¢ï¼Œå®ç°å¯¹æˆ¿å±‹æŸä¼¤çš„å…¨é¢ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **å›¾åƒç¼–ç åˆ†æ”¯**ï¼šä½¿ç”¨ResNetå’ŒTransformeræå–æˆ¿å±‹æŸä¼¤å›¾åƒçš„ç‰¹å¾ã€‚2) **æ–‡æœ¬ç¼–ç åˆ†æ”¯**ï¼šä½¿ç”¨BERTæ£€ç´¢å™¨å¯¹æ–‡æœ¬ä¿¡æ¯ï¼ˆå¸–å­ã€ä¿é™©ç­–ç•¥ï¼‰è¿›è¡Œå‘é‡åŒ–ï¼Œæ„å»ºå¯æ£€ç´¢çš„ä¿®å¤ç´¢å¼•ã€‚3) **è·¨æ¨¡æ€äº¤äº’æ¨¡å—**ï¼šé€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¡¥æ¥å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ã€‚4) **ç”Ÿæˆæ¨¡å—**ï¼šåˆ©ç”¨æ¨¡æ€æ³¨æ„åŠ›é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€æ§åˆ¶è§†è§‰è¯æ®å’Œæ–‡æœ¬å…ˆéªŒä¿¡æ¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯å¤šæ¨¡æ€ç¼–ç å™¨ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå–å›¾åƒå’Œæ–‡æœ¬çš„ç‰¹å¾ã€‚2) å¼•å…¥äº†è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œå®ç°äº†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚3) è®¾è®¡äº†æ¨¡æ€æ³¨æ„åŠ›é—¨æ§æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€æ§åˆ¶è§†è§‰è¯æ®å’Œæ–‡æœ¬å…ˆéªŒä¿¡æ¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å…¨é¢ã€å‡†ç¡®åœ°ç†è§£æˆ¿å±‹æŸä¼¤æƒ…å†µã€‚

**å…³é”®è®¾è®¡**ï¼š1) **æŸå¤±å‡½æ•°**ï¼šé‡‡ç”¨å¤šä»»åŠ¡ä¼˜åŒ–ç›®æ ‡ï¼Œç»“åˆå¯¹æ¯”æŸå¤±ã€æ£€ç´¢æŸå¤±å’Œç”ŸæˆæŸå¤±ï¼Œå®ç°å›¾åƒç†è§£å’Œç­–ç•¥åŒ¹é…çš„ååŒå­¦ä¹ ã€‚2) **æ¨¡æ€æ³¨æ„åŠ›é—¨æ§æœºåˆ¶**ï¼šé€šè¿‡å­¦ä¹ æƒé‡ï¼ŒåŠ¨æ€è°ƒæ•´å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è´¡çŒ®ã€‚3) **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šæ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦æ‰‹åŠ¨è®¾è®¡ç‰¹å¾çš„ç¹çè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥MM-RAGæ¡†æ¶åœ¨æˆ¿å±‹æŸä¼¤ä¸¥é‡ç¨‹åº¦çš„æ£€ç´¢å‡†ç¡®ç‡å’Œåˆ†ç±»æŒ‡æ ‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ŒTop-1æ£€ç´¢å‡†ç¡®ç‡æé«˜äº†9.6%ã€‚è¿™è¡¨æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£æˆ¿å±‹æŸä¼¤æƒ…å†µï¼Œå¹¶æ£€ç´¢åˆ°ç›¸å…³çš„ä¿é™©ç­–ç•¥å’Œä¿®å¤ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¾åæˆ¿å±‹æŸä¼¤å¿«é€Ÿè¯„ä¼°ã€ä¿é™©ç†èµ”è‡ªåŠ¨åŒ–å¤„ç†ã€ç¾åé‡å»ºèµ„æºæ™ºèƒ½åˆ†é…ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è¯„ä¼°æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥åŠ é€Ÿç¾åæ¢å¤è¿›ç¨‹ï¼Œé™ä½ä¿é™©å…¬å¸çš„è¿è¥æˆæœ¬ï¼Œå¹¶ä¸ºå—ç¾ç¾¤ä¼—æä¾›æ›´åŠæ—¶æœ‰æ•ˆçš„å¸®åŠ©ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„ç¾å®³è¯„ä¼°å’Œé£é™©ç®¡ç†ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> After natural disasters, accurate evaluations of damage to housing are important for insurance claims response and planning of resources. In this work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG) framework. On top of classical RAG architecture, we further the framework to devise a two-branch multimodal encoder structure that the image branch employs a visual encoder composed of ResNet and Transformer to extract the characteristic of building damage after disaster, and the text branch harnesses a BERT retriever for the text vectorization of posts as well as insurance policies and for the construction of a retrievable restoration index. To impose cross-modal semantic alignment, the model integrates a cross-modal interaction module to bridge the semantic representation between image and text via multi-head attention. Meanwhile, in the generation module, the introduced modal attention gating mechanism dynamically controls the role of visual evidence and text prior information during generation. The entire framework takes end-to-end training, and combines the comparison loss, the retrieval loss and the generation loss to form multi-task optimization objectives, and achieves image understanding and policy matching in collaborative learning. The results demonstrate superior performance in retrieval accuracy and classification index on damage severity, where the Top-1 retrieval accuracy has been improved by 9.6%.

