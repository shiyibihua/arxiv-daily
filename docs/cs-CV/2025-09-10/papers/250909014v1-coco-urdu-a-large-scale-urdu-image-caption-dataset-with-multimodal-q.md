---
layout: default
title: COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation
---

# COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09014" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09014v1</a>
  <a href="https://arxiv.org/pdf/2509.09014.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09014v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09014v1', 'COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Umair Hassan

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

**å¤‡æ³¨**: 17 pages, 3 figures, 3 tables. Dataset available at https://huggingface.co/datasets/umairhassan02/urdu-translated-coco-captions-subset. Scripts and notebooks to reproduce results available at https://github.com/umair-hassan2/COCO-Urdu

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**COCO-Urduï¼šæ„å»ºå¤§è§„æ¨¡ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°æ•°æ®é›†ï¼Œå¹¶æå‡ºå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¹Œå°”éƒ½è¯­` `å›¾åƒæè¿°` `å¤šæ¨¡æ€` `æ•°æ®é›†` `æœºå™¨ç¿»è¯‘` `è´¨é‡è¯„ä¼°` `è§†è§‰-è¯­è¨€` `ä½èµ„æºè¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€ç ”ç©¶ä¸­ä¹Œå°”éƒ½è¯­èµ„æºä¸¥é‡ä¸è¶³ï¼Œç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†é™åˆ¶äº†ç›¸å…³ç³»ç»Ÿçš„å‘å±•ï¼ŒåŠ å‰§äº†å¤šè¯­è¨€è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„åå·®ã€‚
2. è®ºæ–‡æå‡ºCOCO-Urduæ•°æ®é›†ï¼Œé€šè¿‡é«˜è´¨é‡çš„æœºå™¨ç¿»è¯‘å’Œå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œæ„å»ºå¤§è§„æ¨¡ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°æ•°æ®é›†ã€‚
3. COCO-Urduæ•°æ®é›†åœ¨BLEUç­‰æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä¸ºä¹Œå°”éƒ½è¯­è§†è§‰-è¯­è¨€ç ”ç©¶æä¾›äº†å®è´µèµ„æºï¼Œå¹¶å…¬å¼€äº†è´¨é‡è¯„ä¼°æµç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†COCO-Urduï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°æ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥è¡¥å¤šæ¨¡æ€å’Œè§†è§‰-è¯­è¨€ç ”ç©¶ä¸­ä¹Œå°”éƒ½è¯­èµ„æºçš„åŒ®ä¹ã€‚è¯¥æ•°æ®é›†åŸºäºMS COCOï¼ŒåŒ…å«59,000å¼ å›¾åƒå’Œ319,000æ¡ä¹Œå°”éƒ½è¯­æè¿°ï¼Œé€šè¿‡åˆ†å±‚æŠ½æ ·ä¿ç•™äº†åŸå§‹åˆ†å¸ƒã€‚æè¿°ä½¿ç”¨SeamlessM4T v2ç¿»è¯‘ï¼Œå¹¶é€šè¿‡æ··åˆå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶è¿›è¡ŒéªŒè¯ï¼Œè¯¥æ¡†æ¶é›†æˆäº†COMET-Kiwiï¼ˆç¿»è¯‘è´¨é‡ï¼‰ã€åŸºäºCLIPçš„ç›¸ä¼¼åº¦ï¼ˆè§†è§‰å¯¹é½ï¼‰ä»¥åŠBERTScoreä¸å›è¯‘ï¼ˆè¯­ä¹‰ä¸€è‡´æ€§ï¼‰ã€‚ä½è´¨é‡æè¿°é€šè¿‡å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹è¿­ä»£æ”¹è¿›ã€‚COCO-Urduåœ¨BLEUã€SacreBLEUå’ŒchrFä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨ç°è‰¯å¥½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCOCO-Urduæ˜¯ç›®å‰æœ€å¤§çš„å…¬å¼€ä¹Œå°”éƒ½è¯­æè¿°æ•°æ®é›†ã€‚é€šè¿‡å‘å¸ƒæ•°æ®é›†å’Œè´¨é‡è¯„ä¼°æµç¨‹ï¼Œæ—¨åœ¨å‡å°‘å¤šæ¨¡æ€ç ”ç©¶ä¸­çš„è¯­è¨€åå·®ï¼Œå¹¶ä¸ºåŒ…å®¹æ€§çš„è§†è§‰-è¯­è¨€ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€ä¸Šï¼Œå¯¹äºä¹Œå°”éƒ½è¯­è¿™ç§æ‹¥æœ‰è¶…è¿‡2.5äº¿ä½¿ç”¨è€…çš„è¯­è¨€ï¼Œç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ã€‚è¿™å¯¼è‡´äº†ä¹Œå°”éƒ½è¯­ç›¸å…³çš„å¤šæ¨¡æ€ç³»ç»Ÿå‘å±•å—é™ï¼Œå¹¶ä¸”åŠ å‰§äº†ç°æœ‰æ¨¡å‹ä¸­çš„è¯­è¨€åå·®ã€‚å› æ­¤ï¼Œéœ€è¦æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›ä¹Œå°”éƒ½è¯­ç›¸å…³çš„è§†è§‰-è¯­è¨€ç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç°æœ‰çš„MS COCOæ•°æ®é›†ï¼Œé€šè¿‡é«˜è´¨é‡çš„æœºå™¨ç¿»è¯‘å’Œä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ï¼Œç”Ÿæˆä¹Œå°”éƒ½è¯­çš„å›¾åƒæè¿°ã€‚ä¸ºäº†ä¿è¯ç¿»è¯‘è´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ··åˆå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ä½¿ç”¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä½è´¨é‡çš„æè¿°è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOCO-Urduæ•°æ®é›†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é€‰æ‹©ï¼šä»MS COCOæ•°æ®é›†ä¸­é€‰æ‹©59,000å¼ å›¾åƒï¼Œå¹¶è¿›è¡Œåˆ†å±‚æŠ½æ ·ä»¥ä¿ç•™åŸå§‹åˆ†å¸ƒã€‚2) æœºå™¨ç¿»è¯‘ï¼šä½¿ç”¨SeamlessM4T v2å°†è‹±æ–‡æè¿°ç¿»è¯‘æˆä¹Œå°”éƒ½è¯­ã€‚3) è´¨é‡è¯„ä¼°ï¼šä½¿ç”¨æ··åˆå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶å¯¹ç¿»è¯‘åçš„æè¿°è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šCOMET-Kiwiï¼ˆè¯„ä¼°ç¿»è¯‘è´¨é‡ï¼‰ã€åŸºäºCLIPçš„ç›¸ä¼¼åº¦ï¼ˆè¯„ä¼°è§†è§‰å¯¹é½ç¨‹åº¦ï¼‰å’ŒBERTScoreä¸å›è¯‘ï¼ˆè¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§ï¼‰ã€‚4) è¿­ä»£æ”¹è¿›ï¼šå¯¹äºè´¨é‡è¯„ä¼°å¾—åˆ†è¾ƒä½çš„æè¿°ï¼Œä½¿ç”¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªæ··åˆå¤šæ¨¡æ€è´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»¼åˆè€ƒè™‘äº†ç¿»è¯‘è´¨é‡ã€è§†è§‰å¯¹é½ç¨‹åº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°å’Œç­›é€‰é«˜è´¨é‡çš„ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†è¿­ä»£æ”¹è¿›çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä½è´¨é‡çš„æè¿°è¿›è¡Œä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ•°æ®é›†çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è´¨é‡è¯„ä¼°æ¡†æ¶ä¸­ï¼ŒCOMET-Kiwiç”¨äºè¯„ä¼°ç¿»è¯‘çš„æµç•…åº¦å’Œå‡†ç¡®æ€§ï¼›CLIPæ¨¡å‹ç”¨äºè®¡ç®—å›¾åƒå’Œæè¿°ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä»¥ç¡®ä¿è§†è§‰å¯¹é½ï¼›BERTScoreç»“åˆå›è¯‘æŠ€æœ¯ç”¨äºè¯„ä¼°æè¿°çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¿™äº›æŒ‡æ ‡çš„æƒé‡éœ€è¦æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„è¯„ä¼°æ•ˆæœã€‚åœ¨è¿­ä»£æ”¹è¿›é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶è®¾è®¡æœ‰æ•ˆçš„promptï¼Œä»¥æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ä¹Œå°”éƒ½è¯­æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

COCO-Urduæ•°æ®é›†æ˜¯ç›®å‰æœ€å¤§çš„å…¬å¼€ä¹Œå°”éƒ½è¯­å›¾åƒæè¿°æ•°æ®é›†ï¼ŒåŒ…å«59,000å¼ å›¾åƒå’Œ319,000æ¡ä¹Œå°”éƒ½è¯­æè¿°ã€‚åœ¨BLEUã€SacreBLEUå’ŒchrFç­‰æŒ‡æ ‡ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜COCO-Urduæ•°æ®é›†å…·æœ‰è¾ƒé«˜çš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæµ‹è¯•é›†ä¸Šï¼ŒCOCO-Urduçš„BLEUå¾—åˆ†è¾¾åˆ°äº†XXï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ï¼ŒSacreBLEUå¾—åˆ†è¾¾åˆ°äº†YYï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ï¼ŒchrFå¾—åˆ†è¾¾åˆ°äº†ZZï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

COCO-Urduæ•°æ®é›†å¯å¹¿æ³›åº”ç”¨äºä¹Œå°”éƒ½è¯­ç›¸å…³çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å›¾åƒæ£€ç´¢ç­‰ã€‚è¯¥æ•°æ®é›†çš„å‘å¸ƒæœ‰åŠ©äºä¿ƒè¿›ä¹Œå°”éƒ½è¯­è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€ç ”ç©¶çš„å‘å±•ï¼Œå¹¶å‡å°‘å¤šè¯­è¨€æ¨¡å‹ä¸­çš„è¯­è¨€åå·®ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†å’Œè´¨é‡è¯„ä¼°æµç¨‹å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ä½èµ„æºè¯­è¨€ï¼Œä¸ºæ„å»ºæ›´å¤šåŒ…å®¹æ€§çš„è§†è§‰-è¯­è¨€ç³»ç»Ÿæä¾›å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Urdu, spoken by over 250 million people, remains critically under-served in multimodal and vision-language research. The absence of large-scale, high-quality datasets has limited the development of Urdu-capable systems and reinforced biases in multilingual vision-language models trained primarily on high-resource languages. To address this gap, we present COCO-Urdu, a large-scale image-caption dataset derived from MS COCO, containing 59,000 images and 319,000 Urdu captions selected through stratified sampling to preserve the original distribution. Captions were translated using SeamlessM4T v2 and validated with a hybrid multimodal quality estimation framework that integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual grounding, and BERTScore with back-translation for semantic consistency; low-scoring captions were iteratively refined using open-source large language models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting consistently strong results. To the best of our knowledge, COCO-Urdu is the largest publicly available Urdu captioning dataset. By releasing both the dataset and the quality estimation pipeline, we aim to reduce language bias in multimodal research and establish a foundation for inclusive vision-language systems.

