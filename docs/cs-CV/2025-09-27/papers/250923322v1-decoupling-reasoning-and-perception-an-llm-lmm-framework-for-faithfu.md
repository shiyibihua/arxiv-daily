---
layout: default
title: Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning
---

# Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23322" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23322v1</a>
  <a href="https://arxiv.org/pdf/2509.23322.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23322v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23322v1', 'Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongrui Jia, Chaoya Jiang, Shikun Zhang, Wei Ye

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦æ¨ç†ä¸æ„ŸçŸ¥çš„LLM-LMMæ¡†æ¶ï¼Œæå‡è§†è§‰æ¨ç†çš„å¯é æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤§å‹å¤šæ¨¡æ€æ¨¡å‹` `è§£è€¦æ¨ç†` `æ€ç»´é“¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LMMåœ¨é•¿é“¾è§†è§‰æ¨ç†ä¸­è¿‡åº¦ä¾èµ–æ–‡æœ¬é€»è¾‘ï¼Œå¿½ç•¥è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†é”™è¯¯ã€‚
2. è®ºæ–‡æå‡ºè§£è€¦æ¨ç†ä¸æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œåˆ©ç”¨LLMè¿›è¡Œæ¨ç†ï¼ŒLMMä¸“æ³¨äºè§†è§‰ä¿¡æ¯æå–ã€‚
3. è¯¥æ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘äº†æ— æ ¹æ®çš„æ¨ç†æ­¥éª¤ï¼Œæé«˜äº†è§†è§‰æ¨ç†çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨æ‰©å±•çš„æ€ç»´é“¾(CoT)æ¨ç†ã€‚å—æ­¤å¯å‘ï¼Œç ”ç©¶äººå‘˜å°†è¿™äº›èŒƒå¼æ‰©å±•åˆ°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMM)ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é™åˆ¶æ˜¯ï¼šéšç€æ¨ç†é“¾çš„æ‰©å±•ï¼ŒLMMè¶Šæ¥è¶Šä¾èµ–äºæ–‡æœ¬é€»è¾‘ï¼Œé€æ¸å¤±å»å¯¹åº•å±‚è§†è§‰ä¿¡æ¯çš„ä¾èµ–ã€‚è¿™å¯¼è‡´æ¨ç†è·¯å¾„åç¦»å›¾åƒå†…å®¹ï¼Œæœ€ç»ˆå¯¼è‡´é”™è¯¯çš„ç»“è®ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéå¸¸ç®€å•ä½†æœ‰æ•ˆçš„å…è®­ç»ƒè§†è§‰æ¨ç†æµç¨‹ã€‚æ ¸å¿ƒæ¦‚å¿µæ˜¯å°†æ¨ç†å’Œæ„ŸçŸ¥è¿‡ç¨‹è§£è€¦ã€‚å¼ºå¤§çš„LLMè´Ÿè´£é«˜å±‚æ¬¡çš„æ¨ç†ï¼Œç­–ç•¥æ€§åœ°è¯¢é—®LMMä»¥æå–é€»è¾‘é“¾æ‰€éœ€çš„ç‰¹å®šè§†è§‰ä¿¡æ¯ã€‚LMMåˆ™ä¸“é—¨ä½œä¸ºè§†è§‰é—®ç­”å¼•æ“ï¼ŒæŒ‰éœ€æä¾›å¿…è¦çš„æ„ŸçŸ¥ç»†èŠ‚ã€‚è¿™ç§è½»é‡çº§çš„å³æ’å³ç”¨æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–æ¶æ„æ›´æ”¹ã€‚å…¨é¢çš„è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶æœ‰æ•ˆåœ°æ§åˆ¶äº†è§†è§‰æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†è§†è§‰ä¸Šæ— æ ¹æ®çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶å¤§å¤§æé«˜äº†æ¨ç†çš„ä¿çœŸåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨è§†è§‰æ¨ç†è¿‡ç¨‹ä¸­ï¼Œéšç€æ¨ç†é“¾çš„å»¶é•¿ï¼Œè¶Šæ¥è¶Šä¾èµ–æ–‡æœ¬é€»è¾‘è€Œå¿½ç•¥è§†è§‰ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´æ¨ç†ç»“æœä¸å›¾åƒå†…å®¹ä¸ç¬¦çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ¨ç†å’Œæ„ŸçŸ¥è¿‡ç¨‹è€¦åˆåœ¨ä¸€èµ·ï¼Œå¯¼è‡´LMMéš¾ä»¥ä¿æŒå¯¹è§†è§‰ä¿¡æ¯çš„å‡†ç¡®ç†è§£å’Œåˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨ç†å’Œæ„ŸçŸ¥è¿‡ç¨‹è§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æ¥ orchestrate é«˜å±‚æ¬¡çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶è®©å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä¸“æ³¨äºè§†è§‰ä¿¡æ¯çš„æå–ã€‚LLMæ ¹æ®æ¨ç†çš„éœ€è¦ï¼Œæœ‰ç­–ç•¥åœ°å‘LMMæé—®ï¼Œè·å–ç‰¹å®šçš„è§†è§‰ä¿¡æ¯ï¼Œä»è€Œé¿å…LMMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿‡åº¦ä¾èµ–æ–‡æœ¬é€»è¾‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šLLMæ¨ç†æ¨¡å—å’ŒLMMè§†è§‰é—®ç­”æ¨¡å—ã€‚LLMä½œä¸ºæ¨ç†å¼•æ“ï¼Œè´Ÿè´£ç”Ÿæˆæ¨ç†é“¾ï¼Œå¹¶æ ¹æ®æ¨ç†æ­¥éª¤çš„éœ€è¦ï¼Œå‘LMMæå‡ºè§†è§‰é—®é¢˜ã€‚LMMä½œä¸ºè§†è§‰ä¿¡æ¯æå–å¼•æ“ï¼Œæ¥æ”¶LLMæå‡ºçš„é—®é¢˜ï¼Œå¹¶æ ¹æ®å›¾åƒå†…å®¹ç»™å‡ºç­”æ¡ˆã€‚LLMæ¥æ”¶LMMçš„ç­”æ¡ˆåï¼Œç»§ç»­è¿›è¡Œæ¨ç†ï¼Œç›´åˆ°å¾—å‡ºæœ€ç»ˆç»“è®ºã€‚æ•´ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„é—®ç­”è¿‡ç¨‹ï¼ŒLLMè´Ÿè´£æ¨ç†ï¼ŒLMMè´Ÿè´£æä¾›è§†è§‰ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºè§£è€¦æ¨ç†å’Œæ„ŸçŸ¥è¿‡ç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸å†è®©LMMåŒæ—¶è´Ÿè´£æ¨ç†å’Œæ„ŸçŸ¥ï¼Œè€Œæ˜¯å°†è¿™ä¸¤ä¸ªä»»åŠ¡åˆ†åˆ«äº¤ç»™LLMå’ŒLMMï¼Œä»è€Œå……åˆ†å‘æŒ¥å„è‡ªçš„ä¼˜åŠ¿ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡å¯ä»¥æœ‰æ•ˆé¿å…LMMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿‡åº¦ä¾èµ–æ–‡æœ¬é€»è¾‘ï¼Œä»è€Œæé«˜è§†è§‰æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ¡†æ¶çš„å…³é”®è®¾è®¡åœ¨äºLLMå’ŒLMMä¹‹é—´çš„äº¤äº’æ–¹å¼ã€‚LLMéœ€è¦èƒ½å¤Ÿæ ¹æ®æ¨ç†çš„éœ€è¦ï¼Œç”Ÿæˆåˆé€‚çš„è§†è§‰é—®é¢˜ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨LMMæä¾›çš„ç­”æ¡ˆã€‚LMMéœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°ç†è§£LLMæå‡ºçš„é—®é¢˜ï¼Œå¹¶èƒ½å¤Ÿä»å›¾åƒä¸­æå–å‡ºç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚è®ºæ–‡ä¸­æ²¡æœ‰æåŠå…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œå› ä¸ºè¯¥æ–¹æ³•æ˜¯å…è®­ç»ƒçš„ï¼Œä¸éœ€è¦å¯¹LLMæˆ–LMMè¿›è¡Œé¢å¤–çš„è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æå‡ºçš„æ¡†æ¶åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡è§£è€¦æ¨ç†å’Œæ„ŸçŸ¥è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†è§†è§‰ä¸Šæ— æ ¹æ®çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶å¤§å¤§æé«˜äº†æ¨ç†çš„ä¿çœŸåº¦ã€‚å…·ä½“å®éªŒæ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†å¼ºè°ƒäº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è§†è§‰æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡æé«˜è§†è§‰æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå¯ä»¥æå‡è¿™äº›åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„è§†è§‰æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€ä¸‰ç»´åœºæ™¯ç†è§£ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Significant advancements in the reasoning capabilities of Large Language Models (LLMs) are now driven by test-time scaling laws, particularly those leveraging extended Chain-of-Thought (CoT) reasoning. Inspired by these breakthroughs, researchers have extended these paradigms to Large Multimodal Models (LMMs). However, a critical limitation emerges: as their reasoning chains extend, LMMs increasingly rely on textual logic, progressively losing grounding in the underlying visual information. This leads to reasoning paths that diverge from the image content, culminating in erroneous conclusions. To address this, we introduce a strikingly simple yet effective training-free visual-reasoning pipeline. The core concept is to decouple the reasoning and perception processes. A powerful LLM orchestrates the high-level reasoning, strategically interrogating a LMM to extract specific visual information required for its logical chain. The LMM, in turn, functions exclusively as a visual question-answering engine, supplying the necessary perceptual details on demand. This lightweight, plug-and-play approach requires no additional training or architectural changes. Comprehensive evaluations validate that our framework effectively governs the visual reasoning process, leading to a significant reduction in visually-unfounded reasoning steps and a substantial improvement in reasoning fidelity.

