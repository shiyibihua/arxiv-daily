---
layout: default
title: Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification
---

# Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23310" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23310v1</a>
  <a href="https://arxiv.org/pdf/2509.23310.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23310v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23310v1', 'Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Liu, Yongjie Zheng, Yuhan Kang, Mingyang Zhang, Maoguo Gong, Lorenzo Bruzzone

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HaoLiu-XDU/BDGF)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹³è¡¡æ‰©æ•£å¼•å¯¼èåˆæ¡†æ¶ï¼Œè§£å†³å¤šæ¨¡æ€é¥æ„Ÿåˆ†ç±»ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€é¥æ„Ÿ` `åœ°ç‰©åˆ†ç±»` `æ‰©æ•£æ¨¡å‹` `æ¨¡æ€å¹³è¡¡` `ç‰¹å¾èåˆ` `äº’å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®åˆ†ææ–¹æ³•åœ¨èåˆä¸åŒä¼ æ„Ÿå™¨ä¿¡æ¯æ—¶å­˜åœ¨æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼Œå½±å“åˆ†ç±»ç²¾åº¦ã€‚
2. è®ºæ–‡æå‡ºBDGFæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£ç‰¹å¾å¼•å¯¼å¤šåˆ†æ”¯ç½‘ç»œè¿›è¡Œåœ°ç‰©åˆ†ç±»ï¼Œæ ¸å¿ƒåœ¨äºå¹³è¡¡æ¨¡æ€åˆ†å¸ƒå’Œåˆ†å±‚å¼•å¯¼ç‰¹å¾æå–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„åˆ†ç±»æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¹³è¡¡æ‰©æ•£å¼•å¯¼èåˆ(BDGF)æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®åœ°ç‰©åˆ†ç±»ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ‰©æ•£æ¦‚ç‡æ¨¡å‹(DDPMs)é¢„è®­ç»ƒä¸­å¯èƒ½å‡ºç°çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨æ‰©æ•£ç‰¹å¾å¼•å¯¼äº’è¡¥å¤šæ ·æ€§ç‰¹å¾æå–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è‡ªé€‚åº”æ¨¡æ€æ©è”½ç­–ç•¥ï¼Œé¼“åŠ±DDPMsè·å¾—æ¨¡æ€å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒã€‚éšåï¼Œé€šè¿‡èåˆç‰¹å¾èåˆã€åˆ†ç»„é€šé“æ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†å±‚å¼•å¯¼CNNã€Mambaå’ŒTransformerç½‘ç»œè¿›è¡Œç‰¹å¾æå–ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§äº’å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¯¹é½å­ç½‘ç»œçš„æ¦‚ç‡ç†µå’Œç‰¹å¾ç›¸ä¼¼æ€§æ¥å¢å¼ºåˆ†æ”¯é—´çš„åä½œã€‚åœ¨å››ä¸ªå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„åˆ†ç±»æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€é¥æ„Ÿåˆ†ç±»æ—¨åœ¨èåˆæ¥è‡ªä¸åŒä¼ æ„Ÿå™¨ï¼ˆå¦‚å…‰è°±å›¾åƒã€LiDARæ•°æ®ç­‰ï¼‰çš„ä¿¡æ¯ï¼Œä»¥æé«˜åœ°ç‰©åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç›´æ¥é¢„è®­ç»ƒå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹å¯èƒ½å¯¼è‡´æ¨¡æ€ä¸å¹³è¡¡ï¼Œä¾‹å¦‚ï¼Œæ¨¡å‹å¯èƒ½è¿‡åº¦ä¾èµ–å…‰è°±å›¾åƒçš„ä¿¡æ¯ï¼Œè€Œå¿½ç•¥å…¶ä»–æ¨¡æ€çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹æå–çš„ç‰¹å¾æ¥å¼•å¯¼äº’è¡¥å¤šæ ·æ€§ç‰¹å¾çš„æå–ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ åˆ°çš„å¤šæ¨¡æ€æ•°æ®åˆ†å¸ƒï¼Œå¹¶å°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†æ¥å¼•å¯¼å¤šåˆ†æ”¯ç½‘ç»œçš„ç‰¹å¾æå–ã€‚é€šè¿‡è‡ªé€‚åº”æ¨¡æ€æ©è”½ç­–ç•¥å¹³è¡¡ä¸åŒæ¨¡æ€çš„è´¡çŒ®ï¼Œå¹¶è®¾è®¡åˆ†å±‚å¼•å¯¼æœºåˆ¶ï¼Œå°†æ‰©æ•£ç‰¹å¾èå…¥åˆ°CNNã€Mambaå’ŒTransformerç­‰ä¸åŒç±»å‹çš„ç½‘ç»œä¸­ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾èåˆå’Œåˆ†ç±»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBDGFæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) è‡ªé€‚åº”æ¨¡æ€æ©è”½çš„æ‰©æ•£æ¨¡å‹é¢„è®­ç»ƒï¼šé€šè¿‡è‡ªé€‚åº”åœ°æ©è”½éƒ¨åˆ†æ¨¡æ€çš„æ•°æ®ï¼Œé¼“åŠ±æ‰©æ•£æ¨¡å‹å­¦ä¹ æ¨¡æ€å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒã€‚2) åˆ†å±‚æ‰©æ•£å¼•å¯¼çš„ç‰¹å¾æå–ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–çš„ç‰¹å¾ï¼Œé€šè¿‡ç‰¹å¾èåˆã€åˆ†ç»„é€šé“æ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼•å¯¼CNNã€Mambaå’ŒTransformerç­‰å¤šä¸ªåˆ†æ”¯ç½‘ç»œè¿›è¡Œç‰¹å¾æå–ã€‚3) äº’å­¦ä¹ ï¼šé€šè¿‡å¯¹é½ä¸åŒåˆ†æ”¯ç½‘ç»œçš„æ¦‚ç‡ç†µå’Œç‰¹å¾ç›¸ä¼¼æ€§ï¼Œå¢å¼ºåˆ†æ”¯é—´çš„åä½œï¼Œæé«˜æ•´ä½“åˆ†ç±»æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è‡ªé€‚åº”æ¨¡æ€æ©è”½ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹é¢„è®­ç»ƒä¸­çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚2) è®¾è®¡äº†åˆ†å±‚æ‰©æ•£å¼•å¯¼çš„ç‰¹å¾æå–æœºåˆ¶ï¼Œå°†æ‰©æ•£ç‰¹å¾æœ‰æ•ˆåœ°èå…¥åˆ°ä¸åŒç±»å‹çš„ç½‘ç»œä¸­ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„ç‰¹å¾èåˆã€‚3) æå‡ºäº†äº’å­¦ä¹ ç­–ç•¥ï¼Œå¢å¼ºäº†ä¸åŒåˆ†æ”¯ç½‘ç»œä¹‹é—´çš„åä½œï¼Œæé«˜äº†æ•´ä½“åˆ†ç±»æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªé€‚åº”æ¨¡æ€æ©è”½ç­–ç•¥æ ¹æ®ä¸åŒæ¨¡æ€çš„è´¡çŒ®ç¨‹åº¦ï¼ŒåŠ¨æ€è°ƒæ•´æ©è”½æ¯”ä¾‹ã€‚åˆ†å±‚å¼•å¯¼æœºåˆ¶ä¸­ï¼Œç‰¹å¾èåˆé‡‡ç”¨åŠ æƒèåˆçš„æ–¹å¼ï¼Œæƒé‡ç”±æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ å¾—åˆ°ã€‚åˆ†ç»„é€šé“æ³¨æ„åŠ›æœºåˆ¶å°†ç‰¹å¾åˆ†æˆå¤šä¸ªç»„ï¼Œå¹¶å¯¹æ¯ä¸ªç»„åº”ç”¨é€šé“æ³¨æ„åŠ›ï¼Œä»¥æå–æ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾ã€‚äº’å­¦ä¹ ç­–ç•¥é‡‡ç”¨KLæ•£åº¦æŸå¤±å‡½æ•°å¯¹é½ä¸åŒåˆ†æ”¯ç½‘ç»œçš„æ¦‚ç‡ç†µï¼Œå¹¶é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±å‡½æ•°å¯¹é½ç‰¹å¾ç›¸ä¼¼æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BDGFæ¡†æ¶åœ¨å››ä¸ªå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„åˆ†ç±»æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨WHU-Hiæ•°æ®é›†ä¸Šï¼ŒBDGFçš„æ€»ä½“ç²¾åº¦(OA)æ¯”æœ€ä½³åŸºçº¿æé«˜äº†2%ä»¥ä¸Šã€‚æ¶ˆèå®éªŒéªŒè¯äº†è‡ªé€‚åº”æ¨¡æ€æ©è”½ç­–ç•¥å’Œåˆ†å±‚å¼•å¯¼æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç²¾å‡†å†œä¸šã€åŸå¸‚è§„åˆ’ã€ç¾å®³ç›‘æµ‹ç­‰é¢†åŸŸã€‚é€šè¿‡èåˆå¤šæºé¥æ„Ÿæ•°æ®ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«åœ°ç‰©ç±»å‹ï¼Œä¸ºç›¸å…³å†³ç­–æä¾›æ”¯æŒã€‚ä¾‹å¦‚ï¼Œåœ¨ç²¾å‡†å†œä¸šä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•è¯†åˆ«ä¸åŒä½œç‰©çš„ç”Ÿé•¿çŠ¶å†µï¼Œä»è€Œä¼˜åŒ–çŒæº‰å’Œæ–½è‚¥ç­–ç•¥ã€‚åœ¨åŸå¸‚è§„åˆ’ä¸­ï¼Œå¯ä»¥ç”¨äºåœŸåœ°åˆ©ç”¨åˆ†ç±»å’ŒåŸå¸‚æ‰©å¼ ç›‘æµ‹ã€‚åœ¨ç¾å®³ç›‘æµ‹ä¸­ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°ç¾å®³çš„å½±å“èŒƒå›´å’Œç¨‹åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.

