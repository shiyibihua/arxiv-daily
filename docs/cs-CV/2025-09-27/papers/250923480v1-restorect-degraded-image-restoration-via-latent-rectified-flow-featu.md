---
layout: default
title: RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation
---

# RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23480v1</a>
  <a href="https://arxiv.org/pdf/2509.23480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23480v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23480v1', 'RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shourya Verma, Mengbo Wang, Nadia Atallah Lanman, Ananth Grama

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RestoRectï¼šåŸºäºæ½œåœ¨ç©ºé—´æ ¡æ­£æµä¸ç‰¹å¾è’¸é¦çš„å›¾åƒå¤åŸæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `å›¾åƒå¤åŸ` `ç‰¹å¾è’¸é¦` `æ ¡æ­£æµ` `Transformer` `çŸ¥è¯†è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒå¤åŸæ¨¡å‹åœ¨é€Ÿåº¦å’Œæ€§èƒ½ä¹‹é—´å­˜åœ¨ç“¶é¢ˆï¼Œéš¾ä»¥å…¼é¡¾å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. RestoRectåˆ©ç”¨æ ¡æ­£æµå°†ç‰¹å¾è’¸é¦è½¬åŒ–ä¸ºç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆæˆé«˜è´¨é‡ç‰¹å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRestoRectåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„å¤åŸæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å›¾åƒå¤åŸæ–¹æ³•é¢ä¸´æ€§èƒ½ä¸é€Ÿåº¦çš„æƒè¡¡ã€‚é«˜æ€§èƒ½æ¨¡å‹é€Ÿåº¦æ…¢ï¼Œè€Œå¿«é€Ÿæ¨¡å‹æ•ˆæœå·®ã€‚çŸ¥è¯†è’¸é¦å¯ä»¥å°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ï¼Œä½†ç°æœ‰çš„é™æ€ç‰¹å¾åŒ¹é…æ–¹æ³•æ— æ³•æ•æ‰Transformeræ¶æ„åŠ¨æ€ç”Ÿæˆç‰¹å¾çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†RestoRectï¼Œä¸€ç§æ–°é¢–çš„æ½œåœ¨ç©ºé—´æ ¡æ­£æµç‰¹å¾è’¸é¦æ–¹æ³•ï¼Œç”¨äºå¤åŸé€€åŒ–å›¾åƒã€‚è¯¥æ–¹æ³•å°†ç‰¹å¾è’¸é¦é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªç”Ÿæˆè¿‡ç¨‹ï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„å¯å­¦ä¹ è½¨è¿¹åˆæˆæ•™å¸ˆæ¨¡å‹è´¨é‡çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†åŸºäºç‰©ç†çš„Retinexç†è®ºåˆ†è§£ã€å¯å­¦ä¹ çš„å„å‘å¼‚æ€§æ‰©æ•£çº¦æŸå’Œä¸‰è§’è‰²å½©ç©ºé—´æåŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ç‰¹å¾å±‚æå–æŸå¤±ï¼Œé€šè¿‡äº¤å‰å½’ä¸€åŒ–çš„Transformerç‰¹å¾å¯¹é½å’ŒåŸºäºç™¾åˆ†ä½æ•°çš„å¼‚å¸¸å€¼æ£€æµ‹ï¼Œå®ç°ä¸åŒç½‘ç»œæ¶æ„ä¹‹é—´é²æ£’çš„çŸ¥è¯†è¿ç§»ã€‚RestoRectå®ç°äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ã€æ›´å¿«çš„æ”¶æ•›å’Œæ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†å¤åŸè´¨é‡ã€‚æˆ‘ä»¬åœ¨15ä¸ªå›¾åƒå¤åŸæ•°æ®é›†ä¸Šï¼Œæ¶µç›–4ä¸ªä»»åŠ¡ï¼Œ8ä¸ªæŒ‡æ ‡ä¸Šå±•ç¤ºäº†ä¼˜è¶Šçš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå›¾åƒå¤åŸæ—¨åœ¨ä»é€€åŒ–çš„å›¾åƒä¸­æ¢å¤å‡ºæ¸…æ™°çš„å›¾åƒã€‚ç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šå¸¸é¢ä¸´é€Ÿåº¦å’Œæ€§èƒ½çš„æƒè¡¡ã€‚é«˜æ€§èƒ½æ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜ï¼Œæ¨ç†é€Ÿåº¦æ…¢ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ï¼›è€Œå¿«é€Ÿæ¨¡å‹é€šå¸¸æ€§èƒ½è¾ƒå·®ï¼Œæ— æ³•æœ‰æ•ˆå»é™¤å›¾åƒä¸­çš„å™ªå£°ã€æ¨¡ç³Šç­‰é€€åŒ–å› ç´ ã€‚çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å¸¸ç”¨çš„æ¨¡å‹åŠ é€Ÿæ–¹æ³•ï¼Œä½†ç°æœ‰çš„é™æ€ç‰¹å¾åŒ¹é…æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰Transformeræ¶æ„åŠ¨æ€ç”Ÿæˆç‰¹å¾çš„è¿‡ç¨‹ï¼Œå¯¼è‡´è’¸é¦æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRestoRectçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç‰¹å¾è’¸é¦é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨æ ¡æ­£æµï¼ˆRectified Flowï¼‰å­¦ä¹ æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„ç‰¹å¾æ˜ å°„å…³ç³»ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­æ„å»ºå¯å­¦ä¹ çš„è½¨è¿¹ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥é€æ­¥åˆæˆæ•™å¸ˆæ¨¡å‹è´¨é‡çš„ç‰¹å¾ï¼Œä»è€Œå®ç°çŸ¥è¯†è¿ç§»ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰Transformeræ¶æ„çš„åŠ¨æ€ç‰¹å¾ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜è’¸é¦æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRestoRectæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŸºäºRetinexç†è®ºçš„å›¾åƒåˆ†è§£æ¨¡å—ï¼Œå°†å›¾åƒåˆ†è§£ä¸ºåå°„åˆ†é‡å’Œå…‰ç…§åˆ†é‡ï¼›2) ç‰¹å¾æå–æ¨¡å—ï¼Œä½¿ç”¨Transformeræ¶æ„æå–å›¾åƒç‰¹å¾ï¼›3) æ ¡æ­£æµç‰¹å¾è’¸é¦æ¨¡å—ï¼Œåˆ©ç”¨æ ¡æ­£æµå­¦ä¹ æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„ç‰¹å¾æ˜ å°„å…³ç³»ï¼›4) ç‰¹å¾å±‚æå–æŸå¤±ï¼Œç”¨äºå¯¹é½æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾ï¼›5) å¯å­¦ä¹ çš„å„å‘å¼‚æ€§æ‰©æ•£çº¦æŸå’Œä¸‰è§’è‰²å½©ç©ºé—´æåŒ–ï¼Œç”¨äºæé«˜å›¾åƒå¤åŸè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šRestoRectçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†æ ¡æ­£æµå¼•å…¥ç‰¹å¾è’¸é¦ï¼Œå°†ç‰¹å¾è’¸é¦è½¬åŒ–ä¸ºä¸€ä¸ªç”Ÿæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰Transformeræ¶æ„çš„åŠ¨æ€ç‰¹å¾ç”Ÿæˆè¿‡ç¨‹ï¼›2) æå‡ºäº†ç‰¹å¾å±‚æå–æŸå¤±ï¼Œé€šè¿‡äº¤å‰å½’ä¸€åŒ–çš„Transformerç‰¹å¾å¯¹é½å’ŒåŸºäºç™¾åˆ†ä½æ•°çš„å¼‚å¸¸å€¼æ£€æµ‹ï¼Œå®ç°ä¸åŒç½‘ç»œæ¶æ„ä¹‹é—´é²æ£’çš„çŸ¥è¯†è¿ç§»ï¼›3) ç»“åˆäº†Retinexç†è®ºã€å¯å­¦ä¹ çš„å„å‘å¼‚æ€§æ‰©æ•£çº¦æŸå’Œä¸‰è§’è‰²å½©ç©ºé—´æåŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†å›¾åƒå¤åŸè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ ¡æ­£æµç‰¹å¾è’¸é¦æ¨¡å—ä¸­ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„ç¥ç»ç½‘ç»œæ¥å‚æ•°åŒ–æ ¡æ­£æµçš„è½¨è¿¹ã€‚ç‰¹å¾å±‚æå–æŸå¤±é‡‡ç”¨äº¤å‰å½’ä¸€åŒ–æ–¹æ³•ï¼Œå¯¹æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥å‡å°‘ä¸åŒç½‘ç»œæ¶æ„ä¹‹é—´çš„å·®å¼‚ã€‚åŸºäºç™¾åˆ†ä½æ•°çš„å¼‚å¸¸å€¼æ£€æµ‹ç”¨äºå»é™¤ç‰¹å¾ä¸­çš„å™ªå£°å’Œå¼‚å¸¸å€¼ï¼Œæé«˜ç‰¹å¾å¯¹é½çš„å‡†ç¡®æ€§ã€‚Retinexåˆ†è§£æ¨¡å—é‡‡ç”¨ä¼ ç»Ÿçš„Retinexç®—æ³•ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œä»¥é€‚åº”å›¾åƒå¤åŸä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RestoRectåœ¨15ä¸ªå›¾åƒå¤åŸæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†4ä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒå»å™ªã€å›¾åƒå»æ¨¡ç³Šã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå›¾åƒä¿®å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRestoRectåœ¨8ä¸ªæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œå¹¶ä¸”å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ¨ç†é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒå»å™ªä»»åŠ¡ä¸­ï¼ŒRestoRectç›¸æ¯”äºç°æœ‰æ–¹æ³•ï¼ŒPSNRæŒ‡æ ‡æå‡äº†0.5-1dBï¼Œæ¨ç†é€Ÿåº¦æå‡äº†20%-30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RestoRectåœ¨å›¾åƒå¤åŸé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šç›‘æ§è§†é¢‘å¢å¼ºã€åŒ»å­¦å›¾åƒå¤„ç†ã€è€ç…§ç‰‡ä¿®å¤ã€å«æ˜Ÿå›¾åƒå¢å¼ºç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æé«˜å›¾åƒçš„æ¸…æ™°åº¦å’Œè§†è§‰è´¨é‡ï¼Œä¸ºåç»­çš„å›¾åƒåˆ†æå’Œç†è§£æä¾›æ›´å¥½çš„åŸºç¡€ã€‚æ­¤å¤–ï¼ŒRestoRectçš„å¿«é€Ÿæ¨ç†é€Ÿåº¦ä½¿å…¶é€‚ç”¨äºå®æ—¶å›¾åƒå¤åŸåº”ç”¨ï¼Œä¾‹å¦‚ï¼šè§†é¢‘ä¼šè®®ã€ç›´æ’­ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current approaches for restoration of degraded images face a critical trade-off: high-performance models are too slow for practical use, while fast models produce poor results. Knowledge distillation transfers teacher knowledge to students, but existing static feature matching methods cannot capture how modern transformer architectures dynamically generate features. We propose 'RestoRect', a novel Latent Rectified Flow Feature Distillation method for restoring degraded images. We apply rectified flow to reformulate feature distillation as a generative process where students learn to synthesize teacher-quality features through learnable trajectories in latent space. Our framework combines Retinex theory for physics-based decomposition with learnable anisotropic diffusion constraints, and trigonometric color space polarization. We introduce a Feature Layer Extraction loss for robust knowledge transfer between different network architectures through cross-normalized transformer feature alignment with percentile-based outlier detection. RestoRect achieves better training stability, and faster convergence and inference while preserving restoration quality. We demonstrate superior results across 15 image restoration datasets, covering 4 tasks, on 8 metrics.

