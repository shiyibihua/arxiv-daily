---
layout: default
title: CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP
---

# CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23098" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23098v1</a>
  <a href="https://arxiv.org/pdf/2509.23098.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23098v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23098v1', 'CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Na Min An, Inha Kang, Minhyun Lee, Hyunjung Shim

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: 28 pages, 22 Figures, 11 Tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CoPatchï¼šåˆ©ç”¨CLIPä¸­æœªå¼€å‘çš„ spatial knowledge å®ç°é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æŒ‡ä»£å›¾åƒåˆ†å‰²` `é›¶æ ·æœ¬å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `CLIP` `ç©ºé—´æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŒ‡ä»£å›¾åƒåˆ†å‰²æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨CLIPç­‰VLMä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. CoPatché€šè¿‡æŒ–æ˜CLIPå†…éƒ¨ç»„ä»¶ï¼Œå¢å¼ºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„ç©ºé—´è¡¨ç¤ºï¼Œæå‡ç©ºé—´å®šä½èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCoPatchåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²çš„æ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´å®šä½å¯¹äºæŒ‡ä»£å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰è‡³å…³é‡è¦ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨å®šä½ç”±è¯­è¨€æè¿°çš„å¯¹è±¡ã€‚ç°æœ‰çš„åŸºç¡€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œæ“…é•¿å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼Œä½†åœ¨ç†è§£ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨è¯­è¨€æµä¸­ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨ä¸»è¦åè¯çŸ­è¯­ï¼Œä»è€Œå‰Šå¼±äº†ä¸Šä¸‹æ–‡tokenã€‚åœ¨è§†è§‰æµä¸­ï¼ŒCLIPä¸ºå…·æœ‰ä¸åŒç©ºé—´å¸ƒå±€çš„å›¾åƒç”Ÿæˆç›¸ä¼¼çš„ç‰¹å¾ï¼Œå¯¼è‡´å¯¹ç©ºé—´ç»“æ„çš„æ•æ„Ÿæ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†CoPatchï¼Œä¸€ä¸ªé›¶æ ·æœ¬RISæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å†…éƒ¨æ¨¡å‹ç»„ä»¶æ¥å¢å¼ºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¸­çš„ç©ºé—´è¡¨ç¤ºã€‚å¯¹äºè¯­è¨€ï¼ŒCoPatché€šè¿‡ç»“åˆæºå¸¦ç©ºé—´çº¿ç´¢çš„ä¸Šä¸‹æ–‡tokenæ¥æ„å»ºæ··åˆæ–‡æœ¬ç‰¹å¾ã€‚å¯¹äºè§†è§‰ï¼Œå®ƒä½¿ç”¨æˆ‘ä»¬ä»ä¸­é—´å±‚å‘ç°çš„æ–°è·¯å¾„æå–patchçº§åˆ«çš„å›¾åƒç‰¹å¾ï¼Œå…¶ä¸­ç©ºé—´ç»“æ„å¾—åˆ°æ›´å¥½çš„ä¿ç•™ã€‚è¿™äº›å¢å¼ºçš„ç‰¹å¾è¢«èåˆåˆ°èšç±»çš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦å›¾CoMapä¸­ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„maské€‰æ‹©ã€‚å› æ­¤ï¼ŒCoPatchåœ¨RefCOCOã€RefCOCO+ã€RefCOCOgå’ŒPhraseCutä¸Šæ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬RISçš„ç©ºé—´å®šä½æ€§èƒ½ï¼ˆ+2-7 mIoUï¼‰ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ¢å¤å’Œåˆ©ç”¨VLMsä¸­å›ºæœ‰åµŒå…¥çš„æœªå¼€å‘ç©ºé—´çŸ¥è¯†çš„é‡è¦æ€§ï¼Œä»è€Œä¸ºé›¶æ ·æœ¬RISå¼€è¾Ÿäº†æœºä¼šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨CLIPç­‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­è•´å«çš„ç©ºé—´ä¿¡æ¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨æ–‡æœ¬ä¸­çš„ä¸»è¦åè¯çŸ­è¯­ï¼Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡tokenæä¾›çš„ç©ºé—´çº¿ç´¢ï¼Œå¹¶ä¸”CLIPå¯¹å…·æœ‰ä¸åŒç©ºé—´å¸ƒå±€çš„å›¾åƒäº§ç”Ÿç›¸ä¼¼çš„è§†è§‰ç‰¹å¾ï¼Œå¯¼è‡´ç©ºé—´å®šä½èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æŒ–æ˜å¹¶å¢å¼ºCLIPæ¨¡å‹å†…éƒ¨çš„æ–‡æœ¬å’Œå›¾åƒç‰¹å¾ï¼Œä»¥æå‡å…¶ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡èåˆä¸Šä¸‹æ–‡tokenæ¥ä¸°å¯Œæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶ä»CLIPçš„ä¸­é—´å±‚æå–patchçº§åˆ«çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™ç©ºé—´ç»“æ„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoPatchæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ··åˆæ–‡æœ¬ç‰¹å¾æå–æ¨¡å—ï¼Œé€šè¿‡èåˆä¸Šä¸‹æ–‡tokenæ¥å¢å¼ºæ–‡æœ¬ç‰¹å¾ï¼›2) Patchçº§åˆ«å›¾åƒç‰¹å¾æå–æ¨¡å—ï¼Œä»CLIPçš„ä¸­é—´å±‚æå–patchçº§åˆ«çš„å›¾åƒç‰¹å¾ï¼›3) å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦å›¾ï¼ˆCoMapï¼‰æ„å»ºæ¨¡å—ï¼Œå°†å¢å¼ºçš„æ–‡æœ¬å’Œå›¾åƒç‰¹å¾èåˆåˆ°CoMapä¸­ï¼›4) Maské€‰æ‹©æ¨¡å—ï¼ŒåŸºäºCoMapé€‰æ‹©æœ€ä½³çš„åˆ†å‰²maskã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§æ··åˆæ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†ä¸Šä¸‹æ–‡tokenä¸­çš„ç©ºé—´ä¿¡æ¯ï¼›2) å‘ç°äº†ä¸€ç§ä»CLIPä¸­é—´å±‚æå–patchçº§åˆ«å›¾åƒç‰¹å¾çš„æœ‰æ•ˆè·¯å¾„ï¼Œæ›´å¥½åœ°ä¿ç•™äº†ç©ºé—´ç»“æ„ä¿¡æ¯ï¼›3) æ„å»ºäº†èšç±»çš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦å›¾ï¼ˆCoMapï¼‰ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„maské€‰æ‹©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–‡æœ¬ç‰¹å¾æå–æ–¹é¢ï¼Œè®ºæ–‡æ²¡æœ‰è¯¦ç»†è¯´æ˜ä¸Šä¸‹æ–‡tokenèåˆçš„å…·ä½“æ–¹å¼ï¼Œä½†å¼ºè°ƒäº†å…¶é‡è¦æ€§ã€‚åœ¨å›¾åƒç‰¹å¾æå–æ–¹é¢ï¼Œè®ºæ–‡æåˆ°å‘ç°äº†ä¸€ç§ä»ä¸­é—´å±‚æå–patchçº§åˆ«ç‰¹å¾çš„â€œè·¯å¾„â€ï¼Œä½†æ²¡æœ‰ç»™å‡ºå…·ä½“çš„ç½‘ç»œå±‚æ•°æˆ–æ“ä½œç»†èŠ‚ã€‚CoMapçš„æ„å»ºå’Œmaské€‰æ‹©çš„å…·ä½“ç®—æ³•ä¹Ÿæœªè¯¦ç»†æè¿°ï¼Œè¿™äº›éƒ½å±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoPatchåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆRefCOCOã€RefCOCO+ã€RefCOCOgå’ŒPhraseCutï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒmIoUæŒ‡æ ‡å¹³å‡æå‡äº†2-7ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoPatchçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å›¾åƒç¼–è¾‘ã€è§†è§‰å¯¼èˆªã€æœºå™¨äººäº¤äº’ç­‰ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å›¾åƒç¼–è¾‘ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å®šè¦ç¼–è¾‘çš„å¯¹è±¡åŠå…¶ä½ç½®ï¼›åœ¨è§†è§‰å¯¼èˆªä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®æŒ‡ä»¤æ‰¾åˆ°ç‰¹å®šä½ç½®çš„ç›®æ ‡ç‰©ä½“ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨äººæœºäº¤äº’å’Œæ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial grounding is crucial for referring image segmentation (RIS), where the goal of the task is to localize an object described by language. Current foundational vision-language models (VLMs), such as CLIP, excel at aligning images and text but struggle with understanding spatial relationships. Within the language stream, most existing methods often focus on the primary noun phrase when extracting local text features, undermining contextual tokens. Within the vision stream, CLIP generates similar features for images with different spatial layouts, resulting in limited sensitivity to spatial structure. To address these limitations, we propose \textsc{CoPatch}, a zero-shot RIS framework that leverages internal model components to enhance spatial representations in both text and image modalities. For language, \textsc{CoPatch} constructs hybrid text features by incorporating context tokens carrying spatial cues. For vision, it extracts patch-level image features using our novel path discovered from intermediate layers, where spatial structure is better preserved. These enhanced features are fused into a clustered image-text similarity map, \texttt{CoMap}, enabling precise mask selection. As a result, \textsc{CoPatch} significantly improves spatial grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+ 2--7 mIoU) without requiring any additional training. Our findings underscore the importance of recovering and leveraging the untapped spatial knowledge inherently embedded in VLMs, thereby paving the way for opportunities in zero-shot RIS.

