---
layout: default
title: GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval
---

# GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23370" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23370v1</a>
  <a href="https://arxiv.org/pdf/2509.23370.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23370v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23370v1', 'GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhaohua Zhang, Jianhuan Zhuo, Muxi Chen, Chenchen Zhao, Wenyu Jiang, Tianwen Jiang, Mingyang Chen, Yu Tang, Qiuyong Xiao, Jihong Zhang, Zhixun Su

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Chinese0123456/GRAPE.git)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**GRAPEÔºöÈÄöËøáÊéíÂ∫èÁõëÁù£Êü•ËØ¢ÈáçÂÜôÔºåÊèêÂçáÊ£ÄÁ¥¢ÊïàÊûú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êü•ËØ¢ÈáçÂÜô` `Ê£ÄÁ¥¢ÊéíÂ∫è` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÂàÜÂ∏ÉÂÅèÁßª` `Á≠ñÁï•‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§öËØ≠Ë®Ä„ÄÅÈïøÊñáÊú¨ÊàñÂ§öÊ®°ÊÄÅÊü•ËØ¢Êó∂ÔºåCLIPÊ®°ÂûãÊÄßËÉΩ‰∏ãÈôçÔºå‰∏îÈáçÊñ∞ËÆ≠ÁªÉÊàêÊú¨È´òÊòÇ„ÄÇ
2. GRAPEÂà©Áî®GRPOÂ∞ÜÊü•ËØ¢ËΩ¨Êç¢‰∏∫Êõ¥Á¨¶ÂêàÊ£ÄÁ¥¢Âô®ËÆ≠ÁªÉÂàÜÂ∏ÉÁöÑÂΩ¢ÂºèÔºåÂº•ÂêàÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÊèêÂçáÊ£ÄÁ¥¢ÊïàÊûú„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGRAPEÂú®Â§öËØ≠Ë®Ä„ÄÅÈïøÂ∫¶ÂíåÂ§öÊ®°ÊÄÅÂ∑ÆÂºÇÁ≠âÂàÜÂ∏ÉÂÅèÁßª‰∏ãÔºåRecall@10Âπ≥ÂùáÊèêÈ´ò‰∫Ü4.9%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

CLIPÊ®°ÂûãÈÄöËøáÂú®Áªü‰∏ÄÂµåÂÖ•Á©∫Èó¥‰∏≠ÂØπÈΩêÊñáÊú¨ÂíåÂõæÂÉèÊï∞ÊçÆÔºåÂ∑≤Êàê‰∏∫Â§ßËßÑÊ®°Ê£ÄÁ¥¢Á≥ªÁªüÁöÑÂü∫Áü≥„ÄÇÁÑ∂ËÄåÔºåÂΩìËæìÂÖ•ÂàÜÂ∏ÉÂÅèÁ¶ªÂÖ∂ËÆ≠ÁªÉËØ≠ÊñôÂ∫ìÊó∂Ôºå‰æãÂ¶ÇÊü•ËØ¢ÂÖ∑ÊúâÂ§öËØ≠Ë®Ä„ÄÅÈïøÊñáÊú¨ÊàñÂ§öÊ®°ÊÄÅÂ∑ÆÂºÇÊó∂ÔºåCLIPÁöÑÊïàÊûú‰ºö‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜÈÅøÂÖç‰ª£‰ª∑È´òÊòÇÁöÑÈáçÊñ∞ËÆ≠ÁªÉÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈááÁî®Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊü•ËØ¢ÈáçÂÜôÁ≠ñÁï•ÔºåÊó®Âú®ÁºìËß£Êü•ËØ¢Á∫ßÂà´ÁöÑÂàÜÂ∏ÉÂ∑ÆË∑ù„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁº∫‰πèÁõëÁù£‰ø°Âè∑ÔºåLLMÊó†Ê≥ïÁîüÊàêÊúÄÈÄÇÂêàËÆ≠ÁªÉÂàÜÂ∏ÉÁöÑÈáçÂÜôÊü•ËØ¢„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜGRAPEÔºàGrouped Ranking-Aware Policy Optimization EnhancementÔºâÔºå‰∏ÄÁßçÂç≥ÊèíÂç≥Áî®ÁöÑÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂ∞ÜÊéíÂ∫è‰ø°Âè∑ËûçÂÖ•Âà∞Âü∫‰∫éLLMÁöÑÊ£ÄÁ¥¢ÂºïÂØºÊü•ËØ¢ÈáçÂÜô‰∏≠„ÄÇGRAPEÈÄöËøáÂ∞ÜÊü•ËØ¢ËΩ¨Êç¢‰∏∫Êõ¥Á¨¶ÂêàÊ£ÄÁ¥¢Âô®ËÆ≠ÁªÉÂàÜÂ∏ÉÁöÑÂΩ¢ÂºèÔºåÊù•Âº•ÂêàÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÂåÖÊã¨ÈïøÂ∫¶„ÄÅÂ§öËØ≠Ë®ÄÂíåÊ®°ÊÄÅÂÅèÁßª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Áõ¥Êé•‰ΩøÁî®Áõ∏‰ººÂ∫¶ÂàÜÊï∞ÂæÆË∞ÉLLMÂèØËÉΩÂØºËá¥ÁöÑÂàÜÊï∞ËÜ®ËÉÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËØ≠ÊñôÂ∫ìÁõ∏ÂÖ≥ÁöÑÂü∫‰∫éÊéíÂ∫èÁöÑÂ•ñÂä±ÔºåÂÆÉÂú®ÊäëÂà∂ËôöÂÅáÂàÜÊï∞ËÜ®ËÉÄÁöÑÂêåÊó∂ÔºåÊòæÂºèÂú∞Â∞Ü‰ºòÂåñ‰∏éÊéíÂ∫èÊåáÊ†áÂØπÈΩê„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåGRAPEÂú®ÂàÜÂ∏ÉÂÅèÁßª‰∏ãÔºàÂåÖÊã¨Â§öËØ≠Ë®ÄÂ∑ÆÂºÇ„ÄÅÈïøÂ∫¶Â∑ÆÂºÇÂíåÂ§öÊ®°ÊÄÅÂ∑ÆÂºÇÔºâÂßãÁªàÊèêÈ´òÊ£ÄÁ¥¢ÊÄßËÉΩÔºåÂú®Recall@10‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü4.9%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥CLIPÊ®°ÂûãÂú®Â§ÑÁêÜ‰∏éËÆ≠ÁªÉÊï∞ÊçÆÂàÜÂ∏ÉÂ≠òÂú®Â∑ÆÂºÇÁöÑÊü•ËØ¢Êó∂ÔºåÊ£ÄÁ¥¢ÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éLLMËøõË°åÊü•ËØ¢ÈáçÂÜôÔºå‰ΩÜÁî±‰∫éÁº∫‰πèÊúâÊïàÁöÑÁõëÁù£‰ø°Âè∑ÔºåLLMÈöæ‰ª•ÁîüÊàêÊúÄ‰Ω≥ÁöÑÈáçÂÜôÊü•ËØ¢ÔºåÂØºËá¥Ê£ÄÁ¥¢ÊïàÊûúÊèêÂçáÊúâÈôê„ÄÇÊ≠§Â§ñÔºåÁõ¥Êé•‰ΩøÁî®Áõ∏‰ººÂ∫¶ÂàÜÊï∞ÂæÆË∞ÉLLMÂÆπÊòìÂØºËá¥ÂàÜÊï∞ËÜ®ËÉÄÔºå‰ΩøÂæóÊ®°ÂûãÊó†Ê≥ïÂå∫ÂàÜ‰∏çÂêåÂÄôÈÄâÁªìÊûúÁöÑÁúüÂÆûÁõ∏ÂÖ≥ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÊéíÂ∫è‰ø°Âè∑Êù•ÊåáÂØºLLMËøõË°åÊü•ËØ¢ÈáçÂÜôÔºå‰ªéËÄåÁîüÊàêÊõ¥Á¨¶ÂêàÊ£ÄÁ¥¢Âô®ËÆ≠ÁªÉÂàÜÂ∏ÉÁöÑÊü•ËØ¢„ÄÇÈÄöËøáÂºïÂÖ•ÊéíÂ∫èÊÑüÁü•ÁöÑÁ≠ñÁï•‰ºòÂåñÔºåGRAPEËÉΩÂ§üÊòæÂºèÂú∞Â∞Ü‰ºòÂåñÁõÆÊ†á‰∏éÊéíÂ∫èÊåáÊ†áÂØπÈΩêÔºå‰ªéËÄåÊèêÈ´òÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇÂêåÊó∂Ôºå‰∏∫‰∫ÜËß£ÂÜ≥ÂàÜÊï∞ËÜ®ËÉÄÈóÆÈ¢òÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËØ≠ÊñôÂ∫ìÁõ∏ÂÖ≥ÁöÑÂü∫‰∫éÊéíÂ∫èÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÊäëÂà∂ËôöÂÅáÁöÑÈ´òÂàÜÔºåÁ°Æ‰øùÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞ÁúüÂÆûÁöÑÊéíÂ∫èÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGRAPEÊòØ‰∏Ä‰∏™Âç≥ÊèíÂç≥Áî®ÁöÑÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑÂü∫‰∫éLLMÁöÑÊü•ËØ¢ÈáçÂÜôÁ≠ñÁï•Áõ∏ÁªìÂêà„ÄÇÂÖ∂Êï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Ôºö1Ôºâ‰ΩøÁî®LLMÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÈáçÂÜôÊü•ËØ¢Ôºõ2Ôºâ‰ΩøÁî®Ê£ÄÁ¥¢Âô®ÂØπËøô‰∫õÂÄôÈÄâÊü•ËØ¢ËøõË°åÊéíÂ∫èÔºåÂπ∂ËÆ°ÁÆóÁõ∏Â∫îÁöÑÁõ∏‰ººÂ∫¶ÂàÜÊï∞Ôºõ3ÔºâÂü∫‰∫éÊéíÂ∫èÁªìÊûúÂíåÁõ∏‰ººÂ∫¶ÂàÜÊï∞ÔºåËÆ°ÁÆóËØ≠ÊñôÂ∫ìÁõ∏ÂÖ≥ÁöÑÂü∫‰∫éÊéíÂ∫èÁöÑÂ•ñÂä±Ôºõ4Ôºâ‰ΩøÁî®ËØ•Â•ñÂä±Êù•ÂæÆË∞ÉLLMÔºå‰ΩøÂÖ∂ËÉΩÂ§üÁîüÊàêÊõ¥Â•ΩÁöÑÈáçÂÜôÊü•ËØ¢„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGRAPEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊéíÂ∫è‰ø°Âè∑Êù•ÁõëÁù£LLMÁöÑÊü•ËØ¢ÈáçÂÜôËøáÁ®ã„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåGRAPEËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Ê£ÄÁ¥¢Âô®ÁöÑÂèçÈ¶à‰ø°ÊÅØÔºå‰ªéËÄåÁîüÊàêÊõ¥Á¨¶ÂêàÊ£ÄÁ¥¢Âô®ËÆ≠ÁªÉÂàÜÂ∏ÉÁöÑÊü•ËØ¢„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÊèêÂá∫ÁöÑËØ≠ÊñôÂ∫ìÁõ∏ÂÖ≥ÁöÑÂü∫‰∫éÊéíÂ∫èÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÊäëÂà∂ÂàÜÊï∞ËÜ®ËÉÄÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊéíÂ∫èËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöGRAPEÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1Ôºâ‰ΩøÁî®GRPOÔºàGrouped Ranking-Aware Policy OptimizationÔºâÁÆóÊ≥ïÊù•‰ºòÂåñLLMÁöÑÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÁîüÊàêÊõ¥Â•ΩÁöÑÈáçÂÜôÊü•ËØ¢Ôºõ2ÔºâËÆæËÆ°‰∫Ü‰∏ÄÁßçËØ≠ÊñôÂ∫ìÁõ∏ÂÖ≥ÁöÑÂü∫‰∫éÊéíÂ∫èÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞ËÄÉËôë‰∫ÜÂÄôÈÄâÊü•ËØ¢ÁöÑÊéíÂ∫è‰ΩçÁΩÆÂíåÁõ∏‰ººÂ∫¶ÂàÜÊï∞Ôºå‰ªéËÄåËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞ÂÄôÈÄâÊü•ËØ¢ÁöÑË¥®ÈáèÔºõ3Ôºâ‰ΩøÁî®Ë¥üÈááÊ†∑ÊäÄÊúØÊù•ÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÔºåÂπ∂ÈÅøÂÖçÊ®°ÂûãËøáÂ∫¶ÊãüÂêàËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGRAPEÂú®ÂêÑÁßçÂàÜÂ∏ÉÂÅèÁßª‰∏ãÂùáËÉΩÊòæËëóÊèêÈ´òÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇÂú®Â§öËØ≠Ë®ÄÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÔºåGRAPEÂú®Flickr30k-CN„ÄÅCVLUEÂíåXM3600Êï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÂú®ÈïøÊñáÊú¨Ê£ÄÁ¥¢‰ªªÂä°‰∏≠ÔºåGRAPEÂú®WikipediaÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÂú®Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÔºåGRAPEÂú®CIRRÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåGRAPEÂú®Recall@10‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü4.9%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GRAPEÂèØÂ∫îÁî®‰∫éÂêÑÁßçÂ§ßËßÑÊ®°Ê£ÄÁ¥¢Á≥ªÁªüÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§öËØ≠Ë®Ä„ÄÅÈïøÊñáÊú¨ÊàñÂ§öÊ®°ÊÄÅÊü•ËØ¢ÁöÑÂú∫ÊôØ‰∏ã„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Áî®‰∫éË∑®ËØ≠Ë®ÄÂõæÂÉèÊ£ÄÁ¥¢„ÄÅÈïøÊñáÊ°£Ê£ÄÁ¥¢ÂíåÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢Á≠âÈ¢ÜÂüü„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÊ£ÄÁ¥¢Á≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåÊèêÈ´òÁî®Êà∑‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The CLIP model has become a cornerstone of large-scale retrieval systems by aligning text and image data in a unified embedding space. Despite its simplicity and efficiency, CLIP struggles when applied to tasks whose input distributions diverge from its training corpus, such as queries with multilingual, long-form, or multimodal differences. To avoid costly retraining, existing methods mainly adopt query-rewriting strategies with large language models (LLMs), aiming to mitigate distribution gaps at the query level. However, due to the lack of supervision signals, LLMs fail to generate the optimal one that fits the training distribution. We address this challenge with GRAPE (Grouped Ranking-Aware Policy Optimization Enhancement), a plug-and-play enhancement approach that incorporates ranking signals into retrieval-guided query rewriting with LLMs. Intuitively, GRAPE proposes to leverage GRPO to bridge distributional differences -- including length, multilingual, and modality shifts -- by transforming queries into forms better aligned with the retriever's training distribution. However, our preliminary experiment finds that naively finetuning LLM with similarity scores can lead to score inflation, where nearly all candidates are assigned unexpectedly high scores regardless of their true relevance. To address score inflation, we propose a corpus-relative ranking-based reward, which explicitly aligns optimization with ranking metrics while suppressing spurious score inflation. Extensive experiments demonstrate that GRAPE consistently improves retrieval performance under distributional shifts -- including multilingual differences (Flickr30k-CN, CVLUE, XM3600), length differences (Wikipedia), and multimodal differences (CIRR) -- achieving an average improvement of 4.9\% in Recall\@10. The code is available at https://github.com/Chinese0123456/GRAPE.git

