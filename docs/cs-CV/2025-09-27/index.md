---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-27
---

# cs.CVï¼ˆ2025-09-27ï¼‰

ğŸ“Š å…± **28** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251000041v1-culture-in-a-frame-c3b-as-a-comic-based-benchmark-for-multimodal-cul.html">Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness</a></td>
  <td>æå‡ºC$^3$Bæ¼«ç”»è·¨æ–‡åŒ–åŸºå‡†ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–æ„ŸçŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00041v1" data-paper-url="./papers/251000041v1-culture-in-a-frame-c3b-as-a-comic-based-benchmark-for-multimodal-cul.html" onclick="toggleFavorite(this, '2510.00041v1', 'Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250923014v1-planning-with-unified-multimodal-models.html">Planning with Unified Multimodal Models</a></td>
  <td>Uni-Planï¼šåŸºäºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è§„åˆ’æ¡†æ¶ï¼Œæå‡é•¿ç¨‹å†³ç­–ä»»åŠ¡æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23014v1" data-paper-url="./papers/250923014v1-planning-with-unified-multimodal-models.html" onclick="toggleFavorite(this, '2509.23014v1', 'Planning with Unified Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250923344v1-dentvlm-a-multimodal-vision-language-model-for-comprehensive-dental-.html">DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice</a></td>
  <td>DentVLMï¼šç”¨äºå…¨é¢ç‰™ç§‘è¯Šæ–­å’Œå¢å¼ºä¸´åºŠå®è·µçš„å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23344v1" data-paper-url="./papers/250923344v1-dentvlm-a-multimodal-vision-language-model-for-comprehensive-dental-.html" onclick="toggleFavorite(this, '2509.23344v1', 'DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250923322v1-decoupling-reasoning-and-perception-an-llm-lmm-framework-for-faithfu.html">Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning</a></td>
  <td>æå‡ºè§£è€¦æ¨ç†ä¸æ„ŸçŸ¥çš„LLM-LMMæ¡†æ¶ï¼Œæå‡è§†è§‰æ¨ç†çš„å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23322v1" data-paper-url="./papers/250923322v1-decoupling-reasoning-and-perception-an-llm-lmm-framework-for-faithfu.html" onclick="toggleFavorite(this, '2509.23322v1', 'Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250923267v1-learning-regional-monsoon-patterns-with-a-multimodal-attention-u-net.html">Learning Regional Monsoon Patterns with a Multimodal Attention U-Net</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ³¨æ„åŠ›U-Netï¼Œç”¨äºå°åº¦åŒºåŸŸé«˜åˆ†è¾¨ç‡å­£é£é™é›¨é¢„æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23267v1" data-paper-url="./papers/250923267v1-learning-regional-monsoon-patterns-with-a-multimodal-attention-u-net.html" onclick="toggleFavorite(this, '2509.23267v1', 'Learning Regional Monsoon Patterns with a Multimodal Attention U-Net')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250923242v1-tattoo-training-free-aesthetic-aware-outfit-recommendation.html">TATTOO: Training-free AesTheTic-aware Outfit recOmmendation</a></td>
  <td>æå‡ºTATTOOï¼šä¸€ç§æ— éœ€è®­ç»ƒçš„ã€å…·æœ‰ç¾å­¦æ„è¯†çš„æœè£…æ­é…æ¨èæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23242v1" data-paper-url="./papers/250923242v1-tattoo-training-free-aesthetic-aware-outfit-recommendation.html" onclick="toggleFavorite(this, '2509.23242v1', 'TATTOO: Training-free AesTheTic-aware Outfit recOmmendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250923370v1-grape-let-gpro-supervise-query-rewriting-by-ranking-for-retrieval.html">GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval</a></td>
  <td>GRAPEï¼šé€šè¿‡æ’åºç›‘ç£æŸ¥è¯¢é‡å†™ï¼Œæå‡æ£€ç´¢æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23370v1" data-paper-url="./papers/250923370v1-grape-let-gpro-supervise-query-rewriting-by-ranking-for-retrieval.html" onclick="toggleFavorite(this, '2509.23370v1', 'GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250923311v2-seeing-symbols-missing-cultures-probing-vision-language-models-reaso.html">Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning</a></td>
  <td>æå‡ºç«ä¸»é¢˜æ–‡åŒ–å›¾åƒè¯Šæ–­æ¡†æ¶ï¼Œæ­ç¤ºè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸Šçš„åå·®</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23311v2" data-paper-url="./papers/250923311v2-seeing-symbols-missing-cultures-probing-vision-language-models-reaso.html" onclick="toggleFavorite(this, '2509.23311v2', 'Seeing Symbols, Missing Cultures: Probing Vision-Language Models&#39; Reasoning on Fire Imagery and Cultural Meaning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250923273v1-syndoc-a-hybrid-discriminative-generative-framework-for-enhancing-sy.html">SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction</a></td>
  <td>SynDocï¼šä¸€ç§æ··åˆåˆ¤åˆ«-ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºå¢å¼ºåˆæˆé¢†åŸŸè‡ªé€‚åº”æ–‡æ¡£å…³é”®ä¿¡æ¯æå–</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23273v1" data-paper-url="./papers/250923273v1-syndoc-a-hybrid-discriminative-generative-framework-for-enhancing-sy.html" onclick="toggleFavorite(this, '2509.23273v1', 'SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250923236v1-self-consistency-as-a-free-lunch-reducing-hallucinations-in-vision-l.html">Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection</a></td>
  <td>æå‡ºåŸºäºè‡ªåæ€çš„è‡ªæ´½æ€§æ–¹æ³•ï¼Œå‡å°‘è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23236v1" data-paper-url="./papers/250923236v1-self-consistency-as-a-free-lunch-reducing-hallucinations-in-vision-l.html" onclick="toggleFavorite(this, '2509.23236v1', 'Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251000040v1-uncovering-intrinsic-capabilities-a-paradigm-for-data-curation-in-vi.html">Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models</a></td>
  <td>æå‡ºèƒ½åŠ›å½’å› æ•°æ®ç²¾é€‰æ¡†æ¶CADCï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒä¼˜æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00040v1" data-paper-url="./papers/251000040v1-uncovering-intrinsic-capabilities-a-paradigm-for-data-curation-in-vi.html" onclick="toggleFavorite(this, '2510.00040v1', 'Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250923097v2-streamline-pathology-foundation-model-by-cross-magnification-distill.html">Streamline pathology foundation model by cross-magnification distillation</a></td>
  <td>æå‡ºåŸºäºè·¨å€ç‡è’¸é¦çš„è½»é‡çº§ç—…ç†å­¦åŸºç¡€æ¨¡å‹XMAGï¼ŒåŠ é€Ÿä¸´åºŠéƒ¨ç½²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23097v2" data-paper-url="./papers/250923097v2-streamline-pathology-foundation-model-by-cross-magnification-distill.html" onclick="toggleFavorite(this, '2509.23097v2', 'Streamline pathology foundation model by cross-magnification distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250923310v1-balanced-diffusion-guided-fusion-for-multimodal-remote-sensing-class.html">Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification</a></td>
  <td>æå‡ºå¹³è¡¡æ‰©æ•£å¼•å¯¼èåˆæ¡†æ¶ï¼Œè§£å†³å¤šæ¨¡æ€é¥æ„Ÿåˆ†ç±»ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23310v1" data-paper-url="./papers/250923310v1-balanced-diffusion-guided-fusion-for-multimodal-remote-sensing-class.html" onclick="toggleFavorite(this, '2509.23310v1', 'Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251000046v1-reinforcement-learning-based-prompt-template-stealing-for-text-to-im.html">Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„RLStealeræ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„æç¤ºæ¨¡æ¿çªƒå–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00046v1" data-paper-url="./papers/251000046v1-reinforcement-learning-based-prompt-template-stealing-for-text-to-im.html" onclick="toggleFavorite(this, '2510.00046v1', 'Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250923480v1-restorect-degraded-image-restoration-via-latent-rectified-flow-featu.html">RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation</a></td>
  <td>RestoRectï¼šåŸºäºæ½œåœ¨ç©ºé—´æ ¡æ­£æµä¸ç‰¹å¾è’¸é¦çš„å›¾åƒå¤åŸæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23480v1" data-paper-url="./papers/250923480v1-restorect-degraded-image-restoration-via-latent-rectified-flow-featu.html" onclick="toggleFavorite(this, '2509.23480v1', 'RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250923375v1-caspointr-point-cloud-completion-with-cascaded-networks-and-knowledg.html">CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation</a></td>
  <td>CasPoinTrï¼šåŸºäºçº§è”ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦çš„ç‚¹äº‘è¡¥å…¨æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23375v1" data-paper-url="./papers/250923375v1-caspointr-point-cloud-completion-with-cascaded-networks-and-knowledg.html" onclick="toggleFavorite(this, '2509.23375v1', 'CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250923339v1-lrpo-enhancing-blind-face-restoration-through-online-reinforcement-l.html">LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning</a></td>
  <td>æå‡ºLRPOæ¡†æ¶ï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æå‡ç›²äººè„¸ä¿®å¤æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23339v1" data-paper-url="./papers/250923339v1-lrpo-enhancing-blind-face-restoration-through-online-reinforcement-l.html" onclick="toggleFavorite(this, '2509.23339v1', 'LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250923316v2-c3-owd-a-curriculum-cross-modal-contrastive-learning-framework-for-o.html">C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection</a></td>
  <td>æå‡ºC3-OWDæ¡†æ¶ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ å’Œè·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ å®ç°å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23316v2" data-paper-url="./papers/250923316v2-c3-owd-a-curriculum-cross-modal-contrastive-learning-framework-for-o.html" onclick="toggleFavorite(this, '2509.23316v2', 'C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250923258v2-oraclegs-grounding-generative-priors-for-sparse-view-gaussian-splatt.html">OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting</a></td>
  <td>OracleGSï¼šé€šè¿‡ç”Ÿæˆå…ˆéªŒå¼•å¯¼çš„ç¨€ç–è§†è§’é«˜æ–¯æº…å°„</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23258v2" data-paper-url="./papers/250923258v2-oraclegs-grounding-generative-priors-for-sparse-view-gaussian-splatt.html" onclick="toggleFavorite(this, '2509.23258v2', 'OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250923492v1-orientation-anchored-hyper-gaussian-for-4d-reconstruction-from-casua.html">Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos</a></td>
  <td>æå‡ºåŸºäºæ–¹å‘é”šå®šçš„è¶…é«˜æ–¯æ–¹æ³•OriGSï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­è¿›è¡Œé«˜è´¨é‡4Dé‡å»ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23492v1" data-paper-url="./papers/250923492v1-orientation-anchored-hyper-gaussian-for-4d-reconstruction-from-casua.html" onclick="toggleFavorite(this, '2509.23492v1', 'Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250923438v2-fm-siren-fm-finer-nyquist-informed-frequency-multiplier-for-implicit.html">FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation</a></td>
  <td>FM-SIREN/FINERï¼šé€šè¿‡Nyquisté¢‘ç‡ä¹˜å­æå‡å‘¨æœŸæ¿€æ´»éšå¼ç¥ç»è¡¨ç¤ºæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23438v2" data-paper-url="./papers/250923438v2-fm-siren-fm-finer-nyquist-informed-frequency-multiplier-for-implicit.html" onclick="toggleFavorite(this, '2509.23438v2', 'FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250923038v1-geloc3r-enhancing-relative-camera-pose-regression-with-geometric-con.html">GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization</a></td>
  <td>GeLoc3rï¼šé€šè¿‡å‡ ä½•ä¸€è‡´æ€§æ­£åˆ™åŒ–å¢å¼ºç›¸å¯¹ç›¸æœºä½å§¿å›å½’</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23038v1" data-paper-url="./papers/250923038v1-geloc3r-enhancing-relative-camera-pose-regression-with-geometric-con.html" onclick="toggleFavorite(this, '2509.23038v1', 'GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250923169v1-sparse2dense-a-keypoint-driven-generative-framework-for-human-video-.html">Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction</a></td>
  <td>Sparse2Denseï¼šä¸€ç§å…³é”®ç‚¹é©±åŠ¨çš„ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºäººä½“è§†é¢‘å‹ç¼©å’Œé¡¶ç‚¹é¢„æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23169v1" data-paper-url="./papers/250923169v1-sparse2dense-a-keypoint-driven-generative-framework-for-human-video-.html" onclick="toggleFavorite(this, '2509.23169v1', 'Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250923098v1-copatch-zero-shot-referring-image-segmentation-by-leveraging-untappe.html">CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP</a></td>
  <td>CoPatchï¼šåˆ©ç”¨CLIPä¸­æœªå¼€å‘çš„ spatial knowledge å®ç°é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23098v1" data-paper-url="./papers/250923098v1-copatch-zero-shot-referring-image-segmentation-by-leveraging-untappe.html" onclick="toggleFavorite(this, '2509.23098v1', 'CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250923455v1-3dpcnet-pose-canonicalization-for-robust-viewpoint-invariant-3d-kine.html">3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras</a></td>
  <td>æå‡º3DPCNetä»¥è§£å†³å•ç›®RGBæ‘„åƒå¤´ä¸‹çš„3Då§¿æ€æ ‡å‡†åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23455v1" data-paper-url="./papers/250923455v1-3dpcnet-pose-canonicalization-for-robust-viewpoint-invariant-3d-kine.html" onclick="toggleFavorite(this, '2509.23455v1', '3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250923393v1-generative-modeling-of-shape-dependent-self-contact-human-poses.html">Generative Modeling of Shape-Dependent Self-Contact Human Poses</a></td>
  <td>æå‡ºåŸºäºå½¢çŠ¶æ¡ä»¶çš„è‡ªæ¥è§¦äººä½“å§¿æ€ç”Ÿæˆæ¨¡å‹ï¼Œæå‡å•è§†è§’å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">penetration</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23393v1" data-paper-url="./papers/250923393v1-generative-modeling-of-shape-dependent-self-contact-human-poses.html" onclick="toggleFavorite(this, '2509.23393v1', 'Generative Modeling of Shape-Dependent Self-Contact Human Poses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250923279v1-vid-freeze-protecting-images-from-malicious-image-to-video-generatio.html">Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing</a></td>
  <td>Vid-Freezeï¼šé€šè¿‡æ—¶åºå†»ç»“é˜²å¾¡æ¶æ„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">motion synthesis</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23279v1" data-paper-url="./papers/250923279v1-vid-freeze-protecting-images-from-malicious-image-to-video-generatio.html" onclick="toggleFavorite(this, '2509.23279v1', 'Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250923517v1-evaluating-point-light-biological-motion-in-multimodal-large-languag.html">Evaluating point-light biological motion in multimodal large language models</a></td>
  <td>ActPLDåŸºå‡†æµ‹è¯•æ­ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç‚¹å…‰ç”Ÿç‰©è¿åŠ¨æ–¹é¢çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.23517v1" data-paper-url="./papers/250923517v1-evaluating-point-light-biological-motion-in-multimodal-large-languag.html" onclick="toggleFavorite(this, '2509.23517v1', 'Evaluating point-light biological motion in multimodal large language models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)