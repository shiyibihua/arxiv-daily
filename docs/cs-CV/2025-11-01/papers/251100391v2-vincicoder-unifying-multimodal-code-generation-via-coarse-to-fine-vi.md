---
layout: default
title: VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning
---

# VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.00391" target="_blank" class="toolbar-btn">arXiv: 2511.00391v2</a>
    <a href="https://arxiv.org/pdf/2511.00391.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00391v2" 
            onclick="toggleFavorite(this, '2511.00391v2', 'VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xuanle Zhao, Deyang Jiang, Zhixiong Zeng, Lei Chen, Haibo Qiu, Jing Huang, Yufeng Zhong, Liming Zheng, Yilin Cao, Lin Ma

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-01 (Êõ¥Êñ∞: 2025-11-27)

**Â§áÊ≥®**: 15 pages, 11 figures

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/DocTron-hub/VinciCoder)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VinciCoderÔºöÈÄöËøáÁ≤óÂà∞ÁªÜËßÜËßâÂº∫ÂåñÂ≠¶‰π†Áªü‰∏ÄÂ§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàê**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàê` `ËßÜËßâÂº∫ÂåñÂ≠¶‰π†` `Á≤óÂà∞ÁªÜÂ≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `‰ª£Á†ÅÊô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàê‰∏≠‰æùËµñÂçï‰ªªÂä°ËÆ≠ÁªÉÔºåÁº∫‰πèÈÄöÁî®ËßÜËßâ‰ª£Á†ÅÊô∫ËÉΩ„ÄÇ
2. VinciCoderÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁªìÂêàÁõëÁù£ÂæÆË∞ÉÂíåÁ≤óÂà∞ÁªÜÁöÑËßÜËßâÂº∫ÂåñÂ≠¶‰π†Êù•ÊèêÂçáÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVinciCoderÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÂºÄÊ∫êÊ®°ÂûãÔºåÈ™åËØÅ‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàêÂ∑≤ÂºïËµ∑Á†îÁ©∂ÁïåÁöÑÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ËØ∏Â¶ÇÂõæË°®Âà∞‰ª£Á†ÅÁîüÊàêÁ≠âÁâπÂÆö‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂÆÉ‰ª¨ÂØπÂçï‰ªªÂä°ËÆ≠ÁªÉÊñπÊ°àÁöÑ‰æùËµñÈòªÁ¢ç‰∫ÜÈÄöÁî®ËßÜËßâ‰ª£Á†ÅÊô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜVinciCoderÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàêÊ®°ÂûãÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂Ëß£ÂÜ≥‰∫ÜËøô‰∏ÄÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâËØ≠ÊñôÂ∫ìÔºåÂåÖÂê´160‰∏á‰∏™ÂõæÂÉè-‰ª£Á†ÅÂØπÔºåÁî®‰∫éÁõ¥Êé•‰ª£Á†ÅÁîüÊàêÂíåÂü∫‰∫éËßÜËßâÁöÑ‰ª£Á†ÅÊîπËøõ‰ªªÂä°„ÄÇÂÖ∂Ê¨°ÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçËßÜËßâÂº∫ÂåñÂ≠¶‰π†ÔºàViRLÔºâÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•ÈááÁî®Á≤óÂà∞ÁªÜÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÈÄöËøáËÆ°ÁÆóÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÂõæÂÉèÂùó‰πãÈó¥ÁöÑËßÜËßâÁõ∏‰ººÊÄßÊù•ÊèêÈ´òËßÜËßâ‰øùÁúüÂ∫¶„ÄÇÂú®ÂêÑÁßçÂ§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåVinciCoderÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÊúÄËøëÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇÊ∂àËûçÁ†îÁ©∂Ëøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑÁ≤óÂà∞ÁªÜViRLÁ≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇÊï∞ÊçÆ„ÄÅ‰ª£Á†ÅÂíåÊ®°ÂûãÂèØÂú®https://github.com/DocTron-hub/VinciCoderËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàê‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åËÆ≠ÁªÉÔºåÁº∫‰πèÂú®‰∏çÂêåËßÜËßâËæìÂÖ•Âíå‰ª£Á†ÅÁîüÊàê‰ªªÂä°‰πãÈó¥ÁöÑËøÅÁßªËÉΩÂäõÔºåÈôêÂà∂‰∫ÜÂÖ∂ÈÄöÁî®ÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVinciCoderÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ®°ÂûãÊ°ÜÊû∂ÔºåÁªìÂêàÁõëÁù£Â≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáÊ®°ÂûãÂú®‰∏çÂêåÂ§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàê‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÁ≤óÂà∞ÁªÜÁöÑËßÜËßâÂº∫ÂåñÂ≠¶‰π†Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂõæÂÉèÂÜÖÂÆπÔºåÂπ∂ÁîüÊàê‰∏é‰πãÂØπÂ∫îÁöÑ‰ª£Á†Å„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVinciCoderÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÔºö‰ΩøÁî®Â§ßËßÑÊ®°ÂõæÂÉè-‰ª£Á†ÅÂØπÊï∞ÊçÆÈõÜÂØπÊ®°ÂûãËøõË°åÈ¢ÑËÆ≠ÁªÉÔºå‰ΩøÂÖ∂ÂÖ∑Â§áÂàùÊ≠•ÁöÑ‰ª£Á†ÅÁîüÊàêËÉΩÂäõ„ÄÇ2) ËßÜËßâÂº∫ÂåñÂ≠¶‰π†ÔºàViRLÔºâÔºöÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÔºåÈÄöËøáÂ•ñÂä±Êú∫Âà∂ÂºïÂØºÊ®°ÂûãÁîüÊàêÊõ¥Á¨¶ÂêàËßÜËßâ‰ø°ÊÅØÁöÑ‰ª£Á†Å„ÄÇViRLÈááÁî®Á≤óÂà∞ÁªÜÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÂàÜÂà´‰ªéÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÂõæÂÉèÂùóÁöÑËßÜËßâÁõ∏‰ººÊÄßÊù•ËØÑ‰º∞ÁîüÊàê‰ª£Á†ÅÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVinciCoderÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Á≤óÂà∞ÁªÜÁöÑËßÜËßâÂº∫ÂåñÂ≠¶‰π†ÔºàViRLÔºâÁ≠ñÁï•„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®Êï¥‰ΩìÁöÑÂ•ñÂä±ÔºåËÄåÂøΩÁï•‰∫ÜÂõæÂÉèÁöÑÂ±ÄÈÉ®ÁªÜËäÇ„ÄÇVinciCoderÈÄöËøáÂàÜÂà´ËÆ°ÁÆóÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÂõæÂÉèÂùóÁöÑËßÜËßâÁõ∏‰ººÊÄßÔºåËÉΩÂ§üÊõ¥Á≤æÁªÜÂú∞ËØÑ‰º∞ÁîüÊàê‰ª£Á†ÅÁöÑË¥®ÈáèÔºå‰ªéËÄåÊèêÈ´òËßÜËßâ‰øùÁúüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ViRLÈò∂ÊÆµÔºåÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇVinciCoderÁöÑÂ•ñÂä±ÂáΩÊï∞ÂåÖÂê´‰∏§ÈÉ®ÂàÜÔºöÂÖ®Â±ÄËßÜËßâÁõ∏‰ººÊÄßÂ•ñÂä±ÂíåÂ±ÄÈÉ®ËßÜËßâÁõ∏‰ººÊÄßÂ•ñÂä±„ÄÇÂÖ®Â±ÄËßÜËßâÁõ∏‰ººÊÄßÂ•ñÂä±Ë°°ÈáèÁîüÊàê‰ª£Á†ÅÂØπÂ∫îÁöÑÂõæÂÉè‰∏éÂéüÂßãÂõæÂÉèÁöÑÊï¥‰ΩìÁõ∏‰ººÂ∫¶ÔºåÂ±ÄÈÉ®ËßÜËßâÁõ∏‰ººÊÄßÂ•ñÂä±ÂàôÂÖ≥Ê≥®ÂõæÂÉèÁöÑÂ±ÄÈÉ®ÁªÜËäÇ„ÄÇÈÄöËøáË∞ÉÊï¥‰∏§ÈÉ®ÂàÜÂ•ñÂä±ÁöÑÊùÉÈáçÔºåÂèØ‰ª•ÊéßÂà∂Ê®°ÂûãÂØπÂÖ®Â±ÄÂíåÂ±ÄÈÉ®‰ø°ÊÅØÁöÑÂÖ≥Ê≥®Á®ãÂ∫¶„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÁªÜËäÇÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VinciCoderÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅ‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇÊ∂àËûçÂÆûÈ™åË°®ÊòéÔºåÁ≤óÂà∞ÁªÜÁöÑËßÜËßâÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÂØπÊÄßËÉΩÊèêÂçáËµ∑Âà∞‰∫ÜÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÂèØÂú®ËÆ∫Êñá‰∏≠Êü•ÈòÖ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VinciCoderÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂåÖÊã¨Ëá™Âä®ÂåñÂõæË°®ÁîüÊàê„ÄÅUIÁïåÈù¢‰ª£Á†ÅÁîüÊàê„ÄÅÂõæÂÉèÊèèËø∞ÁîüÊàê‰ª£Á†ÅÁ≠â„ÄÇËØ•ÊäÄÊúØÂèØ‰ª•Â∫îÁî®‰∫éËΩØ‰ª∂ÂºÄÂèë„ÄÅÊï∞ÊçÆÂèØËßÜÂåñ„ÄÅËá™Âä®ÂåñÊä•ÂëäÁîüÊàêÁ≠âÈ¢ÜÂüüÔºåÊèêÈ´òÂºÄÂèëÊïàÁéáÂíåÈôç‰ΩéÂºÄÂèëÊàêÊú¨„ÄÇÊú™Êù•ÔºåVinciCoderÊúâÊúõÊàê‰∏∫ÈÄöÁî®ËßÜËßâ‰ª£Á†ÅÊô∫ËÉΩÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåËµãËÉΩÊõ¥Â§öÊô∫ËÉΩÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on diverse multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, surpassing recent open-source models. The ablation study further validates the effectiveness of our proposed coarse-to-fine ViRL strategy. The data, code and model is available at https://github.com/DocTron-hub/VinciCoder.

