---
layout: default
title: EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT
---

# EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT

**arXiv**: [2510.23569v1](https://arxiv.org/abs/2510.23569) | [PDF](https://arxiv.org/pdf/2510.23569.pdf)

**ä½œè€…**: Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Yu Qiao, Jiangmiao Pang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEgoThinkeræ¡†æž¶ï¼Œé€šè¿‡æ—¶ç©ºé“¾å¼æ€ç»´ç›‘ç£å¢žå¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æŽ¨ç†ä¸­çš„èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æŽ¨ç†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ—¶ç©ºé“¾å¼æ€ç»´` `ä¸¤é˜¶æ®µå­¦ä¹ ` `ç»†ç²’åº¦å®šä½` `EgoRe-5Mæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æŽ¨ç†éœ€æŽ¨æ–­éšè—æ„å›¾å’Œç»†ç²’åº¦äº¤äº’ï¼ŒçŽ°æœ‰MLLMsç¼ºä¹ç¬¬ä¸€äººç§°ç†è§£èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºEgoRe-5Mæ•°æ®é›†ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ï¼ˆSFTå’ŒRFTï¼‰æå‡æ—¶ç©ºå®šä½å’ŒæŽ¨ç†ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªè‡ªæˆ‘ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œç»†ç²’åº¦æ—¶ç©ºå®šä½ä»»åŠ¡æœ‰æ˜¾è‘—æ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Egocentric video reasoning centers on an unobservable agent behind the camera
> who dynamically shapes the environment, requiring inference of hidden
> intentions and recognition of fine-grained interactions. This core challenge
> limits current multimodal large language models MLLMs, which excel at visible
> event reasoning but lack embodied, first-person understanding. To bridge this
> gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust
> egocentric reasoning capabilities through spatio-temporal chain-of-thought
> supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,
> a large-scale egocentric QA dataset constructed from 13M diverse egocentric
> video clips. This dataset features multi-minute segments annotated with
> detailed CoT rationales and dense hand-object grounding. Second, we employ SFT
> on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning
> RFT to further enhance spatio-temporal localization. Experimental results show
> that EgoThinker outperforms existing methods across multiple egocentric
> benchmarks, while achieving substantial improvements in fine-grained
> spatio-temporal localization tasks. Full code and data are released at
> https://github.com/InternRobotics/EgoThinker.

