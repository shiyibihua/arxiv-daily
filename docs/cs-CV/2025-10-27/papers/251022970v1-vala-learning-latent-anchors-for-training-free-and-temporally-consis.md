---
layout: default
title: VALA: Learning Latent Anchors for Training-Free and Temporally Consistent
---

# VALA: Learning Latent Anchors for Training-Free and Temporally Consistent

**arXiv**: [2510.22970v1](https://arxiv.org/abs/2510.22970) | [PDF](https://arxiv.org/pdf/2510.22970.pdf)

**ä½œè€…**: Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Longbing Cao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVALAä»¥è§£å†³è®­ç»ƒå…è´¹è§†é¢‘ç¼–è¾‘ä¸­çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜**

**å…³é”®è¯**: `è®­ç»ƒå…è´¹è§†é¢‘ç¼–è¾‘` `å˜åˆ†å¯¹é½` `æ½œåœ¨é”šç‚¹` `æ—¶é—´ä¸€è‡´æ€§` `å¯¹æ¯”å­¦ä¹ ` `DDIMåæ¼”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–å¯å‘å¼å¸§é€‰æ‹©ï¼Œå¯¼è‡´æ‰‹åŠ¨åå·®å’Œå¯æ‰©å±•æ€§å·®
2. VALAä½¿ç”¨å˜åˆ†å¯¹é½æ¨¡å—è‡ªé€‚åº”é€‰æ‹©å…³é”®å¸§å¹¶åŽ‹ç¼©æ½œåœ¨ç‰¹å¾ä¸ºè¯­ä¹‰é”šç‚¹
3. å®žéªŒæ˜¾ç¤ºåœ¨çœŸå®žè§†é¢‘ç¼–è¾‘åŸºå‡†ä¸Šå®žçŽ°é«˜ä¿çœŸã€é«˜è´¨é‡å’Œé«˜æ•ˆæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in training-free video editing have enabled lightweight and
> precise cross-frame generation by leveraging pre-trained text-to-image
> diffusion models. However, existing methods often rely on heuristic frame
> selection to maintain temporal consistency during DDIM inversion, which
> introduces manual bias and reduces the scalability of end-to-end inference. In
> this paper, we propose~\textbf{VALA} (\textbf{V}ariational \textbf{A}lignment
> for \textbf{L}atent \textbf{A}nchors), a variational alignment module that
> adaptively selects key frames and compresses their latent features into
> semantic anchors for consistent video editing. To learn meaningful assignments,
> VALA propose a variational framework with a contrastive learning objective.
> Therefore, it can transform cross-frame latent representations into compressed
> latent anchors that preserve both content and temporal coherence. Our method
> can be fully integrated into training-free text-to-image based video editing
> models. Extensive experiments on real-world video editing benchmarks show that
> VALA achieves state-of-the-art performance in inversion fidelity, editing
> quality, and temporal consistency, while offering improved efficiency over
> prior methods.

