---
layout: default
title: VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation
---

# VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation

**arXiv**: [2510.23497v1](https://arxiv.org/abs/2510.23497) | [PDF](https://arxiv.org/pdf/2510.23497.pdf)

**ä½œè€…**: Walid Bousselham, Hilde Kuehne, Cordelia Schmid

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVOLDæ¡†æž¶ï¼Œé€šè¿‡åœ¨çº¿ç­–ç•¥è’¸é¦å°†LLMsæŽ¨ç†èƒ½åŠ›è¿ç§»è‡³VLMsï¼Œè§£å†³é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬æŽ¨ç†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†è¿ç§»` `åœ¨çº¿ç­–ç•¥è’¸é¦` `å¼ºåŒ–å­¦ä¹ ` `åˆ†å¸ƒå¯¹é½` `æ•™å¸ˆ-å­¦ç”Ÿæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé«˜è´¨é‡å›¾åƒ-æ–‡æœ¬æŽ¨ç†æ•°æ®ç¨€ç¼ºï¼Œé˜»ç¢è§†è§‰è¯­è¨€æ¨¡åž‹å¤æ‚æŽ¨ç†èƒ½åŠ›å‘å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¼ºåŒ–å­¦ä¹ ä¸Žåœ¨çº¿ç­–ç•¥è’¸é¦ï¼Œåˆ©ç”¨æ–‡æœ¬æ•™å¸ˆæ¨¡åž‹æŒ‡å¯¼å­¦ç”Ÿæ¨¡åž‹æŽ¨ç†è½¨è¿¹ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶ŠåŸºçº¿æ¨¡åž‹ï¼Œå¹¶æå‡å½“å‰æœ€ä¼˜æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Training vision-language models (VLMs) for complex reasoning remains a
> challenging task, i.a. due to the scarcity of high-quality image-text reasoning
> data. Conversely, text-based reasoning resources are abundant and scalable, but
> it is still an open question how to leveraging them for VLM reasoning. To
> address this problem, we propose VOLD, a framework to transfer reasoning
> capabilities from text-only teacher models to VLM student models. To this end,
> VOLD combines reinforcement learning via Group Relative Policy Optimization
> (GRPO) with on-policy distillation, which allows the student reasoning traces
> to be guided by the teacher model, resulting in a significant gain over using
> GRPO alone. We further show that a cold-start alignment is essential for an
> effective transfer during the online training phase in this scenario and that
> without sufficient distributional alignment between teacher and student,
> on-policy distillation fails to provide meaningful guidance. We evaluate VOLD
> across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and
> LogicVista, showing that VOLD outperforms the baseline model significantly and
> improves over the state of the art by a margin. Our ablation shows the
> importance of a cold-start alignment via SFT for on-policy distillation with a
> text-only teacher.

