---
layout: default
title: USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding
---

# USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding

**arXiv**: [2510.22990v1](https://arxiv.org/abs/2510.22990) | [PDF](https://arxiv.org/pdf/2510.22990.pdf)

**ä½œè€…**: Youssef Megahed, Robin Ducharme, Mark Walker, Steven Hawken, Adrian D. C. Chan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUSF-MAEè‡ªç›‘ç£åŸºç¡€æ¨¡åž‹ï¼Œé€šè¿‡æŽ©ç è‡ªç¼–ç è§£å†³è¶…å£°å›¾åƒæ ‡æ³¨ç¨€ç¼ºé—®é¢˜ã€‚**

**å…³é”®è¯**: `è¶…å£°å›¾åƒåˆ†æž` `è‡ªç›‘ç£å­¦ä¹ ` `æŽ©ç è‡ªç¼–ç ` `è§†è§‰å˜æ¢å™¨` `åŒ»å­¦å½±åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¶…å£°å›¾åƒè§£é‡Šå›°éš¾ï¼Œå› å™ªå£°é«˜ã€æ“ä½œä¾èµ–æ€§å¼ºï¼Œå¯¼è‡´è§‚å¯Ÿè€…é—´å·®å¼‚å¤§ã€‚
2. ä½¿ç”¨ViTæž¶æž„ï¼Œåœ¨37ä¸‡è¶…å£°å›¾åƒä¸Šé¢„è®­ç»ƒï¼Œé€šè¿‡é‡å»ºæŽ©ç è¡¥ä¸å­¦ä¹ æ¨¡æ€ç‰¹å®šè¡¨ç¤ºã€‚
3. åœ¨ä¸‰ä¸ªåˆ†ç±»ä»»åŠ¡ä¸­å¾®è°ƒï¼ŒF1åˆ†æ•°è¾¾81.6%ã€79.6%å’Œ82.4%ï¼Œä¼˜äºŽåŸºçº¿æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Ultrasound imaging is one of the most widely used diagnostic modalities,
> offering real-time, radiation-free assessment across diverse clinical domains.
> However, interpretation of ultrasound images remains challenging due to high
> noise levels, operator dependence, and limited field of view, resulting in
> substantial inter-observer variability. Current Deep Learning approaches are
> hindered by the scarcity of large labeled datasets and the domain gap between
> general and sonographic images, which limits the transferability of models
> pretrained on non-medical data. To address these challenges, we introduce the
> Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),
> the first large-scale self-supervised MAE framework pretrained exclusively on
> ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound
> images curated from 46 open-source datasets, collectively termed OpenUS-46,
> spanning over twenty anatomical regions. This curated dataset has been made
> publicly available to facilitate further research and reproducibility. Using a
> Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked
> image patches, enabling it to learn rich, modality-specific representations
> directly from unlabeled data. The pretrained encoder was fine-tuned on three
> public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D
> (ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all
> tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,
> achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using
> labels during pretraining, USF-MAE approached the performance of the supervised
> foundation model UltraSam on breast cancer classification and surpassed it on
> the other tasks, demonstrating strong cross-anatomical generalization.

