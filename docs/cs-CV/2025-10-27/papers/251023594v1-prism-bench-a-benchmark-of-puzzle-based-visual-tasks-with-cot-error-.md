---
layout: default
title: PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection
---

# PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection

**arXiv**: [2510.23594v1](https://arxiv.org/abs/2510.23594) | [PDF](https://arxiv.org/pdf/2510.23594.pdf)

**ä½œè€…**: Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRISM-BenchåŸºå‡†ï¼Œé€šè¿‡è§†è§‰è°œé¢˜å’Œæ€ç»´é“¾é”™è¯¯æ£€æµ‹è¯„ä¼°å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†è§‰æŽ¨ç†åŸºå‡†` `æ€ç»´é“¾é”™è¯¯æ£€æµ‹` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `é€»è¾‘ä¸€è‡´æ€§è¯„ä¼°` `è¯Šæ–­è¯„ä¼°åè®®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¨¡åž‹åœ¨è§†è§‰æŽ¨ç†ä¸­æµç•…ç”Ÿæˆä¸Žå¿ å®žæŽ¨ç†é—´å­˜åœ¨å·®è·ï¼Œéš¾ä»¥æ£€æµ‹é€»è¾‘é”™è¯¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡åŒ…å«å•æ­¥é”™è¯¯çš„æ€ç»´é“¾ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡åž‹è¯†åˆ«é¦–ä¸ªé”™è¯¯æ­¥éª¤ã€‚
3. å®žéªŒæ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºå…ˆè¿›æ¨¡åž‹åœ¨é”™è¯¯æ£€æµ‹ä¸Šè¡¨çŽ°ä¸ä½³ï¼Œçªæ˜¾è¯Šæ–­è¯„ä¼°çš„å¿…è¦æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce \textbf{PRISM-Bench}, a benchmark of puzzle-based visual
> challenges designed to evaluate not only whether models can solve problems, but
> how their reasoning unfolds. Unlike prior evaluations that measure only
> final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual
> puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,
> models must identify the first incorrect step. This setting enables
> fine-grained assessment of logical consistency, error detection, and visual
> reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,
> and analogical reasoning, resisting shortcuts based on superficial pattern
> matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap
> between fluent generation and faithful reasoning: models that produce plausible
> CoTs often fail to locate simple logical faults. By disentangling answer
> generation from reasoning verification, PRISM-Bench offers a sharper lens on
> multimodal reasoning competence and underscores the need for diagnostic
> evaluation protocols in the development of trustworthy MLLMs.

