---
layout: default
title: LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation
---

# LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation

**arXiv**: [2510.22946v1](https://arxiv.org/abs/2510.22946) | [PDF](https://arxiv.org/pdf/2510.22946.pdf)

**ä½œè€…**: Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLightBagelæ¡†æž¶ï¼Œé€šè¿‡åŒèžåˆæœºåˆ¶é«˜æ•ˆç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸Žç”Ÿæˆã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€èžåˆ` `è½»é‡çº§æ¡†æž¶` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `ç»Ÿä¸€æ¨¡åž‹` `é«˜æ•ˆè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹è®­ç»ƒèµ„æºæ¶ˆè€—å¤§ï¼Œéœ€ä»Žé›¶å¼€å§‹æž„å»ºã€‚
2. æ–¹æ³•ï¼šä¿ç•™åŽŸæ¨¡åž‹å—ï¼Œæ’å…¥å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›å—å®žçŽ°åŒèžåˆã€‚
3. æ•ˆæžœï¼šä»…ç”¨35B tokensè®­ç»ƒï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—å¼ºç»“æžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unified multimodal models have recently shown remarkable gains in both
> capability and versatility, yet most leading systems are still trained from
> scratch and require substantial computational resources. In this paper, we show
> that competitive performance can be obtained far more efficiently by
> strategically fusing publicly available models specialized for either
> generation or understanding. Our key design is to retain the original blocks
> while additionally interleaving multimodal self-attention blocks throughout the
> networks. This double fusion mechanism (1) effectively enables rich multi-modal
> fusion while largely preserving the original strengths of the base models, and
> (2) catalyzes synergistic fusion of high-level semantic representations from
> the understanding encoder with low-level spatial signals from the generation
> encoder. By training with only ~ 35B tokens, this approach achieves strong
> results across multiple benchmarks: 0.91 on GenEval for compositional
> text-to-image generation, 82.16 on DPG-Bench for complex text-to-image
> generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By
> fully releasing the entire suite of code, model weights, and datasets, we hope
> to support future research on unified multimodal modeling.

