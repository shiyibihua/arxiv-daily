---
layout: default
title: HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment
---

# HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment

**arXiv**: [2510.22917v1](https://arxiv.org/abs/2510.22917) | [PDF](https://arxiv.org/pdf/2510.22917.pdf)

**ä½œè€…**: Zecheng Yin, Hao Zhao, Zhen Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHyPerNavæ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹èžåˆå±€éƒ¨ä¸Žå…¨å±€æ„ŸçŸ¥ï¼Œæå‡æœªçŸ¥çŽ¯å¢ƒä¸­ç›®æ ‡å¯¼å‘å¯¼èˆªæ€§èƒ½ã€‚**

**å…³é”®è¯**: `ç›®æ ‡å¯¼å‘å¯¼èˆª` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ··åˆæ„ŸçŸ¥` `æœªçŸ¥çŽ¯å¢ƒå¯¼èˆª` `RGB-Dä¼ æ„Ÿå™¨` `ä¿¯è§†å›¾èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœªçŸ¥çŽ¯å¢ƒä¸­ç›®æ ‡å¯¼å‘å¯¼èˆªä¾èµ–å•ä¸€æ„ŸçŸ¥æºï¼Œç¼ºä¹å±€éƒ¨ä¸Žå…¨å±€ä¿¡æ¯èžåˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè§†è§‰è¯­è¨€æ¨¡åž‹ï¼ŒåŒæ—¶å¤„ç†RGB-Dä¼ æ„Ÿå™¨å±€éƒ¨è§‚æµ‹å’Œå®žæ—¶ä¿¯è§†å›¾å…¨å±€ä¸Šä¸‹æ–‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žä¸–ç•Œæµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¼˜äºŽåŸºçº¿ï¼Œæ¶ˆèžç ”ç©¶éªŒè¯æ··åˆæ„ŸçŸ¥æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Objective-oriented navigation(ObjNav) enables robot to navigate to target
> object directly and autonomously in an unknown environment. Effective
> perception in navigation in unknown environment is critical for autonomous
> robots. While egocentric observations from RGB-D sensors provide abundant local
> information, real-time top-down maps offer valuable global context for ObjNav.
> Nevertheless, the majority of existing studies focus on a single source, seldom
> integrating these two complementary perceptual modalities, despite the fact
> that humans naturally attend to both. With the rapid advancement of
> Vision-Language Models(VLMs), we propose Hybrid Perception Navigation
> (HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding
> capabilities to jointly perceive both local and global information to enhance
> the effectiveness and intelligence of navigation in unknown environments. In
> both massive simulation evaluation and real-world validation, our methods
> achieved state-of-the-art performance against popular baselines. Benefiting
> from hybrid perception approach, our method captures richer cues and finds the
> objects more effectively, by simultaneously leveraging information
> understanding from egocentric observations and the top-down map. Our ablation
> study further proved that either of the hybrid perception contributes to the
> navigation performance.

