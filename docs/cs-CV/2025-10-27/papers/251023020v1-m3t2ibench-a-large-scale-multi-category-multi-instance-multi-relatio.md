---
layout: default
title: M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark
---

# M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark

**arXiv**: [2510.23020v1](https://arxiv.org/abs/2510.23020) | [PDF](https://arxiv.org/pdf/2510.23020.pdf)

**ä½œè€…**: Huixuan Zhang, Xiaojun Wan

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMÂ³T2IBenchåŸºå‡†å’ŒAlignScoreæŒ‡æ ‡ä»¥è¯„ä¼°å¤šå®ä¾‹æ–‡æœ¬-å›¾åƒå¯¹é½é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬-å›¾åƒç”Ÿæˆ` `å¤šå®ä¾‹è¯„ä¼°` `å›¾åƒ-æ–‡æœ¬å¯¹é½` `åŸºå‡†æ•°æ®é›†` `ç›®æ ‡æ£€æµ‹æŒ‡æ ‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–‡æœ¬-å›¾åƒæ¨¡å‹åœ¨å¤šå®ä¾‹ã€å¤šç±»åˆ«æç¤ºä¸‹å›¾åƒ-æ–‡æœ¬å¯¹é½ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤§è§„æ¨¡å¤šç±»åˆ«ã€å¤šå®ä¾‹ã€å¤šå…³ç³»åŸºå‡†åŠåŸºäºç›®æ ‡æ£€æµ‹çš„AlignScoreæŒ‡æ ‡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šæå‡ºRevise-Then-Enforceæ–¹æ³•ï¼Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­æå‡å¯¹é½æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-to-image models are known to struggle with generating images that
> perfectly align with textual prompts. Several previous studies have focused on
> evaluating image-text alignment in text-to-image generation. However, these
> evaluations either address overly simple scenarios, especially overlooking the
> difficulty of prompts with multiple different instances belonging to the same
> category, or they introduce metrics that do not correlate well with human
> evaluation. In this study, we introduce M$^3$T2IBench, a large-scale,
> multi-category, multi-instance, multi-relation along with an
> object-detection-based evaluation metric, $AlignScore$, which aligns closely
> with human evaluation. Our findings reveal that current open-source
> text-to-image models perform poorly on this challenging benchmark.
> Additionally, we propose the Revise-Then-Enforce approach to enhance image-text
> alignment. This training-free post-editing method demonstrates improvements in
> image-text alignment across a broad range of diffusion models. \footnote{Our
> code and data has been released in supplementary material and will be made
> publicly available after the paper is accepted.}

