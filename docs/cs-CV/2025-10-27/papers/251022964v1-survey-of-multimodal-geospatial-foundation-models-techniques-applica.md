---
layout: default
title: Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges
---

# Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges

**arXiv**: [2510.22964v1](https://arxiv.org/abs/2510.22964) | [PDF](https://arxiv.org/pdf/2510.22964.pdf)

**ä½œè€…**: Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¤šæ¨¡æ€åœ°ç†ç©ºé—´åŸºç¡€æ¨¡åž‹æŠ€æœ¯ã€åº”ç”¨ä¸ŽæŒ‘æˆ˜ï¼Œä»¥åº”å¯¹é¥æ„Ÿæ•°æ®åˆ†æžéœ€æ±‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºç¡€æ¨¡åž‹` `åœ°ç†ç©ºé—´åˆ†æž` `é¥æ„Ÿå›¾åƒå¤„ç†` `æ¨¡æ€å¯¹é½` `ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°` `é¢†åŸŸæŒ‘æˆ˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé¥æ„Ÿæ•°æ®å¤šæ¨¡æ€ã€å¤šåˆ†è¾¨çŽ‡ã€å¤šæ—¶é—´ç‰¹æ€§å¸¦æ¥çš„æ¨¡æ€å¼‚è´¨æ€§å’Œè¯­ä¹‰é¸¿æ²Ÿã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä»Žæ¨¡æ€é©±åŠ¨è§†è§’ï¼Œåˆ†æžå¯¹é½ã€é›†æˆå’ŒçŸ¥è¯†è½¬ç§»ç­‰å…³é”®æŠ€æœ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ¨¡åž‹åœ¨åé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹å±•ç¤ºå®žé™…åº”ç”¨æ½œåŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation models have transformed natural language processing and computer
> vision, and their impact is now reshaping remote sensing image analysis. With
> powerful generalization and transfer learning capabilities, they align
> naturally with the multimodal, multi-resolution, and multi-temporal
> characteristics of remote sensing data. To address unique challenges in the
> field, multimodal geospatial foundation models (GFMs) have emerged as a
> dedicated research frontier. This survey delivers a comprehensive review of
> multimodal GFMs from a modality-driven perspective, covering five core visual
> and vision-language modalities. We examine how differences in imaging physics
> and data representation shape interaction design, and we analyze key techniques
> for alignment, integration, and knowledge transfer to tackle modality
> heterogeneity, distribution shifts, and semantic gaps. Advances in training
> paradigms, architectures, and task-specific adaptation strategies are
> systematically assessed alongside a wealth of emerging benchmarks.
> Representative multimodal visual and vision-language GFMs are evaluated across
> ten downstream tasks, with insights into their architectures, performance, and
> application scenarios. Real-world case studies, spanning land cover mapping,
> agricultural monitoring, disaster response, climate studies, and geospatial
> intelligence, demonstrate the practical potential of GFMs. Finally, we outline
> pressing challenges in domain generalization, interpretability, efficiency, and
> privacy, and chart promising avenues for future research.

