---
layout: default
title: A Video Is Not Worth a Thousand Words
---

# A Video Is Not Worth a Thousand Words

**arXiv**: [2510.23253v1](https://arxiv.org/abs/2510.23253) | [PDF](https://arxiv.org/pdf/2510.23253.pdf)

**ä½œè€…**: Sam Pollard, Michael Wray

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽShapleyå€¼çš„ç‰¹å¾å½’å› ä¸Žæ¨¡æ€è¯„åˆ†æ–¹æ³•ï¼Œè¯„ä¼°å¤šæ¨¡æ€æ¨¡åž‹åœ¨è§†é¢‘é—®ç­”ä¸­çš„æ–‡æœ¬ä¾èµ–é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `å¤šæ¨¡æ€æ¨¡åž‹` `Shapleyå€¼` `ç‰¹å¾å½’å› ` `æ¨¡æ€äº¤äº’` `æ–‡æœ¬ä¾èµ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æ¨¡åž‹åœ¨è§†é¢‘é—®ç­”ä¸­å¯èƒ½è¿‡åº¦ä¾èµ–æ–‡æœ¬ï¼Œå¿½ç•¥è§†é¢‘æ¨¡æ€çš„äº¤äº’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Shapleyå€¼è®¡ç®—ç‰¹å¾å½’å› å’Œæ¨¡æ€åˆ†æ•°ï¼Œæ”¯æŒä»»æ„å®šä¹‰ç‰¹å¾å’Œæ¨¡æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¯”è¾ƒ6ä¸ªæ¨¡åž‹åœ¨4ä¸ªæ•°æ®é›†ä¸Šï¼Œå‘çŽ°æ¨¡åž‹å€¾å‘äºŽå¿½ç•¥å¹²æ‰°é¡¹ï¼Œä¾èµ–æ–‡æœ¬ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As we become increasingly dependent on vision language models (VLMs) to
> answer questions about the world around us, there is a significant amount of
> research devoted to increasing both the difficulty of video question answering
> (VQA) datasets, and the context lengths of the models that they evaluate. The
> reliance on large language models as backbones has lead to concerns about
> potential text dominance, and the exploration of interactions between
> modalities is underdeveloped. How do we measure whether we're heading in the
> right direction, with the complexity that multi-modal models introduce? We
> propose a joint method of computing both feature attributions and modality
> scores based on Shapley values, where both the features and modalities are
> arbitrarily definable. Using these metrics, we compare $6$ VLM models of
> varying context lengths on $4$ representative datasets, focusing on
> multiple-choice VQA. In particular, we consider video frames and whole textual
> elements as equal features in the hierarchy, and the multiple-choice VQA task
> as an interaction between three modalities: video, question and answer. Our
> results demonstrate a dependence on text and show that the multiple-choice VQA
> task devolves into a model's ability to ignore distractors. Code available at
> https://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.

