---
layout: default
title: Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap
---

# Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap

**arXiv**: [2510.23494v1](https://arxiv.org/abs/2510.23494) | [PDF](https://arxiv.org/pdf/2510.23494.pdf)

**ä½œè€…**: Elisabeth JÃ¼ttner, Leona Krath, Stefan Korfhage, Hannah DrÃ¶ge, Matthias B. Hullin, Markus Plack

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆé‡å…‰ç…§æ¡†æž¶ä»¥è§£å†³ä½“ç§¯è§†é¢‘é‡å…‰ç…§çš„æ—¶åºä¸ç¨³å®šé—®é¢˜**

**å…³é”®è¯**: `ä½“ç§¯è§†é¢‘é‡å…‰ç…§` `æ‰©æ•£æ¨¡åž‹` `æ—¶åºç¨³å®šæ€§` `æ··åˆæ¡†æž¶` `é«˜æ–¯ä¸é€æ˜Žåº¦åœº` `å…‰æµå¼•å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹åœ¨åºåˆ—é‡å…‰ç…§ä¸­äº§ç”Ÿéšæœºå™ªå£°å’Œä¸ç¨³å®šæ€§ï¼Œè§†é¢‘æ‰©æ•£æ¨¡åž‹å—é™äºŽå†…å­˜å’Œè§„æ¨¡
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ‰©æ•£å…ˆéªŒä¸Žæ—¶é—´æ­£åˆ™åŒ–ï¼Œä½¿ç”¨å…‰æµå¼•å¯¼èšåˆææ–™å±žæ€§ï¼Œå¹¶åŸºäºŽé«˜æ–¯ä¸é€æ˜Žåº¦åœºæ¸²æŸ“é—´æŽ¥æ•ˆæžœ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žå’Œåˆæˆæ•èŽ·ä¸Šå®žçŽ°æ¯”çº¯æ‰©æ•£åŸºçº¿æ›´ç¨³å®šçš„é‡å…‰ç…§ï¼Œå¹¶æ‰©å±•åˆ°æ›´é•¿åºåˆ—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Volumetric video relighting is essential for bringing captured performances
> into virtual worlds, but current approaches struggle to deliver temporally
> stable, production-ready results. Diffusion-based intrinsic decomposition
> methods show promise for single frames, yet suffer from stochastic noise and
> instability when extended to sequences, while video diffusion models remain
> constrained by memory and scale. We propose a hybrid relighting framework that
> combines diffusion-derived material priors with temporal regularization and
> physically motivated rendering. Our method aggregates multiple stochastic
> estimates of per-frame material properties into temporally consistent shading
> components, using optical-flow-guided regularization. For indirect effects such
> as shadows and reflections, we extract a mesh proxy from Gaussian Opacity
> Fields and render it within a standard graphics pipeline. Experiments on real
> and synthetic captures show that this hybrid strategy achieves substantially
> more stable relighting across sequences than diffusion-only baselines, while
> scaling beyond the clip lengths feasible for video diffusion. These results
> indicate that hybrid approaches, which balance learned priors with physically
> grounded constraints, are a practical step toward production-ready volumetric
> video relighting.

