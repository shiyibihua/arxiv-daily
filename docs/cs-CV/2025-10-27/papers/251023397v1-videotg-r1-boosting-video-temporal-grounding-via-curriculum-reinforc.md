---
layout: default
title: VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations
---

# VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations

**arXiv**: [2510.23397v1](https://arxiv.org/abs/2510.23397) | [PDF](https://arxiv.org/pdf/2510.23397.pdf)

**ä½œè€…**: Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoTG-R1è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œé€šè¿‡è¾¹ç•Œåå°„å’Œéš¾åº¦ä¼°è®¡æå‡è§†é¢‘æ—¶åºå®šä½æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¼ºåŒ–å­¦ä¹ ` `è¯¾ç¨‹å­¦ä¹ ` `è¾¹ç•Œæ ‡æ³¨` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†é¢‘é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æ—¶åºå®šä½ä¸­éƒ¨åˆ†æ ‡æ³¨æ ·æœ¬å¼•å…¥æ­§ä¹‰ï¼Œéš¾å®šä½æ ·æœ¬åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±ä½Žä¸”æ— åå¥½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¾¹ç•Œåå°„ä»£ç†è¿‡æ»¤éƒ¨åˆ†æ ‡æ³¨æ ·æœ¬ï¼Œéš¾åº¦ä¼°è®¡ä»£ç†åŠ¨æ€å±è”½éš¾æ ·æœ¬ä»¥ä¼˜åŒ–è®­ç»ƒã€‚
3. å®žéªŒæ•ˆæžœï¼šä»…ç”¨10%è®­ç»ƒæ•°æ®å’Œ21%è®¡ç®—é¢„ç®—ï¼Œåœ¨VTGå’ŒVideoQAä»»åŠ¡ä¸­è¶…è¶Šå…¨æ•°æ®æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video temporal grounding (VTG) aims to locate precise segments in videos
> based on language queries, which is a fundamental challenge in video
> understanding. While recent Multimodal Large Language Models (MLLMs) have shown
> promise in tackling VTG through reinforcement learning (RL), they overlook the
> challenges arising from both the quality and difficulty of training samples.
> (1) Partially annotated samples. Many samples contain relevant segments beyond
> the annotated interval, introducing ambiguous supervision. (2) Hard-to-ground
> samples. Samples with poor zero-shot performance produce consistently low and
> indistinguishable rewards during RL training, exhibiting no clear preference
> among multiple outputs and thus hindering learning efficiency. To address these
> challenges, we propose VideoTG-R1, a novel curriculum RL framework with
> reflected boundary annotations, enabling data-efficient training. Specifically,
> we propose a Boundary Reflection Agent that utilizes MLLMs to predict
> query-relevant timestamps outside the annotated intervals, allowing us to
> identify and filter out partially annotated samples, thereby reducing
> ambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess
> the training difficulty of each sample and design a curriculum RL strategy that
> dynamically masks the videos of hard-to-ground samples according to the
> training steps, easing the training difficulty and providing clearer
> preference. Experiments on the VTG and grounded VideoQA tasks demonstrate the
> effectiveness of our method. Remarkably, with only 10% of the training samples
> and 21% of the computational budget, VideoTG-R1 outperforms full-data
> counterparts under both group relative policy optimization (GRPO) and
> supervised fine-tuning (SFT). The code is available at
> https://github.com/ldong1111/VideoTG-R1.

