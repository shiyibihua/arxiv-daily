---
layout: default
title: Evaluation of Vision-LLMs in Surveillance Video
---

# Evaluation of Vision-LLMs in Surveillance Video

**arXiv**: [2510.23190v1](https://arxiv.org/abs/2510.23190) | [PDF](https://arxiv.org/pdf/2510.23190.pdf)

**ä½œè€…**: Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨ç›‘æŽ§è§†é¢‘ä¸­çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡åž‹` `é›¶æ ·æœ¬å­¦ä¹ ` `å¼‚å¸¸æ£€æµ‹` `ç›‘æŽ§è§†é¢‘` `ç©ºé—´æŽ¨ç†` `éšç§ä¿æŠ¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›‘æŽ§è§†é¢‘æ•°æ®é‡åºžå¤§ï¼Œéœ€è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸äº‹ä»¶ä»¥æå‡å…¬å…±å®‰å…¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è§†é¢‘è½¬ä¸ºæ–‡æœ¬æè¿°ï¼Œé€šè¿‡æ–‡æœ¬è•´å«è¯„åˆ†å®žçŽ°é›¶æ ·æœ¬å¼‚å¸¸è¯†åˆ«ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨UCF-Crimeå’ŒRWF-2000æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œéšç§è¿‡æ»¤å¯èƒ½é™ä½Žå‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The widespread use of cameras in our society has created an overwhelming
> amount of video data, far exceeding the capacity for human monitoring. This
> presents a critical challenge for public safety and security, as the timely
> detection of anomalous or criminal events is crucial for effective response and
> prevention. The ability for an embodied agent to recognize unexpected events is
> fundamentally tied to its capacity for spatial reasoning. This paper
> investigates the spatial reasoning of vision-language models (VLMs) by framing
> anomalous action recognition as a zero-shot, language-grounded task, addressing
> the embodied perception challenge of interpreting dynamic 3D scenes from sparse
> 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs
> can act as spatially-grounded, zero-shot anomaly detectors by converting video
> into text descriptions and scoring labels via textual entailment. We evaluate
> four open models on UCF-Crime and RWF-2000 under prompting and
> privacy-preserving conditions. Few-shot exemplars can improve accuracy for some
> models, but may increase false positives, and privacy filters -- especially
> full-body GAN transforms -- introduce inconsistencies that degrade accuracy.
> These results chart where current vision--LLMs succeed (simple, spatially
> salient events) and where they falter (noisy spatial cues, identity
> obfuscation). Looking forward, we outline concrete paths to strengthen spatial
> grounding without task-specific training: structure-aware prompts, lightweight
> spatial memory across clips, scene-graph or 3D-pose priors during description,
> and privacy methods that preserve action-relevant geometry. This positions
> zero-shot, language-grounded pipelines as adaptable building blocks for
> embodied, real-world video understanding. Our implementation for evaluating
> VLMs is publicly available at:
> https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition

