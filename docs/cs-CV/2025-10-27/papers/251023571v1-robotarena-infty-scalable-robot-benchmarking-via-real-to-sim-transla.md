---
layout: default
title: RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation
---

# RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation

**arXiv**: [2510.23571v1](https://arxiv.org/abs/2510.23571) | [PDF](https://arxiv.org/pdf/2510.23571.pdf)

**ä½œè€…**: Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, Kuan-Hsun Tu, Tsung-Wei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRobotArena âˆžæ¡†æž¶ï¼Œé€šè¿‡çœŸå®žåˆ°æ¨¡æ‹Ÿè½¬æ¢å®žçŽ°å¯æ‰©å±•æœºå™¨äººåŸºå‡†æµ‹è¯•**

**å…³é”®è¯**: `æœºå™¨äººåŸºå‡†æµ‹è¯•` `çœŸå®žåˆ°æ¨¡æ‹Ÿè½¬æ¢` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯æ‰©å±•è¯„ä¼°` `äººç±»åé¦ˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçœŸå®žä¸–ç•Œæœºå™¨äººç­–ç•¥æµ‹è¯•å—é™ï¼Œéš¾ä»¥å¤§è§„æ¨¡ã€å®‰å…¨ã€å¯é‡å¤è¯„ä¼°
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹å’Œç”Ÿæˆå»ºæ¨¡ï¼Œè‡ªåŠ¨å°†è§†é¢‘æ¼”ç¤ºè½¬æ¢ä¸ºæ¨¡æ‹ŸçŽ¯å¢ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šç»“åˆè‡ªåŠ¨è¯„åˆ†å’Œäººç±»åå¥½ï¼Œç³»ç»Ÿæ‰°åŠ¨æµ‹è¯•ç­–ç•¥é²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The pursuit of robot generalists - instructable agents capable of performing
> diverse tasks across diverse environments - demands rigorous and scalable
> evaluation. Yet real-world testing of robot policies remains fundamentally
> constrained: it is labor-intensive, slow, unsafe at scale, and difficult to
> reproduce. Existing simulation benchmarks are similarly limited, as they train
> and test policies within the same synthetic domains and cannot assess models
> trained from real-world demonstrations or alternative simulation environments.
> As policies expand in scope and complexity, these barriers only intensify,
> since defining "success" in robotics often hinges on nuanced human judgments of
> execution quality. In this paper, we introduce a new benchmarking framework
> that overcomes these challenges by shifting VLA evaluation into large-scale
> simulated environments augmented with online human feedback. Leveraging
> advances in vision-language models, 2D-to-3D generative modeling, and
> differentiable rendering, our approach automatically converts video
> demonstrations from widely used robot datasets into simulated counterparts.
> Within these digital twins, we assess VLA policies using both automated
> VLM-guided scoring and scalable human preference judgments collected from
> crowdworkers, transforming human involvement from tedious scene setup,
> resetting, and safety supervision into lightweight preference comparisons. To
> measure robustness, we systematically perturb simulated environments along
> multiple axes, such as textures and object placements, stress-testing policy
> generalization under controlled variation. The result is a continuously
> evolving, reproducible, and scalable benchmark for real-world trained robot
> manipulation policies, addressing a critical missing capability in today's
> robotics landscape.

