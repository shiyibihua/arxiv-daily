---
layout: default
title: On the Faithfulness of Visual Thinking: Measurement and Enhancement
---

# On the Faithfulness of Visual Thinking: Measurement and Enhancement

**arXiv**: [2510.23482v1](https://arxiv.org/abs/2510.23482) | [PDF](https://arxiv.org/pdf/2510.23482.pdf)

**ä½œè€…**: Zujing Liu, Junwen Pan, Qi She, Yuan Gao, Guisong Xia

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCCMå­¦ä¹ ç­–ç•¥ä»¥å¢žå¼ºè§†è§‰-è¯­è¨€æ¨¡åž‹å¤šæ¨¡æ€æ€ç»´é“¾çš„å¿ å®žæ€§**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ€ç»´é“¾` `è§†è§‰å¿ å®žæ€§` `å¼ºåŒ–å¾®è°ƒ` `SCCMå­¦ä¹ ` `è‡ªåŠ¨è¯„ä¼°` `è§†è§‰-è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMCoTä¸­è§†è§‰ä¿¡æ¯ä¸å¿ å®žï¼Œå¸¸è¢«å¿½ç•¥ï¼Œå¯¼è‡´æŽ¨ç†è¿‡ç¨‹ä¸å¯é 
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥SCCMå­¦ä¹ ï¼Œé¼“åŠ±ç”Ÿæˆå……åˆ†ä¸”æœ€å°è§†è§‰ç»„ä»¶ï¼Œæ— éœ€é¢å¤–æ ‡æ³¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’ŒæŽ¨ç†åŸºå‡†ä¸Šï¼ŒSCCMä¸€è‡´æå‡è§†è§‰å¿ å®žæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent large vision-language models (LVLMs) can generate vision-text
> multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning
> (RFT). However, we observe that the visual information incorporated in MCoT is
> often inaccurate, though still yield correct answers, indicating a lack of
> faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to
> the RL reward in RFT, which solely incentivizes the format of interleaved
> vision-text cues, ie, it encourages the model to incorporate visual information
> into its text reasoning steps without considering the correctness of the visual
> information. In this paper, we first probe the faithfulness of MCoT by
> measuring how much the prediction changes when its visual and textual thoughts
> are intervened. Surprisingly, the model's predictions remain nearly unchanged
> under visual intervention but change significantly under textual intervention,
> indicating that the visual evidence is largely ignored. To further analyze
> visual information, we introduce an automated LVLM-based evaluation metric that
> quantifies the faithfulness of visual cues from two perspectives: reliability
> and sufficiency. Our evaluation reveals that the visual information in current
> MCoT traces is simultaneously unreliable and insufficient. To address this
> issue, we propose a novel MCoT learning strategy termed Sufficient-Component
> Cause Model (SCCM) learning. This approach encourages the MCoT to generate
> sufficient yet minimal visual components that are independently capable of
> leading to correct answers. We note that the proposed SCCM is annotation-free
> and compatible with various RFT for MCoT in a plug-and-play manner. Empirical
> results demonstrate that SCCM consistently improves the visual faithfulness
> across a suite of fine-grained perception and reasoning benchmarks. Code is
> available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.

