---
layout: default
title: Autoregressive Styled Text Image Generation, but Make it Reliable
---

# Autoregressive Styled Text Image Generation, but Make it Reliable

**arXiv**: [2510.23240v1](https://arxiv.org/abs/2510.23240) | [PDF](https://arxiv.org/pdf/2510.23240.pdf)

**ä½œè€…**: Carmine Zaccagnino, Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºErukuæ–¹æ³•ä»¥è§£å†³è‡ªå›žå½’é£Žæ ¼æ–‡æœ¬å›¾åƒç”Ÿæˆä¸­çš„å†…å®¹å¯æŽ§æ€§é—®é¢˜**

**å…³é”®è¯**: `é£Žæ ¼æ–‡æœ¬å›¾åƒç”Ÿæˆ` `è‡ªå›žå½’æ¨¡åž‹` `å¤šæ¨¡æ€æç¤º` `åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼` `å†…å®¹å¯¹é½` `æ‰‹å†™æ–‡æœ¬ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå›žå½’é£Žæ ¼æ–‡æœ¬å›¾åƒç”Ÿæˆå­˜åœ¨å†…å®¹å¯¹é½å·®ã€ç¼ºä¹åœæ­¢æœºåˆ¶å’Œè§†è§‰ä¼ªå½±é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šå°†ç”Ÿæˆä»»åŠ¡é‡æž„ä¸ºå¤šæ¨¡æ€æç¤ºæ¡ä»¶ç”Ÿæˆï¼Œå¼•å…¥ç‰¹æ®Šæ–‡æœ¬ä»¤ç‰Œå’Œåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šErukuå‡å°‘è¾“å…¥éœ€æ±‚ï¼Œæå‡æœªè§é£Žæ ¼æ³›åŒ–èƒ½åŠ›å’Œæ–‡æœ¬æç¤ºéµå¾ªåº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating faithful and readable styled text images (especially for Styled
> Handwritten Text generation - HTG) is an open problem with several possible
> applications across graphic design, document understanding, and image editing.
> A lot of research effort in this task is dedicated to developing strategies
> that reproduce the stylistic characteristics of a given writer, with promising
> results in terms of style fidelity and generalization achieved by the recently
> proposed Autoregressive Transformer paradigm for HTG. However, this method
> requires additional inputs, lacks a proper stop mechanism, and might end up in
> repetition loops, generating visual artifacts. In this work, we rethink the
> autoregressive formulation by framing HTG as a multimodal prompt-conditioned
> generation task, and tackle the content controllability issues by introducing
> special textual input tokens for better alignment with the visual ones.
> Moreover, we devise a Classifier-Free-Guidance-based strategy for our
> autoregressive model. Through extensive experimental validation, we demonstrate
> that our approach, dubbed Eruku, compared to previous solutions requires fewer
> inputs, generalizes better to unseen styles, and follows more faithfully the
> textual prompt, improving content adherence.

