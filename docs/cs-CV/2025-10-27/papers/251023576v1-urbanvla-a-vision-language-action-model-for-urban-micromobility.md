---
layout: default
title: UrbanVLA: A Vision-Language-Action Model for Urban Micromobility
---

# UrbanVLA: A Vision-Language-Action Model for Urban Micromobility

**arXiv**: [2510.23576v1](https://arxiv.org/abs/2510.23576) | [PDF](https://arxiv.org/pdf/2510.23576.pdf)

**ä½œè€…**: Anqi Li, Zhiyong Wang, Jiazhao Zhang, Minghan Li, Yunpeng Qi, Zhibo Chen, Zhizheng Zhang, He Wang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUrbanVLAæ¨¡å‹ä»¥è§£å†³åŸå¸‚å¾®ç§»åŠ¨ä¸­çš„é•¿è·ç¦»å¯¼èˆªé—®é¢˜**

**å…³é”®è¯**: `åŸå¸‚å¾®ç§»åŠ¨å¯¼èˆª` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `ä¸¤é˜¶æ®µè®­ç»ƒ` `è·¯å¾„-è§†è§‰å¯¹é½` `å¼ºåŒ–å¾®è°ƒ` `å¤§è§„æ¨¡ç¯å¢ƒå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸå¸‚å¾®ç§»åŠ¨åœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°å¯é é•¿è·ç¦»å¯¼èˆªçš„æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œç»“åˆç›‘ç£å’Œå¼ºåŒ–å¾®è°ƒï¼Œå¯¹é½è·¯å¾„ç‚¹ä¸è§†è§‰è§‚å¯Ÿ
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨MetaUrbanä¸Šè¶…è¶ŠåŸºçº¿55%ï¼Œå®ç°å¤§è§„æ¨¡åŸå¸‚ç¯å¢ƒçš„å¯é å¯¼èˆª

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Urban micromobility applications, such as delivery robots, demand reliable
> navigation across large-scale urban environments while following long-horizon
> route instructions. This task is particularly challenging due to the dynamic
> and unstructured nature of real-world city areas, yet most existing navigation
> methods remain tailored to short-scale and controllable scenarios. Effective
> urban micromobility requires two complementary levels of navigation skills:
> low-level capabilities such as point-goal reaching and obstacle avoidance, and
> high-level capabilities, such as route-visual alignment. To this end, we
> propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework
> designed for scalable urban navigation. Our method explicitly aligns noisy
> route waypoints with visual observations during execution, and subsequently
> plans trajectories to drive the robot. To enable UrbanVLA to master both levels
> of navigation, we employ a two-stage training pipeline. The process begins with
> Supervised Fine-Tuning (SFT) using simulated environments and trajectories
> parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on
> a mixture of simulation and real-world data, which enhances the model's safety
> and adaptability in real-world settings. Experiments demonstrate that UrbanVLA
> surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.
> Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both
> scalability to large-scale urban environments and robustness against real-world
> uncertainties.

