---
layout: default
title: Localising under the drape: proprioception in the era of distributed surgical robotic system
---

# Localising under the drape: proprioception in the era of distributed surgical robotic system

**arXiv**: [2510.23512v1](https://arxiv.org/abs/2510.23512) | [PDF](https://arxiv.org/pdf/2510.23512.pdf)

**ä½œè€…**: Martin Huber, Nicola A. Cavalcanti, Ayoob Davoodi, Ruixuan Li, Christopher E. Mower, Fabio Carrillo, Christoph J. Laux, Francois Teyssere, Thibault Chandanson, Antoine HarlÃ©, Elie Saghbiny, Mazda Farshad, Guillaume Morel, Emmanuel Vander Poorten, Philipp FÃ¼rnstahl, SÃ©bastien Ourselin, Christos Bergeles, Tom Vercauteren

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— æ ‡è®°æœ¬ä½“æ„ŸçŸ¥æ–¹æ³•ï¼Œä»¥è§£å†³åˆ†å¸ƒå¼æ‰‹æœ¯æœºå™¨äººå®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ‰‹æœ¯æœºå™¨äººå®šä½` `æ— æ ‡è®°æ„ŸçŸ¥` `ç«‹ä½“è§†è§‰` `Transformeræ¨¡åž‹` `å¤šæœºå™¨äººç³»ç»Ÿ` `æ‰‹æœ¯åœºæ™¯ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹æœ¯æœºå™¨äººç¼ºä¹ç©ºé—´æ„ŸçŸ¥ï¼Œæ˜“ç¢°æ’žä¸”ç¡¬ä»¶è´Ÿæ‹…é‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è½»é‡ç«‹ä½“RGBç›¸æœºå’ŒTransformeræ¨¡åž‹ï¼Œæ— éœ€æ ‡è®°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºäºŽå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæå‡è·Ÿè¸ªå¯è§åº¦25%ï¼Œæ”¯æŒå¤šæœºå™¨äººäº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite their mechanical sophistication, surgical robots remain blind to
> their surroundings. This lack of spatial awareness causes collisions, system
> recoveries, and workflow disruptions, issues that will intensify with the
> introduction of distributed robots with independent interacting arms. Existing
> tracking systems rely on bulky infrared cameras and reflective markers,
> providing only limited views of the surgical scene and adding hardware burden
> in crowded operating rooms. We present a marker-free proprioception method that
> enables precise localisation of surgical robots under their sterile draping
> despite associated obstruction of visual cues. Our method solely relies on
> lightweight stereo-RGB cameras and novel transformer-based deep learning
> models. It builds on the largest multi-centre spatial robotic surgery dataset
> to date (1.4M self-annotated images from human cadaveric and preclinical in
> vivo studies). By tracking the entire robot and surgical scene, rather than
> individual markers, our approach provides a holistic view robust to occlusions,
> supporting surgical scene understanding and context-aware control. We
> demonstrate an example of potential clinical benefits during in vivo breathing
> compensation with access to tissue dynamics, unobservable under state of the
> art tracking, and accurately locate in multi-robot systems for future
> intelligent interaction. In addition, and compared with existing systems, our
> method eliminates markers and improves tracking visibility by 25%. To our
> knowledge, this is the first demonstration of marker-free proprioception for
> fully draped surgical robots, reducing setup complexity, enhancing safety, and
> paving the way toward modular and autonomous robotic surgery.

