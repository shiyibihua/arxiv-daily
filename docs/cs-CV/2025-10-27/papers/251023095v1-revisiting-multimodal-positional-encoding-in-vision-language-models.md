---
layout: default
title: Revisiting Multimodal Positional Encoding in Vision-Language Models
---

# Revisiting Multimodal Positional Encoding in Vision-Language Models

**arXiv**: [2510.23095v1](https://arxiv.org/abs/2510.23095) | [PDF](https://arxiv.org/pdf/2510.23095.pdf)

**ä½œè€…**: Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMHRoPEå’ŒMRoPE-Iä»¥æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç **

**å…³é”®è¯**: `å¤šæ¨¡æ€ä½ç½®ç¼–ç ` `æ—‹è½¬ä½ç½®åµŒå…¥` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å³æ’å³ç”¨æ–¹æ³•` `å¤šæ¨¡æ€ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€ä½ç½®ç¼–ç ç¼ºä¹ç³»ç»Ÿç ”ç©¶ï¼Œå½±å“æ¨¡åž‹å¸ƒå±€ç†è§£å’Œè¡¨ç¤ºèƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä½ç½®è®¾è®¡å’Œé¢‘çŽ‡åˆ†é…åˆ†æžï¼Œæå‡ºæ— éœ€æž¶æž„æ›´æ”¹çš„å³æ’å³ç”¨å˜ä½“
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡é€šç”¨å’Œç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal position encoding is essential for vision-language models, yet
> there has been little systematic investigation into multimodal position
> encoding. We conduct a comprehensive analysis of multimodal Rotary Positional
> Embedding (RoPE) by examining its two core components: position design and
> frequency allocation. Through extensive experiments, we identify three key
> guidelines: positional coherence, full frequency utilization, and preservation
> of textual priors-ensuring unambiguous layout, rich representation, and
> faithful transfer from the pre-trained LLM. Based on these insights, we propose
> Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and
> plug-and-play variants that require no architectural changes. Our methods
> consistently outperform existing approaches across diverse benchmarks, with
> significant improvements in both general and fine-grained multimodal
> understanding. Code will be avaliable at
> https://github.com/JJJYmmm/Multimodal-RoPEs.

