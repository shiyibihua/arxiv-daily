---
layout: default
title: MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification
---

# MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification

**arXiv**: [2510.23301v1](https://arxiv.org/abs/2510.23301) | [PDF](https://arxiv.org/pdf/2510.23301.pdf)

**ä½œè€…**: Yingying Feng, Jie Li, Jie Hu, Yukang Zhang, Lei Tan, Jiayi Ji

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMDReIDæ¡†æž¶ï¼Œé€šè¿‡æ¨¡æ€è§£è€¦å­¦ä¹ è§£å†³å¤šæ¨¡æ€ç‰©ä½“é‡è¯†åˆ«ä¸­çš„æ¨¡æ€ä¸ä¸€è‡´é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç‰©ä½“é‡è¯†åˆ«` `æ¨¡æ€è§£è€¦å­¦ä¹ ` `æ¨¡æ€æ„ŸçŸ¥åº¦é‡å­¦ä¹ ` `è·¨æ¨¡æ€æ£€ç´¢` `ç‰¹å¾åˆ†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°å®žç‰©ä½“é‡è¯†åˆ«å¸¸é¢ä¸´æ¨¡æ€ä¸ä¸€è‡´ï¼Œå¦‚RGBä¸ŽNIRå›¾åƒæŸ¥è¯¢ï¼ŒçŽ°æœ‰æ–¹æ³•å‡è®¾æ¨¡æ€åŒ¹é…ï¼Œé™åˆ¶å®žç”¨æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ¨¡æ€è§£è€¦å­¦ä¹ å’Œæ¨¡æ€æ„ŸçŸ¥åº¦é‡å­¦ä¹ ï¼Œåˆ†è§£ç‰¹å¾ä¸ºå…±äº«ä¸Žç‰¹å®šéƒ¨åˆ†ï¼Œå¢žå¼ºè·¨æ¨¡æ€æ£€ç´¢èƒ½åŠ›ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡æ€åŒ¹é…å’Œä¸åŒ¹é…åœºæ™¯ä¸‹å‡å–å¾—æ˜¾è‘—mAPæå‡ï¼Œæœ€é«˜è¾¾11.5%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Real-world object re-identification (ReID) systems often face modality
> inconsistencies, where query and gallery images come from different sensors
> (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched
> conditions, which limits their robustness and scalability in practical
> applications. To address this challenge, we propose MDReID, a flexible
> any-to-any image-level ReID framework designed to operate under both
> modality-matched and modality-mismatched scenarios. MDReID builds on the
> insight that modality information can be decomposed into two components:
> modality-shared features that are predictable and transferable, and
> modality-specific features that capture unique, modality-dependent
> characteristics. To effectively leverage this, MDReID introduces two key
> components: the Modality Decoupling Learning (MDL) and Modality-aware Metric
> Learning (MML). Specifically, MDL explicitly decomposes modality features into
> modality-shared and modality-specific representations, enabling effective
> retrieval in both modality-aligned and mismatched scenarios. MML, a tailored
> metric learning strategy, further enforces orthogonality and complementarity
> between the two components to enhance discriminative power across modalities.
> Extensive experiments conducted on three challenging multi-modality ReID
> benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the
> superiority of MDReID. Notably, MDReID achieves significant mAP improvements of
> 9.8\%, 3.0\%, and 11.5\% in general modality-matched scenarios, and average
> gains of 3.4\%, 11.8\%, and 10.9\% in modality-mismatched scenarios,
> respectively. The code is available at:
> \textcolor{magenta}{https://github.com/stone96123/MDReID}.

