---
layout: default
title: Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning
---

# Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning

**arXiv**: [2510.22892v1](https://arxiv.org/abs/2510.22892) | [PDF](https://arxiv.org/pdf/2510.22892.pdf)

**ä½œè€…**: Jingzehua Xu, Yangyang Li, Yangfei Chen, Guanwen Xie, Shuai Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è™šæ‹Ÿæ¨¡åž‹æŽ§åˆ¶ï¼Œç»“åˆLLMä¸ŽLyapunovå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æœºæ¢°è‡‚åœ¨ä¸ç¡®å®šçŽ¯å¢ƒä¸­çš„é€‚åº”æ€§ä¸Žç¨³å®šæ€§ã€‚**

**å…³é”®è¯**: `è™šæ‹Ÿæ¨¡åž‹æŽ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹` `Lyapunovç¨³å®šæ€§` `æœºå™¨äººæŽ§åˆ¶` `è‡ªé€‚åº”æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿè™šæ‹Ÿæ¨¡åž‹æŽ§åˆ¶å‚æ•°å›ºå®šã€ç»„ä»¶åè°ƒä¸è¶³ï¼Œåœ¨æ‰°åŠ¨ä¸‹æ˜“å¤±ç¨³ä¸”é€‚åº”æ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šLLMæä¾›å…ˆéªŒä¸ŽæŽ¨ç†å¢žå¼ºåè°ƒï¼ŒLyapunovå¼ºåŒ–å­¦ä¹ ç¡®ä¿ç†è®ºç¨³å®šæ€§çº¦æŸã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨7è‡ªç”±åº¦Pandaè‡‚ä»¿çœŸä¸­ï¼ŒåŠ¨æ€ä»»åŠ¡è¡¨çŽ°ä¼˜å¼‚ï¼Œå¹³è¡¡ç›®æ ‡å¹¶ä¿è¯å®‰å…¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robotic arms are increasingly deployed in uncertain environments, yet
> conventional control pipelines often become rigid and brittle when exposed to
> perturbations or incomplete information. Virtual Model Control (VMC) enables
> compliant behaviors by embedding virtual forces and mapping them into joint
> torques, but its reliance on fixed parameters and limited coordination among
> virtual components constrains adaptability and may undermine stability as task
> objectives evolve. To address these limitations, we propose Adaptive VMC with
> Large Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),
> which preserves the physical interpretability of VMC while supporting
> stability-guaranteed online adaptation. The LLM provides structured priors and
> high-level reasoning that enhance coordination among virtual components,
> improve sample efficiency, and facilitate flexible adjustment to varying task
> requirements. Complementarily, Lyapunov-based RL enforces theoretical stability
> constraints, ensuring safe and reliable adaptation under uncertainty. Extensive
> simulations on a 7-DoF Panda arm demonstrate that our approach effectively
> balances competing objectives in dynamic tasks, achieving superior performance
> while highlighting the synergistic benefits of LLM guidance and
> Lyapunov-constrained adaptation.

