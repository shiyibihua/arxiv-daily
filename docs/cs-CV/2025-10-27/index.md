---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-27
---

# cs.CVï¼ˆ2025-10-27ï¼‰

ğŸ“Š å…± **39** ç¯‡è®ºæ–‡
 | ğŸ”— **8** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (11 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (17 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251022964v1-survey-of-multimodal-geospatial-foundation-models-techniques-applica.html">Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges</a></td>
  <td>ç»¼è¿°å¤šæ¨¡æ€åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œåº”å¯¹é¥æ„Ÿå›¾åƒåˆ†æçš„æŒ‘æˆ˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22964v1" onclick="toggleFavorite(this, '2510.22964v1', 'Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251024795v1-a-survey-on-efficient-vision-language-action-models.html">A Survey on Efficient Vision-Language-Action Models</a></td>
  <td>å¯¹é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆEfficient VLAï¼‰çš„ç»¼è¿°ï¼Œæ—¨åœ¨é™ä½è®¡ç®—å’Œæ•°æ®éœ€æ±‚ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.24795v1" onclick="toggleFavorite(this, '2510.24795v1', 'A Survey on Efficient Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251023415v1-towards-generalisable-foundation-models-for-3d-brain-mri.html">Towards Generalisable Foundation Models for 3D Brain MRI</a></td>
  <td>BrainFoundï¼šé¢å‘3Dè„‘éƒ¨MRIçš„é€šç”¨Foundationæ¨¡å‹ï¼Œæå‡ç–¾ç—…æ£€æµ‹ä¸åˆ†å‰²æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23415v1" onclick="toggleFavorite(this, '2510.23415v1', 'Towards Generalisable Foundation Models for 3D Brain MRI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251024792v2-pisa-bench-the-pisa-index-as-a-multilingual-and-multimodal-metric-fo.html">PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models</a></td>
  <td>PISA-Benchï¼šä¸€ä¸ªå¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.24792v2" onclick="toggleFavorite(this, '2510.24792v2', 'PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251023594v4-prism-bench-a-benchmark-of-puzzle-based-visual-tasks-with-cot-error-.html">PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection</a></td>
  <td>PRISM-Benchï¼šä¸€ä¸ªåŸºäºè°œé¢˜çš„å¯è§£é‡Šå¤šæ¨¡æ€æ¨ç†è¯„æµ‹åŸºå‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23594v4" onclick="toggleFavorite(this, '2510.23594v4', 'PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251023325v1-multitask-multimodal-self-supervised-learning-for-medical-images.html">Multitask Multimodal Self-Supervised Learning for Medical Images</a></td>
  <td>æå‡ºMedformerï¼Œç”¨äºåŒ»å­¦å›¾åƒå¤šä»»åŠ¡å¤šæ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23325v1" onclick="toggleFavorite(this, '2510.23325v1', 'Multitask Multimodal Self-Supervised Learning for Medical Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251023299v1-mmsd30-a-multi-image-benchmark-for-real-world-multimodal-sarcasm-det.html">MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection</a></td>
  <td>æå‡ºMMSD3.0å¤šå›¾è®½åˆºæ£€æµ‹åŸºå‡†å’ŒCIRMæ¨¡å‹ï¼Œè§£å†³çœŸå®åœºæ™¯å¤šå›¾çº¿ç´¢è®½åˆºè¯†åˆ«é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23299v1" onclick="toggleFavorite(this, '2510.23299v1', 'MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251023151v1-ag-fusion-adaptive-gated-multimodal-fusion-for-3d-object-detection-i.html">AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes</a></td>
  <td>æå‡ºè‡ªé€‚åº”é—¨æ§èåˆæ–¹æ³•ä»¥è§£å†³å¤æ‚åœºæ™¯ä¸­çš„3Dç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23151v1" onclick="toggleFavorite(this, '2510.23151v1', 'AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251023145v1-implicit-modeling-for-transferability-estimation-of-vision-foundatio.html">Implicit Modeling for Transferability Estimation of Vision Foundation Models</a></td>
  <td>æå‡ºéšå¼è¿ç§»å»ºæ¨¡(ITM)ï¼Œé«˜æ•ˆè¯„ä¼°è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23145v1" onclick="toggleFavorite(this, '2510.23145v1', 'Implicit Modeling for Transferability Estimation of Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251023095v2-revisiting-multimodal-positional-encoding-in-vision-language-models.html">Revisiting Multimodal Positional Encoding in Vision-Language Models</a></td>
  <td>æå‡ºå¤šå¤´æ—‹è½¬ä½ç½®ç¼–ç MHRoPEåŠå…¶å˜ä½“MRoPE-Iï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç èƒ½åŠ›ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23095v2" onclick="toggleFavorite(this, '2510.23095v2', 'Revisiting Multimodal Positional Encoding in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251022946v4-lightfusion-a-light-weighted-double-fusion-framework-for-unified-mul.html">LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation</a></td>
  <td>LightFusionï¼šè½»é‡çº§åŒé‡èåˆæ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22946v4" onclick="toggleFavorite(this, '2510.22946v4', 'LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251023907v2-dynastride-dynamic-stride-windowing-with-mmcot-for-instructional-mul.html">DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</a></td>
  <td>DynaStrideï¼šç»“åˆMMCoTçš„åŠ¨æ€æ­¥é•¿çª—å£åŒ–æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæ•™å­¦è§†é¢‘çš„å¤šåœºæ™¯å­—å¹•ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23907v2" onclick="toggleFavorite(this, '2510.23907v2', 'DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251023603v2-pixelrefer-a-unified-framework-for-spatio-temporal-object-referring-.html">PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity</a></td>
  <td>æå‡ºPixelReferï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŒºåŸŸçº§MLLMæ¡†æ¶ï¼Œç”¨äºä»»æ„ç²’åº¦çš„æ—¶ç©ºå¯¹è±¡æŒ‡ä»£ç†è§£ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23603v2" onclick="toggleFavorite(this, '2510.23603v2', 'PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251023482v1-on-the-faithfulness-of-visual-thinking-measurement-and-enhancement.html">On the Faithfulness of Visual Thinking: Measurement and Enhancement</a></td>
  <td>æå‡ºSCCMå­¦ä¹ ç­–ç•¥ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹å¤šæ¨¡æ€æ¨ç†ä¸­è§†è§‰ä¿¡æ¯çš„å¯é æ€§å’Œå……åˆ†æ€§ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23482v1" onclick="toggleFavorite(this, '2510.23482v1', 'On the Faithfulness of Visual Thinking: Measurement and Enhancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251023785v1-countformer-a-transformer-framework-for-learning-visual-repetition-a.html">CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting</a></td>
  <td>CountFormerï¼šTransformeræ¡†æ¶å­¦ä¹ è§†è§‰é‡å¤ä¸ç»“æ„ï¼Œå®ç°ç±»åˆ«æ— å…³çš„ç›®æ ‡è®¡æ•°</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23785v1" onclick="toggleFavorite(this, '2510.23785v1', 'CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251023253v1-a-video-is-not-worth-a-thousand-words.html">A Video Is Not Worth a Thousand Words</a></td>
  <td>æå‡ºåŸºäºShapleyå€¼çš„ç‰¹å¾å½’å› å’Œæ¨¡æ€è¯„åˆ†æ–¹æ³•ï¼Œè¯„ä¼°VLMåœ¨VQAä»»åŠ¡ä¸­çš„æ–‡æœ¬ä¾èµ–æ€§ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23253v1" onclick="toggleFavorite(this, '2510.23253v1', 'A Video Is Not Worth a Thousand Words')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251024788v1-the-underappreciated-power-of-vision-models-for-graph-structural-und.html">The Underappreciated Power of Vision Models for Graph Structural Understanding</a></td>
  <td>åˆ©ç”¨è§†è§‰æ¨¡å‹è¿›è¡Œå›¾ç»“æ„ç†è§£ï¼Œæ€§èƒ½åª²ç¾å›¾ç¥ç»ç½‘ç»œï¼Œå¹¶æ­ç¤ºå…¶å…¨å±€æ„ŸçŸ¥ä¼˜åŠ¿</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.24788v1" onclick="toggleFavorite(this, '2510.24788v1', 'The Underappreciated Power of Vision Models for Graph Structural Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251023184v1-finding-3d-scene-analogies-with-multimodal-foundation-models.html">Finding 3D Scene Analogies with Multimodal Foundation Models</a></td>
  <td>åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å®ç°é›¶æ ·æœ¬ä¸‰ç»´åœºæ™¯ç±»æ¯”ï¼Œç”¨äºæœºå™¨äººè½¨è¿¹å’Œè·¯å¾„ç‚¹è¿ç§»ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23184v1" onclick="toggleFavorite(this, '2510.23184v1', 'Finding 3D Scene Analogies with Multimodal Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251023205v1-vr-drive-viewpoint-robust-end-to-end-driving-with-feed-forward-3d-ga.html">VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting</a></td>
  <td>VR-Driveï¼šåˆ©ç”¨å‰é¦ˆ3Dé«˜æ–¯æº…å°„å®ç°è§†è§’é²æ£’çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23205v1" onclick="toggleFavorite(this, '2510.23205v1', 'VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251023473v1-video-thinker-sparking-thinking-with-videos-via-reinforcement-learni.html">Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning</a></td>
  <td>æå‡ºVideo-Thinkerï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹èƒ½MLLMè¿›è¡Œè§†é¢‘æ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23473v1" onclick="toggleFavorite(this, '2510.23473v1', 'Video-Thinker: Sparking &quot;Thinking with Videos&quot; via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251023397v1-videotg-r1-boosting-video-temporal-grounding-via-curriculum-reinforc.html">VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations</a></td>
  <td>VideoTG-R1ï¼šé€šè¿‡åå°„è¾¹ç•Œæ ‡æ³¨çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æå‡è§†é¢‘æ—¶åºå®šä½æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23397v1" onclick="toggleFavorite(this, '2510.23397v1', 'VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251023224v1-accurate-and-scalable-multimodal-pathology-retrieval-via-attentive-v.html">Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment</a></td>
  <td>PathSearchï¼šåŸºäºæ³¨æ„åŠ›è§†è§‰-è¯­è¨€å¯¹é½çš„ç²¾å‡†å¯æ‰©å±•å¤šæ¨¡æ€ç—…ç†å›¾åƒæ£€ç´¢æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23224v1" onclick="toggleFavorite(this, '2510.23224v1', 'Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251023043v1-hieramamba-video-temporal-grounding-via-hierarchical-anchor-mamba-po.html">HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling</a></td>
  <td>HieraMambaï¼šé€šè¿‡åˆ†å±‚Anchor-Mambaæ± åŒ–å®ç°è§†é¢‘æ—¶åºå®šä½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23043v1" onclick="toggleFavorite(this, '2510.23043v1', 'HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251023588v2-farmer-flow-autoregressive-transformer-over-pixels.html">FARMER: Flow AutoRegressive Transformer over Pixels</a></td>
  <td>FARMERï¼šæå‡ºä¸€ç§åŸºäºæµè‡ªå›å½’Transformerçš„åƒç´ ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°ç²¾ç¡®ä¼¼ç„¶ä¼°è®¡å’Œé«˜è´¨é‡å›¾åƒåˆæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23588v2" onclick="toggleFavorite(this, '2510.23588v2', 'FARMER: Flow AutoRegressive Transformer over Pixels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251023497v2-vold-reasoning-transfer-from-llms-to-vision-language-models-via-on-p.html">VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation</a></td>
  <td>æå‡ºVOLDæ¡†æ¶ï¼Œé€šè¿‡ç­–ç•¥è’¸é¦å°†LLMçš„æ¨ç†èƒ½åŠ›è¿ç§»åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23497v2" onclick="toggleFavorite(this, '2510.23497v2', 'VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251023479v1-mergemix-a-unified-augmentation-paradigm-for-visual-and-multi-modal-.html">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</a></td>
  <td>æå‡ºMergeMixï¼Œç»Ÿä¸€è§†è§‰å’Œå¤šæ¨¡æ€ç†è§£çš„å¢å¼ºèŒƒå¼ï¼Œæå‡æ•ˆç‡å’Œå¯¹é½è´¨é‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23479v1" onclick="toggleFavorite(this, '2510.23479v1', 'MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251022937v1-bi-encoder-contrastive-learning-for-fingerprint-and-iris-biometrics.html">Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</a></td>
  <td>æå‡ºåŸºäºBi-Encoderå¯¹æ¯”å­¦ä¹ çš„æŒ‡çº¹å’Œè™¹è†œè·¨æ¨¡æ€ç”Ÿç‰©ç‰¹å¾è¯†åˆ«æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22937v1" onclick="toggleFavorite(this, '2510.22937v1', 'Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251023607v1-concerto-joint-2d-3d-self-supervised-learning-emerges-spatial-repres.html">Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</a></td>
  <td>Concertoï¼šèåˆ2D-3Dè‡ªç›‘ç£å­¦ä¹ ï¼Œæ¶Œç°ç©ºé—´è¡¨å¾</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23607v1" onclick="toggleFavorite(this, '2510.23607v1', 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251023930v1-planargs-high-fidelity-indoor-3d-gaussian-splatting-guided-by-vision.html">PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors</a></td>
  <td>PlanarGSï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€å¹³é¢å…ˆéªŒå®ç°é«˜ä¿çœŸå®¤å†…3Dé«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23930v1" onclick="toggleFavorite(this, '2510.23930v1', 'PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251022930v1-gen-langsplat-generalized-language-gaussian-splatting-with-pre-train.html">Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression</a></td>
  <td>Gen-LangSplatï¼šåˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾å‹ç¼©å®ç°é€šç”¨è¯­è¨€é«˜æ–¯æº…å°„ï¼Œæå‡æ•ˆç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22930v1" onclick="toggleFavorite(this, '2510.22930v1', 'Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251023087v1-endowave-rational-wavelet-4d-gaussian-splatting-for-endoscopic-recon.html">EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction</a></td>
  <td>EndoWaveï¼šç”¨äºå†…çª¥é•œé‡å»ºçš„Rational-Wavelet 4Dé«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23087v1" onclick="toggleFavorite(this, '2510.23087v1', 'EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251023894v1-improving-visual-discriminability-of-clip-for-training-free-open-voc.html">Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation</a></td>
  <td>æå‡ºLHT-CLIPï¼Œæ— éœ€è®­ç»ƒå³å¯æå‡CLIPåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„è§†è§‰åŒºåˆ†æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23894v1" onclick="toggleFavorite(this, '2510.23894v1', 'Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251023574v1-more-than-generation-unifying-generation-and-depth-estimation-via-te.html">More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models</a></td>
  <td>æå‡ºMERGEï¼Œé€šè¿‡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç»Ÿä¸€å›¾åƒç”Ÿæˆä¸æ·±åº¦ä¼°è®¡</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23574v1" onclick="toggleFavorite(this, '2510.23574v1', 'More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251023494v1-yesnt-are-diffusion-relighting-models-ready-for-capture-stage-compos.html">Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap</a></td>
  <td>æå‡ºæ··åˆæ¡†æ¶Yesntï¼Œæå‡æ‰©æ•£æ¨¡å‹åœ¨åŠ¨æ€ä½“ç§¯è§†é¢‘å…‰ç…§é‡æ„ä¸­çš„æ—¶åºç¨³å®šæ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23494v1" onclick="toggleFavorite(this, '2510.23494v1', 'Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251023478v1-urbaning-v2x-a-large-scale-multi-vehicle-multi-infrastructure-datase.html">UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception</a></td>
  <td>UrbanIng-V2Xï¼šç”¨äºååŒæ„ŸçŸ¥çš„å¤šè·¯å£å¤§è§„æ¨¡å¤šè½¦è¾†å¤šåŸºç¡€è®¾æ–½æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23478v1" onclick="toggleFavorite(this, '2510.23478v1', 'UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/251022936v1-positional-preservation-embedding-for-multimodal-large-language-mode.html">Positional Preservation Embedding for Multimodal Large Language Models</a></td>
  <td>æå‡ºä½ç½®ä¿æŒåµŒå…¥ï¼ˆPPEï¼‰ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22936v1" onclick="toggleFavorite(this, '2510.22936v1', 'Positional Preservation Embedding for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/251023569v1-egothinker-unveiling-egocentric-reasoning-with-spatio-temporal-cot.html">EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</a></td>
  <td>EgoThinkerï¼šåˆ©ç”¨æ—¶ç©ºCoTæ­ç¤ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ¨ç†èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23569v1" onclick="toggleFavorite(this, '2510.23569v1', 'EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>38</td>
  <td><a href="./papers/251023203v1-decodino-3d-human-scene-contact-prediction-with-semantic-classificat.html">DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification</a></td>
  <td>æå‡ºDecoDINOä»¥è§£å†³äººç±»ä¸åœºæ™¯æ¥è§¦é¢„æµ‹é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.23203v1" onclick="toggleFavorite(this, '2510.23203v1', 'DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/251022975v1-vomp-predicting-volumetric-mechanical-property-fields.html">VoMP: Predicting Volumetric Mechanical Property Fields</a></td>
  <td>VoMPï¼šé¢„æµ‹ä¸‰ç»´ç‰©ä½“ä½“ç§¯æœºæ¢°å±æ€§åœºï¼ŒåŠ é€Ÿç‰©ç†ä»¿çœŸã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22975v1" onclick="toggleFavorite(this, '2510.22975v1', 'VoMP: Predicting Volumetric Mechanical Property Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)