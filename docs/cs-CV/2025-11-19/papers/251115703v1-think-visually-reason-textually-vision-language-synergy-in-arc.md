---
layout: default
title: Think Visually, Reason Textually: Vision-Language Synergy in ARC
---

# Think Visually, Reason Textually: Vision-Language Synergy in ARC

**arXiv**: [2511.15703v1](https://arxiv.org/abs/2511.15703) | [PDF](https://arxiv.org/pdf/2511.15703.pdf)

**ä½œè€…**: Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰-è¯­è¨€ååŒæ–¹æ³•ä»¥æå‡ARC-AGIæŠ½è±¡æŽ¨ç†æ€§èƒ½**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€ååŒ` `æŠ½è±¡æŽ¨ç†` `ARC-AGI` `æ¨¡æ€åˆ‡æ¢` `è‡ªæ ¡æ­£æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå‰æ²¿åŸºç¡€æ¨¡åž‹åœ¨å°‘é‡ç¤ºä¾‹ä¸‹éš¾ä»¥æŽ¨æ–­ç»“æž„åŒ–è½¬æ¢è§„åˆ™ï¼Œå¦‚ARC-AGIæµ‹è¯•æ‰€ç¤ºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†è§‰-è¯­è¨€ååŒæŽ¨ç†å’Œæ¨¡æ€åˆ‡æ¢è‡ªæ ¡æ­£ï¼Œåˆ©ç”¨è§†è§‰æŠ½è±¡ä¸Žè¯­è¨€è§„åˆ™äº’è¡¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§æ¨¡åž‹å’Œä»»åŠ¡ä¸Šï¼Œç›¸æ¯”çº¯æ–‡æœ¬åŸºçº¿æ€§èƒ½æå‡æœ€é«˜è¾¾4.33%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.

