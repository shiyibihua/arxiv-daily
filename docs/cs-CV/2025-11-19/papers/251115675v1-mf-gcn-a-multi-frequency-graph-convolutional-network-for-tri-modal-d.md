---
layout: default
title: MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features
---

# MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features

**arXiv**: [2511.15675v1](https://arxiv.org/abs/2511.15675) | [PDF](https://arxiv.org/pdf/2511.15675.pdf)

**ä½œè€…**: Sejuti Rahman, Swakshar Deb, MD. Sameer Iqbal Chowdhury, MD. Jubair Ahmed Sourov, Mohammad Shamsuddin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šé¢‘å›¾å·ç§¯ç½‘ç»œä»¥è§£å†³åŸºäºŽçœ¼åŠ¨ã€é¢éƒ¨å’Œå£°éŸ³ç‰¹å¾çš„æŠ‘éƒç—‡æ£€æµ‹é—®é¢˜**

**å…³é”®è¯**: `å¤šé¢‘å›¾å·ç§¯ç½‘ç»œ` `æŠ‘éƒç—‡æ£€æµ‹` `çœ¼åŠ¨ç‰¹å¾` `é¢éƒ¨ç‰¹å¾` `å£°éŸ³ç‰¹å¾` `å¤šæ¨¡æ€èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å›¾æ¨¡åž‹ä»…å…³æ³¨ä½Žé¢‘ä¿¡æ¯ï¼Œé™åˆ¶äº†æŠ‘éƒç—‡æ£€æµ‹çš„å‡†ç¡®æ€§
2. å¼•å…¥å¤šé¢‘æ»¤æ³¢å™¨æ¨¡å—ï¼Œåˆ©ç”¨é«˜ä½Žé¢‘ä¿¡å·å¢žå¼ºç‰¹å¾æå–
3. åœ¨äºŒå…ƒå’Œä¸‰å…ƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¨¡åž‹æ•æ„Ÿæ€§å’ŒF2åˆ†æ•°æ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.

