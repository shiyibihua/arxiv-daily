---
layout: default
title: DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging
---

# DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging

**arXiv**: [2511.15151v1](https://arxiv.org/abs/2511.15151) | [PDF](https://arxiv.org/pdf/2511.15151.pdf)

**ä½œè€…**: Meihua Zhou, Xinyu Tong, Jiarui Zhao, Min Cheng, Li Yang, Lei Tian, Nan Wan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¡†æž¶DCL-SEï¼Œä»¥æå‡è„‘æˆåƒæ—¶ç©ºç¼–ç çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚**

**å…³é”®è¯**: `è„‘æˆåƒåˆ†æž` `åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ` `æ—¶ç©ºç¼–ç ` `è¿‘ä¼¼ç§©æ± åŒ–` `åŒ»å­¦å›¾åƒåˆ†ç±»` `è„‘ç–¾ç—…è¯Šæ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é«˜ç»´è„‘æˆåƒåˆ†æžé¢ä¸´æ—¶ç©ºä¿çœŸåº¦ä¸è¶³å’Œé€šç”¨æ¨¡åž‹é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚
2. ä½¿ç”¨è¿‘ä¼¼ç§©æ± åŒ–ç¼–ç ä¸‰ç»´è„‘æ•°æ®ï¼Œç»“åˆåŠ¨æ€è¯¾ç¨‹å­¦ä¹ ç­–ç•¥é€æ­¥è®­ç»ƒè§£ç å™¨ã€‚
3. åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒDCL-SEåœ¨ç²¾åº¦ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.

