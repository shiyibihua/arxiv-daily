---
layout: default
title: Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization
---

# Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization

**arXiv**: [2511.15194v1](https://arxiv.org/abs/2511.15194) | [PDF](https://arxiv.org/pdf/2511.15194.pdf)

**ä½œè€…**: Jian Deng, Yuandong Wang, Yangfu Zhu, Tao Feng, Tianyu Wo, Zhenzhou Shao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEq.Botæ¡†æž¶ä»¥å¢žå¼ºæœºå™¨äººæ“ä½œå­¦ä¹ ä¸­çš„ç©ºé—´ç­‰å˜æ€§**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œå­¦ä¹ ` `ç©ºé—´ç­‰å˜æ€§` `SE(2)ç¾¤ç†è®º` `è§„èŒƒåŒ–æ¡†æž¶` `æ¨¡åž‹æ— å…³æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æœºå™¨äººæ“ä½œæ¡†æž¶ç¼ºä¹å‡ ä½•ä¸€è‡´æ€§ä¿è¯ï¼Œéš¾ä»¥å¤„ç†æ—‹è½¬å’Œå¹³ç§»ç­‰ç©ºé—´å˜æ¢
2. åŸºäºŽSE(2)ç¾¤ç­‰å˜ç†è®ºï¼Œé€šè¿‡è§„èŒƒåŒ–ç©ºé—´è½¬æ¢å®žçŽ°æ¨¡åž‹æ— å…³çš„ç©ºé—´ç­‰å˜æ€§
3. å®žéªŒæ˜¾ç¤ºåœ¨CNNå’ŒTransformeræž¶æž„ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæœ€é«˜æå‡50.0%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.

