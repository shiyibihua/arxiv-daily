---
layout: default
title: TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition
---

# TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition

**arXiv**: [2511.15085v1](https://arxiv.org/abs/2511.15085) | [PDF](https://arxiv.org/pdf/2511.15085.pdf)

**ä½œè€…**: Wen Yin, Siyu Zhan, Cencen Liu, Xin Hu, Guiduo Duan, Xiurui Xie, Yuan-Fang Li, Tao He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTiCALæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ¨¡æ€é—´æƒ…æ„Ÿå†²çªé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `æ¨¡æ€ä¸€è‡´æ€§å­¦ä¹ ` `å…¸åž‹æ€§ä¼°è®¡` `åŒæ›²ç©ºé—´åµŒå…¥` `æƒ…æ„Ÿå†²çªç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­ï¼ŒåŒä¸€æ ·æœ¬çš„ä¸åŒæ¨¡æ€å¯èƒ½è¡¨è¾¾å†²çªæƒ…æ„Ÿï¼ŒçŽ°æœ‰æ–¹æ³•å¸¸å¿½ç•¥æ­¤é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå…¸åž‹æ€§ä¼°è®¡åŠ¨æ€è¯„ä¼°æ ·æœ¬ä¸€è‡´æ€§ï¼Œå¹¶åœ¨åŒæ›²ç©ºé—´ä¸­åµŒå…¥ç‰¹å¾ä»¥æ•æ‰æƒ…æ„Ÿç»†å¾®å·®å¼‚ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CMU-MOSEIå’ŒMER2023æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ€§èƒ½æå‡çº¦2.6%ï¼Œä¼˜äºŽçŽ°æœ‰æœ€ä½³æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.

