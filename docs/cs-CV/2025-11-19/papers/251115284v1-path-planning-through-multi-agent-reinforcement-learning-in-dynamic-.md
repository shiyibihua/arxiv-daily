---
layout: default
title: Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments
---

# Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments

**arXiv**: [2511.15284v1](https://arxiv.org/abs/2511.15284) | [PDF](https://arxiv.org/pdf/2511.15284.pdf)

**ä½œè€…**: Jonas De Maeyer, Hossein Yarahmadi, Moharram Challenger

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•è·¯å¾„è§„åˆ’æ¡†æž¶ï¼Œä»¥åº”å¯¹åŠ¨æ€çŽ¯å¢ƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚**

**å…³é”®è¯**: `è·¯å¾„è§„åˆ’` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€çŽ¯å¢ƒ` `è”é‚¦å­¦ä¹ ` `å¯æ‰©å±•æ€§` `åˆ†å±‚åˆ†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŠ¨æ€çŽ¯å¢ƒä¸­è·¯å¾„è§„åˆ’éœ€é€‚åº”å˜åŒ–éšœç¢ç‰©ï¼ŒçŽ°æœ‰æ–¹æ³•å‡è®¾ä¸çŽ°å®žä¸”å¯æ‰©å±•æ€§å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚çŽ¯å¢ƒåˆ†è§£å’Œåˆ†å¸ƒå¼RLä»£ç†ï¼Œç»“åˆå±€éƒ¨é€‚åº”å’Œè”é‚¦Qå­¦ä¹ èšåˆç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè”é‚¦å˜ä½“ä¼˜äºŽå•æ™ºèƒ½ä½“ï¼ŒæŽ¥è¿‘A* Oracleæ€§èƒ½ï¼Œé€‚åº”æ—¶é—´çŸ­ä¸”å¯æ‰©å±•æ€§å¼ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.

