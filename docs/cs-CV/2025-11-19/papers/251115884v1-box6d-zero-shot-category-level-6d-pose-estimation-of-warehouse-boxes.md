---
layout: default
title: Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes
---

# Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes

**arXiv**: [2511.15884v1](https://arxiv.org/abs/2511.15884) | [PDF](https://arxiv.org/pdf/2511.15884.pdf)

**ä½œè€…**: Yintao Ma, Sajjad Pakdamansavoji, Amir Rasouli, Tongtong Cao

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Box6Dï¼šé¢å‘ä»“åº“ç®±ä½“çš„é›¶æ ·æœ¬ç±»åˆ«çº§6Dä½å§¿ä¼°è®¡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `6Dä½å§¿ä¼°è®¡` `ç±»åˆ«çº§è¯†åˆ«` `ä»“åº“è‡ªåŠ¨åŒ–` `æœºå™¨äººæ“ä½œ` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰6Dä½å§¿ä¼°è®¡æ–¹æ³•åœ¨ä»“åº“çŽ¯å¢ƒä¸­å­˜åœ¨ä¸è¶³ï¼Œå¦‚ä¾èµ–ç²¾ç¡®CADæ¨¡åž‹ã€æ³›åŒ–æ€§å·®æˆ–å¿½ç•¥çŽ¯å¢ƒå…ˆéªŒã€‚
2. Box6Dåˆ©ç”¨ç±»åˆ«çº§CADæ¨¡æ¿å’Œå¿«é€ŸäºŒåˆ†æœç´¢è¿›è¡Œå°ºå¯¸ä¼°è®¡ï¼Œç»“åˆæ·±åº¦ä¿¡æ¯è¿‡æ»¤ä¸åˆç†å‡è®¾ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒBox6Dåœ¨çœŸå®žä»“åº“åœºæ™¯ä¸­å®žçŽ°äº†æœ‰ç«žäº‰åŠ›çš„6Dä½å§¿ç²¾åº¦ï¼Œå¹¶æ˜¾è‘—é™ä½Žäº†æŽ¨ç†æ—¶é—´ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä»“åº“è‡ªåŠ¨åŒ–ã€æ‹£è´§ã€ç‰©æµå’Œç”µå­å•†åŠ¡å±¥è¡Œä¸­ï¼Œå¯¹æ‚ä¹±å’Œé®æŒ¡ä¸‹çš„æ–°ç‰©ä½“çš„ç²¾ç¡®é«˜æ•ˆçš„6Dä½å§¿ä¼°è®¡è‡³å…³é‡è¦ã€‚è¯¥é¢†åŸŸæœ‰ä¸‰ç§ä¸»è¦æ–¹æ³•ï¼šåŸºäºŽæ¨¡åž‹çš„æ–¹æ³•å‡è®¾æŽ¨ç†æ—¶å­˜åœ¨ç²¾ç¡®çš„CADæ¨¡åž‹ï¼Œä½†éœ€è¦é«˜åˆ†è¾¨çŽ‡ç½‘æ ¼ï¼Œä¸”éš¾ä»¥è¿ç§»åˆ°æ–°çŽ¯å¢ƒï¼›æ— æ¨¡åž‹æ–¹æ³•ä¾èµ–äºŽå°‘é‡å‚è€ƒå›¾åƒæˆ–è§†é¢‘ï¼Œæ›´çµæ´»ï¼Œä½†é€šå¸¸åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å¤±è´¥ï¼›ç±»åˆ«çº§æ–¹æ³•æ—¨åœ¨å¹³è¡¡çµæ´»æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†è®¸å¤šæ–¹æ³•è¿‡äºŽé€šç”¨ï¼Œå¿½ç•¥äº†çŽ¯å¢ƒå’Œå¯¹è±¡å…ˆéªŒï¼Œé™åˆ¶äº†å…¶åœ¨å·¥ä¸šçŽ¯å¢ƒä¸­çš„å®žç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºBox6Dï¼Œä¸€ç§ä¸“ä¸ºä»“åº“çŽ¯å¢ƒä¸­çš„å­˜å‚¨ç®±è®¾è®¡çš„ç±»åˆ«çº§6Dä½å§¿ä¼°è®¡æ–¹æ³•ã€‚Box6Dä»Žå•ä¸ªRGB-Dè§‚æµ‹ä¸­ï¼Œé€šè¿‡å¿«é€ŸäºŒåˆ†æœç´¢æŽ¨æ–­ç®±ä½“çš„å°ºå¯¸ï¼Œå¹¶ä½¿ç”¨ç±»åˆ«CADæ¨¡æ¿è€Œä¸æ˜¯ç‰¹å®šå®žä¾‹æ¨¡åž‹æ¥ä¼°è®¡ä½å§¿ã€‚é€šè¿‡åŸºäºŽæ·±åº¦çš„åˆç†æ€§æ»¤æ³¢å™¨å’Œæ—©åœç­–ç•¥ï¼ŒBox6Dæ‹’ç»ä¸åˆç†çš„å‡è®¾ï¼Œä»Žè€Œé™ä½Žè®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬åœ¨çœŸå®žä¸–ç•Œçš„å­˜å‚¨åœºæ™¯å’Œå…¬å…±åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æä¾›å…·æœ‰ç«žäº‰åŠ›çš„æˆ–æ›´ä¼˜è¶Šçš„6Dä½å§¿ç²¾åº¦çš„åŒæ—¶ï¼Œå°†æŽ¨ç†æ—¶é—´å‡å°‘äº†çº¦76%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»“åº“çŽ¯å¢ƒä¸­å­˜å‚¨ç®±çš„6Dä½å§¿ä¼°è®¡é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºŽCADæ¨¡åž‹çš„æ–¹æ³•ï¼Œéœ€è¦ç²¾ç¡®çš„å®žä¾‹æ¨¡åž‹ï¼Œæ³›åŒ–æ€§å·®ï¼›æ— æ¨¡åž‹æ–¹æ³•åœ¨é®æŒ¡å’Œæ‚ä¹±çŽ¯å¢ƒä¸‹è¡¨çŽ°ä¸ä½³ï¼›ç±»åˆ«çº§æ–¹æ³•è™½ç„¶å…·æœ‰ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¾€å¾€å¿½ç•¥äº†ä»“åº“çŽ¯å¢ƒçš„å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´ç²¾åº¦å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBox6Dçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç±»åˆ«çº§çš„CADæ¨¡æ¿ï¼Œç»“åˆRGB-Dæ•°æ®ï¼Œé€šè¿‡å¿«é€Ÿå°ºå¯¸ä¼°è®¡å’Œå‡è®¾éªŒè¯ï¼Œå®žçŽ°å¯¹ä»“åº“ç®±ä½“çš„ç²¾ç¡®6Dä½å§¿ä¼°è®¡ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹æ¯ä¸ªç®±ä½“è¿›è¡Œå•ç‹¬å»ºæ¨¡çš„éœ€æ±‚ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åˆ©ç”¨çŽ¯å¢ƒå…ˆéªŒçŸ¥è¯†æé«˜äº†ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šBox6Dçš„æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) è¾“å…¥RGB-Då›¾åƒï¼›2) ä½¿ç”¨æ·±åº¦ä¿¡æ¯è¿›è¡Œç®±ä½“å°ºå¯¸çš„å¿«é€ŸäºŒåˆ†æœç´¢ï¼›3) åˆ©ç”¨ç±»åˆ«CADæ¨¡æ¿ç”Ÿæˆå¤šä¸ªä½å§¿å‡è®¾ï¼›4) ä½¿ç”¨æ·±åº¦ä¿¡æ¯è¿›è¡Œåˆç†æ€§è¿‡æ»¤ï¼ŒæŽ’é™¤ä¸åˆç†çš„å‡è®¾ï¼›5) é€šè¿‡æ—©åœç­–ç•¥ï¼Œè¿›ä¸€æ­¥é™ä½Žè®¡ç®—æˆæœ¬ï¼›6) è¾“å‡ºæœ€ç»ˆçš„6Dä½å§¿ä¼°è®¡ç»“æžœã€‚

**å…³é”®åˆ›æ–°**ï¼šBox6Dçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†é’ˆå¯¹ä»“åº“ç®±ä½“çš„ç±»åˆ«çº§6Dä½å§¿ä¼°è®¡æ–¹æ³•ï¼Œé¿å…äº†å¯¹æ¯ä¸ªç®±ä½“è¿›è¡Œå•ç‹¬å»ºæ¨¡ï¼›2) åˆ©ç”¨å¿«é€ŸäºŒåˆ†æœç´¢è¿›è¡Œå°ºå¯¸ä¼°è®¡ï¼Œæé«˜äº†æ•ˆçŽ‡ï¼›3) ç»“åˆæ·±åº¦ä¿¡æ¯è¿›è¡Œå‡è®¾éªŒè¯ï¼Œæé«˜äº†ç²¾åº¦å’Œé²æ£’æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒBox6Dæ›´æ³¨é‡åˆ©ç”¨çŽ¯å¢ƒå…ˆéªŒçŸ¥è¯†ï¼Œä»Žè€Œåœ¨ç²¾åº¦å’Œæ•ˆçŽ‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šBox6Dçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¿«é€ŸäºŒåˆ†æœç´¢çš„å®žçŽ°ç»†èŠ‚ï¼Œä¾‹å¦‚æœç´¢èŒƒå›´å’Œæ­¥é•¿ï¼›2) åŸºäºŽæ·±åº¦çš„åˆç†æ€§è¿‡æ»¤å™¨çš„å…·ä½“å®žçŽ°ï¼Œä¾‹å¦‚ä½¿ç”¨çš„æ·±åº¦é˜ˆå€¼å’Œè·ç¦»åº¦é‡ï¼›3) æ—©åœç­–ç•¥çš„å®žçŽ°ï¼Œä¾‹å¦‚åœæ­¢æ¡ä»¶å’Œè¯„ä¼°æŒ‡æ ‡ã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºŽä¼˜åŒ–ä½å§¿ä¼°è®¡ç»“æžœï¼ˆå…·ä½“ç»†èŠ‚æœªçŸ¥ï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

Box6Dåœ¨çœŸå®žä¸–ç•Œçš„å­˜å‚¨åœºæ™¯å’Œå…¬å…±åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æä¾›å…·æœ‰ç«žäº‰åŠ›çš„æˆ–æ›´ä¼˜è¶Šçš„6Dä½å§¿ç²¾åº¦çš„åŒæ—¶ï¼Œå°†æŽ¨ç†æ—¶é—´å‡å°‘äº†çº¦76%ã€‚è¿™æ„å‘³ç€Box6Dåœ¨å®žé™…åº”ç”¨ä¸­å…·æœ‰æ›´é«˜çš„æ•ˆçŽ‡å’Œå®žç”¨æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œä½å§¿ä¼°è®¡çš„ç²¾åº¦æŒ‡æ ‡ï¼‰å’Œå¯¹æ¯”åŸºçº¿ï¼ˆä¾‹å¦‚ï¼Œå…¶ä»–ç±»åˆ«çº§6Dä½å§¿ä¼°è®¡æ–¹æ³•ï¼‰çš„è¯¦ç»†ä¿¡æ¯æœªçŸ¥ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

Box6Då¯åº”ç”¨äºŽä»“åº“è‡ªåŠ¨åŒ–ã€æ™ºèƒ½ç‰©æµã€æœºå™¨äººæ‹£é€‰ç­‰é¢†åŸŸã€‚é€šè¿‡ç²¾ç¡®ä¼°è®¡ç®±ä½“çš„6Dä½å§¿ï¼Œæœºå™¨äººå¯ä»¥æ›´å‡†ç¡®åœ°æŠ“å–å’Œæ”¾ç½®ç®±ä½“ï¼Œæé«˜ä»“åº“è¿è¥æ•ˆçŽ‡ï¼Œé™ä½Žäººå·¥æˆæœ¬ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰æ½œåŠ›æ‰©å±•åˆ°å…¶ä»–ç»“æž„åŒ–çŽ¯å¢ƒä¸­çš„ç‰©ä½“è¯†åˆ«å’Œå®šä½ï¼Œä¾‹å¦‚ç”Ÿäº§çº¿ä¸Šçš„é›¶ä»¶è¯†åˆ«ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
>   To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.

