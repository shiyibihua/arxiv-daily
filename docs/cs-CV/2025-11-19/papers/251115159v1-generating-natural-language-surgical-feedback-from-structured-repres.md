---
layout: default
title: Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation
---

# Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation

**arXiv**: [2511.15159v1](https://arxiv.org/abs/2511.15159) | [PDF](https://arxiv.org/pdf/2511.15159.pdf)

**ä½œè€…**: Firdavs Nasriddinov, Rafal Kocielnik, Anima Anandkumar, Andrew J. Hung

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç»“æž„åŒ–æ‰‹æœ¯åŠ¨ä½œä¸‰å…ƒç»„çš„åé¦ˆç”Ÿæˆæ–¹æ³•ï¼Œä»¥æå‡æ‰‹æœ¯è®­ç»ƒä¸­çš„è‡ªåŠ¨åŒ–æŒ‡å¯¼è´¨é‡ã€‚**

**å…³é”®è¯**: `æ‰‹æœ¯åé¦ˆç”Ÿæˆ` `ç»“æž„åŒ–è¡¨ç¤ºå­¦ä¹ ` `è§†é¢‘åŠ¨ä½œè¯†åˆ«` `è‡ªç„¶è¯­è¨€ç”Ÿæˆ` `ä¸´åºŠè¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆè‡ªç„¶è¯­è¨€æ‰‹æœ¯åé¦ˆéœ€ç†è§£ä¸´åºŠç›¸å…³è¡¨ç¤ºï¼Œä»¥æä¾›åŠæ—¶ä¸€è‡´çš„æŒ‡å¯¼ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä»ŽçœŸå®žåé¦ˆæ–‡æœ¬æŒ–æŽ˜å™¨æ¢°-åŠ¨ä½œ-ç›®æ ‡ä¸‰å…ƒç»„ï¼Œå¹¶ç”¨äºŽæ¡ä»¶åŒ–GPT-4oç”Ÿæˆåé¦ˆã€‚
3. å®žéªŒæ•ˆæžœï¼šä¸‰å…ƒç»„æ¡ä»¶åŒ–ä½¿åé¦ˆç”Ÿæˆä¿çœŸåº¦æå‡12.4%ï¼Œå¯æŽ¥å—ç”Ÿæˆæ¯”ä¾‹ä»Ž21%å¢žè‡³42%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

