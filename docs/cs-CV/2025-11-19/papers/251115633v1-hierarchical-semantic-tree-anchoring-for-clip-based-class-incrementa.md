---
layout: default
title: Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning
---

# Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning

**arXiv**: [2511.15633v1](https://arxiv.org/abs/2511.15633) | [PDF](https://arxiv.org/pdf/2511.15633.pdf)

**ä½œè€…**: Tao Hu, Lan Li, Zhen-Hao Xie, Da-Wei Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHASTENæ–¹æ³•ï¼Œé€šè¿‡å±‚æ¬¡è¯­ä¹‰æ ‘é”šå®šè§£å†³CLIPç±»å¢žé‡å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç±»å¢žé‡å­¦ä¹ ` `CLIPæ¨¡åž‹` `å±‚æ¬¡è¯­ä¹‰æ ‘` `åŒæ›²ç©ºé—´åµŒå…¥` `ç¾éš¾æ€§é—å¿˜ç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPç±»å¢žé‡å­¦ä¹ æœªæ˜¾å¼æ•èŽ·å±‚æ¬¡ç»“æž„ï¼Œå¯¼è‡´ç»†ç²’åº¦ç±»ç‰¹å¾æ¼‚ç§»å’Œç¾éš¾æ€§é—å¿˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾åœ¨åŒæ›²ç©ºé—´åµŒå…¥ç‰¹å¾ï¼Œå¹¶æŠ•å½±æ¢¯åº¦åˆ°å…±äº«æ˜ å°„å™¨é›¶ç©ºé—´ä»¥å‡è½»é—å¿˜ã€‚
3. å®žéªŒæ•ˆæžœï¼šå¹¿æ³›å®žéªŒæ˜¾ç¤ºHASTENä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæä¾›ç»Ÿä¸€ç»“æž„åŒ–è¡¨ç¤ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.

