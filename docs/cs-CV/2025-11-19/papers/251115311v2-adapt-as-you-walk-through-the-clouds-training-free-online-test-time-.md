---
layout: default
title: Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models
---

# Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models

**arXiv**: [2511.15311v2](https://arxiv.org/abs/2511.15311) | [PDF](https://arxiv.org/pdf/2511.15311.pdf)

**ä½œè€…**: Mehran Tamjidi, Hamidreza Dastmalchi, Mohammadreza Alimoradijazi, Ali Cheraghian, Aijun An, Morteza Saberi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19 (æ›´æ–°: 2025-11-20)

**å¤‡æ³¨**: Accepted by AAAI 2026

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://mehran-tam.github.io/Uni-Adapter)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni-Adapterï¼Œä¸€ç§å…è®­ç»ƒçš„3Dè§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dè§†è§‰` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `æµ‹è¯•æ—¶è‡ªé€‚åº”` `ç‚¹äº‘å¤„ç†` `åŠ¨æ€åŽŸåž‹å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰3Dè§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œé¢å¯¹å™ªå£°ã€ä¸å®Œæ•´æˆ–åˆ†å¸ƒåç§»çš„æ•°æ®æ—¶æ€§èƒ½ä¸‹é™ã€‚
2. Uni-Adapteré€šè¿‡åŠ¨æ€åŽŸåž‹å­¦ä¹ ï¼Œæž„å»ºå¹¶æ›´æ–°ç±»ç‰¹å®šåŽŸåž‹ï¼Œä»¥é€‚åº”å¼‚æž„æ•°æ®åˆ†å¸ƒã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒUni-Adapteråœ¨å¤šä¸ª3Dæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡åž‹çš„é²æ£’æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡åž‹(VLFMs)åœ¨å¼€æ”¾ä¸–ç•Œç‚¹äº‘å¤„ç†ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºå¼ºå¤§çš„æ³›åŒ–å’Œé›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨æ•°æ®å˜ˆæ‚ã€ä¸å®Œæ•´æˆ–æ¥è‡ªä¸Žè®­ç»ƒæ•°æ®ä¸åŒåˆ†å¸ƒçš„å®žé™…åœºæ™¯ä¸­ï¼Œè¿™äº›æ¨¡åž‹çš„æ€§èƒ½é€šå¸¸ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºUni-Adapterï¼Œä¸€ç§åŸºäºŽåŠ¨æ€åŽŸåž‹å­¦ä¹ çš„3D VLFMsæ–°åž‹å…è®­ç»ƒåœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”(TTA)ç­–ç•¥ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª3Dç¼“å­˜æ¥å­˜å‚¨ç±»ç‰¹å®šçš„èšç±»ä¸­å¿ƒä½œä¸ºåŽŸåž‹ï¼Œè¿™äº›åŽŸåž‹ä¸æ–­æ›´æ–°ä»¥æ•èŽ·å¼‚æž„æ•°æ®åˆ†å¸ƒä¸­çš„ç±»å†…å˜å¼‚æ€§ã€‚è¿™äº›åŠ¨æ€åŽŸåž‹ä½œä¸ºé€šè¿‡ç›¸ä¼¼æ€§è¯„åˆ†è¿›è¡ŒåŸºäºŽç¼“å­˜çš„logitè®¡ç®—çš„é”šç‚¹ã€‚åŒæ—¶ï¼ŒåŸºäºŽå›¾çš„æ ‡ç­¾å¹³æ»‘æ¨¡å—æ•èŽ·åŽŸåž‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»¥å¢žå¼ºç›¸ä¼¼åŽŸåž‹ä¹‹é—´çš„æ ‡ç­¾ä¸€è‡´æ€§ã€‚æœ€åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ç†µåŠ æƒèšåˆæ¥ç»Ÿä¸€æ¥è‡ªåŽŸå§‹3D VLFMå’Œç²¾ç‚¼çš„3Dç¼“å­˜çš„é¢„æµ‹ï¼Œä»¥å®žçŽ°å¯é çš„è‡ªé€‚åº”ã€‚æ— éœ€é‡æ–°è®­ç»ƒï¼ŒUni-Adapteræœ‰æ•ˆåœ°ç¼“è§£äº†åˆ†å¸ƒåç§»ï¼Œåœ¨ä¸åŒçš„3DåŸºå‡†æµ‹è¯•ä¸­ï¼Œé’ˆå¯¹ä¸åŒçš„3D VLFMså®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ModelNet-40Cä¸Šæé«˜äº†10.55%ï¼Œåœ¨ScanObjectNN-Cä¸Šæé«˜äº†8.26%ï¼Œåœ¨ShapeNet-Cä¸Šæé«˜äº†4.49%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³3Dè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡åž‹åœ¨æµ‹è¯•æ—¶é‡åˆ°åˆ†å¸ƒåç§»é—®é¢˜ï¼Œå³æ¨¡åž‹åœ¨è®­ç»ƒæ•°æ®å’Œå®žé™…åº”ç”¨æ•°æ®ä¹‹é—´å­˜åœ¨å·®å¼‚æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡åž‹æˆ–è¿›è¡Œå¾®è°ƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ•ˆçŽ‡ä½Žä¸‹ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä½¿æ¨¡åž‹é€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni-Adapterçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŠ¨æ€åŽŸåž‹å­¦ä¹ ï¼Œæž„å»ºä¸€ä¸ªèƒ½å¤Ÿæ•èŽ·ç±»å†…å˜å¼‚æ€§çš„3Dç¼“å­˜ã€‚è¯¥ç¼“å­˜å­˜å‚¨ç±»ç‰¹å®šçš„èšç±»ä¸­å¿ƒä½œä¸ºåŽŸåž‹ï¼Œå¹¶éšç€æ–°æ•°æ®çš„è¾“å…¥ä¸æ–­æ›´æ–°è¿™äº›åŽŸåž‹ã€‚é€šè¿‡å°†è¾“å…¥æ•°æ®ä¸Žç¼“å­˜ä¸­çš„åŽŸåž‹è¿›è¡Œæ¯”è¾ƒï¼Œå¯ä»¥å®žçŽ°å¯¹æ¨¡åž‹é¢„æµ‹ç»“æžœçš„ä¿®æ­£ï¼Œä»Žè€Œé€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæ¨¡åž‹ï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šUni-Adapterçš„æ•´ä½“æ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) 3Dç¼“å­˜ï¼šç”¨äºŽå­˜å‚¨å’Œæ›´æ–°ç±»ç‰¹å®šçš„åŽŸåž‹ã€‚2) åŸºäºŽç¼“å­˜çš„Logitè®¡ç®—ï¼šé€šè¿‡è®¡ç®—è¾“å…¥æ•°æ®ä¸ŽåŽŸåž‹ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œç”Ÿæˆæ–°çš„logitã€‚3) åŸºäºŽå›¾çš„æ ‡ç­¾å¹³æ»‘ï¼šåˆ©ç”¨åŽŸåž‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¢žå¼ºæ ‡ç­¾ä¸€è‡´æ€§ã€‚æœ€åŽï¼Œé€šè¿‡ç†µåŠ æƒèšåˆï¼Œå°†åŽŸå§‹æ¨¡åž‹çš„é¢„æµ‹ç»“æžœå’Œç¼“å­˜çš„é¢„æµ‹ç»“æžœè¿›è¡Œèžåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æžœã€‚

**å…³é”®åˆ›æ–°**ï¼šUni-Adapterçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å…è®­ç»ƒçš„åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”ç­–ç•¥ã€‚ä¸Žä¼ ç»Ÿçš„éœ€è¦é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒçš„æ–¹æ³•ä¸åŒï¼ŒUni-Adapterå¯ä»¥åœ¨æµ‹è¯•æ—¶åŠ¨æ€åœ°é€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œæ— éœ€ä»»ä½•è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€åŽŸåž‹å­¦ä¹ å’ŒåŸºäºŽå›¾çš„æ ‡ç­¾å¹³æ»‘æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•èŽ·ç±»å†…å˜å¼‚æ€§å’Œç±»é—´å…³ç³»ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼š3Dç¼“å­˜ä½¿ç”¨K-meansèšç±»ç®—æ³•æ¥åˆå§‹åŒ–å’Œæ›´æ–°åŽŸåž‹ã€‚ç›¸ä¼¼æ€§è¯„åˆ†é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚åŸºäºŽå›¾çš„æ ‡ç­¾å¹³æ»‘æ¨¡å—ä½¿ç”¨KNNå›¾æ¥æž„å»ºåŽŸåž‹ä¹‹é—´çš„å…³ç³»ã€‚ç†µåŠ æƒèšåˆä½¿ç”¨é¢„æµ‹ç»“æžœçš„ç†µå€¼æ¥ç¡®å®šåŽŸå§‹æ¨¡åž‹å’Œç¼“å­˜é¢„æµ‹ç»“æžœçš„æƒé‡ã€‚å…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚K-meansçš„ç°‡æ•°ã€KNNå›¾çš„é‚»å±…æ•°ï¼‰éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

Uni-Adapteråœ¨ModelNet-40Cã€ScanObjectNN-Cå’ŒShapeNet-Cç­‰å¤šä¸ª3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ModelNet-40Cä¸Šï¼ŒUni-Adapterçš„æ€§èƒ½æå‡äº†10.55%ï¼›åœ¨ScanObjectNN-Cä¸Šï¼Œæ€§èƒ½æå‡äº†8.26%ï¼›åœ¨ShapeNet-Cä¸Šï¼Œæ€§èƒ½æå‡äº†4.49%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒUni-Adapterèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£åˆ†å¸ƒåç§»é—®é¢˜ï¼Œæé«˜æ¨¡åž‹çš„é²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

Uni-Adapterå¯åº”ç”¨äºŽå„ç§éœ€è¦å¤„ç†3Dç‚¹äº‘æ•°æ®çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å®¤å†…åœºæ™¯ç†è§£ã€ä¸‰ç»´é‡å»ºç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡åž‹åœ¨å®žé™…åº”ç”¨ä¸­çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è´¨é‡è¾ƒå·®æˆ–æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–çš„æƒ…å†µä¸‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€çš„æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬ï¼Œä»Žè€Œå®žçŽ°æ›´é€šç”¨çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs. Project page: https://mehran-tam.github.io/Uni-Adapter

