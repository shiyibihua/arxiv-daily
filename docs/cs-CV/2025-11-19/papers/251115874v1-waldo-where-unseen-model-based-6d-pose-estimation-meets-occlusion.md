---
layout: default
title: WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion
---

# WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion

**arXiv**: [2511.15874v1](https://arxiv.org/abs/2511.15874) | [PDF](https://arxiv.org/pdf/2511.15874.pdf)

**ä½œè€…**: Sajjad Pakdamansavoji, Yintao Ma, Amir Rasouli, Tongtong Cao

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WALDOï¼šæå‡ºä¸€ç§æ–°é¢–çš„åŸºäºŽæ¨¡åž‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•ï¼Œæå‡é®æŒ¡åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `6Dä½å§¿ä¼°è®¡` `é®æŒ¡å¤„ç†` `æ¨¡åž‹é©±åŠ¨` `æœºå™¨äºº` `å¢žå¼ºçŽ°å®ž` `åŠ¨æ€é‡‡æ ·` `å¤šå‡è®¾æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽæ¨¡åž‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•åœ¨é®æŒ¡åœºæ™¯ä¸‹ï¼Œç”±äºŽæ—©æœŸé˜¶æ®µçš„æ£€æµ‹å’Œåˆ†å‰²è¯¯å·®ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºWALDOï¼Œé€šè¿‡åŠ¨æ€é‡‡æ ·ã€å¤šå‡è®¾æŽ¨ç†ã€è¿­ä»£ç»†åŒ–å’Œé®æŒ¡å¢žå¼ºç­‰ç­–ç•¥ï¼Œæå‡é®æŒ¡åœºæ™¯ä¸‹çš„ä½å§¿ä¼°è®¡é²æ£’æ€§ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒWALDOåœ¨ICBINå’ŒBOPæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„ç²¾åº¦æå‡ï¼Œå¹¶æé«˜äº†æŽ¨ç†é€Ÿåº¦ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç²¾ç¡®çš„6Dç‰©ä½“ä½å§¿ä¼°è®¡å¯¹äºŽæœºå™¨äººã€å¢žå¼ºçŽ°å®žå’Œåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚å¯¹äºŽå·²è§è¿‡çš„ç‰©ä½“ï¼Œé€šè¿‡é€å¯¹è±¡å¾®è°ƒé€šå¸¸å¯ä»¥å®žçŽ°é«˜ç²¾åº¦ï¼Œä½†æ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿‡åŽ»çš„æ–¹æ³•é€šå¸¸å‡è®¾åœ¨æµ‹è¯•æ—¶å¯ä»¥è®¿é—®CADæ¨¡åž‹ï¼Œå¹¶ä¸”é€šå¸¸éµå¾ªä¸€ä¸ªå¤šé˜¶æ®µæµç¨‹æ¥ä¼°è®¡ä½å§¿ï¼šæ£€æµ‹å’Œåˆ†å‰²ç‰©ä½“ï¼Œæå‡ºåˆå§‹ä½å§¿ï¼Œç„¶åŽå¯¹å…¶è¿›è¡Œç»†åŒ–ã€‚ç„¶è€Œï¼Œåœ¨é®æŒ¡ä¸‹ï¼Œè¿™ç§æµç¨‹çš„æ—©æœŸé˜¶æ®µå®¹æ˜“å‡ºé”™ï¼Œè¿™äº›é”™è¯¯ä¼šé€šè¿‡é¡ºåºå¤„ç†ä¼ æ’­ï¼Œä»Žè€Œé™ä½Žæ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸ªç¼ºç‚¹ï¼Œæˆ‘ä»¬å¯¹åŸºäºŽæ¨¡åž‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•æå‡ºäº†å››ä¸ªæ–°çš„æ‰©å±•ï¼šï¼ˆiï¼‰ä¸€ç§åŠ¨æ€éžå‡åŒ€å¯†é›†é‡‡æ ·ç­–ç•¥ï¼Œå°†è®¡ç®—é›†ä¸­åœ¨å¯è§åŒºåŸŸï¼Œå‡å°‘é®æŒ¡å¼•èµ·çš„è¯¯å·®ï¼›ï¼ˆiiï¼‰ä¸€ç§å¤šå‡è®¾æŽ¨ç†æœºåˆ¶ï¼Œä¿ç•™å‡ ä¸ªç½®ä¿¡åº¦æŽ’åºçš„ä½å§¿å€™é€‰ï¼Œå‡è½»è„†å¼±çš„å•è·¯å¾„å¤±è´¥ï¼›ï¼ˆiiiï¼‰è¿­ä»£ç»†åŒ–ï¼Œä»¥é€æ­¥æé«˜ä½å§¿ç²¾åº¦ï¼›ï¼ˆivï¼‰ä¸€ç³»åˆ—ä»¥é®æŒ¡ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®å¢žå¼ºï¼Œä»¥å¢žå¼ºé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŒ‰å¯è§æ€§åŠ æƒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºŽåœ¨é®æŒ¡ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘çŽ°æœ‰åè®®ä¸­çš„åå·®ã€‚é€šè¿‡å¹¿æ³›çš„å®žè¯è¯„ä¼°ï¼Œæˆ‘ä»¬è¡¨æ˜Žæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨ICBINä¸Šå®žçŽ°äº†è¶…è¿‡5%çš„ç²¾åº¦æå‡ï¼Œåœ¨BOPæ•°æ®é›†åŸºå‡†ä¸Šå®žçŽ°äº†è¶…è¿‡2%çš„ç²¾åº¦æå‡ï¼ŒåŒæ—¶å®žçŽ°äº†å¤§çº¦3å€çš„æŽ¨ç†é€Ÿåº¦ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å­˜åœ¨é®æŒ¡çš„æƒ…å†µä¸‹ï¼ŒåŸºäºŽæ¨¡åž‹çš„6Dä½å§¿ä¼°è®¡ç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽç²¾ç¡®çš„ç‰©ä½“æ£€æµ‹å’Œåˆ†å‰²ï¼Œä½†åœ¨é®æŒ¡çŽ¯å¢ƒä¸‹ï¼Œè¿™äº›æ­¥éª¤å®¹æ˜“å‡ºé”™ï¼Œå¯¼è‡´åŽç»­çš„ä½å§¿ä¼°è®¡æ€§èƒ½å—åˆ°ä¸¥é‡å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‡å°‘å¯¹å®Œæ•´ç‰©ä½“ä¿¡æ¯çš„ä¾èµ–ï¼Œæ›´åŠ å…³æ³¨å¯è§åŒºåŸŸçš„ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å¤šå‡è®¾æŽ¨ç†æ¥åº”å¯¹ä¸ç¡®å®šæ€§ã€‚é€šè¿‡åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œå°†è®¡ç®—èµ„æºé›†ä¸­åœ¨å¯è§åŒºåŸŸï¼Œå‡å°‘é®æŒ¡å¸¦æ¥çš„å¹²æ‰°ã€‚å¤šå‡è®¾æŽ¨ç†åˆ™ä¿ç•™å¤šä¸ªå¯èƒ½çš„ä½å§¿ä¼°è®¡ï¼Œé¿å…å› æ—©æœŸé”™è¯¯è€Œå¯¼è‡´çš„å•è·¯å¾„å¤±è´¥ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šWALDOçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŠ¨æ€éžå‡åŒ€å¯†é›†é‡‡æ ·ï¼šæ ¹æ®å¯è§æ€§å¯¹ç‰©ä½“è¡¨é¢è¿›è¡Œé‡‡æ ·ï¼Œé›†ä¸­è®¡ç®—èµ„æºäºŽå¯è§åŒºåŸŸã€‚2) å¤šå‡è®¾æŽ¨ç†ï¼šç”Ÿæˆå¤šä¸ªå€™é€‰ä½å§¿ï¼Œå¹¶æ ¹æ®ç½®ä¿¡åº¦è¿›è¡ŒæŽ’åºã€‚3) è¿­ä»£ç»†åŒ–ï¼šé€æ­¥ä¼˜åŒ–å€™é€‰ä½å§¿ï¼Œæé«˜ç²¾åº¦ã€‚4) é®æŒ¡å¢žå¼ºï¼šé€šè¿‡æ¨¡æ‹Ÿå„ç§é®æŒ¡æƒ…å†µæ¥å¢žå¼ºæ¨¡åž‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽåŠ¨æ€éžå‡åŒ€å¯†é›†é‡‡æ ·ç­–ç•¥å’Œå¤šå‡è®¾æŽ¨ç†æœºåˆ¶ã€‚åŠ¨æ€é‡‡æ ·èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘é®æŒ¡å¸¦æ¥çš„å¹²æ‰°ï¼Œæé«˜ä½å§¿ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å¤šå‡è®¾æŽ¨ç†åˆ™èƒ½å¤Ÿåº”å¯¹ä¸ç¡®å®šæ€§ï¼Œé¿å…å› æ—©æœŸé”™è¯¯è€Œå¯¼è‡´çš„å•è·¯å¾„å¤±è´¥ã€‚æ­¤å¤–ï¼Œæå‡ºçš„é®æŒ¡æ„ŸçŸ¥è¯„ä¼°æŒ‡æ ‡ä¹Ÿæ›´åŠ åˆç†åœ°è¯„ä¼°äº†ç®—æ³•åœ¨é®æŒ¡çŽ¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€é‡‡æ ·ç­–ç•¥æ ¹æ®ç‰©ä½“çš„å¯è§æ€§æ¦‚çŽ‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œå¯è§æ€§æ¦‚çŽ‡å¯ä»¥é€šè¿‡æ¸²æŸ“æˆ–å…¶ä»–æ–¹æ³•ä¼°è®¡ã€‚å¤šå‡è®¾æŽ¨ç†é‡‡ç”¨ç½®ä¿¡åº¦æŽ’åºï¼Œç½®ä¿¡åº¦å¯ä»¥åŸºäºŽæ¸²æŸ“ç»“æžœä¸Žè§‚æµ‹å›¾åƒçš„ç›¸ä¼¼åº¦æ¥è®¡ç®—ã€‚è¿­ä»£ç»†åŒ–é‡‡ç”¨ICPæˆ–å…¶ä»–ä¼˜åŒ–ç®—æ³•æ¥é€æ­¥æé«˜ä½å§¿ç²¾åº¦ã€‚é®æŒ¡å¢žå¼ºåˆ™é€šè¿‡åœ¨è®­ç»ƒå›¾åƒä¸­éšæœºæ·»åŠ é®æŒ¡ç‰©æ¥å®žçŽ°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒWALDOåœ¨ICBINæ•°æ®é›†ä¸Šå®žçŽ°äº†è¶…è¿‡5%çš„ç²¾åº¦æå‡ï¼Œåœ¨BOPæ•°æ®é›†ä¸Šå®žçŽ°äº†è¶…è¿‡2%çš„ç²¾åº¦æå‡ï¼ŒåŒæ—¶æŽ¨ç†é€Ÿåº¦æé«˜äº†çº¦3å€ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒWALDOåœ¨é®æŒ¡åœºæ™¯ä¸‹å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œæ›´é«˜çš„æ•ˆçŽ‡ï¼Œä¼˜äºŽçŽ°æœ‰çš„åŸºäºŽæ¨¡åž‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽæœºå™¨äººæ“ä½œã€å¢žå¼ºçŽ°å®žã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œå‡†ç¡®çš„6Dä½å§¿ä¼°è®¡æ˜¯å®žçŽ°ç‰©ä½“æŠ“å–å’Œæ“ä½œçš„åŸºç¡€ã€‚åœ¨å¢žå¼ºçŽ°å®žä¸­ï¼Œå¯ä»¥ç”¨äºŽå°†è™šæ‹Ÿç‰©ä½“ç²¾ç¡®åœ°å åŠ åˆ°çœŸå®žåœºæ™¯ä¸­ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºŽæ„ŸçŸ¥å‘¨å›´çŽ¯å¢ƒä¸­çš„ç‰©ä½“ï¼Œå¹¶è¿›è¡Œç²¾ç¡®çš„å®šä½å’Œè·Ÿè¸ªã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.

