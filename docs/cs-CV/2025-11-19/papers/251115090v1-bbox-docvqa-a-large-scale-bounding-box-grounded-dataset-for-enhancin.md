---
layout: default
title: BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer
---

# BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer

**arXiv**: [2511.15090v1](https://arxiv.org/abs/2511.15090) | [PDF](https://arxiv.org/pdf/2511.15090.pdf)

**ä½œè€…**: Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang

**åˆ†ç±»**: cs.DB, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

**å¤‡æ³¨**: 22 pages, 4 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBBox DocVQAæ•°æ®é›†ï¼Œå¢žå¼ºæ–‡æ¡£è§†è§‰é—®ç­”ä¸­ç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ–‡æ¡£è§†è§‰é—®ç­”` `è§†è§‰è¯­è¨€æŽ¨ç†` `ç©ºé—´æŽ¨ç†` `è¾¹ç•Œæ¡†æ ‡æ³¨` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰DocVQAæ•°æ®é›†ç¼ºä¹ç»†ç²’åº¦çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯è§£é‡Šæ€§å’Œç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚
2. æå‡ºBBox DocVQAæ•°æ®é›†ï¼Œé€šè¿‡æ˜¾å¼è¾¹ç•Œæ¡†æ ‡æ³¨QAå¯¹ï¼Œå¢žå¼ºæ¨¡åž‹åœ¨è§†è§‰æ–‡æ¡£ä¸­çš„ç©ºé—´æŽ¨ç†å’Œè¯æ®å®šä½èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œåœ¨BBox DocVQAä¸Šå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£è§†è§‰é—®ç­”(DocVQA)æ˜¯å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„åŸºç¡€ä»»åŠ¡ï¼Œä¹Ÿæ˜¯è§†è§‰è¯­è¨€æŽ¨ç†çš„å…³é”®æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰DocVQAæ•°æ®é›†å¤§å¤šå±€é™äºŽé¡µé¢çº§åˆ«ï¼Œç¼ºä¹ç»†ç²’åº¦çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡åž‹(VLMs)çš„å¯è§£é‡Šæ€§å’ŒæŽ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æŽ¨å‡ºäº†BBox DocVQAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ã€åŸºäºŽè¾¹ç•Œæ¡†çš„æ•°æ®é›†ï¼Œæ—¨åœ¨å¢žå¼ºè§†è§‰æ–‡æ¡£ä¸­çš„ç©ºé—´æŽ¨ç†å’Œè¯æ®å®šä½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨æž„å»ºæµç¨‹ï¼Œå³â€œåˆ†å‰²ã€åˆ¤æ–­å’Œç”Ÿæˆâ€ï¼Œè¯¥æµç¨‹é›†æˆäº†ç”¨äºŽåŒºåŸŸåˆ†å‰²çš„åˆ†å‰²æ¨¡åž‹ã€ç”¨äºŽè¯­ä¹‰åˆ¤æ–­çš„VLMå’Œç”¨äºŽé—®é¢˜ç­”æ¡ˆç”Ÿæˆçš„å¦ä¸€ä¸ªé«˜çº§VLMï¼Œç„¶åŽè¿›è¡Œäººå·¥éªŒè¯ä»¥ç¡®ä¿è´¨é‡ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«3.6Kä¸ªä¸åŒçš„æ–‡æ¡£å’Œ32Kä¸ªQAå¯¹ï¼Œæ¶µç›–å•åŒºåŸŸå’Œå¤šåŒºåŸŸä»¥åŠå•é¡µå’Œå¤šé¡µåœºæ™¯ã€‚æ¯ä¸ªQAå®žä¾‹éƒ½åŸºäºŽæ˜¾å¼çš„è¾¹ç•Œæ¡†ï¼Œä»Žè€Œèƒ½å¤Ÿå¯¹ç©ºé—´è¯­ä¹‰å¯¹é½è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚åœ¨BBox DocVQAä¸Šå¯¹å¤šä¸ªæœ€å…ˆè¿›çš„VLMï¼ˆä¾‹å¦‚ï¼ŒGPT 5ã€Qwen2.5 VLå’ŒInternVLï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ç©ºé—´å®šä½å’ŒæŽ¨ç†å‡†ç¡®æ€§æ–¹é¢æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåœ¨BBox DocVQAä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆï¼Œä»Žè€ŒéªŒè¯äº†å…¶å¢žå¼ºVLMæŽ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æŽ¨è¿›å¯è§£é‡Šå’Œç©ºé—´å®šä½çš„è§†è§‰è¯­è¨€æŽ¨ç†ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰DocVQAæ•°æ®é›†ä¸»è¦å…³æ³¨é¡µé¢çº§åˆ«çš„é—®ç­”ï¼Œç¼ºä¹ç»†ç²’åº¦çš„ç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡åž‹éš¾ä»¥è¿›è¡Œç²¾ç¡®çš„ç©ºé—´æŽ¨ç†å’Œå®šä½ã€‚è¿™é™åˆ¶äº†æ¨¡åž‹åœ¨å¤æ‚æ–‡æ¡£åœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œä¾‹å¦‚éœ€è¦å®šä½ç‰¹å®šè¡¨æ ¼å•å…ƒæ ¼æˆ–å›¾åƒåŒºåŸŸæ‰èƒ½å›žç­”é—®é¢˜çš„æƒ…å†µã€‚çŽ°æœ‰æ–¹æ³•éš¾ä»¥æä¾›å¯è§£é‡Šçš„æŽ¨ç†è¿‡ç¨‹ï¼Œæ— æ³•æ˜Žç¡®æŒ‡å‡ºç­”æ¡ˆçš„ä¾æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBBox DocVQAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œå°†QAå¯¹ä¸Žæ–‡æ¡£ä¸­çš„ç‰¹å®šåŒºåŸŸè¿›è¡Œå…³è”ï¼Œä»Žè€Œæ˜¾å¼åœ°æä¾›ç©ºé—´ä¿¡æ¯ã€‚è¿™ç§æ–¹å¼èƒ½å¤Ÿè¿«ä½¿æ¨¡åž‹å­¦ä¹ ç©ºé—´è¯­ä¹‰å¯¹é½ï¼Œå¹¶æé«˜æ¨¡åž‹åœ¨ç©ºé—´æŽ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç»†ç²’åº¦çš„æ ‡æ³¨ï¼Œæ¨¡åž‹å¯ä»¥æ›´å¥½åœ°ç†è§£é—®é¢˜ä¸Žæ–‡æ¡£åŒºåŸŸä¹‹é—´çš„å…³ç³»ï¼Œä»Žè€Œæé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šBBox DocVQAçš„æž„å»ºæµç¨‹ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šåˆ†å‰²ï¼ˆSegmentï¼‰ã€åˆ¤æ–­ï¼ˆJudgeï¼‰å’Œç”Ÿæˆï¼ˆGenerateï¼‰ã€‚é¦–å…ˆï¼Œä½¿ç”¨åˆ†å‰²æ¨¡åž‹å°†æ–‡æ¡£åˆ†å‰²æˆä¸åŒçš„åŒºåŸŸã€‚ç„¶åŽï¼Œä½¿ç”¨VLMå¯¹åˆ†å‰²åŽçš„åŒºåŸŸè¿›è¡Œè¯­ä¹‰åˆ¤æ–­ï¼Œç­›é€‰å‡ºæœ‰æ„ä¹‰çš„åŒºåŸŸã€‚æœ€åŽï¼Œä½¿ç”¨å¦ä¸€ä¸ªVLMåŸºäºŽè¿™äº›åŒºåŸŸç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨äººå·¥è¿›è¡ŒéªŒè¯å’Œè´¨é‡ä¿è¯ã€‚è¿™ä¸ªæµç¨‹æ—¨åœ¨è‡ªåŠ¨åŒ–åœ°ç”Ÿæˆé«˜è´¨é‡çš„ã€å¸¦æœ‰è¾¹ç•Œæ¡†æ ‡æ³¨çš„QAå¯¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šBBox DocVQAçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å¤§è§„æ¨¡çš„ã€åŸºäºŽè¾¹ç•Œæ¡†çš„æ ‡æ³¨æ–¹å¼ã€‚ä¸ŽçŽ°æœ‰çš„DocVQAæ•°æ®é›†ç›¸æ¯”ï¼ŒBBox DocVQAæä¾›äº†æ›´ç»†ç²’åº¦çš„ç©ºé—´ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç©ºé—´è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨æž„å»ºæµç¨‹ä¹Ÿé™ä½Žäº†æ•°æ®é›†æž„å»ºçš„æˆæœ¬ï¼Œä½¿å¾—å¯ä»¥æž„å»ºæ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªåŠ¨æž„å»ºæµç¨‹ä¸­ï¼Œé€‰æ‹©åˆé€‚çš„åˆ†å‰²æ¨¡åž‹ã€è¯­ä¹‰åˆ¤æ–­VLMå’Œé—®é¢˜ç­”æ¡ˆç”ŸæˆVLMè‡³å…³é‡è¦ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†å…ˆè¿›çš„VLMæ¨¡åž‹ï¼Œå¹¶è¿›è¡Œäº†äººå·¥éªŒè¯ä»¥ç¡®ä¿æ•°æ®é›†çš„è´¨é‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚å¯èƒ½å› æ‰€ä½¿ç”¨çš„VLMæ¨¡åž‹è€Œå¼‚ï¼Œä½†æ€»ä½“ç›®æ ‡æ˜¯ç”Ÿæˆé«˜è´¨é‡çš„ã€ä¸Žæ–‡æ¡£åŒºåŸŸç›¸å…³çš„QAå¯¹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨BBox DocVQAä¸Šå¯¹GPT 5ã€Qwen2.5 VLå’ŒInternVLç­‰å¤šä¸ªæœ€å…ˆè¿›çš„VLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†æ¨¡åž‹åœ¨ç©ºé—´å®šä½å’ŒæŽ¨ç†å‡†ç¡®æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨BBox DocVQAä¸Šè¿›è¡Œå¾®è°ƒåŽï¼Œæ¨¡åž‹çš„è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆèƒ½åŠ›å‡å¾—åˆ°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†å¯¹äºŽå¢žå¼ºVLMæŽ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

BBox DocVQAæ•°æ®é›†å¯åº”ç”¨äºŽå„ç§æ–‡æ¡£ç†è§£å’Œè§†è§‰è¯­è¨€æŽ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ™ºèƒ½æ–‡æ¡£å¤„ç†ã€ä¿¡æ¯æŠ½å–ã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡å¢žå¼ºæ¨¡åž‹åœ¨ç©ºé—´æŽ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æé«˜æ–‡æ¡£å¤„ç†çš„è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œå‡†ç¡®æ€§ï¼Œä»Žè€Œæé«˜å·¥ä½œæ•ˆçŽ‡å¹¶é™ä½Žæˆæœ¬ã€‚è¯¥æ•°æ®é›†è¿˜æœ‰åŠ©äºŽå¼€å‘æ›´å¯è§£é‡Šçš„è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œæå‡ç”¨æˆ·å¯¹æ¨¡åž‹å†³ç­–çš„ä¿¡ä»»åº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

