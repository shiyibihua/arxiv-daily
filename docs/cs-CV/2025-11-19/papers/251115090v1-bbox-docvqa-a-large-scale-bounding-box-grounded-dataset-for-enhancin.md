---
layout: default
title: BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer
---

# BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer

**arXiv**: [2511.15090v1](https://arxiv.org/abs/2511.15090) | [PDF](https://arxiv.org/pdf/2511.15090.pdf)

**ä½œè€…**: Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBBox DocVQAæ•°æ®é›†ä»¥å¢žå¼ºæ–‡æ¡£è§†è§‰é—®ç­”ä¸­çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `æ–‡æ¡£è§†è§‰é—®ç­”` `ç©ºé—´æŽ¨ç†` `è¾¹ç•Œæ¡†æ ‡æ³¨` `è‡ªåŠ¨åŒ–æ•°æ®é›†æž„å»º` `è§†è§‰è¯­è¨€æ¨¡åž‹è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰DocVQAæ•°æ®é›†ç¼ºä¹ç»†ç²’åº¦ç©ºé—´æ ‡æ³¨ï¼Œé™åˆ¶è§†è§‰è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†èƒ½åŠ›
2. é‡‡ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿Segment Judge and Generateæž„å»ºæ•°æ®é›†ï¼ŒåŒ…å«3.6Kæ–‡æ¡£å’Œ32K QAå¯¹
3. åŸºå‡†æµ‹è¯•æ˜¾ç¤ºæ¨¡åž‹åœ¨ç©ºé—´å®šä½ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œå¾®è°ƒåŽå®šä½å’Œç­”æ¡ˆç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

