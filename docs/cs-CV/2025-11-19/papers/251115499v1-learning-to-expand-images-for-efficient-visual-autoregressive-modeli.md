---
layout: default
title: Learning to Expand Images for Efficient Visual Autoregressive Modeling
---

# Learning to Expand Images for Efficient Visual Autoregressive Modeling

**arXiv**: [2511.15499v1](https://arxiv.org/abs/2511.15499) | [PDF](https://arxiv.org/pdf/2511.15499.pdf)

**ä½œè€…**: Ruiqing Yang, Kaixin Zhang, Zheng Zhang, Shan You, Tao Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ‰©å±•è‡ªå›žå½’è¡¨ç¤ºä»¥è§£å†³è§†è§‰ç”Ÿæˆæ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è‡ªå›žå½’å»ºæ¨¡` `å›¾åƒç”Ÿæˆ` `é«˜æ•ˆè§£ç ` `èžºæ—‹å±•å¼€` `é•¿åº¦è‡ªé€‚åº”ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è‡ªå›žå½’æ¨¡åž‹å­˜åœ¨è§£ç æ•ˆçŽ‡ä½Žæˆ–å¤šå°ºåº¦è¡¨ç¤ºå¤æ‚çš„é—®é¢˜
2. å¼•å…¥èžºæ—‹å±•å¼€å’Œé•¿åº¦è‡ªé€‚åº”è§£ç ï¼Œæ¨¡æ‹Ÿäººç±»è§†è§‰ä¸­å¿ƒå‘å¤–æ„ŸçŸ¥
3. åœ¨ImageNetä¸Šå®žçŽ°ä¿çœŸåº¦ä¸Žæ•ˆçŽ‡çš„å…ˆè¿›æƒè¡¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.

