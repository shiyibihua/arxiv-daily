---
layout: default
title: GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning
---

# GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning

**arXiv**: [2511.15256v1](https://arxiv.org/abs/2511.15256) | [PDF](https://arxiv.org/pdf/2511.15256.pdf)

**ä½œè€…**: Yanchen Xu, Ziheng Jiao, Hongyuan Zhang, Xuelong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRPO-RMæ–¹æ³•ï¼Œé€šè¿‡GRPOé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¡¨ç¤ºæ¨¡åž‹ã€‚**

**å…³é”®è¯**: `è¡¨ç¤ºæ¨¡åž‹å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ ` `GRPOæ–¹æ³•` `è¾“å‡ºé›†è®¾è®¡` `å¥–åŠ±å‡½æ•°è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šGRPOèƒ½å¦ä»Žå¤§è¯­è¨€æ¨¡åž‹æ³›åŒ–åˆ°è¡¨ç¤ºå­¦ä¹ æ¨¡åž‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºç«‹é¢„å®šä¹‰è¾“å‡ºé›†ï¼Œè®¾è®¡ä¸“ç”¨å¥–åŠ±å‡½æ•°ï¼Œä¼˜åŒ–è¡¨ç¤ºæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªçœŸå®žæ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

