---
layout: default
title: GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning
---

# GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning

**arXiv**: [2511.15256v1](https://arxiv.org/abs/2511.15256) | [PDF](https://arxiv.org/pdf/2511.15256.pdf)

**ä½œè€…**: Yanchen Xu, Ziheng Jiao, Hongyuan Zhang, Xuelong Li

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRPO-RMï¼Œé€šè¿‡GRPOé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¡¨å¾æ¨¡åž‹**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨å¾å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¾®è°ƒ` `GRPO` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§åž‹è¯­è¨€æ¨¡åž‹å¾®è°ƒä¸­ï¼ŒGRPOè¡¨çŽ°å‡ºæœ‰æ•ˆæ€§ï¼Œä½†å…¶åœ¨è¡¨å¾æ¨¡åž‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…ç ”ç©¶ã€‚
2. GRPO-RMé€šè¿‡é¢„å®šä¹‰è¾“å‡ºé›†æ›¿ä»£tokené‡‡æ ·ï¼Œå¹¶è®¾è®¡ä¸“ç”¨å¥–åŠ±å‡½æ•°ï¼Œå®žçŽ°è¡¨å¾æ¨¡åž‹çš„GRPOä¼˜åŒ–ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒGRPO-RMåœ¨å¤šä¸ªçœŸå®žæ•°æ®é›†ä¸Šæœ‰æ•ˆæå‡äº†è¡¨å¾æ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGRPO-RMï¼ˆGroup Relative Policy Optimization for Representation Modelï¼‰çš„æ–¹æ³•ï¼Œç”¨äºŽå¾®è°ƒè¡¨å¾æ¨¡åž‹ã€‚è¯¥æ–¹æ³•å—åˆ°GRPOåœ¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¾®è°ƒä¸­çš„æˆåŠŸå¯å‘ï¼Œå¹¶æŽ¢ç´¢äº†å°†GRPOç±»ç­–ç•¥åº”ç”¨äºŽè¡¨å¾æ¨¡åž‹åŽè®­ç»ƒçš„å¯èƒ½æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒGRPO-RMå»ºç«‹äº†ä¸€ä¸ªé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆï¼Œä»¥åŠŸèƒ½æ€§åœ°æ›¿ä»£LLMsä¸­çš„tokenåºåˆ—é‡‡æ ·ï¼Œä»Žè€Œç”Ÿæˆä¸€ä¸ªè¾“å‡ºç»„ï¼Œè¿™å¯¹äºŽGRPOçš„æ¦‚çŽ‡é©±åŠ¨ä¼˜åŒ–è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±å‡½æ•°æ¥é€‚åº”è¡¨å¾æ¨¡åž‹çš„ç‰¹æ€§ã€‚åœ¨å„ç§çœŸå®žä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®žéªŒï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¾®è°ƒæ–¹æ³•ï¼Œå¦‚GRPOï¼Œåœ¨è¡¨å¾æ¨¡åž‹ä¸Šçš„ç›´æŽ¥åº”ç”¨é¢ä¸´æŒ‘æˆ˜ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºŽï¼Œè¡¨å¾æ¨¡åž‹çš„è¾“å‡ºé€šå¸¸ä¸æ˜¯ç¦»æ•£çš„tokenåºåˆ—ï¼Œè€Œæ˜¯è¿žç»­çš„å‘é‡ç©ºé—´ï¼Œè¿™ä½¿å¾—ç›´æŽ¥åº”ç”¨åŸºäºŽtokenåºåˆ—é‡‡æ ·çš„GRPOæ–¹æ³•å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥é€‚åº”è¡¨å¾æ¨¡åž‹çš„ç‰¹æ€§ï¼Œå¹¶å®žçŽ°GRPOçš„ä¼˜åŒ–ç›®æ ‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRPO-RMçš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œé€šè¿‡å»ºç«‹ä¸€ä¸ªé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆæ¥æ¨¡æ‹ŸLLMsä¸­çš„tokenåºåˆ—é‡‡æ ·è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºŽç»™å®šçš„è¾“å…¥ï¼Œæ¨¡åž‹ä¸æ˜¯ç›´æŽ¥è¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œè€Œæ˜¯ä»Žé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„å‘é‡ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥å°†è¡¨å¾æ¨¡åž‹çš„ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªç¦»æ•£çš„é€‰æ‹©é—®é¢˜ï¼Œä»Žè€Œå¯ä»¥ä½¿ç”¨GRPOè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦è®¾è®¡ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥åæ˜ è¡¨å¾æ¨¡åž‹çš„æ€§èƒ½æŒ‡æ ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šGRPO-RMçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å»ºç«‹é¢„å®šä¹‰çš„è¾“å‡ºé›†åˆï¼›2) ä½¿ç”¨è¡¨å¾æ¨¡åž‹å¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼›3) ä»Žè¾“å‡ºé›†åˆä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„å‘é‡ï¼›4) è®¡ç®—å¥–åŠ±å‡½æ•°ï¼›5) ä½¿ç”¨GRPOæ›´æ–°è¡¨å¾æ¨¡åž‹çš„å‚æ•°ã€‚å…¶ä¸­ï¼Œé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆå¯ä»¥æ˜¯éšæœºç”Ÿæˆçš„å‘é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯åŸºäºŽæŸç§å…ˆéªŒçŸ¥è¯†ç”Ÿæˆçš„å‘é‡ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRPO-RMçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼Œå®ƒå°†GRPOæ–¹æ³•ä»ŽLLMsæ‰©å±•åˆ°äº†è¡¨å¾æ¨¡åž‹ã€‚é€šè¿‡å»ºç«‹é¢„å®šä¹‰çš„è¾“å‡ºé›†åˆï¼ŒGRPO-RMæˆåŠŸåœ°å°†è¡¨å¾æ¨¡åž‹çš„ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªç¦»æ•£çš„é€‰æ‹©é—®é¢˜ï¼Œä»Žè€Œå¯ä»¥ä½¿ç”¨GRPOè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒGRPO-RMè¿˜è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥é€‚åº”è¡¨å¾æ¨¡åž‹çš„ç‰¹æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGRPO-RMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨GRPOçš„ä¼˜åŒ–èƒ½åŠ›ï¼Œä»Žè€Œæé«˜è¡¨å¾æ¨¡åž‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šé¢„å®šä¹‰è¾“å‡ºé›†åˆçš„å¤§å°æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œå®ƒå†³å®šäº†æ¨¡åž‹é€‰æ‹©çš„èŒƒå›´ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œä»¥è¡¡é‡æ¨¡åž‹è¾“å‡ºä¸Žç›®æ ‡å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚GRPOçš„å‚æ•°ï¼Œå¦‚å­¦ä¹ çŽ‡å’ŒæŠ˜æ‰£å› å­ï¼Œä¹Ÿéœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œä»¥èŽ·å¾—æœ€ä½³çš„æ€§èƒ½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒGRPO-RMåœ¨å¤šä¸ªçœŸå®žä¸–ç•Œæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒGRPO-RMç›¸æ¯”äºŽåŸºçº¿æ–¹æ³•ï¼Œå¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰æé«˜äº†5%ä»¥ä¸Šã€‚è¿™äº›ç»“æžœéªŒè¯äº†GRPO-RMçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜ŽGRPO-RMæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è¡¨å¾æ¨¡åž‹å¾®è°ƒæ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

GRPO-RMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒæ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ã€æŽ¨èç³»ç»Ÿç­‰ã€‚å®ƒå¯ä»¥ç”¨äºŽå¾®è°ƒå„ç§ç±»åž‹çš„è¡¨å¾æ¨¡åž‹ï¼Œä¾‹å¦‚å›¾åƒåµŒå…¥æ¨¡åž‹ã€æ–‡æœ¬åµŒå…¥æ¨¡åž‹ç­‰ã€‚é€šè¿‡æé«˜è¡¨å¾æ¨¡åž‹çš„æ€§èƒ½ï¼ŒGRPO-RMå¯ä»¥æ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„æ•ˆæžœã€‚æœªæ¥ï¼ŒGRPO-RMè¿˜å¯ä»¥åº”ç”¨äºŽæ›´å¤æ‚çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤šæ¨¡æ€å­¦ä¹ ã€è·¨è¯­è¨€å­¦ä¹ ç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

