---
layout: default
title: GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning
---

# GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.15256" target="_blank" class="toolbar-btn">arXiv: 2511.15256v1</a>
    <a href="https://arxiv.org/pdf/2511.15256.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15256v1" 
            onclick="toggleFavorite(this, '2511.15256v1', 'GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yanchen Xu, Ziheng Jiao, Hongyuan Zhang, Xuelong Li

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-19

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫GRPO-RMÔºåÈÄöËøáGRPOÈ©±Âä®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉË°®ÂæÅÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ë°®ÂæÅÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `ÂæÆË∞É` `GRPO` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞É‰∏≠ÔºåGRPOË°®Áé∞Âá∫ÊúâÊïàÊÄßÔºå‰ΩÜÂÖ∂Âú®Ë°®ÂæÅÊ®°Âûã‰∏äÁöÑÊ≥õÂåñËÉΩÂäõÊúâÂæÖÁ†îÁ©∂„ÄÇ
2. GRPO-RMÈÄöËøáÈ¢ÑÂÆö‰πâËæìÂá∫ÈõÜÊõø‰ª£tokenÈááÊ†∑ÔºåÂπ∂ËÆæËÆ°‰∏ìÁî®Â•ñÂä±ÂáΩÊï∞ÔºåÂÆûÁé∞Ë°®ÂæÅÊ®°ÂûãÁöÑGRPO‰ºòÂåñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGRPO-RMÂú®Â§ö‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÊúâÊïàÊèêÂçá‰∫ÜË°®ÂæÅÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫GRPO-RMÔºàGroup Relative Policy Optimization for Representation ModelÔºâÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂæÆË∞ÉË°®ÂæÅÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïÂèóÂà∞GRPOÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂæÆË∞É‰∏≠ÁöÑÊàêÂäüÂêØÂèëÔºåÂπ∂Êé¢Á¥¢‰∫ÜÂ∞ÜGRPOÁ±ªÁ≠ñÁï•Â∫îÁî®‰∫éË°®ÂæÅÊ®°ÂûãÂêéËÆ≠ÁªÉÁöÑÂèØËÉΩÊÄß„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåGRPO-RMÂª∫Á´ã‰∫Ü‰∏Ä‰∏™È¢ÑÂÆö‰πâÁöÑËæìÂá∫ÈõÜÂêàÔºå‰ª•ÂäüËÉΩÊÄßÂú∞Êõø‰ª£LLMs‰∏≠ÁöÑtokenÂ∫èÂàóÈááÊ†∑Ôºå‰ªéËÄåÁîüÊàê‰∏Ä‰∏™ËæìÂá∫ÁªÑÔºåËøôÂØπ‰∫éGRPOÁöÑÊ¶ÇÁéáÈ©±Âä®‰ºòÂåñËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊ≠§Â§ñÔºåËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÁöÑÂ•ñÂä±ÂáΩÊï∞Êù•ÈÄÇÂ∫îË°®ÂæÅÊ®°ÂûãÁöÑÁâπÊÄß„ÄÇÂú®ÂêÑÁßçÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™åÔºåÈ™åËØÅ‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂæÆË∞ÉÊñπÊ≥ïÔºåÂ¶ÇGRPOÔºåÂú®Ë°®ÂæÅÊ®°Âûã‰∏äÁöÑÁõ¥Êé•Â∫îÁî®Èù¢‰∏¥ÊåëÊàò„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÔºåË°®ÂæÅÊ®°ÂûãÁöÑËæìÂá∫ÈÄöÂ∏∏‰∏çÊòØÁ¶ªÊï£ÁöÑtokenÂ∫èÂàóÔºåËÄåÊòØËøûÁª≠ÁöÑÂêëÈáèÁ©∫Èó¥ÔºåËøô‰ΩøÂæóÁõ¥Êé•Â∫îÁî®Âü∫‰∫étokenÂ∫èÂàóÈááÊ†∑ÁöÑGRPOÊñπÊ≥ïÂèòÂæóÂõ∞Èöæ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÊù•ÈÄÇÂ∫îË°®ÂæÅÊ®°ÂûãÁöÑÁâπÊÄßÔºåÂπ∂ÂÆûÁé∞GRPOÁöÑ‰ºòÂåñÁõÆÊ†á„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGRPO-RMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÔºåÈÄöËøáÂª∫Á´ã‰∏Ä‰∏™È¢ÑÂÆö‰πâÁöÑËæìÂá∫ÈõÜÂêàÊù•Ê®°ÊãüLLMs‰∏≠ÁöÑtokenÂ∫èÂàóÈááÊ†∑ËøáÁ®ã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂØπ‰∫éÁªôÂÆöÁöÑËæìÂÖ•ÔºåÊ®°Âûã‰∏çÊòØÁõ¥Êé•ËæìÂá∫‰∏Ä‰∏™ÂêëÈáèÔºåËÄåÊòØ‰ªéÈ¢ÑÂÆö‰πâÁöÑËæìÂá∫ÈõÜÂêà‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÊúÄÂêàÈÄÇÁöÑÂêëÈáè„ÄÇËøôÊ†∑ÔºåÂ∞±ÂèØ‰ª•Â∞ÜË°®ÂæÅÊ®°ÂûãÁöÑ‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Á¶ªÊï£ÁöÑÈÄâÊã©ÈóÆÈ¢òÔºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®GRPOËøõË°å‰ºòÂåñ„ÄÇÊ≠§Â§ñÔºåËøòÈúÄË¶ÅËÆæËÆ°‰∏Ä‰∏™‰∏ìÈó®ÁöÑÂ•ñÂä±ÂáΩÊï∞Ôºå‰ª•ÂèçÊò†Ë°®ÂæÅÊ®°ÂûãÁöÑÊÄßËÉΩÊåáÊ†á„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGRPO-RMÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) Âª∫Á´ãÈ¢ÑÂÆö‰πâÁöÑËæìÂá∫ÈõÜÂêàÔºõ2) ‰ΩøÁî®Ë°®ÂæÅÊ®°ÂûãÂØπËæìÂÖ•ËøõË°åÁºñÁ†ÅÔºõ3) ‰ªéËæìÂá∫ÈõÜÂêà‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÊúÄÂêàÈÄÇÁöÑÂêëÈáèÔºõ4) ËÆ°ÁÆóÂ•ñÂä±ÂáΩÊï∞Ôºõ5) ‰ΩøÁî®GRPOÊõ¥Êñ∞Ë°®ÂæÅÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇÂÖ∂‰∏≠ÔºåÈ¢ÑÂÆö‰πâÁöÑËæìÂá∫ÈõÜÂêàÂèØ‰ª•ÊòØÈöèÊú∫ÁîüÊàêÁöÑÂêëÈáèÔºå‰πüÂèØ‰ª•ÊòØÂü∫‰∫éÊüêÁßçÂÖàÈ™åÁü•ËØÜÁîüÊàêÁöÑÂêëÈáè„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÂ∫îÁî®Âú∫ÊôØËøõË°åË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGRPO-RMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºåÂÆÉÂ∞ÜGRPOÊñπÊ≥ï‰ªéLLMsÊâ©Â±ïÂà∞‰∫ÜË°®ÂæÅÊ®°Âûã„ÄÇÈÄöËøáÂª∫Á´ãÈ¢ÑÂÆö‰πâÁöÑËæìÂá∫ÈõÜÂêàÔºåGRPO-RMÊàêÂäüÂú∞Â∞ÜË°®ÂæÅÊ®°ÂûãÁöÑ‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Á¶ªÊï£ÁöÑÈÄâÊã©ÈóÆÈ¢òÔºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®GRPOËøõË°å‰ºòÂåñ„ÄÇÊ≠§Â§ñÔºåGRPO-RMËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÁöÑÂ•ñÂä±ÂáΩÊï∞Ôºå‰ª•ÈÄÇÂ∫îË°®ÂæÅÊ®°ÂûãÁöÑÁâπÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåGRPO-RMËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®GRPOÁöÑ‰ºòÂåñËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òË°®ÂæÅÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈ¢ÑÂÆö‰πâËæìÂá∫ÈõÜÂêàÁöÑÂ§ßÂ∞èÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÂèÇÊï∞ÔºåÂÆÉÂÜ≥ÂÆö‰∫ÜÊ®°ÂûãÈÄâÊã©ÁöÑËåÉÂõ¥„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÂ∫îÁî®Âú∫ÊôØËøõË°åË∞ÉÊï¥Ôºå‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶‰Ωú‰∏∫Â•ñÂä±ÂáΩÊï∞Ôºå‰ª•Ë°°ÈáèÊ®°ÂûãËæìÂá∫‰∏éÁõÆÊ†áÂêëÈáè‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶„ÄÇGRPOÁöÑÂèÇÊï∞ÔºåÂ¶ÇÂ≠¶‰π†ÁéáÂíåÊäòÊâ£Âõ†Â≠êÔºå‰πüÈúÄË¶ÅËøõË°åË∞ÉÊï¥Ôºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGRPO-RMÂú®Â§ö‰∏™ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂõæÂÉèÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÔºåGRPO-RMÁõ∏ÊØî‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂπ≥ÂùáÁ≤æÂ∫¶ÂùáÂÄºÔºàmAPÔºâÊèêÈ´ò‰∫Ü5%‰ª•‰∏ä„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜGRPO-RMÁöÑÊúâÊïàÊÄßÔºåÂπ∂Ë°®ÊòéGRPO-RMÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑË°®ÂæÅÊ®°ÂûãÂæÆË∞ÉÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GRPO-RMÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂõæÂÉèÊ£ÄÁ¥¢„ÄÅÊñáÊú¨ÂàÜÁ±ª„ÄÅÊé®ËçêÁ≥ªÁªüÁ≠â„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÂæÆË∞ÉÂêÑÁßçÁ±ªÂûãÁöÑË°®ÂæÅÊ®°ÂûãÔºå‰æãÂ¶ÇÂõæÂÉèÂµåÂÖ•Ê®°Âûã„ÄÅÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÁ≠â„ÄÇÈÄöËøáÊèêÈ´òË°®ÂæÅÊ®°ÂûãÁöÑÊÄßËÉΩÔºåGRPO-RMÂèØ‰ª•ÊòæËëóÊèêÂçáËøô‰∫õÂ∫îÁî®ÁöÑÊïàÊûú„ÄÇÊú™Êù•ÔºåGRPO-RMËøòÂèØ‰ª•Â∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂ§öÊ®°ÊÄÅÂ≠¶‰π†„ÄÅË∑®ËØ≠Ë®ÄÂ≠¶‰π†Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

