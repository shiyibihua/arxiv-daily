---
layout: default
title: MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping
---

# MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping

**arXiv**: [2511.15690v1](https://arxiv.org/abs/2511.15690) | [PDF](https://arxiv.org/pdf/2511.15690.pdf)

**ä½œè€…**: Yushi Huang, Zining Wang, Zhihang Yuan, Yifu Ding, Ruihao Gong, Jinyang Guo, Xianglong Liu, Jun Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoDESæ¡†æž¶ä»¥åŠ¨æ€è·³è¿‡ä¸“å®¶ï¼ŒåŠ é€ŸMoEå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ä¸“å®¶æ··åˆæ¨¡åž‹` `åŠ¨æ€ä¸“å®¶è·³è¿‡` `æŽ¨ç†åŠ é€Ÿ` `å…¨å±€è°ƒåˆ¶é—¨æŽ§` `åŒæ¨¡æ€é˜ˆå€¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMoEå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹è®¡ç®—æ•ˆçŽ‡ä½Žï¼ŒçŽ°æœ‰ä¸“å®¶è·³è¿‡æ–¹æ³•å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å…¨å±€è°ƒåˆ¶å±€éƒ¨é—¨æŽ§æœºåˆ¶å’ŒåŒæ¨¡æ€é˜ˆå€¼æ³•ï¼Œè‡ªé€‚åº”è·³è¿‡å†—ä½™ä¸“å®¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡è¾¾10.67%ï¼ŒæŽ¨ç†é€Ÿåº¦æ˜¾è‘—æé«˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.

