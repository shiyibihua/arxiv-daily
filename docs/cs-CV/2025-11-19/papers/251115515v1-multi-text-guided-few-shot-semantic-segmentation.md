---
layout: default
title: Multi-Text Guided Few-Shot Semantic Segmentation
---

# Multi-Text Guided Few-Shot Semantic Segmentation

**arXiv**: [2511.15515v1](https://arxiv.org/abs/2511.15515) | [PDF](https://arxiv.org/pdf/2511.15515.pdf)

**ä½œè€…**: Qiang Jiao, Bin Yan, Yi Yang, Mengrui Shi, Qiang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMTGNetä»¥è§£å†³å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä¸­æ–‡æœ¬å…ˆéªŒä¸è¶³å’Œè·¨æ¨¡æ€äº¤äº’å¼±çš„é—®é¢˜**

**å…³é”®è¯**: `å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²` `è·¨æ¨¡æ€äº¤äº’` `å¤šæ–‡æœ¬å¼•å¯¼` `è§†è§‰å…ˆéªŒä¼˜åŒ–` `è¯­ä¹‰ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•æ–‡æœ¬æç¤ºæ— æ³•è¦†ç›–å¤æ‚ç±»åˆ«è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¯¼è‡´ç›®æ ‡åŒºåŸŸæ¿€æ´»ä¸å®Œæ•´å’Œå™ªå£°å¹²æ‰°
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å¤šæ–‡æœ¬å…ˆéªŒç²¾ç‚¼å’Œæ–‡æœ¬é”šç‚¹ç‰¹å¾èžåˆæ¨¡å—ï¼Œå¢žå¼ºè·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰ä¸€è‡´æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PASCAL-5iå’ŒCOCO-20iåŸºå‡†ä¸Šå–å¾—é«˜mIoUï¼Œæ˜¾è‘—æå‡ç±»å†…å˜åŒ–å¤§çš„åœºæ™¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.

