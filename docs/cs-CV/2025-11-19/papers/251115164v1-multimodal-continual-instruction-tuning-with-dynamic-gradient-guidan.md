---
layout: default
title: Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance
---

# Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance

**arXiv**: [2511.15164v1](https://arxiv.org/abs/2511.15164) | [PDF](https://arxiv.org/pdf/2511.15164.pdf)

**ä½œè€…**: Songze Li, Mingyu Gao, Tonghua Su, Xu-Yao Zhang, Zhongjie Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€æ¢¯åº¦å¼•å¯¼æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `æ¢¯åº¦å¼•å¯¼` `æŒ‡ä»¤è°ƒä¼˜` `å‚æ•°ç©ºé—´å‡ ä½•` `ä¼¯åŠªåˆ©é‡‡æ ·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æŒç»­å­¦ä¹ é¢ä¸´ç¾éš¾æ€§é—å¿˜ï¼Œæ–°ä»»åŠ¡å­¦ä¹ å¯¼è‡´æ—§ä»»åŠ¡æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å‚æ•°ç©ºé—´å‡ ä½•ç‰¹æ€§è¿‘ä¼¼ç¼ºå¤±æ¢¯åº¦ï¼Œç»“åˆé‡æ”¾ç¼“å†²å’Œä¼¯åŠªåˆ©é‡‡æ ·å¹³è¡¡ç¨³å®šæ€§ä¸Žå¯å¡‘æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°æ®é›†ä¸Šå®žçŽ°å…ˆè¿›æ€§èƒ½ï¼Œæ— éœ€æ¨¡åž‹æ‰©å±•ï¼Œæœ‰æ•ˆç¼“è§£é—å¿˜å¹¶ä¿æŒç´§å‡‘æž¶æž„

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.

