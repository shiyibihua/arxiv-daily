---
layout: default
title: Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners
---

# Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners

**arXiv**: [2511.15468v1](https://arxiv.org/abs/2511.15468) | [PDF](https://arxiv.org/pdf/2511.15468.pdf)

**ä½œè€…**: Xabier Lekunberri, Ahmad Kamal, Izaro Goienetxea, Jon Ruiz, IÃ±aki Quincoces, Jaime Valls Miro, Ignacio Arganda-Carreras, Jose A. Fernandes-Salvador

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽYOLOv9-SAM2ä¸Žåˆ†å±‚åˆ†ç±»çš„è§†è§‰ç®¡é“ï¼Œä»¥å‡†ç¡®ä¼°è®¡çƒ­å¸¦é‡‘æžªé±¼å›´ç½‘æ¸”èŽ·ç‰©ç»„æˆã€‚**

**å…³é”®è¯**: `ç‰©ç§è¯†åˆ«` `å®žä¾‹åˆ†å‰²` `ç›®æ ‡è·Ÿè¸ª` `åˆ†å±‚åˆ†ç±»` `ç”µå­ç›‘æŽ§` `æ¸”ä¸šç®¡ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸“å®¶åŒºåˆ†å¤§çœ¼é‡‘æžªé±¼ä¸Žé»„é³é‡‘æžªé±¼çš„ä¸€è‡´çŽ‡ä½Žï¼ŒAIç‰©ç§è¯†åˆ«éœ€å¹³è¡¡æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¯”è¾ƒå¤šç§åˆ†å‰²æ–¹æ³•ï¼ŒYOLOv9-SAM2è¡¨çŽ°æœ€ä½³ï¼Œç»“åˆByteTrackè·Ÿè¸ªä¸Žåˆ†å±‚åˆ†ç±»ã€‚
3. å®žéªŒæ•ˆæžœï¼šæœ€ä½³æ¨¡åž‹å®žçŽ°84.8%ä¸ªä½“åˆ†å‰²åˆ†ç±»ï¼Œå¹³å‡è¯¯å·®4.5%ï¼ŒéªŒè¯mAPä¸º0.66ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.

