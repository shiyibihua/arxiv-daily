---
layout: default
title: Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception
---

# Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception

**arXiv**: [2511.15279v1](https://arxiv.org/abs/2511.15279) | [PDF](https://arxiv.org/pdf/2511.15279.pdf)

**ä½œè€…**: Jiashu Yang, Yifan Han, Yucheng Xie, Ning Guo, Wenzhao Lian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEyeVLAæœºå™¨äººçœ¼çƒç³»ç»Ÿï¼Œä»¥è§£å†³å…·èº«AIä¸­è§†è§‰æ„ŸçŸ¥çš„ä¸»åŠ¨æ•°æ®èŽ·å–é—®é¢˜ã€‚**

**å…³é”®è¯**: `å…·èº«AI` `ä¸»åŠ¨è§†è§‰æ„ŸçŸ¥` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººè§†è§‰ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰æ¨¡åž‹å’Œå›ºå®šç›¸æœºç³»ç»Ÿæ— æ³•å…¼é¡¾å¹¿åŸŸè¦†ç›–ä¸Žç»†ç²’åº¦ç»†èŠ‚èŽ·å–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†åŠ¨ä½œè¡Œä¸ºç¦»æ•£åŒ–ä¸ºåŠ¨ä½œä»¤ç‰Œï¼Œä¸Žè§†è§‰è¯­è¨€æ¨¡åž‹é›†æˆè¿›è¡Œè”åˆå»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç³»ç»Ÿåœ¨çœŸå®žçŽ¯å¢ƒä¸­é«˜æ•ˆæ‰§è¡ŒæŒ‡ä»¤ï¼Œé€šè¿‡æ—‹è½¬å’Œç¼©æ”¾ä¸»åŠ¨èŽ·å–å‡†ç¡®è§†è§‰ä¿¡æ¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.

