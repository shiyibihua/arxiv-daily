---
layout: default
title: When to Think and When to Look: Uncertainty-Guided Lookback
---

# When to Think and When to Look: Uncertainty-Guided Lookback

**arXiv**: [2511.15613v1](https://arxiv.org/abs/2511.15613) | [PDF](https://arxiv.org/pdf/2511.15613.pdf)

**ä½œè€…**: Jing Bi, Filippos Bellos, Junjia Guo, Yayuan Li, Chao Huang, Yunlong, Tang, Luchuan Song, Susan Liang, Zhongfei, Zhang, Jason J. Corso, Chenliang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸ç¡®å®šæ€§å¼•å¯¼å›žçœ‹æ–¹æ³•ï¼Œä»¥æå‡å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹çš„è§†è§‰æŽ¨ç†æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `è§†è§‰æŽ¨ç†` `ä¸ç¡®å®šæ€§å¼•å¯¼` `è§£ç ç­–ç•¥` `å¤šæ¨¡æ€åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿æŽ¨ç†é“¾æ˜“å¿½ç•¥å›¾åƒï¼Œå¯¼è‡´è§†è§‰æŽ¨ç†æ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆä¸ç¡®å®šæ€§ä¿¡å·ä¸Žè‡ªé€‚åº”å›žçœ‹æç¤ºï¼Œæ— éœ€è®­ç»ƒã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨MMMUç­‰åŸºå‡†ä¸Šæå‡æ€§èƒ½ï¼Œä¼˜äºŽå¤šç§è§£ç åŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.

