---
layout: default
title: Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation
---

# Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation

**arXiv**: [2511.15118v1](https://arxiv.org/abs/2511.15118) | [PDF](https://arxiv.org/pdf/2511.15118.pdf)

**ä½œè€…**: Jin Wang, Bingfeng Zhang, Jian Pang, Weifeng Liu, Baodi Liu, Honglong Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— åè¯­ä¹‰è§£ç ç­–ç•¥ï¼Œç»“åˆSAMä¸ŽCLIPæå‡å°‘æ ·æœ¬åˆ†å‰²æ€§èƒ½**

**å…³é”®è¯**: `å°‘æ ·æœ¬åˆ†å‰²` `è¯­ä¹‰è§£ç ` `è§†è§‰åŸºç¡€æ¨¡åž‹` `ç‰¹å¾å¢žå¼º` `æç¤ºç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSAMè§£ç ä¾èµ–ç²¾ç¡®æç¤ºï¼Œåœ¨å°‘æ ·æœ¬åˆ†å‰²ä¸­æ˜“äº§ç”Ÿåå·®ï¼Œé™åˆ¶æ³›åŒ–èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å…¨å±€è¡¥å……å’Œå±€éƒ¨å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨CLIPè¯­ä¹‰å¢žå¼ºSAMç‰¹å¾ï¼Œç”Ÿæˆç›®æ ‡æç¤ºåµŒå…¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡åž‹ï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼æå‡ç›®æ ‡åŒºåŸŸå…³æ³¨ï¼Œå®žçŽ°ä¸€è‡´é¢„æµ‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.

