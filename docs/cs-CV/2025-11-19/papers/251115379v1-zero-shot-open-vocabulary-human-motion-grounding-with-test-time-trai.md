---
layout: default
title: Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training
---

# Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training

**arXiv**: [2511.15379v1](https://arxiv.org/abs/2511.15379) | [PDF](https://arxiv.org/pdf/2511.15379.pdf)

**ä½œè€…**: Yunjiao Zhou, Xinyan Chen, Junlang Qian, Lihua Xie, Jianfei Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZOMGæ¡†æž¶ä»¥é›¶æ ·æœ¬å¼€æ”¾è¯æ±‡æ–¹å¼å®žçŽ°äººç±»è¿åŠ¨åºåˆ—çš„è¯­ä¹‰åˆ†å‰²**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `å¼€æ”¾è¯æ±‡è¿åŠ¨ç†è§£` `è¯­ä¹‰åˆ†å‰²` `è½¯æŽ©ç ä¼˜åŒ–` `è¯­è¨€æ¨¡åž‹é›†æˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–é¢„å®šä¹‰åŠ¨ä½œç±»çš„å¯†é›†ç›‘ç£ï¼Œéš¾ä»¥é€‚åº”å¼€æ”¾è¯æ±‡çœŸå®žåœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè¯­è¨€è¯­ä¹‰åˆ†å‰²å’Œè½¯æŽ©ç ä¼˜åŒ–ï¼Œæ— éœ€æ ‡æ³¨æˆ–å¾®è°ƒå³å¯åˆ†è§£è¿åŠ¨åºåˆ—ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨HumanML3DåŸºå‡†ä¸ŠmAPæå‡8.7%ï¼Œä¸‹æ¸¸æ£€ç´¢ä»»åŠ¡è¡¨çŽ°æ˜¾è‘—æ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.

