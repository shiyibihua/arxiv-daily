---
layout: default
title: From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers
---

# From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers

**arXiv**: [2511.15572v1](https://arxiv.org/abs/2511.15572) | [PDF](https://arxiv.org/pdf/2511.15572.pdf)

**ä½œè€…**: Huiyuan Tian, Bonan Xu, Shijian Li, Xin Jin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç‰¹å¾æå‡ä¸Žå®½åº¦å¯¹é½ç­–ç•¥ä»¥è§£å†³ViTç‰¹å¾è’¸é¦ä¸­çš„ç¼–ç ä¸åŒ¹é…é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰Transformer` `ç‰¹å¾è’¸é¦` `ç¼–ç ä¸åŒ¹é…` `ä½Žç§©ç‰¹å¾` `çŸ¥è¯†è’¸é¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šViTç‰¹å¾è’¸é¦å› å…¨å±€ä½Žç§©ä¸Žä¸ªä½“ä»¤ç‰Œé«˜å¸¦å®½ç¼–ç ä¸åŒ¹é…è€Œå¤±æ•ˆ
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è½»é‡æŠ•å½±å™¨æˆ–ä»…æ‰©å±•å­¦ç”Ÿæœ«å—å®½åº¦ä»¥å¯¹é½ç‰¹å¾ç¼–ç 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet-1Kä¸Šæå‡DeiT-Tinyå‡†ç¡®çŽ‡è‡³77.53%å’Œ78.23%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\%/95\%/90\%/80\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\%$ to $77.53\%$ and $78.23\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.

