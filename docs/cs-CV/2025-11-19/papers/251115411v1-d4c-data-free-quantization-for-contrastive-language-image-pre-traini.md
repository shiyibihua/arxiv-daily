---
layout: default
title: D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models
---

# D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models

**arXiv**: [2511.15411v1](https://arxiv.org/abs/2511.15411) | [PDF](https://arxiv.org/pdf/2511.15411.pdf)

**ä½œè€…**: Wenlun Zhang, Yunshan Zhong, Zihao Ding, Xinyu Li, Kentaro Yoshioka

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD4Cæ¡†æž¶ä»¥è§£å†³CLIPæ¨¡åž‹æ•°æ®æ— å…³é‡åŒ–ä¸­çš„è¯­ä¹‰ä¸è¶³å’Œå¤šæ ·æ€§ä½Žé—®é¢˜**

**å…³é”®è¯**: `æ•°æ®æ— å…³é‡åŒ–` `å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒ` `æ¨¡åž‹åŽ‹ç¼©` `ä¼ªå›¾åƒåˆæˆ` `é›¶-shotåˆ†ç±»` `éšç§ä¿æŠ¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ•°æ®æ— å…³é‡åŒ–æ–¹æ³•ç›´æŽ¥åº”ç”¨äºŽCLIPæ¨¡åž‹æ—¶ï¼Œå› åˆæˆæ ·æœ¬è¯­ä¹‰å†…å®¹ä¸è¶³å’Œå›¾åƒå†…å¤šæ ·æ€§ä½Žå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æç¤ºå¼•å¯¼è¯­ä¹‰æ³¨å…¥ã€ç»“æž„å¯¹æ¯”ç”Ÿæˆå’Œæ‰°åŠ¨æ„ŸçŸ¥å¢žå¼ºåˆæˆè¯­ä¹‰ä¸°å¯Œä¸”ç»“æž„å¤šæ ·çš„ä¼ªå›¾åƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§æ¯”ç‰¹å®½åº¦å’Œæ¨¡åž‹ä¸ŠéªŒè¯ï¼Œä¾‹å¦‚W4A8è®¾ç½®ä¸‹ï¼ŒCIFAR-10é›¶-shotåˆ†ç±»Top-1å‡†ç¡®çŽ‡æå‡è¾¾12.4%å’Œ18.9%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

