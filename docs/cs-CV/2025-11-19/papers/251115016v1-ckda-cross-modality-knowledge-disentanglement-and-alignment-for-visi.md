---
layout: default
title: CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification
---

# CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification

**arXiv**: [2511.15016v1](https://arxiv.org/abs/2511.15016) | [PDF](https://arxiv.org/pdf/2511.15016.pdf)

**ä½œè€…**: Zhenyu Cui, Jiahuan Zhou, Yuxin Peng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCKDAæ–¹æ³•ä»¥è§£å†³å¯è§å…‰-çº¢å¤–ç»ˆèº«è¡Œäººé‡è¯†åˆ«ä¸­çš„çŸ¥è¯†å¹²æ‰°é—®é¢˜**

**å…³é”®è¯**: `ç»ˆèº«å­¦ä¹ ` `è¡Œäººé‡è¯†åˆ«` `è·¨æ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†è§£è€¦` `çŸ¥è¯†å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡æ€ç‰¹å®šçŸ¥è¯†ä¸Žæ¨¡æ€é€šç”¨çŸ¥è¯†åœ¨æŒç»­å­¦ä¹ ä¸­ç›¸äº’å¹²æ‰°ï¼Œå¯¼è‡´åä½œé—å¿˜
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨MCPå’ŒMSPæ¨¡å—è§£è€¦çŸ¥è¯†ï¼ŒCKAæ¨¡å—å¯¹é½æ–°æ—§çŸ¥è¯†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä»£ç å·²å¼€æº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.

