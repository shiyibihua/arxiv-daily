---
layout: default
title: Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models
---

# Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models

**arXiv**: [2511.15390v1](https://arxiv.org/abs/2511.15390) | [PDF](https://arxiv.org/pdf/2511.15390.pdf)

**ä½œè€…**: Haidong Kang, Lihong Lin, Enneng Yang, Hongning Dai, Hao Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoPruneæ–¹æ³•ï¼Œä½¿å¤§è¯­è¨€æ¨¡åž‹è‡ªåŠ¨è®¾è®¡å‰ªæžç®—æ³•ä»¥è§£å†³ä¸“å®¶ä¾èµ–å’Œæ€§èƒ½ä¸‹é™é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å‰ªæž` `è‡ªåŠ¨ç®—æ³•è®¾è®¡` `å›¾é©±åŠ¨æ€ç»´é“¾` `åŠ¨æ€ç¨€ç–åˆ†é…` `ç¦»ç¾¤å€¼é—®é¢˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹å‰ªæžä¾èµ–ä¸“å®¶çŸ¥è¯†ï¼Œä¸”é«˜å‰ªæžæ¯”ä¸‹å› ç¦»ç¾¤å€¼é—®é¢˜å¯¼è‡´æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å›¾é©±åŠ¨æ€ç»´é“¾ä¼˜åŒ–æç¤ºï¼Œä½¿æ¨¡åž‹è‡ªåŠ¨ç”Ÿæˆå‰ªæžç®—æ³•ï¼Œå¹¶å¼•å…¥åæ€æ„ŸçŸ¥åŠ¨æ€ç¨€ç–åˆ†é…
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä»£ç å·²å¼€æº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.

