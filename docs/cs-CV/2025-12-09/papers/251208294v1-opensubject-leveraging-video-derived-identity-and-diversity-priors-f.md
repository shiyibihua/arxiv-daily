---
layout: default
title: OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation
---

# OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation

**arXiv**: [2512.08294v1](https://arxiv.org/abs/2512.08294) | [PDF](https://arxiv.org/pdf/2512.08294.pdf)

**ä½œè€…**: Yexin Liu, Manyuan Zhang, Yueze Wang, Hongyu Li, Dian Zheng, Weiming Zhang, Changsheng Lu, Xunliang Cai, Yan Feng, Peng Pei, Harry Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenSubjectæ•°æ®é›†ä»¥è§£å†³ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆä¸Žç¼–è¾‘ä¸­çš„èº«ä»½åç¦»å’Œå¤æ‚åœºæ™¯éš¾é¢˜**

**å…³é”®è¯**: `ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆ` `è§†é¢‘è¡ç”Ÿæ•°æ®é›†` `èº«ä»½å…ˆéªŒ` `å›¾åƒç¼–è¾‘` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤æ‚åœºæ™¯å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ¨¡åž‹å¸¸åç¦»å‚è€ƒèº«ä»½ï¼Œåœ¨å¤æ‚å¤šä¸»é¢˜åœºæ™¯ä¸­è¡¨çŽ°ä¸ä½³
2. æž„å»ºè§†é¢‘è¡ç”Ÿå¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€šè¿‡å››é˜¶æ®µæµç¨‹åˆ©ç”¨è·¨å¸§èº«ä»½å…ˆéªŒï¼ŒåŒ…æ‹¬è§†é¢‘ç­›é€‰ã€ä¸»é¢˜æŒ–æŽ˜é…å¯¹ã€èº«ä»½ä¿æŒå›¾åƒåˆæˆå’ŒéªŒè¯æ ‡æ³¨
3. å®žéªŒè¡¨æ˜Žä½¿ç”¨OpenSubjectè®­ç»ƒèƒ½æå‡ç”Ÿæˆå’Œç¼–è¾‘æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

