---
layout: default
title: IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams
---

# IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams

**arXiv**: [2512.08877v1](https://arxiv.org/abs/2512.08877) | [PDF](https://arxiv.org/pdf/2512.08877.pdf)

**ä½œè€…**: Ryan LeRoy, Jack Kolb

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—‹è½¬ç­–ç•¥è®­ç»ƒä»¥ç ”ç©¶å¼‚æž„å¤šæ™ºèƒ½ä½“å›¢é˜Ÿä¸­è‡ªå­¦ä¹ PPOçš„æ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `å¼‚æž„æ™ºèƒ½ä½“` `æ³›åŒ–èƒ½åŠ›` `è‡ªå­¦ä¹ ` `æ—‹è½¬ç­–ç•¥è®­ç»ƒ` `PPOç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå­¦ä¹ PPOæ™ºèƒ½ä½“æ˜¯å¦å­¦ä¹ åŸºäºŽæ¸¸æˆçš„é€šç”¨åè°ƒç­–ç•¥ï¼Œè€Œéžè¿‡æ‹Ÿåˆè®­ç»ƒä¼™ä¼´è¡Œä¸º
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ—‹è½¬ç­–ç•¥è®­ç»ƒï¼Œåœ¨è®­ç»ƒä¸­è½®æ¢å¼‚æž„é˜Ÿå‹ç­–ç•¥ä»¥æš´éœ²æ›´å¹¿ä¼™ä¼´ç­–ç•¥èŒƒå›´
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HeMACçŽ¯å¢ƒä¸­ï¼ŒIPPOåŸºçº¿æ³›åŒ–è‡³æ–°é˜Ÿå‹ç®—æ³•ï¼Œæ€§èƒ½ä¸ŽRPTç›¸ä¼¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.

