---
layout: default
title: InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models
---

# InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models

**arXiv**: [2512.08829v1](https://arxiv.org/abs/2512.08829) | [PDF](https://arxiv.org/pdf/2512.08829.pdf)

**ä½œè€…**: Hongyuan Tao, Bencheng Liao, Shaoyu Chen, Haoran Yin, Qian Zhang, Wenyu Liu, Xinggang Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInfiniteVLï¼Œç»“åˆæ»‘åŠ¨çª—å£ä¸Žçº¿æ€§æ³¨æ„åŠ›ï¼Œå®žçŽ°é«˜æ•ˆæ— é™è¾“å…¥è§†è§‰è¯­è¨€æ¨¡åž‹ã€‚**

**å…³é”®è¯**: `æ— é™è¾“å…¥è§†è§‰è¯­è¨€æ¨¡åž‹` `çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›` `æ»‘åŠ¨çª—å£æ³¨æ„åŠ›` `é•¿åºåˆ—è®­ç»ƒ` `å®žæ—¶è§†é¢‘ç†è§£` `è’¸é¦é¢„è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šçª—å£æ³¨æ„åŠ›è¶…çª—æ€§èƒ½ä¸‹é™ï¼Œçº¿æ€§æ³¨æ„åŠ›ä¿¡æ¯å¯†é›†åž‹ä»»åŠ¡è¡¨çŽ°ä¸è¶³ã€‚
2. æ–¹æ³•ï¼šèžåˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸ŽGated DeltaNetï¼Œè®¾è®¡ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚
3. æ•ˆæžœï¼šæ•°æ®é‡å°‘2%ï¼Œæ€§èƒ½åŒ¹é…é¢†å…ˆTransformeræ¨¡åž‹ï¼ŒæŽ¨ç†åŠ é€Ÿ3.6å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

