---
layout: default
title: MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones
---

# MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones

**arXiv**: [2512.08211v1](https://arxiv.org/abs/2512.08211) | [PDF](https://arxiv.org/pdf/2512.08211.pdf)

**ä½œè€…**: Jiaxiang Geng, Lunyu Zhao, Yiyi Lu, Bing Luo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMobileFineTuneræ¡†æž¶ä»¥åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®žçŽ°ç«¯åˆ°ç«¯å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒ**

**å…³é”®è¯**: `ç§»åŠ¨è®¾å¤‡å¾®è°ƒ` `å¤§è¯­è¨€æ¨¡åž‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ç³»ç»Ÿä¼˜åŒ–` `éšç§ä¿æŠ¤` `å¼€æºæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç§»åŠ¨è®¾å¤‡ç¼ºä¹å¼€æºæ¡†æž¶æ”¯æŒå¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒï¼ŒçŽ°æœ‰æ–¹æ³•å¤šåŸºäºŽæ¨¡æ‹Ÿæˆ–éžç§»åŠ¨è®¾å¤‡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å‚æ•°åˆ†ç‰‡ã€æ¢¯åº¦ç´¯ç§¯å’Œèƒ½é‡æ„ŸçŸ¥è°ƒåº¦ç­‰ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œæ”¯æŒå…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ‰‹æœºä¸Šå¾®è°ƒGPT-2ã€Gemma 3å’ŒQwen 2.5ï¼ŒéªŒè¯ä¼˜åŒ–æœ‰æ•ˆæ€§å¹¶ç¡®ç«‹æ¡†æž¶å¯è¡Œæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

