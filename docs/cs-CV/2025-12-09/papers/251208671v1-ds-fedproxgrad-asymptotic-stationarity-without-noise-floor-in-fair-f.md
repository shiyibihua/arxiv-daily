---
layout: default
title: DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning
---

# DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning

**arXiv**: [2512.08671v1](https://arxiv.org/abs/2512.08671) | [PDF](https://arxiv.org/pdf/2512.08671.pdf)

**ä½œè€…**: Huzaifa Arif

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDS FedProxGradä»¥åœ¨å…¬å¹³è”é‚¦å­¦ä¹ ä¸­å®žçŽ°æ— å™ªå£°åº•é™çš„æ¸è¿‘å¹³ç¨³æ€§**

**å…³é”®è¯**: `å…¬å¹³è”é‚¦å­¦ä¹ ` `éžå‡¸ä¼˜åŒ–` `æ¸è¿‘æ”¶æ•›` `è¡°å‡æ­¥é•¿` `è¿‘ç«¯æ¢¯åº¦æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŽŸFedProxGradåœ¨å…¬å¹³è”é‚¦å­¦ä¹ ä¸­æ”¶æ•›è‡³å™ªå£°ä¸»å¯¼é‚»åŸŸï¼Œå­˜åœ¨æ–¹å·®è¯±å¯¼çš„å™ªå£°åº•é™é—®é¢˜
2. æ‰©å±•ä¸ºDS FedProxGradæ¡†æž¶ï¼Œç»“åˆè¡°å‡æ­¥é•¿å’Œä¸ç²¾ç¡®å±€éƒ¨è§£ï¼Œè¯æ˜Žæ¸è¿‘å¹³ç¨³æ€§
3. åœ¨Robbins-Monroæ­¥é•¿è°ƒåº¦ä¸‹ï¼Œå®žçŽ°æ¢¯åº¦èŒƒæ•°å¹³æ–¹æœŸæœ›çš„æžé™ä¸ºé›¶ï¼Œæ¶ˆé™¤å™ªå£°åº•é™ä¾èµ–

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\\|\nabla F(\mathbf{x}^r)\\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

