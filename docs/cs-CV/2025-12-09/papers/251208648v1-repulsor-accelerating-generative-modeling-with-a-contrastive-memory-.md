---
layout: default
title: Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank
---

# Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank

**arXiv**: [2512.08648v1](https://arxiv.org/abs/2512.08648) | [PDF](https://arxiv.org/pdf/2512.08648.pdf)

**ä½œè€…**: Shaofeng Zhang, Xuanqi Chen, Ning Liao, Haoxiang Zhao, Xiaoxing Wang, Haoru Tan, Sitong Wu, Xiaosong Jia, Qi Fan, Junchi Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRepulsoræ¡†æž¶ï¼Œé€šè¿‡å¯¹æ¯”è®°å¿†åº“åŠ é€Ÿç”Ÿæˆæ¨¡åž‹è®­ç»ƒï¼Œæ— éœ€å¤–éƒ¨ç¼–ç å™¨ã€‚**

**å…³é”®è¯**: `ç”Ÿæˆæ¨¡åž‹` `å¯¹æ¯”å­¦ä¹ ` `è®°å¿†åº“æœºåˆ¶` `è®­ç»ƒåŠ é€Ÿ` `å›¾åƒåˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŽ»å™ªç”Ÿæˆæ¨¡åž‹è®­ç»ƒæˆæœ¬é«˜ï¼Œä¾èµ–å¤–éƒ¨ç¼–ç å™¨å¼•å…¥å¼€é”€å’Œé¢†åŸŸåç§»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆåŠ¨æ€æ›´æ–°çš„è®°å¿†åº“æœºåˆ¶ï¼Œè§£è€¦è´Ÿæ ·æœ¬æ•°ä¸Žæ‰¹æ¬¡å¤§å°ï¼Œä½¿ç”¨ä½Žç»´æŠ•å½±å¤´å‡å°‘å¼€é”€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet-256ä¸Šï¼Œ400kæ­¥å†…è¾¾åˆ°FID 2.40ï¼Œæ”¶æ•›æ›´å¿«ï¼ŒæŽ¨ç†æ— é¢å¤–æˆæœ¬ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

