---
layout: default
title: Beyond Real Weights: Hypercomplex Representations for Stable Quantization
---

# Beyond Real Weights: Hypercomplex Representations for Stable Quantization

**arXiv**: [2512.08524v1](https://arxiv.org/abs/2512.08524) | [PDF](https://arxiv.org/pdf/2512.08524.pdf)

**ä½œè€…**: Jawad Ibn Ahad, Maisha Rahman, Amrijit Biswas, Muhammad Rafsan Kabir, Robin Krambroeckers, Sifat Momen, Nabeel Mohammed, Shafin Rahman

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¸è¿›å¼é‡å‚æ•°åŒ–ç­–ç•¥ï¼Œé€šè¿‡PHMå±‚åŽ‹ç¼©å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹ä»¥æå‡æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹` `æ¨¡åž‹åŽ‹ç¼©` `æ¸è¿›å¼é‡å‚æ•°åŒ–` `å‚æ•°åŒ–è¶…å¤æ•°ä¹˜æ³•` `çŸ¥è¯†è’¸é¦` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹å‚æ•°åºžå¤§ï¼Œéƒ¨ç½²å›°éš¾ï¼Œéœ€é«˜æ•ˆåŽ‹ç¼©æ–¹æ³•ã€‚
2. æ¸è¿›æ›¿æ¢å¯†é›†å‰é¦ˆç½‘ç»œä¸ºPHMå±‚ï¼Œç»“åˆæ®‹å·®æ’å€¼å’Œè½»é‡æŸå¤±ä¿æŒåŠŸèƒ½ã€‚
3. åœ¨å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡åž‹ä¸ŠéªŒè¯ï¼Œä¿æŒæ€§èƒ½åŒæ—¶æ˜¾è‘—å‡å°‘å‚æ•°å’ŒæŽ¨ç†å»¶è¿Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

