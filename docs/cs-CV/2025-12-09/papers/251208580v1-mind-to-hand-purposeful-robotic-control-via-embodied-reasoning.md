---
layout: default
title: Mind to Hand: Purposeful Robotic Control via Embodied Reasoning
---

# Mind to Hand: Purposeful Robotic Control via Embodied Reasoning

**arXiv**: [2512.08580v1](https://arxiv.org/abs/2512.08580) | [PDF](https://arxiv.org/pdf/2512.08580.pdf)

**ä½œè€…**: Peijun Tang, Shangjin Xie, Binyan Sun, Baifu Huang, Kuncheng Luo, Haotian Yang, Weiqi Jin, Jianan Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLumo-1æ¨¡åž‹ï¼Œé€šè¿‡ä¸‰é˜¶æ®µé¢„è®­ç»ƒç»Ÿä¸€æœºå™¨äººæŽ¨ç†ä¸ŽåŠ¨ä½œï¼Œä»¥è§£å†³AIç³»ç»Ÿåœ¨ç‰©ç†åŠ¨ä½œä¸­çš„æŽ¨ç†è½åœ°æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `å…·èº«æŽ¨ç†` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æœºå™¨äººæŽ§åˆ¶` `ä¸‰é˜¶æ®µé¢„è®­ç»ƒ` `é•¿æ—¶ç¨‹ä»»åŠ¡` `è·¨å…·èº«æ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIç³»ç»Ÿè™½å…·å¤‡å¹¿æ³›æŽ¨ç†èƒ½åŠ›ï¼Œä½†éš¾ä»¥åœ¨ç‰©ç†åŠ¨ä½œä¸­æœ‰æ•ˆè½åœ°ï¼Œæœºå™¨äººæŽ§åˆ¶éœ€ç»“åˆä¸Šä¸‹æ–‡ä¸Žæ„å›¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œåˆ†ä¸‰é˜¶æ®µæ‰©å±•è‡³å…·èº«æŽ¨ç†ä¸ŽåŠ¨ä½œé¢„æµ‹ï¼ŒåŒ…æ‹¬å¢žå¼ºæŽ¨ç†æŠ€èƒ½ã€è·¨å…·èº«æ•°æ®ååŒè®­ç»ƒå’ŒåŠ¨ä½œè®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šLumo-1åœ¨å…·èº«è§†è§‰è¯­è¨€æŽ¨ç†ä¸­è¡¨çŽ°æ˜¾è‘—æå‡ï¼Œåœ¨çœŸå®žä¸–ç•Œæœºå™¨äººä»»åŠ¡ä¸­è¶…è¶ŠåŸºçº¿ï¼Œå°¤å…¶åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡å’Œè‡ªç„¶æŒ‡ä»¤å“åº”ä¸­è¡¨çŽ°å‡ºè‰²ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.

