---
layout: default
title: HybridToken-VLM: Hybrid Token Compression for Vision-Language Models
---

# HybridToken-VLM: Hybrid Token Compression for Vision-Language Models

**arXiv**: [2512.08240v1](https://arxiv.org/abs/2512.08240) | [PDF](https://arxiv.org/pdf/2512.08240.pdf)

**ä½œè€…**: Jusheng Zhang, Xiaoyang Guo, Kaitong Cai, Qinhan Lv, Yijia Fan, Wenhao Chai, Jian Wang, Keze Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆä»¤ç‰ŒåŽ‹ç¼©æ¡†æž¶HTC-VLMï¼Œé€šè¿‡åŒé€šé“è§£è€¦è¯­ä¹‰ä¸Žå¤–è§‚ï¼Œè§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹çš„è®¡ç®—æ•ˆçŽ‡ä¸Žè¡¨ç¤ºä¿çœŸåº¦å›°å¢ƒã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä»¤ç‰ŒåŽ‹ç¼©` `æ··åˆè¡¨ç¤º` `è®¡ç®—æ•ˆçŽ‡` `å¤šæ¨¡æ€æŽ¨ç†` `è§£è€¦æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­å¤§é‡è§†è§‰è¡¥ä¸ä»¤ç‰Œå¯¼è‡´äºŒæ¬¡è®¡ç®—æˆæœ¬ï¼Œä¼ ç»Ÿè¿žç»­åŽ‹ç¼©ä¸Žç¦»æ•£é‡åŒ–æ–¹æ³•å„æœ‰ç¼ºé™·ã€‚
2. é‡‡ç”¨åŒé€šé“æ··åˆè®¾è®¡ï¼šè¿žç»­è·¯å¾„ä¿ç•™ç»†ç²’åº¦ç»†èŠ‚ï¼Œç¦»æ•£è·¯å¾„é€šè¿‡MGVQé‡åŒ–ç”Ÿæˆç¬¦å·é”šç‚¹ï¼ŒèžåˆåŽåŽ‹ç¼©ä¸ºå•ä¸€ä»¤ç‰Œã€‚
3. åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½ä¿æŒ87.2%ï¼Œä»¥580:1åŽ‹ç¼©æ¯”ä¼˜äºŽè¿žç»­åŸºçº¿ï¼Œæ³¨æ„åŠ›åˆ†æžéªŒè¯è¯­ä¹‰å¼•å¯¼æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

