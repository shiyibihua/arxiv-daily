---
layout: default
title: rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection
---

# rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection

**arXiv**: [2512.08300v1](https://arxiv.org/abs/2512.08300) | [PDF](https://arxiv.org/pdf/2512.08300.pdf)

**ä½œè€…**: Sijia Chen, Baochun Li, Di Niu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–ç­–ç•¥æ³¨å…¥æœºåˆ¶ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æŽ¨ç†è¯­è¨€æ¨¡åž‹` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `ç­–ç•¥æ³¨å…¥` `é“¾å¼æ€è€ƒ` `é€šç”¨è§„åˆ’å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨æŽ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹ç­–ç•¥æ€§æ€è€ƒï¼Œå¦‚è‡ªæˆ‘åæ€å’Œæ·±åº¦æ€è€ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å°åž‹è§„åˆ’å™¨ä½œä¸ºé¢†å¯¼è€…æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªé€‚åº”æ³¨å…¥æŽ¨ç†ç­–ç•¥åˆ°é“¾å¼æ€è€ƒä¸­ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šrSIMä½¿å°æ¨¡åž‹æ€§èƒ½è¶…è¶Šå¤§æ¨¡åž‹ï¼Œè§„åˆ’å™¨å¯é€šç”¨åŒ–å¹¶æ”¯æŒæŒç»­å­¦ä¹ ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

