---
layout: default
title: SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking
---

# SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking

**arXiv**: [2512.08518v1](https://arxiv.org/abs/2512.08518) | [PDF](https://arxiv.org/pdf/2512.08518.pdf)

**ä½œè€…**: Nadezhda Kushina, Ko Watanabe, Aarthi Kannan, Ashita Ashok, Andreas Dengel, Karsten Berns

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽçœ¼åŠ¨è¿½è¸ªçš„èˆ’é€‚äººæœºè·ç¦»æ„ŸçŸ¥æ–¹æ³•ï¼Œä»¥æå‡ç¤¾äº¤æœºå™¨äººäº¤äº’ä½“éªŒã€‚**

**å…³é”®è¯**: `äººæœºäº¤äº’` `çœ¼åŠ¨è¿½è¸ª` `è·ç¦»æ„ŸçŸ¥` `æœºå™¨å­¦ä¹ ` `èˆ’é€‚åº¦è¯„ä¼°` `ç¤¾äº¤æœºå™¨äºº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¤¾äº¤æœºå™¨äººéœ€é€‚åº”äººç±»è·ç¦»è§„èŒƒä»¥ç¡®ä¿ç”¨æˆ·èˆ’é€‚ï¼Œä½†çœ¼åŠ¨ç‰¹å¾åœ¨äººæœºäº¤äº’ä¸­çš„é€‚ç”¨æ€§æœªçŸ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç§»åŠ¨çœ¼åŠ¨è¿½è¸ªå’Œä¸»è§‚æŠ¥å‘Šï¼Œåœ¨å››ä¸ªè·ç¦»ä¸‹è¯„ä¼°ç”¨æˆ·èˆ’é€‚åº¦ï¼Œå¹¶åŸºäºŽæ³¨è§†ç‰¹å¾è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå†³ç­–æ ‘æ¨¡åž‹è¡¨çŽ°æœ€ä½³ï¼ˆF1åˆ†æ•°0.73ï¼‰ï¼Œæœ€å°çž³å­”ç›´å¾„æ˜¯å…³é”®é¢„æµ‹å› å­ï¼Œè¡¨æ˜Žäººæœºäº¤äº’èˆ’é€‚é˜ˆå€¼ä¸Žäººäººäº¤äº’ä¸åŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.

