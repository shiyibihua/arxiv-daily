---
layout: default
title: Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning
---

# Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning

**arXiv**: [2512.08314v1](https://arxiv.org/abs/2512.08314) | [PDF](https://arxiv.org/pdf/2512.08314.pdf)

**ä½œè€…**: M Yashwanth, Gaurav Kumar Nayak, Harsh Rangwani, Arya Singh, R. Venkatesh Babu, Anirban Chakraborty

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMANæ­£åˆ™åŒ–ä»¥æå‡è”é‚¦å­¦ä¹ ä¸­æ¨¡åž‹çš„æ³›åŒ–æ€§èƒ½**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ³›åŒ–æ€§èƒ½` `å¹³å¦æœ€å°å€¼` `Hessianç‰¹å¾å€¼` `æ¿€æ´»èŒƒæ•°æ­£åˆ™åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è”é‚¦å­¦ä¹ æ˜“æ”¶æ•›è‡³å°–é”æœ€å°å€¼ï¼ŒæŸå®³æ¨¡åž‹æ³›åŒ–èƒ½åŠ›
2. é€šè¿‡æœ€å°åŒ–å±‚æ¿€æ´»èŒƒæ•°çº¦æŸHessianç‰¹å¾å€¼ï¼Œç¡®ä¿å¹³å¦æœ€å°å€¼
3. åœ¨çŽ°æœ‰FLæŠ€æœ¯ä¸Šåº”ç”¨ï¼Œæ˜¾è‘—æå‡æ€§èƒ½ï¼Œè¾¾åˆ°æ–°SOTA

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

