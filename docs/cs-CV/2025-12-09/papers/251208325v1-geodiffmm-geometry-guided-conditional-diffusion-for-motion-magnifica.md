---
layout: default
title: GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification
---

# GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification

**arXiv**: [2512.08325v1](https://arxiv.org/abs/2512.08325) | [PDF](https://arxiv.org/pdf/2512.08325.pdf)

**ä½œè€…**: Xuedeng Liu, Jiabao Guo, Zheng Zhang, Fei Wang, Zhi Liu, Dan Guo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGeoDiffMMï¼Œä¸€ç§åŸºäºŽæ‰©æ•£çš„æ‹‰æ ¼æœ—æ—¥è§†é¢‘è¿åŠ¨æ”¾å¤§æ¡†æž¶ï¼Œåˆ©ç”¨å…‰æµä½œä¸ºå‡ ä½•å…ˆéªŒä»¥æå‡æ”¾å¤§æ•ˆæžœã€‚**

**å…³é”®è¯**: `è§†é¢‘è¿åŠ¨æ”¾å¤§` `æ‰©æ•£æ¨¡åž‹` `å…‰æµå¼•å¯¼` `æ‹‰æ ¼æœ—æ—¥æ–¹æ³•` `å‡ ä½•å…ˆéªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¬§æ‹‰æ–¹æ³•åœ¨å¾®å°ä½ç§»ä¸‹éš¾ä»¥åˆ†ç¦»å…‰å­å™ªå£°ä¸ŽçœŸå®žå¾®è¿åŠ¨ï¼Œå¯¼è‡´æ”¾å¤§å™ªå£°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ— å™ªå£°å…‰æµå¢žå¼ºç­–ç•¥å’Œæ‰©æ•£è¿åŠ¨æ”¾å¤§å™¨ï¼Œä»¥å…‰æµä¸ºæ¡ä»¶é€‰æ‹©æ€§æ”¾å¤§ç»“æž„ä¸€è‡´çš„è¿åŠ¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žå’Œåˆæˆæ•°æ®é›†ä¸Šä¼˜äºŽå…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—æ”¹å–„è¿åŠ¨æ”¾å¤§è´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

