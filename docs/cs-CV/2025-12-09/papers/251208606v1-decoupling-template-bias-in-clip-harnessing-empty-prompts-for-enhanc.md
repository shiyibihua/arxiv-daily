---
layout: default
title: Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning
---

# Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning

**arXiv**: [2512.08606v1](https://arxiv.org/abs/2512.08606) | [PDF](https://arxiv.org/pdf/2512.08606.pdf)

**ä½œè€…**: Zhenyu Zhang, Guangyao Chen, Yixiong Zou, Zhimeng Huang, Yuhua Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½¿ç”¨ç©ºæç¤ºæ¡†æž¶ä»¥è§£å†³CLIPä¸­æ¨¡æ¿-æ ·æœ¬ç›¸ä¼¼æ€§åå·®ï¼Œæå‡å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½ã€‚**

**å…³é”®è¯**: `CLIPæ¨¡åž‹` `å°‘æ ·æœ¬å­¦ä¹ ` `æ¨¡æ¿åå·®` `ç©ºæç¤º` `å¯¹æ¯”å­¦ä¹ ` `åˆ†ç±»é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPä¸­æ¨¡æ¿-æ ·æœ¬ç›¸ä¼¼æ€§å¯¼è‡´æ¨¡åž‹ä¾èµ–æ¨¡æ¿è€ŒéžçœŸå®žç±»åˆ«å¯¹é½ï¼Œé™ä½Žåˆ†ç±»å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ç©ºæç¤ºæ•èŽ·æ— åæ¨¡æ¿ç‰¹å¾ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œå°‘æ ·æœ¬å¾®è°ƒä¸¤é˜¶æ®µæ¡†æž¶æ ¡å‡†åå·®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘æ€§èƒ½æ³¢åŠ¨ï¼Œæé«˜åˆ†ç±»å‡†ç¡®çŽ‡å’Œé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

