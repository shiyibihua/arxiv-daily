---
layout: default
title: LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training
---

# LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training

**arXiv**: [2512.08439v1](https://arxiv.org/abs/2512.08439) | [PDF](https://arxiv.org/pdf/2512.08439.pdf)

**ä½œè€…**: Qing Xu, Kun Yuan, Yuxiang Luo, Yuhao Zhai, Wenting Duan, Nassir Navab, Zhen Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLapFMåŸºç¡€æ¨¡åž‹ï¼Œé€šè¿‡åˆ†å±‚æ¦‚å¿µæ¼”åŒ–é¢„è®­ç»ƒè§£å†³è…¹è…”é•œåˆ†å‰²ä¸­æ ‡æ³¨ç¨€ç¼ºä¸Žè¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ã€‚**

**å…³é”®è¯**: `è…¹è…”é•œåˆ†å‰²` `åŸºç¡€æ¨¡åž‹` `åˆ†å±‚æ¦‚å¿µæ¼”åŒ–` `ä¼ªæ ‡ç­¾ç”Ÿæˆ` `æ— ç›‘ç£é¢„è®­ç»ƒ` `æ‰‹æœ¯åœºæ™¯ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹æœ¯åˆ†å‰²å› æ ‡æ³¨ç¨€ç¼ºå’Œè·¨ç¨‹åºè¯­ä¹‰ä¸ä¸€è‡´è€Œå—é™ï¼ŒçŽ°æœ‰æ–¹æ³•æ³›åŒ–èƒ½åŠ›å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚æ¦‚å¿µæ¼”åŒ–é¢„è®­ç»ƒï¼Œæž„å»ºè…¹è…”é•œæ¦‚å¿µå±‚æ¬¡å¹¶è¿­ä»£ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»Žæ— æ ‡æ³¨å›¾åƒä¸­å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LapBench-114KåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºŽå…ˆè¿›æ–¹æ³•ï¼Œå®žçŽ°ç²’åº¦è‡ªé€‚åº”æ³›åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

