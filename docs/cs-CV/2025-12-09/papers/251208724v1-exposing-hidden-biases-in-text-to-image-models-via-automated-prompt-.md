---
layout: default
title: Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search
---

# Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search

**arXiv**: [2512.08724v1](https://arxiv.org/abs/2512.08724) | [PDF](https://arxiv.org/pdf/2512.08724.pdf)

**ä½œè€…**: Manos Plitsis, Giorgos Bouritsas, Vassilis Katsouros, Yannis Panagakis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBias-Guided Prompt Searchæ¡†æž¶ï¼Œé€šè¿‡è‡ªåŠ¨æç¤ºæœç´¢æš´éœ²æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹çš„éšè—åè§**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹` `åè§æ£€æµ‹` `æç¤ºæœç´¢` `å…¬å¹³æ€§è¯„ä¼°` `æ‰©æ•£æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æˆ–LLMæž„å»ºæç¤ºæ•°æ®é›†ï¼Œå¯èƒ½å¿½ç•¥è§¦å‘åè§çš„æœªé¢„æœŸæç¤ºï¼Œå½±å“åè§ç¼“è§£æ•ˆæžœã€‚
2. æ–¹æ³•ï¼šç»“åˆLLMç”Ÿæˆå±žæ€§ä¸­æ€§æç¤ºå’Œå±žæ€§åˆ†ç±»å™¨å¼•å¯¼è§£ç ï¼Œè‡ªåŠ¨æœç´¢æ”¾å¤§å›¾åƒåè§çš„æç¤ºã€‚
3. å®žéªŒï¼šåœ¨Stable Diffusion 1.5å’ŒåŽ»åè§æ¨¡åž‹ä¸Šå‘çŽ°æ–°åè§ï¼Œæç¤ºå¯è§£é‡Šä¸”æå‡å›°æƒ‘åº¦æŒ‡æ ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

