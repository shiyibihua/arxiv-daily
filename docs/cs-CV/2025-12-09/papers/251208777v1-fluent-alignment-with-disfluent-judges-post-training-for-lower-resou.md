---
layout: default
title: Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages
---

# Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages

**arXiv**: [2512.08777v1](https://arxiv.org/abs/2512.08777) | [PDF](https://arxiv.org/pdf/2512.08777.pdf)

**‰ΩúËÄÖ**: David Samuel, Lilja √òvrelid, Erik Velldal, Andrey Kutuzov

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂêéËÆ≠ÁªÉÊñπÊ≥ï‰ª•Âú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏≠ÂÆûÁé∞ÊµÅÁïÖÂÅèÂ•ΩÂØπÈΩêÔºåÊó†ÈúÄÁõÆÊ†áËØ≠Ë®ÄÊåá‰ª§Êï∞ÊçÆ„ÄÇ**

**ÂÖ≥ÈîÆËØç**: `‰ΩéËµÑÊ∫êËØ≠Ë®Ä` `ÂÅèÂ•ΩÂØπÈΩê` `ÂêéËÆ≠ÁªÉ` `Âú®Á∫øÁ≠ñÁï•ËÆ≠ÁªÉ` `ÊµÅÁïÖÊÄßËØÑ‰º∞` `ËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºö‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁº∫‰πèÊØçËØ≠Êï∞ÊçÆÈõÜÂíåÊµÅÁïÖÁîüÊàêÊ®°ÂûãÔºåÂÅèÂ•ΩÂØπÈΩêÊòìÂèó‰∏çÊµÅÁïÖÂ•ñÂä±Ê®°ÂûãÂΩ±Âìç„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÈááÁî®Âú®Á∫øÁ≠ñÁï•ËÆ≠ÁªÉÔºåÈÅøÂÖç‰æùËµñÊú∫Âô®ÁøªËØëÊàñÂ§öËØ≠Ë®ÄÂæÆË∞ÉÔºåÊó†ÈúÄÁõÆÊ†áËØ≠Ë®ÄÊåá‰ª§Êï∞ÊçÆ„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºö‰ª•Êå™Â®ÅËØ≠‰∏∫‰æãÔºåÈÄöËøáÊØçËØ≠ËÄÖËØÑ‰º∞ÔºåÂú®Á∫øÁ≠ñÁï•ÊñπÊ≥ï‰ºò‰∫éÊõø‰ª£ÊñπÊ°àÔºåÊèêÂçáÊµÅÁïÖÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokm√•l and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

