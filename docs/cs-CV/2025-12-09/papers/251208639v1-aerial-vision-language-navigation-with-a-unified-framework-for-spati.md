---
layout: default
title: Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning
---

# Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning

**arXiv**: [2512.08639v1](https://arxiv.org/abs/2512.08639) | [PDF](https://arxiv.org/pdf/2512.08639.pdf)

**ä½œè€…**: Huilin Xu, Zhuoyang Liu, Yixiang Luomei, Feng Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æž¶ï¼Œä»…ç”¨å•ç›®RGBå’Œè¯­è¨€æŒ‡ä»¤å®žçŽ°æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆªï¼Œä¼˜åŒ–ç©ºé—´ã€æ—¶é—´å’Œå…·èº«æŽ¨ç†ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `æ— äººæœºå¯¼èˆª` `å¤šä»»åŠ¡å­¦ä¹ ` `å•ç›®è§†è§‰` `å…·èº«æŽ¨ç†` `å…³é”®å¸§é€‰æ‹©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–å…¨æ™¯ã€æ·±åº¦æˆ–é‡Œç¨‹è®¡ï¼Œå¢žåŠ æˆæœ¬å’Œå¤æ‚æ€§ï¼Œé˜»ç¢è½»é‡æ— äººæœºéƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†å¯¼èˆªå»ºæ¨¡ä¸ºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ï¼Œé€šè¿‡æç¤ºå¼•å¯¼å¤šä»»åŠ¡å­¦ä¹ è”åˆä¼˜åŒ–æ„ŸçŸ¥ã€æŽ¨ç†å’ŒåŠ¨ä½œé¢„æµ‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Aerial VLNåŸºå‡†ä¸ŠéªŒè¯ï¼Œå•ç›®RGBè®¾ç½®ä¸‹æ€§èƒ½ä¼˜äºŽçŽ°æœ‰RGBåŸºçº¿ï¼ŒæŽ¥è¿‘å…¨æ™¯RGB-Dæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

