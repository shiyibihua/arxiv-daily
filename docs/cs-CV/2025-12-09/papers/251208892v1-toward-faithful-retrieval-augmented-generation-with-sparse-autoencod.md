---
layout: default
title: Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders
---

# Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders

**arXiv**: [2512.08892v1](https://arxiv.org/abs/2512.08892) | [PDF](https://arxiv.org/pdf/2512.08892.pdf)

**ä½œè€…**: Guangzhi Xiong, Zhenghao He, Bohan Liu, Sanchit Sinha, Aidong Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAGLensï¼ŒåŸºäºŽç¨€ç–è‡ªç¼–ç å™¨æ£€æµ‹æ£€ç´¢å¢žå¼ºç”Ÿæˆä¸­çš„ä¸å¿ å®žè¾“å‡ºã€‚**

**å…³é”®è¯**: `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `ç¨€ç–è‡ªç¼–ç å™¨` `å¹»è§‰æ£€æµ‹` `å¯è§£é‡Šæ€§` `è½»é‡çº§æ£€æµ‹å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ£€ç´¢å¢žå¼ºç”Ÿæˆå­˜åœ¨ä¸å¿ å®žè¾“å‡ºï¼ŒçŽ°æœ‰æ£€æµ‹æ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®æˆ–é«˜æŽ¨ç†æˆæœ¬ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨è§£è€¦å†…éƒ¨æ¿€æ´»ï¼Œé€šè¿‡ä¿¡æ¯ç‰¹å¾é€‰æ‹©å’ŒåŠ æ€§å»ºæ¨¡æž„å»ºè½»é‡çº§æ£€æµ‹å™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šRAGLensæ£€æµ‹æ€§èƒ½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæä¾›å¯è§£é‡Šç†ç”±ï¼Œå¹¶æ­ç¤ºå¹»è§‰ä¿¡å·åˆ†å¸ƒæ–°è§è§£ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

