---
layout: default
title: SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality
---

# SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality

**arXiv**: [2512.08238v1](https://arxiv.org/abs/2512.08238) | [PDF](https://arxiv.org/pdf/2512.08238.pdf)

**ä½œè€…**: Mahathir Monjur, Shahriar Nirjon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpeechQualityLLMï¼ŒåŸºäºŽLLMçš„å¤šæ¨¡æ€è¯­éŸ³è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ä»¥ä¼˜åŒ–è¯­éŸ³é€šä¿¡è´¨é‡ç›‘æŽ§ã€‚**

**å…³é”®è¯**: `è¯­éŸ³è´¨é‡è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­è¨€æ¨¡åž‹` `è‡ªç„¶è¯­è¨€æŽ¥å£` `ä¸»è§‚è¯„åˆ†é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿè¯­éŸ³è´¨é‡è¯„ä¼°æ–¹æ³•å¦‚PESQå’ŒPOLQAä¾èµ–å—æŽ§æ¡ä»¶å’Œé«˜æˆæœ¬ä¸»è§‚æµ‹è¯•ï¼Œè€Œå­¦ä¹ æ¨¡åž‹å¦‚NISQAç¼ºä¹äº¤äº’æ€§å’Œæ–‡æœ¬è§£é‡Šèƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆéŸ³é¢‘ç¼–ç å™¨å’Œè¯­è¨€æ¨¡åž‹ï¼Œåœ¨NISQAè¯­æ–™ä¸Šè®­ç»ƒï¼Œé€šè¿‡æ¨¡æ¿é—®ç­”å¯¹è¦†ç›–MOSå’Œå››ä¸ªæ„ŸçŸ¥ç»´åº¦ï¼Œæ”¯æŒå•ç«¯å’ŒåŒç«¯è®¾ç½®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨NISQAæµ‹è¯•é›†ä¸Šï¼ŒåŒç«¯æ¨¡åž‹MOSå¹³å‡ç»å¯¹è¯¯å·®ä¸º0.41ï¼Œçš®å°”é€Šç›¸å…³ç³»æ•°0.86ï¼Œå¹¶æä¾›çµæ´»çš„è‡ªç„¶è¯­è¨€æŽ¥å£ä»¥æ¨¡æ‹Ÿä¸åŒå¬ä¼—å’Œç”Ÿæˆå¤šæ ·åŒ–åˆ¤æ–­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.

