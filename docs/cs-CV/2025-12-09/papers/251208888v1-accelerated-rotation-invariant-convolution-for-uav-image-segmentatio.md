---
layout: default
title: Accelerated Rotation-Invariant Convolution for UAV Image Segmentation
---

# Accelerated Rotation-Invariant Convolution for UAV Image Segmentation

**arXiv**: [2512.08888v1](https://arxiv.org/abs/2512.08888) | [PDF](https://arxiv.org/pdf/2512.08888.pdf)

**ä½œè€…**: Manduhu Manduhu, Alexander Dow, Gerard Dooly, James Riordan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPUä¼˜åŒ–çš„æ—‹è½¬ä¸å˜å·ç§¯æ¡†æž¶ï¼Œä»¥é«˜æ•ˆå¤„ç†æ— äººæœºå›¾åƒåˆ†å‰²ä¸­çš„ä»»æ„æ–¹å‘ç›®æ ‡ã€‚**

**å…³é”®è¯**: `æ—‹è½¬ä¸å˜å·ç§¯` `æ— äººæœºå›¾åƒåˆ†å‰²` `GPUä¼˜åŒ–` `è®¡ç®—æ•ˆçŽ‡` `å†…å­˜ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå·ç§¯åœ¨æ— äººæœºå›¾åƒåˆ†å‰²ä¸­ç¼ºä¹æ—‹è½¬ä¸å˜æ€§ï¼Œå¯¼è‡´ç²¾åº¦ä¸‹é™ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç»“æž„åŒ–æ•°æ®å…±äº«æ¶ˆé™¤im2colæ­¥éª¤ï¼Œå‡å°‘å†…å­˜æµé‡å’Œè®¡ç®—å†—ä½™ï¼Œæ”¯æŒå¤šæ–¹å‘å’Œä»»æ„è§’åº¦å·ç§¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”CUDNNï¼Œè®­ç»ƒé€Ÿåº¦æå‡20-55%ï¼Œèƒ½è€—é™ä½Ž15-45%ï¼Œåœ¨U-Netä¸­ç²¾åº¦æå‡è¾¾6%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
>   Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.

