---
layout: default
title: No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers
---

# No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers

**arXiv**: [2512.08889v1](https://arxiv.org/abs/2512.08889) | [PDF](https://arxiv.org/pdf/2512.08889.pdf)

**ä½œè€…**: Damiano Marsili, Georgia Gkioxari

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— æ ‡æ³¨è®­ç»ƒæ¡†æž¶ï¼Œç»“åˆå¤šæ¨¡æ€éªŒè¯å™¨æå‡è§†è§‰æŽ¨ç†ä¸Žå®šä½èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰æŽ¨ç†` `æ— æ ‡æ³¨è®­ç»ƒ` `å¤šæ¨¡æ€éªŒè¯å™¨` `å¼ºåŒ–å­¦ä¹ ` `ç¡¬è´Ÿæ ·æœ¬æŒ–æŽ˜` `ç©ºé—´å…³ç³»ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰æŽ¨ç†éœ€ç²¾ç¡®å¯¹è±¡å®šä½ä¸Žå¤æ‚ç©ºé—´å…³ç³»ç†è§£ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æˆ–å­˜åœ¨é€»è¾‘é”™è¯¯
2. æ¡†æž¶ä½¿ç”¨LLMéªŒè¯å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æŽ¨ç†ï¼ŒVLMéªŒè¯å™¨é€šè¿‡è‡ªåŠ¨ç¡¬è´Ÿæ ·æœ¬æŒ–æŽ˜å¢žå¼ºè§†è§‰å®šä½
3. åœ¨å¤šæ ·ç©ºé—´æŽ¨ç†ä»»åŠ¡ä¸­è¯„ä¼°ï¼Œæ–¹æ³•è¶…è¶Šå¼€æºä¸Žä¸“æœ‰æ¨¡åž‹ï¼Œæ”¹è¿›å®šä½æ¨¡åž‹ä¼˜äºŽçº¯æ–‡æœ¬æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

