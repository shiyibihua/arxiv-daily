---
layout: default
title: Correction of Decoupled Weight Decay
---

# Correction of Decoupled Weight Decay

**arXiv**: [2512.08217v1](https://arxiv.org/abs/2512.08217) | [PDF](https://arxiv.org/pdf/2512.08217.pdf)

**ä½œè€…**: Jason Chuan-Chih Chou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç¨³æ€ç‹¬ç«‹å‡è®¾çš„æƒé‡è¡°å‡æ¯”ä¾‹ä¿®æ­£ï¼Œä»¥ç¨³å®šè®­ç»ƒåŠ¨æ€å¹¶æå‡æ¨¡åž‹æ€§èƒ½**

**å…³é”®è¯**: `æƒé‡è¡°å‡` `ä¼˜åŒ–å™¨` `è®­ç»ƒåŠ¨æ€` `ç¨³æ€åˆ†æž` `AdamW` `Scionä¼˜åŒ–å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§£è€¦æƒé‡è¡°å‡é•¿æœŸè®¾ä¸ºä¸Žå­¦ä¹ çŽ‡Î³æˆæ­£æ¯”ï¼Œä½†è¿‘æœŸæœ‰ç ”ç©¶ä¸»å¼ åº”è®¾ä¸ºÎ³Â²æ¯”ä¾‹
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç¨³æ€ä¸‹æ›´æ–°ä¸Žæƒé‡ç‹¬ç«‹çš„å‡è®¾ï¼ŒæŽ¨å¯¼å‡ºè§£è€¦æƒé‡è¡°å‡âˆÎ³Â²å¯ç¨³å®šæƒé‡èŒƒæ•°
3. å®žéªŒæˆ–æ•ˆæžœï¼šç»éªŒéªŒè¯è¯¥æ¯”ä¾‹èƒ½ç¨³å®šæƒé‡å’Œæ¢¯åº¦èŒƒæ•°ï¼Œæ›´å¥½æŽ§åˆ¶è®­ç»ƒåŠ¨æ€å¹¶æ”¹å–„æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $Î³$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto Î³^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto Î³^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto Î³^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

