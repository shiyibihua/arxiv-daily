---
layout: default
title: The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss
---

# The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss

**arXiv**: [2512.08374v1](https://arxiv.org/abs/2512.08374) | [PDF](https://arxiv.org/pdf/2512.08374.pdf)

**ä½œè€…**: Bozhou Li, Xinda Xue, Sihan Yang, Yang Shi, Xinlong Chen, Yushuo Guan, Yuanxing Zhang, Wentao Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨è§†è§‰æŠ•å½±å™¨åŽæ’å…¥LayerNormå±‚ä»¥è§£å†³MLLMsä¸­è§†è§‰ä¸Žæ–‡æœ¬ä»¤ç‰ŒèŒƒæ•°å·®å¼‚å¯¼è‡´çš„è·¨æ¨¡æ€èžåˆé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `èŒƒæ•°å·®å¼‚` `è·¨æ¨¡æ€èžåˆ` `LayerNorm` `è§†è§‰ä¿¡æ¯æŸå¤±` `ä¸å¯¹ç§°æ›´æ–°åŠ¨æ€`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šPre-Normæž¶æž„å¯¼è‡´è§†è§‰ä»¤ç‰Œé«˜èŒƒæ•°ä¸Žæ–‡æœ¬ä»¤ç‰Œä½ŽèŒƒæ•°å·®å¼‚ï¼Œå¼•å‘ä¸å¯¹ç§°æ›´æ–°åŠ¨æ€ï¼ŒæŸå®³è·¨æ¨¡æ€ç‰¹å¾èžåˆ
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨è§†è§‰æŠ•å½±å™¨åŽæ·»åŠ å•ä¸ªLayerNormå±‚ï¼Œå¼ºåˆ¶å¯¹é½è§†è§‰ä¸Žæ–‡æœ¬ä»¤ç‰ŒèŒƒæ•°ï¼Œå®žçŽ°ç®€å•æœ‰æ•ˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLaVA-1.5æž¶æž„ä¸ŠéªŒè¯ï¼Œå¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬åŸºå‡†ï¼ˆå¦‚MMLUï¼‰æ€§èƒ½æ˜¾è‘—æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

