---
layout: default
title: Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis
---

# Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis

**arXiv**: [2512.08601v1](https://arxiv.org/abs/2512.08601) | [PDF](https://arxiv.org/pdf/2512.08601.pdf)

**ä½œè€…**: Orit Davidovich, Shimrit Shtern, Segev Wasserkrug, Nimrod Megiddo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå€¼å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æž¶ä¸Žåˆ†æžï¼Œç”¨äºŽç»„åˆä¼˜åŒ–å¯å‘å¼å­¦ä¹ **

**å…³é”®è¯**: `ç»„åˆä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å€¼å‡½æ•°å­¦ä¹ ` `æœ€ä¼˜æ€§é—´éš™` `ç†è®ºåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»„åˆä¼˜åŒ–å¯å‘å¼å­¦ä¹ ç¼ºä¹ç†è®ºæ”¯æ’‘ï¼Œéœ€ç»Ÿä¸€å»ºæ¨¡ä¸Žåˆ†æž
2. æ–¹æ³•è¦ç‚¹ï¼šå°†ç»„åˆä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºæ— æŠ˜æ‰£é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæä¾›æ”¶æ•›æ¡ä»¶ä¸Žæœ€ä¼˜æ€§é—´éš™ä¿è¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šåˆ†æžæ·±åº¦Qå­¦ä¹ ç®—æ³•çš„æˆåŠŸä¸Žå±€é™ï¼Œä¸ºå®žé™…åº”ç”¨æä¾›ç†è®ºæŒ‡å¯¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.

