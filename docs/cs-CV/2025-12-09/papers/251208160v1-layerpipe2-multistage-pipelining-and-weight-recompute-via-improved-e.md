---
layout: default
title: LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks
---

# LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks

**arXiv**: [2512.08160v1](https://arxiv.org/abs/2512.08160) | [PDF](https://arxiv.org/pdf/2512.08160.pdf)

**ä½œè€…**: Nanda K. Unnikrishnan, Keshab K. Parhi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLayerPipe2æ¡†æž¶ï¼Œé€šè¿‡å»¶è¿Ÿæ¢¯åº¦åˆ†æžå’Œæƒé‡é‡æž„å®žçŽ°å¯æ‰©å±•çš„ç¥žç»ç½‘ç»œæµæ°´çº¿è®­ç»ƒ**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œè®­ç»ƒ` `æµæ°´çº¿å¹¶è¡Œ` `æ¢¯åº¦å»¶è¿Ÿ` `æƒé‡é‡æž„` `å†…å­˜ä¼˜åŒ–` `å¯æ‰©å±•æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæµæ°´çº¿è®­ç»ƒä¸­æ¢¯åº¦å»¶è¿Ÿçš„é‡åŒ–ä¸ŽåŽ†å²æƒé‡å­˜å‚¨ç“¶é¢ˆ
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç½‘ç»œç»“æž„æŽ¨å¯¼å»¶è¿Ÿéœ€æ±‚ï¼Œå¼€å‘æµæ°´çº¿æ„ŸçŸ¥ç§»åŠ¨å¹³å‡é‡æž„æƒé‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šé™ä½Žå†…å­˜æˆæœ¬ï¼Œä¿æŒç²¾åº¦ï¼Œæ”¯æŒå¯æŽ§è®¡ç®—é€šä¿¡æƒè¡¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

