---
layout: default
title: Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions
---

# Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions

**arXiv**: [2512.08500v1](https://arxiv.org/abs/2512.08500) | [PDF](https://arxiv.org/pdf/2512.08500.pdf)

**ä½œè€…**: Jianan Li, Xiao Chen, Tao Huang, Tien-Tsin Wong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMimic2DMæ¡†æž¶ï¼Œé€šè¿‡ç”Ÿæˆå’Œæ¨¡ä»¿2Dè¿åŠ¨å­¦ä¹ ç‰©ç†æ¨¡æ‹Ÿ3Dè§’è‰²æŽ§åˆ¶ï¼Œæ— éœ€3Dæ•°æ®ã€‚**

**å…³é”®è¯**: `2Dè¿åŠ¨æ¨¡ä»¿` `ç‰©ç†æ¨¡æ‹ŸæŽ§åˆ¶` `å˜æ¢å™¨ç”Ÿæˆ` `åˆ†å±‚æŽ§åˆ¶` `è§†é¢‘æ•°æ®å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žè§†é¢‘å­¦ä¹ 3Dè§’è‰²æŽ§åˆ¶æ—¶ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–3Dè¿åŠ¨é‡å»ºï¼Œæ³›åŒ–æ€§å·®ä¸”éš¾ä»¥å¤„ç†å¤æ‚åœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç›´æŽ¥åˆ©ç”¨2Då…³é”®ç‚¹è½¨è¿¹è®­ç»ƒè·Ÿè¸ªç­–ç•¥ï¼Œç»“åˆå˜æ¢å™¨ç”Ÿæˆ2Då‚è€ƒè¿åŠ¨ï¼Œå®žçŽ°åˆ†å±‚æŽ§åˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨èˆžè¹ˆã€è¶³çƒè¿çƒå’ŒåŠ¨ç‰©è¿åŠ¨ç­‰åœºæ™¯ä¸­åˆæˆç‰©ç†åˆç†ä¸”å¤šæ ·çš„è¿åŠ¨ï¼Œæ— éœ€3Dæ•°æ®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/

