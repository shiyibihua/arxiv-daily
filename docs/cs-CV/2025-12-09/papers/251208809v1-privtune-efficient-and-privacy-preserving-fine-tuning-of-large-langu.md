---
layout: default
title: PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration
---

# PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration

**arXiv**: [2512.08809v1](https://arxiv.org/abs/2512.08809) | [PDF](https://arxiv.org/pdf/2512.08809.pdf)

**ä½œè€…**: Yi Liu, Weixiang Han, Chengjun Cai, Xingliang Yuan, Cong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPrivTuneæ¡†æž¶ï¼Œé€šè¿‡è®¾å¤‡-äº‘åä½œå®žçŽ°å¤§è¯­è¨€æ¨¡åž‹é«˜æ•ˆéšç§ä¿æŠ¤å¾®è°ƒ**

**å…³é”®è¯**: `éšç§ä¿æŠ¤å¾®è°ƒ` `è®¾å¤‡-äº‘åä½œ` `Split Learning` `ä»¤ç‰Œå™ªå£°æ³¨å…¥` `å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å·®åˆ†éšç§æ–¹æ³•åœ¨è®¾å¤‡-äº‘åä½œä¸­éš¾ä»¥å¹³è¡¡éšç§ä¸Žæ•ˆç”¨ï¼Œæ˜“å¯¼è‡´æ•æ„Ÿæ•°æ®æ³„éœ²æˆ–æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽSplit Learningï¼Œå‘åº•å±‚æ¨¡åž‹ä»¤ç‰Œè¡¨ç¤ºæ³¨å…¥ä¼˜åŒ–å™ªå£°ï¼Œä½¿ä»¤ç‰Œç±»ä¼¼é—´æŽ¥é‚»å±…ï¼Œå¹¶è°ƒæ•´å™ªå£°åˆ†å¸ƒå‚æ•°ä»¥æœ€å°åŒ–å¤±çœŸ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå¯¹æŠ—å…­ç§æ”»å‡»ï¼ŒPrivTuneå°†æ”»å‡»æˆåŠŸçŽ‡é™è‡³10%ï¼Œæ•ˆç”¨æ€§èƒ½ä»…ä¸‹é™3.33%ï¼Œä¼˜äºŽåŸºçº¿æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_Ï‡$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.

