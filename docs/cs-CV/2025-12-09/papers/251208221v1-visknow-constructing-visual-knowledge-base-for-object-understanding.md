---
layout: default
title: VisKnow: Constructing Visual Knowledge Base for Object Understanding
---

# VisKnow: Constructing Visual Knowledge Base for Object Understanding

**arXiv**: [2512.08221v1](https://arxiv.org/abs/2512.08221) | [PDF](https://arxiv.org/pdf/2512.08221.pdf)

**ä½œè€…**: Ziwei Yao, Qiyang Wan, Ruiping Wang, Xilin Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisKnowæ¡†æž¶ä»¥æž„å»ºè§†è§‰çŸ¥è¯†åº“ï¼Œæ”¯æŒæ·±åº¦ç‰©ä½“ç†è§£ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `è§†è§‰çŸ¥è¯†åº“` `ç‰©ä½“ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†å›¾è°±` `é›¶æ ·æœ¬è¯†åˆ«` `ç»†ç²’åº¦è§†è§‰é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šæ¨¡æ€æ•°æ®ç¼ºä¹ç³»ç»Ÿç»„ç»‡ï¼Œéš¾ä»¥å®žçŽ°å…¨é¢çš„ç‰©ä½“ç†è§£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆä¸“å®¶è®¾è®¡å’Œå¤§è§„æ¨¡æ¨¡åž‹ï¼Œä»Žæ–‡æœ¬å’Œå›¾åƒä¸­æå–ç‰©ä½“çº§çŸ¥è¯†å¹¶æž„å»ºå›¾ç»“æž„ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºAnimalKBçŸ¥è¯†åº“ï¼Œæå‡é›¶æ ·æœ¬è¯†åˆ«å’Œç»†ç²’åº¦VQAç­‰ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

