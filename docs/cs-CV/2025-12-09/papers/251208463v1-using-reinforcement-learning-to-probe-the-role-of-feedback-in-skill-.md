---
layout: default
title: Using reinforcement learning to probe the role of feedback in skill acquisition
---

# Using reinforcement learning to probe the role of feedback in skill acquisition

**arXiv**: [2512.08463v1](https://arxiv.org/abs/2512.08463) | [PDF](https://arxiv.org/pdf/2512.08463.pdf)

**ä½œè€…**: Antonio Terpin, Raffaello D'Andrea

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¼ºåŒ–å­¦ä¹ æŽ¢ç©¶åé¦ˆåœ¨æŠ€èƒ½èŽ·å–ä¸­çš„ä½œç”¨ï¼Œä½¿ç”¨æ—‹è½¬åœ†æŸ±ä½“å®žéªŒéªŒè¯åé¦ˆå¯¹å­¦ä¹ ä¸Žæ‰§è¡Œçš„å½±å“å·®å¼‚ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æŠ€èƒ½èŽ·å–` `åé¦ˆæœºåˆ¶` `ç‰©ç†ç³»ç»Ÿå®žéªŒ` `æ‹–æ›³åŠ›æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶æŠ€èƒ½èŽ·å–ä¸­åé¦ˆçš„ä½œç”¨ï¼Œä»¥ç‰©ç†ç³»ç»Ÿæ›¿ä»£äººç±»å®žéªŒï¼Œèšç„¦äºŽæ‹–æ›³åŠ›æŽ§åˆ¶ä»»åŠ¡ã€‚
2. é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸Žæ—‹è½¬åœ†æŸ±ä½“äº¤äº’ï¼Œåˆ©ç”¨é«˜ç»´æµåé¦ˆå¿«é€Ÿå‘çŽ°é«˜æ€§èƒ½ç­–ç•¥ã€‚
3. å®žéªŒæ˜¾ç¤ºå­¦ä¹ éœ€è¦åé¦ˆï¼Œä½†æ‰§è¡Œæ— éœ€åé¦ˆï¼Œä¸”å­¦ä¹ æ•ˆæžœå—ç›®æ ‡ï¼ˆæœ€å°åŒ–æˆ–æœ€å¤§åŒ–æ‹–æ›³åŠ›ï¼‰å½±å“æ˜¾è‘—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

