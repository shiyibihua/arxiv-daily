---
layout: default
title: Ergodic Trajectory Planning with Dynamic Sensor Footprints
---

# Ergodic Trajectory Planning with Dynamic Sensor Footprints

**arXiv**: [2512.08661v1](https://arxiv.org/abs/2512.08661) | [PDF](https://arxiv.org/pdf/2512.08661.pdf)

**ä½œè€…**: Ziyue Zheng, Yongce Liu, Hesheng Wang, Zhongqiang Ren

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€ä¼ æ„Ÿå™¨è¶³è¿¹çš„éåŽ†è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–ä¿¡æ¯é‡‡é›†ä»»åŠ¡**

**å…³é”®è¯**: `éåŽ†è§„åˆ’` `åŠ¨æ€ä¼ æ„Ÿå™¨è¶³è¿¹` `è½¨è¿¹ä¼˜åŒ–` `ä¿¡æ¯é‡‡é›†` `å¤šæ— äººæœºç³»ç»Ÿ` `3Dè¦†ç›–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰éåŽ†è§„åˆ’å‡è®¾ä¼ æ„Ÿå™¨è¶³è¿¹å›ºå®šï¼Œå¿½ç•¥åŠ¨æ€å˜åŒ–å¦‚æ— äººæœºç›¸æœºè§†è§’éšå§¿æ€å’Œé«˜åº¦å˜åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ–°åº¦é‡è€ƒè™‘åŠ¨æ€è¶³è¿¹ï¼Œåˆ†æžå±€éƒ¨æœ€ä¼˜æ¡ä»¶ï¼Œå¼€å‘æ•°å€¼è½¨è¿¹ä¼˜åŒ–ç®—æ³•
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºæ–¹æ³•èƒ½åŒæ—¶ä¼˜åŒ–è½¨è¿¹å’Œè¶³è¿¹ï¼ŒéåŽ†æ€§æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡ä¸€ä¸ªæ•°é‡çº§ï¼Œå¹¶åº”ç”¨äºŽå¤šæ— äººæœº3Dç‰©ä½“è¦†ç›–

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.

