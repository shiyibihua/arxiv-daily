---
layout: default
title: RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features
---

# RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features

**arXiv**: [2512.08170v1](https://arxiv.org/abs/2512.08170) | [PDF](https://arxiv.org/pdf/2512.08170.pdf)

**ä½œè€…**: Haoxin Zhang, Shuaixin Li, Xiaozhou Zhu, Hongbo Chen, Wen Yao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAVES-Calibæ–¹æ³•ï¼Œç”¨äºŽæ— ç›®æ ‡çŽ¯å¢ƒä¸‹é²æ£’ã€ç²¾ç¡®çš„LiDAR-ç›¸æœºå¤–å‚è‡ªæ ¡å‡†ã€‚**

**å…³é”®è¯**: `LiDAR-ç›¸æœºæ ¡å‡†` `å¤–å‚è‡ªæ ¡å‡†` `ç‰¹å¾å¯¹åº”` `è‡ªé€‚åº”åŠ æƒ` `æ— ç›®æ ‡çŽ¯å¢ƒ` `å¤šä¼ æ„Ÿå™¨å…¼å®¹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ— éœ€åˆå§‹å˜æ¢ï¼Œåœ¨å•å¯¹æ¿€å…‰ç‚¹ä¸Žå›¾åƒä¸‹å®žçŽ°å¤šä¼ æ„Ÿå™¨å…¼å®¹çš„å¤–å‚æ ¡å‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨Gluestickå»ºç«‹2D-3Dç‰¹å¾å¯¹åº”ï¼ŒåŸºäºŽç‰¹å¾åˆ†å¸ƒè‡ªé€‚åº”åŠ æƒä¼˜åŒ–å‚æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å®¤å†…å¤–å¤šä¼ æ„Ÿå™¨å®žéªŒä¸­ï¼Œç›¸æ¯”SOTAæŠ€æœ¯å±•çŽ°æ›´ä¼˜çš„é²æ£’æ€§å’Œç²¾åº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.

