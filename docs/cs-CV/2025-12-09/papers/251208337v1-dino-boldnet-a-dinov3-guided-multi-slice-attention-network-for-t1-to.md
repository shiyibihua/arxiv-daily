---
layout: default
title: DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation
---

# DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation

**arXiv**: [2512.08337v1](https://arxiv.org/abs/2512.08337) | [PDF](https://arxiv.org/pdf/2512.08337.pdf)

**ä½œè€…**: Jianwei Wang, Qing Wang, Menglan Ruan, Rongjun Ge, Chunfeng Yang, Yang Chen, Chunming Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDINO-BOLDNetï¼Œåˆ©ç”¨DINOv3å¼•å¯¼çš„å¤šåˆ‡ç‰‡æ³¨æ„åŠ›ç½‘ç»œä»ŽT1åŠ æƒå›¾åƒç”ŸæˆBOLDå›¾åƒï¼Œä»¥æ¢å¤ç¼ºå¤±åŠŸèƒ½ä¿¡æ¯ã€‚**

**å…³é”®è¯**: `T1åˆ°BOLDç”Ÿæˆ` `è‡ªç›‘ç£Transformerå¼•å¯¼` `å¤šåˆ‡ç‰‡æ³¨æ„åŠ›` `ç»“æž„åˆ°åŠŸèƒ½æ˜ å°„` `åŒ»å­¦å›¾åƒç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“BOLDå›¾åƒæŸåæˆ–ç¼ºå¤±æ—¶ï¼Œä»ŽT1åŠ æƒå›¾åƒç”ŸæˆBOLDå›¾åƒä»¥æ”¯æŒä¸‹æ¸¸ä»»åŠ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå†»ç»“çš„DINOv3ç¼–ç å™¨æå–åˆ‡ç‰‡å†…ç»“æž„è¡¨ç¤ºï¼Œä½¿ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›æ¨¡å—èžåˆè·¨åˆ‡ç‰‡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¤šå°ºåº¦è§£ç å™¨æ¢å¤åŠŸèƒ½å¯¹æ¯”åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨248åå—è¯•è€…çš„ä¸´åºŠæ•°æ®é›†ä¸Šï¼ŒPSNRå’ŒMS-SSIMæŒ‡æ ‡ä¼˜äºŽæ¡ä»¶GANåŸºçº¿ï¼Œé¦–æ¬¡å®žçŽ°ä»ŽT1åŠ æƒå›¾åƒç›´æŽ¥ç”Ÿæˆå¹³å‡BOLDå›¾åƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.

