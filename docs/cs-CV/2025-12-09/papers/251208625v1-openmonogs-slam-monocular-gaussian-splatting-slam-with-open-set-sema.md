---
layout: default
title: OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics
---

# OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics

**arXiv**: [2512.08625v1](https://arxiv.org/abs/2512.08625) | [PDF](https://arxiv.org/pdf/2512.08625.pdf)

**ä½œè€…**: Jisang Yoo, Gyeongjin Kang, Hyun-kyu Ko, Hyeonwoo Yu, Eunbyung Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenMonoGS-SLAMï¼Œç»“åˆ3Dé«˜æ–¯æº…å°„ä¸Žå¼€æ”¾é›†è¯­ä¹‰ï¼Œå®žçŽ°å•ç›®SLAMåœ¨å¼€æ”¾ä¸–ç•ŒçŽ¯å¢ƒä¸­çš„æ™ºèƒ½æ„ŸçŸ¥ã€‚**

**å…³é”®è¯**: `å•ç›®SLAM` `3Dé«˜æ–¯æº…å°„` `å¼€æ”¾é›†è¯­ä¹‰` `è§†è§‰åŸºç¡€æ¨¡åž‹` `è‡ªç›‘ç£å­¦ä¹ ` `è¯­ä¹‰ç‰¹å¾æ˜ å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰SLAMæ–¹æ³•ä¾èµ–æ·±åº¦ä¼ æ„Ÿå™¨æˆ–å°é—­é›†è¯­ä¹‰æ¨¡åž‹ï¼Œåœ¨å¼€æ”¾ä¸–ç•ŒçŽ¯å¢ƒä¸­å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡åž‹ï¼ˆå¦‚MASt3Rã€SAMã€CLIPï¼‰è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæ— éœ€æ·±åº¦è¾“å…¥æˆ–3Dè¯­ä¹‰çœŸå€¼ï¼Œå¹¶è®¾è®¡å†…å­˜æœºåˆ¶ç®¡ç†é«˜ç»´è¯­ä¹‰ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å°é—­é›†å’Œå¼€æ”¾é›†åˆ†å‰²ä»»åŠ¡ä¸­æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶ŠåŸºçº¿ï¼Œä¸ä¾èµ–é¢å¤–ä¼ æ„Ÿå™¨æˆ–è¯­ä¹‰æ ‡æ³¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

