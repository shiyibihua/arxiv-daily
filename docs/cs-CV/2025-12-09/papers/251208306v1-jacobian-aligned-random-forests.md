---
layout: default
title: Jacobian Aligned Random Forests
---

# Jacobian Aligned Random Forests

**arXiv**: [2512.08306v1](https://arxiv.org/abs/2512.08306) | [PDF](https://arxiv.org/pdf/2512.08306.pdf)

**ä½œè€…**: Sarwesh Rauniyar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJARFæ–¹æ³•ï¼Œé€šè¿‡é›…å¯æ¯”å¯¹é½é¢„å¤„ç†å™¨æå‡è½´å¯¹é½æ£®æž—åœ¨æ—‹è½¬æˆ–äº¤äº’ä¾èµ–å†³ç­–è¾¹ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `éšæœºæ£®æž—` `é›…å¯æ¯”å¯¹é½` `ç‰¹å¾é¢„å¤„ç†å™¨` `å†³ç­–è¾¹ç•Œä¼˜åŒ–` `è¡¨æ ¼æ•°æ®å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è½´å¯¹é½å†³ç­–æ ‘åœ¨æ—‹è½¬æˆ–äº¤äº’ä¾èµ–å†³ç­–è¾¹ç•Œä¸Šè¡¨çŽ°ä¸ä½³ï¼Œéœ€è¦çº¿æ€§ç»„åˆç‰¹å¾è€Œéžå•ç‰¹å¾é˜ˆå€¼ã€‚
2. JARFåˆ©ç”¨æ£®æž—é¢„æµ‹çš„æ¢¯åº¦è®¡ç®—é›…å¯æ¯”å¤–ç§¯ï¼Œä½œä¸ºå…¨å±€çº¿æ€§é¢„å¤„ç†å™¨æ—‹è½¬ç‰¹å¾ç©ºé—´ï¼Œå†è®­ç»ƒè½´å¯¹é½æ£®æž—ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨è¡¨æ ¼åˆ†ç±»å’Œå›žå½’åŸºå‡†ä¸Šæå‡è½´å¯¹é½æ£®æž—æ€§èƒ½ï¼Œå¸¸åŒ¹é…æˆ–è¶…è¶Šæ–œæ£®æž—ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

