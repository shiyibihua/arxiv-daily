---
layout: default
title: FastBEV++: Fast by Algorithm, Deployable by Design
---

# FastBEV++: Fast by Algorithm, Deployable by Design

**arXiv**: [2512.08237v1](https://arxiv.org/abs/2512.08237) | [PDF](https://arxiv.org/pdf/2512.08237.pdf)

**ä½œè€…**: Yuanpeng Chen, Hui Song, Wei Tao, ShanHui Mo, Shuang Zhang, Xiao Hua, TianKun Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFastBEV++æ¡†æž¶ï¼Œé€šè¿‡ç®—æ³•ä¼˜åŒ–ä¸Žè®¾è®¡å¯éƒ¨ç½²æ€§è§£å†³ç›¸æœºBEVæ„ŸçŸ¥çš„æ€§èƒ½ä¸Žéƒ¨ç½²çŸ›ç›¾ã€‚**

**å…³é”®è¯**: `é¸Ÿçž°å›¾æ„ŸçŸ¥` `è§†å›¾å˜æ¢` `å®žæ—¶éƒ¨ç½²` `æ·±åº¦èžåˆ` `è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›¸æœºBEVæ„ŸçŸ¥ä¾èµ–è®¡ç®—å¯†é›†çš„è§†å›¾å˜æ¢å’Œå®šåˆ¶å†…æ ¸ï¼Œå¯¼è‡´æ€§èƒ½ä¸Žéƒ¨ç½²æ•ˆçŽ‡å†²çªã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†è§£è§†å›¾å˜æ¢ä¸ºæ ‡å‡†ç´¢å¼•-èšé›†-é‡å¡‘æµç¨‹ï¼Œç»“åˆæ·±åº¦æ„ŸçŸ¥èžåˆï¼Œæå‡å‡ ä½•ä¿çœŸåº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨nuScenesåŸºå‡†ä¸Šè¾¾åˆ°0.359 NDSï¼ŒTesla T4ç¡¬ä»¶ä¸Šè¶…è¿‡134 FPSï¼Œå®žçŽ°é«˜ç²¾åº¦ä¸Žå®žæ—¶æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

