---
layout: default
title: Reasoning Models Ace the CFA Exams
---

# Reasoning Models Ace the CFA Exams

**arXiv**: [2512.08270v1](https://arxiv.org/abs/2512.08270) | [PDF](https://arxiv.org/pdf/2512.08270.pdf)

**ä½œè€…**: Jaisal Patel, Yunzhe Chen, Kaiwen He, Keyi Wang, David Li, Kairong Xiao, Xiao-Yang Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°æŽ¨ç†æ¨¡åž‹åœ¨CFAè€ƒè¯•ä¸­çš„è¡¨çŽ°ï¼Œå‘çŽ°å¤šæ•°æ¨¡åž‹é€šè¿‡æ‰€æœ‰çº§åˆ«**

**å…³é”®è¯**: `æŽ¨ç†æ¨¡åž‹` `CFAè€ƒè¯•` `ä¸“ä¸šè¯„ä¼°` `å¤§è¯­è¨€æ¨¡åž‹` `æ€§èƒ½æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨CFAè€ƒè¯•ä¸­è¡¨çŽ°ä¸ä½³ï¼Œä½†æŽ¨ç†æ¨¡åž‹åœ¨ä¸“ä¸šè€ƒè¯•ä¸­çš„èƒ½åŠ›æœªçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ¨¡æ‹ŸCFAè€ƒè¯•æ•°æ®é›†ï¼ŒåŒ…å«ä¸‰ä¸ªçº§åˆ«çš„980é“é—®é¢˜ï¼Œè¯„ä¼°å¤šä¸ªå…ˆè¿›æŽ¨ç†æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šæ•°æ¨¡åž‹é€šè¿‡æ‰€æœ‰çº§åˆ«ï¼ŒGemini 3.0 Proåœ¨Level Iåˆ›çºªå½•å¾—åˆ†97.6%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

