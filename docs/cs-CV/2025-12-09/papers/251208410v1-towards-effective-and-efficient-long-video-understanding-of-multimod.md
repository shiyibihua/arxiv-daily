---
layout: default
title: Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval
---

# Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval

**arXiv**: [2512.08410v1](https://arxiv.org/abs/2512.08410) | [PDF](https://arxiv.org/pdf/2512.08410.pdf)

**ä½œè€…**: Tao Chen, Shaobo Ju, Qiong Wu, Chenxin Fang, Kun Zhang, Jun Peng, Hui Li, Yiyi Zhou, Rongrong Ji

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOneClip-RAGèŒƒå¼ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å¤„ç†é•¿è§†é¢‘æ—¶å†…å­˜å¼€é”€è¿‡å¤§çš„é—®é¢˜**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `è§†é¢‘åˆ†å—ç®—æ³•` `é«˜æ•ˆè®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å› å†…å­˜å¼€é”€å¤§ï¼Œåªèƒ½å¤„ç†æœ‰é™å¸§è§†é¢‘ï¼Œé™åˆ¶é•¿è§†é¢‘ç†è§£èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå•æ¬¡è§†é¢‘ç‰‡æ®µæ£€ç´¢å¢žå¼ºï¼Œç»“åˆæŸ¥è¯¢å¼•å¯¼çš„è§†é¢‘åˆ†å—ç®—æ³•ï¼Œæå‡çŸ¥è¯†å®Œæ•´æ€§å’Œè¯­ä¹‰è¿žè´¯æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªMLLMsä¸ŠéªŒè¯ï¼Œæ˜¾è‘—æå‡æ€§èƒ½ï¼Œå¦‚InternLV2 8Bå’ŒQwen2-VL 7Båœ¨MLVUåŸºå‡†ä¸Šè¾¾åˆ°GPT-4oæ°´å¹³ï¼Œå¹¶åœ¨å•GPUä¸Šé«˜æ•ˆå¤„ç†é•¿è¾¾ä¸€å°æ—¶è§†é¢‘

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

