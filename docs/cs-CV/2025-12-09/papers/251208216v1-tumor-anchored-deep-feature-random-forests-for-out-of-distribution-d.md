---
layout: default
title: Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation
---

# Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation

**arXiv**: [2512.08216v1](https://arxiv.org/abs/2512.08216) | [PDF](https://arxiv.org/pdf/2512.08216.pdf)

**ä½œè€…**: Aneesh Rangnekar, Harini Veeraraghavan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè‚¿ç˜¤é”šå®šæ·±åº¦ç‰¹å¾çš„éšæœºæ£®æž—æ¡†æž¶RF-Deepï¼Œç”¨äºŽè‚ºç™Œåˆ†å‰²ä¸­çš„åˆ†å¸ƒå¤–æ£€æµ‹ã€‚**

**å…³é”®è¯**: `è‚ºç™Œåˆ†å‰²` `åˆ†å¸ƒå¤–æ£€æµ‹` `æ·±åº¦ç‰¹å¾` `éšæœºæ£®æž—` `è®¡ç®—æœºæ–­å±‚æ‰«æ` `åŒ»å­¦å½±åƒåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è‚ºç™Œåˆ†å‰²æ¨¡åž‹æ˜“å—åˆ†å¸ƒå¤–è¾“å…¥å½±å“ï¼Œäº§ç”Ÿé”™è¯¯åˆ†å‰²ï¼Œå¨èƒä¸´åºŠå®‰å…¨éƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨çš„å±‚æ¬¡ç‰¹å¾ï¼Œé€šè¿‡è‚¿ç˜¤é”šå®šåŒºåŸŸæå–ç‰¹å¾ï¼Œæž„å»ºè½»é‡çº§éšæœºæ£®æž—è¿›è¡Œæ£€æµ‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è¿‘åˆ†å¸ƒå¤–å’Œè¿œåˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šï¼ŒRF-Deepçš„AUROCåˆ†åˆ«è¶…è¿‡93.50%å’Œ99.00%ï¼Œä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.

