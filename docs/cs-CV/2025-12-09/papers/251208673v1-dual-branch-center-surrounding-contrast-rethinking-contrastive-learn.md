---
layout: default
title: Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds
---

# Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds

**arXiv**: [2512.08673v1](https://arxiv.org/abs/2512.08673) | [PDF](https://arxiv.org/pdf/2512.08673.pdf)

**ä½œè€…**: Shaofeng Zhang, Xuanqi Chen, Xiangdong Zhang, Sitong Wu, Junchi Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒåˆ†æ”¯ä¸­å¿ƒ-å‘¨å›´å¯¹æ¯”æ¡†æž¶ä»¥æ”¹è¿›3Dç‚¹äº‘çš„è‡ªç›‘ç£å­¦ä¹ **

**å…³é”®è¯**: `3Dç‚¹äº‘` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `åŒåˆ†æ”¯æž¶æž„` `å‡ ä½•ç‰¹å¾å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šçŽ°æœ‰3Dç‚¹äº‘è‡ªç›‘ç£å­¦ä¹ ä»¥ç”Ÿæˆæ–¹æ³•ä¸ºä¸»ï¼Œä½†éš¾ä»¥æ•èŽ·é«˜å±‚åˆ¤åˆ«ç‰¹å¾ï¼Œå¯¹æ¯”æ–¹æ³•åœ¨3Dä¸­åº”ç”¨ä¸è¶³ä¸”ç›´æŽ¥è¿ç§»2Dæ–¹æ³•æ•ˆæžœä¸ä½³ã€‚
2. æ–¹æ³•ï¼šé€šè¿‡åˆ†åˆ«æŽ©ç ä¸­å¿ƒå’Œå‘¨å›´éƒ¨åˆ†æž„å»ºåŒåˆ†æ”¯è¾“å…¥ï¼Œç»“åˆè¡¥ä¸çº§å¯¹æ¯”æŸå¤±å¢žå¼ºå‡ ä½•ä¿¡æ¯æ•èŽ·å’Œå±€éƒ¨æ•æ„Ÿæ€§ã€‚
3. æ•ˆæžœï¼šåœ¨å¤šä¸ªåè®®ä¸‹è¾¾åˆ°ä¸Žç”Ÿæˆæ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜æ€§èƒ½ï¼Œå°¤å…¶åœ¨MLP-LINEARåè®®ä¸‹æ˜¾è‘—è¶…è¶ŠåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.

