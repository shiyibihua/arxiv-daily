---
layout: default
title: Reinforcement Learning From State and Temporal Differences
---

# Reinforcement Learning From State and Temporal Differences

**arXiv**: [2512.08855v1](https://arxiv.org/abs/2512.08855) | [PDF](https://arxiv.org/pdf/2512.08855.pdf)

**ä½œè€…**: Lex Weaver, Jonathan Baxter

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTD(Î»)æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–çŠ¶æ€ç›¸å¯¹å€¼è§£å†³TD(Î»)åœ¨ç­–ç•¥æ”¹è¿›ä¸­çš„æ¬¡ä¼˜é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ—¶åºå·®åˆ†å­¦ä¹ ` `å‡½æ•°é€¼è¿‘` `ç­–ç•¥æ”¹è¿›` `çŠ¶æ€ç›¸å¯¹å€¼` `äºŒå…ƒå†³ç­–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTD(Î»)åœ¨å‡½æ•°é€¼è¿‘ä¸­æœ€å°åŒ–çŠ¶æ€å€¼å¹³æ–¹è¯¯å·®ï¼Œä½†ç­–ç•¥æ”¹è¿›æ›´ä¾èµ–çŠ¶æ€ç›¸å¯¹æŽ’åºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥STD(Î»)ï¼Œåœ¨äºŒå…ƒå†³ç­–é—®é¢˜ä¸­åŸºäºŽçŠ¶æ€ç›¸å¯¹å€¼è®­ç»ƒå‡½æ•°é€¼è¿‘å™¨ï¼Œç†è®ºè¯æ˜Žå•è°ƒç­–ç•¥æ”¹è¿›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŒçŠ¶æ€ç³»ç»Ÿã€ä¸‰çŠ¶æ€ç³»ç»Ÿå’Œè¥¿æ´‹åŒé™†æ£‹ä¸­éªŒè¯TD(Î»)æ¬¡ä¼˜ï¼ŒSTD(Î»)åœ¨åŒçŠ¶æ€ç³»ç»Ÿå’Œacrobotå˜ä½“ä¸ŠæˆåŠŸæ¼”ç¤ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> TD($Î»$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($Î»$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($Î»$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($Î»$), called STD($Î»$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($Î»$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($Î»$) on the two-state system and a variation on the well known acrobot problem.

