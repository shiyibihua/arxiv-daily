---
layout: default
title: Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration
---

# Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration

**arXiv**: [2512.08922v1](https://arxiv.org/abs/2512.08922) | [PDF](https://arxiv.org/pdf/2512.08922.pdf)

**ä½œè€…**: Jin Hyeon Kim, Paul Hyunbin Cho, Claire Kim, Jaewon Min, Jaeeun Lee, Jihye Park, Yeji Choi, Seungryong Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniTç»Ÿä¸€æ¡†æž¶ï¼Œé€šè¿‡é›†æˆæ‰©æ•£Transformerã€è§†è§‰è¯­è¨€æ¨¡åž‹å’Œæ–‡æœ¬å®šä½æ¨¡å—ï¼Œè§£å†³æ–‡æœ¬æ„ŸçŸ¥å›¾åƒæ¢å¤ä¸­çš„æ–‡æœ¬å¹»è§‰é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬æ„ŸçŸ¥å›¾åƒæ¢å¤` `æ‰©æ•£Transformer` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ–‡æœ¬å®šä½æ¨¡å—` `æ–‡æœ¬å¹»è§‰æŠ‘åˆ¶` `é«˜ä¿çœŸæ–‡æœ¬æ¢å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹åœ¨æ–‡æœ¬æ„ŸçŸ¥å›¾åƒæ¢å¤ä¸­å› ç¼ºä¹æ˜¾å¼è¯­è¨€çŸ¥è¯†ï¼Œå¸¸äº§ç”Ÿæ–‡æœ¬å¹»è§‰ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆVLMæå–æ–‡æœ¬æŒ‡å¯¼ï¼ŒTSMè¿­ä»£ä¼˜åŒ–OCRé¢„æµ‹ï¼ŒDiTåˆ©ç”¨è¿™äº›çº¿ç´¢æ¢å¤ç²¾ç»†æ–‡æœ¬å¹¶æŠ‘åˆ¶å¹»è§‰ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SA-Textå’ŒReal-TextåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniTæ˜¾è‘—å‡å°‘å¹»è§‰ï¼Œå®žçŽ°æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯F1åˆ†æ•°æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.

