---
layout: default
title: TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models
---

# TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models

**arXiv**: [2512.08153v1](https://arxiv.org/abs/2512.08153) | [PDF](https://arxiv.org/pdf/2512.08153.pdf)

**ä½œè€…**: Zheng Ding, Weirui Ye

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTreeGRPOä»¥é«˜æ•ˆè§£å†³æ‰©æ•£æ¨¡åž‹åœ¨çº¿å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒçš„è®¡ç®—æˆæœ¬é—®é¢˜**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒ` `æ‰©æ•£æ¨¡åž‹å¯¹é½` `æ ‘ç»“æž„æœç´¢` `æ ·æœ¬æ•ˆçŽ‡` `ä¿¡ç”¨åˆ†é…` `è®¡ç®—ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒè®¡ç®—æˆæœ¬é«˜ï¼Œé˜»ç¢ç”Ÿæˆæ¨¡åž‹ä¸Žäººç±»åå¥½çš„å¯¹é½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†åŽ»å™ªè¿‡ç¨‹é‡æž„ä¸ºæœç´¢æ ‘ï¼Œé€šè¿‡åˆ†æ”¯ç”Ÿæˆå€™é€‰è½¨è¿¹å¹¶é‡ç”¨å…¬å…±å‰ç¼€ï¼Œå®žçŽ°é«˜æ•ˆæ ·æœ¬åˆ©ç”¨å’Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ‰©æ•£å’Œæµæ¨¡åž‹ä¸­ï¼Œè®­ç»ƒé€Ÿåº¦æå‡2.4å€ï¼Œåœ¨æ•ˆçŽ‡-å¥–åŠ±æƒè¡¡ç©ºé—´å»ºç«‹æ›´ä¼˜å¸•ç´¯æ‰˜å‰æ²¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

