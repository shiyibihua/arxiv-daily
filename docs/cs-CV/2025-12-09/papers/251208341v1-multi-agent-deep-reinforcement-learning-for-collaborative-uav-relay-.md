---
layout: default
title: Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks
---

# Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks

**arXiv**: [2512.08341v1](https://arxiv.org/abs/2512.08341) | [PDF](https://arxiv.org/pdf/2512.08341.pdf)

**ä½œè€…**: Thai Duong Nguyen, Ngoc-Tan Nguyen, Thanh-Dao Nguyen, Nguyen Van Huynh, Dinh-Hieu Tran, Symeon Chatzinotas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åä½œæ— äººæœºä¸­ç»§ç½‘ç»œæŠ—å¹²æ‰°æ–¹æ³•**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ— äººæœºä¸­ç»§ç½‘ç»œ` `æŠ—å¹²æ‰°ç­–ç•¥` `é›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡Œ` `åä½œé€šä¿¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ— äººæœºç¾¤åœ¨å¹²æ‰°çŽ¯å¢ƒä¸‹éœ€å¹³è¡¡åžåé‡æœ€å¤§åŒ–ã€é˜²ç¢°æ’žå’ŒæŠ—å¹²æ‰°ç­‰å¤šç›®æ ‡åŠ¨æ€ä¼˜åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨é›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡Œæ¡†æž¶ï¼Œé€šè¿‡å…¨å±€çŠ¶æ€æŒ‡å¯¼å±€éƒ¨å†³ç­–ï¼Œå®žçŽ°åä½œæŠ—å¹²æ‰°
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»¿çœŸæ˜¾ç¤ºç³»ç»Ÿåžåé‡æå‡çº¦50%ï¼Œç¢°æ’žçŽ‡æŽ¥è¿‘é›¶ï¼Œæ™ºèƒ½ä½“è‡ªå‘å­¦ä¹ æŠ—å¹²æ‰°ç­–ç•¥

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.

