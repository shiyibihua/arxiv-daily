---
layout: default
title: Fully Decentralized Certified Unlearning
---

# Fully Decentralized Certified Unlearning

**arXiv**: [2512.08443v1](https://arxiv.org/abs/2512.08443) | [PDF](https://arxiv.org/pdf/2512.08443.pdf)

**ä½œè€…**: Hithem Lamri, Michail Maniatakos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRR-DUæ–¹æ³•ä»¥è§£å†³åŽ»ä¸­å¿ƒåŒ–ç½‘ç»œä¸­è®¤è¯é—å¿˜çš„æ ¸å¿ƒæŒ‘æˆ˜**

**å…³é”®è¯**: `è®¤è¯é—å¿˜` `åŽ»ä¸­å¿ƒåŒ–ç½‘ç»œ` `å·®åˆ†éšç§` `éšæœºæ¸¸èµ°` `æ¢¯åº¦ä¼˜åŒ–` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶åŽ»ä¸­å¿ƒåŒ–ç½‘ç»œä¸­çš„è®¤è¯é—å¿˜é—®é¢˜ï¼Œå¡«è¡¥æ— åè°ƒå™¨åœºæ™¯çš„ç©ºç™½
2. åŸºäºŽéšæœºæ¸¸èµ°ç»“åˆæ¢¯åº¦ä¸Šå‡/ä¸‹é™ã€å­é‡‡æ ·é«˜æ–¯å™ªå£°å’Œä¿¡ä»»åŒºåŸŸæŠ•å½±
3. åœ¨å›¾åƒåŸºå‡†ä¸Šå®žçŽ°é«˜æµ‹è¯•ç²¾åº¦ï¼Œé—å¿˜å‡†ç¡®çŽ‡é™è‡³éšæœºçŒœæµ‹æ°´å¹³

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,Î´)$ network-unlearning certificates on client views via subsampled Gaussian $RÃ©nyi$ DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy--utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,Î´)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

