---
layout: default
title: DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals
---

# DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals

**arXiv**: [2512.08379v1](https://arxiv.org/abs/2512.08379) | [PDF](https://arxiv.org/pdf/2512.08379.pdf)

**ä½œè€…**: Kaiwei Liu, Yuting He, Bufang Yang, Mu Yuan, Chun Man Victor Wong, Ho Pong Andrew Sze, Zhenyu Yan, Hongkai Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepFeatureæ¡†æž¶ï¼Œåˆ©ç”¨LLMç”Ÿæˆå¯ç©¿æˆ´ç”Ÿç‰©ä¿¡å·çš„ä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾ä»¥æå‡åŒ»ç–—åº”ç”¨æ€§èƒ½**

**å…³é”®è¯**: `å¯ç©¿æˆ´ç”Ÿç‰©ä¿¡å·` `ç‰¹å¾ç”Ÿæˆ` `å¤§è¯­è¨€æ¨¡åž‹` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `è¿­ä»£ç²¾ç‚¼` `åŒ»ç–—åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç‰¹å¾æå–æ–¹æ³•ç¼ºä¹ä»»åŠ¡ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œåœ¨é«˜ç»´ç©ºé—´éš¾ä»¥ä¼˜åŒ–ï¼Œæ˜“äº§ç”Ÿä»£ç é”™è¯¯
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆä¸“å®¶çŸ¥è¯†ä¸Žä»»åŠ¡è®¾ç½®çš„å¤šæºç”Ÿæˆæœºåˆ¶ï¼ŒåŸºäºŽè¯„ä¼°åé¦ˆçš„è¿­ä»£ç‰¹å¾ç²¾ç‚¼
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…«é¡¹ä»»åŠ¡ä¸­å¹³å‡AUROCæå‡4.21-9.67%ï¼Œä¼˜äºŽæˆ–æŒå¹³çŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

