---
layout: default
title: Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing
---

# Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09965" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09965v2</a>
  <a href="https://arxiv.org/pdf/2506.09965.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09965v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09965v2', 'Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-06-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡ç»˜å›¾å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€æ¨ç†` `ç»˜å›¾æ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `å‡ ä½•ç†è§£` `è‡ªæˆ‘åæ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†éœ€è¦ç²¾ç¡®ç©ºé—´ç†è§£çš„ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œç©ºé—´æ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥åœ¨è§†è§‰ç©ºé—´ä¸­è¿›è¡Œæ“ä½œå’Œåˆ†æã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVILASRæ¨¡å‹åœ¨å¤šç§ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æå‡18.4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œå¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…´è¶£æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒï¼Œå¯¼è‡´åœ¨éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’Œè¿ç»­ç©ºé—´è·Ÿè¸ªçš„ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­é‡åˆ°åŸºæœ¬é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œç©ºé—´æ¨ç†çš„èŒƒå¼ï¼Œä½¿LVLMsèƒ½å¤Ÿé€šè¿‡ç›´æ¥çš„è§†è§‰æ“ä½œè¡¨è¾¾å’Œåˆ†æç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹VILASRåœ¨å¤šç§ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æå‡å¹…åº¦è¾¾åˆ°18.4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹å‡ ä½•å…³ç³»çš„ç²¾ç¡®ç†è§£å’Œç©ºé—´è·Ÿè¸ªèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºé€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œï¼ˆå¦‚æ ‡æ³¨è¾¹ç•Œæ¡†å’Œç»˜åˆ¶è¾…åŠ©çº¿ï¼‰æ¥å¢å¼ºLVLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡è§†è§‰æ“ä½œç›´æ¥åˆ†æç©ºé—´å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒä»¥å»ºç«‹åŸºæœ¬ç»˜å›¾èƒ½åŠ›ï¼›é€šè¿‡åæ€æ‹’ç»é‡‡æ ·å¢å¼ºæ¨¡å‹çš„è‡ªæˆ‘åæ€è¡Œä¸ºï¼›ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–ç›®æ ‡å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥ç»˜å›¾æ“ä½œä½œä¸ºæ¨ç†å·¥å…·ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è§†è§‰ç©ºé—´ä¸­è¿›è¡Œç›´æ¥æ“ä½œï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•çš„æ€§èƒ½ç“¶é¢ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨åˆæˆæ•°æ®è¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç»˜å›¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVILASRæ¨¡å‹åœ¨è¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€åŸºäºè§†é¢‘çš„æ¨ç†å’Œå¤šè§†è§’æ¨ç†ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æå‡å¹…åº¦è¾¾åˆ°18.4%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œè§†é¢‘ç†è§£ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æœºå™¨åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.

