---
layout: default
title: Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search
---

# Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11155" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11155v1</a>
  <a href="https://arxiv.org/pdf/2506.11155.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11155v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11155v1', 'Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linhao Yu, Xinguang Ji, Yahui Liu, Fanheng Kong, Chenxi Sun, Jingyuan Zhang, Hongzhi Zhang, V. W., Fuzheng Zhang, Deyi Xiong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: 28 pages; ACL 2025(main)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/tjunlp-lab/MCTS-VCB)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoCaptionæ¡†æ¶ä»¥è§£å†³è§†é¢‘å­—å¹•ç”Ÿæˆè¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å­—å¹•ç”Ÿæˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è‡ªåŠ¨åŒ–è¯„ä¼°` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†é¢‘å­—å¹•ç”Ÿæˆæ–¹æ³•åœ¨å…³é”®ç‚¹åˆ›å»ºä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„æœ‰æ•ˆæ€§å’Œå¤šæ ·æ€§å—åˆ°é™åˆ¶ã€‚
2. æœ¬æ–‡æå‡ºçš„AutoCaptionæ¡†æ¶åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿­ä»£ç”Ÿæˆå¤šæ ·åŒ–çš„æè¿°æ€§å¥å­ï¼Œå…¨é¢æå‡è§†é¢‘å†…å®¹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMCTS-VCBåŸºå‡†èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°å¤šç§MLLMsçš„å­—å¹•ç”Ÿæˆèƒ½åŠ›ï¼Œä¸”é€šè¿‡å¾®è°ƒï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å­—å¹•ç”Ÿæˆå¯ä»¥ç”¨æ¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†å’Œè¯„ä¼°åè®®å­˜åœ¨å…³é”®é—®é¢˜ï¼Œå¦‚å…³é”®ç‚¹åˆ›å»ºä¸è¶³æˆ–åŒè´¨åŒ–ã€æ•°æ®åˆ›å»ºæˆæœ¬é«˜ä»¥åŠè¯„ä¼°èŒƒå›´æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶AutoCaptionï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä»¥è¿­ä»£æ–¹å¼æ„å»ºå¤šæ ·åŒ–çš„æè¿°æ€§å¥å­ï¼ˆå³å…³é”®ç‚¹ï¼‰ï¼Œå…¨é¢ä»£è¡¨è§†é¢‘å†…å®¹ã€‚è¯¥ç­–ç•¥ä½¿å¾—è§†é¢‘ç»†èŠ‚ï¼ˆå¦‚åŠ¨ä½œã€ç‰©ä½“å±æ€§ã€ç¯å¢ƒç»†èŠ‚ç­‰ï¼‰å¾—ä»¥æŒç»­å¢å¼ºã€‚æˆ‘ä»¬å°†AutoCaptionåº”ç”¨äºMCTS-VCBï¼Œä¸€ä¸ªæ¶µç›–è§†é¢‘ç»†èŠ‚çš„ç»†ç²’åº¦è§†é¢‘å­—å¹•åŸºå‡†ï¼Œä»è€Œå®ç°å¯¹MLLMsåœ¨è§†é¢‘å­—å¹•ç”Ÿæˆä»»åŠ¡ä¸Šçš„å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCTS-VCBèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°è§†é¢‘å­—å¹•ç”Ÿæˆèƒ½åŠ›ï¼ŒGemini-1.5-Proè·å¾—æœ€é«˜F1åˆ†æ•°71.2ã€‚é€šè¿‡AutoCaptionç”Ÿæˆçš„æ•°æ®å¾®è°ƒInternVL2.5-8Bï¼Œæ¨¡å‹åœ¨MCTS-VCBå’ŒDREAM-1Kä¸Šåˆ†åˆ«æå‡äº†25.0%å’Œ16.3%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘å­—å¹•ç”Ÿæˆè¯„ä¼°æ–¹æ³•ä¸­å…³é”®ç‚¹åˆ›å»ºä¸è¶³ã€æ•°æ®åˆ›å»ºæˆæœ¬é«˜åŠè¯„ä¼°èŒƒå›´æœ‰é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å…¨é¢åæ˜ è§†é¢‘å†…å®¹çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„AutoCaptionæ¡†æ¶é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æŠ€æœ¯ï¼Œè¿­ä»£ç”Ÿæˆå¤šæ ·åŒ–çš„æè¿°æ€§å¥å­ï¼Œä»¥å…¨é¢è¡¨è¾¾è§†é¢‘å†…å®¹çš„ç»†èŠ‚ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç”Ÿæˆçš„å­—å¹•æ›´å…·ä¿¡æ¯é‡å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAutoCaptionæ¡†æ¶ä¸»è¦åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€MCTSæœç´¢æ¨¡å—å’Œè¾“å‡ºç”Ÿæˆæ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶è§†é¢‘æ•°æ®ï¼ŒMCTSæœç´¢æ¨¡å—é€šè¿‡è¿­ä»£ç”Ÿæˆå…³é”®ç‚¹å¹¶ä¼˜åŒ–æè¿°ï¼Œè¾“å‡ºç”Ÿæˆæ¨¡å—åˆ™å°†æœ€ç»ˆçš„æè¿°æ€§å¥å­å‘ˆç°å‡ºæ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†MCTSåº”ç”¨äºè§†é¢‘å­—å¹•ç”Ÿæˆï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç”Ÿæˆçš„æè¿°æ€§å¥å­ï¼Œæ˜¾è‘—æå‡äº†å­—å¹•çš„å¤šæ ·æ€§å’Œä¿¡æ¯é‡ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€ç”Ÿæˆæ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMCTSçš„æœç´¢æ·±åº¦å’Œå®½åº¦å¯ä»¥æ ¹æ®è§†é¢‘å†…å®¹çš„å¤æ‚æ€§è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œè€ƒè™‘äº†ç”Ÿæˆå¥å­çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„å­—å¹•æ—¢ä¸°å¯Œåˆç¬¦åˆè§†é¢‘å†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMCTS-VCBåŸºå‡†èƒ½å¤Ÿå…¨é¢è¯„ä¼°è§†é¢‘å­—å¹•ç”Ÿæˆèƒ½åŠ›ï¼Œå…¶ä¸­Gemini-1.5-Proè·å¾—æœ€é«˜F1åˆ†æ•°71.2ã€‚é€šè¿‡AutoCaptionç”Ÿæˆçš„æ•°æ®å¾®è°ƒInternVL2.5-8Bï¼Œæ¨¡å‹åœ¨MCTS-VCBå’ŒDREAM-1Kä¸Šåˆ†åˆ«æå‡äº†25.0%å’Œ16.3%ï¼ŒéªŒè¯äº†AutoCaptionçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘å†…å®¹åˆ†æã€è‡ªåŠ¨åŒ–å­—å¹•ç”Ÿæˆå’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒAutoCaptionæ¡†æ¶èƒ½å¤Ÿä¸ºæ•™è‚²ã€å¨±ä¹å’Œç¤¾äº¤åª’ä½“ç­‰è¡Œä¸šæä¾›æ›´æ™ºèƒ½çš„å†…å®¹å¤„ç†æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨è§†é¢‘åˆ†ææŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video captioning can be used to assess the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, existing benchmarks and evaluation protocols suffer from crucial issues, such as inadequate or homogeneous creation of key points, exorbitant cost of data creation, and limited evaluation scopes. To address these issues, we propose an automatic framework, named AutoCaption, which leverages Monte Carlo Tree Search (MCTS) to construct numerous and diverse descriptive sentences (\textit{i.e.}, key points) that thoroughly represent video content in an iterative way. This iterative captioning strategy enables the continuous enhancement of video details such as actions, objects' attributes, environment details, etc. We apply AutoCaption to curate MCTS-VCB, a fine-grained video caption benchmark covering video details, thereby enabling a comprehensive evaluation of MLLMs on the video captioning task. We evaluate more than 20 open- and closed-source MLLMs of varying sizes on MCTS-VCB. Results show that MCTS-VCB can effectively and comprehensively evaluate the video captioning capability, with Gemini-1.5-Pro achieving the highest F1 score of 71.2. Interestingly, we fine-tune InternVL2.5-8B with the AutoCaption-generated data, which helps the model achieve an overall improvement of 25.0% on MCTS-VCB and 16.3% on DREAM-1K, further demonstrating the effectiveness of AutoCaption. The code and data are available at https://github.com/tjunlp-lab/MCTS-VCB.

