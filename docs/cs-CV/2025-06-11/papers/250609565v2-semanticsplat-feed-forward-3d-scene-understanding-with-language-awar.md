---
layout: default
title: SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields
---

# SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09565" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09565v2</a>
  <a href="https://arxiv.org/pdf/2506.09565.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09565v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09565v2', 'SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qijing Li, Jingxiang Sun, Liang An, Zhaoqi Su, Hongwen Zhang, Yebin Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-06-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSemanticSplatä»¥è§£å†³3Dåœºæ™¯ç†è§£ä¸­çš„è¯­ä¹‰ä¸å‡ ä½•å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `è¯­ä¹‰å»ºæ¨¡` `é«˜æ–¯èåˆ` `å‰é¦ˆé‡å»º` `å¤šæ¨¡æ€ç‰¹å¾` `æœºå™¨äººäº¤äº’` `å¢å¼ºç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dåœºæ™¯ç†è§£æ–¹æ³•åœ¨å…¨é¢å»ºæ¨¡å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´åœºæ™¯ç†è§£æ•ˆæœä¸ä½³ã€‚
2. SemanticSplaté€šè¿‡å°†3Dé«˜æ–¯ä¸æ½œåœ¨è¯­ä¹‰å±æ€§ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å‰é¦ˆè¯­ä¹‰æ„ŸçŸ¥3Dé‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ›´å…¨é¢çš„åœºæ™¯ç†è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSemanticSplatåœ¨3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¯æç¤ºå’Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹é¢æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨é¢çš„3Dåœºæ™¯ç†è§£ï¼Œè”åˆå»ºæ¨¡å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰ï¼Œå¯¹äºå¢å¼ºç°å®å’Œæœºå™¨äººäº¤äº’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å‰é¦ˆ3Dåœºæ™¯ç†è§£æ–¹æ³•ï¼ˆå¦‚LSMï¼‰ä»…é™äºæå–åŸºäºè¯­è¨€çš„è¯­ä¹‰ï¼Œæ— æ³•å®ç°å…¨é¢çš„åœºæ™¯ç†è§£ï¼Œå¹¶ä¸”åœ¨å‡ ä½•é‡å»ºè´¨é‡å’Œå™ªå£°ä¼ªå½±æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼ŒåŸºäºæ¯åœºæ™¯ä¼˜åŒ–çš„æ–¹æ³•ä¾èµ–äºå¯†é›†è¾“å…¥è§†å›¾ï¼Œé™ä½äº†å®ç”¨æ€§å¹¶å¢åŠ äº†éƒ¨ç½²å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºäº†SemanticSplatï¼Œè¿™æ˜¯ä¸€ç§å‰é¦ˆè¯­ä¹‰æ„ŸçŸ¥çš„3Dé‡å»ºæ–¹æ³•ï¼Œç»Ÿä¸€äº†3Dé«˜æ–¯ä¸æ½œåœ¨è¯­ä¹‰å±æ€§ï¼Œå®ç°å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰çš„è”åˆå»ºæ¨¡ã€‚é€šè¿‡èåˆå¤šæ ·çš„ç‰¹å¾åœºï¼ŒSemanticSplatå¢å¼ºäº†åœºæ™¯ç†è§£çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒå¯æç¤ºå’Œå¼€æ”¾è¯æ±‡åˆ†å‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Dåœºæ™¯ç†è§£æ–¹æ³•åœ¨å‡ ä½•é‡å»ºè´¨é‡å’Œè¯­ä¹‰æå–æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ— æ³•å®ç°å…¨é¢çš„åœºæ™¯ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSemanticSplaté€šè¿‡å°†3Dé«˜æ–¯ä¸æ½œåœ¨è¯­ä¹‰å±æ€§ç»“åˆï¼Œé‡‡ç”¨å‰é¦ˆæ–¹å¼è¿›è¡Œè¯­ä¹‰æ„ŸçŸ¥çš„3Dé‡å»ºï¼Œæ—¨åœ¨æé«˜åœºæ™¯ç†è§£çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾èåˆæ¨¡å—å’Œæˆæœ¬ä½“ç§¯è¡¨ç¤ºï¼Œå‰è€…èåˆå¤šç§ç‰¹å¾åœºï¼Œåè€…å­˜å‚¨è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œæœ€ç»ˆç”Ÿæˆå¤šæ¨¡æ€è¯­ä¹‰ç‰¹å¾åœºã€‚

**å…³é”®åˆ›æ–°**ï¼šSemanticSplatçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†3Dé«˜æ–¯ä¸è¯­ä¹‰å±æ€§ç»“åˆï¼Œå½¢æˆä¸€ç§æ–°çš„è”åˆå»ºæ¨¡æ–¹å¼ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯çš„æ•´åˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒSemanticSplaté‡‡ç”¨äº†ä¸¤é˜¶æ®µè’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–è§†å›¾å›¾åƒé‡å»ºå¤šæ¨¡æ€è¯­ä¹‰ç‰¹å¾åœºï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–é‡å»ºæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSemanticSplatåœ¨3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¯æç¤ºå’Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹é¢ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œæœ‰æ•ˆæå‡äº†åœºæ™¯ç†è§£çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¢å¼ºç°å®ã€æœºå™¨äººäº¤äº’å’Œè‡ªåŠ¨é©¾é©¶ç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„åœºæ™¯ç†è§£ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒSemanticSplatæœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Holistic 3D scene understanding, which jointly models geometry, appearance, and semantics, is crucial for applications like augmented reality and robotic interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM) are limited to extracting language-based semantics from scenes, failing to achieve holistic scene comprehension. Additionally, they suffer from low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene optimization methods rely on dense input views, which reduces practicality and increases complexity during deployment. In this paper, we propose SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which unifies 3D Gaussians with latent semantic attributes for joint geometry-appearance-semantics modeling. To predict the semantic anisotropic Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a cost volume representation that stores cross-view feature similarities, enhancing coherent and accurate scene comprehension. Leveraging a two-stage distillation framework, SemanticSplat reconstructs a holistic multi-modal semantic feature field from sparse-view images. Experiments demonstrate the effectiveness of our method for 3D scene understanding tasks like promptable and open-vocabulary segmentation. Video results are available at https://semanticsplat.github.io.

