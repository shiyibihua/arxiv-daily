---
layout: default
title: Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets
---

# Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09745" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09745v1</a>
  <a href="https://arxiv.org/pdf/2506.09745.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09745v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09745v1', 'Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yangrui Zhu, Junhua Bao, Yipan Wei, Yapeng Li, Bo Du

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç±»åˆ«ç›¸ä¼¼æ€§çš„å¤šæ¨¡æ€åˆ†ç±»æ–¹æ³•ä»¥è§£å†³å¼‚æ„ç±»åˆ«é›†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¼‚æ„ç±»åˆ«é›†` `ç±»åˆ«ç›¸ä¼¼æ€§` `è·¨æ¨¡æ€èåˆ` `çŸ¥è¯†è½¬ç§»` `å†³ç­–èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ–¹æ³•å‡è®¾ä¸åŒæ¨¡æ€å…±äº«ç›¸åŒç±»åˆ«é›†ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ç±»åˆ«åˆ†å¸ƒä¸ä¸€è‡´ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. æå‡ºå¤šæ¨¡æ€å¼‚æ„ç±»åˆ«é›†å­¦ä¹ ï¼ˆMMHCLï¼‰è®¾ç½®ï¼Œè®¾è®¡åŸºäºç±»åˆ«ç›¸ä¼¼æ€§çš„è·¨æ¨¡æ€èåˆæ¨¡å‹ï¼ˆCSCFï¼‰ï¼Œå®ç°çŸ¥è¯†è½¬ç§»å’Œå†³ç­–èåˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCSCFåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤šæ¨¡æ€æ–¹æ³•é€šå¸¸å‡è®¾ä¸åŒæ¨¡æ€å…±äº«ç›¸åŒçš„ç±»åˆ«é›†ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¤šæ¨¡æ€æ•°æ®çš„ç±»åˆ«åˆ†å¸ƒå­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œè¿™ä¼šå¦¨ç¢æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯è¿›è¡Œç±»åˆ«è¯†åˆ«ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¤šæ¨¡æ€å¼‚æ„ç±»åˆ«é›†å­¦ä¹ ï¼ˆMMHCLï¼‰çš„å®ç”¨è®¾ç½®ï¼Œæ—¨åœ¨è®­ç»ƒå¼‚æ„ç±»åˆ«é›†çš„å¤šæ¨¡æ€æ•°æ®æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶è¯†åˆ«æ‰€æœ‰æ¨¡æ€çš„å®Œæ•´ç±»åˆ«é›†ã€‚ä¸ºæœ‰æ•ˆè§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œæå‡ºäº†åŸºäºç±»åˆ«ç›¸ä¼¼æ€§çš„è·¨æ¨¡æ€èåˆæ¨¡å‹ï¼ˆCSCFï¼‰ï¼Œè¯¥æ¨¡å‹å°†ç‰¹å®šæ¨¡æ€çš„ç‰¹å¾å¯¹é½åˆ°å…±äº«è¯­ä¹‰ç©ºé—´ï¼Œä»¥å®ç°å·²çŸ¥ç±»åˆ«ä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡é€‰æ‹©æœ€å…·è¾¨åˆ«åŠ›çš„æ¨¡æ€è¿›è¡Œå†³ç­–èåˆã€‚æœ€åï¼ŒåŸºäºç±»åˆ«ç›¸ä¼¼æ€§æ•´åˆè·¨æ¨¡æ€ä¿¡æ¯ï¼Œè¾…åŠ©æ¨¡æ€å¯¹ä¸»å¯¼æ¨¡æ€çš„é¢„æµ‹è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†MMHCLä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ•°æ®ä¸­ç±»åˆ«é›†ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯è¿›è¡Œå…¨é¢çš„ç±»åˆ«è¯†åˆ«ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„CSCFæ¨¡å‹é€šè¿‡å¯¹é½æ¨¡æ€ç‰¹å¾åˆ°å…±äº«è¯­ä¹‰ç©ºé—´ï¼Œå®ç°å·²çŸ¥ä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡é€‰æ‹©æœ€ä¼˜æ¨¡æ€è¿›è¡Œå†³ç­–èåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCSCFæ¨¡å‹åŒ…æ‹¬ç‰¹å¾å¯¹é½æ¨¡å—ã€å†³ç­–èåˆæ¨¡å—å’Œç±»åˆ«ç›¸ä¼¼æ€§æ•´åˆæ¨¡å—ã€‚ç‰¹å¾å¯¹é½æ¨¡å—å°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾æ˜ å°„åˆ°å…±äº«ç©ºé—´ï¼Œå†³ç­–èåˆæ¨¡å—é€‰æ‹©æœ€å…·è¾¨åˆ«åŠ›çš„æ¨¡æ€ï¼Œç±»åˆ«ç›¸ä¼¼æ€§æ•´åˆæ¨¡å—ä¼˜åŒ–æœ€ç»ˆé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCSCFæ¨¡å‹çš„åˆ›æ–°åœ¨äºé€šè¿‡ç±»åˆ«ç›¸ä¼¼æ€§è¿›è¡Œè·¨æ¨¡æ€ä¿¡æ¯æ•´åˆï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€æ¨¡æ€ä¾èµ–ï¼Œæå‡äº†æ¨¡å‹å¯¹æœªçŸ¥ç±»åˆ«çš„è¯†åˆ«èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨ç‰¹å¾å¯¹é½æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡æ€ç‰¹å¾çš„å¯¹é½ï¼ŒåŒæ—¶è®¾è®¡äº†ä¸ç¡®å®šæ€§ä¼°è®¡æœºåˆ¶æ¥é€‰æ‹©æœ€ä¼˜æ¨¡æ€ï¼Œç¡®ä¿å†³ç­–èåˆçš„å‡†ç¡®æ€§ã€‚ç½‘ç»œç»“æ„ä¸Šï¼ŒCSCFé‡‡ç”¨äº†å¤šå±‚æ„ŸçŸ¥æœºå’Œå·ç§¯ç¥ç»ç½‘ç»œçš„ç»„åˆï¼Œä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCSCFæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†æ¨¡å‹åœ¨å¤„ç†å¼‚æ„ç±»åˆ«é›†é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡è§£å†³å¼‚æ„ç±»åˆ«é›†é—®é¢˜ï¼ŒCSCFæ¨¡å‹å¯ä»¥åº”ç”¨äºè·¨æ¨¡æ€æ£€ç´¢ã€å›¾åƒæ ‡æ³¨å’Œè§†é¢‘ç†è§£ç­‰ä»»åŠ¡ï¼Œæå‡å®é™…ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šå¤æ‚åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing multimodal methods typically assume that different modalities share the same category set. However, in real-world applications, the category distributions in multimodal data exhibit inconsistencies, which can hinder the model's ability to effectively utilize cross-modal information for recognizing all categories. In this work, we propose the practical setting termed Multi-Modal Heterogeneous Category-set Learning (MMHCL), where models are trained in heterogeneous category sets of multi-modal data and aim to recognize complete classes set of all modalities during test. To effectively address this task, we propose a Class Similarity-based Cross-modal Fusion model (CSCF). Specifically, CSCF aligns modality-specific features to a shared semantic space to enable knowledge transfer between seen and unseen classes. It then selects the most discriminative modality for decision fusion through uncertainty estimation. Finally, it integrates cross-modal information based on class similarity, where the auxiliary modality refines the prediction of the dominant one. Experimental results show that our method significantly outperforms existing state-of-the-art (SOTA) approaches on multiple benchmark datasets, effectively addressing the MMHCL task.

