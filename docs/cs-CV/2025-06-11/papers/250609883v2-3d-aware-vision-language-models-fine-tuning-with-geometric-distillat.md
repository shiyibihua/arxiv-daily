---
layout: default
title: 3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation
---

# 3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09883" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09883v2</a>
  <a href="https://arxiv.org/pdf/2506.09883.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09883v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09883v2', '3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seonho Lee, Jiho Choi, Inha Kang, Jiwook Kim, Junsung Park, Hyunjung Shim

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-11-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡ ä½•è’¸é¦æ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„3Dç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‡ ä½•è’¸é¦` `è§†è§‰è¯­è¨€æ¨¡å‹` `3Dç†è§£` `å¤šæ¨¡æ€ä»»åŠ¡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£3Dç©ºé—´ç»“æ„æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„å‡ ä½•è’¸é¦æ–¹æ³•é€šè¿‡æ³¨å…¥å‡ ä½•çº¿ç´¢ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒVLMsçš„3Dæ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸”æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dè§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨3Dç©ºé—´ç»“æ„ç†è§£ä¸Šä»å­˜åœ¨å±€é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å‡ ä½•è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡æ³¨å…¥äººç±»å¯å‘çš„å‡ ä½•çº¿ç´¢ï¼Œå¯¹é¢„è®­ç»ƒçš„VLMsè¿›è¡Œæ— æ³¨é‡Šå¾®è°ƒï¼Œè€Œæ— éœ€ä¿®æ”¹å…¶æ¶æ„ã€‚è¯¥æ–¹æ³•ä»ç°æˆçš„3DåŸºç¡€æ¨¡å‹ä¸­æå–ç¨€ç–å¯¹åº”å…³ç³»ã€ç›¸å¯¹æ·±åº¦å…³ç³»å’Œå¯†é›†ä»£ä»·ä½“ç§¯ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒä¸è‡ªç„¶å›¾åƒ-æ–‡æœ¬è¾“å…¥å…¼å®¹çš„åŒæ—¶ï¼Œå…·å¤‡å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡åœ¨3Dè§†è§‰è¯­è¨€æ¨ç†å’Œ3Dæ„ŸçŸ¥åŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å°†2Dè®­ç»ƒçš„VLMsä¸3Dç†è§£ç›¸ç»“åˆçš„å¯æ‰©å±•å’Œé«˜æ•ˆè·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3Dç©ºé—´ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨3Då‡ ä½•ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå‡ ä½•è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡ä»3DåŸºç¡€æ¨¡å‹ä¸­æå–å‡ ä½•çº¿ç´¢ï¼Œå¢å¼ºVLMsçš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€Œä¸éœ€æ”¹å˜å…¶åŸæœ‰æ¶æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä»3Dæ¨¡å‹ä¸­æå–ç¨€ç–å¯¹åº”å…³ç³»ï¼›2) è·å–ç›¸å¯¹æ·±åº¦å…³ç³»ï¼›3) ç”Ÿæˆå¯†é›†ä»£ä»·ä½“ç§¯ï¼Œæœ€ç»ˆå°†è¿™äº›ä¿¡æ¯æ³¨å…¥åˆ°VLMä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå‡ ä½•è’¸é¦æ–¹æ³•çš„æå‡ºï¼Œä½¿å¾—VLMsèƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œè·å¾—3Dç©ºé—´ç†è§£èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å‡ ä½•ä¿¡æ¯çš„èåˆï¼ŒåŒæ—¶ä¿æŒä¸è‡ªç„¶å›¾åƒ-æ–‡æœ¬è¾“å…¥çš„å…¼å®¹æ€§ï¼Œç¡®ä¿äº†æ¨¡å‹çš„å®ç”¨æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨3Dè§†è§‰è¯­è¨€æ¨ç†åŸºå‡†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½äº†30%ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿å’Œæ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¢å¼ºç°å®ã€æœºå™¨äººå¯¼èˆªå’Œè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿå¯¹3Dç¯å¢ƒçš„ç†è§£å’Œäº¤äº’èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€ä»»åŠ¡çš„ä¸æ–­å‘å±•ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have shown remarkable performance on diverse visual and linguistic tasks, yet they remain fundamentally limited in their understanding of 3D spatial structures. We propose Geometric Distillation, a lightweight, annotation-free fine-tuning framework that injects human-inspired geometric cues into pretrained VLMs without modifying their architecture. By distilling (1) sparse correspondences, (2) relative depth relations, and (3) dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R, VGGT), our method shapes representations to be geometry-aware while remaining compatible with natural image-text inputs. Through extensive evaluations on 3D vision-language reasoning and 3D perception benchmarks, our method consistently outperforms prior approaches, achieving improved 3D spatial reasoning with significantly lower computational cost. Our work demonstrates a scalable and efficient path to bridge 2D-trained VLMs with 3D understanding, opening up wider use in spatially grounded multimodal tasks.

