---
layout: default
title: EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models
---

# EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10100v1</a>
  <a href="https://arxiv.org/pdf/2506.10100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10100v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10100v1', 'EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEfficientVLAä»¥è§£å†³VLAæ¨¡å‹çš„åŠ é€Ÿä¸å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `æ¨¡å‹åŠ é€Ÿ` `æ— è®­ç»ƒæ¡†æ¶` `å¤šæ¨¡æ€èåˆ` `è®¡ç®—å†—ä½™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAæ¨¡å‹åœ¨æ¨ç†æ—¶é¢ä¸´é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. EfficientVLAé€šè¿‡æ— è®­ç»ƒçš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°æ¶ˆé™¤VLAæ¨¡å‹ä¸­çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆï¼Œæ•´åˆå¤šç§å†—ä½™åˆ©ç”¨ç­–ç•¥ã€‚
3. åœ¨CogACTæ¨¡å‹ä¸Šï¼ŒEfficientVLAå®ç°äº†1.93å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒFLOPså‡å°‘è‡³28.9%ï¼ŒæˆåŠŸç‡ä»…ä¸‹é™0.6%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åŸºäºæ‰©æ•£çš„æ¶æ„ï¼Œå±•ç°äº†åœ¨å…·èº«æ™ºèƒ½æ–¹é¢çš„å˜é©æ½œåŠ›ï¼Œä½†ç”±äºå›ºæœ‰çš„å†—ä½™å’Œæ¨ç†æ—¶çš„é«˜è®¡ç®—ä¸å†…å­˜éœ€æ±‚ï¼Œé¢ä¸´ä¸¥é‡æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•å¾€å¾€åªé’ˆå¯¹å­¤ç«‹çš„ä½æ•ˆç¯èŠ‚ï¼Œæ— æ³•å…¨é¢è§£å†³VLAç®¡é“ä¸­çš„å„ç§è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºEfficientVLAï¼Œä¸€ä¸ªç»“æ„åŒ–ä¸”æ— è®­ç»ƒçš„æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ¶ˆé™¤è¿™äº›éšœç¢ï¼Œç»¼åˆåˆ©ç”¨å¤šæ–¹é¢çš„å†—ä½™ã€‚EfficientVLAæ•´åˆäº†ä¸‰ç§é’ˆå¯¹æ€§ç­–ç•¥ï¼šè¯­è¨€æ¨¡å—ä¸­åŠŸèƒ½ä¸é‡è¦å±‚çš„å‰ªæã€é€šè¿‡ä»»åŠ¡æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–è§†è§‰å¤„ç†è·¯å¾„ï¼Œä»¥åŠåœ¨è¿­ä»£æ‰©æ•£åŠ¨ä½œå¤´ä¸­é€šè¿‡ç¼“å­˜å’Œé‡ç”¨å…³é”®ä¸­é—´ç‰¹å¾æ¥ç¼“è§£æ—¶é—´è®¡ç®—å†—ä½™ã€‚åº”ç”¨äºæ ‡å‡†VLAæ¨¡å‹CogACTï¼ŒEfficientVLAå®ç°äº†1.93å€çš„æ¨ç†åŠ é€Ÿï¼ŒFLOPsé™ä½è‡³28.9%ï¼Œåœ¨SIMPLERåŸºå‡†æµ‹è¯•ä¸­ä»…æœ‰0.6%çš„æˆåŠŸç‡ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªé’ˆå¯¹å•ä¸€ä½æ•ˆç¯èŠ‚ï¼Œæœªèƒ½å…¨é¢è§£å†³æ•´ä¸ªVLAç®¡é“ä¸­çš„å¤šç§è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEfficientVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ— è®­ç»ƒçš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°æ¶ˆé™¤VLAæ¨¡å‹ä¸­çš„å†—ä½™ï¼Œæ•´åˆå¤šç§ç­–ç•¥ä»¥æå‡æ¨ç†æ•ˆç‡å’Œé™ä½èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEfficientVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­è¨€æ¨¡å—çš„å‰ªæã€è§†è§‰å¤„ç†è·¯å¾„çš„ä¼˜åŒ–å’Œæ‰©æ•£åŠ¨ä½œå¤´çš„æ—¶é—´è®¡ç®—å†—ä½™ç¼“è§£ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºé€šè¿‡ç³»ç»Ÿæ€§åˆ†æå’Œæ•´åˆå¤šç§å†—ä½™ï¼Œæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†VLAæ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯­è¨€æ¨¡å—ä¸­ï¼Œé€šè¿‡åˆ†æå±‚é—´å†—ä½™è¿›è¡ŒåŠŸèƒ½ä¸é‡è¦å±‚çš„å‰ªæï¼›åœ¨è§†è§‰å¤„ç†è·¯å¾„ä¸­ï¼Œé‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥ç­–ç•¥é€‰æ‹©ç´§å‡‘ä¸”å¤šæ ·çš„è§†è§‰æ ‡è®°ï¼›åœ¨æ‰©æ•£åŠ¨ä½œå¤´ä¸­ï¼Œé€šè¿‡ç¼“å­˜å’Œé‡ç”¨å…³é”®ä¸­é—´ç‰¹å¾æ¥å‡å°‘æ—¶é—´è®¡ç®—å†—ä½™ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CogACTæ¨¡å‹ä¸Šï¼ŒEfficientVLAå®ç°äº†1.93å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒFLOPsé™ä½è‡³28.9%ï¼Œå¹¶ä¸”åœ¨SIMPLERåŸºå‡†æµ‹è¯•ä¸­ä»…æœ‰0.6%çš„æˆåŠŸç‡ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨åŠ é€Ÿå’Œå‹ç¼©æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EfficientVLAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½åŠ©æ‰‹å’Œå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€‚é€šè¿‡æå‡VLAæ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡å’Œå®æ—¶åº”ç”¨ï¼Œæ¨åŠ¨å…·èº«æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.

