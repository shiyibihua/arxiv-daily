---
layout: default
title: OctoNav: Towards Generalist Embodied Navigation
---

# OctoNav: Towards Generalist Embodied Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09839" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09839v1</a>
  <a href="https://arxiv.org/pdf/2506.09839.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09839v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09839v1', 'OctoNav: Towards Generalist Embodied Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chen Gao, Liankai Jin, Xingyu Peng, Jiazhao Zhang, Yue Deng, Annan Li, He Wang, Si Liu

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: 31 pages, 25 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOctoNavä»¥è§£å†³å¤šæ¨¡æ€å¯¼èˆªä»»åŠ¡çš„ç»Ÿä¸€æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«å¯¼èˆª` `å¤šæ¨¡æ€å­¦ä¹ ` `é€šç”¨ä»£ç†` `æ¨ç†èƒ½åŠ›` `æ··åˆè®­ç»ƒèŒƒå¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…·èº«å¯¼èˆªç ”ç©¶å¤šé›†ä¸­äºç‰¹å®šä»»åŠ¡ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯¼è‡´æ–¹æ³•å’Œæ•°æ®é›†çš„ç¢ç‰‡åŒ–ã€‚
2. æœ¬æ–‡æå‡ºOctoNav-Benchå’ŒOctoNav-R1ï¼Œæ—¨åœ¨å®ç°èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€å’Œå¤šèƒ½åŠ›æŒ‡ä»¤çš„é€šç”¨å¯¼èˆªä»£ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOctoNav-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾ç¤ºå‡ºè¾ƒå¼ºçš„æ¨ç†èƒ½åŠ›å’Œå¯¼èˆªæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«å¯¼èˆªæ˜¯å…·èº«äººå·¥æ™ºèƒ½çš„åŸºç¡€ï¼Œä½†ç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºä¸åŒçš„ä»»åŠ¡å’Œèƒ½åŠ›ï¼Œå¦‚ObjNavã€ImgNavå’ŒVLNï¼Œå¯¼è‡´æ•°æ®é›†å’Œæ–¹æ³•å„è‡ªç‹¬ç«‹ã€‚æœ¬æ–‡æå‡ºOctoNav-Benchå’ŒOctoNav-R1ï¼Œæ—¨åœ¨å®ç°é€šç”¨å¯¼èˆªä»£ç†ï¼Œèƒ½å¤Ÿå¤„ç†åŒ…å«å¤šæ¨¡æ€å’Œå¤šèƒ½åŠ›çš„è‡ªç”±å½¢å¼æŒ‡ä»¤ã€‚OctoNav-Benché€šè¿‡è®¾è®¡çš„æ³¨é‡Šæµç¨‹æ„å»ºï¼ŒåŒ…å«å¤šæ ·åŒ–çš„æŒ‡ä»¤-è½¨è¿¹å¯¹ï¼Œå¹¶å¼•å…¥Think-Before-Action (TBA-CoT) æ•°æ®é›†ï¼Œæä¾›è¡ŒåŠ¨èƒŒåçš„æ€ç»´è¿‡ç¨‹ã€‚OctoNav-R1åŸºäºMLLMsæ„å»ºï¼Œé€‚é…ä¸ºVLAç±»å‹æ¨¡å‹ï¼Œä»…åŸºäº2Dè§†è§‰è§‚å¯Ÿç”Ÿæˆä½çº§åŠ¨ä½œï¼Œå¹¶è®¾è®¡äº†æ··åˆè®­ç»ƒèŒƒå¼ï¼ˆHTPï¼‰ï¼ŒåŒ…å«å¤šä¸ªé˜¶æ®µï¼Œæœ€ç»ˆæ˜¾ç¤ºå‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…·èº«å¯¼èˆªé¢†åŸŸä¸­ä»»åŠ¡å’Œèƒ½åŠ›çš„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€å’Œå¤šèƒ½åŠ›çš„è‡ªç”±å½¢å¼æŒ‡ä»¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºOctoNav-Benchå’ŒOctoNav-R1ï¼Œæå‡ºä¸€ç§é€šç”¨çš„å¯¼èˆªä»£ç†ï¼Œèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œå¤æ‚çš„æŒ‡ä»¤ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šAction-/TBA-SFTã€Nav-GPROå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOnline RLï¼‰ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‰¹å®šçš„å­¦ä¹ ç­–ç•¥å’Œå¥–åŠ±æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥Think-Before-Action (TBA-CoT) æ•°æ®é›†ï¼Œæä¾›è¡ŒåŠ¨å‰çš„æ€ç»´è¿‡ç¨‹ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚æŒ‡ä»¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒTBA-SFTé˜¶æ®µåˆ©ç”¨TBA-CoTæ•°æ®é›†è¿›è¡Œå†·å¯åŠ¨å¾®è°ƒï¼ŒNav-GPROé˜¶æ®µåˆ™è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€ç»´èƒ½åŠ›ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ‰§è¡Œä½çº§åŠ¨ä½œæ—¶å…·å¤‡æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OctoNav-R1åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸æ¯”äºç°æœ‰æ–¹æ³•ï¼Œæ¨ç†èƒ½åŠ›å’Œå¯¼èˆªæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†å®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤æ‚æŒ‡ä»¤å¤„ç†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸåŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œäººç±»æŒ‡ä»¤ï¼Œæå‡äº¤äº’çš„è‡ªç„¶æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œé€šç”¨å¯¼èˆªä»£ç†æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.

