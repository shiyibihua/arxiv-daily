---
layout: default
title: Q-SAM2: Accurate Quantization for Segment Anything Model 2
---

# Q-SAM2: Accurate Quantization for Segment Anything Model 2

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09782" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09782v2</a>
  <a href="https://arxiv.org/pdf/2506.09782.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09782v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09782v2', 'Q-SAM2: Accurate Quantization for Segment Anything Model 2')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nicola Farronato, Florian Scheidegger, Mattia Rigotti, Cristiano Malossi, Michele Magno, Haotong Qin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-11-24)

**å¤‡æ³¨**: 22 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-SAM2ä»¥è§£å†³SAM2æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„é‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `æ¨¡å‹å‹ç¼©` `å›¾åƒåˆ†å‰²` `æ·±åº¦å­¦ä¹ ` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„SAM2æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶é¢ä¸´é«˜è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„æŒ‘æˆ˜ã€‚
2. Q-SAM2é€šè¿‡å¼•å…¥æ–¹å·®å‡å°‘æ ¡å‡†å’Œå¯å­¦ä¹ ç»Ÿè®¡è£å‰ªï¼Œæä¾›äº†ä¸€ç§æ–°çš„ä½ä½é‡åŒ–æ–¹æ³•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQ-SAM2åœ¨è§†é¢‘åˆ†å‰²åŸºå‡†ä¸Šæé«˜äº†9.7 pptçš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ¨¡å‹å¤§å°å‡å°‘äº†8å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Segment Anything Model 2 (SAM2) æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¯æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œä½†å…¶é«˜è®¡ç®—å’Œå†…å­˜æˆæœ¬åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶æ„æˆäº†ä¸»è¦éšœç¢ã€‚æœ¬æ–‡æå‡ºäº†Q-SAM2ï¼Œä¸€ç§å‡†ç¡®çš„ä½ä½é‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°é«˜å‹ç¼©ç‡å’Œé«˜ä¿çœŸåº¦ã€‚ä¸ºäº†è§£å†³é‡åŒ–è¿‡ç¨‹ä¸­ç”±äºæƒé‡å’Œæ¿€æ´»åˆ†å¸ƒå¤æ‚è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼ŒQ-SAM2å¼•å…¥äº†ä¸¤ä¸ªæ–°é¢–çš„è´¡çŒ®ï¼šæ–¹å·®å‡å°‘æ ¡å‡†ï¼ˆVRCï¼‰ï¼Œä¸€ç§é€šè¿‡æœ€å°åŒ–å°æ‰¹é‡çš„å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°æ¥å‡å°‘æƒé‡ç»Ÿè®¡æ–¹å·®çš„åˆå§‹åŒ–æ–¹æ³•ï¼›å¯å­¦ä¹ ç»Ÿè®¡è£å‰ªï¼ˆLSCï¼‰ï¼Œä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ åŠ¨é‡ç¨³å®šçš„è£å‰ªå› å­æ¥ç®¡ç†æƒé‡å’Œæ¿€æ´»ä¸­çš„å¼‚å¸¸å€¼ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒQ-SAM2åœ¨è¶…ä½2ä½é‡åŒ–æƒ…å†µä¸‹å®ç°äº†é«˜ç²¾åº¦æ¨ç†å’Œæ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„QATæ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Segment Anything Model 2 (SAM2) åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶çš„é«˜è®¡ç®—å’Œå†…å­˜æˆæœ¬é—®é¢˜ã€‚ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å¤„ç†æƒé‡å’Œæ¿€æ´»åˆ†å¸ƒæ—¶å®¹æ˜“å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQ-SAM2çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ–¹å·®å‡å°‘æ ¡å‡†ï¼ˆVRCï¼‰å’Œå¯å­¦ä¹ ç»Ÿè®¡è£å‰ªï¼ˆLSCï¼‰æ¥ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ã€‚VRCé€šè¿‡æœ€å°åŒ–å°æ‰¹é‡çš„å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°æ¥é™ä½æƒé‡çš„ç»Ÿè®¡æ–¹å·®ï¼Œè€ŒLSCåˆ™é€šè¿‡å­¦ä¹ ç¨³å®šçš„è£å‰ªå› å­æ¥å¤„ç†æƒé‡å’Œæ¿€æ´»ä¸­çš„å¼‚å¸¸å€¼ï¼Œä»è€Œæé«˜é‡åŒ–åçš„æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQ-SAM2çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æ–¹å·®å‡å°‘æ ¡å‡†æ¨¡å—ï¼Œé€šè¿‡å¯¹å°æ‰¹é‡æ•°æ®è¿›è¡Œåˆå§‹åŒ–æ¥é™ä½æƒé‡çš„æ–¹å·®ï¼›å…¶æ¬¡æ˜¯å¯å­¦ä¹ ç»Ÿè®¡è£å‰ªæ¨¡å—ï¼Œåœ¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è£å‰ªå› å­ï¼Œä»¥é€‚åº”æƒé‡å’Œæ¿€æ´»çš„å˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šQ-SAM2çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†æ–¹å·®å‡å°‘æ ¡å‡†å’Œå¯å­¦ä¹ ç»Ÿè®¡è£å‰ªï¼Œè¿™ä¸¤è€…æœ‰æ•ˆåœ°è§£å†³äº†ç°æœ‰é‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤æ‚æƒé‡å’Œæ¿€æ´»åˆ†å¸ƒæ—¶çš„ä¸è¶³ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†ç²¾åº¦å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒVRCæ¨¡å—çš„æŸå¤±å‡½æ•°é‡‡ç”¨äº†å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°ï¼Œè€ŒLSCæ¨¡å—åˆ™é€šè¿‡åŠ¨é‡å­¦ä¹ æœºåˆ¶æ¥ç¨³å®šè£å‰ªå› å­çš„æ›´æ–°ã€‚æ­¤å¤–ï¼ŒQ-SAM2åœ¨è¶…ä½2ä½é‡åŒ–æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒè¾ƒé«˜çš„æ¨¡å‹ç²¾åº¦ï¼Œæ˜¾ç¤ºå‡ºå…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Q-SAM2åœ¨è§†é¢‘åˆ†å‰²åŸºå‡†ä¸Šå®ç°äº†æœ€é«˜9.7 pptçš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶åœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­æé«˜äº†7.3 pptçš„mIoUï¼Œç›¸æ¯”äºæœ€ä½³ç«äº‰QATæ¨¡å‹ï¼Œæ¨¡å‹å¤§å°å‡å°‘äº†8å€ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Q-SAM2çš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œè¾¹ç¼˜è®¡ç®—ç­‰èµ„æºå—é™ç¯å¢ƒä¸­ã€‚é€šè¿‡é™ä½æ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼ŒQ-SAM2èƒ½å¤Ÿä½¿å¾—é«˜æ€§èƒ½çš„å›¾åƒåˆ†å‰²æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­å˜å¾—æ›´åŠ å¯è¡Œï¼Œæ¨åŠ¨æ™ºèƒ½è®¾å¤‡åœ¨è§†è§‰ç†è§£é¢†åŸŸçš„åº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Segment Anything Model 2 (SAM2) is a powerful foundation model for promptable segmentation. However, its high computational and memory costs are a major barrier to deployment on resource-constrained devices. In this paper, we present Q-SAM2, an accurate low-bit quantization method that achieves high compression and high fidelity. To address performance degradation arising from challenging weight and activation distributions during quantization, Q-SAM2 introduces two novel contributions: Variance-Reduced Calibration (VRC), an initialization method that reduces weight statistical variance by minimizing the Frobenius norm over a small calibration batch; and Learnable Statistical Clipping (LSC), a Quantization-Aware Training (QAT) method that learns momentum-stabilized clipping factors to manage outliers in weights and activations. Comprehensive experiments demonstrate that Q-SAM2 achieves highly accurate inference with substantial efficiency gains, significantly surpassing state-of-the-art general QAT schemes, particularly in the ultra-low 2-bit regime. Specifically, Q-SAM2 achieves an accuracy gain of up to 9.7 ppt in J&F on the video segmentation benchmark and 7.3 ppt in mIoU for instance segmentation over the best competing QAT model, all while achieving an 8x reduction in model size compared to the BF16 baseline.

