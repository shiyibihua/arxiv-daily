---
layout: default
title: UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting
---

# UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09952" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09952v1</a>
  <a href="https://arxiv.org/pdf/2506.09952.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09952v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09952v1', 'UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyi Wang, Yanran Zhang, Jie Zhou, Jiwen Lu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: Accepted to CVPR 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wangzy22/UniPre3D)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniPre3Dä»¥è§£å†³3Dç‚¹äº‘ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dç‚¹äº‘` `ç»Ÿä¸€é¢„è®­ç»ƒ` `é«˜æ–¯æº…å°„` `å‡ ä½•ç»“æ„` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dç‚¹äº‘æ¨¡å‹åœ¨å¤„ç†ä¸åŒå°ºåº¦çš„æ•°æ®æ—¶æ•ˆæœä¸ä½³ï¼Œç¼ºä¹ç»Ÿä¸€çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚
2. UniPre3Dé€šè¿‡é¢„æµ‹é«˜æ–¯åŸè¯­å’Œå¯å¾®åˆ†çš„é«˜æ–¯æº…å°„æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§ç»Ÿä¸€çš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒUniPre3Dåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç‚¹äº‘æ•°æ®çš„å°ºåº¦å¤šæ ·æ€§ç»™3Dè§†è§‰çš„ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚ç›®å‰ï¼Œå°šæ— æœ‰æ•ˆçš„ç»Ÿä¸€3Dæ¨¡å‹å’Œé¢„è®­ç»ƒæ–¹æ³•èƒ½å¤ŸåŒæ—¶é€‚ç”¨äºç‰©ä½“å’Œåœºæ™¯çº§ç‚¹äº‘ã€‚æœ¬æ–‡æå‡ºäº†UniPre3Dï¼Œè¿™æ˜¯é¦–ä¸ªå¯ä»¥æ— ç¼åº”ç”¨äºä»»æ„å°ºåº¦ç‚¹äº‘å’Œä»»æ„æ¶æ„3Dæ¨¡å‹çš„ç»Ÿä¸€é¢„è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é¢„æµ‹é«˜æ–¯åŸè¯­ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨å¯å¾®åˆ†çš„é«˜æ–¯æº…å°„æŠ€æœ¯è¿›è¡Œå›¾åƒæ¸²æŸ“ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„åƒç´ çº§ç›‘ç£å’Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥è°ƒèŠ‚é¢„è®­ç»ƒä»»åŠ¡çš„å¤æ‚æ€§å¹¶å¼•å¯¼æ¨¡å‹å…³æ³¨å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬æ•´åˆäº†æ¥è‡ªé¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„2Dç‰¹å¾ï¼Œä»¥èå…¥æˆç†Ÿçš„çº¹ç†çŸ¥è¯†ã€‚é€šè¿‡åœ¨å¤šç§ç‰©ä½“å’Œåœºæ™¯çº§ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æ‰€ææ–¹æ³•çš„æ™®éæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³3Dç‚¹äº‘æ•°æ®åœ¨å°ºåº¦å¤šæ ·æ€§ä¸‹çš„ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç‰©ä½“å’Œåœºæ™¯çº§ç‚¹äº‘ä¸Šç¼ºä¹æœ‰æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniPre3Dçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é¢„æµ‹é«˜æ–¯åŸè¯­ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶ç»“åˆå¯å¾®åˆ†çš„é«˜æ–¯æº…å°„æŠ€æœ¯è¿›è¡Œå›¾åƒæ¸²æŸ“ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„åƒç´ çº§ç›‘ç£å’Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ å‡ ä½•ç»“æ„å’Œçº¹ç†ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬é«˜æ–¯åŸè¯­é¢„æµ‹æ¨¡å—å’Œé«˜æ–¯æº…å°„æ¸²æŸ“æ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹é€šè¿‡è¾“å…¥ç‚¹äº‘æ•°æ®é¢„æµ‹é«˜æ–¯åŸè¯­ï¼Œç„¶ååˆ©ç”¨é«˜æ–¯æº…å°„æŠ€æœ¯å°†å…¶æ¸²æŸ“ä¸ºå›¾åƒï¼Œæœ€åé€šè¿‡åƒç´ çº§æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniPre3Dçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿé€‚ç”¨äºä»»æ„å°ºåº¦çš„ç‚¹äº‘å’Œä»»æ„æ¶æ„çš„3Dæ¨¡å‹ã€‚è¿™ä¸€æ–¹æ³•åœ¨å¤„ç†å¤æ‚å‡ ä½•ç»“æ„æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—åŒºåˆ«äºç°æœ‰çš„å•ä¸€ä»»åŠ¡é¢„è®­ç»ƒæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒUniPre3Dæ•´åˆäº†æ¥è‡ªé¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„2Dç‰¹å¾ï¼Œä»¥å¼•å…¥æˆç†Ÿçš„çº¹ç†çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†åƒç´ çº§çš„ç²¾ç¡®åº¦ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­çš„é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§ç‰©ä½“å’Œåœºæ™¯çº§ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUniPre3Dç›¸æ¯”äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯çš„å‡ ä½•ç»“æ„ç†è§£ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºï¼ŒéªŒè¯äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ™®é€‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniPre3Dçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡æä¾›ç»Ÿä¸€çš„3Dç‚¹äº‘è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥æå‡è¿™äº›é¢†åŸŸä¸­3Dè§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

