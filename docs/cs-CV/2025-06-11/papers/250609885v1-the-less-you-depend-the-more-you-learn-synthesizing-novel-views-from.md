---
layout: default
title: The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge
---

# The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09885" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09885v1</a>
  <a href="https://arxiv.org/pdf/2506.09885.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09885v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09885v1', 'The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoru Wang, Kai Ye, Yangyan Li, Wenzheng Chen, Baoquan Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://pku-vcl-geometry.github.io/Less3Depend/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°é¢–çš„è§†å›¾åˆæˆæ–¹æ³•ä»¥è§£å†³ç¨€ç–æ— å§¿æ€å›¾åƒçš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ–°è§†å›¾åˆæˆ` `ç¨€ç–å›¾åƒ` `æ— å§¿æ€å›¾åƒ` `3DçŸ¥è¯†` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `æ•°æ®é©±åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå¤§çš„3DçŸ¥è¯†å’ŒçœŸå®çš„ç›¸æœºå§¿æ€ï¼Œé™åˆ¶äº†å…¶åœ¨ç¨€ç–æˆ–æ— å§¿æ€å›¾åƒä¸Šçš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„NVSæ¡†æ¶ï¼Œæ—¨åœ¨æœ€å°åŒ–3Då½’çº³åå·®å’Œå§¿æ€ä¾èµ–ï¼Œä»è€Œç›´æ¥ä»ç¨€ç–2Då›¾åƒä¸­å­¦ä¹ éšå¼3Dæ„è¯†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ç”Ÿæˆçš„è§†å›¾è´¨é‡ä¸Šä¸ä¾èµ–å§¿æ€è¾“å…¥çš„æ–¹æ³•ç›¸å½“ï¼ŒéªŒè¯äº†å…¶åœ¨å¤§è§„æ¨¡æ•°æ®ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†å¯æ¨å¹¿çš„æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰é—®é¢˜ï¼Œæ—¨åœ¨ä»ç¨€ç–ç”šè‡³æ— å§¿æ€çš„2Då›¾åƒä¸­ç”Ÿæˆé€¼çœŸçš„æ–°è§†å›¾ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸€ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦ä»ä¸å®Œæ•´å’Œæ¨¡ç³Šçš„2Dè§‚å¯Ÿä¸­æ¨æ–­3Dç»“æ„ã€‚æ—©æœŸæ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå¤§çš„3DçŸ¥è¯†ï¼ŒåŒ…æ‹¬å°†æ˜¾å¼3Dè¡¨ç¤ºï¼ˆå¦‚NeRFæˆ–3DGSï¼‰åµŒå…¥ç½‘ç»œè®¾è®¡ä¸­ï¼Œä»¥åŠè¾“å…¥å’Œç›®æ ‡è§†å›¾çš„çœŸå®ç›¸æœºå§¿æ€ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåˆ†æ3DçŸ¥è¯†ï¼Œå‘ç°ä¾èµ–è¾ƒå°‘3DçŸ¥è¯†çš„æ–¹æ³•åœ¨æ•°æ®è§„æ¨¡å¢åŠ æ—¶æ€§èƒ½æå‡æ›´å¿«ï¼Œæœ€ç»ˆä¸ä¾èµ–3DçŸ¥è¯†çš„æ–¹æ³•ç›¸å½“ã€‚åŸºäºè¿™ä¸€è¶‹åŠ¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„NVSæ¡†æ¶ï¼Œæœ€å°åŒ–3Då½’çº³åå·®å’Œå§¿æ€ä¾èµ–ï¼Œå……åˆ†åˆ©ç”¨æ•°æ®è§„æ¨¡ï¼Œä»ç¨€ç–2Då›¾åƒä¸­ç›´æ¥å­¦ä¹ éšå¼3Dæ„è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„è§†å›¾åœ¨é€¼çœŸæ€§å’Œ3Dä¸€è‡´æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†æ•°æ®é©±åŠ¨èŒƒå¼çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»ç¨€ç–æˆ–æ— å§¿æ€çš„2Då›¾åƒç”Ÿæˆæ–°è§†å›¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¼ºå¤§çš„3DçŸ¥è¯†å’ŒçœŸå®çš„ç›¸æœºå§¿æ€ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æœ€å°åŒ–å¯¹3DçŸ¥è¯†çš„ä¾èµ–ï¼Œç›´æ¥ä»ç¨€ç–çš„2Då›¾åƒä¸­å­¦ä¹ éšå¼çš„3Dæ„è¯†ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®3Dä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€ç‰¹å¾æå–æ¨¡å—å’Œè§†å›¾åˆæˆæ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶ç¨€ç–çš„2Då›¾åƒï¼Œç‰¹å¾æå–æ¨¡å—é€šè¿‡æ·±åº¦å­¦ä¹ ç½‘ç»œæå–å›¾åƒç‰¹å¾ï¼Œè§†å›¾åˆæˆæ¨¡å—åˆ™åˆ©ç”¨æå–çš„ç‰¹å¾ç”Ÿæˆæ–°çš„è§†å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„NVSæ¡†æ¶ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹3DçŸ¥è¯†çš„ä¾èµ–ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–æ˜ç¡®3Dè¡¨ç¤ºçš„æŠ€æœ¯æœ‰æœ¬è´¨åŒºåˆ«ï¼Œå¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡æ•°æ®ç¯å¢ƒä¸‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆè§†å›¾çš„è´¨é‡ã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦ä»»ä½•å§¿æ€æ ‡æ³¨ï¼Œè¿›ä¸€æ­¥é™ä½äº†å¯¹å¤–éƒ¨ä¿¡æ¯çš„ä¾èµ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ç”Ÿæˆçš„è§†å›¾è´¨é‡ä¸Šä¸ä¾èµ–å§¿æ€è¾“å…¥çš„åŸºçº¿æ–¹æ³•ç›¸å½“ï¼Œä¸”åœ¨æ•°æ®è§„æ¨¡å¢åŠ æ—¶ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚å…·ä½“è€Œè¨€ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„3Dä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ä»¥åŠè®¡ç®—æœºå›¾å½¢å­¦ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜è´¨é‡çš„è§†å›¾åˆæˆæŠ€æœ¯ã€‚é€šè¿‡å‡å°‘å¯¹3DçŸ¥è¯†çš„ä¾èµ–ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†ç¨€ç–æ•°æ®æ—¶å…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We consider the problem of generalizable novel view synthesis (NVS), which aims to generate photorealistic novel views from sparse or even unposed 2D images without per-scene optimization. This task remains fundamentally challenging, as it requires inferring 3D structure from incomplete and ambiguous 2D observations. Early approaches typically rely on strong 3D knowledge, including architectural 3D inductive biases (e.g., embedding explicit 3D representations, such as NeRF or 3DGS, into network design) and ground-truth camera poses for both input and target views. While recent efforts have sought to reduce the 3D inductive bias or the dependence on known camera poses of input views, critical questions regarding the role of 3D knowledge and the necessity of circumventing its use remain under-explored. In this work, we conduct a systematic analysis on the 3D knowledge and uncover a critical trend: the performance of methods that requires less 3D knowledge accelerates more as data scales, eventually achieving performance on par with their 3D knowledge-driven counterparts, which highlights the increasing importance of reducing dependence on 3D knowledge in the era of large-scale data. Motivated by and following this trend, we propose a novel NVS framework that minimizes 3D inductive bias and pose dependence for both input and target views. By eliminating this 3D knowledge, our method fully leverages data scaling and learns implicit 3D awareness directly from sparse 2D images, without any 3D inductive bias or pose annotation during training. Extensive experiments demonstrate that our model generates photorealistic and 3D-consistent novel views, achieving even comparable performance with methods that rely on posed inputs, thereby validating the feasibility and effectiveness of our data-centric paradigm. Project page: https://pku-vcl-geometry.github.io/Less3Depend/ .

