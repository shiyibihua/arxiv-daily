---
layout: default
title: Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes
---

# Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09989" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09989v1</a>
  <a href="https://arxiv.org/pdf/2506.09989.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09989v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09989v1', 'Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiming Dou, Wonseok Oh, Yuqing Luo, Antonio Loquercio, Andrew Owens

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ , Code: https://github.com/Dou-Yiming/hearing_hands/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–¹æ³•ä»¥é¢„æµ‹3Dåœºæ™¯ä¸­æ‰‹éƒ¨äº¤äº’çš„å£°éŸ³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯é‡å»º` `å£°éŸ³ç”Ÿæˆ` `æ‰‹éƒ¨äº¤äº’` `ä¿®æ­£æµæ¨¡å‹` `è™šæ‹Ÿç°å®` `å¢å¼ºç°å®` `ç”¨æˆ·ä½“éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„3Dåœºæ™¯é‡å»ºæ–¹æ³•ç¼ºä¹äº’åŠ¨æ€§ï¼Œæ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿäººæ‰‹ä¸ç¯å¢ƒçš„å£°éŸ³äº¤äº’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å½•åˆ¶æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’çš„è§†é¢‘ï¼Œè®­ç»ƒæ¨¡å‹å°†æ‰‹éƒ¨è½¨è¿¹æ˜ å°„åˆ°å£°éŸ³ï¼Œå®ç°å£°éŸ³çš„ç”Ÿæˆã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç”Ÿæˆçš„å£°éŸ³åœ¨ææ–™ç‰¹æ€§å’ŒåŠ¨ä½œä¼ è¾¾ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸çœŸå®å£°éŸ³å‡ ä¹æ— æ³•åŒºåˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡é¢„æµ‹äººæ‰‹ä¸3Dåœºæ™¯äº¤äº’æ—¶äº§ç”Ÿçš„å£°éŸ³ï¼Œä½¿3Dåœºæ™¯é‡å»ºå˜å¾—æ›´åŠ äº’åŠ¨ã€‚æˆ‘ä»¬å½•åˆ¶äº†äººç±»åœ¨3Dåœºæ™¯ä¸­ç”¨æ‰‹æ“æ§ç‰©ä½“çš„è§†é¢‘ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŠ¨ä½œ-å£°éŸ³å¯¹è®­ç»ƒäº†ä¸€ä¸ªä¿®æ­£æµæ¨¡å‹ï¼Œå°†3Dæ‰‹éƒ¨è½¨è¿¹æ˜ å°„åˆ°ç›¸åº”çš„éŸ³é¢‘ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ‰‹åŠ¿åºåˆ—æŸ¥è¯¢æ¨¡å‹ï¼Œä»¥ä¼°è®¡å…¶å¯¹åº”çš„å£°éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„å£°éŸ³å‡†ç¡®ä¼ è¾¾äº†ææ–™ç‰¹æ€§å’ŒåŠ¨ä½œï¼Œä¸”å¸¸å¸¸ä¸çœŸå®å£°éŸ³éš¾ä»¥åŒºåˆ†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³3Dåœºæ™¯é‡å»ºä¸­ç¼ºä¹äº’åŠ¨æ€§çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿäººæ‰‹ä¸ç¯å¢ƒäº¤äº’æ—¶çš„å£°éŸ³ï¼Œé™åˆ¶äº†ç”¨æˆ·ä½“éªŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å½•åˆ¶äººæ‰‹ä¸ç‰©ä½“äº¤äº’çš„è§†é¢‘ï¼Œåˆ©ç”¨åŠ¨ä½œ-å£°éŸ³å¯¹è®­ç»ƒä¸€ä¸ªä¿®æ­£æµæ¨¡å‹ï¼Œå°†3Dæ‰‹éƒ¨è½¨è¿¹æ˜ å°„åˆ°ç›¸åº”çš„éŸ³é¢‘ï¼Œä»è€Œå®ç°å£°éŸ³çš„ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ¨¡å‹è®­ç»ƒå’Œå£°éŸ³ç”Ÿæˆä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œå½•åˆ¶æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’çš„è§†é¢‘ï¼Œæå–æ‰‹éƒ¨è½¨è¿¹å’Œå¯¹åº”å£°éŸ³ï¼›ç„¶åè®­ç»ƒä¿®æ­£æµæ¨¡å‹ï¼›æœ€åï¼Œåœ¨æµ‹è¯•é˜¶æ®µï¼Œç”¨æˆ·è¾“å…¥æ‰‹åŠ¿åºåˆ—ä»¥ç”Ÿæˆå£°éŸ³ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡ä¿®æ­£æµæ¨¡å‹å®ç°äº†æ‰‹éƒ¨åŠ¨ä½œä¸å£°éŸ³ä¹‹é—´çš„é«˜æ•ˆæ˜ å°„ï¼Œæ˜¾è‘—æå‡äº†å£°éŸ³ç”Ÿæˆçš„å‡†ç¡®æ€§å’ŒçœŸå®æ„Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å£°éŸ³ç”Ÿæˆçš„è´¨é‡ï¼Œå¹¶è®¾è®¡äº†é€‚åˆæ‰‹éƒ¨åŠ¨ä½œç‰¹å¾çš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„å£°éŸ³åœ¨ææ–™ç‰¹æ€§å’ŒåŠ¨ä½œä¼ è¾¾ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”¨æˆ·è¯„ä¼°ä¸­ï¼Œç”Ÿæˆå£°éŸ³ä¸çœŸå®å£°éŸ³çš„å¯åŒºåˆ†æ€§æä½ï¼Œè¾¾åˆ°äº†95%ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·çš„æ²‰æµ¸æ„Ÿå’Œäº’åŠ¨ä½“éªŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œæ¸¸æˆå¼€å‘ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å¼çš„äº¤äº’ä½“éªŒã€‚é€šè¿‡å®ç°å£°éŸ³çš„å®æ—¶ç”Ÿæˆï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·ä¸è™šæ‹Ÿç¯å¢ƒçš„äº’åŠ¨æ€§ï¼Œæå‡æ•´ä½“ä½“éªŒè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨æ•™è‚²ã€åŸ¹è®­å’Œå¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study the problem of making 3D scene reconstructions interactive by asking the following question: can we predict the sounds of human hands physically interacting with a scene? First, we record a video of a human manipulating objects within a 3D scene using their hands. We then use these action-sound pairs to train a rectified flow model to map 3D hand trajectories to their corresponding audio. At test time, a user can query the model for other actions, parameterized as sequences of hand poses, to estimate their corresponding sounds. In our experiments, we find that our generated sounds accurately convey material properties and actions, and that they are often indistinguishable to human observers from real sounds. Project page: https://www.yimingdou.com/hearing_hands/

