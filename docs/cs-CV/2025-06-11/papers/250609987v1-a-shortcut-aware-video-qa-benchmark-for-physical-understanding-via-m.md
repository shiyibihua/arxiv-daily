---
layout: default
title: A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs
---

# A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09987" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09987v1</a>
  <a href="https://arxiv.org/pdf/2506.09987.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09987v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09987v1', 'A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Benno Krojer, Mojtaba Komeili, Candace Ross, Quentin Garrido, Koustuv Sinha, Nicolas Ballas, Mahmoud Assran

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ€å°è§†é¢‘å¯¹åŸºå‡†ä»¥è§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹çš„ç‰©ç†ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `ç‰©ç†ç†è§£` `æ·å¾„æ„ŸçŸ¥` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†æµ‹è¯•å®¹æ˜“å—åˆ°è¡¨é¢çº¿ç´¢çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸å‡†ç¡®ã€‚
2. æå‡ºæœ€å°è§†é¢‘å¯¹åŸºå‡†ï¼Œé€šè¿‡å¼•å…¥è§†è§‰ç›¸ä¼¼ä½†ç­”æ¡ˆç›¸åçš„è§†é¢‘å¯¹ï¼Œå¢å¼ºæ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›è¯„ä¼°ã€‚
3. äººç±»åœ¨MVPåŸºå‡†ä¸Šçš„è¡¨ç°ä¸º92.9%ï¼Œè€Œæœ€ä½³å¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹çš„è¡¨ç°ä¸º40.2%ï¼Œæ˜¾è‘—é«˜äºéšæœºè¡¨ç°çš„25%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶ç©ºç†è§£å’Œæ¨ç†èƒ½åŠ›æ—¶ï¼Œå®¹æ˜“å—åˆ°åŸºäºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢çš„æ·å¾„è§£å†³æ–¹æ¡ˆçš„å½±å“ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥æœ€å°è§†é¢‘å¯¹ï¼ˆMVPï¼‰åŸºå‡†ï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„æ·å¾„æ„ŸçŸ¥è§†é¢‘é—®ç­”åŸºå‡†ï¼Œä»¥è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«55Kä¸ªé«˜è´¨é‡çš„å¤šé¡¹é€‰æ‹©è§†é¢‘é—®ç­”ç¤ºä¾‹ï¼Œä¸“æ³¨äºç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚æ¯ä¸ªç¤ºä¾‹éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–å¯¹ï¼Œè¦æ±‚æ¨¡å‹åœ¨ä¸¤ä¸ªè§†è§‰ç›¸ä¼¼ä½†ç­”æ¡ˆç›¸åçš„è§†é¢‘ä¸­éƒ½ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä»è€Œæœ‰æ•ˆé¿å…äº†ä¾èµ–è§†è§‰æˆ–æ–‡æœ¬åè§çš„æ·å¾„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­ç”±äºæ·å¾„è§£å†³æ–¹æ¡ˆå¯¼è‡´çš„æ€§èƒ½è™šé«˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢ï¼Œæ— æ³•çœŸå®åæ˜ æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æœ€å°è§†é¢‘å¯¹åŸºå‡†é€šè¿‡å¼•å…¥è§†è§‰ç›¸ä¼¼ä½†ç­”æ¡ˆç›¸åçš„è§†é¢‘å¯¹ï¼Œè¿«ä½¿æ¨¡å‹åœ¨ç†è§£ç‰©ç†ä¸–ç•Œæ—¶è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ¨ç†ï¼Œè€Œéä¾èµ–è¡¨é¢ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMVPåŸºå‡†åŒ…å«55Kä¸ªå¤šé¡¹é€‰æ‹©è§†é¢‘é—®ç­”ç¤ºä¾‹ï¼Œåˆ†ä¸ºå¤šä¸ªæ¨¡å—ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ ·æœ¬è®¾è®¡å’Œè¯„ä¼°æœºåˆ¶ã€‚æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–å¯¹ï¼Œç¡®ä¿æ¨¡å‹å¿…é¡»åœ¨ä¸¤ä¸ªè§†é¢‘ä¸­éƒ½ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥æœ€å°å˜åŒ–å¯¹çš„è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹æ— æ³•ä»…ä¾èµ–è§†è§‰æˆ–æ–‡æœ¬åè§ï¼Œä»è€Œæé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ ·æœ¬è®¾è®¡ä¸­ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘å¯¹åœ¨è§†è§‰ä¸Šç›¸ä¼¼ä½†åœ¨ç­”æ¡ˆä¸Šç›¸åï¼Œé‡‡ç”¨é«˜è´¨é‡çš„å¤šé¡¹é€‰æ‹©é¢˜è®¾è®¡ï¼Œç¡®ä¿é—®é¢˜çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MVPåŸºå‡†æµ‹è¯•ä¸­ï¼Œäººç±»çš„è¡¨ç°è¾¾åˆ°92.9%ï¼Œè€Œæœ€ä½³å¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹çš„è¡¨ç°ä»…ä¸º40.2%ï¼Œæ˜¾è‘—é«˜äºéšæœºè¡¨ç°çš„25%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒMVPåŸºå‡†æœ‰æ•ˆåœ°è¯„ä¼°äº†æ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æœºå™¨äººäº¤äº’å’Œæ™ºèƒ½è§†é¢‘åˆ†æç­‰ã€‚é€šè¿‡æé«˜è§†é¢‘è¯­è¨€æ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå®ç°æ›´é«˜æ•ˆçš„åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing benchmarks for assessing the spatio-temporal understanding and reasoning abilities of video language models are susceptible to score inflation due to the presence of shortcut solutions based on superficial visual or textual cues. This paper mitigates the challenges in accurately assessing model performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple shortcut-aware video QA benchmark for assessing the physical understanding of video language models. The benchmark is comprised of 55K high-quality multiple-choice video QA examples focusing on physical world understanding. Examples are curated from nine video data sources, spanning first-person egocentric and exocentric videos, robotic interaction data, and cognitive science intuitive physics benchmarks. To mitigate shortcut solutions that rely on superficial visual or textual cues and biases, each sample in MVP has a minimal-change pair -- a visually similar video accompanied by an identical question but an opposing answer. To answer a question correctly, a model must provide correct answers for both examples in the minimal-change pair; as such, models that solely rely on visual or textual biases would achieve below random performance. Human performance on MVP is 92.9\%, while the best open-source state-of-the-art video-language model achieves 40.2\% compared to random performance at 25\%.

