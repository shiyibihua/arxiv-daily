---
layout: default
title: InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions
---

# InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09984" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09984v1</a>
  <a href="https://arxiv.org/pdf/2506.09984.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09984v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09984v1', 'InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenzhi Wang, Jiaqi Yang, Jianwen Jiang, Chao Liang, Gaojie Lin, Zerong Zheng, Ceyuan Yang, Dahua Lin

**åˆ†ç±»**: cs.CV, cs.AI, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: TL;DR: The first multi-person dialogue video generation method from pairs of reference image and audio via explicit layout-aligned condition injection. See project page https://zhenzhiwang.github.io/interacthuman/ for more details

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInterActHumanæ¡†æ¶ä»¥è§£å†³å¤šæ¦‚å¿µäººç±»åŠ¨ç”»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŠ¨ç”»` `äººç±»åŠ¨ç”»` `åŒºåŸŸç‰¹å®šç»‘å®š` `éŸ³é¢‘æ¡ä»¶` `å¸ƒå±€æ§åˆ¶` `äººæœºäº¤äº’` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä»…èƒ½å¤„ç†å•ä¸€ä¸»ä½“åŠ¨ç”»ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤šæ¦‚å¿µå’Œå¤æ‚äººé™…äº’åŠ¨åœºæ™¯ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡åŒºåŸŸç‰¹å®šç»‘å®šæ¡ä»¶ï¼Œæ”¯æŒå¤šæ¦‚å¿µçš„æ—¶ç©ºåŠ¨ç”»ç”Ÿæˆï¼Œå¢å¼ºäº†æ§åˆ¶èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ˜¾å¼å¸ƒå±€æ§åˆ¶åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„è¡¨ç°ä¼˜äºéšå¼æ–¹æ³•åŠå…¶ä»–ç°æœ‰æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºå¤šæ¨¡æ€æ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ï¼‰çš„ç«¯åˆ°ç«¯äººç±»åŠ¨ç”»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…èƒ½å¯¹å•ä¸€ä¸»ä½“è¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œä¸”ä»¥å…¨å±€æ–¹å¼æ³¨å…¥æ¡ä»¶ï¼Œå¿½è§†äº†å¤šä¸ªæ¦‚å¿µåœ¨åŒä¸€è§†é¢‘ä¸­å‡ºç°çš„åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ”¾å¼ƒå•å®ä½“å‡è®¾ï¼Œå¼ºåˆ¶å®ç°æ¡ä»¶ä¸æ¯ä¸ªèº«ä»½çš„æ—¶ç©ºè¶³è¿¹çš„åŒºåŸŸç‰¹å®šç»‘å®šã€‚é€šè¿‡å‚è€ƒå›¾åƒï¼Œåˆ©ç”¨æ©ç é¢„æµ‹å™¨è‡ªåŠ¨æ¨æ–­å¸ƒå±€ä¿¡æ¯ï¼Œå¹¶å°†å±€éƒ¨éŸ³é¢‘æ¡ä»¶æ³¨å…¥ç›¸åº”åŒºåŸŸï¼Œä»è€Œç¡®ä¿å¸ƒå±€å¯¹é½çš„æ¨¡æ€åŒ¹é…ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—å¯æ§çš„å¤šæ¦‚å¿µäººç±»ä¸­å¿ƒè§†é¢‘çš„é«˜è´¨é‡ç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚å®è¯ç»“æœå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬æ˜¾å¼å¸ƒå±€æ§åˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€äººç±»åŠ¨ç”»æ–¹æ³•ä»…èƒ½å¤„ç†å•ä¸€ä¸»ä½“çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨å¤šæ¦‚å¿µå’Œå¤æ‚äº¤äº’åœºæ™¯ä¸­ç¼ºä¹ç²¾ç¡®æ§åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ”¾å¼ƒå•å®ä½“å‡è®¾ï¼Œæå‡ºåŒºåŸŸç‰¹å®šçš„æ¡ä»¶ç»‘å®šï¼Œç¡®ä¿æ¯ä¸ªèº«ä»½çš„æ—¶ç©ºè¶³è¿¹ä¸å¤šæ¨¡æ€æ¡ä»¶çš„ç²¾ç¡®åŒ¹é…ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„å¤šæ¦‚å¿µåŠ¨ç”»ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨æ©ç é¢„æµ‹å™¨ä»å‚è€ƒå›¾åƒä¸­æ¨æ–­å¸ƒå±€ä¿¡æ¯ï¼›å…¶æ¬¡ï¼Œå°†å±€éƒ¨éŸ³é¢‘æ¡ä»¶æ³¨å…¥åˆ°ç›¸åº”çš„åŒºåŸŸï¼›æœ€åï¼Œé€šè¿‡è¿­ä»£æ–¹å¼ä¼˜åŒ–æ¨¡æ€åŒ¹é…ï¼Œç”Ÿæˆæœ€ç»ˆåŠ¨ç”»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ˜¾å¼å¸ƒå±€æ§åˆ¶ï¼Œå…è®¸å¯¹å¤šæ¦‚å¿µçš„ç²¾ç¡®æ§åˆ¶ï¼Œä¸ç°æœ‰æ–¹æ³•çš„å…¨å±€å‡è®¾å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åŒºåŸŸåŒ¹é…ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šæ¨¡æ€è¾“å…¥çš„èåˆä¸å¤„ç†ï¼Œç¡®ä¿äº†ç”Ÿæˆæ•ˆæœçš„é«˜è´¨é‡ä¸ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„åŠ¨ç”»ç”Ÿæˆè´¨é‡æ˜¾è‘—ä¼˜äºéšå¼æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ˜¾å¼å¸ƒå±€æ§åˆ¶çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ä»¥åŠè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºåˆ›ä½œè€…æä¾›æ›´é«˜æ•ˆçš„åŠ¨ç”»ç”Ÿæˆå·¥å…·ï¼Œæå‡äººæœºäº¤äº’çš„çœŸå®æ„Ÿå’Œäº’åŠ¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨æ•™è‚²ã€å¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.

