---
layout: default
title: ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs
---

# ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10128" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.10128v1</a>
  <a href="https://arxiv.org/pdf/2506.10128.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10128v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10128v1', 'ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, Linjie Li, Furong Huang, Lijuan Wang

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ViCrit‰ª•Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËßÜËßâÊÑüÁü•ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ËßÜËßâÊÑüÁü•` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂõæÂÉèÁêÜËß£` `‰ªªÂä°È™åËØÅ` `Ê®°ÂûãËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâÊÑüÁü•‰ªªÂä°‰∏≠Áº∫‰πèÊó¢ÂÖ∑ÊåëÊàòÊÄßÂèàÊòéÁ°ÆÂèØÈ™åËØÅÁöÑ‰ªªÂä°ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊÄßËÉΩÊèêÂçá„ÄÇ
2. ViCritÈÄöËøáÊ≥®ÂÖ•ÂæÆÂ¶ôÁöÑËßÜËßâÊèèËø∞ÈîôËØØÔºåËÆ≠ÁªÉÊ®°ÂûãÂú®ÁªôÂÆöÂõæÂÉèÂíå‰øÆÊîπÊ†áÈ¢òÁöÑÊÉÖÂÜµ‰∏ãÂÆö‰ΩçÈîôËØØÔºåÊèê‰æõ‰∫ÜÊòéÁ°ÆÁöÑ‰∫åÂÖÉÂ•ñÂä±„ÄÇ
3. ‰ΩøÁî®ViCrit‰ªªÂä°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§öÁßçËßÜËßâËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÊèêÂçáÔºå‰∏îËøôÁßçÊèêÂçáÂú®ÊäΩË±°Êé®ÁêÜ‰ªªÂä°‰∏≠ÂêåÊ†∑ÊúâÊïà„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞Â≠¶Êé®ÁêÜÊàñ‰ª£Á†ÅÁîüÊàêÁ≠â‰ªªÂä°‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜËøôÁßçÊàêÂäüÊâ©Â±ïÂà∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑËßÜËßâÊÑüÁü•‰∏äÂèóÂà∞Áº∫‰πèÊó¢ÂÖ∑ÊåëÊàòÊÄßÂèàÊòéÁ°ÆÂèØÈ™åËØÅÁöÑËßÜËßâ‰ªªÂä°ÁöÑÈôêÂà∂„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜViCritÔºàËßÜËßâÊèèËø∞ÂπªËßâÊâπËØÑÔºâÔºåËøôÊòØ‰∏Ä‰∏™RL‰ª£ÁêÜ‰ªªÂä°ÔºåÊó®Âú®ËÆ≠ÁªÉVLMsÂÆö‰ΩçÊ≥®ÂÖ•Âà∞‰∫∫Á±ªÁºñÂÜôÁöÑÂõæÂÉèÊ†áÈ¢òÊÆµËêΩ‰∏≠ÁöÑÂæÆÂ¶ôÂêàÊàêËßÜËßâÂπªËßâ„ÄÇÈÄöËøáÂú®200Â≠óÁöÑÊ†áÈ¢ò‰∏≠Ê≥®ÂÖ•Âçï‰∏™ÂæÆÂ¶ôÁöÑËßÜËßâÊèèËø∞ÈîôËØØÔºåÊ®°ÂûãË¢´Ë¶ÅÊ±ÇÂú®ÁªôÂÆöÂõæÂÉèÂíå‰øÆÊîπÂêéÁöÑÊ†áÈ¢òÁöÑÊÉÖÂÜµ‰∏ãÔºåÂáÜÁ°ÆÊâæÂá∫Ë¢´Á†¥ÂùèÁöÑÈÉ®ÂàÜ„ÄÇ‰ΩøÁî®ViCrit‰ªªÂä°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§öÁßçVLÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÊèêÂçáÔºåÂπ∂‰∏îËøôÁßçÊèêÂçá‰∏ç‰ªÖÈôê‰∫éËá™ÁÑ∂ÂõæÂÉèËÆ≠ÁªÉÊï∞ÊçÆÔºåËøòÊâ©Â±ïÂà∞ÊäΩË±°ÂõæÂÉèÊé®ÁêÜÂíåËßÜËßâÊï∞Â≠¶ÔºåÊòæÁ§∫Âá∫Â≠¶‰π†ÊÑüÁü•ÁöÑÊΩúÂäõ„ÄÇ‰∏∫‰æø‰∫éËØÑ‰º∞ÔºåÊú¨ÊñáËøòÂºïÂÖ•‰∫ÜViCrit-BenchÔºå‰∏Ä‰∏™Á±ªÂà´Âπ≥Ë°°ÁöÑËØäÊñ≠Âü∫ÂáÜÔºåÁ≥ªÁªüÊÄßÂú∞Êé¢Êµã‰∏çÂêåÂõæÂÉèÈ¢ÜÂüüÂíåÈîôËØØÁ±ªÂûãÁöÑÊÑüÁü•ÈîôËØØ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÊÑüÁü•‰ªªÂä°‰∏≠Áº∫‰πèÊúâÊïà‰∏îÂèØÈ™åËØÅÁöÑËÆ≠ÁªÉ‰ªªÂä°ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÈöæ‰ª•Êèê‰æõÊòéÁ°ÆÁöÑÂèçÈ¶àÔºåÂØºËá¥Ê®°ÂûãÂ≠¶‰π†ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöViCritÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊ≥®ÂÖ•ÂæÆÂ∞èÁöÑËßÜËßâÊèèËø∞ÈîôËØØÔºåËÆ≠ÁªÉÊ®°ÂûãÂú®ÁªôÂÆöÂõæÂÉèÂíå‰øÆÊîπÂêéÁöÑÊ†áÈ¢ò‰∏≠ÂáÜÁ°ÆÂÆö‰ΩçËøô‰∫õÈîôËØØÔºå‰ªéËÄåÊèê‰æõÊ∏ÖÊô∞ÁöÑÂèçÈ¶àÊú∫Âà∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöViCritÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÁîüÊàêÂåÖÂê´ËßÜËßâÊèèËø∞ÈîôËØØÁöÑÂõæÂÉèÊ†áÈ¢òÔºåÂÖ∂Ê¨°ÊòØËÆ≠ÁªÉÊ®°ÂûãËØÜÂà´Ëøô‰∫õÈîôËØØ„ÄÇÊ®°ÂûãÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÂÖ∂ÂÆö‰ΩçËÉΩÂäõÔºå‰ΩøÁî®ÊòéÁ°ÆÁöÑÂ•ñÂä±Êú∫Âà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöViCritÁöÑÂàõÊñ∞Âú®‰∫éÂÖ∂ÈÄöËøáÂæÆÂ∞èÁöÑËßÜËßâÊèèËø∞ÈîôËØØÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºå‰ΩøÂæó‰ªªÂä°Êó¢ÂÖ∑ÊåëÊàòÊÄßÂèàÊòì‰∫éÈ™åËØÅ„ÄÇËøôÁßçÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑËßÜËßâÊÑüÁü•‰ªªÂä°‰∏çÂêåÔºåÂêéËÄÖÂæÄÂæÄ‰æùËµñ‰∫éÊ®°Á≥äÁöÑÂèçÈ¶à„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÊ®°ÂûãÁöÑÂ•ñÂä±Êú∫Âà∂ÈááÁî®‰∫åÂÖÉÂåπÈÖçÊñπÂºèÔºåÁ°Æ‰øùÂèçÈ¶àÁöÑÊòéÁ°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÁªìÊûÑÂíåÊçüÂ§±ÂáΩÊï∞ÁöÑÈÄâÊã©‰πüÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Â¢ûÂº∫ÂÖ∂ÂØπÁªÜÂæÆÈîôËØØÁöÑÊïèÊÑüÊÄß„ÄÇÈÄöËøáËøô‰∫õËÆæËÆ°ÔºåViCritÊúâÊïàÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑËßÜËßâÊÑüÁü•ËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠Ôºå‰ΩøÁî®ViCrit‰ªªÂä°ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§öÁßçËßÜËßâËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊäΩË±°ÂõæÂÉèÊé®ÁêÜÂíåËßÜËßâÊï∞Â≠¶‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏ä„ÄÇËøôË°®ÊòéViCrit‰∏ç‰ªÖÊúâÊïàÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÑüÁü•ËÉΩÂäõÔºåËøòÂ¢ûÂº∫‰∫ÜÂÖ∂Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®ÂõæÂÉèÊ†áÊ≥®„ÄÅËßÜËßâÈóÆÁ≠îÁ≥ªÁªü‰ª•ÂèäÂ§öÊ®°ÊÄÅÂÜÖÂÆπÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÑüÁü•ËÉΩÂäõÔºåViCritËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Êèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑËßÜËßâÁêÜËß£Âíå‰∫§‰∫í‰ΩìÈ™åÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊú™Êù•ÔºåViCritÁöÑÊ°ÜÊû∂‰πüÂèØËÉΩË¢´Êâ©Â±ïÂà∞ÂÖ∂‰ªñËßÜËßâ‰ªªÂä°‰∏≠ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÈÄöÁî®ÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.

