---
layout: default
title: Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding
---

# Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09522" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09522v2</a>
  <a href="https://arxiv.org/pdf/2506.09522.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09522v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09522v2', 'Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Beomsik Cho, Jaehyung Kim

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-10-11)

**å¤‡æ³¨**: Code available at https://github.com/bscho333/ReVisiT

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReVisiTä»¥è§£å†³è§†è§‰ä¿¡æ¯åœ¨LVLMè§£ç ä¸­çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€ä»»åŠ¡` `æ–‡æœ¬ç”Ÿæˆ` `è§†è§‰ä¿¡æ¯` `è§£ç ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­å¯¹è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨ä¸è¶³ï¼Œå¯¼è‡´é¢‘ç¹çš„å¹»è§‰ç°è±¡ã€‚
2. æœ¬æ–‡æå‡ºReVisiTæ–¹æ³•ï¼Œé€šè¿‡å‚è€ƒè§†è§‰æ ‡è®°æ¥å¼•å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œä¼˜åŒ–è§£ç è¿‡ç¨‹ã€‚
3. åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReVisiTæ˜¾è‘—æå‡äº†è§†è§‰å®šä½æ•ˆæœï¼Œå¹¶å°†è®¡ç®—æˆæœ¬é™ä½äº†å¤šè¾¾2å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ä¸è¯­è¨€ç†è§£ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè§†è§‰ä¿¡æ¯åœ¨æ¨¡å‹è§£ç è¿‡ç¨‹ä¸­çš„è´¡çŒ®ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ï¼Œå¯¼è‡´é¢‘ç¹çš„å¹»è§‰ç°è±¡ã€‚é€šè¿‡ä¸€ç³»åˆ—åˆ†æï¼Œç ”ç©¶å‘ç°è§†è§‰æ ‡è®°åœ¨å¹»è§‰å‘ç”Ÿæ—¶ä»æä¾›æœ‰æ„ä¹‰çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶ä¸”å…¶è¯­ä¹‰åœ¨é€‚å½“çš„è¯æ±‡çº¦æŸä¸‹å¯ä»¥åœ¨æ–‡æœ¬ç©ºé—´ä¸­æ˜¾ç°ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„æ— è®­ç»ƒè§£ç æ–¹æ³•ReVisiTï¼Œè¯¥æ–¹æ³•å‚è€ƒè§†è§‰æ ‡è®°æ¥æŒ‡å¯¼æ–‡æœ¬ç”Ÿæˆã€‚ReVisiTé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„çº¦æŸæ•£åº¦æœ€å°åŒ–åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„è§†è§‰æ ‡è®°ï¼Œå¹¶åˆ©ç”¨å…¶çº¦æŸæŠ•å½±æ¥ä¼˜åŒ–è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œæ›´å¥½åœ°èå…¥è§†è§‰è¯­ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReVisiTåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡è§†è§‰å®šä½ï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›çš„è§£ç åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½äº†å¤šè¾¾2å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­å¯¹è§†è§‰ä¿¡æ¯åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¹»è§‰ç°è±¡é¢‘ç¹å‡ºç°æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReVisiTæ–¹æ³•é€šè¿‡åŠ¨æ€é€‰æ‹©ä¸å½“å‰è§£ç ä¸Šä¸‹æ–‡æœ€ç›¸å…³çš„è§†è§‰æ ‡è®°ï¼Œåˆ©ç”¨å…¶è¯­ä¹‰ä¿¡æ¯æ¥ä¼˜åŒ–æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæå‡è§†è§‰è¯­ä¹‰çš„èå…¥æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReVisiTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰æ ‡è®°çš„é€‰æ‹©æ¨¡å—å’Œæ–‡æœ¬ç”Ÿæˆä¼˜åŒ–æ¨¡å—ã€‚é€‰æ‹©æ¨¡å—é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„çº¦æŸæ•£åº¦æœ€å°åŒ–æ¥åŠ¨æ€é€‰æ‹©è§†è§‰æ ‡è®°ï¼Œè€Œç”Ÿæˆä¼˜åŒ–æ¨¡å—åˆ™åˆ©ç”¨é€‰æ‹©çš„è§†è§‰æ ‡è®°å¯¹æ–‡æœ¬è¾“å‡ºåˆ†å¸ƒè¿›è¡Œçº¦æŸæŠ•å½±ã€‚

**å…³é”®åˆ›æ–°**ï¼šReVisiTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„è§£ç æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰æ ‡è®°çš„è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œæ˜¾è‘—æ”¹å–„äº†ç°æœ‰æ–¹æ³•åœ¨è§†è§‰ä¿¡æ¯åˆ©ç”¨ä¸Šçš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒReVisiTé‡‡ç”¨äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„çº¦æŸæ•£åº¦æœ€å°åŒ–ç­–ç•¥ï¼Œç¡®ä¿é€‰æ‹©çš„è§†è§‰æ ‡è®°ä¸å½“å‰è§£ç ä¸Šä¸‹æ–‡é«˜åº¦ç›¸å…³ï¼ŒåŒæ—¶é€šè¿‡çº¦æŸæŠ•å½±ä¼˜åŒ–è¾“å‡ºåˆ†å¸ƒï¼Œä»¥æ›´å¥½åœ°èå…¥è§†è§‰è¯­ä¹‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReVisiTåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æå‡äº†è§†è§‰å®šä½æ•ˆæœï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„è§£ç åŸºçº¿ï¼Œæ€§èƒ½ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½äº†å¤šè¾¾2å€ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆã€è§†è§‰é—®ç­”ç³»ç»Ÿä»¥åŠæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ï¼ŒReVisiTèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®çš„æ–‡æœ¬ç”Ÿæˆå’Œæ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½å¯¹äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–å†…å®¹åˆ›ä½œäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) achieve strong performance across multimodal tasks by integrating visual perception with language understanding. However, how vision information contributes to the model's decoding process remains under-explored, as reflected in frequent hallucinations. Through a series of analyses, we found that (i) vision tokens provide meaningful visual information even when hallucinations occur, and (ii) their semantics are encoded in the textual space and become explicit under appropriate vocabulary constraints. Building on these observations, we propose ReVisiT, a simple training-free decoding method that references vision tokens to guide text generation. Our approach leverages the semantic information embedded within vision tokens by projecting them into the text token distribution. Specifically, ReVisiT dynamically selects the most relevant vision token at each decoding step via context-aware constrained divergence minimization, and using its constrained projection to refine the output distribution to better incorporate visual semantics. Across five benchmarks on recent LVLMs, ReVisiT consistently enhances visual grounding with minimal computational overhead, and achieves competitive or superior results to state-of-the-art decoding baselines while reducing computational cost by up to $2\times$.

