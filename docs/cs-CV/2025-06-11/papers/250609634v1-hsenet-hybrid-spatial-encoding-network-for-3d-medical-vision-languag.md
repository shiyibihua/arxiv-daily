---
layout: default
title: HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding
---

# HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09634" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.09634v1</a>
  <a href="https://arxiv.org/pdf/2506.09634.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09634v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09634v1', 'HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yanzhao Shi, Xiaodan Zhang, Junzhong Ji, Haoning Jiang, Chengxin Zheng, Yinong Wang, Liangqiong Qu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-11

**Â§áÊ≥®**: 27 pages, 9 figures. arXiv admin note: text overlap with arXiv:2410.14200 by other authors

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/YanzhaoShi/HSENet)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HSENet‰ª•Ëß£ÂÜ≥3DÂåªÂ≠¶ÂõæÂÉèÁêÜËß£‰∏≠ÁöÑËØ≠Ë®Ä-ËßÜËßâËûçÂêàÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `3DÂåªÂ≠¶ÂõæÂÉè` `ËØ≠Ë®Ä-ËßÜËßâÁêÜËß£` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê∑±Â∫¶Â≠¶‰π†` `ÂåªÂ≠¶Êä•ÂëäÁîüÊàê` `ËßÜËßâÈóÆÁ≠î` `Á©∫Èó¥ÁºñÁ†Å` `Ê®°ÂûãÂØπÈΩê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠‰∫é2DÂåªÂ≠¶ÂõæÂÉèÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâÂ§çÊùÇÁöÑ3DËß£ÂâñÁªìÊûÑÔºåÂØºËá¥ËØØËØäÂíåËØäÊñ≠ÂπªËßâ„ÄÇ
2. HSENetÈÄöËøáÂèå3DËßÜËßâÁºñÁ†ÅÂô®ÂíåÁ©∫Èó¥ÊâìÂåÖÂô®ÔºåÂ¢ûÂº∫‰∫ÜÂØπ3DÂåªÂ≠¶ËßÜËßâÁ∫øÁ¥¢ÁöÑÊÑüÁü•ÂíåÊäïÂΩ±ËÉΩÂäõÔºåÊèêÂçáËØ≠Ë®Ä-ËßÜËßâÁêÜËß£ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåHSENetÂú®3DËØ≠Ë®Ä-ËßÜËßâÊ£ÄÁ¥¢„ÄÅÂåªÂ≠¶Êä•ÂëäÁîüÊàêÂíåËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëá™Âä®ÂåñÁöÑ3D CTËØäÊñ≠ËÉΩÂ§üÊèêÂçá‰∏¥Â∫äÂåªÁîüÁöÑÂÜ≥Á≠ñÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏ªË¶ÅÈõÜ‰∏≠‰∫é2DÂåªÂ≠¶ÂõæÂÉèÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂØπÂ§çÊùÇ3DËß£ÂâñÁªìÊûÑÁöÑÁêÜËß£ÔºåÂØºËá¥ÂØπÁªÜÂæÆÁóÖÂèòÁöÑËØØËß£ÂíåËØäÊñ≠ÂπªËßâ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÊ∑∑ÂêàÁ©∫Èó¥ÁºñÁ†ÅÁΩëÁªúÔºàHSENetÔºâÔºåÈÄöËøáÊúâÊïàÁöÑËßÜËßâÊÑüÁü•ÂíåÊäïÂΩ±ÊäÄÊúØÔºåÂà©Áî®‰∏∞ÂØåÁöÑ3DÂåªÂ≠¶ËßÜËßâÁ∫øÁ¥¢ÂÆûÁé∞ÂáÜÁ°ÆÁöÑËØ≠Ë®Ä-ËßÜËßâÁêÜËß£„ÄÇHSENetÈááÁî®Âèå3DËßÜËßâÁºñÁ†ÅÂô®ÔºåÊÑüÁü•ÂÖ®Â±Ä‰ΩìÁßØ‰∏ä‰∏ãÊñáÂíåÁªÜÁ≤íÂ∫¶Ëß£ÂâñÁªÜËäÇÔºåÂπ∂ÈÄöËøáÂèåÈò∂ÊÆµÂØπÈΩêËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑÁ©∫Èó¥ÊâìÂåÖÂô®ÈÄöËøáÂü∫‰∫éË¥®ÂøÉÁöÑÂéãÁº©ÔºåÂ∞ÜÈ´òÂàÜËæ®ÁéáÁöÑ3DÁ©∫Èó¥Âå∫ÂüüÊµìÁº©‰∏∫Á¥ßÂáëÁöÑ‰ø°ÊÅØËßÜËßâÊ†áËÆ∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHSENetÂú®3DËØ≠Ë®Ä-ËßÜËßâÊ£ÄÁ¥¢„ÄÅÂåªÂ≠¶Êä•ÂëäÁîüÊàêÂíåËßÜËßâÈóÆÁ≠îÁ≠â‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÈ¢ÜÂÖàÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®3DÂåªÂ≠¶ÂõæÂÉèÁêÜËß£‰∏≠ÁöÑ‰∏çË∂≥ÔºåÁâπÂà´ÊòØÂØπÂ§çÊùÇ3DËß£ÂâñÁªìÊûÑÁöÑÊçïÊçâËÉΩÂäõ‰∏çË∂≥ÔºåÂØºËá¥ÁöÑËØØËØäÂíåÂπªËßâÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHSENetÈÄöËøáÂºïÂÖ•Âèå3DËßÜËßâÁºñÁ†ÅÂô®Êù•ÂêåÊó∂ÊÑüÁü•ÂÖ®Â±Ä‰ΩìÁßØ‰∏ä‰∏ãÊñáÂíåÁªÜÁ≤íÂ∫¶Ëß£ÂâñÁªÜËäÇÔºåÁªìÂêàÁ©∫Èó¥ÊâìÂåÖÂô®ÊúâÊïàÂéãÁº©‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑËØ≠Ë®Ä-ËßÜËßâÁêÜËß£„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHSENetÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âèå3DËßÜËßâÁºñÁ†ÅÂô®ÂíåÁ©∫Èó¥ÊâìÂåÖÂô®„ÄÇÂèå3DËßÜËßâÁºñÁ†ÅÂô®Ë¥üË¥£ÊèêÂèñ3DÂåªÂ≠¶ÂõæÂÉèÁöÑÁâπÂæÅÔºåËÄåÁ©∫Èó¥ÊâìÂåÖÂô®ÂàôÂ∞ÜËøô‰∫õÁâπÂæÅÂéãÁº©‰∏∫‰ø°ÊÅØ‰∏∞ÂØåÁöÑËßÜËßâÊ†áËÆ∞Ôºå‰æø‰∫éÂêéÁª≠ÁöÑËØ≠Ë®ÄÁîüÊàê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHSENetÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂèå3DËßÜËßâÁºñÁ†ÅÂô®ÁöÑËÆæËÆ°ÂíåÁ©∫Èó¥ÊâìÂåÖÂô®ÁöÑÂºïÂÖ•Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜ3DÂåªÂ≠¶ÂõæÂÉèÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÂØπ2DÂõæÂÉèÁöÑ‰æùËµñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂèåÈò∂ÊÆµÂØπÈΩêÁ≠ñÁï•ËøõË°åÈ¢ÑËÆ≠ÁªÉÔºå‰ª•Á°Æ‰øùÁºñÁ†ÅÂô®ËÉΩÂ§üÂáÜÁ°ÆÁêÜËß£ÂåªÂ≠¶Êä•ÂëäÁöÑËØ≠‰πâ„ÄÇÂêåÊó∂ÔºåÁ©∫Èó¥ÊâìÂåÖÂô®‰ΩøÁî®Ë¥®ÂøÉÂéãÁº©ÊñπÊ≥ïÔºå‰ºòÂåñ‰∫Ü‰ø°ÊÅØÁöÑ‰º†ÈÄíÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHSENetÂú®3DËØ≠Ë®Ä-ËßÜËßâÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ËææÂà∞‰∫Ü39.85%ÁöÑR@100ÔºåËæÉÂü∫Á∫øÊèêÂçá‰∫Ü5.96%ÔºõÂú®3DÂåªÂ≠¶Êä•ÂëäÁîüÊàê‰∏≠ÔºåBLEU-4ÂæóÂàÜ‰∏∫24.01%ÔºåÊèêÂçá‰∫Ü8.01%ÔºõÂú®3DËßÜËßâÈóÆÁ≠î‰∏≠Ôºå‰∏ªË¶ÅÁ±ªÂà´ÂáÜÁ°ÆÁéáËææÂà∞‰∫Ü73.60%ÔºåÊèêÂçá‰∫Ü1.99%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê„ÄÅ‰∏¥Â∫äËæÖÂä©ËØäÊñ≠ÂíåÊô∫ËÉΩÂåªÁñóÁ≥ªÁªü„ÄÇÈÄöËøáÊèêÂçá3DÂåªÂ≠¶ÂõæÂÉèÁöÑËØ≠Ë®Ä-ËßÜËßâÁêÜËß£ËÉΩÂäõÔºåHSENetËÉΩÂ§üÂ∏ÆÂä©ÂåªÁîüÊõ¥ÂáÜÁ°ÆÂú∞ËøõË°åÁñæÁóÖËØäÊñ≠ÔºåÊîπÂñÑÊÇ£ËÄÖÁöÑÊ≤ªÁñóÊïàÊûúÔºåÊú™Êù•ÂèØËÉΩÂú®ÂåªÁñóË°å‰∏ö‰∫ßÁîüÊ∑±ËøúÁöÑÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based decisions by enhancing diagnostic accuracy and workflow efficiency. While multimodal large language models (MLLMs) exhibit promising performance in visual-language understanding, existing methods mainly focus on 2D medical images, which fundamentally limits their ability to capture complex 3D anatomical structures. This limitation often leads to misinterpretation of subtle pathologies and causes diagnostic hallucinations. In this paper, we present Hybrid Spatial Encoding Network (HSENet), a framework that exploits enriched 3D medical visual cues by effective visual perception and projection for accurate and robust vision-language understanding. Specifically, HSENet employs dual-3D vision encoders to perceive both global volumetric contexts and fine-grained anatomical details, which are pre-trained by dual-stage alignment with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient multimodal projector that condenses high-resolution 3D spatial regions into a compact set of informative visual tokens via centroid-based compression. By assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly perceive and transfer hybrid visual representations to LLM's semantic space, facilitating accurate diagnostic text generation. Experimental results demonstrate that our method achieves state-of-the-art performance in 3D language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering (73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness. Our code is available at https://github.com/YanzhaoShi/HSENet.

