---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-11
---

# cs.CVï¼ˆ2025-06-11ï¼‰

ğŸ“Š å…± **40** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250611155v1-evaluating-multimodal-large-language-models-on-video-captioning-via-.html">Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search</a></td>
  <td>æå‡ºAutoCaptionæ¡†æ¶ä»¥è§£å†³è§†é¢‘å­—å¹•ç”Ÿæˆè¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11155v1" data-paper-url="./papers/250611155v1-evaluating-multimodal-large-language-models-on-video-captioning-via-.html" onclick="toggleFavorite(this, '2506.11155v1', 'Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250610100v1-efficientvla-training-free-acceleration-and-compression-for-vision-l.html">EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models</a></td>
  <td>æå‡ºEfficientVLAä»¥è§£å†³VLAæ¨¡å‹çš„åŠ é€Ÿä¸å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10100v1" data-paper-url="./papers/250610100v1-efficientvla-training-free-acceleration-and-compression-for-vision-l.html" onclick="toggleFavorite(this, '2506.10100v1', 'EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250609958v1-kvasir-vqa-x1-a-multimodal-dataset-for-medical-reasoning-and-robust-.html">Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy</a></td>
  <td>æå‡ºKvasir-VQA-x1ä»¥è§£å†³åŒ»ç–—è§†è§‰é—®ç­”æ•°æ®é›†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09958v1" data-paper-url="./papers/250609958v1-kvasir-vqa-x1-a-multimodal-dataset-for-medical-reasoning-and-robust-.html" onclick="toggleFavorite(this, '2506.09958v1', 'Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250609839v1-octonav-towards-generalist-embodied-navigation.html">OctoNav: Towards Generalist Embodied Navigation</a></td>
  <td>æå‡ºOctoNavä»¥è§£å†³å¤šæ¨¡æ€å¯¼èˆªä»»åŠ¡çš„ç»Ÿä¸€æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">VLA</span> <span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09839v1" data-paper-url="./papers/250609839v1-octonav-towards-generalist-embodied-navigation.html" onclick="toggleFavorite(this, '2506.09839v1', 'OctoNav: Towards Generalist Embodied Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250609982v1-animateanymesh-a-feed-forward-4d-foundation-model-for-text-driven-un.html">AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></td>
  <td>æå‡ºAnimateAnyMeshä»¥è§£å†³é«˜è´¨é‡3Dæ¨¡å‹åŠ¨ç”»ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09982v1" data-paper-url="./papers/250609982v1-animateanymesh-a-feed-forward-4d-foundation-model-for-text-driven-un.html" onclick="toggleFavorite(this, '2506.09982v1', 'AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250609745v1-class-similarity-based-multimodal-classification-under-heterogeneous.html">Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets</a></td>
  <td>æå‡ºåŸºäºç±»åˆ«ç›¸ä¼¼æ€§çš„å¤šæ¨¡æ€åˆ†ç±»æ–¹æ³•ä»¥è§£å†³å¼‚æ„ç±»åˆ«é›†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09745v1" data-paper-url="./papers/250609745v1-class-similarity-based-multimodal-classification-under-heterogeneous.html" onclick="toggleFavorite(this, '2506.09745v1', 'Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250610230v2-prompt-guided-latent-diffusion-with-predictive-class-conditioning-fo.html">Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation</a></td>
  <td>æå‡ºCCELLAä»¥è§£å†³åŒ»å­¦å½±åƒæ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10230v2" data-paper-url="./papers/250610230v2-prompt-guided-latent-diffusion-with-predictive-class-conditioning-fo.html" onclick="toggleFavorite(this, '2506.10230v2', 'Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250609634v1-hsenet-hybrid-spatial-encoding-network-for-3d-medical-vision-languag.html">HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding</a></td>
  <td>æå‡ºHSENetä»¥è§£å†³3DåŒ»å­¦å›¾åƒç†è§£ä¸­çš„è¯­è¨€-è§†è§‰èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09634v1" data-paper-url="./papers/250609634v1-hsenet-hybrid-spatial-encoding-network-for-3d-medical-vision-languag.html" onclick="toggleFavorite(this, '2506.09634v1', 'HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250609522v2-revisit-what-you-see-disclose-language-prior-in-vision-tokens-for-lv.html">Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding</a></td>
  <td>æå‡ºReVisiTä»¥è§£å†³è§†è§‰ä¿¡æ¯åœ¨LVLMè§£ç ä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09522v2" data-paper-url="./papers/250609522v2-revisit-what-you-see-disclose-language-prior-in-vision-tokens-for-lv.html" onclick="toggleFavorite(this, '2506.09522v2', 'Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250611156v1-digitization-of-document-and-information-extraction-using-ocr.html">Digitization of Document and Information Extraction using OCR</a></td>
  <td>æå‡ºç»“åˆOCRä¸å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ä»¥æå‡æ–‡æ¡£ä¿¡æ¯æå–å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11156v1" data-paper-url="./papers/250611156v1-digitization-of-document-and-information-extraction-using-ocr.html" onclick="toggleFavorite(this, '2506.11156v1', 'Digitization of Document and Information Extraction using OCR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250609814v2-dreamcs-geometry-aware-text-to-3d-generation-with-unpaired-3d-reward.html">DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision</a></td>
  <td>æå‡ºDreamCSä»¥è§£å†³æ–‡æœ¬åˆ°3Dç”Ÿæˆä¸­çš„å‡ ä½•åå·®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09814v2" data-paper-url="./papers/250609814v2-dreamcs-geometry-aware-text-to-3d-generation-with-unpaired-3d-reward.html" onclick="toggleFavorite(this, '2506.09814v2', 'DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250609782v2-q-sam2-accurate-quantization-for-segment-anything-model-2.html">Q-SAM2: Accurate Quantization for Segment Anything Model 2</a></td>
  <td>æå‡ºQ-SAM2ä»¥è§£å†³SAM2æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„é‡åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09782v2" data-paper-url="./papers/250609782v2-q-sam2-accurate-quantization-for-segment-anything-model-2.html" onclick="toggleFavorite(this, '2506.09782v2', 'Q-SAM2: Accurate Quantization for Segment Anything Model 2')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250611148v1-llm-to-phy3d-physically-conform-online-3d-object-generation-with-llm.html">LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs</a></td>
  <td>æå‡ºLLM-to-Phy3Dä»¥è§£å†³ç‰©ç†çº¦æŸä¸‹çš„3Då¯¹è±¡ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11148v1" data-paper-url="./papers/250611148v1-llm-to-phy3d-physically-conform-online-3d-object-generation-with-llm.html" onclick="toggleFavorite(this, '2506.11148v1', 'LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250609836v1-dynasplat-dynamic-static-gaussian-splatting-with-hierarchical-motion.html">DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction</a></td>
  <td>æå‡ºDynaSplatä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09836v1" data-paper-url="./papers/250609836v1-dynasplat-dynamic-static-gaussian-splatting-with-hierarchical-motion.html" onclick="toggleFavorite(this, '2506.09836v1', 'DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250609518v2-haif-gs-hierarchical-and-induced-flow-guided-gaussian-splatting-for-.html">HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene</a></td>
  <td>æå‡ºHAIF-GSä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09518v2" data-paper-url="./papers/250609518v2-haif-gs-hierarchical-and-induced-flow-guided-gaussian-splatting-for-.html" onclick="toggleFavorite(this, '2506.09518v2', 'HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250609881v3-leveraging-depth-and-language-for-open-vocabulary-domain-generalized.html">Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</a></td>
  <td>æå‡ºVireoæ¡†æ¶ä»¥è§£å†³å¼€æ”¾è¯æ±‡é¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09881v3" data-paper-url="./papers/250609881v3-leveraging-depth-and-language-for-open-vocabulary-domain-generalized.html" onclick="toggleFavorite(this, '2506.09881v3', 'Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250609784v1-accurate-and-efficient-zero-shot-6d-pose-estimation-with-frozen-foun.html">Accurate and efficient zero-shot 6D pose estimation with frozen foundation models</a></td>
  <td>æå‡ºFreeZeV2ä»¥è§£å†³é›¶-shot 6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09784v1" data-paper-url="./papers/250609784v1-accurate-and-efficient-zero-shot-6d-pose-estimation-with-frozen-foun.html" onclick="toggleFavorite(this, '2506.09784v1', 'Accurate and efficient zero-shot 6D pose estimation with frozen foundation models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250609663v1-self-supervised-multi-part-articulated-objects-modeling-via-deformab.html">Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</a></td>
  <td>æå‡ºDeGSSæ¡†æ¶ä»¥è§£å†³å¤šéƒ¨ä»¶å…³èŠ‚ç‰©ä½“å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09663v1" data-paper-url="./papers/250609663v1-self-supervised-multi-part-articulated-objects-modeling-via-deformab.html" onclick="toggleFavorite(this, '2506.09663v1', 'Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250609534v2-gaussian-herding-across-pens-an-optimal-transport-perspective-on-glo.html">Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS</a></td>
  <td>æå‡ºå…¨å±€é«˜æ–¯æ··åˆç®€åŒ–æ–¹æ³•ä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“çš„å†…å­˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09534v2" data-paper-url="./papers/250609534v2-gaussian-herding-across-pens-an-optimal-transport-perspective-on-glo.html" onclick="toggleFavorite(this, '2506.09534v2', 'Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250609919v2-metrichmsrmetric-human-mesh-and-scene-recovery-from-monocular-images.html">MetricHMSR:Metric Human Mesh and Scene Recovery from Monocular Images</a></td>
  <td>æå‡ºMetricHMSRä»¥è§£å†³å•ç›®å›¾åƒä¸­çš„äººç±»å§¿æ€ä¸åœºæ™¯æ¢å¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09919v2" data-paper-url="./papers/250609919v2-metrichmsrmetric-human-mesh-and-scene-recovery-from-monocular-images.html" onclick="toggleFavorite(this, '2506.09919v2', 'MetricHMSR:Metric Human Mesh and Scene Recovery from Monocular Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250609885v1-the-less-you-depend-the-more-you-learn-synthesizing-novel-views-from.html">The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge</a></td>
  <td>æå‡ºä¸€ç§æ–°é¢–çš„è§†å›¾åˆæˆæ–¹æ³•ä»¥è§£å†³ç¨€ç–æ— å§¿æ€å›¾åƒçš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09885v1" data-paper-url="./papers/250609885v1-the-less-you-depend-the-more-you-learn-synthesizing-novel-views-from.html" onclick="toggleFavorite(this, '2506.09885v1', 'The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250609989v1-hearing-hands-generating-sounds-from-physical-interactions-in-3d-sce.html">Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes</a></td>
  <td>æå‡ºä¸€ç§æ–¹æ³•ä»¥é¢„æµ‹3Dåœºæ™¯ä¸­æ‰‹éƒ¨äº¤äº’çš„å£°éŸ³</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09989v1" data-paper-url="./papers/250609989v1-hearing-hands-generating-sounds-from-physical-interactions-in-3d-sce.html" onclick="toggleFavorite(this, '2506.09989v1', 'Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250609952v1-unipre3d-unified-pre-training-of-3d-point-cloud-models-with-cross-mo.html">UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting</a></td>
  <td>æå‡ºUniPre3Dä»¥è§£å†³3Dç‚¹äº‘ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09952v1" data-paper-url="./papers/250609952v1-unipre3d-unified-pre-training-of-3d-point-cloud-models-with-cross-mo.html" onclick="toggleFavorite(this, '2506.09952v1', 'UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250609736v2-revisiting-visual-understanding-in-multimodal-reasoning-through-a-le.html">Revisiting Visual Understanding in Multimodal Reasoning through a Lens of Image Perturbation</a></td>
  <td>æå‡ºè§†è§‰æ‰°åŠ¨æ¡†æ¶ä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09736v2" data-paper-url="./papers/250609736v2-revisiting-visual-understanding-in-multimodal-reasoning-through-a-le.html" onclick="toggleFavorite(this, '2506.09736v2', 'Revisiting Visual Understanding in Multimodal Reasoning through a Lens of Image Perturbation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250609883v2-3d-aware-vision-language-models-fine-tuning-with-geometric-distillat.html">3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation</a></td>
  <td>æå‡ºå‡ ä½•è’¸é¦æ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„3Dç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">VGGT</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09883v2" data-paper-url="./papers/250609883v2-3d-aware-vision-language-models-fine-tuning-with-geometric-distillat.html" onclick="toggleFavorite(this, '2506.09883v2', '3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250609565v2-semanticsplat-feed-forward-3d-scene-understanding-with-language-awar.html">SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields</a></td>
  <td>æå‡ºSemanticSplatä»¥è§£å†³3Dåœºæ™¯ç†è§£ä¸­çš„è¯­ä¹‰ä¸å‡ ä½•å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">open-vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09565v2" data-paper-url="./papers/250609565v2-semanticsplat-feed-forward-3d-scene-understanding-with-language-awar.html" onclick="toggleFavorite(this, '2506.09565v2', 'SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250611167v1-towards-a-general-purpose-foundation-model-for-fmri-analysis.html">Towards a general-purpose foundation model for fMRI analysis</a></td>
  <td>æå‡ºNeuroSTORMä»¥è§£å†³fMRIåˆ†æçš„å¯é‡å¤æ€§ä¸è¿ç§»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11167v1" data-paper-url="./papers/250611167v1-towards-a-general-purpose-foundation-model-for-fmri-analysis.html" onclick="toggleFavorite(this, '2506.11167v1', 'Towards a general-purpose foundation model for fMRI analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250610128v1-vicrit-a-verifiable-reinforcement-learning-proxy-task-for-visual-per.html">ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs</a></td>
  <td>æå‡ºViCritä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10128v1" data-paper-url="./papers/250610128v1-vicrit-a-verifiable-reinforcement-learning-proxy-task-for-visual-per.html" onclick="toggleFavorite(this, '2506.10128v1', 'ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250609995v3-playerone-egocentric-world-simulator.html">PlayerOne: Egocentric World Simulator</a></td>
  <td>æå‡ºPlayerOneä»¥è§£å†³çœŸå®ä¸–ç•Œæ¨¡æ‹Ÿçš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09995v3" data-paper-url="./papers/250609995v3-playerone-egocentric-world-simulator.html" onclick="toggleFavorite(this, '2506.09995v3', 'PlayerOne: Egocentric World Simulator')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250611164v2-synthetic-geology-structural-geology-meets-deep-learning.html">Synthetic Geology: Structural Geology Meets Deep Learning</a></td>
  <td>æå‡ºStructuralGeoä»¥è§£å†³åœ°è´¨é‡å»ºä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.11164v2" data-paper-url="./papers/250611164v2-synthetic-geology-structural-geology-meets-deep-learning.html" onclick="toggleFavorite(this, '2506.11164v2', 'Synthetic Geology: Structural Geology Meets Deep Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250609834v2-mmme-a-spontaneous-multi-modal-micro-expression-dataset-enabling-vis.html">MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion</a></td>
  <td>æå‡ºMMMEæ•°æ®é›†ä»¥è§£å†³å¤šæ¨¡æ€å¾®è¡¨æƒ…åˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09834v2" data-paper-url="./papers/250609834v2-mmme-a-spontaneous-multi-modal-micro-expression-dataset-enabling-vis.html" onclick="toggleFavorite(this, '2506.09834v2', 'MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/250609965v2-reinforcing-spatial-reasoning-in-vision-language-models-with-interwo.html">Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing</a></td>
  <td>æå‡ºé€šè¿‡ç»˜å›¾å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09965v2" data-paper-url="./papers/250609965v2-reinforcing-spatial-reasoning-in-vision-language-models-with-interwo.html" onclick="toggleFavorite(this, '2506.09965v2', 'Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250609699v1-chip-a-multi-sensor-dataset-for-6d-pose-estimation-of-chairs-in-indu.html">CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings</a></td>
  <td>æå‡ºCHIPæ•°æ®é›†ä»¥è§£å†³å·¥ä¸šç¯å¢ƒä¸­æ¤…å­çš„6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09699v1" data-paper-url="./papers/250609699v1-chip-a-multi-sensor-dataset-for-6d-pose-estimation-of-chairs-in-indu.html" onclick="toggleFavorite(this, '2506.09699v1', 'CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250609677v2-benchmarking-gaslighting-negation-attacks-against-reasoning-models.html">Benchmarking Gaslighting Negation Attacks Against Reasoning Models</a></td>
  <td>æå‡ºGaslightingBench-Rä»¥è¯„ä¼°æ¨ç†æ¨¡å‹å¯¹å¦å®šæ”»å‡»çš„æŠµæŠ—åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09677v2" data-paper-url="./papers/250609677v2-benchmarking-gaslighting-negation-attacks-against-reasoning-models.html" onclick="toggleFavorite(this, '2506.09677v2', 'Benchmarking Gaslighting Negation Attacks Against Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250610085v4-vita-zero-shot-value-functions-via-test-time-adaptation-of-vision-la.html">VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models</a></td>
  <td>æå‡ºVITAä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶-shotä»·å€¼å‡½æ•°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10085v4" data-paper-url="./papers/250610085v4-vita-zero-shot-value-functions-via-test-time-adaptation-of-vision-la.html" onclick="toggleFavorite(this, '2506.10085v4', 'VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250609343v1-checkmanual-a-new-challenge-and-benchmark-for-manual-based-appliance.html">CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation</a></td>
  <td>æå‡ºCheckManualåŸºå‡†ä»¥è§£å†³æ‰‹åŠ¨ç”µå™¨æ“ä½œçš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09343v1" data-paper-url="./papers/250609343v1-checkmanual-a-new-challenge-and-benchmark-for-manual-based-appliance.html" onclick="toggleFavorite(this, '2506.09343v1', 'CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/250610082v5-lora-edit-controllable-first-frame-guided-video-editing-via-mask-awa.html">LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning</a></td>
  <td>æå‡ºåŸºäºæ©è†œçš„LoRAå¾®è°ƒæ–¹æ³•ä»¥å®ç°çµæ´»çš„è§†é¢‘ç¼–è¾‘</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.10082v5" data-paper-url="./papers/250610082v5-lora-edit-controllable-first-frame-guided-video-editing-via-mask-awa.html" onclick="toggleFavorite(this, '2506.10082v5', 'LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250609735v1-mpfnet-a-multi-prior-fusion-network-with-a-progressive-training-stra.html">MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition</a></td>
  <td>æå‡ºMPFNetä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­çš„å¤šæºä¿¡æ¯èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09735v1" data-paper-url="./papers/250609735v1-mpfnet-a-multi-prior-fusion-network-with-a-progressive-training-stra.html" onclick="toggleFavorite(this, '2506.09735v1', 'MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250609984v1-interacthuman-multi-concept-human-animation-with-layout-aligned-audi.html">InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions</a></td>
  <td>æå‡ºInterActHumanæ¡†æ¶ä»¥è§£å†³å¤šæ¦‚å¿µäººç±»åŠ¨ç”»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09984v1" data-paper-url="./papers/250609984v1-interacthuman-multi-concept-human-animation-with-layout-aligned-audi.html" onclick="toggleFavorite(this, '2506.09984v1', 'InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/250609987v1-a-shortcut-aware-video-qa-benchmark-for-physical-understanding-via-m.html">A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs</a></td>
  <td>æå‡ºæœ€å°è§†é¢‘å¯¹åŸºå‡†ä»¥è§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹çš„ç‰©ç†ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09987v1" data-paper-url="./papers/250609987v1-a-shortcut-aware-video-qa-benchmark-for-physical-understanding-via-m.html" onclick="toggleFavorite(this, '2506.09987v1', 'A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)