---
layout: default
title: DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding
---

# DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23196" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23196v1</a>
  <a href="https://arxiv.org/pdf/2506.23196.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23196v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23196v1', 'DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mona Ahmadian, Amir Shirian, Frank Guerin, Andrew Gilbert

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDELæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘ä¸­çš„å¯†é›†äº‹ä»¶å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å¯†é›†äº‹ä»¶å®šä½` `å¤šæ¨¡æ€èåˆ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `åŠ¨ä½œè¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘ä¸­çš„é‡å äº‹ä»¶å’Œå¤æ‚æ—¶é—´ä¾èµ–æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´å¤šæ¨¡æ€äº¤äº’å»ºæ¨¡å›°éš¾ã€‚
2. DELæ¡†æ¶é€šè¿‡éŸ³é¢‘ä¸è§†è§‰ç‰¹å¾çš„å¯¹é½å’Œå¤šæ¨¡æ€äº¤äº’ç»†åŒ–æ¨¡å—ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€ä¾èµ–å»ºæ¨¡èƒ½åŠ›ï¼Œæå‡äº†åŠ¨ä½œå®šä½ç²¾åº¦ã€‚
3. åœ¨UnAV-100ã€THUMOS14ã€ActivityNet 1.3å’ŒEPIC-Kitchens-100ç­‰æ•°æ®é›†ä¸Šï¼ŒDELæ¡†æ¶çš„mAPå¹³å‡æå‡æ˜¾è‘—ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œçš„è§†é¢‘é€šå¸¸åŒ…å«é‡å äº‹ä»¶å’Œå¤æ‚çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä½¿å¾—å¤šæ¨¡æ€äº¤äº’å»ºæ¨¡å˜å¾—å°¤ä¸ºå›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†DELï¼Œä¸€ä¸ªç”¨äºå¯†é›†è¯­ä¹‰åŠ¨ä½œå®šä½çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»¥ç»†ç²’åº¦çš„æ—¶é—´åˆ†è¾¨ç‡å‡†ç¡®æ£€æµ‹å’Œåˆ†ç±»é•¿æœªä¿®å‰ªè§†é¢‘ä¸­çš„å¤šä¸ªåŠ¨ä½œã€‚DELç”±ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šéŸ³é¢‘ä¸è§†è§‰ç‰¹å¾çš„å¯¹é½ï¼Œåˆ©ç”¨æ©è”½è‡ªæ³¨æ„åŠ›å¢å¼ºå†…éƒ¨æ¨¡å¼ä¸€è‡´æ€§ï¼Œä»¥åŠå¤šæ¨¡æ€äº¤äº’ç»†åŒ–æ¨¡å—ï¼Œå»ºæ¨¡è·¨æ¨¡æ€ä¾èµ–å…³ç³»ï¼Œæ”¯æŒé«˜å±‚è¯­ä¹‰ä¸ç»†ç²’åº¦ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„æ—¶é—´åŠ¨ä½œå®šä½æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå¹³å‡mAPæå‡åˆ†åˆ«ä¸º+3.3%ã€+2.6%ã€+1.2%ã€+1.7%ï¼ˆåŠ¨è¯ï¼‰å’Œ+1.4%ï¼ˆåè¯ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³é•¿è§†é¢‘ä¸­é‡å äº‹ä»¶çš„å¯†é›†è¯­ä¹‰åŠ¨ä½œå®šä½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ—¶é—´ä¾èµ–å’Œå¤šæ¨¡æ€äº¤äº’æ—¶æ•ˆæœä¸ä½³ï¼Œéš¾ä»¥å‡†ç¡®æ£€æµ‹å’Œåˆ†ç±»å¤šä¸ªåŠ¨ä½œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDELæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡éŸ³é¢‘ä¸è§†è§‰ç‰¹å¾çš„å¯¹é½ï¼Œåˆ©ç”¨æ©è”½è‡ªæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå†…éƒ¨æ¨¡å¼çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡å¤šæ¨¡æ€äº¤äº’ç»†åŒ–æ¨¡å—å»ºæ¨¡è·¨æ¨¡æ€ä¾èµ–å…³ç³»ï¼Œä»¥å®ç°é«˜å±‚è¯­ä¹‰ä¸ç»†ç²’åº¦ç»†èŠ‚çš„ç»“åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDELæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼š1) éŸ³é¢‘ä¸è§†è§‰ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œåˆ©ç”¨æ©è”½è‡ªæ³¨æ„åŠ›å¢å¼ºç‰¹å¾ä¸€è‡´æ€§ï¼›2) å¤šæ¨¡æ€äº¤äº’ç»†åŒ–æ¨¡å—ï¼Œå»ºæ¨¡ä¸åŒå°ºåº¦çš„è·¨æ¨¡æ€ä¾èµ–å…³ç³»ï¼Œæå‡åŠ¨ä½œå®šä½çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDELçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†æ©è”½è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºéŸ³é¢‘å’Œè§†è§‰ç‰¹å¾çš„å¯¹é½æ•ˆæœï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€äº¤äº’ç»†åŒ–æ¨¡å—æœ‰æ•ˆå»ºæ¨¡è·¨æ¨¡æ€ä¾èµ–ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DELä¸­ï¼Œç‰¹å¾å¯¹é½æ¨¡å—çš„è®¾è®¡é‡‡ç”¨äº†å¤šå±‚è‡ªæ³¨æ„åŠ›ç»“æ„ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†åˆ†ç±»æŸå¤±ä¸å®šä½æŸå¤±ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DELæ¡†æ¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„æ—¶é—´åŠ¨ä½œå®šä½æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹³å‡mAPæå‡åˆ†åˆ«ä¸º+3.3%ã€+2.6%ã€+1.2%ã€+1.7%ï¼ˆåŠ¨è¯ï¼‰å’Œ+1.4%ï¼ˆåè¯ï¼‰ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è§†é¢‘ç›‘æ§ã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¯¹å¤æ‚åœºæ™¯ä¸­å¤šé‡äº‹ä»¶çš„ç†è§£ä¸åˆ†æèƒ½åŠ›ã€‚æœªæ¥ï¼ŒDELæ¡†æ¶è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€æ•°æ®å¤„ç†ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world videos often contain overlapping events and complex temporal dependencies, making multimodal interaction modeling particularly challenging. We introduce DEL, a framework for dense semantic action localization, aiming to accurately detect and classify multiple actions at fine-grained temporal resolutions in long untrimmed videos. DEL consists of two key modules: the alignment of audio and visual features that leverage masked self-attention to enhance intra-mode consistency and a multimodal interaction refinement module that models cross-modal dependencies across multiple scales, enabling high-level semantics and fine-grained details. Our method achieves state-of-the-art performance on multiple real-world Temporal Action Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and EPIC-Kitchens-100, surpassing previous approaches with notable average mAP gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

