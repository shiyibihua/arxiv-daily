---
layout: default
title: OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions
---

# OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23361" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23361v2</a>
  <a href="https://arxiv.org/pdf/2506.23361.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23361v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23361v2', 'OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, Yulun Zhang, Xiaokang Yang, Zhe Lin, Alan Yuille

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29 (æ›´æ–°: 2025-10-11)

**å¤‡æ³¨**: NeurIPS 2025; A data construction pipeline and a diffusion Transformer framework for controllable subject-driven video customization

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/caiyuanhao1998/Open-OmniVCus) | [PROJECT_PAGE](https://caiyuanhao1998.github.io/project/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniVCusä»¥è§£å†³å¤šä¸»ä½“è§†é¢‘å®šåˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å®šåˆ¶` `å¤šä¸»ä½“åœºæ™¯` `æ·±åº¦å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `å¤šæ¨¡æ€æ§åˆ¶` `å›¾åƒç¼–è¾‘` `Transformeræ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ç ”ç©¶å•ä¸»ä½“åœºæ™¯ï¼Œç¼ºä¹å¤šä¸»ä½“å®šåˆ¶çš„æœ‰æ•ˆè®­ç»ƒæ•°æ®å’Œæ§åˆ¶ä¿¡å·çš„åˆ©ç”¨ã€‚
2. æå‡ºVideoCus-Factoryæ•°æ®æ„å»ºç®¡é“ï¼Œç”Ÿæˆå¤šä¸»ä½“å®šåˆ¶æ‰€éœ€çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶å¼€å‘IVTMè®­ç»ƒä»¥å®ç°æŒ‡å¯¼æ€§ç¼–è¾‘ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniVCusåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å‰é¦ˆä¸»ä½“é©±åŠ¨è§†é¢‘å®šåˆ¶æ–¹æ³•ä¸»è¦é›†ä¸­äºå•ä¸»ä½“åœºæ™¯ï¼Œå› å¤šä¸»ä½“è®­ç»ƒæ•°æ®å¯¹çš„æ„å»ºéš¾åº¦è¾ƒå¤§ã€‚å¦‚ä½•åˆ©ç”¨æ·±åº¦ã€æ©è†œã€ç›¸æœºå’Œæ–‡æœ¬æç¤ºç­‰ä¿¡å·æ¥æ§åˆ¶å’Œç¼–è¾‘å®šåˆ¶è§†é¢‘ä¸­çš„ä¸»ä½“ä»ç„¶è¾ƒå°‘æ¢è®¨ã€‚æœ¬æ–‡é¦–å…ˆæå‡ºäº†æ•°æ®æ„å»ºç®¡é“VideoCus-Factoryï¼Œä»åŸå§‹è§†é¢‘ä¸­ç”Ÿæˆå¤šä¸»ä½“å®šåˆ¶çš„è®­ç»ƒæ•°æ®å¯¹ã€‚åŸºäºæ„å»ºçš„æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†å›¾åƒ-è§†é¢‘è½¬ç§»æ··åˆï¼ˆIVTMï¼‰è®­ç»ƒï¼Œä»¥å®ç°å®šåˆ¶è§†é¢‘ä¸­ä¸»ä½“çš„æŒ‡å¯¼æ€§ç¼–è¾‘ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£Transformeræ¡†æ¶OmniVCusï¼Œé‡‡ç”¨äº†å½©ç¥¨åµŒå…¥ï¼ˆLEï¼‰å’Œæ—¶é—´å¯¹é½åµŒå…¥ï¼ˆTAEï¼‰ä¸¤ç§åµŒå…¥æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šä¸»ä½“è§†é¢‘å®šåˆ¶ä¸­çš„æ•°æ®æ„å»ºå’Œæ§åˆ¶ä¿¡å·åˆ©ç”¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šä¸»ä½“åœºæ™¯ä¸‹è¡¨ç°ä¸è¶³ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®å¯¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æå‡ºVideoCus-Factoryç®¡é“ï¼Œä»åŸå§‹è§†é¢‘ä¸­ç”Ÿæˆå¤šä¸»ä½“å®šåˆ¶æ‰€éœ€çš„è®­ç»ƒæ•°æ®å¯¹ï¼Œå¹¶ç»“åˆIVTMè®­ç»ƒå®ç°å¯¹ä¸»ä½“çš„æŒ‡å¯¼æ€§ç¼–è¾‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ„å»ºé˜¶æ®µï¼ˆVideoCus-Factoryï¼‰å’Œæ¨¡å‹è®­ç»ƒé˜¶æ®µï¼ˆOmniVCusï¼‰ï¼Œåè€…é‡‡ç”¨æ‰©æ•£Transformeræ¡†æ¶ï¼Œç»“åˆLEå’ŒTAEæœºåˆ¶è¿›è¡Œå¤šä¸»ä½“è§†é¢‘å®šåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥å½©ç¥¨åµŒå…¥ï¼ˆLEï¼‰å’Œæ—¶é—´å¯¹é½åµŒå…¥ï¼ˆTAEï¼‰ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸»ä½“åœºæ™¯ä¸‹è¿›è¡Œæœ‰æ•ˆæ¨ç†ï¼Œå¹¶é€šè¿‡æ—¶é—´å¯¹é½ä¿¡å·ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œé‡‡ç”¨äº†å¤šå±‚Transformerç»“æ„ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡åµŒå…¥æœºåˆ¶æå‡äº†å¯¹æ§åˆ¶ä¿¡å·çš„å“åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniVCusåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®šé‡è¯„ä¼°ä¸­æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå®šæ€§è¯„ä¼°ä¸­ç”¨æˆ·æ»¡æ„åº¦æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä¸»ä½“è§†é¢‘å®šåˆ¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†é¢‘ç¼–è¾‘ã€å½±è§†åˆ¶ä½œã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å®ç°å¤šä¸»ä½“è§†é¢‘çš„å®šåˆ¶åŒ–ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´å…·ä¸ªæ€§åŒ–å’Œäº’åŠ¨æ€§çš„è§†è§‰ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³è¡Œä¸šçš„å‘å±•ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing feedforward subject-driven video customization methods mainly study single-subject scenarios due to the difficulty of constructing multi-subject training data pairs. Another challenging problem that how to use the signals such as depth, mask, camera, and text prompts to control and edit the subject in the customized video is still less explored. In this paper, we first propose a data construction pipeline, VideoCus-Factory, to produce training data pairs for multi-subject customization from raw videos without labels and control signals such as depth-to-video and mask-to-video pairs. Based on our constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with image editing data to enable instructive editing for the subject in the customized video. Then we propose a diffusion Transformer framework, OmniVCus, with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned Embedding (TAE). LE enables inference with more subjects by using the training subjects to activate more frame embeddings. TAE encourages the generation process to extract guidance from temporally aligned control signals by assigning the same frame embeddings to the control and noise tokens. Experiments demonstrate that our method significantly surpasses state-of-the-art methods in both quantitative and qualitative evaluations. Video demos are at our project page: https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released at https://github.com/caiyuanhao1998/Open-OmniVCus

