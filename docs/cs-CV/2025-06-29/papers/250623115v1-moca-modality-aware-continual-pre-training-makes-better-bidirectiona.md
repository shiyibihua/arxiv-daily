---
layout: default
title: MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings
---

# MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23115v1</a>
  <a href="https://arxiv.org/pdf/2506.23115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23115v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23115v1', 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haonan Chen, Hong Liu, Yuping Luo, Liang Wang, Nan Yang, Furu Wei, Zhicheng Dou

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: Homepage: https://haon-chen.github.io/MoCa/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoCaä»¥è§£å†³å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„å…³é”®é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åµŒå…¥` `å› æœè§†è§‰è¯­è¨€æ¨¡å‹` `åŒå‘æ³¨æ„åŠ›` `è”åˆé‡å»º` `å¼‚æ„å¯¹æ¯”å¾®è°ƒ` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `æ¨¡å‹å¯æ‰©å±•æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹åœ¨å› æœæ³¨æ„åŠ›æœºåˆ¶ã€æ•°æ®ä¾èµ–æ€§å’Œè®­ç»ƒç›®æ ‡å¤šæ ·æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. æå‡ºMoCaæ¡†æ¶ï¼Œé€šè¿‡åŒå‘æ³¨æ„åŠ›å’Œè”åˆé‡å»ºç›®æ ‡ï¼Œæå‡å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoCaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹åŸºäºå› æœè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½å‰æ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼šå› æœæ³¨æ„åŠ›åœ¨åµŒå…¥ä»»åŠ¡ä¸­çš„æ•ˆæœä¸ä½³ï¼›å¯¹é«˜è´¨é‡æ ‡æ³¨é…å¯¹æ•°æ®çš„ä¾èµ–å¯¼è‡´å¯æ‰©å±•æ€§é—®é¢˜ï¼›è®­ç»ƒç›®æ ‡å’Œæ•°æ®çš„å¤šæ ·æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MoCaï¼Œä¸€ä¸ªå°†é¢„è®­ç»ƒVLMè½¬åŒ–ä¸ºæœ‰æ•ˆåŒå‘å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥äº†è”åˆé‡å»ºç›®æ ‡ï¼Œå¢å¼ºäº†åŒå‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¼‚æ„å¯¹æ¯”å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoCaåœ¨MMEBå’ŒViDoRe-v2åŸºå‡†ä¸Šå‡å®ç°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œä¸”åœ¨æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®ä¸Šå±•ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹åœ¨å› æœæ³¨æ„åŠ›æœºåˆ¶ã€æ•°æ®ä¾èµ–æ€§å’Œè®­ç»ƒç›®æ ‡å¤šæ ·æ€§æ–¹é¢çš„ä¸è¶³ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMoCaæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŒå‘æ³¨æ„åŠ›æœºåˆ¶å’Œè”åˆé‡å»ºç›®æ ‡ï¼Œå¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoCaæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æ¨¡æ€æ„ŸçŸ¥çš„æŒç»­é¢„è®­ç»ƒï¼Œé‡‡ç”¨è”åˆé‡å»ºç›®æ ‡å¤„ç†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¼‚æ„å¯¹æ¯”å¾®è°ƒï¼Œåˆ©ç”¨å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMoCaçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥åŒå‘æ³¨æ„åŠ›æœºåˆ¶å’Œè”åˆé‡å»ºç›®æ ‡ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†åµŒå…¥æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è”åˆé‡å»ºæŸå¤±å‡½æ•°ï¼Œä»¥åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ä¸°å¯Œçš„è¯­ä¹‰æ•°æ®è¿›è¡Œå¯¹æ¯”å¾®è°ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoCaåœ¨MMEBå’ŒViDoRe-v2åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¯¹æ¯”åŸºçº¿ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬çš„è”åˆç†è§£ã€è·¨æ¨¡æ€æ£€ç´¢ã€ä»¥åŠå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ï¼ŒMoCaèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.

