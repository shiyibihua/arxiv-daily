---
layout: default
title: Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification
---

# Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23285" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23285v1</a>
  <a href="https://arxiv.org/pdf/2506.23285.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23285v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23285v1', 'Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daqian Shi, Xiaolei Diao, Xu Chen, CÃ©dric M. John

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: Accepted by ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç«äº‰è’¸é¦ç­–ç•¥ä»¥æå‡è§†è§‰åˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç«äº‰è’¸é¦` `çŸ¥è¯†è’¸é¦` `æ·±åº¦å­¦ä¹ ` `è§†è§‰åˆ†ç±»` `ç½‘ç»œä¼˜åŒ–` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨ä¸åŒè¿­ä»£ä¸­å¯¹ç½‘ç»œå­¦ä¹ æ–¹å‘çš„ç†è§£ä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ã€‚
2. æœ¬æ–‡æå‡ºç«äº‰è’¸é¦ç­–ç•¥ï¼Œä½¿æ¯ä¸ªç½‘ç»œæ ¹æ®æ€§èƒ½å……å½“æ•™å¸ˆï¼Œå¢å¼ºæ•´ä½“å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç«äº‰è’¸é¦åœ¨å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸ºæ”¹å–„DNNè®­ç»ƒè¿‡ç¨‹ï¼ŒçŸ¥è¯†è’¸é¦æ–¹æ³•é€šè¿‡å¼•å…¥æ•™å¸ˆç½‘ç»œä¸å­¦ç”Ÿç½‘ç»œä¹‹é—´çš„å›ºå®šå­¦ä¹ æ–¹å‘ï¼Œå±•ç¤ºäº†åŠ é€Ÿç½‘ç»œè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è’¸é¦ä¼˜åŒ–ç­–ç•¥å¦‚æ·±åº¦äº’å­¦ä¹ å’Œè‡ªè’¸é¦ï¼Œç”±äºå¯¹ä¸åŒè¿­ä»£ä¸­ç½‘ç»œå­¦ä¹ æ–¹å‘å½±å“çš„ç†è§£ä¸è¶³ï¼Œæå‡æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«äº‰è’¸é¦ç­–ç•¥ï¼Œä½¿å¾—æ¯ä¸ªç½‘ç»œæ ¹æ®å…¶æ€§èƒ½æ½œåœ¨åœ°å……å½“æ•™å¸ˆï¼Œä»è€Œå¢å¼ºæ•´ä½“å­¦ä¹ æ€§èƒ½ã€‚ç«äº‰è’¸é¦ç»„ç»‡ä¸€ç»„ç½‘ç»œå…±åŒæ‰§è¡Œä»»åŠ¡å¹¶è¿›è¡Œç«äº‰ï¼Œæå‡ºäº†ç«äº‰ä¼˜åŒ–ä»¥æ”¹å–„å‚æ•°æ›´æ–°è¿‡ç¨‹ï¼ŒåŒæ—¶å¼•å…¥éšæœºæ‰°åŠ¨ä»¥æ¿€åŠ±ç½‘ç»œè¯±å¯¼å˜å¼‚ï¼Œè¾¾åˆ°æ›´å¥½çš„è§†è§‰è¡¨ç¤ºå’Œå…¨å±€æœ€ä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç«äº‰è’¸é¦åœ¨å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨ä¸åŒè¿­ä»£ä¸­å¯¹å­¦ä¹ æ–¹å‘ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç«äº‰è’¸é¦ç­–ç•¥ï¼Œä½¿å¾—æ¯ä¸ªç½‘ç»œèƒ½å¤Ÿæ ¹æ®å…¶æ€§èƒ½å……å½“æ•™å¸ˆï¼Œä¿ƒè¿›ç½‘ç»œé—´çš„ç«äº‰ï¼Œä»è€Œæå‡æ•´ä½“å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªç½‘ç»œå…±åŒæ‰§è¡Œä»»åŠ¡ï¼Œè¿›è¡Œç«äº‰ä¼˜åŒ–ã€‚æ¯ä¸ªç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®å…¶è¡¨ç°åŠ¨æ€è°ƒæ•´è§’è‰²ï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€çš„å­¦ä¹ ç¯å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†ç«äº‰æœºåˆ¶ï¼Œä½¿å¾—ç½‘ç»œä¹‹é—´èƒ½å¤Ÿç›¸äº’ä¿ƒè¿›ï¼Œè€Œä¸æ˜¯å•å‘çš„çŸ¥è¯†ä¼ é€’ï¼Œè¿™ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨éšæœºæ‰°åŠ¨ä»¥æ¿€åŠ±ç½‘ç»œäº§ç”Ÿå˜å¼‚ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°ä»¥é€‚åº”ç«äº‰ç¯å¢ƒï¼Œç¡®ä¿ç½‘ç»œèƒ½å¤Ÿæ¢ç´¢æ›´ä¼˜çš„è§†è§‰è¡¨ç¤ºã€‚æ•´ä½“ç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼Œç¡®ä¿å„ä¸ªç½‘ç»œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œä¿¡æ¯äº¤æµå’Œç«äº‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç«äº‰è’¸é¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿè’¸é¦æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†æ•´ä½“è¡¨ç°è¢«è¯„ä¼°ä¸ºéå¸¸æœ‰å‰æ™¯ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè§†é¢‘åˆ†æç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚é€šè¿‡æå‡ç½‘ç»œçš„å­¦ä¹ æ€§èƒ½ï¼Œç«äº‰è’¸é¦ç­–ç•¥èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep Neural Networks (DNNs) have significantly advanced the field of computer vision. To improve DNN training process, knowledge distillation methods demonstrate their effectiveness in accelerating network training by introducing a fixed learning direction from the teacher network to student networks. In this context, several distillation-based optimization strategies are proposed, e.g., deep mutual learning and self-distillation, as an attempt to achieve generic training performance enhancement through the cooperative training of multiple networks. However, such strategies achieve limited improvements due to the poor understanding of the impact of learning directions among networks across different iterations. In this paper, we propose a novel competitive distillation strategy that allows each network in a group to potentially act as a teacher based on its performance, enhancing the overall learning performance. Competitive distillation organizes a group of networks to perform a shared task and engage in competition, where competitive optimization is proposed to improve the parameter updating process. We further introduce stochastic perturbation in competitive distillation, aiming to motivate networks to induce mutations to achieve better visual representations and global optimum. The experimental results show that competitive distillation achieves promising performance in diverse tasks and datasets.

