---
layout: default
title: Self-Supervised Contrastive Learning for Multi-Label Images
---

# Self-Supervised Contrastive Learning for Multi-Label Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23156" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23156v1</a>
  <a href="https://arxiv.org/pdf/2506.23156.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23156v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23156v1', 'Self-Supervised Contrastive Learning for Multi-Label Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiale Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä»¥è§£å†³å¤šæ ‡ç­¾å›¾åƒè¡¨ç¤ºå­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ ‡ç­¾å›¾åƒ` `è¡¨ç¤ºå­¦ä¹ ` `å›¾åƒå¢å¼º` `è¿ç§»å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–äºå•æ ‡ç­¾æ•°æ®é›†ï¼Œå¯¼è‡´é¢„è®­ç»ƒæˆæœ¬é«˜ï¼Œä¸”å¤šæ ‡ç­¾å›¾åƒçš„æ½œåŠ›æœªè¢«å……åˆ†åˆ©ç”¨ã€‚
2. æœ¬æ–‡æå‡ºå—çº§å¢å¼ºæ¨¡å—å’Œå›¾åƒæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œæ—¨åœ¨ä»å¤šæ ‡ç­¾å›¾åƒä¸­æå–æ›´å¤šæ­£è§†å›¾å¯¹ï¼Œæå‡è¡¨ç¤ºå­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ ·æœ¬è´¨é‡å’Œæ•°é‡å­˜åœ¨æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¾®è°ƒå’Œè¿ç§»å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é€šè¿‡å¯¹æ¯”æ–¹æ³•æœ‰æ•ˆåœ°å­¦ä¹ è¡¨ç¤ºï¼Œä½†ç°æœ‰ä¸»æµSSLæ–¹æ³•ä¾èµ–äºå•æ ‡ç­¾çš„å¤§å‹æ•°æ®é›†ï¼Œå¦‚ImageNetï¼Œå¯¼è‡´é¢„è®­ç»ƒå¼€é”€è¿‡å¤§ã€‚æ­¤å¤–ï¼Œå¤šæ ‡ç­¾å›¾åƒåœ¨SSLä¸­å¸¸è¢«å¿½è§†ï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯å’Œæ›´å¹¿æ³›çš„ä¸‹æ¸¸åº”ç”¨æ½œåŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡é’ˆå¯¹å¤šæ ‡ç­¾å›¾åƒï¼Œè°ƒæ•´ä¸»æµSSLæ–¹æ³•ï¼Œä»¥ç¡®ä¿åœ¨è¾ƒå°‘æ•°æ®ä¸‹å®ç°ä¼˜ç§€çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§å—çº§å¢å¼ºæ¨¡å—ï¼Œæ—¨åœ¨ä»å¤šæ ‡ç­¾å›¾åƒä¸­æå–é¢å¤–çš„æ½œåœ¨æ­£è§†å›¾å¯¹ã€‚éšåï¼Œè®¾è®¡äº†ä¸€ç§å›¾åƒæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œä»¥å»ºç«‹è¿™äº›è§†å›¾ä¹‹é—´çš„è”ç³»ï¼Œä»è€Œä¿ƒè¿›è¯­ä¹‰ä¸€è‡´è¡¨ç¤ºçš„æå–ã€‚é€šè¿‡å…¨é¢çš„çº¿æ€§å¾®è°ƒå’Œè¿ç§»å­¦ä¹ éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„ç«äº‰åŠ›ï¼Œå°½ç®¡æ ·æœ¬è´¨é‡å’Œæ•°é‡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤šæ ‡ç­¾å›¾åƒè¡¨ç¤ºå­¦ä¹ ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹å•æ ‡ç­¾æ•°æ®é›†çš„ä¾èµ–å’Œé«˜æ˜‚çš„é¢„è®­ç»ƒæˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å—çº§å¢å¼ºæ¨¡å—å’Œå›¾åƒæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œæå–å¤šæ ‡ç­¾å›¾åƒä¸­çš„æ½œåœ¨æ­£è§†å›¾å¯¹ï¼Œä»è€Œæé«˜è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å—çº§å¢å¼ºæ¨¡å—ã€å¯¹æ¯”æŸå¤±è®¡ç®—å’Œè¡¨ç¤ºå­¦ä¹ å››ä¸ªä¸»è¦é˜¶æ®µã€‚å—çº§å¢å¼ºæ¨¡å—è´Ÿè´£ç”Ÿæˆæ­£è§†å›¾å¯¹ï¼Œè€Œå¯¹æ¯”æŸå¤±åˆ™ç”¨äºä¼˜åŒ–æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å—çº§å¢å¼ºæ¨¡å—å’Œå›¾åƒæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å•ä¸€æ ‡ç­¾æ•°æ®é›†çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å¤šæ ‡ç­¾å›¾åƒçš„ä¸°å¯Œä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå—çº§å¢å¼ºæ¨¡å—é€šè¿‡å¯¹å›¾åƒè¿›è¡Œå±€éƒ¨å¢å¼ºæ¥ç”Ÿæˆæ­£è§†å›¾å¯¹ï¼ŒæŸå¤±å‡½æ•°åˆ™é‡‡ç”¨å›¾åƒæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°è¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ ·æœ¬æ•°é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ ‡ç­¾å›¾åƒè¡¨ç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒæ£€ç´¢ç­‰ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤šæ ‡ç­¾æ•°æ®çš„åœºæ™¯ä¸­ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨å¤šæ ‡ç­¾å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-supervised learning (SSL) has demonstrated its effectiveness in learning representations through comparison methods that align with human intuition. However, mainstream SSL methods heavily rely on high body datasets with single label, such as ImageNet, resulting in intolerable pre-training overhead. Besides, more general multi-label images are frequently overlooked in SSL, despite their potential for richer semantic information and broader applicability in downstream scenarios. Therefore, we tailor the mainstream SSL approach to guarantee excellent representation learning capabilities using fewer multi-label images. Firstly, we propose a block-wise augmentation module aimed at extracting additional potential positive view pairs from multi-label images. Subsequently, an image-aware contrastive loss is devised to establish connections between these views, thereby facilitating the extraction of semantically consistent representations. Comprehensive linear fine-tuning and transfer learning validate the competitiveness of our approach despite challenging sample quality and quantity.

