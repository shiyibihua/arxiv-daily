---
layout: default
title: MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition
---

# MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23283" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23283v1</a>
  <a href="https://arxiv.org/pdf/2506.23283.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23283v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23283v1', 'MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhuan Yang, Chaofan Ma, Zhenjie Mao, Jiangchao Yao, Ya Zhang, Yanfeng Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: ICML 2025 paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoMaæ¡†æ¶ä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºå»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `æ—¶ç©ºå»ºæ¨¡` `å›¾åƒåŸºç¡€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤šæ¨¡æ€å­¦ä¹ ` `SeqModæ“ä½œ` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¾€å¾€å°†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯åˆ†å¼€å¤„ç†ï¼Œæ— æ³•å…¨é¢æ•æ‰è§†é¢‘åŠ¨æ€çš„å¤æ‚æ€§ã€‚
2. MoMaæ¡†æ¶é€šè¿‡å¼•å…¥SeqModæ“ä½œï¼Œå°†æ—¶ç©ºä¿¡æ¯æœ‰æ•ˆæ³¨å…¥åˆ°é¢„è®­ç»ƒçš„IFMsä¸­ï¼Œæå‡è§†é¢‘ç†è§£èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoMaå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¸”è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘ç†è§£æ˜¯ä¸€é¡¹å¤æ‚çš„æŒ‘æˆ˜ï¼Œéœ€è¦æœ‰æ•ˆå»ºæ¨¡æ—¶ç©ºåŠ¨æ€ã€‚éšç€å›¾åƒåŸºç¡€æ¨¡å‹ï¼ˆIFMsï¼‰åœ¨å›¾åƒç†è§£ä¸­çš„æˆåŠŸï¼Œè¿‘æœŸç ”ç©¶æ¢ç´¢äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä»¥é€‚åº”IFMsäºè§†é¢‘ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•å€¾å‘äºåˆ†åˆ«å¤„ç†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œå¯èƒ½æ— æ³•æ•æ‰è§†é¢‘åŠ¨æ€çš„å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºMoMaï¼Œä¸€ä¸ªé«˜æ•ˆçš„é€‚é…æ¡†æ¶ï¼Œé€šè¿‡å°†Mambaçš„é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´å»ºæ¨¡æ•´åˆåˆ°IFMsä¸­ï¼Œå®ç°å…¨é¢çš„æ—¶ç©ºå»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„SeqModæ“ä½œï¼Œå°†æ—¶ç©ºä¿¡æ¯æ³¨å…¥é¢„è®­ç»ƒçš„IFMsï¼Œè€Œä¸å¹²æ‰°å…¶åŸæœ‰ç‰¹å¾ã€‚é€šè¿‡å°†SeqModçº³å…¥åˆ†å‰²ä¸è°ƒåˆ¶æ¶æ„ï¼ŒMoMaåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å¢å¼ºäº†è§†é¢‘ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoMaåœ¨å¤šä¸ªè§†é¢‘åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸”è®¡ç®—æˆæœ¬é™ä½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘ç†è§£ä¸­æ—¶ç©ºåŠ¨æ€å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´æ— æ³•å……åˆ†æ•æ‰è§†é¢‘çš„å¤æ‚æ€§å’ŒåŠ¨æ€å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMoMaæ¡†æ¶ï¼Œé€šè¿‡å°†Mambaçš„é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´å»ºæ¨¡æ•´åˆåˆ°IFMsä¸­ï¼Œå®ç°å…¨é¢çš„æ—¶ç©ºå»ºæ¨¡ã€‚SeqModæ“ä½œçš„å¼•å…¥ä½¿å¾—æ—¶ç©ºä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆæ³¨å…¥åˆ°é¢„è®­ç»ƒçš„IFMsä¸­ï¼Œè€Œä¸å¹²æ‰°å…¶åŸæœ‰ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoMaæ¡†æ¶é‡‡ç”¨åˆ†å‰²ä¸è°ƒåˆ¶æ¶æ„ï¼Œé¦–å…ˆå¯¹è¾“å…¥è§†é¢‘è¿›è¡Œåˆ†å‰²ï¼Œç„¶åé€šè¿‡SeqModæ¨¡å—å¯¹æ¯ä¸ªåˆ†æ®µè¿›è¡Œæ—¶ç©ºä¿¡æ¯çš„æ³¨å…¥ï¼Œæœ€åå°†å¤„ç†åçš„ä¿¡æ¯ä¼ é€’ç»™IFMsè¿›è¡Œç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºSeqModæ“ä½œçš„æå‡ºï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ç ´åIFMsåŸæœ‰ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ—¶ç©ºä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å…¨é¢çš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSeqModæ“ä½œçš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ—¶ç©ºä¿¡æ¯çš„æœ‰æ•ˆæ³¨å…¥ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿè€ƒè™‘äº†æ—¶ç©ºåŠ¨æ€çš„ç‰¹æ€§ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoMaæ¡†æ¶å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦15%ï¼Œä¸”è®¡ç®—æˆæœ¬é™ä½äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜MoMaåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡è§†é¢‘åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼ŒMoMaæ¡†æ¶å¯èƒ½åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œå®æ—¶è§†é¢‘å¤„ç†ç­‰é¢†åŸŸå‘æŒ¥æ›´å¤§ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video understanding is a complex challenge that requires effective modeling of spatial-temporal dynamics. With the success of image foundation models (IFMs) in image understanding, recent approaches have explored parameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most of these methods tend to process spatial and temporal information separately, which may fail to capture the full intricacy of video dynamics. In this paper, we propose MoMa, an efficient adapter framework that achieves full spatial-temporal modeling by integrating Mamba's selective state space modeling into IFMs. We propose a novel SeqMod operation to inject spatial-temporal information into pre-trained IFMs, without disrupting their original features. By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances video understanding while maintaining computational efficiency. Extensive experiments on multiple video benchmarks demonstrate the effectiveness of MoMa, achieving superior performance with reduced computational cost.

