---
layout: default
title: Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation
---

# Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23120" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23120v1</a>
  <a href="https://arxiv.org/pdf/2506.23120.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23120v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23120v1', 'Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenhua Ning, Zhuotao Tian, Shaoshuai Shi, Guangming Lu, Daojing He, Wenjie Pei, Li Jiang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRÂ²Sæ¡†æ¶ä»¥è§£å†³å¤æ‚ç©ºé—´æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´æ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `ç‚¹äº‘æ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ ` `è§†è§‰è¯­è¨€å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶ï¼Œå°½ç®¡3Dç‚¹äº‘æ•°æ®æä¾›äº†è¯¦ç»†çš„ç©ºé—´çº¿ç´¢ï¼Œä½†ä»éš¾ä»¥å®ç°å‡†ç¡®çš„ç©ºé—´æ¨ç†ã€‚
2. æœ¬æ–‡æå‡ºçš„RÂ²Sæ¡†æ¶é€šè¿‡å°†ç©ºé—´æ¨ç†åˆ†ä¸ºè¯†åˆ«ç›¸å…³å…ƒç´ å’Œå¤„ç†æŒ‡ä»¤ä¸¤ä¸ªé˜¶æ®µï¼Œæ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRÂ²Så’Œ3D ReasonSegåœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œæˆä¸ºæœªæ¥ç ”ç©¶çš„æ–°åŸºå‡†å’ŒåŸºå‡†æ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç‚¹äº‘æ„ŸçŸ¥åœ¨åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦å‡†ç¡®ç©ºé—´æ¨ç†çš„å¤æ‚æŒ‡ä»¤æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ç›¸å…³æ¨ç†åˆ†å‰²ï¼ˆRÂ²Sï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹ï¼Œå°†ç©ºé—´æ¨ç†åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¯†åˆ«ç›¸å…³å…ƒç´ ï¼Œç„¶åæ ¹æ®è§†è§‰å…ˆéªŒå¤„ç†æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†3D ReasonSegæ•°æ®é›†ï¼ŒåŒ…å«25,185ä¸ªè®­ç»ƒæ ·æœ¬å’Œ3,966ä¸ªéªŒè¯æ ·æœ¬ï¼Œå…·æœ‰ç²¾ç¡®çš„æ³¨é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRÂ²Så’Œ3D ReasonSegæœ‰æ•ˆå¢å¼ºäº†3Dç‚¹äº‘æ„ŸçŸ¥çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒæœŸæœ›ä¸ºæœªæ¥ç ”ç©¶æä¾›æ–°çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤ç†è§£å’Œç›®æ ‡è¯†åˆ«æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨3Dç‚¹äº‘æ•°æ®ä¸­çš„ç©ºé—´ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„RÂ²Sæ¡†æ¶é€šè¿‡åˆ†è§£ç©ºé—´æ¨ç†è¿‡ç¨‹ï¼Œé¦–å…ˆè¯†åˆ«ä¸ä»»åŠ¡ç›¸å…³çš„å…ƒç´ ï¼Œç„¶ååŸºäºè¿™äº›å…ƒç´ çš„è§†è§‰å…ˆéªŒè¿›è¡ŒæŒ‡ä»¤å¤„ç†ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRÂ²Sæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºç›¸å…³å…ƒç´ è¯†åˆ«ï¼Œç¬¬äºŒé˜¶æ®µä¸ºåŸºäºè§†è§‰å…ˆéªŒçš„æŒ‡ä»¤å¤„ç†ã€‚æ•´ä½“æµç¨‹é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°ï¼Œç»“åˆäº†ç‚¹äº‘æ•°æ®çš„ç‰¹å¾æå–ä¸æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRÂ²Sæ¡†æ¶çš„åˆ›æ–°åœ¨äºå…¶å°†ç©ºé—´æ¨ç†è¿‡ç¨‹ç³»ç»ŸåŒ–ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µçš„æ–¹æ³•æ¥æå‡æ¨ç†çš„å‡†ç¡®æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ è´´è¿‘äººç±»çš„è®¤çŸ¥é€»è¾‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå¯¹ç›¸å…³å…ƒç´ çš„å…³æ³¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRÂ²Sæ¡†æ¶åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°å°¤ä¸ºçªå‡ºï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œå¢å¼ºç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿå¯¹å¤æ‚ç¯å¢ƒçš„ç†è§£å’Œå†³ç­–èƒ½åŠ›ã€‚æœªæ¥ï¼ŒRÂ²Sæ¡†æ¶å’Œ3D ReasonSegæ•°æ®é›†æœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in point cloud perception have demonstrated remarkable progress in scene understanding through vision-language alignment leveraging large language models (LLMs). However, existing methods may still encounter challenges in handling complex instructions that require accurate spatial reasoning, even if the 3D point cloud data provides detailed spatial cues such as size and position for identifying the targets. To tackle this issue, we propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based segmentation framework. The framework emulates human cognitive processes by decomposing spatial reasoning into two sequential stages: first identifying relevant elements, then processing instructions guided by their associated visual priors. Furthermore, acknowledging the inadequacy of existing datasets in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based segmentation dataset comprising 25,185 training samples and 3,966 validation samples with precise annotations. Both quantitative and qualitative experiments demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud perception with stronger spatial reasoning capabilities, and we hope that they can serve as a new baseline and benchmark for future work.

