---
layout: default
title: Token Activation Map to Visually Explain Multimodal LLMs
---

# Token Activation Map to Visually Explain Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23270" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.23270v1</a>
  <a href="https://arxiv.org/pdf/2506.23270.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23270v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23270v1', 'Token Activation Map to Visually Explain Multimodal LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, Xiaomeng Li

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-29

**Â§áÊ≥®**: ICCV2025 Accepted

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Token Activation Map‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅLLMÂèØËß£ÈáäÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÂèØËß£ÈáäÊÄß` `Âõ†ÊûúÊé®Êñ≠` `ÊøÄÊ¥ªÂõæ` `È´òÊñØÊª§Ê≥¢Âô®` `Ê®°ÂûãÁêÜËß£` `ÂèØËßÜÂåñÊäÄÊúØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂøΩËßÜ‰∫ÜÂ§öÊ®°ÊÄÅLLM‰∏≠‰∏ä‰∏ãÊñá‰ª§ÁâåÁöÑÂÜó‰ΩôÊøÄÊ¥ªÈóÆÈ¢òÔºåÂØºËá¥Ëß£ÈáäÁöÑÂèØÈù†ÊÄßÈôç‰Ωé„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑToken Activation MapÔºàTAMÔºâÊñπÊ≥ïÔºåÈÄöËøá‰º∞ËÆ°Âõ†ÊûúÊé®Êñ≠Êù•ÂáèÂ∞è‰∏ä‰∏ãÊñáÂπ≤Êâ∞ÔºåÊèêÂçáËß£ÈáäË¥®Èáè„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTAMÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑÂèØËßÜÂåñÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ö‰∏™È¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂ÂèØËß£ÈáäÊÄß‰ªçÁÑ∂ËæÉÂ∞ëË¢´Êé¢Á¥¢ÔºåÈôêÂà∂‰∫ÜÂØπÊ®°ÂûãÁöÑÊ∑±ÂÖ•ÁêÜËß£ÂíåÊúâÊïàÂèØËßÜÂåñ„ÄÇ‰∏é‰º†ÁªüËßÜËßâÊ®°Âûã‰∏çÂêåÔºåMLLMsÈÄêÊ≠•ÁîüÊàêÁöÑ‰ª§ÁâåÂ∫èÂàó‰ΩøÂæóÊó©Êúü‰∏ä‰∏ãÊñá‰ª§ÁâåÂèØËÉΩÂºïÂÖ•ÂÜó‰ΩôÊøÄÊ¥ªÔºåÂπ≤Êâ∞ÂêéÁª≠‰ª§ÁâåÁöÑËß£Èáä„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰º∞ËÆ°Âõ†ÊûúÊé®Êñ≠ÁöÑÊñπÊ≥ïÔºåÁªìÂêàÊñ∞È¢ñÁöÑÁß©È´òÊñØÊª§Ê≥¢Âô®Ôºå‰ª•ÂáèÂ∞èÊøÄÊ¥ªÂô™Â£∞Ôºå‰ªéËÄåÂÆûÁé∞È´òË¥®ÈáèÁöÑMLLMËß£Èáä„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïToken Activation MapÔºàTAMÔºâÂú®Â§ö‰∏™‰ª§ÁâåÁöÑËß£Èáä‰∏äË°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂äÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÈ´òË¥®ÈáèÁöÑÂèØËßÜÂåñÁªìÊûúÔºåÈÄÇÁî®‰∫éÁâ©‰ΩìÂÆö‰Ωç„ÄÅÊïÖÈöúÊ°à‰æãÂàÜÊûêÁ≠âÂ§öÁßçÂú∫ÊôØ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂèØËß£ÈáäÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂ§ÑÁêÜ‰∏ä‰∏ãÊñá‰ª§ÁâåÁöÑÂÜó‰ΩôÊøÄÊ¥ªÔºåÂØºËá¥Ëß£ÈáäÁªìÊûúÁöÑÂèØÈù†ÊÄßÂèóÂà∞ÂΩ±Âìç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫ÁöÑToken Activation MapÔºàTAMÔºâÊñπÊ≥ïÈÄöËøá‰º∞ËÆ°Âõ†ÊûúÊé®Êñ≠ÔºåÂáèÂ∞ë‰∏ä‰∏ãÊñáÂØπÂêéÁª≠‰ª§ÁâåËß£ÈáäÁöÑÂπ≤Êâ∞ÔºåÁ°Æ‰øùËß£ÈáäÁöÑÈ´òË¥®ÈáèÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTAMÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÂõ†ÊûúÊé®Êñ≠Ê®°ÂùóÔºåÁî®‰∫éËØÜÂà´ÂíåÂáèÂ∞èÂÜó‰ΩôÊøÄÊ¥ªÁöÑÂΩ±ÂìçÔºõÂÖ∂Ê¨°ÊòØÁß©È´òÊñØÊª§Ê≥¢Âô®Ê®°ÂùóÔºåËøõ‰∏ÄÊ≠•Èôç‰ΩéÊøÄÊ¥ªÂô™Â£∞ÔºåÊèêÂçáÂèØËßÜÂåñÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTAMÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂ËÄÉËôë‰∫Ü‰ª§Áâå‰πãÈó¥ÁöÑ‰∫§‰∫í‰ΩúÁî®ÔºåÂå∫Âà´‰∫é‰º†ÁªüÁöÑÁ±ªÊøÄÊ¥ªÂõæÔºàCAMÔºâÔºåÂêéËÄÖ‰ªÖÈíàÂØπÂçï‰∏ÄÈ¢ÑÊµãËøõË°åËß£Èáä„ÄÇTAMËÉΩÂ§üÂêåÊó∂Ëß£ÈáäÂ§ö‰∏™‰ª§ÁâåÔºåÂ¢ûÂº∫‰∫ÜÂèØËß£ÈáäÊÄßÁöÑÊ∑±Â∫¶ÂíåÂπøÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåTAMÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÂõ†ÊûúÊé®Êñ≠ÁöÑÊïàÊûúÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äÂºïÂÖ•‰∫ÜÁß©È´òÊñØÊª§Ê≥¢Âô®Ôºå‰ª•ÊúâÊïàÂáèÂ∞ëÊøÄÊ¥ªÂô™Â£∞ÔºåÁ°Æ‰øùÊúÄÁªàÁöÑÂèØËßÜÂåñÁªìÊûúÊ∏ÖÊô∞‰∏îÊúâÊÑè‰πâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåTAMÊñπÊ≥ïÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑÂèØËßÜÂåñÊïàÊûú„ÄÇ‰æãÂ¶ÇÔºåÂú®Áâ©‰ΩìÂÆö‰Ωç‰ªªÂä°‰∏≠ÔºåTAMÁöÑÂèØËßÜÂåñÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü20%ÔºåÂú®ÊïÖÈöúÊ°à‰æãÂàÜÊûê‰∏≠ÔºåËß£ÈáäÁöÑÂèØÈù†ÊÄßÊèêÈ´ò‰∫Ü30%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂπøÊ≥õÔºåÂåÖÊã¨Áâ©‰ΩìÂÆö‰Ωç„ÄÅÊïÖÈöúÊ°à‰æãÂàÜÊûê„ÄÅËßÜÈ¢ëÂèØËßÜÂåñ‰ª•ÂèäÂ§öÊ®°ÊÄÅLLMÁöÑËßÜËßâÊØîËæÉÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÔºåTAMÊñπÊ≥ïËÉΩÂ§üÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíåÂºÄÂèëËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£Ê®°ÂûãÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºå‰ªéËÄåÊé®Âä®Â§öÊ®°ÊÄÅAIÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available atgithub.com/xmed-lab/TAM.

