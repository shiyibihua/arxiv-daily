---
layout: default
title: FA-Seg: A Fast and Accurate Diffusion-Based Method for Open-Vocabulary Segmentation
---

# FA-Seg: A Fast and Accurate Diffusion-Based Method for Open-Vocabulary Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23323" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23323v3</a>
  <a href="https://arxiv.org/pdf/2506.23323.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23323v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23323v3', 'FA-Seg: A Fast and Accurate Diffusion-Based Method for Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Quang-Huy Che, Vinh-Tiep Nguyen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29 (æ›´æ–°: 2025-07-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFA-Segä»¥è§£å†³å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„ç²¾åº¦ä¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡åˆ†å‰²` `æ‰©æ•£æ¨¡å‹` `è¯­ä¹‰åˆ†å‰²` `å¯¹æ¯”å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­å¸¸å¸¸å› å…¨å±€è¡¨ç¤ºåå·®è€Œå¤±å»åƒç´ çº§çš„ç²¾åº¦ã€‚
2. FA-Segé€šè¿‡æ‰©æ•£æ¨¡å‹å®ç°å¿«é€Ÿä¸”å‡†ç¡®çš„åˆ†å‰²ï¼Œé‡‡ç”¨åŒæç¤ºæœºåˆ¶å’Œå±‚æ¬¡æ³¨æ„åŠ›ç²¾ç‚¼æ–¹æ³•æ¥æå‡åˆ†å‰²è´¨é‡ã€‚
3. FA-Segåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†43.8%çš„å¹³å‡mIoUï¼Œå±•ç¤ºäº†å…¶åœ¨åˆ†å‰²è´¨é‡å’Œæ¨ç†æ•ˆç‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰æ—¨åœ¨ä»ä»»æ„æ–‡æœ¬ç±»åˆ«ä¸­åˆ†å‰²å¯¹è±¡ï¼Œè€Œæ— éœ€å¯†é›†æ ‡æ³¨çš„æ•°æ®é›†ã€‚å°½ç®¡åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ¨¡å‹èƒ½å¤Ÿå®ç°é›¶-shot åˆ†å‰²ï¼Œä½†ç”±äºå…¨å±€è¡¨ç¤ºåå·®ï¼Œå¾€å¾€åœ¨åƒç´ çº§åˆ«ä¸Šå¤±å»ç»†è‡´çš„ç©ºé—´ç²¾åº¦ã€‚ä¸æ­¤ä¸åŒï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªç„¶åœ°ç¼–ç ç»†ç²’åº¦çš„ç©ºé—´ç‰¹å¾ï¼Œèƒ½å¤Ÿæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è®¡ç®—æˆæœ¬ä¸åˆ†å‰²è´¨é‡ä¹‹é—´çš„å¹³è¡¡ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†FA-Segï¼Œä¸€ä¸ªå¿«é€Ÿä¸”å‡†ç¡®çš„æ— è®­ç»ƒæ¡†æ¶ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹å®ç°å¼€æ”¾è¯æ±‡åˆ†å‰²ã€‚FA-Segä»…éœ€ä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œä¸€æ¬¡ï¼ˆ1+1ï¼‰æ­¥çš„åˆ†å‰²ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¸€æ¬¡æ€§å¯¹æ‰€æœ‰ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡åˆ†å‰²è´¨é‡ï¼ŒFA-Segå¼•å…¥äº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŒæç¤ºæœºåˆ¶ã€å±‚æ¬¡æ³¨æ„åŠ›ç²¾ç‚¼æ–¹æ³•ï¼ˆHARDï¼‰å’Œæµ‹è¯•æ—¶ç¿»è½¬ï¼ˆTTFï¼‰æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFA-Segåœ¨PASCAL VOCã€PASCAL Contextå’ŒCOCO ObjectåŸºå‡†ä¸Šå®ç°äº†43.8%çš„å¹³å‡mIoUï¼Œä¸”æ¨ç†æ•ˆç‡ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„ç²¾åº¦ä¸æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦ç©ºé—´ç‰¹å¾æ—¶å­˜åœ¨å…¨å±€è¡¨ç¤ºåå·®ï¼Œå¯¼è‡´åˆ†å‰²ç²¾åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFA-Segæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿä¸”å‡†ç¡®çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç±»åˆ«çš„åˆ†å‰²ä»»åŠ¡ï¼Œé¿å…äº†å¤šæ¬¡è¿è¡Œçš„è®¡ç®—å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFA-Segçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šåŒæç¤ºæœºåˆ¶ç”¨äºæå–ç±»æ„ŸçŸ¥çš„æ³¨æ„åŠ›ï¼Œå±‚æ¬¡æ³¨æ„åŠ›ç²¾ç‚¼æ–¹æ³•ï¼ˆHARDï¼‰ç”¨äºå¤šåˆ†è¾¨ç‡çš„æ³¨æ„åŠ›èåˆï¼Œä»¥åŠæµ‹è¯•æ—¶ç¿»è½¬ï¼ˆTTFï¼‰æ–¹æ¡ˆä»¥å¢å¼ºç©ºé—´ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šFA-Segçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è®­ç»ƒ-freeçš„è®¾è®¡å’Œé«˜æ•ˆçš„åˆ†å‰²ç­–ç•¥ï¼Œå°¤å…¶æ˜¯é€šè¿‡åŒæç¤ºæœºåˆ¶å’ŒHARDæ–¹æ³•æ˜¾è‘—æå‡äº†åˆ†å‰²çš„è¯­ä¹‰ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒFA-Segé‡‡ç”¨äº†å¤šåˆ†è¾¨ç‡çš„æ³¨æ„åŠ›èåˆç­–ç•¥ï¼Œå¹¶é€šè¿‡æµ‹è¯•æ—¶ç¿»è½¬æ¥å¢å¼ºåˆ†å‰²ç»“æœçš„ç©ºé—´ä¸€è‡´æ€§ï¼Œç¡®ä¿äº†åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FA-Segåœ¨PASCAL VOCã€PASCAL Contextå’ŒCOCO ObjectåŸºå‡†ä¸Šå®ç°äº†43.8%çš„å¹³å‡mIoUï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è®­ç»ƒ-free æ–¹æ³•ï¼Œä¸”åœ¨æ¨ç†æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„é¢†å…ˆåœ°ä½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FA-Segçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æå’Œæ™ºèƒ½ç›‘æ§ç­‰åœºæ™¯ã€‚å…¶é«˜æ•ˆçš„åˆ†å‰²èƒ½åŠ›èƒ½å¤Ÿæ”¯æŒå®æ—¶å¤„ç†éœ€æ±‚ï¼Œä¸ºå„ç±»è§†è§‰ä»»åŠ¡æä¾›å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing the computation costs and the quality of the segmentation mask. In this work, we present FA-Seg, a Fast and Accurate training-free framework for open-vocabulary segmentation based on diffusion models. FA-Seg performs segmentation using only a (1+1)-step from a pretrained diffusion model. Moreover, instead of running multiple times for different classes, FA-Seg performs segmentation for all classes at once. To further enhance the segmentation quality, FA-Seg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances semantic precision via multi-resolution attention fusion, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FA-Seg achieves state-of-the-art training-free performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FA-Seg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency. The source code will be open-sourced after this paper is accepted.

