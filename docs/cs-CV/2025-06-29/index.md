---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-29
---

# cs.CVï¼ˆ2025-06-29ï¼‰

ğŸ“Š å…± **27** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250623308v1-endo-4dgx-robust-endoscopic-scene-reconstruction-and-illumination-co.html">Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting</a></td>
  <td>æå‡ºEndo-4DGXä»¥è§£å†³å†…çª¥é•œåœºæ™¯ä¸­çš„å…‰ç…§ä¸å‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23308v1" data-paper-url="./papers/250623308v1-endo-4dgx-robust-endoscopic-scene-reconstruction-and-illumination-co.html" onclick="toggleFavorite(this, '2506.23308v1', 'Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250623309v2-surgtpgs-semantic-3d-surgical-scene-understanding-with-text-promptab.html">SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting</a></td>
  <td>æå‡ºSurgTPGSä»¥è§£å†³3Då¤–ç§‘åœºæ™¯ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23309v2" data-paper-url="./papers/250623309v2-surgtpgs-semantic-3d-surgical-scene-understanding-with-text-promptab.html" onclick="toggleFavorite(this, '2506.23309v2', 'SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250623157v1-std-gs-exploring-frame-event-interaction-for-spatiotemporal-disentan.html">STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene</a></td>
  <td>æå‡ºSTD-GSæ¡†æ¶ä»¥è§£å†³é«˜åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„æ—¶ç©ºç‰¹å¾ä¸åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23157v1" data-paper-url="./papers/250623157v1-std-gs-exploring-frame-event-interaction-for-spatiotemporal-disentan.html" onclick="toggleFavorite(this, '2506.23157v1', 'STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250623120v1-enhancing-spatial-reasoning-in-multimodal-large-language-models-thro.html">Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation</a></td>
  <td>æå‡ºRÂ²Sæ¡†æ¶ä»¥è§£å†³å¤æ‚ç©ºé—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23120v1" data-paper-url="./papers/250623120v1-enhancing-spatial-reasoning-in-multimodal-large-language-models-thro.html" onclick="toggleFavorite(this, '2506.23120v1', 'Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250623207v1-tvg-slam-robust-gaussian-splatting-slam-with-tri-view-geometric-cons.html">TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints</a></td>
  <td>æå‡ºTVG-SLAMä»¥è§£å†³RGB-only SLAMç³»ç»Ÿçš„é²æ£’æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23207v1" data-paper-url="./papers/250623207v1-tvg-slam-robust-gaussian-splatting-slam-with-tri-view-geometric-cons.html" onclick="toggleFavorite(this, '2506.23207v1', 'TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250623329v1-ir3d-bench-evaluating-vision-language-model-scene-understanding-as-a.html">IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering</a></td>
  <td>æå‡ºIR3D-Benchä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœºæ™¯ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23329v1" data-paper-url="./papers/250623329v1-ir3d-bench-evaluating-vision-language-model-scene-understanding-as-a.html" onclick="toggleFavorite(this, '2506.23329v1', 'IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250623151v1-memfof-high-resolution-training-for-memory-efficient-multi-frame-opt.html">MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation</a></td>
  <td>æå‡ºMEMFOFä»¥è§£å†³é«˜åˆ†è¾¨ç‡å…‰æµä¼°è®¡ä¸­çš„å†…å­˜æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23151v1" data-paper-url="./papers/250623151v1-memfof-high-resolution-training-for-memory-efficient-multi-frame-opt.html" onclick="toggleFavorite(this, '2506.23151v1', 'MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250623153v1-dynamic-view-synthesis-from-small-camera-motion-videos.html">Dynamic View Synthesis from Small Camera Motion Videos</a></td>
  <td>æå‡ºåŸºäºåˆ†å¸ƒçš„æ·±åº¦æ­£åˆ™åŒ–ä»¥è§£å†³å°ç›¸æœºè¿åŠ¨ä¸‹çš„åŠ¨æ€è§†å›¾åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23153v1" data-paper-url="./papers/250623153v1-dynamic-view-synthesis-from-small-camera-motion-videos.html" onclick="toggleFavorite(this, '2506.23153v1', 'Dynamic View Synthesis from Small Camera Motion Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250623270v1-token-activation-map-to-visually-explain-multimodal-llms.html">Token Activation Map to Visually Explain Multimodal LLMs</a></td>
  <td>æå‡ºToken Activation Mapä»¥è§£å†³å¤šæ¨¡æ€LLMå¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23270v1" data-paper-url="./papers/250623270v1-token-activation-map-to-visually-explain-multimodal-llms.html" onclick="toggleFavorite(this, '2506.23270v1', 'Token Activation Map to Visually Explain Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250623102v1-medregion-ct-region-focused-multimodal-llm-for-comprehensive-3d-ct-r.html">MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation</a></td>
  <td>æå‡ºMedRegion-CTä»¥è§£å†³CTæŠ¥å‘Šç”Ÿæˆä¸­çš„åŒºåŸŸç‰¹å¾æ•æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23102v1" data-paper-url="./papers/250623102v1-medregion-ct-region-focused-multimodal-llm-for-comprehensive-3d-ct-r.html" onclick="toggleFavorite(this, '2506.23102v1', 'MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250702955v1-multimodal-image-registration-for-effective-thermographic-fever-scre.html">Multimodal image registration for effective thermographic fever screening</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å›¾åƒé…å‡†æ–¹æ³•ä»¥æé«˜çƒ­æˆåƒå‘çƒ­ç­›æŸ¥çš„å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.02955v1" data-paper-url="./papers/250702955v1-multimodal-image-registration-for-effective-thermographic-fever-scre.html" onclick="toggleFavorite(this, '2507.02955v1', 'Multimodal image registration for effective thermographic fever screening')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250623361v2-omnivcus-feedforward-subject-driven-video-customization-with-multimo.html">OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions</a></td>
  <td>æå‡ºOmniVCusä»¥è§£å†³å¤šä¸»ä½“è§†é¢‘å®šåˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23361v2" data-paper-url="./papers/250623361v2-omnivcus-feedforward-subject-driven-video-customization-with-multimo.html" onclick="toggleFavorite(this, '2506.23361v2', 'OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250623219v1-urbanllava-a-multi-modal-large-language-model-for-urban-intelligence.html">UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding</a></td>
  <td>æå‡ºUrbanLLaVAä»¥è§£å†³åŸå¸‚æ™ºèƒ½ä¸­çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23219v1" data-paper-url="./papers/250623219v1-urbanllava-a-multi-modal-large-language-model-for-urban-intelligence.html" onclick="toggleFavorite(this, '2506.23219v1', 'UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250623352v1-geoprog3d-compositional-visual-reasoning-for-city-scale-3d-language-.html">GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields</a></td>
  <td>æå‡ºGeoProg3Dä»¥è§£å†³åŸå¸‚è§„æ¨¡3Dè¯­è¨€åœºæ™¯äº¤äº’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23352v1" data-paper-url="./papers/250623352v1-geoprog3d-compositional-visual-reasoning-for-city-scale-3d-language-.html" onclick="toggleFavorite(this, '2506.23352v1', 'GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250623196v1-del-dense-event-localization-for-multi-modal-audio-visual-understand.html">DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding</a></td>
  <td>æå‡ºDELæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘ä¸­çš„å¯†é›†äº‹ä»¶å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23196v1" data-paper-url="./papers/250623196v1-del-dense-event-localization-for-multi-modal-audio-visual-understand.html" onclick="toggleFavorite(this, '2506.23196v1', 'DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250623132v1-dare-to-plagiarize-plagiarized-painting-recognition-and-retrieval.html">Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval</a></td>
  <td>æå‡ºè‰ºæœ¯ä½œå“æŠ„è¢­æ£€æµ‹æ–¹æ³•ä»¥ä¿æŠ¤ç‰ˆæƒ</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23132v1" data-paper-url="./papers/250623132v1-dare-to-plagiarize-plagiarized-painting-recognition-and-retrieval.html" onclick="toggleFavorite(this, '2506.23132v1', 'Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250623283v1-moma-modulating-mamba-for-adapting-image-foundation-models-to-video-.html">MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition</a></td>
  <td>æå‡ºMoMaæ¡†æ¶ä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23283v1" data-paper-url="./papers/250623283v1-moma-modulating-mamba-for-adapting-image-foundation-models-to-video-.html" onclick="toggleFavorite(this, '2506.23283v1', 'MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250623323v3-fa-seg-a-fast-and-accurate-diffusion-based-method-for-open-vocabular.html">FA-Seg: A Fast and Accurate Diffusion-Based Method for Open-Vocabulary Segmentation</a></td>
  <td>æå‡ºFA-Segä»¥è§£å†³å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„ç²¾åº¦ä¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23323v3" data-paper-url="./papers/250623323v3-fa-seg-a-fast-and-accurate-diffusion-based-method-for-open-vocabular.html" onclick="toggleFavorite(this, '2506.23323v3', 'FA-Seg: A Fast and Accurate Diffusion-Based Method for Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250623115v1-moca-modality-aware-continual-pre-training-makes-better-bidirectiona.html">MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings</a></td>
  <td>æå‡ºMoCaä»¥è§£å†³å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„å…³é”®é™åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23115v1" data-paper-url="./papers/250623115v1-moca-modality-aware-continual-pre-training-makes-better-bidirectiona.html" onclick="toggleFavorite(this, '2506.23115v1', 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250623135v1-roboscape-physics-informed-embodied-world-model.html">RoboScape: Physics-informed Embodied World Model</a></td>
  <td>æå‡ºRoboScapeä»¥è§£å†³ç°æœ‰æœºå™¨äººè§†é¢‘ç”Ÿæˆçš„ç‰©ç†æ„è¯†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23135v1" data-paper-url="./papers/250623135v1-roboscape-physics-informed-embodied-world-model.html" onclick="toggleFavorite(this, '2506.23135v1', 'RoboScape: Physics-informed Embodied World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250623156v1-self-supervised-contrastive-learning-for-multi-label-images.html">Self-Supervised Contrastive Learning for Multi-Label Images</a></td>
  <td>æå‡ºè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä»¥è§£å†³å¤šæ ‡ç­¾å›¾åƒè¡¨ç¤ºå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23156v1" data-paper-url="./papers/250623156v1-self-supervised-contrastive-learning-for-multi-label-images.html" onclick="toggleFavorite(this, '2506.23156v1', 'Self-Supervised Contrastive Learning for Multi-Label Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250623285v1-competitive-distillation-a-simple-learning-strategy-for-improving-vi.html">Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification</a></td>
  <td>æå‡ºç«äº‰è’¸é¦ç­–ç•¥ä»¥æå‡è§†è§‰åˆ†ç±»æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23285v1" data-paper-url="./papers/250623285v1-competitive-distillation-a-simple-learning-strategy-for-improving-vi.html" onclick="toggleFavorite(this, '2506.23285v1', 'Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250623236v1-volumetricsmpl-a-neural-volumetric-body-model-for-efficient-interact.html">VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions</a></td>
  <td>æå‡ºVolumetricSMPLä»¥è§£å†³é«˜æ•ˆäººæœºäº¤äº’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion synthesis</span> <span class="paper-tag">human-object interaction</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23236v1" data-paper-url="./papers/250623236v1-volumetricsmpl-a-neural-volumetric-body-model-for-efficient-interact.html" onclick="toggleFavorite(this, '2506.23236v1', 'VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250623205v1-bridgeshape-latent-diffusion-schrÃ¶dinger-bridge-for-3d-shape-complet.html">BridgeShape: Latent Diffusion SchrÃ¶dinger Bridge for 3D Shape Completion</a></td>
  <td>æå‡ºBridgeShapeä»¥è§£å†³3Då½¢çŠ¶è¡¥å…¨ä¸­çš„å…¨å±€ä¼ è¾“è·¯å¾„å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VQ-VAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23205v1" data-paper-url="./papers/250623205v1-bridgeshape-latent-diffusion-schrÃ¶dinger-bridge-for-3d-shape-complet.html" onclick="toggleFavorite(this, '2506.23205v1', 'BridgeShape: Latent Diffusion SchrÃ¶dinger Bridge for 3D Shape Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250623418v1-why-settle-for-mid-a-probabilistic-viewpoint-to-spatial-relationship.html">Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models</a></td>
  <td>æå‡ºæ¦‚ç‡æ¡†æ¶ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´å…³ç³»å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23418v1" data-paper-url="./papers/250623418v1-why-settle-for-mid-a-probabilistic-viewpoint-to-spatial-relationship.html" onclick="toggleFavorite(this, '2506.23418v1', 'Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250623263v1-causal-entity-reflected-egocentric-traffic-accident-video-synthesis.html">Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis</a></td>
  <td>æå‡ºCausal-VidSynä»¥è§£å†³äº¤é€šäº‹æ•…è§†é¢‘åˆæˆä¸­çš„å› æœå…³ç³»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23263v1" data-paper-url="./papers/250623263v1-causal-entity-reflected-egocentric-traffic-accident-video-synthesis.html" onclick="toggleFavorite(this, '2506.23263v1', 'Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/250623189v1-trident-detecting-face-forgeries-with-adversarial-triplet-learning.html">Trident: Detecting Face Forgeries with Adversarial Triplet Learning</a></td>
  <td>æå‡ºTridentæ¡†æ¶ä»¥è§£å†³é¢éƒ¨ä¼ªé€ æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.23189v1" data-paper-url="./papers/250623189v1-trident-detecting-face-forgeries-with-adversarial-triplet-learning.html" onclick="toggleFavorite(this, '2506.23189v1', 'Trident: Detecting Face Forgeries with Adversarial Triplet Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)