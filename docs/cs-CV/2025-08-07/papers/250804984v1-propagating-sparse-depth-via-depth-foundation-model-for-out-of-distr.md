---
layout: default
title: Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion
---

# Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04984" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04984v1</a>
  <a href="https://arxiv.org/pdf/2508.04984.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04984v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04984v1', 'Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shenglun Chen, Xinzhu Ma, Hong Zhang, Haojie Li, Zhihui Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-07

**Â§áÊ≥®**: Accepted by IEEE TIP

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/shenglunch/PSD)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê∑±Â∫¶Âü∫Á°ÄÊ®°Âûã‰ª•Ëß£ÂÜ≥ÂàÜÂ∏ÉÂ§ñÊ∑±Â∫¶Ë°•ÂÖ®ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Ë°•ÂÖ®` `Ê∑±Â∫¶Âü∫Á°ÄÊ®°Âûã` `ÂàÜÂ∏ÉÂ§ñÂ≠¶‰π†` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `ÁéØÂ¢ÉÊÑüÁü•` `È≤ÅÊ£íÊÄß` `ÂèåÁ©∫Èó¥‰º†Êí≠`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊ∑±Â∫¶Ë°•ÂÖ®ÊñπÊ≥ïÂú®ÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ‰∏≠ÊÄßËÉΩ‰∏ãÈôçÔºåÁº∫‰πèË∂≥Â§üÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
2. ÊèêÂá∫Âà©Áî®Ê∑±Â∫¶Âü∫Á°ÄÊ®°ÂûãÊèêÂèñÁéØÂ¢ÉÁ∫øÁ¥¢ÔºåËÆæËÆ°ÂèåÁ©∫Èó¥‰º†Êí≠ÊñπÊ≥ï‰ª•Â¢ûÂº∫Ê∑±Â∫¶Ë°•ÂÖ®ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. Âú®NYUv2ÂíåKITTIÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÊ°ÜÊû∂Âú®16‰∏™ÂÖ∂‰ªñÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂äÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶Ë°•ÂÖ®ÊòØËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàòÔºåÊó®Âú®‰ªéÁ®ÄÁñèÊ∑±Â∫¶ÂõæÈáçÂª∫ÂØÜÈõÜÊ∑±Â∫¶ÂõæÔºåÈÄöÂ∏∏ÈúÄË¶ÅÈÖçÂØπÁöÑRGBÂõæÂÉè„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÂ≠¶‰π†ÁöÑÊñπÊ≥ï‰æùËµñ‰∫éÁ≤æÂøÉÂáÜÂ§á‰ΩÜÊúâÈôêÁöÑÊï∞ÊçÆÔºåÂØºËá¥Âú®ÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ‰∏≠ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑±Â∫¶Ë°•ÂÖ®Ê°ÜÊû∂ÔºåÂà©Áî®Ê∑±Â∫¶Âü∫Á°ÄÊ®°ÂûãÊèêÂèñÁéØÂ¢ÉÁ∫øÁ¥¢Ôºå‰ª•ÂºïÂØºÁ®ÄÁñèÊ∑±Â∫¶‰ø°ÊÅØÁöÑ‰º†Êí≠„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊó†ÂèØÂ≠¶‰π†ÂèÇÊï∞ÁöÑÂèåÁ©∫Èó¥‰º†Êí≠ÊñπÊ≥ïÔºåÊúâÊïàÂú∞Âú®3DÂíå2DÁ©∫Èó¥‰∏≠‰º†Êí≠Á®ÄÁñèÊ∑±Â∫¶Ôºå‰ª•‰øùÊåÅÂá†‰ΩïÁªìÊûÑÂíåÂ±ÄÈÉ®‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂèØÂ≠¶‰π†ÁöÑ‰øÆÊ≠£Ê®°ÂùóÔºåÈÄêÊ≠•Ë∞ÉÊï¥Ê∑±Â∫¶È¢ÑÊµã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®16‰∏™ÂÖ∂‰ªñÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊ∑±Â∫¶Ë°•ÂÖ®ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Ë°•ÂÖ®‰∏≠ÁöÑÂàÜÂ∏ÉÂ§ñÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Èù¢ÂØπÊú™ËßÅËøáÁöÑÊï∞ÊçÆÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂØºËá¥Ê∑±Â∫¶ÈáçÂª∫ÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂà©Áî®Ê∑±Â∫¶Âü∫Á°ÄÊ®°ÂûãÊèêÂèñRGBÂõæÂÉè‰∏≠ÁöÑÁªìÊûÑÂíåËØ≠‰πâ‰ø°ÊÅØÔºåÂºïÂØºÁ®ÄÁñèÊ∑±Â∫¶‰ø°ÊÅØÁöÑ‰º†Êí≠Ôºå‰ªéËÄåÂ¢ûÂº∫Ê∑±Â∫¶Ë°•ÂÖ®Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåÊó†ÈúÄÂ§ßËßÑÊ®°ËÆ≠ÁªÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Ê∑±Â∫¶Âü∫Á°ÄÊ®°Âûã„ÄÅÂèåÁ©∫Èó¥‰º†Êí≠Ê®°ÂùóÂíåÂèØÂ≠¶‰π†ÁöÑ‰øÆÊ≠£Ê®°Âùó„ÄÇÈ¶ñÂÖàÔºåÊ∑±Â∫¶Âü∫Á°ÄÊ®°ÂûãÊèêÂèñÁéØÂ¢ÉÁ∫øÁ¥¢ÔºåÁÑ∂ÂêéÈÄöËøáÂèåÁ©∫Èó¥‰º†Êí≠Âú®3DÂíå2DÁ©∫Èó¥‰∏≠‰º†Êí≠Á®ÄÁñèÊ∑±Â∫¶ÔºåÊúÄÂêéÈÄöËøá‰øÆÊ≠£Ê®°ÂùóË∞ÉÊï¥È¢ÑÊµãÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éËÆæËÆ°‰∫Ü‰∏ÄÁßçÊó†Â≠¶‰π†ÂèÇÊï∞ÁöÑÂèåÁ©∫Èó¥‰º†Êí≠ÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïà‰øùÊåÅÂá†‰ΩïÁªìÊûÑÂíåÂ±ÄÈÉ®‰∏ÄËá¥ÊÄßÔºåËøôÊòØ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°Âûã‰∏≠ÔºåÂèåÁ©∫Èó¥‰º†Êí≠ÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÂèØÂ≠¶‰π†ÂèÇÊï∞ÔºåÁ°Æ‰øù‰∫Ü‰º†Êí≠ËøáÁ®ãÁöÑÁ®≥ÂÆöÊÄßÂíå‰∏ÄËá¥ÊÄßÔºõ‰øÆÊ≠£Ê®°ÂùóÂàôÈÄöËøáÈÄêÊ≠•Ë∞ÉÊï¥Ê∑±Â∫¶È¢ÑÊµãÔºåÊèêÂçá‰∫ÜÊúÄÁªàÁöÑÊ∑±Â∫¶ÈáçÂª∫Ë¥®Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊú¨ÊñáÊ°ÜÊû∂Âú®16‰∏™ÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊ∑±Â∫¶Ë°•ÂÖ®ÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Ëá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™ÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÊ∑±Â∫¶Ë°•ÂÖ®ÁöÑÈ≤ÅÊ£íÊÄßÔºåÂèØ‰ª•Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑÊ∑±Â∫¶ÊÑüÁü•Ôºå‰ªéËÄåÊèêÂçáÁ≥ªÁªüÁöÑÊï¥‰ΩìÊÄßËÉΩÂíåÂèØÈù†ÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®Êõ¥Â§öÂü∫‰∫éÊ∑±Â∫¶‰ø°ÊÅØÁöÑÂ∫îÁî®ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Depth completion is a pivotal challenge in computer vision, aiming at reconstructing the dense depth map from a sparse one, typically with a paired RGB image. Existing learning based models rely on carefully prepared but limited data, leading to significant performance degradation in out-of-distribution (OOD) scenarios. Recent foundation models have demonstrated exceptional robustness in monocular depth estimation through large-scale training, and using such models to enhance the robustness of depth completion models is a promising solution. In this work, we propose a novel depth completion framework that leverages depth foundation models to attain remarkable robustness without large-scale training. Specifically, we leverage a depth foundation model to extract environmental cues, including structural and semantic context, from RGB images to guide the propagation of sparse depth information into missing regions. We further design a dual-space propagation approach, without any learnable parameters, to effectively propagates sparse depth in both 3D and 2D spaces to maintain geometric structure and local consistency. To refine the intricate structure, we introduce a learnable correction module to progressively adjust the depth prediction towards the real depth. We train our model on the NYUv2 and KITTI datasets as in-distribution datasets and extensively evaluate the framework on 16 other datasets. Our framework performs remarkably well in the OOD scenarios and outperforms existing state-of-the-art depth completion methods. Our models are released in https://github.com/shenglunch/PSD.

