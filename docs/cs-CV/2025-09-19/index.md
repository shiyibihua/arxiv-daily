---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-19
---

# cs.CVï¼ˆ2025-09-19ï¼‰

ğŸ“Š å…± **40** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250916149v1-pointing-to-a-llama-and-call-it-a-camel-on-the-sycophancy-of-multimo.html">Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models</a></td>
  <td>é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è°„åªšé—®é¢˜ï¼Œæå‡ºè‡ªåæ€å¾®è°ƒæ–¹æ³•SRT</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16149v1" data-paper-url="./papers/250916149v1-pointing-to-a-llama-and-call-it-a-camel-on-the-sycophancy-of-multimo.html" onclick="toggleFavorite(this, '2509.16149v1', 'Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250916087v1-seetrek-training-free-spatial-prompting-for-multimodal-large-languag.html">See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model</a></td>
  <td>æå‡ºSEE&TREKï¼Œä¸€ç§å…è®­ç»ƒçš„ç©ºé—´æç¤ºæ¡†æ¶ï¼Œæå‡MLLMçš„è§†è§‰ç©ºé—´ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16087v1" data-paper-url="./papers/250916087v1-seetrek-training-free-spatial-prompting-for-multimodal-large-languag.html" onclick="toggleFavorite(this, '2509.16087v1', 'See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250916054v2-language-instructed-reasoning-for-group-activity-detection-via-multi.html">Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model</a></td>
  <td>æå‡ºLIR-GADï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­è¨€æŒ‡å¯¼çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16054v2" data-paper-url="./papers/250916054v2-language-instructed-reasoning-for-group-activity-detection-via-multi.html" onclick="toggleFavorite(this, '2509.16054v2', 'Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250915602v2-tennistv-do-multimodal-large-language-models-understand-tennis-ralli.html">TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</a></td>
  <td>æå‡ºTennisTVåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç½‘çƒè§†é¢‘ç†è§£ä¸­çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15602v2" data-paper-url="./papers/250915602v2-tennistv-do-multimodal-large-language-models-understand-tennis-ralli.html" onclick="toggleFavorite(this, '2509.15602v2', 'TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250916197v1-manzano-a-simple-and-scalable-unified-multimodal-model-with-a-hybrid.html">MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer</a></td>
  <td>Manzanoï¼šä¸€ç§åŸºäºæ··åˆè§†è§‰Tokençš„ç®€å•å¯æ‰©å±•ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16197v1" data-paper-url="./papers/250916197v1-manzano-a-simple-and-scalable-unified-multimodal-model-with-a-hybrid.html" onclick="toggleFavorite(this, '2509.16197v1', 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250915874v1-ensam-an-efficient-foundation-model-for-interactive-segmentation-of-.html">ENSAM: an efficient foundation model for interactive segmentation of 3D medical images</a></td>
  <td>ENSAMï¼šä¸€ç§é«˜æ•ˆçš„ä¸‰ç»´åŒ»å­¦å›¾åƒäº¤äº’åˆ†å‰²åŸºç¡€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15874v1" data-paper-url="./papers/250915874v1-ensam-an-efficient-foundation-model-for-interactive-segmentation-of-.html" onclick="toggleFavorite(this, '2509.15874v1', 'ENSAM: an efficient foundation model for interactive segmentation of 3D medical images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250921352v1-improving-autism-detection-with-multimodal-behavioral-analysis.html">Improving Autism Detection with Multimodal Behavioral Analysis</a></td>
  <td>æå‡ºåŸºäºå¤šæ¨¡æ€è¡Œä¸ºåˆ†æçš„è‡ªé—­ç—‡æ£€æµ‹æ–¹æ³•ï¼Œæå‡è¯Šæ–­å‡†ç¡®ç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21352v1" data-paper-url="./papers/250921352v1-improving-autism-detection-with-multimodal-behavioral-analysis.html" onclick="toggleFavorite(this, '2509.21352v1', 'Improving Autism Detection with Multimodal Behavioral Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250918189v1-qianfan-vl-domain-enhanced-universal-vision-language-models.html">Qianfan-VL: Domain-Enhanced Universal Vision-Language Models</a></td>
  <td>æå‡ºQianfan-VLï¼Œé€šè¿‡é¢†åŸŸå¢å¼ºæŠ€æœ¯å®ç°é¢†å…ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.18189v1" data-paper-url="./papers/250918189v1-qianfan-vl-domain-enhanced-universal-vision-language-models.html" onclick="toggleFavorite(this, '2509.18189v1', 'Qianfan-VL: Domain-Enhanced Universal Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250915578v1-multimodal-learning-for-fake-news-detection-in-short-videos-using-li.html">Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion</a></td>
  <td>æå‡ºå¼‚æ„èåˆç½‘ç»œHFNï¼Œç”¨äºçŸ­è§†é¢‘å‡æ–°é—»æ£€æµ‹ï¼Œæå‡å¤šæ¨¡æ€ä¿¡æ¯åˆ©ç”¨ç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15578v1" data-paper-url="./papers/250915578v1-multimodal-learning-for-fake-news-detection-in-short-videos-using-li.html" onclick="toggleFavorite(this, '2509.15578v1', 'Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250915596v2-eyepcr-a-comprehensive-benchmark-for-fine-grained-perception-knowled.html">EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery</a></td>
  <td>EyePCRï¼šçœ¼ç§‘æ‰‹æœ¯ä¸­ç»†ç²’åº¦æ„ŸçŸ¥ã€çŸ¥è¯†ç†è§£å’Œä¸´åºŠæ¨ç†çš„ç»¼åˆåŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15596v2" data-paper-url="./papers/250915596v2-eyepcr-a-comprehensive-benchmark-for-fine-grained-perception-knowled.html" onclick="toggleFavorite(this, '2509.15596v2', 'EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250916438v1-autoarabic-a-three-stage-framework-for-localizing-video-text-retriev.html">AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks</a></td>
  <td>AutoArabicï¼šæå‡ºä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºè§†é¢‘-æ–‡æœ¬æ£€ç´¢åŸºå‡†çš„é˜¿æ‹‰ä¼¯è¯­æœ¬åœ°åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16438v1" data-paper-url="./papers/250916438v1-autoarabic-a-three-stage-framework-for-localizing-video-text-retriev.html" onclick="toggleFavorite(this, '2509.16438v1', 'AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250916163v1-robust-vision-language-models-via-tensor-decomposition-a-defense-aga.html">Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks</a></td>
  <td>æå‡ºè½»é‡çº§å¼ é‡åˆ†è§£æ–¹æ³•ä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16163v1" data-paper-url="./papers/250916163v1-robust-vision-language-models-via-tensor-decomposition-a-defense-aga.html" onclick="toggleFavorite(this, '2509.16163v1', 'Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250915704v2-pyramid-token-pruning-for-high-resolution-large-vision-language-mode.html">Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance</a></td>
  <td>æå‡ºé‡‘å­—å¡”Tokenå‰ªæï¼ˆPTPï¼‰ç­–ç•¥ï¼Œè§£å†³é«˜åˆ†è¾¨ç‡å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è®¡ç®—å¼€é”€é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15704v2" data-paper-url="./papers/250915704v2-pyramid-token-pruning-for-high-resolution-large-vision-language-mode.html" onclick="toggleFavorite(this, '2509.15704v2', 'Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250915546v1-enhancing-sa2va-for-referent-video-object-segmentation-2nd-solution-.html">Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track</a></td>
  <td>é’ˆå¯¹æŒ‡ä»£è¡¨è¾¾è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼Œæå‡ºè§†é¢‘è¯­è¨€æ£€æŸ¥å™¨ä¸å…³é”®å¸§é‡‡æ ·æ–¹æ³•ï¼Œæ˜¾è‘—æå‡Sa2VAæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15546v1" data-paper-url="./papers/250915546v1-enhancing-sa2va-for-referent-video-object-segmentation-2nd-solution-.html" onclick="toggleFavorite(this, '2509.15546v1', 'Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250915496v1-lynx-towards-high-fidelity-personalized-video-generation.html">Lynx: Towards High-Fidelity Personalized Video Generation</a></td>
  <td>Lynxï¼šé¢å‘é«˜ä¿çœŸä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆçš„æ‰©æ•£Transformeræ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15496v1" data-paper-url="./papers/250915496v1-lynx-towards-high-fidelity-personalized-video-generation.html" onclick="toggleFavorite(this, '2509.15496v1', 'Lynx: Towards High-Fidelity Personalized Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250915548v4-ms-gs-multi-appearance-sparse-view-3d-gaussian-splatting-in-the-wild.html">MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</a></td>
  <td>æå‡ºMS-GSï¼Œåˆ©ç”¨å¤šå¤–è§‚ç¨€ç–è§†å›¾3Dé«˜æ–¯æº…å°„é‡å»ºé‡å¤–åœºæ™¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">3D gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15548v4" data-paper-url="./papers/250915548v4-ms-gs-multi-appearance-sparse-view-3d-gaussian-splatting-in-the-wild.html" onclick="toggleFavorite(this, '2509.15548v4', 'MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250915871v1-zero-shot-visual-grounding-in-3d-gaussians-via-view-retrieval.html">Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval</a></td>
  <td>æå‡ºGVRï¼Œé€šè¿‡è§†å›¾æ£€ç´¢å®ç°3Dé«˜æ–¯åœºæ™¯çš„é›¶æ ·æœ¬è§†è§‰å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15871v1" data-paper-url="./papers/250915871v1-zero-shot-visual-grounding-in-3d-gaussians-via-view-retrieval.html" onclick="toggleFavorite(this, '2509.15871v1', 'Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250915648v1-fingersplat-contactless-fingerprint-3d-reconstruction-and-generation.html">FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting</a></td>
  <td>FingerSplatï¼šåŸºäº3Dé«˜æ–¯æº…å°„çš„éæ¥è§¦å¼æŒ‡çº¹3Dé‡å»ºä¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15648v1" data-paper-url="./papers/250915648v1-fingersplat-contactless-fingerprint-3d-reconstruction-and-generation.html" onclick="toggleFavorite(this, '2509.15648v1', 'FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250915645v1-gs-scale-unlocking-large-scale-3d-gaussian-splatting-training-via-ho.html">GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading</a></td>
  <td>GS-Scaleï¼šé€šè¿‡ä¸»æœºå¸è½½è§£é”å¤§è§„æ¨¡3Dé«˜æ–¯æº…å°„è®­ç»ƒ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15645v1" data-paper-url="./papers/250915645v1-gs-scale-unlocking-large-scale-3d-gaussian-splatting-training-via-ho.html" onclick="toggleFavorite(this, '2509.15645v1', 'GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250915924v1-sparse-multiview-open-vocabulary-3d-detection.html">Sparse Multiview Open-Vocabulary 3D Detection</a></td>
  <td>æå‡ºä¸€ç§ç¨€ç–å¤šè§†è§’å¼€æ”¾è¯æ±‡3Dæ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒä¸”æ€§èƒ½ä¼˜å¼‚</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15924v1" data-paper-url="./papers/250915924v1-sparse-multiview-open-vocabulary-3d-detection.html" onclick="toggleFavorite(this, '2509.15924v1', 'Sparse Multiview Open-Vocabulary 3D Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250916415v1-stereoadapter-adapting-stereo-depth-estimation-to-underwater-scenes.html">StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes</a></td>
  <td>æå‡ºStereoAdapterä»¥è§£å†³æ°´ä¸‹åœºæ™¯æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">stereo depth</span> <span class="paper-tag">metric depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16415v1" data-paper-url="./papers/250916415v1-stereoadapter-adapting-stereo-depth-estimation-to-underwater-scenes.html" onclick="toggleFavorite(this, '2509.16415v1', 'StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250915886v3-rangesam-on-the-potential-of-visual-foundation-models-for-range-view.html">RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation</a></td>
  <td>RangeSAMï¼šæ¢ç´¢è§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ¿€å…‰é›·è¾¾Range-Viewåˆ†å‰²ä¸­çš„æ½œåŠ›</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15886v3" data-paper-url="./papers/250915886v3-rangesam-on-the-potential-of-visual-foundation-models-for-range-view.html" onclick="toggleFavorite(this, '2509.15886v3', 'RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250915980v1-shedding-light-on-depth-explainability-assessment-in-monocular-depth.html">Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation</a></td>
  <td>å•ç›®æ·±åº¦ä¼°è®¡å¯è§£é‡Šæ€§ç ”ç©¶ï¼šæå‡ºAttribution Fidelityè¯„ä¼°è§£é‡Šå¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15980v1" data-paper-url="./papers/250915980v1-shedding-light-on-depth-explainability-assessment-in-monocular-depth.html" onclick="toggleFavorite(this, '2509.15980v1', 'Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250915987v2-towards-sharper-object-boundaries-in-self-supervised-depth-estimatio.html">Towards Sharper Object Boundaries in Self-Supervised Depth Estimation</a></td>
  <td>æå‡ºåŸºäºæ··åˆåˆ†å¸ƒçš„è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ï¼Œæ˜¾è‘—æå‡ç‰©ä½“è¾¹ç•Œæ¸…æ™°åº¦</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15987v2" data-paper-url="./papers/250915987v2-towards-sharper-object-boundaries-in-self-supervised-depth-estimatio.html" onclick="toggleFavorite(this, '2509.15987v2', 'Towards Sharper Object Boundaries in Self-Supervised Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250915677v1-camera-splatting-for-continuous-view-optimization.html">Camera Splatting for Continuous View Optimization</a></td>
  <td>æå‡ºCamera Splattingï¼Œé€šè¿‡è¿ç»­è§†è§’ä¼˜åŒ–å®ç°é«˜è´¨é‡æ–°è§†è§’åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15677v1" data-paper-url="./papers/250915677v1-camera-splatting-for-continuous-view-optimization.html" onclick="toggleFavorite(this, '2509.15677v1', 'Camera Splatting for Continuous View Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250916423v2-3d-gaussian-flats-hybrid-2d3d-photometric-scene-reconstruction.html">3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction</a></td>
  <td>æå‡ºæ··åˆ2D/3Dé«˜æ–¯å¹³é¢è¡¨ç¤ºï¼Œæå‡çº¹ç†ç¼ºå¤±åœºæ™¯çš„ä¸‰ç»´é‡å»ºè´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16423v2" data-paper-url="./papers/250916423v2-3d-gaussian-flats-hybrid-2d3d-photometric-scene-reconstruction.html" onclick="toggleFavorite(this, '2509.16423v2', '3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250916119v1-radargaussiandet3d-an-efficient-and-effective-gaussian-based-3d-dete.html">RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars</a></td>
  <td>RadarGaussianDet3Dï¼šåŸºäºé«˜æ–¯åˆ†å¸ƒçš„4Dæ¯«ç±³æ³¢é›·è¾¾é«˜æ•ˆ3Dç›®æ ‡æ£€æµ‹å™¨</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16119v1" data-paper-url="./papers/250916119v1-radargaussiandet3d-an-efficient-and-effective-gaussian-based-3d-dete.html" onclick="toggleFavorite(this, '2509.16119v1', 'RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250915891v1-global-regulation-and-excitation-via-attention-tuning-for-stereo-mat.html">Global Regulation and Excitation via Attention Tuning for Stereo Matching</a></td>
  <td>æå‡ºGREATæ¡†æ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç«‹ä½“åŒ¹é…å…¨å±€ä¸Šä¸‹æ–‡å’Œå‡ ä½•ä¿¡æ¯ï¼Œæå‡ç—…æ€åŒºåŸŸåŒ¹é…ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15891v1" data-paper-url="./papers/250915891v1-global-regulation-and-excitation-via-attention-tuning-for-stereo-mat.html" onclick="toggleFavorite(this, '2509.15891v1', 'Global Regulation and Excitation via Attention Tuning for Stereo Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250916017v1-distillmatch-leveraging-knowledge-distillation-from-vision-foundatio.html">DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching</a></td>
  <td>æå‡ºDistillMatchï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†è’¸é¦è¿›è¡Œå¤šæ¨¡æ€å›¾åƒåŒ¹é…</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16017v1" data-paper-url="./papers/250916017v1-distillmatch-leveraging-knowledge-distillation-from-vision-foundatio.html" onclick="toggleFavorite(this, '2509.16017v1', 'DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250916127v1-basereward-a-strong-baseline-for-multimodal-reward-model.html">BaseReward: A Strong Baseline for Multimodal Reward Model</a></td>
  <td>BaseRewardï¼šå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†ï¼Œä¸ºMLLMå¯¹é½æä¾›å®ç”¨æŒ‡å—</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.16127v1" data-paper-url="./papers/250916127v1-basereward-a-strong-baseline-for-multimodal-reward-model.html" onclick="toggleFavorite(this, '2509.16127v1', 'BaseReward: A Strong Baseline for Multimodal Reward Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250915642v2-univ-unified-foundation-model-for-infrared-and-visible-modalities.html">UNIV: Unified Foundation Model for Infrared and Visible Modalities</a></td>
  <td>æå‡ºUNIVä»¥è§£å†³çº¢å¤–ä¸å¯è§å…‰æ¨¡æ€çš„è·¨æ¨¡æ€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15642v2" data-paper-url="./papers/250915642v2-univ-unified-foundation-model-for-infrared-and-visible-modalities.html" onclick="toggleFavorite(this, '2509.15642v2', 'UNIV: Unified Foundation Model for Infrared and Visible Modalities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250915563v1-dc-mamba-bi-temporal-deformable-alignment-and-scale-sparse-enhanceme.html">DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection</a></td>
  <td>DC-Mambaï¼šé¥æ„Ÿå½±åƒå˜åŒ–æ£€æµ‹ä¸­ï¼Œé€šè¿‡å¯å˜å½¢å¯¹é½ä¸å°ºåº¦ç¨€ç–å¢å¼ºæå‡æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15563v1" data-paper-url="./papers/250915563v1-dc-mamba-bi-temporal-deformable-alignment-and-scale-sparse-enhanceme.html" onclick="toggleFavorite(this, '2509.15563v1', 'DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250921351v1-random-direct-preference-optimization-for-radiography-report-generat.html">Random Direct Preference Optimization for Radiography Report Generation</a></td>
  <td>æå‡ºåŸºäºéšæœºç›´æ¥åå¥½ä¼˜åŒ–çš„èƒ¸ç‰‡æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼Œæå‡ä¸´åºŠæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.21351v1" data-paper-url="./papers/250921351v1-random-direct-preference-optimization-for-radiography-report-generat.html" onclick="toggleFavorite(this, '2509.21351v1', 'Random Direct Preference Optimization for Radiography Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250922688v2-robust-object-detection-for-autonomous-driving-via-curriculum-guided.html">Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization</a></td>
  <td>æå‡ºè¯¾ç¨‹å¼•å¯¼çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œæå‡è‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">reward design</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.22688v2" data-paper-url="./papers/250922688v2-robust-object-detection-for-autonomous-driving-via-curriculum-guided.html" onclick="toggleFavorite(this, '2509.22688v2', 'Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250915536v2-samposcale-wise-autoregression-with-motion-prompt-for-generative-wor.html">SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</a></td>
  <td>SAMPOï¼šåŸºäºè¿åŠ¨æç¤ºçš„åˆ†å°ºåº¦è‡ªå›å½’ç”Ÿæˆä¸–ç•Œæ¨¡å‹ï¼Œæå‡è§†é¢‘é¢„æµ‹è´¨é‡ä¸æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15536v2" data-paper-url="./papers/250915536v2-samposcale-wise-autoregression-with-motion-prompt-for-generative-wor.html" onclick="toggleFavorite(this, '2509.15536v2', 'SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250915800v1-chronoforge-rl-chronological-forging-through-reinforcement-learning-.html">ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding</a></td>
  <td>ChronoForge-RLï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ—¶åºé”»é€ ï¼Œå¢å¼ºè§†é¢‘ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15800v1" data-paper-url="./papers/250915800v1-chronoforge-rl-chronological-forging-through-reinforcement-learning-.html" onclick="toggleFavorite(this, '2509.15800v1', 'ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250915608v1-enhancing-wsi-based-survival-analysis-with-report-auxiliary-self-dis.html">Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation</a></td>
  <td>æå‡ºRasaæ¡†æ¶ï¼Œåˆ©ç”¨æŠ¥å‘Šè¾…åŠ©è‡ªè’¸é¦å¢å¼ºWSIçš„ç”Ÿå­˜åˆ†æ</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15608v1" data-paper-url="./papers/250915608v1-enhancing-wsi-based-survival-analysis-with-report-auxiliary-self-dis.html" onclick="toggleFavorite(this, '2509.15608v1', 'Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250915566v4-btl-ui-blink-think-link-reasoning-model-for-gui-agent.html">BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent</a></td>
  <td>æå‡ºBTL-UIæ¨¡å‹ï¼Œæ¨¡æ‹Ÿäººè„‘è®¤çŸ¥è¿‡ç¨‹ï¼Œæå‡GUIæ™ºèƒ½ä½“çš„äº¤äº’èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15566v4" data-paper-url="./papers/250915566v4-btl-ui-blink-think-link-reasoning-model-for-gui-agent.html" onclick="toggleFavorite(this, '2509.15566v4', 'BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250915706v1-sgmagnet-a-baseline-model-for-3d-cloud-phase-structure-reconstructio.html">SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark</a></td>
  <td>SGMAGNetï¼šç”¨äºä¸‰ç»´äº‘ç›¸ç»“æ„é‡å»ºçš„è¢«åŠ¨ä¸»åŠ¨å«æ˜ŸåŸºå‡†æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15706v1" data-paper-url="./papers/250915706v1-sgmagnet-a-baseline-model-for-3d-cloud-phase-structure-reconstructio.html" onclick="toggleFavorite(this, '2509.15706v1', 'SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/250915751v1-simulated-cortical-magnification-supports-self-supervised-object-lea.html">Simulated Cortical Magnification Supports Self-Supervised Object Learning</a></td>
  <td>æ¨¡æ‹Ÿçš®å±‚æ”¾å¤§æå‡è‡ªç›‘ç£ç‰©ä½“å­¦ä¹ æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.15751v1" data-paper-url="./papers/250915751v1-simulated-cortical-magnification-supports-self-supervised-object-lea.html" onclick="toggleFavorite(this, '2509.15751v1', 'Simulated Cortical Magnification Supports Self-Supervised Object Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)