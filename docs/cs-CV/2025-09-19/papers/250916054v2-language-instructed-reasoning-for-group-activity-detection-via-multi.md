---
layout: default
title: Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model
---

# Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16054" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16054v2</a>
  <a href="https://arxiv.org/pdf/2509.16054.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16054v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16054v2', 'Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jihua Peng, Qianxiong Xu, Yichen Liu, Chenxi Liu, Cheng Long, Rui Zhao, Ziyue Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19 (æ›´æ–°: 2025-12-05)

**å¤‡æ³¨**: This work is being incorporated into a larger study

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLIR-GADï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­è¨€æŒ‡å¯¼çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¾¤ä½“æ´»åŠ¨æ£€æµ‹` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è¯­è¨€æŒ‡å¯¼æ¨ç†` `è§†é¢‘ç†è§£` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¾¤ä½“æ´»åŠ¨æ£€æµ‹æ–¹æ³•ä¾èµ–è§†è§‰ç‰¹å¾çš„éšå¼æ¨¡å¼è¯†åˆ«ï¼Œç¼ºä¹ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚
2. LIR-GADé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆè¯­è¨€æŒ‡ä»¤å’Œè§†è§‰ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç¾¤ä½“æ´»åŠ¨çš„è¯­ä¹‰ç†è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLIR-GADåœ¨ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¾¤ä½“æ´»åŠ¨æ£€æµ‹(GAD)æ—¨åœ¨è§†é¢‘åºåˆ—ä¸­åŒæ—¶è¯†åˆ«ç¾¤ä½“æˆå‘˜å¹¶åˆ†ç±»ä»–ä»¬çš„é›†ä½“æ´»åŠ¨ã€‚ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¼€å‘äº†ä¸“é—¨çš„æ¶æ„ï¼ˆä¾‹å¦‚ï¼ŒTransformerç½‘ç»œï¼‰æ¥å»ºæ¨¡ä¸ªä½“è§’è‰²çš„åŠ¨æ€ä»¥åŠä¸ªä½“å’Œç¾¤ä½“ä¹‹é—´çš„è¯­ä¹‰ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»…ä»…ä¾èµ–äºè§†è§‰ç‰¹å¾çš„éšå¼æ¨¡å¼è¯†åˆ«ï¼Œå¹¶ä¸”éš¾ä»¥è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†å’Œè§£é‡Šã€‚æœ¬æ–‡æå‡ºäº†LIR-GADï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯­è¨€æŒ‡å¯¼æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)è¿›è¡ŒGADã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥æ´»åŠ¨çº§åˆ«çš„<ACT> tokenå’Œå¤šä¸ªç‰¹å®šäºé›†ç¾¤çš„<GROUP> tokenæ¥æ‰©å±•MLLMçš„åŸå§‹è¯æ±‡è¡¨ã€‚æˆ‘ä»¬å¤„ç†è§†é¢‘å¸§ä»¥åŠä¸¤ä¸ªä¸“é—¨è®¾è®¡çš„tokenå’Œè¯­è¨€æŒ‡ä»¤ï¼Œç„¶åå°†å…¶é›†æˆåˆ°MLLMä¸­ã€‚MLLMä¸­åµŒå…¥çš„é¢„è®­ç»ƒå¸¸è¯†çŸ¥è¯†ä½¿<ACT> tokenå’Œ<GROUP> tokenèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·é›†ä½“æ´»åŠ¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åˆ†åˆ«å­¦ä¹ ä¸åŒç»„çš„ç‹¬ç‰¹è¡¨ç¤ºç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ ‡ç­¾åˆ†ç±»æŸå¤±ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼º<ACT> tokenå­¦ä¹ åŒºåˆ†æ€§è¯­ä¹‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€åŒé‡å¯¹é½èåˆ(MDAF)æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†MLLMçš„éšè—åµŒå…¥ï¼ˆå¯¹åº”äºè®¾è®¡çš„tokenï¼‰ä¸è§†è§‰ç‰¹å¾é›†æˆï¼Œä»è€Œæ˜¾ç€æé«˜äº†GADçš„æ€§èƒ½ã€‚å®šé‡å’Œå®šæ€§å®éªŒéƒ½è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨GADä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¾¤ä½“æ´»åŠ¨æ£€æµ‹(GAD)æ—¨åœ¨åŒæ—¶è¯†åˆ«è§†é¢‘ä¸­ç¾¤ä½“æˆå‘˜åŠå…¶é›†ä½“æ´»åŠ¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Transformerï¼Œæ¥å»ºæ¨¡ä¸ªä½“è§’è‰²åŠ¨æ€å’Œä¸ªä½“-ç¾¤ä½“é—´çš„è¯­ä¹‰ä¾èµ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…ä¾èµ–è§†è§‰ç‰¹å¾çš„éšå¼æ¨¡å¼è¯†åˆ«ï¼Œç¼ºä¹åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å¯è§£é‡Šæ€§è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLIR-GADçš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)çš„é¢„è®­ç»ƒçŸ¥è¯†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œç»“åˆè§†è§‰ä¿¡æ¯ï¼Œè¿›è¡Œè¯­è¨€æŒ‡å¯¼çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ã€‚é€šè¿‡å¼•å…¥ç‰¹å®štokenå’Œè¯­è¨€æŒ‡ä»¤ï¼Œå¼•å¯¼MLLMç†è§£å’Œæ¨ç†ç¾¤ä½“æ´»åŠ¨ï¼Œä»è€Œæå‡æ£€æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLIR-GADæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **è¯æ±‡æ‰©å±•**ï¼šåœ¨MLLMçš„è¯æ±‡è¡¨ä¸­å¼•å…¥æ´»åŠ¨çº§åˆ«çš„<ACT> tokenå’Œå¤šä¸ªé›†ç¾¤ç‰¹å®šçš„<GROUP> tokenã€‚2) **å¤šæ¨¡æ€è¾“å…¥**ï¼šå°†è§†é¢‘å¸§ã€è®¾è®¡çš„tokenå’Œè¯­è¨€æŒ‡ä»¤è¾“å…¥åˆ°MLLMä¸­ã€‚3) **ç‰¹å¾å­¦ä¹ **ï¼šåˆ©ç”¨MLLMçš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œä½¿<ACT>å’Œ<GROUP> tokenæ•è·é›†ä½“æ´»åŠ¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå­¦ä¹ ä¸åŒç»„çš„è¡¨ç¤ºç‰¹å¾ã€‚4) **å¤šæ ‡ç­¾åˆ†ç±»**ï¼šå¼•å…¥å¤šæ ‡ç­¾åˆ†ç±»æŸå¤±ï¼Œå¢å¼º<ACT> tokenå­¦ä¹ åŒºåˆ†æ€§è¯­ä¹‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚5) **å¤šæ¨¡æ€èåˆ**ï¼šè®¾è®¡å¤šæ¨¡æ€åŒé‡å¯¹é½èåˆ(MDAF)æ¨¡å—ï¼Œå°†MLLMçš„éšè—åµŒå…¥ä¸è§†è§‰ç‰¹å¾èåˆï¼Œæå‡GADæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šLIR-GADçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **è¯­è¨€æŒ‡å¯¼çš„æ¨ç†**ï¼šåˆ©ç”¨MLLMçš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡è¯­è¨€æŒ‡ä»¤å¼•å¯¼æ¨¡å‹è¿›è¡Œç¾¤ä½“æ´»åŠ¨æ£€æµ‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚2) **å¤šæ¨¡æ€åŒé‡å¯¹é½èåˆ(MDAF)**ï¼šè®¾è®¡MDAFæ¨¡å—ï¼Œæœ‰æ•ˆèåˆMLLMçš„è¯­ä¹‰ä¿¡æ¯å’Œè§†è§‰ç‰¹å¾ï¼Œæå‡äº†GADçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **<ACT>å’Œ<GROUP> tokenè®¾è®¡**ï¼šé€šè¿‡å¼•å…¥æ´»åŠ¨çº§åˆ«å’Œé›†ç¾¤ç‰¹å®šçš„tokenï¼Œä½¿MLLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè¡¨ç¤ºç¾¤ä½“æ´»åŠ¨ã€‚2) **å¤šæ ‡ç­¾åˆ†ç±»æŸå¤±**ï¼šä½¿ç”¨å¤šæ ‡ç­¾åˆ†ç±»æŸå¤±æ¥è®­ç»ƒ<ACT> tokenï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åŒºåˆ†ä¸åŒçš„æ´»åŠ¨ç±»åˆ«ã€‚3) **MDAFæ¨¡å—**ï¼šMDAFæ¨¡å—çš„å…·ä½“ç»“æ„å’Œèåˆç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›æœºåˆ¶ï¼‰æ˜¯å½±å“æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LIR-GADé€šè¿‡å¼•å…¥è¯­è¨€æŒ‡å¯¼å’Œå¤šæ¨¡æ€èåˆï¼Œåœ¨ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“å®éªŒæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„mAPæå‡ï¼‰éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚è¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œéœ€è¦ä¸Šä¸‹æ–‡æ¨ç†çš„ä»»åŠ¡æ—¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LIR-GADå¯åº”ç”¨äºæ™ºèƒ½è§†é¢‘ç›‘æ§ã€äººç¾¤è¡Œä¸ºåˆ†æã€ç¤¾äº¤æ´»åŠ¨ç†è§£ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç›‘æ§åœºæ™¯ä¸­ï¼Œå¯ä»¥è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸ç¾¤ä½“è¡Œä¸ºï¼Œå¦‚æ‰“æ¶æ–—æ®´ç­‰ã€‚åœ¨ç¤¾äº¤åª’ä½“åˆ†æä¸­ï¼Œå¯ä»¥ç†è§£ç”¨æˆ·å‘å¸ƒçš„è§†é¢‘å†…å®¹ï¼Œè¯†åˆ«å…¶ä¸­çš„ç¾¤ä½“æ´»åŠ¨ç±»å‹ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡è®¡ç®—æœºè§†è§‰ç³»ç»Ÿå¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level <ACT> token and multiple cluster-specific <GROUP> tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP> tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the <ACT> token's ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM's hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.

