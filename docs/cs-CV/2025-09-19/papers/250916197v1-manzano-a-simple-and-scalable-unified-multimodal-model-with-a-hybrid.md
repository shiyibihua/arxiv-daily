---
layout: default
title: MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer
---

# MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16197" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16197v1</a>
  <a href="https://arxiv.org/pdf/2509.16197.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16197v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16197v1', 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao, Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen, Yang Zhao, Ruoming Pang, Zhifeng Chen

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Manzanoï¼šä¸€ç§åŸºäºæ··åˆè§†è§‰Tokençš„ç®€å•å¯æ‰©å±•ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç»Ÿä¸€æ¨¡å‹` `è§†è§‰ç†è§£` `è§†è§‰ç”Ÿæˆ` `æ··åˆToken` `å¤§è¯­è¨€æ¨¡å‹` `æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´å­˜åœ¨æ€§èƒ½æƒè¡¡ã€‚
2. Manzanoé€šè¿‡æ··åˆå›¾åƒåˆ†è¯å™¨å’Œç»Ÿä¸€è®­ç»ƒæ–¹æ¡ˆï¼Œå®ç°äº†ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„å¯æ‰©å±•è”åˆå­¦ä¹ ã€‚
3. Manzanoåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å–å¾—äº†é¢†å…ˆæˆæœï¼Œå¹¶åœ¨æ–‡æœ¬ä¸°å¯Œçš„è¯„ä¼°ä¸­ä¸ä¸“ç”¨æ¨¡å‹ç«äº‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºManzanoï¼Œä¸€ä¸ªç®€å•ä¸”å¯æ‰©å±•çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å°†æ··åˆå›¾åƒåˆ†è¯å™¨ä¸ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒæ–¹æ¡ˆç›¸ç»“åˆï¼Œæ˜¾è‘—é™ä½äº†ç°æœ‰å¼€æ”¾æºä»£ç æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„æ€§èƒ½æƒè¡¡ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸€ä¸ªå…±äº«çš„è§†è§‰ç¼–ç å™¨ï¼Œé€šè¿‡ä¸¤ä¸ªè½»é‡çº§é€‚é…å™¨ï¼Œåœ¨å…¬å…±è¯­ä¹‰ç©ºé—´ä¸­ä¸ºå›¾åƒåˆ°æ–‡æœ¬çš„ç†è§£ç”Ÿæˆè¿ç»­åµŒå…¥ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆç”Ÿæˆç¦»æ•£tokenã€‚ç»Ÿä¸€çš„è‡ªå›å½’LLMä»¥æ–‡æœ¬å’Œå›¾åƒtokençš„å½¢å¼é¢„æµ‹é«˜å±‚è¯­ä¹‰ï¼Œè¾…åŠ©æ‰©æ•£è§£ç å™¨éšåå°†å›¾åƒtokenè½¬æ¢ä¸ºåƒç´ ã€‚è¿™ç§æ¶æ„ä»¥åŠç»Ÿä¸€çš„ç†è§£å’Œç”Ÿæˆæ•°æ®è®­ç»ƒæ–¹æ¡ˆï¼Œå®ç°äº†ä¸¤ç§èƒ½åŠ›çš„å¯æ‰©å±•è”åˆå­¦ä¹ ã€‚Manzanoåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”åœ¨å¯Œæ–‡æœ¬è¯„ä¼°ä¸­ä¸ä¸“ç”¨æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»»åŠ¡å†²çªæœ€å°ï¼Œå¹¶ä¸”æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§å¸¦æ¥äº†æŒç»­çš„æ”¶ç›Šï¼ŒéªŒè¯äº†æ··åˆåˆ†è¯å™¨çš„è®¾è®¡é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç†è§£ï¼ˆimage-to-textï¼‰å’Œè§†è§‰å†…å®¹ç”Ÿæˆï¼ˆtext-to-imageï¼‰ä¹‹é—´å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚å³ï¼Œä¸ºäº†æå‡ç†è§£èƒ½åŠ›ï¼Œå¾€å¾€ç‰ºç‰²ç”Ÿæˆèƒ½åŠ›ï¼Œåä¹‹äº¦ç„¶ã€‚å¼€æºæ¨¡å‹éš¾ä»¥åŒæ—¶å…¼é¡¾ä¸¤ç§èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šManzanoçš„æ ¸å¿ƒæ€è·¯æ˜¯ä½¿ç”¨ä¸€ä¸ªå…±äº«çš„è§†è§‰ç¼–ç å™¨ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºä¸¤ä¸ªè½»é‡çº§çš„é€‚é…å™¨ï¼Œåˆ†åˆ«ç”¨äºå›¾åƒåˆ°æ–‡æœ¬çš„ç†è§£ï¼ˆç”Ÿæˆè¿ç»­åµŒå…¥ï¼‰å’Œæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼ˆç”Ÿæˆç¦»æ•£tokenï¼‰ã€‚é€šè¿‡è¿™ç§æ··åˆtokenizationæ–¹æ³•ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸€ä¸ªå…±åŒçš„è¯­ä¹‰ç©ºé—´ä¸­å¤„ç†ä¸¤ç§æ¨¡æ€ï¼Œä»è€Œå‡å°‘ä»»åŠ¡å†²çªï¼Œå¹¶å®ç°ä¸¤ç§èƒ½åŠ›çš„å¯æ‰©å±•è”åˆå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šManzanoçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å…±äº«è§†è§‰ç¼–ç å™¨ï¼šè´Ÿè´£æå–å›¾åƒçš„è§†è§‰ç‰¹å¾ã€‚2) å›¾åƒåˆ°æ–‡æœ¬é€‚é…å™¨ï¼šå°†è§†è§‰ç‰¹å¾è½¬æ¢ä¸ºè¿ç»­åµŒå…¥ï¼Œç”¨äºå›¾åƒç†è§£ä»»åŠ¡ã€‚3) æ–‡æœ¬åˆ°å›¾åƒé€‚é…å™¨ï¼šå°†è§†è§‰ç‰¹å¾è½¬æ¢ä¸ºç¦»æ•£tokenï¼Œç”¨äºå›¾åƒç”Ÿæˆä»»åŠ¡ã€‚4) ç»Ÿä¸€è‡ªå›å½’LLMï¼šé¢„æµ‹æ–‡æœ¬å’Œå›¾åƒtokenå½¢å¼çš„é«˜å±‚è¯­ä¹‰ã€‚5) æ‰©æ•£è§£ç å™¨ï¼šå°†å›¾åƒtokenè½¬æ¢ä¸ºåƒç´ ï¼Œç”Ÿæˆæœ€ç»ˆå›¾åƒã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒæ—¶åˆ©ç”¨ç†è§£å’Œç”Ÿæˆæ•°æ®è¿›è¡Œè”åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šManzanoçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ··åˆè§†è§‰tokenizationæ–¹æ³•ã€‚ä¼ ç»Ÿçš„è§†è§‰æ¨¡å‹é€šå¸¸åªä½¿ç”¨ä¸€ç§tokenizationæ–¹å¼ï¼Œè¦ä¹ˆæ˜¯è¿ç»­åµŒå…¥ï¼Œè¦ä¹ˆæ˜¯ç¦»æ•£tokenã€‚ManzanoåŒæ—¶ä½¿ç”¨ä¸¤ç§tokenizationæ–¹å¼ï¼Œå¹¶é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡é€‰æ‹©åˆé€‚çš„tokenizationæ–¹å¼ã€‚è¿™ç§æ··åˆæ–¹æ³•å¯ä»¥æ›´å¥½åœ°å¹³è¡¡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶å‡å°‘ä»»åŠ¡å†²çªã€‚

**å…³é”®è®¾è®¡**ï¼šManzanoçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è½»é‡çº§é€‚é…å™¨ï¼šä½¿ç”¨è½»é‡çº§é€‚é…å™¨å¯ä»¥å‡å°‘æ¨¡å‹å‚æ•°é‡ï¼Œå¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚2) ç»Ÿä¸€è®­ç»ƒæ–¹æ¡ˆï¼šä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ–¹æ¡ˆå¯ä»¥åŒæ—¶ä¼˜åŒ–ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚3) è¾…åŠ©æ‰©æ•£è§£ç å™¨ï¼šä½¿ç”¨æ‰©æ•£è§£ç å™¨å¯ä»¥å°†å›¾åƒtokenè½¬æ¢ä¸ºé«˜è´¨é‡çš„å›¾åƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Manzanoåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨æ–‡æœ¬ä¸°å¯Œçš„è¯„ä¼°ä¸­ä¸ä¸“ç”¨æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManzanoåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§å¸¦æ¥äº†æŒç»­çš„æ”¶ç›Šã€‚è¿™äº›ç»“æœéªŒè¯äº†Manzanoçš„æ··åˆtokenizationæ–¹æ³•å’Œç»Ÿä¸€è®­ç»ƒæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Manzanoå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿã€å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰å†…å®¹åˆ›ä½œã€æ•™è‚²å¨±ä¹ç­‰ã€‚è¯¥æ¨¡å‹å¯ä»¥ç”¨äºæ„å»ºæ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äº¤äº’å¼åº”ç”¨ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´ä¸°å¯Œçš„è§†è§‰ä½“éªŒã€‚æœªæ¥ï¼ŒManzanoæœ‰æœ›æˆä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦åŸºçŸ³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.

