---
layout: default
title: See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model
---

# See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16087" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16087v1</a>
  <a href="https://arxiv.org/pdf/2509.16087.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16087v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16087v1', 'See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pengteng Li, Pinhao Song, Wuyang Li, Weiyu Guo, Huizai Yao, Yijie Xu, Dugang Liu, Hui Xiong

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: Accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEE&TREKï¼Œä¸€ç§å…è®­ç»ƒçš„ç©ºé—´æç¤ºæ¡†æ¶ï¼Œæå‡MLLMçš„è§†è§‰ç©ºé—´ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç©ºé—´ç†è§£` `è§†è§‰æç¤º` `å…è®­ç»ƒå­¦ä¹ ` `è¿åŠ¨é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨çº¯è§†è§‰ä¸‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œä¾èµ–æ·±åº¦ç­‰æ¨¡æ€ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚
2. SEE&TREKé€šè¿‡æœ€å¤§è¯­ä¹‰ä¸°å¯Œåº¦é‡‡æ ·å’Œè¿åŠ¨é‡å»ºï¼Œå¢åŠ è§†è§‰å¤šæ ·æ€§ï¼Œä¿ç•™ç©ºé—´å…³ç³»å’Œæ—¶é—´è¿è´¯æ€§ã€‚
3. SEE&TREKæ— éœ€è®­ç»ƒï¼Œæ˜“äºé›†æˆï¼Œåœ¨VSI-BENCHå’ŒSTI-BENCHä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSEE&TREKï¼Œé¦–ä¸ªå…è®­ç»ƒçš„æç¤ºæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)åœ¨çº¯è§†è§‰çº¦æŸä¸‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ·±åº¦æˆ–ç‚¹äº‘ç­‰æ¨¡æ€æ¥æå‡ç©ºé—´æ¨ç†ï¼Œè€Œçº¯è§†è§‰ç©ºé—´ç†è§£ä»æœ‰å¾…æ¢ç´¢ã€‚SEE&TREKé€šè¿‡å¢åŠ è§†è§‰å¤šæ ·æ€§å’Œè¿åŠ¨é‡å»ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚åœ¨è§†è§‰å¤šæ ·æ€§æ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨æœ€å¤§è¯­ä¹‰ä¸°å¯Œåº¦é‡‡æ ·ï¼Œåˆ©ç”¨ç°æˆçš„æ„ŸçŸ¥æ¨¡å‹æå–è¯­ä¹‰ä¸°å¯Œçš„å…³é”®å¸§ï¼Œæ•æ‰åœºæ™¯ç»“æ„ã€‚åœ¨è¿åŠ¨é‡å»ºæ–¹é¢ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿè§†è§‰è½¨è¿¹ï¼Œå¹¶å°†ç›¸å¯¹ç©ºé—´ä½ç½®ç¼–ç åˆ°å…³é”®å¸§ä¸­ï¼Œä»¥ä¿ç•™ç©ºé—´å…³ç³»å’Œæ—¶é—´è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è®­ç»ƒå’ŒGPUï¼Œåªéœ€ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œå³å¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„MLLMä¸­ã€‚åœ¨VSI-BENCHå’ŒSTI-BENCHä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSEE&TREKåœ¨å„ç§ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­æŒç»­æå‡äº†å„ç§MLLMçš„æ€§èƒ½ï¼Œæœ€é«˜æå‡è¾¾3.5%ï¼Œä¸ºæ›´å¼ºçš„ç©ºé—´æ™ºèƒ½æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ä»…æœ‰è§†è§‰ä¿¡æ¯è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œç©ºé—´ç†è§£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ·±åº¦ä¿¡æ¯ã€ç‚¹äº‘æ•°æ®ç­‰è¾…åŠ©æ¨¡æ€æ¥å¢å¼ºç©ºé—´æ¨ç†ï¼Œä½†åœ¨ç¼ºä¹è¿™äº›ä¿¡æ¯æ—¶ï¼Œçº¯è§†è§‰çš„ç©ºé—´ç†è§£èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå¯¹è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨ä¸å¤Ÿå……åˆ†ï¼Œæ— æ³•æœ‰æ•ˆåœ°æå–å’Œåˆ©ç”¨åœºæ™¯ä¸­çš„ç©ºé—´å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSEE&TREKçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢åŠ è§†è§‰è¾“å…¥çš„å¤šæ ·æ€§å’Œé‡å»ºè¿åŠ¨è½¨è¿¹æ¥æå‡MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æœ€å¤§è¯­ä¹‰ä¸°å¯Œåº¦é‡‡æ ·ï¼ˆMaximum Semantic Richness Samplingï¼‰æ¥é€‰æ‹©ä¿¡æ¯é‡å¤§çš„å…³é”®å¸§ï¼Œä»è€Œæ•æ‰åœºæ™¯ç»“æ„ï¼›é€šè¿‡æ¨¡æ‹Ÿè§†è§‰è½¨è¿¹å¹¶å°†ç›¸å¯¹ç©ºé—´ä½ç½®ç¼–ç åˆ°å…³é”®å¸§ä¸­ï¼Œæ¥ä¿ç•™ç©ºé—´å…³ç³»å’Œæ—¶é—´è¿è´¯æ€§ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†è®©MLLMèƒ½å¤Ÿä»æ›´ä¸°å¯Œçš„è§†è§‰ä¿¡æ¯ä¸­å­¦ä¹ åˆ°æ›´å‡†ç¡®çš„ç©ºé—´è¡¨å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSEE&TREKçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæœ€å¤§è¯­ä¹‰ä¸°å¯Œåº¦é‡‡æ ·ï¼ˆMSRSï¼‰å’Œè¿åŠ¨é‡å»ºã€‚é¦–å…ˆï¼ŒMSRSæ¨¡å—åˆ©ç”¨ä¸€ä¸ªç°æˆçš„æ„ŸçŸ¥æ¨¡å‹ï¼ˆoff-the-shell perception modelï¼‰æå–å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€‰æ‹©è¯­ä¹‰æœ€ä¸°å¯Œçš„å…³é”®å¸§ã€‚ç„¶åï¼Œè¿åŠ¨é‡å»ºæ¨¡å—æ¨¡æ‹Ÿè§†è§‰è½¨è¿¹ï¼Œå¹¶å°†ç›¸å¯¹ç©ºé—´ä½ç½®ä¿¡æ¯ç¼–ç åˆ°å…³é”®å¸§ä¸­ã€‚æœ€åï¼Œå°†å¤„ç†åçš„å…³é”®å¸§è¾“å…¥åˆ°MLLMä¸­è¿›è¡Œç©ºé—´æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEE&TREKæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶å…è®­ç»ƒçš„æç¤ºæ¡†æ¶ã€‚ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSEE&TREKåªéœ€è¦ä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯å®Œæˆç©ºé—´ä¿¡æ¯çš„å¢å¼ºï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„MLLMä¸­ã€‚æ­¤å¤–ï¼Œé€šè¿‡æœ€å¤§è¯­ä¹‰ä¸°å¯Œåº¦é‡‡æ ·å’Œè¿åŠ¨é‡å»ºï¼Œæœ‰æ•ˆåœ°æå‡äº†è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨ç‡ï¼Œä»è€Œå¢å¼ºäº†MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æœ€å¤§è¯­ä¹‰ä¸°å¯Œåº¦é‡‡æ ·ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨é¢„è®­ç»ƒçš„æ„ŸçŸ¥æ¨¡å‹ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰æå–å›¾åƒçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶æ ¹æ®è¯­ä¹‰ç‰¹å¾çš„ä¸°å¯Œç¨‹åº¦é€‰æ‹©å…³é”®å¸§ã€‚åœ¨è¿åŠ¨é‡å»ºä¸­ï¼Œè®ºæ–‡æ¨¡æ‹Ÿè§†è§‰è½¨è¿¹çš„å…·ä½“æ–¹æ³•æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç›¸å¯¹ç©ºé—´ä½ç½®ä¿¡æ¯ç¼–ç åˆ°å…³é”®å¸§ä¸­ï¼Œä»¥ä¾¿MLLMèƒ½å¤Ÿå­¦ä¹ åˆ°ç©ºé—´å…³ç³»ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„æ–¹é¢ï¼Œç”±äºæ˜¯å…è®­ç»ƒæ–¹æ³•ï¼Œå› æ­¤æ²¡æœ‰æ¶‰åŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SEE&TREKåœ¨VSI-BENCHå’ŒSTI-BENCHæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å„ç§MLLMçš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒSEE&TREKåœ¨å¤šä¸ªç©ºé—´æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†é«˜è¾¾3.5%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSEE&TREKæ˜¯ä¸€ç§å…è®­ç»ƒçš„æ–¹æ³•ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæå‡ç°æœ‰MLLMçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SEE&TREKå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡MLLMåœ¨çº¯è§†è§‰ä¸‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°æ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„å†³ç­–å’Œæ§åˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå›¾åƒæ£€ç´¢ã€è§†é¢‘åˆ†æç­‰ä»»åŠ¡ï¼Œæå‡ç›¸å…³åº”ç”¨çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce SEE&TREK, the first training-free prompting framework tailored to enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints. While prior efforts have incorporated modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains underexplored. SEE&TREK addresses this gap by focusing on two core principles: increasing visual diversity and motion reconstruction. For visual diversity, we conduct Maximum Semantic Richness Sampling, which employs an off-the-shell perception model to extract semantically rich keyframes that capture scene structure. For motion reconstruction, we simulate visual trajectories and encode relative spatial positions into keyframes to preserve both spatial relations and temporal coherence. Our method is training&GPU-free, requiring only a single forward pass, and can be seamlessly integrated into existing MLLM'S. Extensive experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently boosts various MLLM S performance across diverse spatial reasoning tasks with the most +3.5% improvement, offering a promising path toward stronger spatial intelligence.

