---
layout: default
title: Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks
---

# Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16163" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16163v1</a>
  <a href="https://arxiv.org/pdf/2509.16163.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16163v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16163v1', 'Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Het Patel, Muzammil Allie, Qian Zhang, Jia Chen, Evangelos E. Papalexakis

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: To be presented as a poster at the Workshop on Safe and Trustworthy Multimodal AI Systems (SafeMM-AI), 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§å¼ é‡åˆ†è§£æ–¹æ³•ä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¯¹æŠ—æ€§æ”»å‡»` `å¼ é‡åˆ†è§£` `é²æ£’æ€§å¢å¼º` `å¤šæ¨¡æ€ç†è§£` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹é˜²å¾¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶è¡¨ç°è„†å¼±ï¼Œé˜²å¾¡æªæ–½å¾€å¾€éœ€è¦å¤æ‚çš„é‡æ–°è®­ç»ƒæˆ–æ¶æ„è°ƒæ•´ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ é‡åˆ†è§£çš„è½»é‡çº§é˜²å¾¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Flickr30Kå’ŒCOCOæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„æ¢å¤èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚ç°æœ‰é˜²å¾¡æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒæˆ–æ˜¾è‘—çš„æ¶æ„æ›´æ”¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„é˜²å¾¡æœºåˆ¶ï¼Œåˆ©ç”¨å¼ é‡åˆ†è§£æŠ€æœ¯ï¼Œé€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„VLMï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚é€šè¿‡å¯¹è§†è§‰ç¼–ç å™¨è¡¨ç¤ºè¿›è¡Œåˆ†è§£å’Œé‡æ„ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¿‡æ»¤å¯¹æŠ—å™ªå£°ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨COCOå’ŒFlickr30Kæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨CLIPæ¨¡å‹çš„é²æ£’æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚Flickr30Kä¸Šæ¢å¤äº†12.3%çš„æ€§èƒ½ï¼ŒRecall@1å‡†ç¡®ç‡ä»7.5%æå‡è‡³19.8%ï¼›è€Œåœ¨COCOä¸Šæ¢å¤äº†8.1%çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä»3.8%æå‡è‡³11.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„è„†å¼±æ€§ã€‚ç°æœ‰çš„é˜²å¾¡æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒæˆ–å¤æ‚çš„æ¶æ„è°ƒæ•´ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¼ é‡åˆ†è§£çš„é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡å¯¹è§†è§‰ç¼–ç å™¨çš„è¡¨ç¤ºè¿›è¡Œåˆ†è§£å’Œé‡æ„ï¼Œè¿‡æ»¤å¯¹æŠ—å™ªå£°ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¼ é‡åˆ†è§£æ¨¡å—å’Œé‡æ„æ¨¡å—ã€‚é¦–å…ˆå¯¹è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºè¿›è¡Œå¼ é‡åˆ†è§£ï¼Œç„¶åé€šè¿‡é‡æ„è¿‡ç¨‹æ¢å¤å¹²å‡€çš„è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¼ é‡åˆ†è§£æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å¼ é‡åˆ—è½¦åˆ†è§£ï¼Œèƒ½å¤Ÿåœ¨ä½ç§©å’Œä½æ®‹å·®å¼ºåº¦ä¸‹æœ‰æ•ˆè¿‡æ»¤å¯¹æŠ—å™ªå£°ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½ç§©èŒƒå›´ä¸º8-32ï¼Œä½æ®‹å·®å¼ºåº¦è®¾ç½®ä¸ºÎ±=0.1-0.2ï¼Œç»è¿‡å®éªŒéªŒè¯ï¼Œè¿™äº›è®¾ç½®èƒ½å¤Ÿè¾¾åˆ°æœ€ä½³çš„é˜²å¾¡æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Flickr30Kæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•æ¢å¤äº†12.3%çš„æ€§èƒ½ï¼ŒRecall@1å‡†ç¡®ç‡ä»7.5%æå‡è‡³19.8%ï¼›åœ¨COCOæ•°æ®é›†ä¸Šï¼Œæ¢å¤äº†8.1%çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä»3.8%æå‡è‡³11.9%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¼ é‡åˆ†è§£æŠ€æœ¯åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„é˜²å¾¡æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜é²æ£’æ€§çš„å¤šæ¨¡æ€ç³»ç»Ÿä¸­ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¯ä»¥æé«˜è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision language models (VLMs) excel in multimodal understanding but are prone to adversarial attacks. Existing defenses often demand costly retraining or significant architecture changes. We introduce a lightweight defense using tensor decomposition suitable for any pre-trained VLM, requiring no retraining. By decomposing and reconstructing vision encoder representations, it filters adversarial noise while preserving meaning. Experiments with CLIP on COCO and Flickr30K show improved robustness. On Flickr30K, it restores 12.3\% performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%. Analysis shows Tensor Train decomposition with low rank (8-32) and low residual strength ($Î±=0.1-0.2$) is optimal. This method is a practical, plug-and-play solution with minimal overhead for existing VLMs.

