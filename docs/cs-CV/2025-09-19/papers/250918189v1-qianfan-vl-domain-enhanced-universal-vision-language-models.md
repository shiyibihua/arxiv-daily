---
layout: default
title: Qianfan-VL: Domain-Enhanced Universal Vision-Language Models
---

# Qianfan-VL: Domain-Enhanced Universal Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18189" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18189v1</a>
  <a href="https://arxiv.org/pdf/2509.18189.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18189v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18189v1', 'Qianfan-VL: Domain-Enhanced Universal Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daxiang Dong, Mingming Zheng, Dong Xu, Bairong Zhuang, Wenyu Zhang, Chunhua Luo, Haoran Wang, Zijian Zhao, Jie Li, Yuxuan Li, Hanjun Zhong, Mengyue Liu, Jieting Chen, Shupeng Li, Lun Tian, Yaping Feng, Xin Li, Donggang Jiang, Yong Chen, Yehua Xu, Duohao Qin, Chen Feng, Dan Wang, Henghua Zhang, Jingjing Ha, Jinhui He, Yanfeng Zhai, Chengxin Zheng, Jiayi Mao, Jiacheng Chen, Ruchang Yao, Ziye Yuan, Jianmin Wu, Guangjun Xie, Dou Shen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 12 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQianfan-VLï¼Œé€šè¿‡é¢†åŸŸå¢å¼ºæŠ€æœ¯å®ç°é¢†å…ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `é¢†åŸŸå¢å¼º` `è§†è§‰è¯­è¨€æ¨¡å‹` `OCR` `æ–‡æ¡£ç†è§£` `é•¿é“¾å¼æ€è€ƒ` `æ•°æ®åˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ä¼ä¸šçº§åº”ç”¨éœ€æ±‚ã€‚
2. Qianfan-VLé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒå’Œæ•°æ®åˆæˆï¼Œå¢å¼ºæ¨¡å‹åœ¨OCRã€æ–‡æ¡£ç†è§£ç­‰é¢†åŸŸçš„æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒQianfan-VLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå°¤å…¶åœ¨é¢†åŸŸç›¸å…³ä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†Qianfan-VLï¼Œä¸€ç³»åˆ—å‚æ•°è§„æ¨¡ä»30äº¿åˆ°700äº¿çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„é¢†åŸŸå¢å¼ºæŠ€æœ¯å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒå’Œé«˜ç²¾åº¦æ•°æ®åˆæˆæµç¨‹ï¼Œè¿™äº›æŠ€æœ¯å¯¹äºå¢å¼ºé¢†åŸŸç‰¹å®šèƒ½åŠ›åŒæ—¶ä¿æŒå¼ºå¤§çš„é€šç”¨æ€§èƒ½è‡³å…³é‡è¦ã€‚Qianfan-VLåœ¨é€šç”¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¸é¢†å…ˆå¼€æºæ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œå¹¶åœ¨CCBenchã€SEEDBench IMGã€ScienceQAå’ŒMMStarç­‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é¢†åŸŸå¢å¼ºç­–ç•¥åœ¨OCRå’Œæ–‡æ¡£ç†è§£æ–¹é¢æä¾›äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¿™åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ï¼ˆOCRBench 873ï¼ŒDocVQA 94.75%ï¼‰å’Œå†…éƒ¨è¯„ä¼°ä¸­å¾—åˆ°äº†éªŒè¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒQianfan-VL-8Bå’Œ70Bå˜ä½“ç»“åˆäº†é•¿é“¾å¼æ€è€ƒèƒ½åŠ›ï¼Œåœ¨æ•°å­¦æ¨ç†ï¼ˆMathVista 78.6%ï¼‰å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ‰€æœ‰æ¨¡å‹å®Œå…¨åœ¨ç™¾åº¦çš„æ˜†ä»‘P800èŠ¯ç‰‡ä¸Šè®­ç»ƒï¼ŒéªŒè¯äº†å¤§è§„æ¨¡AIåŸºç¡€è®¾æ–½åœ¨å•ä¸ªä»»åŠ¡ä¸Šä»¥è¶…è¿‡90%çš„æ‰©å±•æ•ˆç‡åœ¨5000ä¸ªèŠ¯ç‰‡ä¸Šè®­ç»ƒSOTAçº§å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘é€‚ç”¨äºå„ç§ä¼ä¸šéƒ¨ç½²åœºæ™¯çš„é¢†åŸŸå¢å¼ºå‹å¤šæ¨¡æ€æ¨¡å‹å»ºç«‹äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€šç”¨ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨OCRã€æ–‡æ¡£ç†è§£ç­‰ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ä»æœ‰æå‡ç©ºé—´ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äºä¼ä¸šçº§åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥å…¼é¡¾é€šç”¨æ€§å’Œé¢†åŸŸç‰¹å®šæ€§ï¼Œæˆ–è€…éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQianfan-VLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é¢†åŸŸå¢å¼ºæŠ€æœ¯ï¼Œåœ¨ä¿æŒé€šç”¨æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚é€šè¿‡å¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒå’Œé«˜ç²¾åº¦æ•°æ®åˆæˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†é¢†åŸŸç›¸å…³çš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQianfan-VLçš„è®­ç»ƒæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é¢„è®­ç»ƒé˜¶æ®µï¼šä½¿ç”¨å¤§è§„æ¨¡é€šç”¨è§†è§‰è¯­è¨€æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œæå‡æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚2) é¢†åŸŸå¢å¼ºé˜¶æ®µï¼šåˆ©ç”¨é«˜ç²¾åº¦æ•°æ®åˆæˆæŠ€æœ¯ç”Ÿæˆé¢†åŸŸç›¸å…³çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚3) é•¿é“¾å¼æ€è€ƒèƒ½åŠ›å¢å¼ºé˜¶æ®µï¼šé’ˆå¯¹æ•°å­¦æ¨ç†å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ï¼Œå¼•å…¥é•¿é“¾å¼æ€è€ƒæœºåˆ¶ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šQianfan-VLçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é¢†åŸŸå¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡å¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒå’Œé«˜ç²¾åº¦æ•°æ®åˆæˆï¼Œæœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å¼•å…¥äº†é•¿é“¾å¼æ€è€ƒèƒ½åŠ›ï¼Œæå‡äº†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šQianfan-VLé‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€é¢†åŸŸå¢å¼ºå’Œé•¿é“¾å¼æ€è€ƒèƒ½åŠ›å¢å¼ºã€‚åœ¨é¢†åŸŸå¢å¼ºé˜¶æ®µï¼Œä½¿ç”¨äº†é«˜ç²¾åº¦æ•°æ®åˆæˆæŠ€æœ¯ï¼Œä¾‹å¦‚ï¼Œé’ˆå¯¹OCRä»»åŠ¡ï¼Œåˆæˆäº†åŒ…å«å„ç§å­—ä½“ã€å¸ƒå±€å’Œå™ªå£°çš„æ–‡æ¡£å›¾åƒã€‚åœ¨é•¿é“¾å¼æ€è€ƒèƒ½åŠ›å¢å¼ºé˜¶æ®µï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒæŠ€å·§ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´é•¿çš„æ¨ç†é“¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Qianfan-VLåœ¨OCRBenchä¸Šè¾¾åˆ°873åˆ†ï¼ŒDocVQAä¸Šè¾¾åˆ°94.75%ï¼ŒMathVistaä¸Šè¾¾åˆ°78.6%ï¼Œå‡å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ã€‚ä¸ç°æœ‰å¼€æºæ¨¡å‹ç›¸æ¯”ï¼ŒQianfan-VLåœ¨é¢†åŸŸç›¸å…³ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒéªŒè¯äº†é¢†åŸŸå¢å¼ºç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨5000ä¸ªæ˜†ä»‘P800èŠ¯ç‰‡ä¸Šå®ç°äº†è¶…è¿‡90%çš„æ‰©å±•æ•ˆç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡AIåŸºç¡€è®¾æ–½ä¸Šçš„å¯è®­ç»ƒæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Qianfan-VLå¯å¹¿æ³›åº”ç”¨äºä¼ä¸šçº§åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½æ–‡æ¡£å¤„ç†ã€è´¢åŠ¡æŠ¥è¡¨åˆ†æã€æ³•å¾‹æ–‡ä¹¦ç†è§£ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼Œå¯ä»¥æœ‰æ•ˆé™ä½äººå·¥æˆæœ¬ï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä¾‹å¦‚åŒ»ç–—å½±åƒåˆ†æã€å·¥ä¸šè´¨æ£€ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Qianfan-VL, a series of multimodal large language models ranging from 3B to 70B parameters, achieving state-of-the-art performance through innovative domain enhancement techniques. Our approach employs multi-stage progressive training and high-precision data synthesis pipelines, which prove to be critical technologies for enhancing domain-specific capabilities while maintaining strong general performance. Qianfan-VL achieves comparable results to leading open-source models on general benchmarks, with state-of-the-art performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and MMStar. The domain enhancement strategy delivers significant advantages in OCR and document understanding, validated on both public benchmarks (OCRBench 873, DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B variants incorporate long chain-of-thought capabilities, demonstrating superior performance on mathematical reasoning (MathVista 78.6%) and logical inference tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating the capability of large-scale AI infrastructure to train SOTA-level multimodal models with over 90% scaling efficiency on 5000 chips for a single task. This work establishes an effective methodology for developing domain-enhanced multimodal models suitable for diverse enterprise deployment scenarios.

