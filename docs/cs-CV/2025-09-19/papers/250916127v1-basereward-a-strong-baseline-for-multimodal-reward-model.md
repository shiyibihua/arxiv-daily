---
layout: default
title: BaseReward: A Strong Baseline for Multimodal Reward Model
---

# BaseReward: A Strong Baseline for Multimodal Reward Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16127" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16127v1</a>
  <a href="https://arxiv.org/pdf/2509.16127.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16127v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16127v1', 'BaseReward: A Strong Baseline for Multimodal Reward Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi-Fan Zhang, Haihua Yang, Huanyu Zhang, Yang Shi, Zezhou Chen, Haochen Tian, Chaoyou Fu, Haotian Wang, Kai Wu, Bo Cui, Xu Wang, Jianfei Pan, Haotian Wang, Zhang Zhang, Liang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BaseRewardï¼šå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†ï¼Œä¸ºMLLMå¯¹é½æä¾›å®ç”¨æŒ‡å—**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹` `äººç±»åå¥½å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMå¥–åŠ±æ¨¡å‹æ„å»ºç¼ºä¹ç³»ç»Ÿæ€§æŒ‡å¯¼ï¼Œéš¾ä»¥æœ‰æ•ˆå¯¹é½äººç±»åå¥½ã€‚
2. æå‡ºBaseRewardï¼Œé€šè¿‡ç³»ç»Ÿå®éªŒåˆ†æï¼Œä¼˜åŒ–å¥–åŠ±å»ºæ¨¡èŒƒå¼ã€æ¶æ„ã€è®­ç»ƒå’Œæ•°æ®ã€‚
3. BaseRewardåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAï¼Œå¹¶æˆåŠŸåº”ç”¨äºå®é™…å¼ºåŒ–å­¦ä¹ æµç¨‹ï¼Œæå‡MLLMæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å¾—å°†å…¶ä¸äººç±»åå¥½å¯¹é½æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯å®ç°æ­¤ç›®æ ‡çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½†ç›®å‰å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½ç¼ºä¹æ„å»ºæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMï¼‰çš„ç³»ç»ŸæŒ‡å—ã€‚æœ¬æ–‡é€šè¿‡è¯¦å°½çš„å®éªŒåˆ†æï¼Œæ—¨åœ¨ä¸ºæ„å»ºé«˜æ€§èƒ½MRMæä¾›æ¸…æ™°çš„â€œé…æ–¹â€ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†MRMå¼€å‘æµç¨‹ä¸­çš„æ¯ä¸ªå…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼ˆä¾‹å¦‚ï¼ŒNaive-RMã€åŸºäºCriticçš„RMå’Œç”Ÿæˆå¼RMï¼‰ã€å¥–åŠ±å¤´æ¶æ„ã€è®­ç»ƒç­–ç•¥ã€æ•°æ®æ•´ç†ï¼ˆæ¶µç›–åå¤šä¸ªå¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬åå¥½æ•°æ®é›†ï¼‰ã€éª¨å¹²æ¨¡å‹å’Œæ¨¡å‹è§„æ¨¡ä»¥åŠé›†æˆæ–¹æ³•ã€‚åŸºäºè¿™äº›å®éªŒè§è§£ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BaseRewardï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œé«˜æ•ˆçš„å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºçº¿ã€‚BaseRewardé‡‡ç”¨ç®€å•è€Œæœ‰æ•ˆçš„æ¶æ„ï¼Œå»ºç«‹åœ¨Qwen2.5-VLéª¨å¹²ä¹‹ä¸Šï¼Œå…·æœ‰ä¼˜åŒ–çš„ä¸¤å±‚å¥–åŠ±å¤´ï¼Œå¹¶åœ¨ç²¾å¿ƒç­–åˆ’çš„é«˜è´¨é‡å¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬åå¥½æ•°æ®æ··åˆä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBaseRewardåœ¨MM-RLHF-Reward Benchã€VL-Reward Benchå’ŒMultimodal Reward Benchç­‰ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„SOTAï¼Œä¼˜äºä¹‹å‰çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†éªŒè¯å…¶åœ¨é™æ€åŸºå‡†ä¹‹å¤–çš„å®é™…æ•ˆç”¨ï¼Œæˆ‘ä»¬å°†BaseRewardé›†æˆåˆ°çœŸå®çš„å¼ºåŒ–å­¦ä¹ æµç¨‹ä¸­ï¼ŒæˆåŠŸåœ°æé«˜äº†MLLMåœ¨å„ç§æ„ŸçŸ¥ã€æ¨ç†å’Œå¯¹è¯ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æä¾›äº†ä¸€ä¸ªé¡¶çº§çš„MRMï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„ã€ç»éªŒæ”¯æŒçš„æŒ‡å—ï¼Œç”¨äºä¸ºä¸‹ä¸€ä»£MLLMå¼€å‘å¼ºå¤§çš„å¥–åŠ±æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„é—®é¢˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMï¼‰æ„å»ºç¼ºä¹ç³»ç»Ÿæ€§çš„æŒ‡å¯¼ï¼Œå¯¼è‡´éš¾ä»¥æœ‰æ•ˆåœ°è®­ç»ƒå‡ºèƒ½å¤Ÿå‡†ç¡®åæ˜ äººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹ã€‚è¿™é˜»ç¢äº†MLLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç†è§£å’Œç”Ÿæˆå¤šæ¨¡æ€å†…å®¹çš„ä»»åŠ¡ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯¦å°½çš„å®éªŒåˆ†æï¼Œç³»ç»Ÿåœ°ç ”ç©¶MRMå¼€å‘æµç¨‹ä¸­çš„å„ä¸ªå…³é”®ç»„ä»¶ï¼Œä»è€Œä¸ºæ„å»ºé«˜æ€§èƒ½MRMæä¾›æ¸…æ™°çš„æŒ‡å¯¼ã€‚é€šè¿‡å¯¹ä¸åŒå¥–åŠ±å»ºæ¨¡èŒƒå¼ã€å¥–åŠ±å¤´æ¶æ„ã€è®­ç»ƒç­–ç•¥ã€æ•°æ®æ•´ç†ã€éª¨å¹²æ¨¡å‹å’Œæ¨¡å‹è§„æ¨¡ä»¥åŠé›†æˆæ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œæ‰¾åˆ°æœ€ä¼˜çš„ç»„åˆæ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBaseRewardçš„æ•´ä½“æ¶æ„åŸºäºQwen2.5-VLéª¨å¹²æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªä¼˜åŒ–çš„ä¸¤å±‚å¥–åŠ±å¤´ã€‚è®­ç»ƒæµç¨‹åŒ…æ‹¬ï¼š1) æ•°æ®æ”¶é›†å’Œæ•´ç†ï¼Œæ„å»ºé«˜è´¨é‡çš„å¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬åå¥½æ•°æ®é›†ï¼›2) æ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨åˆé€‚çš„è®­ç»ƒç­–ç•¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼›3) æ¨¡å‹è¯„ä¼°ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼›4) å®é™…åº”ç”¨ï¼Œå°†æ¨¡å‹é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ æµç¨‹ä¸­ï¼ŒéªŒè¯å…¶åœ¨å®é™…ä»»åŠ¡ä¸­çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„MRMæ„å»ºæŒ‡å—ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å„ä¸ªç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼ŒBaseRewardæœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºä¹‹å‰çš„æ¨¡å‹ã€‚è¿™ç§ç³»ç»Ÿæ€§çš„ç ”ç©¶æ–¹æ³•å’Œé«˜æ€§èƒ½çš„åŸºçº¿æ¨¡å‹ä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

**å…³é”®è®¾è®¡**ï¼šBaseRewardçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©Qwen2.5-VLä½œä¸ºéª¨å¹²æ¨¡å‹ï¼Œå› ä¸ºå®ƒå…·æœ‰å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼›2) ä½¿ç”¨ä¼˜åŒ–çš„ä¸¤å±‚å¥–åŠ±å¤´ï¼Œä»¥æé«˜å¥–åŠ±é¢„æµ‹çš„å‡†ç¡®æ€§ï¼›3) ç²¾å¿ƒç­–åˆ’é«˜è´¨é‡çš„å¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬åå¥½æ•°æ®æ··åˆï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›4) é‡‡ç”¨åˆé€‚çš„è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚å­¦ä¹ ç‡è°ƒæ•´å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BaseRewardåœ¨MM-RLHF-Reward Benchã€VL-Reward Benchå’ŒMultimodal Reward Benchç­‰ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå°†å…¶é›†æˆåˆ°å®é™…å¼ºåŒ–å­¦ä¹ æµç¨‹ä¸­ï¼ŒæˆåŠŸæå‡äº†MLLMåœ¨æ„ŸçŸ¥ã€æ¨ç†å’Œå¯¹è¯ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½è®­ç»ƒï¼Œæå‡æ¨¡å‹åœ¨å›¾åƒç†è§£ã€è§†é¢‘åˆ†æã€äººæœºå¯¹è¯ç­‰é¢†åŸŸçš„æ€§èƒ½ã€‚é€šè¿‡æ›´ç²¾å‡†åœ°æ•æ‰äººç±»åå¥½ï¼Œå¯ä»¥ä½¿MLLMåœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ æ™ºèƒ½ã€å¯é ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including \textit{reward modeling paradigms} (e.g., Naive-RM, Critic-based RM, and Generative RM), \textit{reward head architecture}, \textit{training strategies}, \textit{data curation} (covering over ten multimodal and text-only preference datasets), \textit{backbone model} and \textit{model scale}, and \textit{ensemble methods}.
>   Based on these experimental insights, we introduce \textbf{BaseReward}, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.

