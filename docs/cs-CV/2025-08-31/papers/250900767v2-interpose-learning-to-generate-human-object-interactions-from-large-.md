---
layout: default
title: InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos
---

# InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00767v2</a>
  <a href="https://arxiv.org/pdf/2509.00767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00767v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00767v2', 'InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yangsong Zhang, Abdul Ahad Butt, GÃ¼l Varol, Ivan Laptev

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-12-22)

**å¤‡æ³¨**: Accepted to 3DV 2026. Project page: https://mael-zys.github.io/InterPose/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInterPoseä»¥è§£å†³å¤æ‚åœºæ™¯ä¸­äººæœºäº¤äº’ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `åŠ¨ä½œç”Ÿæˆ` `æ•°æ®é›†æ„å»º` `è®¡ç®—æœºè§†è§‰` `æœºå™¨äººæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹å­¤ç«‹äººç‰©çš„åŠ¨ç”»ï¼Œç¼ºä¹å¤šæ ·åŒ–çš„äººæœºäº¤äº’æ•°æ®ï¼Œé™åˆ¶äº†ç”Ÿæˆçš„çœŸå®æ„Ÿå’Œå¤šæ ·æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¿åŠ¨æå–ç®¡é“ï¼Œæ„å»ºäº†åŒ…å«ä¸°å¯Œäººæœºäº¤äº’çš„3DåŠ¨ä½œæ•°æ®é›†InterPoseï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInterPoseåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†äººç±»åŠ¨ä½œç”Ÿæˆçš„æ•ˆæœï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åŠ¨ä½œç”Ÿæˆåœ¨å¤§è§„æ¨¡åŠ¨ä½œæ•æ‰æ•°æ®çš„æ”¯æŒä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å­¤ç«‹äººç‰©çš„åŠ¨ç”»åˆ¶ä½œä¸Šï¼Œåˆæˆå¤æ‚3Dåœºæ™¯ä¸­çš„çœŸå®äººæœºäº¤äº’ä»ç„¶æ˜¯è®¡ç®—æœºå›¾å½¢å­¦å’Œæœºå™¨äººé¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¿åŠ¨æå–ç®¡é“ï¼Œæ”¶é›†ä¸°å¯Œçš„äººæœºäº¤äº’è¿åŠ¨æ•°æ®ã€‚æ–°æ•°æ®é›†InterPoseåŒ…å«73.8Kä¸ª3Däººç±»åŠ¨ä½œåºåˆ—åŠå…¶å¯¹åº”çš„æ–‡æœ¬æè¿°ï¼Œå‡æ¥è‡ª45.8Kä¸ªåŒ…å«äººæœºäº¤äº’çš„è§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼ŒInterPoseæ˜¾è‘—æå‡äº†äººç±»åŠ¨ä½œç”Ÿæˆçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒåŸºäºInterPoseï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç†ï¼Œèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬åŠ¨ç”»ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤æ‚3Dåœºæ™¯ä¸­äººæœºäº¤äº’ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®å¤šæ ·æ€§å’ŒçœŸå®æ„Ÿæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è‡ªåŠ¨åŒ–è¿åŠ¨æå–ç®¡é“æ”¶é›†ä¸°å¯Œçš„äººæœºäº¤äº’æ•°æ®ï¼Œæ„å»ºæ–°çš„æ•°æ®é›†InterPoseï¼Œä»¥æ”¯æŒé«˜ä¿çœŸåº¦çš„äººæœºäº¤äº’ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è¿åŠ¨æå–å’Œç”Ÿæˆæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»å¤§è§„æ¨¡è§†é¢‘ä¸­æå–äººæœºäº¤äº’åŠ¨ä½œï¼Œç„¶ååˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒç”Ÿæˆæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šInterPoseæ•°æ®é›†çš„æ„å»ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼ŒåŒ…å«73.8Kä¸ªåŠ¨ä½œåºåˆ—ï¼Œæ˜¾è‘—ä¸°å¯Œäº†ç°æœ‰æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒInterPoseèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚åœºæ™¯ä¸­çš„äººæœºäº¤äº’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æå–è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œç¡®ä¿åŠ¨ä½œåºåˆ—çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨InterPoseæ•°æ®é›†çš„ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†çº¦20%çš„ç”Ÿæˆè´¨é‡ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡å±•ç¤ºäº†InterPoseåœ¨æ¨åŠ¨äººæœºäº¤äº’ç”Ÿæˆé¢†åŸŸçš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŠ¨ç”»åˆ¶ä½œã€è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ä»¥åŠäººæœºäº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡ç”Ÿæˆé«˜ä¿çœŸçš„äººæœºäº¤äº’ï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’çš„è‡ªç„¶æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å•†ä¸šåŒ–åº”ç”¨ã€‚æœªæ¥ï¼ŒInterPoseå¯èƒ½åœ¨æ™ºèƒ½æœºå™¨äººå’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.

