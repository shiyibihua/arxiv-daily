---
layout: default
title: Multimodal Iterative RAG for Knowledge-Intensive Visual Question Answering
---

# Multimodal Iterative RAG for Knowledge-Intensive Visual Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00798" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00798v4</a>
  <a href="https://arxiv.org/pdf/2509.00798.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00798v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00798v4', 'Multimodal Iterative RAG for Knowledge-Intensive Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changin Choi, Wonseok Lee, Jungmin Ko, Wonjong Rhee

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMI-RAGæ¡†æ¶ä»¥è§£å†³çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ä¸­çš„çŸ¥è¯†è·å–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æ¨ç†æœºåˆ¶` `çŸ¥è¯†ç»¼åˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®é¢˜æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè·å–å¤–éƒ¨çŸ¥è¯†ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºçš„MI-RAGæ¡†æ¶é€šè¿‡è¿­ä»£æ¨ç†å’ŒçŸ¥è¯†ç»¼åˆï¼Œå¢å¼ºäº†çŸ¥è¯†æ£€ç´¢çš„èƒ½åŠ›ï¼Œæå‡äº†æ¨¡å‹çš„ç†è§£æ·±åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMI-RAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ£€ç´¢å¬å›ç‡å’Œç­”æ¡ˆå‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾è‘—æå‡äº†å…¶åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºéœ€è¦è¶…å‡ºå›¾åƒè§†è§‰å†…å®¹çš„å¤–éƒ¨çŸ¥è¯†çš„çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®é¢˜ï¼ŒMLLMsçš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºæä¾›å¤–éƒ¨çŸ¥è¯†çš„æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶ä¼ ç»Ÿçš„å•æ¬¡å¤„ç†æ¡†æ¶å¾€å¾€æ— æ³•æ”¶é›†è¶³å¤Ÿçš„çŸ¥è¯†ã€‚ä¸ºå…‹æœè¿™ä¸€å±€é™ï¼Œæœ¬æ–‡æå‡ºäº†MI-RAGï¼Œä¸€ä¸ªå¤šæ¨¡æ€è¿­ä»£RAGæ¡†æ¶ï¼Œåˆ©ç”¨æ¨ç†å¢å¼ºæ£€ç´¢ï¼Œå¹¶ç»“åˆçŸ¥è¯†ç»¼åˆæ¥æå‡ç†è§£ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹å½¢æˆæ¨ç†å¼•å¯¼çš„å¤šæŸ¥è¯¢ï¼Œä»¥æ¢ç´¢çŸ¥è¯†çš„å¤šä¸ªæ–¹é¢ã€‚è¿™äº›æŸ¥è¯¢é©±åŠ¨è·¨å¼‚æ„çŸ¥è¯†åº“çš„è”åˆæœç´¢ï¼Œæ£€ç´¢å¤šæ ·çš„çŸ¥è¯†ï¼Œè¿›è€Œç»¼åˆè¿™äº›çŸ¥è¯†ä»¥ä¸°å¯Œæ¨ç†è®°å½•ï¼Œé€æ­¥åŠ æ·±æ¨¡å‹çš„ç†è§£ã€‚åœ¨åŒ…æ‹¬ç™¾ç§‘å…¨ä¹¦VQAã€InfoSeekå’ŒOK-VQAç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMI-RAGæ˜¾è‘—æé«˜äº†æ£€ç´¢å¬å›ç‡å’Œç­”æ¡ˆå‡†ç¡®æ€§ï¼Œä¸ºçŸ¥è¯†å¯†é›†å‹VQAä¸­çš„ç»„åˆæ¨ç†å»ºç«‹äº†å¯æ‰©å±•çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨è·å–å¤–éƒ¨çŸ¥è¯†æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ä¼ ç»ŸRAGæ¡†æ¶çš„å•æ¬¡å¤„ç†é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMI-RAGæ¡†æ¶é€šè¿‡è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼Œå½¢æˆå¤šæŸ¥è¯¢ç­–ç•¥ï¼Œæ¢ç´¢çŸ¥è¯†çš„å¤šç»´åº¦ï¼Œè¿›è€Œå¢å¼ºçŸ¥è¯†æ£€ç´¢å’Œç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMI-RAGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šæ¨ç†å¼•å¯¼çš„å¤šæŸ¥è¯¢ç”Ÿæˆã€è·¨å¼‚æ„çŸ¥è¯†åº“çš„è”åˆæœç´¢å’ŒçŸ¥è¯†ç»¼åˆã€‚æ¯ä¸ªæ¨¡å—åœ¨è¿­ä»£ä¸­ç›¸äº’ä½œç”¨ï¼Œé€æ­¥æå‡æ¨¡å‹çš„çŸ¥è¯†è·å–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMI-RAGçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è¿­ä»£æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡å¤šæŸ¥è¯¢æ¢ç´¢ä¸åŒçŸ¥è¯†é¢ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»ŸRAGçš„å•æ¬¡æ£€ç´¢æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMI-RAGé‡‡ç”¨äº†åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆç­–ç•¥ï¼Œç»“åˆå¤šç§çŸ¥è¯†åº“ï¼Œç¡®ä¿æ£€ç´¢çš„å¤šæ ·æ€§å’Œå…¨é¢æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMI-RAGæ˜¾è‘—æé«˜äº†æ£€ç´¢å¬å›ç‡å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ç™¾ç§‘å…¨ä¹¦VQAä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„ç­”æ¡ˆå‡†ç¡®ç‡æå‡äº†XX%ï¼Œæ£€ç´¢å¬å›ç‡æå‡äº†YY%ï¼Œå±•ç¤ºäº†å…¶åœ¨çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€æ•™è‚²è¾…åŠ©å·¥å…·å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è§†è§‰é—®ç­”çš„å‡†ç¡®æ€§ï¼ŒMI-RAGèƒ½å¤Ÿåœ¨å®é™…åœºæ™¯ä¸­æä¾›æ›´ä¸ºç²¾å‡†å’Œä¸°å¯Œçš„ä¿¡æ¯æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models~(MLLMs) have significantly enhanced the ability of these models in multimodal understanding and reasoning. However, the performance of MLLMs for knowledge-intensive visual questions, which require external knowledge beyond the visual content of an image, still remains limited. While Retrieval-Augmented Generation (RAG) has become a promising solution to provide models with external knowledge, its conventional single-pass framework often fails to gather sufficient knowledge. To overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that leverages reasoning to enhance retrieval and incorporates knowledge synthesis to refine its understanding. At each iteration, the model formulates a reasoning-guided multi-query to explore multiple facets of knowledge. Subsequently, these queries drive a joint search across heterogeneous knowledge bases, retrieving diverse knowledge. This retrieved knowledge is then synthesized to enrich the reasoning record, progressively deepening the model's understanding. Experiments on challenging benchmarks, including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG significantly improves both retrieval recall and answer accuracy, establishing a scalable approach for compositional reasoning in knowledge-intensive VQA.

