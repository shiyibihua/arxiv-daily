---
layout: default
title: Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models
---

# Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00787" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00787v3</a>
  <a href="https://arxiv.org/pdf/2509.00787.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00787v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00787v3', 'Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided Multimodal Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ganxi Xu, Jinyi Long, Jia Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-09-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾åƒåˆ°è„‘ä¿¡å·ç”Ÿæˆæ¡†æ¶ä»¥è§£å†³è§†è§‰å‡ä½“çš„ç¼–ç é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å‡ä½“` `è„‘ä¿¡å·ç”Ÿæˆ` `å¤šæ¨¡æ€èåˆ` `å»å™ªæ‰©æ•£æ¨¡å‹` `äº¤å‰æ³¨æ„åŠ›æœºåˆ¶` `CLIPç¼–ç å™¨` `ç¥ç»ç§‘å­¦` `è„‘æœºæ¥å£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†è§‰å‡ä½“çš„è„‘ç¼–ç é˜¶æ®µç¼ºä¹æœ‰æ•ˆçš„å›¾åƒåˆ°è„‘ä¿¡å·è½¬æ¢æœºåˆ¶ï¼Œé™åˆ¶äº†åŠŸèƒ½ç®¡é“çš„å®Œæ•´æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»å›¾åƒç”ŸæˆM/EEGä¿¡å·ï¼Œå¢å¼ºäº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹å¾å¯¹é½ã€‚
3. åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„è„‘ä¿¡å·å…·æœ‰ç”Ÿç‰©å­¦åˆç†æ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸ªä½“é—´å’Œä¸ªä½“å†…çš„ä¿¡å·å˜åŒ–å¯è§†åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å‡ä½“åœ¨æ¢å¤å¤±æ˜ä¸ªä½“çš„è§†åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å°½ç®¡ç ”ç©¶è€…ä»¬æˆåŠŸåˆ©ç”¨M/EEGä¿¡å·åœ¨è§†è§‰å‡ä½“çš„è„‘è§£ç é˜¶æ®µå¼•å‘è§†è§‰æ„ŸçŸ¥ï¼Œä½†å°†å›¾åƒè½¬æ¢ä¸ºM/EEGä¿¡å·çš„è„‘ç¼–ç é˜¶æ®µä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œé˜»ç¢äº†å®Œæ•´åŠŸèƒ½ç®¡é“çš„å½¢æˆã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå›¾åƒåˆ°è„‘ä¿¡å·æ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œä»å›¾åƒç”ŸæˆM/EEGä¿¡å·ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’Œå¢å¼ºäº¤å‰æ³¨æ„åŠ›çš„U-Netæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œç»“æœè¡¨æ˜å…¶ç”Ÿæˆçš„è„‘ä¿¡å·åœ¨ç”Ÿç‰©å­¦ä¸Šæ˜¯åˆç†çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰å‡ä½“ä¸­å›¾åƒåˆ°è„‘ä¿¡å·è½¬æ¢çš„ç¼ºå¤±é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è„‘è§£ç é˜¶æ®µï¼Œç¼ºä¹å¯¹è„‘ç¼–ç é˜¶æ®µçš„ç ”ç©¶ï¼Œå¯¼è‡´åŠŸèƒ½ç®¡é“ä¸å®Œæ•´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡ç»“åˆå»å™ªæ‰©æ•£æ¨¡å‹å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿä»è¾“å…¥å›¾åƒç”ŸæˆM/EEGä¿¡å·ï¼Œæ•æ‰è§†è§‰ç‰¹å¾ä¸è„‘ä¿¡å·è¡¨ç¤ºä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæå–è¾“å…¥å›¾åƒçš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºï¼›å…¶æ¬¡æ˜¯å¢å¼ºäº¤å‰æ³¨æ„åŠ›çš„U-Netæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£å»å™ªé‡å»ºè„‘ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒºåˆ«äºä¼ ç»Ÿç”Ÿæˆæ¨¡å‹çš„ç®€å•æ‹¼æ¥æ–¹æ³•ï¼Œä½¿å¾—è§†è§‰ç‰¹å¾ä¸è„‘ä¿¡å·è¡¨ç¤ºä¹‹é—´çš„å…³ç³»å¾—ä»¥æ›´å¥½åœ°æ•æ‰å’Œåˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œå¹¶é€šè¿‡å¤šå±‚U-Netç»“æ„å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç¡®ä¿ç”Ÿæˆçš„è„‘ä¿¡å·åœ¨ç”Ÿç‰©å­¦ä¸Šåˆç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¡†æ¶åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šç”Ÿæˆçš„è„‘ä¿¡å·åœ¨ç”Ÿç‰©å­¦ä¸Šåˆç†ï¼Œä¸”é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†æ›´å¥½çš„ç‰¹å¾å¯¹é½ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿å°šæœªæä¾›ï¼Œä½†å®éªŒå±•ç¤ºäº†æ˜¾è‘—çš„ä¸ªä½“é—´å’Œä¸ªä½“å†…ä¿¡å·å˜åŒ–å¯è§†åŒ–æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†è§‰å‡ä½“çš„å¼€å‘ä¸ä¼˜åŒ–ï¼Œèƒ½å¤Ÿä¸ºå¤±æ˜æ‚£è€…æä¾›æ›´æœ‰æ•ˆçš„è§†è§‰æ¢å¤æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„æŠ€æœ¯ä¹Ÿå¯èƒ½æ‰©å±•åˆ°å…¶ä»–è„‘æœºæ¥å£å’Œç¥ç»ç§‘å­¦ç ”ç©¶ä¸­ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual prostheses hold great promise for restoring vision in blind individuals. While researchers have successfully utilized M/EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process of converting images into M/EEG signals in the brain encoding stage remains largely unexplored, hindering the formation of a complete functional pipeline. In this work, we present, to our knowledge, the first image-to-brain signal framework that generates M/EEG from images by leveraging denoising diffusion probabilistic models enhanced with cross-attention mechanisms. Specifically, the proposed framework comprises two key components: a pretrained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that reconstructs brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules capture the complex interplay between visual features and brain signal representations, enabling fine-grained alignment during generation. We evaluate the framework on two multimodal benchmark datasets and demonstrate that it generates biologically plausible brain signals. We also present visualizations of M/EEG topographies across all subjects in both datasets, providing intuitive demonstrations of intra-subject and inter-subject variations in brain signals.

