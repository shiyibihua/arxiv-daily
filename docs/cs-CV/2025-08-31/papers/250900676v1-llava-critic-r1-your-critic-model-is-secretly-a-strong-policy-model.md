---
layout: default
title: LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model
---

# LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00676" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.00676v1</a>
  <a href="https://arxiv.org/pdf/2509.00676.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00676v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00676v1', 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, Furong Huang

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-31

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LLaVA-Critic-R1‰ª•‰ºòÂåñÂ§öÊ®°ÊÄÅÁîüÊàê‰∏éËØÑ‰º∞**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÁîüÊàê` `ËßÜËßâËØ≠Ë®ÄÂª∫Ê®°` `Âº∫ÂåñÂ≠¶‰π†` `ËØÑËÆ∫Ê®°Âûã` `Ëá™ÊàëËØÑËÆ∫` `Êé®ÁêÜ‰ªªÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËØÑËÆ∫Ê®°Âûã‰∏éÁîüÊàêÊ®°Âûã‰πãÈó¥Â≠òÂú®ÊòéÊòæÁöÑÂàÜÁ¶ªÔºåÈôêÂà∂‰∫ÜËØÑËÆ∫Ê®°ÂûãÁöÑÁõ¥Êé•Â∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Â∞ÜËØÑËÆ∫Êï∞ÊçÆÈõÜÈáçÁªÑ‰∏∫ÂèØÈ™åËØÅ‰ø°Âè∑ÔºåÂπ∂Âú®Âü∫Á°ÄÁîüÊàêÊ®°Âûã‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåÂΩ¢ÊàêLLaVA-Critic-R1„ÄÇ
3. LLaVA-Critic-R1Âú®Â§ö‰∏™ËßÜËßâÊé®ÁêÜÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂπ≥ÂùáÊèêÂçá5.7%ÔºåÂπ∂Âú®Êé®ÁêÜÊó∂ÈÄöËøáËá™ÊàëËØÑËÆ∫Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ËßÜËßâËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÔºåËØÑËÆ∫Ê®°ÂûãÈÄöÂ∏∏Áî®‰∫éËØÑ‰º∞ËæìÂá∫ÔºåËÄåÈùûÁîüÊàêÂìçÂ∫î„ÄÇÊú¨ÊñáÊåëÊàòËøô‰∏Ä‰º†ÁªüÔºåÊèêÂá∫Â∞ÜÂÅèÂ•ΩÊ†áËÆ∞ÁöÑËØÑËÆ∫Êï∞ÊçÆÈõÜÈáçÁªÑ‰∏∫ÂèØÈ™åËØÅÁöÑËÆ≠ÁªÉ‰ø°Âè∑ÔºåÂπ∂Áõ¥Êé•Âú®Âü∫Á°ÄÁîüÊàêÊ®°Âûã‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåÁîüÊàêLLaVA-Critic-R1„ÄÇËØ•Ê®°Âûã‰∏ç‰ªÖÂú®ËØÑËÆ∫‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËøòÂú®26‰∏™ËßÜËßâÊé®ÁêÜÂü∫ÂáÜ‰∏ä‰∏é‰∏ìÈó®ÁöÑÊé®ÁêÜÊ®°ÂûãÁõ∏Â™≤ÁæéÔºåÂπ≥ÂùáÊèêÂçá5.7%„ÄÇËøõ‰∏ÄÊ≠•Êâ©Â±ïËá≥Áé∞ÊúâÂº∫Êé®ÁêÜÊ®°ÂûãÔºåLLaVA-Critic-R1+Âú®‰∏çÁâ∫Áâ≤ËØÑËÆ∫Ë¥®ÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∫Ü71.9ÁöÑSoTAÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂ¢ûÂº∫ÁöÑËØÑËÆ∫ËÉΩÂäõÂú®Êé®ÁêÜÊó∂‰πüÂ∏¶Êù•‰∫ÜÊòæËëóÊèêÂçáÔºåÂπ≥ÂùáÊèêÈ´ò13.8%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËØÑËÆ∫Ê®°ÂûãÈÄöÂ∏∏‰ªÖÁî®‰∫éËØÑ‰º∞ËæìÂá∫ÔºåÁº∫‰πèÁîüÊàêËÉΩÂäõÔºåÂØºËá¥ÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÈÄöËøáÂ∞ÜÂÅèÂ•ΩÊ†áËÆ∞ÁöÑËØÑËÆ∫Êï∞ÊçÆÈõÜÈáçÁªÑ‰∏∫ÂèØÈ™åËØÅÁöÑËÆ≠ÁªÉ‰ø°Âè∑ÔºåÁõ¥Êé•Âú®Âü∫Á°ÄÁîüÊàêÊ®°Âûã‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºåÊó®Âú®ÂÆûÁé∞ËØÑËÆ∫‰∏éÁîüÊàêËÉΩÂäõÁöÑÁªü‰∏Ä„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈõÜÈáçÁªÑ„ÄÅÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂíåÁîüÊàêÊ®°Âûã‰ºòÂåñ‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÈáçÁªÑËØÑËÆ∫Êï∞ÊçÆÈõÜÔºåÁÑ∂ÂêéÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÁîüÊàêÊ®°ÂûãÔºåÊúÄÂêéÂÆûÁé∞Â§öÊ®°ÊÄÅÁîüÊàê‰∏éËØÑ‰º∞ÁöÑÁªü‰∏Ä„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLLaVA-Critic-R1‰∏ç‰ªÖ‰Ωú‰∏∫È´òÊïàÁöÑËØÑËÆ∫Ê®°ÂûãÔºåËøòÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÁîüÊàêËÉΩÂäõÔºå‰∏é‰º†ÁªüÁöÑËØÑËÆ∫‰∏éÁîüÊàêÊ®°ÂûãÂàÜÁ¶ªÁöÑÂÅöÊ≥ïÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñËØÑËÆ∫Ë¥®ÈáèÔºåÂπ∂Ë∞ÉÊï¥‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•Â¢ûÂº∫ÁîüÊàêËÉΩÂäõÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LLaVA-Critic-R1Âú®26‰∏™ËßÜËßâÊé®ÁêÜÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂπ≥ÂùáÊèêÂçá5.7%ÔºåÂπ∂Âú®Áé∞ÊúâÂº∫Êé®ÁêÜÊ®°ÂûãÂü∫Á°Ä‰∏äËøõ‰∏ÄÊ≠•‰ºòÂåñÔºåLLaVA-Critic-R1+ËææÂà∞‰∫Ü71.9ÁöÑSoTAÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂ∫îÁî®Ëá™ÊàëËØÑËÆ∫Âú®Êé®ÁêÜÊó∂Âπ≥ÂùáÊèêÈ´ò‰∫Ü13.8%ÁöÑÊÄßËÉΩÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®ÂÜÖÂÆπÁîüÊàêÂíåÂ§öÊ®°ÊÄÅ‰∫§‰∫íÁ≥ªÁªü„ÄÇÈÄöËøá‰ºòÂåñËØÑËÆ∫‰∏éÁîüÊàêÁöÑÁªü‰∏ÄÊ®°ÂûãÔºåÂèØ‰ª•ÊèêÂçá‰∫∫Êú∫‰∫§‰∫íÁöÑËá™ÁÑ∂ÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.

