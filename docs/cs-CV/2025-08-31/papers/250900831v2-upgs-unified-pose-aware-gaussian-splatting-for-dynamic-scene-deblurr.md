---
layout: default
title: UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring
---

# UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00831" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00831v2</a>
  <a href="https://arxiv.org/pdf/2509.00831.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00831v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00831v2', 'UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhijing Wu, Longguang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-09-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å§¿æ€æ„ŸçŸ¥é«˜æ–¯ç‚¹äº‘ä»¥è§£å†³åŠ¨æ€åœºæ™¯å»æ¨¡ç³Šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯é‡å»º` `è¿åŠ¨æ¨¡ç³Š` `é«˜æ–¯ç‚¹äº‘` `å§¿æ€ä¼°è®¡` `ä¼˜åŒ–æ¡†æ¶` `å¢å¼ºç°å®` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠ¨æ€å»æ¨¡ç³Šæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤æ­¥æµç¨‹ï¼Œå§¿æ€ä¼°è®¡å—åˆ°æ¨¡ç³Šä¼ªå½±å½±å“ï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶ï¼Œå°†ç›¸æœºå§¿æ€ä½œä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œä¸ä¸‰ç»´é«˜æ–¯å±æ€§å…±åŒä¼˜åŒ–ï¼Œæå‡é‡å»ºæ•ˆæœã€‚
3. åœ¨Stereo Bluræ•°æ®é›†å’ŒçœŸå®åœºæ™¯åºåˆ—ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡å’Œå§¿æ€ä¼°è®¡ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»å•ç›®è§†é¢‘é‡å»ºåŠ¨æ€ä¸‰ç»´åœºæ™¯åœ¨å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€æœºå™¨äººå’Œè‡ªä¸»å¯¼èˆªç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†ç”±äºç›¸æœºå’Œç‰©ä½“è¿åŠ¨å¼•èµ·çš„ä¸¥é‡è¿åŠ¨æ¨¡ç³Šï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å¤±è´¥ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éµå¾ªä¸¤æ­¥æµç¨‹ï¼Œé¦–å…ˆä¼°è®¡ç›¸æœºå§¿æ€ï¼Œç„¶åä¼˜åŒ–ä¸‰ç»´é«˜æ–¯ã€‚ç”±äºæ¨¡ç³Šä¼ªå½±é€šå¸¸ä¼šå‰Šå¼±å§¿æ€ä¼°è®¡ï¼Œå§¿æ€è¯¯å·®å¯èƒ½ä¼šç´¯ç§¯ï¼Œå¯¼è‡´é‡å»ºç»“æœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶ï¼Œå°†ç›¸æœºå§¿æ€ä½œä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œä¸ä¸‰ç»´é«˜æ–¯å±æ€§äº’è¡¥ï¼Œå®ç°ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†ç›¸æœºå’Œç‰©ä½“è¿åŠ¨é‡æ–°è¡¨è¿°ä¸ºå¯¹ä¸‰ç»´é«˜æ–¯çš„æ¯ä¸ªåŸå§‹ä½“ç´ è¿›è¡ŒSE(3)ä»¿å°„å˜æ¢ï¼Œå¹¶åˆ¶å®šç»Ÿä¸€çš„ä¼˜åŒ–ç›®æ ‡ã€‚é€šè¿‡å¼•å…¥ä¸‰é˜¶æ®µè®­ç»ƒè®¡åˆ’ï¼Œäº¤æ›¿ä¼˜åŒ–ç›¸æœºå§¿æ€å’Œé«˜æ–¯ï¼Œç¡®ä¿ä¼˜åŒ–çš„ç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘é‡å»ºåŠ¨æ€ä¸‰ç»´åœºæ™¯ä¸­çš„è¿åŠ¨æ¨¡ç³Šé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å§¿æ€ä¼°è®¡ä¸­å—åˆ°æ¨¡ç³Šä¼ªå½±çš„å½±å“ï¼Œå¯¼è‡´é‡å»ºæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶ï¼Œå°†ç›¸æœºå§¿æ€è§†ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œä¸ä¸‰ç»´é«˜æ–¯å±æ€§å…±åŒè¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œä»è€Œå‡å°‘å§¿æ€è¯¯å·®å¯¹é‡å»ºçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆå›ºå®šå§¿æ€è®­ç»ƒä¸‰ç»´é«˜æ–¯ï¼Œå…¶æ¬¡ä¼˜åŒ–å§¿æ€è€Œä¸æ”¹å˜é«˜æ–¯ï¼Œæœ€åå…±åŒä¼˜åŒ–æ‰€æœ‰å¯å­¦ä¹ å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå°†ç›¸æœºå’Œç‰©ä½“è¿åŠ¨è§†ä¸ºå¯¹ä¸‰ç»´é«˜æ–¯çš„SE(3)ä»¿å°„å˜æ¢ï¼Œæå‡ºçš„ç»Ÿä¸€ä¼˜åŒ–ç›®æ ‡æœ‰æ•ˆå‡å°‘äº†å§¿æ€è¯¯å·®çš„ç´¯ç§¯ï¼Œæ˜¾è‘—æå‡äº†é‡å»ºè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒè®¡åˆ’ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†é‡å»ºè¯¯å·®å’Œå§¿æ€ä¼°è®¡è¯¯å·®ï¼Œç¡®ä¿ä¼˜åŒ–çš„å…¨é¢æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å‚æ•°æ›´æ–°ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨Stereo Bluræ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰åŠ¨æ€å»æ¨¡ç³Šæ–¹æ³•ï¼Œé‡å»ºè´¨é‡æå‡äº†XX%ï¼Œå§¿æ€ä¼°è®¡ç²¾åº¦æé«˜äº†YY%ã€‚åœ¨çœŸå®åœºæ™¯åºåˆ—ä¸­ï¼Œæ–¹æ³•åŒæ ·å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜åŠ¨æ€åœºæ™¯çš„é‡å»ºè´¨é‡ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·ä½“éªŒï¼Œæå‡è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reconstructing dynamic 3D scenes from monocular video has broad applications in AR/VR, robotics, and autonomous navigation, but often fails due to severe motion blur caused by camera and object motion. Existing methods commonly follow a two-step pipeline, where camera poses are first estimated and then 3D Gaussians are optimized. Since blurring artifacts usually undermine pose estimation, pose errors could be accumulated to produce inferior reconstruction results. To address this issue, we introduce a unified optimization framework by incorporating camera poses as learnable parameters complementary to 3DGS attributes for end-to-end optimization. Specifically, we recast camera and object motion as per-primitive SE(3) affine transformations on 3D Gaussians and formulate a unified optimization objective. For stable optimization, we introduce a three-stage training schedule that optimizes camera poses and Gaussians alternatively. Particularly, 3D Gaussians are first trained with poses being fixed, and then poses are optimized with 3D Gaussians being untouched. Finally, all learnable parameters are optimized together. Extensive experiments on the Stereo Blur dataset and challenging real-world sequences demonstrate that our method achieves significant gains in reconstruction quality and pose estimation accuracy over prior dynamic deblurring methods.

