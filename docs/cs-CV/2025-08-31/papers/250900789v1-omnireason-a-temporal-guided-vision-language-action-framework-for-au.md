---
layout: default
title: OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving
---

# OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00789" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00789v1</a>
  <a href="https://arxiv.org/pdf/2509.00789.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00789v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00789v1', 'OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pei Liu, Qingtian Ning, Xinyan Lu, Haipeng Liu, Weiliang Ma, Dangen She, Peng Jia, Xianpeng Lang, Jun Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniReasonæ¡†æ¶ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„æ—¶ç©ºæ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶ç©ºæ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨é©¾é©¶` `å¤šæ¨¡æ€å­¦ä¹ ` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `åŠ¨æ€ç¯å¢ƒå»ºæ¨¡` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦å…³æ³¨é™æ€åœºæ™¯ï¼Œç¼ºä¹å¯¹åŠ¨æ€ç¯å¢ƒä¸­æ—¶é—´ç»´åº¦çš„ç†è§£ï¼Œé™åˆ¶äº†è‡ªåŠ¨é©¾é©¶çš„å†³ç­–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºOmniReasonæ¡†æ¶ï¼Œé€šè¿‡è”åˆå»ºæ¨¡åŠ¨æ€3Dç¯å¢ƒå’Œå†³ç­–è¿‡ç¨‹ï¼Œå®ç°äº†å¼ºå¤§çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniReason-Agentåœ¨å¼€æ”¾ç¯è§„åˆ’ä»»åŠ¡å’Œè§†è§‰é—®ç­”åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œæ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºé™æ€åœºæ™¯ç†è§£ï¼Œå¿½è§†äº†çœŸå®é©¾é©¶åœºæ™¯ä¸­çš„æ—¶é—´ç»´åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†OmniReasonæ¡†æ¶ï¼Œé€šè¿‡è”åˆå»ºæ¨¡åŠ¨æ€3Dç¯å¢ƒåŠå…¶å†³ç­–è¿‡ç¨‹ï¼Œå»ºç«‹äº†å¼ºå¤§çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰ä¸¤ä¸ªé‡è¦è¿›å±•ï¼šä¸€æ˜¯å¼•å…¥äº†OmniReason-Dataï¼Œä¸¤ä¸ªå¤§è§„æ¨¡çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ•°æ®é›†ï¼Œå…·æœ‰å¯†é›†çš„æ—¶ç©ºæ³¨é‡Šå’Œè‡ªç„¶è¯­è¨€è§£é‡Šï¼›äºŒæ˜¯å¼€å‘äº†OmniReason-Agentæ¶æ„ï¼Œé›†æˆäº†ç¨€ç–æ—¶é—´è®°å¿†æ¨¡å—å’Œè§£é‡Šç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„å†³ç­–ç†ç”±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniReason-Agentåœ¨å¼€æ”¾ç¯è§„åˆ’ä»»åŠ¡å’Œè§†è§‰é—®ç­”åŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¯¹åŠ¨æ€åœºæ™¯æ—¶ç©ºæ¨ç†ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºé™æ€åœºæ™¯ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚çš„åŠ¨æ€ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniReasonæ¡†æ¶é€šè¿‡è”åˆå»ºæ¨¡åŠ¨æ€ç¯å¢ƒä¸å†³ç­–è¿‡ç¨‹ï¼Œå¢å¼ºäº†æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¼•å…¥äº†ç¨€ç–æ—¶é—´è®°å¿†æ¨¡å—ä»¥ä¿æŒåœºæ™¯ä¸Šä¸‹æ–‡çš„æŒä¹…æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniReasonæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šOmniReason-Dataæ•°æ®é›†å’ŒOmniReason-Agentæ¶æ„ã€‚æ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„æ—¶ç©ºæ³¨é‡Šï¼Œè€ŒAgentæ¶æ„åˆ™ç»“åˆäº†æ—¶é—´è®°å¿†æ¨¡å—å’Œè§£é‡Šç”Ÿæˆå™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¨€ç–æ—¶é—´è®°å¿†æ¨¡å—å’Œè§£é‡Šç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„å†³ç­–ç†ç”±ï¼Œå¹¶é€šè¿‡æ—¶ç©ºçŸ¥è¯†è’¸é¦æ•æ‰å› æœæ¨ç†æ¨¡å¼ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒOmniReason-Agenté‡‡ç”¨äº†ç¨€ç–æ—¶é—´è®°å¿†æ¨¡å—ä»¥ä¼˜åŒ–åœºæ™¯ä¸Šä¸‹æ–‡çš„å»ºæ¨¡ï¼ŒåŒæ—¶è®¾è®¡äº†æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ—¶ç©ºæ¨ç†ä¸å†³ç­–è§£é‡Šçš„ç”Ÿæˆï¼Œç¡®ä¿ç”Ÿæˆçš„è§£é‡Šå…·æœ‰ç‰©ç†åˆç†æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniReason-Agentåœ¨å¼€æ”¾ç¯è§„åˆ’ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†20%çš„æ€§èƒ½ï¼Œå¹¶åœ¨è§†è§‰é—®ç­”åŸºå‡†ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä¼˜è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åœ¨æ—¶ç©ºæ¨ç†å’Œå†³ç­–è§£é‡Šæ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniReasonæ¡†æ¶åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæå‡è½¦è¾†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…æœ‰åŠ©äºæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œè¿˜èƒ½ä¸ºæœªæ¥çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿæä¾›é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in vision-language models (VLMs) have demonstrated impressive spatial reasoning capabilities for autonomous driving, yet existing methods predominantly focus on static scene understanding while neglecting the essential temporal dimension of real-world driving scenarios. To address this critical limitation, we propose the OmniReason framework, which establishes robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and their underlying decision-making processes. Our work makes two fundamental advances: (1) We introduce OmniReason-Data, two large-scale vision-language-action (VLA) datasets with dense spatiotemporal annotations and natural language explanations, generated through a novel hallucination-mitigated auto-labeling pipeline that ensures both physical plausibility and temporal coherence; (2) We develop the OmniReason-Agent architecture, which integrates a sparse temporal memory module for persistent scene context modeling and an explanation generator that produces human-interpretable decision rationales, facilitated by our spatiotemporal knowledge distillation approach that effectively captures spatiotemporal causal reasoning patterns. Comprehensive experiments demonstrate state-of-the-art performance, where OmniReason-Agent achieves significant improvements in both open-loop planning tasks and visual question answering (VQA) benchmarks, while establishing new capabilities for interpretable, temporally-aware autonomous vehicles operating in complex, dynamic environments.

