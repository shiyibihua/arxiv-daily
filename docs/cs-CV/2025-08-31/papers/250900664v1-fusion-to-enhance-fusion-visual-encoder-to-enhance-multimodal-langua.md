---
layout: default
title: Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model
---

# Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00664" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00664v1</a>
  <a href="https://arxiv.org/pdf/2509.00664.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00664v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00664v1', 'Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifei She, Huangxuan Wu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFusion to Enhanceä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰æ„ŸçŸ¥` `ç»†ç²’åº¦ç†è§£` `ç‰¹å¾èåˆ` `å¤šå¤´äº¤å‰æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è¯­ä¹‰ç†è§£ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŸºæœ¬è§†è§‰ä»»åŠ¡ä¸­å´å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä¸»è¦æºäºå•ä¸€è§†è§‰ç¼–ç å™¨çš„è®¾è®¡ã€‚
2. æœ¬æ–‡æå‡ºçš„Fusion to Enhanceæ¡†æ¶é€šè¿‡ç»„åˆé”šç¼–ç å™¨ä¸å¢å¼ºç¼–ç å™¨ï¼Œåˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFtZåœ¨TextVQAã€POPEã€MMMUã€MMEå’ŒMM-Vetç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå•ä¸€ç¼–ç å™¨å’Œç°æœ‰ç‰¹å¾èåˆæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥ä¸é«˜å±‚æ¬¡æ–‡æœ¬æ¨ç†ä¹‹é—´å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºæœ¬è§†è§‰ä»»åŠ¡ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºä¾èµ–å•ä¸€è§†è§‰ç¼–ç å™¨ï¼Œå¯¼è‡´æ— æ³•æ•æ‰ç»†ç²’åº¦çš„è§†è§‰ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Fusion to Enhanceï¼ˆFtZï¼‰ï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰å¡”æ¡†æ¶ã€‚FtZé€šè¿‡è½»é‡çº§çš„å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¯­ä¹‰å¼ºå¤§çš„é”šç¼–ç å™¨ä¸æ„ŸçŸ¥ä¸°å¯Œçš„å¢å¼ºç¼–ç å™¨è¿›è¡Œç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFtZåœ¨å¤šä¸ªéœ€è¦ç»†ç²’åº¦è§†è§‰ç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨å•ä¸€ç¼–ç å™¨æˆ–ç°æœ‰ç‰¹å¾èåˆæ–¹æ³•çš„åŸºçº¿æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€è§†è§‰ç¼–ç å™¨ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ç»†èŠ‚ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºFusion to Enhanceæ¡†æ¶ï¼Œé€šè¿‡ç»„åˆè¯­ä¹‰å¼ºçš„é”šç¼–ç å™¨ä¸æ„ŸçŸ¥ä¸°å¯Œçš„å¢å¼ºç¼–ç å™¨ï¼Œåˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFtZæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé”šç¼–ç å™¨è´Ÿè´£é«˜å±‚æ¬¡è¯­ä¹‰ä¿¡æ¯çš„æå–ï¼Œå¢å¼ºç¼–ç å™¨åˆ™ä¸“æ³¨äºç»†èŠ‚æ„ŸçŸ¥ï¼ŒäºŒè€…é€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿¡æ¯èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ‰“ç ´äº†å•ä¸€ç¼–ç å™¨çš„é™åˆ¶ï¼Œé€šè¿‡å¼‚æ„ä¸“å®¶ç¼–ç å™¨çš„ç»„åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨è½»é‡çº§çš„å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿ä¿¡æ¯çš„é«˜æ•ˆèåˆï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œæ³¨é‡ç»†ç²’åº¦ä¿¡æ¯çš„æ•æ‰ä¸è¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªç»†ç²’åº¦è§†è§‰ç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFtZæ¨¡å‹çš„è¡¨ç°è¶…è¶Šäº†ä¼ ç»Ÿå•ä¸€ç¼–ç å™¨å’Œç°æœ‰ç‰¹å¾èåˆæ–¹æ³•ï¼Œå…·ä½“åœ¨TextVQAç­‰ä»»åŠ¡ä¸Šæå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½è§†è§‰åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€åŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ¨¡æ€ç³»ç»Ÿçš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have made significant progress in bridging visual perception with high-level textual reasoning. However, they face a fundamental contradiction: while excelling at complex semantic understanding, these models often fail at basic visual tasks that require precise detail perception. This deficiency primarily stems from the prevalent architectural reliance on a single vision encoder optimized for high-level semantic alignment, which inherently sacrifices the ability to capture fine-grained visual information. To address this issue, we introduce Fusion to Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the single-encoder design by innovatively composing a semantically powerful anchor encoder with a perception-rich augmenting encoder via a lightweight Multi-Head Cross-Attention mechanism. Experimental results demonstrate that on several challenging benchmarks demanding fine-grained visual understanding, such as TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms baselines that use only a single encoder or existing feature fusion methods. This work proves that composing heterogeneous expert encoders is an efficient and effective path to overcoming the visual perception bottleneck in current MLLMs, offering a new design paradigm for building next-generation AI systems with stronger perceptual capabilities.

