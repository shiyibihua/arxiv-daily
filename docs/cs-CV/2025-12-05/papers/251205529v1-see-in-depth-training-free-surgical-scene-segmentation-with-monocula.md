---
layout: default
title: See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors
---

# See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.05529" target="_blank" class="toolbar-btn">arXiv: 2512.05529v1</a>
    <a href="https://arxiv.org/pdf/2512.05529.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.05529v1" 
            onclick="toggleFavorite(this, '2512.05529v1', 'See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kunyi Yang, Qingyu Wang, Cheng Yuan, Yutong Ban

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-05

**Â§áÊ≥®**: The first two authors contributed equally

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂçïÁõÆÊ∑±Â∫¶ÂÖàÈ™åÁöÑÊó†ËÆ≠ÁªÉÊâãÊúØÂú∫ÊôØÂàÜÂâ≤ÊñπÊ≥ïDepSeg**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÊâãÊúØÂú∫ÊôØÂàÜÂâ≤` `ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°` `Êó†ËÆ≠ÁªÉÂ≠¶‰π†` `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Ê®°ÊùøÂåπÈÖç` `ËÆ°ÁÆóÊú∫ËæÖÂä©ÊâãÊúØ` `ËÖπËÖîÈïúÊâãÊúØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËÖπËÖîÈïúÊâãÊúØÂú∫ÊôØÂàÜÂâ≤ÊñπÊ≥ï‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåÊàêÊú¨È´òÊòÇ‰∏îÈöæ‰ª•Êâ©Â±ï„ÄÇ
2. DepSegÂà©Áî®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰Ωú‰∏∫Âá†‰ΩïÂÖàÈ™åÔºåÁªìÂêàÈ¢ÑËÆ≠ÁªÉËßÜËßâÊ®°ÂûãÔºåÂÆûÁé∞Êó†ËÆ≠ÁªÉÁöÑÂú∫ÊôØÂàÜÂâ≤„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDepSegÂú®ÂàÜÂâ≤Á≤æÂ∫¶‰∏äÊòæËëó‰ºò‰∫éÁõ¥Êé•‰ΩøÁî®SAM2ÁöÑÊñπÊ≥ïÔºå‰∏îÂØπÊ®°ÊùøÊï∞Èáè‰∏çÊïèÊÑü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÖπËÖîÈïúÂú∫ÊôØÁöÑÂÉèÁ¥†Á∫ßÂàÜÂâ≤ÂØπ‰∫éËÆ°ÁÆóÊú∫ËæÖÂä©ÊâãÊúØËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÁî±‰∫éÂØÜÈõÜÊ†áÊ≥®ÁöÑÈ´òÊàêÊú¨ËÄåÈöæ‰ª•Êâ©Â±ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑±Â∫¶ÂºïÂØºÁöÑÊâãÊúØÂú∫ÊôØÂàÜÂâ≤Ê°ÜÊû∂(DepSeg)ÔºåËØ•Ê°ÜÊû∂Âà©Áî®ÂçïÁõÆÊ∑±Â∫¶‰Ωú‰∏∫Âá†‰ΩïÂÖàÈ™åÔºåÂπ∂ÁªìÂêàÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºåÊó†ÈúÄËÆ≠ÁªÉ„ÄÇDepSegÈ¶ñÂÖà‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁΩëÁªú‰º∞ËÆ°Áõ∏ÂØπÊ∑±Â∫¶ÂõæÔºåÂπ∂ÊèêÂá∫Ê∑±Â∫¶ÂºïÂØºÁöÑÁÇπÊèêÁ§∫ÔºåSAM2Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫Á±ªÂà´Êó†ÂÖ≥ÁöÑÊé©Á†Å„ÄÇÁÑ∂ÂêéÔºåÊØè‰∏™Êé©Á†ÅÁî±‰∏Ä‰∏™Ê±†ÂåñÁöÑÈ¢ÑËÆ≠ÁªÉËßÜËßâÁâπÂæÅÊèèËø∞ÔºåÂπ∂ÈÄöËøáÊ®°ÊùøÂåπÈÖçÈíàÂØπ‰ªéÂ∏¶Ê≥®ÈáäÁöÑÂ∏ßÊûÑÂª∫ÁöÑÊ®°ÊùøÂ∫ìËøõË°åÂàÜÁ±ª„ÄÇÂú®CholecSeg8kÊï∞ÊçÆÈõÜ‰∏äÔºåDepSeg‰ºò‰∫éÁõ¥Êé•ÁöÑSAM2Ëá™Âä®ÂàÜÂâ≤Âü∫Á∫øÔºà35.9% vs. 14.7% mIoUÔºâÔºåÂç≥‰Ωø‰ªÖ‰ΩøÁî®10-20%ÁöÑÂØπË±°Ê®°Êùø‰πüËÉΩ‰øùÊåÅÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÊ∑±Â∫¶ÂºïÂØºÁöÑÊèêÁ§∫ÂíåÂü∫‰∫éÊ®°ÊùøÁöÑÂàÜÁ±ªÊèê‰æõ‰∫Ü‰∏ÄÁßçÊ≥®ÈáäÈ´òÊïàÁöÑÂàÜÂâ≤ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËÖπËÖîÈïúÊâãÊúØÂú∫ÊôØ‰∏≠ÔºåÁî±‰∫éÁº∫‰πèÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆËÄåÂØºËá¥ÁöÑÂÉèÁ¥†Á∫ßÂàÜÂâ≤ÈöæÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂ§ßÈáèÁöÑÂÉèÁ¥†Á∫ßÊ†áÊ≥®ÔºåËøôÂú®ÂåªÁñóÈ¢ÜÂüüÈùûÂ∏∏ËÄóÊó∂‰∏îÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂèØÊâ©Â±ïÊÄß„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÊàñ‰ªÖÈúÄÂ∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÂ∞±ËÉΩÂÆûÁé∞Á≤æÁ°ÆÂàÜÂâ≤ÁöÑÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰Ωú‰∏∫Âá†‰ΩïÂÖàÈ™å‰ø°ÊÅØÔºåÂºïÂØºÂàÜÂâ≤ËøáÁ®ã„ÄÇÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁΩëÁªúËé∑ÂèñÂú∫ÊôØÁöÑÊ∑±Â∫¶‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÁÇπÊèêÁ§∫Ôºå‰ªéËÄåÂºïÂØºSAM2ÁîüÊàêÁ±ªÂà´Êó†ÂÖ≥ÁöÑÊé©Á†Å„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®Ê®°ÊùøÂåπÈÖçÁöÑÊñπÂºèÔºåÂ∞ÜËøô‰∫õÊé©Á†Å‰∏éÂ∞ëÈáèÊ†áÊ≥®Ê†∑Êú¨ËøõË°åÂåπÈÖçÔºåÂÆûÁé∞ÊúÄÁªàÁöÑÂàÜÂâ≤„ÄÇËøôÁßçÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂú®‰∫éÂà©Áî®Ê∑±Â∫¶‰ø°ÊÅØÂáèÂ∞ëÂØπÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDepSegÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) **ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°**Ôºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁΩëÁªú‰º∞ËÆ°ËæìÂÖ•ÂõæÂÉèÁöÑÁõ∏ÂØπÊ∑±Â∫¶Âõæ„ÄÇ2) **Ê∑±Â∫¶ÂºïÂØºÁöÑÁÇπÊèêÁ§∫**ÔºöÊ†πÊçÆÊ∑±Â∫¶ÂõæÁîüÊàêÁÇπÊèêÁ§∫ÔºåÁî®‰∫éÂºïÂØºSAM2ÁîüÊàêÁ±ªÂà´Êó†ÂÖ≥ÁöÑÊé©Á†Å„ÄÇ3) **Êé©Á†ÅÁîüÊàê**Ôºö‰ΩøÁî®SAM2Â∞ÜÁÇπÊèêÁ§∫ËΩ¨Êç¢‰∏∫Á±ªÂà´Êó†ÂÖ≥ÁöÑÊé©Á†Å„ÄÇ4) **ÁâπÂæÅÊèêÂèñ**ÔºöÂØπÊØè‰∏™Êé©Á†ÅÂå∫ÂüüÊèêÂèñÈ¢ÑËÆ≠ÁªÉËßÜËßâÊ®°ÂûãÁöÑÁâπÂæÅ„ÄÇ5) **Ê®°ÊùøÂåπÈÖç**ÔºöÂ∞ÜÊèêÂèñÁöÑÁâπÂæÅ‰∏é‰ªéÂ∞ëÈáèÊ†áÊ≥®Ê†∑Êú¨ÊûÑÂª∫ÁöÑÊ®°ÊùøÂ∫ìËøõË°åÂåπÈÖçÔºå‰ªéËÄåÁ°ÆÂÆöÊé©Á†ÅÁöÑÁ±ªÂà´„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰∏éÈ¢ÑËÆ≠ÁªÉËßÜËßâÊ®°ÂûãÁõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫Ü‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÊàñ‰ªÖÈúÄÂ∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑËÖπËÖîÈïúÊâãÊúØÂú∫ÊôØÂàÜÂâ≤ÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊòæËëóÈôç‰Ωé‰∫ÜÂØπÊ†áÊ≥®Êï∞ÊçÆÁöÑÈúÄÊ±ÇÔºåÊèêÈ´ò‰∫ÜÂèØÊâ©Â±ïÊÄß„ÄÇ‰∏éÁõ¥Êé•‰ΩøÁî®SAM2Á≠âÈÄöÁî®ÂàÜÂâ≤Ê®°ÂûãÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂà©Áî®Ê∑±Â∫¶‰ø°ÊÅØ‰Ωú‰∏∫ÂÖàÈ™åÔºåÊèêÈ´ò‰∫ÜÂàÜÂâ≤Á≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê∑±Â∫¶ÂºïÂØºÁöÑÁÇπÊèêÁ§∫ÊñπÈù¢ÔºåËÆ∫ÊñáÊ†πÊçÆÊ∑±Â∫¶ÂõæÁöÑÂàÜÂ∏ÉÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÁÇπ‰Ωú‰∏∫ÊèêÁ§∫„ÄÇÂú®ÁâπÂæÅÊèêÂèñÊñπÈù¢ÔºåËÆ∫Êñá‰ΩøÁî®È¢ÑËÆ≠ÁªÉËßÜËßâÊ®°ÂûãÁöÑÊ±†ÂåñÁâπÂæÅÊù•ÊèèËø∞Êé©Á†ÅÂå∫ÂüüÔºå‰ª•ÊèêÈ´òÁâπÂæÅÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®Ê®°ÊùøÂåπÈÖçÊñπÈù¢ÔºåËÆ∫Êñá‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶‰Ωú‰∏∫ÂåπÈÖçÂ∫¶ÈáèÔºåÂπ∂ËÆæÁΩÆÈòàÂÄºÊù•ËøáÊª§‰∏çÂåπÈÖçÁöÑÊé©Á†Å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DepSegÂú®CholecSeg8kÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåmIoUËææÂà∞35.9%ÔºåËøúÈ´ò‰∫éÁõ¥Êé•‰ΩøÁî®SAM2ÁöÑ14.7%„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂç≥‰Ωø‰ªÖ‰ΩøÁî®10-20%ÁöÑÂØπË±°Ê®°ÊùøÔºåDepSeg‰πüËÉΩ‰øùÊåÅÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåËøôË°®ÊòéËØ•ÊñπÊ≥ïÂÖ∑ÊúâÂæàÈ´òÁöÑÊ†áÊ≥®ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËÆ°ÁÆóÊú∫ËæÖÂä©ÊâãÊúØÁ≥ªÁªüÔºå‰æãÂ¶ÇÊú∫Âô®‰∫∫ËæÖÂä©ÊâãÊúØ„ÄÇÈÄöËøáÂÆûÊó∂ÂàÜÂâ≤ÊâãÊúØÂú∫ÊôØÔºåÂèØ‰ª•Â∏ÆÂä©ÂåªÁîüÊõ¥Â•ΩÂú∞ÁêÜËß£ÊâãÊúØËøáÁ®ãÔºåÊèêÈ´òÊâãÊúØÁ≤æÂ∫¶ÂíåÂÆâÂÖ®ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê„ÄÅÊâãÊúØÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüüÔºåÂÖ∑ÊúâÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.

