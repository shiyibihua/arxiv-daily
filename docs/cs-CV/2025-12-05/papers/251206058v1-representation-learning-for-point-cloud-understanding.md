---
layout: default
title: Representation Learning for Point Cloud Understanding
---

# Representation Learning for Point Cloud Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.06058" target="_blank" class="toolbar-btn">arXiv: 2512.06058v1</a>
    <a href="https://arxiv.org/pdf/2512.06058.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06058v1" 
            onclick="toggleFavorite(this, '2512.06058v1', 'Representation Learning for Point Cloud Understanding')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Siming Yan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05

**å¤‡æ³¨**: 181 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§èåˆ2Dé¢„è®­ç»ƒæ¨¡å‹çš„3Dç‚¹äº‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæå‡ç‚¹äº‘ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç‚¹äº‘ç†è§£` `è¡¨ç¤ºå­¦ä¹ ` `2D-3Dè¿ç§»å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `ç‚¹äº‘åˆ†å‰²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dç‚¹äº‘ç†è§£æ–¹æ³•åœ¨ç‰¹å¾æå–å’Œè¯­ä¹‰æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ¨¡å‹çŸ¥è¯†æ¥æŒ‡å¯¼3Dç‚¹äº‘ç½‘ç»œçš„è®­ç»ƒï¼Œä»è€Œæå‡è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‚¹äº‘åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†2DçŸ¥è¯†è¿ç§»çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œ3Dæ•°æ®çš„è·å–å’Œåˆ©ç”¨åœ¨è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººå’Œåœ°ç†ç©ºé—´åˆ†æç­‰é¢†åŸŸæ—¥ç›Šæ™®åŠã€‚é€šè¿‡3Dæ‰«æä»ªã€æ¿€å…‰é›·è¾¾å’ŒRGB-Dç›¸æœºç­‰æ–¹æ³•æ•è·çš„3Dæ•°æ®æä¾›äº†ä¸°å¯Œçš„å‡ ä½•ã€å½¢çŠ¶å’Œå°ºåº¦ä¿¡æ¯ã€‚å½“ä¸2Då›¾åƒç»“åˆæ—¶ï¼Œ3Dæ•°æ®ä½¿æœºå™¨èƒ½å¤Ÿå…¨é¢ç†è§£å…¶ç¯å¢ƒï¼Œä»è€Œæœ‰ç›Šäºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººã€é¥æ„Ÿå’ŒåŒ»ç–—ç­‰åº”ç”¨ã€‚æœ¬è®ºæ–‡ä¾§é‡äºä¸‰ä¸ªä¸»è¦é¢†åŸŸï¼šç‚¹äº‘åŸºå…ƒåˆ†å‰²çš„ç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä»¥åŠä»2Dåˆ°3Dçš„è¿ç§»å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†é¢„è®­ç»ƒçš„2Dæ¨¡å‹æ¥æ”¯æŒ3Dç½‘ç»œè®­ç»ƒï¼Œä»è€Œæ˜¾è‘—æé«˜äº†3Dç†è§£èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯è½¬æ¢2Dæ•°æ®ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å®ƒä»¬é€šè¿‡æœ‰æ•ˆæ•´åˆ2DçŸ¥è¯†æ¥æ¨è¿›ç‚¹äº‘è¡¨ç¤ºå­¦ä¹ çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ç‚¹äº‘ç†è§£æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„3Dæ ‡æ³¨æ•°æ®ï¼Œè€Œè·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼Œç›´æ¥åœ¨3Dæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹å¯èƒ½éš¾ä»¥å……åˆ†åˆ©ç”¨å·²æœ‰çš„2Då›¾åƒçŸ¥è¯†ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨2Då›¾åƒçš„å…ˆéªŒçŸ¥è¯†æ¥æå‡3Dç‚¹äº‘çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é¢„è®­ç»ƒçš„2Dæ¨¡å‹ä½œä¸º3Dç½‘ç»œè®­ç»ƒçš„è¾…åŠ©ä¿¡æ¯æ¥æºã€‚é€šè¿‡æŸç§æ–¹å¼å°†2Dæ¨¡å‹çš„ç‰¹å¾æˆ–çŸ¥è¯†è¿ç§»åˆ°3Dç½‘ç»œä¸­ï¼Œä»è€Œæå‡3Dç½‘ç»œçš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£3Dåœºæ™¯ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥è½¬æ¢2Dæ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨2DçŸ¥è¯†æ¥æŒ‡å¯¼3Dç½‘ç»œçš„å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é¢„è®­ç»ƒçš„2Dæ¨¡å‹ï¼ˆä¾‹å¦‚åœ¨ImageNetä¸Šè®­ç»ƒçš„CNNï¼‰ï¼›2) 3Dç‚¹äº‘ç½‘ç»œï¼ˆä¾‹å¦‚PointNetæˆ–PointNet++ï¼‰ï¼›3) 2D-3DçŸ¥è¯†è¿ç§»æ¨¡å—ï¼Œè´Ÿè´£å°†2Dæ¨¡å‹çš„çŸ¥è¯†ä¼ é€’åˆ°3Dç½‘ç»œä¸­ï¼›4) æŸå¤±å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–3Dç½‘ç»œï¼Œä½¿å…¶æ›´å¥½åœ°åˆ©ç”¨2DçŸ¥è¯†ã€‚å…·ä½“çš„æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„2Dæ¨¡å‹æå–2Då›¾åƒçš„ç‰¹å¾ï¼›ç„¶åï¼Œå°†è¿™äº›ç‰¹å¾é€šè¿‡çŸ¥è¯†è¿ç§»æ¨¡å—ä¼ é€’åˆ°3Dç‚¹äº‘ç½‘ç»œä¸­ï¼›æœ€åï¼Œä½¿ç”¨æŸå¤±å‡½æ•°ä¼˜åŒ–3Dç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£3Dåœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„2D-3DçŸ¥è¯†è¿ç§»æ–¹æ³•ã€‚ä¸ç›´æ¥å°†2Dæ•°æ®è½¬æ¢ä¸º3Dæ•°æ®ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨2Dæ¨¡å‹çš„çŸ¥è¯†æ¥æŒ‡å¯¼3Dç½‘ç»œçš„è®­ç»ƒï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨äº†2Då›¾åƒçš„å…ˆéªŒä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æå‡3Dç‚¹äº‘çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å¤§è§„æ¨¡3Dæ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„çŸ¥è¯†è¿ç§»æ¨¡å—çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬ï¼š1) ç‰¹å¾å¯¹é½ï¼šå°†2Då’Œ3Dç‰¹å¾æ˜ å°„åˆ°åŒä¸€ä¸ªç‰¹å¾ç©ºé—´ï¼›2) æ³¨æ„åŠ›æœºåˆ¶ï¼šåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥é€‰æ‹©æ€§åœ°å…³æ³¨2Dç‰¹å¾ä¸­ä¸3Dåœºæ™¯ç›¸å…³çš„éƒ¨åˆ†ï¼›3) å¯¹æŠ—è®­ç»ƒï¼šä½¿ç”¨å¯¹æŠ—è®­ç»ƒæ¥ä½¿3Dç½‘ç»œå­¦ä¹ åˆ°ä¸2Dæ¨¡å‹ç›¸ä¼¼çš„ç‰¹å¾è¡¨ç¤ºã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬ï¼š1) åˆ†å‰²æŸå¤±ï¼šç”¨äºä¼˜åŒ–ç‚¹äº‘åˆ†å‰²çš„æ€§èƒ½ï¼›2) çŸ¥è¯†è’¸é¦æŸå¤±ï¼šç”¨äºä½¿3Dç½‘ç»œå­¦ä¹ åˆ°2Dæ¨¡å‹çš„çŸ¥è¯†ï¼›3) å¯¹æŠ—æŸå¤±ï¼šç”¨äºä½¿3Dç½‘ç»œå­¦ä¹ åˆ°ä¸2Dæ¨¡å‹ç›¸ä¼¼çš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‚¹äº‘åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œåˆ†å‰²ç²¾åº¦æå‡äº†5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ä¹Ÿè¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººã€é¥æ„Ÿå’ŒåŒ»ç–—ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•æå‡è½¦è¾†å¯¹å‘¨å›´ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•æå‡æœºå™¨äººå¯¹3Dåœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äº¤äº’ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å¯¹åŒ»å­¦å½±åƒè¿›è¡Œåˆ†æï¼Œä»è€Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

