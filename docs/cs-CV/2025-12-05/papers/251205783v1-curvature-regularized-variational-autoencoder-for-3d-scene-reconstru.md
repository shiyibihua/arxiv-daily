---
layout: default
title: Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth
---

# Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.05783" target="_blank" class="toolbar-btn">arXiv: 2512.05783v1</a>
    <a href="https://arxiv.org/pdf/2512.05783.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.05783v1" 
            onclick="toggleFavorite(this, '2512.05783v1', 'Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Maryam Yousefi, Soodeh Bakhshandeh

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Maryousefi/GeoVAE-3D)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ›²ç‡æ­£åˆ™åŒ–VAEï¼Œç”¨äºä»ç¨€ç–æ·±åº¦æ•°æ®é‡å»º3Dåœºæ™¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ä¸‰ç»´é‡å»º` `ç¨€ç–æ·±åº¦æ•°æ®` `å˜åˆ†è‡ªç¼–ç å™¨` `æ›²ç‡æ­£åˆ™åŒ–` `ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¨€ç–æ·±åº¦æ•°æ®ä¸‹é‡å»º3Dåœºæ™¯æ—¶ï¼Œå‡ ä½•è¯¯å·®è¾ƒå¤§ï¼Œéš¾ä»¥æ»¡è¶³è‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºåŸºäºç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„æ›²ç‡æ­£åˆ™åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ŒæŠ‘åˆ¶å™ªå£°å¹¶ç¨³å®šæ¢¯åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨€ç–æ·±åº¦æ•°æ®é‡å»ºä»»åŠ¡ä¸­ï¼Œç²¾åº¦æ¯”æ ‡å‡†VAEæå‡18.1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“æ·±åº¦ä¼ æ„Ÿå™¨ä»…æä¾›æ‰€éœ€æµ‹é‡çš„5%æ—¶ï¼Œé‡å»ºå®Œæ•´çš„3Dåœºæ™¯å˜å¾—éå¸¸å›°éš¾ã€‚è‡ªåŠ¨é©¾é©¶è½¦è¾†å’Œæœºå™¨äººæ— æ³•å®¹å¿ç¨€ç–é‡å»ºå¼•å…¥çš„å‡ ä½•è¯¯å·®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­è¿›è¡Œæ›²ç‡æ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œå®ç°äº†æ¯”æ ‡å‡†å˜åˆ†è‡ªç¼–ç å™¨é«˜18.1%çš„é‡å»ºç²¾åº¦ã€‚æˆ‘ä»¬çš„è´¡çŒ®æŒ‘æˆ˜äº†å‡ ä½•æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªéšå¼å‡è®¾ï¼šå³ç»“åˆå¤šä¸ªå‡ ä½•çº¦æŸå¯ä»¥æé«˜æ€§èƒ½ã€‚ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ­£åˆ™åŒ–é¡¹ä¸ä»…å¯ä»¥åŒ¹é…ï¼Œè€Œä¸”è¶…è¿‡äº†å¤æ‚çš„å¤šé¡¹å¼å…¬å¼çš„æœ‰æ•ˆæ€§ã€‚ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­æä¾›ç¨³å®šçš„æ¢¯åº¦å’Œå™ªå£°æŠ‘åˆ¶ï¼Œä»…éœ€15%çš„è®­ç»ƒå¼€é”€å’Œé›¶æ¨ç†æˆæœ¬ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨https://github.com/Maryousefi/GeoVAE-3D è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»æåº¦ç¨€ç–çš„æ·±åº¦æ•°æ®ï¼ˆä»…å 5%ï¼‰ä¸­å‡†ç¡®é‡å»º3Dåœºæ™¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†è¿™ç§ç¨€ç–æ•°æ®æ—¶ï¼Œå¾€å¾€ä¼šäº§ç”Ÿè¾ƒå¤§çš„å‡ ä½•è¯¯å·®ï¼Œè¿™å¯¹äºéœ€è¦ç²¾ç¡®3Dä¿¡æ¯çš„åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººï¼‰æ¥è¯´æ˜¯ä¸å¯æ¥å—çš„ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„ã€å¤šé¡¹å¼çš„å‡ ä½•çº¦æŸï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ›²ç‡æ­£åˆ™åŒ–æ¥çº¦æŸé‡å»ºçš„3Dåœºæ™¯çš„å¹³æ»‘æ€§ï¼Œä»è€Œå‡å°‘ç”±ç¨€ç–æ•°æ®å¼•èµ·çš„å™ªå£°å’Œè¯¯å·®ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­æ¥ä¼°è®¡æ›²ç‡ï¼Œå¹¶å°†å…¶ä½œä¸ºæ­£åˆ™åŒ–é¡¹æ·»åŠ åˆ°å˜åˆ†è‡ªç¼–ç å™¨çš„æŸå¤±å‡½æ•°ä¸­ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºï¼Œå®ƒå‡è®¾çœŸå®çš„3Dåœºæ™¯é€šå¸¸æ˜¯å¹³æ»‘çš„ï¼Œå› æ­¤å¯ä»¥é€šè¿‡æƒ©ç½šé«˜æ›²ç‡æ¥æé«˜é‡å»ºçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„æ–¹æ³•åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¡†æ¶ã€‚é¦–å…ˆï¼Œå°†ç¨€ç–æ·±åº¦æ•°æ®è¾“å…¥åˆ°ç¼–ç å™¨ä¸­ï¼Œç¼–ç å™¨å°†å…¶æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œä»æ½œåœ¨ç©ºé—´é‡‡æ ·ï¼Œå¹¶ä½¿ç”¨è§£ç å™¨é‡å»º3Dåœºæ™¯ã€‚å…³é”®çš„åŒºåˆ«åœ¨äºï¼Œè®ºæ–‡åœ¨VAEçš„æŸå¤±å‡½æ•°ä¸­æ·»åŠ äº†ä¸€ä¸ªæ›²ç‡æ­£åˆ™åŒ–é¡¹ã€‚è¯¥æ­£åˆ™åŒ–é¡¹åŸºäºç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­è®¡ç®—é‡å»ºåœºæ™¯çš„æ›²ç‡ï¼Œå¹¶æƒ©ç½šé«˜æ›²ç‡ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡æœ€å°åŒ–é‡å»ºè¯¯å·®å’Œæ›²ç‡æ­£åˆ™åŒ–é¡¹çš„åŠ æƒå’Œè¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­è¿›è¡Œæ›²ç‡æ­£åˆ™åŒ–ã€‚ä¸ä¼ ç»Ÿçš„å‡ ä½•çº¦æŸæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ›´åŠ ç®€æ´é«˜æ•ˆï¼Œå¹¶ä¸”èƒ½å¤Ÿæä¾›ç¨³å®šçš„æ¢¯åº¦å’Œå™ªå£°æŠ‘åˆ¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æŒ‘æˆ˜äº†å‡ ä½•æ·±åº¦å­¦ä¹ ä¸­â€œå¤šçº¦æŸä¼˜äºå•çº¦æŸâ€çš„éšå¼å‡è®¾ï¼Œè¯æ˜äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ­£åˆ™åŒ–é¡¹å¯ä»¥è¶…è¶Šå¤æ‚çš„å¤šé¡¹å¼çº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç¦»æ•£æ‹‰æ™®æ‹‰æ–¯ç®—å­æ¥è¿‘ä¼¼æ›²ç‡ï¼Œè¿™ä½¿å¾—è®¡ç®—æ›´åŠ é«˜æ•ˆä¸”æ˜“äºå®ç°ï¼›2) å°†æ›²ç‡æ­£åˆ™åŒ–é¡¹æ·»åŠ åˆ°VAEçš„æŸå¤±å‡½æ•°ä¸­ï¼Œé€šè¿‡è°ƒæ•´æ­£åˆ™åŒ–ç³»æ•°æ¥æ§åˆ¶å¹³æ»‘æ€§çš„å¼ºåº¦ï¼›3) å®éªŒä¸­ï¼Œä½œè€…ä½¿ç”¨äº†ç‰¹å®šçš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒå‚æ•°ï¼Œä½†å…·ä½“ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ã€‚æŸå¤±å‡½æ•°æ˜¯é‡å»ºæŸå¤±å’Œæ›²ç‡æ­£åˆ™åŒ–æŸå¤±çš„åŠ æƒå’Œï¼Œæƒé‡ç³»æ•°éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨€ç–æ·±åº¦æ•°æ®é‡å»ºä»»åŠ¡ä¸­ï¼Œé‡å»ºç²¾åº¦æ¯”æ ‡å‡†å˜åˆ†è‡ªç¼–ç å™¨æé«˜äº†18.1%ã€‚è¯¥æ–¹æ³•ä»…éœ€15%çš„è®­ç»ƒå¼€é”€ï¼Œä¸”åœ¨æ¨ç†é˜¶æ®µæ— éœ€é¢å¤–è®¡ç®—ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚å®éªŒç»“æœéªŒè¯äº†æ›²ç‡æ­£åˆ™åŒ–åœ¨ç¨€ç–æ•°æ®é‡å»ºä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‘æˆ˜äº†å‡ ä½•æ·±åº¦å­¦ä¹ ä¸­å…³äºå¤šçº¦æŸçš„ä¼ ç»Ÿè§‚å¿µã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€ä¸‰ç»´é‡å»ºã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨ç¨€ç–çš„æ¿€å…‰é›·è¾¾æ•°æ®é‡å»ºå‘¨å›´ç¯å¢ƒï¼Œæé«˜è½¦è¾†çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­è¿›è¡Œä¸‰ç»´åœ°å›¾æ„å»ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä»ä¸å®Œæ•´çš„æ‰«ææ•°æ®ä¸­é‡å»ºä¸‰ç»´æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.

