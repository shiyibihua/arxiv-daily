---
layout: default
title: Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection
---

# Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection

**arXiv**: [2510.25094v1](https://arxiv.org/abs/2510.25094) | [PDF](https://arxiv.org/pdf/2510.25094.pdf)

**ä½œè€…**: Chanhyeong Yang, Taehoon Song, Jihwan Park, Hyunwoo J. Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVDRPæ¡†æž¶ä»¥è§£å†³é›¶æ ·æœ¬äºº-ç‰©äº¤äº’æ£€æµ‹ä¸­çš„è§†è§‰å¤šæ ·æ€§å’ŒåŒºåŸŸæ„ŸçŸ¥é—®é¢˜**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `äºº-ç‰©äº¤äº’æ£€æµ‹` `æç¤ºå­¦ä¹ ` `è§†è§‰å¤šæ ·æ€§` `åŒºåŸŸæ„ŸçŸ¥` `CLIPæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›¶æ ·æœ¬HOIæ£€æµ‹ä¸­ï¼ŒåŠ¨è¯ç±»å†…è§†è§‰å¤šæ ·æ€§å’Œç±»é—´è§†è§‰çº ç¼ å¯¼è‡´è¯†åˆ«å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è§†è§‰å¤šæ ·æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ å’ŒåŒºåŸŸç‰¹å®šæ¦‚å¿µæ£€ç´¢ï¼Œå¢žå¼ºæç¤ºåµŒå…¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HICO-DETåŸºå‡†ä¸Šå®žçŽ°SOTAæ€§èƒ½ï¼Œæœ‰æ•ˆå¤„ç†å¤šæ ·æ€§å’Œçº ç¼ é—®é¢˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Zero-shot Human-Object Interaction detection aims to localize humans and
> objects in an image and recognize their interaction, even when specific
> verb-object pairs are unseen during training. Recent works have shown promising
> results using prompt learning with pretrained vision-language models such as
> CLIP, which align natural language prompts with visual features in a shared
> embedding space. However, existing approaches still fail to handle the visual
> complexity of interaction, including (1) intra-class visual diversity, where
> instances of the same verb appear in diverse poses and contexts, and (2)
> inter-class visual entanglement, where distinct verbs yield visually similar
> patterns. To address these challenges, we propose VDRP, a framework for Visual
> Diversity and Region-aware Prompt learning. First, we introduce a visual
> diversity-aware prompt learning strategy that injects group-wise visual
> variance into the context embedding. We further apply Gaussian perturbation to
> encourage the prompts to capture diverse visual variations of a verb. Second,
> we retrieve region-specific concepts from the human, object, and union regions.
> These are used to augment the diversity-aware prompt embeddings, yielding
> region-aware prompts that enhance verb-level discrimination. Experiments on the
> HICO-DET benchmark demonstrate that our method achieves state-of-the-art
> performance under four zero-shot evaluation settings, effectively addressing
> both intra-class diversity and inter-class visual entanglement. Code is
> available at https://github.com/mlvlab/VDRP.

