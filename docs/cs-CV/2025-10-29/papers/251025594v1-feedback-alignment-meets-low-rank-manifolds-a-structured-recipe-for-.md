---
layout: default
title: Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning
---

# Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning

**arXiv**: [2510.25594v1](https://arxiv.org/abs/2510.25594) | [PDF](https://arxiv.org/pdf/2510.25594.pdf)

**ä½œè€…**: Arani Roy, Marco P. Apolinario, Shristi Das Biswas, Kaushik Roy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽSVDä½Žç§©æµå½¢çš„ç»“æž„åŒ–å±€éƒ¨å­¦ä¹ æ¡†æž¶ï¼Œä»¥è§£å†³ç›´æŽ¥åé¦ˆå¯¹é½çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `å±€éƒ¨å­¦ä¹ ` `ä½Žç§©æµå½¢` `ç›´æŽ¥åé¦ˆå¯¹é½` `å¥‡å¼‚å€¼åˆ†è§£` `ç»“æž„åŒ–è®­ç»ƒ` `ç¥žç»ç½‘ç»œä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›´æŽ¥åé¦ˆå¯¹é½åœ¨æ·±å±‚ç½‘ç»œä¸­å› éžç»“æž„åŒ–åé¦ˆå’Œå¯æ‰©å±•æ€§å·®è€Œå—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨SVDåˆ†è§£æƒé‡ä¸Šåº”ç”¨å¤åˆæŸå¤±ï¼Œæž„å»ºç»“æž„åŒ–åé¦ˆçŸ©é˜µå®žçŽ°å±€éƒ¨æ›´æ–°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFARå’ŒImageNetä¸Šè¾¾åˆ°ä¸Žåå‘ä¼ æ’­ç›¸å½“çš„ç²¾åº¦ï¼Œå‡å°‘å¯è®­ç»ƒå‚æ•°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Training deep neural networks (DNNs) with backpropagation (BP) achieves
> state-of-the-art accuracy but requires global error propagation and full
> parameterization, leading to substantial memory and computational overhead.
> Direct Feedback Alignment (DFA) enables local, parallelizable updates with
> lower memory requirements but is limited by unstructured feedback and poor
> scalability in deeper architectures, specially convolutional neural networks.
> To address these limitations, we propose a structured local learning framework
> that operates directly on low-rank manifolds defined by the Singular Value
> Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
> form, with updates applied to the SVD components using a composite loss that
> integrates cross-entropy, subspace alignment, and orthogonality regularization.
> Feedback matrices are constructed to match the SVD structure, ensuring
> consistent alignment between forward and feedback pathways. Our method reduces
> the number of trainable parameters relative to the original DFA model, without
> relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
> and ImageNet show that our method achieves accuracy comparable to that of BP.
> Ablation studies confirm the importance of each loss term in the low-rank
> setting. These results establish local learning on low-rank manifolds as a
> principled and scalable alternative to full-rank gradient-based training.

