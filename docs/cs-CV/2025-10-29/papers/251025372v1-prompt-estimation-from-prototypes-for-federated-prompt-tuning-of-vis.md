---
layout: default
title: Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers
---

# Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers

**arXiv**: [2510.25372v1](https://arxiv.org/abs/2510.25372) | [PDF](https://arxiv.org/pdf/2510.25372.pdf)

**ä½œè€…**: M Yashwanth, Sharannya Ghosh, Aditay Tripathi, Anirban Chakraborty

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPEP-FedPTæ¡†æž¶ä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­è§†è§‰Transformeræç¤ºè°ƒä¼˜çš„æ³›åŒ–ä¸Žä¸ªæ€§åŒ–å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `è§†è§‰Transformer` `æç¤ºè°ƒä¼˜` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `æ•°æ®å¼‚æž„æ€§` `ä¸ªæ€§åŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè”é‚¦å­¦ä¹ ä¸­å…¨å±€æç¤ºè°ƒä¼˜æ³›åŒ–å·®ï¼Œä¸ªæ€§åŒ–è°ƒä¼˜æ˜“è¿‡æ‹Ÿåˆä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ç±»ä¸Šä¸‹æ–‡æ··åˆæç¤ºï¼Œç»“åˆå…¨å±€å…±äº«æç¤ºå’Œç±»ç‰¹å®šæç¤ºï¼Œå®žçŽ°æ¯æ ·æœ¬ä¸ªæ€§åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œé€‚åº”æ•°æ®å¼‚æž„åœºæ™¯ï¼Œæå‡æ•ˆçŽ‡å’Œæ³›åŒ–æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
> proven highly effective as a parameter-efficient fine-tuning technique for
> adapting large models to downstream tasks with limited data. Its parameter
> efficiency makes it particularly suitable for Federated Learning (FL), where
> both communication and computation budgets are often constrained. However,
> global prompt tuning struggles to generalize across heterogeneous clients,
> while personalized tuning overfits to local data and lacks generalization. We
> propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
> Tuning), a unified framework designed to achieve both generalization and
> personalization in federated prompt tuning of ViTs. Within this framework, we
> introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
> class-specific prompts maintained alongside a globally shared prompt. For each
> input, CCMP adaptively combines class-specific prompts using weights derived
> from global class prototypes and client class priors. This approach enables
> per-sample prompt personalization without storing client-dependent trainable
> parameters. The prompts are collaboratively optimized via traditional federated
> averaging technique on the same. Comprehensive evaluations on CIFAR-100,
> TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
> consistently surpasses the state-of-the-art baselines under diverse data
> heterogeneity scenarios, establishing a strong foundation for efficient and
> generalizable federated prompt tuning of Vision Transformers.

