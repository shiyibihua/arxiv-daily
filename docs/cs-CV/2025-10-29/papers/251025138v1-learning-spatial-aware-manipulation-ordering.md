---
layout: default
title: Learning Spatial-Aware Manipulation Ordering
---

# Learning Spatial-Aware Manipulation Ordering

**arXiv**: [2510.25138v1](https://arxiv.org/abs/2510.25138) | [PDF](https://arxiv.org/pdf/2510.25138.pdf)

**ä½œè€…**: Yuxiang Yan, Zhiyuan Zhou, Xin Gao, Guanghao Li, Shenglin Li, Jiaqi Chen, Qunyan Pu, Jian Pu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOrderMindæ¡†æž¶ä»¥è§£å†³æ‚ä¹±çŽ¯å¢ƒä¸­ç‰©ä½“æ“ä½œé¡ºåºé—®é¢˜**

**å…³é”®è¯**: `æ“ä½œé¡ºåºå­¦ä¹ ` `ç©ºé—´æ„ŸçŸ¥` `å›¾ç¥žç»ç½‘ç»œ` `è’¸é¦è®­ç»ƒ` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‚ä¹±çŽ¯å¢ƒä¸­ç‰©ä½“ç©ºé—´ä¾èµ–å¯¼è‡´æ“ä½œé¡ºåºä¸å½“ï¼Œå¼•å‘ç¢°æ’žæˆ–é˜»å¡ž
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆç©ºé—´ä¸Šä¸‹æ–‡ç¼–ç å™¨å’Œæ—¶é—´ä¼˜å…ˆçº§æ¨¡å—ï¼Œå­¦ä¹ åŸºäºŽç©ºé—´ä¸Šä¸‹æ–‡çš„æ“ä½œä¼˜å…ˆçº§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žçŽ¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡æ“ä½œæœ‰æ•ˆæ€§å’Œæ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Manipulation in cluttered environments is challenging due to spatial
> dependencies among objects, where an improper manipulation order can cause
> collisions or blocked access. Existing approaches often overlook these spatial
> relationships, limiting their flexibility and scalability. To address these
> limitations, we propose OrderMind, a unified spatial-aware manipulation
> ordering framework that directly learns object manipulation priorities based on
> spatial context. Our architecture integrates a spatial context encoder with a
> temporal priority structuring module. We construct a spatial graph using
> k-Nearest Neighbors to aggregate geometric information from the local layout
> and encode both object-object and object-manipulator interactions to support
> accurate manipulation ordering in real-time. To generate physically and
> semantically plausible supervision signals, we introduce a spatial prior
> labeling method that guides a vision-language model to produce reasonable
> manipulation orders for distillation. We evaluate OrderMind on our Manipulation
> Ordering Benchmark, comprising 163,222 samples of varying difficulty. Extensive
> experiments in both simulation and real-world environments demonstrate that our
> method significantly outperforms prior approaches in effectiveness and
> efficiency, enabling robust manipulation in cluttered scenes.

