---
layout: default
title: $D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction
---

# $D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction

**arXiv**: [2510.25173v1](https://arxiv.org/abs/2510.25173) | [PDF](https://arxiv.org/pdf/2510.25173.pdf)

**ä½œè€…**: Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD^2GSæ¡†æž¶ï¼Œç”¨äºŽæ— LiDARåŸŽå¸‚åœºæ™¯é‡å»ºï¼Œé€šè¿‡å¯†é›†æ·±åº¦æ­£åˆ™åŒ–æå‡å‡ ä½•ç²¾åº¦**

**å…³é”®è¯**: `åŸŽå¸‚åœºæ™¯é‡å»º` `é«˜æ–¯æ³¼æº…` `æ·±åº¦æ­£åˆ™åŒ–` `æ— LiDARé‡å»º` `æ‰©æ•£æ¨¡åž‹` `å‡ ä½•ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸŽå¸‚é‡å»ºæ–¹æ³•ä¾èµ–LiDARï¼Œä½†èŽ·å–å‡†ç¡®LiDARæ•°æ®å­˜åœ¨æ ¡å‡†å’Œç©ºé—´å¯¹é½å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤šè§†è§’æ·±åº¦é¢„æµ‹åˆå§‹åŒ–ç‚¹äº‘ï¼Œç»“åˆæ‰©æ•£å…ˆéªŒå¢žå¼ºæ·±åº¦ï¼Œä¼˜åŒ–é«˜æ–¯å‡ ä½•
3. å®žéªŒæ•ˆæžœï¼šåœ¨Waymoæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå‡ ä½•ç²¾åº¦é«˜ï¼Œç”šè‡³è¶…è¶Šä½¿ç”¨çœŸå®žLiDARçš„æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Gaussian Splatting (GS) has shown great potential for urban scene
> reconstruction in the field of autonomous driving. However, current urban scene
> reconstruction methods often depend on multimodal sensors as inputs,
> \textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR
> point clouds can largely mitigate ill-posedness in reconstruction, acquiring
> such accurate LiDAR data is still challenging in practice: i) precise
> spatiotemporal calibration between LiDAR and other sensors is required, as they
> may not capture data simultaneously; ii) reprojection errors arise from spatial
> misalignment when LiDAR and cameras are mounted at different locations. To
> avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a
> LiDAR-free urban scene reconstruction framework. In this work, we obtain
> geometry priors that are as effective as LiDAR while being denser and more
> accurate. $\textbf{First}$, we initialize a dense point cloud by
> back-projecting multi-view metric depth predictions. This point cloud is then
> optimized by a Progressive Pruning strategy to improve the global consistency.
> $\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense
> metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors
> from a depth foundation model to enhance the depth maps rendered by Gaussians.
> In turn, the enhanced depths provide stronger geometric constraints during
> Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground
> geometry by constraining the shape and normal attributes of Gaussians within
> road regions. Extensive experiments on the Waymo dataset demonstrate that our
> method consistently outperforms state-of-the-art methods, producing more
> accurate geometry even when compared with those using ground-truth LiDAR data.

