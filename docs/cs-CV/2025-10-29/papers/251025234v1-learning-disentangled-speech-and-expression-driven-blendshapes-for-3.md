---
layout: default
title: Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation
---

# Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation

**arXiv**: [2510.25234v1](https://arxiv.org/abs/2510.25234) | [PDF](https://arxiv.org/pdf/2510.25234.pdf)

**ä½œè€…**: Yuxiang Mao, Zhijie Zhang, Zhiheng Zhang, Jiawei Liu, Chen Zeng, Shihong Xia

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦è¯­éŸ³ä¸Žè¡¨æƒ…é©±åŠ¨çš„æ··åˆå½¢çŠ¶æ–¹æ³•ï¼Œä»¥ç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œçš„3Dè¯´è¯äººè„¸åŠ¨ç”»ã€‚**

**å…³é”®è¯**: `3Dé¢éƒ¨åŠ¨ç”»` `è¯­éŸ³é©±åŠ¨åŠ¨ç”»` `è¡¨æƒ…è§£è€¦` `æ··åˆå½¢çŠ¶å­¦ä¹ ` `ç¨€ç–çº¦æŸ` `FLAMEæ¨¡åž‹æ˜ å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹çœŸå®žæƒ…æ„Ÿ3Dè¯´è¯äººè„¸æ•°æ®ï¼Œéš¾ä»¥ç”Ÿæˆæƒ…æ„Ÿè¡¨è¾¾çš„é¢éƒ¨åŠ¨ç”»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨çº¿æ€§åŠ æ³•å»ºæ¨¡ï¼Œç»“åˆç¨€ç–çº¦æŸæŸå¤±å­¦ä¹ è§£è€¦çš„è¯­éŸ³å’Œè¡¨æƒ…æ··åˆå½¢çŠ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡æ„ŸçŸ¥ç ”ç©¶éªŒè¯ï¼Œåœ¨ä¿æŒå”‡åŒæ­¥å‡†ç¡®æ€§çš„åŒæ—¶æå‡æƒ…æ„Ÿè¡¨è¾¾æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Expressions are fundamental to conveying human emotions. With the rapid
> advancement of AI-generated content (AIGC), realistic and expressive 3D facial
> animation has become increasingly crucial. Despite recent progress in
> speech-driven lip-sync for talking-face animation, generating emotionally
> expressive talking faces remains underexplored. A major obstacle is the
> scarcity of real emotional 3D talking-face datasets due to the high cost of
> data capture. To address this, we model facial animation driven by both speech
> and emotion as a linear additive problem. Leveraging a 3D talking-face dataset
> with neutral expressions (VOCAset) and a dataset of 3D expression sequences
> (Florence4D), we jointly learn a set of blendshapes driven by speech and
> emotion. We introduce a sparsity constraint loss to encourage disentanglement
> between the two types of blendshapes while allowing the model to capture
> inherent secondary cross-domain deformations present in the training data. The
> learned blendshapes can be further mapped to the expression and jaw pose
> parameters of the FLAME model, enabling the animation of 3D Gaussian avatars.
> Qualitative and quantitative experiments demonstrate that our method naturally
> generates talking faces with specified expressions while maintaining accurate
> lip synchronization. Perceptual studies further show that our approach achieves
> superior emotional expressivity compared to existing methods, without
> compromising lip-sync quality.

