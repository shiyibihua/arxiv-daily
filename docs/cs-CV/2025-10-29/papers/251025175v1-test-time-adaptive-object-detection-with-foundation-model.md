---
layout: default
title: Test-Time Adaptive Object Detection with Foundation Model
---

# Test-Time Adaptive Object Detection with Foundation Model

**arXiv**: [2510.25175v1](https://arxiv.org/abs/2510.25175) | [PDF](https://arxiv.org/pdf/2510.25175.pdf)

**ä½œè€…**: Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŸºç¡€æ¨¡åž‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€æºæ•°æ®å¹¶é€‚åº”ä»»æ„åŸŸå’Œç±»åˆ«ã€‚**

**å…³é”®è¯**: `æµ‹è¯•æ—¶è‡ªé€‚åº”` `ç›®æ ‡æ£€æµ‹` `åŸºç¡€æ¨¡åž‹` `å¤šæ¨¡æ€æç¤º` `ä¼ªæ ‡ç­¾å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–æºæ•°æ®ç»Ÿè®¡å’Œé—­é›†å‡è®¾ï¼Œä¸é€‚åº”çœŸå®žä¸–ç•Œè·¨åŸŸè·¨ç±»åˆ«åœºæ™¯ã€‚
2. é‡‡ç”¨å¤šæ¨¡æ€æç¤ºå‡å€¼æ•™å¸ˆæ¡†æž¶ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒä¼˜ï¼Œé«˜æ•ˆé€‚åº”æµ‹è¯•æ•°æ®ã€‚
3. å®žéªŒæ˜¾ç¤ºåœ¨è·¨æŸåå’Œè·¨æ•°æ®é›†åŸºå‡†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä»£ç å·²å¼€æºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, test-time adaptive object detection has attracted increasing
> attention due to its unique advantages in online domain adaptation, which
> aligns more closely with real-world application scenarios. However, existing
> approaches heavily rely on source-derived statistical characteristics while
> making the strong assumption that the source and target domains share an
> identical category space. In this paper, we propose the first foundation
> model-powered test-time adaptive object detection method that eliminates the
> need for source data entirely and overcomes traditional closed-set limitations.
> Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for
> vision-language detector-driven test-time adaptation, which incorporates text
> and visual prompt tuning to adapt both language and vision representation
> spaces on the test data in a parameter-efficient manner. Correspondingly, we
> propose a Test-time Warm-start strategy tailored for the visual prompts to
> effectively preserve the representation capability of the vision branch.
> Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
> maintain an Instance Dynamic Memory (IDM) module that stores high-quality
> pseudo-labels from previous test samples, and propose two novel
> strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
> high-quality instances for enhancing original predictions and hallucinating
> images without available pseudo-labels, respectively. Extensive experiments on
> cross-corruption and cross-dataset benchmarks demonstrate that our method
> consistently outperforms previous state-of-the-art methods, and can adapt to
> arbitrary cross-domain and cross-category target data. Code is available at
> https://github.com/gaoyingjay/ttaod_foundation.

