---
layout: default
title: NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies
---

# NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies

**arXiv**: [2510.25122v1](https://arxiv.org/abs/2510.25122) | [PDF](https://arxiv.org/pdf/2510.25122.pdf)

**ä½œè€…**: Jiahong Chen, Jing Wang, Long Chen, Chuwei Cai, Jinghui Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNanoVLAè½»é‡æž¶æž„ä»¥è§£å†³èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹éƒ¨ç½²æŒ‘æˆ˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `è½»é‡æž¶æž„` `è¾¹ç¼˜è®¡ç®—` `æŽ¨ç†ä¼˜åŒ–` `æœºå™¨äººæ“ä½œ` `åŠ¨æ€è·¯ç”±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ï¼Œè®¡ç®—éœ€æ±‚é«˜ï¼Œå½±å“å®žæ—¶æ€§å’Œèµ„æºæ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è§†è§‰-è¯­è¨€è§£è€¦ã€é•¿çŸ­æœŸåŠ¨ä½œåˆ†å—å’ŒåŠ¨æ€è·¯ç”±ï¼Œä¼˜åŒ–æŽ¨ç†æ•ˆçŽ‡å’Œæ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®žçŽ°é«˜è¾¾52å€åŠ é€Ÿï¼Œå‚æ•°å‡å°‘98%ï¼Œä¿æŒæˆ–è¶…è¶Šä»»åŠ¡ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models have significantly advanced robotic
> manipulation by integrating vision-language models (VLMs), and action decoders
> into a unified architecture. However, their deployment on resource-constrained
> edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin
> Nano), remains challenging due to high computational demands, especially in
> real-world scenarios where power, latency, and computational resources are
> critical. To close this gap, we introduce Nano-scale Vision-Language Action
> (NanoVLA), a family of lightweight VLA architectures that achieve high
> performance with minimal resources. Our core innovations include: (1)
> vision-language decoupling that moves conventional early vision and language
> inputs fusion in VLM to late stage, achieving better performance while enabling
> caching and reduce inference overhead and latency; (2) long-short action
> chunking to ensure smooth, coherent multi-step planning without sacrificing
> real-time responsiveness; (3) dynamic routing that adaptively assigns
> lightweight or heavy backbones based on task complexity, further optimizing
> inference efficiency. Experimental results on several benchmarks, as well as
> real-world deployments, demonstrate that NanoVLA achieves up to 52x faster
> inference on edge devices compared to previous state-of-the-art VLA models,
> with 98% less parameters while maintaining or surpassing their task accuracy
> and generalization. Ablation studies confirm that our decoupling strategy
> preserves cross-task transferability, and the routing module enhances
> cost-performance trade-offs, enabling practical, high-precision robotic
> manipulation on resource-constrained hardware.

