---
layout: default
title: Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization
---

# Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization

**arXiv**: [2510.25616v1](https://arxiv.org/abs/2510.25616) | [PDF](https://arxiv.org/pdf/2510.25616.pdf)

**ä½œè€…**: Nikita Kachaev, Mikhail Kolosov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰è¡¨ç¤ºå¯¹é½æ–¹æ³•ä»¥ç¼“è§£VLAå¾®è°ƒä¸­çš„è§†è§‰è¡¨ç¤ºé€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `è¡¨ç¤ºå¯¹é½` `å¾®è°ƒé€€åŒ–` `OODæ³›åŒ–` `è§†è§‰è¡¨ç¤ºåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨åŠ¨ä½œå¾®è°ƒä¸­è§†è§‰è¡¨ç¤ºé€€åŒ–ï¼Œå½±å“æ³›åŒ–èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å¯¹é½ç­–ç•¥ï¼Œå¯¹æ¯”VLAä¸ŽVLMï¼Œä¿ç•™è§†è§‰è¯­è¨€èƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°OODæ³›åŒ–ï¼Œæ–¹æ³•æœ‰æ•ˆç¼“è§£é€€åŒ–å¹¶æå‡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The growing success of Vision-Language-Action (VLA) models stems from the
> promise that pretrained Vision-Language Models (VLMs) can endow agents with
> transferable world knowledge and vision-language (VL) grounding, laying a
> foundation for action models with broader generalization. Yet when these VLMs
> are adapted to the action modality, it remains unclear to what extent their
> original VL representations and knowledge are preserved. In this work, we
> conduct a systematic study of representation retention during VLA fine-tuning,
> showing that naive action fine-tuning leads to degradation of visual
> representations. To characterize and measure these effects, we probe VLA's
> hidden representations and analyze attention maps, further, we design a set of
> targeted tasks and methods that contrast VLA models with their counterpart
> VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
> further evaluate a range of strategies for aligning visual representations and
> introduce a simple yet effective method that mitigates degradation and yields
> improved generalization to out-of-distribution (OOD) scenarios. Taken together,
> our analysis clarifies the trade-off between action fine-tuning and the
> degradation of VL representations and highlights practical approaches to
> recover inherited VL capabilities. Code is publicly available:
> https://blind-vla-paper.github.io

