---
layout: default
title: Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models
---

# Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models

**arXiv**: [2510.20707v1](https://arxiv.org/abs/2510.20707) | [PDF](https://arxiv.org/pdf/2510.20707.pdf)

**ä½œè€…**: Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMixKVæ–¹æ³•ä»¥ä¼˜åŒ–å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­çš„KVç¼“å­˜åŽ‹ç¼©é—®é¢˜**

**å…³é”®è¯**: `KVç¼“å­˜åŽ‹ç¼©` `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€å†—ä½™` `é‡è¦æ€§å¤šæ ·æ€§å¹³è¡¡` `å¤´çº§è‡ªé€‚åº”` `å†…å­˜ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šKVç¼“å­˜æ‰©å±•å¯¼è‡´å†…å­˜ç“¶é¢ˆï¼ŒçŽ°æœ‰æ–¹æ³•å¿½ç•¥å¤šæ¨¡æ€è¯­ä¹‰å†—ä½™æ¨¡å¼
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆé‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œè‡ªé€‚åº”å¤´çº§è¯­ä¹‰å†—ä½™è¿›è¡ŒKVå¯¹åŽ‹ç¼©
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æžç«¯åŽ‹ç¼©ä¸‹æå‡åŸºå‡†æ–¹æ³•5.1%ï¼ŒGUIä»»åŠ¡å¢žç›Šè¾¾8.0-9.0%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent large vision-language models (LVLMs) demonstrate remarkable
> capabilities in processing extended multi-modal sequences, yet the resulting
> key-value (KV) cache expansion creates a critical memory bottleneck that
> fundamentally limits deployment scalability. While existing KV cache
> compression methods focus on retaining high-importance KV pairs to minimize
> storage, they often overlook the modality-specific semantic redundancy patterns
> that emerge distinctively in multi-modal KV caches. In this work, we first
> analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying
> levels of redundancy across attention heads. We show that relying solely on
> importance can only cover a subset of the full KV cache information
> distribution, leading to potential loss of semantic coverage. To address this,
> we propose \texttt{MixKV}, a novel method that mixes importance with diversity
> for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise
> semantic redundancy, selectively balancing diversity and importance when
> compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}
> consistently enhances existing methods across multiple LVLMs. Under extreme
> compression (budget=64), \texttt{MixKV} improves baseline methods by an average
> of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves
> remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on
> GUI grounding tasks, all while maintaining comparable inference efficiency.
> Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable
> performance gains. Our code is available at
> \href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.

