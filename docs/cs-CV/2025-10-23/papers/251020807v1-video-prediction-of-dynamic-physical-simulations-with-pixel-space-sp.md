---
layout: default
title: Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers
---

# Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers

**arXiv**: [2510.20807v1](https://arxiv.org/abs/2510.20807) | [PDF](https://arxiv.org/pdf/2510.20807.pdf)

**ä½œè€…**: Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåƒç´ ç©ºé—´æ—¶ç©ºTransformeræ¨¡åž‹ï¼Œä»¥æå‡åŠ¨æ€ç‰©ç†æ¨¡æ‹Ÿè§†é¢‘é¢„æµ‹çš„é•¿æœŸå‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `è§†é¢‘é¢„æµ‹` `æ—¶ç©ºTransformer` `ç‰©ç†æ¨¡æ‹Ÿ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `åƒç´ ç©ºé—´è¡¨ç¤º` `å¯è§£é‡Šæ€§åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨ç‰©ç†æ¨¡æ‹Ÿçš„å› æžœå»ºæ¨¡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å®žçŽ°é•¿æœŸå‡†ç¡®é¢„æµ‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç®€å•ç«¯åˆ°ç«¯Transformeræž¶æž„ï¼Œæ¯”è¾ƒä¸åŒæ—¶ç©ºè‡ªæ³¨æ„åŠ›å¸ƒå±€ï¼Œæ— éœ€å¤æ‚è®­ç»ƒç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”æ½œåœ¨ç©ºé—´æ–¹æ³•ï¼Œç‰©ç†å‡†ç¡®é¢„æµ‹æ—¶é—´å»¶é•¿è¾¾50%ï¼Œå¹¶ä¿æŒè§†é¢‘è´¨é‡æŒ‡æ ‡å¯æ¯”æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Inspired by the performance and scalability of autoregressive large language
> models (LLMs), transformer-based models have seen recent success in the visual
> domain. This study investigates a transformer adaptation for video prediction
> with a simple end-to-end approach, comparing various spatiotemporal
> self-attention layouts. Focusing on causal modeling of physical simulations
> over time; a common shortcoming of existing video-generative approaches, we
> attempt to isolate spatiotemporal reasoning via physical object tracking
> metrics and unsupervised training on physical simulation datasets. We introduce
> a simple yet effective pure transformer model for autoregressive video
> prediction, utilizing continuous pixel-space representations for video
> prediction. Without the need for complex training strategies or latent
> feature-learning components, our approach significantly extends the time
> horizon for physically accurate predictions by up to 50% when compared with
> existing latent-space approaches, while maintaining comparable performance on
> common video quality metrics. In addition, we conduct interpretability
> experiments to identify network regions that encode information useful to
> perform accurate estimations of PDE simulation parameters via probing models,
> and find that this generalizes to the estimation of out-of-distribution
> simulation parameters. This work serves as a platform for further
> attention-based spatiotemporal modeling of videos via a simple, parameter
> efficient, and interpretable approach.

