---
layout: default
title: VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation
---

# VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation

**arXiv**: [2510.20818v1](https://arxiv.org/abs/2510.20818) | [PDF](https://arxiv.org/pdf/2510.20818.pdf)

**ä½œè€…**: Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVAMOSåˆ†å±‚æ¨¡åž‹ï¼Œé€šè¿‡è§£è€¦è¯­ä¹‰è§„åˆ’ä¸Žå…·èº«æŽ¥åœ°ï¼Œå®žçŽ°è·¨å…·èº«å¯¼èˆª**

**å…³é”®è¯**: `æœºå™¨äººå¯¼èˆª` `åˆ†å±‚è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `å…·èº«æŽ¥åœ°` `è·¨å…·èº«æ³›åŒ–` `è‡ªç„¶è¯­è¨€å¼•å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººå¯¼èˆªéœ€æ³›åŒ–å¤šçŽ¯å¢ƒå¹¶é€‚åº”ç‰¹å®šç‰©ç†çº¦æŸï¼Œå¦‚å››è¶³ä¸Žè½®å¼æœºå™¨äººèƒ½åŠ›å·®å¼‚
2. æ–¹æ³•è¦ç‚¹ï¼šé«˜å±‚è§„åˆ’å™¨å­¦ä¹ å¼€æ”¾ä¸–ç•Œæ•°æ®ï¼Œä½Žå±‚å…·èº«æ¨¡åž‹åœ¨å®‰å…¨æ¨¡æ‹Ÿä¸­å­¦ä¹ ç‰©ç†çº¦æŸ
3. å®žéªŒæ•ˆæžœï¼šåœ¨å®¤å†…å¤–å¯¼èˆªä¸­æˆåŠŸçŽ‡æ›´é«˜ï¼Œæ”¯æŒè·¨å…·èº«éƒ¨ç½²å¹¶æå‡å•æœºå™¨äººå¯é æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A fundamental challenge in robot navigation lies in learning policies that
> generalize across diverse environments while conforming to the unique physical
> constraints and capabilities of a specific embodiment (e.g., quadrupeds can
> walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that
> decouples semantic planning from embodiment grounding: a generalist planner
> learns from diverse, open-world data, while a specialist affordance model
> learns the robot's physical constraints and capabilities in safe, low-cost
> simulation. We enabled this separation by carefully designing an interface that
> lets a high-level planner propose candidate paths directly in image space that
> the affordance model then evaluates and re-ranks. Our real-world experiments
> show that VAMOS achieves higher success rates in both indoor and complex
> outdoor navigation than state-of-the-art model-based and end-to-end learning
> methods. We also show that our hierarchical design enables cross-embodied
> navigation across legged and wheeled robots and is easily steerable using
> natural language. Real-world ablations confirm that the specialist model is key
> to embodiment grounding, enabling a single high-level planner to be deployed
> across physically distinct wheeled and legged robots. Finally, this model
> significantly enhances single-robot reliability, achieving 3X higher success
> rates by rejecting physically infeasible plans. Website:
> https://vamos-vla.github.io/

