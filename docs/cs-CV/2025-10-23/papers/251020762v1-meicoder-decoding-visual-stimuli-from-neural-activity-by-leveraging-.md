---
layout: default
title: MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs
---

# MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs

**arXiv**: [2510.20762v1](https://arxiv.org/abs/2510.20762) | [PDF](https://arxiv.org/pdf/2510.20762.pdf)

**ä½œè€…**: Jan Sobotka, Luca Baroni, JÃ¡n AntolÃ­k

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMEIcoderæ–¹æ³•ï¼Œåˆ©ç”¨æœ€å…´å¥‹è¾“å…¥è§£ç è§†è§‰åˆºæ¿€ï¼Œè§£å†³å°æ•°æ®é›†ä¸‹ç¥žç»æ´»åŠ¨è§£ç éš¾é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰åˆºæ¿€è§£ç ` `ç¥žç»æ´»åŠ¨åˆ†æž` `å°æ•°æ®é›†å­¦ä¹ ` `å¯¹æŠ—è®­ç»ƒ` `ç»“æž„ç›¸ä¼¼æ€§æŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçµé•¿ç±»æˆ–äººç±»ç¥žç»æ•°æ®ç¨€ç¼ºï¼Œé™åˆ¶æ·±åº¦å­¦ä¹ è§£ç è§†è§‰åˆºæ¿€çš„åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæœ€å…´å¥‹è¾“å…¥ã€ç»“æž„ç›¸ä¼¼æ€§æŸå¤±å’Œå¯¹æŠ—è®­ç»ƒï¼Œæå‡è§£ç æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨V1åŒºå•ç»†èƒžæ´»åŠ¨è§£ç ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œæ”¯æŒå°‘é‡ç¥žç»å…ƒå’Œè®­ç»ƒæ•°æ®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Decoding visual stimuli from neural population activity is crucial for
> understanding the brain and for applications in brain-machine interfaces.
> However, such biological data is often scarce, particularly in primates or
> humans, where high-throughput recording techniques, such as two-photon imaging,
> remain challenging or impossible to apply. This, in turn, poses a challenge for
> deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
> biologically informed decoding method that leverages neuron-specific most
> exciting inputs (MEIs), a structural similarity index measure loss, and
> adversarial training. MEIcoder achieves state-of-the-art performance in
> reconstructing visual stimuli from single-cell activity in primary visual
> cortex (V1), especially excelling on small datasets with fewer recorded
> neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
> of the performance, and in scaling experiments, we show that MEIcoder can
> reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
> neurons and less than 1,000 training data points. We also propose a unified
> benchmark with over 160,000 samples to foster future research. Our results
> demonstrate the feasibility of reliable decoding in early visual system and
> provide practical insights for neuroscience and neuroengineering applications.

