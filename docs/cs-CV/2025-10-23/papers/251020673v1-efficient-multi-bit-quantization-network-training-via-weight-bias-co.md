---
layout: default
title: Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling
---

# Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling

**arXiv**: [2510.20673v1](https://arxiv.org/abs/2510.20673) | [PDF](https://arxiv.org/pdf/2510.20673.pdf)

**ä½œè€…**: Jinhee Kim, Jae Jun An, Kang Eun Jeon, Jong Hwan Ko

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæƒé‡åç½®æ ¡æ­£ä¸Žä½çº§æ ¸å¿ƒé›†é‡‡æ ·ä»¥é™ä½Žå¤šæ¯”ç‰¹é‡åŒ–ç½‘ç»œè®­ç»ƒå¼€é”€**

**å…³é”®è¯**: `å¤šæ¯”ç‰¹é‡åŒ–` `æƒé‡åç½®æ ¡æ­£` `æ ¸å¿ƒé›†é‡‡æ ·` `è®­ç»ƒæ•ˆçŽ‡` `ç¥žç»ç½‘ç»œéƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¯”ç‰¹é‡åŒ–ç½‘ç»œè®­ç»ƒå¼€é”€å¤§ï¼Œéœ€é‡å¤å…¨æ•°æ®é›†æ›´æ–°å’Œé¢å¤–å¾®è°ƒ
2. æƒé‡åç½®æ ¡æ­£å¯¹é½æ¿€æ´»åˆ†å¸ƒï¼Œä½çº§æ ¸å¿ƒé›†é‡‡æ ·åˆ©ç”¨æ¢¯åº¦é‡è¦æ€§é€‰æ‹©å­é›†
3. åœ¨å¤šä¸ªæ•°æ®é›†å’Œæž¶æž„ä¸ŠéªŒè¯ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘è¾¾7.88å€ï¼Œç²¾åº¦ä¿æŒæˆ–æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-bit quantization networks enable flexible deployment of deep neural
> networks by supporting multiple precision levels within a single model.
> However, existing approaches suffer from significant training overhead as
> full-dataset updates are repeated for each supported bit-width, resulting in a
> cost that scales linearly with the number of precisions. Additionally, extra
> fine-tuning stages are often required to support additional or intermediate
> precision options, further compounding the overall training burden. To address
> this issue, we propose two techniques that greatly reduce the training overhead
> without compromising model utility: (i) Weight bias correction enables shared
> batch normalization and eliminates the need for fine-tuning by neutralizing
> quantization-induced bias across bit-widths and aligning activation
> distributions; and (ii) Bit-wise coreset sampling strategy allows each child
> model to train on a compact, informative subset selected via gradient-based
> importance scores by exploiting the implicit knowledge transfer phenomenon.
> Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and
> ViT architectures demonstrate that our method achieves competitive or superior
> accuracy while reducing training time up to 7.88x. Our code is released at
> https://github.com/a2jinhee/EMQNet_jk.

