---
layout: default
title: Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence
---

# Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence

**arXiv**: [2510.20579v1](https://arxiv.org/abs/2510.20579) | [PDF](https://arxiv.org/pdf/2510.20579.pdf)

**ä½œè€…**: Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpen-o3 Videoæ¡†æž¶ï¼Œé€šè¿‡æ˜¾å¼æ—¶ç©ºè¯æ®è§£å†³è§†é¢‘æŽ¨ç†ä¸­çš„æ—¶ç©ºå®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘æŽ¨ç†` `æ—¶ç©ºè¯æ®` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é›†æž„å»º` `åŸºå‡†æµ‹è¯•` `ç½®ä¿¡åº¦éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æŽ¨ç†æ¨¡åž‹ç¼ºä¹æ—¶ç©ºè¯æ®æŒ‡ç¤ºï¼Œéš¾ä»¥è¿½è¸ªåŠ¨æ€åœºæ™¯ä¸­çš„å…³é”®è¯æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé«˜è´¨é‡æ•°æ®é›†å¹¶é‡‡ç”¨å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆå¤šå¥–åŠ±å‡½æ•°ä¼˜åŒ–ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ—¶ç©ºå¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨V-STARåŸºå‡†ä¸Šå®žçŽ°SOTAï¼ŒmAMå’ŒmLGMæ˜¾è‘—æå‡ï¼Œå¹¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šè¡¨çŽ°ä¸€è‡´æ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Most video reasoning models only generate textual reasoning traces without
> indicating when and where key evidence appears. Recent models such as OpenAI-o3
> have sparked wide interest in evidence-centered reasoning for images, yet
> extending this ability to videos is more challenging, as it requires joint
> temporal tracking and spatial localization across dynamic scenes. We introduce
> Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal
> evidence into video reasoning, and carefully collect training data and design
> training strategies to address the aforementioned challenges. The model
> highlights key timestamps, objects, and bounding boxes alongside its answers,
> allowing reasoning to be grounded in concrete visual observations. To enable
> this functionality, we first curate and build two high-quality datasets,
> STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed
> temporal and spatial annotations, since most existing datasets offer either
> temporal spans for videos or spatial boxes on images, lacking unified
> spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start
> reinforcement learning strategy with multiple specially designed rewards that
> jointly encourage answer accuracy, temporal alignment, and spatial precision.
> On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,
> raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent
> improvements are also observed on a broad range of video understanding
> benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond
> accuracy, the reasoning traces produced by Open-o3 Video also provide valuable
> signals for test-time scaling, enabling confidence-aware verification and
> improving answer reliability.

