---
layout: default
title: ARGenSeg: Image Segmentation with Autoregressive Image Generation Model
---

# ARGenSeg: Image Segmentation with Autoregressive Image Generation Model

**arXiv**: [2510.20803v1](https://arxiv.org/abs/2510.20803) | [PDF](https://arxiv.org/pdf/2510.20803.pdf)

**ä½œè€…**: Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARGenSegï¼ŒåŸºäºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹å®ç°å›¾åƒåˆ†å‰²ï¼Œæå‡ç»†ç²’åº¦è§†è§‰ç†è§£ä¸æ¨ç†é€Ÿåº¦ã€‚**

**å…³é”®è¯**: `å›¾åƒåˆ†å‰²` `è‡ªå›å½’ç”Ÿæˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `VQ-VAE` `æ¨ç†åŠ é€Ÿ` `åƒç´ çº§æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰MLLMåˆ†å‰²æ–¹æ³•ä¾èµ–ç¦»æ•£è¡¨ç¤ºæˆ–ä¸“ç”¨å¤´ï¼Œéš¾ä»¥æ•æ‰ç»†ç²’åº¦è§†è§‰ç»†èŠ‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨MLLMè¾“å‡ºè§†è§‰ä»¤ç‰Œï¼Œé€šè¿‡VQ-VAEè§£ç ä¸ºå›¾åƒï¼Œç”Ÿæˆå¯†é›†æ©ç ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶ŠSOTAï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ï¼Œä¿æŒå¼ºç†è§£èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a novel AutoRegressive Generation-based paradigm for image
> Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level
> perception within a unified framework. Prior works integrating image
> segmentation into multimodal large language models (MLLMs) typically employ
> either boundary points representation or dedicated segmentation heads. These
> methods rely on discrete representations or semantic prompts fed into
> task-specific decoders, which limits the ability of the MLLM to capture
> fine-grained visual details. To address these challenges, we introduce a
> segmentation framework for MLLM based on image generation, which naturally
> produces dense masks for target objects. We leverage MLLM to output visual
> tokens and detokenize them into images using an universal VQ-VAE, making the
> segmentation fully dependent on the pixel-level understanding of the MLLM. To
> reduce inference latency, we employ a next-scale-prediction strategy to
> generate required visual tokens in parallel. Extensive experiments demonstrate
> that our method surpasses prior state-of-the-art approaches on multiple
> segmentation datasets with a remarkable boost in inference speed, while
> maintaining strong understanding capabilities.

