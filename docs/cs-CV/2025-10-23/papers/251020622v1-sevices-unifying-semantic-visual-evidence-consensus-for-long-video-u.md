---
layout: default
title: SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding
---

# SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding

**arXiv**: [2510.20622v1](https://arxiv.org/abs/2510.20622) | [PDF](https://arxiv.org/pdf/2510.20622.pdf)

**ä½œè€…**: Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeViCESæ¡†æž¶ä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„è¯æ®é€‰æ‹©é—®é¢˜**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `è¯æ®é€‰æ‹©` `è¯­ä¹‰è§†è§‰å…±è¯†` `è®­ç»ƒæ— å…³æ¡†æž¶` `å¤šæ¨¡æ€èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿è§†é¢‘å†…å®¹å¤æ‚åˆ†æ•£ï¼ŒçŽ°æœ‰æ–¹æ³•å¿½ç•¥æ—¶åºä¾èµ–æˆ–ä¾èµ–å•æ¨¡æ€è¯æ®
2. æ–¹æ³•è¦ç‚¹ï¼šè®­ç»ƒæ— å…³æ¡†æž¶ï¼Œç»“åˆè¯­ä¹‰åˆ†æ”¯å’Œè§†è§‰åˆ†æ”¯è¿›è¡Œå…±è¯†å¸§é€‰æ‹©
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¼˜äºŽå…ˆè¿›æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long video understanding remains challenging due to its complex, diverse, and
> temporally scattered content. Although video large language models (Video-LLMs)
> can process videos lasting tens of minutes, applying them to truly long
> sequences is computationally prohibitive and often leads to unfocused or
> inconsistent reasoning. A promising solution is to select only the most
> informative frames, yet existing approaches typically ignore temporal
> dependencies or rely on unimodal evidence, limiting their ability to provide
> complete and query-relevant context. We propose a Semantic-Visual Consensus
> Evidence Selection (SeViCES) framework for effective and reliable long video
> understanding. SeViCES is training-free and model-agnostic, and introduces two
> key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module
> selects frames through (1) a temporal-aware semantic branch that leverages LLM
> reasoning over captions, and (2) a cluster-guided visual branch that aligns
> embeddings with semantic scores via mutual information. The Answer Consensus
> Refinement (ACR) module further resolves inconsistencies between semantic- and
> visual-based predictions by fusing evidence and constraining the answer space.
> Extensive experiments on long video understanding benchmarks show that SeViCES
> consistently outperforms state-of-the-art methods in both accuracy and
> robustness, demonstrating the importance of consensus-driven evidence selection
> for Video-LLMs.

