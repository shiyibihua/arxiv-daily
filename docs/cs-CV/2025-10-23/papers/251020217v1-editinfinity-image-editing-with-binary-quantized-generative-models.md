---
layout: default
title: EditInfinity: Image Editing with Binary-Quantized Generative Models
---

# EditInfinity: Image Editing with Binary-Quantized Generative Models

**arXiv**: [2510.20217v1](https://arxiv.org/abs/2510.20217) | [PDF](https://arxiv.org/pdf/2510.20217.pdf)

**ä½œè€…**: Jiahuan Wang, Yuxin Chen, Jun Yu, Guangming Lu, Wenjie Pei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEditInfinityæ–¹æ³•ï¼Œåˆ©ç”¨äºŒè¿›åˆ¶é‡åŒ–ç”Ÿæˆæ¨¡åž‹è§£å†³å›¾åƒç¼–è¾‘ä¸­çš„åæ¼”è¯¯å·®é—®é¢˜ã€‚**

**å…³é”®è¯**: `å›¾åƒç¼–è¾‘` `ç”Ÿæˆæ¨¡åž‹` `äºŒè¿›åˆ¶é‡åŒ–` `å›¾åƒåæ¼”` `æ–‡æœ¬é©±åŠ¨ç¼–è¾‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹å›¾åƒåæ¼”å› ç¼ºä¹ä¸­é—´æ­¥éª¤ç²¾ç¡®ç›‘ç£è€Œäº§ç”Ÿè¿‘ä¼¼è¯¯å·®ï¼Œé™åˆ¶ç¼–è¾‘æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽInfinityæ¨¡åž‹ï¼Œå®žçŽ°ç²¾ç¡®ä¸­é—´é‡åŒ–è¡¨ç¤ºï¼Œç»“åˆæ–‡æœ¬æç¤ºä¿®æ­£å’Œé£Žæ ¼ä¿ç•™æœºåˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PIE-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¼˜äºŽçŽ°æœ‰æ‰©æ•£åŸºçº¿ï¼Œæ”¯æŒæ·»åŠ ã€æ›´æ”¹å’Œåˆ é™¤æ“ä½œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adapting pretrained diffusion-based generative models for text-driven image
> editing with negligible tuning overhead has demonstrated remarkable potential.
> A classical adaptation paradigm, as followed by these methods, first infers the
> generative trajectory inversely for a given source image by image inversion,
> then performs image editing along the inferred trajectory guided by the target
> text prompts. However, the performance of image editing is heavily limited by
> the approximation errors introduced during image inversion by diffusion models,
> which arise from the absence of exact supervision in the intermediate
> generative steps. To circumvent this issue, we investigate the
> parameter-efficient adaptation of VQ-based generative models for image editing,
> and leverage their inherent characteristic that the exact intermediate
> quantized representations of a source image are attainable, enabling more
> effective supervision for precise image inversion. Specifically, we propose
> \emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized
> generative model, for image editing. We propose an efficient yet effective
> image inversion mechanism that integrates text prompting rectification and
> image style preservation, enabling precise image inversion. Furthermore, we
> devise a holistic smoothing strategy which allows our \emph{EditInfinity} to
> perform image editing with high fidelity to source images and precise semantic
> alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark
> across "add", "change", and "delete" editing operations, demonstrate the
> superior performance of our model compared to state-of-the-art diffusion-based
> baselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.

