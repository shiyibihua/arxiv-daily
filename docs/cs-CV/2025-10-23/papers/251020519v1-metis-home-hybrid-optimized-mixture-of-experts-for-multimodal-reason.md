---
layout: default
title: Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning
---

# Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning

**arXiv**: [2510.20519v1](https://arxiv.org/abs/2510.20519) | [PDF](https://arxiv.org/pdf/2510.20519.pdf)

**ä½œè€…**: Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆä¼˜åŒ–ä¸“å®¶æ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€æŽ¨ç†ä¸­çš„æ•ˆçŽ‡ä¸Žæ³›åŒ–æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŽ¨ç†` `æ··åˆä¸“å®¶æ¨¡åž‹` `åŠ¨æ€è·¯ç”±` `æ•ˆçŽ‡ä¼˜åŒ–` `æ³›åŒ–èƒ½åŠ›` `Qwen2.5-VL-7B`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨ç®€å•æŸ¥è¯¢ä¸Šè®¡ç®—æˆæœ¬é«˜ï¼Œä¸”æŽ¨ç†ä¸“ä¸šåŒ–æŸå®³æ³›åŒ–èƒ½åŠ›
2. é‡‡ç”¨åŒåˆ†æ”¯ä¸“å®¶ç»“æž„ï¼šæ€è€ƒåˆ†æ”¯å¤„ç†å¤æ‚æŽ¨ç†ï¼Œéžæ€è€ƒåˆ†æ”¯ä¼˜åŒ–å¿«é€ŸæŽ¨æ–­
3. å®žéªŒæ˜¾ç¤ºæ¨¡åž‹åœ¨å¤æ‚æŽ¨ç†å’Œä¸€èˆ¬èƒ½åŠ›ä¸Šå‡æœ‰æå‡ï¼Œé€†è½¬é€€åŒ–è¶‹åŠ¿

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Inspired by recent advancements in LLM reasoning, the field of multimodal
> reasoning has seen remarkable progress, achieving significant performance gains
> on intricate tasks such as mathematical problem-solving. Despite this progress,
> current multimodal large reasoning models exhibit two key limitations. They
> tend to employ computationally expensive reasoning even for simple queries,
> leading to inefficiency. Furthermore, this focus on specialized reasoning often
> impairs their broader, more general understanding capabilities. In this paper,
> we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed
> to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by
> structuring the original dense model into two distinct expert branches: a
> thinking branch tailored for complex, multi-step reasoning, and a non-thinking
> branch optimized for rapid, direct inference on tasks like general VQA and OCR.
> A lightweight, trainable router dynamically allocates queries to the most
> suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into
> an MoE architecture. Comprehensive evaluations reveal that our approach not
> only substantially enhances complex reasoning abilities but also improves the
> model's general capabilities, reversing the degradation trend observed in other
> reasoning-specialized models. Our work establishes a new paradigm for building
> powerful and versatile MLLMs, effectively resolving the prevalent
> reasoning-vs-generalization dilemma.

