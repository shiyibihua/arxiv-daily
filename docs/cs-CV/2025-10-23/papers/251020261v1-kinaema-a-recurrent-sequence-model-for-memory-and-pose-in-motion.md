---
layout: default
title: Kinaema: a recurrent sequence model for memory and pose in motion
---

# Kinaema: a recurrent sequence model for memory and pose in motion

**arXiv**: [2510.20261v1](https://arxiv.org/abs/2510.20261) | [PDF](https://arxiv.org/pdf/2510.20261.pdf)

**ä½œè€…**: Mert Bulent Sariyildiz, Philippe Weinzaepfel, Guillaume Bono, Gianluca Monaci, Christian Wolf

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKinaemaæ¨¡åž‹ä»¥è§£å†³æœºå™¨äººåœ¨è¿žç»­æ“ä½œä¸­åˆ©ç”¨å…ˆå‰è§†è§‰ä¿¡æ¯å®šä½è‡ªèº«çš„é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººå®šä½` `å¾ªçŽ¯å˜æ¢å™¨` `éšå¼è®°å¿†` `è§†è§‰å¯¼èˆª` `Mem-Navä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººåœ¨å…ˆå‰è§è¿‡çš„ç©ºé—´ä¸­å¦‚ä½•é«˜æ•ˆå®šä½è‡ªèº«ï¼Œåˆ©ç”¨åŽ†å²è§‚æµ‹ä¼˜åŒ–æ“ä½œæ•ˆçŽ‡
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¾ªçŽ¯å˜æ¢å™¨ç»´æŠ¤éšå¼æ½œåœ¨è®°å¿†ï¼ŒåŽ‹ç¼©ä¼ æ„Ÿå™¨åŽ†å²ï¼Œæ— éœ€æ˜¾å¼å­˜å‚¨è§‚æµ‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Mem-Navä»»åŠ¡ä¸­éªŒè¯æ¨¡åž‹èƒ½å¯¼èˆªè‡³ç›®æ ‡ï¼Œä¿æŒåœºæ™¯è¡¨ç¤ºï¼Œè®¡ç®—é«˜æ•ˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> One key aspect of spatially aware robots is the ability to "find their
> bearings", ie. to correctly situate themselves in previously seen spaces. In
> this work, we focus on this particular scenario of continuous robotics
> operations, where information observed before an actual episode start is
> exploited to optimize efficiency. We introduce a new model, Kinaema, and agent,
> capable of integrating a stream of visual observations while moving in a
> potentially large scene, and upon request, processing a query image and
> predicting the relative position of the shown space with respect to its current
> position. Our model does not explicitly store an observation history, therefore
> does not have hard constraints on context length. It maintains an implicit
> latent memory, which is updated by a transformer in a recurrent way,
> compressing the history of sensor readings into a compact representation. We
> evaluate the impact of this model in a new downstream task we call "Mem-Nav".
> We show that our large-capacity recurrent model maintains a useful
> representation of the scene, navigates to goals observed before the actual
> episode start, and is computationally efficient, in particular compared to
> classical transformers with attention over an observation history.

