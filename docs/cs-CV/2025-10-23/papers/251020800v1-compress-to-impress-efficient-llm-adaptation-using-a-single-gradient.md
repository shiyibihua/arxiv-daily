---
layout: default
title: Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples
---

# Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples

**arXiv**: [2510.20800v1](https://arxiv.org/abs/2510.20800) | [PDF](https://arxiv.org/pdf/2510.20800.pdf)

**ä½œè€…**: Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆLLMé€‚åº”æ–¹æ³•ï¼Œä½¿ç”¨å•æ­¥æ¢¯åº¦å’Œ100æ ·æœ¬æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**

**å…³é”®è¯**: `LLMé€‚åº”` `æ¢¯åº¦åˆ†æž` `çŸ©é˜µåˆ†è§£` `æ ·æœ¬æ•ˆçŽ‡` `ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLASERæ–¹æ³•å±‚é—´æœç´¢å¼€é”€å¤§ï¼Œéš¾ä»¥å¿«é€Ÿéƒ¨ç½²LLMé€‚åº”ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ¢¯åº¦åˆ†æžé€‰æ‹©å…³é”®çŸ©é˜µï¼Œæ‰©å±•åˆ†è§£ç©ºé—´å‡å°‘è¿‡æ‹Ÿåˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨100æ ·æœ¬ä¸Šå®žçŽ°å¿«é€Ÿé€‚åº”ï¼Œå‡†ç¡®çŽ‡æå‡é«˜è¾¾24.6ä¸ªç™¾åˆ†ç‚¹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Sharma et al. suggested a method called Layer-SElective-Rank
> reduction (LASER) which demonstrated that pruning high-order components of
> carefully chosen LLM's weight matrices can boost downstream accuracy -- without
> any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
> requiring full-dataset forward passes) makes it impractical for rapid
> deployment. We demonstrate that this overhead can be removed and find that: (i)
> Only a small, carefully chosen subset of matrices needs to be inspected --
> eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
> singular values pinpoints which matrices merit reduction, (iii) Increasing the
> factorization search space by allowing matrices rows to cluster around multiple
> subspaces and then decomposing each cluster separately further reduces
> overfitting on the original training data and further lifts accuracy by up to
> 24.6 percentage points, and finally, (iv) we discover that evaluating on just
> 100 samples rather than the full training data -- both for computing the
> indicative gradients and for measuring the final accuracy -- suffices to
> further reduce the search time; we explain that as adaptation to downstream
> tasks is dominated by prompting style, not dataset size. As a result, we show
> that combining these findings yields a fast and robust adaptation algorithm for
> downstream tasks. Overall, with a single gradient step on 100 examples and a
> quick scan of the top candidate layers and factorization techniques, we can
> adapt LLMs to new datasets -- entirely without fine-tuning.

