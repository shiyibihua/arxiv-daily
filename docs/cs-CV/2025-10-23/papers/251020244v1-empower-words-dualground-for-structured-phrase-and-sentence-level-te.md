---
layout: default
title: Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding
---

# Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding

**arXiv**: [2510.20244v1](https://arxiv.org/abs/2510.20244) | [PDF](https://arxiv.org/pdf/2510.20244.pdf)

**ä½œè€…**: Minseok Kang, Minhyeok Lee, Minjung Kim, Donghyeong Kim, Sangyoun Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDualGroundä»¥è§£å†³è§†é¢‘æ—¶åºå®šä½ä¸­è¯­ä¹‰è§’è‰²å¿½è§†é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `åŒåˆ†æ”¯æž¶æž„` `è¯­ä¹‰è§£è€¦` `çŸ­è¯­çº§å¯¹é½` `å¥å­çº§å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ç»Ÿä¸€å¤„ç†æ–‡æœ¬ä»¤ç‰Œï¼Œå¿½ç•¥è¯­ä¹‰è§’è‰²å·®å¼‚ï¼Œå¯¼è‡´ç»†ç²’åº¦å¯¹é½ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒåˆ†æ”¯æž¶æž„ï¼Œåˆ†ç¦»å…¨å±€å¥å­çº§å’Œå±€éƒ¨çŸ­è¯­çº§è¯­ä¹‰ï¼Œå®žçŽ°ç»“æž„åŒ–è§£è€¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨QVHighlightså’ŒCharades-STAåŸºå‡†ä¸Šï¼Œå®žçŽ°æœ€å…ˆè¿›çš„æ—¶åˆ»æ£€ç´¢å’Œé«˜å…‰æ£€æµ‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Temporal Grounding (VTG) aims to localize temporal segments in long,
> untrimmed videos that align with a given natural language query. This task
> typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection
> (HD). While recent advances have been progressed by powerful pretrained
> vision-language models such as CLIP and InternVideo2, existing approaches
> commonly treat all text tokens uniformly during crossmodal attention,
> disregarding their distinct semantic roles. To validate the limitations of this
> approach, we conduct controlled experiments demonstrating that VTG models
> overly rely on [EOS]-driven global semantics while failing to effectively
> utilize word-level signals, which limits their ability to achieve fine-grained
> temporal alignment. Motivated by this limitation, we propose DualGround, a
> dual-branch architecture that explicitly separates global and local semantics
> by routing the [EOS] token through a sentence-level path and clustering word
> tokens into phrase-level units for localized grounding. Our method introduces
> (1) tokenrole- aware cross modal interaction strategies that align video
> features with sentence-level and phrase-level semantics in a structurally
> disentangled manner, and (2) a joint modeling framework that not only improves
> global sentence-level alignment but also enhances finegrained temporal
> grounding by leveraging structured phrase-aware context. This design allows the
> model to capture both coarse and localized semantics, enabling more expressive
> and context-aware video grounding. DualGround achieves state-of-the-art
> performance on both Moment Retrieval and Highlight Detection tasks across
> QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of
> disentangled semantic modeling in video-language alignment.

