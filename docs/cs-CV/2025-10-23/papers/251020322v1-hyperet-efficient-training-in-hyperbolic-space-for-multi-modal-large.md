---
layout: default
title: HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models
---

# HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models

**arXiv**: [2510.20322v1](https://arxiv.org/abs/2510.20322) | [PDF](https://arxiv.org/pdf/2510.20322.pdf)

**ä½œè€…**: Zelin Peng, Zhengqin Xu, Qingyang Liu, Xiaokang Yang, Wei Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHyperETåœ¨åŒæ›²ç©ºé—´ä¸­é«˜æ•ˆè®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œä»¥è§£å†³è·¨æ¨¡æ€å¯¹é½è®¡ç®—èµ„æºé«˜çš„é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŒæ›²ç©ºé—´è®­ç»ƒ` `è·¨æ¨¡æ€å¯¹é½` `é«˜æ•ˆå‚æ•°åŒ–` `è§†è§‰-æ–‡æœ¬ç²’åº¦å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒè®¡ç®—èµ„æºé«˜ï¼Œè§†è§‰ç¼–ç å™¨ç¼ºä¹å¤šç²’åº¦è¯­è¨€å¯¹é½
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨åŒæ›²ç©ºé—´å»ºæ¨¡å±‚æ¬¡ç»“æž„ï¼Œé€šè¿‡åŠ¨æ€åŠå¾„è°ƒæ•´å®žçŽ°ä»»æ„ç²’åº¦è§†è§‰-æ–‡æœ¬å¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥å°‘äºŽ1%é¢å¤–å‚æ•°æ˜¾è‘—æå‡çŽ°æœ‰æ¨¡åž‹æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-modal large language models (MLLMs) have emerged as a transformative
> approach for aligning visual and textual understanding. They typically require
> extremely high computational resources (e.g., thousands of GPUs) for training
> to achieve cross-modal alignment at multi-granularity levels. We argue that a
> key source of this inefficiency lies in the vision encoders they widely equip
> with, e.g., CLIP and SAM, which lack the alignment with language at
> multi-granularity levels. To address this issue, in this paper, we leverage
> hyperbolic space, which inherently models hierarchical levels and thus provides
> a principled framework for bridging the granularity gap between visual and
> textual modalities at an arbitrary granularity level. Concretely, we propose an
> efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize
> visual representations to align with their textual counterparts at an arbitrary
> granularity level through dynamic hyperbolic radius adjustment in hyperbolic
> space. HyperET employs learnable matrices with M\"{o}bius multiplication
> operations, implemented via three effective configurations: diagonal scaling
> matrices, block-diagonal matrices, and banded matrices, providing a flexible
> yet efficient parametrization strategy. Comprehensive experiments across
> multiple MLLM benchmarks demonstrate that HyperET consistently improves both
> existing pre-training and fine-tuning MLLMs clearly with less than 1\%
> additional parameters.

