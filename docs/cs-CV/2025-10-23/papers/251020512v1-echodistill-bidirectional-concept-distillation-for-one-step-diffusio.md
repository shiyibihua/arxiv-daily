---
layout: default
title: EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization
---

# EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization

**arXiv**: [2510.20512v1](https://arxiv.org/abs/2510.20512) | [PDF](https://arxiv.org/pdf/2510.20512.pdf)

**ä½œè€…**: Yixiong Yang, Tao Wu, Senmao Li, Shiqi Yang, Yaxing Wang, Joost van de Weijer, Kai Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEchoDistillåŒå‘æ¦‚å¿µè’¸é¦æ¡†æž¶ä»¥è§£å†³ä¸€æ­¥æ‰©æ•£æ¨¡åž‹ä¸ªæ€§åŒ–éš¾é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `æ¦‚å¿µè’¸é¦` `ä¸€æ­¥ç”Ÿæˆ` `æ¨¡åž‹ä¸ªæ€§åŒ–` `å¸ˆç”Ÿå­¦ä¹ ` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ­¥æ‰©æ•£æ¨¡åž‹éš¾ä»¥æœ‰æ•ˆæ•æ‰æ–°æ¦‚å¿µåˆ†å¸ƒï¼Œé™åˆ¶ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¸ˆç”Ÿæ¨¡åž‹åŒå‘è’¸é¦å’Œå…±äº«æ–‡æœ¬ç¼–ç å™¨ï¼Œæå‡æ¦‚å¿µå­¦ä¹ æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸€æ­¥æ‰©æ•£ä¸ªæ€§åŒ–è®¾ç½®ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in accelerating text-to-image (T2I) diffusion models have
> enabled the synthesis of high-fidelity images even in a single step. However,
> personalizing these models to incorporate novel concepts remains a challenge
> due to the limited capacity of one-step models to capture new concept
> distributions effectively. We propose a bidirectional concept distillation
> framework, EchoDistill, to enable one-step diffusion personalization (1-SDP).
> Our approach involves an end-to-end training process where a multi-step
> diffusion model (teacher) and a one-step diffusion model (student) are trained
> simultaneously. The concept is first distilled from the teacher model to the
> student, and then echoed back from the student to the teacher. During the
> EchoDistill, we share the text encoder between the two models to ensure
> consistent semantic understanding. Following this, the student model is
> optimized with adversarial losses to align with the real image distribution and
> with alignment losses to maintain consistency with the teacher's output.
> Furthermore, we introduce the bidirectional echoing refinement strategy,
> wherein the student model leverages its faster generation capability to
> feedback to the teacher model. This bidirectional concept distillation
> mechanism not only enhances the student ability to personalize novel concepts
> but also improves the generative quality of the teacher model. Our experiments
> demonstrate that this collaborative framework significantly outperforms
> existing personalization methods over the 1-SDP setup, establishing a novel
> paradigm for rapid and effective personalization in T2I diffusion models.

