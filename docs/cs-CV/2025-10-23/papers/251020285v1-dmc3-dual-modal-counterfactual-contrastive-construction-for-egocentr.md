---
layout: default
title: DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering
---

# DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering

**arXiv**: [2510.20285v1](https://arxiv.org/abs/2510.20285) | [PDF](https://arxiv.org/pdf/2510.20285.pdf)

**ä½œè€…**: Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ¨¡æ€åäº‹å®žå¯¹æ¯”æž„å»ºæ¡†æž¶ä»¥è§£å†³ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”ä¸­çš„å¤šäº‹ä»¶ä¸Žæ‰‹ç‰©äº¤äº’æŒ‘æˆ˜**

**å…³é”®è¯**: `ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”` `åäº‹å®žæ ·æœ¬æž„å»º` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ‰‹ç‰©äº¤äº’è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”ä¸­å¿½ç•¥å¤šäº‹ä»¶ä¸Žæ‰‹ç‰©äº¤äº’ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡äº‹ä»¶æè¿°è½¬è¿°å’Œæ ¸å¿ƒäº¤äº’æŒ–æŽ˜æž„å»ºåäº‹å®žæ ·æœ¬ï¼Œå¹¶åº”ç”¨å¯¹æ¯”ä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EgoTaskQAå’ŒQAEGO4Dæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Egocentric Video Question Answering (Egocentric VideoQA) plays an important
> role in egocentric video understanding, which refers to answering questions
> based on first-person videos. Although existing methods have made progress
> through the paradigm of pre-training and fine-tuning, they ignore the unique
> challenges posed by the first-person perspective, such as understanding
> multiple events and recognizing hand-object interactions. To deal with these
> challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
> (DMC$^3$) framework, which contains an egocentric videoqa baseline, a
> counterfactual sample construction module and a counterfactual sample-involved
> contrastive optimization. Specifically, We first develop a counterfactual
> sample construction module to generate positive and negative samples for
> textual and visual modalities through event description paraphrasing and core
> interaction mining, respectively. Then, We feed these samples together with the
> original samples into the baseline. Finally, in the counterfactual
> sample-involved contrastive optimization module, we apply contrastive loss to
> minimize the distance between the original sample features and the positive
> sample features, while maximizing the distance from the negative samples.
> Experiments show that our method achieve 52.51\% and 46.04\% on the
> \textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
> QAEGO4D, both reaching the state-of-the-art performance.

