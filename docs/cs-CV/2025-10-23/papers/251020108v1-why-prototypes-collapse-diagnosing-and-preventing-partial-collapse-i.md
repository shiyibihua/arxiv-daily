---
layout: default
title: Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning
---

# Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning

**arXiv**: [2510.20108v1](https://arxiv.org/abs/2510.20108) | [PDF](https://arxiv.org/pdf/2510.20108.pdf)

**ä½œè€…**: Gabriel Y. Arteaga, Marius Aasan, Rwiddhi Chakraborty, Martine Hjelkrem-Tan, Thalles Silva, Michael Kampffmeyer, AdÃ­n RamÃ­rez Rivera

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£è€¦è®­ç»ƒç­–ç•¥ä»¥è§£å†³åŽŸåž‹è‡ªç›‘ç£å­¦ä¹ ä¸­çš„éƒ¨åˆ†åŽŸåž‹åç¼©é—®é¢˜**

**å…³é”®è¯**: `åŽŸåž‹è‡ªç›‘ç£å­¦ä¹ ` `åŽŸåž‹åç¼©` `è§£è€¦è®­ç»ƒ` `é«˜æ–¯æ··åˆæ¨¡åž‹` `åœ¨çº¿EMç®—æ³•` `è¡¨ç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŽŸåž‹è‡ªç›‘ç£å­¦ä¹ ä¸­å¤šä¸ªåŽŸåž‹æ”¶æ•›åˆ°ç›¸ä¼¼è¡¨ç¤ºï¼Œå‰Šå¼±è¡¨ç¤ºå¤šæ ·æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è§£è€¦è®­ç»ƒï¼ŒåŽŸåž‹é€šè¿‡åœ¨çº¿EMæ›´æ–°ï¼Œç‹¬ç«‹äºŽç¼–ç å™¨ä¼˜åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¶ˆé™¤åŽŸåž‹åç¼©ï¼Œæå‡åŽŸåž‹å¤šæ ·æ€§å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Prototypical self-supervised learning methods consistently suffer from
> partial prototype collapse, where multiple prototypes converge to nearly
> identical representations. This undermines their central purpose -- providing
> diverse and informative targets to guide encoders toward rich representations
> -- and has led practitioners to over-parameterize prototype sets or add ad-hoc
> regularizers, which mitigate symptoms rather than address the root cause. We
> empirically trace the collapse to the joint optimization of encoders and
> prototypes, which encourages a type of shortcut learning: early in training
> prototypes drift toward redundant representations that minimize loss without
> necessarily enhancing representation diversity. To break the joint
> optimization, we introduce a fully decoupled training strategy that learns
> prototypes and encoders under separate objectives. Concretely, we model
> prototypes as a Gaussian mixture updated with an online EM-style procedure,
> independent of the encoder's loss. This simple yet principled decoupling
> eliminates prototype collapse without explicit regularization and yields
> consistently diverse prototypes and stronger downstream performance.

