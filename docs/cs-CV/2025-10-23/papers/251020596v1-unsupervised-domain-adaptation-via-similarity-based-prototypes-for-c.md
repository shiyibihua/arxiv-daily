---
layout: default
title: Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation
---

# Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation

**arXiv**: [2510.20596v1](https://arxiv.org/abs/2510.20596) | [PDF](https://arxiv.org/pdf/2510.20596.pdf)

**ä½œè€…**: Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç›¸ä¼¼æ€§åŽŸåž‹çš„æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•ä»¥è§£å†³è·¨æ¨¡æ€åˆ†å‰²é—®é¢˜**

**å…³é”®è¯**: `æ— ç›‘ç£åŸŸé€‚åº”` `è·¨æ¨¡æ€åˆ†å‰²` `åŽŸåž‹å­¦ä¹ ` `ç›¸ä¼¼æ€§çº¦æŸ` `å¯¹æ¯”å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦å­¦ä¹ æ¨¡åž‹åœ¨æœªè§æ•°æ®ä¸Šæ€§èƒ½ä¸‹é™ï¼Œå› åŸŸåç§»æ•æ„Ÿã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå­¦ä¹ ç±»åŽŸåž‹äºŽåµŒå…¥ç©ºé—´ï¼Œå¼•å…¥ç›¸ä¼¼æ€§çº¦æŸå¢žå¼ºä»£è¡¨æ€§ä¸Žå¯åˆ†æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä½¿ç”¨å­—å…¸å­˜å‚¨åŽŸåž‹ï¼Œé¿å…ç±»ç¼ºå¤±ï¼Œå®žéªŒæ˜¾ç¤ºä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep learning models have achieved great success on various vision
> challenges, but a well-trained model would face drastic performance degradation
> when applied to unseen data. Since the model is sensitive to domain shift,
> unsupervised domain adaptation attempts to reduce the domain gap and avoid
> costly annotation of unseen domains. This paper proposes a novel framework for
> cross-modality segmentation via similarity-based prototypes. In specific, we
> learn class-wise prototypes within an embedding space, then introduce a
> similarity constraint to make these prototypes representative for each semantic
> class while separable from different classes. Moreover, we use dictionaries to
> store prototypes extracted from different images, which prevents the
> class-missing problem and enables the contrastive learning of prototypes, and
> further improves performance. Extensive experiments show that our method
> achieves better results than other state-of-the-art methods.

