---
layout: default
title: Inverse Image-Based Rendering for Light Field Generation from Single Images
---

# Inverse Image-Based Rendering for Light Field Generation from Single Images

**arXiv**: [2510.20132v1](https://arxiv.org/abs/2510.20132) | [PDF](https://arxiv.org/pdf/2510.20132.pdf)

**ä½œè€…**: Hyunjun Jung, Hae-Gon Jeon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€†å›¾åƒæ¸²æŸ“æ–¹æ³•ä»Žå•å›¾åƒç”Ÿæˆå…‰åœºï¼Œç”¨äºŽæ–°è§†è§’åˆæˆ**

**å…³é”®è¯**: `å…‰åœºç”Ÿæˆ` `æ–°è§†è§’åˆæˆ` `é€†å›¾åƒæ¸²æŸ“` `ç¥žç»æ¸²æŸ“` `å•å›¾åƒå¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå…‰åœºèŽ·å–éœ€å¤šè§†å›¾æˆ–ä¸“ç”¨è®¾å¤‡ï¼Œæˆæœ¬é«˜ä¸”ä¸ä¾¿ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ç¥žç»æ¸²æŸ“ç®¡é“ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›é¢„æµ‹ç›®æ ‡å…‰çº¿é¢œè‰²ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A concept of light-fields computed from multiple view images on regular grids
> has proven its benefit for scene representations, and supported realistic
> renderings of novel views and photographic effects such as refocusing and
> shallow depth of field. In spite of its effectiveness of light flow
> computations, obtaining light fields requires either computational costs or
> specialized devices like a bulky camera setup and a specialized microlens
> array. In an effort to broaden its benefit and applicability, in this paper, we
> propose a novel view synthesis method for light field generation from only
> single images, named inverse image-based rendering. Unlike previous attempts to
> implicitly rebuild 3D geometry or to explicitly represent objective scenes, our
> method reconstructs light flows in a space from image pixels, which behaves in
> the opposite way to image-based rendering. To accomplish this, we design a
> neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our
> neural renderer first stores the light flow of source rays from the input
> image, then computes the relationships among them through cross-attention, and
> finally predicts the color of the target ray based on these relationships.
> After the rendering pipeline generates the first novel view from a single input
> image, the generated out-of-view contents are updated to the set of source
> rays. This procedure is iteratively performed while ensuring the consistent
> generation of occluded contents. We demonstrate that our inverse image-based
> rendering works well with various challenging datasets without any retraining
> or finetuning after once trained on synthetic dataset, and outperforms relevant
> state-of-the-art novel view synthesis methods.

