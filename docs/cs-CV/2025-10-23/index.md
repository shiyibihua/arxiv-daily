---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-23
---

# cs.CVï¼ˆ2025-10-23ï¼‰

ğŸ“Š å…± **32** ç¯‡è®ºæ–‡
 | ğŸ”— **11** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251020095v2-biocap-exploiting-synthetic-captions-beyond-labels-in-biological-fou.html">BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</a></td>
  <td>BioCAPï¼šåˆ©ç”¨åˆæˆå­—å¹•å¢å¼ºç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹ï¼Œè¶…è¶Šæ ‡ç­¾ç›‘ç£</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20095v2" onclick="toggleFavorite(this, '2510.20095v2', 'BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251020578v1-embodiedbrain-expanding-performance-boundaries-of-task-planning-for-.html">EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</a></td>
  <td>EmbodiedBrainï¼šé€šè¿‡Step-GRPOæå‡å…·èº«æ™ºèƒ½ä»»åŠ¡è§„åˆ’æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20578v1" onclick="toggleFavorite(this, '2510.20578v1', 'EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251020696v1-diagnosing-visual-reasoning-challenges-insights-and-a-path-forward.html">Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</a></td>
  <td>æå‡ºåŸºäºAgentçš„æ¶æ„ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20696v1" onclick="toggleFavorite(this, '2510.20696v1', 'Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251020519v2-metis-home-hybrid-optimized-mixture-of-experts-for-multimodal-reason.html">Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning</a></td>
  <td>æå‡ºMetis-HOMEï¼Œé€šè¿‡æ··åˆä¸“å®¶æ¨¡å‹è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ•ˆç‡ä¸æ³›åŒ–éš¾é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20519v2" onclick="toggleFavorite(this, '2510.20519v2', 'Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251020322v2-hyperet-efficient-training-in-hyperbolic-space-for-multi-modal-large.html">HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</a></td>
  <td>HyperETï¼šé€šè¿‡åŒæ›²ç©ºé—´é«˜æ•ˆè®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡è·¨æ¨¡æ€å¯¹é½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20322v2" onclick="toggleFavorite(this, '2510.20322v2', 'HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251020256v1-calibrating-multimodal-consensus-for-emotion-recognition.html">Calibrating Multimodal Consensus for Emotion Recognition</a></td>
  <td>æå‡ºæ ¡å‡†å¤šæ¨¡æ€å…±è¯†æ¨¡å‹ä»¥è§£å†³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20256v1" onclick="toggleFavorite(this, '2510.20256v1', 'Calibrating Multimodal Consensus for Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251020531v1-fake-in-facext-towards-fine-grained-explainable-deepfake-analysis.html">Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis</a></td>
  <td>æå‡ºFake-in-Facextæ¡†æ¶ï¼Œå®ç°ç»†ç²’åº¦ã€å¯è§£é‡Šçš„DeepFakeäººè„¸åˆ†æã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20531v1" onclick="toggleFavorite(this, '2510.20531v1', 'Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251020812v3-small-drafts-big-verdict-information-intensive-visual-reasoning-via-.html">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</a></td>
  <td>æå‡ºSpeculative Verdictæ¡†æ¶ï¼Œè§£å†³ä¿¡æ¯å¯†é›†å‹å›¾åƒçš„è§†è§‰æ¨ç†éš¾é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20812v3" onclick="toggleFavorite(this, '2510.20812v3', 'Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251020622v1-sevices-unifying-semantic-visual-evidence-consensus-for-long-video-u.html">SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</a></td>
  <td>æå‡ºSeViCESæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰-è§†è§‰å…±è¯†æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20622v1" onclick="toggleFavorite(this, '2510.20622v1', 'SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251020287v1-breakdance-video-classification-in-the-age-of-generative-ai.html">Breakdance Video classification in the age of Generative AI</a></td>
  <td>é’ˆå¯¹éœ¹é›³èˆè§†é¢‘åˆ†ç±»ï¼Œåˆ†æäº†ç”Ÿæˆå¼AIæ—¶ä»£ä¸‹è§†é¢‘åŸºç¡€æ¨¡å‹ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰çš„é€‚ç”¨æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20287v1" onclick="toggleFavorite(this, '2510.20287v1', 'Breakdance Video classification in the age of Generative AI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/251020994v1-vessa-video-based-object-centric-self-supervised-adaptation-for-visu.html">VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models</a></td>
  <td>æå‡ºVESSAï¼šä¸€ç§åŸºäºè§†é¢‘å¯¹è±¡ä¸­å¿ƒçš„è‡ªç›‘ç£è§†è§‰åŸºç¡€æ¨¡å‹é€‚åº”æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20994v1" onclick="toggleFavorite(this, '2510.20994v1', 'VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251020470v2-conan-progressive-learning-to-reason-like-a-detective-over-multi-sca.html">Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</a></td>
  <td>Conanï¼šæå‡ºåŸºäºå¤šå°ºåº¦è§†è§‰è¯æ®çš„æ¸è¿›å¼å­¦ä¹ æ¡†æ¶ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20470v2" onclick="toggleFavorite(this, '2510.20470v2', 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251020196v1-a-structured-review-and-quantitative-profiling-of-public-brain-mri-d.html">A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development</a></td>
  <td>é’ˆå¯¹è„‘MRIåŸºç¡€æ¨¡å‹ï¼Œè®ºæ–‡ç³»ç»Ÿè¯„ä¼°äº†å…¬å¼€æ•°æ®é›†çš„å¤šæ ·æ€§ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20196v1" onclick="toggleFavorite(this, '2510.20196v1', 'A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251021501v1-granvit-a-fine-grained-vision-model-with-autoregressive-perception-f.html">GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs</a></td>
  <td>GranViTï¼šé¢å‘MLLMçš„ç»†ç²’åº¦è§†è§‰æ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’æ„ŸçŸ¥æå‡æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21501v1" onclick="toggleFavorite(this, '2510.21501v1', 'GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251021867v1-addressing-corner-cases-in-autonomous-driving-a-world-model-based-ap.html">Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs</a></td>
  <td>æå‡ºWM-MoEæ¡†æ¶ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹å’Œæ··åˆä¸“å®¶æ¨¡å‹è§£å†³è‡ªåŠ¨é©¾é©¶Corner Caseé—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21867v1" onclick="toggleFavorite(this, '2510.21867v1', 'Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251020214v1-towards-objective-obstetric-ultrasound-assessment-contrastive-repres.html">Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</a></td>
  <td>æå‡ºCURLæ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œèƒå„¿è¶…å£°è§†é¢‘ä¸­çš„èƒåŠ¨æ£€æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20214v1" onclick="toggleFavorite(this, '2510.20214v1', 'Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251020951v1-generative-point-tracking-with-flow-matching.html">Generative Point Tracking with Flow Matching</a></td>
  <td>æå‡ºåŸºäºFlow Matchingçš„ç”Ÿæˆå¼ç‚¹è·Ÿè¸ªå™¨GenPTï¼Œè§£å†³è§†è§‰é®æŒ¡ä¸‹çš„å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20951v1" onclick="toggleFavorite(this, '2510.20951v1', 'Generative Point Tracking with Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251021879v1-ternaryclip-efficiently-compressing-vision-language-models-with-tern.html">TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge</a></td>
  <td>TernaryCLIPï¼šé€šè¿‡ä¸‰å…ƒæƒé‡å’ŒçŸ¥è¯†è’¸é¦é«˜æ•ˆå‹ç¼©è§†è§‰-è¯­è¨€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21879v1" onclick="toggleFavorite(this, '2510.21879v1', 'TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251020165v1-ib-gan-disentangled-representation-learning-with-information-bottlen.html">IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</a></td>
  <td>æå‡ºIB-GANï¼Œåˆ©ç”¨ä¿¡æ¯ç“¶é¢ˆæ”¹è¿›GANçš„è§£è€¦è¡¨ç¤ºå­¦ä¹ ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20165v1" onclick="toggleFavorite(this, '2510.20165v1', 'IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251020162v1-tomcat-test-time-comprehensive-knowledge-accumulation-for-compositio.html">TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</a></td>
  <td>æå‡ºTOMCATï¼Œé€šè¿‡æµ‹è¯•æ—¶çŸ¥è¯†ç´¯ç§¯è§£å†³ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20162v1" onclick="toggleFavorite(this, '2510.20162v1', 'TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251020238v1-cos3d-collaborative-open-vocabulary-3d-segmentation.html">COS3D: Collaborative Open-Vocabulary 3D Segmentation</a></td>
  <td>æå‡ºCOS3Dï¼Œé€šè¿‡ååŒæç¤ºåˆ†å‰²æ¡†æ¶è§£å†³å¼€æ”¾è¯æ±‡3Dåˆ†å‰²ä¸­çš„è¯­è¨€ä¸åˆ†å‰²èåˆé—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20238v1" onclick="toggleFavorite(this, '2510.20238v1', 'COS3D: Collaborative Open-Vocabulary 3D Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251020549v1-deep-learning-powered-visual-slam-aimed-at-assisting-visually-impair.html">Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</a></td>
  <td>æå‡ºSELM-SLAM3ï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ å¢å¼ºè§†è§‰SLAMï¼Œè¾…åŠ©è§†éšœäººå£«å¯¼èˆªã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20549v1" onclick="toggleFavorite(this, '2510.20549v1', 'Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251020206v1-rapo-cross-stage-prompt-optimization-for-text-to-video-generation-vi.html">RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</a></td>
  <td>RAPO++ï¼šé€šè¿‡æ•°æ®å¯¹é½å’Œæµ‹è¯•æ—¶ç¼©æ”¾ä¼˜åŒ–æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„è·¨é˜¶æ®µPrompt</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20206v1" onclick="toggleFavorite(this, '2510.20206v1', 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251020155v1-partnext-a-next-generation-dataset-for-fine-grained-and-hierarchical.html">PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</a></td>
  <td>æå‡ºPartNeXtæ•°æ®é›†ï¼Œç”¨äºç»†ç²’åº¦åˆ†å±‚3Déƒ¨ä»¶ç†è§£ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20155v1" onclick="toggleFavorite(this, '2510.20155v1', 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251020558v1-from-far-and-near-perceptual-evaluation-of-crowd-representations-acr.html">From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail</a></td>
  <td>ç ”ç©¶ä¸åŒç»†èŠ‚å±‚æ¬¡ä¸‹äººç¾¤è¡¨å¾çš„æ„ŸçŸ¥è´¨é‡ï¼Œä¼˜åŒ–äººç¾¤æ¸²æŸ“ç­–ç•¥ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20558v1" onclick="toggleFavorite(this, '2510.20558v1', 'From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251020178v1-ppmstereo-pick-and-play-memory-construction-for-consistent-dynamic-s.html">PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</a></td>
  <td>æå‡ºPPMStereoï¼Œé€šè¿‡Pick-and-Playè®°å¿†æ„å»ºå®ç°åŠ¨æ€ç«‹ä½“åŒ¹é…ä¸­çš„æ—¶åºä¸€è‡´æ€§ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20178v1" onclick="toggleFavorite(this, '2510.20178v1', 'PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/251020285v2-dmc3-dual-modal-counterfactual-contrastive-construction-for-egocentr.html">DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</a></td>
  <td>æå‡ºDMC$^3$æ¡†æ¶ä»¥è§£å†³ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”ä¸­çš„æŒ‘æˆ˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20285v2" onclick="toggleFavorite(this, '2510.20285v2', 'DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251020794v1-radar-camera-fused-multi-object-tracking-online-calibration-and-comm.html">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a></td>
  <td>æå‡ºä¸€ç§é›·è¾¾-ç›¸æœºèåˆçš„å¤šç›®æ ‡è·Ÿè¸ªæ¡†æ¶ï¼Œå®ç°åœ¨çº¿æ ‡å®šå’Œé€šç”¨ç‰¹å¾åˆ©ç”¨ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20794v1" onclick="toggleFavorite(this, '2510.20794v1', 'Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251021000v1-biodet-boosting-industrial-object-detection-with-image-preprocessing.html">BioDet: Boosting Industrial Object Detection with Image Preprocessing Strategies</a></td>
  <td>BioDetï¼šåˆ©ç”¨å›¾åƒé¢„å¤„ç†ç­–ç•¥æå‡å·¥ä¸šç›®æ ‡æ£€æµ‹æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21000v1" onclick="toggleFavorite(this, '2510.21000v1', 'BioDet: Boosting Industrial Object Detection with Image Preprocessing Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/251020807v1-video-prediction-of-dynamic-physical-simulations-with-pixel-space-sp.html">Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</a></td>
  <td>æå‡ºåŸºäºåƒç´ ç©ºé—´æ—¶ç©ºTransformerçš„ç‰©ç†æ¨¡æ‹Ÿè§†é¢‘é¢„æµ‹æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20807v1" onclick="toggleFavorite(this, '2510.20807v1', 'Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/251020803v1-argenseg-image-segmentation-with-autoregressive-image-generation-mod.html">ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</a></td>
  <td>ARGenSegï¼šæå‡ºåŸºäºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„å›¾åƒåˆ†å‰²æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20803v1" onclick="toggleFavorite(this, '2510.20803v1', 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/251020726v1-autoscape-geometry-consistent-long-horizon-scene-generation.html">AutoScape: Geometry-Consistent Long-Horizon Scene Generation</a></td>
  <td>AutoScapeï¼šæå‡ºå‡ ä½•ä¸€è‡´çš„é•¿æ—¶ç¨‹é©¾é©¶åœºæ™¯ç”Ÿæˆæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.20726v1" onclick="toggleFavorite(this, '2510.20726v1', 'AutoScape: Geometry-Consistent Long-Horizon Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)