---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-24
---

# cs.CVï¼ˆ2025-10-24ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251021122v2-noisygrpo-incentivizing-multimodal-cot-reasoning-via-noise-injection.html">NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation</a></td>
  <td>NoisyGRPOï¼šé€šè¿‡å™ªå£°æ³¨å…¥å’Œè´å¶æ–¯ä¼°è®¡æ¿€åŠ±å¤šæ¨¡æ€CoTæ¨ç†</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21122v2" onclick="toggleFavorite(this, '2510.21122v2', 'NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251021664v1-foundation-models-in-dermatopathology-skin-tissue-classification.html">Foundation Models in Dermatopathology: Skin Tissue Classification</a></td>
  <td>åˆ©ç”¨çš®è‚¤ç—…ç†å­¦Foundation Modelè¿›è¡Œçš®è‚¤ç»„ç»‡åˆ†ç±»ï¼Œæå‡è¯Šæ–­æ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21664v1" onclick="toggleFavorite(this, '2510.21664v1', 'Foundation Models in Dermatopathology: Skin Tissue Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251021635v1-dap-mae-domain-adaptive-point-cloud-masked-autoencoder-for-effective.html">DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning</a></td>
  <td>DAP-MAEï¼šé¢†åŸŸè‡ªé€‚åº”ç‚¹äº‘æ©ç è‡ªç¼–ç å™¨ï¼Œæå‡è·¨åŸŸå­¦ä¹ æ•ˆæœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21635v1" onclick="toggleFavorite(this, '2510.21635v1', 'DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251021311v1-finers-fine-grained-reasoning-and-segmentation-of-small-objects-with.html">FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning</a></td>
  <td>æå‡ºFineRSï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ è§£å†³MLLMåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­å°ç›®æ ‡ç²¾ç»†æ¨ç†ä¸åˆ†å‰²éš¾é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21311v1" onclick="toggleFavorite(this, '2510.21311v1', 'FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251021447v1-physworld-from-real-videos-to-world-models-of-deformable-objects-via.html">PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis</a></td>
  <td>PhysWorldï¼šé€šè¿‡ç‰©ç†æ„ŸçŸ¥æ¼”ç¤ºåˆæˆï¼Œä»çœŸå®è§†é¢‘æ„å»ºå¯å˜å½¢å¯¹è±¡çš„äº¤äº’å¼ä¸–ç•Œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21447v1" onclick="toggleFavorite(this, '2510.21447v1', 'PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251021682v1-worldgrow-generating-infinite-3d-world.html">WorldGrow: Generating Infinite 3D World</a></td>
  <td>WorldGrowï¼šæå‡ºæ— é™3Dä¸–ç•Œç”Ÿæˆæ¡†æ¶ï¼Œè§£å†³åœºæ™¯çº§ç”Ÿæˆéš¾é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21682v1" onclick="toggleFavorite(this, '2510.21682v1', 'WorldGrow: Generating Infinite 3D World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251021649v1-a-dynamic-knowledge-distillation-method-based-on-the-gompertz-curve.html">A Dynamic Knowledge Distillation Method Based on the Gompertz Curve</a></td>
  <td>æå‡ºGompertz-CNNï¼Œåˆ©ç”¨Gompertzæ›²çº¿åŠ¨æ€è°ƒæ•´çŸ¥è¯†è’¸é¦ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21649v1" onclick="toggleFavorite(this, '2510.21649v1', 'A Dynamic Knowledge Distillation Method Based on the Gompertz Curve')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251021167v1-blockwise-flow-matching-improving-flow-matching-models-for-efficient.html">Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation</a></td>
  <td>æå‡ºBlockwise Flow Matchingï¼Œæå‡Flow Matchingæ¨¡å‹ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21167v1" onclick="toggleFavorite(this, '2510.21167v1', 'Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251021079v1-waveseg-enhancing-segmentation-precision-via-high-frequency-prior-an.html">WaveSeg: Enhancing Segmentation Precision via High-Frequency Prior and Mamba-Driven Spectrum Decomposition</a></td>
  <td>WaveSegï¼šåˆ©ç”¨é«˜é¢‘å…ˆéªŒå’ŒMambaé©±åŠ¨çš„é¢‘è°±åˆ†è§£å¢å¼ºåˆ†å‰²ç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21079v1" onclick="toggleFavorite(this, '2510.21079v1', 'WaveSeg: Enhancing Segmentation Precision via High-Frequency Prior and Mamba-Driven Spectrum Decomposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/251021111v1-physvlm-avr-active-visual-reasoning-for-multimodal-large-language-mo.html">PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments</a></td>
  <td>æå‡ºPhysVLM-AVRä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„è§†è§‰æ¨ç†é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21111v1" onclick="toggleFavorite(this, '2510.21111v1', 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251021182v1-kbe-dme-dynamic-multimodal-evaluation-via-knowledge-enhanced-benchma.html">KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution</a></td>
  <td>æå‡ºKBEï¼Œé€šè¿‡çŸ¥è¯†å¢å¼ºåŸºå‡†æ¼”åŒ–å®ç°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„åŠ¨æ€è¯„ä¼°</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21182v1" onclick="toggleFavorite(this, '2510.21182v1', 'KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251021518v1-head-pursuit-probing-attention-specialization-in-multimodal-transfor.html">Head Pursuit: Probing Attention Specialization in Multimodal Transformers</a></td>
  <td>æå‡ºä¸€ç§åŸºäºä¿¡å·å¤„ç†çš„æ³¨æ„åŠ›å¤´åˆ†ææ–¹æ³•ï¼Œç”¨äºç†è§£å’Œç¼–è¾‘å¤šæ¨¡æ€Transformeræ¨¡å‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21518v1" onclick="toggleFavorite(this, '2510.21518v1', 'Head Pursuit: Probing Attention Specialization in Multimodal Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251021449v1-monitor-exploiting-large-language-models-with-instruction-for-online.html">MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection</a></td>
  <td>MoniTorï¼šåˆ©ç”¨æŒ‡ä»¤é©±åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œåœ¨çº¿è§†é¢‘å¼‚å¸¸æ£€æµ‹ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21449v1" onclick="toggleFavorite(this, '2510.21449v1', 'MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251021160v1-towards-physics-informed-spatial-intelligence-with-human-priors-an-a.html">Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study</a></td>
  <td>æå‡ºSIGç»“æ„åŒ–ç©ºé—´æ™ºèƒ½ç½‘æ ¼ï¼Œæå‡è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21160v1" onclick="toggleFavorite(this, '2510.21160v1', 'Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251022045v1-vlm-slideeval-evaluating-vlms-on-structured-comprehension-and-pertur.html">VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT</a></td>
  <td>VLM-SlideEvalï¼šè¯„ä¼°VLMåœ¨PPTç»“æ„åŒ–ç†è§£å’Œæ‰°åŠ¨æ•æ„Ÿæ€§ä¸Šçš„æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22045v1" onclick="toggleFavorite(this, '2510.22045v1', 'VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251021114v1-controllable-lpmoe-adapting-to-challenging-object-segmentation-via-d.html">Controllable-LPMoE: Adapting to Challenging Object Segmentation via Dynamic Local Priors from Mixture-of-Experts</a></td>
  <td>Controllable-LPMoEï¼šé€šè¿‡åŠ¨æ€å±€éƒ¨å…ˆéªŒæ··åˆä¸“å®¶ç½‘ç»œæå‡ç›®æ ‡åˆ†å‰²æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21114v1" onclick="toggleFavorite(this, '2510.21114v1', 'Controllable-LPMoE: Adapting to Challenging Object Segmentation via Dynamic Local Priors from Mixture-of-Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251021441v1-openhype-hyperbolic-embeddings-for-hierarchical-open-vocabulary-radi.html">OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields</a></td>
  <td>OpenHypeï¼šæå‡ºåŸºäºåŒæ›²åµŒå…¥çš„å¼€æ”¾è¯æ±‡ç¥ç»è¾å°„åœºï¼Œç”¨äºå»ºæ¨¡åœºæ™¯å±‚çº§ç»“æ„ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21441v1" onclick="toggleFavorite(this, '2510.21441v1', 'OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251021069v1-zing-3d-zero-shot-incremental-3d-scene-graphs-via-vision-language-mo.html">ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models</a></td>
  <td>ZING-3Dï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å®ç°é›¶æ ·æœ¬å¢é‡å¼3Dåœºæ™¯å›¾æ„å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21069v1" onclick="toggleFavorite(this, '2510.21069v1', 'ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251021307v2-towards-physically-executable-3d-gaussian-for-embodied-navigation.html">Towards Physically Executable 3D Gaussian for Embodied Navigation</a></td>
  <td>æå‡ºSAGE-3Dï¼Œå¢å¼º3Dé«˜æ–¯è¡¨è¾¾çš„è¯­ä¹‰å’Œç‰©ç†å¯æ‰§è¡Œæ€§ï¼Œç”¨äºå…·èº«å¯¼èˆªã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21307v2" onclick="toggleFavorite(this, '2510.21307v2', 'Towards Physically Executable 3D Gaussian for Embodied Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251021432v1-artilatent-realistic-articulated-3d-object-generation-via-structured.html">ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents</a></td>
  <td>ArtiLatentï¼šé€šè¿‡ç»“æ„åŒ–éšç©ºé—´ç”Ÿæˆé€¼çœŸå¯åŠ¨3Dç‰©ä½“</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21432v1" onclick="toggleFavorite(this, '2510.21432v1', 'ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251021356v1-gaze-vlmbridging-gaze-and-vlms-through-attention-regularization-for-.html">Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding</a></td>
  <td>Gaze-VLMï¼šé€šè¿‡æ³¨è§†æ­£åˆ™åŒ–å¢å¼ºVLMçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç†è§£èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21356v1" onclick="toggleFavorite(this, '2510.21356v1', 'Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251024767v1-towards-fine-grained-human-motion-video-captioning.html">Towards Fine-Grained Human Motion Video Captioning</a></td>
  <td>æå‡ºè¿åŠ¨å¢å¼ºçš„å­—å¹•æ¨¡å‹(M-ACM)ï¼Œç”¨äºç”Ÿæˆç»†ç²’åº¦çš„äººä½“è¿åŠ¨è§†é¢‘æè¿°ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.24767v1" onclick="toggleFavorite(this, '2510.24767v1', 'Towards Fine-Grained Human Motion Video Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251021512v1-towards-a-golden-classifier-free-guidance-path-via-foresight-fixed-p.html">Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations</a></td>
  <td>æå‡ºåŸºäºå‰ç»å®šç‚¹è¿­ä»£çš„é»„é‡‘æ— åˆ†ç±»å™¨å¼•å¯¼è·¯å¾„ï¼Œæå‡æ–‡å›¾ç”Ÿæˆè´¨é‡ä¸æ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21512v1" onclick="toggleFavorite(this, '2510.21512v1', 'Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)