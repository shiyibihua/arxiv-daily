---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-15
---

# cs.CVï¼ˆ2025-12-15ï¼‰

ğŸ“Š å…± **24** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (11 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251213147v1-starrygazer-leveraging-monocular-depth-estimation-models-for-domain-.html">StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion</a></td>
  <td>StarryGazerï¼šåˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹å®ç°é¢†åŸŸæ— å…³çš„å•æ·±åº¦å›¾åƒè¡¥å…¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13147v1" onclick="toggleFavorite(this, '2512.13147v1', 'StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251213796v1-nexels-neurally-textured-surfels-for-real-time-novel-view-synthesis-.html">Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries</a></td>
  <td>æå‡ºåŸºäºç¥ç»çº¹ç†Surfelçš„æ–°è§†è§’åˆæˆæ–¹æ³•ï¼Œåœ¨ç¨€ç–å‡ ä½•ä¸‹å®ç°å®æ—¶æ¸²æŸ“ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13796v1" onclick="toggleFavorite(this, '2512.13796v1', 'Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251213639v1-charge-a-comprehensive-novel-view-synthesis-benchmark-and-dataset-to.html">Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All</a></td>
  <td>æå‡ºChargeæ•°æ®é›†ï¼Œç”¨äºé«˜è´¨é‡æ–°è§†è§’åˆæˆçš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13639v1" onclick="toggleFavorite(this, '2512.13639v1', 'Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251213411v1-computer-vision-training-dataset-generation-for-robotic-environments.html">Computer vision training dataset generation for robotic environments using Gaussian splatting</a></td>
  <td>æå‡ºåŸºäºé«˜æ–¯æº…å°„çš„æœºå™¨äººç¯å¢ƒè®¡ç®—æœºè§†è§‰è®­ç»ƒæ•°æ®é›†ç”Ÿæˆæµç¨‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13411v1" onclick="toggleFavorite(this, '2512.13411v1', 'Computer vision training dataset generation for robotic environments using Gaussian splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251213177v2-mmdrive-interactive-scene-understanding-beyond-vision-with-multi-rep.html">MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</a></td>
  <td>MMDriveï¼šæå‡ºå¤šæ¨¡æ€èåˆçš„äº¤äº’å¼åœºæ™¯ç†è§£æ¡†æ¶ï¼Œè¶…è¶Šè§†è§‰å±€é™</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13177v2" onclick="toggleFavorite(this, '2512.13177v2', 'MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251213008v1-twlr-text-guided-weakly-supervised-lesion-localization-and-severity-.html">TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</a></td>
  <td>æå‡ºTWLRæ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬å¼•å¯¼çš„å¼±ç›‘ç£å­¦ä¹ è¿›è¡Œç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†çº§ä¸ç—…ç¶å®šä½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13008v1" onclick="toggleFavorite(this, '2512.13008v1', 'TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251213680v1-laser-layer-wise-scale-alignment-for-training-free-streaming-4d-reco.html">LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</a></td>
  <td>æå‡ºLASERä»¥è§£å†³æµåª’ä½“4Dé‡å»ºä¸­çš„è®­ç»ƒéœ€æ±‚é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13680v1" onclick="toggleFavorite(this, '2512.13680v1', 'LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251213689v1-litept-lighter-yet-stronger-point-transformer.html">LitePT: Lighter Yet Stronger Point Transformer</a></td>
  <td>LitePTï¼šä¸€ç§æ›´è½»é‡ä½†æ›´å¼ºå¤§çš„ç‚¹äº‘Transformerï¼Œé€šè¿‡å·ç§¯ä¸æ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆç»“åˆæå‡æ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13689v1" onclick="toggleFavorite(this, '2512.13689v1', 'LitePT: Lighter Yet Stronger Point Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251213683v1-i-scene-3d-instance-models-are-implicit-generalizable-spatial-learne.html">I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</a></td>
  <td>I-Sceneï¼šåˆ©ç”¨é¢„è®­ç»ƒ3Då®ä¾‹ç”Ÿæˆå™¨å®ç°å¯æ³›åŒ–çš„éšå¼åœºæ™¯ç©ºé—´å­¦ä¹ </td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13683v1" onclick="toggleFavorite(this, '2512.13683v1', 'I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251213122v1-dept3r-joint-dense-point-tracking-and-3d-reconstruction-of-dynamic-s.html">DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</a></td>
  <td>DePT3Rï¼šå•æ¬¡å‰å‘ä¼ æ’­å®ç°åŠ¨æ€åœºæ™¯çš„è”åˆç¨ å¯†ç‚¹è¿½è¸ªä¸3Dé‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13122v1" onclick="toggleFavorite(this, '2512.13122v1', 'DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251212984v1-vorolight-learning-quality-volumetric-voronoi-meshes-from-general-in.html">VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs</a></td>
  <td>VoroLightï¼šæå‡ºåŸºäºå¯å¾®Voronoiå›¾çš„é€šç”¨è¾“å…¥ä¸‰ç»´å½¢çŠ¶é‡å»ºæ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12984v1" onclick="toggleFavorite(this, '2512.12984v1', 'VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/251213030v1-motus-a-unified-latent-action-world-model.html">Motus: A Unified Latent Action World Model</a></td>
  <td>æå‡ºMotusä»¥è§£å†³å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ç»Ÿä¸€é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13030v1" onclick="toggleFavorite(this, '2512.13030v1', 'Motus: A Unified Latent Action World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251213684v1-recurrent-video-masked-autoencoders.html">Recurrent Video Masked Autoencoders</a></td>
  <td>æå‡ºRVMï¼šä¸€ç§åŸºäºTransformerå¾ªç¯ç¥ç»ç½‘ç»œçš„è§†é¢‘æ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºé«˜æ•ˆè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13684v1" onclick="toggleFavorite(this, '2512.13684v1', 'Recurrent Video Masked Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251213636v2-minddrive-a-vision-language-action-model-for-autonomous-driving-via-.html">MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</a></td>
  <td>MindDriveï¼šæå‡ºåŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13636v2" onclick="toggleFavorite(this, '2512.13636v2', 'MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251213434v1-self-supervised-ultrasound-representation-learning-for-renal-anomaly.html">Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging</a></td>
  <td>æå‡ºåŸºäºè‡ªç›‘ç£å­¦ä¹ çš„USF-MAEæ¨¡å‹ï¼Œç”¨äºäº§å‰è¶…å£°è‚¾è„å¼‚å¸¸è‡ªåŠ¨é¢„æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13434v1" onclick="toggleFavorite(this, '2512.13434v1', 'Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251213874v1-sage-training-smart-any-horizon-agents-for-long-video-reasoning-with.html">SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</a></td>
  <td>æå‡ºSAGEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä»»æ„æ—¶åŸŸAgentï¼Œç”¨äºé•¿è§†é¢‘æ¨ç†ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13874v1" onclick="toggleFavorite(this, '2512.13874v1', 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251213604v1-longvie-2-multimodal-controllable-ultra-long-video-world-model.html">LongVie 2: Multimodal Controllable Ultra-Long Video World Model</a></td>
  <td>LongVie 2ï¼šå¤šæ¨¡æ€å¯æ§è¶…é•¿è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œå®ç°é«˜è´¨é‡é•¿æ—¶åºè§†é¢‘ç”Ÿæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13604v1" onclick="toggleFavorite(this, '2512.13604v1', 'LongVie 2: Multimodal Controllable Ultra-Long Video World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251213095v1-adhint-adaptive-hints-with-difficulty-priors-for-reinforcement-learn.html">ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning</a></td>
  <td>ADHintï¼šåˆ©ç”¨éš¾åº¦å…ˆéªŒçš„è‡ªé€‚åº”æç¤ºå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13095v1" onclick="toggleFavorite(this, '2512.13095v1', 'ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251213421v1-rectok-reconstruction-distillation-along-rectified-flow.html">RecTok: Reconstruction Distillation along Rectified Flow</a></td>
  <td>RecTokï¼šé€šè¿‡æ ¡æ­£æµä¸Šçš„é‡æ„è’¸é¦ï¼Œçªç ´é«˜ç»´è§†è§‰Tokenizersçš„æ€§èƒ½ç“¶é¢ˆ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13421v1" onclick="toggleFavorite(this, '2512.13421v1', 'RecTok: Reconstruction Distillation along Rectified Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251213671v1-agentiad-tool-augmented-single-agent-for-industrial-anomaly-detectio.html">AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</a></td>
  <td>AgentIADï¼šå·¥å…·å¢å¼ºçš„å•æ™ºèƒ½ä½“å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13671v1" onclick="toggleFavorite(this, '2512.13671v1', 'AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251213840v1-molingo-motion-language-alignment-for-text-to-motion-generation.html">MoLingo: Motion-Language Alignment for Text-to-Motion Generation</a></td>
  <td>MoLingoï¼šé€šè¿‡è¿åŠ¨-è¯­è¨€å¯¹é½å®ç°æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆï¼Œè¾¾åˆ°æ–°çš„SOTAã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13840v1" onclick="toggleFavorite(this, '2512.13840v1', 'MoLingo: Motion-Language Alignment for Text-to-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/251213665v1-grab-3d-detecting-ai-generated-videos-from-3d-geometric-temporal-con.html">Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</a></td>
  <td>æå‡ºGrab-3Dï¼Œåˆ©ç”¨3Då‡ ä½•æ—¶åºä¸€è‡´æ€§æ£€æµ‹AIç”Ÿæˆè§†é¢‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13665v1" onclick="toggleFavorite(this, '2512.13665v1', 'Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251213560v1-3d-human-human-interaction-anomaly-detection.html">3D Human-Human Interaction Anomaly Detection</a></td>
  <td>æå‡ºIADNetï¼Œç”¨äºæ£€æµ‹3Däººä½“äº¤äº’ä¸­çš„å¼‚å¸¸è¡Œä¸º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13560v1" onclick="toggleFavorite(this, '2512.13560v1', '3D Human-Human Interaction Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/251213313v1-klingavatar-20-technical-report.html">KlingAvatar 2.0 Technical Report</a></td>
  <td>æå‡ºKlingAvatar 2.0ä»¥è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ•ˆç‡ä¸ä¸€è‡´æ€§é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13313v1" onclick="toggleFavorite(this, '2512.13313v1', 'KlingAvatar 2.0 Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)