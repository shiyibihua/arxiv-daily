---
layout: default
title: Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning
---

# Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning

**arXiv**: [2512.13131v1](https://arxiv.org/abs/2512.13131) | [PDF](https://arxiv.org/pdf/2512.13131.pdf)

**ä½œè€…**: Xin Guo, Yifan Zhao, Jia Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å±‚éšå¼å‘¨æœŸæ€§å­¦ä¹ ä»¥ç»Ÿä¸€ç”Ÿæˆè¯­éŸ³é©±åŠ¨çš„3Dæ‰‹åŠ¿ï¼Œè§£å†³è¿åŠ¨å•å…ƒé—´åè°ƒæ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¯­éŸ³é©±åŠ¨æ‰‹åŠ¿ç”Ÿæˆ` `åˆ†å±‚éšå¼å‘¨æœŸæ€§å­¦ä¹ ` `3Dè¿åŠ¨ç”Ÿæˆ` `å‘¨æœŸæ€§è‡ªç¼–ç å™¨` `å¤šæ¨¡æ€åè°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç«¯åˆ°ç«¯æ–¹æ³•éš¾ä»¥å»ºæ¨¡å¤´ã€èº«ä½“å’Œæ‰‹éƒ¨è¿åŠ¨å•å…ƒé—´çš„å†…åœ¨å…³è”ï¼Œå¯¼è‡´ä¸è‡ªç„¶è¿åŠ¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å‘¨æœŸæ€§è‡ªç¼–ç å™¨æŽ¢ç´¢æ‰‹åŠ¿è¿åŠ¨ç›¸ä½æµå½¢ï¼Œç»“åˆçº§è”å¼•å¯¼å»ºæ¨¡åˆ†å±‚å…³ç³»ï¼Œå¢žå¼ºå¤šæ ·æ€§å’Œåè°ƒæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨3Dè™šæ‹Ÿäººä¸ŠéªŒè¯ï¼Œå®šé‡å’Œå®šæ€§è¯„ä¼°å‡ä¼˜äºŽå½“å‰æœ€ä¼˜æ–¹æ³•ï¼Œä»£ç å’Œæ¨¡åž‹å°†å…¬å¼€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

