---
layout: default
title: What Happens Next? Next Scene Prediction with a Unified Video Model
---

# What Happens Next? Next Scene Prediction with a Unified Video Model

**arXiv**: [2512.13015v1](https://arxiv.org/abs/2512.13015) | [PDF](https://arxiv.org/pdf/2512.13015.pdf)

**ä½œè€…**: Xinjie Li, Zhimin Chen, Rui Zhao, Florian Schiffers, Zhenyu Liao, Vimal Bhat

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNext Scene Predictionä»»åŠ¡ä¸Žç»Ÿä¸€æ¡†æž¶ï¼Œä»¥å¢žå¼ºè§†é¢‘æ¨¡åž‹çš„æ—¶åºä¸Žå› æžœæŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `Next Scene Prediction` `ç»Ÿä¸€è§†é¢‘æ¨¡åž‹` `æ—¶åºæŽ¨ç†` `å› æžœä¸€è‡´æ€§å¥–åŠ±` `å¤šæ¨¡æ€ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»Ÿä¸€è§†é¢‘æ¨¡åž‹åœ¨æ—¶åºæŽ¨ç†æ–¹é¢æ½œåŠ›æœªå……åˆ†æŽ¢ç´¢ï¼Œéœ€é¢„æµ‹æœªæ¥åœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆQwen-VLå’ŒLTXï¼Œé€šè¿‡æ½œåœ¨æŸ¥è¯¢åµŒå…¥å’Œè¿žæŽ¥æ¨¡å—ï¼Œåˆ†ä¸‰é˜¶æ®µè®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–°æ•°æ®é›†ä¸Šå®žçŽ°æœ€ä½³æ€§èƒ½ï¼Œæå‡å¤šæ¨¡æ€ç³»ç»Ÿé¢„æµ‹èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

