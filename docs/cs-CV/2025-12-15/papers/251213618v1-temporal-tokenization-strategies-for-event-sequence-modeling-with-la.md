---
layout: default
title: Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models
---

# Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models

**arXiv**: [2512.13618v1](https://arxiv.org/abs/2512.13618) | [PDF](https://arxiv.org/pdf/2512.13618.pdf)

**ä½œè€…**: Zefang Liu, Nam Nguyen, Yinzhu Quan, Austin Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒå¤šç§æ—¶é—´æ ‡è®°åŒ–ç­–ç•¥ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡åž‹åœ¨äº‹ä»¶åºåˆ—å»ºæ¨¡ä¸­çš„æ€§èƒ½**

**å…³é”®è¯**: `æ—¶é—´æ ‡è®°åŒ–` `äº‹ä»¶åºåˆ—å»ºæ¨¡` `å¤§è¯­è¨€æ¨¡åž‹` `è¿žç»­æ—¶é—´è¡¨ç¤º` `ç»Ÿè®¡åˆ†å¸ƒå¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¿žç»­æ—¶é—´è¡¨ç¤ºæ˜¯äº‹ä»¶åºåˆ—å»ºæ¨¡ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒçŽ°æœ‰ç­–ç•¥å¦‚å­—èŠ‚çº§è¡¨ç¤ºæˆ–æ—¥åŽ†æ ‡è®°çš„ä¼˜åŠ£æœªçŸ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé¦–æ¬¡å®žè¯ç ”ç©¶äº”ç§æ—¶é—´æ ‡è®°åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬æœ´ç´ æ•°å­—å­—ç¬¦ä¸²ã€é«˜ç²¾åº¦å­—èŠ‚çº§è¡¨ç¤ºã€äººç±»è¯­ä¹‰æ—¥åŽ†æ ‡è®°ã€ç»å…¸å‡åŒ€åˆ†ç®±å’Œè‡ªé€‚åº”æ®‹å·®æ ‡é‡é‡åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šå¾®è°ƒå¤§è¯­è¨€æ¨¡åž‹ï¼Œå‘çŽ°æ€§èƒ½å–å†³äºŽæ ‡è®°åŒ–ç­–ç•¥ä¸Žæ•°æ®ç»Ÿè®¡ç‰¹æ€§çš„å¯¹é½ï¼Œæ— å•ä¸€æœ€ä¼˜ç­–ç•¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

