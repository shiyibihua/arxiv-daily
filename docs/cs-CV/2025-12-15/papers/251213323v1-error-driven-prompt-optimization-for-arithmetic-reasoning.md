---
layout: default
title: Error-Driven Prompt Optimization for Arithmetic Reasoning
---

# Error-Driven Prompt Optimization for Arithmetic Reasoning

**arXiv**: [2512.13323v1](https://arxiv.org/abs/2512.13323) | [PDF](https://arxiv.org/pdf/2512.13323.pdf)

**ä½œè€…**: ÃrpÃ¡d PÃ¡ndy, RÃ³bert Lakatos, AndrÃ¡s Hajdu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé”™è¯¯é©±åŠ¨æç¤ºä¼˜åŒ–æ¡†æž¶ï¼Œæå‡å°è¯­è¨€æ¨¡åž‹åœ¨ç®—æœ¯æŽ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ï¼Œé€‚ç”¨äºŽéšç§åˆè§„çš„å·¥ä¸šéƒ¨ç½²ã€‚**

**å…³é”®è¯**: `ç®—æœ¯æŽ¨ç†` `æç¤ºä¼˜åŒ–` `å°è¯­è¨€æ¨¡åž‹` `é”™è¯¯é©±åŠ¨å­¦ä¹ ` `éšç§åˆè§„` `å·¥ä¸šéƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå°è¯­è¨€æ¨¡åž‹åœ¨ç®—æœ¯æŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°æœ‰é™ï¼Œéœ€åœ¨éšç§åˆè§„çŽ¯å¢ƒä¸‹æå‡å‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡èšç±»é”™è¯¯é¢„æµ‹ï¼Œè¿­ä»£ä¼˜åŒ–æç¤ºè§„åˆ™ï¼Œä»¥é”™è¯¯é©±åŠ¨æ–¹å¼æ”¹è¿›æ¨¡åž‹æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Qwen3 4Bæ¨¡åž‹ä¸Šï¼Œè¯¥æ–¹æ³•å°†å‡†ç¡®çŽ‡æå‡è‡³70.8%ï¼Œè¶…è¶ŠGPT-3.5 Turboã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

