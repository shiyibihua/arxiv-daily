---
layout: default
title: USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition
---

# USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition

**arXiv**: [2512.13415v1](https://arxiv.org/abs/2512.13415) | [PDF](https://arxiv.org/pdf/2512.13415.pdf)

**ä½œè€…**: Ahmed Abul Hasanaath, Hamzah Luqman

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUSTMæ¡†æž¶ï¼Œé€šè¿‡ç»Ÿä¸€æ—¶ç©ºå»ºæ¨¡è§£å†³è¿žç»­æ‰‹è¯­è¯†åˆ«ä¸­ç»†ç²’åº¦ç‰¹å¾å’Œé•¿ç¨‹ä¾èµ–é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¿žç»­æ‰‹è¯­è¯†åˆ«` `æ—¶ç©ºå»ºæ¨¡` `Swin Transformer` `è½»é‡çº§é€‚é…å™¨` `RGBè§†é¢‘å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰æ‰‹è¯­è§†é¢‘ä¸­çš„ç»†ç²’åº¦æ‰‹éƒ¨å’Œé¢éƒ¨çº¿ç´¢åŠé•¿ç¨‹æ—¶é—´ä¾èµ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆSwin Transformeréª¨å¹²ä¸Žè½»é‡çº§æ—¶é—´é€‚é…å™¨TAPEï¼Œå®žçŽ°é«˜æ•ˆæ—¶ç©ºç‰¹å¾æå–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PHOENIX14ç­‰æ•°æ®é›†ä¸Šè¾¾åˆ°SOTAæ€§èƒ½ï¼Œä»…ç”¨RGBè§†é¢‘è¶…è¶Šå¤šæ¨¡æ€æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

