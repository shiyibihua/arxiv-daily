---
layout: default
title: TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding
---

# TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding

**arXiv**: [2512.13511v1](https://arxiv.org/abs/2512.13511) | [PDF](https://arxiv.org/pdf/2512.13511.pdf)

**ä½œè€…**: Piyush Bagad, Andrew Zisserman

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTARAæ–¹æ³•ï¼Œæ— éœ€è§†é¢‘æ•°æ®é€‚é…MLLMsä¸ºæ—¶é—´æ„ŸçŸ¥è§†é¢‘-æ–‡æœ¬æ£€ç´¢æ¨¡åž‹**

**å…³é”®è¯**: `è§†é¢‘æ£€ç´¢` `æ—¶é—´æ„ŸçŸ¥` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `é›¶æ ·æœ¬æ€§èƒ½` `åµŒå…¥æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæž„å»ºé€šç”¨æ—¶é—´æ„ŸçŸ¥è§†é¢‘-æ–‡æœ¬åµŒå…¥æ¨¡åž‹ï¼Œç”¨äºŽè§†é¢‘æ£€ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç®€å•é«˜æ•ˆé…æ–¹TARAï¼Œé€‚é…å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œæ— éœ€è§†é¢‘æ•°æ®å®žçŽ°æ—¶é—´æ„ŸçŸ¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ—¶é—´å¯¹ç«‹åŠ¨ä½œåŸºå‡†ä¸Šè¶…è¶ŠçŽ°æœ‰æ¨¡åž‹ï¼Œå¹¶åœ¨å¦å®šæ„ŸçŸ¥ã€åŠ¨è¯å‰¯è¯ç†è§£æ–¹é¢è¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

