---
layout: default
title: SACn: Soft Actor-Critic with n-step Returns
---

# SACn: Soft Actor-Critic with n-step Returns

**arXiv**: [2512.13165v1](https://arxiv.org/abs/2512.13165) | [PDF](https://arxiv.org/pdf/2512.13165.pdf)

**ä½œè€…**: Jakub Åyskawa, Jakub Lewandowski, PaweÅ‚ WawrzyÅ„ski

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSACnç®—æ³•ï¼Œç»“åˆnæ­¥å›žæŠ¥ä¸Žç¨³å®šé‡è¦æ€§é‡‡æ ·ï¼Œè§£å†³SACåœ¨ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸­çš„åå·®é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¦»ç­–ç•¥ç®—æ³•` `næ­¥å›žæŠ¥` `é‡è¦æ€§é‡‡æ ·` `ç†µä¼°è®¡` `MuJoCoçŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSACç»“åˆnæ­¥å›žæŠ¥æ—¶ï¼Œå› åŠ¨ä½œåˆ†å¸ƒå˜åŒ–å¼•å…¥åå·®ï¼Œé‡è¦æ€§é‡‡æ ·å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ•°å€¼ç¨³å®šçš„é‡è¦æ€§é‡‡æ ·ï¼Œç®€åŒ–è¶…å‚æ•°é€‰æ‹©ï¼Œå¹¶å¼•å…¥Ï„-é‡‡æ ·ç†µä¼°è®¡ä»¥é™ä½Žæ–¹å·®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MuJoCoæ¨¡æ‹ŸçŽ¯å¢ƒä¸­éªŒè¯SACnç®—æ³•ï¼Œæå‡æ”¶æ•›é€Ÿåº¦ä¸Žç¨³å®šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $Ï„$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

