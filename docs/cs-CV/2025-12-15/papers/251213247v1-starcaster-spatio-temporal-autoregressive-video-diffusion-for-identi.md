---
layout: default
title: STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits
---

# STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits

**arXiv**: [2512.13247v1](https://arxiv.org/abs/2512.13247) | [PDF](https://arxiv.org/pdf/2512.13247.pdf)

**ä½œè€…**: Foivos Paraperas Papantoniou, Stathis Galanakis, Rolandos Alexandros Potamias, Bernhard Kainz, Stefanos Zafeiriou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTARCasterä»¥è§£å†³èº«ä»½æ„ŸçŸ¥å’Œè‡ªç”±è§†è§’çš„è¯´è¯è‚–åƒè§†é¢‘ç”Ÿæˆé—®é¢˜**

**å…³é”®è¯**: `è¯´è¯è‚–åƒåŠ¨ç”»` `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `èº«ä»½æ„ŸçŸ¥` `è‡ªç”±è§†è§’åˆæˆ` `éŸ³é¢‘-è§†è§‰åŒæ­¥` `è‡ªå›žå½’è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰2Dè¯­éŸ³é©±åŠ¨è§†é¢‘æ¨¡åž‹ä¾èµ–å‚è€ƒå¯¼è‡´è¿åŠ¨å¤šæ ·æ€§æœ‰é™ï¼Œ3Dæ„ŸçŸ¥åŠ¨ç”»åŸºäºŽé¢„è®­ç»ƒç”Ÿæˆå™¨å¯¼è‡´é‡å»ºä¸å®Œç¾Žå’Œèº«ä»½æ¼‚ç§»
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è½¯èº«ä»½çº¦æŸå’Œéšå¼3Dæ„ŸçŸ¥ï¼Œé€šè¿‡ç»„åˆæ–¹æ³•ä»Žèº«ä»½æ„ŸçŸ¥è¿åŠ¨å»ºæ¨¡åˆ°éŸ³é¢‘-è§†è§‰åŒæ­¥å†åˆ°æ–°è§†è§’åŠ¨ç”»
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šå…ˆå‰æ–¹æ³•ï¼Œæœ‰æ•ˆæ³›åŒ–åˆ°ä¸åŒä»»åŠ¡å’Œèº«ä»½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

