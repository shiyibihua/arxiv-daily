---
layout: default
title: Directional Textual Inversion for Personalized Text-to-Image Generation
---

# Directional Textual Inversion for Personalized Text-to-Image Generation

**arXiv**: [2512.13672v1](https://arxiv.org/abs/2512.13672) | [PDF](https://arxiv.org/pdf/2512.13672.pdf)

**ä½œè€…**: Kunhee Kim, NaHyeon Park, Kibeom Hong, Hyunjung Shim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–¹å‘æ€§æ–‡æœ¬åè½¬ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒä¸ªæ€§åŒ–ä¸­åµŒå…¥èŒƒæ•°è†¨èƒ€å¯¼è‡´çš„æç¤ºæ¡ä»¶é€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒä¸ªæ€§åŒ–` `æ–¹å‘æ€§æ–‡æœ¬åè½¬` `åµŒå…¥èŒƒæ•°è†¨èƒ€` `é»Žæ›¼ä¼˜åŒ–` `è¶…çƒé¢å‚æ•°åŒ–` `è¯­ä¹‰æ’å€¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬åè½¬åœ¨å¤æ‚æç¤ºä¸‹å¤±è´¥ï¼ŒæºäºŽåµŒå…¥èŒƒæ•°è†¨èƒ€å¯¼è‡´é¢„å½’ä¸€åŒ–Transformerä¸­æç¤ºæ¡ä»¶é€€åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šå›ºå®šåµŒå…¥èŒƒæ•°ï¼Œé€šè¿‡é»Žæ›¼SGDåœ¨å•ä½è¶…çƒé¢ä¸Šä¼˜åŒ–æ–¹å‘ï¼Œé‡‡ç”¨å†¯Â·ç±³å¡žæ–¯-è´¹èˆå°”å…ˆéªŒç®€åŒ–æ¢¯åº¦è®¡ç®—
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸ªæ€§åŒ–ä»»åŠ¡ä¸­æå‡æ–‡æœ¬ä¿çœŸåº¦ï¼Œä¿æŒä¸»ä½“ç›¸ä¼¼æ€§ï¼Œå¹¶æ”¯æŒæ¦‚å¿µé—´å¹³æ»‘è¯­ä¹‰æ’å€¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

