---
layout: default
title: Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks
---

# Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks

**arXiv**: [2512.13410v1](https://arxiv.org/abs/2512.13410) | [PDF](https://arxiv.org/pdf/2512.13410.pdf)

**ä½œè€…**: VÃ­tor M. Hanriot, Luiz C. B. Torres, AntÃ´nio P. Braga

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽGabrielå›¾çš„å¤šå…ƒå¤§é—´éš”åˆ†ç±»å™¨ï¼Œç»Ÿä¸€æ”¯æŒå‘é‡ä¸Žç¥žç»ç½‘ç»œæ–¹æ³•**

**å…³é”®è¯**: `Gabrielå›¾åˆ†ç±»` `å¤§é—´éš”åˆ†ç±»å™¨` `ç¥žç»ç½‘ç»œæž¶æž„` `å›¾æ­£åˆ™åŒ–` `å¤šå…ƒåˆ†ç±»` `æ”¯æŒå‘é‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šGabrielå›¾åœ¨å¤šå…ƒåˆ†ç±»ä¸­çš„åº”ç”¨æ‰©å±•ä¸Žä¼˜åŒ–ï¼Œæå‡åˆ†ç±»æ€§èƒ½ä¸Žè®¡ç®—æ•ˆçŽ‡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¹³æ»‘æ¿€æ´»å‡½æ•°ã€ç»“æž„æ”¯æŒå‘é‡ç¥žç»å…ƒã€æ–°å­å›¾è·ç¦»æˆå‘˜å‡½æ•°å’Œé«˜æ•ˆGabrielå›¾é‡è®¡ç®—ç®—æ³•
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºæ–¹æ³•ä¼˜äºŽå…ˆå‰Gabrielå›¾åˆ†ç±»å™¨ï¼Œç»Ÿè®¡ç­‰æ•ˆäºŽæ ‘æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.

