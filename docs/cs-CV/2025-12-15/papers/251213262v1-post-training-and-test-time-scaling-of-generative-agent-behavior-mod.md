---
layout: default
title: Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving
---

# Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving

**arXiv**: [2512.13262v1](https://arxiv.org/abs/2512.13262) | [PDF](https://arxiv.org/pdf/2512.13262.pdf)

**ä½œè€…**: Hyunki Seong, Jeong-Kyun Lee, Heesoo Myeong, Yongho Shin, Hyun-Mook Cho, Duck Hoon Kim, Pranav Desai, Monu Surana

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRBOå’ŒWarm-Kæ–¹æ³•ï¼Œä»¥å¢žå¼ºè‡ªåŠ¨é©¾é©¶ä¸­ç”Ÿæˆå¼æ™ºèƒ½ä½“è¡Œä¸ºæ¨¡åž‹çš„å®‰å…¨æ€§å’Œé—­çŽ¯æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶è¡Œä¸ºå»ºæ¨¡` `å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒ` `æµ‹è¯•æ—¶é‡‡æ ·ç­–ç•¥` `é—­çŽ¯è¯„ä¼°` `å®‰å…¨æ€§èƒ½ä¼˜åŒ–` `ç¾¤ä½“äº¤äº’å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡ä»¿å­¦ä¹ æ¨¡åž‹å­˜åœ¨æ•°æ®é›†åå·®ï¼Œå¯¼è‡´å®‰å…¨å…³é”®åœºæ™¯ä¸‹é²æ£’æ€§ä¸è¶³ï¼Œä¸”å¤šæ•°ç ”ç©¶ä¾èµ–å¼€çŽ¯è¯„ä¼°ï¼Œå¿½ç•¥é—­çŽ¯æ‰§è¡Œä¸­çš„ç´¯ç§¯è¯¯å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šGRBOé€šè¿‡å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒï¼Œåˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿æœ€å¤§åŒ–å’Œäººç±»æ­£åˆ™åŒ–å¾®è°ƒé¢„è®­ç»ƒæ¨¡åž‹ï¼›Warm-Ké‡‡ç”¨çƒ­å¯åŠ¨Top-Ké‡‡æ ·ç­–ç•¥ï¼Œåœ¨æµ‹è¯•æ—¶å¹³è¡¡ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šGRBOä»…ç”¨10%è®­ç»ƒæ•°æ®æå‡å®‰å…¨æ€§èƒ½è¶…40%ï¼Œä¿æŒè¡Œä¸ºçœŸå®žæ€§ï¼›Warm-Kå¢žå¼ºæµ‹è¯•æ—¶è¡Œä¸ºä¸€è‡´æ€§å’Œååº”æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

