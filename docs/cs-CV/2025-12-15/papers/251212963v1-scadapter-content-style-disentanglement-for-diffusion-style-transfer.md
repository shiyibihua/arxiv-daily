---
layout: default
title: SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer
---

# SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer

**arXiv**: [2512.12963v1](https://arxiv.org/abs/2512.12963) | [PDF](https://arxiv.org/pdf/2512.12963.pdf)

**ä½œè€…**: Luan Thanh Trinh, Kenji Doi, Atsuki Osanai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCAdapterä»¥è§£å†³æ‰©æ•£æ¨¡åž‹é£Žæ ¼è¿ç§»ä¸­å†…å®¹-é£Žæ ¼çº ç¼ å’Œç»†èŠ‚ç¼ºå¤±é—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `é£Žæ ¼è¿ç§»` `å†…å®¹-é£Žæ ¼è§£è€¦` `CLIPç‰¹å¾` `å¿«é€ŸæŽ¨ç†` `å›¾åƒç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹åœ¨é£Žæ ¼è¿ç§»ä¸­æ˜“äº§ç”Ÿç»˜ç”»åŒ–ç»“æžœæˆ–ä¸¢å¤±ç»†èŠ‚ï¼ŒçŽ°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ†ç¦»å†…å®¹é£Žæ ¼å’Œé£Žæ ¼å‚è€ƒå†…å®¹ç‰¹å¾
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨CLIPå›¾åƒç©ºé—´åˆ†ç¦»å†…å®¹ä¸Žé£Žæ ¼ç‰¹å¾ï¼Œç»“åˆCSAdaINã€KVSæ³¨å…¥å’Œä¸€è‡´æ€§ç›®æ ‡å®žçŽ°ç²¾ç¡®è¿ç§»
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¼ ç»Ÿå’Œæ‰©æ•£åŸºçº¿ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ— éœ€DDIMåè½¬å’ŒæŽ¨ç†ä¼˜åŒ–ï¼ŒæŽ¨ç†é€Ÿåº¦è‡³å°‘å¿«2å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

