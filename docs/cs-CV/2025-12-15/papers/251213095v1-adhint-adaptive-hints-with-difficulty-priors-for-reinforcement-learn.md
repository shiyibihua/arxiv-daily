---
layout: default
title: ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning
---

# ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning

**arXiv**: [2512.13095v1](https://arxiv.org/abs/2512.13095) | [PDF](https://arxiv.org/pdf/2512.13095.pdf)

**ä½œè€…**: Feng Zhang, Zezhong Tan, Xinhong Ma, Ziqiang Dong, Xi Leng, Jianfei Zhao, Xin Sun, Yang Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºADHintï¼Œé€šè¿‡éš¾åº¦å…ˆéªŒå’ŒåŽéªŒä¼˜åŒ–æç¤ºè°ƒåº¦ä¸Žä¼˜åŠ¿ä¼°è®¡ï¼Œä»¥æå‡å¼ºåŒ–å­¦ä¹ ä¸­çš„æŽ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æç¤ºè°ƒåº¦` `éš¾åº¦å…ˆéªŒ` `æ¢¯åº¦è°ƒåˆ¶` `ä¼˜åŠ¿ä¼°è®¡` `æŽ¨ç†æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºäºŽæç¤ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¿½ç•¥éš¾åº¦å› ç´ ï¼Œå¯¼è‡´å­¦ä¹ ä¸ç¨³å®šå’Œè¿‡åº¦æ¨¡ä»¿ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è‡ªé€‚åº”æç¤ºè°ƒåº¦ã€åŸºäºŽä¸€è‡´æ€§çš„æ¢¯åº¦è°ƒåˆ¶å’Œé€‰æ‹©æ€§æŽ©ç ï¼Œä»¥åŠåŸºäºŽéš¾åº¦åŽéªŒçš„ä¼˜åŠ¿ä¼°è®¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ¨¡æ€ã€å¤šè§„æ¨¡å’Œé¢†åŸŸå®žéªŒä¸­ï¼ŒADHintåœ¨æŽ¨ç†èƒ½åŠ›å’Œåˆ†å¸ƒå¤–æ³›åŒ–ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

