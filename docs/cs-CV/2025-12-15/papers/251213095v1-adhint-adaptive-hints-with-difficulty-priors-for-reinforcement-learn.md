---
layout: default
title: ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning
---

# ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.13095" target="_blank" class="toolbar-btn">arXiv: 2512.13095v1</a>
    <a href="https://arxiv.org/pdf/2512.13095.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13095v1" 
            onclick="toggleFavorite(this, '2512.13095v1', 'ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Feng Zhang, Zezhong Tan, Xinhong Ma, Ziqiang Dong, Xi Leng, Jianfei Zhao, Xin Sun, Yang Yang

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ADHintÔºöÂà©Áî®ÈöæÂ∫¶ÂÖàÈ™åÁöÑËá™ÈÄÇÂ∫îÊèêÁ§∫Âº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÊèêÁ§∫Â≠¶‰π†` `ÈöæÂ∫¶ÂÖàÈ™å` `Ëá™ÈÄÇÂ∫îÊèêÁ§∫` `‰ºòÂäø‰º∞ËÆ°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊèêÁ§∫ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂøΩÁï•‰∫ÜÊ†∑Êú¨ÈöæÂ∫¶ÔºåÂØºËá¥Â≠¶‰π†‰∏çÁ®≥ÂÆöÂíåËøáÂ∫¶Ê®°‰ªøÁ¶ªÁ≠ñÁï•Êï∞ÊçÆ„ÄÇ
2. ADHintÂ∞ÜÊ†∑Êú¨ÈöæÂ∫¶Á∫≥ÂÖ•ÊèêÁ§∫ÊØî‰æãË∞ÉÂ∫¶Âíå‰ºòÂäø‰º∞ËÆ°ÔºåÂπ≥Ë°°Êé¢Á¥¢‰∏éÊ®°‰ªøÔºåÊèêÂçáÂ≠¶‰π†ÊïàÊûú„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåADHintÂú®Â§öÁßçÊ®°ÊÄÅ„ÄÅÊ®°ÂûãËßÑÊ®°ÂíåÈ¢ÜÂüü‰∏≠ÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜÁªìÂêàÁõëÁù£ÂæÆË∞É(SFT)ÂíåÂº∫ÂåñÂ≠¶‰π†(RL)ÁöÑ‰ºòÂäøÔºåÁé∞ÊúâÊñπÊ≥ïÂ∞Ü‚ÄúÊèêÁ§∫‚ÄùÔºàÂÆåÊï¥Êé®ÁêÜËΩ®ËøπÁöÑÂâçÁºÄÁâáÊÆµÔºâÈõÜÊàêÂà∞ÂêéËÆ≠ÁªÉ‰∏≠ÔºåÊó®Âú®ÂÆûÁé∞Âº∫Â§ßÁöÑÁü•ËØÜÊâ©Â±ïÂíåÊé®ÁêÜÊ≥õÂåñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫‰∫éÊèêÁ§∫ÁöÑRLÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•‰∫ÜÂú®Ë∞ÉÂ∫¶ÊèêÁ§∫ÊØî‰æãÂíå‰º∞ËÆ°Áõ∏ÂØπ‰ºòÂäøÊó∂ÁöÑÈöæÂ∫¶ÔºåÂØºËá¥‰∏çÁ®≥ÂÆöÁöÑÂ≠¶‰π†ÂíåËøáÂ∫¶Ê®°‰ªøÁ¶ªÁ≠ñÁï•ÊèêÁ§∫„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜADHintÔºåÂÆÉÂ∞ÜÈöæÂ∫¶‰Ωú‰∏∫ÊèêÁ§∫ÊØî‰æãË∞ÉÂ∫¶ÂíåÁõ∏ÂØπ‰ºòÂäø‰º∞ËÆ°ÁöÑÂÖ≥ÈîÆÂõ†Á¥†Ôºå‰ª•Âú®Êé¢Á¥¢ÂíåÊ®°‰ªø‰πãÈó¥ÂÆûÁé∞Êõ¥Â•ΩÁöÑÊùÉË°°„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÖ∑ÊúâÊ†∑Êú¨ÈöæÂ∫¶ÂÖàÈ™åÁöÑËá™ÈÄÇÂ∫îÊèêÁ§∫ÔºåÂÆÉËØÑ‰º∞Á≠ñÁï•Ê®°Âûã‰∏ãÊØè‰∏™Ê†∑Êú¨ÁöÑÈöæÂ∫¶ÔºåÂπ∂Áõ∏Â∫îÂú∞Ë∞ÉÂ∫¶ÈÄÇÂΩìÁöÑÊèêÁ§∫ÊØî‰æãÊù•ÊåáÂØºÂÖ∂rollout„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂü∫‰∫é‰∏ÄËá¥ÊÄßÁöÑÊ¢ØÂ∫¶Ë∞ÉÂà∂ÂíåÊèêÁ§∫‰øùÊåÅÁöÑÈÄâÊã©ÊÄßÊé©Á†ÅÔºå‰ª•Ë∞ÉÂà∂ÊèêÁ§∫ÂÜÖÁöÑtokenÁ∫ßÂà´Ê¢ØÂ∫¶ÔºåÈò≤Ê≠¢ÊúâÂÅèÂ∑ÆÂíåÁ†¥ÂùèÊÄßÁöÑÊõ¥Êñ∞„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÖ∑ÊúâRolloutÈöæÂ∫¶ÂêéÈ™åÁöÑ‰ºòÂäø‰º∞ËÆ°ÔºåÂÆÉÂà©Áî®ÊúâÊèêÁ§∫ÂíåÊó†ÊèêÁ§∫ÁöÑrolloutÁöÑÁõ∏ÂØπÈöæÂ∫¶Êù•‰º∞ËÆ°ÂÆÉ‰ª¨ÂêÑËá™ÁöÑ‰ºòÂäøÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Âπ≥Ë°°ÁöÑÊõ¥Êñ∞„ÄÇÂú®‰∏çÂêåÁöÑÊ®°ÊÄÅ„ÄÅÊ®°ÂûãËßÑÊ®°ÂíåÈ¢ÜÂüü‰∏≠ËøõË°åÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåADHintÊèê‰æõ‰∫ÜÂçìË∂äÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËÉΩÂäõÔºåÂú®pass@1Âíåavg@8ÊñπÈù¢ÂßãÁªà‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊï∞ÊçÆÈõÜÂ∞ÜÂú®ËÆ∫ÊñáË¢´Êé•ÂèóÂêéÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÊèêÁ§∫ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Âà©Áî®ÊèêÁ§∫‰ø°ÊÅØËøõË°åÁ≠ñÁï•‰ºòÂåñÊó∂ÔºåÂøΩÁï•‰∫Ü‰∏çÂêåÊ†∑Êú¨ÁöÑÈöæÂ∫¶Â∑ÆÂºÇ„ÄÇËøôÂØºËá¥‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢òÔºö‰∏ÄÊòØÊèêÁ§∫ÊØî‰æãÁöÑÂàÜÈÖç‰∏çÂêàÁêÜÔºåÁÆÄÂçïÊ†∑Êú¨ÂèØËÉΩË¢´ËøáÂ∫¶ÊèêÁ§∫ÔºåËÄåÂõ∞ÈöæÊ†∑Êú¨ÂàôÁº∫‰πèË∂≥Â§üÁöÑÊåáÂØºÔºõ‰∫åÊòØ‰ºòÂäøÂáΩÊï∞‰º∞ËÆ°‰∏çÂáÜÁ°ÆÔºåÊó†Ê≥ïÂå∫ÂàÜÊèêÁ§∫Â∏¶Êù•ÁöÑÁúüÂÆûÊî∂ÁõäÂíåÊ†∑Êú¨Êú¨Ë∫´ÁöÑÈöæÂ∫¶Ôºå‰ªéËÄåÂØºËá¥Á≠ñÁï•Â≠¶‰π†‰∏çÁ®≥ÂÆöÔºåÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöADHintÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊ†∑Êú¨ÈöæÂ∫¶‰Ωú‰∏∫ÂÖ≥ÈîÆÂõ†Á¥†ÔºåËûçÂÖ•Âà∞ÊèêÁ§∫ÊØî‰æãÁöÑË∞ÉÂ∫¶Âíå‰ºòÂäøÂáΩÊï∞‰º∞ËÆ°‰∏≠„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ÊèêÁ§∫ÊØî‰æãÔºå‰∏∫‰∏çÂêåÈöæÂ∫¶ÁöÑÊ†∑Êú¨Êèê‰æõÂêàÈÄÇÁöÑÊåáÂØºÔºåÂêåÊó∂Âà©Áî®rolloutÁöÑÈöæÂ∫¶ÂêéÈ™åÊù•Êõ¥ÂáÜÁ°ÆÂú∞‰º∞ËÆ°‰ºòÂäøÂáΩÊï∞Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÁ≠ñÁï•Â≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöADHint‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºö1) **Adaptive Hint with Sample Difficulty Prior (AH-SDP)**ÔºöÊ†πÊçÆÁ≠ñÁï•Ê®°ÂûãËØÑ‰º∞Ê†∑Êú¨ÈöæÂ∫¶ÔºåËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ÊèêÁ§∫ÊØî‰æã„ÄÇ2) **Consistency-based Gradient Modulation and Selective Masking for Hint Preservation (CGM-SM)**ÔºöË∞ÉÂà∂ÊèêÁ§∫ÂÜÖÈÉ®ÁöÑÊ¢ØÂ∫¶ÔºåÂπ∂ËøõË°åÈÄâÊã©ÊÄßÊé©Á†ÅÔºå‰ª•Èò≤Ê≠¢ÊèêÁ§∫‰ø°ÊÅØË¢´Á†¥Âùè„ÄÇ3) **Advantage Estimation with Rollout Difficulty Posterior (AE-RDP)**ÔºöÂà©Áî®ÊúâÊèêÁ§∫ÂíåÊó†ÊèêÁ§∫rolloutÁöÑÈöæÂ∫¶ÂêéÈ™åÔºåÊõ¥ÂáÜÁ°ÆÂú∞‰º∞ËÆ°‰ºòÂäøÂáΩÊï∞„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖàÂà©Áî®AH-SDPÁ°ÆÂÆöÊèêÁ§∫ÊØî‰æãÔºåÁÑ∂ÂêéËøõË°årolloutÔºåÊé•ÁùÄÂà©Áî®CGM-SM‰øùÊä§ÊèêÁ§∫‰ø°ÊÅØÔºåÊúÄÂêéÂà©Áî®AE-RDP‰º∞ËÆ°‰ºòÂäøÂáΩÊï∞Âπ∂Êõ¥Êñ∞Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöADHintÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊ†∑Êú¨ÈöæÂ∫¶ÊòæÂºèÂú∞Âª∫Ê®°Âà∞ÊèêÁ§∫Âº∫ÂåñÂ≠¶‰π†ËøáÁ®ã‰∏≠„ÄÇAH-SDPÈÄöËøáÊ†∑Êú¨ÈöæÂ∫¶ÂÖàÈ™åËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ÊèêÁ§∫ÊØî‰æãÔºåCGM-SMÈÄöËøáÊ¢ØÂ∫¶Ë∞ÉÂà∂ÂíåÈÄâÊã©ÊÄßÊé©Á†Å‰øùÊä§ÊèêÁ§∫‰ø°ÊÅØÔºåAE-RDPÈÄöËøárolloutÈöæÂ∫¶ÂêéÈ™åÊõ¥ÂáÜÁ°ÆÂú∞‰º∞ËÆ°‰ºòÂäøÂáΩÊï∞„ÄÇËøô‰∫õÂàõÊñ∞ÂÖ±Âêå‰ΩúÁî®Ôºå‰ΩøÂæóADHintËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÊèêÁ§∫‰ø°ÊÅØÔºåÊèêÂçáÁ≠ñÁï•Â≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAH-SDP‰∏≠ÔºåÊ†∑Êú¨ÈöæÂ∫¶ÈÄöËøáÁ≠ñÁï•Ê®°ÂûãÁöÑÁΩÆ‰ø°Â∫¶ÊàñÈ¢ÑÊµãÊ¶ÇÁéáÊù•Ë°°ÈáèÔºåÊèêÁ§∫ÊØî‰æãÊ†πÊçÆÊ†∑Êú¨ÈöæÂ∫¶ËøõË°åË∞ÉÊï¥ÔºåÈöæÂ∫¶È´òÁöÑÊ†∑Êú¨ÂàÜÈÖçÊõ¥È´òÁöÑÊèêÁ§∫ÊØî‰æã„ÄÇCGM-SMÈÄöËøáËÆ°ÁÆóÊèêÁ§∫ÂÜÖÈÉ®tokenÁöÑ‰∏ÄËá¥ÊÄßÊù•Ë∞ÉÂà∂Ê¢ØÂ∫¶ÔºåÂπ∂ÂØπ‰∏ç‰∏ÄËá¥ÁöÑtokenËøõË°åÊé©Á†ÅÔºå‰ª•Èò≤Ê≠¢ÊèêÁ§∫‰ø°ÊÅØË¢´Á†¥Âùè„ÄÇAE-RDPÂà©Áî®ÊúâÊèêÁ§∫ÂíåÊó†ÊèêÁ§∫rolloutÁöÑÂ•ñÂä±ÂíåÈöæÂ∫¶‰ø°ÊÅØÔºåËÆ°ÁÆórolloutÈöæÂ∫¶ÂêéÈ™åÔºåÂπ∂Â∞ÜÂÖ∂Áî®‰∫é‰ºòÂäøÂáΩÊï∞ÁöÑ‰º∞ËÆ°‰∏≠„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADHintÂú®Â§ö‰∏™‰ªªÂä°‰∏äÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÔºåADHintÂú®pass@1Âíåavg@8ÊåáÊ†á‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåADHintÂú®Êüê‰∫õ‰ªªÂä°‰∏äÁöÑpass@1ÊåáÊ†áÊèêÂçá‰∫ÜË∂ÖËøá5‰∏™ÁôæÂàÜÁÇπÔºåË°®ÊòéÂÖ∂ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñÊÄßËÉΩ„ÄÇËøô‰∫õÁªìÊûúËØÅÊòé‰∫ÜADHintÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ADHintÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂà©Áî®ÊèêÁ§∫‰ø°ÊÅØËøõË°åÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂØπËØùÁîüÊàê„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÊú∫Âô®‰∫∫ÊéßÂà∂Á≠â„ÄÇÈÄöËøáÂºïÂÖ•ÈöæÂ∫¶ÂÖàÈ™åÔºåADHintËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÊèêÁ§∫‰ø°ÊÅØÔºåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñÊÄßËÉΩÔºå‰ªéËÄåÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂèñÂæóÊõ¥Â•ΩÁöÑÊïàÊûú„ÄÇÊ≠§Â§ñÔºåADHintËøòÂèØ‰ª•Â∫îÁî®‰∫éÊïôËÇ≤È¢ÜÂüüÔºå‰æãÂ¶Ç‰∏™ÊÄßÂåñËæÖÂØºÁ≥ªÁªüÔºåÊ†πÊçÆÂ≠¶ÁîüÁöÑÂ≠¶‰π†ÈöæÂ∫¶Ëá™ÈÄÇÂ∫îÂú∞Êèê‰æõÊèêÁ§∫‰ø°ÊÅØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

