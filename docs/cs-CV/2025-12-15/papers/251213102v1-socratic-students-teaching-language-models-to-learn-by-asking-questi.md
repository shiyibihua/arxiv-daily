---
layout: default
title: Socratic Students: Teaching Language Models to Learn by Asking Questions
---

# Socratic Students: Teaching Language Models to Learn by Asking Questions

**arXiv**: [2512.13102v1](https://arxiv.org/abs/2512.13102) | [PDF](https://arxiv.org/pdf/2512.13102.pdf)

**ä½œè€…**: Rajeev Bhatt Ambati, Tianyi Niu, Aashu Singh, Shlok Mishra, Shashank Srivastava, Snigdha Chaturvedi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå­¦ç”Ÿä¸»å¯¼æé—®æ–¹æ³•ä»¥æå‡è¯­è¨€æ¨¡åž‹åœ¨åŠ¨æ€äº¤äº’ä¸­çš„å­¦ä¹ æ•ˆçŽ‡**

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹å­¦ä¹ ` `åŠ¨æ€äº¤äº’` `æé—®ç­–ç•¥` `ç›´æŽ¥åå¥½ä¼˜åŒ–` `æ•™è‚²è¾…å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯­è¨€æ¨¡åž‹åœ¨é™æ€äº¤äº’ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦ä¸»åŠ¨èŽ·å–ä¿¡æ¯çš„åŠ¨æ€åœºæ™¯ï¼ˆå¦‚æ•™è‚²è¾…å¯¼ï¼‰ä¸­æ•ˆçŽ‡ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å­¦ç”Ÿæ¨¡åž‹ä¸»åŠ¨å‘æ•™å¸ˆæé—®ï¼Œå¹¶åˆ©ç”¨ç›´æŽ¥åå¥½ä¼˜åŒ–è®­ç»ƒæå‡é—®é¢˜è´¨é‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­ï¼Œå­¦ç”Ÿä¸»å¯¼æ–¹æ³•ç›¸æ¯”é™æ€åŸºçº¿å¸¦æ¥è‡³å°‘0.5çš„ç»å¯¹Pass@kæå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

