---
layout: default
title: SneakPeek: Future-Guided Instructional Streaming Video Generation
---

# SneakPeek: Future-Guided Instructional Streaming Video Generation

**arXiv**: [2512.13019v1](https://arxiv.org/abs/2512.13019) | [PDF](https://arxiv.org/pdf/2512.13019.pdf)

**ä½œè€…**: Cheeun Hong, German Barquero, Fadime Sener, Markos Georgopoulos, Edgar SchÃ¶nfeld, Stefan Popov, Yuming Du, Oscar MaÃ±as, Albert Pumarola

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSneakPeekæ¡†æž¶ï¼Œé€šè¿‡æœªæ¥å¼•å¯¼çš„æµå¼ç”Ÿæˆè§£å†³æ•™å­¦è§†é¢‘ä¸­é•¿åºåˆ—åŠ¨ä½œçš„æ—¶åºä¸€è‡´æ€§å’Œå¯æŽ§æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ•™å­¦è§†é¢‘ç”Ÿæˆ` `æ—¶åºä¸€è‡´æ€§` `æ‰©æ•£æ¨¡åž‹` `æµå¼ç”Ÿæˆ` `å¤šæ­¥éª¤æŽ§åˆ¶` `æœªæ¥é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨ç”Ÿæˆå¤šæ­¥éª¤æ•™å­¦è§†é¢‘æ—¶ï¼Œéš¾ä»¥ä¿æŒæ—¶åºä¸€è‡´æ€§å’Œå¯æŽ§æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥é¢„æµ‹å› æžœé€‚åº”ã€æœªæ¥å¼•å¯¼è‡ªå¼ºåˆ¶å’Œå¤šæç¤ºæ¡ä»¶åŒ–ï¼Œä»¥å¢žå¼ºä¸€è‡´æ€§å’Œäº¤äº’æŽ§åˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆæ—¶åºè¿žè´¯ã€è¯­ä¹‰å¿ å®žä¸”å‡†ç¡®éµå¾ªå¤æ‚å¤šæ­¥éª¤æè¿°çš„æ•™å­¦è§†é¢‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

