---
layout: default
title: Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models
---

# Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models

**arXiv**: [2512.13609v1](https://arxiv.org/abs/2512.13609) | [PDF](https://arxiv.org/pdf/2512.13609.pdf)

**ä½œè€…**: Shweta Mahajan, Shreya Kadambi, Hoang Le, Munawar Hayat, Fatih Porikli

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDo-Undoä»»åŠ¡ä¸ŽåŸºå‡†ï¼Œä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨ç‰©ç†åŠ¨ä½œç†è§£ä¸Žç”Ÿæˆä¸­çš„å¯é€†æ€§æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `ç‰©ç†åŠ¨ä½œç†è§£` `å¯é€†æ€§åŸºå‡†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å…·èº«AI` `ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¨¡åž‹ç¼ºä¹å¯¹ç‰©ç†åŠ¨ä½œå¯é€†æ€§çš„ç†è§£ï¼Œéš¾ä»¥æ¨¡æ‹ŸçœŸå®žä¸–ç•Œä¸­çš„å› æžœå˜æ¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡å¯é€†åŠ¨ä½œæ•°æ®é›†ï¼Œè®¾è®¡è®­ç»ƒç­–ç•¥å¼ºåŒ–åŠ¨ä½œä¸€è‡´æ€§ä¸Žç‰©ç†åˆç†æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºå½“å‰æ¨¡åž‹åœ¨ç‰©ç†å¯é€†ä»»åŠ¡ä¸Šè¡¨çŽ°ä¸ä½³ï¼Œçªæ˜¾è¯¥ä»»åŠ¡å¯¹å…·èº«AIå’Œç‰©ç†æ„ŸçŸ¥ç”Ÿæˆçš„é‡è¦æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

