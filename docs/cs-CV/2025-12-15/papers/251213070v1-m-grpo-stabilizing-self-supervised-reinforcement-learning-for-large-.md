---
layout: default
title: M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization
---

# M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization

**arXiv**: [2512.13070v1](https://arxiv.org/abs/2512.13070) | [PDF](https://arxiv.org/pdf/2512.13070.pdf)

**ä½œè€…**: Bizhe Bai, Hongming Wu, Peng Ye, Tao Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM-GRPOä¸ŽIQRè¿‡æ»¤ä»¥ç¨³å®šå¤§è¯­è¨€æ¨¡åž‹çš„è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**

**å…³é”®è¯**: `è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹` `ç­–ç•¥ä¼˜åŒ–` `è®­ç»ƒç¨³å®šæ€§` `æŽ¨ç†èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶è®­ç»ƒä¸­æ˜“å‘ç”Ÿç­–ç•¥å´©æºƒï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™
2. M-GRPOåˆ©ç”¨åŠ¨é‡æ¨¡åž‹æä¾›ç¨³å®šè®­ç»ƒç›®æ ‡ï¼ŒIQRè¿‡æ»¤åŠ¨æ€å‰ªæžä½Žç†µè½¨è¿¹ä»¥ä¿æŒç­–ç•¥å¤šæ ·æ€§
3. å®žéªŒè¡¨æ˜Žè¯¥æ–¹æ³•æå‡è®­ç»ƒç¨³å®šæ€§å¹¶åœ¨å¤šä¸ªæŽ¨ç†åŸºå‡†ä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

