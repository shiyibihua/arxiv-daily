---
layout: default
title: Large-Language Memorization During the Classification of United States Supreme Court Cases
---

# Large-Language Memorization During the Classification of United States Supreme Court Cases

**arXiv**: [2512.13654v1](https://arxiv.org/abs/2512.13654) | [PDF](https://arxiv.org/pdf/2512.13654.pdf)

**ä½œè€…**: John E. Ortega, Dhruv D. Joshi, Matt P. Borkowski

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶å¤§è¯­è¨€æ¨¡åž‹åœ¨ç¾Žå›½æœ€é«˜æ³•é™¢æ¡ˆä¾‹åˆ†ç±»ä¸­çš„è®°å¿†ç­–ç•¥ï¼Œæå‡åˆ†ç±»å‡†ç¡®æ€§**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹è®°å¿†` `æœ€é«˜æ³•é™¢æ¡ˆä¾‹åˆ†ç±»` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `æç¤ºè®°å¿†æ¨¡åž‹` `æ³•å¾‹æ–‡æœ¬å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å¯èƒ½äº§ç”Ÿå¹»è§‰ï¼Œéœ€æŽ¢ç©¶å…¶è®°å¿†æœºåˆ¶ä»¥ä¼˜åŒ–å“åº”
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒã€è‡ªåŠ¨å»ºæ¨¡ç­‰æœ€æ–°æŠ€æœ¯ï¼Œç»“åˆæç¤ºè®°å¿†æ¨¡åž‹å¦‚DeepSeek
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨15å’Œ279ä¸ªä¸»é¢˜çš„SCOTUSåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæç¤ºè®°å¿†æ¨¡åž‹æ¯”ä¼ ç»ŸBERTæ¨¡åž‹å‡†ç¡®çŽ‡æé«˜çº¦2ç‚¹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

