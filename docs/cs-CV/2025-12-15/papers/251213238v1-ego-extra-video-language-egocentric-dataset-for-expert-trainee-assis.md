---
layout: default
title: Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance
---

# Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance

**arXiv**: [2512.13238v1](https://arxiv.org/abs/2512.13238) | [PDF](https://arxiv.org/pdf/2512.13238.pdf)

**ä½œè€…**: Francesco Ragusa, Michele Mazzamuto, Rosario Forte, Irene D'Ambra, James Fort, Jakob Engel, Antonino Furnari, Giovanni Maria Farinella

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEgo-EXTRAæ•°æ®é›†ï¼Œä»¥æ”¯æŒä¸“å®¶-å­¦å‘˜è¾…åŠ©åœºæ™¯ä¸‹çš„è§†é¢‘-è¯­è¨€å¤šæ¨¡æ€ç ”ç©¶ã€‚**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘` `è§†é¢‘-è¯­è¨€å¯¹è¯` `ä¸“å®¶-å­¦å‘˜è¾…åŠ©` `å¤šæ¨¡æ€åŸºå‡†` `è§†è§‰é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹é«˜è´¨é‡ä¸“å®¶æŒ‡å¯¼çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘-è¯­è¨€å¯¹è¯æ•°æ®é›†ï¼Œç”¨äºŽè¯„ä¼°å¤šæ¨¡æ€åŠ©æ‰‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨â€œWizard of OZâ€èŒƒå¼æ”¶é›†50å°æ—¶éžè„šæœ¬è§†é¢‘ï¼ŒåŒ…å«ä¸“å®¶ä»Žå­¦å‘˜è§†è§’æä¾›çš„è‡ªç„¶è¯­è¨€åé¦ˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºè¶…è¿‡15kè§†è§‰é—®ç­”å¯¹åŸºå‡†ï¼Œæµ‹è¯•æ˜¾ç¤ºå½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨ä¸“å®¶çº§è¾…åŠ©ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

