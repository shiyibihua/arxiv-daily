---
layout: default
title: Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning
---

# Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning

**arXiv**: [2512.13380v1](https://arxiv.org/abs/2512.13380) | [PDF](https://arxiv.org/pdf/2512.13380.pdf)

**ä½œè€…**: Chuan Mao, Haoqi Yuan, Ziye Huang, Chaoyi Xu, Kai Ma, Zongqing Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDemoFunGraspï¼Œé€šè¿‡æ¼”ç¤ºç¼–è¾‘å¼ºåŒ–å­¦ä¹ å®žçŽ°é€šç”¨çµå·§åŠŸèƒ½æŠ“å–**

**å…³é”®è¯**: `çµå·§æŠ“å–` `å¼ºåŒ–å­¦ä¹ ` `æ¼”ç¤ºç¼–è¾‘` `åŠŸèƒ½æŠ“å–` `ä»¿çœŸåˆ°çŽ°å®žè¿ç§»` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçµå·§æŠ“å–ä¸­ç»†ç²’åº¦åŠŸèƒ½æŠ“å–çš„ç›®æ ‡ä¸Žå¥–åŠ±å‡½æ•°è®¾è®¡å¤æ‚ï¼Œå¤šä»»åŠ¡æŽ¢ç´¢å›°éš¾ï¼Œä»¿çœŸåˆ°çŽ°å®žè¿ç§»æŒ‘æˆ˜å¤§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†åŠŸèƒ½æŠ“å–æ¡ä»¶åˆ†è§£ä¸ºæŠ“å–é£Žæ ¼å’Œå¯ä¾›æ€§ï¼Œé›†æˆåˆ°å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œåˆ©ç”¨å•æ¬¡æ¼”ç¤ºè¿›è¡Œä¸€æ­¥ç¼–è¾‘ä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçŽ°å®žä¸­æ³›åŒ–è‡³æœªè§å¯¹è±¡ã€å¯ä¾›æ€§å’ŒæŠ“å–é£Žæ ¼ç»„åˆï¼ŒæˆåŠŸçŽ‡å’ŒåŠŸèƒ½æŠ“å–å‡†ç¡®çŽ‡ä¼˜äºŽåŸºçº¿ï¼Œå…·å¤‡è‡ªä¸»æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.

