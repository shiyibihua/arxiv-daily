---
layout: default
title: Differentiable Evolutionary Reinforcement Learning
---

# Differentiable Evolutionary Reinforcement Learning

**arXiv**: [2512.13399v1](https://arxiv.org/abs/2512.13399) | [PDF](https://arxiv.org/pdf/2512.13399.pdf)

**ä½œè€…**: Sitao Cheng, Tianle Li, Xuhan Huang, Xunjian Yin, Difan Zou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯å¾®åˆ†è¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œä»¥è‡ªåŠ¨å‘çŽ°æœ€ä¼˜å¥–åŠ±ä¿¡å·ï¼Œè§£å†³å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡éš¾é¢˜ã€‚**

**å…³é”®è¯**: `å¯å¾®åˆ†è¿›åŒ–å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°ä¼˜åŒ–` `åŒå±‚æ¡†æž¶` `å…ƒä¼˜åŒ–` `å¤æ‚æŽ¨ç†ä»»åŠ¡` `åˆ†å¸ƒå¤–æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾ï¼ŒçŽ°æœ‰è¿›åŒ–æ–¹æ³•å¿½ç•¥å¥–åŠ±ç»“æž„ä¸Žä»»åŠ¡æ€§èƒ½çš„å› æžœå…³ç³»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒå±‚æ¡†æž¶ï¼Œé€šè¿‡å¯å¾®åˆ†å…ƒä¼˜åŒ–ï¼Œåˆ©ç”¨å†…å¾ªçŽ¯ç­–ç•¥éªŒè¯æ€§èƒ½æ›´æ–°å…ƒä¼˜åŒ–å™¨ï¼Œè¿‘ä¼¼ä»»åŠ¡æˆåŠŸçš„å…ƒæ¢¯åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æœºå™¨äººã€ç§‘å­¦æ¨¡æ‹Ÿå’Œæ•°å­¦æŽ¨ç†é¢†åŸŸéªŒè¯ï¼Œåœ¨ALFWorldå’ŒScienceWorldä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå°¤å…¶åœ¨åˆ†å¸ƒå¤–åœºæ™¯è¡¨çŽ°çªå‡ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

