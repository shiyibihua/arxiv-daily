---
layout: default
title: Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization
---

# Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization

**arXiv**: [2512.13034v1](https://arxiv.org/abs/2512.13034) | [PDF](https://arxiv.org/pdf/2512.13034.pdf)

**ä½œè€…**: Xiaoyu He, Yu Cai, Jin Jia, Canxi Huang, Wenqing Chen, Zibin Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAladaè‡ªé€‚åº”åŠ¨é‡æ–¹æ³•ï¼Œç”¨äºŽå¤§è§„æ¨¡çŸ©é˜µä¼˜åŒ–çš„å†…å­˜é«˜æ•ˆè®­ç»ƒã€‚**

**å…³é”®è¯**: `è‡ªé€‚åº”åŠ¨é‡æ–¹æ³•` `çŸ©é˜µä¼˜åŒ–` `å†…å­˜é«˜æ•ˆè®­ç»ƒ` `ç§©ä¸€åˆ†è§£` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤§è§„æ¨¡æ¨¡åž‹è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§„æ¨¡çŸ©é˜µä¼˜åŒ–ä¸­æ¢¯åº¦äºŒé˜¶çŸ©ä¼°è®¡çš„å†…å­˜å¼€é”€é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç§©ä¸€åˆ†è§£äº¤æ›¿æ›´æ–°å› å­ï¼Œå‡å°‘ä¼°è®¡è¯¯å·®å’Œå†…å­˜å ç”¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”AdamåŠå…¶å˜ä½“ï¼Œå†…å­˜å¼€é”€é™ä½Žä¸”è®­ç»ƒç¨³å¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

