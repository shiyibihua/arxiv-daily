---
layout: default
title: ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding
---

# ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding

**arXiv**: [2512.13586v1](https://arxiv.org/abs/2512.13586) | [PDF](https://arxiv.org/pdf/2512.13586.pdf)

**ä½œè€…**: Jia-Nan Li, Jian Guan, Wei Wu, Chongxuan Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReFusionä»¥è§£å†³æŽ©ç æ‰©æ•£æ¨¡åž‹åœ¨å¹¶è¡Œè§£ç ä¸­çš„è®¡ç®—ä¸Žç”Ÿæˆä¸€è‡´æ€§é—®é¢˜**

**å…³é”®è¯**: `å¹¶è¡Œè§£ç ` `æŽ©ç æ‰©æ•£æ¨¡åž‹` `è‡ªå›žå½’æ¨¡åž‹` `æ§½çº§è§„åˆ’` `KVç¼“å­˜é‡ç”¨` `è¯­è¨€æ¨¡åž‹åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ©ç æ‰©æ•£æ¨¡åž‹å­˜åœ¨é«˜è®¡ç®—å¼€é”€å’Œç”Ÿæˆä¸è¿žè´¯çš„ç¼ºé™·ï¼Œé˜»ç¢å¹¶è¡Œè§£ç æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ§½çº§å¹¶è¡Œè§£ç ï¼Œé‡‡ç”¨è§„åˆ’-å¡«å……ç­–ç•¥ï¼Œåœ¨æ§½çº§åˆ«è¿›è¡Œæ‰©æ•£è§„åˆ’å’Œè‡ªå›žå½’å¡«å……ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æå‡34%ï¼Œé€Ÿåº¦å¹³å‡åŠ é€Ÿ18å€ä»¥ä¸Šï¼ŒæŽ¥è¿‘è‡ªå›žå½’æ¨¡åž‹æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

