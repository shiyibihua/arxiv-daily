---
layout: default
title: Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos
---

# Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos

**arXiv**: [2512.13080v1](https://arxiv.org/abs/2512.13080) | [PDF](https://arxiv.org/pdf/2512.13080.pdf)

**ä½œè€…**: Yicheng Feng, Wanpeng Zhang, Ye Wang, Hao Luo, Haoqi Yuan, Sipeng Zheng, Zongqing Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´æ„ŸçŸ¥VLAé¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡äººç±»è§†é¢‘çš„è§†è§‰-ç‰©ç†å¯¹é½è§£å†³2Dè§†è§‰ä¸Ž3DåŠ¨ä½œçš„é¸¿æ²Ÿã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ç©ºé—´æ„ŸçŸ¥é¢„è®­ç»ƒ` `3Dè§†è§‰ç¼–ç ` `æœºå™¨äººå­¦ä¹ ` `äººç±»è§†é¢‘æ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLAæ¨¡åž‹ä¾èµ–2Dè§†è§‰è¾“å…¥åœ¨3DçŽ¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œå¯¼è‡´æ„ŸçŸ¥ä¸ŽåŠ¨ä½œåŸºç¡€ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤§è§„æ¨¡äººç±»æ¼”ç¤ºè§†é¢‘æå–3Dè§†è§‰å’ŒåŠ¨ä½œæ ‡æ³¨ï¼Œé€šè¿‡åŒç¼–ç å™¨æž¶æž„VIPA-VLAå¢žå¼º3Dç©ºé—´ç†è§£ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‹æ¸¸æœºå™¨äººä»»åŠ¡ä¸­ï¼ŒVIPA-VLAæ˜¾è‘—æå‡2Dè§†è§‰ä¸Ž3DåŠ¨ä½œçš„åŸºç¡€æ€§ï¼Œå®žçŽ°æ›´é²æ£’å’Œå¯æ³›åŒ–çš„ç­–ç•¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.

