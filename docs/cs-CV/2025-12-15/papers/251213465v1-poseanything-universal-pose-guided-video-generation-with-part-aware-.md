---
layout: default
title: PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence
---

# PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence

**arXiv**: [2512.13465v1](https://arxiv.org/abs/2512.13465) | [PDF](https://arxiv.org/pdf/2512.13465.pdf)

**ä½œè€…**: Ruiyan Wang, Teng Hu, Kaihui Huang, Zihan Su, Ran Yi, Lizhuang Ma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoseAnythingæ¡†æž¶ä»¥è§£å†³å§¿æ€å¼•å¯¼è§†é¢‘ç”Ÿæˆä¸­ä»…æ”¯æŒäººç±»å§¿æ€çš„å±€é™æ€§ï¼Œå®žçŽ°é€šç”¨å§¿æ€æŽ§åˆ¶ã€‚**

**å…³é”®è¯**: `å§¿æ€å¼•å¯¼è§†é¢‘ç”Ÿæˆ` `é€šç”¨å§¿æ€æŽ§åˆ¶` `éƒ¨åˆ†æ„ŸçŸ¥æ—¶åºä¸€è‡´æ€§` `ç›¸æœºè¿åŠ¨è§£è€¦` `éžäººç±»å§¿æ€æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä»…æŽ¥å—äººç±»å§¿æ€è¾“å…¥ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œæ— æ³•å¤„ç†éžäººç±»è§’è‰²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Part-aware Temporal Coherence Moduleå®žçŽ°ç»†ç²’åº¦éƒ¨åˆ†ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡Subject and Camera Motion Decoupled CFGç‹¬ç«‹æŽ§åˆ¶ç›¸æœºè¿åŠ¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨XPoseæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ”¯æŒä»»æ„éª¨éª¼è¾“å…¥å’Œé«˜è´¨é‡è§†é¢‘ç”Ÿæˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

