---
layout: default
title: From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis
---

# From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis

**arXiv**: [2512.13491v1](https://arxiv.org/abs/2512.13491) | [PDF](https://arxiv.org/pdf/2512.13491.pdf)

**ä½œè€…**: Åukasz DÄ™bowski

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ä»ŽZipfå®šå¾‹æŽ¨å¯¼ç¥žç»ç¼©æ”¾å®šå¾‹ï¼Œæ­ç¤ºè¯­è¨€æ¨¡åž‹ç»Ÿè®¡è§„å¾‹é—´çš„æ¼”ç»Žè”ç³»**

**å…³é”®è¯**: `ç¥žç»ç¼©æ”¾å®šå¾‹` `Zipfå®šå¾‹` `Heapså®šå¾‹` `Hilbergå‡è®¾` `åŸºç¡€æ¨¡åž‹` `ç»Ÿè®¡è¯­è¨€å­¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç©¶ç¥žç»ç¼©æ”¾å®šå¾‹ä¸ŽZipfå®šå¾‹ä¹‹é—´çš„æ¼”ç»Žå…³ç³»ï¼Œè§£é‡ŠåŸºç¡€æ¨¡åž‹æ€§èƒ½éšè®­ç»ƒæ•°æ®ã€å‚æ•°å’Œè®¡ç®—é‡å˜åŒ–çš„ç»Ÿè®¡åŸºç¡€
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç³»ç»Ÿå‡è®¾ï¼Œä»ŽZipfå®šå¾‹æŽ¨å¯¼Heapså®šå¾‹ï¼Œå†æŽ¨å¯¼Hilbergå‡è®¾ï¼Œæœ€ç»ˆå¾—å‡ºç¥žç»ç¼©æ”¾å®šå¾‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»¥Santa Feè¿‡ç¨‹ä¸ºä¾‹ï¼ŒéªŒè¯äº†å››ç§ç»Ÿè®¡å®šå¾‹çš„æ»¡è¶³æƒ…å†µï¼Œæ”¯æŒç†è®ºæŽ¨å¯¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.

