---
layout: default
title: SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning
---

# SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning

**arXiv**: [2512.13159v1](https://arxiv.org/abs/2512.13159) | [PDF](https://arxiv.org/pdf/2512.13159.pdf)

**ä½œè€…**: Emre Can Acikgoz, Jinoh Oh, Jie Hao, Joo Hyuk Jeon, Heng Ji, Dilek Hakkani-TÃ¼r, Gokhan Tur, Xiang Li, Chengyuan Ma, Xing Fan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpeakRLå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¥–åŠ±ä¸»åŠ¨äº¤äº’æå‡è¯­è¨€æ¨¡åž‹åœ¨ä»»åŠ¡å¯¼å‘å¯¹è¯ä¸­çš„åä½œèƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ä»»åŠ¡å¯¼å‘å¯¹è¯` `ä¸»åŠ¨äº¤äº’` `è¯­è¨€æ¨¡åž‹ä¼˜åŒ–` `åˆæˆæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è¯­è¨€æ¨¡åž‹åœ¨åä½œä¸­å¤šä¸ºè¢«åŠ¨å“åº”ï¼Œç¼ºä¹ä¸»åŠ¨æ¾„æ¸…ç”¨æˆ·æ„å›¾çš„èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¨¡åž‹ä¸»åŠ¨æé—®ï¼Œå¹³è¡¡è¯¢é—®ä¸Žè¡ŒåŠ¨ï¼Œå¹¶æž„å»ºSpeakERåˆæˆæ•°æ®é›†æ”¯æŒè®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»»åŠ¡å®ŒæˆçŽ‡ä¸Šæ¯”åŸºç¡€æ¨¡åž‹æå‡20.14%ï¼Œå¯¹è¯è½®æ¬¡æœªå¢žåŠ ï¼Œä¼˜äºŽæ›´å¤§ä¸“æœ‰æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

