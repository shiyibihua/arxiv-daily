---
layout: default
title: Recurrent Video Masked Autoencoders
---

# Recurrent Video Masked Autoencoders

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.13684" target="_blank" class="toolbar-btn">arXiv: 2512.13684v1</a>
    <a href="https://arxiv.org/pdf/2512.13684.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13684v1" 
            onclick="toggleFavorite(this, '2512.13684v1', 'Recurrent Video Masked Autoencoders')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRVMï¼šä¸€ç§åŸºäºTransformerå¾ªç¯ç¥ç»ç½‘ç»œçš„è§†é¢‘æ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºé«˜æ•ˆè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘è¡¨å¾å­¦ä¹ ` `å¾ªç¯ç¥ç»ç½‘ç»œ` `Transformer` `æ©ç è‡ªç¼–ç å™¨` `æ—¶ç©ºå»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ¨¡å‹åœ¨æ—¶ç©ºå»ºæ¨¡å’Œå‚æ•°æ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶åºè§†é¢‘ç†è§£ä¸­ã€‚
2. RVMåˆ©ç”¨å¾ªç¯Transformerèšåˆå›¾åƒç‰¹å¾ï¼Œé€šè¿‡æ©ç è‡ªç¼–ç å™¨å­¦ä¹ è§†é¢‘çš„æ—¶ç©ºç»“æ„ï¼Œå®ç°é«˜æ•ˆçš„è§†é¢‘è¡¨å¾ã€‚
3. RVMåœ¨åŠ¨ä½œè¯†åˆ«ã€ç›®æ ‡è·Ÿè¸ªç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå‚æ•°æ•ˆç‡æ˜¾è‘—æå‡ï¼Œå¹¶èƒ½ç¨³å®šä¼ æ’­é•¿æ—¶åºç‰¹å¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¾ªç¯è§†é¢‘æ©ç è‡ªç¼–ç å™¨(RVM)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œå®ƒä½¿ç”¨åŸºäºTransformerçš„å¾ªç¯ç¥ç»ç½‘ç»œæ¥èšåˆå¯†é›†å›¾åƒç‰¹å¾éšæ—¶é—´çš„å˜åŒ–ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•è·è‡ªç„¶è§†é¢‘æ•°æ®çš„æ—¶ç©ºç»“æ„ã€‚RVMé€šè¿‡éå¯¹ç§°æ©ç é¢„æµ‹ä»»åŠ¡è¿›è¡Œå­¦ä¹ ï¼Œä»…éœ€è¦æ ‡å‡†çš„åƒç´ é‡å»ºç›®æ ‡ã€‚è¿™ç§è®¾è®¡äº§ç”Ÿäº†ä¸€ä¸ªé«˜æ•ˆçš„â€œé€šç”¨â€ç¼–ç å™¨ï¼šRVMåœ¨è§†é¢‘çº§åˆ«çš„ä»»åŠ¡ï¼ˆå¦‚åŠ¨ä½œè¯†åˆ«å’Œç‚¹/å¯¹è±¡è·Ÿè¸ªï¼‰ä¸Šå®ç°äº†ä¸æœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹ï¼ˆä¾‹å¦‚VideoMAEï¼ŒV-JEPAï¼‰ç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æµ‹è¯•å‡ ä½•å’Œå¯†é›†ç©ºé—´ç†è§£çš„ä»»åŠ¡ä¸Šï¼Œä¹Ÿä¼˜äºå›¾åƒæ¨¡å‹ï¼ˆä¾‹å¦‚DINOv2ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRVMåœ¨å°å‹æ¨¡å‹æœºåˆ¶ä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œæ— éœ€çŸ¥è¯†è’¸é¦ï¼Œä¸ç«äº‰çš„è§†é¢‘æ©ç è‡ªåŠ¨ç¼–ç å™¨ç›¸æ¯”ï¼Œå‚æ•°æ•ˆç‡æé«˜äº†30å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†RVMçš„å¾ªç¯ç‰¹æ€§å…è®¸åœ¨è¾ƒé•¿çš„æ—¶é—´èŒƒå›´å†…è¿›è¡Œç¨³å®šçš„ç‰¹å¾ä¼ æ’­ï¼Œä¸”è®¡ç®—æˆæœ¬å‘ˆçº¿æ€§å¢é•¿ï¼Œå…‹æœäº†æ ‡å‡†åŸºäºæ—¶ç©ºæ³¨æ„åŠ›çš„æ¶æ„çš„ä¸€äº›å±€é™æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å®šæ€§å¯è§†åŒ–æ¥çªå‡ºæ˜¾ç¤ºRVMå­¦ä¹ äº†ä¸°å¯Œçš„åœºæ™¯è¯­ä¹‰ã€ç»“æ„å’Œè¿åŠ¨è¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘æ¨¡å‹ï¼Œå¦‚VideoMAEå’ŒV-JEPAï¼Œåœ¨è®¡ç®—æˆæœ¬å’Œå‚æ•°æ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿æ—¶ç¨‹è§†é¢‘æ—¶ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œå¦‚ä½•å­¦ä¹ åˆ°æ—¢èƒ½ç”¨äºè§†é¢‘ç†è§£ï¼Œåˆèƒ½ç”¨äºå›¾åƒç†è§£çš„é€šç”¨è¡¨å¾ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRVMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)æ¥èšåˆè§†é¢‘å¸§çš„ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•è·è§†é¢‘çš„æ—¶ç©ºç»“æ„ã€‚é€šè¿‡ç»“åˆTransformerçš„å¼ºå¤§è¡¨å¾èƒ½åŠ›å’ŒRNNçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼ŒRVMèƒ½å¤Ÿåœ¨é•¿æ—¶ç¨‹è§†é¢‘ä¸­è¿›è¡Œæœ‰æ•ˆçš„ç‰¹å¾ä¼ æ’­ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ã€‚æ©ç è‡ªç¼–ç å™¨(MAE)çš„éå¯¹ç§°ç»“æ„ç”¨äºé«˜æ•ˆçš„é¢„è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRVMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å›¾åƒç‰¹å¾æå–å™¨ï¼šç”¨äºæå–è§†é¢‘å¸§çš„å¯†é›†å›¾åƒç‰¹å¾ã€‚å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹ï¼Œå¦‚DINOv2ã€‚2) å¾ªç¯Transformerç¼–ç å™¨ï¼šè¯¥æ¨¡å—æ˜¯RVMçš„æ ¸å¿ƒï¼Œå®ƒä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œæ¥èšåˆå›¾åƒç‰¹å¾éšæ—¶é—´çš„å˜åŒ–ã€‚Transformerç”¨äºå¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚3) æ©ç ç­–ç•¥ï¼šé‡‡ç”¨éå¯¹ç§°æ©ç ç­–ç•¥ï¼Œå³ç¼–ç å™¨åªå¤„ç†æœªè¢«æ©ç çš„å¸§ï¼Œè€Œè§£ç å™¨åˆ™éœ€è¦é‡å»ºæ‰€æœ‰å¸§ã€‚4) é‡å»ºæŸå¤±ï¼šä½¿ç”¨åƒç´ é‡å»ºæŸå¤±ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ è§†é¢‘çš„æ—¶ç©ºç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šRVMçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¾ªç¯ç¥ç»ç½‘ç»œä¸Transformerç›¸ç»“åˆï¼Œç”¨äºè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚è¿™ç§ç»“åˆå…‹æœäº†ä¼ ç»ŸåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è§†é¢‘æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼ŒåŒæ—¶å®ç°äº†é•¿æ—¶ç¨‹è§†é¢‘çš„æœ‰æ•ˆå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒRVMçš„éå¯¹ç§°æ©ç ç­–ç•¥å’Œåƒç´ é‡å»ºç›®æ ‡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°é€šç”¨çš„è§†é¢‘è¡¨å¾ï¼Œæ—¢èƒ½ç”¨äºè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œåˆèƒ½ç”¨äºå›¾åƒç†è§£ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šRVMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¾ªç¯Transformerçš„ç»“æ„ï¼šå…·ä½“RNNå•å…ƒçš„é€‰æ‹©ï¼ˆå¦‚GRUæˆ–LSTMï¼‰ä»¥åŠTransformerçš„å±‚æ•°å’Œå¤´æ•°ã€‚2) æ©ç æ¯”ä¾‹ï¼šæ§åˆ¶éœ€è¦æ©ç çš„å¸§çš„æ¯”ä¾‹ï¼Œé€šå¸¸è®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼ˆå¦‚70%-90%ï¼‰ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚3) æŸå¤±å‡½æ•°ï¼šåƒç´ é‡å»ºæŸå¤±çš„å…·ä½“å½¢å¼ï¼Œå¦‚L1æˆ–L2æŸå¤±ã€‚4) è®­ç»ƒç­–ç•¥ï¼šåŒ…æ‹¬å­¦ä¹ ç‡ã€batch sizeå’Œä¼˜åŒ–å™¨ç­‰å‚æ•°çš„è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RVMåœ¨åŠ¨ä½œè¯†åˆ«å’Œç›®æ ‡è·Ÿè¸ªç­‰è§†é¢‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚VideoMAEå’ŒV-JEPAï¼‰ç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å‡ ä½•å’Œå¯†é›†ç©ºé—´ç†è§£çš„å›¾åƒä»»åŠ¡ä¸Šä¼˜äºDINOv2ã€‚RVMåœ¨å°æ¨¡å‹æœºåˆ¶ä¸‹è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€çŸ¥è¯†è’¸é¦ï¼Œå‚æ•°æ•ˆç‡æ¯”å…¶ä»–è§†é¢‘æ©ç è‡ªç¼–ç å™¨é«˜30å€ã€‚RVMèƒ½å¤Ÿç¨³å®šåœ°ä¼ æ’­é•¿æ—¶ç¨‹ç‰¹å¾ï¼Œè®¡ç®—æˆæœ¬å‘ˆçº¿æ€§å¢é•¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RVMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è§†é¢‘ç¼–è¾‘å’Œå†…å®¹åˆ†æç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„è§†é¢‘è¡¨å¾å­¦ä¹ èƒ½åŠ›å¯ä»¥ç”¨äºæå‡è¿™äº›åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­è¿›è¡Œå¼‚å¸¸è¡Œä¸ºæ£€æµ‹ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­è¿›è¡Œåœºæ™¯ç†è§£å’Œé¢„æµ‹ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­è¿›è¡Œè§†è§‰å®šä½å’Œè·¯å¾„è§„åˆ’ã€‚RVMçš„é€šç”¨æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è§†è§‰ä»»åŠ¡ï¼Œå…·æœ‰å¾ˆé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

