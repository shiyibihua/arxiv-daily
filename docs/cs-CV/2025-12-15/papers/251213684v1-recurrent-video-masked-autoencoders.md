---
layout: default
title: Recurrent Video Masked Autoencoders
---

# Recurrent Video Masked Autoencoders

**arXiv**: [2512.13684v1](https://arxiv.org/abs/2512.13684) | [PDF](https://arxiv.org/pdf/2512.13684.pdf)

**ä½œè€…**: Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¾ªçŽ¯è§†é¢‘æŽ©ç è‡ªç¼–ç å™¨ï¼Œé€šè¿‡å¾ªçŽ¯èšåˆå›¾åƒç‰¹å¾é«˜æ•ˆå­¦ä¹ è§†é¢‘æ—¶ç©ºè¡¨ç¤ºã€‚**

**å…³é”®è¯**: `è§†é¢‘è¡¨ç¤ºå­¦ä¹ ` `å¾ªçŽ¯ç¥žç»ç½‘ç»œ` `æŽ©ç è‡ªç¼–ç å™¨` `æ—¶ç©ºå»ºæ¨¡` `å‚æ•°æ•ˆçŽ‡` `é•¿è§†é¢‘ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘è¡¨ç¤ºå­¦ä¹ éœ€é«˜æ•ˆæ•èŽ·æ—¶ç©ºç»“æž„ï¼Œä¼ ç»Ÿæ–¹æ³•è®¡ç®—æˆæœ¬é«˜æˆ–ä¾èµ–å¤æ‚ç›®æ ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŸºäºŽTransformerçš„å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼Œé€šè¿‡éžå¯¹ç§°æŽ©ç é¢„æµ‹ä»»åŠ¡å­¦ä¹ ï¼Œä»…éœ€åƒç´ é‡å»ºç›®æ ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŠ¨ä½œè¯†åˆ«ç­‰ä»»åŠ¡ä¸Šåª²ç¾Žå…ˆè¿›è§†é¢‘æ¨¡åž‹ï¼Œå‚æ•°æ•ˆçŽ‡æå‡é«˜è¾¾30å€ï¼Œæ”¯æŒé•¿æ—¶ç¨³å®šç‰¹å¾ä¼ æ’­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

