---
layout: default
title: SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping
---

# SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping

**arXiv**: [2512.13494v1](https://arxiv.org/abs/2512.13494) | [PDF](https://arxiv.org/pdf/2512.13494.pdf)

**‰ΩúËÄÖ**: Yu-Chen Lu, Sheng-Feng Yu, Hui-Hsien Weng, Pei-Shuo Wang, Yu-Fang Hu, Liang Hung-Chun, Hung-Yueh Chiang, Kai-Chiang Wu

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SkipCatÊ°ÜÊû∂ÔºåÈÄöËøáÂÖ±‰∫´ÊäïÂΩ±ÂíåÂùóË∑≥ËøáÂÆûÁé∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ´òÊïà‰ΩéÁß©ÂéãÁº©**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°ÂûãÂéãÁº©` `‰ΩéÁß©ÂàÜËß£` `ÂÖ±‰∫´ÊäïÂΩ±` `ÂùóË∑≥Ëøá` `ËµÑÊ∫êÂèóÈôêÈÉ®ÁΩ≤` `Èõ∂Ê†∑Êú¨ÊÄßËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºö‰ΩéÁß©ÂéãÁº©ÈúÄÂ§ßÂπÖÈôç‰ΩéÁß©‰ª•ËäÇÁúÅËµÑÊ∫êÔºå‰ΩÜÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÂºïÂÖ•Â±ÇÂÜÖÂÖ±‰∫´‰ΩéÁß©ÊäïÂΩ±ÂíåÂùóË∑≥ËøáÊäÄÊúØÔºåÂú®Áõ∏ÂêåÂéãÁº©Áéá‰∏ã‰øùÁïôÊõ¥Â§öÊúâÊïàÁß©
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®Èõ∂Ê†∑Êú¨‰ªªÂä°‰∏äÔºåÁõ∏ÂêåÂéãÁº©Áéá‰∏ãÂáÜÁ°ÆÁéáÊèêÂçá7%ÔºåÊó†ÈúÄÈ¢ùÂ§ñÂæÆË∞É

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, na√Øve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

