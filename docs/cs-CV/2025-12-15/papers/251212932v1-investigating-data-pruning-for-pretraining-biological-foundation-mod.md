---
layout: default
title: Investigating Data Pruning for Pretraining Biological Foundation Models at Scale
---

# Investigating Data Pruning for Pretraining Biological Foundation Models at Scale

**arXiv**: [2512.12932v1](https://arxiv.org/abs/2512.12932) | [PDF](https://arxiv.org/pdf/2512.12932.pdf)

**ä½œè€…**: Yifan Wu, Jiyue Jiang, Xichen Ye, Yiqi Wang, Chang Zhou, Yitao Xu, Jiayang Chen, He Hu, Weizhong Zhang, Cheng Jin, Jiao Yuan, Yu Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå½±å“å¼•å¯¼çš„æ•°æ®å‰ªæžæ¡†æž¶ä»¥é™ä½Žç”Ÿç‰©åŸºç¡€æ¨¡åž‹é¢„è®­ç»ƒçš„è®¡ç®—æˆæœ¬**

**å…³é”®è¯**: `ç”Ÿç‰©åŸºç¡€æ¨¡åž‹` `æ•°æ®å‰ªæž` `å½±å“ä¼°è®¡` `é¢„è®­ç»ƒä¼˜åŒ–` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç”Ÿç‰©åŸºç¡€æ¨¡åž‹é¢„è®­ç»ƒä¾èµ–æµ·é‡æ•°æ®ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”å¯å¤çŽ°æ€§å·®
2. å¼•å…¥åŸºäºŽå­é›†çš„è‡ªå½±å“å…¬å¼ï¼Œé«˜æ•ˆä¼°è®¡æ ·æœ¬é‡è¦æ€§ï¼Œè®¾è®¡Top-kå’Œè¦†ç›–ä¸­å¿ƒå½±å“é€‰æ‹©ç­–ç•¥
3. åœ¨RNA-FMå’ŒESM-Cä¸ŠéªŒè¯ï¼Œæžç«¯å‰ªæžçŽ‡è¶…99%æ—¶ä¼˜äºŽéšæœºåŸºçº¿ï¼Œæ ¸å¿ƒé›†æ€§èƒ½ä¼˜äºŽåå€å¤§éšæœºå­é›†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.

