---
layout: default
title: A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees
---

# A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees

**arXiv**: [2512.13565v1](https://arxiv.org/abs/2512.13565) | [PDF](https://arxiv.org/pdf/2512.13565.pdf)

**ä½œè€…**: Junye Du, Zhenghao Li, Zhutong Gu, Long Feng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽéžå‚æ•°ç»Ÿè®¡çš„æ·±åº¦ç¥žç»ç½‘ç»œç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œåœ¨éžçº¿æ€§é«˜ç»´åœºæ™¯ä¸‹ä¿è¯ç†è®ºä¸€è‡´æ€§ã€‚**

**å…³é”®è¯**: `ç‰¹å¾é€‰æ‹©` `éžå‚æ•°ç»Ÿè®¡` `æ·±åº¦ç¥žç»ç½‘ç»œ` `ç†è®ºä¿è¯` `é«˜ç»´æ•°æ®` `éžçº¿æ€§æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨æœªçŸ¥éžçº¿æ€§å‡½æ•°ä¸‹ï¼Œä»Žé«˜ç»´ç‰¹å¾ä¸­è¯†åˆ«ç›¸å…³ç‰¹å¾é›†ï¼Œæ»¡è¶³E(y\|x)=G(x_S0)ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†ç¥žç»ç½‘ç»œé‡æž„ä¸ºç´¢å¼•æ¨¡åž‹ï¼Œåˆ©ç”¨äºŒé˜¶Steinå…¬å¼è¿›è¡Œæ— æ¢¯åº¦ç‰¹å¾é€‰æ‹©ï¼Œç»“åˆç­›é€‰æœºåˆ¶å¤„ç†é«˜ç»´ç¨€ç–æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡æ¨¡æ‹Ÿå’ŒçœŸå®žæ•°æ®åˆ†æžéªŒè¯æ–¹æ³•æ€§èƒ½ï¼Œå³ä½¿åœ¨å¤æ‚ç‰¹å¾äº¤äº’ä¸‹ä¹Ÿè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper tackles the problem of feature selection in a highly challenging setting: $\mathbb{E}(y \| \boldsymbol{x}) = G(\boldsymbol{x}_{\mathcal{S}_0})$, where $\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{Ã¶}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Î©(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Î©(s \log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.

