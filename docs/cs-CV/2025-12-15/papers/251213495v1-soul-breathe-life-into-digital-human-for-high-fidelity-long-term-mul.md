---
layout: default
title: Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation
---

# Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation

**arXiv**: [2512.13495v1](https://arxiv.org/abs/2512.13495) | [PDF](https://arxiv.org/pdf/2512.13495.pdf)

**ä½œè€…**: Jiangning Zhang, Junwei Zhu, Zhenye Gan, Donghao Luo, Chuming Lin, Feifan Xu, Xu Peng, Jianlong Hu, Yuansen Liu, Yijia Hong, Weijian Cao, Han Feng, Xu Chen, Chencan Fu, Keke He, Xiaobin Hu, Chengjie Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSoulæ¡†æž¶ï¼Œé€šè¿‡å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆé«˜ä¿çœŸé•¿æ—¶æ•°å­—äººåŠ¨ç”»ï¼Œåº”ç”¨äºŽè™šæ‹Ÿä¸»æ’­å’Œå½±è§†åˆ¶ä½œã€‚**

**å…³é”®è¯**: `æ•°å­—äººåŠ¨ç”»` `å¤šæ¨¡æ€ç”Ÿæˆ` `é•¿æ—¶ä¸€è‡´æ€§` `å”‡åŒæ­¥` `è’¸é¦è®­ç»ƒ` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•°å­—äººåŠ¨ç”»é¢ä¸´æ•°æ®ç¨€ç¼ºã€é•¿æœŸç”Ÿæˆä¸€è‡´æ€§å·®å’ŒæŽ¨ç†æ•ˆçŽ‡ä½Žç­‰æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽWan2.2-5Béª¨å¹²ï¼Œé›†æˆéŸ³é¢‘æ³¨å…¥å±‚ã€é˜ˆå€¼æ„ŸçŸ¥ç æœ¬æ›¿æ¢å’Œè’¸é¦ç­–ç•¥ï¼Œä¼˜åŒ–ç”Ÿæˆè´¨é‡ä¸Žé€Ÿåº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºSoul-1Mæ•°æ®é›†å’ŒSoul-BenchåŸºå‡†ï¼Œåœ¨è§†é¢‘è´¨é‡ã€å”‡åŒæ­¥å’Œèº«ä»½ä¿æŒä¸Šæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

