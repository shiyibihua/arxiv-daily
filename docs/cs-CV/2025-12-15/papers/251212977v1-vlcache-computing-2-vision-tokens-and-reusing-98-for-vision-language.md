---
layout: default
title: VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference
---

# VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference

**arXiv**: [2512.12977v1](https://arxiv.org/abs/2512.12977) | [PDF](https://arxiv.org/pdf/2512.12977.pdf)

**ä½œè€…**: Shengling Qin, Hao Yu, Chenxin Wu, Zheng Li, Yizhong Cao, Zhengyang Zhuge, Yuxin Zhou, Wentao Yao, Yi Zhang, Zhengheng Wang, Shuai Bai, Jianwei Zhang, Junyang Lin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLCacheæ¡†æž¶ï¼Œé€šè¿‡å¤ç”¨å¤šæ¨¡æ€è¾“å…¥çš„KVç¼“å­˜å’Œç¼–ç å™¨ç¼“å­˜ï¼Œå‡å°‘è§†è§‰è¯­è¨€æŽ¨ç†ä¸­çš„é‡å¤è®¡ç®—ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æŽ¨ç†` `ç¼“å­˜å¤ç”¨` `KVç¼“å­˜` `ç¼–ç å™¨ç¼“å­˜` `åŠ¨æ€é‡è®¡ç®—` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€è¾“å…¥é‡å¤å‡ºçŽ°æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€å…¨é‡é‡è®¡ç®—ï¼Œå¯¼è‡´æŽ¨ç†æ•ˆçŽ‡ä½Žä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå½¢å¼åŒ–åˆ†æžç´¯ç§¯å¤ç”¨è¯¯å·®ï¼Œæå‡ºåŠ¨æ€å±‚æ„ŸçŸ¥é‡è®¡ç®—ç­–ç•¥ï¼Œå¹³è¡¡ç²¾åº¦ä¸Žæ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»…éœ€è®¡ç®—2-5%çš„è§†è§‰ä»¤ç‰Œï¼Œå®žçŽ°1.2x-16xçš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´åŠ é€Ÿï¼Œç²¾åº¦ä¸Žå…¨é‡è®¡ç®—ç›¸å½“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

