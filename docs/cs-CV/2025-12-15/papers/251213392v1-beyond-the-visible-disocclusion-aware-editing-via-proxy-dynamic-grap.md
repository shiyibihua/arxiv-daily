---
layout: default
title: Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs
---

# Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs

**arXiv**: [2512.13392v1](https://arxiv.org/abs/2512.13392) | [PDF](https://arxiv.org/pdf/2512.13392.pdf)

**ä½œè€…**: Anran Qi, Changjian Li, Adrien Bousseau, Niloy J. Mitra

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»£ç†åŠ¨æ€å›¾æ–¹æ³•ä»¥è§£å†³å›¾åƒè½¬è§†é¢‘ä¸­ç”¨æˆ·æŽ§åˆ¶æ–°æ˜¾éœ²åŒºåŸŸå†…å®¹çš„é—®é¢˜**

**å…³é”®è¯**: `å›¾åƒè½¬è§†é¢‘ç”Ÿæˆ` `ä»£ç†åŠ¨æ€å›¾` `è¿åŠ¨æŽ§åˆ¶` `å¤–è§‚åˆæˆ` `ç”¨æˆ·ç¼–è¾‘` `åŽ»é®æŒ¡åŒºåŸŸ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å›¾åƒè½¬è§†é¢‘æ–¹æ³•éš¾ä»¥ç”Ÿæˆå¯é¢„æµ‹çš„å…³èŠ‚è¿åŠ¨å¹¶æŽ§åˆ¶æ–°æ˜¾éœ²åŒºåŸŸå†…å®¹
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è½»é‡çº§ä»£ç†åŠ¨æ€å›¾åˆ†ç¦»è¿åŠ¨æŒ‡å®šä¸Žå¤–è§‚åˆæˆï¼Œæ— éœ€è®­ç»ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…³èŠ‚ç‰©ä½“ã€å®¶å…·ç­‰åœºæ™¯ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå®žçŽ°å¯æŽ§è¿åŠ¨ä¸Žå¤–è§‚ç¼–è¾‘

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

