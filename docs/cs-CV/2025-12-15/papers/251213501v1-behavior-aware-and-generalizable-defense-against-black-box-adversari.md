---
layout: default
title: Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS
---

# Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS

**arXiv**: [2512.13501v1](https://arxiv.org/abs/2512.13501) | [PDF](https://arxiv.org/pdf/2512.13501.pdf)

**ä½œè€…**: Sabrine Ennaji, Elhadj Benkhelifa, Luigi Vincenzo Mancini

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”ç‰¹å¾æ±¡æŸ“ä»¥é˜²å¾¡åŸºäºŽæœºå™¨å­¦ä¹ çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿä¸­çš„é»‘ç›’å¯¹æŠ—æ”»å‡»**

**å…³é”®è¯**: `é»‘ç›’å¯¹æŠ—æ”»å‡»` `å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ` `è‡ªé€‚åº”ç‰¹å¾æ±¡æŸ“` `æµé‡åˆ†æž` `å˜åŒ–ç‚¹æ£€æµ‹` `æ”»å‡»é˜²å¾¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é˜²å¾¡æ–¹æ³•é’ˆå¯¹ç‰¹å®šæ”»å‡»ã€éœ€æ¨¡åž‹è®¿é—®æˆ–é™æ€æœºåˆ¶ï¼Œéš¾ä»¥æ³›åŒ–ä¸”å½±å“æ£€æµ‹æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æµé‡åˆ†æžã€å˜åŒ–ç‚¹æ£€æµ‹å’Œè‡ªé€‚åº”ç¼©æ”¾ï¼ŒåŠ¨æ€æ‰°åŠ¨æ”»å‡»è€…å¯èƒ½åˆ©ç”¨çš„ç‰¹å¾ï¼Œç ´ååé¦ˆå¾ªçŽ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºèƒ½æ··æ·†æ”»å‡»è€…ã€é™ä½Žæ”»å‡»æ•ˆæžœå¹¶ä¿æŒæ£€æµ‹æ€§èƒ½ï¼Œå…·æœ‰é€šç”¨æ€§å’Œä¸å¯æ£€æµ‹æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
>   To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
>   We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.

