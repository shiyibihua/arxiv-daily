---
layout: default
title: Towards Scalable Pre-training of Visual Tokenizers for Generation
---

# Towards Scalable Pre-training of Visual Tokenizers for Generation

**arXiv**: [2512.13687v1](https://arxiv.org/abs/2512.13687) | [PDF](https://arxiv.org/pdf/2512.13687.pdf)

**ä½œè€…**: Jingfeng Yao, Yuda Song, Yucong Zhou, Xinggang Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVTPæ¡†æž¶ä»¥è§£å†³è§†è§‰åˆ†è¯å™¨é¢„è®­ç»ƒä¸­çš„æ‰©å±•æ€§é—®é¢˜ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚**

**å…³é”®è¯**: `è§†è§‰åˆ†è¯å™¨` `é¢„è®­ç»ƒæ‰©å±•æ€§` `ç”Ÿæˆæ¨¡åž‹` `è¯­ä¹‰è¡¨ç¤º` `è”åˆä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸåŸºäºŽé‡å»ºçš„è§†è§‰åˆ†è¯å™¨é¢„è®­ç»ƒå¯¼è‡´æ½œåœ¨ç©ºé—´åå‘ä½Žçº§ä¿¡æ¯ï¼Œç”Ÿæˆæ€§èƒ½éšè®¡ç®—æŠ•å…¥æå‡æœ‰é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šVTPæ¡†æž¶è”åˆä¼˜åŒ–å›¾åƒ-æ–‡æœ¬å¯¹æ¯”ã€è‡ªç›‘ç£å’Œé‡å»ºæŸå¤±ï¼Œå¼ºè°ƒé«˜å±‚è¯­ä¹‰è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒåŽï¼ŒVTPåœ¨ç”Ÿæˆä»»åŠ¡ä¸­å®žçŽ°æ›´å¿«æ”¶æ•›å’Œæ˜¾è‘—æ€§èƒ½æå‡ï¼Œæ‰©å±•æ€§ä¼˜äºŽä¼ ç»Ÿæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

