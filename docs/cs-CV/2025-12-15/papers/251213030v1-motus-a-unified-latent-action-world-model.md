---
layout: default
title: Motus: A Unified Latent Action World Model
---

# Motus: A Unified Latent Action World Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.13030" target="_blank" class="toolbar-btn">arXiv: 2512.13030v1</a>
    <a href="https://arxiv.org/pdf/2512.13030.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13030v1" 
            onclick="toggleFavorite(this, '2512.13030v1', 'Motus: A Unified Latent Action World Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, Hongyan Zhao, Hanyu Liu, Zhizhong Su, Lei Ma, Hang Su, Jun Zhu

**ÂàÜÁ±ª**: cs.CV, cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Motus‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõÁªü‰∏ÄÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÁîüÊàê` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ê∑∑ÂêàÂèòÊç¢Âô®` `ÂÖâÊµÅÂ≠¶‰π†` `Áªü‰∏ÄÂª∫Ê®°` `Âä®‰ΩúÈ¢ÑËÆ≠ÁªÉ` `ËßÜÈ¢ëÁîüÊàê` `Êô∫ËÉΩ‰ΩìÁ≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÁêÜËß£„ÄÅ‰∏ñÁïåÂª∫Ê®°ÂíåÊéßÂà∂ÊñπÈù¢Â≠òÂú®Á¢éÁâáÂåñÔºåÈôêÂà∂‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõÁöÑÁªü‰∏Ä„ÄÇ
2. MotusÈÄöËøáÊ∑∑ÂêàÂèòÊç¢Âô®Êû∂ÊûÑÊï¥ÂêàÂ§ö‰∏™‰∏ìÂÆ∂ÔºåÂπ∂ÈááÁî®ÁÅµÊ¥ªÁöÑË∞ÉÂ∫¶Âô®ÔºåÂÆûÁé∞‰∏çÂêåÂª∫Ê®°Ê®°ÂºèÁöÑÂàáÊç¢„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMotusÂú®‰ªøÁúü‰∏≠Áõ∏ËæÉ‰∫éX-VLAÊèêÂçá15%ÔºåÁõ∏ËæÉ‰∫éPi0.5ÊèêÂçá45%ÔºåÂú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÊèêÂçá11%Ëá≥48%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ÂΩìÂâçÁöÑÊô∫ËÉΩ‰ΩìÁ†îÁ©∂‰∏≠ÔºåÁêÜËß£„ÄÅ‰∏ñÁïåÂª∫Ê®°ÂíåÊéßÂà∂ÊñπÊ≥ïÂæÄÂæÄÊòØÂ≠§Á´ãÁöÑÔºåËøôÁßçÁ¢éÁâáÂåñÈòªÁ¢ç‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõÁöÑÁªü‰∏ÄÂíå‰ªéÂ§ßËßÑÊ®°ÂºÇÊûÑÊï∞ÊçÆ‰∏≠Â≠¶‰π†„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜMotusÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊΩúÂú®Âä®‰Ωú‰∏ñÁïåÊ®°ÂûãÔºåÂà©Áî®Áé∞ÊúâÁöÑÈÄöÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÂíå‰∏∞ÂØåÁöÑÂèØÂÖ±‰∫´ËøêÂä®‰ø°ÊÅØ„ÄÇMotusÂºïÂÖ•‰∫ÜÊ∑∑ÂêàÂèòÊç¢Âô®ÔºàMoTÔºâÊû∂ÊûÑÔºåÊï¥ÂêàÁêÜËß£„ÄÅËßÜÈ¢ëÁîüÊàêÂíåÂä®‰Ωú‰∏â‰∏™‰∏ìÂÆ∂ÔºåÂπ∂ÈááÁî®UniDiffuserÈ£éÊ†ºÁöÑË∞ÉÂ∫¶Âô®ÔºåÂÆûÁé∞‰∏çÂêåÂª∫Ê®°Ê®°Âºè‰πãÈó¥ÁöÑÁÅµÊ¥ªÂàáÊç¢„ÄÇÈÄöËøáÂÖâÊµÅÂ≠¶‰π†ÊΩúÂú®Âä®‰ΩúÔºåMotusÈááÁî®‰∏âÈò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÂíåÂÖ≠Â±ÇÊï∞ÊçÆÈáëÂ≠óÂ°îÔºåÊèêÂèñÂÉèÁ¥†Á∫ß‚ÄúÂ¢ûÈáèÂä®‰Ωú‚ÄùÔºåÂÆûÁé∞Â§ßËßÑÊ®°Âä®‰ΩúÈ¢ÑËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMotusÂú®‰ªøÁúüÂíåÁé∞ÂÆûÂú∫ÊôØ‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÊú∫Âô®‰∫∫‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂΩìÂâçÁöÑÊô∫ËÉΩ‰ΩìÊñπÊ≥ïÂæÄÂæÄÊòØÂ≠§Á´ãÁöÑÔºåÂØºËá¥ÁêÜËß£„ÄÅ‰∏ñÁïåÂª∫Ê®°ÂíåÊéßÂà∂‰πãÈó¥Áº∫‰πèÊúâÊïàÁöÑÊï¥ÂêàÔºåÈôêÂà∂‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõÁöÑÂèëÊå•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMotusÈÄöËøáÂºïÂÖ•Ê∑∑ÂêàÂèòÊç¢Âô®Êû∂ÊûÑÔºåÂ∞ÜÁêÜËß£„ÄÅËßÜÈ¢ëÁîüÊàêÂíåÂä®‰Ωú‰∏â‰∏™‰∏ìÂÆ∂Êï¥Âêà‰∏∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁ≥ªÁªüÔºåÂà©Áî®‰∏∞ÂØåÁöÑËøêÂä®‰ø°ÊÅØÂíåÁé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÊèêÂçáÊ®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõÂíåÁîüÊàêËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMotusÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÁêÜËß£Ê®°Âùó„ÄÅËßÜÈ¢ëÁîüÊàêÊ®°ÂùóÂíåÂä®‰ΩúÊ®°Âùó„ÄÇÈÄöËøáUniDiffuserÈ£éÊ†ºÁöÑË∞ÉÂ∫¶Âô®ÔºåÊ®°ÂûãÂèØ‰ª•Âú®‰∏çÂêåÁöÑÂª∫Ê®°Ê®°Âºè‰πãÈó¥ÁÅµÊ¥ªÂàáÊç¢ÔºåÈÄÇÂ∫î‰∏çÂêåÁöÑ‰ªªÂä°ÈúÄÊ±Ç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMotusÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂Ê∑∑ÂêàÂèòÊç¢Âô®Êû∂ÊûÑÂíå‰∏âÈò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºåËÉΩÂ§üÊúâÊïàÊï¥ÂêàÂ§öÁßçÂäüËÉΩÂíåÂÖàÈ™åÁü•ËØÜÔºåÊòæËëóÊèêÂçáÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMotusÈááÁî®ÂÖ≠Â±ÇÊï∞ÊçÆÈáëÂ≠óÂ°îÁªìÊûÑÔºåÊèêÂèñÂÉèÁ¥†Á∫ß‚ÄúÂ¢ûÈáèÂä®‰Ωú‚ÄùÔºåÂπ∂ÈÄöËøáÂÖâÊµÅÂ≠¶‰π†ÊΩúÂú®Âä®‰ΩúÔºåËÆæËÆ°‰∫ÜÈÄÇÂ∫î‰∏çÂêå‰ªªÂä°ÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÔºåÁ°Æ‰øùÊ®°ÂûãÁöÑÈ´òÊïàËÆ≠ÁªÉÂíåÊÄßËÉΩÊèêÂçá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MotusÂú®ÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂú®‰ªøÁúüÁéØÂ¢É‰∏≠Áõ∏ËæÉ‰∫éX-VLAÊèêÂçá‰∫Ü15%ÔºåÁõ∏ËæÉ‰∫éPi0.5ÊèêÂçá‰∫Ü45%„ÄÇÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶Âú®11%Ëá≥48%‰πãÈó¥ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MotusÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄºÔºåÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠â„ÄÇÈÄöËøáÁªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõÔºåMotusËÉΩÂ§üÊèêÂçáÊô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñÂíåÊâßË°åËÉΩÂäõÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

