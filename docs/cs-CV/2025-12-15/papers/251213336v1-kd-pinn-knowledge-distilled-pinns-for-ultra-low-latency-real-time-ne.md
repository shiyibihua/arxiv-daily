---
layout: default
title: KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers
---

# KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers

**arXiv**: [2512.13336v1](https://arxiv.org/abs/2512.13336) | [PDF](https://arxiv.org/pdf/2512.13336.pdf)

**ä½œè€…**: Karim Bounja, Lahcen Laayouni, Abdeljalil Sakat

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçŸ¥è¯†è’¸é¦ç‰©ç†ä¿¡æ¯ç¥žç»ç½‘ç»œæ¡†æž¶ï¼Œç”¨äºŽå®žçŽ°è¶…ä½Žå»¶è¿Ÿå®žæ—¶ç¥žç»åå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨ã€‚**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `ç‰©ç†ä¿¡æ¯ç¥žç»ç½‘ç»œ` `åå¾®åˆ†æ–¹ç¨‹æ±‚è§£` `è¶…ä½Žå»¶è¿Ÿ` `å®žæ—¶è®¡ç®—` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç‰©ç†ä¿¡æ¯ç¥žç»ç½‘ç»œæŽ¨ç†å»¶è¿Ÿé«˜ï¼Œéš¾ä»¥æ»¡è¶³å®žæ—¶æ±‚è§£éœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¿žç»­é€‚åº”KLæ•£åº¦ï¼Œå°†é«˜å®¹é‡æ•™å¸ˆæ¨¡åž‹é¢„æµ‹ç²¾åº¦è¿ç§»è‡³ç´§å‡‘å­¦ç”Ÿæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§åå¾®åˆ†æ–¹ç¨‹ä¸ŠéªŒè¯ï¼Œå­¦ç”Ÿæ¨¡åž‹ä¿æŒç‰©ç†ç²¾åº¦ï¼ŒæŽ¨ç†é€Ÿåº¦æå‡4.8-6.9å€ï¼Œå¹³å‡å»¶è¿Ÿ5.3æ¯«ç§’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

