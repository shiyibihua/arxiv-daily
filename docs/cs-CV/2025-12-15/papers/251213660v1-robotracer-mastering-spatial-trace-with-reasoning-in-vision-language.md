---
layout: default
title: RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics
---

# RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics

**arXiv**: [2512.13660v1](https://arxiv.org/abs/2512.13660) | [PDF](https://arxiv.org/pdf/2512.13660.pdf)

**ä½œè€…**: Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboTracerä»¥è§£å†³æœºå™¨äººç©ºé—´è¿½è¸ªä¸­çš„å¤šæ­¥åº¦é‡æŽ¨ç†ä¸Žç©ºé—´æŒ‡ä»£éš¾é¢˜**

**å…³é”®è¯**: `ç©ºé—´è¿½è¸ª` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åº¦é‡æŽ¨ç†` `å¼ºåŒ–å¾®è°ƒ` `æœºå™¨äººæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç©ºé—´è¿½è¸ªéœ€å¤šæ­¥åº¦é‡æŽ¨ç†ä¸Žå¤æ‚ç©ºé—´æŒ‡ä»£ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é€šç”¨ç©ºé—´ç¼–ç å™¨å’Œå›žå½’ç›‘ç£è§£ç å™¨å¢žå¼ºå°ºåº¦æ„ŸçŸ¥ï¼Œç»“åˆåº¦é‡æ•æ„Ÿå¥–åŠ±è¿›è¡Œå¼ºåŒ–å¾®è°ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨TraceSpatial-Benchä¸Šè¶…è¶ŠåŸºçº¿ï¼Œå¹³å‡æˆåŠŸçŽ‡79.1%ï¼Œä¼˜äºŽGemini-2.5-Pro 36%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

