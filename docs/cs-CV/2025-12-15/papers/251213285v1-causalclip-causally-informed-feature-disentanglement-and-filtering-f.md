---
layout: default
title: CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images
---

# CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images

**arXiv**: [2512.13285v1](https://arxiv.org/abs/2512.13285) | [PDF](https://arxiv.org/pdf/2512.13285.pdf)

**ä½œè€…**: Bo Liu, Qiao Qin, Qinghui He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCausalCLIPæ¡†æž¶ï¼Œé€šè¿‡å› æžœç‰¹å¾è§£è€¦ä¸Žè¿‡æ»¤æå‡ç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `ç”Ÿæˆå›¾åƒæ£€æµ‹` `ç‰¹å¾è§£è€¦` `å› æžœæŽ¨ç†` `æ³›åŒ–èƒ½åŠ›` `CLIPæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ç‰¹å¾é«˜åº¦çº ç¼ ï¼Œæ··åˆå› æžœä¸Žéžå› æžœç‰¹å¾ï¼Œé™åˆ¶æ³›åŒ–
2. åŸºäºŽç»“æž„å› æžœæ¨¡åž‹ï¼Œåˆ©ç”¨Gumbel-SoftmaxæŽ©ç å’ŒHSICçº¦æŸè§£è€¦ç‰¹å¾
3. åœ¨æœªè§ç”Ÿæˆæ¨¡åž‹ä¸Šæµ‹è¯•ï¼Œå‡†ç¡®çŽ‡å’Œå¹³å‡ç²¾åº¦æ˜¾è‘—ä¼˜äºŽå…ˆè¿›æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

