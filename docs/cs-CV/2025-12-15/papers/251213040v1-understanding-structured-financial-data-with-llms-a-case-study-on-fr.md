---
layout: default
title: Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection
---

# Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection

**arXiv**: [2512.13040v1](https://arxiv.org/abs/2512.13040) | [PDF](https://arxiv.org/pdf/2512.13040.pdf)

**ä½œè€…**: Xuwei Tan, Yao Ma, Xueru Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFinFRE-RAGæ–¹æ³•ï¼Œé€šè¿‡ç‰¹å¾ç¼©å‡å’Œæ£€ç´¢å¢žå¼ºå­¦ä¹ æå‡LLMsåœ¨é‡‘èžæ¬ºè¯ˆæ£€æµ‹ä¸­çš„æ€§èƒ½ä¸Žå¯è§£é‡Šæ€§ã€‚**

**å…³é”®è¯**: `é‡‘èžæ¬ºè¯ˆæ£€æµ‹` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `ç‰¹å¾ç¼©å‡` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `è¡¨æ ¼æ•°æ®ç†è§£` `å¯è§£é‡Šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMsç›´æŽ¥åº”ç”¨äºŽè¡¨æ ¼æ¬ºè¯ˆæ£€æµ‹æ—¶æ€§èƒ½å·®ï¼Œå› ç‰¹å¾å¤šã€ç±»åˆ«ä¸å¹³è¡¡å’Œç¼ºä¹ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆé‡è¦æ€§å¼•å¯¼ç‰¹å¾ç¼©å‡åºåˆ—åŒ–ï¼Œå†æ£€ç´¢å¢žå¼ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡F1/MCCï¼ŒæŽ¥è¿‘è¡¨æ ¼åŸºçº¿å¹¶æä¾›å¯è§£é‡Šç†ç”±ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

