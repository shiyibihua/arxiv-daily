---
layout: default
title: Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences
---

# Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences

**arXiv**: [2512.13123v1](https://arxiv.org/abs/2512.13123) | [PDF](https://arxiv.org/pdf/2512.13123.pdf)

**ä½œè€…**: Liviu Aolaritei, Michael I. Jordan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽä»»æ„æ—¶é—´æœ‰æ•ˆç½®ä¿¡åºåˆ—çš„éšæœºæ¢¯åº¦ä¸‹é™åœæ­¢è§„åˆ™ï¼Œç”¨äºŽå‡¸ä¼˜åŒ–**

**å…³é”®è¯**: `éšæœºæ¢¯åº¦ä¸‹é™` `å‡¸ä¼˜åŒ–` `åœæ­¢è§„åˆ™` `ç½®ä¿¡åºåˆ—` `ä»»æ„æ—¶é—´æœ‰æ•ˆæ€§` `éžè´Ÿè¶…éž…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸSGDåˆ†æžç¼ºä¹åœ¨ä»»æ„æ—¶é—´è¯„ä¼°å½“å‰è¿­ä»£æŽ¥è¿‘æœ€ä¼˜è§£çš„ç»Ÿè®¡æœ‰æ•ˆæ–¹æ³•
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡éžè´Ÿè¶…éž…æž„å»ºæŠ•å½±SGDåŠ æƒå¹³å‡æ¬¡ä¼˜æ€§çš„ä»»æ„æ—¶é—´æœ‰æ•ˆç½®ä¿¡åºåˆ—ï¼Œæ— éœ€å¹³æ»‘æ€§æˆ–å¼ºå‡¸æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœæ­¢è§„åˆ™åœ¨æ¦‚çŽ‡è‡³å°‘1-Î±ä¸‹è¯æ˜Žä¸ºÎµ-æœ€ä¼˜ï¼Œä¸”åœ¨æ ‡å‡†æ­¥é•¿ä¸‹å‡ ä¹Žå¿…ç„¶æœ‰é™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-Î±$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.

