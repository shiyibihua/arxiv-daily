---
layout: default
title: FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs
---

# FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs

**arXiv**: [2512.13337v1](https://arxiv.org/abs/2512.13337) | [PDF](https://arxiv.org/pdf/2512.13337.pdf)

**ä½œè€…**: Si Qi Goh, Yongsen Zheng, Ziyao Liu, Sami Hormi, Kwok-Yan Lam

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFROCæ¡†æž¶ï¼Œé€šè¿‡é£Žé™©ä¼˜åŒ–æŽ§åˆ¶è§£å†³å¤§è¯­è¨€æ¨¡åž‹æœºå™¨é—å¿˜ä¸­çš„é£Žé™©å¹³è¡¡é—®é¢˜ã€‚**

**å…³é”®è¯**: `æœºå™¨é—å¿˜` `å¤§è¯­è¨€æ¨¡åž‹` `é£Žé™©æŽ§åˆ¶` `ä¿å½¢é£Žé™©åˆ†æž` `è¶…å‚æ•°ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æœºå™¨é—å¿˜æŠ€æœ¯ç¼ºä¹æœ‰æ•ˆé£Žé™©è¯„ä¼°ä¸ŽæŽ§åˆ¶æœºåˆ¶ï¼Œéš¾ä»¥å¹³è¡¡é—å¿˜å……åˆ†æ€§ä¸Žæ•ˆç”¨ä¿ç•™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä¿å½¢é£Žé™©åˆ†æžï¼Œå¼•å…¥è¿žç»­é£Žé™©æ¨¡åž‹å’Œä¿å½¢é—å¿˜é£Žé™©ï¼Œä»¥æ¦‚çŽ‡çº¦æŸæŒ‡å¯¼è¶…å‚æ•°é€‰æ‹©ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šæ–¹æ³•å®žéªŒæ˜¾ç¤ºFROCèƒ½ç”Ÿæˆç¨³å®šé£Žé™©æ™¯è§‚ï¼Œæ­ç¤ºé…ç½®ä¸Žè¯­ä¹‰åç§»ã€æ•ˆç”¨å½±å“çš„å…³ç³»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

