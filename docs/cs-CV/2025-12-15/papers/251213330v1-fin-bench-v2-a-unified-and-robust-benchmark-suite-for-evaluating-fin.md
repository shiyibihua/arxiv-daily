---
layout: default
title: FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models
---

# FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models

**arXiv**: [2512.13330v1](https://arxiv.org/abs/2512.13330) | [PDF](https://arxiv.org/pdf/2512.13330.pdf)

**ä½œè€…**: Joona KytÃ¶niemi, Jousia Piha, Akseli Reunamo, Fedor Vitiugin, Farrokh Mehryary, Sampo Pyysalo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFIN-bench-v2ç»Ÿä¸€åŸºå‡†å¥—ä»¶ä»¥è¯„ä¼°èŠ¬å…°è¯­å¤§è¯­è¨€æ¨¡åž‹**

**å…³é”®è¯**: `èŠ¬å…°è¯­å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `ä»»åŠ¡é²æ£’æ€§` `å¤šä»»åŠ¡è¦†ç›–` `å…¬å¼€æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹ç»Ÿä¸€ä¸”é²æ£’çš„èŠ¬å…°è¯­å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°åŸºå‡†
2. æ–¹æ³•è¦ç‚¹ï¼šæ•´åˆå¹¶æ‰©å±•çŽ°æœ‰åŸºå‡†ï¼Œé€šè¿‡æ¨¡åž‹å­¦ä¹ æ›²çº¿ç­›é€‰é²æ£’ä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æŒ‡ä»¤è°ƒä¼˜æ¨¡åž‹ï¼Œå…¬å¼€æ•°æ®é›†å’Œè¯„ä¼°é…ç½®

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

