---
layout: default
title: Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens
---

# Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02419" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02419v1</a>
  <a href="https://arxiv.org/pdf/2508.02419.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02419v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02419v1', 'Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haohan Zheng, Zhenguo Zhang

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ³¨æ„åŠ›è°ƒæ•´æ–¹æ³•ä»¥ç¼“è§£LVLMä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç‰©ä½“å¹»è§‰` `å¤šæ¨¡æ€ç†è§£` `æ³¨æ„åŠ›æœºåˆ¶` `å¯¹æ¯”è§£ç ` `è·¨æ¨¡æ€å…¼å®¹æ€§` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LVLMåœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶å­˜åœ¨ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œä¸»è¦ç”±äºå¯¹æ–‡æœ¬æç¤ºçš„è¿‡åº¦ä¾èµ–ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è°ƒæ•´æ–‡æœ¬å’Œè§†è§‰æ ‡è®°çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¹³è¡¡è·¨æ¨¡æ€ä¿¡æ¯çš„å…¼å®¹æ€§ï¼Œä»¥å‡è½»å¹»è§‰ç°è±¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªå¼€æºLVLMsä¸Šæœ‰æ•ˆé™ä½äº†å¹»è§‰ç°è±¡ï¼ŒéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»ç„¶é¢ä¸´ä¸¥é‡çš„ç‰©ä½“å¹»è§‰é—®é¢˜ã€‚ä»¥å¾€ç ”ç©¶ä¸»è¦å°†æ­¤ç¼ºé™·å½’å› äºè§†è§‰ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´çš„è§„æ¨¡ä¸åŒ¹é…ï¼Œå¯¼è‡´LVLMsè¿‡åº¦ä¾èµ–æ–‡æœ¬æç¤ºå’Œå†…éƒ¨çŸ¥è¯†ï¼Œä»è€Œç”Ÿæˆä¸è§†è§‰çº¿ç´¢ä¸ä¸€è‡´çš„æè¿°ã€‚æœ¬æ–‡é€šè¿‡æ·±å…¥ç ”ç©¶å¹»è§‰æœºåˆ¶ï¼Œå‘ç°LVLMsåœ¨å¹»è§‰è¿‡ç¨‹ä¸­ä¸ä»…å¿½è§†è§†è§‰ä¿¡æ¯ï¼Œè¿˜å¿½è§†æ–‡æœ¬æ¨¡æ€ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è®­ç»ƒæ— å…³æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´æ–‡æœ¬å’Œè§†è§‰æ ‡è®°çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¹³è¡¡è·¨æ¨¡æ€å…¼å®¹æ€§ï¼Œæ”¹å–„ä¸ç”¨æˆ·æ„å›¾çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¼€æºLVLMså’ŒåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆç¼“è§£äº†å¹»è§‰ç°è±¡ï¼Œå±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æç¤ºï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸è§†è§‰ä¿¡æ¯ä¸ä¸€è‡´ï¼Œå½±å“æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è°ƒæ•´æ–‡æœ¬å’Œè§†è§‰æ ‡è®°çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¹³è¡¡ä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯æµï¼Œä»è€Œæ”¹å–„æ¨¡å‹å¯¹ç”¨æˆ·æŒ‡ä»¤çš„ç†è§£å’Œå“åº”ï¼Œå‡å°‘å¹»è§‰ç°è±¡çš„å‘ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬å¯¹æ³¨æ„åŠ›æƒé‡çš„å¹²é¢„å’Œè°ƒæ•´ï¼Œé‡‡ç”¨å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œä»¥å‡å°‘æ¨¡å‹å¯¹å…¶å‚æ•°çŸ¥è¯†çš„è¿‡åº¦ä¾èµ–ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ³¨æ„åŠ›è°ƒæ•´æ¨¡å—å’Œå¯¹æ¯”è§£ç æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æå‡ºçš„æ³¨æ„åŠ›è°ƒæ•´æ–¹æ³•æ˜¯å…³é”®åˆ›æ–°ï¼Œä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºåŒæ—¶å…³æ³¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„ä¿¡æ¯æµï¼Œè§£å†³äº†ä»¥å¾€æ–¹æ³•ä¸­æ¨¡æ€åè§çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ³¨æ„åŠ›æƒé‡è°ƒæ•´ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¯¹æ¯”è§£ç ç­–ç•¥æ¥å¢å¼ºæ³¨æ„åŠ›æ“ä½œçš„æ•ˆæœï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªå¼€æºLVLMsä¸Šæœ‰æ•ˆé™ä½äº†ç‰©ä½“å¹»è§‰ç°è±¡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å±•ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆå’Œå¤šæ¨¡æ€æœç´¢å¼•æ“ç­‰ã€‚é€šè¿‡æ”¹å–„LVLMsçš„ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œä½¿å¾—æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å¯é å’Œå‡†ç¡®ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (LVLMs) have demonstrated remarkable multimodal comprehension and reasoning capabilities, but they still suffer from severe object hallucination. Previous studies primarily attribute the flaw to linguistic prior caused by the scale mismatch between visual encoders and large language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs, generating descriptions inconsistent with visual cues. However, through an in-depth investigation of the hallucinated mechanisms, we empirically reveal a previously overlooked phenomenon: LVLMs may ignore not only visual information but also textual modality during hallucination, a behavior termed as modality bias, which indicates that LVLMs struggle to simultaneously attend to both visual and textual modalities, leading to fragmented understanding of user-provided instructions. Based on this observation, we propose a simple yet effective training-free method to mitigate object hallucination. Concretely, we intervene and adjust the attention weights of textual and visual tokens, balancing cross-modal compatibility for better alignment with user intentions. Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's overreliance on its parametric knowledge, synergistically enhancing our attention manipulation. Extensive experiments confirm the widespread presence of modality bias in LVLMs. Notably, our method effectively mitigates hallucination across multiple open-source LVLMs and benchmarks, highlighting its generalizability and efficacy.

