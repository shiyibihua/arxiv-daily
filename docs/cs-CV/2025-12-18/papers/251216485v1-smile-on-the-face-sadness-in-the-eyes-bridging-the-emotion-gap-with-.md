---
layout: default
title: Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors
---

# Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16485" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16485v1</a>
  <a href="https://arxiv.org/pdf/2512.16485.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16485v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16485v1', 'Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Accepted by TMM

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kejun1/EMER)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEMERTæ¨¡å‹å’ŒEMERæ•°æ®é›†ï¼Œåˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºå¼¥åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«å’Œæƒ…æ„Ÿè¯†åˆ«çš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿè¯†åˆ«` `é¢éƒ¨è¡¨æƒ…è¯†åˆ«` `çœ¼éƒ¨è¡Œä¸º` `å¤šæ¨¡æ€èåˆ` `Transformer` `å¯¹æŠ—å­¦ä¹ ` `æ•°æ®é›†` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è¿‡åº¦ä¾èµ–é¢éƒ¨è¡¨æƒ…ï¼Œå¿½ç•¥äº†é¢éƒ¨è¡¨æƒ…å¯èƒ½æ©ç›–çœŸå®æƒ…æ„Ÿçš„é—®é¢˜ã€‚
2. æå‡ºEMERTæ¨¡å‹ï¼Œç»“åˆçœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼Œåˆ©ç”¨æ¨¡æ€å¯¹æŠ—è§£è€¦å’Œå¤šä»»åŠ¡Transformeræ¥æå‡æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEMERTæ¨¡å‹åœ¨EMERæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¤šæ¨¡æ€æ–¹æ³•ï¼ŒéªŒè¯äº†çœ¼éƒ¨è¡Œä¸ºåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æƒ…æ„Ÿè¯†åˆ«(ER)æ˜¯ä»æ„ŸçŸ¥æ•°æ®ä¸­åˆ†æå’Œè¯†åˆ«äººç±»æƒ…æ„Ÿçš„è¿‡ç¨‹ã€‚ç›®å‰ï¼Œè¯¥é¢†åŸŸä¸¥é‡ä¾èµ–äºé¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)ï¼Œå› ä¸ºè§†è§‰é€šé“ä¼ é€’ä¸°å¯Œçš„æƒ…æ„Ÿçº¿ç´¢ã€‚ç„¶è€Œï¼Œé¢éƒ¨è¡¨æƒ…é€šå¸¸è¢«ç”¨ä½œç¤¾äº¤å·¥å…·ï¼Œè€Œä¸æ˜¯çœŸå®å†…åœ¨æƒ…æ„Ÿçš„è¡¨ç°ã€‚ä¸ºäº†ç†è§£å’Œå¼¥åˆFERå’ŒERä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºä¸€ä¸ªé‡è¦çš„æƒ…æ„Ÿçº¿ç´¢ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªçœ¼éƒ¨è¡Œä¸ºè¾…åŠ©çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«(EMER)æ•°æ®é›†ã€‚ä¸ºäº†æ”¶é›†å…·æœ‰çœŸå®æƒ…æ„Ÿçš„æ•°æ®ï¼Œé‡‡ç”¨äº†è‡ªå‘æƒ…æ„Ÿè¯±å¯¼èŒƒå¼ï¼Œä½¿ç”¨åˆºæ¿€ææ–™ï¼Œåœ¨æ­¤æœŸé—´ï¼Œéä¾µå…¥æ€§çš„çœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼Œå¦‚çœ¼åŠ¨åºåˆ—å’Œçœ¼éƒ¨æ³¨è§†å›¾ï¼Œä¸é¢éƒ¨è¡¨æƒ…è§†é¢‘ä¸€èµ·è¢«æ•è·ã€‚ä¸ºäº†æ›´å¥½åœ°è¯´æ˜ERå’ŒFERä¹‹é—´çš„å·®è·ï¼Œåˆ†åˆ«å¯¹å¤šæ¨¡æ€ERå’ŒFERè¿›è¡Œäº†å¤šè§†è§’æƒ…æ„Ÿæ ‡æ³¨ã€‚æ­¤å¤–ï¼ŒåŸºäºæ–°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„çœ¼éƒ¨è¡Œä¸ºè¾…åŠ©MER Transformer (EMERT)ï¼Œé€šè¿‡å¼¥åˆæƒ…æ„Ÿå·®è·æ¥å¢å¼ºERã€‚EMERTåˆ©ç”¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦å’Œä¸€ä¸ªå¤šä»»åŠ¡Transformeræ¥å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºï¼Œä½œä¸ºé¢éƒ¨è¡¨æƒ…çš„æœ‰åŠ›è¡¥å……ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬ä¸ºEMERæ•°æ®é›†çš„å„ç§ç»¼åˆè¯„ä¼°å¼•å…¥äº†ä¸ƒä¸ªå¤šæ¨¡æ€åŸºå‡†åè®®ã€‚ç»“æœè¡¨æ˜ï¼ŒEMERTçš„æ€§èƒ½å¤§å¤§ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œæ­ç¤ºäº†å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºå¯¹äºé²æ£’ERçš„é‡è¦æ€§ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¯¹çœ¼éƒ¨è¡Œä¸ºåœ¨ERä¸­çš„é‡è¦æ€§è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œä»è€Œæ¨è¿›äº†è§£å†³FERå’ŒERä¹‹é—´å·®è·çš„ç ”ç©¶ï¼Œä»¥è·å¾—æ›´å¼ºå¤§çš„ERæ€§èƒ½ã€‚æˆ‘ä»¬çš„EMERæ•°æ®é›†å’Œè®­ç»ƒå¥½çš„EMERTæ¨¡å‹å°†åœ¨https://github.com/kejun1/EMERä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–é¢éƒ¨è¡¨æƒ…ï¼Œä½†é¢éƒ¨è¡¨æƒ…å¸¸å¸¸æ˜¯ç¤¾ä¼šåŒ–çš„ä¼ªè£…ï¼Œä¸èƒ½å®Œå…¨åæ˜ çœŸå®æƒ…æ„Ÿã€‚å› æ­¤ï¼Œå¦‚ä½•å¼¥åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)å’Œæƒ…æ„Ÿè¯†åˆ«(ER)ä¹‹é—´çš„å·®è·ï¼Œæå‡æƒ…æ„Ÿè¯†åˆ«çš„é²æ£’æ€§æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹çœ¼éƒ¨è¡Œä¸ºçš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºä¸€ç§é‡è¦çš„æƒ…æ„Ÿçº¿ç´¢å¼•å…¥åˆ°æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­ã€‚é€šè¿‡ç»“åˆé¢éƒ¨è¡¨æƒ…å’Œçœ¼éƒ¨è¡Œä¸ºï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£äººç±»çš„çœŸå®æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œå¼¥åˆFERå’ŒERä¹‹é—´çš„å·®è·ã€‚è¿™ç§æ€è·¯åŸºäºçœ¼éƒ¨è¡Œä¸ºèƒ½å¤Ÿæ›´çœŸå®åœ°åæ˜ ä¸ªä½“çš„æƒ…æ„ŸçŠ¶æ€ï¼Œå‡å°‘ç¤¾ä¼šåŒ–ä¼ªè£…çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMERTæ¨¡å‹çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1)æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ï¼šç”¨äºè§£è€¦é¢éƒ¨è¡¨æƒ…å’Œçœ¼éƒ¨è¡Œä¸ºä¸­çš„æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œæƒ…æ„Ÿå…±äº«ç‰¹å¾ã€‚2)å¤šä»»åŠ¡Transformeræ¨¡å—ï¼šç”¨äºèåˆè§£è€¦åçš„ç‰¹å¾ï¼Œå¹¶åŒæ—¶è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«å’Œé¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ã€‚3)æƒ…æ„Ÿåˆ†ç±»å™¨ï¼šåŸºäºèåˆåçš„ç‰¹å¾è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šè¾“å…¥é¢éƒ¨è¡¨æƒ…è§†é¢‘å’Œçœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼Œç»è¿‡ç‰¹å¾æå–å’Œè§£è€¦ï¼Œç„¶åé€šè¿‡Transformerè¿›è¡Œèåˆå’Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1)æå‡ºäº†å°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºæƒ…æ„Ÿè¯†åˆ«çš„é‡è¦çº¿ç´¢ï¼Œå¹¶æ„å»ºäº†ç›¸åº”çš„EMERæ•°æ®é›†ã€‚2)è®¾è®¡äº†æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ï¼Œæœ‰æ•ˆåˆ†ç¦»äº†æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œæƒ…æ„Ÿå…±äº«ç‰¹å¾ã€‚3)æå‡ºäº†å¤šä»»åŠ¡Transformerç»“æ„ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ æƒ…æ„Ÿè¯†åˆ«å’Œé¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¯¹æŠ—å­¦ä¹ çš„æ–¹æ³•æ¥åˆ†ç¦»æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œæƒ…æ„Ÿå…±äº«ç‰¹å¾ã€‚åœ¨å¤šä»»åŠ¡Transformeræ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æƒ…æ„Ÿåˆ†ç±»æŸå¤±ã€é¢éƒ¨è¡¨æƒ…åˆ†ç±»æŸå¤±å’Œå¯¹æŠ—æŸå¤±ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æœªåœ¨æ­¤å¤„è¯¦ç»†å±•å¼€ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEMERTæ¨¡å‹åœ¨EMERæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼ŒEMERTæ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡ä¸Šæå‡äº†è¶…è¿‡5%ã€‚å…·ä½“è€Œè¨€ï¼ŒEMERTæ¨¡å‹åœ¨ä¸ƒä¸ªå¤šæ¨¡æ€åŸºå‡†åè®®ä¸Šå‡å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒéªŒè¯äº†çœ¼éƒ¨è¡Œä¸ºåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„é‡è¦æ€§å’ŒEMERTæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æœ‰æ•ˆå»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºï¼Œå¯ä»¥æ˜¾è‘—æå‡æƒ…æ„Ÿè¯†åˆ«çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººæœºäº¤äº’ã€å¿ƒç†å¥åº·è¯„ä¼°ã€å¸‚åœºè¥é”€ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥ä½¿æœºå™¨æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œæä¾›æ›´è‡ªç„¶å’Œä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚åœ¨å¿ƒç†å¥åº·è¯„ä¼°ä¸­ï¼Œå¯ä»¥è¾…åŠ©åŒ»ç”Ÿè¯Šæ–­æ‚£è€…çš„æƒ…æ„Ÿéšœç¢ã€‚åœ¨å¸‚åœºè¥é”€ä¸­ï¼Œå¯ä»¥å¸®åŠ©ä¼ä¸šæ›´å¥½åœ°äº†è§£æ¶ˆè´¹è€…çš„æƒ…æ„Ÿéœ€æ±‚ï¼Œä»è€Œåˆ¶å®šæ›´æœ‰æ•ˆçš„è¥é”€ç­–ç•¥ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œå¦‚è¯­éŸ³å’Œç”Ÿç†ä¿¡å·ï¼Œä»¥å®ç°æ›´å…¨é¢å’Œå‡†ç¡®çš„æƒ…æ„Ÿè¯†åˆ«ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

