---
layout: default
title: Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors
---

# Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16485" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16485v1</a>
  <a href="https://arxiv.org/pdf/2512.16485.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16485v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16485v1', 'Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Accepted by TMM

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kejun1/EMER)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEMERæ•°æ®é›†å’ŒEMERTæ¨¡å‹ï¼Œåˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºå¼¥åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«å’Œæƒ…æ„Ÿè¯†åˆ«ä¹‹é—´çš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿè¯†åˆ«` `é¢éƒ¨è¡¨æƒ…è¯†åˆ«` `çœ¼éƒ¨è¡Œä¸º` `å¤šæ¨¡æ€èåˆ` `Transformer` `æ•°æ®é›†` `ç‰¹å¾è§£è€¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è¿‡åº¦ä¾èµ–é¢éƒ¨è¡¨æƒ…ï¼Œå¿½ç•¥äº†é¢éƒ¨è¡¨æƒ…å¯èƒ½ä¼ªè£…çœŸå®æƒ…æ„Ÿçš„é—®é¢˜ã€‚
2. æå‡ºEMERæ•°æ®é›†å’ŒEMERTæ¨¡å‹ï¼Œå°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºæƒ…æ„Ÿçº¿ç´¢ï¼Œå¼¥åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«å’Œæƒ…æ„Ÿè¯†åˆ«ä¹‹é—´çš„å·®è·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEMERTæ¨¡å‹åœ¨EMERæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¤šæ¨¡æ€æ–¹æ³•ï¼ŒéªŒè¯äº†çœ¼éƒ¨è¡Œä¸ºåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æƒ…æ„Ÿè¯†åˆ«(ER)æ˜¯ä»æ„ŸçŸ¥æ•°æ®ä¸­åˆ†æå’Œè¯†åˆ«äººç±»æƒ…æ„Ÿçš„è¿‡ç¨‹ã€‚ç›®å‰ï¼Œè¯¥é¢†åŸŸä¸¥é‡ä¾èµ–äºé¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)ï¼Œå› ä¸ºè§†è§‰é€šé“ä¼ é€’ä¸°å¯Œçš„æƒ…æ„Ÿçº¿ç´¢ã€‚ç„¶è€Œï¼Œé¢éƒ¨è¡¨æƒ…é€šå¸¸è¢«ç”¨ä½œç¤¾äº¤å·¥å…·ï¼Œè€Œä¸æ˜¯çœŸå®å†…å¿ƒæƒ…æ„Ÿçš„ä½“ç°ã€‚ä¸ºäº†ç†è§£å’Œå¼¥åˆFERå’ŒERä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥çœ¼éƒ¨è¡Œä¸ºä½œä¸ºé‡è¦çš„æƒ…æ„Ÿçº¿ç´¢ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªçœ¼éƒ¨è¡Œä¸ºè¾…åŠ©çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«(EMER)æ•°æ®é›†ã€‚ä¸ºäº†æ”¶é›†å…·æœ‰çœŸå®æƒ…æ„Ÿçš„æ•°æ®ï¼Œåˆ©ç”¨åˆºæ¿€ææ–™è¿›è¡Œè‡ªå‘æƒ…æ„Ÿè¯±å¯¼ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œéä¾µå…¥æ€§çœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼ˆå¦‚çœ¼åŠ¨åºåˆ—å’Œçœ¼éƒ¨æ³¨è§†å›¾ï¼‰ä¸é¢éƒ¨è¡¨æƒ…è§†é¢‘ä¸€èµ·è¢«æ•è·ã€‚ä¸ºäº†æ›´å¥½åœ°è¯´æ˜ERå’ŒFERä¹‹é—´çš„å·®è·ï¼Œåˆ†åˆ«å¯¹å¤šæ¨¡æ€ERå’ŒFERè¿›è¡Œå¤šè§†è§’æƒ…æ„Ÿæ ‡æ³¨ã€‚æ­¤å¤–ï¼ŒåŸºäºæ–°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„çœ¼éƒ¨è¡Œä¸ºè¾…åŠ©çš„MER Transformer (EMERT)ï¼Œé€šè¿‡å¼¥åˆæƒ…æ„Ÿå·®è·æ¥å¢å¼ºERã€‚EMERTåˆ©ç”¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦å’Œä¸€ä¸ªå¤šä»»åŠ¡Transformeræ¥å°†çœ¼éƒ¨è¡Œä¸ºå»ºæ¨¡ä¸ºé¢éƒ¨è¡¨æƒ…çš„æœ‰åŠ›è¡¥å……ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬ä¸ºEMERæ•°æ®é›†çš„å„ç§ç»¼åˆè¯„ä¼°å¼•å…¥äº†ä¸ƒç§å¤šæ¨¡æ€åŸºå‡†åè®®ã€‚ç»“æœè¡¨æ˜ï¼ŒEMERTä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œæ­ç¤ºäº†å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºå¯¹äºé²æ£’ERçš„é‡è¦æ€§ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¯¹çœ¼éƒ¨è¡Œä¸ºåœ¨ERä¸­çš„é‡è¦æ€§è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œä»è€Œæ¨è¿›äº†è§£å†³FERå’ŒERä¹‹é—´å·®è·çš„ç ”ç©¶ï¼Œä»¥è·å¾—æ›´å¼ºå¤§çš„ERæ€§èƒ½ã€‚æˆ‘ä»¬çš„EMERæ•°æ®é›†å’Œè®­ç»ƒå¥½çš„EMERTæ¨¡å‹å°†åœ¨https://github.com/kejun1/EMERä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–é¢éƒ¨è¡¨æƒ…ï¼Œä½†é¢éƒ¨è¡¨æƒ…å®¹æ˜“å—åˆ°ç¤¾ä¼šå› ç´ çš„å½±å“ï¼Œå¯èƒ½æ— æ³•çœŸå®åæ˜ ä¸ªä½“çš„æƒ…æ„ŸçŠ¶æ€ã€‚è¿™å¯¼è‡´é¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)å’ŒçœŸå®æƒ…æ„Ÿè¯†åˆ«(ER)ä¹‹é—´å­˜åœ¨å·®è·ã€‚è®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åˆ©ç”¨æ›´å¯é çš„æƒ…æ„Ÿçº¿ç´¢ï¼ˆå¦‚çœ¼éƒ¨è¡Œä¸ºï¼‰æ¥æå‡æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºé¢éƒ¨è¡¨æƒ…çš„è¡¥å……ï¼Œé€šè¿‡å¤šæ¨¡æ€èåˆçš„æ–¹å¼æ¥æå‡æƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½ã€‚çœ¼éƒ¨è¡Œä¸ºä¸æ˜“ä¼ªè£…ï¼Œèƒ½å¤Ÿæ›´çœŸå®åœ°åæ˜ ä¸ªä½“çš„æƒ…æ„ŸçŠ¶æ€ã€‚é€šè¿‡å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºä¸é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„å…³ç³»ï¼Œå¯ä»¥æœ‰æ•ˆå¼¥åˆFERå’ŒERä¹‹é—´çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºEMERTï¼ˆEye-behavior-aided MER Transformerï¼‰çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šåˆ†åˆ«æå–é¢éƒ¨è¡¨æƒ…è§†é¢‘å’Œçœ¼éƒ¨è¡Œä¸ºæ•°æ®çš„ç‰¹å¾ã€‚2) æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ï¼šç”¨äºè§£è€¦æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«çš„ç‰¹å¾ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) å¤šä»»åŠ¡Transformeræ¨¡å—ï¼šç”¨äºå»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºå’Œé¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆåˆ†åˆ«æå–ä¸¤ç§æ¨¡æ€çš„ç‰¹å¾ï¼Œç„¶åè¿›è¡Œç‰¹å¾è§£è€¦å’Œèåˆï¼Œæœ€åé€šè¿‡Transformerè¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†EMERæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«é¢éƒ¨è¡¨æƒ…è§†é¢‘å’Œçœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼Œå¹¶å¯¹ä¸¤ç§æ¨¡æ€åˆ†åˆ«è¿›è¡Œäº†æƒ…æ„Ÿæ ‡æ³¨ã€‚2) è®¾è®¡äº†æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ï¼Œå¯ä»¥æœ‰æ•ˆåˆ†ç¦»æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«çš„ç‰¹å¾ã€‚3) æå‡ºäº†EMERTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºæ¥æå‡æƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†æ¢¯åº¦åè½¬å±‚(GRL)æ¥å®ç°å¯¹æŠ—è®­ç»ƒã€‚å¤šä»»åŠ¡Transformeræ¨¡å—ä½¿ç”¨äº†æ ‡å‡†çš„Transformerç»“æ„ï¼Œå¹¶é’ˆå¯¹æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æƒ…æ„Ÿåˆ†ç±»æŸå¤±å’Œæ¨¡æ€å¯¹æŠ—æŸå¤±ã€‚å®éªŒä¸­ï¼Œä½¿ç”¨äº†ä¸ƒç§å¤šæ¨¡æ€åŸºå‡†åè®®å¯¹EMERæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEMERTæ¨¡å‹åœ¨EMERæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒEMERTæ¨¡å‹åœ¨ä¸ƒç§å¤šæ¨¡æ€åŸºå‡†åè®®ä¸Šå‡å–å¾—äº†æœ€ä½³ç»“æœï¼Œå¹³å‡æå‡å¹…åº¦è¶…è¿‡5%ã€‚è¿™éªŒè¯äº†çœ¼éƒ¨è¡Œä¸ºåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠEMERTæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººæœºäº¤äº’ã€å¿ƒç†å¥åº·è¯„ä¼°ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆé¢éƒ¨è¡¨æƒ…å’Œçœ¼éƒ¨è¡Œä¸ºè¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œæä¾›æ›´ä¸ªæ€§åŒ–å’Œäººæ€§åŒ–çš„æœåŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¿ƒç†å¥åº·è¯„ä¼°ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ï¼Œæé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

