---
layout: default
title: VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization
---

# VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16906" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16906v1</a>
  <a href="https://arxiv.org/pdf/2512.16906.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16906v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16906v1', 'VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyan Cong, Haotian Yang, Angtian Wang, Yizhi Wang, Yiding Yang, Canyu Zhang, Chongyang Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VIVAï¼šåŸºäºVLMå¼•å¯¼å’Œå¥–åŠ±ä¼˜åŒ–çš„æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç¼–è¾‘` `æŒ‡ä»¤é©±åŠ¨` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ‰©æ•£æ¨¡å‹` `å¥–åŠ±ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä¸»è¦å—é™äºè®­ç»ƒæ•°æ®ç®€å•ã€‚
2. VIVAåˆ©ç”¨VLMæå–è§†è§‰å¯¹é½çš„æŒ‡ä»¤è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨Edit-GRPOè¿›è¡Œå¥–åŠ±ä¼˜åŒ–ï¼Œæå‡ç¼–è¾‘è´¨é‡å’ŒæŒ‡ä»¤éµå¾ªåº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVIVAåœ¨æŒ‡ä»¤éµå¾ªã€æ³›åŒ–èƒ½åŠ›å’Œç¼–è¾‘è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œæ•ˆæœæ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºVIVAï¼Œä¸€ä¸ªå¯æ‰©å±•çš„æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨VLMå¼•å¯¼çš„ç¼–ç å’Œå¥–åŠ±ä¼˜åŒ–æ¥è§£å†³ç°æœ‰æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªåŸºäºVLMçš„æŒ‡å¯¼å™¨ï¼Œå®ƒå°†æ–‡æœ¬æŒ‡ä»¤ã€æºè§†é¢‘é¦–å¸§å’Œå¯é€‰çš„å‚è€ƒå›¾åƒç¼–ç ä¸ºè§†è§‰å¯¹é½çš„æŒ‡ä»¤è¡¨ç¤ºï¼Œä¸ºæ‰©æ•£Transformerä¸»å¹²ç½‘ç»œæä¾›ç»†ç²’åº¦çš„ç©ºé—´å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œæå‡ºäº†Edit-GRPOåè®­ç»ƒé˜¶æ®µï¼Œå°†Group Relative Policy Optimizationåº”ç”¨äºè§†é¢‘ç¼–è¾‘é¢†åŸŸï¼Œä½¿ç”¨ç›¸å¯¹å¥–åŠ±ç›´æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆç¬¦åˆæŒ‡ä»¤ã€ä¿æŒå†…å®¹ä¸€è‡´ä¸”ç¾è§‚çš„ç¼–è¾‘ç»“æœã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ•°æ®æ„å»ºæµç¨‹ï¼Œç”¨äºåˆæˆç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„è§†é¢‘-æŒ‡ä»¤å¯¹æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVIVAåœ¨æŒ‡ä»¤éµå¾ªã€æ³›åŒ–èƒ½åŠ›å’Œç¼–è¾‘è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¿®æ”¹è¾“å…¥è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒå†…å®¹ä¸€è‡´æ€§å’Œæ—¶é—´è¿è´¯æ€§ã€‚ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸åœ¨ç®€å•çš„ç¼–è¾‘æ“ä½œé…å¯¹æ•°æ®ä¸Šè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ³›åŒ–åˆ°å¤šæ ·åŒ–å’Œå¤æ‚çš„çœŸå®ä¸–ç•ŒæŒ‡ä»¤çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVIVAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æ›´å¥½åœ°ç†è§£æŒ‡ä»¤ï¼Œå¹¶ç»“åˆå¥–åŠ±ä¼˜åŒ–æ¥æå‡ç¼–è¾‘è´¨é‡ã€‚VLMèƒ½å¤Ÿå°†æ–‡æœ¬æŒ‡ä»¤ä¸è§†é¢‘å†…å®¹å¯¹é½ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¥–åŠ±ä¼˜åŒ–åˆ™ç›´æ¥é©±åŠ¨æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„ç¼–è¾‘ç»“æœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVIVAæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šVLMå¼•å¯¼çš„ç¼–ç é˜¶æ®µå’ŒEdit-GRPOåè®­ç»ƒé˜¶æ®µã€‚åœ¨ç¼–ç é˜¶æ®µï¼ŒVLMå°†æ–‡æœ¬æŒ‡ä»¤ã€æºè§†é¢‘é¦–å¸§å’Œå¯é€‰çš„å‚è€ƒå›¾åƒç¼–ç ä¸ºè§†è§‰å¯¹é½çš„æŒ‡ä»¤è¡¨ç¤ºã€‚ç„¶åï¼Œè¿™äº›è¡¨ç¤ºè¢«è¾“å…¥åˆ°æ‰©æ•£Transformerä¸»å¹²ç½‘ç»œä¸­è¿›è¡Œè§†é¢‘ç¼–è¾‘ã€‚åœ¨Edit-GRPOé˜¶æ®µï¼Œä½¿ç”¨ç›¸å¯¹å¥–åŠ±ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆæ›´ç¬¦åˆæŒ‡ä»¤ã€ä¿æŒå†…å®¹ä¸€è‡´ä¸”ç¾è§‚çš„ç¼–è¾‘ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šVIVAçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹ä¸¤ç‚¹ï¼š1) åˆ©ç”¨VLMè¿›è¡ŒæŒ‡ä»¤ç¼–ç ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£æŒ‡ä»¤çš„è¯­ä¹‰å’Œè§†è§‰ä¿¡æ¯ï¼›2) æå‡ºEdit-GRPOåè®­ç»ƒæ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–æ¨¡å‹çš„ç¼–è¾‘è´¨é‡ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»åå¥½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVIVAèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆæ›´é«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šVLMæŒ‡å¯¼å™¨ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹è§†é¢‘ç¼–è¾‘ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚Edit-GRPOé‡‡ç”¨Group Relative Policy Optimizationï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒç¼–è¾‘ç»“æœçš„ç›¸å¯¹è´¨é‡æ¥ä¼˜åŒ–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªæ•°æ®æ„å»ºæµç¨‹ï¼Œç”¨äºåˆæˆç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„è§†é¢‘-æŒ‡ä»¤å¯¹æ•°æ®ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16906v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16906v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16906v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVIVAåœ¨å¤šä¸ªè§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒVIVAåœ¨æŒ‡ä»¤éµå¾ªåº¦ã€å†…å®¹ä¸€è‡´æ€§å’Œç¼–è¾‘è´¨é‡æ–¹é¢å‡æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ä¿¡æ¯è¯·å‚è€ƒè®ºæ–‡åŸæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VIVAæŠ€æœ¯å¯åº”ç”¨äºç”µå½±åˆ¶ä½œã€å¹¿å‘Šè®¾è®¡ã€ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸï¼Œå®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿé™ä½è§†é¢‘ç¼–è¾‘çš„é—¨æ§›ï¼Œè®©ç”¨æˆ·é€šè¿‡ç®€å•çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤å³å¯å®Œæˆå¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå•†ä¸šä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

