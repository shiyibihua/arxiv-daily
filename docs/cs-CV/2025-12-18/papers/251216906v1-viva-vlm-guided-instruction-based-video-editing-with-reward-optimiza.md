---
layout: default
title: VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization
---

# VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16906" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16906v1</a>
  <a href="https://arxiv.org/pdf/2512.16906.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16906v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16906v1', 'VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyan Cong, Haotian Yang, Angtian Wang, Yizhi Wang, Yiding Yang, Canyu Zhang, Chongyang Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VIVAï¼šåˆ©ç”¨VLMå¼•å¯¼å’Œå¥–åŠ±ä¼˜åŒ–çš„æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç¼–è¾‘` `æŒ‡ä»¤é©±åŠ¨` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ‰©æ•£æ¨¡å‹` `å¥–åŠ±ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¾èµ–ç®€å•ç¼–è¾‘æ“ä½œçš„é…å¯¹æ•°æ®è®­ç»ƒï¼Œæ³›åŒ–åˆ°å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›æœ‰é™ã€‚
2. VIVAåˆ©ç”¨VLMç¼–ç æŒ‡ä»¤å’Œè§†é¢‘ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å¥–åŠ±ä¼˜åŒ–ç­–ç•¥ï¼Œæå‡æ¨¡å‹å¯¹å¤æ‚æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVIVAåœ¨æŒ‡ä»¤éµå¾ªã€å†…å®¹ä¿æŒå’Œç¼–è¾‘è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºVIVAï¼Œä¸€ä¸ªå¯æ‰©å±•çš„æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨VLMå¼•å¯¼çš„ç¼–ç å’Œå¥–åŠ±ä¼˜åŒ–æ¥è§£å†³ç°æœ‰æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚VIVAé¦–å…ˆå¼•å…¥ä¸€ä¸ªåŸºäºVLMçš„æŒ‡å¯¼å™¨ï¼Œå°†æ–‡æœ¬æŒ‡ä»¤ã€æºè§†é¢‘çš„ç¬¬ä¸€å¸§ä»¥åŠå¯é€‰çš„å‚è€ƒå›¾åƒç¼–ç ä¸ºè§†è§‰å¯¹é½çš„æŒ‡ä»¤è¡¨ç¤ºï¼Œä¸ºæ‰©æ•£Transformerä¸»å¹²ç½‘ç»œæä¾›ç²¾ç»†çš„ç©ºé—´å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ä¸ªåè®­ç»ƒé˜¶æ®µEdit-GRPOï¼Œå°†Group Relative Policy Optimizationé€‚é…åˆ°è§†é¢‘ç¼–è¾‘é¢†åŸŸï¼Œä½¿ç”¨ç›¸å¯¹å¥–åŠ±ç›´æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆç¬¦åˆæŒ‡ä»¤ã€ä¿æŒå†…å®¹ä¸€è‡´ä¸”ç¾è§‚çš„ç¼–è¾‘ç»“æœã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªæ•°æ®æ„å»ºæµç¨‹ï¼Œç”¨äºåˆæˆç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„è§†é¢‘-æŒ‡ä»¤å¯¹æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVIVAåœ¨æŒ‡ä»¤éµå¾ªã€æ³›åŒ–èƒ½åŠ›å’Œç¼–è¾‘è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¿®æ”¹è¾“å…¥è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒå†…å®¹ä¸€è‡´æ€§å’Œæ—¶é—´è¿è´¯æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåœ¨ç®€å•ç¼–è¾‘æ“ä½œçš„é…å¯¹æ•°æ®ä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤„ç†å¤šæ ·åŒ–å’Œå¤æ‚çš„çœŸå®ä¸–ç•ŒæŒ‡ä»¤æ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥ç†è§£å¤æ‚æŒ‡ä»¤ä¸­çš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä¸”éš¾ä»¥åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒè§†é¢‘å†…å®¹çš„åŸæœ‰ç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVIVAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å¢å¼ºæ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¥–åŠ±ä¼˜åŒ–æ¥æå‡ç¼–è¾‘è´¨é‡ã€‚VLMèƒ½å¤Ÿå°†æ–‡æœ¬æŒ‡ä»¤å’Œè§†é¢‘å¸§ç¼–ç ä¸ºç»Ÿä¸€çš„è§†è§‰è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œä»è€Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¥–åŠ±ä¼˜åŒ–åˆ™å…è®¸æ¨¡å‹ç›´æ¥å­¦ä¹ å¦‚ä½•ç”Ÿæˆç¬¦åˆæŒ‡ä»¤ã€ä¿æŒå†…å®¹ä¸€è‡´ä¸”ç¾è§‚çš„ç¼–è¾‘ç»“æœï¼Œè€Œæ— éœ€ä¾èµ–å¤§é‡çš„é…å¯¹æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVIVAæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šVLMå¼•å¯¼çš„ç¼–ç å’Œå¥–åŠ±ä¼˜åŒ–ã€‚é¦–å…ˆï¼ŒVLM-based Instructoræ¨¡å—å°†æ–‡æœ¬æŒ‡ä»¤ã€æºè§†é¢‘çš„ç¬¬ä¸€å¸§ä»¥åŠå¯é€‰çš„å‚è€ƒå›¾åƒç¼–ç ä¸ºè§†è§‰å¯¹é½çš„æŒ‡ä»¤è¡¨ç¤ºã€‚ç„¶åï¼Œè¿™äº›è¡¨ç¤ºè¢«è¾“å…¥åˆ°æ‰©æ•£Transformerä¸»å¹²ç½‘ç»œä¸­ï¼Œç”¨äºæŒ‡å¯¼è§†é¢‘ç¼–è¾‘è¿‡ç¨‹ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼ŒEdit-GRPOæ¨¡å—ä½¿ç”¨Group Relative Policy Optimizationç®—æ³•ï¼Œæ ¹æ®ç›¸å¯¹å¥–åŠ±æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œä»è€Œæå‡ç¼–è¾‘è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVIVAçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1) å¼•å…¥VLMæ¥å¢å¼ºæ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤æ‚æŒ‡ä»¤ã€‚2) æå‡ºEdit-GRPOï¼Œå°†Group Relative Policy Optimizationé€‚é…åˆ°è§†é¢‘ç¼–è¾‘é¢†åŸŸï¼Œç›´æ¥ä¼˜åŒ–ç¼–è¾‘è´¨é‡ã€‚3) è®¾è®¡äº†ä¸€ä¸ªæ•°æ®æ„å»ºæµç¨‹ï¼Œç”¨äºåˆæˆç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„è§†é¢‘-æŒ‡ä»¤å¯¹æ•°æ®ï¼Œä»è€Œç¼“è§£äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVIVAèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤æ‚æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆæ›´é«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šVLM-based Instructorä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚CLIPï¼Œæ¥æå–æ–‡æœ¬å’Œå›¾åƒçš„ç‰¹å¾ã€‚Edit-GRPOä½¿ç”¨ç›¸å¯¹å¥–åŠ±æ¥è¯„ä¼°ç¼–è¾‘ç»“æœçš„è´¨é‡ï¼Œä¾‹å¦‚ï¼Œåˆ¤æ–­ä¸€ä¸ªç¼–è¾‘ç»“æœæ˜¯å¦æ¯”å¦ä¸€ä¸ªæ›´ç¬¦åˆæŒ‡ä»¤æˆ–æ›´ç¾è§‚ã€‚æ•°æ®æ„å»ºæµç¨‹ä½¿ç”¨ç¨‹åºåŒ–ç”Ÿæˆå’Œäººå·¥æ ‡æ³¨ç›¸ç»“åˆçš„æ–¹å¼ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„è§†é¢‘-æŒ‡ä»¤å¯¹æ•°æ®ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°åŒ…æ‹¬æŒ‡ä»¤éµå¾ªæŸå¤±ã€å†…å®¹ä¿æŒæŸå¤±å’Œç¾å­¦æŸå¤±ç­‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16906v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16906v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16906v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVIVAåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŒ‡ä»¤éµå¾ªåº¦æ–¹é¢ï¼ŒVIVAæ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚åœ¨ç”¨æˆ·åå¥½åº¦æ–¹é¢ï¼ŒVIVAç”Ÿæˆçš„ç¼–è¾‘ç»“æœæ›´å—ç”¨æˆ·é’çã€‚æ­¤å¤–ï¼ŒVIVAè¿˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§å¤æ‚æŒ‡ä»¤å’Œä¸åŒç±»å‹çš„è§†é¢‘å†…å®¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VIVAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è§†é¢‘å†…å®¹åˆ›ä½œã€ç¤¾äº¤åª’ä½“ç¼–è¾‘ã€å¹¿å‘Šåˆ¶ä½œã€æ•™è‚²è§†é¢‘ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾åœ°æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¿®æ”¹è§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚æ”¹å˜è§†é¢‘é£æ ¼ã€æ·»åŠ ç‰¹æ•ˆã€æ›¿æ¢å¯¹è±¡ç­‰ã€‚VIVAçš„å‡ºç°æœ‰æœ›é™ä½è§†é¢‘ç¼–è¾‘çš„é—¨æ§›ï¼Œè®©æ›´å¤šäººèƒ½å¤Ÿå‚ä¸åˆ°è§†é¢‘åˆ›ä½œä¸­æ¥ï¼Œå¹¶æ¨åŠ¨è§†é¢‘å†…å®¹äº§ä¸šçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

