---
layout: default
title: TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering
---

# TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16270" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16270v1</a>
  <a href="https://arxiv.org/pdf/2512.16270.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16270v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16270v1', 'TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Gui, Yang Wan, Haochen Han, Dongxing Mao, Fangming Liu, Min Li, Alex Jinpeng Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTextEditBenchï¼Œç”¨äºè¯„ä¼°å›¾åƒæ–‡æœ¬ç¼–è¾‘ä¸­è•´å«æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬ç¼–è¾‘` `å›¾åƒç”Ÿæˆ` `æ¨ç†èƒ½åŠ›` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯„ä¼°åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨æ–‡æœ¬ç¼–è¾‘æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆå­—ç¬¦çš„å¯è¯»æ€§ä»¥åŠè¯­ä¹‰ã€å‡ ä½•å’Œä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚
2. TextEditBenchåŸºå‡†æµ‹è¯•é€šè¿‡å…³æ³¨å›¾åƒä¸­ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„åŒºåŸŸï¼Œå¼ºè°ƒæ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹åœºæ™¯ä¸‹çš„ç¼–è¾‘èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡æ¨ç†ã€ç‰©ç†ä¸€è‡´æ€§å’Œå¸ƒå±€æ„ŸçŸ¥é›†æˆæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæœ‰å¾…è¿›ä¸€æ­¥æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†TextEditBenchï¼Œä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨å…³æ³¨å›¾åƒä¸­ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„åŒºåŸŸã€‚ä¸åŸºæœ¬çš„åƒç´ æ“ä½œä¸åŒï¼Œè¯¥åŸºå‡†å¼ºè°ƒæ¨ç†å¯†é›†å‹çš„ç¼–è¾‘åœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹ç†è§£ç‰©ç†åˆç†æ€§ã€è¯­è¨€æ„ä¹‰å’Œè·¨æ¨¡æ€ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°ç»´åº¦ï¼Œå³è¯­ä¹‰æœŸæœ›ï¼ˆSEï¼‰ï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨æ–‡æœ¬ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œè·¨æ¨¡æ€å¯¹é½çš„æ¨ç†èƒ½åŠ›ã€‚å¯¹æœ€å…ˆè¿›çš„ç¼–è¾‘ç³»ç»Ÿè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„æ¨¡å‹å¯ä»¥éµå¾ªç®€å•çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥å¤„ç†ä¾èµ–äºä¸Šä¸‹æ–‡çš„æ¨ç†ã€ç‰©ç†ä¸€è‡´æ€§å’Œå¸ƒå±€æ„ŸçŸ¥çš„é›†æˆã€‚é€šè¿‡ä¸“æ³¨äºè¿™ç§é•¿æœŸè¢«å¿½è§†ä½†åˆè‡³å…³é‡è¦çš„èƒ½åŠ›ï¼ŒTextEditBench ä¸ºæ¨è¿›æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘å’Œå¤šæ¨¡æ€ç”Ÿæˆä¸­çš„æ¨ç†å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æµ‹è¯•å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬ç¼–è¾‘ï¼Œå¾€å¾€åªå…³æ³¨åƒç´ å±‚é¢çš„æ“ä½œï¼Œç¼ºä¹å¯¹æ·±å±‚è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç†è§£ï¼Œå¯¼è‡´ç¼–è¾‘åçš„å›¾åƒåœ¨ç‰©ç†åˆç†æ€§ã€è¯­è¨€æ„ä¹‰å’Œè·¨æ¨¡æ€ä¾èµ–å…³ç³»ä¸Šå‡ºç°ä¸ä¸€è‡´ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼Œæ ¹æ®åœºæ™¯è°ƒæ•´æ–‡æœ¬çš„å­—ä½“ã€é¢œè‰²æˆ–ä½ç½®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTextEditBenchçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°åŸºå‡†ï¼Œè¯¥åŸºå‡†ä¸ä»…å…³æ³¨æ–‡æœ¬çš„å¯è¯»æ€§ï¼Œæ›´ä¾§é‡äºè¯„ä¼°æ¨¡å‹åœ¨æ–‡æœ¬ç¼–è¾‘è¿‡ç¨‹ä¸­å¯¹ç‰©ç†ä¸–ç•ŒçŸ¥è¯†ã€è¯­è¨€è¯­ä¹‰ä»¥åŠè·¨æ¨¡æ€å…³ç³»çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ¨ç†å¯†é›†å‹çš„ç¼–è¾‘åœºæ™¯ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ å’Œåº”ç”¨æ›´é«˜çº§çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTextEditBenchä¸»è¦åŒ…å«ä¸¤éƒ¨åˆ†ï¼šæ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æ•°æ®é›†åŒ…å«å„ç§å›¾åƒå’Œæ–‡æœ¬ç¼–è¾‘æŒ‡ä»¤ï¼Œæ¶µç›–äº†éœ€è¦ç‰©ç†åˆç†æ€§ã€è¯­è¨€æ„ä¹‰å’Œè·¨æ¨¡æ€ä¾èµ–å…³ç³»çš„å¤æ‚åœºæ™¯ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ä¼ ç»Ÿçš„åƒç´ çº§æŒ‡æ ‡ï¼ˆå¦‚PSNRã€SSIMï¼‰ï¼Œä»¥åŠæ–°æå‡ºçš„è¯­ä¹‰æœŸæœ›ï¼ˆSEï¼‰æŒ‡æ ‡ã€‚SEæŒ‡æ ‡æ—¨åœ¨è¡¡é‡æ¨¡å‹åœ¨æ–‡æœ¬ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œè·¨æ¨¡æ€å¯¹é½çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šTextEditBenchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹æ¨ç†èƒ½åŠ›çš„å¼ºè°ƒå’ŒSEæŒ‡æ ‡çš„æå‡ºã€‚ä¸ä»¥å¾€çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒTextEditBenchæ›´åŠ å…³æ³¨æ¨¡å‹å¯¹å›¾åƒä¸­éšå«ä¿¡æ¯çš„ç†è§£å’Œåˆ©ç”¨ï¼Œä»¥åŠåœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒè¿™äº›ä¿¡æ¯ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚SEæŒ‡æ ‡åˆ™æä¾›äº†ä¸€ç§é‡åŒ–è¯„ä¼°æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ‰‹æ®µï¼Œå¼¥è¡¥äº†ä¼ ç»ŸæŒ‡æ ‡çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šTextEditBenchæ•°æ®é›†çš„è®¾è®¡è€ƒè™‘äº†å¤šç§å› ç´ ï¼ŒåŒ…æ‹¬æ–‡æœ¬çš„ç±»å‹ã€å­—ä½“ã€å¤§å°ã€ä½ç½®ï¼Œä»¥åŠå›¾åƒçš„åœºæ™¯ã€å…‰ç…§ã€é®æŒ¡ç­‰ã€‚SEæŒ‡æ ‡çš„è®¡ç®—æ–¹æ³•æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œéœ€è¦æŸ¥é˜…è¡¥å……ææ–™æˆ–ç›¸å…³æ–‡çŒ®ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16270v1/figures/src/data_collection.jpg" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16270v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16270v1/figures/src/evaluation.jpg" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

TextEditBenchå¯¹ç°æœ‰å›¾åƒç¼–è¾‘æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹åœ¨ç®€å•çš„æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç‰¹åˆ«æ˜¯åœ¨ç‰©ç†ä¸€è‡´æ€§ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢ï¼Œç°æœ‰æ¨¡å‹ä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚è¿™äº›å®éªŒç»“æœçªæ˜¾äº†TextEditBenchçš„ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TextEditBenchçš„ç ”ç©¶æˆæœå¯ä»¥åº”ç”¨äºæ™ºèƒ½å›¾åƒç¼–è¾‘ã€å¹¿å‘Šè®¾è®¡ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨è¯¥åŸºå‡†æµ‹è¯•æ¥æå‡å›¾åƒç¼–è¾‘è½¯ä»¶çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œè‡ªåŠ¨å®Œæˆå¤æ‚çš„æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶ä¿è¯ç¼–è¾‘ç»“æœçš„çœŸå®æ€§å’Œåˆç†æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›å¤šæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äº¤äº’å¼åº”ç”¨å¥ å®šåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

