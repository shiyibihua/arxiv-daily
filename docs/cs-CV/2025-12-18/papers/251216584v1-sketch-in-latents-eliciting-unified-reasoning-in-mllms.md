---
layout: default
title: Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs
---

# Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16584" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16584v1</a>
  <a href="https://arxiv.org/pdf/2512.16584.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16584v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16584v1', 'Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, Ruixuan Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 14 pages, 11 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TungChintao/SkiLa)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSketch-in-Latents (SkiLa)ï¼Œå®ç°MLLMä¸­ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ä¸è§†è§‰æƒ³è±¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æƒ³è±¡` `ç»Ÿä¸€æ¨ç†` `æ½œåœ¨ç©ºé—´` `è‰å›¾ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨è§†è§‰æƒ³è±¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·çµæ´»åœ°è¿›è¡Œè§†è§‰-æ–‡æœ¬äº¤äº’ã€‚
2. SkiLaé€šè¿‡åœ¨MLLMçš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆè¿ç»­çš„è§†è§‰åµŒå…¥ï¼ˆæ½œåœ¨è‰å›¾tokenï¼‰æ¥å®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSkiLaåœ¨è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¯¹é€šç”¨å¤šæ¨¡æ€åŸºå‡†å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æ“…é•¿é€šè¿‡æ–‡æœ¬æ¨ç†è¿›è¡Œè§†è§‰ç†è§£ä»»åŠ¡ï¼Œä½†åœ¨éœ€è¦è§†è§‰æƒ³è±¡çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸é‡‡ç”¨é¢„å®šä¹‰å¤–éƒ¨å·¥å…·åŒ…æˆ–åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå›¾åƒçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œäººç±»å¯ä»¥åœ¨æ€è€ƒè¿‡ç¨‹ä¸­å½¢æˆçµæ´»çš„è§†è§‰-æ–‡æœ¬æƒ³è±¡å’Œäº¤äº’ï¼Œè€Œæ— éœ€é¢„å®šä¹‰çš„å·¥å…·åŒ…ï¼Œå…¶ä¸­ä¸€ä¸ªé‡è¦åŸå› æ˜¯äººç±»åœ¨å¤§è„‘å†…éƒ¨çš„ç»Ÿä¸€ç©ºé—´ä¸­æ„å»ºè§†è§‰-æ–‡æœ¬æ€è€ƒè¿‡ç¨‹ã€‚å—æ­¤èƒ½åŠ›çš„å¯å‘ï¼Œé‰´äºå½“å‰çš„MLLMå·²ç»å°†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ç¼–ç åœ¨ç›¸åŒçš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè§†è§‰tokenå¯ä»¥æ— ç¼åœ°æ’å…¥åˆ°æ–‡æœ¬tokenæ‰€æºå¸¦çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„è§†è§‰æƒ³è±¡è¿‡ç¨‹éƒ½å¯ä»¥ç”±æ½œåœ¨ç‰¹å¾ç¼–ç ã€‚ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºSketch-in-Latents (SkiLa)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„æ–°èŒƒå¼ï¼Œå®ƒæ‰©å±•äº†MLLMçš„è‡ªå›å½’èƒ½åŠ›ï¼Œä»¥åŸç”Ÿç”Ÿæˆè¿ç»­çš„è§†è§‰åµŒå…¥ï¼Œç§°ä¸ºæ½œåœ¨è‰å›¾tokenï¼Œä½œä¸ºè§†è§‰æ€è€ƒã€‚åœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ€è€ƒtokençš„æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œç”Ÿæˆæ½œåœ¨è‰å›¾tokençš„è§†è§‰è‰å›¾æ¨¡å¼ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚æå‡ºäº†ä¸€ç§æ½œåœ¨çš„è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ï¼Œä»¥ç¡®ä¿è¿™äº›æ½œåœ¨çš„è‰å›¾tokenåœ¨è¯­ä¹‰ä¸Šæ˜¯æ¥åœ°çš„ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒSkiLaåœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒæ—¶å¯¹å„ç§é€šç”¨å¤šæ¨¡æ€åŸºå‡†è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMåœ¨å¤„ç†éœ€è¦è§†è§‰æƒ³è±¡çš„ä»»åŠ¡æ—¶ï¼Œä¾èµ–äºé¢„å®šä¹‰çš„å¤–éƒ¨å·¥å…·æˆ–åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆå›¾åƒï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚äººç±»å¯ä»¥åœ¨å¤§è„‘å†…éƒ¨çš„ç»Ÿä¸€ç©ºé—´ä¸­è¿›è¡Œè§†è§‰-æ–‡æœ¬æ€è€ƒï¼Œè€ŒMLLMç¼ºä¹è¿™ç§èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•ä½¿MLLMèƒ½å¤Ÿåƒäººç±»ä¸€æ ·è¿›è¡Œçµæ´»çš„è§†è§‰-æ–‡æœ¬æ¨ç†å’Œæƒ³è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSkiLaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰ä¿¡æ¯ç›´æ¥åµŒå…¥åˆ°MLLMçš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½œä¸ºæ¨ç†è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚é€šè¿‡ç”Ÿæˆâ€œæ½œåœ¨è‰å›¾tokenâ€ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ–‡æœ¬æ¨ç†çš„åŒæ—¶è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤–éƒ¨å·¥å…·çš„ä¾èµ–ï¼Œå¹¶å…è®¸æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­çµæ´»åœ°æ“ä½œè§†è§‰ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSkiLaçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œè§†è§‰è‰å›¾æ¨¡å¼ã€‚åœ¨æ–‡æœ¬æ€è€ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆæ–‡æœ¬tokenè¿›è¡Œæ¨ç†ï¼›åœ¨è§†è§‰è‰å›¾æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆæ½œåœ¨è‰å›¾tokenè¿›è¡Œè§†è§‰æƒ³è±¡ã€‚æ¨¡å‹åœ¨è¿™ä¸¤ç§æ¨¡å¼ä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œä»¥å®Œæˆå¤šæ­¥æ¨ç†ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿æ½œåœ¨è‰å›¾tokençš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒSkiLaè¿˜å¼•å…¥äº†ä¸€ç§æ½œåœ¨è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šSkiLaçš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†è§‰æƒ³è±¡è¿‡ç¨‹èå…¥åˆ°MLLMçš„è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚é€šè¿‡ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆè§†è§‰tokenï¼ŒSkiLaå®ç°äº†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„ç»Ÿä¸€è¡¨ç¤ºå’Œæ¨ç†ã€‚è¿™ç§æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ï¼ˆä¾èµ–å¤–éƒ¨å·¥å…·æˆ–ç”Ÿæˆå›¾åƒï¼‰çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å†…éƒ¨è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œè€Œæ— éœ€é¢å¤–çš„æ­¥éª¤æˆ–æ¨¡å—ã€‚

**å…³é”®è®¾è®¡**ï¼šSkiLaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ½œåœ¨è‰å›¾tokençš„ç”Ÿæˆæ–¹å¼ï¼Œå¯èƒ½æ¶‰åŠç‰¹å®šçš„ç½‘ç»œç»“æ„æˆ–æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„tokenå…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼›2) æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œè§†è§‰è‰å›¾æ¨¡å¼ä¹‹é—´çš„åˆ‡æ¢æœºåˆ¶ï¼Œå¯èƒ½åŸºäºæŸç§ç­–ç•¥æˆ–æ§åˆ¶ä¿¡å·ï¼›3) æ½œåœ¨è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ï¼Œç”¨äºå°†æ½œåœ¨è‰å›¾tokenæ˜ å°„å›è§†è§‰ç©ºé—´ï¼Œå¹¶ç¡®ä¿å…¶ä¸åŸå§‹è§†è§‰ä¿¡æ¯çš„ä¸€è‡´æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/method.jpg" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/hyper.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/case_geo.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSkiLaåœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†ç°æœ‰çš„MLLMæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSkiLaåœ¨å„ç§é€šç”¨å¤šæ¨¡æ€åŸºå‡†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SkiLaå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€æœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨æ›´å¥½åœ°ç†è§£å’Œæ¨ç†è§†è§‰ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆæ›´è‡ªç„¶å’Œè¿è´¯çš„å¤šæ¨¡æ€å†…å®¹ã€‚æœªæ¥ï¼ŒSkiLaæœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€ä¸‰ç»´é‡å»ºå’Œè™šæ‹Ÿç°å®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

