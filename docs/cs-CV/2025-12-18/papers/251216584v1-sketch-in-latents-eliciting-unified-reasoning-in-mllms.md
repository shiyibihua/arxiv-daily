---
layout: default
title: Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs
---

# Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16584" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16584v1</a>
  <a href="https://arxiv.org/pdf/2512.16584.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16584v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16584v1', 'Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, Ruixuan Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 14 pages, 11 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TungChintao/SkiLa)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSketch-in-Latents (SkiLa)ï¼Œå®ç°MLLMä¸­ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ä¸è§†è§‰æƒ³è±¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰æ¨ç†` `è§†è§‰æƒ³è±¡` `æ½œåœ¨ç©ºé—´` `è‡ªå›å½’æ¨¡å‹` `è¯­ä¹‰é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨è§†è§‰æƒ³è±¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·çµæ´»è¿›è¡Œè§†è§‰-æ–‡æœ¬äº¤äº’ã€‚
2. SkiLaé€šè¿‡ç”Ÿæˆæ½œåœ¨è‰å›¾tokenï¼Œå°†è§†è§‰ä¿¡æ¯æ— ç¼èå…¥MLLMçš„æ¨ç†è¿‡ç¨‹ï¼Œå®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSkiLaåœ¨è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¯¹é€šç”¨å¤šæ¨¡æ€åŸºå‡†å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æ“…é•¿é€šè¿‡æ–‡æœ¬æ¨ç†è¿›è¡Œè§†è§‰ç†è§£ä»»åŠ¡ï¼Œä½†åœ¨éœ€è¦è§†è§‰æƒ³è±¡çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸é‡‡ç”¨é¢„å®šä¹‰å¤–éƒ¨å·¥å…·åŒ…æˆ–åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå›¾åƒçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œäººç±»å¯ä»¥åœ¨æ²¡æœ‰é¢„å®šä¹‰å·¥å…·åŒ…çš„æƒ…å†µä¸‹ï¼Œåœ¨æ€è€ƒè¿‡ç¨‹ä¸­å½¢æˆçµæ´»çš„è§†è§‰-æ–‡æœ¬æƒ³è±¡å’Œäº¤äº’ï¼Œä¸€ä¸ªé‡è¦çš„åŸå› æ˜¯äººç±»åœ¨å¤§è„‘å†…éƒ¨çš„ç»Ÿä¸€ç©ºé—´ä¸­æ„å»ºè§†è§‰-æ–‡æœ¬æ€è€ƒè¿‡ç¨‹ã€‚å—æ­¤å¯å‘ï¼Œè€ƒè™‘åˆ°å½“å‰çš„MLLMå·²ç»å°†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ç¼–ç åœ¨ç›¸åŒçš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè§†è§‰tokenå¯ä»¥æ— ç¼åœ°æ’å…¥åˆ°æ–‡æœ¬tokenæ‰€æºå¸¦çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„è§†è§‰æƒ³è±¡è¿‡ç¨‹éƒ½å¯ä»¥ç”±æ½œåœ¨ç‰¹å¾ç¼–ç ã€‚ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºSketch-in-Latents (SkiLa)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„æ–°èŒƒå¼ï¼Œå®ƒæ‰©å±•äº†MLLMçš„è‡ªå›å½’èƒ½åŠ›ï¼Œä»¥åŸç”Ÿç”Ÿæˆè¿ç»­çš„è§†è§‰åµŒå…¥ï¼Œç§°ä¸ºæ½œåœ¨è‰å›¾tokenï¼Œä½œä¸ºè§†è§‰æ€æƒ³ã€‚åœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åŠ¨æ€åœ°åœ¨ç”¨äºç”Ÿæˆæ–‡æœ¬æ€è€ƒtokençš„æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œç”¨äºç”Ÿæˆæ½œåœ¨è‰å›¾tokençš„è§†è§‰è‰å›¾æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚æå‡ºäº†ä¸€ç§æ½œåœ¨çš„è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ï¼Œä»¥ç¡®ä¿è¿™äº›æ½œåœ¨çš„è‰å›¾tokenåœ¨è¯­ä¹‰ä¸Šæ˜¯æ¥åœ°çš„ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒSkiLaåœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒæ—¶å¯¹å„ç§é€šç”¨å¤šæ¨¡æ€åŸºå‡†è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMè™½ç„¶æ“…é•¿è§†è§‰ç†è§£ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„è§†è§‰æƒ³è±¡èƒ½åŠ›ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»åœ°è¿›è¡Œè§†è§‰-æ–‡æœ¬äº¤äº’ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„å¤–éƒ¨å·¥å…·æˆ–åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆå›¾åƒï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSkiLaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰æƒ³è±¡è¿‡ç¨‹ç¼–ç ä¸ºæ½œåœ¨ç‰¹å¾ï¼Œå¹¶å°†å…¶æ— ç¼åœ°æ’å…¥åˆ°MLLMçš„æ–‡æœ¬æ¨ç†è¿‡ç¨‹ä¸­ã€‚é€šè¿‡ç”Ÿæˆè¿ç»­çš„è§†è§‰åµŒå…¥ï¼ˆæ½œåœ¨è‰å›¾tokenï¼‰ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°è¿›è¡Œæ–‡æœ¬æ€è€ƒå’Œè§†è§‰è‰å›¾ç»˜åˆ¶ï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»å¤§è„‘ä¸­è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯åœ¨ç»Ÿä¸€ç©ºé—´ä¸­è¿›è¡Œäº¤äº’çš„æ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSkiLaçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œè§†è§‰è‰å›¾æ¨¡å¼ã€‚åœ¨æ–‡æœ¬æ€è€ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆæ–‡æœ¬tokenè¿›è¡Œæ¨ç†ï¼›åœ¨è§†è§‰è‰å›¾æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆæ½œåœ¨è‰å›¾tokenï¼Œè¡¨ç¤ºè§†è§‰ä¿¡æ¯ã€‚æ¨¡å‹åœ¨è¿™ä¸¤ç§æ¨¡å¼ä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œä»¥å®ç°å¤šæ­¥æ¨ç†ã€‚ä¸ºäº†ç¡®ä¿æ½œåœ¨è‰å›¾tokençš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒSkiLaè¿˜å¼•å…¥äº†ä¸€ç§æ½œåœ¨è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šSkiLaæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå®ƒæ‰©å±•äº†MLLMçš„è‡ªå›å½’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤ŸåŸç”Ÿç”Ÿæˆè¿ç»­çš„è§†è§‰åµŒå…¥ï¼ˆæ½œåœ¨è‰å›¾tokenï¼‰ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤–éƒ¨å·¥å…·æˆ–ç”Ÿæˆç¦»æ•£å›¾åƒçš„æ–¹å¼ä¸åŒï¼ŒSkiLaå°†è§†è§‰æƒ³è±¡è¿‡ç¨‹ç›´æ¥èå…¥åˆ°MLLMçš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ç°äº†æ›´ç´§å¯†çš„è§†è§‰-æ–‡æœ¬èåˆã€‚

**å…³é”®è®¾è®¡**ï¼šSkiLaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ½œåœ¨è‰å›¾tokençš„ç”Ÿæˆæ–¹å¼ï¼Œå¯èƒ½æ¶‰åŠåˆ°ç‰¹å®šçš„ç½‘ç»œç»“æ„æˆ–æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„tokenå…·æœ‰è‰¯å¥½çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼›2) æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œè§†è§‰è‰å›¾æ¨¡å¼ä¹‹é—´çš„åˆ‡æ¢æœºåˆ¶ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ¨ç†æ­¥éª¤è¿›è¡Œè®¾è®¡ï¼›3) æ½œåœ¨è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ï¼Œå¯èƒ½æ¶‰åŠåˆ°å¯¹æŠ—è®­ç»ƒæˆ–è‡ªç›‘ç£å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œä»¥ç¡®ä¿æ½œåœ¨è‰å›¾tokençš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/method.jpg" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/hyper.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/case_geo.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

SkiLaåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å…·ä½“å®éªŒç»“æœï¼ˆè®ºæ–‡ä¸­æä¾›ï¼‰è¡¨æ˜ï¼ŒSkiLaåœ¨è§†è§‰é—®ç­”ã€å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨é€šç”¨å¤šæ¨¡æ€åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSkiLaæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SkiLaå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒç”Ÿæˆã€æœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äº¤äº’ã€‚æœªæ¥ï¼ŒSkiLaæœ‰æœ›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

