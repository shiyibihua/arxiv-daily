---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-18
---

# cs.CVï¼ˆ2025-12-18ï¼‰

ğŸ“Š å…± **37** ç¯‡è®ºæ–‡
 | ğŸ”— **11** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (12 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251216791v1-kinest-a-kinematics-guided-spatiotemporal-state-space-model-for-huma.html">KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</a></td>
  <td>KineSTï¼šä¸€ç§åŸºäºè¿åŠ¨å­¦å¼•å¯¼çš„æ—¶ç©ºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œç”¨äºä»ç¨€ç–ä¿¡å·ä¸­è¿›è¡Œäººä½“è¿åŠ¨è·Ÿè¸ª</td>
  <td class="tags-cell"><span class="paper-tag">state space model</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16791v1" data-paper-url="./papers/251216791v1-kinest-a-kinematics-guided-spatiotemporal-state-space-model-for-huma.html" onclick="toggleFavorite(this, '2512.16791v1', 'KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251216413v1-brepllm-native-boundary-representation-understanding-with-large-lang.html">BrepLLM: Native Boundary Representation Understanding with Large Language Models</a></td>
  <td>BrepLLMï¼šæå‡ºä¸€ç§åŸç”Ÿè¾¹ç•Œè¡¨ç¤ºç†è§£çš„å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">semantic mapping</span> <span class="paper-tag">semantic map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16413v1" data-paper-url="./papers/251216413v1-brepllm-native-boundary-representation-understanding-with-large-lang.html" onclick="toggleFavorite(this, '2512.16413v1', 'BrepLLM: Native Boundary Representation Understanding with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251216461v1-snow-spatio-temporal-scene-understanding-with-world-knowledge-for-op.html">SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</a></td>
  <td>SNOWï¼šèåˆä¸–ç•ŒçŸ¥è¯†çš„æ—¶ç©ºåœºæ™¯ç†è§£æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾ä¸–ç•Œå…·èº«æ¨ç†</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16461v1" data-paper-url="./papers/251216461v1-snow-spatio-temporal-scene-understanding-with-world-knowledge-for-op.html" onclick="toggleFavorite(this, '2512.16461v1', 'SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251216918v1-adatooler-v-adaptive-tool-use-for-images-and-videos.html">AdaTooler-V: Adaptive Tool-Use for Images and Videos</a></td>
  <td>æå‡ºAdaTooler-Vï¼Œé€šè¿‡è‡ªé€‚åº”å·¥å…·ä½¿ç”¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ä¸­çš„æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16918v1" data-paper-url="./papers/251216918v1-adatooler-v-adaptive-tool-use-for-images-and-videos.html" onclick="toggleFavorite(this, '2512.16918v1', 'AdaTooler-V: Adaptive Tool-Use for Images and Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251216893v1-instant-expressive-gaussian-head-avatar-via-3d-aware-expression-dist.html">Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</a></td>
  <td>æå‡ºåŸºäº3Dæ„ŸçŸ¥è¡¨è¾¾è’¸é¦çš„å¿«é€Ÿé«˜è¡¨ç°åŠ›é«˜æ–¯å¤´éƒ¨å¤´åƒæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16893v1" data-paper-url="./papers/251216893v1-instant-expressive-gaussian-head-avatar-via-3d-aware-expression-dist.html" onclick="toggleFavorite(this, '2512.16893v1', 'Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251216635v1-sarmae-masked-autoencoder-for-sar-representation-learning.html">SARMAE: Masked Autoencoder for SAR Representation Learning</a></td>
  <td>SARMAEï¼šé¢å‘SARå›¾åƒè¡¨å¾å­¦ä¹ çš„å™ªå£°æ„ŸçŸ¥æ©ç è‡ªç¼–ç å™¨</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">masked autoencoder</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16635v1" data-paper-url="./papers/251216635v1-sarmae-masked-autoencoder-for-sar-representation-learning.html" onclick="toggleFavorite(this, '2512.16635v1', 'SARMAE: Masked Autoencoder for SAR Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251216924v1-the-world-is-your-canvas-painting-promptable-events-with-reference-i.html">The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</a></td>
  <td>WorldCanvasï¼šç»“åˆæ–‡æœ¬ã€è½¨è¿¹å’Œå‚è€ƒå›¾åƒï¼Œå®ç°å¯æ§çš„ä¸–ç•Œäº‹ä»¶æ¨¡æ‹Ÿã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16924v1" data-paper-url="./papers/251216924v1-the-world-is-your-canvas-painting-promptable-events-with-reference-i.html" onclick="toggleFavorite(this, '2512.16924v1', 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251216740v1-task-oriented-data-synthesis-and-control-rectify-sampling-for-remote.html">Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation</a></td>
  <td>æå‡ºTODSynthæ¡†æ¶ï¼Œç”¨äºé¥æ„Ÿè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„æ•°æ®åˆæˆä¸æ§åˆ¶ä¼˜åŒ–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16740v1" data-paper-url="./papers/251216740v1-task-oriented-data-synthesis-and-control-rectify-sampling-for-remote.html" onclick="toggleFavorite(this, '2512.16740v1', 'Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251216294v1-macl-multi-label-adaptive-contrastive-learning-loss-for-remote-sensi.html">MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</a></td>
  <td>æå‡ºå¤šæ ‡ç­¾è‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ (MACL)æŸå¤±ï¼Œç”¨äºé¥æ„Ÿå›¾åƒæ£€ç´¢ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16294v1" data-paper-url="./papers/251216294v1-macl-multi-label-adaptive-contrastive-learning-loss-for-remote-sensi.html" onclick="toggleFavorite(this, '2512.16294v1', 'MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251216504v1-skeleton-snippet-contrastive-learning-with-multiscale-feature-fusion.html">Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization</a></td>
  <td>æå‡ºåŸºäºéª¨éª¼ç‰‡æ®µå¯¹æ¯”å­¦ä¹ å’Œå¤šå°ºåº¦ç‰¹å¾èåˆçš„åŠ¨ä½œå®šä½æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16504v1" data-paper-url="./papers/251216504v1-skeleton-snippet-contrastive-learning-with-multiscale-feature-fusion.html" onclick="toggleFavorite(this, '2512.16504v1', 'Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251216909v1-momagraph-state-aware-unified-scene-graphs-with-vision-language-mode.html">MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</a></td>
  <td>MomaGraphï¼šé¢å‘å…·èº«ä»»åŠ¡è§„åˆ’ï¼Œèåˆè§†è§‰-è¯­è¨€æ¨¡å‹çš„ã€çŠ¶æ€æ„ŸçŸ¥çš„ç»Ÿä¸€åœºæ™¯å›¾</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16909v1" data-paper-url="./papers/251216909v1-momagraph-state-aware-unified-scene-graphs-with-vision-language-mode.html" onclick="toggleFavorite(this, '2512.16909v1', 'MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251216093v1-turbodiffusion-accelerating-video-diffusion-models-by-100-200-times.html">TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</a></td>
  <td>TurboDiffusionï¼šé€šè¿‡å¤šé‡åŠ é€Ÿç­–ç•¥å®ç°è§†é¢‘æ‰©æ•£æ¨¡å‹100-200å€çš„åŠ é€Ÿã€‚</td>
  <td class="tags-cell"><span class="paper-tag">linear attention</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16093v1" data-paper-url="./papers/251216093v1-turbodiffusion-accelerating-video-diffusion-models-by-100-200-times.html" onclick="toggleFavorite(this, '2512.16093v1', 'TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251216567v1-causal-tune-mining-causal-factors-from-vision-foundation-models-for-.html">Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation</a></td>
  <td>Causal-Tuneï¼šæŒ–æ˜è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„å› æœå› å­ï¼Œç”¨äºé¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16567v1" data-paper-url="./papers/251216567v1-causal-tune-mining-causal-factors-from-vision-foundation-models-for-.html" onclick="toggleFavorite(this, '2512.16567v1', 'Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251216485v1-smile-on-the-face-sadness-in-the-eyes-bridging-the-emotion-gap-with-.html">Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</a></td>
  <td>æå‡ºEMERTæ¨¡å‹å’ŒEMERæ•°æ®é›†ï¼Œåˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºå¼¥åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«å’Œæƒ…æ„Ÿè¯†åˆ«çš„å·®è·</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16485v1" data-paper-url="./papers/251216485v1-smile-on-the-face-sadness-in-the-eyes-bridging-the-emotion-gap-with-.html" onclick="toggleFavorite(this, '2512.16485v1', 'Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251216776v1-kling-omni-technical-report.html">Kling-Omni Technical Report</a></td>
  <td>Kling-Omniï¼šé€šç”¨ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°å¤šæ¨¡æ€è¾“å…¥åˆ°é«˜è´¨é‡è§†é¢‘çš„ç«¯åˆ°ç«¯åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16776v1" data-paper-url="./papers/251216776v1-kling-omni-technical-report.html" onclick="toggleFavorite(this, '2512.16776v1', 'Kling-Omni Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251216584v1-sketch-in-latents-eliciting-unified-reasoning-in-mllms.html">Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs</a></td>
  <td>æå‡ºSketch-in-Latents (SkiLa)ï¼Œå®ç°MLLMä¸­ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ä¸è§†è§‰æƒ³è±¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16584v1" data-paper-url="./papers/251216584v1-sketch-in-latents-eliciting-unified-reasoning-in-mllms.html" onclick="toggleFavorite(this, '2512.16584v1', 'Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251216303v1-pixelarena-a-benchmark-for-pixel-precision-visual-intelligence.html">PixelArena: A benchmark for Pixel-Precision Visual Intelligence</a></td>
  <td>PixelArenaï¼šæå‡ºåƒç´ çº§è§†è§‰æ™ºèƒ½è¯„æµ‹åŸºå‡†ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹å›¾åƒç”Ÿæˆèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16303v1" data-paper-url="./papers/251216303v1-pixelarena-a-benchmark-for-pixel-precision-visual-intelligence.html" onclick="toggleFavorite(this, '2512.16303v1', 'PixelArena: A benchmark for Pixel-Precision Visual Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251216906v1-viva-vlm-guided-instruction-based-video-editing-with-reward-optimiza.html">VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</a></td>
  <td>VIVAï¼šåŸºäºVLMå¼•å¯¼å’Œå¥–åŠ±ä¼˜åŒ–çš„æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16906v1" data-paper-url="./papers/251216906v1-viva-vlm-guided-instruction-based-video-editing-with-reward-optimiza.html" onclick="toggleFavorite(this, '2512.16906v1', 'VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251216636v1-reglue-your-latents-with-global-and-local-semantics-for-entangled-di.html">REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</a></td>
  <td>REGLUEï¼šèåˆå…¨å±€ä¸å±€éƒ¨è¯­ä¹‰çš„è§£è€¦æ‰©æ•£æ¨¡å‹ï¼Œæå‡å›¾åƒåˆæˆè´¨é‡ä¸æ”¶æ•›é€Ÿåº¦</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16636v1" data-paper-url="./papers/251216636v1-reglue-your-latents-with-global-and-local-semantics-for-entangled-di.html" onclick="toggleFavorite(this, '2512.16636v1', 'REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251216501v1-venusbench-gd-a-comprehensive-multi-platform-gui-benchmark-for-diver.html">VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</a></td>
  <td>VenusBench-GDï¼šä¸€ä¸ªå…¨é¢çš„å¤šå¹³å°GUIåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ ·åŒ–çš„Groundingä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16501v1" data-paper-url="./papers/251216501v1-venusbench-gd-a-comprehensive-multi-platform-gui-benchmark-for-diver.html" onclick="toggleFavorite(this, '2512.16501v1', 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251216199v1-avatar4d-synthesizing-domain-specific-4d-humans-for-real-world-pose-.html">Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation</a></td>
  <td>Avatar4Dï¼šåˆæˆç‰¹å®šé¢†åŸŸ4Däººä½“æ•°æ®ï¼Œç”¨äºçœŸå®åœºæ™¯å§¿æ€ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">zero-shot transfer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16199v1" data-paper-url="./papers/251216199v1-avatar4d-synthesizing-domain-specific-4d-humans-for-real-world-pose-.html" onclick="toggleFavorite(this, '2512.16199v1', 'Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251216085v1-machine-learning-enabled-graph-analysis-of-particulate-composites-ap.html">Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes</a></td>
  <td>æå‡ºåŸºäºæœºå™¨å­¦ä¹ çš„å›¾åˆ†ææ–¹æ³•ï¼Œç”¨äºå›ºæ€ç”µæ± æ­£æææ–™å¾®è§‚ç»“æ„è¡¨å¾ä¸æ€§èƒ½é¢„æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16085v1" data-paper-url="./papers/251216085v1-machine-learning-enabled-graph-analysis-of-particulate-composites-ap.html" onclick="toggleFavorite(this, '2512.16085v1', 'Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251216913v1-depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimatio.html">Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</a></td>
  <td>æå‡ºå…¨æ™¯æ·±åº¦ä¼°è®¡åŸºç¡€æ¨¡å‹DAPï¼Œæå‡è·¨åœºæ™¯è·ç¦»çš„æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">metric depth</span> <span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16913v1" data-paper-url="./papers/251216913v1-depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimatio.html" onclick="toggleFavorite(this, '2512.16913v1', 'Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251216706v1-sdfoam-signed-distance-foam-for-explicit-surface-reconstruction.html">SDFoam: Signed-Distance Foam for explicit surface reconstruction</a></td>
  <td>SDFoamï¼šç»“åˆæ˜¾å¼Voronoiå›¾å’Œéšå¼SDFï¼Œå®ç°ç²¾ç¡®è¡¨é¢é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16706v1" data-paper-url="./papers/251216706v1-sdfoam-signed-distance-foam-for-explicit-surface-reconstruction.html" onclick="toggleFavorite(this, '2512.16706v1', 'SDFoam: Signed-Distance Foam for explicit surface reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251216561v1-n3d-vlm-native-3d-grounding-enables-accurate-spatial-reasoning-in-vi.html">N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</a></td>
  <td>N3D-VLMï¼šåŸç”Ÿ3Dæ„ŸçŸ¥èµ‹èƒ½è§†è§‰è¯­è¨€æ¨¡å‹ç²¾ç¡®ç©ºé—´æ¨ç†</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16561v1" data-paper-url="./papers/251216561v1-n3d-vlm-native-3d-grounding-enables-accurate-spatial-reasoning-in-vi.html" onclick="toggleFavorite(this, '2512.16561v1', 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251216564v1-4d-primitive-mÃ¢chÃ©-glueing-primitives-for-persistent-4d-scene-recons.html">4D Primitive-MÃ¢chÃ©: Glueing Primitives for Persistent 4D Scene Reconstruction</a></td>
  <td>æå‡º4D Primitive-MÃ¢chÃ©ï¼Œé€šè¿‡æ‹¼æ¥åŸºå…ƒå®ç°æŒä¹…åŒ–4Dåœºæ™¯é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16564v1" data-paper-url="./papers/251216564v1-4d-primitive-mÃ¢chÃ©-glueing-primitives-for-persistent-4d-scene-recons.html" onclick="toggleFavorite(this, '2512.16564v1', '4D Primitive-MÃ¢chÃ©: Glueing Primitives for Persistent 4D Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251216397v1-using-gaussian-splats-to-create-high-fidelity-facial-geometry-and-te.html">Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</a></td>
  <td>åˆ©ç”¨é«˜æ–¯æº…å°„é‡å»ºé«˜ä¿çœŸé¢éƒ¨å‡ ä½•ä¸çº¹ç†ï¼Œå®ç°å¯æ§äººè„¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16397v1" data-paper-url="./papers/251216397v1-using-gaussian-splats-to-create-high-fidelity-facial-geometry-and-te.html" onclick="toggleFavorite(this, '2512.16397v1', 'Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251216077v1-auto-vocabulary-3d-object-detection.html">Auto-Vocabulary 3D Object Detection</a></td>
  <td>æå‡ºAV3DODæ¡†æ¶ï¼Œå®ç°æ— éœ€ç”¨æˆ·å¹²é¢„çš„è‡ªåŠ¨è¯æ±‡3Dç›®æ ‡æ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16077v1" data-paper-url="./papers/251216077v1-auto-vocabulary-3d-object-detection.html" onclick="toggleFavorite(this, '2512.16077v1', 'Auto-Vocabulary 3D Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251216811v1-geopredict-leveraging-predictive-kinematics-and-3d-gaussian-geometry.html">GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</a></td>
  <td>GeoPredictï¼šåˆ©ç”¨é¢„æµ‹è¿åŠ¨å­¦å’Œ3Dé«˜æ–¯å‡ ä½•å®ç°ç²¾ç¡®çš„VLAæ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16811v1" data-paper-url="./papers/251216811v1-geopredict-leveraging-predictive-kinematics-and-3d-gaussian-geometry.html" onclick="toggleFavorite(this, '2512.16811v1', 'GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251216767v1-make-it-poseable-feed-forward-latent-posing-model-for-3d-humanoid-ch.html">Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation</a></td>
  <td>æå‡ºMake-It-Poseableï¼Œç”¨äºè§£å†³3Däººå½¢è§’è‰²åŠ¨ç”»ä¸­å§¿æ€æ§åˆ¶éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">character animation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16767v1" data-paper-url="./papers/251216767v1-make-it-poseable-feed-forward-latent-posing-model-for-3d-humanoid-ch.html" onclick="toggleFavorite(this, '2512.16767v1', 'Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251216842v1-opentouch-bringing-full-hand-touch-to-real-world-interaction.html">OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</a></td>
  <td>OpenTouchï¼šæ„å»ºçœŸå®åœºæ™¯ä¸‹å®Œæ•´æ‰‹éƒ¨è§¦è§‰äº¤äº’æ•°æ®é›†ä¸åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">egocentric</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16842v1" data-paper-url="./papers/251216842v1-opentouch-bringing-full-hand-touch-to-real-world-interaction.html" onclick="toggleFavorite(this, '2512.16842v1', 'OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251216270v1-texteditbench-evaluating-reasoning-aware-text-editing-beyond-renderi.html">TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering</a></td>
  <td>æå‡ºTextEditBenchï¼Œç”¨äºè¯„ä¼°å›¾åƒæ–‡æœ¬ç¼–è¾‘ä¸­è•´å«æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16270v1" data-paper-url="./papers/251216270v1-texteditbench-evaluating-reasoning-aware-text-editing-beyond-renderi.html" onclick="toggleFavorite(this, '2512.16270v1', 'TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/251216907v1-flowing-from-reasoning-to-motion-learning-3d-hand-trajectory-predict.html">Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</a></td>
  <td>EgoMANï¼šåŸºäºè‡ªä¸­å¿ƒäº¤äº’è§†é¢‘å­¦ä¹ 3Dæ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ï¼Œå®ç°ä»æ¨ç†åˆ°åŠ¨ä½œçš„æµç•…è¡”æ¥</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16907v1" data-paper-url="./papers/251216907v1-flowing-from-reasoning-to-motion-learning-3d-hand-trajectory-predict.html" onclick="toggleFavorite(this, '2512.16907v1', 'Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251216456v1-prime-and-reach-synthesising-body-motion-for-gaze-primed-object-reac.html">Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</a></td>
  <td>æå‡ºåŸºäºæ³¨è§†å¯åŠ¨çš„äººä½“è¿åŠ¨åˆæˆæ–¹æ³•ï¼Œç”¨äºæ¨¡æ‹Ÿæ‹¾å–/æ”¾ç½®ç‰©ä½“çš„è‡ªç„¶è¡Œä¸ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16456v1" data-paper-url="./papers/251216456v1-prime-and-reach-synthesising-body-motion-for-gaze-primed-object-reac.html" onclick="toggleFavorite(this, '2512.16456v1', 'Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>35</td>
  <td><a href="./papers/251216360v1-everybodydance-bipartite-graph-based-identity-correspondence-for-mul.html">EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation</a></td>
  <td>EverybodyDanceï¼šåŸºäºäºŒåˆ†å›¾çš„è§’è‰²åŒ¹é…æ–¹æ³•ï¼Œè§£å†³å¤šè§’è‰²åŠ¨ç”»ä¸­çš„èº«ä»½å¯¹åº”é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">character animation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16360v1" data-paper-url="./papers/251216360v1-everybodydance-bipartite-graph-based-identity-correspondence-for-mul.html" onclick="toggleFavorite(this, '2512.16360v1', 'EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/251216920v1-easyv2v-a-high-quality-instruction-based-video-editing-framework.html">EasyV2V: A High-quality Instruction-based Video Editing Framework</a></td>
  <td>EasyV2Vï¼šæå‡ºä¸€ç§é«˜è´¨é‡çš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œæå‡è§†é¢‘ç¼–è¾‘çš„ä¸€è‡´æ€§ã€æ§åˆ¶æ€§å’Œæ³›åŒ–æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16920v1" data-paper-url="./papers/251216920v1-easyv2v-a-high-quality-instruction-based-video-editing-framework.html" onclick="toggleFavorite(this, '2512.16920v1', 'EasyV2V: A High-quality Instruction-based Video Editing Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/251216143v1-seggraph-leveraging-graphs-of-sam-segments-for-few-shot-3d-part-segm.html">SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</a></td>
  <td>SegGraphï¼šåˆ©ç”¨SAMåˆ†å‰²å›¾è¿›è¡Œå°‘æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.16143v1" data-paper-url="./papers/251216143v1-seggraph-leveraging-graphs-of-sam-segments-for-few-shot-3d-part-segm.html" onclick="toggleFavorite(this, '2512.16143v1', 'SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)