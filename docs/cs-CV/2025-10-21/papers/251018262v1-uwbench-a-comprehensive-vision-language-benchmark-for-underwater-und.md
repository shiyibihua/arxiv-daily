---
layout: default
title: UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding
---

# UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding

**arXiv**: [2510.18262v1](https://arxiv.org/abs/2510.18262) | [PDF](https://arxiv.org/pdf/2510.18262.pdf)

**ä½œè€…**: Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUWBenchåŸºå‡†ä»¥è¯„ä¼°æ°´ä¸‹çŽ¯å¢ƒä¸­çš„è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `æ°´ä¸‹è§†è§‰è¯­è¨€ç†è§£` `åŸºå‡†æ•°æ®é›†` `å›¾åƒæ ‡æ³¨` `è§†è§‰é—®ç­”` `æµ·æ´‹ç”Ÿæ€ç›‘æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ°´ä¸‹å›¾åƒå­˜åœ¨å…‰è¡°å‡ã€é¢œè‰²å¤±çœŸç­‰æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹ä¸“é—¨åŸºå‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«15003å¼ å›¾åƒã€æ ‡æ³¨å’Œé—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæµ‹è¯•å…ˆè¿›æ¨¡åž‹æ˜¾ç¤ºæ°´ä¸‹ç†è§£ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œä¿ƒè¿›æµ·æ´‹åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large vision-language models (VLMs) have achieved remarkable success in
> natural scene understanding, yet their application to underwater environments
> remains largely unexplored. Underwater imagery presents unique challenges
> including severe light attenuation, color distortion, and suspended particle
> scattering, while requiring specialized knowledge of marine ecosystems and
> organism taxonomy. To bridge this gap, we introduce UWBench, a comprehensive
> benchmark specifically designed for underwater vision-language understanding.
> UWBench comprises 15,003 high-resolution underwater images captured across
> diverse aquatic environments, encompassing oceans, coral reefs, and deep-sea
> habitats. Each image is enriched with human-verified annotations including
> 15,281 object referring expressions that precisely describe marine organisms
> and underwater structures, and 124,983 question-answer pairs covering diverse
> reasoning capabilities from object recognition to ecological relationship
> understanding. The dataset captures rich variations in visibility, lighting
> conditions, and water turbidity, providing a realistic testbed for model
> evaluation. Based on UWBench, we establish three comprehensive benchmarks:
> detailed image captioning for generating ecologically informed scene
> descriptions, visual grounding for precise localization of marine organisms,
> and visual question answering for multimodal reasoning about underwater
> environments. Extensive experiments on state-of-the-art VLMs demonstrate that
> underwater understanding remains challenging, with substantial room for
> improvement. Our benchmark provides essential resources for advancing
> vision-language research in underwater contexts and supporting applications in
> marine science, ecological monitoring, and autonomous underwater exploration.
> Our code and benchmark will be available.

