---
layout: default
title: From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation
---

# From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation

**arXiv**: [2510.18263v1](https://arxiv.org/abs/2510.18263) | [PDF](https://arxiv.org/pdf/2510.18263.pdf)

**ä½œè€…**: Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCustomized-GRPOæ¡†æž¶ä»¥è§£å†³ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆä¸­çš„ç«žäº‰é€€åŒ–é—®é¢˜**

**å…³é”®è¯**: `ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å¡‘å½¢` `æ‰©æ•£æ¨¡åž‹` `ç«žäº‰é€€åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆä¸­å¯¼è‡´ç«žäº‰é€€åŒ–ï¼Œèº«ä»½ä¿çœŸä¸Žæç¤ºéµå¾ªå†²çª
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Synergy-Aware Reward Shapingå’ŒTime-Aware Dynamic Weightingï¼Œä¼˜åŒ–å¥–åŠ±ä¿¡å·
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ–¹æ³•æ˜¾è‘—ä¼˜äºŽåŸºçº¿ï¼Œå®žçŽ°èº«ä»½ç‰¹å¾ä¿ç•™ä¸Žå¤æ‚æç¤ºéµå¾ªçš„å¹³è¡¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Subject-driven image generation models face a fundamental trade-off between
> identity preservation (fidelity) and prompt adherence (editability). While
> online reinforcement learning (RL), specifically GPRO, offers a promising
> solution, we find that a naive application of GRPO leads to competitive
> degradation, as the simple linear aggregation of rewards with static weights
> causes conflicting gradient signals and a misalignment with the temporal
> dynamics of the diffusion process. To overcome these limitations, we propose
> Customized-GRPO, a novel framework featuring two key innovations: (i)
> Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
> penalizes conflicted reward signals and amplifies synergistic ones, providing a
> sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
> which aligns the optimization pressure with the model's temporal dynamics by
> prioritizing prompt-following in the early, identity preservation in the later.
> Extensive experiments demonstrate that our method significantly outperforms
> naive GRPO baselines, successfully mitigating competitive degradation. Our
> model achieves a superior balance, generating images that both preserve key
> identity features and accurately adhere to complex textual prompts.

