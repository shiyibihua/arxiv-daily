---
layout: default
title: DeepSeek-OCR: Contexts Optical Compression
---

# DeepSeek-OCR: Contexts Optical Compression

**arXiv**: [2510.18234v1](https://arxiv.org/abs/2510.18234) | [PDF](https://arxiv.org/pdf/2510.18234.pdf)

**ä½œè€…**: Haoran Wei, Yaofeng Sun, Yukun Li

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepSeek-OCRä»¥å‹ç¼©é•¿ä¸Šä¸‹æ–‡å…‰å­¦æ˜ å°„ï¼Œå®ç°é«˜æ•ˆOCRè§£ç ã€‚**

**å…³é”®è¯**: `å…‰å­¦å­—ç¬¦è¯†åˆ«` `ä¸Šä¸‹æ–‡å‹ç¼©` `è§†è§‰ä»¤ç‰Œä¼˜åŒ–` `é•¿æ–‡æ¡£å¤„ç†` `æ¨¡å‹è§£ç `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡å…‰å­¦å‹ç¼©çš„å¯è¡Œæ€§ï¼Œå‡å°‘è§†è§‰ä»¤ç‰Œæ•°ä»¥ä¼˜åŒ–å¤„ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨DeepEncoderå’ŒDeepSeek3B-MoE-A570Mè§£ç å™¨ï¼Œå®ç°é«˜å‹ç¼©æ¯”å’Œä½æ¿€æ´»ã€‚
3. å®éªŒæ•ˆæœï¼šå‹ç¼©æ¯”<10xæ—¶OCRç²¾åº¦è¾¾97%ï¼Œ20xæ—¶çº¦60%ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present DeepSeek-OCR as an initial investigation into the feasibility of
> compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two
> components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically,
> DeepEncoder serves as the core engine, designed to maintain low activations
> under high-resolution input while achieving high compression ratios to ensure
> an optimal and manageable number of vision tokens. Experiments show that when
> the number of text tokens is within 10 times that of vision tokens (i.e., a
> compression ratio < 10x), the model can achieve decoding (OCR) precision of
> 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at
> about 60%. This shows considerable promise for research areas such as
> historical long-context compression and memory forgetting mechanisms in LLMs.
> Beyond this, DeepSeek-OCR also demonstrates high practical value. On
> OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision
> tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while
> utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can
> generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a
> single A100-40G). Codes and model weights are publicly accessible at
> http://github.com/deepseek-ai/DeepSeek-OCR.

