---
layout: default
title: Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models
---

# Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models

**arXiv**: [2510.18457v1](https://arxiv.org/abs/2510.18457) | [PDF](https://arxiv.org/pdf/2510.18457.pdf)

**ä½œè€…**: Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVFM-VAEä»¥ç›´æŽ¥é›†æˆè§†è§‰åŸºç¡€æ¨¡åž‹åˆ°æ½œåœ¨æ‰©æ•£æ¨¡åž‹ï¼Œæå‡è¯­ä¹‰å¯¹é½ä¸Žæ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `è§†è§‰åŸºç¡€æ¨¡åž‹` `æ½œåœ¨æ‰©æ•£æ¨¡åž‹` `å˜åˆ†è‡ªç¼–ç å™¨` `è¯­ä¹‰å¯¹é½` `å¤šå°ºåº¦èžåˆ` `æ‰©æ•£è®­ç»ƒåŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è’¸é¦æ–¹æ³•å¯¼è‡´è§†è§‰åŸºç¡€æ¨¡åž‹è¯­ä¹‰å¯¹é½åœ¨åˆ†å¸ƒåç§»ä¸‹å‡å¼±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡VFM-VAEï¼Œé‡‡ç”¨å¤šå°ºåº¦æ½œåœ¨èžåˆå’Œæ¸è¿›åˆ†è¾¨çŽ‡é‡å»ºå—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨80è½®è®­ç»ƒä¸­gFIDè¾¾2.20ï¼Œ640è½®è¾¾1.62ï¼ŒåŠ é€Ÿ10å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The performance of Latent Diffusion Models (LDMs) is critically dependent on
> the quality of their visual tokenizer. While recent works have explored
> incorporating Vision Foundation Models (VFMs) via distillation, we identify a
> fundamental flaw in this approach: it inevitably weakens the robustness of
> alignment with the original VFM, causing the aligned latents to deviate
> semantically under distribution shifts. In this paper, we bypass distillation
> by proposing a more direct approach: Vision Foundation Model Variational
> Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's
> semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE
> decoder with Multi-Scale Latent Fusion and Progressive Resolution
> Reconstruction blocks, enabling high-quality reconstruction from spatially
> coarse VFM features. Furthermore, we provide a comprehensive analysis of
> representation dynamics during diffusion training, introducing the proposed
> SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows
> us to develop a joint tokenizer-diffusion alignment strategy that dramatically
> accelerates convergence. Our innovations in tokenizer design and training
> strategy lead to superior performance and efficiency: our system reaches a gFID
> (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).
> With continued training to 640 epochs, it further attains a gFID (w/o CFG) of
> 1.62, establishing direct VFM integration as a superior paradigm for LDMs.

