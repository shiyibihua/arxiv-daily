---
layout: default
title: MADR: MPC-guided Adversarial DeepReach
---

# MADR: MPC-guided Adversarial DeepReach

**arXiv**: [2510.18845v1](https://arxiv.org/abs/2510.18845) | [PDF](https://arxiv.org/pdf/2510.18845.pdf)

**ä½œè€…**: Ryan Teoh, Sander Tonkens, William Sharpless, Aijia Yang, Zeyuan Feng, Somil Bansal, Sylvia Herbert

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMADRæ¡†æž¶ä»¥è§£å†³é«˜ç»´å¯¹æŠ—æ€§é›¶å’Œå¾®åˆ†åšå¼ˆä¸­çš„å®‰å…¨ç­–ç•¥è®¡ç®—é—®é¢˜**

**å…³é”®è¯**: `Hamilton-Jacobiå¯è¾¾æ€§` `å¯¹æŠ—æ€§æŽ§åˆ¶` `é›¶å’Œåšå¼ˆ` `ç‰©ç†ä¿¡æ¯æ·±åº¦å­¦ä¹ ` `æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶` `æœºå™¨äººå®‰å…¨ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šHamilton-Jacobiå¯è¾¾æ€§åˆ†æžå—ç»´åº¦è¯…å’’é™åˆ¶ï¼Œç‰©ç†ä¿¡æ¯æ·±åº¦å­¦ä¹ æ”¶æ•›æ…¢ä¸”ä¸å‡†ç¡®
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆMPCæŒ‡å¯¼ä¸Žè‡ªç›‘ç£å­¦ä¹ ï¼Œè¿‘ä¼¼åŒçŽ©å®¶é›¶å’Œåšå¼ˆå€¼å‡½æ•°ï¼Œç”Ÿæˆæœ€ä¼˜ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é«˜ç»´æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººæµ‹è¯•ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰åŸºçº¿ï¼Œç¡¬ä»¶è¡¨çŽ°ä¼˜å¼‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Hamilton-Jacobi (HJ) Reachability offers a framework for generating safe
> value functions and policies in the face of adversarial disturbance, but is
> limited by the curse of dimensionality. Physics-informed deep learning is able
> to overcome this infeasibility, but itself suffers from slow and inaccurate
> convergence, primarily due to weak PDE gradients and the complexity of
> self-supervised learning. A few works, recently, have demonstrated that
> enriching the self-supervision process with regular supervision (based on the
> nature of the optimal control problem), greatly accelerates convergence and
> solution quality, however, these have been limited to single player problems
> and simple games. In this work, we introduce MADR: MPC-guided Adversarial
> DeepReach, a general framework to robustly approximate the two-player, zero-sum
> differential game value function. In doing so, MADR yields the corresponding
> optimal strategies for both players in zero-sum games as well as safe policies
> for worst-case robustness. We test MADR on a multitude of high-dimensional
> simulated and real robotic agents with varying dynamics and games, finding that
> our approach significantly out-performs state-of-the-art baselines in
> simulation and produces impressive results in hardware.

