---
layout: default
title: GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data
---

# GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data

**arXiv**: [2510.18345v1](https://arxiv.org/abs/2510.18345) | [PDF](https://arxiv.org/pdf/2510.18345.pdf)

**ä½œè€…**: Yudong Li, Hao Li, Xianxu Hou, Linlin Shen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPTFaceæ¨¡åž‹ï¼Œåˆ©ç”¨ç½‘ç»œæ•°æ®é¢„è®­ç»ƒè§£å†³é¢éƒ¨çŸ¥è¯†å­¦ä¹ å¯æ‰©å±•æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `é¢éƒ¨çŸ¥è¯†å­¦ä¹ ` `ç”Ÿæˆé¢„è®­ç»ƒ` `è‡ªç›‘ç£å­¦ä¹ ` `å›¾åƒ-æ–‡æœ¬åŒ¹é…` `é¢éƒ¨ç¼–è¾‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é¢éƒ¨çŸ¥è¯†é¢„è®­ç»ƒç ”ç©¶ä¸è¶³ï¼Œä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œå¯æ‰©å±•æ€§æœ‰é™ã€‚
2. é‡‡ç”¨è‡ªç›‘ç£ä»»åŠ¡é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬æŽ©ç å›¾åƒ/è¯­è¨€å»ºæ¨¡å’Œå›¾åƒ-æ–‡æœ¬åŒ¹é…ã€‚
3. å®žéªŒæ˜¾ç¤ºåœ¨å±žæ€§åˆ†ç±»å’Œè¡¨æƒ…è¯†åˆ«ç­‰ä»»åŠ¡ä¸­æ€§èƒ½å¯æ¯”è‚©å…ˆè¿›æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Compared to the prosperity of pre-training models in natural image
> understanding, the research on large-scale pre-training models for facial
> knowledge learning is still limited. Current approaches mainly rely on manually
> assembled and annotated face datasets for training, but labeling such datasets
> is labor-intensive and the trained models have limited scalability beyond the
> training data. To address these limitations, we present a generative
> pre-training model for facial knowledge learning that leverages large-scale
> web-built data for training. We use texts and images containing human faces
> crawled from the internet and conduct pre-training on self-supervised tasks,
> including masked image/language modeling (MILM) and image-text matching (ITM).
> During the generation stage, we further utilize the image-text matching loss to
> pull the generation distribution towards the control signal for controllable
> image/text generation. Experimental results demonstrate that our model achieves
> comparable performance to state-of-the-art pre-training models for various
> facial downstream tasks, such as attribution classification and expression
> recognition. Furthermore, our approach is also applicable to a wide range of
> face editing tasks, including face attribute editing, expression manipulation,
> mask removal, and photo inpainting.

