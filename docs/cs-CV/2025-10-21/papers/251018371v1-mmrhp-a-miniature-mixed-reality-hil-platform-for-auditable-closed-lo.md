---
layout: default
title: MMRHP: A Miniature Mixed-Reality HIL Platform for Auditable Closed-Loop Evaluation
---

# MMRHP: A Miniature Mixed-Reality HIL Platform for Auditable Closed-Loop Evaluation

**arXiv**: [2510.18371v1](https://arxiv.org/abs/2510.18371) | [PDF](https://arxiv.org/pdf/2510.18371.pdf)

**ä½œè€…**: Mingxin Li, Haibo Hu, Jinghuai Deng, Yuchen Xi, Xinhong Chen, Jianping Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMRHPå¹³å°ä»¥æ”¯æŒè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¯å®¡è®¡é—­çŽ¯è¯„ä¼°**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶éªŒè¯` `ç¡¬ä»¶åœ¨çŽ¯å¹³å°` `æ··åˆçŽ°å®ž` `æ—¶ç©ºæµ‹é‡` `SOTIFæ ‡å‡†` `é—­çŽ¯è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªåŠ¨é©¾é©¶ç³»ç»ŸéªŒè¯éœ€å¹³è¡¡æµ‹è¯•ä¿çœŸåº¦ã€æˆæœ¬ä¸Žå¯æ‰©å±•æ€§ï¼ŒçŽ°æœ‰å¾®åž‹HILå¹³å°ç¼ºä¹ç³»ç»Ÿå®šé‡åˆ†æžæ¡†æž¶
2. è®¾è®¡åŸºäºŽç»Ÿä¸€æ—¶ç©ºæµ‹é‡æ ¸å¿ƒçš„HILå¹³å°ï¼Œç¡®ä¿ç‰©ç†è¿åŠ¨å’Œç³»ç»Ÿæ—¶åºçš„ä¸€è‡´å¯è¿½æº¯é‡åŒ–
3. å®žéªŒéªŒè¯å¹³å°ç©ºé—´ç²¾åº¦10.27æ¯«ç±³RMSEï¼Œé—­çŽ¯å»¶è¿Ÿçº¦45æ¯«ç§’ï¼Œè¯†åˆ«Autowareåœ¨40æ¯«ç§’å»¶è¿Ÿä¸‹çš„æ€§èƒ½æ‚¬å´–

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Validation of autonomous driving systems requires a trade-off between test
> fidelity, cost, and scalability. While miniaturized hardware-in-the-loop (HIL)
> platforms have emerged as a promising solution, a systematic framework
> supporting rigorous quantitative analysis is generally lacking, limiting their
> value as scientific evaluation tools. To address this challenge, we propose
> MMRHP, a miniature mixed-reality HIL platform that elevates miniaturized
> testing from functional demonstration to rigorous, reproducible quantitative
> analysis. The core contributions are threefold. First, we propose a systematic
> three-phase testing process oriented toward the Safety of the Intended
> Functionality(SOTIF)standard, providing actionable guidance for identifying the
> performance limits and triggering conditions of otherwise correctly functioning
> systems. Second, we design and implement a HIL platform centered around a
> unified spatiotemporal measurement core to support this process, ensuring
> consistent and traceable quantification of physical motion and system timing.
> Finally, we demonstrate the effectiveness of this solution through
> comprehensive experiments. The platform itself was first validated, achieving a
> spatial accuracy of 10.27 mm RMSE and a stable closed-loop latency baseline of
> approximately 45 ms. Subsequently, an in-depth Autoware case study leveraged
> this validated platform to quantify its performance baseline and identify a
> critical performance cliff at an injected latency of 40 ms. This work shows
> that a structured process, combined with a platform offering a unified
> spatio-temporal benchmark, enables reproducible, interpretable, and
> quantitative closed-loop evaluation of autonomous driving systems.

