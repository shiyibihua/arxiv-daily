---
layout: default
title: UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation
---

# UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation

**arXiv**: [2510.18701v1](https://arxiv.org/abs/2510.18701) | [PDF](https://arxiv.org/pdf/2510.18701.pdf)

**ä½œè€…**: Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniGenBench++ç»Ÿä¸€åŸºå‡†ï¼Œè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¯­ä¹‰è¯„ä¼°çš„å¤šæ ·æ€§å’Œç»†ç²’åº¦ä¸è¶³é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `è¯­ä¹‰è¯„ä¼°åŸºå‡†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¤šè¯­è¨€æç¤º` `ç»†ç²’åº¦è¯„ä¼°` `ç¦»çº¿è¯„ä¼°æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†ç¼ºä¹å¤šè¯­è¨€æ”¯æŒå’Œå¤šæ ·åŒ–æç¤ºåœºæ™¯ï¼Œå½±å“å®é™…åº”ç”¨ã€‚
2. æ„å»º600ä¸ªæç¤ºï¼Œè¦†ç›–5ä¸»é¢˜20å­ä¸»é¢˜ï¼Œè¯„ä¼°10ä¸»27å­æ ‡å‡†ï¼Œä½¿ç”¨MLLMè‡ªåŠ¨è¯„ä¼°ã€‚
3. é€šè¿‡ä¸­è‹±æ–‡é•¿çŸ­æç¤ºæµ‹è¯•æ¨¡å‹é²æ£’æ€§ï¼Œå¹¶è®­ç»ƒç¦»çº¿è¯„ä¼°æ¨¡å‹ï¼Œç³»ç»Ÿæ­ç¤ºæ¨¡å‹ä¼˜ç¼ºç‚¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in text-to-image (T2I) generation underscores the importance
> of reliable benchmarks in evaluating how accurately generated images reflect
> the semantics of their textual prompt. However, (1) existing benchmarks lack
> the diversity of prompt scenarios and multilingual support, both essential for
> real-world applicability; (2) they offer only coarse evaluations across primary
> dimensions, covering a narrow range of sub-dimensions, and fall short in
> fine-grained sub-dimension assessment. To address these limitations, we
> introduce UniGenBench++, a unified semantic assessment benchmark for T2I
> generation. Specifically, it comprises 600 prompts organized hierarchically to
> ensure both coverage and efficiency: (1) spans across diverse real-world
> scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively
> probes T2I models' semantic consistency over 10 primary and 27 sub evaluation
> criteria, with each prompt assessing multiple testpoints. To rigorously assess
> model robustness to variations in language and prompt length, we provide both
> English and Chinese versions of each prompt in short and long forms. Leveraging
> the general world knowledge and fine-grained image understanding capabilities
> of a closed-source Multi-modal Large Language Model (MLLM), i.e.,
> Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark
> construction and streamlined model assessment. Moreover, to further facilitate
> community use, we train a robust evaluation model that enables offline
> assessment of T2I model outputs. Through comprehensive benchmarking of both
> open- and closed-sourced T2I models, we systematically reveal their strengths
> and weaknesses across various aspects.

