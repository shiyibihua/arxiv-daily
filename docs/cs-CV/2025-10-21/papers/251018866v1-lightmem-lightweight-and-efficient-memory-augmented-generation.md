---
layout: default
title: LightMem: Lightweight and Efficient Memory-Augmented Generation
---

# LightMem: Lightweight and Efficient Memory-Augmented Generation

**arXiv**: [2510.18866v1](https://arxiv.org/abs/2510.18866) | [PDF](https://arxiv.org/pdf/2510.18866.pdf)

**ä½œè€…**: Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLightMemä»¥è§£å†³LLMåœ¨åŠ¨æ€çŽ¯å¢ƒä¸­é«˜æ•ˆåˆ©ç”¨åŽ†å²äº¤äº’ä¿¡æ¯çš„é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `å†…å­˜ç³»ç»Ÿ` `è½»é‡åŽ‹ç¼©` `ç¦»çº¿æ›´æ–°` `æ•ˆçŽ‡ä¼˜åŒ–` `è®°å¿†æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LLMåœ¨åŠ¨æ€å¤æ‚çŽ¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨åŽ†å²äº¤äº’ä¿¡æ¯ï¼ŒçŽ°æœ‰å†…å­˜ç³»ç»Ÿå¸¸å¼•å…¥é«˜å¼€é”€
2. LightMemåŸºäºŽäººç±»è®°å¿†æ¨¡åž‹ï¼Œåˆ†ä¸‰é˜¶æ®µç»„ç»‡å†…å­˜ï¼šæ„Ÿå®˜ã€çŸ­æ—¶å’Œé•¿æ—¶è®°å¿†ï¼Œå®žçŽ°è½»é‡åŽ‹ç¼©ä¸Žç¦»çº¿æ›´æ–°
3. å®žéªŒåœ¨LongMemEvalä¸Šæ˜¾ç¤ºï¼Œå‡†ç¡®çŽ‡æå‡è¾¾10.9%ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘tokenä½¿ç”¨ã€APIè°ƒç”¨å’Œè¿è¡Œæ—¶é—´

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite their remarkable capabilities, Large Language Models (LLMs) struggle
> to effectively leverage historical interaction information in dynamic and
> complex environments. Memory systems enable LLMs to move beyond stateless
> interactions by introducing persistent information storage, retrieval, and
> utilization mechanisms. However, existing memory systems often introduce
> substantial time and computational overhead. To this end, we introduce a new
> memory system called LightMem, which strikes a balance between the performance
> and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
> human memory, LightMem organizes memory into three complementary stages. First,
> cognition-inspired sensory memory rapidly filters irrelevant information
> through lightweight compression and groups information according to their
> topics. Next, topic-aware short-term memory consolidates these topic-based
> groups, organizing and summarizing content for more structured access. Finally,
> long-term memory with sleep-time update employs an offline procedure that
> decouples consolidation from online inference. Experiments on LongMemEval with
> GPT and Qwen backbones show that LightMem outperforms strong baselines in
> accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
> calls by up to 159x, and runtime by over 12x. The code is available at
> https://github.com/zjunlp/LightMem.

