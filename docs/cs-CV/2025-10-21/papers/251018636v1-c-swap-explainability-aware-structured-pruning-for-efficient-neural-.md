---
layout: default
title: C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression
---

# C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression

**arXiv**: [2510.18636v1](https://arxiv.org/abs/2510.18636) | [PDF](https://arxiv.org/pdf/2510.18636.pdf)

**ä½œè€…**: Baptiste Bauvin, LoÃ¯c Baret, Ola Ahmad

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºC-SWAPæ¡†æž¶ï¼Œåˆ©ç”¨å¯è§£é‡Šæ€§å®žçŽ°é«˜æ•ˆç¥žç»ç½‘ç»œåŽ‹ç¼©ï¼Œè§£å†³ä¸€æ¬¡æ€§å‰ªæžæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç¥žç»ç½‘ç»œåŽ‹ç¼©` `ç»“æž„åŒ–å‰ªæž` `å¯è§£é‡Šæ·±åº¦å­¦ä¹ ` `ä¸€æ¬¡æ€§å‰ªæž` `å› æžœæ„ŸçŸ¥å‰ªæž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ¬¡æ€§ç»“æž„åŒ–å‰ªæžå¸¸å¯¼è‡´æ¨¡åž‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸”ä¼ ç»Ÿæ–¹æ³•éœ€è¿­ä»£é‡è®­ç»ƒï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå› æžœå…³ç³»çš„æ¸è¿›å‰ªæžï¼Œåˆ©ç”¨å¯è§£é‡Šæ·±åº¦å­¦ä¹ è¯†åˆ«å¹¶ç§»é™¤ä¸å½±å“é¢„æµ‹çš„ç»“æž„ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CNNå’ŒViTåŸºå‡†ä¸ŠéªŒè¯ï¼Œæ¨¡åž‹å¤§å°å¤§å¹…å‡å°ï¼Œæ€§èƒ½å½±å“æœ€å°ï¼Œæ— éœ€å¾®è°ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Neural network compression has gained increasing attention in recent years,
> particularly in computer vision applications, where the need for model
> reduction is crucial for overcoming deployment constraints. Pruning is a widely
> used technique that prompts sparsity in model structures, e.g. weights,
> neurons, and layers, reducing size and inference costs. Structured pruning is
> especially important as it allows for the removal of entire structures, which
> further accelerates inference time and reduces memory overhead. However, it can
> be computationally expensive, requiring iterative retraining and optimization.
> To overcome this problem, recent methods considered one-shot setting, which
> applies pruning directly at post-training. Unfortunately, they often lead to a
> considerable drop in performance. In this paper, we focus on this issue by
> proposing a novel one-shot pruning framework that relies on explainable deep
> learning. First, we introduce a causal-aware pruning approach that leverages
> cause-effect relations between model predictions and structures in a
> progressive pruning process. It allows us to efficiently reduce the size of the
> network, ensuring that the removed structures do not deter the performance of
> the model. Then, through experiments conducted on convolution neural network
> and vision transformer baselines, pre-trained on classification tasks, we
> demonstrate that our method consistently achieves substantial reductions in
> model size, with minimal impact on performance, and without the need for
> fine-tuning. Overall, our approach outperforms its counterparts, offering the
> best trade-off. Our code is available on GitHub.

