---
layout: default
title: Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning
---

# Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning

**arXiv**: [2510.18518v1](https://arxiv.org/abs/2510.18518) | [PDF](https://arxiv.org/pdf/2510.18518.pdf)

**ä½œè€…**: Fang Nan, Hao Ma, Qinghua Guan, Josie Hughes, Michael Muehlebach, Marco Hutter

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºŽæœºå™¨äººæŽ§åˆ¶ï¼Œæå‡æ ·æœ¬æ•ˆçŽ‡ä¸Žé€‚åº”æ€§ã€‚**

**å…³é”®è¯**: `æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæŽ§åˆ¶` `åœ¨çº¿å­¦ä¹ ` `æ ·æœ¬æ•ˆçŽ‡` `åŠ¨æ€é€‚åº”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæœºå™¨äººæŽ§åˆ¶ä¾èµ–ç¦»çº¿ä»¿çœŸï¼Œæ ·æœ¬æ•ˆçŽ‡ä½Žä¸”æ˜“å—æ¨¡æ‹Ÿåå·®å½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä»Žå®žæ—¶äº¤äº’æ•°æ®æž„å»ºåŠ¨æ€æ¨¡åž‹ï¼ŒæŒ‡å¯¼ç­–ç•¥æ›´æ–°ï¼Œå‡å°‘æ ·æœ¬éœ€æ±‚ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨æ¶²åŽ‹æŒ–æŽ˜è‡‚å’Œè½¯ä½“æœºå™¨äººä¸ŠéªŒè¯ï¼Œæ ·æœ¬æ•ˆçŽ‡é«˜ï¼Œé€‚åº”åŠ¨æ€å˜åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present an online model-based reinforcement learning algorithm suitable
> for controlling complex robotic systems directly in the real world. Unlike
> prevailing sim-to-real pipelines that rely on extensive offline simulation and
> model-free policy optimization, our method builds a dynamics model from
> real-time interaction data and performs policy updates guided by the learned
> dynamics model. This efficient model-based reinforcement learning scheme
> significantly reduces the number of samples to train control policies, enabling
> direct training on real-world rollout data. This significantly reduces the
> influence of bias in the simulated data, and facilitates the search for
> high-performance control policies. We adopt online learning analysis to derive
> sublinear regret bounds under standard stochastic online optimization
> assumptions, providing formal guarantees on performance improvement as more
> interaction data are collected. Experimental evaluations were performed on a
> hydraulic excavator arm and a soft robot arm, where the algorithm demonstrates
> strong sample efficiency compared to model-free reinforcement learning methods,
> reaching comparable performance within hours. Robust adaptation to shifting
> dynamics was also observed when the payload condition was randomized. Our
> approach paves the way toward efficient and reliable on-robot learning for a
> broad class of challenging control tasks.

