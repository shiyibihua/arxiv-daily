---
layout: default
title: A Generalizable Light Transport 3D Embedding for Global Illumination
---

# A Generalizable Light Transport 3D Embedding for Global Illumination

**arXiv**: [2510.18189v1](https://arxiv.org/abs/2510.18189) | [PDF](https://arxiv.org/pdf/2510.18189.pdf)

**ä½œè€…**: Bing Xu, Mukund Varma T, Cheng Wang, Tzumao Li, Lifan Wu, Bartlomiej Wronski, Ravi Ramamoorthi, Marco Salvi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯æ³›åŒ–3Då…‰ä¼ è¾“åµŒå…¥ä»¥ä»Ž3Dåœºæ™¯é…ç½®é¢„æµ‹å…¨å±€å…‰ç…§**

**å…³é”®è¯**: `å…¨å±€å…‰ç…§` `3Dåœºæ™¯åµŒå…¥` `Transformeræ¨¡åž‹` `ç‚¹äº‘è¡¨ç¤º` `ç¥žç»æ¸²æŸ“` `å…‰ä¼ è¾“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å…¨å±€å…‰ç…§æ¨¡æ‹Ÿè®¡ç®—æ˜‚è´µï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–é€åœºæ™¯ä¼˜åŒ–æˆ–2Dç©ºé—´ï¼Œå­˜åœ¨è§†å›¾ä¸ä¸€è‡´é—®é¢˜ã€‚
2. ä½¿ç”¨ç‚¹äº‘è¡¨ç¤ºåœºæ™¯ï¼Œé€šè¿‡Transformerå»ºæ¨¡å…¨å±€äº¤äº’ï¼Œç¼–ç ä¸ºç¥žç»åŸºå…ƒä»¥é¢„æµ‹æ¸²æŸ“é‡ã€‚
3. åœ¨å¤šæ ·å®¤å†…åœºæ™¯éªŒè¯æ¼«åå°„å…¨å±€å…‰ç…§é¢„æµ‹ï¼Œå¹¶å¯å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œåˆæ­¥æ”¯æŒå…‰æ³½ææ–™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Global illumination (GI) is essential for realistic rendering but remains
> computationally expensive due to the complexity of simulating indirect light
> transport. Recent neural methods have mainly relied on per-scene optimization,
> sometimes extended to handle changes in camera or geometry. Efforts toward
> cross-scene generalization have largely stayed in 2D screen space, such as
> neural denoising or G-buffer based GI prediction, which often suffer from view
> inconsistency and limited spatial understanding. We propose a generalizable 3D
> light transport embedding that approximates global illumination directly from
> 3D scene configurations, without using rasterized or path-traced cues. Each
> scene is represented as a point cloud with geometric and material features. A
> scalable transformer models global point-to-point interactions to encode these
> features into neural primitives. At render time, each query point retrieves
> nearby primitives via nearest-neighbor search and aggregates their latent
> features through cross-attention to predict the desired rendering quantity. We
> demonstrate results on diffuse global illumination prediction across diverse
> indoor scenes with varying layouts, geometry, and materials. The embedding
> trained for irradiance estimation can be quickly adapted to new rendering tasks
> with limited fine-tuning. We also present preliminary results for
> spatial-directional radiance field estimation for glossy materials and show how
> the normalized field can accelerate unbiased path guiding. This approach
> highlights a path toward integrating learned priors into rendering pipelines
> without explicit ray-traced illumination cues.

