---
layout: default
title: MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation
---

# MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation

**arXiv**: [2510.18692v1](https://arxiv.org/abs/2510.18692) | [PDF](https://arxiv.org/pdf/2510.18692.pdf)

**ä½œè€…**: Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoGAç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä»¥è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­æ³¨æ„åŠ›è®¡ç®—æ•ˆçŽ‡ä½Žçš„é—®é¢˜**

**å…³é”®è¯**: `é•¿è§†é¢‘ç”Ÿæˆ` `ç¨€ç–æ³¨æ„åŠ›` `æ‰©æ•£å˜æ¢å™¨` `ä»¤ç‰Œè·¯ç”±` `ç«¯åˆ°ç«¯è®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£å˜æ¢å™¨åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­ï¼Œå…¨æ³¨æ„åŠ›è®¡ç®—éšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢žé•¿ï¼Œæ•ˆçŽ‡ä½Žä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è½»é‡çº§å¯å­¦ä¹ ä»¤ç‰Œè·¯ç”±å™¨å®žçŽ°è¯­ä¹‰æ„ŸçŸ¥è·¯ç”±ï¼Œé¿å…å—çŠ¶ä¼°è®¡ï¼Œæå‡é•¿ç¨‹äº¤äº’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹èƒ½ç«¯åˆ°ç«¯ç”Ÿæˆé•¿è¾¾åˆ†é’Ÿçº§ã€å¤šé•œå¤´ã€480pè§†é¢‘ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long video generation with Diffusion Transformers (DiTs) is bottlenecked by
> the quadratic scaling of full attention with sequence length. Since attention
> is highly redundant, outputs are dominated by a small subset of query-key
> pairs. Existing sparse methods rely on blockwise coarse estimation, whose
> accuracy-efficiency trade-offs are constrained by block size. This paper
> introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention
> that uses a lightweight, learnable token router to precisely match tokens
> without blockwise estimation. Through semantic-aware routing, MoGA enables
> effective long-range interactions. As a kernel-free method, MoGA integrates
> seamlessly with modern attention stacks, including FlashAttention and sequence
> parallelism. Building on MoGA, we develop an efficient long video generation
> model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,
> with a context length of approximately 580k. Comprehensive experiments on
> various video generation tasks validate the effectiveness of our approach.

