---
layout: default
title: Biomechanically consistent real-time action recognition for human-robot interaction
---

# Biomechanically consistent real-time action recognition for human-robot interaction

**arXiv**: [2510.18373v1](https://arxiv.org/abs/2510.18373) | [PDF](https://arxiv.org/pdf/2510.18373.pdf)

**ä½œè€…**: Wanchen Li, Kahina Chalabi, Sabbah Maxime, Thomas Bousquet, Robin Passama, Sofiane Ramdani, Andrea Cherubini, Vincent Bonnet

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç”Ÿç‰©åŠ›å­¦å…ˆéªŒçš„å®žæ—¶åŠ¨ä½œè¯†åˆ«æ¡†æž¶ï¼Œç”¨äºŽå·¥ä¸šäººæœºäº¤äº’ã€‚**

**å…³é”®è¯**: `å®žæ—¶åŠ¨ä½œè¯†åˆ«` `ç”Ÿç‰©åŠ›å­¦å…ˆéªŒ` `Transformerç½‘ç»œ` `äººæœºäº¤äº’` `å…³èŠ‚è§’åº¦ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–å…³èŠ‚ä¸­å¿ƒä½ç½®ï¼Œå¤šä¸ºç¦»çº¿ï¼Œéš¾ä»¥å®žæ—¶é²æ£’è¯†åˆ«ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å…³èŠ‚è§’åº¦ä½œä¸ºè¾“å…¥ï¼Œç»“åˆTransformerç½‘ç»œï¼Œå®žçŽ°å®žæ—¶åŠ¨ä½œè¯†åˆ«ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨11äººæ•°æ®é›†ä¸Šè¾¾88%å‡†ç¡®çŽ‡ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼Œæ”¯æŒæœºå™¨äººå®žæ—¶äº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a novel framework for real-time human action recognition
> in industrial contexts, using standard 2D cameras. We introduce a complete
> pipeline for robust and real-time estimation of human joint kinematics, input
> to a temporally smoothed Transformer-based network, for action recognition. We
> rely on a new dataset including 11 subjects performing various actions, to
> evaluate our approach. Unlike most of the literature that relies on joint
> center positions (JCP) and is offline, ours uses biomechanical prior, eg. joint
> angles, for fast and robust real-time recognition. Besides, joint angles make
> the proposed method agnostic to sensor and subject poses as well as to
> anthropometric differences, and ensure robustness across environments and
> subjects. Our proposed learning model outperforms the best baseline model,
> running also in real-time, along various metrics. It achieves 88% accuracy and
> shows great generalization ability, for subjects not facing the cameras.
> Finally, we demonstrate the robustness and usefulness of our technique, through
> an online interaction experiment, with a simulated robot controlled in
> real-time via the recognized actions.

