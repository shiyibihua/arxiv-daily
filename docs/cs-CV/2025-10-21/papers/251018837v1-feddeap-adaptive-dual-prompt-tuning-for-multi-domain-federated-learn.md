---
layout: default
title: FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning
---

# FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning

**arXiv**: [2510.18837v1](https://arxiv.org/abs/2510.18837) | [PDF](https://arxiv.org/pdf/2510.18837.pdf)

**ä½œè€…**: Yubin Zheng, Pak-Hei Yeung, Jing Xia, Tianjie Ju, Peng Tang, Weidong Qiu, Jagath C. Rajapakse

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedDEAPè‡ªé€‚åº”åŒæç¤ºè°ƒä¼˜æ¡†æž¶ä»¥å¢žå¼ºå¤šé¢†åŸŸè”é‚¦å­¦ä¹ ä¸­CLIPçš„æ³›åŒ–èƒ½åŠ›**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤šé¢†åŸŸå­¦ä¹ ` `æç¤ºè°ƒä¼˜` `CLIPæ¨¡åž‹` `å›¾åƒè¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè”é‚¦å­¦ä¹ ä¸­é¢†åŸŸåç§»å’Œæ ‡ç­¾å¼‚æž„æ€§é˜»ç¢å…¨å±€æ¨¡åž‹æ³›åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è¯­ä¹‰å’Œé¢†åŸŸå˜æ¢ç½‘ç»œè§£è€¦ç‰¹å¾ï¼Œå¹¶è®¾è®¡å…¨å±€è¯­ä¹‰æç¤ºä¸Žå±€éƒ¨é¢†åŸŸæç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•å¯¹å¤šé¢†åŸŸå›¾åƒè¯†åˆ«çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Federated learning (FL) enables multiple clients to collaboratively train
> machine learning models without exposing local data, balancing performance and
> privacy. However, domain shift and label heterogeneity across clients often
> hinder the generalization of the aggregated global model. Recently, large-scale
> vision-language models like CLIP have shown strong zero-shot classification
> capabilities, raising the question of how to effectively fine-tune CLIP across
> domains in a federated setting. In this work, we propose an adaptive federated
> prompt tuning framework, FedDEAP, to enhance CLIP's generalization in
> multi-domain scenarios. Our method includes the following three key components:
> (1) To mitigate the loss of domain-specific information caused by
> label-supervised tuning, we disentangle semantic and domain-specific features
> in images by using semantic and domain transformation networks with unbiased
> mappings; (2) To preserve domain-specific knowledge during global prompt
> aggregation, we introduce a dual-prompt design with a global semantic prompt
> and a local domain prompt to balance shared and personalized information; (3)
> To maximize the inclusion of semantic and domain information from images in the
> generated text features, we align textual and visual representations under the
> two learned transformations to preserve semantic and domain consistency.
> Theoretical analysis and extensive experiments on four datasets demonstrate the
> effectiveness of our method in enhancing the generalization of CLIP for
> federated image recognition across multiple domains.

