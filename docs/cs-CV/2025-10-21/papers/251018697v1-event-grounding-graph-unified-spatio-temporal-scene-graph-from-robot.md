---
layout: default
title: Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations
---

# Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations

**arXiv**: [2510.18697v1](https://arxiv.org/abs/2510.18697) | [PDF](https://arxiv.org/pdf/2510.18697.pdf)

**ä½œè€…**: Phuoc Nguyen, Francesco Verdoja, Ville Kyrki

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº‹ä»¶æŽ¥åœ°å›¾ä»¥è¿žæŽ¥ç©ºé—´ç‰¹å¾ä¸ŽåŠ¨æ€äº‹ä»¶ï¼Œæå‡æœºå™¨äººåœºæ™¯ç†è§£ã€‚**

**å…³é”®è¯**: `æœºå™¨äººåœºæ™¯ç†è§£` `äº‹ä»¶æŽ¥åœ°å›¾` `æ—¶ç©ºå›¾è¡¨ç¤º` `è¯­ä¹‰åœºæ™¯è¡¨ç¤º` `è‡ªä¸»æœºå™¨äºº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è¯­ä¹‰åœºæ™¯è¡¨ç¤ºç¼ºä¹ç©ºé—´ç‰¹å¾ä¸ŽåŠ¨æ€äº‹ä»¶çš„è¿žæŽ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºäº‹ä»¶æŽ¥åœ°å›¾æ¡†æž¶ï¼Œå°†äº‹ä»¶äº¤äº’æŽ¥åœ°åˆ°åœºæ™¯ç©ºé—´ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä½¿ç”¨çœŸå®žæœºå™¨äººæ•°æ®éªŒè¯ï¼Œèƒ½å‡†ç¡®å“åº”å¤æ‚æ—¶ç©ºæŸ¥è¯¢ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A fundamental aspect for building intelligent autonomous robots that can
> assist humans in their daily lives is the construction of rich environmental
> representations. While advances in semantic scene representations have enriched
> robotic scene understanding, current approaches lack a connection between
> spatial features and dynamic events; e.g., connecting the blue mug to the event
> washing a mug. In this work, we introduce the event-grounding graph (EGG), a
> framework grounding event interactions to spatial features of a scene. This
> representation allows robots to perceive, reason, and respond to complex
> spatio-temporal queries. Experiments using real robotic data demonstrate EGG's
> capability to retrieve relevant information and respond accurately to human
> inquiries concerning the environment and events within. Furthermore, the EGG
> framework's source code and evaluation dataset are released as open-source at:
> https://github.com/aalto-intelligent-robotics/EGG.

