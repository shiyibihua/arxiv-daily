---
layout: default
title: CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder
---

# CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder

**arXiv**: [2510.18583v1](https://arxiv.org/abs/2510.18583) | [PDF](https://arxiv.org/pdf/2510.18583.pdf)

**ä½œè€…**: Yongmin Lee, Hye Won Chung

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCovMatchä»¥è§£å†³å¤šæ¨¡æ€æ•°æ®é›†è’¸é¦ä¸­çš„è·¨æ¨¡æ€å¯¹é½å’Œå¯æ‰©å±•æ€§é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ•°æ®é›†è’¸é¦` `è·¨æ¨¡æ€å¯¹é½` `è·¨åæ–¹å·®æŒ‡å¯¼` `è”åˆç¼–ç å™¨ä¼˜åŒ–` `æ£€ç´¢å‡†ç¡®çŽ‡æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ•°æ®é›†è’¸é¦é¢ä¸´è·¨æ¨¡æ€å¯¹é½å›°éš¾å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è·¨åæ–¹å·®å¯¹é½å’Œæ¨¡æ€å†…ç‰¹å¾æ­£åˆ™åŒ–ï¼Œè”åˆä¼˜åŒ–å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Flickr30Kå’ŒCOCOä¸Šï¼Œä½¿ç”¨500åˆæˆå¯¹å®žçŽ°æ£€ç´¢å‡†ç¡®çŽ‡æœ€é«˜æå‡6.8%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal dataset distillation aims to synthesize a small set of image-text
> pairs that enables efficient training of large-scale vision-language models.
> While dataset distillation has shown promise in unimodal tasks, extending it to
> multimodal contrastive learning presents key challenges: learning cross-modal
> alignment and managing the high computational cost of large encoders. Prior
> approaches address scalability by freezing the text encoder and update only the
> image encoder and text projection layer. However, we find this severely limits
> semantic alignment and becomes a bottleneck for performance scaling. We propose
> CovMatch, a scalable dataset distillation framework that aligns the
> cross-covariance of real and synthetic features while regularizing feature
> distributions within each modality. Unlike prior approaches, CovMatch enables
> joint optimization of both encoders, leading to stronger cross-modal alignment
> and improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms
> state-of-the-art multimodal distillation methods and achieves up to 6.8%
> absolute gains in retrieval accuracy using only 500 synthetic pairs.

