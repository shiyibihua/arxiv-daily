---
layout: default
title: VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety
---

# VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety

**arXiv**: [2510.18214v1](https://arxiv.org/abs/2510.18214) | [PDF](https://arxiv.org/pdf/2510.18214.pdf)

**ä½œè€…**: Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLSUæ¡†æ¶ä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨è”åˆå›¾åƒ-æ–‡æœ¬ç†è§£ä¸­çš„å®‰å…¨é£é™©**

**å…³é”®è¯**: `å¤šæ¨¡æ€å®‰å…¨è¯„ä¼°` `è”åˆå›¾åƒ-æ–‡æœ¬ç†è§£` `å®‰å…¨æ¨¡å¼åˆ†ç±»` `åŸºå‡†æ•°æ®é›†` `ç»„åˆæ¨ç†å¤±è´¥` `æ¨¡å‹å¯¹é½å·®è·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å®‰å…¨è¯„ä¼°å¿½ç•¥å¤šæ¨¡æ€è”åˆè§£é‡Šé£é™©ï¼Œå¯¼è‡´è¿‡é˜»æ–­æˆ–æ¬ æ‹’ç»æœ‰å®³å†…å®¹
2. æ–¹æ³•è¦ç‚¹ï¼šæ„å»ºç»†ç²’åº¦ä¸¥é‡æ€§åˆ†ç±»å’Œç»„åˆåˆ†æï¼Œè¦†ç›–17ç§å®‰å…¨æ¨¡å¼çš„å¤§è§„æ¨¡åŸºå‡†
3. å®éªŒæˆ–æ•ˆæœï¼šæ¨¡å‹åœ¨è”åˆæ¨ç†æ—¶å‡†ç¡®ç‡é™è‡³20-55%ï¼Œ34%é”™è¯¯æºäºç»„åˆæ¨ç†ç¼ºå¤±

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Safety evaluation of multimodal foundation models often treats vision and
> language inputs separately, missing risks from joint interpretation where
> benign content becomes harmful in combination. Existing approaches also fail to
> distinguish clearly unsafe content from borderline cases, leading to
> problematic over-blocking or under-refusal of genuinely harmful content. We
> present Vision Language Safety Understanding (VLSU), a comprehensive framework
> to systematically evaluate multimodal safety through fine-grained severity
> classification and combinatorial analysis across 17 distinct safety patterns.
> Using a multi-stage pipeline with real-world images and human annotation, we
> construct a large-scale benchmark of 8,187 samples spanning 15 harm categories.
> Our evaluation of eleven state-of-the-art models reveals systematic joint
> understanding failures: while models achieve 90%-plus accuracy on clear
> unimodal safety signals, performance degrades substantially to 20-55% when
> joint image-text reasoning is required to determine the safety label. Most
> critically, 34% of errors in joint image-text safety classification occur
> despite correct classification of the individual modalities, further
> demonstrating absent compositional reasoning capabilities. Additionally, we
> find that models struggle to balance refusing unsafe content while still
> responding to borderline cases that deserve engagement. For example, we find
> that instruction framing can reduce the over-blocking rate on borderline
> content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of
> under-refusing on unsafe content with refusal rate dropping from 90.8% to
> 53.9%. Overall, our framework exposes weaknesses in joint image-text
> understanding and alignment gaps in current models, and provides a critical
> test bed to enable the next milestones in research on robust vision-language
> safety.

