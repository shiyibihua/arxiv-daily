---
layout: default
title: Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models
---

# Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models

**arXiv**: [2510.18303v1](https://arxiv.org/abs/2510.18303) | [PDF](https://arxiv.org/pdf/2510.18303.pdf)

**ä½œè€…**: Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMed-RwRæ¡†æž¶ä»¥è§£å†³åŒ»å­¦å¤šæ¨¡æ€å¤§æ¨¡åž‹æŽ¨ç†ä¸­çš„å¹»è§‰å’Œäº‹å®žé”™è¯¯é—®é¢˜**

**å…³é”®è¯**: `åŒ»å­¦å¤šæ¨¡æ€å¤§æ¨¡åž‹` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `ä¸»åŠ¨æŽ¨ç†` `å¤–éƒ¨çŸ¥è¯†é›†æˆ` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŒ»å­¦MLLMsä¾èµ–å†…éƒ¨çŸ¥è¯†ï¼Œå¯¼è‡´æŽ¨ç†å¹»è§‰å’Œäº‹å®žä¸å‡†ç¡®
2. è®¾è®¡ä¸»åŠ¨æ£€ç´¢å¤–éƒ¨çŸ¥è¯†çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯
3. åœ¨å¤šä¸ªåŒ»å­¦åŸºå‡†ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼Œå¹¶åœ¨é™Œç”Ÿé¢†åŸŸå±•çŽ°å¼ºæ³›åŒ–èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Incentivizing the reasoning ability of Multimodal Large Language Models
> (MLLMs) is essential for medical applications to transparently analyze medical
> scans and provide reliable diagnosis. However, existing medical MLLMs rely
> solely on internal knowledge during reasoning, leading to hallucinated
> reasoning and factual inaccuracies when encountering cases beyond their
> training scope. Although recent Agentic Retrieval-Augmented Generation (RAG)
> methods elicit the medical model's proactive retrieval ability during
> reasoning, they are confined to unimodal LLMs, neglecting the crucial visual
> information during reasoning and retrieval. Consequently, we propose the first
> Multimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively
> retrieves external knowledge by querying observed symptoms or domain-specific
> medical concepts during reasoning. Specifically, we design a two-stage
> reinforcement learning strategy with tailored rewards that stimulate the model
> to leverage both visual diagnostic findings and textual clinical information
> for effective retrieval. Building on this foundation, we further propose a
> Confidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when
> low prediction confidence is detected. Evaluation on various public medical
> benchmarks demonstrates Med-RwR's significant improvements over baseline
> models, proving the effectiveness of enhancing reasoning capabilities with
> external knowledge integration. Furthermore, Med-RwR demonstrates remarkable
> generalizability to unfamiliar domains, evidenced by 8.8% performance gain on
> our proposed EchoCardiography Benchmark (ECBench), despite the scarcity of
> echocardiography data in the training corpus. Our data, model, and codes will
> be made publicly available at https://github.com/xmed-lab/Med-RwR.

