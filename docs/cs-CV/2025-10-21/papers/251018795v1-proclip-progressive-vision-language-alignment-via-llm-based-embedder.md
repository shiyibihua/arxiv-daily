---
layout: default
title: ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder
---

# ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder

**arXiv**: [2510.18795v1](https://arxiv.org/abs/2510.18795) | [PDF](https://arxiv.org/pdf/2510.18795.pdf)

**ä½œè€…**: Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProCLIPæ¡†æž¶ä»¥è§£å†³LLMåµŒå…¥å™¨ä¸ŽCLIPå›¾åƒç¼–ç å™¨å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¹é½` `LLMåµŒå…¥å™¨` `è¯¾ç¨‹å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPæ–‡æœ¬ç¼–ç å™¨å¤„ç†é•¿æ–‡æœ¬å’Œå¤šè¯­è¨€èƒ½åŠ›æœ‰é™ï¼Œç›´æŽ¥å¯¹é½LLMåµŒå…¥å™¨ä¼šç ´åCLIPé¢„è®­ç»ƒçŸ¥è¯†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ï¼Œå…ˆè’¸é¦CLIPæ–‡æœ¬ç¼–ç å™¨çŸ¥è¯†ï¼Œå†é€šè¿‡å¯¹æ¯”è°ƒä¼˜å’Œè‡ªè’¸é¦æ­£åˆ™åŒ–è¿›è¡Œæ¸è¿›å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥ï¼Œä½†ä»£ç å·²å¼€æºï¼Œå¯èƒ½æå‡é•¿æ–‡æœ¬å’Œå¤šè¯­è¨€è§†è§‰è¯­è¨€ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The original CLIP text encoder is limited by a maximum input length of 77
> tokens, which hampers its ability to effectively process long texts and perform
> fine-grained semantic understanding. In addition, the CLIP text encoder lacks
> support for multilingual inputs. All these limitations significantly restrict
> its applicability across a broader range of tasks. Recent studies have
> attempted to replace the CLIP text encoder with an LLM-based embedder to
> enhance its ability in processing long texts, multilingual understanding, and
> fine-grained semantic comprehension. However, because the representation spaces
> of LLMs and the vision-language space of CLIP are pretrained independently
> without alignment priors, direct alignment using contrastive learning can
> disrupt the intrinsic vision-language alignment in the CLIP image encoder,
> leading to an underutilization of the knowledge acquired during pre-training.
> To address this challenge, we propose ProCLIP, a curriculum learning-based
> progressive vision-language alignment framework to effectively align the CLIP
> image encoder with an LLM-based embedder. Specifically, ProCLIP first distills
> knowledge from CLIP's text encoder into the LLM-based embedder to leverage
> CLIP's rich pretrained knowledge while establishing initial alignment between
> the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns
> the CLIP image encoder with the LLM-based embedder through image-text
> contrastive tuning, employing self-distillation regularization to avoid
> overfitting. To achieve a more effective alignment, instance semantic alignment
> loss and embedding structure alignment loss are employed during representation
> inheritance and contrastive tuning. The Code is available at
> https://github.com/VisionXLab/ProCLIP

