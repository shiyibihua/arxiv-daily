---
layout: default
title: AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering
---

# AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering

**arXiv**: [2510.18346v1](https://arxiv.org/abs/2510.18346) | [PDF](https://arxiv.org/pdf/2510.18346.pdf)

**ä½œè€…**: Jiayu Zhang, Qilang Ye, Shuo Ye, Xun Lin, Zihan Song, Zitong Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAV-Masteræ¡†æž¶ä»¥è§£å†³éŸ³é¢‘è§†è§‰é—®ç­”ä¸­çš„å†—ä½™å’Œæ¨¡æ€åå¥½é—®é¢˜**

**å…³é”®è¯**: `éŸ³é¢‘è§†è§‰é—®ç­”` `åŠ¨æ€é‡‡æ ·` `æ¨¡æ€åå¥½` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨æ—¶é—´é‡‡æ ·å’Œæ¨¡æ€åå¥½ä¸Šç¼ºä¹çµæ´»æ€§ï¼Œéš¾ä»¥èšç„¦å…³é”®ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŠ¨æ€è‡ªé€‚åº”ç„¦ç‚¹é‡‡æ ·å’Œåå¥½æ„ŸçŸ¥ç­–ç•¥ï¼Œå¢žå¼ºå…³é”®ä¿¡æ¯æå–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Audio-Visual Question Answering (AVQA) requires models to effectively utilize
> both visual and auditory modalities to answer complex and diverse questions
> about audio-visual scenes. However, existing methods lack sufficient
> flexibility and dynamic adaptability in temporal sampling and modality
> preference awareness, making it difficult to focus on key information based on
> the question. This limits their reasoning capability in complex scenarios. To
> address these challenges, we propose a novel framework named AV-Master. It
> enhances the model's ability to extract key information from complex
> audio-visual scenes with substantial redundant content by dynamically modeling
> both temporal and modality dimensions. In the temporal dimension, we introduce
> a dynamic adaptive focus sampling mechanism that progressively focuses on
> audio-visual segments most relevant to the question, effectively mitigating
> redundancy and segment fragmentation in traditional sampling methods. In the
> modality dimension, we propose a preference-aware strategy that models each
> modality's contribution independently, enabling selective activation of
> critical features. Furthermore, we introduce a dual-path contrastive loss to
> reinforce consistency and complementarity across temporal and modality
> dimensions, guiding the model to learn question-specific cross-modal
> collaborative representations. Experiments on four large-scale benchmarks show
> that AV-Master significantly outperforms existing methods, especially in
> complex reasoning tasks.

