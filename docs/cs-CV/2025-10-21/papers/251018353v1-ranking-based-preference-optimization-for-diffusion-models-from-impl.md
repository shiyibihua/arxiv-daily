---
layout: default
title: Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback
---

# Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback

**arXiv**: [2510.18353v1](https://arxiv.org/abs/2510.18353) | [PDF](https://arxiv.org/pdf/2510.18353.pdf)

**ä½œè€…**: Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiffusion-DROæ¡†æž¶ï¼Œé€šè¿‡æŽ’åä¼˜åŒ–è§£å†³æ‰©æ•£æ¨¡åž‹åå¥½å­¦ä¹ ä¸­çš„éžçº¿æ€§ä¼°è®¡å’Œç¦»çº¿æ•°æ®é™åˆ¶é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `åå¥½ä¼˜åŒ–` `é€†å¼ºåŒ–å­¦ä¹ ` `æŽ’åå­¦ä¹ ` `åŽ»å™ªè®­ç»ƒ` `ç¦»çº¿åœ¨çº¿æ•°æ®èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰DPOæ–¹æ³•åœ¨ä¼°è®¡å›¾åƒæ¦‚çŽ‡æ—¶å› sigmoidå‡½æ•°éžçº¿æ€§å’Œç¦»çº¿æ•°æ®é›†å¤šæ ·æ€§ä¸è¶³è€Œå—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé€†å¼ºåŒ–å­¦ä¹ ï¼Œå°†åå¥½å­¦ä¹ è½¬åŒ–ä¸ºæŽ’åé—®é¢˜ï¼Œç®€åŒ–è®­ç»ƒç›®æ ‡ä¸ºåŽ»å™ªå½¢å¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æŒ‘æˆ˜æ€§å’Œæœªè§æç¤ºä¸‹ï¼Œç”Ÿæˆè´¨é‡ä¼˜äºŽåŸºçº¿ï¼Œå®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶å‡æ˜¾ç¤ºæ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Direct preference optimization (DPO) methods have shown strong potential in
> aligning text-to-image diffusion models with human preferences by training on
> paired comparisons. These methods improve training stability by avoiding the
> REINFORCE algorithm but still struggle with challenges such as accurately
> estimating image probabilities due to the non-linear nature of the sigmoid
> function and the limited diversity of offline datasets. In this paper, we
> introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new
> preference learning framework grounded in inverse reinforcement learning.
> Diffusion-DRO removes the dependency on a reward model by casting preference
> learning as a ranking problem, thereby simplifying the training objective into
> a denoising formulation and overcoming the non-linear estimation issues found
> in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert
> demonstrations with online policy-generated negative samples, enabling it to
> effectively capture human preferences while addressing the limitations of
> offline data. Comprehensive experiments show that Diffusion-DRO delivers
> improved generation quality across a range of challenging and unseen prompts,
> outperforming state-of-the-art baselines in both both quantitative metrics and
> user studies. Our source code and pre-trained models are available at
> https://github.com/basiclab/DiffusionDRO.

