---
layout: default
title: BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining
---

# BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining

**arXiv**: [2510.18244v1](https://arxiv.org/abs/2510.18244) | [PDF](https://arxiv.org/pdf/2510.18244.pdf)

**ä½œè€…**: Ajinkya Khoche, GergÅ‘ LÃ¡szlÃ³ Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBlendCLIPæ¡†æž¶ï¼Œé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒå¼¥åˆåˆæˆä¸ŽçœŸå®žåŸŸå·®è·ï¼Œå®žçŽ°é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»ã€‚**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `3Dç‰©ä½“åˆ†ç±»` `å¤šæ¨¡æ€é¢„è®­ç»ƒ` `åŸŸé€‚åº”` `LiDARæ•°æ®` `åˆæˆæ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›¶æ ·æœ¬3Dåˆ†ç±»å—åˆæˆä¸ŽçœŸå®žæ•°æ®åŸŸå·®è·å½±å“ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è¯¾ç¨‹å¼æ•°æ®æ··åˆç­–ç•¥ï¼Œç»“åˆåˆæˆCADæ•°æ®å’ŒçœŸå®žLiDARæ‰«æè¿›è¡Œå¤šæ¨¡æ€é¢„è®­ç»ƒã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨nuScenesåŸºå‡†ä¸Šï¼Œä»…æ·»åŠ å°‘é‡çœŸå®žæ ·æœ¬å³å¯æå‡é›¶æ ·æœ¬å‡†ç¡®çŽ‡27%ï¼Œè¾¾åˆ°SOTAæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Zero-shot 3D object classification is crucial for real-world applications
> like autonomous driving, however it is often hindered by a significant domain
> gap between the synthetic data used for training and the sparse, noisy LiDAR
> scans encountered in the real-world. Current methods trained solely on
> synthetic data fail to generalize to outdoor scenes, while those trained only
> on real data lack the semantic diversity to recognize rare or unseen objects.
>   We introduce BlendCLIP, a multimodal pretraining framework that bridges this
> synthetic-to-real gap by strategically combining the strengths of both domains.
> We first propose a pipeline to generate a large-scale dataset of object-level
> triplets -- consisting of a point cloud, image, and text description -- mined
> directly from real-world driving data and human annotated 3D boxes. Our core
> contribution is a curriculum-based data mixing strategy that first grounds the
> model in the semantically rich synthetic CAD data before progressively adapting
> it to the specific characteristics of real-world scans.
>   Our experiments show that our approach is highly label-efficient: introducing
> as few as 1.5\% real-world samples per batch into training boosts zero-shot
> accuracy on the nuScenes benchmark by 27\%. Consequently, our final model
> achieves state-of-the-art performance on challenging outdoor datasets like
> nuScenes and TruckScenes, improving over the best prior method by 19.3\% on
> nuScenes, while maintaining strong generalization on diverse synthetic
> benchmarks. Our findings demonstrate that effective domain adaptation, not
> full-scale real-world annotation, is the key to unlocking robust
> open-vocabulary 3D perception. Our code and dataset will be released upon
> acceptance on https://github.com/kesu1/BlendCLIP.

