---
layout: default
title: Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents
---

# Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents

**arXiv**: [2510.18703v1](https://arxiv.org/abs/2510.18703) | [PDF](https://arxiv.org/pdf/2510.18703.pdf)

**ä½œè€…**: Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVC2Læ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€ç½‘é¡µæ–‡æ¡£ä¸­å¤æ‚è·¨æ¨¡æ€å…³ç³»å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `è§†è§‰Transformer` `è·¨æ¨¡æ€æ£€ç´¢` `ç½‘é¡µæ–‡æ¡£ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¯¹æ¯”æ¨¡åž‹éš¾ä»¥å¤„ç†ç½‘é¡µæ–‡æ¡£ä¸­æ–‡æœ¬ä¸Žå›¾åƒäº¤é”™ã€æ¾æ•£å¯¹é½æˆ–è§†è§‰åµŒå…¥çš„å¤æ‚åœºæ™¯
2. æ–¹æ³•è¦ç‚¹ï¼šVC2Lä½¿ç”¨å•ä¸€è§†è§‰Transformeråœ¨åƒç´ ç©ºé—´ç»Ÿä¸€å»ºæ¨¡ï¼Œæ— éœ€OCRæˆ–æ¨¡æ€èžåˆç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ£€ç´¢åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜äºŽæˆ–åª²ç¾ŽCLIPæ¨¡åž‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive vision-language models such as CLIP have demonstrated strong
> performance across a wide range of multimodal tasks by learning from aligned
> image-text pairs. However, their ability to handle complex, real-world web
> documents remains limited, particularly in scenarios where text and images are
> interleaved, loosely aligned, or embedded in visual form. To address these
> challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
> framework that models text, images, and their combinations using a single
> vision transformer. VC2L operates entirely in pixel space by rendering all
> inputs, whether textual, visual, or combined, as images, thus eliminating the
> need for OCR, text tokenization, or modality fusion strategy. To capture
> complex cross-modal relationships in multimodal web documents, VC2L employs a
> snippet-level contrastive learning objective that aligns consecutive multimodal
> segments, leveraging the inherent coherence of documents without requiring
> explicitly paired image-text data. To assess the effectiveness of this
> approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
> designed to evaluate cross-modal retrieval, fine-grained sequential
> understanding, and generalization to unseen data, respectively. Empirical
> results show that VC2L achieves competitive or superior performance compared to
> CLIP-style models on both the proposed benchmarks and established datasets such
> as M-BEIR and MTEB. These findings underscore the potential of multimodal web
> data as a valuable training resource for contrastive learning and illustrate
> the scalability of a unified, vision-centric approach for multimodal
> representation learning. Code and models are available at:
> https://github.com/showlab/VC2L.

