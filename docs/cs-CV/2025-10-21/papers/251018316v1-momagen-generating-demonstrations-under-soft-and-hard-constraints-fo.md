---
layout: default
title: MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation
---

# MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation

**arXiv**: [2510.18316v1](https://arxiv.org/abs/2510.18316) | [PDF](https://arxiv.org/pdf/2510.18316.pdf)

**ä½œè€…**: Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto MartÃ­n-MartÃ­n, Li Fei-Fei

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoMaGenæ¡†æ¶ï¼Œåœ¨è½¯ç¡¬çº¦æŸä¸‹ç”Ÿæˆå¤šæ­¥éª¤åŒæ‰‹æœºå™¨äººç§»åŠ¨æ“ä½œæ¼”ç¤ºæ•°æ®**

**å…³é”®è¯**: `æœºå™¨äººæ¨¡ä»¿å­¦ä¹ ` `åŒæ‰‹æœºå™¨äººæ“ä½œ` `ç§»åŠ¨æœºå™¨äºº` `æ•°æ®ç”Ÿæˆä¼˜åŒ–` `çº¦æŸä¼˜åŒ–é—®é¢˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ­¥éª¤åŒæ‰‹æœºå™¨äººç§»åŠ¨æ“ä½œä¸­ï¼Œæ•°æ®æ”¶é›†æˆæœ¬é«˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†åŸºåº§æ”¾ç½®å’Œç›¸æœºå®šä½æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ•°æ®ç”Ÿæˆå»ºæ¨¡ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œå¼ºåˆ¶æ‰§è¡Œç¡¬çº¦æŸå¦‚å¯è¾¾æ€§ï¼Œå¹³è¡¡è½¯çº¦æŸå¦‚å¯¼èˆªå¯è§æ€§
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å››ä¸ªä»»åŠ¡ä¸­ç”Ÿæˆæ›´å¤šæ ·æ•°æ®é›†ï¼Œä»å•æ¼”ç¤ºè®­ç»ƒç­–ç•¥ï¼Œä»…éœ€40çœŸå®æ¼”ç¤ºå³å¯éƒ¨ç½²åˆ°ç‰©ç†ç¡¬ä»¶

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning from large-scale, diverse human demonstrations has proven
> effective for training robots, but collecting such data is costly and
> time-consuming. This challenge is amplified for multi-step bimanual mobile
> manipulation, where humans must teleoperate both a mobile base and two
> high-degree-of-freedom arms. Prior automated data generation frameworks have
> addressed static bimanual manipulation by augmenting a few human demonstrations
> in simulation, but they fall short for mobile settings due to two key
> challenges: (1) determining base placement to ensure reachability, and (2)
> positioning the camera to provide sufficient visibility for visuomotor
> policies. To address these issues, we introduce MoMaGen, which formulates data
> generation as a constrained optimization problem that enforces hard constraints
> (e.g., reachability) while balancing soft constraints (e.g., visibility during
> navigation). This formulation generalizes prior approaches and provides a
> principled foundation for future methods. We evaluate MoMaGen on four
> multi-step bimanual mobile manipulation tasks and show that it generates
> significantly more diverse datasets than existing methods. Leveraging this
> diversity, MoMaGen can train successful imitation learning policies from a
> single source demonstration, and these policies can be fine-tuned with as few
> as 40 real-world demonstrations to achieve deployment on physical robotic
> hardware. More details are available at our project page: momagen.github.io.

