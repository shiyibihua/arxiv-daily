---
layout: default
title: IF-VidCap: Can Video Caption Models Follow Instructions?
---

# IF-VidCap: Can Video Caption Models Follow Instructions?

**arXiv**: [2510.18726v1](https://arxiv.org/abs/2510.18726) | [PDF](https://arxiv.org/pdf/2510.18726.pdf)

**ä½œè€…**: Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, Jiaheng Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIF-VidCapåŸºå‡†ä»¥è¯„ä¼°å¯æŽ§è§†é¢‘æè¿°ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›**

**å…³é”®è¯**: `å¯æŽ§è§†é¢‘æè¿°` `æŒ‡ä»¤éµå¾ªåŸºå‡†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ ¼å¼æ­£ç¡®æ€§` `å†…å®¹æ­£ç¡®æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è§†é¢‘æè¿°åŸºå‡†å¿½è§†æŒ‡ä»¤éµå¾ªï¼Œä¸“æ³¨äºŽæè¿°å…¨é¢æ€§
2. å¼•å…¥IF-VidCapåŸºå‡†ï¼Œè¯„ä¼°æ ¼å¼å’Œå†…å®¹æ­£ç¡®æ€§ï¼Œå«1400æ ·æœ¬
3. è¯„ä¼°20+æ¨¡åž‹æ˜¾ç¤ºä¸“æœ‰æ¨¡åž‹é¢†å…ˆï¼Œå¼€æºæ¨¡åž‹æŽ¥è¿‘ï¼Œå¯†é›†æè¿°æ¨¡åž‹è¡¨çŽ°ä¸ä½³

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Although Multimodal Large Language Models (MLLMs) have demonstrated
> proficiency in video captioning, practical applications require captions that
> follow specific user instructions rather than generating exhaustive,
> unconstrained descriptions. Current benchmarks, however, primarily assess
> descriptive comprehensiveness while largely overlooking instruction-following
> capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for
> evaluating controllable video captioning, which contains 1,400 high-quality
> samples. Distinct from existing video captioning or general
> instruction-following benchmarks, IF-VidCap incorporates a systematic framework
> that assesses captions on two dimensions: format correctness and content
> correctness. Our comprehensive evaluation of over 20 prominent models reveals a
> nuanced landscape: despite the continued dominance of proprietary models, the
> performance gap is closing, with top-tier open-source solutions now achieving
> near-parity. Furthermore, we find that models specialized for dense captioning
> underperform general-purpose MLLMs on complex instructions, indicating that
> future work should simultaneously advance both descriptive richness and
> instruction-following fidelity.

