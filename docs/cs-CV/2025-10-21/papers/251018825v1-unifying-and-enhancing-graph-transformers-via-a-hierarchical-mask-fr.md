---
layout: default
title: Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework
---

# Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework

**arXiv**: [2510.18825v1](https://arxiv.org/abs/2510.18825) | [PDF](https://arxiv.org/pdf/2510.18825.pdf)

**ä½œè€…**: Yujie Xing, Xiao Wang, Bin Wu, Hai Huang, Chuan Shi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å±‚æ¬¡æŽ©ç æ¡†æž¶ä¸ŽM3Dphormeræ¨¡åž‹ï¼Œä»¥å¢žå¼ºå›¾å˜æ¢å™¨çš„çµæ´»æ€§ä¸Žæ€§èƒ½ã€‚**

**å…³é”®è¯**: `å›¾å˜æ¢å™¨` `æ³¨æ„åŠ›æŽ©ç ` `å±‚æ¬¡å»ºæ¨¡` `æ··åˆä¸“å®¶` `åŒæ³¨æ„åŠ›è®¡ç®—` `å›¾è¡¨ç¤ºå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å›¾å˜æ¢å™¨ä¾èµ–ç‰¹å®šæž¶æž„ï¼Œçµæ´»æ€§å—é™ï¼Œæ— æ³•ç»Ÿä¸€å»ºæ¨¡å¤šæ ·èŠ‚ç‚¹äº¤äº’ã€‚
2. å¼•å…¥å±‚æ¬¡æŽ©ç æ¡†æž¶ï¼Œç»Ÿä¸€æž¶æž„ä¸Žæ³¨æ„åŠ›æŽ©ç ï¼Œå¹¶è®¾è®¡M3Dphormeré›†æˆå¤šçº§æŽ©ç ä¸ŽåŒæ³¨æ„åŠ›è®¡ç®—ã€‚
3. å¤šåŸºå‡†å®žéªŒæ˜¾ç¤ºM3Dphormerè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯æ¡†æž¶ä¸Žæ¨¡åž‹æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graph Transformers (GTs) have emerged as a powerful paradigm for graph
> representation learning due to their ability to model diverse node
> interactions. However, existing GTs often rely on intricate architectural
> designs tailored to specific interactions, limiting their flexibility. To
> address this, we propose a unified hierarchical mask framework that reveals an
> underlying equivalence between model architecture and attention mask
> construction. This framework enables a consistent modeling paradigm by
> capturing diverse interactions through carefully designed attention masks.
> Theoretical analysis under this framework demonstrates that the probability of
> correct classification positively correlates with the receptive field size and
> label consistency, leading to a fundamental design principle: an effective
> attention mask should ensure both a sufficiently large receptive field and a
> high level of label consistency. While no single existing mask satisfies this
> principle across all scenarios, our analysis reveals that hierarchical masks
> offer complementary strengths, motivating their effective integration. Then, we
> introduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with
> Multi-Level Masking and Dual Attention Computation. M3Dphormer incorporates
> three theoretically grounded hierarchical masks and employs a bi-level expert
> routing mechanism to adaptively integrate multi-level interaction information.
> To ensure scalability, we further introduce a dual attention computation scheme
> that dynamically switches between dense and sparse modes based on local mask
> sparsity. Extensive experiments across multiple benchmarks demonstrate that
> M3Dphormer achieves state-of-the-art performance, validating the effectiveness
> of our unified framework and model design.

