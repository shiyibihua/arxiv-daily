---
layout: default
title: Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views
---

# Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views

**arXiv**: [2510.18632v1](https://arxiv.org/abs/2510.18632) | [PDF](https://arxiv.org/pdf/2510.18632.pdf)

**ä½œè€…**: Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º3DThinkeræ¡†æž¶ï¼Œé€šè¿‡å‡ ä½•æƒ³è±¡ä»Žæœ‰é™è§†å›¾è¿›è¡Œ3Dç©ºé—´æŽ¨ç†ã€‚**

**å…³é”®è¯**: `3Dç©ºé—´æŽ¨ç†` `å‡ ä½•æƒ³è±¡` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æœ‰é™è§†å›¾ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨æœ‰é™è§†å›¾ä¸‹ç†è§£3Dç©ºé—´å…³ç³»èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ— éœ€3Då…ˆéªŒè¾“å…¥ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒå¯¹é½3Dæ½œåœ¨è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽåŸºçº¿ï¼Œç»Ÿä¸€3Dè¡¨ç¤ºäºŽå¤šæ¨¡æ€æŽ¨ç†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Though recent advances in vision-language models (VLMs) have achieved
> remarkable progress across a wide range of multimodal tasks, understanding 3D
> spatial relationships from limited views remains a significant challenge.
> Previous reasoning methods typically rely on pure text (e.g., topological
> cognitive maps) or on 2D visual cues. However, their limited representational
> capacity hinders performance in specific tasks that require 3D spatial
> imagination. To address this limitation, we propose 3DThinker, a framework that
> can effectively exploits the rich geometric information embedded within images
> while reasoning, like humans do. Our framework is the first to enable 3D
> mentaling during reasoning without any 3D prior input, and it does not rely on
> explicitly labeled 3D data for training. Specifically, our training consists of
> two stages. First, we perform supervised training to align the 3D latent
> generated by VLM while reasoning with that of a 3D foundation model (e.g.,
> VGGT). Then, we optimize the entire reasoning trajectory solely based on
> outcome signals, thereby refining the underlying 3D mentaling. Extensive
> experiments across multiple benchmarks show that 3DThinker consistently
> outperforms strong baselines and offers a new perspective toward unifying 3D
> representations into multimodal reasoning. Our code will be available at
> https://github.com/zhangquanchen/3DThinker.

