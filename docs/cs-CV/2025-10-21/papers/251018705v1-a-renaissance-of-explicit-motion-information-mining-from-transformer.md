---
layout: default
title: A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition
---

# A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition

**arXiv**: [2510.18705v1](https://arxiv.org/abs/2510.18705) | [PDF](https://arxiv.org/pdf/2510.18705.pdf)

**ä½œè€…**: Peiqin Zhuang, Lei Bai, Yichao Wu, Ding Liang, Luping Zhou, Yali Wang, Wanli Ouyang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEMIMæ¨¡å—ï¼Œå°†æˆæœ¬ä½“ç§¯è¿åŠ¨å»ºæ¨¡èžå…¥Transformerä»¥æå‡åŠ¨ä½œè¯†åˆ«æ€§èƒ½**

**å…³é”®è¯**: `åŠ¨ä½œè¯†åˆ«` `Transformer` `è¿åŠ¨å»ºæ¨¡` `æˆæœ¬ä½“ç§¯` `äº²å’ŒçŸ©é˜µ` `EMIMæ¨¡å—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeræ–¹æ³•åœ¨åŠ¨ä½œè¯†åˆ«ä¸­ç¼ºä¹ç²¾ç»†è¿åŠ¨å»ºæ¨¡ï¼Œå¯¼è‡´åœ¨è¿åŠ¨æ•æ„Ÿæ•°æ®é›†ä¸Šè¡¨çŽ°ä¸ä½³
2. é€šè¿‡æž„å»ºæˆæœ¬ä½“ç§¯å¼äº²å’ŒçŸ©é˜µï¼Œä»Žä¸‹ä¸€å¸§é‡‡æ ·å…³é”®ä»¤ç‰Œï¼ŒåŒæ—¶å»ºæ¨¡å¤–è§‚å’Œè¿åŠ¨ç‰¹å¾
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå°¤å…¶åœ¨Something-Something V1&V2ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, action recognition has been dominated by transformer-based methods,
> thanks to their spatiotemporal contextual aggregation capacities. However,
> despite the significant progress achieved on scene-related datasets, they do
> not perform well on motion-sensitive datasets due to the lack of elaborate
> motion modeling designs. Meanwhile, we observe that the widely-used cost volume
> in traditional action recognition is highly similar to the affinity matrix
> defined in self-attention, but equipped with powerful motion modeling
> capacities. In light of this, we propose to integrate those effective motion
> modeling properties into the existing transformer in a unified and neat way,
> with the proposal of the Explicit Motion Information Mining module (EMIM). In
> EMIM, we propose to construct the desirable affinity matrix in a cost volume
> style, where the set of key candidate tokens is sampled from the query-based
> neighboring area in the next frame in a sliding-window manner. Then, the
> constructed affinity matrix is used to aggregate contextual information for
> appearance modeling and is converted into motion features for motion modeling
> as well. We validate the motion modeling capacities of our method on four
> widely-used datasets, and our method performs better than existing
> state-of-the-art approaches, especially on motion-sensitive datasets, i.e.,
> Something-Something V1 & V2.

