---
layout: default
title: MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning
---

# MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning

**arXiv**: [2510.18337v1](https://arxiv.org/abs/2510.18337) | [PDF](https://arxiv.org/pdf/2510.18337.pdf)

**ä½œè€…**: Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, Heng Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoTVLAæ¨¡åž‹ï¼Œé€šè¿‡ç»Ÿä¸€å¿«æ…¢æŽ¨ç†è§£å†³è§†è§‰è¯­è¨€åŠ¨ä½œä»»åŠ¡ä¸­çš„è¯­è¨€å¼•å¯¼ä¸Žå»¶è¿Ÿé—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æ··åˆå˜æ¢å™¨` `å¿«æ…¢æŽ¨ç†` `æœºå™¨äººå­¦ä¹ ` `è¯­è¨€å¼•å¯¼æ€§` `ç­–ç•¥æ‰§è¡Œæ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•è¯­è¨€å¼•å¯¼æ€§å·®æˆ–æŽ¨ç†å»¶è¿Ÿé«˜ï¼Œé™åˆ¶æœºå™¨äººå¼€æ”¾ä¸–ç•Œæ³›åŒ–ã€‚
2. MoTVLAç»“åˆé€šç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ä¸Žé¢†åŸŸä¸“å®¶ï¼Œå®žçŽ°å¿«æ…¢æŽ¨ç†ä¸Žè¡Œä¸ºç­–ç•¥å­¦ä¹ ã€‚
3. å®žéªŒåœ¨NLPåŸºå‡†ã€æœºå™¨äººä»¿çœŸå’ŒçœŸå®žä¸–ç•ŒéªŒè¯å…¶æŽ¨ç†ä¸Žä»»åŠ¡æ€§èƒ½ä¼˜è¶Šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Integrating visual-language instructions into visuomotor policies is gaining
> momentum in robot learning for enhancing open-world generalization. Despite
> promising advances, existing approaches face two challenges: limited language
> steerability when no generated reasoning is used as a condition, or significant
> inference latency when reasoning is incorporated.In this work, we introduce
> MoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)
> model that integrates fast-slow unified reasoning with behavior policy
> learning. MoTVLA preserves the general intelligence of pre-trained VLMs
> (serving as the generalist) for tasks such as perception, scene understanding,
> and semantic planning, while incorporating a domain expert, a second
> transformer that shares knowledge with the pretrained VLM, to generate
> domain-specific fast reasoning (e.g., robot motion decomposition), thereby
> improving policy execution efficiency. By conditioning the action expert on
> decomposed motion instructions, MoTVLA can learn diverse behaviors and
> substantially improve language steerability. Extensive evaluations across
> natural language processing benchmarks, robotic simulation environments, and
> real-world experiments confirm the superiority of MoTVLA in both fast-slow
> reasoning and manipulation task performance.

