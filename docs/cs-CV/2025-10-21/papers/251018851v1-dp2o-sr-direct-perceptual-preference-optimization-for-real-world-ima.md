---
layout: default
title: DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution
---

# DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution

**arXiv**: [2510.18851v1](https://arxiv.org/abs/2510.18851) | [PDF](https://arxiv.org/pdf/2510.18851.pdf)

**ä½œè€…**: Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDP^2O-SRæ¡†æž¶ï¼Œä¼˜åŒ–çœŸå®žä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨çŽ‡çš„æ„ŸçŸ¥è´¨é‡ã€‚**

**å…³é”®è¯**: `å›¾åƒè¶…åˆ†è¾¨çŽ‡` `æ„ŸçŸ¥åå¥½ä¼˜åŒ–` `æ‰©æ•£æ¨¡åž‹` `å›¾åƒè´¨é‡è¯„ä¼°` `æ— ç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šT2Iæ‰©æ•£æ¨¡åž‹éšæœºæ€§å¯¼è‡´è¾“å‡ºæ„ŸçŸ¥è´¨é‡ä¸ç¨³å®šï¼Œå½±å“çœŸå®žä¸–ç•Œè¶…åˆ†è¾¨çŽ‡ã€‚
2. æ–¹æ³•ï¼šç»“åˆå…¨å‚è€ƒå’Œæ— å‚è€ƒIQAæ¨¡åž‹æž„å»ºæ··åˆå¥–åŠ±ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ä¼˜åŒ–æ„ŸçŸ¥åå¥½ã€‚
3. æ•ˆæžœï¼šåœ¨æ‰©æ•£å’Œæµå¼T2Iéª¨å¹²ä¸Šå®žéªŒï¼Œæ˜¾è‘—æå‡æ„ŸçŸ¥è´¨é‡å¹¶æ³›åŒ–è‡³çœŸå®žåŸºå‡†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world
> image super-resolution (Real-ISR) methods can synthesize rich and realistic
> details. However, due to the inherent stochasticity of T2I models, different
> noise inputs often lead to outputs with varying perceptual quality. Although
> this randomness is sometimes seen as a limitation, it also introduces a wider
> perceptual quality range, which can be exploited to improve Real-ISR
> performance. To this end, we introduce Direct Perceptual Preference
> Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative
> models with perceptual preferences without requiring costly human annotations.
> We construct a hybrid reward signal by combining full-reference and
> no-reference image quality assessment (IQA) models trained on large-scale human
> preference datasets. This reward encourages both structural fidelity and
> natural appearance. To better utilize perceptual diversity, we move beyond the
> standard best-vs-worst selection and construct multiple preference pairs from
> outputs of the same model. Our analysis reveals that the optimal selection
> ratio depends on model capacity: smaller models benefit from broader coverage,
> while larger models respond better to stronger contrast in supervision.
> Furthermore, we propose hierarchical preference optimization, which adaptively
> weights training pairs based on intra-group reward gaps and inter-group
> diversity, enabling more efficient and stable learning. Extensive experiments
> across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR
> significantly improves perceptual quality and generalizes well to real-world
> benchmarks.

