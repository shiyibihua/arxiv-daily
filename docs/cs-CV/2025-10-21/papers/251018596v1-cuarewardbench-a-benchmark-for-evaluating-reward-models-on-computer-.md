---
layout: default
title: CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent
---

# CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent

**arXiv**: [2510.18596v1](https://arxiv.org/abs/2510.18596) | [PDF](https://arxiv.org/pdf/2510.18596.pdf)

**ä½œè€…**: Haojia Lin, Xiaoyu Tan, Yulei Qin, Zihan Xu, Yuchen Shi, Zongyi Li, Gang Li, Shaofei Cai, Siqi Cai, Chaoyou Fu, Ke Li, Xing Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCUARewardBenchåŸºå‡†ä»¥è¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å¥–åŠ±æ¨¡åž‹**

**å…³é”®è¯**: `è®¡ç®—æœºä½¿ç”¨ä»£ç†` `å¥–åŠ±æ¨¡åž‹è¯„ä¼°` `è½¨è¿¹æ•°æ®é›†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä¸€è‡´æç¤ºé›†æˆ` `è½¯ä»¶äº¤äº’ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºäºŽè„šæœ¬çš„è¯„ä¼°æ–¹æ³•å¯æ‰©å±•æ€§å·®ï¼Œæ— æ³•æä¾›é€æ­¥è¯„ä¼°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé¦–ä¸ªç»¼åˆåŸºå‡†ï¼Œæ”¯æŒç»“æžœå’Œè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹çš„è½¨è¿¹çº§ä¸Žæ­¥éª¤çº§è¯„ä¼°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡å¹¿æ³›å®žéªŒæ­ç¤ºå½“å‰æ¨¡åž‹å±€é™ï¼Œå¹¶æå‡ºUPEæ–¹æ³•æ˜¾è‘—æå‡å¯é æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Computer-using agents (CUAs) enable task completion through natural
> interaction with operating systems and software interfaces. While script-based
> verifiers are widely adopted for evaluation, they suffer from limited
> scalability and inability to provide step-wise assessment. Reward models offer
> promising alternatives, but their effectiveness on CUA evaluation remains
> largely underexplored. To address this gap, we present CUARewardBench,
> comprising four key contributions: (1) First-ever Comprehensive CUA Reward
> Benchmark: We introduce the first benchmark for evaluating both outcome reward
> models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
> assessment across trajectory-level and step-level evaluation. (2) Diverse,
> Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
> software categories and 7 agent architectures with varying performance levels
> (25.9%-50.8% success rates). All trajectories are expertly annotated through
> carefully designed protocols, with rigorous quality control to ensure
> reliability and practical applicability. (3) Comprehensive Analysis and
> Insights: Through extensive experiments across 7 vision-language models and 3
> prompt templates, we reveal critical limitations of current CUA RMs, including
> insufficient visual reasoning capabilities, knowledge deficiencies, and the
> superiority of general VLMs over specialized CUA models for reward evaluation.
> (4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
> comprehensive analysis, we propose UPE, a novel ensemble method that
> significantly enhances reward model reliability through strict unanimous voting
> and strategic prompt-template configurations. UPE achieves 89.8% precision and
> 93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
> outperforming single VLMs and traditional ensemble approaches.

