---
layout: default
title: Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs
---

# Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs

**arXiv**: [2510.18876v1](https://arxiv.org/abs/2510.18876) | [PDF](https://arxiv.org/pdf/2510.18876.pdf)

**ä½œè€…**: Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGrasp Any Regionä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨åŒºåŸŸçº§è§†è§‰ç†è§£ä¸­å¿½ç•¥å…¨å±€ä¸Šä¸‹æ–‡çš„é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡å‹` `åŒºåŸŸçº§è§†è§‰ç†è§£` `RoIç‰¹å¾å¯¹é½` `ç»„åˆæ¨ç†` `åŸºå‡†æµ‹è¯•` `é›¶æ ·æœ¬è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­éš¾ä»¥è¿›è¡Œç»†ç²’åº¦åˆ†æå’Œå¯¹è±¡é—´å…³ç³»å»ºæ¨¡ï¼Œç°æœ‰åŒºåŸŸçº§æ–¹æ³•å¿½è§†å…¨å±€ä¸Šä¸‹æ–‡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥RoIå¯¹é½ç‰¹å¾é‡æ”¾æŠ€æœ¯ï¼Œæ”¯æŒç²¾ç¡®æ„ŸçŸ¥ã€å¤šæç¤ºäº¤äº’å»ºæ¨¡å’Œç»„åˆæ¨ç†
3. å®éªŒæˆ–æ•ˆæœï¼šGAR-1Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ¨¡å‹ï¼ŒGAR-8Båœ¨é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) excel at holistic
> understanding, they struggle in capturing the dense world with complex scenes,
> requiring fine-grained analysis of intricate details and object
> inter-relationships. Region-level MLLMs have been a promising step. However,
> previous attempts are generally optimized to understand given regions in
> isolation, neglecting crucial global contexts. To address this, we introduce
> Grasp Any Region (GAR) for comprehen- sive region-level visual understanding.
> Empowered by an effective RoI-aligned feature replay technique, GAR supports
> (1) precise perception by leveraging necessary global contexts, and (2)
> modeling interactions between multiple prompts. Together, it then naturally
> achieves (3) advanced compositional reasoning to answer specific free-form
> questions about any region, shifting the paradigm from passive description to
> active dialogue. Moreover, we construct GAR-Bench, which not only provides a
> more accurate evaluation of single-region comprehension, but also, more
> importantly, measures interactions and complex reasoning across multiple
> regions. Extensive experiments have demonstrated that GAR-1B not only maintains
> the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5
> on DLC-Bench, but also excels at modeling relationships between multiple
> prompts with advanced comprehension capabilities, even surpassing InternVL3-78B
> on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms
> in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong
> capabilities can be easily transferred to videos.

