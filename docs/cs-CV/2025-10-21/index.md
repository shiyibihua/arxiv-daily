---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-21
---

# cs.CVï¼ˆ2025-10-21ï¼‰

ğŸ“Š å…± **7** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251018253v1-openinsgaussian-open-vocabulary-instance-gaussian-segmentation-with-.html">OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion</a></td>
  <td>æå‡ºOpenInsGaussianï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥è·¨è§†è§’èåˆå®ç°å¼€æ”¾è¯æ±‡å®ä¾‹é«˜æ–¯åˆ†å‰²ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18253v1" onclick="toggleFavorite(this, '2510.18253v1', 'OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251018244v1-blendclip-bridging-synthetic-and-real-domains-for-zero-shot-3d-objec.html">BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining</a></td>
  <td>BlendCLIPï¼šé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡¥æ¥åˆæˆä¸çœŸå®åŸŸï¼Œå®ç°é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18244v1" onclick="toggleFavorite(this, '2510.18244v1', 'BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251018262v1-uwbench-a-comprehensive-vision-language-benchmark-for-underwater-und.html">UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding</a></td>
  <td>UWBenchï¼šç”¨äºæ°´ä¸‹ç¯å¢ƒç†è§£çš„ç»¼åˆæ€§è§†è§‰-è¯­è¨€åŸºå‡†æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18262v1" onclick="toggleFavorite(this, '2510.18262v1', 'UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251018187v1-velocitynet-real-time-crowd-anomaly-detection-via-person-specific-ve.html">VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis</a></td>
  <td>VelocityNetï¼šåŸºäºä¸ªä½“é€Ÿåº¦åˆ†æçš„å®æ—¶äººç¾¤å¼‚å¸¸æ£€æµ‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18187v1" onclick="toggleFavorite(this, '2510.18187v1', 'VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/251018267v1-latent-info-and-low-dimensional-learning-for-human-mesh-recovery-and.html">Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization</a></td>
  <td>æå‡ºåŸºäºæ½œåœ¨ä¿¡æ¯å’Œä½ç»´å­¦ä¹ çš„äººä½“ç½‘æ ¼æ¢å¤ä¸å¹¶è¡Œä¼˜åŒ–æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18267v1" onclick="toggleFavorite(this, '2510.18267v1', 'Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251018256v1-hyperbolic-space-learning-method-leveraging-temporal-motion-priors-f.html">Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery</a></td>
  <td>æå‡ºä¸€ç§åˆ©ç”¨æ—¶åºè¿åŠ¨å…ˆéªŒçš„ hyperbolic ç©ºé—´å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºäººä½“ç½‘æ ¼é‡å»ºã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18256v1" onclick="toggleFavorite(this, '2510.18256v1', 'Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/251018214v2-vlsu-mapping-the-limits-of-joint-multimodal-understanding-for-ai-saf.html">VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</a></td>
  <td>VLSUï¼šæ„å»ºå¤šæ¨¡æ€AIå®‰å…¨è¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºè§†è§‰-è¯­è¨€è”åˆç†è§£çš„å±€é™æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.18214v2" onclick="toggleFavorite(this, '2510.18214v2', 'VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)