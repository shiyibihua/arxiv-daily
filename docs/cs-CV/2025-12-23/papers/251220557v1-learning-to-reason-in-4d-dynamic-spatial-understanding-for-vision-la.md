---
layout: default
title: Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models
---

# Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20557" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20557v1</a>
  <a href="https://arxiv.org/pdf/2512.20557.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20557v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20557v1', 'Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDSR Suiteå’Œå‡ ä½•é€‰æ‹©æ¨¡å—GSMï¼Œæå‡VLMåœ¨åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€ç©ºé—´æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `4Dæ„ŸçŸ¥` `å‡ ä½•é€‰æ‹©æ¨¡å—` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨åŠ¨æ€ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹å¤§è§„æ¨¡4Dæ„ŸçŸ¥çš„è®­ç»ƒæ•°æ®æ˜¯ä¸»è¦ç“¶é¢ˆã€‚
2. æå‡ºDSR Suiteï¼ŒåŒ…å«è‡ªåŠ¨ç”ŸæˆDSRé—®ç­”å¯¹çš„æµç¨‹ï¼Œä»¥åŠè½»é‡çº§çš„å‡ ä½•é€‰æ‹©æ¨¡å—GSMï¼Œç”¨äºå°†å‡ ä½•å…ˆéªŒçŸ¥è¯†èå…¥VLMã€‚
3. å®éªŒè¡¨æ˜ï¼Œå°†DSR-Trainå’ŒGSMé›†æˆåˆ°Qwen2.5-VL-7Bä¸­ï¼Œæ˜¾è‘—æå‡äº†å…¶åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨è§†é¢‘ç†è§£çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨é€šç”¨ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€ç©ºé—´æ¨ç†(DSR)æ–¹é¢ä»ç„¶è¾ƒå¼±ï¼Œå³æ¨ç†3Dç©ºé—´ä¸­ç‰©ä½“å‡ ä½•å’Œå…³ç³»éšæ—¶é—´çš„æ¼”å˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å¯æ‰©å±•çš„4Dæ„ŸçŸ¥è®­ç»ƒèµ„æºã€‚ä¸ºäº†å¼¥åˆæ•°æ®é›†ã€åŸºå‡†å’Œæ¨¡å‹æ–¹é¢çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSR Suiteã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œä»çœŸå®è§†é¢‘ä¸­ç”Ÿæˆç”¨äºDSRçš„å¤šé¡¹é€‰æ‹©é—®ç­”å¯¹ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè¯¥æµç¨‹æå–ä¸°å¯Œçš„å‡ ä½•å’Œè¿åŠ¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›¸æœºå§¿æ€ã€å±€éƒ¨ç‚¹äº‘ã€ç‰©ä½“æ©ç ã€æ–¹å‘å’Œ3Dè½¨è¿¹ã€‚è¿™äº›å‡ ä½•çº¿ç´¢ä½¿å¾—æ„å»ºç”¨äºå­¦ä¹ çš„DSR-Trainå’Œè¿›ä¸€æ­¥äººå·¥æ”¹è¿›çš„ç”¨äºè¯„ä¼°çš„DSR-Benchæˆä¸ºå¯èƒ½ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ•°æ®å¼ºè°ƒ(i)çœŸå®è§†é¢‘æ¥æºï¼Œ(ii)ç‰©ä½“å’Œåœºæ™¯çº§åˆ«çš„3Dè¦æ±‚ï¼Œ(iii)è§†ç‚¹è½¬æ¢ï¼Œ(iv)å¤šç‰©ä½“äº¤äº’ï¼Œä»¥åŠ(v)ç»†ç²’åº¦çš„ç¨‹åºæ€§ç­”æ¡ˆã€‚é™¤äº†æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„å‡ ä½•é€‰æ‹©æ¨¡å—(GSM)ï¼Œä»¥æ— ç¼åœ°å°†å‡ ä½•å…ˆéªŒçŸ¥è¯†é›†æˆåˆ°VLMä¸­ï¼Œè¯¥æ¨¡å—å°†é—®é¢˜è¯­ä¹‰æµ“ç¼©ï¼Œå¹¶ä»é¢„è®­ç»ƒçš„4Dé‡å»ºå…ˆéªŒçŸ¥è¯†ä¸­æå–é—®é¢˜ç›¸å…³çš„çŸ¥è¯†åˆ°ä¸€ç»„ç´§å‡‘çš„å‡ ä½•tokenä¸­ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„æå–é¿å…äº†ç”¨ä¸ç›¸å…³çš„çŸ¥è¯†æ·¹æ²¡æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œå°†DSR-Trainå’ŒGSMé›†æˆåˆ°Qwen2.5-VL-7Bä¸­æ˜¾è‘—å¢å¼ºäº†å…¶åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†åœ¨é€šç”¨è§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨ç†è§£é™æ€å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£å’Œæ¨ç†åŠ¨æ€3Dç©ºé—´ä¸­çš„ç‰©ä½“è¿åŠ¨å’Œå…³ç³»å˜åŒ–æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„4Dæ„ŸçŸ¥è®­ç»ƒæ•°æ®ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°å°†3Då‡ ä½•ä¿¡æ¯èå…¥åˆ°VLMä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–çš„æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œæ„å»ºå¤§è§„æ¨¡çš„DSRæ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªè½»é‡çº§çš„å‡ ä½•é€‰æ‹©æ¨¡å—(GSM)ï¼Œå°†é—®é¢˜ç›¸å…³çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†æå–å¹¶èå…¥åˆ°VLMä¸­ã€‚è¿™æ ·æ—¢è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œåˆé¿å…äº†ç”¨ä¸ç›¸å…³çš„å‡ ä½•ä¿¡æ¯æ·¹æ²¡æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šDSR Suiteå’Œå‡ ä½•é€‰æ‹©æ¨¡å—(GSM)ã€‚DSR Suiteè´Ÿè´£ç”Ÿæˆå¤§è§„æ¨¡çš„DSR-Trainå’ŒDSR-Benchæ•°æ®é›†ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå–è§†é¢‘ä¸­çš„å‡ ä½•å’Œè¿åŠ¨ä¿¡æ¯ã€‚GSMåˆ™è´Ÿè´£å°†é—®é¢˜è¯­ä¹‰å’Œ4Dé‡å»ºå…ˆéªŒçŸ¥è¯†èåˆï¼Œç”Ÿæˆå‡ ä½•tokenï¼Œå¹¶è¾“å…¥åˆ°VLMä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œå¯ä»¥ä»çœŸå®è§†é¢‘ä¸­ç”Ÿæˆå¤§è§„æ¨¡çš„DSRé—®ç­”å¯¹ï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚2) è®¾è®¡äº†è½»é‡çº§çš„å‡ ä½•é€‰æ‹©æ¨¡å—(GSM)ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†å‡ ä½•å…ˆéªŒçŸ¥è¯†èå…¥åˆ°VLMä¸­ï¼Œé¿å…äº†ä¿¡æ¯è¿‡è½½çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šDSR Suiteçš„æ•°æ®ç”Ÿæˆæµç¨‹ä¾èµ–äºè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºæå–ç›¸æœºå§¿æ€ã€ç‚¹äº‘ã€ç‰©ä½“æ©ç å’Œ3Dè½¨è¿¹çš„æ¨¡å‹ã€‚GSMæ¨¡å—çš„è®¾è®¡ç›®æ ‡æ˜¯è½»é‡çº§ï¼Œå› æ­¤é‡‡ç”¨äº†ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œç”¨äºé€‰æ‹©å’Œèåˆå‡ ä½•ç‰¹å¾ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°é—®é¢˜ç›¸å…³çš„å‡ ä½•ä¿¡æ¯ï¼Œå¹¶æŠ‘åˆ¶ä¸ç›¸å…³ä¿¡æ¯çš„å¹²æ‰°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20557v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20557v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20557v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°†DSR-Trainå’ŒGSMé›†æˆåˆ°Qwen2.5-VL-7Bä¸­ï¼Œæ˜¾è‘—å¢å¼ºäº†å…¶åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åœ¨DSR-Benchæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨é€šç”¨è§†é¢‘ç†è§£åŸºå‡†ä¸Šä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨æå‡åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæ²¡æœ‰ç‰ºç‰²æ¨¡å‹çš„é€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æ§ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡VLMå¯¹åŠ¨æ€ç©ºé—´çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®è§†é¢‘ä¸­çš„ç‰©ä½“è¿åŠ¨è½¨è¿¹ï¼Œé¢„æµ‹æœªæ¥çš„çŠ¶æ€ï¼Œä»è€Œåšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.

