---
layout: default
title: AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model
---

# AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20157" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20157v1</a>
  <a href="https://arxiv.org/pdf/2512.20157.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20157v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20157v1', 'AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sofian Chaybouti, Sanath Narayan, Yasser Dahou, PhÃºc H. LÃª Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**å¤‡æ³¨**: 17 pages, 8 figures, 11 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAMoEï¼Œä¸€ç§é«˜æ•ˆçš„Agglomerative Mixture-of-Expertsè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¤šæ•™å¸ˆè’¸é¦å®ç°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰åŸºç¡€æ¨¡å‹` `å¤šæ•™å¸ˆè’¸é¦` `çŸ¥è¯†è’¸é¦` `æ··åˆä¸“å®¶æ¨¡å‹` `æ•°æ®é‡‡æ ·` `è¡¨å¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹è®­ç»ƒæ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¯¹å¤šæ•™å¸ˆè’¸é¦çš„å­¦ä¹ åŠ¨æ€å’Œæ•°æ®æ•ˆç‡ç ”ç©¶ä¸è¶³ã€‚
2. AMoEé€šè¿‡éå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦ã€tokenå¹³è¡¡æ‰¹å¤„ç†å’Œåˆ†å±‚èšç±»é‡‡æ ·ç­‰æŠ€æœ¯ï¼Œæå‡å¤šæ•™å¸ˆè’¸é¦çš„æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAMoEåœ¨2äº¿å›¾åƒè¯­æ–™åº“OpenLVD200Mä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ•ˆç‡ï¼Œå¹¶å‘å¸ƒäº†è¯¥æ•°æ®é›†å’Œè’¸é¦æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†è§†è§‰åŸºç¡€æ¨¡å‹çš„å¤šæ•™å¸ˆè’¸é¦æ–¹æ³•ï¼Œå¹¶ç¡®å®šäº†åœ¨è¾ƒä½è®¡ç®—æˆæœ¬ä¸‹è¿›è¡Œè®­ç»ƒçš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬æå‡ºäº†Agglomerative Mixture-of-Expertsè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆAMoEï¼‰ï¼Œå®ƒåŒæ—¶ä»SigLIP2å’ŒDINOv3ä¸­æå–çŸ¥è¯†åˆ°ä¸€ä¸ªæ··åˆä¸“å®¶å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬è¯æ˜äº†ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬çš„éå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦æŸå¤±ä¿ç•™äº†æ¯ä¸ªæ•™å¸ˆçš„å‡ ä½•å±æ€§ï¼ŒåŒæ—¶å®ç°äº†æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ï¼›ï¼ˆ2ï¼‰tokenå¹³è¡¡æ‰¹å¤„ç†å°†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒæ‰“åŒ…æˆå…·æœ‰ç»Ÿä¸€tokené¢„ç®—çš„åºåˆ—ï¼Œç¨³å®šäº†è·¨åˆ†è¾¨ç‡çš„è¡¨ç¤ºå­¦ä¹ ï¼Œä¸”ä¸ç‰ºç‰²æ€§èƒ½ï¼›ï¼ˆ3ï¼‰è®­ç»ƒæ•°æ®çš„åˆ†å±‚èšç±»å’Œé‡‡æ ·ï¼ˆé€šå¸¸ç”¨äºè‡ªç›‘ç£å­¦ä¹ ï¼‰æ˜¾è‘—æé«˜äº†å¤šæ•™å¸ˆè’¸é¦çš„æ ·æœ¬æ•ˆç‡ï¼Œä¼˜äºéšæœºé‡‡æ ·ã€‚é€šè¿‡ç»“åˆè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†OpenLVD200Mï¼Œä¸€ä¸ª2äº¿å›¾åƒè¯­æ–™åº“ï¼Œå±•ç¤ºäº†å¤šæ•™å¸ˆè’¸é¦çš„å“è¶Šæ•ˆç‡ã€‚æˆ‘ä»¬å‘å¸ƒäº†OpenLVD200Må’Œè’¸é¦æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åŸºäºå¤šæ•™å¸ˆè’¸é¦çš„æ–¹æ³•ï¼Œé¢ä¸´ç€è®¡ç®—èµ„æºéœ€æ±‚é«˜ã€æ•°æ®åˆ©ç”¨ç‡ä½çš„é—®é¢˜ã€‚å¦‚ä½•é™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿è¯ç”šè‡³æå‡æ¨¡å‹æ€§èƒ½ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨çŸ¥è¯†è¿ç§»è¿‡ç¨‹ä¸­å¯èƒ½æ— æ³•å……åˆ†ä¿ç•™å„ä¸ªæ•™å¸ˆæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¸”å¯¹ä¸åŒåˆ†è¾¨ç‡å›¾åƒçš„å¤„ç†ä¸å¤Ÿé«˜æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ç§é«˜æ•ˆçš„å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œå°†å¤šä¸ªé¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„çŸ¥è¯†æ•´åˆåˆ°ä¸€ä¸ªè½»é‡çº§çš„æ··åˆä¸“å®¶æ¨¡å‹ä¸­ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°ã€æ•°æ®é‡‡æ ·ç­–ç•¥å’Œæ‰¹å¤„ç†æ–¹æ³•ï¼Œä¼˜åŒ–çŸ¥è¯†è¿ç§»è¿‡ç¨‹ï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œä»è€Œé™ä½è®­ç»ƒæˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAMoEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ•™å¸ˆæ¨¡å‹ï¼ˆSigLIP2å’ŒDINOv3ï¼‰ï¼›2) æ··åˆä¸“å®¶å­¦ç”Ÿæ¨¡å‹ï¼›3) éå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦æŸå¤±ï¼›4) tokenå¹³è¡¡æ‰¹å¤„ç†ï¼›5) åˆ†å±‚èšç±»é‡‡æ ·ã€‚é¦–å…ˆï¼Œåˆ©ç”¨åˆ†å±‚èšç±»é‡‡æ ·ä»è®­ç»ƒæ•°æ®é›†ä¸­é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æ ·æœ¬ã€‚ç„¶åï¼Œå°†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒé€šè¿‡tokenå¹³è¡¡æ‰¹å¤„ç†æ„å»ºç»Ÿä¸€tokené¢„ç®—çš„åºåˆ—ã€‚æ¥ç€ï¼Œåˆ©ç”¨éå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦æŸå¤±ï¼Œå°†å¤šæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°æ··åˆä¸“å®¶å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†éå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦æŸå¤±ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å„ä¸ªæ•™å¸ˆæ¨¡å‹çš„å‡ ä½•å±æ€§ï¼Œå®ç°æ›´æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ï¼›2) å¼•å…¥äº†tokenå¹³è¡¡æ‰¹å¤„ç†ï¼Œè§£å†³äº†ä¸åŒåˆ†è¾¨ç‡å›¾åƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼›3) å°†åˆ†å±‚èšç±»é‡‡æ ·åº”ç”¨äºå¤šæ•™å¸ˆè’¸é¦ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šéå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦æŸå¤±çš„è®¾è®¡è€ƒè™‘äº†ä¸åŒæ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œé€šè¿‡ä¸å¯¹ç§°çš„æ–¹å¼ä¿ç•™å„è‡ªçš„ä¼˜åŠ¿ã€‚Tokenå¹³è¡¡æ‰¹å¤„ç†é€šè¿‡è°ƒæ•´ä¸åŒåˆ†è¾¨ç‡å›¾åƒçš„æ•°é‡ï¼Œä½¿å¾—æ¯ä¸ªæ‰¹æ¬¡çš„tokenæ•°é‡ä¿æŒä¸€è‡´ï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚åˆ†å±‚èšç±»é‡‡æ ·é€šè¿‡å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œèšç±»ï¼Œå¹¶ä»æ¯ä¸ªç°‡ä¸­é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬ï¼Œå‡å°‘äº†æ•°æ®å†—ä½™ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20157v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20157v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20157v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

AMoEåœ¨OpenLVD200Mæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç»“åˆéå¯¹ç§°å…³ç³»çŸ¥è¯†è’¸é¦ã€tokenå¹³è¡¡æ‰¹å¤„ç†å’Œåˆ†å±‚èšç±»é‡‡æ ·ï¼ŒAMoEèƒ½å¤Ÿä»¥æ›´é«˜çš„æ•ˆç‡è¿›è¡Œå¤šæ•™å¸ˆè’¸é¦ã€‚ç›¸æ¯”äºéšæœºé‡‡æ ·ï¼Œåˆ†å±‚èšç±»é‡‡æ ·æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AMoEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ã€‚é€šè¿‡é«˜æ•ˆçš„å¤šæ•™å¸ˆè’¸é¦ï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹éƒ¨ç½²ï¼Œå¹¶ä¸ºèµ„æºå—é™çš„è®¾å¤‡æä¾›é«˜æ€§èƒ½çš„è§†è§‰æ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæ¨åŠ¨è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.

