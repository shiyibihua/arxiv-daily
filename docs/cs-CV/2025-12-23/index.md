---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-23
---

# cs.CVï¼ˆ2025-12-23ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251220362v1-craft-continuous-reasoning-and-agentic-feedback-tuning-for-multimoda.html">CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</a></td>
  <td>CRAFTï¼šç”¨äºå¤šæ¨¡æ€æ–‡å›¾ç”Ÿæˆçš„æŒç»­æ¨ç†å’ŒAgentåé¦ˆè°ƒæ•´æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20362v1" data-paper-url="./papers/251220362v1-craft-continuous-reasoning-and-agentic-feedback-tuning-for-multimoda.html" onclick="toggleFavorite(this, '2512.20362v1', 'CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251220561v1-flashvlm-text-guided-visual-token-selection-for-large-multimodal-mod.html">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</a></td>
  <td>FlashVLMï¼šæ–‡æœ¬å¼•å¯¼çš„è§†è§‰Tokené€‰æ‹©ï¼Œæå‡å¤§æ¨¡å‹å¤šæ¨¡æ€æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20561v1" data-paper-url="./papers/251220561v1-flashvlm-text-guided-visual-token-selection-for-large-multimodal-mod.html" onclick="toggleFavorite(this, '2512.20561v1', 'FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251220026v1-mapi-gnn-multi-activation-plane-interaction-graph-neural-network-for.html">MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis</a></td>
  <td>MAPI-GNNï¼šç”¨äºå¤šæ¨¡æ€åŒ»å­¦è¯Šæ–­çš„å¤šæ¿€æ´»å¹³é¢äº¤äº’å›¾ç¥ç»ç½‘ç»œ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20026v1" data-paper-url="./papers/251220026v1-mapi-gnn-multi-activation-plane-interaction-graph-neural-network-for.html" onclick="toggleFavorite(this, '2512.20026v1', 'MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251220617v1-spatialtree-how-spatial-abilities-branch-out-in-mllms.html">SpatialTree: How Spatial Abilities Branch Out in MLLMs</a></td>
  <td>SpatialTreeï¼šæ„å»ºå¤šæ¨¡æ€LLMç©ºé—´èƒ½åŠ›åˆ†å±‚è¯„ä¼°ä½“ç³»ä¸æå‡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20617v1" data-paper-url="./papers/251220617v1-spatialtree-how-spatial-abilities-branch-out-in-mllms.html" onclick="toggleFavorite(this, '2512.20617v1', 'SpatialTree: How Spatial Abilities Branch Out in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251220557v1-learning-to-reason-in-4d-dynamic-spatial-understanding-for-vision-la.html">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</a></td>
  <td>æå‡ºDSR Suiteå’Œå‡ ä½•é€‰æ‹©æ¨¡å—GSMï¼Œæå‡VLMåœ¨åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20557v1" data-paper-url="./papers/251220557v1-learning-to-reason-in-4d-dynamic-spatial-understanding-for-vision-la.html" onclick="toggleFavorite(this, '2512.20557v1', 'Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251220174v1-towards-natural-language-based-document-image-retrieval-new-dataset-.html">Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</a></td>
  <td>æå‡ºNL-DIRåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè§£å†³è‡ªç„¶è¯­è¨€æè¿°çš„æ–‡æ¡£å›¾åƒæ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20174v1" data-paper-url="./papers/251220174v1-towards-natural-language-based-document-image-retrieval-new-dataset-.html" onclick="toggleFavorite(this, '2512.20174v1', 'Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251220042v1-beyond-vision-contextually-enriched-image-captioning-with-multi-moda.html">Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºçš„å›¾åƒæè¿°æ–¹æ³•ï¼Œæå‡äº‹ä»¶èƒŒæ™¯å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20042v1" data-paper-url="./papers/251220042v1-beyond-vision-contextually-enriched-image-captioning-with-multi-moda.html" onclick="toggleFavorite(this, '2512.20042v1', 'Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251220011v1-pavesync-a-unified-and-comprehensive-dataset-for-pavement-distress-a.html">PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification</a></td>
  <td>PaveSyncï¼šç»Ÿä¸€å…¨é¢çš„è·¯é¢ç—…å®³åˆ†æä¸åˆ†ç±»æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">zero-shot transfer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20011v1" data-paper-url="./papers/251220011v1-pavesync-a-unified-and-comprehensive-dataset-for-pavement-distress-a.html" onclick="toggleFavorite(this, '2512.20011v1', 'PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/251220501v1-bridging-modalities-and-transferring-knowledge-enhanced-multimodal-u.html">Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å¯¹é½ã€ç¿»è¯‘ã€èåˆä¸è¿ç§»æ–¹æ³•ï¼Œæå‡å¤æ‚è¾“å…¥ç†è§£ä¸è¯†åˆ«èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">egocentric</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20501v1" data-paper-url="./papers/251220501v1-bridging-modalities-and-transferring-knowledge-enhanced-multimodal-u.html" onclick="toggleFavorite(this, '2512.20501v1', 'Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251220157v1-amoe-agglomerative-mixture-of-experts-vision-foundation-model.html">AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</a></td>
  <td>æå‡ºAMoEï¼Œä¸€ç§é«˜æ•ˆçš„Agglomerative Mixture-of-Expertsè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¤šæ•™å¸ˆè’¸é¦å®ç°ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20157v1" data-paper-url="./papers/251220157v1-amoe-agglomerative-mixture-of-experts-vision-foundation-model.html" onclick="toggleFavorite(this, '2512.20157v1', 'AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251220615v1-active-intelligence-in-video-avatars-via-closed-loop-world-modeling.html">Active Intelligence in Video Avatars via Closed-loop World Modeling</a></td>
  <td>æå‡ºORCAæ¡†æ¶ï¼Œé€šè¿‡é—­ç¯ä¸–ç•Œå»ºæ¨¡å®ç°è§†é¢‘åŒ–èº«çš„ä¸»åŠ¨æ™ºèƒ½</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20615v1" data-paper-url="./papers/251220615v1-active-intelligence-in-video-avatars-via-closed-loop-world-modeling.html" onclick="toggleFavorite(this, '2512.20615v1', 'Active Intelligence in Video Avatars via Closed-loop World Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251220128v1-millimamba-specular-aware-human-pose-estimation-via-dual-mmwave-rada.html">milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</a></td>
  <td>milliMambaï¼šåŸºäºåŒæ¯«ç±³æ³¢é›·è¾¾å’Œå¤šå¸§Mambaèåˆçš„æŠ—é•œé¢åå°„äººä½“å§¿æ€ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20128v1" data-paper-url="./papers/251220128v1-millimamba-specular-aware-human-pose-estimation-via-dual-mmwave-rada.html" onclick="toggleFavorite(this, '2512.20128v1', 'milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251220117v1-ddavs-disentangled-audio-semantics-and-delayed-bidirectional-alignme.html">DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation</a></td>
  <td>DDAVSï¼šè§£è€¦éŸ³é¢‘è¯­ä¹‰ä¸å»¶è¿ŸåŒå‘å¯¹é½ï¼Œç”¨äºéŸ³è§†é¢‘åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20117v1" data-paper-url="./papers/251220117v1-ddavs-disentangled-audio-semantics-and-delayed-bidirectional-alignme.html" onclick="toggleFavorite(this, '2512.20117v1', 'DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251220148v1-enhancing-annotations-for-5d-apple-pose-estimation-through-3d-gaussi.html">Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)</a></td>
  <td>åˆ©ç”¨3Dé«˜æ–¯æº…å°„å¢å¼º5Dè‹¹æœå§¿æ€ä¼°è®¡çš„æ ‡æ³¨æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20148v1" data-paper-url="./papers/251220148v1-enhancing-annotations-for-5d-apple-pose-estimation-through-3d-gaussi.html" onclick="toggleFavorite(this, '2512.20148v1', 'Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251220531v1-sirenpose-dynamic-scene-reconstruction-via-geometric-supervision.html">SirenPose: Dynamic Scene Reconstruction via Geometric Supervision</a></td>
  <td>SirenPoseï¼šé€šè¿‡å‡ ä½•ç›‘ç£å®ç°åŠ¨æ€åœºæ™¯çš„ç²¾ç¡®é‡å»ºä¸æ—¶åºä¸€è‡´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span> <span class="paper-tag">physically plausible</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20531v1" data-paper-url="./papers/251220531v1-sirenpose-dynamic-scene-reconstruction-via-geometric-supervision.html" onclick="toggleFavorite(this, '2512.20531v1', 'SirenPose: Dynamic Scene Reconstruction via Geometric Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251220538v1-alignpose-generalizable-6d-pose-estimation-via-multi-view-feature-me.html">AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</a></td>
  <td>AlignPoseï¼šé€šè¿‡å¤šè§†è§’ç‰¹å¾åº¦é‡å¯¹é½å®ç°é€šç”¨6Dä½å§¿ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20538v1" data-paper-url="./papers/251220538v1-alignpose-generalizable-6d-pose-estimation-via-multi-view-feature-me.html" onclick="toggleFavorite(this, '2512.20538v1', 'AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251220377v1-smartsplat-feature-smart-gaussians-for-scalable-compression-of-ultra.html">SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</a></td>
  <td>SmartSplatï¼šæå‡ºç‰¹å¾æ„ŸçŸ¥çš„GSå›¾åƒå‹ç¼©æ¡†æ¶ï¼Œå®ç°è¶…é«˜åˆ†è¾¨ç‡å›¾åƒçš„é«˜æ•ˆå‹ç¼©ä¸é«˜è´¨é‡é‡å»ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20377v1" data-paper-url="./papers/251220377v1-smartsplat-feature-smart-gaussians-for-scalable-compression-of-ultra.html" onclick="toggleFavorite(this, '2512.20377v1', 'SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251220257v1-ladle-mm-limited-annotation-based-detector-with-learned-ensembles-fo.html">LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation</a></td>
  <td>æå‡ºLADLE-MMï¼Œä¸€ç§åŸºäºæœ‰é™æ ‡æ³¨å’Œé›†æˆå­¦ä¹ çš„å¤šæ¨¡æ€ä¿¡æ¯æ£€æµ‹å™¨ï¼Œé€‚ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20257v1" data-paper-url="./papers/251220257v1-ladle-mm-limited-annotation-based-detector-with-learned-ensembles-fo.html" onclick="toggleFavorite(this, '2512.20257v1', 'LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251220129v1-dreamcrafter-immersive-editing-of-3d-radiance-fields-through-flexibl.html">Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</a></td>
  <td>Dreamcrafterï¼šé€šè¿‡çµæ´»çš„ç”Ÿæˆå¼è¾“å…¥è¾“å‡ºå®ç°æ²‰æµ¸å¼3Dè¾å°„åœºç¼–è¾‘</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20129v1" data-paper-url="./papers/251220129v1-dreamcrafter-immersive-editing-of-3d-radiance-fields-through-flexibl.html" onclick="toggleFavorite(this, '2512.20129v1', 'Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251220563v1-lead-minimizing-learner-expert-asymmetry-in-end-to-end-driving.html">LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</a></td>
  <td>LEADï¼šæœ€å°åŒ–ç«¯åˆ°ç«¯é©¾é©¶ä¸­å­¦ä¹ è€…-ä¸“å®¶ä¸å¯¹ç§°æ€§ï¼Œæå‡CARLAæ¨¡æ‹Ÿå™¨é©¾é©¶æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">imitation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20563v1" data-paper-url="./papers/251220563v1-lead-minimizing-learner-expert-asymmetry-in-end-to-end-driving.html" onclick="toggleFavorite(this, '2512.20563v1', 'LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251220296v1-tavid-text-driven-audio-visual-interactive-dialogue-generation.html">TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</a></td>
  <td>æå‡ºTAVIDï¼Œé€šè¿‡è·¨æ¨¡æ€æ˜ å°„å®ç°æ–‡æœ¬é©±åŠ¨çš„äº¤äº’å¼éŸ³è§†é¢‘å¯¹è¯ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">dyadic interaction</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20296v1" data-paper-url="./papers/251220296v1-tavid-text-driven-audio-visual-interactive-dialogue-generation.html" onclick="toggleFavorite(this, '2512.20296v1', 'TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/251220025v1-a-contextual-analysis-of-driver-facing-and-dual-view-video-inputs-fo.html">A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments</a></td>
  <td>ç ”ç©¶åŒè§†è§’è§†é¢‘è¾“å…¥å¯¹è‡ªç„¶é©¾é©¶ç¯å¢ƒä¸‹åˆ†å¿ƒæ£€æµ‹çš„å½±å“ï¼Œå¼ºè°ƒèåˆè®¾è®¡çš„é‡è¦æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20025v1" data-paper-url="./papers/251220025v1-a-contextual-analysis-of-driver-facing-and-dual-view-video-inputs-fo.html" onclick="toggleFavorite(this, '2512.20025v1', 'A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251220409v1-detach-decomposed-spatio-temporal-alignment-for-exocentric-video-and.html">DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning</a></td>
  <td>æå‡ºDETACHæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦æ—¶ç©ºå¯¹é½è§£å†³å¤–ä¸­å¿ƒè§†é¢‘ä¸ç¯å¢ƒä¼ æ„Ÿå™¨èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20409v1" data-paper-url="./papers/251220409v1-detach-decomposed-spatio-temporal-alignment-for-exocentric-video-and.html" onclick="toggleFavorite(this, '2512.20409v1', 'DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)