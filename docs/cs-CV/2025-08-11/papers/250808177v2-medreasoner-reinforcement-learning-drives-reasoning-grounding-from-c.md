---
layout: default
title: MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision
---

# MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08177" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08177v2</a>
  <a href="https://arxiv.org/pdf/2508.08177.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08177v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08177v2', 'MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhonghao Yan, Muxi Diao, Yuxuan Yang, Ruoyan Jing, Jiayuan Xu, Kaizhou Zhang, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-12-11)

**å¤‡æ³¨**: AAAI2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMedReasonerä»¥è§£å†³åŒ»ç–—å½±åƒä¸­ROIç²¾å‡†å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»ç–—å½±åƒ` `ROIå®šä½` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ä¸´åºŠæ¨ç†` `æ•°æ®é›†æ„å»º` `åƒç´ çº§ç²¾åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒ»ç–—å½±åƒå®šä½æ–¹æ³•ä¾èµ–æ˜¾å¼ç©ºé—´æç¤ºï¼Œéš¾ä»¥å¤„ç†éšå¼æŸ¥è¯¢ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ä»»åŠ¡ï¼Œå¹¶å¼•å…¥MedReasoneræ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæå‡äº†å®šä½ç²¾åº¦ã€‚
3. MedReasoneråœ¨U-MRG-14Kæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å¯¹æ–°å‹ä¸´åºŠæŸ¥è¯¢çš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®å®šä½åŒ»ç–—å½±åƒä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰å¯¹è¯Šæ–­å’Œæ²»ç–—è§„åˆ’è‡³å…³é‡è¦ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ï¼Œä½†å½“å‰çš„åŒ»ç–—å®šä½æµç¨‹ä»ä¾èµ–äºå¸¦æœ‰æ˜¾å¼ç©ºé—´æç¤ºçš„ç›‘ç£å¾®è°ƒï¼Œéš¾ä»¥å¤„ç†ä¸´åºŠå®è·µä¸­å¸¸è§çš„éšå¼æŸ¥è¯¢ã€‚æœ¬æ–‡çš„ä¸‰é¡¹æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼šå®šä¹‰ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ï¼Œæå‡ºä¸€ç§éœ€è¦ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½çš„æ–°ä»»åŠ¡ï¼›å‘å¸ƒåŒ…å«14Kæ ·æœ¬çš„U-MRG-14Kæ•°æ®é›†ï¼Œæ¶µç›–éšå¼ä¸´åºŠæŸ¥è¯¢å’Œæ¨ç†è½¨è¿¹ï¼›å¼•å…¥MedReasonerï¼Œä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–MLLMæ¨ç†å™¨ï¼ŒåŒæ—¶å°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºæ©è†œçš„åˆ†å‰²ä¸“å®¶ä¿æŒå†»ç»“ï¼Œä»è€Œå®ç°æ ¼å¼å’Œå‡†ç¡®æ€§å¥–åŠ±çš„å¯¹é½ã€‚MedReasoneråœ¨U-MRG-14Kä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å¯¹æœªè§ä¸´åºŠæŸ¥è¯¢çš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¯è§£é‡ŠåŒ»ç–—å®šä½ä¸­çš„é‡è¦æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŒ»ç–—å½±åƒä¸­ROIçš„ç²¾å‡†å®šä½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜¾å¼çš„ç©ºé—´æç¤ºï¼Œéš¾ä»¥åº”å¯¹ä¸´åºŠä¸­å¸¸è§çš„éšå¼æŸ¥è¯¢ï¼Œå¯¼è‡´å®šä½æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ä»»åŠ¡ï¼Œç»“åˆä¸´åºŠæ¨ç†ä¸åƒç´ çº§å®šä½ã€‚é€šè¿‡å¼•å…¥MedReasoneræ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†éšå¼æŸ¥è¯¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMedReasoneræ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šä¸€ä¸ªæ˜¯ä¼˜åŒ–çš„MLLMæ¨ç†å™¨ï¼Œå¦ä¸€ä¸ªæ˜¯å†»ç»“çš„åˆ†å‰²ä¸“å®¶ã€‚æ¨ç†å™¨è´Ÿè´£å¤„ç†ä¸´åºŠæ¨ç†ï¼Œè€Œåˆ†å‰²ä¸“å®¶å°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºæ©è†œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ¨ç†ä¸åˆ†å‰²è¿‡ç¨‹åˆ†ç¦»ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†å™¨ï¼Œå®ç°äº†æ›´é«˜çš„å®šä½ç²¾åº¦å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMedReasoneråœ¨å¤„ç†éšå¼æŸ¥è¯¢æ—¶è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–æ¨ç†å™¨çš„è¾“å‡ºï¼ŒåŒ…æ‹¬æ ¼å¼å’Œå‡†ç¡®æ€§å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†U-MRG-14Kçš„æ„å»ºä¹Ÿä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„æ ·æœ¬å’Œå¤šæ ·çš„ä¸´åºŠåœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MedReasoneråœ¨U-MRG-14Kæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨éšå¼ä¸´åºŠæŸ¥è¯¢çš„å¤„ç†ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼Œå±•ç°äº†å¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—å®šä½ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å½±åƒåˆ†æã€è¾…åŠ©è¯Šæ–­ç³»ç»Ÿå’Œä¸ªæ€§åŒ–æ²»ç–—è§„åˆ’ã€‚MedReasonerçš„åˆ›æ–°æ¡†æ¶èƒ½å¤Ÿæå‡åŒ»ç–—å½±åƒçš„å¤„ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„ä¸´åºŠåº”ç”¨ä»·å€¼ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨åŒ»ç–—AIæŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.

