---
layout: default
title: AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning
---

# AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07626" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.07626v1</a>
  <a href="https://arxiv.org/pdf/2508.07626.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07626v1', 'AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dejie Yang, Zijing Zhao, Yang Liu

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-11

**Â§áÊ≥®**: Accepted by ICCV2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AR-VRM‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫ËßÜËßâÊìçÊéß‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÊú∫Âô®‰∫∫ÊìçÊéß` `Á±ªÊØîÊé®ÁêÜ` `‰∫∫Á±ªÂä®‰ΩúÊ®°‰ªø` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂÖ≥ÈîÆÁÇπÈ¢ÑÊµã` `Êï∞ÊçÆÁ®ÄÁº∫` `Êú∫Âô®‰∫∫ÊäÄÊúØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâÊú∫Âô®‰∫∫ÊìçÊéßÊñπÊ≥ï‰æùËµñ‰∫éÂ§öÊ®°ÊÄÅÊï∞ÊçÆÔºå‰∏îÂú®Êï∞ÊçÆ‰∏çË∂≥Êó∂Ë°®Áé∞Âá∫ÊúâÈôêÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫AR-VRMÔºåÈÄöËøáÊ®°‰ªø‰∫∫Á±ªÂä®‰ΩúËßÜÈ¢ë‰∏≠ÁöÑÊâãÈÉ®ÂÖ≥ÈîÆÁÇπÔºåÊòæÂºèÂ≠¶‰π†‰∫∫Á±ªÂä®‰ΩúÁü•ËØÜÔºåÂ¢ûÂº∫Êú∫Âô®‰∫∫ÊìçÊéßËÉΩÂäõ„ÄÇ
3. AR-VRMÂú®CALVINÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæóÈ¢ÜÂÖàË°®Áé∞ÔºåÂ∞§ÂÖ∂Âú®Â∞ëÊ†∑Êú¨Âú∫ÊôØ‰∏ãÔºåÁõ∏ËæÉ‰∫é‰πãÂâçÁöÑÊñπÊ≥ïÊúâÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÊú∫Âô®‰∫∫ÊìçÊéßÔºàVRMÔºâÊó®Âú®‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊ†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÂíåËßÜËßâËßÇÂØüËøõË°åÊìç‰ΩúÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÊòÇË¥µÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆ„ÄÇ‰∏∫Âº•Ë°•Êú∫Âô®‰∫∫Êï∞ÊçÆÁöÑ‰∏çË∂≥ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïAR-VRMÔºåÈÄöËøáÊ®°‰ªø‰∫∫Á±ªÂä®‰ΩúËßÜÈ¢ë‰∏≠ÁöÑÊâãÈÉ®ÂÖ≥ÈîÆÁÇπÔºåÊòæÂºèÂú∞Â≠¶‰π†‰∫∫Á±ªÂä®‰ΩúÁü•ËØÜ„ÄÇËØ•ÊñπÊ≥ïÂú®Êú∫Âô®‰∫∫Êï∞ÊçÆÁöÑÂæÆË∞ÉÈò∂ÊÆµÔºåÈÄöËøáÊ£ÄÁ¥¢ÊâßË°åÁõ∏‰ººÊìç‰ΩúÁöÑ‰∫∫Á±ªËßÜÈ¢ëÔºåÂª∫Á´ã‰∫∫Á±ªÊâãÈÉ®ÂÖ≥ÈîÆÁÇπ‰∏éÊú∫Âô®‰∫∫ÁªÑ‰ª∂‰πãÈó¥ÁöÑÁ±ªÊØîÊé®ÁêÜÊò†Â∞Ñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAR-VRMÂú®CALVINÂü∫ÂáÜÊµãËØïÂíåÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®Â∞ëÊ†∑Êú¨Âú∫ÊôØ‰∏ãÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÊú∫Âô®‰∫∫ÊìçÊéß‰∏≠Áî±‰∫éÊï∞ÊçÆÁ®ÄÁº∫ÂØºËá¥ÁöÑÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ§ö‰æùËµñ‰∫éÁΩëÁªúÊï∞ÊçÆÔºåÁº∫‰πè‰∏éÊú∫Âô®‰∫∫‰ªªÂä°ÁöÑÁõ¥Êé•ÂÖ≥ËÅîÔºåÂØºËá¥Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAR-VRMÈÄöËøá‰ªéÂ§ßËßÑÊ®°‰∫∫Á±ªÂä®‰ΩúËßÜÈ¢ë‰∏≠ÊòæÂºèÂ≠¶‰π†ÔºåÊ®°‰ªø‰∫∫Á±ªÂä®‰ΩúÁöÑÊâãÈÉ®ÂÖ≥ÈîÆÁÇπÔºåËøõËÄåÊèêÂçáÊú∫Âô®‰∫∫Âú®ËßÜËßâÊìçÊéß‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•ËÆæËÆ°‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°åÂ§çÊùÇÁöÑÊìç‰Ωú‰ªªÂä°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAR-VRMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÊòØÂÖ≥ÈîÆÁÇπËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†‰∫∫Á±ªÂä®‰ΩúÁü•ËØÜÔºõÂÖ∂Ê¨°ÊòØÂú®Êú∫Âô®‰∫∫Êï∞ÊçÆ‰∏äËøõË°åÂæÆË∞ÉÔºåÈÄöËøáÁ±ªÊØîÊé®ÁêÜÊò†Â∞Ñ‰∫∫Á±ªÂä®‰Ωú‰∏éÊú∫Âô®‰∫∫ÁªÑ‰ª∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•Á±ªÊØîÊé®ÁêÜÊú∫Âà∂Ôºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Áº∫‰πèË∂≥Â§üÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÄöËøáÊ®°‰ªø‰∫∫Á±ªÂä®‰ΩúÊù•ÊèêÂçáÂÖ∂ÊìçÊéßËÉΩÂäõ„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÈöêÂºèÂ≠¶‰π†ÊñπÂºèÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏äÔºåÈááÁî®‰∫Ü‰∏ìÊ≥®‰∫éÂä®‰ΩúÂÖ≥ÈîÆÁÇπÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁ°Æ‰øùÊ®°ÂûãËÉΩÂ§üÂáÜÁ°ÆÈ¢ÑÊµã‰∫∫Á±ªÊâãÈÉ®ÂÖ≥ÈîÆÁÇπ„ÄÇÊ≠§Â§ñÔºåÊ£ÄÁ¥¢Áõ∏‰ººÊìç‰ΩúÁöÑ‰∫∫Á±ªËßÜÈ¢ëÁöÑÁ≠ñÁï•‰πü‰∏∫ÂæÆË∞ÉÈò∂ÊÆµÊèê‰æõ‰∫ÜÊúâÊïàÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®CALVINÂü∫ÂáÜÊµãËØï‰∏≠ÔºåAR-VRMÁöÑË°®Áé∞Ë∂ÖËøá‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Â∞ëÊ†∑Êú¨Âú∫ÊôØ‰∏ãÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞ÊòæËëóÁöÑÊ∞¥Âπ≥ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êï∞ÊçÆÁ®ÄÁº∫ÊÉÖÂÜµ‰∏ãÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AR-VRMÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂåÖÊã¨‰∫∫Êú∫Âçè‰Ωú„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫ÂØπ‰∫∫Á±ªÂä®‰ΩúÁöÑÁêÜËß£ËÉΩÂäõÔºåËØ•ÊäÄÊúØÂèØ‰ª•Âú®ÂÆûÈôÖÊìç‰Ωú‰∏≠ÂÆûÁé∞Êõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄßÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï‰∏éÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multi-modal data. To compensate for the deficiency of robot data, existing approaches have employed vision-language pretraining with large-scale data. However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under insufficient robot data. In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introducing Visual Robot Manipulation with Analogical Reasoning (AR-VRM). To acquire action knowledge explicitly from human action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical observations , and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components. Taking advantage of focusing on action keypoints instead of irrelevant visual cues, our method achieves leading performance on the CALVIN benchmark {and real-world experiments}. In few-shot scenarios, our AR-VRM outperforms previous methods by large margins , underscoring the effectiveness of explicitly imitating human actions under data scarcity.

