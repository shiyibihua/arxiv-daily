---
layout: default
title: Towards Scalable Training for Handwritten Mathematical Expression Recognition
---

# Towards Scalable Training for Handwritten Mathematical Expression Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09220" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09220v3</a>
  <a href="https://arxiv.org/pdf/2508.09220.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09220v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09220v3', 'Towards Scalable Training for Handwritten Mathematical Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyang Li, Jiaqing Li, Jialun Cao, Zongyuan Yang, Yongping Xiong

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-09-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTexTellerä»¥è§£å†³æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹å†™æ•°å­¦è¯†åˆ«` `æ•°æ®ç”Ÿæˆ` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹è®­ç»ƒ` `LaTeXæ¸²æŸ“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«ï¼ˆHMERï¼‰é¢ä¸´æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸»è¦ç”±äºæ‰‹åŠ¨æ ‡æ³¨çš„é«˜æˆæœ¬å’Œå¤æ‚æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ç”ŸæˆLaTeXæ¸²æŸ“å…¬å¼ä¸æœ‰é™æ‰‹å†™å…¬å¼ç»“åˆï¼Œæ„å»ºäº†Tex80Mæ•°æ®é›†ã€‚
3. TexTelleræ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¾¾åˆ°äº†å‡ ä¹æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ¨åŠ¨äº†HMERé¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹åŸºç¡€æ¨¡å‹é€šè¿‡åœ¨æµ·é‡æ•°æ®é›†ä¸Šè¿›è¡Œå¯æ‰©å±•è®­ç»ƒå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œæ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«ï¼ˆHMERï¼‰é¢†åŸŸç”±äºæ‰‹åŠ¨æ ‡æ³¨è¿‡ç¨‹ç¹çä¸”æˆæœ¬é«˜æ˜‚ï¼Œæ•°æ®ç¨€ç¼ºé—®é¢˜ä¾ç„¶ä¸¥é‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†æœ‰é™çš„æ‰‹å†™å…¬å¼ä¸å¤§è§„æ¨¡LaTeXæ¸²æŸ“å…¬å¼ç›¸ç»“åˆï¼Œå¼€å‘å‡ºå¯æ‰©å±•çš„æ•°æ®å¼•æ“ä»¥ç”Ÿæˆå¤æ‚ä¸”ä¸€è‡´çš„LaTeXåºåˆ—ã€‚åŸºäºæ­¤å¼•æ“ï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…¬å¼æ•°æ®é›†Tex80Mï¼ŒåŒ…å«è¶…è¿‡8000ä¸‡é«˜è´¨é‡è®­ç»ƒå®ä¾‹ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡è®­ç»ƒçš„HMERæ¨¡å‹TexTellerï¼Œé€šè¿‡å°†Tex80Mä¸ç›¸å¯¹è¾ƒå°çš„HMEæ•°æ®é›†æ··åˆè®­ç»ƒï¼ŒTexTelleråœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºæ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒå®Œæ•´æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç åº“ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«ï¼ˆHMERï¼‰é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®ï¼Œå¯¼è‡´è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡å¼€å‘å¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œå°†æœ‰é™çš„æ‰‹å†™å…¬å¼ä¸å¤§è§„æ¨¡çš„LaTeXæ¸²æŸ“å…¬å¼ç»“åˆï¼Œä»è€Œç”Ÿæˆä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£ç”ŸæˆTex80Mæ•°æ®é›†ï¼Œè€Œæ¨¡å‹è®­ç»ƒæ¨¡å—åˆ™ä½¿ç”¨Tex80Mä¸å°è§„æ¨¡HMEæ•°æ®é›†è¿›è¡Œæ··åˆè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ„å»ºäº†Tex80Mæ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æ‰‹å†™æ•°å­¦å…¬å¼æ•°æ®é›†ï¼Œä¸”é€šè¿‡ç”ŸæˆLaTeXåºåˆ—è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨å¤æ‚æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«ä¸­çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TexTelleræ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“è€Œè¨€ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿ï¼Œè¯†åˆ«å‡†ç¡®ç‡æå‡äº†è¶…è¿‡15%ã€‚æ­¤å¤–ï¼ŒTex80Mæ•°æ®é›†çš„æ„å»ºä¸ºåç»­ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æ•™è‚²ã€ç§‘å­¦è®¡ç®—å’Œè‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼çš„è¯†åˆ«å‡†ç¡®æ€§ï¼Œå¯ä»¥ä¿ƒè¿›æ™ºèƒ½æ•™è‚²å·¥å…·çš„å‘å±•ï¼Œæå‡å­¦ä¹ æ•ˆç‡ï¼Œå¹¶åœ¨ç§‘ç ”æ–‡çŒ®çš„è‡ªåŠ¨åŒ–å¤„ç†ä¸Šæä¾›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large foundation models have achieved significant performance gains through scalable training on massive datasets. However, the field of \textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression \textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily due to the arduous and costly process of manual annotation. To bridge this gap, we propose a novel method integrating limited handwritten formulas with large-scale LaTeX-rendered formulas by developing a scalable data engine to generate complex and consistent LaTeX sequences. With this engine, we built the largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80 million high-quality training instances. Then we propose \texttt{TexTeller}, the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a relatively small HME dataset. The expansive training dataset and our refined pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA) performance across nearly all benchmarks. To advance the field, we will openly release our complete model, entire dataset, and full codebase, enabling further research building upon our contributions.

