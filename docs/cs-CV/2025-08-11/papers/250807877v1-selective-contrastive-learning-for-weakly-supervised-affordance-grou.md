---
layout: default
title: Selective Contrastive Learning for Weakly Supervised Affordance Grounding
---

# Selective Contrastive Learning for Weakly Supervised Affordance Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07877" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07877v1</a>
  <a href="https://arxiv.org/pdf/2508.07877.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07877v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07877v1', 'Selective Contrastive Learning for Weakly Supervised Affordance Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: WonJun Moon, Hyun Seok Seong, Jae-Pil Heo

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: Accepted to ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€‰æ‹©æ€§å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³å¼±ç›‘ç£æ•ˆèƒ½å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `å¼±ç›‘ç£å­¦ä¹ ` `æ•ˆèƒ½å®šä½` `å¯¹æ¯”å­¦ä¹ ` `å¤šè§†è§’å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼±ç›‘ç£æ•ˆèƒ½å®šä½æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆ†ç±»ï¼Œéš¾ä»¥æœ‰æ•ˆåŒºåˆ†ä¸æ•ˆèƒ½ç›¸å…³çš„éƒ¨ä½ä¸èƒŒæ™¯ã€‚
2. æœ¬æ–‡æå‡ºé€‰æ‹©æ€§åŸå‹å’Œåƒç´ å¯¹æ¯”ç›®æ ‡ï¼Œé€šè¿‡é€‚åº”æ€§å­¦ä¹ æ•ˆèƒ½ç›¸å…³çº¿ç´¢ï¼Œæå‡äº†æ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ä¸åŒè§†è§’ä¸‹çš„æ•ˆèƒ½å®šä½ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡æ•ˆæœæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³å¼±ç›‘ç£æ•ˆèƒ½å®šä½ï¼ˆWSAGï¼‰ä¸­çš„å…³é”®é—®é¢˜ï¼Œå³å¦‚ä½•åœ¨ç¼ºä¹åƒç´ çº§æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®è¯†åˆ«ä¸ç‰¹å®šåŠ¨ä½œç›¸å…³çš„ç‰©ä½“éƒ¨ä½ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆ†ç±»ï¼Œå¾€å¾€å¿½è§†äº†ä¸æ•ˆèƒ½ç›¸å…³çš„ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥é€‰æ‹©æ€§åŸå‹å’Œåƒç´ å¯¹æ¯”ç›®æ ‡ï¼Œé€‚åº”æ€§åœ°å­¦ä¹ ä¸æ•ˆèƒ½ç›¸å…³çš„çº¿ç´¢ï¼Œä»è€Œæœ‰æ•ˆåŒºåˆ†æ•ˆèƒ½ç›¸å…³åŒºåŸŸä¸èƒŒæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼±ç›‘ç£æ•ˆèƒ½å®šä½ï¼ˆWSAGï¼‰ä¸­çš„å…³é”®é—®é¢˜ï¼Œå³åœ¨ç¼ºä¹åƒç´ çº§æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•å‡†ç¡®è¯†åˆ«ä¸ç‰¹å®šåŠ¨ä½œç›¸å…³çš„ç‰©ä½“éƒ¨ä½ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆ†ç±»ï¼Œå¾€å¾€å¿½è§†äº†ä¸æ•ˆèƒ½ç›¸å…³çš„ç‰¹å¾ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåŒºåˆ†æ•ˆèƒ½ç›¸å…³åŒºåŸŸä¸èƒŒæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥é€‰æ‹©æ€§åŸå‹å’Œåƒç´ å¯¹æ¯”ç›®æ ‡ï¼Œé€‚åº”æ€§åœ°å­¦ä¹ ä¸æ•ˆèƒ½ç›¸å…³çš„çº¿ç´¢ã€‚è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨ç‰©ä½“çš„æ•´ä½“ç‰¹å¾ï¼Œè¿˜èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„ä¿¡æ¯ç²’åº¦ï¼Œçµæ´»è°ƒæ•´å­¦ä¹ ç­–ç•¥ï¼Œä»è€Œæå‡æ•ˆèƒ½å®šä½çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡CLIPæ¨¡å‹è¯†åˆ«ä¸åŠ¨ä½œç›¸å…³çš„ç‰©ä½“ï¼›å…¶æ¬¡ï¼Œäº¤å‰å‚è€ƒä¸åŒè§†è§’ä¸‹å‘ç°çš„ç‰©ä½“ï¼ŒæŒ–æ˜æ¯ä¸ªè§†è§’ä¸­çš„ç²¾ç¡®éƒ¨ä½æ•ˆèƒ½çº¿ç´¢ã€‚æ•´ä¸ªæµç¨‹å¼ºè°ƒäº†ä»èƒŒæ™¯ä¸­åŒºåˆ†å‡ºæ•ˆèƒ½ç›¸å…³åŒºåŸŸçš„é‡è¦æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†é€‰æ‹©æ€§å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒè§†è§’ä¸‹è‡ªé€‚åº”åœ°å­¦ä¹ ä¸æ•ˆèƒ½ç›¸å…³çš„ç‰¹å¾ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å•ä¸€åˆ†ç±»æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚çš„æ•ˆèƒ½ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚æ¬¡çš„æŸå¤±å‡½æ•°è®¾è®¡ï¼Œä»¥ç¡®ä¿å¯¹æ•ˆèƒ½ç›¸å…³åŒºåŸŸçš„å‡†ç¡®å­¦ä¹ ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„ä¸Šç»“åˆäº†åŸå‹å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¼±ç›‘ç£æ•ˆèƒ½å®šä½æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œæ¨¡å‹çš„æ•ˆèƒ½å®šä½å‡†ç¡®ç‡æå‡äº†çº¦15%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„è¡¨ç°ä¸€è‡´æ€§ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æœºå™¨äººäº¤äº’ã€æ™ºèƒ½å®¶å±…å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œå‡†ç¡®è¯†åˆ«ç‰©ä½“çš„æ•ˆèƒ½å¯ä»¥æ˜¾è‘—æå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¹¿æ³›çš„æ™ºèƒ½ç³»ç»Ÿå‘å±•ï¼Œæå‡å…¶è‡ªä¸»å­¦ä¹ å’Œé€‚åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.

