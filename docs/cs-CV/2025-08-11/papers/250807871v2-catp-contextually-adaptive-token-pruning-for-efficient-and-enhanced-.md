---
layout: default
title: CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning
---

# CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07871v2</a>
  <a href="https://arxiv.org/pdf/2508.07871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07871v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07871v2', 'CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanshu Li, Jianjiang Yang, Zhennan Shen, Ligong Han, Haoyan Xu, Ruixiang Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-12-09)

**å¤‡æ³¨**: 14 pages, 12 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCATPä»¥è§£å†³å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å›¾åƒä»¤ç‰Œå†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å›¾åƒä»¤ç‰Œå‰ªæ` `ä¸Šä¸‹æ–‡è‡ªé€‚åº”` `æ¨ç†æ•ˆç‡` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒä»¤ç‰Œå‰ªææ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•å›¾åƒä»»åŠ¡ï¼Œæœªèƒ½æœ‰æ•ˆåº”å¯¹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„é«˜å†—ä½™æ€§å’Œæ•ˆç‡éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºçš„ä¸Šä¸‹æ–‡è‡ªé€‚åº”ä»¤ç‰Œå‰ªæï¼ˆCATPï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ¸è¿›å‰ªæï¼Œä¼˜åŒ–å¤šæ¨¡æ€ICLä¸­çš„å›¾åƒä»¤ç‰Œä½¿ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCATPåœ¨å»é™¤77.8%å›¾åƒä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡0.6%ï¼Œå¹¶ä¸”æ¨ç†å»¶è¿Ÿå‡å°‘10.78%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å°†æ¯ä¸ªè¾“å…¥å›¾åƒè½¬æ¢ä¸ºå¤§é‡ä»¤ç‰Œï¼Œå¯¼è‡´å›¾åƒä»¤ç‰Œå†—ä½™ä¸¥é‡ã€‚è™½ç„¶è¿™æå‡äº†è§†è§‰æ„ŸçŸ¥ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨ç†æˆæœ¬ã€‚ç°æœ‰çš„å›¾åƒä»¤ç‰Œå‰ªææ–¹æ³•ä¸»è¦é’ˆå¯¹å•å›¾åƒä»»åŠ¡ï¼Œå¿½è§†äº†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­çš„å†—ä½™é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸Šä¸‹æ–‡è‡ªé€‚åº”ä»¤ç‰Œå‰ªæï¼ˆCATPï¼‰ï¼Œä¸€ç§æ— è®­ç»ƒçš„å‰ªææ–¹æ³•ï¼Œé’ˆå¯¹å¤šæ¨¡æ€ICLè¿›è¡Œä¼˜åŒ–ã€‚CATPé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„æ¸è¿›å‰ªæï¼Œå……åˆ†åæ˜ è¾“å…¥åºåˆ—ä¸­çš„å¤æ‚è·¨æ¨¡æ€äº¤äº’ã€‚åœ¨å»é™¤77.8%çš„å›¾åƒä»¤ç‰Œåï¼ŒCATPåœ¨å››ä¸ªLVLMå’Œå…«ä¸ªåŸºå‡†ä¸Šå®ç°äº†å¹³å‡0.6%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ¨ç†å»¶è¿Ÿå¹³å‡å‡å°‘10.78%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å›¾åƒä»¤ç‰Œçš„å†—ä½™é—®é¢˜ã€‚ç°æœ‰å‰ªææ–¹æ³•åœ¨æ­¤åœºæ™¯ä¸‹åº”ç”¨æ—¶ï¼Œå¾€å¾€å¯¼è‡´æ˜¾è‘—çš„å‡†ç¡®ç‡ä¸‹é™ï¼Œæ— æ³•æ»¡è¶³é«˜æ•ˆæ¨ç†çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCATPæ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„æ–¹å¼è¿›è¡Œä»¤ç‰Œå‰ªæï¼Œè®¾è®¡äº†ä¸¤ä¸ªé˜¶æ®µçš„æ¸è¿›å‰ªæç­–ç•¥ï¼Œä»¥å……åˆ†è€ƒè™‘è¾“å…¥åºåˆ—ä¸­å¤æ‚çš„è·¨æ¨¡æ€äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCATPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºåˆæ­¥å‰ªæï¼Œé€šè¿‡åˆ†æä»¤ç‰Œçš„é‡è¦æ€§è¿›è¡Œç­›é€‰ï¼›ç¬¬äºŒé˜¶æ®µåˆ™è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œç¡®ä¿ä¿ç•™å¯¹æ¨ç†æœ€æœ‰ä»·å€¼çš„ä»¤ç‰Œã€‚

**å…³é”®åˆ›æ–°**ï¼šCATPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è®­ç»ƒæ— å…³çš„å‰ªæç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€ICLåœºæ™¯ä¸­æœ‰æ•ˆå‡å°‘å†—ä½™ä»¤ç‰Œï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å‰ªææ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCATPåœ¨å‰ªæè¿‡ç¨‹ä¸­é‡‡ç”¨äº†åŠ¨æ€é˜ˆå€¼è®¾å®šï¼Œä»¥é€‚åº”ä¸åŒè¾“å…¥çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‰ªæåçš„æ€§èƒ½ä¸æ•ˆç‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†æ—¶çš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CATPåœ¨å»é™¤77.8%å›¾åƒä»¤ç‰Œåï¼Œå¹³å‡æ€§èƒ½æå‡0.6%ï¼Œå¹¶ä¸”æ¨ç†å»¶è¿Ÿå‡å°‘10.78%ã€‚è¿™ä¸€ç»“æœæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¿«é€Ÿé¢†åŸŸé€‚åº”çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡å’Œç¨³å®šæ€§ï¼ŒCATPä¸ºæœªæ¥çš„å¤šæ¨¡æ€å­¦ä¹ æä¾›äº†åšå®çš„åŸºç¡€ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern large vision-language models (LVLMs) convert each input image into a large set of tokens that far outnumber the text tokens. Although this improves visual perception, it also introduces severe image token redundancy. Because image tokens contain sparse information, many contribute little to reasoning but greatly increase inference cost. Recent image token pruning methods address this issue by identifying important tokens and removing the rest. These methods improve efficiency with only small performance drops. However, most of them focus on single-image tasks and overlook multimodal in-context learning (ICL), where redundancy is higher and efficiency is more important. Redundant tokens weaken the advantage of multimodal ICL for rapid domain adaptation and lead to unstable performance. When existing pruning methods are applied in this setting, they cause large accuracy drops, which exposes a clear gap and the need for new approaches. To address this, we propose Contextually Adaptive Token Pruning (CATP), a training-free pruning method designed for multimodal ICL. CATP uses two stages of progressive pruning that fully reflect the complex cross-modal interactions in the input sequence. After removing 77.8% of the image tokens, CATP achieves an average performance gain of 0.6% over the vanilla model on four LVLMs and eight benchmarks, clearly outperforming all baselines. At the same time, it improves efficiency by reducing inference latency by an average of 10.78%. CATP strengthens the practical value of multimodal ICL and lays the foundation for future progress in interleaved image-text settings.

