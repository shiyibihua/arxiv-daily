---
layout: default
title: Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models
---

# Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07996" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07996v1</a>
  <a href="https://arxiv.org/pdf/2508.07996.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07996v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07996v1', 'Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thinesh Thiyakesan Ponbagavathi, Chengzheng Yang, Alina Roitberg

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProGraDä»¥è§£å†³ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä¸­çš„ç¤¾äº¤è¡Œä¸ºç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¾¤ä½“æ´»åŠ¨æ£€æµ‹` `ç¤¾äº¤è¡Œä¸ºç†è§£` `è§†è§‰åŸºç¡€æ¨¡å‹` `Transformer` `å¯è§£é‡Šæ€§` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ¶æ„ï¼Œä¸”åœ¨ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æ—¶æœªèƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºçš„ProGraDæ–¹æ³•é€šè¿‡å­¦ä¹ ç¾¤ä½“æç¤ºå’ŒGroupContext Transformerå®ç°å¯¹ç¤¾äº¤é…ç½®çš„å¼•å¯¼å’Œæ¨ç†ã€‚
3. åœ¨ä¸¤ä¸ªGADåŸºå‡†ä¸Šï¼ŒProGraDåœ¨å¤æ‚å¤šç¾¤ä½“åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡å¹…åº¦è¾¾åˆ°6.5%å’Œ8.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ï¼ˆGADï¼‰æ¶‰åŠåœ¨è§†é¢‘ä¸­è¯†åˆ«ç¤¾äº¤ç¾¤ä½“åŠå…¶é›†ä½“è¡Œä¸ºã€‚å°½ç®¡è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å¦‚DinoV2æä¾›äº†ä¼˜ç§€çš„ç‰¹å¾ï¼Œä½†å…¶ä¸»è¦åœ¨ç‰©ä½“ä¸­å¿ƒæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå°šæœªå……åˆ†æ¢ç´¢ç”¨äºå»ºæ¨¡ç¾¤ä½“åŠ¨æ€ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæç¤ºçš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹æ–¹æ³•ï¼ˆProGraDï¼‰ï¼Œé€šè¿‡å¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºå¼•å¯¼VFMå…³æ³¨ç¤¾äº¤é…ç½®ï¼Œä»¥åŠè½»é‡çº§çš„ä¸¤å±‚GroupContext Transformeræ¨æ–­æ¼”å‘˜ä¸ç¾¤ä½“çš„å…³è”åŠé›†ä½“è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªGADåŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚çš„å¤šç¾¤ä½“åœºæ™¯ä¸­ï¼ŒProGraDæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”ç”Ÿæˆå¯è§£é‡Šçš„æ³¨æ„åŠ›å›¾ï¼Œæä¾›äº†å¯¹æ¼”å‘˜ä¸ç¾¤ä½“æ¨ç†çš„æ´å¯Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä¸­çš„ç¤¾äº¤è¡Œä¸ºç†è§£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰ç¾¤ä½“åŠ¨æ€ï¼Œå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šProGraDæ–¹æ³•é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºï¼ŒæŒ‡å¯¼è§†è§‰åŸºç¡€æ¨¡å‹å…³æ³¨ç¤¾äº¤é…ç½®ï¼ŒåŒæ—¶åˆ©ç”¨è½»é‡çº§çš„GroupContext Transformeræ¨æ–­æ¼”å‘˜ä¸ç¾¤ä½“çš„å…³ç³»ï¼Œå¢å¼ºæ¨¡å‹çš„ç¾¤ä½“æ„è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºæ¨¡å—å’ŒGroupContext Transformerã€‚å‰è€…ç”¨äºå¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ï¼Œåè€…è´Ÿè´£æ¨æ–­ç¾¤ä½“è¡Œä¸ºå’Œæ¼”å‘˜å…³è”ã€‚

**å…³é”®åˆ›æ–°**ï¼šProGraDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†å¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºä¸è½»é‡çº§Transformerç»“æ„ï¼Œæ˜¾è‘—æå‡äº†å¯¹å¤æ‚å¤šç¾¤ä½“åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºçµæ´»å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒProGraDä½¿ç”¨äº†ä»…10Mçš„å¯è®­ç»ƒå‚æ•°ï¼Œä¸”é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°ä¼˜åŒ–ç¾¤ä½“æç¤ºçš„å­¦ä¹ æ•ˆæœï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šç¾¤ä½“åœºæ™¯ä¸­å…·å¤‡è¾ƒå¼ºçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜ç”Ÿæˆå¯è§£é‡Šçš„æ³¨æ„åŠ›å›¾ï¼Œå¸®åŠ©ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸¤ä¸ªGADåŸºå‡†ä¸Šï¼ŒProGraDæ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå°¤å…¶åœ¨å¤æ‚çš„å¤šç¾¤ä½“åœºæ™¯ä¸­ï¼ŒGroup mAP@1.0æå‡äº†6.5%ï¼ŒGroup mAP@0.5æå‡äº†8.2%ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨10Mçš„å¯è®­ç»ƒå‚æ•°ï¼Œæ˜¾ç¤ºå‡ºé«˜æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨ç¤¾äº¤è¡Œä¸ºåˆ†æã€è§†é¢‘ç›‘æ§ã€æ™ºèƒ½äº¤é€šå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡ç¾¤ä½“æ´»åŠ¨æ£€æµ‹çš„å‡†ç¡®æ€§ï¼ŒProGraDèƒ½å¤Ÿä¸ºç¤¾ä¼šè¡Œä¸ºç†è§£æä¾›æ›´æ·±å…¥çš„æ´å¯Ÿï¼Œä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Group Activity Detection (GAD) involves recognizing social groups and their collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2, offer excellent features, but are pretrained primarily on object-centric data and remain underexplored for modeling group dynamics. While they are a promising alternative to highly task-specific GAD architectures that require full fine-tuning, our initial investigation reveals that simply swapping CNN backbones used in these methods with VFMs brings little gain, underscoring the need for structured, group-aware reasoning on top.
>   We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method that bridges this gap through 1) learnable group prompts to guide the VFM attention toward social configurations, and 2) a lightweight two-layer GroupContext Transformer that infers actor-group associations and collective behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which features multiple concurrent social groups, and Social-CAD, which focuses on single-group interactions. While we surpass state-of-the-art in both settings, our method is especially effective in complex multi-group scenarios, where we yield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only 10M trainable parameters. Furthermore, our experiments reveal that ProGraD produces interpretable attention maps, offering insights into actor-group reasoning. Code and models will be released.

