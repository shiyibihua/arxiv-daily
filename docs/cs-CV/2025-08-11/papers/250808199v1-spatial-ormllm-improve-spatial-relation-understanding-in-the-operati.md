---
layout: default
title: Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model
---

# Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08199" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08199v1</a>
  <a href="https://arxiv.org/pdf/2508.08199.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08199v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08199v1', 'Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peiqi He, Zhenhao Zhang, Yixiang Zhang, Xiongjun Zhao, Shaoliang Peng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpatial-ORMLLMä»¥è§£å†³æ‰‹æœ¯å®¤ç©ºé—´å…³ç³»ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´å…³ç³»ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `3Dç©ºé—´æ¨ç†` `æ‰‹æœ¯å®¤åº”ç”¨` `ç‰¹å¾èåˆ` `åŒ»å­¦å†³ç­–æ”¯æŒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤šæ¨¡æ€æ•°æ®è¿›è¡Œç©ºé—´å…³ç³»å­¦ä¹ ï¼Œä½†åœ¨æ‰‹æœ¯å®¤ä¸­ç¼ºä¹è¶³å¤Ÿçš„3Dæ•°æ®ï¼Œå¯¼è‡´ç»†èŠ‚æ•æ‰ä¸è¶³ã€‚
2. Spatial-ORMLLMé€šè¿‡ä»…ä½¿ç”¨RGBæ¨¡æ€ï¼Œç»“åˆç©ºé—´å¢å¼ºç‰¹å¾èåˆæ¨¡å—ï¼Œè¿›è¡Œ3Dç©ºé—´æ¨ç†ï¼Œè§£å†³äº†æ•°æ®è·å–çš„å›°éš¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpatial-ORMLLMåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆé€‚åº”æ–°çš„æ‰‹æœ¯åœºæ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ‰‹æœ¯å®¤ä¸­ï¼Œç²¾ç¡®çš„ç©ºé—´å»ºæ¨¡å¯¹è®¸å¤šä¸´åºŠä»»åŠ¡è‡³å…³é‡è¦ï¼Œæ”¯æŒæœ¯ä¸­æ„è¯†ã€å±é™©è§„é¿å’Œå¤–ç§‘å†³ç­–ã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œæ½œåœ¨ç©ºé—´å¯¹é½ï¼Œä½†å¿½è§†äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„3Dèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Spatial-ORMLLMï¼Œè¿™æ˜¯é¦–ä¸ªä»…ä½¿ç”¨RGBæ¨¡æ€è¿›è¡Œ3Dç©ºé—´æ¨ç†çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ¨æ–­ä½“ç§¯å’Œè¯­ä¹‰çº¿ç´¢ï¼Œæ”¯æŒä¸‹æ¸¸åŒ»å­¦ä»»åŠ¡ã€‚Spatial-ORMLLMç»“åˆäº†ç©ºé—´å¢å¼ºç‰¹å¾èåˆæ¨¡å—ï¼Œå°†2Dæ¨¡æ€è¾“å…¥ä¸æå–çš„3Dç©ºé—´çŸ¥è¯†æ•´åˆï¼Œå½¢æˆå¼ºå¤§çš„ç©ºé—´å’Œæ–‡æœ¬ç‰¹å¾ç»„åˆï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤šä¸ªåŸºå‡†ä¸´åºŠæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„æ‰‹æœ¯åœºæ™¯å’Œä¸‹æ¸¸ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰‹æœ¯å®¤ä¸­ç©ºé—´å…³ç³»ç†è§£çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¼ºä¹å¤šæ¨¡æ€3Dæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ— æ³•æ•æ‰å¤æ‚åœºæ™¯ä¸­çš„ç»†èŠ‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSpatial-ORMLLMçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨RGBæ¨¡æ€è¿›è¡Œ3Dç©ºé—´æ¨ç†ï¼Œé€šè¿‡ç©ºé—´å¢å¼ºç‰¹å¾èåˆæ¨¡å—ï¼Œå°†2Dè¾“å…¥ä¸3Dç©ºé—´çŸ¥è¯†ç»“åˆï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„ç©ºé—´ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹é‡‡ç”¨ç»Ÿä¸€çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç©ºé—´å¢å¼ºç‰¹å¾èåˆå—å’Œè§†è§‰å¡”ï¼Œå‰è€…æ•´åˆ2Då’Œ3Dç‰¹å¾ï¼Œåè€…è´Ÿè´£å¤„ç†è§†è§‰ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šSpatial-ORMLLMçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–ä¸“å®¶æ ‡æ³¨æˆ–ä¼ æ„Ÿå™¨è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œå¼ºå¤§çš„3Dåœºæ™¯æ¨ç†ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­ï¼Œç‰¹å¾èåˆæ¨¡å—é‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–2Då’Œ3Dç‰¹å¾çš„ç»“åˆï¼ŒåŒæ—¶ä½¿ç”¨äº†é€‚åˆç©ºé—´æ¨ç†çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†ä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpatial-ORMLLMåœ¨3Dç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡15%ã€‚è¯¥æ¨¡å‹åœ¨æœªè§è¿‡çš„æ‰‹æœ¯åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ‰‹æœ¯å®¤çš„å®æ—¶ç›‘æ§ã€æ‰‹æœ¯å†³ç­–æ”¯æŒç³»ç»Ÿä»¥åŠåŒ»å­¦æ•™è‚²ç­‰ã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„ç©ºé—´ç†è§£ï¼ŒSpatial-ORMLLMèƒ½å¤Ÿå¸®åŠ©å¤–ç§‘åŒ»ç”Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´å¥½çš„å†³ç­–ï¼Œæå‡æ‰‹æœ¯å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ‰©å±•åˆ°å…¶ä»–åŒ»ç–—åœºæ™¯ï¼Œæ¨åŠ¨æ™ºèƒ½åŒ»ç–—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Precise spatial modeling in the operating room (OR) is foundational to many clinical tasks, supporting intraoperative awareness, hazard avoidance, and surgical decision-making. While existing approaches leverage large-scale multimodal datasets for latent-space alignment to implicitly learn spatial relationships, they overlook the 3D capabilities of MLLMs. However, this approach raises two issues: (1) Operating rooms typically lack multiple video and audio sensors, making multimodal 3D data difficult to obtain; (2) Training solely on readily available 2D data fails to capture fine-grained details in complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first large vision-language model for 3D spatial reasoning in operating rooms using only RGB modality to infer volumetric and semantic cues, enabling downstream medical tasks with detailed and holistic spatial context. Spatial-ORMLLM incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D modality inputs with rich 3D spatial knowledge extracted by the estimation algorithm and then feeds the combined features into the visual tower. By employing a unified end-to-end MLLM framework, it combines powerful spatial features with textual features to deliver robust 3D scene reasoning without any additional expert annotations or sensor inputs. Experiments on multiple benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves state-of-the-art performance and generalizes robustly to previously unseen surgical scenarios and downstream tasks.

