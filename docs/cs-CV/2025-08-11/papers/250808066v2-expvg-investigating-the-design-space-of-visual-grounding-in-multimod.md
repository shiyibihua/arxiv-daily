---
layout: default
title: ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model
---

# ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08066" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08066v2</a>
  <a href="https://arxiv.org/pdf/2508.08066.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08066v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08066v2', 'ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weitai Kang, Weiming Zhuang, Zhizhong Li, Yan Yan, Lingjuan Lyu

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-08-19)

**å¤‡æ³¨**: 8 pages for the main paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºExpVGä»¥ç³»ç»Ÿç ”ç©¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰å®šä½` `LLaVA-1.5` `æ¶ˆèç ”ç©¶` `è®¾è®¡é€‰æ‹©` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­å­˜åœ¨è®¾è®¡é€‰æ‹©ä¸ä¸€è‡´å’Œç¼ºä¹ç³»ç»ŸéªŒè¯çš„é—®é¢˜ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹ä¸åŒè§†è§‰å®šä½èŒƒå¼çš„æ¢ç´¢å’Œå¯¹åŸºç¡€æ•°æ®è®¾è®¡çš„æ¶ˆèç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„ç ”ç©¶æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æ¨¡å‹åœ¨RefCOCO/+/gæ•°æ®é›†ä¸Šåˆ†åˆ«æå‡äº†+5.6% / +6.9% / +7.0%çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»†ç²’åº¦å¤šæ¨¡æ€èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶æ—¥ç›Šé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰å®šä½ï¼ˆVGï¼‰é—®é¢˜ä¸Šã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒMLLMsä»¥è§£å†³VGæ—¶ï¼Œé‡‡ç”¨äº†ä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œç¼ºä¹ç³»ç»ŸéªŒè¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¯¹å½±å“MLLMs VGæ€§èƒ½çš„å¤šç§è®¾è®¡é€‰æ‹©è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œä½¿ç”¨LLaVA-1.5è¿›è¡Œåˆ†æï¼Œæ¢ç´¢ä¸åŒçš„è§†è§‰å®šä½èŒƒå¼ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶ä¼˜åŒ–VGä»»åŠ¡çš„å¾®è°ƒè®¾è®¡ã€‚æœ€ç»ˆï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºåœ¨RefCOCO/+/gä¸Šåˆ†åˆ«æå‡äº†+5.6% / +6.9% / +7.0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è®¾è®¡é€‰æ‹©ä¸ä¸€è‡´å’Œç¼ºä¹ç³»ç»ŸéªŒè¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ²¡æœ‰ç³»ç»Ÿæ€§åœ°è¯„ä¼°ä¸åŒè®¾è®¡å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¯¼è‡´æ•ˆæœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡å¯¹ä¸åŒè§†è§‰å®šä½èŒƒå¼çš„æ¢ç´¢ï¼Œè¯†åˆ«å‡ºæœ€æœ‰æ•ˆçš„è®¾è®¡ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶ä¼˜åŒ–å¾®è°ƒè¿‡ç¨‹ï¼Œä»¥æé«˜æ¨¡å‹åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨LLaVA-1.5ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œåˆ†æä¸åŒçš„è§†è§‰å®šä½è®¾è®¡é€‰æ‹©ï¼Œä¸»è¦åŒ…æ‹¬è§†è§‰ä¿¡æ¯çš„å¤„ç†æ–¹å¼å’Œæ•°æ®é›†çš„æ„å»ºã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬è®¾è®¡é€‰æ‹©çš„è¯„ä¼°ã€æ¶ˆèå®éªŒçš„å®æ–½ä»¥åŠæ€§èƒ½çš„æ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†æäº†å¤šç§è®¾è®¡é€‰æ‹©å¯¹è§†è§‰å®šä½æ€§èƒ½çš„å½±å“ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶ä¸­çš„ç©ºç™½ï¼Œå¹¶ä¸ºåç»­ç ”ç©¶æä¾›äº†å®è¯ä¾æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†é’ˆå¯¹è§†è§‰å®šä½ä»»åŠ¡çš„ç‰¹å®šæŸå¤±å‡½æ•°å’Œæ•°æ®å¢å¼ºç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†è§†è§‰ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¼˜åŒ–åçš„æ¨¡å‹åœ¨RefCOCO/+/gæ•°æ®é›†ä¸Šåˆ†åˆ«æå‡äº†+5.6% / +6.9% / +7.0%çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºLLaVA-1.5æ¨¡å‹ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†æ‰€æå‡ºè®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©è¿™äº›ç³»ç»Ÿæ›´å‡†ç¡®åœ°ç†è§£å’Œå¤„ç†è§†è§‰ä¿¡æ¯ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ï¼Œè¯¥ç ”ç©¶çš„æˆæœæœ‰æœ›æ¨åŠ¨æ›´å¤æ‚çš„è§†è§‰ç†è§£ä»»åŠ¡çš„å®ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

