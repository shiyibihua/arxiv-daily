---
layout: default
title: Reinforcement Learning for Large Model: A Survey
---

# Reinforcement Learning for Large Model: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08189" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08189v3</a>
  <a href="https://arxiv.org/pdf/2508.08189.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08189v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08189v3', 'Reinforcement Learning for Large Model: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-12-23)

**å¤‡æ³¨**: 22 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°è§†è§‰å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æ¨¡å‹` `å¥–åŠ±å·¥ç¨‹` `è¯¾ç¨‹é©±åŠ¨è®­ç»ƒ` `ç»Ÿä¸€å¥–åŠ±å»ºæ¨¡` `æ™ºèƒ½ä½“å†³ç­–` `æ ·æœ¬æ•ˆç‡` `å®‰å…¨éƒ¨ç½²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œå®‰å…¨éƒ¨ç½²æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡é€šè¿‡ç³»ç»ŸåŒ–çš„ç»¼è¿°ï¼Œæå‡ºäº†è§†è§‰å¼ºåŒ–å­¦ä¹ çš„æœ€æ–°ç­–ç•¥å’Œæ¡†æ¶ï¼Œæ¶µç›–å¤šæ¨¡æ€æ¨¡å‹ä¸å¥–åŠ±è®¾è®¡ã€‚
3. é€šè¿‡å¯¹200å¤šé¡¹ç ”ç©¶çš„åˆ†æï¼Œè¯†åˆ«å‡ºè¯¾ç¨‹é©±åŠ¨è®­ç»ƒå’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ç­‰æ–°è¶‹åŠ¿ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„è¿›æ­¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸è§†è§‰æ™ºèƒ½çš„äº¤å‰è¿›å±•ä½¿å¾—æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæ„ŸçŸ¥å¤æ‚çš„è§†è§‰åœºæ™¯ï¼Œè¿˜èƒ½åœ¨å…¶ä¸­è¿›è¡Œæ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚æœ¬æ–‡ç»¼è¿°äº†è¯¥é¢†åŸŸçš„æœ€æ–°åŠ¨æ€ï¼Œé¦–å…ˆå¯¹è§†è§‰å¼ºåŒ–å­¦ä¹ é—®é¢˜è¿›è¡Œäº†å½¢å¼åŒ–å®šä¹‰ï¼Œå¹¶è¿½æº¯äº†ä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±èŒƒå¼çš„ç­–ç•¥ä¼˜åŒ–æ¼”å˜ï¼Œæ¶µç›–äº†ä»è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–åˆ°ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„è¿›å±•ã€‚æˆ‘ä»¬å°†200å¤šé¡¹ä»£è¡¨æ€§å·¥ä½œç»„ç»‡ä¸ºå››ä¸ªä¸»é¢˜æ”¯æŸ±ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ã€‚é’ˆå¯¹æ¯ä¸ªæ”¯æŸ±ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ç®—æ³•è®¾è®¡ã€å¥–åŠ±å·¥ç¨‹å’ŒåŸºå‡†è¿›å±•ï¼Œå¹¶æç‚¼å‡ºå¦‚è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ç­‰è¶‹åŠ¿ã€‚æœ€åï¼Œæˆ‘ä»¬å›é¡¾äº†è¯„ä¼°åè®®ï¼Œå¹¶è¯†åˆ«å‡ºæ ·æœ¬æ•ˆç‡ã€æ³›åŒ–å’Œå®‰å…¨éƒ¨ç½²ç­‰å¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›è§†è§‰å¼ºåŒ–å­¦ä¹ å¿«é€Ÿæ‰©å±•é¢†åŸŸçš„æ¸…æ™°åœ°å›¾ï¼Œå¹¶å¼ºè°ƒæœªæ¥ç ”ç©¶çš„æœ‰å‰æ™¯æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­æ ·æœ¬æ•ˆç‡ä½ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œå®‰å…¨éƒ¨ç½²éš¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚è§†è§‰åœºæ™¯æ—¶å¸¸å¸¸é¢ä¸´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹è§†è§‰å¼ºåŒ–å­¦ä¹ çš„ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œè®ºæ–‡æå‡ºäº†å¤šç§ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†å¥–åŠ±è®¾è®¡çš„æ¼”å˜ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›æ¸…æ™°çš„ç ”ç©¶æ¡†æ¶å’Œæ–¹å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ã€‚æ¯ä¸ªæ¨¡å—éƒ½å›´ç»•ç®—æ³•è®¾è®¡å’Œå¥–åŠ±å·¥ç¨‹å±•å¼€ï¼Œå½¢æˆä¸€ä¸ªå…¨é¢çš„è§†è§‰å¼ºåŒ–å­¦ä¹ ç”Ÿæ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºç³»ç»ŸåŒ–åœ°æ•´åˆäº†200å¤šé¡¹ç ”ç©¶ï¼Œæå‡ºäº†è¯¾ç¨‹é©±åŠ¨è®­ç»ƒå’Œåå¥½å¯¹é½æ‰©æ•£ç­‰æ–°è¶‹åŠ¿ï¼Œæ˜¾è‘—æ¨åŠ¨äº†è§†è§‰å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶è¿›å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå¼ºè°ƒäº†å¥–åŠ±å·¥ç¨‹çš„é‡è¦æ€§ï¼Œæå‡ºäº†ç»Ÿä¸€å¥–åŠ±å»ºæ¨¡çš„æ¦‚å¿µï¼Œå¹¶åœ¨ä¸åŒæ¨¡å—ä¸­é‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¾ç¨‹é©±åŠ¨è®­ç»ƒå’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡çš„æ¨¡å‹åœ¨æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†æ–°çš„ç ”ç©¶æ–¹å‘çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è§†è§‰å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œèƒ½å¤Ÿæ¨åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–èƒ½åŠ›ï¼Œè¿›è€Œå®ç°æ›´é«˜æ°´å¹³çš„æ™ºèƒ½åŒ–åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.

