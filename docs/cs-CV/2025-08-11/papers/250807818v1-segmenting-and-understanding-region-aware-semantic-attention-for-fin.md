---
layout: default
title: Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models
---

# Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07818" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07818v1</a>
  <a href="https://arxiv.org/pdf/2508.07818.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07818v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07818v1', 'Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenyue Song, Chen Hui, Haiqi Zhu, Feng Jiang, Yachun Mi, Wei Zhang, Shaohui Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRSFIQAä»¥è§£å†³æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„åŒºåŸŸæ•æ„Ÿæ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°` `åŒºåŸŸæ„ŸçŸ¥` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç»†ç²’åº¦è¯„ä¼°` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•å…³æ³¨å…¨å±€ç‰¹å¾ï¼Œå¯¼è‡´å¯¹å±€éƒ¨è´¨é‡å˜åŒ–çš„æ•æ„Ÿæ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºRSFIQAæ¨¡å‹ï¼Œåˆ©ç”¨SAMåŠ¨æ€åˆ’åˆ†å›¾åƒåŒºåŸŸï¼Œå¹¶é€šè¿‡MLLMæå–åŒºåŸŸæè¿°ï¼Œå¢å¼ºåŒºåŸŸè´¨é‡æ„ŸçŸ¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRSFIQAåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ç«äº‰åŠ›çš„è´¨é‡é¢„æµ‹æ€§èƒ½ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰æ—¨åœ¨æ¨¡æ‹Ÿä¸äººç±»ä¸»è§‚æ„ŸçŸ¥ä¸€è‡´çš„å›¾åƒè´¨é‡è¯„ä¼°è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰NR-IQAæ–¹æ³•å¾€å¾€å…³æ³¨å…¨å±€ç‰¹å¾ï¼Œå¯¼è‡´å¯¹è¯­ä¹‰æ˜¾è‘—åŒºåŸŸçš„æ´å¯Ÿæœ‰é™ï¼Œæˆ–é‡‡ç”¨ç»Ÿä¸€åŠ æƒçš„åŒºåŸŸç‰¹å¾ï¼Œå‰Šå¼±äº†å¯¹å±€éƒ¨è´¨é‡å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»†ç²’åº¦å›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹RSFIQAï¼Œé›†æˆåŒºåŸŸçº§å¤±çœŸä¿¡æ¯ä»¥æ„ŸçŸ¥å¤šç»´è´¨é‡å·®å¼‚ã€‚é€šè¿‡ä½¿ç”¨Segment Anything Modelï¼ˆSAMï¼‰åŠ¨æ€åˆ’åˆ†è¾“å…¥å›¾åƒä¸ºéé‡å è¯­ä¹‰åŒºåŸŸï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æå–æè¿°æ€§å†…å®¹ï¼ŒRSFIQAå®ç°äº†å¯¹å±€éƒ¨è¯­ä¹‰å’Œè´¨é‡é€€åŒ–çš„å…¨é¢ç†è§£ã€‚æ­¤å¤–ï¼Œæå‡ºçš„åŒºåŸŸæ„ŸçŸ¥è¯­ä¹‰æ³¨æ„åŠ›ï¼ˆRSAï¼‰æœºåˆ¶é€šè¿‡èšåˆå±€éƒ¨åŒºåŸŸçš„ç»†ç²’åº¦è¡¨ç¤ºç”Ÿæˆå…¨å±€æ³¨æ„åŠ›å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨å±€éƒ¨è´¨é‡å˜åŒ–æ•æ„Ÿæ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å…¨å±€ç‰¹å¾æˆ–ç»Ÿä¸€åŠ æƒåŒºåŸŸç‰¹å¾ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å›¾åƒä¸­çš„ç»†å¾®è´¨é‡å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRSFIQAæ¨¡å‹é€šè¿‡é›†æˆåŒºåŸŸçº§å¤±çœŸä¿¡æ¯ï¼Œåˆ©ç”¨SAMå¯¹å›¾åƒè¿›è¡ŒåŠ¨æ€åˆ†å‰²ï¼Œå¹¶é€šè¿‡MLLMæå–åŒºåŸŸæè¿°ï¼Œä»è€Œå®ç°å¯¹å¤šç»´è´¨é‡å·®å¼‚çš„æ„ŸçŸ¥ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å±€éƒ¨è¯­ä¹‰å’Œè´¨é‡é€€åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRSFIQAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨SAMå°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºéé‡å çš„è¯­ä¹‰åŒºåŸŸï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨MLLMå¯¹æ¯ä¸ªåŒºåŸŸè¿›è¡Œæè¿°æ€§å†…å®¹æå–ï¼›æœ€åï¼Œé€šè¿‡RSAæœºåˆ¶ç”Ÿæˆå…¨å±€æ³¨æ„åŠ›å›¾ï¼Œèšåˆå±€éƒ¨åŒºåŸŸçš„ç»†ç²’åº¦è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŒºåŸŸæ„ŸçŸ¥è¯­ä¹‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆRSAï¼‰ï¼Œè¯¥æœºåˆ¶é€šè¿‡èšåˆå±€éƒ¨åŒºåŸŸä¿¡æ¯ç”Ÿæˆå…¨å±€æ³¨æ„åŠ›å›¾ï¼Œæ˜¾è‘—æå‡äº†å¯¹å›¾åƒè´¨é‡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å…¨å±€ç‰¹å¾ä¾èµ–å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€åˆ†å‰²å’Œå¤šæ¨¡æ€èåˆçš„ç­–ç•¥ï¼Œç¡®ä¿äº†åŒºåŸŸç‰¹å¾çš„æœ‰æ•ˆæå–ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šç»´è´¨é‡å·®å¼‚çš„æ„ŸçŸ¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°åŒºåŸŸç‰¹å¾çš„ç»†å¾®å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRSFIQAåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ç»†ç²’åº¦è´¨é‡é¢„æµ‹æ–¹é¢ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†çº¦15%çš„æ€§èƒ½ï¼Œå±•ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå¤šåª’ä½“å†…å®¹è¯„ä¼°ç­‰ã€‚RSFIQAæ¨¡å‹èƒ½å¤Ÿåœ¨æ— å‚è€ƒæƒ…å†µä¸‹æä¾›æ›´ç²¾å‡†çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ï¼Œæœªæ¥å¯åº”ç”¨äºå›¾åƒå‹ç¼©ã€ä¼ è¾“å’Œå­˜å‚¨ç­‰åœºæ™¯ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> No-reference image quality assessment (NR-IQA) aims to simulate the process of perceiving image quality aligned with subjective human perception. However, existing NR-IQA methods either focus on global representations that leads to limited insights into the semantically salient regions or employ a uniform weighting for region features that weakens the sensitivity to local quality variations. In this paper, we propose a fine-grained image quality assessment model, named RSFIQA, which integrates region-level distortion information to perceive multi-dimensional quality discrepancies. To enhance regional quality awareness, we first utilize the Segment Anything Model (SAM) to dynamically partition the input image into non-overlapping semantic regions. For each region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract descriptive content and perceive multi-dimensional distortions, enabling a comprehensive understanding of both local semantics and quality degradations. To effectively leverage this information, we introduce Region-Aware Semantic Attention (RSA) mechanism, which generates a global attention map by aggregating fine-grained representations from local regions. In addition, RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep neural network architectures. Extensive experiments demonstrate the robustness and effectiveness of the proposed method, which achieves competitive quality prediction performance across multiple benchmark datasets.

