---
layout: default
title: MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization
---

# MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07833" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07833v1</a>
  <a href="https://arxiv.org/pdf/2508.07833.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07833v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07833v1', 'MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Animesh Jain, Alexandros Stergiou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: Project page: https://anaekin.github.io/MIMIC

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIMICæ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `å¤šæ¨¡æ€åæ¼”` `æ¨¡å‹å¯è§†åŒ–` `ç‰¹å¾å¯¹é½` `æ­£åˆ™åŒ–æŠ€æœ¯` `è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¶æ„ä¸‹éš¾ä»¥è§£é‡Šï¼Œé™åˆ¶äº†å…¶é€æ˜æ€§å’Œå¯ä¿¡åº¦ã€‚
2. æœ¬æ–‡æå‡ºMIMICæ¡†æ¶ï¼Œé€šè¿‡åˆæˆè§†è§‰æ¦‚å¿µæ¥å¯è§†åŒ–VLMçš„å†…éƒ¨è¡¨ç¤ºï¼Œå¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMIMICåœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰æ–‡æœ¬æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚çš„æ¶æ„ä¸­ç¼–ç å¤šæ¨¡æ€è¾“å…¥ï¼Œå¯¼è‡´å…¶é€æ˜æ€§å’Œå¯ä¿¡åº¦å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€åæ¼”æ¡†æ¶MIMICï¼Œæ—¨åœ¨é€šè¿‡åˆæˆä¸å†…éƒ¨ç¼–ç å¯¹åº”çš„è§†è§‰æ¦‚å¿µæ¥å¯è§†åŒ–VLMçš„å†…éƒ¨è¡¨ç¤ºã€‚MIMICç»“åˆäº†åŸºäºVLMçš„åæ¼”å’Œç‰¹å¾å¯¹é½ç›®æ ‡ï¼Œä»¥é€‚åº”VLMçš„è‡ªå›å½’å¤„ç†ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥äº†ç©ºé—´å¯¹é½ã€è‡ªç„¶å›¾åƒå¹³æ»‘æ€§å’Œè¯­ä¹‰çœŸå®æ„Ÿçš„ä¸‰é‡æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸åŒé•¿åº¦çš„è‡ªç”±å½¢å¼VLMè¾“å‡ºæ–‡æœ¬è¿›è¡Œè§†è§‰æ¦‚å¿µåæ¼”ï¼Œè¿›è¡Œäº†å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰æ–‡æœ¬æŒ‡æ ‡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹VLMæ¦‚å¿µçš„è§†è§‰è§£é‡Šçš„æ¨¡å‹åæ¼”æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚æ¶æ„ä¸‹çš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥ç›´è§‚ç†è§£æ¨¡å‹å†…éƒ¨çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œé™åˆ¶äº†å…¶åº”ç”¨çš„é€æ˜æ€§å’Œä¿¡ä»»åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMIMICæ¡†æ¶é€šè¿‡åæ¼”æŠ€æœ¯åˆæˆä¸VLMå†…éƒ¨ç¼–ç ç›¸å¯¹åº”çš„è§†è§‰æ¦‚å¿µï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„å¯è§†åŒ–ã€‚è¯¥æ–¹æ³•è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMIMICæ¡†æ¶åŒ…æ‹¬åŸºäºVLMçš„åæ¼”æ¨¡å—å’Œç‰¹å¾å¯¹é½ç›®æ ‡ï¼Œé€‚åº”VLMçš„è‡ªå›å½’å¤„ç†ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥äº†ä¸‰é‡æ­£åˆ™åŒ–ï¼Œåˆ†åˆ«ç”¨äºç©ºé—´å¯¹é½ã€è‡ªç„¶å›¾åƒå¹³æ»‘æ€§å’Œè¯­ä¹‰çœŸå®æ„Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šMIMICæ˜¯é¦–ä¸ªé’ˆå¯¹VLMæ¦‚å¿µçš„è§†è§‰è§£é‡Šçš„æ¨¡å‹åæ¼”æ–¹æ³•ï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•åœ¨å¤šæ¨¡æ€ç†è§£ä¸Šçš„å±€é™ï¼Œæä¾›äº†ä¸€ç§æ–°çš„å¯è§†åŒ–æ‰‹æ®µã€‚

**å…³é”®è®¾è®¡**ï¼šMIMICçš„è®¾è®¡åŒ…æ‹¬ç‰¹å¾å¯¹é½æŸå¤±ã€ç©ºé—´å¯¹é½æ­£åˆ™åŒ–ã€è‡ªç„¶å›¾åƒå¹³æ»‘æ€§æ­£åˆ™åŒ–å’Œè¯­ä¹‰çœŸå®æ„Ÿæ­£åˆ™åŒ–ç­‰å…³é”®å‚æ•°è®¾ç½®ï¼Œç¡®ä¿ç”Ÿæˆçš„è§†è§‰æ¦‚å¿µåœ¨è´¨é‡å’Œè¯­ä¹‰ä¸Šéƒ½èƒ½ä¸VLMçš„è¾“å‡ºç›¸åŒ¹é…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMIMICåœ¨è§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚åŒæ—¶ï¼Œåœ¨è¯­ä¹‰æ–‡æœ¬æŒ‡æ ‡ä¸Šï¼ŒMIMICä¹Ÿè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MIMICæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦æé«˜æ¨¡å‹é€æ˜æ€§å’Œå¯è§£é‡Šæ€§çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡å¢å¼ºå¯¹æ¨¡å‹å†…éƒ¨å†³ç­–è¿‡ç¨‹çš„ç†è§£ï¼ŒMIMICå¯ä»¥å¸®åŠ©å¼€å‘è€…å’Œç”¨æˆ·å»ºç«‹å¯¹AIç³»ç»Ÿçš„ä¿¡ä»»ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Language Models (VLMs) encode multimodal inputs over large, complex, and difficult-to-interpret architectures, which limit transparency and trust. We propose a Multimodal Inversion for Model Interpretation and Conceptualization (MIMIC) framework to visualize the internal representations of VLMs by synthesizing visual concepts corresponding to internal encodings. MIMIC uses a joint VLM-based inversion and a feature alignment objective to account for VLM's autoregressive processing. It additionally includes a triplet of regularizers for spatial alignment, natural image smoothness, and semantic realism. We quantitatively and qualitatively evaluate MIMIC by inverting visual concepts over a range of varying-length free-form VLM output texts. Reported results include both standard visual quality metrics as well as semantic text-based metrics. To the best of our knowledge, this is the first model inversion approach addressing visual interpretations of VLM concepts.

