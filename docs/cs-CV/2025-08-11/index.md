---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-11
---

# cs.CVï¼ˆ2025-08-11ï¼‰

ğŸ“Š å…± **35** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—6)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250808066v2-expvg-investigating-the-design-space-of-visual-grounding-in-multimod.html">ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</a></td>
  <td>æå‡ºExpVGä»¥ç³»ç»Ÿç ”ç©¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08066v2" data-paper-url="./papers/250808066v2-expvg-investigating-the-design-space-of-visual-grounding-in-multimod.html" onclick="toggleFavorite(this, '2508.08066v2', 'ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250808093v1-mdd-net-multimodal-depression-detection-through-mutual-transformer.html">MDD-Net: Multimodal Depression Detection through Mutual Transformer</a></td>
  <td>æå‡ºMDD-Netä»¥è§£å†³å¤šæ¨¡æ€æŠ‘éƒæ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08093v1" data-paper-url="./papers/250808093v1-mdd-net-multimodal-depression-detection-through-mutual-transformer.html" onclick="toggleFavorite(this, '2508.08093v1', 'MDD-Net: Multimodal Depression Detection through Mutual Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250807996v1-prompt-guided-relational-reasoning-for-social-behavior-understanding.html">Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</a></td>
  <td>æå‡ºProGraDä»¥è§£å†³ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä¸­çš„ç¤¾äº¤è¡Œä¸ºç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07996v1" data-paper-url="./papers/250807996v1-prompt-guided-relational-reasoning-for-social-behavior-understanding.html" onclick="toggleFavorite(this, '2508.07996v1', 'Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250807871v2-catp-contextually-adaptive-token-pruning-for-efficient-and-enhanced-.html">CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</a></td>
  <td>æå‡ºCATPä»¥è§£å†³å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å›¾åƒä»¤ç‰Œå†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07871v2" data-paper-url="./papers/250807871v2-catp-contextually-adaptive-token-pruning-for-efficient-and-enhanced-.html" onclick="toggleFavorite(this, '2508.07871v2', 'CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250807833v1-mimic-multimodal-inversion-for-model-interpretation-and-conceptualiz.html">MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization</a></td>
  <td>æå‡ºMIMICæ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07833v1" data-paper-url="./papers/250807833v1-mimic-multimodal-inversion-for-model-interpretation-and-conceptualiz.html" onclick="toggleFavorite(this, '2508.07833v1', 'MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250807818v1-segmenting-and-understanding-region-aware-semantic-attention-for-fin.html">Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models</a></td>
  <td>æå‡ºRSFIQAä»¥è§£å†³æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„åŒºåŸŸæ•æ„Ÿæ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07818v1" data-paper-url="./papers/250807818v1-segmenting-and-understanding-region-aware-semantic-attention-for-fin.html" onclick="toggleFavorite(this, '2508.07818v1', 'Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250809218v1-towards-effective-mllm-jailbreaking-through-balanced-on-topicness-an.html">Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity</a></td>
  <td>æå‡ºå››è½´è¯„ä¼°æ¡†æ¶ä¸BSDç­–ç•¥ä»¥æå‡MLLMè¶Šç‹±æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09218v1" data-paper-url="./papers/250809218v1-towards-effective-mllm-jailbreaking-through-balanced-on-topicness-an.html" onclick="toggleFavorite(this, '2508.09218v1', 'Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250808220v1-learning-user-preferences-for-image-generation-model.html">Learning User Preferences for Image Generation Model</a></td>
  <td>æå‡ºåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç”¨æˆ·åå¥½å­¦ä¹ æ–¹æ³•ä»¥æå‡å›¾åƒç”Ÿæˆè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08220v1" data-paper-url="./papers/250808220v1-learning-user-preferences-for-image-generation-model.html" onclick="toggleFavorite(this, '2508.08220v1', 'Learning User Preferences for Image Generation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250808098v2-tbac-uniimage-unified-understanding-and-generation-by-ladder-side-di.html">TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning</a></td>
  <td>æå‡ºTBAC-UniImageä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ·±åº¦æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08098v2" data-paper-url="./papers/250808098v2-tbac-uniimage-unified-understanding-and-generation-by-ladder-side-di.html" onclick="toggleFavorite(this, '2508.08098v2', 'TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250807989v1-the-escalator-problem-identifying-implicit-motion-blindness-in-ai-fo.html">The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</a></td>
  <td>æå‡ºéšæ€§è¿åŠ¨å¤±æ˜é—®é¢˜ä»¥æå‡è¾…åŠ©æŠ€æœ¯çš„å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07989v1" data-paper-url="./papers/250807989v1-the-escalator-problem-identifying-implicit-motion-blindness-in-ai-fo.html" onclick="toggleFavorite(this, '2508.07989v1', 'The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250808508v3-reverse-can-your-vlm-read-a-manga.html">Re:Verse -- Can Your VLM Read a Manga?</a></td>
  <td>æå‡ºæ–°è¯„ä¼°æ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¼«ç”»å™äº‹ç†è§£ä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08508v3" data-paper-url="./papers/250808508v3-reverse-can-your-vlm-read-a-manga.html" onclick="toggleFavorite(this, '2508.08508v3', 'Re:Verse -- Can Your VLM Read a Manga?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250808487v4-mavis-a-multi-agent-framework-for-long-sequence-video-storytelling.html">MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling</a></td>
  <td>æå‡ºMAViSæ¡†æ¶ä»¥è§£å†³é•¿è§†é¢‘ç”Ÿæˆçš„å¤šé‡æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08487v4" data-paper-url="./papers/250808487v4-mavis-a-multi-agent-framework-for-long-sequence-video-storytelling.html" onclick="toggleFavorite(this, '2508.08487v4', 'MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250809220v3-towards-scalable-training-for-handwritten-mathematical-expression-re.html">Towards Scalable Training for Handwritten Mathematical Expression Recognition</a></td>
  <td>æå‡ºTexTellerä»¥è§£å†³æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09220v3" data-paper-url="./papers/250809220v3-towards-scalable-training-for-handwritten-mathematical-expression-re.html" onclick="toggleFavorite(this, '2508.09220v3', 'Towards Scalable Training for Handwritten Mathematical Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250808136v2-fantasystyle-controllable-stylized-distillation-for-3d-gaussian-spla.html">FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</a></td>
  <td>æå‡ºFantasyStyleä»¥è§£å†³3Dé£æ ¼è½¬ç§»ä¸­çš„ä¸ä¸€è‡´æ€§ä¸å†…å®¹æ³„éœ²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08136v2" data-paper-url="./papers/250808136v2-fantasystyle-controllable-stylized-distillation-for-3d-gaussian-spla.html" onclick="toggleFavorite(this, '2508.08136v2', 'FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250807877v1-selective-contrastive-learning-for-weakly-supervised-affordance-grou.html">Selective Contrastive Learning for Weakly Supervised Affordance Grounding</a></td>
  <td>æå‡ºé€‰æ‹©æ€§å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³å¼±ç›‘ç£æ•ˆèƒ½å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07877v1" data-paper-url="./papers/250807877v1-selective-contrastive-learning-for-weakly-supervised-affordance-grou.html" onclick="toggleFavorite(this, '2508.07877v1', 'Selective Contrastive Learning for Weakly Supervised Affordance Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250808189v3-reinforcement-learning-for-large-model-a-survey.html">Reinforcement Learning for Large Model: A Survey</a></td>
  <td>ç»¼è¿°è§†è§‰å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">vision-language-action</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08189v3" data-paper-url="./papers/250808189v3-reinforcement-learning-for-large-model-a-survey.html" onclick="toggleFavorite(this, '2508.08189v3', 'Reinforcement Learning for Large Model: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250808177v2-medreasoner-reinforcement-learning-drives-reasoning-grounding-from-c.html">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</a></td>
  <td>æå‡ºMedReasonerä»¥è§£å†³åŒ»ç–—å½±åƒä¸­ROIç²¾å‡†å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08177v2" data-paper-url="./papers/250808177v2-medreasoner-reinforcement-learning-drives-reasoning-grounding-from-c.html" onclick="toggleFavorite(this, '2508.08177v2', 'MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250808038v2-tride-a-text-assisted-radar-image-weather-aware-fusion-network-for-d.html">TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation</a></td>
  <td>æå‡ºTRIDEä»¥è§£å†³å¤©æ°”å½±å“ä¸‹çš„æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08038v2" data-paper-url="./papers/250808038v2-tride-a-text-assisted-radar-image-weather-aware-fusion-network-for-d.html" onclick="toggleFavorite(this, '2508.08038v2', 'TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250808421v1-neural-tangent-knowledge-distillation-for-optical-convolutional-netw.html">Neural Tangent Knowledge Distillation for Optical Convolutional Networks</a></td>
  <td>æå‡ºç¥ç»åˆ‡çº¿çŸ¥è¯†è’¸é¦ä»¥è§£å†³å…‰å­¦å·ç§¯ç½‘ç»œçš„å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08421v1" data-paper-url="./papers/250808421v1-neural-tangent-knowledge-distillation-for-optical-convolutional-netw.html" onclick="toggleFavorite(this, '2508.08421v1', 'Neural Tangent Knowledge Distillation for Optical Convolutional Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250808186v3-karma-efficient-structural-defect-segmentation-via-kolmogorov-arnold.html">KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</a></td>
  <td>æå‡ºKARMAä»¥è§£å†³åŸºç¡€è®¾æ–½ç»“æ„ç¼ºé™·è¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08186v3" data-paper-url="./papers/250808186v3-karma-efficient-structural-defect-segmentation-via-kolmogorov-arnold.html" onclick="toggleFavorite(this, '2508.08186v3', 'KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250807847v1-deep-space-weather-model-long-range-solar-flare-prediction-from-mult.html">Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images</a></td>
  <td>æå‡ºæ·±ç©ºå¤©æ°”æ¨¡å‹ä»¥è§£å†³å¤ªé˜³è€€æ–‘é•¿æ—¶é—´é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">state space model</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">masked autoencoder</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07847v1" data-paper-url="./papers/250807847v1-deep-space-weather-model-long-range-solar-flare-prediction-from-mult.html" onclick="toggleFavorite(this, '2508.07847v1', 'Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250808082v1-me-tst-micro-expression-analysis-via-temporal-state-transition-with-.html">ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness</a></td>
  <td>æå‡ºME-TST+ä»¥è§£å†³å¾®è¡¨æƒ…åˆ†æä¸­çš„æ—¶åºä¸ä»»åŠ¡å…³è”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08082v1" data-paper-url="./papers/250808082v1-me-tst-micro-expression-analysis-via-temporal-state-transition-with-.html" onclick="toggleFavorite(this, '2508.08082v1', 'ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250808252v1-refersplat-referring-segmentation-in-3d-gaussian-splatting.html">ReferSplat: Referring Segmentation in 3D Gaussian Splatting</a></td>
  <td>æå‡ºReferSplatä»¥è§£å†³3Dåœºæ™¯ä¸­çš„ç›®æ ‡åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08252v1" data-paper-url="./papers/250808252v1-refersplat-referring-segmentation-in-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.08252v1', 'ReferSplat: Referring Segmentation in 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250807701v2-multi-view-normal-and-distance-guidance-gaussian-splatting-for-surfa.html">Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction</a></td>
  <td>æå‡ºå¤šè§†è§’æ³•å‘ä¸è·ç¦»å¼•å¯¼çš„é«˜æ–¯ç‚¹äº‘é‡å»ºæ–¹æ³•ä»¥è§£å†³è¡¨é¢é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">metric depth</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07701v2" data-paper-url="./papers/250807701v2-multi-view-normal-and-distance-guidance-gaussian-splatting-for-surfa.html" onclick="toggleFavorite(this, '2508.07701v2', 'Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250808219v1-sagonline-segment-any-gaussians-online.html">SAGOnline: Segment Any Gaussians Online</a></td>
  <td>æå‡ºSAGOnlineä»¥è§£å†³é«˜æ•ˆ3Dåˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08219v1" data-paper-url="./papers/250808219v1-sagonline-segment-any-gaussians-online.html" onclick="toggleFavorite(this, '2508.08219v1', 'SAGOnline: Segment Any Gaussians Online')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250807908v2-mem4d-decoupling-static-and-dynamic-memory-for-dynamic-scene-reconst.html">Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction</a></td>
  <td>æå‡ºMem4Dä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„è®°å¿†éœ€æ±‚å›°å¢ƒ</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07908v2" data-paper-url="./papers/250807908v2-mem4d-decoupling-static-and-dynamic-memory-for-dynamic-scene-reconst.html" onclick="toggleFavorite(this, '2508.07908v2', 'Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250808117v1-grasptrack-geometry-reasoned-association-via-segmentation-and-projec.html">GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</a></td>
  <td>æå‡ºGRASPTrackä»¥è§£å†³å•ç›®è§†é¢‘ä¸­çš„å¤šç›®æ ‡è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08117v1" data-paper-url="./papers/250808117v1-grasptrack-geometry-reasoned-association-via-segmentation-and-projec.html" onclick="toggleFavorite(this, '2508.08117v1', 'GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250808086v1-matrix-3d-omnidirectional-explorable-3d-world-generation.html">Matrix-3D: Omnidirectional Explorable 3D World Generation</a></td>
  <td>æå‡ºMatrix-3Dä»¥è§£å†³å…¨æ™¯å¯æ¢ç´¢3Dä¸–ç•Œç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08086v1" data-paper-url="./papers/250808086v1-matrix-3d-omnidirectional-explorable-3d-world-generation.html" onclick="toggleFavorite(this, '2508.08086v1', 'Matrix-3D: Omnidirectional Explorable 3D World Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250808170v2-recondreamer-rl-enhancing-reinforcement-learning-via-diffusion-based.html">ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</a></td>
  <td>æå‡ºReconDreamer-RLä»¥è§£å†³ä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08170v2" data-paper-url="./papers/250808170v2-recondreamer-rl-enhancing-reinforcement-learning-via-diffusion-based.html" onclick="toggleFavorite(this, '2508.08170v2', 'ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250807626v1-ar-vrm-imitating-human-motions-for-visual-robot-manipulation-with-an.html">AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning</a></td>
  <td>æå‡ºAR-VRMä»¥è§£å†³æœºå™¨äººè§†è§‰æ“æ§ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07626v1" data-paper-url="./papers/250807626v1-ar-vrm-imitating-human-motions-for-visual-robot-manipulation-with-an.html" onclick="toggleFavorite(this, '2508.07626v1', 'AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250808521v1-visor-visual-input-based-steering-for-output-redirection-in-vision-l.html">VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models</a></td>
  <td>æå‡ºVISORä»¥è§£å†³è§†è§‰è¾“å…¥å¼•å¯¼è¾“å‡ºé‡å®šå‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08521v1" data-paper-url="./papers/250808521v1-visor-visual-input-based-steering-for-output-redirection-in-vision-l.html" onclick="toggleFavorite(this, '2508.08521v1', 'VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/250808179v2-pp-motion-physical-perceptual-fidelity-evaluation-for-human-motion-g.html">PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation</a></td>
  <td>æå‡ºPP-Motionä»¥è§£å†³äººç±»åŠ¨ä½œç”Ÿæˆçš„è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08179v2" data-paper-url="./papers/250808179v2-pp-motion-physical-perceptual-fidelity-evaluation-for-human-motion-g.html" onclick="toggleFavorite(this, '2508.08179v2', 'PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250808254v1-learning-an-implicit-physics-model-for-image-based-fluid-simulation.html">Learning an Implicit Physics Model for Image-based Fluid Simulation</a></td>
  <td>æå‡ºä¸€ç§éšå¼ç‰©ç†æ¨¡å‹ä»¥è§£å†³åŸºäºå›¾åƒçš„æµä½“æ¨¡æ‹Ÿé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08254v1" data-paper-url="./papers/250808254v1-learning-an-implicit-physics-model-for-image-based-fluid-simulation.html" onclick="toggleFavorite(this, '2508.08254v1', 'Learning an Implicit Physics Model for Image-based Fluid Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250807863v1-being-m05-a-real-time-controllable-vision-language-motion-model.html">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></td>
  <td>æå‡ºBeing-M0.5ä»¥è§£å†³äººç±»åŠ¨ä½œç”Ÿæˆçš„å¯æ§æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.07863v1" data-paper-url="./papers/250807863v1-being-m05-a-real-time-controllable-vision-language-motion-model.html" onclick="toggleFavorite(this, '2508.07863v1', 'Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>35</td>
  <td><a href="./papers/250808199v1-spatial-ormllm-improve-spatial-relation-understanding-in-the-operati.html">Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model</a></td>
  <td>æå‡ºSpatial-ORMLLMä»¥è§£å†³æ‰‹æœ¯å®¤ç©ºé—´å…³ç³»ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08199v1" data-paper-url="./papers/250808199v1-spatial-ormllm-improve-spatial-relation-understanding-in-the-operati.html" onclick="toggleFavorite(this, '2508.08199v1', 'Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)