---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-11-17
---

# cs.CVï¼ˆ2025-11-17ï¼‰

ğŸ“Š å…± **47** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (25 ğŸ”—6)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (16 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (25 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251113011v1-beyond-darkness-thermal-supervised-3d-gaussian-splatting-for-low-lig.html">Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis</a></td>
  <td>æå‡ºDTGSï¼šä¸€ç§çƒ­ç›‘ç£3Dé«˜æ–¯æº…å°„æ–¹æ³•ï¼Œç”¨äºä½å…‰ç…§ä¸‹çš„æ–°è§†è§’åˆæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13011v1" onclick="toggleFavorite(this, '2511.13011v1', 'Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251113571v1-opt3dgs-optimizing-3d-gaussian-splatting-with-adaptive-exploration-a.html">Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation</a></td>
  <td>Opt3DGSï¼šé€šè¿‡è‡ªé€‚åº”æ¢ç´¢å’Œæ›²ç‡æ„ŸçŸ¥åˆ©ç”¨ä¼˜åŒ–3Dé«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13571v1" onclick="toggleFavorite(this, '2511.13571v1', 'Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251113278v2-sf-recon-simplification-free-lightweight-building-reconstruction-via.html">SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</a></td>
  <td>SF-Reconï¼šé€šè¿‡3Dé«˜æ–¯æº…å°„å®ç°å…ç®€åŒ–çš„è½»é‡çº§å»ºç­‘é‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13278v2" onclick="toggleFavorite(this, '2511.13278v2', 'SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251113264v2-symgs-leveraging-local-symmetries-for-3d-gaussian-splatting-compress.html">SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression</a></td>
  <td>SymGSï¼šåˆ©ç”¨å±€éƒ¨å¯¹ç§°æ€§å‹ç¼©3Dé«˜æ–¯æº…å°„æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13264v2" onclick="toggleFavorite(this, '2511.13264v2', 'SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251113259v1-geox-bench-benchmarking-cross-view-geo-localization-and-pose-estimat.html">GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models</a></td>
  <td>GeoX-Benchï¼šç”¨äºè¯„ä¼°å¤§æ¨¡å‹è·¨è§†è§’åœ°ç†å®šä½ä¸å§¿æ€ä¼°è®¡èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13259v1" onclick="toggleFavorite(this, '2511.13259v1', 'GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251112930v1-neo-real-time-on-device-3d-gaussian-splatting-with-reuse-and-update-.html">Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration</a></td>
  <td>Neoï¼šåŸºäºé‡ç”¨-æ›´æ–°æ’åºåŠ é€Ÿçš„å®æ—¶ç«¯ä¾§3Dé«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12930v1" onclick="toggleFavorite(this, '2511.12930v1', 'Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251113864v1-grloc-geometric-representation-regression-for-visual-localization.html">GRLoc: Geometric Representation Regression for Visual Localization</a></td>
  <td>æå‡ºGRLocï¼šé€šè¿‡å‡ ä½•è¡¨ç¤ºå›å½’å®ç°æ›´é²æ£’çš„è§†è§‰å®šä½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13864v1" onclick="toggleFavorite(this, '2511.13864v1', 'GRLoc: Geometric Representation Regression for Visual Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251112935v2-pfavatar-pose-fusion-3d-personalized-avatar-reconstruction-from-real.html">PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</a></td>
  <td>PFAvatarï¼šä»æ—¥å¸¸ç…§ç‰‡ä¸­è¿›è¡Œå§¿æ€èåˆçš„ä¸ªæ€§åŒ–3Då¤´åƒé‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12935v2" onclick="toggleFavorite(this, '2511.12935v2', 'PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251113857v1-rspose-ranking-based-losses-for-human-pose-estimation.html">RSPose: Ranking Based Losses for Human Pose Estimation</a></td>
  <td>RSPoseï¼šæå‡ºåŸºäºæ’åºæŸå¤±çš„äººä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œæ˜¾è‘—æå‡mAP</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13857v1" onclick="toggleFavorite(this, '2511.13857v1', 'RSPose: Ranking Based Losses for Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251113269v1-is-your-vlm-sky-ready-a-comprehensive-spatial-intelligence-benchmark.html">Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation</a></td>
  <td>æå‡ºSpatialSky-Benchä»¥è¯„ä¼°æ— äººæœºå¯¼èˆªä¸­çš„ç©ºé—´æ™ºèƒ½èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13269v1" onclick="toggleFavorite(this, '2511.13269v1', 'Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251113121v1-closeupshot-close-up-novel-view-synthesis-from-sparse-views-via-poin.html">CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model</a></td>
  <td>æå‡ºCloseUpShotï¼Œé€šè¿‡ç‚¹äº‘æ¡ä»¶æ‰©æ•£æ¨¡å‹å®ç°ç¨€ç–è§†è§’ä¸‹çš„è¿‘è·ç¦»æ–°è§†è§’åˆæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13121v1" onclick="toggleFavorite(this, '2511.13121v1', 'CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251112895v1-reconstructing-3d-scenes-in-native-high-dynamic-range.html">Reconstructing 3D Scenes in Native High Dynamic Range</a></td>
  <td>æå‡ºNH-3DGSï¼Œç›´æ¥ä»åŸç”ŸHDRæ•°æ®é‡å»ºé«˜è´¨é‡3Dåœºæ™¯</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12895v1" onclick="toggleFavorite(this, '2511.12895v1', 'Reconstructing 3D Scenes in Native High Dynamic Range')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251113282v2-towards-metric-aware-multi-person-mesh-recovery-by-jointly-optimizin.html">Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</a></td>
  <td>æå‡ºæ·±åº¦æ¡ä»¶å¹³ç§»ä¼˜åŒ–ä¸åº¦é‡æ„ŸçŸ¥ç½‘ç»œï¼Œå®ç°ç›¸æœºç©ºé—´å¤šäººç½‘æ ¼é‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13282v2" onclick="toggleFavorite(this, '2511.13282v2', 'Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251113208v2-end-to-end-multi-person-pose-estimation-with-pose-aware-video-transf.html">End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer</a></td>
  <td>æå‡ºPAVE-Netï¼Œä¸€ç§ç«¯åˆ°ç«¯å§¿æ€æ„ŸçŸ¥è§†é¢‘Transformerç½‘ç»œï¼Œç”¨äºå¤šäººè§†é¢‘å§¿æ€ä¼°è®¡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13208v2" onclick="toggleFavorite(this, '2511.13208v2', 'End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251113102v2-capenext-rethinking-and-refining-dynamic-support-information-for-cat.html">CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation</a></td>
  <td>CapeNextï¼šé€šè¿‡ä¼˜åŒ–åŠ¨æ€æ”¯æŒä¿¡æ¯ï¼Œæ”¹è¿›ç±»åˆ«æ— å…³çš„å§¿æ€ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13102v2" onclick="toggleFavorite(this, '2511.13102v2', 'CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251113039v1-mgca-net-multi-grained-category-aware-network-for-open-vocabulary-te.html">MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization</a></td>
  <td>æå‡ºMGCA-Netï¼Œé€šè¿‡å¤šç²’åº¦ç±»åˆ«æ„ŸçŸ¥è§£å†³å¼€æ”¾è¯æ±‡æ—¶åºåŠ¨ä½œå®šä½é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13039v1" onclick="toggleFavorite(this, '2511.13039v1', 'MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251112961v1-inertia-informed-orientation-priors-for-event-based-optical-flow-est.html">Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation</a></td>
  <td>æå‡ºä¸€ç§èåˆæƒ¯æ€§ä¿¡æ¯çš„äº‹ä»¶ç›¸æœºå…‰æµä¼°è®¡æ–¹æ³•ï¼Œæå‡é²æ£’æ€§å’Œæ”¶æ•›æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12961v1" onclick="toggleFavorite(this, '2511.12961v1', 'Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251112919v3-coordar-one-reference-6d-pose-estimation-of-novel-objects-via-autore.html">CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</a></td>
  <td>CoordARï¼šåŸºäºè‡ªå›å½’åæ ‡å›¾ç”Ÿæˆçš„å•å‚è€ƒæ–°ç‰©ä½“6Dä½å§¿ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12919v3" onclick="toggleFavorite(this, '2511.12919v3', 'CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251113684v1-training-free-multi-view-extension-of-ic-light-for-textual-position-.html">Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</a></td>
  <td>GS-Lightï¼šåŸºäºé«˜æ–¯æº…å°„çš„æ–‡æœ¬å¼•å¯¼ã€æ— è®­ç»ƒå¤šè§†è§’åœºæ™¯é‡å…‰ç…§æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13684v1" onclick="toggleFavorite(this, '2511.13684v1', 'Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251113647v1-part-x-mllm-part-aware-3d-multimodal-large-language-model.html">Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</a></td>
  <td>Part-X-MLLMï¼šæå‡ºåŸºäºéƒ¨ä»¶æ„ŸçŸ¥çš„3Då¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»Ÿä¸€è§£å†³å¤šç§3Dä»»åŠ¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13647v1" onclick="toggleFavorite(this, '2511.13647v1', 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251113315v1-computer-vision-based-group-activity-detection-and-action-spotting.html">Computer Vision based group activity detection and action spotting</a></td>
  <td>æå‡ºåŸºäºè®¡ç®—æœºè§†è§‰çš„ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ä¸è¡Œä¸ºå®šä½æ¡†æ¶ï¼Œèåˆæ·±åº¦å­¦ä¹ ä¸å›¾æ¨ç†ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13315v1" onclick="toggleFavorite(this, '2511.13315v1', 'Computer Vision based group activity detection and action spotting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251113132v1-shedding-light-on-vln-robustness-a-black-box-framework-for-indoor-li.html">Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack</a></td>
  <td>æå‡ºåŸºäºå®¤å†…å…‰ç…§å¯¹æŠ—æ”»å‡»çš„VLNé²æ£’æ€§é»‘ç›’è¯„ä¼°æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13132v1" onclick="toggleFavorite(this, '2511.13132v1', 'Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251113115v2-a-lightweight-3d-anomaly-detection-method-with-rotationally-invarian.html">A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features</a></td>
  <td>æå‡ºåŸºäºæ—‹è½¬ä¸å˜ç‰¹å¾çš„è½»é‡çº§3Då¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œæå‡ç‚¹äº‘æ•°æ®å¤„ç†çš„é²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13115v2" onclick="toggleFavorite(this, '2511.13115v2', 'A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251113047v1-diffpixelformer-differential-pixel-aware-transformer-for-rgb-d-indoo.html">DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation</a></td>
  <td>æå‡ºDiffPixelFormerï¼Œç”¨äºæå‡RGB-Då®¤å†…åœºæ™¯åˆ†å‰²çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13047v1" onclick="toggleFavorite(this, '2511.13047v1', 'DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251112977v2-artiworld-llm-driven-articulation-of-3d-objects-in-scenes.html">ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes</a></td>
  <td>ArtiWorldï¼šæå‡ºLLMé©±åŠ¨çš„3Dåœºæ™¯ç‰©ä½“å¯åŠ¨æ€§è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12977v2" onclick="toggleFavorite(this, '2511.12977v2', 'ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251113138v1-winmamba-multi-scale-shifted-windows-in-state-space-model-for-3d-obj.html">WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection</a></td>
  <td>WinMambaï¼šé¢å‘3Dç›®æ ‡æ£€æµ‹ï¼Œæå‡ºåŸºäºå¤šå°ºåº¦ç§»ä½çª—å£çš„çŠ¶æ€ç©ºé—´æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13138v1" onclick="toggleFavorite(this, '2511.13138v1', 'WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251113306v1-dap-a-discrete-token-autoregressive-planner-for-autonomous-driving.html">DAP: A Discrete-token Autoregressive Planner for Autonomous Driving</a></td>
  <td>DAPï¼šä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶çš„ç¦»æ•£tokenè‡ªå›å½’è§„åˆ’å™¨ï¼Œå®ç°BEVè¯­ä¹‰å’Œè½¨è¿¹è”åˆé¢„æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13306v1" onclick="toggleFavorite(this, '2511.13306v1', 'DAP: A Discrete-token Autoregressive Planner for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251117596v1-reconstruction-driven-multimodal-representation-learning-for-automat.html">Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding</a></td>
  <td>æå‡ºåŸºäºé‡æ„é©±åŠ¨çš„å¤šæ¨¡æ€è‡ªç¼–ç å™¨ï¼Œç”¨äºè‡ªåŠ¨åŒ–åª’ä½“å†…å®¹ç†è§£ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.17596v1" onclick="toggleFavorite(this, '2511.17596v1', 'Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251113649v3-distribution-matching-distillation-meets-reinforcement-learning.html">Distribution Matching Distillation Meets Reinforcement Learning</a></td>
  <td>æå‡ºDMDRæ¡†æ¶ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼Œæå‡å°‘æ­¥æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13649v3" onclick="toggleFavorite(this, '2511.13649v3', 'Distribution Matching Distillation Meets Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251113545v1-robust-defense-strategies-for-multimodal-contrastive-learning-effici.html">Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks</a></td>
  <td>æå‡ºä¸€ç§é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œå¢å¼ºå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹æŠµæŠ—åé—¨æ”»å‡»çš„é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13545v1" onclick="toggleFavorite(this, '2511.13545v1', 'Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251113222v1-hybrid-domain-adaptative-representation-learning-for-gaze-estimation.html">Hybrid-Domain Adaptative Representation Learning for Gaze Estimation</a></td>
  <td>æå‡ºæ··åˆé¢†åŸŸè‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ ä»¥è§£å†³æ³¨è§†ä¼°è®¡ä¸­çš„è·¨åŸŸé—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13222v1" onclick="toggleFavorite(this, '2511.13222v1', 'Hybrid-Domain Adaptative Representation Learning for Gaze Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251113168v1-soma-feature-gradient-enhanced-affine-flow-matching-for-sar-optical-.html">SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration</a></td>
  <td>SOMAï¼šé€šè¿‡ç‰¹å¾æ¢¯åº¦å¢å¼ºçš„ä»¿å°„æµåŒ¹é…å®ç°SAR-å…‰å­¦å›¾åƒé…å‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13168v1" onclick="toggleFavorite(this, '2511.13168v1', 'SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251112976v1-mcaq-yolo-morphological-complexity-aware-quantization-for-efficient-.html">MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning</a></td>
  <td>æå‡ºMCAQ-YOLOï¼Œé€šè¿‡å½¢æ€å¤æ‚åº¦æ„ŸçŸ¥é‡åŒ–æå‡ç›®æ ‡æ£€æµ‹æ•ˆç‡ï¼Œé€‚ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12976v1" onclick="toggleFavorite(this, '2511.12976v1', 'MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251112908v1-deepsport-a-multimodal-large-language-model-for-comprehensive-sports.html">DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning</a></td>
  <td>DeepSportï¼šåŸºäºAgentå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå…¨é¢çš„ä½“è‚²è§†é¢‘æ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12908v1" onclick="toggleFavorite(this, '2511.12908v1', 'DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251113794v1-fusionfm-all-in-one-multi-modal-image-fusion-with-flow-matching.html">FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching</a></td>
  <td>æå‡ºFusionFMï¼Œåˆ©ç”¨Flow Matchingå®ç°é«˜æ•ˆå¤šæ¨¡æ€å›¾åƒèåˆ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13794v1" onclick="toggleFavorite(this, '2511.13794v1', 'FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/251113924v1-start-small-think-big-curriculum-based-relative-policy-optimization-.html">Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding</a></td>
  <td>æå‡ºåŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–CuRPOï¼Œæå‡è§†è§‰å®šä½ä»»åŠ¡ä¸­CoTæ¨ç†çš„æ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13924v1" onclick="toggleFavorite(this, '2511.13924v1', 'Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251113431v1-fuse-a-flow-based-mapping-between-shapes.html">FUSE: A Flow-based Mapping Between Shapes</a></td>
  <td>æå‡ºåŸºäºFlow-Matchingçš„å½¢çŠ¶æ˜ å°„æ–¹æ³•ï¼Œé«˜æ•ˆæ”¯æŒè·¨è¡¨ç¤ºå½¢çŠ¶åŒ¹é…ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13431v1" onclick="toggleFavorite(this, '2511.13431v1', 'FUSE: A Flow-based Mapping Between Shapes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251112909v1-casl-curvature-augmented-self-supervised-learning-for-3d-anomaly-det.html">CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection</a></td>
  <td>æå‡ºCASLï¼šä¸€ç§æ›²ç‡å¢å¼ºçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæå‡3Då¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12909v1" onclick="toggleFavorite(this, '2511.12909v1', 'CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/251113648v1-physx-anything-simulation-ready-physical-3d-assets-from-single-image.html">PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</a></td>
  <td>PhysX-Anythingï¼šé¦–ä¸ªå•å›¾ç”Ÿæˆå¯ç”¨äºä»¿çœŸçš„ç‰©ç†3Dèµ„äº§æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13648v1" onclick="toggleFavorite(this, '2511.13648v1', 'PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/251113798v1-kangura-kolmogorov-arnold-network-based-geometry-aware-learning-with.html">KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures</a></td>
  <td>KANGURAï¼šåŸºäºKANçš„å‡ ä½•æ„ŸçŸ¥å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¤æ‚ç»“æ„çš„ä¸‰ç»´å»ºæ¨¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13798v1" onclick="toggleFavorite(this, '2511.13798v1', 'KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/251112940v1-recurrent-autoregressive-diffusion-global-memory-meets-local-attenti.html">Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention</a></td>
  <td>æå‡ºRADæ¡†æ¶ï¼Œé€šè¿‡å¾ªç¯è‡ªå›å½’æ‰©æ•£æ¨¡å‹è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„è®°å¿†å’Œæ—¶ç©ºä¸€è‡´æ€§é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12940v1" onclick="toggleFavorite(this, '2511.12940v1', 'Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/251113713v1-free-form-scene-editor-enabling-multi-round-object-manipulation-like.html">Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine</a></td>
  <td>æå‡ºFFSEï¼Œå®ç°3Då¼•æ“èˆ¬çš„å¤šè½®ç‰©ä½“æ“ä½œå›¾åƒç¼–è¾‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13713v1" onclick="toggleFavorite(this, '2511.13713v1', 'Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/251113065v1-robustgait-robustness-analysis-for-appearance-based-gait-recognition.html">RobustGait: Robustness Analysis for Appearance Based Gait Recognition</a></td>
  <td>RobustGaitï¼šé’ˆå¯¹åŸºäºå¤–è§‚çš„æ­¥æ€è¯†åˆ«çš„é²æ£’æ€§åˆ†ææ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13065v1" onclick="toggleFavorite(this, '2511.13065v1', 'RobustGait: Robustness Analysis for Appearance Based Gait Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/251113458v1-trust-in-vision-language-models-insights-from-a-participatory-user-w.html">Trust in Vision-Language Models: Insights from a Participatory User Workshop</a></td>
  <td>é€šè¿‡ç”¨æˆ·å‚ä¸å¼ç ”è®¨ä¼šæ´å¯Ÿè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„ç”¨æˆ·ä¿¡ä»»é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13458v1" onclick="toggleFavorite(this, '2511.13458v1', 'Trust in Vision-Language Models: Insights from a Participatory User Workshop')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/251112921v1-generative-photographic-control-for-scene-consistent-video-cinematic.html">Generative Photographic Control for Scene-Consistent Video Cinematic Editing</a></td>
  <td>CineCtrlï¼šæå‡ºä¸€ç§ç”Ÿæˆå¼è§†é¢‘ç”µå½±ç¼–è¾‘æ¡†æ¶ï¼Œå®ç°å¯¹ä¸“ä¸šç›¸æœºå‚æ•°çš„ç²¾ç»†æ§åˆ¶ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12921v1" onclick="toggleFavorite(this, '2511.12921v1', 'Generative Photographic Control for Scene-Consistent Video Cinematic Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/251112878v3-uni-hand-universal-hand-motion-forecasting-in-egocentric-views.html">Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views</a></td>
  <td>Uni-Handï¼šç”¨äºç¬¬ä¸€äººç§°è§†è§’çš„é€šç”¨æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.12878v3" onclick="toggleFavorite(this, '2511.12878v3', 'Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/251113032v1-uni-inter-unifying-3d-human-motion-synthesis-across-diverse-interact.html">Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts</a></td>
  <td>æå‡ºUni-Interæ¡†æ¶ä»¥è§£å†³å¤šç§äº¤äº’åœºæ™¯ä¸‹çš„äººç±»åŠ¨ä½œç”Ÿæˆé—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.13032v1" onclick="toggleFavorite(this, '2511.13032v1', 'Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)