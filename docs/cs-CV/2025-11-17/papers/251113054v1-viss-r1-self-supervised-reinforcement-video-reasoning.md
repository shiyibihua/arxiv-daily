---
layout: default
title: ViSS-R1: Self-Supervised Reinforcement Video Reasoning
---

# ViSS-R1: Self-Supervised Reinforcement Video Reasoning

**arXiv**: [2511.13054v1](https://arxiv.org/abs/2511.13054) | [PDF](https://arxiv.org/pdf/2511.13054.pdf)

**ä½œè€…**: Bo Fang, Yuxin Song, Qiangqiang Wu, Haoyuan Sun, Wenhao Wu, Antoni B. Chan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViSS-R1æ¡†æž¶ï¼Œé€šè¿‡è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„å¤æ‚è§†é¢‘æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘æŽ¨ç†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `R1åŽè®­ç»ƒ` `è§†è§‰å˜æ¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰R1æ–¹æ³•åœ¨è§†é¢‘æŽ¨ç†ä¸­è¿‡åº¦ä¾èµ–æ–‡æœ¬ï¼Œå¿½è§†è§†è§‰ä¿¡æ¯ï¼Œæ˜“å¯¼è‡´æ·å¾„å­¦ä¹ å’Œå¹»è§‰
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Pretext-GRPOç®—æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£ä»»åŠ¡å¥–åŠ±æ¨¡åž‹å¤„ç†å˜æ¢åŽçš„è§†è§‰è¾“å…¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…­ä¸ªè§†é¢‘æŽ¨ç†åŸºå‡†ä¸ŠéªŒè¯äº†ViSS-R1çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

