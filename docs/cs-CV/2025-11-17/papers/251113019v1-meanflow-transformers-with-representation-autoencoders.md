---
layout: default
title: MeanFlow Transformers with Representation Autoencoders
---

# MeanFlow Transformers with Representation Autoencoders

**arXiv**: [2511.13019v1](https://arxiv.org/abs/2511.13019) | [PDF](https://arxiv.org/pdf/2511.13019.pdf)

**ä½œè€…**: Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè¡¨ç¤ºè‡ªç¼–ç å™¨çš„MeanFlowæ–¹æ³•ï¼Œä»¥é«˜æ•ˆç¨³å®šè®­ç»ƒå’Œé‡‡æ ·ï¼Œç”¨äºŽå›¾åƒç”Ÿæˆã€‚**

**å…³é”®è¯**: `å›¾åƒç”Ÿæˆ` `è¡¨ç¤ºè‡ªç¼–ç å™¨` `MeanFlow` `è’¸é¦è®­ç»ƒ` `é«˜æ•ˆé‡‡æ ·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMeanFlowè®­ç»ƒè®¡ç®—é‡å¤§ã€ä¸ç¨³å®šï¼Œä¸”ä¾èµ–å¤æ‚è¶…å‚æ•°æŒ‡å¯¼ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨è¡¨ç¤ºè‡ªç¼–ç å™¨æ½œåœ¨ç©ºé—´è®­ç»ƒï¼Œé‡‡ç”¨ä¸€è‡´æ€§ä¸­è®­ç»ƒå’Œä¸¤é˜¶æ®µè’¸é¦æ–¹æ¡ˆã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ImageNet 256ä¸Š1æ­¥FIDè¾¾2.03ï¼Œé™ä½Žé‡‡æ ·GFLOPS 38%å’Œè®­ç»ƒæˆæœ¬83%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

