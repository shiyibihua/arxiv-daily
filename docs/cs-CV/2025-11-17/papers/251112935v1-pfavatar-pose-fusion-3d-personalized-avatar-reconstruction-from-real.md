---
layout: default
title: PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos
---

# PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos

**arXiv**: [2511.12935v1](https://arxiv.org/abs/2511.12935) | [PDF](https://arxiv.org/pdf/2511.12935.pdf)

**ä½œè€…**: Dianbing Xi, Guoyuan An, Jingsen Zhu, Zhijian Liu, Yuan Liu, Ruiyuan Zhang, Jiayuan Lu, Rui Wang, Yuchi Huo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPFAvataræ–¹æ³•ï¼Œä»Žæ—¥å¸¸ç©¿æ­ç…§ç‰‡é‡å»ºé«˜è´¨é‡3Dè™šæ‹ŸåŒ–èº«**

**å…³é”®è¯**: `3Dè™šæ‹ŸåŒ–èº«é‡å»º` `ç¥žç»è¾å°„åœº` `å§¿æ€èžåˆ` `å°‘æ ·æœ¬å­¦ä¹ ` `æ—¥å¸¸ç©¿æ­ç…§ç‰‡` `æ‰©æ•£æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žå¤šæ ·å§¿æ€ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯çš„æ—¥å¸¸ç©¿æ­ç…§ç‰‡ä¸­é‡å»º3DåŒ–èº«ï¼Œé¿å…ä¼ ç»Ÿåˆ†è§£æ–¹æ³•çš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆå¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡åž‹ï¼Œå†è’¸é¦NeRFè¡¨ç¤ºï¼Œç»“åˆControlNetå’ŒCPPLæŸå¤±ä¼˜åŒ–ç»†èŠ‚ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™å’Œé®æŒ¡é²æ£’æ€§ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ”¯æŒè™šæ‹Ÿè¯•ç©¿ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

