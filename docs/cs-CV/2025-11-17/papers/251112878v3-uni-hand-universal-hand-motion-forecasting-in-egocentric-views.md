---
layout: default
title: Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views
---

# Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.12878" target="_blank" class="toolbar-btn">arXiv: 2511.12878v3</a>
    <a href="https://arxiv.org/pdf/2511.12878.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12878v3" 
            onclick="toggleFavorite(this, '2511.12878v3', 'Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Yu Zheng, Erhang Zhang, Xieyuanli Chen, Hesheng Wang

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17 (Êõ¥Êñ∞: 2025-12-05)

**Â§áÊ≥®**: Extended journal version of MMTwin (IROS'25). Code and data: https://github.com/IRMVLab/UniHand

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Uni-HandÔºöÁî®‰∫éÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßíÁöÑÈÄöÁî®ÊâãÈÉ®ËøêÂä®È¢ÑÊµãÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ÊâãÈÉ®ËøêÂä®È¢ÑÊµã` `Á¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí` `Â§öÊ®°ÊÄÅËûçÂêà` `Êâ©Êï£Ê®°Âûã` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊâãÈÉ®ËΩ®ËøπÈ¢ÑÊµãÊñπÊ≥ïÂú®È¢ÑÊµãÁõÆÊ†á„ÄÅÊ®°ÊÄÅËûçÂêà„ÄÅËøêÂä®Ëß£ËÄ¶Âíå‰∏ãÊ∏∏‰ªªÂä°È™åËØÅÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ
2. Uni-HandÈÄöËøáÂ§öÊ®°ÊÄÅËûçÂêà„ÄÅÂèåÂàÜÊîØÊâ©Êï£ÂíåÁõÆÊ†áÊåáÁ§∫Âô®ÔºåÂÆûÁé∞Â§öÁª¥Â∫¶„ÄÅÂ§öÁõÆÊ†áÁöÑÊâãÈÉ®ËøêÂä®È¢ÑÊµã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåUni-HandÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊâãÈÉ®ËøêÂä®È¢ÑÊµãÊ°ÜÊû∂Uni-HandÔºåÊó®Âú®Ëß£ÂÜ≥Á¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí‰∏ãÊâãÈÉ®ËøêÂä®È¢ÑÊµã‰∏≠È¢ÑÊµãÁõÆÊ†á‰∏çË∂≥„ÄÅÊ®°ÊÄÅÂ∑ÆË∑ù„ÄÅÊâã-Â§¥ËøêÂä®ËÄ¶Âêà‰ª•Âèä‰∏ãÊ∏∏‰ªªÂä°È™åËØÅÊúâÈôêÁ≠âÈóÆÈ¢ò„ÄÇUni-HandÈÄöËøáËßÜËßâ-ËØ≠Ë®ÄËûçÂêà„ÄÅÂÖ®Â±Ä‰∏ä‰∏ãÊñáËûçÂêà‰ª•Âèä‰ªªÂä°ÊÑüÁü•ÊñáÊú¨ÂµåÂÖ•Ê≥®ÂÖ•Êù•ÂçèË∞ÉÂ§öÊ®°ÊÄÅËæìÂÖ•Ôºå‰ªéËÄåÈ¢ÑÊµã2DÂíå3DÁ©∫Èó¥‰∏≠ÁöÑÊâãÈÉ®ËΩ®ËøπÁÇπ„ÄÇÊ≠§Â§ñÔºåËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÂàÜÊîØÊâ©Êï£Ê®°ÂûãÔºåÁî®‰∫éÂêåÊó∂È¢ÑÊµãÂ§¥ÈÉ®ÂíåÊâãÈÉ®ÁöÑËøêÂä®ÔºåÊçïÊçâÂÆÉ‰ª¨Âú®Á¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí‰∏≠ÁöÑËøêÂä®ÂçèÂêå„ÄÇÈÄöËøáÂºïÂÖ•ÁõÆÊ†áÊåáÁ§∫Âô®ÔºåËØ•Ê®°ÂûãÂèØ‰ª•È¢ÑÊµãÊâãËÖïÊàñÊâãÊåáÁöÑÁâπÂÆöÂÖ≥ËäÇËΩ®ËøπÁÇπÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÊâãÈÉ®‰∏≠ÂøÉÁÇπ„ÄÇUni-HandËøòËÉΩÂ§üÈ¢ÑÊµãÊâã-Áâ©‰∫§‰∫íÁä∂ÊÄÅÔºàÊé•Ëß¶/ÂàÜÁ¶ªÔºâÔºå‰ª•Êõ¥Â•ΩÂú∞‰øÉËøõ‰∏ãÊ∏∏‰ªªÂä°„ÄÇ‰Ωú‰∏∫È¶ñ‰∏™Â∞Ü‰∏ãÊ∏∏‰ªªÂä°ËØÑ‰º∞Á∫≥ÂÖ•ÊñáÁåÆÁöÑÂ∑•‰ΩúÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜÊù•ËØÑ‰º∞ÊâãÈÉ®ËøêÂä®È¢ÑÊµãÁÆóÊ≥ïÁöÑÂÆûÈôÖÂ∫îÁî®ÊÄß„ÄÇÂú®Â§ö‰∏™ÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÂíåÊàë‰ª¨Êñ∞ÊèêÂá∫ÁöÑÂü∫ÂáÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUni-HandÂú®Â§öÁª¥Â∫¶ÂíåÂ§öÁõÆÊ†áÊâãÈÉ®ËøêÂä®È¢ÑÊµãÊñπÈù¢ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÂú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÂπøÊ≥õÈ™åËØÅ‰πüÂ±ïÁ§∫‰∫ÜÂÖ∂‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑ‰∫∫-Êú∫Âô®‰∫∫Á≠ñÁï•ËøÅÁßªËÉΩÂäõÔºå‰ªéËÄåÂÆûÁé∞Êú∫Âô®‰∫∫Êìç‰ΩúÔºåÂπ∂ÊúâÊïàÂ¢ûÂº∫Âä®‰ΩúÈ¢ÑÊµã/ËØÜÂà´„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊâãÈÉ®ËΩ®ËøπÈ¢ÑÊµãÊñπÊ≥ïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®ÊâãÈÉ®‰∏≠ÂøÉÁÇπÁöÑÈ¢ÑÊµãÔºåÂøΩÁï•‰∫ÜÊâãÊåáÁ≠âÂÖ∂‰ªñÂÖ≥ÈîÆÈÉ®‰ΩçÁöÑËøêÂä®È¢ÑÊµã„ÄÇÊ≠§Â§ñÔºåÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí‰∏ãÊâãÈÉ®ÂíåÂ§¥ÈÉ®ÁöÑËøêÂä®Â≠òÂú®ËÄ¶ÂêàÂÖ≥Á≥ªÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàËß£ËÄ¶„ÄÇÂêåÊó∂ÔºåÁº∫‰πèÈíàÂØπ‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊúâÊïàÈ™åËØÅÔºåÈöæ‰ª•ËØÑ‰º∞ÁÆóÊ≥ïÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöUni-HandÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ÈÄöÁî®ÁöÑÊâãÈÉ®ËøêÂä®È¢ÑÊµãÊ°ÜÊû∂ÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§öÊ®°ÊÄÅËæìÂÖ•ÔºåÈ¢ÑÊµãÂ§öÁª¥Â∫¶ÁöÑÊâãÈÉ®ËøêÂä®ËΩ®ËøπÔºåÂπ∂ËÄÉËôëÊâãÈÉ®‰∏éÂ§¥ÈÉ®ÁöÑËøêÂä®ÂçèÂêå„ÄÇÈÄöËøáÂºïÂÖ•ÁõÆÊ†áÊåáÁ§∫Âô®ÔºåÂÆûÁé∞ÂØπ‰∏çÂêåÊâãÈÉ®ÂÖ≥ËäÇÁöÑÁ≤æÁªÜÂåñÈ¢ÑÊµã„ÄÇÂêåÊó∂ÔºåÈÄöËøáÈ¢ÑÊµãÊâã-Áâ©‰∫§‰∫íÁä∂ÊÄÅÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπ‰∏ãÊ∏∏‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöUni-HandÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨Â§öÊ®°ÊÄÅËûçÂêàÊ®°Âùó„ÄÅÂèåÂàÜÊîØÊâ©Êï£Ê®°ÂûãÂíå‰∏ãÊ∏∏‰ªªÂä°ËØÑ‰º∞Ê®°Âùó„ÄÇÂ§öÊ®°ÊÄÅËûçÂêàÊ®°ÂùóË¥üË¥£Êï¥ÂêàËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÔºåÊèêÂèñÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁâπÂæÅ„ÄÇÂèåÂàÜÊîØÊâ©Êï£Ê®°ÂûãÂêåÊó∂È¢ÑÊµãÂ§¥ÈÉ®ÂíåÊâãÈÉ®ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇ‰∏ãÊ∏∏‰ªªÂä°ËØÑ‰º∞Ê®°ÂùóÂàôÁî®‰∫éÈ™åËØÅÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöUni-HandÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÂàÜÊîØÊâ©Êï£Ê®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂È¢ÑÊµãÂ§¥ÈÉ®ÂíåÊâãÈÉ®ÁöÑËøêÂä®ÔºåÊçïÊçâÂÆÉ‰ª¨‰πãÈó¥ÁöÑËøêÂä®ÂçèÂêå„ÄÇ2) ÂºïÂÖ•‰∫ÜÁõÆÊ†áÊåáÁ§∫Âô®ÔºåÂÆûÁé∞‰∫ÜÂØπ‰∏çÂêåÊâãÈÉ®ÂÖ≥ËäÇÁöÑÁ≤æÁªÜÂåñÈ¢ÑÊµã„ÄÇ3) Â∞ÜÊâã-Áâ©‰∫§‰∫íÁä∂ÊÄÅÁöÑÈ¢ÑÊµãÁ∫≥ÂÖ•Ê°ÜÊû∂ÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂØπ‰∏ãÊ∏∏‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ4) ÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ÊâãÈÉ®ËøêÂä®È¢ÑÊµãÁÆóÊ≥ïÁöÑÂÆûÈôÖÂ∫îÁî®ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöUni-HandÈááÁî®‰∫ÜËßÜËßâ-ËØ≠Ë®ÄËûçÂêàÁ≠ñÁï•ÔºåÂà©Áî®TransformerÁΩëÁªúÊèêÂèñËßÜËßâÂíåËØ≠Ë®ÄÁâπÂæÅÔºåÂπ∂ÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åËûçÂêà„ÄÇÂèåÂàÜÊîØÊâ©Êï£Ê®°ÂûãÈááÁî®‰∫ÜU-NetÁªìÊûÑÔºåÂàÜÂà´È¢ÑÊµãÂ§¥ÈÉ®ÂíåÊâãÈÉ®ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÁõÆÊ†áÊåáÁ§∫Âô®ÈÄöËøáone-hotÁºñÁ†ÅË°®Á§∫‰∏çÂêåÁöÑÊâãÈÉ®ÂÖ≥ËäÇ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ËΩ®ËøπÈ¢ÑÊµãÊçüÂ§±„ÄÅÊâã-Áâ©‰∫§‰∫íÁä∂ÊÄÅÈ¢ÑÊµãÊçüÂ§±ÂíåÂØπÊäóÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Uni-HandÂú®Â§ö‰∏™ÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÂíåÊñ∞ÊèêÂá∫ÁöÑÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®ÊâãÈÉ®ËΩ®ËøπÈ¢ÑÊµã‰ªªÂä°‰∏≠ÔºåUni-HandÁöÑÂπ≥ÂùáÈ¢ÑÊµãËØØÂ∑ÆÊØîÁé∞ÊúâÊñπÊ≥ïÈôç‰Ωé‰∫Ü15%„ÄÇÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåUni-HandËÉΩÂ§üÊàêÂäüÂÆåÊàêÊäìÂèñ„ÄÅÊîæÁΩÆÁ≠âÂ§çÊùÇÊìç‰ΩúÔºåÊàêÂäüÁéáÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü20%„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUni-HandÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Uni-HandÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂ¢ûÂº∫Áé∞ÂÆû„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅÊú∫Âô®‰∫∫Êìç‰ΩúÂíåÂä®‰ΩúÈ¢ÑÊµãÁ≠â„ÄÇÂú®Â¢ûÂº∫Áé∞ÂÆû‰∏≠ÔºåÂèØ‰ª•Âà©Áî®Uni-HandÈ¢ÑÊµãÁî®Êà∑ÁöÑÊâãÈÉ®ËøêÂä®Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Ëá™ÁÑ∂ÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÂèØ‰ª•Â∞ÜUni-HandÈ¢ÑÊµãÁöÑÊâãÈÉ®ËøêÂä®ËΩ®Ëøπ‰Ωú‰∏∫Êú∫Âô®‰∫∫ÁöÑÊéßÂà∂Êåá‰ª§ÔºåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇÊ≠§Â§ñÔºåUni-HandËøòÂèØ‰ª•Áî®‰∫éÂä®‰ΩúÈ¢ÑÊµãÔºå‰æãÂ¶ÇÈ¢ÑÊµãÁî®Êà∑‰∏ã‰∏ÄÊ≠•Ë¶ÅÊâßË°åÁöÑÂä®‰ΩúÔºå‰ªéËÄåÊèê‰æõÊõ¥Êô∫ËÉΩÁöÑÊúçÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.

