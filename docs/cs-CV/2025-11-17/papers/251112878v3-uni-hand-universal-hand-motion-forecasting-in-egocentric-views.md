---
layout: default
title: Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views
---

# Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views

**arXiv**: [2511.12878v3](https://arxiv.org/abs/2511.12878) | [PDF](https://arxiv.org/pdf/2511.12878.pdf)

**ä½œè€…**: Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Yu Zheng, Erhang Zhang, Xieyuanli Chen, Hesheng Wang

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17 (æ›´æ–°: 2025-12-05)

**å¤‡æ³¨**: Extended journal version of MMTwin (IROS'25). Code and data: https://github.com/IRMVLab/UniHand

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Uni-Handï¼šç”¨äºŽç¬¬ä¸€äººç§°è§†è§’çš„é€šç”¨æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æž¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹` `ç¬¬ä¸€äººç§°è§†è§’` `å¤šæ¨¡æ€èžåˆ` `æ‰©æ•£æ¨¡åž‹` `äººæœºäº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹æ–¹æ³•åœ¨é¢„æµ‹ç›®æ ‡ã€æ¨¡æ€èžåˆã€è¿åŠ¨è§£è€¦å’Œä¸‹æ¸¸ä»»åŠ¡éªŒè¯æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. Uni-Handé€šè¿‡å¤šæ¨¡æ€èžåˆã€åŒåˆ†æ”¯æ‰©æ•£å’Œç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œå®žçŽ°å¤šç»´åº¦ã€å¤šç›®æ ‡çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒUni-Handåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•çŽ°äº†è‰¯å¥½çš„åº”ç”¨æ½œåŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æž¶Uni-Handï¼Œæ—¨åœ¨è§£å†³ç¬¬ä¸€äººç§°è§†è§’ä¸‹æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ä¸­é¢„æµ‹ç›®æ ‡ä¸è¶³ã€æ¨¡æ€å·®è·ã€æ‰‹-å¤´è¿åŠ¨è€¦åˆä»¥åŠä¸‹æ¸¸ä»»åŠ¡éªŒè¯æœ‰é™ç­‰é—®é¢˜ã€‚Uni-Handé€šè¿‡è§†è§‰-è¯­è¨€èžåˆã€å…¨å±€ä¸Šä¸‹æ–‡èžåˆä»¥åŠä»»åŠ¡æ„ŸçŸ¥æ–‡æœ¬åµŒå…¥æ³¨å…¥æ¥åè°ƒå¤šæ¨¡æ€è¾“å…¥ï¼Œä»Žè€Œé¢„æµ‹2Då’Œ3Dç©ºé—´ä¸­çš„æ‰‹éƒ¨è½¨è¿¹ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯æ‰©æ•£æ¨¡åž‹ï¼Œç”¨äºŽåŒæ—¶é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨ï¼Œæ•æ‰å®ƒä»¬åœ¨ç¬¬ä¸€äººç§°è§†è§’ä¸­çš„è¿åŠ¨ååŒã€‚é€šè¿‡å¼•å…¥ç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œè¯¥æ¨¡åž‹å¯ä»¥é¢„æµ‹æ‰‹è…•æˆ–æ‰‹æŒ‡çš„ç‰¹å®šå…³èŠ‚è½¨è¿¹ç‚¹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‰‹éƒ¨ä¸­å¿ƒç‚¹ã€‚Uni-Handè¿˜èƒ½å¤Ÿé¢„æµ‹æ‰‹-ç‰©äº¤äº’çŠ¶æ€ï¼ˆæŽ¥è§¦/åˆ†ç¦»ï¼‰ï¼Œä»¥æ›´å¥½åœ°ä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡ã€‚ä½œä¸ºé¦–ä¸ªå°†ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°çº³å…¥æ–‡çŒ®çš„å·¥ä½œï¼Œæˆ‘ä»¬æž„å»ºäº†æ–°çš„åŸºå‡†æ¥è¯„ä¼°æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ç®—æ³•çš„å®žé™…åº”ç”¨æ€§ã€‚åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†å’Œæˆ‘ä»¬æ–°æå‡ºçš„åŸºå‡†ä¸Šçš„å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒUni-Handåœ¨å¤šç»´åº¦å’Œå¤šç›®æ ‡æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ–¹é¢å®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¹¿æ³›éªŒè¯ä¹Ÿå±•ç¤ºäº†å…¶ä»¤äººå°è±¡æ·±åˆ»çš„äºº-æœºå™¨äººç­–ç•¥è¿ç§»èƒ½åŠ›ï¼Œä»Žè€Œå®žçŽ°æœºå™¨äººæ“ä½œï¼Œå¹¶æœ‰æ•ˆå¢žå¼ºåŠ¨ä½œé¢„æµ‹/è¯†åˆ«ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹æ–¹æ³•é€šå¸¸åªå…³æ³¨æ‰‹éƒ¨ä¸­å¿ƒç‚¹çš„é¢„æµ‹ï¼Œå¿½ç•¥äº†æ‰‹æŒ‡ç­‰å…¶ä»–å…³é”®éƒ¨ä½çš„è¿åŠ¨é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç¬¬ä¸€äººç§°è§†è§’ä¸‹æ‰‹éƒ¨å’Œå¤´éƒ¨çš„è¿åŠ¨å­˜åœ¨è€¦åˆå…³ç³»ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£è€¦ã€‚åŒæ—¶ï¼Œç¼ºä¹é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆéªŒè¯ï¼Œéš¾ä»¥è¯„ä¼°ç®—æ³•çš„å®žé™…åº”ç”¨ä»·å€¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni-Handçš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªé€šç”¨çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æž¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œé¢„æµ‹å¤šç»´åº¦çš„æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ï¼Œå¹¶è€ƒè™‘æ‰‹éƒ¨ä¸Žå¤´éƒ¨çš„è¿åŠ¨ååŒã€‚é€šè¿‡å¼•å…¥ç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œå®žçŽ°å¯¹ä¸åŒæ‰‹éƒ¨å…³èŠ‚çš„ç²¾ç»†åŒ–é¢„æµ‹ã€‚åŒæ—¶ï¼Œé€šè¿‡é¢„æµ‹æ‰‹-ç‰©äº¤äº’çŠ¶æ€ï¼Œå¢žå¼ºæ¨¡åž‹å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šUni-Handçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬å¤šæ¨¡æ€èžåˆæ¨¡å—ã€åŒåˆ†æ”¯æ‰©æ•£æ¨¡åž‹å’Œä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°æ¨¡å—ã€‚å¤šæ¨¡æ€èžåˆæ¨¡å—è´Ÿè´£æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œæå–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚åŒåˆ†æ”¯æ‰©æ•£æ¨¡åž‹åŒæ—¶é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨è½¨è¿¹ã€‚ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°æ¨¡å—åˆ™ç”¨äºŽéªŒè¯æ¨¡åž‹åœ¨å®žé™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šUni-Handçš„å…³é”®åˆ›æ–°åœ¨äºŽä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ‰©æ•£æ¨¡åž‹ï¼Œèƒ½å¤ŸåŒæ—¶é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨ï¼Œæ•æ‰å®ƒä»¬ä¹‹é—´çš„è¿åŠ¨ååŒã€‚2) å¼•å…¥äº†ç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œå®žçŽ°äº†å¯¹ä¸åŒæ‰‹éƒ¨å…³èŠ‚çš„ç²¾ç»†åŒ–é¢„æµ‹ã€‚3) å°†æ‰‹-ç‰©äº¤äº’çŠ¶æ€çš„é¢„æµ‹çº³å…¥æ¡†æž¶ï¼Œå¢žå¼ºäº†æ¨¡åž‹å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚4) æž„å»ºäº†æ–°çš„åŸºå‡†ï¼Œç”¨äºŽè¯„ä¼°æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ç®—æ³•çš„å®žé™…åº”ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šUni-Handé‡‡ç”¨äº†è§†è§‰-è¯­è¨€èžåˆç­–ç•¥ï¼Œåˆ©ç”¨Transformerç½‘ç»œæå–è§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèžåˆã€‚åŒåˆ†æ”¯æ‰©æ•£æ¨¡åž‹é‡‡ç”¨äº†U-Netç»“æž„ï¼Œåˆ†åˆ«é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨è½¨è¿¹ã€‚ç›®æ ‡æŒ‡ç¤ºå™¨é€šè¿‡one-hotç¼–ç è¡¨ç¤ºä¸åŒçš„æ‰‹éƒ¨å…³èŠ‚ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è½¨è¿¹é¢„æµ‹æŸå¤±ã€æ‰‹-ç‰©äº¤äº’çŠ¶æ€é¢„æµ‹æŸå¤±å’Œå¯¹æŠ—æŸå¤±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

Uni-Handåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†å’Œæ–°æå‡ºçš„åŸºå‡†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒUni-Handçš„å¹³å‡é¢„æµ‹è¯¯å·®æ¯”çŽ°æœ‰æ–¹æ³•é™ä½Žäº†15%ã€‚åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒUni-Handèƒ½å¤ŸæˆåŠŸå®ŒæˆæŠ“å–ã€æ”¾ç½®ç­‰å¤æ‚æ“ä½œï¼ŒæˆåŠŸçŽ‡æ¯”çŽ°æœ‰æ–¹æ³•æé«˜äº†20%ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒUni-Handå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®žé™…åº”ç”¨ä»·å€¼ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

Uni-Handå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¢žå¼ºçŽ°å®žã€äººæœºäº¤äº’ã€æœºå™¨äººæ“ä½œå’ŒåŠ¨ä½œé¢„æµ‹ç­‰ã€‚åœ¨å¢žå¼ºçŽ°å®žä¸­ï¼Œå¯ä»¥åˆ©ç”¨Uni-Handé¢„æµ‹ç”¨æˆ·çš„æ‰‹éƒ¨è¿åŠ¨ï¼Œä»Žè€Œå®žçŽ°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œå¯ä»¥å°†Uni-Handé¢„æµ‹çš„æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ä½œä¸ºæœºå™¨äººçš„æŽ§åˆ¶æŒ‡ä»¤ï¼Œå®žçŽ°æ›´ç²¾ç¡®çš„æœºå™¨äººæ“ä½œã€‚æ­¤å¤–ï¼ŒUni-Handè¿˜å¯ä»¥ç”¨äºŽåŠ¨ä½œé¢„æµ‹ï¼Œä¾‹å¦‚é¢„æµ‹ç”¨æˆ·ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„åŠ¨ä½œï¼Œä»Žè€Œæä¾›æ›´æ™ºèƒ½çš„æœåŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.

