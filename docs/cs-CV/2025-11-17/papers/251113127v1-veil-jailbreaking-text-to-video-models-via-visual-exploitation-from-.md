---
layout: default
title: VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language
---

# VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language

**arXiv**: [2511.13127v1](https://arxiv.org/abs/2511.13127) | [PDF](https://arxiv.org/pdf/2511.13127.pdf)

**ä½œè€…**: Zonghao Ying, Moyang Chen, Nizhang Li, Zhiqiang Wang, Wenxin Zhang, Quanchen Zou, Zonglei Jing, Aishan Liu, Xianglong Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVEILæ¡†æž¶ï¼Œé€šè¿‡éšå¼è¯­è¨€è§†è§‰åˆ©ç”¨å®žçŽ°æ–‡æœ¬åˆ°è§†é¢‘æ¨¡åž‹çš„è¶Šç‹±æ”»å‡»**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘æ¨¡åž‹` `è¶Šç‹±æ”»å‡»` `æ¨¡å—åŒ–æç¤ºè®¾è®¡` `è·¨æ¨¡æ€å…³è”` `å®‰å…¨æ¼æ´ž` `éšå¼è¯­è¨€åˆ©ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿè¶Šç‹±æ”»å‡»æ˜“è¢«æ£€æµ‹ï¼Œéœ€éšè”½è¯±å¯¼æ¨¡åž‹ç”Ÿæˆè¿åå®‰å…¨ç­–ç•¥çš„è§†é¢‘
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä¸­æ€§åœºæ™¯é”šç‚¹ã€æ½œåœ¨å¬è§‰è§¦å‘å™¨å’Œé£Žæ ¼è°ƒåˆ¶å™¨è®¾è®¡æ¨¡å—åŒ–æç¤º
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨7ä¸ªT2Væ¨¡åž‹ä¸­æµ‹è¯•ï¼Œå•†ä¸šæ¨¡åž‹å¹³å‡æ”»å‡»æˆåŠŸçŽ‡æå‡23%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

