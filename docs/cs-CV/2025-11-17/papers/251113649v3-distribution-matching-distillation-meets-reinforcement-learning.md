---
layout: default
title: Distribution Matching Distillation Meets Reinforcement Learning
---

# Distribution Matching Distillation Meets Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.13649" target="_blank" class="toolbar-btn">arXiv: 2511.13649v3</a>
    <a href="https://arxiv.org/pdf/2511.13649.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13649v3" 
            onclick="toggleFavorite(this, '2511.13649v3', 'Distribution Matching Distillation Meets Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Dengyang Jiang, Dongyang Liu, Zanyi Wang, Qilong Wu, Liuzhuozheng Li, Hengzhuang Li, Xin Jin, David Liu, Zhen Li, Bo Zhang, Mengmeng Wang, Steven Hoi, Peng Gao, Harry Yang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17 (Êõ¥Êñ∞: 2025-12-08)

**Â§áÊ≥®**: The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DMDRÊ°ÜÊû∂ÔºåÁªìÂêàÂº∫ÂåñÂ≠¶‰π†‰∏éÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÔºåÊèêÂçáÂ∞ëÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêË¥®Èáè„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `ÂàÜÂ∏ÉÂåπÈÖçËí∏È¶è` `Âº∫ÂåñÂ≠¶‰π†` `ÂõæÂÉèÁîüÊàê` `Â∞ëÊ≠•Êé®ÁêÜ` `Ê®°ÂûãÂéãÁº©` `Á≠ñÁï•‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÔºàDMDÔºâÂèóÈôê‰∫éÊïôÂ∏àÊ®°ÂûãÊÄßËÉΩÔºåÊó†Ê≥ïÂÖÖÂàÜÂèëÊå•Â∞ëÊ≠•ÁîüÊàêÂô®ÁöÑÊΩúÂäõ„ÄÇ
2. DMDRÊ°ÜÊû∂ÁªìÂêàÂº∫ÂåñÂ≠¶‰π†‰∏éDMDÔºåÂà©Áî®DMDÊçüÂ§±‰Ωú‰∏∫Ê≠£ÂàôÂåñÈ°πÔºåÂπ∂Áî®RLÊåáÂØºÊ®°ÂºèË¶ÜÁõñÔºåËß£ÈîÅÂ∞ëÊ≠•ÁîüÊàêÂô®ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDMDRÂú®ËßÜËßâË¥®ÈáèÂíåÊèêÁ§∫‰∏ÄËá¥ÊÄßÊñπÈù¢Ë°®Áé∞È¢ÜÂÖàÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÂ§öÊ≠•ÊïôÂ∏àÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DMDRÁöÑÊñ∞Ê°ÜÊû∂ÔºåÂÆÉÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊäÄÊúØËûçÂÖ•Âà∞ÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÔºàDMDÔºâËøáÁ®ã‰∏≠„ÄÇDMDÊó®Âú®Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂ§öÊ≠•Êâ©Êï£Ê®°ÂûãÊèêÁÇº‰∏∫Â∞ëÊ≠•Ê®°ÂûãÔºå‰ª•ÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºå‰ΩÜÂÖ∂ÊÄßËÉΩÈÄöÂ∏∏ÂèóÈôê‰∫éÊïôÂ∏àÊ®°Âûã„ÄÇDMDRÈÄöËøáÂ∞ÜDMDÊçüÂ§±Êú¨Ë∫´‰Ωú‰∏∫Â∞ëÊ≠•ÁîüÊàêÂô®RLÁöÑÊúâÊïàÊ≠£ÂàôÂåñÈ°πÔºåÂÖãÊúç‰∫ÜËøô‰∏ÄÈôêÂà∂„ÄÇÂèçËøáÊù•ÔºåRLÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞ÊåáÂØºDMD‰∏≠ÁöÑÊ®°ÂºèË¶ÜÁõñËøáÁ®ã„ÄÇËøô‰ΩøÂæóÊàë‰ª¨ËÉΩÂ§üÈÄöËøáÂêåÊó∂ËøõË°åËí∏È¶èÂíåRLÊù•ÈáäÊîæÂ∞ëÊ≠•ÁîüÊàêÂô®ÁöÑËÉΩÂäõ„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÂä®ÊÄÅÂàÜÂ∏ÉÊåáÂØºÂíåÂä®ÊÄÅÈáçÂô™Â£∞ÈááÊ†∑ËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ª•ÊîπÂñÑÂàùÂßãËí∏È¶èËøáÁ®ã„ÄÇÂÆûÈ™åË°®ÊòéÔºåDMDRÂèØ‰ª•Âú®Â∞ëÊ≠•ÊñπÊ≥ï‰∏≠ÂÆûÁé∞È¢ÜÂÖàÁöÑËßÜËßâË¥®ÈáèÂíåÊèêÁ§∫‰∏ÄËá¥ÊÄßÔºåÁîöËá≥Ë°®Áé∞Âá∫Ë∂ÖËøáÂ§öÊ≠•ÊïôÂ∏àÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÔºàDMDÔºâÊñπÊ≥ïÔºåËôΩÁÑ∂ËÉΩÂ§üÂ∞ÜÂ§öÊ≠•Êâ©Êï£Ê®°ÂûãÂéãÁº©‰∏∫Â∞ëÊ≠•Ê®°Âûã‰ª•ÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩÜÂ∞ëÊ≠•Ê®°ÂûãÁöÑÊÄßËÉΩÂæÄÂæÄÂèóÂà∞È¢ÑËÆ≠ÁªÉÁöÑÂ§öÊ≠•ÊïôÂ∏àÊ®°ÂûãÁöÑÈôêÂà∂ÔºåÈöæ‰ª•Á™ÅÁ†¥ÊïôÂ∏àÊ®°ÂûãÁöÑÊÄßËÉΩ‰∏äÈôê„ÄÇÁé∞ÊúâÁöÑDMDÊñπÊ≥ïÂú®Ê®°ÂºèË¶ÜÁõñÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥ÁîüÊàêÁªìÊûúÁöÑÂ§öÊ†∑ÊÄßÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDMDRÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂºïÂÖ•Âà∞DMDËøáÁ®ã‰∏≠ÔºåÂà©Áî®RLÊù•‰ºòÂåñÂ∞ëÊ≠•ÁîüÊàêÂô®ÁöÑÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÁîüÊàêÊõ¥È´òË¥®Èáè„ÄÅÊõ¥Â§öÊ†∑ÊÄßÁöÑÂõæÂÉè„ÄÇÂêåÊó∂ÔºåÂ∞ÜDMDÊçüÂ§±Êú¨Ë∫´‰Ωú‰∏∫RLÁöÑÊ≠£ÂàôÂåñÈ°πÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊ≠£ÂàôÂåñÊñπÊ≥ïÂèØËÉΩÂºïÂÖ•ÁöÑÂÅèÂ∑Æ„ÄÇÈÄöËøáDMDÂíåRLÁöÑÂçèÂêå‰ΩúÁî®ÔºåDMDRËÉΩÂ§üÁ™ÅÁ†¥ÊïôÂ∏àÊ®°ÂûãÁöÑÊÄßËÉΩÈôêÂà∂ÔºåÂÖÖÂàÜÂèëÊå•Â∞ëÊ≠•ÁîüÊàêÂô®ÁöÑÊΩúÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDMDRÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºö1) **DMDËí∏È¶èÊ®°Âùó**Ôºö‰ΩøÁî®ÂàÜÂ∏ÉÂåπÈÖçÊçüÂ§±Â∞ÜÂ§öÊ≠•ÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â∞ëÊ≠•Â≠¶ÁîüÊ®°Âûã„ÄÇ2) **RL‰ºòÂåñÊ®°Âùó**Ôºö‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºà‰æãÂ¶ÇÔºåPPOÔºâ‰ºòÂåñÂ∞ëÊ≠•ÁîüÊàêÂô®ÁöÑÁ≠ñÁï•ÔºåÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ3) **Âä®ÊÄÅÂàÜÂ∏ÉÊåáÂØºÊ®°Âùó**ÔºöÂú®ÂàùÂßãËí∏È¶èÈò∂ÊÆµÔºåÂä®ÊÄÅË∞ÉÊï¥ÂàÜÂ∏ÉÊåáÂØºÁöÑÂº∫Â∫¶Ôºå‰ª•ÊîπÂñÑËí∏È¶èÊïàÊûú„ÄÇ4) **Âä®ÊÄÅÈáçÂô™Â£∞ÈááÊ†∑Ê®°Âùó**ÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂä®ÊÄÅË∞ÉÊï¥Âô™Â£∞ÁöÑÈááÊ†∑Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊï¥‰∏™ËÆ≠ÁªÉËøáÁ®ãÊòØDMDËí∏È¶èÂíåRL‰ºòÂåñ‰∫§ÊõøËøõË°åÔºåÁõ∏‰∫í‰øÉËøõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDMDRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂº∫ÂåñÂ≠¶‰π†‰∏éÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÁõ∏ÁªìÂêàÔºåÂπ∂Âà©Áî®DMDÊçüÂ§±‰Ωú‰∏∫RLÁöÑÊ≠£ÂàôÂåñÈ°π„ÄÇËøôÁßçÁªìÂêàÊñπÂºèËÉΩÂ§üÊúâÊïàÂú∞ÂÖãÊúç‰º†ÁªüDMDÊñπÊ≥ïÁöÑÊÄßËÉΩÁì∂È¢àÔºåÂπ∂ÂÖÖÂàÜÂèëÊå•Â∞ëÊ≠•ÁîüÊàêÂô®ÁöÑÊΩúÂäõ„ÄÇÊ≠§Â§ñÔºåÂä®ÊÄÅÂàÜÂ∏ÉÊåáÂØºÂíåÂä®ÊÄÅÈáçÂô™Â£∞ÈááÊ†∑Á≠ñÁï•‰πüËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®RL‰ºòÂåñÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®‰∫ÜProximal Policy Optimization (PPO) ÁÆóÊ≥ï„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈÄöÂ∏∏ÁªìÂêà‰∫ÜÂõæÂÉèË¥®ÈáèËØÑ‰º∞ÊåáÊ†áÔºà‰æãÂ¶ÇÔºåFID„ÄÅInception ScoreÔºâÂíåÊèêÁ§∫‰∏ÄËá¥ÊÄßÊåáÊ†á„ÄÇDMDÊçüÂ§±Ë¢´Áî®‰ΩúRLÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ª•Á∫¶ÊùüÁ≠ñÁï•ÁöÑÊõ¥Êñ∞ÊñπÂêëÔºåÈÅøÂÖçÁîüÊàêÂô®ÂÅèÁ¶ªÊïôÂ∏àÊ®°ÂûãÁöÑÂàÜÂ∏ÉÂ§™Ëøú„ÄÇÂä®ÊÄÅÂàÜÂ∏ÉÊåáÂØºÊ®°ÂùóÈÄöËøáË∞ÉÊï¥ÂàÜÂ∏ÉÂåπÈÖçÊçüÂ§±ÁöÑÊùÉÈáçÊù•ÂÆûÁé∞ÔºåÂä®ÊÄÅÈáçÂô™Â£∞ÈááÊ†∑Ê®°ÂùóÂàôÈÄöËøáË∞ÉÊï¥Âô™Â£∞ÁöÑÈááÊ†∑ËåÉÂõ¥Êù•ÂÆûÁé∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDMDRÂú®ËßÜËßâË¥®ÈáèÂíåÊèêÁ§∫‰∏ÄËá¥ÊÄßÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÁöÑÂ∞ëÊ≠•Êâ©Êï£Ê®°Âûã„ÄÇÂú®ÂõæÂÉèÁîüÊàê‰ªªÂä°‰∏≠ÔºåDMDRÁîüÊàêÁöÑÂõæÂÉèÁöÑFIDÂæóÂàÜÊòæËëó‰Ωé‰∫éÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ïÔºåË°®ÊòéÂÖ∂ÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÊõ¥È´ò„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåDMDRÁîöËá≥ËÉΩÂ§üË∂ÖË∂äÂ§öÊ≠•ÊïôÂ∏àÊ®°ÂûãÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂ËÉΩÂ§üÊúâÊïàÁ™ÅÁ†¥ÊïôÂ∏àÊ®°ÂûãÁöÑÊÄßËÉΩÈôêÂà∂„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁâπÂÆöÊï∞ÊçÆÈõÜ‰∏äÔºåDMDRÁöÑFIDÂæóÂàÜÊØîÊïôÂ∏àÊ®°ÂûãÈôç‰Ωé‰∫Ü5%‰ª•‰∏ä„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DMDRÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂõæÂÉèÁîüÊàê„ÄÅÂõæÂÉèÁºñËæë„ÄÅËßÜÈ¢ëÁîüÊàêÁ≠â„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁîüÊàêÈ´òË¥®Èáè„ÄÅÈ´òÂàÜËæ®ÁéáÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÔºåÂπ∂ÂèØ‰ª•Â∫îÁî®‰∫éÊ∏∏ÊàèÂºÄÂèë„ÄÅÁîµÂΩ±Âà∂‰Ωú„ÄÅÂπøÂëäËÆæËÆ°Á≠âÈ¢ÜÂüü„ÄÇÊ≠§Â§ñÔºåDMDRËøòÂèØ‰ª•Áî®‰∫éÊï∞ÊçÆÂ¢ûÂº∫ÔºåÈÄöËøáÁîüÊàêÊñ∞ÁöÑËÆ≠ÁªÉÊ†∑Êú¨Êù•ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁî±‰∫éÂÖ∂Â∞ëÊ≠•Êé®ÁêÜÁöÑÁâπÊÄßÔºåDMDRÂú®ÂØπÂÆûÊó∂ÊÄßË¶ÅÊ±ÇËæÉÈ´òÁöÑÂú∫ÊôØ‰∏ãÂÖ∑Êúâ‰ºòÂäø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

