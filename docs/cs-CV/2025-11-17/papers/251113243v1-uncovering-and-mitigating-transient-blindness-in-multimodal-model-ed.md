---
layout: default
title: Uncovering and Mitigating Transient Blindness in Multimodal Model Editing
---

# Uncovering and Mitigating Transient Blindness in Multimodal Model Editing

**arXiv**: [2511.13243v1](https://arxiv.org/abs/2511.13243) | [PDF](https://arxiv.org/pdf/2511.13243.pdf)

**ä½œè€…**: Xiaoqi Han, Ru Li, Ran Yi, Hongye Tan, Zhuomin Liang, VÃ­ctor GutiÃ©rrez-Basulto, Jeff Z. Pan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ¨¡åž‹ç¼–è¾‘çš„å±€éƒ¨æ€§è¯„ä¼°æ¡†æž¶å’Œå¯¹æŠ—æŸå¤±ä»¥ç¼“è§£çž¬æ€ç›²è§†**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡åž‹ç¼–è¾‘` `çž¬æ€ç›²è§†` `å±€éƒ¨æ€§è¯„ä¼°` `è§†è§‰é—®ç­”` `å¯¹æŠ—è®­ç»ƒ` `è·¨æ¨¡æ€è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šæ¨¡æ€æ¨¡åž‹ç¼–è¾‘è¯„ä¼°æ–¹æ³•é«˜ä¼°æˆåŠŸï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆå’Œçž¬æ€ç›²è§†çŽ°è±¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å±€éƒ¨æ€§è¯„ä¼°æ¡†æž¶å’ŒDe-VQAåŠ¨æ€è¯„ä¼°ï¼Œä½¿ç”¨å¯¹æŠ—æŸå¤±å¹³è¡¡è·¨æ¨¡æ€è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ–¹æ³•ä¼˜äºŽåŸºçº¿ï¼Œå¹³å‡å‡å°‘çž¬æ€ç›²è§†å¹¶æå‡å±€éƒ¨æ€§17%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

