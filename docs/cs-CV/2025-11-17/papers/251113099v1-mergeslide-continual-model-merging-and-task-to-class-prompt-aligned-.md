---
layout: default
title: MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images
---

# MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images

**arXiv**: [2511.13099v1](https://arxiv.org/abs/2511.13099) | [PDF](https://arxiv.org/pdf/2511.13099.pdf)

**ä½œè€…**: Doanh C. Bui, Ba Hung Ngo, Hoai Luan Pham, Khang Nguyen, MaÃ¯ K. Nguyen, Yasuhiko Nakashima

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMergeSlideæ¡†æž¶ï¼Œé€šè¿‡æ¨¡åž‹åˆå¹¶å’Œæç¤ºå¯¹é½æŽ¨ç†è§£å†³å…¨åˆ‡ç‰‡å›¾åƒç»ˆèº«å­¦ä¹ é—®é¢˜**

**å…³é”®è¯**: `ç»ˆèº«å­¦ä¹ ` `å…¨åˆ‡ç‰‡å›¾åƒ` `æ¨¡åž‹åˆå¹¶` `æç¤ºå¯¹é½` `ç¾éš¾æ€§é—å¿˜` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨åˆ‡ç‰‡å›¾åƒç»ˆèº«å­¦ä¹ ä¸­èµ„æºæ¶ˆè€—å¤§ä¸”æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨æ­£äº¤åˆå¹¶ç­–ç•¥å’Œä»»åŠ¡åˆ°ç±»æç¤ºå¯¹é½æŽ¨ç†å®žçŽ°æ¨¡åž‹ç»Ÿä¸€
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨TCGAæ•°æ®é›†ä¸Šä¼˜äºŽåŸºäºŽæŽ’ç»ƒå’Œé›¶æ ·æœ¬çš„åŸºçº¿æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

