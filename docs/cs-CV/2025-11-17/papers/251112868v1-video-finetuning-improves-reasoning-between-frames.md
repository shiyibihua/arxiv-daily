---
layout: default
title: Video Finetuning Improves Reasoning Between Frames
---

# Video Finetuning Improves Reasoning Between Frames

**arXiv**: [2511.12868v1](https://arxiv.org/abs/2511.12868) | [PDF](https://arxiv.org/pdf/2511.12868.pdf)

**ä½œè€…**: Ruiqi Yang, Tian Yun, Zihan Wang, Ellie Pavlick

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰æ€ç»´é“¾ä»¥æå‡å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨è§†é¢‘ä¸­çš„å¸§é—´æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†é¢‘å¾®è°ƒ` `è§†è§‰æ€ç»´é“¾` `å¸§é—´æŽ¨ç†` `é•¿è§†é¢‘é—®ç­”` `è§†è§‰æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§æ¨¡åž‹ä»Žå›¾åƒæ‰©å±•åˆ°è§†é¢‘æ—¶ï¼Œå¸¸ç®€å•æ‹¼æŽ¥å¸§ä»¤ç‰Œï¼Œç¼ºä¹å¸§é—´æŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†è§‰æ€ç»´é“¾ï¼Œç”Ÿæˆè¿žç»­å¸§é—´çš„è¿‡æ¸¡äº‹ä»¶æè¿°ï¼Œç”¨äºŽç³»ç»Ÿæ¯”è¾ƒæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè§†é¢‘å¾®è°ƒæ¨¡åž‹åœ¨é•¿è§†é¢‘é—®ç­”ä¸­è¡¨çŽ°æ›´ä¼˜ï¼Œå¹¶èƒ½è¿ç§»åˆ°é™æ€è§†è§‰æŽ¨ç†ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

