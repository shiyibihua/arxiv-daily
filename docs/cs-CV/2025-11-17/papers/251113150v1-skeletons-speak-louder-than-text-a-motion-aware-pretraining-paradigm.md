---
layout: default
title: Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification
---

# Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification

**arXiv**: [2511.13150v1](https://arxiv.org/abs/2511.13150) | [PDF](https://arxiv.org/pdf/2511.13150.pdf)

**ä½œè€…**: Rifen Lin, Alex Jinpeng Wang, Jiawei Mo, Min Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéª¨æž¶é©±åŠ¨çš„é¢„è®­ç»ƒæ¡†æž¶CSIP-ReIDï¼Œä»¥è§£å†³è§†é¢‘è¡Œäººé‡è¯†åˆ«ä¸­æ–‡æœ¬æ¨¡æ€æ— æ³•æ•æ‰ç»†ç²’åº¦è¿åŠ¨çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘è¡Œäººé‡è¯†åˆ«` `éª¨æž¶åºåˆ—` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€é¢„è®­ç»ƒ` `æ—¶åºå»ºæ¨¡` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬æ¨¡æ€åœ¨è§†é¢‘è¡Œäººé‡è¯†åˆ«ä¸­éš¾ä»¥æ•æ‰ç»†ç²’åº¦æ—¶é—´è¿åŠ¨ï¼Œå¯¼è‡´çŽ°æœ‰æ–¹æ³•ç¼ºä¹çœŸæ­£å¤šæ¨¡æ€é¢„è®­ç»ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½éª¨æž¶ä¸Žè§†è§‰ç‰¹å¾ï¼Œå¹¶å¼•å…¥åŽŸåž‹èžåˆæ›´æ–°å™¨å’Œéª¨æž¶å¼•å¯¼æ—¶åºå»ºæ¨¡æ¨¡å—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†è§†é¢‘ReIDåŸºå‡†ä¸Šè¾¾åˆ°æ–°SOTAï¼Œå¹¶åœ¨éª¨æž¶ReIDä»»åŠ¡ä¸­è¡¨çŽ°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

