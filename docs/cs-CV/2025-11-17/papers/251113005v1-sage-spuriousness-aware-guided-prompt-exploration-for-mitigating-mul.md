---
layout: default
title: SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias
---

# SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias

**arXiv**: [2511.13005v1](https://arxiv.org/abs/2511.13005) | [PDF](https://arxiv.org/pdf/2511.13005.pdf)

**ä½œè€…**: Wenqian Ye, Di Wang, Guangtao Zheng, Bohan Liu, Aidong Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAGEæ–¹æ³•ä»¥ç¼“è§£å¤šæ¨¡æ€è™šå‡åè§ï¼Œæå‡é›¶æ ·æœ¬åˆ†ç±»é²æ£’æ€§**

**å…³é”®è¯**: `å¤šæ¨¡æ€åè§ç¼“è§£` `é›¶æ ·æœ¬åˆ†ç±»` `æç¤ºå·¥ç¨‹` `CLIPæ¨¡åž‹` `é²æ£’æ€§æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPæ¨¡åž‹å­˜åœ¨å¤šæ¨¡æ€è™šå‡åè§ï¼Œä¾èµ–è™šå‡ç‰¹å¾å¦‚èƒŒæ™¯è€Œéžæ ¸å¿ƒå¯¹è±¡ç‰¹å¾
2. SAGEé€šè¿‡å¼•å¯¼æç¤ºé€‰æ‹©ï¼Œæ— éœ€è®­ç»ƒæˆ–å¤–éƒ¨çŸ¥è¯†ï¼Œå¢žå¼ºç±»é—´è¯­ä¹‰åˆ†ç¦»
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œéª¨å¹²æ¨¡åž‹ä¸Šå®žéªŒï¼ŒSAGEæå‡é›¶æ ·æœ¬æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

