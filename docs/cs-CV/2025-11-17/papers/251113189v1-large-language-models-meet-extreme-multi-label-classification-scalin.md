---
layout: default
title: Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework
---

# Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework

**arXiv**: [2511.13189v1](https://arxiv.org/abs/2511.13189) | [PDF](https://arxiv.org/pdf/2511.13189.pdf)

**ä½œè€…**: Diego Ortego, Marlon RodrÃ­guez, Mario Almagro, Kunal Dahiya, David JimÃ©nez, Juan C. SanMiguel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViXMLæ¡†æž¶ä»¥è§£å†³æžç«¯å¤šæ ‡ç­¾åˆ†ç±»ä¸­çš„æ•ˆçŽ‡ä¸Žæ€§èƒ½å¹³è¡¡é—®é¢˜ã€‚**

**å…³é”®è¯**: `æžç«¯å¤šæ ‡ç­¾åˆ†ç±»` `å¤šæ¨¡æ€å­¦ä¹ ` `è§£ç å™¨æ¨¡åž‹` `è§†è§‰å¢žå¼º` `æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæžç«¯å¤šæ ‡ç­¾åˆ†ç±»éœ€åœ¨è¶…å¤§æ ‡ç­¾ç©ºé—´ä¸­å¹³è¡¡æ•ˆçŽ‡ä¸Žæ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè§£ç å™¨æ¨¡åž‹å’Œè§†è§‰ä¿¡æ¯ï¼Œé€šè¿‡å•å›¾åƒåµŒå…¥é›†æˆå¤šæ¨¡æ€èƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒP@1æå‡æœ€é«˜è¾¾8.21%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

