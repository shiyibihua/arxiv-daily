---
layout: default
title: Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew
---

# Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew

**arXiv**: [2511.13535v1](https://arxiv.org/abs/2511.13535) | [PDF](https://arxiv.org/pdf/2511.13535.pdf)

**ä½œè€…**: Farhin Farhad Riya, Shahinul Hoque, Jinyuan Stella Sun, Olivera Kotevska

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè”é‚¦å­¦ä¹ ä¸­é€šè¿‡é¢œè‰²æ‰°åŠ¨æ¯’åŒ–æ¨¡åž‹å¯è§£é‡Šæ€§çš„æ”»å‡»æ–¹æ³•**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ¨¡åž‹å¯è§£é‡Šæ€§` `å¯¹æŠ—æ”»å‡»` `æ˜¾è‘—æ€§å›¾` `é¢œè‰²æ‰°åŠ¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡åž‹é¢„æµ‹å‡†ç¡®ä½†å¯è§£é‡Šæ€§è¢«ç ´åï¼ŒæŒ‘æˆ˜äº†å‡†ç¡®é¢„æµ‹å³å¿ å®žè§£é‡Šçš„å‡è®¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è‰²åº¦æ‰°åŠ¨æ¨¡å—ï¼Œé€šè¿‡æ”¹å˜å‰æ™¯èƒŒæ™¯é¢œè‰²å¯¹æ¯”æ¥æ“çºµæ˜¾è‘—æ€§å›¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ”»å‡»ä½¿Grad-CAMå³°å€¼æ¿€æ´»é‡å é™ä½Žè¾¾35%ï¼ŒåŒæ—¶ä¿æŒåˆ†ç±»å‡†ç¡®çŽ‡é«˜äºŽ96%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

