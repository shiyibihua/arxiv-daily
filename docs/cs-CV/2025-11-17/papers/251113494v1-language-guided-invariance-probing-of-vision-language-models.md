---
layout: default
title: Language-Guided Invariance Probing of Vision-Language Models
---

# Language-Guided Invariance Probing of Vision-Language Models

**arXiv**: [2511.13494v1](https://arxiv.org/abs/2511.13494) | [PDF](https://arxiv.org/pdf/2511.13494.pdf)

**ä½œè€…**: Jae Joong Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€å¼•å¯¼ä¸å˜æ€§æŽ¢æµ‹ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹çš„è¯­è¨€é²æ£’æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `è¯­è¨€é²æ£’æ€§` `ä¸å˜æ€§æŽ¢æµ‹` `è¯­ä¹‰ç¿»è½¬` `é›¶æ ·æœ¬æ€§èƒ½` `æ¨¡åž‹è¯Šæ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹å¯¹è¯­è¨€æ‰°åŠ¨çš„å“åº”å¯é æ€§æœªçŸ¥ï¼Œæ ‡å‡†æ£€ç´¢æŒ‡æ ‡å¯èƒ½æŽ©ç›–ç¼ºé™·
2. æ–¹æ³•è¦ç‚¹ï¼šè‡ªåŠ¨ç”Ÿæˆé‡Šä¹‰å’Œè¯­ä¹‰ç¿»è½¬ï¼Œå®šä¹‰ä¸å˜æ€§è¯¯å·®å’Œè¯­ä¹‰æ•æ„Ÿåº¦å·®è·æŒ‡æ ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šEVA02-CLIPå’Œå¤§åž‹OpenCLIPå˜ä½“è¡¨çŽ°ç¨³å¥ï¼ŒSigLIPç³»åˆ—æ˜“åå¥½ç¿»è½¬æè¿°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
>   Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

