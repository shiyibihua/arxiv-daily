---
layout: default
title: CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation
---

# CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.13102" target="_blank" class="toolbar-btn">arXiv: 2511.13102v2</a>
    <a href="https://arxiv.org/pdf/2511.13102.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13102v2" 
            onclick="toggleFavorite(this, '2511.13102v2', 'CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yu Zhu, Dan Zeng, Shuiwang Li, Qijun Zhao, Qiaomu Shen, Bo Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17 (Êõ¥Êñ∞: 2025-12-15)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**CapeNextÔºöÈÄöËøá‰ºòÂåñÂä®ÊÄÅÊîØÊåÅ‰ø°ÊÅØÔºåÊîπËøõÁ±ªÂà´Êó†ÂÖ≥ÁöÑÂßøÊÄÅ‰º∞ËÆ°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Á±ªÂà´Êó†ÂÖ≥ÂßøÊÄÅ‰º∞ËÆ°` `Ë∑®Ê®°ÊÄÅ‰∫§‰∫í` `ÂèåÊµÅÁâπÂæÅÁªÜÂåñ` `Âä®ÊÄÅÊîØÊåÅ‰ø°ÊÅØ` `ÂßøÊÄÅ‰º∞ËÆ°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ±ªÂà´Êó†ÂÖ≥ÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ï‰æùËµñÈùôÊÄÅÊñáÊú¨ÊèèËø∞ÔºåÂøΩÁï•‰∫ÜË∑®Á±ªÂà´ÂíåÁ±ªÂà´ÂÜÖÁöÑËßÜËßâÂ∑ÆÂºÇ„ÄÇ
2. CapeNextÈÄöËøáÂàÜÂ±ÇË∑®Ê®°ÊÄÅ‰∫§‰∫íÂíåÂèåÊµÅÁâπÂæÅÁªÜÂåñÔºåËûçÂêàÊñáÊú¨ÂíåÂõæÂÉè‰ø°ÊÅØÔºåÂ¢ûÂº∫ËÅîÂêàÂµåÂÖ•„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCapeNextÂú®MP-100Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á±ªÂà´Êó†ÂÖ≥ÁöÑÂßøÊÄÅ‰º∞ËÆ°(CAPE)È¢ÜÂüüÁöÑÁ†îÁ©∂ÔºåÈÄöÂ∏∏ÈááÁî®Âõ∫ÂÆöÁöÑÊñáÊú¨ÂÖ≥ÈîÆÁÇπÊèèËø∞‰Ωú‰∏∫ËØ≠‰πâÂÖàÈ™åÔºåÁî®‰∫é‰∏§Èò∂ÊÆµÁöÑÂßøÊÄÅÂåπÈÖçÊ°ÜÊû∂„ÄÇËøôÁßçËåÉÂºèÈÄöËøáËß£ËÄ¶ÊîØÊåÅÂõæÂÉèÁöÑ‰æùËµñÊÄßÔºåÂ¢ûÂº∫‰∫ÜÈ≤ÅÊ£íÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨ÁöÑÂàÜÊûêÊè≠Á§∫‰∫ÜÈùôÊÄÅËÅîÂêàÂµåÂÖ•ÁöÑ‰∏§‰∏™Âõ∫ÊúâÂ±ÄÈôêÊÄßÔºö(1)Â§ö‰πâÊÄßÂØºËá¥ÂåπÈÖçËøáÁ®ã‰∏≠ÁöÑË∑®Á±ªÂà´Ê≠ß‰πâÔºà‰æãÂ¶ÇÔºå‚ÄúËÖø‚ÄùÂú®‰∫∫Á±ªÂíåÂÆ∂ÂÖ∑‰∏≠Ë°®Áé∞Âá∫‰∏çÂêåÁöÑËßÜËßâÂΩ¢ÊÄÅÔºâÔºõ(2)ÂØπ‰∫éÁªÜÁ≤íÂ∫¶ÁöÑÁ±ªÂà´ÂÜÖÂ∑ÆÂºÇÔºåÂà§Âà´ÊÄß‰∏çË∂≥Ôºà‰æãÂ¶ÇÔºå‰∏ÄÂè™Áù°ËßâÁöÑÁôΩËâ≤Áå´Âíå‰∏ÄÂè™Á´ôÁ´ãÁöÑÈªëËâ≤Áå´Âú®ÂßøÂäøÂíåÊØõÁöÆ‰∏äÁöÑÂ∑ÆÂºÇÔºâ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÂàõÊñ∞ÊÄßÂú∞Â∞ÜÂàÜÂ±ÇË∑®Ê®°ÊÄÅ‰∫§‰∫í‰∏éÂèåÊµÅÁâπÂæÅÁªÜÂåñÁõ∏ÁªìÂêàÔºåÂà©Áî®ÊñáÊú¨ÊèèËø∞ÂíåÁâπÂÆöÂõæÂÉè‰∏≠ÁöÑÁ±ªÁ∫ßÂà´ÂíåÂÆû‰æãÁâπÂÆöÁ∫øÁ¥¢Êù•Â¢ûÂº∫ËÅîÂêàÂµåÂÖ•„ÄÇÂú®MP-100Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊó†ËÆ∫ÁΩëÁªúÈ™®Âπ≤Â¶Ç‰ΩïÔºåCapeNextÂßãÁªàÂ§ßÂπÖ‰ºò‰∫éÊúÄÂÖàËøõÁöÑCAPEÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁ±ªÂà´Êó†ÂÖ≥ÂßøÊÄÅ‰º∞ËÆ°Êó®Âú®‰º∞ËÆ°ÂõæÂÉè‰∏≠Áâ©‰ΩìÁöÑÂÖ≥ÈîÆÁÇπ‰ΩçÁΩÆÔºåËÄåÊó†ÈúÄÈ¢ÑÂÖàÁü•ÈÅìÁâ©‰ΩìÁöÑÁ±ªÂà´„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®Âõ∫ÂÆöÁöÑÊñáÊú¨ÊèèËø∞‰Ωú‰∏∫ÂÖ≥ÈîÆÁÇπÁöÑËØ≠‰πâÂÖàÈ™åÔºå‰ΩÜËøôÁßçÊñπÊ≥ïÂøΩÁï•‰∫Ü‰∏çÂêåÁ±ªÂà´‰πãÈó¥‰ª•ÂèäÂêå‰∏ÄÁ±ªÂà´ÂÜÖÈÉ®ÁöÑËßÜËßâÂ∑ÆÂºÇÔºåÂØºËá¥ÂåπÈÖçÊ®°Á≥äÂíåÂà§Âà´ÊÄß‰∏çË∂≥„ÄÇ‰æãÂ¶ÇÔºå‚ÄúËÖø‚ÄùÂú®‰∫∫ÂíåÊ°åÂ≠ê‰∏äÁöÑËßÜËßâË°®Áé∞Â∑ÆÂºÇÂæàÂ§ßÔºåËÄåÂêå‰∏ÄÂìÅÁßçÁöÑÁå´‰πüÂèØËÉΩÂõ†‰∏∫ÂßøÂäøÂíåÊØõËâ≤‰∏çÂêåËÄåÈöæ‰ª•Âå∫ÂàÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCapeNextÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂä®ÊÄÅÂú∞ËûçÂêàÊñáÊú¨ÊèèËø∞ÂíåÂõæÂÉè‰ø°ÊÅØÔºåÊù•Â¢ûÂº∫ÂÖ≥ÈîÆÁÇπÁöÑËØ≠‰πâË°®Á§∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÂà©Áî®ÂàÜÂ±ÇË∑®Ê®°ÊÄÅ‰∫§‰∫íÊù•ÊçïÊçâÁ±ªÁ∫ßÂà´ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂπ∂Âà©Áî®ÂèåÊµÅÁâπÂæÅÁªÜÂåñÊù•ÊçïÊçâÂÆû‰æãÁ∫ßÂà´ÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåCapeNextÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞Ë°®Á§∫ÂÖ≥ÈîÆÁÇπÁöÑËØ≠‰πâ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCapeNextÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1)ÊñáÊú¨ÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÂ∞ÜÊñáÊú¨ÊèèËø∞ÁºñÁ†ÅÊàêÊñáÊú¨ÁâπÂæÅ„ÄÇ2)ÂõæÂÉèÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÂ∞ÜÂõæÂÉèÁºñÁ†ÅÊàêÂõæÂÉèÁâπÂæÅ„ÄÇ3)ÂàÜÂ±ÇË∑®Ê®°ÊÄÅ‰∫§‰∫íÊ®°ÂùóÔºöÁî®‰∫éËûçÂêàÊñáÊú¨ÁâπÂæÅÂíåÂõæÂÉèÁâπÂæÅÔºåÁîüÊàêÁ±ªÁ∫ßÂà´ÁöÑËØ≠‰πâË°®Á§∫„ÄÇ4)ÂèåÊµÅÁâπÂæÅÁªÜÂåñÊ®°ÂùóÔºöÁî®‰∫éËøõ‰∏ÄÊ≠•ÁªÜÂåñÂõæÂÉèÁâπÂæÅÔºåÁîüÊàêÂÆû‰æãÁ∫ßÂà´ÁöÑËßÜËßâË°®Á§∫„ÄÇ5)ÂßøÊÄÅ‰º∞ËÆ°Ê®°ÂùóÔºöÁî®‰∫éÊ†πÊçÆËûçÂêàÂêéÁöÑÁâπÂæÅÔºå‰º∞ËÆ°ÂÖ≥ÈîÆÁÇπÁöÑ‰ΩçÁΩÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCapeNextÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âä®ÊÄÅÂú∞ËûçÂêàÊñáÊú¨ÂíåÂõæÂÉè‰ø°ÊÅØÁöÑÊñπÂºè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåCapeNext‰∏çÊòØÁÆÄÂçïÂú∞Â∞ÜÊñáÊú¨ÊèèËø∞‰Ωú‰∏∫Âõ∫ÂÆöÁöÑËØ≠‰πâÂÖàÈ™åÔºåËÄåÊòØÂà©Áî®ÂàÜÂ±ÇË∑®Ê®°ÊÄÅ‰∫§‰∫íÂíåÂèåÊµÅÁâπÂæÅÁªÜÂåñÔºåÊù•Âä®ÊÄÅÂú∞Ë∞ÉÊï¥ÂÖ≥ÈîÆÁÇπÁöÑËØ≠‰πâË°®Á§∫„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•Êõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÁ±ªÂà´Âíå‰∏çÂêåÂÆû‰æã‰πãÈó¥ÁöÑËßÜËßâÂ∑ÆÂºÇÔºå‰ªéËÄåÊèêÈ´òÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂàÜÂ±ÇË∑®Ê®°ÊÄÅ‰∫§‰∫íÊ®°Âùó‰∏≠ÔºåCapeNext‰ΩøÁî®‰∫ÜÂ§öÂ±ÇTransformerÁªìÊûÑÔºåÊù•ÊçïÊçâÊñáÊú¨ÁâπÂæÅÂíåÂõæÂÉèÁâπÂæÅ‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇÂú®ÂèåÊµÅÁâπÂæÅÁªÜÂåñÊ®°Âùó‰∏≠ÔºåCapeNext‰ΩøÁî®‰∫ÜÊÆãÂ∑ÆËøûÊé•ÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊù•Â¢ûÂº∫ÁâπÂæÅÁöÑË°®ËææËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåCapeNextËøò‰ΩøÁî®‰∫ÜÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÔºåÊù•ÈºìÂä±Ê®°ÂûãÂ≠¶‰π†Êõ¥ÂÖ∑Âà§Âà´ÊÄßÁöÑÁâπÂæÅË°®Á§∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CapeNextÂú®MP-100Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊó†ËÆ∫‰ΩøÁî®‰ΩïÁßçÁΩëÁªúÈ™®Âπ≤ÔºåCapeNextÈÉΩÂ§ßÂπÖ‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®‰ΩøÁî®ResNet-50‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÊó∂ÔºåCapeNextÁöÑÂπ≥ÂùáÁ≤æÂ∫¶(AP)ÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü5‰∏™ÁôæÂàÜÁÇπ‰ª•‰∏ä„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåCapeNextËÉΩÂ§üÊúâÊïàÂú∞Ëß£ÂÜ≥Á±ªÂà´Êó†ÂÖ≥ÂßøÊÄÅ‰º∞ËÆ°‰∏≠ÁöÑË∑®Á±ªÂà´Ê≠ß‰πâÂíåÁ±ªÂà´ÂÜÖÂ∑ÆÂºÇÈóÆÈ¢ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CapeNextÂú®Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫È¢ÜÂüüÔºåCapeNextÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ËØÜÂà´ÂíåÊìç‰ΩúÂêÑÁßçÁâ©‰ΩìÔºåËÄåÊó†ÈúÄÈ¢ÑÂÖàÁü•ÈÅìÁâ©‰ΩìÁöÑÁ±ªÂà´„ÄÇÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÔºåCapeNextÂèØ‰ª•Â∏ÆÂä©ËΩ¶ËæÜËØÜÂà´Ë°å‰∫∫„ÄÅËΩ¶ËæÜÁ≠â‰∫§ÈÄöÂèÇ‰∏éËÄÖÔºå‰ªéËÄåÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄß„ÄÇÂú®Êô∫ËÉΩÁõëÊéßÈ¢ÜÂüüÔºåCapeNextÂèØ‰ª•Â∏ÆÂä©ÁõëÊéßÁ≥ªÁªüËØÜÂà´ÂºÇÂ∏∏Ë°å‰∏∫Ôºå‰ªéËÄåÊèêÈ´òÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

