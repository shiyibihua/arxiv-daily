---
layout: default
title: Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images
---

# Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images

**arXiv**: [2511.13353v1](https://arxiv.org/abs/2511.13353) | [PDF](https://arxiv.org/pdf/2511.13353.pdf)

**ä½œè€…**: Lucas Gabriel Telesco, Danila Nejamkin, EstefanÃ­a Mata, Francisco Filizzola, Kevin Wignall, LucÃ­a Franco Troilo, MarÃ­a de los Angeles Cenoz, Melissa Thompson, Mercedes LeguÃ­a, Ignacio Larrabide, JosÃ© Ignacio Orlando

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠç›‘ç£å¤šä»»åŠ¡å­¦ä¹ æ¡†æž¶ä»¥æå‡çœ¼åº•å›¾åƒè´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§**

**å…³é”®è¯**: `çœ¼åº•å›¾åƒè´¨é‡è¯„ä¼°` `åŠç›‘ç£å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `ä¼ªæ ‡ç­¾ç”Ÿæˆ` `å¯è§£é‡Šæ€§å¢žå¼º` `å›¾åƒé‡‡é›†ç¼ºé™·æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰çœ¼åº•å›¾åƒè´¨é‡è¯„ä¼°å·¥å…·ç¼ºä¹å¯¹é‡‡é›†ç¼ºé™·çš„è¯¦ç»†è§£é‡Šï¼Œä¸”æ ‡æ³¨æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆäººå·¥æ•´ä½“è´¨é‡æ ‡ç­¾ä¸Žæ•™å¸ˆæ¨¡åž‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼Œåœ¨å¤šä»»åŠ¡æ¡†æž¶ä¸­è®­ç»ƒæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EyeQå’ŒDeepDRiDæ•°æ®é›†ä¸Šæ€§èƒ½ä¼˜äºŽåŸºçº¿ï¼Œæä¾›å¯è§£é‡Šçš„æ•èŽ·æ¡ä»¶åé¦ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

