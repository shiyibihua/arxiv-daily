---
layout: default
title: CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding
---

# CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding

**arXiv**: [2511.13644v1](https://arxiv.org/abs/2511.13644) | [PDF](https://arxiv.org/pdf/2511.13644.pdf)

**ä½œè€…**: Shrenik Patel, Daivik Patel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCacheFlowä»¥è§£å†³é•¿è§†é¢‘é—®ç­”ä¸­æ³¨æ„åŠ›ä¸ŽKVç¼“å­˜å¢žé•¿å¯¼è‡´çš„æ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `åŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒ` `KVç¼“å­˜åŽ‹ç¼©` `æµå¼è§†é¢‘é—®ç­”` `æ— éœ€è®­ç»ƒæ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿è§†é¢‘é—®ç­”ä¸­æ³¨æ„åŠ›æœºåˆ¶å’ŒKVç¼“å­˜éšè¿è¡Œæ—¶é—´å¢žé•¿ï¼Œå¯¼è‡´æŽ¨ç†æˆæœ¬é«˜æˆ–è§†é‡Žå—é™
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆåŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒå’ŒåŽ‹ç¼©é•¿æœŸè®°å¿†ï¼Œåœ¨çº¿å¤„ç†ä»¤ç‰Œå¹¶æž„å»ºæ£€ç´¢ç´¢å¼•ï¼Œæ— éœ€è®­ç»ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç¦»çº¿ä¸Žæµå¼VQAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽåŸºçº¿ï¼Œå¤„ç†ä»¤ç‰Œå‡å°‘é«˜è¾¾87%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

