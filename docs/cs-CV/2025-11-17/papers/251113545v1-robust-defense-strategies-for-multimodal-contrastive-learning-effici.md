---
layout: default
title: Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks
---

# Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks

**arXiv**: [2511.13545v1](https://arxiv.org/abs/2511.13545) | [PDF](https://arxiv.org/pdf/2511.13545.pdf)

**ä½œè€…**: Md. Iqbal Hossain, Afia Sajeeda, Neeresh Kumar Perla, Ming Shao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆå¾®è°ƒç­–ç•¥ä»¥é˜²å¾¡å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ä¸­çš„åŽé—¨æ”»å‡»**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ` `åŽé—¨æ”»å‡»é˜²å¾¡` `CLIPæ¨¡åž‹` `å›¾åƒåˆ†å‰²oracle` `é«˜æ•ˆå¾®è°ƒ` `è§†è§‰è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€æ¨¡åž‹å¦‚CLIPæ˜“å—åŽé—¨æ”»å‡»ï¼ŒçŽ°æœ‰é˜²å¾¡æ–¹æ³•æ•ˆçŽ‡ä½Žä¸”ä¸ç²¾ç¡®
2. å¼•å…¥å›¾åƒåˆ†å‰²oracleç›‘ç£ï¼Œè¯†åˆ«è§¦å‘å™¨å’Œå—å®³æ ·æœ¬ï¼Œæž„å»ºç´§å‡‘å¾®è°ƒæ•°æ®é›†
3. åœ¨è§†è§‰è¯†åˆ«åŸºå‡†ä¸ŠéªŒè¯ç­–ç•¥æœ‰æ•ˆï¼Œèƒ½æ¶ˆé™¤åŽé—¨å½±å“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

