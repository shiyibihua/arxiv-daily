---
layout: default
title: UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective
---

# UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective

**arXiv**: [2511.12988v1](https://arxiv.org/abs/2511.12988) | [PDF](https://arxiv.org/pdf/2511.12988.pdf)

**ä½œè€…**: Furui Xu, Shaobo Wang, Jiajun Zhang, Chenghao Sun, Haixiang Tang, Linfeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUNSEENæ¡†æž¶ä»Žæ³›åŒ–è§†è§’å¢žå¼ºæ•°æ®é›†å‰ªæžï¼Œæå‡æ ¸å¿ƒé›†æ€§èƒ½**

**å…³é”®è¯**: `æ•°æ®é›†å‰ªæž` `æ³›åŒ–è¯„åˆ†` `æ ¸å¿ƒé›†ä¼˜åŒ–` `å¤šæ­¥é€‰æ‹©` `æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ•°æ®é›†å‰ªæžä¸­æ ·æœ¬è¯„åˆ†å¯†é›†åŒ–ï¼Œé™ä½Žé€‰æ‹©åŒºåˆ†åº¦ï¼Œå½±å“æ ¸å¿ƒé›†æž„å»º
2. åŸºäºŽæœªè®­ç»ƒæ¨¡åž‹è¯„åˆ†æ ·æœ¬ï¼Œå¹¶æ‰©å±•è‡³å¤šæ­¥å¢žé‡é€‰æ‹©ä¼˜åŒ–æ ¸å¿ƒé›†è´¨é‡
3. åœ¨CIFARå’ŒImageNetä¸Šæ˜¾è‘—ä¼˜äºŽSOTAæ–¹æ³•ï¼ŒImageNet-1Kå‰ªæž30%æ— æŸæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

