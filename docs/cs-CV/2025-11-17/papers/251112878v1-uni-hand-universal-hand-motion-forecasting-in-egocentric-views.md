---
layout: default
title: Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views
---

# Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views

**arXiv**: [2511.12878v1](https://arxiv.org/abs/2511.12878) | [PDF](https://arxiv.org/pdf/2511.12878.pdf)

**ä½œè€…**: Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Yu Zheng, Erhang Zhang, Xieyuanli Chen, Hesheng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEgoLocæ–¹æ³•ä»¥é›¶æ ·æœ¬å®šä½æ‰‹-ç‰©æŽ¥è§¦/åˆ†ç¦»æ—¶åˆ»ï¼Œæå‡ç¬¬ä¸€äººç§°è§†é¢‘äº¤äº’åˆ†æžã€‚**

**å…³é”®è¯**: `æ—¶åºäº¤äº’å®šä½` `é›¶æ ·æœ¬å­¦ä¹ ` `ç¬¬ä¸€äººç§°è§†è§‰` `æ‰‹-ç‰©äº¤äº’` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥ç²¾ç¡®å®šä½æ‰‹ä¸Žç‰©ä½“æŽ¥è§¦å’Œåˆ†ç¦»çš„å…³é”®æ—¶åˆ»ï¼Œå½±å“æ²‰æµ¸å¼äº¤äº’å’Œæœºå™¨äººè§„åˆ’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ‰‹åŠ¨æ€å¼•å¯¼é‡‡æ ·å’Œè§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œæ— éœ€å¯¹è±¡æŽ©ç æˆ–ç±»åˆ«æ³¨é‡Šï¼Œå®žçŽ°é›¶æ ·æœ¬æ—¶åºå®šä½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…¬å…±æ•°æ®é›†å’Œæ–°åŸºå‡†ä¸ŠéªŒè¯ï¼ŒEgoLocå®žçŽ°å¯ä¿¡æ—¶åºäº¤äº’å®šä½ï¼Œå¹¶ä¿ƒè¿›ä¸‹æ¸¸åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

