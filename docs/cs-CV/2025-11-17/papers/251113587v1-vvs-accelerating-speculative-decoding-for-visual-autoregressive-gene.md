---
layout: default
title: VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping
---

# VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping

**arXiv**: [2511.13587v1](https://arxiv.org/abs/2511.13587) | [PDF](https://arxiv.org/pdf/2511.13587.pdf)

**ä½œè€…**: Haotian Dong, Ye Li, Rongwei Lu, Chen Tang, Shu-Tao Xia, Zhi Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVVSæ¡†æž¶ä»¥åŠ é€Ÿè§†è§‰è‡ªå›žå½’ç”Ÿæˆï¼Œé€šè¿‡éƒ¨åˆ†éªŒè¯è·³è¿‡å‡å°‘æŽ¨ç†å»¶è¿Ÿ**

**å…³é”®è¯**: `è§†è§‰è‡ªå›žå½’ç”Ÿæˆ` `æŽ¨æµ‹è§£ç ` `æŽ¨ç†åŠ é€Ÿ` `éªŒè¯è·³è¿‡` `ç‰¹å¾ç¼“å­˜` `ä»¤ç‰Œè°ƒåº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è‡ªå›žå½’æ¨¡åž‹æŽ¨ç†å»¶è¿Ÿé«˜ï¼ŒæŽ¨æµ‹è§£ç æ— æ³•ç›´æŽ¥å‡å°‘å‰å‘ä¼ é€’æ¬¡æ•°
2. åˆ©ç”¨è§†è§‰ä»¤ç‰Œå¯äº’æ¢æ€§ï¼Œè®¾è®¡éªŒè¯è·³è¿‡ã€ç‰¹å¾ç¼“å­˜å’Œè°ƒåº¦æ¨¡å—
3. å®žéªŒæ˜¾ç¤ºå‰å‘ä¼ é€’å‡å°‘2.8å€ï¼Œä¿æŒç”Ÿæˆè´¨é‡ï¼Œä¼˜äºŽä¼ ç»Ÿæ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

