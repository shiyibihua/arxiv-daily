---
layout: default
title: DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping
---

# DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping

**arXiv**: [2511.12912v1](https://arxiv.org/abs/2511.12912) | [PDF](https://arxiv.org/pdf/2511.12912.pdf)

**ä½œè€…**: Yingting Zhou, Wenbo Cui, Weiheng Liu, Guixing Chen, Haoran Li, Dongbin Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiffuDepGraspæ¡†æž¶ï¼Œé€šè¿‡æ‰©æ•£æ¨¡åž‹æ¨¡æ‹Ÿæ·±åº¦å™ªå£°ï¼Œå®žçŽ°é›¶æ ·æœ¬Sim2RealæŠ“å–ã€‚**

**å…³é”®è¯**: `Sim2Realè¿ç§»` `æ‰©æ•£æ¨¡åž‹` `æ·±åº¦å™ªå£°å»ºæ¨¡` `æœºå™¨äººæŠ“å–` `é›¶æ ·æœ¬å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçœŸå®žæ·±åº¦å›¾ä¸­çš„ä¼ æ„Ÿå™¨ä¼ªå½±ï¼ˆå¦‚ç©ºæ´žå’Œå™ªå£°ï¼‰é˜»ç¢ä»¿çœŸåˆ°çŽ°å®žçš„ç­–ç•¥è¿ç§»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ‰©æ•£æ·±åº¦ç”Ÿæˆå™¨åˆæˆä»¿çœŸæ·±åº¦ï¼Œç»“åˆå™ªå£°å«æŽ¥æ¨¡å—æ³¨å…¥ä¼ æ„Ÿå™¨çœŸå®žå™ªå£°ã€‚
3. å®žéªŒæ•ˆæžœï¼šé›¶æ ·æœ¬è½¬ç§»ä¸‹ï¼Œåœ¨12ä¸ªç‰©ä½“æŠ“å–ä¸­å¹³å‡æˆåŠŸçŽ‡95.7%ï¼Œæ³›åŒ–æ€§å¼ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.

