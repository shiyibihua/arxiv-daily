---
layout: default
title: TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing
---

# TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing

**arXiv**: [2511.13283v1](https://arxiv.org/abs/2511.13283) | [PDF](https://arxiv.org/pdf/2511.13283.pdf)

**ä½œè€…**: Jongha Kim, Minseong Bae, Sanghyeok Lee, Jinsung Yoon, Hyunwoo J. Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTabFlashæ–¹æ³•ï¼Œé€šè¿‡æ¸è¿›é—®é¢˜æ³¨å…¥å’Œä»¤ç‰Œèšç„¦æå‡è¡¨æ ¼å›¾åƒç†è§£æ•ˆçŽ‡ä¸Žæ•ˆæžœ**

**å…³é”®è¯**: `è¡¨æ ¼å›¾åƒç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ¸è¿›é—®é¢˜æ³¨å…¥` `ä»¤ç‰Œå‰ªæž` `ä»¤ç‰Œèšç„¦è®­ç»ƒ` `æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¡¨æ ¼å›¾åƒå­˜åœ¨å†—ä½™èƒŒæ™¯å’Œé—®é¢˜æ— å…³åŒºåŸŸï¼Œå¯¼è‡´MLLMè§†è§‰ç‰¹å¾å†—ä½™ä¸”ä½Žæ•ˆ
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ¸è¿›é—®é¢˜æ³¨å…¥ã€ä»¤ç‰Œå‰ªæžå’Œä»¤ç‰Œèšç„¦è®­ç»ƒï¼Œç”Ÿæˆç´§å‡‘ä¸”é—®é¢˜æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾
3. å®žéªŒæ•ˆæžœï¼šåœ¨è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸­å®žçŽ°SOTAæ€§èƒ½ï¼ŒFLOPså’Œå†…å­˜ä½¿ç”¨åˆ†åˆ«å‡å°‘27%å’Œ30%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

