---
layout: default
title: End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer
---

# End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.13208" target="_blank" class="toolbar-btn">arXiv: 2511.13208v2</a>
    <a href="https://arxiv.org/pdf/2511.13208.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13208v2" 
            onclick="toggleFavorite(this, '2511.13208v2', 'End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yonghui Yu, Jiahang Cai, Xun Wang, Wenwu Yang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17 (Êõ¥Êñ∞: 2025-12-02)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/zgspose/PAVENet)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PAVE-NetÔºå‰∏ÄÁßçÁ´ØÂà∞Á´ØÂßøÊÄÅÊÑüÁü•ËßÜÈ¢ëTransformerÁΩëÁªúÔºåÁî®‰∫éÂ§ö‰∫∫ËßÜÈ¢ëÂßøÊÄÅ‰º∞ËÆ°„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§ö‰∫∫ÂßøÊÄÅ‰º∞ËÆ°` `ËßÜÈ¢ëÂßøÊÄÅ‰º∞ËÆ°` `Á´ØÂà∞Á´ØÂ≠¶‰π†` `TransformerÁΩëÁªú` `ÂßøÊÄÅÊÑüÁü•Ê≥®ÊÑèÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ï‰æùËµñÂêØÂèëÂºèÊìç‰ΩúÔºåÂ¶ÇÊ£ÄÊµãÂíåNMSÔºåÈôêÂà∂‰∫ÜÂ§ö‰∫∫ËßÜÈ¢ëÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ
2. PAVE-NetÈÄöËøáÂßøÊÄÅÊÑüÁü•TransformerÁΩëÁªúÔºåÁõ¥Êé•Âª∫Ê®°Êó∂Á©∫‰æùËµñÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÂ§ö‰∫∫ÂßøÊÄÅ‰º∞ËÆ°„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPAVE-NetÂú®PoseTrack2017‰∏äÊèêÂçá6.0 mAPÔºåÂπ∂Âú®ÊïàÁéá‰∏ä‰ºò‰∫é‰∏§Èò∂ÊÆµÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÂ§ö‰∫∫ËßÜÈ¢ëÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®‰∏§Èò∂ÊÆµÊµÅÁ®ãÔºöÈ¶ñÂÖàÊ£ÄÊµãÊØè‰∏ÄÂ∏ß‰∏≠ÁöÑ‰∏™‰ΩìÔºåÁÑ∂ÂêéËøõË°åÂçï‰∫∫ÂßøÊÄÅ‰º∞ËÆ°ÁöÑÊó∂Â∫èÂª∫Ê®°„ÄÇËøôÁßçËÆæËÆ°‰æùËµñ‰∫éÊ£ÄÊµã„ÄÅRoIË£ÅÂâ™ÂíåÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºàNMSÔºâÁ≠âÂêØÂèëÂºèÊìç‰ΩúÔºåÈôêÂà∂‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÆåÂÖ®Á´ØÂà∞Á´ØÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éËßÜÈ¢ë‰∏≠ÁöÑÂ§ö‰∫∫2DÂßøÊÄÅ‰º∞ËÆ°ÔºåÊúâÊïàÂú∞Ê∂àÈô§‰∫ÜÂêØÂèëÂºèÊìç‰Ωú„ÄÇ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàòÊòØÂú®Â§çÊùÇÂíåÈáçÂè†ÁöÑÊó∂Â∫èËΩ®Ëøπ‰∏ãÂÖ≥ËÅîË∑®Â∏ßÁöÑ‰∏™‰Ωì„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂßøÊÄÅÊÑüÁü•ËßÜÈ¢ëTransformerÁΩëÁªúÔºàPAVE-NetÔºâÔºåÂÆÉÂÖ∑Êúâ‰∏Ä‰∏™Á©∫Èó¥ÁºñÁ†ÅÂô®Êù•Âª∫Ê®°Â∏ßÂÜÖÂÖ≥Á≥ªÔºå‰ª•Âèä‰∏Ä‰∏™Êó∂Á©∫ÂßøÊÄÅËß£Á†ÅÂô®Êù•ÊçïËé∑Ë∑®Â∏ßÁöÑÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ª„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞ÂáÜÁ°ÆÁöÑÊó∂Â∫èÂÖ≥ËÅîÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂßøÊÄÅÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÊØè‰∏™ÂßøÊÄÅÊü•ËØ¢ËÉΩÂ§üÈÄâÊã©ÊÄßÂú∞ËÅöÂêàÊù•Ëá™ËøûÁª≠Â∏ß‰∏≠Áõ∏Âêå‰∏™‰ΩìÁöÑÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊòæÂºèÂú∞Âª∫Ê®°ÂßøÊÄÅÂÖ≥ÈîÆÁÇπ‰πãÈó¥ÁöÑÊó∂Á©∫‰æùËµñÂÖ≥Á≥ªÔºå‰ª•ÊèêÈ´òÂáÜÁ°ÆÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂ§öÂ∏ß2D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÁ´ØÂà∞Á´ØÊñπÊ≥ï„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåPAVE-NetÊòæËëó‰ºò‰∫éÂÖàÂâçÁöÑÂü∫‰∫éÂõæÂÉèÁöÑÁ´ØÂà∞Á´ØÊñπÊ≥ïÔºåÂú®PoseTrack2017‰∏äÂÆûÁé∞‰∫Ü6.0 mAPÁöÑÊîπËøõÔºåÂπ∂‰∏îÊèê‰æõ‰∫Ü‰∏éÊúÄÂÖàËøõÁöÑ‰∏§Èò∂ÊÆµËßÜÈ¢ëÊñπÊ≥ïÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂Âú®ÊïàÁéáÊñπÈù¢ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ö‰∫∫ËßÜÈ¢ëÂßøÊÄÅ‰º∞ËÆ°ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÊòØ‰∏§Èò∂ÊÆµÁöÑÔºåÈ¶ñÂÖàÊ£ÄÊµãÊØè‰∏ÄÂ∏ß‰∏≠ÁöÑ‰∫∫ÔºåÁÑ∂ÂêéÂØπÊØè‰∏™‰∫∫ÁöÑÂßøÊÄÅËøõË°åÊó∂Â∫èÂª∫Ê®°„ÄÇËøôÁßçÊñπÊ≥ï‰æùËµñ‰∫éÂêØÂèëÂºèÊìç‰ΩúÔºå‰æãÂ¶ÇÁõÆÊ†áÊ£ÄÊµã„ÄÅRoIË£ÅÂâ™ÂíåÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºàNMSÔºâÔºåËøô‰∫õÊìç‰Ωú‰ºöÂºïÂÖ•ËØØÂ∑ÆÔºåÂπ∂‰∏îÊïàÁéáËæÉ‰Ωé„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÁ´ØÂà∞Á´ØÁöÑÊñπÊ≥ïÊù•Áõ¥Êé•‰ªéËßÜÈ¢ë‰∏≠‰º∞ËÆ°Â§ö‰∫∫ÂßøÊÄÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®TransformerÁΩëÁªúÁõ¥Êé•Âª∫Ê®°ËßÜÈ¢ë‰∏≠Â§ö‰∫∫ÂßøÊÄÅÁöÑÊó∂Á©∫‰æùËµñÂÖ≥Á≥ª„ÄÇÈÄöËøáÂºïÂÖ•ÂßøÊÄÅÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÂæóÁΩëÁªúËÉΩÂ§üÂÖ≥Ê≥®Âà∞Âêå‰∏Ä‰∫∫Âú®‰∏çÂêåÂ∏ß‰πãÈó¥ÁöÑÂßøÊÄÅÁâπÂæÅÔºå‰ªéËÄåÂÆûÁé∞ÂáÜÁ°ÆÁöÑÊó∂Â∫èÂÖ≥ËÅî„ÄÇÂêåÊó∂ÔºåÊòæÂºèÂú∞Âª∫Ê®°ÂßøÊÄÅÂÖ≥ÈîÆÁÇπ‰πãÈó¥ÁöÑÊó∂Á©∫‰æùËµñÂÖ≥Á≥ªÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPAVE-NetÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™Á©∫Èó¥ÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™Êó∂Á©∫ÂßøÊÄÅËß£Á†ÅÂô®„ÄÇÁ©∫Èó¥ÁºñÁ†ÅÂô®Áî®‰∫éÊèêÂèñÊØè‰∏ÄÂ∏ßÁöÑÁâπÂæÅÔºåÊó∂Á©∫ÂßøÊÄÅËß£Á†ÅÂô®ÂàôÂà©Áî®TransformerÁΩëÁªúÂª∫Ê®°Ë∑®Â∏ßÁöÑÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ª„ÄÇËß£Á†ÅÂô®‰ΩøÁî®ÂßøÊÄÅÊü•ËØ¢Ôºàpose queryÔºâÊù•ÂÆö‰ΩçÂíå‰º∞ËÆ°ÊØè‰∏™‰∫∫ÁöÑÂßøÊÄÅ„ÄÇÂßøÊÄÅÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂Ë¢´ÈõÜÊàêÂà∞Ëß£Á†ÅÂô®‰∏≠Ôºå‰ª•ÂÆûÁé∞ÂáÜÁ°ÆÁöÑÊó∂Â∫èÂÖ≥ËÅî„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂ§öÂ∏ß2D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÁ´ØÂà∞Á´ØÊñπÊ≥ï„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåPAVE-NetÈÅøÂÖç‰∫ÜÂêØÂèëÂºèÊìç‰ΩúÔºåÂèØ‰ª•Áõ¥Êé•‰ªéËßÜÈ¢ë‰∏≠‰º∞ËÆ°Â§ö‰∫∫ÂßøÊÄÅ„ÄÇÂßøÊÄÅÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂè¶‰∏Ä‰∏™ÂÖ≥ÈîÆÂàõÊñ∞ÔºåÂÆÉ‰ΩøÂæóÁΩëÁªúËÉΩÂ§üÂÖ≥Ê≥®Âà∞Âêå‰∏Ä‰∫∫Âú®‰∏çÂêåÂ∏ß‰πãÈó¥ÁöÑÂßøÊÄÅÁâπÂæÅÔºå‰ªéËÄåÂÆûÁé∞ÂáÜÁ°ÆÁöÑÊó∂Â∫èÂÖ≥ËÅî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPAVE-NetÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÂßøÊÄÅÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÆÉÈÄöËøáËÆ°ÁÆóÂßøÊÄÅÊü•ËØ¢ÂíåÁâπÂæÅ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶Êù•Á°ÆÂÆöÊ≥®ÊÑèÂäõÊùÉÈáçÔºõ2) Êó∂Á©∫ÂßøÊÄÅËß£Á†ÅÂô®ÔºåÂÆÉ‰ΩøÁî®TransformerÁΩëÁªúÂª∫Ê®°Ë∑®Â∏ßÁöÑÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ªÔºõ3) ÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫é‰ºòÂåñÁΩëÁªúÁöÑÂèÇÊï∞ÔºåÂåÖÊã¨ÂßøÊÄÅ‰º∞ËÆ°ÊçüÂ§±ÂíåÊó∂Â∫èÂÖ≥ËÅîÊçüÂ§±„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PAVE-NetÂú®PoseTrack2017Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÂÖàÂâçÁöÑÂü∫‰∫éÂõæÂÉèÁöÑÁ´ØÂà∞Á´ØÊñπÊ≥ïÔºåÂÆûÁé∞‰∫Ü6.0 mAPÁöÑÊîπËøõ„ÄÇÂêåÊó∂ÔºåPAVE-NetÁöÑÊÄßËÉΩ‰∏éÊúÄÂÖàËøõÁöÑ‰∏§Èò∂ÊÆµËßÜÈ¢ëÊñπÊ≥ïÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåÂπ∂‰∏îÂú®ÊïàÁéáÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPAVE-NetÊòØ‰∏ÄÁßçÊúâÊïàÁöÑÂ§ö‰∫∫ËßÜÈ¢ëÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËßÜÈ¢ëÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅËøêÂä®ÂàÜÊûê„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜÈ¢ëÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÊ£ÄÊµãÂºÇÂ∏∏Ë°å‰∏∫ÔºõÂú®‰∫∫Êú∫‰∫§‰∫í‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÂÆûÁé∞Âü∫‰∫éÂßøÊÄÅÁöÑÊéßÂà∂ÔºõÂú®ËøêÂä®ÂàÜÊûê‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éËØÑ‰º∞ËøêÂä®ÂëòÁöÑË°®Áé∞„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõËøõ‰∏ÄÊ≠•ÊèêÂçáËøô‰∫õÂ∫îÁî®Âú∫ÊôØÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames. Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation. Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a 6.0 mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video based approaches, while offering significant gains in efficiency. Project page: https://github.com/zgspose/PAVENet.

