---
layout: default
title: End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer
---

# End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer

**arXiv**: [2511.13208v2](https://arxiv.org/abs/2511.13208) | [PDF](https://arxiv.org/pdf/2511.13208.pdf)

**ä½œè€…**: Yonghui Yu, Jiahang Cai, Xun Wang, Wenwu Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17 (æ›´æ–°: 2025-12-02)

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zgspose/PAVENet)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPAVE-Netï¼Œä¸€ç§ç«¯åˆ°ç«¯å§¿æ€æ„ŸçŸ¥è§†é¢‘Transformerç½‘ç»œï¼Œç”¨äºŽå¤šäººè§†é¢‘å§¿æ€ä¼°è®¡ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šäººå§¿æ€ä¼°è®¡` `è§†é¢‘å§¿æ€ä¼°è®¡` `ç«¯åˆ°ç«¯å­¦ä¹ ` `Transformerç½‘ç»œ` `å§¿æ€æ„ŸçŸ¥æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–å¯å‘å¼æ“ä½œï¼Œå¦‚æ£€æµ‹å’ŒNMSï¼Œé™åˆ¶äº†å¤šäººè§†é¢‘å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ã€‚
2. PAVE-Neté€šè¿‡å§¿æ€æ„ŸçŸ¥Transformerç½‘ç»œï¼Œç›´æŽ¥å»ºæ¨¡æ—¶ç©ºä¾èµ–ï¼Œå®žçŽ°ç«¯åˆ°ç«¯çš„å¤šäººå§¿æ€ä¼°è®¡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒPAVE-Netåœ¨PoseTrack2017ä¸Šæå‡6.0 mAPï¼Œå¹¶åœ¨æ•ˆçŽ‡ä¸Šä¼˜äºŽä¸¤é˜¶æ®µæ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŽ°æœ‰çš„å¤šäººè§†é¢‘å§¿æ€ä¼°è®¡æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆæ£€æµ‹æ¯ä¸€å¸§ä¸­çš„ä¸ªä½“ï¼Œç„¶åŽè¿›è¡Œå•äººå§¿æ€ä¼°è®¡çš„æ—¶åºå»ºæ¨¡ã€‚è¿™ç§è®¾è®¡ä¾èµ–äºŽæ£€æµ‹ã€RoIè£å‰ªå’Œéžæžå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ç­‰å¯å‘å¼æ“ä½œï¼Œé™åˆ¶äº†å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œå…¨ç«¯åˆ°ç«¯çš„æ¡†æž¶ï¼Œç”¨äºŽè§†é¢‘ä¸­çš„å¤šäºº2Då§¿æ€ä¼°è®¡ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¯å‘å¼æ“ä½œã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯åœ¨å¤æ‚å’Œé‡å çš„æ—¶åºè½¨è¿¹ä¸‹å…³è”è·¨å¸§çš„ä¸ªä½“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å§¿æ€æ„ŸçŸ¥è§†é¢‘Transformerç½‘ç»œï¼ˆPAVE-Netï¼‰ï¼Œå®ƒå…·æœ‰ä¸€ä¸ªç©ºé—´ç¼–ç å™¨æ¥å»ºæ¨¡å¸§å†…å…³ç³»ï¼Œä»¥åŠä¸€ä¸ªæ—¶ç©ºå§¿æ€è§£ç å™¨æ¥æ•èŽ·è·¨å¸§çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚ä¸ºäº†å®žçŽ°å‡†ç¡®çš„æ—¶åºå…³è”ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å§¿æ€æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¯ä¸ªå§¿æ€æŸ¥è¯¢èƒ½å¤Ÿé€‰æ‹©æ€§åœ°èšåˆæ¥è‡ªè¿žç»­å¸§ä¸­ç›¸åŒä¸ªä½“çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¾å¼åœ°å»ºæ¨¡å§¿æ€å…³é”®ç‚¹ä¹‹é—´çš„æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œä»¥æé«˜å‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªç”¨äºŽå¤šå¸§2Däººä½“å§¿æ€ä¼°è®¡çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒPAVE-Netæ˜¾è‘—ä¼˜äºŽå…ˆå‰çš„åŸºäºŽå›¾åƒçš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œåœ¨PoseTrack2017ä¸Šå®žçŽ°äº†6.0 mAPçš„æ”¹è¿›ï¼Œå¹¶ä¸”æä¾›äº†ä¸Žæœ€å…ˆè¿›çš„ä¸¤é˜¶æ®µè§†é¢‘æ–¹æ³•å…·æœ‰ç«žäº‰åŠ›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨æ•ˆçŽ‡æ–¹é¢å®žçŽ°äº†æ˜¾è‘—çš„æå‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šäººè§†é¢‘å§¿æ€ä¼°è®¡é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸æ˜¯ä¸¤é˜¶æ®µçš„ï¼Œé¦–å…ˆæ£€æµ‹æ¯ä¸€å¸§ä¸­çš„äººï¼Œç„¶åŽå¯¹æ¯ä¸ªäººçš„å§¿æ€è¿›è¡Œæ—¶åºå»ºæ¨¡ã€‚è¿™ç§æ–¹æ³•ä¾èµ–äºŽå¯å‘å¼æ“ä½œï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€RoIè£å‰ªå’Œéžæžå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ï¼Œè¿™äº›æ“ä½œä¼šå¼•å…¥è¯¯å·®ï¼Œå¹¶ä¸”æ•ˆçŽ‡è¾ƒä½Žã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§ç«¯åˆ°ç«¯çš„æ–¹æ³•æ¥ç›´æŽ¥ä»Žè§†é¢‘ä¸­ä¼°è®¡å¤šäººå§¿æ€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Transformerç½‘ç»œç›´æŽ¥å»ºæ¨¡è§†é¢‘ä¸­å¤šäººå§¿æ€çš„æ—¶ç©ºä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼•å…¥å§¿æ€æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿå…³æ³¨åˆ°åŒä¸€äººåœ¨ä¸åŒå¸§ä¹‹é—´çš„å§¿æ€ç‰¹å¾ï¼Œä»Žè€Œå®žçŽ°å‡†ç¡®çš„æ—¶åºå…³è”ã€‚åŒæ—¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡å§¿æ€å…³é”®ç‚¹ä¹‹é—´çš„æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œè¿›ä¸€æ­¥æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šPAVE-Netçš„æ•´ä½“æž¶æž„åŒ…æ‹¬ä¸€ä¸ªç©ºé—´ç¼–ç å™¨å’Œä¸€ä¸ªæ—¶ç©ºå§¿æ€è§£ç å™¨ã€‚ç©ºé—´ç¼–ç å™¨ç”¨äºŽæå–æ¯ä¸€å¸§çš„ç‰¹å¾ï¼Œæ—¶ç©ºå§¿æ€è§£ç å™¨åˆ™åˆ©ç”¨Transformerç½‘ç»œå»ºæ¨¡è·¨å¸§çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚è§£ç å™¨ä½¿ç”¨å§¿æ€æŸ¥è¯¢ï¼ˆpose queryï¼‰æ¥å®šä½å’Œä¼°è®¡æ¯ä¸ªäººçš„å§¿æ€ã€‚å§¿æ€æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶è¢«é›†æˆåˆ°è§£ç å™¨ä¸­ï¼Œä»¥å®žçŽ°å‡†ç¡®çš„æ—¶åºå…³è”ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†ç¬¬ä¸€ä¸ªç”¨äºŽå¤šå¸§2Däººä½“å§¿æ€ä¼°è®¡çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPAVE-Neté¿å…äº†å¯å‘å¼æ“ä½œï¼Œå¯ä»¥ç›´æŽ¥ä»Žè§†é¢‘ä¸­ä¼°è®¡å¤šäººå§¿æ€ã€‚å§¿æ€æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¦ä¸€ä¸ªå…³é”®åˆ›æ–°ï¼Œå®ƒä½¿å¾—ç½‘ç»œèƒ½å¤Ÿå…³æ³¨åˆ°åŒä¸€äººåœ¨ä¸åŒå¸§ä¹‹é—´çš„å§¿æ€ç‰¹å¾ï¼Œä»Žè€Œå®žçŽ°å‡†ç¡®çš„æ—¶åºå…³è”ã€‚

**å…³é”®è®¾è®¡**ï¼šPAVE-Netçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å§¿æ€æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒé€šè¿‡è®¡ç®—å§¿æ€æŸ¥è¯¢å’Œç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥ç¡®å®šæ³¨æ„åŠ›æƒé‡ï¼›2) æ—¶ç©ºå§¿æ€è§£ç å™¨ï¼Œå®ƒä½¿ç”¨Transformerç½‘ç»œå»ºæ¨¡è·¨å¸§çš„å…¨å±€ä¾èµ–å…³ç³»ï¼›3) æŸå¤±å‡½æ•°ï¼Œç”¨äºŽä¼˜åŒ–ç½‘ç»œçš„å‚æ•°ï¼ŒåŒ…æ‹¬å§¿æ€ä¼°è®¡æŸå¤±å’Œæ—¶åºå…³è”æŸå¤±ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

PAVE-Netåœ¨PoseTrack2017æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºŽå…ˆå‰çš„åŸºäºŽå›¾åƒçš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå®žçŽ°äº†6.0 mAPçš„æ”¹è¿›ã€‚åŒæ—¶ï¼ŒPAVE-Netçš„æ€§èƒ½ä¸Žæœ€å…ˆè¿›çš„ä¸¤é˜¶æ®µè§†é¢‘æ–¹æ³•å…·æœ‰ç«žäº‰åŠ›ï¼Œå¹¶ä¸”åœ¨æ•ˆçŽ‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒPAVE-Netæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤šäººè§†é¢‘å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽè§†é¢‘ç›‘æŽ§ã€äººæœºäº¤äº’ã€è¿åŠ¨åˆ†æžã€è™šæ‹ŸçŽ°å®žç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æŽ§ä¸­ï¼Œå¯ä»¥ç”¨äºŽæ£€æµ‹å¼‚å¸¸è¡Œä¸ºï¼›åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥ç”¨äºŽå®žçŽ°åŸºäºŽå§¿æ€çš„æŽ§åˆ¶ï¼›åœ¨è¿åŠ¨åˆ†æžä¸­ï¼Œå¯ä»¥ç”¨äºŽè¯„ä¼°è¿åŠ¨å‘˜çš„è¡¨çŽ°ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æå‡è¿™äº›åº”ç”¨åœºæ™¯çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames. Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation. Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a 6.0 mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video based approaches, while offering significant gains in efficiency. Project page: https://github.com/zgspose/PAVENet.

