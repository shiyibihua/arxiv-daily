---
layout: default
title: Low-Level Dataset Distillation for Medical Image Enhancement
---

# Low-Level Dataset Distillation for Medical Image Enhancement

**arXiv**: [2511.13106v1](https://arxiv.org/abs/2511.13106) | [PDF](https://arxiv.org/pdf/2511.13106.pdf)

**ä½œè€…**: Fengzhi Xu, Ziyuan Yang, Mengyu Sun, Joey Tianyi Zhou, Yi Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½Žå±‚æ•°æ®é›†è’¸é¦æ–¹æ³•ä»¥è§£å†³åŒ»å­¦å›¾åƒå¢žå¼ºä¸­çš„è®­ç»ƒæˆæœ¬ä¸Žéšç§é—®é¢˜**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒå¢žå¼º` `æ•°æ®é›†è’¸é¦` `ä½Žå±‚è§†è§‰ä»»åŠ¡` `éšç§ä¿æŠ¤` `åƒç´ çº§æ˜ å°„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä½Žå±‚ä»»åŠ¡åƒç´ çº§ä¿çœŸåº¦è¦æ±‚é«˜ï¼Œæ•°æ®é›†è’¸é¦ä¸ºæ¬ å®šé—®é¢˜ï¼Œéš¾ä»¥çº¦æŸå¯†é›†æ˜ å°„
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è§£å‰–ç›¸ä¼¼æ€§æž„å»ºå…±äº«å…ˆéªŒï¼Œé€šè¿‡SPGæ¨¡å—ä¸ªæ€§åŒ–å¹¶æ³¨å…¥æ‚£è€…çŸ¥è¯†
3. å®žéªŒæˆ–æ•ˆæžœï¼šè’¸é¦æ•°æ®é›†ä»…å«æŠ½è±¡ä¿¡æ¯ï¼Œä¿æŠ¤éšç§ï¼ŒæœªçŸ¥å…·ä½“æ€§èƒ½æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

