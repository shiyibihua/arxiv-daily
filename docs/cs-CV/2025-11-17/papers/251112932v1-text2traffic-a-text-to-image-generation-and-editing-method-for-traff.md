---
layout: default
title: Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes
---

# Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes

**arXiv**: [2511.12932v1](https://arxiv.org/abs/2511.12932) | [PDF](https://arxiv.org/pdf/2511.12932.pdf)

**ä½œè€…**: Feng Lv, Haoxuan Feng, Zilu Zhang, Chunlong Xia, Yanfeng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºText2Trafficæ¡†æž¶ï¼Œé€šè¿‡å¯æŽ§æŽ©ç æœºåˆ¶å’Œå¤šè§†è§’æ•°æ®å¢žå¼ºäº¤é€šåœºæ™¯å›¾åƒç”Ÿæˆä¸Žç¼–è¾‘**

**å…³é”®è¯**: `æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆ` `äº¤é€šåœºæ™¯ç¼–è¾‘` `å¯æŽ§æŽ©ç æœºåˆ¶` `å¤šè§†è§’æ•°æ®å¢žå¼º` `ä¸¤é˜¶æ®µè®­ç»ƒ` `æŽ©ç åŠ æƒæŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäº¤é€šåœºæ™¯ç”Ÿæˆä¸­è¯­ä¹‰ä¸°å¯Œåº¦ä¸è¶³ã€è§†è§’å—é™ã€è§†è§‰ä¿çœŸåº¦ä½Žå’Œæ–‡æœ¬-å›¾åƒå¯¹é½å·®
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒå’ŒæŽ©ç åŒºåŸŸåŠ æƒæŸå¤±ï¼Œæå‡å°å…ƒç´ ç”Ÿæˆè´¨é‡å’Œæ–‡æœ¬å¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº¤é€šåœºæ™¯æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¸­å®žçŽ°é¢†å…ˆæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

