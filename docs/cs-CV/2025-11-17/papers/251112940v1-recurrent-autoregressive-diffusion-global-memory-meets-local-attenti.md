---
layout: default
title: Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention
---

# Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention

**arXiv**: [2511.12940v1](https://arxiv.org/abs/2511.12940) | [PDF](https://arxiv.org/pdf/2511.12940.pdf)

**ä½œè€…**: Taiye Chen, Zihan Ding, Anjian Li, Christina Zhang, Zeqi Xiao, Yisen Wang, Chi Jin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRADæ¡†æž¶ï¼Œé€šè¿‡å¾ªçŽ¯è‡ªå›žå½’æ‰©æ•£æ¨¡åž‹è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„è®°å¿†å’Œæ—¶ç©ºä¸€è‡´æ€§é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç”Ÿæˆ` `å¾ªçŽ¯ç¥žç»ç½‘ç»œ` `æ‰©æ•£æ¨¡åž‹` `è‡ªå›žå½’æ¨¡åž‹` `è®°å¿†ç½‘ç»œ` `æ—¶ç©ºä¸€è‡´æ€§` `LSTM` `è§†é¢‘å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡åž‹åœ¨é•¿æ—¶åºå»ºæ¨¡ä¸­é¢ä¸´è®°å¿†å®¹é‡ä¸è¶³å’Œæ—¶ç©ºä¸€è‡´æ€§é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘å‡ºçŽ°é—å¿˜å’Œä¸è¿žè´¯çŽ°è±¡ã€‚
2. è®ºæ–‡æå‡ºå¾ªçŽ¯è‡ªå›žå½’æ‰©æ•£ï¼ˆRADï¼‰æ¡†æž¶ï¼Œåˆ©ç”¨LSTMçš„å¾ªçŽ¯ç‰¹æ€§è¿›è¡Œè®°å¿†æ›´æ–°å’Œæ£€ç´¢ï¼Œä¿è¯è®­ç»ƒå’ŒæŽ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒRADåœ¨Memory Mazeå’ŒMinecraftæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†LSTMåœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå±•çŽ°å‡ºæ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é€šè¿‡æŽ©ç æ¡ä»¶è¿›è¡Œæ— é™é•¿è§†é¢‘çš„è‡ªå›žå½’ç”Ÿæˆæ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™ç±»æ¨¡åž‹é€šå¸¸é‡‡ç”¨å±€éƒ¨å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„è®°å¿†åŽ‹ç¼©å’Œæ£€ç´¢èƒ½åŠ›ï¼Œéš¾ä»¥è¿›è¡Œè¶…å‡ºçª—å£å¤§å°çš„é•¿æœŸç”Ÿæˆï¼Œå¯¼è‡´é—å¿˜å’Œæ—¶ç©ºä¸ä¸€è‡´é—®é¢˜ã€‚ä¸ºäº†åœ¨å›ºå®šçš„è®°å¿†é¢„ç®—å†…å¢žå¼ºåŽ†å²ä¿¡æ¯çš„ä¿ç•™ï¼Œæœ¬æ–‡å°†å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼ˆRNNï¼‰å¼•å…¥æ‰©æ•£Transformeræ¡†æž¶ã€‚å…·ä½“è€Œè¨€ï¼Œç»“åˆLSTMä¸Žæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡åž‹èƒ½å¤Ÿè¾¾åˆ°ä¸Žæœ€å…ˆè¿›çš„RNNæ¨¡å—ï¼ˆå¦‚TTTå’ŒMamba2ï¼‰ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒçŽ°æœ‰çš„æ‰©æ•£-RNNæ–¹æ³•å¸¸å¸¸ç”±äºŽè®­ç»ƒ-æŽ¨ç†å·®å¼‚æˆ–çª—å£é—´ç¼ºä¹é‡å è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¾ªçŽ¯è‡ªå›žå½’æ‰©æ•£ï¼ˆRADï¼‰æ¡†æž¶ï¼Œè¯¥æ¡†æž¶åœ¨è®­ç»ƒå’ŒæŽ¨ç†æ—¶å‡æ‰§è¡Œé€å¸§è‡ªå›žå½’ä»¥è¿›è¡Œè®°å¿†æ›´æ–°å’Œæ£€ç´¢ã€‚åœ¨Memory Mazeå’ŒMinecraftæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒRADåœ¨é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ï¼Œçªå‡ºäº†LSTMåœ¨åºåˆ—å»ºæ¨¡ä¸­çš„æ•ˆçŽ‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽTransformerçš„è§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶ï¼Œç”±äºŽå±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„é™åˆ¶ï¼Œéš¾ä»¥æœ‰æ•ˆè®°å¿†å’Œæ£€ç´¢åŽ†å²ä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘å‡ºçŽ°æ—¶ç©ºä¸ä¸€è‡´æ€§å’Œé—å¿˜çŽ°è±¡ã€‚è¿™äº›æ¨¡åž‹é€šå¸¸æ— æ³•ç»´æŒé•¿æœŸä¾èµ–å…³ç³»ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œç‰¹åˆ«æ˜¯LSTMï¼Œèžå…¥åˆ°æ‰©æ•£æ¨¡åž‹ä¸­ï¼Œåˆ©ç”¨LSTMçš„è®°å¿†èƒ½åŠ›æ¥å­˜å‚¨å’Œæ£€ç´¢åŽ†å²ä¿¡æ¯ã€‚é€šè¿‡å¾ªçŽ¯æ›´æ–°è®°å¿†çŠ¶æ€ï¼Œæ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œä»Žè€Œç”Ÿæˆæ›´è¿žè´¯å’Œä¸€è‡´çš„é•¿è§†é¢‘ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRADæ¡†æž¶çš„æ ¸å¿ƒæ˜¯å¾ªçŽ¯è‡ªå›žå½’æ‰©æ•£æ¨¡åž‹ã€‚è¯¥æ¨¡åž‹ä»¥å¸§ä¸ºå•ä½è¿›è¡Œè‡ªå›žå½’ç”Ÿæˆï¼Œæ¯ä¸€å¸§çš„ç”Ÿæˆéƒ½ä¾èµ–äºŽå‰ä¸€å¸§çš„è®°å¿†çŠ¶æ€ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š1) è¾“å…¥å½“å‰å¸§å’Œå‰ä¸€å¸§çš„è®°å¿†çŠ¶æ€ï¼›2) ä½¿ç”¨æ‰©æ•£æ¨¡åž‹ç”Ÿæˆä¸‹ä¸€å¸§ï¼›3) ä½¿ç”¨LSTMæ›´æ–°è®°å¿†çŠ¶æ€ï¼Œå¹¶å°†æ›´æ–°åŽçš„è®°å¿†çŠ¶æ€ä¼ é€’ç»™ä¸‹ä¸€å¸§çš„ç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRADçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å¾ªçŽ¯è‡ªå›žå½’çš„ç»“æž„ï¼Œä»¥åŠåœ¨è®­ç»ƒå’ŒæŽ¨ç†è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´çš„è®°å¿†æ›´æ–°æ–¹å¼ã€‚ä¼ ç»Ÿçš„æ‰©æ•£-RNNæ–¹æ³•å¸¸å¸¸å­˜åœ¨è®­ç»ƒ-æŽ¨ç†å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚RADé€šè¿‡é€å¸§è‡ªå›žå½’çš„æ–¹å¼ï¼Œç¡®ä¿äº†è®­ç»ƒå’ŒæŽ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§ï¼Œä»Žè€Œæé«˜äº†æ¨¡åž‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šRADçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LSTMä½œä¸ºè®°å¿†æ¨¡å—ï¼Œè´Ÿè´£å­˜å‚¨å’Œæ›´æ–°åŽ†å²ä¿¡æ¯ï¼›2) é‡‡ç”¨å¸§çº§åˆ«çš„è‡ªå›žå½’ç”Ÿæˆæ–¹å¼ï¼Œä¿è¯è®­ç»ƒå’ŒæŽ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§ï¼›3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡åž‹å­¦ä¹ åˆ°æœ‰æ•ˆçš„è®°å¿†è¡¨ç¤ºã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä¾‹å¦‚LSTMçš„éšè—å±‚å¤§å°ï¼Œæ‰©æ•£æ¨¡åž‹çš„æž¶æž„é€‰æ‹©ç­‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRADåœ¨Memory Mazeå’ŒMinecraftæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨Memory Mazeæ•°æ®é›†ä¸Šï¼ŒRADèƒ½å¤Ÿç”Ÿæˆæ›´é•¿ä¸”æ›´è¿žè´¯çš„è§†é¢‘ï¼ŒæˆåŠŸçŽ‡æé«˜äº†XX%ã€‚åœ¨Minecraftæ•°æ®é›†ä¸Šï¼ŒRADç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰è´¨é‡å’Œæ—¶ç©ºä¸€è‡´æ€§æ–¹é¢ä¹Ÿä¼˜äºŽå…¶ä»–åŸºçº¿æ¨¡åž‹ã€‚è¿™äº›ç»“æžœéªŒè¯äº†RADåœ¨é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦ç”Ÿæˆé•¿æ—¶åºè§†é¢‘çš„åœºæ™¯ï¼Œä¾‹å¦‚æ¸¸æˆAIã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡ç”Ÿæˆé€¼çœŸä¸”è¿žè´¯çš„è§†é¢‘ï¼Œå¯ä»¥å¸®åŠ©AIç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œé¢„æµ‹çŽ¯å¢ƒå˜åŒ–ï¼Œä»Žè€Œåšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºŽè§†é¢‘ç¼–è¾‘ã€å†…å®¹åˆ›ä½œç­‰é¢†åŸŸï¼Œæé«˜è§†é¢‘åˆ¶ä½œçš„æ•ˆçŽ‡å’Œè´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

