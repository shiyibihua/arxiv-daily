---
layout: default
title: Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention
---

# Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.12940" target="_blank" class="toolbar-btn">arXiv: 2511.12940v1</a>
    <a href="https://arxiv.org/pdf/2511.12940.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12940v1" 
            onclick="toggleFavorite(this, '2511.12940v1', 'Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Taiye Chen, Zihan Ding, Anjian Li, Christina Zhang, Zeqi Xiao, Yisen Wang, Chi Jin

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RADÊ°ÜÊû∂ÔºåÈÄöËøáÂæ™ÁéØËá™ÂõûÂΩíÊâ©Êï£Ê®°ÂûãËß£ÂÜ≥ÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËÆ∞ÂøÜÂíåÊó∂Á©∫‰∏ÄËá¥ÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁîüÊàê` `Âæ™ÁéØÁ•ûÁªèÁΩëÁªú` `Êâ©Êï£Ê®°Âûã` `Ëá™ÂõûÂΩíÊ®°Âûã` `ËÆ∞ÂøÜÁΩëÁªú` `Êó∂Á©∫‰∏ÄËá¥ÊÄß` `LSTM` `ËßÜÈ¢ëÂª∫Ê®°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÈïøÊó∂Â∫èÂª∫Ê®°‰∏≠Èù¢‰∏¥ËÆ∞ÂøÜÂÆπÈáè‰∏çË∂≥ÂíåÊó∂Á©∫‰∏ÄËá¥ÊÄßÈóÆÈ¢òÔºåÂØºËá¥ÁîüÊàêËßÜÈ¢ëÂá∫Áé∞ÈÅóÂøòÂíå‰∏çËøûË¥ØÁé∞Ë±°„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Âæ™ÁéØËá™ÂõûÂΩíÊâ©Êï£ÔºàRADÔºâÊ°ÜÊû∂ÔºåÂà©Áî®LSTMÁöÑÂæ™ÁéØÁâπÊÄßËøõË°åËÆ∞ÂøÜÊõ¥Êñ∞ÂíåÊ£ÄÁ¥¢Ôºå‰øùËØÅËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ãÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåRADÂú®Memory MazeÂíåMinecraftÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜLSTMÂú®ÈïøÂ∫èÂàóÂª∫Ê®°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÂ±ïÁé∞Âá∫ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÄöËøáÊé©Á†ÅÊù°‰ª∂ËøõË°åÊó†ÈôêÈïøËßÜÈ¢ëÁöÑËá™ÂõûÂΩíÁîüÊàêÊñπÈù¢„ÄÇÁÑ∂ËÄåÔºåËøôÁ±ªÊ®°ÂûãÈÄöÂ∏∏ÈááÁî®Â±ÄÈÉ®ÂÖ®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁº∫‰πèÊúâÊïàÁöÑËÆ∞ÂøÜÂéãÁº©ÂíåÊ£ÄÁ¥¢ËÉΩÂäõÔºåÈöæ‰ª•ËøõË°åË∂ÖÂá∫Á™óÂè£Â§ßÂ∞èÁöÑÈïøÊúüÁîüÊàêÔºåÂØºËá¥ÈÅóÂøòÂíåÊó∂Á©∫‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÂú®Âõ∫ÂÆöÁöÑËÆ∞ÂøÜÈ¢ÑÁÆóÂÜÖÂ¢ûÂº∫ÂéÜÂè≤‰ø°ÊÅØÁöÑ‰øùÁïôÔºåÊú¨ÊñáÂ∞ÜÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâÂºïÂÖ•Êâ©Êï£TransformerÊ°ÜÊû∂„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÁªìÂêàLSTM‰∏éÊ≥®ÊÑèÂäõÁöÑÊâ©Êï£Ê®°ÂûãËÉΩÂ§üËææÂà∞‰∏éÊúÄÂÖàËøõÁöÑRNNÊ®°ÂùóÔºàÂ¶ÇTTTÂíåMamba2ÔºâÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÁöÑÊâ©Êï£-RNNÊñπÊ≥ïÂ∏∏Â∏∏Áî±‰∫éËÆ≠ÁªÉ-Êé®ÁêÜÂ∑ÆÂºÇÊàñÁ™óÂè£Èó¥Áº∫‰πèÈáçÂè†ËÄåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂæ™ÁéØËá™ÂõûÂΩíÊâ©Êï£ÔºàRADÔºâÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂ÂùáÊâßË°åÈÄêÂ∏ßËá™ÂõûÂΩí‰ª•ËøõË°åËÆ∞ÂøÜÊõ¥Êñ∞ÂíåÊ£ÄÁ¥¢„ÄÇÂú®Memory MazeÂíåMinecraftÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåRADÂú®ÈïøËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÂÖ∑Êúâ‰ºòË∂äÊÄßÔºåÁ™ÅÂá∫‰∫ÜLSTMÂú®Â∫èÂàóÂª∫Ê®°‰∏≠ÁöÑÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éTransformerÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®ÁîüÊàêÈïøËßÜÈ¢ëÊó∂ÔºåÁî±‰∫éÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÈôêÂà∂ÔºåÈöæ‰ª•ÊúâÊïàËÆ∞ÂøÜÂíåÊ£ÄÁ¥¢ÂéÜÂè≤‰ø°ÊÅØÔºåÂØºËá¥ÁîüÊàêÁöÑËßÜÈ¢ëÂá∫Áé∞Êó∂Á©∫‰∏ç‰∏ÄËá¥ÊÄßÂíåÈÅóÂøòÁé∞Ë±°„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏Êó†Ê≥ïÁª¥ÊåÅÈïøÊúü‰æùËµñÂÖ≥Á≥ªÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâÔºåÁâπÂà´ÊòØLSTMÔºåËûçÂÖ•Âà∞Êâ©Êï£Ê®°Âûã‰∏≠ÔºåÂà©Áî®LSTMÁöÑËÆ∞ÂøÜËÉΩÂäõÊù•Â≠òÂÇ®ÂíåÊ£ÄÁ¥¢ÂéÜÂè≤‰ø°ÊÅØ„ÄÇÈÄöËøáÂæ™ÁéØÊõ¥Êñ∞ËÆ∞ÂøÜÁä∂ÊÄÅÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÈïøÊúü‰æùËµñÂÖ≥Á≥ªÔºå‰ªéËÄåÁîüÊàêÊõ¥ËøûË¥ØÂíå‰∏ÄËá¥ÁöÑÈïøËßÜÈ¢ë„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRADÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØÂæ™ÁéØËá™ÂõûÂΩíÊâ©Êï£Ê®°Âûã„ÄÇËØ•Ê®°Âûã‰ª•Â∏ß‰∏∫Âçï‰ΩçËøõË°åËá™ÂõûÂΩíÁîüÊàêÔºåÊØè‰∏ÄÂ∏ßÁöÑÁîüÊàêÈÉΩ‰æùËµñ‰∫éÂâç‰∏ÄÂ∏ßÁöÑËÆ∞ÂøÜÁä∂ÊÄÅ„ÄÇÂÖ∑‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ËæìÂÖ•ÂΩìÂâçÂ∏ßÂíåÂâç‰∏ÄÂ∏ßÁöÑËÆ∞ÂøÜÁä∂ÊÄÅÔºõ2) ‰ΩøÁî®Êâ©Êï£Ê®°ÂûãÁîüÊàê‰∏ã‰∏ÄÂ∏ßÔºõ3) ‰ΩøÁî®LSTMÊõ¥Êñ∞ËÆ∞ÂøÜÁä∂ÊÄÅÔºåÂπ∂Â∞ÜÊõ¥Êñ∞ÂêéÁöÑËÆ∞ÂøÜÁä∂ÊÄÅ‰º†ÈÄíÁªô‰∏ã‰∏ÄÂ∏ßÁöÑÁîüÊàêËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRADÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âæ™ÁéØËá™ÂõûÂΩíÁöÑÁªìÊûÑÔºå‰ª•ÂèäÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠‰øùÊåÅ‰∏ÄËá¥ÁöÑËÆ∞ÂøÜÊõ¥Êñ∞ÊñπÂºè„ÄÇ‰º†ÁªüÁöÑÊâ©Êï£-RNNÊñπÊ≥ïÂ∏∏Â∏∏Â≠òÂú®ËÆ≠ÁªÉ-Êé®ÁêÜÂ∑ÆÂºÇÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇRADÈÄöËøáÈÄêÂ∏ßËá™ÂõûÂΩíÁöÑÊñπÂºèÔºåÁ°Æ‰øù‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ãÁöÑ‰∏ÄËá¥ÊÄßÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRADÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®LSTM‰Ωú‰∏∫ËÆ∞ÂøÜÊ®°ÂùóÔºåË¥üË¥£Â≠òÂÇ®ÂíåÊõ¥Êñ∞ÂéÜÂè≤‰ø°ÊÅØÔºõ2) ÈááÁî®Â∏ßÁ∫ßÂà´ÁöÑËá™ÂõûÂΩíÁîüÊàêÊñπÂºèÔºå‰øùËØÅËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ãÁöÑ‰∏ÄËá¥ÊÄßÔºõ3) ËÆæËÆ°ÂêàÈÄÇÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±Ê®°ÂûãÂ≠¶‰π†Âà∞ÊúâÊïàÁöÑËÆ∞ÂøÜË°®Á§∫„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞Ôºå‰æãÂ¶ÇLSTMÁöÑÈöêËóèÂ±ÇÂ§ßÂ∞èÔºåÊâ©Êï£Ê®°ÂûãÁöÑÊû∂ÊûÑÈÄâÊã©Á≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRADÂú®Memory MazeÂíåMinecraftÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Memory MazeÊï∞ÊçÆÈõÜ‰∏äÔºåRADËÉΩÂ§üÁîüÊàêÊõ¥Èïø‰∏îÊõ¥ËøûË¥ØÁöÑËßÜÈ¢ëÔºåÊàêÂäüÁéáÊèêÈ´ò‰∫ÜXX%„ÄÇÂú®MinecraftÊï∞ÊçÆÈõÜ‰∏äÔºåRADÁîüÊàêÁöÑËßÜÈ¢ëÂú®ËßÜËßâË¥®ÈáèÂíåÊó∂Á©∫‰∏ÄËá¥ÊÄßÊñπÈù¢‰πü‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊ®°Âûã„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜRADÂú®ÈïøËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁîüÊàêÈïøÊó∂Â∫èËßÜÈ¢ëÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊ∏∏ÊàèAI„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂Á≠â„ÄÇÈÄöËøáÁîüÊàêÈÄºÁúü‰∏îËøûË¥ØÁöÑËßÜÈ¢ëÔºåÂèØ‰ª•Â∏ÆÂä©AIÁ≥ªÁªüÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÈ¢ÑÊµãÁéØÂ¢ÉÂèòÂåñÔºå‰ªéËÄåÂÅöÂá∫Êõ¥ÂêàÁêÜÁöÑÂÜ≥Á≠ñ„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Áî®‰∫éËßÜÈ¢ëÁºñËæë„ÄÅÂÜÖÂÆπÂàõ‰ΩúÁ≠âÈ¢ÜÂüüÔºåÊèêÈ´òËßÜÈ¢ëÂà∂‰ΩúÁöÑÊïàÁéáÂíåË¥®Èáè„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

