---
layout: default
title: CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model
---

# CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model

**arXiv**: [2511.13121v1](https://arxiv.org/abs/2511.13121) | [PDF](https://arxiv.org/pdf/2511.13121.pdf)

**ä½œè€…**: Yuqi Zhang, Guanying Chen, Jiaxing Chen, Chuanyu Fu, Chuan Huang, Shuguang Cui

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCloseUpShotæ¡†æž¶ï¼Œé€šè¿‡ç‚¹æ¡ä»¶æ‰©æ•£æ¨¡åž‹è§£å†³ç¨€ç–è§†å›¾ä¸‹è¿‘æ™¯æ–°è§†è§’åˆæˆçš„æŒ‘æˆ˜**

**å…³é”®è¯**: `æ–°è§†è§’åˆæˆ` `ç‚¹æ¡ä»¶æ‰©æ•£æ¨¡åž‹` `ç¨€ç–è§†å›¾é‡å»º` `é®æŒ¡æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶` `å…¨å±€ç»“æž„å¼•å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–è¾“å…¥è§†å›¾åœ¨è¿‘æ™¯åœºæ™¯ä¸­éš¾ä»¥æ•æ‰ç»†ç²’åº¦ç»†èŠ‚ï¼Œå¯¼è‡´é‡å»ºè´¨é‡å·®
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚æ‰­æ›²å’Œé®æŒ¡æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶ï¼Œç»“åˆå…¨å±€ç»“æž„å¼•å¯¼æå‡æ¡ä»¶å›¾åƒè´¨é‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨è¿‘æ™¯æ–°è§†è§’åˆæˆä¸­è¡¨çŽ°çªå‡º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

