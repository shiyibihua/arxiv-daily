---
layout: default
title: OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving
---

# OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving

**arXiv**: [2511.13707v1](https://arxiv.org/abs/2511.13707) | [PDF](https://arxiv.org/pdf/2511.13707.pdf)

**ä½œè€…**: Xiaoyu Liang, Ziang Liu, Kelvin Lin, Edward Gu, Ruolin Ye, Tam Nguyen, Cynthia Hsu, Zhanxin Wu, Xiaoman Yang, Christy Sum Yu Cheung, Harold Soh, Katherine Dimitropoulou, Tapomayukh Bhattacharjee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenRoboCareå¤šæ¨¡æ€æ•°æ®é›†ä»¥è§£å†³æœºå™¨äººç…§æŠ¤ä¸­ç¼ºä¹ä¸“å®¶ç¤ºèŒƒæ•°æ®çš„é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººç…§æŠ¤` `å¤šæ¨¡æ€æ•°æ®é›†` `ä¸“å®¶ç¤ºèŒƒ` `æ—¥å¸¸æ´»åŠ¨ä»»åŠ¡` `ç‰©ç†äººæœºäº¤äº’` `æ„ŸçŸ¥æŒ‘æˆ˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººç…§æŠ¤ä»»åŠ¡ç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„ä¸“å®¶ç¤ºèŒƒæ•°æ®é›†ï¼Œå½±å“ç‰©ç†äººæœºäº¤äº’çš„æ„ŸçŸ¥ä¸Žè§„åˆ’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ”¶é›†21åèŒä¸šæ²»ç–—å¸ˆæ‰§è¡Œ15é¡¹æ—¥å¸¸æ´»åŠ¨ä»»åŠ¡çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒåŒ…æ‹¬RGB-Dè§†é¢‘ã€å§¿æ€è·Ÿè¸ªç­‰äº”ç§æ¨¡æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºæ•°æ®é›†å¯¹çŽ°æœ‰æœºå™¨äººæ„ŸçŸ¥å’Œäººç±»æ´»åŠ¨è¯†åˆ«æ–¹æ³•æž„æˆæŒ‘æˆ˜ï¼Œæå‡è¾…åŠ©æœºå™¨äººå®‰å…¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.

