---
layout: default
title: Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos
---

# Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos

**arXiv**: [2511.12882v1](https://arxiv.org/abs/2511.12882) | [PDF](https://arxiv.org/pdf/2511.12882.pdf)

**ä½œè€…**: Taiyi Su, Jian Zhu, Yaxuan Li, Chong Ma, Zitai Huang, Yichen Zhu, Hanli Wang, Yi Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMTV-Worldæ¨¡åž‹ï¼Œé€šè¿‡å¤šè§†è§’è½¨è¿¹è§†é¢‘æŽ§åˆ¶è§£å†³å…·èº«ä¸–ç•Œæ¨¡åž‹ç‰©ç†äº¤äº’ä¸ä¸€è‡´é—®é¢˜**

**å…³é”®è¯**: `å…·èº«ä¸–ç•Œæ¨¡åž‹` `å¤šè§†è§’è½¨è¿¹è§†é¢‘` `ç‰©ç†äº¤äº’ä¸€è‡´æ€§` `æœºå™¨äººæŽ§åˆ¶` `è§†è§‰é¢„æµ‹` `ç©ºé—´ä¿¡æ¯è¡¥å¿`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¨¡åž‹éš¾ä»¥å°†ä½Žçº§åŠ¨ä½œç²¾ç¡®è½¬æ¢ä¸ºæœºå™¨äººè¿åŠ¨ï¼Œå¯¼è‡´é¢„æµ‹å¸§ä¸ŽçŽ°å®žç‰©ç†äº¤äº’ä¸ä¸€è‡´
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤šè§†è§’è½¨è¿¹è§†é¢‘ä½œä¸ºæŽ§åˆ¶ä¿¡å·ï¼Œè¡¥å¿ç©ºé—´ä¿¡æ¯æŸå¤±ï¼Œæå‡é¢„æµ‹ä¸€è‡´æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤æ‚åŒè‡‚åœºæ™¯ä¸­å®žçŽ°ç²¾ç¡®æŽ§åˆ¶å’Œå‡†ç¡®ç‰©ç†äº¤äº’å»ºæ¨¡ï¼Œé‡‡ç”¨JaccardæŒ‡æ•°è¯„ä¼°ç©ºé—´ä¸€è‡´æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

