---
layout: default
title: CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation
---

# CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.12919" target="_blank" class="toolbar-btn">arXiv: 2511.12919v3</a>
    <a href="https://arxiv.org/pdf/2511.12919.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12919v3" 
            onclick="toggleFavorite(this, '2511.12919v3', 'CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Dexin Zuo, Ang Li, Wei Wang, Wenxian Yu, Danping Zou

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17 (Êõ¥Êñ∞: 2025-12-15)

**Â§áÊ≥®**: 7 pages, accepted by AAAI 2026 (oral)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**CoordARÔºöÂü∫‰∫éËá™ÂõûÂΩíÂùêÊ†áÂõæÁîüÊàêÁöÑÂçïÂèÇËÄÉÊñ∞Áâ©‰Ωì6D‰ΩçÂßø‰º∞ËÆ°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `6D‰ΩçÂßø‰º∞ËÆ°` `ÂçïÂèÇËÄÉÂõæÂÉè` `Ëá™ÂõûÂΩíÊ®°Âûã` `ÂùêÊ†áÂõæÁîüÊàê` `Transformer`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÂçïÂèÇËÄÉÂõæÂÉèÁöÑ6D‰ΩçÂßø‰º∞ËÆ°ÊñπÊ≥ï‰æùËµñÂÆûÂÄºÂùêÊ†áÂõûÂΩíÔºåÁº∫‰πèÂÖ®Â±Ä‰∏ÄËá¥ÊÄßÔºåÈöæ‰ª•Â§ÑÁêÜÂØπÁß∞ÂíåÈÅÆÊå°ÊÉÖÂÜµ„ÄÇ
2. CoordARÊèêÂá∫‰∏ÄÁßçËá™ÂõûÂΩíÊ°ÜÊû∂ÔºåÂ∞Ü3D-3DÂØπÂ∫îÂÖ≥Á≥ªÂª∫Ê®°‰∏∫Á¶ªÊï£tokenÁöÑÊò†Â∞ÑÔºåÈÄöËøáÊ¶ÇÁéáËá™ÂõûÂΩíÊñπÂºèÈ¢ÑÊµãÂùêÊ†á„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoordARÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂØπÂØπÁß∞„ÄÅÈÅÆÊå°Á≠âÊÉÖÂÜµÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫CoordARÔºå‰∏ÄÁßçÁî®‰∫éÊñ∞Áâ©‰ΩìÂçïÂèÇËÄÉ6D‰ΩçÂßø‰º∞ËÆ°ÁöÑËá™ÂõûÂΩíÊ°ÜÊû∂„ÄÇÈíàÂØπÁé∞ÊúâÊñπÊ≥ï‰æùËµñÂÆûÂÄºÂùêÊ†áÂõûÂΩíÔºåÂ≠òÂú®ÂÖ®Â±Ä‰∏ÄËá¥ÊÄß‰∏çË∂≥„ÄÅÂØπÁß∞ÊàñÈÅÆÊå°Âú∫ÊôØ‰∏ãÁº∫‰πè‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°Á≠âÈóÆÈ¢òÔºåCoordARÂ∞ÜÂèÇËÄÉËßÜÂõæÂíåÊü•ËØ¢ËßÜÂõæ‰πãÈó¥ÁöÑ3D-3DÂØπÂ∫îÂÖ≥Á≥ªÂª∫Ê®°‰∏∫Á¶ªÊï£tokenÁöÑÊò†Â∞ÑÔºåÂπ∂ÈÄöËøáËá™ÂõûÂΩíÂíåÊ¶ÇÁéáÁöÑÊñπÂºèËé∑Âæó„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Á≤æÁ°ÆÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂõûÂΩíÔºåCoordARÂºïÂÖ•‰∫ÜÔºö1) ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂùêÊ†áÂõætokenÂåñÊñπÊ≥ïÔºåÊîØÊåÅÂú®Á¶ªÊï£3DÁ©∫Èó¥‰∏äÁöÑÊ¶ÇÁéáÈ¢ÑÊµãÔºõ2) ‰∏ÄÁßçÊ®°ÊÄÅËß£ËÄ¶ÁºñÁ†ÅÁ≠ñÁï•ÔºåÂàÜÂà´ÁºñÁ†ÅRGBÂ§ñËßÇÂíåÂùêÊ†áÁ∫øÁ¥¢Ôºõ3) ‰∏Ä‰∏™Ëá™ÂõûÂΩíTransformerËß£Á†ÅÂô®Ôºå‰ª•‰ΩçÁΩÆÂØπÈΩêÁöÑÊü•ËØ¢ÁâπÂæÅÂíåÈÉ®ÂàÜÁîüÊàêÁöÑtokenÂ∫èÂàó‰∏∫Êù°‰ª∂„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåCoordARÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®ÁúüÂÆû‰∏ñÁïåÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÂØπÂØπÁß∞ÊÄß„ÄÅÈÅÆÊå°ÂíåÂÖ∂‰ªñÊåëÊàòÁöÑÂº∫Â§ßÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂçïÂèÇËÄÉÂõæÂÉèÁöÑ6D‰ΩçÂßø‰º∞ËÆ°ÊñπÊ≥ïÔºå‰æùËµñ‰∫éÁõ¥Êé•ÂõûÂΩí3DÂùêÊ†áÔºåËøôÁßçÊñπÊ≥ïÂèóÈôê‰∫éÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÁöÑÂ±ÄÈÉ®ÊÄßÔºåÈöæ‰ª•‰øùËØÅÂÖ®Â±Ä‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÂú®Áâ©‰ΩìÂÖ∑ÊúâÂØπÁß∞ÊÄßÊàñÂ≠òÂú®ÈÅÆÊå°Êó∂ÔºåÂùêÊ†áÂõûÂΩíÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈöæ‰ª•Âª∫Ê®°ÔºåÂØºËá¥‰ΩçÂßø‰º∞ËÆ°Á≤æÂ∫¶‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCoordARÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü3DÂùêÊ†áÂõûÂΩíÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Á¶ªÊï£tokenÁöÑÁîüÊàêÈóÆÈ¢òÔºåÂà©Áî®Ëá™ÂõûÂΩíÊ®°ÂûãÈ¢ÑÊµãÂèÇËÄÉÂõæÂÉèÂíåÊü•ËØ¢ÂõæÂÉè‰πãÈó¥ÁöÑ3D-3DÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÈÄöËøáÂ∞Ü3DÁ©∫Èó¥Á¶ªÊï£Âåñ‰∏∫‰∏ÄÁ≥ªÂàótokenÔºåÂπ∂‰ΩøÁî®TransformerËøõË°åÂ∫èÂàóÂª∫Ê®°ÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÂÖ®Â±Ä‰ø°ÊÅØÂíåÂª∫Ê®°‰∏çÁ°ÆÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoordARÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÂùêÊ†áÂõætokenÂåñÊ®°ÂùóÔºåÂ∞Ü3DÁ©∫Èó¥Á¶ªÊï£Âåñ‰∏∫tokenÔºõ2) Ê®°ÊÄÅËß£ËÄ¶ÁºñÁ†ÅÂô®ÔºåÂàÜÂà´ÊèêÂèñÂèÇËÄÉÂõæÂÉèÂíåÊü•ËØ¢ÂõæÂÉèÁöÑRGBÂ§ñËßÇÁâπÂæÅÂíåÂùêÊ†á‰ø°ÊÅØÔºõ3) Ëá™ÂõûÂΩíTransformerËß£Á†ÅÂô®Ôºå‰ª•‰ΩçÁΩÆÂØπÈΩêÁöÑÊü•ËØ¢ÁâπÂæÅÂíåÈÉ®ÂàÜÁîüÊàêÁöÑtokenÂ∫èÂàó‰∏∫Êù°‰ª∂ÔºåÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™tokenÔºå‰ªéËÄåÁîüÊàêÂÆåÊï¥ÁöÑÂùêÊ†áÂõæ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCoordARÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜÂùêÊ†áÂõætokenÂåñÊñπÊ≥ïÔºåÂ∞ÜËøûÁª≠ÁöÑ3DÂùêÊ†áÁ©∫Èó¥Á¶ªÊï£Âåñ‰∏∫tokenÔºå‰æø‰∫é‰ΩøÁî®Â∫èÂàóÊ®°ÂûãËøõË°åÂª∫Ê®°Ôºõ2) ÈááÁî®‰∫ÜÊ®°ÊÄÅËß£ËÄ¶ÁºñÁ†ÅÁ≠ñÁï•ÔºåÂàÜÂà´ÁºñÁ†ÅRGBÂ§ñËßÇÂíåÂùêÊ†á‰ø°ÊÅØÔºåÈÅøÂÖç‰∫Ü‰∏çÂêåÊ®°ÊÄÅ‰ø°ÊÅØ‰πãÈó¥ÁöÑÂπ≤Êâ∞Ôºõ3) ‰ΩøÁî®Ëá™ÂõûÂΩíTransformerËß£Á†ÅÂô®ÔºåËÉΩÂ§üÊçïÊçâÂÖ®Â±Ä‰ø°ÊÅØÂíåÂª∫Ê®°‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÊèêÈ´ò‰ΩçÂßø‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂùêÊ†áÂõætokenÂåñÔºöÂ∞Ü3DÁ©∫Èó¥ÂàíÂàÜ‰∏∫Â§ö‰∏™‰ΩìÁ¥†ÔºåÊØè‰∏™‰ΩìÁ¥†ÂØπÂ∫î‰∏Ä‰∏™token„ÄÇÊ®°ÊÄÅËß£ËÄ¶ÁºñÁ†ÅÂô®Ôºö‰ΩøÁî®‰∏§‰∏™Áã¨Á´ãÁöÑÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂàÜÂà´ÊèêÂèñRGBÂ§ñËßÇÁâπÂæÅÂíåÂùêÊ†á‰ø°ÊÅØ„ÄÇËá™ÂõûÂΩíTransformerËß£Á†ÅÂô®Ôºö‰ΩøÁî®Ê†áÂáÜÁöÑTransformerÁªìÊûÑÔºåÂπ∂ÂºïÂÖ•‰ΩçÁΩÆÁºñÁ†ÅÊù•Ë°®Á§∫tokenÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇÊçüÂ§±ÂáΩÊï∞Ôºö‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉËá™ÂõûÂΩíTransformerËß£Á†ÅÂô®„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CoordARÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®MOPEDÊï∞ÊçÆÈõÜ‰∏äÔºåCoordARÁöÑ‰ΩçÂßø‰º∞ËÆ°Á≤æÂ∫¶ÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜX%„ÄÇÊ≠§Â§ñÔºåCoordARÂú®Â§ÑÁêÜÂØπÁß∞Áâ©‰ΩìÂíåÈÅÆÊå°Âú∫ÊôØÊó∂Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇÁúüÂÆû‰∏ñÁïåÊµãËØï‰πüÈ™åËØÅ‰∫ÜCoordARÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoordARÂú®Êú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫ÊäìÂèñ‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•Âà©Áî®CoordAR‰º∞ËÆ°Áâ©‰ΩìÁöÑ6D‰ΩçÂßøÔºå‰ªéËÄåÂºïÂØºÊú∫Âô®‰∫∫ÂáÜÁ°ÆÊäìÂèñÁâ©‰Ωì„ÄÇÂú®Â¢ûÂº∫Áé∞ÂÆûÂ∫îÁî®‰∏≠ÔºåÂèØ‰ª•Â∞ÜËôöÊãüÁâ©‰Ωì‰∏éÁúüÂÆûÂú∫ÊôØËøõË°åÁ≤æÁ°ÆÂØπÈΩêÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇËØ•Á†îÁ©∂ËøòÊúâÂä©‰∫éÊé®Âä®Êó†Ê®°ÂûãÁâ©‰Ωì‰ΩçÂßø‰º∞ËÆ°ÊäÄÊúØÁöÑÂèëÂ±ïÔºåÈôç‰ΩéÂØπ3DÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

