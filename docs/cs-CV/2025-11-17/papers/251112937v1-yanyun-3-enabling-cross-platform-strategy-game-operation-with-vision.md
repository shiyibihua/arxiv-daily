---
layout: default
title: Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models
---

# Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models

**arXiv**: [2511.12937v1](https://arxiv.org/abs/2511.12937) | [PDF](https://arxiv.org/pdf/2511.12937.pdf)

**ä½œè€…**: Guoyan Wang, Yanyan Huang, Chunlin Chen, Lifeng Wang, Yuxiang Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYanyun-3æ¡†æž¶ï¼Œå®žçŽ°è·¨å¹³å°ç­–ç•¥æ¸¸æˆçš„è‡ªä¸»æ“ä½œã€‚**

**å…³é”®è¯**: `è·¨å¹³å°æ¸¸æˆè‡ªåŠ¨åŒ–` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€æ•°æ®èžåˆ` `å®žæ—¶æ“ä½œ` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè·¨å¹³å°ç­–ç•¥æ¸¸æˆè‡ªåŠ¨åŒ–éœ€æ³›åŒ–å¤„ç†å¤šæ ·ç•Œé¢å’ŒåŠ¨æ€æˆ˜åœºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆQwen2.5-VLè§†è§‰è¯­è¨€æŽ¨ç†ä¸ŽUI-TARSç²¾ç¡®æ‰§è¡Œèƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šMV+Sæ··åˆç­–ç•¥æå‡BLEU-4åˆ†æ•°çº¦12.98å€ï¼Œå‡å°‘æŽ¨ç†æ—¶é—´63%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

