---
layout: default
title: ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation
---

# ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation

**arXiv**: [2511.12893v1](https://arxiv.org/abs/2511.12893) | [PDF](https://arxiv.org/pdf/2511.12893.pdf)

**ä½œè€…**: Kaixin Zhang, Ruiqing Yang, Yuan Zhang, Shan You, Tao Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºActVARæ¡†æž¶ä»¥è§£å†³è§†è§‰è‡ªå›žå½’æ¨¡åž‹è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è‡ªå›žå½’ç”Ÿæˆ` `åŠ¨æ€æ¿€æ´»` `çŸ¥è¯†è’¸é¦` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–` `ä»¤ç‰Œé€‰æ‹©` `ä¸“å®¶ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è‡ªå›žå½’æ¨¡åž‹åºåˆ—å¢žé•¿å¯¼è‡´è®¡ç®—æˆæœ¬å‰§å¢žï¼Œé™æ€å‰ªæžç ´åé¢„è®­ç»ƒä¾èµ–
2. åŠ¨æ€æ¿€æ´»æƒé‡å’Œä»¤ç‰Œï¼Œåˆ†è§£FFNä¸ºä¸“å®¶å­ç½‘ï¼Œè·¯ç”±é€‰æ‹©ä¸“å®¶ï¼Œé—¨æŽ§é€‰æ‹©ä»¤ç‰Œ
3. ImageNet 256Ã—256åŸºå‡†ä¸Šï¼ŒFLOPså‡å°‘21.2%ï¼Œæ€§èƒ½æŸå¤±æœ€å°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

