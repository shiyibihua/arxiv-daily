---
layout: default
title: DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation
---

# DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation

**arXiv**: [2511.13047v1](https://arxiv.org/abs/2511.13047) | [PDF](https://arxiv.org/pdf/2511.13047.pdf)

**ä½œè€…**: Yan Gong, Jianli Lu, Yongsheng Gao, Jie Zhao, Xiaojuan Zhang, Susanto Rahardja

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

**å¤‡æ³¨**: 11 pages, 5 figures, 5 tables

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/gongyan1/DiffPixelFormer)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiffPixelFormerï¼Œç”¨äºŽæå‡RGB-Då®¤å†…åœºæ™¯åˆ†å‰²çš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `RGB-Dåœºæ™¯åˆ†å‰²` `Transformer` `è·¨æ¨¡æ€èžåˆ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `å®¤å†…åœºæ™¯ç†è§£` `è¯­ä¹‰åˆ†å‰²` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰RGB-Då®¤å†…åœºæ™¯åˆ†å‰²æ–¹æ³•ä¾èµ–è®¡ç®—é‡å¤§çš„äº¤å‰æ³¨æ„åŠ›ï¼Œä¸”æ¨¡æ€å†…å’Œæ¨¡æ€é—´ç‰¹å¾å…³ç³»å»ºæ¨¡ä¸è¶³ã€‚
2. DiffPixelFormeré€šè¿‡å·®åˆ†åƒç´ æ„ŸçŸ¥Transformerï¼Œå¢žå¼ºæ¨¡æ€å†…è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨DSIMæ¨¡å—è§£è€¦æ¨¡æ€ç‰¹å®šå’Œå…±äº«çº¿ç´¢ã€‚
3. åœ¨SUN RGB-Då’ŒNYUDv2æ•°æ®é›†ä¸Šï¼ŒDiffPixelFormer-Lçš„mIoUåˆ†åˆ«æå‡äº†1.78%å’Œ2.75%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®¤å†…è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººå­¦çš„åŸºçŸ³ï¼Œæ”¯æŒè‡ªä¸»å¯¼èˆªã€å¢žå¼ºçŽ°å®žå’Œæ™ºèƒ½çŽ¯å¢ƒç­‰åº”ç”¨ã€‚å°½ç®¡RGB-Dèžåˆåˆ©ç”¨äº†äº’è¡¥çš„å¤–è§‚å’Œå‡ ä½•çº¿ç´¢ï¼Œä½†çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽè®¡ç®—å¯†é›†åž‹çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä¸”å¯¹æ¨¡æ€å†…å’Œæ¨¡æ€é—´ç‰¹å¾å…³ç³»çš„å»ºæ¨¡ä¸è¶³ï¼Œå¯¼è‡´ç‰¹å¾å¯¹é½ä¸ç²¾ç¡®å’Œåˆ¤åˆ«è¡¨ç¤ºæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å·®åˆ†åƒç´ æ„ŸçŸ¥Transformerï¼Œå³DiffPixelFormerï¼Œç”¨äºŽRGB-Då®¤å†…åœºæ™¯åˆ†å‰²ï¼Œå®ƒåŒæ—¶å¢žå¼ºæ¨¡æ€å†…è¡¨ç¤ºå¹¶å»ºæ¨¡æ¨¡æ€é—´äº¤äº’ã€‚å…¶æ ¸å¿ƒæ˜¯æ¨¡æ€å†…-æ¨¡æ€é—´äº¤äº’å—ï¼ˆIIMIBï¼‰ï¼Œå®ƒé€šè¿‡è‡ªæ³¨æ„åŠ›æ•èŽ·æ¨¡æ€å†…é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨å·®åˆ†-å…±äº«æ¨¡æ€é—´ï¼ˆDSIMï¼‰æ¨¡å—å»ºæ¨¡æ¨¡æ€é—´äº¤äº’ï¼Œä»¥è§£è€¦æ¨¡æ€ç‰¹å®šå’Œå…±äº«çº¿ç´¢ï¼Œä»Žè€Œå®žçŽ°ç»†ç²’åº¦çš„åƒç´ çº§è·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€èžåˆç­–ç•¥å¹³è¡¡äº†æ¨¡æ€è´¡çŒ®ï¼Œå¹¶æ ¹æ®åœºæ™¯ç‰¹å¾å……åˆ†åˆ©ç”¨RGB-Dä¿¡æ¯ã€‚åœ¨SUN RGB-Då’ŒNYUDv2åŸºå‡†ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒDiffPixelFormer-Lå®žçŽ°äº†54.28%å’Œ59.95%çš„mIoUåˆ†æ•°ï¼Œåˆ†åˆ«ä¼˜äºŽDFormer-L 1.78%å’Œ2.75%ã€‚ä»£ç å·²åœ¨https://github.com/gongyan1/DiffPixelFormerä¸Šæä¾›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰RGB-Då®¤å†…åœºæ™¯åˆ†å‰²æ–¹æ³•åœ¨èžåˆRGBå’Œæ·±åº¦ä¿¡æ¯æ—¶ï¼Œé€šå¸¸é‡‡ç”¨è®¡ç®—å¤æ‚åº¦é«˜çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä¸”å¯¹æ¨¡æ€å†…éƒ¨å’Œæ¨¡æ€ä¹‹é—´çš„ç‰¹å¾å…³ç³»å»ºæ¨¡ä¸å¤Ÿå……åˆ†ï¼Œå¯¼è‡´ç‰¹å¾å¯¹é½ä¸ç²¾ç¡®ï¼Œæœ€ç»ˆé™åˆ¶äº†åˆ†å‰²çš„ç²¾åº¦ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†æ¨¡æ€ç‰¹å®šä¿¡æ¯å’Œæ¨¡æ€å…±äº«ä¿¡æ¯ï¼Œä»Žè€Œå½±å“äº†æœ€ç»ˆçš„åˆ†å‰²æ•ˆæžœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiffPixelFormerçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å·®åˆ†åƒç´ æ„ŸçŸ¥Transformerï¼ŒåŒæ—¶å¢žå¼ºæ¨¡æ€å†…è¡¨ç¤ºå’Œå»ºæ¨¡æ¨¡æ€é—´äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•èŽ·æ¨¡æ€å†…çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶è®¾è®¡äº†å·®åˆ†-å…±äº«æ¨¡æ€é—´ï¼ˆDSIMï¼‰æ¨¡å—æ¥è§£è€¦æ¨¡æ€ç‰¹å®šå’Œå…±äº«çš„çº¿ç´¢ï¼Œä»Žè€Œå®žçŽ°ç»†ç²’åº¦çš„åƒç´ çº§è·¨æ¨¡æ€å¯¹é½ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ›´æœ‰æ•ˆåœ°åˆ©ç”¨RGB-Dä¿¡æ¯ï¼Œæå‡åˆ†å‰²ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDiffPixelFormerçš„æ•´ä½“æž¶æž„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ†åˆ«å¯¹RGBå’Œæ·±åº¦å›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚ç„¶åŽï¼Œé€šè¿‡æå‡ºçš„æ¨¡æ€å†…-æ¨¡æ€é—´äº¤äº’å—ï¼ˆIIMIBï¼‰è¿›è¡Œç‰¹å¾èžåˆï¼Œè¯¥æ¨¡å—åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒDSIMæ¨¡å—ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶ç”¨äºŽæ•èŽ·æ¨¡æ€å†…çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼ŒDSIMæ¨¡å—ç”¨äºŽè§£è€¦æ¨¡æ€ç‰¹å®šå’Œå…±äº«çš„çº¿ç´¢ã€‚æœ€åŽï¼Œé€šè¿‡ä¸€ä¸ªåŠ¨æ€èžåˆç­–ç•¥æ¥å¹³è¡¡æ¨¡æ€è´¡çŒ®ï¼Œå¹¶è¿›è¡Œåƒç´ çº§åˆ«çš„è¯­ä¹‰åˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiffPixelFormerçš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†å·®åˆ†-å…±äº«æ¨¡æ€é—´ï¼ˆDSIMï¼‰æ¨¡å—ã€‚ä¸Žä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼ŒDSIMæ¨¡å—èƒ½å¤Ÿæ˜¾å¼åœ°è§£è€¦æ¨¡æ€ç‰¹å®šå’Œå…±äº«çš„ç‰¹å¾ï¼Œä»Žè€Œå®žçŽ°æ›´ç²¾ç»†çš„è·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€èžåˆç­–ç•¥èƒ½å¤Ÿæ ¹æ®åœºæ™¯ç‰¹å¾è‡ªé€‚åº”åœ°è°ƒæ•´RGBå’Œæ·±åº¦ä¿¡æ¯çš„æƒé‡ï¼Œè¿›ä¸€æ­¥æå‡åˆ†å‰²æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šDSIMæ¨¡å—çš„è®¾è®¡æ˜¯å…³é”®ã€‚å®ƒé€šè¿‡å·®åˆ†å­¦ä¹ çš„æ–¹å¼ï¼Œå°†æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾åˆ†è§£ä¸ºå…±äº«éƒ¨åˆ†å’Œç‰¹å®šéƒ¨åˆ†ã€‚å…±äº«éƒ¨åˆ†ç”¨äºŽè¡¨ç¤ºä¸¤ä¸ªæ¨¡æ€å…±æœ‰çš„ä¿¡æ¯ï¼Œè€Œç‰¹å®šéƒ¨åˆ†åˆ™ç”¨äºŽè¡¨ç¤ºæ¯ä¸ªæ¨¡æ€ç‹¬æœ‰çš„ä¿¡æ¯ã€‚è¿™ç§åˆ†è§£æ–¹å¼æœ‰åŠ©äºŽæ¨¡åž‹æ›´å¥½åœ°ç†è§£RGB-Dæ•°æ®ï¼Œå¹¶æå‡åˆ†å‰²ç²¾åº¦ã€‚åŠ¨æ€èžåˆç­–ç•¥ä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„æƒé‡æ¥å¹³è¡¡RGBå’Œæ·±åº¦ä¿¡æ¯çš„è´¡çŒ®ï¼Œè¯¥æƒé‡æ ¹æ®è¾“å…¥å›¾åƒçš„ç‰¹å¾åŠ¨æ€è°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

DiffPixelFormeråœ¨SUN RGB-Då’ŒNYUDv2æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨SUN RGB-Dæ•°æ®é›†ä¸Šï¼ŒDiffPixelFormer-Lè¾¾åˆ°äº†54.28%çš„mIoUï¼Œç›¸æ¯”DFormer-Læå‡äº†1.78%ã€‚åœ¨NYUDv2æ•°æ®é›†ä¸Šï¼ŒDiffPixelFormer-Lè¾¾åˆ°äº†59.95%çš„mIoUï¼Œç›¸æ¯”DFormer-Læå‡äº†2.75%ã€‚è¿™äº›ç»“æžœè¡¨æ˜ŽDiffPixelFormeråœ¨RGB-Då®¤å†…åœºæ™¯åˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

DiffPixelFormeråœ¨å®¤å†…åœºæ™¯ç†è§£æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœåŠ¡æœºå™¨äººã€å¢žå¼ºçŽ°å®žã€æ™ºèƒ½å®¶å±…ç­‰ã€‚é«˜ç²¾åº¦çš„å®¤å†…åœºæ™¯åˆ†å‰²å¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´çŽ¯å¢ƒï¼Œä»Žè€Œå®žçŽ°æ›´æ™ºèƒ½çš„å¯¼èˆªå’Œäº¤äº’ã€‚åœ¨å¢žå¼ºçŽ°å®žåº”ç”¨ä¸­ï¼Œå¯ä»¥æä¾›æ›´é€¼çœŸçš„åœºæ™¯æ¸²æŸ“å’Œå¯¹è±¡äº¤äº’ã€‚åœ¨æ™ºèƒ½å®¶å±…é¢†åŸŸï¼Œå¯ä»¥å®žçŽ°æ›´ç²¾ç»†çš„çŽ¯å¢ƒæ„ŸçŸ¥å’ŒæŽ§åˆ¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

