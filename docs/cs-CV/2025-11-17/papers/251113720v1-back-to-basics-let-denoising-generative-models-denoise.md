---
layout: default
title: Back to Basics: Let Denoising Generative Models Denoise
---

# Back to Basics: Let Denoising Generative Models Denoise

**arXiv**: [2511.13720v1](https://arxiv.org/abs/2511.13720) | [PDF](https://arxiv.org/pdf/2511.13720.pdf)

**ä½œè€…**: Tianhong Li, Kaiming He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJiTæ–¹æ³•ï¼Œé€šè¿‡ç›´æŽ¥é¢„æµ‹å¹²å‡€å›¾åƒè§£å†³æ‰©æ•£æ¨¡åž‹åœ¨é«˜ç»´ç©ºé—´å¤±æ•ˆé—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `æµå½¢å‡è®¾` `Transformerç”Ÿæˆ` `å›¾åƒåŽ»å™ª` `é«˜ç»´æ•°æ®ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹é¢„æµ‹å™ªå£°è€Œéžå¹²å‡€å›¾åƒï¼Œè¿èƒŒæµå½¢å‡è®¾ï¼Œå¯¼è‡´é«˜ç»´ç”Ÿæˆå¤±è´¥
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤§å—Transformerç›´æŽ¥é¢„æµ‹å¹²å‡€æ•°æ®ï¼Œæ— éœ€åˆ†è¯å™¨ã€é¢„è®­ç»ƒæˆ–é¢å¤–æŸå¤±
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet 256å’Œ512åˆ†è¾¨çŽ‡ä¸Šï¼ŒJiTå–å¾—ç«žäº‰æ€§ç»“æžœï¼ŒéªŒè¯æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

