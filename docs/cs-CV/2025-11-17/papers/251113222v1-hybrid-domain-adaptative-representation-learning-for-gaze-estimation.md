---
layout: default
title: Hybrid-Domain Adaptative Representation Learning for Gaze Estimation
---

# Hybrid-Domain Adaptative Representation Learning for Gaze Estimation

**arXiv**: [2511.13222v1](https://arxiv.org/abs/2511.13222) | [PDF](https://arxiv.org/pdf/2511.13222.pdf)

**ä½œè€…**: Qida Tan, Hongyu Yang, Wenchao Du

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆåŸŸè‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ æ¡†æž¶ä»¥æå‡è·¨åŸŸè§†çº¿ä¼°è®¡é²æ£’æ€§**

**å…³é”®è¯**: `è§†çº¿ä¼°è®¡` `åŸŸé€‚åº”` `è¡¨ç¤ºå­¦ä¹ ` `å¤´éƒ¨å§¿æ€èžåˆ` `è·¨åŸŸè¯„ä¼°` `æ— ç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè·¨åŸŸè§†çº¿ä¼°è®¡å—è¡¨æƒ…ã€ä½©æˆ´ç‰©ç­‰æ— å…³å› ç´ å¹²æ‰°ï¼Œæ€§èƒ½ä¸‹é™æ˜¾è‘—ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ— ç›‘ç£åŸŸé€‚åº”å¯¹é½é«˜ä½Žè´¨é‡å›¾åƒç‰¹å¾ï¼Œå¹¶èžåˆå¤´éƒ¨å§¿æ€å‡ ä½•çº¦æŸã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°SOTAç²¾åº¦ï¼Œå¦‚EyeDiap 5.02åº¦ï¼Œè·¨åŸŸè¯„ä¼°è¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

