---
layout: default
title: Hybrid-Domain Adaptative Representation Learning for Gaze Estimation
---

# Hybrid-Domain Adaptative Representation Learning for Gaze Estimation

**arXiv**: [2511.13222v1](https://arxiv.org/abs/2511.13222) | [PDF](https://arxiv.org/pdf/2511.13222.pdf)

**ä½œè€…**: Qida Tan, Hongyu Yang, Wenchao Du

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

**å¤‡æ³¨**: AAAI2026

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/da60266/HARL)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆé¢†åŸŸè‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ ä»¥è§£å†³æ³¨è§†ä¼°è®¡ä¸­çš„è·¨åŸŸé—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æ³¨è§†ä¼°è®¡` `é¢†åŸŸé€‚åº”` `è¡¨ç¤ºå­¦ä¹ ` `å›¾åƒå¤„ç†` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„æ³¨è§†ä¼°è®¡æ–¹æ³•åœ¨è·¨åŸŸè¯„ä¼°ä¸­è¡¨çŽ°ä¸ä½³ï¼Œå—åˆ°è¡¨æƒ…ã€ä½©æˆ´ç‰©å’Œå›¾åƒè´¨é‡ç­‰å› ç´ çš„å¹²æ‰°ã€‚
2. æœ¬æ–‡æå‡ºçš„HARLæ¡†æž¶é€šè¿‡æ— ç›‘ç£é¢†åŸŸé€‚åº”ï¼Œä»Žä½Žè´¨é‡é¢éƒ¨å›¾åƒä¸­æå–æ³¨è§†ç›¸å…³è¡¨ç¤ºï¼Œå‡å°‘äº†è®¡ç®—å’ŒæŽ¨ç†æˆæœ¬ã€‚
3. åœ¨EyeDiapã€MPIIFaceGazeå’ŒGaze360æ•°æ®é›†ä¸Šçš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«è¾¾åˆ°äº†5.02Â°ã€3.36Â°å’Œ9.26Â°çš„å‡†ç¡®æ€§ï¼Œè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽå¤–è§‚çš„æ³¨è§†ä¼°è®¡æ—¨åœ¨ä»Žå•å¼ é¢éƒ¨å›¾åƒä¸­é¢„æµ‹å‡†ç¡®çš„3Dæ³¨è§†æ–¹å‘ï¼Œè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨è·¨åŸŸè¯„ä¼°ä¸­è¡¨çŽ°ä¸ä½³ï¼Œå—åˆ°ä¸Žæ³¨è§†æ— å…³å› ç´ çš„å¹²æ‰°ï¼Œå¦‚è¡¨æƒ…ã€ä½©æˆ´ç‰©å’Œå›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆé¢†åŸŸè‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ æ¡†æž¶ï¼ˆHARLï¼‰ï¼Œåˆ©ç”¨å¤šæºæ··åˆæ•°æ®é›†å­¦ä¹ ç¨³å¥çš„æ³¨è§†è¡¨ç¤ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹å¼å¯¹é½ä»Žé«˜è´¨é‡è¿‘çœ¼å›¾åƒæå–çš„ç‰¹å¾ï¼Œä»¥ä»Žä½Žè´¨é‡é¢éƒ¨å›¾åƒä¸­è§£è€¦æ³¨è§†ç›¸å…³è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æžäº†å¤´éƒ¨å§¿æ€çš„å½±å“ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„ç¨€ç–å›¾èžåˆæ¨¡å—ï¼Œä»¥æŽ¢ç´¢æ³¨è§†æ–¹å‘ä¸Žå¤´éƒ¨å§¿æ€ä¹‹é—´çš„å‡ ä½•çº¦æŸï¼Œä»Žè€ŒèŽ·å¾—å¯†é›†ä¸”ç¨³å¥çš„æ³¨è§†è¡¨ç¤ºã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºŽå¤–è§‚çš„æ³¨è§†ä¼°è®¡åœ¨è·¨åŸŸè¯„ä¼°ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä¸åŒçŽ¯å¢ƒå’Œæ¡ä»¶æ—¶ï¼Œå®¹æ˜“å—åˆ°ä¸Žæ³¨è§†æ— å…³å› ç´ çš„å¹²æ‰°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„HARLæ¡†æž¶é€šè¿‡åˆ©ç”¨å¤šæºæ··åˆæ•°æ®é›†ï¼Œé‡‡ç”¨æ— ç›‘ç£é¢†åŸŸé€‚åº”çš„æ–¹æ³•ï¼Œä»Žä½Žè´¨é‡å›¾åƒä¸­æå–å‡ºæ³¨è§†ç›¸å…³çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ—¨åœ¨å¢žå¼ºæ¨¡åž‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šHARLæ¡†æž¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šç‰¹å¾å¯¹é½æ¨¡å—å’Œç¨€ç–å›¾èžåˆæ¨¡å—ã€‚ç‰¹å¾å¯¹é½æ¨¡å—è´Ÿè´£ä»Žé«˜è´¨é‡å›¾åƒä¸­æå–ç‰¹å¾å¹¶ä¸Žä½Žè´¨é‡å›¾åƒè¿›è¡Œå¯¹é½ï¼Œè€Œç¨€ç–å›¾èžåˆæ¨¡å—åˆ™ç”¨äºŽæŽ¢ç´¢æ³¨è§†æ–¹å‘ä¸Žå¤´éƒ¨å§¿æ€ä¹‹é—´çš„å‡ ä½•å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºŽé€šè¿‡æ— ç›‘ç£æ–¹å¼å®žçŽ°ä½Žè´¨é‡å›¾åƒä¸Žé«˜è´¨é‡å›¾åƒç‰¹å¾çš„å¯¹é½ï¼Œæ˜¾è‘—é™ä½Žäº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶å¼•å…¥ç¨€ç–å›¾èžåˆæ¨¡å—æ¥å¢žå¼ºæ³¨è§†è¡¨ç¤ºçš„å‡ ä½•çº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šæˆ‘ä»¬åœ¨æ¨¡åž‹ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰¹å¾å¯¹é½è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†ç¨€ç–å›¾èžåˆçš„ç»“æž„ï¼Œä»¥æœ‰æ•ˆæ•æ‰æ³¨è§†æ–¹å‘ä¸Žå¤´éƒ¨å§¿æ€ä¹‹é—´çš„å…³ç³»ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒHARLæ¡†æž¶åœ¨EyeDiapã€MPIIFaceGazeå’ŒGaze360æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†5.02Â°ã€3.36Â°å’Œ9.26Â°çš„æ³¨è§†ä¼°è®¡å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­çš„ç«žäº‰åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€è™šæ‹ŸçŽ°å®žå’Œå¢žå¼ºçŽ°å®žç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿå¯¹ç”¨æˆ·æ³¨è§†è¡Œä¸ºçš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒHARLæ¡†æž¶æœ‰æœ›åœ¨æ™ºèƒ½è®¾å¤‡å’Œè¾…åŠ©æŠ€æœ¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼ŒæŽ¨åŠ¨æ³¨è§†ä¼°è®¡æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

