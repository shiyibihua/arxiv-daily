---
layout: default
title: Hybrid-Domain Adaptative Representation Learning for Gaze Estimation
---

# Hybrid-Domain Adaptative Representation Learning for Gaze Estimation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.13222" target="_blank" class="toolbar-btn">arXiv: 2511.13222v1</a>
    <a href="https://arxiv.org/pdf/2511.13222.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13222v1" 
            onclick="toggleFavorite(this, '2511.13222v1', 'Hybrid-Domain Adaptative Representation Learning for Gaze Estimation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Qida Tan, Hongyu Yang, Wenchao Du

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-17

**Â§áÊ≥®**: AAAI2026

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/da60266/HARL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê∑∑ÂêàÈ¢ÜÂüüËá™ÈÄÇÂ∫îË°®Á§∫Â≠¶‰π†‰ª•Ëß£ÂÜ≥Ê≥®ËßÜ‰º∞ËÆ°‰∏≠ÁöÑË∑®ÂüüÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê≥®ËßÜ‰º∞ËÆ°` `È¢ÜÂüüÈÄÇÂ∫î` `Ë°®Á§∫Â≠¶‰π†` `ÂõæÂÉèÂ§ÑÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊ≥®ËßÜ‰º∞ËÆ°ÊñπÊ≥ïÂú®Ë∑®ÂüüËØÑ‰º∞‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂèóÂà∞Ë°®ÊÉÖ„ÄÅ‰Ω©Êà¥Áâ©ÂíåÂõæÂÉèË¥®ÈáèÁ≠âÂõ†Á¥†ÁöÑÂπ≤Êâ∞„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑHARLÊ°ÜÊû∂ÈÄöËøáÊó†ÁõëÁù£È¢ÜÂüüÈÄÇÂ∫îÔºå‰ªé‰ΩéË¥®ÈáèÈù¢ÈÉ®ÂõæÂÉè‰∏≠ÊèêÂèñÊ≥®ËßÜÁõ∏ÂÖ≥Ë°®Á§∫ÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂíåÊé®ÁêÜÊàêÊú¨„ÄÇ
3. Âú®EyeDiap„ÄÅMPIIFaceGazeÂíåGaze360Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂàÜÂà´ËææÂà∞‰∫Ü5.02¬∞„ÄÅ3.36¬∞Âíå9.26¬∞ÁöÑÂáÜÁ°ÆÊÄßÔºåË°®Áé∞‰ºòÂºÇ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫‰∫éÂ§ñËßÇÁöÑÊ≥®ËßÜ‰º∞ËÆ°Êó®Âú®‰ªéÂçïÂº†Èù¢ÈÉ®ÂõæÂÉè‰∏≠È¢ÑÊµãÂáÜÁ°ÆÁöÑ3DÊ≥®ËßÜÊñπÂêëÔºåËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∞ÊñπÊ≥ïÂú®Ë∑®ÂüüËØÑ‰º∞‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂèóÂà∞‰∏éÊ≥®ËßÜÊó†ÂÖ≥Âõ†Á¥†ÁöÑÂπ≤Êâ∞ÔºåÂ¶ÇË°®ÊÉÖ„ÄÅ‰Ω©Êà¥Áâ©ÂíåÂõæÂÉèË¥®Èáè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑∑ÂêàÈ¢ÜÂüüËá™ÈÄÇÂ∫îË°®Á§∫Â≠¶‰π†Ê°ÜÊû∂ÔºàHARLÔºâÔºåÂà©Áî®Â§öÊ∫êÊ∑∑ÂêàÊï∞ÊçÆÈõÜÂ≠¶‰π†Á®≥ÂÅ•ÁöÑÊ≥®ËßÜË°®Á§∫„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÈÄöËøáÊó†ÁõëÁù£È¢ÜÂüüÈÄÇÂ∫îÊñπÂºèÂØπÈΩê‰ªéÈ´òË¥®ÈáèËøëÁúºÂõæÂÉèÊèêÂèñÁöÑÁâπÂæÅÔºå‰ª•‰ªé‰ΩéË¥®ÈáèÈù¢ÈÉ®ÂõæÂÉè‰∏≠Ëß£ËÄ¶Ê≥®ËßÜÁõ∏ÂÖ≥Ë°®Á§∫„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜÂ§¥ÈÉ®ÂßøÊÄÅÁöÑÂΩ±ÂìçÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïËÄåÈ´òÊïàÁöÑÁ®ÄÁñèÂõæËûçÂêàÊ®°ÂùóÔºå‰ª•Êé¢Á¥¢Ê≥®ËßÜÊñπÂêë‰∏éÂ§¥ÈÉ®ÂßøÊÄÅ‰πãÈó¥ÁöÑÂá†‰ΩïÁ∫¶ÊùüÔºå‰ªéËÄåËé∑ÂæóÂØÜÈõÜ‰∏îÁ®≥ÂÅ•ÁöÑÊ≥®ËßÜË°®Á§∫„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âü∫‰∫éÂ§ñËßÇÁöÑÊ≥®ËßÜ‰º∞ËÆ°Âú®Ë∑®ÂüüËØÑ‰º∞‰∏≠ÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Èù¢ÂØπ‰∏çÂêåÁéØÂ¢ÉÂíåÊù°‰ª∂Êó∂ÔºåÂÆπÊòìÂèóÂà∞‰∏éÊ≥®ËßÜÊó†ÂÖ≥Âõ†Á¥†ÁöÑÂπ≤Êâ∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫ÁöÑHARLÊ°ÜÊû∂ÈÄöËøáÂà©Áî®Â§öÊ∫êÊ∑∑ÂêàÊï∞ÊçÆÈõÜÔºåÈááÁî®Êó†ÁõëÁù£È¢ÜÂüüÈÄÇÂ∫îÁöÑÊñπÊ≥ïÔºå‰ªé‰ΩéË¥®ÈáèÂõæÂÉè‰∏≠ÊèêÂèñÂá∫Ê≥®ËßÜÁõ∏ÂÖ≥ÁöÑÁâπÂæÅË°®Á§∫ÔºåÊó®Âú®Â¢ûÂº∫Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHARLÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰∏§‰∏™Ê®°ÂùóÔºöÁâπÂæÅÂØπÈΩêÊ®°ÂùóÂíåÁ®ÄÁñèÂõæËûçÂêàÊ®°Âùó„ÄÇÁâπÂæÅÂØπÈΩêÊ®°ÂùóË¥üË¥£‰ªéÈ´òË¥®ÈáèÂõæÂÉè‰∏≠ÊèêÂèñÁâπÂæÅÂπ∂‰∏é‰ΩéË¥®ÈáèÂõæÂÉèËøõË°åÂØπÈΩêÔºåËÄåÁ®ÄÁñèÂõæËûçÂêàÊ®°ÂùóÂàôÁî®‰∫éÊé¢Á¥¢Ê≥®ËßÜÊñπÂêë‰∏éÂ§¥ÈÉ®ÂßøÊÄÅ‰πãÈó¥ÁöÑÂá†‰ΩïÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈÄöËøáÊó†ÁõëÁù£ÊñπÂºèÂÆûÁé∞‰ΩéË¥®ÈáèÂõæÂÉè‰∏éÈ´òË¥®ÈáèÂõæÂÉèÁâπÂæÅÁöÑÂØπÈΩêÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂØπÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñÔºåÂêåÊó∂ÂºïÂÖ•Á®ÄÁñèÂõæËûçÂêàÊ®°ÂùóÊù•Â¢ûÂº∫Ê≥®ËßÜË°®Á§∫ÁöÑÂá†‰ΩïÁ∫¶Êùü„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊàë‰ª¨Âú®Ê®°Âûã‰∏≠ÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÁâπÂæÅÂØπÈΩêËøáÁ®ãÔºåÂπ∂ËÆæËÆ°‰∫ÜÁ®ÄÁñèÂõæËûçÂêàÁöÑÁªìÊûÑÔºå‰ª•ÊúâÊïàÊçïÊçâÊ≥®ËßÜÊñπÂêë‰∏éÂ§¥ÈÉ®ÂßøÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHARLÊ°ÜÊû∂Âú®EyeDiap„ÄÅMPIIFaceGazeÂíåGaze360Êï∞ÊçÆÈõÜ‰∏äÂàÜÂà´ËææÂà∞‰∫Ü5.02¬∞„ÄÅ3.36¬∞Âíå9.26¬∞ÁöÑÊ≥®ËßÜ‰º∞ËÆ°ÂáÜÁ°ÆÊÄßÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂú®Ë∑®Êï∞ÊçÆÈõÜËØÑ‰º∞‰∏≠ÁöÑÁ´û‰∫âÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∫∫Êú∫‰∫§‰∫í„ÄÅËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÂú∫ÊôØÔºåËÉΩÂ§üÊèêÂçáÁ≥ªÁªüÂØπÁî®Êà∑Ê≥®ËßÜË°å‰∏∫ÁöÑÁêÜËß£ÂíåÂìçÂ∫îËÉΩÂäõ„ÄÇÈöèÁùÄÊäÄÊúØÁöÑËøõÊ≠•ÔºåHARLÊ°ÜÊû∂ÊúâÊúõÂú®Êô∫ËÉΩËÆæÂ§áÂíåËæÖÂä©ÊäÄÊúØ‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÊé®Âä®Ê≥®ËßÜ‰º∞ËÆ°ÊäÄÊúØÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

