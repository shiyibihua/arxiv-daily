---
layout: default
title: Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)
---

# Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)

**arXiv**: [2511.13397v1](https://arxiv.org/abs/2511.13397) | [PDF](https://arxiv.org/pdf/2511.13397.pdf)

**ä½œè€…**: Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·ç¦»æ ‡æ³¨äº¤é€šæ„ŸçŸ¥é—®ç­”åŸºå‡†DTPQAï¼Œç”¨äºŽè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `äº¤é€šåœºæ™¯æ„ŸçŸ¥` `è·ç¦»æ ‡æ³¨åŸºå‡†` `è‡ªåŠ¨é©¾é©¶è¯„ä¼°` `è§†è§‰é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹éœ€å…·å¤‡é²æ£’æ„ŸçŸ¥èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚äº¤é€šåœºæ™¯å’Œè¿œè·ç¦»å¯¹è±¡è¯†åˆ«ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåˆæˆå’ŒçœŸå®žä¸–ç•ŒåŸºå‡†ï¼ŒåŒ…å«å›¾åƒã€é—®é¢˜ã€ç­”æ¡ˆå’Œå¯¹è±¡è·ç¦»æ ‡æ³¨ï¼Œä»¥éš”ç¦»æ„ŸçŸ¥è¯„ä¼°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæä¾›æ•°æ®é›†å’ŒPythonè„šæœ¬ï¼Œæ”¯æŒåˆ†æžæ¨¡åž‹æ€§èƒ½éšå¯¹è±¡è·ç¦»å¢žåŠ è€Œä¸‹é™çš„æƒ…å†µã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

