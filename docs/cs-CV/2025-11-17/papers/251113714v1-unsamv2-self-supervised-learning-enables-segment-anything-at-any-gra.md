---
layout: default
title: UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity
---

# UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity

**arXiv**: [2511.13714v1](https://arxiv.org/abs/2511.13714) | [PDF](https://arxiv.org/pdf/2511.13714.pdf)

**ä½œè€…**: Junwei Yu, Trevor Darrell, XuDong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUnSAMv2ä»¥æ— ç›‘ç£æ–¹å¼å®žçŽ°ä»»æ„ç²’åº¦å›¾åƒåˆ†å‰²**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å›¾åƒåˆ†å‰²` `ç²’åº¦æŽ§åˆ¶` `æ— æ ‡æ³¨æ•°æ®` `è§†è§‰åŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. SAMæ¨¡åž‹éš¾ä»¥æŽ§åˆ¶åˆ†å‰²ç²’åº¦ï¼Œéœ€äººå·¥å¹²é¢„ï¼Œæ ‡æ³¨æˆæœ¬é«˜
2. å¼•å…¥è‡ªç›‘ç£å­¦ä¹ ï¼Œå‘çŽ°æŽ©ç -ç²’åº¦å¯¹ï¼Œä½¿ç”¨ç²’åº¦æŽ§åˆ¶åµŒå…¥
3. å°‘é‡æœªæ ‡æ³¨æ•°æ®æ˜¾è‘—æå‡SAM-2ï¼Œåœ¨11ä¸ªåŸºå‡†ä¸Šæ”¹è¿›æŒ‡æ ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

