---
layout: default
title: Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach
---

# Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach

**arXiv**: [2511.12978v1](https://arxiv.org/abs/2511.12978) | [PDF](https://arxiv.org/pdf/2511.12978.pdf)

**ä½œè€…**: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCCIæ–¹æ³•ä»¥è¯„ä¼°CLIPæ¨¡åž‹å¯¹è™šå‡ç›¸å…³æ€§çš„é²æ£’æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `è§£é‡Šæ€§æ–¹æ³•` `è™šå‡ç›¸å…³æ€§` `åŸºå‡†æµ‹è¯•` `èšç±»åˆ†æž` `é›¶æ ·æœ¬è¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPç­‰è§†è§‰è¯­è¨€æ¨¡åž‹æ˜“å—èƒŒæ™¯è™šå‡ç›¸å…³æ€§å½±å“ï¼Œå¯¼è‡´é¢„æµ‹åå·®
2. CCIåˆ©ç”¨è¡¥ä¸åµŒå…¥èšç±»å’ŒæŽ©ç ï¼Œè¯„ä¼°é¢„æµ‹å˜åŒ–ï¼Œæå‡è§£é‡Šæ€§
3. ç»“åˆCOVARåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°18ä¸ªCLIPå˜ä½“ï¼ŒæŽ¨åŠ¨æ¨¡åž‹é²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

