---
layout: default
title: Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts
---

# Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts

**arXiv**: [2511.13032v1](https://arxiv.org/abs/2511.13032) | [PDF](https://arxiv.org/pdf/2511.13032.pdf)

**ä½œè€…**: Sheng Liu, Yuanzhi Liang, Jiepeng Wang, Sidan Du, Chi Zhang, Xuelong Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni-Interæ¡†æž¶ä»¥è§£å†³å¤šç§äº¤äº’åœºæ™¯ä¸‹çš„äººç±»åŠ¨ä½œç”Ÿæˆé—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `äººç±»åŠ¨ä½œç”Ÿæˆ` `äº¤äº’å»ºæ¨¡` `ç»Ÿä¸€æ¡†æž¶` `ç©ºé—´ä¾èµ–æ€§` `è™šæ‹ŸçŽ°å®ž` `æœºå™¨äººæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽç‰¹å®šä»»åŠ¡çš„è®¾è®¡ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†å¤šæ ·åŒ–çš„äº¤äº’åœºæ™¯ã€‚
2. Uni-Interé€šè¿‡å¼•å…¥ç»Ÿä¸€äº¤äº’ä½“ç§¯ï¼ˆUIVï¼‰ï¼Œå®žçŽ°äº†å¯¹å¼‚æž„äº¤äº’å®žä½“çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œæ”¯æŒå¤šç§äº¤äº’ç±»åž‹ã€‚
3. å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒUni-Interåœ¨å¤šä¸ªäº¤äº’ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ–°ç»„åˆçš„å®žä½“ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Uni-Interï¼Œä¸€ä¸ªç»Ÿä¸€çš„äººç±»åŠ¨ä½œç”Ÿæˆæ¡†æž¶ï¼Œæ”¯æŒå¤šç§äº¤äº’åœºæ™¯ï¼ŒåŒ…æ‹¬äºº-äººã€äºº-ç‰©ä½“å’Œäºº-åœºæ™¯çš„äº¤äº’ã€‚ä¸ŽçŽ°æœ‰ä¾èµ–äºŽç‰¹å®šä»»åŠ¡è®¾è®¡ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™çš„æ–¹æ³•ä¸åŒï¼ŒUni-Interå¼•å…¥äº†ç»Ÿä¸€äº¤äº’ä½“ç§¯ï¼ˆUIVï¼‰ï¼Œè¯¥ä½“ç§¯è¡¨ç¤ºå°†å¼‚æž„äº¤äº’å®žä½“ç¼–ç ä¸ºå…±äº«ç©ºé—´åœºã€‚è¿™ä½¿å¾—ä¸€è‡´çš„å…³ç³»æŽ¨ç†å’Œå¤åˆäº¤äº’å»ºæ¨¡æˆä¸ºå¯èƒ½ã€‚åŠ¨ä½œç”Ÿæˆè¢«å½¢å¼åŒ–ä¸ºå¯¹UIVçš„å…³èŠ‚æ¦‚çŽ‡é¢„æµ‹ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ•æ‰ç»†ç²’åº¦çš„ç©ºé—´ä¾èµ–æ€§ï¼Œå¹¶äº§ç”Ÿè¿žè´¯çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡Œä¸ºã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒUni-Interåœ¨ä¸‰ä¸ªä»£è¡¨æ€§äº¤äº’ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºç«žäº‰åŠ›ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°å®žä½“ç»„åˆã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œå¤åˆäº¤äº’çš„ç»Ÿä¸€å»ºæ¨¡ä¸ºå¤æ‚çŽ¯å¢ƒä¸­çš„å¯æ‰©å±•åŠ¨ä½œåˆæˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³çŽ°æœ‰æ–¹æ³•åœ¨å¤šç§äº¤äº’åœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å…¶å¯¹ç‰¹å®šä»»åŠ¡çš„ä¾èµ–æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni-Interçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ç»Ÿä¸€äº¤äº’ä½“ç§¯ï¼ˆUIVï¼‰ï¼Œé€šè¿‡å°†ä¸åŒçš„äº¤äº’å®žä½“ç¼–ç åˆ°ä¸€ä¸ªå…±äº«ç©ºé—´ä¸­ï¼Œå®žçŽ°ä¸€è‡´çš„å…³ç³»æŽ¨ç†å’Œå¤åˆäº¤äº’å»ºæ¨¡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿåœ¨å¤šç§äº¤äº’åœºæ™¯ä¸­ä¿æŒä¸€è‡´æ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šUni-Interçš„æ•´ä½“æž¶æž„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç»Ÿä¸€äº¤äº’ä½“ç§¯çš„æž„å»ºã€å…³èŠ‚æ¦‚çŽ‡é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆç­‰ä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè¾“å…¥çš„äº¤äº’å®žä½“è¢«ç¼–ç ä¸ºUIVï¼Œç„¶åŽé€šè¿‡è”åˆæ¦‚çŽ‡é¢„æµ‹ç”ŸæˆåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºŽç»Ÿä¸€äº¤äº’ä½“ç§¯ï¼ˆUIVï¼‰çš„å¼•å…¥ï¼Œå®ƒå…è®¸æ¨¡åž‹åœ¨ä¸€ä¸ªå…±äº«çš„ç©ºé—´ä¸­å¤„ç†å¼‚æž„äº¤äº’å®žä½“ï¼Œä»Žè€Œå®žçŽ°æ›´å¥½çš„å…³ç³»æŽ¨ç†å’ŒåŠ¨ä½œç”Ÿæˆã€‚è¿™ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ç‰¹å®šä»»åŠ¡è®¾è®¡å½¢æˆäº†é²œæ˜Žå¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡åž‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å…³èŠ‚é—´çš„ç©ºé—´ä¾èµ–æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æž„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æ”¯æŒå¯¹å¤æ‚äº¤äº’çš„å»ºæ¨¡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒUni-Interåœ¨ä¸‰ä¸ªä»£è¡¨æ€§äº¤äº’ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ–°å®žä½“ç»„åˆçš„æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨çŽ°çªå‡ºã€‚ä¸ŽåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒUni-Interåœ¨åŠ¨ä½œç”Ÿæˆçš„è¿žè´¯æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªè¯¦è¿°ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹ŸçŽ°å®žã€æ¸¸æˆå¼€å‘å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªç»Ÿä¸€çš„åŠ¨ä½œç”Ÿæˆæ¡†æž¶ï¼ŒUni-Interèƒ½å¤Ÿåœ¨å¤šç§å¤æ‚çŽ¯å¢ƒä¸­å®žçŽ°è‡ªç„¶çš„äººç±»åŠ¨ä½œåˆæˆï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨æœºå™¨äººæŽ§åˆ¶å’ŒåŠ¨ç”»ç”Ÿæˆç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

