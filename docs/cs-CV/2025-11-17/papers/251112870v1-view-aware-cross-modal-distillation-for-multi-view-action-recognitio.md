---
layout: default
title: View-aware Cross-modal Distillation for Multi-view Action Recognition
---

# View-aware Cross-modal Distillation for Multi-view Action Recognition

**arXiv**: [2511.12870v1](https://arxiv.org/abs/2511.12870) | [PDF](https://arxiv.org/pdf/2511.12870.pdf)

**ä½œè€…**: Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†å›¾æ„ŸçŸ¥è·¨æ¨¡æ€è’¸é¦ä»¥è§£å†³éƒ¨åˆ†é‡å å¤šè§†å›¾åŠ¨ä½œè¯†åˆ«é—®é¢˜**

**å…³é”®è¯**: `å¤šè§†å›¾åŠ¨ä½œè¯†åˆ«` `è·¨æ¨¡æ€è’¸é¦` `è§†å›¾æ„ŸçŸ¥ä¸€è‡´æ€§` `éƒ¨åˆ†é‡å è§†å›¾` `çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šéƒ¨åˆ†é‡å å¤šè§†å›¾åœºæ™¯ä¸­åŠ¨ä½œä»…éƒ¨åˆ†å¯è§ï¼Œä¸”æ¨¡æ€å’Œæ ‡æ³¨æœ‰é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè§†å›¾ä¸€è‡´æ€§æ¨¡å—ï¼Œè’¸é¦å¤šæ¨¡æ€æ•™å¸ˆçŸ¥è¯†åˆ°å­¦ç”Ÿæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MultiSensor-Homeæ•°æ®é›†ä¸Šè¶…è¶Šç«žäº‰æ–¹æ³•ï¼Œå¹¶åœ¨æœ‰é™æ¡ä»¶ä¸‹ä¼˜äºŽæ•™å¸ˆæ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

