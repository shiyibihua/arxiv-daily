---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-18
---

# cs.CVï¼ˆ2025-08-18ï¼‰

ğŸ“Š å…± **26** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250813068v1-eyes-on-the-image-gaze-supervised-multimodal-learning-for-chest-x-ra.html">Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation</a></td>
  <td>æå‡ºåŸºäºæ³¨è§†ç›‘ç£çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ä»¥æå‡èƒ¸éƒ¨Xå…‰è¯Šæ–­ä¸æŠ¥å‘Šç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13068v1" data-paper-url="./papers/250813068v1-eyes-on-the-image-gaze-supervised-multimodal-learning-for-chest-x-ra.html" onclick="toggleFavorite(this, '2508.13068v1', 'Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250812957v1-breaking-reward-collapse-adaptive-reinforcement-for-open-ended-medic.html">Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination</a></td>
  <td>æå‡ºARMedä»¥è§£å†³åŒ»ç–—æ¨ç†ä¸­çš„å¥–åŠ±å´©æºƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12957v1" data-paper-url="./papers/250812957v1-breaking-reward-collapse-adaptive-reinforcement-for-open-ended-medic.html" onclick="toggleFavorite(this, '2508.12957v1', 'Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250812628v1-creative4u-mllms-based-advertising-creative-image-selector-with-comp.html">Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning</a></td>
  <td>æå‡ºCreative4Uä»¥è§£å†³å¹¿å‘Šåˆ›æ„å›¾åƒé€‰æ‹©çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12628v1" data-paper-url="./papers/250812628v1-creative4u-mllms-based-advertising-creative-image-selector-with-comp.html" onclick="toggleFavorite(this, '2508.12628v1', 'Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250812750v3-d2-mamba-dual-scale-fusion-and-dual-path-scanning-with-ssms-for-shad.html">D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal</a></td>
  <td>æå‡ºD2-Mambaä»¥è§£å†³é˜´å½±å»é™¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12750v3" data-paper-url="./papers/250812750v3-d2-mamba-dual-scale-fusion-and-dual-path-scanning-with-ssms-for-shad.html" onclick="toggleFavorite(this, '2508.12750v3', 'D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250813009v3-matrix-game-20-an-open-source-real-time-and-streaming-interactive-wo.html">Matrix-game 2.0: An open-source real-time and streaming interactive world model</a></td>
  <td>æå‡ºMatrix-Game 2.0ä»¥è§£å†³å®æ—¶äº¤äº’ä¸–ç•Œå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13009v3" data-paper-url="./papers/250813009v3-matrix-game-20-an-open-source-real-time-and-streaming-interactive-wo.html" onclick="toggleFavorite(this, '2508.13009v3', 'Matrix-game 2.0: An open-source real-time and streaming interactive world model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250813280v1-cloe-curriculum-learning-on-endoscopic-images-for-robust-mes-classif.html">CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification</a></td>
  <td>æå‡ºCLoEæ¡†æ¶ä»¥è§£å†³å†…çª¥é•œå›¾åƒMESåˆ†ç±»ä¸­çš„æ ‡ç­¾å™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">curriculum learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13280v1" data-paper-url="./papers/250813280v1-cloe-curriculum-learning-on-endoscopic-images-for-robust-mes-classif.html" onclick="toggleFavorite(this, '2508.13280v1', 'CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250812692v2-multi-level-knowledge-distillation-and-dynamic-self-supervised-learn.html">Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning</a></td>
  <td>æå‡ºå¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦ä¸åŠ¨æ€è‡ªç›‘ç£å­¦ä¹ ä»¥è§£å†³æŒç»­å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12692v2" data-paper-url="./papers/250812692v2-multi-level-knowledge-distillation-and-dynamic-self-supervised-learn.html" onclick="toggleFavorite(this, '2508.12692v2', 'Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250812986v1-point-upsampling-networks-for-single-photon-sensing.html">Point upsampling networks for single-photon sensing</a></td>
  <td>æå‡ºç‚¹ä¸Šé‡‡æ ·ç½‘ç»œä»¥è§£å†³å•å…‰å­ä¼ æ„Ÿä¸­çš„ç¨€ç–ç‚¹äº‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12986v1" data-paper-url="./papers/250812986v1-point-upsampling-networks-for-single-photon-sensing.html" onclick="toggleFavorite(this, '2508.12986v1', 'Point upsampling networks for single-photon sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250812587v2-multimodal-chain-of-continuous-thought-for-latent-space-reasoning-in.html">Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models</a></td>
  <td>æå‡ºå¤šæ¨¡æ€è¿ç»­æ€ç»´é“¾ä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12587v2" data-paper-url="./papers/250812587v2-multimodal-chain-of-continuous-thought-for-latent-space-reasoning-in.html" onclick="toggleFavorite(this, '2508.12587v2', 'Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250813142v4-holistic-evaluation-of-multimodal-llms-on-spatial-intelligence.html">Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</a></td>
  <td>æå‡ºEASIä»¥å…¨é¢è¯„ä¼°å¤šæ¨¡æ€LLMsåœ¨ç©ºé—´æ™ºèƒ½ä¸Šçš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13142v4" data-paper-url="./papers/250813142v4-holistic-evaluation-of-multimodal-llms-on-spatial-intelligence.html" onclick="toggleFavorite(this, '2508.13142v4', 'Holistic Evaluation of Multimodal LLMs on Spatial Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250813000v1-omni-survey-for-multimodality-analysis-in-visual-object-tracking.html">Omni Survey for Multimodality Analysis in Visual Object Tracking</a></td>
  <td>æå‡ºå¤šæ¨¡æ€è§†è§‰ç›®æ ‡è·Ÿè¸ªçš„å…¨æ™¯è°ƒæŸ¥ä»¥è§£å†³æ•°æ®æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13000v1" data-paper-url="./papers/250813000v1-omni-survey-for-multimodality-analysis-in-visual-object-tracking.html" onclick="toggleFavorite(this, '2508.13000v1', 'Omni Survey for Multimodality Analysis in Visual Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250812842v1-multi-source-multimodal-progressive-domain-adaption-for-audio-visual.html">Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection</a></td>
  <td>æå‡ºå¤šæºå¤šæ¨¡æ€æ¸è¿›é¢†åŸŸé€‚åº”æ¡†æ¶ä»¥è§£å†³éŸ³è§†é¢‘æ¬ºéª—æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12842v1" data-paper-url="./papers/250812842v1-multi-source-multimodal-progressive-domain-adaption-for-audio-visual.html" onclick="toggleFavorite(this, '2508.12842v1', 'Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250812605v1-vida-ugc-detailed-image-quality-analysis-via-visual-distortion-asses.html">ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images</a></td>
  <td>æå‡ºViDA-UGCä»¥è§£å†³UGCå›¾åƒè´¨é‡è¯„ä¼°ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12605v1" data-paper-url="./papers/250812605v1-vida-ugc-detailed-image-quality-analysis-via-visual-distortion-asses.html" onclick="toggleFavorite(this, '2508.12605v1', 'ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250812711v4-drifting-away-from-truth-genai-driven-news-diversity-challenges-lvlm.html">Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</a></td>
  <td>æå‡ºDriftBenchä»¥è§£å†³GenAIé©±åŠ¨çš„æ–°é—»å¤šæ ·æ€§å¯¹è™šå‡ä¿¡æ¯æ£€æµ‹çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12711v4" data-paper-url="./papers/250812711v4-drifting-away-from-truth-genai-driven-news-diversity-challenges-lvlm.html" onclick="toggleFavorite(this, '2508.12711v4', 'Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250813238v2-dianjin-ocr-r1-enhancing-ocr-capabilities-via-a-reasoning-and-tool-i.html">DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model</a></td>
  <td>æå‡ºDianJin-OCR-R1ä»¥è§£å†³OCRä»»åŠ¡ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13238v2" data-paper-url="./papers/250813238v2-dianjin-ocr-r1-enhancing-ocr-capabilities-via-a-reasoning-and-tool-i.html" onclick="toggleFavorite(this, '2508.13238v2', 'DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250812586v1-foundation-model-for-skeleton-based-human-action-understanding.html">Foundation Model for Skeleton-Based Human Action Understanding</a></td>
  <td>æå‡ºç»Ÿä¸€éª¨æ¶åŸºç¡€æ¨¡å‹ä»¥è§£å†³äººç±»åŠ¨ä½œç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12586v1" data-paper-url="./papers/250812586v1-foundation-model-for-skeleton-based-human-action-understanding.html" onclick="toggleFavorite(this, '2508.12586v1', 'Foundation Model for Skeleton-Based Human Action Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250812718v1-single-reference-text-to-image-manipulation-with-dual-contrastive-de.html">Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score</a></td>
  <td>æå‡ºåŒå¯¹æ¯”å»å™ªè¯„åˆ†ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">structure preservation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12718v1" data-paper-url="./papers/250812718v1-single-reference-text-to-image-manipulation-with-dual-contrastive-de.html" onclick="toggleFavorite(this, '2508.12718v1', 'Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250813104v1-precise-action-to-video-generation-through-visual-action-prompts.html">Precise Action-to-Video Generation Through Visual Action Prompts</a></td>
  <td>æå‡ºè§†è§‰åŠ¨ä½œæç¤ºä»¥è§£å†³åŠ¨ä½œåˆ°è§†é¢‘ç”Ÿæˆçš„ç²¾åº¦ä¸é€šç”¨æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13104v1" data-paper-url="./papers/250813104v1-precise-action-to-video-generation-through-visual-action-prompts.html" onclick="toggleFavorite(this, '2508.13104v1', 'Precise Action-to-Video Generation Through Visual Action Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250813153v1-igfuse-interactive-3d-gaussian-scene-reconstruction-via-multi-scans-.html">IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion</a></td>
  <td>æå‡ºIGFuseä»¥è§£å†³3Dåœºæ™¯é‡å»ºä¸­çš„é®æŒ¡ä¸è¦†ç›–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13153v1" data-paper-url="./papers/250813153v1-igfuse-interactive-3d-gaussian-scene-reconstruction-via-multi-scans-.html" onclick="toggleFavorite(this, '2508.13153v1', 'IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250813065v3-odo-depth-guided-diffusion-for-identity-preserving-body-reshaping.html">Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping</a></td>
  <td>æå‡ºOdoä»¥è§£å†³äººå½¢ç¼–è¾‘ä¸­çš„å½¢çŠ¶ä¿ç•™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">SMPL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13065v3" data-paper-url="./papers/250813065v3-odo-depth-guided-diffusion-for-identity-preserving-body-reshaping.html" onclick="toggleFavorite(this, '2508.13065v3', 'Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250813287v1-innergs-internal-scenes-rendering-via-factorized-3d-gaussian-splatti.html">InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting</a></td>
  <td>æå‡ºInnerGSä»¥é‡å»ºå†…éƒ¨åœºæ™¯ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13287v1" data-paper-url="./papers/250813287v1-innergs-internal-scenes-rendering-via-factorized-3d-gaussian-splatti.html" onclick="toggleFavorite(this, '2508.13287v1', 'InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250812720v3-quantifying-and-alleviating-co-adaptation-in-sparse-view-3d-gaussian.html">Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting</a></td>
  <td>æå‡ºæ–°ç­–ç•¥ä»¥ç¼“è§£ç¨€ç–è§†å›¾3Dé«˜æ–¯ç‚¹äº‘çš„å…±é€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12720v3" data-paper-url="./papers/250812720v3-quantifying-and-alleviating-co-adaptation-in-sparse-view-3d-gaussian.html" onclick="toggleFavorite(this, '2508.12720v3', 'Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250813091v1-dmsdiffusion-based-multi-baseline-stereo-generation-for-improving-se.html">DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation</a></td>
  <td>æå‡ºDMSä»¥è§£å†³è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ä¸­çš„è§†å·®æ¨¡ç³Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13091v1" data-paper-url="./papers/250813091v1-dmsdiffusion-based-multi-baseline-stereo-generation-for-improving-se.html" onclick="toggleFavorite(this, '2508.13091v1', 'DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250813043v1-intellicap-intelligent-guidance-for-consistent-view-sampling.html">IntelliCap: Intelligent Guidance for Consistent View Sampling</a></td>
  <td>æå‡ºIntelliCapä»¥è§£å†³å›¾åƒé‡‡é›†ä¸­çš„å¼•å¯¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13043v1" data-paper-url="./papers/250813043v1-intellicap-intelligent-guidance-for-consistent-view-sampling.html" onclick="toggleFavorite(this, '2508.13043v1', 'IntelliCap: Intelligent Guidance for Consistent View Sampling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250813013v1-egotwin-dreaming-body-and-view-in-first-person.html">EgoTwin: Dreaming Body and View in First Person</a></td>
  <td>æå‡ºEgoTwinä»¥è§£å†³ç¬¬ä¸€äººç§°è§†é¢‘ç”Ÿæˆä¸äººä½“è¿åŠ¨å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span> <span class="paper-tag">egocentric</span> <span class="paper-tag">first-person view</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13013v1" data-paper-url="./papers/250813013v1-egotwin-dreaming-body-and-view-in-first-person.html" onclick="toggleFavorite(this, '2508.13013v1', 'EgoTwin: Dreaming Body and View in First Person')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250812969v1-compact-attention-exploiting-structured-spatio-temporal-sparsity-for.html">Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation</a></td>
  <td>æå‡ºç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠ é€Ÿè§†é¢‘ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12969v1" data-paper-url="./papers/250812969v1-compact-attention-exploiting-structured-spatio-temporal-sparsity-for.html" onclick="toggleFavorite(this, '2508.12969v1', 'Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)