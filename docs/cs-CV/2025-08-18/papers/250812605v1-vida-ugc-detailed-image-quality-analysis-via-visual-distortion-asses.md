---
layout: default
title: ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images
---

# ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12605v1</a>
  <a href="https://arxiv.org/pdf/2508.12605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12605v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12605v1', 'ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenjie Liao, Jieyu Yuan, Yifang Xu, Chunle Guo, Zilong Zhang, Jihong Li, Jiachen Fu, Haotian Fan, Tao Li, Junhui Cui, Chongyi Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViDA-UGCä»¥è§£å†³UGCå›¾åƒè´¨é‡è¯„ä¼°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒè´¨é‡è¯„ä¼°` `ç”¨æˆ·ç”Ÿæˆå†…å®¹` `è§†è§‰å¤±çœŸè¯„ä¼°` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `é“¾å¼æ€ç»´` `æ•°æ®é›†æ„å»º` `è´¨é‡æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯„ä¼°UGCå’ŒAIGCå›¾åƒï¼Œç¼ºä¹è¯¦ç»†çš„è´¨é‡åˆ†æã€‚
2. æœ¬ç ”ç©¶æå‡ºViDA-UGCæ•°æ®é›†ï¼Œé€šè¿‡å¤±çœŸå¯¼å‘çš„æµç¨‹å’ŒCoTæ¡†æ¶ï¼Œæä¾›ç»†ç²’åº¦çš„UGCå›¾åƒè´¨é‡è¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViDA-UGCåŠå…¶æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡åˆ†æèƒ½åŠ›ï¼Œè¶…è¶Šäº†GPT-4oã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•ä½¿å¾—å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä»ä¸å¯è§£é‡Šçš„å›¾åƒè´¨é‡è¯„åˆ†è½¬å‘å¯è§£é‡Šçš„IQAï¼Œå±•ç°å‡ºåœ¨è´¨é‡æ§åˆ¶å’Œä¼˜åŒ–æŒ‡å¯¼ç­‰å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯è§£é‡ŠIQAæ–¹æ³•åœ¨è¯„ä¼°ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰å›¾åƒæ—¶ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ç›¸åŒçš„å¤±çœŸæ ‡å‡†ï¼Œä¸”ç¼ºä¹å¯¹å›¾åƒè´¨é‡çš„è¯¦ç»†åˆ†æã€‚æœ¬ç ”ç©¶å»ºç«‹äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„UGCå›¾åƒè§†è§‰å¤±çœŸè¯„ä¼°æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ViDA-UGCï¼ŒåŒ…å«11Kå¼ å›¾åƒåŠå…¶ç»†ç²’åº¦è´¨é‡åŸºç¡€ã€è¯¦ç»†è´¨é‡æ„ŸçŸ¥å’Œæ¨ç†è´¨é‡æè¿°æ•°æ®ã€‚è¯¥æ•°æ®é›†é€šè¿‡å¤±çœŸå¯¼å‘çš„æµç¨‹æ„å»ºï¼Œç»“åˆäººç±»æ ‡æ³¨å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¯„ä¼°æ¡†æ¶ï¼Œå¸®åŠ©æ•æ‰ä¸å¤±çœŸæ¨¡å¼ç›¸å…³çš„ä¸°å¯Œä½çº§è§†è§‰ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViDA-UGCåŠCoTæ¡†æ¶åœ¨å¤šä¸ªåŸºç¡€MLLMsä¸Šæ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡åˆ†æèƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨UGCå’ŒAIGCå›¾åƒè¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹è¯¦ç»†çš„è´¨é‡åˆ†æå’Œå¤±çœŸæ ‡å‡†çš„æœ‰æ•ˆåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºViDA-UGCæ•°æ®é›†ï¼Œç»“åˆäººç±»æ ‡æ³¨å’Œé“¾å¼æ€ç»´æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°UGCå›¾åƒçš„è§†è§‰å¤±çœŸï¼Œè¿›è€Œå®ç°ç»†ç²’åº¦çš„è´¨é‡åˆ†æã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€å¤±çœŸå¯¼å‘çš„æ ‡æ³¨æµç¨‹å’ŒCoTè¯„ä¼°æ¡†æ¶ã€‚æ•°æ®é›†åŒ…å«11Kå¼ å›¾åƒåŠå…¶è´¨é‡æè¿°ï¼ŒCoTæ¡†æ¶åˆ™å¼•å¯¼æ¨¡å‹ç”Ÿæˆè´¨é‡åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šViDA-UGCæ•°æ®é›†çš„æ„å»ºåŠå…¶å¤±çœŸå¯¼å‘çš„è¯„ä¼°æ–¹æ³•æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œæ˜¾è‘—æé«˜äº†UGCå›¾åƒè´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œç»†è‡´åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†äººç±»æ ‡æ³¨å’Œä¸“ä¸šå›¢é˜Ÿçš„å®¡æ ¸ï¼Œç¡®ä¿äº†æ•°æ®çš„å‡†ç¡®æ€§å’Œè´¨é‡ï¼›åŒæ—¶ï¼Œé€‰æ‹©äº†476å¼ å›¾åƒåŠå…¶6149ä¸ªé—®ç­”å¯¹è¿›è¡Œæ·±å…¥åˆ†æï¼Œæå‡äº†æ¨¡å‹ç”Ÿæˆä¿¡æ¯çš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒViDA-UGCåŠå…¶CoTæ¡†æ¶åœ¨ViDA-UGC-Benchå’ŒQ-Benchä¸Šæ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡åˆ†æèƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†GPT-4oï¼Œå±•ç¤ºäº†å…¶åœ¨UGCå›¾åƒè¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒè´¨é‡æ§åˆ¶ã€å†…å®¹åˆ›ä½œä¼˜åŒ–å’Œç¤¾äº¤åª’ä½“å¹³å°çš„UGCç›‘æµ‹ç­‰ã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼ŒViDA-UGCå¯ä»¥å¸®åŠ©å†…å®¹åˆ›ä½œè€…å’Œå¹³å°è¿è¥è€…æå‡ç”¨æˆ·ä½“éªŒï¼Œä¼˜åŒ–å†…å®¹è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models (MLLMs) have introduced a paradigm shift for Image Quality Assessment (IQA) from unexplainable image quality scoring to explainable IQA, demonstrating practical applications like quality control and optimization guidance. However, current explainable IQA methods not only inadequately use the same distortion criteria to evaluate both User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also lack detailed quality analysis for monitoring image quality and guiding image restoration. In this study, we establish the first large-scale Visual Distortion Assessment Instruction Tuning Dataset for UGC images, termed ViDA-UGC, which comprises 11K images with fine-grained quality grounding, detailed quality perception, and reasoning quality description data. This dataset is constructed through a distortion-oriented pipeline, which involves human subject annotation and a Chain-of-Thought (CoT) assessment framework. This framework guides GPT-4o to generate quality descriptions by identifying and analyzing UGC distortions, which helps capturing rich low-level visual features that inherently correlate with distortion patterns. Moreover, we carefully select 476 images with corresponding 6,149 question answer pairs from ViDA-UGC and invite a professional team to ensure the accuracy and quality of GPT-generated information. The selected and revised data further contribute to the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench. Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT framework for consistently enhancing various image quality analysis abilities across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing GPT-4o.

