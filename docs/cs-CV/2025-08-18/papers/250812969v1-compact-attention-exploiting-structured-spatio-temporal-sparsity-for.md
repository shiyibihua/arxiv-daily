---
layout: default
title: Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation
---

# Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12969" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12969v1</a>
  <a href="https://arxiv.org/pdf/2508.12969.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12969v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12969v1', 'Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qirui Li, Guangcong Zheng, Qi Zhao, Jie Li, Bin Dong, Yiwu Yao, Xi Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://yo-ava.github.io/Compact-Attention.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠ é€Ÿè§†é¢‘ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `æ—¶ç©ºç¨€ç–æ€§` `æ·±åº¦å­¦ä¹ ` `å˜æ¢å™¨` `è®¡ç®—æ•ˆç‡` `åŠ¨æ€åˆ†å—` `è‡ªåŠ¨é…ç½®æœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è§†é¢‘æ•°æ®çš„æ—¶ç©ºå†—ä½™ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡è‡ªé€‚åº”åˆ†å—ã€æ—¶é—´å˜åŒ–çª—å£å’Œè‡ªåŠ¨é…ç½®æœç´¢æ¥ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—ã€‚
3. åœ¨å•GPUè®¾ç½®ä¸‹ï¼Œæ–¹æ³•å®ç°äº†1.6~2.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒè§†è§‰è´¨é‡ï¼Œä¸å…¨æ³¨æ„åŠ›åŸºçº¿ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—éœ€æ±‚å¯¹åŸºäºå˜æ¢å™¨çš„è§†é¢‘ç”Ÿæˆæ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨åˆæˆè¶…é•¿åºåˆ—æ—¶ã€‚ç°æœ‰æ–¹æ³•å¦‚åˆ†è§£æ³¨æ„åŠ›å’Œå›ºå®šç¨€ç–æ¨¡å¼æœªèƒ½å……åˆ†åˆ©ç”¨è§†é¢‘æ•°æ®ä¸­çš„æ—¶ç©ºå†—ä½™ã€‚é€šè¿‡å¯¹è§†é¢‘æ‰©æ•£å˜æ¢å™¨çš„ç³»ç»Ÿåˆ†æï¼Œæœ¬æ–‡å‘ç°æ³¨æ„åŠ›çŸ©é˜µå‘ˆç°å‡ºç»“æ„åŒ–ä½†å¼‚è´¨çš„ç¨€ç–æ¨¡å¼ï¼Œç‰¹å®šçš„å¤´éƒ¨åŠ¨æ€å…³æ³¨ä¸åŒçš„æ—¶ç©ºåŒºåŸŸã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸€ä¸ªç¡¬ä»¶æ„ŸçŸ¥çš„åŠ é€Ÿæ¡†æ¶ï¼ŒåŒ…å«ä¸‰é¡¹åˆ›æ–°ï¼šè‡ªé€‚åº”åˆ†å—ç­–ç•¥ã€æ—¶é—´å˜åŒ–çª—å£å’Œè‡ªåŠ¨é…ç½®æœç´¢ç®—æ³•ã€‚è¯¥æ–¹æ³•åœ¨å•GPUè®¾ç½®ä¸‹å®ç°äº†1.6~2.5å€çš„æ³¨æ„åŠ›è®¡ç®—åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒä¸å…¨æ³¨æ„åŠ›åŸºçº¿ç›¸å½“çš„è§†è§‰è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºå˜æ¢å™¨çš„è§†é¢‘ç”Ÿæˆä¸­è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é«˜è®¡ç®—éœ€æ±‚ï¼Œç°æœ‰æ–¹æ³•å¦‚åˆ†è§£æ³¨æ„åŠ›å’Œå›ºå®šç¨€ç–æ¨¡å¼æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è§†é¢‘æ•°æ®çš„æ—¶ç©ºå†—ä½™ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›è®¡ç®—çš„ç¨€ç–æ€§ï¼Œä»¥é€‚åº”è§†é¢‘æ•°æ®çš„ç»“æ„åŒ–ç¨€ç–æ€§ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè‡ªé€‚åº”åˆ†å—ç­–ç•¥ã€æ—¶é—´å˜åŒ–çª—å£å’Œè‡ªåŠ¨é…ç½®æœç´¢ç®—æ³•ã€‚è‡ªé€‚åº”åˆ†å—ç­–ç•¥é€šè¿‡åŠ¨æ€åˆ†ç»„æ¥è¿‘ä¼¼å¤šæ ·çš„ç©ºé—´äº¤äº’æ¨¡å¼ï¼›æ—¶é—´å˜åŒ–çª—å£æ ¹æ®å¸§çš„æ¥è¿‘ç¨‹åº¦è°ƒæ•´ç¨€ç–æ€§ï¼›è‡ªåŠ¨é…ç½®æœç´¢ç®—æ³•ä¼˜åŒ–ç¨€ç–æ¨¡å¼ï¼ŒåŒæ—¶ä¿ç•™å…³é”®çš„æ³¨æ„åŠ›è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€çš„ç¨€ç–æ¨¡å¼å’Œè‡ªé€‚åº”ç­–ç•¥ï¼Œä½¿å¾—æ³¨æ„åŠ›è®¡ç®—ä¸ä»…é«˜æ•ˆä¸”çµæ´»ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„åˆšæ€§çº¦æŸå’Œæ˜¾è‘—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€åˆ†å—å’Œæ—¶é—´å˜åŒ–çª—å£çš„å‚æ•°è®¾ç½®ï¼Œç¡®ä¿åœ¨ä¸åŒæ—¶é—´å¸§ä¹‹é—´çš„ç¨€ç–æ€§è°ƒæ•´ï¼ŒåŒæ—¶ä½¿ç”¨äº†ä¼˜åŒ–çš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶åœ¨å•GPUè®¾ç½®ä¸‹å®ç°äº†1.6~2.5å€çš„åŠ é€Ÿï¼Œä¸”åœ¨è§†è§‰è´¨é‡ä¸Šä¸å…¨æ³¨æ„åŠ›åŸºçº¿ç›¸å½“ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç”Ÿæˆã€å®æ—¶è§†é¢‘å¤„ç†å’Œé•¿è§†é¢‘ç†è§£ç­‰ã€‚é€šè¿‡æé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡ï¼Œç´§å‡‘æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­å®ç°æ›´å¿«çš„å¤„ç†é€Ÿåº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The computational demands of self-attention mechanisms pose a critical challenge for transformer-based video generation, particularly in synthesizing ultra-long sequences. Current approaches, such as factorized attention and fixed sparse patterns, fail to fully exploit the inherent spatio-temporal redundancies in video data. Through systematic analysis of video diffusion transformers (DiT), we uncover a key insight: Attention matrices exhibit structured, yet heterogeneous sparsity patterns, where specialized heads dynamically attend to distinct spatiotemporal regions (e.g., local pattern, cross-shaped pattern, or global pattern). Existing sparse attention methods either impose rigid constraints or introduce significant overhead, limiting their effectiveness. To address this, we propose Compact Attention, a hardware-aware acceleration framework featuring three innovations: 1) Adaptive tiling strategies that approximate diverse spatial interaction patterns via dynamic tile grouping, 2) Temporally varying windows that adjust sparsity levels based on frame proximity, and 3) An automated configuration search algorithm that optimizes sparse patterns while preserving critical attention pathways. Our method achieves 1.6~2.5x acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines. This work provides a principled approach to unlocking efficient long-form video generation through structured sparsity exploitation. Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

