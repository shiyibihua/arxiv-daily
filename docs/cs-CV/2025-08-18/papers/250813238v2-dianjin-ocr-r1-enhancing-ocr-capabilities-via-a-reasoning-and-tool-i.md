---
layout: default
title: DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model
---

# DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13238" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13238v2</a>
  <a href="https://arxiv.org/pdf/2508.13238.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13238v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13238v2', 'DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, Chi Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18 (æ›´æ–°: 2025-09-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDianJin-OCR-R1ä»¥è§£å†³OCRä»»åŠ¡ä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å…‰å­¦å­—ç¬¦è¯†åˆ«` `æ¨ç†æœºåˆ¶` `ä¸“å®¶æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ–‡æ¡£è§£æ` `æ¨¡å‹èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„OCRæ–¹æ³•åœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå¯¼è‡´è¯†åˆ«é”™è¯¯ã€‚
2. DianJin-OCR-R1é€šè¿‡ç»“åˆè‡ªèº«OCRèƒ½åŠ›ä¸ä¸“å®¶æ¨¡å‹çš„ç»“æœï¼Œå¢å¼ºäº†æ¨ç†è¿‡ç¨‹ï¼Œæå‡äº†è¯†åˆ«å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDianJin-OCR-R1åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†ä¼ ç»ŸOCRæ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›å±•ä½¿å¾—ç«¯åˆ°ç«¯æ–‡æ¡£å›¾åƒè§£ææˆä¸ºå¯èƒ½ï¼Œåœ¨æ–‡æœ¬ã€è¡¨æ ¼å’Œå…¬å¼è¯†åˆ«ç­‰OCRä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”Ÿæˆå¼LVLMsä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸€æ ·ï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆè¾“å…¥å›¾åƒä¸­ä¸å­˜åœ¨çš„è¯æ±‡ã€‚æ­¤å¤–ï¼ŒLVLMsé€šå¸¸ä¸ºé€šç”¨è®¾è®¡ï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸæ•°æ®é›†è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹åœ¨OCRä»»åŠ¡ä¸Šæ›´ä¸ºæœ‰æ•ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DianJin-OCR-R1ï¼Œä¸€ä¸ªå¢å¼ºæ¨ç†çš„æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒæ¨ç†ä¸å·¥å…·äº¤æ›¿çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¥åº”å¯¹è¿™äº›å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDianJin-OCR-R1åœ¨ReSTå’ŒOmniDocBenchæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºéæ¨ç†æ¨¡å‹å’Œä¸“å®¶OCRæ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰OCRæ¨¡å‹åœ¨å¤æ‚æ–‡æ¡£è¯†åˆ«ä¸­äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚ç°æœ‰çš„ç”Ÿæˆå¼LVLMsåœ¨å¤„ç†ç‰¹å®šä»»åŠ¡æ—¶æ•ˆæœä¸ä½³ï¼Œå®¹æ˜“ç”Ÿæˆé”™è¯¯çš„æ–‡æœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDianJin-OCR-R1é€šè¿‡æ¨ç†ä¸å·¥å…·äº¤æ›¿çš„æ–¹å¼ï¼Œé¦–å…ˆåˆ©ç”¨è‡ªèº«çš„OCRèƒ½åŠ›è¯†åˆ«å›¾åƒå†…å®¹ï¼Œç„¶åè°ƒç”¨å…¶ä»–ä¸“å®¶æ¨¡å‹çš„ç»“æœä½œä¸ºå‚è€ƒï¼Œæœ€åå†è¿›è¡Œä¸€æ¬¡æ¨ç†ï¼Œç¡®ä¿è¯†åˆ«ç»“æœçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è‡ªèº«OCRè¯†åˆ«æ¨¡å—ï¼Œ2) ä¸“å®¶æ¨¡å‹è°ƒç”¨æ¨¡å—ï¼Œ3) æ¨ç†ä¸ç»“æœæ•´åˆæ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—ååŒå·¥ä½œï¼Œä»¥æå‡æœ€ç»ˆçš„è¯†åˆ«æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šDianJin-OCR-R1çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ¨ç†è¿‡ç¨‹ä¸å·¥å…·è°ƒç”¨ç›¸ç»“åˆï¼Œåˆ©ç”¨ä¸“å®¶æ¨¡å‹çš„ç»“æœæ¥å‡å°‘å¹»è§‰ç°è±¡ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸åŒæ¨¡å—çš„è¾“å‡ºï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå¤šæ¬¡æ¨ç†å’Œç»“æœæ•´åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ReSTå’ŒOmniDocBenchæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDianJin-OCR-R1æ¨¡å‹åœ¨OCRä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„éæ¨ç†æ¨¡å‹å’Œä¸“å®¶OCRæ¨¡å‹ï¼Œè¯†åˆ«å‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DianJin-OCR-R1çš„ç ”ç©¶æˆæœåœ¨æ–‡æ¡£è‡ªåŠ¨åŒ–å¤„ç†ã€æ™ºèƒ½åŠå…¬ç³»ç»Ÿå’Œä¿¡æ¯æå–ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜OCRçš„å‡†ç¡®æ€§ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æå‡æ–‡æ¡£ç®¡ç†å’Œæ•°æ®åˆ†æçš„æ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šè¡Œä¸šçš„æ™ºèƒ½åŒ–è½¬å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large vision-language models (LVLMs) have enabled a new paradigm of end-to-end document image parsing, excelling in Optical Character Recognition (OCR) tasks such as text, table, and formula recognition. However, generative LVLMs, similarly to large language models (LLMs), are prone to hallucinations--generating words that do not exist in input images. Furthermore, LVLMs are designed for general purposes and tend to be less effective on OCR tasks compared to expert models that are trained on domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a reasoning-enhanced framework designed to address these limitations through training reasoning-and-tool interleaved VLMs. Given a recognition instruction, our DianJin-OCR-R1 model first recognizes the content in the input image by its own OCR capabilities, and then calls other tools (i.e., other expert models) to obtain their results as references, finally "looks again" the image and rethinks about the reasoning process to provide the final recognized content. Since architectures of expert models are tailored for specific OCR tasks, which makes them less prone to hallucinations, their results can help VLMs mitigate hallucinations. We evaluate our model on ReST and OmniDocBench, and experimental results show that our DianJin-OCR-R1 models consistently outperform their non-reasoning counterparts and expert OCR models, which proves the effectiveness of our method. Additionally, the results indicate that enhancing expert models, which are typically small and easy to iterate, enable performance improvements for VLMs.

