---
layout: default
title: Foundation Model for Skeleton-Based Human Action Understanding
---

# Foundation Model for Skeleton-Based Human Action Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12586" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12586v1</a>
  <a href="https://arxiv.org/pdf/2508.12586.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12586v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12586v1', 'Foundation Model for Skeleton-Based Human Action Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongsong Wang, Wanjiang Weng, Junbo Wang, Fang Zhao, Guo-Sen Xie, Xin Geng, Liang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: Accepted by TPAMI, Code is available at: https://github.com/wengwanjiang/FoundSkelModel

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€éª¨æ¶åŸºç¡€æ¨¡å‹ä»¥è§£å†³äººç±»åŠ¨ä½œç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éª¨æ¶åŠ¨ä½œç†è§£` `å¯†é›†è¡¨ç¤ºå­¦ä¹ ` `Transformer` `ç‰¹å¾å»ç›¸å…³` `ä¸€è‡´æ€§è®­ç»ƒ` `å¤šæ¨¡æ€å­¦ä¹ ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éª¨æ¶åŠ¨ä½œç†è§£æ–¹æ³•åœ¨å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„åŠ¨ä½œç†è§£ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºçš„USDRLæ¡†æ¶é€šè¿‡DSTEã€MG-FDå’ŒMPCTæ¨¡å—ï¼Œæ—¨åœ¨æå‡éª¨æ¶åŠ¨ä½œç†è§£çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
3. åœ¨25ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUSDRLåœ¨ç²—é¢„æµ‹ã€å¯†é›†é¢„æµ‹å’Œè¿ç§»é¢„æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åŠ¨ä½œç†è§£æ˜¯æ™ºèƒ½è¿åŠ¨æ„ŸçŸ¥é¢†åŸŸçš„åŸºç¡€æ”¯æŸ±ã€‚éª¨æ¶ä½œä¸ºä¸€ç§ä¸è®¾å¤‡æ— å…³çš„äººç±»å»ºæ¨¡è¡¨ç¤ºï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–åŠ¨ä½œç†è§£ä»»åŠ¡æ—¶ç¼ºä¹å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„éª¨æ¶å¯†é›†è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼ˆUSDRLï¼‰ï¼Œä½œä¸ºéª¨æ¶åŸºç¡€æ¨¡å‹ã€‚USDRLåŒ…æ‹¬åŸºäºTransformerçš„å¯†é›†æ—¶ç©ºç¼–ç å™¨ï¼ˆDSTEï¼‰ã€å¤šç²’åº¦ç‰¹å¾å»ç›¸å…³ï¼ˆMG-FDï¼‰å’Œå¤šè§†è§’ä¸€è‡´æ€§è®­ç»ƒï¼ˆMPCTï¼‰ã€‚é€šè¿‡åœ¨25ä¸ªåŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒUSDRLæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†éª¨æ¶åŠ¨ä½œç†è§£çš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰éª¨æ¶åŠ¨ä½œç†è§£æ–¹æ³•åœ¨å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç¼ºä¹é€‚åº”å¤šæ ·åŒ–ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç»Ÿä¸€éª¨æ¶å¯†é›†è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼ˆUSDRLï¼‰ï¼Œé€šè¿‡å¤šæ¨¡å—ååŒå·¥ä½œï¼Œæå‡åŠ¨ä½œç†è§£çš„å‡†ç¡®æ€§å’Œä¿¡æ¯æå–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUSDRLæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šDSTEç”¨äºå­¦ä¹ æ—¶ç©ºç‰¹å¾ï¼ŒMG-FDç”¨äºç‰¹å¾å»ç›¸å…³ï¼ŒMPCTç”¨äºå¤šè§†è§’å’Œå¤šæ¨¡æ€ä¸€è‡´æ€§è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDSTEæ¨¡å—é‡‡ç”¨åŒæµç»“æ„ï¼Œåˆ†åˆ«æ•æ‰æ—¶é—´åŠ¨æ€å’Œç©ºé—´ç»“æ„ç‰¹å¾ï¼›MG-FDæ¨¡å—é€šè¿‡è·¨åŸŸåä½œå‡å°‘å†—ä½™ï¼›MPCTæ¨¡å—å¢å¼ºé«˜å±‚è¯­ä¹‰å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DSTEä¸­ï¼Œé‡‡ç”¨Transformeræ¶æ„ï¼›MG-FDé€šè¿‡ç‰¹å¾å»ç›¸å…³æŠ€æœ¯å‡å°‘ç»´åº¦å†—ä½™ï¼›MPCTç»“åˆå¤šè§†è§’å’Œå¤šæ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ï¼Œæå‡ä¿¡æ¯æå–æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨25ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUSDRLåœ¨å¤šä¸ªéª¨æ¶åŠ¨ä½œç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨ç±»äººæœºå™¨äººæ§åˆ¶å’Œäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡éª¨æ¶åŠ¨ä½œç†è§£çš„å‡†ç¡®æ€§ï¼ŒUSDRLå¯ä»¥ä¿ƒè¿›äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½åŒ–ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human action understanding serves as a foundational pillar in the field of intelligent motion perception. Skeletons serve as a modality- and device-agnostic representation for human modeling, and skeleton-based action understanding has potential applications in humanoid robot control and interaction. \RED{However, existing works often lack the scalability and generalization required to handle diverse action understanding tasks. There is no skeleton foundation model that can be adapted to a wide range of action understanding tasks}. This paper presents a Unified Skeleton-based Dense Representation Learning (USDRL) framework, which serves as a foundational model for skeleton-based human action understanding. USDRL consists of a Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The DSTE module adopts two parallel streams to learn temporal dynamic and spatial structure features. The MG-FD module collaboratively performs feature decorrelation across temporal, spatial, and instance domains to reduce dimensional redundancy and enhance information extraction. The MPCT module employs both multi-view and multi-modal self-supervised consistency training. The former enhances the learning of high-level semantics and mitigates the impact of low-level discrepancies, while the latter effectively facilitates the learning of informative multimodal features. We perform extensive experiments on 25 benchmarks across across 9 skeleton-based action understanding tasks, covering coarse prediction, dense prediction, and transferred prediction. Our approach significantly outperforms the current state-of-the-art methods. We hope that this work would broaden the scope of research in skeleton-based action understanding and encourage more attention to dense prediction tasks.

