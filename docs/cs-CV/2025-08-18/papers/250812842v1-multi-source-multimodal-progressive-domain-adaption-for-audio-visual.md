---
layout: default
title: Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection
---

# Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12842" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12842v1</a>
  <a href="https://arxiv.org/pdf/2508.12842.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12842v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12842v1', 'Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ronghao Lin, Sijie Mai, Ying Zeng, Qiaolin He, Aolin Xiong, Haifeng Hu

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: Accepted at ACM MM 2025 SVC Workshop

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/RH-Lin/MMPDA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæºå¤šæ¨¡æ€æ¸è¿›é¢†åŸŸé€‚åº”æ¡†æ¶ä»¥è§£å†³éŸ³è§†é¢‘æ¬ºéª—æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `é¢†åŸŸé€‚åº”` `æ¬ºéª—æ£€æµ‹` `éŸ³è§†é¢‘åˆ†æ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„é¢†åŸŸè½¬ç§»æ—¶å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´éŸ³è§†é¢‘æ¬ºéª—æ£€æµ‹æ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºçš„å¤šæºå¤šæ¨¡æ€æ¸è¿›é¢†åŸŸé€‚åº”æ¡†æ¶é€šè¿‡é€æ­¥å¯¹é½æºåŸŸä¸ç›®æ ‡åŸŸçš„ç‰¹å¾å’Œå†³ç­–å±‚ï¼Œè§£å†³äº†é¢†åŸŸè½¬ç§»é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ä¸Šå‡ä¼˜äºå…¶ä»–å‚èµ›å›¢é˜Ÿï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†åœ¨ç¬¬ä¸€å±Šç»†å¾®è§†è§‰è®¡ç®—ç ”è®¨ä¼šçš„å¤šæ¨¡æ€æ¬ºéª—æ£€æµ‹æŒ‘æˆ˜èµ›ä¸­è·èƒœçš„æ–¹æ³•ã€‚é’ˆå¯¹æºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„é¢†åŸŸè½¬ç§»é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæºå¤šæ¨¡æ€æ¸è¿›é¢†åŸŸé€‚åº”ï¼ˆMMPDAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ¥è‡ªä¸åŒæºåŸŸçš„éŸ³è§†é¢‘çŸ¥è¯†è¿ç§»åˆ°ç›®æ ‡åŸŸã€‚é€šè¿‡é€æ­¥åœ¨ç‰¹å¾å’Œå†³ç­–å±‚é¢å¯¹é½æºåŸŸä¸ç›®æ ‡åŸŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å¼¥åˆäº†ä¸åŒå¤šæ¨¡æ€æ•°æ®é›†ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”èµ›ç¬¬äºŒé˜¶æ®µä¸­å–å¾—äº†60.43%çš„å‡†ç¡®ç‡å’Œ56.99%çš„F1åˆ†æ•°ï¼Œè¶…è¶Šäº†ç¬¬ä¸€åå›¢é˜Ÿ5.59%çš„F1åˆ†æ•°å’Œç¬¬ä¸‰åå›¢é˜Ÿ6.75%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/RH-Lin/MMPDAè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³éŸ³è§†é¢‘æ¬ºéª—æ£€æµ‹ä¸­çš„é¢†åŸŸè½¬ç§»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒæºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„çŸ¥è¯†è¿ç§»æ•ˆæœä¸ä½³ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„å¤šæºå¤šæ¨¡æ€æ¸è¿›é¢†åŸŸé€‚åº”æ¡†æ¶é€šè¿‡é€æ­¥å¯¹é½æºåŸŸä¸ç›®æ ‡åŸŸçš„ç‰¹å¾å’Œå†³ç­–å±‚ï¼Œæ—¨åœ¨æœ‰æ•ˆè¿ç§»éŸ³è§†é¢‘çŸ¥è¯†ï¼Œå‡å°‘é¢†åŸŸé—´çš„å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆé€šè¿‡ç‰¹å¾æå–æ¨¡å—è·å–æºåŸŸå’Œç›®æ ‡åŸŸçš„éŸ³è§†é¢‘ç‰¹å¾ï¼Œç„¶ååœ¨å¯¹é½æ¨¡å—ä¸­é€æ­¥å¯¹é½è¿™äº›ç‰¹å¾ï¼Œæœ€åé€šè¿‡å†³ç­–æ¨¡å—è¿›è¡Œåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ¸è¿›å¼å¯¹é½ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å¾å’Œå†³ç­–å±‚é¢ä¸Šæœ‰æ•ˆå‡å°‘é¢†åŸŸè½¬ç§»çš„å½±å“ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€å¯¹é½æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæºè¾“å…¥å’Œé€æ­¥å¯¹é½çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°ä¸åŒæºåŸŸçš„ç‰¹å¾ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥æé«˜æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨æ¯”èµ›ç¬¬äºŒé˜¶æ®µä¸­å–å¾—äº†60.43%çš„å‡†ç¡®ç‡å’Œ56.99%çš„F1åˆ†æ•°ï¼Œåˆ†åˆ«è¶…è¶Šç¬¬ä¸€åå›¢é˜Ÿ5.59%çš„F1åˆ†æ•°å’Œç¬¬ä¸‰åå›¢é˜Ÿ6.75%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€æ¬ºéª—æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨éŸ³è§†é¢‘æ¬ºéª—æ£€æµ‹é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©æå‡å®‰å…¨ç›‘æ§ã€åœ¨çº¿ä¼šè®®å’Œç¤¾äº¤åª’ä½“å¹³å°ä¸­çš„æ¬ºè¯ˆæ£€æµ‹èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents the winning approach for the 1st MultiModal Deception Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing (SVC). Aiming at the domain shift issue across source and target domains, we propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA) framework that transfers the audio-visual knowledge from diverse source domains to the target domain. By gradually aligning source and the target domain at both feature and decision levels, our method bridges domain shifts across diverse multimodal datasets. Extensive experiments demonstrate the effectiveness of our approach securing Top-2 place. Our approach reaches 60.43% on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy. Our code is available at https://github.com/RH-Lin/MMPDA.

