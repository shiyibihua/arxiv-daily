---
layout: default
title: Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models
---

# Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12587" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12587v2</a>
  <a href="https://arxiv.org/pdf/2508.12587.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12587v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12587v2', 'Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tan-Hanh Pham, Chris Ngo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18 (æ›´æ–°: 2025-09-23)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Hanhpt23/OmniMod)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€è¿ç»­æ€ç»´é“¾ä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è¿ç»­æ€ç»´é“¾` `æ½œåœ¨ç©ºé—´` `è·¨æ¨¡æ€å¯¹é½` `åæ€è®¤çŸ¥` `è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–äºè¯­è¨€æ¨¡å‹ï¼Œéš¾ä»¥æœ‰æ•ˆæ•´åˆè§†è§‰ã€æ–‡æœ¬å’ŒéŸ³é¢‘ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„MCOUTæ–¹æ³•é€šè¿‡åœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨è¿ç»­éšè—å‘é‡è¡¨ç¤ºæ¨ç†çŠ¶æ€ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMCOUTåœ¨å¤šä¸ªåŸºå‡†ä¸Šç›¸è¾ƒäºå¼ºåŸºçº¿æé«˜äº†æœ€é«˜8.23%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸­BLEUåˆ†æ•°æå‡äº†8.27%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†æŠ€æœ¯é‡‡ç”¨è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œå¦‚é“¾å¼æ€ç»´æç¤ºï¼Œè¿™äº›æ–¹æ³•åœ¨æ–‡æœ¬ä¸Šæœ‰æ•ˆï¼Œä½†åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥åŠ¨æ€å¯¹é½éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€è¿ç»­æ€ç»´é“¾ï¼ˆMCOUTï¼‰ï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œè€Œéè‡ªç„¶è¯­è¨€ã€‚MCOUTé€šè¿‡å°†æ¨ç†çŠ¶æ€è¡¨ç¤ºä¸ºè¿ç»­çš„éšè—å‘é‡ï¼Œè¿­ä»£åœ°ä¸è§†è§‰å’Œæ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„åæ€è®¤çŸ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCOUTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€æ¨ç†çš„å‡†ç¡®æ€§ï¼Œå±•ç°å‡ºå…¶ä½œä¸ºäººç±»åæ€å¼å¤šæ¨¡æ€æ¨ç†çš„å¯æ‰©å±•æ¡†æ¶çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•åœ¨åŠ¨æ€å¯¹é½éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯é“¾å¼æ€ç»´æç¤ºåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMCOUTæ–¹æ³•çš„æ ¸å¿ƒåœ¨äºç›´æ¥åœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨è¿ç»­çš„éšè—å‘é‡æ¥è¡¨ç¤ºæ¨ç†çŠ¶æ€ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„åæ€è®¤çŸ¥è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMCOUTåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦å˜ä½“ï¼šMCOUT-Baseå’ŒMCOUT-Multiã€‚MCOUT-Baseé‡ç”¨è¯­è¨€æ¨¡å‹çš„æœ€åéšè—çŠ¶æ€ä½œä¸ºè¿ç»­æ€ç»´è¿›è¡Œè¿­ä»£æ¨ç†ï¼Œè€ŒMCOUT-Multiåˆ™é›†æˆäº†å¤šæ¨¡æ€æ½œåœ¨æ³¨æ„åŠ›ï¼Œä»¥å¢å¼ºè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šMCOUTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿ç»­æ¨ç†çš„èƒ½åŠ›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„åŸºäºè¯­è¨€çš„æ¨ç†æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§æ›´è‡ªç„¶çš„æ¨ç†æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MCOUTä¸­ï¼Œé‡‡ç”¨äº†è¿ç»­éšè—å‘é‡çš„è¿­ä»£æ›´æ–°æœºåˆ¶ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ï¼Œè®¾è®¡äº†é€‚åº”å¤šæ¨¡æ€ç‰¹å¾çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–æ¨ç†æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMCOUTåœ¨MMMã€ScienceQAå’ŒMMStarç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†æœ€é«˜8.23%ï¼Œåœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸­çš„BLEUåˆ†æ•°æå‡äº†8.27%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå’Œå¤šæ¨¡æ€æ•°æ®åˆ†æç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼ŒMCOUTèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­å®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many reasoning techniques for large multimodal models adapt language model approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning as word sequences. While effective for text, these methods are suboptimal for multimodal contexts, struggling to align audio, visual, and textual information dynamically. To explore an alternative paradigm, we propose the Multimodal Chain of Continuous Thought (MCOUT), which enables reasoning directly in a joint latent space rather than in natural language. In MCOUT, the reasoning state is represented as a continuous hidden vector, iteratively refined and aligned with visual and textual embeddings, inspired by human reflective cognition. We develop two variants: MCOUT-Base, which reuses the language model`s last hidden state as the continuous thought for iterative reasoning, and MCOUT-Multi, which integrates multimodal latent attention to strengthen cross-modal alignment between visual and textual features. Experiments on benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong baselines and improving BLEU scores up to 8.27% across multiple-choice and open-ended tasks. These findings highlight latent continuous reasoning as a promising direction for advancing LMMs beyond language-bound CoT, offering a scalable framework for human-like reflective multimodal inference. Code is available at https://github.com/Hanhpt23/OmniMod.

