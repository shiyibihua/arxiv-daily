---
layout: default
title: IntelliCap: Intelligent Guidance for Consistent View Sampling
---

# IntelliCap: Intelligent Guidance for Consistent View Sampling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13043" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13043v1</a>
  <a href="https://arxiv.org/pdf/2508.13043.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13043v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13043v1', 'IntelliCap: Intelligent Guidance for Consistent View Sampling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ayaka Yasunaga, Hideo Saito, Dieter Schmalstieg, Shohei Mori

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: This work is a pre-print version of a paper that has been accepted to the IEEE International Symposium on Mixed and Augmented Reality for future publication. Project Page: https://mediated-reality.github.io/projects/yasunaga_ismar25/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIntelliCapä»¥è§£å†³å›¾åƒé‡‡é›†ä¸­çš„å¼•å¯¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†å›¾åˆæˆ` `å›¾åƒé‡‡é›†` `è¯­ä¹‰åˆ†å‰²` `è§†è§‰-è¯­è¨€æ¨¡å‹` `è™šæ‹Ÿç°å®` `å¢å¼ºç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¼•å¯¼ç”¨æˆ·è¿›è¡Œå›¾åƒé‡‡é›†æ—¶ï¼Œå¾€å¾€å¿½è§†äº†åœºæ™¯ç»“æ„å’Œè§†å›¾ä¾èµ–çš„ææ–™ç‰¹æ€§ï¼Œå¯¼è‡´é‡‡æ ·è´¨é‡ä¸é«˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰åˆ†å‰²å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„å¤šå°ºåº¦æ‰«æå¯è§†åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç”Ÿæˆçƒå½¢ä»£ç†æ¥å¼•å¯¼ç”¨æˆ·é‡‡é›†é‡è¦å¯¹è±¡çš„å›¾åƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„è§†å›¾é‡‡æ ·ç­–ç•¥ï¼Œæå‡äº†å›¾åƒé‡‡é›†çš„æ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å›¾åƒçš„è§†å›¾åˆæˆä¸­ï¼Œä¾‹å¦‚ä½¿ç”¨3Dé«˜æ–¯æº…å°„ï¼Œå°½ç®¡æ¸²æŸ“çš„ä¿çœŸåº¦å’Œé€Ÿåº¦å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¸®åŠ©äººç±»æ”¶é›†è¾“å…¥å›¾åƒæ–¹é¢çš„ç ”ç©¶å´ç›¸å¯¹è¾ƒå°‘ã€‚é«˜è´¨é‡çš„è§†å›¾åˆæˆéœ€è¦å‡åŒ€ä¸”å¯†é›†çš„è§†å›¾é‡‡æ ·ï¼Œè€Œè¿™ä¸€éœ€æ±‚å¾€å¾€éš¾ä»¥æ»¡è¶³ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€å¯¹è±¡æˆ–å¿½ç•¥è§†å›¾ä¾èµ–çš„ææ–™ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æƒ…å¢ƒå¯è§†åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸‹è¿›è¡Œæ‰«æï¼Œè¯†åˆ«éœ€è¦æ‰©å±•å›¾åƒè¦†ç›–çš„é‡è¦å¯¹è±¡ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰åˆ†å‰²å’Œç±»åˆ«è¯†åˆ«æ¥æŒ‡å¯¼ç”¨æˆ·è¿›è¡Œæ‰«æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿè§†å›¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆå¼•å¯¼ç”¨æˆ·è¿›è¡Œå‡åŒ€ä¸”å¯†é›†çš„è§†å›¾é‡‡æ ·çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ»¡è¶³é«˜è´¨é‡è§†å›¾åˆæˆçš„è¦æ±‚ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æƒ…å¢ƒå¯è§†åŒ–æŠ€æœ¯ï¼Œè¯†åˆ«å¹¶æ ‡è®°å‡ºéœ€è¦æ›´å¤šå›¾åƒè¦†ç›–çš„é‡è¦å¯¹è±¡ï¼Œä»¥ä¾¿ç”¨æˆ·åœ¨æ‰«ææ—¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¿™äº›å¯¹è±¡çš„è§†å›¾ä¾èµ–ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨è¯­ä¹‰åˆ†å‰²å’Œç±»åˆ«è¯†åˆ«æŠ€æœ¯è¯†åˆ«é‡è¦å¯¹è±¡ï¼›å…¶æ¬¡ï¼ŒåŸºäºè¿™äº›å¯¹è±¡ç”Ÿæˆçƒå½¢ä»£ç†ï¼›æœ€åï¼Œå®æ—¶å¼•å¯¼ç”¨æˆ·åœ¨æ‰«æè¿‡ç¨‹ä¸­è¿›è¡Œæœ‰æ•ˆçš„å›¾åƒé‡‡é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç»“åˆäº†è¯­ä¹‰åˆ†å‰²ä¸è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šå°ºåº¦ä¸‹è¿›è¡Œæœ‰æ•ˆçš„è§†å›¾å¼•å¯¼ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·çš„å›¾åƒé‡‡é›†æ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•æ›´å…³æ³¨äºåœºæ™¯çš„æ•´ä½“ç»“æ„å’Œè§†å›¾ä¾èµ–ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°æ¥æé«˜é‡è¦å¯¹è±¡çš„è¯†åˆ«ç²¾åº¦ã€‚æ­¤å¤–ï¼Œçƒå½¢ä»£ç†çš„ç”Ÿæˆè¿‡ç¨‹ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼ç”¨æˆ·è¿›è¡Œå›¾åƒé‡‡é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIntelliCapæ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„è§†å›¾é‡‡æ ·æ•ˆç‡æ˜¾è‘—é«˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºå›¾åƒè¦†ç›–ç‡æå‡äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨è§†å›¾åˆæˆçš„è´¨é‡ä¸Šä¹Ÿæœ‰æ˜æ˜¾æ”¹å–„ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œè®¡ç®—æœºå›¾å½¢å­¦ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å›¾åƒé‡‡é›†æ•ˆç‡ã€‚é€šè¿‡æä¾›æ™ºèƒ½å¼•å¯¼ï¼Œç”¨æˆ·èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åœºæ™¯ç»“æ„ï¼Œä»è€Œæé«˜æœ€ç»ˆæ¸²æŸ“çš„è´¨é‡å’ŒçœŸå®æ„Ÿã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨è‡ªåŠ¨åŒ–å›¾åƒé‡‡é›†å’Œæ™ºèƒ½æ‘„å½±ç­‰æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Novel view synthesis from images, for example, with 3D Gaussian splatting, has made great progress. Rendering fidelity and speed are now ready even for demanding virtual reality applications. However, the problem of assisting humans in collecting the input images for these rendering algorithms has received much less attention. High-quality view synthesis requires uniform and dense view sampling. Unfortunately, these requirements are not easily addressed by human camera operators, who are in a hurry, impatient, or lack understanding of the scene structure and the photographic process. Existing approaches to guide humans during image acquisition concentrate on single objects or neglect view-dependent material characteristics. We propose a novel situated visualization technique for scanning at multiple scales. During the scanning of a scene, our method identifies important objects that need extended image coverage to properly represent view-dependent appearance. To this end, we leverage semantic segmentation and category identification, ranked by a vision-language model. Spherical proxies are generated around highly ranked objects to guide the user during scanning. Our results show superior performance in real scenes compared to conventional view sampling strategies.

