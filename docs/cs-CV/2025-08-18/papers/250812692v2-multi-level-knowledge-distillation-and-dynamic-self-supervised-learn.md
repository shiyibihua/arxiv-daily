---
layout: default
title: Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning
---

# Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12692" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12692v2</a>
  <a href="https://arxiv.org/pdf/2508.12692.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12692v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12692v2', 'Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Taeheon Kim, San Kim, Minhyuk Seo, Dongjae Jeon, Wonje Jeung, Jonghyun Choi

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18 (æ›´æ–°: 2025-08-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦ä¸åŠ¨æ€è‡ªç›‘ç£å­¦ä¹ ä»¥è§£å†³æŒç»­å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç±»å¢é‡å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `è‡ªç›‘ç£å­¦ä¹ ` `åŠ¨æ€åŠ æƒ` `æœªæ ‡è®°æ•°æ®åˆ©ç”¨` `æ¨¡å‹ç¨³å®šæ€§` `æŒç»­å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç±»å¢é‡å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾æ¯ä¸ªä»»åŠ¡åŒ…å«æœªè§è¿‡çš„ç±»ï¼Œç¼ºä¹å¯¹é‡å¤ç±»å¼•å…¥çš„æœ‰æ•ˆå¤„ç†ã€‚
2. è®ºæ–‡æå‡ºçš„å¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦ï¼ˆMLKDï¼‰å’ŒåŠ¨æ€è‡ªç›‘ç£æŸå¤±ï¼ˆSSLï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆåˆ©ç”¨å¤–éƒ¨æœªæ ‡è®°æ•°æ®ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨CIRè®¾ç½®ä¸‹æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œåœ¨CVPR CLVISIONæŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬äºŒåã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ç±»å¢é‡å­¦ä¹ ä¸­çš„é‡å¤ç±»å¼•å…¥é—®é¢˜ï¼Œæå‡ºäº†å¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦ï¼ˆMLKDï¼‰å’ŒåŠ¨æ€è‡ªç›‘ç£æŸå¤±ï¼ˆSSLï¼‰ä¸¤ç§æ–°æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨æœªæ ‡è®°æ•°æ®ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç±»å¢é‡å­¦ä¹ ä¸­çš„ç¨³å®šæ€§ä¸çµæ´»æ€§ã€‚MLKDä»å¤šä¸ªå…ˆå‰æ¨¡å‹ä¸­æå–çŸ¥è¯†ï¼Œæ¶µç›–ç‰¹å¾å’Œè¾“å‡ºï¼Œå¢å¼ºæ¨¡å‹å¯¹å†å²çŸ¥è¯†çš„ä¿æŒèƒ½åŠ›ï¼›è€ŒåŠ¨æ€SSLåˆ™åŠ é€Ÿæ–°ç±»çš„å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡åŠ¨æ€åŠ æƒä¿æŒå¯¹ä¸»è¦ä»»åŠ¡çš„å…³æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨CVPRç¬¬äº”å±ŠCLVISIONæŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬äºŒåï¼Œæ˜¾è‘—æå‡äº†CIRè®¾ç½®ä¸‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³ç±»å¢é‡å­¦ä¹ ä¸­çš„é‡å¤ç±»å¼•å…¥é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆä¿æŒå†å²çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦ï¼ˆMLKDï¼‰ä»å¤šä¸ªå…ˆå‰æ¨¡å‹ä¸­æå–çŸ¥è¯†ï¼Œå¹¶ç»“åˆåŠ¨æ€è‡ªç›‘ç£æŸå¤±ï¼ˆSSLï¼‰åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ æ–°ç±»æ—¶ä»èƒ½ä¿æŒå¯¹æ—§ç±»çš„è®°å¿†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šMLKDæ¨¡å—ç”¨äºçŸ¥è¯†æå–ï¼ŒSSLæ¨¡å—ç”¨äºæœªæ ‡è®°æ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ ã€‚é€šè¿‡åŠ¨æ€åŠ æƒæœºåˆ¶ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡ç‚¹å§‹ç»ˆæ”¾åœ¨å½“å‰ä¸»è¦ä»»åŠ¡ä¸Šã€‚

**å…³é”®åˆ›æ–°**ï¼šMLKDçš„åˆ›æ–°åœ¨äºä»å¤šä¸ªè§†è§’ï¼ˆç‰¹å¾ä¸è¾“å‡ºï¼‰æå–çŸ¥è¯†ï¼Œå¢å¼ºæ¨¡å‹çš„çŸ¥è¯†ä¿æŒèƒ½åŠ›ï¼›åŠ¨æ€SSLåˆ™é€šè¿‡åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡ï¼Œä¼˜åŒ–æ–°ç±»å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MLKDä¸­ï¼Œè®¾è®¡äº†å¤šå±‚æ¬¡çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œç¡®ä¿ä¸åŒå±‚æ¬¡çš„çŸ¥è¯†è¢«æœ‰æ•ˆåˆ©ç”¨ï¼›åœ¨åŠ¨æ€SSLä¸­ï¼Œé‡‡ç”¨åŠ¨æ€åŠ æƒç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ æ–°ç±»æ—¶ä¸é—å¿˜æ—§ç±»ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CVPRç¬¬äº”å±ŠCLVISIONæŒ‘æˆ˜èµ›ä¸­ï¼Œæ‰€ææ–¹æ³•è·å¾—ç¬¬äºŒåï¼Œæ˜¾è‘—æå‡äº†ç±»å¢é‡å­¦ä¹ çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MLKDå’ŒåŠ¨æ€SSLåï¼Œæ¨¡å‹åœ¨CIRè®¾ç½®ä¸‹çš„å‡†ç¡®ç‡æå‡äº†XX%ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰éœ€è¦æŒç»­å­¦ä¹ çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆå¤„ç†é‡å¤ç±»å¼•å…¥é—®é¢˜ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¸æ–­é€‚åº”æ–°ä»»åŠ¡ï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„çµæ´»æ€§å’Œç¨³å®šæ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Class-incremental with repetition (CIR), where previously trained classes repeatedly introduced in future tasks, is a more realistic scenario than the traditional class incremental setup, which assumes that each task contains unseen classes. CIR assumes that we can easily access abundant unlabeled data from external sources, such as the Internet. Therefore, we propose two components that efficiently use the unlabeled data to ensure the high stability and the plasticity of models trained in CIR setup. First, we introduce multi-level knowledge distillation (MLKD) that distills knowledge from multiple previous models across multiple perspectives, including features and logits, so the model can maintain much various previous knowledge. Moreover, we implement dynamic self-supervised loss (SSL) to utilize the unlabeled data that accelerates the learning of new classes, while dynamic weighting of SSL keeps the focus of training to the primary task. Both of our proposed components significantly improve the performance in CIR setup, achieving 2nd place in the CVPR 5th CLVISION Challenge.

