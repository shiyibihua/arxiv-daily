---
layout: default
title: Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination
---

# Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12957" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.12957v1</a>
  <a href="https://arxiv.org/pdf/2508.12957.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12957v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12957v1', 'Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yizhou Liu, Jingwei Wei, Zizhi Chen, Minghao Han, Xukun Zhang, Keliang Liu, Lihua Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ARMed‰ª•Ëß£ÂÜ≥ÂåªÁñóÊé®ÁêÜ‰∏≠ÁöÑÂ•ñÂä±Â¥©Ê∫ÉÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÂåªÂ≠¶Êé®ÁêÜ` `ËßÜËßâÈóÆÁ≠î` `Ëá™ÈÄÇÂ∫îÂ•ñÂä±` `ËØ≠‰πâÂå∫ÂàÜ` `Ê®°ÂûãÊ≥õÂåñ` `‰∏¥Â∫äÂ∫îÁî®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÈ¢ÜÂüüÂ∫îÁî®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂºÄÊîæÂºèÂåªÁñóËßÜËßâÈóÆÁ≠î‰∏≠ÔºåÂØºËá¥‰∏¥Â∫äÊé®ÁêÜËÉΩÂäõÂèóÈôê„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ARMedÊ°ÜÊû∂ÔºåÈÄöËøáÁõëÁù£ÂæÆË∞ÉÂºïÂÖ•È¢ÜÂüüÁü•ËØÜÔºåÂπ∂ÁªìÂêàËá™ÈÄÇÂ∫îËØ≠‰πâÂ•ñÂä±ÔºåÊèêÂçáÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåË¥®Èáè„ÄÇ
3. ARMedÂú®ÂÖ≠‰∏™ÂåªÂ≠¶VQAÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂüüÂÜÖ‰ªªÂä°ÂáÜÁ°ÆÁéáÊèêÂçá32.64%ÔºåÂüüÂ§ñÂü∫ÂáÜÊèêÂçá11.65%ÔºåÈ™åËØÅ‰∫ÜÂ•ñÂä±ÂèØÂå∫ÂàÜÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜ‰∏éÊ≥õÂåñËÉΩÂäõÊñπÈù¢Â±ïÁé∞Âá∫Âº∫Â§ßÊΩúÂäõÔºå‰ΩÜÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÈ¢ÜÂüüÁöÑÂ∫îÁî®‰ªçÁÑ∂ËæÉÂ∞ë„ÄÇÁé∞ÊúâÁöÑÂº∫ÂåñÂæÆË∞ÉÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÂ∞ÅÈó≠ÂºèËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÁúüÂÆû‰∏¥Â∫äÊé®ÁêÜ‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑRLÊ°ÜÊû∂ARMedÔºàAdaptive Reinforcement for Medical ReasoningÔºâÔºåÈÄöËøáÂú®ÈìæÂºèÊÄùÁª¥Êï∞ÊçÆ‰∏äËøõË°åÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÔºåÁªìÂêàÊñáÊú¨Ê≠£Á°ÆÊÄßÂíåËá™ÈÄÇÂ∫îËØ≠‰πâÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåARMedÂú®ÂÖ≠‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂåªÂ≠¶VQAÂü∫ÂáÜ‰∏äÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÂüüÂÜÖ‰ªªÂä°ÂáÜÁ°ÆÁéáÊèêÂçá32.64%ÔºåÂüüÂ§ñÂü∫ÂáÜÊèêÂçá11.65%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂåªÂ≠¶ÂΩ±ÂÉèÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂºÄÊîæÂºèËßÜËßâÈóÆÁ≠î‰∏≠Èù¢‰∏¥ÁöÑÂ•ñÂä±Â¥©Ê∫ÉÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂ§öÈõÜ‰∏≠‰∫éÂ∞ÅÈó≠ÂºèËßÜËßâÈóÆÁ≠îÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöARMedÊ°ÜÊû∂ÈÄöËøáÂú®ÈìæÂºèÊÄùÁª¥Êï∞ÊçÆ‰∏äËøõË°åÁõëÁù£ÂæÆË∞ÉÔºåÁªìÂêàÊñáÊú¨Ê≠£Á°ÆÊÄßÂíåËá™ÈÄÇÂ∫îËØ≠‰πâÂ•ñÂä±ÔºåÊó®Âú®ÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜË¥®ÈáèÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöARMedÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàËøõË°åÁõëÁù£ÂæÆË∞É‰ª•ÂºïÂÖ•È¢ÜÂüüÁü•ËØÜÔºåÁÑ∂ÂêéÂ∫îÁî®Âº∫ÂåñÂ≠¶‰π†‰ª•‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöARMedÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•Ëá™ÈÄÇÂ∫îËØ≠‰πâÂ•ñÂä±Êú∫Âà∂ÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊ®°ÂûãÂú®ËØ≠‰πâÂ•ñÂä±‰∏≠Âá∫Áé∞ÁöÑÂ¥©Ê∫ÉÁé∞Ë±°Ôºå‰ΩøÂæóËØ≠‰πâÂ∑ÆÂºÇÊòæËëóÁöÑÂìçÂ∫îËÉΩÂ§üËé∑Âæó‰∏çÂêåÁöÑËØÑÂàÜ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåARMedÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°ÊñáÊú¨Ê≠£Á°ÆÊÄß‰∏éËØ≠‰πâÂ•ñÂä±ÔºåÂπ∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•Á°Æ‰øùÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂ≠¶‰π†ÂíåÊé®ÁêÜ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ARMedÂú®ÂÖ≠‰∏™ÂåªÂ≠¶VQAÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÂÆûÈ™åÁªìÊûúÔºåÂüüÂÜÖ‰ªªÂä°ÂáÜÁ°ÆÁéáÊèêÂçá32.64%ÔºåÂüüÂ§ñÂü∫ÂáÜÊèêÂçá11.65%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåARMedÂú®ÊèêÂçáÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñËÉΩÂäõÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºåÈ™åËØÅ‰∫ÜËá™ÈÄÇÂ∫îËØ≠‰πâÂ•ñÂä±ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê„ÄÅ‰∏¥Â∫äÂÜ≥Á≠ñÊîØÊåÅÁ≥ªÁªüÂíåÊô∫ËÉΩÂåªÁñóÂä©ÊâãÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂú®ÂºÄÊîæÂºèÂåªÁñóËßÜËßâÈóÆÁ≠î‰∏≠ÁöÑË°®Áé∞ÔºåARMedÊúâÊúõÂú®ÂÆûÈôÖ‰∏¥Â∫äÁéØÂ¢É‰∏≠Êèê‰æõÊõ¥‰∏∫Á≤æÂáÜÂíåÂèØÈù†ÁöÑÊé®ÁêÜÊîØÊåÅÔºåÊé®Âä®Êô∫ËÉΩÂåªÁñóÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) with rule-based rewards has demonstrated strong potential in enhancing the reasoning and generalization capabilities of vision-language models (VLMs) and large language models (LLMs), while reducing computational overhead. However, its application in medical imaging remains underexplored. Existing reinforcement fine-tuning (RFT) approaches in this domain primarily target closed-ended visual question answering (VQA), limiting their applicability to real-world clinical reasoning. In contrast, open-ended medical VQA better reflects clinical practice but has received limited attention. While some efforts have sought to unify both formats via semantically guided RL, we observe that model-based semantic rewards often suffer from reward collapse, where responses with significant semantic differences receive similar scores. To address this, we propose ARMed (Adaptive Reinforcement for Medical Reasoning), a novel RL framework for open-ended medical VQA. ARMed first incorporates domain knowledge through supervised fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning with textual correctness and adaptive semantic rewards to enhance reasoning quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results show that ARMed consistently boosts both accuracy and generalization, achieving a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain benchmarks. These results highlight the critical role of reward discriminability in medical RL and the promise of semantically guided rewards for enabling robust and clinically meaningful multimodal reasoning.

