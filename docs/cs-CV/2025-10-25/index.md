---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-25
---

# cs.CVï¼ˆ2025-10-25ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251024777v1-cross-enhanced-multimodal-fusion-of-eye-tracking-and-facial-features.html">Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis</a></td>
  <td>æå‡ºä¸€ç§äº¤å‰å¢å¼ºçš„å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œç”¨äºçœ¼åŠ¨è¿½è¸ªå’Œé¢éƒ¨ç‰¹å¾çš„é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.24777v1" onclick="toggleFavorite(this, '2510.24777v1', 'Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer&#39;s Disease Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251022319v2-grpo-guard-mitigating-implicit-over-optimization-in-flow-matching-vi.html">GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</a></td>
  <td>GRPO-Guardï¼šé€šè¿‡è°ƒèŠ‚è£å‰ªç¼“è§£Flow Matchingä¸­çš„éšå¼è¿‡åº¦ä¼˜åŒ–</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22319v2" onclick="toggleFavorite(this, '2510.22319v2', 'GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251022282v1-cityrise-reasoning-urban-socio-economic-status-in-vision-language-mo.html">CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning</a></td>
  <td>æå‡ºCityRiSEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚ç¤¾ä¼šç»æµåœ°ä½æ¨ç†ä¸­çš„èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22282v1" onclick="toggleFavorite(this, '2510.22282v1', 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251022141v1-loc-a-general-language-guided-framework-for-open-set-3d-occupancy-pr.html">LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction</a></td>
  <td>LOCï¼šä¸€ç§é€šç”¨çš„è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾é›†3D occupancyé¢„æµ‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22141v1" onclick="toggleFavorite(this, '2510.22141v1', 'LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251022322v1-beyond-augmentation-leveraging-inter-instance-relation-in-self-super.html">Beyond Augmentation: Leveraging Inter-Instance Relation in Self-Supervised Representation Learning</a></td>
  <td>æå‡ºåŸºäºå›¾ç¥ç»ç½‘ç»œçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å®ä¾‹é—´å…³ç³»æå‡è¡¨å¾è´¨é‡</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22322v1" onclick="toggleFavorite(this, '2510.22322v1', 'Beyond Augmentation: Leveraging Inter-Instance Relation in Self-Supervised Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251022200v2-longcat-video-technical-report.html">LongCat-Video Technical Report</a></td>
  <td>LongCat-Videoï¼šåŸºäºæ‰©æ•£Transformerçš„é«˜æ•ˆé•¿è§†é¢‘ç”Ÿæˆæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22200v2" onclick="toggleFavorite(this, '2510.22200v2', 'LongCat-Video Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/251022359v1-endosfm3d-learning-to-3d-reconstruct-any-endoscopic-surgery-scene-us.html">EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model</a></td>
  <td>EndoSfM3Dï¼šåˆ©ç”¨è‡ªç›‘ç£åŸºç¡€æ¨¡å‹å­¦ä¹ å†…çª¥é•œæ‰‹æœ¯åœºæ™¯çš„3Dé‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22359v1" onclick="toggleFavorite(this, '2510.22359v1', 'EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251022161v1-i2-nerf-learning-neural-radiance-fields-under-physically-grounded-me.html">I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions</a></td>
  <td>I2-NeRFï¼šæå‡ºä¸€ç§ç‰©ç†å¯ä¿¡çš„ç¥ç»è¾å°„åœºï¼Œå¢å¼ºä»‹è´¨é€€åŒ–ä¸‹çš„ä¸‰ç»´é‡å»ºã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22161v1" onclick="toggleFavorite(this, '2510.22161v1', 'I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251022119v1-cogstereo-neural-stereo-matching-with-implicit-spatial-cognition-emb.html">CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding</a></td>
  <td>CogStereoï¼šåˆ©ç”¨éšå¼ç©ºé—´è®¤çŸ¥åµŒå…¥çš„ç¥ç»ç«‹ä½“åŒ¹é…ï¼Œæå‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22119v1" onclick="toggleFavorite(this, '2510.22119v1', 'CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251022213v2-dynamictree-interactive-real-tree-animation-via-sparse-voxel-spectru.html">DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum</a></td>
  <td>DynamicTreeï¼šåˆ©ç”¨ç¨€ç–ä½“ç´ è°±å®ç°äº¤äº’å¼çœŸå®æ ‘æœ¨åŠ¨ç”»</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22213v2" onclick="toggleFavorite(this, '2510.22213v2', 'DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251022140v1-stg-avatar-animatable-human-avatars-via-spacetime-gaussian.html">STG-Avatar: Animatable Human Avatars via Spacetime Gaussian</a></td>
  <td>æå‡ºSTG-Avatarï¼Œé€šè¿‡æ—¶ç©ºé«˜æ–¯ä¼˜åŒ–å®ç°é«˜ä¿çœŸå¯åŠ¨ç”»äººä½“åŒ–èº«é‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22140v1" onclick="toggleFavorite(this, '2510.22140v1', 'STG-Avatar: Animatable Human Avatars via Spacetime Gaussian')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/251022443v1-benchmarking-egocentric-multimodal-goal-inference-for-assistive-wear.html">Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents</a></td>
  <td>WAGIBenchï¼šç”¨äºè¾…åŠ©å¯ç©¿æˆ´ä»£ç†çš„è‡ªä¸­å¿ƒå¤šæ¨¡æ€ç›®æ ‡æ¨æ–­åŸºå‡†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22443v1" onclick="toggleFavorite(this, '2510.22443v1', 'Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251022129v1-egoemotion-egocentric-vision-and-physiological-signals-for-emotion-a.html">egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks</a></td>
  <td>egoEMOTIONï¼šç»“åˆç¬¬ä¸€äººç§°è§†è§‰ä¸ç”Ÿç†ä¿¡å·çš„æƒ…æ„Ÿä¸äººæ ¼è¯†åˆ«æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22129v1" onclick="toggleFavorite(this, '2510.22129v1', 'egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251022102v1-mitigating-coordinate-prediction-bias-from-positional-encoding-failu.html">Mitigating Coordinate Prediction Bias from Positional Encoding Failures</a></td>
  <td>é’ˆå¯¹MLLMåæ ‡é¢„æµ‹åå·®ï¼Œæå‡ºVision-PE Shuffle Guidanceæ–¹æ³•æå‡å®šä½ç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22102v1" onclick="toggleFavorite(this, '2510.22102v1', 'Mitigating Coordinate Prediction Bias from Positional Encoding Failures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251022171v2-harmony-hidden-activation-representations-and-model-output-aware-unc.html">HARMONY: Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models</a></td>
  <td>æå‡ºHARMONYï¼Œåˆ©ç”¨éšå±‚æ¿€æ´»å’Œæ¨¡å‹è¾“å‡ºæ¥æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22171v2" onclick="toggleFavorite(this, '2510.22171v2', 'HARMONY: Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/251022199v1-mogras-human-motion-with-grasping-in-3d-scenes.html">MOGRAS: Human Motion with Grasping in 3D Scenes</a></td>
  <td>MOGRASï¼šæå‡ºå¤§è§„æ¨¡3Dåœºæ™¯ä¸­äººä½“æŠ“å–äº¤äº’è¿åŠ¨æ•°æ®é›†ä¸åŸºå‡†æ–¹æ³•ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22199v1" onclick="toggleFavorite(this, '2510.22199v1', 'MOGRAS: Human Motion with Grasping in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251022118v2-graid-enhancing-spatial-reasoning-of-vlms-through-high-fidelity-data.html">GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation</a></td>
  <td>GRAIDï¼šé€šè¿‡é«˜è´¨é‡æ•°æ®ç”Ÿæˆå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.22118v2" onclick="toggleFavorite(this, '2510.22118v2', 'GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)