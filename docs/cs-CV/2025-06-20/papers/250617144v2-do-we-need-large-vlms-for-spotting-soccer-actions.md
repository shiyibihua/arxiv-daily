---
layout: default
title: Do We Need Large VLMs for Spotting Soccer Actions?
---

# Do We Need Large VLMs for Spotting Soccer Actions?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17144" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17144v2</a>
  <a href="https://arxiv.org/pdf/2506.17144.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17144v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17144v2', 'Do We Need Large VLMs for Spotting Soccer Actions?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ritabrata Chakraborty, Rajatsubhra Chakraborty, Avijit Dasgupta, Sandeep Chaurasia

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-09-27)

**å¤‡æ³¨**: 6 pages, 2 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯­è¨€æ¨¡å‹çš„è¶³çƒåŠ¨ä½œè¯†åˆ«æ–¹æ³•ä»¥æ›¿ä»£è§†é¢‘å¤„ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶³çƒåŠ¨ä½œè¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†é¢‘å¤„ç†` `ä¸“å®¶è¯„è®º` `æ–‡æœ¬ä¸­å¿ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†é¢‘åŸºç¡€æ–¹æ³•åœ¨å¤„ç†è¶³çƒåŠ¨ä½œè¯†åˆ«æ—¶ï¼Œé€šå¸¸éœ€è¦å¤æ‚çš„è®¡ç®—èµ„æºï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ä¸“å®¶è¯„è®ºçš„æ–‡æœ¬ä¿¡æ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥å®ç°è¶³çƒåŠ¨ä½œçš„è¯†åˆ«ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«å…³é”®æ¯”èµ›äº‹ä»¶æ–¹é¢ä¸è§†é¢‘åŸºç¡€æ–¹æ³•ç›¸å½“ï¼Œä¸”å¤„ç†æ•ˆç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„è§†é¢‘åŸºç¡€ä»»åŠ¡å¦‚è¶³çƒåŠ¨ä½œè¯†åˆ«é€šå¸¸ä¾èµ–äºå¤æ‚ä¸”è®¡ç®—å¯†é›†çš„æ¨¡å‹æ¥å¤„ç†å¯†é›†çš„è§†é¢‘æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»è§†é¢‘ä¸­å¿ƒæ–¹æ³•å‘æ–‡æœ¬ä¸­å¿ƒä»»åŠ¡çš„è½¬å˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è€Œéè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä½¿å¾—æ–¹æ³•æ›´è½»é‡ä¸”å¯æ‰©å±•ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸“å®¶è¯„è®ºæä¾›çš„ä¸°å¯Œæè¿°å’Œä¸Šä¸‹æ–‡çº¿ç´¢åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œå¯ä»¥å¯é åœ°è¯†åˆ«æ¯”èµ›ä¸­çš„å…³é”®åŠ¨ä½œã€‚é€šè¿‡é‡‡ç”¨ä¸‰ä¸ªLLMsä½œä¸ºè¯„å®¡ï¼Œä¸“æ³¨äºç»“æœã€å…´å¥‹åº¦å’Œæˆ˜æœ¯æ¥è¯†åˆ«è¶³çƒæ¯”èµ›ä¸­çš„åŠ¨ä½œï¼Œå®éªŒè¡¨æ˜è¯¥è¯­è¨€ä¸­å¿ƒæ–¹æ³•åœ¨æ£€æµ‹å…³é”®æ¯”èµ›äº‹ä»¶æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘äºæœ€å…ˆè¿›çš„è§†é¢‘åŸºç¡€è¯†åˆ«å™¨ï¼ŒåŒæ—¶åœ¨å¤„ç†æ•´ä¸ªæ¯”èµ›æ—¶å‡ ä¹ä¸éœ€è¦è§†é¢‘å¤„ç†è®¡ç®—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†é¢‘å¤„ç†æ–¹æ³•åœ¨è¶³çƒåŠ¨ä½œè¯†åˆ«ä¸­çš„é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¯†é›†çš„è§†é¢‘æ•°æ®å¤„ç†ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¸“å®¶è¯„è®ºè¿›è¡Œåˆ†æï¼Œè½¬å˜ä¸ºæ–‡æœ¬ä¸­å¿ƒçš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå‡å°‘äº†å¯¹è§†é¢‘å¤„ç†çš„ä¾èµ–ï¼Œä»è€Œé™ä½äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„ç”±ä¸‰ä¸ªLLMsç»„æˆï¼Œåˆ†åˆ«ä¸“æ³¨äºç»“æœã€å…´å¥‹åº¦å’Œæˆ˜æœ¯åˆ†æã€‚æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹å¤„ç†è¾“å…¥çš„æ–‡æœ¬è¯„è®ºï¼Œå¹¶è¾“å‡ºå¯¹æ¯”èµ›åŠ¨ä½œçš„åˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¯­è¨€æ¨¡å‹åº”ç”¨äºåŠ¨ä½œè¯†åˆ«ä»»åŠ¡ï¼Œçªç ´äº†ä¼ ç»Ÿè§†é¢‘å¤„ç†çš„é™åˆ¶ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆç†è§£å’Œå¤„ç†è¶³çƒè¯„è®ºä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„è¯†åˆ«æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¯­è¨€ä¸­å¿ƒæ–¹æ³•åœ¨å…³é”®æ¯”èµ›äº‹ä»¶çš„æ£€æµ‹ä¸Šï¼Œä¸æœ€å…ˆè¿›çš„è§†é¢‘åŸºç¡€è¯†åˆ«å™¨çš„æ€§èƒ½ç›¸è¿‘ï¼ŒåŒæ—¶åœ¨å¤„ç†è¿‡ç¨‹ä¸­å‡ ä¹ä¸éœ€è¦è§†é¢‘è®¡ç®—èµ„æºï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä½“è‚²èµ›äº‹åˆ†æã€å®æ—¶æ¯”èµ›è¯„è®ºç”Ÿæˆä»¥åŠè‡ªåŠ¨åŒ–çš„ä½“è‚²æ•°æ®æŒ–æ˜ã€‚é€šè¿‡é™ä½å¯¹è§†é¢‘å¤„ç†çš„ä¾èµ–ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„åŠ¨ä½œè¯†åˆ«ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. We propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich descriptions and contextual cues contains sufficient information to reliably spot key actions in a match. To demonstrate this, we employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics for spotting actions in soccer matches. Our experiments show that this language-centric approach performs effectively in detecting critical match events coming close to state-of-the-art video-based spotters while using zero video processing compute and similar amount of time to process the entire match.

