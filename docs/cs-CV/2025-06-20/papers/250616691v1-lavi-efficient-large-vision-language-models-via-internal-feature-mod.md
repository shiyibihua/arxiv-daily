---
layout: default
title: LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation
---

# LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16691" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16691v1</a>
  <a href="https://arxiv.org/pdf/2506.16691.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16691v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16691v1', 'LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tongtian Yue, Longteng Guo, Yepeng Tang, Zijia Zhao, Xinxin Zhu, Hua Huang, Jing Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLaViä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€èåˆ` `ç‰¹å¾è°ƒåˆ¶` `è®¡ç®—æ•ˆç‡` `å®æ—¶æ¨ç†` `è§†è§‰è¯­è¨€å¯¹é½` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€é›†æˆæ–¹é¢æ•ˆç‡ä½ä¸‹ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œåº”ç”¨ã€‚
2. LaVié€šè¿‡å†…éƒ¨ç‰¹å¾è°ƒåˆ¶å®ç°è§†è§‰è¯­è¨€èåˆï¼Œé¿å…äº†é•¿ä¸Šä¸‹æ–‡è®¡ç®—è´Ÿæ‹…ï¼Œæå‡äº†æ•ˆç‡ã€‚
3. LaViåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”LLaVA-OV-7Bï¼ŒFLOPså‡å°‘94.0%ï¼Œæ¨ç†é€Ÿåº¦æå‡3.1å€ï¼Œå†…å­˜ä½¿ç”¨å‡åŠã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨è§†è§‰è¯­è¨€é›†æˆæ–¹é¢å­˜åœ¨æ ¹æœ¬ç“¶é¢ˆï¼šæ•ˆç‡ä½ä¸‹ã€‚å½“å‰æ–¹æ³•è¦ä¹ˆç ´åæ¨¡å‹çš„å›ºæœ‰ç»“æ„ï¼Œè¦ä¹ˆå¼•å…¥ä¸¥é‡çš„é•¿ä¸Šä¸‹æ–‡è®¡ç®—è´Ÿæ‹…ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚æœ¬æ–‡é‡æ–°æ€è€ƒå¤šæ¨¡æ€é›†æˆï¼Œæå‡ºLaViï¼Œä¸€ç§æ–°å‹LVLMï¼Œé€šè¿‡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨ç‰¹å¾è°ƒåˆ¶å®ç°æ— ç¼é«˜æ•ˆçš„è§†è§‰è¯­è¨€èåˆã€‚ä¸ä¾èµ–è§†è§‰æ ‡è®°è¿æ¥çš„ä¸»æµLVLMä¸åŒï¼ŒLaVié€šè¿‡å¼•å…¥è½»é‡çº§è‡ªé€‚åº”å˜æ¢ï¼Œé¿å…äº†é•¿ä¸Šä¸‹æ–‡æ‰©å±•ï¼Œç¡®ä¿è§†è§‰è¾“å…¥ä¸è¯­è¨€éšè—çŠ¶æ€çš„ç²¾ç¡®å¯¹é½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒLaViåœ¨15ä¸ªå›¾åƒå’Œè§†é¢‘åŸºå‡†ä¸Šä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ€§èƒ½ï¼Œè¿˜å¤§å¹…æå‡äº†æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€é›†æˆä¸­æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç ´åæ¨¡å‹ç»“æ„æˆ–å¼•å…¥é•¿ä¸Šä¸‹æ–‡è®¡ç®—è´Ÿæ‹…ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLaViçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å†…éƒ¨ç‰¹å¾è°ƒåˆ¶å®ç°è§†è§‰è¯­è¨€çš„é«˜æ•ˆèåˆã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨å…¥è§†è§‰æ¡ä»¶çš„å¢é‡åˆ°å±‚å½’ä¸€åŒ–çš„ä»¿å°„å‚æ•°ä¸­ï¼Œç›´æ¥è°ƒåˆ¶è¯­è¨€éšè—çŠ¶æ€ï¼Œä»è€Œç¡®ä¿è§†è§‰ä¸è¯­è¨€çš„ç²¾ç¡®å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€æ¨¡å‹çš„è¯­è¨€å…ˆéªŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLaViçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥è§†è§‰ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯ï¼Œé€šè¿‡è½»é‡çº§è‡ªé€‚åº”å˜æ¢æ¨¡å—è¿›è¡Œç‰¹å¾è°ƒåˆ¶ï¼Œæœ€ç»ˆè¾“å‡ºèåˆåçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚è¯¥æ¡†æ¶æœ‰æ•ˆé¿å…äº†é•¿ä¸Šä¸‹æ–‡æ‰©å±•çš„é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šLaViçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å†…éƒ¨ç‰¹å¾è°ƒåˆ¶æœºåˆ¶ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–è§†è§‰æ ‡è®°è¿æ¥çš„æ–¹å¼æœ¬è´¨ä¸åŒã€‚è¯¥æœºåˆ¶é€šè¿‡è§†è§‰è¾“å…¥ç›´æ¥è°ƒåˆ¶è¯­è¨€éšè—çŠ¶æ€ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šLaVié‡‡ç”¨è½»é‡çº§çš„å˜æ¢æ¨¡å—ï¼Œå…·ä½“è®¾è®¡åŒ…æ‹¬è§†è§‰æ¡ä»¶å¢é‡çš„æ³¨å…¥æ–¹å¼å’Œå±‚å½’ä¸€åŒ–çš„ä»¿å°„å‚æ•°è°ƒæ•´ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—æˆæœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LaViåœ¨15ä¸ªå›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”äºLLaVA-OV-7Bï¼ŒFLOPså‡å°‘äº†94.0%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†3.1å€ï¼Œå†…å­˜ä½¿ç”¨å‡åŠã€‚è¿™äº›ç»“æœè¡¨æ˜LaViåœ¨å¤šæ¨¡æ€æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡è¾¾åˆ°äº†æ–°çš„é«˜åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LaViçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬å®æ—¶å¤šæ¨¡æ€æ¨ç†ã€å›¾åƒä¸æ–‡æœ¬çš„äº¤äº’ç†è§£ã€ä»¥åŠè§†é¢‘å†…å®¹åˆ†æç­‰ã€‚å…¶é«˜æ•ˆçš„ç‰¹å¾è°ƒåˆ¶æœºåˆ¶ä½¿å¾—åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œæ¨åŠ¨äº†æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç­‰æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the impressive advancements of Large Vision-Language Models (LVLMs), existing approaches suffer from a fundamental bottleneck: inefficient visual-language integration. Current methods either disrupt the model's inherent structure or introduce severe long-context computational burden, severely limiting scalability and efficiency. In this paper, we rethink multimodal integration and present LaVi, a novel LVLM that enables seamless and efficient vision-language fusion through internal feature modulation within the Large Language Models (LLMs). Unlike dominant LVLMs that rely on visual token concatenation, LaVi bypasses long-context expansion by introducing a lightweight and adaptive transformation, which incorporates visual context by injecting token-wise vision-conditioned deltas into the affine parameters of layer normalization. This mechanism directly modulates linguistic hidden states based on visual input, ensuring precise vision-language alignment while preserving the LLM's linguistic priors and drastically reducing computational costs. Extensive evaluations across 15 image and video benchmarks demonstrate that LaVi not only achieves state-of-the-art multimodal performance but also dramatically enhances efficiency. Compared to LLaVA-OV-7B, LaVi reduces FLOPs by 94.0%, improves inference speed by 3.1 times, and cuts memory usage in half - establishing LaVi as a scalable and practical solution for real-time multimodal reasoning. The code and models will be released soon.

