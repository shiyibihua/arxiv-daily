---
layout: default
title: Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes
---

# Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16805" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16805v3</a>
  <a href="https://arxiv.org/pdf/2506.16805.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16805v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16805v3', 'Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chao Chen, Nobel Dang, Juexiao Zhang, Wenkai Sun, Pengfei Zheng, Xuhang He, Yimeng Ye, Jiasheng Zhang, Taarun Srinivas, Chen Feng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-08-10)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://ai4ce.github.io/CoVISION)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCo-VisiONåŸºå‡†ä»¥è§£å†³ç¨€ç–å›¾åƒé›†ä¸­çš„å…±è§†æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `å…±è§†æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `ç¨€ç–å›¾åƒ` `å¤šè§†å›¾èåˆ` `å®¤å†…åœºæ™¯` `è®¤çŸ¥æ¨ç†` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æ¨¡å‹åœ¨ç¨€ç–å›¾åƒæ¡ä»¶ä¸‹çš„å…±è§†æ€§æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•è¾¾åˆ°äººç±»çš„è®¤çŸ¥æ°´å¹³ã€‚
2. æå‡ºCo-VisiONåŸºå‡†ï¼Œè¯„ä¼°äººç±»å¯å‘çš„å…±è§†æ¨ç†ï¼Œè®¾è®¡äº†Covisæ¨¡å‹ä»¥æå‡æ€§èƒ½ã€‚
3. Covisæ¨¡å‹åœ¨çº¯è§†è§‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œç¼©å°äº†ä¸ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å·®è·ï¼Œæ¨åŠ¨äº†ç ”ç©¶è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åœ¨å¤æ‚åœºæ™¯ä¸­è¯†åˆ«å¤šä¸ªå›¾åƒåŒæ—¶å¯è§çš„3DåŒºåŸŸçš„èƒ½åŠ›ç§°ä¸ºå…±è§†æ€§ï¼Œè¿™ä¸€èƒ½åŠ›å¯¹3Dè§†è§‰å’Œæœºå™¨äººæ„ŸçŸ¥è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†Co-VisiONåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°äººç±»å¯å‘çš„å…±è§†æ¨ç†èƒ½åŠ›ï¼Œè¦†ç›–1000å¤šä¸ªç¨€ç–è§†å›¾çš„å®¤å†…åœºæ™¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å…±è§†æ€§é€šå¸¸è¢«è§†ä¸ºä½çº§ç‰¹å¾åŒ¹é…ä»»åŠ¡ï¼Œä½†åœ¨ç¨€ç–æ¡ä»¶ä¸‹ï¼Œç°æœ‰è§†è§‰æ¨¡å‹çš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºçš„Covisæ¨¡å‹åœ¨çº¯è§†è§‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œç¼©å°äº†ä¸ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å·®è·ï¼ŒæœŸæœ›æ¨åŠ¨è§†è§‰æ¨¡å‹åœ¨ç¨€ç–ç¯å¢ƒä¸­å®ç°æ›´å¼ºçš„è®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç¨€ç–å›¾åƒé›†ä¸­è¿›è¡Œå…±è§†æ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºä½çº§ç‰¹å¾åŒ¹é…ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚åœºæ™¯ä¸­çš„ç©ºé—´å…³ç³»å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCo-VisiONåŸºå‡†ä»¥è¯„ä¼°å…±è§†æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡Covisæ¨¡å‹ï¼Œæ—¨åœ¨æ¨¡ä»¿äººç±»çš„è§†è§‰è®¤çŸ¥ï¼Œé€šè¿‡æ•´åˆç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®¾è®¡å’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ¶µç›–1000å¤šä¸ªç¨€ç–è§†å›¾çš„å®¤å†…åœºæ™¯ï¼Œæ¨¡å‹åˆ™ç»“åˆäº†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCovisæ¨¡å‹æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œé‡‡ç”¨äº†å¤šè§†å›¾èåˆç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å…±è§†æ¨ç†çš„å‡†ç¡®æ€§ï¼Œä¸ä¼ ç»Ÿè§†è§‰æ¨¡å‹ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯çš„æ•´åˆã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å…±è§†æ€§æ¨ç†æ•ˆæœï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå¯¹ç©ºé—´å…³ç³»çš„ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCovisæ¨¡å‹åœ¨çº¯è§†è§‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºæ‰€æœ‰è§†è§‰åŸºçº¿ï¼Œç¼©å°äº†ä¸ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å·®è·ï¼Œå±•ç¤ºäº†åœ¨ç¨€ç–æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®¤å†…å¯¼èˆªã€å¢å¼ºç°å®å’Œæœºå™¨äººè§†è§‰ç­‰ã€‚é€šè¿‡æå‡è§†è§‰æ¨¡å‹çš„å…±è§†æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´æ™ºèƒ½çš„å†³ç­–å’Œäº¤äº’ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans exhibit a remarkable ability to recognize co-visibility-the 3D regions simultaneously visible in multiple images-even when these images are sparsely distributed across a complex scene. This ability is foundational to 3D vision, robotic perception, and relies not only on low-level feature matching but also on high-level spatial reasoning and cognitive integration. Yet, it remains unclear whether current vision models can replicate this human-level proficiency. In this work, we introduce the Co-VisiON benchmark, designed to evaluate human-inspired co-visibility reasoning across more than 1,000 sparse-view indoor scenarios. Our results show that while co-visibility is often approached as a low-level feature-matching task, it remains challenging for existing vision models under sparse conditions. Notably, a proprietary vision-language model surpasses all vision-only baselines, but all models fall significantly short of human performance. This gap underscores the limitations of current architectures and motivates the need for models that integrate spatial and semantic information in a human-like manner. Inspired by human visual cognition, we propose a novel multi-view baseline, Covis, which achieves top performance among pure vision models and narrows the gap to the proprietary VLM. We hope our benchmark and findings will spur further advancements in developing vision models capable of robust, cognitively inspired reasoning in challenging, sparse environments. Our dataset and source code can be found at https://ai4ce.github.io/CoVISION.

