---
layout: default
title: Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge
---

# Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16673" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16673v1</a>
  <a href="https://arxiv.org/pdf/2506.16673.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16673v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16673v1', 'Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruiming Chen, Junming Yang, Shiyu Xia, Xu Yang, Jing Wang, Xin Geng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMM-LGä»¥é«˜æ•ˆæå–CLIPä¸­çš„å¤šæ¨¡æ€å¯æ³›åŒ–çŸ¥è¯†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¯æ³›åŒ–çŸ¥è¯†` `CLIP` `æ¨¡å‹åˆå§‹åŒ–` `é«˜æ•ˆéƒ¨ç½²` `è®¡ç®—æœºè§†è§‰` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Learngeneæ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€åœºæ™¯ä¸­çš„å¯æ³›åŒ–çŸ¥è¯†ï¼Œå¯¼è‡´åœ¨ä¸åŒè§„æ¨¡çš„CLIPé¢„è®­ç»ƒä¸­é¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºMM-LGæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å’Œå•æ¨¡æ€æ¨¡å—åŠ æƒæå–å¯æ³›åŒ–çŸ¥è¯†ï¼Œåˆå§‹åŒ–ä¸åŒè§„æ¨¡å’Œæ¨¡æ€çš„åä»£æ¨¡å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMM-LGåœ¨Oxford-IIIT PETå’ŒFlickr30kæ•°æ®é›†ä¸Šåˆ†åˆ«æå‡äº†3.1%å’Œ4.13%çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†çº¦25%çš„å‚æ•°å­˜å‚¨å’Œ2.8å€çš„é¢„è®­ç»ƒæˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

CLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰å› å…¶å¤šæ¨¡æ€å¯æ³›åŒ–çŸ¥è¯†è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œè¿™å¯¹ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§é‡å‚æ•°å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒçš„è®¡ç®—å¼€é”€ä½¿å¾—é¢„è®­ç»ƒä¸åŒè§„æ¨¡çš„CLIPé¢ä¸´æŒ‘æˆ˜ã€‚Learngeneä»ç¥–å…ˆæ¨¡å‹ä¸­æå–å¯æ³›åŒ–ç»„ä»¶å¹¶åˆå§‹åŒ–å¤šæ ·çš„åä»£æ¨¡å‹ï¼Œä½†ç°æœ‰çš„LearngeneèŒƒå¼æœªèƒ½å¤„ç†å¤šæ¨¡æ€åœºæ™¯ä¸­çš„å¯æ³›åŒ–çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤šæ¨¡æ€æ¨¡å—æå–å¤šæ¨¡æ€å¯æ³›åŒ–çŸ¥è¯†ï¼Œè¿›è€Œæå‡ºMM-LGï¼ˆå¤šæ¨¡æ€Learngeneï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä»CLIPä¸­æå–å’Œåˆ©ç”¨å¯æ³›åŒ–ç»„ä»¶ã€‚å®éªŒè¡¨æ˜ï¼ŒMM-LGåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸”æ˜¾è‘—é™ä½äº†é¢„è®­ç»ƒæˆæœ¬ï¼Œé€‚åˆé«˜æ•ˆéƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰Learngeneæ–¹æ³•åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­æ— æ³•æœ‰æ•ˆæå–å¯æ³›åŒ–çŸ¥è¯†çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒè§„æ¨¡çš„CLIPé¢„è®­ç»ƒä¸­é¢ä¸´çš„è®¡ç®—å¼€é”€æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMM-LGæ¡†æ¶ï¼Œé€šè¿‡å»ºç«‹å¤šæ¨¡æ€å’Œå•æ¨¡æ€æ¨¡å—ï¼Œé‡‡ç”¨åŠ æƒæ±‚å’Œçš„æ–¹å¼æå–å¯æ³›åŒ–çŸ¥è¯†ï¼Œä»è€Œä¸ºä¸åŒè§„æ¨¡å’Œæ¨¡æ€çš„åä»£æ¨¡å‹æä¾›åˆå§‹åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMM-LGæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šæ¨¡æ€æ¨¡å—å’Œå•æ¨¡æ€æ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºæå–å¤šæ¨¡æ€å’Œå•æ¨¡æ€çš„å¯æ³›åŒ–çŸ¥è¯†ã€‚é€šè¿‡åŠ æƒæ±‚å’Œçš„æ–¹å¼æ•´åˆè¿™äº›çŸ¥è¯†ï¼Œå½¢æˆå¯¹åä»£æ¨¡å‹çš„æœ‰æ•ˆåˆå§‹åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMM-LGçš„åˆ›æ–°åœ¨äºå¼•å…¥å¤šæ¨¡æ€æ¨¡å—ä»¥æå–å¤šæ¨¡æ€å¯æ³›åŒ–çŸ¥è¯†ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­çš„ä¸è¶³ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMM-LGä»…éœ€çº¦25%çš„å‚æ•°å­˜å‚¨ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œé™ä½äº†é¢„è®­ç»ƒæˆæœ¬ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMM-LGåœ¨Oxford-IIIT PETå’ŒFlickr30kæ•°æ®é›†ä¸Šåˆ†åˆ«æå‡äº†3.1%å’Œ4.13%çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„é¢„è®­ç»ƒå’Œå¾®è°ƒæ–¹æ³•ï¼ŒMM-LGåœ¨æ€§èƒ½ä¸Šä¹Ÿæœ‰1.9%å’Œ3.65%çš„æå‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†é¢„è®­ç»ƒæˆæœ¬ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†åŠå…¶äº¤å‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨¡å‹éƒ¨ç½²çš„åœºæ™¯ä¸­ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€æ–‡æœ¬ç”Ÿæˆå’Œå¤šæ¨¡æ€æ£€ç´¢ç­‰ã€‚æœªæ¥ï¼ŒMM-LGæœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€å­¦ä¹ æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ï¼Œæå‡æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> CLIP (Contrastive Language-Image Pre-training) has attracted widespread attention for its multimodal generalizable knowledge, which is significant for downstream tasks. However, the computational overhead of a large number of parameters and large-scale pre-training poses challenges of pre-training a different scale of CLIP. Learngene extracts the generalizable components termed as learngene from an ancestry model and initializes diverse descendant models with it. Previous Learngene paradigms fail to handle the generalizable knowledge in multimodal scenarios. In this paper, we put forward the idea of utilizing a multimodal block to extract the multimodal generalizable knowledge, which inspires us to propose MM-LG (Multimodal Learngene), a novel framework designed to extract and leverage generalizable components from CLIP. Specifically, we first establish multimodal and unimodal blocks to extract the multimodal and unimodal generalizable knowledge in a weighted-sum manner. Subsequently, we employ these components to numerically initialize descendant models of varying scales and modalities. Extensive experiments demonstrate MM-LG's effectiveness, which achieves performance gains over existing learngene approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and comparable or superior results to the pre-training and fine-tuning paradigm (e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG requires only around 25% of the parameter storage while reducing around 2.8 times pre-training costs for diverse model scales compared to the pre-training and fine-tuning paradigm, making it particularly suitable for efficient deployment across diverse downstream tasks.

