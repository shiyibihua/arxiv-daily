---
layout: default
title: MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation
---

# MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17113" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17113v2</a>
  <a href="https://arxiv.org/pdf/2506.17113.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17113v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17113v2', 'MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon, Mohit Bansal

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: EMNLP 2025 Findings; The first two authors contributed equally; Github link: https://github.com/Yui010206/MEXA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMEXAä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¸“å®¶æ¨¡å‹èšåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `ä¸“å®¶æ¨¡å‹` `åŠ¨æ€èšåˆ` `å¤§å‹æ¨ç†æ¨¡å‹` `åŒ»ç–—è¯Šæ–­` `é‡‘èé¢„æµ‹` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­éš¾ä»¥æœ‰æ•ˆèšåˆä¸åŒä¸“å®¶æ¨¡å‹ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœå—é™ã€‚
2. MEXAé€šè¿‡åŠ¨æ€é€‰æ‹©ä¸“å®¶æ¨¡å‹å¹¶è¿›è¡Œæ¨¡æ€å’Œä»»åŠ¡æ„ŸçŸ¥çš„èšåˆï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„èšåˆé—®é¢˜ã€‚
3. åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMEXAç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»“åˆé¢„è®­ç»ƒä¸“å®¶æ¨¡å‹ä¸ºå¯æ‰©å±•çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†é‡è¦æ½œåŠ›ï¼Œä½†ç”±äºè¾“å…¥æ¨¡æ€å’Œä»»åŠ¡å¤æ‚æ€§çš„å¤šæ ·æ€§ï¼Œæ„å»ºç»Ÿä¸€æ¡†æ¶ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MEXAï¼Œä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ¨¡æ€å’Œä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©å’Œèšåˆå¤šä¸ªä¸“å®¶æ¨¡å‹ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ã€‚MEXAé€šè¿‡å¤§å‹æ¨ç†æ¨¡å‹å¯¹ä¸“å®¶æ¨¡å‹ç”Ÿæˆçš„å¯è§£é‡Šæ–‡æœ¬æ¨ç†è¾“å‡ºè¿›è¡Œèšåˆï¼Œæœ€ç»ˆç”Ÿæˆç­”æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒMEXAåœ¨è§†é¢‘æ¨ç†ã€éŸ³é¢‘æ¨ç†ã€3Dç†è§£å’ŒåŒ»å­¦é—®ç­”ç­‰å¤šç§åŸºå‡†ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­ä¸“å®¶æ¨¡å‹èšåˆçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–è¾“å…¥æ¨¡æ€å’Œå¤æ‚ä»»åŠ¡æ—¶æ•ˆæœä¸ä½³ï¼Œéš¾ä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMEXAçš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®è¾“å…¥æ¨¡æ€å’Œä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©ä¸“å®¶æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¾“å‡ºè¿›è¡Œèšåˆï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ã€‚è¯¥è®¾è®¡é¿å…äº†é¢å¤–çš„è®­ç»ƒå¼€é”€ï¼Œæå‡äº†æ¨ç†çš„çµæ´»æ€§å’Œé€æ˜æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMEXAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªä¸“å®¶æ¨¡å‹å’Œä¸€ä¸ªå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ã€‚ä¸“å®¶æ¨¡å‹é’ˆå¯¹ç‰¹å®šæ¨¡æ€å’Œä»»åŠ¡å¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ–‡æœ¬æ¨ç†è¾“å‡ºï¼ŒLRMåˆ™è´Ÿè´£å¯¹è¿™äº›è¾“å‡ºè¿›è¡Œèšåˆå’Œæ¨ç†ï¼Œæœ€ç»ˆç”Ÿæˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šMEXAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€é€‰æ‹©å’Œèšåˆæœºåˆ¶ï¼Œä½¿å¾—ä¸åŒæ¨¡æ€å’Œä»»åŠ¡çš„æ¨ç†è¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œé€æ˜ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é™æ€æ¨¡å‹èšåˆæ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤šæ ·åŒ–çš„è¾“å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šMEXAåœ¨è®¾è®¡ä¸Šä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬ä¸“å®¶æ¨¡å‹çš„é€‰æ‹©æ ‡å‡†å’Œèšåˆç­–ç•¥ï¼Œç¡®ä¿äº†è¾“å‡ºçš„å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™ç§æ¨¡å—åŒ–è®¾è®¡ï¼ŒMEXAèƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸä¸­çµæ´»åº”ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMEXAç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è€Œè¨€ï¼Œåœ¨è§†é¢‘æ¨ç†å’ŒåŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MEXAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬åŒ»ç–—è¯Šæ–­ã€é‡‘èé¢„æµ‹å’Œå¤šåª’ä½“å†…å®¹åˆ†æç­‰ã€‚é€šè¿‡å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ï¼ŒMEXAèƒ½å¤Ÿå¸®åŠ©ä¸“ä¸šäººå£«æ›´å¥½åœ°ç†è§£å¤æ‚æ•°æ®ï¼Œä»è€Œåšå‡ºæ›´ä¸ºå‡†ç¡®çš„å†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–åˆ†æå’Œå†³ç­–æ”¯æŒç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.

