---
layout: default
title: VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning
---

# VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17221" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17221v2</a>
  <a href="https://arxiv.org/pdf/2506.17221.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17221v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17221v2', 'VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-06-25)

**å¤‡æ³¨**: project page: vlnr1.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLN-R1ä»¥è§£å†³è§†è§‰-è¯­è¨€å¯¼èˆªä¸­çš„è·¯å¾„è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€å¯¼èˆª` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹` `è·¯å¾„è§„åˆ’` `é•¿çŸ­æœŸè®°å¿†é‡‡æ ·` `å…·èº«äººå·¥æ™ºèƒ½` `æ•°æ®é«˜æ•ˆè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹å¯¼èˆªç³»ç»Ÿä»…åœ¨ç¦»æ•£æ‹“æ‰‘å›¾ä¸Šæ“ä½œï¼Œé™åˆ¶äº†è·¯å¾„è§„åˆ’çš„çµæ´»æ€§å’Œå®æ—¶æ€§ã€‚
2. VLN-R1æ¡†æ¶é€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç›´æ¥å°†è§†é¢‘æµè½¬æ¢ä¸ºè¿ç»­å¯¼èˆªåŠ¨ä½œï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVLN-R1åœ¨VLN-CEåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶åœ¨å…·èº«å¯¼èˆªä¸­çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯å…·èº«äººå·¥æ™ºèƒ½ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¦æ±‚æ™ºèƒ½ä½“ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨ç°å®ç¯å¢ƒä¸­å¯¼èˆªã€‚ç›®å‰åŸºäºè¯­è¨€æ¨¡å‹çš„å¯¼èˆªç³»ç»Ÿä»…åœ¨ç¦»æ•£æ‹“æ‰‘å›¾ä¸Šæ“ä½œï¼Œé™åˆ¶äº†è·¯å¾„è§„åˆ’çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†VLN-R1ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ç›´æ¥å°†è‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘æµè½¬æ¢ä¸ºè¿ç»­å¯¼èˆªåŠ¨ä½œï¼Œå¹¶é‡‡ç”¨å—DeepSeek-R1å¯å‘çš„GRPOè®­ç»ƒæ–¹æ³•ã€‚ä¸ºäº†æœ‰æ•ˆè®­ç»ƒï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨3Dæ¨¡æ‹Ÿå™¨Habitatæ„å»ºäº†VLN-Egoæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†é•¿çŸ­æœŸè®°å¿†é‡‡æ ·æ–¹æ³•ï¼Œä»¥å¹³è¡¡å†å²å’Œå½“å‰è§‚å¯Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLN-R1åœ¨VLN-CEåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜LVLMå¯ä»¥é©±åŠ¨å…·èº«å¯¼èˆªå¹¶é€šè¿‡æ•°æ®é«˜æ•ˆã€å¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒå¢å¼ºä»»åŠ¡ç‰¹å®šæ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€å¯¼èˆªç³»ç»Ÿåœ¨ç¦»æ•£æ‹“æ‰‘å›¾ä¸Šæ“ä½œçš„å±€é™æ€§ï¼Œå¯¼è‡´è·¯å¾„è§„åˆ’ä¸å¤Ÿçµæ´»å’Œå®æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLN-R1é€šè¿‡ç›´æ¥å°†è‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘æµè½¬æ¢ä¸ºè¿ç»­å¯¼èˆªåŠ¨ä½œï¼Œé‡‡ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°æ›´è‡ªç„¶çš„å¯¼èˆªæ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLN-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚SFTé˜¶æ®µå¯¹æ¨¡å‹çš„åŠ¨ä½œåºåˆ—æ–‡æœ¬é¢„æµ‹è¿›è¡Œå¯¹é½ï¼Œè€ŒRFTé˜¶æ®µåˆ™å¼•å…¥æ—¶é—´è¡°å‡å¥–åŠ±æœºåˆ¶ä»¥ä¼˜åŒ–å¤šæ­¥æœªæ¥åŠ¨ä½œçš„æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é•¿çŸ­æœŸè®°å¿†é‡‡æ ·æ–¹æ³•ï¼Œä»¥å¹³è¡¡å†å²å’Œå½“å‰è§‚å¯Ÿï¼Œä»è€Œæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œå¯¼èˆªç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†GRPOè®­ç»ƒæ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†æ—¶é—´è¡°å‡å¥–åŠ±æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹æœªæ¥åŠ¨ä½œçš„é¢„æµ‹èƒ½åŠ›ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒVLN-R1èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯å’Œå½“å‰çŠ¶æ€è¿›è¡Œå†³ç­–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVLN-R1åœ¨VLN-CEåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚å¯¼èˆªä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒVLN-R1æœ‰æœ›åœ¨å…·èº«äººå·¥æ™ºèƒ½é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.

