---
layout: default
title: With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You
---

# With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16895" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16895v2</a>
  <a href="https://arxiv.org/pdf/2506.16895.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16895v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16895v2', 'With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fabian GrÃ¶ger, Shuo Wen, Huyen Le, Maria BrbiÄ‡

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-10-22)

**å¤‡æ³¨**: NeurIPS 2025 camera-ready

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTRUCTUREä»¥è§£å†³å¤šæ¨¡æ€å¯¹é½ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹é½` `æœ‰é™æ ·æœ¬å­¦ä¹ ` `STRUCTUREæŠ€æœ¯` `é¢„è®­ç»ƒæ¨¡å‹` `å‡ ä½•ç»“æ„ä¿æŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡é…å¯¹æ ·æœ¬ï¼Œè·å–æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™é¢†åŸŸçš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºSTRUCTUREæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ä¿æŒå•æ¨¡æ€ç¼–ç å™¨çš„é‚»åŸŸå‡ ä½•ç»“æ„ï¼Œå®ç°æœ‰é™æ ·æœ¬ä¸‹çš„é«˜æ•ˆå¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†51.6%å’Œ91.8%çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ¨¡å‹åœ¨éœ€è¦å¤šæ¨¡æ€å¯¹é½çš„å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–äºæ•°ç™¾ä¸‡å¯¹çš„å¤šæ¨¡æ€æ ·æœ¬ï¼Œè¿™åœ¨è®¸å¤šé¢†åŸŸæ˜¯ä¸å¯è¡Œçš„ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨æœ‰é™é…å¯¹æ•°æ®ä¸‹æ„å»ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œå±•ç¤ºäº†åœ¨ä»…æœ‰æ•°ä¸‡å¯¹æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡å¯¹é½çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æŠ€æœ¯STRUCTUREï¼Œä»¥ä¿æŒå•æ¨¡æ€ç¼–ç å™¨æ½œåœ¨ç©ºé—´çš„é‚»åŸŸå‡ ä½•ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜å¯¹é½æœ€åä¸€å±‚é€šå¸¸ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå±•ç¤ºäº†å¯¹é½å…·æœ‰æœ€é«˜è¡¨ç¤ºç›¸ä¼¼æ€§çš„å±‚çš„å¥½å¤„ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨24ä¸ªé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œåˆ†ç±»ä»»åŠ¡å¹³å‡æå‡51.6%ï¼Œæ£€ç´¢ä»»åŠ¡å¹³å‡æå‡91.8%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šæ¨¡æ€å¯¹é½ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡é…å¯¹æ ·æœ¬çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦æ•°ç™¾ä¸‡å¯¹æ ·æœ¬ï¼Œè·å–è¿™äº›æ•°æ®åœ¨è®¸å¤šåº”ç”¨åœºæ™¯ä¸­æ˜¯ä¸åˆ‡å®é™…çš„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¯¹é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œåˆ©ç”¨STRUCTUREæ­£åˆ™åŒ–æŠ€æœ¯æ¥ä¿æŒæ½œåœ¨ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œä»è€Œåœ¨æœ‰é™çš„é…å¯¹æ•°æ®ä¸‹å®ç°é«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯é€šè¿‡STRUCTUREæŠ€æœ¯è¿›è¡Œå•æ¨¡æ€ç¼–ç å™¨çš„å¯¹é½ï¼ŒäºŒæ˜¯ä¼˜åŒ–å¯¹é½å±‚çš„é€‰æ‹©ï¼Œä¼˜å…ˆå¯¹é½è¡¨ç¤ºç›¸ä¼¼æ€§æœ€é«˜çš„å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥STRUCTUREæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿æŒå•æ¨¡æ€ç¼–ç å™¨çš„é‚»åŸŸå‡ ä½•ç»“æ„ï¼Œå¹¶ä¸”é€šè¿‡å¯¹é½é«˜ç›¸ä¼¼æ€§å±‚è€Œéæœ€åä¸€å±‚ï¼Œæ˜¾è‘—æå‡å¯¹é½æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSTRUCTUREæ­£åˆ™åŒ–çš„å…·ä½“å®ç°æ–¹å¼å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œç¡®ä¿äº†å¯¹é½è¿‡ç¨‹ä¸­çš„å‡ ä½•ç»“æ„ä¿æŒï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé€‰æ‹©åˆé€‚çš„å±‚è¿›è¡Œå¯¹é½ä»¥æé«˜è¡¨ç¤ºèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨STRUCTUREæŠ€æœ¯åï¼Œåœ¨24ä¸ªé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†ä¸Šï¼Œåˆ†ç±»ä»»åŠ¡å¹³å‡æå‡51.6%ï¼Œæ£€ç´¢ä»»åŠ¡å¹³å‡æå‡91.8%ã€‚è¿™äº›æ˜¾è‘—çš„æ€§èƒ½æå‡è¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰éœ€è¦å¤šæ¨¡æ€æ•°æ®èåˆçš„åœºæ™¯ã€‚é€šè¿‡åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ï¼Œèƒ½å¤Ÿé™ä½æ¨¡å‹è®­ç»ƒçš„æˆæœ¬ï¼Œæé«˜åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ä»·å€¼ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment, including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\unicode{x2013}$less than $1\%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.

