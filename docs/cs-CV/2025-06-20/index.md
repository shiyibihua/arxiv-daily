---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-20
---

# cs.CVï¼ˆ2025-06-20ï¼‰

ğŸ“Š å…± **20** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250616895v2-with-limited-data-for-multimodal-alignment-let-the-structure-guide-y.html">With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</a></td>
  <td>æå‡ºSTRUCTUREä»¥è§£å†³å¤šæ¨¡æ€å¯¹é½ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16895v2" data-paper-url="./papers/250616895v2-with-limited-data-for-multimodal-alignment-let-the-structure-guide-y.html" onclick="toggleFavorite(this, '2506.16895v2', 'With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250617457v1-when-every-millisecond-counts-real-time-anomaly-detection-via-the-mu.html">When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å¼‚æ­¥æ··åˆç½‘ç»œä»¥è§£å†³å®æ—¶å¼‚å¸¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17457v1" data-paper-url="./papers/250617457v1-when-every-millisecond-counts-real-time-anomaly-detection-via-the-mu.html" onclick="toggleFavorite(this, '2506.17457v1', 'When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250617113v2-mexa-towards-general-multimodal-reasoning-with-dynamic-multi-expert-.html">MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation</a></td>
  <td>æå‡ºMEXAä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¸“å®¶æ¨¡å‹èšåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17113v2" data-paper-url="./papers/250617113v2-mexa-towards-general-multimodal-reasoning-with-dynamic-multi-expert-.html" onclick="toggleFavorite(this, '2506.17113v2', 'MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250616673v1-extracting-multimodal-learngene-in-clip-unveiling-the-multimodal-gen.html">Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge</a></td>
  <td>æå‡ºMM-LGä»¥é«˜æ•ˆæå–CLIPä¸­çš„å¤šæ¨¡æ€å¯æ³›åŒ–çŸ¥è¯†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16673v1" data-paper-url="./papers/250616673v1-extracting-multimodal-learngene-in-clip-unveiling-the-multimodal-gen.html" onclick="toggleFavorite(this, '2506.16673v1', 'Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250616691v1-lavi-efficient-large-vision-language-models-via-internal-feature-mod.html">LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation</a></td>
  <td>æå‡ºLaViä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16691v1" data-paper-url="./papers/250616691v1-lavi-efficient-large-vision-language-models-via-internal-feature-mod.html" onclick="toggleFavorite(this, '2506.16691v1', 'LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250617144v2-do-we-need-large-vlms-for-spotting-soccer-actions.html">Do We Need Large VLMs for Spotting Soccer Actions?</a></td>
  <td>æå‡ºåŸºäºè¯­è¨€æ¨¡å‹çš„è¶³çƒåŠ¨ä½œè¯†åˆ«æ–¹æ³•ä»¥æ›¿ä»£è§†é¢‘å¤„ç†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17144v2" data-paper-url="./papers/250617144v2-do-we-need-large-vlms-for-spotting-soccer-actions.html" onclick="toggleFavorite(this, '2506.17144v2', 'Do We Need Large VLMs for Spotting Soccer Actions?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250617101v3-multi-label-scene-classification-for-autonomous-vehicles-acquiring-a.html">Multi-label Scene Classification for Autonomous Vehicles: Acquiring and Accumulating Knowledge from Diverse Datasets</a></td>
  <td>æå‡ºKAA-CALä»¥è§£å†³è‡ªåŠ¨é©¾é©¶åœºæ™¯å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17101v3" data-paper-url="./papers/250617101v3-multi-label-scene-classification-for-autonomous-vehicles-acquiring-a.html" onclick="toggleFavorite(this, '2506.17101v3', 'Multi-label Scene Classification for Autonomous Vehicles: Acquiring and Accumulating Knowledge from Diverse Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250617212v1-part2gs-part-aware-modeling-of-articulated-objects-using-3d-gaussian.html">Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting</a></td>
  <td>æå‡ºPartÂ²GSä»¥è§£å†³å…³èŠ‚ç‰©ä½“å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17212v1" data-paper-url="./papers/250617212v1-part2gs-part-aware-modeling-of-articulated-objects-using-3d-gaussian.html" onclick="toggleFavorite(this, '2506.17212v1', 'Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250616690v2-depthvanish-optimizing-adversarial-interval-structures-for-stereo-de.html">DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches</a></td>
  <td>æå‡ºDepthVanishä»¥ä¼˜åŒ–ç«‹ä½“æ·±åº¦ä¼°è®¡ä¸­çš„å¯¹æŠ—æ€§è¡¥ä¸</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">stereo depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16690v2" data-paper-url="./papers/250616690v2-depthvanish-optimizing-adversarial-interval-structures-for-stereo-de.html" onclick="toggleFavorite(this, '2506.16690v2', 'DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250617119v1-rgbtrack-fast-robust-depth-free-6d-pose-estimation-and-tracking.html">RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking</a></td>
  <td>æå‡ºRGBTrackä»¥è§£å†³å®æ—¶6Då§¿æ€ä¼°è®¡ä¸è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17119v1" data-paper-url="./papers/250617119v1-rgbtrack-fast-robust-depth-free-6d-pose-estimation-and-tracking.html" onclick="toggleFavorite(this, '2506.17119v1', 'RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250616826v1-anytraverse-an-off-road-traversability-framework-with-vlm-and-human-.html">AnyTraverse: An off-road traversability framework with VLM and human operator in the loop</a></td>
  <td>æå‡ºAnyTraverseæ¡†æ¶ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„è¶Šé‡å¯é€šè¡Œæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">traversability</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16826v1" data-paper-url="./papers/250616826v1-anytraverse-an-off-road-traversability-framework-with-vlm-and-human-.html" onclick="toggleFavorite(this, '2506.16826v1', 'AnyTraverse: An off-road traversability framework with VLM and human operator in the loop')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250616940v1-lunarloc-segment-based-global-localization-on-the-moon.html">LunarLoc: Segment-Based Global Localization on the Moon</a></td>
  <td>æå‡ºLunarLocä»¥è§£å†³æœˆçƒè¡¨é¢å…¨çƒå®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VIO</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16940v1" data-paper-url="./papers/250616940v1-lunarloc-segment-based-global-localization-on-the-moon.html" onclick="toggleFavorite(this, '2506.16940v1', 'LunarLoc: Segment-Based Global Localization on the Moon')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250616962v2-chiron-o1-igniting-multimodal-large-language-models-towards-generali.html">Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search</a></td>
  <td>æå‡ºMICSä»¥è§£å†³åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">curriculum learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16962v2" data-paper-url="./papers/250616962v2-chiron-o1-igniting-multimodal-large-language-models-towards-generali.html" onclick="toggleFavorite(this, '2506.16962v2', 'Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250616796v2-realsr-r1-reinforcement-learning-for-real-world-image-super-resoluti.html">RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought</a></td>
  <td>æå‡ºRealSR-R1ä»¥è§£å†³çœŸå®åœºæ™¯å›¾åƒè¶…åˆ†è¾¨ç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16796v2" data-paper-url="./papers/250616796v2-realsr-r1-reinforcement-learning-for-real-world-image-super-resoluti.html" onclick="toggleFavorite(this, '2506.16796v2', 'RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250617202v1-unifork-exploring-modality-alignment-for-unified-multimodal-understa.html">UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation</a></td>
  <td>æå‡ºUniForkä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸­çš„ä»»åŠ¡å¹²æ‰°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17202v1" data-paper-url="./papers/250617202v1-unifork-exploring-modality-alignment-for-unified-multimodal-understa.html" onclick="toggleFavorite(this, '2506.17202v1', 'UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250617221v2-vln-r1-vision-language-navigation-via-reinforcement-fine-tuning.html">VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning</a></td>
  <td>æå‡ºVLN-R1ä»¥è§£å†³è§†è§‰-è¯­è¨€å¯¼èˆªä¸­çš„è·¯å¾„è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17221v2" data-paper-url="./papers/250617221v2-vln-r1-vision-language-navigation-via-reinforcement-fine-tuning.html" onclick="toggleFavorite(this, '2506.17221v2', 'VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250617505v1-learning-golf-swing-signatures-from-a-single-wrist-worn-inertial-sen.html">Learning golf swing signatures from a single wrist-worn inertial sensor</a></td>
  <td>æå‡ºåŸºäºå•ä¸ªè…•éƒ¨ä¼ æ„Ÿå™¨çš„é«˜å°”å¤«æŒ¥æ†åˆ†ææ¡†æ¶ä»¥è§£å†³ç°æœ‰æ–¹æ³•ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">human mesh recovery</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17505v1" data-paper-url="./papers/250617505v1-learning-golf-swing-signatures-from-a-single-wrist-worn-inertial-sen.html" onclick="toggleFavorite(this, '2506.17505v1', 'Learning golf swing signatures from a single wrist-worn inertial sensor')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250616805v3-co-vision-co-visibility-reasoning-on-sparse-image-sets-of-indoor-sce.html">Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes</a></td>
  <td>æå‡ºCo-VisiONåŸºå‡†ä»¥è§£å†³ç¨€ç–å›¾åƒé›†ä¸­çš„å…±è§†æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16805v3" data-paper-url="./papers/250616805v3-co-vision-co-visibility-reasoning-on-sparse-image-sets-of-indoor-sce.html" onclick="toggleFavorite(this, '2506.16805v3', 'Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250617218v1-machine-mental-imagery-empower-multimodal-reasoning-with-latent-visu.html">Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens</a></td>
  <td>æå‡ºæœºå™¨å¿ƒç†æ„è±¡æ¡†æ¶ä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.17218v1" data-paper-url="./papers/250617218v1-machine-mental-imagery-empower-multimodal-reasoning-with-latent-visu.html" onclick="toggleFavorite(this, '2506.17218v1', 'Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250616821v1-self-supervised-feature-extraction-for-enhanced-ball-detection-on-so.html">Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots</a></td>
  <td>æå‡ºè‡ªç›‘ç£ç‰¹å¾æå–æ–¹æ³•ä»¥å¢å¼ºè¶³çƒæœºå™¨äººä¸­çš„çƒæ£€æµ‹èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.16821v1" data-paper-url="./papers/250616821v1-self-supervised-feature-extraction-for-enhanced-ball-detection-on-so.html" onclick="toggleFavorite(this, '2506.16821v1', 'Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)