---
layout: default
title: From Pixels to Words -- Towards Native Vision-Language Primitives at Scale
---

# From Pixels to Words -- Towards Native Vision-Language Primitives at Scale

**arXiv**: [2510.14979v1](https://arxiv.org/abs/2510.14979) | [PDF](https://arxiv.org/pdf/2510.14979.pdf)

**ä½œè€…**: Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNEOåŽŸç”Ÿè§†è§‰è¯­è¨€æ¨¡åž‹ä»¥è§£å†³è§†è§‰è¯­è¨€å¯¹é½ä¸Žé›†æˆé—®é¢˜**

**å…³é”®è¯**: `åŽŸç”Ÿè§†è§‰è¯­è¨€æ¨¡åž‹` `åƒç´ -è¯å¯¹é½` `è·¨æ¨¡æ€æŽ¨ç†` `æ¨¡åž‹æž¶æž„` `è§†è§‰è¯­è¨€é›†æˆ` `å¯æ‰©å±•ç”Ÿæ€ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŽŸç”Ÿè§†è§‰è¯­è¨€æ¨¡åž‹ä¸Žæ¨¡å—åŒ–æ¨¡åž‹çš„åŒºåˆ«åŠå¯è®¿é—®æ€§æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šå®šä¹‰åŽŸç”ŸVLMåŽŸåˆ™ï¼ŒåŒ…æ‹¬åƒç´ -è¯å¯¹é½ã€æ¨¡å—é›†æˆå’Œè·¨æ¨¡æ€å±žæ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šNEOæ¨¡åž‹åœ¨390Må›¾åƒ-æ–‡æœ¬æ•°æ®ä¸Šå®žçŽ°é«˜æ•ˆè§†è§‰æ„ŸçŸ¥ï¼Œåª²ç¾Žé¡¶çº§æ¨¡å—åŒ–æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The edifice of native Vision-Language Models (VLMs) has emerged as a rising
> contender to typical modular VLMs, shaped by evolving model architectures and
> training paradigms. Yet, two lingering clouds cast shadows over its widespread
> exploration and promotion: (-) What fundamental constraints set native VLMs
> apart from modular ones, and to what extent can these barriers be overcome? (-)
> How to make research in native VLMs more accessible and democratized, thereby
> accelerating progress in the field. In this paper, we clarify these challenges
> and outline guiding principles for constructing native VLMs. Specifically, one
> native VLM primitive should: (i) effectively align pixel and word
> representations within a shared semantic space; (ii) seamlessly integrate the
> strengths of formerly separate vision and language modules; (iii) inherently
> embody various cross-modal properties that support unified vision-language
> encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
> native VLMs built from first principles, capable of rivaling top-tier modular
> counterparts across diverse real-world scenarios. With only 390M image-text
> examples, NEO efficiently develops visual perception from scratch while
> mitigating vision-language conflicts inside a dense and monolithic model
> crafted from our elaborate primitives. We position NEO as a cornerstone for
> scalable and powerful native VLMs, paired with a rich set of reusable
> components that foster a cost-effective and extensible ecosystem. Our code and
> models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.

