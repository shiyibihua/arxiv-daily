---
layout: default
title: CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection
---

# CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection

**arXiv**: [2510.14792v1](https://arxiv.org/abs/2510.14792) | [PDF](https://arxiv.org/pdf/2510.14792.pdf)

**ä½œè€…**: Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoT-PLæ¡†æž¶ï¼Œç»“åˆè§†è§‰é“¾å¼æŽ¨ç†ä¸Žä¼ªæ ‡æ³¨ï¼Œæå‡æ‹¥æŒ¤æˆ–é®æŒ¡åœºæ™¯ä¸‹çš„å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹` `è§†è§‰é“¾å¼æŽ¨ç†` `ä¼ªæ ‡æ³¨` `å¯¹æ¯”èƒŒæ™¯å­¦ä¹ ` `é›¶æ ·æœ¬è¯†åˆ«` `åœºæ™¯é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–ç›´æŽ¥å›¾åƒ-æ–‡æœ¬åŒ¹é…ï¼Œå¿½ç•¥ä¸­é—´æŽ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´åœ¨æ‹¥æŒ¤æˆ–é®æŒ¡åœºæ™¯ä¸­é²æ£’æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†è§‰é“¾å¼æŽ¨ç†ï¼Œåˆ†è§£å¯¹è±¡ç†è§£ä¸ºåŒºåŸŸæ„ŸçŸ¥ã€ç±»åˆ«è¯†åˆ«å’ŒèƒŒæ™¯æŽ¥åœ°ä¸‰æ­¥ï¼Œå¹¶é›†æˆå¯¹æ¯”èƒŒæ™¯å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¼€æ”¾è¯æ±‡COCOå’ŒLVISæ•°æ®é›†ä¸Šï¼Œæ–°ç±»æ£€æµ‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä¼ªæ ‡ç­¾è´¨é‡ç›¸å¯¹æ”¹è¿›è¶…100%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Open-vocabulary object detection (OVD) seeks to recognize and localize object
> categories beyond those seen during training. Recent approaches typically
> leverage vision-language models (VLMs) to generate pseudo-labels using
> image-text alignment, allowing detectors to generalize to unseen classes
> without explicit supervision. However, these methods depend heavily on direct
> image-text matching, neglecting the intermediate reasoning steps essential for
> interpreting semantically complex scenes. This results in limited robustness
> when confronted with crowded or occluded visual contexts. In this paper, we
> introduce CoT-PL, a new framework that employs structured visual
> chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
> decomposes object understanding into three interpretable steps: (1) region
> perception even for unseen objects, (2) category recognition via zero-shot
> reasoning, and (3) background grounding to separate semantically complex
> objects. Crucially, the third step naturally motivates our contrastive
> background learning (CBL) that uses the pre-computed background cues as
> negatives to promote feature disentanglement between objects and background. In
> this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
> pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
> our novel-class pseudo-label quality achieves relative improvements of 103.4%
> and 168.4% over the best prior, respectively. Our extensive experiments
> demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
> mask AP on LVIS for novel classes, setting a new state of the art.

