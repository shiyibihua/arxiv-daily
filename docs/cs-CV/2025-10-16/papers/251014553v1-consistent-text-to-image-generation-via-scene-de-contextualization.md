---
layout: default
title: Consistent text-to-image generation via scene de-contextualization
---

# Consistent text-to-image generation via scene de-contextualization

**arXiv**: [2510.14553v1](https://arxiv.org/abs/2510.14553) | [PDF](https://arxiv.org/pdf/2510.14553.pdf)

**ä½œè€…**: Song Tang, Peihao Gong, Kunyu Li, Kai Guo, Boyu Wang, Mao Ye, Jianwei Zhang, Xiatian Zhu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœºæ™¯åŽ»ä¸Šä¸‹æ–‡åŒ–æ–¹æ³•ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„èº«ä»½åç§»é—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `èº«ä»½ä¿æŒ` `åœºæ™¯åŽ»ä¸Šä¸‹æ–‡åŒ–` `æç¤ºåµŒå…¥ç¼–è¾‘` `è®­ç»ƒæ— å…³æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­èº«ä»½åç§»æºäºŽä¸»ä½“ä¸Žåœºæ™¯ä¸Šä¸‹æ–‡çš„å†…åœ¨ç›¸å…³æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è®­ç»ƒæ— å…³çš„æç¤ºåµŒå…¥ç¼–è¾‘ï¼ŒæŠ‘åˆ¶æ½œåœ¨åœºæ™¯-èº«ä»½ç›¸å…³æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ˜¾è‘—æå‡èº«ä»½ä¿æŒï¼ŒåŒæ—¶ç»´æŒåœºæ™¯å¤šæ ·æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Consistent text-to-image (T2I) generation seeks to produce
> identity-preserving images of the same subject across diverse scenes, yet it
> often fails due to a phenomenon called identity (ID) shift. Previous methods
> have tackled this issue, but typically rely on the unrealistic assumption of
> knowing all target scenes in advance. This paper reveals that a key source of
> ID shift is the native correlation between subject and scene context, called
> scene contextualization, which arises naturally as T2I models fit the training
> distribution of vast natural images. We formally prove the near-universality of
> this scene-ID correlation and derive theoretical bounds on its strength. On
> this basis, we propose a novel, efficient, training-free prompt embedding
> editing approach, called Scene De-Contextualization (SDeC), that imposes an
> inversion process of T2I's built-in scene contextualization. Specifically, it
> identifies and suppresses the latent scene-ID correlation within the ID
> prompt's embedding by quantifying the SVD directional stability to adaptively
> re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
> use (one scene per prompt) without requiring prior access to all target scenes.
> This makes it a highly flexible and general solution well-suited to real-world
> applications where such prior knowledge is often unavailable or varies over
> time. Experiments demonstrate that SDeC significantly enhances identity
> preservation while maintaining scene diversity.

