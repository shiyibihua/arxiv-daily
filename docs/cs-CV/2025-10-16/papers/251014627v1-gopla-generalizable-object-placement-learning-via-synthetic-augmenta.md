---
layout: default
title: GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement
---

# GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement

**arXiv**: [2510.14627v1](https://arxiv.org/abs/2510.14627) | [PDF](https://arxiv.org/pdf/2510.14627.pdf)

**ä½œè€…**: Yao Zhong, Hanzhi Chen, Simon Schaefer, Anran Zhang, Stefan Leutenegger

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGOPLAæ¡†æž¶ï¼Œé€šè¿‡åˆæˆå¢žå¼ºäººç±»æ¼”ç¤ºå­¦ä¹ é€šç”¨ç‰©ä½“æ”¾ç½®ï¼Œä»¥è§£å†³æœºå™¨äººå®¶åº­ç»„ç»‡ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `ç‰©ä½“æ”¾ç½®å­¦ä¹ ` `åˆæˆæ•°æ®å¢žå¼º` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ‰©æ•£è§„åˆ’å™¨` `æœºå™¨äººè¾…åŠ©` `å‡ ä½•å¯è¡Œæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººç‰©ä½“æ”¾ç½®éœ€å…¼é¡¾è¯­ä¹‰åå¥½å’Œå‡ ä½•å¯è¡Œæ€§ï¼Œé¢ä¸´æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ç”Ÿæˆç»“æž„åŒ–è®¡åˆ’ï¼Œç»“åˆæ‰©æ•£è§„åˆ’å™¨ç”Ÿæˆæ”¾ç½®ä½å§¿ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨çœŸå®žåœºæ™¯ä¸­ï¼Œæ”¾ç½®æˆåŠŸçŽ‡æ¯”æ¬¡ä¼˜æ–¹æ³•æé«˜30.04ä¸ªç™¾åˆ†ç‚¹ï¼Œæ³›åŒ–æ€§å¼ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robots are expected to serve as intelligent assistants, helping humans with
> everyday household organization. A central challenge in this setting is the
> task of object placement, which requires reasoning about both semantic
> preferences (e.g., common-sense object relations) and geometric feasibility
> (e.g., collision avoidance). We present GOPLA, a hierarchical framework that
> learns generalizable object placement from augmented human demonstrations. A
> multi-modal large language model translates human instructions and visual
> inputs into structured plans that specify pairwise object relationships. These
> plans are then converted into 3D affordance maps with geometric common sense by
> a spatial mapper, while a diffusion-based planner generates placement poses
> guided by test-time costs, considering multi-plan distributions and collision
> avoidance. To overcome data scarcity, we introduce a scalable pipeline that
> expands human placement demonstrations into diverse synthetic training data.
> Extensive experiments show that our approach improves placement success rates
> by 30.04 percentage points over the runner-up, evaluated on positioning
> accuracy and physical plausibility, demonstrating strong generalization across
> a wide range of real-world robotic placement scenarios.

