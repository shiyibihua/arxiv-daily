---
layout: default
title: Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection
---

# Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection

**arXiv**: [2510.14896v1](https://arxiv.org/abs/2510.14896) | [PDF](https://arxiv.org/pdf/2510.14896.pdf)

**ä½œè€…**: Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æè¿°æ´»åŠ¨çš„åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œä»¥æå‡å¤æ‚å¼‚å¸¸æ£€æµ‹ä¸å¯è§£é‡Šæ€§ã€‚**

**å…³é”®è¯**: `è§†é¢‘å¼‚å¸¸æ£€æµ‹` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åŠç›‘ç£å­¦ä¹ ` `å¯è§£é‡Šæ€§` `å¯¹è±¡äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•éš¾ä»¥æ£€æµ‹å¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚
2. æ–¹æ³•åˆ©ç”¨MLLMç”Ÿæˆå¯¹è±¡æ´»åŠ¨ä¸äº¤äº’çš„æ–‡æœ¬æè¿°ï¼Œé€šè¿‡æ¯”è¾ƒè®­ç»ƒä¸æµ‹è¯•è§†é¢‘æè¿°æ£€æµ‹å¼‚å¸¸ã€‚
3. å®éªŒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæ£€æµ‹äº¤äº’å¼‚å¸¸å¹¶åœ¨æ— äº¤äº’æ•°æ®é›†ä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing semi-supervised video anomaly detection (VAD) methods often struggle
> with detecting complex anomalies involving object interactions and generally
> lack explainability. To overcome these limitations, we propose a novel VAD
> framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
> MLLM-based approaches that make direct anomaly judgments at the frame level,
> our method focuses on extracting and interpreting object activity and
> interactions over time. By querying an MLLM with visual inputs of object pairs
> at different moments, we generate textual descriptions of the activity and
> interactions from nominal videos. These textual descriptions serve as a
> high-level representation of the activity and interactions of objects in a
> video. They are used to detect anomalies during test time by comparing them to
> textual descriptions found in nominal training videos. Our approach inherently
> provides explainability and can be combined with many traditional VAD methods
> to further enhance their interpretability. Extensive experiments on benchmark
> datasets demonstrate that our method not only detects complex interaction-based
> anomalies effectively but also achieves state-of-the-art performance on
> datasets without interaction anomalies.

