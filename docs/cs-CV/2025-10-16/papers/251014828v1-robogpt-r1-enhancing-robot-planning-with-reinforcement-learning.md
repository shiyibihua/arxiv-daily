---
layout: default
title: RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning
---

# RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning

**arXiv**: [2510.14828v1](https://arxiv.org/abs/2510.14828) | [PDF](https://arxiv.org/pdf/2510.14828.pdf)

**ä½œè€…**: Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboGPT-R1ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æž¶ä»¥å¢žå¼ºæœºå™¨äººåœ¨é•¿è§†è·æ“ä½œä»»åŠ¡ä¸­çš„è§„åˆ’èƒ½åŠ›**

**å…³é”®è¯**: `æœºå™¨äººè§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `é•¿è§†è·æ“ä½œ` `ä¸¤é˜¶æ®µå¾®è°ƒ` `å¥–åŠ±å‡½æ•°è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé€šç”¨è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æœºå™¨äººè§„åˆ’ä¸­æ³›åŒ–å·®ä¸”ç‰©ç†ç†è§£ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†å¤æ‚é•¿è§†è·ä»»åŠ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ç›‘ç£å¾®è°ƒèŽ·å–åŸºç¡€çŸ¥è¯†ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰ç©ºé—´ç†è§£å’ŒæŽ¨ç†èƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EmbodiedBenchåŸºå‡†ä¸Šï¼Œæ˜¾è‘—è¶…è¶ŠGPT-4o-miniå’Œå…¶ä»–æ¨¡åž‹ï¼Œæå‡è¶…è¿‡20%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Improving the reasoning capabilities of embodied agents is crucial for robots
> to complete complex human instructions in long-view manipulation tasks
> successfully. Despite the success of large language models and vision language
> models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
> facing challenges in performing long-horizon manipulation tasks in complex
> real-world environments, owing to their restricted common sense and reasoning
> capabilities. Considering that aligning general-purpose vision language models
> to robotic planning tasks via supervised fine-tuning suffers from poor
> generalization and insufficient physical understanding, we propose RoboGPT-R1,
> a two-stage fine-tuning framework for embodied planning. In this framework,
> supervised training acquires foundational knowledge through expert sequences,
> followed by RL to address the model's shortcomings in visual-spatial
> understanding and reasoning. To achieve physical understanding and action
> sequence consistency in multi-step reasoning tasks, we design a rule-based
> reward function that simultaneously considers long-horizon performance and
> action constraint in the environment. The reasoning model, trained on
> Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
> by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
> EmbodiedBench benchmark.

