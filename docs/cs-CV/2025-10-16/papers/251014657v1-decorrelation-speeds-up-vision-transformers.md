---
layout: default
title: Decorrelation Speeds Up Vision Transformers
---

# Decorrelation Speeds Up Vision Transformers

**arXiv**: [2510.14657v1](https://arxiv.org/abs/2510.14657) | [PDF](https://arxiv.org/pdf/2510.14657.pdf)

**ä½œè€…**: Kieran Carrigg, Rob van Gastel, Melda Yeghaian, Sander Dalm, Faysal Boughorbel, Marcel van Gerven

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŽ»ç›¸å…³åå‘ä¼ æ’­ä»¥åŠ é€Ÿè§†è§‰å˜æ¢å™¨é¢„è®­ç»ƒï¼Œé™ä½Žè®¡ç®—æˆæœ¬**

**å…³é”®è¯**: `è§†è§‰å˜æ¢å™¨` `åŽ»ç›¸å…³åå‘ä¼ æ’­` `æŽ©ç è‡ªç¼–ç å™¨` `é¢„è®­ç»ƒåŠ é€Ÿ` `å›¾åƒåˆ†å‰²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MAEé¢„è®­ç»ƒViTåœ¨ä½Žæ ‡ç­¾åœºæ™¯æ€§èƒ½å¼ºï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œå·¥ä¸šåº”ç”¨å—é™
2. é›†æˆDBPåˆ°MAEé¢„è®­ç»ƒï¼Œé€šè¿‡å‡å°‘å±‚é—´è¾“å…¥ç›¸å…³æ€§åŠ é€Ÿæ”¶æ•›ï¼Œä»…ç”¨äºŽç¼–ç å™¨
3. å®žéªŒæ˜¾ç¤ºé¢„è®­ç»ƒæ—¶é—´å‡å°‘21.1%ï¼Œç¢³æŽ’æ”¾é™ä½Ž21.4%ï¼Œåˆ†å‰²mIoUæå‡1.1ç‚¹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
> strong performance in low-label regimes but comes with substantial
> computational costs, making it impractical in time- and resource-constrained
> industrial settings. We address this by integrating Decorrelated
> Backpropagation (DBP) into MAE pre-training, an optimization method that
> iteratively reduces input correlations at each layer to accelerate convergence.
> Applied selectively to the encoder, DBP achieves faster pre-training without
> loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
> reduces wall-clock time to baseline performance by 21.1%, lowers carbon
> emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
> similar gains when pre-training and fine-tuning on proprietary industrial data,
> confirming the method's applicability in real-world scenarios. These results
> demonstrate that DBP can reduce training time and energy use while improving
> downstream performance for large-scale ViT pre-training.

