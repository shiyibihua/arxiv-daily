---
layout: default
title: VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation
---

# VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation

**arXiv**: [2510.14902v1](https://arxiv.org/abs/2510.14902) | [PDF](https://arxiv.org/pdf/2510.14902.pdf)

**ä½œè€…**: Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLA^2ä»£ç†æ¡†æž¶ä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹å¯¹æœªè§æ¦‚å¿µæ³›åŒ–å¤±è´¥é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä»£ç†æ¡†æž¶` `æ³›åŒ–èƒ½åŠ›` `æœªè§æ¦‚å¿µ` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹åœ¨è®­ç»ƒæ•°æ®å¤–å¯¹è±¡æ¦‚å¿µä¸ŠæˆåŠŸçŽ‡æ˜¾è‘—ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨OpenVLAæ‰§è¡Œéª¨å¹²ï¼Œé›†æˆå¤–éƒ¨æ¨¡å—æä¾›ç›®æ ‡å¯¹è±¡çŸ¥è¯†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç¡¬çº§æ³›åŒ–åŸºå‡†ä¸ŠæˆåŠŸçŽ‡æå‡44.2%ï¼Œå¹³å‡æå‡20.2%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Current vision-language-action (VLA) models, pre-trained on large-scale
> robotic data, exhibit strong multi-task capabilities and generalize well to
> variations in visual and language instructions for manipulation. However, their
> success rate drops significantly when faced with object concepts outside the
> training data, such as unseen object descriptions and textures in the dataset.
> To address this, we propose a novel agentic framework, VLA^2, which leverages
> OpenVLA as the execution backbone and effectively leverages external modules
> such as web retrieval and object detection to provide visual and textual
> knowledge about target objects to the VLA. This approach mitigates
> generalization failure when handling out-of-distribution objects. Based on the
> LIBERO simulation environment, we introduced novel objects and object
> descriptions to construct a new evaluation benchmark with three difficulty
> levels to test the effectiveness of our method. Our framework successfully
> outperformed the current state-of-the-art models on our designed hard-level
> generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2
> achieves a 44.2% improvement in the success rate in the hard-level benchmark
> and an average improvement of 20.2% in all customized environments without any
> performance degradation on in-domain tasks. Project website:
> https://vla-2.github.io.

