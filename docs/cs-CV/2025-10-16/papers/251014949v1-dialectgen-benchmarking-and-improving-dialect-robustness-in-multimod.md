---
layout: default
title: DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation
---

# DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation

**arXiv**: [2510.14949v1](https://arxiv.org/abs/2510.14949) | [PDF](https://arxiv.org/pdf/2510.14949.pdf)

**ä½œè€…**: Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¼–ç å™¨æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€ç”Ÿæˆæ¨¡åž‹å¯¹è‹±è¯­æ–¹è¨€çš„é²æ£’æ€§**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç”Ÿæˆ` `æ–¹è¨€é²æ£’æ€§` `ç¼–ç å™¨æ–¹æ³•` `åŸºå‡†æµ‹è¯•` `æ€§èƒ½ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€ç”Ÿæˆæ¨¡åž‹åœ¨æ–¹è¨€æ–‡æœ¬è¾“å…¥ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™å¹…è¾¾32.26%è‡³48.17%
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ç¼–ç å™¨ç­–ç•¥ï¼Œä½¿æ¨¡åž‹è¯†åˆ«æ–°æ–¹è¨€ç‰¹å¾å¹¶ä¿æŒæ ‡å‡†è‹±è¯­æ€§èƒ½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Stable Diffusion 1.5ç­‰æ¨¡åž‹ä¸Šï¼Œæ–¹è¨€æ€§èƒ½æå‡34.4%ï¼Œæ ‡å‡†è‹±è¯­æ€§èƒ½å‡ ä¹Žæ— æŸ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contact languages like English exhibit rich regional variations in the form
> of dialects, which are often used by dialect speakers interacting with
> generative models. However, can multimodal generative models effectively
> produce content given dialectal textual input? In this work, we study this
> question by constructing a new large-scale benchmark spanning six common
> English dialects. We work with dialect speakers to collect and verify over 4200
> unique prompts and evaluate on 17 image and video generative models. Our
> automatic and human evaluation results show that current state-of-the-art
> multimodal generative models exhibit 32.26% to 48.17% performance degradation
> when a single dialect word is used in the prompt. Common mitigation methods
> such as fine-tuning and prompt rewriting can only improve dialect performance
> by small margins (< 7%), while potentially incurring significant performance
> degradation in Standard American English (SAE). To this end, we design a
> general encoder-based mitigation strategy for multimodal generative models. Our
> method teaches the model to recognize new dialect features while preserving SAE
> performance. Experiments on models such as Stable Diffusion 1.5 show that our
> method is able to simultaneously raise performance on five dialects to be on
> par with SAE (+34.4%), while incurring near zero cost to SAE performance.

