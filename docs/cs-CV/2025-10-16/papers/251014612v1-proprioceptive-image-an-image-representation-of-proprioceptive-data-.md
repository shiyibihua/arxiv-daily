---
layout: default
title: Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning
---

# Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning

**arXiv**: [2510.14612v1](https://arxiv.org/abs/2510.14612) | [PDF](https://arxiv.org/pdf/2510.14612.pdf)

**ä½œè€…**: Gabriel Fischer Abati, JoÃ£o Carlos Virgolino Soares, Giulio Turrisi, Victor Barasuol, Claudio Semini

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå°†å››è¶³æœºå™¨äººæœ¬ä½“æ„Ÿè§‰æ•°æ®ç¼–ç ä¸ºå›¾åƒä»¥æå‡æŽ¥è§¦ä¼°è®¡æ€§èƒ½**

**å…³é”®è¯**: `å››è¶³æœºå™¨äºº` `æœ¬ä½“æ„Ÿè§‰æ•°æ®` `å›¾åƒè¡¨ç¤º` `å·ç§¯ç¥žç»ç½‘ç»œ` `æŽ¥è§¦ä¼°è®¡` `è·¨æ¨¡æ€ç¼–ç `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå››è¶³æœºå™¨äººåœ¨å¤šå˜åœ°å½¢ä¸Šçš„ç¨³å®šè¿åŠ¨éœ€å‡†ç¡®ä¼°è®¡è¶³éƒ¨æŽ¥è§¦çŠ¶æ€
2. æ–¹æ³•è¦ç‚¹ï¼šå°†å¤šæºæœ¬ä½“æ„Ÿè§‰æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºç»“æž„åŒ–äºŒç»´å›¾åƒï¼Œä¿ç•™æœºå™¨äººå½¢æ€å’ŒåŠ¨æ€æ¨¡å¼
3. å®žéªŒæ•ˆæžœï¼šåœ¨çœŸå®žå’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šï¼ŒæŽ¥è§¦ä¼°è®¡å‡†ç¡®çŽ‡ä»Ž87.7%æå‡è‡³94.5%ï¼Œçª—å£å°ºå¯¸ç¼©çŸ­15å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a novel approach for representing proprioceptive
> time-series data from quadruped robots as structured two-dimensional images,
> enabling the use of convolutional neural networks for learning
> locomotion-related tasks. The proposed method encodes temporal dynamics from
> multiple proprioceptive signals, such as joint positions, IMU readings, and
> foot velocities, while preserving the robot's morphological structure in the
> spatial arrangement of the image. This transformation captures inter-signal
> correlations and gait-dependent patterns, providing a richer feature space than
> direct time-series processing. We apply this concept in the problem of contact
> estimation, a key capability for stable and adaptive locomotion on diverse
> terrains. Experimental evaluations on both real-world datasets and simulated
> environments show that our image-based representation consistently enhances
> prediction accuracy and generalization over conventional sequence-based models,
> underscoring the potential of cross-modal encoding strategies for robotic state
> learning. Our method achieves superior performance on the contact dataset,
> improving contact state accuracy from 87.7% to 94.5% over the recently proposed
> MI-HGNN method, using a 15 times shorter window size.

