---
layout: default
title: WithAnyone: Towards Controllable and ID Consistent Image Generation
---

# WithAnyone: Towards Controllable and ID Consistent Image Generation

**arXiv**: [2510.14975v1](https://arxiv.org/abs/2510.14975) | [PDF](https://arxiv.org/pdf/2510.14975.pdf)

**ä½œè€…**: Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWithAnyoneæ¨¡å‹ä»¥è§£å†³èº«ä»½ä¸€è‡´å›¾åƒç”Ÿæˆä¸­çš„å¤åˆ¶ç²˜è´´é—®é¢˜**

**å…³é”®è¯**: `èº«ä»½ä¸€è‡´ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `å¤åˆ¶ç²˜è´´ç¼“è§£` `å¯æ§å›¾åƒç”Ÿæˆ` `å¯¹æ¯”å­¦ä¹ ` `å¤šèº«ä»½æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ¨¡å‹å› ä¾èµ–é‡å»ºè®­ç»ƒï¼Œæ˜“äº§ç”Ÿå¤åˆ¶ç²˜è´´ï¼Œé™åˆ¶å§¿æ€å’Œè¡¨æƒ…å¯æ§æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ„å»ºMultiID-2Mæ•°æ®é›†ï¼Œå¼•å…¥å¯¹æ¯”èº«ä»½æŸå¤±ï¼Œå¹³è¡¡èº«ä»½ä¿çœŸä¸å¤šæ ·æ€§ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šæ¨¡å‹æ˜¾è‘—å‡å°‘å¤åˆ¶ç²˜è´´ï¼Œæå‡å¯æ§æ€§ï¼Œä¿æŒé«˜èº«ä»½ç›¸ä¼¼å’Œæ„ŸçŸ¥è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Identity-consistent generation has become an important focus in text-to-image
> research, with recent models achieving notable success in producing images
> aligned with a reference identity. Yet, the scarcity of large-scale paired
> datasets containing multiple images of the same individual forces most
> approaches to adopt reconstruction-based training. This reliance often leads to
> a failure mode we term copy-paste, where the model directly replicates the
> reference face rather than preserving identity across natural variations in
> pose, expression, or lighting. Such over-similarity undermines controllability
> and limits the expressive power of generation. To address these limitations, we
> (1) construct a large-scale paired dataset MultiID-2M, tailored for
> multi-person scenarios, providing diverse references for each identity; (2)
> introduce a benchmark that quantifies both copy-paste artifacts and the
> trade-off between identity fidelity and variation; and (3) propose a novel
> training paradigm with a contrastive identity loss that leverages paired data
> to balance fidelity with diversity. These contributions culminate in
> WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
> preserving high identity similarity. Extensive qualitative and quantitative
> experiments demonstrate that WithAnyone significantly reduces copy-paste
> artifacts, improves controllability over pose and expression, and maintains
> strong perceptual quality. User studies further validate that our method
> achieves high identity fidelity while enabling expressive controllable
> generation.

