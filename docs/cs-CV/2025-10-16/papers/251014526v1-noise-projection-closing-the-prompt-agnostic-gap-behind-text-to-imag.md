---
layout: default
title: Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models
---

# Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models

**arXiv**: [2510.14526v1](https://arxiv.org/abs/2510.14526) | [PDF](https://arxiv.org/pdf/2510.14526.pdf)

**ä½œè€…**: Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå™ªå£°æŠ•å½±å™¨ä»¥è§£å†³æ‰©æ•£æ¨¡åž‹ä¸­æ–‡æœ¬-å›¾åƒä¸å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `å™ªå£°æŠ•å½±` `æ–‡æœ¬-å›¾åƒå¯¹é½` `åå¥½ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒä¸ŽæŽ¨ç†å™ªå£°åˆ†å¸ƒä¸åŒ¹é…å¯¼è‡´æ–‡æœ¬-å›¾åƒå¯¹é½å·®
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å™ªå£°æŠ•å½±å™¨å°†åˆå§‹å™ªå£°æ˜ å°„ä¸ºæç¤ºæ„ŸçŸ¥ç‰ˆæœ¬
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡VLMåé¦ˆä¼˜åŒ–ï¼Œæå‡å¤šæ ·æç¤ºä¸‹çš„å¯¹é½æ•ˆæžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In text-to-image generation, different initial noises induce distinct
> denoising paths with a pretrained Stable Diffusion (SD) model. While this
> pattern could output diverse images, some of them may fail to align well with
> the prompt. Existing methods alleviate this issue either by altering the
> denoising dynamics or by drawing multiple noises and conducting post-selection.
> In this paper, we attribute the misalignment to a training-inference mismatch:
> during training, prompt-conditioned noises lie in a prompt-specific subset of
> the latent space, whereas at inference the noise is drawn from a
> prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
> that applies text-conditioned refinement to the initial noise before denoising.
> Conditioned on the prompt embedding, it maps the noise to a prompt-aware
> counterpart that better matches the distribution observed during SD training,
> without modifying the SD model. Our framework consists of these steps: we first
> sample some noises and obtain token-level feedback for their corresponding
> images from a vision-language model (VLM), then distill these signals into a
> reward model, and finally optimize the noise projector via a quasi-direct
> preference optimization. Our design has two benefits: (i) it requires no
> reference images or handcrafted priors, and (ii) it incurs small inference
> cost, replacing multi-sample selection with a single forward pass. Extensive
> experiments further show that our prompt-aware noise projection improves
> text-image alignment across diverse prompts.

