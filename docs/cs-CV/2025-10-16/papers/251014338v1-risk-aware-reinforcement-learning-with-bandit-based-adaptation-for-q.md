---
layout: default
title: Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion
---

# Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion

**arXiv**: [2510.14338v1](https://arxiv.org/abs/2510.14338) | [PDF](https://arxiv.org/pdf/2510.14338.pdf)

**ä½œè€…**: Yuanhong Zeng, Anushri Dixit

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé£Žé™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ä¸Žå¤šè‡‚è€è™Žæœºè‡ªé€‚åº”æ–¹æ³•ï¼Œä»¥æå‡å››è¶³æœºå™¨äººåœ¨æœªçŸ¥çŽ¯å¢ƒä¸­çš„è¿åŠ¨æ€§èƒ½ã€‚**

**å…³é”®è¯**: `é£Žé™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ` `å››è¶³æœºå™¨äººè¿åŠ¨` `CVaRçº¦æŸä¼˜åŒ–` `å¤šè‡‚è€è™Žæœº` `åœ¨çº¿è‡ªé€‚åº”` `æœªçŸ¥çŽ¯å¢ƒé²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå››è¶³æœºå™¨äººåœ¨æœªçŸ¥çŽ¯å¢ƒä¸­è¿åŠ¨æ—¶ï¼Œé¢ä¸´ç¨³å®šæ€§ä¸Žæ€§èƒ½ä¸‹é™çš„é£Žé™©ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨CVaRçº¦æŸç­–ç•¥ä¼˜åŒ–è®­ç»ƒé£Žé™©æ¡ä»¶ç­–ç•¥å®¶æ—ï¼Œå¹¶é€šè¿‡å¤šè‡‚è€è™Žæœºåœ¨çº¿è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜ç­–ç•¥ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žæœºå™¨äººæµ‹è¯•ä¸­ï¼Œé£Žé™©æ„ŸçŸ¥ç­–ç•¥åœ¨æœªçŸ¥çŽ¯å¢ƒä¸­æ€§èƒ½æå‡è¿‘ä¸€å€ï¼Œè‡ªé€‚åº”é€‰æ‹©åœ¨2åˆ†é’Ÿå†…å®Œæˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this work, we study risk-aware reinforcement learning for quadrupedal
> locomotion. Our approach trains a family of risk-conditioned policies using a
> Conditional Value-at-Risk (CVaR) constrained policy optimization technique that
> provides improved stability and sample efficiency. At deployment, we adaptively
> select the best performing policy from the family of policies using a
> multi-armed bandit framework that uses only observed episodic returns, without
> any privileged environment information, and adapts to unknown conditions on the
> fly. Hence, we train quadrupedal locomotion policies at various levels of
> robustness using CVaR and adaptively select the desired level of robustness
> online to ensure performance in unknown environments. We evaluate our method in
> simulation across eight unseen settings (by changing dynamics, contacts,
> sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen
> terrains. Our risk-aware policy attains nearly twice the mean and tail
> performance in unseen environments compared to other baselines and our
> bandit-based adaptation selects the best-performing risk-aware policy in
> unknown terrain within two minutes of operation.

