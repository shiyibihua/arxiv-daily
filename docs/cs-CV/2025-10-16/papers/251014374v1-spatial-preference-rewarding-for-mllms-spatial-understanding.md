---
layout: default
title: Spatial Preference Rewarding for MLLMs Spatial Understanding
---

# Spatial Preference Rewarding for MLLMs Spatial Understanding

**arXiv**: [2510.14374v1](https://arxiv.org/abs/2510.14374) | [PDF](https://arxiv.org/pdf/2510.14374.pdf)

**ä½œè€…**: Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´åå¥½å¥–åŠ±æ–¹æ³•ä»¥å¢žå¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„ç»†ç²’åº¦ç©ºé—´ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç©ºé—´ç†è§£` `åå¥½å¥–åŠ±` `å¯¹è±¡å®šä½` `ç»†ç²’åº¦æ„ŸçŸ¥` `ç›´æŽ¥åå¥½ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMLLMsåœ¨ç»†ç²’åº¦ç©ºé—´æ„ŸçŸ¥å¦‚åŒºåŸŸæè¿°å’Œå¯¹è±¡å®šä½ä¸­è¡¨çŽ°ä¸è¶³ï¼Œä¸”éš¾ä»¥å“åº”ç”¨æˆ·éœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šSPRé€šè¿‡å¥–åŠ±è¯¦ç»†å“åº”å’Œç²¾ç¡®å®šä½ï¼Œä½¿ç”¨è¯­ä¹‰å’Œå®šä½åˆ†æ•°è¯„ä¼°å¹¶ä¼˜åŒ–æè¿°è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPRæœ‰æ•ˆæå‡ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œè®­ç»ƒå¼€é”€æœ€å°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models~(MLLMs) have demonstrated promising spatial
> understanding capabilities, such as referencing and grounding object
> descriptions. Despite their successes, MLLMs still fall short in fine-grained
> spatial perception abilities, such as generating detailed region descriptions
> or accurately localizing objects. Additionally, they often fail to respond to
> the user's requirements for desired fine-grained spatial understanding. This
> issue might arise because existing approaches primarily focus on tuning MLLMs
> to model pre-annotated instruction data to inject spatial knowledge, without
> direct supervision of MLLMs' actual responses. We address this issue by SPR, a
> Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
> capabilities by rewarding MLLMs' detailed responses with precise object
> localization over vague or inaccurate responses. With randomly selected image
> regions and region descriptions from MLLMs, SPR introduces semantic and
> localization scores to comprehensively evaluate the text quality and
> localization quality in MLLM-generated descriptions. We also refine the MLLM
> descriptions with better localization accuracy and pair the best-scored
> refinement with the initial descriptions of the lowest score for direct
> preference optimization, thereby enhancing fine-grained alignment with visual
> input. Extensive experiments over standard referring and grounding benchmarks
> show that SPR improves MLLM spatial understanding capabilities effectively with
> minimal overhead in training. Data and code will be released at
> https://github.com/hanqiu-hq/SPR

