---
layout: default
title: Vision-Centric Activation and Coordination for Multimodal Large Language Models
---

# Vision-Centric Activation and Coordination for Multimodal Large Language Models

**arXiv**: [2510.14349v1](https://arxiv.org/abs/2510.14349) | [PDF](https://arxiv.org/pdf/2510.14349.pdf)

**ä½œè€…**: Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVaCoæ–¹æ³•ä»¥ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è§†è§‰ä¸­å¿ƒè¡¨ç¤º**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰ä¸­å¿ƒæ¿€æ´»` `è§†è§‰åŸºç¡€æ¨¡åž‹` `è¡¨ç¤ºä¼˜åŒ–` `è§†è§‰ç†è§£` `æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¸»æµMLLMsä»…ç›‘ç£æ–‡æœ¬æ ‡è®°é¢„æµ‹ï¼Œå¿½è§†è§†è§‰ä¸­å¿ƒä¿¡æ¯ï¼Œå½±å“åˆ†æžèƒ½åŠ›
2. å¼•å…¥è§†è§‰åˆ¤åˆ«å¯¹é½ã€æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢å’Œè§†è§‰å¯¹é½å±‚ï¼Œåè°ƒå¤šè§†è§‰åŸºç¡€æ¨¡åž‹ç‰¹å¾
3. å®žéªŒæ˜¾ç¤ºVaCoæ˜¾è‘—æå‡å¤šç§MLLMsåœ¨è§†è§‰ç†è§£åŸºå‡†ä¸Šçš„æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) integrate image features from visual
> encoders with LLMs, demonstrating advanced comprehension capabilities. However,
> mainstream MLLMs are solely supervised by the next-token prediction of textual
> tokens, neglecting critical vision-centric information essential for analytical
> abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
> representations through Vision-Centric activation and Coordination from
> multiple vision foundation models (VFMs). VaCo introduces visual discriminative
> alignment to integrate task-aware perceptual features extracted from VFMs,
> thereby unifying the optimization of both textual and visual outputs in MLLMs.
> Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
> Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
> under the supervision of diverse VFMs. To coordinate representation conflicts
> across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
> flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
> significantly improves the performance of different MLLMs on various
> benchmarks, showcasing its superior capabilities in visual comprehension.

