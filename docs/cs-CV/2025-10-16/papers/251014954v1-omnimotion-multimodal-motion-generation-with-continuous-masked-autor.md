---
layout: default
title: OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression
---

# OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression

**arXiv**: [2510.14954v1](https://arxiv.org/abs/2510.14954) | [PDF](https://arxiv.org/pdf/2510.14954.pdf)

**ä½œè€…**: Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿žç»­æŽ©ç è‡ªå›žå½’è¿åŠ¨å˜æ¢å™¨ä»¥è§£å†³å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆé—®é¢˜**

**å…³é”®è¯**: `äººä½“è¿åŠ¨ç”Ÿæˆ` `å¤šæ¨¡æ€èžåˆ` `è‡ªå›žå½’å˜æ¢å™¨` `è¿žç»­æŽ©ç å»ºæ¨¡` `é—¨æŽ§æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆéœ€æœ‰æ•ˆæœºåˆ¶ä¸Žæ¨¡æ€èžåˆï¼Œå¦‚æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è¿žç»­æŽ©ç è‡ªå›žå½’å˜æ¢å™¨ï¼Œç»“åˆé—¨æŽ§çº¿æ€§æ³¨æ„åŠ›å’ŒRMSNormæ¨¡å—
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€è¯­éŸ³åˆ°æ‰‹åŠ¿å’ŒéŸ³ä¹åˆ°èˆžè¹ˆä»»åŠ¡ä¸­ä¼˜äºŽå…ˆå‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Whole-body multi-modal human motion generation poses two primary challenges:
> creating an effective motion generation mechanism and integrating various
> modalities, such as text, speech, and music, into a cohesive framework. Unlike
> previous methods that usually employ discrete masked modeling or autoregressive
> modeling, we develop a continuous masked autoregressive motion transformer,
> where a causal attention is performed considering the sequential nature within
> the human motion. Within this transformer, we introduce a gated linear
> attention and an RMSNorm module, which drive the transformer to pay attention
> to the key actions and suppress the instability caused by either the abnormal
> movements or the heterogeneous distributions within multi-modalities. To
> further enhance both the motion generation and the multimodal generalization,
> we employ the DiT structure to diffuse the conditions from the transformer
> towards the targets. To fuse different modalities, AdaLN and cross-attention
> are leveraged to inject the text, speech, and music signals. Experimental
> results demonstrate that our framework outperforms previous methods across all
> modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
> The code of our method will be made public.

