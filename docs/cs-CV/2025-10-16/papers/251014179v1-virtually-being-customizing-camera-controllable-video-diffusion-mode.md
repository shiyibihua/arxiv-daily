---
layout: default
title: Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures
---

# Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures

**arXiv**: [2510.14179v1](https://arxiv.org/abs/2510.14179) | [PDF](https://arxiv.org/pdf/2510.14179.pdf)

**ä½œè€…**: Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent TaÅŸel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®šåˆ¶åŒ–è§†é¢‘æ‰©æ•£æ¨¡åž‹æ¡†æž¶ï¼Œå®žçŽ°å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§å’Œ3Dç›¸æœºæŽ§åˆ¶ã€‚**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `å¤šè§†è§’ä¸€è‡´æ€§` `3Dç›¸æœºæŽ§åˆ¶` `4Dé«˜æ–¯æº…å°„` `è™šæ‹Ÿç”Ÿäº§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æ‰©æ•£æ¨¡åž‹åœ¨å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§å’Œ3Dç›¸æœºæŽ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨4Dé«˜æ–¯æº…å°„å’Œè§†é¢‘é‡å…‰ç…§æ¨¡åž‹æž„å»ºå®šåˆ¶æ•°æ®ç®¡é“è¿›è¡Œå¾®è°ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºè§†é¢‘è´¨é‡ã€ä¸ªæ€§åŒ–ç²¾åº¦å’Œç›¸æœºæŽ§åˆ¶èƒ½åŠ›å¾—åˆ°æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce a framework that enables both multi-view character consistency
> and 3D camera control in video diffusion models through a novel customization
> data pipeline. We train the character consistency component with recorded
> volumetric capture performances re-rendered with diverse camera trajectories
> via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
> relighting model. We fine-tune state-of-the-art open-source video diffusion
> models on this data to provide strong multi-view identity preservation, precise
> camera control, and lighting adaptability. Our framework also supports core
> capabilities for virtual production, including multi-subject generation using
> two approaches: joint training and noise blending, the latter enabling
> efficient composition of independently customized models at inference time; it
> also achieves scene and real-life video customization as well as control over
> motion and spatial layout during customization. Extensive experiments show
> improved video quality, higher personalization accuracy, and enhanced camera
> control and lighting adaptability, advancing the integration of video
> generation into virtual production. Our project page is available at:
> https://eyeline-labs.github.io/Virtually-Being.

