---
layout: default
title: Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking
---

# Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking

**arXiv**: [2510.14824v1](https://arxiv.org/abs/2510.14824) | [PDF](https://arxiv.org/pdf/2510.14824.pdf)

**ä½œè€…**: Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒç›‘ç£å¾®è°ƒä¸Žå¯¹æ¯”å­¦ä¹ åœ¨LLMé‡æŽ’åºä¸­çš„ä¼˜åŠ£ï¼Œå‘çŽ°SFTæ›´ä¼˜**

**å…³é”®è¯**: `LLMé‡æŽ’åº` `ç›‘ç£å¾®è°ƒ` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€æ£€ç´¢` `æƒé‡åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMé‡æŽ’åºä¸­ï¼Œç›‘ç£å¾®è°ƒä¸Žå¯¹æ¯”å­¦ä¹ å“ªç§ç›®æ ‡æ›´æœ‰æ•ˆï¼Ÿ
2. æ–¹æ³•è¦ç‚¹ï¼šå°†ç›®æ ‡åˆ†è§£ä¸ºæƒé‡å’Œæ–¹å‘ï¼Œæå‡ºç»Ÿä¸€åˆ†æžæ¡†æž¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šSFTæƒé‡æ›´å¼ºï¼Œåœ¨MRBåŸºå‡†ä¸Šå®žçŽ°SOTAæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In information retrieval, training reranking models mainly focuses on two
> types of objectives: metric learning (e.g. contrastive loss to increase the
> predicted scores on relevant query-document pairs) and classification (binary
> label prediction of relevance vs. irrelevance). For BERT-style encoders,
> various studies have shown that contrastive learning (CL) can be more effective
> than discriminative (classification) learning. However, for large language
> models (LLMs), classification via supervised fine-tuning (SFT), which predicts
> ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears
> more promising as it aligns well with the generative nature of LLMs. This
> divergence raises a central question: which objective is intrinsically better
> suited to LLM-based reranking, and what mechanism underlies the difference? In
> this work, we conduct a comprehensive comparison and analysis between CL and
> SFT for reranking, taking the universal multimodal retrieval (UMR) as the
> experimental playground. We first decompose the objectives into two components:
> weight, which controls the magnitude of those updates, and direction, which
> guides the model updates, then present a unified framework for understanding
> their interactions. Through probing experiments, we find that SFT provides a
> substantially stronger weighting scheme than CL, whereas the preferred scoring
> direction shows no clear winner. Taken together, these results point to a
> consistent advantage of SFT over CL for LLM reranking. To further validate our
> findings, we conduct large-scale training with SFT and present new
> state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
> SFT settings and expect our findings to benefit future research and
> applications in this area.

