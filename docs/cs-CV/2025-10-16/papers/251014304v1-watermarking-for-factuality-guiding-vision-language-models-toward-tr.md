---
layout: default
title: Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding
---

# Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding

**arXiv**: [2510.14304v1](https://arxiv.org/abs/2510.14304) | [PDF](https://arxiv.org/pdf/2510.14304.pdf)

**ä½œè€…**: Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ°´å°çš„ä¸‰å±‚å¯¹æ¯”è§£ç æ–¹æ³•ï¼Œä»¥å‡å°‘è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¹»è§‰å‡å°‘` `å¯¹æ¯”è§£ç ` `è§†è§‰æ¥åœ°` `æ— è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡å‹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œä¾èµ–å•ä¸€æ¨¡æ€æˆ–è®°å¿†è®­ç»ƒæ•°æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ— è®­ç»ƒä¸‰å±‚å¯¹æ¯”è§£ç ï¼Œé€‰æ‹©æˆç†Ÿå±‚ã€ä¸šä½™å±‚å’Œè§†è§‰æ¥åœ°æ”¯ç‚¹å±‚ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨POPEç­‰åŸºå‡†ä¸Šå®ç°SOTAï¼Œå‡å°‘å¹»è§‰å¹¶å¢å¼ºè§†è§‰æ¥åœ°å“åº”ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have recently shown promising results on
> various multimodal tasks, even achieving human-comparable performance in
> certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
> rely heavily on a single modality or memorize training data without properly
> grounding their outputs. To address this, we propose a training-free, tri-layer
> contrastive decoding with watermarking, which proceeds in three steps: (1)
> select a mature layer and an amateur layer among the decoding layers, (2)
> identify a pivot layer using a watermark-related question to assess whether the
> layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
> to generate the final output. Experiments on public benchmarks such as POPE,
> MME and AMBER demonstrate that our method achieves state-of-the-art performance
> in reducing hallucinations in LVLMs and generates more visually grounded
> responses.

