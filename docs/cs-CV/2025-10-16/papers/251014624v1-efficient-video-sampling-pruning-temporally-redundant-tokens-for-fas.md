---
layout: default
title: Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference
---

# Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference

**arXiv**: [2510.14624v1](https://arxiv.org/abs/2510.14624) | [PDF](https://arxiv.org/pdf/2510.14624.pdf)

**ä½œè€…**: Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆè§†é¢‘é‡‡æ ·æ–¹æ³•ä»¥è§£å†³è§†é¢‘è¯­è¨€æ¨¡åž‹æŽ¨ç†ä¸­çš„å†—ä½™ä»¤ç‰Œé—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œå‰ªæž` `æŽ¨ç†åŠ é€Ÿ` `é•¿è§†é¢‘å¤„ç†` `é«˜æ•ˆé‡‡æ ·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘è¯­è¨€æ¨¡åž‹å¤„ç†é•¿è§†é¢‘æ—¶ä»¤ç‰Œæ•°é‡è¶…å‡ºé¢„ç®—ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡é™åˆ¶å’Œå»¶è¿Ÿé—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¯†åˆ«å¹¶å‰ªæžæ—¶é—´ä¸Šé™æ€çš„è¡¥ä¸æ¥å‡å°‘ä»¤ç‰Œå†—ä½™ï¼Œæ— éœ€æž¶æž„ä¿®æ”¹æˆ–é‡è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåº”ç”¨æ—¶å¯å°†å¤§åž‹è¯­è¨€æ¨¡åž‹é¦–ä»¤ç‰Œæ—¶é—´å‡å°‘é«˜è¾¾4å€ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¿çœŸåº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) have recently expanded from static image
> understanding to video reasoning, but their scalability is fundamentally
> limited by the quadratic cost of processing dense frame sequences. Long videos
> often exceed the token budget of modern language models, leading to severe
> context limitations and latency issues. We introduce Efficient Video Sampling
> (EVS), a simple, plug-and-play method for reducing token redundancy in videos
> by identifying and pruning temporally static patches -- spatial regions that
> remain unchanged across consecutive frames. EVS preserves positional identity,
> requires no architectural changes or retraining. We show that EVS substantially
> reduces token count while maintaining semantic fidelity, enabling faster
> inference and longer input sequences. Applied at inference time, EVS reduces
> large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
> accuracy loss. When combined with an uptraining phase using stochastic pruning
> rates, EVS yields models that are robust to varying compression levels and
> retain full performance under aggressive pruning. Extensive experiments
> demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
> unlocking scalable video-language understanding without sacrificing quality.

