---
layout: default
title: Learning an Image Editing Model without Image Editing Pairs
---

# Learning an Image Editing Model without Image Editing Pairs

**arXiv**: [2510.14978v1](https://arxiv.org/abs/2510.14978) | [PDF](https://arxiv.org/pdf/2510.14978.pdf)

**ä½œè€…**: Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— é…å¯¹æ•°æ®å›¾åƒç¼–è¾‘è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹åé¦ˆä¼˜åŒ–æ‰©æ•£æ¨¡åž‹**

**å…³é”®è¯**: `å›¾åƒç¼–è¾‘` `æ— é…å¯¹è®­ç»ƒ` `æ‰©æ•£æ¨¡åž‹` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åˆ†å¸ƒåŒ¹é…æŸå¤±` `ç«¯åˆ°ç«¯ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒç¼–è¾‘æ¨¡åž‹ä¾èµ–å¤§è§„æ¨¡è¾“å…¥-ç›®æ ‡é…å¯¹æ•°æ®ï¼Œéš¾ä»¥èŽ·å–ä¸”æ˜“ä¼ æ’­é¢„è®­ç»ƒæ¨¡åž‹ä¼ªå½±
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å±•å¼€æ‰©æ•£æ¨¡åž‹è®­ç»ƒï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡åž‹è¯„ä¼°ç¼–è¾‘æŒ‡ä»¤éµå¾ªå’Œå†…å®¹ä¿ç•™ï¼Œæä¾›ç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ— é…å¯¹æ•°æ®ä¸‹æ€§èƒ½ä¸Žç›‘ç£è®­ç»ƒæ¨¡åž‹ç›¸å½“ï¼Œä¼˜äºŽåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent image editing models have achieved impressive results while following
> natural language editing instructions, but they rely on supervised fine-tuning
> with large datasets of input-target pairs. This is a critical bottleneck, as
> such naturally occurring pairs are hard to curate at scale. Current workarounds
> use synthetic training pairs that leverage the zero-shot capabilities of
> existing models. However, this can propagate and magnify the artifacts of the
> pretrained model into the final trained model. In this work, we present a new
> training paradigm that eliminates the need for paired data entirely. Our
> approach directly optimizes a few-step diffusion model by unrolling it during
> training and leveraging feedback from vision-language models (VLMs). For each
> input and editing instruction, the VLM evaluates if an edit follows the
> instruction and preserves unchanged content, providing direct gradients for
> end-to-end optimization. To ensure visual fidelity, we incorporate distribution
> matching loss (DMD), which constrains generated images to remain within the
> image manifold learned by pretrained models. We evaluate our method on standard
> benchmarks and include an extensive ablation study. Without any paired data,
> our method performs on par with various image editing diffusion models trained
> on extensive supervised paired data, under the few-step setting. Given the same
> VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.

