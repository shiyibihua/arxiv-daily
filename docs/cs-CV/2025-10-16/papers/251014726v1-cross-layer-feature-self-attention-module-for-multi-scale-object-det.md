---
layout: default
title: Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection
---

# Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection

**arXiv**: [2510.14726v1](https://arxiv.org/abs/2510.14726) | [PDF](https://arxiv.org/pdf/2510.14726.pdf)

**ä½œè€…**: Dingzhou Xie, Rushi Lan, Cheng Pang, Enhao Ning, Jiahao Zeng, Wei Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨å±‚ç‰¹å¾è‡ªæ³¨æ„åŠ›æ¨¡å—ä»¥æå‡å¤šå°ºåº¦ç›®æ ‡æ£€æµ‹æ€§èƒ½**

**å…³é”®è¯**: `å¤šå°ºåº¦ç›®æ ‡æ£€æµ‹` `è·¨å±‚æ³¨æ„åŠ›` `ç‰¹å¾èžåˆ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `SSDæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å¿½è§†å¤šå°ºåº¦ç‰¹å¾é—´çš„å±‚é—´ä¾èµ–ï¼Œé™åˆ¶æ£€æµ‹å¤§å°ºåº¦å˜åŒ–ç‰©ä½“çš„èƒ½åŠ›
2. æ¨¡å—ç»“åˆå·ç§¯å±€éƒ¨ç‰¹å¾æå–ã€Transformerå…¨å±€å»ºæ¨¡å’Œç‰¹å¾èžåˆï¼Œå¢žå¼ºå¤šå°ºåº¦è¡¨ç¤º
3. é›†æˆSSD300æ˜¾è‘—æå‡PASCAL VOCå’ŒCOCOæ•°æ®é›†mAPï¼ŒåŠ é€Ÿæ”¶æ•›ä¸”è®¡ç®—å¼€é”€å°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent object detection methods have made remarkable progress by leveraging
> attention mechanisms to improve feature discriminability. However, most
> existing approaches are confined to refining single-layer or fusing dual-layer
> features, overlooking the rich inter-layer dependencies across multi-scale
> representations. This limits their ability to capture comprehensive contextual
> information essential for detecting objects with large scale variations. In
> this paper, we propose a novel Cross-Layer Feature Self-Attention Module
> (CFSAM), which holistically models both local and global dependencies within
> multi-scale feature maps. CFSAM consists of three key components: a
> convolutional local feature extractor, a Transformer-based global modeling unit
> that efficiently captures cross-layer interactions, and a feature fusion
> mechanism to restore and enhance the original representations. When integrated
> into the SSD300 framework, CFSAM significantly boosts detection performance,
> achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
> (vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
> module accelerates convergence during training without introducing substantial
> computational overhead. Our work highlights the importance of explicit
> cross-layer attention modeling in advancing multi-scale object detection.

