---
layout: default
title: STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding
---

# STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding

**arXiv**: [2510.14588v1](https://arxiv.org/abs/2510.14588) | [PDF](https://arxiv.org/pdf/2510.14588.pdf)

**ä½œè€…**: Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTANCEæ¡†æž¶ï¼Œé€šè¿‡ç¨€ç–åˆ°ç¨ å¯†é”šå®šç¼–ç è§£å†³è§†é¢‘ç”Ÿæˆä¸­è¿åŠ¨ä¸€è‡´æ€§é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `è¿åŠ¨ä¸€è‡´æ€§` `ç¨€ç–åˆ°ç¨ å¯†ç¼–ç ` `å®žä¾‹çº¿ç´¢` `Dense RoPE` `æ—¶é—´ç›¸å¹²æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘ç”Ÿæˆä¸­å¯¹è±¡è¿åŠ¨ä¸è¿žè´¯ï¼ŒæºäºŽè¿åŠ¨æç¤ºç¼–ç åŽæœ‰æ•ˆä»¤ç‰Œè¿‡å°‘å’Œå¤–è§‚-è¿åŠ¨ä¼˜åŒ–å†²çª
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å®žä¾‹çº¿ç´¢ç”Ÿæˆç¨ å¯†2.5Dè¿åŠ¨åœºï¼Œå¹¶å¼•å…¥Dense RoPEä¿æŒè¿åŠ¨ä»¤ç‰Œæ˜¾è‘—æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šç»“åˆRGBä¸Žè¾…åŠ©å›¾é¢„æµ‹ï¼Œæå‡æ—¶é—´ä¸€è‡´æ€§ï¼Œæ— éœ€é€å¸§è½¨è¿¹è„šæœ¬

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video generation has recently made striking visual progress, but maintaining
> coherent object motion and interactions remains difficult. We trace two
> practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
> often collapse to too few effective tokens after encoding, weakening guidance;
> and (ii) optimizing for appearance and motion in a single head can favor
> texture over temporal consistency. We present STANCE, an image-to-video
> framework that addresses both issues with two simple components. First, we
> introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
> user-editable hints into a dense 2.5D (camera-relative) motion field by
> averaging per-instance flow and augmenting with monocular depth over the
> instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
> remaining easy to use. Second, we preserve the salience of these cues in token
> space with Dense RoPE, which tags a small set of motion tokens (anchored on the
> first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
> \(+\) auxiliary-map prediction (segmentation or depth), our model anchors
> structure while RGB handles appearance, stabilizing optimization and improving
> temporal coherence without requiring per-frame trajectory scripts.

