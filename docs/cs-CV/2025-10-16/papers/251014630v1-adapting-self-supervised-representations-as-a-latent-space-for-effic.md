---
layout: default
title: Adapting Self-Supervised Representations as a Latent Space for Efficient Generation
---

# Adapting Self-Supervised Representations as a Latent Space for Efficient Generation

**arXiv**: [2510.14630v1](https://arxiv.org/abs/2510.14630) | [PDF](https://arxiv.org/pdf/2510.14630.pdf)

**ä½œè€…**: Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, BjÃ¶rn Ommer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRepTokæ¡†æž¶ï¼Œåˆ©ç”¨è‡ªç›‘ç£è¡¨ç¤ºä½œä¸ºç´§å‡‘æ½œç©ºé—´ï¼Œå®žçŽ°é«˜æ•ˆå›¾åƒç”Ÿæˆã€‚**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å›¾åƒç”Ÿæˆ` `æ½œç©ºé—´ä¼˜åŒ–` `æµåŒ¹é…` `ä»¤ç‰ŒåŒ–è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿ2Dæ½œç©ºé—´å­˜åœ¨ç©ºé—´å†—ä½™ï¼Œè®­ç»ƒæˆæœ¬é«˜ï¼Œå½±å“ç”Ÿæˆæ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé¢„è®­ç»ƒSSLç¼–ç å™¨ï¼Œå¾®è°ƒè¯­ä¹‰ä»¤ç‰ŒåµŒå…¥ï¼Œç»“åˆæµåŒ¹é…ç›®æ ‡è®­ç»ƒè§£ç å™¨ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ImageNetç±»æ¡ä»¶ç”Ÿæˆå’ŒMS-COCOæ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­è¾¾åˆ°ç«žäº‰æ€§ç»“æžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce Representation Tokenizer (RepTok), a generative modeling
> framework that represents an image using a single continuous latent token
> obtained from self-supervised vision transformers. Building on a pre-trained
> SSL encoder, we fine-tune only the semantic token embedding and pair it with a
> generative decoder trained jointly using a standard flow matching objective.
> This adaptation enriches the token with low-level, reconstruction-relevant
> details, enabling faithful image reconstruction. To preserve the favorable
> geometry of the original SSL space, we add a cosine-similarity loss that
> regularizes the adapted token, ensuring the latent space remains smooth and
> suitable for generation. Our single-token formulation resolves spatial
> redundancies of 2D latent spaces and significantly reduces training costs.
> Despite its simplicity and efficiency, RepTok achieves competitive results on
> class-conditional ImageNet generation and naturally extends to text-to-image
> synthesis, reaching competitive zero-shot performance on MS-COCO under
> extremely limited training budgets. Our findings highlight the potential of
> fine-tuned SSL representations as compact and effective latent spaces for
> efficient generative modeling.

