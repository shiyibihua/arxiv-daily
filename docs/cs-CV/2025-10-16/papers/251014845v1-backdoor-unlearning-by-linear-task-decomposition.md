---
layout: default
title: Backdoor Unlearning by Linear Task Decomposition
---

# Backdoor Unlearning by Linear Task Decomposition

**arXiv**: [2510.14845v1](https://arxiv.org/abs/2510.14845) | [PDF](https://arxiv.org/pdf/2510.14845.pdf)

**ä½œè€…**: Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçº¿æ€§ä»»åŠ¡åˆ†è§£æ–¹æ³•ä»¥ç§»é™¤åŸºç¡€æ¨¡å‹ä¸­çš„åé—¨æ”»å‡»**

**å…³é”®è¯**: `åé—¨ç§»é™¤` `åŸºç¡€æ¨¡å‹å®‰å…¨` `çº¿æ€§ä»»åŠ¡åˆ†è§£` `æƒé‡ç©ºé—´åˆ†æ` `CLIPæ¨¡å‹` `å¯¹æŠ—æ”»å‡»é˜²å¾¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºç¡€æ¨¡å‹æ˜“å—åé—¨æ”»å‡»ï¼Œç°æœ‰æ–¹æ³•éœ€é«˜æˆæœ¬å¾®è°ƒä¸”å½±å“å…¶ä»–ä»»åŠ¡æ€§èƒ½
2. å‘ç°åé—¨åœ¨æƒé‡ç©ºé—´ä¸­ä¸è‰¯æ€§ä»»åŠ¡è§£è€¦ï¼Œå¯éš”ç¦»å¹¶æ“¦é™¤å…¶å½±å“
3. å®éªŒæ˜¾ç¤ºæ–¹æ³•è¿‘ä¹å®Œç¾ç§»é™¤åé—¨ï¼Œå¹³å‡ä¿æŒ96%æ¸…æ´å‡†ç¡®ç‡

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models have revolutionized computer vision by enabling broad
> generalization across diverse tasks. Yet, they remain highly susceptible to
> adversarial perturbations and targeted backdoor attacks. Mitigating such
> vulnerabilities remains an open challenge, especially given that the
> large-scale nature of the models prohibits retraining to ensure safety.
> Existing backdoor removal approaches rely on costly fine-tuning to override the
> harmful behavior, and can often degrade performance on other unrelated tasks.
> This raises the question of whether backdoors can be removed without
> compromising the general capabilities of the models. In this work, we address
> this question and study how backdoors are encoded in the model weight space,
> finding that they are disentangled from other benign tasks. Specifically, this
> separation enables the isolation and erasure of the backdoor's influence on the
> model with minimal impact on clean performance. Building on this insight, we
> introduce a simple unlearning method that leverages such disentanglement.
> Through extensive experiments with CLIP-based models and common adversarial
> triggers, we show that, given the knowledge of the attack, our method achieves
> approximately perfect unlearning, while retaining, on average, 96% of clean
> accuracy. Additionally, we demonstrate that even when the attack and its
> presence are unknown, our method successfully unlearns backdoors by proper
> estimation using reverse-engineered triggers. Overall, our method consistently
> yields better unlearning and clean accuracy tradeoffs when compared to present
> state-of-the-art defenses.

