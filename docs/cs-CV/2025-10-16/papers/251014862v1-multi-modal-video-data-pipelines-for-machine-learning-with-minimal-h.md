---
layout: default
title: Multi-modal video data-pipelines for machine learning with minimal human supervision
---

# Multi-modal video data-pipelines for machine learning with minimal human supervision

**arXiv**: [2510.14862v1](https://arxiv.org/abs/2510.14862) | [PDF](https://arxiv.org/pdf/2510.14862.pdf)

**ä½œè€…**: Mihai-Cristian PÃ®rvu, Marius Leordeanu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€è§†é¢‘æ•°æ®æµæ°´çº¿ï¼Œå®žçŽ°æ— ç›‘ç£å­¦ä¹ å¹¶éƒ¨ç½²äºŽå®žæ—¶è¯­ä¹‰åˆ†å‰²**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ— ç›‘ç£æ•°æ®æµæ°´çº¿` `æ¨¡åž‹è’¸é¦` `å®žæ—¶è¯­ä¹‰åˆ†å‰²` `æ·±åº¦ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°å®žä¸–ç•Œå¤šæ¨¡æ€æ•°æ®åœ¨æ•°å­—åŒ–ä¸­ä¸¢å¤±ï¼Œéœ€æ•´åˆå¤šæ¨¡æ€ä»¥æå‡ç†è§£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨é¢„è®­ç»ƒä¸“å®¶å’Œè‡ªä¸»æ•°æ®æµæ°´çº¿ï¼Œç»“åˆPHG-MAEæ¨¡åž‹è¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹å‚æ•°é‡å°äºŽ1Mï¼Œæ€§èƒ½åª²ç¾Ž300Må‚æ•°æ¨¡åž‹ï¼Œå¹¶éƒ¨ç½²äºŽå®žæ—¶åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The real-world is inherently multi-modal at its core. Our tools observe and
> take snapshots of it, in digital form, such as videos or sounds, however much
> of it is lost. Similarly for actions and information passing between humans,
> languages are used as a written form of communication. Traditionally, Machine
> Learning models have been unimodal (i.e. rgb -> semantic or text ->
> sentiment_class). Recent trends go towards bi-modality, where images and text
> are learned together, however, in order to truly understand the world, we need
> to integrate all these independent modalities. In this work we try to combine
> as many visual modalities as we can using little to no human supervision. In
> order to do this, we use pre-trained experts and procedural combinations
> between them on top of raw videos using a fully autonomous data-pipeline, which
> we also open-source. We then make use of PHG-MAE, a model specifically designed
> to leverage multi-modal data. We show that this model which was efficiently
> distilled into a low-parameter (<1M) can have competitive results compared to
> models of ~300M parameters. We deploy this model and analyze the use-case of
> real-time semantic segmentation from handheld devices or webcams on commodity
> hardware. Finally, we deploy other off-the-shelf models using the same
> framework, such as DPT for near real-time depth estimation.

