---
layout: default
title: QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models
---

# QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models

**arXiv**: [2510.14836v1](https://arxiv.org/abs/2510.14836) | [PDF](https://arxiv.org/pdf/2510.14836.pdf)

**ä½œè€…**: Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQDepth-VLAæ¡†æž¶ï¼Œé€šè¿‡é‡åŒ–æ·±åº¦é¢„æµ‹å¢žå¼ºVLAæ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æ·±åº¦é¢„æµ‹` `ç©ºé—´æŽ¨ç†` `é‡åŒ–è¡¨ç¤º` `è¾…åŠ©ç›‘ç£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹ç¼ºä¹å¯¹3Dç»“æž„çš„ç†è§£ï¼Œå½±å“ç²¾ç»†æ“ä½œä»»åŠ¡æ€§èƒ½
2. å¼•å…¥è¾…åŠ©æ·±åº¦é¢„æµ‹ä»»åŠ¡ï¼Œä½¿ç”¨VQ-VAEç¼–ç é‡åŒ–æ·±åº¦å›¾ä»¥å­¦ä¹ æ·±åº¦æ„ŸçŸ¥è¡¨ç¤º
3. åœ¨ä»¿çœŸå’ŒçœŸå®žä»»åŠ¡ä¸­éªŒè¯ï¼Œæå‡ç©ºé—´æŽ¨ç†å’Œæ“ä½œä»»åŠ¡è¡¨çŽ°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
> models to accomplish fine-grained manipulation tasks. However, existing
> approaches often lack the ability to understand and reason over the essential
> 3D structures necessary for precise control. To address this limitation, we
> propose QDepth-VLA, a general framework that augments VLA models with an
> auxiliary depth prediction task. A dedicated depth expert is designed to
> predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
> enabling the model to learn depth-aware representations that capture critical
> geometric cues. Experimental results on the simulation benchmarks and
> real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
> and competitive performance on manipulation tasks.

