---
layout: default
title: pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation
---

# pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation

**arXiv**: [2510.14974v1](https://arxiv.org/abs/2510.14974) | [PDF](https://arxiv.org/pdf/2510.14974.pdf)

**ä½œè€…**: Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºÏ€-Flowç­–ç•¥æ¨¡åž‹ä»¥è§£å†³å°‘æ­¥ç”Ÿæˆä¸­çš„è´¨é‡-å¤šæ ·æ€§æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `å°‘æ­¥ç”Ÿæˆæ¨¡åž‹` `ç­–ç•¥è’¸é¦` `æµåŒ¹é…` `ODEç§¯åˆ†` `è´¨é‡-å¤šæ ·æ€§æƒè¡¡` `æ¨¡ä»¿å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå°‘æ­¥æ‰©æ•£æˆ–æµæ¨¡åž‹è’¸é¦ä¸­æ ¼å¼ä¸åŒ¹é…å¯¼è‡´å¤æ‚è’¸é¦å’Œè´¨é‡-å¤šæ ·æ€§æƒè¡¡
2. æ–¹æ³•è¦ç‚¹ï¼šä¿®æ”¹å­¦ç”Ÿæ¨¡åž‹è¾“å‡ºå±‚ä¸ºæ— ç½‘ç»œç­–ç•¥ï¼Œé¢„æµ‹åŠ¨æ€æµé€Ÿå®žçŽ°å¿«é€ŸODEç§¯åˆ†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet 256Â²ä¸Š1-NFE FIDè¾¾2.85ï¼Œä¼˜äºŽMeanFlowï¼Œ4 NFEsä¸‹ä¿æŒæ•™å¸ˆè´¨é‡å¹¶æå‡å¤šæ ·æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Few-step diffusion or flow-based generative models typically distill a
> velocity-predicting teacher into a student that predicts a shortcut towards
> denoised data. This format mismatch has led to complex distillation procedures
> that often suffer from a quality-diversity trade-off. To address this, we
> propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output
> layer of a student flow model to predict a network-free policy at one timestep.
> The policy then produces dynamic flow velocities at future substeps with
> negligible overhead, enabling fast and accurate ODE integration on these
> substeps without extra network evaluations. To match the policy's ODE
> trajectory to the teacher's, we introduce a novel imitation distillation
> approach, which matches the policy's velocity to the teacher's along the
> policy's trajectory using a standard $\ell_2$ flow matching loss. By simply
> mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable
> training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it
> attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT
> architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves
> substantially better diversity than state-of-the-art few-step methods, while
> maintaining teacher-level quality.

