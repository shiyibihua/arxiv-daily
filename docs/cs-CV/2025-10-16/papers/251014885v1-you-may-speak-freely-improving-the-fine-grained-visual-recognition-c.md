---
layout: default
title: You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction
---

# You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction

**arXiv**: [2510.14885v1](https://arxiv.org/abs/2510.14885) | [PDF](https://arxiv.org/pdf/2510.14885.pdf)

**ä½œè€…**: Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºnlg2choiceæ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä¸­çš„æ€§èƒ½**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç»†ç²’åº¦è§†è§‰åˆ†ç±»` `ç­”æ¡ˆæå–` `çº¦æŸè§£ç ` `é›¶æ ·æœ¬åˆ†ç±»` `æ£€ç´¢æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä¸­ï¼Œå¤„ç†è‡ªç”±å½¢å¼å“åº”å’Œé«˜é€‰é¡¹å¤šé€‰é—®é¢˜æ—¶è¯„ä¼°å›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆå¼€æ”¾æé—®ï¼Œå†æ–‡æœ¬çº¦æŸè§£ç é¢„æµ‹æœ€å¯èƒ½é€‰é¡¹ï¼Œæå‡æ£€ç´¢æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸ƒä¸ªç»†ç²’åº¦è§†è§‰æ•°æ®é›†ä¸Šï¼Œåˆ†ç±»å’Œæ£€ç´¢æ€§èƒ½å‡ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite the renewed interest in zero-shot visual classification due to the
> rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
> free-form responses of auto-regressive models remains a persistent challenge.
> Most existing works focus on language-only tasks or don't consider Multiple
> Choice Questions (MCQs) beyond 5-way options, both of which are critical
> capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
> choice counts are in the hundreds to thousands and the choices are highly
> related. Furthermore, in this highly multi-way MCQ setting it is not clear how
> to extend LLM choice extraction to retrieval-based problems, where computing
> probabilities over the choice set is computationally costly. In this work we
> investigate nlg2choice, a simple two-stage method which first asks the MLLM an
> open-ended question for the task with minimal constraints, then uses text-only
> constrained decoding to predict the most likely choice. In retrieval settings,
> we compute the probability of the constrained response taking that choice with
> an early stopping method to significantly improve throughput. Our results show
> improvement over a suite of seven fine-grained visual datasets when evaluating
> in terms of classification and retrieval, and show that this performance holds
> over the various ways that users of LLMs can implement tasks in natural
> language.

