---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-16
---

# cs.CVï¼ˆ2025-10-16ï¼‰

ğŸ“Š å…± **47** ç¯‡è®ºæ–‡
 | ğŸ”— **12** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (16 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251014349v3-vision-centric-activation-and-coordination-for-multimodal-large-lang.html">Vision-Centric Activation and Coordination for Multimodal Large Language Models</a></td>
  <td>æå‡ºVaCoï¼Œé€šè¿‡è§†è§‰ä¸­å¿ƒæ¿€æ´»ä¸åè°ƒæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14349v3" onclick="toggleFavorite(this, '2510.14349v3', 'Vision-Centric Activation and Coordination for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251016036v1-iad-gpt-advancing-visual-knowledge-in-multimodal-large-language-mode.html">IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection</a></td>
  <td>æå‡ºIAD-GPTï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æå‡å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„è§†è§‰çŸ¥è¯†ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16036v1" onclick="toggleFavorite(this, '2510.16036v1', 'IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251014885v2-you-may-speak-freely-improving-the-fine-grained-visual-recognition-c.html">You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction</a></td>
  <td>æå‡ºnlg2choiceæ–¹æ³•ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰è¯†åˆ«ä¸­çš„åˆ†ç±»ä¸æ£€ç´¢èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14885v2" onclick="toggleFavorite(this, '2510.14885v2', 'You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251014866v1-benchmarking-multimodal-large-language-models-for-face-recognition.html">Benchmarking Multimodal Large Language Models for Face Recognition</a></td>
  <td>ç³»ç»Ÿæ€§è¯„æµ‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨äººè„¸è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14866v1" onclick="toggleFavorite(this, '2510.14866v1', 'Benchmarking Multimodal Large Language Models for Face Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251015162v1-train-a-unified-multimodal-data-quality-classifier-with-synthetic-da.html">Train a Unified Multimodal Data Quality Classifier with Synthetic Data</a></td>
  <td>æå‡ºUniFilterï¼šä¸€ç§åŸºäºåˆæˆæ•°æ®çš„ç»Ÿä¸€å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15162v1" onclick="toggleFavorite(this, '2510.15162v1', 'Train a Unified Multimodal Data Quality Classifier with Synthetic Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251014896v1-leveraging-multimodal-llm-descriptions-of-activity-for-explainable-s.html">Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection</a></td>
  <td>æå‡ºåŸºäºå¤šæ¨¡æ€LLMæè¿°çš„åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œæå‡å¤æ‚å¼‚å¸¸æ£€æµ‹èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14896v1" onclick="toggleFavorite(this, '2510.14896v1', 'Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251014965v1-changinggrounding-3d-visual-grounding-in-changing-scenes.html">ChangingGrounding: 3D Visual Grounding in Changing Scenes</a></td>
  <td>æå‡ºChangingGroundingåŸºå‡†ä¸Mem-ChangingGrounderæ–¹æ³•ï¼Œè§£å†³åŠ¨æ€åœºæ™¯ä¸‹çš„3Dè§†è§‰å®šä½é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14965v1" onclick="toggleFavorite(this, '2510.14965v1', 'ChangingGrounding: 3D Visual Grounding in Changing Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251014672v1-vtimecot-thinking-by-drawing-for-video-temporal-grounding-and-reason.html">VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</a></td>
  <td>VTimeCoTï¼šé€šè¿‡ç»˜åˆ¶è§†é¢‘è¿›åº¦æ¡è¿›è¡Œè§†é¢‘æ—¶åºå®šä½ä¸æ¨ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14672v1" onclick="toggleFavorite(this, '2510.14672v1', 'VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251014532v1-towards-generalist-intelligence-in-dentistry-vision-foundation-model.html">Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</a></td>
  <td>æå‡ºDentVFMï¼šç”¨äºå£è…”é¢Œé¢æ”¾å°„å­¦çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14532v1" onclick="toggleFavorite(this, '2510.14532v1', 'Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251014203v1-joint-modeling-of-big-five-and-hexaco-for-multimodal-apparent-person.html">Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition</a></td>
  <td>æå‡ºBig Fiveå’ŒHEXACOè”åˆå»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºå¤šæ¨¡æ€è¡¨è§‚äººæ ¼ç‰¹è´¨è¯†åˆ«</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14203v1" onclick="toggleFavorite(this, '2510.14203v1', 'Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251015026v1-mobius-big-to-mobile-universal-instance-segmentation-via-multi-modal.html">MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning</a></td>
  <td>MOBIUSï¼šé€šè¿‡å¤šæ¨¡æ€ç“¶é¢ˆèåˆä¸æ ¡å‡†è§£ç å™¨å‰ªæå®ç°Big-to-Mobileé€šç”¨å®ä¾‹åˆ†å‰²</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15026v1" onclick="toggleFavorite(this, '2510.15026v1', 'MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251014904v2-maskcaptioner-learning-to-jointly-segment-and-caption-object-traject.html">MaskCaptioner: Learning to Jointly Segment and Caption Object Trajectories in Videos</a></td>
  <td>æå‡ºMaskCaptionerï¼Œé€šè¿‡è”åˆå­¦ä¹ åˆ†å‰²å’Œæè¿°è§†é¢‘ä¸­çš„ç‰©ä½“è½¨è¿¹ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¯†é›†è§†é¢‘ç‰©ä½“æè¿°ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14904v2" onclick="toggleFavorite(this, '2510.14904v2', 'MaskCaptioner: Learning to Jointly Segment and Caption Object Trajectories in Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251014741v2-dexter-diffusion-guided-explanations-with-textual-reasoning-for-visi.html">DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models</a></td>
  <td>DEXTERï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œæ–‡æœ¬æ¨ç†å®ç°è§†è§‰æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œæ— éœ€æ•°æ®ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14741v2" onclick="toggleFavorite(this, '2510.14741v2', 'DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251014648v1-in-context-learning-with-unpaired-clips-for-instruction-based-video-.html">In-Context Learning with Unpaired Clips for Instruction-based Video Editing</a></td>
  <td>æå‡ºåŸºäºéé…å¯¹è§†é¢‘ç‰‡æ®µçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14648v1" onclick="toggleFavorite(this, '2510.14648v1', 'In-Context Learning with Unpaired Clips for Instruction-based Video Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251014624v1-efficient-video-sampling-pruning-temporally-redundant-tokens-for-fas.html">Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference</a></td>
  <td>æå‡ºé«˜æ•ˆè§†é¢‘é‡‡æ ·EVSï¼Œé€šè¿‡å‰ªææ—¶åºå†—ä½™tokenåŠ é€ŸVLMæ¨ç†ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14624v1" onclick="toggleFavorite(this, '2510.14624v1', 'Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251014304v1-watermarking-for-factuality-guiding-vision-language-models-toward-tr.html">Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding</a></td>
  <td>æå‡ºåŸºäºæ°´å°çš„ä¸‰å±‚å¯¹æ¯”è§£ç æ–¹æ³•ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§å’Œè§†è§‰ groundingã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14304v1" onclick="toggleFavorite(this, '2510.14304v1', 'Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251014954v1-omnimotion-multimodal-motion-generation-with-continuous-masked-autor.html">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a></td>
  <td>OmniMotionï¼šæå‡ºè¿ç»­æ©ç è‡ªå›å½’Transformerï¼Œç”¨äºå¤šæ¨¡æ€å…¨èº«äººä½“è¿åŠ¨ç”Ÿæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14954v1" onclick="toggleFavorite(this, '2510.14954v1', 'OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251014668v2-weckd-weakly-supervised-chained-distillation-network-for-efficient-m.html">WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging</a></td>
  <td>æå‡ºWeCKDï¼šä¸€ç§å¼±ç›‘ç£é“¾å¼è’¸é¦ç½‘ç»œï¼Œç”¨äºé«˜æ•ˆå¤šæ¨¡æ€åŒ»å­¦å½±åƒåˆ†æã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14668v2" onclick="toggleFavorite(this, '2510.14668v2', 'WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251014605v2-knowledge-based-visual-question-answer-with-multimodal-processing-re.html">Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering</a></td>
  <td>æå‡ºWiki-PRFæ¡†æ¶ï¼Œè§£å†³çŸ¥è¯†åº“VQAä¸­å¤šæ¨¡æ€æŸ¥è¯¢è´¨é‡å’Œæ£€ç´¢ç»“æœç›¸å…³æ€§é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14605v2" onclick="toggleFavorite(this, '2510.14605v2', 'Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251014819v2-capturing-context-aware-route-choice-semantics-for-trajectory-repres.html">Capturing Context-Aware Route Choice Semantics for Trajectory Representation Learning</a></td>
  <td>æå‡ºCOREæ¡†æ¶ï¼Œèåˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è·¯å¾„é€‰æ‹©è¯­ä¹‰ï¼Œæå‡è½¨è¿¹è¡¨ç¤ºå­¦ä¹ æ•ˆæœ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14819v2" onclick="toggleFavorite(this, '2510.14819v2', 'Capturing Context-Aware Route Choice Semantics for Trajectory Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251015050v1-directional-reasoning-injection-for-fine-tuning-mllms.html">Directional Reasoning Injection for Fine-Tuning MLLMs</a></td>
  <td>æå‡ºDRIFTï¼Œé€šè¿‡æ¢¯åº¦ç©ºé—´æ³¨å…¥æ–¹å‘æ€§æ¨ç†çŸ¥è¯†ï¼Œé«˜æ•ˆå¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15050v1" onclick="toggleFavorite(this, '2510.15050v1', 'Directional Reasoning Injection for Fine-Tuning MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251015040v1-composition-grounded-instruction-synthesis-for-visual-reasoning.html">Composition-Grounded Instruction Synthesis for Visual Reasoning</a></td>
  <td>æå‡ºCOGSæ¡†æ¶ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15040v1" onclick="toggleFavorite(this, '2510.15040v1', 'Composition-Grounded Instruction Synthesis for Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251014374v1-spatial-preference-rewarding-for-mllms-spatial-understanding.html">Spatial Preference Rewarding for MLLMs Spatial Understanding</a></td>
  <td>æå‡ºç©ºé—´åå¥½å¥–åŠ±SPRï¼Œæå‡MLLMåœ¨ç»†ç²’åº¦ç©ºé—´ç†è§£ä¸Šçš„èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14374v1" onclick="toggleFavorite(this, '2510.14374v1', 'Spatial Preference Rewarding for MLLMs Spatial Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251014955v2-realdpo-real-or-not-real-that-is-the-preference.html">RealDPO: Real or Not Real, that is the Preference</a></td>
  <td>RealDPOï¼šåˆ©ç”¨çœŸå®æ•°æ®åå¥½å­¦ä¹ ï¼Œæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨çœŸå®æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14955v2" onclick="toggleFavorite(this, '2510.14955v2', 'RealDPO: Real or Not Real, that is the Preference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251014977v1-terra-explorable-native-3d-world-model-with-point-latents.html">Terra: Explorable Native 3D World Model with Point Latents</a></td>
  <td>Terraï¼šåŸºäºç‚¹æ½œå˜é‡çš„å¯æ¢ç´¢åŸç”Ÿ3Dä¸–ç•Œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14977v1" onclick="toggleFavorite(this, '2510.14977v1', 'Terra: Explorable Native 3D World Model with Point Latents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251014383v2-drbd-mamba-for-robust-and-efficient-brain-tumor-segmentation-with-an.html">DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights</a></td>
  <td>æå‡ºDRBD-Mambaæ¨¡å‹ï¼Œç”¨äºé²æ£’é«˜æ•ˆçš„è„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå¹¶æä¾›åˆ†ææ€§è§è§£</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14383v2" onclick="toggleFavorite(this, '2510.14383v2', 'DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251015041v1-generalized-dynamics-generation-towards-scannable-physical-world-mod.html">Generalized Dynamics Generation towards Scannable Physical World Model</a></td>
  <td>GDGenï¼šåŸºäºåŠ¿èƒ½çš„é€šç”¨åŠ¨åŠ›å­¦ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºå¯æ‰«æç‰©ç†ä¸–ç•Œå»ºæ¨¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15041v1" onclick="toggleFavorite(this, '2510.15041v1', 'Generalized Dynamics Generation towards Scannable Physical World Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251014516v2-vision-mamba-for-permeability-prediction-of-porous-media.html">Vision Mamba for Permeability Prediction of Porous Media</a></td>
  <td>æå‡ºåŸºäºVision Mambaçš„å¤šå­”ä»‹è´¨æ¸—é€ç‡é¢„æµ‹æ¨¡å‹ï¼Œæå‡è®¡ç®—æ•ˆç‡å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14516v2" onclick="toggleFavorite(this, '2510.14516v2', 'Vision Mamba for Permeability Prediction of Porous Media')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251014256v2-identity-grpo-optimizing-multi-human-identity-preserving-video-gener.html">Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning</a></td>
  <td>æå‡ºIdentity-GRPOï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤šäººè§†é¢‘ç”Ÿæˆä¸­çš„èº«ä»½ä¿æŒé—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14256v2" onclick="toggleFavorite(this, '2510.14256v2', 'Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251014862v1-multi-modal-video-data-pipelines-for-machine-learning-with-minimal-h.html">Multi-modal video data-pipelines for machine learning with minimal human supervision</a></td>
  <td>æå‡ºä¸€ç§åŸºäºå¼±ç›‘ç£å¤šæ¨¡æ€è§†é¢‘æ•°æ®ç®¡é“çš„æœºå™¨å­¦ä¹ æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14862v1" onclick="toggleFavorite(this, '2510.14862v1', 'Multi-modal video data-pipelines for machine learning with minimal human supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251014657v2-decorrelation-speeds-up-vision-transformers.html">Decorrelation Speeds Up Vision Transformers</a></td>
  <td>æå‡ºDBP-MAEåŠ é€ŸViTé¢„è®­ç»ƒï¼Œé™ä½è®¡ç®—æˆæœ¬å’Œç¢³æ’æ”¾ï¼Œæå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14657v2" onclick="toggleFavorite(this, '2510.14657v2', 'Decorrelation Speeds Up Vision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/251014792v1-cot-pl-visual-chain-of-thought-reasoning-meets-pseudo-labeling-for-o.html">CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection</a></td>
  <td>æå‡ºCoT-PLæ¡†æ¶ï¼Œé€šè¿‡è§†è§‰é“¾å¼æ¨ç†å’Œä¼ªæ ‡ç­¾æå‡å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14792v1" onclick="toggleFavorite(this, '2510.14792v1', 'CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251014270v3-gaussmart-enhanced-3d-reconstruction-through-2d-foundation-models-an.html">GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</a></td>
  <td>GauSSmartï¼šèåˆ2DåŸºç¡€æ¨¡å‹ä¸å‡ ä½•æ»¤æ³¢å¢å¼º3Dé«˜æ–¯æº…å°„é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14270v3" onclick="toggleFavorite(this, '2510.14270v3', 'GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251014564v1-balancegs-algorithm-system-co-design-for-efficient-3d-gaussian-splat.html">BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU</a></td>
  <td>BalanceGSï¼šé¢å‘GPUçš„3Dé«˜æ–¯æº…å°„é«˜æ•ˆè®­ç»ƒçš„ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14564v1" onclick="toggleFavorite(this, '2510.14564v1', 'BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251015072v1-salon3r-structure-aware-long-term-generalizable-3d-reconstruction-fr.html">SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images</a></td>
  <td>SaLon3Rï¼šç»“æ„æ„ŸçŸ¥çš„é•¿æœŸé€šç”¨3Dé‡å»ºï¼Œè§£å†³å†—ä½™å’Œå‡ ä½•ä¸ä¸€è‡´é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15072v1" onclick="toggleFavorite(this, '2510.15072v1', 'SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/251014705v1-leveraging-learned-image-prior-for-3d-gaussian-compression.html">Leveraging Learned Image Prior for 3D Gaussian Compression</a></td>
  <td>åˆ©ç”¨å›¾åƒå…ˆéªŒçŸ¥è¯†æå‡3Dé«˜æ–¯å‹ç¼©ç‡ä¸æ¸²æŸ“è´¨é‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14705v1" onclick="toggleFavorite(this, '2510.14705v1', 'Leveraging Learned Image Prior for 3D Gaussian Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251014960v1-c4d-4d-made-from-3d-through-dual-correspondences.html">C4D: 4D Made from 3D through Dual Correspondences</a></td>
  <td>C4Dï¼šé€šè¿‡åŒé‡å¯¹åº”å…³ç³»ä»3Dé‡å»º4DåŠ¨æ€åœºæ™¯</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14960v1" onclick="toggleFavorite(this, '2510.14960v1', 'C4D: 4D Made from 3D through Dual Correspondences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251014179v1-virtually-being-customizing-camera-controllable-video-diffusion-mode.html">Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures</a></td>
  <td>æå‡ºåŸºäºå¤šè§†è§’è¡¨æ¼”æ•æ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å®šåˆ¶æ¡†æ¶ï¼Œå®ç°ç›¸æœºå¯æ§å’Œè§’è‰²ä¸€è‡´æ€§ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14179v1" onclick="toggleFavorite(this, '2510.14179v1', 'Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/251014588v2-stance-motion-coherent-video-generation-via-sparse-to-dense-anchored.html">STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding</a></td>
  <td>STANCEï¼šé€šè¿‡ç¨€ç–åˆ°ç¨ å¯†é”šå®šç¼–ç å®ç°è¿åŠ¨è¿è´¯çš„è§†é¢‘ç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14588v2" onclick="toggleFavorite(this, '2510.14588v2', 'STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/251014958v1-mathcanvas-intrinsic-visual-chain-of-thought-for-multimodal-mathemat.html">MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a></td>
  <td>MathCanvasï¼šç”¨äºå¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„å†…åœ¨è§†è§‰æ€ç»´é“¾</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14958v1" onclick="toggleFavorite(this, '2510.14958v1', 'MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/251014836v1-qdepth-vla-quantized-depth-prediction-as-auxiliary-supervision-for-v.html">QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</a></td>
  <td>QDepth-VLAï¼šåˆ©ç”¨é‡åŒ–æ·±åº¦é¢„æµ‹è¾…åŠ©è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæå‡ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14836v1" onclick="toggleFavorite(this, '2510.14836v1', 'QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/251015018v1-urbanverse-scaling-urban-simulation-by-watching-city-tour-videos.html">UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</a></td>
  <td>UrbanVerseï¼šé€šè¿‡åŸå¸‚æ¼«æ¸¸è§†é¢‘æ‰©å±•åŸå¸‚æ¨¡æ‹Ÿè§„æ¨¡ï¼Œç”¨äºå…·èº«æ™ºèƒ½ä½“è®­ç»ƒã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15018v1" onclick="toggleFavorite(this, '2510.15018v1', 'UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>43</td>
  <td><a href="./papers/251014661v1-eurominenet-a-multitemporal-sentinel-2-benchmark-for-spatiotemporal-.html">EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)</a></td>
  <td>EuroMineNetï¼šæ¬§ç›Ÿå¤šæ—¶ç›¸Sentinel-2çŸ¿åŒºæ—¶ç©ºè¶³è¿¹åˆ†æåŸºå‡†æ•°æ®é›†</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14661v1" onclick="toggleFavorite(this, '2510.14661v1', 'EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/251014245v1-event-interval-modulation-a-novel-scheme-for-event-based-optical-cam.html">Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication</a></td>
  <td>æå‡ºäº‹ä»¶é—´éš”è°ƒåˆ¶(EIM)æ–¹æ¡ˆï¼Œæå‡äº‹ä»¶ç›¸æœºå…‰é€šä¿¡çš„ä¼ è¾“é€Ÿç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14245v1" onclick="toggleFavorite(this, '2510.14245v1', 'Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>45</td>
  <td><a href="./papers/251014874v1-touch-text-guided-controllable-generation-of-free-form-hand-object-i.html">TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</a></td>
  <td>æå‡ºTOUCHæ¡†æ¶ï¼Œå®ç°æ–‡æœ¬å¼•å¯¼çš„å¯æ§è‡ªç”±æ‰‹éƒ¨-ç‰©ä½“äº¤äº’ç”Ÿæˆã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14874v1" onclick="toggleFavorite(this, '2510.14874v1', 'TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/251014427v1-deep-compositional-phase-diffusion-for-long-motion-sequence-generati.html">Deep Compositional Phase Diffusion for Long Motion Sequence Generation</a></td>
  <td>æå‡ºç»„åˆç›¸ä½æ‰©æ•£æ–¹æ³•ï¼Œè§£å†³é•¿è¿åŠ¨åºåˆ—ç”Ÿæˆä¸­ç‰‡æ®µè¡”æ¥ä¸æµç•…é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.14427v1" onclick="toggleFavorite(this, '2510.14427v1', 'Deep Compositional Phase Diffusion for Long Motion Sequence Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/251015060v1-a-solution-to-generalized-learning-from-small-training-sets-found-in.html">A solution to generalized learning from small training sets found in everyday infant experiences</a></td>
  <td>åˆ†æå©´å„¿è§†è§‰ç»éªŒçš„â€œå—çŠ¶â€ç›¸ä¼¼æ€§ï¼Œæå‡å°æ ·æœ¬å­¦ä¹ æ³›åŒ–èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15060v1" onclick="toggleFavorite(this, '2510.15060v1', 'A solution to generalized learning from small training sets found in everyday infant experiences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)