---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-13
---

# cs.CVï¼ˆ2025-10-13ï¼‰

ğŸ“Š å…± **51** ç¯‡è®ºæ–‡
 | ğŸ”— **11** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (22 ğŸ”—7)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (22 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251011496v2-andesvl-technical-report-an-efficient-mobile-side-multimodal-large-l.html">AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model</a></td>
  <td>AndesVLï¼šé¢å‘ç§»åŠ¨ç«¯çš„é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11496v2" onclick="toggleFavorite(this, '2510.11496v2', 'AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251011341v2-internsvg-towards-unified-svg-tasks-with-multimodal-large-language-m.html">InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models</a></td>
  <td>InternSVGï¼šåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°ç»Ÿä¸€çš„SVGä»»åŠ¡å¤„ç†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11341v2" onclick="toggleFavorite(this, '2510.11341v2', 'InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251011190v3-flexac-towards-flexible-control-of-associative-reasoning-in-multimod.html">FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models</a></td>
  <td>FlexACï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­è”æƒ³æ¨ç†çš„çµæ´»æ§åˆ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11190v3" onclick="toggleFavorite(this, '2510.11190v3', 'FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251010991v1-a-survey-on-agentic-multimodal-large-language-models.html">A Survey on Agentic Multimodal Large Language Models</a></td>
  <td>ç»¼è¿°Agenticå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ¢ç´¢è‡ªä¸»æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åº”ç”¨ä¸å‘å±•ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10991v1" onclick="toggleFavorite(this, '2510.10991v1', 'A Survey on Agentic Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251011178v1-blend-vis-benchmarking-multimodal-cultural-understanding-in-vision-l.html">BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models</a></td>
  <td>BLEnD-Visï¼šæ„å»ºå¤šæ¨¡æ€æ–‡åŒ–ç†è§£åŸºå‡†ï¼Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–çŸ¥è¯†é²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11178v1" onclick="toggleFavorite(this, '2510.11178v1', 'BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251011718v1-codeplot-cot-mathematical-visual-reasoning-by-thinking-with-code-dri.html">CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images</a></td>
  <td>æå‡ºCodePlot-CoTï¼Œé€šè¿‡ä»£ç é©±åŠ¨å›¾åƒçš„æ€ç»´é“¾è§£å†³æ•°å­¦è§†è§‰æ¨ç†éš¾é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11718v1" onclick="toggleFavorite(this, '2510.11718v1', 'CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251011606v1-expvid-a-benchmark-for-experiment-video-understanding-reasoning.html">ExpVid: A Benchmark for Experiment Video Understanding & Reasoning</a></td>
  <td>ExpVidï¼šç”¨äºå®éªŒè§†é¢‘ç†è§£ä¸æ¨ç†çš„åŸºå‡†æ•°æ®é›†ï¼ŒæŒ‘æˆ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦å®éªŒä¸­çš„åº”ç”¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11606v1" onclick="toggleFavorite(this, '2510.11606v1', 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251011579v1-ms-mix-unveiling-the-power-of-mixup-for-multimodal-sentiment-analysi.html">MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis</a></td>
  <td>æå‡ºMS-Mixä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11579v1" onclick="toggleFavorite(this, '2510.11579v1', 'MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251011576v2-benchmarking-foundation-models-for-hyperspectral-image-classificatio.html">Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping</a></td>
  <td>è¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨ hyperspectral å›¾åƒåˆ†ç±»ä¸­çš„æ€§èƒ½ï¼Œåº”ç”¨äºè°·ç±»ä½œç‰©ç±»å‹è¯†åˆ«ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11576v2" onclick="toggleFavorite(this, '2510.11576v2', 'Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251011553v2-how-many-samples-to-label-for-an-application-given-a-foundation-mode.html">How many samples to label for an application given a foundation model? Chest X-ray classification study</a></td>
  <td>ç ”ç©¶èƒ¸éƒ¨Xå…‰ç‰‡åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å‡å°‘æ ‡æ³¨æ ·æœ¬éœ€æ±‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11553v2" onclick="toggleFavorite(this, '2510.11553v2', 'How many samples to label for an application given a foundation model? Chest X-ray classification study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251011260v1-a-large-language-model-assisted-automated-scale-bar-detection-and-ex.html">A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images</a></td>
  <td>æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ‰«æç”µé•œå›¾åƒæ¯”ä¾‹å°ºè‡ªåŠ¨æ£€æµ‹ä¸æå–æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11260v1" onclick="toggleFavorite(this, '2510.11260v1', 'A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251011173v2-coprs-learning-positional-prior-from-chain-of-thought-for-reasoning-.html">CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation</a></td>
  <td>CoPRSï¼šæå‡ºåŸºäºæ€ç»´é“¾çš„ä½ç½®å…ˆéªŒå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæå‡æ¨ç†åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ä¸å¯è§£é‡Šæ€§</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11173v2" onclick="toggleFavorite(this, '2510.11173v2', 'CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251011115v1-connecting-giants-synergistic-knowledge-transfer-of-large-multimodal.html">Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning</a></td>
  <td>æå‡ºSynTransæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ååŒçŸ¥è¯†è¿ç§»æå‡å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11115v1" onclick="toggleFavorite(this, '2510.11115v1', 'Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251010986v1-mixup-helps-understanding-multimodal-video-better.html">Mixup Helps Understanding Multimodal Video Better</a></td>
  <td>æå‡ºå¤šæ¨¡æ€Mixupæ–¹æ³•ï¼Œæå‡å¤šæ¨¡æ€è§†é¢‘ç†è§£æ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10986v1" onclick="toggleFavorite(this, '2510.10986v1', 'Mixup Helps Understanding Multimodal Video Better')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251011647v1-ivebench-modern-benchmark-suite-for-instruction-guided-video-editing.html">IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment</a></td>
  <td>IVEBenchï¼šç”¨äºæŒ‡ä»¤å¼•å¯¼è§†é¢‘ç¼–è¾‘è¯„ä¼°çš„ç°ä»£åŸºå‡†å¥—ä»¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11647v1" onclick="toggleFavorite(this, '2510.11647v1', 'IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251011549v1-odi-bench-can-mllms-understand-immersive-omnidirectional-environment.html">ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?</a></td>
  <td>æå‡ºODI-Benchï¼Œè¯„ä¼°MLLMåœ¨å…¨æ™¯å›¾åƒç†è§£ä¸­çš„èƒ½åŠ›å¹¶æå‡ºOmni-CoTæ–¹æ³•ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11549v1" onclick="toggleFavorite(this, '2510.11549v1', 'ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251011026v1-gir-bench-versatile-benchmark-for-generating-images-with-reasoning.html">GIR-Bench: Versatile Benchmark for Generating Images with Reasoning</a></td>
  <td>æå‡ºGIR-Benchä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ä¸è¶³é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11026v1" onclick="toggleFavorite(this, '2510.11026v1', 'GIR-Bench: Versatile Benchmark for Generating Images with Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251011012v1-coco-tree-compositional-hierarchical-concept-trees-for-enhanced-reas.html">COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models</a></td>
  <td>æå‡ºCOCO-Treeï¼Œåˆ©ç”¨ç¥ç»ç¬¦å·æ¦‚å¿µæ ‘å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç»„åˆæ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11012v1" onclick="toggleFavorite(this, '2510.11012v1', 'COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251011631v1-evocad-evolutionary-cad-code-generation-with-vision-language-models.html">EvoCAD: Evolutionary CAD Code Generation with Vision Language Models</a></td>
  <td>EvoCADï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸è¿›åŒ–ç®—æ³•ç”ŸæˆCADä»£ç </td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11631v1" onclick="toggleFavorite(this, '2510.11631v1', 'EvoCAD: Evolutionary CAD Code Generation with Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251011028v1-enhancing-zero-shot-anomaly-detection-clip-sam-collaboration-with-ca.html">Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts</a></td>
  <td>æå‡ºCLIP-SAMååŒä¸çº§è”æç¤ºçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæå‡é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11028v1" onclick="toggleFavorite(this, '2510.11028v1', 'Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251010969v1-iut-plug-a-plug-in-tool-for-interleaved-image-text-generation.html">IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation</a></td>
  <td>æå‡ºIUT-Plugæ’ä»¶ï¼Œé€šè¿‡æ˜¾å¼ç»“æ„åŒ–æ¨ç†å¢å¼ºå¤šæ¨¡æ€å›¾æ–‡ç”Ÿæˆä¸­ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10969v1" onclick="toggleFavorite(this, '2510.10969v1', 'IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251010921v2-fg-clip-2-a-bilingual-fine-grained-vision-language-alignment-model.html">FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model</a></td>
  <td>æå‡ºFG-CLIP 2ï¼Œç”¨äºæå‡è‹±æ±‰åŒè¯­ç¯å¢ƒä¸‹çš„ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10921v2" onclick="toggleFavorite(this, '2510.10921v2', 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251011649v1-physic-physically-plausible-3d-human-scene-interaction-and-contact-f.html">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a></td>
  <td>PhySICï¼šä»å•å¼ å›¾åƒé‡å»ºç‰©ç†ä¸Šåˆç†çš„3Däºº-åœºæ™¯äº¤äº’ä¸æ¥è§¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11649v1" onclick="toggleFavorite(this, '2510.11649v1', 'PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251011473v2-va-gs-enhancing-the-geometric-representation-of-gaussian-splatting-v.html">VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment</a></td>
  <td>VA-GSï¼šé€šè¿‡è§†è§’å¯¹é½å¢å¼ºé«˜æ–¯æº…å°„çš„å‡ ä½•è¡¨ç¤ºï¼Œæå‡è¡¨é¢é‡å»ºç²¾åº¦ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11473v2" onclick="toggleFavorite(this, '2510.11473v2', 'VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251011387v2-materialrefgs-reflective-gaussian-splatting-with-multi-view-consiste.html">MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference</a></td>
  <td>æå‡ºMaterialRefGSï¼Œé€šè¿‡å¤šè§†è§’ä¸€è‡´æè´¨æ¨æ–­å®ç°é«˜è´¨é‡åå°„é«˜æ–¯æº…å°„æ¸²æŸ“</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11387v2" onclick="toggleFavorite(this, '2510.11387v2', 'MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251011717v1-ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-even.html">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></td>
  <td>æå‡ºEv4DGSä»¥è§£å†³å•ç›®äº‹ä»¶æµä¸‹éåˆšæ€§ç‰©ä½“çš„æ–°è§†è§’æ¸²æŸ“é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11717v1" onclick="toggleFavorite(this, '2510.11717v1', 'Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251011305v1-evaluating-the-effects-of-preprocessing-method-selection-and-hyperpa.html">Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation</a></td>
  <td>ç ”ç©¶é¢„å¤„ç†ã€æ–¹æ³•é€‰æ‹©å’Œè¶…å‚æ•°è°ƒæ•´å¯¹SARæ´ªæ°´åˆ¶å›¾å’Œæ°´æ·±ä¼°è®¡çš„å½±å“</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11305v1" onclick="toggleFavorite(this, '2510.11305v1', 'Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251010933v1-dkpmv-dense-keypoints-fusion-from-multi-view-rgb-frames-for-6d-pose-.html">DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects</a></td>
  <td>DKPMVï¼šåŸºäºå¤šè§†è§’RGBå›¾åƒçš„ç¨ å¯†å…³é”®ç‚¹èåˆï¼Œç”¨äºæ— çº¹ç†ç‰©ä½“6Dä½å§¿ä¼°è®¡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10933v1" onclick="toggleFavorite(this, '2510.10933v1', 'DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251011567v1-a-framework-for-low-effort-training-data-generation-for-urban-semant.html">A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation</a></td>
  <td>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„ä½æˆæœ¬è®­ç»ƒæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæå‡åŸå¸‚è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11567v1" onclick="toggleFavorite(this, '2510.11567v1', 'A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251011565v1-snap-towards-segmenting-anything-in-any-point-cloud.html">SNAP: Towards Segmenting Anything in Any Point Cloud</a></td>
  <td>æå‡ºSNAPï¼Œä¸€ä¸ªé€šç”¨çš„ç‚¹äº‘äº¤äº’å¼åˆ†å‰²æ¨¡å‹ï¼Œæ”¯æŒè·¨åŸŸå’Œå¤šç§æç¤ºæ–¹å¼ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11565v1" onclick="toggleFavorite(this, '2510.11565v1', 'SNAP: Towards Segmenting Anything in Any Point Cloud')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251011520v2-mmwalk-towards-multi-modal-multi-view-walking-assistance.html">mmWalk: Towards Multi-modal Multi-view Walking Assistance</a></td>
  <td>mmWalkï¼šé¢å‘ç›²äººæˆ–ä½è§†åŠ›äººç¾¤çš„å¤šæ¨¡æ€å¤šè§†è§’æ­¥è¡Œè¾…åŠ©æ•°æ®é›†ä¸æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11520v2" onclick="toggleFavorite(this, '2510.11520v2', 'mmWalk: Towards Multi-modal Multi-view Walking Assistance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251011340v2-react3d-recovering-articulations-for-interactive-physical-3d-scenes.html">REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</a></td>
  <td>REACT3Dï¼šç”¨äºäº¤äº’å¼ç‰©ç†3Dåœºæ™¯çš„é“°æ¥ç»“æ„æ¢å¤æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11340v2" onclick="toggleFavorite(this, '2510.11340v2', 'REACT3D: Recovering Articulations for Interactive Physical 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/251011176v1-g2lfrom-giga-scale-to-cancer-specific-large-scale-pathology-foundati.html">G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</a></td>
  <td>æå‡ºG2Læ¡†æ¶ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦å°†åƒäº¿çº§ç—…ç†æ¨¡å‹èƒ½åŠ›è¿ç§»è‡³ç™Œç—‡ç‰¹å¼‚æ€§å¤§å‹æ¨¡å‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11176v1" onclick="toggleFavorite(this, '2510.11176v1', 'G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251011027v1-vlaser-vision-language-action-model-with-synergistic-embodied-reason.html">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</a></td>
  <td>Vlaserï¼šæå‡ºå…·æœ‰ååŒå…·èº«æ¨ç†èƒ½åŠ›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå¼¥åˆVLMæ¨ç†ä¸VLAç­–ç•¥å­¦ä¹ çš„é¸¿æ²Ÿã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11027v1" onclick="toggleFavorite(this, '2510.11027v1', 'Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251011017v1-high-resolution-spatiotemporal-modeling-with-global-local-state-spac.html">High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation</a></td>
  <td>æå‡ºåŸºäºå…¨å±€-å±€éƒ¨çŠ¶æ€ç©ºé—´æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡æ—¶ç©ºå»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºè§†é¢‘äººä½“å§¿æ€ä¼°è®¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11017v1" onclick="toggleFavorite(this, '2510.11017v1', 'High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/251011204v1-class-prototypes-based-contrastive-learning-for-classifying-multi-la.html">Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos</a></td>
  <td>æå‡ºåŸºäºç±»åŸå‹å¯¹æ¯”å­¦ä¹ çš„å¤šæ ‡ç­¾ç»†ç²’åº¦æ•™è‚²è§†é¢‘åˆ†ç±»æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11204v1" onclick="toggleFavorite(this, '2510.11204v1', 'Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251010973v1-chart-rvr-reinforcement-learning-with-verifiable-rewards-for-explain.html">Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning</a></td>
  <td>æå‡ºChart-RVRæ¡†æ¶ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æå‡å›¾è¡¨æ¨ç†çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10973v1" onclick="toggleFavorite(this, '2510.10973v1', 'Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251011369v1-reasoning-as-representation-rethinking-visual-reinforcement-learning.html">Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment</a></td>
  <td>æå‡ºRALIç®—æ³•ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹é½å›¾åƒå’Œæ–‡æœ¬è¡¨å¾ï¼Œå®ç°é«˜æ•ˆå›¾åƒè´¨é‡è¯„ä¼°ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11369v1" onclick="toggleFavorite(this, '2510.11369v1', 'Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/251010889v1-topological-alignment-of-shared-vision-language-embedding-space.html">Topological Alignment of Shared Vision-Language Embedding Space</a></td>
  <td>æå‡ºToMCLIPï¼Œé€šè¿‡æ‹“æ‰‘å¯¹é½å¢å¼ºå¤šè¯­è¨€è§†è§‰-è¯­è¨€æ¨¡å‹çš„å…±äº«åµŒå…¥ç©ºé—´ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10889v1" onclick="toggleFavorite(this, '2510.10889v1', 'Topological Alignment of Shared Vision-Language Embedding Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/251011090v1-source-free-object-detection-with-detection-transformer.html">Source-Free Object Detection with Detection Transformer</a></td>
  <td>æå‡ºFRANCKæ¡†æ¶ï¼Œé€šè¿‡æŸ¥è¯¢ä¸­å¿ƒç‰¹å¾å¢å¼ºå®ç°DETRçš„æ— æºåŸŸç›®æ ‡æ£€æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11090v1" onclick="toggleFavorite(this, '2510.11090v1', 'Source-Free Object Detection with Detection Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/251011509v1-situat3dchange-situated-3d-change-understanding-dataset-for-multimod.html">Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model</a></td>
  <td>æå‡ºSituat3DChangeæ•°æ®é›†ï¼Œç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç†è§£æƒ…å¢ƒåŒ–3Dåœºæ™¯å˜åŒ–</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11509v1" onclick="toggleFavorite(this, '2510.11509v1', 'Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/251010868v1-fasthmr-accelerating-human-mesh-recovery-via-token-and-layer-merging.html">FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding</a></td>
  <td>FastHMRï¼šé€šè¿‡Tokenå’Œå±‚åˆå¹¶åŠæ‰©æ•£è§£ç åŠ é€Ÿäººä½“ç½‘æ ¼é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.10868v1" onclick="toggleFavorite(this, '2510.10868v1', 'FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/251011605v1-ace-g-improving-generalization-of-scene-coordinate-regression-throug.html">ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training</a></td>
  <td>ACE-Gï¼šé€šè¿‡æŸ¥è¯¢é¢„è®­ç»ƒæå‡åœºæ™¯åæ ‡å›å½’çš„æ³›åŒ–èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11605v1" onclick="toggleFavorite(this, '2510.11605v1', 'ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/251011417v1-robust-ego-exo-correspondence-with-long-term-memory.html">Robust Ego-Exo Correspondence with Long-Term Memory</a></td>
  <td>æå‡ºåŸºäºé•¿æ—¶è®°å¿†çš„LM-EECæ¡†æ¶ï¼Œè§£å†³Ego-Exoè§†è§’å¯¹åº”ä¸­çš„ç‰¹å¾èåˆå’Œè®°å¿†å®¹é‡é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11417v1" onclick="toggleFavorite(this, '2510.11417v1', 'Robust Ego-Exo Correspondence with Long-Term Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>45</td>
  <td><a href="./papers/251011687v1-beyond-templates-category-agnostic-object-pose-size-and-shape-estima.html">Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View</a></td>
  <td>æå‡ºä¸€ç§ç±»åˆ«æ— å…³çš„å•è§†å›¾ç‰©ä½“ä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ä¼°è®¡æ¡†æ¶ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11687v1" onclick="toggleFavorite(this, '2510.11687v1', 'Beyond &#39;Templates&#39;: Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/251011096v1-codefend-cross-modal-collaborative-defense-via-diffusion-purificatio.html">CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization</a></td>
  <td>æå‡ºCoDefendï¼Œé€šè¿‡æ‰©æ•£å‡€åŒ–å’Œæç¤ºä¼˜åŒ–ååŒé˜²å¾¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11096v1" onclick="toggleFavorite(this, '2510.11096v1', 'CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/251011050v1-zero-shot-face-editing-via-id-attribute-decoupled-inversion.html">Zero-shot Face Editing via ID-Attribute Decoupled Inversion</a></td>
  <td>æå‡ºåŸºäºID-å±æ€§è§£è€¦åæ¼”çš„é›¶æ ·æœ¬äººè„¸ç¼–è¾‘æ–¹æ³•ï¼Œè§£å†³IDä¿æŒå’Œç»“æ„ä¸€è‡´æ€§é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11050v1" onclick="toggleFavorite(this, '2510.11050v1', 'Zero-shot Face Editing via ID-Attribute Decoupled Inversion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>48</td>
  <td><a href="./papers/251011107v1-momaps-semantics-aware-scene-motion-generation-with-motion-maps.html">MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps</a></td>
  <td>æå‡ºåŸºäºè¿åŠ¨åœ°å›¾ï¼ˆMoMapï¼‰çš„è¯­ä¹‰æ„ŸçŸ¥åœºæ™¯è¿åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œå®ç°ä»å•å¼ å›¾åƒé¢„æµ‹æœªæ¥3Dåœºæ™¯è¿åŠ¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11107v1" onclick="toggleFavorite(this, '2510.11107v1', 'MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/251011538v2-massive-activations-are-the-key-to-local-detail-synthesis-in-diffusi.html">Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers</a></td>
  <td>æå‡ºDetail Guidanceï¼Œé€šè¿‡è°ƒæ§Diffusion Transformerä¸­çš„å¤§è§„æ¨¡æ¿€æ´»æå‡å›¾åƒç»†èŠ‚ç”Ÿæˆè´¨é‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11538v2" onclick="toggleFavorite(this, '2510.11538v2', 'Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/251011512v2-likephys-evaluating-intuitive-physics-understanding-in-video-diffusi.html">LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference</a></td>
  <td>æå‡ºLikePhysï¼Œé€šè¿‡ä¼¼ç„¶åå¥½è¯„ä¼°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç›´è§‚ç‰©ç†ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11512v2" onclick="toggleFavorite(this, '2510.11512v2', 'LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>51</td>
  <td><a href="./papers/251011112v1-multimodal-disease-progression-modeling-via-spatiotemporal-disentang.html">Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment</a></td>
  <td>DiProï¼šæ—¶ç©ºè§£è€¦ä¸å¤šå°ºåº¦å¯¹é½çš„å¤šæ¨¡æ€ç–¾ç—…è¿›å±•å»ºæ¨¡æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.11112v1" onclick="toggleFavorite(this, '2510.11112v1', 'Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)