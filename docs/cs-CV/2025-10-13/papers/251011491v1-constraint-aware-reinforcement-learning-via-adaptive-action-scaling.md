---
layout: default
title: Constraint-Aware Reinforcement Learning via Adaptive Action Scaling
---

# Constraint-Aware Reinforcement Learning via Adaptive Action Scaling

**arXiv**: [2510.11491v1](https://arxiv.org/abs/2510.11491) | [PDF](https://arxiv.org/pdf/2510.11491.pdf)

**ä½œè€…**: Murad Dawood, Usama Ahmed Siddiquie, Shahram Khorshidi, Maren Bennewitz

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”åŠ¨ä½œç¼©æ”¾æ–¹æ³•ä»¥è§£å†³å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­çš„çº¦æŸå†²çªé—®é¢˜**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `åŠ¨ä½œç¼©æ”¾` `çº¦æŸä¼˜åŒ–` `æ¨¡å—åŒ–è°ƒèŠ‚å™¨` `ç¨€ç–æˆæœ¬`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å› ç›®æ ‡å†²çªæˆ–éœ€å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šæˆ–çº¦æŸè¿å
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ¨¡å—åŒ–æˆæœ¬æ„ŸçŸ¥è°ƒèŠ‚å™¨ï¼Œå¹³æ»‘ç¼©æ”¾åŠ¨ä½œä»¥æœ€å°åŒ–çº¦æŸè¿å
3. å®žéªŒæ•ˆæžœï¼šåœ¨Safety Gymä»»åŠ¡ä¸­ï¼Œçº¦æŸè¿åå‡å°‘126å€ï¼Œå›žæŠ¥æå‡è¶…10å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that
> arise from exploration during training by reducing constraint violations while
> maintaining task performance. Existing approaches typically rely on a single
> policy to jointly optimize reward and safety, which can cause instability due
> to conflicting objectives, or they use external safety filters that override
> actions and require prior system knowledge. In this paper, we propose a modular
> cost-aware regulator that scales the agent's actions based on predicted
> constraint violations, preserving exploration through smooth action modulation
> rather than overriding the policy. The regulator is trained to minimize
> constraint violations while avoiding degenerate suppression of actions. Our
> approach integrates seamlessly with off-policy RL methods such as SAC and TD3,
> and achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion
> tasks with sparse costs, reducing constraint violations by up to 126 times
> while increasing returns by over an order of magnitude compared to prior
> methods.

