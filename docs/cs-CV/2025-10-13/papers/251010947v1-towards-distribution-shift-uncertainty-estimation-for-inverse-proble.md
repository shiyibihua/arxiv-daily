---
layout: default
title: Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors
---

# Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors

**arXiv**: [2510.10947v1](https://arxiv.org/abs/2510.10947) | [PDF](https://arxiv.org/pdf/2510.10947.pdf)

**ä½œè€…**: Namhoon Kim, Sara Fridovich-Keil

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé‡æž„ç¨³å®šæ€§çš„åˆ†å¸ƒåç§»ä¸ç¡®å®šæ€§æŒ‡æ ‡ï¼Œç”¨äºŽç”Ÿæˆå…ˆéªŒé€†é—®é¢˜**

**å…³é”®è¯**: `é€†é—®é¢˜` `ç”Ÿæˆå…ˆéªŒ` `åˆ†å¸ƒåç§»æ£€æµ‹` `ä¸ç¡®å®šæ€§ä¼°è®¡` `é‡æž„ç¨³å®šæ€§` `è®¡ç®—æˆåƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç”Ÿæˆå…ˆé€†é—®é¢˜ä¸­ï¼Œæµ‹è¯•å›¾åƒåˆ†å¸ƒåç§»æ˜“è‡´å¹»è§‰ï¼ŒçŽ°æœ‰æ–¹æ³•éœ€æ ¡å‡†æˆ–éžç»Ÿè®¡ä¼°è®¡
2. åˆ©ç”¨é‡æž„ç¨³å®šæ€§ä½œä¸ºä»£ç†æŒ‡æ ‡ï¼Œæ— éœ€è®­ç»ƒåˆ†å¸ƒçŸ¥è¯†æˆ–é‡è®­ç»ƒï¼Œæ£€æµ‹åˆ†å¸ƒåç§»
3. åœ¨MNISTæ–­å±‚é‡å»ºå®žéªŒä¸­ï¼ŒOODæ•°å­—é‡æž„å˜å¼‚æ€§é«˜ï¼ŒéªŒè¯æŒ‡æ ‡æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generative models have shown strong potential as data-driven priors for
> solving inverse problems such as reconstructing medical images from
> undersampled measurements. While these priors improve reconstruction quality
> with fewer measurements, they risk hallucinating features when test images lie
> outside the training distribution. Existing uncertainty quantification methods
> in this setting (i) require an in-distribution calibration dataset, which may
> not be available, (ii) provide heuristic rather than statistical estimates, or
> (iii) quantify uncertainty from model capacity or limited measurements rather
> than distribution shift. We propose an instance-level, calibration-free
> uncertainty indicator that is sensitive to distribution shift, requires no
> knowledge of the training distribution, and incurs no retraining cost. Our key
> hypothesis is that reconstructions of in-distribution images remain stable
> under random measurement variations, while reconstructions of
> out-of-distribution (OOD) images exhibit greater instability. We use this
> stability as a proxy for detecting distribution shift. Our proposed OOD
> indicator is efficiently computable for any computational imaging inverse
> problem; we demonstrate it on tomographic reconstruction of MNIST digits, where
> a learned proximal network trained only on digit "0" is evaluated on all ten
> digits. Reconstructions of OOD digits show higher variability and
> correspondingly higher reconstruction error, validating this indicator. These
> results suggest a deployment strategy that pairs generative priors with
> lightweight guardrails, enabling aggressive measurement reduction for
> in-distribution cases while automatically warning when priors are applied out
> of distribution.

