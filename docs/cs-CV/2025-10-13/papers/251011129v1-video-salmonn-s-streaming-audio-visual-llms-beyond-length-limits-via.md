---
layout: default
title: video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory
---

# video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory

**arXiv**: [2510.11129v1](https://arxiv.org/abs/2510.11129) | [PDF](https://arxiv.org/pdf/2510.11129.pdf)

**ä½œè€…**: Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, Chao Zhang

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºvideo-SALMONN Sä»¥åœ¨å›ºå®šå†…å­˜ä¸‹å¤„ç†é•¿è§†é¢‘æµï¼Œå®ç°é«˜è´¨é‡éŸ³é¢‘-è§†è§‰ç†è§£**

**å…³é”®è¯**: `æµå¼éŸ³é¢‘-è§†è§‰å¤§è¯­è¨€æ¨¡å‹` `é•¿è§†é¢‘ç†è§£` `æµ‹è¯•æ—¶è®­ç»ƒ` `è®°å¿†æ¨¡å—` `å›ºå®šå†…å­˜å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è§†é¢‘ç†è§£LLMéš¾ä»¥å¤„ç†é•¿è§†é¢‘æµï¼Œå­˜åœ¨å†…å­˜é™åˆ¶å’Œä¿¡æ¯ä¸¢å¤±é—®é¢˜
2. å¼•å…¥æµ‹è¯•æ—¶è®­ç»ƒè®°å¿†æ¨¡å—å’Œæç¤ºä¾èµ–è®°å¿†è¯»å–å™¨ï¼Œæ›¿ä»£ä»¤ç‰Œåˆå¹¶ä»¥æ•è·é•¿ç¨‹ä¾èµ–
3. åœ¨é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹åœ¨3å°æ—¶è§†é¢‘ä¸Šä¿æŒé«˜æ€§èƒ½ï¼Œä¼˜äºç¦»çº¿ä¸æµå¼åŸºçº¿

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continuous, high-frame-rate, high-resolution processing of long video streams
> is critical for future AI agents, yet current video-understanding LLMs struggle
> to scale. Offline, fixed-frame-number methods require the stream length to
> adapt frame rates; streaming methods constrain memory by merging or discarding
> tokens, losing information. We propose video-SALMONN S, a streaming
> audio-visual LLM that, to our knowledge, is the first to process 3-hour videos
> at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces
> (i) a test-time-training (TTT) memory module that continually updates token
> representations to capture long-range dependencies by replacing token merging,
> and (ii) a prompt-dependent memory reader that selectively retrieves
> context-relevant content from fixed-size memory. The TTT module is optimised
> with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient
> adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),
> video-SALMONN S sustains high-quality understanding on multi-hour videos with
> 10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and
> 67.8% on the Video-MME long split, outperforming both offline and streaming
> baselines.

