---
layout: default
title: RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model
---

# RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model

**arXiv**: [2510.10975v1](https://arxiv.org/abs/2510.10975) | [PDF](https://arxiv.org/pdf/2510.10975.pdf)

**ä½œè€…**: Mingtong Dai, Lingbo Liu, Yongjie Bai, Yang Liu, Zhouxia Wang, Rui SU, Chunjie Chen, Liang Lin, Xinyu Wu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoVeræ¡†æž¶ï¼Œé€šè¿‡æµ‹è¯•æ—¶éªŒè¯å¢žå¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æ€§èƒ½**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æµ‹è¯•æ—¶ç¼©æ”¾` `è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹` `åŠ¨ä½œç©ºé—´æ–¹å‘` `å…±äº«æ„ŸçŸ¥ç¼“å­˜` `æœºå™¨äººå†³ç­–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹æ€§èƒ½æå‡ä¾èµ–æ•°æ®ä¸Žæ¨¡åž‹è§„æ¨¡æ‰©å±•ï¼Œæˆæœ¬é«˜æ˜‚ä¸”å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æœºå™¨äººè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹éªŒè¯å€™é€‰åŠ¨ä½œï¼Œå¹¶é¢„æµ‹åŠ¨ä½œç©ºé—´æ–¹å‘è¿›è¡Œæ‰©å±•ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡å…±äº«æ„ŸçŸ¥ç‰¹å¾ç¼“å­˜ï¼Œåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹è¯„ä¼°æ›´å¤šå€™é€‰åŠ¨ä½œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have become a prominent paradigm for
> embodied intelligence, yet further performance improvements typically rely on
> scaling up training data and model size -- an approach that is prohibitively
> expensive for robotics and fundamentally limited by data collection costs.We
> address this limitation with $\mathbf{RoVer}$, an embodied test-time scaling
> framework that uses a $\mathbf{Ro}$bot Process Reward Model (PRM) as a
> Test-Time $\mathbf{Ver}$ifier to enhance the capabilities of existing VLA
> models without modifying their architectures or weights. Specifically, RoVer
> (i) assigns scalar-based process rewards to evaluate the reliability of
> candidate actions, and (ii) predicts an action-space direction for candidate
> expansion/refinement. During inference, RoVer generates multiple candidate
> actions concurrently from the base policy, expands them along PRM-predicted
> directions, and then scores all candidates with PRM to select the optimal
> action for execution. Notably, by caching shared perception features, it can
> amortize perception cost and evaluate more candidates under the same test-time
> computational budget. Essentially, our approach effectively transforms
> available computing resources into better action decision-making, realizing the
> benefits of test-time scaling without extra training overhead. Our
> contributions are threefold: (1) a general, plug-and-play test-time scaling
> framework for VLAs; (2) a PRM that jointly provides scalar process rewards and
> an action-space direction to guide exploration; and (3) an efficient
> direction-guided sampling strategy that leverages a shared perception cache to
> enable scalable candidate generation and selection during inference.

