---
layout: default
title: FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model
---

# FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model

**arXiv**: [2510.10921v1](https://arxiv.org/abs/2510.10921) | [PDF](https://arxiv.org/pdf/2510.10921.pdf)

**ä½œè€…**: Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Ji Ao, Dawei Leng, Yuhui Yin

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFG-CLIP 2æ¨¡å‹ä»¥è§£å†³åŒè¯­ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½` `åŒè¯­æ¨¡å‹` `åŒºåŸŸ-æ–‡æœ¬åŒ¹é…` `é•¿å­—å¹•å»ºæ¨¡` `TICæŸå¤±` `å¤šæ¨¡æ€åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½ä¸Šè¡¨ç°ä¸è¶³ï¼Œå°¤å…¶åœ¨éè‹±è¯­åœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒºåŸŸ-æ–‡æœ¬åŒ¹é…ã€é•¿å­—å¹•å»ºæ¨¡åŠTICæŸå¤±ï¼Œæå‡åŒè¯­ç»†ç²’åº¦å¯¹é½ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨29ä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå®ç°åŒè¯­æœ€å…ˆè¿›æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-grained vision-language understanding requires precise alignment between
> visual content and linguistic descriptions, a capability that remains limited
> in current models, particularly in non-English settings. While models like CLIP
> perform well on global alignment, they often struggle to capture fine-grained
> details in object attributes, spatial relations, and linguistic expressions,
> with limited support for bilingual comprehension. To address these challenges,
> we introduce FG-CLIP 2, a bilingual vision-language model designed to advance
> fine-grained alignment for both English and Chinese. Our approach leverages
> rich fine-grained supervision, including region-text matching and long-caption
> modeling, alongside multiple discriminative objectives. We further introduce
> the Textual Intra-modal Contrastive (TIC) loss to better distinguish
> semantically similar captions. Trained on a carefully curated mixture of
> large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual
> performance. To enable rigorous evaluation, we present a new benchmark for
> Chinese multimodal understanding, featuring long-caption retrieval and bounding
> box classification. Extensive experiments on 29 datasets across 8 tasks show
> that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results
> in both languages. We release the model, code, and benchmark to facilitate
> future research on bilingual fine-grained alignment.

