---
layout: default
title: TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models
---

# TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models

**arXiv**: [2510.10932v1](https://arxiv.org/abs/2510.10932) | [PDF](https://arxiv.org/pdf/2510.10932.pdf)

**ä½œè€…**: Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTabVLAæ¡†æž¶ï¼Œé’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹è¿›è¡Œç›®æ ‡åŽé—¨æ”»å‡»**

**å…³é”®è¯**: `åŽé—¨æ”»å‡»` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `é»‘ç›’å¾®è°ƒ` `ä¸­æ¯’æ•°æ®ç”Ÿæˆ` `ç›®æ ‡æ”»å‡»` `å®‰å…¨å¨èƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹æ˜“å—ç›®æ ‡åŽé—¨æ”»å‡»ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿæ•…éšœæˆ–ç‰©ç†ä¼¤å®³
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é»‘ç›’å¾®è°ƒä¼˜åŒ–ä¸­æ¯’æ•°æ®ç”Ÿæˆï¼Œæ”¯æŒè¾“å…¥æµç¼–è¾‘å’Œåœºæ™¯å†…è§¦å‘
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨OpenVLA-7Bä¸ŠéªŒè¯ï¼Œè§†è§‰é€šé“ä¸ºä¸»è¦æ”»å‡»é¢ï¼Œæ”»å‡»æˆåŠŸçŽ‡é«˜ä¸”é²æ£’

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the growing deployment of Vision-Language-Action (VLA) models in
> real-world embodied AI systems, their increasing vulnerability to backdoor
> attacks poses a serious safety threat. A backdoored VLA agent can be covertly
> triggered by a pre-injected backdoor to execute adversarial actions,
> potentially causing system failures or even physical harm. Although backdoor
> attacks on VLA models have been explored, prior work has focused only on
> untargeted attacks, leaving the more practically threatening scenario of
> targeted manipulation unexamined. In this paper, we study targeted backdoor
> attacks on VLA models and introduce TabVLA, a novel framework that enables such
> attacks via black-box fine-tuning. TabVLA explores two deployment-relevant
> inference-time threat models: input-stream editing and in-scene triggering. It
> formulates poisoned data generation as an optimization problem to improve
> attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal
> that the vision channel is the principal attack surface: targeted backdoors
> succeed with minimal poisoning, remain robust across variations in trigger
> design, and are degraded only by positional mismatches between fine-tuning and
> inference triggers. We also investigate a potential detection-based defense
> against TabVLA, which reconstructs latent visual triggers from the input stream
> to flag activation-conditioned backdoor samples. Our work highlights the
> vulnerability of VLA models to targeted backdoor manipulation and underscores
> the need for more advanced defenses.

