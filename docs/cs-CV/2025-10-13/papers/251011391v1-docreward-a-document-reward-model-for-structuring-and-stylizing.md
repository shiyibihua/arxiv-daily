---
layout: default
title: DocReward: A Document Reward Model for Structuring and Stylizing
---

# DocReward: A Document Reward Model for Structuring and Stylizing

**arXiv**: [2510.11391v1](https://arxiv.org/abs/2510.11391) | [PDF](https://arxiv.org/pdf/2510.11391.pdf)

**ä½œè€…**: Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang, Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDocRewardæ–‡æ¡£å¥–åŠ±æ¨¡åž‹ï¼Œä»¥è§£å†³ä»£ç†å·¥ä½œæµä¸­è§†è§‰ç»“æž„ä¸Žé£Žæ ¼è¯„ä¼°çš„ç¼ºå¤±é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ–‡æ¡£å¥–åŠ±æ¨¡åž‹` `ä»£ç†å·¥ä½œæµ` `å¤šé¢†åŸŸæ•°æ®é›†` `Bradley-TerryæŸå¤±` `æ–‡æ¡£ç”Ÿæˆè¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»£ç†å·¥ä½œæµè‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆæ—¶ï¼Œç¼ºä¹å¯¹è§†è§‰ç»“æž„ä¸Žé£Žæ ¼çš„è¯„ä¼°ï¼Œå½±å“å¯è¯»æ€§ä¸Žå‚ä¸Žåº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤šé¢†åŸŸDocPairæ•°æ®é›†ï¼Œä½¿ç”¨Bradley-TerryæŸå¤±è®­ç»ƒæ¨¡åž‹ï¼Œå®žçŽ°æ–‡æœ¬è´¨é‡æ— å…³çš„ä¸“ä¸šæ€§è¯„åˆ†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æµ‹è¯•é›†ä¸Šï¼ŒDocRewardå‡†ç¡®çŽ‡è¶…è¶ŠGPT-4oå’ŒGPT-5ï¼Œå¹¶åœ¨æ–‡æ¡£ç”Ÿæˆè¯„ä¼°ä¸­èµ¢å¾—æ›´é«˜èƒœçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in agentic workflows have enabled the automation of tasks
> such as professional document generation. However, they primarily focus on
> textual quality, neglecting visual structure and style, which are crucial for
> readability and engagement. This gap arises mainly from the absence of suitable
> reward models to guide agentic workflows toward producing documents with
> stronger structural and stylistic quality. To address this, we propose
> DocReward, a document reward model that evaluates documents based on their
> structure and style. We construct a multi-domain dataset DocPair of 117K paired
> documents, covering 32 domains and 267 document types, each including a high-
> and low-professionalism document with identical content but different structure
> and style. This enables the model to evaluate professionalism comprehensively,
> and in a textual-quality-agnostic way. DocReward is trained using the
> Bradley-Terry loss to score documents, penalizing predictions that contradict
> the annotated ranking. To assess the performance of reward models, we create a
> test dataset containing document bundles ranked by well-educated human
> evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6
> and 19.4 percentage points, respectively, demonstrating its superiority over
> baselines. In an extrinsic evaluation of document generation, DocReward
> achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%
> win rate, demonstrating its utility in guiding generation agents toward
> producing human-preferred documents.

