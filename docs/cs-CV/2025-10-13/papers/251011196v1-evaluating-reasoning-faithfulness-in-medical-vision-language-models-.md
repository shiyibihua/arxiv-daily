---
layout: default
title: Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations
---

# Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations

**arXiv**: [2510.11196v1](https://arxiv.org/abs/2510.11196) | [PDF](https://arxiv.org/pdf/2510.11196.pdf)

**ä½œè€…**: Johannes Moll, Markus Graf, Tristan Lemke, Nicolas Lenhart, Daniel Truhn, Jean-Benoit Delbrouck, Jiazhen Pan, Daniel Rueckert, Lisa C. Adams, Keno K. Bressem

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šæ¨¡æ€æ‰°åŠ¨çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†å¿ å®žæ€§è¯„ä¼°æ¡†æž¶ï¼Œç”¨äºŽèƒ¸éƒ¨Xå…‰è§†è§‰é—®ç­”ã€‚**

**å…³é”®è¯**: `åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†å¿ å®žæ€§è¯„ä¼°` `å¤šæ¨¡æ€æ‰°åŠ¨` `èƒ¸éƒ¨Xå…‰è§†è§‰é—®ç­”` `é“¾å¼æ€ç»´è§£é‡Š` `ä¸´åºŠå¯ä¿¡åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹çš„é“¾å¼æ€ç»´è§£é‡Šå¸¸ä¸Žå†³ç­–è¿‡ç¨‹ä¸ç¬¦ï¼Œå½±å“ä¸´åºŠé«˜é£Žé™©åº”ç”¨çš„å¯ä¿¡åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ–‡æœ¬å’Œå›¾åƒä¿®æ”¹ï¼Œè¯„ä¼°ä¸´åºŠä¿çœŸåº¦ã€å› æžœå½’å› å’Œç½®ä¿¡åº¦æ ¡å‡†ä¸‰ä¸ªç»´åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºå‡†æµ‹è¯•æ˜¾ç¤ºç­”æ¡ˆå‡†ç¡®æ€§ä¸Žè§£é‡Šè´¨é‡è„±é’©ï¼Œä¸“æœ‰æ¨¡åž‹åœ¨å½’å› å’Œä¿çœŸåº¦ä¸Šè¡¨çŽ°æ›´ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) often produce chain-of-thought (CoT)
> explanations that sound plausible yet fail to reflect the underlying decision
> process, undermining trust in high-stakes clinical use. Existing evaluations
> rarely catch this misalignment, prioritizing answer accuracy or adherence to
> formats. We present a clinically grounded framework for chest X-ray visual
> question answering (VQA) that probes CoT faithfulness via controlled text and
> image modifications across three axes: clinical fidelity, causal attribution,
> and confidence calibration. In a reader study (n=4), evaluator-radiologist
> correlations fall within the observed inter-radiologist range for all axes,
> with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
> alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
> ($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
> that answer accuracy and explanation quality are decoupled, acknowledging
> injected cues does not ensure grounding, and text cues shift explanations more
> than visual cues. While some open-source models match final answer accuracy,
> proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
> fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
> evaluate beyond final answer accuracy.

