---
layout: default
title: Adversarial Attacks Leverage Interference Between Features in Superposition
---

# Adversarial Attacks Leverage Interference Between Features in Superposition

**arXiv**: [2510.11709v1](https://arxiv.org/abs/2510.11709) | [PDF](https://arxiv.org/pdf/2510.11709.pdf)

**ä½œè€…**: Edward Stevinson, Lucas Prieto, Melih Barsbey, Tolga Birdal

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨ç‰¹å¾å åŠ å¹²æ‰°è§£é‡Šå¯¹æŠ—æ€§æ”»å‡»ï¼Œæ­ç¤ºç½‘ç»œåŽ‹ç¼©å¯¼è‡´æ¼æ´ž**

**å…³é”®è¯**: `å¯¹æŠ—æ€§æ”»å‡»` `ç‰¹å¾å åŠ ` `ç¥žç»ç½‘ç»œåŽ‹ç¼©` `æ”»å‡»å¯è½¬ç§»æ€§` `ViTæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹æŠ—æ€§æ¼æ´žæºäºŽç¥žç»ç½‘ç»œé«˜æ•ˆä¿¡æ¯ç¼–ç ä¸­çš„ç‰¹å¾å åŠ å¹²æ‰°
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç‰¹å¾å åŠ æœºåˆ¶é¢„æµ‹æ”»å‡»æ¨¡å¼ï¼Œè§£é‡Šæ¨¡åž‹é—´æ”»å‡»å¯è½¬ç§»æ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒViT-CIFAR-10å®žéªŒä¸­éªŒè¯å åŠ è¶³ä»¥å¼•å‘å¯¹æŠ—æ€§æ¼æ´ž

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fundamental questions remain about when and why adversarial examples arise in
> neural networks, with competing views characterising them either as artifacts
> of the irregularities in the decision landscape or as products of sensitivity
> to non-robust input features. In this paper, we instead argue that adversarial
> vulnerability can stem from efficient information encoding in neural networks.
> Specifically, we show how superposition - where networks represent more
> features than they have dimensions - creates arrangements of latent
> representations that adversaries can exploit. We demonstrate that adversarial
> perturbations leverage interference between superposed features, making attack
> patterns predictable from feature arrangements. Our framework provides a
> mechanistic explanation for two known phenomena: adversarial attack
> transferability between models with similar training regimes and class-specific
> vulnerability patterns. In synthetic settings with precisely controlled
> superposition, we establish that superposition suffices to create adversarial
> vulnerability. We then demonstrate that these findings persist in a ViT trained
> on CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct
> of networks' representational compression, rather than flaws in the learning
> process or non-robust inputs.

