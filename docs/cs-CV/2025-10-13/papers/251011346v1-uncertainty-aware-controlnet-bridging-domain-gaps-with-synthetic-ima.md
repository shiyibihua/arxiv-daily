---
layout: default
title: Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation
---

# Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation

**arXiv**: [2510.11346v1](https://arxiv.org/abs/2510.11346) | [PDF](https://arxiv.org/pdf/2510.11346.pdf)

**ä½œè€…**: Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸ç¡®å®šæ€§æ„ŸçŸ¥ControlNetï¼Œåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ç”Ÿæˆåˆæˆå›¾åƒä»¥å¼¥åˆé¢†åŸŸå·®è·**

**å…³é”®è¯**: `ControlNet` `ä¸ç¡®å®šæ€§æŽ§åˆ¶` `åˆæˆå›¾åƒç”Ÿæˆ` `é¢†åŸŸé€‚åº”` `è¯­ä¹‰åˆ†å‰²` `è§†ç½‘è†œOCT`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šControlNetç”Ÿæˆå›¾åƒæ—¶æ˜“å¤åˆ¶åŽŸè®­ç»ƒåˆ†å¸ƒï¼Œé™åˆ¶åˆæˆæ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰çš„å¢žå¼ºæ•ˆæžœ
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ä¸ç¡®å®šæ€§æŽ§åˆ¶æœºåˆ¶ï¼Œç»“åˆæœªæ ‡æ³¨å’Œæ ‡æ³¨æ•°æ®è®­ç»ƒï¼Œç”Ÿæˆé«˜ä¸ç¡®å®šæ€§ç›®æ ‡åŸŸæ ‡æ³¨å›¾åƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†ç½‘è†œOCTå’Œäº¤é€šåœºæ™¯å®žéªŒä¸­ï¼Œæ˜¾è‘—æå‡åˆ†å‰²æ€§èƒ½ï¼Œæ— éœ€é¢å¤–ç›‘ç£

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generative Models are a valuable tool for the controlled creation of
> high-quality image data. Controlled diffusion models like the ControlNet have
> allowed the creation of labeled distributions. Such synthetic datasets can
> augment the original training distribution when discriminative models, like
> semantic segmentation, are trained. However, this augmentation effect is
> limited since ControlNets tend to reproduce the original training distribution.
>   This work introduces a method to utilize data from unlabeled domains to train
> ControlNets by introducing the concept of uncertainty into the control
> mechanism. The uncertainty indicates that a given image was not part of the
> training distribution of a downstream task, e.g., segmentation. Thus, two types
> of control are engaged in the final network: an uncertainty control from an
> unlabeled dataset and a semantic control from the labeled dataset. The
> resulting ControlNet allows us to create annotated data with high uncertainty
> from the target domain, i.e., synthetic data from the unlabeled distribution
> with labels. In our scenario, we consider retinal OCTs, where typically
> high-quality Spectralis images are available with given ground truth
> segmentations, enabling the training of segmentation networks. The recent
> development in Home-OCT devices, however, yields retinal OCTs with lower
> quality and a large domain shift, such that out-of-the-pocket segmentation
> networks cannot be applied for this type of data. Synthesizing annotated images
> from the Home-OCT domain using the proposed approach closes this gap and leads
> to significantly improved segmentation results without adding any further
> supervision. The advantage of uncertainty-guidance becomes obvious when
> compared to style transfer: it enables arbitrary domain shifts without any
> strict learning of an image style. This is also demonstrated in a traffic scene
> experiment.

