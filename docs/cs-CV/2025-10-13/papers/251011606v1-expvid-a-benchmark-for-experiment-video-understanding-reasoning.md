---
layout: default
title: ExpVid: A Benchmark for Experiment Video Understanding & Reasoning
---

# ExpVid: A Benchmark for Experiment Video Understanding & Reasoning

**arXiv**: [2510.11606v1](https://arxiv.org/abs/2510.11606) | [PDF](https://arxiv.org/pdf/2510.11606.pdf)

**ä½œè€…**: Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºExpVidåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨ç§‘å­¦å®žéªŒè§†é¢‘ç†è§£ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `ç§‘å­¦å®žéªŒè§†é¢‘` `åŸºå‡†è¯„ä¼°` `ç»†ç²’åº¦æ„ŸçŸ¥` `è¿‡ç¨‹ç†è§£` `ç§‘å­¦æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†å¿½ç•¥æ¹¿å®žéªŒå®¤å·¥ä½œçš„ç»†ç²’åº¦å’Œé•¿ç¨‹ç‰¹æ€§ï¼Œéš¾ä»¥è¯„ä¼°MLLMsçš„çœŸå®žèƒ½åŠ›
2. å¼•å…¥ä¸‰çº§ä»»åŠ¡å±‚æ¬¡ï¼šç»†ç²’åº¦æ„ŸçŸ¥ã€è¿‡ç¨‹ç†è§£å’Œç§‘å­¦æŽ¨ç†ï¼Œç¡®ä¿è§†è§‰åŸºç¡€
3. è¯„ä¼°19ä¸ªMLLMsï¼Œå‘çŽ°å…¶åœ¨ç»†ç²’åº¦ç»†èŠ‚å’ŒæŽ¨ç†æ–¹é¢è¡¨çŽ°ä¸ä½³ï¼Œå¼€æºæ¨¡åž‹å·®è·æ˜¾è‘—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) hold promise for accelerating
> scientific discovery by interpreting complex experimental procedures. However,
> their true capabilities are poorly understood, as existing benchmarks neglect
> the fine-grained and long-horizon nature of authentic laboratory work,
> especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the
> first benchmark designed to systematically evaluate MLLMs on scientific
> experiment videos. Curated from peer-reviewed video publications, ExpVid
> features a new three-level task hierarchy that mirrors the scientific process:
> (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
> Understanding of step order and completeness; and (3) Scientific Reasoning that
> connects the full experiment to its published conclusions. Our vision-centric
> annotation pipeline, combining automated generation with multi-disciplinary
> expert validation, ensures that tasks require visual grounding. We evaluate 19
> leading MLLMs on ExpVid and find that while they excel at coarse-grained
> recognition, they struggle with disambiguating fine details, tracking state
> changes over time, and linking experimental procedures to scientific outcomes.
> Our results reveal a notable performance gap between proprietary and
> open-source models, particularly in high-order reasoning. ExpVid not only
> provides a diagnostic tool but also charts a roadmap for developing MLLMs
> capable of becoming trustworthy partners in scientific experimentation.

