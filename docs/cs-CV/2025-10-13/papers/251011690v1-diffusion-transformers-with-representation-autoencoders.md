---
layout: default
title: Diffusion Transformers with Representation Autoencoders
---

# Diffusion Transformers with Representation Autoencoders

**arXiv**: [2510.11690v1](https://arxiv.org/abs/2510.11690) | [PDF](https://arxiv.org/pdf/2510.11690.pdf)

**ä½œè€…**: Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¡¨ç¤ºè‡ªç¼–ç å™¨ä»¥æ”¹è¿›æ‰©æ•£å˜æ¢å™¨çš„æ½œåœ¨ç”Ÿæˆå»ºæ¨¡**

**å…³é”®è¯**: `æ‰©æ•£å˜æ¢å™¨` `è¡¨ç¤ºè‡ªç¼–ç å™¨` `æ½œåœ¨ç”Ÿæˆå»ºæ¨¡` `å›¾åƒç”Ÿæˆ` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸVAEç¼–ç å™¨åœ¨æ‰©æ•£å˜æ¢å™¨ä¸­å­˜åœ¨æ¶æ„è¿‡æ—¶ã€æ½œåœ¨ç©ºé—´ä½ç»´å’Œè¡¨ç¤ºå¼±çš„é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç”¨é¢„è®­ç»ƒè¡¨ç¤ºç¼–ç å™¨ï¼ˆå¦‚DINOï¼‰ä¸è§£ç å™¨æ„å»ºè¡¨ç¤ºè‡ªç¼–ç å™¨ï¼Œæä¾›é«˜è´¨é‡é‡æ„å’Œè¯­ä¹‰ä¸°å¯Œæ½œåœ¨ç©ºé—´ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨ImageNetä¸Šå®ç°ä½FIDåˆ†æ•°ï¼Œå¦‚1.51ï¼ˆæ— å¼•å¯¼ï¼‰ï¼Œå¹¶å®ç°æ›´å¿«æ”¶æ•›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Latent generative modeling, where a pretrained autoencoder maps pixels into a
> latent space for the diffusion process, has become the standard strategy for
> Diffusion Transformers (DiT); however, the autoencoder component has barely
> evolved. Most DiTs continue to rely on the original VAE encoder, which
> introduces several limitations: outdated backbones that compromise
> architectural simplicity, low-dimensional latent spaces that restrict
> information capacity, and weak representations that result from purely
> reconstruction-based training and ultimately limit generative quality. In this
> work, we explore replacing the VAE with pretrained representation encoders
> (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term
> Representation Autoencoders (RAEs). These models provide both high-quality
> reconstructions and semantically rich latent spaces, while allowing for a
> scalable transformer-based architecture. Since these latent spaces are
> typically high-dimensional, a key challenge is enabling diffusion transformers
> to operate effectively within them. We analyze the sources of this difficulty,
> propose theoretically motivated solutions, and validate them empirically. Our
> approach achieves faster convergence without auxiliary representation alignment
> losses. Using a DiT variant equipped with a lightweight, wide DDT head, we
> achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no
> guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers
> clear advantages and should be the new default for diffusion transformer
> training.

