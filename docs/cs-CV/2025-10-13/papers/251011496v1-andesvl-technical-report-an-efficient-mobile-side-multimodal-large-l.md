---
layout: default
title: AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model
---

# AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model

**arXiv**: [2510.11496v1](https://arxiv.org/abs/2510.11496) | [PDF](https://arxiv.org/pdf/2510.11496.pdf)

**ä½œè€…**: Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAndesVLç§»åŠ¨ç«¯å¤šæ¨¡æ€å¤§æ¨¡åž‹ï¼Œä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡èµ„æºé™åˆ¶é—®é¢˜**

**å…³é”®è¯**: `ç§»åŠ¨ç«¯å¤šæ¨¡æ€æ¨¡åž‹` `å‚æ•°é«˜æ•ˆè®¾è®¡` `è§†è§‰è¯­è¨€ç†è§£` `è¾¹ç¼˜è®¡ç®—ä¼˜åŒ–` `å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäº‘ç«¯å¤§æ¨¡åž‹å‚æ•°åºžå¤§ï¼Œæ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œ
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽQwen3 LLMå’Œè§†è§‰ç¼–ç å™¨ï¼Œæž„å»º0.6Bè‡³4Bå‚æ•°æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šé¡¹å¼€æºåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°åŒç±»æ¨¡åž‹é¢†å…ˆæ°´å¹³

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,
> Gemini, and Claude Sonnet have demonstrated outstanding performance with
> enormous model sizes reaching hundreds of billions of parameters, they
> significantly surpass the limitations in memory, power consumption, and
> computing capacity of edge devices such as mobile phones. This paper introduces
> AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on
> Qwen3's LLM and various visual encoders. We comprehensively outline the model
> architectures, training pipeline, and training data of AndesVL, which achieves
> first-tier performance across a wide range of open-source benchmarks, including
> fields such as text-rich image understanding, reasoning and math, multi-image
> comprehension, general VQA, hallucination mitigation, multilingual
> understanding, and GUI-related tasks when compared with state-of-the-art models
> of a similar scale. Furthermore, we introduce a 1+N LoR

