---
layout: default
title: QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs
---

# QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs

**arXiv**: [2510.11696v1](https://arxiv.org/abs/2510.11696) | [PDF](https://arxiv.org/pdf/2510.11696.pdf)

**ä½œè€…**: Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQeRLæ¡†æž¶ï¼Œç»“åˆé‡åŒ–ä¸Žå¼ºåŒ–å­¦ä¹ ï¼Œæå‡å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡ä¸ŽæŽ¢ç´¢èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `é‡åŒ–å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒ` `ä½Žç§©é€‚åº”` `è‡ªé€‚åº”å™ªå£°` `GPUæ•ˆçŽ‡ä¼˜åŒ–` `æ•°å­¦æŽ¨ç†åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡åž‹ä¸­èµ„æºå¯†é›†ï¼Œéœ€é«˜GPUå†…å­˜å’Œé•¿è®­ç»ƒæ—¶é—´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆNVFP4é‡åŒ–å’ŒLoRAï¼Œå¼•å…¥è‡ªé€‚åº”é‡åŒ–å™ªå£°æœºåˆ¶ï¼ŒåŠ é€Ÿè®­ç»ƒå¹¶å¢žå¼ºæŽ¢ç´¢ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨7Bæ¨¡åž‹ä¸Šå®žçŽ°1.5å€åŠ é€Ÿï¼Œå•GPUè®­ç»ƒ32Bæ¨¡åž‹ï¼Œæ•°å­¦åŸºå‡†æ€§èƒ½åª²ç¾Žå…¨å‚æ•°å¾®è°ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
> large language models (LLMs). While RL is essential for LLMs' reasoning
> capabilities, it is resource-intensive, requiring substantial GPU memory and
> long rollout durations. QeRL addresses these issues by combining NVFP4
> quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
> while reducing memory overhead. Beyond efficiency, our findings show that
> quantization noise increases policy entropy, enhancing exploration, and
> enabling the discovery of better strategies during RL. To further optimize
> exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
> which dynamically adjusts noise during training. Experiments demonstrate that
> QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
> the first framework to enable RL training of a 32B LLM on a single H100 80GB
> GPU, while delivering overall speedups for RL training. It also achieves faster
> reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
> matching the performance of full-parameter fine-tuning on mathematical
> benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
> results establish QeRL as an efficient and effective framework for RL training
> in LLMs.

