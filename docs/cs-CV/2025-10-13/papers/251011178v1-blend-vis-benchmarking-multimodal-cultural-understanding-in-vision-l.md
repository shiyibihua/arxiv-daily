---
layout: default
title: BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models
---

# BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models

**arXiv**: [2510.11178v1](https://arxiv.org/abs/2510.11178) | [PDF](https://arxiv.org/pdf/2510.11178.pdf)

**ä½œè€…**: Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBLEnD-VisåŸºå‡†ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¤šæ¨¡æ€æ–‡åŒ–ç†è§£é²æ£’æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ–‡åŒ–ç†è§£åŸºå‡†` `å¤šæ¨¡æ€è¯„ä¼°` `é²æ£’æ€§åˆ†æž` `è·¨æ¨¡æ€ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è¯„ä¼°å¿½è§†è§†è§‰è¯­è¨€æ¨¡åž‹çš„æ–‡åŒ–ç†è§£é²æ£’æ€§å’Œå¯è¿ç§»æ€§
2. æž„å»ºå¤šæ¨¡æ€å¤šæ–‡åŒ–åŸºå‡†ï¼Œæ¶µç›–æ–‡æœ¬é‡è¿°å’Œè§†è§‰æ¨¡æ€çš„313ä¸ªé—®é¢˜æ¨¡æ¿
3. å®žéªŒæ˜¾ç¤ºæ¨¡åž‹åœ¨è¯­è¨€é‡è¿°ä¸‹æ€§èƒ½ä¸‹é™ï¼Œè§†è§‰çº¿ç´¢å¸®åŠ©ä½†è·¨æ¨¡æ€ä¸€è‡´æ€§ä½Ž

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As vision-language models (VLMs) are deployed globally, their ability to
> understand culturally situated knowledge becomes essential. Yet, existing
> evaluations largely assess static recall or isolated visual grounding, leaving
> unanswered whether VLMs possess robust and transferable cultural understanding.
> We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to
> evaluate the robustness of everyday cultural knowledge in VLMs across
> linguistic rephrasings and visual modalities. Building on the BLEnD dataset,
> BLEnD-Vis constructs 313 culturally grounded question templates spanning 16
> regions and generates three aligned multiple-choice formats: (i) a text-only
> baseline querying from Region $\to$ Entity, (ii) an inverted text-only variant
> (Entity $\to$ Region), and (iii) a VQA-style version of (ii) with generated
> images. The resulting benchmark comprises 4,916 images and over 21,000
> multiple-choice question (MCQ) instances, validated through human annotation.
> BLEnD-Vis reveals significant fragility in current VLM cultural knowledge;
> models exhibit performance drops under linguistic rephrasing and, whilst visual
> cues often aid performance, low cross-modal consistency highlights challenges
> in robustly integrating textual and visual understanding, particularly for
> lower-resource regions. BLEnD-Vis thus provides a crucial testbed for
> systematically analysing cultural robustness and multimodal grounding, exposing
> limitations and guiding the development of more culturally competent VLMs.

