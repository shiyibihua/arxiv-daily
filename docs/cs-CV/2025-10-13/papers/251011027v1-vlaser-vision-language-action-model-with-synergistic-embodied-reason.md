---
layout: default
title: Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning
---

# Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning

**arXiv**: [2510.11027v1](https://arxiv.org/abs/2510.11027) | [PDF](https://arxiv.org/pdf/2510.11027.pdf)

**ä½œè€…**: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVlaseræ¨¡åž‹ä»¥å¼¥åˆè§†è§‰è¯­è¨€æŽ¨ç†ä¸Žå…·èº«ç­–ç•¥å­¦ä¹ é—´çš„å·®è·**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `å…·èº«æŽ¨ç†` `ç­–ç•¥å­¦ä¹ ` `æ•°æ®é›†æž„å»º` `åŸºå‡†æµ‹è¯•` `é¢†åŸŸè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç ”ç©¶æœªç›´æŽ¥è§£å†³ä¸Šæ¸¸è§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†ä¸Žä¸‹æ¸¸è§†è§‰è¯­è¨€åŠ¨ä½œç­–ç•¥å­¦ä¹ é—´çš„å…³é”®å·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºVlaseræ¨¡åž‹ï¼Œé›†æˆé«˜å±‚æ¬¡æŽ¨ç†ä¸Žä½Žå±‚æ¬¡æŽ§åˆ¶ï¼ŒåŸºäºŽVlaser-6Mæ•°æ®é›†å®žçŽ°ååŒå…·èº«æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå…·èº«æŽ¨ç†åŸºå‡†ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶åœ¨WidowXå’ŒGoogle RobotåŸºå‡†ä¸Šå–å¾—é¢†å…ˆæˆ–ç«žäº‰æ€§ç»“æžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While significant research has focused on developing embodied reasoning
> capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs
> into Vision-Language-Action (VLA) models for end-to-end robot control, few
> studies directly address the critical gap between upstream VLM-based reasoning
> and downstream VLA policy learning. In this work, we take an initial step
> toward bridging embodied reasoning with VLA policy learning by introducing
> Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning
> capability, which is a foundational vision-language model designed to integrate
> high-level reasoning with low-level control for embodied agents. Built upon the
> high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance
> across a range of embodied reasoning benchmarks - including spatial reasoning,
> embodied grounding, embodied QA, and task planning. Furthermore, we
> systematically examine how different VLM initializations affect supervised VLA
> fine-tuning, offering novel insights into mitigating the domain shift between
> internet-scale pre-training data and embodied-specific policy learning data.
> Based on these insights, our approach achieves state-of-the-art results on the
> WidowX benchmark and competitive performance on the Google Robot benchmark.

