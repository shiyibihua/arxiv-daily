---
layout: default
title: mmWalk: Towards Multi-modal Multi-view Walking Assistance
---

# mmWalk: Towards Multi-modal Multi-view Walking Assistance

**arXiv**: [2510.11520v1](https://arxiv.org/abs/2510.11520) | [PDF](https://arxiv.org/pdf/2510.11520.pdf)

**ä½œè€…**: Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºmmWalkå¤šæ¨¡æ€å¤šè§†å›¾æ•°æ®é›†ä»¥è§£å†³ç›²äººæˆ–ä½Žè§†åŠ›è€…æˆ·å¤–å®‰å…¨å¯¼èˆªé—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ•°æ®é›†` `æˆ·å¤–å¯¼èˆª` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯è®¿é—®æ€§ç‰¹å¾` `é£Žé™©è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›²äººæˆ–ä½Žè§†åŠ›è€…åœ¨å¤æ‚çŽ¯å¢ƒä¸­è¡Œèµ°ç¼ºä¹æ•´ä½“åœºæ™¯ç†è§£ï¼Œå¯¼è‡´å¯¼èˆªå›°éš¾ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºæ¨¡æ‹Ÿå¤šæ¨¡æ€æ•°æ®é›†ï¼Œé›†æˆå¤šè§†å›¾ä¼ æ„Ÿå™¨å’Œå¯è®¿é—®æ€§ç‰¹å¾ï¼ŒåŒ…å«12ä¸‡å¸§åŒæ­¥æ•°æ®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é£Žé™©ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œå¾®è°ƒæ¨¡åž‹åœ¨çœŸå®žæ•°æ®é›†ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Walking assistance in extreme or complex environments remains a significant
> challenge for people with blindness or low vision (BLV), largely due to the
> lack of a holistic scene understanding. Motivated by the real-world needs of
> the BLV community, we build mmWalk, a simulated multi-modal dataset that
> integrates multi-view sensor and accessibility-oriented features for outdoor
> safe navigation. Our dataset comprises 120 manually controlled,
> scenario-categorized walking trajectories with 62k synchronized frames. It
> contains over 559k panoramic images across RGB, depth, and semantic modalities.
> Furthermore, to emphasize real-world relevance, each trajectory involves
> outdoor corner cases and accessibility-specific landmarks for BLV users.
> Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual
> question-answer triplets across 9 categories tailored for safe and informed
> walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)
> using zero- and few-shot settings and found they struggle with our risk
> assessment and navigational tasks. We validate our mmWalk-finetuned model on
> real-world datasets and show the effectiveness of our dataset for advancing
> multi-modal walking assistance.

