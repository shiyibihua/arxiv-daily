---
layout: default
title: IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation
---

# IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation

**arXiv**: [2510.10969v1](https://arxiv.org/abs/2510.10969) | [PDF](https://arxiv.org/pdf/2510.10969.pdf)

**ä½œè€…**: Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIUT-Plugæ¨¡å—ä»¥è§£å†³å¤šæ¨¡æ€å›¾åƒ-æ–‡æœ¬ç”Ÿæˆä¸­çš„é€»è¾‘ã€å®žä½“å’Œé£Žæ ¼æ¼‚ç§»é—®é¢˜**

**å…³é”®è¯**: `å›¾åƒç†è§£æ ‘` `å¤šæ¨¡æ€ç”Ÿæˆ` `ä¸Šä¸‹æ–‡æ¼‚ç§»` `ç»“æž„åŒ–æŽ¨ç†` `è·¨æ¨¡æ€ä¸€è‡´æ€§` `é—®ç­”åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¤æ‚å›¾åƒ-æ–‡æœ¬åœºæ™¯ä¸­éš¾ä»¥ä¿æŒé€»è¾‘ã€å¯¹è±¡èº«ä»½å’Œé£Žæ ¼ä¸€è‡´æ€§
2. åŸºäºŽå›¾åƒç†è§£æ ‘æž„å»ºåŠ¨æ€æå–å’Œåè°ƒæœºåˆ¶ï¼Œå¢žå¼ºç»“æž„åŒ–æŽ¨ç†å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§
3. å®žéªŒæ˜¾ç¤ºåœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€é—®ç­”åœºæ™¯ä¸­æœ‰æ•ˆç¼“è§£ä¸Šä¸‹æ–‡æ¼‚ç§»å¹¶æå‡å‡†ç¡®æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing vision language models (VLMs), including GPT-4 and DALL-E, often
> struggle to preserve logic, object identity, and style in multimodal image-text
> generation. This limitation significantly hinders the generalization capability
> of VLMs in complex image-text input-output scenarios. To address this issue, we
> propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which
> enhances existing interleaved VLMs through explicit structured reasoning,
> thereby mitigating context drift in logic, entity identity, and style. The
> proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction
> module parses visual scenes into hierarchical symbolic structures. (2) A
> coordinated narrative-flow and image synthesis mechanism ensures cross-modal
> consistency. To evaluate our approach, we construct a novel benchmark based on
> 3,000 real human-generated question-answer pairs over fine-tuned large models,
> introducing a dynamic evaluation protocol for quantifying context drift in
> interleaved VLMs. Experimental results demonstrate that IUT-Plug not only
> improves accuracy on established benchmarks but also effectively alleviates the
> three critical forms of context drift across diverse multimodal question
> answering (QA) scenarios.

