---
layout: default
title: GIR-Bench: Versatile Benchmark for Generating Images with Reasoning
---

# GIR-Bench: Versatile Benchmark for Generating Images with Reasoning

**arXiv**: [2510.11026v1](https://arxiv.org/abs/2510.11026) | [PDF](https://arxiv.org/pdf/2510.11026.pdf)

**ä½œè€…**: Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGIR-BenchåŸºå‡†ä»¥è¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹åœ¨æŽ¨ç†é©±åŠ¨å›¾åƒç”Ÿæˆä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºå‡†` `å›¾åƒç”Ÿæˆ` `æŽ¨ç†è¯„ä¼°` `ç†è§£-ç”Ÿæˆä¸€è‡´æ€§` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¤šæ­¥ç¼–è¾‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹ç³»ç»ŸåŸºå‡†è¯„ä¼°å¤šæ¨¡æ€æ¨¡åž‹ç†è§£ä¸Žç”Ÿæˆçš„ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ä¸‰ä¸ªå­é›†è¯„ä¼°ç†è§£-ç”Ÿæˆä¸€è‡´æ€§ã€æŽ¨ç†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå¤šæ­¥ç¼–è¾‘
3. å®žéªŒæˆ–æ•ˆæžœï¼šç»Ÿä¸€æ¨¡åž‹åœ¨æŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°æ›´å¥½ï¼Œä½†ç†è§£ä¸Žç”Ÿæˆé—´ä»å­˜åœ¨å·®è·

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unified multimodal models integrate the reasoning capacity of large language
> models with both image understanding and generation, showing great promise for
> advanced multimodal intelligence. However, the community still lacks a rigorous
> reasoning-centric benchmark to systematically evaluate the alignment between
> understanding and generation, and their generalization potential in complex
> visual tasks. To this end, we introduce \textbf{GIR-Bench}, a comprehensive
> benchmark that evaluates unified models across three complementary
> perspectives. Firstly, we investigate understanding-generation consistency
> (GIR-Bench-UGC), asking whether models can consistently leverage the same
> knowledge in both understanding and generation tasks. Secondly, we investigate
> whether models can perform reasoning-centric text-to-image generation that
> requires applying logical constraints and implicit knowledge to generate
> faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models
> can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,
> we carefully design different task-specific evaluation pipelines tailored for
> each task. This enables fine-grained and interpretable evaluation while
> mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive
> ablations over various unified models and generation-only systems have shown
> that: Although unified models are more capable of reasoning-driven visual
> tasks, they still exhibit a persistent gap between understanding and
> generation. The data and code for GIR-Bench are available at
> \href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.

