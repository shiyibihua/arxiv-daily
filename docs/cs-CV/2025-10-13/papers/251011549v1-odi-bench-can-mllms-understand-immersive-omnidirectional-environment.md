---
layout: default
title: ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?
---

# ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?

**arXiv**: [2510.11549v1](https://arxiv.org/abs/2510.11549) | [PDF](https://arxiv.org/pdf/2510.11549.pdf)

**ä½œè€…**: Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºODI-BenchåŸºå‡†å’ŒOmni-CoTæ–¹æ³•ä»¥è¯„ä¼°å’Œå¢žå¼ºMLLMsåœ¨å…¨æ™¯å›¾åƒç†è§£ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å…¨æ™¯å›¾åƒç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `æ€ç»´é“¾æŽ¨ç†` `æ²‰æµ¸å¼çŽ¯å¢ƒ` `é—®ç­”å¯¹æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨å…¨æ™¯å›¾åƒç†è§£ä¸­è¡¨çŽ°ä¸ä½³ï¼Œç¼ºä¹æ²‰æµ¸å¼ä¸Šä¸‹æ–‡æ•æ‰èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Omni-CoTï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡æ€ç»´é“¾æŽ¨ç†ç»“åˆæ–‡æœ¬å’Œè§†è§‰çº¿ç´¢æå‡ç†è§£
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ODI-Benchä¸Šæµ‹è¯•20ä¸ªMLLMsï¼Œæ˜¾ç¤ºå½“å‰æ¨¡åž‹ä»å­˜åœ¨å›°éš¾ï¼ŒOmni-CoTæ˜¾è‘—æ”¹å–„æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Omnidirectional images (ODIs) provide full 360x180 view which are widely
> adopted in VR, AR and embodied intelligence applications. While multi-modal
> large language models (MLLMs) have demonstrated remarkable performance on
> conventional 2D image and video understanding benchmarks, their ability to
> comprehend the immersive environments captured by ODIs remains largely
> unexplored. To address this gap, we first present ODI-Bench, a novel
> comprehensive benchmark specifically designed for omnidirectional image
> understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and
> over 4,000 manually annotated question-answering (QA) pairs across 10
> fine-grained tasks, covering both general-level and spatial-level ODI
> understanding. Extensive experiments are conducted to benchmark 20
> representative MLLMs, including proprietary and open-source models, under both
> close-ended and open-ended settings. Experimental results reveal that current
> MLLMs still struggle to capture the immersive context provided by ODIs. To this
> end, we further introduce Omni-CoT, a training-free method which significantly
> enhances MLLMs' comprehension ability in the omnidirectional environment
> through chain-of-thought reasoning across both textual information and visual
> cues. Both the benchmark and the code will be released upon the publication.

