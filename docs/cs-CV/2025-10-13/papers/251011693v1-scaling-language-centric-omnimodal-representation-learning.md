---
layout: default
title: Scaling Language-Centric Omnimodal Representation Learning
---

# Scaling Language-Centric Omnimodal Representation Learning

**arXiv**: [2510.11693v1](https://arxiv.org/abs/2510.11693) | [PDF](https://arxiv.org/pdf/2510.11693.pdf)

**ä½œè€…**: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€ä¸­å¿ƒå…¨æ¨¡æ€åµŒå…¥æ¡†æž¶ä»¥å¢žå¼ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ **

**å…³é”®è¯**: `å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ` `è¯­è¨€ä¸­å¿ƒåµŒå…¥` `å¯¹æ¯”å­¦ä¹ ` `ç”Ÿæˆé¢„è®­ç»ƒ` `è·¨æ¨¡æ€å¯¹é½` `ç¼©æ”¾å®šå¾‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºäºŽMLLMçš„å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•ä¼˜åŠ¿åŽŸå› æœªæ˜Žï¼Œéœ€æŽ¢ç´¢éšå¼è·¨æ¨¡æ€å¯¹é½æœºåˆ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨ç”Ÿæˆé¢„è®­ç»ƒä¸­çš„éšå¼å¯¹é½ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ è½»é‡åŒ–ä¼˜åŒ–è¡¨ç¤ºç©ºé—´ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°SOTAï¼ŒéªŒè¯ç”Ÿæˆ-è¡¨ç¤ºç¼©æ”¾å®šå¾‹çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent multimodal embedding approaches leveraging multimodal large language
> models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
> results, yet the underlying reasons behind their superiority remain
> underexplored. This work argues that a crucial advantage of MLLM-based
> approaches stems from implicit cross-modal alignment achieved during generative
> pretraining, where the language decoder learns to exploit multimodal signals
> within a shared representation space for generating unimodal outputs. Through
> analysis of anisotropy and kernel similarity structure, we empirically confirm
> that latent alignment emerges within MLLM representations, allowing CL to serve
> as a lightweight refinement stage. Leveraging this insight, we propose a
> Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
> experiments across diverse backbones and benchmarks demonstrate its
> effectiveness, achieving state-of-the-art performance across modalities.
> Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
> showing that the representational capabilities gained through contrastive
> refinement scales positively with the MLLM's generative capabilities. This
> suggests that improving generative abilities evolves as an effective paradigm
> for enhancing representation quality. We provide a theoretical explanation of
> GRSL, which formally links the MLLM's generative quality to the upper bound on
> its representation performance, and validate it on a challenging, low-resource
> visual-document retrieval task, showing that continual generative pretraining
> before CL can further enhance the potential of a model's embedding
> capabilities. Codes, models, and resources are available at
> https://github.com/LCO-Embedding/LCO-Embedding.

