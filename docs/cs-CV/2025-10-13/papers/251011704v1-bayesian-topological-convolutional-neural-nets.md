---
layout: default
title: Bayesian Topological Convolutional Neural Nets
---

# Bayesian Topological Convolutional Neural Nets

**arXiv**: [2510.11704v1](https://arxiv.org/abs/2510.11704) | [PDF](https://arxiv.org/pdf/2510.11704.pdf)

**ä½œè€…**: Sarah Harkins Dayton, Hayden Everett, Ioannis Schizas, David L. Boothe Jr., Vasileios Maroulas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè´å¶æ–¯æ‹“æ‰‘å·ç§¯ç¥žç»ç½‘ç»œä»¥è§£å†³å›¾åƒåˆ†ç±»ä¸­æ•°æ®ä¸è¶³å’Œä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜**

**å…³é”®è¯**: `è´å¶æ–¯ç¥žç»ç½‘ç»œ` `æ‹“æ‰‘å·ç§¯ç½‘ç»œ` `ä¸ç¡®å®šæ€§é‡åŒ–` `å›¾åƒåˆ†ç±»` `æ•°æ®æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸCNNéœ€è¦å¤§é‡æ•°æ®è®­ç»ƒã€é¢„æµ‹è¿‡åº¦è‡ªä¿¡ä¸”ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ‹“æ‰‘æ„ŸçŸ¥å­¦ä¹ å’Œè´å¶æ–¯é‡‡æ ·ï¼Œé€šè¿‡å…ˆéªŒåˆ†å¸ƒå’Œä¸€è‡´æ€§æ¡ä»¶ä¼˜åŒ–ç½‘ç»œå‚æ•°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºŽä¼ ç»ŸCNNã€BNNå’Œæ‹“æ‰‘CNNï¼Œå°¤å…¶åœ¨æ•°æ®æœ‰é™æˆ–æŸåæ—¶è¡¨çŽ°ä¼˜è¶Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Convolutional neural networks (CNNs) have been established as the main
> workhorse in image data processing; nonetheless, they require large amounts of
> data to train, often produce overconfident predictions, and frequently lack the
> ability to quantify the uncertainty of their predictions. To address these
> concerns, we propose a new Bayesian topological CNN that promotes a novel
> interplay between topology-aware learning and Bayesian sampling. Specifically,
> it utilizes information from important manifolds to accelerate training while
> reducing calibration error by placing prior distributions on network parameters
> and properly learning appropriate posteriors. One important contribution of our
> work is the inclusion of a consistency condition in the learning cost, which
> can effectively modify the prior distributions to improve the performance of
> our novel network architecture. We evaluate the model on benchmark image
> classification datasets and demonstrate its superiority over conventional CNNs,
> Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply
> evidence that our method provides an advantage in situations where training
> data is limited or corrupted. Furthermore, we show that the new model allows
> for better uncertainty quantification than standard BNNs since it can more
> readily identify examples of out-of-distribution data on which it has not been
> trained. Our results highlight the potential of our novel hybrid approach for
> more efficient and robust image classification.

