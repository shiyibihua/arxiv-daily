---
layout: default
title: SNAP: Towards Segmenting Anything in Any Point Cloud
---

# SNAP: Towards Segmenting Anything in Any Point Cloud

**arXiv**: [2510.11565v1](https://arxiv.org/abs/2510.11565) | [PDF](https://arxiv.org/pdf/2510.11565.pdf)

**ä½œè€…**: Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSNAPæ¨¡åž‹ï¼Œå®žçŽ°è·¨åŸŸäº¤äº’å¼3Dç‚¹äº‘åˆ†å‰²ï¼Œæ”¯æŒç‚¹å’Œæ–‡æœ¬æç¤ºã€‚**

**å…³é”®è¯**: `3Dç‚¹äº‘åˆ†å‰²` `äº¤äº’å¼åˆ†å‰²` `è·¨åŸŸæ³›åŒ–` `å¼€æ”¾è¯æ±‡åˆ†å‰²` `åŸŸè‡ªé€‚åº”å½’ä¸€åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰äº¤äº’å¼3Dç‚¹äº‘åˆ†å‰²æ–¹æ³•å±€é™äºŽå•ä¸€åŸŸå’Œäº¤äº’å½¢å¼ï¼Œä¸”å¤šæ•°æ®é›†è®­ç»ƒæ˜“å¯¼è‡´è´Ÿè¿ç§»ã€‚
2. SNAPé‡‡ç”¨åŸŸè‡ªé€‚åº”å½’ä¸€åŒ–ï¼Œåœ¨7ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ”¯æŒç‚¹å’Œæ–‡æœ¬æç¤ºï¼Œå®žçŽ°å¼€æ”¾è¯æ±‡åˆ†å‰²ã€‚
3. å®žéªŒæ˜¾ç¤ºSNAPåœ¨é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œè¶…è¶Šå¤šä¸ªé¢†åŸŸç‰¹å®šæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Interactive 3D point cloud segmentation enables efficient annotation of
> complex 3D scenes through user-guided prompts. However, current approaches are
> typically restricted in scope to a single domain (indoor or outdoor), and to a
> single form of user interaction (either spatial clicks or textual prompts).
> Moreover, training on multiple datasets often leads to negative transfer,
> resulting in domain-specific tools that lack generalizability. To address these
> limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in
> \textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D
> segmentation that supports both point-based and text-based prompts across
> diverse domains. Our approach achieves cross-domain generalizability by
> training on 7 datasets spanning indoor, outdoor, and aerial environments, while
> employing domain-adaptive normalization to prevent negative transfer. For
> text-prompted segmentation, we automatically generate mask proposals without
> human intervention and match them against CLIP embeddings of textual queries,
> enabling both panoptic and open-vocabulary segmentation. Extensive experiments
> demonstrate that SNAP consistently delivers high-quality segmentation results.
> We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for
> spatial-prompted segmentation and demonstrate competitive results on all 5
> text-prompted benchmarks. These results show that a unified model can match or
> exceed specialized domain-specific approaches, providing a practical tool for
> scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/

