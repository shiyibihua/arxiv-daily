---
layout: default
title: G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation
---

# G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation

**arXiv**: [2510.11176v1](https://arxiv.org/abs/2510.11176) | [PDF](https://arxiv.org/pdf/2510.11176.pdf)

**ä½œè€…**: Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºG2Læ¡†æž¶é€šè¿‡çŸ¥è¯†è’¸é¦æå‡ç—…ç†å¤§æ¨¡åž‹æ€§èƒ½ï¼Œé™ä½Žè®¡ç®—æˆæœ¬**

**å…³é”®è¯**: `ç—…ç†åŸºç¡€æ¨¡åž‹` `çŸ¥è¯†è’¸é¦` `ç™Œç—‡ç‰¹å¼‚æ€§` `è®¡ç®—æ•ˆçŽ‡` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåƒäº¿çº§ç—…ç†åŸºç¡€æ¨¡åž‹è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥å®žç”¨éƒ¨ç½²
2. æ–¹æ³•è¦ç‚¹ï¼šç”¨å°‘é‡ç›®æ ‡ç™Œç—‡æ•°æ®è’¸é¦ï¼Œå°†åƒäº¿æ¨¡åž‹èƒ½åŠ›è½¬ç§»è‡³å¤§åž‹æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šè’¸é¦æ¨¡åž‹åœ¨å¤šé¡¹åŸºå‡†ä¸­è¶…è¶ŠåŒè§„æ¨¡æ¨¡åž‹ï¼Œéƒ¨åˆ†è¶…è¶Šåƒäº¿æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent studies in pathology foundation models have shown that scaling
> training data, diversifying cancer types, and increasing model size
> consistently improve their performance. However, giga-scale foundation models,
> which are trained on hundreds of thousands of slides covering tens of cancer
> types and contain billions of parameters, pose significant challenges for
> practical use due to their tremendous computational costs in both development
> and deployment. In this work, we present a novel strategy, named the G2L
> framework, to increase the performance of large-scale foundation models, which
> consist of only $15\%$ of the parameters of giga-scale models, to a comparable
> performance level of giga-scale models in cancer-specific tasks. Our approach
> applies knowledge distillation, transferring the capabilities of a giga-scale
> model to a large-scale model, using just 1K pathology slides of a target cancer
> (e.g., breast, prostate, etc.). The resulting distilled model not only
> outperformed state-of-the-art models of the same size (i.e., large-scale)
> across several benchmarks but also, interestingly, surpassed the giga-scale
> teacher and huge-scale models in some benchmarks. In addition, the distilled
> model exhibited a higher robustness index, indicating improved resilience to
> image variations originating from multiple institutions. These findings suggest
> that the proposed distillation approach for a large-scale model is a data- and
> parameter-efficient way to achieve giga-scale-level performance for
> cancer-specific applications without prohibitive computational burden.

