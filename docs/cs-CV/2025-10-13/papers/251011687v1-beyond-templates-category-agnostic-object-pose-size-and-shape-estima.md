---
layout: default
title: Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View
---

# Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View

**arXiv**: [2510.11687v1](https://arxiv.org/abs/2510.11687) | [PDF](https://arxiv.org/pdf/2510.11687.pdf)

**ä½œè€…**: Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç±»åˆ«æ— å…³æ¡†æž¶ï¼Œä»Žå•è§†å›¾åŒæ—¶ä¼°è®¡ç‰©ä½“6Dä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ï¼Œæ— éœ€æ¨¡æ¿æˆ–CADæ¨¡åž‹ã€‚**

**å…³é”®è¯**: `6Dä½å§¿ä¼°è®¡` `ç±»åˆ«æ— å…³å­¦ä¹ ` `å½¢çŠ¶é‡å»º` `Transformeræ¨¡åž‹` `é›¶æ ·æœ¬æ³›åŒ–` `å®žæ—¶æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»Žè§†è§‰è¾“å…¥ä¼°è®¡ç‰©ä½“6Dä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å…ˆéªŒæˆ–æ³›åŒ–å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆ2Dç‰¹å¾ä¸Ž3Dç‚¹äº‘ï¼Œä½¿ç”¨Transformerå’Œä¸“å®¶æ··åˆï¼Œå¹¶è¡Œè§£ç å®žçŽ°å®žæ—¶æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆæ•°æ®è®­ç»ƒï¼Œå¤šåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°SOTAï¼Œé›¶æ ·æœ¬æ³›åŒ–å¼ºäºŽæœªçŸ¥ç‰©ä½“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Estimating an object's 6D pose, size, and shape from visual input is a
> fundamental problem in computer vision, with critical applications in robotic
> grasping and manipulation. Existing methods either rely on object-specific
> priors such as CAD models or templates, or suffer from limited generalization
> across categories due to pose-shape entanglement and multi-stage pipelines. In
> this work, we propose a unified, category-agnostic framework that
> simultaneously predicts 6D pose, size, and dense shape from a single RGB-D
> image, without requiring templates, CAD models, or category labels at test
> time. Our model fuses dense 2D features from vision foundation models with
> partial 3D point clouds using a Transformer encoder enhanced by a
> Mixture-of-Experts, and employs parallel decoders for pose-size estimation and
> shape reconstruction, achieving real-time inference at 28 FPS. Trained solely
> on synthetic data from 149 categories in the SOPE dataset, our framework is
> evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,
> spanning over 300 categories. It achieves state-of-the-art accuracy on seen
> categories while demonstrating remarkably strong zero-shot generalization to
> unseen real-world objects, establishing a new standard for open-set 6D
> understanding in robotics and embodied AI.

