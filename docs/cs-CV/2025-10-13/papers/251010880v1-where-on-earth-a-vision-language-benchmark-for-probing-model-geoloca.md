---
layout: default
title: Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales
---

# Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales

**arXiv**: [2510.10880v1](https://arxiv.org/abs/2510.10880) | [PDF](https://arxiv.org/pdf/2510.10880.pdf)

**ä½œè€…**: Zhaofang Qian, Hardy Chen, Zeyu Wang, Li Zhang, Zijun Wang, Xiaoke Huang, Hui Liu, Xianfeng Tang, Zeyu Zheng, Haoqin Tu, Cihang Xie, Yuyin Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEarthWhereåŸºå‡†ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¼€æ”¾ä¸–ç•Œå›¾åƒåœ°ç†å®šä½èƒ½åŠ›**

**å…³é”®è¯**: `å›¾åƒåœ°ç†å®šä½` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `å¤šå°ºåº¦æŽ¨ç†` `åŒºåŸŸåè§` `å¼€æ”¾ä¸–ç•Œä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¼€æ”¾ä¸–ç•Œå›¾åƒåœ°ç†å®šä½èƒ½åŠ›æœªå…¨é¢è¯„ä¼°ï¼Œä»»åŠ¡å…·æŒ‘æˆ˜æ€§å’ŒçŽ°å®žéœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºEarthWhereåŸºå‡†ï¼ŒåŒ…å«å¤šå°ºåº¦åœ°ç†å®šä½ä»»åŠ¡ï¼Œå¹¶å¼•å…¥æŽ¨ç†é“¾è¯„åˆ†å’ŒShapleyé‡åŠ æƒæ€è€ƒåˆ†æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°13ä¸ªå…ˆè¿›æ¨¡åž‹ï¼ŒGemini-2.5-Proå¹³å‡å‡†ç¡®çŽ‡æœ€é«˜ï¼Œæ¨¡åž‹å­˜åœ¨åŒºåŸŸåè§å’ŒæŽ¨ç†é™åˆ¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) have advanced rapidly, yet their capacity for
> image-grounded geolocation in open-world conditions, a task that is challenging
> and of demand in real life, has not been comprehensively evaluated. We present
> EarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates
> visual recognition, step-by-step reasoning, and evidence use. EarthWhere
> comprises 810 globally distributed images across two complementary geolocation
> scales: WhereCountry (i.e., 500 multiple-choice question-answering, with
> country-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained
> street-level identification tasks requiring multi-step reasoning with optional
> web search). For evaluation, we adopt the final-prediction metrics: location
> accuracies within k km (Acc@k) for coordinates and hierarchical path scores for
> textual localization. Beyond this, we propose to explicitly score intermediate
> reasoning chains using human-verified key visual clues and a Shapley-reweighted
> thinking score that attributes credit to each clue's marginal contribution. We
> benchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere
> and report different types of final answer accuracies as well as the calibrated
> model thinking scores. Overall, Gemini-2.5-Pro achieves the best average
> accuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches
> 34.71%. We reveal that web search and reasoning do not guarantee improved
> performance when visual clues are limited, and models exhibit regional biases,
> achieving up to 42.7% higher scores in certain areas than others. These
> findings highlight not only the promise but also the persistent challenges of
> models to mitigate bias and achieve robust, fine-grained localization. We
> open-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.

