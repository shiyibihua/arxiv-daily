---
layout: default
title: Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos
---

# Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos

**arXiv**: [2510.11204v1](https://arxiv.org/abs/2510.11204) | [PDF](https://arxiv.org/pdf/2510.11204.pdf)

**ä½œè€…**: Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç±»åŽŸåž‹çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä»¥åˆ†ç±»å¤šæ ‡ç­¾ç»†ç²’åº¦æ•™è‚²è§†é¢‘ã€‚**

**å…³é”®è¯**: `å¤šæ ‡ç­¾åˆ†ç±»` `ç»†ç²’åº¦åˆ†ç±»` `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€Transformer` `æ•™è‚²è§†é¢‘åˆ†æž` `ç±»åŽŸåž‹å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿è§†é¢‘ä¸­æ£€æµ‹å„¿ç«¥æ•™è‚²å†…å®¹ï¼Œå¦‚è¯†å­—å’Œæ•°å­¦çš„ç»†ç²’åº¦å­ç±»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç±»åŽŸåž‹ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œæœ€å°åŒ–ç±»å†…è·ç¦»å¹¶æœ€å¤§åŒ–ç±»é—´è·ç¦»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨APPROVEæ•°æ®é›†ä¸Šä¼˜äºŽåŸºçº¿ï¼Œå¹¶éªŒè¯äºŽYoutube-8Må’ŒCOINã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The recent growth in the consumption of online media by children during early
> childhood necessitates data-driven tools enabling educators to filter out
> appropriate educational content for young learners. This paper presents an
> approach for detecting educational content in online videos. We focus on two
> widely used educational content classes: literacy and math. For each class, we
> choose prominent codes (sub-classes) based on the Common Core Standards. For
> example, literacy codes include `letter names', `letter sounds', and math codes
> include `counting', `sorting'. We pose this as a fine-grained multilabel
> classification problem as videos can contain multiple types of educational
> content and the content classes can get visually similar (e.g., `letter names'
> vs `letter sounds'). We propose a novel class prototypes based supervised
> contrastive learning approach that can handle fine-grained samples associated
> with multiple labels. We learn a class prototype for each class and a loss
> function is employed to minimize the distances between a class prototype and
> the samples from the class. Similarly, distances between a class prototype and
> the samples from other classes are maximized. As the alignment between visual
> and audio cues are crucial for effective comprehension, we consider a
> multimodal transformer network to capture the interaction between visual and
> audio cues in videos while learning the embedding for videos. For evaluation,
> we present a dataset, APPROVE, employing educational videos from YouTube
> labeled with fine-grained education classes by education researchers. APPROVE
> consists of 193 hours of expert-annotated videos with 19 classes. The proposed
> approach outperforms strong baselines on APPROVE and other benchmarks such as
> Youtube-8M, and COIN. The dataset is available at
> https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE

