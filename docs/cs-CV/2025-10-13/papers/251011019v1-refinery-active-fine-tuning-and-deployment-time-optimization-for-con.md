---
layout: default
title: Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies
---

# Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies

**arXiv**: [2510.11019v1](https://arxiv.org/abs/2510.11019) | [PDF](https://arxiv.org/pdf/2510.11019.pdf)

**ä½œè€…**: Bingjie Tang, Iretiayo Akinola, Jie Xu, Bowen Wen, Dieter Fox, Gaurav S. Sukhatme, Fabio Ramos, Abhishek Gupta, Yashraj Narang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRefineryæ¡†æž¶ä»¥ä¼˜åŒ–æŽ¥è§¦ä¸°å¯Œç­–ç•¥çš„å¾®è°ƒä¸Žéƒ¨ç½²**

**å…³é”®è¯**: `æœºå™¨äººè£…é…` `æ¨¡æ‹Ÿå­¦ä¹ ` `è´å¶æ–¯ä¼˜åŒ–` `é«˜æ–¯æ··åˆæ¨¡åž‹` `ç­–ç•¥å¾®è°ƒ` `éƒ¨ç½²ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡æ‹Ÿå­¦ä¹ ç­–ç•¥åœ¨å¤šæ ·åŒ–åˆå§‹æ¡ä»¶ä¸‹æ€§èƒ½æ–¹å·®é«˜ï¼Œå½±å“å·¥ä¸šåº”ç”¨
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè´å¶æ–¯ä¼˜åŒ–å¾®è°ƒç­–ç•¥å’Œé«˜æ–¯æ··åˆæ¨¡åž‹é‡‡æ ·éƒ¨ç½²åˆå§‹åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»¿çœŸæˆåŠŸçŽ‡æå‡è‡³91.51%ï¼Œæ”¯æŒå¤šéƒ¨ä»¶é“¾å¼ç»„è£…

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Simulation-based learning has enabled policies for precise, contact-rich
> tasks (e.g., robotic assembly) to reach high success rates (~80%) under high
> levels of observation noise and control error. Although such performance may be
> sufficient for research applications, it falls short of industry standards and
> makes policy chaining exceptionally brittle. A key limitation is the high
> variance in individual policy performance across diverse initial conditions. We
> introduce Refinery, an effective framework that bridges this performance gap,
> robustifying policy performance across initial conditions. We propose Bayesian
> Optimization-guided fine-tuning to improve individual policies, and Gaussian
> Mixture Model-based sampling during deployment to select initializations that
> maximize execution success. Using Refinery, we improve mean success rates by
> 10.98% over state-of-the-art methods in simulation-based learning for robotic
> assembly, reaching 91.51% in simulation and comparable performance in the real
> world. Furthermore, we demonstrate that these fine-tuned policies can be
> chained to accomplish long-horizon, multi-part
> assembly$\unicode{x2013}$successfully assembling up to 8 parts without
> requiring explicit multi-step training.

