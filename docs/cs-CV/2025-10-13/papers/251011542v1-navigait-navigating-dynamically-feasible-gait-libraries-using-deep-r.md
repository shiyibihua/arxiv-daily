---
layout: default
title: NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning
---

# NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning

**arXiv**: [2510.11542v1](https://arxiv.org/abs/2510.11542) | [PDF](https://arxiv.org/pdf/2510.11542.pdf)

**ä½œè€…**: Neil C. Janwani, Varun Madabushi, Maegan Tucker

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNaviGaitæ¡†æž¶ï¼Œç»“åˆè½¨è¿¹ä¼˜åŒ–ä¸Žå¼ºåŒ–å­¦ä¹ ï¼Œå®žçŽ°ç¨³å¥åŒè¶³è¿åŠ¨æŽ§åˆ¶ã€‚**

**å…³é”®è¯**: `åŒè¶³è¿åŠ¨æŽ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `è½¨è¿¹ä¼˜åŒ–` `æ­¥æ€åº“` `æ®‹å·®ä¿®æ­£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ å¥–åŠ±è®¾è®¡å¤æ‚ï¼Œè½¨è¿¹ä¼˜åŒ–æ–¹æ³•å¯¹æ‰°åŠ¨è„†å¼±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨ç¦»çº¿ä¼˜åŒ–æ­¥æ€åº“ï¼Œæ’å€¼ç”Ÿæˆå‚è€ƒè¿åŠ¨å¹¶æ·»åŠ æ®‹å·®ä¿®æ­£ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè®­ç»ƒæ›´å¿«ï¼Œè¿åŠ¨æ›´æŽ¥è¿‘å‚è€ƒï¼Œæå‡ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) has emerged as a powerful method to learn robust
> control policies for bipedal locomotion. Yet, it can be difficult to tune
> desired robot behaviors due to unintuitive and complex reward design. In
> comparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,
> offer more tuneable, interpretable, and mathematically grounded motion plans
> for high-dimensional legged systems. However, these methods often remain
> brittle to real-world disturbances like external perturbations.
>   In this work, we present NaviGait, a hierarchical framework that combines the
> structure of trajectory optimization with the adaptability of RL for robust and
> intuitive locomotion control. NaviGait leverages a library of offline-optimized
> gaits and smoothly interpolates between them to produce continuous reference
> motions in response to high-level commands. The policy provides both
> joint-level and velocity command residual corrections to modulate and stabilize
> the reference trajectories in the gait library. One notable advantage of
> NaviGait is that it dramatically simplifies reward design by encoding rich
> motion priors from trajectory optimization, reducing the need for finely tuned
> shaping terms and enabling more stable and interpretable learning. Our
> experimental results demonstrate that NaviGait enables faster training compared
> to conventional and imitation-based RL, and produces motions that remain
> closest to the original reference. Overall, by decoupling high-level motion
> generation from low-level correction, NaviGait offers a more scalable and
> generalizable approach for achieving dynamic and robust locomotion.

