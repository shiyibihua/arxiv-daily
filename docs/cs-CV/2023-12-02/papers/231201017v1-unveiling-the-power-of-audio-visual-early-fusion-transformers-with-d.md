---
layout: default
title: Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling
---

# Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.01017" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.01017v1</a>
  <a href="https://arxiv.org/pdf/2312.01017.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.01017v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.01017v1', 'Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shentong Mo, Pedro Morgado

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.MM, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ©ç å»ºæ¨¡çš„éŸ³è§†é¢‘æ—©æœŸèåˆTransformerï¼Œæå‡å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³è§†é¢‘èåˆ` `æ—©æœŸèåˆ` `Transformer` `æ©ç å»ºæ¨¡` `å¤šæ¨¡æ€å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—©æœŸèåˆéŸ³è§†é¢‘æ¨¡å‹è®­ç»ƒå›°éš¾ï¼Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›å—é™ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚
2. åˆ©ç”¨æ©ç é‡å»ºæ¡†æ¶è®­ç»ƒæ—©æœŸèåˆéŸ³è§†é¢‘ç¼–ç å™¨ï¼Œå¹¶è®¾è®¡æ³¨æ„åŠ›èåˆæ¨¡å—æ•è·å±€éƒ¨éŸ³è§†é¢‘äº¤äº’ã€‚
3. åœ¨éŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€è§†è§‰å£°éŸ³å®šä½ç­‰ä»»åŠ¡ä¸Šï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»å…·å¤‡æ•´åˆå¬è§‰å’Œè§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä»è€Œæ›´æ·±å…¥åœ°ç†è§£å‘¨å›´ç¯å¢ƒã€‚è®¤çŸ¥å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦ç ”ç©¶è¡¨æ˜ï¼ŒéŸ³è§†é¢‘çº¿ç´¢çš„æ—©æœŸèåˆä¸ºå¼€å‘å¤šæ¨¡æ€æ„ŸçŸ¥æ¨¡å‹æä¾›äº†æœ‰æ½œåŠ›çš„é€”å¾„ã€‚ç„¶è€Œï¼Œè®­ç»ƒæ—©æœŸèåˆæ¶æ„é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„å¢å¼ºéœ€è¦å¼ºå¤§çš„å­¦ä¹ æ¡†æ¶æ¥é©¾é©­å…¶å¢å¼ºçš„èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡åˆ©ç”¨åœ¨å•æ¨¡æ€è®¾ç½®ä¸­å·²æˆåŠŸçš„æ©ç é‡å»ºæ¡†æ¶æ¥è®­ç»ƒå…·æœ‰æ—©æœŸèåˆçš„éŸ³è§†é¢‘ç¼–ç å™¨ï¼Œä»è€Œåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ•è·å±€éƒ¨éŸ³é¢‘å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹æ•è·ç»†ç²’åº¦äº¤äº’çš„èƒ½åŠ›ã€‚è™½ç„¶æœ‰æ•ˆï¼Œä½†éšç€å±€éƒ¨è¡¨ç¤ºæ•°é‡çš„å¢åŠ ï¼Œæ­¤è¿‡ç¨‹åœ¨è®¡ç®—ä¸Šå˜å¾—éš¾ä»¥å¤„ç†ã€‚å› æ­¤ï¼Œä¸ºäº†è§£å†³è®¡ç®—å¤æ‚æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£ç¨‹åºï¼Œè¯¥ç¨‹åºåœ¨è¡¨ç¤ºéŸ³è§†é¢‘äº¤äº’ä¹‹å‰å¯¹å±€éƒ¨è¡¨ç¤ºè¿›è¡Œåˆ†è§£ã€‚åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€è§†è§‰å£°éŸ³å®šä½ã€å£°éŸ³åˆ†ç¦»å’ŒéŸ³è§†é¢‘åˆ†å‰²æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚è¿™äº›è´¡çŒ®ä½¿å¾—èƒ½å¤Ÿæœ‰æ•ˆè®­ç»ƒæ·±åº¦é›†æˆçš„éŸ³è§†é¢‘æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—æé«˜æ—©æœŸèåˆæ¶æ„çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³éŸ³è§†é¢‘å¤šæ¨¡æ€å­¦ä¹ ä¸­ï¼Œæ—©æœŸèåˆæ¨¡å‹è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨éŸ³è§†é¢‘ä¿¡æ¯çš„æ—©æœŸäº¤äº’ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚ç—›ç‚¹åœ¨äºæ¨¡å‹å¤æ‚åº¦é«˜ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œéš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„éŸ³è§†é¢‘å…³è”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ©ç å»ºæ¨¡ï¼ˆMasked Modelingï¼‰ä½œä¸ºè‡ªç›‘ç£å­¦ä¹ çš„æ‰‹æ®µï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¥æœ‰æ•ˆè®­ç»ƒæ—©æœŸèåˆçš„éŸ³è§†é¢‘Transformeræ¨¡å‹ã€‚é€šè¿‡æ©ç éƒ¨åˆ†è¾“å…¥ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ éŸ³è§†é¢‘ä¹‹é—´çš„å…³è”ï¼Œå¹¶é‡å»ºè¢«æ©ç çš„ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) éŸ³è§†é¢‘ç‰¹å¾æå–æ¨¡å—ï¼šåˆ†åˆ«æå–éŸ³é¢‘å’Œè§†é¢‘çš„å±€éƒ¨ç‰¹å¾è¡¨ç¤ºã€‚2) æ—©æœŸèåˆæ¨¡å—ï¼šå°†æå–çš„éŸ³è§†é¢‘ç‰¹å¾è¿›è¡Œèåˆï¼Œå½¢æˆç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚3) åŸºäºæ³¨æ„åŠ›çš„äº¤äº’æ¨¡å—ï¼šåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡å±€éƒ¨éŸ³è§†é¢‘è¡¨ç¤ºä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚4) æ©ç å»ºæ¨¡æ¨¡å—ï¼šéšæœºæ©ç éƒ¨åˆ†è¾“å…¥ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹é‡å»ºè¢«æ©ç çš„ä¿¡æ¯ã€‚5) é¢„æµ‹æ¨¡å—ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚ï¼Œè¿›è¡ŒéŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€è§†è§‰å£°éŸ³å®šä½ç­‰ä»»åŠ¡çš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†æ©ç å»ºæ¨¡å¼•å…¥åˆ°æ—©æœŸèåˆçš„éŸ³è§†é¢‘Transformeræ¨¡å‹è®­ç»ƒä¸­ï¼Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚2) æå‡ºäº†åŸºäºæ³¨æ„åŠ›çš„èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å±€éƒ¨éŸ³è§†é¢‘è¡¨ç¤ºä¹‹é—´çš„ç»†ç²’åº¦äº¤äº’ã€‚3) é’ˆå¯¹è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†åˆ†è§£å±€éƒ¨è¡¨ç¤ºçš„æ–¹æ³•ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ©ç å»ºæ¨¡æ–¹é¢ï¼Œé‡‡ç”¨äº†éšæœºæ©ç ç­–ç•¥ï¼Œæ©ç æ¯”ä¾‹ä¸ºä¸€å®šçš„ç™¾åˆ†æ¯”ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’Œä»»åŠ¡ç›¸å…³çš„æŸå¤±ã€‚æ³¨æ„åŠ›æ¨¡å—é‡‡ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ•æ‰ä¸åŒå±‚é¢çš„éŸ³è§†é¢‘äº¤äº’ã€‚ä¸ºäº†é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œè®ºæ–‡æå‡ºäº†å¯¹å±€éƒ¨è¡¨ç¤ºè¿›è¡Œåˆ†è§£çš„æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨çº¿æ€§æŠ•å½±æˆ–èšç±»ç­‰æ–¹å¼ï¼Œå‡å°‘äº†æ³¨æ„åŠ›è®¡ç®—çš„è§„æ¨¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éŸ³é¢‘äº‹ä»¶åˆ†ç±»ã€è§†è§‰å£°éŸ³å®šä½ã€å£°éŸ³åˆ†ç¦»å’ŒéŸ³è§†é¢‘åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨éŸ³é¢‘äº‹ä»¶åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡äº†X%ï¼›åœ¨è§†è§‰å£°éŸ³å®šä½ä»»åŠ¡ä¸Šï¼Œå®šä½ç²¾åº¦æå‡äº†Y%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€æœºå™¨äººæ„ŸçŸ¥ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨éŸ³è§†é¢‘ä¿¡æ¯è¿›è¡Œå¼‚å¸¸äº‹ä»¶æ£€æµ‹ï¼›åœ¨æœºå™¨äººæ„ŸçŸ¥ä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£å‘¨å›´ç¯å¢ƒï¼Œè¿›è¡Œå¯¼èˆªå’Œäº¤äº’ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æé«˜è½¦è¾†å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œæå‡å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans possess a remarkable ability to integrate auditory and visual information, enabling a deeper understanding of the surrounding environment. This early fusion of audio and visual cues, demonstrated through cognitive psychology and neuroscience research, offers promising potential for developing multimodal perception models. However, training early fusion architectures poses significant challenges, as the increased model expressivity requires robust learning frameworks to harness their enhanced capabilities. In this paper, we address this challenge by leveraging the masked reconstruction framework, previously successful in unimodal settings, to train audio-visual encoders with early fusion. Additionally, we propose an attention-based fusion module that captures interactions between local audio and visual representations, enhancing the model's ability to capture fine-grained interactions. While effective, this procedure can become computationally intractable, as the number of local representations increases. Thus, to address the computational complexity, we propose an alternative procedure that factorizes the local representations before representing audio-visual interactions. Extensive evaluations on a variety of datasets demonstrate the superiority of our approach in audio-event classification, visual sound localization, sound separation, and audio-visual segmentation. These contributions enable the efficient training of deeply integrated audio-visual models and significantly advance the usefulness of early fusion architectures.

