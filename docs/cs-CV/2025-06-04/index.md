---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-04
---

# cs.CVï¼ˆ2025-06-04ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250604174v1-flexgs-train-once-deploy-everywhere-with-many-in-one-flexible-3d-gau.html">FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting</a></td>
  <td>æå‡ºFlexGSä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“å†…å­˜é™åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04174v1" data-paper-url="./papers/250604174v1-flexgs-train-once-deploy-everywhere-with-many-in-one-flexible-3d-gau.html" onclick="toggleFavorite(this, '2506.04174v1', 'FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250604444v1-photoreal-scene-reconstruction-from-an-egocentric-device.html">Photoreal Scene Reconstruction from an Egocentric Device</a></td>
  <td>æå‡ºè§†è§‰æƒ¯æ€§æŸè°ƒæ•´ä»¥è§£å†³æ»šåŠ¨å¿«é—¨ç›¸æœºé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04444v1" data-paper-url="./papers/250604444v1-photoreal-scene-reconstruction-from-an-egocentric-device.html" onclick="toggleFavorite(this, '2506.04444v1', 'Photoreal Scene Reconstruction from an Egocentric Device')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250604351v1-hugediff-3d-human-generation-via-diffusion-with-gaussian-splatting.html">HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</a></td>
  <td>æå‡ºHuGeDiffä»¥è§£å†³3Däººç±»ç”Ÿæˆçš„æ§åˆ¶ä¸ç»†èŠ‚é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04351v1" data-paper-url="./papers/250604351v1-hugediff-3d-human-generation-via-diffusion-with-gaussian-splatting.html" onclick="toggleFavorite(this, '2506.04351v1', 'HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250604134v4-unicue-unified-recognition-and-generation-framework-for-chinese-cued.html">UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</a></td>
  <td>æå‡ºUniCUEæ¡†æ¶ä»¥è§£å†³ä¸­æ–‡æ‰‹è¯­è§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">semantic mapping</span> <span class="paper-tag">semantic map</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04134v4" data-paper-url="./papers/250604134v4-unicue-unified-recognition-and-generation-framework-for-chinese-cued.html" onclick="toggleFavorite(this, '2506.04134v4', 'UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250604225v1-voyager-long-range-and-world-consistent-video-diffusion-for-explorab.html">Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation</a></td>
  <td>æå‡ºVoyagerä»¥è§£å†³é•¿è·ç¦»ä¸€è‡´æ€§3Dåœºæ™¯ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04225v1" data-paper-url="./papers/250604225v1-voyager-long-range-and-world-consistent-video-diffusion-for-explorab.html" onclick="toggleFavorite(this, '2506.04225v1', 'Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250604106v1-globalbuildingatlas-an-open-global-and-complete-dataset-of-building-.html">GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models</a></td>
  <td>æå‡ºGlobalBuildingAtlasä»¥è§£å†³å…¨çƒå»ºç­‘æ•°æ®ç¼ºä¹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">height map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04106v1" data-paper-url="./papers/250604106v1-globalbuildingatlas-an-open-global-and-complete-dataset-of-building-.html" onclick="toggleFavorite(this, '2506.04106v1', 'GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/250604141v1-mmr-v-whats-left-unsaid-a-benchmark-for-multimodal-deep-reasoning-in.html">MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos</a></td>
  <td>æå‡ºMMR-Vä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘æ¨ç†çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04141v1" data-paper-url="./papers/250604141v1-mmr-v-whats-left-unsaid-a-benchmark-for-multimodal-deep-reasoning-in.html" onclick="toggleFavorite(this, '2506.04141v1', 'MMR-V: What&#39;s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250604039v2-mitigating-hallucinations-in-large-vision-language-models-via-entity.html">Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization</a></td>
  <td>æå‡ºå®ä½“ä¸­å¿ƒå¤šæ¨¡æ€åå¥½ä¼˜åŒ–ä»¥è§£å†³å¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04039v2" data-paper-url="./papers/250604039v2-mitigating-hallucinations-in-large-vision-language-models-via-entity.html" onclick="toggleFavorite(this, '2506.04039v2', 'Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250604034v1-rex-thinker-grounded-object-referring-via-chain-of-thought-reasoning.html">Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning</a></td>
  <td>æå‡ºRex-Thinkerä»¥è§£å†³å¯¹è±¡æŒ‡ç§°çš„å¯è§£é‡Šæ€§ä¸å¯é æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04034v1" data-paper-url="./papers/250604034v1-rex-thinker-grounded-object-referring-via-chain-of-thought-reasoning.html" onclick="toggleFavorite(this, '2506.04034v1', 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250604353v1-rexvqa-a-large-scale-visual-question-answering-benchmark-for-general.html">ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding</a></td>
  <td>æå‡ºReXVQAä»¥è§£å†³èƒ¸éƒ¨Xå…‰è§†è§‰é—®ç­”åŸºå‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04353v1" data-paper-url="./papers/250604353v1-rexvqa-a-large-scale-visual-question-answering-benchmark-for-general.html" onclick="toggleFavorite(this, '2506.04353v1', 'ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250604220v3-struct2d-a-perception-guided-framework-for-spatial-reasoning-in-mllm.html">Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs</a></td>
  <td>æå‡ºStruct2Dæ¡†æ¶ä»¥è§£å†³MLLMsç©ºé—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04220v3" data-paper-url="./papers/250604220v3-struct2d-a-perception-guided-framework-for-spatial-reasoning-in-mllm.html" onclick="toggleFavorite(this, '2506.04220v3', 'Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250604224v1-seeing-in-the-dark-benchmarking-egocentric-3d-vision-with-the-oxford.html">Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset</a></td>
  <td>æå‡ºOxford Day-and-Nightæ•°æ®é›†ä»¥è§£å†³å¤œé—´è§†è§‰é‡å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04224v1" data-paper-url="./papers/250604224v1-seeing-in-the-dark-benchmarking-egocentric-3d-vision-with-the-oxford.html" onclick="toggleFavorite(this, '2506.04224v1', 'Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250605414v1-savvy-spatial-awareness-via-audio-visual-llms-through-seeing-and-hea.html">SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing</a></td>
  <td>æå‡ºSAVVYä»¥è§£å†³åŠ¨æ€3Dç©ºé—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05414v1" data-paper-url="./papers/250605414v1-savvy-spatial-awareness-via-audio-visual-llms-through-seeing-and-hea.html" onclick="toggleFavorite(this, '2506.05414v1', 'SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250604209v1-language-image-alignment-with-fixed-text-encoders.html">Language-Image Alignment with Fixed Text Encoders</a></td>
  <td>æå‡ºLIFTæ–¹æ³•ä»¥ç®€åŒ–è¯­è¨€-å›¾åƒå¯¹é½è¿‡ç¨‹</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04209v1" data-paper-url="./papers/250604209v1-language-image-alignment-with-fixed-text-encoders.html" onclick="toggleFavorite(this, '2506.04209v1', 'Language-Image Alignment with Fixed Text Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250605409v1-object-level-self-distillation-for-vision-pretraining.html">Object-level Self-Distillation for Vision Pretraining</a></td>
  <td>æå‡ºå¯¹è±¡çº§è‡ªè’¸é¦æ–¹æ³•ä»¥è§£å†³å›¾åƒçº§è‡ªè’¸é¦å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05409v1" data-paper-url="./papers/250605409v1-object-level-self-distillation-for-vision-pretraining.html" onclick="toggleFavorite(this, '2506.05409v1', 'Object-level Self-Distillation for Vision Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250603662v4-zero-shot-temporal-interaction-localization-for-egocentric-videos.html">Zero-Shot Temporal Interaction Localization for Egocentric Videos</a></td>
  <td>æå‡ºEgoLocä»¥è§£å†³è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„æ—¶åºäº¤äº’å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span> <span class="paper-tag">egocentric</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03662v4" data-paper-url="./papers/250603662v4-zero-shot-temporal-interaction-localization-for-egocentric-videos.html" onclick="toggleFavorite(this, '2506.03662v4', 'Zero-Shot Temporal Interaction Localization for Egocentric Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250604363v1-worldprediction-a-benchmark-for-high-level-world-modeling-and-long-h.html">WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning</a></td>
  <td>æå‡ºWorldPredictionåŸºå‡†ä»¥è§£å†³é«˜å±‚æ¬¡ä¸–ç•Œå»ºæ¨¡ä¸é•¿è¿œè§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04363v1" data-paper-url="./papers/250604363v1-worldprediction-a-benchmark-for-high-level-world-modeling-and-long-h.html" onclick="toggleFavorite(this, '2506.04363v1', 'WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)