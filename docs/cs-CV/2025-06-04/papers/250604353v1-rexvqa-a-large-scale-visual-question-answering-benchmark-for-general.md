---
layout: default
title: ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding
---

# ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04353" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04353v1</a>
  <a href="https://arxiv.org/pdf/2506.04353.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04353v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04353v1', 'ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ankit Pal, Jung-Oh Lee, Xiaoman Zhang, Malaikannan Sankarasubbu, Seunghyeon Roh, Won Jung Kim, Meesun Lee, Pranav Rajpurkar

**åˆ†ç±»**: cs.CV, cs.AI, cs.CE, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/rajpurkarlab)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReXVQAä»¥è§£å†³èƒ¸éƒ¨Xå…‰è§†è§‰é—®ç­”åŸºå‡†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰é—®ç­”` `èƒ¸éƒ¨Xå…‰` `æ”¾å°„å­¦` `å¤šæ¨¡æ€å­¦ä¹ ` `äººå·¥æ™ºèƒ½` `åŒ»å­¦å½±åƒ` `æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰é—®ç­”åŸºå‡†åœ¨èƒ¸éƒ¨Xå…‰é¢†åŸŸç¼ºä¹å¤šæ ·æ€§å’Œä¸´åºŠçœŸå®æ€§ï¼Œé™åˆ¶äº†AIæ¨¡å‹çš„è¯„ä¼°å’Œåº”ç”¨ã€‚
2. ReXVQAé€šè¿‡å¼•å…¥å¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œæ¶µç›–æ”¾å°„å­¦æ¨ç†æŠ€èƒ½ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨äº†AIåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„åº”ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedGemmaæ¨¡å‹çš„å‡†ç¡®ç‡ä¸º83.24%ï¼Œè¶…è¿‡äº†äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¡¨ç°ï¼Œå±•ç¤ºäº†AIåœ¨èƒ¸éƒ¨Xå…‰è§£è¯»ä¸­çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ReXVQAï¼Œè¿™æ˜¯èƒ¸éƒ¨æ”¾å°„å­¦é¢†åŸŸæœ€å¤§çš„è§†è§‰é—®ç­”åŸºå‡†ï¼ŒåŒ…å«çº¦696,000ä¸ªé—®é¢˜å’Œ160,000ä¸ªèƒ¸éƒ¨Xå…‰ç ”ç©¶ã€‚ä¸ä»¥å¾€ä¾èµ–æ¨¡æ¿æŸ¥è¯¢çš„æ–¹æ³•ä¸åŒï¼ŒReXVQAå¼•å…¥äº†å¤šæ ·ä¸”ä¸´åºŠçœŸå®çš„ä»»åŠ¡ï¼Œæ¶µç›–äº”ç§æ ¸å¿ƒæ”¾å°„å­¦æ¨ç†æŠ€èƒ½ã€‚æˆ‘ä»¬è¯„ä¼°äº†å…«ç§æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹MedGemmaè¾¾åˆ°äº†83.24%çš„æ•´ä½“å‡†ç¡®ç‡ã€‚é€šè¿‡ä¸äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿçš„æ¯”è¾ƒç ”ç©¶ï¼ŒMedGemmaçš„è¡¨ç°è¶…è¶Šäº†äººç±»ä¸“å®¶ï¼Œæ ‡å¿—ç€AIåœ¨èƒ¸éƒ¨Xå…‰è§£è¯»ä¸­çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚ReXVQAä¸ºè¯„ä¼°é€šç”¨æ”¾å°„å­¦AIç³»ç»Ÿè®¾ç«‹äº†æ–°æ ‡å‡†ï¼Œå¹¶å°†æ•°æ®é›†å¼€æ”¾æºä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³èƒ¸éƒ¨Xå…‰è§†è§‰é—®ç­”é¢†åŸŸç¼ºä¹å…¨é¢ã€çœŸå®åŸºå‡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–æ¨¡æ¿æŸ¥è¯¢ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°AIæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReXVQAé€šè¿‡è®¾è®¡å¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œåæ˜ ä¸´åºŠå®é™…éœ€æ±‚ï¼Œæ¶µç›–äº”ç§æ ¸å¿ƒæ”¾å°„å­¦æ¨ç†æŠ€èƒ½ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ä»»åŠ¡è®¾è®¡å’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†åŒ…å«å¤§é‡é—®é¢˜ä¸Xå…‰å›¾åƒçš„é…å¯¹ï¼Œä»»åŠ¡è®¾è®¡åˆ™èšç„¦äºä¸´åºŠæ¨ç†æŠ€èƒ½çš„è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šReXVQAçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤šæ ·åŒ–çš„ä»»åŠ¡è®¾è®¡å’Œå¤§è§„æ¨¡æ•°æ®é›†ï¼Œçªç ´äº†ä»¥å¾€åŸºäºæ¨¡æ¿çš„è¯„ä¼°æ–¹å¼ï¼Œä½¿å¾—AIæ¨¡å‹çš„è¯„ä¼°æ›´å…·ä¸´åºŠç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è¯„ä¼°ä¸­ï¼Œä½¿ç”¨äº†å¤šç§å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿçš„æ¯”è¾ƒç ”ç©¶ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedGemmaæ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰è§£è¯»ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°83.24%ï¼Œè¶…è¿‡äº†äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿçš„æœ€ä½³è¡¨ç°ï¼ˆ77.27%ï¼‰ï¼Œå±•ç¤ºäº†AIåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€æ”¾å°„å­¦æ•™è‚²å’ŒAIè¾…åŠ©è¯Šæ–­ã€‚é€šè¿‡æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ï¼ŒReXVQAå°†æ¨åŠ¨AIåœ¨ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œæå‡æ”¾å°„å­¦çš„å·¥ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

