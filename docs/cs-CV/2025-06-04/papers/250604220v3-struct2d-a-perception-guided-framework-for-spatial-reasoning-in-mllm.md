---
layout: default
title: Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs
---

# Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04220" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04220v3</a>
  <a href="https://arxiv.org/pdf/2506.04220.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04220v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04220v3', 'Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: NeurIPS 2025, code link: https://github.com/neu-vi/struct2d

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStruct2Dæ¡†æ¶ä»¥è§£å†³MLLMsç©ºé—´æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´æ¨ç†` `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–2Dè¡¨ç¤º` `é¸Ÿç°å›¾` `å¯¹è±¡æ ‡è®°` `æŒ‡ä»¤è°ƒä¼˜` `3Dé—®ç­”` `æ™ºèƒ½äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¤šä¾èµ–æ˜¾å¼çš„3Dè¾“å…¥æˆ–ç‰¹å®šæ¨¡å‹æ¶æ„ï¼Œé™åˆ¶äº†MLLMsåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. æœ¬ç ”ç©¶æå‡ºStruct2Dæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çš„2Dè¡¨ç¤ºæ¥å¼•å¯¼ç©ºé—´æ¨ç†ï¼Œæ¢ç´¢MLLMsåœ¨ä»…ä½¿ç”¨2Dè¾“å…¥æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„å¼€æºMLLMåœ¨3Dé—®ç­”ã€å¯†é›†æ ‡æ³¨å’Œå¯¹è±¡å®šä½ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œè§£é”ç©ºé—´æ¨ç†èƒ½åŠ›å¯¹äºæ™ºèƒ½ä¸3Dç¯å¢ƒçš„äº¤äº’è‡³å…³é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶é€šå¸¸ä¾èµ–äºæ˜¾å¼çš„3Dè¾“å…¥æˆ–ä¸“é—¨çš„æ¨¡å‹æ¶æ„ï¼Œè€Œæœ¬ç ”ç©¶æå‡ºStruct2Dæ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„2Dè¡¨ç¤ºï¼ˆå¦‚é¸Ÿç°å›¾å’Œå¯¹è±¡æ ‡è®°ï¼‰è¿›è¡Œç©ºé—´æ¨ç†ã€‚é€šè¿‡å¯¹é—­æºMLLMsçš„é›¶-shotåˆ†æï¼Œå‘ç°å…¶åœ¨å¤„ç†ç›¸å¯¹æ–¹å‘ä¼°è®¡å’Œè·¯å¾„è§„åˆ’ç­‰ä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†åŒ…å«20ä¸‡ä¸ªç»†ç²’åº¦é—®ç­”å¯¹çš„å¤§è§„æ¨¡æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†Struct2D-Setï¼Œå¹¶åœ¨å¼€æºMLLMä¸Šè¿›è¡Œå¾®è°ƒï¼Œå–å¾—äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰æ€§è¡¨ç°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œç»“æ„åŒ–çš„2Dè¾“å…¥å¯ä»¥æœ‰æ•ˆåœ°è¿æ¥æ„ŸçŸ¥ä¸è¯­è¨€æ¨ç†ï¼Œä¸”æ— éœ€æ˜¾å¼çš„3Dè¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­å¯¹æ˜¾å¼3Dè¾“å…¥çš„ä¾èµ–ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†3Dç¯å¢ƒæ—¶å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºStruct2Dæ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„2Dè¡¨ç¤ºï¼ˆå¦‚é¸Ÿç°å›¾å’Œå¯¹è±¡æ ‡è®°ï¼‰æ¥å¼•å¯¼MLLMsè¿›è¡Œç©ºé—´æ¨ç†ï¼Œæ¢ç´¢å…¶åœ¨ä»…ä½¿ç”¨2Dè¾“å…¥æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¸Ÿç°å›¾è¾“å…¥ã€å¯¹è±¡æ ‡è®°å’Œå¯¹è±¡ä¸­å¿ƒå…ƒæ•°æ®ï¼Œå¿…è¦æ—¶å¯åŠ å…¥è‡ªæˆ‘ä¸­å¿ƒå…³é”®å¸§ã€‚æ¡†æ¶é€šè¿‡æ„ŸçŸ¥å¼•å¯¼çš„æç¤ºæ–¹å¼ï¼Œç»“åˆè¿™äº›2Dä¿¡æ¯è¿›è¡Œç©ºé—´æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç»“æ„åŒ–çš„2Dè¾“å…¥å®ç°äº†æ„ŸçŸ¥ä¸è¯­è¨€æ¨ç†çš„æœ‰æ•ˆè¿æ¥ï¼Œé¿å…äº†å¯¹æ˜¾å¼3Dè¡¨ç¤ºçš„éœ€æ±‚ï¼Œæ‹“å®½äº†MLLMsçš„åº”ç”¨èŒƒå›´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œç”Ÿæˆäº†åŒ…å«20ä¸‡ä¸ªç»†ç²’åº¦é—®ç­”å¯¹çš„Struct2D-Setï¼Œæ¶µç›–å…«ä¸ªç©ºé—´æ¨ç†ç±»åˆ«ï¼Œå¹¶åœ¨å¼€æºMLLMï¼ˆQwen2.5VLï¼‰ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œé‡‡ç”¨äº†é€‚å½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„å¼€æºMLLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨3Dé—®ç­”ã€å¯†é›†æ ‡æ³¨å’Œå¯¹è±¡å®šä½ä»»åŠ¡ä¸Šï¼Œå–å¾—äº†ä¸é—­æºæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†ç»“æ„åŒ–2Dè¾“å…¥çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰3Dç¯å¢ƒä¸­çš„äººæœºäº¤äº’ã€‚é€šè¿‡æå‡MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶çš„äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼ŒStruct2Dæ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°æ¨å¹¿å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unlocking spatial reasoning in Multimodal Large Language Models (MLLMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can MLLMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines bird's-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source MLLMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source MLLM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in MLLMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research.

