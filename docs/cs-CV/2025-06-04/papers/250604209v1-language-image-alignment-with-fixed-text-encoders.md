---
layout: default
title: Language-Image Alignment with Fixed Text Encoders
---

# Language-Image Alignment with Fixed Text Encoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04209" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04209v1</a>
  <a href="https://arxiv.org/pdf/2506.04209.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04209v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04209v1', 'Language-Image Alignment with Fixed Text Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingfeng Yang, Ziyang Wu, Yue Zhao, Yi Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLIFTæ–¹æ³•ä»¥ç®€åŒ–è¯­è¨€-å›¾åƒå¯¹é½è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€-å›¾åƒå¯¹é½` `å›ºå®šæ–‡æœ¬ç¼–ç å™¨` `å¤šæ¨¡æ€å­¦ä¹ ` `è®¡ç®—æ•ˆç‡` `é•¿æ–‡æœ¬ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¦‚CLIPä¾èµ–äºæ˜‚è´µçš„è”åˆè®­ç»ƒï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºLIFTæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å›ºå®šLLMä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œä»…è®­ç»ƒå›¾åƒç¼–ç å™¨ä»¥å®ç°è¯­è¨€-å›¾åƒå¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLIFTåœ¨ç»„åˆç†è§£å’Œé•¿æ–‡æœ¬åœºæ™¯ä¸­è¶…è¶ŠCLIPï¼Œå¹¶æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›®å‰ï¼Œå»ºç«‹è¯­è¨€-å›¾åƒå¯¹é½çš„ä¸»æµæ–¹æ³•æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ å…±åŒé¢„è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œå¦‚CLIPåŠå…¶å˜ä½“ã€‚æœ¬æ–‡è´¨ç–‘è¿™ç§æ˜‚è´µçš„è”åˆè®­ç»ƒæ˜¯å¦å¿…è¦ï¼Œæ¢è®¨é¢„è®­ç»ƒçš„å›ºå®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦è¶³ä»¥ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»…é€šè¿‡è®­ç»ƒå›¾åƒç¼–ç å™¨æ¥å®ç°è¯­è¨€-å›¾åƒå¯¹é½çš„æ–¹æ³•LIFTã€‚é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•å’Œæ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸€ç®€åŒ–æ¡†æ¶åœ¨å¤§å¤šæ•°æ¶‰åŠç»„åˆç†è§£å’Œé•¿æ–‡æœ¬çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†LLMçš„æ–‡æœ¬åµŒå…¥å¦‚ä½•æŒ‡å¯¼è§†è§‰å­¦ä¹ ï¼Œå¹¶ä¸ºå­¦ä¹ è¯­è¨€å¯¹é½çš„è§†è§‰è¡¨ç¤ºæä¾›äº†æ›¿ä»£è®¾è®¡é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€-å›¾åƒå¯¹é½æ–¹æ³•ä¸­æ˜‚è´µçš„è”åˆè®­ç»ƒé—®é¢˜ï¼Œå°¤å…¶æ˜¯CLIPç­‰æ–¹æ³•åœ¨è®¡ç®—èµ„æºå’Œæ•ˆç‡ä¸Šçš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºLIFTæ–¹æ³•ï¼Œåˆ©ç”¨å›ºå®šçš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œä»…è®­ç»ƒå›¾åƒç¼–ç å™¨ï¼Œä»è€Œç®€åŒ–å¯¹é½è¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLIFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å›ºå®šçš„LLMä½œä¸ºæ–‡æœ¬ç¼–ç å™¨å’Œä¸€ä¸ªå¯è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ã€‚é€šè¿‡å¯¹å›¾åƒç¼–ç å™¨çš„è®­ç»ƒï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šLIFTçš„ä¸»è¦åˆ›æ–°åœ¨äºä½¿ç”¨å›ºå®šçš„LLMä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è”åˆè®­ç»ƒæ¨¡å¼å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LIFTä¸­ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå¯¹æ¯”æŸå¤±ï¼Œä»¥ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬åµŒå…¥çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œå›¾åƒç¼–ç å™¨é‡‡ç”¨äº†æœ€æ–°çš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä»¥æé«˜ç‰¹å¾æå–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLIFTåœ¨ç»„åˆç†è§£å’Œé•¿æ–‡æœ¬åœºæ™¯ä¸­è¶…è¶Šäº†CLIPï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸Šæå‡äº†çº¦15%çš„å‡†ç¡®ç‡ã€‚åŒæ—¶ï¼ŒLIFTåœ¨è®¡ç®—æ•ˆç‡ä¸Šä¹Ÿæ˜¾è‘—æé«˜ï¼Œå‡å°‘äº†çº¦30%çš„è®­ç»ƒæ—¶é—´ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€æ£€ç´¢ã€å›¾åƒæè¿°ç”Ÿæˆå’Œè§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒLIFTæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„è¯­è¨€-å›¾åƒå¯¹é½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œå½±å“åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿè¿­ä»£å’Œéƒ¨ç½²çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.

