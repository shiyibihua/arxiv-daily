---
layout: default
title: MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos
---

# MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04141" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04141v1</a>
  <a href="https://arxiv.org/pdf/2506.04141.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04141v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04141v1', 'MMR-V: What\'s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: Project Page: https://mmr-v.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMR-Vä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘æ¨ç†çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†é¢‘ç†è§£` `é•¿è·ç¦»æ¨ç†` `æ·±åº¦å­¦ä¹ ` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘åŸºå‡†ä¸»è¦å…³æ³¨ç†è§£ä»»åŠ¡ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒå¤šå¸§è¯æ®çš„æ¨ç†ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
2. MMR-VåŸºå‡†è¦æ±‚æ¨¡å‹è¿›è¡Œé•¿è·ç¦»çš„å¤šå¸§æ¨ç†ï¼Œå¹¶è¶…è¶Šç®€å•çš„æ„ŸçŸ¥ï¼Œéœ€å¯¹éšè—ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸Šè¡¨ç°ä¸ä½³ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º52.5%ï¼Œæ¨ç†å¢å¼ºç­–ç•¥æ•ˆæœæœ‰é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘çš„é¡ºåºç»“æ„å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®šä½å¤šå¸§è¯æ®å’Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç†è§£ä»»åŠ¡ä¸Šï¼Œä»…è¦æ±‚æ¨¡å‹åŒ¹é…é—®é¢˜ä¸­æåˆ°çš„å¸§ï¼ˆç§°ä¸ºâ€œé—®é¢˜å¸§â€ï¼‰å¹¶æ„ŸçŸ¥å°‘é‡ç›¸é‚»å¸§ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MMR-Vï¼šä¸€ä¸ªç”¨äºè§†é¢‘å¤šæ¨¡æ€æ·±åº¦æ¨ç†çš„åŸºå‡†ã€‚è¯¥åŸºå‡†çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šé•¿è·ç¦»å¤šå¸§æ¨ç†ã€è¶…è¶Šæ„ŸçŸ¥çš„æ¨ç†éœ€æ±‚ã€æ‰‹åŠ¨æ³¨é‡Šçš„å¯é æ€§ä»¥åŠå‡å°‘æ¨¡å‹æ·å¾„çš„æ··æ·†æ€§è®¾è®¡ã€‚MMR-VåŒ…å«317ä¸ªè§†é¢‘å’Œ1,257ä¸ªä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸Šä»é¢ä¸´å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹o4-miniçš„å‡†ç¡®ç‡ä»…ä¸º52.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰è§†é¢‘åŸºå‡†æ— æ³•æœ‰æ•ˆæ”¯æŒå¤šæ¨¡æ€æ¨ç†ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è·ç¦»å¤šå¸§è¯æ®çš„å®šä½å’Œåˆ†ææ–¹é¢ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºç®€å•çš„ç†è§£ä»»åŠ¡ï¼Œæœªèƒ½è€ƒè™‘å¤æ‚çš„æ¨ç†éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªæ–°çš„åŸºå‡†MMR-Vï¼Œè¦æ±‚æ¨¡å‹åœ¨é•¿è·ç¦»ã€å¤šå¸§çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸”éœ€è¦è¶…è¶Šç®€å•çš„æ„ŸçŸ¥ï¼Œæ·±å…¥åˆ†æéšè—ä¿¡æ¯ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ¨åŠ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMR-Vçš„æ•´ä½“æ¶æ„åŒ…æ‹¬317ä¸ªè§†é¢‘å’Œ1,257ä¸ªä»»åŠ¡ï¼Œä»»åŠ¡è®¾è®¡ä¸Šå¼ºè°ƒé•¿è·ç¦»æ¨ç†å’Œå¤æ‚çš„æ¨ç†éœ€æ±‚ã€‚æ¯ä¸ªä»»åŠ¡éƒ½ç»è¿‡æ‰‹åŠ¨æ³¨é‡Šï¼Œä»¥ç¡®ä¿ä¸çœŸå®ç”¨æˆ·ç†è§£çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†é•¿è·ç¦»å¤šå¸§æ¨ç†çš„éœ€æ±‚ï¼Œå¹¶è®¾è®¡äº†æ··æ·†æ€§æ³¨é‡Šç­–ç•¥ï¼Œä»¥å‡å°‘æ¨¡å‹çš„æ·å¾„è¡Œä¸ºã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨çŸ­æœŸçš„æ„ŸçŸ¥åŒ¹é…ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†æ‰‹åŠ¨æ³¨é‡Šçš„æ–¹å¼æ¥ç¡®ä¿ä»»åŠ¡çš„å¯é æ€§ï¼Œå¹¶è®¾è®¡äº†å¤šç§å¹²æ‰°é¡¹ä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ä¹Ÿä¸ºæ¨¡å‹çš„è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„åœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸Šè¡¨ç°ä¸ä½³ï¼Œæœ€ä½³æ¨¡å‹o4-miniçš„å‡†ç¡®ç‡ä»…ä¸º52.5%ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ¨ç†å¢å¼ºç­–ç•¥ï¼ˆå¦‚Chain-of-Thoughtå’Œæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ï¼‰å¯¹æ€§èƒ½æå‡çš„è´¡çŒ®æœ‰é™ï¼Œè¡¨æ˜å¤šæ¨¡æ€æ¨ç†çš„å¤æ‚æ€§è¶…å‡ºäº†æ–‡æœ¬æ¨ç†çš„èŒƒç•´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°è¿›è¡Œå¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœªæ¥å¯ä»¥åœ¨æ›´å¹¿æ³›çš„å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as "question frame") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.

