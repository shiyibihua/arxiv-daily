---
layout: default
title: Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization
---

# Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04039" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04039v2</a>
  <a href="https://arxiv.org/pdf/2506.04039.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04039v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04039v2', 'Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiulong Wu, Zhengliang Shi, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Lingyong Yan, Min Cao, Min Zhang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: This paper is accepted by EMNLP2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®ä½“ä¸­å¿ƒå¤šæ¨¡æ€åå¥½ä¼˜åŒ–ä»¥è§£å†³å¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `åå¥½å¯¹é½` `å¹»è§‰é—®é¢˜` `å®ä½“ä¸­å¿ƒä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜æ—¶ï¼Œå¾€å¾€å¿½è§†äº†å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å¯¹é½ï¼Œå¯¼è‡´æ¨¡å‹äº§ç”Ÿä¸å¯é çš„è¾“å‡ºã€‚
2. æœ¬æ–‡æå‡ºçš„å®ä½“ä¸­å¿ƒå¤šæ¨¡æ€åå¥½ä¼˜åŒ–ï¼ˆEMPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºæ¨¡æ€å¯¹é½æ¥æ”¹å–„æ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Œå‡å°‘å¹»è§‰ç°è±¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEMPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†å¹»è§‰ç‡ï¼Œå°¤å…¶åœ¨Object-HalBenchå’ŒMM-HalBenchä¸Šåˆ†åˆ«å‡å°‘äº†85.9%å’Œ49.8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å¯ä¿¡åº¦å¸¸å¸¸å—åˆ°å¹»è§‰çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æºäºæ¨¡æ€ä¸å¯¹é½å’Œå…¶åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å›ºæœ‰çš„å¹»è§‰ã€‚ç°æœ‰çš„åå¥½å¯¹é½æ–¹æ³•ä¾§é‡äºå°†æ¨¡å‹å“åº”ä¸äººç±»åå¥½å¯¹é½ï¼Œä½†å¿½è§†äº†å›¾åƒ-æ–‡æœ¬æ¨¡æ€çš„å¯¹é½ï¼Œå¯¼è‡´å¯¹LLMsçš„è¿‡åº¦ä¾èµ–å’Œå¹»è§‰ç°è±¡ã€‚æœ¬æ–‡æå‡ºäº†å®ä½“ä¸­å¿ƒå¤šæ¨¡æ€åå¥½ä¼˜åŒ–ï¼ˆEMPOï¼‰ï¼Œåœ¨æ¨¡æ€å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰çš„äººç±»åå¥½å¯¹é½æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœé«˜è´¨é‡å¤šæ¨¡æ€åå¥½æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨å¼€æºæŒ‡ä»¤æ•°æ®é›†è‡ªåŠ¨æ„å»ºäº†æ¶µç›–å›¾åƒã€æŒ‡ä»¤å’Œå“åº”ä¸‰ä¸ªæ–¹é¢çš„é«˜è´¨é‡åå¥½æ•°æ®ã€‚åœ¨ä¸¤ä¸ªäººå·¥åå¥½æ•°æ®é›†å’Œäº”ä¸ªå¤šæ¨¡æ€å¹»è§‰åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEMPOçš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚åœ¨Object-HalBenchä¸Šå‡å°‘å¹»è§‰ç‡85.9%ï¼Œåœ¨MM-HalBenchä¸Šå‡å°‘49.8%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äººç±»åå¥½å¯¹é½ï¼Œå¿½è§†äº†å›¾åƒä¸æ–‡æœ¬æ¨¡æ€çš„å¯¹é½ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºçš„ä¸å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå®ä½“ä¸­å¿ƒå¤šæ¨¡æ€åå¥½ä¼˜åŒ–ï¼ˆEMPOï¼‰ï¼Œé€šè¿‡å¢å¼ºæ¨¡æ€å¯¹é½ï¼Œå‡å°‘å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿‡åº¦ä¾èµ–ï¼Œä»è€Œé™ä½å¹»è§‰ç°è±¡çš„å‘ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå›¾åƒã€æŒ‡ä»¤å’Œå“åº”çš„é«˜è´¨é‡åå¥½æ•°æ®æ„å»ºï¼Œæ¨¡æ€å¯¹é½ä¼˜åŒ–ï¼Œä»¥åŠæœ€ç»ˆçš„æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šEMPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è‡ªåŠ¨æ„å»ºé«˜è´¨é‡çš„å¤šæ¨¡æ€åå¥½æ•°æ®ï¼Œå¢å¼ºäº†æ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¾“å‡ºå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¨¡æ€å¯¹é½ä¸äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å¤šæ¨¡æ€è¾“å…¥çš„å¤„ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEMPOåœ¨Object-HalBenchä¸Šå‡å°‘äº†85.9%çš„å¹»è§‰ç‡ï¼Œåœ¨MM-HalBenchä¸Šå‡å°‘äº†49.8%ã€‚è¿™äº›ç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰çš„åå¥½å¯¹é½æ–¹æ³•ï¼Œè¯æ˜äº†EMPOåœ¨å¤šæ¨¡æ€å¹»è§‰é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆã€è·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦ï¼ŒEMPOèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Visual Language Models (LVLMs) have demonstrated impressive capabilities across multiple tasks. However, their trustworthiness is often challenged by hallucinations, which can be attributed to the modality misalignment and the inherent hallucinations of their underlying Large Language Models (LLMs) backbone. Existing preference alignment methods focus on aligning model responses with human preferences while neglecting image-text modality alignment, resulting in over-reliance on LLMs and hallucinations. In this paper, we propose Entity-centric Multimodal Preference Optimization (EMPO), which achieves enhanced modality alignment compared to existing human preference alignment methods. Besides, to overcome the scarcity of high-quality multimodal preference data, we utilize open-source instruction datasets to automatically construct high-quality preference data across three aspects: image, instruction, and response. Experiments on two human preference datasets and five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO, e.g., reducing hallucination rates by 85.9\% on Object-HalBench and 49.8\% on MM-HalBench.

