---
layout: default
title: Object-level Self-Distillation for Vision Pretraining
---

# Object-level Self-Distillation for Vision Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05409" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05409v1</a>
  <a href="https://arxiv.org/pdf/2506.05409.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05409v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05409v1', 'Object-level Self-Distillation for Vision Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ã‡aÄŸlar HÄ±zlÄ±, Ã‡aÄŸatay YÄ±ldÄ±z, Pekka Marttinen

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹è±¡çº§è‡ªè’¸é¦æ–¹æ³•ä»¥è§£å†³å›¾åƒçº§è‡ªè’¸é¦å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹è±¡çº§è‡ªè’¸é¦` `è§†è§‰é¢„è®­ç»ƒ` `å›¾åƒå¤„ç†` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰é¢„è®­ç»ƒæ–¹æ³•å‡è®¾æ¯å¼ å›¾åƒä»…åŒ…å«ä¸€ä¸ªå¯¹è±¡ï¼Œå¯¼è‡´åœ¨å¤šå¯¹è±¡å›¾åƒå’Œå¤æ‚åœºæ™¯æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºå¯¹è±¡çº§è‡ªè’¸é¦ï¼ˆODISï¼‰æ–¹æ³•ï¼Œé€šè¿‡èšç„¦äºå•ä¸ªå¯¹è±¡è€Œéæ•´å¼ å›¾åƒï¼Œæå‡è‡ªè’¸é¦çš„æœ‰æ•ˆæ€§ã€‚
3. ODISæ–¹æ³•åœ¨ImageNet1kä¸Šä½¿ç”¨ViT-Largeæ¨¡å‹å®ç°äº†82.6%çš„k-NNå‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¡¨ç¤ºèƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰é¢„è®­ç»ƒæ–¹æ³•ä¾èµ–äºä»ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰è¿›è¡Œå›¾åƒçº§è‡ªè’¸é¦ï¼Œéšå«å‡è®¾æ¯å¼ å›¾åƒåªåŒ…å«ä¸€ä¸ªå¯¹è±¡ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾å¹¶ä¸æ€»æ˜¯æˆç«‹ï¼Œè®¸å¤šImageNetå›¾åƒå®é™…ä¸ŠåŒ…å«å¤šä¸ªå¯¹è±¡ã€‚æ­¤å¤–ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­å¿ƒæ•°æ®é›†ä¸Šçš„æ‰©å±•èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯¹è±¡çº§è‡ªè’¸é¦ï¼ˆODISï¼‰æ–¹æ³•ï¼Œå°†è‡ªè’¸é¦çš„ç²’åº¦ä»æ•´å¼ å›¾åƒè½¬ç§»åˆ°å•ä¸ªå¯¹è±¡ã€‚é€šè¿‡å¯¹è±¡æ„ŸçŸ¥è£å‰ªå’Œæ©ç æ³¨æ„åŠ›ï¼ŒODISèƒ½å¤Ÿéš”ç¦»å¯¹è±¡ç‰¹å®šåŒºåŸŸï¼Œå¼•å¯¼å˜æ¢å™¨å…³æ³¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å†…å®¹ï¼Œä»è€Œå°†å™ªå£°è¾ƒå¤§çš„åœºæ™¯çº§ä»»åŠ¡è½¬åŒ–ä¸ºæ›´ç®€å•çš„å¯¹è±¡çº§å­ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè¡¥ä¸çº§åˆ«å‡æå‡äº†è§†è§‰è¡¨ç¤ºèƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰é¢„è®­ç»ƒæ–¹æ³•é€šå¸¸å‡è®¾æ¯å¼ å›¾åƒåªåŒ…å«ä¸€ä¸ªå¯¹è±¡ï¼Œè¿™åœ¨å¤šå¯¹è±¡å›¾åƒä¸­å¹¶ä¸æˆç«‹ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨ç°å’Œæ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å¯¹è±¡çº§è‡ªè’¸é¦ï¼ˆODISï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†è‡ªè’¸é¦çš„å…³æ³¨ç‚¹ä»æ•´å¼ å›¾åƒè½¬ç§»åˆ°å•ä¸ªå¯¹è±¡ï¼Œè§£å†³äº†è¿™ä¸€å‡è®¾å¸¦æ¥çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹è±¡æ„ŸçŸ¥è£å‰ªå’Œæ©ç æ³¨æ„åŠ›ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå¤„ç†å›¾åƒä¸­çš„è¯­ä¹‰ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šODISçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹è±¡æ„ŸçŸ¥è£å‰ªæ¨¡å—å’Œæ©ç æ³¨æ„åŠ›æœºåˆ¶ã€‚é¦–å…ˆï¼Œé€šè¿‡è£å‰ªæŠ€æœ¯æå–å¯¹è±¡ç‰¹å®šåŒºåŸŸï¼Œç„¶ååˆ©ç”¨æ©ç æ³¨æ„åŠ›å¼•å¯¼å˜æ¢å™¨å…³æ³¨è¿™äº›åŒºåŸŸï¼Œæœ€ç»ˆå°†å¤æ‚çš„åœºæ™¯çº§ä»»åŠ¡åˆ†è§£ä¸ºç®€å•çš„å¯¹è±¡çº§å­ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šODISçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†è‡ªè’¸é¦çš„ç²’åº¦ä»å›¾åƒçº§åˆ«è½¬å‘å¯¹è±¡çº§åˆ«ï¼Œè¿™ä¸€è½¬å˜ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œæå–å¯¹è±¡ç‰¹å¾ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒODISä½¿ç”¨äº†å¯¹è±¡æ„ŸçŸ¥è£å‰ªæŠ€æœ¯æ¥ç²¾ç¡®å®šä½å¯¹è±¡åŒºåŸŸï¼Œå¹¶é€šè¿‡æ©ç æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºæ¨¡å‹å¯¹è¿™äº›åŒºåŸŸçš„å…³æ³¨ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¯¹è±¡çº§åˆ«çš„ç‰¹å¾æå–ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒODISæ–¹æ³•åœ¨ImageNet1kæ•°æ®é›†ä¸Šä½¿ç”¨ViT-Largeæ¨¡å‹è¾¾åˆ°äº†82.6%çš„k-NNå‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå›¾åƒçº§è‡ªè’¸é¦æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¡¨ç¤ºèƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚åœºæ™¯å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„ç‰©ä½“è¯†åˆ«ã€åœºæ™¯ç†è§£å’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ï¼ŒODISæ–¹æ³•èƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¼ºçš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State-of-the-art vision pretraining methods rely on image-level self-distillation from object-centric datasets such as ImageNet, implicitly assuming each image contains a single object. This assumption does not always hold: many ImageNet images already contain multiple objects. Further, it limits scalability to scene-centric datasets that better mirror real-world complexity. We address these challenges by introducing Object-level Self-DIStillation (ODIS), a pretraining approach that shifts the self-distillation granularity from whole images to individual objects. Using object-aware cropping and masked attention, ODIS isolates object-specific regions, guiding the transformer toward semantically meaningful content and transforming a noisy, scene-level task into simpler object-level sub-tasks. We show that this approach improves visual representations both at the image and patch levels. Using masks at inference time, our method achieves an impressive $82.6\%$ $k$-NN accuracy on ImageNet1k with ViT-Large.

