---
layout: default
title: Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning
---

# Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04034" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04034v1</a>
  <a href="https://arxiv.org/pdf/2506.04034.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04034v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04034v1', 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Lei Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04

**å¤‡æ³¨**: homepage: https://rexthinker.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRex-Thinkerä»¥è§£å†³å¯¹è±¡æŒ‡ç§°çš„å¯è§£é‡Šæ€§ä¸å¯é æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹è±¡æŒ‡ç§°` `é“¾å¼æ¨ç†` `å¯è§£é‡Šæ€§` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹è±¡æŒ‡ç§°æ–¹æ³•é€šå¸¸å°†å…¶è§†ä¸ºç›´æ¥çš„è¾¹ç•Œæ¡†é¢„æµ‹ä»»åŠ¡ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§ä¸è¶³ï¼Œéš¾ä»¥æ‹’ç»æ— åŒ¹é…å¯¹è±¡çš„è¡¨è¾¾ã€‚
2. Rex-Thinkeræ¨¡å‹å°†å¯¹è±¡æŒ‡ç§°è§†ä¸ºé“¾å¼æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡é€æ­¥æ¨ç†æ¥è¯„ä¼°å€™é€‰å¯¹è±¡ä¸ç»™å®šè¡¨è¾¾çš„åŒ¹é…ç¨‹åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRex-Thinkeråœ¨ç²¾åº¦å’Œå¯è§£é‡Šæ€§ä¸Šè¶…è¶Šäº†æ ‡å‡†åŸºçº¿ï¼Œå¹¶åœ¨æ‹’ç»è™šå‡è¾“å‡ºå’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹è±¡æŒ‡ç§°æ—¨åœ¨æ£€æµ‹ä¸ç»™å®šè‡ªç„¶è¯­è¨€æè¿°åŒ¹é…çš„å›¾åƒä¸­çš„æ‰€æœ‰å¯¹è±¡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸€ä¸ªç¨³å¥çš„å¯¹è±¡æŒ‡ç§°æ¨¡å‹åº”å…·å¤‡æ‰æ ¹æ€§ï¼Œå³å…¶é¢„æµ‹åº”å¯è§£é‡Šä¸”å¿ å®äºè§†è§‰å†…å®¹ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåº”æ»¡è¶³ä¸¤ä¸ªå…³é”®å±æ€§ï¼š1) å¯éªŒè¯æ€§ï¼Œé€šè¿‡ç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†æ¥è¯æ˜å…¶é¢„æµ‹ï¼Œå¹¶æ¸…æ™°åœ°å°†å…¶ä¸è§†è§‰è¯æ®å…³è”ï¼›2) å¯ä¿¡æ€§ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒä¸­æ²¡æœ‰æ»¡è¶³ç»™å®šè¡¨è¾¾çš„å¯¹è±¡æ—¶é€‰æ‹©ä¸ä½œé¢„æµ‹ã€‚å¤§å¤šæ•°æ–¹æ³•å°†æŒ‡ç§°è§†ä¸ºç›´æ¥çš„è¾¹ç•Œæ¡†é¢„æµ‹ä»»åŠ¡ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§æœ‰é™ï¼Œå¹¶éš¾ä»¥æ‹’ç»æ²¡æœ‰åŒ¹é…å¯¹è±¡çš„è¡¨è¾¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Rex-Thinkeræ¨¡å‹ï¼Œå°†å¯¹è±¡æŒ‡ç§°æ˜ç¡®åœ°æ„å»ºä¸ºé“¾å¼æ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºHumanRef-CoTçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥æ”¯æŒè¿™ä¸€èŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç²¾åº¦å’Œå¯è§£é‡Šæ€§ä¸Šå‡ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¯¹è±¡æŒ‡ç§°ä»»åŠ¡ä¸­çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å°†å…¶ç®€åŒ–ä¸ºè¾¹ç•Œæ¡†é¢„æµ‹ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æä¾›å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰åŒ¹é…å¯¹è±¡æ—¶æ— æ³•æœ‰æ•ˆæ‹’ç»ä¸åˆé€‚çš„è¡¨è¾¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRex-Thinkeré€šè¿‡å°†å¯¹è±¡æŒ‡ç§°ä»»åŠ¡è½¬åŒ–ä¸ºé“¾å¼æ¨ç†ä»»åŠ¡ï¼Œé€æ­¥è¯„ä¼°å€™é€‰å¯¹è±¡ä¸ç»™å®šè¡¨è¾¾çš„åŒ¹é…ç¨‹åº¦ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œä¿¡ä»»åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRex-Thinkerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡è¯†åˆ«ä¸æ‰€æŒ‡å¯¹è±¡ç±»åˆ«å¯¹åº”çš„å€™é€‰å¯¹è±¡å®ä¾‹ï¼›å…¶æ¬¡ï¼Œé’ˆå¯¹æ¯ä¸ªå€™é€‰å¯¹è±¡è¿›è¡Œé€æ­¥æ¨ç†ï¼Œæœ€ç»ˆåšå‡ºé¢„æµ‹ã€‚æ¨¡å‹çš„è®­ç»ƒåˆ†ä¸ºå†·å¯åŠ¨çš„ç›‘ç£å¾®è°ƒé˜¶æ®µå’ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä»¥æå‡å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRex-Thinkerçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¯¹è±¡æŒ‡ç§°æ˜ç¡®æ„å»ºä¸ºé“¾å¼æ¨ç†ä»»åŠ¡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›´æ¥è¾¹ç•Œæ¡†é¢„æµ‹æ–¹æ³•æœ¬è´¨ä¸Šæœ‰æ‰€ä¸åŒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œå¹¶æœ‰æ•ˆæ‹’ç»ä¸åˆé€‚çš„è¡¨è¾¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç»“æ„åŒ–çš„æ¨ç†æ ¼å¼ï¼ŒåŒ…æ‹¬è§„åˆ’ã€è¡ŒåŠ¨å’Œæ€»ç»“ä¸‰ä¸ªæ­¥éª¤ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒæ¨¡å‹çš„é€æ­¥æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRex-Thinkeråœ¨ç²¾åº¦å’Œå¯è§£é‡Šæ€§ä¸Šå‡è¶…è¿‡äº†æ ‡å‡†åŸºçº¿ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ç²¾åº¦ä¸Šæå‡äº†X%ï¼Œåœ¨å¯è§£é‡Šæ€§è¯„åˆ†ä¸Šæé«˜äº†Y%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ‹’ç»è™šå‡è¾“å‡ºæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨è·¨åŸŸè®¾ç½®ä¸­å±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Rex-Thinkeråœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¯¹è±¡æŒ‡ç§°ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å¯è§£é‡Šæ€§å’Œå¯é æ€§çš„åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§å’Œäººæœºäº¤äº’ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦è§£é‡Šæ€§æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.

