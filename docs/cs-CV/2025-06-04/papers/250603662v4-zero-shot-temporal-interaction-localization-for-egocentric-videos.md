---
layout: default
title: Zero-Shot Temporal Interaction Localization for Egocentric Videos
---

# Zero-Shot Temporal Interaction Localization for Egocentric Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03662" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.03662v4</a>
  <a href="https://arxiv.org/pdf/2506.03662.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03662v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03662v4', 'Zero-Shot Temporal Interaction Localization for Egocentric Videos')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Erhang Zhang, Junyi Ma, Yin-Dong Zheng, Yixuan Zhou, Hesheng Wang

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-04 (Êõ¥Êñ∞: 2025-11-14)

**Â§áÊ≥®**: Accepted to IROS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/IRMVLab/EgoLoc)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EgoLoc‰ª•Ëß£ÂÜ≥Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ë‰∏≠ÁöÑÊó∂Â∫è‰∫§‰∫íÂÆö‰ΩçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)** **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êó∂Â∫è‰∫§‰∫íÂÆö‰Ωç` `‰∫∫-Áâ©‰∫§‰∫í` `Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ë` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Ëá™ÈÄÇÂ∫îÈááÊ†∑` `Èó≠ÁéØÂèçÈ¶à` `Ë°å‰∏∫ÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊó∂Â∫èÂä®‰ΩúÂÆö‰ΩçÊñπÊ≥ï‰æùËµñ‰∫éÊ†áÊ≥®ÁöÑÂä®‰ΩúÂíåÂØπË±°Á±ªÂà´ÔºåÂØºËá¥È¢ÜÂüüÂÅèÂ∑ÆÂíå‰ΩéÊïàÁöÑÈÉ®ÁΩ≤„ÄÇ
2. Êú¨ÊñáÊèêÂá∫EgoLocÔºåÈÄöËøáËá™ÈÄÇÂ∫îÈááÊ†∑Á≠ñÁï•ÁîüÊàêÂêàÁêÜÁöÑËßÜËßâÊèêÁ§∫ÔºåÁõ¥Êé•ÂÆö‰Ωç‰∫∫-Áâ©‰∫§‰∫íÁöÑÊäìÂèñÂä®‰ΩúÊó∂Êú∫„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEgoLocÂú®Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁöÑÊó∂Â∫è‰∫§‰∫íÂÆö‰Ωç‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÆö‰ΩçËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫-Áâ©‰∫§‰∫íÔºàHOIÔºâÂä®‰ΩúÊòØ‰∫∫Á±ªË°å‰∏∫ÂàÜÊûêÂíå‰∫∫Êú∫ÊäÄËÉΩËΩ¨ÁßªÁ≠âÂ§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°ÁöÑÂü∫Á°Ä„ÄÇÁé∞ÊúâÁöÑÊó∂Â∫èÂä®‰ΩúÂÆö‰ΩçÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊ†áÊ≥®ÁöÑÂä®‰ΩúÂíåÂØπË±°Á±ªÂà´ÔºåÂØºËá¥È¢ÜÂüüÂÅèÂ∑ÆÂíå‰ΩéÊïàÁöÑÈÉ®ÁΩ≤„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õÊúÄÊñ∞Á†îÁ©∂Âà©Áî®Â§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂÆûÁé∞‰∫ÜÈõ∂-shotÊó∂Â∫èÂä®‰ΩúÂÆö‰ΩçÔºàZS-TALÔºâÔºå‰ΩÜÂÖ∂Á≤óÁ≥ôÁöÑ‰º∞ËÆ°ÂíåÂºÄÊîæÂºèÊµÅÁ®ãÈôêÂà∂‰∫ÜÊó∂Â∫è‰∫§‰∫íÂÆö‰ΩçÔºàTILÔºâÁöÑËøõ‰∏ÄÊ≠•ÊÄßËÉΩÊèêÂçá„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÈõ∂-shot TILÊñπÊ≥ïEgoLocÔºåÊó®Âú®ÂÆö‰ΩçËá™Êàë‰∏≠ÂøÉËßÜÈ¢ë‰∏≠ÁöÑÊäìÂèñÂä®‰ΩúÊó∂Êú∫„ÄÇEgoLocÂºïÂÖ•Ëá™ÈÄÇÂ∫îÈááÊ†∑Á≠ñÁï•Ôºå‰∏∫VLMÊé®ÁêÜÁîüÊàêÂêàÁêÜÁöÑËßÜËßâÊèêÁ§∫ÔºåÂπ∂ÈÄöËøáÂê∏Êî∂2DÂíå3DËßÇÊµãÔºåÁõ¥Êé•Ê†πÊçÆ3DÊâãÈÉ®ÈÄüÂ∫¶Âú®ÂèØËÉΩÁöÑÊé•Ëß¶/ÂàÜÁ¶ªÊó∂Èó¥Êà≥Âë®Âõ¥ÈááÊ†∑È´òË¥®ÈáèÂàùÂßãÁåúÊµãÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÊ≠§Â§ñÔºåEgoLocËøòÈÄöËøáËßÜËßâÂíåÂä®ÊÄÅÁ∫øÁ¥¢ÁîüÊàêÈó≠ÁéØÂèçÈ¶àÔºå‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÂÆö‰ΩçÁªìÊûú„ÄÇÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåEgoLocÂú®ÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÂíåÊñ∞ÊèêÂá∫ÁöÑÂü∫ÂáÜ‰∏äÔºåÁõ∏ËæÉ‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜËá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁöÑÊó∂Â∫è‰∫§‰∫íÂÆö‰ΩçÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ë‰∏≠‰∫∫-Áâ©‰∫§‰∫íÁöÑÊó∂Â∫èÂÆö‰ΩçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÊ†áÊ≥®Êï∞ÊçÆÔºåÂØºËá¥È¢ÜÂüüÂÅèÂ∑ÆÂíå‰ΩéÊïàÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEgoLocÈÄöËøáËá™ÈÄÇÂ∫îÈááÊ†∑Á≠ñÁï•ÁîüÊàêÂêàÁêÜÁöÑËßÜËßâÊèêÁ§∫ÔºåÁªìÂêà2DÂíå3DËßÇÊµãÔºåÁõ¥Êé•ÂÆö‰ΩçÊäìÂèñÂä®‰ΩúÁöÑÊó∂Èó¥Êà≥Ôºå‰ªéËÄåÊèêÈ´òÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEgoLocÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Ëá™ÈÄÇÂ∫îÈááÊ†∑Ê®°Âùó„ÄÅËßÜËßâÊèêÁ§∫ÁîüÊàêÊ®°ÂùóÂíåÈó≠ÁéØÂèçÈ¶àÊ®°Âùó„ÄÇËá™ÈÄÇÂ∫îÈááÊ†∑Ê®°ÂùóÊ†πÊçÆ3DÊâãÈÉ®ÈÄüÂ∫¶ÁîüÊàêÂàùÂßãÊó∂Èó¥Êà≥ÁåúÊµãÔºåËßÜËßâÊèêÁ§∫ÁîüÊàêÊ®°Âùó‰∏∫VLMÊèê‰æõËæìÂÖ•ÔºåÈó≠ÁéØÂèçÈ¶àÊ®°ÂùóÂàôÊ†πÊçÆÂä®ÊÄÅÁ∫øÁ¥¢‰ºòÂåñÂÆö‰ΩçÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEgoLocÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Ëá™ÈÄÇÂ∫îÈááÊ†∑Á≠ñÁï•ÂíåÈó≠ÁéØÂèçÈ¶àÊú∫Âà∂ÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÂºÄÊîæÂºèÊµÅÁ®ãÂΩ¢ÊàêÈ≤úÊòéÂØπÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÊó∂Â∫è‰∫§‰∫íÂÆö‰ΩçÁöÑÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåEgoLocÈÄöËøá‰ºòÂåñÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÔºåÁ°Æ‰øùÈ´òË¥®ÈáèÁöÑÂàùÂßãÁåúÊµãÔºåÂπ∂ÈÄöËøáÂä®ÊÄÅÁ∫øÁ¥¢ÁöÑÂèçÈ¶àÊú∫Âà∂Ëøõ‰∏ÄÊ≠•ÊèêÂçáÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Âú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EgoLocÂú®ÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÂíåÊñ∞ÊèêÂá∫ÁöÑÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÊó∂Â∫è‰∫§‰∫íÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÊèêÂçá‰∫ÜXX%ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÂàÜÊûê‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∫∫Á±ªË°å‰∏∫ÂàÜÊûê„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËôöÊãüÁé∞ÂÆûÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊó∂Â∫è‰∫§‰∫íÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÔºåEgoLocËÉΩÂ§ü‰∏∫Êú∫Âô®‰∫∫ÊäÄËÉΩËΩ¨ÁßªÂíåËá™Âä®ÂåñÁ≥ªÁªüÊèê‰æõÊõ¥ÂèØÈù†ÁöÑÊîØÊåÅÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Locating human-object interaction (HOI) actions within video serves as the foundation for multiple downstream tasks, such as human behavior analysis and human-robot skill transfer. Current temporal action localization methods typically rely on annotated action and object categories of interactions for optimization, which leads to domain bias and low deployment efficiency. Although some recent works have achieved zero-shot temporal action localization (ZS-TAL) with large vision-language models (VLMs), their coarse-grained estimations and open-loop pipelines hinder further performance improvements for temporal interaction localization (TIL). To address these issues, we propose a novel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp actions for human-object interaction in egocentric videos. EgoLoc introduces a self-adaptive sampling strategy to generate reasonable visual prompts for VLM reasoning. By absorbing both 2D and 3D observations, it directly samples high-quality initial guesses around the possible contact/separation timestamps of HOI according to 3D hand velocities, leading to high inference accuracy and efficiency. In addition, EgoLoc generates closed-loop feedback from visual and dynamic cues to further refine the localization results. Comprehensive experiments on the publicly available dataset and our newly proposed benchmark demonstrate that EgoLoc achieves better temporal interaction localization for egocentric videos compared to state-of-the-art baselines. We have released our code and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.

