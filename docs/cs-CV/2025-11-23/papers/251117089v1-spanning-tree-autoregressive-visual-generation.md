---
layout: default
title: Spanning Tree Autoregressive Visual Generation
---

# Spanning Tree Autoregressive Visual Generation

**arXiv**: [2511.17089v1](https://arxiv.org/abs/2511.17089) | [PDF](https://arxiv.org/pdf/2511.17089.pdf)

**ä½œè€…**: Sangkyu Lee, Changho Lee, Janghoon Han, Hosung Song, Tackgeun You, Hwasup Lim, Stanley Jungkyu Choi, Honglak Lee, Youngjae Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpanning Tree Autoregressiveå»ºæ¨¡ï¼Œä»¥åœ¨è§†è§‰ç”Ÿæˆä¸­ç»“åˆå…ˆéªŒçŸ¥è¯†ï¼Œä¿æŒé‡‡æ ·æ€§èƒ½å¹¶æ”¯æŒçµæ´»åºåˆ—ç¼–è¾‘ã€‚**

**å…³é”®è¯**: `è§†è§‰ç”Ÿæˆ` `è‡ªå›žå½’å»ºæ¨¡` `åºåˆ—é¡ºåºä¼˜åŒ–` `å›¾åƒç¼–è¾‘` `ç”Ÿæˆæ ‘éåŽ†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿè‡ªå›žå½’æ¨¡åž‹åœ¨è§†è§‰ç”Ÿæˆä¸­ï¼Œéšæœºåºåˆ—é¡ºåºå¯¼è‡´æ€§èƒ½ä¸‹é™æˆ–æŽ¨ç†çµæ´»æ€§å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å‡åŒ€ç”Ÿæˆæ ‘çš„éåŽ†é¡ºåºï¼Œé€šè¿‡å¹¿åº¦ä¼˜å…ˆæœç´¢å’Œæ‹’ç»é‡‡æ ·æž„å»ºåºåˆ—ï¼Œç¡®ä¿éƒ¨åˆ†è§‚å¯Ÿä½œä¸ºå‰ç¼€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¿æŒé‡‡æ ·æ€§èƒ½çš„åŒæ—¶ï¼Œæä¾›çµæ´»åºåˆ—é¡ºåºï¼Œæ— éœ€æ˜¾è‘—æ”¹å˜æ¨¡åž‹æž¶æž„ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Spanning Tree Autoregressive (STAR) modeling, which can incorporate prior knowledge of images, such as center bias and locality, to maintain sampling performance while also providing sufficiently flexible sequence orders to accommodate image editing at inference. Approaches that expose randomly permuted sequence orders to conventional autoregressive (AR) models in visual generation for bidirectional context either suffer from a decline in performance or compromise the flexibility in sequence order choice at inference. Instead, STAR utilizes traversal orders of uniform spanning trees sampled in a lattice defined by the positions of image patches. Traversal orders are obtained through breadth-first search, allowing us to efficiently construct a spanning tree whose traversal order ensures that the connected partial observation of the image appears as a prefix in the sequence through rejection sampling. Through the tailored yet structured randomized strategy compared to random permutation, STAR preserves the capability of postfix completion while maintaining sampling performance without any significant changes to the model architecture widely adopted in the language AR modeling.

