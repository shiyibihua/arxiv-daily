---
layout: default
title: A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs
---

# A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs

**arXiv**: [2511.17135v1](https://arxiv.org/abs/2511.17135) | [PDF](https://arxiv.org/pdf/2511.17135.pdf)

**ä½œè€…**: Jiaxun Fang, Li Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šé˜¶æ®µä¼˜åŒ–æ¡†æž¶ä»¥åœ¨FPGAä¸Šé«˜æ•ˆéƒ¨ç½²å­¦ä¹ åž‹å›¾åƒåŽ‹ç¼©æ¨¡åž‹**

**å…³é”®è¯**: `å­¦ä¹ åž‹å›¾åƒåŽ‹ç¼©` `FPGAéƒ¨ç½²` `é‡åŒ–ä¼˜åŒ–` `æ··åˆç²¾åº¦æœç´¢` `é€šé“å‰ªæž` `ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå­¦ä¹ åž‹å›¾åƒåŽ‹ç¼©æ¨¡åž‹åœ¨FPGAéƒ¨ç½²æ—¶é¢ä¸´é‡åŒ–æ€§èƒ½ä¸‹é™å’Œèµ„æºé™åˆ¶æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŠ¨æ€èŒƒå›´æ„ŸçŸ¥é‡åŒ–ã€æ··åˆç²¾åº¦æœç´¢å’Œé€šé“å‰ªæžä¼˜åŒ–ç¡¬ä»¶å®žçŽ°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé‡åŒ–æ–¹æ³•å°†BD-rateå¼€é”€ä»Ž30%é™è‡³6.3%ï¼Œä¼˜åŒ–åŽè®¡ç®—å¤æ‚åº¦é™ä½Žè¶…20%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep learning-based image compression (LIC) has achieved state-of-the-art rate-distortion (RD) performance, yet deploying these models on resource-constrained FPGAs remains a major challenge. This work presents a complete, multi-stage optimization framework to bridge the gap between high-performance floating-point models and efficient, hardware-friendly integer-based implementations. First, we address the fundamental problem of quantization-induced performance degradation. We propose a Dynamic Range-Aware Quantization (DRAQ) method that uses statistically-calibrated activation clipping and a novel weight regularization scheme to counteract the effects of extreme data outliers and large dynamic ranges, successfully creating a high-fidelity 8-bit integer model. Second, building on this robust foundation, we introduce two hardware-aware optimization techniques tailored for FPGAs. A progressive mixed-precision search algorithm exploits FPGA flexibility to assign optimal, non-uniform bit-widths to each layer, minimizing complexity while preserving performance. Concurrently, a channel pruning method, adapted to work with the Generalized Divisive Normalization (GDN) layers common in LIC, removes model redundancy by eliminating inactive channels. Our comprehensive experiments show that the foundational DRAQ method reduces the BD-rate overhead of a GDN-based model from $30\%$ to $6.3\%$. The subsequent hardware-aware optimizations further reduce computational complexity by over $20\%$ with negligible impact on RD performance, yielding a final model that is both state-of-the-art in efficiency and superior in quality to existing FPGA-based LIC implementations.

