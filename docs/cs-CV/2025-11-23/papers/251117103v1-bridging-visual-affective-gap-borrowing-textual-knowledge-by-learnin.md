---
layout: default
title: Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs
---

# Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs

**arXiv**: [2511.17103v1](https://arxiv.org/abs/2511.17103) | [PDF](https://arxiv.org/pdf/2511.17103.pdf)

**ä½œè€…**: Daiqing Wu, Dongbao Yang, Yu Zhou, Can Ma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†åŒºè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³è§†è§‰æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æƒ…æ„Ÿé¸¿æ²Ÿé—®é¢˜**

**å…³é”®è¯**: `è§†è§‰æƒ…æ„Ÿè¯†åˆ«` `æƒ…æ„Ÿé¸¿æ²Ÿ` `å¯¹æ¯”å­¦ä¹ ` `å›¾åƒ-æ–‡æœ¬å¯¹` `é¢„è®­ç»ƒæ¨¡åž‹` `å™ªå£°æ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰æƒ…æ„Ÿè¯†åˆ«ä¸­é¢„è®­ç»ƒæ¨¡åž‹å­˜åœ¨æƒ…æ„Ÿé¸¿æ²Ÿï¼Œå³äº‹å®žç‰¹å¾ä¸Žæƒ…æ„Ÿç±»åˆ«ç¼ºä¹ç›´æŽ¥å…³è”
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å™ªå£°å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œé€šè¿‡åˆ†åŒºè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ åŠ¨æ€æž„å»ºæ­£è´Ÿæ ·æœ¬å¯¹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æƒ…æ„Ÿç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æå‡å¤šç§é¢„è®­ç»ƒè§†è§‰æ¨¡åž‹çš„æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the "affective gap", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the "affective gap". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the "affective gap" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.

