---
layout: default
title: SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding
---

# SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding

**arXiv**: [2511.17411v1](https://arxiv.org/abs/2511.17411) | [PDF](https://arxiv.org/pdf/2511.17411.pdf)

**ä½œè€…**: Nikolay Nikolov, Giuliano Albanese, Sombit Dey, Aleksandar Yanev, Luc Van Gool, Jan-Nico Zaech, Danda Pani Paudel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPEAR-1æœºå™¨äººåŸºç¡€æ¨¡åž‹ï¼Œé€šè¿‡å¢žå¼º3Dæ„ŸçŸ¥è§£å†³æœºå™¨äººæŽ§åˆ¶æ³›åŒ–é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººåŸºç¡€æ¨¡åž‹` `3Dç©ºé—´æŽ¨ç†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å•å›¾åƒ3Dåæ ‡æŽ¨æ–­` `è¯­è¨€æŒ‡ä»¤æŽ§åˆ¶` `æ•°æ®é«˜æ•ˆè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººåŸºç¡€æ¨¡åž‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œå› ä¾èµ–ç¼ºä¹3Dç©ºé—´æŽ¨ç†çš„2Dè§†è§‰è¯­è¨€æ¨¡åž‹
2. æ–¹æ³•è¦ç‚¹ï¼šè®­ç»ƒSPEAR-VLMä»Žå•å¼ 2Då›¾åƒæŽ¨æ–­3Dåæ ‡ï¼Œå¹¶é›†æˆåˆ°è¯­è¨€æŒ‡ä»¤æŽ§åˆ¶ä¸­
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨24ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ€§èƒ½ä¼˜äºŽæˆ–åŒ¹é…å…ˆè¿›æ¨¡åž‹ï¼Œä¸”æœºå™¨äººæ¼”ç¤ºæ•°æ®å‡å°‘20å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $Ï€_0$-FAST and $Ï€_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.

