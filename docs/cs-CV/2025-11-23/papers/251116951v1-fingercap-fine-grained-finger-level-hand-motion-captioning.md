---
layout: default
title: FingerCap: Fine-grained Finger-level Hand Motion Captioning
---

# FingerCap: Fine-grained Finger-level Hand Motion Captioning

**arXiv**: [2511.16951v1](https://arxiv.org/abs/2511.16951) | [PDF](https://arxiv.org/pdf/2511.16951.pdf)

**ä½œè€…**: Xin Shen, Rui Zhu, Lei Shen, Xinyu Wang, Kaihao Zhang, Tianqing Zhu, Shuchen Wu, Chenxi Miao, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang, Xin Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFiGOPæ–¹æ³•ä»¥è§£å†³è§†é¢‘å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨ç»†ç²’åº¦æ‰‹æŒ‡è¿åŠ¨ç†è§£ä¸­çš„æ—¶é—´ç¨€ç–æ€§é—®é¢˜**

**å…³é”®è¯**: `æ‰‹æŒ‡è¿åŠ¨æè¿°` `è§†é¢‘å¤šæ¨¡æ€å¤§æ¨¡åž‹` `æ—¶é—´ç¨€ç–æ€§` `æ‰‹éƒ¨å…³é”®ç‚¹` `è¿åŠ¨åµŒå…¥` `ç»†ç²’åº¦ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘å¤šæ¨¡æ€å¤§æ¨¡åž‹å› RGBé‡‡æ ·ç¨€ç–ï¼Œéš¾ä»¥æ•æ‰æ‰‹æŒ‡ç»†å¾®é«˜é¢‘åŠ¨æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥FiGOPï¼Œå°†RGBå…³é”®å¸§ä¸ŽåŽç»­æ‰‹éƒ¨å…³é”®ç‚¹é…å¯¹ï¼Œç¼–ç è¿åŠ¨åµŒå…¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨FingerCap-40Kæ•°æ®é›†ä¸Šï¼ŒFiGOPå¢žå¼ºæ¨¡åž‹åœ¨HandJudgeè¯„ä¼°ä¸­è¡¨çŽ°æ›´ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.

