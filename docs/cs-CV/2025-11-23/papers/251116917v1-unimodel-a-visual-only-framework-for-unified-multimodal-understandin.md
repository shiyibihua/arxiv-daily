---
layout: default
title: UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation
---

# UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation

**arXiv**: [2511.16917v1](https://arxiv.org/abs/2511.16917) | [PDF](https://arxiv.org/pdf/2511.16917.pdf)

**ä½œè€…**: Chi Zhang, Jiepeng Wang, Youming Wang, Yuanzhi Liang, Xiaoyan Yang, Zuoxin Li, Haibin Huang, Xuelong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniModelä»¥ç»Ÿä¸€è§†è§‰ç†è§£ä¸Žç”Ÿæˆï¼Œé€šè¿‡åƒç´ çº§æ‰©æ•£æ¡†æž¶å®žçŽ°å¤šæ¨¡æ€å­¦ä¹ ã€‚**

**å…³é”®è¯**: `ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹` `åƒç´ çº§æ‰©æ•£` `è§†è§‰ç©ºé—´æ˜ å°„` `ç”Ÿæˆä¸Žç†è§£` `æ‰©æ•£å˜æ¢å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å­¦ä¹ ä¸­æ¨¡æ€å·®å¼‚é˜»ç¢ç»Ÿä¸€æ¨¡åž‹å¼€å‘ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ–‡æœ¬å’Œå›¾åƒæ˜ å°„åˆ°å…±äº«è§†è§‰ç©ºé—´ï¼Œä½¿ç”¨åƒç´ åˆ°åƒç´ å˜æ¢ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°æ–‡æœ¬ä»»åŠ¡ä¸­å±•ç¤ºå¼ºå¯¹é½å’Œå¯æŽ§æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.

