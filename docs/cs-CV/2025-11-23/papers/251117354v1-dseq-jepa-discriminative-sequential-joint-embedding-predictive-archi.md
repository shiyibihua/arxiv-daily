---
layout: default
title: DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture
---

# DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture

**arXiv**: [2511.17354v1](https://arxiv.org/abs/2511.17354) | [PDF](https://arxiv.org/pdf/2511.17354.pdf)

**ä½œè€…**: Xiangteng He, Shunsuke Sakai, Kun Yuan, Nicolas Padoy, Tatsuhito Hasegawa, Leonid Sigal

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDSeq-JEPAä»¥æ”¹è¿›å›¾åƒè¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡é¡ºåºé¢„æµ‹å¢žå¼ºåˆ¤åˆ«æ€§ã€‚**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å›¾åƒè¡¨ç¤ºå­¦ä¹ ` `é¡ºåºé¢„æµ‹` `åˆ¤åˆ«æ€§åŒºåŸŸ` `è”åˆåµŒå…¥é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. I-JEPAåœ¨é¢„æµ‹æŽ©ç åŒºåŸŸæ—¶ç¼ºä¹é¡ºåºå’Œåˆ¤åˆ«æ€§ï¼Œå¯¼è‡´è¡¨ç¤ºå­¦ä¹ ä¸å……åˆ†ã€‚
2. DSeq-JEPAç»“åˆJEPAæ½œåœ¨é¢„æµ‹ä¸ŽGPTé¡ºåºæŽ¨ç†ï¼ŒæŒ‰æ˜¾è‘—æ€§é¡ºåºé¢„æµ‹åŒºåŸŸã€‚
3. å®žéªŒæ˜¾ç¤ºåœ¨åˆ†ç±»ã€æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ï¼ŒDSeq-JEPAæ¯”I-JEPAå˜ä½“æ›´æœ‰æ•ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning. Specifically, DSeq-JEPA (i) first identifies primary discriminative regions based on a transformer-derived saliency map, emphasizing the distribution of visual importance, and then (ii) predicts subsequent regions in this discriminative order, progressively forming a curriculum-like semantic progression from primary to secondary cues -- a form of GPT-style pre-training. Extensive experiments across diverse tasks, including image classification (ImageNet), fine-grained visual categorization (iNaturalist21, CUB-200-2011, Stanford-Cars), detection and segmentation (MS-COCO, ADE20K), and low-level reasoning tasks (Clevr/Count, Clevr/Dist), demonstrate that DSeq-JEPA consistently focuses on more discriminative and generalizable representations than I-JEPA variants. Project page: https://github.com/SkyShunsuke/DSeq-JEPA.

