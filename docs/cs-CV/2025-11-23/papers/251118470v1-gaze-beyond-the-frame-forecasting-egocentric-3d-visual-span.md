---
layout: default
title: Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span
---

# Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18470" target="_blank" class="toolbar-btn">arXiv: 2511.18470v1</a>
    <a href="https://arxiv.org/pdf/2511.18470.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18470v1" 
            onclick="toggleFavorite(this, '2511.18470v1', 'Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Heeseung Yun, Joonil Na, Jaeyeon Kim, Calvin Murdock, Gunhee Kim

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-23

**Â§áÊ≥®**: NeurIPS 2025 Spotlight

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**EgoSpanLiftÔºöÈ¢ÑÊµãÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí‰∏ãÁöÑ3DËßÜËßâËåÉÂõ¥ÔºåÊèêÂçáAR/VR‰ΩìÈ™å„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Á¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí` `3DËßÜËßâËåÉÂõ¥È¢ÑÊµã` `SLAM` `Ê∑±Â∫¶Â≠¶‰π†` `Êó∂Á©∫ËûçÂêà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßíÁ†îÁ©∂‰∏ªË¶ÅÂÖ≥Ê≥®ËøêÂä®Âíå‰∫§‰∫íÔºåÂøΩÁï•‰∫ÜËßÜËßâÊÑüÁü•È¢ÑÊµãÂú®ÂºïÂØºË°å‰∏∫‰∏≠ÁöÑ‰ΩúÁî®ÂèäÂÖ∂Âú®AR/VR‰∏≠ÁöÑÊΩúÂäõ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫EgoSpanLiftÔºåÂ∞Ü2DËßÜËßâËåÉÂõ¥È¢ÑÊµãÊèêÂçáÂà∞3DÂú∫ÊôØÔºåÂà©Áî®SLAMÂÖ≥ÈîÆÁÇπÂíå‰ΩìÁßØËßÜËßâÂå∫ÂüüËøõË°åÈ¢ÑÊµã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®3DËßÜËßâËåÉÂõ¥È¢ÑÊµã‰∏ä‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂Âú®2DÊäïÂΩ±‰∏äËææÂà∞ÂèØÊØîÁªìÊûúÔºåÈ™åËØÅ‰∫ÜÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ¢ÑÊµãÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí‰∏ã3DËßÜËßâËåÉÂõ¥ÁöÑÊñπÊ≥ïÔºåÊó®Âú®È¢ÑÊµã‰∏™‰ΩìÂú®‰∏âÁª¥ÁéØÂ¢É‰∏≠Êé•‰∏ãÊù•ËßÜËßâÂÖ≥Ê≥®ÁöÑÁÑ¶ÁÇπ„ÄÇÁé∞ÊúâÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âü∫‰∫éËøêÂä®ÂíåÊé•Ëß¶ÁöÑ‰∫§‰∫íÔºåËÄåÂØπ‰∫∫Á±ªËßÜËßâÊÑüÁü•Êú¨Ë∫´ÁöÑÈ¢ÑÊµãÁ†îÁ©∂ËæÉÂ∞ë„ÄÇ‰∏∫Ê≠§ÔºåËÆ∫ÊñáÊèêÂá∫‰∫ÜEgoSpanLiftÔºå‰∏ÄÁßçÂ∞ÜÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßâËåÉÂõ¥È¢ÑÊµã‰ªé2DÂõæÂÉèÂπ≥Èù¢ËΩ¨Êç¢Âà∞3DÂú∫ÊôØÁöÑÊñ∞ÊñπÊ≥ï„ÄÇEgoSpanLiftÂ∞ÜSLAMÂØºÂá∫ÁöÑÂÖ≥ÈîÆÁÇπËΩ¨Êç¢‰∏∫‰∏éÊ≥®ËßÜÂÖºÂÆπÁöÑÂá†‰Ωï‰ΩìÔºåÂπ∂ÊèêÂèñ‰ΩìÁßØËßÜËßâËåÉÂõ¥Âå∫Âüü„ÄÇÊ≠§Â§ñÔºåEgoSpanLift‰∏é3D U-NetÂíåÂçïÂêëTransformerÁõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫ÜÊó∂Á©∫ËûçÂêàÔºå‰ªéËÄåÊúâÊïàÂú∞È¢ÑÊµã3DÁΩëÊ†º‰∏≠Êú™Êù•ÁöÑËßÜËßâËåÉÂõ¥„ÄÇËÆ∫ÊñáËøòÊï¥ÁêÜ‰∫Ü‰∏Ä‰∏™Êù•Ëá™ÂéüÂßãÁ¨¨‰∏Ä‰∫∫Áß∞Â§ö‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÁªºÂêàÂü∫ÂáÜÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´364.6KÊ†∑Êú¨ÁöÑ3DËßÜËßâËåÉÂõ¥È¢ÑÊµãÊµãËØïÂπ≥Âè∞„ÄÇËØ•ÊñπÊ≥ïÂú®Á¨¨‰∏Ä‰∫∫Áß∞2DÊ≥®ËßÜÈ¢ÑÊµãÂíå3DÂÆö‰ΩçÊñπÈù¢‰ºò‰∫éÁ´û‰∫âÂü∫Á∫øÔºåÂç≥‰ΩøÂú®Ê≤°ÊúâÈ¢ùÂ§ñ2DÁâπÂÆöËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÊäïÂΩ±Âõû2DÂõæÂÉèÂπ≥Èù¢Êó∂Ôºå‰πüËÉΩËé∑ÂæóÁõ∏ÂΩìÁöÑÁªìÊûú„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Á¨¨‰∏Ä‰∫∫Áß∞ËßÜËßí‰∏ã3DËßÜËßâËåÉÂõ¥È¢ÑÊµãÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®2DÂõæÂÉèÂπ≥Èù¢‰∏äÁöÑÊ≥®ËßÜÈ¢ÑÊµãÔºåÁº∫‰πèÂØπ3DÂú∫ÊôØÂá†‰Ωï‰ø°ÊÅØÁöÑÊúâÊïàÂà©Áî®ÔºåÈöæ‰ª•ÂáÜÁ°ÆÈ¢ÑÊµã‰∏™‰ΩìÂú®3DÁ©∫Èó¥‰∏≠ÁöÑËßÜËßâÂÖ≥Ê≥®ÁÇπ„ÄÇÊ≠§Â§ñÔºåÁº∫‰πèÂ§ßËßÑÊ®°ÁöÑ3DËßÜËßâËåÉÂõ¥È¢ÑÊµãÊï∞ÊçÆÈõÜ‰πüÈôêÂà∂‰∫ÜÁõ∏ÂÖ≥Á†îÁ©∂ÁöÑËøõÂ±ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü2DÂõæÂÉèÂπ≥Èù¢‰∏äÁöÑËßÜËßâËåÉÂõ¥È¢ÑÊµãÈóÆÈ¢òËΩ¨Âåñ‰∏∫3DÂú∫ÊôØ‰∏≠ÁöÑ‰ΩìÁßØÈ¢ÑÊµãÈóÆÈ¢ò„ÄÇÈÄöËøáÂà©Áî®SLAMÊäÄÊúØËé∑ÂèñÁöÑ3DÂú∫ÊôØÂá†‰Ωï‰ø°ÊÅØÔºåÂ∞Ü2DÊ≥®ËßÜÈ¢ÑÊµãÁªìÊûúÊèêÂçáÂà∞3DÁ©∫Èó¥Ôºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµã‰∏™‰ΩìÂú®3DÁéØÂ¢É‰∏≠ÁöÑËßÜËßâÂÖ≥Ê≥®Âå∫Âüü„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÂú∫ÊôØÁöÑ3DÁªìÊûÑ‰ø°ÊÅØÔºåÊèêÈ´òÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) Âà©Áî®SLAMÊäÄÊúØÈáçÂª∫3DÂú∫ÊôØÔºåÂπ∂ÊèêÂèñÂÖ≥ÈîÆÁÇπÔºõ2) Â∞ÜSLAMÂÖ≥ÈîÆÁÇπËΩ¨Êç¢‰∏∫‰∏éÊ≥®ËßÜÂÖºÂÆπÁöÑÂá†‰Ωï‰ΩìÔºõ3) ÊèêÂèñ‰ΩìÁßØËßÜËßâËåÉÂõ¥Âå∫ÂüüÔºõ4) Âà©Áî®3D U-NetÂíåÂçïÂêëTransformerËøõË°åÊó∂Á©∫ÁâπÂæÅËûçÂêàÔºåÈ¢ÑÊµãÊú™Êù•ÁöÑ3DËßÜËßâËåÉÂõ¥„ÄÇËØ•Ê°ÜÊû∂Â∞ÜSLAM„ÄÅËÆ°ÁÆóÊú∫ËßÜËßâÂíåÊ∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÁõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫Ü‰ªé2DÂà∞3DÁöÑËßÜËßâËåÉÂõ¥È¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫ÜEgoSpanLiftÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂ∞Ü2DÂõæÂÉèÂπ≥Èù¢‰∏äÁöÑËßÜËßâËåÉÂõ¥È¢ÑÊµãÈóÆÈ¢òËΩ¨Âåñ‰∏∫3DÂú∫ÊôØ‰∏≠ÁöÑ‰ΩìÁßØÈ¢ÑÊµãÈóÆÈ¢ò„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåEgoSpanLiftËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®3DÂú∫ÊôØÁöÑÂá†‰Ωï‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµã‰∏™‰ΩìÂú®3DÁéØÂ¢É‰∏≠ÁöÑËßÜËßâÂÖ≥Ê≥®Âå∫Âüü„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑ3DËßÜËßâËåÉÂõ¥È¢ÑÊµãÊï∞ÊçÆÈõÜÔºå‰∏∫Áõ∏ÂÖ≥Á†îÁ©∂Êèê‰æõ‰∫ÜÊï∞ÊçÆÊîØÊåÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®SLAMÊäÄÊúØËé∑Âèñ3DÂú∫ÊôØÂá†‰Ωï‰ø°ÊÅØÔºõ2) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ∞ÜSLAMÂÖ≥ÈîÆÁÇπËΩ¨Êç¢‰∏∫‰∏éÊ≥®ËßÜÂÖºÂÆπÁöÑÂá†‰Ωï‰ΩìÁöÑÊñπÊ≥ïÔºõ3) Âà©Áî®3D U-NetÊèêÂèñÁ©∫Èó¥ÁâπÂæÅÔºåÂà©Áî®ÂçïÂêëTransformerÊèêÂèñÊó∂Èó¥ÁâπÂæÅÔºõ4) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫é‰ºòÂåñ3DËßÜËßâËåÉÂõ¥È¢ÑÊµãÊ®°Âûã„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEgoSpanLiftÊñπÊ≥ïÂú®Á¨¨‰∏Ä‰∫∫Áß∞2DÊ≥®ËßÜÈ¢ÑÊµãÂíå3DÂÆö‰ΩçÊñπÈù¢‰ºò‰∫éÁ´û‰∫âÂü∫Á∫ø„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®3DËßÜËßâËåÉÂõ¥È¢ÑÊµã‰ªªÂä°‰∏äÔºåEgoSpanLiftÁöÑÊÄßËÉΩÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÂç≥‰ΩøÂú®Ê≤°ÊúâÈ¢ùÂ§ñ2DÁâπÂÆöËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞ÜEgoSpanLiftÁöÑÈ¢ÑÊµãÁªìÊûúÊäïÂΩ±Âõû2DÂõæÂÉèÂπ≥Èù¢Êó∂Ôºå‰πüËÉΩËé∑Âæó‰∏éÁé∞Êúâ2DÊ≥®ËßÜÈ¢ÑÊµãÊñπÊ≥ïÁõ∏ÂΩìÁöÑÁªìÊûúÔºåÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ¢ûÂº∫Áé∞ÂÆûÔºàARÔºâÂíåËôöÊãüÁé∞ÂÆûÔºàVRÔºâÈ¢ÜÂüüÔºå‰æãÂ¶ÇÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑËßÜËßâÂÖ≥Ê≥®ÁÇπÂä®ÊÄÅË∞ÉÊï¥AR/VRÂú∫ÊôØÁöÑÊ∏≤ÊüìË¥®ÈáèÔºåÊèêÈ´òÁî®Êà∑‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Â∫îÁî®‰∫éËæÖÂä©ÊäÄÊúØÈ¢ÜÂüüÔºå‰æãÂ¶ÇÔºåÂèØ‰ª•Â∏ÆÂä©ËßÜÂäõÈöúÁ¢çËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÊèêÈ´òÁîüÊ¥ªË¥®Èáè„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

