---
layout: default
title: Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span
---

# Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span

**arXiv**: [2511.18470v1](https://arxiv.org/abs/2511.18470) | [PDF](https://arxiv.org/pdf/2511.18470.pdf)

**ä½œè€…**: Heeseung Yun, Joonil Na, Jaeyeon Kim, Calvin Murdock, Gunhee Kim

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-23

**å¤‡æ³¨**: NeurIPS 2025 Spotlight

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EgoSpanLiftï¼šé¢„æµ‹ç¬¬ä¸€äººç§°è§†è§’ä¸‹çš„3Dè§†è§‰èŒƒå›´ï¼Œæå‡AR/VRä½“éªŒã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç¬¬ä¸€äººç§°è§†è§’` `3Dè§†è§‰èŒƒå›´é¢„æµ‹` `SLAM` `æ·±åº¦å­¦ä¹ ` `æ—¶ç©ºèžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç¬¬ä¸€äººç§°è§†è§’ç ”ç©¶ä¸»è¦å…³æ³¨è¿åŠ¨å’Œäº¤äº’ï¼Œå¿½ç•¥äº†è§†è§‰æ„ŸçŸ¥é¢„æµ‹åœ¨å¼•å¯¼è¡Œä¸ºä¸­çš„ä½œç”¨åŠå…¶åœ¨AR/VRä¸­çš„æ½œåŠ›ã€‚
2. è®ºæ–‡æå‡ºEgoSpanLiftï¼Œå°†2Dè§†è§‰èŒƒå›´é¢„æµ‹æå‡åˆ°3Dåœºæ™¯ï¼Œåˆ©ç”¨SLAMå…³é”®ç‚¹å’Œä½“ç§¯è§†è§‰åŒºåŸŸè¿›è¡Œé¢„æµ‹ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨3Dè§†è§‰èŒƒå›´é¢„æµ‹ä¸Šä¼˜äºŽçŽ°æœ‰åŸºçº¿ï¼Œå¹¶åœ¨2DæŠ•å½±ä¸Šè¾¾åˆ°å¯æ¯”ç»“æžœï¼ŒéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢„æµ‹ç¬¬ä¸€äººç§°è§†è§’ä¸‹3Dè§†è§‰èŒƒå›´çš„æ–¹æ³•ï¼Œæ—¨åœ¨é¢„æµ‹ä¸ªä½“åœ¨ä¸‰ç»´çŽ¯å¢ƒä¸­æŽ¥ä¸‹æ¥è§†è§‰å…³æ³¨çš„ç„¦ç‚¹ã€‚çŽ°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŸºäºŽè¿åŠ¨å’ŒæŽ¥è§¦çš„äº¤äº’ï¼Œè€Œå¯¹äººç±»è§†è§‰æ„ŸçŸ¥æœ¬èº«çš„é¢„æµ‹ç ”ç©¶è¾ƒå°‘ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†EgoSpanLiftï¼Œä¸€ç§å°†ç¬¬ä¸€äººç§°è§†è§‰èŒƒå›´é¢„æµ‹ä»Ž2Då›¾åƒå¹³é¢è½¬æ¢åˆ°3Dåœºæ™¯çš„æ–°æ–¹æ³•ã€‚EgoSpanLiftå°†SLAMå¯¼å‡ºçš„å…³é”®ç‚¹è½¬æ¢ä¸ºä¸Žæ³¨è§†å…¼å®¹çš„å‡ ä½•ä½“ï¼Œå¹¶æå–ä½“ç§¯è§†è§‰èŒƒå›´åŒºåŸŸã€‚æ­¤å¤–ï¼ŒEgoSpanLiftä¸Ž3D U-Netå’Œå•å‘Transformerç›¸ç»“åˆï¼Œå®žçŽ°äº†æ—¶ç©ºèžåˆï¼Œä»Žè€Œæœ‰æ•ˆåœ°é¢„æµ‹3Dç½‘æ ¼ä¸­æœªæ¥çš„è§†è§‰èŒƒå›´ã€‚è®ºæ–‡è¿˜æ•´ç†äº†ä¸€ä¸ªæ¥è‡ªåŽŸå§‹ç¬¬ä¸€äººç§°å¤šä¼ æ„Ÿå™¨æ•°æ®çš„ç»¼åˆåŸºå‡†ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«364.6Kæ ·æœ¬çš„3Dè§†è§‰èŒƒå›´é¢„æµ‹æµ‹è¯•å¹³å°ã€‚è¯¥æ–¹æ³•åœ¨ç¬¬ä¸€äººç§°2Dæ³¨è§†é¢„æµ‹å’Œ3Då®šä½æ–¹é¢ä¼˜äºŽç«žäº‰åŸºçº¿ï¼Œå³ä½¿åœ¨æ²¡æœ‰é¢å¤–2Dç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹æŠ•å½±å›ž2Då›¾åƒå¹³é¢æ—¶ï¼Œä¹Ÿèƒ½èŽ·å¾—ç›¸å½“çš„ç»“æžœã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¬¬ä¸€äººç§°è§†è§’ä¸‹3Dè§†è§‰èŒƒå›´é¢„æµ‹é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨2Då›¾åƒå¹³é¢ä¸Šçš„æ³¨è§†é¢„æµ‹ï¼Œç¼ºä¹å¯¹3Dåœºæ™¯å‡ ä½•ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œéš¾ä»¥å‡†ç¡®é¢„æµ‹ä¸ªä½“åœ¨3Dç©ºé—´ä¸­çš„è§†è§‰å…³æ³¨ç‚¹ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤§è§„æ¨¡çš„3Dè§†è§‰èŒƒå›´é¢„æµ‹æ•°æ®é›†ä¹Ÿé™åˆ¶äº†ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†2Då›¾åƒå¹³é¢ä¸Šçš„è§†è§‰èŒƒå›´é¢„æµ‹é—®é¢˜è½¬åŒ–ä¸º3Dåœºæ™¯ä¸­çš„ä½“ç§¯é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨SLAMæŠ€æœ¯èŽ·å–çš„3Dåœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œå°†2Dæ³¨è§†é¢„æµ‹ç»“æžœæå‡åˆ°3Dç©ºé—´ï¼Œä»Žè€Œæ›´å‡†ç¡®åœ°é¢„æµ‹ä¸ªä½“åœ¨3DçŽ¯å¢ƒä¸­çš„è§†è§‰å…³æ³¨åŒºåŸŸã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åœºæ™¯çš„3Dç»“æž„ä¿¡æ¯ï¼Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åˆ©ç”¨SLAMæŠ€æœ¯é‡å»º3Dåœºæ™¯ï¼Œå¹¶æå–å…³é”®ç‚¹ï¼›2) å°†SLAMå…³é”®ç‚¹è½¬æ¢ä¸ºä¸Žæ³¨è§†å…¼å®¹çš„å‡ ä½•ä½“ï¼›3) æå–ä½“ç§¯è§†è§‰èŒƒå›´åŒºåŸŸï¼›4) åˆ©ç”¨3D U-Netå’Œå•å‘Transformerè¿›è¡Œæ—¶ç©ºç‰¹å¾èžåˆï¼Œé¢„æµ‹æœªæ¥çš„3Dè§†è§‰èŒƒå›´ã€‚è¯¥æ¡†æž¶å°†SLAMã€è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œå®žçŽ°äº†ä»Ž2Dåˆ°3Dçš„è§†è§‰èŒƒå›´é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†EgoSpanLiftæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå°†2Då›¾åƒå¹³é¢ä¸Šçš„è§†è§‰èŒƒå›´é¢„æµ‹é—®é¢˜è½¬åŒ–ä¸º3Dåœºæ™¯ä¸­çš„ä½“ç§¯é¢„æµ‹é—®é¢˜ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEgoSpanLiftèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨3Dåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œä»Žè€Œæ›´å‡†ç¡®åœ°é¢„æµ‹ä¸ªä½“åœ¨3DçŽ¯å¢ƒä¸­çš„è§†è§‰å…³æ³¨åŒºåŸŸã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æž„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„3Dè§†è§‰èŒƒå›´é¢„æµ‹æ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨SLAMæŠ€æœ¯èŽ·å–3Dåœºæ™¯å‡ ä½•ä¿¡æ¯ï¼›2) è®¾è®¡äº†ä¸€ç§å°†SLAMå…³é”®ç‚¹è½¬æ¢ä¸ºä¸Žæ³¨è§†å…¼å®¹çš„å‡ ä½•ä½“çš„æ–¹æ³•ï¼›3) åˆ©ç”¨3D U-Netæå–ç©ºé—´ç‰¹å¾ï¼Œåˆ©ç”¨å•å‘Transformeræå–æ—¶é—´ç‰¹å¾ï¼›4) è®¾è®¡äº†ä¸€ç§æŸå¤±å‡½æ•°ï¼Œç”¨äºŽä¼˜åŒ–3Dè§†è§‰èŒƒå›´é¢„æµ‹æ¨¡åž‹ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒEgoSpanLiftæ–¹æ³•åœ¨ç¬¬ä¸€äººç§°2Dæ³¨è§†é¢„æµ‹å’Œ3Då®šä½æ–¹é¢ä¼˜äºŽç«žäº‰åŸºçº¿ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨3Dè§†è§‰èŒƒå›´é¢„æµ‹ä»»åŠ¡ä¸Šï¼ŒEgoSpanLiftçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨æ²¡æœ‰é¢å¤–2Dç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†EgoSpanLiftçš„é¢„æµ‹ç»“æžœæŠ•å½±å›ž2Då›¾åƒå¹³é¢æ—¶ï¼Œä¹Ÿèƒ½èŽ·å¾—ä¸ŽçŽ°æœ‰2Dæ³¨è§†é¢„æµ‹æ–¹æ³•ç›¸å½“çš„ç»“æžœï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå¢žå¼ºçŽ°å®žï¼ˆARï¼‰å’Œè™šæ‹ŸçŽ°å®žï¼ˆVRï¼‰é¢†åŸŸï¼Œä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è§†è§‰å…³æ³¨ç‚¹åŠ¨æ€è°ƒæ•´AR/VRåœºæ™¯çš„æ¸²æŸ“è´¨é‡ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºŽè¾…åŠ©æŠ€æœ¯é¢†åŸŸï¼Œä¾‹å¦‚ï¼Œå¯ä»¥å¸®åŠ©è§†åŠ›éšœç¢è€…æ›´å¥½åœ°ç†è§£å‘¨å›´çŽ¯å¢ƒï¼Œæé«˜ç”Ÿæ´»è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½ç›‘æŽ§ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

