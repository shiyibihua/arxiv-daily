---
layout: default
title: MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment
---

# MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment

**arXiv**: [2511.17397v1](https://arxiv.org/abs/2511.17397) | [PDF](https://arxiv.org/pdf/2511.17397.pdf)

**ä½œè€…**: Huangbiao Xu, Huanqi Wu, Xiao Ke, Junyi Wu, Rui Xu, Jinglin Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMCMoEæ¡†æž¶ï¼Œé€šè¿‡ä¸“å®¶æ··åˆå®Œæˆç¼ºå¤±æ¨¡æ€ï¼Œè§£å†³ä¸å®Œæ•´å¤šæ¨¡æ€åŠ¨ä½œè´¨é‡è¯„ä¼°é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŠ¨ä½œè´¨é‡è¯„ä¼°` `æ¨¡æ€ç¼ºå¤±å®Œæˆ` `ä¸“å®¶æ··åˆæ¨¡åž‹` `è‡ªé€‚åº”é—¨æŽ§ç”Ÿæˆ` `è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ` `å•é˜¶æ®µè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¨ç†æ—¶æ¨¡æ€ç¼ºå¤±å¯¼è‡´å¤šæ¨¡æ€æ¨¡åž‹å¤±æ•ˆå’Œæ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè‡ªé€‚åº”é—¨æŽ§æ¨¡æ€ç”Ÿæˆå™¨åŠ¨æ€èžåˆå¯ç”¨ä¿¡æ¯é‡æž„ç¼ºå¤±æ¨¡æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªå…¬å…±AQAåŸºå‡†ä¸Šå®žçŽ°å®Œæ•´å’Œä¸å®Œæ•´å¤šæ¨¡æ€å­¦ä¹ çš„æœ€ä¼˜ç»“æžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.

