---
layout: default
title: A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback
---

# A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback

**arXiv**: [2511.17255v1](https://arxiv.org/abs/2511.17255) | [PDF](https://arxiv.org/pdf/2511.17255.pdf)

**ä½œè€…**: Bulat Khaertdinov, Mirela Popa, Nava Tintarev

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç›¸å…³æ€§åé¦ˆçš„æœºåˆ¶ï¼Œä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢` `ç›¸å…³æ€§åé¦ˆ` `ä¼ªç›¸å…³åé¦ˆ` `ç”Ÿæˆç›¸å…³åé¦ˆ` `æ³¨æ„åŠ›åé¦ˆæ‘˜è¦å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹æ£€ç´¢æ€§èƒ½æå‡å¸¸éœ€å¾®è°ƒæˆ–æ›´å¤§æ¨¡åž‹ï¼Œç¼ºä¹æŽ¨ç†æ—¶ä¼˜åŒ–æ–¹æ³•
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å››ç§åé¦ˆç­–ç•¥ï¼ŒåŒ…æ‹¬ä¼ªç›¸å…³åé¦ˆã€ç”Ÿæˆç›¸å…³åé¦ˆå’Œæ³¨æ„åŠ›åé¦ˆæ‘˜è¦å™¨
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Flickr30kå’ŒCOCOæ•°æ®é›†ä¸Šï¼Œåé¦ˆç­–ç•¥ä½¿MRR@5æå‡1-5%ï¼Œå¢žå¼ºé²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.

