---
layout: default
title: Counterfactual World Models via Digital Twin-conditioned Video Diffusion
---

# Counterfactual World Models via Digital Twin-conditioned Video Diffusion

**arXiv**: [2511.17481v1](https://arxiv.org/abs/2511.17481) | [PDF](https://arxiv.org/pdf/2511.17481.pdf)

**ä½œè€…**: Yiqing Shen, Aiza Maksutova, Chenjia Li, Mathias Unberath

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCWMDTæ¡†æž¶ï¼Œé€šè¿‡æ•°å­—å­ªç”Ÿå’Œè§†é¢‘æ‰©æ•£æ¨¡åž‹å®žçŽ°åäº‹å®žä¸–ç•Œå»ºæ¨¡**

**å…³é”®è¯**: `åäº‹å®žä¸–ç•Œæ¨¡åž‹` `æ•°å­—å­ªç”Ÿ` `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `å¤§è¯­è¨€æ¨¡åž‹` `åœºæ™¯å¹²é¢„` `è§†é¢‘ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿä¸–ç•Œæ¨¡åž‹æ— æ³•å¤„ç†åäº‹å®žæŸ¥è¯¢ï¼Œå¦‚å¯¹è±¡ç§»é™¤ç­‰å‡è®¾æ€§å¹²é¢„ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºæ•°å­—å­ªç”Ÿç¼–ç åœºæ™¯ç»“æž„ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†å¹²é¢„ä¼ æ’­ï¼Œå¹¶æ¡ä»¶è§†é¢‘æ‰©æ•£ç”Ÿæˆåºåˆ—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯æ•°å­—å­ªç”Ÿä½œä¸ºæŽ§åˆ¶ä¿¡å·çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as "what would happen if this object was removed?", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.

