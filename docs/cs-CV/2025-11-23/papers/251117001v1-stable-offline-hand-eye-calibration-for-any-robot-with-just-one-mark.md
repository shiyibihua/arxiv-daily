---
layout: default
title: Stable Offline Hand-Eye Calibration for any Robot with Just One Mark
---

# Stable Offline Hand-Eye Calibration for any Robot with Just One Mark

**arXiv**: [2511.17001v1](https://arxiv.org/abs/2511.17001) | [PDF](https://arxiv.org/pdf/2511.17001.pdf)

**ä½œè€…**: Sicheng Xie, Lingchen Meng, Zhiying Du, Shuyuan Tu, Haidong Cao, Jiaqi Leng, Zuxuan Wu, Yu-Gang Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCalibAllæ–¹æ³•ï¼Œä»…éœ€å•ä¸€æ ‡è®°å®žçŽ°ç¨³å®šç¦»çº¿æ‰‹çœ¼æ ‡å®š**

**å…³é”®è¯**: `æ‰‹çœ¼æ ‡å®š` `ç›¸æœºå¤–å‚ä¼°è®¡` `è§†è§‰åŸºç¡€æ¨¡åž‹` `ç¦»çº¿æ ¡å‡†` `æœºå™¨äººè§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººæ‰‹çœ¼æ ‡å®šä¸­ç›¸æœºå¤–å‚ä¼°è®¡å¸¸ä¸å¯ç”¨ï¼ŒçŽ°æœ‰æ–¹æ³•æ˜“é™·å±€éƒ¨æœ€ä¼˜ä¸”æ³›åŒ–å·®
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡åž‹å®šä½æ ‡è®°ï¼Œç»“åˆç‚¹è·Ÿè¸ªä¸Ž3Dè½¨è¿¹ï¼Œé€šè¿‡ç²—åˆ°ç²¾æµç¨‹ä¼˜åŒ–å¤–å‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªæœºå™¨äººå¹³å°ä¸Šè¶…è¶Šå…ˆè¿›æ–¹æ³•ï¼Œå±•çŽ°å¼ºé²æ£’æ€§å’Œé€šç”¨æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information ({\ie}, camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. In this paper, we present CalibAll, a simple yet effective method that \textbf{requires only a single mark} and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks, yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms. It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.

