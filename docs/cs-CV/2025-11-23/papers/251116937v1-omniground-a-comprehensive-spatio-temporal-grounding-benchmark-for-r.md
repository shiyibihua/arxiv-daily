---
layout: default
title: OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios
---

# OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios

**arXiv**: [2511.16937v1](https://arxiv.org/abs/2511.16937) | [PDF](https://arxiv.org/pdf/2511.16937.pdf)

**ä½œè€…**: Hong Gao, Jingyu Wu, Xiangkai Xu, Kangni Xie, Yunchen Zhang, Bin Zhong, Xurui Gao, Min-Ling Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniGroundåŸºå‡†å’ŒPG-TAFæ¡†æž¶ä»¥è§£å†³å¤æ‚åœºæ™¯æ—¶ç©ºè§†é¢‘å®šä½é—®é¢˜**

**å…³é”®è¯**: `æ—¶ç©ºè§†é¢‘å®šä½` `åŸºå‡†æž„å»º` `è®­ç»ƒå…è´¹æ¡†æž¶` `å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹` `å¤æ‚æŸ¥è¯¢å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰STVGæ¨¡åž‹åœ¨çœŸå®žå¤æ‚åœºæ™¯ä¸­è¡¨çŽ°ä¸ä½³ï¼Œå­˜åœ¨ç±»åˆ«åè§å’ŒæŽ¨ç†ç®€åŒ–é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥OmniGroundåŸºå‡†å’ŒPG-TAFè®­ç»ƒå…è´¹æ¡†æž¶ï¼Œåˆ†è§£æ—¶ç©ºå®šä½ä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šPG-TAFåœ¨OmniGroundä¸Šm_tIoUå’Œm_vIoUåˆ†åˆ«æå‡25.6%å’Œ35.6%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\_tIoU and m\_vIoU on OmniGround with consistent gains across four benchmarks.

