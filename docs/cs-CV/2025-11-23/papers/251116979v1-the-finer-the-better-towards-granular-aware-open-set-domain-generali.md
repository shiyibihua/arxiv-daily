---
layout: default
title: The Finer the Better: Towards Granular-aware Open-set Domain Generalization
---

# The Finer the Better: Towards Granular-aware Open-set Domain Generalization

**arXiv**: [2511.16979v1](https://arxiv.org/abs/2511.16979) | [PDF](https://arxiv.org/pdf/2511.16979.pdf)

**ä½œè€…**: Yunyun Wang, Zheng Duan, Xinyue Liao, Ke-Jia Chen, Songcan Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeeCLIPæ¡†æž¶ä»¥è§£å†³å¼€æ”¾é›†åŸŸæ³›åŒ–ä¸­ç»†ç²’åº¦æœªçŸ¥ç±»è¯†åˆ«éš¾é¢˜**

**å…³é”®è¯**: `å¼€æ”¾é›†åŸŸæ³›åŒ–` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç»†ç²’åº¦è¯­ä¹‰` `å¯¹æ¯”å­¦ä¹ ` `ä¼ªæœªçŸ¥ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€æ”¾é›†åŸŸæ³›åŒ–ä¸­æ¨¡åž‹æ˜“å¯¹ä¸Žå·²çŸ¥ç±»è§†è§‰ç›¸ä¼¼çš„ç¡¬æœªçŸ¥ç±»è¿‡åº¦è‡ªä¿¡
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¯­ä¹‰å¢žå¼ºæç¤ºå’ŒåŒå·¥å¯¹æ¯”å­¦ä¹ æå‡ç»†ç²’åº¦è§†è§‰è¯­è¨€å¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®çŽ‡æå‡3%ï¼ŒH-scoreæå‡5%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.

