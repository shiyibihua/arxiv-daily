---
layout: default
title: MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models
---

# MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models

**arXiv**: [2511.17448v1](https://arxiv.org/abs/2511.17448) | [PDF](https://arxiv.org/pdf/2511.17448.pdf)

**ä½œè€…**: Yuqi Li, Junhao Dong, Chuanguang Yang, Shiping Wen, Piotr Koniusz, Tingwen Huang, Yingli Tian, Yew-Soon Ong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¤šæ•™å¸ˆå¯¹æŠ—è’¸é¦æ¡†æž¶ä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯¹æŠ—é²æ£’æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¯¹æŠ—é²æ£’æ€§` `çŸ¥è¯†è’¸é¦` `å¤šæ•™å¸ˆå­¦ä¹ ` `æ¨¡æ€èžåˆ` `åŠ¨æ€æƒé‡åˆ†é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å¯¹æŠ—é²æ£’æ€§ä¸è¶³ï¼Œä¼ ç»Ÿå•æ•™å¸ˆæ–¹æ³•çŸ¥è¯†å¤šæ ·æ€§æœ‰é™
2. é‡‡ç”¨åŒæ•™å¸ˆçŸ¥è¯†èžåˆæž¶æž„ï¼ŒåŠ¨æ€æƒé‡åˆ†é…å’Œè‡ªé€‚åº”åŠ æƒå‡½æ•°å¹³è¡¡æ¨¡æ€é—´çŸ¥è¯†è½¬ç§»
3. åœ¨ImageNetå’Œé›¶æ ·æœ¬åŸºå‡†ä¸Šï¼ŒViT-B-32æ¨¡åž‹é²æ£’å‡†ç¡®çŽ‡æå‡4.32%ï¼Œè®­ç»ƒæ•ˆçŽ‡æé«˜2.3å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.

