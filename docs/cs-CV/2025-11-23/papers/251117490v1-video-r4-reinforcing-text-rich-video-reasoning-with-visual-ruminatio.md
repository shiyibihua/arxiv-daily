---
layout: default
title: Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination
---

# Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination

**arXiv**: [2511.17490v1](https://arxiv.org/abs/2511.17490) | [PDF](https://arxiv.org/pdf/2511.17490.pdf)

**ä½œè€…**: Yolo Yunlong Tang, Daiki Shimada, Hang Hua, Chao Huang, Jing Bi, Rogerio Feris, Chenliang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo-R4æ¨¡åž‹ï¼Œé€šè¿‡è§†è§‰ååˆè§£å†³æ–‡æœ¬ä¸°å¯Œè§†é¢‘ä¸­çš„ç»†ç²’åº¦æŽ¨ç†é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘æŽ¨ç†` `è§†è§‰ååˆ` `æ–‡æœ¬ä¸°å¯Œè§†é¢‘` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `ç»†ç²’åº¦è¯æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬ä¸°å¯Œè§†é¢‘ä¸­ç»†å°ã€çž¬æ€æ–‡æœ¬çº¿ç´¢æ˜“è¢«å¿½ç•¥ï¼Œå¯¼è‡´å¹»è§‰å’ŒæŽ¨ç†å¤±è´¥
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†è§‰ååˆï¼Œè¿­ä»£é€‰æ‹©å¸§ã€æ”¾å¤§åŒºåŸŸã€é‡æ–°ç¼–ç åƒç´ ä»¥æ›´æ–°æŽ¨ç†çŠ¶æ€
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨M4-ViteVQAä¸Šè¾¾åˆ°SOTAï¼Œå¹¶æ³›åŒ–åˆ°æ–‡æ¡£å’Œé€šç”¨è§†é¢‘QAä»»åŠ¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.

