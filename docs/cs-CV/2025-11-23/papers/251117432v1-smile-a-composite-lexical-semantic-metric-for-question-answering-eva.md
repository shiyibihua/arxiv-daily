---
layout: default
title: SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation
---

# SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation

**arXiv**: [2511.17432v1](https://arxiv.org/abs/2511.17432) | [PDF](https://arxiv.org/pdf/2511.17432.pdf)

**ä½œè€…**: Shrikant Kendre, Austin Xu, Honglu Zhou, Michael Ryoo, Shafiq Joty, Juan Carlos Niebles

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSMILEå¤åˆæŒ‡æ ‡ï¼Œç»“åˆè¯æ³•å’Œè¯­ä¹‰è¯„ä¼°ä»¥æ”¹è¿›é—®ç­”ç³»ç»Ÿè¯„ä»·ã€‚**

**å…³é”®è¯**: `é—®ç­”ç³»ç»Ÿè¯„ä¼°` `è¯­ä¹‰ç›¸ä¼¼åº¦` `è¯æ³•åŒ¹é…` `å¤åˆæŒ‡æ ‡` `è½»é‡è®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿé—®ç­”è¯„ä¼°æŒ‡æ ‡ä¾èµ–n-gramï¼Œå¿½ç•¥æ·±å±‚è¯­ä¹‰ç†è§£ï¼Œå¯¼è‡´è¯„ä¼°ä¸å‡†ç¡®ã€‚
2. SMILEèžåˆå¥å­çº§å’Œå…³é”®è¯çº§è¯­ä¹‰ï¼Œå¹¶ä¿ç•™è¯æ³•åŒ¹é…ï¼Œå¹³è¡¡ç²¾ç¡®æ€§ä¸Žç›¸å…³æ€§ã€‚
3. åœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘QAä»»åŠ¡ä¸­ï¼ŒSMILEä¸Žäººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œä¸”è®¡ç®—è½»é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.

