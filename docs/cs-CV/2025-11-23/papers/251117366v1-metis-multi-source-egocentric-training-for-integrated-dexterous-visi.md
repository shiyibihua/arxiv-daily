---
layout: default
title: METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model
---

# METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model

**arXiv**: [2511.17366v1](https://arxiv.org/abs/2511.17366) | [PDF](https://arxiv.org/pdf/2511.17366.pdf)

**ä½œè€…**: Yankai Fu, Ning Chen, Junkai Zhao, Shaozhe Shan, Guocai Yao, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMETISæ¨¡åž‹ï¼Œé€šè¿‡å¤šæºè‡ªæˆ‘ä¸­å¿ƒæ•°æ®è®­ç»ƒè§£å†³çµå·§æ“ä½œçš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

**å…³é”®è¯**: `çµå·§æ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `å¤šæºæ•°æ®é›†æˆ` `è¿åŠ¨è¡¨ç¤º` `è‡ªæˆ‘ä¸­å¿ƒè§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçµå·§æ“ä½œç¼ºä¹å¤§è§„æ¨¡åŠ¨ä½œæ ‡æ³¨æ•°æ®ï¼Œäººç±»ä¸Žæœºå™¨äººè§†è§‰å·®å¼‚å¤§
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºEgoAtlasæ•°æ®é›†ï¼Œç»Ÿä¸€åŠ¨ä½œç©ºé—´å¹¶æå–è¿åŠ¨æ„ŸçŸ¥åŠ¨æ€è¡¨ç¤º
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…­é¡¹çœŸå®žä»»åŠ¡ä¸­è¾¾åˆ°æœ€é«˜å¹³å‡æˆåŠŸçŽ‡ï¼Œæ³›åŒ–æ€§å¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.

