---
layout: default
title: CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images
---

# CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images

**arXiv**: [2511.18424v1](https://arxiv.org/abs/2511.18424) | [PDF](https://arxiv.org/pdf/2511.18424.pdf)

**ä½œè€…**: Avishka Perera, Kumal Hewagamage, Saeedha Nazar, Kavishka Abeywardana, Hasitha Gallella, Ranga Rodrigo, Mohamed Afham

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-23

**å¤‡æ³¨**: 24 pages, 10 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCrossJEPAä»¥è§£å†³3Dè¡¨ç¤ºå­¦ä¹ ä¸­çš„2Då›¾åƒæ•°æ®ç¨€ç¼ºé—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è·¨æ¨¡æ€å­¦ä¹ ` `3Dè¡¨ç¤ºå­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `è”åˆåµŒå…¥` `å›¾åƒåŸºç¡€æ¨¡åž‹` `é«˜æ•ˆè®­ç»ƒ` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„å›¾åƒåˆ°ç‚¹äº‘è·¨æ¨¡æ€å­¦ä¹ æ–¹æ³•åœ¨æ¨¡åž‹è§„æ¨¡å’Œè®­ç»ƒé€Ÿåº¦ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—é«˜ã€‚
2. CrossJEPAé€šè¿‡å¼•å…¥å›¾åƒåŸºç¡€æ¨¡åž‹çš„çŸ¥è¯†ï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„è”åˆåµŒå…¥é¢„æµ‹æž¶æž„ï¼Œä¼˜åŒ–äº†è·¨æ¨¡æ€å­¦ä¹ è¿‡ç¨‹ã€‚
3. åœ¨ModelNet40å’ŒScanObjectNNåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCrossJEPAåˆ†åˆ«è¾¾åˆ°äº†94.2%å’Œ88.3%çš„å‡†ç¡®çŽ‡ï¼Œå±•çŽ°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œæ•ˆçŽ‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾åƒåˆ°ç‚¹äº‘çš„è·¨æ¨¡æ€å­¦ä¹ å·²æˆä¸ºè§£å†³3Dè¡¨ç¤ºå­¦ä¹ ä¸­å¤§è§„æ¨¡3Dæ•°æ®é›†ç¨€ç¼ºé—®é¢˜çš„é‡è¦æ–¹æ³•ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰åˆ©ç”¨2Dæ•°æ®çš„æ–¹æ³•å¾€å¾€å¯¼è‡´æ¨¡åž‹åºžå¤§ä¸”è®­ç»ƒç¼“æ…¢ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„çŽ¯å¢ƒä¸­éƒ¨ç½²ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºCrossJEPAï¼Œä¸€ç§ç®€å•çš„è·¨æ¨¡æ€è”åˆåµŒå…¥é¢„æµ‹æž¶æž„ï¼Œåˆ©ç”¨å›¾åƒåŸºç¡€æ¨¡åž‹çš„çŸ¥è¯†ï¼Œè®­ç»ƒé¢„æµ‹å™¨ä»Žå¯¹åº”çš„3Dç‚¹äº‘ä¸­æŽ¨æ–­ç‰¹å®šæ¸²æŸ“2Dè§†å›¾çš„åµŒå…¥ã€‚CrossJEPAåœ¨åˆæˆæ•°æ®é›†ModelNet40å’ŒçœŸå®žæ•°æ®é›†ScanObjectNNä¸Šå®žçŽ°äº†æ–°çš„çº¿æ€§æŽ¢æµ‹çŠ¶æ€ï¼Œåˆ†åˆ«è¾¾åˆ°94.2%å’Œ88.3%çš„å‡†ç¡®çŽ‡ï¼Œä¸”ä»…ä½¿ç”¨14.1Mçš„é¢„è®­ç»ƒå‚æ•°ï¼Œå±•çŽ°å‡ºé«˜æ•ˆçš„å†…å­˜ä½¿ç”¨å’Œå¿«é€Ÿè®­ç»ƒçš„ä¼˜åŠ¿ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³3Dè¡¨ç¤ºå­¦ä¹ ä¸­ç”±äºŽç¼ºä¹å¤§è§„æ¨¡3Dæ•°æ®é›†è€Œå¯¼è‡´çš„æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡ä½Žä¸‹å’Œè®¡ç®—èµ„æºæµªè´¹çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åºžå¤§çš„æ¨¡åž‹å’Œé•¿æ—¶é—´çš„è®­ç»ƒï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„çŽ¯å¢ƒä¸­åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCrossJEPAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾åƒåŸºç¡€æ¨¡åž‹çš„çŸ¥è¯†ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªé¢„æµ‹å™¨æ¥æŽ¨æ–­ç‰¹å®šæ¸²æŸ“2Dè§†å›¾çš„åµŒå…¥ï¼Œä»Žè€Œå®žçŽ°è·¨æ¨¡æ€çš„è”åˆåµŒå…¥å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„æŽ©è”½ç­–ç•¥ï¼Œæä¾›äº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCrossJEPAçš„æ•´ä½“æž¶æž„åŒ…æ‹¬ä¸€ä¸ªå›¾åƒåŸºç¡€æ¨¡åž‹å’Œä¸€ä¸ªé¢„æµ‹å™¨æ¨¡å—ã€‚é¢„æµ‹å™¨æ ¹æ®è·¨åŸŸæŠ•å½±ä¿¡æ¯è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»Žè€Œæå–ç›®æ ‡åŸŸç‰¹æœ‰çš„è¯­ä¹‰ä¿¡æ¯ã€‚è®¾è®¡ä¸­è¿˜å¼•å…¥äº†å†»ç»“æ•™å¸ˆæ¨¡åž‹å’Œä¸€æ¬¡æ€§ç›®æ ‡åµŒå…¥ç¼“å­˜æœºåˆ¶ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆçŽ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCrossJEPAçš„ä¸»è¦åˆ›æ–°åœ¨äºŽå…¶ç®€åŒ–çš„è·¨æ¨¡æ€è”åˆåµŒå…¥é¢„æµ‹æž¶æž„ï¼Œæ‰“ç ´äº†å¯¹æŽ©è”½ç­–ç•¥çš„ä¾èµ–ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯å®žçŽ°äº†é«˜æ•ˆçš„3Dè¡¨ç¤ºå­¦ä¹ ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCrossJEPAåœ¨æ¨¡åž‹è§„æ¨¡å’Œè®­ç»ƒæ—¶é—´ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCrossJEPAä½¿ç”¨äº†14.1Mçš„é¢„è®­ç»ƒå‚æ•°ï¼Œå…¶ä¸­ç‚¹ç¼–ç å™¨å 8.5Mï¼Œè®­ç»ƒæ—¶é—´çº¦ä¸º6å°æ—¶ï¼Œå±•çŽ°å‡ºè‰¯å¥½çš„å†…å­˜æ•ˆçŽ‡å’Œå¿«é€Ÿè®­ç»ƒèƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

CrossJEPAåœ¨åˆæˆæ•°æ®é›†ModelNet40ä¸Šè¾¾åˆ°äº†94.2%çš„çº¿æ€§æŽ¢æµ‹å‡†ç¡®çŽ‡ï¼Œåœ¨çœŸå®žæ•°æ®é›†ScanObjectNNä¸Šè¾¾åˆ°äº†88.3%ã€‚è¿™äº›ç»“æžœä¸ä»…åˆ·æ–°äº†çŽ°æœ‰çš„æ€§èƒ½è®°å½•ï¼Œè¿˜è¡¨æ˜Žè¯¥æ–¹æ³•åœ¨å‚æ•°ä½¿ç”¨å’Œè®­ç»ƒæ•ˆçŽ‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CrossJEPAåœ¨3Dè¡¨ç¤ºå­¦ä¹ é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸçŽ°å®žç­‰éœ€è¦é«˜æ•ˆå¤„ç†3Dæ•°æ®çš„åœºæ™¯ä¸­ã€‚å…¶é«˜æ•ˆçš„è®­ç»ƒå’Œå†…å­˜ä½¿ç”¨ç‰¹æ€§ä½¿å…¶é€‚åˆåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²ï¼ŒæŽ¨åŠ¨äº†ç›¸å…³æŠ€æœ¯çš„å®žé™…åº”ç”¨å’Œå‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

