---
layout: default
title: CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images
---

# CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18424" target="_blank" class="toolbar-btn">arXiv: 2511.18424v1</a>
    <a href="https://arxiv.org/pdf/2511.18424.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18424v1" 
            onclick="toggleFavorite(this, '2511.18424v1', 'CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Avishka Perera, Kumal Hewagamage, Saeedha Nazar, Kavishka Abeywardana, Hasitha Gallella, Ranga Rodrigo, Mohamed Afham

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-23

**Â§áÊ≥®**: 24 pages, 10 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CrossJEPA‰ª•Ëß£ÂÜ≥3DË°®Á§∫Â≠¶‰π†‰∏≠ÁöÑ2DÂõæÂÉèÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `3DË°®Á§∫Â≠¶‰π†` `Áü•ËØÜËí∏È¶è` `ËÅîÂêàÂµåÂÖ•` `ÂõæÂÉèÂü∫Á°ÄÊ®°Âûã` `È´òÊïàËÆ≠ÁªÉ` `Ê®°ÂûãÂéãÁº©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂõæÂÉèÂà∞ÁÇπ‰∫ëË∑®Ê®°ÊÄÅÂ≠¶‰π†ÊñπÊ≥ïÂú®Ê®°ÂûãËßÑÊ®°ÂíåËÆ≠ÁªÉÈÄüÂ∫¶‰∏äÂ≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÂØºËá¥ËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÈ´ò„ÄÇ
2. CrossJEPAÈÄöËøáÂºïÂÖ•ÂõæÂÉèÂü∫Á°ÄÊ®°ÂûãÁöÑÁü•ËØÜÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑÔºå‰ºòÂåñ‰∫ÜË∑®Ê®°ÊÄÅÂ≠¶‰π†ËøáÁ®ã„ÄÇ
3. Âú®ModelNet40ÂíåScanObjectNNÂü∫ÂáÜÊµãËØï‰∏≠ÔºåCrossJEPAÂàÜÂà´ËææÂà∞‰∫Ü94.2%Âíå88.3%ÁöÑÂáÜÁ°ÆÁéáÔºåÂ±ïÁé∞Âá∫‰ºòÂºÇÁöÑÊÄßËÉΩÂíåÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂõæÂÉèÂà∞ÁÇπ‰∫ëÁöÑË∑®Ê®°ÊÄÅÂ≠¶‰π†Â∑≤Êàê‰∏∫Ëß£ÂÜ≥3DË°®Á§∫Â≠¶‰π†‰∏≠Â§ßËßÑÊ®°3DÊï∞ÊçÆÈõÜÁ®ÄÁº∫ÈóÆÈ¢òÁöÑÈáçË¶ÅÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÂà©Áî®2DÊï∞ÊçÆÁöÑÊñπÊ≥ïÂæÄÂæÄÂØºËá¥Ê®°ÂûãÂ∫ûÂ§ß‰∏îËÆ≠ÁªÉÁºìÊÖ¢ÔºåËÆ°ÁÆóÊàêÊú¨È´òÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÈÉ®ÁΩ≤„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫CrossJEPAÔºå‰∏ÄÁßçÁÆÄÂçïÁöÑË∑®Ê®°ÊÄÅËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑÔºåÂà©Áî®ÂõæÂÉèÂü∫Á°ÄÊ®°ÂûãÁöÑÁü•ËØÜÔºåËÆ≠ÁªÉÈ¢ÑÊµãÂô®‰ªéÂØπÂ∫îÁöÑ3DÁÇπ‰∫ë‰∏≠Êé®Êñ≠ÁâπÂÆöÊ∏≤Êüì2DËßÜÂõæÁöÑÂµåÂÖ•„ÄÇCrossJEPAÂú®ÂêàÊàêÊï∞ÊçÆÈõÜModelNet40ÂíåÁúüÂÆûÊï∞ÊçÆÈõÜScanObjectNN‰∏äÂÆûÁé∞‰∫ÜÊñ∞ÁöÑÁ∫øÊÄßÊé¢ÊµãÁä∂ÊÄÅÔºåÂàÜÂà´ËææÂà∞94.2%Âíå88.3%ÁöÑÂáÜÁ°ÆÁéáÔºå‰∏î‰ªÖ‰ΩøÁî®14.1MÁöÑÈ¢ÑËÆ≠ÁªÉÂèÇÊï∞ÔºåÂ±ïÁé∞Âá∫È´òÊïàÁöÑÂÜÖÂ≠ò‰ΩøÁî®ÂíåÂø´ÈÄüËÆ≠ÁªÉÁöÑ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥3DË°®Á§∫Â≠¶‰π†‰∏≠Áî±‰∫éÁº∫‰πèÂ§ßËßÑÊ®°3DÊï∞ÊçÆÈõÜËÄåÂØºËá¥ÁöÑÊ®°ÂûãËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÂíåËÆ°ÁÆóËµÑÊ∫êÊµ™Ë¥πÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ∫ûÂ§ßÁöÑÊ®°ÂûãÂíåÈïøÊó∂Èó¥ÁöÑËÆ≠ÁªÉÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Â∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCrossJEPAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂõæÂÉèÂü∫Á°ÄÊ®°ÂûãÁöÑÁü•ËØÜÔºåÈÄöËøáËÆ≠ÁªÉ‰∏Ä‰∏™È¢ÑÊµãÂô®Êù•Êé®Êñ≠ÁâπÂÆöÊ∏≤Êüì2DËßÜÂõæÁöÑÂµåÂÖ•Ôºå‰ªéËÄåÂÆûÁé∞Ë∑®Ê®°ÊÄÅÁöÑËÅîÂêàÂµåÂÖ•Â≠¶‰π†„ÄÇËøôÁßçÊñπÊ≥ïË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑÊé©ËîΩÁ≠ñÁï•ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCrossJEPAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™ÂõæÂÉèÂü∫Á°ÄÊ®°ÂûãÂíå‰∏Ä‰∏™È¢ÑÊµãÂô®Ê®°Âùó„ÄÇÈ¢ÑÊµãÂô®Ê†πÊçÆË∑®ÂüüÊäïÂΩ±‰ø°ÊÅØËøõË°åÊù°‰ª∂ÂåñÔºå‰ªéËÄåÊèêÂèñÁõÆÊ†áÂüüÁâπÊúâÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇËÆæËÆ°‰∏≠ËøòÂºïÂÖ•‰∫ÜÂÜªÁªìÊïôÂ∏àÊ®°ÂûãÂíå‰∏ÄÊ¨°ÊÄßÁõÆÊ†áÂµåÂÖ•ÁºìÂ≠òÊú∫Âà∂Ôºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCrossJEPAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÁÆÄÂåñÁöÑË∑®Ê®°ÊÄÅËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑÔºåÊâìÁ†¥‰∫ÜÂØπÊé©ËîΩÁ≠ñÁï•ÁöÑ‰æùËµñÔºåÂà©Áî®Áü•ËØÜËí∏È¶èÊäÄÊúØÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑ3DË°®Á§∫Â≠¶‰π†„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCrossJEPAÂú®Ê®°ÂûãËßÑÊ®°ÂíåËÆ≠ÁªÉÊó∂Èó¥‰∏äÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåCrossJEPA‰ΩøÁî®‰∫Ü14.1MÁöÑÈ¢ÑËÆ≠ÁªÉÂèÇÊï∞ÔºåÂÖ∂‰∏≠ÁÇπÁºñÁ†ÅÂô®Âç†8.5MÔºåËÆ≠ÁªÉÊó∂Èó¥Á∫¶‰∏∫6Â∞èÊó∂ÔºåÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÂÜÖÂ≠òÊïàÁéáÂíåÂø´ÈÄüËÆ≠ÁªÉËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CrossJEPAÂú®ÂêàÊàêÊï∞ÊçÆÈõÜModelNet40‰∏äËææÂà∞‰∫Ü94.2%ÁöÑÁ∫øÊÄßÊé¢ÊµãÂáÜÁ°ÆÁéáÔºåÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜScanObjectNN‰∏äËææÂà∞‰∫Ü88.3%„ÄÇËøô‰∫õÁªìÊûú‰∏ç‰ªÖÂà∑Êñ∞‰∫ÜÁé∞ÊúâÁöÑÊÄßËÉΩËÆ∞ÂΩïÔºåËøòË°®ÊòéËØ•ÊñπÊ≥ïÂú®ÂèÇÊï∞‰ΩøÁî®ÂíåËÆ≠ÁªÉÊïàÁéá‰∏äÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CrossJEPAÂú®3DË°®Á§∫Â≠¶‰π†È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫ËßÜËßâ„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈúÄË¶ÅÈ´òÊïàÂ§ÑÁêÜ3DÊï∞ÊçÆÁöÑÂú∫ÊôØ‰∏≠„ÄÇÂÖ∂È´òÊïàÁöÑËÆ≠ÁªÉÂíåÂÜÖÂ≠ò‰ΩøÁî®ÁâπÊÄß‰ΩøÂÖ∂ÈÄÇÂêàÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÈÉ®ÁΩ≤ÔºåÊé®Âä®‰∫ÜÁõ∏ÂÖ≥ÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

