---
layout: default
title: UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification
---

# UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification

**arXiv**: [2511.17355v1](https://arxiv.org/abs/2511.17355) | [PDF](https://arxiv.org/pdf/2511.17355.pdf)

**ä½œè€…**: Taixi Chen, Jingyun Chen, Nancy Guo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ³¨æ„åŠ›-Mambaéª¨å¹²ç½‘ç»œï¼Œç”¨äºŽè‚¿ç˜¤ç»†èƒžåˆ†ç±»çš„å¤šæ¨¡æ€æ¡†æž¶**

**å…³é”®è¯**: `ç»†èƒžçº§æ”¾å°„ç»„å­¦` `æ³¨æ„åŠ›æœºåˆ¶` `Mambaæž¶æž„` `å¤šæ¨¡æ€å­¦ä¹ ` `è‚¿ç˜¤åˆ†ç±»` `å›¾åƒåˆ†å‰²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç ”ç©¶å¤šå…³æ³¨åˆ‡ç‰‡æˆ–æ–‘å—çº§è‚¿ç˜¤åˆ†ç±»ï¼Œç»†èƒžçº§æ”¾å°„ç»„å­¦åˆ†æžæœªè¢«å……åˆ†æŽ¢ç´¢
2. ç»Ÿä¸€è®¾è®¡çµæ´»ç»“åˆæ³¨æ„åŠ›å’ŒMambaæ¨¡å—ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´æ¯”ä¾‹ï¼Œæå‡ç¼–ç èƒ½åŠ›
3. å®žéªŒæ˜¾ç¤ºç»†èƒžåˆ†ç±»å‡†ç¡®çŽ‡ä»Ž74%æå‡è‡³78%ï¼Œè‚¿ç˜¤åˆ†å‰²ç²¾åº¦ä»Ž75%æå‡è‡³80%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.

