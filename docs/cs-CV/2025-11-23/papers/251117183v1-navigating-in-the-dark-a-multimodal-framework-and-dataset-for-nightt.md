---
layout: default
title: Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition
---

# Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition

**arXiv**: [2511.17183v1](https://arxiv.org/abs/2511.17183) | [PDF](https://arxiv.org/pdf/2511.17183.pdf)

**ä½œè€…**: Aditya Mishra, Akshay Agarwal, Haroon Lone

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLENS-Netå’ŒINTSDæ•°æ®é›†ä»¥è§£å†³å¤œé—´äº¤é€šæ ‡å¿—è¯†åˆ«æŒ‘æˆ˜**

**å…³é”®è¯**: `å¤œé—´äº¤é€šæ ‡å¿—è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `å›¾åƒå¢žå¼º` `æ•°æ®é›†æž„å»º` `å›¾å·ç§¯ç½‘ç»œ` `è·¨æ¨¡æ€æ³¨æ„åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤œé—´äº¤é€šæ ‡å¿—è¯†åˆ«å› è§†è§‰å™ªå£°å’Œæ•°æ®é›†ç¨€ç¼ºè€Œå›°éš¾ï¼ŒçŽ°æœ‰æ–¹æ³•åœ¨ä½Žå…‰ç…§ä¸‹ä¸é²æ£’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šLENS-Neté›†æˆè‡ªé€‚åº”å›¾åƒå¢žå¼ºæ£€æµ‹å™¨å’Œå¤šæ¨¡æ€CLIP-GCNNåˆ†ç±»å™¨ï¼Œæå‡è¯†åˆ«é²æ£’æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨INTSDæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒLENS-Netè¶…è¶ŠçŽ°æœ‰æ¡†æž¶ï¼Œæ¶ˆèžç ”ç©¶éªŒè¯ç»„ä»¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.

