---
layout: default
title: RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models
---

# RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18380" target="_blank" class="toolbar-btn">arXiv: 2511.18380v1</a>
    <a href="https://arxiv.org/pdf/2511.18380.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18380v1" 
            onclick="toggleFavorite(this, '2511.18380v1', 'RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Timing Yang, Guoyizhe Wei, Alan Yuille, Feng Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æMambaè§†è§‰æ¨¡å‹è¡¨å¾èƒ½åŠ›ï¼Œæ­ç¤ºå…¶ä¸çº¿æ€§Transformerçš„å…³è”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰Mamba` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `çº¿æ€§æ³¨æ„åŠ›` `ä½ç§©è¿‘ä¼¼` `æ¿€æ´»å›¾è¯„ä¼°` `è‡ªç›‘ç£å­¦ä¹ ` `æ¨¡å‹å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰Mambaæ¨¡å‹çš„å†…éƒ¨æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œé˜»ç¢äº†å¯¹å…¶è¡¨å¾èƒ½åŠ›çš„æ·±å…¥ç†è§£ã€‚
2. è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ­ç¤ºäº†Mambaä¸Softmaxå’Œçº¿æ€§æ³¨æ„åŠ›ä¹‹é—´çš„è”ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMambaåœ¨é•¿ç¨‹ä¾èµ–å»ºæ¨¡å’Œå¯è§£é‡Šæ€§æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¹¶åœ¨ImageNetä¸Šå–å¾—äº†ä¼˜å¼‚çš„çº¿æ€§æ¢æµ‹ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Mambaæœ€è¿‘ä½œä¸ºè§†è§‰ä»»åŠ¡çš„æœ‰æ•ˆéª¨å¹²ç½‘ç»œå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå…¶åœ¨è§†è§‰é¢†åŸŸä¸­çš„æ½œåœ¨æœºåˆ¶ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†Mambaçš„è¡¨å¾ç‰¹æ€§ï¼Œå¹¶åšå‡ºäº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†Mambaä¸Softmaxå’Œçº¿æ€§æ³¨æ„åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œè¯å®å®ƒå¯ä»¥è¢«è§†ä¸ºSoftmaxæ³¨æ„åŠ›çš„ä½ç§©è¿‘ä¼¼ï¼Œä»è€Œå¼¥åˆäº†Softmaxå’Œçº¿æ€§å½¢å¼ä¹‹é—´çš„è¡¨å¾å·®è·ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„äºŒå…ƒåˆ†å‰²æŒ‡æ ‡ç”¨äºæ¿€æ´»å›¾è¯„ä¼°ï¼Œå°†å®šæ€§è¯„ä¼°æ‰©å±•åˆ°å®šé‡æµ‹é‡ï¼Œè¯æ˜äº†Mambaå»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰ï¼Œé€šè¿‡åˆ©ç”¨DINOè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæˆ‘ä»¬è·å¾—äº†æ¯”æ ‡å‡†ç›‘ç£æ–¹æ³•äº§ç”Ÿçš„æ›´æ¸…æ™°çš„æ¿€æ´»å›¾ï¼Œçªå‡ºäº†Mambaçš„å¯è§£é‡Šæ€§æ½œåŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ImageNetä¸Šå®ç°äº†78.5%çš„çº¿æ€§æ¢æµ‹ç²¾åº¦ï¼Œçªæ˜¾äº†å…¶å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿä¸ºæœªæ¥åŸºäºMambaçš„è§†è§‰æ¶æ„çš„ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰Transformeræ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥å¤„ç†é•¿åºåˆ—ã€‚Mambaä½œä¸ºä¸€ç§æ–°å‹åºåˆ—æ¨¡å‹ï¼Œåœ¨è§†è§‰é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶å’Œè¡¨å¾èƒ½åŠ›å°šä¸æ˜ç¡®ï¼Œé™åˆ¶äº†å…¶è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹Mambaè¡¨å¾èƒ½åŠ›çš„æ·±å…¥åˆ†æå’Œæœ‰æ•ˆè¯„ä¼°æ‰‹æ®µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Mambaä¸Softmaxå’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶è”ç³»èµ·æ¥ï¼Œé€šè¿‡ç†è®ºåˆ†æè¯æ˜Mambaå¯ä»¥è§†ä¸ºSoftmaxæ³¨æ„åŠ›çš„ä½ç§©è¿‘ä¼¼ã€‚åŒæ—¶ï¼Œå¼•å…¥æ–°çš„äºŒå…ƒåˆ†å‰²æŒ‡æ ‡å¯¹Mambaçš„æ¿€æ´»å›¾è¿›è¡Œå®šé‡è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒæå‡Mambaçš„å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š1) ç†è®ºåˆ†æMambaä¸Softmaxå’Œçº¿æ€§æ³¨æ„åŠ›çš„å…³ç³»ï¼›2) æå‡ºæ–°çš„äºŒå…ƒåˆ†å‰²æŒ‡æ ‡è¯„ä¼°Mambaçš„æ¿€æ´»å›¾ï¼›3) åˆ©ç”¨DINOè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæå‡Mambaçš„å¯è§£é‡Šæ€§ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆé€šè¿‡ç†è®ºåˆ†æå»ºç«‹è”ç³»ï¼Œç„¶åé€šè¿‡å®éªŒéªŒè¯å’Œè¯„ä¼°Mambaçš„è¡¨å¾èƒ½åŠ›ï¼Œæœ€åé€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒæå‡å…¶æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ä»ç†è®ºä¸Šè¯æ˜äº†Mambaå¯ä»¥è§†ä¸ºSoftmaxæ³¨æ„åŠ›çš„ä½ç§©è¿‘ä¼¼ï¼Œå¼¥åˆäº†Softmaxå’Œçº¿æ€§å½¢å¼ä¹‹é—´çš„è¡¨å¾å·®è·ï¼›2) æå‡ºäº†æ–°çš„äºŒå…ƒåˆ†å‰²æŒ‡æ ‡ï¼Œå°†æ¿€æ´»å›¾çš„è¯„ä¼°ä»å®šæ€§åˆ†ææ‰©å±•åˆ°å®šé‡åˆ†æã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä½œä¸ºMambaçš„åŸºç¡€æ¶æ„ï¼›2) åˆ©ç”¨é€‰æ‹©æœºåˆ¶åŠ¨æ€è°ƒæ•´SSMçš„å‚æ•°ï¼›3) å¼•å…¥æ–°çš„äºŒå…ƒåˆ†å‰²æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åŸºäºæ¿€æ´»å›¾çš„é˜ˆå€¼åˆ†å‰²ï¼Œè¯„ä¼°åˆ†å‰²ç»“æœä¸ground truthçš„ç›¸ä¼¼åº¦ï¼›4) ä½¿ç”¨DINOè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†Mambaåœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ImageNetçº¿æ€§æ¢æµ‹ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å–å¾—äº†78.5%çš„ç²¾åº¦ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„è¡¨å¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡DINOè‡ªç›‘ç£é¢„è®­ç»ƒï¼ŒMambaè·å¾—äº†æ›´æ¸…æ™°çš„æ¿€æ´»å›¾ï¼Œæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£Mambaçš„è¡¨å¾èƒ½åŠ›ï¼Œå¯ä»¥è®¾è®¡æ›´é«˜æ•ˆã€æ›´å¯è§£é‡Šçš„è§†è§‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºçš„äºŒå…ƒåˆ†å‰²æŒ‡æ ‡å¯ä»¥ä½œä¸ºè¯„ä¼°å…¶ä»–è§†è§‰æ¨¡å‹æ¿€æ´»å›¾çš„é€šç”¨å·¥å…·ï¼Œä¿ƒè¿›æ¨¡å‹å¯è§£é‡Šæ€§ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.

