---
layout: default
title: Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models
---

# Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models

**arXiv**: [2511.16955v1](https://arxiv.org/abs/2511.16955) | [PDF](https://arxiv.org/pdf/2511.16955.pdf)

**ä½œè€…**: Dailan He, Guanlin Feng, Xingtong Ge, Yazhe Niu, Yi Zhang, Bingqi Ma, Guanglu Song, Yu Liu, Hongsheng Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNeighbor GRPOä»¥è§£å†³æµåŒ¹é…æ¨¡åž‹å¯¹é½ä¸­çš„é‡‡æ ·æ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `æµåŒ¹é…æ¨¡åž‹` `ç­–ç•¥ä¼˜åŒ–` `å¯¹æ¯”å­¦ä¹ ` `ODEé‡‡æ ·` `å¯¹é½ç®—æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSDE-based GRPOåœ¨æµåŒ¹é…æ¨¡åž‹ä¸­å­˜åœ¨ä¿¡ç”¨åˆ†é…ä½Žæ•ˆå’Œä¸Žé«˜é˜¶æ±‚è§£å™¨ä¸å…¼å®¹
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ‰°åŠ¨ODEåˆå§‹å™ªå£°ç”Ÿæˆå¤šæ ·è½¨è¿¹ï¼Œä½¿ç”¨åŸºäºŽè·ç¦»çš„ä»£ç†ç­–ç•¥ä¼˜åŒ–æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è®­ç»ƒæˆæœ¬ã€æ”¶æ•›é€Ÿåº¦å’Œç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºŽSDE-basedæ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.

