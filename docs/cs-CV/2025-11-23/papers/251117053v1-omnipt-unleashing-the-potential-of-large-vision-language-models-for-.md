---
layout: default
title: OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding
---

# OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding

**arXiv**: [2511.17053v1](https://arxiv.org/abs/2511.17053) | [PDF](https://arxiv.org/pdf/2511.17053.pdf)

**ä½œè€…**: Teng Fu, Mengyang Zhao, Ke Niu, Kaixin Peng, Bin Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniPTæ¡†æž¶ï¼Œåˆ©ç”¨å¤§è§†è§‰è¯­è¨€æ¨¡åž‹ç»Ÿä¸€è¡Œäººè·Ÿè¸ªä¸Žè¯­ä¹‰ç†è§£ä»»åŠ¡**

**å…³é”®è¯**: `è¡Œäººè·Ÿè¸ª` `å¤§è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ è®­ç»ƒ` `è¯­ä¹‰ç†è§£` `ç»Ÿä¸€æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å®žä¾‹çº§ä»»åŠ¡å¦‚è¡Œäººè·Ÿè¸ªä¸­æ€§èƒ½ä¸è¶³ï¼Œéœ€ç»“åˆé«˜çº§è¯­ä¹‰ç†è§£
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨RL-ä¸­æœŸè®­ç»ƒ-SFT-RLè®­ç»ƒæµç¨‹ï¼Œä½¿æ¨¡åž‹è¾“å‡ºæ ¼å¼åŒ–è¾¹ç•Œæ¡†å¹¶æå‡è·Ÿè¸ªèƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼ŒéªŒè¯äº†æ¡†æž¶æœ‰æ•ˆæ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.

