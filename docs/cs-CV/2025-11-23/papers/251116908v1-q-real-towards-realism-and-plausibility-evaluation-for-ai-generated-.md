---
layout: default
title: Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content
---

# Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content

**arXiv**: [2511.16908v1](https://arxiv.org/abs/2511.16908) | [PDF](https://arxiv.org/pdf/2511.16908.pdf)

**ä½œè€…**: Shushi Wang, Zicheng Zhang, Chunyi Li, Wei Wang, Liya Ma, Fengjiao Chen, Xiaoyu Li, Xuezhi Cao, Guangtao Zhai, Xiaohong Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-Realæ•°æ®é›†ä»¥ç²¾ç»†è¯„ä¼°AIç”Ÿæˆå›¾åƒçš„çœŸå®žæ€§ä¸Žåˆç†æ€§**

**å…³é”®è¯**: `AIç”Ÿæˆå†…å®¹è¯„ä¼°` `çœŸå®žæ€§è¯„ä¼°` `åˆç†æ€§è¯„ä¼°` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å›¾åƒç”Ÿæˆ` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰AIç”Ÿæˆå†…å®¹è´¨é‡è¯„ä¼°ä»…æä¾›ç²—ç²’åº¦åˆ†æ•°ï¼Œæ— æ³•é’ˆå¯¹æ€§æŒ‡å¯¼æ¨¡åž‹ä¼˜åŒ–
2. æž„å»ºåŒ…å«3088å¼ å›¾åƒçš„Q-Realæ•°æ®é›†ï¼Œæ ‡æ³¨å®žä½“ä½ç½®å¹¶æä¾›çœŸå®žæ€§ä¸Žåˆç†æ€§åˆ¤æ–­é—®é¢˜
3. å®žéªŒéªŒè¯æ•°æ®é›†é«˜è´¨é‡ï¼Œå¹¶è®¾è®¡å¾®è°ƒæ¡†æž¶æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Quality assessment of AI-generated content is crucial for evaluating model capability and guiding model optimization. However, most existing quality assessment datasets and models provide only a single quality score, which is too coarse to offer targeted guidance for improving generative models. In current applications of AI-generated images, realism and plausibility are two critical dimensions, and with the emergence of unified generation-understanding models, fine-grained evaluation along these dimensions becomes especially effective for improving generative performance. Therefore, we introduce Q-Real, a novel dataset for fine-grained evaluation of realism and plausibility in AI-generated images. Q-Real consists of 3,088 images generated by popular text-to-image models. For each image, we annotate the locations of major entities and provide a set of judgment questions and attribution descriptions for these along the dimensions of realism and plausibility. Considering that recent advances in multi-modal large language models (MLLMs) enable fine-grained evaluation of AI-generated images, we construct Q-Real Bench to evaluate them on two tasks: judgment and grounding with reasoning. Finally, to enhance MLLM capabilities, we design a fine-tuning framework and conduct experiments on multiple MLLMs using our dataset. Experimental results demonstrate the high quality and significance of our dataset and the comprehensiveness of the benchmark. Dataset and code will be released upon publication.

