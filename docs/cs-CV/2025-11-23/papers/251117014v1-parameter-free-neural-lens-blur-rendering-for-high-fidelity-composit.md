---
layout: default
title: Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites
---

# Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites

**arXiv**: [2511.17014v1](https://arxiv.org/abs/2511.17014) | [PDF](https://arxiv.org/pdf/2511.17014.pdf)

**ä½œè€…**: Lingyan Ruan, Bin Chen, Taehyun Rhee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‚æ•°è‡ªç”±ç¥žç»é•œå¤´æ¨¡ç³Šæ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºŽé«˜ä¿çœŸæ··åˆçŽ°å®žåˆæˆ**

**å…³é”®è¯**: `é•œå¤´æ¨¡ç³Šæ¸²æŸ“` `æ··åˆçŽ°å®žåˆæˆ` `å¼¥æ•£åœ†ä¼°è®¡` `ç¥žç»é‡æ¨¡ç³Šç½‘ç»œ` `å‚æ•°è‡ªç”±æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ··åˆçŽ°å®žä¸­è™šæ‹Ÿå¯¹è±¡ä¸ŽçœŸå®žåœºæ™¯èžåˆæ—¶ï¼Œé•œå¤´æ¨¡ç³Šä¸ä¸€è‡´å½±å“è§†è§‰ä¿çœŸåº¦ï¼Œä¸”ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ç›¸æœºå‚æ•°å’Œåœºæ™¯æ·±åº¦ï¼Œæ™®é€šç”¨æˆ·éš¾ä»¥èŽ·å–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç›´æŽ¥ä»ŽRGBå›¾åƒä¼°è®¡å¼¥æ•£åœ†å›¾ï¼Œé€šè¿‡çº¿æ€§å…³ç³»æŽ¨æ–­è™šæ‹Ÿå¯¹è±¡æ¨¡ç³Šï¼Œä½¿ç”¨ç¥žç»é‡æ¨¡ç³Šç½‘ç»œæ¸²æŸ“çœŸå®žé•œå¤´æ¨¡ç³Šã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºæ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ä¼˜äºŽçŽ°æœ‰æŠ€æœ¯ï¼Œå®žçŽ°é«˜ä¿çœŸåˆæˆä¸ŽçœŸå®žæ•£ç„¦æ•ˆæžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.

