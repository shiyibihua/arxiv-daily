---
layout: default
title: Rethinking Diffusion Model-Based Video Super-Resolution: Leveraging Dense Guidance from Aligned Features
---

# Rethinking Diffusion Model-Based Video Super-Resolution: Leveraging Dense Guidance from Aligned Features

**arXiv**: [2511.16928v1](https://arxiv.org/abs/2511.16928) | [PDF](https://arxiv.org/pdf/2511.16928.pdf)

**ä½œè€…**: Jingyi Xu, Meisong Zheng, Ying Chen, Minglang Qiao, Xin Deng, Mai Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDGAF-VSRæ¨¡åž‹ï¼Œåˆ©ç”¨å¯¹é½ç‰¹å¾å¯†é›†å¼•å¯¼è§£å†³è§†é¢‘è¶…åˆ†è¾¨çŽ‡ä¸­çš„è¯¯å·®ç´¯ç§¯ä¸Žä¿çœŸåº¦æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘è¶…åˆ†è¾¨çŽ‡` `æ‰©æ•£æ¨¡åž‹` `ç‰¹å¾å¯¹é½` `æ—¶åºä¸€è‡´æ€§` `å…‰å­¦å¼•å¯¼å˜å½¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„è§†é¢‘è¶…åˆ†è¾¨çŽ‡å­˜åœ¨è¯¯å·®ç´¯ç§¯ã€ç©ºé—´ä¼ªå½±åŠæ„ŸçŸ¥è´¨é‡ä¸Žä¿çœŸåº¦æƒè¡¡é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å…‰å­¦å¼•å¯¼å˜å½¢æ¨¡å—å’Œç‰¹å¾åŸŸæ—¶åºæ¡ä»¶æ¨¡å—ï¼Œåœ¨ç‰¹å¾åŸŸè¿›è¡Œå¯†é›†å¼•å¯¼ä¸Žå¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒçœŸå®žæ•°æ®é›†ä¸Šï¼Œæ„ŸçŸ¥è´¨é‡ã€ä¿çœŸåº¦å’Œæ—¶åºä¸€è‡´æ€§å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion model (DM) based Video Super-Resolution (VSR) approaches achieve impressive perceptual quality. However, they suffer from error accumulation, spatial artifacts, and a trade-off between perceptual quality and fidelity, primarily caused by inaccurate alignment and insufficient compensation between video frames. In this paper, within the DM-based VSR pipeline, we revisit the role of alignment and compensation between adjacent video frames and reveal two crucial observations: (a) the feature domain is better suited than the pixel domain for information compensation due to its stronger spatial and temporal correlations, and (b) warping at an upscaled resolution better preserves high-frequency information, but this benefit is not necessarily monotonic. Therefore, we propose a novel Densely Guided diffusion model with Aligned Features for Video Super-Resolution (DGAF-VSR), with an Optical Guided Warping Module (OGWM) to maintain high-frequency details in the aligned features and a Feature-wise Temporal Condition Module (FTCM) to deliver dense guidance in the feature domain. Extensive experiments on synthetic and real-world datasets demonstrate that DGAF-VSR surpasses state-of-the-art methods in key aspects of VSR, including perceptual quality (35.82\% DISTS reduction), fidelity (0.20 dB PSNR gain), and temporal consistency (30.37\% tLPIPS reduction).

