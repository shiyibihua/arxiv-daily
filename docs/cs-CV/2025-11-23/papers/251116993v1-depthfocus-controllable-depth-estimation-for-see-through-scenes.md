---
layout: default
title: DepthFocus: Controllable Depth Estimation for See-Through Scenes
---

# DepthFocus: Controllable Depth Estimation for See-Through Scenes

**arXiv**: [2511.16993v1](https://arxiv.org/abs/2511.16993) | [PDF](https://arxiv.org/pdf/2511.16993.pdf)

**ä½œè€…**: Junhong Min, Jimin Kim, Cheol-Hui Min, Minwook Kim, Youngpil Jeon, Minyong Choi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDepthFocuså¯æŽ§æ·±åº¦ä¼°è®¡æ¨¡åž‹ï¼Œä»¥è§£å†³é€è§†åœºæ™¯ä¸­çš„å¤šå±‚æ·±åº¦æ¨¡ç³Šé—®é¢˜ã€‚**

**å…³é”®è¯**: `æ·±åº¦ä¼°è®¡` `å¯æŽ§è§†è§‰` `é€è§†åœºæ™¯` `Vision Transformer` `åˆæˆæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçœŸå®žä¸–ç•Œæ·±åº¦å¤šå±‚é¢ï¼Œé€å°„ææ–™å¯¼è‡´ä¼ ç»Ÿç³»ç»Ÿéš¾ä»¥ä¼°è®¡åŠ¨æ€ç„¦ç‚¹æ·±åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæ·±åº¦åå¥½æ ‡é‡ï¼Œä½¿ç”¨å¯å¼•å¯¼Vision Transformerå®žçŽ°æ„å›¾é©±åŠ¨çš„æ·±åº¦ä¼°è®¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨BOOSTERç­‰åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œå¹¶åœ¨æ–°å¤šæ·±åº¦æ•°æ®é›†ä¸ŠéªŒè¯æ„å›¾å¯¹é½ä¼°è®¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Depth in the real world is rarely singular. Transmissive materials create layered ambiguities that confound conventional perception systems. Existing models remain passive, attempting to estimate static depth maps anchored to the nearest surface, while humans actively shift focus to perceive a desired depth. We introduce DepthFocus, a steerable Vision Transformer that redefines stereo depth estimation as intent-driven control. Conditioned on a scalar depth preference, the model dynamically adapts its computation to focus on the intended depth, enabling selective perception within complex scenes. The training primarily leverages our newly constructed 500k multi-layered synthetic dataset, designed to capture diverse see-through effects. DepthFocus not only achieves state-of-the-art performance on conventional single-depth benchmarks like BOOSTER, a dataset notably rich in transparent and reflective objects, but also quantitatively demonstrates intent-aligned estimation on our newly proposed real and synthetic multi-depth datasets. Moreover, it exhibits strong generalization capabilities on unseen see-through scenes, underscoring its robustness as a significant step toward active and human-like 3D perception.

