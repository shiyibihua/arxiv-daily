---
layout: default
title: EvDiff: High Quality Video with an Event Camera
---

# EvDiff: High Quality Video with an Event Camera

**arXiv**: [2511.17492v1](https://arxiv.org/abs/2511.17492) | [PDF](https://arxiv.org/pdf/2511.17492.pdf)

**ä½œè€…**: Weilun Li, Lei Sun, Ruixi Gao, Qi Jiang, Yuqin Ma, Kaiwei Wang, Ming-Hsuan Yang, Luc Van Gool, Danda Pani Paudel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvDiffæ‰©æ•£æ¨¡åž‹ä»¥ä»Žäº‹ä»¶ç›¸æœºç”Ÿæˆé«˜è´¨é‡å½©è‰²è§†é¢‘**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `æ‰©æ•£æ¨¡åž‹` `è§†é¢‘ç”Ÿæˆ` `ä»£ç†è®­ç»ƒ` `æ—¶é—´ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. äº‹ä»¶ç›¸æœºé‡å»ºå¼ºåº¦å›¾åƒå­˜åœ¨ç»å¯¹äº®åº¦æ¨¡ç³Šçš„ä¸¥é‡ä¸é€‚å®šé—®é¢˜
2. é‡‡ç”¨ä»£ç†è®­ç»ƒæ¡†æž¶å’Œå•æ­¥æ‰©æ•£æ¨¡åž‹ï¼Œç»“åˆæ—¶é—´ä¸€è‡´ç¼–ç å™¨é™ä½Žè®¡ç®—æˆæœ¬
3. åœ¨çœŸå®žæ•°æ®é›†ä¸Šï¼Œåƒç´ çº§å’Œæ„ŸçŸ¥æŒ‡æ ‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹³è¡¡ä¿çœŸåº¦ä¸ŽçœŸå®žæ„Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.

