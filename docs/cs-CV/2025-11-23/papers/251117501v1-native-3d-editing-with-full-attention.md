---
layout: default
title: Native 3D Editing with Full Attention
---

# Native 3D Editing with Full Attention

**arXiv**: [2511.17501v1](https://arxiv.org/abs/2511.17501) | [PDF](https://arxiv.org/pdf/2511.17501.pdf)

**ä½œè€…**: Weiwei Cai, Shuangkang Fang, Weicai Ye, Xin Dong, Yunhan Yang, Xuanyang Zhang, Wei Cheng, Yanpei Cao, Gang Yu, Tao Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŽŸç”Ÿ3Dç¼–è¾‘æ¡†æž¶ä»¥è§£å†³çŽ°æœ‰æ–¹æ³•é€Ÿåº¦æ…¢å’Œå‡ ä½•ä¸ä¸€è‡´é—®é¢˜**

**å…³é”®è¯**: `3Dç¼–è¾‘` `æŒ‡ä»¤å¼•å¯¼` `å¤šæ¨¡æ€æ•°æ®é›†` `ä»¤ç‰Œæ‹¼æŽ¥` `å‡ ä½•ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰3Dç¼–è¾‘æ–¹æ³•ä¼˜åŒ–æ…¢æˆ–2Dæå‡å¯¼è‡´å‡ ä½•ä¸ä¸€è‡´å’Œè´¨é‡ä¸‹é™
2. æž„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒæŽ¢ç´¢äº¤å‰æ³¨æ„åŠ›å’Œ3Dä»¤ç‰Œæ‹¼æŽ¥ç­–ç•¥
3. å®žéªŒæ˜¾ç¤ºä»¤ç‰Œæ‹¼æŽ¥æ›´é«˜æ•ˆï¼Œåœ¨è´¨é‡ã€ä¸€è‡´æ€§å’ŒæŒ‡ä»¤å¿ å®žåº¦ä¸Šé¢†å…ˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.

