---
layout: default
title: SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors
---

# SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors

**arXiv**: [2511.17207v1](https://arxiv.org/abs/2511.17207) | [PDF](https://arxiv.org/pdf/2511.17207.pdf)

**ä½œè€…**: Kunyi Li, Michael Niemeyer, Sen Wang, Stefano Gasperini, Nassir Navab, Federico Tombari

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSING3R-SLAMä»¥è§£å†³å•ç›®SLAMä¸­çš„æ¼‚ç§»å’Œå†—ä½™é—®é¢˜ï¼Œå®žçŽ°é«˜æ•ˆ3Dé‡å»ºã€‚**

**å…³é”®è¯**: `å•ç›®SLAM` `3Dé‡å»º` `é«˜æ–¯è¡¨ç¤º` `å­å›¾èžåˆ` `æ¼‚ç§»æ ¡æ­£` `æ–°é¢–è§†å›¾æ¸²æŸ“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•ç›®SLAMä¸­æ¼‚ç§»å’Œå†—ä½™ç‚¹äº‘é™åˆ¶æ•ˆçŽ‡å’Œä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå±€éƒ¨ä¸€è‡´å­å›¾ä¸Žå…¨å±€é«˜æ–¯è¡¨ç¤ºï¼Œè”åˆä¼˜åŒ–å‡ ä½•å’Œç›¸æœºä½å§¿ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šå®žçŽ°SOTAè·Ÿè¸ªã€é‡å»ºå’Œæ¸²æŸ“ï¼Œè·Ÿè¸ªç²¾åº¦æå‡è¶…12%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis. To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework. The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications. SING3R-SLAM first builds locally consistent submaps through our lightweight tracking and reconstruction module, and then progressively aligns and fuses them into a global Gaussian map that enforces cross-view geometric consistency. This global map, in turn, provides feedback to correct local drift and enhance the robustness of tracking. Extensive experiments demonstrate that SING3R-SLAM achieves state-of-the-art tracking, 3D reconstruction, and novel view rendering, resulting in over 12% improvement in tracking and producing finer, more detailed geometry, all while maintaining a compact and memory-efficient global representation on real-world datasets.

