---
layout: default
title: AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models
---

# AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12715" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12715v1</a>
  <a href="https://arxiv.org/pdf/2509.12715.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12715v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.12715v1', 'AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heng Zhang, Haichuan Hu, Yaomin Shen, Weihao Yu, Yilei Yuan, Haochen You, Guo Cheng, Zijian Zhang, Lubin Gan, Huihui Wei, Hao Zhang, Jin Huang

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AsyMoEï¼šåˆ©ç”¨æ¨¡æ€ä¸å¯¹ç§°æ€§å¢å¼ºå¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„ä¸“å®¶ç‰¹åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ··åˆä¸“å®¶` `æ¨¡æ€ä¸å¯¹ç§°æ€§` `è·¨æ¨¡æ€äº¤äº’` `ä¸“å®¶ç‰¹åŒ–` `åŒæ›²ç©ºé—´` `è·¯ç”±æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MoEæ¨¡å‹åœ¨å¤„ç†è§†è§‰-è¯­è¨€ä»»åŠ¡æ—¶ï¼Œç”±äºè§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„ä¸å¯¹ç§°æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆå¹³è¡¡æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œè·¨æ¨¡æ€äº¤äº’ã€‚
2. AsyMoEé€šè¿‡å¼•å…¥ä¸‰ç§ä¸“å®¶ç»„ï¼šæ¨¡æ€å†…ä¸“å®¶ã€åŒæ›²æ¨¡æ€é—´ä¸“å®¶å’Œè¯æ®ä¼˜å…ˆè¯­è¨€ä¸“å®¶ï¼Œæ¥å»ºæ¨¡è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„ä¸å¯¹ç§°æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAsyMoEåœ¨å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸMoEå’Œæ¨¡æ€ç‰¹å®šMoEï¼ŒåŒæ—¶å‡å°‘äº†æ¿€æ´»å‚æ•°çš„æ•°é‡ï¼Œæå‡äº†æ¨¡å‹æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹(LVLMs)é€šè¿‡æ‰©å±•æ¶æ„å’Œå¹¿æ³›çš„è®­ç»ƒï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ··åˆä¸“å®¶(MoE)æ–¹æ³•ç”±äºè§†è§‰å’Œè¯­è¨€å¤„ç†ä¹‹é—´çš„ä¸å¯¹ç§°æ€§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚è§†è§‰ä¿¡æ¯åœ¨ç©ºé—´ä¸Šæ˜¯å®Œæ•´çš„ï¼Œè€Œè¯­è¨€éœ€è¦ç»´æŠ¤åºåˆ—ä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼ŒMoEæ¨¡å‹éš¾ä»¥å¹³è¡¡æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œè·¨æ¨¡æ€äº¤äº’ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ·±å±‚ä¸­çš„è¯­è¨€ä¸“å®¶é€æ¸å¤±å»ä¸Šä¸‹æ–‡åŸºç¡€ï¼Œæ›´å¤šåœ°ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œè€Œä¸æ˜¯åˆ©ç”¨æ‰€æä¾›çš„è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„AsyMoEï¼Œå®ƒä½¿ç”¨ä¸‰ä¸ªä¸“é—¨çš„ä¸“å®¶ç»„æ¥æ¨¡æ‹Ÿè¿™ç§ä¸å¯¹ç§°æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ç”¨äºæ¨¡æ€ç‰¹å®šå¤„ç†çš„æ¨¡æ€å†…ä¸“å®¶ï¼Œç”¨äºåˆ†å±‚è·¨æ¨¡æ€äº¤äº’çš„åŒæ›²æ¨¡æ€é—´ä¸“å®¶ï¼Œä»¥åŠç”¨äºæŠ‘åˆ¶å‚æ•°åå·®å’Œä¿æŒä¸Šä¸‹æ–‡åŸºç¡€çš„è¯æ®ä¼˜å…ˆè¯­è¨€ä¸“å®¶ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒAsyMoEæ¯”vanilla MoEå’Œæ¨¡æ€ç‰¹å®šMoEåˆ†åˆ«å®ç°äº†26.58%å’Œ15.45%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶ä¸”æ¯”å¯†é›†æ¨¡å‹å‡å°‘äº†25.45%çš„æ¿€æ´»å‚æ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ï¼Œç”±äºè§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´å­˜åœ¨æœ¬è´¨ä¸Šçš„ä¸å¯¹ç§°æ€§ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æœ‰æ•ˆåœ°å­¦ä¹ å’Œåˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ä»¥åŠè¿›è¡Œè·¨æ¨¡æ€äº¤äº’ã€‚è§†è§‰ä¿¡æ¯é€šå¸¸æ˜¯ç©ºé—´ä¸Šå®Œæ•´çš„ï¼Œè€Œè¯­è¨€ä¿¡æ¯åˆ™éœ€è¦ç»´æŠ¤åºåˆ—ä¸Šä¸‹æ–‡ï¼Œè¿™ä½¿å¾—æ¨¡å‹éš¾ä»¥å¹³è¡¡ä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAsyMoEçš„æ ¸å¿ƒæ€è·¯æ˜¯æ˜¾å¼åœ°å»ºæ¨¡è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„ä¸å¯¹ç§°æ€§ã€‚é€šè¿‡è®¾è®¡ä¸‰ç§ä¸åŒç±»å‹çš„ä¸“å®¶ç»„ï¼Œåˆ†åˆ«å¤„ç†æ¨¡æ€å†…ä¿¡æ¯ã€æ¨¡æ€é—´äº¤äº’ä»¥åŠè¯­è¨€ä¿¡æ¯çš„ä¸Šä¸‹æ–‡ä¿æŒï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒæ¨¡æ€çš„ç‰¹æ€§ï¼Œå¹¶æå‡æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAsyMoEçš„æ•´ä½“æ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦çš„ä¸“å®¶ç»„ï¼š
1. **æ¨¡æ€å†…ä¸“å®¶**ï¼šä¸“æ³¨äºå¤„ç†ç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼Œä¾‹å¦‚è§†è§‰ç‰¹å¾æå–å’Œè¯­è¨€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚
2. **åŒæ›²æ¨¡æ€é—´ä¸“å®¶**ï¼šè´Ÿè´£è¿›è¡Œåˆ†å±‚çš„è·¨æ¨¡æ€äº¤äº’ï¼Œåˆ©ç”¨åŒæ›²ç©ºé—´çš„ç‰¹æ€§æ¥æ›´å¥½åœ°è¡¨ç¤ºä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚
3. **è¯æ®ä¼˜å…ˆè¯­è¨€ä¸“å®¶**ï¼šæ—¨åœ¨æŠ‘åˆ¶è¯­è¨€ä¸“å®¶å¯¹å‚æ•°çŸ¥è¯†çš„è¿‡åº¦ä¾èµ–ï¼Œå¹¶ä¿æŒè¯­è¨€ä¿¡æ¯çš„ä¸Šä¸‹æ–‡åŸºç¡€ã€‚
æ¨¡å‹é€šè¿‡è·¯ç”±æœºåˆ¶å°†è¾“å…¥åˆ†é…ç»™ä¸åŒçš„ä¸“å®¶ç»„è¿›è¡Œå¤„ç†ï¼Œæœ€ç»ˆèåˆå„ä¸ªä¸“å®¶çš„è¾“å‡ºï¼Œå®Œæˆè§†è§‰-è¯­è¨€ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šAsyMoEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹æ¨¡æ€ä¸å¯¹ç§°æ€§çš„æ˜¾å¼å»ºæ¨¡ä»¥åŠä¸‰ç§ä¸“å®¶ç»„çš„ååŒå·¥ä½œã€‚ä¸ä¼ ç»Ÿçš„MoEæ–¹æ³•ç›¸æ¯”ï¼ŒAsyMoEèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å·®å¼‚ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚åŒæ›²æ¨¡æ€é—´ä¸“å®¶çš„å¼•å…¥ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¡¨ç¤ºä¸åŒæ¨¡æ€ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **ä¸“å®¶ç»„æ•°é‡å’Œç±»å‹**ï¼šé€‰æ‹©ä¸‰ç§ä¸“å®¶ç»„ï¼Œåˆ†åˆ«å¤„ç†æ¨¡æ€å†…ä¿¡æ¯ã€æ¨¡æ€é—´äº¤äº’å’Œè¯­è¨€ä¸Šä¸‹æ–‡ã€‚
2. **åŒæ›²ç©ºé—´çš„ä½¿ç”¨**ï¼šåˆ©ç”¨åŒæ›²ç©ºé—´æ¥è¡¨ç¤ºæ¨¡æ€é—´å…³ç³»ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„ç½‘ç»œç»“æ„ã€‚
3. **è·¯ç”±æœºåˆ¶**ï¼šè®¾è®¡æœ‰æ•ˆçš„è·¯ç”±æœºåˆ¶ï¼Œå°†è¾“å…¥åˆ†é…ç»™åˆé€‚çš„ä¸“å®¶ç»„ã€‚
4. **æŸå¤±å‡½æ•°**ï¼šä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚äº¤å‰ç†µæŸå¤±æˆ–å¯¹æ¯”å­¦ä¹ æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AsyMoEåœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸vanilla MoEç›¸æ¯”ï¼ŒAsyMoEçš„å‡†ç¡®ç‡æå‡äº†26.58%ï¼Œä¸æ¨¡æ€ç‰¹å®šMoEç›¸æ¯”ï¼Œå‡†ç¡®ç‡æå‡äº†15.45%ã€‚åŒæ—¶ï¼ŒAsyMoEæ¯”å¯†é›†æ¨¡å‹å‡å°‘äº†25.45%çš„æ¿€æ´»å‚æ•°ï¼Œè¡¨æ˜å…¶å…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚è¿™äº›å®éªŒç»“æœå……åˆ†è¯æ˜äº†AsyMoEçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AsyMoEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼ŒAsyMoEå¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸï¼Œä¸ºäººä»¬æä¾›æ›´æ™ºèƒ½ã€æ›´ä¾¿æ·çš„æœåŠ¡ã€‚æœªæ¥ï¼ŒAsyMoEè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.

