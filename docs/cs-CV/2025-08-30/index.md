---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-30
---

# cs.CVï¼ˆ2025-08-30ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250900549v1-a-modality-agnostic-multi-task-foundation-model-for-human-brain-imag.html">A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging</a></td>
  <td>æå‡ºBrainFMä»¥è§£å†³è„‘éƒ¨æˆåƒå¤šæ¨¡æ€æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00549v1" data-paper-url="./papers/250900549v1-a-modality-agnostic-multi-task-foundation-model-for-human-brain-imag.html" onclick="toggleFavorite(this, '2509.00549v1', 'A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250900419v1-lightvlm-acceleraing-large-multimodal-models-with-pyramid-token-merg.html">LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression</a></td>
  <td>æå‡ºLightVLMä»¥åŠ é€Ÿå¤šæ¨¡æ€æ¨¡å‹æ¨ç†è¿‡ç¨‹</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00419v1" data-paper-url="./papers/250900419v1-lightvlm-acceleraing-large-multimodal-models-with-pyramid-token-merg.html" onclick="toggleFavorite(this, '2509.00419v1', 'LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250900374v1-adaptive-point-prompt-tuning-fine-tuning-heterogeneous-foundation-mo.html">Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis</a></td>
  <td>æå‡ºè‡ªé€‚åº”ç‚¹æç¤ºè°ƒä¼˜æ–¹æ³•ä»¥è§£å†³3Dç‚¹äº‘åˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00374v1" data-paper-url="./papers/250900374v1-adaptive-point-prompt-tuning-fine-tuning-heterogeneous-foundation-mo.html" onclick="toggleFavorite(this, '2509.00374v1', 'Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250900367v3-a-multimodal-and-multi-centric-head-and-neck-cancer-dataset-for-segm.html">A Multimodal and Multi-centric Head and Neck Cancer Dataset for Segmentation, Diagnosis and Outcome Prediction</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å¤´é¢ˆç™Œæ•°æ®é›†ä»¥ä¿ƒè¿›è‚¿ç˜¤åˆ†å‰²ä¸é¢„åé¢„æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00367v3" data-paper-url="./papers/250900367v3-a-multimodal-and-multi-centric-head-and-neck-cancer-dataset-for-segm.html" onclick="toggleFavorite(this, '2509.00367v3', 'A Multimodal and Multi-centric Head and Neck Cancer Dataset for Segmentation, Diagnosis and Outcome Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250900357v1-surgllm-a-versatile-large-multimodal-model-with-spatial-focus-and-te.html">SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding</a></td>
  <td>æå‡ºSurgLLMä»¥è§£å†³å¤–ç§‘è§†é¢‘ç†è§£ä¸­çš„ç©ºé—´å’Œæ—¶é—´æ„ŸçŸ¥ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00357v1" data-paper-url="./papers/250900357v1-surgllm-a-versatile-large-multimodal-model-with-spatial-focus-and-te.html" onclick="toggleFavorite(this, '2509.00357v1', 'SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250900320v2-trimtokenator-towards-adaptive-visual-token-pruning-for-large-multim.html">TrimTokenator: Towards Adaptive Visual Token Pruning for Large Multimodal Models</a></td>
  <td>æå‡ºè§†è§‰ä»¤ç‰Œä¿®å‰ªç­–ç•¥ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00320v2" data-paper-url="./papers/250900320v2-trimtokenator-towards-adaptive-visual-token-pruning-for-large-multim.html" onclick="toggleFavorite(this, '2509.00320v2', 'TrimTokenator: Towards Adaptive Visual Token Pruning for Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250900371v1-two-causes-not-one-rethinking-omission-and-fabrication-hallucination.html">Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs</a></td>
  <td>æå‡ºè§†è§‰æ½œåŠ›åœºæ ¡å‡†ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00371v1" data-paper-url="./papers/250900371v1-two-causes-not-one-rethinking-omission-and-fabrication-hallucination.html" onclick="toggleFavorite(this, '2509.00371v1', 'Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250905321v1-a-dataset-generation-scheme-based-on-video2eeg-spgn-diffusion-for-se.html">A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD</a></td>
  <td>æå‡ºVideo2EEG-SPGN-Diffusionä»¥ç”Ÿæˆè§†é¢‘åˆºæ¿€ä¸‹çš„EEGæ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05321v1" data-paper-url="./papers/250905321v1-a-dataset-generation-scheme-based-on-video2eeg-spgn-diffusion-for-se.html" onclick="toggleFavorite(this, '2509.05321v1', 'A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250900373v1-activation-steering-meets-preference-optimization-defense-against-ja.html">Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</a></td>
  <td>æå‡ºSPO-VLMä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00373v1" data-paper-url="./papers/250900373v1-activation-steering-meets-preference-optimization-defense-against-ja.html" onclick="toggleFavorite(this, '2509.00373v1', 'Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250900484v1-videorewardbench-comprehensive-evaluation-of-multimodal-reward-model.html">VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</a></td>
  <td>æå‡ºVideoRewardBenchä»¥è§£å†³è§†é¢‘ç†è§£ä¸­å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è¯„ä¼°ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00484v1" data-paper-url="./papers/250900484v1-videorewardbench-comprehensive-evaluation-of-multimodal-reward-model.html" onclick="toggleFavorite(this, '2509.00484v1', 'VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250900442v2-semamil-semantic-aware-multiple-instance-learning-with-retrieval-gui.html">SemaMIL: Semantic-Aware Multiple Instance Learning with Retrieval-Guided State Space Modeling for Whole Slide Images</a></td>
  <td>æå‡ºSemaMILä»¥è§£å†³å…¨åˆ‡ç‰‡å›¾åƒä¸­çš„å¤šå®ä¾‹å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00442v2" data-paper-url="./papers/250900442v2-semamil-semantic-aware-multiple-instance-learning-with-retrieval-gui.html" onclick="toggleFavorite(this, '2509.00442v2', 'SemaMIL: Semantic-Aware Multiple Instance Learning with Retrieval-Guided State Space Modeling for Whole Slide Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250900311v1-morphgen-morphology-guided-representation-learning-for-robust-single.html">MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</a></td>
  <td>æå‡ºMorphGenä»¥è§£å†³ç»„ç»‡ç—…ç†å­¦ç™Œç—‡åˆ†ç±»ä¸­çš„é¢†åŸŸæ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00311v1" data-paper-url="./papers/250900311v1-morphgen-morphology-guided-representation-learning-for-robust-single.html" onclick="toggleFavorite(this, '2509.00311v1', 'MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250900509v1-make-me-an-expert-distilling-from-generalist-black-box-models-into-s.html">Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation</a></td>
  <td>æå‡ºé»‘ç®±è’¸é¦æ–¹æ³•ä»¥è§£å†³å±€éƒ¨æ¨¡å‹è®­ç»ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00509v1" data-paper-url="./papers/250900509v1-make-me-an-expert-distilling-from-generalist-black-box-models-into-s.html" onclick="toggleFavorite(this, '2509.00509v1', 'Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250905319v1-context-aware-knowledge-distillation-with-adaptive-weighting-for-ima.html">Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification</a></td>
  <td>æå‡ºè‡ªé€‚åº”çŸ¥è¯†è’¸é¦æ¡†æ¶ä»¥ä¼˜åŒ–å›¾åƒåˆ†ç±»æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05319v1" data-paper-url="./papers/250905319v1-context-aware-knowledge-distillation-with-adaptive-weighting-for-ima.html" onclick="toggleFavorite(this, '2509.05319v1', 'Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250900346v1-lut-fuse-towards-extremely-fast-infrared-and-visible-image-fusion-vi.html">LUT-Fuse: Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables</a></td>
  <td>æå‡ºLUT-Fuseä»¥è§£å†³å®æ—¶çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00346v1" data-paper-url="./papers/250900346v1-lut-fuse-towards-extremely-fast-infrared-and-visible-image-fusion-vi.html" onclick="toggleFavorite(this, '2509.00346v1', 'LUT-Fuse: Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250900490v2-multi-focused-video-group-activities-hashing.html">Multi-Focused Video Group Activities Hashing</a></td>
  <td>æå‡ºå¤šèšç„¦è§†é¢‘ç»„æ´»åŠ¨å“ˆå¸ŒæŠ€æœ¯ä»¥è§£å†³è§†é¢‘æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00490v2" data-paper-url="./papers/250900490v2-multi-focused-video-group-activities-hashing.html" onclick="toggleFavorite(this, '2509.00490v2', 'Multi-Focused Video Group Activities Hashing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250900353v1-aqfusionnet-multimodal-deep-learning-for-air-quality-index-predictio.html">AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data</a></td>
  <td>æå‡ºAQFusionNetä»¥è§£å†³èµ„æºå—é™åœ°åŒºç©ºæ°”è´¨é‡ç›‘æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sparse sensors</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00353v1" data-paper-url="./papers/250900353v1-aqfusionnet-multimodal-deep-learning-for-air-quality-index-predictio.html" onclick="toggleFavorite(this, '2509.00353v1', 'AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250900385v1-hero-vql-hierarchical-egocentric-and-robust-visual-query-localizatio.html">HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization</a></td>
  <td>æå‡ºHERO-VQLä»¥è§£å†³è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„è§†è§‰æŸ¥è¯¢å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00385v1" data-paper-url="./papers/250900385v1-hero-vql-hierarchical-egocentric-and-robust-visual-query-localizatio.html" onclick="toggleFavorite(this, '2509.00385v1', 'HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250900527v1-learning-yourself-class-incremental-semantic-segmentation-with-langu.html">Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement</a></td>
  <td>æå‡ºè¯­è¨€å¯å‘çš„è‡ªæˆ‘å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¢é‡è¯­ä¹‰åˆ†å‰²ä¸­çš„ç¾éš¾æ€§è¯­ä¹‰çº ç¼ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00527v1" data-paper-url="./papers/250900527v1-learning-yourself-class-incremental-semantic-segmentation-with-langu.html" onclick="toggleFavorite(this, '2509.00527v1', 'Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250900598v2-dgl-rsis-decoupling-global-spatial-context-and-local-class-semantics.html">DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation</a></td>
  <td>æå‡ºDGL-RSISä»¥è§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„è®­ç»ƒéœ€æ±‚é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00598v2" data-paper-url="./papers/250900598v2-dgl-rsis-decoupling-global-spatial-context-and-local-class-semantics.html" onclick="toggleFavorite(this, '2509.00598v2', 'DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250900451v2-encoder-only-image-registration.html">Encoder-Only Image Registration</a></td>
  <td>æå‡ºEncoder-Onlyå›¾åƒé…å‡†æ¡†æ¶ä»¥è§£å†³è®¡ç®—å¤æ‚æ€§ä¸å¤§å˜å½¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00451v2" data-paper-url="./papers/250900451v2-encoder-only-image-registration.html" onclick="toggleFavorite(this, '2509.00451v2', 'Encoder-Only Image Registration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250900428v1-mixture-of-global-and-local-experts-with-diffusion-transformer-for-c.html">Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation</a></td>
  <td>æå‡ºFace-MoGLEä»¥è§£å†³å¯æ§äººè„¸ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.00428v1" data-paper-url="./papers/250900428v1-mixture-of-global-and-local-experts-with-diffusion-transformer-for-c.html" onclick="toggleFavorite(this, '2509.00428v1', 'Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)