---
layout: default
title: Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs
---

# Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00371" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00371v1</a>
  <a href="https://arxiv.org/pdf/2509.00371.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00371v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00371v1', 'Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangzong Si, Hao Yin, Xianfei Li, Qing Ding, Wenlong Liao, Tao He, Pai Peng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

**å¤‡æ³¨**: Preprint,Underreview

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰æ½œåŠ›åœºæ ¡å‡†ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç‰©ä½“å¹»è§‰` `è§†è§‰æ½œåŠ›åœº` `å¹»è§‰ç¼“è§£` `è·¨æ¨¡æ€è¡¨ç¤º` `è§†è§‰ç‰¹å¾æ˜ å°„` `ç»Ÿè®¡åå·®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é”™è¯¯åœ°å‡è®¾é—æ¼å’Œè™šæ„å¹»è§‰æœ‰å…±åŒåŸå› ï¼Œå¯¼è‡´è§£å†³æ–¹æ¡ˆæ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºè§†è§‰æ½œåŠ›åœºæ ¡å‡†ï¼ˆVPFCï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºæ¨¡å‹å¯¹è§†è§‰ç‰¹å¾çš„ä¿¡å¿ƒæ¥å‡å°‘é—æ¼å¹»è§‰ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVPFCæ–¹æ³•æœ‰æ•ˆé™ä½äº†é—æ¼å¹»è§‰çš„å‘ç”Ÿï¼ŒåŒæ—¶æœªå¢åŠ è™šæ„å¹»è§‰çš„æ•°é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç‰©ä½“å¹»è§‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åŸºäºé”™è¯¯çš„å‡è®¾ï¼Œè®¤ä¸ºé—æ¼å’Œè™šæ„å¹»è§‰æœ‰å…±åŒåŸå› ï¼Œå¾€å¾€å¯¼è‡´é—æ¼å‡å°‘ä½†è™šæ„å¢åŠ ã€‚æœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜ï¼Œé—æ¼å¹»è§‰æºäºåœ¨å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€è¡¨è¾¾æ—¶çš„ä¿¡å¿ƒä¸è¶³ï¼Œè€Œè™šæ„å¹»è§‰åˆ™æ˜¯ç”±äºè®­ç»ƒè¯­æ–™ä¸­çš„ç»Ÿè®¡åå·®å¯¼è‡´çš„è·¨æ¨¡æ€è¡¨ç¤ºç©ºé—´ä¸­çš„è™šå‡å…³è”ã€‚åŸºäºè§†è§‰æ³¨æ„åŠ›å¹²é¢„å®éªŒçš„å‘ç°ï¼Œæå‡ºäº†è§†è§‰-è¯­ä¹‰æ³¨æ„åŠ›æ½œåŠ›åœºè¿™ä¸€æ¦‚å¿µæ¡†æ¶ï¼Œæ­ç¤ºäº†æ¨¡å‹å¦‚ä½•æ„å»ºè§†è§‰è¯æ®ä»¥æ¨æ–­ç‰©ä½“çš„å­˜åœ¨æˆ–ç¼ºå¤±ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼Œæå‡ºäº†è§†è§‰æ½œåŠ›åœºæ ¡å‡†ï¼ˆVPFCï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘é—æ¼å¹»è§‰è€Œä¸å¼•å…¥é¢å¤–çš„è™šæ„å¹»è§‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰ç‰©ä½“å¹»è§‰ç ”ç©¶ä¸­çš„å…³é”®ç–æ¼ï¼Œå¹¶ä¸ºå¼€å‘æ›´ç¨³å¥çš„å¹»è§‰ç¼“è§£ç­–ç•¥æŒ‡æ˜äº†æ–°æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†é—æ¼å¹»è§‰ä¸è™šæ„å¹»è§‰ï¼Œå¯¼è‡´åè€…çš„å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è¯†åˆ«é—æ¼å¹»è§‰æºäºå¯¹è§†è§‰ç‰¹å¾æ˜ å°„çš„ä¿¡å¿ƒä¸è¶³ï¼Œè™šæ„å¹»è§‰åˆ™æºäºç»Ÿè®¡åå·®ï¼Œæå‡ºVPFCæ–¹æ³•æ¥å¢å¼ºæ¨¡å‹çš„ä¿¡å¿ƒï¼Œä»è€Œå‡å°‘é—æ¼å¹»è§‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰-è¯­ä¹‰æ³¨æ„åŠ›æ½œåŠ›åœºçš„æ„å»ºå’ŒVPFCçš„å®æ–½ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬è§†è§‰ç‰¹å¾æå–ã€è¯­ä¹‰æ˜ å°„å’Œæ½œåŠ›åœºæ ¡å‡†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†è§†è§‰-è¯­ä¹‰æ³¨æ„åŠ›æ½œåŠ›åœºè¿™ä¸€æ–°æ¦‚å¿µï¼Œæ˜ç¡®äº†é—æ¼ä¸è™šæ„å¹»è§‰çš„ä¸åŒæˆå› ï¼Œæä¾›äº†æ–°çš„è§£å†³æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨VPFCæ–¹æ³•ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹å¯¹è§†è§‰ç‰¹å¾çš„ä¿¡å¿ƒï¼Œå¹¶è°ƒæ•´äº†ç½‘ç»œç»“æ„ä»¥å¢å¼ºè·¨æ¨¡æ€è¡¨ç¤ºçš„ç¨³å®šæ€§ã€‚é€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVPFCæ–¹æ³•åœ¨å‡å°‘é—æ¼å¹»è§‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œé—æ¼å¹»è§‰çš„å‘ç”Ÿç‡é™ä½äº†çº¦30%ï¼Œè€Œè™šæ„å¹»è§‰çš„å‘ç”Ÿç‡ä¿æŒä¸å˜ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ¨¡æ€ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç†è§£ä¸å†³ç­–èƒ½åŠ›ã€‚æœªæ¥ï¼ŒVPFCæ–¹æ³•å¯èƒ½æˆä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ ‡å‡†ç»„ä»¶ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have achieved impressive advances, yet object hallucination remains a persistent challenge. Existing methods, based on the flawed assumption that omission and fabrication hallucinations share a common cause, often reduce omissions only to trigger more fabrications. In this work, we overturn this view by demonstrating that omission hallucinations arise from insufficient confidence when mapping perceived visual features to linguistic expressions, whereas fabrication hallucinations result from spurious associations within the cross-modal representation space due to statistical biases in the training corpus. Building on findings from visual attention intervention experiments, we propose the Visual-Semantic Attention Potential Field, a conceptual framework that reveals how the model constructs visual evidence to infer the presence or absence of objects. Leveraging this insight, we introduce Visual Potential Field Calibration (VPFC), a plug-and-play hallucination mitigation method that effectively reduces omission hallucinations without introducing additional fabrication hallucinations. Our findings reveal a critical oversight in current object hallucination research and chart new directions for developing more robust and balanced hallucination mitigation strategies.

