---
layout: default
title: LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression
---

# LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00419" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.00419v1</a>
  <a href="https://arxiv.org/pdf/2509.00419.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00419v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00419v1', 'LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Lianyu Hu, Fanhua Shang, Wei Feng, Liang Wan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-30

**Â§áÊ≥®**: EMNLP2025 Findings

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LightVLM‰ª•Âä†ÈÄüÂ§öÊ®°ÊÄÅÊ®°ÂûãÊé®ÁêÜËøáÁ®ã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊ®°Âûã` `Êé®ÁêÜÂä†ÈÄü` `ÈáëÂ≠óÂ°î‰ª§ÁâåÂêàÂπ∂` `KVÁºìÂ≠òÂéãÁº©` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Ê®°ÂûãÊïàÁéá` `ÈïøÊñáÊú¨ÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Â≠òÂú®ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂Âª∂ËøüËæÉÈ´ò„ÄÇ
2. LightVLMÈÄöËøáÈáëÂ≠óÂ°î‰ª§ÁâåÂêàÂπ∂ÂíåKVÁºìÂ≠òÂéãÁº©ÔºåÂàÜÂà´Âú®ÁºñÁ†ÅÂíåËß£Á†ÅÈò∂ÊÆµÂêåÊó∂Âä†ÈÄüÊé®ÁêÜËøáÁ®ãÔºåÊòæËëóÊèêÂçáÊ®°ÂûãÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLightVLMÂú®‰øùÁïôÊûÅÂ∞ëÂõæÂÉè‰ª§ÁâåÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªçËÉΩ‰øùÊåÅÈ´òÊÄßËÉΩÔºåÂπ∂Âú®ÈïøÊñáÊú¨ÁîüÊàê‰∏≠ÊòæËëóÂáèÂ∞ëÊé®ÁêÜÊó∂Èó¥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Êñá‰ªãÁªç‰∫ÜLightVLMÔºå‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Êó†ÁºùÈÉ®ÁΩ≤Âú®Áé∞ÊúâÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏äÔºå‰ª•ÊòæËëóÂä†ÈÄüÊé®ÁêÜËøáÁ®ãÔºå‰∏îÊó†ÈúÄËÆ≠ÁªÉ„ÄÇÊàë‰ª¨Â∞ÜVLMÁöÑÊé®ÁêÜËøáÁ®ãÂàÜ‰∏∫ÁºñÁ†ÅÂíåËß£Á†Å‰∏§‰∏™Èò∂ÊÆµÔºåÂπ∂ÊèêÂá∫ÂêåÊó∂Âä†ÈÄüËøô‰∏§‰∏™Èò∂ÊÆµ‰ª•ÊèêÈ´òÊ®°ÂûãÊïàÁéá„ÄÇÂú®ÁºñÁ†ÅÈò∂ÊÆµÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈáëÂ≠óÂ°î‰ª§ÁâåÂêàÂπ∂ÔºåÈÄöËøáÂàÜÂ±ÇÊñπÂºèÂáèÂ∞ë‰∏çÂêåLLMÂ±ÇÁöÑ‰ª§ÁâåÔºåÊúÄÁªà‰ªÖ‰øùÁïôÂ∞ëÈáè‰∏ªÂØº‰ª§Áâå‰ª•ÂÆûÁé∞È´òÊïà„ÄÇÂú®Ëß£Á†ÅÈò∂ÊÆµÔºå‰∏∫‰∫ÜÈôç‰ΩéËæìÂá∫ÈïøÂ∫èÂàóÁöÑÈ´òÂª∂ËøüÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜKVÁºìÂ≠òÂéãÁº©ÔºåÂéªÈô§‰∏çÂøÖË¶ÅÁöÑÁºìÂ≠ò‰ª•ÊèêÈ´òÁΩëÁªúÂêûÂêêÈáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLightVLMÂú®‰ªÖ‰øùÁïô35%ÂõæÂÉè‰ª§ÁâåÊó∂ÊàêÂäü‰øùÊåÅ100%ÁöÑÊÄßËÉΩÔºåËÄåÂú®‰ªÖ‰øùÁïô3%ÂõæÂÉè‰ª§ÁâåÊó∂‰øùÊåÅÁ∫¶98%ÁöÑÊÄßËÉΩ„ÄÇLightVLMËÉΩÂ§üÂ∞ÜÁΩëÁªúÂêûÂêêÈáèÊèêÈ´ò2.02ÂÄçÔºåÂπ∂Â∞ÜÈ¢ÑÂ°´ÂÖÖÊó∂Èó¥ÂáèÂ∞ë3.65ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÁöÑÈ´òÂª∂ËøüÂíåËµÑÊ∫êÊ∂àËÄó„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Êú™ËÉΩÊúâÊïàÂà©Áî®‰ª§Áâå‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩÁì∂È¢à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLightVLMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÈáëÂ≠óÂ°î‰ª§ÁâåÂêàÂπ∂ÂíåKVÁºìÂ≠òÂéãÁº©ÔºåÂàÜÂà´Âú®ÁºñÁ†ÅÂíåËß£Á†ÅÈò∂ÊÆµ‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºå‰ª•ÊèêÈ´òÊï¥‰ΩìÊ®°ÂûãÁöÑÊïàÁéáÂíåÂìçÂ∫îÈÄüÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLightVLMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁºñÁ†ÅÈò∂ÊÆµÂíåËß£Á†ÅÈò∂ÊÆµ„ÄÇÂú®ÁºñÁ†ÅÈò∂ÊÆµÔºåÈÄöËøáÈáëÂ≠óÂ°îÁªìÊûÑÈÄêÂ±ÇÂêàÂπ∂‰ª§ÁâåÔºõÂú®Ëß£Á†ÅÈò∂ÊÆµÔºåÈÄöËøáÂéãÁº©KVÁºìÂ≠òÊù•ÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÂíåÂ≠òÂÇ®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLightVLMÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈáëÂ≠óÂ°î‰ª§ÁâåÂêàÂπ∂ÊäÄÊúØÂíåKVÁºìÂ≠òÂéãÁº©Á≠ñÁï•ÔºåËøô‰∏§ËÄÖÁªìÂêà‰ΩøÂæóÊ®°ÂûãÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶Ôºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÊú¨Ë¥®ÁöÑÊïàÁéáÊèêÂçá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÈáëÂ≠óÂ°î‰ª§ÁâåÂêàÂπ∂‰∏≠ÔºåËÆæËÆ°‰∫ÜÂàÜÂ±ÇÂêàÂπ∂Á≠ñÁï•Ôºå‰ª•‰øùÁïôÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑ‰ª§ÁâåÔºõÂú®KVÁºìÂ≠òÂéãÁº©‰∏≠Ôºå‰ºòÂåñ‰∫ÜÁºìÂ≠òÁÆ°ÁêÜÁ≠ñÁï•ÔºåÂéªÈô§‰∫ÜÂÜó‰ΩôÊï∞ÊçÆÔºåÊèêÂçá‰∫ÜÁΩëÁªúÁöÑÂêûÂêêÈáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLightVLMÂú®‰ªÖ‰øùÁïô35%ÂõæÂÉè‰ª§ÁâåÊó∂‰øùÊåÅ100%ÊÄßËÉΩÔºå‰øùÁïô3%ÂõæÂÉè‰ª§ÁâåÊó∂‰ªç‰øùÊåÅÁ∫¶98%ÊÄßËÉΩ„ÄÇÂêåÊó∂ÔºåLightVLMÂ∞ÜÁΩëÁªúÂêûÂêêÈáèÊèêÈ´ò‰∫Ü2.02ÂÄçÔºåÈ¢ÑÂ°´ÂÖÖÊó∂Èó¥ÂáèÂ∞ë‰∫Ü3.65ÂÄçÔºåÈïøÊñáÊú¨ÁîüÊàêÁöÑÊé®ÁêÜÊó∂Èó¥ÂáèÂ∞ë‰∫Ü3.21ÂÄçÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LightVLMÁöÑÁ†îÁ©∂ÊàêÊûúÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂÆûÊó∂Â§ÑÁêÜËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁöÑÂú∫ÊôØÔºåÂ¶ÇÊô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®È©æÈ©∂„ÄÅËßÜÈ¢ëÂàÜÊûêÁ≠âÈ¢ÜÂüü„ÄÇÂÖ∂È´òÊïàÁöÑÊé®ÁêÜËÉΩÂäõËÉΩÂ§üÊîØÊåÅÊõ¥Â§çÊùÇÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In this paper, we introduce LightVLM, a simple but effective method that can be seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly accelerate the inference process in a training-free manner. We divide the inference procedure of VLMs into two stages, i.e., encoding and decoding, and propose to simultaneously accelerate VLMs in both stages to largely improve model efficiency. During encoding, we propose pyramid token merging to reduce tokens of different LLM layers in a hierarchical manner by finally only keeping a few dominant tokens to achieve high efficiency. During decoding, aimed at reducing the high latency of outputting long sequences, we propose KV Cache compression to remove unnecessary caches to increase the network throughput. Experimental results show that LightVLM successfully retains 100% performance when only preserving 35% image tokens, and maintains around 98% performance when keeping only 3% image tokens. LightVLM could 2.02$\times$ the network throughput and reduce the prefilling time by 3.65$\times$. LightVLM also makes large VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to infer faster than significantly smaller models (e.g., InternVL2.5 8B), hopefully facilitating the real-world deployment. When generating long text sequences (e.g., 4096 tokens), LightVLM could reduce the inference time by 3.21$\times$, largely outperforming existing methods.

