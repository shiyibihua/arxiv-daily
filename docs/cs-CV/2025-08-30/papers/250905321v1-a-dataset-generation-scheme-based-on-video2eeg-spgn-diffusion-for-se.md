---
layout: default
title: A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD
---

# A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05321v1</a>
  <a href="https://arxiv.org/pdf/2509.05321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05321v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05321v1', 'A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunfei Guo, Tao Zhang, Wu Huang, Yao Song

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo2EEG-SPGN-Diffusionä»¥ç”Ÿæˆè§†é¢‘åˆºæ¿€ä¸‹çš„EEGæ•°æ®é›†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ•°æ®é›†` `EEGä¿¡å·ç”Ÿæˆ` `è§†é¢‘åˆºæ¿€` `æƒ…æ„Ÿåˆ†æ` `è„‘æœºæ¥å£` `è‡ªæˆ‘å¯¹å¼ˆå›¾ç½‘ç»œ` `æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†é¢‘åˆºæ¿€ä¸EEGä¿¡å·çš„å¯¹é½ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚
2. è®ºæ–‡æå‡ºçš„Video2EEG-SPGN-Diffusionæ¡†æ¶é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå›¾ç½‘ç»œä¸æ‰©æ•£æ¨¡å‹ç»“åˆï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„EEGä¿¡å·ã€‚
3. æ–°æ•°æ®é›†åŒ…å«1000å¤šä¸ªè§†é¢‘åˆºæ¿€æ ·æœ¬åŠå¯¹åº”çš„62é€šé“EEGä¿¡å·ï¼Œæ˜¾è‘—æ¨åŠ¨äº†å¤šæ¨¡æ€ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¼€æºæ¡†æ¶Video2EEG-SPGN-Diffusionï¼Œè¯¥æ¡†æ¶åˆ©ç”¨SEED-VDæ•°æ®é›†ç”ŸæˆåŸºäºè§†é¢‘åˆºæ¿€çš„å¤šæ¨¡æ€EEGä¿¡å·æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŠ«éœ²äº†ä¸€ç§å¯¹é½è§†é¢‘å’ŒEEGæ•°æ®å¯¹çš„å·¥ç¨‹æµç¨‹ï¼Œä¿ƒè¿›äº†å…·æœ‰EEGå¯¹é½èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå›¾ç½‘ç»œ(SPGN)ä¸æ‰©æ•£æ¨¡å‹çš„ç»“åˆï¼Œç”Ÿæˆä¸ªæ€§åŒ–EEGä¿¡å·ã€‚ä½œä¸ºä¸»è¦è´¡çŒ®ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1000ä¸ªSEED-VDè§†é¢‘åˆºæ¿€æ ·æœ¬ï¼Œé…å¯¹ç”Ÿæˆçš„62é€šé“EEGä¿¡å·ï¼ˆ200 Hzï¼‰åŠæƒ…æ„Ÿæ ‡ç­¾ï¼Œæ¨åŠ¨è§†é¢‘-EEGå¯¹é½åŠå¤šæ¨¡æ€ç ”ç©¶ã€‚è¯¥æ¡†æ¶ä¸ºæƒ…æ„Ÿåˆ†æã€æ•°æ®å¢å¼ºå’Œè„‘æœºæ¥å£åº”ç”¨æä¾›äº†æ–°å·¥å…·ï¼Œå…·æœ‰é‡è¦çš„ç ”ç©¶å’Œå·¥ç¨‹æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘åˆºæ¿€ä¸EEGä¿¡å·å¯¹é½ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®é›†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆè‡ªæˆ‘å¯¹å¼ˆå›¾ç½‘ç»œ(SPGN)ä¸æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„EEGä¿¡å·ï¼Œä»¥æé«˜è§†é¢‘ä¸EEGä¿¡å·çš„å¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†é¢‘åˆºæ¿€è¾“å…¥ã€EEGä¿¡å·ç”Ÿæˆæ¨¡å—å’Œæ•°æ®å¯¹é½å·¥ç¨‹æµç¨‹ï¼Œç¡®ä¿è§†é¢‘ä¸EEGä¿¡å·çš„é«˜æ•ˆé…å¯¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†Video2EEG-SPGN-Diffusionæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„EEGä¿¡å·å¹¶å®ç°è§†é¢‘-EEGå¯¹é½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨62é€šé“EEGä¿¡å·ï¼Œé‡‡æ ·ç‡ä¸º200 Hzï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºé€‚åº”å¤šæ¨¡æ€å¯¹é½ï¼Œç½‘ç»œç»“æ„ç»“åˆäº†SPGNå’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„EEGä¿¡å·ä¸è§†é¢‘åˆºæ¿€çš„å¯¹é½ç²¾åº¦æ˜¾è‘—æé«˜ï¼Œæ•°æ®é›†åŒ…å«è¶…è¿‡1000ä¸ªæ ·æœ¬ï¼Œ62é€šé“EEGä¿¡å·çš„ç”Ÿæˆè´¨é‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€æ•°æ®å¢å¼ºå’Œè„‘æœºæ¥å£ç­‰ã€‚é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œèƒ½å¤Ÿä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œå¹¶ä¸ºå®é™…åº”ç”¨æä¾›æ›´ä¸ºç²¾å‡†çš„æƒ…æ„Ÿè¯†åˆ«å’Œäº¤äº’æ–¹å¼ï¼Œæœªæ¥å¯èƒ½åœ¨åŒ»ç–—ã€å¨±ä¹ç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.

