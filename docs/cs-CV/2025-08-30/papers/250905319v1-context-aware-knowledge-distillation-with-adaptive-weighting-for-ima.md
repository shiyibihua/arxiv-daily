---
layout: default
title: Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification
---

# Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05319v1</a>
  <a href="https://arxiv.org/pdf/2509.05319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05319v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05319v1', 'Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengda Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”çŸ¥è¯†è’¸é¦æ¡†æ¶ä»¥ä¼˜åŒ–å›¾åƒåˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è‡ªé€‚åº”å­¦ä¹ ` `å›¾åƒåˆ†ç±»` `æ·±åº¦å­¦ä¹ ` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä½¿ç”¨å›ºå®šçš„å¹³è¡¡å› å­ï¼Œå¯¼è‡´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— æ³•é€‚åº”ç¡¬æ ‡ç­¾ä¸è½¯æ ‡ç­¾ä¹‹é—´çš„æœ€ä½³æƒè¡¡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å°†å¹³è¡¡å› å­alphaè®¾ä¸ºå¯å­¦ä¹ å‚æ•°ï¼ŒåŠ¨æ€è®¡ç®—ä»¥ä¼˜åŒ–çŸ¥è¯†è½¬ç§»è¿‡ç¨‹ã€‚
3. åœ¨CIFAR-10æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ”¶æ•›ç¨³å®šæ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„å›ºå®šæƒé‡KDæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºå°†å¤§è§„æ¨¡æ•™å¸ˆç½‘ç»œçš„çŸ¥è¯†è½¬ç§»åˆ°è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚ä¼ ç»Ÿçš„KDæ–¹æ³•ä½¿ç”¨å›ºå®šçš„å¹³è¡¡å› å­alphaä½œä¸ºè¶…å‚æ•°ï¼Œå°†ç¡¬æ ‡ç­¾äº¤å‰ç†µæŸå¤±ä¸è½¯æ ‡ç­¾è’¸é¦æŸå¤±ç»“åˆã€‚ç„¶è€Œï¼Œé™æ€çš„alphaåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶éæœ€ä¼˜ï¼Œå› ä¸ºç¡¬ç›‘ç£å’Œè½¯ç›‘ç£ä¹‹é—´çš„æœ€ä½³æƒè¡¡å¯èƒ½ä¼šæœ‰æ‰€å˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çŸ¥è¯†è’¸é¦ï¼ˆAKDï¼‰æ¡†æ¶ï¼Œé¦–å…ˆå°†alphaè®¾ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å­¦ä¹ å’Œä¼˜åŒ–ã€‚ç„¶åå¼•å…¥ä¸€ç§å…¬å¼ï¼ŒåŸºäºå­¦ç”Ÿä¸æ•™å¸ˆä¹‹é—´çš„å·®å¼‚åŠ¨æ€è®¡ç®—alphaï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCAMï¼‰ï¼Œä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºå’Œæ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°é‡æ–°åŠ æƒç±»åˆ«æ•™å¸ˆè¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨CIFAR-10æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ResNet-50ä½œä¸ºæ•™å¸ˆï¼ŒResNet-18ä½œä¸ºå­¦ç”Ÿï¼Œæ‰€ææ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºå›ºå®šæƒé‡çš„KDåŸºçº¿ï¼Œå¹¶ä¸”æ”¶æ•›æ›´åŠ ç¨³å®šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•ä¸­å›ºå®šå¹³è¡¡å› å­alphaçš„ä¸è¶³ï¼Œå¯¼è‡´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— æ³•åŠ¨æ€è°ƒæ•´ç¡¬æ ‡ç­¾ä¸è½¯æ ‡ç­¾çš„æƒé‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªé€‚åº”çŸ¥è¯†è’¸é¦ï¼ˆAKDï¼‰æ¡†æ¶ï¼Œå°†å¹³è¡¡å› å­alphaè®¾ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œèƒ½å¤Ÿæ ¹æ®å­¦ç”Ÿä¸æ•™å¸ˆä¹‹é—´çš„å·®å¼‚åŠ¨æ€è°ƒæ•´ï¼Œä»è€Œä¼˜åŒ–çŸ¥è¯†è½¬ç§»æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŠ¨æ€è®¡ç®—alphaçš„æœºåˆ¶ï¼Œå…¶æ¬¡æ˜¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCAMï¼‰ï¼Œåè€…ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºå’Œæ³¨æ„åŠ›æœºåˆ¶å¯¹æ•™å¸ˆè¾“å‡ºè¿›è¡Œè‡ªé€‚åº”åŠ æƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¹³è¡¡å› å­alphaè½¬å˜ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå¹¶å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—ä»¥å¢å¼ºæ•™å¸ˆè¾“å‡ºçš„é€‚åº”æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€æƒé‡è®¾ç½®å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†ç¡¬æ ‡ç­¾äº¤å‰ç†µæŸå¤±å’Œè½¯æ ‡ç­¾è’¸é¦æŸå¤±ï¼Œalphaçš„åŠ¨æ€è®¡ç®—åŸºäºå­¦ç”Ÿä¸æ•™å¸ˆçš„è¾“å‡ºå·®å¼‚ï¼ŒCAMæ¨¡å—é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–ç±»åˆ«è¾“å‡ºçš„æƒé‡åˆ†é…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æè‡ªé€‚åº”çŸ¥è¯†è’¸é¦æ¡†æ¶åœ¨CIFAR-10æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºå›ºå®šæƒé‡çš„KDåŸºçº¿ï¼Œå‡†ç¡®æ€§æå‡äº†çº¦3%ï¼Œå¹¶ä¸”æ”¶æ•›è¿‡ç¨‹æ›´åŠ ç¨³å®šï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨çŸ¥è¯†è½¬ç§»æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡ä¼˜åŒ–çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œæœªæ¥å¯èƒ½åœ¨ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge distillation (KD) is a widely used technique to transfer knowledge from a large teacher network to a smaller student model. Traditional KD uses a fixed balancing factor alpha as a hyperparameter to combine the hard-label cross-entropy loss with the soft-label distillation loss. However, a static alpha is suboptimal because the optimal trade-off between hard and soft supervision can vary during training.
>   In this work, we propose an Adaptive Knowledge Distillation (AKD) framework. First we try to make alpha as learnable parameter that can be automatically learned and optimized during training. Then we introduce a formula to reflect the gap between the student and the teacher to compute alpha dynamically, guided by student-teacher discrepancies, and further introduce a Context-Aware Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as student demonstrate that our approach achieves superior accuracy compared to fixed-weight KD baselines, and yields more stable convergence.

