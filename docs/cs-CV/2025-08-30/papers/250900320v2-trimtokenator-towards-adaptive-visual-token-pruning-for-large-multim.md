---
layout: default
title: TrimTokenator: Towards Adaptive Visual Token Pruning for Large Multimodal Models
---

# TrimTokenator: Towards Adaptive Visual Token Pruning for Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00320" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00320v2</a>
  <a href="https://arxiv.org/pdf/2509.00320.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00320v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00320v2', 'TrimTokenator: Towards Adaptive Visual Token Pruning for Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Zhang, Mengsi Lyu, Chenrui He, Yulong Ao, Yonghua Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30 (æ›´æ–°: 2025-10-02)

**å¤‡æ³¨**: 15 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰ä»¤ç‰Œä¿®å‰ªç­–ç•¥ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `è§†è§‰ä»¤ç‰Œä¿®å‰ª` `äº’ä¿¡æ¯` `æ¨ç†æ•ˆç‡` `è®¡ç®—æœºè§†è§‰` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯å¤šæ ·æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´è®¡ç®—å’Œå†…å­˜æˆæœ¬æ˜¾è‘—å¢åŠ çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä»¤ç‰Œæ•°é‡çš„å¢åŠ å¯¼è‡´å†—ä½™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è§†è§‰ä»¤ç‰Œçš„ä¿®å‰ªç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡ä¿æŒè·¨æ¨¡æ€å¯¹é½å’Œä¿¡æ¯å¤šæ ·æ€§æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨LLaVA-1.5-7Bå’ŒLLaVA-NEXT-7Bæ¨¡å‹ä¸Šå®ç°äº†88.9%çš„ä»¤ç‰Œå‡å°‘å’Œ56.7%çš„æ¨ç†é€Ÿåº¦æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚è¿™äº›æ¨¡å‹é€šå¸¸å°†è§†è§‰è¾“å…¥ç¼–ç ä¸ºå¯†é›†çš„ä»¤ç‰Œåºåˆ—ï¼Œå¹¶ä¸æ–‡æœ¬ä»¤ç‰Œè¿æ¥åå…±åŒå¤„ç†ã€‚ç„¶è€Œï¼Œä»¤ç‰Œæ•°é‡çš„å¢åŠ æ˜¾è‘—æé«˜äº†æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚ä»¤ç‰Œä¿®å‰ªä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ã€‚ç°æœ‰çš„ä¿®å‰ªæ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜‚è´µçš„æ ¡å‡†æˆ–æ¬¡ä¼˜çš„é‡è¦æ€§åº¦é‡ï¼Œå¯¼è‡´ä¿ç•™çš„ä»¤ç‰Œå†—ä½™ã€‚æœ¬æ–‡åˆ†æäº†è§†è§‰å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´çš„å†—ä½™å·®å¼‚ï¼Œæå‡ºä»…å¯¹è§†è§‰ä»¤ç‰Œè¿›è¡Œä¿®å‰ªçš„ç­–ç•¥ï¼Œç¡®ä¿è·¨æ¨¡æ€å¯¹é½å’Œæ¨¡æ€å†…éƒ¨ä¿¡æ¯å¤šæ ·æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºäº’ä¿¡æ¯çš„ä¿®å‰ªç­–ç•¥ï¼Œå»é™¤ä¸æ–‡æœ¬ä»¤ç‰Œè¯­ä¹‰ä¸å¯¹é½çš„è§†è§‰ä»¤ç‰Œï¼Œä»è€Œæœ‰æ•ˆä¿æŒè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¼ºå¤§æ€§èƒ½çš„åŒæ—¶ï¼Œä»¤ç‰Œå‡å°‘äº†88.9%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†56.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºä»¤ç‰Œæ•°é‡å¢åŠ è€Œå¯¼è‡´çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„ä»¤ç‰Œä¿®å‰ªæ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜‚è´µçš„æ ¡å‡†æˆ–ä¸å¤Ÿæœ‰æ•ˆçš„é‡è¦æ€§åº¦é‡ï¼Œå¯¼è‡´ä¿ç•™çš„ä»¤ç‰Œå†—ä½™ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»…å¯¹è§†è§‰ä»¤ç‰Œè¿›è¡Œä¿®å‰ªï¼Œåˆ†æè§†è§‰å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´çš„å†—ä½™å·®å¼‚ï¼Œç¡®ä¿è·¨æ¨¡æ€å¯¹é½å’Œæ¨¡æ€å†…éƒ¨ä¿¡æ¯çš„å¤šæ ·æ€§ã€‚é€šè¿‡å¼•å…¥åŸºäºäº’ä¿¡æ¯çš„ä¿®å‰ªç­–ç•¥ï¼Œå»é™¤ä¸æ–‡æœ¬ä»¤ç‰Œè¯­ä¹‰ä¸å¯¹é½çš„è§†è§‰ä»¤ç‰Œï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯äº’ä¿¡æ¯è®¡ç®—æ¨¡å—ï¼Œç”¨äºè¯„ä¼°è§†è§‰ä»¤ç‰Œä¸æ–‡æœ¬ä»¤ç‰Œçš„å¯¹é½ç¨‹åº¦ï¼›å…¶æ¬¡æ˜¯å†—ä½™ä»¤ç‰Œä¿®å‰ªæ¨¡å—ï¼Œé€šè¿‡æœ€å¤§åŒ–åµŒå…¥ç©ºé—´ä¸­çš„æœŸæœ›æˆå¯¹è·ç¦»æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ä¿ç•™çš„è§†è§‰ä»¤ç‰Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰ä»¤ç‰Œä¿®å‰ªç­–ç•¥ï¼Œå¼ºè°ƒäº†è·¨æ¨¡æ€å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡äº’ä¿¡æ¯åº¦é‡æ¥ä¼˜åŒ–è§†è§‰ä»¤ç‰Œçš„é€‰æ‹©ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰ä¾èµ–äºå•ä¸€é‡è¦æ€§åº¦é‡çš„ä¿®å‰ªæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†è´ªå¿ƒç®—æ³•æ¥é«˜æ•ˆè§£å†³æœ€å¤§åŒ–æœŸæœ›æˆå¯¹è·ç¦»çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ç¡®ä¿äº†ä¿ç•™çš„è§†è§‰ä»¤ç‰Œåœ¨è¯­ä¹‰ä¸Šä¸æ–‡æœ¬ä»¤ç‰Œä¿æŒä¸€è‡´ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ•´ä½“è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTrimTokenatoræ–¹æ³•åœ¨LLaVA-1.5-7Bå’ŒLLaVA-NEXT-7Bæ¨¡å‹ä¸Šå®ç°äº†88.9%çš„ä»¤ç‰Œå‡å°‘ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æå‡äº†56.7%ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ä»¥åŠå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶åº”ç”¨ä¸­æå‡ç”¨æˆ·ä½“éªŒï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Multimodal Models (LMMs) have achieved significant success across various tasks. These models usually encode visual inputs into dense token sequences, which are then concatenated with textual tokens and jointly processed by a language model. However, the increased token count substantially raises computational and memory costs during inference. Token pruning has emerged as a promising approach to address this issue. Existing token pruning methods often rely on costly calibration or suboptimal importance metrics, leading to redundant retained tokens. In this paper, we analyze the redundancy differences between visual and textual tokens and propose pruning exclusively on visual tokens. Based on this, we propose a visual token pruning strategy that explicitly preserves both cross-modal alignment and intra-modal informational diversity. We introduce a mutual information-based token pruning strategy that removes visual tokens semantically misaligned with textual tokens, effectively preserving the alignment between the visual and textual modalities. To further improve the representational quality of the retained tokens, we additionally prune redundant visual tokens by maximizing the expected pairwise distances in the embedding space, which is solved efficiently with a greedy algorithm. Extensive experiments demonstrate that our method maintains strong performance while reducing tokens by 88.9% on models such as LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference speed.

