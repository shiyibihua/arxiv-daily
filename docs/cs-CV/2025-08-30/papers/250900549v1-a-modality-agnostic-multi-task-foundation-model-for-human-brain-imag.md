---
layout: default
title: A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging
---

# A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00549" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00549v1</a>
  <a href="https://arxiv.org/pdf/2509.00549.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00549v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00549v1', 'A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peirong Liu, Oula Puonti, Xiaoling Hu, Karthik Gopinath, Annabel Sorby-Adams, Daniel C. Alexander, W. Taylor Kimberly, Juan E. Iglesias

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-30

**å¤‡æ³¨**: 16 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jhuldr/BrainFM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBrainFMä»¥è§£å†³è„‘éƒ¨æˆåƒå¤šæ¨¡æ€æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‘éƒ¨æˆåƒ` `å¤šæ¨¡æ€å­¦ä¹ ` `åŒ»å­¦å›¾åƒå¤„ç†` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹æ³›åŒ–` `å›¾åƒåˆæˆ` `è§£å‰–åˆ†å‰²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒ»å­¦æˆåƒå­¦ä¹ æ–¹æ³•åœ¨æœªæ ¡å‡†æ¨¡æ€ï¼ˆå¦‚MRæˆåƒï¼‰ä¸­éš¾ä»¥æ³›åŒ–ï¼Œå½±å“äº†å…¶ä¸´åºŠåº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºBrainFMæ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„è®­ç»ƒç­–ç•¥å¢å¼ºå¯¹ä¸åŒæˆåƒæ¡ä»¶çš„é€‚åº”æ€§ï¼Œæ”¯æŒå¤šç§è„‘éƒ¨æˆåƒä»»åŠ¡ã€‚
3. åœ¨11ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBrainFMåœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ˜¾è‘—çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨åŒ»å­¦æˆåƒï¼ˆå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æCTï¼‰ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æœªæ ¡å‡†çš„æ¨¡æ€ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒMRï¼‰ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶å¯¹MRå¯¹æ¯”åº¦ã€åˆ†è¾¨ç‡å’Œæ–¹å‘çš„æ•æ„Ÿæ€§é™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†BrainFMï¼Œä¸€ä¸ªæ¨¡æ€æ— å…³çš„å¤šä»»åŠ¡è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºäººè„‘æˆåƒçš„é€‚ç”¨æ€§ã€‚é€šè¿‡â€œè½»åº¦åˆ°é‡åº¦â€çš„ä¸ªä½“å†…ç”Ÿæˆå’Œâ€œçœŸå®-åˆæˆâ€æ··åˆè®­ç»ƒç­–ç•¥ï¼ŒBrainFMèƒ½å¤ŸæŠµå¾¡æˆåƒå›¾åƒçš„å¤–è§‚å˜åŒ–ï¼Œé€‚ç”¨äºCTå’Œå¤šç§MRIä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒåˆæˆã€è§£å‰–åˆ†å‰²ã€å¤´çš®åˆ°çš®å±‚è·ç¦»ã€åç½®åœºä¼°è®¡å’Œé…å‡†ã€‚æˆ‘ä»¬åœ¨11ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†BrainFMçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ‰€æœ‰ä»»åŠ¡å’Œè¾“å…¥æ¨¡æ€ä¸‹çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŒ»å­¦æˆåƒæ–¹æ³•åœ¨å¤„ç†æœªæ ¡å‡†æ¨¡æ€ï¼ˆå¦‚MRæˆåƒï¼‰æ—¶ï¼Œå› å¯¹æˆåƒæ¡ä»¶çš„æ•æ„Ÿæ€§è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBrainFMæ¨¡å‹é€šè¿‡â€œè½»åº¦åˆ°é‡åº¦â€çš„ä¸ªä½“å†…ç”Ÿæˆå’Œâ€œçœŸå®-åˆæˆâ€æ··åˆè®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸åŒæˆåƒæ¡ä»¶ï¼ˆå¦‚æ¨¡æ€ã€å¯¹æ¯”åº¦ã€å˜å½¢ç­‰ï¼‰çš„é€‚åº”èƒ½åŠ›ï¼Œä»è€Œå®ç°å¤šä»»åŠ¡å¤„ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBrainFMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œå¤šä»»åŠ¡è¾“å‡ºç­‰ä¸»è¦æ¨¡å—ã€‚æ¨¡å‹é€šè¿‡å¯¹ä¸åŒæ¨¡æ€çš„è¾“å…¥è¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œç”Ÿæˆé€‚ç”¨äºå¤šç§ä»»åŠ¡çš„ç‰¹å¾è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šBrainFMçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶æ¨¡æ€æ— å…³æ€§å’Œå¤šä»»åŠ¡èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒæˆåƒæ¡ä»¶ä¸‹ä¿æŒé«˜æ•ˆæ€§èƒ½ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€æ¨¡æ€ä¾èµ–å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨11ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBrainFMåœ¨å›¾åƒåˆæˆã€è§£å‰–åˆ†å‰²ç­‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç°å‡ºå…¶åœ¨å¤šæ¨¡æ€è„‘éƒ¨æˆåƒä¸­çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BrainFMæ¨¡å‹åœ¨è„‘éƒ¨æˆåƒé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæ”¯æŒä¸´åºŠè¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹ç­‰å¤šç§ä»»åŠ¡ã€‚å…¶æ¨¡æ€æ— å…³çš„ç‰¹æ€§ä½¿å¾—è¯¥æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æˆåƒæŠ€æœ¯ï¼Œæå‡åŒ»ç–—å½±åƒåˆ†æçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ä¸ªæ€§åŒ–åŒ»ç–—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT), yet they struggle to generalize in uncalibrated modalities -- notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. Here we introduce BrainFM, a modality-agnostic, multi-task vision foundation model for human brain imaging. With the proposed "mild-to-severe" intra-subject generation and "real-synth" mix-up training strategy, BrainFM is resilient to the appearance of acquired images (e.g., modality, contrast, deformation, resolution, artifacts), and can be directly applied to five fundamental brain imaging tasks, including image synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical distance, bias field estimation, and registration. We evaluate the efficacy of BrainFM on eleven public datasets, and demonstrate its robustness and effectiveness across all tasks and input modalities. Code is available at https://github.com/jhuldr/BrainFM.

