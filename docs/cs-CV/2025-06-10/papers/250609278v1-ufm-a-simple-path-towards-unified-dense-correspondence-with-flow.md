---
layout: default
title: UFM: A Simple Path towards Unified Dense Correspondence with Flow
---

# UFM: A Simple Path towards Unified Dense Correspondence with Flow

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09278v1</a>
  <a href="https://arxiv.org/pdf/2506.09278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09278v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09278v1', 'UFM: A Simple Path towards Unified Dense Correspondence with Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, Sebastian Scherer, Wenshan Wang

**åˆ†ç±»**: cs.CV, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10

**å¤‡æ³¨**: Project Page: https://uniflowmatch.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æµä¸åŒ¹é…æ¨¡å‹ä»¥è§£å†³ç¨ å¯†å›¾åƒå¯¹åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `ç¨ å¯†å›¾åƒå¯¹åº”` `å…‰æµä¼°è®¡` `è§†è§‰é‡Œç¨‹è®¡` `3Dé‡å»º` `ç‰©ä½“å…³è”` `å†è¯†åˆ«` `ç»Ÿä¸€è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¹¿åŸºçº¿åœºæ™¯å’Œå…‰æµä¼°è®¡æ—¶ï¼Œé€šå¸¸å°†ä¸¤è€…åˆ†å¼€ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºçš„UFMæ¨¡å‹é€šè¿‡ç»Ÿä¸€è®­ç»ƒåœ¨å…±åŒå¯è§åƒç´ ä¸Šè¿›è¡Œæµå’ŒåŒ¹é…çš„ç›´æ¥å›å½’ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒUFMåœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤§æµæƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨ å¯†å›¾åƒå¯¹åº”æ˜¯è§†è§‰é‡Œç¨‹è®¡ã€3Dé‡å»ºã€ç‰©ä½“å…³è”å’Œå†è¯†åˆ«ç­‰å¤šä¸ªåº”ç”¨çš„æ ¸å¿ƒã€‚å°½ç®¡å¹¿æ³›åŸºçº¿åœºæ™¯å’Œå…‰æµä¼°è®¡çš„ç›®æ ‡ç›¸åŒï¼Œä½†å†å²ä¸Šè¿™ä¸¤è€…ä¸€ç›´è¢«åˆ†å¼€å¤„ç†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æµä¸åŒ¹é…æ¨¡å‹ï¼ˆUFMï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨æºå›¾åƒå’Œç›®æ ‡å›¾åƒä¸­å…±åŒå¯è§çš„åƒç´ ä¸Šè¿›è¡Œç»Ÿä¸€æ•°æ®è®­ç»ƒã€‚UFMé‡‡ç”¨ç®€å•çš„é€šç”¨å˜æ¢å™¨æ¶æ„ï¼Œç›´æ¥å›å½’(u,v)æµã€‚ä¸ä¼ ç»Ÿçš„ç²—åˆ°ç»†ä»£ä»·ä½“ç§¯æ–¹æ³•ç›¸æ¯”ï¼ŒUFMæ›´æ˜“äºè®­ç»ƒä¸”åœ¨å¤§æµæƒ…å†µä¸‹æ›´ä¸ºå‡†ç¡®ã€‚UFMçš„å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„æµæ–¹æ³•ï¼ˆUnimatchï¼‰é«˜å‡º28%ï¼Œé”™è¯¯ç‡ä½62%ï¼Œé€Ÿåº¦æ¯”ç¨ å¯†å¹¿åŸºçº¿åŒ¹é…å™¨ï¼ˆRoMaï¼‰å¿«6.7å€ã€‚UFMé¦–æ¬¡è¯æ˜äº†ç»Ÿä¸€è®­ç»ƒå¯ä»¥åœ¨ä¸¤ä¸ªé¢†åŸŸè¶…è¶Šä¸“ä¸šæ–¹æ³•ï¼Œè¿™ä¸€ç»“æœä¸ºå¿«é€Ÿé€šç”¨çš„å¯¹åº”ä»»åŠ¡å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨ å¯†å›¾åƒå¯¹åº”é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¹¿åŸºçº¿åœºæ™¯å’Œå…‰æµä¼°è®¡ä¸Šé€šå¸¸åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUFMæ¨¡å‹é€šè¿‡ç»Ÿä¸€è®­ç»ƒåœ¨å…±åŒå¯è§çš„åƒç´ ä¸Šè¿›è¡Œæµå’ŒåŒ¹é…çš„ç›´æ¥å›å½’ï¼Œé‡‡ç”¨ç®€å•çš„å˜æ¢å™¨æ¶æ„ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUFMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€æµå›å½’å’ŒåŒ¹é…æ¨¡å—ï¼Œæ‰€æœ‰æ¨¡å—ååŒå·¥ä½œä»¥å®ç°é«˜æ•ˆçš„ç¨ å¯†å¯¹åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šUFMé¦–æ¬¡å±•ç¤ºäº†ç»Ÿä¸€è®­ç»ƒåœ¨æµå’ŒåŒ¹é…ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åˆ†å¼€è®­ç»ƒæ–¹æ³•ï¼Œæä¾›äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´å¿«çš„é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†é€šç”¨çš„å˜æ¢å™¨æ¶æ„ï¼Œç›´æ¥å›å½’(u,v)æµï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥é€‚åº”å¤§æµæƒ…å†µï¼Œç¡®ä¿äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UFMæ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„æµæ–¹æ³•ï¼ˆUnimatchï¼‰é«˜å‡º28%ï¼Œé”™è¯¯ç‡ä½62%ï¼Œé€Ÿåº¦æ¯”ç¨ å¯†å¹¿åŸºçº¿åŒ¹é…å™¨ï¼ˆRoMaï¼‰å¿«6.7å€ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒUFMåœ¨å¤„ç†ç¨ å¯†å›¾åƒå¯¹åº”ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UFMæ¨¡å‹åœ¨è§†è§‰é‡Œç¨‹è®¡ã€3Dé‡å»ºã€ç‰©ä½“å…³è”å’Œå†è¯†åˆ«ç­‰å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„ç¨ å¯†å¯¹åº”èƒ½åŠ›èƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›ä»»åŠ¡çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å®æ—¶å¤„ç†çš„åœºæ™¯ä¸­ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹ä¹Ÿå¯èƒ½ä¸ºå¤šæ¨¡æ€å’Œé•¿è·ç¦»å¯¹åº”ä»»åŠ¡æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow. It is easier to train and more accurate for large flows compared to the typical coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This result enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.

