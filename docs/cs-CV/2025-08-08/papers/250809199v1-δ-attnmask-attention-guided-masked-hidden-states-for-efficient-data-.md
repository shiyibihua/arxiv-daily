---
layout: default
title: $Î”$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation
---

# $Î”$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09199" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09199v1</a>
  <a href="https://arxiv.org/pdf/2508.09199.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09199v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09199v1', '$Î”$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jucheng Hu, Suorong Yang, Dongzhan Zhou

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º$Î”$-AttnMaskä»¥è§£å†³è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸­çš„æ•°æ®é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æŒ‡ä»¤å¾®è°ƒ` `å¤šæ¨¡æ€æ•°æ®` `æ ·æœ¬è´¨é‡è¯„ä¼°` `æ³¨æ„åŠ›æœºåˆ¶` `æ•°æ®é€‰æ‹©` `æ¨¡å‹æ— å…³` `è®­ç»ƒåŠ é€Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•åœ¨æ•°æ®é€‰æ‹©ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€æ•°æ®çš„è´¨é‡å’Œå¯¹é½æ€§æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºçš„$Î”$-AttnMaské€šè¿‡æ³¨æ„åŠ›å¼•å¯¼çš„æ©è”½æ–¹æ³•ï¼Œè¯„ä¼°æ ·æœ¬è´¨é‡ï¼Œé¿å…äº†å¯¹é¢å¤–æ ‡ç­¾å’Œæ¨¡å‹çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ$Î”$-AttnMaskåœ¨å¤šä¸ªVLMå’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä»…ç”¨20%æ•°æ®å³å¯å®ç°5å€çš„è®­ç»ƒåŠ é€Ÿå’Œ10.1%çš„å‡†ç¡®ç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVIFï¼‰å¯¹åè®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è‡³å…³é‡è¦ã€‚ä¸å•æ¨¡æ€æŒ‡ä»¤å¾®è°ƒä¸åŒï¼ŒVIFéœ€è¦å¤šæ¨¡æ€æ•°æ®ä»¥å®ç°è§†è§‰å’Œæ–‡æœ¬çš„è”åˆç†è§£ï¼Œå› æ­¤å¯¹æ•°æ®çš„éœ€æ±‚æ›´å¤§ã€‚å°½ç®¡æ•°æ®é€‰æ‹©å¯¹æ€§èƒ½å½±å“é‡å¤§ï¼Œä½†è¿™ä¸€é¢†åŸŸä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡æå‡º$Î”$-AttnMaskæ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¨¡å‹éšè—çŠ¶æ€çš„æ³¨æ„åŠ›å¼•å¯¼æ©è”½é‡åŒ–æ ·æœ¬è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢†åŸŸæ ‡ç­¾ã€è¾…åŠ©æ¨¡å‹æˆ–é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¯„ä¼°å›¾åƒ-æ–‡æœ¬å¯¹çš„è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œ$Î”$-AttnMaskåœ¨ä»…ä½¿ç”¨20%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒé€Ÿåº¦æå‡5å€ï¼Œå¹¶åœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šè¶…è¶Šå…¨æ•°æ®é›†åŸºçº¿10.1%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVIFï¼‰ä¸­æ•°æ®é€‰æ‹©çš„æ•ˆç‡å’Œè´¨é‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´æ•°æ®é€‰æ‹©è¿‡ç¨‹å¤æ‚ä¸”ä½æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼š$Î”$-AttnMaské€šè¿‡å¯¹æ¨¡å‹éšè—çŠ¶æ€è¿›è¡Œæ³¨æ„åŠ›å¼•å¯¼çš„æ©è”½ï¼Œé‡åŒ–æ ·æœ¬è´¨é‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¢å¤–æ ‡ç­¾æˆ–è¾…åŠ©æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œç›´æ¥è¯„ä¼°å›¾åƒ-æ–‡æœ¬å¯¹çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶çš„æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆè®¡ç®—æ¨¡å‹çš„éšè—çŠ¶æ€ï¼Œç„¶åæ ¹æ®æ³¨æ„åŠ›æƒé‡è¿›è¡Œæ©è”½ï¼Œæœ€åé€šè¿‡æ¯”è¾ƒåŸå§‹çŠ¶æ€ä¸æ©è”½çŠ¶æ€ä¹‹é—´çš„æŸå¤±å·®å¼‚æ¥è¯„ä¼°æ ·æœ¬è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼š$Î”$-AttnMaskçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— ç›‘ç£çš„æ ·æœ¬è´¨é‡è¯„ä¼°æœºåˆ¶ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥ä»æ¨¡å‹å†…éƒ¨è·å–ä¿¡æ¯ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•å¯¹å¤–éƒ¨æ ‡ç­¾å’Œæ¨¡å‹çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œ$Î”$-AttnMaské‡‡ç”¨äº†åŸºäºæ³¨æ„åŠ›æƒé‡çš„æ©è”½ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°é€šè¿‡è®¡ç®—åŸå§‹çŠ¶æ€ä¸æ©è”½çŠ¶æ€çš„å·®å¼‚æ¥å®ç°æ ·æœ¬è´¨é‡çš„é‡åŒ–ï¼Œç¡®ä¿äº†æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œ$Î”$-AttnMaskåœ¨å¤šä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨20%çš„æ•°æ®ä¾¿å®ç°äº†5å€çš„è®­ç»ƒåŠ é€Ÿï¼Œå¹¶åœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šè¶…è¶Šå…¨æ•°æ®é›†åŸºçº¿10.1%ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†å…¶åœ¨æ•°æ®é€‰æ‹©å’Œæ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

$Î”$-AttnMaskçš„è®¾è®¡å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜æ•ˆæ•°æ®é€‰æ‹©å’Œå¢å¼ºçš„å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”å’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§å°†æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Instruction Finetuning (VIF) is pivotal for post-training Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in plain-text large language models, which mainly requires instruction datasets to enable model instruction-following ability, VIF also requires multimodal data to enable joint visual and textual understanding; therefore, it typically requires more data. Consequently, VIF imposes stricter data selection challenges: the method must scale efficiently to handle larger data demands while ensuring the quality of both visual and textual content, as well as their alignment. Despite its critical impact on performance, data selection for VIF remains an understudied area. In this paper, we propose $Î”$-AttnMask. This data-efficient framework quantifies sample quality through attention-guided masking of the model's hidden states, jointly evaluating image-text pairs without requiring domain labels, auxiliary models, or extra training. By computing loss differences ($Î”$) between the original states and states masked using high-attention regions, $Î”$-AttnMask intrinsically assesses sample quality. Experiments across multiple VLMs and datasets show that $Î”$-AttnMask achieves state-of-the-art performance with just 20% of data, accelerating training by 5x while surpassing full-dataset baselines by +10.1% in overall accuracy. Its model-agnostic and data-agnostic design ensures broad applicability across modalities and architectures.

