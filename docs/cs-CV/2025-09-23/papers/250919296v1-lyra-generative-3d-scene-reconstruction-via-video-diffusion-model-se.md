---
layout: default
title: Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation
---

# Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19296" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19296v1</a>
  <a href="https://arxiv.org/pdf/2509.19296.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19296v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19296v1', 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren

**åˆ†ç±»**: cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-23

**å¤‡æ³¨**: Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Lyraï¼šé€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹è‡ªè’¸é¦å®ç°ç”Ÿæˆå¼3Dåœºæ™¯é‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯é‡å»º` `è§†é¢‘æ‰©æ•£æ¨¡å‹` `è‡ªè’¸é¦` `3Dé«˜æ–¯æº…å°„` `ç”Ÿæˆæ¨¡å‹` `å•ç›®è§†è§‰` `è™šæ‹Ÿç¯å¢ƒç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dé‡å»ºæ–¹æ³•ä¾èµ–å¤šè§†è§’æ•°æ®ï¼Œè·å–å›°éš¾ï¼Œé™åˆ¶äº†åº”ç”¨åœºæ™¯ã€‚
2. Lyraé€šè¿‡è‡ªè’¸é¦æ¡†æ¶ï¼Œå°†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„éšå¼3DçŸ¥è¯†æç‚¼ä¸ºæ˜¾å¼3DGSè¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€3Dåœºæ™¯ç”Ÿæˆæ–¹é¢å‡è¾¾åˆ°SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆè™šæ‹Ÿç¯å¢ƒçš„èƒ½åŠ›å¯¹äºæ¸¸æˆã€æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œå·¥ä¸šAIç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åŸºäºå­¦ä¹ çš„3Dé‡å»ºæ–¹æ³•ä¾èµ–äºæ•è·çš„çœŸå®ä¸–ç•Œå¤šè§†è§’æ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®å¹¶éæ€»æ˜¯å®¹æ˜“è·å¾—ã€‚æœ€è¿‘è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å±•ç¤ºäº†å“è¶Šçš„æƒ³è±¡èƒ½åŠ›ï¼Œä½†å…¶2Dæ€§è´¨é™åˆ¶äº†åœ¨æœºå™¨äººéœ€è¦å¯¼èˆªå’Œä¸ç¯å¢ƒäº¤äº’çš„æ¨¡æ‹Ÿä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„éšå¼3DçŸ¥è¯†æç‚¼æˆæ˜¾å¼çš„3Dé«˜æ–¯æº…å°„(3DGS)è¡¨ç¤ºï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤šè§†è§’è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨3DGSè§£ç å™¨å¢å¼ºäº†å…¸å‹çš„RGBè§£ç å™¨ï¼Œè¯¥è§£ç å™¨ç”±RGBè§£ç å™¨çš„è¾“å‡ºç›‘ç£ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œ3DGSè§£ç å™¨å¯ä»¥å®Œå…¨ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä»æ–‡æœ¬æç¤ºæˆ–å•ä¸ªå›¾åƒåˆæˆ3Dåœºæ™¯ä»¥è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿›ä¸€æ­¥æ‰©å±•åˆ°ä»å•ç›®è¾“å…¥è§†é¢‘ç”ŸæˆåŠ¨æ€3Dåœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€3Dåœºæ™¯ç”Ÿæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå­¦ä¹ çš„3Dé‡å»ºæ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤šè§†è§’å›¾åƒæ•°æ®ï¼Œè€Œè·å–é«˜è´¨é‡ã€å¤šè§†è§’çš„çœŸå®ä¸–ç•Œæ•°æ®æˆæœ¬é«˜æ˜‚ä¸”ä¸æ€»æ˜¯å¯è¡Œã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹è™½ç„¶å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶æœ¬è´¨æ˜¯2Dçš„ï¼Œæ— æ³•ç›´æ¥ç”¨äºéœ€è¦3Dä¿¡æ¯çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªå’Œäº¤äº’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLyraçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„2Dç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡è‡ªè’¸é¦çš„æ–¹å¼ï¼Œå°†å…¶ä¸­è•´å«çš„éšå¼3DçŸ¥è¯†æå–å‡ºæ¥ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ˜¾å¼çš„3Dè¡¨ç¤ºã€‚è¿™æ ·ï¼Œå°±å¯ä»¥åœ¨ä¸éœ€è¦å¤šè§†è§’æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥è®­ç»ƒ3Dé‡å»ºæ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLyraæ¡†æ¶ä¸»è¦åŒ…å«ä¸€ä¸ªè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆä½œä¸ºæ•™å¸ˆç½‘ç»œï¼‰å’Œä¸€ä¸ª3DGSè§£ç å™¨ï¼ˆä½œä¸ºå­¦ç”Ÿç½‘ç»œï¼‰ã€‚é¦–å…ˆï¼Œä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”ŸæˆRGBå›¾åƒã€‚ç„¶åï¼Œå°†ç”Ÿæˆçš„RGBå›¾åƒä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒ3DGSè§£ç å™¨ã€‚3DGSè§£ç å™¨å°†å›¾åƒè§£ç ä¸º3Dé«˜æ–¯æº…å°„è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºå¯ä»¥ç”¨äºå®æ—¶æ¸²æŸ“å’Œ3Dåœºæ™¯é‡å»ºã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹æ˜¯ä¸€ä¸ªè‡ªè’¸é¦çš„è¿‡ç¨‹ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ æ¨¡ä»¿æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºï¼Œä»è€Œè·å¾—3Dåœºæ™¯çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šLyraçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨è‡ªè’¸é¦çš„æ–¹å¼ï¼Œå°†2Dè§†é¢‘æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°3Dè¡¨ç¤ºå­¦ä¹ ä¸­ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤šè§†è§’æ•°æ®çš„ä¾èµ–ï¼Œä½¿å¾—3Dåœºæ™¯é‡å»ºå¯ä»¥åœ¨ä»…æœ‰å•è§†è§’å›¾åƒæˆ–æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œã€‚æ­¤å¤–ï¼Œä½¿ç”¨3DGSä½œä¸º3Dè¡¨ç¤ºï¼Œå¯ä»¥å®ç°å®æ—¶æ¸²æŸ“ã€‚

**å…³é”®è®¾è®¡**ï¼šLyraçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆç½‘ç»œï¼Œæä¾›é«˜è´¨é‡çš„ç›‘ç£ä¿¡å·ï¼›2) ä½¿ç”¨3DGSä½œä¸º3Dè¡¨ç¤ºï¼Œå®ç°å®æ—¶æ¸²æŸ“ï¼›3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä½¿å¾—3DGSè§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ è§†é¢‘æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°åŒ…æ‹¬RGBé‡å»ºæŸå¤±ã€æ·±åº¦ä¸€è‡´æ€§æŸå¤±ç­‰ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œ3DGSè§£ç å™¨é€šå¸¸é‡‡ç”¨MLPç»“æ„ï¼Œå°†å›¾åƒç‰¹å¾æ˜ å°„åˆ°3Dé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Lyraåœ¨é™æ€å’ŒåŠ¨æ€3Dåœºæ™¯ç”Ÿæˆæ–¹é¢å‡å–å¾—äº†SOTAæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLyraæ— éœ€å¤šè§†è§’è®­ç»ƒæ•°æ®ï¼Œä»…ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å³å¯è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLyraç”Ÿæˆçš„3Dåœºæ™¯å…·æœ‰æ›´é«˜çš„è´¨é‡å’Œæ›´å¼ºçš„çœŸå®æ„Ÿã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶åœ¨ä¸»è§‚è§†è§‰æ•ˆæœä¸Šçš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Lyraçš„åº”ç”¨åœºæ™¯å¹¿æ³›ï¼ŒåŒ…æ‹¬æ¸¸æˆå¼€å‘ã€æœºå™¨äººä»¿çœŸã€è‡ªåŠ¨é©¾é©¶ã€å·¥ä¸šAIç­‰ã€‚å®ƒå¯ä»¥ç”¨äºå¿«é€Ÿç”Ÿæˆè™šæ‹Ÿç¯å¢ƒï¼Œä¸ºæœºå™¨äººæä¾›è®­ç»ƒæ•°æ®ï¼Œæˆ–è€…ç”¨äºåˆ›å»ºé€¼çœŸçš„æ¸¸æˆåœºæ™¯ã€‚æ­¤å¤–ï¼ŒLyraè¿˜å¯ä»¥ç”¨äºä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æè¿°ä¸­é‡å»º3Dåœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å¼çš„ä½“éªŒã€‚æœªæ¥ï¼ŒLyraæœ‰æœ›æˆä¸ºè™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.

