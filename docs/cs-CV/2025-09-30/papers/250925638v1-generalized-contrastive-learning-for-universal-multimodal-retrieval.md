---
layout: default
title: Generalized Contrastive Learning for Universal Multimodal Retrieval
---

# Generalized Contrastive Learning for Universal Multimodal Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25638v1</a>
  <a href="https://arxiv.org/pdf/2509.25638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25638v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25638v1', 'Generalized Contrastive Learning for Universal Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jungsoo Lee, Janghoon Cho, Hyojin Park, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Accepted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹¿ä¹‰å¯¹æ¯”å­¦ä¹ GCLï¼Œè§£å†³é€šç”¨å¤šæ¨¡æ€æ£€ç´¢ä¸­ç»„åˆæ¨¡æ€æ³›åŒ–æ€§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `å¯¹æ¯”å­¦ä¹ ` `è·¨æ¨¡æ€å­¦ä¹ ` `å›¾åƒæ–‡æœ¬æ£€ç´¢` `ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨æ¨¡æ€æ£€ç´¢æ¨¡å‹åœ¨å¤„ç†å›¾åƒ-æ–‡æœ¬ç»„åˆæ¨¡æ€æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé€šç”¨æ€§ä¸è¶³ã€‚
2. æå‡ºå¹¿ä¹‰å¯¹æ¯”å­¦ä¹ ï¼ˆGCLï¼‰ï¼Œé€šè¿‡è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼Œå­¦ä¹ ç»Ÿä¸€è¡¨ç¤ºç©ºé—´ï¼Œæ— éœ€é¢å¤–æ•°æ®æ ‡æ³¨ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒGCLæ˜¾è‘—æå‡äº†ç°æœ‰æ¨¡å‹ï¼ˆå¦‚VISTAã€CLIPï¼‰çš„å¤šæ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¹¿ä¹‰å¯¹æ¯”å­¦ä¹ ï¼ˆGCLï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è·¨æ¨¡æ€æ£€ç´¢æ¨¡å‹åœ¨æ£€ç´¢èåˆå›¾åƒ-æ–‡æœ¬æ¨¡æ€ï¼ˆä¾‹å¦‚ï¼ŒåŒ…å«å›¾åƒå’Œæ–‡æœ¬çš„ç»´åŸºç™¾ç§‘é¡µé¢ï¼‰æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢æ–¹æ³•é€šå¸¸éœ€è¦æ„å»ºæ–°çš„å›¾åƒ-æ–‡æœ¬ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œè¿™éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„æ¨¡æ€ç»„åˆã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼ŒGCLé€šè¿‡åœ¨mini-batchä¸­è·¨æ‰€æœ‰æ¨¡æ€å¼ºåˆ¶æ‰§è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨ç°æœ‰çš„å›¾åƒ-æ ‡é¢˜é…å¯¹æ•°æ®é›†æ¥å­¦ä¹ ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ï¼Œä»è€Œæé«˜å¤šæ¨¡æ€æ£€ç´¢æ€§èƒ½ï¼Œè€Œæ— éœ€ç¹ççš„æ–°æ•°æ®é›†æ ‡æ³¨ã€‚åœ¨M-BEIRã€MMEBå’ŒCoVRåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGCLåœ¨VISTAã€CLIPå’ŒTinyCLIPç­‰ç°æˆçš„å¤šæ¨¡æ€æ£€ç´¢æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è·¨æ¨¡æ€æ£€ç´¢æ¨¡å‹ï¼Œå¦‚CLIPï¼Œåœ¨å¤„ç†ç”±å¤šç§æ¨¡æ€ç»„åˆè€Œæˆçš„æ•°æ®æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼Œå½“æ£€ç´¢ç›®æ ‡æ˜¯åŒ…å«å›¾åƒå’Œæ–‡æœ¬çš„ç½‘é¡µæ—¶ï¼Œè¿™äº›æ¨¡å‹çš„æ•ˆæœå¾€å¾€ä¸å¦‚å¤„ç†å•ä¸€æ¨¡æ€æ•°æ®ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šçš„æ¨¡æ€ç»„åˆæ„å»ºæ–°çš„æ•°æ®é›†ï¼Œè¿™éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨å’Œæ•°æ®æ¸…æ´—ï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„æ¨¡æ€ç»„åˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¹¿ä¹‰å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬é…å¯¹æ•°æ®é›†ä¸Šï¼Œå­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ã€‚GCLçš„ç›®æ ‡æ˜¯ä½¿å¾—æ¥è‡ªåŒä¸€å›¾åƒ-æ–‡æœ¬å¯¹çš„è¡¨ç¤ºå°½å¯èƒ½æ¥è¿‘ï¼Œè€Œæ¥è‡ªä¸åŒå›¾åƒ-æ–‡æœ¬å¯¹çš„è¡¨ç¤ºå°½å¯èƒ½è¿œç¦»ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ï¼Œä»è€Œæé«˜åœ¨å„ç§æ¨¡æ€ç»„åˆä¸‹çš„æ£€ç´¢æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGCLçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä»ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬é…å¯¹æ•°æ®é›†ä¸­æŠ½å–mini-batchï¼›2) ä½¿ç”¨é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æå–å›¾åƒå’Œæ–‡æœ¬çš„ç‰¹å¾è¡¨ç¤ºï¼›3) ä½¿ç”¨GCLæŸå¤±å‡½æ•°è®¡ç®—mini-batchä¸­æ‰€æœ‰å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„å¯¹æ¯”æŸå¤±ï¼›4) ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å¥½çš„ç»Ÿä¸€è¡¨ç¤ºç©ºé—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šGCLçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æŸå¤±å‡½æ•°çš„è®¾è®¡ã€‚ä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ é€šå¸¸åªè€ƒè™‘æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œè€ŒGCLåˆ™è€ƒè™‘äº†mini-batchä¸­æ‰€æœ‰æ ·æœ¬ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼ŒGCLå°†mini-batchä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½è§†ä¸ºä¸€ä¸ªæŸ¥è¯¢ï¼Œç„¶åè®¡ç®—è¯¥æŸ¥è¯¢ä¸mini-batchä¸­æ‰€æœ‰å…¶ä»–æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚é€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼ŒGCLå¯ä»¥å­¦ä¹ åˆ°æ›´åŠ é²æ£’å’Œæ³›åŒ–çš„è¡¨ç¤ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGCLä¸éœ€è¦æ„å»ºæ–°çš„æ•°æ®é›†ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå„ç§è·¨æ¨¡æ€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šGCLçš„å…³é”®è®¾è®¡åœ¨äºå¯¹æ¯”æŸå¤±å‡½æ•°çš„å…·ä½“å½¢å¼ã€‚è®ºæ–‡ä¸­ä½¿ç”¨çš„å¯¹æ¯”æŸå¤±å‡½æ•°æ˜¯åŸºäºInfoNCEæŸå¤±çš„å˜ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºmini-batchä¸­çš„æ¯ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹(i, t)ï¼ŒGCLè®¡ç®—å›¾åƒiå’Œæ–‡æœ¬tä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä»¥åŠå›¾åƒiå’Œmini-batchä¸­æ‰€æœ‰å…¶ä»–æ–‡æœ¬çš„ç›¸ä¼¼åº¦ã€‚ç„¶åï¼ŒGCLä½¿ç”¨softmaxå‡½æ•°å°†è¿™äº›ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¡¡é‡é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå’ŒçœŸå®æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼ŒGCLè¿˜å¼•å…¥äº†ä¸€ä¸ªæ¸©åº¦å‚æ•°æ¥æ§åˆ¶softmaxå‡½æ•°çš„é”åº¦ï¼Œä»è€Œå½±å“å¯¹æ¯”å­¦ä¹ çš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGCLåœ¨M-BEIRã€MMEBå’ŒCoVRç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼Œæ˜¾è‘—æå‡äº†VISTAã€CLIPå’ŒTinyCLIPç­‰ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨M-BEIRæ•°æ®é›†ä¸Šï¼ŒGCLå°†CLIPçš„æ£€ç´¢å‡†ç¡®ç‡æé«˜äº†5%ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¯æ˜äº†GCLçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œä¾‹å¦‚å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†é¢‘-æ–‡æœ¬æ£€ç´¢ç­‰ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ£€ç´¢ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å„ç§æ¨¡æ€ç»„åˆçš„æŸ¥è¯¢ï¼Œä»è€Œæé«˜æ£€ç´¢æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨ä¸€å¼ å›¾ç‰‡æˆ–è€…ä¸€æ®µæ–‡å­—æ¥æ£€ç´¢åŒ…å«ç›¸å…³ä¿¡æ¯çš„ç½‘é¡µã€æ–‡æ¡£æˆ–è§†é¢‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their consistent performance improvements, cross-modal retrieval models (e.g., CLIP) show degraded performances with retrieving keys composed of fused image-text modality (e.g., Wikipedia pages with both images and text). To address this critical challenge, multimodal retrieval has been recently explored to develop a unified single retrieval model capable of retrieving keys across diverse modality combinations. A common approach involves constructing new composed sets of image-text triplets (e.g., retrieving a pair of image and text given a query image). However, such an approach requires careful curation to ensure the dataset quality and fails to generalize to unseen modality combinations. To overcome these limitations, this paper proposes Generalized Contrastive Learning (GCL), a novel loss formulation that improves multimodal retrieval performance without the burdensome need for new dataset curation. Specifically, GCL operates by enforcing contrastive learning across all modalities within a mini-batch, utilizing existing image-caption paired datasets to learn a unified representation space. We demonstrate the effectiveness of GCL by showing consistent performance improvements on off-the-shelf multimodal retrieval models (e.g., VISTA, CLIP, and TinyCLIP) using the M-BEIR, MMEB, and CoVR benchmarks.

