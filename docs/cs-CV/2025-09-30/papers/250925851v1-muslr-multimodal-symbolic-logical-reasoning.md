---
layout: default
title: MuSLR: Multimodal Symbolic Logical Reasoning
---

# MuSLR: Multimodal Symbolic Logical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25851" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25851v1</a>
  <a href="https://arxiv.org/pdf/2509.25851.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25851v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25851v1', 'MuSLR: Multimodal Symbolic Logical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jundong Xu, Hao Fei, Yuhui Zhang, Liangming Pan, Qijun Huang, Qian Liu, Preslav Nakov, Min-Yen Kan, William Yang Wang, Mong-Li Lee, Wynne Hsu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://llm-symbol.github.io/MuSLR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMuSLRåŸºå‡†æµ‹è¯•å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œå¹¶æå‡ºLogiCAMæ¡†æ¶æå‡æ¨ç†æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç¬¦å·é€»è¾‘æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `å½¢å¼é€»è¾‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³é«˜é£é™©åº”ç”¨çš„éœ€æ±‚ã€‚
2. æå‡ºLogiCAMæ¡†æ¶ï¼Œé€šè¿‡æ¨¡å—åŒ–åœ°åº”ç”¨å½¢å¼é€»è¾‘è§„åˆ™æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLogiCAMæ¡†æ¶æ˜¾è‘—æå‡äº†GPT-4.1åœ¨MuSLRåŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é€»è¾‘æ¨ç†ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†æ—¨åœ¨é€šè¿‡å½¢å¼é€»è¾‘ä»å¤šæ¨¡æ€è¾“å…¥ä¸­æ¨å¯¼å‡ºæ–°çš„äº‹å®ï¼Œè¿™åœ¨è‡ªåŠ¨é©¾é©¶å’ŒåŒ»ç–—è¯Šæ–­ç­‰é«˜é£é™©åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºå…¶ä¸¥è°¨ã€ç¡®å®šæ€§çš„æ¨ç†æœ‰åŠ©äºé¿å…ä¸¥é‡åæœã€‚ä¸ºäº†è¯„ä¼°å½“å‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªåŸºå‡†MuSLRï¼Œç”¨äºåŸºäºå½¢å¼é€»è¾‘è§„åˆ™çš„å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†ã€‚MuSLRåŒ…å«7ä¸ªé¢†åŸŸçš„1,093ä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬35ä¸ªåŸå­ç¬¦å·é€»è¾‘å’Œ976ä¸ªé€»è¾‘ç»„åˆï¼Œæ¨ç†æ·±åº¦ä»2åˆ°9ä¸ç­‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†7ä¸ªæœ€å…ˆè¿›çš„VLMåœ¨MuSLRä¸Šçš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬éƒ½åœ¨å¤šæ¨¡æ€ç¬¦å·æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæœ€å¥½çš„æ¨¡å‹GPT-4.1ä»…è¾¾åˆ°46.8%çš„å‡†ç¡®ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LogiCAMï¼Œä¸€ä¸ªå°†å½¢å¼é€»è¾‘è§„åˆ™åº”ç”¨äºå¤šæ¨¡æ€è¾“å…¥ï¼Œä»è€Œæå‡GPT-4.1çš„Chain-of-Thoughtæ€§èƒ½14.13%çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œå¹¶ä¸”åœ¨è¯¸å¦‚ä¸€é˜¶é€»è¾‘ç­‰å¤æ‚é€»è¾‘ä¸Šå®ç°äº†æ›´å¤§çš„æ”¶ç›Šã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„é”™è¯¯åˆ†æï¼Œè¡¨æ˜å¤§çº¦70%çš„å¤±è´¥æºäºæ¨¡æ€ä¹‹é—´çš„é€»è¾‘ä¸å¯¹é½ï¼Œä¸ºæŒ‡å¯¼æœªæ¥çš„æ”¹è¿›æä¾›äº†å…³é”®è§è§£ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å‡å¯åœ¨https://llm-symbol.github.io/MuSLRå…¬å¼€è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰VLMéš¾ä»¥æœ‰æ•ˆåœ°å°†è§†è§‰ä¿¡æ¯ä¸ç¬¦å·é€»è¾‘è§„åˆ™ç›¸ç»“åˆï¼Œå¯¼è‡´æ¨ç†å‡†ç¡®ç‡ä½ï¼Œæ— æ³•æ»¡è¶³è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é«˜é£é™©åœºæ™¯çš„éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´é€»è¾‘å…³ç³»çš„æœ‰æ•ˆå»ºæ¨¡ï¼Œå®¹æ˜“å‡ºç°é€»è¾‘è¯¯åˆ¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä¸€ä¸ªæ¨¡å—åŒ–çš„æ¡†æ¶LogiCAMï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†å½¢å¼é€»è¾‘è§„åˆ™åº”ç”¨äºå¤šæ¨¡æ€è¾“å…¥ï¼Œä»è€Œå¢å¼ºVLMçš„æ¨ç†èƒ½åŠ›ã€‚LogiCAMé€šè¿‡å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªé€»è¾‘æ­¥éª¤ï¼Œå¹¶åˆ©ç”¨å½¢å¼é€»è¾‘è§„åˆ™è¿›è¡Œçº¦æŸï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¿™ç§æ˜¾å¼å»ºæ¨¡é€»è¾‘å…³ç³»çš„æ–¹æ³•æœ‰åŠ©äºå‡å°‘æ¨¡æ€ä¹‹é—´çš„é€»è¾‘ä¸å¯¹é½é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLogiCAMæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥ç¼–ç æ¨¡å—ï¼šç”¨äºå°†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ç¼–ç ä¸ºå‘é‡è¡¨ç¤ºã€‚2) é€»è¾‘è§„åˆ™è§£ææ¨¡å—ï¼šç”¨äºè§£æå½¢å¼é€»è¾‘è§„åˆ™ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„è®¡ç®—å›¾ã€‚3) æ¨ç†æ‰§è¡Œæ¨¡å—ï¼šæ ¹æ®è®¡ç®—å›¾ï¼Œé€æ­¥æ‰§è¡Œé€»è¾‘æ¨ç†ï¼Œå¹¶ç”Ÿæˆæ–°çš„äº‹å®ã€‚4) ç»“æœéªŒè¯æ¨¡å—ï¼šç”¨äºéªŒè¯æ¨ç†ç»“æœçš„æ­£ç¡®æ€§ï¼Œå¹¶è¿›è¡Œé”™è¯¯çº æ­£ã€‚æ•´ä¸ªæµç¨‹é¦–å…ˆå¯¹å¤šæ¨¡æ€è¾“å…¥è¿›è¡Œç¼–ç ï¼Œç„¶åè§£æé€»è¾‘è§„åˆ™ï¼Œæ‰§è¡Œæ¨ç†ï¼Œæœ€åéªŒè¯ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šLogiCAMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–çš„è®¾è®¡å’Œå¯¹å½¢å¼é€»è¾‘è§„åˆ™çš„æ˜¾å¼åº”ç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLogiCAMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å½¢å¼é€»è¾‘è§„åˆ™æ¥çº¦æŸæ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼ŒLogiCAMçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—å…¶æ˜“äºæ‰©å±•å’Œå®šåˆ¶ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œé€»è¾‘è§„åˆ™ã€‚

**å…³é”®è®¾è®¡**ï¼šLogiCAMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4.1ï¼‰ä½œä¸ºåŸºç¡€ç¼–ç å™¨ã€‚2) è®¾è®¡äº†ä¸€ç§æ–°çš„é€»è¾‘è§„åˆ™è§£ææ–¹æ³•ï¼Œå¯ä»¥å°†å½¢å¼é€»è¾‘è§„åˆ™è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„è®¡ç®—å›¾ã€‚3) é‡‡ç”¨Chain-of-Thought (CoT) promptingç­–ç•¥å¼•å¯¼æ¨¡å‹è¿›è¡Œé€æ­¥æ¨ç†ã€‚4) ä½¿ç”¨ä¸€ç§åŸºäºè§„åˆ™çš„éªŒè¯æ–¹æ³•æ¥éªŒè¯æ¨ç†ç»“æœçš„æ­£ç¡®æ€§ã€‚è®ºæ–‡æ²¡æœ‰è¯¦ç»†è¯´æ˜æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œå¯èƒ½ä½¿ç”¨äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å’ŒTransformerç½‘ç»œç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLogiCAMæ¡†æ¶æ˜¾è‘—æå‡äº†GPT-4.1åœ¨MuSLRåŸºå‡†ä¸Šçš„æ€§èƒ½ï¼ŒCoTæ€§èƒ½æå‡äº†14.13%ã€‚å°¤å…¶æ˜¯åœ¨å¤æ‚é€»è¾‘æ¨ç†ï¼ˆå¦‚ä¸€é˜¶é€»è¾‘ï¼‰ä¸­ï¼ŒLogiCAMçš„æå‡æ›´ä¸ºæ˜¾è‘—ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œå¤§çº¦70%çš„å¤±è´¥æºäºæ¨¡æ€ä¹‹é—´çš„é€»è¾‘ä¸å¯¹é½ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€æ™ºèƒ½é—®ç­”ç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚å›¾åƒã€æ¿€å…‰é›·è¾¾æ•°æ®ï¼‰å’Œé€»è¾‘è§„åˆ™è¿›è¡Œåœºæ™¯ç†è§£å’Œå†³ç­–ã€‚åœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼Œå¯ä»¥ç»“åˆåŒ»å­¦å½±åƒå’Œç—…å†ä¿¡æ¯è¿›è¡Œç–¾ç—…è¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæé«˜äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, as its rigorous, deterministic reasoning helps prevent serious consequences. To evaluate such capabilities of current state-of-the-art vision language models (VLMs), we introduce the first benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35 atomic symbolic logic and 976 logical combinations, with reasoning depths ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that they all struggle with multimodal symbolic reasoning, with the best model, GPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, boosting GPT-4.1's Chain-of-Thought performance by 14.13%, and delivering even larger gains on complex logics such as first-order logic. We also conduct a comprehensive error analysis, showing that around 70% of failures stem from logical misalignment between modalities, offering key insights to guide future improvements. All data and code are publicly available at https://llm-symbol.github.io/MuSLR.

