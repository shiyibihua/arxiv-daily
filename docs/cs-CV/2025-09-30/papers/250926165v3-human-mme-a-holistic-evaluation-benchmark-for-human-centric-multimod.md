---
layout: default
title: Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models
---

# Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26165" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26165v3</a>
  <a href="https://arxiv.org/pdf/2509.26165.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26165v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26165v3', 'Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuansen Liu, Haiming Tang, Jinlong Peng, Jiangning Zhang, Xiaozhong Ji, Qingdong He, Wenbin Wu, Donghao Luo, Zhenye Gan, Junwei Zhu, Yunhang Shen, Chaoyou Fu, Chengjie Wang, Xiaobin Hu, Shuicheng Yan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-15)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Yuan-Hou/Human-MME)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHuman-MMEåŸºå‡†ï¼Œç”¨äºå…¨é¢è¯„ä¼°ä»¥äººä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ä»¥äººä¸ºä¸­å¿ƒ` `è¯„ä¼°åŸºå‡†` `åœºæ™¯ç†è§£` `äººä½“æ„ŸçŸ¥` `å› æœæ¨ç†` `æ•°æ®æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ç†è§£ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯æ–¹é¢èƒ½åŠ›ä¸è¶³ï¼Œç¼ºä¹åŒæ—¶è€ƒè™‘ç»†ç²’åº¦å’Œé«˜ç»´æ¨ç†çš„å…¨é¢è¯„ä¼°åŸºå‡†ã€‚
2. æå‡ºHuman-MMEåŸºå‡†ï¼Œé€šè¿‡å¤šæ ·åŒ–çš„äººç±»åœºæ™¯ã€æ¸è¿›çš„è¯„ä¼°ç»´åº¦å’Œé«˜è´¨é‡çš„æ ‡æ³¨ï¼Œå®ç°æ›´å…¨é¢çš„è¯„ä¼°ã€‚
3. å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰MLLMåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„å›¾åƒç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç†è§£ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯çš„èƒ½åŠ›å¾ˆå°‘è¢«æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œè¿™äº›åŸºå‡†æ—¢è€ƒè™‘äº†ä»¥äººä¸ºæœ¬çš„ç»†ç²’åº¦çº§åˆ«ï¼Œåˆè€ƒè™‘äº†æ›´é«˜ç»´åº¦çš„å› æœæ¨ç†èƒ½åŠ›ã€‚é‰´äºäººä½“çš„ç‰©ç†å¤æ‚æ€§å’Œæ ‡æ³¨ç»†ç²’åº¦ç»“æ„çš„éš¾åº¦ï¼Œè¿™ç§é«˜è´¨é‡çš„è¯„ä¼°åŸºå‡†é¢ä¸´ç€ä¸¥å³»çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Human-MMEï¼Œä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨ä¸ºMLLMåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ç†è§£ä¸­æä¾›æ›´å…¨é¢çš„è¯„ä¼°ã€‚ä¸å…¶ä»–ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼š1. äººç±»åœºæ™¯çš„å¤šæ ·æ€§ï¼Œè·¨è¶Š4ä¸ªä¸»è¦è§†è§‰é¢†åŸŸï¼ŒåŒ…å«15ä¸ªäºŒçº§é¢†åŸŸå’Œ43ä¸ªå­é¢†åŸŸï¼Œä»¥ç¡®ä¿å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚2. æ¸è¿›å’Œå¤šæ ·åŒ–çš„è¯„ä¼°ç»´åº¦ï¼Œä»ä»¥äººä¸ºæœ¬çš„ç»†ç²’åº¦æ„ŸçŸ¥åˆ°æ›´é«˜ç»´åº¦çš„æ¨ç†ï¼Œé€æ­¥è¯„ä¼°åŸºäºäººç±»çš„æ´»åŠ¨ï¼ŒåŒ…æ‹¬å…«ä¸ªç»´åº¦ï¼ŒåŒ…å«19,945ä¸ªçœŸå®ä¸–ç•Œçš„å›¾åƒé—®é¢˜å¯¹å’Œä¸€ä¸ªè¯„ä¼°å¥—ä»¶ã€‚3. é«˜è´¨é‡çš„æ³¨é‡Šä¸ä¸°å¯Œçš„æ•°æ®èŒƒå¼ï¼Œæ„å»ºè‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“å’Œäººå·¥æ³¨é‡Šå¹³å°ï¼Œæ”¯æŒä¸¥æ ¼çš„äººå·¥æ ‡æ³¨ï¼Œä»¥ä¿ƒè¿›ç²¾ç¡®å’Œå¯é çš„æ¨¡å‹è¯„ä¼°ã€‚æˆ‘ä»¬çš„åŸºå‡†é€šè¿‡æ„å»ºé€‰æ‹©ã€ç®€ç­”ã€å®šä½ã€æ’åºå’Œåˆ¤æ–­é—®é¢˜ç»„ä»¶ä»¥åŠå®ƒä»¬çš„ç»„åˆçš„å¤æ‚é—®é¢˜ï¼Œå°†å•ç›®æ ‡ç†è§£æ‰©å±•åˆ°å¤šäººå’Œå¤šå›¾åƒçš„ç›¸äº’ç†è§£ã€‚å¯¹17ä¸ªæœ€å…ˆè¿›çš„MLLMçš„å¹¿æ³›å®éªŒæœ‰æ•ˆåœ°æ­ç¤ºäº†å±€é™æ€§ï¼Œå¹¶æŒ‡å¯¼æœªæ¥çš„MLLMç ”ç©¶æœç€æ›´å¥½çš„äººç±»ä¸­å¿ƒå›¾åƒç†è§£å‘å±•ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç éƒ½å¯ä»¥åœ¨https://github.com/Yuan-Hou/Human-MMEä¸Šæ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹ä¸€ä¸ªç»¼åˆæ€§çš„è¯„ä¼°åŸºå‡†æ¥è¡¡é‡æ¨¡å‹åœ¨ç»†ç²’åº¦äººä½“æ„ŸçŸ¥å’Œé«˜å±‚æ¬¡å› æœæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æ•°æ®é›†é€šå¸¸å…³æ³¨é€šç”¨åœºæ™¯ï¼Œå¿½ç•¥äº†äººä½“å§¿æ€ã€åŠ¨ä½œã€äº¤äº’ç­‰å¤æ‚å› ç´ ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHuman-MMEçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå…¨é¢ã€å¤šæ ·ã€é«˜è´¨é‡çš„è¯„ä¼°åŸºå‡†ï¼Œè¦†ç›–å„ç§ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ï¼Œå¹¶è®¾è®¡å¤šç»´åº¦çš„è¯„ä¼°ä»»åŠ¡ï¼Œä»ç»†ç²’åº¦æ„ŸçŸ¥åˆ°é«˜å±‚æ¬¡æ¨ç†ï¼Œé€æ­¥è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡è‡ªåŠ¨åŒ–æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨ç›¸ç»“åˆçš„æ–¹å¼ï¼Œä¿è¯æ•°æ®çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHuman-MMEåŸºå‡†çš„æ„å»ºåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1. æ•°æ®æ”¶é›†ï¼šæ”¶é›†æ¶µç›–4ä¸ªä¸»è¦è§†è§‰é¢†åŸŸï¼ˆå¦‚æ—¥å¸¸ç”Ÿæ´»ã€åŒ»ç–—ä¿å¥ã€ä½“è‚²è¿åŠ¨ç­‰ï¼‰å’Œ15ä¸ªäºŒçº§é¢†åŸŸã€43ä¸ªå­é¢†åŸŸçš„å›¾åƒæ•°æ®ï¼Œç¡®ä¿åœºæ™¯çš„å¤šæ ·æ€§ã€‚2. é—®é¢˜è®¾è®¡ï¼šè®¾è®¡å…«ä¸ªç»´åº¦çš„è¯„ä¼°ä»»åŠ¡ï¼ŒåŒ…æ‹¬äººä½“æ„ŸçŸ¥ã€åŠ¨ä½œè¯†åˆ«ã€å…³ç³»æ¨ç†ã€å› æœæ¨ç†ç­‰ï¼Œæ¶µç›–é€‰æ‹©é¢˜ã€ç®€ç­”é¢˜ã€å®šä½é¢˜ã€æ’åºé¢˜å’Œåˆ¤æ–­é¢˜ç­‰å¤šç§é¢˜å‹ã€‚3. æ•°æ®æ ‡æ³¨ï¼šæ„å»ºè‡ªåŠ¨åŒ–æ ‡æ³¨ç®¡é“å’Œäººå·¥æ ‡æ³¨å¹³å°ï¼Œå¯¹å›¾åƒæ•°æ®è¿›è¡Œç»†è‡´çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬äººä½“å§¿æ€ã€åŠ¨ä½œã€å¯¹è±¡å…³ç³»ç­‰ä¿¡æ¯ã€‚4. åŸºå‡†è¯„ä¼°ï¼šä½¿ç”¨Human-MMEåŸºå‡†è¯„ä¼°ç°æœ‰çš„MLLMæ¨¡å‹ï¼Œåˆ†æå…¶åœ¨ä¸åŒç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ­ç¤ºå…¶å±€é™æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šHuman-MMEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…¨é¢æ€§å’Œå¤šæ ·æ€§ã€‚å®ƒä¸ä»…è¦†ç›–äº†å¹¿æ³›çš„ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ï¼Œè¿˜è®¾è®¡äº†å¤šç»´åº¦çš„è¯„ä¼°ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°MLLMåœ¨äººä½“æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHuman-MMEè¿˜é‡‡ç”¨äº†è‡ªåŠ¨åŒ–æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨ç›¸ç»“åˆçš„æ–¹å¼ï¼Œä¿è¯äº†æ•°æ®çš„è´¨é‡å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šHuman-MMEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. å¤šæ ·åŒ–çš„åœºæ™¯é€‰æ‹©ï¼šè¦†ç›–4ä¸ªä¸»è¦è§†è§‰é¢†åŸŸã€15ä¸ªäºŒçº§é¢†åŸŸå’Œ43ä¸ªå­é¢†åŸŸï¼Œç¡®ä¿åœºæ™¯çš„å¤šæ ·æ€§ã€‚2. å¤šç»´åº¦çš„è¯„ä¼°ä»»åŠ¡ï¼šè®¾è®¡å…«ä¸ªç»´åº¦çš„è¯„ä¼°ä»»åŠ¡ï¼Œä»ç»†ç²’åº¦æ„ŸçŸ¥åˆ°é«˜å±‚æ¬¡æ¨ç†ï¼Œé€æ­¥è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚3. æ··åˆæ ‡æ³¨ç­–ç•¥ï¼šé‡‡ç”¨è‡ªåŠ¨åŒ–æ ‡æ³¨å’Œäººå·¥æ ‡æ³¨ç›¸ç»“åˆçš„æ–¹å¼ï¼Œä¿è¯æ•°æ®çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚4. å¤šç§é¢˜å‹è®¾è®¡ï¼šæ¶µç›–é€‰æ‹©é¢˜ã€ç®€ç­”é¢˜ã€å®šä½é¢˜ã€æ’åºé¢˜å’Œåˆ¤æ–­é¢˜ç­‰å¤šç§é¢˜å‹ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å¯¹17ä¸ªæœ€å…ˆè¿›çš„MLLMè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„å›¾åƒç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æ˜æ˜¾ä½äºäººç±»æ°´å¹³ã€‚Human-MMEåŸºå‡†çš„è¯„ä¼°ç»“æœä¸ºæœªæ¥çš„MLLMç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Human-MMEåŸºå‡†å¯å¹¿æ³›åº”ç”¨äºè¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€äººæœºäº¤äº’ã€åŒ»ç–—è¯Šæ–­ã€è¿åŠ¨åˆ†æç­‰é¢†åŸŸã€‚è¯¥åŸºå‡†èƒ½å¤Ÿä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä¸ºäººä»¬çš„ç”Ÿæ´»å¸¦æ¥ä¾¿åˆ©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks. However, their capacity to comprehend human-centric scenes has rarely been explored, primarily due to the absence of comprehensive evaluation benchmarks that take into account both the human-oriented granular level and higher-dimensional causal reasoning ability. Such high-quality evaluation benchmarks face tough obstacles, given the physical complexity of the human body and the difficulty of annotating granular structures. In this paper, we propose Human-MME, a curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric scene understanding. Compared with other existing benchmarks, our work provides three key features: 1. Diversity in human scene, spanning 4 primary visual domains with 15 secondary domains and 43 sub-fields to ensure broad scenario coverage. 2. Progressive and diverse evaluation dimensions, evaluating the human-based activities progressively from the human-oriented granular perception to the higher-dimensional reasoning, consisting of eight dimensions with 19,945 real-world image question pairs and an evaluation suite. 3. High-quality annotations with rich data paradigms, constructing the automated annotation pipeline and human-annotation platform, supporting rigorous manual labeling to facilitate precise and reliable model assessment. Our benchmark extends the single-target understanding to the multi-person and multi-image mutual understanding by constructing the choice, short-answer, grounding, ranking and judgment question components, and complex questions of their combination. The extensive experiments on 17 state-of-the-art MLLMs effectively expose the limitations and guide future MLLMs research toward better human-centric image understanding. All data and code are available at https://github.com/Yuan-Hou/Human-MME.

