---
layout: default
title: MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval
---

# MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26378" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26378v1</a>
  <a href="https://arxiv.org/pdf/2509.26378.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26378v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26378v1', 'MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junjie Zhou, Ze Liu, Lei Xiong, Jin-Ge Yao, Yueze Wang, Shitao Xiao, Fenfen Lin, Miguel Hu Chen, Zhicheng Dou, Siqi Bao, Defu Lian, Yongping Xiong, Zheng Liu

**åˆ†ç±»**: cs.IR, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/VectorSpaceLab/MR2-Bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMR$^2$-Benchï¼Œä¸€ä¸ªé¢å‘å¤šæ¨¡æ€æ£€ç´¢æ¨ç†èƒ½åŠ›çš„ç»¼åˆæ€§è¯„æµ‹åŸºå‡†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `æ¨ç†èƒ½åŠ›` `è¯„æµ‹åŸºå‡†` `æ•°æ®é›†` `è§†è§‰æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†ä¾§é‡äºæµ…å±‚è¯­ä¹‰åŒ¹é…ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸­è¿›è¡Œæ·±å±‚æ¨ç†çš„èƒ½åŠ›ã€‚
2. MR$^2$-Benché€šè¿‡æ¨ç†é©±åŠ¨çš„ä»»åŠ¡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®å’Œå¤æ‚çš„æŸ¥è¯¢æ–‡æ¡£ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰SOTAæ¨¡å‹åœ¨MR$^2$-Benchä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†è¯¥åŸºå‡†çš„æŒ‘æˆ˜æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ£€ç´¢æ­£æˆä¸ºç°ä»£äººå·¥æ™ºèƒ½åº”ç”¨çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä½†å…¶è¯„ä¼°æ»åäºæ›´çœŸå®å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯éœ€æ±‚ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦æ¢æµ‹è¡¨é¢è¯­ä¹‰å¯¹åº”ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡-æ–‡æœ¬åŒ¹é…ï¼‰ï¼Œè€Œæœªèƒ½è¯„ä¼°æ•è·è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´å¤æ‚å…³ç³»æ‰€éœ€çš„æ›´æ·±å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MR$^2$-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨ç†å¯†é›†å‹çš„å¤šæ¨¡æ€æ£€ç´¢åŸºå‡†ã€‚MR$^2$-Bench å…·æœ‰ä»¥ä¸‹å…³é”®ä»·å€¼ï¼š1) æ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯æ¨ç†é©±åŠ¨çš„ï¼Œè¶…è¶Šäº†æµ…å±‚åŒ¹é…ï¼Œæœ‰æ•ˆåœ°è¯„ä¼°äº†æ¨¡å‹è¿›è¡Œé€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†çš„èƒ½åŠ›ï¼›2) å®ƒå…·æœ‰å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®ï¼Œä¾‹å¦‚è‡ªç„¶å›¾åƒã€å›¾è¡¨å’Œè§†è§‰è°œé¢˜ï¼Œä»è€Œèƒ½å¤Ÿè·¨å†…å®¹ç±»å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼›3) å®ƒæ”¯æŒåŒ…å«å¤šä¸ªå›¾åƒçš„å¤æ‚æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œå¹¶æ¶µç›–å¤šæ ·åŒ–çš„æ£€ç´¢åœºæ™¯ï¼Œæ›´å‡†ç¡®åœ°åæ˜ äº†çœŸå®ä¸–ç•Œçš„åº”ç”¨ã€‚æˆ‘ä»¬çš„åŸºå‡†åŒ…å« 1,309 ä¸ªç²¾å¿ƒç­–åˆ’çš„æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢æ¥è‡ªæ‰‹åŠ¨æ”¶é›†å’Œæ³¨é‡Šæˆ–é€‰æ‹©æ€§åœ°æ•´åˆå…¬å…±æ•°æ®é›†ã€‚å°½ç®¡åœ¨ç°æœ‰åŸºå‡†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ MR$^2$-Bench ä¸Šä»ç„¶è¡¨ç°ä¸ä½³ï¼šä¾‹å¦‚ï¼Œé¢†å…ˆçš„ Seed1.6-Embedding æ¨¡å‹åœ¨ MMEB ä¸Šçš„ Recall@1 ä¸º 77.78ï¼Œä½†åœ¨ MR$^2$-Bench ä¸Šä»…ä¸º 9.91ã€‚è¿™ç§å·¨å¤§çš„æ€§èƒ½å·®è·å‡¸æ˜¾äº†æˆ‘ä»¬çš„åŸºå‡†å¸¦æ¥çš„æ›´å¤§æŒ‘æˆ˜ï¼Œä»¥åŠå¯¹æ¨ç†å¯†é›†å‹å¤šæ¨¡æ€æ£€ç´¢çš„è¿›ä¸€æ­¥å‘å±•çš„è¿«åˆ‡éœ€æ±‚ã€‚æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å°†åœ¨ https://github.com/VectorSpaceLab/MR2-Bench ä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†åœ¨å¤šæ¨¡æ€æ£€ç´¢è¯„ä¼°ä¸­æ— æ³•æœ‰æ•ˆè¡¡é‡æ¨¡å‹æ·±å±‚æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹è±¡-æ–‡æœ¬åŒ¹é…ç­‰æµ…å±‚è¯­ä¹‰å¯¹åº”ï¼Œå¿½ç•¥äº†é€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†ç­‰é«˜çº§è®¤çŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ›´å¤æ‚çš„çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ¨ç†å¯†é›†å‹çš„å¤šæ¨¡æ€æ£€ç´¢åŸºå‡†ï¼Œå³MR$^2$-Benchã€‚è¯¥åŸºå‡†é€šè¿‡è®¾è®¡éœ€è¦é€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†çš„ä»»åŠ¡ï¼Œä»¥åŠåŒ…å«å¤šæ ·åŒ–å¤šæ¨¡æ€æ•°æ®å’Œå¤æ‚æŸ¥è¯¢æ–‡æ¡£çš„åœºæ™¯ï¼Œæ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†æ›´çœŸå®åœ°åæ˜ å®é™…åº”ç”¨ä¸­å¤šæ¨¡æ€æ£€ç´¢çš„éœ€æ±‚ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMR$^2$-Benchçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1) æ•°æ®é›†æ„å»ºï¼šé€šè¿‡æ‰‹åŠ¨æ”¶é›†ã€æ³¨é‡Šå’Œæ•´åˆå…¬å…±æ•°æ®é›†ï¼Œæ„å»ºåŒ…å«1309ä¸ªæŸ¥è¯¢çš„æ•°æ®é›†ï¼Œæ¶µç›–è‡ªç„¶å›¾åƒã€å›¾è¡¨å’Œè§†è§‰è°œé¢˜ç­‰å¤šç§æ¨¡æ€æ•°æ®ã€‚2) ä»»åŠ¡è®¾è®¡ï¼šè®¾è®¡éœ€è¦é€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†çš„å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ï¼Œä¾‹å¦‚æ ¹æ®å›¾åƒæ¨ç†æ–‡æœ¬æè¿°ï¼Œæˆ–æ ¹æ®æ–‡æœ¬æè¿°æ£€ç´¢ç›¸å…³å›¾åƒã€‚3) è¯„ä¼°æŒ‡æ ‡ï¼šé‡‡ç”¨Recall@Kç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åˆ†ææ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMR$^2$-Benchçš„æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶æ¨ç†å¯†é›†å‹çš„ä»»åŠ¡è®¾è®¡ã€‚ä¸ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨æµ…å±‚è¯­ä¹‰åŒ¹é…ä¸åŒï¼ŒMR$^2$-Benchçš„ä»»åŠ¡éœ€è¦æ¨¡å‹è¿›è¡Œæ·±å±‚çš„é€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMR$^2$-Benchè¿˜åŒ…å«äº†å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®å’Œå¤æ‚çš„æŸ¥è¯¢æ–‡æ¡£ï¼Œæ›´çœŸå®åœ°åæ˜ äº†å®é™…åº”ç”¨åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šMR$^2$-Benchçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®é›†çš„é€‰æ‹©å’Œæ ‡æ³¨ï¼šé€‰æ‹©åŒ…å«ä¸°å¯Œæ¨ç†ä¿¡æ¯çš„å›¾åƒã€å›¾è¡¨å’Œè§†è§‰è°œé¢˜ï¼Œå¹¶è¿›è¡Œç²¾ç»†çš„æ ‡æ³¨ï¼Œä»¥ç¡®ä¿ä»»åŠ¡çš„éš¾åº¦å’Œå‡†ç¡®æ€§ã€‚2) ä»»åŠ¡çš„è®¾è®¡ï¼šè®¾è®¡éœ€è¦é€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ ¹æ®å›¾åƒæ¨ç†æ–‡æœ¬æè¿°ï¼Œæˆ–æ ¹æ®æ–‡æœ¬æè¿°æ£€ç´¢ç›¸å…³å›¾åƒã€‚3) è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼šé‡‡ç”¨Recall@Kç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åˆ†ææ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨MR$^2$-Benchä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼Œé¢†å…ˆçš„Seed1.6-Embeddingæ¨¡å‹åœ¨MMEBä¸Šçš„Recall@1ä¸º77.78ï¼Œä½†åœ¨MR$^2$-Benchä¸Šä»…ä¸º9.91ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹çš„å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ä»ç„¶å­˜åœ¨å¾ˆå¤§çš„æå‡ç©ºé—´ï¼ŒMR$^2$-Benchèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MR$^2$-Benchå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿï¼Œä¾‹å¦‚æ™ºèƒ½æœç´¢å¼•æ“ã€å›¾åƒç†è§£ç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨å¥ å®šåŸºç¡€ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯ä»¥æ‰©å±•åˆ°æ›´å¤šé¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­å’Œæ•™è‚²ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal retrieval is becoming a crucial component of modern AI applications, yet its evaluation lags behind the demands of more realistic and challenging scenarios. Existing benchmarks primarily probe surface-level semantic correspondence (e.g., object-text matching) while failing to assess the deeper reasoning required to capture complex relationships between visual and textual information. To address this gap, we introduce MR$^2$-Bench, a reasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents the following critical values: 1) all tasks are reasoning-driven, going beyond shallow matching to effectively assess models' capacity for logical, spatial, and causal inference; 2) it features diverse multimodal data, such as natural images, diagrams, and visual puzzles, enabling comprehensive evaluation across content types; 3) it supports complex queries and documents containing multiple images and covers diverse retrieval scenarios, more accurately reflecting real-world applications. Our benchmark contains 1,309 curated queries, derived either from manual collection and annotation or from selective consolidation of public datasets. Despite achieving strong results on existing benchmarks, current state-of-the-art models still struggle on MR$^2$-Bench: for example, the leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but only 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the increased challenge posed by our benchmark and the pressing need for further advances in reasoning-intensive multimodal retrieval. The dataset and evaluation code will be made publicly available at https://github.com/VectorSpaceLab/MR2-Bench.

