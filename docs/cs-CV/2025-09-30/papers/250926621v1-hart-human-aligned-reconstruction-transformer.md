---
layout: default
title: HART: Human Aligned Reconstruction Transformer
---

# HART: Human Aligned Reconstruction Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26621" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26621v1</a>
  <a href="https://arxiv.org/pdf/2509.26621.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26621v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26621v1', 'HART: Human Aligned Reconstruction Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiyi Chen, Shaofei Wang, Marko Mihajlovic, Taewon Kang, Sergey Prokudin, Ming Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Project page: https://xiyichen.github.io/hart

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HARTï¼šæå‡ºä¸€ç§å¯¹é½äººä½“çš„é‡å»ºTransformerï¼Œç”¨äºç¨€ç–è§†è§’äººä½“é‡å»ºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `äººä½“é‡å»º` `Transformerç½‘ç»œ` `ç¨€ç–è§†è§’` `SMPL-Xæ¨¡å‹` `é«˜æ–¯æº…å°„` `æ–°è§†è§’åˆæˆ` `é®æŒ¡æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å®½æ¾æœè£…ã€äººä¸ç‰©ä½“çš„äº¤äº’ï¼Œä¸”å¯¹ç›¸æœºå‚æ•°æœ‰è¾ƒå¼ºå‡è®¾ï¼Œé™åˆ¶äº†çœŸå®åœºæ™¯çš„åº”ç”¨ã€‚
2. HARTé€šè¿‡é¢„æµ‹é€åƒç´ çš„3Dä¿¡æ¯å’Œäººä½“å¯¹åº”å…³ç³»ï¼Œç»“åˆé®æŒ¡æ„ŸçŸ¥æ³Šæ¾é‡å»ºï¼Œæ¢å¤å®Œæ•´ä¸”ä¸äººä½“ç»“æ„å¯¹é½çš„å‡ ä½•å½¢çŠ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHARTåœ¨æœè£…ç½‘æ ¼é‡å»ºã€SMPL-Xä¼°è®¡å’Œæ–°è§†è§’åˆæˆæ–¹é¢å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„ç¨€ç–è§†è§’äººä½“é‡å»ºæ¡†æ¶HARTã€‚ç»™å®šå°‘é‡æœªæ ¡å‡†çš„äººä½“RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼ŒHARTè¾“å‡ºä¸€ä¸ªæ°´å¯†çš„æœè£…ç½‘æ ¼ã€å¯¹é½çš„SMPL-Xäººä½“ç½‘æ ¼ä»¥åŠç”¨äºé€¼çœŸæ–°è§†è§’æ¸²æŸ“çš„é«˜æ–¯æº…å°„è¡¨ç¤ºã€‚ç°æœ‰æœè£…äººä½“é‡å»ºæ–¹æ³•è¦ä¹ˆä¼˜åŒ–å‚æ•°åŒ–æ¨¡æ¿ï¼Œå¿½ç•¥å®½æ¾æœè£…å’Œäººä¸ç‰©ä½“çš„äº¤äº’ï¼Œè¦ä¹ˆåœ¨ç®€åŒ–çš„ç›¸æœºå‡è®¾ä¸‹è®­ç»ƒéšå¼å‡½æ•°ï¼Œé™åˆ¶äº†åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒHARTé¢„æµ‹é€åƒç´ çš„3Dç‚¹äº‘å›¾ã€æ³•çº¿å’Œäººä½“å¯¹åº”å…³ç³»ï¼Œå¹¶é‡‡ç”¨é®æŒ¡æ„ŸçŸ¥çš„æ³Šæ¾é‡å»ºæ¥æ¢å¤å®Œæ•´çš„å‡ ä½•å½¢çŠ¶ï¼Œå³ä½¿åœ¨è‡ªé®æŒ¡åŒºåŸŸä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›é¢„æµ‹è¿˜ä¸å‚æ•°åŒ–çš„SMPL-Xäººä½“æ¨¡å‹å¯¹é½ï¼Œç¡®ä¿é‡å»ºçš„å‡ ä½•å½¢çŠ¶ä¸äººä½“ç»“æ„ä¿æŒä¸€è‡´ï¼ŒåŒæ—¶æ•æ‰å®½æ¾çš„æœè£…å’Œäº¤äº’ã€‚è¿™äº›äººä½“å¯¹é½çš„ç½‘æ ¼åˆå§‹åŒ–é«˜æ–¯æº…å°„ï¼Œè¿›ä¸€æ­¥å®ç°ç¨€ç–è§†è§’æ¸²æŸ“ã€‚å°½ç®¡ä»…åœ¨2.3Kä¸ªåˆæˆæ‰«æä¸Šè¿›è¡Œè®­ç»ƒï¼ŒHARTå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼šæœè£…ç½‘æ ¼é‡å»ºçš„Chamferè·ç¦»æé«˜äº†18-23%ï¼ŒSMPL-Xä¼°è®¡çš„PA-V2Vé™ä½äº†6-27%ï¼Œæ–°è§†è§’åˆæˆçš„LPIPSåœ¨å„ç§æ•°æ®é›†ä¸Šé™ä½äº†15-27%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå‰é¦ˆTransformerå¯ä»¥ä½œä¸ºä¸€ç§å¯æ‰©å±•çš„æ¨¡å‹ï¼Œç”¨äºåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œé²æ£’çš„äººä½“é‡å»ºã€‚ä»£ç å’Œæ¨¡å‹å°†ä¼šå¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœè£…äººä½“é‡å»ºæ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯åŸºäºå‚æ•°åŒ–æ¨¡æ¿çš„æ–¹æ³•éš¾ä»¥æ•æ‰å®½æ¾çš„æœè£…å’Œäººä¸ç‰©ä½“çš„äº¤äº’ï¼›äºŒæ˜¯åŸºäºéšå¼å‡½æ•°çš„æ–¹æ³•é€šå¸¸éœ€è¦ç®€åŒ–çš„ç›¸æœºå‡è®¾ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ç¨€ç–è§†è§’ä¸‹ï¼Œé²æ£’åœ°é‡å»ºå…·æœ‰å¤æ‚æœè£…å’Œäº¤äº’çš„çœŸå®äººä½“å‡ ä½•ç»“æ„æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHARTçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Transformerç½‘ç»œç›´æ¥é¢„æµ‹é€åƒç´ çš„3Dç‚¹äº‘ã€æ³•çº¿å’Œäººä½“å¯¹åº”å…³ç³»ï¼Œä»è€Œé¿å…äº†å¯¹å‚æ•°åŒ–æ¨¡æ¿çš„è¿‡åº¦ä¾èµ–å’Œå¯¹ç›¸æœºå‚æ•°çš„å¼ºå‡è®¾ã€‚é€šè¿‡å°†é¢„æµ‹çš„å‡ ä½•ä¿¡æ¯ä¸SMPL-Xäººä½“æ¨¡å‹å¯¹é½ï¼Œä¿è¯é‡å»ºç»“æœä¸äººä½“ç»“æ„çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨é®æŒ¡æ„ŸçŸ¥çš„æ³Šæ¾é‡å»ºæ¥å¤„ç†è‡ªé®æŒ¡åŒºåŸŸï¼Œæ¢å¤å®Œæ•´çš„å‡ ä½•å½¢çŠ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHARTçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨Transformerç½‘ç»œä»è¾“å…¥çš„ç¨€ç–è§†è§’RGBå›¾åƒä¸­æå–é€åƒç´ çš„ç‰¹å¾è¡¨ç¤ºã€‚2) 3Dé¢„æµ‹æ¨¡å—ï¼šåŸºäºæå–çš„ç‰¹å¾ï¼Œé¢„æµ‹æ¯ä¸ªåƒç´ çš„3Dåæ ‡ã€æ³•çº¿å’ŒSMPL-Xäººä½“å¯¹åº”å…³ç³»ã€‚3) å‡ ä½•é‡å»ºæ¨¡å—ï¼šåˆ©ç”¨é¢„æµ‹çš„3Dç‚¹äº‘å’Œæ³•çº¿ï¼Œé‡‡ç”¨é®æŒ¡æ„ŸçŸ¥çš„æ³Šæ¾é‡å»ºç®—æ³•ç”Ÿæˆæ°´å¯†çš„æœè£…ç½‘æ ¼ã€‚4) æ¸²æŸ“æ¨¡å—ï¼šä½¿ç”¨é‡å»ºçš„ç½‘æ ¼åˆå§‹åŒ–é«˜æ–¯æº…å°„ï¼Œç”¨äºæ–°è§†è§’çš„æ¸²æŸ“ã€‚

**å…³é”®åˆ›æ–°**ï¼šHARTæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶ç›´æ¥é¢„æµ‹é€åƒç´ 3Dä¿¡æ¯çš„æ¡†æ¶ï¼Œé¿å…äº†å¯¹å‚æ•°åŒ–æ¨¡æ¿çš„è¿‡åº¦ä¾èµ–ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å®½æ¾æœè£…å’Œäººä¸ç‰©ä½“çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œé®æŒ¡æ„ŸçŸ¥çš„æ³Šæ¾é‡å»ºç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è‡ªé®æŒ¡åŒºåŸŸï¼Œæ¢å¤æ›´å®Œæ•´çš„å‡ ä½•å½¢çŠ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒHARTåœ¨çœŸå®åœºæ™¯ä¸­å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šHARTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformerç½‘ç»œè¿›è¡Œç‰¹å¾æå–å’Œ3Dé¢„æµ‹ï¼Œå……åˆ†åˆ©ç”¨äº†Transformerçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ã€‚2) è®¾è®¡äº†ä¸“é—¨çš„æŸå¤±å‡½æ•°æ¥çº¦æŸé¢„æµ‹çš„3Dä¿¡æ¯ä¸SMPL-Xäººä½“æ¨¡å‹çš„ä¸€è‡´æ€§ã€‚3) é‡‡ç”¨äº†é®æŒ¡æ„ŸçŸ¥çš„æ³Šæ¾é‡å»ºç®—æ³•ï¼Œå¹¶å¯¹å…¶å‚æ•°è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°å¤„ç†è‡ªé®æŒ¡åŒºåŸŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HARTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æœè£…ç½‘æ ¼é‡å»ºæ–¹é¢ï¼ŒChamferè·ç¦»é™ä½äº†18-23%ã€‚åœ¨SMPL-Xäººä½“å§¿æ€ä¼°è®¡æ–¹é¢ï¼ŒPA-V2Vé™ä½äº†6-27%ã€‚åœ¨æ–°è§†è§’åˆæˆæ–¹é¢ï¼ŒLPIPSé™ä½äº†15-27%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHARTåœ¨æœè£…äººä½“é‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HARTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è™šæ‹Ÿç°å®/å¢å¼ºç°å®ã€æ¸¸æˆã€åŠ¨ç”»åˆ¶ä½œã€è™šæ‹Ÿè¯•è¡£ã€äººä½“å§¿æ€ä¼°è®¡ç­‰é¢†åŸŸã€‚è¯¥æŠ€æœ¯å¯ä»¥ç”¨äºåˆ›å»ºé€¼çœŸä¸”å¯äº¤äº’çš„è™šæ‹Ÿäººç‰©ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¸ºç›¸å…³åº”ç”¨æä¾›æ›´å‡†ç¡®çš„äººä½“å‡ ä½•ä¿¡æ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.

