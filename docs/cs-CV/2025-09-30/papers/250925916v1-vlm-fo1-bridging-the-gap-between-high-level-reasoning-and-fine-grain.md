---
layout: default
title: VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs
---

# VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25916" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25916v1</a>
  <a href="https://arxiv.org/pdf/2509.25916.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25916v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25916v1', 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peng Liu, Haozhan Shen, Chunxin Fang, Zhicheng Sun, Jiajia Liao, Tiancheng Zhao

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 22 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLM-FO1ï¼šé€šè¿‡ç‰¹å¾æ£€ç´¢å¼¥åˆVLMé«˜å±‚æ¨ç†ä¸ç»†ç²’åº¦æ„ŸçŸ¥ä¹‹é—´çš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç»†ç²’åº¦æ„ŸçŸ¥` `ç‰¹å¾æ£€ç´¢` `å¯¹è±¡å®šä½` `åŒºåŸŸç†è§£` `æ··åˆç¼–ç å™¨` `åŒé˜¶æ®µè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºéš¾ä»¥ç”Ÿæˆç²¾ç¡®çš„æ•°å€¼åæ ‡ã€‚
2. VLM-FO1å°†å¯¹è±¡æ„ŸçŸ¥é‡æ„ä¸ºç‰¹å¾æ£€ç´¢ä»»åŠ¡ï¼Œé¿å…äº†ç›´æ¥åæ ‡ç”Ÿæˆï¼Œæå‡äº†é²æ£’æ€§ã€‚
3. VLM-FO1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAï¼Œä¸”ä¸å½±å“VLMçš„é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)æ“…é•¿é«˜å±‚åœºæ™¯ç†è§£ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®å®šä½çš„ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™ç§å¤±è´¥æºäºæ ¹æœ¬çš„ä¸åŒ¹é…ï¼Œå› ä¸ºç”Ÿæˆç²¾ç¡®çš„æ•°å€¼åæ ‡å¯¹äºä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¶æ„æ¥è¯´æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶VLM-FO1ï¼Œé€šè¿‡å°†ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥ä»è„†å¼±çš„åæ ‡ç”Ÿæˆé—®é¢˜é‡æ–°å®šä¹‰ä¸ºé²æ£’çš„ç‰¹å¾æ£€ç´¢ä»»åŠ¡ï¼Œä»è€Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨æ¨¡å—è¿è¡Œï¼Œå¯ä»¥ä¸ä»»ä½•é¢„è®­ç»ƒçš„VLMé›†æˆã€‚å®ƒåˆ©ç”¨æ··åˆç»†ç²’åº¦åŒºåŸŸç¼–ç å™¨(HFRE)ï¼Œå…·æœ‰åŒè§†è§‰ç¼–ç å™¨ï¼Œä»¥ç”Ÿæˆå¯Œå«è¯­ä¹‰å’Œç©ºé—´ç»†èŠ‚çš„å¼ºå¤§åŒºåŸŸtokensã€‚ç„¶åï¼ŒåŸºäºtokençš„å¼•ç”¨ç³»ç»Ÿä½¿LLMèƒ½å¤Ÿæ— ç¼åœ°æ¨ç†å’Œå°†è¯­è¨€å®šä½åœ¨è¿™äº›ç‰¹å®šçš„è§†è§‰åŒºåŸŸä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒVLM-FO1åœ¨ä¸€ç³»åˆ—ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨å¯¹è±¡å®šä½ã€åŒºåŸŸç”Ÿæˆç†è§£å’Œè§†è§‰åŒºåŸŸæ¨ç†æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ç¡®ä¿äº†è¿™äº›æ„ŸçŸ¥å¢ç›Šçš„å®ç°ä¸ä¼šæŸå®³åŸºç¡€æ¨¡å‹çš„ä¸€èˆ¬è§†è§‰ç†è§£èƒ½åŠ›ã€‚VLM-FO1ä¸ºæ„å»ºå…·æœ‰æ„ŸçŸ¥èƒ½åŠ›çš„VLMå»ºç«‹äº†ä¸€ç§æœ‰æ•ˆè€Œçµæ´»çš„èŒƒä¾‹ï¼Œå¼¥åˆäº†é«˜å±‚æ¨ç†å’Œç»†ç²’åº¦è§†è§‰å®šä½ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é«˜å±‚æ¬¡çš„åœºæ™¯ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®å®šä½çš„ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ä¸­å­˜åœ¨ä¸è¶³ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºï¼Œè¯­è¨€æ¨¡å‹éš¾ä»¥ç›´æ¥ç”Ÿæˆç²¾ç¡®çš„æ•°å€¼åæ ‡ï¼Œè¿™ä½¿å¾—VLMåœ¨å¯¹è±¡å®šä½ã€åŒºåŸŸç†è§£ç­‰ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLM-FO1çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ä»åæ ‡ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºç‰¹å¾æ£€ç´¢é—®é¢˜ã€‚ä¸å†è¦æ±‚æ¨¡å‹ç›´æ¥é¢„æµ‹åæ ‡ï¼Œè€Œæ˜¯è®©æ¨¡å‹ä»è§†è§‰ç‰¹å¾ç©ºé—´ä¸­æ£€ç´¢ä¸è¯­è¨€æè¿°æœ€ç›¸å…³çš„åŒºåŸŸç‰¹å¾ã€‚è¿™ç§æ–¹æ³•æ›´ç¬¦åˆè¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¹Ÿæ›´å…·é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLM-FO1æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥é›†æˆåˆ°ä»»ä½•é¢„è®­ç»ƒçš„VLMä¸­ã€‚å…¶ä¸»è¦ç»„æˆéƒ¨åˆ†åŒ…æ‹¬ï¼š1) æ··åˆç»†ç²’åº¦åŒºåŸŸç¼–ç å™¨ï¼ˆHFREï¼‰ï¼šä½¿ç”¨åŒè§†è§‰ç¼–ç å™¨æå–åŒºåŸŸçš„è¯­ä¹‰å’Œç©ºé—´ç‰¹å¾ã€‚2) åŸºäºTokençš„å¼•ç”¨ç³»ç»Ÿï¼šå…è®¸LLMé€šè¿‡tokenså¼•ç”¨ç‰¹å®šçš„è§†è§‰åŒºåŸŸï¼Œä»è€Œè¿›è¡Œæ¨ç†å’Œå®šä½ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆHFREå¯¹å›¾åƒåŒºåŸŸè¿›è¡Œç¼–ç ï¼Œç”ŸæˆåŒºåŸŸtokensï¼›ç„¶åï¼ŒLLMç»“åˆè¯­è¨€è¾“å…¥å’ŒåŒºåŸŸtokensè¿›è¡Œæ¨ç†ï¼Œå®Œæˆå¯¹è±¡å®šä½ã€åŒºåŸŸç†è§£ç­‰ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLM-FO1æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç‰¹å¾æ£€ç´¢é—®é¢˜ã€‚è¿™ç§é‡æ„é¿å…äº†è¯­è¨€æ¨¡å‹ç›´æ¥ç”Ÿæˆåæ ‡çš„å›°éš¾ï¼Œå……åˆ†åˆ©ç”¨äº†è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒHFREçš„è®¾è®¡ä¹Ÿä¿è¯äº†åŒºåŸŸç‰¹å¾åŒ…å«ä¸°å¯Œçš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šVLM-FO1é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œè®­ç»ƒHFREï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åŒºåŸŸç‰¹å¾ã€‚ç¬¬äºŒé˜¶æ®µï¼Œå°†HFREé›†æˆåˆ°é¢„è®­ç»ƒçš„VLMä¸­ï¼Œå¹¶è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°çš„ç‰¹å¾æ£€ç´¢ä»»åŠ¡ã€‚HFREä½¿ç”¨äº†åŒè§†è§‰ç¼–ç å™¨ï¼Œåˆ†åˆ«æå–è¯­ä¹‰å’Œç©ºé—´ç‰¹å¾ï¼Œå¹¶é€šè¿‡èåˆæœºåˆ¶å°†ä¸¤è€…ç»“åˆèµ·æ¥ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°èƒ½å¤ŸåŒºåˆ†ä¸åŒåŒºåŸŸçš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VLM-FO1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è±¡å®šä½ä»»åŠ¡ä¸­ï¼ŒVLM-FO1çš„ç²¾åº¦æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†æ˜¾è‘—çš„ç™¾åˆ†æ¯”ã€‚æ­¤å¤–ï¼ŒVLM-FO1çš„é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›æ²¡æœ‰å—åˆ°å½±å“ï¼Œè¿™è¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLM-FO1å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å›¾åƒç¼–è¾‘ç­‰ã€‚å®ƒå¯ä»¥æå‡VLMåœ¨éœ€è¦ç²¾ç¡®å®šä½çš„ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€è§†è§‰é—®ç­”ç­‰ã€‚æœªæ¥ï¼ŒVLM-FO1å¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç»“åˆï¼Œä¾‹å¦‚3Dè§†è§‰ã€çŸ¥è¯†å›¾è°±ç­‰ï¼Œä»¥å®ç°æ›´å¤æ‚çš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.

