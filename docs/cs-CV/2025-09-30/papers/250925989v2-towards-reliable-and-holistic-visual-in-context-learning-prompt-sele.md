---
layout: default
title: Towards Reliable and Holistic Visual In-Context Learning Prompt Selection
---

# Towards Reliable and Holistic Visual In-Context Learning Prompt Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25989" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25989v2</a>
  <a href="https://arxiv.org/pdf/2509.25989.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25989v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25989v2', 'Towards Reliable and Holistic Visual In-Context Learning Prompt Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenxiao Wu, Jing-Hao Xue, Chengming Xu, Chen Liu, Xinwei Sun, Changxin Gao, Nong Sang, Yanwei Fu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRH-Partial2Globalï¼Œæå‡è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­prompté€‰æ‹©çš„å¯é æ€§å’Œå…¨é¢æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ` `ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©` `å…±å½¢é¢„æµ‹` `è¦†ç›–è®¾è®¡` `å°æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VICLæ–¹æ³•ä¾èµ–â€œç›¸ä¼¼æ€§ä¼˜å…ˆâ€å‡è®¾ï¼Œä½†ç¼ºä¹ç†è®ºæ”¯æ’‘ï¼Œä¸”Partial2Globalçš„éšæœºé‡‡æ ·å¯¼è‡´è¦†ç›–ä¸å…¨å’Œå†—ä½™ã€‚
2. RH-Partial2Globalåˆ©ç”¨jackknifeå…±å½¢é¢„æµ‹æ„å»ºå¯é æ›¿ä»£é›†ï¼Œå¹¶é‡‡ç”¨è¦†ç›–è®¾è®¡é‡‡æ ·ç¡®ä¿æˆå¯¹åå¥½çš„å…¨é¢è¦†ç›–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRH-Partial2Globalåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºPartial2Globalã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ (VICL)å·²æˆä¸ºä¸€ç§å°†è§†è§‰åŸºç¡€æ¨¡å‹é€‚åº”äºæ–°ä»»åŠ¡çš„é‡è¦æ–¹æ³•ï¼Œå®ƒé€šè¿‡æœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡ä¸­åµŒå…¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å®ç°ï¼Œè¿™å¯ä»¥è¢«å½¢å¼åŒ–ä¸ºæ½œåœ¨å€™é€‰è€…çš„å…¨å±€æ’åºé—®é¢˜ã€‚ç°æœ‰çš„VICLæ–¹æ³•ï¼Œå¦‚Partial2Globalå’ŒVPRï¼Œéƒ½åŸºäºç›¸ä¼¼æ€§ä¼˜å…ˆçš„å‡è®¾ï¼Œå³ä¸æŸ¥è¯¢å›¾åƒåœ¨è§†è§‰ä¸Šæ›´ç›¸ä¼¼çš„å›¾åƒå¯ä»¥ä½œä¸ºæ›´å¥½çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§åŸºæœ¬å‡è®¾ç¼ºä¹å……åˆ†çš„ç†ç”±æ¥è¯æ˜å…¶åœ¨é€‰æ‹©æœ€ä½³ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒPartial2Globalé€šè¿‡ä¸€ç³»åˆ—éšæœºé‡‡æ ·çš„æˆå¯¹åå¥½é¢„æµ‹æ¥æ„å»ºå…¶å…¨å±€æ’åºã€‚è¿™ç§å¯¹éšæœºé‡‡æ ·çš„ä¾èµ–å¯èƒ½å¯¼è‡´æ¯”è¾ƒçš„ä¸å®Œå…¨è¦†ç›–å’Œå†—ä½™é‡‡æ ·ï¼Œä»è€Œè¿›ä¸€æ­¥ä¸åˆ©åœ°å½±å“æœ€ç»ˆçš„å…¨å±€æ’åºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„Partial2Globalå˜ä½“ï¼Œæ—¨åœ¨å¯é å’Œå…¨é¢åœ°é€‰æ‹©VICLä¸­çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œç§°ä¸ºRH-Partial2Globalï¼Œåˆ©ç”¨jackknifeå…±å½¢é¢„æµ‹å¼•å¯¼ç­–ç•¥æ¥æ„å»ºå¯é çš„æ›¿ä»£é›†ï¼Œå¹¶é‡‡ç”¨åŸºäºè¦†ç›–è®¾è®¡çš„é‡‡æ ·æ–¹æ³•æ¥ç¡®ä¿æˆå¯¹åå¥½çš„å…¨é¢å’Œå‡åŒ€è¦†ç›–ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒRH-Partial2Globalå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­ä¼˜äºPartial2Globalã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆVICLï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯Partial2Globalï¼Œåœ¨é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶å­˜åœ¨é—®é¢˜ã€‚å®ƒä»¬ä¾èµ–äºâ€œç›¸ä¼¼æ€§ä¼˜å…ˆâ€çš„å‡è®¾ï¼Œå³ä¸æŸ¥è¯¢å›¾åƒæ›´ç›¸ä¼¼çš„å›¾åƒæ˜¯æ›´å¥½çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œä½†è¿™ç§å‡è®¾ç¼ºä¹å……åˆ†çš„ç†è®ºä¾æ®ã€‚æ­¤å¤–ï¼ŒPartial2Globalé€šè¿‡éšæœºé‡‡æ ·æˆå¯¹åå¥½é¢„æµ‹æ¥æ„å»ºå…¨å±€æ’åºï¼Œå¯¼è‡´æ¯”è¾ƒè¦†ç›–ä¸å…¨å’Œå†—ä½™é‡‡æ ·ï¼Œå½±å“æœ€ç»ˆæ’åºçš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRH-Partial2Globalçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ›´å¯é å’Œå…¨é¢çš„æ–¹æ³•æ¥é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œä»è€Œæé«˜VICLçš„æ€§èƒ½ã€‚å®ƒæ—¨åœ¨è§£å†³Partial2Globalä¸­å­˜åœ¨çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šç¼ºä¹å¯¹â€œç›¸ä¼¼æ€§ä¼˜å…ˆâ€å‡è®¾çš„ç†è®ºæ”¯æŒï¼Œä»¥åŠéšæœºé‡‡æ ·å¯¼è‡´çš„è¦†ç›–ä¸å…¨å’Œå†—ä½™é‡‡æ ·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRH-Partial2Globalçš„æ•´ä½“æ¡†æ¶å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨jackknifeå…±å½¢é¢„æµ‹æ„å»ºå¯é çš„æ›¿ä»£é›†ï¼›2) ä½¿ç”¨è¦†ç›–è®¾è®¡ï¼ˆcovering designï¼‰çš„é‡‡æ ·æ–¹æ³•æ¥ç¡®ä¿æˆå¯¹åå¥½çš„å…¨é¢å’Œå‡åŒ€è¦†ç›–ã€‚é¦–å…ˆï¼Œåˆ©ç”¨jackknifeå…±å½¢é¢„æµ‹æ¥è¯„ä¼°æ¯ä¸ªå€™é€‰ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„å¯é æ€§ï¼Œå¹¶æ„å»ºä¸€ä¸ªåŒ…å«å¤šä¸ªå¯é æ›¿ä»£æ–¹æ¡ˆçš„é›†åˆã€‚ç„¶åï¼Œé‡‡ç”¨è¦†ç›–è®¾è®¡æ–¹æ³•ï¼Œç³»ç»Ÿåœ°é€‰æ‹©æˆå¯¹æ¯”è¾ƒï¼Œä»¥ç¡®ä¿æ‰€æœ‰å¯èƒ½çš„åå¥½å…³ç³»éƒ½è¢«å……åˆ†è€ƒè™‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šRH-Partial2Globalçš„å…³é”®åˆ›æ–°åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯ä½¿ç”¨jackknifeå…±å½¢é¢„æµ‹æ¥è¯„ä¼°å’Œé€‰æ‹©å¯é çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œè¿™æä¾›äº†ä¸€ç§æ›´ä¸¥è°¨å’Œç†è®ºæ”¯æŒçš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ä¾èµ–ç›¸ä¼¼æ€§ï¼›äºŒæ˜¯é‡‡ç”¨è¦†ç›–è®¾è®¡æ¥æŒ‡å¯¼æˆå¯¹åå¥½çš„é‡‡æ ·ï¼Œè¿™ç¡®ä¿äº†æ‰€æœ‰å¯èƒ½çš„åå¥½å…³ç³»éƒ½è¢«å……åˆ†è€ƒè™‘ï¼Œé¿å…äº†éšæœºé‡‡æ ·å¯èƒ½å¯¼è‡´çš„è¦†ç›–ä¸å…¨å’Œå†—ä½™ã€‚ä¸Partial2Globalç›¸æ¯”ï¼ŒRH-Partial2Globalä¸å†ä¾èµ–äºæœªç»è¯å®çš„ç›¸ä¼¼æ€§å‡è®¾ï¼Œå¹¶é¿å…äº†éšæœºé‡‡æ ·å¸¦æ¥çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šRH-Partial2Globalçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) jackknifeå…±å½¢é¢„æµ‹çš„å…·ä½“å®ç°ï¼ŒåŒ…æ‹¬å¦‚ä½•è®¡ç®—p-valueå’Œæ„å»ºæ›¿ä»£é›†ï¼›2) è¦†ç›–è®¾è®¡çš„å…·ä½“ç­–ç•¥ï¼ŒåŒ…æ‹¬å¦‚ä½•é€‰æ‹©æˆå¯¹æ¯”è¾ƒä»¥ç¡®ä¿å…¨é¢è¦†ç›–ï¼›3) å¦‚ä½•å°†å¯é çš„æ›¿ä»£é›†å’Œå…¨é¢çš„åå¥½ä¿¡æ¯æ•´åˆåˆ°æœ€ç»ˆçš„å…¨å±€æ’åºä¸­ã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠä¸€äº›è¶…å‚æ•°çš„è®¾ç½®ï¼Œä¾‹å¦‚jackknifeå…±å½¢é¢„æµ‹ä¸­çš„æ˜¾è‘—æ€§æ°´å¹³ï¼Œä»¥åŠè¦†ç›–è®¾è®¡ä¸­çš„è¦†ç›–ç‡ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRH-Partial2Globalåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºPartial2Globalã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºRH-Partial2Globalå–å¾—äº†â€œä¼˜å¼‚çš„æ€§èƒ½â€ï¼Œè¡¨æ˜å…¶åœ¨å‡†ç¡®ç‡ã€å¬å›ç‡æˆ–å…¶ä»–ç›¸å…³æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ›´å¯é å’Œå…¨é¢çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©ï¼Œæœ‰æ•ˆæé«˜äº†VICLçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ï¼Œå°¤å…¶æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ æˆ–é›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼Œé€šè¿‡é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual In-Context Learning (VICL) has emerged as a prominent approach for adapting visual foundation models to novel tasks, by effectively exploiting contextual information embedded in in-context examples, which can be formulated as a global ranking problem of potential candidates. Current VICL methods, such as Partial2Global and VPR, are grounded in the similarity-priority assumption that images more visually similar to a query image serve as better in-context examples. This foundational assumption, while intuitive, lacks sufficient justification for its efficacy in selecting optimal in-context examples. Furthermore, Partial2Global constructs its global ranking from a series of randomly sampled pairwise preference predictions. Such a reliance on random sampling can lead to incomplete coverage and redundant samplings of comparisons, thus further adversely impacting the final global ranking. To address these issues, this paper introduces an enhanced variant of Partial2Global designed for reliable and holistic selection of in-context examples in VICL. Our proposed method, dubbed RH-Partial2Global, leverages a jackknife conformal prediction-guided strategy to construct reliable alternative sets and a covering design-based sampling approach to ensure comprehensive and uniform coverage of pairwise preferences. Extensive experiments demonstrate that RH-Partial2Global achieves excellent performance and outperforms Partial2Global across diverse visual tasks.

