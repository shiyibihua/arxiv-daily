---
layout: default
title: Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding
---

# Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25794" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25794v1</a>
  <a href="https://arxiv.org/pdf/2509.25794.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25794v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25794v1', 'Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haotian Xue, Yunhao Ge, Yu Zeng, Zhaoshuo Li, Ming-Yu Liu, Yongxin Chen, Jiaojiao Fan

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoint-It-OutåŸºå‡†ï¼Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šé˜¶æ®µè§†è§‰å®šä½ä¸­çš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å…·èº«æ¨ç†` `è§†è§‰å®šä½` `åŸºå‡†æµ‹è¯•` `å¤šé˜¶æ®µè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«æ¨ç†åŸºå‡†ä¸»è¦ä¾èµ–å›¾åƒæ ‡æ³¨å’Œé€‰æ‹©é¢˜ï¼Œæ— æ³•ç²¾ç¡®è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„è§†è§‰å®šä½èƒ½åŠ›ã€‚
2. Point-It-Out (PIO)åŸºå‡†é€šè¿‡å¤šé˜¶æ®µè§†è§‰å®šä½ä»»åŠ¡ï¼Œç³»ç»Ÿè¯„ä¼°VLMåœ¨å…·èº«æ™ºèƒ½åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œé€šç”¨VLMåœ¨è§†è§‰å®šä½æ–¹é¢è¡¨ç°ä¸å¦‚ç‰¹å®šå¼€æºæ¨¡å‹ï¼Œä¸”æ¨¡å‹åœ¨ä¸åŒé˜¶æ®µè¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½¿å…¶æˆä¸ºå…·èº«æ¨ç†åº”ç”¨çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†ä¸»è¦é€šè¿‡åŸºäºå›¾åƒæ³¨é‡Šçš„å¤šé¡¹é€‰æ‹©é¢˜æ¥è¯„ä¼°VLMçš„å…·èº«æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œé€‰æ‹©å“ªä¸ªè½¨è¿¹æ›´å¥½åœ°æè¿°äº†å›¾åƒä¸­çš„äº‹ä»¶ã€‚æœ¬æ–‡æå‡ºäº†Point-It-Out (PIO)åŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ç²¾ç¡®çš„è§†è§‰å®šä½ç³»ç»Ÿåœ°è¯„ä¼°VLMçš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†å±‚è¯„ä¼°åè®®ï¼Œè·¨è¶Šä¸‰ä¸ªé˜¶æ®µ(S1:å‚è€ƒå¯¹è±¡å®šä½ï¼ŒS2:ä»»åŠ¡é©±åŠ¨çš„æŒ‡å‘ï¼ŒS3:è§†è§‰è½¨è¿¹é¢„æµ‹)ï¼Œæ•°æ®æ”¶é›†è‡ªå…·èº«æ™ºèƒ½çš„å…³é”®é¢†åŸŸï¼ŒåŒ…æ‹¬å®¤å†…ã€å¨æˆ¿ã€é©¾é©¶å’Œæœºå™¨äººæ“ä½œåœºæ™¯ã€‚å¯¹åå¤šä¸ªæœ€å…ˆè¿›çš„VLMçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†ä¸€äº›æœ‰è¶£çš„å‘ç°ã€‚ä¾‹å¦‚ï¼ŒåƒGPT-4oè¿™æ ·å¼ºå¤§çš„é€šç”¨æ¨¡å‹ï¼Œè™½ç„¶åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•(ä¾‹å¦‚ï¼Œè¯­è¨€ã€æ„ŸçŸ¥å’Œæ¨ç†)ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç¡®çš„è§†è§‰å®šä½æ–¹é¢ï¼Œå…¶æ€§èƒ½ä¸å¦‚ä¸€äº›å¼€æºæ¨¡å‹;åƒMoLMOè¿™æ ·çš„æ¨¡å‹åœ¨S1å’ŒS2ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨S3ä¸­è¡¨ç°ä¸ä½³ï¼ŒS3éœ€è¦å°†å®šä½ä¸è§†è§‰è½¨è¿¹è§„åˆ’ç›¸ç»“åˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«æ¨ç†è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå›¾åƒæ ‡æ³¨å’Œé€‰æ‹©é¢˜ï¼Œç¼ºä¹å¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ç²¾ç¡®è§†è§‰å®šä½èƒ½åŠ›çš„æœ‰æ•ˆè¯„ä¼°ã€‚è¿™ä½¿å¾—æˆ‘ä»¬éš¾ä»¥äº†è§£VLMåœ¨çœŸå®å…·èº«æ™ºèƒ½ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œå’Œå¯¼èˆªç­‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPoint-It-Out (PIO)åŸºå‡†çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªå¤šé˜¶æ®µçš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œç³»ç»Ÿåœ°è¯„ä¼°VLMçš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¨¡æ‹Ÿäº†çœŸå®ä¸–ç•Œä¸­å…·èº«æ™ºèƒ½ä½“éœ€è¦æ‰§è¡Œçš„å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚å¯»æ‰¾ç‰¹å®šç‰©ä½“ã€æ ¹æ®æŒ‡ä»¤æŒ‡å‘ç›®æ ‡ä½ç½®ä»¥åŠé¢„æµ‹è§†è§‰è½¨è¿¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPIOåŸºå‡†åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šS1ï¼ˆå‚è€ƒå¯¹è±¡å®šä½ï¼‰ï¼šVLMéœ€è¦æ ¹æ®æ–‡æœ¬æè¿°åœ¨å›¾åƒä¸­å®šä½ç›®æ ‡å¯¹è±¡ï¼›S2ï¼ˆä»»åŠ¡é©±åŠ¨çš„æŒ‡å‘ï¼‰ï¼šVLMéœ€è¦æ ¹æ®ä»»åŠ¡æŒ‡ä»¤æŒ‡å‘å›¾åƒä¸­çš„ç‰¹å®šä½ç½®ï¼›S3ï¼ˆè§†è§‰è½¨è¿¹é¢„æµ‹ï¼‰ï¼šVLMéœ€è¦é¢„æµ‹å®Œæˆä»»åŠ¡æ‰€éœ€çš„è§†è§‰è½¨è¿¹ã€‚æ•°æ®æ”¶é›†è‡ªå®¤å†…ã€å¨æˆ¿ã€é©¾é©¶å’Œæœºå™¨äººæ“ä½œç­‰å¤šä¸ªå…·èº«æ™ºèƒ½å…³é”®é¢†åŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šPIOåŸºå‡†çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šé˜¶æ®µçš„è¯„ä¼°åè®®ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°VLMçš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼ŒPIOæ›´åŠ å…³æ³¨VLMçš„è§†è§‰å®šä½ç²¾åº¦ï¼Œå¹¶å¼•å…¥äº†è§†è§‰è½¨è¿¹é¢„æµ‹ä»»åŠ¡ï¼Œæ›´è´´è¿‘çœŸå®ä¸–ç•Œçš„å…·èº«æ™ºèƒ½åº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šPIOåŸºå‡†çš„æ•°æ®é›†åŒ…å«å„ç§å…·èº«æ™ºèƒ½åœºæ™¯ï¼Œä¾‹å¦‚å®¤å†…ç¯å¢ƒã€å¨æˆ¿ç¯å¢ƒã€é©¾é©¶åœºæ™¯å’Œæœºå™¨äººæ“ä½œåœºæ™¯ã€‚æ¯ä¸ªåœºæ™¯éƒ½åŒ…å«å¤šä¸ªè§†è§‰å®šä½ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«æ–‡æœ¬æè¿°ã€å›¾åƒå’Œç›®æ ‡ä½ç½®çš„æ ‡æ³¨ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å®šä½ç²¾åº¦ã€æŒ‡å‘ç²¾åº¦å’Œè½¨è¿¹é¢„æµ‹ç²¾åº¦ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€è¯„ä¼°çš„VLMæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oç­‰é€šç”¨VLMåœ¨è§†è§‰å®šä½æ–¹é¢è¡¨ç°ä¸å¦‚MoLMOç­‰å¼€æºæ¨¡å‹ã€‚MoLMOåœ¨S1å’ŒS2é˜¶æ®µè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨S3é˜¶æ®µè¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜è§†è§‰è½¨è¿¹é¢„æµ‹å¯¹VLMæå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæŒ‡å¯¼VLMåœ¨å…·èº«æ™ºèƒ½é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡ç²¾ç¡®è¯„ä¼°å’Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å…·èº«æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£ç¯å¢ƒã€æ‰§è¡Œä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have demonstrated impressive world knowledge across a wide range of tasks, making them promising candidates for embodied reasoning applications. However, existing benchmarks primarily evaluate the embodied reasoning ability of VLMs through multiple-choice questions based on image annotations -- for example, selecting which trajectory better describes an event in the image. In this work, we introduce the Point-It-Out (PIO) benchmark, a novel benchmark designed to systematically assess the embodied reasoning abilities of VLMs through precise visual grounding. We propose a hierarchical evaluation protocol spanning three stages (S1: referred-object localization, S2: task-driven pointing, and S3: visual trace prediction), with data collected from critical domains for embodied intelligence, including indoor, kitchen, driving, and robotic manipulation scenarios. Extensive experiments with over ten state-of-the-art VLMs reveal several interesting findings. For example, strong general-purpose models such as GPT-4o, while excelling on many benchmarks (e.g., language, perception, and reasoning), underperform compared to some open-source models in precise visual grounding; models such as MoLMO perform well in S1 and S2 but struggle in S3, where requires grounding combined with visual trace planning.

