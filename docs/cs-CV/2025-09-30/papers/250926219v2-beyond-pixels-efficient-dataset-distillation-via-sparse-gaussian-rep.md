---
layout: default
title: Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation
---

# Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26219" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26219v2</a>
  <a href="https://arxiv.org/pdf/2509.26219.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26219v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26219v2', 'Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenyang Jiang, Zhengcen Li, Hang Zhao, Qiben Shan, Shaocong Wu, Jingyong Su

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-12-02)

**å¤‡æ³¨**: 19 pages; Code is available on https://github.com/j-cyoung/GSDatasetDistillation

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/j-cyoung/GSDatasetDistillation)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¨€ç–é«˜æ–¯è¡¨ç¤ºçš„æ•°æ®é›†è’¸é¦æ–¹æ³•GSDDï¼Œæå‡æ•ˆç‡ä¸æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `ç¨€ç–è¡¨ç¤º` `é«˜æ–¯æ¨¡å‹` `CUDAåŠ é€Ÿ` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæ•°æ®é›†è’¸é¦æ–¹æ³•ä¾èµ–å¯†é›†åƒç´ è¡¨ç¤ºï¼Œå­˜åœ¨å†—ä½™ä¸”æ‰©å±•æ€§å·®çš„é—®é¢˜ã€‚
2. GSDDåˆ©ç”¨å°‘é‡é«˜æ–¯åŸºå…ƒç¨€ç–è¡¨ç¤ºå›¾åƒï¼Œç¼–ç å…³é”®åˆ¤åˆ«ä¿¡æ¯ï¼Œæå‡æ•°æ®é›†å¤šæ ·æ€§ã€‚
3. é‡‡ç”¨CUDAåŠ é€Ÿçš„splattingç®—å­ï¼Œå®ç°é«˜æ•ˆå¹¶è¡Œæ¨ç†å’Œè®­ç»ƒï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†è’¸é¦æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„èŒƒå¼ï¼Œå®ƒåˆæˆç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ•°æ®é›†ï¼Œèƒ½å¤Ÿä¿ç•™å¤§è§„æ¨¡æ•°æ®é›†çš„çŸ¥è¯†ï¼Œä»è€Œè§£å†³ç°ä»£æ¨¡å‹è®­ç»ƒä¸­å·¨å¤§çš„è®¡ç®—å’Œå­˜å‚¨è´Ÿæ‹…ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºå¯†é›†çš„åƒç´ çº§è¡¨ç¤ºï¼Œè¿™å¼•å…¥äº†å†—ä½™å¹¶ä¸”éš¾ä»¥æ‰©å±•ã€‚æœ¬æ–‡æå‡ºGSDDï¼Œä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„åŸºäº2Dé«˜æ–¯åˆ†å¸ƒçš„æ•°æ®é›†è’¸é¦ç¨€ç–è¡¨ç¤ºæ–¹æ³•ã€‚GSDDå¹¶éå¹³ç­‰åœ°è¡¨ç¤ºæ‰€æœ‰åƒç´ ï¼Œè€Œæ˜¯ä»…ä½¿ç”¨å°‘é‡é«˜æ–¯åŸºå…ƒåœ¨è’¸é¦å›¾åƒä¸­ç¼–ç å…³é”®çš„åˆ¤åˆ«ä¿¡æ¯ã€‚è¿™ç§ç¨€ç–è¡¨ç¤ºå¯ä»¥åœ¨ç›¸åŒçš„å­˜å‚¨é¢„ç®—ä¸‹æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œå¢å¼ºå¯¹å›°éš¾æ ·æœ¬çš„è¦†ç›–ï¼Œå¹¶æé«˜è’¸é¦æ€§èƒ½ã€‚ä¸ºäº†ç¡®ä¿æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºCUDAçš„splattingç®—å­è¿›è¡Œå¹¶è¡Œæ¨ç†å’Œè®­ç»ƒï¼Œä»è€Œä»¥æœ€å°çš„è®¡ç®—å’Œå†…å­˜å¼€é”€å®ç°é«˜è´¨é‡çš„æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€å•è€Œæœ‰æ•ˆï¼Œå¹¿æ³›é€‚ç”¨äºä¸åŒçš„è’¸é¦æµç¨‹ï¼Œå¹¶ä¸”å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGSDDåœ¨CIFAR-10ã€CIFAR-100å’ŒImageNetå­é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„ç¼–ç å’Œè§£ç æˆæœ¬ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ•°æ®é›†è’¸é¦æ—¨åœ¨ç”¨ä¸€ä¸ªè¿œå°äºåŸå§‹æ•°æ®é›†çš„åˆæˆæ•°æ®é›†ï¼Œè®­ç»ƒå‡ºä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒæ•ˆæœç›¸è¿‘çš„æ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºåƒç´ çš„è’¸é¦ï¼Œå­˜åœ¨ä¿¡æ¯å†—ä½™ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®é›†ç­‰é—®é¢˜ã€‚è¿™äº›æ–¹æ³•å¹³ç­‰åœ°å¯¹å¾…æ¯ä¸ªåƒç´ ï¼Œå¿½ç•¥äº†å›¾åƒä¸­ä¸åŒåŒºåŸŸçš„é‡è¦æ€§å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGSDDçš„æ ¸å¿ƒæ€è·¯æ˜¯ä½¿ç”¨ç¨€ç–çš„é«˜æ–¯åˆ†å¸ƒæ¥è¡¨ç¤ºè’¸é¦æ•°æ®é›†ä¸­çš„å›¾åƒã€‚é€šè¿‡å°‘é‡çš„é«˜æ–¯åŸºå…ƒï¼Œæ•æ‰å›¾åƒä¸­çš„å…³é”®åˆ¤åˆ«ä¿¡æ¯ï¼Œé¿å…äº†åƒç´ çº§è¡¨ç¤ºçš„å†—ä½™ã€‚è¿™ç§ç¨€ç–è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å­˜å‚¨ç©ºé—´ï¼Œæé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œä»è€Œæå‡è’¸é¦æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGSDDçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åˆå§‹åŒ–ï¼šéšæœºåˆå§‹åŒ–ä¸€ç»„é«˜æ–¯å‚æ•°ï¼ˆä½ç½®ã€æ–¹å·®ã€å¹…åº¦ç­‰ï¼‰ã€‚2) å‰å‘ä¼ æ’­ï¼šä½¿ç”¨åŸºäºCUDAçš„splattingç®—å­ï¼Œå°†é«˜æ–¯åŸºå…ƒæ¸²æŸ“æˆå›¾åƒã€‚3) æŸå¤±è®¡ç®—ï¼šè®¡ç®—æ¸²æŸ“å›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„æŸå¤±ï¼Œä¾‹å¦‚äº¤å‰ç†µæŸå¤±ã€‚4) åå‘ä¼ æ’­ï¼šé€šè¿‡åå‘ä¼ æ’­ç®—æ³•ï¼Œæ›´æ–°é«˜æ–¯å‚æ•°ã€‚5) è¿­ä»£ä¼˜åŒ–ï¼šé‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°é«˜æ–¯å‚æ•°æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šGSDDæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨ç¨€ç–é«˜æ–¯è¡¨ç¤ºè¿›è¡Œæ•°æ®é›†è’¸é¦ã€‚ä¸ä¼ ç»Ÿçš„åƒç´ çº§è¡¨ç¤ºç›¸æ¯”ï¼ŒGSDDèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç¼–ç å›¾åƒä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå‡å°‘å†—ä½™ï¼Œæé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼ŒGSDDé‡‡ç”¨åŸºäºCUDAçš„splattingç®—å­ï¼Œå®ç°äº†é«˜æ•ˆçš„å¹¶è¡Œæ¸²æŸ“å’Œè®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šGSDDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é«˜æ–¯åŸºå…ƒçš„å‚æ•°åŒ–ï¼šæ¯ä¸ªé«˜æ–¯åŸºå…ƒç”±ä½ç½®ã€æ–¹å·®ã€å¹…åº¦ç­‰å‚æ•°è¡¨ç¤ºã€‚2) åŸºäºCUDAçš„splattingç®—å­ï¼šç”¨äºå°†é«˜æ–¯åŸºå…ƒæ¸²æŸ“æˆå›¾åƒï¼Œå®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚3) æŸå¤±å‡½æ•°ï¼šå¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€å‡æ–¹è¯¯å·®æŸå¤±ç­‰ï¼Œç”¨äºè¡¡é‡æ¸²æŸ“å›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚4) ä¼˜åŒ–ç®—æ³•ï¼šå¯ä»¥ä½¿ç”¨Adamã€SGDç­‰ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæ›´æ–°é«˜æ–¯å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GSDDåœ¨CIFAR-10ã€CIFAR-100å’ŒImageNetå­é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨CIFAR-10ä¸Šï¼Œä½¿ç”¨10ä¸ªå›¾åƒè¿›è¡Œè’¸é¦ï¼ŒGSDDçš„å‡†ç¡®ç‡è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”ç¼–ç å’Œè§£ç æˆæœ¬æ›´ä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGSDDèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œå¢å¼ºå¯¹å›°éš¾æ ·æœ¬çš„è¦†ç›–ï¼Œä»è€Œæå‡è’¸é¦æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GSDDå¯åº”ç”¨äºæ¨¡å‹å‹ç¼©ã€è”é‚¦å­¦ä¹ ã€æŒç»­å­¦ä¹ ç­‰é¢†åŸŸã€‚åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œå¯ä»¥ä½¿ç”¨GSDDè’¸é¦å¾—åˆ°çš„å°å‹æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œé™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œå¯ä»¥ä½¿ç”¨GSDDå¯¹æœ¬åœ°æ•°æ®è¿›è¡Œè’¸é¦ï¼Œå‡å°‘ä¸Šä¼ çš„æ•°æ®é‡ï¼Œä¿æŠ¤ç”¨æˆ·éšç§ã€‚æœªæ¥ï¼ŒGSDDæœ‰æœ›æ‰©å±•åˆ°è§†é¢‘æ•°æ®é›†çš„è’¸é¦ï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at https://github.com/j-cyoung/GSDatasetDistillation.

