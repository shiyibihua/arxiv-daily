---
layout: default
title: Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation
---

# Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26497" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26497v1</a>
  <a href="https://arxiv.org/pdf/2509.26497.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26497v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26497v1', 'Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Miao Rang, Zhenni Bi, Hang Zhou, Hanting Chen, An Xiao, Tianyu Guo, Kai Han, Xinghao Chen, Yunhe Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 7

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçŸ¥è¯†è’¸é¦çš„åè®­ç»ƒæµç¨‹ï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è’¸é¦` `åè®­ç»ƒ` `è¾¹ç¼˜è®¡ç®—` `è¯¾ç¨‹å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `æŒ‡ä»¤è°ƒä¼˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼Œå°å‹æ¨¡å‹æ€§èƒ½ä¸è¶³ã€‚
2. æå‡ºåŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç›‘ç£å¾®è°ƒå’Œç¦»çº¿çŸ¥è¯†è’¸é¦çš„åè®­ç»ƒæµç¨‹ã€‚
3. è¯¥æ–¹æ³•ä½¿åäº¿å‚æ•°æ¨¡å‹è¾¾åˆ°SOTAæ€§èƒ½ï¼Œå¹¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šä¿æŒç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ˜¾è‘—æå‡äº†äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åºå¤§çš„è§„æ¨¡å’Œé«˜è®¡ç®—æˆæœ¬ä½¿å…¶ä¸é€‚åˆç›´æ¥éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜ç¯å¢ƒä¸­ã€‚è¿™å°±è¿«åˆ‡éœ€è¦èƒ½å¤Ÿåœ¨è¾¹ç¼˜é«˜æ•ˆè¿è¡Œçš„é«˜æ€§èƒ½å°å‹æ¨¡å‹ã€‚ç„¶è€Œï¼Œä»…ç»è¿‡é¢„è®­ç»ƒåï¼Œè¿™äº›è¾ƒå°çš„æ¨¡å‹é€šå¸¸æ— æ³•æ»¡è¶³å¤æ‚ä»»åŠ¡çš„æ€§èƒ½è¦æ±‚ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç³»ç»Ÿçš„åè®­ç»ƒæµç¨‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜å°å‹æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åè®­ç»ƒæµç¨‹åŒ…æ‹¬åŸºäºè¯¾ç¨‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¦»çº¿on-policyçŸ¥è¯†è’¸é¦ã€‚ç”±æ­¤äº§ç”Ÿçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹åœ¨åäº¿å‚æ•°æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä¸¥æ ¼çš„ç¡¬ä»¶çº¦æŸä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨æ˜‡è…¾è¾¹ç¼˜è®¾å¤‡ä¸Šå¼€å‘é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§å®ç”¨è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹ï¼ˆå‚æ•°é‡åœ¨åäº¿çº§åˆ«ï¼‰åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶ï¼Œæ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æˆ–ç®€å•å¾®è°ƒï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„éœ€æ±‚ï¼Œæ— æ³•å……åˆ†å‘æŒ¥å°å‹æ¨¡å‹çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨èµ„æºå—é™çš„æ¡ä»¶ä¸‹ï¼Œæœ‰æ•ˆæå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒæµç¨‹ï¼Œå……åˆ†åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†å¤§å‹æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°å‹æ¨¡å‹ä¸­ï¼Œä»è€Œæå‡å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æµç¨‹åŒ…å«åŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¦»çº¿on-policyçŸ¥è¯†è’¸é¦ä¸¤ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨é€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) åŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç›‘ç£å¾®è°ƒï¼ˆCurriculum-based SFTï¼‰ï¼šä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„è¯¾ç¨‹ï¼Œä»ç®€å•åˆ°å¤æ‚åœ°è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶é€æ­¥é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚2) ç¦»çº¿On-policyçŸ¥è¯†è’¸é¦ï¼šä½¿ç”¨å¤§å‹æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œç„¶åä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒå°å‹æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰ï¼Œä»è€Œå°†å¤§å‹æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°å‹æ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç³»ç»Ÿæ€§çš„åè®­ç»ƒæµç¨‹ï¼Œå°†è¯¾ç¨‹å­¦ä¹ å’Œç¦»çº¿çŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°æå‡äº†å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ç¦»çº¿On-policyçš„æ–¹å¼ï¼Œé¿å…äº†åœ¨çº¿è’¸é¦å¸¦æ¥çš„è®¡ç®—è´Ÿæ‹…ï¼Œæ›´é€‚åˆèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ã€‚æ­¤å¤–ï¼Œè¯¾ç¨‹å­¦ä¹ çš„å¼•å…¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç›®æ ‡ä»»åŠ¡ï¼Œæå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯¾ç¨‹å­¦ä¹ é˜¶æ®µï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ç³»åˆ—éš¾åº¦é€’å¢çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä»ç®€å•çš„æ–‡æœ¬ç”Ÿæˆåˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚åœ¨çŸ¥è¯†è’¸é¦é˜¶æ®µï¼Œè®ºæ–‡ä½¿ç”¨å¤§å‹æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶é‡‡ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚KLæ•£åº¦ï¼‰æ¥è¡¡é‡å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹æ¨¡å‹çš„è¶…å‚æ•°è¿›è¡Œäº†ç²¾ç»†çš„è°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶æå‡ºçš„åè®­ç»ƒæµç¨‹åœ¨åäº¿å‚æ•°æ¨¡å‹ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªbenchmarkä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä¾‹å¦‚åœ¨XXXæ•°æ®é›†ä¸Šæå‡äº†X%ï¼Œåœ¨YYYæ•°æ®é›†ä¸Šæå‡äº†Y%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸¥æ ¼çš„ç¡¬ä»¶çº¦æŸä¸‹ï¼Œæœ‰æ•ˆåœ°æå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®‰é˜²ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥ä½¿è¿™äº›è®¾å¤‡å…·å¤‡æ›´å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œå¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œè¯­éŸ³æ§åˆ¶ã€æ™ºèƒ½é—®ç­”ç­‰ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œè¯­éŸ³äº¤äº’ã€åœºæ™¯ç†è§£ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of large language models (LLMs) has significantly advanced the capabilities of artificial intelligence across various domains. However, their massive scale and high computational costs render them unsuitable for direct deployment in resource-constrained edge environments. This creates a critical need for high-performance small models that can operate efficiently at the edge. Yet, after pre-training alone, these smaller models often fail to meet the performance requirements of complex tasks. To bridge this gap, we introduce a systematic post-training pipeline that efficiently enhances small model accuracy. Our post training pipeline consists of curriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge distillation. The resulting instruction-tuned model achieves state-of-the-art performance among billion-parameter models, demonstrating strong generalization under strict hardware constraints while maintaining competitive accuracy across a variety of tasks. This work provides a practical and efficient solution for developing high-performance language models on Ascend edge devices.

