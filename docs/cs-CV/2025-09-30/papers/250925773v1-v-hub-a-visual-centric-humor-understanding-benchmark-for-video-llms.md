---
layout: default
title: V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs
---

# V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25773" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25773v1</a>
  <a href="https://arxiv.org/pdf/2509.25773.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25773v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25773v1', 'V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengpeng Shi, Hengli Li, Yanpeng Zhao, Jianqun Zhou, Yuxuan Wang, Qinrong Cui, Wei Bi, Songchun Zhu, Bo Zhao, Zilong Zheng

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**V-HUBï¼šé¢å‘è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸­å¿ƒå¹½é»˜ç†è§£è¯„æµ‹åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¹½é»˜ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `è¯„æµ‹åŸºå‡†` `è§†è§‰ä¸­å¿ƒ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ç†è§£è§†è§‰å¹½é»˜æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨çš„è¯„æµ‹åŸºå‡†ã€‚
2. æ„å»ºäº†v-HUBåŸºå‡†ï¼ŒåŒ…å«æ— /å°‘æ–‡æœ¬çš„å¹½é»˜è§†é¢‘ï¼Œå¹¶æä¾›å­—å¹•ã€æè¿°ç­‰ä¸°å¯Œæ ‡æ³¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨ä»…å‡­è§†è§‰ç†è§£å¹½é»˜æ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒéŸ³é¢‘ä¿¡æ¯æœ‰æ˜¾è‘—å¸®åŠ©ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è¯„ä¼°å’Œè¯Šæ–­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ç†è§£å¹½é»˜çš„èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è§†è§‰ä¸­å¿ƒè§†é¢‘å¹½é»˜ç†è§£åŸºå‡†v-HUBã€‚v-HUBåŒ…å«ç²¾é€‰çš„ã€æå°‘è¯­è¨€çš„çŸ­è§†é¢‘ï¼Œè¿™äº›è§†é¢‘æ¥æºäºç»å…¸é»˜ç‰‡å’Œåœ¨çº¿èµ„æºï¼Œåæ˜ äº†ä»…é€šè¿‡è§†è§‰çº¿ç´¢å°±èƒ½ç†è§£å¹½é»˜çš„çœŸå®åœºæ™¯ã€‚æ¯ä¸ªè§†é¢‘ç‰‡æ®µéƒ½é…æœ‰ä¸°å¯Œçš„æ³¨é‡Šï¼ŒåŒ…æ‹¬å­—å¹•ã€æè¿°å’Œè§£é‡Šï¼Œæ”¯æŒå­—å¹•åŒ¹é…å’Œå¹½é»˜è§£é‡Šç­‰è¯„ä¼°ä»»åŠ¡ã€‚ä¸ºäº†æ‰©å¤§å…¶é€‚ç”¨æ€§ï¼Œè¿›ä¸€æ­¥æ„å»ºäº†ä¸€ä¸ªå¼€æ”¾å¼çš„è§†é¢‘é—®ç­”ä»»åŠ¡ï¼Œä½¿å…¶æ˜“äºé›†æˆåˆ°ç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†ä¸­ã€‚è¯„ä¼°äº†å„ç§MLLMï¼Œä»ä¸“é—¨çš„Video-LLMåˆ°å¯ä»¥å¤„ç†éŸ³é¢‘çš„å¤šåŠŸèƒ½OmniLLMï¼Œæ¶µç›–äº†å¼€æºå’Œä¸“æœ‰é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMåœ¨ä»…é€šè¿‡è§†è§‰çº¿ç´¢ç†è§£å¹½é»˜æ–¹é¢é¢ä¸´å›°éš¾ã€‚ä¾‹å¦‚ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨ä»åŸºäºæ–‡æœ¬çš„è¯„ä¼°è½¬å‘åŸºäºè§†é¢‘çš„è¯„ä¼°ï¼ˆæ²¡æœ‰éŸ³é¢‘ï¼‰æ—¶ï¼Œå­—å¹•åŒ¹é…çš„æ€§èƒ½éƒ½æ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶ç»“æœè¿˜è¡¨æ˜ï¼ŒåŠ å…¥éŸ³é¢‘æœ‰åŠ©äºè§†é¢‘å¹½é»˜ç†è§£ï¼Œçªå‡ºäº†å£°éŸ³çš„ä¿¡æ¯é‡ä»¥åŠæ•´åˆæ›´ä¸°å¯Œçš„æ¨¡æ€ä»¥è¿›è¡Œå¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡çš„å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è§†é¢‘ä¸­çº¯è§†è§‰å¹½é»˜ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆè¯„ä¼°ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†é¢‘å¹½é»˜æ—¶ï¼Œå¾€å¾€ä¾èµ–äºæ–‡æœ¬ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†è§†è§‰çº¿ç´¢çš„é‡è¦æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªä¸“é—¨çš„åŸºå‡†æ¥è¯„ä¼°æ¨¡å‹ä»…é€šè¿‡è§†è§‰ä¿¡æ¯ç†è§£å¹½é»˜çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è§†é¢‘å¹½é»˜ç†è§£åŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«å¤§é‡æ— /å°‘æ–‡æœ¬çš„å¹½é»˜è§†é¢‘ï¼Œå¹¶æä¾›ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œä¾‹å¦‚å­—å¹•ã€æè¿°å’Œè§£é‡Šã€‚é€šè¿‡è®¾è®¡ä¸åŒçš„è¯„ä¼°ä»»åŠ¡ï¼Œä¾‹å¦‚å­—å¹•åŒ¹é…å’Œå¹½é»˜è§£é‡Šï¼Œæ¥è¯„ä¼°æ¨¡å‹ç†è§£è§†è§‰å¹½é»˜çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡æ€ï¼ˆä¾‹å¦‚ï¼Œä»…è§†è§‰ã€è§†è§‰+éŸ³é¢‘ï¼‰ä¸‹çš„æ¨¡å‹è¡¨ç°ï¼Œæ¥åˆ†æä¸åŒæ¨¡æ€ä¿¡æ¯å¯¹å¹½é»˜ç†è§£çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šv-HUBåŸºå‡†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šä»ç»å…¸é»˜ç‰‡å’Œåœ¨çº¿èµ„æºä¸­æ”¶é›†å¹½é»˜è§†é¢‘ç‰‡æ®µã€‚2) æ•°æ®æ ‡æ³¨ï¼šä¸ºæ¯ä¸ªè§†é¢‘ç‰‡æ®µæä¾›ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—å¹•ã€æè¿°å’Œè§£é‡Šã€‚3) ä»»åŠ¡æ„å»ºï¼šæ„å»ºå­—å¹•åŒ¹é…ã€å¹½é»˜è§£é‡Šå’Œå¼€æ”¾å¼è§†é¢‘é—®ç­”ç­‰è¯„ä¼°ä»»åŠ¡ã€‚4) æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨å„ç§MLLMï¼ˆåŒ…æ‹¬Video-LLMå’ŒOmniLLMï¼‰åœ¨v-HUBåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è§†é¢‘å¹½é»˜ç†è§£åŸºå‡†v-HUBã€‚ä¸ç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†ç›¸æ¯”ï¼Œv-HUBæ›´åŠ å…³æ³¨æ¨¡å‹ä»…é€šè¿‡è§†è§‰ä¿¡æ¯ç†è§£å¹½é»˜çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œv-HUBè¿˜æä¾›äº†ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œæ”¯æŒå¤šç§è¯„ä¼°ä»»åŠ¡ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹å¯¹å¹½é»˜çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ”¶é›†æ–¹é¢ï¼Œè®ºæ–‡ä¾§é‡äºé€‰æ‹©é‚£äº›å‡ ä¹ä¸åŒ…å«æ–‡æœ¬ä¿¡æ¯çš„è§†é¢‘ç‰‡æ®µï¼Œä»¥ç¡®ä¿æ¨¡å‹ä¸»è¦ä¾èµ–è§†è§‰çº¿ç´¢æ¥ç†è§£å¹½é»˜ã€‚åœ¨æ•°æ®æ ‡æ³¨æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†äººå·¥æ ‡æ³¨çš„æ–¹å¼ï¼Œä»¥ç¡®ä¿æ ‡æ³¨ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚åœ¨è¯„ä¼°ä»»åŠ¡æ–¹é¢ï¼Œè®ºæ–‡è®¾è®¡äº†å­—å¹•åŒ¹é…ã€å¹½é»˜è§£é‡Šå’Œå¼€æ”¾å¼è§†é¢‘é—®ç­”ç­‰å¤šç§ä»»åŠ¡ï¼Œä»¥ä»ä¸åŒè§’åº¦è¯„ä¼°æ¨¡å‹å¯¹å¹½é»˜çš„ç†è§£èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºè¢«è¯„ä¼°çš„MLLMã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨ä»…é€šè¿‡è§†è§‰çº¿ç´¢ç†è§£å¹½é»˜æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œåœ¨è§†é¢‘å­—å¹•åŒ¹é…ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”äºæ–‡æœ¬è¾“å…¥ï¼Œè§†é¢‘è¾“å…¥çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åŒæ—¶ï¼ŒåŠ å…¥éŸ³é¢‘ä¿¡æ¯å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹å¯¹è§†é¢‘å¹½é»˜çš„ç†è§£èƒ½åŠ›ï¼Œè¿™è¡¨æ˜å£°éŸ³åœ¨è§†é¢‘å¹½é»˜ç†è§£ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡äººæœºäº¤äº’çš„è¶£å‘³æ€§å’Œè‡ªç„¶æ€§ï¼Œä¾‹å¦‚åœ¨æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹ç­‰é¢†åŸŸï¼Œä½¿AIèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„æƒ…æ„Ÿå’Œæ„å›¾ï¼Œä»è€Œæä¾›æ›´ä¸ªæ€§åŒ–å’Œæ›´å…·å¸å¼•åŠ›çš„æœåŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†ä¹Ÿå¯ä¿ƒè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> AI models capable of comprehending humor hold real-world promise -- for example, enhancing engagement in human-machine interactions. To gauge and diagnose the capacity of multimodal large language models (MLLMs) for humor understanding, we introduce v-HUB, a novel visual-centric video humor understanding benchmark. v-HUB comprises a curated collection of minimally verbal short videos, sourced from classic silent films and online resources, and reflecting real-world scenarios where humor can be appreciated purely through visual cues. Each video clip is paired with rich annotations, including captions, descriptions, and explanations, supporting evaluation tasks like caption matching and humor explanation. To broaden its applicability, we further construct an open-ended video QA task, making it readily integrable into existing video understanding benchmarks. We evaluate a diverse set of MLLMs, from specialized Video-LLMs to versatile OmniLLMs that can process audio, covering both open-source and proprietary domains. The experimental results expose the difficulties MLLMs face in comprehending humor from visual cues alone. For example, all models exhibit a marked performance drop on caption matching when moving from text-based to video-based evaluation (without audio). Our findings also demonstrate that incorporating audio helps with video humor understanding, highlighting the informativeness of sound and the promise of integrating richer modalities for complex video understanding tasks.

