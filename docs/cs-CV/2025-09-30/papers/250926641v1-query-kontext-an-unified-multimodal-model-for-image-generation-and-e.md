---
layout: default
title: Query-Kontext: An Unified Multimodal Model for Image Generation and Editing
---

# Query-Kontext: An Unified Multimodal Model for Image Generation and Editing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26641" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.26641v1</a>
  <a href="https://arxiv.org/pdf/2509.26641.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26641v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26641v1', 'Query-Kontext: An Unified Multimodal Model for Image Generation and Editing')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-30

**Â§áÊ≥®**: 23 pages, 10 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Query-Kontext‰ª•ÊèêÂçáÂ§öÊ®°ÊÄÅÂõæÂÉèÁîüÊàê‰∏éÁºñËæëËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊ®°Âûã` `ÂõæÂÉèÁîüÊàê` `ÂõæÂÉèÁºñËæë` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êâ©Êï£Ê®°Âûã` `ÁîüÊàêÊé®ÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÁîüÊàêÊé®ÁêÜËÉΩÂäõ‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÈöæ‰ª•ÂÆûÁé∞È´ò‰øùÁúüÂêàÊàê‰∏éË∫´‰ªΩ‰øùÁïô„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Query-KontextÔºåÈÄöËøáÂ§öÊ®°ÊÄÅËØ≠Â¢ÉËøûÊé•VLM‰∏éÊâ©Êï£Ê®°ÂûãÔºåÂÖÖÂàÜÂèëÊå•VLMÁöÑÁîüÊàêÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™‰ªªÂä°‰∏ä‰∏éÂº∫Âü∫Á∫øÁõ∏ÂåπÈÖçÔºåÁîöËá≥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãË∂ÖË∂ä‰∫ÜÁâπÂÆö‰ªªÂä°ÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàUMMsÔºâÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÔºàT2IÔºâÂíåÁºñËæëÔºàTI2IÔºâÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁé∞ÊúâÊ°ÜÊû∂Âú®Â§öÊ®°ÊÄÅÁîüÊàêÊé®ÁêÜËÉΩÂäõ‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇÊú¨ÊñáÊèêÂá∫Query-KontextÔºåÈÄöËøáÂ∞ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ‰∏éÊâ©Êï£Ê®°ÂûãËøûÊé•ÔºåÂà©Áî®ËØ≠‰πâÁ∫øÁ¥¢ÂíåÁ≤óÁ≤íÂ∫¶ÂõæÂÉèÊù°‰ª∂ÁöÑÂ§öÊ®°ÊÄÅ‚Äúkontext‚ÄùÊù•ÊèêÂçáÁîüÊàêË¥®Èáè„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏âÈò∂ÊÆµÁöÑÊ∏êËøõËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊúÄÁªàÂú®Â§ö‰∏™‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜÂº∫Âü∫Á∫øÂíåÁâπÂÆö‰ªªÂä°ÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÁîüÊàêÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®È´ò‰øùÁúüÂêàÊàêÂíåË∫´‰ªΩ‰øùÁïôÊñπÈù¢ÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Query-KontextÔºåÂ∞ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ‰∏éÊâ©Êï£Ê®°ÂûãÁªìÂêàÔºåÂà©Áî®Â§öÊ®°ÊÄÅËØ≠Â¢ÉÊù•Â¢ûÂº∫ÁîüÊàêÊé®ÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÁïôÊâ©Êï£Ê®°ÂûãÂú®ËßÜËßâÂêàÊàê‰∏≠ÁöÑ‰ºòÂäø„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµËøûÊé•ËΩªÈáèÁ∫ßÊâ©Êï£Â§¥‰∏éVLMÔºåÈáäÊîæVLMÁöÑÁîüÊàêÊé®ÁêÜËÉΩÂäõÔºõÁ¨¨‰∫åÈò∂ÊÆµÂ∞ÜÊâ©Êï£Â§¥Êâ©Â±ïËá≥Â§ßÂûãÈ¢ÑËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÔºå‰ª•Â¢ûÂº∫ËßÜËßâÁªÜËäÇÔºõÁ¨¨‰∏âÈò∂ÊÆµÂºïÂÖ•‰ΩéÁ∫ßÂõæÂÉèÁºñÁ†ÅÂô®ÔºåÊèêÂçáÂõæÂÉè‰øùÁúüÂ∫¶Âπ∂ËøõË°åÊåá‰ª§Ë∞É‰ºò„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂ§öÊ®°ÊÄÅ‚Äúkontext‚ÄùÔºå‰ΩøÂæóVLM‰∏éÊâ©Êï£Ê®°Âûã‰πãÈó¥ÁöÑÂçèÂêåÂ∑•‰ΩúÊõ¥Âä†È´òÊïàÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®ÁîüÊàêÊé®ÁêÜ‰∏äÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÊ®°ÊÄÅkontext‰ª§ÁâåËøûÊé•VLM‰∏éÊâ©Êï£Â§¥ÔºåËÆæËÆ°‰∫ÜÈÄÇÂ∫î‰∏çÂêå‰ªªÂä°ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂπ∂‰ºòÂåñ‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•ÊèêÂçáÁîüÊàêË¥®Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåQuery-KontextÂú®Â§ö‰∏™‰ªªÂä°‰∏ä‰∏éÂº∫Âü∫Á∫øÊ®°ÂûãÁõ∏ÂåπÈÖçÔºåÁîöËá≥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãË∂ÖË∂ä‰∫ÜÁâπÂÆö‰ªªÂä°ÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂÖ∑‰ΩìÊï∞ÊçÆÊú™ËØ¶Ëø∞„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂõæÂÉèÁîüÊàê„ÄÅÊåá‰ª§È©±Âä®ÁöÑÂõæÂÉèÁºñËæë„ÄÅÂÆöÂà∂ÂåñÁîüÊàê‰ª•ÂèäÂ§ö‰∏ª‰ΩìÁªÑÂêàÁ≠â„ÄÇÂÖ∂ÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éËÉΩÂ§üÊèê‰æõÊõ¥È´òË¥®ÈáèÁöÑÂõæÂÉèÂêàÊàêÂíåÁºñËæëËÉΩÂäõÔºåÊú™Êù•ÂèØËÉΩÂú®Ëâ∫ÊúØÂàõ‰Ωú„ÄÅÂπøÂëäËÆæËÆ°ÂíåËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext'' composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model's role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM's generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.

