---
layout: default
title: Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA
---

# Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26251" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26251v1</a>
  <a href="https://arxiv.org/pdf/2509.26251.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26251v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26251v1', 'Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhejia Cai, Yandan Yang, Xinyuan Chang, Shiyi Liang, Ronghan Chen, Feng Xiong, Mu Xu, Ruqi Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFarsighted-LAMå’ŒSSM-VLAï¼Œå¢å¼ºVLAç³»ç»Ÿä¸­æ½œåœ¨åŠ¨ä½œæ¨¡å‹çš„ç©ºé—´å’ŒåŠ¨æ€æ„ŸçŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `è§†è§‰è¯­è¨€åŠ¨ä½œ` `æ½œåœ¨åŠ¨ä½œæ¨¡å‹` `å‡ ä½•æ„ŸçŸ¥` `å¤šå°ºåº¦æ—¶é—´å»ºæ¨¡` `é“¾å¼æ€è€ƒ` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LAMsçš„å›¾åƒç¼–ç å™¨ç©ºé—´ç†è§£èƒ½åŠ›è¾ƒå¼±ï¼Œä¸”å¯¹è¾“å…¥å¸§è·ç¦»æ•æ„Ÿï¼Œå¯¼è‡´æ—¶é—´æ„ŸçŸ¥å—é™ï¼Œå½±å“åŠ¨ä½œå»ºæ¨¡çš„ç¨³å®šæ€§å’Œæ¸…æ™°åº¦ã€‚
2. æœ¬æ–‡æå‡ºFarsighted-LAMï¼Œåˆ©ç”¨å‡ ä½•æ„ŸçŸ¥çš„ç©ºé—´ç¼–ç å’Œå¤šå°ºåº¦æ—¶é—´å»ºæ¨¡ï¼Œä»è¿ç»­å¸§ä¸­æå–ç»“æ„å…ˆéªŒå’ŒåŠ¨æ€è¿åŠ¨æ¨¡å¼ã€‚
3. æ„å»ºäºFarsighted-LAMä¹‹ä¸Šçš„SSM-VLAï¼Œé€šè¿‡æ•´åˆç»“æ„åŒ–æ„ŸçŸ¥å’Œè§†è§‰é“¾å¼æ€è€ƒï¼Œåœ¨å¤šä¸ªVLAä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹Vision-Language-Action (VLA) ç³»ç»Ÿä¸­æ½œåœ¨åŠ¨ä½œæ¨¡å‹ (LAM) çš„ä¸¤ä¸ªç“¶é¢ˆé—®é¢˜ï¼šå›¾åƒç¼–ç å™¨ç©ºé—´ç†è§£ä¸è¶³å’Œæ—¶é—´æ„ŸçŸ¥æœ‰é™ï¼Œæå‡ºäº†Farsighted-LAMæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥çš„ç©ºé—´ç¼–ç å’Œå¤šå°ºåº¦æ—¶é—´å»ºæ¨¡ï¼Œä»è¿ç»­å¸§ä¸­æ•è·ç»“æ„å…ˆéªŒå’ŒåŠ¨æ€è¿åŠ¨æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäºFarsighted-LAMçš„ç«¯åˆ°ç«¯VLAæ¡†æ¶SSM-VLAï¼Œå®ƒé›†æˆäº†ç»“æ„åŒ–æ„ŸçŸ¥å’Œè§†è§‰é“¾å¼æ€è€ƒæ¨¡å—ï¼Œæ˜¾å¼åœ°æ¨ç†ç¯å¢ƒåŠ¨æ€ï¼Œä»è€Œå¢å¼ºå†³ç­–ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å¤šä¸ªVLAä»»åŠ¡ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼Œç»“åˆå‡ ä½•æ„ŸçŸ¥å»ºæ¨¡ã€æ—¶é—´ä¸€è‡´æ€§å’Œæ˜¾å¼æ¨ç†çš„ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæé«˜å…·èº«æ™ºèƒ½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å–å¾—äº†å½“å‰æœ€ä¼˜çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ½œåœ¨åŠ¨ä½œæ¨¡å‹(LAMs)åœ¨VLAç³»ç»Ÿä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚ä¸€æ˜¯å¸¸ç”¨çš„ç«¯åˆ°ç«¯è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ç¼ºä¹è‰¯å¥½çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰åœºæ™¯çš„å‡ ä½•ç»“æ„ä¿¡æ¯ã€‚äºŒæ˜¯LAMsåœ¨å¤„ç†æ—¶é—´è·¨åº¦è¾ƒå¤§çš„è¾“å…¥å¸§æ—¶è¡¨ç°è„†å¼±ï¼Œå¯¼è‡´æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡é•¿æœŸä¾èµ–å…³ç³»ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†LAMsåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å‡ ä½•æ„ŸçŸ¥å’Œå¤šå°ºåº¦æ—¶é—´å»ºæ¨¡æ¥å¢å¼ºLAMsçš„ç©ºé—´å’ŒåŠ¨æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å‡ ä½•ä¿¡æ¯æ¥æå‡å›¾åƒç¼–ç å™¨çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨å¤šå°ºåº¦æ—¶é—´å»ºæ¨¡æ¥æ•æ‰ä¸åŒæ—¶é—´å°ºåº¦çš„è¿åŠ¨æ¨¡å¼ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹é•¿æœŸä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªä¸»è¦æ¡†æ¶ï¼šFarsighted-LAMå’ŒSSM-VLAã€‚Farsighted-LAMæ˜¯ä¸€ä¸ªæ½œåœ¨åŠ¨ä½œæ¡†æ¶ï¼ŒåŒ…å«å‡ ä½•æ„ŸçŸ¥çš„ç©ºé—´ç¼–ç æ¨¡å—å’Œå¤šå°ºåº¦æ—¶é—´å»ºæ¨¡æ¨¡å—ã€‚å‡ ä½•æ„ŸçŸ¥çš„ç©ºé—´ç¼–ç æ¨¡å—ç”¨äºæå–å›¾åƒçš„å‡ ä½•ç‰¹å¾ï¼Œå¤šå°ºåº¦æ—¶é—´å»ºæ¨¡æ¨¡å—ç”¨äºæ•æ‰ä¸åŒæ—¶é—´å°ºåº¦çš„è¿åŠ¨æ¨¡å¼ã€‚SSM-VLAæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„VLAæ¡†æ¶ï¼Œå»ºç«‹åœ¨Farsighted-LAMä¹‹ä¸Šï¼Œé›†æˆäº†ç»“æ„åŒ–æ„ŸçŸ¥æ¨¡å—å’Œè§†è§‰é“¾å¼æ€è€ƒæ¨¡å—ã€‚ç»“æ„åŒ–æ„ŸçŸ¥æ¨¡å—ç”¨äºæå–ç¯å¢ƒçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œè§†è§‰é“¾å¼æ€è€ƒæ¨¡å—ç”¨äºæ˜¾å¼åœ°æ¨ç†ç¯å¢ƒåŠ¨æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å‡ ä½•æ„ŸçŸ¥å’Œå¤šå°ºåº¦æ—¶é—´å»ºæ¨¡å¼•å…¥åˆ°LAMsä¸­ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç©ºé—´å’ŒåŠ¨æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰åœºæ™¯çš„å‡ ä½•ç»“æ„ä¿¡æ¯å’Œè¿åŠ¨æ¨¡å¼ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‡ ä½•æ„ŸçŸ¥çš„ç©ºé—´ç¼–ç æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„æ·±åº¦ä¼°è®¡æ¨¡å‹æ¥æå–å›¾åƒçš„æ·±åº¦ä¿¡æ¯ï¼Œå¹¶å°†æ·±åº¦ä¿¡æ¯ä¸å›¾åƒç‰¹å¾è¿›è¡Œèåˆã€‚åœ¨å¤šå°ºåº¦æ—¶é—´å»ºæ¨¡æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¤šä¸ªä¸åŒæ—¶é—´å°ºåº¦çš„å·ç§¯ç¥ç»ç½‘ç»œæ¥æ•æ‰ä¸åŒæ—¶é—´å°ºåº¦çš„è¿åŠ¨æ¨¡å¼ã€‚åœ¨SSM-VLAæ¡†æ¶ä¸­ï¼Œä½¿ç”¨äº†è§†è§‰é“¾å¼æ€è€ƒæ¨¡å—æ¥æ˜¾å¼åœ°æ¨ç†ç¯å¢ƒåŠ¨æ€ï¼Œä»è€Œå¢å¼ºäº†å†³ç­–çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å’Œå¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„Farsighted-LAMå’ŒSSM-VLAåœ¨å¤šä¸ªVLAä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¯¼èˆªä»»åŠ¡ä¸­ï¼ŒSSM-VLAçš„æˆåŠŸç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ“ä½œä»»åŠ¡ä¸­ï¼ŒSSM-VLAä¹Ÿè¡¨ç°å‡ºäº†æ›´å¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºæœºå™¨äººå¯¹ç¯å¢ƒçš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯é çš„VLAç³»ç»Ÿï¼Œä»è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Latent Action Models (LAMs) enable Vision-Language-Action (VLA) systems to learn semantic action representations from large-scale unannotated data. Yet, we identify two bottlenecks of LAMs: 1) the commonly adopted end-to-end trained image encoder suffers from poor spatial understanding; 2) LAMs can be fragile when input frames are distant, leading to limited temporal perception. Such factors inevitably hinder stable and clear action modeling. To this end, we propose Farsighted-LAM, a latent action framework with geometry-aware spatial encoding and multi-scale temporal modeling, capturing structural priors and dynamic motion patterns from consecutive frames. We further propose SSM-VLA, an end-to-end VLA framework built upon Farsighted-LAM, which integrates structured perception with a visual Chain-of-Thought module to explicitly reason about environmental dynamics, enhancing decision consistency and interpretability. We validate SSM-VLA on multiple VLA tasks in both simulation and real-world settings, and achieve state-of-the-art performance. Our results demonstrate that our strategy of combining geometry-aware modeling, temporal coherence, and explicit reasoning is effective in enhancing the robustness and generalizability of embodied intelligence.

