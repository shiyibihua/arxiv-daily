---
layout: default
title: Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations
---

# Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26004" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26004v2</a>
  <a href="https://arxiv.org/pdf/2509.26004.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26004v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26004v2', 'Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nicola Messina, Rosario Leonardi, Luca Ciampi, Fabio Carrara, Giovanni Maria Farinella, Fabrizio Falchi, Antonino Furnari

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-12-02)

**å¤‡æ³¨**: Under consideration at Pattern Recognition Letters

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://fpv-iplab.github.io/WISH)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºäººç±»å™è¿°å¼±ç›‘ç£çš„å•ç›®æ‰‹æŒç‰©ä½“åˆ†å‰²æ–¹æ³•NS-iHOS**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `æ‰‹æŒç‰©ä½“åˆ†å‰²` `å¼±ç›‘ç£å­¦ä¹ ` `äººç±»å™è¿°` `è·¨æ¨¡æ€å­¦ä¹ ` `ç¬¬ä¸€äººç§°è§†è§’` `è§†è§‰-è¯­è¨€æ¨¡å‹` `EPIC-Kitchens` `Ego4D`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹æŒç‰©ä½“åˆ†å‰²æ–¹æ³•ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸå‘å±•ã€‚
2. åˆ©ç”¨äººç±»å™è¿°ä¸­å…³äºè¢«æ“ä½œç‰©ä½“çš„çº¿ç´¢ï¼Œä»¥å¼±ç›‘ç£æ–¹å¼å­¦ä¹ æ‰‹æŒç‰©ä½“åˆ†å‰²ï¼Œæ— éœ€æµ‹è¯•æ—¶ä½¿ç”¨å™è¿°ã€‚
3. æå‡ºçš„WISHæ¨¡å‹åœ¨EPIC-Kitchenså’ŒEgo4Dæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ€§èƒ½æ¥è¿‘å…¨ç›‘ç£æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šåŸºäºå™è¿°ç›‘ç£çš„æ‰‹æŒç‰©ä½“åˆ†å‰²ï¼ˆNS-iHOSï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€å™è¿°ï¼ˆå³ç›¸æœºä½©æˆ´è€…æ‰§è¡ŒåŠ¨ä½œçš„æè¿°ï¼ŒåŒ…å«å…³äºè¢«æ“ä½œç‰©ä½“çš„çº¿ç´¢ï¼‰å­¦ä¹ åˆ†å‰²æ‰‹æŒç‰©ä½“ã€‚ç”±äºç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œè¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°æ•°æ®é›†ç¨€ç¼ºçš„é˜»ç¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºWeakly-Supervised In-hand Object Segmentation from Human Narrations (WISH)æ¨¡å‹ï¼Œé€šè¿‡ä»å™è¿°ä¸­æç‚¼çŸ¥è¯†æ¥å­¦ä¹ åˆç†çš„æ‰‹-ç‰©ä½“å…³è”ï¼Œä»è€Œå®ç°æ‰‹æŒç‰©ä½“åˆ†å‰²ï¼Œä¸”åœ¨æµ‹è¯•æ—¶ä¸ä½¿ç”¨å™è¿°ã€‚åœ¨EPIC-Kitchenså’ŒEgo4Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWISHè¶…è¶Šäº†æ‰€æœ‰åŸºäºå¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹å™¨å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨ä¸ä½¿ç”¨ç²¾ç»†åƒç´ çº§æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œæ¢å¤äº†è¶…è¿‡50%çš„å…¨ç›‘ç£æ–¹æ³•æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»ç¬¬ä¸€äººç§°è§†è§’å›¾åƒä¸­åˆ†å‰²ç”¨æˆ·æ‰‹æŒç‰©ä½“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„åƒç´ çº§æ ‡æ³¨æ•°æ®ï¼Œæ ‡æ³¨æˆæœ¬é«˜ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å¼±ç›‘ç£ä¿¡æ¯ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œæ˜¯æœ¬ç ”ç©¶è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨äººç±»å¯¹åŠ¨ä½œçš„è‡ªç„¶è¯­è¨€å™è¿°ä½œä¸ºå¼±ç›‘ç£ä¿¡å·ã€‚å™è¿°ä¸­åŒ…å«äº†å…³äºè¢«æ“ä½œç‰©ä½“çš„ä¸°å¯Œä¿¡æ¯ï¼Œé€šè¿‡å­¦ä¹ å™è¿°ä¸å›¾åƒä¹‹é—´çš„å…³è”ï¼Œå¯ä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æ‰‹æŒç‰©ä½“çš„åˆ†å‰²ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥æ ‡æ³¨åƒç´ çº§æ ‡ç­¾ï¼Œé™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWISHæ¨¡å‹çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºæå–ç¬¬ä¸€äººç§°è§†è§’å›¾åƒçš„è§†è§‰ç‰¹å¾ã€‚2) æ–‡æœ¬ç¼–ç å™¨ï¼šç”¨äºæå–äººç±»å™è¿°çš„æ–‡æœ¬ç‰¹å¾ã€‚3) è·¨æ¨¡æ€èåˆæ¨¡å—ï¼šå°†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œèåˆï¼Œå­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³è”ã€‚4) åˆ†å‰²æ¨¡å—ï¼šåŸºäºèåˆåçš„ç‰¹å¾ï¼Œé¢„æµ‹æ‰‹æŒç‰©ä½“çš„åˆ†å‰²æ©ç ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åˆ©ç”¨äººç±»å™è¿°ä½œä¸ºå¼±ç›‘ç£ä¿¡å·æ¥å­¦ä¹ æ‰‹æŒç‰©ä½“åˆ†å‰²çš„æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦åƒç´ çº§çš„æ ‡æ³¨ï¼Œå¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆçš„è·¨æ¨¡æ€èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§†è§‰ç¼–ç å™¨æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆå¦‚ResNetï¼‰æå–å›¾åƒç‰¹å¾ã€‚åœ¨æ–‡æœ¬ç¼–ç å™¨æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰æå–æ–‡æœ¬ç‰¹å¾ã€‚è·¨æ¨¡æ€èåˆæ¨¡å—å¯ä»¥ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–Transformerç»“æ„ï¼Œå­¦ä¹ è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„å…³è”ã€‚åˆ†å‰²æ¨¡å—å¯ä»¥ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæˆ–Transformerç»“æ„ï¼Œé¢„æµ‹åƒç´ çº§çš„åˆ†å‰²æ©ç ã€‚æŸå¤±å‡½æ•°å¯ä»¥åŒ…æ‹¬åˆ†å‰²æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µæŸå¤±ï¼‰å’Œè·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼ˆå¦‚å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼‰ã€‚å…·ä½“å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å®éªŒç»“æœè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

WISHæ¨¡å‹åœ¨EPIC-Kitchenså’ŒEgo4Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒWISHæ¨¡å‹åœ¨ä¸ä½¿ç”¨åƒç´ çº§æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°è¶…è¿‡50%çš„å…¨ç›‘ç£æ–¹æ³•æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºåŸºäºå¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹å™¨å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ã€‚è¿™è¡¨æ˜åˆ©ç”¨äººç±»å™è¿°ä½œä¸ºå¼±ç›‘ç£ä¿¡å·æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ‰‹æŒç‰©ä½“åˆ†å‰²æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¾…åŠ©æŠ€æœ¯ã€å·¥ä¸šå®‰å…¨å’Œæ´»åŠ¨ç›‘æ§ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è¾…åŠ©æŠ€æœ¯æ–¹é¢ï¼Œå¯ä»¥å¸®åŠ©è§†åŠ›éšœç¢è€…è¯†åˆ«æ‰‹æŒç‰©ä½“ï¼›åœ¨å·¥ä¸šå®‰å…¨æ–¹é¢ï¼Œå¯ä»¥ç›‘æ§å·¥äººæ˜¯å¦æ­£ç¡®æ“ä½œå·¥å…·ï¼›åœ¨æ´»åŠ¨ç›‘æ§æ–¹é¢ï¼Œå¯ä»¥è¯†åˆ«ç”¨æˆ·æ­£åœ¨è¿›è¡Œçš„æ´»åŠ¨ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨äººæœºäº¤äº’å’Œæœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations $\unicode{x2013}$ natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations in a weakly-supervised regime. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations. Code and data can be found at https://fpv-iplab.github.io/WISH.

