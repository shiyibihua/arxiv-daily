---
layout: default
title: DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning
---

# DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25866" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25866v1</a>
  <a href="https://arxiv.org/pdf/2509.25866.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25866v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25866v1', 'DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chi Zhang, Haibo Qiu, Qiming Zhang, Zhixiong Zeng, Lin Ma, Jing Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DeepSketcherï¼šé€šè¿‡å†…éƒ¨è§†è§‰æ“ä½œå®ç°å¤šæ¨¡æ€æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `è§†è§‰åµŒå…¥` `å›¾åƒæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­ä¾èµ–æ–‡æœ¬ä¸»å¯¼çš„æ€ç»´é“¾ï¼Œç¼ºä¹å¯¹å›¾åƒç»†ç²’åº¦åŒºåŸŸçš„æœ‰æ•ˆäº¤äº’ã€‚
2. DeepSketcheré€šè¿‡æ„å»ºå›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®é›†ï¼Œå¹¶è®¾è®¡åœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­ç›´æ¥æ“ä½œçš„æ¨¡å‹ï¼Œå®ç°å…å·¥å…·çš„è§†è§‰æ€è€ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDeepSketcheråœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹è®¾è®¡çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

â€œç”¨å›¾åƒæ€è€ƒâ€èŒƒå¼ä»£è¡¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†çš„ä¸€ä¸ªå…³é”®è½¬å˜ï¼Œå®ƒä»ä»¥æ–‡æœ¬ä¸ºä¸»çš„æ€ç»´é“¾è½¬å‘å›¾åƒäº¤äº’å¼æ¨ç†ã€‚é€šè¿‡è°ƒç”¨è§†è§‰å·¥å…·æˆ–ç”Ÿæˆä¸­é—´è§†è§‰è¡¨ç¤ºï¼ŒVLMå¯ä»¥è¿­ä»£åœ°å…³æ³¨ç»†ç²’åº¦åŒºåŸŸï¼Œä»è€Œå®ç°æ›´æ·±å…¥çš„å›¾åƒç†è§£å’Œæ›´å¿ å®çš„å¤šæ¨¡æ€æ¨ç†ã€‚ç„¶è€Œï¼Œä½œä¸ºä¸€ä¸ªæ–°å…´çš„èŒƒå¼ï¼Œå®ƒåœ¨æ•°æ®æ„å»ºçš„å‡†ç¡®æ€§ã€ç»“æ„è®¾è®¡å’Œæ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ¢ç´¢ç©ºé—´ï¼Œè¿™ä¸ºæ¨è¿›å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸°å¯Œçš„æœºä¼šã€‚ä¸ºäº†è¿›ä¸€æ­¥æ¨è¿›è¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºäº†DeepSketcherï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆå¥—ä»¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®é›†å’Œä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å‹ã€‚è¯¥æ•°æ®é›†åŒ…å«31kä¸ªæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è½¨è¿¹ï¼Œå…·æœ‰ä¸åŒçš„å·¥å…·è°ƒç”¨å’Œç”Ÿæˆçš„ç¼–è¾‘å›¾åƒï¼Œæ¶µç›–äº†å„ç§æ•°æ®ç±»å‹å’Œå…·æœ‰é«˜æ ‡æ³¨å‡†ç¡®æ€§çš„æ“ä½œæŒ‡ä»¤ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ‰§è¡Œäº¤é”™çš„å›¾åƒ-æ–‡æœ¬æ¨ç†ï¼Œå¹¶é€šè¿‡ç›´æ¥åœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­æ“ä½œæ¥åŸç”Ÿç”Ÿæˆâ€œè§†è§‰æ€æƒ³â€ï¼Œè€Œä¸æ˜¯è°ƒç”¨å¤–éƒ¨å·¥å…·å¹¶é‡å¤é‡æ–°ç¼–ç ç”Ÿæˆçš„å›¾åƒã€‚è¿™ç§è®¾è®¡å®ç°äº†å…å·¥å…·å’Œæ›´çµæ´»çš„â€œç”¨å›¾åƒæ€è€ƒâ€ã€‚åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å¼ºå¤§çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æ•ˆç”¨å’Œæ¨¡å‹è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è¿›è¡Œå¤šæ¨¡æ€æ¨ç†æ—¶ï¼Œé€šå¸¸ä¾èµ–äºæ–‡æœ¬ä¸»å¯¼çš„æ€ç»´é“¾ï¼Œæˆ–è€…éœ€è¦è°ƒç”¨å¤–éƒ¨è§†è§‰å·¥å…·å¹¶é‡å¤ç¼–ç å›¾åƒã€‚è¿™ç§æ–¹å¼è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¯¹å›¾åƒçš„ç†è§£ä¸å¤Ÿæ·±å…¥ï¼Œéš¾ä»¥è¿›è¡Œç»†ç²’åº¦çš„è§†è§‰æ“ä½œã€‚å› æ­¤ï¼Œå¦‚ä½•è®©VLMæ›´æœ‰æ•ˆåœ°â€œç”¨å›¾åƒæ€è€ƒâ€ï¼Œç›´æ¥åœ¨è§†è§‰ç©ºé—´è¿›è¡Œæ¨ç†ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDeepSketcherçš„æ ¸å¿ƒæ€è·¯æ˜¯è®©VLMèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œåœ¨è„‘æµ·ä¸­â€œç»˜åˆ¶â€æˆ–â€œä¿®æ”¹â€å›¾åƒï¼Œä»è€Œè¿›è¡Œæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡åœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œç”Ÿæˆâ€œè§†è§‰æ€æƒ³â€ï¼Œé¿å…äº†å¯¹å¤–éƒ¨å·¥å…·çš„ä¾èµ–å’Œé‡å¤çš„å›¾åƒç¼–ç ã€‚è¿™ç§æ–¹å¼æ›´åŠ é«˜æ•ˆï¼Œä¹Ÿæ›´çµæ´»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeepSketcheråŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šä¸€ä¸ªå›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®é›†å’Œä¸€ä¸ªè‡ªåŒ…å«æ¨¡å‹ã€‚æ•°æ®é›†åŒ…å«31kä¸ªæ€ç»´é“¾æ¨ç†è½¨è¿¹ï¼Œæ¶µç›–äº†å„ç§æ•°æ®ç±»å‹å’Œæ“ä½œæŒ‡ä»¤ã€‚æ¨¡å‹åˆ™åŸºäºTransformeræ¶æ„ï¼Œèƒ½å¤Ÿæ‰§è¡Œäº¤é”™çš„å›¾åƒ-æ–‡æœ¬æ¨ç†ï¼Œå¹¶åœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­ç”Ÿæˆâ€œè§†è§‰æ€æƒ³â€ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬ï¼šè¾“å…¥å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œæ¨¡å‹è¿›è¡Œå¤šè½®æ¨ç†ï¼Œæ¯ä¸€è½®æ¨ç†éƒ½å¯èƒ½ç”Ÿæˆæ–°çš„è§†è§‰è¡¨ç¤ºï¼Œæœ€ç»ˆè¾“å‡ºç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepSketcheræœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå®ƒèƒ½å¤Ÿåœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œæ“ä½œï¼Œç”Ÿæˆâ€œè§†è§‰æ€æƒ³â€ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·æˆ–é‡å¤ç¼–ç å›¾åƒæœ‰ç€æœ¬è´¨çš„åŒºåˆ«ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´é«˜æ•ˆã€æ›´çµæ´»åœ°è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šDeepSketcherçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®é›†ï¼Œæä¾›äº†ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼›2) åŸºäºTransformerçš„æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼›3) åœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œæ“ä½œçš„æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹ç›´æ¥ç”Ÿæˆâ€œè§†è§‰æ€æƒ³â€ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DeepSketcheråœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“çš„æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ï¼Œä½†æ€»ä½“è€Œè¨€ï¼Œå®éªŒç»“æœéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹è®¾è®¡çš„ä¼˜è¶Šæ€§ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œå¹¶ç”Ÿæˆæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DeepSketcherçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½é—®ç­”ã€å›¾åƒç¼–è¾‘ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½é—®ç­”ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å›¾åƒå†…å®¹å’Œç”¨æˆ·æé—®ï¼Œç”Ÿæˆä¸­é—´è§†è§‰è¡¨ç¤ºï¼Œä»è€Œæ›´å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚åœ¨å›¾åƒç¼–è¾‘ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ï¼Œç›´æ¥åœ¨è§†è§‰åµŒå…¥ç©ºé—´ä¸­ä¿®æ”¹å›¾åƒï¼Œå®ç°æ›´çµæ´»çš„ç¼–è¾‘æ•ˆæœã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®è§†è§‰è¾“å…¥å’Œå¯¼èˆªæŒ‡ä»¤ï¼Œç”Ÿæˆä¸­é—´è§†è§‰è¡¨ç¤ºï¼Œå¼•å¯¼æœºå™¨äººå®Œæˆå¯¼èˆªä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The "thinking with images" paradigm represents a pivotal shift in the reasoning of Vision Language Models (VLMs), moving from text-dominant chain-of-thought to image-interactive reasoning. By invoking visual tools or generating intermediate visual representations, VLMs can iteratively attend to fine-grained regions, enabling deeper image understanding and more faithful multimodal reasoning. As an emerging paradigm, however, it still leaves substantial room for exploration in data construction accuracy, structural design, and broader application scenarios, which offer rich opportunities for advancing multimodal reasoning. To further advance this line of work, we present DeepSketcher, a comprehensive suite comprising both an image-text interleaved dataset and a self-contained model. The dataset contains 31k chain-of-thought (CoT) reasoning trajectories with diverse tool calls and resulting edited images, covering a wide range of data types and manipulation instructions with high annotation accuracy. Building on this resource, we design a model that performs interleaved image-text reasoning and natively generates "visual thoughts" by operating directly in the visual embedding space, rather than invoking external tools and repeatedly re-encoding generated images. This design enables tool-free and more flexible "thinking with images". Extensive experiments on multimodal reasoning benchmarks demonstrate strong performance, validating both the utility of the dataset and the effectiveness of the model design.

