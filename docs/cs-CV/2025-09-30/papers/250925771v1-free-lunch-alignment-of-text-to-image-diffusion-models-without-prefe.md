---
layout: default
title: Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs
---

# Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25771v1</a>
  <a href="https://arxiv.org/pdf/2509.25771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25771v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25771v1', 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–‡æœ¬åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰ï¼Œå®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…æ ‡æ³¨å¯¹é½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `æ–‡æœ¬åå¥½ä¼˜åŒ–` `æ— ç›‘ç£å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¾èµ–äººå·¥æ ‡æ³¨çš„å›¾åƒåå¥½æ•°æ®è¿›è¡Œå¯¹é½ï¼Œæˆæœ¬é«˜æ˜‚ä¸”é™åˆ¶äº†æ¨¡å‹æ‰©å±•ã€‚
2. TPOé€šè¿‡è®­ç»ƒæ¨¡å‹åŒºåˆ†åŒ¹é…å’Œä¸åŒ¹é…çš„æ–‡æœ¬æç¤ºï¼Œæ— éœ€å›¾åƒåå¥½æ•°æ®å³å¯å®ç°æ¨¡å‹å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTPOæ¡†æ¶ä¸‹çš„TDPOå’ŒTKTOæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸå§‹DPOå’ŒKTOã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ–‡æœ¬ä¸ç”Ÿæˆå›¾åƒçš„å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼Œä¾èµ–å›¾åƒåå¥½æ•°æ®æˆ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œéœ€è¦å¤§é‡æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†æ–‡æœ¬åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶ï¼Œæ— éœ€æˆå¯¹å›¾åƒåå¥½æ•°æ®å³å¯å®ç°T2Iæ¨¡å‹çš„â€œå…è´¹åˆé¤â€å¯¹é½ã€‚TPOé€šè¿‡è®­ç»ƒæ¨¡å‹åå¥½åŒ¹é…çš„æç¤ºè€Œéä¸åŒ¹é…çš„æç¤ºæ¥å®ç°å¯¹é½ï¼Œä¸åŒ¹é…çš„æç¤ºç”±å¤§å‹è¯­è¨€æ¨¡å‹æ‰°åŠ¨åŸå§‹æ ‡é¢˜ç”Ÿæˆã€‚è¯¥æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä¸ç°æœ‰åŸºäºåå¥½çš„ç®—æ³•å…¼å®¹ã€‚æˆ‘ä»¬å°†DPOå’ŒKTOæ‰©å±•åˆ°æˆ‘ä»¬çš„è®¾ç½®ï¼Œåˆ†åˆ«å¾—åˆ°TDPOå’ŒTKTOã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå…¶åŸå§‹å¯¹åº”æ–¹æ³•ï¼Œæä¾›æ›´å¥½çš„äººç±»åå¥½åˆ†æ•°å’Œæ”¹è¿›çš„æ–‡æœ¬åˆ°å›¾åƒå¯¹é½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒå¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦å€ŸåŠ©å¼ºåŒ–å­¦ä¹ å’Œäººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œä¼˜åŒ–ã€‚ç„¶è€Œï¼ŒRLHFæ–¹æ³•ä¾èµ–äºå¤§é‡äººå·¥æ ‡æ³¨çš„å›¾åƒåå¥½æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯¹é½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯æœ¬èº«ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒæ¨¡å‹åŒºåˆ†åŒ¹é…å’Œä¸åŒ¹é…çš„æ–‡æœ¬æç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç»™å®šçš„å›¾åƒï¼Œæ¨¡å‹åº”è¯¥æ›´åå¥½ä¸å…¶åŸå§‹æ–‡æœ¬æè¿°ç›¸åŒ¹é…çš„æç¤ºï¼Œè€Œä¸æ˜¯ç»è¿‡æ‰°åŠ¨åä¸åŒ¹é…çš„æç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTPOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹åŸå§‹æ–‡æœ¬æç¤ºè¿›è¡Œæ‰°åŠ¨ï¼Œç”Ÿæˆä¸åŒ¹é…çš„æç¤ºã€‚2) å°†åŸå§‹æç¤ºå’Œæ‰°åŠ¨åçš„æç¤ºè¾“å…¥åˆ°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”Ÿæˆå¯¹åº”çš„å›¾åƒã€‚3) ä½¿ç”¨åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚DPOæˆ–KTOï¼‰è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åå¥½ä¸åŸå§‹æç¤ºå¯¹åº”çš„å›¾åƒï¼Œè€Œéä¸æ‰°åŠ¨æç¤ºå¯¹åº”çš„å›¾åƒã€‚4) å°†DPOå’ŒKTOæ‰©å±•åˆ°TPOæ¡†æ¶ï¼Œåˆ†åˆ«å¾—åˆ°TDPOå’ŒTKTOã€‚

**å…³é”®åˆ›æ–°**ï¼šTPOçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯æœ¬èº«ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œé¿å…äº†å¯¹äººå·¥æ ‡æ³¨çš„å›¾åƒåå¥½æ•°æ®çš„ä¾èµ–ï¼Œå®ç°äº†â€œå…è´¹åˆé¤â€å¼çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„RLHFæ–¹æ³•ç›¸æ¯”ï¼ŒTPOå…·æœ‰æ›´å¼ºçš„å¯æ‰©å±•æ€§å’Œæ›´ä½çš„æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨TPOæ¡†æ¶ä¸­ï¼Œå…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡çš„ä¸åŒ¹é…æç¤ºï¼Œä»¥ä¿è¯è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚2) å¦‚ä½•é€‰æ‹©åˆé€‚çš„åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚DPOæˆ–KTOï¼‰ï¼Œä»¥å®ç°æœ€ä½³çš„å¯¹é½æ•ˆæœã€‚3) å¦‚ä½•å¹³è¡¡åŸå§‹æç¤ºå’Œæ‰°åŠ¨æç¤ºä¹‹é—´çš„åå¥½å¼ºåº¦ï¼Œä»¥é¿å…æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTPOæ¡†æ¶ä¸‹çš„TDPOå’ŒTKTOæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸå§‹DPOå’ŒKTOã€‚ä¾‹å¦‚ï¼Œåœ¨Human Preference ScoreæŒ‡æ ‡ä¸Šï¼ŒTDPOå’ŒTKTOç›¸æ¯”DPOå’ŒKTOåˆ†åˆ«æå‡äº†X%å’ŒY%ï¼ˆå…·ä½“æ•°æ®è®ºæ–‡ä¸­ç»™å‡ºï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTPOèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯¹é½ç¨‹åº¦ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„å›¾åƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆåœºæ™¯ï¼Œä¾‹å¦‚å›¾åƒç¼–è¾‘ã€å†…å®¹åˆ›ä½œã€è™šæ‹Ÿç°å®ç­‰ã€‚é€šè¿‡æé«˜æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œå¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æ„å›¾ã€æ›´å…·åˆ›æ„å’Œæ›´é«˜è´¨é‡çš„å›¾åƒå†…å®¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨ï¼Œé™ä½äº†æ¨¡å‹è®­ç»ƒçš„æˆæœ¬ï¼Œæœ‰åˆ©äºæ¨åŠ¨æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.

