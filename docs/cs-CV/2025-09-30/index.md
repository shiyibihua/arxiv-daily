---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-30
---

# cs.CVï¼ˆ2025-09-30ï¼‰

ğŸ“Š å…± **62** ç¯‡è®ºæ–‡
 | ğŸ”— **17** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (20 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (6)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (20 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250925699v1-aimcot-active-information-driven-multimodal-chain-of-thought-for-vis.html">AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning</a></td>
  <td>æå‡ºAIMCoTï¼Œé€šè¿‡ä¸»åŠ¨ä¿¡æ¯é©±åŠ¨çš„å¤šæ¨¡æ€æ€ç»´é“¾æå‡è§†è§‰-è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25699v1" data-paper-url="./papers/250925699v1-aimcot-active-information-driven-multimodal-chain-of-thought-for-vis.html" onclick="toggleFavorite(this, '2509.25699v1', 'AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250925620v1-lmod-a-comprehensive-multimodal-dataset-and-benchmark-for-developing.html">LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology</a></td>
  <td>æå‡ºLMOD+çœ¼ç§‘å¤šæ¨¡æ€æ•°æ®é›†ä¸åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25620v1" data-paper-url="./papers/250925620v1-lmod-a-comprehensive-multimodal-dataset-and-benchmark-for-developing.html" onclick="toggleFavorite(this, '2509.25620v1', 'LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250926016v1-geolink-empowering-remote-sensing-foundation-model-with-openstreetma.html">GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data</a></td>
  <td>GeoLinkï¼šåˆ©ç”¨OpenStreetMapæ•°æ®å¢å¼ºé¥æ„ŸåŸºç¡€æ¨¡å‹ï¼Œæå‡åœ°ç†ç©ºé—´æ™ºèƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26016v1" data-paper-url="./papers/250926016v1-geolink-empowering-remote-sensing-foundation-model-with-openstreetma.html" onclick="toggleFavorite(this, '2509.26016v1', 'GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250925851v1-muslr-multimodal-symbolic-logical-reasoning.html">MuSLR: Multimodal Symbolic Logical Reasoning</a></td>
  <td>æå‡ºMuSLRåŸºå‡†æµ‹è¯•å¤šæ¨¡æ€ç¬¦å·é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œå¹¶æå‡ºLogiCAMæ¡†æ¶æå‡æ¨ç†æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25851v1" data-paper-url="./papers/250925851v1-muslr-multimodal-symbolic-logical-reasoning.html" onclick="toggleFavorite(this, '2509.25851v1', 'MuSLR: Multimodal Symbolic Logical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250926641v1-query-kontext-an-unified-multimodal-model-for-image-generation-and-e.html">Query-Kontext: An Unified Multimodal Model for Image Generation and Editing</a></td>
  <td>æå‡ºQuery-Kontextä»¥æå‡å¤šæ¨¡æ€å›¾åƒç”Ÿæˆä¸ç¼–è¾‘èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26641v1" data-paper-url="./papers/250926641v1-query-kontext-an-unified-multimodal-model-for-image-generation-and-e.html" onclick="toggleFavorite(this, '2509.26641v1', 'Query-Kontext: An Unified Multimodal Model for Image Generation and Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250926378v1-mr2-bench-going-beyond-matching-to-reasoning-in-multimodal-retrieval.html">MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval</a></td>
  <td>æå‡ºMR$^2$-Benchï¼Œä¸€ä¸ªé¢å‘å¤šæ¨¡æ€æ£€ç´¢æ¨ç†èƒ½åŠ›çš„ç»¼åˆæ€§è¯„æµ‹åŸºå‡†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26378v1" data-paper-url="./papers/250926378v1-mr2-bench-going-beyond-matching-to-reasoning-in-multimodal-retrieval.html" onclick="toggleFavorite(this, '2509.26378v1', 'MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250925896v2-llavashield-safeguarding-multimodal-multi-turn-dialogues-in-vision-l.html">LLaVAShield: Safeguarding Multimodal Multi-Turn Dialogues in Vision-Language Models</a></td>
  <td>æå‡ºLLaVAShieldï¼Œç”¨äºä¿éšœè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­å¤šæ¨¡æ€å¤šè½®å¯¹è¯çš„å®‰å…¨æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25896v2" data-paper-url="./papers/250925896v2-llavashield-safeguarding-multimodal-multi-turn-dialogues-in-vision-l.html" onclick="toggleFavorite(this, '2509.25896v2', 'LLaVAShield: Safeguarding Multimodal Multi-Turn Dialogues in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250925889v2-a-multimodal-llm-approach-for-visual-question-answering-on-multipara.html">A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI</a></td>
  <td>æå‡ºmpLLMï¼Œç”¨äºå¤šå‚æ•°3Dè„‘éƒ¨MRIçš„è§†è§‰é—®ç­”ï¼Œæå‡åŒ»å­¦è¯Šæ–­æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25889v2" data-paper-url="./papers/250925889v2-a-multimodal-llm-approach-for-visual-question-answering-on-multipara.html" onclick="toggleFavorite(this, '2509.25889v2', 'A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250926330v1-square-semantic-query-augmented-fusion-and-efficient-batch-reranking.html">SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval</a></td>
  <td>æå‡ºSQUAREæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å¢å¼ºå’Œé«˜æ•ˆé‡æ’åºå®ç°å…è®­ç»ƒé›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26330v1" data-paper-url="./papers/250926330v1-square-semantic-query-augmented-fusion-and-efficient-batch-reranking.html" onclick="toggleFavorite(this, '2509.26330v1', 'SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250925818v1-vela-an-llm-hybrid-as-a-judge-approach-for-evaluating-long-image-cap.html">VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions</a></td>
  <td>VELAï¼šæå‡ºä¸€ç§LLMæ··åˆåˆ¤åˆ«å™¨æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°é•¿å›¾åƒæè¿°</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25818v1" data-paper-url="./papers/250925818v1-vela-an-llm-hybrid-as-a-judge-approach-for-evaluating-long-image-cap.html" onclick="toggleFavorite(this, '2509.25818v1', 'VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250925811v1-logo-vgr-visual-grounded-reasoning-for-open-world-logo-recognition.html">Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition</a></td>
  <td>æå‡ºLogo-VGRï¼Œé€šè¿‡è§†è§‰å¸¸è¯†æ¨ç†å®ç°å¼€æ”¾ä¸–ç•ŒLogoè¯†åˆ«</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25811v1" data-paper-url="./papers/250925811v1-logo-vgr-visual-grounded-reasoning-for-open-world-logo-recognition.html" onclick="toggleFavorite(this, '2509.25811v1', 'Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250925745v1-fincap-topic-aligned-captions-for-short-form-financial-youtube-video.html">FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos</a></td>
  <td>FinCapï¼šæå‡ºä¸»é¢˜å¯¹é½çš„é‡‘èçŸ­è§†é¢‘å­—å¹•ç”Ÿæˆæ–¹æ³•ï¼Œè§£å†³å¤šæ¨¡æ€ä¿¡æ¯èåˆéš¾é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25745v1" data-paper-url="./papers/250925745v1-fincap-topic-aligned-captions-for-short-form-financial-youtube-video.html" onclick="toggleFavorite(this, '2509.25745v1', 'FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251001284v1-ovi-twin-backbone-cross-modal-fusion-for-audio-video-generation.html">Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation</a></td>
  <td>Oviï¼šåŸºäºå­ªç”Ÿéª¨å¹²è·¨æ¨¡æ€èåˆçš„éŸ³è§†é¢‘ç”Ÿæˆæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01284v1" data-paper-url="./papers/251001284v1-ovi-twin-backbone-cross-modal-fusion-for-audio-video-generation.html" onclick="toggleFavorite(this, '2510.01284v1', 'Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250926645v3-ttt3r-3d-reconstruction-as-test-time-training.html">TTT3R: 3D Reconstruction as Test-Time Training</a></td>
  <td>TTT3Rï¼šå°†æµ‹è¯•æ—¶è®­ç»ƒåº”ç”¨äº3Dé‡å»ºï¼Œæ˜¾è‘—æå‡é•¿åºåˆ—æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26645v3" data-paper-url="./papers/250926645v3-ttt3r-3d-reconstruction-as-test-time-training.html" onclick="toggleFavorite(this, '2509.26645v3', 'TTT3R: 3D Reconstruction as Test-Time Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250926604v1-video-object-segmentation-aware-audio-generation.html">Video Object Segmentation-Aware Audio Generation</a></td>
  <td>æå‡ºSAGANetï¼Œé€šè¿‡è§†é¢‘å¯¹è±¡åˆ†å‰²å®ç°å¯æ§éŸ³é¢‘ç”Ÿæˆï¼Œæå‡Foleyå·¥ä½œæµæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26604v1" data-paper-url="./papers/250926604v1-video-object-segmentation-aware-audio-generation.html" onclick="toggleFavorite(this, '2509.26604v1', 'Video Object Segmentation-Aware Audio Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250926360v3-timescope-towards-task-oriented-temporal-grounding-in-long-videos.html">TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos</a></td>
  <td>æå‡ºTimeScopeï¼Œè§£å†³é•¿è§†é¢‘ä¸­é¢å‘ä»»åŠ¡çš„æ—¶åºå®šä½éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26360v3" data-paper-url="./papers/250926360v3-timescope-towards-task-oriented-temporal-grounding-in-long-videos.html" onclick="toggleFavorite(this, '2509.26360v3', 'TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250926225v1-an-experimental-study-on-generating-plausible-textual-explanations-f.html">An Experimental Study on Generating Plausible Textual Explanations for Video Summarization</a></td>
  <td>æå‡ºä¸€ç§åŸºäºå¤§æ¨¡å‹å’Œè¯­ä¹‰é‡å çš„è§†é¢‘æ‘˜è¦å¯ä¿¡è§£é‡Šç”Ÿæˆä¸è¯„ä¼°æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26225v1" data-paper-url="./papers/250926225v1-an-experimental-study-on-generating-plausible-textual-explanations-f.html" onclick="toggleFavorite(this, '2509.26225v1', 'An Experimental Study on Generating Plausible Textual Explanations for Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250925989v2-towards-reliable-and-holistic-visual-in-context-learning-prompt-sele.html">Towards Reliable and Holistic Visual In-Context Learning Prompt Selection</a></td>
  <td>æå‡ºRH-Partial2Globalï¼Œæå‡è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­prompté€‰æ‹©çš„å¯é æ€§å’Œå…¨é¢æ€§</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25989v2" data-paper-url="./papers/250925989v2-towards-reliable-and-holistic-visual-in-context-learning-prompt-sele.html" onclick="toggleFavorite(this, '2509.25989v2', 'Towards Reliable and Holistic Visual In-Context Learning Prompt Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250925856v1-patchead-unifying-industrial-visual-prompting-frameworks-for-patch-e.html">PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection</a></td>
  <td>PatchEADï¼šç»Ÿä¸€çš„å·¥ä¸šè§†è§‰æç¤ºæ¡†æ¶ï¼Œç”¨äºè¡¥ä¸äº’æ–¥å¼‚å¸¸æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25856v1" data-paper-url="./papers/250925856v1-patchead-unifying-industrial-visual-prompting-frameworks-for-patch-e.html" onclick="toggleFavorite(this, '2509.25856v1', 'PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250925805v1-adapting-sam-with-dynamic-similarity-graphs-for-few-shot-parameter-e.html">Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions</a></td>
  <td>æå‡ºåŸºäºåŠ¨æ€ç›¸ä¼¼å›¾çš„SAMè‡ªé€‚åº”æ–¹æ³•ï¼Œç”¨äºå°‘æ ·æœ¬å¯†é›†å°ç›®æ ‡æ£€æµ‹ï¼Œä»¥ç”°é—´é¹°å˜´è±†èšä¸ºä¾‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25805v1" data-paper-url="./papers/250925805v1-adapting-sam-with-dynamic-similarity-graphs-for-few-shot-parameter-e.html" onclick="toggleFavorite(this, '2509.25805v1', 'Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250926251v1-seeing-space-and-motion-enhancing-latent-actions-with-spatial-and-dy.html">Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA</a></td>
  <td>æå‡ºFarsighted-LAMå’ŒSSM-VLAï¼Œå¢å¼ºVLAç³»ç»Ÿä¸­æ½œåœ¨åŠ¨ä½œæ¨¡å‹çš„ç©ºé—´å’ŒåŠ¨æ€æ„ŸçŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">SSM</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26251v1" data-paper-url="./papers/250926251v1-seeing-space-and-motion-enhancing-latent-actions-with-spatial-and-dy.html" onclick="toggleFavorite(this, '2509.26251v1', 'Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250925717v1-importance-sampling-for-multi-negative-multimodal-direct-preference-.html">Importance Sampling for Multi-Negative Multimodal Direct Preference Optimization</a></td>
  <td>æå‡ºMISP-DPOæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€åå¥½ä¼˜åŒ–ä¸­çš„è´Ÿæ ·æœ¬é€‰æ‹©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25717v1" data-paper-url="./papers/250925717v1-importance-sampling-for-multi-negative-multimodal-direct-preference-.html" onclick="toggleFavorite(this, '2509.25717v1', 'Importance Sampling for Multi-Negative Multimodal Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250926231v1-img-calibrating-diffusion-models-via-implicit-multimodal-guidance.html">IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance</a></td>
  <td>æå‡ºIMGï¼Œé€šè¿‡éšå¼å¤šæ¨¡æ€å¼•å¯¼æ ¡å‡†æ‰©æ•£æ¨¡å‹ï¼Œæå‡å›¾æ–‡å¯¹é½ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26231v1" data-paper-url="./papers/250926231v1-img-calibrating-diffusion-models-via-implicit-multimodal-guidance.html" onclick="toggleFavorite(this, '2509.26231v1', 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250925638v1-generalized-contrastive-learning-for-universal-multimodal-retrieval.html">Generalized Contrastive Learning for Universal Multimodal Retrieval</a></td>
  <td>æå‡ºå¹¿ä¹‰å¯¹æ¯”å­¦ä¹ GCLï¼Œè§£å†³é€šç”¨å¤šæ¨¡æ€æ£€ç´¢ä¸­ç»„åˆæ¨¡æ€æ³›åŒ–æ€§é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25638v1" data-paper-url="./papers/250925638v1-generalized-contrastive-learning-for-universal-multimodal-retrieval.html" onclick="toggleFavorite(this, '2509.25638v1', 'Generalized Contrastive Learning for Universal Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250925711v1-probmed-a-probabilistic-framework-for-medical-multimodal-binding.html">ProbMed: A Probabilistic Framework for Medical Multimodal Binding</a></td>
  <td>ProbMEDï¼šæå‡ºæ¦‚ç‡å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæå‡åŒ»å­¦å½±åƒä¸æ–‡æœ¬çš„è”åˆè¯Šæ–­èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25711v1" data-paper-url="./papers/250925711v1-probmed-a-probabilistic-framework-for-medical-multimodal-binding.html" onclick="toggleFavorite(this, '2509.25711v1', 'ProbMed: A Probabilistic Framework for Medical Multimodal Binding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250926497v1-revealing-the-power-of-post-training-for-small-language-models-via-k.html">Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation</a></td>
  <td>æå‡ºåŸºäºçŸ¥è¯†è’¸é¦çš„åè®­ç»ƒæµç¨‹ï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26497v1" data-paper-url="./papers/250926497v1-revealing-the-power-of-post-training-for-small-language-models-via-k.html" onclick="toggleFavorite(this, '2509.26497v1', 'Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250926272v2-prpo-paragraph-level-policy-optimization-for-vision-language-deepfak.html">PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection</a></td>
  <td>æå‡ºPRPOç®—æ³•ï¼Œé€šè¿‡æ®µè½çº§ç­–ç•¥ä¼˜åŒ–æå‡è§†è§‰-è¯­è¨€å¤§æ¨¡å‹åœ¨Deepfakeæ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26272v2" data-paper-url="./papers/250926272v2-prpo-paragraph-level-policy-optimization-for-vision-language-deepfak.html" onclick="toggleFavorite(this, '2509.26272v2', 'PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250925963v1-self-supervised-anatomical-consistency-learning-for-vision-grounded-.html">Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation</a></td>
  <td>æå‡ºè‡ªç›‘ç£è§£å‰–ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§†è§‰å¼•å¯¼çš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25963v1" data-paper-url="./papers/250925963v1-self-supervised-anatomical-consistency-learning-for-vision-grounded-.html" onclick="toggleFavorite(this, '2509.25963v1', 'Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250925848v2-more-thought-less-accuracy-on-the-dual-nature-of-reasoning-in-vision.html">More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models</a></td>
  <td>æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†çš„äºŒå…ƒæ€§ï¼Œæå‡ºVAPOä¼˜åŒ–è§†è§‰æ„ŸçŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25848v2" data-paper-url="./papers/250925848v2-more-thought-less-accuracy-on-the-dual-nature-of-reasoning-in-vision.html" onclick="toggleFavorite(this, '2509.25848v2', 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250925771v1-free-lunch-alignment-of-text-to-image-diffusion-models-without-prefe.html">Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs</a></td>
  <td>æå‡ºæ–‡æœ¬åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰ï¼Œå®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å…æ ‡æ³¨å¯¹é½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">DPO</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25771v1" data-paper-url="./papers/250925771v1-free-lunch-alignment-of-text-to-image-diffusion-models-without-prefe.html" onclick="toggleFavorite(this, '2509.25771v1', 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250925748v3-dolphin-v10-technical-report.html">Dolphin v1.0 Technical Report</a></td>
  <td>Dolphin v1.0ï¼šé¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€è¶…å£°å½±åƒåŸºç¡€æ¨¡å‹ï¼Œç»Ÿä¸€è§£å†³å¤šç§ä¸´åºŠä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25748v3" data-paper-url="./papers/250925748v3-dolphin-v10-technical-report.html" onclick="toggleFavorite(this, '2509.25748v3', 'Dolphin v1.0 Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250926219v2-beyond-pixels-efficient-dataset-distillation-via-sparse-gaussian-rep.html">Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation</a></td>
  <td>æå‡ºåŸºäºç¨€ç–é«˜æ–¯è¡¨ç¤ºçš„æ•°æ®é›†è’¸é¦æ–¹æ³•GSDDï¼Œæå‡æ•ˆç‡ä¸æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26219v2" data-paper-url="./papers/250926219v2-beyond-pixels-efficient-dataset-distillation-via-sparse-gaussian-rep.html" onclick="toggleFavorite(this, '2509.26219v2', 'Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250926539v1-ferret-ui-lite-lessons-from-building-small-on-device-gui-agents.html">Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents</a></td>
  <td>æå‡ºFerret-UI Liteï¼Œä¸€ä¸ªç´§å‡‘å‹ç«¯åˆ°ç«¯GUIæ™ºèƒ½ä½“ï¼Œç”¨äºè·¨å¹³å°äº¤äº’ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26539v1" data-paper-url="./papers/250926539v1-ferret-ui-lite-lessons-from-building-small-on-device-gui-agents.html" onclick="toggleFavorite(this, '2509.26539v1', 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250926287v1-flower-a-flow-matching-solver-for-inverse-problems.html">FLOWER: A Flow-Matching Solver for Inverse Problems</a></td>
  <td>æå‡ºFLOWERï¼Œä¸€ç§åŸºäºFlow-Matchingçš„é€†é—®é¢˜æ±‚è§£å™¨</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26287v1" data-paper-url="./papers/250926287v1-flower-a-flow-matching-solver-for-inverse-problems.html" onclick="toggleFavorite(this, '2509.26287v1', 'FLOWER: A Flow-Matching Solver for Inverse Problems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250926227v1-generalized-fine-grained-category-discovery-with-multi-granularity-c.html">Generalized Fine-Grained Category Discovery with Multi-Granularity Conceptual Experts</a></td>
  <td>æå‡ºå¤šç²’åº¦æ¦‚å¿µä¸“å®¶ç½‘ç»œMGCEï¼Œè§£å†³å¹¿ä¹‰ç»†ç²’åº¦ç±»åˆ«å‘ç°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26227v1" data-paper-url="./papers/250926227v1-generalized-fine-grained-category-discovery-with-multi-granularity-c.html" onclick="toggleFavorite(this, '2509.26227v1', 'Generalized Fine-Grained Category Discovery with Multi-Granularity Conceptual Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/250925866v1-deepsketcher-internalizing-visual-manipulation-for-multimodal-reason.html">DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning</a></td>
  <td>DeepSketcherï¼šé€šè¿‡å†…éƒ¨è§†è§‰æ“ä½œå®ç°å¤šæ¨¡æ€æ¨ç†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25866v1" data-paper-url="./papers/250925866v1-deepsketcher-internalizing-visual-manipulation-for-multimodal-reason.html" onclick="toggleFavorite(this, '2509.25866v1', 'DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250925794v1-point-it-out-benchmarking-embodied-reasoning-for-vision-language-mod.html">Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding</a></td>
  <td>æå‡ºPoint-It-OutåŸºå‡†ï¼Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šé˜¶æ®µè§†è§‰å®šä½ä¸­çš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25794v1" data-paper-url="./papers/250925794v1-point-it-out-benchmarking-embodied-reasoning-for-vision-language-mod.html" onclick="toggleFavorite(this, '2509.25794v1', 'Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250925731v1-lato-landmark-tokenized-diffusion-transformer-for-fine-grained-human.html">LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing</a></td>
  <td>LaToï¼šç”¨äºç²¾ç»†äººè„¸ç¼–è¾‘çš„åœ°æ ‡TokenåŒ–æ‰©æ•£Transformer</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25731v1" data-paper-url="./papers/250925731v1-lato-landmark-tokenized-diffusion-transformer-for-fine-grained-human.html" onclick="toggleFavorite(this, '2509.25731v1', 'LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/251000086v1-behavioural-classification-in-c-elegans-a-spatio-temporal-analysis-o.html">Behavioural Classification in C. elegans: a Spatio-Temporal Analysis of Locomotion</a></td>
  <td>æå‡ºä¸€ç§åŸºäºæ—¶ç©ºåˆ†æçš„çº¿è™«è¡Œä¸ºè‡ªåŠ¨åˆ†ç±»æ–¹æ³•ï¼Œæ— éœ€å®Œæ•´è™«ä½“è§†å›¾ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00086v1" data-paper-url="./papers/251000086v1-behavioural-classification-in-c-elegans-a-spatio-temporal-analysis-o.html" onclick="toggleFavorite(this, '2510.00086v1', 'Behavioural Classification in C. elegans: a Spatio-Temporal Analysis of Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250925776v3-editable-noise-map-inversion-encoding-target-image-into-noise-for-hi.html">Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation</a></td>
  <td>æå‡ºå¯ç¼–è¾‘å™ªå£°å›¾åæ¼”æ–¹æ³•ï¼Œæå‡æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25776v3" data-paper-url="./papers/250925776v3-editable-noise-map-inversion-encoding-target-image-into-noise-for-hi.html" onclick="toggleFavorite(this, '2509.25776v3', 'Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250926047v1-dgm4-dataset-extension-for-global-scene-inconsistency.html">DGM4+: Dataset Extension for Global Scene Inconsistency</a></td>
  <td>DGM4+ï¼šæ‰©å±•æ•°æ®é›†ä»¥åº”å¯¹å…¨å±€åœºæ™¯ä¸ä¸€è‡´æ€§ï¼Œæå‡å¤šæ¨¡æ€ä¼ªé€ æ£€æµ‹èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26047v1" data-paper-url="./papers/250926047v1-dgm4-dataset-extension-for-global-scene-inconsistency.html" onclick="toggleFavorite(this, '2509.26047v1', 'DGM4+: Dataset Extension for Global Scene Inconsistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250926039v1-sgs-segmentation-guided-scoring-for-global-scene-inconsistencies.html">SGS: Segmentation-Guided Scoring for Global Scene Inconsistencies</a></td>
  <td>æå‡ºSGSï¼šä¸€ç§åˆ†å‰²å¼•å¯¼çš„è¯„åˆ†æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å…¨å±€åœºæ™¯ä¸ä¸€è‡´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26039v1" data-paper-url="./papers/250926039v1-sgs-segmentation-guided-scoring-for-global-scene-inconsistencies.html" onclick="toggleFavorite(this, '2509.26039v1', 'SGS: Segmentation-Guided Scoring for Global Scene Inconsistencies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250925970v1-pinpoint3d-fine-grained-3d-part-segmentation-from-a-few-clicks.html">PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks</a></td>
  <td>PinPoint3Dï¼šæå‡ºä¸€ç§åŸºäºå°‘é‡ç‚¹å‡»çš„ç²¾ç»†3Déƒ¨ä»¶åˆ†å‰²äº¤äº’å¼æ¡†æ¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25970v1" data-paper-url="./papers/250925970v1-pinpoint3d-fine-grained-3d-part-segmentation-from-a-few-clicks.html" onclick="toggleFavorite(this, '2509.25970v1', 'PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250926599v1-diffcamera-arbitrary-refocusing-on-images.html">DiffCamera: Arbitrary Refocusing on Images</a></td>
  <td>DiffCameraï¼šæå‡ºä¸€ç§åŸºäºæ‰©æ•£Transformerçš„å›¾åƒä»»æ„é‡èšç„¦æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26599v1" data-paper-url="./papers/250926599v1-diffcamera-arbitrary-refocusing-on-images.html" onclick="toggleFavorite(this, '2509.26599v1', 'DiffCamera: Arbitrary Refocusing on Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250925740v1-dragging-with-geometry-from-pixels-to-geometry-guided-image-editing.html">Dragging with Geometry: From Pixels to Geometry-Guided Image Editing</a></td>
  <td>æå‡ºGeoDragï¼Œé€šè¿‡å‡ ä½•å¼•å¯¼å®ç°ç²¾ç¡®ã€ç»“æ„ä¸€è‡´çš„å›¾åƒæ‹–æ‹½ç¼–è¾‘</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25740v1" data-paper-url="./papers/250925740v1-dragging-with-geometry-from-pixels-to-geometry-guided-image-editing.html" onclick="toggleFavorite(this, '2509.25740v1', 'Dragging with Geometry: From Pixels to Geometry-Guided Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/250926165v3-human-mme-a-holistic-evaluation-benchmark-for-human-centric-multimod.html">Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models</a></td>
  <td>æå‡ºHuman-MMEåŸºå‡†ï¼Œç”¨äºå…¨é¢è¯„ä¼°ä»¥äººä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26165v3" data-paper-url="./papers/250926165v3-human-mme-a-holistic-evaluation-benchmark-for-human-centric-multimod.html" onclick="toggleFavorite(this, '2509.26165v3', 'Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/250926004v2-learning-egocentric-in-hand-object-segmentation-through-weak-supervi.html">Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</a></td>
  <td>æå‡ºåŸºäºäººç±»å™è¿°å¼±ç›‘ç£çš„å•ç›®æ‰‹æŒç‰©ä½“åˆ†å‰²æ–¹æ³•NS-iHOS</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">human-object interaction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26004v2" data-paper-url="./papers/250926004v2-learning-egocentric-in-hand-object-segmentation-through-weak-supervi.html" onclick="toggleFavorite(this, '2509.26004v2', 'Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250926455v1-stylos-multi-view-3d-stylization-with-single-forward-gaussian-splatt.html">Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting</a></td>
  <td>Stylosï¼šåŸºäºå•æ¬¡å‰å‘é«˜æ–¯æº…å°„çš„å¤šè§†è§’3Dé£æ ¼è¿ç§»</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26455v1" data-paper-url="./papers/250926455v1-stylos-multi-view-3d-stylization-with-single-forward-gaussian-splatt.html" onclick="toggleFavorite(this, '2509.26455v1', 'Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250926618v5-da2-depth-anything-in-any-direction.html">DA$^{2}$: Depth Anything in Any Direction</a></td>
  <td>æå‡ºDAÂ²ï¼Œè§£å†³å…¨æ™¯æ·±åº¦ä¼°è®¡çš„é›¶æ ·æœ¬æ³›åŒ–ä¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">Depth Anything</span> <span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26618v5" data-paper-url="./papers/250926618v5-da2-depth-anything-in-any-direction.html" onclick="toggleFavorite(this, '2509.26618v5', 'DA$^{2}$: Depth Anything in Any Direction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250926087v3-easyocc-3d-pseudo-label-supervision-for-fully-self-supervised-semant.html">EasyOcc: 3D Pseudo-Label Supervision for Fully Self-Supervised Semantic Occupancy Prediction Models</a></td>
  <td>EasyOccï¼šåˆ©ç”¨3Dä¼ªæ ‡ç­¾ç›‘ç£å®ç°å…¨è‡ªç›‘ç£è¯­ä¹‰å æ®é¢„æµ‹æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">Metric3D</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26087v3" data-paper-url="./papers/250926087v3-easyocc-3d-pseudo-label-supervision-for-fully-self-supervised-semant.html" onclick="toggleFavorite(this, '2509.26087v3', 'EasyOcc: 3D Pseudo-Label Supervision for Fully Self-Supervised Semantic Occupancy Prediction Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>51</td>
  <td><a href="./papers/250926008v1-pfdepth-heterogeneous-pinhole-fisheye-joint-depth-estimation-via-dis.html">PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion</a></td>
  <td>PFDepthï¼šæå‡ºä¸€ç§åŸºäºç•¸å˜æ„ŸçŸ¥é«˜æ–¯æº…å°„ä½“ç´ èåˆçš„å¼‚æ„é’ˆå­”-é±¼çœ¼è”åˆæ·±åº¦ä¼°è®¡æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26008v1" data-paper-url="./papers/250926008v1-pfdepth-heterogeneous-pinhole-fisheye-joint-depth-estimation-via-dis.html" onclick="toggleFavorite(this, '2509.26008v1', 'PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>52</td>
  <td><a href="./papers/250925744v2-image-plane-geometric-decoding-for-view-invariant-indoor-scene-recon.html">Image-Plane Geometric Decoding for View-Invariant Indoor Scene Reconstruction</a></td>
  <td>æå‡ºå›¾åƒå¹³é¢å‡ ä½•è§£ç æ¡†æ¶ï¼Œè§£å†³å®¤å†…åœºæ™¯é‡å»ºå¯¹è§†è§’ä¾èµ–çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25744v2" data-paper-url="./papers/250925744v2-image-plane-geometric-decoding-for-view-invariant-indoor-scene-recon.html" onclick="toggleFavorite(this, '2509.25744v2', 'Image-Plane Geometric Decoding for View-Invariant Indoor Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>53</td>
  <td><a href="./papers/250925916v1-vlm-fo1-bridging-the-gap-between-high-level-reasoning-and-fine-grain.html">VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs</a></td>
  <td>VLM-FO1ï¼šé€šè¿‡ç‰¹å¾æ£€ç´¢å¼¥åˆVLMé«˜å±‚æ¨ç†ä¸ç»†ç²’åº¦æ„ŸçŸ¥ä¹‹é—´çš„å·®è·</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25916v1" data-paper-url="./papers/250925916v1-vlm-fo1-bridging-the-gap-between-high-level-reasoning-and-fine-grain.html" onclick="toggleFavorite(this, '2509.25916v1', 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>54</td>
  <td><a href="./papers/250926498v1-depthor-robust-depth-enhancement-from-a-real-world-lightweight-dtof-.html">DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance</a></td>
  <td>DEPTHOR++ï¼šé’ˆå¯¹çœŸå®ä¸–ç•ŒdToFå™ªå£°çš„é²æ£’æ·±åº¦å¢å¼ºæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26498v1" data-paper-url="./papers/250926498v1-depthor-robust-depth-enhancement-from-a-real-world-lightweight-dtof-.html" onclick="toggleFavorite(this, '2509.26498v1', 'DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>55</td>
  <td><a href="./papers/250925773v1-v-hub-a-visual-centric-humor-understanding-benchmark-for-video-llms.html">V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs</a></td>
  <td>V-HUBï¼šé¢å‘è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸­å¿ƒå¹½é»˜ç†è§£è¯„æµ‹åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">HuMoR</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25773v1" data-paper-url="./papers/250925773v1-v-hub-a-visual-centric-humor-understanding-benchmark-for-video-llms.html" onclick="toggleFavorite(this, '2509.25773v1', 'V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/250926391v1-motionrag-motion-retrieval-augmented-image-to-video-generation.html">MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</a></td>
  <td>MotionRAGï¼šé€šè¿‡æ£€ç´¢å¢å¼ºè¿åŠ¨å…ˆéªŒå®ç°é€¼çœŸçš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">motion retrieval</span> <span class="paper-tag">motion adaptation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26391v1" data-paper-url="./papers/250926391v1-motionrag-motion-retrieval-augmented-image-to-video-generation.html" onclick="toggleFavorite(this, '2509.26391v1', 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>57</td>
  <td><a href="./papers/250925739v1-liehmr-autoregressive-human-mesh-recovery-with-so3-diffusion.html">LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion</a></td>
  <td>æå‡ºåŸºäº$SO(3)$æ‰©æ•£çš„è‡ªå›å½’äººä½“ç½‘æ ¼é‡å»ºLieHMRæ¨¡å‹ï¼Œè§£å†³å•ç›®å›¾åƒä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡çš„æ­§ä¹‰æ€§é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">human mesh recovery</span> <span class="paper-tag">HMR</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.25739v1" data-paper-url="./papers/250925739v1-liehmr-autoregressive-human-mesh-recovery-with-so3-diffusion.html" onclick="toggleFavorite(this, '2509.25739v1', 'LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>58</td>
  <td><a href="./papers/250926639v1-benchmarking-egocentric-visual-inertial-slam-at-city-scale.html">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></td>
  <td>æå‡ºåŸå¸‚çº§ç¬¬ä¸€äººç§°è§†è§‰æƒ¯æ€§SLAMåŸºå‡†ï¼Œæ­ç¤ºç°æœ‰ç®—æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¸è¶³ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26639v1" data-paper-url="./papers/250926639v1-benchmarking-egocentric-visual-inertial-slam-at-city-scale.html" onclick="toggleFavorite(this, '2509.26639v1', 'Benchmarking Egocentric Visual-Inertial SLAM at City Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>59</td>
  <td><a href="./papers/250926012v1-setr-a-two-stage-semantic-enhanced-framework-for-zero-shot-composed-.html">SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval</a></td>
  <td>æå‡ºSETRï¼šä¸€ç§è¯­ä¹‰å¢å¼ºçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºé›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26012v1" data-paper-url="./papers/250926012v1-setr-a-two-stage-semantic-enhanced-framework-for-zero-shot-composed-.html" onclick="toggleFavorite(this, '2509.26012v1', 'SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>60</td>
  <td><a href="./papers/250926278v1-profvlm-a-lightweight-video-language-model-for-multi-view-proficienc.html">ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation</a></td>
  <td>ProfVLMï¼šè½»é‡çº§è§†é¢‘è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå¤šè§†è§’æŠ€èƒ½ç†Ÿç»ƒåº¦è¯„ä¼°</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26278v1" data-paper-url="./papers/250926278v1-profvlm-a-lightweight-video-language-model-for-multi-view-proficienc.html" onclick="toggleFavorite(this, '2509.26278v1', 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>61</td>
  <td><a href="./papers/250926644v1-stitch-training-free-position-control-in-multimodal-diffusion-transf.html">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></td>
  <td>Stitchï¼šä¸€ç§å…è®­ç»ƒçš„å¤šæ¨¡æ€æ‰©æ•£Transformerä½ç½®æ§åˆ¶æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26644v1" data-paper-url="./papers/250926644v1-stitch-training-free-position-control-in-multimodal-diffusion-transf.html" onclick="toggleFavorite(this, '2509.26644v1', 'Stitch: Training-Free Position Control in Multimodal Diffusion Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>62</td>
  <td><a href="./papers/250926621v1-hart-human-aligned-reconstruction-transformer.html">HART: Human Aligned Reconstruction Transformer</a></td>
  <td>HARTï¼šæå‡ºä¸€ç§å¯¹é½äººä½“çš„é‡å»ºTransformerï¼Œç”¨äºç¨€ç–è§†è§’äººä½“é‡å»ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">SMPL</span> <span class="paper-tag">SMPL-X</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.26621v1" data-paper-url="./papers/250926621v1-hart-human-aligned-reconstruction-transformer.html" onclick="toggleFavorite(this, '2509.26621v1', 'HART: Human Aligned Reconstruction Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)