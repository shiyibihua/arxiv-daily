---
layout: default
title: ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow
---

# ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03218" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03218v1</a>
  <a href="https://arxiv.org/pdf/2508.03218.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03218v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03218v1', 'ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shanshan Guo, Xiwen Liang, Junfan Lin, Yuzheng Zhuang, Liang Lin, Xiaodan Liang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ActionSink‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰ΩúÁ≤æÂ∫¶‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `Âä®‰Ωú‰º∞ËÆ°` `Ëá™ÁõëÁù£Â≠¶‰π†` `Âä®ÊÄÅÈõÜÊàê` `ÂÖâÊµÅ` `Â§öÂ±ÇËûçÂêà` `ÈïøÊó∂Èó¥ËßÜËßâ‰ªªÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®È´òÂ±ÇÊÑüÁü•ÂíåËßÑÂàíÊñπÈù¢‰∏çÊñ≠ËøõÊ≠•Ôºå‰ΩÜ‰ΩéÁ∫ßÂä®‰Ωú‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶‰ªçÁÑ∂‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÊÄßËÉΩ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑActionSinkÊ°ÜÊû∂ÈÄöËøáÂ∞ÜÂä®‰ΩúÈáçÊûÑ‰∏∫ËßÜÈ¢ë‰∏≠ÁöÑÂä®‰ΩúÊµÅÔºåÂà©Áî®Ëá™ÁõëÁù£Â≠¶‰π†ÊèêÂçáÂä®‰Ωú‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåActionSinkÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÊàêÂäüÁéáÊèêÈ´ò‰∫Ü7.9%ÔºåÂú®ÈïøÊó∂Èó¥ËßÜËßâ‰ªªÂä°LIBERO-Long‰∏≠ÂáÜÁ°ÆÁéáÊèêÂçáËøë8%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËØ≠Ë®ÄÊåá‰ª§È©±Âä®ÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÂõ†ÂÖ∂‰ªéÊî∂ÈõÜÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑÊΩúÂäõËÄåÂèóÂà∞ÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºå‰ΩéÁ∫ßÂä®‰Ωú‰º∞ËÆ°ÁöÑ‰ΩéÁ≤æÂ∫¶Â∑≤Êàê‰∏∫Êìç‰ΩúÊÄßËÉΩÁöÑÂÖ≥ÈîÆÈôêÂà∂Âõ†Á¥†„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊ°ÜÊû∂ActionSinkÔºåÊó®Âú®ÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑÂä®‰Ωú‰º∞ËÆ°„ÄÇActionSinkÂ∞ÜÊú∫Âô®‰∫∫Âä®‰ΩúÈáçÊñ∞ÂÆö‰πâ‰∏∫ËßÜÈ¢ë‰∏≠ÁöÑÂä®‰ΩúÂºïËµ∑ÁöÑÂÖâÊµÅÔºåÁß∞‰∏∫‚ÄúÂä®‰ΩúÊµÅ‚ÄùÔºåÂπ∂ÈÄöËøáËá™ÁõëÁù£ÊñπÂºèËøõË°åÊ£ÄÁ¥¢ÂíåÈõÜÊàêÔºå‰ª•Â¢ûÂº∫Âä®‰Ωú‰º∞ËÆ°„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÁ≤óÂà∞ÁªÜÁöÑÂä®‰ΩúÊµÅÂåπÈÖçÂô®ÂíåÂä®ÊÄÅÂä®‰ΩúÊµÅÈõÜÊàêÂô®ÔºåÂêéËÄÖÊúâÊïàÁÆ°ÁêÜÂéÜÂè≤Âä®‰ΩúÊµÅ‰ª•ÊèêÂçáÂΩìÂâçÂä®‰Ωú‰º∞ËÆ°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåActionSinkÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØÔºåÊàêÂäüÁéáÊèêÂçá‰∫Ü7.9%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠‰ΩéÁ∫ßÂä®‰Ωú‰º∞ËÆ°Á≤æÂ∫¶‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®È´òÂ±ÇÊ¨°ÊÑüÁü•ÂíåËßÑÂàí‰∏äÊúâÊâÄËøõÂ±ïÔºå‰ΩÜÂú®‰ΩéÁ∫ßÂä®‰Ωú‰º∞ËÆ°ÊñπÈù¢‰ªçÂ≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÂΩ±Âìç‰∫ÜÊï¥‰ΩìÊìç‰ΩúÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöActionSinkÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÊú∫Âô®‰∫∫Âä®‰ΩúËßÜ‰∏∫ËßÜÈ¢ë‰∏≠ÁöÑÂä®‰ΩúÊµÅÔºåÈÄöËøáËá™ÁõëÁù£Â≠¶‰π†ËøõË°åÊ£ÄÁ¥¢ÂíåÈõÜÊàêÔºå‰ª•ÊèêÈ´òÂä®‰Ωú‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶„ÄÇËøôÁßçËÆæËÆ°ËÉΩÂ§üÊúâÊïàÂà©Áî®ÂéÜÂè≤Êï∞ÊçÆÔºåÂ¢ûÂº∫ÂΩìÂâçÁöÑÂä®‰Ωú‰º∞ËÆ°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöActionSinkÊ°ÜÊû∂‰∏ªË¶ÅÁî±‰∏§‰∏™Ê®°ÂùóÁªÑÊàêÔºöÁ≤óÂà∞ÁªÜÁöÑÂä®‰ΩúÊµÅÂåπÈÖçÂô®ÂíåÂä®ÊÄÅÂä®‰ΩúÊµÅÈõÜÊàêÂô®„ÄÇÂâçËÄÖÈÄöËøáËø≠‰ª£Ê£ÄÁ¥¢ÂíåÂéªÂô™ËøáÁ®ã‰∏çÊñ≠ÊèêÈ´òÂä®‰ΩúÊµÅÁöÑÂáÜÁ°ÆÊÄßÔºåÂêéËÄÖÂàôÈÄöËøáÂä®ÊÄÅÁÆ°ÁêÜÂéÜÂè≤Âä®‰ΩúÊµÅÊù•Â¢ûÂº∫ÂΩìÂâçÁöÑÂä®‰Ωú‰º∞ËÆ°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöActionSinkÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÂä®‰ΩúÊµÅÁöÑÊ¶ÇÂøµÂºïÂÖ•Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÂπ∂ÈÄöËøáÂä®ÊÄÅÈõÜÊàêÂéÜÂè≤Âä®‰ΩúÊµÅÊù•ÊèêÂçáÂΩìÂâçÂä®‰ΩúÁöÑ‰º∞ËÆ°Á≤æÂ∫¶„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÈùôÊÄÅÂä®‰Ωú‰º∞ËÆ°ÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Âä®ÊÄÅÂä®‰ΩúÊµÅÈõÜÊàêÂô®‰∏≠ÔºåËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â§öÂ±ÇËûçÂêàÊ®°ÂùóÔºåÁî®‰∫éÊï¥ÂêàÊù•Ëá™ÂΩìÂâçÂíåÂ∑•‰ΩúËÆ∞ÂøÜÁöÑÁõ¥Êé•‰º∞ËÆ°ÂíåÂä®‰ΩúÊµÅ„ÄÇËØ•Ê®°ÂùóÈÄöËøá‰∏ÄÁ≥ªÂàó‰º∞ËÆ°-ÈõÜÊàêËøáÁ®ãÂÆûÁé∞È´òÁ≤æÂ∫¶ÁöÑÂä®‰Ωú‰º∞ËÆ°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåActionSinkÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÊàêÂäüÁéáÊèêÂçá‰∫Ü7.9%ÔºåÂú®ÈïøÊó∂Èó¥ËßÜËßâ‰ªªÂä°LIBERO-Long‰∏≠ÂáÜÁ°ÆÁéáÊèêÂçáËøë8%„ÄÇËøô‰∫õÁªìÊûúÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØÔºåÂ±ïÁ§∫‰∫ÜËØ•Ê°ÜÊû∂Âú®Êú∫Âô®‰∫∫Êìç‰ΩúÁ≤æÂ∫¶ÊèêÂçáÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈ´òÁ≤æÂ∫¶Êìç‰ΩúÁöÑÈ¢ÜÂüüÔºåÂ¶ÇÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÊúçÂä°Êú∫Âô®‰∫∫ÂíåÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõÔºåActionSinkËÉΩÂ§üÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÔºåÂπ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Â∏¶Êù•ÊòæËëóÁöÑÊïàÁéáÊèêÂçá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Language-instructed robot manipulation has garnered significant interest due to the potential of learning from collected data. While the challenges in high-level perception and planning are continually addressed along the progress of general large pre-trained models, the low precision of low-level action estimation has emerged as the key limiting factor in manipulation performance. To this end, this paper introduces a novel robot manipulation framework, i.e., ActionSink, to pave the way toward precise action estimations in the field of learning-based robot manipulation. As the name suggests, ActionSink reformulates the actions of robots as action-caused optical flows from videos, called "action flow", in a self-supervised manner, which are then used to be retrieved and integrated to enhance the action estimation. Specifically, ActionSink incorporates two primary modules. The first module is a coarse-to-fine action flow matcher, which continuously refines the accuracy of action flow via iterative retrieval and denoising process. The second module is a dynamic action flow integrator, which employs a working memory pool that dynamically and efficiently manages the historical action flows that should be used to integrate to enhance the current action estimation. In this module, a multi-layer fusion module is proposed to integrate direct estimation and action flows from both the current and the working memory, achieving highly accurate action estimation through a series of estimation-integration processes. Our ActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\% success rate, and obtained nearly an 8\% accuracy gain on the challenging long-horizon visual task LIBERO-Long.

