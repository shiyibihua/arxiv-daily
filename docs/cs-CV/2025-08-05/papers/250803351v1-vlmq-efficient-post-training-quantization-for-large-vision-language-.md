---
layout: default
title: "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation"
---

# VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03351" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03351v1</a>
  <a href="https://arxiv.org/pdf/2508.03351.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03351v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03351v1', 'VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yufei Xue, Yushi Huang, Jiawei Shao, Jun Zhang

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

**Â§áÊ≥®**: 13 pages, 5 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VLMQ‰ª•Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂêéËÆ≠ÁªÉÈáèÂåñ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Hessian‰ºòÂåñ` `ÈáçË¶ÅÊÄßÊÑüÁü•` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê®°ÂûãÂéãÁº©` `Êé®ÁêÜÂä†ÈÄü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÂú®ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠Â∫îÁî®Êó∂ÔºåÂõ†Ê®°ÊÄÅÂ∑ÆÂºÇÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫VLMQÊ°ÜÊû∂ÔºåÈÄöËøá‰ºòÂåñÈáçË¶ÅÊÄßÊÑüÁü•ÁõÆÊ†áÂíåËΩªÈáèÁ∫ßËÆ°ÁÆóÊñπÊ≥ïÔºåËß£ÂÜ≥ËßÜËßâÊ†áËÆ∞ÂÜó‰ΩôÈóÆÈ¢ò„ÄÇ
3. Âú®8‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåVLMQÂú®‰ΩéÊØîÁâπÈáèÂåñ‰∏ãË°®Áé∞‰ºòÂºÇÔºåÁâπÂà´ÊòØÂú®MME-RealWorld‰∏äÊèêÂçá‰∫Ü16.45%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâÂ∑≤Êàê‰∏∫ÂéãÁº©Â§ßÂûãÊ®°ÂûãÂíåÂä†ÈÄüÊé®ÁêÜÁöÑÊúâÊïàÊñπÊ≥ïÔºå‰ΩÜÂÖ∂Âú®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏≠ÁöÑÂ∫îÁî®Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊú¨ÊñáËØÜÂà´‰∫ÜVLMs‰∏≠ÁöÑÊ®°ÊÄÅÂ∑ÆÂºÇÔºåÂç≥ÊñáÊú¨Ê†áËÆ∞ÊúâÈôê‰∏éËßÜËßâÊ†áËÆ∞ÂÜó‰Ωô„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éHessianÁöÑPTQÊñπÊ≥ïÂú®ÈáèÂåñËøáÁ®ã‰∏≠ÂØπÊâÄÊúâÊ†áËÆ∞‰∏ÄËßÜÂêå‰ªÅÔºåÂØºËá¥Âú®VLMs‰∏≠Â∫îÁî®Êó∂ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈíàÂØπVLMsÁöÑÈáçËßÜÈáçË¶ÅÊÄßÁöÑPTQÊ°ÜÊû∂VLMQÔºå‰ºòÂåñ‰∫ÜÂ∏¶ÊúâÊ†áËÆ∞Á∫ßÈáçË¶ÅÊÄßÂõ†Â≠êÁöÑHessianÔºåÂπ∂ÈÄöËøáËΩªÈáèÁ∫ßÁöÑÂùóÁ∫ßÂèçÂêë‰º†Êí≠ËÆ°ÁÆóËøô‰∫õÂõ†Â≠êÔºåÁ°Æ‰øù‰∫ÜÊïàÁéáÂíåÊúâÊïàÊÄß„ÄÇÂÆûÈ™åËØÅÊòéÔºåVLMQÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ΩéÊØîÁâπËÆæÁΩÆ‰∏ãÔºåÂèñÂæó‰∫Ü16.45%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâ‰∏≠ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢òÔºåÁé∞ÊúâHessianÂü∫ÊñπÊ≥ïÊú™ËÄÉËôëÊ®°ÊÄÅÂ∑ÆÂºÇÔºåÂØºËá¥ÈáèÂåñÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫VLMQÊ°ÜÊû∂ÔºåÈíàÂØπVLMsÁöÑÁâπÊÄßÔºå‰ºòÂåñÈáçË¶ÅÊÄßÊÑüÁü•ÁõÆÊ†áÔºåËÆ°ÁÆóÊ†áËÆ∞Á∫ßÈáçË¶ÅÊÄßÂõ†Â≠êÔºå‰ª•ÊèêÈ´òÈáèÂåñÊïàÊûúÂπ∂‰øùÊåÅÈ´òÊïàÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLMQÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÈáçË¶ÅÊÄßÊÑüÁü•ÁõÆÊ†á‰ºòÂåñÔºå2) ËΩªÈáèÁ∫ßÂùóÁ∫ßÂèçÂêë‰º†Êí≠ËÆ°ÁÆóÈáçË¶ÅÊÄßÂõ†Â≠êÔºåÁ°Æ‰øù‰∏éÂπ∂Ë°åÊùÉÈáçÊõ¥Êñ∞ÂÖºÂÆπ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLMQÈÄöËøáÂºïÂÖ•Ê†áËÆ∞Á∫ßÈáçË¶ÅÊÄßÂõ†Â≠êÔºåÊòæËëóÊîπÂñÑ‰∫ÜHessianÁöÑËÆ°ÁÆóÊñπÂºèÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜËßÜËßâÊ†áËÆ∞ÁöÑÂÜó‰ΩôÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞‰∏≠ÂºïÂÖ•‰∫ÜÈáçË¶ÅÊÄßÂõ†Â≠êÔºåÈááÁî®ËΩªÈáèÁ∫ßÁöÑÂùóÁ∫ßÂèçÂêë‰º†Êí≠Á≠ñÁï•ÔºåÁ°Æ‰øùËÆ°ÁÆóÊïàÁéáÔºåÂπ∂ÈÄöËøáÁêÜËÆ∫ËÅîÁ≥ªÊåáÂØºÊ†áËÆ∞Á∫ßÊâ∞Âä®ÁöÑËÆ°ÁÆó„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VLMQÂú®8‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁâπÂà´ÊòØÂú®‰ΩéÊØîÁâπÈáèÂåñËÆæÁΩÆ‰∏ãÔºåMME-RealWorld‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü16.45%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂêéËÆ≠ÁªÉÈáèÂåñ‰∏≠ÁöÑ‰ºòË∂äÊÄßÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑ‰∏ªÊµÅÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞ÁîüÊàêÂíåÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøáÊèêÈ´òËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºåVLMQËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊòæËëóÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÊé®Âä®Êô∫ËÉΩÂä©ÊâãÂíåËá™Âä®ÂåñÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑ‰∏çÊñ≠Êâ©Â§ßÔºåVLMQÁöÑÊäÄÊúØÊ°ÜÊû∂ÂèØËÉΩ‰ºöÂú®Êõ¥Â§öÈ¢ÜÂüüÂæóÂà∞Â∫îÁî®Ôºå‰øÉËøõÂ§öÊ®°ÊÄÅAIÁöÑËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Post-training quantization (PTQ) has emerged as an effective approach for compressing large models and accelerating their inference without retraining. While PTQ has been extensively studied in the context of large language models (LLMs), its applicability to vision-language models (VLMs) remains underexplored. In this paper, we identify a modality discrepancy (\emph{i.e.}, limited text tokens \emph{vs.} excessive and redundant vision tokens) of VLMs. However, existing Hessian-based LLM PTQ methods treat all tokens equally during quantization, resulting in severe performance drops when applied to VLMs. Motivated by this observation, we propose a novel importance-aware PTQ framework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token redundancy, VLMQ 1) optimizes an importance-aware objective that yields an enhanced Hessian with token-level importance factors, while retaining compatibility with parallelized weight updates, and 2) ensures efficiency and effectiveness by computing these factors via a single lightweight block-wise backward pass, guided by a theoretical connection to token-level perturbations. Extensive evaluations on 8 benchmarks across 0.5B$\sim$32B VLMs demonstrate the state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit settings. For example, it achieves a substantial \textbf{16.45\% } improvement on MME-RealWorld under 2-bit quantization.

