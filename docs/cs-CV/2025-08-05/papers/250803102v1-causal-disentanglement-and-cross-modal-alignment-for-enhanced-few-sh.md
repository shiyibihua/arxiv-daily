---
layout: default
title: Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning
---

# Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03102" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03102v1</a>
  <a href="https://arxiv.org/pdf/2508.03102.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03102v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03102v1', 'Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianjiao Jiang, Zhen Zhang, Yuhang Liu, Javen Qinfeng Shi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/tianjiao-j/CCA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCausal CLIP Adapterä»¥è§£å†³å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„è¡¨ç¤ºçº ç¼ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°‘æ ·æœ¬å­¦ä¹ ` `è§†è§‰ç‰¹å¾è§£ç¼ ` `å¤šæ¨¡æ€å¯¹é½` `ç‹¬ç«‹æˆåˆ†åˆ†æ` `äº¤å‰æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡å‹é€‚åº”æ€§` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä¾èµ–äºçº ç¼ è¡¨ç¤ºï¼Œå¯¼è‡´æ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸‹éš¾ä»¥æœ‰æ•ˆé€‚åº”ã€‚
2. æœ¬æ–‡æå‡ºCausal CLIP Adapterï¼Œé€šè¿‡æ— ç›‘ç£ICAæ˜¾å¼è§£ç¼ è§†è§‰ç‰¹å¾ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚
3. åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCCAåœ¨å°‘æ ·æœ¬æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é€šå¸¸éœ€è¦åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æœ‰æ•ˆé€‚åº”æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰FSLæ–¹æ³•ä¾èµ–äºçº ç¼ è¡¨ç¤ºï¼Œè¦æ±‚æ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸‹éšå¼æ¢å¤è§£æ··è¿‡ç¨‹ï¼Œä»è€Œè·å¾—è§£ç¼ è¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†æœ‰æ•ˆé€‚åº”ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Causal CLIP Adapterï¼ˆCCAï¼‰ï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æ— ç›‘ç£ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰æ˜¾å¼è§£ç¼ ä»CLIPæå–çš„è§†è§‰ç‰¹å¾ï¼Œå‡å°‘äº†å¯¹æ ‡æ³¨æ•°æ®å­¦ä¹ è§£æ··è¿‡ç¨‹çš„éœ€æ±‚ï¼Œé™ä½äº†å¯è®­ç»ƒå‚æ•°æ•°é‡å¹¶å‡è½»äº†è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼ŒCCAé€šè¿‡å•å‘å¾®è°ƒCLIPåŸºç¡€çš„æ–‡æœ¬åˆ†ç±»å™¨å’ŒåŒå‘çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†CLIPå›ºæœ‰çš„è·¨æ¨¡æ€å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šåœ¨å°‘æ ·æœ¬æ€§èƒ½å’Œå¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°‘æ ·æœ¬å­¦ä¹ ä¸­æ¨¡å‹å¯¹çº ç¼ è¡¨ç¤ºçš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦ä»æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸­éšå¼æ¢å¤è§£æ··è¿‡ç¨‹ï¼Œå¯¼è‡´é€‚åº”æ€§å·®å’Œè¿‡æ‹Ÿåˆé£é™©é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCausal CLIP Adapterï¼ˆCCAï¼‰ï¼Œé€šè¿‡æ— ç›‘ç£ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰æ˜¾å¼è§£ç¼ è§†è§‰ç‰¹å¾ï¼Œé¿å…äº†å¯¹æ ‡æ³¨æ•°æ®çš„è¿‡åº¦ä¾èµ–ï¼ŒåŒæ—¶å¢å¼ºäº†CLIPçš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCCAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡ICAå¯¹è§†è§‰ç‰¹å¾è¿›è¡Œè§£ç¼ ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å•å‘å¾®è°ƒæ–‡æœ¬åˆ†ç±»å™¨å’ŒåŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCCAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ˜¾å¼è§£ç¼ è§†è§‰ç‰¹å¾ï¼Œå‡å°‘äº†å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºäº†è§†è§‰ä¸æ–‡æœ¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„éšå¼è§£æ··è¿‡ç¨‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ— ç›‘ç£ICAè¿›è¡Œç‰¹å¾è§£ç¼ ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ç¡®ä¿äº†è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„æœ‰æ•ˆå¯¹é½ï¼ŒåŒæ—¶é€šè¿‡çº¿æ€§ç»„åˆçš„æ–¹å¼æå‡åˆ†ç±»å‡†ç¡®æ€§ã€‚æ•´ä½“æ¶æ„ä¿æŒäº†è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCCAåœ¨å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•æå‡äº†çº¦10%-15%ï¼Œå¹¶åœ¨é¢å¯¹åˆ†å¸ƒå˜åŒ–æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„åœºæ™¯ä¸­ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰é¢†åŸŸã€‚é€šè¿‡å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒCCAèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Few-shot learning (FSL) often requires effective adaptation of models using limited labeled data. However, most existing FSL methods rely on entangled representations, requiring the model to implicitly recover the unmixing process to obtain disentangled representations using only limited supervision, which hinders effective adaptation. Recent theoretical studies show that multimodal contrastive learning methods, such as CLIP, can disentangle latent representations up to linear transformations. In light of this, we propose the Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles visual features extracted from CLIP using unsupervised Independent Component Analysis (ICA). This removes the need to learn the unmixing process from the labeled data, thereby reducing the number of trainable parameters and mitigating overfitting. Taking a step further, while ICA can obtain visual disentangled representations, it may also disrupt CLIP's intra- and inter-modal alignment. To counteract this, CCA further leverages CLIP's inherent cross-modal alignment by enhancing it in two ways: unidirectionally, through fine-tuning a CLIP-based text classifier, and bidirectionally, via a cross-attention mechanism that enriches visual and textual representations through mutual interaction. Both unimodal and cross-modal classification outputs can be effectively combined linearly to improve classification accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches in terms of few-shot performance and robustness to distributional shifts, while maintaining computational efficiency. Code will be available at https://github.com/tianjiao-j/CCA.

