---
layout: default
title: FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models
---

# FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03356v2</a>
  <a href="https://arxiv.org/pdf/2508.03356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03356v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03356v2', 'FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matteo Caligiuri, Francesco Barbato, Donald Shenaj, Umberto Michieli, Pietro Zanuttigh

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-11-24)

**å¤‡æ³¨**: 8 pages (main document) + 13 pages (suppl. mat.), 4 figures (main) + 11 figures (suppl. mat.), 6 tables (main) + 5 tables (suppl. mat.) + 4 algorithms (suppl. mat.)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedPromoä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡èµ„æºä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `è½»é‡çº§æ¨¡å‹` `éšç§ä¿æŠ¤` `å¤šé¢†åŸŸå­¦ä¹ ` `å›¾åƒåˆ†ç±»` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶ï¼Œå®¢æˆ·ç«¯è®¾å¤‡çš„è®¡ç®—èµ„æºéœ€æ±‚æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚
2. FedPromoé€šè¿‡ä¼˜åŒ–è½»é‡çº§ä»£ç†æ¨¡å‹ï¼Œé‡‡ç”¨çŸ¥è¯†è’¸é¦å’Œå±€éƒ¨è®­ç»ƒåˆ†ç±»å™¨çš„æ–¹å¼ï¼Œè§£å†³äº†å¤§æ¨¡å‹åœ¨å®¢æˆ·ç«¯çš„é€‚åº”é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFedPromoåœ¨äº”ä¸ªå›¾åƒåˆ†ç±»åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨èµ„æºæœ‰é™çš„å®¢æˆ·ç«¯ç¯å¢ƒä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§åœ¨å»ä¸­å¿ƒåŒ–æ•°æ®ä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æˆç†ŸèŒƒå¼ã€‚ç„¶è€Œï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œä¼ ç»Ÿçš„FLæ–¹æ³•å¾€å¾€éœ€è¦å®¢æˆ·ç«¯è®¾å¤‡å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½ä¸å¯è¡Œã€‚æœ¬æ–‡æå‡ºäº†FedPromoï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†å­˜å‚¨åœ¨ä¸­å¤®æœåŠ¡å™¨ä¸Šçš„å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹é€‚åº”äºä»…ç”±è¿œç¨‹å®¢æˆ·ç«¯é‡åˆ°çš„æ–°é¢†åŸŸã€‚FedPromoé€šè¿‡ä¼˜åŒ–è½»é‡çº§ä»£ç†æ¨¡å‹æ¥å‡å°‘è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒéšç§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šé¦–å…ˆï¼Œåœ¨æœåŠ¡å™¨ç«¯è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå°†å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºä¸ç´§å‡‘æ¨¡å‹çš„è¡¨ç¤ºå¯¹é½ï¼›ç„¶åï¼Œå°†ç´§å‡‘æ¨¡å‹ç¼–ç å™¨éƒ¨ç½²åˆ°å®¢æˆ·ç«¯è®¾å¤‡ï¼Œå±€éƒ¨å­¦ä¹ å¯è®­ç»ƒçš„åˆ†ç±»å™¨ã€‚è¿™äº›åˆ†ç±»å™¨éšåè¢«èšåˆå¹¶æ— ç¼åœ°è½¬ç§»å›åŸºç¡€æ¨¡å‹ï¼Œå®ç°ä¸ªæ€§åŒ–é€‚åº”ï¼Œè€Œæ— éœ€ç›´æ¥è®¿é—®ç”¨æˆ·æ•°æ®ã€‚é€šè¿‡æ–°é¢–çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼ŒFedPromoå®ç°äº†å»ä¸­å¿ƒåŒ–çš„å¤šé¢†åŸŸå­¦ä¹ ï¼Œå¹³è¡¡äº†æ€§èƒ½ã€éšç§å’Œèµ„æºæ•ˆç‡ã€‚äº”ä¸ªå›¾åƒåˆ†ç±»åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFedPromoåœ¨å‡è®¾æœ‰é™èµ„æºå®¢æˆ·ç«¯çš„æƒ…å†µä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºæœ‰é™çš„å®¢æˆ·ç«¯è®¾å¤‡ä¸Šè®­ç»ƒå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„é—®é¢˜ã€‚ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFedPromoçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¼˜åŒ–è½»é‡çº§çš„ä»£ç†æ¨¡å‹æ¥é€‚åº”æ–°é¢†åŸŸï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨å®¢æˆ·ç«¯è®­ç»ƒå¤§æ¨¡å‹ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦å’Œå±€éƒ¨è®­ç»ƒåˆ†ç±»å™¨çš„æ–¹å¼ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†ç”¨æˆ·æ•°æ®çš„éšç§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFedPromoçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨æœåŠ¡å™¨ç«¯è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå°†å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºä¸ç´§å‡‘æ¨¡å‹çš„è¡¨ç¤ºå¯¹é½ï¼›å…¶æ¬¡ï¼Œå°†ç´§å‡‘æ¨¡å‹çš„ç¼–ç å™¨éƒ¨ç½²åˆ°å®¢æˆ·ç«¯ï¼Œå±€éƒ¨å­¦ä¹ å¯è®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œæœ€åå°†è¿™äº›åˆ†ç±»å™¨èšåˆå¹¶è½¬ç§»å›åŸºç¡€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šFedPromoçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é€šè¿‡è½»é‡çº§ä»£ç†æ¨¡å‹çš„ä¼˜åŒ–å’ŒçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå®ç°äº†å»ä¸­å¿ƒåŒ–çš„å¤šé¢†åŸŸå­¦ä¹ ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›´æ¥è®­ç»ƒå¤§æ¨¡å‹çš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒFedPromoé‡‡ç”¨äº†æ–°é¢–çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥å¹³è¡¡æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒé¢†åŸŸçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨äº”ä¸ªå›¾åƒåˆ†ç±»åŸºå‡†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedPromoåœ¨å‡è®¾æœ‰é™èµ„æºå®¢æˆ·ç«¯çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FedPromoçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†æ•æ„Ÿæ•°æ®çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯ä¸­ï¼Œå¦‚åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½å®¶å±…ç­‰ã€‚é€šè¿‡åœ¨ä¸ç›´æ¥è®¿é—®ç”¨æˆ·æ•°æ®çš„æƒ…å†µä¸‹å®ç°ä¸ªæ€§åŒ–æ¨¡å‹çš„é€‚åº”ï¼ŒFedPromoèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·ä½“éªŒï¼ŒåŒæ—¶ä¿æŠ¤éšç§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ‰©å±•åˆ°æ›´å¤šçš„åº”ç”¨é¢†åŸŸï¼Œæ¨åŠ¨è”é‚¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated Learning (FL) is an established paradigm for training deep learning models on decentralized data. However, as the size of the models grows, conventional FL approaches often require significant computational resources on client devices, which may not be feasible. We introduce FedPromo, a novel framework that enables efficient adaptation of large-scale foundation models stored on a central server to new domains encountered only by remote clients. Instead of directly training the large model on client devices, FedPromo optimizes lightweight proxy models via FL, significantly reducing computational overhead while maintaining privacy. Our method follows a two-stage process: first, server-side knowledge distillation aligns the representations of a large-scale foundation model (e.g., a transformer) with those of a compact counterpart (e.g., a CNN). Then, the compact model encoder is deployed to client devices, where trainable classifiers are learned locally. These classifiers are subsequently aggregated and seamlessly transferred back to the foundation model, facilitating personalized adaptation without requiring direct access to user data. Through novel regularization strategies, our framework enables decentralized multi-domain learning, balancing performance, privacy, and resource efficiency. Extensive experiments on five image classification benchmarks demonstrate that FedPromo outperforms existing methods while assuming limited-resource clients.

