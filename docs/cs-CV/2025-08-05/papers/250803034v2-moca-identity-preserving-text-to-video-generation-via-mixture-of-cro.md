---
layout: default
title: MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention
---

# MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03034" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03034v2</a>
  <a href="https://arxiv.org/pdf/2508.03034.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03034v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03034v2', 'MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Xie, Yongjia Ma, Donglin Di, Xuehao Gao, Xun Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-08-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoCAä»¥è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„èº«ä»½ä¿æŒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ` `èº«ä»½ä¿æŒ` `æ‰©æ•£æ¨¡å‹` `äº¤å‰æ³¨æ„åŠ›` `æ—¶ç©ºå»ºæ¨¡` `é¢éƒ¨åŠ¨æ€` `è§†é¢‘ç”Ÿæˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨èº«ä»½ä¿æŒå’Œç»†è‡´é¢éƒ¨åŠ¨æ€æ•æ‰æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘çš„è´¨é‡ä¸é«˜ã€‚
2. æœ¬æ–‡æå‡ºMoCAï¼Œé€šè¿‡åœ¨æ‰©æ•£å˜æ¢å™¨ä¸­å¼•å…¥æ··åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†è§†é¢‘ç”Ÿæˆä¸­çš„èº«ä»½ä¸€è‡´æ€§å’Œç»†èŠ‚è¡¨ç°ã€‚
3. åœ¨CelebIPVidæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMoCAåœ¨é¢éƒ¨ç›¸ä¼¼æ€§æŒ‡æ ‡ä¸Šæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†è¶…è¿‡5%çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡æœ€è¿‘åœ¨æ‰©æ•£æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œå®ç°èº«ä»½ä¿æŒçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰ç»†è‡´çš„é¢éƒ¨åŠ¨æ€æˆ–ä¿æŒæ—¶é—´ä¸Šçš„èº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†MoCAï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰éª¨å¹²ç½‘çš„æ–°å‹è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†å—ä¸“å®¶æ··åˆèŒƒå¼å¯å‘çš„æ··åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡åœ¨æ¯ä¸ªDiTå—ä¸­åµŒå…¥MoCAå±‚æ¥æ”¹å–„å¸§é—´èº«ä»½ä¸€è‡´æ€§ï¼Œå…¶ä¸­åˆ†å±‚æ—¶é—´æ± æ•æ‰ä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„èº«ä»½ç‰¹å¾ï¼Œæ—¶é—´æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›ä¸“å®¶åŠ¨æ€å»ºæ¨¡æ—¶ç©ºå…³ç³»ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ½œåœ¨è§†é¢‘æ„ŸçŸ¥æŸå¤±ï¼Œä»¥å¢å¼ºè§†é¢‘å¸§ä¹‹é—´çš„èº«ä»½ä¸€è‡´æ€§å’Œç»†èŠ‚ã€‚é€šè¿‡åœ¨CelebIPVidæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒMoCAåœ¨é¢éƒ¨ç›¸ä¼¼æ€§æ–¹é¢è¶…è¿‡ç°æœ‰T2Væ–¹æ³•5%ä»¥ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„èº«ä»½ä¿æŒé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•æ‰ç»†è‡´é¢éƒ¨åŠ¨æ€å’Œä¿æŒæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoCAé€šè¿‡å¼•å…¥æ··åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆåˆ†å±‚æ—¶é—´æ± å’Œæ—¶é—´æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›ä¸“å®¶ï¼ŒåŠ¨æ€å»ºæ¨¡æ—¶ç©ºå…³ç³»ï¼Œä»è€Œæ”¹å–„èº«ä»½ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoCAçš„æ•´ä½“æ¶æ„åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œåœ¨æ¯ä¸ªDiTå—ä¸­åµŒå…¥MoCAå±‚ï¼Œåˆ†å±‚æ—¶é—´æ± ç”¨äºæ•æ‰èº«ä»½ç‰¹å¾ï¼Œäº¤å‰æ³¨æ„åŠ›ä¸“å®¶ç”¨äºå»ºæ¨¡æ—¶ç©ºå…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoCAçš„ä¸»è¦åˆ›æ–°åœ¨äºæ··åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†èº«ä»½ä¸€è‡´æ€§é—®é¢˜ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€æ³¨æ„åŠ›æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä¸­é‡‡ç”¨äº†æ½œåœ¨è§†é¢‘æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œä»¥å¢å¼ºè§†é¢‘å¸§ä¹‹é—´çš„èº«ä»½ä¸€è‡´æ€§å’Œç»†èŠ‚è¡¨ç°ï¼ŒåŒæ—¶åœ¨æ•°æ®é›†CelebIPVidä¸Šè¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿æ¨¡å‹çš„è·¨ç§æ—æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CelebIPVidæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoCAåœ¨é¢éƒ¨ç›¸ä¼¼æ€§æ–¹é¢æ¯”ç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•æé«˜äº†è¶…è¿‡5%çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨èº«ä»½ä¿æŒå’Œç»†èŠ‚æ•æ‰æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å½±è§†åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„èº«ä»½ä¿æŒèƒ½åŠ›ï¼ŒMoCAå¯ä»¥å¸®åŠ©åˆ›ä½œè€…ç”Ÿæˆæ›´é«˜è´¨é‡çš„å†…å®¹ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œæœªæ¥å¯èƒ½åœ¨ä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆå’Œè‡ªåŠ¨åŒ–å†…å®¹åˆ›ä½œä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving ID-preserving text-to-video (T2V) generation remains challenging despite recent advances in diffusion-based models. Existing approaches often fail to capture fine-grained facial dynamics or maintain temporal identity coherence. To address these limitations, we propose MoCA, a novel Video Diffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating a Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts paradigm. Our framework improves inter-frame identity consistency by embedding MoCA layers into each DiT block, where Hierarchical Temporal Pooling captures identity features over varying timescales, and Temporal-Aware Cross-Attention Experts dynamically model spatiotemporal relationships. We further incorporate a Latent Video Perceptual Loss to enhance identity coherence and fine-grained details across video frames. To train this model, we collect CelebIPVid, a dataset of 10,000 high-resolution videos from 1,000 diverse individuals, promoting cross-ethnicity generalization. Extensive experiments on CelebIPVid show that MoCA outperforms existing T2V methods by over 5% across Face similarity.

