---
layout: default
title: Enhancing Long Video Question Answering with Scene-Localized Frame Grouping
---

# Enhancing Long Video Question Answering with Scene-Localized Frame Grouping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03009" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03009v1</a>
  <a href="https://arxiv.org/pdf/2508.03009.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03009v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03009v1', 'Enhancing Long Video Question Answering with Scene-Localized Frame Grouping')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xuyi Yang, Wenhao Zhang, Hongbo Jin, Lin Liu, Hongbo Xu, Yongwei Nie, Fei Yu, Fei Ma

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SLFGÊñπÊ≥ï‰ª•Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÈóÆÁ≠î‰∏≠ÁöÑ‰ø°ÊÅØÊèêÂèñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Âú∫ÊôØÊÑüÁü•` `ÈóÆÁ≠îÁ≥ªÁªü` `Âä®ÊÄÅÂ∏ßÈáçÁªÑ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÈïøËßÜÈ¢ëÁêÜËß£‰∏≠Èù¢‰∏¥ËµÑÊ∫êÈôêÂà∂ÔºåÊó†Ê≥ïÊúâÊïàÊèêÂèñÁõ∏ÂÖ≥‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩ‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫SceneQA‰ªªÂä°ÔºåÁªìÂêàSLFGÊñπÊ≥ïÔºåÈÄöËøáÂú∫ÊôØÊú¨‰ΩìÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõÊèêÂçáÈïøËßÜÈ¢ëÈóÆÁ≠îÊïàÊûú„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSLFGÂú®ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÈïøËßÜÈ¢ëÁêÜËß£ÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÁî±‰∫éËµÑÊ∫êÈôêÂà∂ÔºåÊó†Ê≥ïÂ§ÑÁêÜÊâÄÊúâËßÜÈ¢ëÂ∏ßÂèäÂÖ∂Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂú∫ÊôØÈóÆÁ≠î‰ªªÂä°SceneQAÔºåÂπ∂ÂºÄÂèë‰∫ÜLVSQAÊï∞ÊçÆÈõÜÔºå‰ª•ÊîØÊåÅËØ•‰ªªÂä°„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïSLFGÔºåÈÄöËøáÂ∞ÜÂçïÂ∏ßÂêàÂπ∂‰∏∫ËØ≠‰πâ‰∏ÄËá¥ÁöÑÂú∫ÊôØÂ∏ßÔºåÊòæËëóÊèêÂçá‰∫ÜÁé∞ÊúâMLLMsÂú®ÈïøËßÜÈ¢ë‰∏≠ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÊó†ÈúÄ‰øÆÊîπÂéüÊúâÊ®°ÂûãÊû∂ÊûÑÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÂç≥ÊèíÂç≥Áî®ÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°‰∏≠‰ø°ÊÅØÊèêÂèñÁöÑ‰ΩéÊïàÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜÂ§ßÈáèÊó†ÂÖ≥Â∏ßÔºåÂØºËá¥Ê®°ÂûãÁêÜËß£ËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫SLFGÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÂçïÁã¨Â∏ßÂêàÂπ∂‰∏∫ËØ≠‰πâ‰∏ÄËá¥ÁöÑÂú∫ÊôØÂ∏ßÔºåÊèêÂçáÊ®°ÂûãÁöÑÂú∫ÊôØÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇËøôÁßçËÆæËÆ°ÁÅµÊÑüÊù•Ê∫ê‰∫é‰∫∫Á±ªÁöÑËÆ§Áü•ÊñπÂºèÔºåÊó®Âú®Êõ¥Â•ΩÂú∞Ê®°Êãü‰∫∫Á±ªÂØπÂú∫ÊôØÁöÑÁêÜËß£„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSLFGÊñπÊ≥ïÂåÖÊã¨Âú∫ÊôØÂÆö‰ΩçÂíåÂä®ÊÄÅÂ∏ßÈáçÁªÑ‰∏§‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÂú∫ÊôØÂÆö‰ΩçÊäÄÊúØËØÜÂà´ËßÜÈ¢ë‰∏≠ÁöÑÈáçË¶ÅÂú∫ÊôØÔºåÁÑ∂ÂêéÂ∞ÜÁõ∏ÂÖ≥Â∏ßÂä®ÊÄÅÁªÑÂêàÊàêÂú∫ÊôØÂ∏ßÔºå‰ª•‰æø‰∫éÂêéÁª≠ÁöÑÈóÆÁ≠îÂ§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSLFGÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂Êó†ÈúÄ‰øÆÊîπÂéüÊúâÊ®°ÂûãÊû∂ÊûÑÔºå‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑÂç≥ÊèíÂç≥Áî®ÁâπÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏éÁé∞ÊúâÊ°ÜÊû∂ÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÂÖ∂Âº∫Ë∞ÉÂú∫ÊôØÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄßÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂçïÂ∏ßÁöÑËØÜÂà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®SLFG‰∏≠ÔºåÊàë‰ª¨ÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•‰ºòÂåñÂú∫ÊôØÂ∏ßÁöÑÁîüÊàêËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåÂä®ÊÄÅÂ∏ßÈáçÁªÑÊú∫Âà∂ÁöÑËÆæËÆ°Á°Æ‰øù‰∫ÜÂ∏ß‰πãÈó¥ÁöÑËØ≠‰πâËøûË¥ØÊÄßÔºå‰ªéËÄåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSLFGÊñπÊ≥ïÂú®Â§ö‰∏™ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%„ÄÇËØ•ÊñπÊ≥ïÁöÑÂç≥ÊèíÂç≥Áî®ÁâπÊÄß‰ΩøÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÈÄÇÁî®ÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÁõëÊéß„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÊïôËÇ≤ÂíåÂ®±‰πêÁ≠âÂ§ö‰∏™Âú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáÈïøËßÜÈ¢ëÁöÑÁêÜËß£ËÉΩÂäõÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊîØÊåÅËá™Âä®ÂåñÈóÆÁ≠î„ÄÅÂÜÖÂÆπÊ£ÄÁ¥¢ÂíåÁî®Êà∑‰∫§‰∫íÁ≠âÂÆûÈôÖÂ∫îÁî®ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÁ§æ‰ºöÂíåÁªèÊµé‰ª∑ÂÄº„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®Â§öÊ®°ÊÄÅÂ≠¶‰π†ÂíåËßÜÈ¢ëÁêÜËß£È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Current Multimodal Large Language Models (MLLMs) often perform poorly in long video understanding, primarily due to resource limitations that prevent them from processing all video frames and their associated information. Efficiently extracting relevant information becomes a challenging task. Existing frameworks and evaluation tasks focus on identifying specific frames containing core objects from a large number of irrelevant frames, which does not align with the practical needs of real-world applications. To address this issue, we propose a new scenario under the video question-answering task, SceneQA, which emphasizes scene-based detail perception and reasoning abilities. And we develop the LVSQA dataset to support the SceneQA task, which is built upon carefully selected videos from LVBench and contains a new collection of question-answer pairs to promote a more fair evaluation of MLLMs' scene perception abilities in long videos. Inspired by human cognition, we introduce a novel method called SLFG. The core idea of SLFG is to combine individual frames into semantically coherent scene frames. By leveraging scene localization methods and dynamic frame reassembly mechanisms, SLFG significantly enhances the understanding capabilities of existing MLLMs in long videos. SLFG requires no modification to the original model architecture and boasts excellent plug-and-play usability. Experimental results show that this method performs exceptionally well in several long video benchmark tests. Code and dataset will be released at http://www.slfg.pkuzwh.cn.

