---
layout: default
title: RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification
---

# RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03967" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03967v1</a>
  <a href="https://arxiv.org/pdf/2508.03967.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03967v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03967v1', 'RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid

**åˆ†ç±»**: cs.CV, cs.CR, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAVIDæ¡†æ¶ä»¥è§£å†³AIç”Ÿæˆå›¾åƒæ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `AIç”Ÿæˆå›¾åƒ` `å›¾åƒæ£€æµ‹` `è§†è§‰æ£€ç´¢` `å¤šæ¨¡æ€èåˆ` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé€šå¸¸ä¾èµ–ä½çº§ç‰¹å¾ï¼Œé™åˆ¶äº†é€‚åº”æ€§ã€‚
2. RAVIDæ¡†æ¶é€šè¿‡åŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒï¼Œç»“åˆå¾®è°ƒçš„CLIPç¼–ç å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºäº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
3. åœ¨UniversalFakeDetectåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAVIDå®ç°äº†93.85%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œä¸”åœ¨å›¾åƒé™è´¨æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°80.27%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†RAVIDï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨è§†è§‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æ¡†æ¶ã€‚å°½ç®¡RAGæ–¹æ³•åœ¨å‡è½»åŸºç¡€æ¨¡å‹çš„äº‹å®ä¸å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬ä¸Šï¼Œè§†è§‰çŸ¥è¯†çš„æ¢ç´¢ä»æ˜¾ä¸è¶³ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨æ³›åŒ–å’Œé²æ£’æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œé€šå¸¸ä¾èµ–ä½çº§ä¼ªå½±å’Œæ¨¡å‹ç‰¹å®šç‰¹å¾ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§ã€‚RAVIDé€šè¿‡åŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒæ¥å¢å¼ºæ£€æµ‹ï¼Œé‡‡ç”¨å¾®è°ƒçš„CLIPå›¾åƒç¼–ç å™¨RAVID CLIPï¼Œå¹¶ç»“åˆç±»åˆ«ç›¸å…³æç¤ºä»¥æ”¹å–„è¡¨ç¤ºå­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAVIDåœ¨UniversalFakeDetectåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†93.85%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œä¸”åœ¨å›¾åƒé™è´¨æ¡ä»¶ä¸‹ä»ä¿æŒé«˜é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–å’Œé²æ£’æ€§æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¾èµ–äºä½çº§ä¼ªå½±å’Œç‰¹å®šæ¨¡å‹ç‰¹å¾ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRAVIDé€šè¿‡åŠ¨æ€æ£€ç´¢ä¸æŸ¥è¯¢å›¾åƒç›¸å…³çš„å›¾åƒï¼Œç»“åˆå¾®è°ƒçš„CLIPå›¾åƒç¼–ç å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºè¾“å…¥ä¿¡æ¯ï¼Œä»è€Œæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRAVIDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨RAVID CLIPç”ŸæˆæŸ¥è¯¢å›¾åƒçš„åµŒå…¥ï¼›å…¶æ¬¡ï¼Œä»æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„å›¾åƒï¼›æœ€åï¼Œå°†æ£€ç´¢åˆ°çš„å›¾åƒä¸æŸ¥è¯¢å›¾åƒç»“åˆï¼Œå½¢æˆä¸°å¯Œçš„è¾“å…¥ä¾›è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šRAVIDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†è§†è§‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼ŒåŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒä»¥å¢å¼ºæ£€æµ‹èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å›ºå®šç‰¹å¾çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒRAVID CLIPç»è¿‡å¾®è°ƒä»¥é€‚åº”ç‰¹å®šç±»åˆ«ï¼Œå¹¶ç»“åˆç±»åˆ«ç›¸å…³æç¤ºä»¥æ”¹å–„è¡¨ç¤ºå­¦ä¹ ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šç§ç”Ÿæˆæ¨¡å‹ä¸‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RAVIDåœ¨UniversalFakeDetectåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°93.85%ï¼Œåœ¨å›¾åƒé™è´¨æ¡ä»¶ä¸‹çš„å‡†ç¡®ç‡ä¸º80.27%ï¼Œæ˜¾è‘—é«˜äºä¼ ç»Ÿæ–¹æ³•C2P-CLIPçš„63.44%ã€‚è¯¥æ¡†æ¶åœ¨é«˜æ–¯æ¨¡ç³Šå’ŒJPEGå‹ç¼©ç­‰é™è´¨åœºæ™¯ä¸­å‡å±•ç°å‡ºä¼˜è¶Šçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RAVIDæ¡†æ¶åœ¨AIç”Ÿæˆå›¾åƒæ£€æµ‹é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œåˆ†ç±»ç”Ÿæˆå›¾åƒï¼Œé€‚ç”¨äºç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€è™šå‡ä¿¡æ¯æ£€æµ‹ä»¥åŠæ•°å­—ç‰ˆæƒä¿æŠ¤ç­‰åœºæ™¯ã€‚éšç€ç”ŸæˆæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯¥ç ”ç©¶å°†ä¸ºç›¸å…³é¢†åŸŸæä¾›é‡è¦çš„æŠ€æœ¯æ”¯æŒå’Œç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we introduce RAVID, the first framework for AI-generated image detection that leverages visual retrieval-augmented generation (RAG). While RAG methods have shown promise in mitigating factual inaccuracies in foundation models, they have primarily focused on text, leaving visual knowledge underexplored. Meanwhile, existing detection methods, which struggle with generalization and robustness, often rely on low-level artifacts and model-specific features, limiting their adaptability. To address this, RAVID dynamically retrieves relevant images to enhance detection. Our approach utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with category-related prompts to improve representation learning. We further integrate a vision-language model (VLM) to fuse retrieved images with the query, enriching the input and improving accuracy. Given a query image, RAVID generates an embedding using RAVID CLIP, retrieves the most relevant images from a database, and combines these with the query image to form an enriched input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the UniversalFakeDetect benchmark, which covers 19 generative models, show that RAVID achieves state-of-the-art performance with an average accuracy of 93.85%. RAVID also outperforms traditional methods in terms of robustness, maintaining high accuracy even under image degradations such as Gaussian blur and JPEG compression. Specifically, RAVID achieves an average accuracy of 80.27% under degradation conditions, compared to 63.44% for the state-of-the-art model C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG compression scenarios. The code will be publicly available upon acceptance.

