---
layout: default
title: AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video
---

# AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03100v3</a>
  <a href="https://arxiv.org/pdf/2508.03100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03100v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03100v3', 'AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yogesh Kulkarni, Pooyan Fazli

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-11-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAVATARä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘æ¨ç†ä¸­çš„æ•°æ®æ•ˆç‡å’Œä¿¡ç”¨åˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è§†é¢‘æ¨ç†` `ç¦»çº¿è®­ç»ƒ` `æ—¶é—´ä¼˜åŠ¿å¡‘é€ ` `ä¿¡ç”¨åˆ†é…` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€è§†é¢‘æ¨ç†ä¸­å­˜åœ¨æ•°æ®æ•ˆç‡ä½ã€æ¶ˆå¤±ä¼˜åŠ¿å’Œä¿¡ç”¨åˆ†é…ä¸å‡ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. AVATARé€šè¿‡ç¦»çº¿è®­ç»ƒæ¶æ„æé«˜æ ·æœ¬æ•ˆç‡ï¼Œå¹¶å¼•å…¥æ—¶é—´ä¼˜åŠ¿å¡‘é€ ç­–ç•¥ï¼Œä¼˜åŒ–å…³é”®æ¨ç†é˜¶æ®µçš„ä¿¡ç”¨åˆ†é…ã€‚
3. AVATARåœ¨MMVUã€OmniBenchå’ŒVideo-Holmesç­‰åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†Qwen2.5-OmniåŸºçº¿ï¼Œæ ·æœ¬æ•ˆç‡æå‡è¾¾åˆ°5å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€è§†é¢‘çš„é•¿æ—¶é—´æ¨ç†é¢ä¸´ç€æ—¶ç©ºèåˆå’Œå¯¹é½çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨æ­¤é¢†åŸŸè¡¨ç°å‡ºä¸€å®šæ½œåŠ›ï¼Œä½†å­˜åœ¨æ•°æ®æ•ˆç‡ä½ã€æ¶ˆå¤±ä¼˜åŠ¿é—®é¢˜ä»¥åŠç»Ÿä¸€ä¿¡ç”¨åˆ†é…ç­‰ä¸‰å¤§å±€é™ã€‚æœ¬æ–‡æå‡ºäº†AVATARï¼ˆéŸ³é¢‘-è§†é¢‘å¯¹é½ä¸æ¨ç†ä»£ç†ï¼‰ï¼Œé€šè¿‡ç¦»çº¿è®­ç»ƒæ¶æ„å’Œæ—¶é—´ä¼˜åŠ¿å¡‘é€ ï¼ˆTASï¼‰ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæ¨ç†æ­¥éª¤çš„ä¿¡ç”¨åˆ†é…ã€‚AVATARåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶ŠQwen2.5-OmniåŸºçº¿ï¼Œæ ·æœ¬æ•ˆç‡æå‡è¾¾åˆ°5å€ï¼Œæ‰€éœ€ç”Ÿæˆçš„å®Œæˆæ•°é‡å‡å°‘80%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è§†é¢‘æ¨ç†ä¸­çš„æ•°æ®æ•ˆç‡ä½ã€æ¶ˆå¤±ä¼˜åŠ¿å’Œä¿¡ç”¨åˆ†é…ä¸å‡ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚GRPOåœ¨è¿™äº›æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å­¦ä¹ ä¿¡å·ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAVATARçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¦»çº¿è®­ç»ƒæ¶æ„é‡ç”¨è¿‡å»çš„ç»éªŒæ¥æé«˜æ ·æœ¬æ•ˆç‡ï¼Œå¹¶é€šè¿‡æ—¶é—´ä¼˜åŠ¿å¡‘é€ ç­–ç•¥æ¥å¼ºè°ƒå…³é”®æ¨ç†é˜¶æ®µï¼Œä»è€Œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAVATARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¦»çº¿è®­ç»ƒæ¶æ„å’Œæ—¶é—´ä¼˜åŠ¿å¡‘é€ ã€‚ç¦»çº¿è®­ç»ƒæ¶æ„é€šè¿‡é‡ç”¨å†å²ç»éªŒæ¥æé«˜æ ·æœ¬æ•ˆç‡ï¼Œè€Œæ—¶é—´ä¼˜åŠ¿å¡‘é€ åˆ™åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹å…³é”®æ¨ç†é˜¶æ®µè¿›è¡ŒåŠ æƒã€‚

**å…³é”®åˆ›æ–°**ï¼šAVATARçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç¦»çº¿è®­ç»ƒæ¶æ„å’Œæ—¶é—´ä¼˜åŠ¿å¡‘é€ ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„åœ¨çº¿è®­ç»ƒå’Œç»Ÿä¸€ä¿¡ç”¨åˆ†é…å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡å’Œæ¨ç†è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒAVATARé‡‡ç”¨äº†å¤šæ ·åŒ–çš„å¥–åŠ±æœºåˆ¶ä»¥è§£å†³æ¶ˆå¤±ä¼˜åŠ¿é—®é¢˜ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¯¹å…³é”®æ¨ç†é˜¶æ®µçš„åŠ æƒï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å…³æ³¨é‡è¦çš„æ¨ç†æ­¥éª¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AVATARåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºQwen2.5-OmniåŸºçº¿ï¼ŒMMVUæå‡5.4åˆ†ï¼ŒOmniBenchæå‡4.9åˆ†ï¼ŒVideo-Holmesæå‡4.5åˆ†ã€‚åŒæ—¶ï¼ŒAVATARå±•ç¤ºäº†5å€çš„æ ·æœ¬æ•ˆç‡ï¼Œè¾¾åˆ°ç›®æ ‡æ€§èƒ½æ‰€éœ€çš„ç”Ÿæˆå®Œæˆæ•°é‡å‡å°‘80%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AVATARçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è§†é¢‘æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒAVATARèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒAVATARçš„æ¡†æ¶ä¹Ÿå¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„å¤šæ¨¡æ€æ•°æ®åˆ†æä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal reasoning over long-horizon video is challenging due to the need for precise spatiotemporal fusion and alignment across modalities. While recent methods such as Group Relative Policy Optimization (GRPO) have shown promise in this domain, they suffer from three key limitations: (1) data inefficiency from their on-policy design, (2) a vanishing advantage problem, where identical or near-identical rewards within a group eliminate the learning signal by producing zero-valued advantages, and (3) uniform credit assignment that fails to emphasize critical reasoning steps. We introduce $\textbf{AVATAR}$ ($\textbf{A}$udio-$\textbf{V}$ideo $\textbf{A}$gen$\textbf{t}$ for $\textbf{A}$lignment and $\textbf{R}$easoning), a framework that addresses these limitations through two core components: (1) an off-policy training architecture that improves sample efficiency and resolves vanishing advantages by reusing past experiences with greater reward diversity, and (2) Temporal Advantage Shaping (TAS), a novel credit assignment strategy that upweights key reasoning phases during learning. $\textbf{AVATAR}$ achieves strong performance across various benchmarks, outperforming the Qwen2.5-Omni baseline by $\mathbf{+5.4}$ on MMVU, $\mathbf{+4.9}$ on OmniBench, and $\mathbf{+4.5}$ on Video-Holmes, while demonstrating $\textbf{$5$$\times$ sample efficiency}$, requiring $80\%$ fewer generated completions to reach target performance.

