---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-05
---

# cs.CVï¼ˆ2025-08-05ï¼‰

ğŸ“Š å…± **47** ç¯‡è®ºæ–‡
 | ğŸ”— **12** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (16 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250803694v1-longvie-multimodal-guided-controllable-ultra-long-video-generation.html">LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation</a></td>
  <td>æå‡ºLongVieä»¥è§£å†³è¶…é•¿è§†é¢‘ç”Ÿæˆä¸­çš„å¯æ§æ€§ä¸ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03694v1" data-paper-url="./papers/250803694v1-longvie-multimodal-guided-controllable-ultra-long-video-generation.html" onclick="toggleFavorite(this, '2508.03694v1', 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250803566v1-sam2-unext-an-improved-high-resolution-baseline-for-adapting-foundat.html">SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks</a></td>
  <td>æå‡ºSAM2-UNeXTä»¥æå‡åŸºç¡€æ¨¡å‹åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03566v1" data-paper-url="./papers/250803566v1-sam2-unext-an-improved-high-resolution-baseline-for-adapting-foundat.html" onclick="toggleFavorite(this, '2508.03566v1', 'SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250803539v1-quality-aware-language-conditioned-local-auto-regressive-anomaly-syn.html">Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection</a></td>
  <td>æå‡ºARASæ–¹æ³•ä»¥è§£å†³ç°æœ‰å¼‚å¸¸åˆæˆçš„ç»“æ„ç¼ºé™·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">language conditioned</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03539v1" data-paper-url="./papers/250803539v1-quality-aware-language-conditioned-local-auto-regressive-anomaly-syn.html" onclick="toggleFavorite(this, '2508.03539v1', 'Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250803524v1-semantic-mosaicing-of-histo-pathology-image-fragments-using-visual-f.html">Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models</a></td>
  <td>æå‡ºSemanticStitcherä»¥è§£å†³ç»„ç»‡ç—…ç†å›¾åƒæ‹¼æ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03524v1" data-paper-url="./papers/250803524v1-semantic-mosaicing-of-histo-pathology-image-fragments-using-visual-f.html" onclick="toggleFavorite(this, '2508.03524v1', 'Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250803441v2-medcal-bench-a-comprehensive-benchmark-on-cold-start-active-learning.html">MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis</a></td>
  <td>æå‡ºMedCAL-Benchä»¥è§£å†³åŒ»ç–—å›¾åƒåˆ†æä¸­çš„å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03441v2" data-paper-url="./papers/250803441v2-medcal-bench-a-comprehensive-benchmark-on-cold-start-active-learning.html" onclick="toggleFavorite(this, '2508.03441v2', 'MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250803235v1-zero-shot-shape-classification-of-nanoparticles-in-sem-images-using-.html">Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models</a></td>
  <td>æå‡ºé›¶-shotåˆ†ç±»æ–¹æ³•ä»¥è§£å†³çº³ç±³é¢—ç²’å½¢æ€è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03235v1" data-paper-url="./papers/250803235v1-zero-shot-shape-classification-of-nanoparticles-in-sem-images-using-.html" onclick="toggleFavorite(this, '2508.03235v1', 'Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250803562v1-beyond-meme-templates-limitations-of-visual-similarity-measures-in-m.html">Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching</a></td>
  <td>æå‡ºè¶…è¶Šæ¨¡æ¿åŒ¹é…çš„è§†è§‰ç›¸ä¼¼æ€§åº¦é‡ä»¥è§£å†³è¡¨æƒ…åŒ…åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03562v1" data-paper-url="./papers/250803562v1-beyond-meme-templates-limitations-of-visual-similarity-measures-in-m.html" onclick="toggleFavorite(this, '2508.03562v1', 'Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250803535v1-coemogen-towards-semantically-coherent-and-scalable-emotional-image-.html">CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation</a></td>
  <td>æå‡ºCoEmoGenä»¥è§£å†³æƒ…æ„Ÿå›¾åƒç”Ÿæˆä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03535v1" data-paper-url="./papers/250803535v1-coemogen-towards-semantically-coherent-and-scalable-emotional-image-.html" onclick="toggleFavorite(this, '2508.03535v1', 'CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250803426v1-r2genkg-hierarchical-multi-modal-knowledge-graph-for-llm-based-radio.html">R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation</a></td>
  <td>æå‡ºR2GenKGä»¥è§£å†³Xå…‰æŠ¥å‘Šç”Ÿæˆä¸­çš„å¹»è§‰ä¸è¯Šæ–­èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03426v1" data-paper-url="./papers/250803426v1-r2genkg-hierarchical-multi-modal-knowledge-graph-for-llm-based-radio.html" onclick="toggleFavorite(this, '2508.03426v1', 'R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250803337v7-less-is-more-token-efficient-video-qa-via-adaptive-frame-pruning-and.html">Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</a></td>
  <td>æå‡ºè‡ªé€‚åº”å¸§å‰ªæä¸è¯­ä¹‰å›¾é›†æˆä»¥è§£å†³è§†é¢‘é—®ç­”ä¸­çš„å†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03337v7" data-paper-url="./papers/250803337v7-less-is-more-token-efficient-video-qa-via-adaptive-frame-pruning-and.html" onclick="toggleFavorite(this, '2508.03337v7', 'Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250803094v1-augmenting-continual-learning-of-diseases-with-llm-generated-visual-.html">Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts</a></td>
  <td>æå‡ºåˆ©ç”¨LLMç”Ÿæˆè§†è§‰æ¦‚å¿µä»¥å¢å¼ºç–¾ç—…æŒç»­å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03094v1" data-paper-url="./papers/250803094v1-augmenting-continual-learning-of-diseases-with-llm-generated-visual-.html" onclick="toggleFavorite(this, '2508.03094v1', 'Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250803009v1-enhancing-long-video-question-answering-with-scene-localized-frame-g.html">Enhancing Long Video Question Answering with Scene-Localized Frame Grouping</a></td>
  <td>æå‡ºSLFGæ–¹æ³•ä»¥è§£å†³é•¿è§†é¢‘é—®ç­”ä¸­çš„ä¿¡æ¯æå–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03009v1" data-paper-url="./papers/250803009v1-enhancing-long-video-question-answering-with-scene-localized-frame-g.html" onclick="toggleFavorite(this, '2508.03009v1', 'Enhancing Long Video Question Answering with Scene-Localized Frame Grouping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250803490v1-particlesam-small-particle-segmentation-for-material-quality-monitor.html">ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes</a></td>
  <td>æå‡ºParticleSAMä»¥è§£å†³å»ºç­‘ææ–™å›æ”¶ä¸­å°é¢—ç²’åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03490v1" data-paper-url="./papers/250803490v1-particlesam-small-particle-segmentation-for-material-quality-monitor.html" onclick="toggleFavorite(this, '2508.03490v1', 'ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250803351v1-vlmq-efficient-post-training-quantization-for-large-vision-language-.html">VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation</a></td>
  <td>æå‡ºVLMQä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒé‡åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03351v1" data-paper-url="./papers/250803351v1-vlmq-efficient-post-training-quantization-for-large-vision-language-.html" onclick="toggleFavorite(this, '2508.03351v1', 'VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250803079v2-bias-beyond-demographics-probing-decision-boundaries-in-black-box-lv.html">Bias Beyond Demographics: Probing Decision Boundaries in Black-Box LVLMs via Counterfactual VQA</a></td>
  <td>æå‡ºåäº‹å®è§†è§‰é—®ç­”åŸºå‡†ä»¥å®¡è®¡é»‘ç®±LVLMçš„å†³ç­–åå·®</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03079v2" data-paper-url="./papers/250803079v2-bias-beyond-demographics-probing-decision-boundaries-in-black-box-lv.html" onclick="toggleFavorite(this, '2508.03079v2', 'Bias Beyond Demographics: Probing Decision Boundaries in Black-Box LVLMs via Counterfactual VQA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250803007v1-multi-granularity-feature-calibration-via-vfm-for-domain-generalized.html">Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation</a></td>
  <td>æå‡ºå¤šç²’åº¦ç‰¹å¾æ ¡å‡†æ–¹æ³•ä»¥è§£å†³é¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03007v1" data-paper-url="./papers/250803007v1-multi-granularity-feature-calibration-via-vfm-for-domain-generalized.html" onclick="toggleFavorite(this, '2508.03007v1', 'Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250803017v2-sa-3dgs-a-self-adaptive-compression-method-for-3d-gaussian-splatting.html">SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting</a></td>
  <td>æå‡ºSA-3DGSä»¥è§£å†³3Dé«˜æ–¯ç‚¹å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03017v2" data-paper-url="./papers/250803017v2-sa-3dgs-a-self-adaptive-compression-method-for-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.03017v2', 'SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250803207v1-open-vocabulary-hoi-detection-with-interaction-aware-prompt-and-conc.html">Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration</a></td>
  <td>æå‡ºINP-CCä»¥è§£å†³å¼€æ”¾è¯æ±‡HOIæ£€æµ‹ä¸­çš„äº¤äº’è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">human-object interaction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03207v1" data-paper-url="./papers/250803207v1-open-vocabulary-hoi-detection-with-interaction-aware-prompt-and-conc.html" onclick="toggleFavorite(this, '2508.03207v1', 'Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250803077v1-robustgs-unified-boosting-of-feedforward-3d-gaussian-splatting-under.html">RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions</a></td>
  <td>æå‡ºRobustGSä»¥è§£å†³ä½è´¨é‡æ¡ä»¶ä¸‹3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03077v1" data-paper-url="./papers/250803077v1-robustgs-unified-boosting-of-feedforward-3d-gaussian-splatting-under.html" onclick="toggleFavorite(this, '2508.03077v1', 'RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250803643v3-uni3r-unified-3d-reconstruction-and-semantic-understanding-via-gener.html">Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images</a></td>
  <td>æå‡ºUni3Rä»¥è§£å†³æ— å§¿æ€å¤šè§†å›¾å›¾åƒçš„3Dé‡å»ºä¸è¯­ä¹‰ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03643v3" data-paper-url="./papers/250803643v3-uni3r-unified-3d-reconstruction-and-semantic-understanding-via-gener.html" onclick="toggleFavorite(this, '2508.03643v3', 'Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250803180v2-duplex-gs-proxy-guided-weighted-blending-for-real-time-order-indepen.html">Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting</a></td>
  <td>æå‡ºDuplex-GSä»¥è§£å†³å®æ—¶é«˜æ•ˆçš„é«˜æ–¯æ¸²æŸ“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03180v2" data-paper-url="./papers/250803180v2-duplex-gs-proxy-guided-weighted-blending-for-real-time-order-indepen.html" onclick="toggleFavorite(this, '2508.03180v2', 'Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250803186v1-monocular-depth-estimation-with-global-aware-discretization-and-loca.html">Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling</a></td>
  <td>æå‡ºGated Large Kernel Attention Moduleä»¥è§£å†³å•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03186v1" data-paper-url="./papers/250803186v1-monocular-depth-estimation-with-global-aware-discretization-and-loca.html" onclick="toggleFavorite(this, '2508.03186v1', 'Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250803118v1-h3r-hybrid-multi-view-correspondence-for-generalizable-3d-reconstruc.html">H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction</a></td>
  <td>æå‡ºH3Ræ¡†æ¶ä»¥è§£å†³å¤šè§†è§’å¯¹åº”å»ºæ¨¡çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03118v1" data-paper-url="./papers/250803118v1-h3r-hybrid-multi-view-correspondence-for-generalizable-3d-reconstruc.html" onclick="toggleFavorite(this, '2508.03118v1', 'H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250803227v1-trace3d-consistent-segmentation-lifting-via-gaussian-instance-tracin.html">Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing</a></td>
  <td>æå‡ºGaussianå®ä¾‹è¿½è¸ªä»¥è§£å†³2Dåˆ°3Dåˆ†å‰²ä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03227v1" data-paper-url="./papers/250803227v1-trace3d-consistent-segmentation-lifting-via-gaussian-instance-tracin.html" onclick="toggleFavorite(this, '2508.03227v1', 'Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250803449v1-video-demoireing-using-focused-defocused-dual-camera-system.html">Video Demoireing using Focused-Defocused Dual-Camera System</a></td>
  <td>æå‡ºåŒæ‘„åƒå¤´ç³»ç»Ÿä»¥è§£å†³è§†é¢‘ä¸­çš„æ‘©å°”çº¹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03449v1" data-paper-url="./papers/250803449v1-video-demoireing-using-focused-defocused-dual-camera-system.html" onclick="toggleFavorite(this, '2508.03449v1', 'Video Demoireing using Focused-Defocused Dual-Camera System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250803060v2-charm-collaborative-harmonization-across-arbitrary-modalities-for-mo.html">CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation</a></td>
  <td>æå‡ºCHARMä»¥è§£å†³å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ä¸­çš„åŒè´¨åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03060v2" data-paper-url="./papers/250803060v2-charm-collaborative-harmonization-across-arbitrary-modalities-for-mo.html" onclick="toggleFavorite(this, '2508.03060v2', 'CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/250803356v2-fedpromo-federated-lightweight-proxy-models-at-the-edge-bring-new-do.html">FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models</a></td>
  <td>æå‡ºFedPromoä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡èµ„æºä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03356v2" data-paper-url="./papers/250803356v2-fedpromo-federated-lightweight-proxy-models-at-the-edge-bring-new-do.html" onclick="toggleFavorite(this, '2508.03356v2', 'FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250803201v3-aligncat-visual-linguistic-alignment-of-category-and-attribute-for-w.html">AlignCAT: Visual-Linguistic Alignment of Category and Attribute for Weakly Supervised Visual Grounding</a></td>
  <td>æå‡ºAlignCATä»¥è§£å†³å¼±ç›‘ç£è§†è§‰å®šä½ä¸­çš„è¯­ä¹‰å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">visual grounding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03201v3" data-paper-url="./papers/250803201v3-aligncat-visual-linguistic-alignment-of-category-and-attribute-for-w.html" onclick="toggleFavorite(this, '2508.03201v3', 'AlignCAT: Visual-Linguistic Alignment of Category and Attribute for Weakly Supervised Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250803073v1-nexus-inr-diverse-knowledge-guided-arbitrary-scale-multimodal-medica.html">Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution</a></td>
  <td>æå‡ºNexus-INRä»¥è§£å†³å¤šæ¨¡æ€åŒ»å­¦å›¾åƒçš„ä»»æ„å°ºåº¦è¶…åˆ†è¾¨ç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03073v1" data-paper-url="./papers/250803073v1-nexus-inr-diverse-knowledge-guided-arbitrary-scale-multimodal-medica.html" onclick="toggleFavorite(this, '2508.03073v1', 'Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250803100v3-avatar-reinforcement-learning-to-see-hear-and-reason-over-video.html">AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video</a></td>
  <td>æå‡ºAVATARä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘æ¨ç†ä¸­çš„æ•°æ®æ•ˆç‡å’Œä¿¡ç”¨åˆ†é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03100v3" data-paper-url="./papers/250803100v3-avatar-reinforcement-learning-to-see-hear-and-reason-over-video.html" onclick="toggleFavorite(this, '2508.03100v3', 'AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250803313v1-baroposer-real-time-human-motion-tracking-from-imus-and-barometers-i.html">BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices</a></td>
  <td>æå‡ºBaroPoserä»¥è§£å†³ä¸å¹³å¦åœ°å½¢ä¸‹äººç±»åŠ¨ä½œè¿½è¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">motion tracking</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03313v1" data-paper-url="./papers/250803313v1-baroposer-real-time-human-motion-tracking-from-imus-and-barometers-i.html" onclick="toggleFavorite(this, '2508.03313v1', 'BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250803254v1-vip-iterative-online-preference-distillation-for-efficient-video-dif.html">V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models</a></td>
  <td>æå‡ºV.I.P.æ¡†æ¶ä»¥è§£å†³è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">VIP</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03254v1" data-paper-url="./papers/250803254v1-vip-iterative-online-preference-distillation-for-efficient-video-dif.html" onclick="toggleFavorite(this, '2508.03254v1', 'V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250803692v3-lidarcrafter-dynamic-4d-world-modeling-from-lidar-sequences.html">LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</a></td>
  <td>æå‡ºLiDARCrafterä»¥è§£å†³åŠ¨æ€4Dä¸–ç•Œå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">occupancy grid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03692v3" data-paper-url="./papers/250803692v3-lidarcrafter-dynamic-4d-world-modeling-from-lidar-sequences.html" onclick="toggleFavorite(this, '2508.03692v3', 'LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250803967v1-ravid-retrieval-augmented-visual-detection-a-knowledge-driven-approa.html">RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification</a></td>
  <td>æå‡ºRAVIDæ¡†æ¶ä»¥è§£å†³AIç”Ÿæˆå›¾åƒæ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03967v1" data-paper-url="./papers/250803967v1-ravid-retrieval-augmented-visual-detection-a-knowledge-driven-approa.html" onclick="toggleFavorite(this, '2508.03967v1', 'RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250803317v1-architectural-insights-into-knowledge-distillation-for-object-detect.html">Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review</a></td>
  <td>æå‡ºåŸºäºæ¶æ„çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä»¥è§£å†³ç›®æ ‡æ£€æµ‹ä¸­çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03317v1" data-paper-url="./papers/250803317v1-architectural-insights-into-knowledge-distillation-for-object-detect.html" onclick="toggleFavorite(this, '2508.03317v1', 'Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250803102v1-causal-disentanglement-and-cross-modal-alignment-for-enhanced-few-sh.html">Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning</a></td>
  <td>æå‡ºCausal CLIP Adapterä»¥è§£å†³å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„è¡¨ç¤ºçº ç¼ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03102v1" data-paper-url="./papers/250803102v1-causal-disentanglement-and-cross-modal-alignment-for-enhanced-few-sh.html" onclick="toggleFavorite(this, '2508.03102v1', 'Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>37</td>
  <td><a href="./papers/250803266v2-egoprompt-prompt-learning-for-egocentric-action-recognition.html">EgoPrompt: Prompt Learning for Egocentric Action Recognition</a></td>
  <td>æå‡ºEgoPromptä»¥è§£å†³ç¬¬ä¸€äººç§°åŠ¨ä½œè¯†åˆ«ä¸­çš„è¯­ä¹‰å…³ç³»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">Ego4D</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03266v2" data-paper-url="./papers/250803266v2-egoprompt-prompt-learning-for-egocentric-action-recognition.html" onclick="toggleFavorite(this, '2508.03266v2', 'EgoPrompt: Prompt Learning for Egocentric Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250803343v1-wamo-wavelet-enhanced-multi-frequency-trajectory-analysis-for-fine-g.html">WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval</a></td>
  <td>æå‡ºWaMoæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬ä¸3DåŠ¨ä½œåºåˆ—åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion retrieval</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03343v1" data-paper-url="./papers/250803343v1-wamo-wavelet-enhanced-multi-frequency-trajectory-analysis-for-fine-g.html" onclick="toggleFavorite(this, '2508.03343v1', 'WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250803132v1-coffee-a-shadow-resilient-real-time-pose-estimator-for-unknown-tumbl.html">COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks</a></td>
  <td>æå‡ºCOFFEEä»¥è§£å†³æœªçŸ¥ç¿»æ»šå°è¡Œæ˜Ÿçš„å®æ—¶å§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03132v1" data-paper-url="./papers/250803132v1-coffee-a-shadow-resilient-real-time-pose-estimator-for-unknown-tumbl.html" onclick="toggleFavorite(this, '2508.03132v1', 'COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/250803144v2-lore-latent-optimization-for-precise-semantic-control-in-rectified-f.html">LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing</a></td>
  <td>æå‡ºLOREä»¥è§£å†³å›¾åƒç¼–è¾‘ä¸­çš„è¯­ä¹‰æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">latent optimization</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03144v2" data-paper-url="./papers/250803144v2-lore-latent-optimization-for-precise-semantic-control-in-rectified-f.html" onclick="toggleFavorite(this, '2508.03144v2', 'LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250806543v2-mild-multi-layer-diffusion-strategy-for-complex-and-precise-multi-ip.html">MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing</a></td>
  <td>æå‡ºå¤šå±‚æ‰©æ•£ç­–ç•¥ä»¥è§£å†³å¤æ‚åœºæ™¯ä¸‹çš„äººç‰©æŠ¹é™¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.06543v2" data-paper-url="./papers/250806543v2-mild-multi-layer-diffusion-strategy-for-complex-and-precise-multi-ip.html" onclick="toggleFavorite(this, '2508.06543v2', 'MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/250803218v1-actionsink-toward-precise-robot-manipulation-with-dynamic-integratio.html">ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow</a></td>
  <td>æå‡ºActionSinkä»¥è§£å†³æœºå™¨äººæ“ä½œç²¾åº¦ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03218v1" data-paper-url="./papers/250803218v1-actionsink-toward-precise-robot-manipulation-with-dynamic-integratio.html" onclick="toggleFavorite(this, '2508.03218v1', 'ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250803397v1-depthgait-multi-scale-cross-level-feature-fusion-of-rgb-derived-dept.html">DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition</a></td>
  <td>æå‡ºDepthGaitä»¥è§£å†³æ­¥æ€è¯†åˆ«ä¸­çš„æ¨¡æ€èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03397v1" data-paper-url="./papers/250803397v1-depthgait-multi-scale-cross-level-feature-fusion-of-rgb-derived-dept.html" onclick="toggleFavorite(this, '2508.03397v1', 'DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250803442v2-raag-ratio-aware-adaptive-guidance.html">RAAG: Ratio Aware Adaptive Guidance</a></td>
  <td>æå‡ºè‡ªé€‚åº”å¼•å¯¼æ–¹æ³•ä»¥è§£å†³æµå¼ç”Ÿæˆæ¨¡å‹é‡‡æ ·ä¸ç¨³å®šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03442v2" data-paper-url="./papers/250803442v2-raag-ratio-aware-adaptive-guidance.html" onclick="toggleFavorite(this, '2508.03442v2', 'RAAG: Ratio Aware Adaptive Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250802973v1-diffusion-models-with-adaptive-negative-sampling-without-external-re.html">Diffusion Models with Adaptive Negative Sampling Without External Resources</a></td>
  <td>æå‡ºè‡ªé€‚åº”è´Ÿé‡‡æ ·æ–¹æ³•ä»¥æå‡æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.02973v1" data-paper-url="./papers/250802973v1-diffusion-models-with-adaptive-negative-sampling-without-external-re.html" onclick="toggleFavorite(this, '2508.02973v1', 'Diffusion Models with Adaptive Negative Sampling Without External Resources')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/250803960v1-fast-magnetic-resonance-simulation-using-combined-update-with-groupe.html">Fast Magnetic Resonance Simulation Using Combined Update with Grouped Isochromats</a></td>
  <td>æå‡ºåŸºäºåˆ†ç»„ç­‰ç£ä½“çš„å¿«é€Ÿç£å…±æŒ¯æ¨¡æ‹Ÿæ–¹æ³•ä»¥è§£å†³è®¡ç®—æ—¶é—´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">PULSE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03960v1" data-paper-url="./papers/250803960v1-fast-magnetic-resonance-simulation-using-combined-update-with-groupe.html" onclick="toggleFavorite(this, '2508.03960v1', 'Fast Magnetic Resonance Simulation Using Combined Update with Grouped Isochromats')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/250803034v2-moca-identity-preserving-text-to-video-generation-via-mixture-of-cro.html">MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention</a></td>
  <td>æå‡ºMoCAä»¥è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„èº«ä»½ä¿æŒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.03034v2" data-paper-url="./papers/250803034v2-moca-identity-preserving-text-to-video-generation-via-mixture-of-cro.html" onclick="toggleFavorite(this, '2508.03034v2', 'MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)