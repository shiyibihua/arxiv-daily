---
layout: default
title: DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition
---

# DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition

**arXiv**: [2511.10948v1](https://arxiv.org/abs/2511.10948) | [PDF](https://arxiv.org/pdf/2511.10948.pdf)

**ä½œè€…**: Ren Zhang, Huilai Li, Chao qi, Guoliang Xu, Tianyu Zhou, Wei wei, Jianqin Yin

**åˆ†ç±»**: cs.CV, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-11-14

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDEFT-LLMä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­çš„è¿åŠ¨è¯­ä¹‰å¯¹é½é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¾®è¡¨æƒ…è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `è¿åŠ¨è¯­ä¹‰å¯¹é½` `å¯è§£é‡Šæ€§å»ºæ¨¡` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¾®è¡¨æƒ…è¯†åˆ«æ–¹æ³•é¢ä¸´é™æ€å¤–è§‚ä¸ŽåŠ¨æ€è¿åŠ¨çº¿ç´¢çº ç¼ çš„é—®é¢˜ï¼Œå¯¼è‡´ç»†å¾®è¿åŠ¨éš¾ä»¥æ•æ‰ã€‚
2. æå‡ºDEFT-LLMï¼Œé€šè¿‡å¤šä¸“å®¶è§£è€¦å’ŒUni-MERæ•°æ®é›†ï¼Œå®žçŽ°è¿åŠ¨è¯­ä¹‰ä¸Žæ–‡æœ¬çš„å¯¹é½ã€‚
3. åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§MERåŸºå‡†ä¸Šï¼ŒDEFT-LLMå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å±€éƒ¨é¢éƒ¨è¿åŠ¨çš„å¯è§£é‡Šæ€§å»ºæ¨¡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è¡¨æƒ…è¯†åˆ«ï¼ˆMERï¼‰å¯¹äºŽæŽ¨æ–­çœŸå®žæƒ…æ„Ÿè‡³å…³é‡è¦ã€‚å°†å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰åº”ç”¨äºŽæ­¤ä»»åŠ¡ï¼Œå¯ä»¥å®žçŽ°é¢éƒ¨è¿åŠ¨çš„æ—¶ç©ºåˆ†æžå¹¶æä¾›å¯è§£é‡Šçš„æè¿°ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯é™æ€å¤–è§‚ä¸ŽåŠ¨æ€è¿åŠ¨çº¿ç´¢çš„çº ç¼ ï¼Œå¯¼è‡´æ¨¡åž‹éš¾ä»¥å…³æ³¨ç»†å¾®è¿åŠ¨ï¼›äºŒæ˜¯çŽ°æœ‰MERæ•°æ®é›†ä¸­æ–‡æœ¬æ ‡ç­¾ä¸Žé¢éƒ¨è‚Œè‚‰è¿åŠ¨ä¹‹é—´å­˜åœ¨è¯­ä¹‰å·®è·ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DEFT-LLMï¼Œé€šè¿‡å¤šä¸“å®¶è§£è€¦å®žçŽ°è¿åŠ¨è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥Uni-MERï¼Œä¸€ä¸ªè¿åŠ¨é©±åŠ¨çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæ—¨åœ¨å°†æ–‡æœ¬ä¸Žå±€éƒ¨é¢éƒ¨è¿åŠ¨å¯¹é½ã€‚æŽ¥ç€è®¾è®¡äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªä¸“å®¶çš„æž¶æž„ï¼Œå°†é¢éƒ¨åŠ¨æ€è§£è€¦ä¸ºç‹¬ç«‹ä¸”å¯è§£é‡Šçš„è¡¨ç¤ºã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDEFT-LLMåœ¨å¤šä¸ªMERåŸºå‡†ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å±€éƒ¨é¢éƒ¨è¿åŠ¨çš„å¯è§£é‡Šå»ºæ¨¡æ–¹é¢å…·æœ‰æ˜Žæ˜¾ä¼˜åŠ¿ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­çš„è¿åŠ¨è¯­ä¹‰å¯¹é½é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåŒºåˆ†é™æ€å¤–è§‚ä¸ŽåŠ¨æ€è¿åŠ¨çº¿ç´¢ï¼Œå¯¼è‡´ç»†å¾®æƒ…æ„Ÿè¡¨è¾¾çš„æ•æ‰ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Uni-MERæ•°æ®é›†ï¼Œåˆ©ç”¨å…‰æµå’ŒåŠ¨ä½œå•å…ƒï¼ˆAUï¼‰æ ‡ç­¾çš„åŒé‡çº¦æŸï¼Œç¡®ä¿æ–‡æœ¬ä¸Žé¢éƒ¨è¿åŠ¨ä¹‹é—´çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œä»Žè€Œå®žçŽ°è¿åŠ¨è¯­ä¹‰çš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDEFT-LLMçš„æ•´ä½“æž¶æž„åŒ…å«ä¸‰ä¸ªä¸“å®¶æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£é¢éƒ¨åŠ¨æ€çš„ç»“æž„ã€åŠ¨æ€çº¹ç†å’Œè¿åŠ¨è¯­ä¹‰çš„è§£è€¦ï¼Œç»“åˆUni-MERæä¾›çš„æŒ‡ä»¤çŸ¥è¯†ï¼Œå¢žå¼ºæ¨¡åž‹å¯¹å¾®è¡¨æƒ…çš„ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºŽå¤šä¸“å®¶è§£è€¦æœºåˆ¶ï¼Œä½¿å¾—é¢éƒ¨åŠ¨æ€å¯ä»¥è¢«ç‹¬ç«‹ä¸”å¯è§£é‡Šåœ°è¡¨ç¤ºï¼Œä»Žè€Œå…‹æœäº†çŽ°æœ‰æ–¹æ³•åœ¨ç»†å¾®è¿åŠ¨æ•æ‰ä¸Šçš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡åž‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è¿åŠ¨è¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æž„ä¸­å¼•å…¥äº†å…‰æµå’ŒAUæ ‡ç­¾çš„çº¦æŸï¼Œç¡®ä¿æ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°é¢éƒ¨è¿åŠ¨çš„ç»†å¾®å˜åŒ–ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨å¤šä¸ªå¾®è¡¨æƒ…è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDEFT-LLMå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å±€éƒ¨é¢éƒ¨è¿åŠ¨çš„å¯è§£é‡Šå»ºæ¨¡æ–¹é¢ï¼Œè¾ƒåŸºçº¿æ–¹æ³•æå‡äº†çº¦15%çš„å‡†ç¡®çŽ‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç»†å¾®æƒ…æ„Ÿæ•æ‰ä¸Šçš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿè®¡ç®—ã€å¿ƒç†å¥åº·ç›‘æµ‹å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜å¾®è¡¨æƒ…è¯†åˆ«çš„å‡†ç¡®æ€§ï¼ŒDEFT-LLMå¯ä»¥å¸®åŠ©åœ¨å„ç§åœºæ™¯ä¸­æ›´å¥½åœ°ç†è§£å’Œå“åº”äººç±»æƒ…æ„Ÿï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

