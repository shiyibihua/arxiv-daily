---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-17
---

# cs.CVï¼ˆ2025-10-17ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251015386v1-pfgs-pose-fused-3d-gaussian-splatting-for-complete-multi-pose-object.html">PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction</a></td>
  <td>æå‡ºPFGSï¼Œé€šè¿‡å§¿æ€èåˆ3Dé«˜æ–¯æº…å°„å®ç°å®Œæ•´çš„å¤šå§¿æ€ç‰©ä½“é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15386v1" onclick="toggleFavorite(this, '2510.15386v1', 'PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251015398v2-maris-marine-open-vocabulary-instance-segmentation-with-geometric-en.html">MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment</a></td>
  <td>æå‡ºMARISæ°´ä¸‹å¼€æ”¾è¯æ±‡å®ä¾‹åˆ†å‰²åŸºå‡†ï¼Œå¹¶è®¾è®¡GPEMå’ŒSAIMæ¨¡å—æå‡åˆ†å‰²æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15398v2" onclick="toggleFavorite(this, '2510.15398v2', 'MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251021769v1-h2oflow-grounding-human-object-affordances-with-3d-generative-models.html">H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows</a></td>
  <td>H2OFlowï¼šåˆ©ç”¨3Dç”Ÿæˆæ¨¡å‹å’Œç¨ å¯†æ‰©æ•£æµå­¦ä¹ äºº-ç‰©äº¤äº’è¡Œä¸º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21769v1" onclick="toggleFavorite(this, '2510.21769v1', 'H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251016272v1-proactive-scene-decomposition-and-reconstruction.html">Proactive Scene Decomposition and Reconstruction</a></td>
  <td>æå‡ºä¸»åŠ¨åœºæ™¯åˆ†è§£ä¸é‡å»ºæ–¹æ³•ï¼Œåˆ©ç”¨äººæœºäº¤äº’åŠ¨æ€ä¼˜åŒ–åœºæ™¯ç†è§£ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16272v1" onclick="toggleFavorite(this, '2510.16272v1', 'Proactive Scene Decomposition and Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251015471v1-a-novel-combined-optical-flow-approach-for-comprehensive-micro-expre.html">A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition</a></td>
  <td>æå‡ºç»“åˆèµ·å§‹åˆ°å³°å€¼ä¸å³°å€¼åˆ°ç»“æŸé˜¶æ®µå…‰æµçš„å¾®è¡¨æƒ…è¯†åˆ«æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15471v1" onclick="toggleFavorite(this, '2510.15471v1', 'A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251015264v1-drivegen3d-boosting-feed-forward-driving-scene-generation-with-effic.html">DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion</a></td>
  <td>DriveGen3Dï¼šé€šè¿‡é«˜æ•ˆè§†é¢‘æ‰©æ•£åŠ é€Ÿå‰é¦ˆå¼é©¾é©¶åœºæ™¯ç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15264v1" onclick="toggleFavorite(this, '2510.15264v1', 'DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251015841v1-neuro-symbolic-spatial-reasoning-in-segmentation.html">Neuro-Symbolic Spatial Reasoning in Segmentation</a></td>
  <td>æå‡ºRelateSegï¼Œé€šè¿‡ç¥ç»ç¬¦å·ç©ºé—´æ¨ç†æå‡å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15841v1" onclick="toggleFavorite(this, '2510.15841v1', 'Neuro-Symbolic Spatial Reasoning in Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251015775v1-sanr-scene-aware-neural-representation-for-light-field-image-compres.html">SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization</a></td>
  <td>æå‡ºSANRï¼šä¸€ç§åœºæ™¯æ„ŸçŸ¥ç¥ç»è¡¨ç¤ºå…‰åœºå›¾åƒå‹ç¼©æ¡†æ¶ï¼Œå®ç°ç‡å¤±çœŸä¼˜åŒ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15775v1" onclick="toggleFavorite(this, '2510.15775v1', 'SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251015725v1-dgme-t-directional-grid-motion-encoding-for-transformer-based-histor.html">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a></td>
  <td>æå‡ºDGME-Tï¼Œé€šè¿‡æ–¹å‘ç½‘æ ¼è¿åŠ¨ç¼–ç å¢å¼ºTransformeråœ¨å†å²å½±åƒé•œå¤´è¿åŠ¨åˆ†ç±»ä¸­çš„é²æ£’æ€§</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15725v1" onclick="toggleFavorite(this, '2510.15725v1', 'DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/251015857v1-blip3o-next-next-frontier-of-native-image-generation.html">BLIP3o-NEXT: Next Frontier of Native Image Generation</a></td>
  <td>BLIP3o-NEXTï¼šåŸç”Ÿå›¾åƒç”Ÿæˆçš„æ–°å‰æ²¿ï¼Œç»Ÿä¸€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸å›¾åƒç¼–è¾‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15857v1" onclick="toggleFavorite(this, '2510.15857v1', 'BLIP3o-NEXT: Next Frontier of Native Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251015710v2-unimedvl-unifying-medical-multimodal-understanding-and-generation-th.html">UniMedVL: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis</a></td>
  <td>æå‡ºUniMedVLï¼Œç»Ÿä¸€åŒ»å­¦å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆï¼Œæå‡åŒ»ç–—è¯Šæ–­åº”ç”¨æ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15710v2" onclick="toggleFavorite(this, '2510.15710v2', 'UniMedVL: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251015304v1-layer-as-puzzle-pieces-compressing-large-language-models-through-lay.html">Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation</a></td>
  <td>æå‡ºCoMeï¼šé€šè¿‡å±‚æ‹¼æ¥å‹ç¼©å¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ˜¾è‘—å‰ªæçš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15304v1" onclick="toggleFavorite(this, '2510.15304v1', 'Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251016220v1-vm-beautynet-a-synergistic-ensemble-of-vision-transformer-and-mamba-.html">VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction</a></td>
  <td>VM-BeautyNetï¼šèåˆVision Transformerä¸Mambaçš„é¢éƒ¨ç¾å­¦é¢„æµ‹æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16220v1" onclick="toggleFavorite(this, '2510.16220v1', 'VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251015371v1-cortical-ssm-a-deep-state-space-model-for-eeg-and-ecog-motor-imagery.html">Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding</a></td>
  <td>æå‡ºCortical-SSMï¼Œåˆ©ç”¨æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹è§£ç è„‘ç”µå’Œçš®å±‚è„‘ç”µè¿åŠ¨æƒ³è±¡ä¿¡å·</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15371v1" onclick="toggleFavorite(this, '2510.15371v1', 'Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251016209v1-stretchysnake-flexible-ssm-training-unlocks-action-recognition-acros.html">StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales</a></td>
  <td>StretchySnakeï¼šçµæ´»çš„SSMè®­ç»ƒè§£é”è·¨æ—¶ç©ºå°ºåº¦çš„åŠ¨ä½œè¯†åˆ«</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16209v1" onclick="toggleFavorite(this, '2510.16209v1', 'StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251015742v1-scaling-instruction-based-video-editing-with-a-high-quality-syntheti.html">Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset</a></td>
  <td>æå‡ºDittoæ¡†æ¶ï¼Œé€šè¿‡é«˜è´¨é‡åˆæˆæ•°æ®é›†Editto-1Mï¼Œæ˜¾è‘—æå‡æŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15742v1" onclick="toggleFavorite(this, '2510.15742v1', 'Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251015440v1-select-less-reason-more-prioritizing-evidence-purity-for-video-reaso.html">Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning</a></td>
  <td>æå‡ºåŸºäºè¯æ®ä¼˜å…ˆçš„è‡ªé€‚åº”æ¡†æ¶EARLï¼Œè§£å†³è§†é¢‘LLMé•¿è§†é¢‘æ¨ç†ä¸­ä¿¡æ¯ç¨€é‡Šé—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15440v1" onclick="toggleFavorite(this, '2510.15440v1', 'Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251015564v1-imaginarium-vision-guided-high-quality-3d-scene-layout-generation.html">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a></td>
  <td>Imaginariumï¼šæå‡ºè§†è§‰å¼•å¯¼çš„é«˜è´¨é‡3Dåœºæ™¯å¸ƒå±€ç”Ÿæˆæ–¹æ³•ï¼Œæå‡åœºæ™¯ä¸°å¯Œåº¦å’Œè´¨é‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15564v1" onclick="toggleFavorite(this, '2510.15564v1', 'Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251015467v1-mrasfm-multi-camera-reconstruction-and-aggregation-through-structure.html">MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes</a></td>
  <td>MRASfMï¼šæå‡ºå¤šç›¸æœºSfMæ¡†æ¶ï¼Œè§£å†³è‡ªåŠ¨é©¾é©¶åœºæ™¯é‡å»ºéš¾é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15467v1" onclick="toggleFavorite(this, '2510.15467v1', 'MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/251016258v1-embody-3d-a-large-scale-multimodal-motion-and-behavior-dataset.html">Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset</a></td>
  <td>Metaå‘å¸ƒEmbody 3Dï¼šå¤§è§„æ¨¡å¤šæ¨¡æ€äººä½“è¿åŠ¨ä¸è¡Œä¸ºæ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16258v1" onclick="toggleFavorite(this, '2510.16258v1', 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251015684v1-towards-label-free-brain-tumor-segmentation-unsupervised-learning-wi.html">Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI</a></td>
  <td>æå‡ºåŸºäºå¤šæ¨¡æ€MRIçš„æ— ç›‘ç£è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œè§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15684v1" onclick="toggleFavorite(this, '2510.15684v1', 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/251016134v1-aria-gen-2-pilot-dataset.html">Aria Gen 2 Pilot Dataset</a></td>
  <td>å‘å¸ƒAria Gen 2 Pilot Datasetï¼Œç”¨äºå¯ç©¿æˆ´è®¾å¤‡çš„ç¬¬ä¸€äººç§°è§†è§’å¤šæ¨¡æ€æ„ŸçŸ¥ç ”ç©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16134v1" onclick="toggleFavorite(this, '2510.16134v1', 'Aria Gen 2 Pilot Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251015392v1-lilac-long-sequence-incremental-low-latency-arbitrary-motion-styliza.html">LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding</a></td>
  <td>LILACï¼šåŸºäºæµå¼VAE-Diffusionå’Œå› æœè§£ç çš„é•¿åºåˆ—å¢é‡ä½å»¶è¿Ÿä»»æ„åŠ¨ä½œé£æ ¼åŒ–</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.15392v1" onclick="toggleFavorite(this, '2510.15392v1', 'LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)