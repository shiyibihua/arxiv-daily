---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-30
---

# cs.CVï¼ˆ2025-10-30ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251026641v2-all-you-need-for-object-detection-from-pixels-points-and-prompts-to-.html">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</a></td>
  <td>é¢å‘è‡ªåŠ¨é©¾é©¶ï¼Œç»¼è¿°èåˆLLM/VLMçš„æ–°ä¸€ä»£å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æŠ€æœ¯</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26641v2" onclick="toggleFavorite(this, '2510.26641v2', 'All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251026114v1-oracleagent-a-multimodal-reasoning-agent-for-oracle-bone-script-rese.html">OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research</a></td>
  <td>OracleAgentï¼šç”¨äºç”²éª¨æ–‡ç ”ç©¶çš„å¤šæ¨¡æ€æ¨ç†Agentç³»ç»Ÿ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26114v1" onclick="toggleFavorite(this, '2510.26114v1', 'OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251027047v1-ad-sam-fine-tuning-the-segment-anything-vision-foundation-model-for-.html">AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception</a></td>
  <td>AD-SAMï¼šå¾®è°ƒSAMè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27047v1" onclick="toggleFavorite(this, '2510.27047v1', 'AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251026703v1-prostnfound-a-prospective-study-using-medical-foundation-models-for-.html">ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection</a></td>
  <td>ProstNFound+ï¼šåˆ©ç”¨åŒ»å­¦åŸºç¡€æ¨¡å‹å®ç°å‰åˆ—è…ºç™Œå¾®è¶…å£°æ£€æµ‹çš„å‰ç»æ€§ç ”ç©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26703v1" onclick="toggleFavorite(this, '2510.26703v1', 'ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251100095v1-spinalsam-r1-a-vision-language-multimodal-interactive-system-for-spi.html">SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</a></td>
  <td>SpinalSAM-R1ï¼šç”¨äºè„ŠæŸ±CTåˆ†å‰²çš„è§†è§‰-è¯­è¨€å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00095v1" onclick="toggleFavorite(this, '2511.00095v1', 'SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251026996v1-mome-mixture-of-visual-language-medical-experts-for-medical-imaging-.html">MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation</a></td>
  <td>æå‡ºMoMEï¼šä¸€ç§ç”¨äºåŒ»å­¦å½±åƒåˆ†å‰²çš„è§†è§‰è¯­è¨€æ··åˆä¸“å®¶æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26996v1" onclick="toggleFavorite(this, '2510.26996v1', 'MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251026125v3-wod-e2e-waymo-open-dataset-for-end-to-end-driving-in-challenging-lon.html">WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios</a></td>
  <td>WOD-E2Eï¼šé’ˆå¯¹ç«¯åˆ°ç«¯é©¾é©¶ä¸­é•¿å°¾åœºæ™¯çš„Waymoå¼€æ”¾æ•°æ®é›†</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26125v3" onclick="toggleFavorite(this, '2510.26125v3', 'WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251026978v1-semantic-frame-aggregation-based-transformer-for-live-video-comment-.html">Semantic Frame Aggregation-based Transformer for Live Video Comment Generation</a></td>
  <td>æå‡ºåŸºäºè¯­ä¹‰å¸§èšåˆTransformerçš„SFATæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆç›´æ’­è§†é¢‘è¯„è®ºã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26978v1" onclick="toggleFavorite(this, '2510.26978v1', 'Semantic Frame Aggregation-based Transformer for Live Video Comment Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251026800v1-omnix-from-unified-panoramic-generation-and-perception-to-graphics-r.html">OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes</a></td>
  <td>OmniXï¼šåˆ©ç”¨å…¨æ™¯ç”Ÿæˆä¸æ„ŸçŸ¥ï¼Œç”Ÿæˆå¯ç”¨äºå›¾å½¢æ¸²æŸ“çš„3Dåœºæ™¯</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26800v1" onclick="toggleFavorite(this, '2510.26800v1', 'OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251026769v1-steervlm-robust-model-control-through-lightweight-activation-steerin.html">SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models</a></td>
  <td>æå‡ºSteerVLMä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ§åˆ¶èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26769v1" onclick="toggleFavorite(this, '2510.26769v1', 'SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251026466v2-representation-level-counterfactual-calibration-for-debiased-zero-sh.html">Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition</a></td>
  <td>æå‡ºè¡¨å¾çº§åäº‹å®æ ¡å‡†æ–¹æ³•ï¼Œè§£å†³é›¶æ ·æœ¬è¯†åˆ«ä¸­çš„ä¸Šä¸‹æ–‡åå·®é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26466v2" onclick="toggleFavorite(this, '2510.26466v2', 'Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251026241v2-which-way-does-time-flow-a-psychophysics-grounded-evaluation-for-vis.html">Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</a></td>
  <td>æå‡ºAoT-PsyPhyBENCHåŸºå‡†ï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘æ—¶é—´æ–¹å‘çš„ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26241v2" onclick="toggleFavorite(this, '2510.26241v2', 'Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251026186v1-conceptscope-characterizing-dataset-bias-via-disentangled-visual-con.html">ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts</a></td>
  <td>ConceptScopeï¼šé€šè¿‡è§£è€¦è§†è§‰æ¦‚å¿µè¡¨å¾æ¥é‡åŒ–å’Œè¯†åˆ«æ•°æ®é›†åå·®ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26186v1" onclick="toggleFavorite(this, '2510.26186v1', 'ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251026117v1-jogs-joint-optimization-of-pose-estimation-and-3d-gaussian-splatting.html">JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting</a></td>
  <td>æå‡ºJOGSï¼Œè”åˆä¼˜åŒ–ä½å§¿ä¼°è®¡å’Œ3Dé«˜æ–¯æº…å°„ï¼Œæ— éœ€é¢„æ ¡å‡†è¾“å…¥ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26117v1" onclick="toggleFavorite(this, '2510.26117v1', 'JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251026694v1-the-impact-and-outlook-of-3d-gaussian-splatting.html">The Impact and Outlook of 3D Gaussian Splatting</a></td>
  <td>3Dé«˜æ–¯æº…å°„æŠ€æœ¯ç»¼è¿°ï¼šå›é¡¾è¿›å±•ã€æ´å¯Ÿæ–¹å‘ã€å±•æœ›æœªæ¥åº”ç”¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26694v1" onclick="toggleFavorite(this, '2510.26694v1', 'The Impact and Outlook of 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251026921v1-dc4gs-directional-consistency-driven-adaptive-density-control-for-3d.html">DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting</a></td>
  <td>æå‡ºæ–¹å‘ä¸€è‡´æ€§é©±åŠ¨çš„è‡ªé€‚åº”å¯†åº¦æ§åˆ¶æ–¹æ³•DC4GSï¼Œæå‡3Dé«˜æ–¯ Splattingçš„é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26921v1" onclick="toggleFavorite(this, '2510.26921v1', 'DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251026786v1-heir-learning-graph-based-motion-hierarchies.html">HEIR: Learning Graph-Based Motion Hierarchies</a></td>
  <td>æå‡ºHEIRï¼Œå­¦ä¹ åŸºäºå›¾çš„è¿åŠ¨å±‚æ¬¡ç»“æ„ï¼Œå®ç°æ•°æ®é©±åŠ¨çš„è¿åŠ¨å»ºæ¨¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26786v1" onclick="toggleFavorite(this, '2510.26786v1', 'HEIR: Learning Graph-Based Motion Hierarchies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251026653v1-towards-reliable-sea-ice-drift-estimation-in-the-arctic-deep-learnin.html">Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2</a></td>
  <td>åˆ©ç”¨æ·±åº¦å­¦ä¹ å…‰æµæ³•ï¼Œæå‡RADARSAT-2å«æ˜Ÿå›¾åƒæµ·å†°æ¼‚ç§»ä¼°è®¡çš„å¯é æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26653v1" onclick="toggleFavorite(this, '2510.26653v1', 'Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251100107v1-ai-powered-high-quality-text-to-video-generation-with-enhanced-tempo.html">AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</a></td>
  <td>MOVAIï¼šæå‡ºä¸€ç§æ—¶åºä¸€è‡´çš„AIé©±åŠ¨é«˜è´¨é‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00107v1" onclick="toggleFavorite(this, '2511.00107v1', 'AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251026173v1-motdiff-high-resolution-motion-trajectory-estimation-from-a-single-b.html">MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models</a></td>
  <td>MoTDiffï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»å•å¼ æ¨¡ç³Šå›¾åƒä¸­ä¼°è®¡é«˜åˆ†è¾¨ç‡è¿åŠ¨è½¨è¿¹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26173v1" onclick="toggleFavorite(this, '2510.26173v1', 'MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251027492v2-thinkmorph-emergent-properties-in-multimodal-interleaved-chain-of-th.html">ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning</a></td>
  <td>ThinkMorphï¼šé€šè¿‡å¤šæ¨¡æ€äº¤é”™CoTæ¨ç†æ¶Œç°è§†è§‰æ“ä½œèƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27492v2" onclick="toggleFavorite(this, '2510.27492v2', 'ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251026583v1-emu35-native-multimodal-models-are-world-learners.html">Emu3.5: Native Multimodal Models are World Learners</a></td>
  <td>Emu3.5ï¼šåŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡é¢„æµ‹è§†è§‰å’Œè¯­è¨€çš„ä¸‹ä¸€ä¸ªçŠ¶æ€å®ç°ä¸–ç•Œç†è§£ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26583v1" onclick="toggleFavorite(this, '2510.26583v1', 'Emu3.5: Native Multimodal Models are World Learners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251100091v1-self-improving-vision-language-action-models-with-data-generation-vi.html">Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</a></td>
  <td>æå‡ºPLDæ¡†æ¶ï¼Œé€šè¿‡æ®‹å·®å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆè‡ªæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.00091v1" onclick="toggleFavorite(this, '2511.00091v1', 'Self-Improving Vision-Language-Action Models with Data Generation via Residual RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251026292v1-beyond-imitation-constraint-aware-trajectory-generation-with-flow-ma.html">Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving</a></td>
  <td>æå‡ºCATGï¼Œåˆ©ç”¨çº¦æŸæµåŒ¹é…è¿›è¡Œç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶è½¨è¿¹ç”Ÿæˆï¼Œè§£å†³æ¨¡ä»¿å­¦ä¹ æ¨¡å¼å´©å¡Œé—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26292v1" onclick="toggleFavorite(this, '2510.26292v1', 'Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251026802v1-are-video-models-ready-as-zero-shot-reasoners-an-empirical-study-wit.html">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</a></td>
  <td>è¯„ä¼°è§†é¢‘æ¨¡å‹é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ï¼šæå‡ºMME-CoFåŸºå‡†å¹¶åˆ†æVeo-3çš„æ¨ç†å±€é™æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26802v1" onclick="toggleFavorite(this, '2510.26802v1', 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251027020v1-incremental-human-object-interaction-detection-with-invariant-relati.html">Incremental Human-Object Interaction Detection with Invariant Relation Representation Learning</a></td>
  <td>æå‡ºå¢é‡å…³ç³»è’¸é¦æ¡†æ¶IRDï¼Œè§£å†³å¼€æ”¾ä¸–ç•Œä¸­äºº-ç‰©äº¤äº’çš„æŒç»­å­¦ä¹ é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.27020v1" onclick="toggleFavorite(this, '2510.27020v1', 'Incremental Human-Object Interaction Detection with Invariant Relation Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251026794v1-the-quest-for-generalizable-motion-generation-data-model-and-evaluat.html">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a></td>
  <td>æå‡ºViMoGenæ¡†æ¶ï¼Œé€šè¿‡è¿ç§»è§†é¢‘ç”ŸæˆçŸ¥è¯†ï¼Œæå‡3Däººä½“åŠ¨ä½œç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26794v1" onclick="toggleFavorite(this, '2510.26794v1', 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251026113v1-egoexo-con-exploring-view-invariant-video-temporal-understanding.html">EgoExo-Con: Exploring View-Invariant Video Temporal Understanding</a></td>
  <td>æå‡ºEgoExo-ConåŸºå‡†ä¸View-GRPOæ¡†æ¶ï¼Œæå‡è§†é¢‘LLMè§†è§’ä¸å˜çš„æ—¶é—´ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26113v1" onclick="toggleFavorite(this, '2510.26113v1', 'EgoExo-Con: Exploring View-Invariant Video Temporal Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251026160v1-crag-mm-multi-modal-multi-turn-comprehensive-rag-benchmark.html">CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</a></td>
  <td>æå‡ºCRAG-MMï¼šä¸€ä¸ªç”¨äºå¯ç©¿æˆ´è®¾å¤‡åœºæ™¯çš„å¤šæ¨¡æ€å¤šè½®å¯¹è¯RAGç»¼åˆåŸºå‡†ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.26160v1" onclick="toggleFavorite(this, '2510.26160v1', 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)