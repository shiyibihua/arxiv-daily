---
layout: default
title: SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting
---

# SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting

**arXiv**: [2510.26796v1](https://arxiv.org/abs/2510.26796) | [PDF](https://arxiv.org/pdf/2510.26796.pdf)

**ä½œè€…**: Dongyue Lu, Ao Liang, Tianxin Huang, Xiao Fu, Yuyang Zhao, Baorui Ma, Liang Pan, Wei Yin, Lingdong Kong, Wei Tsang Ooi, Ziwei Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEE4Dæ–¹æ³•ï¼Œé€šè¿‡è‡ªå›žå½’è§†é¢‘ä¿®å¤å®žçŽ°æ— å§¿æ€4Dç”Ÿæˆï¼Œä»Žéšæ„è§†é¢‘åˆæˆæ—¶ç©ºå†…å®¹ã€‚**

**å…³é”®è¯**: `4Dç”Ÿæˆ` `è§†é¢‘ä¿®å¤` `æ— å§¿æ€å­¦ä¹ ` `è™šæ‹Ÿç›¸æœº` `è‡ªå›žå½’æŽ¨ç†` `æ—¶ç©ºå»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘åˆ°4Dæ–¹æ³•ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨ç›¸æœºå§¿æ€ï¼Œæˆæœ¬é«˜ä¸”å¯¹é‡Žå¤–è§†é¢‘è„†å¼±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å›ºå®šè™šæ‹Ÿç›¸æœºåº“å’Œè§†å›¾æ¡ä»¶è§†é¢‘ä¿®å¤æ¨¡åž‹ï¼Œåˆ†ç¦»ç›¸æœºæŽ§åˆ¶ä¸Žåœºæ™¯å»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è·¨è§†å›¾è§†é¢‘ç”Ÿæˆå’Œç¨€ç–é‡å»ºåŸºå‡†ä¸Šï¼Œå®žçŽ°ä¼˜äºŽå§¿æ€æˆ–è½¨è¿¹æ¡ä»¶åŸºçº¿çš„æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Immersive applications call for synthesizing spatiotemporal 4D content from
> casual videos without costly 3D supervision. Existing video-to-4D methods
> typically rely on manually annotated camera poses, which are labor-intensive
> and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
> mitigate the need for pose labels by warping input frames along a novel camera
> trajectory and using an inpainting model to fill missing regions, thereby
> depicting the 4D scene from diverse viewpoints. However, this
> trajectory-to-trajectory formulation often entangles camera motion with scene
> dynamics and complicates both modeling and inference. We introduce SEE4D, a
> pose-free, trajectory-to-camera framework that replaces explicit trajectory
> prediction with rendering to a bank of fixed virtual cameras, thereby
> separating camera control from scene modeling. A view-conditional video
> inpainting model is trained to learn a robust geometry prior by denoising
> realistically synthesized warped images and to inpaint occluded or missing
> regions across virtual viewpoints, eliminating the need for explicit 3D
> annotations. Building on this inpainting core, we design a spatiotemporal
> autoregressive inference pipeline that traverses virtual-camera splines and
> extends videos with overlapping windows, enabling coherent generation at
> bounded per-step complexity. We validate See4D on cross-view video generation
> and sparse reconstruction benchmarks. Across quantitative metrics and
> qualitative assessments, our method achieves superior generalization and
> improved performance relative to pose- or trajectory-conditioned baselines,
> advancing practical 4D world modeling from casual videos.

