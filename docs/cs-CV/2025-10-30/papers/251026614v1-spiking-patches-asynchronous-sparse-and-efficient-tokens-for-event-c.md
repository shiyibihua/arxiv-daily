---
layout: default
title: Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras
---

# Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras

**arXiv**: [2510.26614v1](https://arxiv.org/abs/2510.26614) | [PDF](https://arxiv.org/pdf/2510.26614.pdf)

**ä½œè€…**: Christoffer Koo Ã˜hrstrÃ¸m, Ronja GÃ¼ldenring, Lazaros Nalpantidis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpiking Patchesä»¤ç‰ŒåŒ–æ–¹æ³•ï¼Œä»¥ä¿æŒäº‹ä»¶ç›¸æœºå¼‚æ­¥ç¨€ç–ç‰¹æ€§å¹¶æå‡æ•ˆçŽ‡ã€‚**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `ä»¤ç‰ŒåŒ–` `å¼‚æ­¥å¤„ç†` `ç¨€ç–è¡¨ç¤º` `é«˜æ•ˆæŽ¨ç†` `è®¡ç®—æœºè§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. äº‹ä»¶ç›¸æœºè¾“å‡ºå¼‚æ­¥ç¨€ç–äº‹ä»¶æµï¼ŒçŽ°æœ‰å¸§æˆ–ä½“ç´ è¡¨ç¤ºç‰ºç‰²è¿™äº›ç‰¹æ€§ã€‚
2. Spiking Patchesä»¤ç‰ŒåŒ–æ–¹æ³•ç”Ÿæˆå¼‚æ­¥ç¨€ç–ä»¤ç‰Œï¼Œä¿ç•™äº‹ä»¶ç›¸æœºç‹¬ç‰¹å±žæ€§ã€‚
3. å®žéªŒæ˜¾ç¤ºï¼Œåœ¨å§¿æ€è¯†åˆ«å’Œç‰©ä½“æ£€æµ‹ä¸­ï¼Œé€Ÿåº¦æå‡è¾¾10.4å€ï¼Œç²¾åº¦ç›¸å½“æˆ–æ›´é«˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose tokenization of events and present a tokenizer, Spiking Patches,
> specifically designed for event cameras. Given a stream of asynchronous and
> spatially sparse events, our goal is to discover an event representation that
> preserves these properties. Prior works have represented events as frames or as
> voxels. However, while these representations yield high accuracy, both frames
> and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
> gives the means to preserve the unique properties of event cameras and we show
> in our experiments that this comes without sacrificing accuracy. We evaluate
> our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
> object detection. Tokens from Spiking Patches yield inference times that are up
> to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
> achieve this while matching their accuracy and even surpassing in some cases
> with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
> object detection. Thus, tokenization constitutes a novel direction in
> event-based vision and marks a step towards methods that preserve the
> properties of event cameras.

