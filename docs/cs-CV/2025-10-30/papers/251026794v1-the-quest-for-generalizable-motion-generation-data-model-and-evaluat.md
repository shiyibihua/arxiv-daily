---
layout: default
title: The Quest for Generalizable Motion Generation: Data, Model, and Evaluation
---

# The Quest for Generalizable Motion Generation: Data, Model, and Evaluation

**arXiv**: [2510.26794v1](https://arxiv.org/abs/2510.26794) | [PDF](https://arxiv.org/pdf/2510.26794.pdf)

**ä½œè€…**: Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViMoGenæ¡†æž¶ï¼Œé€šè¿‡è§†é¢‘ç”ŸæˆçŸ¥è¯†è¿ç§»æå‡3Däººä½“è¿åŠ¨ç”Ÿæˆçš„æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `3Däººä½“è¿åŠ¨ç”Ÿæˆ` `çŸ¥è¯†è¿ç§»` `æ‰©æ•£æ¨¡åž‹` `æ•°æ®é›†æž„å»º` `åŸºå‡†è¯„ä¼°` `æ¨¡åž‹è’¸é¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰3Däººä½“è¿åŠ¨ç”Ÿæˆæ¨¡åž‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œè€Œè§†é¢‘ç”Ÿæˆé¢†åŸŸå·²å±•ç¤ºå¼ºæ³›åŒ–æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºViMoGen-228Kæ•°æ®é›†ï¼Œè®¾è®¡åŸºäºŽæµåŒ¹é…çš„æ‰©æ•£Transformeræ¨¡åž‹ï¼Œå¹¶å¼€å‘è½»é‡è’¸é¦ç‰ˆæœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶å‘å¸ƒä»£ç ã€æ•°æ®å’ŒåŸºå‡†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite recent advances in 3D human motion generation (MoGen) on standard
> benchmarks, existing models still face a fundamental bottleneck in their
> generalization capability. In contrast, adjacent generative fields, most
> notably video generation (ViGen), have demonstrated remarkable generalization
> in modeling human behaviors, highlighting transferable insights that MoGen can
> leverage. Motivated by this observation, we present a comprehensive framework
> that systematically transfers knowledge from ViGen to MoGen across three key
> pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
> large-scale dataset comprising 228,000 high-quality motion samples that
> integrates high-fidelity optical MoCap data with semantically annotated motions
> from web videos and synthesized samples generated by state-of-the-art ViGen
> models. The dataset includes both text-motion pairs and text-video-motion
> triplets, substantially expanding semantic diversity. Second, we propose
> ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
> MoCap data and ViGen models through gated multimodal conditioning. To enhance
> efficiency, we further develop ViMoGen-light, a distilled variant that
> eliminates video generation dependencies while preserving strong
> generalization. Finally, we present MBench, a hierarchical benchmark designed
> for fine-grained evaluation across motion quality, prompt fidelity, and
> generalization ability. Extensive experiments show that our framework
> significantly outperforms existing approaches in both automatic and human
> evaluations. The code, data, and benchmark will be made publicly available.

