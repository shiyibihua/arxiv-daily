---
layout: default
title: Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing
---

# Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing

**arXiv**: [2510.26474v1](https://arxiv.org/abs/2510.26474) | [PDF](https://arxiv.org/pdf/2510.26474.pdf)

**ä½œè€…**: Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤´å°¾é‡å¹³è¡¡ç­–ç•¥ä»¥è§£å†³LVLMè‡ªæ”¹è¿›ä¸­çš„é©¬å¤ªæ•ˆåº”é—®é¢˜**

**å…³é”®è¯**: `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `è‡ªæ”¹è¿›` `é©¬å¤ªæ•ˆåº”` `å¤´å°¾é‡å¹³è¡¡` `è§†è§‰æŽ¨ç†` `åˆ†å¸ƒé‡å¡‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LVLMè‡ªæ”¹è¿›ä¸­æ¨¡åž‹å¯¹ç®€å•æŸ¥è¯¢ä¼˜åŒ–è¿‡åº¦ï¼Œå¤æ‚æŸ¥è¯¢ä¼˜åŒ–ä¸è¶³ï¼Œå¯¼è‡´é©¬å¤ªæ•ˆåº”ã€‚
2. ä»Žåˆ†å¸ƒé‡å¡‘å’Œè½¨è¿¹é‡é‡‡æ ·è§’åº¦å¼•å…¥å››ç§ç­–ç•¥ï¼Œå®žçŽ°å¤´å°¾æ•°æ®å¹³è¡¡ä¼˜åŒ–ã€‚
3. åœ¨è§†è§‰æŽ¨ç†ä»»åŠ¡ä¸­ï¼Œæ–¹æ³•å¹³å‡æå‡3.86åˆ†ï¼Œä¼˜äºŽåŸºçº¿è‡ªæ”¹è¿›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self-improvement has emerged as a mainstream paradigm for advancing the
> reasoning capabilities of large vision-language models (LVLMs), where models
> explore and learn from successful trajectories iteratively. However, we
> identify a critical issue during this process: the model excels at generating
> high-quality trajectories for simple queries (i.e., head data) but struggles
> with more complex ones (i.e., tail data). This leads to an imbalanced
> optimization that drives the model to prioritize simple reasoning skills, while
> hindering its ability to tackle more complex reasoning tasks. Over iterations,
> this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
> effect"--which ultimately hinders further model improvement and leads to
> performance bottlenecks. To counteract this challenge, we introduce four
> efficient strategies from two perspectives: distribution-reshaping and
> trajectory-resampling, to achieve head-tail re-balancing during the
> exploration-and-learning self-improvement process. Extensive experiments on
> Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
> demonstrate that our methods consistently improve visual reasoning
> capabilities, outperforming vanilla self-improvement by 3.86 points on average.

