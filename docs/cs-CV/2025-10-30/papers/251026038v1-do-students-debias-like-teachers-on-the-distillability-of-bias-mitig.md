---
layout: default
title: Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods
---

# Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods

**arXiv**: [2510.26038v1](https://arxiv.org/abs/2510.26038) | [PDF](https://arxiv.org/pdf/2510.26038.pdf)

**ä½œè€…**: Jiali Cheng, Chirag Agarwal, Hadi Amiri

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶çŸ¥è¯†è’¸é¦å¯¹åŽ»åèƒ½åŠ›å¯è’¸é¦æ€§çš„å½±å“å¹¶æå‡ºæ”¹è¿›æ–¹æ³•**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `åŽ»åæ–¹æ³•` `æ¨¡åž‹é²æ£’æ€§` `è‡ªç„¶è¯­è¨€æŽ¨ç†` `å›¾åƒåˆ†ç±»` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŸ¥è¯†è’¸é¦æ˜¯å¦å½±å“æ¨¡åž‹åŽ»åèƒ½åŠ›åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„é²æ£’æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å®žéªŒåˆ†æžåŽ»åèƒ½åŠ›åœ¨è’¸é¦åŽçš„å˜åŒ–åŠå†…éƒ¨æœºåˆ¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°åŽ»åèƒ½åŠ›å—æŸï¼Œæå‡ºæ•°æ®å¢žå¼ºç­‰ä¸‰ç§æ”¹è¿›æ–¹æ¡ˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge distillation (KD) is an effective method for model compression and
> transferring knowledge between models. However, its effect on model's
> robustness against spurious correlations that degrade performance on
> out-of-distribution data remains underexplored. This study investigates the
> effect of knowledge distillation on the transferability of ``debiasing''
> capabilities from teacher models to student models on natural language
> inference (NLI) and image classification tasks. Through extensive experiments,
> we illustrate several key findings: (i) overall the debiasing capability of a
> model is undermined post-KD; (ii) training a debiased model does not benefit
> from injecting teacher knowledge; (iii) although the overall robustness of a
> model may remain stable post-distillation, significant variations can occur
> across different types of biases; and (iv) we pin-point the internal attention
> pattern and circuit that causes the distinct behavior post-KD. Given the above
> findings, we propose three effective solutions to improve the distillability of
> debiasing methods: developing high quality data for augmentation, implementing
> iterative knowledge distillation, and initializing student models with weights
> obtained from teacher models. To the best of our knowledge, this is the first
> study on the effect of KD on debiasing and its interenal mechanism at scale.
> Our findings provide understandings on how KD works and how to design better
> debiasing methods.

