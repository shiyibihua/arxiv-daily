---
layout: default
title: Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition
---

# Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition

**arXiv**: [2510.26466v1](https://arxiv.org/abs/2510.26466) | [PDF](https://arxiv.org/pdf/2510.26466.pdf)

**ä½œè€…**: Pei Peng, MingKun Xie, Hang Hao, Tong Jin, ShengJun Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¡¨ç¤ºçº§åäº‹å®žæ ¡å‡†æ–¹æ³•ä»¥è§£å†³é›¶æ ·æœ¬è¯†åˆ«ä¸­çš„å¯¹è±¡-ä¸Šä¸‹æ–‡åè§é—®é¢˜**

**å…³é”®è¯**: `é›¶æ ·æœ¬è¯†åˆ«` `åäº‹å®žæ ¡å‡†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å› æžœæŽ¨ç†` `è¡¨ç¤ºå­¦ä¹ ` `åè§ç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨é›¶æ ·æœ¬è¯†åˆ«ä¸­å› å¯¹è±¡-ä¸Šä¸‹æ–‡æ·å¾„è€Œä¸å¯é ï¼Œæµ‹è¯•åœºæ™¯ä¸Žè®­ç»ƒå…±çŽ°ä¸åŒæ—¶æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨CLIPè¡¨ç¤ºç©ºé—´ä¸­ä¼°è®¡å¯¹è±¡å’ŒèƒŒæ™¯æœŸæœ›ï¼Œåˆæˆåäº‹å®žåµŒå…¥ä»¥æ¨¡æ‹Ÿå¯¹è±¡åœ¨ä¸åŒçŽ¯å¢ƒä¸­çš„é¢„æµ‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ— éœ€é‡è®­ç»ƒæˆ–æç¤ºè®¾è®¡ï¼Œåœ¨ä¸Šä¸‹æ–‡æ•æ„ŸåŸºå‡†ä¸Šæ˜¾è‘—æå‡æœ€å·®ç»„å’Œå¹³å‡å‡†ç¡®çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Object-context shortcuts remain a persistent challenge in vision-language
> models, undermining zero-shot reliability when test-time scenes differ from
> familiar training co-occurrences. We recast this issue as a causal inference
> problem and ask: Would the prediction remain if the object appeared in a
> different environment? To answer this at inference time, we estimate object and
> background expectations within CLIP's representation space, and synthesize
> counterfactual embeddings by recombining object features with diverse
> alternative contexts sampled from external datasets, batch neighbors, or
> text-derived descriptions. By estimating the Total Direct Effect and simulating
> intervention, we further subtract background-only activation, preserving
> beneficial object-context interactions while mitigating hallucinated scores.
> Without retraining or prompt design, our method substantially improves both
> worst-group and average accuracy on context-sensitive benchmarks, establishing
> a new zero-shot state of the art. Beyond performance, our framework provides a
> lightweight representation-level counterfactual approach, offering a practical
> causal avenue for debiased and reliable multimodal reasoning.

