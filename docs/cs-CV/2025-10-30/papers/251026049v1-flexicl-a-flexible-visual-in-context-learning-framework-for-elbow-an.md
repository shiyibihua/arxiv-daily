---
layout: default
title: FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation
---

# FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation

**arXiv**: [2510.26049v1](https://arxiv.org/abs/2510.26049) | [PDF](https://arxiv.org/pdf/2510.26049.pdf)

**ä½œè€…**: Yuyue Zhou, Jessica Knight, Shrimanti Ghosh, Banafshe Felfeliyan, Jacob L. Jaremko, Abhilash R. Hareendranathan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlexICLæ¡†æž¶ä»¥è§£å†³è‚˜è…•è¶…å£°åˆ†å‰²ä¸­æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜**

**å…³é”®è¯**: `è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ` `è¶…å£°å›¾åƒåˆ†å‰²` `åŒ»å­¦å½±åƒ` `æ ‡æ³¨æ•ˆçŽ‡` `æ¨¡åž‹é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‚˜è…•éª¨æŠ˜è¶…å£°åˆ†å‰²éœ€ä¸“å®¶æ ‡æ³¨ï¼Œä½†æ ‡æ³¨æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»…éœ€å°‘é‡æ ‡æ³¨å¸§ï¼Œé€šè¿‡å›¾åƒæ‹¼æŽ¥å’Œå¢žå¼ºç­–ç•¥æå‡æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªæ•°æ®é›†ä¸Šï¼Œä»…ç”¨5%è®­ç»ƒå›¾åƒï¼ŒDiceç³»æ•°ä¼˜äºŽçŽ°æœ‰æ¨¡åž‹1-27%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Elbow and wrist fractures are the most common fractures in pediatric
> populations. Automatic segmentation of musculoskeletal structures in ultrasound
> (US) can improve diagnostic accuracy and treatment planning. Fractures appear
> as cortical defects but require expert interpretation. Deep learning (DL) can
> provide real-time feedback and highlight key structures, helping lightly
> trained users perform exams more confidently. However, pixel-wise expert
> annotations for training remain time-consuming and costly. To address this
> challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
> framework for segmenting bony regions in US images. We apply it to an
> intra-video segmentation setting, where experts annotate only a small subset of
> frames, and the model segments unseen frames. We systematically investigate
> various image concatenation techniques and training strategies for visual ICL
> and introduce novel concatenation methods that significantly enhance model
> performance with limited labeled data. By integrating multiple augmentation
> strategies, FlexICL achieves robust segmentation performance across four wrist
> and elbow US datasets while requiring only 5% of the training images. It
> outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
> conventional segmentation models like U-Net and TransUNet by 1-27% Dice
> coefficient on 1,252 US sweeps. These initial results highlight the potential
> of FlexICL as an efficient and scalable solution for US image segmentation well
> suited for medical imaging use cases where labeled data is scarce.

