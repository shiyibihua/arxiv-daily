---
layout: default
title: Improving Classification of Occluded Objects through Scene Context
---

# Improving Classification of Occluded Objects through Scene Context

**arXiv**: [2510.26681v1](https://arxiv.org/abs/2510.26681) | [PDF](https://arxiv.org/pdf/2510.26681.pdf)

**ä½œè€…**: Courtney M. King, Daniel D. Leeds, Damian Lyons, George Kalaitzis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤ç§åœºæ™¯ä¿¡æ¯èžåˆæ–¹æ³•ä»¥æå‡é®æŒ¡ç‰©ä½“åˆ†ç±»æ€§èƒ½**

**å…³é”®è¯**: `é®æŒ¡ç‰©ä½“è¯†åˆ«` `åœºæ™¯ä¸Šä¸‹æ–‡èžåˆ` `RPN-DCNNç½‘ç»œ` `å¤šè®­ç»ƒç­–ç•¥` `ç‰©ä½“æ£€æµ‹é²æ£’æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé®æŒ¡å¯¼è‡´ç‰©ä½“è¯†åˆ«ç®—æ³•æ€§èƒ½ä¸‹é™ï¼Œéœ€é¢å¤–ä¿¡æ¯å¢žå¼ºé²æ£’æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽåœºæ™¯èƒŒæ™¯é€‰æ‹©å®šåˆ¶ç½‘ç»œï¼Œå¹¶åœ¨æ£€æµ‹åŽèžåˆåœºæ™¯çŸ¥è¯†ä¼˜åŒ–å¾—åˆ†ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨é®æŒ¡æ•°æ®é›†ä¸Šï¼Œå¬å›žçŽ‡å’Œç²¾ç¡®åº¦å‡ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The presence of occlusions has provided substantial challenges to
> typically-powerful object recognition algorithms. Additional sources of
> information can be extremely valuable to reduce errors caused by occlusions.
> Scene context is known to aid in object recognition in biological vision. In
> this work, we attempt to add robustness into existing Region Proposal
> Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks
> through two distinct scene-based information fusion techniques. We present one
> algorithm under each methodology: the first operates prior to prediction,
> selecting a custom object network to use based on the identified background
> scene, and the second operates after detection, fusing scene knowledge into
> initial object scores output by the RPN. We demonstrate our algorithms on
> challenging datasets featuring partial occlusions, which show overall
> improvement in both recall and precision against baseline methods. In addition,
> our experiments contrast multiple training methodologies for occlusion
> handling, finding that training on a combination of both occluded and
> unoccluded images demonstrates an improvement over the others. Our method is
> interpretable and can easily be adapted to other datasets, offering many future
> directions for research and practical applications.

