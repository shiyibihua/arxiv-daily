---
layout: default
title: OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation
---

# OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation

**arXiv**: [2510.26213v1](https://arxiv.org/abs/2510.26213) | [PDF](https://arxiv.org/pdf/2510.26213.pdf)

**ä½œè€…**: Hengrui Kang, Zhuangcheng Gu, Zhiyuan Zhao, Zichen Wen, Bin Wang, Weijia Li, Conghui He

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniLayout-LLMä¸æ•°æ®é›†ï¼Œé€šè¿‡ç²—åˆ°ç»†å­¦ä¹ è§£å†³é€šç”¨æ–‡æ¡£å¸ƒå±€ç”Ÿæˆé—®é¢˜**

**å…³é”®è¯**: `æ–‡æ¡£å¸ƒå±€ç”Ÿæˆ` `ç²—åˆ°ç»†å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®é›†æ„å»º` `å¤šé¢†åŸŸè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æ¡£å¸ƒå±€ç”Ÿæˆé¢†åŸŸç¼ºä¹å¤šæ ·å¸ƒå±€æ•°æ®ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚å’Œé•¿åºåˆ—å¸ƒå±€
2. æ–¹æ³•è¦ç‚¹ï¼šæ„å»ºç™¾ä¸‡çº§æ•°æ®é›†OmniLayout-1Mï¼Œå¹¶è®¾è®¡ä¸¤é˜¶æ®µç²—åˆ°ç»†å­¦ä¹ èŒƒå¼è®­ç»ƒ0.5Bæ¨¡å‹
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨M$^{6}$Docæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰å¸ƒå±€ç”Ÿæˆä¸“å®¶å’Œé€šç”¨å¤§è¯­è¨€æ¨¡å‹

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Document AI has advanced rapidly and is attracting increasing attention. Yet,
> while most efforts have focused on document layout analysis (DLA), its
> generative counterpart, document layout generation, remains underexplored. A
> major obstacle lies in the scarcity of diverse layouts: academic papers with
> Manhattan-style structures dominate existing studies, while open-world genres
> such as newspapers and magazines remain severely underrepresented. To address
> this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
> document layouts, covering six common document types and comprising
> contemporary layouts collected from multiple sources. Moreover, since existing
> methods struggle in complex domains and often fail to arrange long sequences
> coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
> Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
> OmniLayout-1M with coarse category definitions, and 2) transferring the
> knowledge to a specific domain with fine-grained annotations. Extensive
> experiments demonstrate that our approach achieves strong performance on
> multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
> layout generation experts and several latest general-purpose LLMs. Our code,
> models, and dataset will be publicly released.

