---
layout: default
title: Emu3.5: Native Multimodal Models are World Learners
---

# Emu3.5: Native Multimodal Models are World Learners

**arXiv**: [2510.26583v1](https://arxiv.org/abs/2510.26583) | [PDF](https://arxiv.org/pdf/2510.26583.pdf)

**ä½œè€…**: Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEmu3.5åŽŸç”Ÿå¤šæ¨¡æ€ä¸–ç•Œæ¨¡åž‹ï¼Œç”¨äºŽè·¨è§†è§‰è¯­è¨€çš„çŠ¶æ€é¢„æµ‹ä¸Žç”Ÿæˆã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€ä¸–ç•Œæ¨¡åž‹` `è§†è§‰è¯­è¨€äº¤ç»‡ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒ` `æŽ¨ç†åŠ é€Ÿ` `å¼€æºæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæž„å»ºèƒ½åŽŸç”Ÿå¤„ç†è§†è§‰è¯­è¨€äº¤ç»‡è¾“å…¥è¾“å‡ºçš„å¤šæ¨¡æ€ä¸–ç•Œæ¨¡åž‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç«¯åˆ°ç«¯é¢„è®­ç»ƒä¸Žå¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒï¼Œç»“åˆDiDAæå‡æŽ¨ç†æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­åª²ç¾ŽGemini 2.5 Flashï¼Œå¼€æºæ”¯æŒç ”ç©¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce Emu3.5, a large-scale multimodal world model that natively
> predicts the next state across vision and language. Emu3.5 is pre-trained
> end-to-end with a unified next-token prediction objective on a corpus of
> vision-language interleaved data containing over 10 trillion tokens, primarily
> derived from sequential frames and transcripts of internet videos. The model
> naturally accepts interleaved vision-language inputs and generates interleaved
> vision-language outputs. Emu3.5 is further post-trained with large-scale
> reinforcement learning to enhance multimodal reasoning and generation. To
> improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
> which converts token-by-token decoding into bidirectional parallel prediction,
> accelerating per-image inference by about 20x without sacrificing performance.
> Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
> vision-language generation, any-to-image (X2I) generation, and complex
> text-rich image generation. It also exhibits generalizable world-modeling
> abilities, enabling spatiotemporally consistent world exploration and
> open-world embodied manipulation across diverse scenarios and tasks. For
> comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
> (Nano Banana) on image generation and editing tasks and demonstrates superior
> results on a suite of interleaved generation tasks. We open-source Emu3.5 at
> https://github.com/baaivision/Emu3.5 to support community research.

