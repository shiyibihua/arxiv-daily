---
layout: default
title: Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models
---

# Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models

**arXiv**: [2510.26241v1](https://arxiv.org/abs/2510.26241) | [PDF](https://arxiv.org/pdf/2510.26241.pdf)

**ä½œè€…**: Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAoT-PsyPhyBENCHåŸºå‡†ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹çš„æ—¶é—´æµå‘åˆ¤æ–­èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ—¶é—´æµå‘åˆ¤æ–­` `å¿ƒç†ç‰©ç†å­¦åŸºå‡†` `è§†é¢‘ç†è§£` `å› æžœæŽ¨ç†` `æ¨¡åž‹è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘æ—¶é—´ä¿¡æ¯ç†è§£ä¸Šå­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œç¼ºä¹ç³»ç»Ÿè¯„ä¼°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¿ƒç†ç‰©ç†å­¦æž„å»ºåŸºå‡†ï¼Œæµ‹è¯•æ¨¡åž‹åˆ¤æ–­è§†é¢‘æ­£å‘æˆ–åå‘æ’­æ”¾çš„èƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šæ•°æ¨¡åž‹è¡¨çŽ°æŽ¥è¿‘éšæœºï¼Œæœ€ä¼˜æ¨¡åž‹è¿œä½ŽäºŽäººç±»åœ¨ç‰©ç†ä¸å¯é€†å’Œå› æžœåŠ¨ä½œä»»åŠ¡ä¸Šçš„å‡†ç¡®çŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern vision-language models (VLMs) excel at many multimodal tasks, yet
> their grasp of temporal information in video remains weak and, crucially,
> under-evaluated. We probe this gap with a deceptively simple but revealing
> challenge: judging the arrow of time (AoT)-whether a short clip is played
> forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
> benchmark that tests whether VLMs can infer temporal direction in natural
> videos using the same stimuli and behavioral baselines established for humans.
> Our comprehensive evaluation of open-weight and proprietary, reasoning and
> non-reasoning VLMs reveals that most models perform near chance, and even the
> best lag far behind human accuracy on physically irreversible processes (e.g.,
> free fall, diffusion/explosion) and causal manual actions (division/addition)
> that humans recognize almost instantly. These results highlight a fundamental
> gap in current multimodal systems: while they capture rich visual-semantic
> correlations, they lack the inductive biases required for temporal continuity
> and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
> encourage further progress in the physical and temporal reasoning capabilities
> of VLMs.

