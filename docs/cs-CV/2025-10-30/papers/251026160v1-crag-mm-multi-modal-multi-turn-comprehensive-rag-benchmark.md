---
layout: default
title: CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark
---

# CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark

**arXiv**: [2510.26160v1](https://arxiv.org/abs/2510.26160) | [PDF](https://arxiv.org/pdf/2510.26160.pdf)

**ä½œè€…**: Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil Iyer, Haidar Khan, Lingkun Kong, Roy Luo, Tiffany Ma, Zhen Qiao, David Tran, Wenfang Xu, Skyler Yeatman, Chen Zhou, Gunveer Gujral, Yinglong Xia, Shane Moon, Nicolas Scheffer, Nirav Shah, Eun Chang, Yue Liu, Florian Metze, Tammy Stark, Zhaleh Feizollahi, Andrea Jessee, Mangesh Pujari, Ahmed Aly, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Wen-tau Yih, Xin Luna Dong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCRAG-MMåŸºå‡†ä»¥è¯„ä¼°å¯ç©¿æˆ´åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€å¤šè½®RAGç³»ç»Ÿ**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `å¯ç©¿æˆ´è®¾å¤‡åŸºå‡†` `å¤šè½®å¯¹è¯è¯„ä¼°` `å›¾åƒçŸ¥è¯†å›¾è°±æ£€ç´¢` `çœŸå®žä¸–ç•Œåœºæ™¯æ¨¡æ‹Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹é’ˆå¯¹å¯ç©¿æˆ´è®¾å¤‡çš„å¤šæ¨¡æ€å¤šè½®æ£€ç´¢å¢žå¼ºç”Ÿæˆç»¼åˆåŸºå‡†
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«6.5Kä¸‰å…ƒç»„å’Œ2Kå¤šè½®å¯¹è¯çš„æ•°æ®é›†ï¼Œè¦†ç›–13ä¸ªé¢†åŸŸ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºçº¿æ–¹æ³•çœŸå®žåº¦ä»…32-43%ï¼Œæ˜¾ç¤ºæ”¹è¿›ç©ºé—´ï¼Œå·²ç”¨äºŽKDD Cup 2025

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Wearable devices such as smart glasses are transforming the way people
> interact with their surroundings, enabling users to seek information regarding
> entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
> plays a key role in supporting such questions, yet there is still no
> comprehensive benchmark for this task, especially regarding wearables
> scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
> benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
> set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
> conversations across 13 domains, including 6.2K egocentric images designed to
> mimic captures from wearable devices. We carefully constructed the questions to
> reflect real-world scenarios and challenges, including five types of
> image-quality issues, six question types, varying entity popularity, differing
> information dynamism, and different conversation turns. We design three tasks:
> single-source augmentation, multi-source augmentation, and multi-turn
> conversations -- each paired with an associated retrieval corpus and APIs for
> both image-KG retrieval and webpage retrieval. Our evaluation shows that
> straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
> single- and multi-turn QA, respectively, whereas state-of-the-art industry
> solutions have similar quality (32%/45%), underscoring ample room for
> improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
> participants and 5K submissions, with winning solutions improving baseline
> performance by 28%, highlighting its early impact on advancing the field.

