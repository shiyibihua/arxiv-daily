---
layout: default
title: All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles
---

# All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles

**arXiv**: [2510.26641v1](https://arxiv.org/abs/2510.26641) | [PDF](https://arxiv.org/pdf/2510.26641.pdf)

**ä½œè€…**: Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Ahmad Sarlak, Mahlagha Fazeli, Abolfazl Razi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°è‡ªåŠ¨é©¾é©¶ä¸­åŸºäºŽå¤šæ¨¡æ€èžåˆå’ŒLLM/VLMçš„ç‰©ä½“æ£€æµ‹æ–¹æ³•ï¼Œä»¥åº”å¯¹å¤æ‚çŽ¯å¢ƒæŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶ç‰©ä½“æ£€æµ‹` `å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èžåˆ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `transformeræ–¹æ³•` `æ•°æ®é›†åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨é©¾é©¶ç‰©ä½“æ£€æµ‹åœ¨å¤æ‚å¤šæ¨¡æ€çŽ¯å¢ƒä¸­çŸ¥è¯†åˆ†æ•£ï¼Œå¯é æ€§ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç³»ç»Ÿåˆ†æžä¼ æ„Ÿå™¨èžåˆã€æ•°æ®é›†åˆ†ç±»åŠæ–°å…´transformeré©±åŠ¨æ£€æµ‹æ–¹æ³•ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥å…·ä½“å®žéªŒï¼Œä½†æä¾›å½“å‰èƒ½åŠ›ã€æŒ‘æˆ˜å’Œæœªæ¥æœºä¼šçš„è·¯çº¿å›¾ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous Vehicles (AVs) are transforming the future of transportation
> through advances in intelligent perception, decision-making, and control
> systems. However, their success is tied to one core capability, reliable object
> detection in complex and multimodal environments. While recent breakthroughs in
> Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
> progress, the field still faces a critical challenge as knowledge remains
> fragmented across multimodal perception, contextual reasoning, and cooperative
> intelligence. This survey bridges that gap by delivering a forward-looking
> analysis of object detection in AVs, emphasizing emerging paradigms such as
> Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
> rather than re-examining outdated techniques. We begin by systematically
> reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
> and Radar) and their fusion strategies, highlighting not only their
> capabilities and limitations in dynamic driving environments but also their
> potential to integrate with recent advances in LLM/VLM-driven perception
> frameworks. Next, we introduce a structured categorization of AV datasets that
> moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
> and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
> cross-analysis of data structures and characteristics. Ultimately, we analyze
> cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
> hybrid sensor fusion, with particular attention to emerging transformer-driven
> approaches powered by Vision Transformers (ViTs), Large and Small Language
> Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
> delivers a clear roadmap of current capabilities, open challenges, and future
> opportunities.

