---
layout: default
title: MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction
---

# MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction

**arXiv**: [2510.26151v1](https://arxiv.org/abs/2510.26151) | [PDF](https://arxiv.org/pdf/2510.26151.pdf)

**ä½œè€…**: Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè§†å›¾ä¹³è…ºXå…‰ä¸Žè¯­è¨€æ¨¡åž‹ä»¥æå‡ä¹³è…ºç™Œè¯Šæ–­å’Œé£Žé™©é¢„æµ‹çš„æ•°æ®æ•ˆçŽ‡**

**å…³é”®è¯**: `å¤šè§†å›¾ä¹³è…ºXå…‰` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä¹³è…ºç™Œåˆ†ç±»` `é£Žé™©é¢„æµ‹` `è·¨æ¨¡æ€å­¦ä¹ ` `æ•°æ®æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šèŽ·å–ç²¾ç»†æ ‡æ³¨çš„ä¹³è…ºXå…‰æ•°æ®é›†æˆæœ¬é«˜ä¸”è€—æ—¶ï¼Œé™åˆ¶CADæ¨¡åž‹è®­ç»ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤šè§†å›¾å›¾åƒä¸ŽåˆæˆæŠ¥å‘Šè¿›è¡Œè·¨æ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ï¼Œå¢žå¼ºæ³›åŒ–èƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç§æœ‰å’Œå…¬å…±æ•°æ®é›†ä¸Šå®žçŽ°SOTAæ€§èƒ½ï¼Œåˆ†ç±»ä»»åŠ¡ä¸­æ•°æ®æ•ˆçŽ‡ä¼˜äºŽåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large annotated datasets are essential for training robust Computer-Aided
> Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
> acquiring such datasets with fine-detailed annotation is both costly and
> time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
> pre-trained on large image-text pairs, offer a promising solution by enhancing
> robustness and data efficiency in medical imaging tasks. This paper introduces
> a novel Multi-View Mammography and Language Model for breast cancer
> classification and risk prediction, trained on a dataset of paired mammogram
> images and synthetic radiology reports. Our MV-MLM leverages multi-view
> supervision to learn rich representations from extensive radiology data by
> employing cross-modal self-supervision across image-text pairs. This includes
> multiple views and the corresponding pseudo-radiology reports. We propose a
> novel joint visual-textual learning strategy to enhance generalization and
> accuracy performance over different data types and tasks to distinguish breast
> tissues or cancer characteristics(calcification, mass) and utilize these
> patterns to understand mammography images and predict cancer risk. We evaluated
> our method on both private and publicly available datasets, demonstrating that
> the proposed model achieves state-of-the-art performance in three
> classification tasks: (1) malignancy classification, (2) subtype
> classification, and (3) image-based cancer risk prediction. Furthermore, the
> model exhibits strong data efficiency, outperforming existing fully supervised
> or VLM baselines while trained on synthetic text reports and without the need
> for actual radiology reports.

