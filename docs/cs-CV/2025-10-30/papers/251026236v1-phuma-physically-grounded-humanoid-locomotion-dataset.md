---
layout: default
title: PHUMA: Physically-Grounded Humanoid Locomotion Dataset
---

# PHUMA: Physically-Grounded Humanoid Locomotion Dataset

**arXiv**: [2510.26236v1](https://arxiv.org/abs/2510.26236) | [PDF](https://arxiv.org/pdf/2510.26236.pdf)

**ä½œè€…**: Kyungmin Lee, Sibeen Kim, Minho Park, Hyunseung Kim, Dongyoon Hwang, Hojoon Lee, Jaegul Choo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPHUMAæ•°æ®é›†ä»¥è§£å†³äººå½¢æœºå™¨äººè¿åŠ¨æ¨¡ä»¿ä¸­çš„ç‰©ç†ä¼ªå½±é—®é¢˜**

**å…³é”®è¯**: `äººå½¢æœºå™¨äººè¿åŠ¨æ¨¡ä»¿` `ç‰©ç†çº¦æŸé‡å®šå‘` `å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†` `è¿åŠ¨ä¼ªå½±æ¶ˆé™¤` `è·¯å¾„è·ŸéšæŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è¿åŠ¨æ•æ‰æ•°æ®é›†ç¨€ç¼ºæ˜‚è´µï¼Œç½‘ç»œè§†é¢‘è½¬æ¢å¼•å…¥ç‰©ç†ä¼ªå½±å¦‚æ¼‚æµ®å’Œè„šæ»‘
2. PHUMAé€šè¿‡å¤§è§„æ¨¡äººç±»è§†é¢‘ã€æ•°æ®ç­›é€‰å’Œç‰©ç†çº¦æŸé‡å®šå‘æ¶ˆé™¤ä¼ªå½±
3. åœ¨æœªè§è¿åŠ¨æ¨¡ä»¿å’Œè·¯å¾„è·Ÿéšå®žéªŒä¸­ï¼ŒPHUMAè®­ç»ƒç­–ç•¥ä¼˜äºŽHumanoid-Xå’ŒAMASS

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Motion imitation is a promising approach for humanoid locomotion, enabling
> agents to acquire humanlike behaviors. Existing methods typically rely on
> high-quality motion capture datasets such as AMASS, but these are scarce and
> expensive, limiting scalability and diversity. Recent studies attempt to scale
> data collection by converting large-scale internet videos, exemplified by
> Humanoid-X. However, they often introduce physical artifacts such as floating,
> penetration, and foot skating, which hinder stable imitation. In response, we
> introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that
> leverages human video at scale, while addressing physical artifacts through
> careful data curation and physics-constrained retargeting. PHUMA enforces joint
> limits, ensures ground contact, and eliminates foot skating, producing motions
> that are both large-scale and physically reliable. We evaluated PHUMA in two
> sets of conditions: (i) imitation of unseen motion from self-recorded test
> videos and (ii) path following with pelvis-only guidance. In both cases,
> PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant
> gains in imitating diverse motions. The code is available at
> https://davian-robotics.github.io/PHUMA.

