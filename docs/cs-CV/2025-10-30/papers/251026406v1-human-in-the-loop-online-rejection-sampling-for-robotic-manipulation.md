---
layout: default
title: Human-in-the-loop Online Rejection Sampling for Robotic Manipulation
---

# Human-in-the-loop Online Rejection Sampling for Robotic Manipulation

**arXiv**: [2510.26406v1](https://arxiv.org/abs/2510.26406) | [PDF](https://arxiv.org/pdf/2510.26406.pdf)

**ä½œè€…**: Guanxing Lu, Rui Zhao, Haitao Lin, He Zhang, Yansong Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHi-ORSæ–¹æ³•ï¼Œé€šè¿‡æ‹’ç»é‡‡æ ·å’Œäººæœºäº¤äº’æå‡æœºå™¨äººæ“ä½œç­–ç•¥çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ‹’ç»é‡‡æ ·` `äººæœºäº¤äº’` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹æ—¶ï¼Œä»·å€¼ä¼°è®¡ä¸å‡†ç¡®å’Œä¸­é—´æ­¥éª¤ç›‘ç£ç¨€ç–å¯¼è‡´ä¸ç¨³å®šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ‹’ç»é‡‡æ ·è¿‡æ»¤è´Ÿå¥–åŠ±æ ·æœ¬ï¼Œç»“åˆå¥–åŠ±åŠ æƒç›‘ç£è®­ç»ƒæä¾›å¯†é›†ä¸­é—´ç›‘ç£ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žä¸–ç•Œä»»åŠ¡ä¸­ï¼Œ1.5å°æ—¶è®­ç»ƒå³è¶…è¶ŠåŸºçº¿ï¼Œå®žçŽ°é«˜æ•ˆé”™è¯¯æ¢å¤è¡Œä¸ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement learning (RL) is widely used to produce robust robotic
> manipulation policies, but fine-tuning vision-language-action (VLA) models with
> RL can be unstable due to inaccurate value estimates and sparse supervision at
> intermediate steps. In contrast, imitation learning (IL) is easy to train but
> often underperforms due to its offline nature. In this paper, we propose
> Hi-ORS, a simple yet effective post-training method that utilizes rejection
> sampling to achieve both training stability and high robustness. Hi-ORS
> stabilizes value estimation by filtering out negatively rewarded samples during
> online fine-tuning, and adopts a reward-weighted supervised training objective
> to provide dense intermediate-step supervision. For systematic study, we
> develop an asynchronous inference-training framework that supports flexible
> online human-in-the-loop corrections, which serve as explicit guidance for
> learning error-recovery behaviors. Across three real-world tasks and two
> embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich
> manipulation in just 1.5 hours of real-world training, outperforming RL and IL
> baselines by a substantial margin in both effectiveness and efficiency.
> Notably, the fine-tuned policy exhibits strong test-time scalability by
> reliably executing complex error-recovery behaviors to achieve better
> performance.

