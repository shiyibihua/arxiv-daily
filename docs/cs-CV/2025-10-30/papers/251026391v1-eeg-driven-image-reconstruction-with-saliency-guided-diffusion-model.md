---
layout: default
title: EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models
---

# EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models

**arXiv**: [2510.26391v1](https://arxiv.org/abs/2510.26391) | [PDF](https://arxiv.org/pdf/2510.26391.pdf)

**ä½œè€…**: Igor Abramov, Ilya Makarov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒæ¡ä»¶æ¡†æž¶ç»“åˆEEGåµŒå…¥ä¸Žç©ºé—´æ˜¾è‘—å›¾ï¼Œä»¥æå‡EEGé©±åŠ¨å›¾åƒé‡å»ºçš„ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚**

**å…³é”®è¯**: `EEGå›¾åƒé‡å»º` `æ‰©æ•£æ¨¡åž‹` `ç©ºé—´æ˜¾è‘—å›¾` `ç¥žç»è§£ç ` `ä½Žç§©é€‚åº”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰EEGé©±åŠ¨å›¾åƒé‡å»ºæ–¹æ³•å¿½è§†ç©ºé—´æ³¨æ„æœºåˆ¶ï¼Œå¯¼è‡´ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§å—é™ã€‚
2. é‡‡ç”¨ATMæå–EEGç‰¹å¾ï¼ŒLoRAå¾®è°ƒStable Diffusion 2.1ï¼Œå¹¶é›†æˆControlNetåˆ†æ”¯è¿›è¡Œç©ºé—´æŽ§åˆ¶ã€‚
3. åœ¨THINGS-EEGæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾è‘—æ”¹å–„å›¾åƒç‰¹å¾è´¨é‡ï¼Œå¹¶ä¸Žäººç±»è§†è§‰æ³¨æ„é«˜åº¦å¯¹é½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing EEG-driven image reconstruction methods often overlook spatial
> attention mechanisms, limiting fidelity and semantic coherence. To address
> this, we propose a dual-conditioning framework that combines EEG embeddings
> with spatial saliency maps to enhance image generation. Our approach leverages
> the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
> Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
> with visual semantics, while a ControlNet branch conditions generation on
> saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
> a significant improvement in the quality of low- and high-level image features
> over existing approaches. Simultaneously, strongly aligning with human visual
> attention. The results demonstrate that attentional priors resolve EEG
> ambiguities, enabling high-fidelity reconstructions with applications in
> medical diagnostics and neuroadaptive interfaces, advancing neural decoding
> through efficient adaptation of pre-trained diffusion models.

