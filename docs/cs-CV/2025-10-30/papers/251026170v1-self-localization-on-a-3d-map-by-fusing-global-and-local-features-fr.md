---
layout: default
title: Self-localization on a 3D map by fusing global and local features from a monocular camera
---

# Self-localization on a 3D map by fusing global and local features from a monocular camera

**arXiv**: [2510.26170v1](https://arxiv.org/abs/2510.26170) | [PDF](https://arxiv.org/pdf/2510.26170.pdf)

**ä½œè€…**: Satoshi Kikuch, Masaya Kato, Tsuyoshi Tasaki

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèžåˆCNNä¸ŽVision Transformerçš„æ–¹æ³•ï¼Œä»¥è§£å†³å•ç›®ç›¸æœºåœ¨åŠ¨æ€éšœç¢ç‰©ä¸‹è‡ªå®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `è‡ªå®šä½` `å•ç›®ç›¸æœº` `CNN` `Vision Transformer` `åŠ¨æ€éšœç¢ç‰©` `3Dåœ°å›¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•ç›®ç›¸æœºè‡ªå®šä½åœ¨åŠ¨æ€éšœç¢ç‰©å­˜åœ¨æ—¶ï¼ŒCNNæå–å±€éƒ¨ç‰¹å¾æ•ˆæžœä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆCNNæå–å±€éƒ¨ç‰¹å¾ä¸ŽVision Transformeræå–å…¨å±€ç‰¹å¾ï¼Œæå‡é²æ£’æ€§ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨CGæ•°æ®é›†ä¸Šï¼ŒåŠ¨æ€éšœç¢ç‰©ä¸‹ç²¾åº¦æå‡çŽ‡æ˜¯æ— éšœç¢ç‰©çš„1.5å€ï¼›å…¬å…±æ•°æ®é›†ä¸Šè¯¯å·®æ¯”SOTAå‡å°‘20.1%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Self-localization on a 3D map by using an inexpensive monocular camera is
> required to realize autonomous driving. Self-localization based on a camera
> often uses a convolutional neural network (CNN) that can extract local features
> that are calculated by nearby pixels. However, when dynamic obstacles, such as
> people, are present, CNN does not work well. This study proposes a new method
> combining CNN with Vision Transformer, which excels at extracting global
> features that show the relationship of patches on whole image. Experimental
> results showed that, compared to the state-of-the-art method (SOTA), the
> accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times
> higher than that without dynamic obstacles. Moreover, the self-localization
> error of our method is 20.1% smaller than that of SOTA on public datasets.
> Additionally, our robot using our method can localize itself with 7.51cm error
> on average, which is more accurate than SOTA.

