---
layout: default
title: GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?
---

# GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?

**arXiv**: [2510.26339v1](https://arxiv.org/abs/2510.26339) | [PDF](https://arxiv.org/pdf/2510.26339.pdf)

**ä½œè€…**: Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGLYPH-SRï¼Œé€šè¿‡VLMå¼•å¯¼çš„æ½œåœ¨æ‰©æ•£æ¨¡åž‹å®žçŽ°é«˜è´¨é‡å›¾åƒè¶…åˆ†ä¸Žé«˜ä¿çœŸæ–‡æœ¬æ¢å¤ã€‚**

**å…³é”®è¯**: `å›¾åƒè¶…åˆ†è¾¨çŽ‡` `åœºæ™¯æ–‡æœ¬æ¢å¤` `æ½œåœ¨æ‰©æ•£æ¨¡åž‹` `è§†è§‰è¯­è¨€æ¨¡åž‹` `OCRå¼•å¯¼` `èžåˆæŽ§åˆ¶ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è¶…åˆ†æ–¹æ³•å¯¹åœºæ™¯æ–‡æœ¬æ¢å¤ä¸æ•æ„Ÿï¼Œå¯¼è‡´OCRå¤±è´¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ–‡æœ¬-åœºæ™¯èžåˆControlNetå’Œä¹’ä¹“è°ƒåº¦å™¨ï¼Œç»“åˆOCRæ•°æ®å¼•å¯¼ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡OCR F1åˆ†æ•°ï¼ŒåŒæ—¶ä¿æŒæ„ŸçŸ¥è´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image super-resolution(SR) is fundamental to many vision system-from
> surveillance and autonomy to document analysis and retail analytics-because
> recovering high-frequency details, especially scene-text, enables reliable
> downstream perception. Scene-text, i.e., text embedded in natural images such
> as signs, product labels, and storefronts, often carries the most actionable
> information; when characters are blurred or hallucinated, optical character
> recognition(OCR) and subsequent decisions fail even if the rest of the image
> appears sharp. Yet previous SR research has often been tuned to distortion
> (PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
> are largely insensitive to character-level errors. Furthermore, studies that do
> address text SR often focus on simplified benchmarks with isolated characters,
> overlooking the challenges of text within complex natural scenes. As a result,
> scene-text is effectively treated as generic texture. For SR to be effective in
> practical deployments, it is therefore essential to explicitly optimize for
> both text legibility and perceptual quality. We present GLYPH-SR, a
> vision-language-guided diffusion framework that aims to achieve both objectives
> jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
> OCR data, and a ping-pong scheduler that alternates between text- and
> scene-centric guidance. To enable targeted text restoration, we train these
> components on a synthetic corpus while keeping the main SR branch frozen.
> Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
> up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
> while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
> to satisfy both objectives simultaneously-high readability and high visual
> realism-delivering SR that looks right and reds right.

