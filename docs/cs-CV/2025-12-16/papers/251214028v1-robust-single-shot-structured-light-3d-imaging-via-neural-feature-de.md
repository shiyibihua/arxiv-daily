---
layout: default
title: Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding
---

# Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.14028" target="_blank" class="toolbar-btn">arXiv: 2512.14028v1</a>
    <a href="https://arxiv.org/pdf/2512.14028.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14028v1" 
            onclick="toggleFavorite(this, '2512.14028v1', 'Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiaheng Li, Qiyu Dai, Lihan Li, Praneeth Chakravarthula, He Sun, Baoquan Chen, Wenzheng Chen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://namisntimpot.github.io/NSLweb/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÁ•ûÁªèÁâπÂæÅËß£Á†ÅÁöÑÈ≤ÅÊ£íÂçïÁõÆÁªìÊûÑÂÖâ3DÊàêÂÉèÊñπÊ≥ïÔºåÊèêÂçáÂ§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊ∑±Â∫¶‰º∞ËÆ°Á≤æÂ∫¶„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction & Matching)**

**ÂÖ≥ÈîÆËØç**: `ÁªìÊûÑÂÖâ` `‰∏âÁª¥ÈáçÂª∫` `Ê∑±Â∫¶‰º∞ËÆ°` `Á•ûÁªèÁâπÂæÅ` `ÁâπÂæÅÂåπÈÖç` `ÂçïÁõÆËßÜËßâ` `È≤ÅÊ£íÊÄß` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÁªìÊûÑÂÖâÊñπÊ≥ïÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÔºåÁî±‰∫éÂÉèÁ¥†ÂüüÂåπÈÖçÁöÑÂ±ÄÈôêÊÄßÔºåÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÈ≤ÅÊ£íÊÄßËæÉÂ∑ÆÔºåÂÆπÊòìÂèóÂà∞ÈÅÆÊå°„ÄÅÁªÜËäÇÂíåÊùêË¥®ÁöÑÂΩ±Âìç„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÁ•ûÁªèÁâπÂæÅËß£Á†ÅÁöÑÊ°ÜÊû∂ÔºåÂú®ÁâπÂæÅÁ©∫Èó¥ËøõË°åÂØπÂ∫îÂÖ≥Á≥ªÂåπÈÖçÔºåÂπ∂ÁªìÂêàÂá†‰ΩïÂÖàÈ™åÔºå‰ªéËÄåÊèêÂçáÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. ÈÄöËøáÂêàÊàêÊï∞ÊçÆËÆ≠ÁªÉÔºåËØ•ÊñπÊ≥ïÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ºò‰∫éÂïÜ‰∏öÁªìÊûÑÂÖâÁ≥ªÁªüÂíåË¢´Âä®Á´ã‰ΩìËßÜËßâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫Ü‰ΩøÁî®ÂçïÁõÆÁªìÊûÑÂÖâÁ≥ªÁªüËøõË°å‰∏ªÂä®3DÊàêÂÉèÁöÑÈóÆÈ¢òÔºåËØ•Á≥ªÁªüÂπøÊ≥õÂ∫îÁî®‰∫éÂïÜ‰∏ö3D‰º†ÊÑüËÆæÂ§áÔºåÂ¶ÇApple Face IDÂíåIntel RealSense„ÄÇ‰º†ÁªüÁöÑÁªìÊûÑÂÖâÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂÉèÁ¥†ÂüüÂåπÈÖçÁÆóÊ≥ïËß£Á†ÅÊ∑±Â∫¶ÂØπÂ∫îÂÖ≥Á≥ªÔºåËøôÂØºËá¥Âú®ÈÅÆÊå°„ÄÅÁ≤æÁªÜÁªìÊûÑÁªÜËäÇÂíåÈùûÊúó‰ºØË°®Èù¢Á≠âÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂú∫ÊôØ‰∏ãÈ≤ÅÊ£íÊÄßÊúâÈôê„ÄÇÂèóÁ•ûÁªèÁâπÂæÅÂåπÈÖçÊúÄÊñ∞ËøõÂ±ïÁöÑÂêØÂèëÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ≠¶‰π†ÁöÑÁªìÊûÑÂÖâËß£Á†ÅÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âú®ÁâπÂæÅÁ©∫Èó¥ËÄåÈùûËÑÜÂº±ÁöÑÂÉèÁ¥†Âüü‰∏≠ÊâßË°åÈ≤ÅÊ£íÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂåπÈÖç„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ªéÊäïÂΩ±ÂõæÊ°àÂíåÊçïËé∑ÁöÑÁ∫¢Â§ñÔºàIRÔºâÂõæÂÉè‰∏≠ÊèêÂèñÁ•ûÁªèÁâπÂæÅÔºåÈÄöËøáÂú®ÁâπÂæÅÁ©∫Èó¥‰∏≠ÊûÑÂª∫‰ª£‰ª∑‰ΩìÊù•ÊòæÂºèÂú∞ÁªìÂêàÂÆÉ‰ª¨ÁöÑÂá†‰ΩïÂÖàÈ™åÔºå‰ªéËÄåÂÆûÁé∞ÊØîÂÉèÁ¥†ÂüüËß£Á†ÅÊñπÊ≥ïÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÊ∑±Â∫¶Ë¥®ÈáèÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Ê∑±Â∫¶ÁªÜÂåñÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂà©Áî®Êù•Ëá™Â§ßËßÑÊ®°ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁöÑÂº∫Â§ßÂÖàÈ™åÔºåÊîπÂñÑ‰∫ÜÁ≤æÁªÜÁªÜËäÇÊÅ¢Â§çÂíåÂÖ®Â±ÄÁªìÊûÑ‰∏ÄËá¥ÊÄß„ÄÇ‰∏∫‰∫Ü‰øÉËøõÊúâÊïàÁöÑÂ≠¶‰π†ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÁâ©ÁêÜÁöÑÁªìÊûÑÂÖâÊ∏≤ÊüìÁÆ°Á∫øÔºåÁîüÊàê‰∫ÜËøë‰∏ÄÁôæ‰∏á‰∏™ÂÖ∑Êúâ‰∏çÂêåÂØπË±°ÂíåÊùêÊñôÁöÑÂêàÊàêÂõæÊ°à-ÂõæÂÉèÂØπÔºåÁî®‰∫éÂÆ§ÂÜÖÁéØÂ¢É„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ªÖÂú®ÂÖ∑ÊúâÂ§ö‰∏™ÁªìÊûÑÂÖâÂõæÊ°àÁöÑÂêàÊàêÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂèØ‰ª•ÂæàÂ•ΩÂú∞Êé®ÂπøÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂÆ§ÂÜÖÁéØÂ¢ÉÔºåÊúâÊïàÂú∞Â§ÑÁêÜÂêÑÁßçÂõæÊ°àÁ±ªÂûãËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÔºåÂπ∂‰∏îÂßãÁªà‰ºò‰∫éÂïÜ‰∏öÁªìÊûÑÂÖâÁ≥ªÁªüÂíåÂü∫‰∫éË¢´Âä®Á´ã‰ΩìRGBÁöÑÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂçïÁõÆÁªìÊûÑÂÖâ3DÊàêÂÉèÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÈ≤ÅÊ£íÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñÂÉèÁ¥†ÂüüÁöÑÂåπÈÖçÔºåÂÆπÊòìÂèóÂà∞ÂÖâÁÖßÂèòÂåñ„ÄÅÈÅÆÊå°„ÄÅÈùûÊúó‰ºØË°®Èù¢‰ª•ÂèäÁâ©‰ΩìÁ≤æÁªÜÁªìÊûÑÁöÑÂΩ±ÂìçÔºåÂØºËá¥Ê∑±Â∫¶‰º∞ËÆ°Á≤æÂ∫¶‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂÉèÁ¥†ÂüüÁöÑÂåπÈÖçÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÁâπÂæÅÁ©∫Èó¥ÁöÑÂåπÈÖçÈóÆÈ¢ò„ÄÇÈÄöËøáÊèêÂèñÊäïÂΩ±ÂõæÊ°àÂíåÁ∫¢Â§ñÂõæÂÉèÁöÑÁ•ûÁªèÁâπÂæÅÔºåÂπ∂Âú®ÁâπÂæÅÁ©∫Èó¥ÊûÑÂª∫‰ª£‰ª∑‰ΩìÔºåÂà©Áî®Â≠¶‰π†Âà∞ÁöÑÁâπÂæÅËøõË°åÊõ¥È≤ÅÊ£íÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂåπÈÖç„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®ÂõæÂÉèÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÂíåÂá†‰ΩïÂÖàÈ™åÔºå‰ªéËÄåÊèêÈ´òÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºö‰ΩøÁî®Á•ûÁªèÁΩëÁªú‰ªéÊäïÂΩ±ÂõæÊ°àÂíåÁ∫¢Â§ñÂõæÂÉè‰∏≠ÊèêÂèñÁâπÂæÅ„ÄÇ2) ÁâπÂæÅÂåπÈÖçÊ®°ÂùóÔºöÂú®ÁâπÂæÅÁ©∫Èó¥ÊûÑÂª∫‰ª£‰ª∑‰ΩìÔºåËøõË°åÂØπÂ∫îÂÖ≥Á≥ªÂåπÈÖç„ÄÇ3) Ê∑±Â∫¶ÁªÜÂåñÊ®°ÂùóÔºöÂà©Áî®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜÔºåÂØπÂàùÂßãÊ∑±Â∫¶ÂõæËøõË°åÁªÜÂåñÔºåÊèêÈ´òÁªÜËäÇÊÅ¢Â§çÂíåÂÖ®Â±Ä‰∏ÄËá¥ÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÁªìÊûÑÂÖâËß£Á†ÅÈóÆÈ¢ò‰ªéÂÉèÁ¥†ÂüüËΩ¨Êç¢Âà∞ÁâπÂæÅÂüü„ÄÇÈÄöËøáÂ≠¶‰π†Âà∞ÁöÑÁ•ûÁªèÁâπÂæÅËøõË°åÂåπÈÖçÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Â∫îÂØπÂ§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊåëÊàòÔºåÊòæËëóÊèêÂçáÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºåÂà©Áî®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂÖàÈ™åÁü•ËØÜËøõË°åÊ∑±Â∫¶ÁªÜÂåñ‰πüÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂàõÊñ∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰ΩøÁî®Âü∫‰∫éÁâ©ÁêÜÁöÑÊ∏≤ÊüìÁÆ°Á∫øÁîüÊàêÂ§ßËßÑÊ®°ÂêàÊàêÊï∞ÊçÆÈõÜÔºåÁî®‰∫éËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªú„ÄÇ‰ª£‰ª∑‰ΩìÁöÑÊûÑÂª∫ÊñπÂºè‰ª•ÂèäÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÂØπÊúÄÁªàÁöÑÊÄßËÉΩËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÂÖ∑‰ΩìÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÂèØËÉΩÊ≤°ÊúâËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇÊ∑±Â∫¶ÁªÜÂåñÊ®°ÂùóÂèØËÉΩÈááÁî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÔºåÂπ∂ÂØπÂÖ∂ËøõË°å‰∫ÜÂæÆË∞ÉÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÂêéÔºåËÉΩÂ§üÂæàÂ•ΩÂú∞Ê≥õÂåñÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂÆ§ÂÜÖÁéØÂ¢ÉÔºåÂπ∂‰∏îÊó†ÈúÄÈíàÂØπ‰∏çÂêåÁöÑÁªìÊûÑÂÖâÂõæÊ°àËøõË°åÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê∑±Â∫¶‰º∞ËÆ°Á≤æÂ∫¶‰∏äÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂïÜ‰∏öÁªìÊûÑÂÖâÁ≥ªÁªüÂíåÂü∫‰∫éË¢´Âä®Á´ã‰ΩìRGBÁöÑÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ïÔºå‰ΩÜÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÊú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÈ´òÁ≤æÂ∫¶„ÄÅÈ´òÈ≤ÅÊ£íÊÄß3DÊàêÂÉèÁöÑÈ¢ÜÂüüÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§áÁöÑ3D‰∫∫ËÑ∏ËØÜÂà´„ÄÅÂ¢ûÂº∫Áé∞ÂÆû/ËôöÊãüÁé∞ÂÆûÔºàAR/VRÔºâ„ÄÅÊú∫Âô®‰∫∫ÂØºËà™‰∏éÊäìÂèñ„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊ£ÄÊµãÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáÂú®Â§çÊùÇÂÖâÁÖß„ÄÅÈÅÆÊå°ÂíåÊùêË¥®Êù°‰ª∂‰∏ãÁöÑ3DÈáçÂª∫Ë¥®ÈáèÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂïÜ‰∏öÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.

