---
layout: default
title: LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction
---

# LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14594" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14594v1</a>
  <a href="https://arxiv.org/pdf/2512.14594.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14594v1" onclick="toggleFavorite(this, '2512.14594v1', 'LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenyu Zhao, Yingxue Xu, Fengtao Zhou, Yihui Wang, Hao Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKEMMæ¨¡å‹ï¼Œåˆ©ç”¨LLMå¢å¼ºçŸ¥è¯†çš„å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç™Œç—‡ç”Ÿå­˜é¢„æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å¢å¼º` `è·¨æ¨¡æ€æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥ä»é«˜ç»´å†—ä½™çš„ç—…ç†å›¾åƒå’ŒåŸºå› ç»„æ•°æ®ä¸­æå–æœ‰æ•ˆç‰¹å¾ï¼Œä¸”ç¼ºä¹å……åˆ†çš„ç›‘ç£ä¿¡å·ã€‚
2. KEMMæ¨¡å‹åˆ©ç”¨LLMå¤„ç†ä¸“å®¶æŠ¥å‘Šå’Œç”Ÿæˆé¢„åèƒŒæ™¯çŸ¥è¯†ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç”Ÿå­˜é¢„æµ‹ç›¸å…³ç‰¹å¾çš„å…³æ³¨ã€‚
3. åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒKEMMæ¨¡å‹å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤šæ¨¡æ€ç”Ÿå­˜é¢„æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºç—…ç†å›¾åƒï¼ˆWSIsï¼‰å’ŒåŸºå› ç»„æ•°æ®ï¼Œè¿™äº›æ•°æ®ç»´åº¦é«˜ä¸”å†—ä½™ï¼Œéš¾ä»¥æå–åˆ¤åˆ«æ€§ç‰¹å¾å¹¶å¯¹é½ä¸åŒæ¨¡æ€ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ç®€å•çš„ç”Ÿå­˜éšè®¿æ ‡ç­¾ä¸è¶³ä»¥ç›‘ç£å¦‚æ­¤å¤æ‚çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†KEMMï¼Œä¸€ç§ç”±LLMé©±åŠ¨çš„çŸ¥è¯†å¢å¼ºå¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºç™Œç—‡ç”Ÿå­˜é¢„æµ‹ï¼Œå®ƒé›†æˆäº†ä¸“å®¶æŠ¥å‘Šå’Œé¢„åèƒŒæ™¯çŸ¥è¯†ã€‚1) ä¸“å®¶æŠ¥å‘Šç”±ç—…ç†å­¦å®¶é€ä¸ªæ¡ˆä¾‹æä¾›ï¼Œå¹¶ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç‚¼ï¼Œæä¾›ç®€æ´ä¸”ä¸´åºŠé‡ç‚¹çªå‡ºçš„è¯Šæ–­é™ˆè¿°ã€‚è¿™äº›ä¿¡æ¯é€šå¸¸æš—ç¤ºä¸åŒçš„ç”Ÿå­˜ç»“æœã€‚2) é¢„åèƒŒæ™¯çŸ¥è¯†ï¼ˆPBKï¼‰ç”±LLMç®€æ´åœ°ç”Ÿæˆï¼Œæä¾›å…³äºä¸åŒç™Œç—‡ç±»å‹çš„æœ‰ä»·å€¼çš„é¢„åèƒŒæ™¯çŸ¥è¯†ï¼Œè¿™ä¹Ÿå¢å¼ºäº†ç”Ÿå­˜é¢„æµ‹ã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¥è¯†å¢å¼ºçš„è·¨æ¨¡æ€ï¼ˆKECMï¼‰æ³¨æ„åŠ›æ¨¡å—ã€‚KECMå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼ç½‘ç»œå…³æ³¨æ¥è‡ªé«˜åº¦å†—ä½™æ¨¡æ€çš„åˆ¤åˆ«æ€§å’Œç”Ÿå­˜ç›¸å…³çš„ç‰¹å¾ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKEMMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ä¸­ï¼Œç—…ç†å›¾åƒå’ŒåŸºå› ç»„æ•°æ®ç»´åº¦é«˜ã€å†—ä½™åº¦å¤§ï¼Œéš¾ä»¥æœ‰æ•ˆæå–åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œä»¥åŠç°æœ‰æ–¹æ³•ç¼ºä¹å……åˆ†ç›‘ç£ä¿¡å·çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¯¹é½ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»ä¸“å®¶æŠ¥å‘Šä¸­æå–å…³é”®è¯Šæ–­ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆé¢„åèƒŒæ™¯çŸ¥è¯†ï¼Œä»è€Œä¸ºå¤šæ¨¡æ€æ¨¡å‹æä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ›´å¼ºçš„ç›‘ç£ä¿¡å·ã€‚é€šè¿‡çŸ¥è¯†å¢å¼ºçš„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸ç”Ÿå­˜é¢„æµ‹ç›¸å…³çš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKEMMæ¨¡å‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) LLMé©±åŠ¨çš„çŸ¥è¯†æå–æ¨¡å—ï¼Œç”¨äºå¤„ç†ä¸“å®¶æŠ¥å‘Šå¹¶ç”Ÿæˆé¢„åèƒŒæ™¯çŸ¥è¯†ï¼›2) å¤šæ¨¡æ€ç‰¹å¾æå–æ¨¡å—ï¼Œç”¨äºæå–ç—…ç†å›¾åƒå’ŒåŸºå› ç»„æ•°æ®çš„ç‰¹å¾ï¼›3) çŸ¥è¯†å¢å¼ºçš„è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆKECMï¼‰æ¨¡å—ï¼Œç”¨äºèåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾å’ŒçŸ¥è¯†ï¼›4) ç”Ÿå­˜é¢„æµ‹æ¨¡å—ï¼Œç”¨äºé¢„æµ‹æ‚£è€…çš„ç”Ÿå­˜æ¦‚ç‡ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆåˆ©ç”¨LLMæå–çŸ¥è¯†ï¼Œç„¶åå°†çŸ¥è¯†ä¸å¤šæ¨¡æ€ç‰¹å¾èåˆï¼Œæœ€åè¿›è¡Œç”Ÿå­˜é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†LLMæ¥å¢å¼ºå¤šæ¨¡æ€æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶è®¾è®¡äº†çŸ¥è¯†å¢å¼ºçš„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒKEMMæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸“å®¶çŸ¥è¯†å’Œé¢„åèƒŒæ™¯çŸ¥è¯†ï¼Œä»è€Œæé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚KECMæ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶çªå‡ºä¸ç”Ÿå­˜é¢„æµ‹ç›¸å…³çš„ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šKECMæ¨¡å—çš„è®¾è®¡æ˜¯å…³é”®ã€‚è¯¥æ¨¡å—åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®LLMæå–çš„çŸ¥è¯†ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸åŒæ¨¡æ€ç‰¹å¾çš„æƒé‡ã€‚å…·ä½“çš„æ³¨æ„åŠ›è®¡ç®—æ–¹å¼æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹æ˜¯åŸºäºquery-key-valueçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­queryæ¥è‡ªLLMæå–çš„çŸ¥è¯†ï¼Œkeyå’Œvalueæ¥è‡ªå¤šæ¨¡æ€ç‰¹å¾ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬ç”Ÿå­˜åˆ†æä¸­å¸¸ç”¨çš„C-indexæŸå¤±å’Œå¯èƒ½å­˜åœ¨çš„äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºç›‘ç£æ¨¡å‹çš„è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

KEMMæ¨¡å‹åœ¨äº”ä¸ªç™Œç—‡æ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜é¢„æµ‹æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­å¼ºè°ƒäº†â€œachieves state-of-the-art performanceâ€ï¼Œè¯´æ˜KEMMæ¨¡å‹ç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—çš„æå‡ã€‚å®éªŒç»“æœéªŒè¯äº†åˆ©ç”¨LLMå¢å¼ºçŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¸´åºŠè¾…åŠ©è¯Šæ–­ï¼Œå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹ç™Œç—‡æ‚£è€…çš„ç”Ÿå­˜æ¦‚ç‡ï¼Œä»è€Œåˆ¶å®šæ›´ä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚é€šè¿‡æ•´åˆç—…ç†å›¾åƒã€åŸºå› ç»„æ•°æ®å’Œä¸“å®¶çŸ¥è¯†ï¼ŒKEMMæ¨¡å‹æœ‰æœ›æé«˜ç™Œç—‡æ²»ç–—çš„æœ‰æ•ˆæ€§å’Œæ‚£è€…çš„ç”Ÿå­˜è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç–¾ç—…çš„ç”Ÿå­˜é¢„æµ‹å’Œè¯Šæ–­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.

