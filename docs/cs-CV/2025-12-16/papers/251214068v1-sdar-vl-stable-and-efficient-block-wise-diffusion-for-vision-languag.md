---
layout: default
title: SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding
---

# SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding

**arXiv**: [2512.14068v1](https://arxiv.org/abs/2512.14068) | [PDF](https://arxiv.org/pdf/2512.14068.pdf)

**ä½œè€…**: Shuang Cheng, Yuhua Jiang, Zineng Zhou, Dawei Liu, Wang Tao, Linfeng Zhang, Biqing Qi, Bowen Zhou

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSDAR-VLæ¡†æž¶ï¼Œé€šè¿‡å¼‚æ­¥å—å™ªå£°è°ƒåº¦ç­‰æŠ€æœ¯è§£å†³å—çŠ¶ç¦»æ•£æ‰©æ•£åœ¨è§†è§‰è¯­è¨€ç†è§£ä¸­çš„è®­ç»ƒä¸ç¨³å®šå’Œæ•ˆçŽ‡ä½Žé—®é¢˜ã€‚**

**å…³é”®è¯**: `å—çŠ¶ç¦»æ•£æ‰©æ•£` `è§†è§‰è¯­è¨€ç†è§£` `å¼‚æ­¥å™ªå£°è°ƒåº¦` `æŽ©ç æ¯”çŽ‡ç¼©æ”¾` `å™ªå£°è¯¾ç¨‹å­¦ä¹ ` `å¤šæ¨¡æ€å»ºæ¨¡` `è®­ç»ƒç¨³å®šæ€§` `é«˜æ•ˆè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å—çŠ¶ç¦»æ•£æ‰©æ•£åœ¨è§†è§‰è¯­è¨€ç†è§£ä¸­é¢ä¸´è®­ç»ƒæˆæœ¬é«˜ã€æ”¶æ•›æ…¢å’Œä¸ç¨³å®šæ€§ï¼Œé™åˆ¶äº†å…¶å®žé™…åº”ç”¨ã€‚
2. SDAR-VLé€šè¿‡å¼‚æ­¥å—å™ªå£°è°ƒåº¦ã€æœ‰æ•ˆæŽ©ç æ¯”çŽ‡ç¼©æ”¾å’Œæ¸è¿›å¼Betaå™ªå£°è¯¾ç¨‹ï¼Œå®žçŽ°é«˜æ•ˆç¨³å®šè®­ç»ƒã€‚
3. åœ¨21ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSDAR-VLæå‡äº†è®­ç»ƒæ•ˆçŽ‡ã€æ”¶æ•›ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šå¼ºåŸºçº¿ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å—çŠ¶ç¦»æ•£æ‰©æ•£åœ¨å¹¶è¡Œç”Ÿæˆä¸Žå› æžœä¾èµ–å»ºæ¨¡ä¹‹é—´æä¾›äº†æœ‰å¸å¼•åŠ›çš„å¹³è¡¡ï¼Œä½¿å…¶æˆä¸ºè§†è§‰è¯­è¨€å»ºæ¨¡çš„æœ‰å‰æ™¯çš„éª¨å¹²ã€‚ç„¶è€Œï¼Œå…¶å®žé™…åº”ç”¨å—åˆ°é«˜è®­ç»ƒæˆæœ¬ã€æ”¶æ•›æ…¢å’Œä¸ç¨³å®šæ€§çš„é™åˆ¶ï¼Œè¿„ä»Šä»è½åŽäºŽå¼ºå¤§çš„è‡ªå›žå½’åŸºçº¿ã€‚æˆ‘ä»¬æå‡ºäº†SDAR-VLï¼Œè¿™æ˜¯å—çŠ¶ç¦»æ•£æ‰©æ•£åœ¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€ç†è§£ä¸­çš„é¦–æ¬¡ç³»ç»Ÿæ€§åº”ç”¨ï¼ŒåŒæ—¶æä¾›äº†ä¸€ä¸ªé«˜æ•ˆç¨³å®šè®­ç»ƒçš„ç»¼åˆæ¡†æž¶ã€‚è¯¥æ¡†æž¶ç»Ÿä¸€äº†ä¸‰ä¸ªç»„ä»¶ï¼š(1) å¼‚æ­¥å—çŠ¶å™ªå£°è°ƒåº¦ï¼Œä»¥åœ¨æ¯ä¸ªæ‰¹æ¬¡å†…å¤šæ ·åŒ–ç›‘ç£ï¼›(2) æœ‰æ•ˆæŽ©ç æ¯”çŽ‡ç¼©æ”¾ï¼Œç”¨äºŽåœ¨éšæœºæŽ©ç ä¸‹è¿›è¡Œæ— åæŸå¤±å½’ä¸€åŒ–ï¼›(3) æ¸è¿›å¼Betaå™ªå£°è¯¾ç¨‹ï¼Œå¢žåŠ æœ‰æ•ˆæŽ©ç è¦†ç›–çŽ‡åŒæ—¶ä¿æŒç ´åå¤šæ ·æ€§ã€‚åœ¨21ä¸ªå•å›¾åƒã€å¤šå›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒSDAR-VLåœ¨è®­ç»ƒæ•ˆçŽ‡ã€æ”¶æ•›ç¨³å®šæ€§å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢æŒç»­ä¼˜äºŽä¼ ç»Ÿå—æ‰©æ•£ã€‚åœ¨æ­¤è¯„ä¼°å¥—ä»¶ä¸­ï¼ŒSDAR-VLåœ¨åŸºäºŽæ‰©æ•£çš„è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­è®¾ç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨åŒ¹é…è®¾ç½®ä¸‹ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†å¦‚LLaVA-OneVisionç­‰å¼ºè‡ªå›žå½’åŸºçº¿ä»¥åŠå…¨å±€æ‰©æ•£åŸºçº¿LLaDA-Vï¼Œç¡®ç«‹äº†å—çŠ¶æ‰©æ•£ä½œä¸ºè§†è§‰è¯­è¨€ç†è§£çš„å®žç”¨éª¨å¹²ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

SDAR-VLæ˜¯ä¸€ä¸ªé›†æˆæ¡†æž¶ï¼Œå°†å—çŠ¶ç¦»æ•£æ‰©æ•£åº”ç”¨äºŽå¤§è§„æ¨¡è§†è§‰è¯­è¨€ç†è§£ã€‚æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æŠ€æœ¯åˆ›æ–°ï¼šå¼‚æ­¥å—çŠ¶å™ªå£°è°ƒåº¦ï¼Œé€šè¿‡åœ¨ä¸åŒå—ä¸­åº”ç”¨ä¸åŒå™ªå£°æ°´å¹³æ¥å¤šæ ·åŒ–ç›‘ç£ï¼›æœ‰æ•ˆæŽ©ç æ¯”çŽ‡ç¼©æ”¾ï¼Œç¡®ä¿åœ¨éšæœºæŽ©ç ç­–ç•¥ä¸‹æŸå¤±è®¡ç®—çš„å…¬å¹³æ€§ï¼›æ¸è¿›å¼Betaå™ªå£°è¯¾ç¨‹ï¼Œé€æ­¥å¢žåŠ æŽ©ç è¦†ç›–çŽ‡åŒæ—¶ç»´æŒå™ªå£°å¤šæ ·æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œå®ƒç³»ç»Ÿæ€§åœ°è§£å†³äº†å—æ‰©æ•£çš„è®­ç»ƒä¸ç¨³å®šå’Œæ•ˆçŽ‡é—®é¢˜ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸å¿½è§†è¿™äº›ä¼˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨21ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSDAR-VLåœ¨è®­ç»ƒæ•ˆçŽ‡ã€æ”¶æ•›ç¨³å®šæ€§å’Œä»»åŠ¡æ€§èƒ½ä¸Šä¼˜äºŽä¼ ç»Ÿå—æ‰©æ•£ï¼Œè¾¾åˆ°åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ–°SOTAï¼Œå¹¶åœ¨åŒ¹é…è®¾ç½®ä¸‹åŒ¹é…æˆ–è¶…è¶ŠLLaVA-OneVisionç­‰å¼ºåŸºçº¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ï¼Œå¦‚å•å›¾åƒæè¿°ã€å¤šå›¾åƒæŽ¨ç†å’Œè§†é¢‘ç†è§£ï¼Œæå‡æ¨¡åž‹åœ¨åŒ»ç–—å½±åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸçš„å®žé™…æ€§èƒ½ï¼ŒæŽ¨åŠ¨å¤šæ¨¡æ€AIçš„å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.

