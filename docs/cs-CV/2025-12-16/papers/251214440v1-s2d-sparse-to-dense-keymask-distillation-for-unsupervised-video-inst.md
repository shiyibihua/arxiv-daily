---
layout: default
title: S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation
---

# S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation

**arXiv**: [2512.14440v1](https://arxiv.org/abs/2512.14440) | [PDF](https://arxiv.org/pdf/2512.14440.pdf)

**ä½œè€…**: Leon Sick, Lukas Hoyer, Dominik Engel, Pedro Hermosilla, Timo Ropinski

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS2Dï¼šä¸€ç§ç¨€ç–åˆ°ç¨ å¯†çš„Keymaskè’¸é¦æ–¹æ³•ï¼Œç”¨äºŽæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å››è¶³ç§»åŠ¨ (Quadruped Locomotion)** **ç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `è§†é¢‘å®žä¾‹åˆ†å‰²` `å…³é”®å¸§é€‰æ‹©` `ç¨€ç–åˆ°ç¨ å¯†` `çŸ¥è¯†è’¸é¦` `æ·±åº¦è¿åŠ¨å…ˆéªŒ` `æ—¶é—´ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ–¹æ³•ä¾èµ–åˆæˆæ•°æ®ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®žè§†é¢‘ä¸­çš„å¤æ‚è¿åŠ¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºŽçœŸå®žè§†é¢‘æ•°æ®çš„ç¨€ç–åˆ°ç¨ å¯†Keymaskè’¸é¦æ–¹æ³•ï¼ˆS2Dï¼‰ï¼Œæå‡åˆ†å‰²è´¨é‡ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²é¢†åŸŸçš„æœ€å…ˆè¿›æ–¹æ³•ä¸¥é‡ä¾èµ–äºŽåˆæˆè§†é¢‘æ•°æ®ï¼Œè¿™äº›æ•°æ®é€šå¸¸ç”±ImageNetç­‰ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒæ•°æ®é›†ç”Ÿæˆã€‚ç„¶è€Œï¼Œé€šè¿‡äººä¸ºåœ°å¹³ç§»å’Œç¼©æ”¾å›¾åƒå®žä¾‹æŽ©ç æ¥åˆæˆè§†é¢‘ï¼Œæ— æ³•å‡†ç¡®åœ°æ¨¡æ‹Ÿè§†é¢‘ä¸­çœŸå®žçš„è¿åŠ¨ï¼Œä¾‹å¦‚é€è§†å˜åŒ–ã€å•ä¸ªæˆ–å¤šä¸ªå®žä¾‹çš„éƒ¨åˆ†è¿åŠ¨æˆ–ç›¸æœºè¿åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®Œå…¨åœ¨çœŸå®žè§†é¢‘æ•°æ®ä¸Šè®­ç»ƒçš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ã€‚æˆ‘ä»¬ä»Žå•ä¸ªè§†é¢‘å¸§ä¸Šçš„æ— ç›‘ç£å®žä¾‹åˆ†å‰²æŽ©ç å¼€å§‹ã€‚ç„¶è€Œï¼Œè¿™äº›å•å¸§åˆ†å‰²è¡¨çŽ°å‡ºæ—¶é—´å™ªå£°ï¼Œå¹¶ä¸”å…¶è´¨é‡åœ¨æ•´ä¸ªè§†é¢‘ä¸­å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒæ¥è¯†åˆ«è§†é¢‘ä¸­çš„é«˜è´¨é‡å…³é”®æŽ©ç ï¼Œä»Žè€Œå»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚ç„¶åŽï¼Œç¨€ç–çš„å…³é”®æŽ©ç ä¼ªæ³¨é‡Šç”¨äºŽè®­ç»ƒåˆ†å‰²æ¨¡åž‹ä»¥è¿›è¡Œéšå¼æŽ©ç ä¼ æ’­ï¼Œä¸ºæ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±æ—¶é—´DropLossè¾…åŠ©çš„ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•ã€‚åœ¨ç”±æ­¤äº§ç”Ÿçš„ç¨ å¯†æ ‡ç­¾é›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡åž‹åŽï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ—¨åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¯¹è§†é¢‘ä¸­çš„æ¯ä¸ªå®žä¾‹è¿›è¡Œåˆ†å‰²å’Œè·Ÿè¸ªã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–äºŽåˆæˆæ•°æ®ï¼Œä½†åˆæˆæ•°æ®éš¾ä»¥æ¨¡æ‹ŸçœŸå®žè§†é¢‘ä¸­çš„å¤æ‚è¿åŠ¨ï¼Œå¯¼è‡´æ¨¡åž‹åœ¨çœŸå®žè§†é¢‘ä¸Šçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æ­¤å¤–ï¼Œç›´æŽ¥åœ¨çœŸå®žè§†é¢‘ä¸Šè¿›è¡Œæ— ç›‘ç£åˆ†å‰²ï¼Œå•å¸§åˆ†å‰²ç»“æžœå­˜åœ¨æ—¶é—´å™ªå£°ï¼Œè´¨é‡ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çœŸå®žè§†é¢‘æ•°æ®ï¼Œé€šè¿‡è¯†åˆ«é«˜è´¨é‡çš„å…³é”®å¸§åˆ†å‰²æŽ©ç ï¼ˆKeymasksï¼‰å¹¶å°†å…¶ä¼ æ’­åˆ°æ•´ä¸ªè§†é¢‘åºåˆ—ï¼Œä»Žè€Œå®žçŽ°é«˜è´¨é‡çš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²ã€‚é€šè¿‡ç¨€ç–çš„å…³é”®å¸§æŽ©ç ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒæ¨¡åž‹è¿›è¡Œç¨ å¯†çš„æŽ©ç é¢„æµ‹ï¼Œä»Žè€Œå…‹æœå•å¸§åˆ†å‰²ç»“æžœçš„æ—¶é—´å™ªå£°é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) å•å¸§æ— ç›‘ç£å®žä¾‹åˆ†å‰²ï¼šå¯¹è§†é¢‘çš„æ¯ä¸€å¸§è¿›è¡Œæ— ç›‘ç£å®žä¾‹åˆ†å‰²ï¼Œå¾—åˆ°åˆå§‹çš„åˆ†å‰²æŽ©ç ã€‚2) å…³é”®å¸§é€‰æ‹©ï¼šåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒï¼Œé€‰æ‹©è§†é¢‘ä¸­è´¨é‡è¾ƒé«˜çš„å…³é”®å¸§ï¼Œå¹¶å°†å…¶åˆ†å‰²æŽ©ç ä½œä¸ºå…³é”®æŽ©ç ã€‚3) ç¨€ç–åˆ°ç¨ å¯†è’¸é¦ï¼šåˆ©ç”¨å…³é”®æŽ©ç ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒä¸€ä¸ªåˆ†å‰²æ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿä»Žç¨€ç–çš„å…³é”®æŽ©ç ä¸­é¢„æµ‹å‡ºç¨ å¯†çš„åˆ†å‰²æŽ©ç ã€‚4) æ¨¡åž‹è®­ç»ƒï¼šåœ¨ç”Ÿæˆçš„ç¨ å¯†æ ‡ç­¾é›†ä¸Šè®­ç»ƒæœ€ç»ˆçš„è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†ç¨€ç–åˆ°ç¨ å¯†çš„Keymaskè’¸é¦æ–¹æ³•ï¼ˆS2Dï¼‰ã€‚ä¸Žç›´æŽ¥åœ¨å•å¸§åˆ†å‰²ç»“æžœä¸Šè®­ç»ƒæ¨¡åž‹ä¸åŒï¼ŒS2Dæ–¹æ³•é¦–å…ˆè¯†åˆ«é«˜è´¨é‡çš„å…³é”®æŽ©ç ï¼Œç„¶åŽåˆ©ç”¨è¿™äº›å…³é”®æŽ©ç ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒæ¨¡åž‹è¿›è¡ŒæŽ©ç ä¼ æ’­ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å…‹æœå•å¸§åˆ†å‰²ç»“æžœçš„æ—¶é—´å™ªå£°é—®é¢˜ï¼Œå¹¶æé«˜åˆ†å‰²è´¨é‡ã€‚æ­¤å¤–ï¼ŒTemporal DropLossçš„å¼•å…¥è¿›ä¸€æ­¥å¢žå¼ºäº†æ¨¡åž‹çš„æ—¶é—´ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®å¸§é€‰æ‹©é˜¶æ®µï¼Œè®ºæ–‡åˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒæ¥è¯„ä¼°åˆ†å‰²æŽ©ç çš„è´¨é‡ã€‚åœ¨ç¨€ç–åˆ°ç¨ å¯†è’¸é¦é˜¶æ®µï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªTemporal DropLossï¼Œç”¨äºŽé¼“åŠ±æ¨¡åž‹å­¦ä¹ æ—¶é—´ä¸€è‡´çš„åˆ†å‰²ç»“æžœã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä¾‹å¦‚ä½¿ç”¨äº†ç‰¹å®šçš„å·ç§¯ç¥žç»ç½‘ç»œä½œä¸ºåˆ†å‰²æ¨¡åž‹ï¼Œå¹¶é‡‡ç”¨äº†ç‰¹å®šçš„ä¼˜åŒ–ç®—æ³•å’Œå­¦ä¹ çŽ‡ç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºï¼Œä¾‹å¦‚åœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„åˆ†å‰²ç²¾åº¦æ¯”çŽ°æœ‰æ–¹æ³•æé«˜äº†X%ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å…‹æœå•å¸§åˆ†å‰²ç»“æžœçš„æ—¶é—´å™ªå£°é—®é¢˜ï¼Œå¹¶æé«˜åˆ†å‰²è´¨é‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æŽ§ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºŽè¯†åˆ«å’Œåˆ†å‰²é“è·¯ä¸Šçš„è½¦è¾†ã€è¡Œäººç­‰ç›®æ ‡ã€‚åœ¨è§†é¢‘ç›‘æŽ§ä¸­ï¼Œå¯ä»¥ç”¨äºŽæ£€æµ‹å’Œè·Ÿè¸ªå¼‚å¸¸è¡Œä¸ºã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥ç”¨äºŽè¯†åˆ«å’Œåˆ†å‰²çŽ¯å¢ƒä¸­çš„ç‰©ä½“ï¼Œä»Žè€Œå¸®åŠ©æœºå™¨äººè¿›è¡Œè‡ªä¸»å¯¼èˆªã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®žé™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.

