---
layout: default
title: Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models
---

# Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models

**arXiv**: [2512.14137v1](https://arxiv.org/abs/2512.14137) | [PDF](https://arxiv.org/pdf/2512.14137.pdf)

**ä½œè€…**: Ashish Mishra, Tarun Kumar, Gyanaranjan Nayak, Arpit Shah, Suparna Bhattacharya, Martin Foltin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé›¶ç©ºé—´æŠ•å½±çš„éžç ´åæ€§æ•°æ®æ— å…³é›¶æ ·æœ¬ç±»åˆ«é—å¿˜æ–¹æ³•ï¼Œä»¥è§£å†³CLIPæ¨¡åž‹é€‰æ‹©æ€§é—å¿˜é—®é¢˜ã€‚**

**å…³é”®è¯**: `é€‰æ‹©æ€§é—å¿˜` `é›¶ç©ºé—´æŠ•å½±` `å¤šæ¨¡æ€æ¨¡åž‹` `CLIPæ¨¡åž‹` `é›¶æ ·æœ¬å­¦ä¹ ` `æ¨¡åž‹åŽ»æ±¡` `éšç§ä¿æŠ¤` `é—­å¼æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–è¿­ä»£å¾®è°ƒå’Œå¤§é‡æ•°æ®æ•´ç†ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥ç²¾ç¡®æŽ§åˆ¶é—å¿˜è¿‡ç¨‹ã€‚
2. æå‡ºåŸºäºŽé›¶ç©ºé—´æŠ•å½±çš„é—­å¼æ–¹æ³•ï¼Œé€šè¿‡æ­£äº¤åŸºæŠ•å½±æ“¦é™¤ç›®æ ‡ç±»åˆ«ä¿¡æ¯ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–æ•°æ®ã€‚
3. å®žéªŒæ˜¾ç¤ºç›®æ ‡ç±»åˆ«é›¶æ ·æœ¬æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ŒåŒæ—¶æ•´ä½“å¤šæ¨¡æ€çŸ¥è¯†å¾—ä»¥ä¿ç•™ï¼Œéƒ¨åˆ†æŠ•å½±å®žçŽ°å¹³è¡¡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é—­å¼æ–¹æ³•ï¼Œç”¨äºŽå¤šæ¨¡æ€æ¨¡åž‹ä¸­çš„é€‰æ‹©æ€§é—å¿˜ï¼Œç‰¹åˆ«é’ˆå¯¹å¦‚CLIPè¿™æ ·çš„é¢„è®­ç»ƒæ¨¡åž‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é›¶ç©ºé—´æŠ•å½±æ¥æ“¦é™¤åµŒå…¥åœ¨æœ€ç»ˆæŠ•å½±å±‚ä¸­çš„ç›®æ ‡ç±»åˆ«ä¿¡æ¯ï¼Œæ— éœ€ä»»ä½•é‡æ–°è®­ç»ƒæˆ–ä½¿ç”¨é—å¿˜é›†ä¸­çš„å›¾åƒã€‚é€šè¿‡è®¡ç®—ç›®æ ‡æ–‡æœ¬åµŒå…¥æ‰€å¼ æˆå­ç©ºé—´çš„æ­£äº¤åŸºå¹¶æŠ•å½±è¿™äº›æ–¹å‘ï¼Œæˆ‘ä»¬æ˜¾è‘—é™ä½Žäº†å›¾åƒç‰¹å¾ä¸Žä¸éœ€è¦ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚ä¸Žä¾èµ–è¿­ä»£å¾®è°ƒå’Œå¤§é‡æ•°æ®æ•´ç†çš„ä¼ ç»Ÿé—å¿˜æŠ€æœ¯ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢è®¡ç®—é«˜æ•ˆåˆå…·æœ‰å¤–ç§‘æ‰‹æœ¯èˆ¬çš„ç²¾ç¡®æ€§ã€‚è¿™å¯¼è‡´ç›®æ ‡ç±»åˆ«çš„é›¶æ ·æœ¬æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡åž‹çš„æ•´ä½“å¤šæ¨¡æ€çŸ¥è¯†ã€‚å®žéªŒè¡¨æ˜Žï¼Œå³ä½¿æ˜¯éƒ¨åˆ†æŠ•å½±ä¹Ÿèƒ½åœ¨å®Œå…¨é—å¿˜å’Œä¿ç•™æœ‰ç”¨ä¿¡æ¯ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œè§£å†³äº†æ¨¡åž‹åŽ»æ±¡å’Œéšç§ä¿æŠ¤ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è¯¥æ–¹æ³•æ•´ä½“æ¡†æž¶åŸºäºŽCLIPæ¨¡åž‹çš„æœ€ç»ˆæŠ•å½±å±‚ï¼Œé€šè¿‡è®¡ç®—ç›®æ ‡ç±»åˆ«æ–‡æœ¬åµŒå…¥çš„æ­£äº¤åŸºï¼Œæž„å»ºé›¶ç©ºé—´æŠ•å½±çŸ©é˜µæ¥æ“¦é™¤è¿™äº›æ–¹å‘ä¸Šçš„ä¿¡æ¯ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽåˆ©ç”¨é—­å¼é›¶ç©ºé—´æŠ•å½±å®žçŽ°éžç ´åæ€§ã€æ•°æ®æ— å…³çš„ç±»åˆ«é—å¿˜ï¼Œæ— éœ€å¾®è°ƒæˆ–è®¿é—®é—å¿˜é›†å›¾åƒã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽé¿å…äº†è¿­ä»£ä¼˜åŒ–å’Œæ•°æ®ä¾èµ–ï¼Œæä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆä¸”ç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œç›´æŽ¥æ“ä½œåµŒå…¥ç©ºé—´è€Œéžä¿®æ”¹æ¨¡åž‹å‚æ•°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—é™ä½Žç›®æ ‡ç±»åˆ«çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ¨¡åž‹å¯¹å…¶ä»–ç±»åˆ«çš„è¯†åˆ«èƒ½åŠ›ï¼›éƒ¨åˆ†æŠ•å½±ç­–ç•¥åœ¨å®Œå…¨é—å¿˜å’ŒçŸ¥è¯†ä¿ç•™ä¹‹é—´å–å¾—å¹³è¡¡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æ¨¡åž‹åŽ»æ±¡å’Œéšç§ä¿æŠ¤é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œä¾‹å¦‚åœ¨éœ€è¦ç§»é™¤æ•æ„Ÿæˆ–é”™è¯¯ç±»åˆ«ä¿¡æ¯çš„åœºæ™¯ä¸­ï¼Œå¦‚åŒ»ç–—å›¾åƒåˆ†æžã€å†…å®¹è¿‡æ»¤å’Œåˆè§„æ€§è°ƒæ•´ï¼Œèƒ½é«˜æ•ˆå®žçŽ°é€‰æ‹©æ€§é—å¿˜è€Œä¸æŸå®³æ¨¡åž‹æ•´ä½“æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.

