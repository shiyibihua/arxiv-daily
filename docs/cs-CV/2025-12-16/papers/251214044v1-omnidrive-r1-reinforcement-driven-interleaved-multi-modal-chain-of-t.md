---
layout: default
title: OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving
---

# OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving

**arXiv**: [2512.14044v1](https://arxiv.org/abs/2512.14044) | [PDF](https://arxiv.org/pdf/2512.14044.pdf)

**ä½œè€…**: Zhenguo Zhang, Haohan Zhen, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniDrive-R1æ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–é©±åŠ¨çš„äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯é æ€§é—®é¢˜**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ€ç»´é“¾æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€èžåˆ` `ç«¯åˆ°ç«¯ä¼˜åŒ–` `è§†è§‰æŽ¥åœ°` `è·¨æ¨¡æ€ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å› ç‰©ä½“å¹»è§‰ç­‰å¯é æ€§é—®é¢˜å—é™ï¼ŒæºäºŽæ–‡æœ¬æ€ç»´é“¾æŽ¨ç†æœªæŽ¥åœ°ï¼Œä¸”å¤šæ¨¡æ€æ–¹æ³•å­˜åœ¨æ„ŸçŸ¥ä¸ŽæŽ¨ç†è§£è€¦ã€ä¾èµ–å¯†é›†æ ‡æ³¨çš„ç¼ºé™·ã€‚
2. æå‡ºOmniDrive-R1æ¡†æž¶ï¼Œé€šè¿‡äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾ç»Ÿä¸€æ„ŸçŸ¥ä¸ŽæŽ¨ç†ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ é©±åŠ¨è§†è§‰æŽ¥åœ°ï¼Œå®žçŽ°ç«¯åˆ°ç«¯ä¼˜åŒ–å’Œæ— éœ€æ ‡æ³¨çš„å®žæ—¶è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚
3. åœ¨DriveLMM-o1å®žéªŒä¸­ï¼Œæ¨¡åž‹æ•´ä½“æŽ¨ç†åˆ†æ•°æå‡è‡³80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡è¾¾73.62%ï¼Œæ˜¾è‘—ä¼˜äºŽåŸºçº¿Qwen2.5VL-7Bçš„æ€§èƒ½è¡¨çŽ°ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®é¢†åŸŸéƒ¨ç½²è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰æ—¶ï¼Œå¯é æ€§é—®é¢˜ï¼ˆç‰¹åˆ«æ˜¯ç‰©ä½“å¹»è§‰ï¼‰ä¸¥é‡é˜»ç¢äº†å…¶åº”ç”¨ã€‚è¿™ç§å¤±è´¥æºäºŽæ¨¡åž‹ä¾èµ–æœªæŽ¥åœ°çš„ã€åŸºäºŽæ–‡æœ¬çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æŽ¨ç†ã€‚çŽ°æœ‰çš„å¤šæ¨¡æ€CoTæ–¹æ³•è¯•å›¾ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬ç¼ºé™·ï¼šï¼ˆ1ï¼‰è§£è€¦çš„æ„ŸçŸ¥å’ŒæŽ¨ç†é˜¶æ®µé˜»ç¢äº†ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ï¼›ï¼ˆ2ï¼‰ä¾èµ–æ˜‚è´µã€å¯†é›†çš„å®šä½æ ‡æ³¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†OmniDrive-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„ç«¯åˆ°ç«¯VLMæ¡†æž¶ï¼Œé€šè¿‡äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆiMCoTï¼‰æœºåˆ¶ç»Ÿä¸€äº†æ„ŸçŸ¥å’ŒæŽ¨ç†ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å¼ºåŒ–é©±åŠ¨çš„è§†è§‰æŽ¥åœ°èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿè‡ªä¸»å¼•å¯¼æ³¨æ„åŠ›å¹¶â€œæ”¾å¤§â€å…³é”®åŒºåŸŸè¿›è¡Œç»†ç²’åº¦åˆ†æžã€‚è¿™ä¸€èƒ½åŠ›é€šè¿‡æˆ‘ä»¬çš„çº¯ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹å’ŒClip-GRPOç®—æ³•å®žçŽ°ã€‚å…³é”®çš„æ˜¯ï¼ŒClip-GRPOå¼•å…¥äº†æ— éœ€æ ‡æ³¨ã€åŸºäºŽè¿‡ç¨‹çš„æŽ¥åœ°å¥–åŠ±ã€‚è¯¥å¥–åŠ±ä¸ä»…æ¶ˆé™¤äº†å¯¹å¯†é›†æ ‡æ³¨çš„éœ€æ±‚ï¼Œè¿˜é€šè¿‡å¼ºåˆ¶è§†è§‰ç„¦ç‚¹ä¸Žæ–‡æœ¬æŽ¨ç†ä¹‹é—´çš„å®žæ—¶è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼Œè§„é¿äº†å¤–éƒ¨å·¥å…·è°ƒç”¨çš„ä¸ç¨³å®šæ€§ã€‚åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šçš„å¤§é‡å®žéªŒè¯æ˜Žäº†æˆ‘ä»¬æ¨¡åž‹çš„æ˜¾è‘—æ”¹è¿›ã€‚ä¸ŽåŸºçº¿Qwen2.5VL-7Bç›¸æ¯”ï¼ŒOmniDrive-R1å°†æ•´ä½“æŽ¨ç†åˆ†æ•°ä»Ž51.77%æå‡è‡³80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡ä»Ž37.81%æå‡è‡³73.62%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

OmniDrive-R1æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§†è§‰è¯­è¨€æ¨¡åž‹æ¡†æž¶ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡ã€‚å…¶æ ¸å¿ƒæ˜¯äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆiMCoTï¼‰æœºåˆ¶ï¼Œå°†æ„ŸçŸ¥å’ŒæŽ¨ç†é˜¶æ®µç»Ÿä¸€èµ·æ¥ï¼Œå®žçŽ°è”åˆä¼˜åŒ–ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬å¼ºåŒ–é©±åŠ¨çš„è§†è§‰æŽ¥åœ°èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½è‡ªä¸»èšç„¦å…³é”®åŒºåŸŸè¿›è¡Œç»†ç²’åº¦åˆ†æžï¼›ä»¥åŠClip-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åŸºäºŽè¿‡ç¨‹çš„æŽ¥åœ°å¥–åŠ±ï¼Œæ— éœ€å¯†é›†æ ‡æ³¨ï¼Œå¹¶å¼ºåˆ¶è§†è§‰ç„¦ç‚¹ä¸Žæ–‡æœ¬æŽ¨ç†çš„å®žæ—¶ä¸€è‡´æ€§ï¼Œé¿å…å¤–éƒ¨å·¥å…·çš„ä¸ç¨³å®šæ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šå®ƒè§£å†³äº†æ„ŸçŸ¥ä¸ŽæŽ¨ç†è§£è€¦çš„é—®é¢˜ï¼Œå®žçŽ°äº†ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼›åŒæ—¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ›¿ä»£äº†ä¼ ç»Ÿä¾èµ–æ˜‚è´µæ ‡æ³¨çš„æ–¹æ³•ï¼Œæé«˜äº†æ•ˆçŽ‡å’Œç¨³å®šæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šï¼ŒOmniDrive-R1ç›¸æ¯”åŸºçº¿Qwen2.5VL-7Bï¼Œæ•´ä½“æŽ¨ç†åˆ†æ•°ä»Ž51.77%å¤§å¹…æå‡è‡³80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡ä»Ž37.81%è·ƒå‡è‡³73.62%ï¼Œè¯æ˜Žäº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶æŽ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—æ€§èƒ½æ”¹è¿›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯é éƒ¨ç½²ï¼Œå¯æå‡è½¦è¾†åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œå‡å°‘ç‰©ä½“å¹»è§‰ç­‰å®‰å…¨é£Žé™©ï¼ŒæŽ¨åŠ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.

