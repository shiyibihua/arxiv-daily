---
layout: default
title: Optimizing Rank for High-Fidelity Implicit Neural Representations
---

# Optimizing Rank for High-Fidelity Implicit Neural Representations

**arXiv**: [2512.14366v1](https://arxiv.org/abs/2512.14366) | [PDF](https://arxiv.org/pdf/2512.14366.pdf)

**ä½œè€…**: Julian McGinnis, Florian A. HÃ¶lzl, Suprosanna Shit, Florentin Bieder, Paul Friedrich, Mark MÃ¼hlau, BjÃ¶rn Menze, Daniel Rueckert, Benedikt Wiestler

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡ä¼˜åŒ–ç½‘ç»œç§©æ¥æå‡éšå¼ç¥žç»è¡¨ç¤ºçš„é«˜é¢‘ä¿¡å·ä¿çœŸåº¦ï¼ŒæŒ‘æˆ˜ä¼ ç»Ÿæž¶æž„é™åˆ¶è§‚ç‚¹ã€‚**

**å…³é”®è¯**: `éšå¼ç¥žç»è¡¨ç¤º` `å¤šå±‚æ„ŸçŸ¥æœº` `ç§©ä¼˜åŒ–` `é«˜é¢‘ä¿¡å·` `å›¾åƒé‡å»º` `æ–°è§†è§’åˆæˆ` `ä¼˜åŒ–å™¨è®¾è®¡` `è®¡ç®—æœºè§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•è®¤ä¸ºæ™®é€šMLPså› æž¶æž„é™åˆ¶æ— æ³•è¡¨ç¤ºé«˜é¢‘ä¿¡å·ï¼Œä¾èµ–å¤æ‚å¹²é¢„å¦‚åæ ‡åµŒå…¥ã€‚
2. è®ºæ–‡æå‡ºé«˜é¢‘å­¦ä¹ å—é™æºäºŽè®­ç»ƒä¸­ç§©é€€åŒ–ï¼Œé€šè¿‡ä¼˜åŒ–å™¨å¦‚Muonè°ƒèŠ‚ç§©æ¥æå‡ä¿çœŸåº¦ã€‚
3. å®žéªŒæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œæ–°è§†è§’åˆæˆä¸­PSNRæå‡é«˜è¾¾9 dBï¼ŒéªŒè¯äº†ç®€å•MLPsçš„æ½œåŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽæ™®é€šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰çš„éšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰è¢«å¹¿æ³›è®¤ä¸ºæ— æ³•è¡¨ç¤ºé«˜é¢‘å†…å®¹ï¼Œè¿™å¼•å¯¼ç ”ç©¶è½¬å‘åæ ‡åµŒå…¥æˆ–ç‰¹æ®Šæ¿€æ´»å‡½æ•°ç­‰æž¶æž„å¹²é¢„ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†æ™®é€šMLPsçš„ä½Žé¢‘åå·®æ˜¯å­¦ä¹ é«˜é¢‘å†…å®¹çš„å†…åœ¨æž¶æž„é™åˆ¶è¿™ä¸€è§‚ç‚¹ï¼Œè®¤ä¸ºè¿™æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šç§©é€€åŒ–çš„ç—‡çŠ¶ã€‚æˆ‘ä»¬é€šè¿‡å®žéªŒè¯æ˜Žï¼Œåœ¨è®­ç»ƒæœŸé—´è°ƒèŠ‚ç½‘ç»œçš„ç§©æ˜¾è‘—æé«˜äº†å­¦ä¹ ä¿¡å·çš„ä¿çœŸåº¦ï¼Œä½¿å³ä½¿æ˜¯ç®€å•çš„MLPæž¶æž„ä¹Ÿå…·æœ‰è¡¨è¾¾åŠ›ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œä½¿ç”¨åƒMuonè¿™æ ·çš„ä¼˜åŒ–å™¨ï¼Œå…·æœ‰é«˜ç§©ã€è¿‘æ­£äº¤æ›´æ–°ï¼Œèƒ½æŒç»­å¢žå¼ºINRsæž¶æž„ï¼Œç”šè‡³è¶…è¶Šç®€å•çš„ReLU MLPsã€‚è¿™äº›æ˜¾è‘—æ”¹è¿›é€‚ç”¨äºŽå¤šç§é¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶å’ŒåŒ»å­¦å›¾åƒä»¥åŠæ–°è§†è§’åˆæˆï¼Œä¸Žå…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒPSNRæå‡é«˜è¾¾9 dBã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢åŒ…å«ä»£ç å’Œå®žéªŒç»“æžœï¼Œå¯åœ¨https://muon-inrs.github.ioè®¿é—®ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æ•´ä½“æ¡†æž¶åŸºäºŽéšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰ï¼Œä½¿ç”¨æ™®é€šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰ä½œä¸ºåŸºç¡€æž¶æž„ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæŒ‘æˆ˜ä¼ ç»Ÿè§‚ç‚¹ï¼Œå°†é«˜é¢‘å­¦ä¹ é™åˆ¶å½’å› äºŽè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šç§©é€€åŒ–ï¼Œè€ŒéžMLPsçš„å›ºæœ‰æž¶æž„ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œæå‡ºé€šè¿‡ä¼˜åŒ–å™¨ï¼ˆå¦‚Muonï¼‰è°ƒèŠ‚ç½‘ç»œç§©ï¼Œå®žçŽ°é«˜ç§©ã€è¿‘æ­£äº¤çš„æƒé‡æ›´æ–°ï¼Œä»Žè€Œå¢žå¼ºä¿¡å·ä¿çœŸåº¦ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œä¸ä¾èµ–é¢å¤–çš„æž¶æž„å¹²é¢„ï¼ˆå¦‚åæ ‡åµŒå…¥æˆ–ç‰¹æ®Šæ¿€æ´»å‡½æ•°ï¼‰ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹æœ¬èº«æ¥æå‡æ€§èƒ½ï¼Œä½¿ç®€å•MLPæž¶æž„ä¹Ÿèƒ½æœ‰æ•ˆè¡¨ç¤ºé«˜é¢‘å†…å®¹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œä½¿ç”¨ä¼˜åŒ–å™¨å¦‚Muonè°ƒèŠ‚ç§©åŽï¼Œåœ¨å¤šç§é¢†åŸŸï¼ˆè‡ªç„¶å›¾åƒã€åŒ»å­¦å›¾åƒã€æ–°è§†è§’åˆæˆï¼‰ä¸­ï¼ŒPSNRæå‡é«˜è¾¾9 dBï¼Œæ˜¾è‘—è¶…è¶Šå…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†ç§©ä¼˜åŒ–å¯¹æå‡INRsæ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒå¤„ç†ã€åŒ»å­¦å›¾åƒåˆ†æžä»¥åŠæ–°è§†è§’åˆæˆç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡éšå¼ç¥žç»è¡¨ç¤ºçš„é«˜é¢‘ä¿çœŸåº¦ï¼Œå¯åº”ç”¨äºŽé«˜è´¨é‡å›¾åƒé‡å»ºã€3Dåœºæ™¯è¡¨ç¤ºå’Œè™šæ‹ŸçŽ°å®žï¼Œä¸ºå®žé™…åº”ç”¨æä¾›æ›´ç²¾ç¡®å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).

