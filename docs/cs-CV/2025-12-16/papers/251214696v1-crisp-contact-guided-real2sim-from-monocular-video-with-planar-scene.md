---
layout: default
title: CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives
---

# CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives

**arXiv**: [2512.14696v1](https://arxiv.org/abs/2512.14696) | [PDF](https://arxiv.org/pdf/2512.14696.pdf)

**ä½œè€…**: Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan

**åˆ†ç±»**: cs.CV, cs.GR, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CRISPï¼šåŸºäºŽå•ç›®è§†é¢‘å’Œå¹³é¢åœºæ™¯åŽŸè¯­çš„æŽ¥è§¦å¼•å¯¼Real2Simæ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **äººå½¢ç§»åŠ¨ (Humanoid Locomotion)** **é¥æ“ä½œä¸Žæ¨¡ä»¿ (Teleoperation & Imitation)** **3Dé‡å»º (3D Reconstruction)**

**å…³é”®è¯**: `Real2Sim` `å•ç›®è§†é¢‘é‡å»º` `äººä½“-åœºæ™¯äº¤äº’` `å¹³é¢åŽŸè¯­` `ç‰©ç†ä»¿çœŸ` `å¼ºåŒ–å­¦ä¹ ` `åœºæ™¯é‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨äººä½“-åœºæ™¯è”åˆé‡å»ºä¸­å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆä¾èµ–æ•°æ®å…ˆéªŒï¼Œè¦ä¹ˆé‡å»ºçš„å‡ ä½•ä½“å™ªå£°å¤§ï¼Œå¯¼è‡´äº¤äº’æ¨¡æ‹Ÿå¤±è´¥ã€‚
2. CRISPé€šè¿‡æ‹Ÿåˆå¹³é¢åŽŸè¯­åˆ°ç‚¹äº‘é‡å»ºï¼Œå¹¶ç»“åˆäººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡ï¼Œæ¢å¤å¹²å‡€ã€å‡¸çš„ã€å¯ç”¨äºŽä»¿çœŸçš„åœºæ™¯å‡ ä½•ä½“ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒCRISPæ˜¾è‘—é™ä½Žäº†è¿åŠ¨è·Ÿè¸ªå¤±è´¥çŽ‡ï¼Œæé«˜äº†å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿçš„åžåé‡ï¼Œå¹¶åœ¨çœŸå®žè§†é¢‘å’Œç”Ÿæˆè§†é¢‘ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

CRISPæ˜¯ä¸€ç§ä»Žå•ç›®è§†é¢‘ä¸­æ¢å¤å¯æ¨¡æ‹Ÿçš„äººä½“è¿åŠ¨å’Œåœºæ™¯å‡ ä½•çš„æ–¹æ³•ã€‚çŽ°æœ‰çš„äººä½“-åœºæ™¯è”åˆé‡å»ºå·¥ä½œä¾èµ–äºŽæ•°æ®é©±åŠ¨çš„å…ˆéªŒå’Œæ— ç‰©ç†å¼•æ“Žçš„è”åˆä¼˜åŒ–ï¼Œæˆ–è€…æ¢å¤çš„å‡ ä½•ä½“å™ªå£°å¤§ï¼Œå¯¼è‡´å¸¦æœ‰åœºæ™¯äº¤äº’çš„è¿åŠ¨è·Ÿè¸ªç­–ç•¥å¤±è´¥ã€‚CRISPçš„å…³é”®åœ¨äºŽé€šè¿‡æ‹Ÿåˆå¹³é¢åŽŸè¯­åˆ°åœºæ™¯çš„ç‚¹äº‘é‡å»ºï¼Œæ¥æ¢å¤å‡¸çš„ã€å¹²å‡€çš„ã€å¯ç”¨äºŽä»¿çœŸçš„å‡ ä½•ä½“ï¼Œè¿™é€šè¿‡ä¸€ä¸ªç®€å•çš„æ·±åº¦ã€æ³•çº¿å’Œå…‰æµèšç±»æµç¨‹å®žçŽ°ã€‚ä¸ºäº†é‡å»ºäº¤äº’è¿‡ç¨‹ä¸­å¯èƒ½è¢«é®æŒ¡çš„åœºæ™¯å‡ ä½•ä½“ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†äººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨äººä½“å§¿åŠ¿æ¥é‡å»ºæ¤…å­è¢«é®æŒ¡çš„åº§ä½ï¼‰ã€‚æœ€åŽï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨äººå½¢æŽ§åˆ¶å™¨ï¼Œç¡®ä¿äººä½“å’Œåœºæ™¯é‡å»ºåœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆEMDBã€PROXï¼‰ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è¿åŠ¨è·Ÿè¸ªå¤±è´¥çŽ‡ä»Ž55.2ï¼…é™ä½Žåˆ°6.9ï¼…ï¼ŒåŒæ—¶æä¾›äº†å¿«43ï¼…çš„RLæ¨¡æ‹Ÿåžåé‡ã€‚æˆ‘ä»¬è¿˜åœ¨åŒ…æ‹¬éšæ„æ‹æ‘„çš„è§†é¢‘ã€äº’è”ç½‘è§†é¢‘ç”šè‡³Soraç”Ÿæˆçš„è§†é¢‘åœ¨å†…çš„çœŸå®žè§†é¢‘ä¸ŠéªŒè¯äº†å®ƒã€‚è¿™è¯æ˜Žäº†CRISPå¤§è§„æ¨¡ç”Ÿæˆç‰©ç†ä¸Šæœ‰æ•ˆçš„äººä½“è¿åŠ¨å’Œäº¤äº’çŽ¯å¢ƒçš„èƒ½åŠ›ï¼Œæžå¤§åœ°æŽ¨è¿›äº†æœºå™¨äººå’ŒAR/VRçš„real-to-simåº”ç”¨ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„äººä½“-åœºæ™¯è”åˆé‡å»ºæ–¹æ³•è¦ä¹ˆä¾èµ–å¤§é‡æ•°æ®å…ˆéªŒï¼Œç¼ºä¹å¯¹ç‰©ç†è§„å¾‹çš„çº¦æŸï¼Œå¯¼è‡´é‡å»ºç»“æžœä¸çœŸå®žï¼›è¦ä¹ˆé‡å»ºçš„åœºæ™¯å‡ ä½•ä½“å™ªå£°è¾ƒå¤§ï¼Œå­˜åœ¨ä¼ªå½±ï¼Œä½¿å¾—åŸºäºŽç‰©ç†ä»¿çœŸçš„è¿åŠ¨æŽ§åˆ¶ç­–ç•¥éš¾ä»¥æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨äººä¸ŽçŽ¯å¢ƒå­˜åœ¨äº¤äº’çš„æƒ…å†µä¸‹ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»Žå•ç›®è§†é¢‘ä¸­é‡å»ºå‡ºæ—¢çœŸå®žåˆå¹²å‡€ï¼Œä¸”èƒ½ç”¨äºŽç‰©ç†ä»¿çœŸçš„ä¸‰ç»´äººä½“å’Œåœºæ™¯æ¨¡åž‹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCRISPçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¹³é¢åŽŸè¯­æ¥è¡¨ç¤ºåœºæ™¯å‡ ä½•ä½“ï¼Œå¹¶ç»“åˆäººä½“-åœºæ™¯æŽ¥è§¦ä¿¡æ¯æ¥æŽ¨æ–­è¢«é®æŒ¡çš„åŒºåŸŸã€‚é€šè¿‡å°†ç‚¹äº‘é‡å»ºç»“æžœæ‹Ÿåˆä¸ºä¸€ç³»åˆ—å¹³é¢ï¼Œå¯ä»¥å¾—åˆ°å¹²å‡€ã€å‡¸çš„åœºæ™¯å‡ ä½•ä½“ï¼Œè¿™æ›´é€‚åˆäºŽç‰©ç†ä»¿çœŸã€‚åŒæ—¶ï¼Œåˆ©ç”¨äººä½“å§¿åŠ¿ä¿¡æ¯ï¼Œå¯ä»¥æŽ¨æ–­å‡ºäººä½“ä¸Žåœºæ™¯çš„æŽ¥è§¦åŒºåŸŸï¼Œä»Žè€Œæ¢å¤è¢«é®æŒ¡çš„åœºæ™¯éƒ¨åˆ†ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCRISPçš„æ•´ä½“æµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä»Žå•ç›®è§†é¢‘ä¸­é‡å»ºç‚¹äº‘ï¼›2) å¯¹ç‚¹äº‘è¿›è¡Œèšç±»ï¼Œæå–å¹³é¢åŽŸè¯­ï¼›3) åˆ©ç”¨äººä½“å§¿åŠ¿ä¿¡æ¯å’ŒæŽ¥è§¦æ¨¡åž‹ï¼ŒæŽ¨æ–­å¹¶è¡¥å…¨è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼›4) ä½¿ç”¨é‡å»ºçš„äººä½“å’Œåœºæ™¯æ¨¡åž‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒäººå½¢æŽ§åˆ¶å™¨ï¼Œç¡®ä¿è¿åŠ¨çš„ç‰©ç†åˆç†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRISPçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†å¹³é¢åŽŸè¯­è¡¨ç¤ºå’Œäººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡ç›¸ç»“åˆï¼Œç”¨äºŽä»Žå•ç›®è§†é¢‘ä¸­é‡å»ºå¯ç”¨äºŽç‰©ç†ä»¿çœŸçš„åœºæ™¯å‡ ä½•ä½“ã€‚ä¸Žä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒCRISPä¸éœ€è¦å¤§é‡çš„æ•°æ®å…ˆéªŒï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆæ›´å¹²å‡€ã€æ›´çœŸå®žçš„åœºæ™¯æ¨¡åž‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥é©±åŠ¨äººå½¢æŽ§åˆ¶å™¨ï¼Œå¯ä»¥ç¡®ä¿é‡å»ºçš„äººä½“è¿åŠ¨åœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šCRISPä½¿ç”¨æ·±åº¦ã€æ³•çº¿å’Œå…‰æµä¿¡æ¯è¿›è¡Œç‚¹äº‘èšç±»ï¼Œä»¥æå–å¹³é¢åŽŸè¯­ã€‚äººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡åŸºäºŽäººä½“å§¿åŠ¿å’Œåœºæ™¯å‡ ä½•ä½“ä¹‹é—´çš„å…³ç³»ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™æ¥æŽ¨æ–­æŽ¥è§¦åŒºåŸŸã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªèƒ½å¤ŸæŽ§åˆ¶äººå½¢è§’è‰²åœ¨é‡å»ºåœºæ™¯ä¸­è¿åŠ¨çš„ç­–ç•¥ï¼Œå¥–åŠ±å‡½æ•°åŒ…æ‹¬æ¨¡ä»¿çœŸå®žè¿åŠ¨ã€ä¿æŒå¹³è¡¡å’Œé¿å…ç¢°æ’žç­‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

CRISPåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆEMDBã€PROXï¼‰ä¸­ï¼Œå°†è¿åŠ¨è·Ÿè¸ªå¤±è´¥çŽ‡ä»Ž55.2ï¼…é™ä½Žåˆ°6.9ï¼…ï¼Œæ˜¾è‘—æå‡äº†è¿åŠ¨è·Ÿè¸ªçš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼ŒCRISPè¿˜æä¾›äº†å¿«43ï¼…çš„å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿåžåé‡ï¼Œæé«˜äº†ä»¿çœŸæ•ˆçŽ‡ã€‚æ­¤å¤–ï¼ŒCRISPåœ¨çœŸå®žè§†é¢‘å’ŒSoraç”Ÿæˆçš„è§†é¢‘ä¸Šçš„æˆåŠŸåº”ç”¨ï¼Œè¯æ˜Žäº†å…¶åœ¨å„ç§åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CRISPå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æœºå™¨äººæŠ€æœ¯ã€å¢žå¼ºçŽ°å®žï¼ˆARï¼‰å’Œè™šæ‹ŸçŽ°å®žï¼ˆVRï¼‰ã€‚å®ƒå¯ä»¥ç”¨äºŽåˆ›å»ºé€¼çœŸçš„è™šæ‹ŸçŽ¯å¢ƒï¼Œç”¨äºŽè®­ç»ƒæœºå™¨äººæˆ–è¿›è¡Œè™šæ‹Ÿä»¿çœŸã€‚æ­¤å¤–ï¼ŒCRISPè¿˜å¯ä»¥ç”¨äºŽAR/VRåº”ç”¨ä¸­ï¼Œå°†è™šæ‹Ÿç‰©ä½“ä¸ŽçœŸå®žåœºæ™¯è¿›è¡Œäº¤äº’ï¼Œä¾‹å¦‚ï¼Œåœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­æ¨¡æ‹Ÿäººåœ¨çœŸå®žæˆ¿é—´ä¸­è¡Œèµ°å’Œäº¤äº’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.

