---
layout: default
title: SuperCLIP: CLIP with Simple Classification Supervision
---

# SuperCLIP: CLIP with Simple Classification Supervision

**arXiv**: [2512.14480v1](https://arxiv.org/abs/2512.14480) | [PDF](https://arxiv.org/pdf/2512.14480.pdf)

**ä½œè€…**: Weiheng Zhao, Zilong Huang, Jiashi Feng, Xinggang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSuperCLIPæ¡†æž¶ï¼Œé€šè¿‡åˆ†ç±»ç›‘ç£å¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œè§£å†³CLIPæ¨¡åž‹ç»†ç²’åº¦è¯­ä¹‰åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€å¯¹é½` `ç»†ç²’åº¦è¯­ä¹‰` `é›¶æ ·æœ¬åˆ†ç±»` `å›¾åƒ-æ–‡æœ¬æ£€ç´¢` `è½»é‡çº§ç›‘ç£` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `é¢„è®­ç»ƒæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPæ¨¡åž‹ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥è¯å…ƒçº§ç›‘ç£ï¼Œå¯¼è‡´ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·åˆ©ç”¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤„ç†é•¿æè¿°æ—¶è¡¨çŽ°æ›´å·®ã€‚
2. SuperCLIPé€šè¿‡æ·»åŠ è½»é‡çº§çº¿æ€§å±‚ï¼Œå¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£ï¼Œå¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œä»¥è¯å…ƒçº§çº¿ç´¢æå‡è§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ— éœ€é¢å¤–æ•°æ®ã€‚
3. å®žéªŒæ˜¾ç¤ºSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œè§†è§‰ä»»åŠ¡ä¸Šå‡æå‡æ€§èƒ½ï¼Œå¹¶ç¼“è§£å°æ‰¹é‡è®­ç»ƒçš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒå’Œæ–‡æœ¬æ¥å®žçŽ°è§†è§‰-è¯­è¨€ä»»åŠ¡çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å‘çŽ°ï¼ŒCLIPç±»æ¨¡åž‹åœ¨å¤„ç†æ–‡æœ¬æ—¶ä»æœªèƒ½å……åˆ†åˆ©ç”¨ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·ï¼Œè¿™ä¸€é—®é¢˜åœ¨å¤„ç†é•¿è€Œè¯¦ç»†çš„æè¿°æ—¶å°¤ä¸ºæ˜Žæ˜¾ã€‚è¿™æºäºŽCLIPçš„è®­ç»ƒç›®æ ‡ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†è¯å…ƒçº§ç›‘ç£ï¼Œé™åˆ¶äº†å…¶å®žçŽ°ç»†ç²’åº¦è§†è§‰-æ–‡æœ¬å¯¹é½çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SuperCLIPï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æž¶ï¼Œé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£æ¥å¢žå¼ºå¯¹æ¯”å­¦ä¹ ã€‚ä»…éœ€åœ¨è§†è§‰ç¼–ç å™¨ä¸Šæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼ŒSuperCLIPå°±èƒ½åˆ©ç”¨è¯å…ƒçº§çº¿ç´¢æ¥æå‡è§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼Œä¸”æ— éœ€é¢å¤–æ ‡æ³¨æ•°æ®ã€‚å®žéªŒè¡¨æ˜Žï¼ŒSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œçº¯è§†è§‰ä»»åŠ¡ä¸Šå‡èƒ½æŒç»­æå‡æ€§èƒ½ã€‚è¿™äº›å¢žç›Šæ— è®ºæ¨¡åž‹æ˜¯åœ¨åŽŸå§‹ç½‘ç»œæ•°æ®è¿˜æ˜¯ä¸°å¯Œçš„é‡æ–°æè¿°æ•°æ®ä¸Šè®­ç»ƒéƒ½æˆç«‹ï¼Œè¯æ˜Žäº†SuperCLIPåœ¨ä¸¤ç§æƒ…å†µä¸‹æ¢å¤æ–‡æœ¬ç›‘ç£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒSuperCLIPé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£å‡è½»äº†CLIPåœ¨å°æ‰¹é‡æƒ…å†µä¸‹çš„æ€§èƒ½ä¸‹é™ï¼Œé¿å…äº†ä¾èµ–å¤§æ‰¹é‡è®­ç»ƒã€‚ä»£ç å’Œæ¨¡åž‹å°†å¼€æºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

SuperCLIPçš„æ•´ä½“æ¡†æž¶åŸºäºŽCLIPï¼Œé€šè¿‡å¢žå¼ºå¯¹æ¯”å­¦ä¹ æ¥å®žçŽ°ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯åœ¨è§†è§‰ç¼–ç å™¨ä¸Šæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼Œç”¨äºŽç”Ÿæˆåˆ†ç±»é¢„æµ‹ï¼Œä»Žè€Œå¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨æ–‡æœ¬ä¸­çš„è¯å…ƒçº§ä¿¡æ¯ï¼ˆå¦‚åè¯æˆ–çŸ­è¯­ï¼‰ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œé€šè¿‡åˆ†ç±»æŸå¤±å‡½æ•°ä¼˜åŒ–è§†è§‰ç‰¹å¾ä¸Žæ–‡æœ¬ç»†ç²’åº¦è¯­ä¹‰çš„å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒSuperCLIPä¸ä¾èµ–å¤æ‚çš„æž¶æž„æˆ–é¢å¤–æ ‡æ³¨æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ç®€å•çš„åˆ†ç±»ç›‘ç£ç›´æŽ¥æå‡CLIPçš„ç»†ç²’åº¦å¯¹é½èƒ½åŠ›ï¼Œæ€»è®¡ç®—å¼€é”€ä»…å¾®å¢ž0.077%ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡å‡†ç¡®çŽ‡ï¼Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢æ€§èƒ½ä¼˜äºŽåŸºçº¿CLIPï¼ŒåŒæ—¶çº¯è§†è§‰ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹ä¹Ÿæœ‰æ”¹è¿›ã€‚å®žéªŒè¿˜è¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç¼“è§£å°æ‰¹é‡è®­ç»ƒçš„æ€§èƒ½ä¸‹é™ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼Œæ— éœ€é¢å¤–æ•°æ®ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SuperCLIPå¯åº”ç”¨äºŽå¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¦‚é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå†…å®¹ç”Ÿæˆä»»åŠ¡ã€‚å…¶æå‡çš„ç»†ç²’åº¦å¯¹é½èƒ½åŠ›æœ‰åŠ©äºŽåœ¨åŒ»ç–—å½±åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶è§†è§‰ç†è§£å’Œæ™ºèƒ½å†…å®¹æŽ¨èç­‰å®žé™…åœºæ™¯ä¸­å®žçŽ°æ›´ç²¾å‡†çš„è¯­ä¹‰ç†è§£ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.

