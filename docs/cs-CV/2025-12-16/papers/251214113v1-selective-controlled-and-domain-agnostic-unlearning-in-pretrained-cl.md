---
layout: default
title: Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach
---

# Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14113" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14113v1</a>
  <a href="https://arxiv.org/pdf/2512.14113.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14113v1" onclick="toggleFavorite(this, '2512.14113v1', 'Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ashish Mishra, Gyanaranjan Nayak, Tarun Kumar, Arpit Shah, Suparna Bhattacharya, Martin Foltin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å…è®­ç»ƒå…æ•°æ®çš„CLIPå¯æ§é€‰æ‹©æ€§é¢†åŸŸæ— å…³çŸ¥è¯†é—å¿˜æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `CLIP` `çŸ¥è¯†é—å¿˜` `å…è®­ç»ƒ` `é›¶æ ·æœ¬å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `é¢†åŸŸè‡ªé€‚åº”` `æ¨¡å‹å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CLIPæ¨¡å‹éš¾ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹é€‰æ‹©æ€§åœ°é—å¿˜ç‰¹å®šç±»åˆ«çŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§å…è®­ç»ƒå…æ•°æ®çš„é—å¿˜æ¡†æ¶ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå’Œåˆæˆè§†è§‰åŸå‹ï¼Œåœ¨CLIPçš„è”åˆåµŒå…¥ç©ºé—´ä¸­å®ç°çŸ¥è¯†æ“¦é™¤ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ç§»é™¤æŒ‡å®šç±»åˆ«çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ§æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒæ¨¡å‹å¦‚CLIPåœ¨å„ç§è§†è§‰é¢†åŸŸï¼ˆåŒ…æ‹¬è‡ªç„¶å›¾åƒã€è‰ºæœ¯æ¸²æŸ“å’ŒæŠ½è±¡è¡¨ç¤ºï¼‰ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨é€šå¸¸éœ€è¦åœ¨ä¸éœ€è¦é¢å¤–æ•°æ®æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç§»é™¤ï¼ˆæˆ–â€œé—å¿˜â€ï¼‰ç‰¹å®šçš„å¯¹è±¡ç±»åˆ«ï¼ŒåŒæ—¶ä¸å½±å“æ¨¡å‹åœ¨ä¸ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…è®­ç»ƒå’Œå…æ•°æ®çš„é—å¿˜æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒä¸‰ç§ä¸åŒçš„é—å¿˜èŒƒå¼ï¼šï¼ˆ1ï¼‰è·¨æ‰€æœ‰é¢†åŸŸçš„é€‰å®šå¯¹è±¡çš„å…¨å±€é—å¿˜ï¼Œï¼ˆ2ï¼‰ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ç§»é™¤ï¼ˆä¾‹å¦‚ï¼Œæ¶ˆé™¤è‰å›¾è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™ç…§ç‰‡è¯†åˆ«ï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰é€‰æ‹©æ€§é¢†åŸŸçš„å®Œå…¨é—å¿˜ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€é›¶ç©ºé—´ï¼ŒååŒæ•´åˆæ–‡æœ¬æç¤ºå’Œä»CLIPè”åˆåµŒå…¥ç©ºé—´åˆæˆçš„è§†è§‰åŸå‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç§»é™¤äº†ä¸éœ€è¦çš„ç±»åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™äº†å‰©ä½™çš„çŸ¥è¯†ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰åŸºäºé‡æ–°è®­ç»ƒçš„æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸ºå¯æ§æ¨¡å‹é—å¿˜æä¾›äº†ä¸€ç§çµæ´»ä¸”è®¡ç®—é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é¢„è®­ç»ƒCLIPæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦ç§»é™¤ç‰¹å®šç±»åˆ«çŸ¥è¯†ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºé‡æ–°è®­ç»ƒæˆ–é¢å¤–æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•åœ¨ä¸å½±å“æ¨¡å‹æ•´ä½“æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°å¯æ§çš„ã€é€‰æ‹©æ€§çš„çŸ¥è¯†é—å¿˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨CLIPçš„è”åˆæ–‡æœ¬-å›¾åƒåµŒå…¥ç©ºé—´ï¼Œé€šè¿‡æ“çºµè¯¥ç©ºé—´ä¸­çš„è¡¨ç¤ºæ¥å®ç°çŸ¥è¯†é—å¿˜ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€é›¶ç©ºé—´ï¼Œå°†éœ€è¦é—å¿˜çš„ç±»åˆ«ä¿¡æ¯æŠ•å½±åˆ°è¯¥é›¶ç©ºé—´ä¸­ï¼Œä»è€Œè¾¾åˆ°æ“¦é™¤çŸ¥è¯†çš„ç›®çš„ã€‚è¿™ç§æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæˆ–é¢å¤–æ•°æ®ï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ–‡æœ¬æç¤ºæ„å»º**ï¼šåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥å®šä¹‰éœ€è¦é—å¿˜çš„ç±»åˆ«ã€‚2) **è§†è§‰åŸå‹åˆæˆ**ï¼šåŸºäºCLIPçš„å›¾åƒç¼–ç å™¨ï¼Œåˆæˆä»£è¡¨éœ€è¦é—å¿˜ç±»åˆ«çš„è§†è§‰åŸå‹ã€‚3) **å¤šæ¨¡æ€é›¶ç©ºé—´æ„å»º**ï¼šç»“åˆæ–‡æœ¬æç¤ºå’Œè§†è§‰åŸå‹ï¼Œæ„å»ºä¸€ä¸ªå¤šæ¨¡æ€é›¶ç©ºé—´ã€‚4) **çŸ¥è¯†æ“¦é™¤**ï¼šå°†éœ€è¦é—å¿˜çš„ç±»åˆ«ä¿¡æ¯æŠ•å½±åˆ°å¤šæ¨¡æ€é›¶ç©ºé—´ä¸­ï¼Œä»è€Œå®ç°çŸ¥è¯†æ“¦é™¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªå…è®­ç»ƒå…æ•°æ®çš„é—å¿˜æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨CLIPçš„è”åˆåµŒå…¥ç©ºé—´ï¼Œé€šè¿‡æ„å»ºå¤šæ¨¡æ€é›¶ç©ºé—´æ¥å®ç°å¯æ§çš„é€‰æ‹©æ€§çŸ¥è¯†é—å¿˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæˆ–é¢å¤–æ•°æ®ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•æœ‰æ•ˆåœ°æ„å»ºä»£è¡¨éœ€è¦é—å¿˜ç±»åˆ«çš„è§†è§‰åŸå‹ï¼›2) å¦‚ä½•å°†æ–‡æœ¬æç¤ºå’Œè§†è§‰åŸå‹æœ‰æ•ˆåœ°ç»“åˆèµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªå¤šæ¨¡æ€é›¶ç©ºé—´ï¼›3) å¦‚ä½•å°†éœ€è¦é—å¿˜çš„ç±»åˆ«ä¿¡æ¯æŠ•å½±åˆ°å¤šæ¨¡æ€é›¶ç©ºé—´ä¸­ï¼ŒåŒæ—¶æœ€å°åŒ–å¯¹æ¨¡å‹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä»¥åŠå¦‚ä½•é€‰æ‹©åˆé€‚çš„æŠ•å½±æ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ç§»é™¤æŒ‡å®šç±»åˆ«çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç§»é™¤ç‰¹å®šç±»åˆ«çŸ¥è¯†çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹åœ¨å‰©ä½™ç±»åˆ«ä¸Šçš„å‡†ç¡®ç‡ä¸‹é™åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å…·æœ‰è‰¯å¥½çš„å¯æ§æ€§ï¼Œå¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©æ€§åœ°é—å¿˜ç‰¹å®šé¢†åŸŸæˆ–ç±»åˆ«çš„çŸ¥è¯†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§æˆ–éµå®ˆæ³•è§„çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œåœ¨å›¾åƒæœç´¢å¼•æ“ä¸­ç§»é™¤ç‰¹å®šäººç‰©æˆ–æ•æ„Ÿå†…å®¹ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ç§»é™¤å¯¹ç‰¹å®šäº¤é€šæ ‡å¿—çš„è¯†åˆ«ï¼Œæˆ–åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­ç§»é™¤å¯¹ç‰¹å®šç–¾ç—…çš„å…³è”ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå¹¶é™ä½æ¨¡å‹ç»´æŠ¤å’Œæ›´æ–°çš„æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or "unlearning") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.

