---
layout: default
title: Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in
---

# Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.14273" target="_blank" class="toolbar-btn">arXiv: 2512.14273v1</a>
    <a href="https://arxiv.org/pdf/2512.14273.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14273v1" 
            onclick="toggleFavorite(this, '2512.14273v1', 'Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoqian Shen, Min-Hung Chen, Yu-Chiang Frank Wang, Mohamed Elhoseiny, Ryo Hachiuma

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

**Â§áÊ≥®**: Project page: https://xiaoqian-shen.github.io/Zoom-Zero/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Zoom-ZeroÔºöÈÄöËøáÊó∂Èó¥ÂüüÁº©ÊîæÂ¢ûÂº∫ËßÜÈ¢ëÁêÜËß£ÔºåËß£ÂÜ≥GVQA‰∏≠Êó∂Â∫èÂÆö‰Ωç‰∏çÂáÜÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁêÜËß£` `Grounded Video Question Answering` `Êó∂Èó¥ÂüüÁº©Êîæ` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâGVQAÊ®°ÂûãÊó∂Â∫èÊÑüÁü•ËÉΩÂäõÊúâÈôêÔºåÈöæ‰ª•ÂáÜÁ°ÆÂú∞Â∞ÜÁ≠îÊ°àÂÆö‰ΩçÂà∞ËßÜÈ¢ëÁâáÊÆµÔºåÂØºËá¥Êó∂Â∫èËØØÂà§„ÄÇ
2. Zoom-ZeroÈááÁî®Áî±Á≤óÂà∞Á≤æÁöÑÁ≠ñÁï•ÔºåÂÖàÁ≤óÁï•ÂÆö‰ΩçÁõ∏ÂÖ≥ÁâáÊÆµÔºåÂÜçÁ≤æÁªÜÁº©ÊîæÂÖ≥ÈîÆÂ∏ßÔºåËøõË°åËßÜËßâÈ™åËØÅ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåZoom-ZeroÂú®Êó∂Â∫èÂÆö‰ΩçÂíåÁ≠îÊ°àÂáÜÁ°ÆÁéá‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®ÈïøËßÜÈ¢ëÁêÜËß£ÊñπÈù¢„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫Zoom-ZeroÔºå‰∏Ä‰∏™Áî±Á≤óÂà∞Á≤æÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáGrounded Video Question Answering (GVQA) ‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑÂ§ßÂûãËßÜÈ¢ëËØ≠Ë®ÄÊ®°Âûã(LVLMs)Âú®Êó∂Â∫èÊÑüÁü•ÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåËÄåÂü∫‰∫éGroup Relative Policy Optimization (GRPO)ÁöÑÊñπÊ≥ïÈöæ‰ª•ÂáÜÁ°ÆÂú∞Â∞ÜÁ≠îÊ°àÂÆö‰ΩçÂà∞Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ëÁâáÊÆµÔºåÂØºËá¥Êó∂Â∫èËØØÂà§ÂíåÂπªËßâ„ÄÇZoom-ZeroÈ¶ñÂÖàÂÆö‰Ωç‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÁâáÊÆµÔºåÁÑ∂ÂêéËøõË°åÊó∂Èó¥ÂüüÁº©ÊîæÔºåËÅöÁÑ¶‰∫éÊúÄÊòæËëóÁöÑÂ∏ßÔºå‰ª•ËøõË°åÊõ¥Á≤æÁªÜÁöÑËßÜËßâÈ™åËØÅ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂºïÂÖ•Áº©ÊîæÁ≤æÂ∫¶Â•ñÂä±Êù•È™åËØÅÊó∂Â∫èÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂‰øÉËøõÂØπÂÆö‰ΩçÂ∏ßÁöÑÁªÜÁ≤íÂ∫¶ËßÜËßâÈ™åËØÅÔºõÂêåÊó∂ÈááÁî®tokenÈÄâÊã©ÊÄß‰ø°Áî®ÂàÜÈÖçÔºåÂ∞ÜÂ•ñÂä±ÂàÜÈÖçÁªôË¥üË¥£Êó∂Â∫èÂÆö‰ΩçÊàñÁ≠îÊ°àÁîüÊàêÁöÑtokenÔºå‰ªéËÄåÁºìËß£GRPOÂú®Â§ÑÁêÜÂ§öÊñπÈù¢Â•ñÂä±‰ø°Âè∑Êó∂ÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®NExT-GQAÂíåReXTimeÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´Â∞ÜÊó∂Â∫èÂÆö‰ΩçÁ≤æÂ∫¶ÊèêÈ´ò‰∫Ü5.2%Âíå4.6%ÔºåÂπ∂Â∞ÜÂπ≥ÂùáÁ≠îÊ°àÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü2.4%„ÄÇÊ≠§Â§ñÔºåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈááÁî®Áî±Á≤óÂà∞Á≤æÁöÑÁº©ÊîæÊñπÊ≥ïÔºåÂú®‰∏çÂΩ±ÂìçÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁöÑÊÉÖÂÜµ‰∏ã‰øùÁïô‰∫ÜÂÖ≥ÈîÆÁöÑËßÜËßâÁªÜËäÇÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºåÂú®ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Âπ≥ÂùáÊèêÈ´ò‰∫Ü6.4%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Grounded Video Question Answering (GVQA)‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÂ§ßÂûãËßÜÈ¢ëËØ≠Ë®ÄÊ®°Âûã(LVLMs)Êó∂Â∫èÊÑüÁü•ËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÂü∫‰∫éGroup Relative Policy Optimization (GRPO)ÁöÑÊñπÊ≥ïÔºåÈöæ‰ª•ÂáÜÁ°ÆÂú∞Â∞ÜÁ≠îÊ°àÂÆö‰ΩçÂà∞Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ëÁâáÊÆµÔºåÂØºËá¥Êó∂Â∫èËØØÂà§ÂíåÂπªËßâÔºåÂΩ±ÂìçÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®‰∏ÄÁßçÁî±Á≤óÂà∞Á≤æÁöÑÊó∂Èó¥ÂüüÁº©ÊîæÁ≠ñÁï•„ÄÇÈ¶ñÂÖàÔºåÁ≤óÁï•Âú∞ÂÆö‰Ωç‰∏éÈóÆÈ¢òÁõ∏ÂÖ≥ÁöÑËßÜÈ¢ëÁâáÊÆµÔºõÁÑ∂ÂêéÔºåÂØπËøô‰∫õÁâáÊÆµËøõË°åÊó∂Èó¥ÂüüÁöÑ‚ÄúÊîæÂ§ß‚ÄùÔºåËÅöÁÑ¶‰∫éÊúÄÂÖ≥ÈîÆÁöÑÂ∏ßÔºåËøõË°åÊõ¥ÁªÜËá¥ÁöÑËßÜËßâÈ™åËØÅ„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®ÊèêÈ´òÊ®°ÂûãÂØπËßÜÈ¢ëÂÜÖÂÆπÁöÑÊó∂Â∫èÊÑüÁü•ËÉΩÂäõÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ÂÆö‰ΩçÁ≠îÊ°à„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöZoom-ZeroÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ≤óÁï•ÂÆö‰ΩçÈò∂ÊÆµÂíåÁ≤æÁªÜÁº©ÊîæÈò∂ÊÆµ„ÄÇÂú®Á≤óÁï•ÂÆö‰ΩçÈò∂ÊÆµÔºåÊ®°ÂûãÈ¶ñÂÖàÊ†πÊçÆÈóÆÈ¢òÂÆö‰ΩçÂà∞Ëã•Âπ≤‰∏™ÂÄôÈÄâÁöÑËßÜÈ¢ëÁâáÊÆµ„ÄÇÂú®Á≤æÁªÜÁº©ÊîæÈò∂ÊÆµÔºåÊ®°ÂûãÂØπËøô‰∫õÁâáÊÆµËøõË°åÊó∂Èó¥ÂüüÁöÑÊîæÂ§ßÔºåÊèêÂèñÂÖ≥ÈîÆÂ∏ßÔºåÂπ∂ËøõË°åÊõ¥ÁªÜËá¥ÁöÑËßÜËßâÈ™åËØÅ„ÄÇÊï¥‰∏™Ê°ÜÊû∂Âà©Áî®Âº∫ÂåñÂ≠¶‰π†ËøõË°åËÆ≠ÁªÉÔºåÈÄöËøáÂ•ñÂä±Êú∫Âà∂Êù•‰ºòÂåñÊ®°ÂûãÁöÑÊó∂Â∫èÂÆö‰ΩçËÉΩÂäõÂíåÁ≠îÊ°àÁîüÊàêËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÁÇπÂú®‰∫é‰∏§‰∏™ÊñπÈù¢Ôºö‰∏ÄÊòØÂºïÂÖ•‰∫Ü‚ÄúÁº©ÊîæÁ≤æÂ∫¶Â•ñÂä±‚ÄùÔºåÁî®‰∫éÈ™åËØÅÊó∂Â∫èÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂‰øÉËøõÂØπÂÆö‰ΩçÂ∏ßÁöÑÁªÜÁ≤íÂ∫¶ËßÜËßâÈ™åËØÅÔºõ‰∫åÊòØÈááÁî®‰∫Ü‚ÄútokenÈÄâÊã©ÊÄß‰ø°Áî®ÂàÜÈÖç‚ÄùÔºåÂ∞ÜÂ•ñÂä±ÂàÜÈÖçÁªôË¥üË¥£Êó∂Â∫èÂÆö‰ΩçÊàñÁ≠îÊ°àÁîüÊàêÁöÑtokenÔºå‰ªéËÄåÁºìËß£GRPOÂú®Â§ÑÁêÜÂ§öÊñπÈù¢Â•ñÂä±‰ø°Âè∑Êó∂ÁöÑÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Áº©ÊîæÁ≤æÂ∫¶Â•ñÂä±ÊñπÈù¢ÔºåËÆ∫ÊñáËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éIoUÔºàIntersection over UnionÔºâÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÁî®‰∫éË°°ÈáèÊ®°ÂûãÈ¢ÑÊµãÁöÑÊó∂Â∫èÁâáÊÆµ‰∏éÁúüÂÆûÁ≠îÊ°àÁâáÊÆµ‰πãÈó¥ÁöÑÈáçÂè†Á®ãÂ∫¶„ÄÇÂú®tokenÈÄâÊã©ÊÄß‰ø°Áî®ÂàÜÈÖçÊñπÈù¢ÔºåËÆ∫Êñá‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Á°ÆÂÆöÊØè‰∏™tokenÂØπÊó∂Â∫èÂÆö‰ΩçÂíåÁ≠îÊ°àÁîüÊàêÁöÑË¥°ÁåÆÁ®ãÂ∫¶ÔºåÂπ∂Ê†πÊçÆË¥°ÁåÆÁ®ãÂ∫¶ÂàÜÈÖçÂ•ñÂä±„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞Ôºå‰ΩÜÊú™Âú®Ê≠§Â§ÑËØ¶ÁªÜÂ±ïÂºÄ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Zoom-ZeroÂú®NExT-GQAÂíåReXTimeÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´Â∞ÜÊó∂Â∫èÂÆö‰ΩçÁ≤æÂ∫¶ÊèêÈ´ò‰∫Ü5.2%Âíå4.6%ÔºåÂπ∂Â∞ÜÂπ≥ÂùáÁ≠îÊ°àÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü2.4%„ÄÇÊ≠§Â§ñÔºåÂú®ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠ÔºåZoom-ZeroÂπ≥ÂùáÊèêÈ´ò‰∫Ü6.4%ÔºåË°®ÊòéÂÖ∂Âú®Â§ÑÁêÜÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåZoom-ZeroÂú®GVQA‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Zoom-ZeroÊäÄÊúØÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩËßÜÈ¢ëÂàÜÊûê„ÄÅËßÜÈ¢ëÊêúÁ¥¢„ÄÅÊô∫ËÉΩÂÆ¢ÊúçÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜÈ¢ëÊêúÁ¥¢‰∏≠ÔºåÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑Âø´ÈÄüÂÆö‰ΩçÂà∞ËßÜÈ¢ë‰∏≠ÂåÖÂê´ÁâπÂÆö‰ø°ÊÅØÁöÑÁâáÊÆµÔºõÂú®Êô∫ËÉΩÂÆ¢Êúç‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÊèêÂá∫ÁöÑÈóÆÈ¢òÔºåÂáÜÁ°ÆÂú∞‰ªéËßÜÈ¢ëÁü•ËØÜÂ∫ì‰∏≠ÊâæÂà∞Á≠îÊ°à„ÄÇËØ•Á†îÁ©∂ÁöÑÊú™Êù•ÂΩ±ÂìçÂú®‰∫éÊèêÂçáËßÜÈ¢ëÁêÜËß£ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåÊé®Âä®ËßÜÈ¢ëÊô∫ËÉΩÂåñÂ∫îÁî®ÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.

