---
layout: default
title: Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in
---

# Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in

**arXiv**: [2512.14273v1](https://arxiv.org/abs/2512.14273) | [PDF](https://arxiv.org/pdf/2512.14273.pdf)

**ä½œè€…**: Xiaoqian Shen, Min-Hung Chen, Yu-Chiang Frank Wang, Mohamed Elhoseiny, Ryo Hachiuma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://xiaoqian-shen.github.io/Zoom-Zero/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZoom-Zeroæ¡†æž¶ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„æ—¶åºæ”¾å¤§æœºåˆ¶è§£å†³è§†é¢‘é—®ç­”ä¸­çš„æ—¶åºå®šä½ä¸å‡†ç¡®é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `æ—¶åºå®šä½` `ç²—åˆ°ç»†æ¡†æž¶` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€èžåˆ` `é•¿è§†é¢‘ç†è§£` `è§†è§‰éªŒè¯` `ä»¤ç‰Œä¿¡ç”¨åˆ†é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¤§åž‹è§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨æ—¶åºæ„ŸçŸ¥æ–¹é¢æœ‰é™ï¼ŒåŸºäºŽGRPOçš„æ–¹æ³•ä»éš¾ä»¥å¿ å®žå®šä½è§†é¢‘è¯æ®ï¼Œå¯¼è‡´æ—¶åºé”™ä½å’Œå¹»è§‰ã€‚
2. æå‡ºZoom-Zeroæ¡†æž¶ï¼Œé‡‡ç”¨ç²—åˆ°ç»†çš„æ—¶åºæ”¾å¤§æœºåˆ¶ï¼Œç»“åˆæ”¾å¤§ç²¾åº¦å¥–åŠ±å’Œä»¤ç‰Œé€‰æ‹©æ€§ä¿¡ç”¨åˆ†é…ï¼Œæå‡æ—¶åºå®šä½å’Œè§†è§‰éªŒè¯ã€‚
3. åœ¨NExT-GQAå’ŒReXTimeæ•°æ®é›†ä¸Šæ—¶åºå®šä½ç²¾åº¦åˆ†åˆ«æå‡5.2%å’Œ4.6%ï¼Œç­”æ¡ˆå‡†ç¡®çŽ‡æé«˜2.4%ï¼Œé•¿è§†é¢‘ç†è§£å¹³å‡æå‡6.4%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽè§†é¢‘çš„é—®ç­”ä»»åŠ¡æ—¨åœ¨å®šä½è§†é¢‘ä¸­çš„ç›¸å…³æ—¶åºç‰‡æ®µå¹¶ç”Ÿæˆå‡†ç¡®ç­”æ¡ˆï¼Œä½†çŽ°æœ‰å¤§åž‹è§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨æ—¶åºæ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™ã€‚è™½ç„¶åŸºäºŽç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„æ–¹æ³•è¯•å›¾æ”¹è¿›æ—¶åºå®šä½ï¼Œä½†ä»éš¾ä»¥å¿ å®žå°†ç­”æ¡ˆåŸºäºŽç›¸å…³è§†é¢‘è¯æ®ï¼Œå¯¼è‡´æ—¶åºé”™ä½å’Œå¹»è§‰ã€‚æœ¬æ–‡æå‡ºZoom-Zeroï¼Œä¸€ç§ç²—åˆ°ç»†çš„æ¡†æž¶ï¼Œé¦–å…ˆå®šä½æŸ¥è¯¢ç›¸å…³ç‰‡æ®µï¼Œç„¶åŽæ—¶åºæ”¾å¤§åˆ°æœ€æ˜¾è‘—å¸§è¿›è¡Œç»†ç²’åº¦è§†è§‰éªŒè¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°è§£å†³GVQAä»»åŠ¡ä¸­GRPOçš„å±€é™ï¼š(i) æ”¾å¤§ç²¾åº¦å¥–åŠ±ï¼ŒéªŒè¯æ—¶åºå®šä½é¢„æµ‹çš„ä¿çœŸåº¦å¹¶ä¿ƒè¿›å¯¹å®šä½å¸§çš„ç»†ç²’åº¦è§†è§‰éªŒè¯ï¼›(ii) ä»¤ç‰Œé€‰æ‹©æ€§ä¿¡ç”¨åˆ†é…ï¼Œå°†å¥–åŠ±å½’å› äºŽè´Ÿè´£æ—¶åºå®šä½æˆ–ç­”æ¡ˆç”Ÿæˆçš„ä»¤ç‰Œï¼Œç¼“è§£GRPOåœ¨å¤„ç†å¤šæ–¹é¢å¥–åŠ±ä¿¡å·æ—¶çš„é—®é¢˜ã€‚æ‰€ææ–¹æ³•æŽ¨è¿›äº†åŸºäºŽè§†é¢‘çš„é—®ç­”ï¼Œåœ¨NExT-GQAå’ŒReXTimeæ•°æ®é›†ä¸Šåˆ†åˆ«å°†æ—¶åºå®šä½ç²¾åº¦æå‡5.2%å’Œ4.6%ï¼ŒåŒæ—¶å°†å¹³å‡ç­”æ¡ˆå‡†ç¡®çŽ‡æé«˜2.4%ã€‚æ­¤å¤–ï¼ŒæŽ¨ç†è¿‡ç¨‹ä¸­çš„ç²—åˆ°ç»†æ”¾å¤§é€šè¿‡ä¿ç•™å…³é”®è§†è§‰ç»†èŠ‚è€Œä¸æŸå®³å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè¿›ä¸€æ­¥æœ‰ç›ŠäºŽé•¿è§†é¢‘ç†è§£ï¼Œåœ¨é•¿è§†é¢‘åŸºå‡†ä¸Šå¹³å‡æå‡6.4%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

Zoom-Zeroæ˜¯ä¸€ä¸ªç²—åˆ°ç»†çš„æ¡†æž¶ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡ç²—ç²’åº¦å®šä½æ¨¡å—è¯†åˆ«æŸ¥è¯¢ç›¸å…³çš„è§†é¢‘æ—¶åºç‰‡æ®µï¼›ç„¶åŽï¼Œæ—¶åºæ”¾å¤§åˆ°è¿™äº›ç‰‡æ®µä¸­æœ€æ˜¾è‘—çš„å¸§è¿›è¡Œç»†ç²’åº¦è§†è§‰éªŒè¯ï¼Œä»¥ç”Ÿæˆå‡†ç¡®ç­”æ¡ˆã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š(i) æ”¾å¤§ç²¾åº¦å¥–åŠ±æœºåˆ¶ï¼Œç”¨äºŽè¯„ä¼°æ—¶åºå®šä½é¢„æµ‹çš„ä¿çœŸåº¦å¹¶ä¿ƒè¿›å¯¹å®šä½å¸§çš„ç»†ç²’åº¦éªŒè¯ï¼›(ii) ä»¤ç‰Œé€‰æ‹©æ€§ä¿¡ç”¨åˆ†é…ï¼Œå°†å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç²¾ç¡®åˆ†é…ç»™è´Ÿè´£æ—¶åºå®šä½æˆ–ç­”æ¡ˆç”Ÿæˆçš„ä»¤ç‰Œï¼Œä¼˜åŒ–å¤šä»»åŠ¡å­¦ä¹ ã€‚ä¸ŽçŽ°æœ‰åŸºäºŽGRPOçš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´æœ‰æ•ˆåœ°å¤„ç†å¤šæ–¹é¢å¥–åŠ±ä¿¡å·ï¼Œå‡å°‘äº†æ—¶åºé”™ä½å’Œå¹»è§‰é—®é¢˜ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨NExT-GQAæ•°æ®é›†ä¸Šæ—¶åºå®šä½ç²¾åº¦æå‡5.2%ï¼Œåœ¨ReXTimeæ•°æ®é›†ä¸Šæå‡4.6%ï¼›å¹³å‡ç­”æ¡ˆå‡†ç¡®çŽ‡æé«˜2.4%ï¼›é•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šå¹³å‡æ€§èƒ½æå‡6.4%ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽæ™ºèƒ½è§†é¢‘åˆ†æžã€æ•™è‚²è¾…åŠ©ã€å®‰é˜²ç›‘æŽ§å’Œå†…å®¹æ£€ç´¢ç­‰é¢†åŸŸï¼Œé€šè¿‡æå‡è§†é¢‘é—®ç­”çš„æ—¶åºå®šä½ç²¾åº¦ï¼Œå¢žå¼ºå¯¹é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›æ›´å¯é çš„è§†é¢‘è¯æ®æ”¯æŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.

