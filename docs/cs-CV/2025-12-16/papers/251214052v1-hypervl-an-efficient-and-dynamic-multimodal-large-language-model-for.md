---
layout: default
title: HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices
---

# HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14052" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14052v1</a>
  <a href="https://arxiv.org/pdf/2512.14052.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14052v1" onclick="toggleFavorite(this, '2512.14052v1', 'HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: HyperAI Team, Yuchen Liu, Kaiyang Han, Zhiqiang Xia, Yuhang Dong, Chen Song, Kangyu Tang, Jiaming Xu, Xiushi Feng, WenXuan Yu, Li Peng, Mingyang Wang, Kai Wang, Changpeng Yang, Yang Li, Haoyu Lu, Hao Wang, Bingna Xu, Guangyao Liu, Long Huang, Kaibin Guo, Jinyang Wu, Dan Wu, Hongzhen Wang, Peng Zhou, Shuai Nie, Shande Wang, Runyu Shi, Ying Huang

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Technical report of Xiaomi HyperAI Team

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HyperVLï¼šé¢å‘è¾¹ç¼˜è®¾å¤‡çš„é«˜æ•ˆåŠ¨æ€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è¾¹ç¼˜è®¡ç®—` `è§†è§‰åˆ†è¾¨ç‡å‹ç¼©` `åŠ¨æ€æ¨ç†` `åŒé‡ä¸€è‡´æ€§å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§æ¨¡å‹è®¡ç®—å’Œå†…å­˜éœ€æ±‚é«˜ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œè€ŒViTåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸‹å­˜åœ¨å»¶è¿Ÿå’Œå†…å­˜ç“¶é¢ˆã€‚
2. HyperVLé€šè¿‡å›¾åƒåˆ†å—é™åˆ¶å†…å­˜ï¼Œåˆ©ç”¨è§†è§‰åˆ†è¾¨ç‡å‹ç¼©å™¨(VRC)è‡ªé€‚åº”é¢„æµ‹åˆ†è¾¨ç‡ï¼Œå¹¶ä½¿ç”¨åŒé‡ä¸€è‡´æ€§å­¦ä¹ (DCL)å¯¹é½å¤šå°ºåº¦ViTç¼–ç å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHyperVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°åŒç­‰è§„æ¨¡æ¨¡å‹çš„SOTAæ€§èƒ½ï¼Œå¹¶æ˜¾è‘—é™ä½äº†ç§»åŠ¨è®¾å¤‡ä¸Šçš„å»¶è¿Ÿå’ŒåŠŸè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ‹¥æœ‰å¼ºå¤§çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†å…¶é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä½¿å…¶éš¾ä»¥ç›´æ¥éƒ¨ç½²åœ¨ç«¯ä¾§è®¾å¤‡ä¸Šã€‚è™½ç„¶å°å‚æ•°æ¨¡å‹çš„èƒ½åŠ›é€æ¸å¢å¼ºï¼Œä½†æ ‡å‡†çš„Vision Transformer (ViT)ç¼–ç å™¨ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼Œåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸‹ä¼šäº§ç”Ÿè¿‡é«˜çš„å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HyperVLï¼Œä¸€ç§ä¸“ä¸ºç«¯ä¾§æ¨ç†è®¾è®¡çš„é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚HyperVLé‡‡ç”¨å›¾åƒåˆ†å—ç­–ç•¥æ¥é™åˆ¶å³°å€¼å†…å­˜ä½¿ç”¨ï¼Œå¹¶ç»“åˆäº†ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯ï¼š(1) è§†è§‰åˆ†è¾¨ç‡å‹ç¼©å™¨(VRC)ï¼Œè‡ªé€‚åº”åœ°é¢„æµ‹æœ€ä½³ç¼–ç åˆ†è¾¨ç‡ï¼Œä»¥æ¶ˆé™¤å†—ä½™è®¡ç®—ï¼›(2) åŒé‡ä¸€è‡´æ€§å­¦ä¹ (DCL)ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…å¯¹é½å¤šå°ºåº¦ViTç¼–ç å™¨ï¼Œä»è€Œå®ç°å…±äº«LLMä¸‹è§†è§‰åˆ†æ”¯çš„åŠ¨æ€åˆ‡æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHyperVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨åŒç­‰è§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ˜¾è‘—é™ä½äº†çœŸå®ç§»åŠ¨è®¾å¤‡ä¸Šçš„å»¶è¿Ÿå’ŒåŠŸè€—ï¼Œè¯æ˜äº†å…¶åœ¨ç«¯ä¾§å¤šæ¨¡æ€æ¨ç†ä¸­çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶ï¼Œç”±äºè®¡ç®—å’Œå†…å­˜èµ„æºæœ‰é™è€Œé¢ä¸´çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºVision Transformer (ViT)çš„è§†è§‰ç¼–ç å™¨ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒè¾“å…¥æ—¶ä¼šäº§ç”Ÿè¿‡é«˜çš„å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€è°ƒæ•´è§†è§‰ç¼–ç çš„åˆ†è¾¨ç‡ï¼Œé¿å…å¯¹æ‰€æœ‰å›¾åƒåŒºåŸŸéƒ½è¿›è¡Œé«˜åˆ†è¾¨ç‡ç¼–ç ï¼Œä»è€Œå‡å°‘è®¡ç®—é‡å’Œå†…å­˜å ç”¨ã€‚åŒæ—¶ï¼Œé€šè¿‡åŒé‡ä¸€è‡´æ€§å­¦ä¹ ï¼Œç¡®ä¿ä¸åŒåˆ†è¾¨ç‡çš„è§†è§‰ç¼–ç å™¨èƒ½å¤Ÿä¸è¯­è¨€æ¨¡å‹ä¿æŒä¸€è‡´çš„è¯­ä¹‰è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHyperVLçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬å›¾åƒåˆ†å—æ¨¡å—ã€è§†è§‰åˆ†è¾¨ç‡å‹ç¼©å™¨(VRC)ã€å¤šå°ºåº¦ViTç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ã€‚å›¾åƒé¦–å…ˆè¢«åˆ†å‰²æˆå°å—ï¼ŒVRCæ ¹æ®å›¾åƒå—çš„å¤æ‚åº¦è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„ç¼–ç åˆ†è¾¨ç‡ã€‚ç„¶åï¼Œå¤šå°ºåº¦ViTç¼–ç å™¨åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹æå–è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡åŒé‡ä¸€è‡´æ€§å­¦ä¹ è¿›è¡Œå¯¹é½ã€‚æœ€åï¼Œè¯­è¨€æ¨¡å‹å°†è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ä¿¡æ¯èåˆï¼Œè¿›è¡Œæ¨ç†å’Œç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºè§†è§‰åˆ†è¾¨ç‡å‹ç¼©å™¨(VRC)å’ŒåŒé‡ä¸€è‡´æ€§å­¦ä¹ (DCL)ã€‚VRCèƒ½å¤Ÿæ ¹æ®å›¾åƒå†…å®¹åŠ¨æ€åœ°é€‰æ‹©æœ€ä½³ç¼–ç åˆ†è¾¨ç‡ï¼Œé¿å…äº†å¯¹æ‰€æœ‰åŒºåŸŸéƒ½è¿›è¡Œé«˜åˆ†è¾¨ç‡ç¼–ç çš„å†—ä½™è®¡ç®—ã€‚DCLåˆ™é€šè¿‡åœ¨ä¸åŒåˆ†è¾¨ç‡çš„è§†è§‰ç‰¹å¾ä¹‹é—´å»ºç«‹ä¸€è‡´æ€§çº¦æŸï¼Œä¿è¯äº†è§†è§‰ç¼–ç å™¨åœ¨åŠ¨æ€åˆ‡æ¢åˆ†è¾¨ç‡æ—¶çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šVRCä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œæ¥é¢„æµ‹æ¯ä¸ªå›¾åƒå—çš„æœ€ä½³ç¼–ç åˆ†è¾¨ç‡ã€‚DCLåŒ…å«ä¸¤ä¸ªä¸€è‡´æ€§çº¦æŸï¼šä¸€æ˜¯ä¸åŒåˆ†è¾¨ç‡çš„è§†è§‰ç‰¹å¾ä¸è¯­è¨€æ¨¡å‹è¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§ï¼ŒäºŒæ˜¯ä¸åŒåˆ†è¾¨ç‡çš„è§†è§‰ç‰¹å¾ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æŸå¤±å‡½æ•°ç»“åˆäº†äº¤å‰ç†µæŸå¤±å’ŒKLæ•£åº¦æŸå¤±ï¼Œä»¥ä¼˜åŒ–VRCå’Œå¤šå°ºåº¦ViTç¼–ç å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HyperVLåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸åŒç­‰è§„æ¨¡æ¨¡å‹ç›¸æ¯”æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨çœŸå®ç§»åŠ¨è®¾å¤‡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHyperVLæ˜¾è‘—é™ä½äº†å»¶è¿Ÿå’ŒåŠŸè€—ï¼Œä¾‹å¦‚åœ¨XXXæ•°æ®é›†ä¸Šï¼Œå»¶è¿Ÿé™ä½äº†XX%ï¼ŒåŠŸè€—é™ä½äº†YY%ã€‚è¿™äº›ç»“æœè¯æ˜äº†HyperVLåœ¨ç«¯ä¾§å¤šæ¨¡æ€æ¨ç†ä¸­çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HyperVLé€‚ç”¨äºå„ç§éœ€è¦åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œå¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼ŒHyperVLä½¿å¾—è¿™äº›åº”ç”¨èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å®æ—¶çš„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥çš„å‘å±•æ–¹å‘åŒ…æ‹¬è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ç»“æ„ã€æ¢ç´¢æ›´æœ‰æ•ˆçš„åŠ¨æ€åˆ†è¾¨ç‡è°ƒæ•´ç­–ç•¥ï¼Œä»¥åŠæ”¯æŒæ›´å¤šæ¨¡æ€çš„è¾“å…¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.

