---
layout: default
title: CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer
---

# CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.14560" target="_blank" class="toolbar-btn">arXiv: 2512.14560v1</a>
    <a href="https://arxiv.org/pdf/2512.14560.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14560v1" 
            onclick="toggleFavorite(this, '2512.14560v1', 'CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xianwei Cao, Dou Quan, Shuang Wang, Ning Huyan, Wei Wang, Yunan Li, Licheng Jiao

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

**Â§áÊ≥®**: 16 pages, 6 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CLNetÔºåÈÄöËøáË∑®ËßÜËßíÂØπÂ∫îÂÖ≥Á≥ªÂ¢ûÂº∫ÂõæÂÉèÊ£ÄÁ¥¢Âú∞ÁêÜÂÆö‰Ωç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ë∑®ËßÜËßíÂú∞ÁêÜÂÆö‰Ωç` `ÂõæÂÉèÊ£ÄÁ¥¢` `ÂØπÂ∫îÂÖ≥Á≥ªÂ≠¶‰π†` `ÁâπÂæÅÂØπÈΩê` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâË∑®ËßÜËßíÂú∞ÁêÜÂÆö‰ΩçÊñπÊ≥ïÈöæ‰ª•Âª∫Ê®°ÊòæÂºèÁöÑÁ©∫Èó¥ÂØπÂ∫îÂÖ≥Á≥ªÔºåÈôêÂà∂‰∫ÜÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ
2. CLNetÈÄöËøáÁ•ûÁªèÂØπÂ∫îÂõæ„ÄÅÈùûÁ∫øÊÄßÂµåÂÖ•ËΩ¨Êç¢Âô®ÂíåÂÖ®Â±ÄÁâπÂæÅÈáçÊ†°ÂáÜÊ®°ÂùóÔºåÊòæÂºèÂú∞Â≠¶‰π†ÂíåÂà©Áî®Ë∑®ËßÜËßíÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ
3. Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÈ™åË°®ÊòéÔºåCLNet ËææÂà∞‰∫Ü SOTA ÊÄßËÉΩÔºåÂπ∂ÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØËß£ÈáäÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫‰∫éÂõæÂÉèÊ£ÄÁ¥¢ÁöÑË∑®ËßÜËßíÂú∞ÁêÜÂÆö‰Ωç(IRCVGL)ÊñπÊ≥ïÔºåÊó®Âú®ÂåπÈÖç‰ªéÊòæËëó‰∏çÂêåËßÜËßíÊçïËé∑ÁöÑÂõæÂÉèÔºå‰æãÂ¶ÇÂç´ÊòüÂõæÂÉèÂíåË°óÊôØÂõæÂÉè„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂ≠¶‰π†È≤ÅÊ£íÁöÑÂÖ®Â±ÄË°®Á§∫ÊàñÈöêÂºèÁöÑÁâπÂæÅÂØπÈΩêÔºå‰ΩÜÈÄöÂ∏∏Êó†Ê≥ïÂª∫Ê®°ÂØπ‰∫éÁ≤æÁ°ÆÂÆö‰ΩçËá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòæÂºèÁ©∫Èó¥ÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫CLNetÁöÑÂØπÂ∫îÂÖ≥Á≥ªÊÑüÁü•ÁâπÂæÅÁªÜÂåñÊ°ÜÊû∂ÔºåÂÆÉÊòæÂºèÂú∞Âº•Âêà‰∫Ü‰∏çÂêåËßÜËßí‰πãÈó¥ÁöÑËØ≠‰πâÂíåÂá†‰ΩïÂ∑ÆË∑ù„ÄÇCLNetÂ∞ÜËßÜËßíÂØπÈΩêËøáÁ®ãÂàÜËß£‰∏∫‰∏â‰∏™ÂèØÂ≠¶‰π†‰∏î‰∫íË°•ÁöÑÊ®°ÂùóÔºöÁ•ûÁªèÂØπÂ∫îÂõæ(NCM)ÔºåÈÄöËøáÊΩúÂú®ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂú∫Âú®Á©∫Èó¥‰∏äÂØπÈΩêË∑®ËßÜËßíÁâπÂæÅÔºõÈùûÁ∫øÊÄßÂµåÂÖ•ËΩ¨Êç¢Âô®(NEC)Ôºå‰ΩøÁî®Âü∫‰∫éMLPÁöÑËΩ¨Êç¢ÈáçÊñ∞Êò†Â∞ÑË∑®ËßÜËßíÁöÑÁâπÂæÅÔºõ‰ª•ÂèäÂÖ®Â±ÄÁâπÂæÅÈáçÊ†°ÂáÜ(GFR)Ê®°ÂùóÔºåËØ•Ê®°ÂùóÂú®Â≠¶‰π†Âà∞ÁöÑÁ©∫Èó¥Á∫øÁ¥¢ÁöÑÊåáÂØº‰∏ãÔºåÈáçÊñ∞Âä†ÊùÉ‰ø°ÊÅØ‰∏∞ÂØåÁöÑÁâπÂæÅÈÄöÈÅì„ÄÇÊâÄÊèêÂá∫ÁöÑCLNetÂèØ‰ª•ËÅîÂêàÊçïËé∑È´òÂ±ÇËØ≠‰πâÂíåÁªÜÁ≤íÂ∫¶ÁöÑÂØπÈΩê„ÄÇÂú®CVUSA„ÄÅCVACT„ÄÅVIGORÂíåUniversity-1652Âõõ‰∏™ÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑCLNetÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂêåÊó∂Êèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÂèØËß£ÈáäÊÄßÂíåÊ≥õÂåñÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöË∑®ËßÜËßíÂú∞ÁêÜÂÆö‰ΩçÊó®Âú®ÂåπÈÖçÊù•Ëá™‰∏çÂêåËßÜËßíÁöÑÂõæÂÉèÔºå‰æãÂ¶ÇÂç´ÊòüÂõæÂÉèÂíåË°óÊôØÂõæÂÉè„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂ≠¶‰π†È≤ÅÊ£íÁöÑÂÖ®Â±ÄÁâπÂæÅÊàñÈöêÂºèÂú∞ÂØπÈΩêÁâπÂæÅÔºå‰ΩÜÂøΩÁï•‰∫ÜÊòæÂºèÁ©∫Èó¥ÂØπÂ∫îÂÖ≥Á≥ªÔºåÂØºËá¥ÂÆö‰ΩçÁ≤æÂ∫¶ÂèóÈôê„ÄÇËøô‰∫õÊñπÊ≥ïÈöæ‰ª•Â§ÑÁêÜËßÜËßíÂ∑ÆÂºÇÂ∏¶Êù•ÁöÑÂá†‰ΩïÂíåËØ≠‰πâÂèòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCLNetÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊòæÂºèÂú∞Âª∫Ê®°Ë∑®ËßÜËßíÂõæÂÉè‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÊù•ÊèêÂçáÂú∞ÁêÜÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÆÉÂ∞ÜËßÜËßíÂØπÈΩêËøáÁ®ãÂàÜËß£‰∏∫Â§ö‰∏™ÂèØÂ≠¶‰π†ÁöÑÊ®°ÂùóÔºåÂàÜÂà´Ë¥üË¥£Á©∫Èó¥ÂØπÈΩê„ÄÅÁâπÂæÅËΩ¨Êç¢ÂíåÁâπÂæÅÈáçÊ†°ÂáÜÔºå‰ªéËÄåÂº•Âêà‰∏çÂêåËßÜËßí‰πãÈó¥ÁöÑËØ≠‰πâÂíåÂá†‰ΩïÂ∑ÆË∑ù„ÄÇËøôÁßçÊòæÂºèÂª∫Ê®°ÂØπÂ∫îÂÖ≥Á≥ªÁöÑÊñπÂºèËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®ÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥‰ø°ÊÅØÔºåÊèêÈ´òÂÆö‰ΩçÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCLNetÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÁ•ûÁªèÂØπÂ∫îÂõæ(NCM)„ÄÅÈùûÁ∫øÊÄßÂµåÂÖ•ËΩ¨Êç¢Âô®(NEC)ÂíåÂÖ®Â±ÄÁâπÂæÅÈáçÊ†°ÂáÜ(GFR)„ÄÇÈ¶ñÂÖàÔºåNCMÈÄöËøáÂ≠¶‰π†ÊΩúÂú®ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂú∫ÔºåÂú®Á©∫Èó¥‰∏äÂØπÈΩêË∑®ËßÜËßíÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºåNEC‰ΩøÁî®Âü∫‰∫éMLPÁöÑËΩ¨Êç¢ÔºåÂ∞ÜÁâπÂæÅÈáçÊñ∞Êò†Â∞ÑÂà∞Áªü‰∏ÄÁöÑËßÜËßíÁ©∫Èó¥„ÄÇÊúÄÂêéÔºåGFRÊ®°ÂùóÊ†πÊçÆÂ≠¶‰π†Âà∞ÁöÑÁ©∫Èó¥Á∫øÁ¥¢ÔºåÂØπÁâπÂæÅÈÄöÈÅìËøõË°åÈáçÂä†ÊùÉÔºåÁ™ÅÂá∫‰ø°ÊÅØ‰∏∞ÂØåÁöÑÁâπÂæÅ„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈÄöËøáÁ´ØÂà∞Á´ØÁöÑÊñπÂºèËøõË°åËÆ≠ÁªÉÔºå‰ª•‰ºòÂåñË∑®ËßÜËßíÂõæÂÉèÂåπÈÖçÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCLNetÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊòæÂºèÂú∞Âª∫Ê®°Ë∑®ËßÜËßíÂõæÂÉè‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ‰∏é‰ª•ÂæÄ‰æùËµñÂÖ®Â±ÄÁâπÂæÅÊàñÈöêÂºèÂØπÈΩêÁöÑÊñπÊ≥ï‰∏çÂêåÔºåCLNetÈÄöËøáNCMÊ®°ÂùóÂ≠¶‰π†Á©∫Èó¥ÂØπÂ∫îÂÖ≥Á≥ªÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Â§ÑÁêÜËßÜËßíÂ∑ÆÂºÇÂ∏¶Êù•ÁöÑÂá†‰ΩïÂíåËØ≠‰πâÂèòÂåñ„ÄÇÊ≠§Â§ñÔºåNECÂíåGFRÊ®°ÂùóËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÁâπÂæÅÁöÑË°®ËææËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºåÊèêÂçá‰∫ÜÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöNCMÊ®°Âùó‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂ≠¶‰π†Ë∑®ËßÜËßíÂõæÂÉè‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂú∫„ÄÇNECÊ®°ÂùóÈááÁî®Â§öÂ±ÇÊÑüÁü•Êú∫(MLP)ËøõË°åÈùûÁ∫øÊÄßÁâπÂæÅËΩ¨Êç¢„ÄÇGFRÊ®°Âùó‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÂØπÁâπÂæÅÈÄöÈÅìËøõË°åÈáçÂä†ÊùÉ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂåπÈÖçÊçüÂ§±ÂíåÂØπÂ∫îÂÖ≥Á≥ªÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÊ†πÊçÆ‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CLNetÂú®CVUSA„ÄÅCVACT„ÄÅVIGORÂíåUniversity-1652Âõõ‰∏™ÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏äÈÉΩÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®CVUSAÊï∞ÊçÆÈõÜ‰∏äÔºåCLNetÁöÑRecall@1ÊåáÊ†áÁõ∏ÊØî‰∫é‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫ÜÊòæËëóÁöÑÁôæÂàÜÊØî„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCLNetËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜËßÜËßíÂ∑ÆÂºÇÔºåÊèêÈ´òË∑®ËßÜËßíÂõæÂÉèÂåπÈÖçÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CLNetÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂüéÂ∏ÇËßÑÂàí„ÄÅÁéØÂ¢ÉÁõëÊµãÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Âà©Áî®Âç´ÊòüÂõæÂÉèÂíåË°óÊôØÂõæÂÉèËøõË°åÁ≤æÁ°ÆÂÆö‰ΩçÔºåÂ∏ÆÂä©Ëá™Âä®È©æÈ©∂ËΩ¶ËæÜÂú®Â§çÊùÇÁöÑÂüéÂ∏ÇÁéØÂ¢É‰∏≠ÂÆâÂÖ®Ë°åÈ©∂„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÊûÑÂª∫Â§ßËßÑÊ®°ÁöÑÂú∞ÁêÜ‰ø°ÊÅØÁ≥ªÁªüÔºå‰∏∫ÂüéÂ∏ÇËßÑÂàíÂíåÁÆ°ÁêÜÊèê‰æõÊîØÊåÅ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

