---
layout: default
title: MMGR: Multi-Modal Generative Reasoning
---

# MMGR: Multi-Modal Generative Reasoning

**arXiv**: [2512.14691v1](https://arxiv.org/abs/2512.14691) | [PDF](https://arxiv.org/pdf/2512.14691.pdf)

**ä½œè€…**: Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Xiao Wen, Jiuxiang Gu, Nanyun Peng, Junjie Hu

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: work in progress

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMGRå¤šæ¨¡æ€ç”ŸæˆæŽ¨ç†è¯„ä¼°æ¡†æž¶ï¼Œä»¥è§£å†³çŽ°æœ‰è§†é¢‘åŸºç¡€æ¨¡åž‹åœ¨ç‰©ç†ã€é€»è¾‘å’Œç©ºé—´çº¦æŸæ–¹é¢ç¼ºä¹å¯é è¯„ä¼°çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŽ¨ç†è¯„ä¼°` `è§†é¢‘åŸºç¡€æ¨¡åž‹` `ç”Ÿæˆä¸–ç•Œæ¨¡åž‹` `ç‰©ç†å¸¸è¯†æŽ¨ç†` `å…·èº«å¯¼èˆª` `æŠ½è±¡æŽ¨ç†` `åŸºå‡†æµ‹è¯•` `å› æžœæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘åŸºç¡€æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚FVDï¼‰è¿‡äºŽå…³æ³¨æ„ŸçŸ¥è´¨é‡ï¼Œå¿½è§†äº†ç‰©ç†ã€é€»è¾‘å’Œç©ºé—´æŽ¨ç†å¤±è´¥ï¼Œå¯¼è‡´æ¨¡åž‹ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨çš„å¯é æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºMMGRæ¡†æž¶ï¼ŒåŸºäºŽäº”ç§æŽ¨ç†èƒ½åŠ›ï¼ˆç‰©ç†ã€é€»è¾‘ã€3Dç©ºé—´ã€2Dç©ºé—´ã€æ—¶åºï¼‰æž„å»ºåŽŸåˆ™æ€§è¯„ä¼°ï¼Œè¦†ç›–æŠ½è±¡æŽ¨ç†ã€å…·èº«å¯¼èˆªå’Œç‰©ç†å¸¸è¯†ä¸‰å¤§é¢†åŸŸã€‚
3. åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œä¸»æµæ¨¡åž‹åœ¨ç‰©ç†å¸¸è¯†ä»»åŠ¡ä¸Šè¡¨çŽ°ä¸­ç­‰ï¼Œä½†åœ¨æŠ½è±¡æŽ¨ç†ï¼ˆå¦‚ARC-AGIå‡†ç¡®çŽ‡ä½ŽäºŽ10%ï¼‰å’Œé•¿æ—¶ç¨‹ç©ºé—´è§„åˆ’ä¸Šè¡¨çŽ°ä¸ä½³ï¼Œæ­ç¤ºäº†å…³é”®å±€é™æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘åŸºç¡€æ¨¡åž‹èƒ½å¤Ÿç”Ÿæˆè§†è§‰é€¼çœŸä¸”æ—¶åºè¿žè´¯çš„å†…å®¹ï¼Œä½†å…¶ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨çš„å¯é æ€§å–å†³äºŽæ˜¯å¦æ•æ‰äº†ç‰©ç†ã€é€»è¾‘å’Œç©ºé—´çº¦æŸã€‚çŽ°æœ‰æŒ‡æ ‡å¦‚å¼—é›·æ­‡è§†é¢‘è·ç¦»ï¼ˆFVDï¼‰å¼ºè°ƒæ„ŸçŸ¥è´¨é‡ï¼Œå´å¿½è§†äº†æŽ¨ç†å¤±è´¥ï¼ŒåŒ…æ‹¬è¿åå› æžœå…³ç³»ã€ç‰©ç†è§„å¾‹å’Œå…¨å±€ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MMGRï¼ˆå¤šæ¨¡æ€ç”ŸæˆæŽ¨ç†è¯„ä¼°ä¸ŽåŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºŽäº”ç§æŽ¨ç†èƒ½åŠ›çš„åŽŸåˆ™æ€§è¯„ä¼°æ¡†æž¶ï¼šç‰©ç†æŽ¨ç†ã€é€»è¾‘æŽ¨ç†ã€3Dç©ºé—´æŽ¨ç†ã€2Dç©ºé—´æŽ¨ç†å’Œæ—¶åºæŽ¨ç†ã€‚MMGRåœ¨ä¸‰ä¸ªé¢†åŸŸè¯„ä¼°ç”ŸæˆæŽ¨ç†ï¼šæŠ½è±¡æŽ¨ç†ï¼ˆARC-AGIã€æ•°ç‹¬ï¼‰ã€å…·èº«å¯¼èˆªï¼ˆçœŸå®žä¸–ç•Œ3Då¯¼èˆªä¸Žå®šä½ï¼‰å’Œç‰©ç†å¸¸è¯†ï¼ˆè¿åŠ¨å’Œç»„åˆäº¤äº’ï¼‰ã€‚MMGRåº”ç”¨ç»†ç²’åº¦æŒ‡æ ‡ï¼Œè¦æ±‚è§†é¢‘å’Œå›¾åƒç”Ÿæˆåœ¨æ•´ä½“ä¸Šæ­£ç¡®ã€‚æˆ‘ä»¬å¯¹é¢†å…ˆçš„è§†é¢‘æ¨¡åž‹ï¼ˆVeo-3ã€Sora-2ã€Wan-2.2ï¼‰å’Œå›¾åƒæ¨¡åž‹ï¼ˆNano-bananaã€Nano-banana Proã€GPT-4o-imageã€Qwen-imageï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†è·¨é¢†åŸŸçš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚æ¨¡åž‹åœ¨ç‰©ç†å¸¸è¯†ä»»åŠ¡ä¸Šè¡¨çŽ°ä¸­ç­‰ï¼Œä½†åœ¨æŠ½è±¡æŽ¨ç†ä¸Šè¡¨çŽ°ä¸ä½³ï¼ˆARC-AGIå‡†ç¡®çŽ‡ä½ŽäºŽ10%ï¼‰ï¼Œå¹¶åœ¨å…·èº«è®¾ç½®ä¸­çš„é•¿æ—¶ç¨‹ç©ºé—´è§„åˆ’ä¸Šé‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬çš„åˆ†æžçªå‡ºäº†å½“å‰æ¨¡åž‹çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬è¿‡åº¦ä¾èµ–æ„ŸçŸ¥æ•°æ®ã€å…¨å±€çŠ¶æ€ä¸€è‡´æ€§å¼±ï¼Œä»¥åŠç›®æ ‡å‡½æ•°å¥–åŠ±è§†è§‰åˆç†æ€§è€Œéžå› æžœæ­£ç¡®æ€§ã€‚MMGRæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯Šæ–­åŸºå‡†ï¼Œå¹¶ä¸ºæŽ¨ç†æ„ŸçŸ¥çš„ç”Ÿæˆä¸–ç•Œæ¨¡åž‹æŒ‡æ˜Žäº†è·¯å¾„ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

MMGRæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç”ŸæˆæŽ¨ç†è¯„ä¼°æ¡†æž¶ï¼Œæ•´ä½“æ¡†æž¶åŸºäºŽäº”ç§æ ¸å¿ƒæŽ¨ç†èƒ½åŠ›ï¼ˆç‰©ç†ã€é€»è¾‘ã€3Dç©ºé—´ã€2Dç©ºé—´ã€æ—¶åºï¼‰ï¼Œè®¾è®¡ç»†ç²’åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œè¦æ±‚è§†é¢‘å’Œå›¾åƒç”Ÿæˆåœ¨æ•´ä½“ä¸Šæ­£ç¡®ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†æŽ¨ç†èƒ½åŠ›ç³»ç»ŸåŒ–åˆ†ç±»ï¼Œå¹¶åº”ç”¨äºŽä¸‰ä¸ªå…·ä½“é¢†åŸŸï¼ˆæŠ½è±¡æŽ¨ç†ã€å…·èº«å¯¼èˆªã€ç‰©ç†å¸¸è¯†ï¼‰ï¼Œé€šè¿‡ç»Ÿä¸€åŸºå‡†è¿›è¡Œè·¨æ¨¡åž‹æ¯”è¾ƒã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚FVDä¾§é‡äºŽæ„ŸçŸ¥è´¨é‡è¯„ä¼°ï¼Œè€ŒMMGRå¼ºè°ƒæŽ¨ç†æ­£ç¡®æ€§ï¼Œèƒ½å¤Ÿè¯Šæ–­æ¨¡åž‹åœ¨å› æžœå…³ç³»ã€ç‰©ç†çº¦æŸå’Œå…¨å±€ä¸€è‡´æ€§æ–¹é¢çš„å¤±è´¥ï¼Œä»Žè€Œæä¾›æ›´å…¨é¢çš„è¯„ä¼°è§†è§’ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œä¸»æµè§†é¢‘å’Œå›¾åƒæ¨¡åž‹åœ¨ç‰©ç†å¸¸è¯†ä»»åŠ¡ä¸Šè¡¨çŽ°ä¸­ç­‰ï¼Œä½†åœ¨æŠ½è±¡æŽ¨ç†ä»»åŠ¡ï¼ˆå¦‚ARC-AGIï¼‰ä¸Šå‡†ç¡®çŽ‡ä½ŽäºŽ10%ï¼Œä¸”åœ¨å…·èº«å¯¼èˆªçš„é•¿æ—¶ç¨‹ç©ºé—´è§„åˆ’ä¸­è¡¨çŽ°ä¸ä½³ï¼Œæ­ç¤ºäº†æ¨¡åž‹åœ¨å…¨å±€çŠ¶æ€ä¸€è‡´æ€§å’Œå› æžœæŽ¨ç†æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè§†é¢‘ç”Ÿæˆæ¨¡åž‹çš„è´¨é‡è¯„ä¼°ä¸Žä¼˜åŒ–ã€å…·èº«æ™ºèƒ½ç³»ç»Ÿçš„å¯¼èˆªä¸Žè§„åˆ’ã€ä»¥åŠç‰©ç†æ¨¡æ‹Ÿå’Œæ¸¸æˆå¼€å‘ä¸­çš„ä¸–ç•Œå»ºæ¨¡ã€‚å®ƒä¸ºå¼€å‘æ›´å¯é çš„ç”Ÿæˆä¸–ç•Œæ¨¡åž‹æä¾›äº†è¯Šæ–­å·¥å…·ï¼Œæœ‰åŠ©äºŽæå‡AIåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æŽ¨ç†èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.

