---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14698" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14698v1</a>
  <a href="https://arxiv.org/pdf/2512.14698.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14698v1" onclick="toggleFavorite(this, '2512.14698v1', 'TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTimeLensä»¥æå‡è§†é¢‘æ—¶é—´å®šä½çš„å‡†ç¡®æ€§ä¸å¯é æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ—¶é—´å®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®è´¨é‡æå‡` `ç®—æ³•è®¾è®¡ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹è¯„ä¼°` `è§†é¢‘ç†è§£` `è‡ªåŠ¨æ³¨é‡Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ—¶é—´å®šä½æ–¹æ³•åœ¨æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´è¯„ä¼°æ ‡å‡†ä¸å¯é ã€‚
2. è®ºæ–‡æå‡ºTimeLensï¼Œé€šè¿‡é‡æ–°æ³¨é‡Šæ•°æ®é›†å’Œä¼˜åŒ–ç®—æ³•è®¾è®¡ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨VTGä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeLensæ¨¡å‹åœ¨VTGæ€§èƒ½ä¸Šè¶…è¶Šäº†å¼€æºå’Œä¸€äº›ä¸“æœ‰æ¨¡å‹ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æå‡å¹…åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶æœªæå‡ºæ–°æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘æ—¶é—´å®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç®€å•ã€æ¸è¿›ä¸”é‡è¦çš„åŸºå‡†ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å®ƒä»¬ä»¥é€‚åº”VTGçš„æ–¹æ¡ˆä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬é¦–å…ˆæ­ç¤ºäº†ç°æœ‰VTGåŸºå‡†ä¸­çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼ŒåŒ…å«ç»è¿‡ä¸¥æ ¼è´¨é‡æ ‡å‡†é‡æ–°æ³¨é‡Šçš„ä¸‰ä¸ªæµè¡ŒåŸºå‡†ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œä¸ä¼ ç»ŸåŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹çš„é‡æ–°æ’åå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œç¡®è®¤äº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªåŠ¨é‡æ–°æ³¨é‡Šç®¡é“è§£å†³äº†å™ªå£°è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œç”Ÿæˆäº†å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®é›†TimeLens-100Kã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ç®—æ³•è®¾è®¡åŸåˆ™ï¼Œæå‡ºäº†ä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆçš„å®è·µï¼Œæœ€ç»ˆå½¢æˆäº†åœ¨å¼€æºæ¨¡å‹ä¸­å…·æœ‰æœ€å…ˆè¿›VTGæ€§èƒ½çš„TimeLensæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘æ—¶é—´å®šä½ï¼ˆVTGï¼‰ä¸­çš„æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°æ ‡å‡†å’Œè®­ç»ƒæ•°æ®çš„å¯é æ€§ä¸Šå­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½å’Œåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å»ºç«‹é«˜è´¨é‡çš„æ•°æ®é›†å’Œä¼˜åŒ–ç®—æ³•è®¾è®¡ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨VTGä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡é‡æ–°æ³¨é‡Šç°æœ‰åŸºå‡†å’Œå¼•å…¥æ–°çš„è®­ç»ƒæ•°æ®é›†ï¼Œç¡®ä¿æ¨¡å‹è®­ç»ƒçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°æ®è´¨é‡æå‡å’Œç®—æ³•è®¾è®¡ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œé€šè¿‡TimeLens-Benchå’ŒTimeLens-100Kæä¾›é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨æ–°çš„ç®—æ³•è®¾è®¡åŸåˆ™ï¼Œå¦‚äº¤é”™æ–‡æœ¬ç¼–ç å’Œæ— æ€è€ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒèŒƒå¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸¥æ ¼çš„é‡æ–°æ³¨é‡Šæ ‡å‡†å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯„ä¼°å¯é æ€§ã€‚æ­¤å¤–ï¼ŒRLVRè®­ç»ƒèŒƒå¼çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å¾—å¯éªŒè¯çš„å¥–åŠ±ï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†äº¤é”™æ–‡æœ¬ç¼–ç ä»¥å¢å¼ºæ—¶é—´è¡¨ç¤ºçš„èƒ½åŠ›ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†RLVRçš„å¥–åŠ±æœºåˆ¶ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„å±‚æ¬¡å’Œè¿æ¥æ–¹å¼ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆæœå’Œæ¨ç†é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTimeLensæ¨¡å‹åœ¨è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å¼€æºæ¨¡å‹å’Œä¸€äº›ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-5å’ŒGemini-2.5-Flashã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼ŒéªŒè¯äº†æ–°æ•°æ®é›†å’Œç®—æ³•è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨è§†é¢‘æ‘˜è¦ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è§†é¢‘æ—¶é—´å®šä½çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºè§†é¢‘å†…å®¹æ£€ç´¢ã€äº‹ä»¶æ£€æµ‹ç­‰ä»»åŠ¡æä¾›æ›´å¯é çš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒTimeLensçš„ç ”ç©¶æˆæœæœ‰æœ›åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œè§†é¢‘åˆ†æé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

