---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14698" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14698v1</a>
  <a href="https://arxiv.org/pdf/2512.14698.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14698v1" onclick="toggleFavorite(this, '2512.14698v1', 'TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TimeLensï¼šåˆ©ç”¨å¤šæ¨¡æ€LLMé‡æ–°æ€è€ƒè§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ï¼Œæ„å»ºé«˜è´¨é‡åŸºçº¿ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€LLM` `æ•°æ®è´¨é‡` `å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘ç†è§£` `åŸºå‡†æµ‹è¯•` `æ—¶é—´è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ—¶åºå®šä½åŸºå‡†æµ‹è¯•å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹è¯„ä¼°ç»“æœä¸å¯é ï¼Œé˜»ç¢äº†æœ‰æ•ˆæ–¹æ³•çš„å‘å±•ã€‚
2. TimeLensé€šè¿‡é«˜è´¨é‡æ•°æ®æ„å»ºå’Œç®—æ³•è®¾è®¡ï¼Œç³»ç»Ÿæ€§åœ°æå‡å¤šæ¨¡æ€LLMåœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
3. TimeLensæ¨¡å‹åœ¨å¼€æºæ¨¡å‹ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è§†é¢‘æ—¶åºå®šä½æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶éæå‡ºä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£ä¸­çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½(VTG)å»ºç«‹äº†ä¸€ä¸ªç›´æ¥ã€å¢é‡ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å®ƒä»¬ä»¥é€‚åº”VTGçš„æ–¹æ³•ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†TimeLensï¼Œå¯¹æ„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMè¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œä¸»è¦å…³æ³¨æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæ­ç¤ºäº†ç°æœ‰VTGåŸºå‡†æµ‹è¯•ä¸­çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼Œå…¶ä¸­åŒ…å«ç»è¿‡ä¸¥æ ¼è´¨é‡æ ‡å‡†é‡æ–°æ³¨é‡Šçš„ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸ä¼ ç»ŸåŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹é‡æ–°æ’åºå‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œè¯å®äº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ³¨é‡Šç®¡é“è§£å†³äº†å˜ˆæ‚çš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œä»è€Œäº§ç”Ÿäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†TimeLens-100Kã€‚åœ¨æ•°æ®åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹ç®—æ³•è®¾è®¡åŸåˆ™è¿›è¡Œäº†æ·±å…¥æ¢ç´¢ï¼Œäº§ç”Ÿäº†ä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆä½†é«˜æ•ˆçš„å®è·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºæ—¶é—´è¡¨ç¤ºçš„äº¤é”™æ–‡æœ¬ç¼–ç ã€ä¸€ç§æ— éœ€æ€è€ƒçš„å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)æ–¹æ³•ä½œä¸ºè®­ç»ƒèŒƒä¾‹ï¼Œä»¥åŠä¸ºRLVRè®­ç»ƒç²¾å¿ƒè®¾è®¡çš„æ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆäº§ç”Ÿäº†TimeLensæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—MLLMï¼Œåœ¨å¼€æºæ¨¡å‹ä¸­å…·æœ‰æœ€å…ˆè¿›çš„VTGæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡å‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹éƒ½å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨ä»è§†é¢‘ä¸­æ‰¾åˆ°ä¸ç»™å®šæ–‡æœ¬æŸ¥è¯¢ç›¸å¯¹åº”çš„æ—¶é—´ç‰‡æ®µã€‚ç°æœ‰æ–¹æ³•å—é™äºä½è´¨é‡çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ï¼Œä¸”éš¾ä»¥å…¬å¹³æ¯”è¾ƒä¸åŒæ–¹æ³•çš„ä¼˜åŠ£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTimeLensçš„æ ¸å¿ƒæ€è·¯æ˜¯â€œæ•°æ®ä¸ºç‹â€ï¼Œé¦–å…ˆæ„å»ºé«˜è´¨é‡çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ï¼Œç„¶åæ¢ç´¢æœ‰æ•ˆçš„ç®—æ³•è®¾è®¡ï¼Œä»è€Œæå‡å¤šæ¨¡æ€LLMåœ¨VTGä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡é«˜è´¨é‡çš„æ•°æ®ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´å‡†ç¡®çš„æ—¶åºå®šä½çŸ¥è¯†ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeLensçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š1) TimeLens-Benchï¼šé«˜è´¨é‡çš„VTGè¯„ä¼°åŸºå‡†ï¼Œé€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶æµç¨‹é‡æ–°æ ‡æ³¨ç°æœ‰æ•°æ®é›†ï¼›2) TimeLens-100Kï¼šå¤§è§„æ¨¡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨é‡æ–°æ ‡æ³¨æµç¨‹æ¸…æ´—å™ªå£°æ•°æ®ï¼›3) TimeLensæ¨¡å‹ï¼šåŸºäºå¤šæ¨¡æ€LLMï¼Œé‡‡ç”¨äº¤é”™æ–‡æœ¬ç¼–ç ã€RLVRè®­ç»ƒç­‰æŠ€æœ¯ï¼Œæå‡VTGæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTimeLensçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹æ•°æ®è´¨é‡çš„é‡è§†ï¼Œä»¥åŠå°†å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ç›¸ç»“åˆçš„è®­ç»ƒèŒƒå¼ï¼ˆRLVRï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€å¿½ç•¥æ•°æ®è´¨é‡ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚RLVRæ–¹æ³•åˆ™å¯ä»¥æ›´æœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£è§†é¢‘å†…å®¹å’Œæ–‡æœ¬æŸ¥è¯¢ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šTimeLensçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) äº¤é”™æ–‡æœ¬ç¼–ç ï¼šå°†æ—¶é—´ä¿¡æ¯ä¸æ–‡æœ¬æŸ¥è¯¢äº¤é”™ç¼–ç ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£æ—¶é—´ä¸Šä¸‹æ–‡ï¼›2) RLVRè®­ç»ƒï¼šä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å‡†ç¡®çš„æ—¶åºå®šä½ï¼›3) æ•°æ®å¢å¼ºï¼šé‡‡ç”¨å¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14698v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14698v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14698v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

TimeLensæ¨¡å‹åœ¨TimeLens-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé«˜è´¨é‡çš„æ•°æ®å’Œæœ‰æ•ˆçš„ç®—æ³•è®¾è®¡æ˜¯æå‡è§†é¢‘æ—¶åºå®šä½æ€§èƒ½çš„å…³é”®ã€‚TimeLens-Benchçš„å¼•å…¥ä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ›´å¯é çš„è¯„ä¼°åŸºå‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TimeLensçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè§†é¢‘æœç´¢ã€è§†é¢‘ç¼–è¾‘ã€æ™ºèƒ½ç›‘æ§ã€æ•™è‚²è§†é¢‘åˆ†æç­‰é¢†åŸŸã€‚é«˜è´¨é‡çš„è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›å¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶ä¸ºæ›´é«˜çº§çš„è§†é¢‘ç†è§£ä»»åŠ¡å¥ å®šåŸºç¡€ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

