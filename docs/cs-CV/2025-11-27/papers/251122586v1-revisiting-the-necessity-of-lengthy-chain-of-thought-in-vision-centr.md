---
layout: default
title: Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization
---

# Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.22586" target="_blank" class="toolbar-btn">arXiv: 2511.22586v1</a>
    <a href="https://arxiv.org/pdf/2511.22586.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.22586v1" 
            onclick="toggleFavorite(this, '2511.22586v1', 'Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yifan Du, Kun Zhou, Yingqian Min, Yue Ling, Wayne Xin Zhao, Youbin Wu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Á†îÁ©∂Ë°®ÊòéÔºåÂú®ËßÜËßâÊé®ÁêÜÊ≥õÂåñ‰∏≠ÔºåÁÆÄÊ¥ÅÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâ‰ºò‰∫éÂÜóÈïøÁöÑCoT„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÊé®ÁêÜ` `ÊÄùÁª¥Èìæ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Ê≥õÂåñËÉΩÂäõ` `Ëø∑ÂÆ´Ê±ÇËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰æùËµñÂÜóÈïøÁöÑÊÄùÁª¥ÈìæËøõË°åËßÜËßâÊé®ÁêÜÔºå‰ΩÜÂÖ∂ÂøÖË¶ÅÊÄßÂíåÊ≥õÂåñËÉΩÂäõÊúâÂæÖËÄÉÂØü„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÂØπÊØî‰∏çÂêåCoTÊ†ºÂºèÔºàËØ≠Ë®Ä„ÄÅÊé•Âú∞„ÄÅËßÜËßâÔºâÂú®Ëø∑ÂÆ´Ê±ÇËß£‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊé¢Á©∂CoTËÆæËÆ°ÂØπÊ≥õÂåñËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÁÆÄÊ¥ÅÁöÑ„ÄÅ‰ªÖÂåÖÂê´ÂøÖË¶ÅÊé•Âú∞Ê≠•È™§ÁöÑCoTÔºåÂú®‰∏çÂêåËø∑ÂÆ´Â∞∫ÂØ∏‰∏äË°®Áé∞Âá∫ÊúÄ‰Ω≥ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫Ü‰∏çÂêåÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâËÆæËÆ°Â¶Ç‰ΩïÂΩ±ÂìçËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏≠ÂèØÊ≥õÂåñÁöÑËßÜËßâÊé®ÁêÜËÉΩÂäõÁöÑËé∑Âèñ„ÄÇÂ∞ΩÁÆ°CoTÊï∞ÊçÆÔºåÁâπÂà´ÊòØÈïøCoTÊàñËßÜËßâCoTÔºàÂ¶Ç‚Äúthink with image‚ÄùÔºâÂ∑≤Ë¢´ÂπøÊ≥õÁî®‰∫éÁõëÁù£‰∏≠Èó¥Êé®ÁêÜËøáÁ®ãÔºå‰ΩÜÂ∞ö‰∏çÊ∏ÖÊ•ö‰∏∫‰ªÄ‰πàÁâπÂÆöÁöÑCoTËÆæËÆ°ÊúâÊïàÔºå‰ª•ÂèäÂì™‰∫õËÆæËÆ°ÁúüÊ≠£ÊîØÊåÅÂèØÊ≥õÂåñÁöÑÊé®ÁêÜ„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÂú∞ËØÑ‰º∞Ëøô‰∏ÄÁÇπÔºåÊàë‰ª¨‰∏ìÊ≥®‰∫é‰∏Ä‰∏™ÂèóÊéßÁöÑËø∑ÂÆ´Ê±ÇËß£Âü∫ÂáÜÔºåÂÖ∂‰∏≠Êé®ÁêÜËßÑÂàôÂÆåÂÖ®ÊòØËßÜËßâÁöÑÔºåÈöæÂ∫¶ÂèØ‰ª•ÈÄöËøáÁΩëÊ†ºÂ§ßÂ∞èËøõË°åË∞ÉÊï¥ÔºåÂπ∂‰∏îÊâÄÊúâ‰∏≠Èó¥Ê≠•È™§ÈÉΩÂèØ‰ª•Ëá™Âä®ÁîüÊàê„ÄÇ‰ΩøÁî®Qwen2.5-VL-7BÂú®Ê†áÂáÜÁöÑSFT-then-RLÊµÅÁ®ã‰∏ãÔºåÊàë‰ª¨ÊØîËæÉ‰∫Ü‰∏âÁßçÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑCoTÊ†ºÂºèÔºöËØ≠Ë®ÄCoT„ÄÅÂ∏¶ÊúâÁ©∫Èó¥ÂùêÊ†áËΩ®ËøπÁöÑÊé•Âú∞CoTÂíåÂ∏¶ÊúâÂõæÂÉèÊìç‰ΩúÁöÑËßÜËßâCoT„ÄÇÂÆûÈ™åË°®ÊòéÔºåËßÜËßâÂíåËæÉÈïøÁöÑCoT‰∏ªË¶ÅÂä†ÈÄüÊî∂ÊïõÔºå‰ΩÜ‰∏ç‰ºöÊèêÈ´òÊúÄÁªàÊÄßËÉΩ‰∏äÈôêÔºõÂåÖÂê´Âü∫Êú¨Êé•Âú∞Ê≠•È™§ÁöÑÁÆÄÊ¥ÅCoT‰ºò‰∫éËæÉÈïøÁöÑËΩ®ËøπÔºõÂπ∂‰∏îÔºå‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºå‰ªÖ‰øùÁïôÊúÄÂ∞èÊé•Âú∞ÁªìÊûúÁöÑCoTÂú®‰∏çÂêåÁöÑËø∑ÂÆ´Â∞∫ÂØ∏‰∏äÊ≥õÂåñÊïàÊûúÊúÄÂ•Ω„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•Âú®ÂÖ∂‰ªñ‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÁöÑ‰ªªÂä°‰∏äÈ™åËØÅ‰∫ÜËøô‰∫õËßÅËß£„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫Ü‰∏ÄÁßç‚ÄúÁü≠Âç≥ÊòØÈïø‚ÄùÁöÑÊïàÂ∫îÔºåÂπ∂‰∏∫ÊûÑÂª∫Êõ¥ÂÖ∑Ê≥õÂåñÊÄßÁöÑËßÜËßâÊé®ÁêÜSFTÊï∞ÊçÆÈõÜÊèê‰æõ‰∫ÜÂÆûË∑µÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ÔºåÈÄöÂ∏∏ÈááÁî®ÂÜóÈïøÁöÑÊÄùÁª¥ÈìæÔºàChain-of-Thought, CoTÔºâÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºå‰ª•ÊúüËé∑ÂæóÊõ¥Â•ΩÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÂÜóÈïøÁöÑCoTÊòØÂê¶ÁúüÊ≠£ÂøÖË¶ÅÔºå‰ª•ÂèäÂì™ÁßçCoTËÆæËÆ°ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊîØÊåÅÊ≥õÂåñËÉΩÂäõÔºå‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂºÄÊîæÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπ‰∏çÂêåCoTËÆæËÆ°ËøõË°åÁ≥ªÁªüÊÄßÁöÑËØÑ‰º∞ÔºåÈöæ‰ª•ÊåáÂØºÊõ¥ÊúâÊïàÁöÑËßÜËßâÊé®ÁêÜÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂØπÊØî‰∏çÂêåÁ±ªÂûãÁöÑCoTÊï∞ÊçÆÂú®ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊù•Êè≠Á§∫CoTËÆæËÆ°ÂØπÊ≥õÂåñËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËÆ∫ÊñáËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÂèóÊéßÁöÑËø∑ÂÆ´Ê±ÇËß£ÁéØÂ¢ÉÔºåÂπ∂ÊØîËæÉ‰∫ÜËØ≠Ë®ÄCoT„ÄÅÊé•Âú∞CoTÔºàÂåÖÂê´Á©∫Èó¥ÂùêÊ†áËΩ®ËøπÔºâÂíåËßÜËßâCoTÔºàÂåÖÂê´ÂõæÂÉèÊìç‰ΩúÔºâ‰∏âÁßçCoTÊ†ºÂºè„ÄÇÈÄöËøáÂàÜÊûê‰∏çÂêåCoTÊ†ºÂºèÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÊâæÂà∞Êõ¥ÊúâÊïàÁöÑCoTËÆæËÆ°ÊñπÊ°à„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÈááÁî®Ê†áÂáÜÁöÑSFT-then-RLÔºàSupervised Fine-Tuning followed by Reinforcement LearningÔºâÊµÅÁ®ã„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®‰∏çÂêåÁ±ªÂûãÁöÑCoTÊï∞ÊçÆÂØπQwen2.5-VL-7BÊ®°ÂûãËøõË°åÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâ„ÄÇÁÑ∂ÂêéÔºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂú®Ëø∑ÂÆ´Ê±ÇËß£ÁéØÂ¢É‰∏≠ÔºåÊ®°ÂûãÈúÄË¶ÅÊ†πÊçÆËßÜËßâËæìÂÖ•ÔºåÈÄêÊ≠•Êé®ÁêÜÂá∫Âà∞ËææÁõÆÊ†á‰ΩçÁΩÆÁöÑË∑ØÂæÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂèëÁé∞‚ÄúÁü≠Âç≥ÊòØÈïø‚ÄùÁöÑÊïàÂ∫îÔºåÂç≥ÁÆÄÊ¥ÅÁöÑ„ÄÅ‰ªÖÂåÖÂê´ÂøÖË¶ÅÊé•Âú∞Ê≠•È™§ÁöÑCoTÔºåÂú®ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ËÉΩÂ§üËé∑ÂæóÊõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøô‰∏é‰ª•ÂæÄËÆ§‰∏∫Êõ¥Èïø„ÄÅÊõ¥Â§çÊùÇÁöÑCoTËÉΩÂ§üÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑËßÇÁÇπ‰∏çÂêå„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÂèØÊéßÁöÑËø∑ÂÆ´Ê±ÇËß£ÁéØÂ¢ÉÔºåÂèØ‰ª•Ëá™Âä®ÁîüÊàê‰∏çÂêåÈöæÂ∫¶ÁöÑËø∑ÂÆ´ÂíåÂØπÂ∫îÁöÑCoTÊï∞ÊçÆÔºõ2) ÊØîËæÉ‰∫Ü‰∏âÁßçÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑCoTÊ†ºÂºèÔºöËØ≠Ë®ÄCoT„ÄÅÊé•Âú∞CoTÂíåËßÜËßâCoTÔºõ3) ‰ΩøÁî®Qwen2.5-VL-7BÊ®°ÂûãÂíåÊ†áÂáÜÁöÑSFT-then-RLÊµÅÁ®ãËøõË°åÂÆûÈ™å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Ëø∑ÂÆ´Ê±ÇËß£‰ªªÂä°‰∏≠ÔºåÁÆÄÊ¥ÅÁöÑÊé•Âú∞CoT‰ºò‰∫éÂÜóÈïøÁöÑËßÜËßâCoTÂíåËØ≠Ë®ÄCoT„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ªÖ‰øùÁïôÊúÄÂ∞èÊé•Âú∞ÁªìÊûúÁöÑCoTÂú®‰∏çÂêåËø∑ÂÆ´Â∞∫ÂØ∏‰∏äË°®Áé∞Âá∫ÊúÄ‰Ω≥ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøô‰∏ÄÂèëÁé∞ÊåëÊàò‰∫Ü‰ª•ÂæÄËÆ§‰∏∫Êõ¥ÈïøCoTÊõ¥ÊúâÊïàÁöÑËßÇÁÇπÔºåÂπ∂‰∏∫ÊûÑÂª∫Êõ¥ÂÖ∑Ê≥õÂåñÊÄßÁöÑËßÜËßâÊé®ÁêÜSFTÊï∞ÊçÆÈõÜÊèê‰æõ‰∫ÜÂÆûË∑µÊåáÂØº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßçËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰æãÂ¶ÇÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂõæÂÉèÁêÜËß£„ÄÅËßÜËßâÈóÆÁ≠îÁ≠â„ÄÇÈÄöËøáÈááÁî®Êõ¥ÁÆÄÊ¥ÅÊúâÊïàÁöÑCoTÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂèØ‰ª•Èôç‰ΩéÊ®°ÂûãÂØπËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰æùËµñÔºåÊèêÈ´òÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇËØ•Á†îÁ©∂‰∏∫ÊûÑÂª∫Êõ¥ÈÄöÁî®ÁöÑËßÜËßâÊé®ÁêÜÁ≥ªÁªüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.

