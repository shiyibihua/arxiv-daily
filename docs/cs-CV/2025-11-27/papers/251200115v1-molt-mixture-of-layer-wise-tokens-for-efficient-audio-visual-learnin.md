---
layout: default
title: MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning
---

# MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00115" target="_blank" class="toolbar-btn">arXiv: 2512.00115v1</a>
    <a href="https://arxiv.org/pdf/2512.00115.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00115v1" 
            onclick="toggleFavorite(this, '2512.00115v1', 'MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kyeongha Rho, Hyeongkeun Lee, Jae Won Cho, Joon Son Chung

**ÂàÜÁ±ª**: cs.SD, cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-27

**Â§áÊ≥®**: 10 pages, 5 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MoLTÔºåÈÄöËøáÊ∑∑ÂêàÂ±ÇÁ∫ßTokenÂÆûÁé∞È´òÊïàÁöÑÈü≥ËßÜÈ¢ëÂ≠¶‰π†„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Èü≥ËßÜÈ¢ëÂ≠¶‰π†` `Â§öÊ®°ÊÄÅËûçÂêà` `Transformer` `Ëá™ÈÄÇÂ∫îÂ≠¶‰π†` `Â±ÇÁ∫ßToken`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈü≥ËßÜÈ¢ëÂ≠¶‰π†ÊñπÊ≥ïÂú®TransformerÁöÑÊØè‰∏ÄÂ±ÇËøõË°å‰∏≤Ë°åËá™ÈÄÇÂ∫îÔºåËÆ°ÁÆóÈáèÂ§ßÔºåÊïàÁéá‰Ωé„ÄÇ
2. MoLTÈÄöËøáÂπ∂Ë°åÁöÑËΩªÈáèÁ∫ßÊñπÊ°àÔºå‰ªÖ‰ªéTransformerÁöÑÂêéÊúüÂ±ÇÊèêÂèñÂíåËûçÂêàÂ±ÇÁ∫ßTokenÔºåÂÆûÁé∞È´òÊïàËá™ÈÄÇÂ∫î„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMoLTÂú®Èü≥ËßÜÈ¢ëÈóÆÁ≠î„ÄÅÂàÜÂâ≤Âíå‰∫ã‰ª∂ÂÆö‰ΩçÁ≠â‰ªªÂä°‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂‰øùÊåÅÂèÇÊï∞ÂíåÂÜÖÂ≠òÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑∑ÂêàÂ±ÇÁ∫ßTokenÔºàMoLTÔºâÁöÑÂèÇÊï∞ÂíåÂÜÖÂ≠òÈ´òÊïàÁöÑÈü≥ËßÜÈ¢ëÂ≠¶‰π†Ëá™ÈÄÇÂ∫îÊ°ÜÊû∂„ÄÇMoLTÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÁî®Âπ∂Ë°åÁöÑËΩªÈáèÁ∫ßÊñπÊ°àÂèñ‰ª£‰º†ÁªüTransformer‰∏≠ËÆ°ÁÆóÈáèÂ§ßÁöÑ‰∏≤Ë°åËá™ÈÄÇÂ∫îÔºåËØ•ÊñπÊ°à‰ªÖ‰ªéÂêéÊúüÁöÑTransformerÂ±ÇÊèêÂèñÂíåËûçÂêàÂ±ÇÁ∫ßToken„ÄÇÊàë‰ª¨ÈááÁî®‰∏§ÁßçÁ±ªÂûãÁöÑÈÄÇÈÖçÂô®ÔºåÂ∞ÜÊ®°ÊÄÅÁâπÂÆö‰ø°ÊÅØÂíåË∑®Ê®°ÊÄÅ‰∫§‰∫íÊèêÁÇºÊàêÁ¥ßÂáëÁöÑÊΩúÂú®Token„ÄÇÁÑ∂ÂêéÔºåTokenËûçÂêàÊ®°ÂùóÈÄöËøáËÄÉËôëÂÆÉ‰ª¨ÁöÑÁõ∏ÂØπÈáçË¶ÅÊÄßÊù•Âä®ÊÄÅÂú∞ËûçÂêàËøô‰∫õÂ±ÇÁ∫ßToken„ÄÇ‰∏∫‰∫ÜÈò≤Ê≠¢ÊΩúÂú®TokenÁöÑÂÜó‰ΩôÔºåÊàë‰ª¨Âú®ËÆ≠ÁªÉÊúüÈó¥ÂØπÊΩúÂú®TokenÂ∫îÁî®Ê≠£‰∫§Ê≠£ÂàôÂåñ„ÄÇÈÄöËøáÂØπÈ¢ÑËÆ≠ÁªÉTransformer‰∏≠Ëá™ÈÄÇÂ∫î‰ΩçÁΩÆÁöÑÁ≥ªÁªüÂàÜÊûêÔºåÊàë‰ª¨‰ªÖ‰ªéTransformerÁöÑÂêéÊúüÂ±ÇÊèêÂèñÊΩúÂú®Token„ÄÇËøôÁßçÁ≠ñÁï•ÊÄßÁöÑËá™ÈÄÇÂ∫îÊñπÊ≥ïÈÅøÂÖç‰∫ÜÊù•Ëá™ÊòìÂ§±ÁöÑÊó©ÊúüÂ±ÇÁâπÂæÅÁöÑËØØÂ∑Æ‰º†Êí≠Ôºå‰ªéËÄåÂú®‰øùÊåÅÂèÇÊï∞ÂíåÂÜÖÂ≠òÊïàÁéáÁöÑÂêåÊó∂ÔºåÊúÄÂ§ßÂåñ‰∫ÜËá™ÈÄÇÂ∫îÊÄßËÉΩ„ÄÇÈÄöËøáÂ§ßÈáèÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜMoLTÂú®ÂêÑÁßçÈü≥ËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂåÖÊã¨Èü≥ËßÜÈ¢ëÈóÆÁ≠î„ÄÅÈü≥ËßÜÈ¢ëÂàÜÂâ≤ÂíåÈü≥ËßÜÈ¢ë‰∫ã‰ª∂ÂÆö‰Ωç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÈü≥ËßÜÈ¢ëÂ≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏Âú®TransformerÁöÑÊØè‰∏ÄÂ±ÇËøõË°å‰∏≤Ë°åËá™ÈÄÇÂ∫îÔºåÂØºËá¥ËÆ°ÁÆóÈáèÂ§ß„ÄÅÂèÇÊï∞ÊïàÁéá‰ΩéÔºåÈöæ‰ª•ÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏ä„ÄÇÊ≠§Â§ñÔºåÊó©ÊúüÂ±ÇÁöÑÁâπÂæÅÂèØËÉΩ‰∏çÁ®≥ÂÆöÔºåÂÆπÊòìÂØºËá¥ËØØÂ∑Æ‰º†Êí≠ÔºåÂΩ±ÂìçÊúÄÁªàÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMoLTÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®‰∏ÄÁßçÂπ∂Ë°åÁöÑ„ÄÅËΩªÈáèÁ∫ßÁöÑËá™ÈÄÇÂ∫îÊñπÊ°àÔºå‰ªÖ‰ªéTransformerÁöÑÂêéÊúüÂ±ÇÊèêÂèñÂíåËûçÂêàÂ±ÇÁ∫ßToken„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•ÈÅøÂÖçÂØπÊØè‰∏ÄÂ±ÇÈÉΩËøõË°åÂ§çÊùÇÁöÑËÆ°ÁÆóÔºåÂêåÊó∂Âà©Áî®ÂêéÊúüÂ±ÇÊõ¥Á®≥ÂÆöÁöÑÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMoLTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Ê®°ÊÄÅÁâπÂÆöÈÄÇÈÖçÂô®ÔºöÁî®‰∫éÊèêÂèñÈü≥È¢ëÂíåËßÜÈ¢ëÊ®°ÊÄÅÁöÑÁâπÂÆö‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫Á¥ßÂáëÁöÑÊΩúÂú®Token„ÄÇ2) Ë∑®Ê®°ÊÄÅ‰∫§‰∫íÈÄÇÈÖçÂô®ÔºöÁî®‰∫éÊçïÊçâÈü≥È¢ëÂíåËßÜÈ¢ëÊ®°ÊÄÅ‰πãÈó¥ÁöÑ‰∫§‰∫í‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ÊΩúÂú®Token„ÄÇ3) TokenËûçÂêàÊ®°ÂùóÔºöÁî®‰∫éÂä®ÊÄÅÂú∞ËûçÂêàÊù•Ëá™‰∏çÂêåÂ±ÇÁöÑÊΩúÂú®TokenÔºåËØ•Ê®°Âùó‰ºöÊ†πÊçÆTokenÁöÑÁõ∏ÂØπÈáçË¶ÅÊÄßËøõË°åÂä†ÊùÉËûçÂêà„ÄÇ4) Ê≠£‰∫§Ê≠£ÂàôÂåñÔºöÁî®‰∫éÈò≤Ê≠¢ÊΩúÂú®TokenÁöÑÂÜó‰ΩôÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMoLTÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Ê∑∑ÂêàÂ±ÇÁ∫ßTokenÁöÑÊèêÂèñÂíåËûçÂêàÊú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∏≤Ë°åËá™ÈÄÇÂ∫îÊñπÊ≥ï‰∏çÂêåÔºåMoLTÈááÁî®Âπ∂Ë°åÁöÑËΩªÈáèÁ∫ßÊñπÊ°àÔºå‰ªÖ‰ªéTransformerÁöÑÂêéÊúüÂ±ÇÊèêÂèñTokenÔºå‰ªéËÄåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÊ≠§Â§ñÔºåTokenËûçÂêàÊ®°ÂùóËÉΩÂ§üÂä®ÊÄÅÂú∞Ë∞ÉÊï¥‰∏çÂêåÂ±ÇTokenÁöÑÊùÉÈáçÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Âà©Áî®‰∏çÂêåÂ±ÇÁöÑ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMoLTÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈÄÇÈÖçÂô®ÁöÑÁ±ªÂûãÂíåÊï∞ÈáèÔºöËÆ∫ÊñáÈááÁî®‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑÈÄÇÈÖçÂô®ÔºåÂàÜÂà´Áî®‰∫éÊèêÂèñÊ®°ÊÄÅÁâπÂÆö‰ø°ÊÅØÂíåË∑®Ê®°ÊÄÅ‰∫§‰∫í‰ø°ÊÅØ„ÄÇ2) TokenËûçÂêàÊ®°ÂùóÁöÑÊùÉÈáçËÆ°ÁÆóÊñπÂºèÔºöËÆ∫ÊñáÈááÁî®‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊú∫Âà∂Êù•ËÆ°ÁÆóTokenÁöÑÊùÉÈáç„ÄÇ3) Ê≠£‰∫§Ê≠£ÂàôÂåñÁöÑÂº∫Â∫¶ÔºöËÆ∫ÊñáÈÄöËøáÂÆûÈ™åÁ°ÆÂÆö‰∫ÜÊ≠£‰∫§Ê≠£ÂàôÂåñÁöÑÊúÄ‰Ω≥Âº∫Â∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoLTÂú®Èü≥ËßÜÈ¢ëÈóÆÁ≠î„ÄÅÈü≥ËßÜÈ¢ëÂàÜÂâ≤ÂíåÈü≥ËßÜÈ¢ë‰∫ã‰ª∂ÂÆö‰ΩçÁ≠â‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®Audio-Visual Question Answering‰ªªÂä°‰∏äÔºåMoLTÁöÑÊÄßËÉΩ‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂‰∏îÂèÇÊï∞ÈáèÊõ¥Â∞ë„ÄÇËøô‰∫õÁªìÊûúËØÅÊòé‰∫ÜMoLTÁöÑÊúâÊïàÊÄßÂíåÈ´òÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MoLTÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇËßÜÈ¢ë‰ºöËÆÆ‰∏≠ÁöÑËØ≠Èü≥Â¢ûÂº∫„ÄÅÊô∫ËÉΩÁõëÊéß‰∏≠ÁöÑ‰∫ã‰ª∂Ê£ÄÊµã„ÄÅ‰ª•ÂèäËá™Âä®È©æÈ©∂‰∏≠ÁöÑÁéØÂ¢ÉÊÑüÁü•„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÈÉ®ÁΩ≤Âú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºåÂÆûÁé∞È´òÊïàÁöÑÈü≥ËßÜÈ¢ëÂ§ÑÁêÜÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂïÜ‰∏öÊΩúÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In this paper, we propose Mixture of Layer-Wise Tokens (MoLT), a parameter- and memory-efficient adaptation framework for audio-visual learning. The key idea of MoLT is to replace conventional, computationally heavy sequential adaptation at every transformer layer with a parallel, lightweight scheme that extracts and fuses layer-wise tokens only from the late layers. We adopt two types of adapters to distill modality-specific information and cross-modal interaction into compact latent tokens in a layer-wise manner. A token fusion module then dynamically fuses these layer-wise tokens by taking into account their relative significance. To prevent the redundancy of latent tokens, we apply an orthogonality regularization between latent tokens during training. Through the systematic analysis of the position of adaptation in the pre-trained transformers, we extract latent tokens only from the late layers of the transformers. This strategic adaptation approach avoids error propagation from the volatile early-layer features, thereby maximizing the adaptation performance while maintaining parameter and memory efficiency. Through extensive experiments, we demonstrate that MoLT outperforms existing methods on diverse audio-visual benchmarks, including Audio-Visual Question Answering, Audio-Visual Segmentation, and Audio-Visual Event Localization.

