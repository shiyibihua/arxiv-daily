---
layout: default
title: Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning
---

# Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning

**arXiv**: [2511.22172v1](https://arxiv.org/abs/2511.22172) | [PDF](https://arxiv.org/pdf/2511.22172.pdf)

**ä½œè€…**: Zhaoyang Wei, Wenchao Ding, Yanchao Hao, Xi Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

**å¤‡æ³¨**: 9pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRiPæ¡†æž¶ï¼Œé€šè¿‡è®¤çŸ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰åŸºç¡€æŽ¨ç†çš„é²æ£’æ€§å’Œçµæ´»æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰åŸºç¡€æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `è®¤çŸ¥å¼•å¯¼` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰åŸºç¡€æŽ¨ç†æ–¹æ³•å—å›°äºŽå¼ºåŒ–å­¦ä¹ çš„ä¸ç¨³å®šå’Œç›‘ç£å¾®è°ƒçš„åˆšæ€§ï¼Œéš¾ä»¥å…¼é¡¾å­¦ä¹ èƒ½åŠ›å’Œè®¤çŸ¥çµæ´»æ€§ã€‚
2. GRiPæ¡†æž¶é€šè¿‡è®¤çŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾å¼å¼•å¯¼æ¨¡åž‹çš„æ„ŸçŸ¥ç„¦ç‚¹å’Œé€»è¾‘è·¯å¾„ï¼Œæå‡è§†è§‰æŽ¨ç†çš„é²æ£’æ€§å’Œçµæ´»æ€§ã€‚
3. GRiPåœ¨TreeBenchå’ŒV* Benchç­‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼€æºæ¨¡åž‹ä¸­æœ€ä¼˜ç»“æžœï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚è§†è§‰æŽ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºGRiPï¼ˆGuided Reasoning and Perceptionï¼‰æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³çŽ°æœ‰è§†è§‰åŸºç¡€æŽ¨ç†æ–¹æ³•åœ¨ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ çš„ä¸ç¨³å®šæ€§å’Œç›‘ç£å¾®è°ƒçš„åˆšæ€§ä¹‹é—´çš„å›°å¢ƒã€‚GRiPé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼å¼•å¯¼æ¨¡åž‹çš„æ„ŸçŸ¥ç„¦ç‚¹å’Œé€»è¾‘è·¯å¾„ï¼ŒåŸ¹å…»é²æ£’ä¸”çµæ´»çš„è§†è§‰åŸºç¡€æŽ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æž¶çš„æ ¸å¿ƒåœ¨äºŽè®¤çŸ¥å¢žå¼ºçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šä¸€æ˜¯æ˜¾è‘—æ€§åŠ æƒIoUå¥–åŠ±ï¼Œæ¿€åŠ±æ¨¡åž‹ä¼˜å…ˆå®šä½ä»»åŠ¡å…³é”®å¯¹è±¡è€Œéžæ— å…³å¹²æ‰°é¡¹ï¼›äºŒæ˜¯å¤šå¯å‘å¼å¥–åŠ±ï¼Œé¼“åŠ±å¤šæ ·ä½†é€»è¾‘ä¸Šæœ‰æ•ˆçš„æŽ¨ç†è·¯å¾„ï¼Œä»Žè€Œæå‡è®¤çŸ¥çµæ´»æ€§ã€‚åŸºäºŽQwen2.5-VL-7Bæ¨¡åž‹åˆå§‹åŒ–ï¼ŒGRiPåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨TreeBenchå’ŒV* Benchä¸Šå–å¾—äº†å¼€æºæ¨¡åž‹ä¸­æœ€å…ˆè¿›çš„ç»“æžœï¼Œè¯æ˜Žäº†å…¶åœ¨å¤æ‚è§†è§‰æŽ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰åŸºç¡€æŽ¨ç†æ¨¡åž‹åœ¨å¤æ‚åœºæ™¯ä¸‹è¡¨çŽ°ä¸ä½³ï¼Œä¸»è¦ç—›ç‚¹åœ¨äºŽï¼šç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸ç¨³å®šï¼Œéš¾ä»¥æ”¶æ•›ï¼›ç›‘ç£å¾®è°ƒè™½ç„¶ç¨³å®šï¼Œä½†æ¨¡åž‹ç¼ºä¹è®¤çŸ¥çµæ´»æ€§ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°çš„åœºæ™¯ã€‚æ¨¡åž‹éš¾ä»¥åŒºåˆ†å…³é”®å¯¹è±¡å’Œå¹²æ‰°é¡¹ï¼ŒæŽ¨ç†è·¯å¾„å•ä¸€ï¼Œç¼ºä¹æŽ¢ç´¢èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRiPçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¤çŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾å¼åœ°å¼•å¯¼æ¨¡åž‹å…³æ³¨ä»»åŠ¡å…³é”®å¯¹è±¡ï¼Œå¹¶é¼“åŠ±æ¨¡åž‹æŽ¢ç´¢å¤šæ ·åŒ–çš„æŽ¨ç†è·¯å¾„ã€‚é€šè¿‡è®¾è®¡ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ï¼Œæ¿€åŠ±æ¨¡åž‹å­¦ä¹ æ›´é²æ£’ã€æ›´çµæ´»çš„è§†è§‰æŽ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šGRiPæ¡†æž¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚Qwen2.5-VL-7Bï¼‰è¿›è¡Œåˆå§‹åŒ–ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯è®¤çŸ¥å¢žå¼ºçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œè¯¥é˜¶æ®µä½¿ç”¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°æ¥è®­ç»ƒæ¨¡åž‹ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šè¾“å…¥å›¾åƒå’Œé—®é¢˜ï¼Œæ¨¡åž‹é€šè¿‡è§†è§‰æ„ŸçŸ¥æ¨¡å—å®šä½ç›¸å…³å¯¹è±¡ï¼Œç„¶åŽè¿›è¡Œé€»è¾‘æŽ¨ç†ï¼Œæœ€ç»ˆè¾“å‡ºç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šGRiPçš„å…³é”®åˆ›æ–°åœ¨äºŽä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯æ˜¾è‘—æ€§åŠ æƒIoUå¥–åŠ±ï¼Œè¯¥å¥–åŠ±å‡½æ•°æ ¹æ®å¯¹è±¡çš„é‡è¦æ€§å¯¹IoUè¿›è¡ŒåŠ æƒï¼Œæ¿€åŠ±æ¨¡åž‹ä¼˜å…ˆå…³æ³¨ä»»åŠ¡å…³é”®å¯¹è±¡ï¼›äºŒæ˜¯å¤šå¯å‘å¼å¥–åŠ±ï¼Œè¯¥å¥–åŠ±å‡½æ•°é¼“åŠ±æ¨¡åž‹æŽ¢ç´¢å¤šæ ·åŒ–çš„æŽ¨ç†è·¯å¾„ï¼Œæå‡æ¨¡åž‹çš„è®¤çŸ¥çµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ˜¾è‘—æ€§åŠ æƒIoUå¥–åŠ±çš„å…·ä½“è®¡ç®—æ–¹å¼æ˜¯ï¼šé¦–å…ˆç¡®å®šå›¾åƒä¸­æ¯ä¸ªå¯¹è±¡çš„æ˜¾è‘—æ€§æƒé‡ï¼Œç„¶åŽè®¡ç®—æ¨¡åž‹é¢„æµ‹çš„è¾¹ç•Œæ¡†ä¸ŽçœŸå®žè¾¹ç•Œæ¡†çš„IoUï¼Œæœ€åŽå°†IoUä¸Žæ˜¾è‘—æ€§æƒé‡ç›¸ä¹˜å¾—åˆ°æœ€ç»ˆçš„å¥–åŠ±å€¼ã€‚å¤šå¯å‘å¼å¥–åŠ±çš„å…·ä½“å®žçŽ°æ–¹å¼æ˜¯ï¼šè®¾è®¡å¤šä¸ªä¸åŒçš„å¯å‘å¼è§„åˆ™ï¼Œæ ¹æ®æ¨¡åž‹æ˜¯å¦æ»¡è¶³è¿™äº›è§„åˆ™æ¥ç»™äºˆä¸åŒçš„å¥–åŠ±ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

GRiPæ¡†æž¶åœ¨TreeBenchå’ŒV* Benchç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æŽ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œé€šè¿‡è®¤çŸ¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ å¯ä»¥æœ‰æ•ˆåœ°æå‡è§†è§‰åŸºç¡€æŽ¨ç†çš„é²æ£’æ€§å’Œçµæ´»æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†ç»™å‡ºï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

GRiPæ¡†æž¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽæ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ¨¡åž‹åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸‹çš„æŽ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®žçŽ°æ›´æ™ºèƒ½ã€æ›´å¯é çš„AIç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒGRiPå¯ä»¥å¸®åŠ©è½¦è¾†æ›´å‡†ç¡®åœ°è¯†åˆ«äº¤é€šä¿¡å·ç¯å’Œè¡Œäººï¼Œä»Žè€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.

