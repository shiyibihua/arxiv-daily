---
layout: default
title: Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning
---

# Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.22172" target="_blank" class="toolbar-btn">arXiv: 2511.22172v1</a>
    <a href="https://arxiv.org/pdf/2511.22172.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.22172v1" 
            onclick="toggleFavorite(this, '2511.22172v1', 'Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhaoyang Wei, Wenchao Ding, Yanchao Hao, Xi Chen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-27

**Â§áÊ≥®**: 9pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫GRiPÊ°ÜÊû∂ÔºåÈÄöËøáËÆ§Áü•ÂºïÂØºÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáËßÜËßâÂü∫Á°ÄÊé®ÁêÜÁöÑÈ≤ÅÊ£íÊÄßÂíåÁÅµÊ¥ªÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂü∫Á°ÄÊé®ÁêÜ` `Âº∫ÂåñÂ≠¶‰π†` `ËÆ§Áü•ÂºïÂØº` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâÂü∫Á°ÄÊé®ÁêÜÊñπÊ≥ïÂèóÂõ∞‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰∏çÁ®≥ÂÆöÂíåÁõëÁù£ÂæÆË∞ÉÁöÑÂàöÊÄßÔºåÈöæ‰ª•ÂÖºÈ°æÂ≠¶‰π†ËÉΩÂäõÂíåËÆ§Áü•ÁÅµÊ¥ªÊÄß„ÄÇ
2. GRiPÊ°ÜÊû∂ÈÄöËøáËÆ§Áü•ÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæÂºèÂºïÂØºÊ®°ÂûãÁöÑÊÑüÁü•ÁÑ¶ÁÇπÂíåÈÄªËæëË∑ØÂæÑÔºåÊèêÂçáËßÜËßâÊé®ÁêÜÁöÑÈ≤ÅÊ£íÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ
3. GRiPÂú®TreeBenchÂíåV* BenchÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÂºÄÊ∫êÊ®°Âûã‰∏≠ÊúÄ‰ºòÁªìÊûúÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§çÊùÇËßÜËßâÊé®ÁêÜ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫GRiPÔºàGuided Reasoning and PerceptionÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜËßâÂü∫Á°ÄÊé®ÁêÜÊñπÊ≥ïÂú®Á´ØÂà∞Á´ØÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíåÁõëÁù£ÂæÆË∞ÉÁöÑÂàöÊÄß‰πãÈó¥ÁöÑÂõ∞Â¢É„ÄÇGRiPÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊñπÊ≥ïÔºåÈÄöËøáÊòæÂºèÂºïÂØºÊ®°ÂûãÁöÑÊÑüÁü•ÁÑ¶ÁÇπÂíåÈÄªËæëË∑ØÂæÑÔºåÂüπÂÖªÈ≤ÅÊ£í‰∏îÁÅµÊ¥ªÁöÑËßÜËßâÂü∫Á°ÄÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÂú®‰∫éËÆ§Áü•Â¢ûÂº∫ÁöÑÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÔºåÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÂàõÊñ∞Ôºö‰∏ÄÊòØÊòæËëóÊÄßÂä†ÊùÉIoUÂ•ñÂä±ÔºåÊøÄÂä±Ê®°Âûã‰ºòÂÖàÂÆö‰Ωç‰ªªÂä°ÂÖ≥ÈîÆÂØπË±°ËÄåÈùûÊó†ÂÖ≥Âπ≤Êâ∞È°πÔºõ‰∫åÊòØÂ§öÂêØÂèëÂºèÂ•ñÂä±ÔºåÈºìÂä±Â§öÊ†∑‰ΩÜÈÄªËæë‰∏äÊúâÊïàÁöÑÊé®ÁêÜË∑ØÂæÑÔºå‰ªéËÄåÊèêÂçáËÆ§Áü•ÁÅµÊ¥ªÊÄß„ÄÇÂü∫‰∫éQwen2.5-VL-7BÊ®°ÂûãÂàùÂßãÂåñÔºåGRiPÂú®Â§ö‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂Âú®TreeBenchÂíåV* Bench‰∏äÂèñÂæó‰∫ÜÂºÄÊ∫êÊ®°Âûã‰∏≠ÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§çÊùÇËßÜËßâÊé®ÁêÜÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâÂü∫Á°ÄÊé®ÁêÜÊ®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãË°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÔºöÁ´ØÂà∞Á´ØÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºåÈöæ‰ª•Êî∂ÊïõÔºõÁõëÁù£ÂæÆË∞ÉËôΩÁÑ∂Á®≥ÂÆöÔºå‰ΩÜÊ®°ÂûãÁº∫‰πèËÆ§Áü•ÁÅµÊ¥ªÊÄßÔºåÈöæ‰ª•Ê≥õÂåñÂà∞Êñ∞ÁöÑÂú∫ÊôØ„ÄÇÊ®°ÂûãÈöæ‰ª•Âå∫ÂàÜÂÖ≥ÈîÆÂØπË±°ÂíåÂπ≤Êâ∞È°πÔºåÊé®ÁêÜË∑ØÂæÑÂçï‰∏ÄÔºåÁº∫‰πèÊé¢Á¥¢ËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGRiPÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËÆ§Áü•ÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæÂºèÂú∞ÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®‰ªªÂä°ÂÖ≥ÈîÆÂØπË±°ÔºåÂπ∂ÈºìÂä±Ê®°ÂûãÊé¢Á¥¢Â§öÊ†∑ÂåñÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇÈÄöËøáËÆæËÆ°ÁâπÂÆöÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÊøÄÂä±Ê®°ÂûãÂ≠¶‰π†Êõ¥È≤ÅÊ£í„ÄÅÊõ¥ÁÅµÊ¥ªÁöÑËßÜËßâÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGRiPÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÊòØ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇQwen2.5-VL-7BÔºâËøõË°åÂàùÂßãÂåñÔºõÁ¨¨‰∫åÈò∂ÊÆµÊòØËÆ§Áü•Â¢ûÂº∫ÁöÑÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÔºåËØ•Èò∂ÊÆµ‰ΩøÁî®ËÆæËÆ°ÁöÑÂ•ñÂä±ÂáΩÊï∞Êù•ËÆ≠ÁªÉÊ®°Âûã„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöËæìÂÖ•ÂõæÂÉèÂíåÈóÆÈ¢òÔºåÊ®°ÂûãÈÄöËøáËßÜËßâÊÑüÁü•Ê®°ÂùóÂÆö‰ΩçÁõ∏ÂÖ≥ÂØπË±°ÔºåÁÑ∂ÂêéËøõË°åÈÄªËæëÊé®ÁêÜÔºåÊúÄÁªàËæìÂá∫Á≠îÊ°à„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGRiPÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰∏§‰∏™ÊñπÈù¢Ôºö‰∏ÄÊòØÊòæËëóÊÄßÂä†ÊùÉIoUÂ•ñÂä±ÔºåËØ•Â•ñÂä±ÂáΩÊï∞Ê†πÊçÆÂØπË±°ÁöÑÈáçË¶ÅÊÄßÂØπIoUËøõË°åÂä†ÊùÉÔºåÊøÄÂä±Ê®°Âûã‰ºòÂÖàÂÖ≥Ê≥®‰ªªÂä°ÂÖ≥ÈîÆÂØπË±°Ôºõ‰∫åÊòØÂ§öÂêØÂèëÂºèÂ•ñÂä±ÔºåËØ•Â•ñÂä±ÂáΩÊï∞ÈºìÂä±Ê®°ÂûãÊé¢Á¥¢Â§öÊ†∑ÂåñÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÊèêÂçáÊ®°ÂûãÁöÑËÆ§Áü•ÁÅµÊ¥ªÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊòæËëóÊÄßÂä†ÊùÉIoUÂ•ñÂä±ÁöÑÂÖ∑‰ΩìËÆ°ÁÆóÊñπÂºèÊòØÔºöÈ¶ñÂÖàÁ°ÆÂÆöÂõæÂÉè‰∏≠ÊØè‰∏™ÂØπË±°ÁöÑÊòæËëóÊÄßÊùÉÈáçÔºåÁÑ∂ÂêéËÆ°ÁÆóÊ®°ÂûãÈ¢ÑÊµãÁöÑËæπÁïåÊ°Ü‰∏éÁúüÂÆûËæπÁïåÊ°ÜÁöÑIoUÔºåÊúÄÂêéÂ∞ÜIoU‰∏éÊòæËëóÊÄßÊùÉÈáçÁõ∏‰πòÂæóÂà∞ÊúÄÁªàÁöÑÂ•ñÂä±ÂÄº„ÄÇÂ§öÂêØÂèëÂºèÂ•ñÂä±ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÊòØÔºöËÆæËÆ°Â§ö‰∏™‰∏çÂêåÁöÑÂêØÂèëÂºèËßÑÂàôÔºåÊ†πÊçÆÊ®°ÂûãÊòØÂê¶Êª°Ë∂≥Ëøô‰∫õËßÑÂàôÊù•Áªô‰∫à‰∏çÂêåÁöÑÂ•ñÂä±„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠Êú™ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

GRiPÊ°ÜÊû∂Âú®TreeBenchÂíåV* BenchÁ≠âÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËßÜËßâÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂Âú®ÂºÄÊ∫êÊ®°Âûã‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÈÄöËøáËÆ§Áü•ÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•ÊúâÊïàÂú∞ÊèêÂçáËßÜËßâÂü∫Á°ÄÊé®ÁêÜÁöÑÈ≤ÅÊ£íÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Âú®ËÆ∫Êñá‰∏≠Êú™ËØ¶ÁªÜÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GRiPÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂú®Â§çÊùÇËßÜËßâÂú∫ÊôØ‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂèØÈù†ÁöÑAIÁ≥ªÁªü„ÄÇ‰æãÂ¶ÇÔºåÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåGRiPÂèØ‰ª•Â∏ÆÂä©ËΩ¶ËæÜÊõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´‰∫§ÈÄö‰ø°Âè∑ÁÅØÂíåË°å‰∫∫Ôºå‰ªéËÄåÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.

