---
layout: default
title: DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA
---

# DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA

**arXiv**: [2511.22521v1](https://arxiv.org/abs/2511.22521) | [PDF](https://arxiv.org/pdf/2511.22521.pdf)

**ä½œè€…**: Ahmad Mohammadshirazi, Pinaki Prasad Guha Neogi, Dheeraj Kulshrestha, Rajiv Ramnath

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDocVALï¼šä¸€ç§ç»éªŒè¯çš„æ€ç»´é“¾è’¸é¦æ¡†æž¶ï¼Œç”¨äºŽæå‡æ–‡æ¡£VQAçš„ç©ºé—´æŽ¨ç†èƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æ–‡æ¡£è§†è§‰é—®ç­”` `çŸ¥è¯†è’¸é¦` `æ€ç»´é“¾` `ç©ºé—´æŽ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰DocVQAæ¨¡åž‹åœ¨ç²¾åº¦å’Œæ•ˆçŽ‡ä¹‹é—´å­˜åœ¨trade-offï¼Œå¤§åž‹æ¨¡åž‹ç²¾åº¦é«˜ä½†éƒ¨ç½²æˆæœ¬é«˜ï¼Œå°åž‹æ¨¡åž‹æ•ˆçŽ‡é«˜ä½†å®šä½æ€§èƒ½å·®ã€‚
2. DocVALé€šè¿‡éªŒè¯çš„æ€ç»´é“¾è’¸é¦ï¼Œå°†å¤§åž‹æ•™å¸ˆæ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›è¿ç§»åˆ°å°åž‹å­¦ç”Ÿæ¨¡åž‹ï¼Œå®žçŽ°ç²¾åº¦å’Œæ•ˆçŽ‡çš„å¹³è¡¡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒDocVALæ¡†æž¶ä¸‹çš„å­¦ç”Ÿæ¨¡åž‹åœ¨DocVQAä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯åé¦ˆå’Œè¿­ä»£ç»†åŒ–å‡æœ‰è´¡çŒ®ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£è§†è§‰é—®ç­”(DocVQA)è¦æ±‚æ¨¡åž‹è”åˆæŽ¨ç†æ–‡æœ¬å†…å®¹å’Œç©ºé—´å¸ƒå±€ï¼Œä½†çŽ°æœ‰ç³»ç»Ÿå­˜åœ¨ç²¾åº¦ä¸Žæ•ˆçŽ‡çš„æƒè¡¡ï¼šå¤§åž‹æ•™å¸ˆæ¨¡åž‹å®žçŽ°äº†å¼ºå¤§çš„ groundingï¼Œä½†éƒ¨ç½²æˆæœ¬è¿‡é«˜ï¼Œè€Œå°åž‹å­¦ç”Ÿæ¨¡åž‹åœ¨å®šä½æ€§èƒ½æ–¹é¢å¤§å¹…ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†DocVALï¼Œä¸€ä¸ªç»éªŒè¯çš„æ€ç»´é“¾è’¸é¦æ¡†æž¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å°†å¤§åž‹æ•™å¸ˆæ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å¯éƒ¨ç½²çš„å­¦ç”ŸVLMä¸­ï¼šï¼ˆ1ï¼‰é€šè¿‡éªŒè¯æ—¶æ–‡æœ¬æ£€æµ‹è¿›è¡Œæ•™å¸ˆç›‘ç£ï¼Œä»¥è¿‡æ»¤å’ŒåŽ»å™ªè®­ç»ƒä¿¡å·ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå¤šæ¨¡å—éªŒè¯å™¨(VAL)ï¼Œåœ¨äº§ç”Ÿç»†ç²’åº¦çš„åƒç´ çº§é”™è¯¯åé¦ˆçš„åŒæ—¶ï¼Œå¼ºåˆ¶æ‰§è¡Œç­”æ¡ˆæ­£ç¡®æ€§å’Œå‡ ä½•ä¸€è‡´æ€§ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªä¸¤é˜¶æ®µå­¦ç”Ÿè®­ç»ƒæ–¹æ¡ˆï¼Œé¦–å…ˆä»Žç»è¿‡éªŒè¯çš„CoTè½¨è¿¹ä¸­å­¦ä¹ ï¼Œç„¶åŽè¿›è¡Œç”±VALåé¦ˆé©±åŠ¨çš„è¿­ä»£ç»†åŒ–ã€‚æˆ‘ä»¬çš„å­¦ç”Ÿæ¨¡åž‹(Gemma-3 12B)åœ¨DocVQAä¸Šå®žçŽ°äº†91.4%çš„ANLSå’Œ82.4%çš„mAPï¼Œä½œä¸ºä¸€ä¸ªçº¯VLMï¼Œåœ¨æŽ¨ç†æ—¶ä¸éœ€è¦æ–‡æœ¬æ£€æµ‹æˆ–OCRã€‚å¤§é‡çš„æ¶ˆèžå®žéªŒè¡¨æ˜Žï¼Œç»è¿‡éªŒè¯çš„åé¦ˆè´¡çŒ®äº†6.3 mAPçš„å¢žç›Šï¼Œè¿­ä»£ç»†åŒ–è´¡çŒ®äº†9.7 mAPçš„æ”¹è¿›ã€‚æˆ‘ä»¬å‘å¸ƒäº†9.5ä¸‡æ¡é«˜è´¨é‡ã€ç»è¿‡éªŒè¯å™¨éªŒè¯çš„CoTè½¨è¿¹ï¼Œä»¥æŽ¨è¿›æ–‡æ¡£ç†è§£ä¸­çš„ç©ºé—´æŽ¨ç†ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šDocVQAä»»åŠ¡éœ€è¦æ¨¡åž‹ç†è§£æ–‡æ¡£ä¸­çš„æ–‡æœ¬å†…å®¹å’Œç©ºé—´å¸ƒå±€ï¼ŒçŽ°æœ‰çš„å¤§åž‹æ¨¡åž‹è™½ç„¶ç²¾åº¦é«˜ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥éƒ¨ç½²ã€‚å°åž‹æ¨¡åž‹è™½ç„¶æ•ˆçŽ‡é«˜ï¼Œä½†åœ¨å®šä½å’Œç©ºé—´æŽ¨ç†æ–¹é¢è¡¨çŽ°ä¸ä½³ï¼Œå¯¼è‡´æ•´ä½“æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œæé«˜DocVQAæ¨¡åž‹çš„æ•ˆçŽ‡ï¼Œä½¿å…¶èƒ½å¤Ÿéƒ¨ç½²åœ¨èµ„æºå—é™çš„çŽ¯å¢ƒä¸­ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œå°†å¤§åž‹æ•™å¸ˆæ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›è¿ç§»åˆ°å°åž‹å­¦ç”Ÿæ¨¡åž‹ã€‚ä¸ºäº†æé«˜è’¸é¦çš„è´¨é‡ï¼Œå¼•å…¥äº†éªŒè¯æœºåˆ¶ï¼Œå¯¹æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºè¿›è¡Œè¿‡æ»¤å’Œçº æ­£ï¼Œä»Žè€Œä¸ºå­¦ç”Ÿæ¨¡åž‹æä¾›æ›´å¯é çš„è®­ç»ƒä¿¡å·ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†è¿­ä»£ç»†åŒ–çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å­¦ç”Ÿæ¨¡åž‹èƒ½å¤Ÿé€æ­¥é€¼è¿‘æ•™å¸ˆæ¨¡åž‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDocVALæ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) æ•™å¸ˆç›‘ç£ä¸ŽéªŒè¯æ—¶æ–‡æœ¬æ£€æµ‹ï¼šåˆ©ç”¨å¤§åž‹æ•™å¸ˆæ¨¡åž‹ç”Ÿæˆæ€ç»´é“¾(CoT)æŽ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨éªŒè¯æ—¶æ–‡æœ¬æ£€æµ‹è¿‡æ»¤å’ŒåŽ»å™ªè®­ç»ƒä¿¡å·ã€‚2) å¤šæ¨¡å—éªŒè¯å™¨(VAL)ï¼šVALæ¨¡å—ç”¨äºŽè¯„ä¼°å­¦ç”Ÿæ¨¡åž‹çš„è¾“å‡ºï¼ŒåŒ…æ‹¬ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œå‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶æä¾›åƒç´ çº§åˆ«çš„é”™è¯¯åé¦ˆã€‚3) ä¸¤é˜¶æ®µå­¦ç”Ÿè®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡åž‹ä»Žç»è¿‡éªŒè¯çš„CoTè½¨è¿¹ä¸­å­¦ä¹ ï¼›ç¬¬äºŒé˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡åž‹æ ¹æ®VALæ¨¡å—çš„åé¦ˆè¿›è¡Œè¿­ä»£ç»†åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šDocVALçš„å…³é”®åˆ›æ–°åœ¨äºŽå¼•å…¥äº†éªŒè¯æœºåˆ¶ï¼Œå¯¹æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºè¿›è¡ŒéªŒè¯å’Œçº æ­£ï¼Œä»Žè€Œæé«˜äº†è’¸é¦çš„è´¨é‡ã€‚ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•é€šå¸¸ç›´æŽ¥ä½¿ç”¨æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œä½†æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºå¯èƒ½å­˜åœ¨é”™è¯¯æˆ–å™ªå£°ï¼Œä»Žè€Œå½±å“å­¦ç”Ÿæ¨¡åž‹çš„æ€§èƒ½ã€‚DocVALé€šè¿‡éªŒè¯æœºåˆ¶ï¼Œå¯ä»¥è¿‡æ»¤æŽ‰è¿™äº›é”™è¯¯æˆ–å™ªå£°ï¼Œä»Žè€Œä¸ºå­¦ç”Ÿæ¨¡åž‹æä¾›æ›´å¯é çš„è®­ç»ƒä¿¡å·ã€‚æ­¤å¤–ï¼Œè¿­ä»£ç»†åŒ–çš„è®­ç»ƒç­–ç•¥ä¹Ÿè¿›ä¸€æ­¥æé«˜äº†å­¦ç”Ÿæ¨¡åž‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šéªŒè¯å™¨(VAL)åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºŽè¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚å‡ ä½•ä¸€è‡´æ€§è¯„ä¼°æ¨¡å—ä¼šæ£€æŸ¥å­¦ç”Ÿæ¨¡åž‹é¢„æµ‹çš„æ–‡æœ¬æ¡†æ˜¯å¦ä¸Žæ–‡æ¡£ä¸­çš„å®žé™…æ–‡æœ¬æ¡†å¯¹é½ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†ç­”æ¡ˆæ­£ç¡®æ€§å’Œå‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶æ ¹æ®VALæ¨¡å—çš„åé¦ˆè¿›è¡Œè°ƒæ•´ã€‚å­¦ç”Ÿæ¨¡åž‹çš„ç½‘ç»œç»“æž„é€‰æ‹©Gemma-3 12Bï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„VLMï¼Œé€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„çŽ¯å¢ƒä¸­ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDocVALæ¡†æž¶ä¸‹çš„å­¦ç”Ÿæ¨¡åž‹(Gemma-3 12B)åœ¨DocVQAæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒANLSè¾¾åˆ°91.4%ï¼ŒmAPè¾¾åˆ°82.4%ã€‚æ¶ˆèžå®žéªŒè¡¨æ˜Žï¼ŒéªŒè¯åé¦ˆè´¡çŒ®äº†6.3 mAPçš„å¢žç›Šï¼Œè¿­ä»£ç»†åŒ–è´¡çŒ®äº†9.7 mAPçš„æ”¹è¿›ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒDocVALæ¡†æž¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å¤§åž‹æ•™å¸ˆæ¨¡åž‹çš„çŸ¥è¯†è¿ç§»åˆ°å°åž‹å­¦ç”Ÿæ¨¡åž‹ï¼Œå¹¶æ˜¾è‘—æé«˜å­¦ç”Ÿæ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

DocVALæ¡†æž¶å¯åº”ç”¨äºŽå„ç§æ–‡æ¡£æ™ºèƒ½åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç†ã€æ™ºèƒ½è¡¨å•å¡«å†™ã€ä¿¡æ¯æŠ½å–ç­‰ã€‚é€šè¿‡å°†å¤§åž‹æ¨¡åž‹çš„çŸ¥è¯†è¿ç§»åˆ°å°åž‹æ¨¡åž‹ï¼Œå¯ä»¥åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œæé«˜æ¨¡åž‹çš„æ•ˆçŽ‡ï¼Œä½¿å…¶èƒ½å¤Ÿéƒ¨ç½²åœ¨ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼ç³»ç»Ÿä¸­ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºŽæŽ¨åŠ¨æ–‡æ¡£æ™ºèƒ½æŠ€æœ¯åœ¨å®žé™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.

