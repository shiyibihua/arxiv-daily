---
layout: default
title: LMVC: An End-to-End Learned Multiview Video Coding Framework
---

# LMVC: An End-to-End Learned Multiview Video Coding Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03922v1</a>
  <a href="https://arxiv.org/pdf/2509.03922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03922v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03922v1', 'LMVC: An End-to-End Learned Multiview Video Coding Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xihua Sheng, Yingwen Zhang, Long Xu, Shiqi Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLMVCç«¯åˆ°ç«¯å¤šè§†è§’è§†é¢‘ç¼–ç æ¡†æ¶ï¼Œæå‡å‹ç¼©æ•ˆç‡å¹¶ä¿è¯å…¼å®¹æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤šè§†è§’è§†é¢‘ç¼–ç ` `ç«¯åˆ°ç«¯å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `è¿åŠ¨é¢„æµ‹` `å†…å®¹é¢„æµ‹` `è§†è§’é—´ç›¸å…³æ€§` `è§†é¢‘å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šè§†è§’è§†é¢‘æ•°æ®é‡å¤§ï¼Œä¼ ç»Ÿæ–¹æ³•å‹ç¼©æ•ˆç‡æœ‰é™ï¼Œéš¾ä»¥æ»¡è¶³æ²‰æµ¸å¼åº”ç”¨éœ€æ±‚ã€‚
2. åˆ©ç”¨ç‹¬ç«‹è§†è§’ä¿¡æ¯è¾…åŠ©ä¾èµ–è§†è§’å‹ç¼©ï¼Œè®¾è®¡è§†è§’é—´è¿åŠ¨å’Œå†…å®¹é¢„æµ‹æ¨¡å—ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ä¼˜äºMV-HEVCæ ‡å‡†ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç¼–ç æä¾›æ–°æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å­¦ä¹ çš„å¤šè§†è§’è§†é¢‘ç¼–ç (LMVC)æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å‹ç¼©æ•ˆç‡ï¼ŒåŒæ—¶ç¡®ä¿éšæœºè®¿é—®å’Œå‘åå…¼å®¹æ€§ã€‚å¤šè§†è§’è§†é¢‘æ˜¯ä½“è§†é¢‘çš„å…³é”®æ•°æ®æ¥æºï¼Œèƒ½å¤Ÿå®ç°æ²‰æµ¸å¼3Dåœºæ™¯é‡å»ºï¼Œä½†å…¶åºå¤§çš„æ•°æ®é‡ç»™å­˜å‚¨å’Œä¼ è¾“å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯è§†é¢‘ç¼–ç å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å¤§å¤šé›†ä¸­äºå•è§†è§’æˆ–ç«‹ä½“è§†é¢‘ï¼Œè€Œå¯¹ä¸€èˆ¬å¤šè§†è§’åœºæ™¯çš„æ¢ç´¢ä¸è¶³ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºæœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è§†è§’çš„è¿åŠ¨å’Œå†…å®¹ä¿¡æ¯æ¥å¢å¼ºä¾èµ–è§†è§’çš„å‹ç¼©ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†åˆ©ç”¨è§†è§’é—´çš„è¿åŠ¨ç›¸å…³æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾çš„è§†è§’é—´è¿åŠ¨çŸ¢é‡é¢„æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä¾èµ–è§†è§’çš„è¿åŠ¨ç¼–ç å»ºç«‹åœ¨è§£ç åçš„ç‹¬ç«‹è§†è§’è¿åŠ¨ç‰¹å¾çš„åŸºç¡€ä¸Šï¼Œä»¥åŠä¸€ä¸ªå­¦ä¹ è§†è§’é—´è¿åŠ¨å…ˆéªŒçš„è§†è§’é—´è¿åŠ¨ç†µæ¨¡å‹ã€‚ä¸ºäº†åˆ©ç”¨è§†è§’é—´çš„å†…å®¹ç›¸å…³æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è§†å·®çš„è§†è§’é—´ä¸Šä¸‹æ–‡é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»è§£ç åçš„ç‹¬ç«‹è§†è§’å†…å®¹ç‰¹å¾ä¸­é¢„æµ‹è§†è§’é—´ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»“åˆä¸€ä¸ªæ•è·è§†è§’é—´ä¸Šä¸‹æ–‡å…ˆéªŒçš„è§†è§’é—´ä¸Šä¸‹æ–‡ç†µæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„LMVCæ¡†æ¶æ˜æ˜¾ä¼˜äºä¼ ç»ŸMV-HEVCæ ‡å‡†çš„å‚è€ƒè½¯ä»¶ï¼Œä¸ºè¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šè§†è§’è§†é¢‘ç¼–ç æ—¨åœ¨é«˜æ•ˆå‹ç¼©å¤šä¸ªè§†è§’çš„è§†é¢‘æ•°æ®ï¼Œä»¥æ»¡è¶³å­˜å‚¨å’Œä¼ è¾“éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚MV-HEVCï¼Œåœ¨å‹ç¼©æ•ˆç‡æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨é«˜åˆ†è¾¨ç‡å’Œé«˜å¸§ç‡åœºæ™¯ä¸‹ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è§†è§’é—´çš„ç›¸å…³æ€§ï¼Œè¿›ä¸€æ­¥æå‡å‹ç¼©æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ è§†è§’é—´çš„è¿åŠ¨å’Œå†…å®¹ç›¸å…³æ€§ï¼Œæ¥æé«˜ä¾èµ–è§†è§’çš„ç¼–ç æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯åˆ©ç”¨å·²è§£ç çš„ç‹¬ç«‹è§†è§’çš„ä¿¡æ¯ï¼Œæ¥é¢„æµ‹å’Œç¼–ç ä¾èµ–è§†è§’çš„è¿åŠ¨çŸ¢é‡å’Œå†…å®¹ç‰¹å¾ï¼Œä»è€Œå‡å°‘å†—ä½™ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLMVCæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) ç‹¬ç«‹è§†è§’ç¼–ç å™¨ï¼šå¯¹ç‹¬ç«‹è§†è§’è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆè¿åŠ¨ç‰¹å¾å’Œå†…å®¹ç‰¹å¾ã€‚2) è§†è§’é—´è¿åŠ¨é¢„æµ‹æ¨¡å—ï¼šåˆ©ç”¨ç‹¬ç«‹è§†è§’çš„è¿åŠ¨ç‰¹å¾ï¼Œé¢„æµ‹ä¾èµ–è§†è§’çš„è¿åŠ¨çŸ¢é‡ã€‚3) è§†è§’é—´å†…å®¹é¢„æµ‹æ¨¡å—ï¼šåˆ©ç”¨ç‹¬ç«‹è§†è§’çš„å†…å®¹ç‰¹å¾ï¼Œé¢„æµ‹ä¾èµ–è§†è§’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚4) ç†µæ¨¡å‹ï¼šåˆ†åˆ«å¯¹è¿åŠ¨çŸ¢é‡å’Œä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œç†µç¼–ç ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ•°æ®ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŸºäºç‰¹å¾çš„è§†è§’é—´è¿åŠ¨çŸ¢é‡é¢„æµ‹æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†è§†è§’é—´çš„è¿åŠ¨ç›¸å…³æ€§ã€‚2) æå‡ºäº†æ— è§†å·®çš„è§†è§’é—´ä¸Šä¸‹æ–‡é¢„æµ‹æ¨¡å—ï¼Œé¿å…äº†è§†å·®ä¼°è®¡çš„å¤æ‚æ€§ï¼Œå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨äº†è§†è§’é—´çš„å†…å®¹ç›¸å…³æ€§ã€‚3) è®¾è®¡äº†è§†è§’é—´è¿åŠ¨ç†µæ¨¡å‹å’Œè§†è§’é—´ä¸Šä¸‹æ–‡ç†µæ¨¡å‹ï¼Œè¿›ä¸€æ­¥æå‡äº†å‹ç¼©æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§†è§’é—´è¿åŠ¨é¢„æµ‹æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œæ¥æå–ç‹¬ç«‹è§†è§’çš„è¿åŠ¨ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œæ¥é¢„æµ‹ä¾èµ–è§†è§’çš„è¿åŠ¨çŸ¢é‡ã€‚åœ¨è§†è§’é—´å†…å®¹é¢„æµ‹æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†ç±»ä¼¼çš„è®¾è®¡ï¼Œä½†é¢„æµ‹çš„æ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’Œç‡å¤±çœŸæŸå¤±ï¼Œç”¨äºå¹³è¡¡å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LMVCæ¡†æ¶åœ¨å‹ç¼©æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºMV-HEVCæ ‡å‡†ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ç›¸åŒè§†è§‰è´¨é‡ä¸‹ï¼ŒLMVCçš„ç ç‡é™ä½äº†XX%ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼ŒåŸæ–‡æœªæä¾›ï¼‰ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶ä¸ºæœªæ¥çš„å¤šè§†è§’è§†é¢‘ç¼–ç ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºVR/ARã€è‡ªç”±è§†ç‚¹è§†é¢‘ã€3Dè§†é¢‘ä¼šè®®ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½å¤šè§†è§’è§†é¢‘çš„å­˜å‚¨å’Œä¼ è¾“æˆæœ¬ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨æ²‰æµ¸å¼åª’ä½“çš„å‘å±•ï¼Œå¹¶ä¸ºç›¸å…³äº§ä¸šå¸¦æ¥æ–°çš„å¢é•¿ç‚¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multiview video is a key data source for volumetric video, enabling immersive 3D scene reconstruction but posing significant challenges in storage and transmission due to its massive data volume. Recently, deep learning-based end-to-end video coding has achieved great success, yet most focus on single-view or stereo videos, leaving general multiview scenarios underexplored. This paper proposes an end-to-end learned multiview video coding (LMVC) framework that ensures random access and backward compatibility while enhancing compression efficiency. Our key innovation lies in effectively leveraging independent-view motion and content information to enhance dependent-view compression. Specifically, to exploit the inter-view motion correlation, we propose a feature-based inter-view motion vector prediction method that conditions dependent-view motion encoding on decoded independent-view motion features, along with an inter-view motion entropy model that learns inter-view motion priors. To exploit the inter-view content correlation, we propose a disparity-free inter-view context prediction module that predicts inter-view contexts from decoded independent-view content features, combined with an inter-view contextual entropy model that captures inter-view context priors. Experimental results show that our proposed LMVC framework outperforms the reference software of the traditional MV-HEVC standard by a large margin, establishing a strong baseline for future research in this field.

