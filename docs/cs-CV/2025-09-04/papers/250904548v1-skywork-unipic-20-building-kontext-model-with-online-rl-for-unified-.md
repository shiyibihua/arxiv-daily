---
layout: default
title: Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model
---

# Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04548v1</a>
  <a href="https://arxiv.org/pdf/2509.04548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04548v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04548v1', 'Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, Chuanxin Tang, Zidong Wang, Yichen Wei, Liang Hu, Boyi Jiang, William Li, Ying He, Yang Liu, Xuchen Song, Eric Li, Yahui Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniPic 2.0ï¼šé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ„å»ºKontextæ¨¡å‹ï¼Œå®ç°ç»Ÿä¸€å¤šæ¨¡æ€å›¾åƒç”Ÿæˆä¸ç¼–è¾‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `å›¾åƒç”Ÿæˆ` `å›¾åƒç¼–è¾‘` `å¼ºåŒ–å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `é¢„è®­ç»ƒ` `æŒ‡ä»¤éµå¾ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æºå¤šæ¨¡æ€æ¨¡å‹ä¾§é‡äºæ‰©å±•æ¨¡å‹å‚æ•°ï¼Œè€Œå¿½ç•¥äº†è®­ç»ƒç­–ç•¥çš„ä¼˜åŒ–ï¼Œå¯¼è‡´æ•ˆç‡å’Œæ€§èƒ½å—é™ã€‚
2. UniPic 2.0é€šè¿‡æ¶æ„ä¿®æ”¹ã€å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œæ¸è¿›å¼åŒä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆPDTRï¼‰ç­–ç•¥ï¼Œæå‡å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚
3. UniPic2-SD3.5M-Kontextåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢ä¼˜äºå‚æ•°é‡æ›´å¤§çš„æ¨¡å‹ï¼ŒUniPic2-Metaqueryåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†UniPic2-SD3.5M-Kontextï¼Œä¸€ä¸ªåŸºäºSD3.5-Mediumçš„20äº¿å‚æ•°DiTæ¨¡å‹ï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶æ— ç¼æ‰©å±•åˆ°ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ä¸­ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¯¹SD3.5-Mediumè¿›è¡Œæ¶æ„ä¿®æ”¹ï¼Œå¹¶åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œä»è€Œå®ç°è”åˆæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚ä¸ºäº†å¢å¼ºæŒ‡ä»¤éµå¾ªå’Œç¼–è¾‘ä¸€è‡´æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¸è¿›å¼åŒä»»åŠ¡å¼ºåŒ–ç­–ç•¥ï¼ˆPDTRï¼‰ï¼Œæœ‰æ•ˆåœ°åˆ†é˜¶æ®µåŠ å¼ºäº†è¿™ä¸¤é¡¹ä»»åŠ¡ã€‚å®éªŒéªŒè¯äº†ä¸åŒä»»åŠ¡çš„å¼ºåŒ–é˜¶æ®µæ˜¯äº’åˆ©çš„ï¼Œä¸ä¼šå¼•èµ·è´Ÿé¢å¹²æ‰°ã€‚ç»è¿‡é¢„è®­ç»ƒå’Œå¼ºåŒ–ç­–ç•¥åï¼ŒUniPic2-SD3.5M-Kontextå±•ç¤ºäº†æ¯”å…·æœ‰æ›´å¤§ç”Ÿæˆå‚æ•°çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬BAGEL (7B) å’Œ Flux-Kontext (12B)ï¼‰æ›´å¼ºçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œéµå¾ªMetaQueryï¼Œé€šè¿‡è¿æ¥å™¨å°†UniPic2-SD3.5M-Kontextå’ŒQwen2.5-VL-7Bè¿æ¥èµ·æ¥ï¼Œå¹¶è¿›è¡Œè”åˆè®­ç»ƒï¼Œæ¨å‡ºäº†ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹UniPic2-Metaqueryã€‚UniPic2-Metaqueryé›†æˆäº†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ï¼Œé€šè¿‡ç®€å•ä¸”å¯æ‰©å±•çš„è®­ç»ƒèŒƒå¼ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å®ç°äº†é¡¶çº§çš„æ€§èƒ½ã€‚è¿™å§‹ç»ˆéªŒè¯äº†æ‰€æå‡ºçš„è®­ç»ƒèŒƒå¼çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºSkywork UniPic 2.0ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼€æºå¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œç”±äºè¿‡åº¦å…³æ³¨æ¨¡å‹å‚æ•°è§„æ¨¡è€Œå¿½ç•¥è®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼Œå¯¼è‡´æ•ˆç‡å’Œæ€§èƒ½ç“¶é¢ˆçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¹¶ä¸”åœ¨æŒ‡ä»¤éµå¾ªå’Œç¼–è¾‘ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼Œè€Œéå•çº¯å¢åŠ æ¨¡å‹å‚æ•°ï¼Œæ¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡æ¶æ„æ”¹è¿›ã€å¤§è§„æ¨¡é¢„è®­ç»ƒä»¥åŠåˆ›æ–°çš„æ¸è¿›å¼åŒä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆPDTRï¼‰ç­–ç•¥ï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´ä¸€è‡´çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniPic 2.0çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œå¯¹SD3.5-Mediumæ¨¡å‹è¿›è¡Œæ¶æ„ä¿®æ”¹ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€ä»»åŠ¡ï¼›å…¶æ¬¡ï¼Œåœ¨å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹å…·å¤‡åŸºæœ¬çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ï¼›æœ€åï¼Œé‡‡ç”¨PDTRç­–ç•¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’Œç¼–è¾‘ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚UniPic2-Metaqueryåˆ™è¿›ä¸€æ­¥å°†UniPic2-SD3.5M-Kontextä¸Qwen2.5-VL-7Bé€šè¿‡è¿æ¥å™¨è¿æ¥ï¼Œè¿›è¡Œè”åˆè®­ç»ƒï¼Œå½¢æˆç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯æå‡ºçš„æ¸è¿›å¼åŒä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆPDTRï¼‰ç­–ç•¥ã€‚PDTRç­–ç•¥é€šè¿‡åˆ†é˜¶æ®µçš„æ–¹å¼ï¼Œåˆ†åˆ«å¯¹å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œé¿å…äº†ä¸¤ç§ä»»åŠ¡ä¹‹é—´çš„è´Ÿé¢å¹²æ‰°ï¼Œå®ç°äº†äº’åˆ©å…±èµ¢çš„æ•ˆæœã€‚è¿™ç§ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªå’Œç¼–è¾‘ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šPDTRç­–ç•¥çš„å…³é”®è®¾è®¡åœ¨äºå…¶æ¸è¿›å¼çš„è®­ç»ƒæ–¹å¼ã€‚é¦–å…ˆï¼Œå¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶å…·å¤‡åŸºæœ¬çš„ç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚ç„¶åï¼Œåˆ†åˆ«é’ˆå¯¹ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ï¼Œè®¾è®¡ç›¸åº”çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€æ­¥å¢åŠ å¥–åŠ±å‡½æ•°çš„æƒé‡ï¼Œä½¿æ¨¡å‹é€æ¸é€‚åº”æŒ‡ä»¤çš„è¦æ±‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†MetaQueryæ–¹æ³•ï¼Œå°†UniPic2-SD3.5M-Kontextä¸Qwen2.5-VL-7Bè¿æ¥ï¼Œå¹¶é€šè¿‡è”åˆè®­ç»ƒï¼Œå®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniPic2-SD3.5M-Kontextåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢ä¼˜äºå‚æ•°é‡æ›´å¤§çš„æ¨¡å‹ï¼Œä¾‹å¦‚BAGEL (7B) å’Œ Flux-Kontext (12B)ã€‚UniPic2-Metaqueryé€šè¿‡ç®€å•çš„è®­ç»ƒèŒƒå¼ï¼Œåœ¨å„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°äº†é¡¶çº§çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€æå‡ºçš„è®­ç»ƒèŒƒå¼çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€å†…å®¹åˆ›ä½œã€è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„è´¨é‡å’Œæ•ˆç‡ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´ä¸°å¯Œçš„è§†è§‰ä½“éªŒï¼Œå¹¶é™ä½å†…å®¹åˆ›ä½œçš„é—¨æ§›ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½è®¾è®¡ã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆç­‰æ›´å¹¿æ³›çš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.

