---
layout: default
title: SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation
---

# SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03897" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03897v2</a>
  <a href="https://arxiv.org/pdf/2509.03897.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03897v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03897v2', 'SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaofu Chen, Israfel Salazar, Yova Kementchedjhieva

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04 (æ›´æ–°: 2025-09-12)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mbzuai-nlp/SPECS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SPECSï¼šç”¨äºé•¿å›¾åƒæè¿°è¯„ä¼°çš„ç‰¹å¼‚æ€§å¢å¼ºCLIP-Score**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒæè¿°è¯„ä¼°` `CLIPæ¨¡å‹` `è¡¨å¾ç›¸ä¼¼æ€§` `ç‰¹å¼‚æ€§å¢å¼º` `é•¿æ–‡æœ¬ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ— å‚è€ƒè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒæè¿°è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚N-gramå’Œä¼ ç»ŸRSæŒ‡æ ‡ï¼Œåœ¨è¯„ä¼°é•¿æ–‡æœ¬æè¿°æ—¶å­˜åœ¨è¯­ä¹‰æ•æ‰ä¸è¶³å’Œä¸äººç±»åˆ¤æ–­ç›¸å…³æ€§ä½çš„é—®é¢˜ã€‚
2. SPECSé€šè¿‡å¼•å…¥ç‰¹å¼‚æ€§å¢å¼ºç›®æ ‡å‡½æ•°æ”¹è¿›CLIPæ¨¡å‹ï¼Œå¥–åŠ±æ­£ç¡®ç»†èŠ‚å¹¶æƒ©ç½šé”™è¯¯ç»†èŠ‚ï¼Œä»è€Œæå‡è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSPECSåœ¨ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§ä¸Šå¯ä¸åŸºäºLLMçš„æŒ‡æ ‡åª²ç¾ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œæ›´é€‚åˆè¿­ä»£å¼€å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç”Ÿæˆé•¿è€Œè¯¦ç»†çš„å›¾åƒæè¿°çš„éœ€æ±‚å¢é•¿ï¼Œæ ‡å‡†çš„è¯„ä¼°æŒ‡æ ‡å˜å¾—è¶Šæ¥è¶Šä¸å¯é ã€‚åŸºäºN-gramçš„æŒ‡æ ‡è™½ç„¶é«˜æ•ˆï¼Œä½†æ— æ³•æ•æ‰è¯­ä¹‰æ­£ç¡®æ€§ã€‚è¡¨å¾ç›¸ä¼¼æ€§(RS)æŒ‡æ ‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œæœ€åˆçš„ä½¿ç”¨å—åˆ°é™åˆ¶ã€‚å°½ç®¡ç¡¬ä»¶å–å¾—äº†è¿›æ­¥ï¼Œä½†ç”±äºä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§è¾ƒä½ï¼ŒRSæŒ‡æ ‡ä»ç„¶ä¸å—æ¬¢è¿ã€‚åŒæ—¶ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­è¡¨ç°å‡ºå¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œä½†å¯¹äºæ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­çš„è¿­ä»£ä½¿ç”¨æ¥è¯´ï¼Œæˆæœ¬ä»ç„¶è¿‡é«˜ã€‚æˆ‘ä»¬å¼•å…¥äº†SPECSï¼ˆç‰¹å¼‚æ€§å¢å¼ºCLIPScoreï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºé•¿å›¾åƒæè¿°é‡èº«å®šåˆ¶çš„æ— å‚è€ƒRSæŒ‡æ ‡ã€‚SPECSé€šè¿‡ä¸€ä¸ªæ–°çš„ç›®æ ‡ä¿®æ”¹äº†CLIPï¼Œè¯¥ç›®æ ‡å¼ºè°ƒç‰¹å¼‚æ€§ï¼šå¥–åŠ±æ­£ç¡®çš„ç»†èŠ‚ï¼Œæƒ©ç½šä¸æ­£ç¡®çš„ç»†èŠ‚ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒSPECSåœ¨ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ–¹é¢ä¸åŸºäºå¼€æºLLMçš„æŒ‡æ ‡çš„æ€§èƒ½ç›¸åŒ¹é…ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ã€‚è¿™ä½¿å…¶æˆä¸ºå›¾åƒæè¿°æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­è¿­ä»£æ£€æŸ¥ç‚¹è¯„ä¼°çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é•¿å›¾åƒæè¿°è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚N-gramæŒ‡æ ‡æ— æ³•æ•æ‰è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œä¼ ç»Ÿçš„è¡¨å¾ç›¸ä¼¼æ€§(RS)æŒ‡æ ‡è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§è¾ƒä½ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æŒ‡æ ‡è™½ç„¶ç›¸å…³æ€§é«˜ï¼Œä½†è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œä¸é€‚åˆæ¨¡å‹è¿­ä»£å¼€å‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPECSçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¼ºCLIPæ¨¡å‹çš„ç‰¹å¼‚æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°é•¿å›¾åƒæè¿°çš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼ŒSPECSä¿®æ”¹äº†CLIPçš„ç›®æ ‡å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿå¥–åŠ±æè¿°ä¸­æ­£ç¡®çš„ç»†èŠ‚ï¼Œå¹¶æƒ©ç½šé”™è¯¯çš„ç»†èŠ‚ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPECSåŸºäºCLIPæ¨¡å‹ï¼Œå¹¶å¯¹å…¶ç›®æ ‡å‡½æ•°è¿›è¡Œäº†ä¿®æ”¹ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨CLIPæ¨¡å‹æå–å›¾åƒå’Œæè¿°çš„ç‰¹å¾ï¼›2) ä½¿ç”¨ä¿®æ”¹åçš„ç›®æ ‡å‡½æ•°è®¡ç®—å›¾åƒå’Œæè¿°ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼›3) ä½¿ç”¨è¯¥å¾—åˆ†ä½œä¸ºå›¾åƒæè¿°çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥æ¡†æ¶æ˜¯å‚è€ƒæ— ç›‘ç£çš„ï¼Œä¸éœ€è¦é¢å¤–çš„å‚è€ƒæè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPECSçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç‰¹å¼‚æ€§å¢å¼ºçš„CLIPç›®æ ‡å‡½æ•°ã€‚è¯¥ç›®æ ‡å‡½æ•°é€šè¿‡å¥–åŠ±æ­£ç¡®ç»†èŠ‚å’Œæƒ©ç½šé”™è¯¯ç»†èŠ‚ï¼Œæé«˜äº†CLIPæ¨¡å‹åœ¨é•¿å›¾åƒæè¿°è¯„ä¼°ä¸­çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSPECSåœ¨ä¿æŒè¾ƒé«˜è¯„ä¼°å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šSPECSçš„å…³é”®è®¾è®¡åœ¨äºç‰¹å¼‚æ€§å¢å¼ºçš„æŸå¤±å‡½æ•°ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å¥–åŠ±ä¸å›¾åƒå†…å®¹ç›¸å…³çš„ç‰¹å®šç»†èŠ‚ï¼Œå¹¶æƒ©ç½šä¸å›¾åƒå†…å®¹ä¸ç¬¦çš„ç»†èŠ‚ã€‚è¿™ç§è®¾è®¡ä½¿å¾—SPECSèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°é•¿å›¾åƒæè¿°çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SPECSåœ¨ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ–¹é¢è¾¾åˆ°äº†ä¸å¼€æºLLMæŒ‡æ ‡ç›¸å½“çš„æ°´å¹³ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡è¿œé«˜äºLLMæŒ‡æ ‡ã€‚è¿™ä½¿å¾—SPECSæˆä¸ºé•¿å›¾åƒæè¿°æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­è¿­ä»£è¯„ä¼°çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPECSå¯åº”ç”¨äºå›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°å’Œè¿­ä»£ä¼˜åŒ–ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦ç”Ÿæˆé•¿è€Œè¯¦ç»†æè¿°çš„åœºæ™¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´é«˜æ•ˆåœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å¿«é€Ÿè¿­ä»£æ”¹è¿›æ¨¡å‹ï¼Œä»è€Œæå‡å›¾åƒæè¿°ç”Ÿæˆçš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒSPECSä¹Ÿå¯ç”¨äºè¯„ä¼°å…¶ä»–å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘æè¿°å’Œè§†è§‰æ•…äº‹è®²è¿°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development.
>   We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS.

