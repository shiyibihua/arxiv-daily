---
layout: default
title: TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection
---

# TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04448" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04448v2</a>
  <a href="https://arxiv.org/pdf/2509.04448.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04448v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04448v2', 'TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04 (æ›´æ–°: 2025-10-30)

**å¤‡æ³¨**: EMNLP 2025 Oral; Project Homepage: https://yanzehong.github.io/trust-vl/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTRUST-VLï¼Œä¸€ä¸ªå¯è§£é‡Šçš„å¤šæ¨¡æ€æ–°é—»åŠ©æ‰‹ï¼Œç”¨äºæ£€æµ‹é€šç”¨å¤šæ¨¡æ€è™šå‡ä¿¡æ¯ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `Question-Aware Visual Amplifier` `æŒ‡ä»¤å­¦ä¹ ` `çŸ¥è¯†å…±äº«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹ä¸­ï¼Œé€šå¸¸åªå…³æ³¨å•ä¸€ç±»å‹çš„æ‰­æ›²ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. TRUST-VLé€šè¿‡è”åˆè®­ç»ƒä¸åŒæ‰­æ›²ç±»å‹ï¼Œå¹¶å¼•å…¥Question-Aware Visual Amplifieræ¨¡å—ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
3. TRUST-VLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶æ„å»ºäº†å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†TRUST-Instructã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€è™šå‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’Œè·¨æ¨¡æ€æ‰­æ›²ï¼Œå¯¹ç¤¾ä¼šæ„æˆæ—¥ç›Šä¸¥é‡çš„å¨èƒï¼Œå¹¶ä¸”è¢«ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ”¾å¤§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºå•ä¸€ç±»å‹çš„æ‰­æ›²ï¼Œéš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„åœºæ™¯ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ï¼Œä¸åŒçš„æ‰­æ›²ç±»å‹å…±äº«é€šç”¨çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿéœ€è¦ç‰¹å®šäºä»»åŠ¡çš„æŠ€èƒ½ã€‚æˆ‘ä»¬å‡è®¾è·¨æ‰­æ›²ç±»å‹è¿›è¡Œè”åˆè®­ç»ƒæœ‰åŠ©äºçŸ¥è¯†å…±äº«ï¼Œå¹¶å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†TRUST-VLï¼Œä¸€ä¸ªç»Ÿä¸€ä¸”å¯è§£é‡Šçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé€šç”¨å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹ã€‚TRUST-VLåŒ…å«ä¸€ä¸ªæ–°é¢–çš„Question-Aware Visual Amplifieræ¨¡å—ï¼Œæ—¨åœ¨æå–ç‰¹å®šäºä»»åŠ¡çš„è§†è§‰ç‰¹å¾ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†TRUST-Instructï¼Œä¸€ä¸ªåŒ…å«198Kæ ·æœ¬çš„å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸äººå·¥äº‹å®æ ¸æŸ¥å·¥ä½œæµç¨‹å¯¹é½çš„ç»“æ„åŒ–æ¨ç†é“¾ã€‚åœ¨é¢†åŸŸå†…å’Œé›¶æ ·æœ¬åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTRUST-VLå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¼ºå¤§çš„æ³›åŒ–æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é€šç”¨å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆå¤„ç†å¤šç§ç±»å‹çš„æ‰­æ›²ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œå¹¶ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚è¿™äº›æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šç±»å‹çš„è™šå‡ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ï¼Œéš¾ä»¥é€‚åº”æ–°çš„ã€æœªçŸ¥çš„è™šå‡ä¿¡æ¯æ¨¡å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸åŒç±»å‹çš„è™šå‡ä¿¡æ¯ä¹‹é—´å…±äº«çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡è”åˆè®­ç»ƒæ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¼•å…¥Question-Aware Visual Amplifieræ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å…·ä½“ä»»åŠ¡æå–ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTRUST-VLæ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æ–‡æœ¬ç¼–ç å™¨ï¼šç”¨äºæå–æ–‡æœ¬ç‰¹å¾ï¼›2) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºæå–è§†è§‰ç‰¹å¾ï¼›3) Question-Aware Visual Amplifierï¼šæ ¹æ®é—®é¢˜æå–ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼›4) èåˆæ¨¡å—ï¼šå°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾è¿›è¡Œèåˆï¼›5) åˆ†ç±»å™¨ï¼šç”¨äºåˆ¤æ–­ä¿¡æ¯æ˜¯å¦ä¸ºè™šå‡ä¿¡æ¯ã€‚æ•´ä¸ªæµç¨‹æ˜¯å…ˆåˆ†åˆ«ç¼–ç æ–‡æœ¬å’Œå›¾åƒï¼Œç„¶åé€šè¿‡Question-Aware Visual Amplifierå¢å¼ºè§†è§‰ç‰¹å¾ï¼Œå†å°†å¢å¼ºåçš„è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾èåˆï¼Œæœ€åé€šè¿‡åˆ†ç±»å™¨è¿›è¡Œåˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯Question-Aware Visual Amplifieræ¨¡å—ã€‚è¯¥æ¨¡å—èƒ½å¤Ÿæ ¹æ®æå‡ºçš„é—®é¢˜ï¼ŒåŠ¨æ€åœ°è°ƒæ•´è§†è§‰ç‰¹å¾çš„æƒé‡ï¼Œä»è€Œæå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¸­ç›´æ¥ä½¿ç”¨å…¨å±€è§†è§‰ç‰¹å¾çš„æ–¹å¼ä¸åŒï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰åˆ°è™šå‡ä¿¡æ¯ä¸­çš„ç»†å¾®çº¿ç´¢ã€‚

**å…³é”®è®¾è®¡**ï¼šTRUST-VLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Question-Aware Visual Amplifierçš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è®¡ç®—è§†è§‰ç‰¹å¾çš„æƒé‡ï¼›2) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒåˆ†ç±»å™¨ï¼›3) TRUST-Instructæ•°æ®é›†çš„æ„å»ºï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†å¤§é‡çš„ç»“æ„åŒ–æ¨ç†é“¾ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TRUST-VLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨é¢†åŸŸå†…æµ‹è¯•ä¸­ï¼ŒTRUST-VLçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†X%ã€‚åœ¨é›¶æ ·æœ¬æµ‹è¯•ä¸­ï¼ŒTRUST-VLä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†æœªçŸ¥è™šå‡ä¿¡æ¯æ–¹é¢çš„ä¼˜åŠ¿ã€‚TRUST-Instructæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºå¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹é¢†åŸŸæä¾›äº†å®è´µçš„æ•°æ®èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ–°é—»åª’ä½“ã€ç¤¾äº¤å¹³å°ç­‰é¢†åŸŸï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ«å’Œè¿‡æ»¤è™šå‡ä¿¡æ¯ï¼Œæé«˜ä¿¡æ¯çš„å¯ä¿¡åº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€åœºæ™¯ï¼Œä¾‹å¦‚è§†é¢‘å†…å®¹åˆ†æã€åŒ»å­¦å›¾åƒè¯Šæ–­ç­‰ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.

