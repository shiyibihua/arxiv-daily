---
layout: default
title: VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation
---

# VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04669" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04669v1</a>
  <a href="https://arxiv.org/pdf/2509.04669.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04669v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04669v1', 'VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mustafa Munir, Alex Zhang, Radu Marculescu

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Proceedings of the 2025 IEEE/CVF International Conference on Computer Vision (ICCV) Workshops

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Wertyuui345/VCMamba)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VCMambaï¼šèåˆå·ç§¯ä¸å¤šå‘Mambaï¼Œå®ç°é«˜æ•ˆè§†è§‰è¡¨å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰éª¨å¹²ç½‘ç»œ` `å·ç§¯ç¥ç»ç½‘ç»œ` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `Mamba` `å›¾åƒåˆ†ç±»` `è¯­ä¹‰åˆ†å‰²` `é•¿ç¨‹ä¾èµ–` `å±€éƒ¨ç‰¹å¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ViTå’ŒMambaæ¨¡å‹åœ¨æ•è·å…¨å±€ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹å±€éƒ¨ç»†ç²’åº¦ç‰¹å¾çš„æå–èƒ½åŠ›ä¸å¦‚CNNã€‚
2. VCMambaèåˆCNNå’Œå¤šå‘Mamba SSMçš„ä¼˜åŠ¿ï¼Œåˆ©ç”¨CNNæå–å±€éƒ¨ç‰¹å¾ï¼ŒMambaå»ºæ¨¡é•¿ç¨‹ä¾èµ–ï¼Œå®ç°é«˜æ•ˆè§†è§‰è¡¨å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVCMambaåœ¨ImageNet-1Kå’ŒADE20Kä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå‚æ•°é‡æ˜¾è‘—å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†è§‰Transformer (ViT) å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM) å¯¹å·ç§¯ç¥ç»ç½‘ç»œ (CNN) åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç»Ÿæ²»åœ°ä½æå‡ºäº†æŒ‘æˆ˜ã€‚ViTæ“…é•¿æ•è·å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè€ŒåƒMambaè¿™æ ·çš„SSMä¸ºé•¿åºåˆ—æä¾›äº†çº¿æ€§å¤æ‚åº¦ï¼Œä½†å®ƒä»¬åœ¨æ•è·ç»†ç²’åº¦çš„å±€éƒ¨ç‰¹å¾æ–¹é¢ä¸å¦‚CNNæœ‰æ•ˆã€‚ç›¸åï¼ŒCNNå…·æœ‰å¼ºå¤§çš„å±€éƒ¨ç‰¹å¾å½’çº³åç½®ï¼Œä½†ç¼ºä¹Transformerå’ŒMambaçš„å…¨å±€æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†VCMambaï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰éª¨å¹²ç½‘ç»œï¼Œå®ƒé›†æˆäº†CNNå’Œå¤šå‘Mamba SSMçš„ä¼˜åŠ¿ã€‚VCMambaé‡‡ç”¨å·ç§¯stemå’Œå…·æœ‰å·ç§¯å—çš„åˆ†å±‚ç»“æ„ï¼Œä»¥æå–ä¸°å¯Œçš„å±€éƒ¨ç‰¹å¾ã€‚è¿™äº›å·ç§¯å—éšåç”±åŒ…å«å¤šå‘Mambaå—çš„åç»­é˜¶æ®µå¤„ç†ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚è¿™ç§æ··åˆè®¾è®¡å…è®¸å®ç°å“è¶Šçš„ç‰¹å¾è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒç›¸å¯¹äºå›¾åƒåˆ†è¾¨ç‡çš„çº¿æ€§å¤æ‚åº¦ã€‚æˆ‘ä»¬åœ¨ImageNet-1Kåˆ†ç±»å’ŒADE20Kè¯­ä¹‰åˆ†å‰²ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†VCMambaçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„VCMamba-Båœ¨ImageNet-1Kä¸Šå®ç°äº†82.6%çš„top-1å‡†ç¡®ç‡ï¼Œè¶…è¿‡PlainMamba-L3 0.3%ï¼Œå‚æ•°å‡å°‘äº†37%ï¼Œå¹¶ä¸”è¶…è¿‡Vision GNN-B 0.3%ï¼Œå‚æ•°å‡å°‘äº†64%ã€‚æ­¤å¤–ï¼ŒVCMamba-Båœ¨ADE20Kä¸Šè·å¾—äº†47.1 mIoUï¼Œè¶…è¿‡EfficientFormer-L7 2.0 mIoUï¼ŒåŒæ—¶å‚æ•°å‡å°‘äº†62%ã€‚ä»£ç å¯åœ¨https://github.com/Wertyuui345/VCMamba è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰æ¨¡å‹ï¼Œå¦‚ViTå’ŒMambaï¼Œè™½ç„¶åœ¨å…¨å±€ä¿¡æ¯å»ºæ¨¡ä¸Šæœ‰æ‰€çªç ´ï¼Œä½†åœ¨å±€éƒ¨ç‰¹å¾æå–æ–¹é¢ä»æœ‰ä¸è¶³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å›¾åƒçš„å±€éƒ¨ç»“æ„ä¿¡æ¯ã€‚CNNè™½ç„¶æ“…é•¿å±€éƒ¨ç‰¹å¾æå–ï¼Œä½†ç¼ºä¹å…¨å±€å»ºæ¨¡èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆCNNå’ŒMambaçš„ä¼˜åŠ¿ï¼Œæ„å»ºä¸€ä¸ªæ—¢èƒ½æå–å±€éƒ¨ç‰¹å¾åˆèƒ½å»ºæ¨¡å…¨å±€ä¾èµ–çš„è§†è§‰éª¨å¹²ç½‘ç»œï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVCMambaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†CNNå’Œå¤šå‘Mamba SSMç»“åˆèµ·æ¥ï¼Œåˆ©ç”¨CNNæå–å±€éƒ¨ç‰¹å¾ï¼Œåˆ©ç”¨Mambaå»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚é€šè¿‡è¿™ç§æ··åˆæ¶æ„ï¼ŒVCMambaå¯ä»¥åŒæ—¶æ•è·å›¾åƒçš„å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è§†è§‰è¡¨å¾ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¼¥è¡¥ç°æœ‰è§†è§‰æ¨¡å‹åœ¨å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVCMambaçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå·ç§¯stemå’Œåˆ†å±‚ç»“æ„ã€‚å·ç§¯stemç”¨äºæå–åˆå§‹çš„å±€éƒ¨ç‰¹å¾ã€‚åˆ†å±‚ç»“æ„åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œæ—©æœŸé˜¶æ®µä½¿ç”¨å·ç§¯å—æ¥æå–ä¸°å¯Œçš„å±€éƒ¨ç‰¹å¾ï¼Œåç»­é˜¶æ®µä½¿ç”¨å¤šå‘Mambaå—æ¥å»ºæ¨¡é•¿ç¨‹ä¾èµ–å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚è¿™ç§åˆ†å±‚ç»“æ„ä½¿å¾—VCMambaèƒ½å¤Ÿé€æ­¥åœ°ä»å±€éƒ¨åˆ°å…¨å±€åœ°æå–å›¾åƒç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šVCMambaçš„å…³é”®åˆ›æ–°åœ¨äºå°†å·ç§¯å’Œå¤šå‘Mamba SSMæœ‰æœºåœ°ç»“åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡å·ç§¯stemå’Œæ—©æœŸå·ç§¯å—ï¼ŒVCMambaèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å±€éƒ¨ç‰¹å¾ï¼Œè€Œå¤šå‘Mambaå—åˆ™èƒ½å¤Ÿå»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚è¿™ç§æ··åˆæ¶æ„ä½¿å¾—VCMambaèƒ½å¤ŸåŒæ—¶æ•è·å›¾åƒçš„å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è§†è§‰è¡¨å¾ã€‚ä¸çº¯CNNæˆ–çº¯Mambaæ¨¡å‹ç›¸æ¯”ï¼ŒVCMambaèƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚

**å…³é”®è®¾è®¡**ï¼šVCMambaçš„å…³é”®è®¾è®¡åŒ…æ‹¬å·ç§¯stemçš„å‚æ•°è®¾ç½®ã€å·ç§¯å—çš„ç»“æ„è®¾è®¡ã€å¤šå‘Mambaå—çš„é…ç½®ä»¥åŠåˆ†å±‚ç»“æ„çš„å±‚æ•°å’Œé€šé“æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œå·ç§¯stemé€šå¸¸é‡‡ç”¨è¾ƒå°çš„å·ç§¯æ ¸å’Œæ­¥é•¿ï¼Œä»¥æå–ç»†ç²’åº¦çš„å±€éƒ¨ç‰¹å¾ã€‚å·ç§¯å—å¯ä»¥é‡‡ç”¨ä¸åŒçš„å·ç§¯æ“ä½œï¼Œå¦‚æ·±åº¦å¯åˆ†ç¦»å·ç§¯æˆ–åˆ†ç»„å·ç§¯ï¼Œä»¥å‡å°‘è®¡ç®—é‡ã€‚å¤šå‘Mambaå—çš„è®¾è®¡éœ€è¦è€ƒè™‘ä¸åŒæ–¹å‘ä¸Šçš„ä¿¡æ¯äº¤äº’ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚åˆ†å±‚ç»“æ„çš„å±‚æ•°å’Œé€šé“æ•°éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VCMamba-Båœ¨ImageNet-1Kä¸Šå®ç°äº†82.6%çš„top-1å‡†ç¡®ç‡ï¼Œè¶…è¿‡PlainMamba-L3 0.3%ï¼Œå‚æ•°å‡å°‘äº†37%ï¼Œè¶…è¿‡Vision GNN-B 0.3%ï¼Œå‚æ•°å‡å°‘äº†64%ã€‚åœ¨ADE20Kä¸Šè·å¾—äº†47.1 mIoUï¼Œè¶…è¿‡EfficientFormer-L7 2.0 mIoUï¼ŒåŒæ—¶å‚æ•°å‡å°‘äº†62%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVCMambaåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VCMambaä½œä¸ºä¸€ç§é€šç”¨çš„è§†è§‰éª¨å¹²ç½‘ç»œï¼Œå¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ã€‚å…¶é«˜æ•ˆçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›å’Œçº¿æ€§å¤æ‚åº¦ä½¿å…¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½ã€‚æœªæ¥ï¼ŒVCMambaæœ‰æœ›åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.

