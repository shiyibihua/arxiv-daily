---
layout: default
title: Promptception: How Sensitive Are Large Multimodal Models to Prompts?
---

# Promptception: How Sensitive Are Large Multimodal Models to Prompts?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03986" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03986v1</a>
  <a href="https://arxiv.org/pdf/2509.03986.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03986v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03986v1', 'Promptception: How Sensitive Are Large Multimodal Models to Prompts?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamed Insaf Ismithdeen, Muhammad Uzair Khattak, Salman Khan

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Accepted to EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Promptceptionæ¡†æ¶æ­ç¤ºå¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹æç¤ºè¯çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºä¼˜åŒ–åŸåˆ™ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡å‹` `æç¤ºè¯å·¥ç¨‹` `æ•æ„Ÿæ€§åˆ†æ` `å¤šé¡¹é€‰æ‹©é—®ç­”` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LMMè¯„ä¼°ä¸­ï¼Œæç¤ºè¯çš„å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—æ³¢åŠ¨ï¼Œå½±å“è¯„ä¼°çš„å…¬å¹³æ€§å’Œé€æ˜æ€§ã€‚
2. Promptceptionæ¡†æ¶é€šè¿‡ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒç±»å‹çš„æç¤ºè¯ï¼Œé‡åŒ–LMMå¯¹æç¤ºè¯çš„æ•æ„Ÿç¨‹åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹å¯¹æç¤ºè¯æ›´æ•æ„Ÿï¼Œè€Œå¼€æºæ¨¡å‹æ›´ç¨³å®šï¼Œå¹¶æ®æ­¤æå‡ºäº†é’ˆå¯¹ä¸åŒç±»å‹LMMçš„æç¤ºåŸåˆ™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é’ˆå¯¹å¤šé¡¹é€‰æ‹©é—®ç­”ï¼ˆMCQAï¼‰ä»»åŠ¡çš„LMMæç¤ºè¯è®¾è®¡ä»ç„¶ç¼ºä¹æ·±å…¥ç†è§£ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æç¤ºè¯çš„æªè¾å’Œç»“æ„å‘ç”Ÿç»†å¾®å˜åŒ–ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´æŸäº›æç¤ºè¯å’Œæ¨¡å‹çš„å‡†ç¡®ç‡åå·®é«˜è¾¾15%ã€‚è¿™ç§å¯å˜æ€§å¯¹é€æ˜å’Œå…¬å¹³çš„LMMè¯„ä¼°æ„æˆäº†æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹é€šå¸¸ä¼šæŠ¥å‘Šä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„æç¤ºè¯è·å¾—çš„æœ€ä½³æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Promptceptionï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°LMMä¸­æç¤ºè¯æ•æ„Ÿæ€§çš„ç³»ç»Ÿæ¡†æ¶ã€‚å®ƒåŒ…å«61ç§æç¤ºè¯ç±»å‹ï¼Œæ¶µç›–15ä¸ªç±»åˆ«å’Œ6ä¸ªè¶…ç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«éƒ½é’ˆå¯¹æç¤ºè¯åˆ¶å®šçš„ç‰¹å®šæ–¹é¢ã€‚è¯¥æ¡†æ¶ç”¨äºè¯„ä¼°10ä¸ªLMMï¼ŒèŒƒå›´ä»è½»é‡çº§å¼€æºæ¨¡å‹åˆ°GPT-4oå’ŒGemini 1.5 Proï¼Œå¹¶ä½¿ç”¨MMStarã€MMMU-Proã€MVBenchè¿™3ä¸ªMCQAåŸºå‡†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹å¯¹æç¤ºè¯æªè¾çš„æ•æ„Ÿæ€§æ›´é«˜ï¼Œåæ˜ äº†ä¸æŒ‡ä»¤è¯­ä¹‰çš„æ›´ç´§å¯†å¯¹é½ï¼Œè€Œå¼€æºæ¨¡å‹åˆ™æ›´ç¨³å®šï¼Œä½†åœ¨ç»†è‡´å’Œå¤æ‚çš„æªè¾æ–¹é¢è¡¨ç°ä¸ä½³ã€‚åŸºäºæ­¤åˆ†æï¼Œæœ¬æ–‡æå‡ºäº†é’ˆå¯¹ä¸“æœ‰å’Œå¼€æºLMMé‡èº«å®šåˆ¶çš„æç¤ºåŸåˆ™ï¼Œä»è€Œå®ç°æ›´å¼ºå¤§å’Œå…¬å¹³çš„æ¨¡å‹è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤šé¡¹é€‰æ‹©é—®ç­”ï¼ˆMCQAï¼‰ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½å¯¹æç¤ºè¯çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„å°‘é‡æç¤ºè¯æ¥è¯„ä¼°æ¨¡å‹ï¼Œå¿½ç•¥äº†æç¤ºè¯çš„å¾®å°å˜åŒ–å¯èƒ½å¸¦æ¥çš„æ€§èƒ½æ³¢åŠ¨ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœç¼ºä¹å¯é æ€§å’Œå…¬å¹³æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªå…¨é¢çš„æç¤ºè¯é›†åˆï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LMMå¯¹ä¸åŒç±»å‹æç¤ºè¯çš„æ•æ„Ÿç¨‹åº¦ã€‚é€šè¿‡åˆ†ææ¨¡å‹åœ¨ä¸åŒæç¤ºè¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œæ­ç¤ºæ¨¡å‹å¯¹æç¤ºè¯çš„ä¾èµ–æ€§ï¼Œå¹¶æ®æ­¤æå‡ºæ›´é²æ£’çš„è¯„ä¼°æ–¹æ³•å’Œæç¤ºè¯è®¾è®¡åŸåˆ™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPromptceptionæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) æç¤ºè¯é›†åˆæ„å»ºï¼šè®¾è®¡äº†61ç§æç¤ºè¯ç±»å‹ï¼Œæ¶µç›–15ä¸ªç±»åˆ«å’Œ6ä¸ªè¶…ç±»åˆ«ï¼Œé’ˆå¯¹æç¤ºè¯çš„ä¸åŒæ–¹é¢è¿›è¡Œæµ‹è¯•ã€‚2) æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨æ„å»ºçš„æç¤ºè¯é›†åˆï¼Œåœ¨å¤šä¸ªMCQAåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ä¸åŒçš„LMMã€‚3) æ•æ„Ÿæ€§åˆ†æï¼šåˆ†ææ¨¡å‹åœ¨ä¸åŒæç¤ºè¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œé‡åŒ–æ¨¡å‹å¯¹æç¤ºè¯çš„æ•æ„Ÿç¨‹åº¦ã€‚4) æç¤ºåŸåˆ™ï¼šåŸºäºåˆ†æç»“æœï¼Œæå‡ºé’ˆå¯¹ä¸åŒç±»å‹LMMçš„æç¤ºåŸåˆ™ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LMMå¯¹æç¤ºè¯çš„æ•æ„Ÿæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPromptceptionæ¡†æ¶æ›´åŠ å…¨é¢å’Œç³»ç»Ÿï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸ºæç¤ºè¯è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šPromptceptionæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æç¤ºè¯é›†åˆçš„è®¾è®¡ï¼šæç¤ºè¯é›†åˆçš„è®¾è®¡éœ€è¦è¦†ç›–æç¤ºè¯çš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬æªè¾ã€ç»“æ„ã€ä¸Šä¸‹æ–‡ç­‰ã€‚2) è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼šè¯„ä¼°æŒ‡æ ‡éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨ä¸åŒæç¤ºè¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚3) æ•æ„Ÿæ€§åˆ†ææ–¹æ³•ï¼šæ•æ„Ÿæ€§åˆ†ææ–¹æ³•éœ€è¦èƒ½å¤Ÿé‡åŒ–æ¨¡å‹å¯¹æç¤ºè¯çš„æ•æ„Ÿç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒGemini 1.5 Proï¼‰å¯¹æç¤ºè¯æªè¾çš„æ•æ„Ÿæ€§é«˜äºå¼€æºæ¨¡å‹ï¼Œä½†å¼€æºæ¨¡å‹åœ¨å¤„ç†å¤æ‚æç¤ºè¯æ—¶è¡¨ç°è¾ƒå·®ã€‚æŸäº›æç¤ºè¯çš„å¾®å°å˜åŒ–ä¼šå¯¼è‡´æ¨¡å‹å‡†ç¡®ç‡åå·®é«˜è¾¾15%ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†é’ˆå¯¹ä¸“æœ‰å’Œå¼€æºLMMçš„æç¤ºåŸåˆ™ï¼Œä¸ºæ›´å…¬å¹³å’Œé²æ£’çš„æ¨¡å‹è¯„ä¼°æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è¯„æµ‹åŸºå‡†æ„å»ºã€æ¨¡å‹é²æ£’æ€§æå‡å’Œæç¤ºè¯å·¥ç¨‹ä¼˜åŒ–ã€‚é€šè¿‡Promptceptionæ¡†æ¶ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå‘ç°æ¨¡å‹çš„å¼±ç‚¹ï¼Œå¹¶æ®æ­¤æ”¹è¿›æ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºçš„æç¤ºåŸåˆ™å¯ä»¥æŒ‡å¯¼ç”¨æˆ·è®¾è®¡æ›´æœ‰æ•ˆçš„æç¤ºè¯ï¼Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the success of Large Multimodal Models (LMMs) in recent years, prompt design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly understood. We show that even minor variations in prompt phrasing and structure can lead to accuracy deviations of up to 15% for certain prompts and models. This variability poses a challenge for transparent and fair LMM evaluation, as models often report their best-case performance using carefully selected prompts. To address this, we introduce Promptception, a systematic framework for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types, spanning 15 categories and 6 supercategories, each targeting specific aspects of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks: MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit greater sensitivity to prompt phrasing, reflecting tighter alignment with instruction semantics, while open-source models are steadier but struggle with nuanced and complex phrasing. Based on this analysis, we propose Prompting Principles tailored to proprietary and open-source LMMs, enabling more robust and fair model evaluation.

