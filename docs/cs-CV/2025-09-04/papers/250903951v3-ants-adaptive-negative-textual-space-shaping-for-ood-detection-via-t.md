---
layout: default
title: ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning
---

# ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03951" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03951v3</a>
  <a href="https://arxiv.org/pdf/2509.03951.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03951v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03951v3', 'ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenjie Zhu, Yabin Zhang, Xin Jin, Wenjun Zeng, Lei Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04 (æ›´æ–°: 2025-11-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºANTSï¼šåˆ©ç”¨MLLMç†è§£å’Œæ¨ç†è‡ªé€‚åº”åœ°å¡‘é€ è´Ÿæ–‡æœ¬ç©ºé—´ï¼Œæå‡OODæ£€æµ‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `OODæ£€æµ‹` `è´Ÿæ–‡æœ¬ç©ºé—´` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `MLLM` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰OODæ£€æµ‹æ–¹æ³•éš¾ä»¥å‡†ç¡®æ„å»ºè´Ÿç©ºé—´ï¼Œç¼ºä¹å¯¹OODå›¾åƒçš„ç†è§£ï¼Œé™åˆ¶äº†æ£€æµ‹æ€§èƒ½ã€‚
2. ANTSåˆ©ç”¨MLLMçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè‡ªé€‚åº”åœ°ç”Ÿæˆè¿œOODå’Œè¿‘OODçš„è´Ÿæ–‡æœ¬ç©ºé—´ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒANTSåœ¨ImageNetä¸Šæ˜¾è‘—é™ä½äº†FPR95ï¼Œä¸”å…·æœ‰å…è®­ç»ƒå’Œé›¶æ ·æœ¬çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è´Ÿæ–‡æœ¬ç©ºé—´å¡‘é€ æ–¹æ³•ï¼ˆANTSï¼‰ï¼Œæ—¨åœ¨æå‡OODï¼ˆOut-of-Distributionï¼‰æ£€æµ‹æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•åœ¨æ„å»ºç²¾ç¡®çš„è´Ÿç©ºé—´æ—¶ï¼Œç¼ºä¹å¯¹OODå›¾åƒçš„ç†è§£ï¼Œå¹¶ä¸”ç¼ºä¹ä¸IDï¼ˆIn-Distributionï¼‰æ ‡ç­¾è¯­ä¹‰ç›¸ä¼¼çš„è´Ÿæ ‡ç­¾ï¼Œé™åˆ¶äº†å…¶åœ¨è¿‘OODæ£€æµ‹ä¸­çš„èƒ½åŠ›ã€‚ANTSåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•ç¼“å­˜å†å²æµ‹è¯•å›¾åƒä¸­å¯èƒ½ä¸ºOODçš„æ ·æœ¬ï¼Œå¹¶æç¤ºMLLMæè¿°è¿™äº›å›¾åƒï¼Œç”Ÿæˆè¡¨è¾¾æ€§çš„è´Ÿå¥å­ï¼Œç²¾ç¡®åœ°è¡¨å¾OODåˆ†å¸ƒï¼Œä»è€Œå¢å¼ºè¿œOODæ£€æµ‹ã€‚å¯¹äºè¿‘OODåœºæ™¯ï¼Œè¯¥æ–¹æ³•ç¼“å­˜ä¸å†å²æµ‹è¯•å›¾åƒè§†è§‰ä¸Šç›¸ä¼¼çš„IDç±»åˆ«å­é›†ï¼Œå¹¶åˆ©ç”¨MLLMæ¨ç†ç”Ÿæˆé’ˆå¯¹è¯¥å­é›†çš„è§†è§‰ä¸Šç›¸ä¼¼çš„è´Ÿæ ‡ç­¾ï¼Œæœ‰æ•ˆå‡å°‘å‡é˜´æ€§ï¼Œæé«˜è¿‘OODæ£€æµ‹æ€§èƒ½ã€‚ä¸ºäº†å¹³è¡¡è¿™ä¸¤ç§ç±»å‹çš„è´Ÿæ–‡æœ¬ç©ºé—´ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”åŠ æƒè¯„åˆ†ï¼Œä½¿è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†ä¸åŒçš„OODä»»åŠ¡è®¾ç½®ï¼ˆè¿‘OODå’Œè¿œOODï¼‰ï¼Œä»è€Œåœ¨å¼€æ”¾ç¯å¢ƒä¸­å…·æœ‰é«˜åº¦çš„é€‚åº”æ€§ã€‚åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒANTSæ˜¾è‘—é™ä½äº†3.1%çš„FPR95ï¼Œå»ºç«‹äº†æ–°çš„state-of-the-artã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¯å…è®­ç»ƒå’Œé›¶æ ·æœ¬çš„ï¼Œå…·æœ‰å¾ˆé«˜çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„OODæ£€æµ‹æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºè´Ÿæ ‡ç­¾çš„æ–¹æ³•ï¼Œåœ¨æ„å»ºè´Ÿç©ºé—´æ—¶å­˜åœ¨å±€é™æ€§ã€‚å®ƒä»¬é€šå¸¸ç¼ºä¹å¯¹OODæ•°æ®çš„ç†è§£ï¼Œéš¾ä»¥ç”Ÿæˆå‡†ç¡®æè¿°OODåˆ†å¸ƒçš„è´Ÿæ ·æœ¬ã€‚æ­¤å¤–ï¼Œå¯¹äºä¸IDæ•°æ®ç›¸ä¼¼çš„è¿‘OODæ ·æœ¬ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥ç”Ÿæˆè¯­ä¹‰ç›¸å…³çš„è´Ÿæ ‡ç­¾ï¼Œå¯¼è‡´æ£€æµ‹æ•ˆæœä¸ä½³ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†OODæ£€æµ‹æ–¹æ³•åœ¨å®é™…å¼€æ”¾ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šANTSçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¼ºå¤§ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè‡ªé€‚åº”åœ°æ„å»ºè´Ÿæ–‡æœ¬ç©ºé—´ã€‚é€šè¿‡MLLMå¯¹OODå›¾åƒçš„æè¿°å’Œæ¨ç†ï¼Œç”Ÿæˆæ›´å…·è¡¨è¾¾æ€§å’Œé’ˆå¯¹æ€§çš„è´Ÿæ ·æœ¬ï¼Œä»è€Œæé«˜OODæ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•åŒºåˆ†è¿œOODå’Œè¿‘OODåœºæ™¯ï¼Œåˆ†åˆ«é‡‡ç”¨ä¸åŒçš„ç­–ç•¥ç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œå¹¶è®¾è®¡è‡ªé€‚åº”æƒé‡å¹³è¡¡ä¸¤ç§è´Ÿç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šANTSçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) å†å²æµ‹è¯•å›¾åƒç¼“å­˜ï¼šç¼“å­˜å†å²æµ‹è¯•ä¸­å¯èƒ½ä¸ºOODçš„æ ·æœ¬ä»¥åŠä¸æµ‹è¯•å›¾åƒè§†è§‰ç›¸ä¼¼çš„IDç±»åˆ«å­é›†ã€‚2) MLLMæç¤ºä¸è´Ÿæ ·æœ¬ç”Ÿæˆï¼šé’ˆå¯¹è¿œOODæ ·æœ¬ï¼Œæç¤ºMLLMæè¿°å›¾åƒï¼Œç”Ÿæˆè´Ÿå¥å­ï¼›é’ˆå¯¹è¿‘OODæ ·æœ¬ï¼Œåˆ©ç”¨MLLMæ¨ç†ç”Ÿæˆè§†è§‰ä¸Šç›¸ä¼¼çš„è´Ÿæ ‡ç­¾ã€‚3) è‡ªé€‚åº”åŠ æƒï¼šè®¾è®¡è‡ªé€‚åº”æƒé‡ï¼Œå¹³è¡¡è¿œOODå’Œè¿‘OODè´Ÿæ–‡æœ¬ç©ºé—´ã€‚4) OODæ£€æµ‹ï¼šåˆ©ç”¨æ„å»ºçš„è´Ÿæ–‡æœ¬ç©ºé—´è¿›è¡ŒOODæ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šANTSæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨MLLMçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè‡ªé€‚åº”åœ°ç”Ÿæˆè´Ÿæ–‡æœ¬ç©ºé—´ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒANTSèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æè¿°OODåˆ†å¸ƒï¼Œå¹¶é’ˆå¯¹è¿‘OODæ ·æœ¬ç”Ÿæˆè¯­ä¹‰ç›¸å…³çš„è´Ÿæ ‡ç­¾ã€‚æ­¤å¤–ï¼ŒANTSçš„è‡ªé€‚åº”åŠ æƒæœºåˆ¶èƒ½å¤Ÿå¹³è¡¡ä¸åŒç±»å‹çš„è´Ÿç©ºé—´ï¼Œä½¿å…¶åœ¨ä¸åŒçš„OODä»»åŠ¡è®¾ç½®ä¸­å…·æœ‰æ›´å¥½çš„é€‚åº”æ€§ã€‚è¯¥æ–¹æ³•æ˜¯å…è®­ç»ƒå’Œé›¶æ ·æœ¬çš„ï¼Œå…·æœ‰å¾ˆé«˜çš„å¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šANTSçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) MLLMæç¤ºå·¥ç¨‹ï¼šè®¾è®¡æœ‰æ•ˆçš„æç¤ºè¯­ï¼Œå¼•å¯¼MLLMç”Ÿæˆé«˜è´¨é‡çš„è´Ÿæ ·æœ¬æè¿°å’Œæ¨ç†ç»“æœã€‚2) ç›¸ä¼¼åº¦åº¦é‡ï¼šé€‰æ‹©åˆé€‚çš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼Œç”¨äºç¼“å­˜ä¸æµ‹è¯•å›¾åƒè§†è§‰ç›¸ä¼¼çš„IDç±»åˆ«å­é›†ã€‚3) è‡ªé€‚åº”æƒé‡è®¡ç®—ï¼šè®¾è®¡è‡ªé€‚åº”æƒé‡è®¡ç®—å…¬å¼ï¼Œæ ¹æ®OODä»»åŠ¡çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€è°ƒæ•´è¿œOODå’Œè¿‘OODè´Ÿç©ºé—´çš„æƒé‡ã€‚4) OODæ£€æµ‹å™¨ï¼šé€‰æ‹©åˆé€‚çš„OODæ£€æµ‹å™¨ï¼Œå¹¶åˆ©ç”¨æ„å»ºçš„è´Ÿæ–‡æœ¬ç©ºé—´è¿›è¡Œè®­ç»ƒæˆ–æ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ANTSåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒFPR95é™ä½äº†3.1%ï¼Œè¾¾åˆ°äº†æ–°çš„state-of-the-artã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå…·æœ‰é›¶æ ·æœ¬çš„ç‰¹æ€§ï¼Œä½¿å…¶æ˜“äºéƒ¨ç½²å’Œæ‰©å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒANTSèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è¿œOODå’Œè¿‘OODæ ·æœ¬ï¼Œå¹¶åœ¨ä¸åŒçš„OODä»»åŠ¡è®¾ç½®ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ANTSå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ£€æµ‹æœªçŸ¥çš„äº¤é€šçŠ¶å†µï¼Œåœ¨åŒ»ç–—è¯Šæ–­ä¸­è¯†åˆ«ç½•è§ç–¾ç—…ï¼Œåœ¨é‡‘èé£æ§ä¸­è¯†åˆ«æ¬ºè¯ˆäº¤æ˜“ç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæé«˜AIç³»ç»Ÿåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„OODæ£€æµ‹ç ”ç©¶æä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. Furthermore, the absence of negative labels semantically similar to ID labels constrains their capability in near-OOD detection. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we cache images likely to be OOD samples from the historical test images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we cache the subset of ID classes that are visually similar to historical test images and then leverage MLLM reasoning to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD), making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 3.1\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.

