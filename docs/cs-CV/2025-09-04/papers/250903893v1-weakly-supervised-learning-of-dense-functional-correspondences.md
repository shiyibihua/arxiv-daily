---
layout: default
title: Weakly-Supervised Learning of Dense Functional Correspondences
---

# Weakly-Supervised Learning of Dense Functional Correspondences

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03893" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03893v1</a>
  <a href="https://arxiv.org/pdf/2509.03893.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03893v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03893v1', 'Weakly-Supervised Learning of Dense Functional Correspondences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

**å¤‡æ³¨**: Accepted at ICCV 2025. Project website: https://dense-functional-correspondence.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ å¯†é›†çš„å‡½æ•°å¯¹åº”å…³ç³»ï¼Œæå‡è·¨ç±»åˆ«å›¾åƒåŒ¹é…æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯†é›†å¯¹åº”` `å¼±ç›‘ç£å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å‡½æ•°ç›¸ä¼¼æ€§` `è·¨ç±»åˆ«åŒ¹é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è·¨ç±»åˆ«å›¾åƒåŒ¹é…ä¸­éš¾ä»¥å»ºç«‹å‡†ç¡®çš„å¯†é›†å¯¹åº”å…³ç³»ï¼Œå°¤å…¶ç¼ºä¹å¯¹å¯¹è±¡åŠŸèƒ½ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç»“åˆå¯†é›†å¯¹æ¯”å­¦ä¹ ï¼Œå°†åŠŸèƒ½å’Œç©ºé—´çŸ¥è¯†èå…¥æ¨¡å‹ï¼Œå®ç°å‡½æ•°å¯¹åº”å…³ç³»çš„é¢„æµ‹ã€‚
3. é€šè¿‡åˆæˆå’ŒçœŸå®æ•°æ®é›†çš„å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å»ºç«‹å¯†é›†å‡½æ•°å¯¹åº”å…³ç³»æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å»ºç«‹å›¾åƒå¯¹ä¹‹é—´çš„å¯†é›†å¯¹åº”å…³ç³»å¯¹äºå½¢çŠ¶é‡å»ºå’Œæœºå™¨äººæ“ä½œç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚åœ¨è·¨ä¸åŒç±»åˆ«è¿›è¡ŒåŒ¹é…çš„æŒ‘æˆ˜æ€§ç¯å¢ƒä¸­ï¼Œå¯¹è±¡çš„åŠŸèƒ½ï¼ˆå³å¯¹è±¡å¯ä»¥å¯¹å…¶ä»–å¯¹è±¡äº§ç”Ÿçš„å½±å“ï¼‰å¯ä»¥æŒ‡å¯¼å¦‚ä½•å»ºç«‹å¯¹åº”å…³ç³»ã€‚è¿™æ˜¯å› ä¸ºå®ç°ç‰¹å®šåŠŸèƒ½çš„å¯¹è±¡éƒ¨åˆ†é€šå¸¸åœ¨å½¢çŠ¶å’Œå¤–è§‚ä¸Šå…·æœ‰ç›¸ä¼¼æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ¨å¯¼äº†å¯†é›†å‡½æ•°å¯¹åº”å…³ç³»çš„å®šä¹‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¼±ç›‘ç£å­¦ä¹ èŒƒå¼æ¥è§£å†³é¢„æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬æ–¹æ³•èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¥ä¼ªæ ‡è®°å¤šè§†è§’å›¾åƒä»¥è·å¾—åŠŸèƒ½éƒ¨ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å…¶ä¸åƒç´ å¯¹åº”å…³ç³»çš„å¯†é›†å¯¹æ¯”å­¦ä¹ ç›¸ç»“åˆï¼Œå°†åŠŸèƒ½å’Œç©ºé—´çŸ¥è¯†æç‚¼åˆ°ä¸€ä¸ªæ–°çš„æ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹å¯ä»¥å»ºç«‹å¯†é›†çš„å‡½æ•°å¯¹åº”å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç­–åˆ’äº†åˆæˆå’ŒçœŸå®è¯„ä¼°æ•°æ®é›†ä½œä¸ºä»»åŠ¡åŸºå‡†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç”±ç°æˆçš„è‡ªç›‘ç£å›¾åƒè¡¨ç¤ºå’Œæ¥åœ°çš„è§†è§‰è¯­è¨€æ¨¡å‹ç»„æˆçš„åŸºçº¿è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è·¨ç±»åˆ«å›¾åƒå¯¹ä¹‹é—´å»ºç«‹å¯†é›†å‡½æ•°å¯¹åº”å…³ç³»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºå¤–è§‚æˆ–å½¢çŠ¶ç›¸ä¼¼æ€§çš„æ–¹æ³•ï¼Œåœ¨ç±»åˆ«å·®å¼‚è¾ƒå¤§æ—¶è¡¨ç°ä¸ä½³ï¼Œå¿½ç•¥äº†å¯¹è±¡çš„åŠŸèƒ½ä¿¡æ¯ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¯¹è±¡çš„åŠŸèƒ½ä¿¡æ¯æ¥æŒ‡å¯¼å¯¹åº”å…³ç³»çš„å»ºç«‹æ˜¯å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯¹è±¡çš„åŠŸèƒ½ç›¸ä¼¼æ€§æ¥æŒ‡å¯¼å¯†é›†å¯¹åº”å…³ç³»çš„å»ºç«‹ã€‚å…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„ç‰©ä½“éƒ¨ä»¶é€šå¸¸å…·æœ‰ç›¸ä¼¼çš„å½¢çŠ¶å’Œå¤–è§‚ã€‚é€šè¿‡å­¦ä¹ è¿™ç§åŠŸèƒ½ç›¸ä¼¼æ€§ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°å»ºç«‹è·¨ç±»åˆ«å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è®ºæ–‡åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¥æå–å¯¹è±¡çš„åŠŸèƒ½ä¿¡æ¯ï¼Œå¹¶å°†å…¶èå…¥åˆ°å¯¹åº”å…³ç³»çš„å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹å¤šè§†è§’å›¾åƒè¿›è¡Œä¼ªæ ‡è®°ï¼Œå¾—åˆ°åŠŸèƒ½éƒ¨ä»¶çš„åˆ†å‰²ç»“æœã€‚2) ä½¿ç”¨å¯†é›†å¯¹æ¯”å­¦ä¹ ï¼Œä»åƒç´ å¯¹åº”å…³ç³»ä¸­å­¦ä¹ ç©ºé—´ä¿¡æ¯ã€‚3) å°†åŠŸèƒ½éƒ¨ä»¶çš„åˆ†å‰²ç»“æœå’Œç©ºé—´ä¿¡æ¯èåˆï¼Œè®­ç»ƒä¸€ä¸ªå¯ä»¥é¢„æµ‹å¯†é›†å‡½æ•°å¯¹åº”å…³ç³»çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»¥å›¾åƒå¯¹ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºåƒç´ çº§åˆ«çš„å¯¹åº”å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å¼±ç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»è€Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯è·å¾—åŠŸèƒ½éƒ¨ä»¶çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶å°†å…¶è¿ç§»åˆ°å¯†é›†å¯¹åº”å…³ç³»çš„å­¦ä¹ ä¸­ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†åŸºäºåŠŸèƒ½ç›¸ä¼¼æ€§çš„å¯†é›†å¯¹åº”å…³ç³»å®šä¹‰ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨CLIPç­‰è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚å¯†é›†å¯¹æ¯”å­¦ä¹ é‡‡ç”¨InfoNCEæŸå¤±å‡½æ•°ã€‚æ¨¡å‹ç»“æ„æœªçŸ¥ï¼Œä½†éœ€è¦èƒ½å¤Ÿå¤„ç†å›¾åƒå¯¹å¹¶è¾“å‡ºåƒç´ çº§åˆ«çš„å¯¹åº”å…³ç³»ã€‚æ•°æ®é›†åŒ…æ‹¬åˆæˆå’ŒçœŸå®æ•°æ®ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„è‡ªç›‘ç£å›¾åƒè¡¨ç¤ºå’ŒåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶æ–¹æ³•åœ¨è·¨ç±»åˆ«åŒ¹é…æ–¹é¢çš„ä¼˜åŠ¿ã€‚é€šè¿‡å¯è§†åŒ–ç»“æœï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°å»ºç«‹å…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„éƒ¨ä»¶ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€ä¸‰ç»´é‡å»ºã€å›¾åƒç¼–è¾‘ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œå¯ä»¥åˆ©ç”¨å‡½æ•°å¯¹åº”å…³ç³»å°†ä¸€ä¸ªå¯¹è±¡çš„æ“ä½œçŸ¥è¯†è¿ç§»åˆ°å¦ä¸€ä¸ªå…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„å¯¹è±¡ä¸Šã€‚åœ¨ä¸‰ç»´é‡å»ºä¸­ï¼Œå¯ä»¥åˆ©ç”¨å‡½æ•°å¯¹åº”å…³ç³»å¯¹ä¸åŒè§†è§’çš„å›¾åƒè¿›è¡Œå¯¹é½ã€‚åœ¨å›¾åƒç¼–è¾‘ä¸­ï¼Œå¯ä»¥åˆ©ç”¨å‡½æ•°å¯¹åº”å…³ç³»å°†ä¸€ä¸ªå¯¹è±¡çš„éƒ¨ä»¶æ›¿æ¢ä¸ºå¦ä¸€ä¸ªå¯¹è±¡çš„éƒ¨ä»¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Establishing dense correspondences across image pairs is essential for tasks such as shape reconstruction and robot manipulation. In the challenging setting of matching across different categories, the function of an object, i.e., the effect that an object can cause on other objects, can guide how correspondences should be established. This is because object parts that enable specific functions often share similarities in shape and appearance. We derive the definition of dense functional correspondence based on this observation and propose a weakly-supervised learning paradigm to tackle the prediction task. The main insight behind our approach is that we can leverage vision-language models to pseudo-label multi-view images to obtain functional parts. We then integrate this with dense contrastive learning from pixel correspondences to distill both functional and spatial knowledge into a new model that can establish dense functional correspondence. Further, we curate synthetic and real evaluation datasets as task benchmarks. Our results demonstrate the advantages of our approach over baseline solutions consisting of off-the-shelf self-supervised image representations and grounded vision language models.

