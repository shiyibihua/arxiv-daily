---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-04
---

# cs.CVï¼ˆ2025-09-04ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **9** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ğŸ”—4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250904403v1-self-adaptive-dataset-construction-for-real-world-multimodal-safety-.html">Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios</a></td>
  <td>æå‡ºé¢å‘å›¾åƒçš„è‡ªé€‚åº”æ•°æ®é›†æ„å»ºæ–¹æ³•ï¼Œåº”å¯¹çœŸå®ä¸–ç•Œå¤šæ¨¡æ€å®‰å…¨åœºæ™¯æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04403v1" data-paper-url="./papers/250904403v1-self-adaptive-dataset-construction-for-real-world-multimodal-safety-.html" onclick="toggleFavorite(this, '2509.04403v1', 'Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250904548v1-skywork-unipic-20-building-kontext-model-with-online-rl-for-unified-.html">Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model</a></td>
  <td>UniPic 2.0ï¼šé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ„å»ºKontextæ¨¡å‹ï¼Œå®ç°ç»Ÿä¸€å¤šæ¨¡æ€å›¾åƒç”Ÿæˆä¸ç¼–è¾‘</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04548v1" data-paper-url="./papers/250904548v1-skywork-unipic-20-building-kontext-model-with-online-rl-for-unified-.html" onclick="toggleFavorite(this, '2509.04548v1', 'Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250904448v2-trust-vl-an-explainable-news-assistant-for-general-multimodal-misinf.html">TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection</a></td>
  <td>æå‡ºTRUST-VLï¼Œä¸€ä¸ªå¯è§£é‡Šçš„å¤šæ¨¡æ€æ–°é—»åŠ©æ‰‹ï¼Œç”¨äºæ£€æµ‹é€šç”¨å¤šæ¨¡æ€è™šå‡ä¿¡æ¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04448v2" data-paper-url="./papers/250904448v2-trust-vl-an-explainable-news-assistant-for-general-multimodal-misinf.html" onclick="toggleFavorite(this, '2509.04448v2', 'TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250903999v1-slicesemocc-vertical-slice-based-multimodal-3d-semantic-occupancy-re.html">SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation</a></td>
  <td>æå‡ºSliceSemOccä»¥è§£å†³3Dè¯­ä¹‰å ç”¨é¢„æµ‹ä¸­çš„é«˜åº¦ä¿¡æ¯ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03999v1" data-paper-url="./papers/250903999v1-slicesemocc-vertical-slice-based-multimodal-3d-semantic-occupancy-re.html" onclick="toggleFavorite(this, '2509.03999v1', 'SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250903986v1-promptception-how-sensitive-are-large-multimodal-models-to-prompts.html">Promptception: How Sensitive Are Large Multimodal Models to Prompts?</a></td>
  <td>Promptceptionæ¡†æ¶æ­ç¤ºå¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹æç¤ºè¯çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºä¼˜åŒ–åŸåˆ™ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03986v1" data-paper-url="./papers/250903986v1-promptception-how-sensitive-are-large-multimodal-models-to-prompts.html" onclick="toggleFavorite(this, '2509.03986v1', 'Promptception: How Sensitive Are Large Multimodal Models to Prompts?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250903961v1-multimodal-feature-fusion-network-with-text-difference-enhancement-f.html">Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection</a></td>
  <td>æå‡ºMMChangeï¼Œä¸€ç§èåˆå›¾åƒä¸æ–‡æœ¬å·®å¼‚å¢å¼ºçš„å¤šæ¨¡æ€é¥æ„Ÿå˜åŒ–æ£€æµ‹ç½‘ç»œã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03961v1" data-paper-url="./papers/250903961v1-multimodal-feature-fusion-network-with-text-difference-enhancement-f.html" onclick="toggleFavorite(this, '2509.03961v1', 'Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250903903v1-a-generative-foundation-model-for-chest-radiography.html">A Generative Foundation Model for Chest Radiography</a></td>
  <td>ChexGenï¼šç”¨äºèƒ¸éƒ¨Xå…‰ç‰‡çš„ç”Ÿæˆå¼åŸºç¡€æ¨¡å‹ï¼Œæå‡åŒ»ç–—AIæ€§èƒ½ä¸å…¬å¹³æ€§</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03903v1" data-paper-url="./papers/250903903v1-a-generative-foundation-model-for-chest-radiography.html" onclick="toggleFavorite(this, '2509.03903v1', 'A Generative Foundation Model for Chest Radiography')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250904326v1-efficient-odd-one-out-anomaly-detection.html">Efficient Odd-One-Out Anomaly Detection</a></td>
  <td>æå‡ºä¸€ç§é«˜æ•ˆçš„åŸºäºDINOçš„å¥‡æ•°é¡¹å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å‚æ•°é‡å’Œè®­ç»ƒæ—¶é—´ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04326v1" data-paper-url="./papers/250904326v1-efficient-odd-one-out-anomaly-detection.html" onclick="toggleFavorite(this, '2509.04326v1', 'Efficient Odd-One-Out Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250903951v3-ants-adaptive-negative-textual-space-shaping-for-ood-detection-via-t.html">ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning</a></td>
  <td>æå‡ºANTSï¼šåˆ©ç”¨MLLMç†è§£å’Œæ¨ç†è‡ªé€‚åº”åœ°å¡‘é€ è´Ÿæ–‡æœ¬ç©ºé—´ï¼Œæå‡OODæ£€æµ‹æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03951v3" data-paper-url="./papers/250903951v3-ants-adaptive-negative-textual-space-shaping-for-ood-detection-via-t.html" onclick="toggleFavorite(this, '2509.03951v3', 'ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250904180v1-visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vi.html">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></td>
  <td>æå‡ºVisioFirmä»¥è§£å†³è®¡ç®—æœºè§†è§‰æ ‡æ³¨æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04180v1" data-paper-url="./papers/250904180v1-visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vi.html" onclick="toggleFavorite(this, '2509.04180v1', 'VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250903897v2-specs-specificity-enhanced-clip-score-for-long-image-caption-evaluat.html">SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation</a></td>
  <td>SPECSï¼šç”¨äºé•¿å›¾åƒæè¿°è¯„ä¼°çš„ç‰¹å¼‚æ€§å¢å¼ºCLIP-Score</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03897v2" data-paper-url="./papers/250903897v2-specs-specificity-enhanced-clip-score-for-long-image-caption-evaluat.html" onclick="toggleFavorite(this, '2509.03897v2', 'SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250906996v5-visible-yet-unreadable-a-systematic-blind-spot-of-vision-language-mo.html">Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</a></td>
  <td>æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨ä¹¦å†™ç³»ç»Ÿä¸­çš„ç›²ç‚¹ï¼šå¯¹å¯è§ä½†ä¸å¯è¯»æ–‡æœ¬çš„è„†å¼±æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.06996v5" data-paper-url="./papers/250906996v5-visible-yet-unreadable-a-systematic-blind-spot-of-vision-language-mo.html" onclick="toggleFavorite(this, '2509.06996v5', 'Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250904545v5-promptenhancer-a-simple-approach-to-enhance-text-to-image-models-via.html">PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting</a></td>
  <td>æå‡ºPromptEnhancerï¼Œé€šè¿‡æ€ç»´é“¾æç¤ºé‡å†™å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04545v5" data-paper-url="./papers/250904545v5-promptenhancer-a-simple-approach-to-enhance-text-to-image-models-via.html" onclick="toggleFavorite(this, '2509.04545v5', 'PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250904669v1-vcmamba-bridging-convolutions-with-multi-directional-mamba-for-effic.html">VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</a></td>
  <td>VCMambaï¼šèåˆå·ç§¯ä¸å¤šå‘Mambaï¼Œå®ç°é«˜æ•ˆè§†è§‰è¡¨å¾</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04669v1" data-paper-url="./papers/250904669v1-vcmamba-bridging-convolutions-with-multi-directional-mamba-for-effic.html" onclick="toggleFavorite(this, '2509.04669v1', 'VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250903973v1-sac-mil-spatial-aware-correlated-multiple-instance-learning-for-hist.html">SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification</a></td>
  <td>æå‡ºç©ºé—´æ„ŸçŸ¥ç›¸å…³å¤šç¤ºä¾‹å­¦ä¹ (SAC-MIL)ç”¨äºç—…ç†å…¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">SAC</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03973v1" data-paper-url="./papers/250903973v1-sac-mil-spatial-aware-correlated-multiple-instance-learning-for-hist.html" onclick="toggleFavorite(this, '2509.03973v1', 'SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250903887v1-occtens-3d-occupancy-world-model-via-temporal-next-scale-prediction.html">OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction</a></td>
  <td>OccTENSï¼šé€šè¿‡æ—¶åºä¸‹ä¸€å°ºåº¦é¢„æµ‹å®ç°å¯æ§ã€é«˜æ•ˆçš„3D occupancyä¸–ç•Œæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03887v1" data-paper-url="./papers/250903887v1-occtens-3d-occupancy-world-model-via-temporal-next-scale-prediction.html" onclick="toggleFavorite(this, '2509.03887v1', 'OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250907996v3-3d-and-4d-world-modeling-a-survey.html">3D and 4D World Modeling: A Survey</a></td>
  <td>å¯¹3Då’Œ4Dä¸–ç•Œå»ºæ¨¡ä¸ç”Ÿæˆè¿›è¡Œå…¨é¢ç»¼è¿°ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç³»ç»Ÿæ€§ç ”ç©¶çš„ç©ºç™½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">occupancy grid</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07996v3" data-paper-url="./papers/250907996v3-3d-and-4d-world-modeling-a-survey.html" onclick="toggleFavorite(this, '2509.07996v3', '3D and 4D World Modeling: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250904687v2-guideline-consistent-segmentation-via-multi-agent-refinement.html">Guideline-Consistent Segmentation via Multi-Agent Refinement</a></td>
  <td>æå‡ºä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“è¿­ä»£ä¼˜åŒ–çš„æ— è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œå®ç°æŒ‡å—ä¸€è‡´æ€§åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04687v2" data-paper-url="./papers/250904687v2-guideline-consistent-segmentation-via-multi-agent-refinement.html" onclick="toggleFavorite(this, '2509.04687v2', 'Guideline-Consistent Segmentation via Multi-Agent Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250904406v1-few-step-flow-for-3d-generation-via-marginal-data-transport-distilla.html">Few-step Flow for 3D Generation via Marginal-Data Transport Distillation</a></td>
  <td>æå‡ºMDT-distä»¥è§£å†³3Dç”Ÿæˆæ¨¡å‹çš„é‡‡æ ·æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04406v1" data-paper-url="./papers/250904406v1-few-step-flow-for-3d-generation-via-marginal-data-transport-distilla.html" onclick="toggleFavorite(this, '2509.04406v1', 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250904344v1-micacl-multi-instance-category-aware-contrastive-learning-for-long-t.html">MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition</a></td>
  <td>æå‡ºMICACLæ¡†æ¶ï¼Œè§£å†³é•¿å°¾åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡å’Œæ—¶ç©ºå»ºæ¨¡éš¾é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04344v1" data-paper-url="./papers/250904344v1-micacl-multi-instance-category-aware-contrastive-learning-for-long-t.html" onclick="toggleFavorite(this, '2509.04344v1', 'MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250903872v1-focus-through-motion-rgb-event-collaborative-token-sparsification-fo.html">Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection</a></td>
  <td>æå‡ºFocusMambaï¼Œé€šè¿‡RGB-EventååŒTokenç¨€ç–åŒ–å®ç°é«˜æ•ˆç›®æ ‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03872v1" data-paper-url="./papers/250903872v1-focus-through-motion-rgb-event-collaborative-token-sparsification-fo.html" onclick="toggleFavorite(this, '2509.03872v1', 'Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250904117v1-dvs-pedx-synthetic-and-real-event-based-pedestrian-dataset.html">DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset</a></td>
  <td>DVS-PedXï¼šç”¨äºäº‹ä»¶ç›¸æœºè¡Œäººæ£€æµ‹ä¸æ„å›¾åˆ†æçš„åˆæˆä¸çœŸå®æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04117v1" data-paper-url="./papers/250904117v1-dvs-pedx-synthetic-and-real-event-based-pedestrian-dataset.html" onclick="toggleFavorite(this, '2509.04117v1', 'DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250903883v1-human-motion-video-generation-a-survey.html">Human Motion Video Generation: A Survey</a></td>
  <td>å…¨é¢ç»¼è¿°äººä½“è¿åŠ¨è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œæ¶µç›–å…³é”®é˜¶æ®µã€æ¨¡æ€åŠå¤§è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03883v1" data-paper-url="./papers/250903883v1-human-motion-video-generation-a-survey.html" onclick="toggleFavorite(this, '2509.03883v1', 'Human Motion Video Generation: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250903893v1-weakly-supervised-learning-of-dense-functional-correspondences.html">Weakly-Supervised Learning of Dense Functional Correspondences</a></td>
  <td>æå‡ºä¸€ç§å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ å¯†é›†çš„å‡½æ•°å¯¹åº”å…³ç³»ï¼Œæå‡è·¨ç±»åˆ«å›¾åƒåŒ¹é…æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03893v1" data-paper-url="./papers/250903893v1-weakly-supervised-learning-of-dense-functional-correspondences.html" onclick="toggleFavorite(this, '2509.03893v1', 'Weakly-Supervised Learning of Dense Functional Correspondences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250904379v1-ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer.html">SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</a></td>
  <td>æå‡ºSSGaussianï¼Œé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥å’Œç»“æ„ä¿æŒå®ç°3Dé£æ ¼è¿ç§»</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04379v1" data-paper-url="./papers/250904379v1-ssgaussian-semantic-aware-and-structure-preserving-3d-style-transfer.html" onclick="toggleFavorite(this, '2509.04379v1', 'SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250903922v1-lmvc-an-end-to-end-learned-multiview-video-coding-framework.html">LMVC: An End-to-End Learned Multiview Video Coding Framework</a></td>
  <td>æå‡ºLMVCç«¯åˆ°ç«¯å¤šè§†è§’è§†é¢‘ç¼–ç æ¡†æ¶ï¼Œæå‡å‹ç¼©æ•ˆç‡å¹¶ä¿è¯å…¼å®¹æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03922v1" data-paper-url="./papers/250903922v1-lmvc-an-end-to-end-learned-multiview-video-coding-framework.html" onclick="toggleFavorite(this, '2509.03922v1', 'LMVC: An End-to-End Learned Multiview Video Coding Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/250904378v3-aesthetic-image-captioning-with-saliency-enhanced-mllms.html">Aesthetic Image Captioning with Saliency Enhanced MLLMs</a></td>
  <td>æå‡ºASE-MLLMï¼Œé€šè¿‡æ˜¾è‘—æ€§å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æå‡å›¾åƒç¾å­¦æè¿°ç”Ÿæˆæ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">ASE</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04378v3" data-paper-url="./papers/250904378v3-aesthetic-image-captioning-with-saliency-enhanced-mllms.html" onclick="toggleFavorite(this, '2509.04378v3', 'Aesthetic Image Captioning with Saliency Enhanced MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250903808v1-egtm-event-guided-efficient-turbulence-mitigation.html">EGTM: Event-guided Efficient Turbulence Mitigation</a></td>
  <td>æå‡ºåŸºäºäº‹ä»¶ç›¸æœºçš„EGTMæ¡†æ¶ï¼Œé«˜æ•ˆæ¶ˆé™¤å¤§æ°”æ¹æµå½±å“ï¼Œå®ç°é«˜è´¨é‡å›¾åƒå¤åŸã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03808v1" data-paper-url="./papers/250903808v1-egtm-event-guided-efficient-turbulence-mitigation.html" onclick="toggleFavorite(this, '2509.03808v1', 'EGTM: Event-guided Efficient Turbulence Mitigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250904351v2-global-to-local-or-local-to-global-enhancing-image-retrieval-with-ef.html">Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking</a></td>
  <td>æå‡ºå±€éƒ¨åˆ°å…¨å±€å›¾åƒæ£€ç´¢æ¡†æ¶ï¼Œç»“åˆé«˜æ•ˆå±€éƒ¨æœç´¢ä¸æœ‰æ•ˆå…¨å±€é‡æ’åºï¼Œæ˜¾è‘—æå‡æ£€ç´¢æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04351v2" data-paper-url="./papers/250904351v2-global-to-local-or-local-to-global-enhancing-image-retrieval-with-ef.html" onclick="toggleFavorite(this, '2509.04351v2', 'Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)