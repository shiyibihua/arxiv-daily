---
layout: default
title: Dense Video Understanding with Gated Residual Tokenization
---

# Dense Video Understanding with Gated Residual Tokenization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14199" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14199v2</a>
  <a href="https://arxiv.org/pdf/2509.14199.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14199v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14199v2', 'Dense Video Understanding with Gated Residual Tokenization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17 (æ›´æ–°: 2025-09-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé—¨æ§æ®‹å·®TokenåŒ–ï¼ˆGRTï¼‰æ¡†æ¶ï¼Œå®ç°é«˜æ•ˆé«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `é«˜å¸§ç‡è§†é¢‘` `TokenåŒ–` `è¿åŠ¨è¡¥å¿` `è¯­ä¹‰åˆ†å‰²` `å¤§è¯­è¨€æ¨¡å‹` `æ—¶åºæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£æ¨¡å‹ä¾èµ–ä½å¸§ç‡é‡‡æ ·ï¼Œå¿½ç•¥äº†è§†é¢‘ä¸­å¯†é›†çš„æ—¶åºä¿¡æ¯ï¼Œå¯¼è‡´æ— æ³•å¤„ç†éœ€è¦ç²¾ç»†æ—¶é—´å¯¹é½çš„ä»»åŠ¡ã€‚
2. æå‡ºé—¨æ§æ®‹å·®TokenåŒ–ï¼ˆGRTï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¿åŠ¨è¡¥å¿å’Œè¯­ä¹‰åœºæ™¯èåˆï¼Œå‡å°‘TokenåŒ–æ—¶é—´å’ŒTokenå†—ä½™ï¼Œå®ç°é«˜æ•ˆçš„é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚
3. åœ¨æå‡ºçš„DIVEåŸºå‡†æµ‹è¯•ä¸Šï¼ŒGRTä¼˜äºæ›´å¤§çš„VLLMåŸºçº¿ï¼Œå¹¶è¡¨ç°å‡ºéšå¸§ç‡å¢åŠ è€Œæ€§èƒ½æå‡çš„è¶‹åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜æ—¶é—´åˆ†è¾¨ç‡å¯¹äºæ•æ‰è§†é¢‘ç†è§£ä¸­çš„ç»†ç²’åº¦ç»†èŠ‚è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰å’ŒåŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºä½å¸§ç‡é‡‡æ ·ï¼Œä¾‹å¦‚å‡åŒ€é‡‡æ ·æˆ–å…³é”®å¸§é€‰æ‹©ï¼Œä»è€Œä¸¢å¼ƒäº†å¯†é›†çš„æ—¶åºä¿¡æ¯ã€‚è¿™ç§æŠ˜è¡·æ–¹æ¡ˆé¿å…äº†å¯¹æ¯ä¸€å¸§è¿›è¡ŒTokenåŒ–çš„é«˜æˆæœ¬ï¼Œå¦åˆ™ä¼šå¯¼è‡´å†—ä½™è®¡ç®—å’Œéšç€è§†é¢‘é•¿åº¦å¢åŠ çš„çº¿æ€§Tokenå¢é•¿ã€‚è™½ç„¶è¿™ç§æƒè¡¡é€‚ç”¨äºå˜åŒ–ç¼“æ…¢çš„å†…å®¹ï¼Œä½†å®ƒä¸é€‚ç”¨äºè¯¸å¦‚è®²åº§ç†è§£ä¹‹ç±»çš„ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¿¡æ¯å‡ ä¹å‡ºç°åœ¨æ¯ä¸€å¸§ä¸­ï¼Œå¹¶ä¸”éœ€è¦ç²¾ç¡®çš„æ—¶é—´å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯†é›†è§†é¢‘ç†è§£ï¼ˆDVUï¼‰ï¼Œå®ƒé€šè¿‡å‡å°‘TokenåŒ–æ—¶é—´å’ŒTokenå¼€é”€æ¥å®ç°é«˜FPSè§†é¢‘ç†è§£ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¹Ÿå—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬çš„QAå¯¹ä¾§é‡äºç²—ç•¥çš„å†…å®¹æ›´æ”¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DIVEï¼ˆå¯†é›†ä¿¡æ¯è§†é¢‘è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ºå¯†é›†æ—¶åºæ¨ç†è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†ä½¿DVUå®ç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†é—¨æ§æ®‹å·®TokenåŒ–ï¼ˆGRTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼šï¼ˆ1ï¼‰è¿åŠ¨è¡¥å¿çš„å¸§é—´é—¨æ§TokenåŒ–ä½¿ç”¨åƒç´ çº§è¿åŠ¨ä¼°è®¡æ¥è·³è¿‡TokenåŒ–æœŸé—´çš„é™æ€åŒºåŸŸï¼Œä»è€Œå®ç°Tokenæ•°é‡å’Œè®¡ç®—çš„äºšçº¿æ€§å¢é•¿ã€‚ï¼ˆ2ï¼‰è¯­ä¹‰åœºæ™¯çš„å¸§å†…TokenåŒ–åˆå¹¶èåˆåœºæ™¯å†…é™æ€åŒºåŸŸçš„Tokenï¼Œè¿›ä¸€æ­¥å‡å°‘å†—ä½™ï¼ŒåŒæ—¶ä¿ç•™åŠ¨æ€è¯­ä¹‰ã€‚åœ¨DIVEä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRTä¼˜äºæ›´å¤§çš„VLLMåŸºçº¿ï¼Œå¹¶éšç€FPSçš„å¢åŠ è€Œç§¯ææ‰©å±•ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¯†é›†æ—¶åºä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜GRTèƒ½å¤Ÿå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„é«˜FPSè§†é¢‘ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•ä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œé€šå¸¸é‡‡ç”¨ä½å¸§ç‡é‡‡æ ·ï¼Œå¿½ç•¥äº†è§†é¢‘ä¸­ä¸°å¯Œçš„æ—¶åºä¿¡æ¯ã€‚è¿™å¯¼è‡´æ¨¡å‹éš¾ä»¥æ•æ‰è§†é¢‘ä¸­çš„ç»†å¾®å˜åŒ–ï¼Œæ— æ³•èƒœä»»éœ€è¦é«˜æ—¶é—´åˆ†è¾¨ç‡çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è®²åº§ç†è§£ç­‰ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œä¿¡æ¯å®Œæ•´æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‡å°‘TokenåŒ–è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—å’ŒTokenæ•°é‡ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ©ç”¨è§†é¢‘å¸§ä¹‹é—´çš„è¿åŠ¨ä¿¡æ¯å’Œåœºæ™¯è¯­ä¹‰ä¿¡æ¯ï¼Œè·³è¿‡é™æ€åŒºåŸŸçš„TokenåŒ–ï¼Œå¹¶å°†ç›¸ä¼¼çš„Tokenè¿›è¡Œåˆå¹¶ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦å’ŒTokenæ•°é‡ã€‚è¿™æ ·æ—¢ä¿ç•™äº†è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œåˆé¿å…äº†å¯¹æ¯ä¸€å¸§éƒ½è¿›è¡ŒTokenåŒ–å¸¦æ¥çš„å·¨å¤§å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRTæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š
1. **è¿åŠ¨è¡¥å¿çš„å¸§é—´é—¨æ§TokenåŒ– (Motion-Compensated Inter-Gated Tokenization)**ï¼šåˆ©ç”¨åƒç´ çº§è¿åŠ¨ä¼°è®¡ï¼Œè¯†åˆ«å¹¶è·³è¿‡è§†é¢‘å¸§ä¹‹é—´çš„é™æ€åŒºåŸŸï¼Œåªå¯¹è¿åŠ¨åŒºåŸŸè¿›è¡ŒTokenåŒ–ï¼Œä»è€Œå‡å°‘Tokenæ•°é‡ã€‚
2. **è¯­ä¹‰åœºæ™¯çš„å¸§å†…TokenåŒ–åˆå¹¶ (Semantic-Scene Intra-Tokenization Merging)**ï¼šåœ¨åŒä¸€åœºæ™¯å†…ï¼Œå°†é™æ€åŒºåŸŸçš„Tokenè¿›è¡Œåˆå¹¶ï¼Œè¿›ä¸€æ­¥å‡å°‘Tokenå†—ä½™ï¼ŒåŒæ—¶ä¿ç•™åŠ¨æ€è¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é—¨æ§æ®‹å·®TokenåŒ–æœºåˆ¶ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è§†é¢‘å†…å®¹çš„åŠ¨æ€ç¨‹åº¦è‡ªé€‚åº”åœ°è°ƒæ•´TokenåŒ–è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·æˆ–å…³é”®å¸§é€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼ŒGRTèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™ç§æ–¹æ³•å®ç°äº†Tokenæ•°é‡å’Œè®¡ç®—é‡çš„äºšçº¿æ€§å¢é•¿ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘åºåˆ—ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **è¿åŠ¨ä¼°è®¡**ï¼šé‡‡ç”¨åƒç´ çº§è¿åŠ¨ä¼°è®¡æ–¹æ³•ï¼Œä¾‹å¦‚å…‰æµæ³•ï¼Œæ¥è¯†åˆ«è§†é¢‘å¸§ä¹‹é—´çš„è¿åŠ¨åŒºåŸŸã€‚
* **é—¨æ§æœºåˆ¶**ï¼šä½¿ç”¨é—¨æ§æœºåˆ¶æ¥æ§åˆ¶TokenåŒ–çš„è¿‡ç¨‹ï¼Œåªæœ‰å½“åƒç´ çš„è¿åŠ¨å¹…åº¦è¶…è¿‡ä¸€å®šé˜ˆå€¼æ—¶ï¼Œæ‰å¯¹å…¶è¿›è¡ŒTokenåŒ–ã€‚
* **è¯­ä¹‰åœºæ™¯åˆ†å‰²**ï¼šä½¿ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹å°†è§†é¢‘å¸§åˆ†å‰²æˆä¸åŒçš„åœºæ™¯åŒºåŸŸã€‚
* **Tokenåˆå¹¶**ï¼šåœ¨åŒä¸€åœºæ™¯å†…ï¼Œä½¿ç”¨èšç±»ç®—æ³•å°†ç›¸ä¼¼çš„Tokenè¿›è¡Œåˆå¹¶ï¼Œä¾‹å¦‚K-meansèšç±»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRTåœ¨DIVEåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæ›´å¤§çš„VLLMåŸºçº¿ã€‚å…·ä½“æ¥è¯´ï¼ŒGRTåœ¨QAä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†æ˜¾è‘—å¹…åº¦ï¼Œå¹¶ä¸”éšç€è¾“å…¥è§†é¢‘å¸§ç‡çš„å¢åŠ ï¼ŒGRTçš„æ€§èƒ½ä¹Ÿéšä¹‹æå‡ã€‚è¿™è¡¨æ˜GRTèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨é«˜å¸§ç‡è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯ï¼Œå®ç°æ›´å‡†ç¡®çš„è§†é¢‘ç†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§è§†é¢‘ç†è§£åœºæ™¯ï¼Œä¾‹å¦‚åœ¨çº¿æ•™è‚²ã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚åœ¨åœ¨çº¿æ•™è‚²ä¸­ï¼Œå¯ä»¥ç”¨äºç†è§£è®²åº§è§†é¢‘ï¼Œè‡ªåŠ¨ç”Ÿæˆç¬”è®°æˆ–æ‘˜è¦ã€‚åœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥ç”¨äºæ£€æµ‹å¼‚å¸¸è¡Œä¸ºæˆ–äº‹ä»¶ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºç†è§£äº¤é€šåœºæ™¯ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨è§†é¢‘ç†è§£æŠ€æœ¯çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºäººä»¬çš„ç”Ÿæ´»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> High temporal resolution is essential for capturing fine-grained details in video understanding. However, current video large language models (VLLMs) and benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or keyframe selection, discarding dense temporal information. This compromise avoids the high cost of tokenizing every frame, which otherwise leads to redundant computation and linear token growth as video length increases. While this trade-off works for slowly changing content, it fails for tasks like lecture comprehension, where information appears in nearly every frame and requires precise temporal alignment. To address this gap, we introduce Dense Video Understanding (DVU), which enables high-FPS video comprehension by reducing both tokenization time and token overhead. Existing benchmarks are also limited, as their QA pairs focus on coarse content changes. We therefore propose DIVE (Dense Information Video Evaluation), the first benchmark designed for dense temporal reasoning. To make DVU practical, we present Gated Residual Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated Tokenization uses pixel-level motion estimation to skip static regions during tokenization, achieving sub-linear growth in token count and compute. (2) Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions within a scene, further reducing redundancy while preserving dynamic semantics. Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales positively with FPS. These results highlight the importance of dense temporal information and demonstrate that GRT enables efficient, scalable high-FPS video understanding.

