---
layout: default
title: "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection"
---

# From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13233" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13233v1</a>
  <a href="https://arxiv.org/pdf/2505.13233.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13233v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13233v1', 'From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/BIT-DA/ABS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ³¨æ„åŠ›å¼•å¯¼é€‰æ‹©æ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `ç‰¹å¾é€‰æ‹©` `é›¶-shotå­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦ç±»æè¿°æ—¶ï¼Œéšæœºå¢å¼ºæ–¹æ³•å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡åº¦å…³æ³¨å±€éƒ¨ç»†èŠ‚ï¼Œå½±å“æ•´ä½“è¯­ä¹‰ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºçš„ABSæ–¹æ³•é€šè¿‡æ³¨æ„åŠ›å¼•å¯¼è£å‰ªï¼Œç»“åˆç‰¹å¾é€‰æ‹©ï¼Œæ—¨åœ¨ä»å±€éƒ¨ç»†èŠ‚ä¸­æå–å…¨çƒä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒABSåœ¨é›¶-shotåˆ†ç±»å’Œåˆ†å¸ƒå¤–æ³›åŒ–ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶-shotèƒ½åŠ›ã€‚ä»¥å¾€ç ”ç©¶å¼ºè°ƒè§†è§‰å¢å¼ºæŠ€æœ¯ï¼ˆå¦‚éšæœºè£å‰ªï¼‰åœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„ç»†ç²’åº¦ç±»æè¿°å¯¹é½ä¸­çš„é‡è¦æ€§ï¼Œæ˜¾è‘—æå‡äº†é›¶-shotæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›å¢å¼ºçš„éšæœºæ€§å¯èƒ½å¼•å…¥èƒŒæ™¯ä¼ªå½±ï¼Œå¹¶å¯¼è‡´æ¨¡å‹è¿‡äºå…³æ³¨å±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œå¦¨ç¢å…¨çƒè¯­ä¹‰ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„é€‰æ‹©ï¼ˆABSï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨åŸå§‹å›¾åƒå’Œç‰¹å¾ç©ºé—´ä¸­åº”ç”¨æ³¨æ„åŠ›å¼•å¯¼è£å‰ªï¼Œè¡¥å……å…¨çƒè¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½¯åŒ¹é…æŠ€æœ¯ï¼Œä»¥æœ‰æ•ˆè¿‡æ»¤LLMæè¿°ä»¥å®ç°æ›´å¥½çš„å¯¹é½ã€‚ABSåœ¨åˆ†å¸ƒå¤–æ³›åŒ–å’Œé›¶-shotåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”æ— éœ€è®­ç»ƒï¼Œç”šè‡³ä¸å°‘é‡æ ·æœ¬å’Œæµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ç›¸åª²ç¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦ç±»æè¿°æ—¶ï¼Œç”±äºéšæœºå¢å¼ºæ–¹æ³•å¼•å…¥çš„èƒŒæ™¯ä¼ªå½±å’Œå±€éƒ¨ç»†èŠ‚è¿‡åº¦å…³æ³¨çš„é—®é¢˜ï¼Œå¯¼è‡´å…¨çƒè¯­ä¹‰ç†è§£å—æŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„ABSæ–¹æ³•é€šè¿‡æ³¨æ„åŠ›å¼•å¯¼è£å‰ªï¼Œæ—¨åœ¨ä»å±€éƒ¨ç»†èŠ‚ä¸­æå–å…¨çƒä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶ç»“åˆç‰¹å¾é€‰æ‹©æ¥å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šABSæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ³¨æ„åŠ›å¼•å¯¼è£å‰ªæ¨¡å—å’Œç‰¹å¾é€‰æ‹©æ¨¡å—ã€‚å‰è€…åœ¨åŸå§‹å›¾åƒå’Œç‰¹å¾ç©ºé—´ä¸­åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œåè€…åˆ™é€šè¿‡è½¯åŒ¹é…æŠ€æœ¯ä¼˜åŒ–ä¸LLMæè¿°çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šABSæ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è®­ç»ƒè‡ªç”±æ€§å’Œé«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸å°‘é‡æ ·æœ¬å’Œæµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒABSæ–¹æ³•é‡‡ç”¨äº†ç‰¹å®šçš„æ³¨æ„åŠ›æœºåˆ¶å’Œè½¯åŒ¹é…ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç‰¹å¾é€‰æ‹©çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒABSæ–¹æ³•åœ¨é›¶-shotåˆ†ç±»å’Œåˆ†å¸ƒå¤–æ³›åŒ–ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®éœ€æŸ¥é˜…åŸæ–‡ï¼‰ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œå±•ç°å‡ºæé«˜çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼ŒABSæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½æœç´¢ã€è‡ªåŠ¨æ ‡æ³¨ã€å†…å®¹ç”Ÿæˆç­‰å®é™…åœºæ™¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive zero-shot capabilities on downstream tasks. Prior research highlights the crucial role of visual augmentation techniques, like random cropping, in alignment with fine-grained class descriptions generated by large language models (LLMs), significantly enhancing zero-shot performance by incorporating multi-view information. However, the inherent randomness of these augmentations can inevitably introduce background artifacts and cause models to overly focus on local details, compromising global semantic understanding. To address these issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election (\textbf{ABS}) method from local details to global context, which applies attention-guided cropping in both raw images and feature space, supplement global semantic information through strategic feature selection. Additionally, we introduce a soft matching technique to effectively filter LLM descriptions for better alignment. \textbf{ABS} achieves state-of-the-art performance on out-of-distribution generalization and zero-shot classification tasks. Notably, \textbf{ABS} is training-free and even rivals few-shot and test-time adaptation methods. Our code is available at \href{https://github.com/BIT-DA/ABS}{\textcolor{darkgreen}{https://github.com/BIT-DA/ABS} }.

