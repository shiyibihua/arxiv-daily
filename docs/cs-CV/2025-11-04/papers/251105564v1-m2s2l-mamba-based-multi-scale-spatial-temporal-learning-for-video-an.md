---
layout: default
title: M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection
---

# M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.05564" target="_blank" class="toolbar-btn">arXiv: 2511.05564v1</a>
    <a href="https://arxiv.org/pdf/2511.05564.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05564v1" 
            onclick="toggleFavorite(this, '2511.05564v1', 'M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yang Liu, Boan Chen, Xiaoguang Zhu, Jing Liu, Peng Sun, Wei Zhou

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-04

**Â§áÊ≥®**: IEEE VCIP 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éMambaÁöÑÂ§öÂ∞∫Â∫¶Êó∂Á©∫Â≠¶‰π†Ê°ÜÊû∂M2S2LÔºåÁî®‰∫éÊèêÂçáËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÁöÑÁ≤æÂ∫¶ÂíåÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµã` `Mamba` `Â§öÂ∞∫Â∫¶Â≠¶‰π†` `Êó∂Á©∫Âª∫Ê®°` `ÁâπÂæÅÂàÜËß£` `ËßÜÈ¢ëÁõëÊéß` `Â∫èÂàóÂª∫Ê®°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÈöæ‰ª•ÂÖºÈ°æÂ§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊ£ÄÊµãÁ≤æÂ∫¶ÂíåÂÆûÊó∂ÊÄßÈúÄÊ±ÇÔºåÁº∫‰πèÊúâÊïàÁöÑÊó∂Á©∫Âª∫Ê®°ËÉΩÂäõ„ÄÇ
2. M2S2LÊ°ÜÊû∂Âà©Áî®MambaÊû∂ÊûÑÔºåÈÄöËøáÂ§öÂ∞∫Â∫¶Á©∫Èó¥ÁºñÁ†ÅÂíåÂ§öÊó∂Èó¥Â∞∫Â∫¶ËøêÂä®Âª∫Ê®°ÔºåÂÆûÁé∞Êõ¥ÂÖ®Èù¢ÁöÑÊó∂Á©∫ÁâπÂæÅÊèêÂèñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåM2S2LÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÂºÇÂ∏∏Ê£ÄÊµãÊÄßËÉΩÔºåÂπ∂‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµã(VAD)ÊòØÂõæÂÉèÂ§ÑÁêÜÈ¢ÜÂüüÁöÑ‰∏ÄÈ°πÈáçË¶Å‰ªªÂä°ÔºåÂú®ËßÜÈ¢ëÁõëÊéßÊñπÈù¢ÂÖ∑ÊúâÂπøÈòîÂâçÊôØÔºå‰ΩÜÂÖ∂Âú®Ê£ÄÊµãÁ≤æÂ∫¶ÂíåËÆ°ÁÆóÊïàÁéá‰πãÈó¥ÂèñÂæóÂπ≥Ë°°Èù¢‰∏¥ÁùÄÊ†πÊú¨ÊÄßÊåëÊàò„ÄÇÈöèÁùÄËßÜÈ¢ëÂÜÖÂÆπÊó•ÁõäÂ§çÊùÇÔºåË°å‰∏∫Ê®°ÂºèÂíå‰∏ä‰∏ãÊñáÂú∫ÊôØÂ§öÊ†∑ÂåñÔºå‰º†ÁªüÁöÑVADÊñπÊ≥ïÈöæ‰ª•ÂØπÁé∞‰ª£ÁõëÊéßÁ≥ªÁªüÊèê‰æõÁ®≥ÂÅ•ÁöÑËØÑ‰º∞„ÄÇÁé∞ÊúâÊñπÊ≥ïË¶Å‰πàÁº∫‰πèÂÖ®Èù¢ÁöÑÊó∂Á©∫Âª∫Ê®°ÔºåË¶Å‰πàÈúÄË¶ÅËøáÂ§öÁöÑËÆ°ÁÆóËµÑÊ∫êÊâçËÉΩÂÆûÁé∞ÂÆûÊó∂Â∫îÁî®„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éMambaÁöÑÂ§öÂ∞∫Â∫¶Êó∂Á©∫Â≠¶‰π†(M2S2L)Ê°ÜÊû∂„ÄÇËØ•ÊñπÊ≥ïÈááÁî®Âú®Â§ö‰∏™Á≤íÂ∫¶‰∏äËøêË°åÁöÑÂàÜÂ±ÇÁ©∫Èó¥ÁºñÁ†ÅÂô®ÂíåË∑®‰∏çÂêåÊó∂Èó¥Â∞∫Â∫¶ÊçïËé∑ËøêÂä®Âä®ÊÄÅÁöÑÂ§öÊó∂Èó¥ÁºñÁ†ÅÂô®„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁâπÂæÅÂàÜËß£Êú∫Âà∂Ôºå‰ª•ÂÆûÁé∞ÈíàÂØπÂ§ñËßÇÂíåËøêÂä®ÈáçÂª∫ÁöÑ‰ªªÂä°ÁâπÂÆö‰ºòÂåñÔºå‰ªéËÄå‰øÉËøõÊõ¥ÁªÜËá¥ÁöÑË°å‰∏∫Âª∫Ê®°ÂíåË¥®ÈáèÊÑüÁü•ÁöÑÂºÇÂ∏∏ËØÑ‰º∞„ÄÇÂú®‰∏â‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåM2S2LÊ°ÜÊû∂Âú®UCSD Ped2„ÄÅCUHK AvenueÂíåShanghaiTech‰∏äÂàÜÂà´ÂÆûÁé∞‰∫Ü98.5%„ÄÅ92.1%Âíå77.9%ÁöÑÂ∏ßÁ∫ßÂà´AUCÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü20.1G FLOPsÁöÑÊïàÁéáÂíå45 FPSÁöÑÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂÆûÈôÖÁöÑÁõëÊéßÈÉ®ÁΩ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÊó®Âú®ËØÜÂà´ËßÜÈ¢ëÂ∫èÂàó‰∏≠‰∏éÊ≠£Â∏∏Ê®°ÂºèÊòæËëó‰∏çÂêåÁöÑ‰∫ã‰ª∂„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãËøõË°åÁ≤æÁ°ÆÁöÑÊó∂Á©∫Âª∫Ê®°ÔºåÂØºËá¥Ê£ÄÊµãÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÂêåÊó∂Ôºå‰∏Ä‰∫õÊñπÊ≥ïËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈöæ‰ª•Êª°Ë∂≥ÂÆûÊó∂ÊÄßË¶ÅÊ±ÇÔºåÈôêÂà∂‰∫ÜÂÆûÈôÖÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöM2S2LÁöÑÊ†∏ÂøÉÂú®‰∫éÂà©Áî®MambaÊû∂ÊûÑÂº∫Â§ßÁöÑÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÔºåÂêåÊó∂ÁªìÂêàÂ§öÂ∞∫Â∫¶Á©∫Èó¥‰ø°ÊÅØÂíåÂ§öÊó∂Èó¥Â∞∫Â∫¶ËøêÂä®‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥ÂÖ®Èù¢Âú∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÂºÇÂ∏∏Ë°å‰∏∫„ÄÇÈÄöËøáÁâπÂæÅÂàÜËß£Êú∫Âà∂ÔºåÈíàÂØπÂ§ñËßÇÂíåËøêÂä®ËøõË°å‰ªªÂä°ÁâπÂÆöÁöÑ‰ºòÂåñÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöM2S2LÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Â§öÂ∞∫Â∫¶Á©∫Èó¥ÁºñÁ†ÅÂô®ÔºöÈááÁî®ÂàÜÂ±ÇÁªìÊûÑÔºåÂú®‰∏çÂêåÁ≤íÂ∫¶‰∏äÊèêÂèñÁ©∫Èó¥ÁâπÂæÅ„ÄÇ2) Â§öÊó∂Èó¥Â∞∫Â∫¶ËøêÂä®ÁºñÁ†ÅÂô®ÔºöÊçïÊçâ‰∏çÂêåÊó∂Èó¥Ë∑®Â∫¶ÁöÑËøêÂä®Âä®ÊÄÅ„ÄÇ3) ÁâπÂæÅÂàÜËß£Ê®°ÂùóÔºöÂ∞ÜÁâπÂæÅÂàÜËß£‰∏∫Â§ñËßÇÂíåËøêÂä®ÂàÜÈáèÔºåÂàÜÂà´ËøõË°åÈáçÂª∫„ÄÇ4) ÂºÇÂ∏∏ËØÑÂàÜÊ®°ÂùóÔºöÂü∫‰∫éÈáçÂª∫ËØØÂ∑ÆËØÑ‰º∞ÂºÇÂ∏∏Á®ãÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöM2S2LÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜMambaÊû∂ÊûÑÂºïÂÖ•ËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÈ¢ÜÂüüÔºåÂπ∂ÁªìÂêàÂ§öÂ∞∫Â∫¶Êó∂Á©∫Âª∫Ê®°ÂíåÁâπÂæÅÂàÜËß£Êú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éCNNÊàñRNNÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåMambaÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÂíåÊõ¥È´òÁöÑËÆ°ÁÆóÊïàÁéáÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÈïøÁ®ã‰æùËµñÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ§öÂ∞∫Â∫¶Á©∫Èó¥ÁºñÁ†ÅÂô®ÈááÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂÆûÁé∞Ôºå‰∏çÂêåÂ±ÇÁ∫ßÁöÑÂç∑ÁßØÊ†∏Â§ßÂ∞èÂíåÊ≠•Èïø‰∏çÂêåÔºå‰ª•ÊèêÂèñ‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁ©∫Èó¥ÁâπÂæÅ„ÄÇÂ§öÊó∂Èó¥Â∞∫Â∫¶ËøêÂä®ÁºñÁ†ÅÂô®ÈááÁî®MambaÊû∂ÊûÑÂÆûÁé∞ÔºåÈÄöËøáË∞ÉÊï¥Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÂèÇÊï∞ÔºåÊçïÊçâ‰∏çÂêåÊó∂Èó¥Ë∑®Â∫¶ÁöÑËøêÂä®‰ø°ÊÅØ„ÄÇÁâπÂæÅÂàÜËß£Ê®°ÂùóÈááÁî®Á∫øÊÄßÂèòÊç¢ÂÆûÁé∞ÔºåÂ∞ÜÁâπÂæÅÂàÜËß£‰∏∫Â§ñËßÇÂíåËøêÂä®ÂàÜÈáè„ÄÇÂºÇÂ∏∏ËØÑÂàÜÊ®°ÂùóÈááÁî®ÈáçÂª∫ËØØÂ∑Æ‰Ωú‰∏∫ÂºÇÂ∏∏ÊåáÊ†áÔºåÈÄöËøáËÆæÂÆöÈòàÂÄºÂà§Êñ≠ÊòØÂê¶‰∏∫ÂºÇÂ∏∏‰∫ã‰ª∂„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

M2S2LÊ°ÜÊû∂Âú®‰∏â‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®UCSD Ped2Êï∞ÊçÆÈõÜ‰∏äÔºåÂ∏ßÁ∫ßÂà´AUCËææÂà∞‰∫Ü98.5%ÔºõÂú®CUHK AvenueÊï∞ÊçÆÈõÜ‰∏äÔºåËææÂà∞‰∫Ü92.1%ÔºõÂú®ShanghaiTechÊï∞ÊçÆÈõÜ‰∏äÔºåËææÂà∞‰∫Ü77.9%„ÄÇÂêåÊó∂ÔºåËØ•ÊñπÊ≥ï‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºåËææÂà∞‰∫Ü45 FPSÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰∏∫20.1G FLOPsÔºå‰ºò‰∫éËÆ∏Â§öÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊô∫ËÉΩËßÜÈ¢ëÁõëÊéßÈ¢ÜÂüüÔºå‰æãÂ¶ÇÂÖ¨ÂÖ±ÂÆâÂÖ®„ÄÅ‰∫§ÈÄöÁÆ°ÁêÜ„ÄÅÂ∑•‰∏öÁîü‰∫ßÁ≠â„ÄÇÈÄöËøáÂÆûÊó∂Ê£ÄÊµãÂºÇÂ∏∏‰∫ã‰ª∂ÔºåÂèØ‰ª•ÂèäÊó∂È¢ÑË≠¶ÂíåÈááÂèñÊé™ÊñΩÔºåÊúâÊïàÈôç‰ΩéÂÆâÂÖ®È£éÈô©ÂíåÊçüÂ§±„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñËßÜÈ¢ëÂàÜÊûê‰ªªÂä°ÔºåÂ¶ÇË°å‰∏∫ËØÜÂà´„ÄÅ‰∫ã‰ª∂Ê£ÄÊµãÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Video anomaly detection (VAD) is an essential task in the image processing community with prospects in video surveillance, which faces fundamental challenges in balancing detection accuracy with computational efficiency. As video content becomes increasingly complex with diverse behavioral patterns and contextual scenarios, traditional VAD approaches struggle to provide robust assessment for modern surveillance systems. Existing methods either lack comprehensive spatial-temporal modeling or require excessive computational resources for real-time applications. In this regard, we present a Mamba-based multi-scale spatial-temporal learning (M2S2L) framework in this paper. The proposed method employs hierarchical spatial encoders operating at multiple granularities and multi-temporal encoders capturing motion dynamics across different time scales. We also introduce a feature decomposition mechanism to enable task-specific optimization for appearance and motion reconstruction, facilitating more nuanced behavioral modeling and quality-aware anomaly assessment. Experiments on three benchmark datasets demonstrate that M2S2L framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G FLOPs and 45 FPS inference speed, making it suitable for practical surveillance deployment.

