---
layout: default
title: Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models
---

# Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models

**arXiv**: [2511.02650v1](https://arxiv.org/abs/2511.02650) | [PDF](https://arxiv.org/pdf/2511.02650.pdf)

**ä½œè€…**: Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniPruneBenchåŸºå‡†ä»¥è¯„ä¼°å¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹ä¸­çš„è§†è§‰ä»¤ç‰ŒåŽ‹ç¼©æ–¹æ³•**

**å…³é”®è¯**: `è§†è§‰ä»¤ç‰ŒåŽ‹ç¼©` `å¤šæ¨¡æ€æ¨¡åž‹åŸºå‡†` `å‰ªæžç®—æ³•` `æŽ¨ç†æ•ˆçŽ‡` `OCRä»»åŠ¡æ•æ„Ÿæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹å› è§†è§‰ä»¤ç‰Œè¿‡å¤šå¯¼è‡´æŽ¨ç†æ•ˆçŽ‡ä½Žä¸‹ï¼ŒçŽ°æœ‰åŽ‹ç¼©æ–¹æ³•è¯„ä¼°ä¸ä¸€è‡´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºç»Ÿä¸€åŸºå‡†ï¼Œæ¶µç›–å…­èƒ½åŠ›ç»´åº¦ã€åæ•°æ®é›†ã€ååŽ‹ç¼©ç®—æ³•å’Œä¸‰æ¨¡åž‹å®¶æ—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°éšæœºå‰ªæžä¸ºå¼ºåŸºçº¿ï¼Œæ€§èƒ½å—å‰ªæžæ¯”ä¸»å¯¼ï¼Œä»»åŠ¡æ•æ„Ÿæ€§å·®å¼‚å¤§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large multimodal models (LMMs) often suffer from severe inference
> inefficiency due to the large number of visual tokens introduced by image
> encoders. While recent token compression methods, such as pruning and merging,
> have shown promise in reducing redundancy, their evaluation remains fragmented
> and inconsistent. In this work, we present UniPruneBench, a unified and
> extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench
> provides standardized protocols across six ability dimensions and ten datasets,
> covering ten representative compression algorithms and three families of LMMs
> (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates
> system-level metrics such as runtime and prefilling latency to provide a
> holistic view. Our experiments uncover several key findings: (1) random pruning
> is a surprisingly strong baseline, (2) no single method consistently
> outperforms others across scenarios, (3) pruning sensitivity varies
> significantly across tasks, with OCR being most vulnerable, and (4) pruning
> ratio is the dominant factor governing performance degradation. We believe
> UniPruneBench will serve as a reliable foundation for future research on
> efficient multimodal modeling.

