---
layout: default
title: Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds
---

# Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds

**arXiv**: [2511.02395v1](https://arxiv.org/abs/2511.02395) | [PDF](https://arxiv.org/pdf/2511.02395.pdf)

**ä½œè€…**: Leon Schwarzer, Matthias Zeller, Daniel Casado Herraez, Simon Dierl, Michael Heidingsfeld, Cyrill Stachniss

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸Žèšç±»æŸå¤±æ–¹æ³•ï¼Œä»¥è§£å†³ç¨€ç–å™ªå£°é›·è¾¾ç‚¹äº‘çš„è¿åŠ¨ç›®æ ‡åˆ†å‰²é—®é¢˜ã€‚**

**å…³é”®è¯**: `è¿åŠ¨ç›®æ ‡åˆ†å‰²` `é›·è¾¾ç‚¹äº‘` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `èšç±»æŸå¤±` `æ ‡ç­¾æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›·è¾¾ç‚¹äº‘ç¨€ç–å™ªå£°å¯¼è‡´ç›‘ç£å­¦ä¹ æ ‡æ³¨æˆæœ¬é«˜ï¼Œä¸”éœ€å•æ¬¡æ‰«æåˆ†å‰²è¿åŠ¨ç›®æ ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ä¸Žç›‘ç£å¾®è°ƒï¼Œå¼•å…¥èšç±»å¯¹æ¯”æŸå¤±å’ŒåŠ¨æ€ç‚¹ç§»é™¤ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè‡ªç›‘ç£é¢„è®­ç»ƒæå‡æ ‡ç­¾æ•ˆçŽ‡ï¼Œå¾®è°ƒåŽè¶…è¶ŠçŽ°æœ‰æœ€ä½³æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Moving object segmentation is a crucial task for safe and reliable autonomous
> mobile systems like self-driving cars, improving the reliability and robustness
> of subsequent tasks like SLAM or path planning. While the segmentation of
> camera or LiDAR data is widely researched and achieves great results, it often
> introduces an increased latency by requiring the accumulation of temporal
> sequences to gain the necessary temporal context. Radar sensors overcome this
> problem with their ability to provide a direct measurement of a point's Doppler
> velocity, which can be exploited for single-scan moving object segmentation.
> However, radar point clouds are often sparse and noisy, making data annotation
> for use in supervised learning very tedious, time-consuming, and
> cost-intensive. To overcome this problem, we address the task of
> self-supervised moving object segmentation of sparse and noisy radar point
> clouds. We follow a two-step approach of contrastive self-supervised
> representation learning with subsequent supervised fine-tuning using limited
> amounts of annotated data. We propose a novel clustering-based contrastive loss
> function with cluster refinement based on dynamic points removal to pretrain
> the network to produce motion-aware representations of the radar data. Our
> method improves label efficiency after fine-tuning, effectively boosting
> state-of-the-art performance by self-supervised pretraining.

