---
layout: default
title: HAGI++: Head-Assisted Gaze Imputation and Generation
---

# HAGI++: Head-Assisted Gaze Imputation and Generation

**arXiv**: [2511.02468v1](https://arxiv.org/abs/2511.02468) | [PDF](https://arxiv.org/pdf/2511.02468.pdf)

**ä½œè€…**: Chuhan Jiao, Zhiming Hu, Andreas Bulling

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHAGI++æ–¹æ³•ï¼Œåˆ©ç”¨å¤´å‘ä¼ æ„Ÿå™¨è§£å†³ç§»åŠ¨çœ¼åŠ¨è¿½è¸ªä¸­ç¼ºå¤±å€¼é—®é¢˜**

**å…³é”®è¯**: `çœ¼åŠ¨è¿½è¸ª` `æ•°æ®å¡«è¡¥` `å¤šæ¨¡æ€å­¦ä¹ ` `æ‰©æ•£æ¨¡åž‹` `å¤´çœ¼ç›¸å…³æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç§»åŠ¨çœ¼åŠ¨è¿½è¸ªä¸­ï¼Œçœ¨çœ¼æˆ–æ£€æµ‹é”™è¯¯å¯¼è‡´ç¼ºå¤±å€¼ï¼Œå½±å“æ•°æ®åˆ†æž
2. é‡‡ç”¨å¤šæ¨¡æ€æ‰©æ•£æ¨¡åž‹ï¼Œç»“åˆå¤´çœ¼è¿åŠ¨ç›¸å…³æ€§è¿›è¡Œæ•°æ®å¡«è¡¥
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºŽä¼ ç»Ÿæ–¹æ³•ï¼Œç”Ÿæˆæ›´çœŸå®žçš„çœ¼åŠ¨é€Ÿåº¦åˆ†å¸ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mobile eye tracking plays a vital role in capturing human visual attention
> across both real-world and extended reality (XR) environments, making it an
> essential tool for applications ranging from behavioural research to
> human-computer interaction. However, missing values due to blinks, pupil
> detection errors, or illumination changes pose significant challenges for
> further gaze data analysis. To address this challenge, we introduce HAGI++ - a
> multi-modal diffusion-based approach for gaze data imputation that, for the
> first time, uses the integrated head orientation sensors to exploit the
> inherent correlation between head and eye movements. HAGI++ employs a
> transformer-based diffusion model to learn cross-modal dependencies between eye
> and head representations and can be readily extended to incorporate additional
> body movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D,
> and HOT3D datasets demonstrate that HAGI++ consistently outperforms
> conventional interpolation methods and deep learning-based time-series
> imputation baselines in gaze imputation. Furthermore, statistical analyses
> confirm that HAGI++ produces gaze velocity distributions that closely match
> actual human gaze behaviour, ensuring more realistic gaze imputations.
> Moreover, by incorporating wrist motion captured from commercial wearable
> devices, HAGI++ surpasses prior methods that rely on full-body motion capture
> in the extreme case of 100% missing gaze data (pure gaze generation). Our
> method paves the way for more complete and accurate eye gaze recordings in
> real-world settings and has significant potential for enhancing gaze-based
> analysis and interaction across various application domains.

