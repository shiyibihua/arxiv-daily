---
layout: default
title: LLEXICORP: End-user Explainability of Convolutional Neural Networks
---

# LLEXICORP: End-user Explainability of Convolutional Neural Networks

**arXiv**: [2511.02720v1](https://arxiv.org/abs/2511.02720) | [PDF](https://arxiv.org/pdf/2511.02720.pdf)

**ä½œè€…**: VojtÄ›ch KÅ¯r, Adam Bajger, Adam KukuÄka, Marek Hradil, VÃ­t Musil, TomÃ¡Å¡ BrÃ¡zdil

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLEXICORPä»¥è‡ªåŠ¨ç”ŸæˆCNNæ¦‚å¿µè§£é‡Šï¼Œæå‡å¯è§£é‡Šæ€§å¯è®¿é—®æ€§ã€‚**

**å…³é”®è¯**: `å·ç§¯ç¥žç»ç½‘ç»œ` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `æ¦‚å¿µç›¸å…³æ€§ä¼ æ’­` `å¤§è¯­è¨€æ¨¡åž‹` `è‡ªç„¶è¯­è¨€è§£é‡Š` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCNNæ¦‚å¿µç›¸å…³æ€§ä¼ æ’­æ–¹æ³•ä¾èµ–ä¸“å®¶æ‰‹åŠ¨å‘½åå’Œè§£é‡Šï¼Œé™åˆ¶å¯æ‰©å±•æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ¦‚å¿µç›¸å…³æ€§ä¼ æ’­ä¸Žå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œè‡ªåŠ¨å‘½åæ¦‚å¿µå¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNetå›¾åƒä¸Šå®šæ€§è¯„ä¼°ï¼Œæ˜¾ç¤ºèƒ½é™ä½Žè§£é‡Šæ·±åº¦ç¥žç»ç½‘ç»œçš„éšœç¢ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Convolutional neural networks (CNNs) underpin many modern computer vision
> systems. With applications ranging from common to critical areas, a need to
> explain and understand the model and its decisions (XAI) emerged. Prior works
> suggest that in the top layers of CNNs, the individual channels can be
> attributed to classifying human-understandable concepts. Concept relevance
> propagation (CRP) methods can backtrack predictions to these channels and find
> images that most activate these channels. However, current CRP workflows are
> largely manual: experts must inspect activation images to name the discovered
> concepts and must synthesize verbose explanations from relevance maps, limiting
> the accessibility of the explanations and their scalability.
>   To address these issues, we introduce Large Language model EXplaIns COncept
> Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a
> multimodal large language model. Our approach automatically assigns descriptive
> names to concept prototypes and generates natural-language explanations that
> translate quantitative relevance distributions into intuitive narratives. To
> ensure faithfulness, we craft prompts that teach the language model the
> semantics of CRP through examples and enforce a separation between naming and
> explanation tasks. The resulting text can be tailored to different audiences,
> offering low-level technical descriptions for experts and high-level summaries
> for non-technical stakeholders.
>   We qualitatively evaluate our method on various images from ImageNet on a
> VGG16 model. Our findings suggest that integrating concept-based attribution
> methods with large language models can significantly lower the barrier to
> interpreting deep neural networks, paving the way for more transparent AI
> systems.

