---
layout: default
title: Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes
---

# Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes

**arXiv**: [2511.02503v1](https://arxiv.org/abs/2511.02503) | [PDF](https://arxiv.org/pdf/2511.02503.pdf)

**ä½œè€…**: Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPtychoBenchåŸºå‡†ï¼Œæ¯”è¾ƒSFTä¸ŽICLç­–ç•¥åœ¨ä½Žæ•°æ®Xå°„çº¿å å±‚æˆåƒä¸­çš„ä»»åŠ¡ä¾èµ–æ€§ä¼˜åŒ–**

**å…³é”®è¯**: `Xå°„çº¿å å±‚æˆåƒ` `åŸºç¡€æ¨¡åž‹é€‚åº”` `ç›‘ç£å¾®è°ƒ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å¤šæ¨¡æ€åŸºå‡†` `ä½Žæ•°æ®å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé€šç”¨åŸºç¡€æ¨¡åž‹åœ¨ç§‘å­¦ä»»åŠ¡ä¸­é€‚åº”ç­–ç•¥ä¸æ˜Žç¡®ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šæ¨¡æ€åŸºå‡†ï¼Œç³»ç»Ÿæ¯”è¾ƒç›‘ç£å¾®è°ƒä¸Žä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè§†è§‰ä»»åŠ¡SFTä¸ŽICLäº’è¡¥ï¼Œæ–‡æœ¬ä»»åŠ¡ICLæ›´ä¼˜ï¼Œæä¾›AIç§‘å­¦åº”ç”¨æ¡†æž¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The automation of workflows in advanced microscopy is a key goal where
> foundation models like Language Models (LLMs) and Vision-Language Models (VLMs)
> show great potential. However, adapting these general-purpose models for
> specialized scientific tasks is critical, and the optimal domain adaptation
> strategy is often unclear. To address this, we introduce PtychoBench, a new
> multi-modal, multi-task benchmark for ptychographic analysis. Using this
> benchmark, we systematically compare two specialization strategies: Supervised
> Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies
> on a visual artifact detection task with VLMs and a textual parameter
> recommendation task with LLMs in a data-scarce regime. Our findings reveal that
> the optimal specialization pathway is task-dependent. For the visual task, SFT
> and ICL are highly complementary, with a fine-tuned model guided by
> context-aware examples achieving the highest mean performance (Micro-F1 of
> 0.728). Conversely, for the textual task, ICL on a large base model is the
> superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a
> powerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirm
> the superiority of context-aware prompting and identify a consistent contextual
> interference phenomenon in fine-tuned models. These results, benchmarked
> against strong baselines including GPT-4o and a DINOv3-based classifier, offer
> key observations for AI in science: the optimal specialization path in our
> benchmark is dependent on the task modality, offering a clear framework for
> developing more effective science-based agentic systems.

