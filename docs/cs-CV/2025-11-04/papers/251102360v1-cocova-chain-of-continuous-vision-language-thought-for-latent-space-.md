---
layout: default
title: CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning
---

# CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning

**arXiv**: [2511.02360v1](https://arxiv.org/abs/2511.02360) | [PDF](https://arxiv.org/pdf/2511.02360.pdf)

**ä½œè€…**: Jizheng Ma, Xiaofei Zhou, Yanlong Song, Han Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoCoVaæ¡†æž¶ï¼Œé€šè¿‡è¿žç»­æ½œç©ºé—´æŽ¨ç†è§£å†³è§†è§‰è¯­è¨€æ¨¡åž‹ç¦»æ•£åŒ–ç“¶é¢ˆé—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `è¿žç»­æŽ¨ç†` `æ½œç©ºé—´ä¼˜åŒ–` `å¤šä»»åŠ¡å­¦ä¹ ` `è·¨æ¨¡æ€èžåˆ` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹å—é™äºŽç¦»æ•£è¯­è¨€ç©ºé—´ï¼Œæ— æ³•å……åˆ†è¡¨è¾¾è§†è§‰æ„ŸçŸ¥çš„ä¸°å¯Œæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥LQ-Formerè¿­ä»£ä¼˜åŒ–æ½œæ€ç»´å‘é‡ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£é‡å»ºè¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨1.5Béª¨å¹²æ¨¡åž‹ä¸Šè¶…è¶Šæˆ–åª²ç¾Žæ›´å¤§æ¨¡åž‹ï¼Œæ½œç©ºé—´æ•èŽ·å¯è§£é‡ŠæŽ¨ç†æ¨¡å¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In human cognition, there exist numerous thought processes that are tacit and
> beyond verbal expression, enabling us to understand and interact with the world
> in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
> constrained to reasoning within the discrete and rigid space of linguistic
> tokens, thereby bottlenecking the rich, high-dimensional nature of visual
> perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
> Vision-Language Thought), a novel framework for vision-language model that
> leverages continuous cross-modal reasoning for diverse vision-language tasks.
> The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
> Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
> chain of latent thought vectors through cross-modal fusion. To focus this
> process, a token selection mechanism dynamically identifies salient visual
> regions, mimicking attentional focus. To ensure these latent thoughts remain
> grounded, we train the model with a multi-task objective that combines
> contrastive learning and diffusion-based reconstruction, enforcing alignment
> between latent representations and both visual and textual modalities.
> Evaluations show CoCoVa improves accuracy and token efficiency over strong
> baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
> models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
> competitive with state-of-the-art models. Qualitative analysis validates that
> learned latent space captures interpretable and structured reasoning patterns,
> highlighting the potential of CoCoVa to bridge the representational gap between
> discrete language processing and the continuous nature of visual understanding.

