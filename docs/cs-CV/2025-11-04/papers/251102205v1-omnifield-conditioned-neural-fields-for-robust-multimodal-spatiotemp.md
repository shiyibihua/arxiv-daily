---
layout: default
title: OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning
---

# OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning

**arXiv**: [2511.02205v1](https://arxiv.org/abs/2511.02205) | [PDF](https://arxiv.org/pdf/2511.02205.pdf)

**ä½œè€…**: Kevin Valencia, Thilina Balasooriya, Xihaier Luo, Shinjae Yoo, David Keetae Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniFieldä»¥è§£å†³å¤šæ¨¡æ€æ—¶ç©ºæ•°æ®ç¨€ç–ã€å™ªå£°å’Œæ¨¡æ€ç¼ºå¤±é—®é¢˜**

**å…³é”®è¯**: `ç¥žç»åœº` `å¤šæ¨¡æ€å­¦ä¹ ` `æ—¶ç©ºå»ºæ¨¡` `è·¨æ¨¡æ€èžåˆ` `é²æ£’æ€§å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æ—¶ç©ºæ•°æ®ç¨€ç–ã€ä¸è§„åˆ™ã€å™ªå£°å¤§ï¼Œä¸”æ¨¡æ€åœ¨æ—¶ç©ºä¸Šå˜åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ¡ä»¶ç¥žç»åœºå’Œè·¨æ¨¡æ€èžåˆå—ï¼Œå®žçŽ°è¿žç»­å­¦ä¹ ä¸Žå¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å™ªå£°ä¸‹æ€§èƒ½ç¨³å®šï¼Œä¼˜äºŽå¤šä¸ªåŸºçº¿æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal spatiotemporal learning on real-world experimental data is
> constrained by two challenges: within-modality measurements are sparse,
> irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set of
> available modalities varies across space and time, shrinking the usable record
> unless models can adapt to arbitrary subsets at train and test time. We propose
> OmniField, a continuity-aware framework that learns a continuous neural field
> conditioned on available modalities and iteratively fuses cross-modal context.
> A multimodal crosstalk block architecture paired with iterative cross-modal
> refinement aligns signals prior to the decoder, enabling unified
> reconstruction, interpolation, forecasting, and cross-modal prediction without
> gridding or surrogate preprocessing. Extensive evaluations show that OmniField
> consistently outperforms eight strong multimodal spatiotemporal baselines.
> Under heavy simulated sensor noise, performance remains close to clean-input
> levels, highlighting robustness to corrupted measurements.

