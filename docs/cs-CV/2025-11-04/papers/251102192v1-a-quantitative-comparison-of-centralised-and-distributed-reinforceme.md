---
layout: default
title: A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms
---

# A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms

**arXiv**: [2511.02192v1](https://arxiv.org/abs/2511.02192) | [PDF](https://arxiv.org/pdf/2511.02192.pdf)

**ä½œè€…**: Linxin Hou, Qirui Wu, Zhihang Qin, Neil Banerjee, Yongxin Guo, Cecilia Laschi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒé›†ä¸­ä¸Žåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ åœ¨è½¯ä½“æœºæ¢°è‡‚æŽ§åˆ¶ä¸­çš„æ€§èƒ½ï¼ŒåŸºäºŽCosseratæ¨¡åž‹ä»¿çœŸã€‚**

**å…³é”®è¯**: `è½¯ä½“æœºå™¨äººæŽ§åˆ¶` `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `Cosseratæ¨¡åž‹ä»¿çœŸ` `ç­–ç•¥ä¼˜åŒ–æ¯”è¾ƒ` `æ ·æœ¬æ•ˆçŽ‡åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›†ä¸­ä¸Žåˆ†å¸ƒå¼MARLåœ¨è½¯ä½“æœºæ¢°è‡‚æŽ§åˆ¶ä¸­çš„æ€§èƒ½å·®å¼‚ï¼ŒéšæŽ§åˆ¶æ®µæ•°å˜åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨PPOå’ŒMAPPOï¼Œåœ¨PyElasticaå’ŒOpenAI Gymä¸­è®­ç»ƒï¼Œè¯„ä¼°å¤šç§åœºæ™¯ã€‚
3. å®žéªŒæ•ˆæžœï¼šåˆ†å¸ƒå¼ç­–ç•¥åœ¨é«˜æŽ§åˆ¶æ®µæ•°æ—¶æ ·æœ¬æ•ˆçŽ‡é«˜ï¼Œé›†ä¸­ç­–ç•¥è®­ç»ƒæ—¶é—´æ›´çŸ­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a quantitative comparison between centralised and
> distributed multi-agent reinforcement learning (MARL) architectures for
> controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
> PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
> Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
> budgets. Both approaches are based on the arm having $n$ number of controlled
> sections. The study systematically varies $n$ and evaluates the performance of
> the arm to reach a fixed target in three scenarios: default baseline condition,
> recovery from external disturbance, and adaptation to actuator failure.
> Quantitative metrics used for the evaluation are mean action magnitude, mean
> final distance, mean episode length, and success rate. The results show that
> there are no significant benefits of the distributed policy when the number of
> controlled sections $n\le4$. In very simple systems, when $n\le2$, the
> centralised policy outperforms the distributed one. When $n$ increases to $4<
> n\le 12$, the distributed policy shows a high sample efficiency. In these
> systems, distributed policy promotes a stronger success rate, resilience, and
> robustness under local observability and yields faster convergence given the
> same sample size. However, centralised policies achieve much higher time
> efficiency during training as it takes much less time to train the same size of
> samples. These findings highlight the trade-offs between centralised and
> distributed policy in reinforcement learning-based control for soft robotic
> systems and provide actionable design guidance for future sim-to-real transfer
> in soft rod-like manipulators.

