---
layout: default
title: When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought
---

# When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought

**arXiv**: [2511.02779v1](https://arxiv.org/abs/2511.02779) | [PDF](https://arxiv.org/pdf/2511.02779.pdf)

**ä½œè€…**: Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIRAåŸºå‡†ä»¥è¯„ä¼°æ¨¡åž‹åœ¨ç”Ÿæˆä¸­é—´è§†è§‰å›¾åƒè¾…åŠ©æŽ¨ç†çš„åœºæ™¯**

**å…³é”®è¯**: `è§†è§‰æŽ¨ç†åŸºå‡†` `å¤šæ¨¡æ€è¯„ä¼°` `é“¾å¼æ€ç»´` `ä¸­é—´å›¾åƒç”Ÿæˆ` `ç©ºé—´å…³ç³»æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ–‡æœ¬é“¾å¼æ€ç»´æ–¹æ³•åœ¨å¤æ‚ç©ºé—´æŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ï¼Œéœ€è§†è§‰è¾…åŠ©ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡546ä¸ªå¤šæ¨¡æ€é—®é¢˜ï¼ŒåŒ…å«ä¸­é—´è§†è§‰å›¾åƒå’Œç»Ÿä¸€è¯„ä¼°åè®®ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæä¾›è§†è§‰çº¿ç´¢ä½¿æ¨¡åž‹æ€§èƒ½å¹³å‡æå‡33.7%ï¼Œå¼ºè°ƒè§†è§‰æƒ³è±¡çš„å…³é”®ä½œç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose MIRA, a new benchmark designed to evaluate models in scenarios
> where generating intermediate visual images is essential for successful
> reasoning. Unlike traditional CoT methods that rely solely on text, tasks in
> MIRA require models to generate and utilize intermediate images - such as
> sketches, structural diagrams, or path drawings - to guide their reasoning
> process. This setup closely mirrors how humans solve complex problems through
> "drawing to think". To solve this, MIRA focuses on tasks that are intrinsically
> challenging and involve complex structures, spatial relationships, or reasoning
> steps that are difficult to express through language alone. To ensure that our
> evaluation data is of high-quality, we include 546 multimodal problems,
> annotated with intermediate visual images and final answers. We also propose a
> unified evaluation protocol for MIRA that spans three levels of evaluation
> input: direct input with image and question only, text-only CoT input with
> image and thinking prompts, and Visual-CoT input with both annotated image
> clues and textual thinking prompts. To probe the upper bound of model capacity
> on our benchmark, we also report pass@k and majority voting accuracies under
> different k settings. Experimental results show that existing multimodal large
> language models, including strongest private models as well as strong
> open-weight models, perform poorly when relying solely on textual prompts.
> However, when intermediate visual cues are provided, model performance improves
> consistently, yielding an average relative gain of 33.7% across all models and
> tasks. We also probe the upper bound by expanding the search space and
> designing textual prompts aligned with Visual-CoT, but both yield only limited
> improvements compared to our Visual-CoT setting. These results underscore the
> critical role of imagined visual information in enabling successful reasoning
> on MIRA.

