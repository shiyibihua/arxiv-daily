---
layout: default
title: LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation
---

# LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation

**arXiv**: [2511.02239v1](https://arxiv.org/abs/2511.02239) | [PDF](https://arxiv.org/pdf/2511.02239.pdf)

**ä½œè€…**: Youngjin Hong, Houjian Yu, Mingen Li, Changhyun Choi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLACYæ¡†æž¶ï¼Œé€šè¿‡åŒå‘è¯­è¨€-åŠ¨ä½œæ˜ å°„å®žçŽ°æœºå™¨äººæ“ä½œçš„è‡ªæˆ‘æ”¹è¿›**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŒå‘æ˜ å°„` `è‡ªæˆ‘ç›‘ç£å­¦ä¹ ` `è¯­è¨€-åŠ¨ä½œå¾ªçŽ¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•å‘è¯­è¨€åˆ°åŠ¨ä½œæ˜ å°„ç¼ºä¹ä¸Šä¸‹æ–‡ç†è§£ï¼Œé™åˆ¶æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šè”åˆè®­ç»ƒè¯­è¨€åˆ°åŠ¨ä½œã€åŠ¨ä½œåˆ°è¯­è¨€å’Œè¯­è¨€ä¸€è‡´æ€§éªŒè¯ä»»åŠ¡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ‹¾å–ä»»åŠ¡ä¸­å¹³å‡æé«˜æˆåŠŸçŽ‡56.46%ï¼Œå¢žå¼ºè¯­è¨€-åŠ¨ä½œåŸºç¡€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Learning generalizable policies for robotic manipulation increasingly relies
> on large-scale models that map language instructions to actions (L2A). However,
> this one-way paradigm often produces policies that execute tasks without deeper
> contextual understanding, limiting their ability to generalize or explain their
> behavior. We argue that the complementary skill of mapping actions back to
> language (A2L) is essential for developing more holistic grounding. An agent
> capable of both acting and explaining its actions can form richer internal
> representations and unlock new paradigms for self-supervised learning. We
> introduce LACY (Language-Action Cycle), a unified framework that learns such
> bidirectional mappings within a single vision-language model. LACY is jointly
> trained on three synergistic tasks: generating parameterized actions from
> language (L2A), explaining observed actions in language (A2L), and verifying
> semantic consistency between two language descriptions (L2C). This enables a
> self-improving cycle that autonomously generates and filters new training data
> through an active augmentation strategy targeting low-confidence cases, thereby
> improving the model without additional human labels. Experiments on
> pick-and-place tasks in both simulation and the real world show that LACY
> improves task success rates by 56.46% on average and yields more robust
> language-action grounding for robotic manipulation. Project page:
> https://vla2026.github.io/LACY/

