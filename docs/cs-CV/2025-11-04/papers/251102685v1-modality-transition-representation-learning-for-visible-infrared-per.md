---
layout: default
title: Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification
---

# Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification

**arXiv**: [2511.02685v1](https://arxiv.org/abs/2511.02685) | [PDF](https://arxiv.org/pdf/2511.02685.pdf)

**ä½œè€…**: Chao Yuan, Zanwu Liu, Guiwei Zhang, Haoxuan Xu, Yujian Zhao, Guanglin Niu, Bo Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡æ€è½¬æ¢è¡¨ç¤ºå­¦ä¹ æ¡†æž¶ä»¥è§£å†³å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚é—®é¢˜**

**å…³é”®è¯**: `å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«` `æ¨¡æ€è½¬æ¢è¡¨ç¤ºå­¦ä¹ ` `è·¨æ¨¡æ€ç‰¹å¾å¯¹é½` `å¯¹æ¯”å­¦ä¹ ` `æ— é¢å¤–å‚æ•°è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯è§å…‰ä¸Žçº¢å¤–æ¨¡æ€é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–ä¸­é—´è¡¨ç¤ºä½†æœªå……åˆ†åˆ©ç”¨
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç”Ÿæˆä¸­é—´å›¾åƒä½œä¸ºè½¬æ¢å™¨ï¼Œç»“åˆå¯¹æ¯”æŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±å¯¹é½è·¨æ¨¡æ€ç‰¹å¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªVI-ReIDæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ— éœ€é¢å¤–å‚æ•°ï¼ŒæŽ¨ç†é€Ÿåº¦ä¸Žéª¨å¹²ç½‘ç»œç›¸åŒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visible-infrared person re-identification (VI-ReID) technique could associate
> the pedestrian images across visible and infrared modalities in the practical
> scenarios of background illumination changes. However, a substantial gap
> inherently exists between these two modalities. Besides, existing methods
> primarily rely on intermediate representations to align cross-modal features of
> the same person. The intermediate feature representations are usually create by
> generating intermediate images (kind of data enhancement), or fusing
> intermediate features (more parameters, lack of interpretability), and they do
> not make good use of the intermediate features. Thus, we propose a novel
> VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a
> middle generated image as a transmitter from visible to infrared modals, which
> are fully aligned with the original visible images and similar to the infrared
> modality. After that, using a modality-transition contrastive loss and a
> modality-query regularization loss for training, which could align the
> cross-modal features more effectively. Notably, our proposed framework does not
> need any additional parameters, which achieves the same inference speed to the
> backbone while improving its performance on VI-ReID task. Extensive
> experimental results illustrate that our model significantly and consistently
> outperforms existing SOTAs on three typical VI-ReID datasets.

