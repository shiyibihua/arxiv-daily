---
layout: default
title: MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer
---

# MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer

**arXiv**: [2511.02473v1](https://arxiv.org/abs/2511.02473) | [PDF](https://arxiv.org/pdf/2511.02473.pdf)

**ä½œè€…**: Taiga Yamane, Satoshi Suzuki, Ryo Masumura, Shotaro Tora

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMVAFormerä»¥è§£å†³å¤šè§†è§’æ—¶ç©ºåŠ¨ä½œè¯†åˆ«ä¸­çš„åä½œé—®é¢˜**

**å…³é”®è¯**: `å¤šè§†è§’åŠ¨ä½œè¯†åˆ«` `æ—¶ç©ºåŠ¨ä½œè¯†åˆ«` `Transformer` `ç‰¹å¾å›¾åä½œ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šè§†è§’åŠ¨ä½œè¯†åˆ«æ–¹æ³•ä¸é€‚ç”¨äºŽæ—¶ç©ºåŠ¨ä½œè¯†åˆ«è®¾ç½®ï¼Œæ— æ³•å¤„ç†åºåˆ—åŒ–ä¸ªäººåŠ¨ä½œè¯†åˆ«ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŸºäºŽTransformerçš„åä½œæ¨¡å—ï¼Œåˆ©ç”¨ç‰¹å¾å›¾ä¿ç•™ç©ºé—´ä¿¡æ¯ï¼Œå¹¶åˆ†è§†å›¾è‡ªæ³¨æ„åŠ›å»ºæ¨¡å…³ç³»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–°æ•°æ®é›†ä¸Šï¼ŒF-measureæ¯”åŸºçº¿æå‡çº¦4.4ç‚¹ï¼ŒéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-view action recognition aims to recognize human actions using multiple
> camera views and deals with occlusion caused by obstacles or crowds. In this
> task, cooperation among views, which generates a joint representation by
> combining multiple views, is vital. Previous studies have explored promising
> cooperation methods for improving performance. However, since their methods
> focus only on the task setting of recognizing a single action from an entire
> video, they are not applicable to the recently popular spatio-temporal action
> recognition~(STAR) setting, in which each person's action is recognized
> sequentially. To address this problem, this paper proposes a multi-view action
> recognition method for the STAR setting, called MVAFormer. In MVAFormer, we
> introduce a novel transformer-based cooperation module among views. In contrast
> to previous studies, which utilize embedding vectors with lost spatial
> information, our module utilizes the feature map for effective cooperation in
> the STAR setting, which preserves the spatial information. Furthermore, in our
> module, we divide the self-attention for the same and different views to model
> the relationship between multiple views effectively. The results of experiments
> using a newly collected dataset demonstrate that MVAFormer outperforms the
> comparison baselines by approximately $4.4$ points on the F-measure.

