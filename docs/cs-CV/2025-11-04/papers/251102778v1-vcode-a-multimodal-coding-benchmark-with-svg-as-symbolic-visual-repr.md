---
layout: default
title: VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation
---

# VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation

**arXiv**: [2511.02778v1](https://arxiv.org/abs/2511.02778) | [PDF](https://arxiv.org/pdf/2511.02778.pdf)

**ä½œè€…**: Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVCodeåŸºå‡†ä¸ŽVCoderæ¡†æž¶ï¼Œä»¥SVGä½œä¸ºç¬¦å·è§†è§‰è¡¨ç¤ºè§£å†³å¤šæ¨¡æ€ç†è§£é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç¼–ç åŸºå‡†` `SVGç¬¦å·è¡¨ç¤º` `ä»£ç ç”Ÿæˆè¯„ä¼°` `è§†è§‰è¯­è¨€æ¨¡åž‹å¢žå¼º` `è¿­ä»£ä¿®è®¢æ¡†æž¶` `è§†è§‰å·¥å…·é›†æˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰ä¸­å¿ƒç¼–ç åœ¨ä»£ç†æ—¶ä»£è¢«å¿½è§†ï¼ŒçŽ°æœ‰æ¨¡åž‹éš¾ä»¥ç”Ÿæˆå¿ å®žç¬¦å·è§†è§‰è¡¨ç¤º
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥SVGä»£ç ä½œä¸ºç´§å‡‘å¯æ‰§è¡Œè¡¨ç¤ºï¼Œå¹¶å¼€å‘VCoderæ¡†æž¶é€šè¿‡è¿­ä»£ä¿®è®¢å’Œè§†è§‰å·¥å…·å¢žå¼ºVLM
3. å®žéªŒæˆ–æ•ˆæžœï¼šVCoderåœ¨åŸºå‡†ä¸Šä¼˜äºŽClaude-4-Opus 12.3åˆ†ï¼Œäººç±»ä¸ŽVLMåœ¨SVGä¸Šè¡¨çŽ°ä¸€è‡´

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Code has emerged as a precise and executable medium for reasoning and action
> in the agent era. Yet, progress has largely focused on language-centric tasks
> such as program synthesis and debugging, leaving visual-centric coding
> underexplored. Inspired by how humans reason over sketches, we advocate SVG
> code as a compact, interpretable, and executable visual representation. We
> introduce VCode, a benchmark that reframes multimodal understanding as code
> generation: given an image, a model must produce SVG that preserves symbolic
> meaning for downstream reasoning. VCode covers three domains - general
> commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
> perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
> evaluation protocol in which a policy model answers questions over rendered
> SVGs; correct answers indicate faithful symbolic preservation. Empirically,
> frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
> between language-centric and visual-centric coding. To close this gap, we
> introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
> Thinking with Revision, which iteratively analyzes discrepancies and refines
> SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
> structured cues such as objects, shapes, and text beyond the model's intrinsic
> capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
> score well overall yet remain limited in professional knowledge and 3D
> reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
> Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
> rendered SVGs, their consistency reveals the promise of symbolic visual
> representation. The benchmark and code are available at
> https://github.com/CSU-JPG/VCode.

