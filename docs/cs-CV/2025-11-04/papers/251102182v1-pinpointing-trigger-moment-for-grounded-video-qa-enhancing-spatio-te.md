---
layout: default
title: Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models
---

# Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models

**arXiv**: [2511.02182v1](https://arxiv.org/abs/2511.02182) | [PDF](https://arxiv.org/pdf/2511.02182.pdf)

**ä½œè€…**: Jinhwan Seo, Yoonki Cho, Junhyug Noh, Sung-eui Yoon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§¦å‘æ—¶åˆ»æ¡†æž¶ä»¥å¢žå¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘é—®ç­”ä¸­çš„æ—¶ç©ºå®šä½èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `æ—¶ç©ºå®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§¦å‘æ—¶åˆ»` `ç›®æ ‡è·Ÿè¸ª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘é—®ç­”ä»»åŠ¡éœ€æ¨¡åž‹è¿›è¡Œå¤æ‚æŽ¨ç†ã€è§†è§‰å®šä½å’Œæ—¶é—´è·Ÿè¸ªã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œå¼•å…¥è§¦å‘æ—¶åˆ»ä½œä¸ºç›®æ ‡å¯¹è±¡æœ€å¯è§å¸§çš„é”šç‚¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨GVQAä»»åŠ¡ä¸­HOTAå¾—åˆ†0.4968ï¼Œæ˜¾è‘—ä¼˜äºŽåŽ»å¹´èŽ·èƒœåˆ†æ•°0.2704ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this technical report, we introduce a framework to address Grounded Video
> Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The
> GVQA task demands robust multimodal models capable of complex reasoning over
> video content, grounding the resulting answers visually, and tracking the
> referenced objects temporally. To achieve this capability, our proposed
> approach decomposes the GVQA task into a three-stage pipeline: (1) Video
> Reasoning \& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key
> contribution is the introduction of a trigger moment, derived from our proposed
> CORTEX prompt, which pinpoints the single most visible frame of a target object
> to serve as a robust anchor for grounding and tracking. To this end, we achieve
> the HOTA score of 0.4968, which marks a significant improvement over the
> previous year's winning score of 0.2704 on GVQA task.

