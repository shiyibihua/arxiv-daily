---
layout: default
title: Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs
---

# Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs

**arXiv**: [2511.02404v1](https://arxiv.org/abs/2511.02404) | [PDF](https://arxiv.org/pdf/2511.02404.pdf)

**ä½œè€…**: Arya Shah, Vaibhav Tripathi

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€åŸºå‡†è¯„ä¼°çŒ«ä¸äººç±»è§†è§‰è¡¨å¾å¯¹é½ï¼Œå‘ç°è‡ªç›‘ç£ViTåœ¨è·¨ç‰©ç§å¯¹é½ä¸­è¡¨ç°æœ€ä½³**

**å…³é”®è¯**: `è·¨ç‰©ç§è§†è§‰å¯¹é½` `è¡¨å¾ç›¸ä¼¼æ€§åˆ†æ` `è‡ªç›‘ç£è§†è§‰Transformer` `å·ç§¯ç¥ç»ç½‘ç»œ` `è§†è§‰TransformeråŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŒ«ä¸äººç±»çœ¼éƒ¨è§£å‰–å·®å¼‚å¦‚ä½•å½±å“è§†è§‰è¡¨å¾å¯¹é½ï¼Œå°¤å…¶åœ¨è·¨ç‰©ç§åœºæ™¯ä¸­
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å†»ç»“ç¼–ç å™¨åŸºå‡†ï¼Œç»“åˆCKAå’ŒRSAåˆ†æå¤šç§æ¨¡å‹è¡¨å¾ç›¸ä¼¼æ€§
3. å®éªŒæˆ–æ•ˆæœï¼šDINO ViT-B/16å¯¹é½åº¦æœ€é«˜ï¼Œè‡ªç›‘ç£ViTä¼˜äºCNNå’Œçª—å£Transformer

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic
> cats) have vertically elongated pupils linked to ambush predation; yet, how
> such specializations manifest in downstream visual representations remains
> incompletely understood. We present a unified, frozen-encoder benchmark that
> quantifies feline-human cross-species representational alignment in the wild,
> across convolutional networks, supervised Vision Transformers, windowed
> transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel
> Alignment (linear and RBF) and Representational Similarity Analysis, with
> additional distributional and stability tests reported in the paper. Across
> models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF
> $\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$),
> peaking at early blocks, indicating that token-level self-supervision induces
> early-stage features that bridge species-specific statistics. Supervised ViTs
> are competitive on CKA yet show weaker geometric correspondence than DINO
> (e.g., ViT-B/16 RSA $\approx0.53$ at block8; ViT-L/16 $\approx0.47$ at
> block14), revealing depth-dependent divergences between similarity and
> representational geometry. CNNs remain strong baselines but below plain ViTs on
> alignment, and windowed transformers underperform plain ViTs, implicating
> architectural inductive biases in cross-species alignment. Results indicate
> that self-supervision coupled with ViT inductive biases yields representational
> geometries that more closely align feline and human visual systems than widely
> used CNNs and windowed Transformers, providing testable neuroscientific
> hypotheses about where and how cross-species visual computations converge. We
> release our code and dataset for reference and reproducibility.

