---
layout: default
title: XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations
---

# XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations

**arXiv**: [2511.02776v1](https://arxiv.org/abs/2511.02776) | [PDF](https://arxiv.org/pdf/2511.02776.pdf)

**ä½œè€…**: Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºXR-1æ¡†æž¶ï¼Œé€šè¿‡ç»Ÿä¸€è§†è§‰-è¿åŠ¨è¡¨ç¤ºè§£å†³æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ä¸­çš„åŠ¨ä½œç”Ÿæˆä¸Žæ•°æ®å¼‚æž„æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ç»Ÿä¸€è§†è§‰-è¿åŠ¨è¡¨ç¤º` `åŒåˆ†æ”¯VQ-VAE` `è·¨æœºå™¨äººå­¦ä¹ ` `å¤šæ¨¡æ€å¯¹é½` `æœºå™¨äººæ“ä½œä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLAæ¨¡åž‹éš¾ä»¥ä»Žé«˜ç»´è§‚å¯Ÿç”Ÿæˆç²¾ç¡®ä½Žçº§åŠ¨ä½œï¼Œä¸”è·¨å¼‚æž„æ•°æ®æºå­˜åœ¨é¢†åŸŸå·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ç»Ÿä¸€è§†è§‰-è¿åŠ¨ä»£ç ï¼Œé€šè¿‡åŒåˆ†æ”¯VQ-VAEè”åˆç¼–ç è§†è§‰åŠ¨æ€ä¸Žæœºå™¨äººè¿åŠ¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…­ç§æœºå™¨äººä¸ŠéªŒè¯ï¼Œä¼˜äºŽå¤šä¸ªåŸºçº¿ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºäºŽæ–°å¯¹è±¡å’ŒèƒŒæ™¯å˜åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in large-scale robotic datasets and vision-language models
> (VLMs) has advanced research on vision-language-action (VLA) models. However,
> existing VLA models still face two fundamental challenges: (i) producing
> precise low-level actions from high-dimensional observations, (ii) bridging
> domain gaps across heterogeneous data sources, including diverse robot
> embodiments and human demonstrations. Existing methods often encode latent
> variables from either visual dynamics or robotic actions to guide policy
> learning, but they fail to fully exploit the complementary multi-modal
> knowledge present in large-scale, heterogeneous datasets. In this work, we
> present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
> VLA learning across diverse robots, tasks, and environments. XR-1 introduces
> the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
> learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
> robotic motion. UVMC addresses these challenges by (i) serving as an
> intermediate representation between the observations and actions, and (ii)
> aligning multimodal dynamic information from heterogeneous data sources to
> capture complementary knowledge. To effectively exploit UVMC, we propose a
> three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
> UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
> (iii) task-specific post-training. We validate XR-1 through extensive
> real-world experiments with more than 14,000 rollouts on six different robot
> embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
> outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
> UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
> objects, background variations, distractors, and illumination changes. Our
> project is at https://xr-1-vla.github.io/.

