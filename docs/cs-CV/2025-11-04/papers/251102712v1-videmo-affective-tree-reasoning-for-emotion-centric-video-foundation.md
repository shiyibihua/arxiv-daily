---
layout: default
title: VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models
---

# VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models

**arXiv**: [2511.02712v1](https://arxiv.org/abs/2511.02712) | [PDF](https://arxiv.org/pdf/2511.02712.pdf)

**ä½œè€…**: Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVidEmoæ¡†æž¶ä»¥è§£å†³è§†é¢‘ä¸­åŠ¨æ€æƒ…æ„Ÿç†è§£éš¾é¢˜**

**å…³é”®è¯**: `è§†é¢‘æƒ…æ„Ÿç†è§£` `æƒ…æ„Ÿæ ‘æŽ¨ç†` `æŒ‡ä»¤å¾®è°ƒ` `æƒ…æ„Ÿæ•°æ®é›†` `å¼ºåŒ–å­¦ä¹ ` `åŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æƒ…æ„ŸåŠ¨æ€ä¸”ä¾èµ–çº¿ç´¢ï¼Œéš¾ä»¥ç†è§£å¤æ‚æƒ…æ„ŸçŠ¶æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æƒ…æ„Ÿçº¿ç´¢å¼•å¯¼æŽ¨ç†ï¼Œç»“åˆå±žæ€§æ„ŸçŸ¥å’Œæƒ…æ„Ÿæ ‘å¼ºåŒ–å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨15é¡¹äººè„¸æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œè®¾ç«‹æ–°é‡Œç¨‹ç¢‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding and predicting emotion from videos has gathered significant
> attention in recent studies, driven by advancements in video large language
> models (VideoLLMs). While advanced methods have made progress in video emotion
> analysis, the intrinsic nature of emotions poses significant challenges.
> Emotions are characterized by dynamic and cues-dependent properties, making it
> difficult to understand complex and evolving emotional states with reasonable
> rationale. To tackle these challenges, we propose a novel affective cues-guided
> reasoning framework that unifies fundamental attribute perception, expression
> analysis, and high-level emotional understanding in a stage-wise manner. At the
> core of our approach is a family of video emotion foundation models (VidEmo),
> specifically designed for emotion reasoning and instruction-following. These
> models undergo a two-stage tuning process: first, curriculum emotion learning
> for injecting emotion knowledge, followed by affective-tree reinforcement
> learning for emotion reasoning. Moreover, we establish a foundational data
> infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)
> consisting of 2.1M diverse instruction-based samples. Emo-CFG includes
> explainable emotional question-answering, fine-grained captions, and associated
> rationales, providing essential resources for advancing emotion understanding
> tasks. Experimental results demonstrate that our approach achieves competitive
> performance, setting a new milestone across 15 face perception tasks.

