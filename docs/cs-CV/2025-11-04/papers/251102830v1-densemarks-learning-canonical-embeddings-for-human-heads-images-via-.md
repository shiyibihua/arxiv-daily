---
layout: default
title: Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks
---

# Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks

**arXiv**: [2511.02830v1](https://arxiv.org/abs/2511.02830) | [PDF](https://arxiv.org/pdf/2511.02830.pdf)

**ä½œè€…**: Dmitrii Pozdeev, Alexey Artemov, Ananta R. Bhattarai, Artem Sevastopolsky

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDenseMarkså­¦ä¹ å¤´éƒ¨å›¾åƒçš„è§„èŒƒåµŒå…¥ï¼Œå®žçŽ°é«˜è´¨é‡å¯†é›†å¯¹åº”**

**å…³é”®è¯**: `å¯†é›†å¯¹åº”å­¦ä¹ ` `å¤´éƒ¨å›¾åƒåˆ†æž` `è§„èŒƒåµŒå…¥` `å¯¹æ¯”æŸå¤±` `å¤šä»»åŠ¡å­¦ä¹ ` `3DåµŒå…¥é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ä»Ž2Då¤´éƒ¨å›¾åƒä¸­å­¦ä¹ å¯†é›†å¯¹åº”ï¼Œä»¥å¤„ç†å§¿æ€å˜åŒ–å’Œè¦†ç›–æ•´ä¸ªå¤´éƒ¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Vision Transformeré¢„æµ‹åƒç´ çº§3DåµŒå…¥ï¼Œé€šè¿‡å¯¹æ¯”æŸå¤±å’Œå¤šä»»åŠ¡å­¦ä¹ è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å‡ ä½•æ„ŸçŸ¥ç‚¹åŒ¹é…å’Œå•ç›®å¤´éƒ¨è·Ÿè¸ªä¸­è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œä»£ç å°†å…¬å¼€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose DenseMarks - a new learned representation for human heads,
> enabling high-quality dense correspondences of human head images. For a 2D
> image of a human head, a Vision Transformer network predicts a 3D embedding for
> each pixel, which corresponds to a location in a 3D canonical unit cube. In
> order to train our network, we collect a dataset of pairwise point matches,
> estimated by a state-of-the-art point tracker over a collection of diverse
> in-the-wild talking heads videos, and guide the mapping via a contrastive loss,
> encouraging matched points to have close embeddings. We further employ
> multi-task learning with face landmarks and segmentation constraints, as well
> as imposing spatial continuity of embeddings through latent cube features,
> which results in an interpretable and queryable canonical space. The
> representation can be used for finding common semantic parts, face/head
> tracking, and stereo reconstruction. Due to the strong supervision, our method
> is robust to pose variations and covers the entire head, including hair.
> Additionally, the canonical space bottleneck makes sure the obtained
> representations are consistent across diverse poses and individuals. We
> demonstrate state-of-the-art results in geometry-aware point matching and
> monocular head tracking with 3D Morphable Models. The code and the model
> checkpoint will be made available to the public.

