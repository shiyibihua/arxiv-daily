---
layout: default
title: Dynamic Reflections: Probing Video Representations with Text Alignment
---

# Dynamic Reflections: Probing Video Representations with Text Alignment

**arXiv**: [2511.02767v1](https://arxiv.org/abs/2511.02767) | [PDF](https://arxiv.org/pdf/2511.02767.pdf)

**ä½œè€…**: Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica PÄƒtrÄƒucean, Maks Ovsjanikov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†é¢‘-æ–‡æœ¬å¯¹é½æ–¹æ³•ä»¥æŽ¢ç©¶è§†é¢‘ç¼–ç å™¨çš„è¡¨ç¤ºèƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘-æ–‡æœ¬å¯¹é½` `è·¨æ¨¡æ€è¡¨ç¤º` `æ—¶åºæŽ¨ç†` `é›¶æ ·æœ¬è¯„ä¼°` `è§†é¢‘ç¼–ç å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æ•°æ®çš„æ—¶åºç‰¹æ€§åœ¨è·¨æ¨¡æ€å¯¹é½ä¸­æœªè¢«å……åˆ†æŽ¢ç´¢
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å‚æ•°åŒ–æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹ï¼Œåˆ†æžè§†è§‰å’Œæ–‡æœ¬æ•°æ®ä¸°å¯Œåº¦çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°å¯¹é½ä¸Žä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ç›¸å…³ï¼Œæä¾›é›¶æ ·æœ¬è¯„ä¼°åŸºå‡†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The alignment of representations from different modalities has recently been
> shown to provide insights on the structural similarities and downstream
> capabilities of different encoders across diverse data types. While significant
> progress has been made in aligning images with text, the temporal nature of
> video data remains largely unexplored in this context. In this work, we conduct
> the first comprehensive study of video-text representation alignment, probing
> the capabilities of modern video and language encoders. Our findings reveal
> several key insights. First, we demonstrate that cross-modal alignment highly
> depends on the richness of both visual (static images vs. multi-frame videos)
> and text (single caption vs. a collection) data provided at test time,
> especially when using state-of-the-art video encoders. We propose parametric
> test-time scaling laws that capture this behavior and show remarkable
> predictive power against empirical observations. Secondly, we investigate the
> correlation between semantic alignment and performance on both semantic and
> non-semantic downstream tasks, providing initial evidence that strong alignment
> against text encoders may be linked to general-purpose video representation and
> understanding. Finally, we correlate temporal reasoning with cross-modal
> alignment providing a challenging test-bed for vision and language models.
> Overall, our work introduces video-text alignment as an informative zero-shot
> way to probe the representation power of different encoders for spatio-temporal
> data. Project page can be found at https://video-prh.github.io/

