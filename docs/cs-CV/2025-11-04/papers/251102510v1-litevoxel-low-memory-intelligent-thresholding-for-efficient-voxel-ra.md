---
layout: default
title: LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization
---

# LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization

**arXiv**: [2511.02510v1](https://arxiv.org/abs/2511.02510) | [PDF](https://arxiv.org/pdf/2511.02510.pdf)

**ä½œè€…**: Jee Won Lee, Jongseong Brad Choi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLiteVoxelä»¥è§£å†³ç¨€ç–ä½“ç´ æ …æ ¼åŒ–ä¸­çš„å†…å­˜è†¨èƒ€å’Œä½Žé¢‘ç»†èŠ‚ä¸¢å¤±é—®é¢˜**

**å…³é”®è¯**: `ç¨€ç–ä½“ç´ æ …æ ¼åŒ–` `å†…å­˜ä¼˜åŒ–` `è‡ªè°ƒä¼˜è®­ç»ƒ` `ä½Žé¢‘æ„ŸçŸ¥æŸå¤±` `æ·±åº¦åˆ†ä½æ•°å‰ªæž` `åœºæ™¯é‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¨€ç–ä½“ç´ æ …æ ¼åŒ–å­˜åœ¨å†…å­˜è†¨èƒ€ã€ä½Žé¢‘å†…å®¹æ¬ æ‹Ÿåˆå’Œè¾¹ç•Œä¸ç¨³å®šé—®é¢˜
2. é‡‡ç”¨è‡ªè°ƒä¼˜è®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬é€†Sobelé‡åŠ æƒå’Œæ·±åº¦åˆ†ä½æ•°å‰ªæžé€»è¾‘
3. å®žéªŒæ˜¾ç¤ºå†…å­˜å‡å°‘40%-60%ï¼Œä¿æŒPSNR/SSIMå’Œè®­ç»ƒæ—¶é—´ï¼Œæå‡ä½Žé¢‘ç»†èŠ‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sparse-voxel rasterization is a fast, differentiable alternative for
> optimization-based scene reconstruction, but it tends to underfit low-frequency
> content, depends on brittle pruning heuristics, and can overgrow in ways that
> inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that
> makes SV rasterization both steadier and lighter. Our loss is made
> low-frequency aware via an inverse-Sobel reweighting with a mid-training
> gamma-ramp, shifting gradient budget to flat regions only after geometry
> stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning
> logic on maximum blending weight, stabilized by EMA-hysteresis guards and
> refines structure through ray-footprint-based, priority-driven subdivision
> under an explicit growth budget. Ablations and full-system results across
> Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation
> of errors in low-frequency regions and boundary instability while keeping
> PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline.
> Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency
> detail that prior setups miss, enabling more predictable, memory-efficient
> training without sacrificing perceptual quality.

