---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-19
---

# cs.CVï¼ˆ2025-10-19ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251016926v3-res-bench-benchmarking-the-robustness-of-multimodal-large-language-m.html">Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input</a></td>
  <td>æå‡ºRes-Benchï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€åˆ†è¾¨ç‡è¾“å…¥ä¸‹çš„é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16926v3" onclick="toggleFavorite(this, '2510.16926v3', 'Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251017023v1-enrich-and-detect-video-temporal-grounding-with-multimodal-llms.html">Enrich and Detect: Video Temporal Grounding with Multimodal LLMs</a></td>
  <td>æå‡ºED-VTGï¼Œåˆ©ç”¨å¤šæ¨¡æ€LLMè¿›è¡Œç»†ç²’åº¦è§†é¢‘æ—¶åºå®šä½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17023v1" onclick="toggleFavorite(this, '2510.17023v1', 'Enrich and Detect: Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251016785v1-segmentation-as-a-plug-and-play-capability-for-frozen-multimodal-llm.html">Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs</a></td>
  <td>LENSï¼šä¸ºå†»ç»“å¤šæ¨¡æ€LLMæä¾›å³æ’å³ç”¨çš„åˆ†å‰²èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16785v1" onclick="toggleFavorite(this, '2510.16785v1', 'Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251016989v1-training-free-online-video-step-grounding.html">Training-free Online Video Step Grounding</a></td>
  <td>æå‡ºBaGLMï¼Œåˆ©ç”¨å¤§æ¨¡å‹é›¶æ ·æœ¬èƒ½åŠ›åœ¨çº¿è§†é¢‘æ­¥éª¤å®šä½ï¼Œè¶…è¶Šç¦»çº¿è®­ç»ƒæ–¹æ³•ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16989v1" onclick="toggleFavorite(this, '2510.16989v1', 'Training-free Online Video Step Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251016870v1-uncovering-brain-like-hierarchical-patterns-in-vision-language-model.html">Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding</a></td>
  <td>é€šè¿‡fMRIç¥ç»ç¼–ç æ­ç¤ºè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ç±»è„‘åˆ†å±‚æ¨¡å¼</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16870v1" onclick="toggleFavorite(this, '2510.16870v1', 'Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251021786v1-eventformer-a-node-graph-hierarchical-attention-transformer-for-acti.html">EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction</a></td>
  <td>æå‡ºEventFormerï¼Œç”¨äºè§£å†³åŠ¨ä½œä¸­å¿ƒè§†é¢‘äº‹ä»¶é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†AVEPã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.21786v1" onclick="toggleFavorite(this, '2510.21786v1', 'EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/251016973v1-foundation-models-in-medical-image-analysis-a-systematic-review-and-.html">Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis</a></td>
  <td>ç»¼è¿°æ€§åˆ†æåŒ»å­¦å½±åƒé¢†åŸŸä¸­çš„Foundation Modelï¼Œç³»ç»Ÿæ€§åœ°å½’çº³æ¶æ„ã€è®­ç»ƒèŒƒå¼å’Œä¸´åºŠåº”ç”¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16973v1" onclick="toggleFavorite(this, '2510.16973v1', 'Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251016732v2-a-comprehensive-survey-on-world-models-for-embodied-ai.html">A Comprehensive Survey on World Models for Embodied AI</a></td>
  <td>å¯¹å…·èº«æ™ºèƒ½ä¸­ä¸–ç•Œæ¨¡å‹çš„å…¨é¢ç»¼è¿°ï¼Œæ¶µç›–åŠŸèƒ½ã€æ—¶åºå»ºæ¨¡å’Œç©ºé—´è¡¨ç¤ºä¸‰ä¸ªç»´åº¦ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16732v2" onclick="toggleFavorite(this, '2510.16732v2', 'A Comprehensive Survey on World Models for Embodied AI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251016776v1-emrrg-efficient-fine-tuning-pre-trained-x-ray-mamba-networks-for-rad.html">EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation</a></td>
  <td>EMRRGï¼šé«˜æ•ˆå¾®è°ƒé¢„è®­ç»ƒMamba Xå°„çº¿ç½‘ç»œï¼Œç”¨äºæ”¾å°„æŠ¥å‘Šç”Ÿæˆ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16776v1" onclick="toggleFavorite(this, '2510.16776v1', 'EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251017045v1-video-reasoning-without-training.html">Video Reasoning without Training</a></td>
  <td>æå‡ºV-Reasonï¼Œæ— éœ€è®­ç»ƒå³å¯æå‡å¤§æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17045v1" onclick="toggleFavorite(this, '2510.17045v1', 'Video Reasoning without Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251016888v3-uniworld-v2-reinforce-image-editing-with-diffusion-negative-aware-fi.html">Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback</a></td>
  <td>Uniworld-V2ï¼šåˆ©ç”¨æ‰©æ•£è´Ÿæ„ŸçŸ¥å¾®è°ƒå’ŒMLLMéšå¼åé¦ˆå¢å¼ºå›¾åƒç¼–è¾‘èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16888v3" onclick="toggleFavorite(this, '2510.16888v3', 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251017034v1-where-not-what-compelling-video-llms-to-learn-geometric-causality-fo.html">Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding</a></td>
  <td>æå‡ºW2R2æ¡†æ¶ï¼Œè§£å†³è§†é¢‘LLMä¸­3D groundingçš„2Dè¯­ä¹‰åè§é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17034v1" onclick="toggleFavorite(this, '2510.17034v1', 'Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251016714v2-scenecot-eliciting-grounded-chain-of-thought-reasoning-in-3d-scenes.html">SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes</a></td>
  <td>SceneCOTï¼šæå‡º3Dåœºæ™¯ä¸­åŸºäºå¸¸è¯†é“¾çš„æ¨ç†æ¡†æ¶ï¼Œæå‡å…·èº«é—®ç­”æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16714v2" onclick="toggleFavorite(this, '2510.16714v2', 'SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251016837v1-2dgs-r-revisiting-the-normal-consistency-regularization-in-2d-gaussi.html">2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting</a></td>
  <td>2DGS-Rï¼šé€šè¿‡åˆ†å±‚è®­ç»ƒå’ŒåŸä½å…‹éš†æå‡2Dé«˜æ–¯æº…å°„çš„æ¸²æŸ“è´¨é‡å’Œå‡ ä½•ç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16837v1" onclick="toggleFavorite(this, '2510.16837v1', '2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251016777v1-gs2pose-marry-gaussian-splatting-to-6d-object-pose-estimation.html">GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation</a></td>
  <td>GS2POSEï¼šç»“åˆé«˜æ–¯æº…å°„çš„6Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16777v1" onclick="toggleFavorite(this, '2510.16777v1', 'GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251017051v1-how-universal-are-sam2-features.html">How Universal Are SAM2 Features?</a></td>
  <td>é‡åŒ–é€šç”¨è§†è§‰æ¨¡å‹ä¸åˆ†å‰²ä¸“ç”¨æ¨¡å‹ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›å·®å¼‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.17051v1" onclick="toggleFavorite(this, '2510.17051v1', 'How Universal Are SAM2 Features?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251016709v2-humancm-one-step-human-motion-prediction.html">HumanCM: One Step Human Motion Prediction</a></td>
  <td>æå‡ºHumanCMï¼Œä¸€ç§åŸºäºä¸€è‡´æ€§æ¨¡å‹çš„äººä½“è¿åŠ¨å•æ­¥é¢„æµ‹æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16709v2" onclick="toggleFavorite(this, '2510.16709v2', 'HumanCM: One Step Human Motion Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)