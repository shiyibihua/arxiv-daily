---
layout: default
title: MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention
---

# MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention

**arXiv**: [2510.15448v1](https://arxiv.org/abs/2510.15448) | [PDF](https://arxiv.org/pdf/2510.15448.pdf)

**ä½œè€…**: Nengbo Zhang, Hann Woei Ho

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAVR-Netå¤šè§†å›¾å­¦ä¹ æ¡†æž¶ä»¥è§£å†³MAVåŠ¨ä½œè¯†åˆ«ä¸­RGBæ•°æ®ä¸è¶³çš„é—®é¢˜**

**å…³é”®è¯**: `å¤šè§†å›¾å­¦ä¹ ` `MAVåŠ¨ä½œè¯†åˆ«` `è·¨è§†å›¾æ³¨æ„åŠ›` `æ—¶ç©ºç‰¹å¾æå–` `å¤šæ¨¡æ€èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»…ä¾èµ–RGBæ•°æ®éš¾ä»¥æ•æ‰MAVå¤æ‚æ—¶ç©ºè¿åŠ¨ç‰¹å¾ï¼Œå¯¼è‡´åŠ¨ä½œè¯†åˆ«ç²¾åº¦å—é™
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆRGBã€å…‰æµå’Œåˆ†å‰²æŽ©ç ï¼Œé‡‡ç”¨è·¨è§†å›¾æ³¨æ„åŠ›æ¨¡å—å¢žå¼ºå¤šæ¨¡æ€äº¤äº’
3. å®žéªŒæ•ˆæžœï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå‡†ç¡®çŽ‡é«˜è¾¾97.8%ã€96.5%å’Œ92.8%ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for
> enabling cooperative perception and control in autonomous aerial swarms. Yet,
> vision-based recognition models relying only on RGB data often fail to capture
> the complex spatial temporal characteristics of MAV motion, which limits their
> ability to distinguish different actions. To overcome this problem, this paper
> presents MAVR-Net, a multi-view learning-based MAV action recognition
> framework. Unlike traditional single-view methods, the proposed approach
> combines three complementary types of data, including raw RGB frames, optical
> flow, and segmentation masks, to improve the robustness and accuracy of MAV
> motion recognition. Specifically, ResNet-based encoders are used to extract
> discriminative features from each view, and a multi-scale feature pyramid is
> adopted to preserve the spatiotemporal details of MAV motion patterns. To
> enhance the interaction between different views, a cross-view attention module
> is introduced to model the dependencies among various modalities and feature
> scales. In addition, a multi-view alignment loss is designed to ensure semantic
> consistency and strengthen cross-view feature representations. Experimental
> results on benchmark MAV action datasets show that our method clearly
> outperforms existing approaches, achieving 97.8\%, 96.5\%, and 92.8\% accuracy
> on the Short MAV, Medium MAV, and Long MAV datasets, respectively.

