---
layout: default
title: SHARE: Scene-Human Aligned Reconstruction
---

# SHARE: Scene-Human Aligned Reconstruction

**arXiv**: [2510.15342v1](https://arxiv.org/abs/2510.15342) | [PDF](https://arxiv.org/pdf/2510.15342.pdf)

**ä½œè€…**: Joshua Li, Brendan Chharawala, Chang Shu, Xue Bin Peng, Pengcheng Xi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSHAREæ–¹æ³•ï¼Œåˆ©ç”¨åœºæ™¯å‡ ä½•çº¿ç´¢æå‡å•ç›®è§†é¢‘ä¸­3Däººä½“è¿åŠ¨é‡å»ºçš„å‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `3Däººä½“é‡å»º` `åœºæ™¯å‡ ä½•å¯¹é½` `å•ç›®è§†é¢‘å¤„ç†` `è¿åŠ¨ä¼˜åŒ–` `è™šæ‹ŸçŽ¯å¢ƒäº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥åœ¨3Dç©ºé—´ä¸­å‡†ç¡®å®šä½äººä½“ï¼Œå½±å“è™šæ‹Ÿè§’è‰²ä¸ŽçŽ¯å¢ƒçš„äº¤äº’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¿­ä»£ä¼˜åŒ–å…³é”®å¸§äººä½“ç½‘æ ¼ä¸Žåœºæ™¯ç‚¹äº‘å¯¹é½ï¼Œå¹¶ä¿æŒéžå…³é”®å¸§ç›¸å¯¹ä½ç½®ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°æ®é›†å’Œé‡Žå¤–è§†é¢‘ä¸­ï¼ŒSHAREä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå®žçŽ°æ›´ç²¾ç¡®çš„3Däººä½“æ”¾ç½®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Animating realistic character interactions with the surrounding environment
> is important for autonomous agents in gaming, AR/VR, and robotics. However,
> current methods for human motion reconstruction struggle with accurately
> placing humans in 3D space. We introduce Scene-Human Aligned REconstruction
> (SHARE), a technique that leverages the scene geometry's inherent spatial cues
> to accurately ground human motion reconstruction. Each reconstruction relies
> solely on a monocular RGB video from a stationary camera. SHARE first estimates
> a human mesh and segmentation mask for every frame, alongside a scene point map
> at keyframes. It iteratively refines the human's positions at these keyframes
> by comparing the human mesh against the human point map extracted from the
> scene using the mask. Crucially, we also ensure that non-keyframe human meshes
> remain consistent by preserving their relative root joint positions to keyframe
> root joints during optimization. Our approach enables more accurate 3D human
> placement while reconstructing the surrounding scene, facilitating use cases on
> both curated datasets and in-the-wild web videos. Extensive experiments
> demonstrate that SHARE outperforms existing methods.

