---
layout: default
title: MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval
---

# MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval

**arXiv**: [2510.15470v1](https://arxiv.org/abs/2510.15470) | [PDF](https://arxiv.org/pdf/2510.15470.pdf)

**ä½œè€…**: Jinghao Huang, Yaxiong Chen, Ganchao Liu

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­ä¹‰è‡ªé€‚åº”æŒ–æ˜æ–¹æ³•ä»¥è§£å†³æ— äººæœºè§†é¢‘-æ–‡æœ¬æ£€ç´¢ä¸­çš„è¯­ä¹‰å»ºæ¨¡æŒ‘æˆ˜**

**å…³é”®è¯**: `æ— äººæœºè§†é¢‘æ£€ç´¢` `è·¨æ¨¡æ€å­¦ä¹ ` `å¤šè¯­ä¹‰æŒ–æ˜` `è‡ªé€‚åº”æœºåˆ¶` `ç‰¹å¾èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ— äººæœºè§†é¢‘å…·æœ‰ä¿¯è§†è§†è§’ã€ç»“æ„åŒè´¨æ€§å’Œç›®æ ‡ç»„åˆå¤šæ ·æ€§ï¼Œç°æœ‰è·¨æ¨¡æ€æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¤šè¯­ä¹‰è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€å¸§é—´å˜åŒ–å’ŒåŒºåŸŸè¯­ä¹‰æå–å¢å¼ºè§†é¢‘å†…å®¹ç†è§£ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨è‡ªå»ºæ•°æ®é›†ä¸Šå®éªŒè¡¨æ˜ï¼ŒMSAMä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ£€ç´¢æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the advancement of drone technology, the volume of video data increases
> rapidly, creating an urgent need for efficient semantic retrieval. We are the
> first to systematically propose and study the drone video-text retrieval (DVTR)
> task. Drone videos feature overhead perspectives, strong structural
> homogeneity, and diverse semantic expressions of target combinations, which
> challenge existing cross-modal methods designed for ground-level views in
> effectively modeling their characteristics. Therefore, dedicated retrieval
> mechanisms tailored for drone scenarios are necessary. To address this issue,
> we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
> introduces a multi-semantic adaptive learning mechanism, which incorporates
> dynamic changes between frames and extracts rich semantic information from
> specific scene regions, thereby enhancing the deep understanding and reasoning
> of drone video content. This method relies on fine-grained interactions between
> words and drone video frames, integrating an adaptive semantic construction
> module, a distribution-driven semantic learning term and a diversity semantic
> term to deepen the interaction between text and drone video modalities and
> improve the robustness of feature representation. To reduce the interference of
> complex backgrounds in drone videos, we introduce a cross-modal interactive
> feature fusion pooling mechanism that focuses on feature extraction and
> matching in target regions, minimizing noise effects. Extensive experiments on
> two self-constructed drone video-text datasets show that MSAM outperforms other
> existing methods in the drone video-text retrieval task. The source code and
> dataset will be made publicly available.

