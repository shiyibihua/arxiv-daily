---
layout: default
title: HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward
---

# HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward

**arXiv**: [2510.15679v1](https://arxiv.org/abs/2510.15679) | [PDF](https://arxiv.org/pdf/2510.15679.pdf)

**ä½œè€…**: Yuhong Cao, Yizhuo Wang, Jingsong Liang, Shuhao Liao, Yifeng Zhang, Peizhuo Li, Guillaume Sartoretti

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHEADERæ–¹æ³•ä»¥è§£å†³å¤§è§„æ¨¡çŽ¯å¢ƒä¸­æœºå™¨äººè‡ªä¸»æŽ¢ç´¢æ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººæŽ¢ç´¢` `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `å…¨å±€å›¾æž„å»º` `æŽ¢ç´¢æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è§„æ¨¡çŽ¯å¢ƒä¸‹çš„æœºå™¨äººè‡ªä¸»æŽ¢ç´¢æ•ˆçŽ‡ä¸Žå¯æ‰©å±•æ€§ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŸºäºŽæ³¨æ„åŠ›çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆç¤¾åŒºç®—æ³•æž„å»ºå…¨å±€å›¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žåœºæ™¯ä¸­ï¼ŒæŽ¢ç´¢æ•ˆçŽ‡æå‡è¾¾20%ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This work pushes the boundaries of learning-based methods in autonomous robot
> exploration in terms of environmental scale and exploration efficiency. We
> present HEADER, an attention-based reinforcement learning approach with
> hierarchical graphs for efficient exploration in large-scale environments.
> HEADER follows existing conventional methods to construct hierarchical
> representations for the robot belief/map, but further designs a novel
> community-based algorithm to construct and update a global graph, which remains
> fully incremental, shape-adaptive, and operates with linear complexity.
> Building upon attention-based networks, our planner finely reasons about the
> nearby belief within the local range while coarsely leveraging distant
> information at the global scale, enabling next-best-viewpoint decisions that
> consider multi-scale spatial dependencies. Beyond novel map representation, we
> introduce a parameter-free privileged reward that significantly improves model
> performance and produces near-optimal exploration behaviors, by avoiding
> training objective bias caused by handcrafted reward shaping. In simulated
> challenging, large-scale exploration scenarios, HEADER demonstrates better
> scalability than most existing learning and non-learning methods, while
> achieving a significant improvement in exploration efficiency (up to 20%) over
> state-of-the-art baselines. We also deploy HEADER on hardware and validate it
> in complex, large-scale real-life scenarios, including a 300m*230m campus
> environment.

