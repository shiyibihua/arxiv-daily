---
layout: default
title: FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers
---

# FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers

**arXiv**: [2510.15385v1](https://arxiv.org/abs/2510.15385) | [PDF](https://arxiv.org/pdf/2510.15385.pdf)

**ä½œè€…**: Haisheng Su, Junjie Zhang, Feixiang Song, Sanping Zhou, Wei Wu, Nanning Zheng, Junchi Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFreqPDEæ–¹æ³•ï¼Œé€šè¿‡é¢‘çŽ‡æ„ŸçŸ¥æ·±åº¦åµŒå…¥æ”¹è¿›å¤šè§†å›¾3Dç›®æ ‡æ£€æµ‹**

**å…³é”®è¯**: `å¤šè§†å›¾3Dç›®æ ‡æ£€æµ‹` `é¢‘çŽ‡æ„ŸçŸ¥æ·±åº¦åµŒå…¥` `è·¨è§†å›¾ä¸€è‡´æ€§` `å°ºåº¦ä¸å˜æ€§` `æ·±åº¦é¢„æµ‹` `Transformerè§£ç å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–æ·±åº¦é¢„æµ‹ï¼Œä½†å­˜åœ¨è¾¹ç•Œä¸è¿žç»­å’Œå°ç‰©ä½“æ¨¡ç³Šé—®é¢˜ï¼Œä¸”å¿½ç•¥è·¨è§†å›¾ä¸€è‡´æ€§å’Œå°ºåº¦ä¸å˜æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨é¢‘çŽ‡æ„ŸçŸ¥ç©ºé—´é‡‘å­—å¡”ç¼–ç å™¨ã€è·¨è§†å›¾å°ºåº¦ä¸å˜æ·±åº¦é¢„æµ‹å™¨å’Œä½ç½®æ·±åº¦ç¼–ç å™¨ç”Ÿæˆ3Dæ·±åº¦æ„ŸçŸ¥ç‰¹å¾ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨nuScenesæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œé€šè¿‡æ··åˆæ·±åº¦ç›‘ç£æå‡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Detecting 3D objects accurately from multi-view 2D images is a challenging
> yet essential task in the field of autonomous driving. Current methods resort
> to integrating depth prediction to recover the spatial information for object
> query decoding, which necessitates explicit supervision from LiDAR points
> during the training phase. However, the predicted depth quality is still
> unsatisfactory such as depth discontinuity of object boundaries and
> indistinction of small objects, which are mainly caused by the sparse
> supervision of projected points and the use of high-level image features for
> depth prediction. Besides, cross-view consistency and scale invariance are also
> overlooked in previous methods. In this paper, we introduce Frequency-aware
> Positional Depth Embedding (FreqPDE) to equip 2D image features with spatial
> information for 3D detection transformer decoder, which can be obtained through
> three main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder
> (FSPE) constructs a feature pyramid by combining high-frequency edge clues and
> low-frequency semantics from different levels respectively. Then the Cross-view
> Scale-invariant Depth Predictor (CSDP) estimates the pixel-level depth
> distribution with cross-view and efficient channel attention mechanism.
> Finally, the Positional Depth Encoder (PDE) combines the 2D image features and
> 3D position embeddings to generate the 3D depth-aware features for query
> decoding. Additionally, hybrid depth supervision is adopted for complementary
> depth learning from both metric and distribution aspects. Extensive experiments
> conducted on the nuScenes dataset demonstrate the effectiveness and superiority
> of our proposed method.

