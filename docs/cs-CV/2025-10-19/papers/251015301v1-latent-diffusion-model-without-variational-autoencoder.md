---
layout: default
title: Latent Diffusion Model without Variational Autoencoder
---

# Latent Diffusion Model without Variational Autoencoder

**arXiv**: [2510.15301v1](https://arxiv.org/abs/2510.15301) | [PDF](https://arxiv.org/pdf/2510.15301.pdf)

**ä½œè€…**: Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, Jiwen Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSVGæ¨¡åž‹ä»¥è§£å†³æ½œåœ¨æ‰©æ•£æ¨¡åž‹ä¸­VAEå¯¼è‡´çš„æ•ˆçŽ‡ä¸Žè¯­ä¹‰é—®é¢˜**

**å…³é”®è¯**: `æ½œåœ¨æ‰©æ•£æ¨¡åž‹` `è‡ªç›‘ç£è¡¨ç¤º` `è§†è§‰ç”Ÿæˆ` `è¯­ä¹‰ç©ºé—´` `é«˜æ•ˆè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVAEæ½œåœ¨ç©ºé—´ç¼ºä¹è¯­ä¹‰åˆ†ç¦»å’Œåˆ¤åˆ«ç»“æž„ï¼Œå½±å“è®­ç»ƒæ•ˆçŽ‡å’Œä»»åŠ¡è¿ç§»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å†»ç»“DINOç‰¹å¾æž„å»ºè¯­ä¹‰ç©ºé—´ï¼Œè½»é‡æ®‹å·®åˆ†æ”¯è¡¥å……ç»†èŠ‚ï¼Œç›´æŽ¥è®­ç»ƒæ‰©æ•£æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šSVGåŠ é€Ÿè®­ç»ƒã€æ”¯æŒå°‘æ­¥é‡‡æ ·ã€æå‡ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¿ç•™è¯­ä¹‰åˆ¤åˆ«èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in diffusion-based visual generation has largely relied on
> latent diffusion models with variational autoencoders (VAEs). While effective
> for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited
> training efficiency, slow inference, and poor transferability to broader vision
> tasks. These issues stem from a key limitation of VAE latent spaces: the lack
> of clear semantic separation and strong discriminative structure. Our analysis
> confirms that these properties are crucial not only for perception and
> understanding tasks, but also for the stable and efficient training of latent
> diffusion models. Motivated by this insight, we introduce SVG, a novel latent
> diffusion model without variational autoencoders, which leverages
> self-supervised representations for visual generation. SVG constructs a feature
> space with clear semantic discriminability by leveraging frozen DINO features,
> while a lightweight residual branch captures fine-grained details for
> high-fidelity reconstruction. Diffusion models are trained directly on this
> semantically structured latent space to facilitate more efficient learning. As
> a result, SVG enables accelerated diffusion training, supports few-step
> sampling, and improves generative quality. Experimental results further show
> that SVG preserves the semantic and discriminative capabilities of the
> underlying self-supervised representations, providing a principled pathway
> toward task-general, high-quality visual representations.

