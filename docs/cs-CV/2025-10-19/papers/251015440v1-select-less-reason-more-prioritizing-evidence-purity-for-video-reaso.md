---
layout: default
title: Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning
---

# Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning

**arXiv**: [2510.15440v1](https://arxiv.org/abs/2510.15440) | [PDF](https://arxiv.org/pdf/2510.15440.pdf)

**ä½œè€…**: Xuchen Li, Xuzhao Li, Shiyu Hu, Kaiqi Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯æ®ä¼˜å…ˆè‡ªé€‚åº”æ¡†æž¶ä»¥è§£å†³é•¿è§†é¢‘æŽ¨ç†ä¸­çš„ä¿¡æ¯ç¨€é‡Šé—®é¢˜**

**å…³é”®è¯**: `é•¿è§†é¢‘æŽ¨ç†` `è¯æ®æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”å¸§é€‰æ‹©` `è§†é¢‘å¤§è¯­è¨€æ¨¡åž‹` `å±€éƒ¨é‡é‡‡æ ·`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿è§†é¢‘æŽ¨ç†ä¸­å‡åŒ€å¸§é‡‡æ ·å¯¼è‡´ä¿¡æ¯ç¨€é‡Šï¼ŒçŽ°æœ‰æ–¹æ³•ç¼ºä¹è¯æ®çº¯åº¦å’Œæ—¶é—´è¡¥å……æœºåˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è¯æ®æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼ŒåŠ¨æ€é€‰æ‹©å…³é”®å¸§å¹¶è¿›è¡Œå±€éƒ¨é‡é‡‡æ ·ä»¥èŽ·å–ç»†èŠ‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°å¼€æºè§†é¢‘å¤§è¯­è¨€æ¨¡åž‹çš„æ–°æœ€ä¼˜æ€§èƒ½ï¼Œæå‡æŽ¨ç†å‡†ç¡®çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-form video reasoning remains a major challenge for Video Large Language
> Models (Video LLMs), as static uniform frame sampling leads to information
> dilution and obscures critical evidence. Furthermore, existing pixel-space
> video reasoning agents, which are designed to actively interact with the video
> to acquire new visual information, remain suboptimal due to their lack of
> rigorous reward mechanisms to enforce evidence purity and their inability to
> perform temporal information supplementation beyond pre-sampled frames. To
> address this critical gap, we propose a novel evidence-prioritized adaptive
> framework built upon our core philosophy: "Select Less, Reason More." Our core
> contribution is the evidence-aware reinforcement learning (EARL) framework,
> which transforms the model into an active interrogator of evidence. EARL is
> precisely engineered to dynamically select the most relevant frames and,
> crucially, to perform localized re-sampling around the selected key frames to
> access fine-grained temporal detail. Extensive experiments on five demanding
> video reasoning benchmarks demonstrate that our EARL-trained model achieves new
> state-of-the-art among open-source Video LLMs, simultaneously learning an
> effective and high-purity visual evidence selection policy. Impressively, our
> 7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on
> VideoMME. These results highlight the importance of prioritizing evidence
> purity and the effectiveness of our framework.

