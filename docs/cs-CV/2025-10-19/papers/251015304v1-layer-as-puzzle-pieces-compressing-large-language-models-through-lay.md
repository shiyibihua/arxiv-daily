---
layout: default
title: Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation
---

# Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation

**arXiv**: [2510.15304v1](https://arxiv.org/abs/2510.15304) | [PDF](https://arxiv.org/pdf/2510.15304.pdf)

**ä½œè€…**: Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoMeæ–¹æ³•ï¼Œé€šè¿‡å±‚æ‹¼æŽ¥åŽ‹ç¼©å¤§è¯­è¨€æ¨¡åž‹ä»¥é™ä½Žè®¡ç®—å­˜å‚¨éœ€æ±‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹åŽ‹ç¼©` `ç»“æž„åŒ–å‰ªæž` `å±‚æ‹¼æŽ¥åˆå¹¶` `åˆ†å±‚è’¸é¦` `é€šé“æ•æ„Ÿåº¦` `åŽè®­ç»ƒæ¢å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹å°ºå¯¸å¤§å¯¼è‡´é«˜è®¡ç®—å­˜å‚¨æˆæœ¬ï¼ŒçŽ°æœ‰ç»“æž„åŒ–å‰ªæžæ–¹æ³•å¿½ç•¥ä¿ç•™å‰ªæžéƒ¨åˆ†èƒ½åŠ›
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨é€šé“æ•æ„Ÿåº¦æŒ‡æ ‡å’Œå±‚æ‹¼æŽ¥åˆå¹¶æŠ€æœ¯ï¼Œç»“åˆåˆ†å±‚è’¸é¦åŽè®­ç»ƒè¿‡ç¨‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå‰ªæž30%å‚æ•°çš„LLaMA-2-7bæ¨¡åž‹ä¿æŒ83%åŽŸå§‹å¹³å‡å‡†ç¡®çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models excel at natural language processing tasks, but their
> massive size leads to high computational and storage demands. Recent works have
> sought to reduce their model size through layer-wise structured pruning.
> However, they tend to ignore retaining the capabilities in the pruned part. In
> this work, we re-examine structured pruning paradigms and uncover several key
> limitations: 1) notable performance degradation due to direct layer removal, 2)
> incompetent linear weight layer aggregation, and 3) the lack of effective
> post-training recovery mechanisms. To address these limitations, we propose
> CoMe, including a progressive layer pruning framework with a
> Concatenation-based Merging technology and a hierarchical distillation
> post-training process. Specifically, we introduce a channel sensitivity metric
> that utilizes activation intensity and weight norms for fine-grained channel
> selection. Subsequently, we employ a concatenation-based layer merging method
> to fuse the most critical channels across adjacent layers, enabling progressive
> model size reduction. Finally, we propose a hierarchical distillation protocol
> that leverages the correspondences between the original and pruned model layers
> established during pruning, thereby enabling efficient knowledge transfer.
> Experiments on seven benchmarks show that CoMe achieves state-of-the-art
> performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model
> retains 83% of its original average accuracy. Our code is available at
> https://github.com/MPI-Lab/CoMe.

