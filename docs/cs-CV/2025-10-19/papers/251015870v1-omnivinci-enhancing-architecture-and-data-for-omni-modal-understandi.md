---
layout: default
title: OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM
---

# OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM

**arXiv**: [2510.15870v1](https://arxiv.org/abs/2510.15870) | [PDF](https://arxiv.org/pdf/2510.15870.pdf)

**ä½œè€…**: Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniVinciæž¶æž„ä¸Žæ•°æ®å¢žå¼ºæ–¹æ³•ï¼Œæå‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `æ¨¡åž‹æž¶æž„ä¼˜åŒ–` `æ•°æ®åˆæˆ` `è·¨æ¨¡æ€å¯¹é½` `æ—¶é—´åµŒå…¥` `å¼€æºLLM`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨éœ€è·¨æ¨¡æ€æ„ŸçŸ¥ï¼Œå¦‚äººç±»å¤šæ„Ÿå®˜èžåˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥OmniAlignNetã€Temporal Embedding Groupingå’ŒConstrained Rotary Time Embeddingã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶ŠQwen2.5-Omniï¼Œè®­ç»ƒæ•°æ®å‡å°‘6å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Advancing machine intelligence requires developing the ability to perceive
> across multiple modalities, much as humans sense the world. We introduce
> OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
> carefully study the design choices across model architecture and data curation.
> For model architecture, we present three key innovations: (i) OmniAlignNet for
> strengthening alignment between vision and audio embeddings in a shared
> omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
> relative temporal alignment between vision and audio signals; and (iii)
> Constrained Rotary Time Embedding for encoding absolute temporal information in
> omni-modal embeddings. We introduce a curation and synthesis pipeline that
> generates 24M single-modal and omni-modal conversations. We find that
> modalities reinforce one another in both perception and reasoning. Our model,
> OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
> understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
> using just 0.2T training tokens - a 6 times reduction compared to
> Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
> applications spanning robotics, medical AI, and smart factory.

