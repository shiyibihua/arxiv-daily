---
layout: default
title: BLIP3o-NEXT: Next Frontier of Native Image Generation
---

# BLIP3o-NEXT: Next Frontier of Native Image Generation

**arXiv**: [2510.15857v1](https://arxiv.org/abs/2510.15857) | [PDF](https://arxiv.org/pdf/2510.15857.pdf)

**ä½œè€…**: Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBLIP3o-NEXTç»Ÿä¸€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸Žç¼–è¾‘ï¼Œå®žçŽ°é«˜æ•ˆæŽ¨ç†å’Œé«˜ä¿çœŸå›¾åƒã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å›¾åƒç¼–è¾‘` `è‡ªå›žå½’æ¨¡åž‹` `æ‰©æ•£æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®è´¨é‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒç¼–è¾‘ä»»åŠ¡ä»å…·æŒ‘æˆ˜ï¼Œéœ€æå‡æŒ‡ä»¤éµå¾ªå’Œå›¾åƒä¸€è‡´æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è‡ªå›žå½’+æ‰©æ•£æž¶æž„ï¼Œç»“åˆæŽ¨ç†èƒ½åŠ›å’Œç»†èŠ‚æ¸²æŸ“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ï¼Œå±•ç¤ºä¼˜è¶Šæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
> series that advances the next frontier of native image generation. BLIP3o-NEXT
> unifies text-to-image generation and image editing within a single
> architecture, demonstrating strong image generation and image editing
> capabilities. In developing the state-of-the-art native image generation model,
> we identify four key insights: (1) Most architectural choices yield comparable
> performance; an architecture can be deemed effective provided it scales
> efficiently and supports fast inference; (2) The successful application of
> reinforcement learning can further push the frontier of native image
> generation; (3) Image editing still remains a challenging task, yet instruction
> following and the consistency between generated and reference images can be
> significantly enhanced through post-training and data engine; (4) Data quality
> and scale continue to be decisive factors that determine the upper bound of
> model performance. Building upon these insights, BLIP3o-NEXT leverages an
> Autoregressive + Diffusion architecture in which an autoregressive model first
> generates discrete image tokens conditioned on multimodal inputs, whose hidden
> states are then used as conditioning signals for a diffusion model to generate
> high-fidelity images. This architecture integrates the reasoning strength and
> instruction following of autoregressive models with the fine-detail rendering
> ability of diffusion models, achieving a new level of coherence and realism.
> Extensive evaluations of various text-to-image and image-editing benchmarks
> show that BLIP3o-NEXT achieves superior performance over existing models.

