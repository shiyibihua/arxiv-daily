---
layout: default
title: VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving
---

# VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving

**arXiv**: [2510.15446v1](https://arxiv.org/abs/2510.15446) | [PDF](https://arxiv.org/pdf/2510.15446.pdf)

**ä½œè€…**: Ziang Guo, Zufeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVDRiveç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ–¹æ³•ï¼Œç»“åˆVLAå’Œæ‰©æ•£ç­–ç•¥æå‡é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æ‰©æ•£ç­–ç•¥` `å¼ºåŒ–å­¦ä¹ ` `çŠ¶æ€å»ºæ¨¡` `é—­çŽ¯è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ¨æ€çŽ¯å¢ƒå’Œæžç«¯æ¡ˆä¾‹æŒ‘æˆ˜è‡ªåŠ¨é©¾é©¶çš„çŠ¶æ€ç†è§£å’Œå†³ç­–é²æ£’æ€§ã€‚
2. åˆ©ç”¨VLAè¿›è¡ŒçŠ¶æ€å»ºæ¨¡å’Œæ‰©æ•£ç­–ç•¥ç”ŸæˆåŠ¨ä½œï¼Œå®žçŽ°ä¸Šä¸‹æ–‡å’Œå‡ ä½•å¼•å¯¼ã€‚
3. åœ¨Bench2Driveå’ŒnuScenesåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In autonomous driving, dynamic environment and corner cases pose significant
> challenges to the robustness of ego vehicle's state understanding and decision
> making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving
> that explicitly models state-action mapping to address these challenges,
> enabling interpretable and robust decision making. By leveraging the
> advancement of the state understanding of the Vision Language Action Model
> (VLA) with generative diffusion policy-based action head, our VDRive guides the
> driving contextually and geometrically. Contextually, VLA predicts future
> observations through token generation pre-training, where the observations are
> represented as discrete codes by a Conditional Vector Quantized Variational
> Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning
> fine-tuning of the VLA to predict future trajectories and actions based on
> current driving conditions. VLA supplies the current state tokens and predicted
> state tokens for the action policy head to generate hierarchical actions and
> trajectories. During policy training, a learned critic evaluates the actions
> generated by the policy and provides gradient-based feedback, forming an
> actor-critic framework that enables a reinforcement-based policy learning
> pipeline. Experiments show that our VDRive achieves state-of-the-art
> performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop
> planning.

