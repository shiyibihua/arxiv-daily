---
layout: default
title: Towards Robust Zero-Shot Reinforcement Learning
---

# Towards Robust Zero-Shot Reinforcement Learning

**arXiv**: [2510.15382v1](https://arxiv.org/abs/2510.15382) | [PDF](https://arxiv.org/pdf/2510.15382.pdf)

**ä½œè€…**: Kexin Zheng, Lauriane Teyssier, Yinan Zheng, Yu Luo, Xiayuan Zhan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBREEZEä»¥è§£å†³é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨è¾¾æ€§ä¸è¶³å’Œåˆ†å¸ƒå¤–åŠ¨ä½œåå·®é—®é¢˜**

**å…³é”®è¯**: `é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ ` `è¡Œä¸ºæ­£åˆ™åŒ–` `æ‰©æ•£æ¨¡åž‹` `è¡¨ç¤ºå­¦ä¹ ` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ ä¸­FBæ–¹æ³•è¡¨è¾¾æ€§ä¸è¶³ï¼Œåˆ†å¸ƒå¤–åŠ¨ä½œå¯¼è‡´è¡¨ç¤ºåå·®å’Œæ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è¡Œä¸ºæ­£åˆ™åŒ–ç¨³å®šç­–ç•¥å­¦ä¹ ï¼Œä½¿ç”¨ä»»åŠ¡æ¡ä»¶æ‰©æ•£æ¨¡åž‹æå–é«˜è´¨é‡å¤šæ¨¡æ€ç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ExORLå’ŒD4RL Kitchenæ•°æ®é›†ä¸Šè¡¨çŽ°æœ€ä½³æˆ–æŽ¥è¿‘æœ€ä½³ï¼Œé²æ£’æ€§ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The recent development of zero-shot reinforcement learning (RL) has opened a
> new avenue for learning pre-trained generalist policies that can adapt to
> arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
> representations (FB) and related methods have shown promise in zero-shot RL, we
> empirically found that their modeling lacks expressivity and that extrapolation
> errors caused by out-of-distribution (OOD) actions during offline learning
> sometimes lead to biased representations, ultimately resulting in suboptimal
> performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
> RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
> simultaneously enhances learning stability, policy extraction capability, and
> representation learning quality. BREEZE introduces behavioral regularization in
> zero-shot RL policy learning, transforming policy optimization into a stable
> in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
> task-conditioned diffusion model, enabling the generation of high-quality and
> multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
> employs expressive attention-based architectures for representation modeling to
> capture the complex relationships between environmental dynamics. Extensive
> experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
> or near-the-best performance while exhibiting superior robustness compared to
> prior offline zero-shot RL methods. The official implementation is available
> at: https://github.com/Whiterrrrr/BREEZE.

