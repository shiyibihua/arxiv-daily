---
layout: default
title: VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation
---

# VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation

**arXiv**: [2510.15530v1](https://arxiv.org/abs/2510.15530) | [PDF](https://arxiv.org/pdf/2510.15530.pdf)

**ä½œè€…**: Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVO-DPæ–¹æ³•ä»¥è§£å†³ä»…è§†è§‰æœºå™¨äººæ“ä½œä¸­çš„è¯­ä¹‰-å‡ ä½•ç‰¹å¾èžåˆé—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ‰©æ•£ç­–ç•¥` `è§†è§‰åŸºç¡€æ¨¡åž‹` `ç‰¹å¾èžåˆ` `é²æ£’æ€§è¯„ä¼°` `å¼€æºè®­ç»ƒåº“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–ç‚¹äº‘ï¼Œç¼ºä¹å¯¹ä»…è§†è§‰æ–¹æ¡ˆçš„æ·±å…¥æŽ¢ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡åž‹èžåˆè¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å’ŒCNNåŽ‹ç¼©è¾“å…¥ç­–ç•¥å¤´ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žä»»åŠ¡ä¸­ï¼ŒVO-DPæ€§èƒ½ä¼˜äºŽåŸºçº¿ï¼Œå¹¶å±•ç¤ºé«˜é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In the context of imitation learning, visuomotor-based diffusion policy
> learning is one of the main directions in robotic manipulation. Most of these
> approaches rely on point clouds as observation inputs and construct scene
> representations through point clouds feature learning, which enables them to
> achieve remarkable accuracy. However, the existing literature lacks an in-depth
> exploration of vision-only solutions that have significant potential. In this
> paper, we propose a Vision-Only and single-view Diffusion Policy learning
> method (VO-DP) that leverages pretrained visual foundation models to achieve
> effective fusion of semantic and geometric features. We utilize intermediate
> features from VGGT incorporating semantic features from DINOv2 and geometric
> features from Alternating Attention blocks. Features are fused via
> cross-attention and spatially compressed with a CNN to form the input to the
> policy head. Extensive experiments demonstrate that VO-DP not only outperforms
> the vision-only baseline DP significantly but also exhibits distinct
> performance trends against the point cloud-based method DP3: in simulation
> tasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%
> and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,
> outperforming both DP3 67.5% and DP 11.2% by a notable margin. Further
> robustness evaluations confirm that VO-DP remains highly stable under varying
> conditions including color, size, background, and lighting. Lastly, we
> open-source a training library for robotic manipulation. Built on Accelerate,
> this library supports multi-machine and multi-GPU parallel training, as well as
> mixed precision training. It is compatible with visuomotor policies such as DP,
> DP3 and VO-DP, and also supports the RoboTwin simulator.

