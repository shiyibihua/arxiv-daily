---
layout: default
title: Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation
---

# Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation

**arXiv**: [2510.15466v1](https://arxiv.org/abs/2510.15466) | [PDF](https://arxiv.org/pdf/2510.15466.pdf)

**ä½œè€…**: Vu Tram Anh Khuong, Luu Tu Nguyen, Thanh Ha Le, Thi Duyen Ngo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›¸ä½æ„ŸçŸ¥æ—¶åºå¢žå¼ºæ–¹æ³•ä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­æ•°æ®ç¨€ç¼ºé—®é¢˜**

**å…³é”®è¯**: `å¾®è¡¨æƒ…è¯†åˆ«` `æ—¶åºå¢žå¼º` `åŠ¨æ€å›¾åƒ` `ç›¸ä½åˆ†è§£` `æ•°æ®ç¨€ç¼º` `æ¨¡åž‹æ— å…³æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¾®è¡¨æƒ…æ•°æ®é›†ç¨€ç¼ºé™åˆ¶æ¨¡åž‹æ³›åŒ–å’Œè¿åŠ¨æ¨¡å¼å¤šæ ·æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è¡¨æƒ…åºåˆ—åˆ†è§£ä¸ºèµ·å§‹åˆ°é¡¶ç‚¹å’Œé¡¶ç‚¹åˆ°ç»“æŸä¸¤é˜¶æ®µï¼Œç”ŸæˆåŒé˜¶æ®µåŠ¨æ€å›¾åƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CASME-IIå’ŒSAMMæ•°æ®é›†ä¸Šï¼Œç»“åˆç©ºé—´å¢žå¼ºå®žçŽ°æœ€é«˜10%å‡†ç¡®çŽ‡æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Micro-expressions (MEs) are brief, involuntary facial movements that reveal
> genuine emotions, typically lasting less than half a second. Recognizing these
> subtle expressions is critical for applications in psychology, security, and
> behavioral analysis. Although deep learning has enabled significant advances in
> micro-expression recognition (MER), its effectiveness is limited by the
> scarcity of annotated ME datasets. This data limitation not only hinders
> generalization but also restricts the diversity of motion patterns captured
> during training. Existing MER studies predominantly rely on simple spatial
> augmentations (e.g., flipping, rotation) and overlook temporal augmentation
> strategies that can better exploit motion characteristics. To address this gap,
> this paper proposes a phase-aware temporal augmentation method based on dynamic
> image. Rather than encoding the entire expression as a single onset-to-offset
> dynamic image (DI), our approach decomposes each expression sequence into two
> motion phases: onset-to-apex and apex-to-offset. A separate DI is generated for
> each phase, forming a Dual-phase DI augmentation strategy. These phase-specific
> representations enrich motion diversity and introduce complementary temporal
> cues that are crucial for recognizing subtle facial transitions. Extensive
> experiments on CASME-II and SAMM datasets using six deep architectures,
> including CNNs, Vision Transformer, and the lightweight LEARNet, demonstrate
> consistent performance improvements in recognition accuracy, unweighted
> F1-score, and unweighted average recall, which are crucial for addressing class
> imbalance in MER. When combined with spatial augmentations, our method achieves
> up to a 10\% relative improvement. The proposed augmentation is simple,
> model-agnostic, and effective in low-resource settings, offering a promising
> direction for robust and generalizable MER.

