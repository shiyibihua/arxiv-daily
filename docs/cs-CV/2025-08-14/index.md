---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-14
---

# cs.CVï¼ˆ2025-08-14ï¼‰

ğŸ“Š å…± **7** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250810294v1-a-sub-pixel-multimodal-optical-remote-sensing-images-matching-method.html">A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method</a></td>
  <td>æå‡ºPCWLADæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å…‰å­¦å›¾åƒåŒ¹é…ç²¾åº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10294v1" data-paper-url="./papers/250810294v1-a-sub-pixel-multimodal-optical-remote-sensing-images-matching-method.html" onclick="toggleFavorite(this, '2508.10294v1', 'A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250810264v2-mrfd-multi-region-fusion-decoding-with-self-consistency-for-mitigati.html">MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs</a></td>
  <td>æå‡ºMRFDä»¥è§£å†³LVLMä¸­å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10264v2" data-paper-url="./papers/250810264v2-mrfd-multi-region-fusion-decoding-with-self-consistency-for-mitigati.html" onclick="toggleFavorite(this, '2508.10264v2', 'MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250810256v2-deep-learning-for-crack-detection-a-review-of-learning-paradigms-gen.html">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></td>
  <td>ç»¼è¿°æ·±åº¦å­¦ä¹ åœ¨è£‚ç¼æ£€æµ‹ä¸­çš„åº”ç”¨ä¸å‘å±•è¶‹åŠ¿</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10256v2" data-paper-url="./papers/250810256v2-deep-learning-for-crack-detection-a-review-of-learning-paradigms-gen.html" onclick="toggleFavorite(this, '2508.10256v2', 'Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>4</td>
  <td><a href="./papers/250810281v1-vifss-view-invariant-and-figure-skating-specific-pose-representation.html">VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation</a></td>
  <td>æå‡ºVIFSSä»¥è§£å†³èŠ±æ ·æ»‘å†°åŠ¨ä½œåˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10281v1" data-paper-url="./papers/250810281v1-vifss-view-invariant-and-figure-skating-specific-pose-representation.html" onclick="toggleFavorite(this, '2508.10281v1', 'VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250810316v2-integrating-reinforcement-learning-with-visual-generative-models-fou.html">Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances</a></td>
  <td>å°†å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10316v2" data-paper-url="./papers/250810316v2-integrating-reinforcement-learning-with-visual-generative-models-fou.html" onclick="toggleFavorite(this, '2508.10316v2', 'Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>6</td>
  <td><a href="./papers/250810297v1-intersyn-interleaved-learning-for-dynamic-motion-synthesis-in-the-wi.html">InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild</a></td>
  <td>æå‡ºInterSynä»¥è§£å†³åŠ¨æ€è¿åŠ¨åˆæˆä¸­çš„äº¤äº’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion synthesis</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10297v1" data-paper-url="./papers/250810297v1-intersyn-interleaved-learning-for-dynamic-motion-synthesis-in-the-wi.html" onclick="toggleFavorite(this, '2508.10297v1', 'InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/250810287v2-jrdb-reasoning-a-difficulty-graded-benchmark-for-visual-reasoning-in.html">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></td>
  <td>æå‡ºJRDB-Reasoningä»¥è§£å†³è§†è§‰æ¨ç†åŸºå‡†çš„å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10287v2" data-paper-url="./papers/250810287v2-jrdb-reasoning-a-difficulty-graded-benchmark-for-visual-reasoning-in.html" onclick="toggleFavorite(this, '2508.10287v2', 'JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)