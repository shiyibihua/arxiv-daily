---
layout: default
title: MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs
---

# MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10264" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10264v2</a>
  <a href="https://arxiv.org/pdf/2508.10264.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10264v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10264v2', 'MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haonan Ge, Yiwei Wang, Ming-Hsuan Yang, Yujun Cai

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-14 (æ›´æ–°: 2025-10-13)

**å¤‡æ³¨**: EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMRFDä»¥è§£å†³LVLMä¸­å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€ä»»åŠ¡` `å¹»è§‰é—®é¢˜` `åŒºåŸŸèåˆ` `ä¸€è‡´æ€§å»ºæ¨¡` `äº¤å‰æ³¨æ„åŠ›` `äº‹å®åŸºç¡€` `å“åº”ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶ï¼Œå¸¸å¸¸äº§ç”Ÿä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´çš„å¹»è§‰ï¼Œå½±å“å…¶å®é™…åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„å¤šåŒºåŸŸèåˆè§£ç ï¼ˆMRFDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡åŒºåŸŸé—´çš„ä¸€è‡´æ€§ï¼Œæå‡äº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„éªŒè¯èƒ½åŠ›ï¼Œè§£å†³äº†å¹»è§‰é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMRFDåœ¨å¤šä¸ªLVLMå’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†å¹»è§‰ç°è±¡ï¼Œæé«˜äº†å“åº”çš„äº‹å®æ€§ï¼Œä¸”æ— éœ€å¯¹æ¨¡å‹è¿›è¡Œæ›´æ–°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸äº§ç”Ÿä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´çš„å¹»è§‰ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šåŒºåŸŸèåˆè§£ç ï¼ˆMRFDï¼‰çš„æ— è®­ç»ƒè§£ç æ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡åŒºåŸŸé—´ä¸€è‡´æ€§æ¥æ”¹å–„äº‹å®åŸºç¡€ã€‚MRFDåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›è¯†åˆ«æ˜¾è‘—åŒºåŸŸï¼Œä¸ºæ¯ä¸ªåŒºåŸŸç”Ÿæˆåˆå§‹å“åº”ï¼Œå¹¶åŸºäºå“åº”ä¹‹é—´çš„è©¹æ£®-é¦™å†œæ•£åº¦ï¼ˆJSDï¼‰è®¡ç®—å¯é æ€§æƒé‡ã€‚è¿™äº›æƒé‡å¼•å¯¼åŒºåŸŸæ„ŸçŸ¥çš„é¢„æµ‹èåˆï¼Œçµæ„Ÿæ¥è‡ªäºé“¾å¼æ€ç»´æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMRFDæ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œæé«˜äº†å“åº”çš„äº‹å®æ€§ï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œæ›´æ–°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨éªŒè¯å›¾åƒä¸åŒåŒºåŸŸä¿¡æ¯æ—¶èƒ½åŠ›æœ‰é™ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMRFDé€šè¿‡å»ºæ¨¡åŒºåŸŸé—´çš„ä¸€è‡´æ€§æ¥æ”¹å–„äº‹å®åŸºç¡€ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›è¯†åˆ«æ˜¾è‘—åŒºåŸŸï¼Œå¹¶ä¸ºæ¯ä¸ªåŒºåŸŸç”Ÿæˆåˆå§‹å“åº”ï¼Œè®¡ç®—å“åº”çš„å¯é æ€§æƒé‡ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„æ–‡æœ¬ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMRFDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«å›¾åƒä¸­çš„æ˜¾è‘—åŒºåŸŸï¼›å…¶æ¬¡ï¼Œä¸ºæ¯ä¸ªåŒºåŸŸç”Ÿæˆåˆå§‹æ–‡æœ¬å“åº”ï¼›æœ€åï¼ŒåŸºäºJSDè®¡ç®—çš„æƒé‡è¿›è¡ŒåŒºåŸŸæ„ŸçŸ¥çš„å“åº”èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šMRFDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†åŒºåŸŸé—´ä¸€è‡´æ€§å»ºæ¨¡å’ŒåŸºäºJSDçš„æƒé‡è®¡ç®—ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç›´æ¥ç”Ÿæˆæ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMRFDä½¿ç”¨äº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥è¯†åˆ«åŒºåŸŸï¼Œé‡‡ç”¨JSDä½œä¸ºå¯é æ€§æƒé‡çš„è®¡ç®—ä¾æ®ï¼Œå¹¶ç»“åˆåŒºåŸŸæ„ŸçŸ¥çš„æç¤ºè¿›è¡Œå“åº”èåˆï¼Œç¡®ä¿ç”Ÿæˆçš„æ–‡æœ¬ä¸è§†è§‰ä¿¡æ¯é«˜åº¦ä¸€è‡´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMRFDåœ¨å¤šä¸ªLVLMå’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†å¹»è§‰ç°è±¡ï¼Œå“åº”çš„äº‹å®æ€§æé«˜äº†çº¦20%ï¼Œä¸”åœ¨ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•æ›´æ–°çš„æƒ…å†µä¸‹å®ç°äº†è¿™äº›æ”¹è¿›ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆä»¥åŠå¤šæ¨¡æ€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡å‡å°‘å¹»è§‰ç°è±¡ï¼ŒMRFDèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have shown strong performance across multimodal tasks. However, they often produce hallucinations -- text that is inconsistent with visual input, due to the limited ability to verify information in different regions of the image. To address this, we propose Multi-Region Fusion Decoding (MRFD), a training-free decoding method that improves factual grounding by modeling inter-region consistency. MRFD identifies salient regions using cross-attention, generates initial responses for each, and computes reliability weights based on Jensen-Shannon Divergence (JSD) among the responses. These weights guide a consistency-aware fusion of per-region predictions, using region-aware prompts inspired by Chain-of-Thought reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD significantly reduces hallucinations and improves response factuality without requiring model updates.

