---
layout: default
title: InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild
---

# InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10297" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10297v1</a>
  <a href="https://arxiv.org/pdf/2508.10297.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10297v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10297v1', 'InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiyi Ma, Yuanzhi Liang, Xiu Li, Chi Zhang, Xuelong Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-14

**å¤‡æ³¨**: Accepted by ICCV2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInterSynä»¥è§£å†³åŠ¨æ€è¿åŠ¨åˆæˆä¸­çš„äº¤äº’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `åŠ¨æ€è¿åŠ¨åˆæˆ` `äº¤äº’å­¦ä¹ ` `å¤šè§’è‰²äº¤äº’` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºåŠ¨ç”»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸å°†å•äººå’Œå¤šäººåŠ¨æ€åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ä½œç¼ºä¹çœŸå®æ„Ÿå’Œåè°ƒæ€§ã€‚
2. InterSyné€šè¿‡äº¤é”™å­¦ä¹ ç­–ç•¥ï¼Œè”åˆå»ºæ¨¡å•äººå’Œäº¤äº’è¡Œä¸ºï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰è‡ªç„¶çš„åŠ¨æ€äº¤äº’ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInterSynåœ¨æ–‡æœ¬ä¸åŠ¨ä½œçš„å¯¹é½åº¦å’Œå¤šæ ·æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†è¿åŠ¨åˆæˆçš„äº¤é”™å­¦ä¹ æ¡†æ¶InterSynï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ ç»¼åˆè€ƒè™‘å•äººå’Œå¤šäººåŠ¨æ€çš„è¿åŠ¨ç”ŸæˆçœŸå®çš„äº¤äº’åŠ¨ä½œã€‚ä¸ä»¥å¾€å°†è¿™äº›ç»„ä»¶åˆ†å¼€å¤„ç†çš„æ–¹æ³•ä¸åŒï¼ŒInterSyné‡‡ç”¨äº¤é”™å­¦ä¹ ç­–ç•¥ï¼Œä»¥æ•æ‰ç°å®åœºæ™¯ä¸­è‡ªç„¶çš„åŠ¨æ€äº¤äº’å’Œç»†å¾®çš„åè°ƒã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šäº¤é”™äº¤äº’åˆæˆæ¨¡å—ï¼ˆINSï¼‰ï¼Œä»ç¬¬ä¸€äººç§°è§†è§’ç»Ÿä¸€å»ºæ¨¡å•äººå’Œäº¤äº’è¡Œä¸ºï¼Œä»¥æ”¯æŒå¤šè§’è‰²äº¤äº’ï¼›ç›¸å¯¹åè°ƒç²¾ç‚¼æ¨¡å—ï¼ˆRECï¼‰ï¼Œç²¾ç‚¼ç›¸äº’åŠ¨æ€ï¼Œç¡®ä¿è§’è‰²ä¹‹é—´çš„åŒæ­¥åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInterSynç”Ÿæˆçš„è¿åŠ¨åºåˆ—åœ¨æ–‡æœ¬ä¸åŠ¨ä½œçš„å¯¹é½åº¦å’Œå¤šæ ·æ€§ä¸Šä¼˜äºè¿‘æœŸæ–¹æ³•ï¼Œä¸ºç¨³å¥å’Œè‡ªç„¶çš„è¿åŠ¨åˆæˆè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä»£ç å°†åœ¨æœªæ¥å¼€æºï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åŠ¨æ€è¿åŠ¨åˆæˆä¸­äº¤äº’åŠ¨ä½œç”Ÿæˆçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å°†å•äººå’Œå¤šäººåŠ¨æ€åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ä½œç¼ºä¹çœŸå®æ„Ÿå’Œåè°ƒæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨äº¤é”™å­¦ä¹ ç­–ç•¥ï¼Œè”åˆå»ºæ¨¡å•äººå’Œäº¤äº’è¡Œä¸ºï¼Œä»¥æ•æ‰ç°å®åœºæ™¯ä¸­çš„è‡ªç„¶åŠ¨æ€äº¤äº’å’Œç»†å¾®åè°ƒã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ çœŸå®ä¸–ç•Œä¸­çš„å¤æ‚äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInterSynæ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šäº¤é”™äº¤äº’åˆæˆæ¨¡å—ï¼ˆINSï¼‰å’Œç›¸å¯¹åè°ƒç²¾ç‚¼æ¨¡å—ï¼ˆRECï¼‰ã€‚INSæ¨¡å—ä»ç¬¬ä¸€äººç§°è§†è§’å»ºæ¨¡å•äººå’Œäº¤äº’è¡Œä¸ºï¼Œè€ŒRECæ¨¡å—åˆ™è´Ÿè´£ç²¾ç‚¼è§’è‰²ä¹‹é—´çš„ç›¸äº’åŠ¨æ€ï¼Œç¡®ä¿åŠ¨ä½œçš„åŒæ­¥æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šInterSynçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶äº¤é”™å­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘å•äººå’Œå¤šäººåŠ¨æ€ï¼Œæ˜¾è‘—æå‡äº†ç”ŸæˆåŠ¨ä½œçš„è‡ªç„¶æ€§å’Œåè°ƒæ€§ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„åˆ†å¼€å¤„ç†å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒINSæ¨¡å—é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å•äººå’Œäº¤äº’è¡Œä¸ºçš„å»ºæ¨¡ï¼Œè€ŒRECæ¨¡å—åˆ™é€šè¿‡ç›¸å¯¹åŠ¨æ€çš„ç²¾ç‚¼æœºåˆ¶æ¥ç¡®ä¿è§’è‰²ä¹‹é—´çš„åŠ¨ä½œåŒæ­¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒInterSynç”Ÿæˆçš„è¿åŠ¨åºåˆ—åœ¨æ–‡æœ¬ä¸åŠ¨ä½œçš„å¯¹é½åº¦ä¸Šæé«˜äº†æ˜¾è‘—æ€§ï¼Œä¸”å¤šæ ·æ€§æ–¹é¢ä¹Ÿä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ï¼Œè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€è¿åŠ¨åˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŠ¨ç”»åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºè§’è‰²äº¤äº’å’ŒåŠ¨æ€åœºæ™¯ç”Ÿæˆæä¾›æ›´çœŸå®çš„è¿åŠ¨åˆæˆæ–¹æ¡ˆã€‚æœªæ¥ï¼ŒInterSynå¯èƒ½ä¼šæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Interleaved Learning for Motion Synthesis (InterSyn), a novel framework that targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics. Unlike previous methods that treat these components separately, InterSyn employs an interleaved learning strategy to capture the natural, dynamic interactions and nuanced coordination inherent in real-world scenarios. Our framework comprises two key modules: the Interleaved Interaction Synthesis (INS) module, which jointly models solo and interactive behaviors in a unified paradigm from a first-person perspective to support multiple character interactions, and the Relative Coordination Refinement (REC) module, which refines mutual dynamics and ensures synchronized motions among characters. Experimental results show that the motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods, setting a new benchmark for robust and natural motion synthesis. Additionally, our code will be open-sourced in the future to promote further research and development in this area.

