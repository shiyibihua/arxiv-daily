---
layout: default
title: Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach
---

# Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach

**arXiv**: [2511.05057v1](https://arxiv.org/abs/2511.05057) | [PDF](https://arxiv.org/pdf/2511.05057.pdf)

**ä½œè€…**: Yuanxiang Huangfu, Chaochao Wang, Weilei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRole-SynthCLIPï¼Œé€šè¿‡å¤šè§†è§’è§’è‰²æ‰®æ¼”æç¤ºå¢žå¼ºåˆæˆæ•°æ®è¯­ä¹‰å¤šæ ·æ€§ä»¥æ”¹è¿›CLIPæ¨¡åž‹è®­ç»ƒã€‚**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ` `åˆæˆæ•°æ®ç”Ÿæˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è¯­ä¹‰å¤šæ ·æ€§` `å›¾åƒ-æ–‡æœ¬å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åˆæˆæ•°æ®æ–¹æ³•å¼ºè°ƒæ•°æ®é‡ï¼Œä½†è¯­ä¹‰å¤šæ ·æ€§æœ‰é™ä¸”æ ‡é¢˜å†—ä½™æµ…æ˜¾ã€‚
2. åˆ©ç”¨å¤šè§†è§’è§’è‰²æ‰®æ¼”æç¤ºå¼•å¯¼MLLMsç”Ÿæˆå¤šæ ·åŒ–æ ‡é¢˜ï¼Œæå‡å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚
3. å®žéªŒæ˜¾ç¤ºï¼Œä»…ç”¨100ä¸‡å¯¹æ•°æ®è®­ç»ƒCLIP-B/16ï¼Œåœ¨MS COCOä¸ŠRecall@1è¾¾64.1%ï¼Œä¼˜äºŽçŽ°æœ‰åŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
> critically depends on the semantic diversity and quality of their training
> data. However, while existing synthetic data generation methods primarily focus
> on increasing data volume, such emphasis often leads to limited semantic
> diversity and redundant or shallow captions. To address this limitation, we
> propose Role-SynthCLIP, a novel data synthesis framework that leverages
> multi-perspective role-playing prompts (e.g., a compositional analyst, an
> interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
> in generating semantically diverse captions from distinct viewpoints. This
> mechanism enhances the semantic diversity and fine-grained image-text alignment
> of synthetic pairs, thereby improving caption expressiveness and accuracy while
> keeping the total number of image-text pairs unchanged. Experimental results
> demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
> trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
> the MS COCO validation set, surpassing the best existing synthetic data
> baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
> models are released at https://github.com/huangfu170/Role-SynthCLIP.

