---
layout: default
title: Deep learning models are vulnerable, but adversarial examples are even more vulnerable
---

# Deep learning models are vulnerable, but adversarial examples are even more vulnerable

**arXiv**: [2511.05073v1](https://arxiv.org/abs/2511.05073) | [PDF](https://arxiv.org/pdf/2511.05073.pdf)

**ä½œè€…**: Jun Li, Yanwei Xu, Keran Li, Xiaoli Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ»‘åŠ¨çª—å£æŽ©ç æ£€æµ‹æ–¹æ³•ä»¥å¢žå¼ºå¯¹æŠ—æ ·æœ¬æ£€æµ‹ä¸Žæ¨¡åž‹é²æ£’æ€§**

**å…³é”®è¯**: `å¯¹æŠ—æ ·æœ¬æ£€æµ‹` `æ¨¡åž‹é²æ£’æ€§` `é®æŒ¡æ•æ„Ÿæ€§` `æ·±åº¦å­¦ä¹ å®‰å…¨` `CIFAR-10æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹æŠ—æ ·æœ¬åœ¨é®æŒ¡ä¸‹æ¯”å¹²å‡€æ ·æœ¬æ›´æ•æ„Ÿï¼Œå½±å“DNNé²æ£’æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥SMCEé‡åŒ–é®æŒ¡ä¸‹ç½®ä¿¡åº¦æ³¢åŠ¨ï¼Œå¼€å‘SWM-AEDæ£€æµ‹æ–¹æ³•ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-10ä¸Šè¯„ä¼°ï¼Œæ£€æµ‹å‡†ç¡®çŽ‡æœ€é«˜è¾¾96.5%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding intrinsic differences between adversarial examples and clean
> samples is key to enhancing DNN robustness and detection against adversarial
> attacks. This study first empirically finds that image-based adversarial
> examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10
> used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,
> paired with original samples for evaluation. We introduce Sliding Mask
> Confidence Entropy (SMCE) to quantify model confidence fluctuation under
> occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy
> Field Maps and statistical distributions show adversarial examples have
> significantly higher confidence volatility under occlusion than originals.
> Based on this, we propose Sliding Window Mask-based Adversarial Example
> Detection (SWM-AED), which avoids catastrophic overfitting of conventional
> adversarial training. Evaluations across classifiers and attacks on CIFAR-10
> demonstrate robust performance, with accuracy over 62% in most cases and up to
> 96.5%.

