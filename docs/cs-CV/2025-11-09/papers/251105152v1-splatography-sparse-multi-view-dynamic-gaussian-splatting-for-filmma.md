---
layout: default
title: Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges
---

# Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges

**arXiv**: [2511.05152v1](https://arxiv.org/abs/2511.05152) | [PDF](https://arxiv.org/pdf/2511.05152.pdf)

**ä½œè€…**: Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–å¤šè§†è§’åŠ¨æ€é«˜æ–¯æ³¼æº…æ–¹æ³•ä»¥è§£å†³ä½Žæˆæœ¬ç”µå½±åˆ¶ä½œä¸­åŠ¨æ€3Dé‡å»ºéš¾é¢˜**

**å…³é”®è¯**: `åŠ¨æ€3Dé‡å»º` `é«˜æ–¯æ³¼æº…` `ç¨€ç–å¤šè§†è§’` `å‰æ™¯èƒŒæ™¯åˆ†å‰²` `å˜å½¢åœºå»ºæ¨¡` `ç”µå½±åˆ¶ä½œåº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–ç›¸æœºé…ç½®é™åˆ¶çŽ°æœ‰æ–¹æ³•æ•æ‰å¤æ‚åŠ¨æ€ç‰¹å¾ï¼Œå½±å“ç”µå½±åˆ¶ä½œæˆæœ¬æ•ˆç›Š
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†å‰²é«˜æ–¯æ³¼æº…å’Œå˜å½¢åœºä¸ºå‰æ™¯ä¸ŽèƒŒæ™¯ï¼Œä½¿ç”¨ç¨€ç–æŽ©ç åˆ†åˆ«è®­ç»ƒä¸åŒæŸå¤±å‡½æ•°
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨3Då’Œ2.5Dæ•°æ®é›†ä¸Šå®žçŽ°SotAç»“æžœï¼ŒPSNRæå‡é«˜è¾¾3ï¼Œæ¨¡åž‹å°ºå¯¸å‡åŠ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
> reconstruction from dense multi-view video (MVV) by learning to deform a
> canonical GS representation. However, in filmmaking, tight budgets can result
> in sparse camera configurations, which limits state-of-the-art (SotA) methods
> when capturing complex dynamic features. To address this issue, we introduce an
> approach that splits the canonical Gaussians and deformation field into
> foreground and background components using a sparse set of masks for frames at
> t=0. Each representation is separately trained on different loss functions
> during canonical pre-training. Then, during dynamic training, different
> parameters are modeled for each deformation field following common filmmaking
> practices. The foreground stage contains diverse dynamic features so changes in
> color, position and rotation are learned. While, the background containing
> film-crew and equipment, is typically dimmer and less dynamic so only changes
> in point position are learned. Experiments on 3-D and 2.5-D entertainment
> datasets show that our method produces SotA qualitative and quantitative
> results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
> SotA and without the need for dense mask supervision, our method also produces
> segmented dynamic reconstructions including transparent and dynamic textures.
> Code and video comparisons are available online:
> https://interims-git.github.io/

