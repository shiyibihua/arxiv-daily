---
layout: default
title: Decomposed Object Manipulation via Dual-Actor Policy
---

# Decomposed Object Manipulation via Dual-Actor Policy

**arXiv**: [2511.05129v1](https://arxiv.org/abs/2511.05129) | [PDF](https://arxiv.org/pdf/2511.05129.pdf)

**ä½œè€…**: Bin Fan, Jianjian Jiang, Zhuohao Li, Yixiang He, Xiaoming Wu, Yihan Yang, Shengbang Liu, Weishi Zheng

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒè¡ŒåŠ¨è€…ç­–ç•¥ä»¥åˆ†è§£ç‰©ä½“æ“ä½œä»»åŠ¡ï¼Œæå‡æœºå™¨äººæ“ä½œæ€§èƒ½**

**å…³é”®è¯**: `ç‰©ä½“æ“ä½œ` `åŒè¡ŒåŠ¨è€…ç­–ç•¥` `å¯ä¾›æ€§å­¦ä¹ ` `è¿åŠ¨æµ` `æœºå™¨äººæ¨¡æ‹Ÿ` `å¤šé˜¶æ®µä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•ç”¨å•ä¸€ç­–ç•¥å­¦ä¹ ç‰©ä½“æ“ä½œï¼Œå¿½ç•¥ä»»åŠ¡åˆ†é˜¶æ®µç‰¹æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŸºäºå¯ä¾›æ€§å’Œè¿åŠ¨æµçš„åŒè¡ŒåŠ¨è€…ï¼Œåˆ†åˆ«ä¼˜åŒ–æ¥è¿‘ä¸æ“ä½œé˜¶æ®µ
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨æ¨¡æ‹Ÿã€åŸºå‡†å’ŒçœŸå®åœºæ™¯ä¸­å¹³å‡ä¼˜äºSOTAæ–¹æ³•5.55%è‡³14.7%

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Object manipulation, which focuses on learning to perform tasks on similar
> parts across different types of objects, can be divided into an approaching
> stage and a manipulation stage. However, previous works often ignore this
> characteristic of the task and rely on a single policy to directly learn the
> whole process of object manipulation. To address this problem, we propose a
> novel Dual-Actor Policy, termed DAP, which explicitly considers different
> stages and leverages heterogeneous visual priors to enhance each stage.
> Specifically, we introduce an affordance-based actor to locate the functional
> part in the manipulation task, thereby improving the approaching process.
> Following this, we propose a motion flow-based actor to capture the movement of
> the component, facilitating the manipulation process. Finally, we introduce a
> decision maker to determine the current stage of DAP and select the
> corresponding actor. Moreover, existing object manipulation datasets contain
> few objects and lack the visual priors needed to support training. To address
> this, we construct a simulated dataset, the Dual-Prior Object Manipulation
> Dataset, which combines the two visual priors and includes seven tasks,
> including two challenging long-term, multi-stage tasks. Experimental results on
> our dataset, the RoboTwin benchmark and real-world scenarios illustrate that
> our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%
> on average respectively.

