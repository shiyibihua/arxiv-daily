---
layout: default
title: EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation
---

# EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation

**arXiv**: [2511.05397v1](https://arxiv.org/abs/2511.05397) | [PDF](https://arxiv.org/pdf/2511.05397.pdf)

**ä½œè€…**: Samarth Chopra, Alex McMoil, Ben Carnovale, Evan Sokolson, Rajkumar Kubendran, Samuel Dickerson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEverydayVLAæ¨¡åž‹ï¼Œç»“åˆä½Žæˆæœ¬ç¡¬ä»¶ä»¥è§£å†³æœºå™¨äººæ“ä½œåœ¨å¤æ‚åœºæ™¯ä¸­çš„å¯é æ€§é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `ä½Žæˆæœ¬æœºå™¨äºº` `è‡ªé€‚åº”é‡è§„åˆ’` `6è‡ªç”±åº¦æ“ä½œ` `æœºå™¨äººåŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLAæ¨¡åž‹ä¾èµ–æ˜‚è´µç¡¬ä»¶ï¼Œåœ¨é™Œç”Ÿæˆ–æ‚ä¹±åœºæ™¯ä¸­è¡¨çŽ°ä¸ä½³
2. æ–¹æ³•è¦ç‚¹ï¼šç»Ÿä¸€æ¨¡åž‹è¾“å‡ºç¦»æ•£å’Œè¿žç»­åŠ¨ä½œï¼Œè‡ªé€‚åº”é›†æˆç›‘æŽ§ä¸ç¡®å®šæ€§è§¦å‘é‡è§„åˆ’
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†åŒ¹é…SOTAï¼ŒçœŸå®žä¸–ç•Œæµ‹è¯•ä¸­åˆ†å¸ƒå†…å¤–æ€§èƒ½æå‡æ˜¾è‘—

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Vision-Language-Action (VLA) models map visual inputs and language
> instructions directly to robot actions, they often rely on costly hardware and
> struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
> manipulator that can be assembled for under $300, capable of modest payloads
> and workspace. A single unified model jointly outputs discrete and continuous
> actions, and our adaptive-horizon ensemble monitors motion uncertainty to
> trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
> EverydayVLA matches state-of-the-art success rates, and in real-world tests it
> outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
> By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
> democratizes access to a robotic foundation model and paves the way for
> economical use in homes and research labs alike. Experiment videos and details:
> https://everydayvla.github.io/

