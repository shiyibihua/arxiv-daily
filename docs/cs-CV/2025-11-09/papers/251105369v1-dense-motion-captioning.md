---
layout: default
title: Dense Motion Captioning
---

# Dense Motion Captioning

**arXiv**: [2511.05369v1](https://arxiv.org/abs/2511.05369) | [PDF](https://arxiv.org/pdf/2511.05369.pdf)

**ä½œè€…**: Shiyao Xu, Benedetta Liberatori, GÃ¼l Varol, Paolo Rota

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDense Motion Captioningä»»åŠ¡å’ŒDEMOæ¨¡åž‹ï¼Œä»¥è§£å†³3Däººä½“è¿åŠ¨åºåˆ—ä¸­åŠ¨ä½œçš„æ—¶åºå®šä½ä¸Žæè¿°é—®é¢˜ã€‚**

**å…³é”®è¯**: `3Däººä½“è¿åŠ¨ç†è§£` `æ—¶åºåŠ¨ä½œå®šä½` `å¯†é›†è¿åŠ¨æè¿°` `CompMoæ•°æ®é›†` `DEMOæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰3Dè¿åŠ¨ä¸Žè¯­è¨€é›†æˆç ”ç©¶å¤šå…³æ³¨æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆï¼Œè€Œè¿åŠ¨ç†è§£ä»»åŠ¡ç›¸å¯¹æœªè¢«å……åˆ†æŽ¢ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥DEMOæ¨¡åž‹ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡åž‹ä¸Žç®€å•è¿åŠ¨é€‚é…å™¨ï¼Œç”Ÿæˆæ—¶åºé”šå®šçš„å¯†é›†æè¿°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CompMoæ•°æ®é›†å’Œé€‚åº”åŸºå‡†ä¸Šï¼ŒDEMOæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä¸º3Dè¿åŠ¨ç†è§£å»ºç«‹å¼ºåŸºçº¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in 3D human motion and language integration have primarily
> focused on text-to-motion generation, leaving the task of motion understanding
> relatively unexplored. We introduce Dense Motion Captioning, a novel task that
> aims to temporally localize and caption actions within 3D human motion
> sequences. Current datasets fall short in providing detailed temporal
> annotations and predominantly consist of short sequences featuring few actions.
> To overcome these limitations, we present the Complex Motion Dataset (CompMo),
> the first large-scale dataset featuring richly annotated, complex motion
> sequences with precise temporal boundaries. Built through a carefully designed
> data generation pipeline, CompMo includes 60,000 motion sequences, each
> composed of multiple actions ranging from at least two to ten, accurately
> annotated with their temporal extents. We further present DEMO, a model that
> integrates a large language model with a simple motion adapter, trained to
> generate dense, temporally grounded captions. Our experiments show that DEMO
> substantially outperforms existing methods on CompMo as well as on adapted
> benchmarks, establishing a robust baseline for future research in 3D motion
> understanding and captioning.

