---
layout: default
title: VideoSSR: Video Self-Supervised Reinforcement Learning
---

# VideoSSR: Video Self-Supervised Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.06281" target="_blank" class="toolbar-btn">arXiv: 2511.06281v1</a>
    <a href="https://arxiv.org/pdf/2511.06281.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06281v1" 
            onclick="toggleFavorite(this, '2511.06281v1', 'VideoSSR: Video Self-Supervised Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, Yu Cheng

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-09

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/lcqysl/VideoSSR)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VideoSSRÔºåÂà©Áî®ËßÜÈ¢ëËá™ÁõëÁù£Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁêÜËß£` `Ëá™ÁõëÁù£Â≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ËßÜÈ¢ëÈóÆÁ≠î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÊï∞ÊçÆÈõÜÈöæ‰ª•Êª°Ë∂≥Âø´ÈÄüÂèëÂ±ïÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈúÄÊ±ÇÔºåËÄå‰∫∫Â∑•Ê†áÊ≥®È´òË¥®ÈáèÊï∞ÊçÆÊàêÊú¨È´òÊòÇ„ÄÇ
2. VideoSSRÂà©Áî®ËßÜÈ¢ëÂÜÖÂú®‰ø°ÊÅØËá™ÁõëÁù£ÁîüÊàêÈ´òË¥®Èáè„ÄÅÂèØÈ™åËØÅÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVideoSSRÂú®Â§ö‰∏™ËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÊåÅÁª≠ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºåÂπ≥ÂùáÊèêÂçáË∂ÖËøá5%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ËßÜÈ¢ë‰∏≠‰∏∞ÂØåÁöÑÂÜÖÂú®‰ø°ÊÅØÊù•Ëá™ÊàëÁîüÊàêÈ´òË¥®Èáè„ÄÅÂèØÈ™åËØÅÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ª•ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏â‰∏™Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉ‰ªªÂä°ÔºöÂºÇÂ∏∏ÂÆö‰Ωç„ÄÅÁâ©‰ΩìËÆ°Êï∞ÂíåÊó∂Èó¥ÊãºÂõæÔºåÂπ∂ÊûÑÂª∫‰∫ÜËßÜÈ¢ëÂÜÖÂú®ÁêÜËß£Âü∫ÂáÜÔºàVIUBenchÔºâÊù•È™åËØÅËøô‰∫õ‰ªªÂä°ÁöÑÈöæÂ∫¶„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑMLLMÂú®Ëøô‰∫õ‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇÂü∫‰∫éËøô‰∫õÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°Ôºå‰ΩúËÄÖÊûÑÂª∫‰∫ÜVideoSSR-30KÊï∞ÊçÆÈõÜÔºåÂπ∂ÊèêÂá∫‰∫ÜVideoSSRÔºå‰∏ÄÁßçÁî®‰∫éRLVRÁöÑËßÜÈ¢ëËá™ÁõëÁù£Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂„ÄÇÂú®Ê∂µÁõñÂõõ‰∏™‰∏ªË¶ÅËßÜÈ¢ëÈ¢ÜÂüüÔºàÈÄöÁî®ËßÜÈ¢ëÈóÆÁ≠î„ÄÅÈïøËßÜÈ¢ëÈóÆÁ≠î„ÄÅÊó∂Èó¥ÂÆö‰ΩçÂíåÂ§çÊùÇÊé®ÁêÜÔºâÁöÑ17‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÂπøÊ≥õÁöÑÂÆûÈ™åË°®ÊòéVideoSSRËÉΩÂ§üÊåÅÁª≠ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºåÂπ≥ÂùáÊèêÂçáË∂ÖËøá5%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéVideoSSRÊòØÂºÄÂèëÊõ¥È´òÁ∫ßMLLMËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÁöÑÂº∫Â§ßÂü∫Á°ÄÊ°ÜÊû∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜÈ¢ëÊï∞ÊçÆÈõÜÁöÑÂ§çÊùÇÂ∫¶ÂíåËßÑÊ®°Â∑≤ÁªèÊó†Ê≥ïÊª°Ë∂≥Âø´ÈÄüÂèëÂ±ïÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÈúÄÊ±Ç„ÄÇ‰∫∫Â∑•Ê†áÊ≥®Êñ∞ÁöÑÈ´òË¥®ÈáèËßÜÈ¢ëÊï∞ÊçÆÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜMLLMÂú®ËßÜÈ¢ëÁêÜËß£ÊñπÈù¢ÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÈ´òÊïàÂú∞Ëé∑ÂèñÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëËÆ≠ÁªÉÊï∞ÊçÆÊàê‰∏∫‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜÈ¢ëÊú¨Ë∫´ÊâÄËï¥Âê´ÁöÑ‰∏∞ÂØå‰ø°ÊÅØÔºåÈÄöËøáËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÂºèÔºåËá™Âä®ÁîüÊàêÈ´òË¥®Èáè„ÄÅÂèØÈ™åËØÅÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÊòÇË¥µÁöÑ‰∫∫Â∑•Ê†áÊ≥®ÔºåÂπ∂‰∏îËÉΩÂ§üÂÖÖÂàÜÊåñÊéòËßÜÈ¢ëÊï∞ÊçÆÁöÑÂÜÖÂú®‰ª∑ÂÄºÔºå‰ªéËÄåÊèêÂçáMLLMÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVideoSSRÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉ‰ªªÂä°ËÆæËÆ°ÔºöËÆæËÆ°‰∫ÜAnomaly GroundingÔºàÂºÇÂ∏∏ÂÆö‰ΩçÔºâ„ÄÅObject CountingÔºàÁâ©‰ΩìËÆ°Êï∞ÔºâÂíåTemporal JigsawÔºàÊó∂Èó¥ÊãºÂõæÔºâ‰∏â‰∏™Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉ‰ªªÂä°ÔºåÁî®‰∫éÊåñÊéòËßÜÈ¢ëÁöÑÂÜÖÂú®‰ø°ÊÅØ„ÄÇ2) VideoSSR-30KÊï∞ÊçÆÈõÜÊûÑÂª∫ÔºöÂü∫‰∫é‰∏äËø∞È¢ÑËÆ≠ÁªÉ‰ªªÂä°ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑËá™ÁõëÁù£ËßÜÈ¢ëÊï∞ÊçÆÈõÜVideoSSR-30K„ÄÇ3) Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºöÂà©Áî®VideoSSR-30KÊï∞ÊçÆÈõÜÔºåÈááÁî®Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïËÆ≠ÁªÉMLLMÔºåÊèêÂçáÂÖ∂ËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVideoSSRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥ÁöÑËßÜÈ¢ëËá™ÁõëÁù£Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üËá™Âä®ÁîüÊàêÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂Âà©Áî®Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊèêÂçáMLLMÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåVideoSSRÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÔºåËÉΩÂ§üÊõ¥È´òÊïàÂú∞Âà©Áî®ËßÜÈ¢ëÊï∞ÊçÆ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉ‰ªªÂä°ËÆæËÆ°ÊñπÈù¢ÔºåAnomaly GroundingÊó®Âú®ËÆ©Ê®°ÂûãÂ≠¶‰π†ËØÜÂà´ËßÜÈ¢ë‰∏≠ÁöÑÂºÇÂ∏∏‰∫ã‰ª∂ÔºõObject CountingÊó®Âú®ËÆ©Ê®°ÂûãÂ≠¶‰π†ËßÜÈ¢ë‰∏≠Áâ©‰ΩìÁöÑÊï∞ÈáèÔºõTemporal JigsawÊó®Âú®ËÆ©Ê®°ÂûãÂ≠¶‰π†ËßÜÈ¢ëÁöÑÊó∂Èó¥È°∫Â∫è„ÄÇÂú®Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊñπÈù¢ÔºåÈááÁî®‰∫ÜReinforcement Learning with Verifiable Rewards (RLVR)Ê°ÜÊû∂ÔºåÂπ∂Ê†πÊçÆ‰∏çÂêåÁöÑËßÜÈ¢ëÁêÜËß£‰ªªÂä°ËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideoSSRÂú®17‰∏™ËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ≥ÂùáÊèêÂçáË∂ÖËøá5%„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®ÈÄöÁî®ËßÜÈ¢ëÈóÆÁ≠î„ÄÅÈïøËßÜÈ¢ëÈóÆÁ≠î„ÄÅÊó∂Èó¥ÂÆö‰ΩçÂíåÂ§çÊùÇÊé®ÁêÜÁ≠â‰ªªÂä°‰∏äÔºåVideoSSRÈÉΩ‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVideoSSRÊòØ‰∏ÄÁßçÂæàÊúâÊΩúÂäõÁöÑËßÜÈ¢ëËá™ÁõëÁù£Â≠¶‰π†Ê°ÜÊû∂ÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáMLLMÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VideoSSRÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅËßÜÈ¢ëÊêúÁ¥¢„ÄÅÊô∫ËÉΩÂÆ¢ÊúçÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáMLLMÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Êô∫ËÉΩÂåñÁöÑËßÜÈ¢ëÂàÜÊûêÂíåÂ§ÑÁêÜÔºå‰æãÂ¶ÇËá™Âä®ËØÜÂà´ÁõëÊéßËßÜÈ¢ë‰∏≠ÁöÑÂºÇÂ∏∏Ë°å‰∏∫„ÄÅÁêÜËß£Ëá™Âä®È©æÈ©∂ËΩ¶ËæÜÂë®Âõ¥ÁöÑ‰∫§ÈÄöÁä∂ÂÜµ„ÄÅÊ†πÊçÆÁî®Êà∑queryÊ£ÄÁ¥¢Áõ∏ÂÖ≥ËßÜÈ¢ëÂÜÖÂÆπ„ÄÅ‰ª•Âèä‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Á≤æÂáÜÁöÑËßÜÈ¢ëÈóÆÁ≠îÊúçÂä°„ÄÇÊú™Êù•ÔºåVideoSSRÊúâÊúõÊàê‰∏∫ÊûÑÂª∫Êõ¥Âº∫Â§ßÁöÑËßÜÈ¢ëÊô∫ËÉΩÁ≥ªÁªüÁöÑÂÖ≥ÈîÆÊäÄÊúØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR.

