---
layout: default
title: Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis
---

# Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis

**arXiv**: [2511.05432v1](https://arxiv.org/abs/2511.05432) | [PDF](https://arxiv.org/pdf/2511.05432.pdf)

**ä½œè€…**: Dogucan Yaman, Seymanur Akti, Fevziye Irem Eyiokur, Alexander Waibel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå…±äº«æ½œåœ¨è¡¨ç¤ºçš„æ–‡æœ¬åˆ°éŸ³è§†é¢‘åˆæˆæ¡†æž¶ï¼Œå®žçŽ°æ— çœŸå®žéŸ³é¢‘çš„åŒæ­¥è¯­éŸ³ä¸Žé¢éƒ¨ç”Ÿæˆã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°éŸ³è§†é¢‘åˆæˆ` `æ½œåœ¨è¡¨ç¤ºå­¦ä¹ ` `éŸ³é¢‘-è§†è§‰å¯¹é½` `ä¸¤é˜¶æ®µè®­ç»ƒ` `å”‡åŒæ­¥ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬åˆ°éŸ³è§†é¢‘åˆæˆä¸­ï¼Œå¦‚ä½•å®žçŽ°ç´§å¯†çš„éŸ³é¢‘-è§†è§‰å¯¹é½å¹¶ä¿æŒè¯´è¯è€…èº«ä»½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨Text-to-Vecæ¨¡å—ç”ŸæˆWav2Vec2åµŒå…¥ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒå¤„ç†ç‰¹å¾åˆ†å¸ƒåç§»ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨TTSé¢„æµ‹æ½œåœ¨ç‰¹å¾ä¸Šæ¡ä»¶åŒ–ï¼Œä¼˜äºŽçº§è”æ–¹æ³•ï¼Œæå‡å”‡åŒæ­¥å’Œè§†è§‰çœŸå®žæ„Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose a text-to-talking-face synthesis framework leveraging latent
> speech representations from HierSpeech++. A Text-to-Vec module generates
> Wav2Vec2 embeddings from text, which jointly condition speech and face
> generation. To handle distribution shifts between clean and TTS-predicted
> features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and
> finetuning on TTS outputs. This enables tight audio-visual alignment, preserves
> speaker identity, and produces natural, expressive speech and synchronized
> facial motion without ground-truth audio at inference. Experiments show that
> conditioning on TTS-predicted latent features outperforms cascaded pipelines,
> improving both lip-sync and visual realism.

