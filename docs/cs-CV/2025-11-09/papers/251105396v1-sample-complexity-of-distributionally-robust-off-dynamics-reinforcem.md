---
layout: default
title: Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction
---

# Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction

**arXiv**: [2511.05396v1](https://arxiv.org/abs/2511.05396) | [PDF](https://arxiv.org/pdf/2511.05396.pdf)

**ä½œè€…**: Yiting He, Zhishuai Liu, Weixin Wang, Pan Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿ç®—æ³•ä»¥è§£å†³è®­ç»ƒä¸Žéƒ¨ç½²åŠ¨æ€ä¸åŒ¹é…çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜**

**å…³é”®è¯**: `åˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å­¦ä¹ ` `æ ·æœ¬å¤æ‚åº¦` `ä¸Šç¡®ç•Œè®¿é—®æ¯”` `æ¬¡çº¿æ€§é—æ†¾` `åŠ¨æ€ä¸åŒ¹é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿äº¤äº’ä¸­è®­ç»ƒä¸Žéƒ¨ç½²åŠ¨æ€ä¸åŒ¹é…å¯¼è‡´æŽ¢ç´¢å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ä¸Šç¡®ç•Œè®¿é—®æ¯”å¹¶è®¾è®¡é«˜æ•ˆç®—æ³•å®žçŽ°æ¬¡çº¿æ€§é—æ†¾
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ•°å€¼å®žéªŒéªŒè¯ç†è®ºç»“æžœï¼Œç®—æ³•è¾¾åˆ°æœ€ä¼˜é—æ†¾ç•Œ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Off-dynamics reinforcement learning (RL), where training and deployment
> transition dynamics are different, can be formulated as learning in a robust
> Markov decision process (RMDP) where uncertainties in transition dynamics are
> imposed. Existing literature mostly assumes access to generative models
> allowing arbitrary state-action queries or pre-collected datasets with a good
> state coverage of the deployment environment, bypassing the challenge of
> exploration. In this work, we study a more realistic and challenging setting
> where the agent is limited to online interaction with the training environment.
> To capture the intrinsic difficulty of exploration in online RMDPs, we
> introduce the supremal visitation ratio, a novel quantity that measures the
> mismatch between the training dynamics and the deployment dynamics. We show
> that if this ratio is unbounded, online learning becomes exponentially hard. We
> propose the first computationally efficient algorithm that achieves sublinear
> regret in online RMDPs with $f$-divergence based transition uncertainties. We
> also establish matching regret lower bounds, demonstrating that our algorithm
> achieves optimal dependence on both the supremal visitation ratio and the
> number of interaction episodes. Finally, we validate our theoretical results
> through comprehensive numerical experiments.

