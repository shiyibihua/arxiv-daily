---
layout: default
title: 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos
---

# 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos

**arXiv**: [2511.05229v1](https://arxiv.org/abs/2511.05229) | [PDF](https://arxiv.org/pdf/2511.05229.pdf)

**ä½œè€…**: Mengqi Guo, Bo Xu, Yanyan Li, Gim Hee Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4D3Ræ¡†æž¶ä»¥è§£å†³å•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯çš„ä½å§¿æœªçŸ¥æ–°è§†å›¾åˆæˆé—®é¢˜**

**å…³é”®è¯**: `åŠ¨æ€ç¥žç»æ¸²æŸ“` `è¿åŠ¨æ„ŸçŸ¥ä¼˜åŒ–` `é«˜æ–¯æº…å°„` `å•ç›®è§†é¢‘é‡å»º` `ä½å§¿ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯ä¸­æœªçŸ¥ç›¸æœºä½å§¿çš„æ–°è§†å›¾åˆæˆä»å…·æŒ‘æˆ˜æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ä¸¤é˜¶æ®µè§£è€¦é™æ€ä¸ŽåŠ¨æ€ç»„ä»¶ï¼Œç»“åˆè¿åŠ¨æ„ŸçŸ¥ä¼˜åŒ–æ¨¡å—
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žåŠ¨æ€æ•°æ®é›†ä¸ŠPSNRæå‡è¾¾1.8dBï¼Œè®¡ç®—æˆæœ¬é™ä½Ž5å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Novel view synthesis from monocular videos of dynamic scenes with unknown
> camera poses remains a fundamental challenge in computer vision and graphics.
> While recent advances in 3D representations such as Neural Radiance Fields
> (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
> scenes, they struggle with dynamic content and typically rely on pre-computed
> camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
> that decouples static and dynamic components through a two-stage approach. Our
> method first leverages 3D foundational models for initial pose and geometry
> estimation, followed by motion-aware refinement. 4D3R introduces two key
> technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
> combines transformer-based learned priors with SAM2 for robust dynamic object
> segmentation, enabling more accurate camera pose refinement; and (2) an
> efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
> control points with a deformation field MLP and linear blend skinning to model
> dynamic motion, significantly reducing computational cost while maintaining
> high-quality reconstruction. Extensive experiments on real-world dynamic
> datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
> over state-of-the-art methods, particularly in challenging scenarios with large
> dynamic objects, while reducing computational requirements by 5x compared to
> previous dynamic scene representations.

