---
layout: default
title: Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration
---

# Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration

**arXiv**: [2511.05421v1](https://arxiv.org/abs/2511.05421) | [PDF](https://arxiv.org/pdf/2511.05421.pdf)

**ä½œè€…**: Aupendu Kar, Krishnendu Ghosh, Prabir Kumar Biswas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…±äº«çŸ¥è¯†åº“çš„å·ç§¯å±‚ä¿®æ”¹æ–¹æ³•ï¼Œä»¥åœ¨æŒç»­å›¾åƒæ¢å¤ä¸­é€‚åº”æ–°ä»»åŠ¡è€Œä¸å¿˜æ—§ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `å›¾åƒæ¢å¤` `å·ç§¯å±‚ä¿®æ”¹` `çŸ¥è¯†å…±äº«` `è®¡ç®—æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŒç»­å­¦ä¹ ä¸­å›¾åƒæ¢å¤ä»»åŠ¡é¢ä¸´å¤§å›¾åƒå°ºå¯¸å’Œå¤šæ ·åŒ–é€€åŒ–æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•éœ€å¤æ‚æž¶æž„ä¿®æ”¹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç®€å•ä¿®æ”¹å·ç§¯å±‚ï¼Œå…±äº«å…ˆå‰ä»»åŠ¡çŸ¥è¯†ï¼Œæ— éœ€æ”¹åŠ¨ä¸»å¹²æž¶æž„ï¼Œå‡å°‘è®¡ç®—å¼€é”€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒéªŒè¯æ–°ä»»åŠ¡å¼•å…¥ä¸æŸå®³æ—§ä»»åŠ¡æ€§èƒ½ï¼Œä¸”æ–°ä»»åŠ¡æ€§èƒ½é€šè¿‡çŸ¥è¯†åº“é€‚åº”å¾—åˆ°æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Continual learning is an emerging topic in the field of deep learning, where
> a model is expected to learn continuously for new upcoming tasks without
> forgetting previous experiences. This field has witnessed numerous
> advancements, but few works have been attempted in the direction of image
> restoration. Handling large image sizes and the divergent nature of various
> degradation poses a unique challenge in the restoration domain. However,
> existing works require heavily engineered architectural modifications for new
> task adaptation, resulting in significant computational overhead.
> Regularization-based methods are unsuitable for restoration, as different
> restoration challenges require different kinds of feature processing. In this
> direction, we propose a simple modification of the convolution layer to adapt
> the knowledge from previous restoration tasks without touching the main
> backbone architecture. Therefore, it can be seamlessly applied to any deep
> architecture without any structural modifications. Unlike other approaches, we
> demonstrate that our model can increase the number of trainable parameters
> without significantly increasing computational overhead or inference time.
> Experimental validation demonstrates that new restoration tasks can be
> introduced without compromising the performance of existing tasks. We also show
> that performance on new restoration tasks improves by adapting the knowledge
> from the knowledge base created by previous restoration tasks. The code is
> available at https://github.com/aupendu/continual-restore.

