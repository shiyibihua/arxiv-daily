---
layout: default
title: Visual Spatial Tuning
---

# Visual Spatial Tuning

**arXiv**: [2511.05491v1](https://arxiv.org/abs/2511.05491) | [PDF](https://arxiv.org/pdf/2511.05491.pdf)

**ä½œè€…**: Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰ç©ºé—´è°ƒä¼˜æ¡†æž¶ä»¥å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹çš„ç©ºé—´èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç©ºé—´æ„ŸçŸ¥` `æ•°æ®é›†æž„å»º` `æ¸è¿›å¼è®­ç»ƒ` `ç©ºé—´æŽ¨ç†` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¢žå¼ºç©ºé—´æ„ŸçŸ¥éœ€é¢å¤–ç¼–ç å™¨ï¼Œå¢žåŠ å¼€é”€å¹¶æŸå®³é€šç”¨èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡æ•°æ®é›†VST-På’ŒVST-Rï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒæå‡ç©ºé—´æŽ¨ç†ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAï¼Œå¦‚MMSI-Bench 34.8%å’ŒVSIBench 61.2%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Capturing spatial relationships from visual inputs is a cornerstone of
> human-like general intelligence. Several previous studies have tried to enhance
> the spatial awareness of Vision-Language Models (VLMs) by adding extra expert
> encoders, which brings extra overhead and usually harms general capabilities.
> To enhance the spatial ability in general architectures, we introduce Visual
> Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with
> human-like visuospatial abilities, from spatial perception to reasoning. We
> first attempt to enhance spatial perception in VLMs by constructing a
> large-scale dataset termed VST-P, which comprises 4.1 million samples spanning
> 19 skills across single views, multiple images, and videos. Then, we present
> VST-R, a curated dataset with 135K samples that instruct models to reason in
> space. In particular, we adopt a progressive training pipeline: supervised
> fine-tuning to build foundational spatial knowledge, followed by reinforcement
> learning to further improve spatial reasoning abilities. Without the
> side-effect to general capabilities, the proposed VST consistently achieves
> state-of-the-art results on several spatial benchmarks, including $34.8\%$ on
> MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the
> Vision-Language-Action models can be significantly enhanced with the proposed
> spatial tuning paradigm, paving the way for more physically grounded AI.

