---
layout: default
title: Temporal-Guided Visual Foundation Models for Event-Based Vision
---

# Temporal-Guided Visual Foundation Models for Event-Based Vision

**arXiv**: [2511.06238v1](https://arxiv.org/abs/2511.06238) | [PDF](https://arxiv.org/pdf/2511.06238.pdf)

**ä½œè€…**: Ruihao Xia, Junhong Cai, Luziwei Leng, Liuyi Wang, Chengju Liu, Ran Cheng, Yang Tang, Pan Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-09

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/XiaRho/TGVFM)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTGVFMï¼Œåˆ©ç”¨æ—¶åºå¼•å¯¼çš„è§†è§‰åŸºç¡€æ¨¡åž‹è§£å†³äº‹ä»¶ç›¸æœºè§†è§‰ä»»åŠ¡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `è§†è§‰åŸºç¡€æ¨¡åž‹` `æ—¶åºå»ºæ¨¡` `æ·±åº¦å­¦ä¹ ` `è¯­ä¹‰åˆ†å‰²` `æ·±åº¦ä¼°è®¡` `ç›®æ ‡æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰äº‹ä»¶ç›¸æœºè§†è§‰æ–¹æ³•ä¾èµ–ä¸“ç”¨æž¶æž„æˆ–é«˜æˆæœ¬è®­ç»ƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡åž‹ã€‚
2. TGVFMé€šè¿‡æ—¶åºä¸Šä¸‹æ–‡èžåˆå—é›†æˆè§†è§‰åŸºç¡€æ¨¡åž‹ï¼Œåˆ©ç”¨é•¿ç¨‹æ—¶åºæ³¨æ„åŠ›å’ŒåŒæ—¶ç©ºæ³¨æ„åŠ›å»ºæ¨¡æ—¶åºä¾èµ–å’Œå¸§ç›¸å…³æ€§ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒTGVFMåœ¨è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šå‡å–å¾—SOTAç»“æžœï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶ç›¸æœºåœ¨å¤æ‚çŽ¯å¢ƒä¸­å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä½†å¼‚æ­¥äº‹ä»¶æµçš„å¤„ç†ä»æ˜¯æŒ‘æˆ˜ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–ä¸“ç”¨æž¶æž„æˆ–é«˜èµ„æºæ¶ˆè€—çš„è®­ç»ƒï¼Œè€Œåˆ©ç”¨å›¾åƒæ•°æ®é¢„è®­ç»ƒçš„çŽ°ä»£è§†è§‰åŸºç¡€æ¨¡åž‹(VFM)åœ¨äº‹ä»¶è§†è§‰ä¸­çš„æ½œåŠ›å°šæœªå……åˆ†æŒ–æŽ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶åºå¼•å¯¼çš„VFM(TGVFM)æ¡†æž¶ï¼Œå®ƒå°†VFMä¸Žæˆ‘ä»¬çš„æ—¶åºä¸Šä¸‹æ–‡èžåˆå—æ— ç¼é›†æˆã€‚è¯¥æ—¶åºå—åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š(1)é•¿ç¨‹æ—¶åºæ³¨æ„åŠ›ï¼Œç”¨äºŽå»ºæ¨¡å…¨å±€æ—¶åºä¾èµ–ï¼›(2)åŒæ—¶ç©ºæ³¨æ„åŠ›ï¼Œç”¨äºŽå¤šå°ºåº¦å¸§ç›¸å…³æ€§ï¼›(3)æ·±åº¦ç‰¹å¾å¼•å¯¼æœºåˆ¶ï¼Œç”¨äºŽèžåˆè¯­ä¹‰-æ—¶åºç‰¹å¾ã€‚é€šè¿‡åœ¨çœŸå®žæ•°æ®ä¸Šé‡æ–°è®­ç»ƒäº‹ä»¶åˆ°è§†é¢‘æ¨¡åž‹å¹¶åˆ©ç”¨åŸºäºŽTransformerçš„VFMï¼ŒTGVFMåœ¨ä¿ç•™æ—¶ç©ºåŠ¨æ€çš„åŒæ—¶ï¼Œåˆ©ç”¨äº†é¢„è®­ç»ƒçš„è¡¨ç¤ºã€‚å®žéªŒè¡¨æ˜Žï¼Œåœ¨è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œç›®æ ‡æ£€æµ‹æ–¹é¢ï¼ŒTGVFMå®žçŽ°äº†SoTAæ€§èƒ½ï¼Œåˆ†åˆ«æ¯”çŽ°æœ‰æ–¹æ³•æé«˜äº†16%ã€21%å’Œ16%ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œé€šè¿‡æ—¶åºæŽ¨ç†ï¼Œé‡Šæ”¾äº†åŸºäºŽå›¾åƒçš„VFMåœ¨äº‹ä»¶è§†è§‰ä¸­çš„è·¨æ¨¡æ€æ½œåŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šäº‹ä»¶ç›¸æœºäº§ç”Ÿçš„å¼‚æ­¥äº‹ä»¶æµéš¾ä»¥å¤„ç†ï¼ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡çš„ç½‘ç»œç»“æž„æˆ–è€—è´¹å¤§é‡èµ„æºè¿›è¡Œè®­ç»ƒã€‚å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨åœ¨å›¾åƒæ•°æ®ä¸Šé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡åž‹ï¼ˆVFMsï¼‰æ¥æå‡äº‹ä»¶ç›¸æœºè§†è§‰ä»»åŠ¡çš„æ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽæ— æ³•æœ‰æ•ˆæ¡¥æŽ¥å›¾åƒåŸŸå’Œäº‹ä»¶åŸŸä¹‹é—´çš„å·®å¼‚ï¼Œä»¥åŠç¼ºä¹å¯¹äº‹ä»¶æµä¸­æ—¶åºä¿¡æ¯çš„å……åˆ†åˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªæ—¶åºå¼•å¯¼æ¨¡å—ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡åž‹ä¸Žäº‹ä»¶æ•°æ®è¿›è¡Œæœ‰æ•ˆèžåˆã€‚è¯¥æ¨¡å—æ—¨åœ¨æå–å’Œåˆ©ç”¨äº‹ä»¶æµä¸­çš„æ—¶åºä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸Žè§†è§‰åŸºç¡€æ¨¡åž‹æå–çš„å›¾åƒç‰¹å¾ç›¸ç»“åˆï¼Œä»Žè€Œæå‡äº‹ä»¶ç›¸æœºè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ï¼ŒåŒæ—¶å…‹æœäº‹ä»¶æ•°æ®å¼‚æ­¥æ€§å’Œç¨€ç–æ€§çš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTGVFMæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šäº‹ä»¶åˆ°è§†é¢‘çš„è½¬æ¢æ¨¡å—ï¼ˆå°†äº‹ä»¶æµè½¬æ¢ä¸ºè§†é¢‘å¸§ï¼‰ã€è§†è§‰åŸºç¡€æ¨¡åž‹ï¼ˆæå–å›¾åƒç‰¹å¾ï¼‰å’Œæ—¶åºä¸Šä¸‹æ–‡èžåˆå—ã€‚æ—¶åºä¸Šä¸‹æ–‡èžåˆå—æ˜¯è¯¥æ¡†æž¶çš„å…³é”®åˆ›æ–°ï¼Œå®ƒåŒ…å«é•¿ç¨‹æ—¶åºæ³¨æ„åŠ›ã€åŒæ—¶ç©ºæ³¨æ„åŠ›å’Œæ·±åº¦ç‰¹å¾å¼•å¯¼æœºåˆ¶ã€‚é¦–å…ˆï¼Œäº‹ä»¶æµè¢«è½¬æ¢ä¸ºè§†é¢‘å¸§ã€‚ç„¶åŽï¼Œè§†è§‰åŸºç¡€æ¨¡åž‹æå–è§†é¢‘å¸§çš„å›¾åƒç‰¹å¾ã€‚æœ€åŽï¼Œæ—¶åºä¸Šä¸‹æ–‡èžåˆå—åˆ©ç”¨é•¿ç¨‹æ—¶åºæ³¨æ„åŠ›å»ºæ¨¡å…¨å±€æ—¶åºä¾èµ–ï¼Œåˆ©ç”¨åŒæ—¶ç©ºæ³¨æ„åŠ›è¿›è¡Œå¤šå°ºåº¦å¸§ç›¸å…³æ€§åˆ†æžï¼Œå¹¶åˆ©ç”¨æ·±åº¦ç‰¹å¾å¼•å¯¼æœºåˆ¶èžåˆè¯­ä¹‰-æ—¶åºç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºçš„æ—¶åºä¸Šä¸‹æ–‡èžåˆå—ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å’Œåˆ©ç”¨äº‹ä»¶æµä¸­çš„æ—¶åºä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸Žè§†è§‰åŸºç¡€æ¨¡åž‹æå–çš„å›¾åƒç‰¹å¾ç›¸ç»“åˆã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTGVFMæ— éœ€ä»Žå¤´å¼€å§‹è®­ç»ƒå¤æ‚çš„ç½‘ç»œç»“æž„ï¼Œè€Œæ˜¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡åž‹ï¼Œå®žçŽ°äº†æ›´é«˜çš„æ€§èƒ½å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œé•¿ç¨‹æ—¶åºæ³¨æ„åŠ›å’ŒåŒæ—¶ç©ºæ³¨æ„åŠ›çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äº‹ä»¶æµä¸­çš„æ—¶åºä¾èµ–å…³ç³»å’Œç©ºé—´ç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šé•¿ç¨‹æ—¶åºæ³¨æ„åŠ›é‡‡ç”¨Transformerç»“æž„ï¼Œç”¨äºŽå»ºæ¨¡å…¨å±€æ—¶åºä¾èµ–å…³ç³»ã€‚åŒæ—¶ç©ºæ³¨æ„åŠ›æ¨¡å—åŒ…å«ç©ºé—´æ³¨æ„åŠ›å’Œæ—¶é—´æ³¨æ„åŠ›ä¸¤ä¸ªåˆ†æ”¯ï¼Œåˆ†åˆ«ç”¨äºŽæå–ç©ºé—´ç‰¹å¾å’Œæ—¶é—´ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèžåˆã€‚æ·±åº¦ç‰¹å¾å¼•å¯¼æœºåˆ¶åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡åž‹æå–çš„æ·±åº¦ç‰¹å¾ä½œä¸ºå¼•å¯¼ä¿¡å·ï¼ŒæŒ‡å¯¼æ—¶åºä¸Šä¸‹æ–‡èžåˆå—çš„å­¦ä¹ è¿‡ç¨‹ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°å’ŒL1æŸå¤±å‡½æ•°çš„ç»„åˆï¼Œç”¨äºŽä¼˜åŒ–è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TGVFMåœ¨ä¸‰ä¸ªä¸»æµçš„äº‹ä»¶ç›¸æœºè§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒTGVFMæ¯”çŽ°æœ‰æ–¹æ³•æé«˜äº†16%ï¼›åœ¨æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒTGVFMæé«˜äº†21%ï¼›åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šï¼ŒTGVFMæé«˜äº†16%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒTGVFMèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡åž‹å’Œäº‹ä»¶æµä¸­çš„æ—¶åºä¿¡æ¯ï¼Œä»Žè€Œå®žçŽ°æ›´é«˜çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€ç›‘æŽ§å®‰é˜²ç­‰é¢†åŸŸã€‚äº‹ä»¶ç›¸æœºåœ¨é«˜é€Ÿè¿åŠ¨ã€é«˜åŠ¨æ€èŒƒå›´å’Œä½Žå…‰ç…§ç­‰åœºæ™¯ä¸‹å…·æœ‰ä¼˜åŠ¿ï¼Œç»“åˆTGVFMæ¡†æž¶ï¼Œå¯ä»¥æå‡è¿™äº›åœºæ™¯ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¾‹å¦‚æé«˜è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤œé—´æˆ–é›¨é›¾å¤©æ°”ä¸‹çš„å®‰å…¨æ€§ï¼Œå¢žå¼ºæœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œä»¥åŠæå‡ç›‘æŽ§ç³»ç»Ÿåœ¨å…‰çº¿ä¸è¶³çŽ¯å¢ƒä¸‹çš„ç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Event cameras offer unique advantages for vision tasks in challenging environments, yet processing asynchronous event streams remains an open challenge. While existing methods rely on specialized architectures or resource-intensive training, the potential of leveraging modern Visual Foundation Models (VFMs) pretrained on image data remains under-explored for event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a novel framework that integrates VFMs with our temporal context fusion block seamlessly to bridge this gap. Our temporal block introduces three key components: (1) Long-Range Temporal Attention to model global temporal dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal features. By retraining event-to-video models on real-world data and leveraging transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while harnessing pretrained representations. Experiments demonstrate SoTA performance across semantic segmentation, depth estimation, and object detection, with improvements of 16%, 21%, and 16% over existing methods, respectively. Overall, this work unlocks the cross-modality potential of image-based VFMs for event-based vision with temporal reasoning. Code is available at https://github.com/XiaRho/TGVFM.

