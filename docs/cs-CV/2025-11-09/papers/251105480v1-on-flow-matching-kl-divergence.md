---
layout: default
title: On Flow Matching KL Divergence
---

# On Flow Matching KL Divergence

**arXiv**: [2511.05480v1](https://arxiv.org/abs/2511.05480) | [PDF](https://arxiv.org/pdf/2511.05480.pdf)

**ä½œè€…**: Maojiang Su, Jerry Yao-Chieh Hu, Sophia Pi, Han Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµåŒ¹é…KLæ•£åº¦ä¸Šç•Œï¼Œæå‡åˆ†å¸ƒä¼°è®¡æ•ˆçŽ‡**

**å…³é”®è¯**: `æµåŒ¹é…` `KLæ•£åº¦ä¸Šç•Œ` `åˆ†å¸ƒä¼°è®¡` `ç»Ÿè®¡æ”¶æ•›` `æ€»å˜å·®è·ç¦»` `æžå°æžå¤§æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæµåŒ¹é…åˆ†å¸ƒè¿‘ä¼¼çš„KLæ•£åº¦ç¼ºä¹ç¡®å®šæ€§ä¸Šç•Œã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽL2æµåŒ¹é…æŸå¤±æŽ¨å¯¼KLæ•£åº¦ä¸Šç•Œï¼Œä¾èµ–æ•°æ®ä¸Žé€Ÿåº¦åœºæ­£åˆ™æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ•°å€¼ç ”ç©¶éªŒè¯ç†è®ºï¼ŒæµåŒ¹é…åœ¨TVè·ç¦»ä¸‹æŽ¥è¿‘æžå°æžå¤§æœ€ä¼˜æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler
> (KL) divergence of the flow-matching distribution approximation. In particular,
> if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL
> divergence between the true data distribution and the estimated distribution is
> bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$
> depend only on the regularities of the data and velocity fields. Consequently,
> this bound implies statistical convergence rates of Flow Matching Transformers
> under the Total Variation (TV) distance. We show that, flow matching achieves
> nearly minimax-optimal efficiency in estimating smooth distributions. Our
> results make the statistical efficiency of flow matching comparable to that of
> diffusion models under the TV distance. Numerical studies on synthetic and
> learned velocities corroborate our theory.

