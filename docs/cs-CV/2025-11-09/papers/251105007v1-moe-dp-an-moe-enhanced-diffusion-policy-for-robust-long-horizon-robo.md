---
layout: default
title: MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery
---

# MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery

**arXiv**: [2511.05007v1](https://arxiv.org/abs/2511.05007) | [PDF](https://arxiv.org/pdf/2511.05007.pdf)

**ä½œè€…**: Baiye Cheng, Tianhai Liang, Suning Huang, Maanping Shao, Feihong Zhang, Botian Xu, Zhengrong Xue, Huazhe Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoE-DPä»¥å¢žå¼ºé•¿è§†é‡Žæœºå™¨äººæ“ä½œä¸­çš„é²æ£’æ€§å’Œå¯è§£é‡Šæ€§**

**å…³é”®è¯**: `æ‰©æ•£ç­–ç•¥` `æœºå™¨äººæ“ä½œ` `ä¸“å®¶æ··åˆ` `æŠ€èƒ½åˆ†è§£` `å¤±è´¥æ¢å¤` `å¯è§£é‡Šå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£ç­–ç•¥åœ¨é•¿è§†é‡Žå¤šé˜¶æ®µä»»åŠ¡ä¸­ç¼ºä¹ä»Žå­ä»»åŠ¡å¤±è´¥ä¸­æ¢å¤çš„é²æ£’æ€§
2. åœ¨è§†è§‰ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡åž‹é—´æ’å…¥MoEå±‚ï¼Œåˆ†è§£çŸ¥è¯†ä¸ºä¸“å®¶å¤„ç†ä¸åŒä»»åŠ¡é˜¶æ®µ
3. åœ¨6ä¸ªæ¨¡æ‹Ÿä»»åŠ¡ä¸­ï¼Œæ‰°åŠ¨æ¡ä»¶ä¸‹æˆåŠŸçŽ‡å¹³å‡ç›¸å¯¹æå‡36%ï¼Œå¹¶éªŒè¯äºŽçœŸå®žä¸–ç•Œ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion policies have emerged as a powerful framework for robotic
> visuomotor control, yet they often lack the robustness to recover from subtask
> failures in long-horizon, multi-stage tasks and their learned representations
> of observations are often difficult to interpret. In this work, we propose the
> Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is
> to insert a Mixture of Experts (MoE) layer between the visual encoder and the
> diffusion model. This layer decomposes the policy's knowledge into a set of
> specialized experts, which are dynamically activated to handle different phases
> of a task. We demonstrate through extensive experiments that MoE-DP exhibits a
> strong capability to recover from disturbances, significantly outperforming
> standard baselines in robustness. On a suite of 6 long-horizon simulation
> tasks, this leads to a 36% average relative improvement in success rate under
> disturbed conditions. This enhanced robustness is further validated in the real
> world, where MoE-DP also shows significant performance gains. We further show
> that MoE-DP learns an interpretable skill decomposition, where distinct experts
> correspond to semantic task primitives (e.g., approaching, grasping). This
> learned structure can be leveraged for inference-time control, allowing for the
> rearrangement of subtasks without any re-training.Our video and code are
> available at the https://moe-dp-website.github.io/MoE-DP-Website/.

