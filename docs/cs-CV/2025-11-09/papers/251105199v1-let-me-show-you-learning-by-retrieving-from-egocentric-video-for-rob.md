---
layout: default
title: Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation
---

# Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation

**arXiv**: [2511.05199v1](https://arxiv.org/abs/2511.05199) | [PDF](https://arxiv.org/pdf/2511.05199.pdf)

**ä½œè€…**: Yichen Zhu, Feifei Feng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»Žè§†é¢‘æ£€ç´¢æ–¹æ³•ä»¥è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡å­¦ä¹ é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†é¢‘æ£€ç´¢` `ç‰©ä½“å¯æ“ä½œæŽ©ç ` `æ‰‹éƒ¨è½¨è¿¹` `ç­–ç•¥ç”Ÿæˆ` `æ³›åŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­å­¦ä¹ æ“ä½œä»»åŠ¡æ—¶ä¾èµ–å¤§é‡æ•°æ®ï¼Œç¼ºä¹äººç±»å¼è§†é¢‘å­¦ä¹ èƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºè§†é¢‘åº“ï¼Œæå–ç‰©ä½“å¯æ“ä½œæŽ©ç å’Œæ‰‹éƒ¨è½¨è¿¹ï¼Œç»“åˆæ£€ç´¢å™¨å’Œç­–ç•¥ç”Ÿæˆå™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žçŽ¯å¢ƒä¸­æµ‹è¯•ï¼Œæ€§èƒ½ä¼˜äºŽä¼ ç»Ÿç³»ç»Ÿï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robots operating in complex and uncertain environments face considerable
> challenges. Advanced robotic systems often rely on extensive datasets to learn
> manipulation tasks. In contrast, when humans are faced with unfamiliar tasks,
> such as assembling a chair, a common approach is to learn by watching video
> demonstrations. In this paper, we propose a novel method for learning robot
> policies by Retrieving-from-Video (RfV), using analogies from human
> demonstrations to address manipulation tasks. Our system constructs a video
> bank comprising recordings of humans performing diverse daily tasks. To enrich
> the knowledge from these videos, we extract mid-level information, such as
> object affordance masks and hand motion trajectories, which serve as additional
> inputs to enhance the robot model's learning and generalization capabilities.
> We further feature a dual-component system: a video retriever that taps into an
> external video bank to fetch task-relevant video based on task specification,
> and a policy generator that integrates this retrieved knowledge into the
> learning cycle. This approach enables robots to craft adaptive responses to
> various scenarios and generalize to tasks beyond those in the training data.
> Through rigorous testing in multiple simulated and real-world settings, our
> system demonstrates a marked improvement in performance over conventional
> robotic systems, showcasing a significant breakthrough in the field of
> robotics.

