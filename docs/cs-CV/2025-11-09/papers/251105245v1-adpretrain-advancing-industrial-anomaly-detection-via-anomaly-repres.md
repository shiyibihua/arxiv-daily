---
layout: default
title: ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining
---

# ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining

**arXiv**: [2511.05245v1](https://arxiv.org/abs/2511.05245) | [PDF](https://arxiv.org/pdf/2511.05245.pdf)

**ä½œè€…**: Xincheng Yao, Yan Luo, Zefeng Qian, Chongyang Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºADPretrainæ¡†æž¶ï¼Œé€šè¿‡å¼‚å¸¸è¡¨ç¤ºé¢„è®­ç»ƒæå‡å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ€§èƒ½**

**å…³é”®è¯**: `å¼‚å¸¸æ£€æµ‹` `è¡¨ç¤ºå­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `å·¥ä¸šå›¾åƒ` `é¢„è®­ç»ƒæ¡†æž¶` `ç‰¹å¾ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šImageNeté¢„è®­ç»ƒç‰¹å¾ä¸Žå¼‚å¸¸æ£€æµ‹ç›®æ ‡ä¸åŒ¹é…ï¼Œä¸”å­˜åœ¨æ•°æ®åˆ†å¸ƒåç§»é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡è§’åº¦å’ŒèŒƒæ•°å¯¹æ¯”æŸå¤±ï¼Œæœ€å¤§åŒ–æ­£å¸¸ä¸Žå¼‚å¸¸ç‰¹å¾å·®å¼‚ï¼Œä½¿ç”¨å¤§è§„æ¨¡ADæ•°æ®é›†é¢„è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªæ•°æ®é›†å’Œéª¨å¹²ç½‘ç»œä¸Šï¼Œæ›¿æ¢ç‰¹å¾åŽå‡æ˜¾ç¤ºä¼˜è¶Šæ€§ï¼Œä»£ç å·²å¼€æºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The current mainstream and state-of-the-art anomaly detection (AD) methods
> are substantially established on pretrained feature networks yielded by
> ImageNet pretraining. However, regardless of supervised or self-supervised
> pretraining, the pretraining process on ImageNet does not match the goal of
> anomaly detection (i.e., pretraining in natural images doesn't aim to
> distinguish between normal and abnormal). Moreover, natural images and
> industrial image data in AD scenarios typically have the distribution shift.
> The two issues can cause ImageNet-pretrained features to be suboptimal for AD
> tasks. To further promote the development of the AD field, pretrained
> representations specially for AD tasks are eager and very valuable. To this
> end, we propose a novel AD representation learning framework specially designed
> for learning robust and discriminative pretrained representations for
> industrial anomaly detection. Specifically, closely surrounding the goal of
> anomaly detection (i.e., focus on discrepancies between normals and anomalies),
> we propose angle- and norm-oriented contrastive losses to maximize the angle
> size and norm difference between normal and abnormal features simultaneously.
> To avoid the distribution shift from natural images to AD images, our
> pretraining is performed on a large-scale AD dataset, RealIAD. To further
> alleviate the potential shift between pretraining data and downstream AD
> datasets, we learn the pretrained AD representations based on the
> class-generalizable representation, residual features. For evaluation, based on
> five embedding-based AD methods, we simply replace their original features with
> our pretrained representations. Extensive experiments on five AD datasets and
> five backbones consistently show the superiority of our pretrained features.
> The code is available at https://github.com/xcyao00/ADPretrain.

