---
layout: default
title: iFlyBot-VLM Technical Report
---

# iFlyBot-VLM Technical Report

**arXiv**: [2511.04976v1](https://arxiv.org/abs/2511.04976) | [PDF](https://arxiv.org/pdf/2511.04976.pdf)

**ä½œè€…**: Xin Nie, Zhiyuan Cheng, Yuan Zhang, Chao Ji, Jiajia Wu, Yuhan Zhang, Jia Pan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºiFlyBot-VLMä»¥è§£å†³å…·èº«æ™ºèƒ½ä¸­è§†è§‰æ„ŸçŸ¥ä¸Žè¿åŠ¨æŽ§åˆ¶çš„è·¨æ¨¡æ€è¯­ä¹‰é¸¿æ²Ÿ**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å…·èº«æ™ºèƒ½` `è·¨æ¨¡æ€è¯­ä¹‰` `æ“ä½œè¯­è¨€` `æ„ŸçŸ¥-åŠ¨ä½œé—­çŽ¯` `åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé«˜ç»´çŽ¯å¢ƒæ„ŸçŸ¥ä¸Žä½Žå±‚æœºå™¨äººè¿åŠ¨æŽ§åˆ¶é—´çš„è¯­ä¹‰é¸¿æ²Ÿé˜»ç¢å…·èº«æ™ºèƒ½å‘å±•
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è§†è§‰ç©ºé—´ä¿¡æ¯æŠ½è±¡ä¸ºå¯è½¬ç§»çš„æ“ä½œè¯­è¨€ï¼Œå®žçŽ°æ„ŸçŸ¥-åŠ¨ä½œé—­çŽ¯åè°ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨10ä¸ªä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶ä¿æŒæ¨¡åž‹é€šç”¨èƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used
> to improve the domain of Embodied Intelligence. The central objective of
> iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional
> environmental perception and low-level robotic motion control. To this end, the
> model abstracts complex visual and spatial information into a body-agnostic and
> transferable Operational Language, thereby enabling seamless perception-action
> closed-loop coordination across diverse robotic platforms. The architecture of
> iFlyBot-VLM is systematically designed to realize four key functional
> capabilities essential for embodied intelligence: 1) Spatial Understanding and
> Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and
> Control Parameter Generation; 4) Task Planning and Skill Sequencing. We
> envision iFlyBot-VLM as a scalable and generalizable foundation model for
> embodied AI, facilitating the progression from specialized task-oriented
> systems toward generalist, cognitively capable agents. We conducted evaluations
> on 10 current mainstream embodied intelligence-related VLM benchmark datasets,
> such as Blink and Where2Place, and achieved optimal performance while
> preserving the model's general capabilities. We will publicly release both the
> training data and model weights to foster further research and development in
> the field of Embodied Intelligence.

