---
layout: default
title: DeepEyesV2: Toward Agentic Multimodal Model
---

# DeepEyesV2: Toward Agentic Multimodal Model

**arXiv**: [2511.05271v1](https://arxiv.org/abs/2511.05271) | [PDF](https://arxiv.org/pdf/2511.05271.pdf)

**ä½œè€…**: Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, Xing Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepEyesV2ä»¥æž„å»ºä»£ç†å¼å¤šæ¨¡æ€æ¨¡åž‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒä¼˜åŒ–å·¥å…·è°ƒç”¨ã€‚**

**å…³é”®è¯**: `ä»£ç†å¼å¤šæ¨¡æ€æ¨¡åž‹` `å·¥å…·è°ƒç”¨è®­ç»ƒ` `ä¸¤é˜¶æ®µè®­ç»ƒ` `å¤šæ¨¡æ€åŸºå‡†è¯„ä¼°` `å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»£ç†å¼å¤šæ¨¡æ€æ¨¡åž‹éœ€ä¸»åŠ¨è°ƒç”¨å¤–éƒ¨å·¥å…·å¹¶æ•´åˆåˆ°æŽ¨ç†ä¸­ï¼Œä½†ç›´æŽ¥å¼ºåŒ–å­¦ä¹ éš¾ä»¥è¯±å¯¼ç¨³å¥å·¥å…·ä½¿ç”¨è¡Œä¸ºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨é˜¶æ®µå»ºç«‹å·¥å…·ä½¿ç”¨æ¨¡å¼å’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µç²¾ç‚¼å·¥å…·è°ƒç”¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨RealX-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡åž‹åœ¨çœŸå®žä¸–ç•Œç†è§£ã€æ•°å­¦æŽ¨ç†å’Œæœç´¢å¯†é›†åž‹ä»»åŠ¡ä¸­è¡¨çŽ°æœ‰æ•ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Agentic multimodal models should not only comprehend text and images, but
> also actively invoke external tools, such as code execution environments and
> web search, and integrate these operations into reasoning. In this work, we
> introduce DeepEyesV2 and explore how to build an agentic multimodal model from
> the perspectives of data construction, training methods, and model evaluation.
> We observe that direct reinforcement learning alone fails to induce robust
> tool-use behavior. This phenomenon motivates a two-stage training pipeline: a
> cold-start stage to establish tool-use patterns, and reinforcement learning
> stage to further refine tool invocation. We curate a diverse, moderately
> challenging training dataset, specifically including examples where tool use is
> beneficial. We further introduce RealX-Bench, a comprehensive benchmark
> designed to evaluate real-world multimodal reasoning, which inherently requires
> the integration of multiple capabilities, including perception, search, and
> reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative
> benchmarks, demonstrating its effectiveness across real-world understanding,
> mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2
> exhibits task-adaptive tool invocation, tending to use image operations for
> perception tasks and numerical computations for reasoning tasks. Reinforcement
> learning further enables complex tool combinations and allows model to
> selectively invoke tools based on context. We hope our study can provide
> guidance for community in developing agentic multimodal models.

