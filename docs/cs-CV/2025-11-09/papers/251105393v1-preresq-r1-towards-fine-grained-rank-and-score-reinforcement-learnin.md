---
layout: default
title: PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization
---

# PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization

**arXiv**: [2511.05393v1](https://arxiv.org/abs/2511.05393) | [PDF](https://arxiv.org/pdf/2511.05393.pdf)

**ä½œè€…**: Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPreResQ-R1æ¡†æž¶ï¼Œé€šè¿‡åå¥½-å“åº”è§£è€¦å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€è¯„åˆ†ä¸ŽæŽ’åºï¼Œæå‡è§†è§‰è´¨é‡è¯„ä¼°æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†è§‰è´¨é‡è¯„ä¼°` `å¼ºåŒ–å­¦ä¹ ` `åå¥½-å“åº”è§£è€¦` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†é¢‘è´¨é‡è¯„ä¼°` `æŽ¨ç†ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è´¨é‡è¯„ä¼°æ–¹æ³•ä¾èµ–ç›‘ç£å¾®è°ƒæˆ–ä»…æŽ’åºç›®æ ‡ï¼Œå¯¼è‡´æŽ¨ç†æµ…å±‚ã€åˆ†æ•°æ ¡å‡†å·®å’Œè·¨åŸŸæ³›åŒ–å¼±ã€‚
2. å¼•å…¥åŒåˆ†æ”¯å¥–åŠ±è®¾è®¡ï¼Œåˆ†ç¦»æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ï¼Œé‡‡ç”¨GRPOä¼˜åŒ–æŽ¨ç†è¿‡ç¨‹ã€‚
3. åœ¨å°‘é‡æ•°æ®ä¸Šå¾®è°ƒï¼Œåœ¨å¤šä¸ªIQAå’ŒVQAåŸºå‡†ä¸Šå®žçŽ°SOTAï¼ŒæŽ¨ç†è½¨è¿¹ä¸Žäººç±»æ„ŸçŸ¥å¯¹é½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
> visual fidelity. While recent multimodal large language models (MLLMs) show
> promise in reasoning about image and video quality, existing approaches mainly
> rely on supervised fine-tuning or rank-only objectives, resulting in shallow
> reasoning, poor score calibration, and limited cross-domain generalization. We
> propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
> framework that unifies absolute score regression and relative ranking
> consistency within a single reasoning-driven optimization scheme. Unlike prior
> QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
> separately models intra-sample response coherence and inter-sample preference
> alignment, optimized via Group Relative Policy Optimization (GRPO). This design
> encourages fine-grained, stable, and interpretable chain-of-thought reasoning
> about perceptual quality. To extend beyond static imagery, we further design a
> global-temporal and local-spatial data flow strategy for Video Quality
> Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
> 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
> VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
> and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
> produces human-aligned reasoning traces that reveal the perceptual cues
> underlying quality judgments. Code and model are available.

