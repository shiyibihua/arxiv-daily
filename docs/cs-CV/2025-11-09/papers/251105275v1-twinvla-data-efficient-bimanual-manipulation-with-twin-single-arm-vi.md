---
layout: default
title: TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models
---

# TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models

**arXiv**: [2511.05275v1](https://arxiv.org/abs/2511.05275) | [PDF](https://arxiv.org/pdf/2511.05275.pdf)

**ä½œè€…**: Hokyun Im, Euijin Jeong, Jianlong Fu, Andrey Kolobov, Youngwoon Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTwinVLAæ¡†æž¶ï¼Œé€šè¿‡ç»„åˆå•è‡‚VLAå®žçŽ°æ•°æ®é«˜æ•ˆçš„åŒè‡‚æ“ä½œã€‚**

**å…³é”®è¯**: `åŒè‡‚æ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æ¨¡å—åŒ–ç»„åˆ` `æ•°æ®æ•ˆçŽ‡` `æœºå™¨äººå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šåŒè‡‚æ“ä½œéœ€å¤§é‡åŒè‡‚æ•°æ®ï¼Œä½†å…¬å¼€æ•°æ®é›†å¤šä¸ºå•è‡‚æ¼”ç¤ºã€‚
2. æ–¹æ³•ï¼šç»„åˆä¸¤ä¸ªé¢„è®­ç»ƒå•è‡‚VLAï¼Œæž„å»ºæ¨¡å—åŒ–åŒè‡‚æ¨¡åž‹ã€‚
3. æ•ˆæžœï¼šåœ¨çœŸå®žä¸Žä»¿çœŸä»»åŠ¡ä¸­ï¼Œä¼˜äºŽå•å—æ¨¡åž‹ï¼Œç¼©å°ä¸Žé¡¶å°–æ¨¡åž‹å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action models (VLAs) trained on large-scale robotic datasets
> have demonstrated strong performance on manipulation tasks, including bimanual
> tasks. However, because most public datasets focus on single-arm
> demonstrations, adapting VLAs for bimanual tasks typically requires substantial
> additional bimanual data and fine-tuning. To address this challenge, we
> introduce TwinVLA, a modular framework that composes two copies of a pretrained
> single-arm VLA into a coordinated bimanual VLA. Unlike monolithic
> cross-embodiment models trained on mixtures of single-arm and bimanual data,
> TwinVLA improves both data efficiency and performance by composing pretrained
> single-arm policies. Across diverse bimanual tasks in real-world and simulation
> settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model
> without requiring any bimanual pretraining. Furthermore, it narrows the gap to
> state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual
> data and compute cost. These results establish our modular composition approach
> as a data-efficient and scalable path toward high-performance bimanual
> manipulation, leveraging public single-arm data.

