---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-20
---

# cs.CVï¼ˆ2025-08-20ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250814443v1-reconstruction-using-the-invisible-intuition-from-nir-and-metadata-f.html">Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting</a></td>
  <td>æå‡ºNIRSplatä»¥è§£å†³å†œä¸šåœºæ™¯ä¸‹3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14443v1" data-paper-url="./papers/250814443v1-reconstruction-using-the-invisible-intuition-from-nir-and-metadata-f.html" onclick="toggleFavorite(this, '2508.14443v1', 'Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250814682v1-gems-efficient-gaussian-splatting-for-extreme-motion-blur.html">GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</a></td>
  <td>æå‡ºGeMSæ¡†æ¶ä»¥è§£å†³æç«¯è¿åŠ¨æ¨¡ç³Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14682v1" data-paper-url="./papers/250814682v1-gems-efficient-gaussian-splatting-for-extreme-motion-blur.html" onclick="toggleFavorite(this, '2508.14682v1', 'GeMS: Efficient Gaussian Splatting for Extreme Motion Blur')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250814717v1-gsfix3d-diffusion-guided-repair-of-novel-views-in-gaussian-splatting.html">GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting</a></td>
  <td>æå‡ºGSFix3Dä»¥è§£å†³æç«¯è§†è§’ä¸‹çš„3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14717v1" data-paper-url="./papers/250814717v1-gsfix3d-diffusion-guided-repair-of-novel-views-in-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.14717v1', 'GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250814563v1-gogs-high-fidelity-geometry-and-relighting-for-glossy-objects-via-ga.html">GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</a></td>
  <td>æå‡ºGOGSä»¥è§£å†³å…‰æ»‘ç‰©ä½“é€†å‘æ¸²æŸ“ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14563v1" data-paper-url="./papers/250814563v1-gogs-high-fidelity-geometry-and-relighting-for-glossy-objects-via-ga.html" onclick="toggleFavorite(this, '2508.14563v1', 'GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250814729v1-multiscale-video-transformers-for-class-agnostic-segmentation-in-aut.html">Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving</a></td>
  <td>æå‡ºå¤šå°ºåº¦è§†é¢‘å˜æ¢å™¨ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„ç±»æ— å…³åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14729v1" data-paper-url="./papers/250814729v1-multiscale-video-transformers-for-class-agnostic-segmentation-in-aut.html" onclick="toggleFavorite(this, '2508.14729v1', 'Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250814597v1-reliable-smoke-detection-via-optical-flow-guided-feature-fusion-and-.html">Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling</a></td>
  <td>æå‡ºå…‰æµå¼•å¯¼ç‰¹å¾èåˆä¸å˜æ¢å™¨ä¸ç¡®å®šæ€§å»ºæ¨¡ä»¥å®ç°å¯é çƒŸé›¾æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14597v1" data-paper-url="./papers/250814597v1-reliable-smoke-detection-via-optical-flow-guided-feature-fusion-and-.html" onclick="toggleFavorite(this, '2508.14597v1', 'Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250814776v1-6-dof-object-tracking-with-event-based-optical-flow-and-frames.html">6-DoF Object Tracking with Event-based Optical Flow and Frames</a></td>
  <td>æå‡ºåŸºäºäº‹ä»¶ç›¸æœºçš„å…‰æµä¸RGBèåˆæ–¹æ³•ä»¥è§£å†³é«˜é€Ÿç‰©ä½“6è‡ªç”±åº¦è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14776v1" data-paper-url="./papers/250814776v1-6-dof-object-tracking-with-event-based-optical-flow-and-frames.html" onclick="toggleFavorite(this, '2508.14776v1', '6-DoF Object Tracking with Event-based Optical Flow and Frames')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250817437v2-pixie-fast-and-generalizable-supervised-learning-of-3d-physics-from-.html">Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels</a></td>
  <td>æå‡ºPIXIEä»¥è§£å†³3Dåœºæ™¯ç‰©ç†å±æ€§æ¨æ–­é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.17437v2" data-paper-url="./papers/250817437v2-pixie-fast-and-generalizable-supervised-learning-of-3d-physics-from-.html" onclick="toggleFavorite(this, '2508.17437v2', 'Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250814437v1-focus-frequency-optimized-conditioning-of-diffusion-models-for-mitig.html">FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation</a></td>
  <td>æå‡ºFOCUSä»¥è§£å†³æµ‹è¯•æ—¶é€‚åº”ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14437v1" data-paper-url="./papers/250814437v1-focus-frequency-optimized-conditioning-of-diffusion-models-for-mitig.html" onclick="toggleFavorite(this, '2508.14437v1', 'FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250814504v1-pb-iad-utilizing-multimodal-foundation-models-for-semantic-industria.html">PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</a></td>
  <td>æå‡ºPB-IADæ¡†æ¶ä»¥è§£å†³åŠ¨æ€åˆ¶é€ ç¯å¢ƒä¸­çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14504v1" data-paper-url="./papers/250814504v1-pb-iad-utilizing-multimodal-foundation-models-for-semantic-industria.html" onclick="toggleFavorite(this, '2508.14504v1', 'PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250814707v2-seeing-further-on-the-shoulders-of-giants-knowledge-inheritance-for-.html">Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</a></td>
  <td>æå‡ºçŸ¥è¯†ç»§æ‰¿æ–¹æ³•ä»¥æå‡è§†è§‰åŸºç¡€æ¨¡å‹çš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14707v2" data-paper-url="./papers/250814707v2-seeing-further-on-the-shoulders-of-giants-knowledge-inheritance-for-.html" onclick="toggleFavorite(this, '2508.14707v2', 'Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250816654v3-msnav-zero-shot-vision-and-language-navigation-with-dynamic-memory-a.html">MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning</a></td>
  <td>æå‡ºMSNavæ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„ç©ºé—´æ¨ç†ä¸è®°å¿†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.16654v3" data-paper-url="./papers/250816654v3-msnav-zero-shot-vision-and-language-navigation-with-dynamic-memory-a.html" onclick="toggleFavorite(this, '2508.16654v3', 'MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250814609v1-anchorsync-global-consistency-optimization-for-long-video-editing.html">AnchorSync: Global Consistency Optimization for Long Video Editing</a></td>
  <td>æå‡ºAnchorSyncä»¥è§£å†³é•¿è§†é¢‘ç¼–è¾‘ä¸­çš„ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14609v1" data-paper-url="./papers/250814609v1-anchorsync-global-consistency-optimization-for-long-video-editing.html" onclick="toggleFavorite(this, '2508.14609v1', 'AnchorSync: Global Consistency Optimization for Long Video Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250814562v1-locality-aware-concept-bottleneck-model.html">Locality-aware Concept Bottleneck Model</a></td>
  <td>æå‡ºå±€éƒ¨æ„ŸçŸ¥æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ä»¥è§£å†³æ¦‚å¿µå®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14562v1" data-paper-url="./papers/250814562v1-locality-aware-concept-bottleneck-model.html" onclick="toggleFavorite(this, '2508.14562v1', 'Locality-aware Concept Bottleneck Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250814359v1-taming-transformer-for-emotion-controllable-talking-face-generation.html">Taming Transformer for Emotion-Controllable Talking Face Generation</a></td>
  <td>æå‡ºæƒ…æ„Ÿå¯æ§çš„è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å…³ç³»å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14359v1" data-paper-url="./papers/250814359v1-taming-transformer-for-emotion-controllable-talking-face-generation.html" onclick="toggleFavorite(this, '2508.14359v1', 'Taming Transformer for Emotion-Controllable Talking Face Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250814604v1-ust-ssm-unified-spatio-temporal-state-space-models-for-point-cloud-v.html">UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling</a></td>
  <td>æå‡ºUST-SSMä»¥è§£å†³ç‚¹äº‘è§†é¢‘å»ºæ¨¡ä¸­çš„æ—¶ç©ºæ— åºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14604v1" data-paper-url="./papers/250814604v1-ust-ssm-unified-spatio-temporal-state-space-models-for-point-cloud-v.html" onclick="toggleFavorite(this, '2508.14604v1', 'UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250814483v3-vivid-vr-distilling-concepts-from-text-to-video-diffusion-transforme.html">Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</a></td>
  <td>æå‡ºVivid-VRä»¥è§£å†³è§†é¢‘æ¢å¤ä¸­çš„çº¹ç†çœŸå®æ„Ÿä¸æ—¶é—´ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14483v3" data-paper-url="./papers/250814483v3-vivid-vr-distilling-concepts-from-text-to-video-diffusion-transforme.html" onclick="toggleFavorite(this, '2508.14483v3', 'Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250814889v1-ms-clr-multi-skeleton-contrastive-learning-for-human-action-recognit.html">MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition</a></td>
  <td>æå‡ºå¤šéª¨æ¶å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä»¥è§£å†³åŠ¨ä½œè¯†åˆ«ä¸­çš„éª¨æ¶ç»“æ„å¤šæ ·æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14889v1" data-paper-url="./papers/250814889v1-ms-clr-multi-skeleton-contrastive-learning-for-human-action-recognit.html" onclick="toggleFavorite(this, '2508.14889v1', 'MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250814527v2-adversarial-generation-and-collaborative-evolution-of-safety-critica.html">Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</a></td>
  <td>æå‡ºScenGEæ¡†æ¶ä»¥ç”Ÿæˆå®‰å…¨å…³é”®åœºæ™¯ï¼Œæå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14527v2" data-paper-url="./papers/250814527v2-adversarial-generation-and-collaborative-evolution-of-safety-critica.html" onclick="toggleFavorite(this, '2508.14527v2', 'Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250814466v1-lookout-real-world-humanoid-egocentric-navigation.html">LookOut: Real-World Humanoid Egocentric Navigation</a></td>
  <td>æå‡ºLookOutä»¥è§£å†³äººå½¢æœºå™¨äººè‡ªæˆ‘ä¸­å¿ƒå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14466v1" data-paper-url="./papers/250814466v1-lookout-real-world-humanoid-egocentric-navigation.html" onclick="toggleFavorite(this, '2508.14466v1', 'LookOut: Real-World Humanoid Egocentric Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250814767v1-fusing-monocular-rgb-images-with-ais-data-to-create-a-6d-pose-estima.html">Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels</a></td>
  <td>é€šè¿‡èåˆå•ç›®RGBå›¾åƒä¸AISæ•°æ®è§£å†³æµ·æ´‹èˆ¹èˆ¶çš„6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14767v1" data-paper-url="./papers/250814767v1-fusing-monocular-rgb-images-with-ais-data-to-create-a-6d-pose-estima.html" onclick="toggleFavorite(this, '2508.14767v1', 'Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250814561v1-making-pose-representations-more-expressive-and-disentangled-via-res.html">Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization</a></td>
  <td>æå‡ºæ®‹å·®å‘é‡é‡åŒ–ä»¥å¢å¼ºå§¿æ€è¡¨ç¤ºçš„è¡¨è¾¾èƒ½åŠ›ä¸è§£è€¦æ€§</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14561v1" data-paper-url="./papers/250814561v1-making-pose-representations-more-expressive-and-disentangled-via-res.html" onclick="toggleFavorite(this, '2508.14561v1', 'Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250814891v2-gaussianart-unified-modeling-of-geometry-and-motion-for-articulated-.html">GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects</a></td>
  <td>æå‡ºGaussianArtä»¥è§£å†³å…³èŠ‚ç‰©ä½“é‡å»ºä¸­çš„å‡ ä½•ä¸è¿åŠ¨å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-scene interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14891v2" data-paper-url="./papers/250814891v2-gaussianart-unified-modeling-of-geometry-and-motion-for-articulated-.html" onclick="toggleFavorite(this, '2508.14891v2', 'GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)