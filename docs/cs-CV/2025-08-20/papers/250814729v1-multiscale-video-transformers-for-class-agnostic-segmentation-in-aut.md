---
layout: default
title: Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving
---

# Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14729" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14729v1</a>
  <a href="https://arxiv.org/pdf/2508.14729.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14729v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14729v1', 'Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leila Cheshmi, Mennatullah Siam

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

**å¤‡æ³¨**: 6 pages, 2 figures, 1 table

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå°ºåº¦è§†é¢‘å˜æ¢å™¨ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„ç±»æ— å…³åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç±»æ— å…³åˆ†å‰²` `å¤šå°ºåº¦è§†é¢‘å˜æ¢å™¨` `è‡ªåŠ¨é©¾é©¶` `æ—¶ç©ºç‰¹å¾` `è¿åŠ¨çº¿ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªçŸ¥ç‰©ä½“å’Œæ–°åœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé€šå¸¸ä¾èµ–äºå·²çŸ¥ç±»åˆ«ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘å˜æ¢å™¨ï¼Œé‡‡ç”¨å¤šé˜¶æ®µå¤šå°ºåº¦æŸ¥è¯¢-è®°å¿†è§£ç ï¼Œèƒ½å¤Ÿå®ç°ç±»æ— å…³åˆ†å‰²ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†å¤šå°ºåº¦åŸºçº¿ï¼ŒåŒæ—¶åœ¨GPUå†…å­˜å’Œè¿è¡Œæ—¶é—´ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¡®ä¿è‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§æ˜¯ä¸€é¡¹å¤æ‚çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†æœªçŸ¥ç‰©ä½“å’Œä¸å¯é¢„è§çš„é©¾é©¶åœºæ™¯ã€‚æœ¬æ–‡å¼€å‘äº†å¤šå°ºåº¦è§†é¢‘å˜æ¢å™¨ï¼Œèƒ½å¤Ÿä»…é€šè¿‡è¿åŠ¨çº¿ç´¢æ£€æµ‹æœªçŸ¥ç‰©ä½“ã€‚è§†é¢‘è¯­ä¹‰å’Œå…¨æ™¯åˆ†å‰²é€šå¸¸ä¾èµ–äºè®­ç»ƒæœŸé—´è§è¿‡çš„å·²çŸ¥ç±»åˆ«ï¼Œå¿½è§†äº†æ–°ç±»åˆ«ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘å˜æ¢å™¨ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯è®­ç»ƒï¼Œå®ç°ç±»æ— å…³åˆ†å‰²ï¼Œè€Œæ— éœ€å…‰æµã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µå¤šå°ºåº¦æŸ¥è¯¢-è®°å¿†è§£ç å’Œç‰¹å®šå°ºåº¦çš„éšæœºä¸¢å¼ƒæ ‡è®°ï¼Œä»¥ç¡®ä¿æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒè¯¦ç»†çš„æ—¶ç©ºç‰¹å¾ï¼Œåˆ©ç”¨å…±äº«çš„å¯å­¦ä¹ è®°å¿†æ¨¡å—ã€‚ä¸ä¼ ç»Ÿè§£ç å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„è®°å¿†ä¸­å¿ƒè®¾è®¡åœ¨å¤šä¸ªå°ºåº¦ä¸Šä¿ç•™é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨DAVIS'16ã€KITTIå’ŒCityscapesä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨GPUå†…å­˜å’Œè¿è¡Œæ—¶é—´ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†åœ¨å®‰å…¨å…³é”®æœºå™¨äººé¢†åŸŸè¿›è¡Œå®æ—¶ã€ç¨³å¥å¯†é›†é¢„æµ‹çš„è‰¯å¥½æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­ç±»æ— å…³åˆ†å‰²çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå·²çŸ¥ç±»åˆ«ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†æœªçŸ¥ç‰©ä½“å’Œæ–°åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§å¤šå°ºåº¦è§†é¢‘å˜æ¢å™¨ï¼Œé€šè¿‡è¿åŠ¨çº¿ç´¢è¿›è¡ŒæœªçŸ¥ç‰©ä½“æ£€æµ‹ï¼Œé‡‡ç”¨å¤šé˜¶æ®µå¤šå°ºåº¦æŸ¥è¯¢-è®°å¿†è§£ç ï¼Œé¿å…ä½¿ç”¨å…‰æµï¼Œä»è€Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šé˜¶æ®µè§£ç å™¨å’Œå…±äº«çš„å¯å­¦ä¹ è®°å¿†æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šä¿ç•™é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚è§£ç å™¨é€šè¿‡éšæœºä¸¢å¼ƒæ ‡è®°æ¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè®°å¿†ä¸­å¿ƒè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šæœ‰æ•ˆä¿ç•™æ—¶ç©ºç‰¹å¾ï¼Œä¸ä¼ ç»Ÿè§£ç å™¨ç›¸æ¯”ï¼Œé¿å…äº†ç‰¹å¾å‹ç¼©å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨ç‰¹å®šå°ºåº¦çš„éšæœºä¸¢å¼ƒæ ‡è®°ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç¡®ä¿äº†åˆ†å‰²ç²¾åº¦ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨DAVIS'16ã€KITTIå’ŒCityscapesæ•°æ®é›†ä¸Šå‡ä¼˜äºå¤šå°ºåº¦åŸºçº¿ï¼Œå°¤å…¶åœ¨GPUå†…å­˜å’Œè¿è¡Œæ—¶é—´ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†åœ¨å®æ—¶å¯†é›†é¢„æµ‹ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæé«˜å¯¹æœªçŸ¥ç‰©ä½“çš„æ£€æµ‹èƒ½åŠ›ï¼Œå¢å¼ºç³»ç»Ÿçš„å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯æ‰©å±•è‡³å…¶ä»–å®‰å…¨å…³é”®çš„æœºå™¨äººåº”ç”¨ï¼Œå¦‚æ— äººæœºç›‘æ§å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video transformers capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video transformer trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory decoding and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional decoders that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics.

