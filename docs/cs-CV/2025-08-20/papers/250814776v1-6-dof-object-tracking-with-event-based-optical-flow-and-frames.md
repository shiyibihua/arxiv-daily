---
layout: default
title: 6-DoF Object Tracking with Event-based Optical Flow and Frames
---

# 6-DoF Object Tracking with Event-based Optical Flow and Frames

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14776" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14776v1</a>
  <a href="https://arxiv.org/pdf/2508.14776.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14776v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14776v1', '6-DoF Object Tracking with Event-based Optical Flow and Frames')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhichao Li, Arren Glover, Chiara Bartolozzi, Lorenzo Natale

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºäº‹ä»¶ç›¸æœºçš„å…‰æµä¸RGBèåˆæ–¹æ³•ä»¥è§£å†³é«˜é€Ÿç‰©ä½“6è‡ªç”±åº¦è·Ÿè¸ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `6è‡ªç”±åº¦è·Ÿè¸ª` `äº‹ä»¶ç›¸æœº` `å…‰æµç®—æ³•` `ç‰©ä½“å§¿æ€ä¼°è®¡` `é«˜é€Ÿè¿åŠ¨` `æœºå™¨äººäº¤äº’` `å®æ—¶è·Ÿè¸ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é«˜é€Ÿç‰©ä½“è·Ÿè¸ªä¸­é¢ä¸´å¸§ç‡é™åˆ¶å’Œè¿åŠ¨æ¨¡ç³Šç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´è·Ÿè¸ªç²¾åº¦ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆäº‹ä»¶ç›¸æœºå…‰æµä¸RGBç›¸æœºçš„å…¨å±€å§¿æ€ä¼°è®¡å™¨çš„æ–¹æ³•ï¼Œä»¥å®ç°é«˜æ•ˆçš„6è‡ªç”±åº¦è·Ÿè¸ªã€‚
3. å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é€Ÿè¿åŠ¨åœºæ™¯ä¸­æ˜¾è‘—æå‡äº†è·Ÿè¸ªæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººé¢†åŸŸï¼Œå®æ—¶è·Ÿè¸ªç‰©ä½“åœ¨ç©ºé—´ä¸­çš„ä½ç½®å’Œæ–¹å‘ï¼ˆå³6è‡ªç”±åº¦ï¼‰æ˜¯ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚å½“ç‰©ä½“ä»¥é«˜é€Ÿç§»åŠ¨æ—¶ï¼Œä¼ ç»Ÿç›¸æœºç”±äºå¸§ç‡é™åˆ¶å’Œè¿åŠ¨æ¨¡ç³Šä½¿å¾—è¿™ä¸€ä»»åŠ¡å˜å¾—æ›´åŠ å›°éš¾ã€‚äº‹ä»¶ç›¸æœºå…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡ã€ä½å»¶è¿Ÿå’Œé«˜åŠ¨æ€èŒƒå›´ï¼Œå¯ä»¥æœ‰æ•ˆå…‹æœè¿åŠ¨æ¨¡ç³Šçš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆäº‹ä»¶åŸºç¡€å…‰æµå’ŒRGBå…¨å±€ç‰©ä½“å§¿æ€ä¼°è®¡å™¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜é€Ÿç‰©ä½“çš„6è‡ªç”±åº¦å§¿æ€è·Ÿè¸ªã€‚é€šè¿‡å°†è·Ÿè¸ªåˆ°çš„ç‰©ä½“6è‡ªç”±åº¦é€Ÿåº¦ä¸å…¨å±€å§¿æ€ä¼°è®¡å™¨çš„ä½é¢‘ä¼°è®¡å§¿æ€ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨é«˜é€Ÿè¿åŠ¨æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆè·Ÿè¸ªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é€Ÿè¿åŠ¨åœºæ™¯ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é«˜é€Ÿè¿åŠ¨ç‰©ä½“çš„6è‡ªç”±åº¦è·Ÿè¸ªé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é«˜é€Ÿæƒ…å†µä¸‹å®¹æ˜“å—åˆ°å¸§ç‡é™åˆ¶å’Œè¿åŠ¨æ¨¡ç³Šçš„å½±å“ï¼Œå¯¼è‡´è·Ÿè¸ªç²¾åº¦ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶æå‡ºç»“åˆäº‹ä»¶ç›¸æœºçš„å…‰æµç®—æ³•ä¸RGBç›¸æœºçš„å…¨å±€ç‰©ä½“å§¿æ€ä¼°è®¡å™¨ï¼Œé€šè¿‡åˆ©ç”¨ä¸¤ç§ä¼ æ„Ÿå™¨çš„ä¼˜åŠ¿ï¼Œæå‡é«˜é€Ÿç‰©ä½“çš„è·Ÿè¸ªèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨äº‹ä»¶ç›¸æœºè·å–ç‰©ä½“è¿åŠ¨çš„å…‰æµä¿¡æ¯ï¼›å…¶æ¬¡ï¼Œç»“åˆRGBç›¸æœºçš„å…¨å±€å§¿æ€ä¼°è®¡ï¼Œè¿›è¡Œä½é¢‘å§¿æ€ä¼°è®¡ä¸é«˜é¢‘é€Ÿåº¦è·Ÿè¸ªçš„èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡å°†äº‹ä»¶ç›¸æœºçš„å…‰æµç®—æ³•ä¸RGBç›¸æœºçš„å§¿æ€ä¼°è®¡ç›¸ç»“åˆï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨é«˜é€Ÿåœºæ™¯ä¸­çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å…‰æµè®¡ç®—çš„æ—¶é—´çª—å£å’Œå§¿æ€ä¼°è®¡çš„é¢‘ç‡è®¾ç½®ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†é€Ÿåº¦è·Ÿè¸ªä¸å§¿æ€ä¼°è®¡çš„è¯¯å·®ï¼Œä»¥ç¡®ä¿è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é«˜é€Ÿè¿åŠ¨åœºæ™¯ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†è·Ÿè¸ªç²¾åº¦ï¼Œå°¤å…¶åœ¨åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®é›†ä¸Šï¼Œè·Ÿè¸ªç²¾åº¦æé«˜äº†çº¦30%ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€ç¯å¢ƒä¸­çš„ç‰©ä½“è·Ÿè¸ªä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºå¯¼èˆªç­‰éœ€è¦å®æ—¶è·Ÿè¸ªé«˜é€Ÿç‰©ä½“çš„åœºæ™¯ã€‚é€šè¿‡æé«˜ç‰©ä½“è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººä¸ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tracking the position and orientation of objects in space (i.e., in 6-DoF) in real time is a fundamental problem in robotics for environment interaction. It becomes more challenging when objects move at high-speed due to frame rate limitations in conventional cameras and motion blur. Event cameras are characterized by high temporal resolution, low latency and high dynamic range, that can potentially overcome the impacts of motion blur. Traditional RGB cameras provide rich visual information that is more suitable for the challenging task of single-shot object pose estimation. In this work, we propose using event-based optical flow combined with an RGB based global object pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the core advantages of both types of vision sensors. Specifically, we propose an event-based optical flow algorithm for object motion measurement to implement an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF velocity with low frequency estimated pose from the global pose estimator, the method can track pose when objects move at high-speed. The proposed algorithm is tested and validated on both synthetic and real world data, demonstrating its effectiveness, especially in high-speed motion scenarios.

