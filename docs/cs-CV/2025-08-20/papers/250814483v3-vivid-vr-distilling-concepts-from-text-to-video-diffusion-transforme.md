---
layout: default
title: Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration
---

# Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14483" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14483v3</a>
  <a href="https://arxiv.org/pdf/2508.14483.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14483v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14483v3', 'Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-09-26)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/csbhr/Vivid-VR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVivid-VRä»¥è§£å†³è§†é¢‘æ¢å¤ä¸­çš„çº¹ç†çœŸå®æ„Ÿä¸æ—¶é—´ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¢å¤` `çº¹ç†çœŸå®æ„Ÿ` `æ—¶é—´ä¸€è‡´æ€§` `æ¦‚å¿µè’¸é¦` `å¤šæ¨¡æ€å¯¹é½` `ç”Ÿæˆæ¨¡å‹` `ControlNet` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†é¢‘æ¢å¤ä¸­é¢ä¸´çº¹ç†çœŸå®æ„Ÿå’Œæ—¶é—´ä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå¸¸å› å¤šæ¨¡æ€å¯¹é½ä¸å®Œå–„è€Œå¯¼è‡´åˆ†å¸ƒæ¼‚ç§»ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ¦‚å¿µè’¸é¦è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„T2Væ¨¡å‹åˆæˆè®­ç»ƒæ ·æœ¬ï¼Œä¿ç•™çº¹ç†å’Œæ—¶é—´è´¨é‡ã€‚
3. Vivid-VRåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨çº¹ç†çœŸå®æ„Ÿå’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Vivid-VRï¼Œä¸€ç§åŸºäºDiTçš„ç”Ÿæˆè§†é¢‘æ¢å¤æ–¹æ³•ï¼Œæ„å»ºäºå…ˆè¿›çš„T2VåŸºç¡€æ¨¡å‹ä¹‹ä¸Šã€‚é€šè¿‡åˆ©ç”¨ControlNetæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿å†…å®¹ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¯æ§ç®¡é“å¾®è°ƒå¸¸å› å¤šæ¨¡æ€å¯¹é½ä¸è¶³è€Œå¯¼è‡´åˆ†å¸ƒæ¼‚ç§»ï¼Œä»è€Œå½±å“çº¹ç†çœŸå®æ„Ÿå’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚å¿µè’¸é¦è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„T2Væ¨¡å‹åˆæˆåµŒå…¥æ–‡æœ¬æ¦‚å¿µçš„è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œä¿ç•™çº¹ç†å’Œæ—¶é—´è´¨é‡ã€‚ä¸ºå¢å¼ºç”Ÿæˆå¯æ§æ€§ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†æ§åˆ¶æ¶æ„ï¼ŒåŒ…å«æ§åˆ¶ç‰¹å¾æŠ•å½±å™¨å’ŒåŒåˆ†æ”¯ControlNetè¿æ¥å™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVivid-VRåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„çº¹ç†çœŸå®æ„Ÿã€è§†è§‰ç”ŸåŠ¨æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³è§†é¢‘æ¢å¤ä¸­çº¹ç†çœŸå®æ„Ÿå’Œæ—¶é—´ä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¸¸å› å¤šæ¨¡æ€å¯¹é½ä¸å®Œå–„è€Œå¯¼è‡´åˆ†å¸ƒæ¼‚ç§»ï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡æ¦‚å¿µè’¸é¦è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„T2Væ¨¡å‹åˆæˆå¸¦æœ‰æ–‡æœ¬æ¦‚å¿µçš„è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œå¢å¼ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„çº¹ç†å’Œæ—¶é—´è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ§åˆ¶ç‰¹å¾æŠ•å½±å™¨å’ŒåŒåˆ†æ”¯ControlNetè¿æ¥å™¨ã€‚æ§åˆ¶ç‰¹å¾æŠ•å½±å™¨ç”¨äºè¿‡æ»¤è¾“å…¥è§†é¢‘æ½œåœ¨çš„é™çº§ä¼ªå½±ï¼ŒåŒåˆ†æ”¯è¿æ¥å™¨ç»“åˆMLPç‰¹å¾æ˜ å°„ä¸äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°åŠ¨æ€æ§åˆ¶ç‰¹å¾çš„æå–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºçš„æ¦‚å¿µè’¸é¦è®­ç»ƒç­–ç•¥å’ŒåŒåˆ†æ”¯ControlNetè¿æ¥å™¨ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºæ›´å¥½åœ°ä¿ç•™äº†ç”Ÿæˆå†…å®¹çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æ§åˆ¶ç‰¹å¾æŠ•å½±å™¨çš„å‚æ•°è®¾ç½®ï¼Œä»¥åŠåŒåˆ†æ”¯è¿æ¥å™¨çš„ç½‘ç»œç»“æ„ï¼Œç¡®ä¿åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ•ˆå‡å°‘ä¼ªå½±ä¼ æ’­å¹¶å¢å¼ºæ§åˆ¶ä¿¡å·çš„è°ƒåˆ¶èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Vivid-VRåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨çº¹ç†çœŸå®æ„Ÿã€è§†è§‰ç”ŸåŠ¨æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†å½“å‰ä¸»æµæŠ€æœ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å½±è§†åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è§†é¢‘å†…å®¹çš„çœŸå®æ„Ÿå’Œè§†è§‰æ•ˆæœã€‚æœªæ¥ï¼ŒVivid-VRæœ‰æœ›åœ¨è‡ªåŠ¨åŒ–è§†é¢‘ç¼–è¾‘å’Œç”Ÿæˆé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Vivid-VR, a DiT-based generative video restoration method built upon an advanced T2V foundation model, where ControlNet is leveraged to control the generation process, ensuring content consistency. However, conventional fine-tuning of such controllable pipelines frequently suffers from distribution drift due to limitations in imperfect multimodal alignment, resulting in compromised texture realism and temporal coherence. To tackle this challenge, we propose a concept distillation training strategy that utilizes the pretrained T2V model to synthesize training samples with embedded textual concepts, thereby distilling its conceptual understanding to preserve texture and temporal quality. To enhance generation controllability, we redesign the control architecture with two key components: 1) a control feature projector that filters degradation artifacts from input video latents to minimize their propagation through the generation pipeline, and 2) a new ControlNet connector employing a dual-branch design. This connector synergistically combines MLP-based feature mapping with cross-attention mechanism for dynamic control feature retrieval, enabling both content preservation and adaptive control signal modulation. Extensive experiments show that Vivid-VR performs favorably against existing approaches on both synthetic and real-world benchmarks, as well as AIGC videos, achieving impressive texture realism, visual vividness, and temporal consistency. The codes and checkpoints are publicly available at https://github.com/csbhr/Vivid-VR.

