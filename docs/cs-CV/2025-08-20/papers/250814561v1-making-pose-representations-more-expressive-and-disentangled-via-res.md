---
layout: default
title: Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization
---

# Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14561" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14561v1</a>
  <a href="https://arxiv.org/pdf/2508.14561.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14561v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14561v1', 'Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sukhyun Jeong, Hong-Gi Shin, Yong-Hoon Choi

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ®‹å·®å‘é‡é‡åŒ–ä»¥å¢å¼ºå§¿æ€è¡¨ç¤ºçš„è¡¨è¾¾èƒ½åŠ›ä¸è§£è€¦æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `3Däººç±»è¿åŠ¨ç”Ÿæˆ` `å¯æ§è¿åŠ¨ç”Ÿæˆ` `æ®‹å·®å‘é‡é‡åŒ–` `å§¿æ€è¡¨ç¤º` `è¿åŠ¨æ§åˆ¶` `ç»†è‡´è¿åŠ¨ç‰¹å¾` `æ•°æ®é›†å®éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯æ§è¿åŠ¨ç”Ÿæˆæ–¹æ³•ä¾èµ–äºå§¿æ€ä»£ç è¡¨ç¤ºï¼Œä½†ç¦»æ•£å§¿æ€ä»£ç æ— æ³•æ•æ‰ç»†è‡´çš„è¿åŠ¨ç»†èŠ‚ï¼Œé™åˆ¶äº†å…¶è¡¨è¾¾èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰å°†è¿ç»­è¿åŠ¨ç‰¹å¾ä¸å§¿æ€ä»£ç ç›¸ç»“åˆï¼Œä»è€Œå¢å¼ºå§¿æ€è¡¨ç¤ºçš„è¡¨è¾¾èƒ½åŠ›å’Œå¯æ“æ§æ€§ã€‚
3. åœ¨HumanML3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹æ˜¾è‘—é™ä½äº†FIDå€¼ï¼Œå¹¶åœ¨Top-1 R-Precisionä¸Šå®ç°äº†å°å¹…æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°è¿åŠ¨çš„è¿›å±•æ¨åŠ¨äº†3Däººç±»è¿åŠ¨ç”Ÿæˆå’ŒåŸºäºæ–‡æœ¬çš„è¿åŠ¨æ§åˆ¶çš„å‘å±•ã€‚å¯æ§è¿åŠ¨ç”Ÿæˆï¼ˆCoMoï¼‰ä¾èµ–äºå§¿æ€ä»£ç è¡¨ç¤ºï¼Œä½†ç¦»æ•£å§¿æ€ä»£ç æ— æ³•æ•æ‰ç»†è‡´çš„è¿åŠ¨ç»†èŠ‚ï¼Œé™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰å°†è¿ç»­è¿åŠ¨ç‰¹å¾ä¸åŸºäºå§¿æ€ä»£ç çš„æ½œåœ¨è¡¨ç¤ºç›¸ç»“åˆã€‚è¿™ç§è®¾è®¡åœ¨æœ‰æ•ˆæ•æ‰é«˜é¢‘ç»†èŠ‚ç­‰å¾®å¦™è¿åŠ¨ç‰¹å¾çš„åŒæ—¶ï¼Œä¿æŒäº†å§¿æ€ä»£ç çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨HumanML3Dæ•°æ®é›†ä¸Šå°†Frechet inception distanceï¼ˆFIDï¼‰ä»0.041é™ä½åˆ°0.015ï¼ŒTop-1 R-Precisionä»0.508æå‡è‡³0.510ã€‚å¯¹å§¿æ€ä»£ç ä¹‹é—´çš„æ–¹å‘ç›¸ä¼¼æ€§çš„å®šæ€§åˆ†æè¿›ä¸€æ­¥ç¡®è®¤äº†æ¨¡å‹åœ¨è¿åŠ¨ç¼–è¾‘ä¸­çš„å¯æ§æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯æ§è¿åŠ¨ç”Ÿæˆæ–¹æ³•ä¸­ï¼Œç¦»æ•£å§¿æ€ä»£ç æ— æ³•æ•æ‰ç»†è‡´è¿åŠ¨ç»†èŠ‚çš„é—®é¢˜ï¼Œå¯¼è‡´è¡¨è¾¾èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰ï¼Œå°†è¿ç»­è¿åŠ¨ç‰¹å¾ä¸å§¿æ€ä»£ç ç»“åˆï¼Œå¢å¼ºäº†å§¿æ€è¡¨ç¤ºçš„ç»†è…»åº¦å’Œå¯æ“æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å§¿æ€ç¼–ç æ¨¡å—ã€è¿ç»­è¿åŠ¨ç‰¹å¾æå–æ¨¡å—å’Œæ®‹å·®å‘é‡é‡åŒ–æ¨¡å—ï¼Œç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆèåˆä¸è¡¨è¾¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ç¦»æ•£çš„å§¿æ€ä»£ç ä¸è¿ç»­è¿åŠ¨ç‰¹å¾ç›¸ç»“åˆï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„é™åˆ¶ï¼Œæå‡äº†è¿åŠ¨ç”Ÿæˆçš„ç»†è…»åº¦ä¸å¯æ§æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å§¿æ€ä»£ç å’Œè¿ç»­ç‰¹å¾çš„è´¡çŒ®ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨HumanML3Dæ•°æ®é›†ä¸Šå°†Frechet inception distanceï¼ˆFIDï¼‰ä»0.041é™ä½è‡³0.015ï¼Œè¡¨æ˜ç”Ÿæˆè´¨é‡æ˜¾è‘—æå‡ã€‚åŒæ—¶ï¼ŒTop-1 R-Precisionä»0.508æå‡è‡³0.510ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨è¿åŠ¨ç¼–è¾‘ä¸­çš„å¯æ§æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨3Däººç±»è¿åŠ¨ç”Ÿæˆå’ŒåŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å¢å¼ºå§¿æ€è¡¨ç¤ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°æ›´è‡ªç„¶çš„è¿åŠ¨æ§åˆ¶å’Œç¼–è¾‘ï¼Œæ¨åŠ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰è¡Œä¸šçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæå‡å…¶è¡¨ç°åŠ›å’Œå¯æ“æ§æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in text-to-motion has advanced both 3D human motion generation and text-based motion control. Controllable motion generation (CoMo), which enables intuitive control, typically relies on pose code representations, but discrete pose codes alone cannot capture fine-grained motion details, limiting expressiveness. To overcome this, we propose a method that augments pose code-based latent representations with continuous motion features using residual vector quantization (RVQ). This design preserves the interpretability and manipulability of pose codes while effectively capturing subtle motion characteristics such as high-frequency details. Experiments on the HumanML3D dataset show that our model reduces Frechet inception distance (FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510. Qualitative analysis of pairwise direction similarity between pose codes further confirms the model's controllability for motion editing.

