---
layout: default
title: Taming Transformer for Emotion-Controllable Talking Face Generation
---

# Taming Transformer for Emotion-Controllable Talking Face Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14359" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14359v1</a>
  <a href="https://arxiv.org/pdf/2508.14359.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14359v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14359v1', 'Taming Transformer for Emotion-Controllable Talking Face Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziqi Zhang, Cheng Deng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæƒ…æ„Ÿå¯æ§çš„è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å…³ç³»å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿç”Ÿæˆ` `è¯´è¯äººè„¸` `å¤šæ¨¡æ€å­¦ä¹ ` `è‡ªå›å½’å˜æ¢å™¨` `è§†è§‰æ ‡è®°` `éŸ³é¢‘è§£è€¦` `èº«ä»½ä¿ç•™` `æƒ…æ„Ÿé”šè¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æƒ…æ„Ÿå¯æ§çš„è¯´è¯äººè„¸ç”Ÿæˆä¸­é¢ä¸´å¤šæ¨¡æ€å…³ç³»å»ºæ¨¡å’Œèº«ä»½ä¿ç•™çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä¸¤ç§é¢„è®­ç»ƒç­–ç•¥è§£è€¦éŸ³é¢‘å’Œé‡åŒ–è§†é¢‘ï¼Œç»“åˆæƒ…æ„Ÿé”šè¡¨ç¤ºæ¥æ•´åˆæƒ…æ„Ÿä¿¡æ¯ã€‚
3. åœ¨MEADæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æƒ…æ„Ÿæ§åˆ¶æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·æœ‰æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯´è¯äººè„¸ç”Ÿæˆæ˜¯ä¸€é¡¹æ–°é¢–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ç‰¹å®šéŸ³é¢‘åˆæˆç”ŸåŠ¨çš„è¯´è¯è§†é¢‘ã€‚ä¸ºå®ç°æƒ…æ„Ÿå¯æ§çš„è¯´è¯äººè„¸ç”Ÿæˆï¼Œå½“å‰æ–¹æ³•éœ€å…‹æœä¸¤ä¸ªæŒ‘æˆ˜ï¼šä¸€æ˜¯å¦‚ä½•æœ‰æ•ˆå»ºæ¨¡ä¸ç‰¹å®šæƒ…æ„Ÿç›¸å…³çš„å¤šæ¨¡æ€å…³ç³»ï¼ŒäºŒæ˜¯å¦‚ä½•åˆ©ç”¨è¿™ä¸€å…³ç³»åˆæˆä¿ç•™èº«ä»½çš„æƒ…æ„Ÿè§†é¢‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé‡‡ç”¨ä¸¤ç§é¢„è®­ç»ƒç­–ç•¥å°†éŸ³é¢‘è§£è€¦ä¸ºç‹¬ç«‹æˆåˆ†ï¼Œå¹¶å°†è§†é¢‘é‡åŒ–ä¸ºè§†è§‰æ ‡è®°çš„ç»„åˆã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†æƒ…æ„Ÿé”šï¼ˆEAï¼‰è¡¨ç¤ºï¼Œå°†æƒ…æ„Ÿä¿¡æ¯æ•´åˆåˆ°è§†è§‰æ ‡è®°ä¸­ã€‚æœ€åï¼Œå¼•å…¥è‡ªå›å½’å˜æ¢å™¨å»ºæ¨¡è§†è§‰æ ‡è®°çš„å…¨å±€åˆ†å¸ƒï¼Œå¹¶é¢„æµ‹ç´¢å¼•åºåˆ—ä»¥åˆæˆæ“æ§åçš„è§†é¢‘ã€‚å®éªŒåœ¨MEADæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ§åˆ¶è§†é¢‘æƒ…æ„Ÿä»¥é€‚åº”å¤šç§æƒ…æ„ŸéŸ³é¢‘ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æƒ…æ„Ÿå¯æ§çš„è¯´è¯äººè„¸ç”Ÿæˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å…³ç³»å»ºæ¨¡å’Œæƒ…æ„Ÿè¡¨è¾¾ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æƒ…æ„Ÿè§†é¢‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è§£è€¦éŸ³é¢‘å’Œé‡åŒ–è§†é¢‘æ¥å¤„ç†æƒ…æ„Ÿä¿¡æ¯ï¼Œåˆ©ç”¨æƒ…æ„Ÿé”šè¡¨ç¤ºå°†æƒ…æ„Ÿä¿¡æ¯æ•´åˆåˆ°è§†è§‰æ ‡è®°ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘çš„æƒ…æ„Ÿæ§åˆ¶èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œé‡‡ç”¨é¢„è®­ç»ƒç­–ç•¥å°†éŸ³é¢‘è§£è€¦ä¸ºç‹¬ç«‹æˆåˆ†ï¼Œå¹¶å°†è§†é¢‘é‡åŒ–ä¸ºè§†è§‰æ ‡è®°ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨è‡ªå›å½’å˜æ¢å™¨å»ºæ¨¡è§†è§‰æ ‡è®°çš„å…¨å±€åˆ†å¸ƒï¼Œé¢„æµ‹ç”Ÿæˆè§†é¢‘çš„ç´¢å¼•åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºæƒ…æ„Ÿé”šï¼ˆEAï¼‰è¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæƒ…æ„Ÿä¿¡æ¯ä¸è§†è§‰æ ‡è®°ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„æƒ…æ„Ÿä¸€è‡´æ€§å’Œèº«ä»½ä¿ç•™èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æƒ…æ„Ÿè¡¨è¾¾ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§ç½‘ç»œç»“æ„ä»¥å¤„ç†ä¸åŒæƒ…æ„ŸéŸ³é¢‘çš„è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨MEADæ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•åœ¨æƒ…æ„Ÿæ§åˆ¶æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œç”Ÿæˆè§†é¢‘çš„æƒ…æ„Ÿä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œå½±è§†åˆ¶ä½œç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´åŠ ç”ŸåŠ¨å’Œæƒ…æ„Ÿä¸°å¯Œçš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½¿å¾—è™šæ‹Ÿè§’è‰²èƒ½å¤Ÿæ›´è‡ªç„¶åœ°è¡¨è¾¾æƒ…æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Talking face generation is a novel and challenging generation task, aiming at synthesizing a vivid speaking-face video given a specific audio. To fulfill emotion-controllable talking face generation, current methods need to overcome two challenges: One is how to effectively model the multimodal relationship related to the specific emotion, and the other is how to leverage this relationship to synthesize identity preserving emotional videos. In this paper, we propose a novel method to tackle the emotion-controllable talking face generation task discretely. Specifically, we employ two pre-training strategies to disentangle audio into independent components and quantize videos into combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA) representation that integrates the emotional information into visual tokens. Finally, we introduce an autoregressive transformer to model the global distribution of the visual tokens under the given conditions and further predict the index sequence for synthesizing the manipulated videos. We conduct experiments on the MEAD dataset that controls the emotion of videos conditioned on multiple emotional audios. Extensive experiments demonstrate the superiorities of our method both qualitatively and quantitatively.

