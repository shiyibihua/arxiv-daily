---
layout: default
title: StreamingVLM: Real-Time Understanding for Infinite Video Streams
---

# StreamingVLM: Real-Time Understanding for Infinite Video Streams

**arXiv**: [2510.09608v1](https://arxiv.org/abs/2510.09608) | [PDF](https://arxiv.org/pdf/2510.09608.pdf)

**ä½œè€…**: Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStreamingVLMä»¥è§£å†³æ— é™è§†é¢‘æµå®žæ—¶ç†è§£ä¸­çš„å»¶è¿Ÿä¸Žå†…å­˜é—®é¢˜**

**å…³é”®è¯**: `æ— é™è§†é¢‘æµç†è§£` `å®žæ—¶è§†è§‰è¯­è¨€æ¨¡åž‹` `KVç¼“å­˜ä¼˜åŒ–` `ç›‘ç£å¾®è°ƒç­–ç•¥` `é•¿è§†é¢‘åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤„ç†æ— é™è§†é¢‘æµæ—¶ï¼Œå…¨æ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´äºŒæ¬¡è®¡ç®—æˆæœ¬å’Œé•¿è§†é¢‘æ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤ç”¨æ³¨æ„åŠ›sinkçŠ¶æ€ã€çŸ­çª—å£è§†è§‰ä»¤ç‰Œå’Œé•¿çª—å£æ–‡æœ¬ä»¤ç‰Œï¼Œç»´æŒç´§å‡‘KVç¼“å­˜ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨Inf-Streams-EvalåŸºå‡†ä¸Šï¼Œä»¥66.18%èƒœçŽ‡ä¼˜äºŽGPT-4O miniï¼Œå®žæ—¶æ€§èƒ½è¾¾8 FPSã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models (VLMs) could power real-time assistants and autonomous
> agents, but they face a critical challenge: understanding near-infinite video
> streams without escalating latency and memory usage. Processing entire videos
> with full attention leads to quadratic computational costs and poor performance
> on long videos. Meanwhile, simple sliding window methods are also flawed, as
> they either break coherence or suffer from high latency due to redundant
> recomputation. In this paper, we introduce StreamingVLM, a model designed for
> real-time, stable understanding of infinite visual input. Our approach is a
> unified framework that aligns training with streaming inference. During
> inference, we maintain a compact KV cache by reusing states of attention sinks,
> a short window of recent vision tokens, and a long window of recent text
> tokens. This streaming ability is instilled via a simple supervised fine-tuning
> (SFT) strategy that applies full attention on short, overlapped video chunks,
> which effectively mimics the inference-time attention pattern without training
> on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a
> new benchmark with videos averaging over two hours that requires dense,
> per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM
> achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time
> performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy
> also enhances general VQA abilities without any VQA-specific fine-tuning,
> improving performance on LongVideoBench by +4.30 and OVOBench Realtime by
> +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.

