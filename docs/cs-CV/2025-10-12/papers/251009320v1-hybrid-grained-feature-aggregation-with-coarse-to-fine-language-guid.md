---
layout: default
title: Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation
---

# Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation

**arXiv**: [2510.09320v1](https://arxiv.org/abs/2510.09320) | [PDF](https://arxiv.org/pdf/2510.09320.pdf)

**ä½œè€…**: Wenyao Zhang, Hongsi Liu, Bohan Li, Jiawei He, Zekun Qi, Yunnan Wang, Shengyang Zhao, Xinqiang Yu, Wenjun Zeng, Xin Jin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆç²’åº¦ç‰¹å¾èšåˆæ¡†æž¶ï¼Œé€šè¿‡ç²—åˆ°ç»†è¯­è¨€å¼•å¯¼è§£å†³è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ä¸­è¯­ä¹‰-ç©ºé—´çŸ¥è¯†ä¸è¶³é—®é¢˜ã€‚**

**å…³é”®è¯**: `è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡` `æ··åˆç²’åº¦ç‰¹å¾èšåˆ` `è¯­è¨€å¼•å¯¼å­¦ä¹ ` `CLIPæ¨¡åž‹` `DINOæ¨¡åž‹` `KITTIåŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ä¸­è¯­ä¹‰-ç©ºé—´çŸ¥è¯†æå–ä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆCLIPå’ŒDINOæ¨¡åž‹ï¼Œé€šè¿‡å¯¹æ¯”è¯­è¨€å¼•å¯¼èšåˆå¤šç²’åº¦ç‰¹å¾ï¼Œå¹¶è®¾è®¡ä»£ç†ä»»åŠ¡è¿›è¡Œæ·±åº¦æ„ŸçŸ¥å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡å¦‚BEVæ„ŸçŸ¥æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Current self-supervised monocular depth estimation (MDE) approaches encounter
> performance limitations due to insufficient semantic-spatial knowledge
> extraction. To address this challenge, we propose Hybrid-depth, a novel
> framework that systematically integrates foundation models (e.g., CLIP and
> DINO) to extract visual priors and acquire sufficient contextual information
> for MDE. Our approach introduces a coarse-to-fine progressive learning
> framework: 1) Firstly, we aggregate multi-grained features from CLIP (global
> semantics) and DINO (local spatial details) under contrastive language
> guidance. A proxy task comparing close-distant image patches is designed to
> enforce depth-aware feature alignment using text prompts; 2) Next, building on
> the coarse features, we integrate camera pose information and pixel-wise
> language alignment to refine depth predictions. This module seamlessly
> integrates with existing self-supervised MDE pipelines (e.g., Monodepth2,
> ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth
> estimation. By aggregating CLIP's semantic context and DINO's spatial details
> through language guidance, our method effectively addresses feature granularity
> mismatches. Extensive experiments on the KITTI benchmark demonstrate that our
> method significantly outperforms SOTA methods across all metrics, which also
> indeed benefits downstream tasks like BEV perception. Code is available at
> https://github.com/Zhangwenyao1/Hybrid-depth.

