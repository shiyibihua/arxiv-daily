---
layout: default
title: Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning
---

# Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning

**arXiv**: [2510.08938v1](https://arxiv.org/abs/2510.08938) | [PDF](https://arxiv.org/pdf/2510.08938.pdf)

**ä½œè€…**: Zhen Yang, Yansong Ma, Lei Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…ƒç­–ç•¥æŽ§åˆ¶å™¨ä»¥è§£å†³åŠ¨æ€æ•°æ®åˆ†å¸ƒä¸‹ä¸ç¡®å®šæ€§æ ¡å‡†é—®é¢˜**

**å…³é”®è¯**: `å…ƒå­¦ä¹ ` `ä¸ç¡®å®šæ€§æ ¡å‡†` `è¯æ®æ·±åº¦å­¦ä¹ ` `åŒå±‚ä¼˜åŒ–` `Dirichletå…ˆéªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸEDLä¾èµ–é™æ€è¶…å‚æ•°ï¼Œåœ¨åŠ¨æ€æ•°æ®åˆ†å¸ƒä¸­æ ¡å‡†å’Œæ³›åŒ–èƒ½åŠ›å·®
2. é‡‡ç”¨åŒå±‚ä¼˜åŒ–ï¼šå†…å±‚åŠ¨æ€é…ç½®æŸå¤±å‡½æ•°ï¼Œå¤–å±‚ç­–ç•¥ç½‘ç»œä¼˜åŒ–KLç³»æ•°å’ŒDirichletå…ˆéªŒ
3. å®žéªŒæ˜¾ç¤ºæ˜¾è‘—æå‡ä¸ç¡®å®šæ€§æ ¡å‡†ã€é¢„æµ‹å‡†ç¡®æ€§å’Œç½®ä¿¡åº¦æ ·æœ¬æ‹’ç»åŽæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Traditional Evidence Deep Learning (EDL) methods rely on static
> hyperparameter for uncertainty calibration, limiting their adaptability in
> dynamic data distributions, which results in poor calibration and
> generalization in high-risk decision-making tasks. To address this limitation,
> we propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework
> that adjusts the KL divergence coefficient and Dirichlet prior strengths for
> optimal uncertainty modeling. Specifically, MPC employs a bi-level optimization
> approach: in the inner loop, model parameters are updated through a dynamically
> configured loss function that adapts to the current training state; in the
> outer loop, a policy network optimizes the KL divergence coefficient and
> class-specific Dirichlet prior strengths based on multi-objective rewards
> balancing prediction accuracy and uncertainty quality. Unlike previous methods
> with fixed priors, our learnable Dirichlet prior enables flexible adaptation to
> class distributions and training dynamics. Extensive experimental results show
> that MPC significantly enhances the reliability and calibration of model
> predictions across various tasks, improving uncertainty calibration, prediction
> accuracy, and performance retention after confidence-based sample rejection.

