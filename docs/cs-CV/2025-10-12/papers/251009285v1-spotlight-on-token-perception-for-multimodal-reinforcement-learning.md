---
layout: default
title: Spotlight on Token Perception for Multimodal Reinforcement Learning
---

# Spotlight on Token Perception for Multimodal Reinforcement Learning

**arXiv**: [2510.09285v1](https://arxiv.org/abs/2510.09285) | [PDF](https://arxiv.org/pdf/2510.09285.pdf)

**ä½œè€…**: Siyuan Huang, Xiaoye Qu, Yafu Li, Yun Luo, Zefeng He, Daizong Liu, Yu Cheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ä»¥å¢žå¼ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­çš„è§†è§‰æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ` `ä»¤ç‰Œæ„ŸçŸ¥` `è§†è§‰ä¾èµ–` `ç­–ç•¥ä¼˜åŒ–` `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `æŽ¨ç†åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å¿½è§†è§†è§‰æ„ŸçŸ¥åœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­çš„ä½œç”¨ï¼Œå¯¼è‡´å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›å—é™
2. å¼•å…¥ä»¤ç‰Œæ„ŸçŸ¥è§†è§’ï¼Œé€šè¿‡é‡æ–°åŠ æƒè½¨è¿¹ä¼˜åŠ¿å’Œèšç„¦å…³é”®ä»¤ç‰Œä¼˜åŒ–ç­–ç•¥
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the
> reasoning capabilities of Large Vision-Language Models (LVLMs), most existing
> methods in multimodal reasoning neglect the critical role of visual perception
> within the RLVR optimization process. In this paper, we undertake a pioneering
> exploration of multimodal RLVR through the novel perspective of token
> perception, which measures the visual dependency of each generated token. With
> a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key
> insights: first, token perception in a rollout trajectory is sparsely
> distributed, where only a small fraction of tokens have high visual dependency
> for visually-grounded reasoning; second, different trajectories exhibit
> significant divergence in their overall visual dependency. Based on these
> observations, we propose Visually-Perceptive Policy Optimization (VPPO), a
> novel policy gradient algorithm that explicitly leverages token perception to
> refine the learning signal. Specifically, VPPO achieves this through a dual
> mechanism: it reweights a trajectory's advantage by its overall visual
> dependency, and focuses policy updates exclusively on perceptually pivotal
> tokens. On a comprehensive suite of eight perception and reasoning benchmarks,
> VPPO demonstrates substantial gains over leading open-source RL-tuned models,
> with its effectiveness consistently validated across 7B and 32B model scales.
> Our findings not only establish a new token-level perceptual perspective for
> analyzing multimodal RLVR but also present a novel and effective optimization
> strategy to significantly enhance the multimodal reasoning capabilities of
> LVLMs.

