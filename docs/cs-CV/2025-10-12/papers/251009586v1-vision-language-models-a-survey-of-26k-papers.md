---
layout: default
title: Vision Language Models: A Survey of 26K Papers
---

# Vision Language Models: A Survey of 26K Papers

**arXiv**: [2510.09586v1](https://arxiv.org/abs/2510.09586) | [PDF](https://arxiv.org/pdf/2510.09586.pdf)

**ä½œè€…**: Fengming Lin

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æ26Kç¯‡è®ºæ–‡ä»¥é‡åŒ–è®¡ç®—æœºè§†è§‰ä¸è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¶‹åŠ¿**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€ç ”ç©¶` `ç”Ÿæˆæ–¹æ³•` `3Dè§†é¢‘åˆ†æ` `å‚æ•°é«˜æ•ˆé€‚åº”` `ç ”ç©¶è¶‹åŠ¿åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé‡åŒ–CVPRã€ICLRå’ŒNeurIPSä¼šè®®ä¸­è§†è§‰è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¶‹åŠ¿ä¸å®è§‚å˜åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ‰‹å·¥è¯å…¸å¯¹æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œåˆ†é…ä¸»é¢˜æ ‡ç­¾å¹¶æŒ–æ˜ä»»åŠ¡ç»†èŠ‚ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šè¯†åˆ«å¤šæ¨¡æ€ã€ç”Ÿæˆæ–¹æ³•å’Œ3Dè§†é¢‘æ´»åŠ¨çš„ä¸‰å¤§å®è§‚è½¬å˜ï¼Œå¹¶æ¯”è¾ƒä¸åŒä¼šè®®ç‰¹ç‚¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a transparent, reproducible measurement of research trends across
> 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles
> and abstracts are normalized, phrase-protected, and matched against a
> hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained
> cues about tasks, architectures, training regimes, objectives, datasets, and
> co-mentioned modalities. The analysis quantifies three macro shifts: (1) a
> sharp rise of multimodal vision-language-LLM work, which increasingly reframes
> classic perception as instruction following and multi-step reasoning; (2)
> steady expansion of generative methods, with diffusion research consolidating
> around controllability, distillation, and speed; and (3) resilient 3D and video
> activity, with composition moving from NeRFs to Gaussian splatting and a
> growing emphasis on human- and agent-centric understanding. Within VLMs,
> parameter-efficient adaptation like prompting/adapters/LoRA and lightweight
> vision-language bridges dominate; training practice shifts from building
> encoders from scratch to instruction tuning and finetuning strong backbones;
> contrastive objectives recede relative to cross-entropy/ranking and
> distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and
> ICLR the highest VLM share, while reliability themes such as efficiency or
> robustness diffuse across areas. We release the lexicon and methodology to
> enable auditing and extension. Limitations include lexicon recall and
> abstract-only scope, but the longitudinal signals are consistent across venues
> and years.

