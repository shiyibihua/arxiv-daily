---
layout: default
title: Utilizing dynamic sparsity on pretrained DETR
---

# Utilizing dynamic sparsity on pretrained DETR

**arXiv**: [2510.09380v1](https://arxiv.org/abs/2510.09380) | [PDF](https://arxiv.org/pdf/2510.09380.pdf)

**ä½œè€…**: Reza Sedghi, Anand Subramoney, David Kappel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€ç¨€ç–åŒ–æ–¹æ³•ä»¥æå‡é¢„è®­ç»ƒDETRåœ¨ç›®æ ‡æ£€æµ‹ä¸­çš„æŽ¨ç†æ•ˆçŽ‡**

**å…³é”®è¯**: `åŠ¨æ€ç¨€ç–åŒ–` `DETRæ¨¡åž‹` `ç›®æ ‡æ£€æµ‹` `æŽ¨ç†æ•ˆçŽ‡` `é—¨æŽ§æœºåˆ¶` `COCOæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformeræ¨¡åž‹åœ¨è§†è§‰ä»»åŠ¡ä¸­æŽ¨ç†æ•ˆçŽ‡ä½Žï¼ŒDETRçš„MLPå±‚å­˜åœ¨å›ºæœ‰ç¨€ç–æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥MGSè½»é‡é—¨æŽ§æœºåˆ¶ï¼Œé¢„æµ‹åŠ¨æ€ç¨€ç–æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨COCOæ•°æ®é›†ä¸Šï¼ŒMGSå®žçŽ°85-95%æ¿€æ´»ç¨€ç–ï¼Œä¿æŒæˆ–æå‡æ€§èƒ½å¹¶å‡å°‘è®¡ç®—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Efficient inference with transformer-based models remains a challenge,
> especially in vision tasks like object detection. We analyze the inherent
> sparsity in the MLP layers of DETR and introduce two methods to exploit it
> without retraining. First, we propose Static Indicator-Based Sparsification
> (SIBS), a heuristic method that predicts neuron inactivity based on fixed
> activation patterns. While simple, SIBS offers limited gains due to the
> input-dependent nature of sparsity. To address this, we introduce Micro-Gated
> Sparsification (MGS), a lightweight gating mechanism trained on top of a
> pretrained DETR. MGS predicts dynamic sparsity using a small linear layer and
> achieves up to 85 to 95% activation sparsity. Experiments on the COCO dataset
> show that MGS maintains or even improves performance while significantly
> reducing computation. Our method offers a practical, input-adaptive approach to
> sparsification, enabling efficient deployment of pretrained vision transformers
> without full model retraining.

