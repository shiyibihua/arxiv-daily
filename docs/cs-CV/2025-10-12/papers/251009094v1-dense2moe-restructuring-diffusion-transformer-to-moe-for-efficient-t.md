---
layout: default
title: Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation
---

# Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation

**arXiv**: [2510.09094v1](https://arxiv.org/abs/2510.09094) | [PDF](https://arxiv.org/pdf/2510.09094.pdf)

**ä½œè€…**: Youwei Zheng, Yuxi Ren, Xin Xia, Xuefeng Xiao, Xiaohua Xie

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDense2MoEå°†æ‰©æ•£Transformerè½¬æ¢ä¸ºMoEç»“æž„ä»¥é«˜æ•ˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ‰©æ•£Transformer` `ä¸“å®¶æ··åˆ` `æ¨¡åž‹åŽ‹ç¼©` `çŸ¥è¯†è’¸é¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£Transformerå‚æ•°å¤§å¯¼è‡´æŽ¨ç†å¼€é”€é«˜ï¼ŒçŽ°æœ‰å‰ªæžæ–¹æ³•æ˜“è‡´æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šç”¨MoEå±‚æ›¿æ¢FFNï¼Œå¼•å…¥MoBé€‰æ‹©æ€§æ¿€æ´»å—ï¼Œå¤šæ­¥è’¸é¦ä¼˜åŒ–è½¬æ¢
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¿€æ´»å‚æ•°å‡å°‘60%ï¼Œä¿æŒåŽŸæ€§èƒ½ï¼Œä¼˜äºŽå‰ªæžæ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion Transformer (DiT) has demonstrated remarkable performance in
> text-to-image generation; however, its large parameter size results in
> substantial inference overhead. Existing parameter compression methods
> primarily focus on pruning, but aggressive pruning often leads to severe
> performance degradation due to reduced model capacity. To address this
> limitation, we pioneer the transformation of a dense DiT into a Mixture of
> Experts (MoE) for structured sparsification, reducing the number of activated
> parameters while preserving model capacity. Specifically, we replace the
> Feed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number
> of activated parameters in the FFNs by 62.5\%. Furthermore, we propose the
> Mixture of Blocks (MoB) to selectively activate DiT blocks, thereby further
> enhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a
> multi-step distillation pipeline, incorporating Taylor metric-based expert
> initialization, knowledge distillation with load balancing, and group feature
> loss for MoB optimization. We transform large diffusion transformers (e.g.,
> FLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\%
> while maintaining original performance and surpassing pruning-based approaches
> in extensive experiments. Overall, Dense2MoE establishes a new paradigm for
> efficient text-to-image generation.

