---
layout: default
title: RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos
---

# RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos

**arXiv**: [2510.08936v1](https://arxiv.org/abs/2510.08936) | [PDF](https://arxiv.org/pdf/2510.08936.pdf)

**ä½œè€…**: Zixi Yang, Jiapeng Li, Muxi Diao, Yinuo Jing, Kongming Liang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRo-BenchåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨åäº‹å®žè§†é¢‘ä¸­çš„é²æ£’æ€§**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†é¢‘ç†è§£` `é²æ£’æ€§è¯„ä¼°` `åäº‹å®žæ•°æ®` `åˆ†å¸ƒå¤–æµ‹è¯•` `åŸºå‡†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œä½†é¢å¯¹æ“çºµè§†é¢‘å†…å®¹æ—¶é²æ£’æ€§æœªçŸ¥
2. é€šè¿‡ç¼–è¾‘é£Žæ ¼ã€å¯¹è±¡ã€èƒŒæ™¯åŠå…¶ç»„åˆï¼Œæž„å»ºåŠ¨æ€åˆ†å¸ƒå¤–åäº‹å®žè§†é¢‘æµ‹è¯•é›†
3. è¯„ä¼°æ˜¾ç¤ºæ¨¡åž‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œåäº‹å®žæ•°æ®å¾®è°ƒå¯æå‡é²æ£’æ€§ï¼Œåœ¨Ro-Benchå’ŒMVBenchä¸Šåˆ†åˆ«æé«˜21.73%å’Œ12.78%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Multi-modal Large Language Models (MLLMs) have demonstrated
> significant performance across various video understanding tasks. However,
> their robustness, particularly when faced with manipulated video content,
> remains largely unexplored. In this paper, we introduce Ro-Bench, the first
> benchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)
> counterfactual video test sets. Ro-Bench incorporates high-quality, diverse and
> temporally relevant video data, by editing Style, Object, Background and their
> compositions. We evaluated eight recent video MLLMs and found that current
> models exhibit substantial performance degradation on Ro-Bench when exposed to
> counterfactual video content. Furthermore, we demonstrate that fine-tuning
> MLLMs with counterfactual data enhances robustness, achieving a 21.73%
> performance increase on Ro-Bench and a 12.78% improvement across 20 tasks in
> the MVBench dataset. These findings underscore the effectiveness of
> counterfactual data in enhancing the video understanding ability of MLLMs. The
> code and data will be released shortly.

