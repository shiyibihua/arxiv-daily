---
layout: default
title: SpaceVista: All-Scale Visual Spatial Reasoning from mm to km
---

# SpaceVista: All-Scale Visual Spatial Reasoning from mm to km

**arXiv**: [2510.09606v1](https://arxiv.org/abs/2510.09606) | [PDF](https://arxiv.org/pdf/2510.09606.pdf)

**ä½œè€…**: Peiwen Sun, Shiqiang Lang, Dongming Wu, Yi Ding, Kaituo Feng, Huadai Liu, Zhen Ye, Rui Liu, Yun-Hui Liu, Jianan Wang, Xiangyu Yue

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpaceVistaä»¥è§£å†³å…¨å°ºåº¦è§†è§‰ç©ºé—´æŽ¨ç†é—®é¢˜ï¼Œæ¶µç›–æ¯«ç±³åˆ°åƒç±³åœºæ™¯ã€‚**

**å…³é”®è¯**: `å…¨å°ºåº¦ç©ºé—´æŽ¨ç†` `å°ºåº¦æ„ŸçŸ¥å»ºæ¨¡` `æ¸è¿›è®­ç»ƒ` `ç©ºé—´é—®ç­”æ•°æ®é›†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¾èµ–å®¤å†…3Dæ‰«æå’Œæ‰‹åŠ¨æ ‡æ³¨ï¼Œç¼ºä¹æœ‰æ•ˆå…¨å°ºåº¦åœºæ™¯å»ºæ¨¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆç»“æž„åŒ–çŸ¥è¯†ç³»ç»Ÿã€å°ºåº¦æ„ŸçŸ¥å»ºæ¨¡å’Œæ¸è¿›è®­ç»ƒèŒƒå¼ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ç«žäº‰æ€§ï¼Œå±•ç¤ºè·¨å°ºåº¦å’Œåœºæ™¯çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the current surge in spatial reasoning explorations, researchers have
> made significant progress in understanding indoor scenes, but still struggle
> with diverse applications such as robotics and autonomous driving. This paper
> aims to advance all-scale spatial reasoning across diverse scenarios by
> tackling two key challenges: 1) the heavy reliance on indoor 3D scans and
> labor-intensive manual annotations for dataset curation; 2) the absence of
> effective all-scale scene modeling, which often leads to overfitting to
> individual scenes. In this paper, we introduce a holistic solution that
> integrates a structured spatial reasoning knowledge system, scale-aware
> modeling, and a progressive training paradigm, as the first attempt to broaden
> the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using
> a task-specific, specialist-driven automated pipeline, we curate over 38K video
> scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising
> approximately 1M spatial QA pairs spanning 19 diverse task types. While
> specialist models can inject useful domain knowledge, they are not reliable for
> evaluation. We then build an all-scale benchmark with precise annotations by
> manually recording, retrieving, and assembling video-based data. However, naive
> training with SpaceVista-1M often yields suboptimal results due to the
> potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a
> spatial reasoning model that accepts dense inputs beyond semantics and uses
> scale as an anchor for scale-aware experts and progressive rewards. Finally,
> extensive evaluations across 5 benchmarks, including our SpaceVista-Bench,
> demonstrate competitive performance, showcasing strong generalization across
> all scales and scenarios. Our dataset, model, and benchmark will be released on
> https://peiwensun2000.github.io/mm2km .

