---
layout: default
title: HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images
---

# HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images

**arXiv**: [2510.08978v1](https://arxiv.org/abs/2510.08978) | [PDF](https://arxiv.org/pdf/2510.08978.pdf)

**ä½œè€…**: Zichuan Wang, Bo Peng, Songlin Yang, Zhenchen Tang, Jing Dong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHandEvalä»¥è¯„ä¼°ç”Ÿæˆå›¾åƒä¸­æ‰‹éƒ¨è´¨é‡ï¼Œæå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**

**å…³é”®è¯**: `æ‰‹éƒ¨è´¨é‡è¯„ä¼°` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ‰‹éƒ¨å…³é”®ç‚¹` `AIGCæ£€æµ‹` `æ•°æ®é›†æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹åœ¨å¤æ‚å±€éƒ¨åŒºåŸŸå¦‚æ‰‹éƒ¨ç”Ÿæˆç»†èŠ‚ä¸å‡†ç¡®ï¼Œå­˜åœ¨ç»“æž„æ‰­æ›²å’Œçº¹ç†ä¸çœŸå®žé—®é¢˜
2. åŸºäºŽHandPairæ•°æ®é›†å¼€å‘HandEvalæ¨¡åž‹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å’Œæ‰‹éƒ¨å…³é”®ç‚¹å…ˆéªŒçŸ¥è¯†
3. å®žéªŒæ˜¾ç¤ºHandEvalä¸Žäººç±»åˆ¤æ–­æ›´ä¸€è‡´ï¼Œé›†æˆåˆ°ç”Ÿæˆå’Œæ£€æµ‹ç®¡é“ä¸­æ˜¾è‘—æå‡æ‰‹éƒ¨çœŸå®žæ€§å’Œæ£€æµ‹å‡†ç¡®çŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Although recent text-to-image (T2I) models have significantly improved the
> overall visual quality of generated images, they still struggle in the
> generation of accurate details in complex local regions, especially human
> hands. Generated hands often exhibit structural distortions and unrealistic
> textures, which can be very noticeable even when the rest of the body is
> well-generated. However, the quality assessment of hand regions remains largely
> neglected, limiting downstream task performance like human-centric generation
> quality optimization and AIGC detection. To address this, we propose the first
> quality assessment task targeting generated hand regions and showcase its
> abundant downstream applications. We first introduce the HandPair dataset for
> training hand quality assessment models. It consists of 48k images formed by
> high- and low-quality hand pairs, enabling low-cost, efficient supervision
> without manual annotation. Based on it, we develop HandEval, a carefully
> designed hand-specific quality assessment model. It leverages the powerful
> visual understanding capability of Multimodal Large Language Model (MLLM) and
> incorporates prior knowledge of hand keypoints, gaining strong perception of
> hand quality. We further construct a human-annotated test set with hand images
> from various state-of-the-art (SOTA) T2I models to validate its quality
> evaluation capability. Results show that HandEval aligns better with human
> judgments than existing SOTA methods. Furthermore, we integrate HandEval into
> image generation and AIGC detection pipelines, prominently enhancing generated
> hand realism and detection accuracy, respectively, confirming its universal
> effectiveness in downstream applications. Code and dataset will be available.

