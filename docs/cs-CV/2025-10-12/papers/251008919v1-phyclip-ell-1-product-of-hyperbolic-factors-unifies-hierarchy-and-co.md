---
layout: default
title: PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning
---

# PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning

**arXiv**: [2510.08919v1](https://arxiv.org/abs/2510.08919) | [PDF](https://arxiv.org/pdf/2510.08919.pdf)

**ä½œè€…**: Daiki Yoshikawa, Takashi Matsubara

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPHyCLIPä»¥ç»Ÿä¸€è§†è§‰è¯­è¨€è¡¨ç¤ºä¸­çš„å±‚æ¬¡æ€§å’Œç»„åˆæ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ` `åŒæ›²ç©ºé—´` `â„“â‚-ç§¯åº¦é‡` `å±‚æ¬¡æ€§å»ºæ¨¡` `ç»„åˆæ€§å»ºæ¨¡` `é›¶æ ·æœ¬å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡åž‹éš¾ä»¥åŒæ—¶è¡¨è¾¾æ¦‚å¿µæ—å†…å±‚æ¬¡å’Œè·¨æ—ç»„åˆæ€§
2. ä½¿ç”¨åŒæ›²ç©ºé—´â„“â‚-ç§¯åº¦é‡ï¼Œåˆ†åˆ«æ•èŽ·å±‚æ¬¡å’Œç»„åˆç»“æž„
3. åœ¨é›¶æ ·æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ä¸­ä¼˜äºŽå•ç©ºé—´æ–¹æ³•ï¼ŒåµŒå…¥æ›´å¯è§£é‡Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language models have achieved remarkable success in multi-modal
> representation learning from large-scale pairs of visual scenes and linguistic
> descriptions. However, they still struggle to simultaneously express two
> distinct types of semantic structures: the hierarchy within a concept family
> (e.g., dog $\preceq$ mammal $\preceq$ animal) and the compositionality across
> different concept families (e.g., "a dog in a car" $\preceq$ dog, car). Recent
> works have addressed this challenge by employing hyperbolic space, which
> efficiently captures tree-like hierarchy, yet its suitability for representing
> compositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,
> which employs an $\ell_1$-Product metric on a Cartesian product of Hyperbolic
> factors. With our design, intra-family hierarchies emerge within individual
> hyperbolic factors, and cross-family composition is captured by the
> $\ell_1$-product metric, analogous to a Boolean algebra. Experiments on
> zero-shot classification, retrieval, hierarchical classification, and
> compositional understanding tasks demonstrate that PHyCLIP outperforms existing
> single-space approaches and offers more interpretable structures in the
> embedding space.

