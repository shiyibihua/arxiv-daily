---
layout: default
title: Towards Safer and Understandable Driver Intention Prediction
---

# Towards Safer and Understandable Driver Intention Prediction

**arXiv**: [2510.09200v1](https://arxiv.org/abs/2510.09200) | [PDF](https://arxiv.org/pdf/2510.09200.pdf)

**ä½œè€…**: Mukilan Karuppasamy, Shankar Gangisetty, Shyam Nandan Rai, Carlo Masone, C V Jawahar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†é¢‘æ¦‚å¿µç“¶é¢ˆæ¨¡åž‹ä»¥å¢žå¼ºè‡ªåŠ¨é©¾é©¶ä¸­é©¾é©¶å‘˜æ„å›¾é¢„æµ‹çš„å¯è§£é‡Šæ€§**

**å…³é”®è¯**: `é©¾é©¶å‘˜æ„å›¾é¢„æµ‹` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å¤šæ¨¡æ€æ•°æ®é›†` `è§†é¢‘ç†è§£` `Transformeræ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå†³ç­–è¿‡ç¨‹ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå½±å“äººæœºäº¤äº’å®‰å…¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥DAAD-Xæ•°æ®é›†å’ŒVCBMæ¡†æž¶ï¼Œç”Ÿæˆæ—¶ç©ºä¸€è‡´çš„è§£é‡Šã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºåŸºäºŽTransformerçš„æ¨¡åž‹æ¯”CNNæ›´å…·å¯è§£é‡Šæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous driving (AD) systems are becoming increasingly capable of handling
> complex tasks, mainly due to recent advances in deep learning and AI. As
> interactions between autonomous systems and humans increase, the
> interpretability of decision-making processes in driving systems becomes
> increasingly crucial for ensuring safe driving operations. Successful
> human-machine interaction requires understanding the underlying representations
> of the environment and the driving task, which remains a significant challenge
> in deep learning-based systems. To address this, we introduce the task of
> interpretability in maneuver prediction before they occur for driver safety,
> i.e., driver intent prediction (DIP), which plays a critical role in AD
> systems. To foster research in interpretable DIP, we curate the eXplainable
> Driving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric
> video dataset to provide hierarchical, high-level textual explanations as
> causal reasoning for the driver's decisions. These explanations are derived
> from both the driver's eye-gaze and the ego-vehicle's perspective. Next, we
> propose Video Concept Bottleneck Model (VCBM), a framework that generates
> spatio-temporally coherent explanations inherently, without relying on post-hoc
> techniques. Finally, through extensive evaluations of the proposed VCBM on the
> DAAD-X dataset, we demonstrate that transformer-based models exhibit greater
> interpretability than conventional CNN-based models. Additionally, we introduce
> a multilabel t-SNE visualization technique to illustrate the disentanglement
> and causal correlation among multiple explanations. Our data, code and models
> are available at: https://mukil07.github.io/VCBM.github.io/

