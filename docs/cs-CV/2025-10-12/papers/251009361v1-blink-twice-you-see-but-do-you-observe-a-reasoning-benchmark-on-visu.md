---
layout: default
title: BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception
---

# BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception

**arXiv**: [2510.09361v1](https://arxiv.org/abs/2510.09361) | [PDF](https://arxiv.org/pdf/2510.09361.pdf)

**ä½œè€…**: Junyan Ye, Dongzhi Jiang, Jun He, Baichuan Zhou, Zilong Huang, Zhiyuan Yan, Hongsheng Li, Conghui He, Weijia Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBLINK-TwiceåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨è§†è§‰æ„ŸçŸ¥æŽ¨ç†ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰æŽ¨ç†åŸºå‡†` `æ„ŸçŸ¥ä»»åŠ¡` `å¯¹æŠ—å›¾åƒå¯¹` `æŽ¨ç†é“¾è¯„ä¼°` `æ¨¡åž‹æ€§èƒ½åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†å¤šå…³æ³¨è¯­è¨€æŽ¨ç†ï¼Œè§†è§‰è¾“å…¥å¸¸è¢«å¿½ç•¥ï¼Œå¯¼è‡´è§†è§‰ä¸­å¿ƒæŽ¨ç†è¯„ä¼°ä¸è¶³
2. åŸºå‡†åŒ…å«ä¸ƒç±»è§†è§‰æŒ‘æˆ˜ã€è‡ªç„¶å¯¹æŠ—å›¾åƒå¯¹å’Œæ ‡æ³¨æŽ¨ç†é“¾ï¼Œå¼ºè°ƒä»Žè§†è§‰å†…å®¹æŽ¨ç†
3. è¯„ä¼°20ä¸ªé¢†å…ˆæ¨¡åž‹ï¼Œå‘çŽ°çŽ°æœ‰æŽ¨ç†ç­–ç•¥ä¸ç¨³å®šï¼Œé‡å¤è§‚å¯Ÿå’Œä¸»åŠ¨äº¤äº’å¯æå‡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,
> particularly in enhancing their reasoning capabilities. However, existing
> reasoning benchmarks still primarily assess language-based reasoning, often
> treating visual input as replaceable context. To address this gap, we introduce
> BLINK-Twice, a vision-centric reasoning benchmark grounded in challenging
> perceptual tasks. Instead of relying on external knowledge, our tasks require
> models to reason from visual content alone, shifting the focus from
> language-based to image-grounded reasoning. Compared to prior perception
> benchmarks, it moves beyond shallow perception ("see") and requires
> fine-grained observation and analytical reasoning ("observe"). BLINK-Twice
> integrates three core components: seven types of visual challenges for testing
> visual reasoning, natural adversarial image pairs that enforce reliance on
> visual content, and annotated reasoning chains for fine-grained evaluation of
> the reasoning process rather than final answers alone. We evaluate 20 leading
> MLLMs, including 12 foundation models and 8 reasoning-enhanced models.
> BLINK-Twice poses a significant challenge to current models. While existing
> reasoning strategies in the language space-such as chain-of-thought or
> self-criticism can improve performance, they often result in unstable and
> redundant reasoning. We observe that repeated image observation improves
> performance across models, and active visual interaction, as demonstrated by
> models like o3, highlights the need for a new paradigm for vision reasoning.
> The dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice

