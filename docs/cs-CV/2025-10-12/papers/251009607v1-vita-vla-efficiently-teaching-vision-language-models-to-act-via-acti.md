---
layout: default
title: VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation
---

# VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation

**arXiv**: [2510.09607v1](https://arxiv.org/abs/2510.09607) | [PDF](https://arxiv.org/pdf/2510.09607.pdf)

**ä½œè€…**: Shaoqi Dong, Chaoyou Fu, Haihan Gao, Yi-Fan Zhang, Chi Yan, Chu Wu, Xiaoyu Liu, Yunhang Shen, Jing Huo, Deqiang Jiang, Haoyu Cao, Yang Gao, Xing Sun, Ran He, Caifeng Shan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŠ¨ä½œä¸“å®¶è’¸é¦çš„æ¡†æž¶ï¼Œé«˜æ•ˆèµ‹äºˆè§†è§‰è¯­è¨€æ¨¡åž‹åŠ¨ä½œæ‰§è¡Œèƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `çŸ¥è¯†è’¸é¦` `æœºå™¨äººæ“ä½œ` `åŠ¨ä½œé¢„æµ‹` `é«˜æ•ˆè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹æˆæœ¬é«˜æ˜‚ï¼Œéœ€ä»Žé›¶å¼€å§‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ä¸¤é˜¶æ®µè’¸é¦ï¼Œé‡ç”¨é¢„è®­ç»ƒå°åŠ¨ä½œæ¨¡åž‹çŸ¥è¯†ï¼Œæ·»åŠ åŠ¨ä½œä»¤ç‰Œå’ŒçŠ¶æ€ç¼–ç å™¨ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨LIBEROå’ŒçœŸå®žä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒæˆåŠŸçŽ‡æ˜¾è‘—æå‡ï¼Œè®­ç»ƒæˆæœ¬é™ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Action (VLA) models significantly advance robotic
> manipulation by leveraging the strong perception capabilities of pretrained
> vision-language models (VLMs). By integrating action modules into these
> pretrained models, VLA methods exhibit improved generalization. However,
> training them from scratch is costly. In this work, we propose a simple yet
> effective distillation-based framework that equips VLMs with action-execution
> capability by transferring knowledge from pretrained small action models. Our
> architecture retains the original VLM structure, adding only an action token
> and a state encoder to incorporate physical inputs. To distill action
> knowledge, we adopt a two-stage training strategy. First, we perform
> lightweight alignment by mapping VLM hidden states into the action space of the
> small action model, enabling effective reuse of its pretrained action decoder
> and avoiding expensive pretraining. Second, we selectively fine-tune the
> language model, state encoder, and action modules, enabling the system to
> integrate multimodal inputs with precise action generation. Specifically, the
> action token provides the VLM with a direct handle for predicting future
> actions, while the state encoder allows the model to incorporate robot dynamics
> not captured by vision alone. This design yields substantial efficiency gains
> over training large VLA models from scratch. Compared with previous
> state-of-the-art methods, our method achieves 97.3% average success rate on
> LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In
> real-world experiments across five manipulation tasks, our method consistently
> outperforms the teacher model, achieving 82.0% success rate (17% improvement),
> which demonstrate that action distillation effectively enables VLMs to generate
> precise actions while substantially reducing training costs.

