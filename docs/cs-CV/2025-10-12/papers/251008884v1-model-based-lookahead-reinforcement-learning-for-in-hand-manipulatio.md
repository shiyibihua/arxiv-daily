---
layout: default
title: Model-Based Lookahead Reinforcement Learning for in-hand manipulation
---

# Model-Based Lookahead Reinforcement Learning for in-hand manipulation

**arXiv**: [2510.08884v1](https://arxiv.org/abs/2510.08884) | [PDF](https://arxiv.org/pdf/2510.08884.pdf)

**ä½œè€…**: Alexandre Lopes, Catarina Barata, Plinio Moreno

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆå¼ºåŒ–å­¦ä¹ æ¡†æž¶ä»¥æå‡æœºå™¨äººçµå·§æ“ä½œæ€§èƒ½**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `çµå·§æ“ä½œ` `æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶` `æœºå™¨äººä»¿çœŸ` `åŠ¨æ€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæœºå™¨äººçµå·§æ“ä½œä»»åŠ¡å¤æ‚ï¼Œæ¶‰åŠåŠ¨æ€ç³»ç»Ÿä¸Žå¤šå¯¹è±¡æŽ§åˆ¶æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ— æ¨¡åž‹ä¸ŽåŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡è½¨è¿¹è¯„ä¼°æŒ‡å¯¼ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ¨¡æ‹Ÿæ‰‹éƒ¨æµ‹è¯•ä¸­ï¼Œå¤šæ•°æƒ…å†µä¸‹æ€§èƒ½æå‡ï¼Œä½†è®¡ç®—æˆæœ¬å¢žåŠ ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In-Hand Manipulation, as many other dexterous tasks, remains a difficult
> challenge in robotics by combining complex dynamic systems with the capability
> to control and manoeuvre various objects using its actuators. This work
> presents the application of a previously developed hybrid Reinforcement
> Learning (RL) Framework to In-Hand Manipulation task, verifying that it is
> capable of improving the performance of the task. The model combines concepts
> of both Model-Free and Model-Based Reinforcement Learning, by guiding a trained
> policy with the help of a dynamic model and value-function through trajectory
> evaluation, as done in Model Predictive Control. This work evaluates the
> performance of the model by comparing it with the policy that will be guided.
> To fully explore this, various tests are performed using both fully-actuated
> and under-actuated simulated robotic hands to manipulate different objects for
> a given task. The performance of the model will also be tested for
> generalization tests, by changing the properties of the objects in which both
> the policy and dynamic model were trained, such as density and size, and
> additionally by guiding a trained policy in a certain object to perform the
> same task in a different one. The results of this work show that, given a
> policy with high average reward and an accurate dynamic model, the hybrid
> framework improves the performance of in-hand manipulation tasks for most test
> cases, even when the object properties are changed. However, this improvement
> comes at the expense of increasing the computational cost, due to the
> complexity of trajectory evaluation.

