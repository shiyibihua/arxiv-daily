---
layout: default
title: Training Feature Attribution for Vision Models
---

# Training Feature Attribution for Vision Models

**arXiv**: [2510.09135v1](https://arxiv.org/abs/2510.09135) | [PDF](https://arxiv.org/pdf/2510.09135.pdf)

**ä½œè€…**: Aziz Bacha, Thomas George

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè®­ç»ƒç‰¹å¾å½’å› æ–¹æ³•ï¼Œè”åˆåˆ†æžæµ‹è¯•é¢„æµ‹ä¸Žè®­ç»ƒå›¾åƒåŒºåŸŸï¼Œæå‡è§†è§‰æ¨¡åž‹å¯è§£é‡Šæ€§ã€‚**

**å…³é”®è¯**: `è®­ç»ƒç‰¹å¾å½’å› ` `è§†è§‰æ¨¡åž‹å¯è§£é‡Šæ€§` `æ·±åº¦ç¥žç»ç½‘ç»œ` `æµ‹è¯•é¢„æµ‹å½’å› ` `è™šå‡ç›¸å…³æ€§æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ·±åº¦ç¥žç»ç½‘ç»œè¢«è§†ä¸ºé»‘ç›’ï¼Œéœ€å¯è§£é‡Šæ€§æ–¹æ³•å¢žå¼ºä¿¡ä»»ä¸Žé—®è´£ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè”åˆå½’å› æµ‹è¯•é¢„æµ‹åˆ°ç‰¹å®šè®­ç»ƒå›¾åƒåŒºåŸŸï¼Œæä¾›ç»†ç²’åº¦è§£é‡Šã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†è§‰æ•°æ®é›†ä¸Šè¯†åˆ«æœ‰å®³æ ·æœ¬å’Œè™šå‡ç›¸å…³æ€§ï¼Œä¼˜äºŽä¼ ç»Ÿæ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep neural networks are often considered opaque systems, prompting the need
> for explainability methods to improve trust and accountability. Existing
> approaches typically attribute test-time predictions either to input features
> (e.g., pixels in an image) or to influential training examples. We argue that
> both perspectives should be studied jointly. This work explores *training
> feature attribution*, which links test predictions to specific regions of
> specific training images and thereby provides new insights into the inner
> workings of deep models. Our experiments on vision datasets show that training
> feature attribution yields fine-grained, test-specific explanations: it
> identifies harmful examples that drive misclassifications and reveals spurious
> correlations, such as patch-based shortcuts, that conventional attribution
> methods fail to expose.

