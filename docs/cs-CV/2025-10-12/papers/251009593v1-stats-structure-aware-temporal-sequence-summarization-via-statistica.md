---
layout: default
title: STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging
---

# STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging

**arXiv**: [2510.09593v1](https://arxiv.org/abs/2510.09593) | [PDF](https://arxiv.org/pdf/2510.09593.pdf)

**ä½œè€…**: Disharee Bhowmick, Ranjith Ramanathan, Sathyanarayanan N. Aakur

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTaTSæ¡†æž¶ä»¥è§£å†³æ—¶é—´åºåˆ—ç»“æž„æ„ŸçŸ¥åŽ‹ç¼©é—®é¢˜**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—åŽ‹ç¼©` `ç»“æž„æ„ŸçŸ¥å»ºæ¨¡` `æ— ç›‘ç£å­¦ä¹ ` `å˜ç‚¹æ£€æµ‹` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ—¶é—´åºåˆ—æ•°æ®å¸¸å«æ½œåœ¨ç»“æž„ï¼Œä½†çŽ°æœ‰æ–¹æ³•å¿½ç•¥ç»“æž„å¯¼è‡´æ•ˆçŽ‡ä½Žã€é²æ£’æ€§å·®
2. STaTSä½¿ç”¨BICå‡†åˆ™æ£€æµ‹å˜ç‚¹ï¼Œè‡ªé€‚åº”åŽ‹ç¼©åºåˆ—ä¸ºç´§å‡‘tokenï¼Œä¿ç•™æ ¸å¿ƒåŠ¨æ€
3. å®žéªŒæ˜¾ç¤ºSTaTSåœ¨150+æ•°æ®é›†ä¸Šå®žçŽ°é«˜åŽ‹ç¼©æ¯”ï¼Œä¿æŒ85-90%æ€§èƒ½å¹¶æå‡é²æ£’æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Time series data often contain latent temporal structure, transitions between
> locally stationary regimes, repeated motifs, and bursts of variability, that
> are rarely leveraged in standard representation learning pipelines. Existing
> models typically operate on raw or fixed-window sequences, treating all time
> steps as equally informative, which leads to inefficiencies, poor robustness,
> and limited scalability in long or noisy sequences. We propose STaTS, a
> lightweight, unsupervised framework for Structure-Aware Temporal Summarization
> that adaptively compresses both univariate and multivariate time series into
> compact, information-preserving token sequences. STaTS detects change points
> across multiple temporal resolutions using a BIC-based statistical divergence
> criterion, then summarizes each segment using simple functions like the mean or
> generative models such as GMMs. This process achieves up to 30x sequence
> compression while retaining core temporal dynamics. STaTS operates as a
> model-agnostic preprocessor and can be integrated with existing unsupervised
> time series encoders without retraining. Extensive experiments on 150+
> datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30
> archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,
> demonstrate that STaTS enables 85-90\% of the full-model performance while
> offering dramatic reductions in computational cost. Moreover, STaTS improves
> robustness under noise and preserves discriminative structure, outperforming
> uniform and clustering-based compression baselines. These results position
> STaTS as a principled, general-purpose solution for efficient, structure-aware
> time series modeling.

