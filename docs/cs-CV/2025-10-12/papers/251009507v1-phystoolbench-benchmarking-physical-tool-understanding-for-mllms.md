---
layout: default
title: PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs
---

# PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs

**arXiv**: [2510.09507v1](https://arxiv.org/abs/2510.09507) | [PDF](https://arxiv.org/pdf/2510.09507.pdf)

**ä½œè€…**: Zixin Zhang, Kanghao Chen, Xingwang Lin, Lutao Jiang, Xu Zheng, Yuanhuiyi Lyu, Litao Guo, Yinchuan Li, Ying-Cong Chen

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPhysToolBenchåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†å·¥å…·çš„ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `ç‰©ç†å·¥å…·ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰é—®ç­”åŸºå‡†` `å…·èº«AI` `å·¥å…·åˆ›é€ è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†å·¥å…·çš„ç†è§£èƒ½åŠ›å°šæœªé‡åŒ–ï¼Œå½±å“å…¶åœ¨å…·èº«AIä¸­çš„åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ„å»ºè§†è§‰é—®ç­”æ•°æ®é›†ï¼ŒåŒ…å«1000+å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œè¯„ä¼°å·¥å…·è¯†åˆ«ã€ç†è§£å’Œåˆ›é€ èƒ½åŠ›ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šè¯„ä¼°32ä¸ªæ¨¡å‹ï¼Œå‘ç°å·¥å…·ç†è§£å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå¹¶æå‡ºåˆæ­¥è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The ability to use, understand, and create tools is a hallmark of human
> intelligence, enabling sophisticated interaction with the physical world. For
> any general-purpose intelligent agent to achieve true versatility, it must also
> master these fundamental skills. While modern Multimodal Large Language Models
> (MLLMs) leverage their extensive common knowledge for high-level planning in
> embodied AI and in downstream Vision-Language-Action (VLA) models, the extent
> of their true understanding of physical tools remains unquantified. To bridge
> this gap, we present PhysToolBench, the first benchmark dedicated to evaluating
> the comprehension of physical tools by MLLMs. Our benchmark is structured as a
> Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.
> It assesses capabilities across three distinct difficulty levels: (1) Tool
> Recognition: Requiring the recognition of a tool's primary function. (2) Tool
> Understanding: Testing the ability to grasp the underlying principles of a
> tool's operation. (3) Tool Creation: Challenging the model to fashion a new
> tool from surrounding objects when conventional options are unavailable. Our
> comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,
> specialized embodied, and backbones in VLAs-reveals a significant deficiency in
> tool understanding. Furthermore, we provide an in-depth analysis and propose
> preliminary solutions. Code and dataset are publicly available.

