---
layout: default
title: Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy
---

# Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy

**arXiv**: [2510.09256v1](https://arxiv.org/abs/2510.09256) | [PDF](https://arxiv.org/pdf/2510.09256.pdf)

**ä½œè€…**: Patrick Wienholt, Sophie Caselitz, Robert Siepmann, Philipp Bruners, Keno Bressem, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»æ•£è¯­ä¹‰ç†µæ–¹æ³•ä»¥è¿‡æ»¤æ”¾å°„å­¦è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­çš„å¹»è§‰é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç¦»æ•£è¯­ä¹‰ç†µ` `æ”¾å°„å­¦è§†è§‰é—®ç­”` `å¹»è§‰è¿‡æ»¤` `è¯Šæ–­å‡†ç¡®æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé»‘ç›’è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æ”¾å°„å­¦è§†è§‰é—®ç­”ä¸­æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå½±å“è¯Šæ–­å‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç¦»æ•£è¯­ä¹‰ç†µé‡åŒ–è¯­ä¹‰ä¸ä¸€è‡´æ€§ï¼Œè¿‡æ»¤é«˜ç†µé—®é¢˜ä»¥å‡å°‘å¹»è§‰ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨706ä¸ªå›¾åƒ-é—®é¢˜å¯¹ä¸­ï¼Œè¿‡æ»¤åŽGPT-4oå‡†ç¡®çŽ‡ä»Ž51.7%æå‡è‡³76.3%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To determine whether using discrete semantic entropy (DSE) to reject
> questions likely to generate hallucinations can improve the accuracy of
> black-box vision-language models (VLMs) in radiologic image based visual
> question answering (VQA). This retrospective study evaluated DSE using two
> publicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500
> images with clinical questions and short-text answers) and (ii) a diagnostic
> radiology dataset (206 cases: 60 computed tomography scans, 60 magnetic
> resonance images, 60 radiographs, 26 angiograms) with corresponding
> ground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times
> using a temperature of 1.0. Baseline accuracy was determined using
> low-temperature answers (temperature 0.1). Meaning-equivalent responses were
> grouped using bidirectional entailment checks, and DSE was computed from the
> relative frequencies of the resulting semantic clusters. Accuracy was
> recalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and
> 95% confidence intervals were obtained using bootstrap resampling and a
> Bonferroni-corrected threshold of p < .004 for statistical significance. Across
> 706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for
> GPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on
> the remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and
> 63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains
> were observed across both datasets and largely remained statistically
> significant after Bonferroni correction. DSE enables reliable hallucination
> detection in black-box VLMs by quantifying semantic inconsistency. This method
> significantly improves diagnostic answer accuracy and offers a filtering
> strategy for clinical VLM applications.

