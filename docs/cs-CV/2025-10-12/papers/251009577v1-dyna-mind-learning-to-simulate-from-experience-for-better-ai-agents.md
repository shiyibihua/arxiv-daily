---
layout: default
title: Dyna-Mind: Learning to Simulate from Experience for Better AI Agents
---

# Dyna-Mind: Learning to Simulate from Experience for Better AI Agents

**arXiv**: [2510.09577v1](https://arxiv.org/abs/2510.09577) | [PDF](https://arxiv.org/pdf/2510.09577.pdf)

**ä½œè€…**: Xiao Yu, Baolin Peng, Michel Galley, Hao Cheng, Qianhui Wu, Janardhan Kulkarni, Suman Nath, Zhou Yu, Jianfeng Gao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDyna-Mindæ¡†æž¶ä»¥æå‡AIä»£ç†åœ¨å¤æ‚äº¤äº’ä»»åŠ¡ä¸­çš„æ¨¡æ‹Ÿä¸Žå†³ç­–èƒ½åŠ›**

**å…³é”®è¯**: `AIä»£ç†` `æ¨¡æ‹Ÿå­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `é•¿è§†é‡Žä»»åŠ¡` `æŽ¨ç†æ¨¡åž‹` `äº¤äº’çŽ¯å¢ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIä»£ç†åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸè¡¨çŽ°ä¼˜å¼‚ï¼Œä½†åœ¨é•¿è§†é‡Žäº¤äº’ä»»åŠ¡ä¸­æ€§èƒ½ä¸è¶³ï¼Œç¼ºä¹å¿ƒç†æ¨¡æ‹Ÿèƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼ŒReSimåŸºäºŽçœŸå®žç»éªŒç”Ÿæˆç»“æž„åŒ–æŽ¨ç†è½¨è¿¹ï¼ŒDyna-GRPOåˆ©ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡æ‹Ÿä¸Žå†³ç­–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Sokobanã€ALFWorldå’ŒAndroidWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒéªŒè¯äº†æ¨¡æ‹Ÿèƒ½åŠ›æå‡å’Œç­–ç•¥å­¦ä¹ æ•ˆæžœã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reasoning models have recently shown remarkable progress in domains such as
> math and coding. However, their expert-level abilities in math and coding
> contrast sharply with their performance in long-horizon, interactive tasks such
> as web navigation and computer/phone-use. Inspired by literature on human
> cognition, we argue that current AI agents need ''vicarious trial and error'' -
> the capacity to mentally simulate alternative futures before acting - in order
> to enhance their understanding and performance in complex interactive
> environments. We introduce Dyna-Mind, a two-stage training framework that
> explicitly teaches (V)LM agents to integrate such simulation into their
> reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which
> trains the agent to generate structured reasoning traces from expanded search
> trees built from real experience gathered through environment interactions.
> ReSim thus grounds the agent's reasoning in faithful world dynamics and equips
> it with the ability to anticipate future states in its reasoning. In stage 2,
> we propose Dyna-GRPO, an online reinforcement learning method to further
> strengthen the agent's simulation and decision-making ability by using both
> outcome rewards and intermediate states as feedback from real rollouts.
> Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one
> realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively
> infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome
> and interaction-level signals to learn better policies for long-horizon,
> planning-intensive tasks. Together, these results highlight the central role of
> simulation in enabling AI agents to reason, plan, and act more effectively in
> the ever more challenging environments.

