---
layout: default
title: Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving
---

# Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving

**arXiv**: [2511.19221v1](https://arxiv.org/abs/2511.19221) | [PDF](https://arxiv.org/pdf/2511.19221.pdf)

**ä½œè€…**: Jianhua Han, Meng Tian, Jiangtong Zhu, Fan He, Huixin Zhang, Sitong Guo, Dechang Zhu, Hao Tang, Pei Xu, Yuze Guo, Minzhe Niu, Haojie Zhu, Qichao Dong, Xuechao Yan, Siyuan Dong, Lu Hou, Qingqiu Huang, Xiaosong Jia, Hang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPercept-WAMæ¨¡åž‹ä»¥å¢žå¼ºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„æ„ŸçŸ¥é²æ£’æ€§**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `2D/3Dæ„ŸçŸ¥` `é•¿å°¾åœºæ™¯` `è½¨è¿¹è§„åˆ’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨é©¾é©¶ä¸­ç©ºé—´æ„ŸçŸ¥ä¸å‡†ç¡®å’Œä¸ç¨³å®šï¼Œå°¤å…¶åœ¨é•¿å°¾åœºæ™¯å’Œå¤æ‚äº¤äº’ä¸­ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»Ÿä¸€2D/3Dæ„ŸçŸ¥ä»»åŠ¡äºŽå•ä¸€è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œä½¿ç”¨World-PVå’ŒWorld-BEVä»¤ç‰Œç¼–ç ç©ºé—´ä¿¡æ¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨COCOå’ŒnuScenesåŸºå‡†ä¸Šè¶…è¶Šç»å…¸æ£€æµ‹å™¨ï¼Œé›†æˆè½¨è¿¹è§£ç å™¨æå‡è§„åˆ’æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

