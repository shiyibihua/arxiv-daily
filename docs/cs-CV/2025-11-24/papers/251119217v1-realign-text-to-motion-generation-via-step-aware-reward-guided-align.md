---
layout: default
title: ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment
---

# ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment

**arXiv**: [2511.19217v1](https://arxiv.org/abs/2511.19217) | [PDF](https://arxiv.org/pdf/2511.19217.pdf)

**ä½œè€…**: Wanjiang Weng, Xiaofeng Tan, Junbo Wang, Guo-Sen Xie, Pan Zhou, Hongsong Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReAlignæ–¹æ³•ï¼Œé€šè¿‡å¥–åŠ±å¼•å¯¼å¯¹é½è§£å†³æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `å¥–åŠ±å¼•å¯¼å¯¹é½` `æ­¥æ„ŸçŸ¥å¥–åŠ±` `è¯­ä¹‰ä¸€è‡´æ€§` `è¿åŠ¨è´¨é‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£æ¨¡åž‹ä¸­æ–‡æœ¬ä¸Žè¿åŠ¨åˆ†å¸ƒä¸åŒ¹é…ï¼Œå¯¼è‡´è¯­ä¹‰ä¸ä¸€è‡´æˆ–ä½Žè´¨é‡è¿åŠ¨
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ­¥æ„ŸçŸ¥å¥–åŠ±æ¨¡åž‹å’Œå¥–åŠ±å¼•å¯¼ç­–ç•¥ï¼Œä¼˜åŒ–åŽ»å™ªè¿‡ç¨‹ä»¥æå‡å¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç”Ÿæˆå’Œæ£€ç´¢ä»»åŠ¡ä¸­æ˜¾è‘—æ”¹è¿›æ–‡æœ¬-è¿åŠ¨å¯¹é½å’Œè¿åŠ¨è´¨é‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.

