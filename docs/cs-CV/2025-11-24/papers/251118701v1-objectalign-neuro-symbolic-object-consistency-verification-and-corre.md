---
layout: default
title: ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction
---

# ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction

**arXiv**: [2511.18701v1](https://arxiv.org/abs/2511.18701) | [PDF](https://arxiv.org/pdf/2511.18701.pdf)

**ä½œè€…**: Mustafa Munir, Harsh Goel, Xiwen Wei, Minkyu Choi, Sahil Shah, Kartikeya Bhardwaj, Paul Whatmough, Sandeep Chinchali, Radu Marculescu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºObjectAlignæ¡†æž¶ï¼Œé€šè¿‡ç¥žç»ç¬¦å·æ–¹æ³•æ£€æµ‹å’Œä¿®æ­£è§†é¢‘ç¼–è¾‘ä¸­çš„å¯¹è±¡ä¸ä¸€è‡´é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†é¢‘ç¼–è¾‘ä¸€è‡´æ€§` `ç¥žç»ç¬¦å·éªŒè¯` `å¯¹è±¡æ£€æµ‹ä¿®æ­£` `æ—¶é—´é€»è¾‘è§„èŒƒ` `è‡ªé€‚åº”å¸§ä¿®å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†é¢‘ç¼–è¾‘å¸¸å¯¼è‡´å¯¹è±¡é—ªçƒå’Œèº«ä»½æ¼‚ç§»ï¼Œé™ä½Žæ„ŸçŸ¥è´¨é‡ã€‚
2. ç»“åˆå¯å­¦ä¹ æŒ‡æ ‡é˜ˆå€¼ä¸Žç¥žç»ç¬¦å·éªŒè¯å™¨ï¼Œç¡®ä¿å¯¹è±¡ä¸€è‡´æ€§å’Œæ—¶é—´ä¿çœŸåº¦ã€‚
3. åœ¨DAVISå’ŒPexelsæ•°æ®é›†ä¸Šï¼ŒCLIPåˆ†æ•°å’Œwarpè¯¯å·®æ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.

