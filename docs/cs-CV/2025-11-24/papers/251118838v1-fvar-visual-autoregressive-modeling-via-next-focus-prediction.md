---
layout: default
title: FVAR: Visual Autoregressive Modeling via Next Focus Prediction
---

# FVAR: Visual Autoregressive Modeling via Next Focus Prediction

**arXiv**: [2511.18838v1](https://arxiv.org/abs/2511.18838) | [PDF](https://arxiv.org/pdf/2511.18838.pdf)

**‰ΩúËÄÖ**: Xiaofan Li, Chenming Wu, Yanpeng Sun, Jiaming Zhou, Delin Qu, Yansong Qu, Weihao Bo, Haibao Yu, Dingkang Liang

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FVARÈÄöËøá‰∏ã‰∏ÄÁÑ¶ÁÇπÈ¢ÑÊµãËß£ÂÜ≥ËßÜËßâËá™ÂõûÂΩíÊ®°Âûã‰∏≠ÁöÑÊ∑∑Âè†‰º™ÂΩ±ÈóÆÈ¢ò**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËá™ÂõûÂΩíÂª∫Ê®°` `Ê∑∑Âè†‰º™ÂΩ±Ê∂àÈô§` `Â§öÂ∞∫Â∫¶Ë°®Á§∫` `ÁÑ¶ÁÇπÈ¢ÑÊµã` `È´òÈ¢ëÊÆãÂ∑ÆÂ≠¶‰π†` `ÂõæÂÉèÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºö‰º†ÁªüÂ§öÂ∞∫Â∫¶Ëá™ÂõûÂΩíÊ®°ÂûãÂõ†ÂùáÂåÄ‰∏ãÈááÊ†∑‰∫ßÁîüÊ∑∑Âè†‰º™ÂΩ±ÔºåÊçüÂÆ≥ÁªÜËäÇÂíåÂºïÂÖ•ÈîØÈΩø
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÂºïÂÖ•‰∏ã‰∏ÄÁÑ¶ÁÇπÈ¢ÑÊµãËåÉÂºèÔºå‰ΩøÁî®Áâ©ÁêÜ‰∏ÄËá¥Êï£ÁÑ¶Ê†∏ÊûÑÂª∫Êó†Ê∑∑Âè†ÈáëÂ≠óÂ°îÔºåÁªìÂêàÈ´òÈ¢ëÊÆãÂ∑ÆÂ≠¶‰π†
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®ImageNet‰∏äÊòæËëóÂáèÂ∞ëÊ∑∑Âè†ÔºåÊèêÂçáÁªÜËäÇ‰øùÁïôÂíåÊñáÊú¨ÂèØËØªÊÄßÔºåÂÖºÂÆπÁé∞ÊúâÊ°ÜÊû∂

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moir√© patterns. To tackle this issue, we present \textbf{FVAR}, which reframes the paradigm from \emph{next-scale prediction} to \emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.

