---
layout: default
title: Mixture of Horizons in Action Chunking
---

# Mixture of Horizons in Action Chunking

**arXiv**: [2511.19433v1](https://arxiv.org/abs/2511.19433) | [PDF](https://arxiv.org/pdf/2511.19433.pdf)

**ä½œè€…**: Dong Jing, Gang Wang, Jiaqi Liu, Weiliang Tang, Zelong Sun, Yunchao Yao, Zhenyu Wei, Yunhui Liu, Zhiwu Lu, Mingyu Ding

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆè§†é‡Žç­–ç•¥ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­åŠ¨ä½œå—é•¿åº¦æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `åŠ¨ä½œå—é•¿åº¦` `æ··åˆè§†é‡Ž` `å¹¶è¡Œå¤„ç†` `åŠ¨æ€æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›ºå®šåŠ¨ä½œå—é•¿åº¦åœ¨æœºå™¨äººæ“ä½œä¸­å¯¼è‡´é•¿æœŸä»»åŠ¡ä¸Žç²¾ç»†æŽ§åˆ¶é—´çš„æƒè¡¡
2. æ–¹æ³•è¦ç‚¹ï¼šå°†åŠ¨ä½œå—åˆ†æ®µå¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨å…±äº«å˜æ¢å™¨å’Œçº¿æ€§é—¨èžåˆè¾“å‡º
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ··åˆä»»åŠ¡è®¾ç½®ä¸­è¾¾åˆ°99%å¹³å‡æˆåŠŸçŽ‡ï¼Œæå‡åžåé‡2.5å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $Ï€_0$, $Ï€_{0.5}$, and one-step regression policy $Ï€_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $Ï€_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons

