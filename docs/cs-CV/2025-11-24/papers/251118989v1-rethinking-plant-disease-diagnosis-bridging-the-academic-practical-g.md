---
layout: default
title: Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning
---

# Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning

**arXiv**: [2511.18989v1](https://arxiv.org/abs/2511.18989) | [PDF](https://arxiv.org/pdf/2511.18989.pdf)

**ä½œè€…**: Wassim Benabbas, Mohammed Brahimi, Samir Akhrouf, Bilal Fortas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æŽ¢ç´¢è§†è§‰å˜æ¢å™¨ä¸Žé›¶æ ·æœ¬å­¦ä¹ ä»¥å¼¥åˆæ¤ç‰©ç—…å®³åˆ†ç±»çš„å­¦æœ¯-å®žè·µå·®è·**

**å…³é”®è¯**: `æ¤ç‰©ç—…å®³åˆ†ç±»` `è§†è§‰å˜æ¢å™¨` `é›¶æ ·æœ¬å­¦ä¹ ` `é¢†åŸŸé€‚åº”` `CLIPæ¨¡åž‹` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºäºŽPlantVillageæ•°æ®é›†çš„æ¨¡åž‹åœ¨çœŸå®žå†œç”°å›¾åƒä¸Šæ³›åŒ–èƒ½åŠ›å·®ï¼Œå­˜åœ¨å­¦æœ¯-å®žè·µé¸¿æ²Ÿã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè¯„ä¼°CNNã€è§†è§‰å˜æ¢å™¨å’ŒåŸºäºŽCLIPçš„é›¶æ ·æœ¬æ¨¡åž‹ï¼ŒåŽè€…æ— éœ€ä»»åŠ¡è®­ç»ƒå³å¯åˆ†ç±»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè§†è§‰å˜æ¢å™¨æ³›åŒ–æ›´å¼ºï¼ŒCLIPæ¨¡åž‹æä¾›é«˜é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ï¼Œé›¶æ ·æœ¬å­¦ä¹ æ½œåŠ›æ˜¾è‘—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.

