---
layout: default
title: Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning
---

# Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning

**arXiv**: [2511.19343v1](https://arxiv.org/abs/2511.19343) | [PDF](https://arxiv.org/pdf/2511.19343.pdf)

**ä½œè€…**: Qihan Huang, Haofei Zhang, Rong Wei, Yi Wang, Rui Tang, Mingli Song, Jie Song

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSyn-GRPOä»¥è§£å†³MLLMå¼ºåŒ–å­¦ä¹ ä¸­æ•°æ®è´¨é‡ä½Žçš„é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®åˆæˆ` `è§†è§‰æ„ŸçŸ¥` `å¤šæ ·æ€§å¥–åŠ±` `è‡ªæ¼”åŒ–å¼ºåŒ–å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ•°æ®è´¨é‡ä½Žï¼Œæ ·æœ¬æ— æ³•æ¿€å‘MLLMå¤šæ ·å“åº”ï¼Œé™åˆ¶æŽ¢ç´¢èŒƒå›´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åœ¨çº¿æ•°æ®ç”Ÿæˆå™¨åˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œç»“åˆæ•°æ®æœåŠ¡å™¨å’ŒGRPOå·¥ä½œæµæå‡å¤šæ ·æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ•°æ®è´¨é‡å’Œæ€§èƒ½ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.

