---
layout: default
title: A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation
---

# A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation

**arXiv**: [2511.19004v1](https://arxiv.org/abs/2511.19004) | [PDF](https://arxiv.org/pdf/2511.19004.pdf)

**ä½œè€…**: Wentao Qu, Guofeng Mei, Yang Wu, Yongshun Gong, Xiaoshui Huang, Liang Xiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ¡ä»¶è¡¨ç¤ºå¼•å¯¼æ‰©æ•£æ¨¡åž‹ï¼Œç”¨äºŽæ–‡æœ¬åˆ°LiDARåœºæ™¯ç”Ÿæˆï¼Œæå‡ç»†èŠ‚ä¸Žå¯æŽ§æ€§ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°LiDARç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `è‡ªæ¡ä»¶è¡¨ç¤ºå¼•å¯¼` `åœºæ™¯ç”Ÿæˆ` `å¯æŽ§æ€§è¯„ä¼°` `å¤šæ¡ä»¶ä»»åŠ¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ–‡æœ¬-LiDARæ•°æ®ç¨€ç¼ºå¯¼è‡´ç”Ÿæˆåœºæ™¯è¿‡äºŽå¹³æ»‘ï¼Œä½Žè´¨é‡æ–‡æœ¬æè¿°å½±å“ç”Ÿæˆè´¨é‡ã€‚
2. å¼•å…¥è‡ªæ¡ä»¶è¡¨ç¤ºå¼•å¯¼ï¼Œåœ¨è®­ç»ƒä¸­å¯¹åŽ»å™ªç½‘ç»œæä¾›è½¯ç›‘ç£ï¼ŒæŽ¨ç†æ—¶è§£è€¦ä»¥æ„ŸçŸ¥å‡ ä½•ç»“æž„ã€‚
3. æž„å»ºT2nuScenesåŸºå‡†ï¼Œå®žéªŒæ˜¾ç¤ºæ¨¡åž‹åœ¨æ— æ¡ä»¶ä¸Žæ¡ä»¶ç”Ÿæˆä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.

