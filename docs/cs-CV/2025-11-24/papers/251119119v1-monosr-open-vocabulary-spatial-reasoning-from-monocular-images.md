---
layout: default
title: MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images
---

# MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images

**arXiv**: [2511.19119v1](https://arxiv.org/abs/2511.19119) | [PDF](https://arxiv.org/pdf/2511.19119.pdf)

**ä½œè€…**: Qirui Wang, Jingyi He, Yining Pan, Si Yong Yeo, Xulei Yang, Shijie Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMonoSRæ•°æ®é›†ä»¥è§£å†³å•ç›®å›¾åƒå¼€æ”¾è¯æ±‡ç©ºé—´æŽ¨ç†é—®é¢˜**

**å…³é”®è¯**: `å•ç›®å›¾åƒ` `ç©ºé—´æŽ¨ç†` `å¼€æ”¾è¯æ±‡` `æ•°æ®é›†æž„å»º` `æ¨¡åž‹è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç©ºé—´æŽ¨ç†ç ”ç©¶ä¾èµ–å¤šè§†å›¾ï¼Œéš¾ä»¥æ³›åŒ–åˆ°å•ç›®å›¾åƒå’Œå®¤å¤–åœºæ™¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡å•ç›®ç©ºé—´æŽ¨ç†æ•°æ®é›†ï¼Œæ¶µç›–å®¤å†…ã€å®¤å¤–å’Œç‰©ä½“ä¸­å¿ƒåœºæ™¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°å…ˆè¿›æ¨¡åž‹å±€é™æ€§ï¼Œåˆ†æžè¾…åŠ©ä¿¡æ¯é‡è¦æ€§ï¼Œæä¾›æœªæ¥æ¨¡åž‹è®¾è®¡æŒ‡å¯¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.

