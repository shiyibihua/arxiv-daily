---
layout: default
title: ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion
---

# ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion

**arXiv**: [2511.18742v1](https://arxiv.org/abs/2511.18742) | [PDF](https://arxiv.org/pdf/2511.18742.pdf)

**ä½œè€…**: Zhenghan Fang, Jian Zheng, Qiaozi Gao, Xiaofeng Gao, Jeremias Sulam

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProxT2Iæ–¹æ³•ï¼Œé€šè¿‡è¿‘ç«¯æ‰©æ•£å’Œå¥–åŠ±ä¼˜åŒ–æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ•ˆçŽ‡ä¸Žè´¨é‡ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `è¿‘ç«¯ç®—å­` `å¥–åŠ±ä¼˜åŒ–` `é«˜æ•ˆé‡‡æ ·` `è½»é‡æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŸºäºŽå‰å‘ç¦»æ•£åŒ–çš„æ‰©æ•£æ¨¡åž‹é‡‡æ ·æ…¢ä¸”ä¸ç¨³å®šï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŽå‘ç¦»æ•£åŒ–å’Œæ¡ä»¶è¿‘ç«¯ç®—å­æ›¿ä»£åˆ†æ•°å‡½æ•°ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é‡‡æ ·æ•ˆçŽ‡å’Œäººç±»åå¥½å¯¹é½ä¸Šä¼˜äºŽåŸºçº¿ï¼Œæ¨¡åž‹æ›´è½»é‡ä¸”æ€§èƒ½ç›¸å½“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.

