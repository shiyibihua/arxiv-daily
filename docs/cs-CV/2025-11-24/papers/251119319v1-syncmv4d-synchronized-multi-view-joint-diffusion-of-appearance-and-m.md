---
layout: default
title: SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis
---

# SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis

**arXiv**: [2511.19319v1](https://arxiv.org/abs/2511.19319) | [PDF](https://arxiv.org/pdf/2511.19319.pdf)

**ä½œè€…**: Lingwei Dang, Zonghan Li, Juntong Li, Hongwen Zhang, Liang An, Yebin Liu, Qingyao Wu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSyncMV4Dæ¨¡åž‹ï¼Œè”åˆç”ŸæˆåŒæ­¥å¤šè§†è§’æ‰‹ç‰©äº¤äº’è§†é¢‘ä¸Ž4Dè¿åŠ¨ï¼Œä»¥è§£å†³å•è§†è§’å‡ ä½•å¤±çœŸå’Œ3Dæ–¹æ³•æ³›åŒ–å·®é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ‰‹ç‰©äº¤äº’åˆæˆ` `å¤šè§†è§’è§†é¢‘ç”Ÿæˆ` `4Dè¿åŠ¨ç”Ÿæˆ` `è”åˆæ‰©æ•£æ¨¡åž‹` `ç‚¹äº‘å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå•è§†è§’HOIç”Ÿæˆæ˜“å¯¼è‡´å‡ ä½•å¤±çœŸï¼Œ3Dæ–¹æ³•ä¾èµ–å®žéªŒå®¤æ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šè§†è§’è”åˆæ‰©æ•£æ¨¡åž‹å’Œæ‰©æ•£ç‚¹å¯¹é½å™¨ï¼Œè€¦åˆ2Då¤–è§‚ä¸Ž4DåŠ¨æ€ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨è§†è§‰çœŸå®žæ„Ÿã€è¿åŠ¨åˆç†æ€§å’Œå¤šè§†è§’ä¸€è‡´æ€§ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

