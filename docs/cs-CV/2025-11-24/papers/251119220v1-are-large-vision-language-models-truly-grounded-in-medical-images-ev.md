---
layout: default
title: Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering
---

# Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering

**arXiv**: [2511.19220v1](https://arxiv.org/abs/2511.19220) | [PDF](https://arxiv.org/pdf/2511.19220.pdf)

**ä½œè€…**: Federico Felizzi, Olivia Riccomi, Michele Ferramola, Francesco Andrea Causio, Manuel Del Medico, Vittorio De Vita, Lorenzo De Mori, Alessandra Piscitelli Pietro Eric Risuleo, Bianca Destro Castaniti, Antonio Cristiano Alessia Longo, Luigi De Angelis, Mariapia Vassalli, Marcello Di Pumpo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æ„å¤§åˆ©åŒ»å­¦è§†è§‰é—®ç­”ä¸­çš„è§†è§‰ä¾èµ–æ€§ï¼Œæ­ç¤ºæ¨¡åž‹å·®å¼‚ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `åŒ»å­¦è§†è§‰é—®ç­”` `è§†è§‰ä¾èµ–æ€§è¯„ä¼°` `æ¨¡åž‹é²æ£’æ€§` `æ„å¤§åˆ©æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹æ˜¯å¦çœŸæ­£ä¾èµ–åŒ»å­¦å›¾åƒè¿›è¡Œè§†è§‰é—®ç­”ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç©ºç™½å›¾åƒæ›¿æ¢æµ‹è¯•æ¨¡åž‹è§†è§‰ä¾èµ–æ€§ï¼Œåˆ†æžå››ç§å‰æ²¿æ¨¡åž‹ã€‚
3. å®žéªŒæ•ˆæžœï¼šGPT-4oè§†è§‰ä¾èµ–æ€§æœ€å¼ºï¼Œå…¶ä»–æ¨¡åž‹ä¾èµ–æ–‡æœ¬æ·å¾„ï¼Œå‡†ç¡®çŽ‡ä¸‹é™ä¸ä¸€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.

