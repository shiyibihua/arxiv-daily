---
layout: default
title: Leveraging LLMs for reward function design in reinforcement learning control tasks
---

# Leveraging LLMs for reward function design in reinforcement learning control tasks

**arXiv**: [2511.19355v1](https://arxiv.org/abs/2511.19355) | [PDF](https://arxiv.org/pdf/2511.19355.pdf)

**ä½œè€…**: Franklin Cardenoso, Wouter Caarls

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLEARN-Optæ¡†æž¶ï¼Œä»¥è‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `å¤§è¯­è¨€æ¨¡åž‹` `è‡ªåŠ¨åŒ–æ¡†æž¶` `æ— ç›‘ç£è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡ä¾èµ–ä¸“å®¶çŸ¥è¯†ï¼Œè€—æ—¶ä¸”ä½Žæ•ˆ
2. LEARN-OptåŸºäºŽLLMè‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°å¥–åŠ±å‡½æ•°ï¼Œæ— éœ€é¢„å®šä¹‰æŒ‡æ ‡
3. å®žéªŒæ˜¾ç¤ºæ€§èƒ½åª²ç¾Žæˆ–ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œé™ä½Žå…ˆéªŒçŸ¥è¯†éœ€æ±‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

