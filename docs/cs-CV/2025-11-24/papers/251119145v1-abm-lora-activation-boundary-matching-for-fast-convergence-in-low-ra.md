---
layout: default
title: ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation
---

# ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation

**arXiv**: [2511.19145v1](https://arxiv.org/abs/2511.19145) | [PDF](https://arxiv.org/pdf/2511.19145.pdf)

**ä½œè€…**: Dongha Lee, Jinhee Park, Minjun Kim, Junseok Kwon

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºABM-LoRAä»¥åŠ é€Ÿä½Žç§©é€‚é…å™¨æ”¶æ•›ï¼Œé€šè¿‡æ¿€æ´»è¾¹ç•ŒåŒ¹é…ä¼˜åŒ–åˆå§‹åŒ–ã€‚**

**å…³é”®è¯**: `ä½Žç§©é€‚é…` `åˆå§‹åŒ–ç­–ç•¥` `æ¢¯åº¦æŠ•å½±` `æ¿€æ´»è¾¹ç•ŒåŒ¹é…` `æ¨¡åž‹å¾®è°ƒ` `æ”¶æ•›åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LoRAéšæœºåˆå§‹åŒ–å¯¼è‡´æ¢¯åº¦æ›´æ–°ä¸åŒ¹é…ï¼Œé€ æˆä¿¡æ¯æŸå¤±å’Œæ”¶æ•›ç¼“æ…¢ã€‚
2. ABM-LoRAåœ¨è®­ç»ƒå‰å¯¹é½é€‚é…å™¨ä¸Žé¢„è®­ç»ƒæ¨¡åž‹çš„æ¿€æ´»è¾¹ç•Œï¼Œæœ€å¤§åŒ–æ¢¯åº¦æŠ•å½±ã€‚
3. åœ¨è¯­è¨€ç†è§£ã€å¯¹è¯ç”Ÿæˆå’Œè§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒABM-LoRAæ˜¾è‘—æå‡æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®çŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.

