---
layout: default
title: Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers
---

# Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers

**arXiv**: [2511.18673v1](https://arxiv.org/abs/2511.18673) | [PDF](https://arxiv.org/pdf/2511.18673.pdf)

**ä½œè€…**: Yiqing Shi, Yiren Song, Mike Zheng Shou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEdit2Perceiveæ¡†æž¶ï¼Œåˆ©ç”¨å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡åž‹è¿›è¡Œå¯†é›†æ„ŸçŸ¥ä»»åŠ¡**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `å¯†é›†æ„ŸçŸ¥` `å›¾åƒç¼–è¾‘` `å‡ ä½•æ„ŸçŸ¥` `ä¸€è‡´æ€§æŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå¯†é›†æ„ŸçŸ¥æ–¹æ³•ä¾èµ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨ï¼Œç¼ºä¹å›¾åƒä¸€è‡´æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽFLUX.1 Kontextæž¶æž„ï¼Œé‡‡ç”¨å…¨å‚æ•°å¾®è°ƒå’Œåƒç´ ç©ºé—´ä¸€è‡´æ€§æŸå¤±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ·±åº¦ã€æ³•çº¿å’ŒæŠ å›¾ä»»åŠ¡ä¸­å®žçŽ°SOTAï¼ŒæŽ¨ç†é€Ÿåº¦æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.

