---
layout: default
title: VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models
---

# VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models

**arXiv**: [2511.18823v1](https://arxiv.org/abs/2511.18823) | [PDF](https://arxiv.org/pdf/2511.18823.pdf)

**ä½œè€…**: Fufangchen Zhao, Liao Zhang, Daiqi Shi, Yuanjun Gao, Chen Ye, Yang Cai, Jian Gao, Danfeng Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoPerceiverä»¥å¢žå¼ºè§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨ç»†ç²’åº¦æ—¶é—´æ„ŸçŸ¥ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç»†ç²’åº¦æ—¶é—´æ„ŸçŸ¥` `ä¸¤é˜¶æ®µè®­ç»ƒ` `å…³é”®ä¿¡æ¯ç¼ºå¤±` `ç›¸å¯¹å¥–åŠ±æœºåˆ¶` `ç½•è§äº‹ä»¶æè¿°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVMLLMsåœ¨çŸ­ç‰‡æ®µä¸­æŽ¨ç†çŸ­æš‚åŠ¨ä½œæˆ–é•¿è§†é¢‘ä¸­ç½•è§çž¬æ€äº‹ä»¶çš„èƒ½åŠ›æœ‰é™
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æž¶ï¼ŒåŒ…æ‹¬SFTä¸­çš„å…³é”®ä¿¡æ¯ç¼ºå¤±è§†é¢‘æž„å»ºå’ŒRLä¸­çš„ç›¸å¯¹å¥–åŠ±æœºåˆ¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç»†ç²’åº¦åŠ¨ä½œç†è§£å’Œç½•è§äº‹ä»¶æè¿°åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰VMLLMsï¼ŒåŒæ—¶ä¿æŒæ ‡å‡†ä»»åŠ¡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

