---
layout: default
title: From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation
---

# From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation

**arXiv**: [2511.19149v1](https://arxiv.org/abs/2511.19149) | [PDF](https://arxiv.org/pdf/2511.19149.pdf)

**ä½œè€…**: Moazzam Umer Gondal, Hamad Ul Qudous, Daniya Siddiqui, Asma Ahmad Farhan

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: Submitted to Expert Systems with Applications

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ£€ç´¢å¢žå¼ºçš„æ—¶å°šæè¿°ä¸Žæ ‡ç­¾ç”Ÿæˆæ¡†æž¶ï¼Œæå‡å±žæ€§ä¿çœŸåº¦å’Œé¢†åŸŸæ³›åŒ–æ€§ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ—¶å°šå›¾åƒæè¿°` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `å¤šæœè£…æ£€æµ‹` `å±žæ€§æŽ¨ç†` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `æœè£…æ ‡ç­¾ç”Ÿæˆ` `CLIP-FAISS` `é¢†åŸŸæ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç«¯åˆ°ç«¯æ—¶å°šå›¾åƒæè¿°æ¨¡åž‹åœ¨å±žæ€§ä¿çœŸåº¦å’Œé¢†åŸŸæ³›åŒ–æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å‡†ç¡®æè¿°æœè£…ç»†èŠ‚å’Œé£Žæ ¼ã€‚
2. æå‡ºä¸€ç§æ£€ç´¢å¢žå¼ºæ¡†æž¶ï¼Œåˆ©ç”¨å¤šæœè£…æ£€æµ‹ã€å±žæ€§æŽ¨ç†å’ŒLLMæç¤ºï¼Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·é£Žæ ¼çš„æ—¶å°šæè¿°å’Œæ ‡ç­¾ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶åœ¨å±žæ€§è¦†ç›–çŽ‡å’Œäº‹å®žåŸºç¡€æ–¹é¢ä¼˜äºŽåŸºçº¿æ¨¡åž‹BLIPï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ£€ç´¢å¢žå¼ºæ¡†æž¶ï¼Œç”¨äºŽè‡ªåŠ¨ç”Ÿæˆæ—¶å°šå›¾åƒçš„æè¿°å’Œæ ‡ç­¾ã€‚è¯¥æ¡†æž¶ç»“åˆäº†å¤šæœè£…æ£€æµ‹ã€å±žæ€§æŽ¨ç†å’Œå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æç¤ºã€‚æ—¨åœ¨ä¸ºæ—¶å°šå›¾åƒç”Ÿæˆè§†è§‰ä¸Šæœ‰å…³è”ã€æè¿°æ€§å¼ºä¸”é£Žæ ¼æœ‰è¶£çš„æ–‡æœ¬ï¼Œå…‹æœäº†ç«¯åˆ°ç«¯æè¿°å™¨åœ¨å±žæ€§ä¿çœŸåº¦å’Œé¢†åŸŸæ³›åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æµç¨‹ç»“åˆäº†åŸºäºŽYOLOçš„æ£€æµ‹å™¨è¿›è¡Œå¤šæœè£…å®šä½ï¼Œk-meansèšç±»æå–ä¸»è‰²è°ƒï¼Œä»¥åŠåŸºäºŽç»“æž„åŒ–äº§å“ç´¢å¼•çš„CLIP-FAISSæ£€ç´¢æ¨¡å—è¿›è¡Œé¢æ–™å’Œæ€§åˆ«å±žæ€§æŽ¨æ–­ã€‚è¿™äº›å±žæ€§ä¸Žæ£€ç´¢åˆ°çš„é£Žæ ¼ç¤ºä¾‹ä¸€èµ·ï¼Œåˆ›å»ºäº†ä¸€ä¸ªäº‹å®žè¯æ®åŒ…ï¼Œç”¨äºŽå¼•å¯¼LLMç”Ÿæˆç±»ä¼¼äººç±»çš„æè¿°å’Œä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ ‡ç­¾ã€‚ä½¿ç”¨å¾®è°ƒçš„BLIPæ¨¡åž‹ä½œä¸ºæœ‰ç›‘ç£çš„åŸºçº¿æ¨¡åž‹è¿›è¡Œæ¯”è¾ƒã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒYOLOæ£€æµ‹å™¨åœ¨ä¹ä¸ªæœè£…ç±»åˆ«ä¸­èŽ·å¾—äº†0.71çš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@0.5ï¼‰ã€‚RAG-LLMæµç¨‹ç”Ÿæˆäº†å¯Œæœ‰è¡¨çŽ°åŠ›çš„å±žæ€§å¯¹é½æè¿°ï¼Œå¹¶åœ¨æ ‡ç­¾ç”Ÿæˆä¸­å®žçŽ°äº†0.80çš„å¹³å‡å±žæ€§è¦†ç›–çŽ‡ï¼Œåœ¨50%é˜ˆå€¼ä¸‹å®žçŽ°äº†å®Œå…¨è¦†ç›–ï¼Œè€ŒBLIPæä¾›äº†æ›´é«˜çš„è¯æ±‡é‡å å’Œæ›´ä½Žçš„æ³›åŒ–èƒ½åŠ›ã€‚æ£€ç´¢å¢žå¼ºæ–¹æ³•è¡¨çŽ°å‡ºæ›´å¥½çš„äº‹å®žåŸºç¡€ã€æ›´å°‘çš„å¹»è§‰ï¼Œå¹¶ä¸”åœ¨å„ç§æœè£…é¢†åŸŸå…·æœ‰å·¨å¤§çš„å¯æ‰©å±•éƒ¨ç½²æ½œåŠ›ã€‚è¿™äº›ç»“æžœè¯æ˜Žäº†æ£€ç´¢å¢žå¼ºç”Ÿæˆä½œä¸ºä¸€ç§æœ‰æ•ˆä¸”å¯è§£é‡Šçš„èŒƒä¾‹ï¼Œç”¨äºŽè‡ªåŠ¨å’Œè§†è§‰åŸºç¡€çš„æ—¶å°šå†…å®¹ç”Ÿæˆã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ—¶å°šå›¾åƒæè¿°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç«¯åˆ°ç«¯æ¨¡åž‹ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆæè¿°çš„å±žæ€§å‡†ç¡®æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒæœè£…é¢†åŸŸæ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚è¿™äº›æ¨¡åž‹å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œæ— æ³•å‡†ç¡®æ•æ‰å›¾åƒä¸­æœè£…çš„ç»†èŠ‚å’Œé£Žæ ¼ç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æž¶ï¼Œé€šè¿‡æ£€ç´¢ä¸Žå›¾åƒç›¸å…³çš„å±žæ€§å’Œé£Žæ ¼ä¿¡æ¯ï¼Œä¸ºå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æä¾›æ›´ä¸°å¯Œçš„äº‹å®žä¾æ®ï¼Œä»Žè€Œå¼•å¯¼LLMç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·é£Žæ ¼çš„æè¿°å’Œæ ‡ç­¾ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘å¹»è§‰ï¼Œæé«˜å±žæ€§ä¿çœŸåº¦ï¼Œå¹¶å¢žå¼ºé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) åŸºäºŽYOLOçš„å¤šæœè£…æ£€æµ‹å™¨ï¼Œç”¨äºŽå®šä½å›¾åƒä¸­çš„å„ä¸ªæœè£…ï¼›2) k-meansèšç±»ï¼Œç”¨äºŽæå–æœè£…çš„ä¸»è‰²è°ƒï¼›3) CLIP-FAISSæ£€ç´¢æ¨¡å—ï¼ŒåŸºäºŽç»“æž„åŒ–äº§å“ç´¢å¼•æŽ¨æ–­æœè£…çš„é¢æ–™å’Œæ€§åˆ«å±žæ€§ï¼›4) LLMï¼Œç”¨äºŽç”Ÿæˆæè¿°å’Œæ ‡ç­¾ï¼Œå…¶è¾“å…¥åŒ…æ‹¬æ£€æµ‹åˆ°çš„æœè£…ã€æå–çš„å±žæ€§å’Œæ£€ç´¢åˆ°çš„é£Žæ ¼ç¤ºä¾‹ã€‚æ•´ä¸ªæµç¨‹é¦–å…ˆå¯¹å›¾åƒè¿›è¡Œåˆ†æžï¼Œæå–ç›¸å…³ä¿¡æ¯ï¼Œç„¶åŽå°†è¿™äº›ä¿¡æ¯ä½œä¸ºæç¤ºè¾“å…¥LLMï¼Œæœ€åŽç”±LLMç”Ÿæˆæœ€ç»ˆçš„æè¿°å’Œæ ‡ç­¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†æ£€ç´¢å¢žå¼ºç”Ÿæˆåº”ç”¨äºŽæ—¶å°šå›¾åƒæè¿°å’Œæ ‡ç­¾ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡æ£€ç´¢ä¸Žå›¾åƒç›¸å…³çš„å±žæ€§å’Œé£Žæ ¼ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºLLMæä¾›æ›´ä¸°å¯Œçš„äº‹å®žä¾æ®ï¼Œä»Žè€Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·é£Žæ ¼çš„æè¿°å’Œæ ‡ç­¾ã€‚ä¸Žä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ¨¡åž‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„å±žæ€§ä¿çœŸåº¦ã€æ›´å°‘çš„å¹»è§‰å’Œæ›´å¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CLIP-FAISSæ£€ç´¢æ¨¡å—ä¸­ï¼Œæž„å»ºäº†ä¸€ä¸ªç»“æž„åŒ–çš„äº§å“ç´¢å¼•ï¼Œç”¨äºŽå­˜å‚¨æœè£…çš„å±žæ€§ä¿¡æ¯ã€‚åœ¨LLMæç¤ºæ–¹é¢ï¼Œè®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„äº‹å®žè¯æ®åŒ…ï¼ŒåŒ…æ‹¬æ£€æµ‹åˆ°çš„æœè£…ã€æå–çš„å±žæ€§å’Œæ£€ç´¢åˆ°çš„é£Žæ ¼ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹BLIPæ¨¡åž‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½œä¸ºæœ‰ç›‘ç£çš„åŸºçº¿æ¨¡åž‹è¿›è¡Œæ¯”è¾ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶åœ¨ä¹ä¸ªæœè£…ç±»åˆ«ä¸­èŽ·å¾—äº†0.71çš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@0.5ï¼‰ã€‚RAG-LLMæµç¨‹ç”Ÿæˆäº†å¯Œæœ‰è¡¨çŽ°åŠ›çš„å±žæ€§å¯¹é½æè¿°ï¼Œå¹¶åœ¨æ ‡ç­¾ç”Ÿæˆä¸­å®žçŽ°äº†0.80çš„å¹³å‡å±žæ€§è¦†ç›–çŽ‡ï¼Œåœ¨50%é˜ˆå€¼ä¸‹å®žçŽ°äº†å®Œå…¨è¦†ç›–ã€‚ä¸ŽåŸºçº¿æ¨¡åž‹BLIPç›¸æ¯”ï¼Œè¯¥æ¡†æž¶å…·æœ‰æ›´å¥½çš„å±žæ€§ä¿çœŸåº¦ã€æ›´å°‘çš„å¹»è§‰å’Œæ›´å¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽç”µå•†å¹³å°ã€æ—¶å°šåšå®¢ã€ç¤¾äº¤åª’ä½“ç­‰é¢†åŸŸï¼Œè‡ªåŠ¨ç”Ÿæˆå•†å“æè¿°å’Œæ ‡ç­¾ï¼Œæé«˜å•†å“æ›å…‰çŽ‡å’Œç”¨æˆ·å‚ä¸Žåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºŽè¾…åŠ©æ—¶å°šè®¾è®¡å¸ˆè¿›è¡Œé£Žæ ¼åˆ†æžå’Œçµæ„ŸæŒ–æŽ˜ï¼Œä»¥åŠä¸ºæ¶ˆè´¹è€…æä¾›ä¸ªæ€§åŒ–çš„æ—¶å°šæŽ¨èã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

