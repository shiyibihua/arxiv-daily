---
layout: default
title: From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation
---

# From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation

**arXiv**: [2511.19149v1](https://arxiv.org/abs/2511.19149) | [PDF](https://arxiv.org/pdf/2511.19149.pdf)

**ä½œè€…**: Moazzam Umer Gondal, Hamad Ul Qudous, Daniya Siddiqui, Asma Ahmad Farhan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ£€ç´¢å¢žå¼ºæ¡†æž¶ä»¥è§£å†³æ—¶å°šå›¾åƒæè¿°å’Œæ ‡ç­¾ç”Ÿæˆä¸­çš„å±žæ€§ä¿çœŸåº¦ä¸Žæ³›åŒ–é—®é¢˜**

**å…³é”®è¯**: `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `æ—¶å°šå›¾åƒæè¿°` `å¤šæœè£…æ£€æµ‹` `å±žæ€§æŽ¨ç†` `LLMæç¤º` `äº‹å®žè¯æ®åŒ…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç«¯åˆ°ç«¯æ–¹æ³•åœ¨æ—¶å°šå›¾åƒæè¿°ä¸­å±žæ€§ä¿çœŸåº¦ä½Žã€é¢†åŸŸæ³›åŒ–å·®ï¼Œæ˜“äº§ç”Ÿå¹»è§‰ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¤šæœè£…æ£€æµ‹ã€å±žæ€§æŽ¨ç†å’ŒLLMæç¤ºï¼Œæž„å»ºäº‹å®žè¯æ®åŒ…æŒ‡å¯¼æ–‡æœ¬ç”Ÿæˆã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šRAG-LLMåœ¨å±žæ€§è¦†ç›–çŽ‡è¾¾0.80ï¼Œä¼˜äºŽBLIPï¼Œå‡å°‘å¹»è§‰ï¼Œæå‡å¯æ‰©å±•æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

