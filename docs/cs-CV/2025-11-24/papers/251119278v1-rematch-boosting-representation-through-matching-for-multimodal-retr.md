---
layout: default
title: ReMatch: Boosting Representation through Matching for Multimodal Retrieval
---

# ReMatch: Boosting Representation through Matching for Multimodal Retrieval

**arXiv**: [2511.19278v1](https://arxiv.org/abs/2511.19278) | [PDF](https://arxiv.org/pdf/2511.19278.pdf)

**ä½œè€…**: Qianying Liu, Xiao Liang, Zhiqiang Zhang, Yibo Chen, Xu Tang, Zhongfei Qing, Fengfan Zhou, Yao Hu, Paul Henderson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReMatchæ¡†æž¶ï¼Œåˆ©ç”¨MLLMçš„ç”Ÿæˆèƒ½åŠ›å¢žå¼ºå¤šæ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `ç”ŸæˆåŒ¹é…` `é›¶æ ·æœ¬æ³›åŒ–` `MLLMè®­ç»ƒ` `åµŒå…¥å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å°†MLLMä»…ç”¨ä½œç¼–ç å™¨ï¼Œå¿½ç•¥å…¶ç”Ÿæˆç‰¹æ€§ï¼Œå¯¼è‡´æŽ¨ç†èƒ½åŠ›åˆ©ç”¨ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç«¯åˆ°ç«¯è®­ç»ƒåµŒå…¥MLLMï¼Œç»“åˆèŠå¤©å¼ç”ŸæˆåŒ¹é…é˜¶æ®µï¼Œæä¾›å®žä¾‹çº§åˆ¤åˆ«ç›‘ç£ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨MMEBåŸºå‡†ä¸Šå®žçŽ°æ–°SOTAï¼Œé›¶æ ·æœ¬æ³›åŒ–åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.

