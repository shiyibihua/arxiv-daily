---
layout: default
title: AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention
---

# AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention

**arXiv**: [2511.18960v2](https://arxiv.org/abs/2511.18960) | [PDF](https://arxiv.org/pdf/2511.18960.pdf)

**ä½œè€…**: Lei Xiao, Jifeng Li, Juntao Gao, Feiyang Ye, Yan Jin, Jingjing Qian, Jing Zhang, Yong Wu, Xiaoyuan Yu

**åˆ†ç±»**: cs.LG, cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24 (æ›´æ–°: 2025-12-02)

**å¤‡æ³¨**: 18 pages, 10 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AVA-VLAï¼šé€šè¿‡ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨å…·èº«æ™ºèƒ½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å…·èº«æ™ºèƒ½` `ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›` `éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æœºå™¨äººæ“ä½œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨å¤„ç†è§†è§‰è¾“å…¥æ—¶ç¼ºä¹å¯¹åŽ†å²ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´åœ¨åŠ¨æ€åºåˆ—å†³ç­–ä»»åŠ¡ä¸­æ€§èƒ½å—é™ã€‚
2. AVA-VLAé€šè¿‡å¼•å…¥ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨å¾ªçŽ¯çŠ¶æ€åŠ¨æ€è°ƒèŠ‚è§†è§‰å¤„ç†ï¼Œä»Žè€Œå…³æ³¨ä¸Žä»»åŠ¡ç›¸å…³çš„è§†è§‰tokenã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAVA-VLAåœ¨LIBEROå’ŒCALVINç­‰æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®žæœºå™¨äººå¹³å°ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨å…·èº«AIä»»åŠ¡ä¸­å±•çŽ°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„VLAæ¨¡åž‹é€šå¸¸æž„å»ºäºŽè§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰ä¹‹ä¸Šï¼Œå¹¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹å¤„ç†å¯†é›†çš„è§†è§‰è¾“å…¥ã€‚è¿™ç§æ–¹æ³•éšå¼åœ°å°†ä»»åŠ¡å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§ä¸ŽåŽ†å²æ— å…³çš„è®¾è®¡å¯¹äºŽåŠ¨æ€åºåˆ—å†³ç­–ä¸­çš„æœ‰æ•ˆè§†è§‰tokenå¤„ç†æ¥è¯´å¹¶éžæœ€ä¼˜ï¼Œå› ä¸ºå®ƒæœªèƒ½åˆ©ç”¨åŽ†å²çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå±€é™æ€§ï¼Œæˆ‘ä»¬ä»Žéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰çš„è§’åº¦é‡æ–°å®šä¹‰äº†é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºAVA-VLAçš„æ–°æ¡†æž¶ã€‚å—åˆ°POMDPçš„å¯å‘ï¼Œå³åŠ¨ä½œç”Ÿæˆåº”è¯¥ä»¥ç½®ä¿¡çŠ¶æ€ä¸ºæ¡ä»¶ï¼ŒAVA-VLAå¼•å…¥äº†ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›ï¼ˆAVAï¼‰æ¥åŠ¨æ€åœ°è°ƒèŠ‚è§†è§‰å¤„ç†ã€‚å®ƒé€šè¿‡åˆ©ç”¨å¾ªçŽ¯çŠ¶æ€æ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œå¾ªçŽ¯çŠ¶æ€æ˜¯å¯¹ä»£ç†çš„ç½®ä¿¡çŠ¶æ€çš„ç¥žç»è¿‘ä¼¼ï¼Œè¯¥ç½®ä¿¡çŠ¶æ€æ¥è‡ªå…ˆå‰çš„å†³ç­–æ­¥éª¤ã€‚å…·ä½“æ¥è¯´ï¼ŒAVAæ¨¡å—ä½¿ç”¨å¾ªçŽ¯çŠ¶æ€æ¥è®¡ç®—è½¯æƒé‡ï¼Œä»Žè€ŒåŸºäºŽå…¶åŽ†å²ä¸Šä¸‹æ–‡ä¸»åŠ¨å¤„ç†ä¸Žä»»åŠ¡ç›¸å…³çš„è§†è§‰tokenã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜Žï¼ŒAVA-VLAåœ¨æµè¡Œçš„æœºå™¨äººåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬LIBEROå’ŒCALVINï¼‰ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨åŒè‡‚æœºå™¨äººå¹³å°ä¸Šçš„çœŸå®žéƒ¨ç½²éªŒè¯äº†è¯¥æ¡†æž¶çš„å®žé™…é€‚ç”¨æ€§å’Œå¼ºå¤§çš„sim-to-realå¯è¿ç§»æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹é€šå¸¸å°†å…·èº«æ™ºèƒ½ä»»åŠ¡è§†ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¿½ç•¥äº†åŽ†å²ä¿¡æ¯å¯¹å½“å‰å†³ç­–çš„å½±å“ã€‚è¿™ç§history-agnosticçš„è®¾è®¡å¯¼è‡´æ¨¡åž‹æ— æ³•æœ‰æ•ˆåœ°åˆ©ç”¨åŽ†å²ä¸Šä¸‹æ–‡æ¥å¤„ç†è§†è§‰è¾“å…¥ï¼Œä»Žè€Œå½±å“äº†åœ¨åŠ¨æ€åºåˆ—å†³ç­–ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ¨¡åž‹å¹³ç­‰åœ°å¤„ç†æ‰€æœ‰è§†è§‰tokenï¼Œç¼ºä¹é€‰æ‹©æ€§å…³æ³¨å…³é”®ä¿¡æ¯çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAVA-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å…·èº«æ™ºèƒ½ä»»åŠ¡é‡æ–°å»ºæ¨¡ä¸ºéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ã€‚åœ¨POMDPä¸­ï¼Œæ™ºèƒ½ä½“çš„å†³ç­–ä¾èµ–äºŽå…¶ç½®ä¿¡çŠ¶æ€ï¼ˆbelief stateï¼‰ï¼Œå³å¯¹çŽ¯å¢ƒçŠ¶æ€çš„æ¦‚çŽ‡åˆ†å¸ƒä¼°è®¡ã€‚AVA-VLAé€šè¿‡å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼ˆRNNï¼‰æ¥è¿‘ä¼¼æ™ºèƒ½ä½“çš„ç½®ä¿¡çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨è¯¥çŠ¶æ€æ¥åŠ¨æ€åœ°è°ƒèŠ‚è§†è§‰å¤„ç†è¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ ¹æ®åŽ†å²ä¿¡æ¯ï¼Œä¸»åŠ¨å…³æ³¨ä¸Žå½“å‰ä»»åŠ¡ç›¸å…³çš„è§†è§‰tokenã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šAVA-VLAçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºŽæå–è§†è§‰ç‰¹å¾ã€‚2) å¾ªçŽ¯ç¥žç»ç½‘ç»œï¼ˆRNNï¼‰ï¼šç”¨äºŽç»´æŠ¤æ™ºèƒ½ä½“çš„ç½®ä¿¡çŠ¶æ€ï¼Œå¹¶å¯¹åŽ†å²ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚3) ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›ï¼ˆAVAï¼‰æ¨¡å—ï¼šåˆ©ç”¨RNNçš„è¾“å‡ºï¼ˆç½®ä¿¡çŠ¶æ€ï¼‰æ¥è®¡ç®—è§†è§‰tokençš„æ³¨æ„åŠ›æƒé‡ï¼Œä»Žè€ŒåŠ¨æ€åœ°é€‰æ‹©ä¸Žä»»åŠ¡ç›¸å…³çš„è§†è§‰tokenã€‚4) åŠ¨ä½œè§£ç å™¨ï¼šæ ¹æ®è§†è§‰ç‰¹å¾å’Œç½®ä¿¡çŠ¶æ€ç”ŸæˆåŠ¨ä½œã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œè§†è§‰è¾“å…¥é€šè¿‡è§†è§‰ç¼–ç å™¨å¾—åˆ°è§†è§‰ç‰¹å¾ï¼ŒRNNæ ¹æ®ä¹‹å‰çš„çŠ¶æ€å’Œå½“å‰è§†è§‰ç‰¹å¾æ›´æ–°ç½®ä¿¡çŠ¶æ€ï¼ŒAVAæ¨¡å—åˆ©ç”¨ç½®ä¿¡çŠ¶æ€å¯¹è§†è§‰ç‰¹å¾è¿›è¡ŒåŠ æƒï¼Œæœ€åŽåŠ¨ä½œè§£ç å™¨æ ¹æ®åŠ æƒåŽçš„è§†è§‰ç‰¹å¾å’Œç½®ä¿¡çŠ¶æ€ç”ŸæˆåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šAVA-VLAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›ï¼ˆAVAï¼‰æ¨¡å—ã€‚ä¸Žä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼ŒAVAæ¨¡å—çš„æ³¨æ„åŠ›æƒé‡ä¸æ˜¯ç›´æŽ¥ä»Žè§†è§‰ç‰¹å¾è®¡ç®—å¾—åˆ°çš„ï¼Œè€Œæ˜¯ä»Žå¾ªçŽ¯ç¥žç»ç½‘ç»œçš„è¾“å‡ºï¼ˆç½®ä¿¡çŠ¶æ€ï¼‰è®¡ç®—å¾—åˆ°çš„ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ ¹æ®åŽ†å²ä¿¡æ¯æ¥åŠ¨æ€åœ°è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»Žè€Œå…³æ³¨ä¸Žä»»åŠ¡ç›¸å…³çš„è§†è§‰tokenã€‚è¿™ä¸ŽçŽ°æœ‰æ–¹æ³•ä¸­ç‹¬ç«‹å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥çš„è§†è§‰è¾“å…¥å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šAVAæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºŽå¦‚ä½•åˆ©ç”¨å¾ªçŽ¯çŠ¶æ€æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼ŒAVAæ¨¡å—é¦–å…ˆå°†å¾ªçŽ¯çŠ¶æ€é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„åˆ°ä¸€ä¸ªä¸Žè§†è§‰tokenæ•°é‡ç›¸åŒçš„å‘é‡ï¼Œç„¶åŽä½¿ç”¨softmaxå‡½æ•°å°†è¯¥å‘é‡è½¬æ¢ä¸ºæ³¨æ„åŠ›æƒé‡ã€‚è¿™äº›æƒé‡ç”¨äºŽå¯¹è§†è§‰tokenè¿›è¡ŒåŠ æƒï¼Œä»Žè€Œå®žçŽ°ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±æˆ–ç±»ä¼¼çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡åž‹ï¼Œä»¥æœ€å°åŒ–é¢„æµ‹åŠ¨ä½œä¸ŽçœŸå®žåŠ¨ä½œä¹‹é—´çš„å·®å¼‚ã€‚ç½‘ç»œç»“æž„æ–¹é¢ï¼ŒRNNé€šå¸¸é‡‡ç”¨LSTMæˆ–GRUç­‰å˜ä½“ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

AVA-VLAåœ¨LIBEROå’ŒCALVINç­‰æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨LIBEROæ•°æ®é›†ä¸Šï¼ŒAVA-VLAçš„æˆåŠŸçŽ‡æ¯”çŽ°æœ‰æœ€ä½³æ¨¡åž‹æé«˜äº†X%ã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®žæœºå™¨äººå¹³å°ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒAVA-VLAå…·æœ‰è‰¯å¥½çš„sim-to-realå¯è¿ç§»æ€§ï¼Œèƒ½å¤Ÿåœ¨çœŸå®žçŽ¯å¢ƒä¸­ç¨³å®šè¿è¡Œå¹¶å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚è¿™äº›å®žéªŒç»“æžœå……åˆ†éªŒè¯äº†AVA-VLAçš„æœ‰æ•ˆæ€§å’Œå®žç”¨æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

AVA-VLAåœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸåŠ©æ‰‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚é€šè¿‡æå‡VLAæ¨¡åž‹å¯¹çŽ¯å¢ƒçš„ç†è§£å’Œå†³ç­–èƒ½åŠ›ï¼Œå¯ä»¥å®žçŽ°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿï¼Œä»Žè€Œåœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©ã€å®¶åº­æœåŠ¡ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºŽæŽ¨åŠ¨å…·èº«æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºæž„å»ºæ›´å®‰å…¨ã€æ›´å¯é çš„æ™ºèƒ½ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

