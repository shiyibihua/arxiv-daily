---
layout: default
title: Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach
---

# Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach

**arXiv**: [2511.19080v1](https://arxiv.org/abs/2511.19080) | [PDF](https://arxiv.org/pdf/2511.19080.pdf)

**ä½œè€…**: Fan Nie, Jiangqun Ni, Jian Zhang, Bin Zhang, Weizhe Zhang, Bin Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFoVBæ¡†æž¶ä»¥æå‡éŸ³è§†é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–æ€§**

**å…³é”®è¯**: `æ·±åº¦ä¼ªé€ æ£€æµ‹` `éŸ³è§†é¢‘ç›¸å…³æ€§å­¦ä¹ ` `å˜åˆ†è´å¶æ–¯ä¼°è®¡` `å¤šæ¨¡æ€å­¦ä¹ ` `æ³›åŒ–æ€§æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­ï¼ŒéŸ³è§†é¢‘ä¸ä¸€è‡´æ€§éš¾ä»¥æ³›åŒ–è¯†åˆ«ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å˜åˆ†è´å¶æ–¯ä¼°è®¡éŸ³è§†é¢‘ç›¸å…³æ€§ï¼Œåˆ†è§£æ¨¡æ€ç‰¹å®šä¸Žç›¸å…³æ€§å˜é‡ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æ³›åŒ–æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.

