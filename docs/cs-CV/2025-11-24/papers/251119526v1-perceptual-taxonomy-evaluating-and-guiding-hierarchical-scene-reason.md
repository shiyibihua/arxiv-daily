---
layout: default
title: Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models
---

# Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models

**arXiv**: [2511.19526v1](https://arxiv.org/abs/2511.19526) | [PDF](https://arxiv.org/pdf/2511.19526.pdf)

**ä½œè€…**: Jonathan Lee, Xingrui Wang, Jiawei Peng, Luoxin Ye, Zehan Zheng, Tiezheng Zhang, Tao Wang, Wufei Ma, Siyi Chen, Yu-Cheng Chou, Prakhar Kaushik, Alan Yuille

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„ŸçŸ¥åˆ†ç±»æ³•ï¼Œç”¨äºŽè¯„ä¼°å’ŒæŒ‡å¯¼è§†è§‰-è¯­è¨€æ¨¡åž‹ä¸­çš„åˆ†å±‚åœºæ™¯æŽ¨ç†**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡åž‹` `åœºæ™¯ç†è§£` `ç‰©ç†æŽ¨ç†` `åˆ†å±‚æŽ¨ç†` `åŸºå‡†æµ‹è¯•` `å±žæ€§æŽ¨æ–­` `æ„ŸçŸ¥åˆ†ç±»æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ç¼ºä¹å¯¹ç‰©ç†åŸºç¡€è§†è§‰æŽ¨ç†çš„å…¨é¢è¯„ä¼°ï¼Œä¸»è¦é›†ä¸­äºŽè¡¨é¢è¯†åˆ«æˆ–å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚
2. è®ºæ–‡æå‡ºPerceptual Taxonomyï¼Œé€šè¿‡è¯†åˆ«ç‰©ä½“ã€ç©ºé—´é…ç½®å¹¶æŽ¨æ–­å±žæ€§æ¥æ”¯æŒç›®æ ‡å¯¼å‘çš„æŽ¨ç†ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒçŽ°æœ‰æ¨¡åž‹åœ¨å±žæ€§é©±åŠ¨é—®é¢˜ä¸Šæ€§èƒ½ä¸‹é™ï¼Œä½†é€šè¿‡æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºå¯ä»¥æœ‰æ•ˆæå‡æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æž„åŒ–çš„åœºæ™¯ç†è§£è¿‡ç¨‹ï¼Œç§°ä¸ºæ„ŸçŸ¥åˆ†ç±»æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆè¯†åˆ«ç‰©ä½“åŠå…¶ç©ºé—´é…ç½®ï¼Œç„¶åŽæŽ¨æ–­ä»»åŠ¡ç›¸å…³çš„å±žæ€§ï¼Œå¦‚ææ–™ã€å¯ä¾›æ€§ã€åŠŸèƒ½å’Œç‰©ç†å±žæ€§ï¼Œä»¥æ”¯æŒç›®æ ‡å¯¼å‘çš„æŽ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥çŽ°æœ‰è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•åœ¨å…¨é¢è¯„ä¼°è¿™ç§èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œæœ¬æ–‡å¼•å…¥äº†Perceptual Taxonomyï¼Œä¸€ä¸ªç”¨äºŽç‰©ç†åŸºç¡€è§†è§‰æŽ¨ç†çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä½¿ç”¨å››ä¸ªå±žæ€§æ—è¦†ç›–çš„84ä¸ªç»†ç²’åº¦å±žæ€§æ ‡æ³¨äº†3173ä¸ªå¯¹è±¡ã€‚åˆ©ç”¨è¿™äº›æ ‡æ³¨ï¼Œæž„å»ºäº†ä¸€ä¸ªåŒ…å«5802å¼ å›¾åƒçš„å¤šé¡¹é€‰æ‹©é¢˜åŸºå‡†ï¼Œæ¶µç›–åˆæˆå’ŒçœŸå®žé¢†åŸŸã€‚è¯¥åŸºå‡†åŒ…å«28033ä¸ªåŸºäºŽæ¨¡æ¿çš„é—®é¢˜ï¼Œè·¨è¶Šå››ç§ç±»åž‹ï¼ˆå¯¹è±¡æè¿°ã€ç©ºé—´æŽ¨ç†ã€å±žæ€§åŒ¹é…å’Œåˆ†ç±»æŽ¨ç†ï¼‰ï¼Œä»¥åŠ50ä¸ªä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡åž‹åœ¨æ„ŸçŸ¥åˆ†ç±»æŽ¨ç†çš„å„ä¸ªæ–¹é¢çš„èƒ½åŠ›ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œé¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨å±žæ€§é©±åŠ¨çš„é—®é¢˜ä¸Šæ€§èƒ½ä¸‹é™10%åˆ°20%ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¯¹ç»“æž„åŒ–å±žæ€§è¿›è¡Œå¤šæ­¥æŽ¨ç†çš„é—®é¢˜ä¸Šã€‚è¿™äº›å‘çŽ°çªå‡ºäº†ç»“æž„åŒ–è§†è§‰ç†è§£æ–¹é¢å­˜åœ¨çš„å·®è·ï¼Œä»¥åŠå½“å‰ä¸¥é‡ä¾èµ–æ¨¡å¼åŒ¹é…çš„æ¨¡åž‹çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜Žï¼Œæä¾›æ¥è‡ªæ¨¡æ‹Ÿåœºæ™¯çš„ä¸Šä¸‹æ–‡æŽ¨ç†ç¤ºä¾‹å¯ä»¥æé«˜æ¨¡åž‹åœ¨çœŸå®žä¸–ç•Œå’Œä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œè¯æ˜Žäº†æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºçš„æœ‰æ•ˆæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨åœºæ™¯ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¿›è¡Œå¤šæ­¥éª¤æŽ¨ç†å’Œç†è§£ç‰©ä½“å±žæ€§ï¼ˆå¦‚æè´¨ã€åŠŸèƒ½ç­‰ï¼‰çš„ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ã€‚çŽ°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¡¨é¢è¯†åˆ«å’Œå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œç¼ºä¹å¯¹ç‰©ç†åŸºç¡€è§†è§‰æŽ¨ç†çš„å…¨é¢è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªç»“æž„åŒ–çš„åœºæ™¯ç†è§£è¿‡ç¨‹ï¼Œå³Perceptual Taxonomyã€‚è¯¥æ–¹æ³•æ¨¡æ‹Ÿäººç±»çš„æ„ŸçŸ¥è¿‡ç¨‹ï¼Œé¦–å…ˆè¯†åˆ«åœºæ™¯ä¸­çš„ç‰©ä½“åŠå…¶ç©ºé—´å…³ç³»ï¼Œç„¶åŽæŽ¨æ–­è¿™äº›ç‰©ä½“çš„å±žæ€§ï¼Œä»Žè€Œæ”¯æŒæ›´é«˜çº§åˆ«çš„æŽ¨ç†ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§åˆ†å±‚æŽ¨ç†ï¼Œæ¨¡åž‹å¯ä»¥æ›´å¥½åœ°ç†è§£åœºæ™¯çš„ç‰©ç†å±žæ€§å’ŒåŠŸèƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šPerceptual TaxonomyåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) å¯¹è±¡è¯†åˆ«æ¨¡å—ï¼Œç”¨äºŽè¯†åˆ«åœºæ™¯ä¸­çš„ç‰©ä½“ï¼›2) ç©ºé—´å…³ç³»æŽ¨ç†æ¨¡å—ï¼Œç”¨äºŽç†è§£ç‰©ä½“ä¹‹é—´çš„ç©ºé—´é…ç½®ï¼›3) å±žæ€§æŽ¨æ–­æ¨¡å—ï¼Œç”¨äºŽæŽ¨æ–­ç‰©ä½“çš„å±žæ€§ï¼Œå¦‚ææ–™ã€å¯ä¾›æ€§ã€åŠŸèƒ½å’Œç‰©ç†å±žæ€§ï¼›4) åŸºäºŽæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆæ¨¡å—ï¼Œç”¨äºŽç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–å¯¹è±¡æè¿°ã€ç©ºé—´æŽ¨ç†ã€å±žæ€§åŒ¹é…å’Œåˆ†ç±»æŽ¨ç†ç­‰ç±»åž‹ï¼›5) ä¸“å®¶ç­–åˆ’é—®é¢˜é›†ï¼Œç”¨äºŽè¯„ä¼°æ¨¡åž‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æŽ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†Perceptual Taxonomyè¿™ä¸€ç»“æž„åŒ–çš„åœºæ™¯ç†è§£è¿‡ç¨‹ï¼Œå¹¶æž„å»ºäº†ä¸€ä¸ªç›¸åº”çš„åŸºå‡†æµ‹è¯•ã€‚ä¸Žä»¥å¾€çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒPerceptual Taxonomyæ›´æ³¨é‡å¯¹æ¨¡åž‹ç‰©ç†åŸºç¡€è§†è§‰æŽ¨ç†èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡¨é¢è¯†åˆ«ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä½¿ç”¨æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºçš„æ–¹æ³•ï¼Œé€šè¿‡æä¾›æ¥è‡ªæ¨¡æ‹Ÿåœºæ™¯çš„ä¸Šä¸‹æ–‡æŽ¨ç†ç¤ºä¾‹æ¥æé«˜æ¨¡åž‹åœ¨çœŸå®žä¸–ç•Œå’Œä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šPerceptual TaxonomyåŸºå‡†æµ‹è¯•åŒ…å«3173ä¸ªå¯¹è±¡ï¼Œå¹¶ä½¿ç”¨å››ä¸ªå±žæ€§æ—è¦†ç›–çš„84ä¸ªç»†ç²’åº¦å±žæ€§è¿›è¡Œæ ‡æ³¨ã€‚åŸºå‡†æµ‹è¯•åŒ…å«5802å¼ å›¾åƒï¼Œæ¶µç›–åˆæˆå’ŒçœŸå®žé¢†åŸŸã€‚é—®é¢˜ç”Ÿæˆé‡‡ç”¨åŸºäºŽæ¨¡æ¿çš„æ–¹æ³•ï¼Œç”Ÿæˆ28033ä¸ªé—®é¢˜ï¼Œå¹¶è¾…ä»¥50ä¸ªä¸“å®¶ç­–åˆ’çš„é—®é¢˜ã€‚åœ¨å®žéªŒä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†å¤šç§é¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨ä¸åŒç±»åž‹é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†ä¸åŒæç¤ºç­–ç•¥å¯¹æ¨¡åž‹æ€§èƒ½çš„å½±å“ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œé¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨å±žæ€§é©±åŠ¨çš„é—®é¢˜ä¸Šæ€§èƒ½ä¸‹é™10%åˆ°20%ã€‚é€šè¿‡æä¾›æ¥è‡ªæ¨¡æ‹Ÿåœºæ™¯çš„ä¸Šä¸‹æ–‡æŽ¨ç†ç¤ºä¾‹ï¼Œæ¨¡åž‹åœ¨çœŸå®žä¸–ç•Œå’Œä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šçš„æ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ï¼Œè¯æ˜Žäº†æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºçš„æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡5%ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰-è¯­è¨€æ¨¡åž‹å¯¹åœºæ™¯çš„ç»“æž„åŒ–ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´çŽ¯å¢ƒï¼Œä»Žè€Œå®žçŽ°æ›´æ™ºèƒ½çš„äº¤äº’å’Œå†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºŽå¼€å‘æ›´å¼ºå¤§çš„è§†è§‰è¾…åŠ©å·¥å…·ï¼Œå¸®åŠ©è§†éšœäººå£«æ›´å¥½åœ°ç†è§£å‘¨å›´ä¸–ç•Œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose Perceptual Taxonomy, a structured process of scene understanding that first recognizes objects and their spatial configurations, then infers task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. While this form of reasoning is fundamental to human cognition, current vision-language benchmarks lack comprehensive evaluation of this ability and instead focus on surface-level recognition or image-text alignment.
>   To address this gap, we introduce Perceptual Taxonomy, a benchmark for physically grounded visual reasoning. We annotate 3173 objects with four property families covering 84 fine-grained attributes. Using these annotations, we construct a multiple-choice question benchmark with 5802 images across both synthetic and real domains. The benchmark contains 28033 template-based questions spanning four types (object description, spatial reasoning, property matching, and taxonomy reasoning), along with 50 expert-crafted questions designed to evaluate models across the full spectrum of perceptual taxonomy reasoning.
>   Experimental results show that leading vision-language models perform well on recognition tasks but degrade by 10 to 20 percent on property-driven questions, especially those requiring multi-step reasoning over structured attributes. These findings highlight a persistent gap in structured visual understanding and the limitations of current models that rely heavily on pattern matching. We also show that providing in-context reasoning examples from simulated scenes improves performance on real-world and expert-curated questions, demonstrating the effectiveness of perceptual-taxonomy-guided prompting.

