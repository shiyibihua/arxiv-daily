---
layout: default
title: Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models
---

# Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19526" target="_blank" class="toolbar-btn">arXiv: 2511.19526v1</a>
    <a href="https://arxiv.org/pdf/2511.19526.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19526v1" 
            onclick="toggleFavorite(this, '2511.19526v1', 'Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Jonathan Lee, Xingrui Wang, Jiawei Peng, Luoxin Ye, Zehan Zheng, Tiezheng Zhang, Tao Wang, Wufei Ma, Siyi Chen, Yu-Cheng Chou, Prakhar Kaushik, Alan Yuille

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„ŸçŸ¥åˆ†ç±»æ³•ï¼Œç”¨äºè¯„ä¼°å’ŒæŒ‡å¯¼è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„åˆ†å±‚åœºæ™¯æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `åœºæ™¯ç†è§£` `ç‰©ç†æ¨ç†` `åˆ†å±‚æ¨ç†` `åŸºå‡†æµ‹è¯•` `å±æ€§æ¨æ–­` `æ„ŸçŸ¥åˆ†ç±»æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ç¼ºä¹å¯¹ç‰©ç†åŸºç¡€è§†è§‰æ¨ç†çš„å…¨é¢è¯„ä¼°ï¼Œä¸»è¦é›†ä¸­äºè¡¨é¢è¯†åˆ«æˆ–å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚
2. è®ºæ–‡æå‡ºPerceptual Taxonomyï¼Œé€šè¿‡è¯†åˆ«ç‰©ä½“ã€ç©ºé—´é…ç½®å¹¶æ¨æ–­å±æ€§æ¥æ”¯æŒç›®æ ‡å¯¼å‘çš„æ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å±æ€§é©±åŠ¨é—®é¢˜ä¸Šæ€§èƒ½ä¸‹é™ï¼Œä½†é€šè¿‡æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºå¯ä»¥æœ‰æ•ˆæå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„åœºæ™¯ç†è§£è¿‡ç¨‹ï¼Œç§°ä¸ºæ„ŸçŸ¥åˆ†ç±»æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆè¯†åˆ«ç‰©ä½“åŠå…¶ç©ºé—´é…ç½®ï¼Œç„¶åæ¨æ–­ä»»åŠ¡ç›¸å…³çš„å±æ€§ï¼Œå¦‚ææ–™ã€å¯ä¾›æ€§ã€åŠŸèƒ½å’Œç‰©ç†å±æ€§ï¼Œä»¥æ”¯æŒç›®æ ‡å¯¼å‘çš„æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•åœ¨å…¨é¢è¯„ä¼°è¿™ç§èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œæœ¬æ–‡å¼•å…¥äº†Perceptual Taxonomyï¼Œä¸€ä¸ªç”¨äºç‰©ç†åŸºç¡€è§†è§‰æ¨ç†çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä½¿ç”¨å››ä¸ªå±æ€§æ—è¦†ç›–çš„84ä¸ªç»†ç²’åº¦å±æ€§æ ‡æ³¨äº†3173ä¸ªå¯¹è±¡ã€‚åˆ©ç”¨è¿™äº›æ ‡æ³¨ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«5802å¼ å›¾åƒçš„å¤šé¡¹é€‰æ‹©é¢˜åŸºå‡†ï¼Œæ¶µç›–åˆæˆå’ŒçœŸå®é¢†åŸŸã€‚è¯¥åŸºå‡†åŒ…å«28033ä¸ªåŸºäºæ¨¡æ¿çš„é—®é¢˜ï¼Œè·¨è¶Šå››ç§ç±»å‹ï¼ˆå¯¹è±¡æè¿°ã€ç©ºé—´æ¨ç†ã€å±æ€§åŒ¹é…å’Œåˆ†ç±»æ¨ç†ï¼‰ï¼Œä»¥åŠ50ä¸ªä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ„ŸçŸ¥åˆ†ç±»æ¨ç†çš„å„ä¸ªæ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å±æ€§é©±åŠ¨çš„é—®é¢˜ä¸Šæ€§èƒ½ä¸‹é™10%åˆ°20%ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¯¹ç»“æ„åŒ–å±æ€§è¿›è¡Œå¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šã€‚è¿™äº›å‘ç°çªå‡ºäº†ç»“æ„åŒ–è§†è§‰ç†è§£æ–¹é¢å­˜åœ¨çš„å·®è·ï¼Œä»¥åŠå½“å‰ä¸¥é‡ä¾èµ–æ¨¡å¼åŒ¹é…çš„æ¨¡å‹çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ï¼Œæä¾›æ¥è‡ªæ¨¡æ‹Ÿåœºæ™¯çš„ä¸Šä¸‹æ–‡æ¨ç†ç¤ºä¾‹å¯ä»¥æé«˜æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå’Œä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨åœºæ™¯ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¿›è¡Œå¤šæ­¥éª¤æ¨ç†å’Œç†è§£ç‰©ä½“å±æ€§ï¼ˆå¦‚æè´¨ã€åŠŸèƒ½ç­‰ï¼‰çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¡¨é¢è¯†åˆ«å’Œå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œç¼ºä¹å¯¹ç‰©ç†åŸºç¡€è§†è§‰æ¨ç†çš„å…¨é¢è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„åœºæ™¯ç†è§£è¿‡ç¨‹ï¼Œå³Perceptual Taxonomyã€‚è¯¥æ–¹æ³•æ¨¡æ‹Ÿäººç±»çš„æ„ŸçŸ¥è¿‡ç¨‹ï¼Œé¦–å…ˆè¯†åˆ«åœºæ™¯ä¸­çš„ç‰©ä½“åŠå…¶ç©ºé—´å…³ç³»ï¼Œç„¶åæ¨æ–­è¿™äº›ç‰©ä½“çš„å±æ€§ï¼Œä»è€Œæ”¯æŒæ›´é«˜çº§åˆ«çš„æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§åˆ†å±‚æ¨ç†ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£åœºæ™¯çš„ç‰©ç†å±æ€§å’ŒåŠŸèƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPerceptual TaxonomyåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) å¯¹è±¡è¯†åˆ«æ¨¡å—ï¼Œç”¨äºè¯†åˆ«åœºæ™¯ä¸­çš„ç‰©ä½“ï¼›2) ç©ºé—´å…³ç³»æ¨ç†æ¨¡å—ï¼Œç”¨äºç†è§£ç‰©ä½“ä¹‹é—´çš„ç©ºé—´é…ç½®ï¼›3) å±æ€§æ¨æ–­æ¨¡å—ï¼Œç”¨äºæ¨æ–­ç‰©ä½“çš„å±æ€§ï¼Œå¦‚ææ–™ã€å¯ä¾›æ€§ã€åŠŸèƒ½å’Œç‰©ç†å±æ€§ï¼›4) åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆæ¨¡å—ï¼Œç”¨äºç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–å¯¹è±¡æè¿°ã€ç©ºé—´æ¨ç†ã€å±æ€§åŒ¹é…å’Œåˆ†ç±»æ¨ç†ç­‰ç±»å‹ï¼›5) ä¸“å®¶ç­–åˆ’é—®é¢˜é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Perceptual Taxonomyè¿™ä¸€ç»“æ„åŒ–çš„åœºæ™¯ç†è§£è¿‡ç¨‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªç›¸åº”çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä»¥å¾€çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒPerceptual Taxonomyæ›´æ³¨é‡å¯¹æ¨¡å‹ç‰©ç†åŸºç¡€è§†è§‰æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œä¸ä»…ä»…æ˜¯è¡¨é¢è¯†åˆ«ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä½¿ç”¨æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºçš„æ–¹æ³•ï¼Œé€šè¿‡æä¾›æ¥è‡ªæ¨¡æ‹Ÿåœºæ™¯çš„ä¸Šä¸‹æ–‡æ¨ç†ç¤ºä¾‹æ¥æé«˜æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå’Œä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šPerceptual TaxonomyåŸºå‡†æµ‹è¯•åŒ…å«3173ä¸ªå¯¹è±¡ï¼Œå¹¶ä½¿ç”¨å››ä¸ªå±æ€§æ—è¦†ç›–çš„84ä¸ªç»†ç²’åº¦å±æ€§è¿›è¡Œæ ‡æ³¨ã€‚åŸºå‡†æµ‹è¯•åŒ…å«5802å¼ å›¾åƒï¼Œæ¶µç›–åˆæˆå’ŒçœŸå®é¢†åŸŸã€‚é—®é¢˜ç”Ÿæˆé‡‡ç”¨åŸºäºæ¨¡æ¿çš„æ–¹æ³•ï¼Œç”Ÿæˆ28033ä¸ªé—®é¢˜ï¼Œå¹¶è¾…ä»¥50ä¸ªä¸“å®¶ç­–åˆ’çš„é—®é¢˜ã€‚åœ¨å®éªŒä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†å¤šç§é¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨ä¸åŒç±»å‹é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†ä¸åŒæç¤ºç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å±æ€§é©±åŠ¨çš„é—®é¢˜ä¸Šæ€§èƒ½ä¸‹é™10%åˆ°20%ã€‚é€šè¿‡æä¾›æ¥è‡ªæ¨¡æ‹Ÿåœºæ™¯çš„ä¸Šä¸‹æ–‡æ¨ç†ç¤ºä¾‹ï¼Œæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå’Œä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šçš„æ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†æ„ŸçŸ¥åˆ†ç±»æ³•å¼•å¯¼æç¤ºçš„æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸“å®¶ç­–åˆ’é—®é¢˜ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹åœºæ™¯çš„ç»“æ„åŒ–ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äº¤äº’å’Œå†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºå¼€å‘æ›´å¼ºå¤§çš„è§†è§‰è¾…åŠ©å·¥å…·ï¼Œå¸®åŠ©è§†éšœäººå£«æ›´å¥½åœ°ç†è§£å‘¨å›´ä¸–ç•Œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose Perceptual Taxonomy, a structured process of scene understanding that first recognizes objects and their spatial configurations, then infers task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. While this form of reasoning is fundamental to human cognition, current vision-language benchmarks lack comprehensive evaluation of this ability and instead focus on surface-level recognition or image-text alignment.
>   To address this gap, we introduce Perceptual Taxonomy, a benchmark for physically grounded visual reasoning. We annotate 3173 objects with four property families covering 84 fine-grained attributes. Using these annotations, we construct a multiple-choice question benchmark with 5802 images across both synthetic and real domains. The benchmark contains 28033 template-based questions spanning four types (object description, spatial reasoning, property matching, and taxonomy reasoning), along with 50 expert-crafted questions designed to evaluate models across the full spectrum of perceptual taxonomy reasoning.
>   Experimental results show that leading vision-language models perform well on recognition tasks but degrade by 10 to 20 percent on property-driven questions, especially those requiring multi-step reasoning over structured attributes. These findings highlight a persistent gap in structured visual understanding and the limitations of current models that rely heavily on pattern matching. We also show that providing in-context reasoning examples from simulated scenes improves performance on real-world and expert-curated questions, demonstrating the effectiveness of perceptual-taxonomy-guided prompting.

