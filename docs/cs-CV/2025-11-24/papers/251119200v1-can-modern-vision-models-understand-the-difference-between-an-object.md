---
layout: default
title: Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?
---

# Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?

**arXiv**: [2511.19200v1](https://arxiv.org/abs/2511.19200) | [PDF](https://arxiv.org/pdf/2511.19200.pdf)

**ä½œè€…**: Itay Cohen, Ethan Fetaya, Amir Rosenfeld

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoLAæ•°æ®é›†å’ŒåµŒå…¥æ–¹å‘æ–¹æ³•ï¼Œä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹åŒºåˆ†çœŸå®žç‰©ä½“ä¸Žç›¸ä¼¼ç‰©çš„èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç›¸ä¼¼ç‰©è¯†åˆ«` `CLIPåµŒå…¥` `è·¨æ¨¡æ€æ£€ç´¢` `å›¾åƒæè¿°å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹èƒ½å¦åŒºåˆ†çœŸå®žç‰©ä½“ä¸Žç›¸ä¼¼ç‰©ï¼ˆå¦‚çŽ©å…·ã€é›•åƒï¼‰ï¼Œå¼¥è¡¥ä¸Žäººç±»æ„ŸçŸ¥çš„å·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºRoLAæ•°æ®é›†ï¼Œä¼°è®¡CLIPåµŒå…¥ç©ºé—´ä¸­çœŸå®žä¸Žç›¸ä¼¼ç‰©çš„æ–¹å‘ï¼Œå¹¶åº”ç”¨äºŽè·¨æ¨¡æ€æ£€ç´¢å’Œå›¾åƒæè¿°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯¥æ–¹æ³•åœ¨Conceptual12Mä¸Šæå‡æ£€ç´¢æ€§èƒ½ï¼Œå¹¶æ”¹è¿›CLIPå‰ç¼€æè¿°å™¨çš„æè¿°è´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.

