---
layout: default
title: Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning
---

# Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19515" target="_blank" class="toolbar-btn">arXiv: 2511.19515v1</a>
    <a href="https://arxiv.org/pdf/2511.19515.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19515v1" 
            onclick="toggleFavorite(this, '2511.19515v1', 'Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Shawn Young, Xingyu Zeng, Lijian Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è§†è§‰åŸºï¼Œå‡å°‘è§†è§‰Tokenæ•°é‡ï¼Œæå‡è§†è§‰è¡¨å¾å­¦ä¹ çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰Transformer` `Tokenå‹ç¼©` `æ­£äº¤æ»¤æ³¢` `è‡ªé€‚åº”è¡¨ç¤º` `æ¨¡å‹ç¼©æ”¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰Transformeræ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„Tokenæ¥è¡¨å¾å›¾åƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚
2. è®ºæ–‡æå‡ºæ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°èšç±»å†—ä½™Tokenï¼Œç”Ÿæˆä¸€ç»„æ›´ç´§å‡‘çš„æ­£äº¤åŸºï¼Œä»è€Œå‡å°‘Tokenæ•°é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç”šè‡³æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†Tokenæ•°é‡ï¼Œå¹¶æ­ç¤ºäº†Tokenæ•°é‡ä¸æ¨¡å‹å¤§å°ä¹‹é—´çš„ç¼©æ”¾è§„å¾‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ¨¡å‹å®¹é‡ä¸ä¿æŒå›¾åƒè¯­ä¹‰æ‰€éœ€çš„æœ€å°è§†è§‰Tokenæ•°é‡ä¹‹é—´çš„æ ¹æœ¬å…³ç³»ã€‚å—æœ€å°æè¿°é•¿åº¦åŸåˆ™çš„å¯å‘ï¼Œæˆ‘ä»¬å°†å›¾åƒTokené‡æ–°è§£é‡Šä¸ºè§†è§‰è¯­ä¹‰ç©ºé—´ä¸­çš„å‘é‡ï¼Œå¹¶å°†å›¾åƒçš„å†…åœ¨è¯­ä¹‰å¤æ‚åº¦å®šä¹‰ä¸ºè·¨è¶Šè¯¥ç©ºé—´æ‰€éœ€çš„æœ€å°‘åŸºå‘é‡é›†åˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°å°†å†—ä½™Tokenèšç±»æˆä¸€ç»„ç´§å‡‘çš„æ­£äº¤åŸºã€‚é€šè¿‡å¯¹ä¸€ç³»åˆ—ViTæ¨¡å‹è¿›è¡Œçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ä¸ªä¸€è‡´çš„Token-æ¨¡å‹ç¼©æ”¾è§„å¾‹ï¼šæ›´å¤§çš„æ¨¡å‹éœ€è¦æ˜æ˜¾æ›´å°‘çš„Tokenæ¥è·¨è¶Šè§†è§‰è¯­ä¹‰ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªè§†è§‰é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰Transformeræ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ViTç³»åˆ—ï¼Œåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œéœ€è¦å°†å›¾åƒåˆ†å‰²æˆå¤§é‡çš„Tokenï¼Œè¿™å¯¼è‡´è®¡ç®—å¤æ‚åº¦æ˜¾è‘—å¢åŠ ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„Tokenæ•°é‡ï¼Œå¿½ç•¥äº†å›¾åƒæœ¬èº«è¯­ä¹‰å¤æ‚åº¦çš„å·®å¼‚ï¼Œé€ æˆäº†å†—ä½™è®¡ç®—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯å¯ä»¥ç”¨è§†è§‰è¯­ä¹‰ç©ºé—´ä¸­çš„ä¸€ç»„åŸºå‘é‡æ¥è¡¨ç¤ºã€‚å›¾åƒçš„å†…åœ¨è¯­ä¹‰å¤æ‚åº¦å¯¹åº”äºè·¨è¶Šè¯¥ç©ºé—´æ‰€éœ€çš„æœ€å°‘åŸºå‘é‡é›†åˆã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡å‡å°‘å†—ä½™Tokenï¼Œæå–æ›´å…·ä»£è¡¨æ€§çš„åŸºå‘é‡ï¼Œæ¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. ä½¿ç”¨æ ‡å‡†çš„ViTæ¨¡å‹æå–å›¾åƒçš„Tokenè¡¨ç¤ºã€‚2. å¼•å…¥æ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°å°†å†—ä½™Tokenèšç±»æˆä¸€ç»„æ­£äº¤åŸºã€‚3. ä½¿ç”¨è¿™äº›æ­£äº¤åŸºæ¥é‡æ„åŸå§‹çš„Tokenè¡¨ç¤ºï¼Œä»è€Œå®ç°Tokenæ•°é‡çš„å‹ç¼©ã€‚4. å°†å‹ç¼©åçš„Tokenè¡¨ç¤ºè¾“å…¥åˆ°åç»­çš„Transformerå±‚è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å›¾åƒçš„è¯­ä¹‰ç»“æ„ï¼Œå¹¶æå–ä¸€ç»„ç´§å‡‘çš„æ­£äº¤åŸºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦é¢„å…ˆè®¾å®šå›ºå®šçš„Tokenæ•°é‡ï¼Œè€Œæ˜¯æ ¹æ®å›¾åƒçš„è¯­ä¹‰å¤æ‚åº¦åŠ¨æ€åœ°è°ƒæ•´Tokenæ•°é‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ­ç¤ºäº†Tokenæ•°é‡ä¸æ¨¡å‹å¤§å°ä¹‹é—´çš„ç¼©æ”¾è§„å¾‹ï¼Œä¸ºæ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šæ­£äº¤æ»¤æ³¢æ¨¡å—çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. ä½¿ç”¨K-meansèšç±»ç®—æ³•å°†Tokenèšç±»æˆä¸åŒçš„ç°‡ã€‚2. å¯¹æ¯ä¸ªç°‡è¿›è¡Œæ­£äº¤åŒ–å¤„ç†ï¼Œå¾—åˆ°ä¸€ç»„æ­£äº¤åŸºã€‚3. ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ æ¯ä¸ªTokenå¯¹ä¸åŒæ­£äº¤åŸºçš„è´¡çŒ®ï¼Œä»è€Œå®ç°Tokençš„é‡æ„ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±å’Œæ­£äº¤æ€§æŸå¤±ï¼Œç”¨äºä¿è¯é‡æ„çš„å‡†ç¡®æ€§å’ŒåŸºå‘é‡çš„æ­£äº¤æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ImageNetå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œèƒ½å¤Ÿåœ¨ä¿æŒç”šè‡³æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘Tokenæ•°é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ViT-Bæ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†Tokenæ•°é‡å‡å°‘50%ï¼ŒåŒæ—¶Top-1å‡†ç¡®ç‡æå‡0.5%ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ­ç¤ºäº†Tokenæ•°é‡ä¸æ¨¡å‹å¤§å°ä¹‹é—´çš„ç¼©æ”¾è§„å¾‹ï¼Œå³æ›´å¤§çš„æ¨¡å‹éœ€è¦æ›´å°‘çš„Tokenæ¥è·¨è¶Šè§†è§‰è¯­ä¹‰ç©ºé—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ã€‚é€šè¿‡å‡å°‘Tokenæ•°é‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼Œä½¿å…¶æ›´æ˜“äºéƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè§†è§‰é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€å›¾åƒæè¿°ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates the fundamental relationship between model capacity and the minimal number of visual tokens required to preserve image semantics. Inspired by the Minimum Description Length principle, we reinterpret image tokens as vectors in a visual semantic space and define the intrinsic semantic complexity of an image as the smallest set of basis vectors needed to span this space. Building on this perspective, we propose Orthogonal Filtering, a lightweight module that adaptively clusters redundant tokens into a compact set of orthogonal bases. Through extensive experiments across a range of ViT models, we reveal a consistent token, model scaling law: larger models require significantly fewer tokens to span visual semantic space. Besides, we also contribute a visual long-context dataset.

