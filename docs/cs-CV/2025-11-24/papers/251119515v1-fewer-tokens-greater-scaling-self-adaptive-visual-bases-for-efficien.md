---
layout: default
title: Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning
---

# Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning

**arXiv**: [2511.19515v1](https://arxiv.org/abs/2511.19515) | [PDF](https://arxiv.org/pdf/2511.19515.pdf)

**ä½œè€…**: Shawn Young, Xingyu Zeng, Lijian Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è§†è§‰åŸºï¼Œå‡å°‘è§†è§‰Tokenæ•°é‡ï¼Œæå‡è§†è§‰è¡¨å¾å­¦ä¹ çš„æ•ˆçŽ‡å’Œå¯æ‰©å±•æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰Transformer` `TokenåŽ‹ç¼©` `æ­£äº¤æ»¤æ³¢` `è‡ªé€‚åº”è¡¨ç¤º` `æ¨¡åž‹ç¼©æ”¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰Transformeræ¨¡åž‹é€šå¸¸éœ€è¦å¤§é‡çš„Tokenæ¥è¡¨å¾å›¾åƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†æ¨¡åž‹çš„å¯æ‰©å±•æ€§ã€‚
2. è®ºæ–‡æå‡ºæ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°èšç±»å†—ä½™Tokenï¼Œç”Ÿæˆä¸€ç»„æ›´ç´§å‡‘çš„æ­£äº¤åŸºï¼Œä»Žè€Œå‡å°‘Tokenæ•°é‡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç”šè‡³æå‡æ¨¡åž‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†Tokenæ•°é‡ï¼Œå¹¶æ­ç¤ºäº†Tokenæ•°é‡ä¸Žæ¨¡åž‹å¤§å°ä¹‹é—´çš„ç¼©æ”¾è§„å¾‹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ¨¡åž‹å®¹é‡ä¸Žä¿æŒå›¾åƒè¯­ä¹‰æ‰€éœ€çš„æœ€å°è§†è§‰Tokenæ•°é‡ä¹‹é—´çš„æ ¹æœ¬å…³ç³»ã€‚å—æœ€å°æè¿°é•¿åº¦åŽŸåˆ™çš„å¯å‘ï¼Œæˆ‘ä»¬å°†å›¾åƒTokené‡æ–°è§£é‡Šä¸ºè§†è§‰è¯­ä¹‰ç©ºé—´ä¸­çš„å‘é‡ï¼Œå¹¶å°†å›¾åƒçš„å†…åœ¨è¯­ä¹‰å¤æ‚åº¦å®šä¹‰ä¸ºè·¨è¶Šè¯¥ç©ºé—´æ‰€éœ€çš„æœ€å°‘åŸºå‘é‡é›†åˆã€‚åŸºäºŽæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°å°†å†—ä½™Tokenèšç±»æˆä¸€ç»„ç´§å‡‘çš„æ­£äº¤åŸºã€‚é€šè¿‡å¯¹ä¸€ç³»åˆ—ViTæ¨¡åž‹è¿›è¡Œçš„å¤§é‡å®žéªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ä¸ªä¸€è‡´çš„Token-æ¨¡åž‹ç¼©æ”¾è§„å¾‹ï¼šæ›´å¤§çš„æ¨¡åž‹éœ€è¦æ˜Žæ˜¾æ›´å°‘çš„Tokenæ¥è·¨è¶Šè§†è§‰è¯­ä¹‰ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªè§†è§‰é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰Transformeræ¨¡åž‹ï¼Œç‰¹åˆ«æ˜¯ViTç³»åˆ—ï¼Œåœ¨å¤„ç†é«˜åˆ†è¾¨çŽ‡å›¾åƒæ—¶ï¼Œéœ€è¦å°†å›¾åƒåˆ†å‰²æˆå¤§é‡çš„Tokenï¼Œè¿™å¯¼è‡´è®¡ç®—å¤æ‚åº¦æ˜¾è‘—å¢žåŠ ï¼Œé™åˆ¶äº†æ¨¡åž‹çš„å¯æ‰©å±•æ€§ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„Tokenæ•°é‡ï¼Œå¿½ç•¥äº†å›¾åƒæœ¬èº«è¯­ä¹‰å¤æ‚åº¦çš„å·®å¼‚ï¼Œé€ æˆäº†å†—ä½™è®¡ç®—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯å¯ä»¥ç”¨è§†è§‰è¯­ä¹‰ç©ºé—´ä¸­çš„ä¸€ç»„åŸºå‘é‡æ¥è¡¨ç¤ºã€‚å›¾åƒçš„å†…åœ¨è¯­ä¹‰å¤æ‚åº¦å¯¹åº”äºŽè·¨è¶Šè¯¥ç©ºé—´æ‰€éœ€çš„æœ€å°‘åŸºå‘é‡é›†åˆã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡å‡å°‘å†—ä½™Tokenï¼Œæå–æ›´å…·ä»£è¡¨æ€§çš„åŸºå‘é‡ï¼Œæ¥é™ä½Žè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè®ºæ–‡æå‡ºçš„æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. ä½¿ç”¨æ ‡å‡†çš„ViTæ¨¡åž‹æå–å›¾åƒçš„Tokenè¡¨ç¤ºã€‚2. å¼•å…¥æ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°å°†å†—ä½™Tokenèšç±»æˆä¸€ç»„æ­£äº¤åŸºã€‚3. ä½¿ç”¨è¿™äº›æ­£äº¤åŸºæ¥é‡æž„åŽŸå§‹çš„Tokenè¡¨ç¤ºï¼Œä»Žè€Œå®žçŽ°Tokenæ•°é‡çš„åŽ‹ç¼©ã€‚4. å°†åŽ‹ç¼©åŽçš„Tokenè¡¨ç¤ºè¾“å…¥åˆ°åŽç»­çš„Transformerå±‚è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†æ­£äº¤æ»¤æ³¢æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å›¾åƒçš„è¯­ä¹‰ç»“æž„ï¼Œå¹¶æå–ä¸€ç»„ç´§å‡‘çš„æ­£äº¤åŸºã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦é¢„å…ˆè®¾å®šå›ºå®šçš„Tokenæ•°é‡ï¼Œè€Œæ˜¯æ ¹æ®å›¾åƒçš„è¯­ä¹‰å¤æ‚åº¦åŠ¨æ€åœ°è°ƒæ•´Tokenæ•°é‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ­ç¤ºäº†Tokenæ•°é‡ä¸Žæ¨¡åž‹å¤§å°ä¹‹é—´çš„ç¼©æ”¾è§„å¾‹ï¼Œä¸ºæ¨¡åž‹è®¾è®¡æä¾›äº†æ–°çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šæ­£äº¤æ»¤æ³¢æ¨¡å—çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. ä½¿ç”¨K-meansèšç±»ç®—æ³•å°†Tokenèšç±»æˆä¸åŒçš„ç°‡ã€‚2. å¯¹æ¯ä¸ªç°‡è¿›è¡Œæ­£äº¤åŒ–å¤„ç†ï¼Œå¾—åˆ°ä¸€ç»„æ­£äº¤åŸºã€‚3. ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ æ¯ä¸ªTokenå¯¹ä¸åŒæ­£äº¤åŸºçš„è´¡çŒ®ï¼Œä»Žè€Œå®žçŽ°Tokençš„é‡æž„ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æž„æŸå¤±å’Œæ­£äº¤æ€§æŸå¤±ï¼Œç”¨äºŽä¿è¯é‡æž„çš„å‡†ç¡®æ€§å’ŒåŸºå‘é‡çš„æ­£äº¤æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨ImageNetå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œèƒ½å¤Ÿåœ¨ä¿æŒç”šè‡³æå‡æ¨¡åž‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘Tokenæ•°é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ViT-Bæ¨¡åž‹ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å°†Tokenæ•°é‡å‡å°‘50%ï¼ŒåŒæ—¶Top-1å‡†ç¡®çŽ‡æå‡0.5%ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ­ç¤ºäº†Tokenæ•°é‡ä¸Žæ¨¡åž‹å¤§å°ä¹‹é—´çš„ç¼©æ”¾è§„å¾‹ï¼Œå³æ›´å¤§çš„æ¨¡åž‹éœ€è¦æ›´å°‘çš„Tokenæ¥è·¨è¶Šè§†è§‰è¯­ä¹‰ç©ºé—´ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦å¤„ç†é«˜åˆ†è¾¨çŽ‡å›¾åƒçš„è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ã€‚é€šè¿‡å‡å°‘Tokenæ•°é‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½Žè®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨¡åž‹çš„æŽ¨ç†é€Ÿåº¦ï¼Œä½¿å…¶æ›´æ˜“äºŽéƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽè§†è§‰é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€å›¾åƒæè¿°ç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper investigates the fundamental relationship between model capacity and the minimal number of visual tokens required to preserve image semantics. Inspired by the Minimum Description Length principle, we reinterpret image tokens as vectors in a visual semantic space and define the intrinsic semantic complexity of an image as the smallest set of basis vectors needed to span this space. Building on this perspective, we propose Orthogonal Filtering, a lightweight module that adaptively clusters redundant tokens into a compact set of orthogonal bases. Through extensive experiments across a range of ViT models, we reveal a consistent token, model scaling law: larger models require significantly fewer tokens to span visual semantic space. Besides, we also contribute a visual long-context dataset.

