---
layout: default
title: Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos
---

# Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos

**arXiv**: [2511.18856v1](https://arxiv.org/abs/2511.18856) | [PDF](https://arxiv.org/pdf/2511.18856.pdf)

**ä½œè€…**: Sana Alamgeer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆæ˜¾è‘—æ€§æ¨¡åž‹ä»¥é¢„æµ‹360åº¦è§†é¢‘ä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼Œä¼˜åŒ–æµåª’ä½“å¸¦å®½ä¸Žè§‚çœ‹ä½“éªŒã€‚**

**å…³é”®è¯**: `360åº¦è§†é¢‘` `æ„Ÿå…´è¶£åŒºåŸŸæ£€æµ‹` `æ··åˆæ˜¾è‘—æ€§æ¨¡åž‹` `è§†é¢‘æµåª’ä½“ä¼˜åŒ–` `å¤´æˆ´è®¾å¤‡è§‚çœ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š360åº¦è§†é¢‘ä¸­æ„Ÿå…´è¶£åŒºåŸŸæ£€æµ‹å¯¹å¸¦å®½ä¼˜åŒ–å’Œå¤´æˆ´è®¾å¤‡è§‚çœ‹ä½“éªŒè‡³å…³é‡è¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ··åˆæ˜¾è‘—æ€§æ¨¡åž‹ï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€æ¨¡åž‹é¢„æµ‹å’ŒåŽå¤„ç†æ­¥éª¤ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨360RATæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡åž‹æ€§èƒ½ï¼Œå¹¶ä¸Žä¸»è§‚æ ‡æ³¨è¿›è¡Œæ¯”è¾ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.

