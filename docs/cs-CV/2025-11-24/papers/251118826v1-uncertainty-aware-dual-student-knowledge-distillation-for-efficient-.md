---
layout: default
title: Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification
---

# Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification

**arXiv**: [2511.18826v1](https://arxiv.org/abs/2511.18826) | [PDF](https://arxiv.org/pdf/2511.18826.pdf)

**ä½œè€…**: Aakash Gore, Anoushka Dey, Aryan Mishra

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸ç¡®å®šæ€§æ„ŸçŸ¥åŒå­¦ç”ŸçŸ¥è¯†è’¸é¦æ¡†æž¶ä»¥æå‡å›¾åƒåˆ†ç±»æ•ˆçŽ‡**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `ä¸ç¡®å®šæ€§æ„ŸçŸ¥` `åŒå­¦ç”Ÿæž¶æž„` `å›¾åƒåˆ†ç±»` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•å¿½è§†æ•™å¸ˆé¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå½±å“å­¦ç”Ÿæ¨¡åž‹å­¦ä¹ æ•ˆæžœ
2. å¼•å…¥åŒå­¦ç”Ÿæž¶æž„ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥å’ŒåŒä¼´å­¦ä¹ æœºåˆ¶ä¼˜åŒ–çŸ¥è¯†ä¼ é€’
3. åœ¨ImageNet-100ä¸ŠéªŒè¯ï¼ŒResNet-18å’ŒMobileNetV2å‡†ç¡®çŽ‡åˆ†åˆ«æå‡2.04%å’Œ0.92%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.

