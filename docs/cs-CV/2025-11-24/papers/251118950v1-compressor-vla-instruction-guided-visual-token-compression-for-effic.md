---
layout: default
title: Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation
---

# Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation

**arXiv**: [2511.18950v1](https://arxiv.org/abs/2511.18950) | [PDF](https://arxiv.org/pdf/2511.18950.pdf)

**ä½œè€…**: Juntao Gao, Feiyang Ye, Jing Zhang, Wenjing Qian

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCompressor-VLAä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ä¸­è§†è§‰ä»¤ç‰Œå†—ä½™é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä»¤ç‰ŒåŽ‹ç¼©` `æœºå™¨äººæ“ä½œ` `æŒ‡ä»¤å¼•å¯¼` `è®¡ç®—æ•ˆçŽ‡` `sim-to-realè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰ä»¤ç‰Œå†—ä½™å¯¼è‡´è®¡ç®—å¼€é”€å¤§ï¼Œé˜»ç¢æœºå™¨äººå®žæ—¶éƒ¨ç½²
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè¯­ä¹‰ä»»åŠ¡åŽ‹ç¼©å™¨å’Œç©ºé—´ç»†åŒ–åŽ‹ç¼©å™¨ï¼ŒåŠ¨æ€è°ƒåˆ¶åŽ‹ç¼©
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIBEROåŸºå‡†ä¸ŠæˆåŠŸçŽ‡é«˜ï¼ŒFLOPså‡å°‘59%ï¼Œä»¤ç‰Œæ•°é™3å€ä»¥ä¸Š

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.

