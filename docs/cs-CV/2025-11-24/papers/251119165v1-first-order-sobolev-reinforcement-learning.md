---
layout: default
title: First-order Sobolev Reinforcement Learning
---

# First-order Sobolev Reinforcement Learning

**arXiv**: [2511.19165v1](https://arxiv.org/abs/2511.19165) | [PDF](https://arxiv.org/pdf/2511.19165.pdf)

**ä½œè€…**: Fabian Schramm, Nicolas Perrin-Gilbert, Justin Carpentier

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€é˜¶Sobolevå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ä¸€é˜¶è´å°”æ›¼ä¸€è‡´æ€§æ”¹è¿›TDå­¦ä¹ ï¼Œæå‡æ”¶æ•›ä¸Žç¨³å®šæ€§ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ—¶é—´å·®åˆ†å­¦ä¹ ` `SobolevæŸå¤±` `è´å°”æ›¼æ–¹ç¨‹` `ç­–ç•¥æ¢¯åº¦` `å€¼å‡½æ•°é€¼è¿‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸTDå­¦ä¹ ä»…åŒ¹é…å€¼å‡½æ•°ï¼Œå¿½ç•¥å¯¼æ•°ä¸€è‡´æ€§ï¼Œå¯èƒ½å½±å“æ”¶æ•›ä¸Žç¨³å®šæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¯å¾®åŠ¨æ€å¾®åˆ†è´å°”æ›¼å¤‡ä»½ï¼Œä½¿ç”¨SobolevæŸå¤±å‡½æ•°å¯¹é½å€¼ä¸Žå¯¼æ•°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¯é›†æˆçŽ°æœ‰ç®—æ³•å¦‚DDPGã€SACï¼Œæ½œåœ¨åŠ é€Ÿæ”¶æ•›å¹¶ç¨³å®šç­–ç•¥æ¢¯åº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.

