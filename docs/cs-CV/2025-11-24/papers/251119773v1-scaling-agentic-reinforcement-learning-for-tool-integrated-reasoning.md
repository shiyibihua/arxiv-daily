---
layout: default
title: Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs
---

# Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs

**arXiv**: [2511.19773v1](https://arxiv.org/abs/2511.19773) | [PDF](https://arxiv.org/pdf/2511.19773.pdf)

**ä½œè€…**: Meng Lu, Ran Xu, Yi Fang, Wenxuan Zhang, Yue Yu, Gaurav Srivastava, Yuchen Zhuang, Mohamed Elhoseiny, Charles Fleming, Carl Yang, Zhengzhong Tu, Yang Xie, Guanghua Xiao, Hanrui Wang, Di Jin, Wenqi Shi, Xuan Wang

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: 17 pages, 9 figures, work in progress

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VISTA-Gymï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å·¥å…·é›†æˆæŽ¨ç†æ–¹é¢çš„èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å·¥å…·é›†æˆæŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æŽ¨ç†` `VQA` `VISTA-Gym` `Agentic Reinforcement Learning`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¤šæ­¥è§†è§‰äº¤äº’æŽ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆâ€œæ€è€ƒå›¾åƒâ€ã€‚
2. VISTA-Gymé€šè¿‡ç»Ÿä¸€çš„æŽ¥å£ã€å¯æ‰§è¡Œå¾ªçŽ¯å’Œå¯éªŒè¯åé¦ˆï¼Œä¿ƒè¿›è§†è§‰Agentå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡åž‹å·¥å…·é›†æˆæŽ¨ç†èƒ½åŠ›ã€‚
3. VISTA-R1åœ¨å¤šä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ¨¡åž‹ï¼ŒéªŒè¯äº†VISTA-Gymçš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†VISTA-Gymï¼Œä¸€ä¸ªå¯æ‰©å±•çš„è®­ç»ƒçŽ¯å¢ƒï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰åœ¨å·¥å…·é›†æˆè§†è§‰æŽ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚VISTA-Gymé€šè¿‡æ ‡å‡†åŒ–çš„è§†è§‰å·¥å…·æŽ¥å£ï¼ˆä¾‹å¦‚ï¼Œå®šä½ã€è§£æžï¼‰ã€å¯æ‰§è¡Œçš„äº¤äº’å¾ªçŽ¯ã€å¯éªŒè¯çš„åé¦ˆä¿¡å·å’Œé«˜æ•ˆçš„è½¨è¿¹è®°å½•ï¼Œç»Ÿä¸€äº†å¤šç§çœŸå®žä¸–ç•Œçš„å¤šæ¨¡æ€æŽ¨ç†ä»»åŠ¡ï¼ˆæ€»å…±æ¥è‡ª13ä¸ªæ•°æ®é›†çš„7ä¸ªä»»åŠ¡ï¼‰ï¼Œä»Žè€Œå®žçŽ°å¤§è§„æ¨¡çš„è§†è§‰Agentå¼ºåŒ–å­¦ä¹ ã€‚å°½ç®¡çŽ°æœ‰çš„VLMsåœ¨çº¯æ–‡æœ¬æŽ¨ç†æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨å·¥å…·é€‰æ‹©ã€è°ƒç”¨å’Œåè°ƒæ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å€ŸåŠ©VISTA-Gymï¼Œæˆ‘ä»¬è®­ç»ƒäº†VISTA-R1ï¼Œé€šè¿‡å¤šè½®è½¨è¿¹é‡‡æ ·å’Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼Œå°†å·¥å…·ä½¿ç”¨ä¸ŽAgentæŽ¨ç†ç›¸ç»“åˆã€‚åœ¨11ä¸ªå…¬å¼€çš„æŽ¨ç†å¯†é›†åž‹VQAåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒVISTA-R1-8Bçš„æ€§èƒ½ä¼˜äºŽç±»ä¼¼è§„æ¨¡çš„state-of-the-artåŸºçº¿9.51%-18.72%ï¼Œè¯æ˜Žäº†VISTA-Gymæ˜¯é‡Šæ”¾VLMså·¥å…·é›†æˆæŽ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè®­ç»ƒå¹³å°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å›¾åƒæ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨éœ€è¦å¤šæ­¥éª¤è§†è§‰äº¤äº’çš„æŽ¨ç†ä»»åŠ¡ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡åž‹åœ¨å·¥å…·é€‰æ‹©ã€è°ƒç”¨å’Œåè°ƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— æ³•æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥è¾…åŠ©è§†è§‰æŽ¨ç†ã€‚çŽ°æœ‰æ–¹æ³•ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„è®­ç»ƒçŽ¯å¢ƒï¼Œæ¥ä¿ƒè¿›VLMsåœ¨å·¥å…·é›†æˆæŽ¨ç†æ–¹é¢çš„å­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªåä¸ºVISTA-Gymçš„è®­ç»ƒçŽ¯å¢ƒï¼Œè¯¥çŽ¯å¢ƒæä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æŽ¥å£ï¼Œç”¨äºŽä¸Žå„ç§è§†è§‰å·¥å…·è¿›è¡Œäº¤äº’ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒVISTA-Gymé¼“åŠ±VLMså­¦ä¹ å¦‚ä½•é€‰æ‹©ã€è°ƒç”¨å’Œåè°ƒè¿™äº›å·¥å…·ï¼Œä»Žè€Œæé«˜å…¶åœ¨å¤æ‚è§†è§‰æŽ¨ç†ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºŽå°†å·¥å…·ä½¿ç”¨ä¸ŽAgentæŽ¨ç†ç›¸ç»“åˆï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿé€šè¿‡å¤šè½®äº¤äº’æ¥é€æ­¥è§£å†³é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVISTA-Gymæ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š1) æ ‡å‡†åŒ–çš„è§†è§‰å·¥å…·æŽ¥å£ï¼Œå…è®¸VLMsä¸Žå„ç§å·¥å…·ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ï¼‰è¿›è¡Œäº¤äº’ï¼›2) å¯æ‰§è¡Œçš„äº¤äº’å¾ªçŽ¯ï¼Œå…è®¸VLMsé€šè¿‡å¤šè½®äº¤äº’æ¥é€æ­¥è§£å†³é—®é¢˜ï¼›3) å¯éªŒè¯çš„åé¦ˆä¿¡å·ï¼Œç”¨äºŽæŒ‡å¯¼VLMsçš„å­¦ä¹ è¿‡ç¨‹ï¼›4) é«˜æ•ˆçš„è½¨è¿¹è®°å½•ï¼Œç”¨äºŽå­˜å‚¨VLMsçš„äº¤äº’åŽ†å²ï¼Œä»¥ä¾¿è¿›è¡Œç¦»çº¿å­¦ä¹ ã€‚VISTA-R1æ¨¡åž‹åŸºäºŽæ­¤æ¡†æž¶ï¼Œé€šè¿‡å¤šè½®è½¨è¿¹é‡‡æ ·å’Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†VISTA-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè®­ç»ƒVLMsè¿›è¡Œå·¥å…·é›†æˆæŽ¨ç†è€Œè®¾è®¡çš„çŽ¯å¢ƒã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVISTA-Gymæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„å¹³å°ï¼Œå¯ä»¥æ”¯æŒå„ç§ä¸åŒçš„è§†è§‰æŽ¨ç†ä»»åŠ¡å’Œå·¥å…·ã€‚æ­¤å¤–ï¼ŒVISTA-Gymè¿˜å¼•å…¥äº†å¯æ‰§è¡Œçš„äº¤äº’å¾ªçŽ¯å’Œå¯éªŒè¯çš„åé¦ˆä¿¡å·ï¼Œä»Žè€Œä½¿VLMsèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¦‚ä½•ä½¿ç”¨å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šVISTA-R1æ¨¡åž‹é‡‡ç”¨äº†ä¸€ä¸ªTransformeræž¶æž„ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡åž‹ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¥ä¼˜åŒ–å…¶å·¥å…·é€‰æ‹©å’Œè°ƒç”¨ç­–ç•¥ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸€ä¸ªå¥–åŠ±é¡¹ï¼Œç”¨äºŽé¼“åŠ±æ¨¡åž‹é€‰æ‹©æ­£ç¡®çš„å·¥å…·å¹¶èŽ·å¾—æ­£ç¡®çš„ç­”æ¡ˆï¼Œä»¥åŠä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºŽé˜²æ­¢æ¨¡åž‹è¿‡åº¦ä¾èµ–æŸäº›å·¥å…·ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡åž‹é€šè¿‡ä¸ŽVISTA-GymçŽ¯å¢ƒè¿›è¡Œäº¤äº’æ¥æ”¶é›†è®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®æ¥æ›´æ–°å…¶å‚æ•°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨11ä¸ªå…¬å¼€çš„æŽ¨ç†å¯†é›†åž‹VQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVISTA-R1-8Bçš„æ€§èƒ½ä¼˜äºŽç±»ä¼¼è§„æ¨¡çš„state-of-the-artåŸºçº¿9.51%-18.72%ã€‚è¿™è¡¨æ˜ŽVISTA-Gymæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„è®­ç»ƒå¹³å°ï¼Œå¯ä»¥æ˜¾è‘—æé«˜VLMsåœ¨å·¥å…·é›†æˆæŽ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå…·ä½“benchmarkä¸Šï¼ŒVISTA-R1-8Bè¾¾åˆ°äº†XX%çš„å‡†ç¡®çŽ‡ï¼Œè€Œä¹‹å‰çš„æœ€ä½³æ¨¡åž‹åªæœ‰YY%ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½åŠ©æ‰‹å¯ä»¥åˆ©ç”¨å·¥å…·é›†æˆæŽ¨ç†èƒ½åŠ›ï¼Œæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„è§†è§‰æŸ¥è¯¢å¹¶æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥èƒ½åŠ›æ¥è¯†åˆ«äº¤é€šæ ‡å¿—ã€è¡Œäººå’Œå…¶ä»–è½¦è¾†ï¼Œä»Žè€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åŒ»ç–—è¯Šæ–­ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥èƒ½åŠ›æ¥åˆ†æžåŒ»å­¦å›¾åƒï¼Œä»Žè€Œå¸®åŠ©åŒ»ç”Ÿåšå‡ºæ›´å‡†ç¡®çš„è¯Šæ–­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

