---
layout: default
title: Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction
---

# Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction

**arXiv**: [2511.18873v1](https://arxiv.org/abs/2511.18873) | [PDF](https://arxiv.org/pdf/2511.18873.pdf)

**ä½œè€…**: Yiming Wang, Shaofei Wang, Marko Mihajlovic, Siyu Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¥žç»çº¹ç†æº…å°„ä»¥å¢žå¼º3Dé«˜æ–¯æº…å°„åœ¨è§†å›¾åˆæˆã€å‡ ä½•å’ŒåŠ¨æ€é‡å»ºä¸­çš„è¡¨è¾¾åŠ›**

**å…³é”®è¯**: `3Dé«˜æ–¯æº…å°„` `ç¥žç»çº¹ç†æº…å°„` `è§†å›¾åˆæˆ` `å‡ ä½•é‡å»º` `åŠ¨æ€é‡å»º` `å…¨å±€ç¥žç»åœº`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. 3Dé«˜æ–¯æº…å°„è¡¨ç¤ºèƒ½åŠ›å—é™ï¼Œéš¾ä»¥å¤„ç†ä¸€èˆ¬é‡å»ºä»»åŠ¡
2. å¼•å…¥å…¨å±€ç¥žç»åœºé¢„æµ‹å±€éƒ¨å¤–è§‚å’Œå‡ ä½•ï¼Œå‡å°‘æ¨¡åž‹å¤§å°å¹¶ä¿ƒè¿›ä¿¡æ¯äº¤æ¢
3. å®žéªŒæ˜¾ç¤ºåœ¨ç¨€ç–å’Œå¯†é›†è¾“å…¥ä¸‹ï¼Œå¤šåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

