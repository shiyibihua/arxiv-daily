---
layout: default
title: Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation
---

# Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation

**arXiv**: [2511.19049v1](https://arxiv.org/abs/2511.19049) | [PDF](https://arxiv.org/pdf/2511.19049.pdf)

**ä½œè€…**: Ruojun Xu, Yu Kai, Xuhua Ren, Jiaxiang Cheng, Bing Ma, Tianxiang Zheng, Qinhlin Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPG-DPOä»¥è§£å†³æ‰©æ•£æ¨¡åž‹ä¸­ä¼¼ç„¶ä½ç§»é—®é¢˜ï¼Œæå‡è§†é¢‘ç”Ÿæˆåå¥½å¯¹é½**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `ç›´æŽ¥åå¥½ä¼˜åŒ–` `ä¼¼ç„¶ä½ç§»` `è§†é¢‘ç”Ÿæˆ` `åå¥½å¯¹é½` `ä¼˜åŒ–å†²çª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šDPOåœ¨æ‰©æ•£æ¨¡åž‹ä¸­å¯¼è‡´ä¼¼ç„¶ä½ç§»ï¼Œé™ä½Žç”Ÿæˆè´¨é‡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥PG-DPOï¼Œç»“åˆARSå’ŒIPRç¼“è§£ä¼˜åŒ–å†²çªä¸Žæ¬¡ä¼˜æœ€å¤§åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šPG-DPOåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.

