---
layout: default
title: Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization
---

# Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization

**arXiv**: [2511.18851v1](https://arxiv.org/abs/2511.18851) | [PDF](https://arxiv.org/pdf/2511.18851.pdf)

**ä½œè€…**: Yilin Wen, Kechuan Dong, Yusuke Sugano

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè¿åŠ¨ç¦»æ•£åŒ–çš„é•¿æ—¶æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ï¼Œä»¥è§£å†³3Däººä½“å§¿æ€ä¼°è®¡ä¸­çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚**

**å…³é”®è¯**: `3Däººä½“å§¿æ€ä¼°è®¡` `æµ‹è¯•æ—¶é€‚åº”` `è¿åŠ¨ç¦»æ•£åŒ–` `è¯¯å·®ç´¯ç§¯ç¼“è§£` `åœ¨çº¿å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨çº¿æµ‹è¯•æ—¶é€‚åº”ä¸­ï¼Œä¾èµ–ä¸å®Œç¾Žé¢„æµ‹çš„è‡ªç›‘ç£å¯¼è‡´è¯¯å·®ç´¯ç§¯ï¼Œæ€§èƒ½éšæ—¶é—´ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ— ç›‘ç£èšç±»èŽ·å–é”šå®šè¿åŠ¨ï¼Œåˆ©ç”¨å…¶è§„å¾‹æ€§ç›‘ç£å§¿æ€ä¼°è®¡å™¨å¹¶å®žçŽ°é«˜æ•ˆè‡ªå›žæ”¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é•¿æ—¶åœ¨çº¿é€‚åº”å®žéªŒä¸­ï¼Œæ–¹æ³•ä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼ŒéªŒè¯äº†è®¾è®¡æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

