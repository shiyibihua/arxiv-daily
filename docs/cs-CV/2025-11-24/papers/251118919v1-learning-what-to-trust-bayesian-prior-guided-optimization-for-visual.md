---
layout: default
title: Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation
---

# Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation

**arXiv**: [2511.18919v1](https://arxiv.org/abs/2511.18919) | [PDF](https://arxiv.org/pdf/2511.18919.pdf)

**ä½œè€…**: Ruiying Liu, Yuanzhi Liang, Haibin Huang, Tianshu Yu, Chi Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè´å¶æ–¯å…ˆéªŒå¼•å¯¼ä¼˜åŒ–ä»¥è§£å†³è§†è§‰ç”Ÿæˆä¸­å¥–åŠ±ä¸ç¡®å®šæ€§é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰ç”Ÿæˆ` `è´å¶æ–¯ä¼˜åŒ–` `å¥–åŠ±æ¨¡åž‹` `è¯­ä¹‰å¯¹é½` `åŽè®­ç»ƒä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬ä¸Žè§†è§‰å¯¹åº”å…³ç³»æ¨¡ç³Šï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸ç¡®å®šå’Œå¼±åŒºåˆ†æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¯­ä¹‰å…ˆéªŒé”šå»ºæ¨¡å¥–åŠ±ä¸ç¡®å®šæ€§ï¼Œè‡ªé€‚åº”è°ƒèŠ‚ç»„é—´å’Œç»„å†…ä¿¡ä»»
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­ï¼Œè¯­ä¹‰å¯¹é½ã€æ„ŸçŸ¥ä¿çœŸåº¦å’Œæ”¶æ•›é€Ÿåº¦å‡æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.

