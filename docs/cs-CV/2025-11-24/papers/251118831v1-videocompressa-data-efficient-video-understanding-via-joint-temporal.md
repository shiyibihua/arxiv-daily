---
layout: default
title: VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction
---

# VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction

**arXiv**: [2511.18831v1](https://arxiv.org/abs/2511.18831) | [PDF](https://arxiv.org/pdf/2511.18831.pdf)

**ä½œè€…**: Shaobo Wang, Tianle Niu, Runkang Yang, Deshan Liu, Xu He, Zichen Wen, Conghui He, Xuming Hu, Linfeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoCompressaæ¡†æž¶ï¼Œé€šè¿‡è”åˆæ—¶é—´åŽ‹ç¼©å’Œç©ºé—´é‡å»ºè§£å†³è§†é¢‘æ•°æ®æ•ˆçŽ‡é—®é¢˜**

**å…³é”®è¯**: `è§†é¢‘æ•°æ®åˆæˆ` `æ—¶é—´åŽ‹ç¼©` `ç©ºé—´é‡å»º` `æ•°æ®æ•ˆçŽ‡` `å…³é”®å¸§é€‰æ‹©` `å˜åˆ†è‡ªç¼–ç å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æ•°æ®æ•ˆçŽ‡ä½ŽæºäºŽå¸§çº§å†—ä½™ï¼Œè€Œéžæ ·æœ¬é—´å†—ä½™
2. æ–¹æ³•è¦ç‚¹ï¼šè”åˆä¼˜åŒ–å¯å¾®åˆ†å…³é”®å¸§é€‰æ‹©å™¨å’ŒVAEï¼ŒåŽ‹ç¼©å¸§ä¸ºè¯­ä¹‰ä¸°å¯Œæ½œç 
3. å®žéªŒæ•ˆæžœï¼šåœ¨UCF101ä¸Šä»…ç”¨0.13%æ•°æ®è¶…è¶Šå…¨æ•°æ®è®­ç»ƒï¼Œé€Ÿåº¦æå‡5800å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\% points using only 0.13\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\% of the training data-outperforming zero-shot baseline by 10.61\%.

