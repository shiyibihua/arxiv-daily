---
layout: default
title: VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking
---

# VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking

**arXiv**: [2511.18692v1](https://arxiv.org/abs/2511.18692) | [PDF](https://arxiv.org/pdf/2511.18692.pdf)

**ä½œè€…**: Kichang Yang, Seonjun Kim, Minjae Kim, Nairan Zhang, Chi Zhang, Youngki Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¥žç»å…ƒåˆ†å—æ–¹æ³•ä»¥ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„I/Oæ•ˆçŽ‡**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `I/Oä¼˜åŒ–` `æ¿€æ´»ç¨€ç–åŒ–` `è¾¹ç¼˜è®¡ç®—` `ç¥žç»å…ƒåˆ†å—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ¿€æ´»ç¨€ç–åŒ–ä»…åŸºäºŽç¥žç»å…ƒé‡è¦æ€§ï¼Œå¿½ç•¥å­˜å‚¨è®¿é—®æ¨¡å¼å¯¹é—ªå­˜æ€§èƒ½çš„å½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†å—æ“ä½œï¼Œç»“åˆç¥žç»å…ƒé‡è¦æ€§å’Œè®¿é—®è¿žç»­æ€§ï¼Œé€‰æ‹©é«˜æ•ˆç”¨å—ä»¥å‡å°‘I/Oå¼€é”€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Jetsonè®¾å¤‡ä¸Šï¼ŒI/Oæ•ˆçŽ‡æå‡æœ€é«˜è¾¾4.65å€å’Œ5.76å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

