---
layout: default
title: VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning
---

# VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning

**arXiv**: [2511.19524v1](https://arxiv.org/abs/2511.19524) | [PDF](https://arxiv.org/pdf/2511.19524.pdf)

**ä½œè€…**: Boyu Chen, Zikang Wang, Zhengrong Yue, Kainan Yan, Chenyun Yu, Yi Huang, Zijun Liu, Yafei Wen, Xiaoxin Chen, Yang Liu, Peng Li, Yali Wang

**åˆ†ç±»**: cs.CV, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: 21 pages, 9 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoChat-M1ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å®žçŽ°è§†é¢‘ç†è§£çš„ååŒç­–ç•¥è§„åˆ’ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `ååŒç­–ç•¥è§„åˆ’` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å·¥å…·å¢žå¼ºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘ç†è§£ä¸­é‡‡ç”¨é™æ€ä¸”ä¸å¯å­¦ä¹ çš„å·¥å…·è°ƒç”¨æœºåˆ¶ï¼Œé™åˆ¶äº†å¯¹å¤æ‚è§†é¢‘çº¿ç´¢çš„å‘çŽ°ã€‚
2. VideoChat-M1é‡‡ç”¨ååŒç­–ç•¥è§„åˆ’ï¼ˆCPPï¼‰èŒƒå¼ï¼Œé€šè¿‡å¤šä¸ªç­–ç•¥æ™ºèƒ½ä½“ååŒå·¥ä½œï¼ŒåŠ¨æ€ä¼˜åŒ–ç­–ç•¥ä»¥å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚
3. é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•è”åˆä¼˜åŒ–ç­–ç•¥æ™ºèƒ½ä½“å›¢é˜Ÿï¼Œå®žéªŒè¡¨æ˜ŽVideoChat-M1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAæ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºŽè§†é¢‘ç†è§£çš„æ–°åž‹å¤šæ™ºèƒ½ä½“ç³»ç»ŸVideoChat-M1ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ç‹¬ç‰¹çš„ååŒç­–ç•¥è§„åˆ’ï¼ˆCPPï¼‰èŒƒå¼ï¼ŒåŒ…å«å¤šä¸ªç­–ç•¥æ™ºèƒ½ä½“ï¼Œè€Œéžå•ä¸€æˆ–å›ºå®šçš„ç­–ç•¥ã€‚CPPåŒ…å«ä¸‰ä¸ªå…³é”®è¿‡ç¨‹ï¼š(1)ç­–ç•¥ç”Ÿæˆï¼šæ¯ä¸ªæ™ºèƒ½ä½“æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆç‹¬ç‰¹çš„å·¥å…·è°ƒç”¨ç­–ç•¥ï¼›(2)ç­–ç•¥æ‰§è¡Œï¼šæ¯ä¸ªæ™ºèƒ½ä½“ä¾æ¬¡è°ƒç”¨ç›¸å…³å·¥å…·æ¥æ‰§è¡Œå…¶ç­–ç•¥å¹¶æŽ¢ç´¢è§†é¢‘å†…å®¹ï¼›(3)ç­–ç•¥é€šä¿¡ï¼šåœ¨ç­–ç•¥æ‰§è¡Œçš„ä¸­é—´é˜¶æ®µï¼Œæ™ºèƒ½ä½“ç›¸äº’äº¤äº’ä»¥æ›´æ–°å„è‡ªçš„ç­–ç•¥ã€‚é€šè¿‡è¿™ç§ååŒæ¡†æž¶ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“ååŒå·¥ä½œï¼ŒåŸºäºŽæ¥è‡ªåŒè¡Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ¨æ€åœ°æ”¹è¿›å…¶ç­–ç•¥ï¼Œä»Žè€Œæœ‰æ•ˆåœ°å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºCPPèŒƒå¼é…å¤‡äº†ä¸€ç§ç®€æ´çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•ã€‚å› æ­¤ï¼Œå¯ä»¥è”åˆä¼˜åŒ–ç­–ç•¥æ™ºèƒ½ä½“å›¢é˜Ÿï¼Œä»¥æé«˜VideoChat-M1çš„æ€§èƒ½ï¼Œå¹¶ç”±æœ€ç»ˆç­”æ¡ˆå¥–åŠ±å’Œä¸­é—´åä½œè¿‡ç¨‹åé¦ˆæ¥æŒ‡å¯¼ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒVideoChat-M1åœ¨è·¨è¶Šå››ä¸ªä»»åŠ¡çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†SOTAæ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨LongVideoBenchä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºŽSOTAæ¨¡åž‹Gemini 2.5 pro 3.6%ï¼Œä¼˜äºŽGPT-4o 15.6%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽå·¥å…·å¢žå¼ºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œé€šå¸¸é‡‡ç”¨é™æ€æˆ–å›ºå®šçš„å·¥å…·è°ƒç”¨ç­–ç•¥ï¼Œæ— æ³•å……åˆ†æŒ–æŽ˜è§†é¢‘ä¸­å­˜åœ¨çš„æ—¶åºæˆ–ç©ºé—´ä¸Šçš„å¤æ‚çº¿ç´¢ã€‚è¿™å¯¼è‡´æ¨¡åž‹åœ¨å¤„ç†é•¿è§†é¢‘æˆ–éœ€è¦å¤æ‚æŽ¨ç†çš„è§†é¢‘ç†è§£ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½å—åˆ°é™åˆ¶ã€‚çŽ°æœ‰æ–¹æ³•ç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œå­¦ä¹ çš„èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”ä¸åŒè§†é¢‘å†…å®¹å’Œç”¨æˆ·æŸ¥è¯¢çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVideoChat-M1çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥å¤šæ™ºèƒ½ä½“ååŒç­–ç•¥è§„åˆ’ï¼ˆCPPï¼‰èŒƒå¼ï¼Œè®©å¤šä¸ªæ™ºèƒ½ä½“å„è‡ªç”Ÿæˆå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨ç­–ç•¥ï¼Œå¹¶é€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡å’Œåä½œï¼ŒåŠ¨æ€åœ°è°ƒæ•´å’Œä¼˜åŒ–è¿™äº›ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æ¨¡æ‹Ÿäººç±»å›¢é˜Ÿåä½œè§£å†³é—®é¢˜çš„è¿‡ç¨‹ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£æŽ¢ç´¢è§†é¢‘çš„ä¸åŒæ–¹é¢ï¼Œå¹¶é€šè¿‡ä¿¡æ¯å…±äº«å’Œç­–ç•¥è°ƒæ•´ï¼Œå…±åŒå®Œæˆè§†é¢‘ç†è§£ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVideoChat-M1çš„æ•´ä½“æ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç­–ç•¥ç”Ÿæˆã€ç­–ç•¥æ‰§è¡Œå’Œç­–ç•¥é€šä¿¡ã€‚åœ¨ç­–ç•¥ç”Ÿæˆé˜¶æ®µï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªç‹¬ç‰¹çš„å·¥å…·è°ƒç”¨ç­–ç•¥ã€‚åœ¨ç­–ç•¥æ‰§è¡Œé˜¶æ®µï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æŒ‰ç…§å…¶ç­–ç•¥ä¾æ¬¡è°ƒç”¨ç›¸å…³å·¥å…·æ¥æŽ¢ç´¢è§†é¢‘å†…å®¹ã€‚åœ¨ç­–ç•¥é€šä¿¡é˜¶æ®µï¼Œæ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œä¿¡æ¯äº¤äº’ï¼Œæ›´æ–°å„è‡ªçš„ç­–ç•¥ã€‚æ•´ä¸ªè¿‡ç¨‹é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œåˆ©ç”¨æœ€ç»ˆç­”æ¡ˆå¥–åŠ±å’Œä¸­é—´åä½œè¿‡ç¨‹åé¦ˆæ¥æŒ‡å¯¼æ™ºèƒ½ä½“çš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šVideoChat-M1çš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ååŒç­–ç•¥è§„åˆ’ï¼ˆCPPï¼‰èŒƒå¼å’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•çš„ç»“åˆã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•é‡‡ç”¨çš„å•ä¸€æˆ–å›ºå®šç­–ç•¥ä¸åŒï¼ŒCPPå…è®¸æ¯ä¸ªæ™ºèƒ½ä½“ç”Ÿæˆå¹¶æ‰§è¡Œä¸åŒçš„ç­–ç•¥ï¼Œä»Žè€Œå®žçŽ°å¯¹è§†é¢‘å†…å®¹æ›´å…¨é¢çš„æŽ¢ç´¢ã€‚MARLæ–¹æ³•çš„å¼•å…¥ä½¿å¾—æ™ºèƒ½ä½“å›¢é˜Ÿèƒ½å¤Ÿé€šè¿‡å­¦ä¹ ä¸æ–­ä¼˜åŒ–å…¶ç­–ç•¥ï¼Œä»Žè€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šVideoChat-M1é‡‡ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå¥–åŠ±å‡½æ•°åŒ…æ‹¬æœ€ç»ˆç­”æ¡ˆçš„å¥–åŠ±å’Œä¸­é—´åä½œè¿‡ç¨‹çš„åé¦ˆã€‚å…·ä½“æ¥è¯´ï¼Œæœ€ç»ˆç­”æ¡ˆçš„å¥–åŠ±ç”¨äºŽé¼“åŠ±æ™ºèƒ½ä½“ç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆï¼Œè€Œä¸­é—´åä½œè¿‡ç¨‹çš„åé¦ˆåˆ™ç”¨äºŽé¼“åŠ±æ™ºèƒ½ä½“ä¹‹é—´çš„æœ‰æ•ˆé€šä¿¡å’Œç­–ç•¥è°ƒæ•´ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

VideoChat-M1åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†SOTAæ€§èƒ½ï¼Œå°¤å…¶åœ¨LongVideoBenchä¸Šï¼Œè¶…è¶Šäº†Gemini 2.5 pro 3.6%ï¼Œè¶…è¶Šäº†GPT-4o 15.6%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘å’Œéœ€è¦å¤æ‚æŽ¨ç†çš„è§†é¢‘ç†è§£ä»»åŠ¡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

VideoChat-M1åœ¨è§†é¢‘ç†è§£é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºŽæ™ºèƒ½è§†é¢‘ç›‘æŽ§ã€è§†é¢‘å†…å®¹åˆ†æžã€æ™ºèƒ½å®¢æœã€æ•™è‚²è§†é¢‘ç†è§£ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ï¼Œè¯¥ç ”ç©¶å¯ä»¥å¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨è§†é¢‘ä¿¡æ¯ï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œç¤¾ä¼šæ„ä¹‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºŽæ›´å¤æ‚çš„è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘ç”Ÿæˆã€è§†é¢‘ç¼–è¾‘ç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> By leveraging tool-augmented Multimodal Large Language Models (MLLMs), multi-agent frameworks are driving progress in video understanding. However, most of them adopt static and non-learnable tool invocation mechanisms, which limit the discovery of diverse clues essential for robust perception and reasoning regarding temporally or spatially complex videos. To address this challenge, we propose a novel Multi-agent system for video understanding, namely VideoChat-M1. Instead of using a single or fixed policy, VideoChat-M1 adopts a distinct Collaborative Policy Planning (CPP) paradigm with multiple policy agents, which comprises three key processes. (1) Policy Generation: Each agent generates its unique tool invocation policy tailored to the user's query; (2) Policy Execution: Each agent sequentially invokes relevant tools to execute its policy and explore the video content; (3) Policy Communication: During the intermediate stages of policy execution, agents interact with one another to update their respective policies. Through this collaborative framework, all agents work in tandem, dynamically refining their preferred policies based on contextual insights from peers to effectively respond to the user's query. Moreover, we equip our CPP paradigm with a concise Multi-Agent Reinforcement Learning (MARL) method. Consequently, the team of policy agents can be jointly optimized to enhance VideoChat-M1's performance, guided by both the final answer reward and intermediate collaborative process feedback. Extensive experiments demonstrate that VideoChat-M1 achieves SOTA performance across eight benchmarks spanning four tasks. Notably, on LongVideoBench, our method outperforms the SOTA model Gemini 2.5 pro by 3.6% and GPT-4o by 15.6%.

