---
layout: default
title: Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks
---

# Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks

**arXiv**: [2511.19198v1](https://arxiv.org/abs/2511.19198) | [PDF](https://arxiv.org/pdf/2511.19198.pdf)

**ä½œè€…**: Ann-Sophia MÃ¼ller, Moonkwang Jeong, Meng Zhang, Jiyuan Tian, Arkadiusz Miernik, Stefanie Speidel, Tian Qiu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽç‰©ç†æ¨¡åž‹å’Œ3D GANçš„è‡ªåŠ¨åŒ–3Dè§£å‰–æ•°æ®ç”Ÿæˆå·¥ä½œæµï¼Œä»¥è§£å†³æ‰‹æœ¯è§„åˆ’ä¸­æ•°æ®èŽ·å–ç“¶é¢ˆã€‚**

**å…³é”®è¯**: `3Dè§£å‰–æ•°æ®ç”Ÿæˆ` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ` `åŒ»å­¦å›¾åƒåˆ†å‰²` `æ‰‹æœ¯è§„åˆ’` `è¶…å£°æˆåƒ` `ç‰©ç†æ¨¡åž‹ä»¿çœŸ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹æœ¯è§„åˆ’ä¾èµ–3Dè§£å‰–æ¨¡åž‹ï¼Œä½†çœŸå®žæ‚£è€…æ•°æ®èŽ·å–é¢ä¸´æ³•å¾‹ã€ä¼¦ç†å’ŒæŠ€æœ¯æŒ‘æˆ˜ï¼Œå°¤å…¶å¯¹ä½Žå¯¹æ¯”åº¦è½¯ç»„ç»‡å™¨å®˜å¦‚å‰åˆ—è…ºã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç‰©ç†å™¨å®˜æ¨¡åž‹å’Œ3D GANç”Ÿæˆ3Dæ•°æ®ï¼Œè®­ç»ƒç¥žç»ç½‘ç»œåˆ†å‰²è¶…å£°å›¾åƒï¼Œå¹¶é‡å»º3Dç½‘æ ¼æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äººå·¥å‰åˆ—è…ºæ¨¡åž‹ä¸ŠéªŒè¯ï¼Œç¥žç»ç½‘ç»œåˆ†å‰²çš„IoUä¼˜äºŽä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼Œå¹¶æä¾›æ€§èƒ½åé¦ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.

