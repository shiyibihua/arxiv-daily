---
layout: default
title: Learning Plug-and-play Memory for Guiding Video Diffusion Models
---

# Learning Plug-and-play Memory for Guiding Video Diffusion Models

**arXiv**: [2511.19229v1](https://arxiv.org/abs/2511.19229) | [PDF](https://arxiv.org/pdf/2511.19229.pdf)

**ä½œè€…**: Selena Song, Ziming Xu, Zijun Zhang, Kun Zhou, Jiaxian Guo, Lianhui Qin, Biwei Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯æ’æ‹”è®°å¿†æ¨¡å—ä»¥æå‡è§†é¢‘æ‰©æ•£æ¨¡åž‹çš„ä¸–ç•ŒçŸ¥è¯†æ³¨å…¥èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡åž‹` `å¯æ’æ‹”è®°å¿†` `ä¸–ç•ŒçŸ¥è¯†æ³¨å…¥` `Transformerå¹²é¢„` `é«˜æ•ˆè®­ç»ƒ` `ç‰©ç†è§„åˆ™éµå¾ª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘æ‰©æ•£æ¨¡åž‹å¸¸è¿åç‰©ç†è§„å¾‹å’Œå¸¸è¯†åŠ¨æ€ï¼Œç¼ºä¹æ˜¾å¼ä¸–ç•ŒçŸ¥è¯†
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å¯å­¦ä¹ è®°å¿†ç¼–ç å™¨ï¼Œé€šè¿‡å¹²é¢„éšè—çŠ¶æ€æ³¨å…¥å‚è€ƒè§†é¢‘çŸ¥è¯†
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å°‘é‡æ•°æ®å’Œå‚æ•°ä¸‹é«˜æ•ˆè®­ç»ƒï¼Œæå‡è§†é¢‘ç‰©ç†è§„åˆ™éµå¾ªå’Œä¿çœŸåº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.

