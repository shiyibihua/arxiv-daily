---
layout: default
title: Any4D: Open-Prompt 4D Generation from Natural Language and Images
---

# Any4D: Open-Prompt 4D Generation from Natural Language and Images

**arXiv**: [2511.18746v1](https://arxiv.org/abs/2511.18746) | [PDF](https://arxiv.org/pdf/2511.18746.pdf)

**ä½œè€…**: Hao Li, Qiao Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPrimitive Embodied World Modelsä»¥è§£å†³å…·èº«æ•°æ®ç¨€ç¼ºä¸Žé•¿è§†é¢‘ç”Ÿæˆéš¾é¢˜**

**å…³é”®è¯**: `å…·èº«ä¸–ç•Œæ¨¡åž‹` `è§†é¢‘ç”Ÿæˆ` `è¯­è¨€-åŠ¨ä½œå¯¹é½` `é—­çŽ¯æŽ§åˆ¶` `æ•°æ®æ•ˆçŽ‡` `æŽ¨ç†ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…·èº«æ•°æ®ç¨€ç¼ºã€é«˜ç»´å’Œæ”¶é›†å›°éš¾ï¼Œé™åˆ¶è¯­è¨€ä¸ŽåŠ¨ä½œçš„ç»†ç²’åº¦å¯¹é½å’Œé•¿è§†é¢‘ç”Ÿæˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé™åˆ¶è§†é¢‘ç”Ÿæˆä¸ºçŸ­æ—¶åŸŸï¼Œç»“åˆVLMè§„åˆ’å™¨å’ŒSGGæœºåˆ¶ï¼Œå®žçŽ°ç»†ç²’åº¦å¯¹é½å’Œé—­çŽ¯æŽ§åˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæé«˜æ•°æ®æ•ˆçŽ‡ã€é™ä½ŽæŽ¨ç†å»¶è¿Ÿï¼Œæ”¯æŒåŽŸå§‹ç­–ç•¥åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ç»„åˆæ³›åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

