---
layout: default
title: BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment
---

# BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment

**arXiv**: [2511.19268v1](https://arxiv.org/abs/2511.19268) | [PDF](https://arxiv.org/pdf/2511.19268.pdf)

**ä½œè€…**: Dewei Zhou, Mingwei Li, Zongxin Yang, Yu Lu, Yunqiu Xu, Zhizhong Wang, Zeyi Huang, Yi Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBideDPOæ¡†æž¶ä»¥è§£å†³æ¡ä»¶å›¾åƒç”Ÿæˆä¸­çš„æ–‡æœ¬ä¸Žæ¡ä»¶å†²çªé—®é¢˜**

**å…³é”®è¯**: `æ¡ä»¶å›¾åƒç”Ÿæˆ` `åå¥½ä¼˜åŒ–` `æ¢¯åº¦è§£è€¦` `å†²çªè§£å†³` `è‡ªé€‚åº”æŸå¤±å¹³è¡¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¡ä»¶å›¾åƒç”Ÿæˆä¸­æ–‡æœ¬ä¸Žæ¡ä»¶æºå†²çªï¼ŒåŒ…æ‹¬è¾“å…¥çº§å’Œæ¨¡åž‹åç½®å†²çª
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŒå‘è§£è€¦åå¥½å¯¹å’Œè‡ªé€‚åº”æŸå¤±å¹³è¡¡ç­–ç•¥å‡å°‘æ¢¯åº¦çº ç¼ 
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨DualAlignåŸºå‡†ä¸Šæ˜¾è‘—æå‡æ–‡æœ¬æˆåŠŸçŽ‡å’Œæ¡ä»¶éµå¾ªåº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.

