---
layout: default
title: DiP: Taming Diffusion Models in Pixel Space
---

# DiP: Taming Diffusion Models in Pixel Space

**arXiv**: [2511.18822v1](https://arxiv.org/abs/2511.18822) | [PDF](https://arxiv.org/pdf/2511.18822.pdf)

**ä½œè€…**: Zhennan Chen, Junwei Zhu, Xu Chen, Jiangning Zhang, Xiaobin Hu, Hanzhen Zhao, Chengjie Wang, Jian Yang, Ying Tai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiPæ¡†æž¶ä»¥è§£å†³æ‰©æ•£æ¨¡åž‹åœ¨åƒç´ ç©ºé—´çš„è®¡ç®—æ•ˆçŽ‡ä¸Žè´¨é‡æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹` `åƒç´ ç©ºé—´ç”Ÿæˆ` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–` `å…¨å±€å±€éƒ¨è§£è€¦` `è½»é‡ç»†èŠ‚æ¢å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡åž‹é¢ä¸´ç”Ÿæˆè´¨é‡ä¸Žè®¡ç®—æ•ˆçŽ‡çš„æ ¹æœ¬æƒè¡¡ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡åž‹æ•ˆçŽ‡é«˜ä½†ä¿¡æ¯æŸå¤±
2. DiPå°†ç”Ÿæˆè§£è€¦ä¸ºå…¨å±€å’Œå±€éƒ¨é˜¶æ®µï¼Œä½¿ç”¨DiTæž„å»ºç»“æž„ï¼Œè½»é‡å¤´æ¢å¤ç»†èŠ‚
3. å®žéªŒæ˜¾ç¤ºDiPæŽ¨ç†é€Ÿåº¦æå‡10å€ï¼ŒFIDè¾¾1.90ï¼Œå‚æ•°ä»…å¢ž0.3%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.

