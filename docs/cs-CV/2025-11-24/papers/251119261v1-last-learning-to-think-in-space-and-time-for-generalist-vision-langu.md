---
layout: default
title: LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models
---

# LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models

**arXiv**: [2511.19261v1](https://arxiv.org/abs/2511.19261) | [PDF](https://arxiv.org/pdf/2511.19261.pdf)

**ä½œè€…**: Shuai Wang, Daoan Zhang, Tianyi Bai, Shitong Shao, Jiebo Luo, Jiaheng Wei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLASTæ–¹æ³•ï¼Œé€šè¿‡ç©ºé—´ä¸Žæ—¶é—´æ€ç»´è½¨è¿¹æå‡é€šç”¨è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨3Dç©ºé—´å’Œé•¿è§†é¢‘ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `3Dç©ºé—´ç†è§£` `é•¿è§†é¢‘ç†è§£` `æ€ç»´è½¨è¿¹` `é›¶æ ·æœ¬å­¦ä¹ ` `å¾®è°ƒä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰è§†è§‰è¯­è¨€æ¨¡åž‹éš¾ä»¥ç†è§£3Dç©ºé—´å’Œé•¿è§†é¢‘ï¼Œä¾èµ–ä¸“ç”¨æž¶æž„
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨2Då›¾åƒè¾“å…¥ï¼Œæž„å»ºè§†è§‰æ€ç»´è½¨è¿¹ï¼Œè”åˆä¼˜åŒ–ç©ºé—´ä¸Žæ—¶é—´ç†è§£
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸‹ï¼Œå¤šä»»åŠ¡åŸºå‡†æ˜¾è‘—æå‡ï¼Œå¦‚EgoSchemaå¢žç›Š15.8%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.

