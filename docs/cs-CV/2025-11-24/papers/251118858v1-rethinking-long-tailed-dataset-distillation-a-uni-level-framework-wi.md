---
layout: default
title: Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling
---

# Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling

**arXiv**: [2511.18858v1](https://arxiv.org/abs/2511.18858) | [PDF](https://arxiv.org/pdf/2511.18858.pdf)

**ä½œè€…**: Xiao Cui, Yulei Qin, Xinyue Li, Wengang Zhou, Hongsheng Li, Houqiang Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æž¶ï¼Œé€šè¿‡æ— åæ¢å¤å’Œè½¯é‡æ ‡è§£å†³é•¿å°¾æ•°æ®é›†è’¸é¦é—®é¢˜**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `é•¿å°¾åˆ†å¸ƒ` `æ— åæ¢å¤` `è½¯é‡æ ‡` `ç»Ÿè®¡å¯¹é½` `æ¨¡åž‹åå·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿å°¾åˆ†å¸ƒå¯¼è‡´æ¨¡åž‹è¡¨ç¤ºåå·®å’Œç»Ÿè®¡ä¼°è®¡é”™è¯¯ï¼Œå½±å“è’¸é¦æ•ˆæžœ
2. æ–¹æ³•åŒ…æ‹¬å¢žå¼ºä¸“å®¶æ¨¡åž‹ã€é‡æ ¡å‡†BNç»Ÿè®¡å’Œå¤šè½®åˆå§‹åŒ–åˆæˆå›¾åƒ
3. åœ¨å¤šä¸ªé•¿å°¾åŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡†ç¡®çŽ‡æ˜¾è‘—æå‡ï¼Œå¦‚CIFAR-100-LTæé«˜15.6%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

