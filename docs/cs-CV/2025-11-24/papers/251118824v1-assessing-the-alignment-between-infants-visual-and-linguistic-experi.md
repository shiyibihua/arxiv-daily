---
layout: default
title: Assessing the alignment between infants' visual and linguistic experience using multimodal language models
---

# Assessing the alignment between infants' visual and linguistic experience using multimodal language models

**arXiv**: [2511.18824v1](https://arxiv.org/abs/2511.18824) | [PDF](https://arxiv.org/pdf/2511.18824.pdf)

**ä½œè€…**: Alvin Wei Ming Tan, Jane Yang, Tarun Sepuri, Khai Loong Aw, Robert Z. Sparks, Zi Yin, Virginia A. Marchman, Michael C. Frank, Bria Long

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½¿ç”¨CLIPæ¨¡åž‹è‡ªåŠ¨è¯„ä¼°å©´å„¿è§†è§’è§†é¢‘ä¸­è§†è§‰ä¸Žè¯­è¨€å¯¹é½ï¼Œæ­ç¤ºå­¦ä¹ å¯¹é½æ—¶åˆ»ç¨€å°‘ã€‚**

**å…³é”®è¯**: `å©´å„¿è¯­è¨€å­¦ä¹ ` `è§†è§‰è¯­è¨€å¯¹é½` `CLIPæ¨¡åž‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ç¬¬ä¸€äººç§°è§†é¢‘åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå©´å„¿æ—¥å¸¸å­¦ä¹ ä¸­è§†è§‰ä¸Žè¯­è¨€ç»éªŒçš„æ—¶é—´å¯¹é½ç¨‹åº¦æœªçŸ¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨CLIPæ¨¡åž‹è‡ªåŠ¨åˆ†æžå©´å„¿è§†è§’è§†é¢‘ä¸­çš„è§†è§‰-è¯­è¨€å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šéªŒè¯CLIPåˆ†æ•°ä¸Žäººå·¥åˆ¤æ–­ä¸€è‡´ï¼Œå‘çŽ°å¯¹é½æ—¶åˆ»åœ¨çœŸå®žæ•°æ®ä¸­ç½•è§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.

