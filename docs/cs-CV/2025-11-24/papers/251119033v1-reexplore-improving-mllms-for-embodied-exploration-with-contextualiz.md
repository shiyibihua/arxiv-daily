---
layout: default
title: ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay
---

# ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.19033" target="_blank" class="toolbar-btn">arXiv: 2511.19033v1</a>
    <a href="https://arxiv.org/pdf/2511.19033.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19033v1" 
            onclick="toggleFavorite(this, '2511.19033v1', 'ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Gengyuan Zhang, Mingcong Ding, Jingpei Wu, Ruotong Liao, Volker Tresp

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-24

**Â§áÊ≥®**: 8 main pages plus 13 pages Appendix

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ReEXploreÔºöÂà©Áî®ÊÉÖÂ¢ÉÂåñÂõûÈ°æÁªèÈ™åÂõûÊîæÊîπËøõMLLMÂú®ÂÖ∑Ë∫´Êé¢Á¥¢‰∏≠ÁöÑÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂÖ∑Ë∫´Êé¢Á¥¢` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÁªèÈ™åÂõûÊîæ` `ÂàÜÂ±ÇÂÜ≥Á≠ñ` `Êú∫Âô®‰∫∫ÂØºËà™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éMLLMÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰Ωì‰æùËµñ‰∫éÈ¢ÑËÆ≠ÁªÉÁü•ËØÜÔºåÁº∫‰πèÂØπÊñ∞ÁéØÂ¢ÉÁöÑÈÄÇÂ∫îÊÄßÔºå‰∏îËÆ≠ÁªÉÊàêÊú¨È´òÊòÇ„ÄÇ
2. ReEXploreÈÄöËøáÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÊ≥®ÂÖ•ÊèêÁÇºÁöÑÊäΩË±°ÁªèÈ™åÔºåÂπ∂ÈááÁî®ÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©Á≠ñÁï•ÔºåÊèêÂçáÊé¢Á¥¢ÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåReEXploreÂú®Â§ö‰∏™ÂÖ∑Ë∫´Êé¢Á¥¢Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÊàêÂäüÁéáÂíåÂØºËà™ÊïàÁéáÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâMLLMÂü∫Á∫ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÖ∑Ë∫´Êé¢Á¥¢ÊòØ‰∏Ä‰∏™ÁõÆÊ†áÈ©±Âä®ÁöÑËøáÁ®ãÔºåÂÆÉË¶ÅÊ±ÇÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂÖ∑Â§áÁ≤æÁªÜÁöÑÊÑüÁü•ÂíåÁü•ËØÜÂ¢ûÂº∫ÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÁöÑÁ†îÁ©∂Â∞ùËØïÂà©Áî®MLLMËøõË°åÊé¢Á¥¢ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂÖ∑ÊúâÂº∫Â§ßÁöÑÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÊàë‰ª¨ÂèëÁé∞Âü∫‰∫éMLLMÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®Êé¢Á¥¢Êñ∞ÁéØÂ¢ÉÊó∂‰ªçÁÑ∂‰∏çÂ∞ΩÂ¶Ç‰∫∫ÊÑèÔºöÔºàiÔºâÂÆÉ‰ª¨‰æùËµñ‰∫éÊ∑±Âàª‰ΩÜÈôàÊóßÁöÑÈ¢ÑËÆ≠ÁªÉÁü•ËØÜÔºõÔºàiiÔºâÂü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÂ¶ÇÊ®°‰ªøÂ≠¶‰π†ÊàñÂº∫ÂåñÂ≠¶‰π†ÔºåÂØπ‰∫éÂÖ∑ÊúâÁ®ÄÁñèÁªìÊûúÂ•ñÂä±ÁöÑÈïøÊó∂Á®ã‰ªªÂä°Êù•ËØ¥ÊàêÊú¨È´òÊòÇÔºõÔºàiiiÔºâÂü∫‰∫éÂâçÊ≤øÁöÑÊé¢Á¥¢‰∫ßÁîü‰∫Ü‰∏Ä‰∏™Â∑®Â§ßÁöÑ„ÄÅËßÜËßâ‰∏äÁªÜÂæÆÁöÑÂä®‰ΩúÁ©∫Èó¥ÔºåËøô‰ΩøÂæóMLLMÈöæ‰ª•ÂÅöÂá∫ÂèØÈù†ÁöÑÂÜ≥Á≠ñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜReEXploreÔºåËøôÊòØ‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂú®Êé®ÁêÜÊó∂ÊâßË°åÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÔºå‰ª•Ê≥®ÂÖ•ÊèêÁÇºÁöÑ„ÄÅÊäΩË±°ÁöÑÁªèÈ™åÔºåÂπ∂ËøõË°åÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©ÔºåÂ∞ÜÂâçÊ≤øÊéíÂ∫èÂàÜËß£‰∏∫Áî±Á≤óÂà∞Á≤æÁöÑÂÜ≥Á≠ñ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞Á®≥ÂÅ•„ÄÅÂèØËøΩÊ∫ØÂíåÈ´òÊïàÁöÑÊé¢Á¥¢„ÄÇÂú®Â§ö‰∏™ÂÖ∑Ë∫´Êé¢Á¥¢Âü∫ÂáÜÊµãËØï‰∏≠ÔºåReEXploreÂú®ÂºÄÊ∫êÈ™®Âπ≤ÁΩëÁªú‰∏ãÔºåÂú®ÊàêÂäüÁéáÂíåÂØºËà™ÊïàÁéáÊñπÈù¢ÈÉΩÊØîÂº∫Â§ßÁöÑMLLMÂü∫Á∫øÊèêÈ´ò‰∫ÜÈ´òËææ3ÂÄçÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âü∫‰∫éMLLMÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®Êú™Áü•ÁéØÂ¢É‰∏≠Êé¢Á¥¢ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÈ¢ÑËÆ≠ÁªÉÁü•ËØÜÔºåÈöæ‰ª•ÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÔºõÊ®°‰ªøÂ≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†Á≠âËÆ≠ÁªÉÊñπÊ≥ïÊàêÊú¨È´òÊòÇÔºõÂü∫‰∫éÂâçÊ≤øÁöÑÊé¢Á¥¢‰∫ßÁîüÂ∫ûÂ§ß‰∏îÁªÜÂæÆÁöÑÂä®‰ΩúÁ©∫Èó¥ÔºåÂØºËá¥ÂÜ≥Á≠ñÂõ∞Èöæ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöReEXploreÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÔºåÂ∞ÜÊô∫ËÉΩ‰ΩìËøáÂéªÊé¢Á¥¢ÁöÑÁªèÈ™åÊèêÁÇºÊàêÊäΩË±°Áü•ËØÜÔºåÂπ∂Âú®Êé®ÁêÜÊó∂Ê≥®ÂÖ•Ôºå‰ªéËÄåÂ¢ûÂº∫Êô∫ËÉΩ‰ΩìÂØπÁéØÂ¢ÉÁöÑÁêÜËß£ÂíåÈÄÇÂ∫îËÉΩÂäõ„ÄÇÂêåÊó∂ÔºåÈááÁî®ÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©Á≠ñÁï•ÔºåÂ∞ÜÂ§çÊùÇÁöÑÂä®‰ΩúÁ©∫Èó¥ÂàÜËß£‰∏∫Áî±Á≤óÂà∞Á≤æÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºåÈôç‰ΩéÂÜ≥Á≠ñÈöæÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöReEXploreÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê®°ÂùóÔºöÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÂíåÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©„ÄÇÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÊ®°ÂùóË¥üË¥£‰ªéÂéÜÂè≤ÁªèÈ™å‰∏≠ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÊäΩË±°ÁöÑÁü•ËØÜË°®Á§∫„ÄÇÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©Ê®°ÂùóÂàôÂ∞ÜÂâçÊ≤øÊé¢Á¥¢‰ªªÂä°ÂàÜËß£‰∏∫Â§ö‰∏™Â±ÇÁ∫ßÔºåÈ¶ñÂÖàËøõË°åÁ≤óÁï•ÁöÑÈÄâÊã©ÔºåÁÑ∂ÂêéÈÄêÊ≠•ÁªÜÂåñÔºåÊúÄÁªàÁ°ÆÂÆöÊúÄ‰Ω≥ÁöÑÊé¢Á¥¢ÊñπÂêë„ÄÇÊï¥‰∏™ËøáÁ®ãÊó†ÈúÄËÆ≠ÁªÉÔºåÂèØ‰ª•Âú®Êé®ÁêÜÈò∂ÊÆµÁõ¥Êé•Â∫îÁî®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöReEXploreÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÂíåÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©Áõ∏ÁªìÂêàÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÈ´òÊïà„ÄÅÁ®≥ÂÅ•‰∏îÂèØËøΩÊ∫ØÁöÑÂÖ∑Ë∫´Êé¢Á¥¢„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåReEXploreÊó†ÈúÄËÆ≠ÁªÉÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÔºåÂπ∂‰∏îËÉΩÂ§üÊèê‰æõÊõ¥Ê∏ÖÊô∞ÁöÑÂÜ≥Á≠ñËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂõûÈ°æÊÄßÁªèÈ™åÂõûÊîæÊ®°Âùó‰ΩøÁî®MLLMÂØπÂéÜÂè≤ÁªèÈ™åËøõË°åÁºñÁ†ÅÂíåÊèêÁÇºÔºåÊèêÂèñÂÖ≥ÈîÆÁöÑËßÜËßâÂíåËØ≠‰πâ‰ø°ÊÅØ„ÄÇÂàÜÂ±ÇÂâçÊ≤øÈÄâÊã©Ê®°ÂùóÈááÁî®Â§öÂ±ÇÊÑüÁü•Êú∫ÔºàMLPÔºâÂØπÂâçÊ≤øËøõË°åÊéíÂ∫èÔºåÂπ∂Ê†πÊçÆÊéíÂ∫èÁªìÊûúÈÄâÊã©ÊúÄ‰Ω≥ÁöÑÊé¢Á¥¢ÊñπÂêë„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Á≠âÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ReEXploreÂú®Â§ö‰∏™ÂÖ∑Ë∫´Êé¢Á¥¢Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÊàêÂäüÁéáÂíåÂØºËà™ÊïàÁéáÂùáÊèêÈ´ò‰∫ÜÈ´òËææ3ÂÄç„ÄÇËØ•ÊñπÊ≥ïÂú®ÂºÄÊ∫êÈ™®Âπ≤ÁΩëÁªú‰∏ãËøõË°å‰∫ÜÈ™åËØÅÔºåË°®ÊòéÂÖ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÈÄöÁî®ÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReEXploreËÉΩÂ§üÊúâÊïàËß£ÂÜ≥MLLMÂú®ÂÖ∑Ë∫´Êé¢Á¥¢‰∏≠Èù¢‰∏¥ÁöÑÊåëÊàòÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ReEXploreÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊêúÁ¥¢ÊïëÊè¥Á≠âÈ¢ÜÂüü„ÄÇËØ•Á†îÁ©∂ËÉΩÂ§üÊèêÂçáÊô∫ËÉΩ‰ΩìÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéáÂíåÈÄÇÂ∫îËÉΩÂäõÔºåÈôç‰ΩéÂØπËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰æùËµñÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÊú™Êù•ÂèëÂ±ïÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁÅæÈöæÊïëÊè¥Âú∫ÊôØ‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®ReEXploreÂø´ÈÄüÊé¢Á¥¢ÂèóÁÅæÂå∫ÂüüÔºåÂØªÊâæÂπ∏Â≠òËÄÖ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.

