---
layout: default
title: Q-Save: Towards Scoring and Attribution for Generated Video Evaluation
---

# Q-Save: Towards Scoring and Attribution for Generated Video Evaluation

**arXiv**: [2511.18825v1](https://arxiv.org/abs/2511.18825) | [PDF](https://arxiv.org/pdf/2511.18825.pdf)

**ä½œè€…**: Xiele Wu, Zicheng Zhang, Mingtao Chen, Yixian Liu, Yiming Liu, Shushi Wang, Zhichao Hu, Yuhong Liu, Guangtao Zhai, Xiaohong Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-SaveåŸºå‡†æ•°æ®é›†ä¸Žæ¨¡åž‹ï¼Œç”¨äºŽAIç”Ÿæˆè§†é¢‘çš„å…¨é¢å¯è§£é‡Šè¯„ä¼°ã€‚**

**å…³é”®è¯**: `AIç”Ÿæˆè§†é¢‘è¯„ä¼°` `å¤šç»´åº¦æ ‡æ³¨` `SlowFastæ¡†æž¶` `å¯è§£é‡ŠAI` `è´¨é‡è¯„åˆ†ä¸Žå½’å› `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIç”Ÿæˆè§†é¢‘ç¼ºä¹å…¨é¢ä¸”å¯è§£é‡Šçš„è´¨é‡è¯„ä¼°åŸºå‡†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤šç»´åº¦æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶åŸºäºŽSlowFastæ¡†æž¶è”åˆè¯„åˆ†ä¸Žå½’å› ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹åœ¨è§†é¢‘è´¨é‡é¢„æµ‹ä¸Šè¾¾åˆ°SOTAï¼Œå¹¶æä¾›äººç±»å¯¹é½çš„è§£é‡Šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.

