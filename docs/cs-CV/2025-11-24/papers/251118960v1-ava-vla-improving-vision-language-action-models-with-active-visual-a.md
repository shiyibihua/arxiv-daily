---
layout: default
title: AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention
---

# AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention

**arXiv**: [2511.18960v1](https://arxiv.org/abs/2511.18960) | [PDF](https://arxiv.org/pdf/2511.18960.pdf)

**ä½œè€…**: Lei Xiao, Jifeng Li, Juntao Gao, Feiyang Ye, Yan Jin, Jingjing Qian, Jing Zhang, Yong Wu, Xiaoyuan Yu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAVA-VLAæ¡†æž¶ï¼Œé€šè¿‡ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›æ”¹è¿›è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨åŠ¨æ€å†³ç­–ä¸­çš„æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›` `éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æœºå™¨äººåŸºå‡†æµ‹è¯•` `æ¨¡æ‹Ÿåˆ°çœŸå®žè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨åŠ¨æ€åºåˆ—å†³ç­–ä¸­å¿½ç•¥åŽ†å²ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è§†è§‰å¤„ç†æ•ˆçŽ‡ä½Žä¸‹ã€‚
2. å¼•å…¥ä¸»åŠ¨è§†è§‰æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨å¾ªçŽ¯çŠ¶æ€åŠ¨æ€è°ƒåˆ¶è§†è§‰ä»¤ç‰Œå¤„ç†ã€‚
3. åœ¨LIBEROå’ŒCALVINåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä¼˜ï¼Œå¹¶åœ¨çœŸå®žæœºå™¨äººå¹³å°éªŒè¯å®žç”¨æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

