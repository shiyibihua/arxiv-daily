---
layout: default
title: Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection
---

# Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection

**arXiv**: [2511.19274v1](https://arxiv.org/abs/2511.19274) | [PDF](https://arxiv.org/pdf/2511.19274.pdf)

**ä½œè€…**: Mingyang Chen, Jiawei Du, Bo Huang, Yi Wang, Xiaobo Zhang, Wei Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ‰©æ•£é‡å»ºçš„æ•°æ®ä¼¼ç„¶ä¼°è®¡æ–¹æ³•ä»¥ä¼˜åŒ–æ ¸å¿ƒé›†é€‰æ‹©**

**å…³é”®è¯**: `æ ¸å¿ƒé›†é€‰æ‹©` `æ‰©æ•£æ¨¡åž‹` `æ•°æ®ä¼¼ç„¶ä¼°è®¡` `é‡å»ºåå·®` `ä¿¡æ¯ç†è®ºä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ ¸å¿ƒé›†é€‰æ‹©æ–¹æ³•ä¾èµ–å¯å‘å¼è¯„åˆ†ï¼Œç¼ºä¹æ•°æ®ä¼¼ç„¶æ˜¾å¼å»ºæ¨¡
2. åˆ©ç”¨æ‰©æ•£æ¨¡åž‹é€šè¿‡éƒ¨åˆ†åå‘åŽ»å™ªé‡å»ºåå·®ä¼°è®¡æ•°æ®ä¼¼ç„¶
3. åœ¨ImageNetä¸Šå®žéªŒï¼Œä»…ç”¨50%æ•°æ®æŽ¥è¿‘å…¨æ•°æ®è®­ç»ƒæ•ˆæžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.

