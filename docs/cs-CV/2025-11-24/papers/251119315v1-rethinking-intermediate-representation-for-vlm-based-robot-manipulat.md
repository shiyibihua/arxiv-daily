---
layout: default
title: Rethinking Intermediate Representation for VLM-based Robot Manipulation
---

# Rethinking Intermediate Representation for VLM-based Robot Manipulation

**arXiv**: [2511.19315v1](https://arxiv.org/abs/2511.19315) | [PDF](https://arxiv.org/pdf/2511.19315.pdf)

**ä½œè€…**: Weiliang Tang, Jialin Gao, Jia-Hui Pan, Gang Wang, Li Erran Li, Yunhui Liu, Mingyu Ding, Pheng-Ann Heng, Chi-Wing Fu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEAMä¸­é—´è¡¨ç¤ºä»¥æå‡VLMæœºå™¨äººæ“ä½œçš„å¯ç†è§£æ€§ä¸Žæ³›åŒ–æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æœºå™¨äººæ“ä½œ` `ä¸­é—´è¡¨ç¤º` `å¼€æ”¾è¯æ±‡åˆ†å‰²` `æ£€ç´¢å¢žå¼ºå­¦ä¹ ` `è¯­ä¹‰ç»„è£…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLMä¸­é—´è¡¨ç¤ºåœ¨å¯ç†è§£æ€§ä¸Žæ³›åŒ–æ€§é—´å­˜åœ¨æƒè¡¡
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä¸Šä¸‹æ–‡æ— å…³è¯­æ³•è®¾è®¡SEAMè¡¨ç¤ºï¼Œåˆ†è§£ä¸ºè¯æ±‡ä¸Žè¯­æ³•
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žä¸–ç•Œå®žéªŒä¸­å®žçŽ°SOTAæ€§èƒ½ï¼Œå¹¶å®šä¹‰æ–°è¯„ä¼°æŒ‡æ ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.

