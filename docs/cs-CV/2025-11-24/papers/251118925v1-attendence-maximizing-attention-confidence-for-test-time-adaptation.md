---
layout: default
title: AttenDence: Maximizing Attention Confidence for Test Time Adaptation
---

# AttenDence: Maximizing Attention Confidence for Test Time Adaptation

**arXiv**: [2511.18925v1](https://arxiv.org/abs/2511.18925) | [PDF](https://arxiv.org/pdf/2511.18925.pdf)

**ä½œè€…**: Yash Mali

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ€å°åŒ–æ³¨æ„åŠ›åˆ†å¸ƒç†µä»¥å¢žå¼ºæµ‹è¯•æ—¶é€‚åº”ä¸­æ¨¡åž‹å¯¹ç›¸å…³å›¾åƒåŒºåŸŸçš„å…³æ³¨ç½®ä¿¡åº¦**

**å…³é”®è¯**: `æµ‹è¯•æ—¶é€‚åº”` `æ³¨æ„åŠ›æœºåˆ¶` `ç†µæœ€å°åŒ–` `åˆ†å¸ƒåç§»` `Transformeræ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæµ‹è¯•æ—¶åˆ†å¸ƒåç§»ä¸‹æ¨¡åž‹é€‚åº”èƒ½åŠ›ä¸è¶³ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–è¾“å‡ºåˆ†å¸ƒç†µæœ€å°åŒ–
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨Transformeræ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ€å°åŒ–CLSä»¤ç‰Œåˆ°å›¾åƒè¡¥ä¸çš„æ³¨æ„åŠ›åˆ†å¸ƒç†µ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§æŸåç±»åž‹ä¸‹æå‡é²æ£’æ€§ï¼Œå•æµ‹è¯•æ ·æœ¬æœ‰æ•ˆï¼Œä¸å½±å“å¹²å‡€æ•°æ®æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.

