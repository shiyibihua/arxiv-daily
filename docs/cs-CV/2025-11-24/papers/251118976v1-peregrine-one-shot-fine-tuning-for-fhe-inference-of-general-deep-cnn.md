---
layout: default
title: Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs
---

# Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs

**arXiv**: [2511.18976v1](https://arxiv.org/abs/2511.18976) | [PDF](https://arxiv.org/pdf/2511.18976.pdf)

**ä½œè€…**: Huaming Ling, Ying Wang, Si Chen, Junfeng Fan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Peregrineï¼šç”¨äºŽé€šç”¨æ·±åº¦CNNçš„FHEæŽ¨ç†çš„å•æ¬¡å¾®è°ƒæ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸Žååº” (Interaction & Reaction)**

**å…³é”®è¯**: `åŒæ€åŠ å¯†` `æ·±åº¦å­¦ä¹ ` `å·ç§¯ç¥žç»ç½‘ç»œ` `éšç§ä¿æŠ¤` `å•é˜¶æ®µå¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•éš¾ä»¥åœ¨FHEæŽ¨ç†ä¸­å…¼é¡¾éžçº¿æ€§æ¿€æ´»çš„ä½Žé˜¶å¤šé¡¹å¼è¿‘ä¼¼å’Œç²¾åº¦ä¿æŒã€‚
2. æå‡ºå•é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œç›´æŽ¥å°†é¢„è®­ç»ƒCNNè½¬åŒ–ä¸ºFHEå‹å¥½å½¢å¼ï¼Œé™ä½Žè®­ç»ƒå¼€é”€ã€‚
3. è®¾è®¡å¹¿ä¹‰äº¤é”™æ‰“åŒ…æ–¹æ¡ˆï¼Œå…¼å®¹ä»»æ„åˆ†è¾¨çŽ‡ç‰¹å¾å›¾ï¼Œå¹¶ä¿æŒåŒæ€åŠ å¯†å½¢å¼ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³å°†é€šç”¨æ·±åº¦CNNåº”ç”¨äºŽåŸºäºŽåŒæ€åŠ å¯†ï¼ˆFHEï¼‰çš„æŽ¨ç†æ—¶é¢ä¸´çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ä½¿ç”¨ä½Žé˜¶å¤šé¡¹å¼é€¼è¿‘ReLUç­‰éžçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘ç²¾åº¦æŸå¤±ï¼›äºŒæ˜¯å…‹æœå¯†æ–‡å®¹é‡é™åˆ¶ï¼Œè¯¥é™åˆ¶é˜»ç¢äº†FHEæŽ¨ç†ä¸­çš„é«˜åˆ†è¾¨çŽ‡å›¾åƒå¤„ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤æ–¹é¢çš„è´¡çŒ®ï¼šï¼ˆ1ï¼‰ä¸€ç§å•é˜¶æ®µå¾®è°ƒï¼ˆSFTï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨ä½Žé˜¶å¤šé¡¹å¼ç›´æŽ¥å°†é¢„è®­ç»ƒçš„CNNè½¬æ¢ä¸ºFHEå‹å¥½çš„å½¢å¼ï¼Œä»¥æœ€å°çš„è®­ç»ƒå¼€é”€å®žçŽ°å…·æœ‰ç«žäº‰åŠ›çš„ç²¾åº¦ï¼›ï¼ˆ2ï¼‰ä¸€ç§å¹¿ä¹‰äº¤é”™æ‰“åŒ…ï¼ˆGIPï¼‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä¸Žå‡ ä¹Žä»»æ„ç©ºé—´åˆ†è¾¨çŽ‡çš„ç‰¹å¾å›¾å…¼å®¹ï¼Œå¹¶é…æœ‰ä¸€å¥—ç²¾å¿ƒè®¾è®¡çš„åŒæ€ç®—å­ï¼Œå¯åœ¨æ•´ä¸ªè®¡ç®—è¿‡ç¨‹ä¸­ä¿æŒGIPå½¢å¼çš„åŠ å¯†ã€‚è¿™äº›è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿè·¨å„ç§CNNæž¶æž„è¿›è¡Œé«˜æ•ˆçš„ç«¯åˆ°ç«¯FHEæŽ¨ç†ã€‚åœ¨CIFAR-10ã€ImageNetå’ŒMS COCOä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œé€šè¿‡æˆ‘ä»¬çš„SFTç­–ç•¥èŽ·å¾—çš„FHEå‹å¥½åž‹CNNå®žçŽ°äº†ä¸Žä½¿ç”¨ReLUæˆ–SiLUæ¿€æ´»å‡½æ•°çš„åŸºçº¿ç›¸å½“çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡å±•ç¤ºäº†åˆ©ç”¨ä½Žé˜¶å¤šé¡¹å¼æ¿€æ´»è¿›è¡Œå¯¹è±¡æ£€æµ‹çš„YOLOæž¶æž„çš„åŸºäºŽFHEçš„æŽ¨ç†ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°†æ·±åº¦å·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰åº”ç”¨äºŽåŒæ€åŠ å¯†ï¼ˆFHEï¼‰æŽ¨ç†æ—¶é‡åˆ°çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚é¦–å…ˆï¼Œå¦‚ä½•åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œä½¿ç”¨ä½Žé˜¶å¤šé¡¹å¼æ¥è¿‘ä¼¼ReLUç­‰éžçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚å…¶æ¬¡ï¼Œå¦‚ä½•å…‹æœFHEæŽ¨ç†ä¸­å¯†æ–‡å®¹é‡çš„é™åˆ¶ï¼Œä»Žè€Œæ”¯æŒé«˜åˆ†è¾¨çŽ‡å›¾åƒçš„å¤„ç†ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åœ¨ç²¾åº¦å’Œè®¡ç®—å¤æ‚åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œæˆ–è€…æ— æ³•æœ‰æ•ˆåœ°å¤„ç†é«˜åˆ†è¾¨çŽ‡å›¾åƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å•é˜¶æ®µå¾®è°ƒï¼ˆSFTï¼‰å°†é¢„è®­ç»ƒçš„CNNç›´æŽ¥è½¬æ¢ä¸ºFHEå‹å¥½çš„å½¢å¼ï¼Œé¿å…äº†å¤æ‚çš„ä¸­é—´æ­¥éª¤ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§å¹¿ä¹‰äº¤é”™æ‰“åŒ…ï¼ˆGIPï¼‰æ–¹æ¡ˆï¼Œä»¥æé«˜å¯†æ–‡çš„åˆ©ç”¨çŽ‡ï¼Œä»Žè€Œæ”¯æŒé«˜åˆ†è¾¨çŽ‡å›¾åƒçš„å¤„ç†ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æœ€å¤§é™åº¦åœ°å‡å°‘ç²¾åº¦æŸå¤±ï¼Œå¹¶æé«˜FHEæŽ¨ç†çš„æ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå•é˜¶æ®µå¾®è°ƒï¼ˆSFTï¼‰å’Œå¹¿ä¹‰äº¤é”™æ‰“åŒ…ï¼ˆGIPï¼‰ã€‚SFTé¦–å…ˆä½¿ç”¨ä½Žé˜¶å¤šé¡¹å¼æ›¿æ¢åŽŸå§‹CNNä¸­çš„éžçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œç„¶åŽé€šè¿‡å¾®è°ƒæ¥æ¢å¤ç²¾åº¦ã€‚GIPåˆ™å°†ç‰¹å¾å›¾ä»¥äº¤é”™çš„æ–¹å¼æ‰“åŒ…åˆ°å¯†æ–‡ä¸­ï¼Œä»Žè€Œæé«˜å¯†æ–‡çš„åˆ©ç”¨çŽ‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€å¥—åŒæ€ç®—å­ï¼Œä»¥æ”¯æŒåœ¨GIPå½¢å¼çš„å¯†æ–‡ä¸Šè¿›è¡Œè®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽå•é˜¶æ®µå¾®è°ƒç­–ç•¥å’Œå¹¿ä¹‰äº¤é”™æ‰“åŒ…æ–¹æ¡ˆã€‚SFTå¯ä»¥ç›´æŽ¥å°†é¢„è®­ç»ƒçš„CNNè½¬æ¢ä¸ºFHEå‹å¥½çš„å½¢å¼ï¼Œè€Œæ— éœ€å¤æ‚çš„ä¸­é—´æ­¥éª¤ã€‚GIPåˆ™å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¯†æ–‡ç©ºé—´ï¼Œä»Žè€Œæ”¯æŒé«˜åˆ†è¾¨çŽ‡å›¾åƒçš„å¤„ç†ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›åˆ›æ–°å¯ä»¥æ˜¾è‘—æé«˜FHEæŽ¨ç†çš„æ•ˆçŽ‡å’Œç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šSFTçš„å…³é”®è®¾è®¡åœ¨äºŽé€‰æ‹©åˆé€‚çš„ä½Žé˜¶å¤šé¡¹å¼æ¥è¿‘ä¼¼éžçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå¹¶é€šè¿‡å¾®è°ƒæ¥ä¼˜åŒ–ç½‘ç»œçš„å‚æ•°ã€‚GIPçš„å…³é”®è®¾è®¡åœ¨äºŽå¦‚ä½•å°†ç‰¹å¾å›¾ä»¥äº¤é”™çš„æ–¹å¼æ‰“åŒ…åˆ°å¯†æ–‡ä¸­ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„åŒæ€ç®—å­æ¥æ”¯æŒè®¡ç®—ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è€ƒè™‘äº†å¦‚ä½•åœ¨ä¿è¯å®‰å…¨æ€§çš„å‰æä¸‹ï¼Œæœ€å¤§é™åº¦åœ°æé«˜è®¡ç®—æ•ˆçŽ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œé€šè¿‡SFTç­–ç•¥èŽ·å¾—çš„FHEå‹å¥½åž‹CNNåœ¨CIFAR-10ã€ImageNetå’ŒMS COCOæ•°æ®é›†ä¸Šå®žçŽ°äº†ä¸Žä½¿ç”¨ReLUæˆ–SiLUæ¿€æ´»å‡½æ•°çš„åŸºçº¿ç›¸å½“çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œé¦–æ¬¡å±•ç¤ºäº†åˆ©ç”¨ä½Žé˜¶å¤šé¡¹å¼æ¿€æ´»è¿›è¡Œå¯¹è±¡æ£€æµ‹çš„YOLOæž¶æž„çš„åŸºäºŽFHEçš„æŽ¨ç†ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œæœ‰æ•ˆåœ°æ”¯æŒFHEæŽ¨ç†ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽéšç§ä¿æŠ¤çš„å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—å½±åƒåˆ†æžä¸­ï¼Œå¯ä»¥åœ¨ä¸æ³„éœ²æ‚£è€…éšç§çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨FHEå¯¹åŒ»ç–—å½±åƒè¿›è¡Œåˆ†æžå’Œè¯Šæ–­ã€‚åœ¨é‡‘èžé£ŽæŽ§é¢†åŸŸï¼Œå¯ä»¥åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹ï¼Œåˆ©ç”¨FHEå¯¹ç”¨æˆ·çš„ä¿¡ç”¨é£Žé™©è¿›è¡Œè¯„ä¼°ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šéœ€è¦éšç§ä¿æŠ¤çš„åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

