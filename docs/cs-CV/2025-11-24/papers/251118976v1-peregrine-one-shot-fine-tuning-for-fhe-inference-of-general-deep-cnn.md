---
layout: default
title: Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs
---

# Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs

**arXiv**: [2511.18976v1](https://arxiv.org/abs/2511.18976) | [PDF](https://arxiv.org/pdf/2511.18976.pdf)

**ä½œè€…**: Huaming Ling, Ying Wang, Si Chen, Junfeng Fan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•é˜¶æ®µå¾®è°ƒå’Œå¹¿ä¹‰äº¤é”™æ‰“åŒ…æ–¹æ¡ˆï¼Œä»¥è§£å†³å…¨åŒæ€åŠ å¯†ä¸‹CNNæŽ¨ç†çš„æ¿€æ´»è¿‘ä¼¼å’Œå®¹é‡é™åˆ¶é—®é¢˜ã€‚**

**å…³é”®è¯**: `å…¨åŒæ€åŠ å¯†æŽ¨ç†` `CNNå¾®è°ƒ` `å¤šé¡¹å¼æ¿€æ´»è¿‘ä¼¼` `äº¤é”™æ‰“åŒ…` `å¯¹è±¡æ£€æµ‹` `ä½Žé˜¶å¤šé¡¹å¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨åŒæ€åŠ å¯†æŽ¨ç†ä¸­ï¼ŒReLUç­‰éžçº¿æ€§æ¿€æ´»çš„è¿‘ä¼¼å’Œå¯†æ–‡å®¹é‡é™åˆ¶é«˜åˆ†è¾¨çŽ‡å›¾åƒå¤„ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ä½Žé˜¶å¤šé¡¹å¼è¿‘ä¼¼æ¿€æ´»ï¼Œå•é˜¶æ®µå¾®è°ƒç›´æŽ¥è½¬æ¢é¢„è®­ç»ƒCNNï¼Œå‡å°‘è®­ç»ƒå¼€é”€ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨CIFAR-10ã€ImageNetå’ŒMS COCOä¸Šï¼ŒFHEå‹å¥½CNNè¾¾åˆ°ä¸ŽReLU/SiLUåŸºçº¿ç›¸å½“çš„ç²¾åº¦ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

