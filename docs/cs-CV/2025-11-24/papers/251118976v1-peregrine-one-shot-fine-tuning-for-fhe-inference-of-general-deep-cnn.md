---
layout: default
title: Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs
---

# Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18976" target="_blank" class="toolbar-btn">arXiv: 2511.18976v1</a>
    <a href="https://arxiv.org/pdf/2511.18976.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18976v1" 
            onclick="toggleFavorite(this, '2511.18976v1', 'Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Huaming Ling, Ying Wang, Si Chen, Junfeng Fan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**PeregrineÔºöÁî®‰∫éÈÄöÁî®Ê∑±Â∫¶CNNÁöÑFHEÊé®ÁêÜÁöÑÂçïÊ¨°ÂæÆË∞ÉÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `ÂêåÊÄÅÂä†ÂØÜ` `Ê∑±Â∫¶Â≠¶‰π†` `Âç∑ÁßØÁ•ûÁªèÁΩëÁªú` `ÈöêÁßÅ‰øùÊä§` `ÂçïÈò∂ÊÆµÂæÆË∞É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Âú®FHEÊé®ÁêÜ‰∏≠ÂÖºÈ°æÈùûÁ∫øÊÄßÊøÄÊ¥ªÁöÑ‰ΩéÈò∂Â§öÈ°πÂºèËøë‰ººÂíåÁ≤æÂ∫¶‰øùÊåÅ„ÄÇ
2. ÊèêÂá∫ÂçïÈò∂ÊÆµÂæÆË∞ÉÁ≠ñÁï•ÔºåÁõ¥Êé•Â∞ÜÈ¢ÑËÆ≠ÁªÉCNNËΩ¨Âåñ‰∏∫FHEÂèãÂ•ΩÂΩ¢ÂºèÔºåÈôç‰ΩéËÆ≠ÁªÉÂºÄÈîÄ„ÄÇ
3. ËÆæËÆ°Âπø‰πâ‰∫§ÈîôÊâìÂåÖÊñπÊ°àÔºåÂÖºÂÆπ‰ªªÊÑèÂàÜËæ®ÁéáÁâπÂæÅÂõæÔºåÂπ∂‰øùÊåÅÂêåÊÄÅÂä†ÂØÜÂΩ¢Âºè„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞ÜÈÄöÁî®Ê∑±Â∫¶CNNÂ∫îÁî®‰∫éÂü∫‰∫éÂêåÊÄÅÂä†ÂØÜÔºàFHEÔºâÁöÑÊé®ÁêÜÊó∂Èù¢‰∏¥ÁöÑ‰∏§Â§ßÊåëÊàòÔºö‰∏ÄÊòØ‰ΩøÁî®‰ΩéÈò∂Â§öÈ°πÂºèÈÄºËøëReLUÁ≠âÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåÂêåÊó∂ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëÁ≤æÂ∫¶ÊçüÂ§±Ôºõ‰∫åÊòØÂÖãÊúçÂØÜÊñáÂÆπÈáèÈôêÂà∂ÔºåËØ•ÈôêÂà∂ÈòªÁ¢ç‰∫ÜFHEÊé®ÁêÜ‰∏≠ÁöÑÈ´òÂàÜËæ®ÁéáÂõæÂÉèÂ§ÑÁêÜ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÊñπÈù¢ÁöÑË¥°ÁåÆÔºöÔºà1Ôºâ‰∏ÄÁßçÂçïÈò∂ÊÆµÂæÆË∞ÉÔºàSFTÔºâÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•‰ΩøÁî®‰ΩéÈò∂Â§öÈ°πÂºèÁõ¥Êé•Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑCNNËΩ¨Êç¢‰∏∫FHEÂèãÂ•ΩÁöÑÂΩ¢ÂºèÔºå‰ª•ÊúÄÂ∞èÁöÑËÆ≠ÁªÉÂºÄÈîÄÂÆûÁé∞ÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÁ≤æÂ∫¶ÔºõÔºà2Ôºâ‰∏ÄÁßçÂπø‰πâ‰∫§ÈîôÊâìÂåÖÔºàGIPÔºâÊñπÊ°àÔºåËØ•ÊñπÊ°à‰∏éÂá†‰πé‰ªªÊÑèÁ©∫Èó¥ÂàÜËæ®ÁéáÁöÑÁâπÂæÅÂõæÂÖºÂÆπÔºåÂπ∂ÈÖçÊúâ‰∏ÄÂ•óÁ≤æÂøÉËÆæËÆ°ÁöÑÂêåÊÄÅÁÆóÂ≠êÔºåÂèØÂú®Êï¥‰∏™ËÆ°ÁÆóËøáÁ®ã‰∏≠‰øùÊåÅGIPÂΩ¢ÂºèÁöÑÂä†ÂØÜ„ÄÇËøô‰∫õËøõÊ≠•‰ΩøÂæóËÉΩÂ§üË∑®ÂêÑÁßçCNNÊû∂ÊûÑËøõË°åÈ´òÊïàÁöÑÁ´ØÂà∞Á´ØFHEÊé®ÁêÜ„ÄÇÂú®CIFAR-10„ÄÅImageNetÂíåMS COCO‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÈÄöËøáÊàë‰ª¨ÁöÑSFTÁ≠ñÁï•Ëé∑ÂæóÁöÑFHEÂèãÂ•ΩÂûãCNNÂÆûÁé∞‰∫Ü‰∏é‰ΩøÁî®ReLUÊàñSiLUÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂü∫Á∫øÁõ∏ÂΩìÁöÑÁ≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåËøôÈ°πÂ∑•‰ΩúÈ¶ñÊ¨°Â±ïÁ§∫‰∫ÜÂà©Áî®‰ΩéÈò∂Â§öÈ°πÂºèÊøÄÊ¥ªËøõË°åÂØπË±°Ê£ÄÊµãÁöÑYOLOÊû∂ÊûÑÁöÑÂü∫‰∫éFHEÁöÑÊé®ÁêÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞ÜÊ∑±Â∫¶Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÂ∫îÁî®‰∫éÂêåÊÄÅÂä†ÂØÜÔºàFHEÔºâÊé®ÁêÜÊó∂ÈÅáÂà∞ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇÈ¶ñÂÖàÔºåÂ¶Ç‰ΩïÂú®‰øùËØÅÁ≤æÂ∫¶ÁöÑÂâçÊèê‰∏ãÔºå‰ΩøÁî®‰ΩéÈò∂Â§öÈ°πÂºèÊù•Ëøë‰ººReLUÁ≠âÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞„ÄÇÂÖ∂Ê¨°ÔºåÂ¶Ç‰ΩïÂÖãÊúçFHEÊé®ÁêÜ‰∏≠ÂØÜÊñáÂÆπÈáèÁöÑÈôêÂà∂Ôºå‰ªéËÄåÊîØÊåÅÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁöÑÂ§ÑÁêÜ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂú®Á≤æÂ∫¶ÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰πãÈó¥ËøõË°åÊùÉË°°ÔºåÊàñËÄÖÊó†Ê≥ïÊúâÊïàÂú∞Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂçïÈò∂ÊÆµÂæÆË∞ÉÔºàSFTÔºâÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑCNNÁõ¥Êé•ËΩ¨Êç¢‰∏∫FHEÂèãÂ•ΩÁöÑÂΩ¢ÂºèÔºåÈÅøÂÖç‰∫ÜÂ§çÊùÇÁöÑ‰∏≠Èó¥Ê≠•È™§„ÄÇÂêåÊó∂ÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÂπø‰πâ‰∫§ÈîôÊâìÂåÖÔºàGIPÔºâÊñπÊ°àÔºå‰ª•ÊèêÈ´òÂØÜÊñáÁöÑÂà©Áî®ÁéáÔºå‰ªéËÄåÊîØÊåÅÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁöÑÂ§ÑÁêÜ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëÁ≤æÂ∫¶ÊçüÂ§±ÔºåÂπ∂ÊèêÈ´òFHEÊé®ÁêÜÁöÑÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÂçïÈò∂ÊÆµÂæÆË∞ÉÔºàSFTÔºâÂíåÂπø‰πâ‰∫§ÈîôÊâìÂåÖÔºàGIPÔºâ„ÄÇSFTÈ¶ñÂÖà‰ΩøÁî®‰ΩéÈò∂Â§öÈ°πÂºèÊõøÊç¢ÂéüÂßãCNN‰∏≠ÁöÑÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåÁÑ∂ÂêéÈÄöËøáÂæÆË∞ÉÊù•ÊÅ¢Â§çÁ≤æÂ∫¶„ÄÇGIPÂàôÂ∞ÜÁâπÂæÅÂõæ‰ª•‰∫§ÈîôÁöÑÊñπÂºèÊâìÂåÖÂà∞ÂØÜÊñá‰∏≠Ôºå‰ªéËÄåÊèêÈ´òÂØÜÊñáÁöÑÂà©Áî®Áéá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòËÆæËÆ°‰∫Ü‰∏ÄÂ•óÂêåÊÄÅÁÆóÂ≠êÔºå‰ª•ÊîØÊåÅÂú®GIPÂΩ¢ÂºèÁöÑÂØÜÊñá‰∏äËøõË°åËÆ°ÁÆó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂçïÈò∂ÊÆµÂæÆË∞ÉÁ≠ñÁï•ÂíåÂπø‰πâ‰∫§ÈîôÊâìÂåÖÊñπÊ°à„ÄÇSFTÂèØ‰ª•Áõ¥Êé•Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑCNNËΩ¨Êç¢‰∏∫FHEÂèãÂ•ΩÁöÑÂΩ¢ÂºèÔºåËÄåÊó†ÈúÄÂ§çÊùÇÁöÑ‰∏≠Èó¥Ê≠•È™§„ÄÇGIPÂàôÂèØ‰ª•ÊúâÊïàÂú∞Âà©Áî®ÂØÜÊñáÁ©∫Èó¥Ôºå‰ªéËÄåÊîØÊåÅÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁöÑÂ§ÑÁêÜ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËøô‰∫õÂàõÊñ∞ÂèØ‰ª•ÊòæËëóÊèêÈ´òFHEÊé®ÁêÜÁöÑÊïàÁéáÂíåÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSFTÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÈÄâÊã©ÂêàÈÄÇÁöÑ‰ΩéÈò∂Â§öÈ°πÂºèÊù•Ëøë‰ººÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåÂπ∂ÈÄöËøáÂæÆË∞ÉÊù•‰ºòÂåñÁΩëÁªúÁöÑÂèÇÊï∞„ÄÇGIPÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ¶Ç‰ΩïÂ∞ÜÁâπÂæÅÂõæ‰ª•‰∫§ÈîôÁöÑÊñπÂºèÊâìÂåÖÂà∞ÂØÜÊñá‰∏≠ÔºåÂπ∂ËÆæËÆ°Áõ∏Â∫îÁöÑÂêåÊÄÅÁÆóÂ≠êÊù•ÊîØÊåÅËÆ°ÁÆó„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòËÄÉËôë‰∫ÜÂ¶Ç‰ΩïÂú®‰øùËØÅÂÆâÂÖ®ÊÄßÁöÑÂâçÊèê‰∏ãÔºåÊúÄÂ§ßÈôêÂ∫¶Âú∞ÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáSFTÁ≠ñÁï•Ëé∑ÂæóÁöÑFHEÂèãÂ•ΩÂûãCNNÂú®CIFAR-10„ÄÅImageNetÂíåMS COCOÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü‰∏é‰ΩøÁî®ReLUÊàñSiLUÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂü∫Á∫øÁõ∏ÂΩìÁöÑÁ≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåËØ•Â∑•‰ΩúÈ¶ñÊ¨°Â±ïÁ§∫‰∫ÜÂà©Áî®‰ΩéÈò∂Â§öÈ°πÂºèÊøÄÊ¥ªËøõË°åÂØπË±°Ê£ÄÊµãÁöÑYOLOÊû∂ÊûÑÁöÑÂü∫‰∫éFHEÁöÑÊé®ÁêÜ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Âú®‰øùËØÅÁ≤æÂ∫¶ÁöÑÂâçÊèê‰∏ãÔºåÊúâÊïàÂú∞ÊîØÊåÅFHEÊé®ÁêÜ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÈöêÁßÅ‰øùÊä§ÁöÑÂõæÂÉèËØÜÂà´„ÄÅÁõÆÊ†áÊ£ÄÊµãÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂåªÁñóÂΩ±ÂÉèÂàÜÊûê‰∏≠ÔºåÂèØ‰ª•Âú®‰∏çÊ≥ÑÈú≤ÊÇ£ËÄÖÈöêÁßÅÁöÑÊÉÖÂÜµ‰∏ãÔºåÂà©Áî®FHEÂØπÂåªÁñóÂΩ±ÂÉèËøõË°åÂàÜÊûêÂíåËØäÊñ≠„ÄÇÂú®ÈáëËûçÈ£éÊéßÈ¢ÜÂüüÔºåÂèØ‰ª•Âú®‰øùÊä§Áî®Êà∑ÈöêÁßÅÁöÑÂâçÊèê‰∏ãÔºåÂà©Áî®FHEÂØπÁî®Êà∑ÁöÑ‰ø°Áî®È£éÈô©ËøõË°åËØÑ‰º∞„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®Êõ¥Â§öÈúÄË¶ÅÈöêÁßÅ‰øùÊä§ÁöÑÂú∫ÊôØ‰∏≠ÂæóÂà∞Â∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

