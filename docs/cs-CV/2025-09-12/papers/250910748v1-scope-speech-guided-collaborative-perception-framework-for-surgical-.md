---
layout: default
title: SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation
---

# SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10748" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10748v1</a>
  <a href="https://arxiv.org/pdf/2509.10748.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10748v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10748v1', 'SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jecia Z. Y. Mao, Francis X Creighton, Russell H Taylor, Manish Sahu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SCOPEæ¡†æ¶ï¼šè¯­éŸ³å¼•å¯¼çš„ååŒæ„ŸçŸ¥ï¼Œç”¨äºæ‰‹æœ¯åœºæ™¯åˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹æœ¯åœºæ™¯åˆ†å‰²` `è¯­éŸ³å¼•å¯¼` `ååŒæ„ŸçŸ¥` `è§†è§‰åŸºç¡€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººæœºåä½œ` `æœ¯ä¸­è¾…åŠ©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹æœ¯åœºæ™¯åˆ†å‰²æ–¹æ³•ä¾èµ–é¢†åŸŸç‰¹å®šæ•°æ®å’Œæ ‡æ³¨ï¼Œéš¾ä»¥é€‚åº”æ–°åœºæ™¯å’Œç±»åˆ«ã€‚
2. SCOPEæ¡†æ¶ç»“åˆLLMæ¨ç†å’ŒVFMæ„ŸçŸ¥ï¼Œé€šè¿‡è¯­éŸ³å¼•å¯¼å®ç°æ‰‹æœ¯å™¨æ¢°çš„å³æ—¶åˆ†å‰²å’Œè·Ÿè¸ªã€‚
3. åœ¨Cataract1kå’Œé¢…åº•æ•°æ®é›†ä¸Šçš„å®éªŒä»¥åŠæ¨¡æ‹Ÿå®éªŒéªŒè¯äº†SCOPEæ¡†æ¶çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç²¾ç¡®åˆ†å‰²å’Œè·Ÿè¸ªæ‰‹æœ¯åœºæ™¯ä¸­çš„ç›¸å…³å…ƒç´ å¯¹äºå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æœ¯ä¸­è¾…åŠ©å’Œå†³ç­–è‡³å…³é‡è¦ã€‚ç›®å‰çš„è§£å†³æ–¹æ¡ˆä¾èµ–äºç‰¹å®šé¢†åŸŸçš„ç›‘ç£æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºæ ‡è®°æ•°æ®ï¼Œå¹¶ä¸”éœ€è¦ç‰¹å®šé¢†åŸŸçš„æ•°æ®æ¥é€‚åº”æ–°çš„æ‰‹æœ¯åœºæ™¯å’Œè¶…å‡ºé¢„å®šä¹‰æ ‡ç­¾ç±»åˆ«ã€‚æç¤ºé©±åŠ¨çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰çš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†è·¨å¼‚æ„åŒ»å­¦å›¾åƒçš„å¼€æ”¾é›†ã€é›¶æ ·æœ¬åˆ†å‰²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¯¹äººå·¥è§†è§‰æˆ–æ–‡æœ¬æç¤ºçš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨æœ¯ä¸­æ‰‹æœ¯ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­éŸ³å¼•å¯¼çš„ååŒæ„ŸçŸ¥ï¼ˆSCOPEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸å¼€æ”¾é›†VFMçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥æ”¯æŒæœ¯ä¸­è§†é¢‘æµä¸­æ‰‹æœ¯å™¨æ¢°å’Œè§£å‰–ç»“æ„çš„å³æ—¶åˆ†å‰²ã€æ ‡è®°å’Œè·Ÿè¸ªã€‚è¯¥æ¡†æ¶çš„ä¸€ä¸ªå…³é”®ç»„ä»¶æ˜¯ååŒæ„ŸçŸ¥ä»£ç†ï¼Œå®ƒç”ŸæˆVFMç”Ÿæˆçš„åˆ†å‰²çš„é¡¶çº§å€™é€‰å¯¹è±¡ï¼Œå¹¶ç»“åˆæ¥è‡ªä¸´åºŠåŒ»ç”Ÿçš„ç›´è§‚è¯­éŸ³åé¦ˆï¼Œä»¥å¼•å¯¼æ‰‹æœ¯å™¨æ¢°çš„åˆ†å‰²ï¼Œä»è€Œå½¢æˆè‡ªç„¶çš„äººæœºåä½œæ¨¡å¼ã€‚ä¹‹åï¼Œå™¨æ¢°æœ¬èº«å……å½“äº¤äº’å¼æŒ‡é’ˆï¼Œä»¥æ ‡è®°æ‰‹æœ¯åœºæ™¯çš„å…¶ä»–å…ƒç´ ã€‚æˆ‘ä»¬åœ¨å…¬å¼€çš„Cataract1kæ•°æ®é›†çš„ä¸€ä¸ªå­é›†å’Œä¸€ä¸ªå†…éƒ¨çš„ç¦»ä½“é¢…åº•æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ¡†æ¶ï¼Œä»¥è¯æ˜å…¶ç”Ÿæˆæ‰‹æœ¯åœºæ™¯çš„å³æ—¶åˆ†å‰²å’Œè·Ÿè¸ªçš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç°åœºæ¨¡æ‹Ÿç¦»ä½“å®éªŒå±•ç¤ºäº†å…¶åŠ¨æ€èƒ½åŠ›ã€‚è¿™ç§äººæœºåä½œæ¨¡å¼å±•ç¤ºäº†å¼€å‘é€‚åº”æ€§å¼ºã€å…æã€ä»¥å¤–ç§‘åŒ»ç”Ÿä¸ºä¸­å¿ƒçš„åŠ¨æ€æ‰‹æœ¯å®¤ç¯å¢ƒå·¥å…·çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ‰‹æœ¯åœºæ™¯åˆ†å‰²æ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°çš„æ‰‹æœ¯åœºæ™¯å’ŒæœªçŸ¥çš„ç‰©ä½“ç±»åˆ«ã€‚æ­¤å¤–ï¼Œä¾èµ–äººå·¥è§†è§‰æˆ–æ–‡æœ¬æç¤ºçš„æ–¹å¼åœ¨æœ¯ä¸­æ‰‹æœ¯ç¯å¢ƒä¸­ä¸å®ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œé€šè¿‡è¯­éŸ³å¼•å¯¼å®ç°æ‰‹æœ¯åœºæ™¯çš„å³æ—¶åˆ†å‰²å’Œè·Ÿè¸ªã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†åŒ»ç”Ÿçš„è¯­éŸ³æŒ‡ä»¤ä½œä¸ºæŒ‡å¯¼ä¿¡å·ï¼Œé©±åŠ¨VFMè¿›è¡Œåˆ†å‰²ï¼Œå¹¶åˆ©ç”¨åˆ†å‰²ç»“æœåè¿‡æ¥è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ›´ç²¾ç¡®çš„æ ‡æ³¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCOPEæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è¯­éŸ³è¯†åˆ«æ¨¡å—ï¼šå°†åŒ»ç”Ÿçš„è¯­éŸ³æŒ‡ä»¤è½¬æ¢ä¸ºæ–‡æœ¬ï¼›2) LLMæ¨ç†æ¨¡å—ï¼šæ ¹æ®è¯­éŸ³æŒ‡ä»¤ç”ŸæˆVFMçš„åˆ†å‰²æç¤ºï¼›3) VFMåˆ†å‰²æ¨¡å—ï¼šæ ¹æ®LLMç”Ÿæˆçš„æç¤ºï¼Œå¯¹è§†é¢‘å¸§è¿›è¡Œåˆ†å‰²ï¼›4) ååŒæ„ŸçŸ¥ä»£ç†ï¼šé€‰æ‹©æœ€ä½³åˆ†å‰²å€™é€‰ï¼Œå¹¶å…è®¸åŒ»ç”Ÿé€šè¿‡è¯­éŸ³è¿›è¡Œä¿®æ­£ï¼›5) è·Ÿè¸ªæ¨¡å—ï¼šå¯¹åˆ†å‰²åçš„ç‰©ä½“è¿›è¡Œè·Ÿè¸ªã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†è¯­éŸ³ä½œä¸ºVFMçš„å¼•å¯¼ä¿¡å·ï¼Œå®ç°äº†äººæœºååŒçš„åˆ†å‰²å’Œæ ‡æ³¨ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äººå·¥è§†è§‰æˆ–æ–‡æœ¬æç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒSCOPEæ¡†æ¶æ›´åŠ è‡ªç„¶å’Œé«˜æ•ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰‹æœ¯å™¨æ¢°ä½œä¸ºäº¤äº’å¼æŒ‡é’ˆï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†æ ‡æ³¨èŒƒå›´ã€‚

**å…³é”®è®¾è®¡**ï¼šååŒæ„ŸçŸ¥ä»£ç†æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œå®ƒè´Ÿè´£ä»VFMç”Ÿæˆçš„å¤šä¸ªåˆ†å‰²å€™é€‰ä¸­é€‰æ‹©æœ€ä½³ç»“æœï¼Œå¹¶å…è®¸åŒ»ç”Ÿé€šè¿‡è¯­éŸ³è¿›è¡Œä¿®æ­£ã€‚å…·ä½“çš„é€‰æ‹©ç­–ç•¥å’Œä¿®æ­£æœºåˆ¶æœªçŸ¥ï¼Œä½†æ¨æµ‹å¯èƒ½æ¶‰åŠç½®ä¿¡åº¦è¯„åˆ†ã€è§†è§‰ä¸€è‡´æ€§ä»¥åŠè¯­éŸ³æŒ‡ä»¤çš„è¯­ä¹‰åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨Cataract1kæ•°æ®é›†å’Œå†…éƒ¨é¢…åº•æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¿›è¡Œäº†æ¨¡æ‹Ÿçš„ç¦»ä½“å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOPEæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ‰‹æœ¯åœºæ™¯çš„å³æ—¶åˆ†å‰²å’Œè·Ÿè¸ªã€‚å…·ä½“çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ï¼Œä½†æ¨¡æ‹Ÿå®éªŒå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨åŠ¨æ€æ‰‹æœ¯ç¯å¢ƒä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SCOPEæ¡†æ¶å¯åº”ç”¨äºæœ¯ä¸­å¯¼èˆªã€æ‰‹æœ¯æœºå™¨äººè¾…åŠ©ã€è¿œç¨‹æ‰‹æœ¯ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›å®æ—¶ã€ç²¾ç¡®çš„æ‰‹æœ¯åœºæ™¯åˆ†å‰²å’Œè·Ÿè¸ªï¼Œå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å¥½åœ°ç†è§£æ‰‹æœ¯è¿‡ç¨‹ï¼Œæé«˜æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æˆä¸ºæ™ºèƒ½æ‰‹æœ¯å®¤çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ç°æ›´åŠ æ™ºèƒ½åŒ–å’Œä¸ªæ€§åŒ–çš„æ‰‹æœ¯è¾…åŠ©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate segmentation and tracking of relevant elements of the surgical scene is crucial to enable context-aware intraoperative assistance and decision making. Current solutions remain tethered to domain-specific, supervised models that rely on labeled data and required domain-specific data to adapt to new surgical scenarios and beyond predefined label categories. Recent advances in prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot segmentation across heterogeneous medical images. However, dependence of these models on manual visual or textual cues restricts their deployment in introperative surgical settings. We introduce a speech-guided collaborative perception (SCOPE) framework that integrates reasoning capabilities of large language model (LLM) with perception capabilities of open-set VFMs to support on-the-fly segmentation, labeling and tracking of surgical instruments and anatomy in intraoperative video streams. A key component of this framework is a collaborative perception agent, which generates top candidates of VFM-generated segmentation and incorporates intuitive speech feedback from clinicians to guide the segmentation of surgical instruments in a natural human-machine collaboration paradigm. Afterwards, instruments themselves serve as interactive pointers to label additional elements of the surgical scene. We evaluated our proposed framework on a subset of publicly available Cataract1k dataset and an in-house ex-vivo skull-base dataset to demonstrate its potential to generate on-the-fly segmentation and tracking of surgical scene. Furthermore, we demonstrate its dynamic capabilities through a live mock ex-vivo experiment. This human-AI collaboration paradigm showcase the potential of developing adaptable, hands-free, surgeon-centric tools for dynamic operating-room environments.

