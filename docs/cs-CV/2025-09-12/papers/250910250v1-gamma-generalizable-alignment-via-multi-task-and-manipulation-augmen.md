---
layout: default
title: GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection
---

# GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10250" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10250v1</a>
  <a href="https://arxiv.org/pdf/2509.10250.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10250v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10250v1', 'GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haozhen Yan, Yan Hong, Suning Lang, Jiahui Zhan, Yikun Ji, Yujie Gao, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: 11 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GAMMAï¼šé€šè¿‡å¤šä»»åŠ¡å’Œæ“çºµå¢å¼ºè®­ç»ƒå®ç°AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ³›åŒ–å¯¹é½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `AIç”Ÿæˆå›¾åƒæ£€æµ‹` `æ³›åŒ–èƒ½åŠ›` `å¤šä»»åŠ¡å­¦ä¹ ` `æ“çºµå¢å¼º` `åå‘äº¤å‰æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰AIç”Ÿæˆå›¾åƒæ£€æµ‹å™¨ä¾èµ–äºç”Ÿæˆæ¨¡å‹çš„ç‰¹å®šä¼ªå½±ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹æ–°å‹ç”Ÿæˆæ¨¡å‹ã€‚
2. GAMMAæ¡†æ¶é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œæ“çºµå¢å¼ºè®­ç»ƒï¼Œå‡å°‘é¢†åŸŸåå·®ï¼Œå¢å¼ºè¯­ä¹‰å¯¹é½ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGAMMAåœ¨GenImageåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAçš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶åœ¨GPT-4oç­‰æ–°æ¨¡å‹ä¸Šä¿æŒäº†é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç”Ÿæˆæ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤æ‚å’Œå¤šæ ·åŒ–ï¼Œæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒå˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹å™¨åœ¨åŒåˆ†å¸ƒç”Ÿæˆå›¾åƒä¸Šå–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¯¹æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è¿™ç§å±€é™æ€§ä¸»è¦å½’å› äºå®ƒä»¬å¯¹ç‰¹å®šäºç”Ÿæˆçš„ä¼ªå½±çš„ä¾èµ–ï¼Œä¾‹å¦‚é£æ ¼å…ˆéªŒå’Œå‹ç¼©æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶GAMMAï¼Œæ—¨åœ¨å‡å°‘é¢†åŸŸåå·®å¹¶å¢å¼ºè¯­ä¹‰å¯¹é½ã€‚GAMMAå¼•å…¥äº†å¤šç§æ“çºµç­–ç•¥ï¼Œä¾‹å¦‚åŸºäºä¿®å¤çš„æ“çºµå’Œè¯­ä¹‰ä¿æŒæ‰°åŠ¨ï¼Œä»¥ç¡®ä¿æ“çºµå†…å®¹å’ŒçœŸå®å†…å®¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨å…·æœ‰åŒåˆ†å‰²å¤´å’Œåˆ†ç±»å¤´çš„å¤šä»»åŠ¡ç›‘ç£ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ä¸åŒçš„ç”ŸæˆåŸŸä¸­è¿›è¡Œåƒç´ çº§æºå±æ€§å½’å› ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§åå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å…è®¸åˆ†å‰²å¤´æŒ‡å¯¼å’Œçº æ­£åˆ†ç±»åˆ†æ”¯ä¸­çš„æœ‰åè¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨GenImageåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ³›åŒ–æ€§èƒ½ï¼Œæé«˜äº†5.8%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨æ–°å‘å¸ƒçš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ä¸Šä¿æŒäº†å¼ºå¤§çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•è¿‡åº¦ä¾èµ–ç‰¹å®šç”Ÿæˆæ¨¡å‹çš„é£æ ¼å…ˆéªŒå’Œå‹ç¼©æ¨¡å¼ç­‰ä¼ªå½±ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æœªçŸ¥çš„ç”Ÿæˆæ¨¡å‹æ—¶æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†çœŸå®å›¾åƒå’Œæ–°å‹AIç”Ÿæˆå›¾åƒï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGAMMAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ“çºµå¢å¼ºè®­ç»ƒå’Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´é€šç”¨çš„ã€ä¸ç”Ÿæˆæ¨¡å‹æ— å…³çš„å›¾åƒç‰¹å¾ã€‚é€šè¿‡å¼•å…¥å¤šç§æ“çºµç­–ç•¥ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨å›¾åƒçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œè€Œéç‰¹å®šç”Ÿæˆå™¨çš„ä¼ªå½±ã€‚å¤šä»»åŠ¡å­¦ä¹ åˆ™é€šè¿‡åƒç´ çº§çš„æºå±æ€§å½’å› ï¼Œå¢å¼ºæ¨¡å‹å¯¹å›¾åƒæ¥æºçš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGAMMAçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šæ“çºµå¢å¼ºæ¨¡å—ã€å¤šä»»åŠ¡å­¦ä¹ æ¨¡å—å’Œåå‘äº¤å‰æ³¨æ„åŠ›æ¨¡å—ã€‚æ“çºµå¢å¼ºæ¨¡å—è´Ÿè´£ç”Ÿæˆå„ç§è¢«æ“çºµçš„å›¾åƒï¼ŒåŒ…æ‹¬åŸºäºä¿®å¤çš„æ“çºµå’Œè¯­ä¹‰ä¿æŒæ‰°åŠ¨ã€‚å¤šä»»åŠ¡å­¦ä¹ æ¨¡å—åŒ…å«ä¸€ä¸ªåˆ†ç±»å¤´å’Œä¸¤ä¸ªåˆ†å‰²å¤´ï¼Œåˆ†åˆ«ç”¨äºå›¾åƒåˆ†ç±»å’Œåƒç´ çº§æºå±æ€§åˆ†å‰²ã€‚åå‘äº¤å‰æ³¨æ„åŠ›æ¨¡å—åˆ™ç”¨äºå°†åˆ†å‰²å¤´çš„çŸ¥è¯†ä¼ é€’ç»™åˆ†ç±»å¤´ï¼Œä»¥çº æ­£åˆ†ç±»åˆ†æ”¯ä¸­çš„æœ‰åè¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šGAMMAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ“çºµå¢å¼ºè®­ç»ƒç­–ç•¥å’Œåå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚æ“çºµå¢å¼ºè®­ç»ƒé€šè¿‡å¼•å…¥å¤šç§æ“çºµæ–¹å¼ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºï¼Œå‡å°‘å¯¹ç‰¹å®šç”Ÿæˆå™¨ä¼ªå½±çš„ä¾èµ–ã€‚åå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åˆ™å…è®¸åˆ†å‰²å¤´æŒ‡å¯¼åˆ†ç±»å¤´ï¼Œä»è€Œæé«˜åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šGAMMAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šæ ·åŒ–çš„æ“çºµç­–ç•¥ï¼Œä¾‹å¦‚in-paintingå’Œè¯­ä¹‰ä¿æŒæ‰°åŠ¨ï¼Œä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„å›¾åƒç¯¡æ”¹ï¼›2) åŒåˆ†å‰²å¤´çš„è®¾è®¡ï¼Œç”¨äºæä¾›åƒç´ çº§åˆ«çš„æºå±æ€§ä¿¡æ¯ï¼›3) åå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡åˆ†å‰²å¤´æ¥æŒ‡å¯¼åˆ†ç±»å¤´ï¼Œä»è€Œçº æ­£åˆ†ç±»åå·®ï¼›4) å¤šä»»åŠ¡æŸå¤±å‡½æ•°ï¼Œç»“åˆäº†åˆ†ç±»æŸå¤±å’Œåˆ†å‰²æŸå¤±ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GAMMAåœ¨GenImageåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå‡†ç¡®ç‡æé«˜äº†5.8%ï¼Œè¾¾åˆ°äº†SOTAæ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹æ–°å‘å¸ƒçš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰æ—¶ï¼Œä»ç„¶è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å®éªŒç»“æœéªŒè¯äº†GAMMAæ¡†æ¶åœ¨å‡å°‘é¢†åŸŸåå·®å’Œå¢å¼ºè¯­ä¹‰å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GAMMAçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå†…å®¹å®‰å…¨é¢†åŸŸï¼Œä¾‹å¦‚æ£€æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„AIç”Ÿæˆè™šå‡ä¿¡æ¯ã€ä¿æŠ¤çŸ¥è¯†äº§æƒã€é˜²æ­¢æ¶æ„è½¯ä»¶ä¼ æ’­ç­‰ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰åŠ©äºæé«˜æ•°å­—å–è¯çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä¸ºç»´æŠ¤ç½‘ç»œå®‰å…¨å’Œç¤¾ä¼šç¨³å®šæä¾›æŠ€æœ¯æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ã€éŸ³é¢‘ç­‰å…¶ä»–æ¨¡æ€çš„AIç”Ÿæˆå†…å®¹æ£€æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.

