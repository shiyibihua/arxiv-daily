---
layout: default
title: Efficient Learned Image Compression Through Knowledge Distillation
---

# Efficient Learned Image Compression Through Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10366" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10366v1</a>
  <a href="https://arxiv.org/pdf/2509.10366.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10366v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10366v1', 'Efficient Learned Image Compression Through Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fabien Allemand, Attilio Fiandrotti, Sumanta Chaudhuri, Alaa Eddine Mazouz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: 19 pages, 21 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/FABallemand/PRIM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçŸ¥è¯†è’¸é¦çš„é«˜æ•ˆå›¾åƒå‹ç¼©æ–¹æ³•ï¼Œé™ä½èµ„æºå ç”¨ï¼Œæå‡å®é™…åº”ç”¨æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾åƒå‹ç¼©` `çŸ¥è¯†è’¸é¦` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å‹ç¼©` `èµ„æºå—é™å¹³å°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºç¥ç»ç½‘ç»œçš„å›¾åƒå‹ç¼©æ–¹æ³•è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œã€‚
2. åˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼Œå°†å¤§å‹å¤æ‚æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°å‹ç½‘ç»œï¼Œä½¿å…¶åœ¨èµ„æºå ç”¨æ›´å°‘çš„æƒ…å†µä¸‹è¾¾åˆ°ç›¸è¿‘ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚
3. å®éªŒè¯æ˜ï¼ŒçŸ¥è¯†è’¸é¦åœ¨ä¸åŒç½‘ç»œæ¶æ„å’Œæ¯”ç‰¹ç‡/è´¨é‡æƒè¡¡ä¸‹å‡æœ‰æ•ˆï¼Œå¹¶èƒ½æ˜¾è‘—é™ä½è®¡ç®—èµ„æºå’Œèƒ½è€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æœºå™¨å­¦ä¹ å’Œå›¾åƒå¤„ç†äº¤å‰é¢†åŸŸçš„å­¦ä¹ å‹å›¾åƒå‹ç¼©ã€‚éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„å‹ç¼©æ–¹æ³•ä¸æ–­æ¶Œç°ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¼–ç å™¨å°†å›¾åƒæ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œç„¶åè¿›è¡Œé‡åŒ–ã€ç†µç¼–ç æˆäºŒè¿›åˆ¶ç æµï¼Œå¹¶ä¼ è¾“åˆ°æ¥æ”¶ç«¯ã€‚æ¥æ”¶ç«¯å¯¹ç æµè¿›è¡Œç†µè§£ç ï¼Œè§£ç å™¨é‡å»ºåŸå§‹å›¾åƒçš„è¿‘ä¼¼ã€‚è™½ç„¶ç°æœ‰ç ”ç©¶è¡¨æ˜è¿™äº›æ¨¡å‹ä¼˜äºä¼ ç»Ÿç¼–è§£ç å™¨ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„å¤„ç†èƒ½åŠ›ï¼Œä¸é€‚ç”¨äºèµ„æºå—é™å¹³å°çš„å®æ—¶åº”ç”¨ï¼Œé˜»ç¢äº†å…¶åœ¨ä¸»æµåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡çŸ¥è¯†è’¸é¦æ¥é™ä½ç”¨äºå›¾åƒå‹ç¼©çš„ç¥ç»ç½‘ç»œçš„èµ„æºéœ€æ±‚ã€‚çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§è®­ç»ƒèŒƒå¼ï¼Œå…¶ä¸­è¾ƒå°çš„ç¥ç»ç½‘ç»œåœ¨è¾ƒå¤§ã€æ›´å¤æ‚æ¨¡å‹çš„è¾“å‡ºä¸Šè¿›è¡Œéƒ¨åˆ†è®­ç»ƒï¼Œå¯ä»¥è·å¾—æ¯”ç‹¬ç«‹è®­ç»ƒæ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼ŒçŸ¥è¯†è’¸é¦å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºå›¾åƒå‹ç¼©ä»»åŠ¡ï¼ši) è·¨å„ç§æ¶æ„å¤§å°ï¼Œii) å®ç°ä¸åŒçš„å›¾åƒè´¨é‡/æ¯”ç‰¹ç‡æƒè¡¡ï¼Œä»¥åŠ iii) èŠ‚çœå¤„ç†å’Œèƒ½æºèµ„æºã€‚è¿™ç§æ–¹æ³•å¼•å…¥äº†æ–°çš„è®¾ç½®å’Œè¶…å‚æ•°ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢ä¸åŒæ•™å¸ˆæ¨¡å‹çš„å½±å“ï¼Œä»¥åŠæ›¿ä»£æŸå¤±å‡½æ•°ã€‚çŸ¥è¯†è’¸é¦ä¹Ÿå¯ä»¥æ‰©å±•åˆ°åŸºäºTransformerçš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå‹ç¼©æ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜ã€èµ„æºæ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œä½¿å…¶éš¾ä»¥åœ¨ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼ç³»ç»Ÿç­‰èµ„æºå—é™çš„å¹³å°ä¸Šéƒ¨ç½²ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶åœ¨å‹ç¼©æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿç¼–è§£ç å™¨ï¼Œä½†å…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œè®­ç»ƒä¸€ä¸ªå°å‹ã€é«˜æ•ˆçš„å­¦ç”Ÿç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿å¤§å‹ã€å¤æ‚çš„æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå­¦ç”Ÿç½‘ç»œå¯ä»¥åœ¨ä¿æŒè¾ƒé«˜å‹ç¼©æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦å’Œèµ„æºæ¶ˆè€—ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†åœ¨å‹ç¼©æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ•™å¸ˆç½‘ç»œå’Œä¸€ä¸ªå¾…è®­ç»ƒçš„å­¦ç”Ÿç½‘ç»œã€‚æ•™å¸ˆç½‘ç»œè´Ÿè´£ç”Ÿæˆé«˜è´¨é‡çš„å‹ç¼©è¡¨ç¤ºï¼Œå­¦ç”Ÿç½‘ç»œåˆ™å­¦ä¹ æ¨¡ä»¿æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºï¼ŒåŒ…æ‹¬ä¸­é—´ç‰¹å¾å’Œæœ€ç»ˆçš„é‡å»ºå›¾åƒã€‚è®­ç»ƒè¿‡ç¨‹æ¶‰åŠå¤šä¸ªæŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬é‡å»ºæŸå¤±ã€ç‰¹å¾åŒ¹é…æŸå¤±ç­‰ï¼Œä»¥ç¡®ä¿å­¦ç”Ÿç½‘ç»œèƒ½å¤Ÿå°½å¯èƒ½åœ°é€¼è¿‘æ•™å¸ˆç½‘ç»œçš„è¡Œä¸ºã€‚æœ€ç»ˆï¼Œå­¦ç”Ÿç½‘ç»œè¢«éƒ¨ç½²ç”¨äºå®é™…çš„å›¾åƒå‹ç¼©ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†çŸ¥è¯†è’¸é¦æŠ€æœ¯æˆåŠŸåº”ç”¨äºå›¾åƒå‹ç¼©é¢†åŸŸï¼Œå¹¶éªŒè¯äº†å…¶åœ¨ä¸åŒç½‘ç»œæ¶æ„å’Œæ¯”ç‰¹ç‡/è´¨é‡æƒè¡¡ä¸‹çš„æœ‰æ•ˆæ€§ã€‚ä¸ç›´æ¥è®­ç»ƒå°å‹ç½‘ç»œç›¸æ¯”ï¼ŒçŸ¥è¯†è’¸é¦èƒ½å¤Ÿæ˜¾è‘—æå‡å°å‹ç½‘ç»œçš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°é«˜è´¨é‡çš„å›¾åƒå‹ç¼©ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æ•™å¸ˆç½‘ç»œçš„é€‰æ‹©ï¼ˆå¯ä»¥ä½¿ç”¨å„ç§å…ˆè¿›çš„å›¾åƒå‹ç¼©æ¨¡å‹ï¼‰ï¼Œå­¦ç”Ÿç½‘ç»œçš„æ¶æ„è®¾è®¡ï¼ˆé€šå¸¸æ¯”æ•™å¸ˆç½‘ç»œå°å¾—å¤šï¼‰ï¼Œä»¥åŠæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼ˆåŒ…æ‹¬åƒç´ çº§åˆ«çš„é‡å»ºæŸå¤±ã€ç‰¹å¾çº§åˆ«çš„åŒ¹é…æŸå¤±ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œè¶…å‚æ•°çš„è®¾ç½®ï¼Œå¦‚è’¸é¦æ¸©åº¦ã€æŸå¤±æƒé‡ç­‰ï¼Œä¹Ÿä¼šå½±å“æœ€ç»ˆçš„æ€§èƒ½ã€‚è®ºæ–‡å¯èƒ½è¿˜æ¢ç´¢äº†ä¸åŒçš„è’¸é¦ç­–ç•¥ï¼Œä¾‹å¦‚åªè’¸é¦æœ€ç»ˆè¾“å‡ºï¼Œè¿˜æ˜¯åŒæ—¶è’¸é¦ä¸­é—´ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶æˆåŠŸåœ°å°†çŸ¥è¯†è’¸é¦åº”ç”¨äºå›¾åƒå‹ç¼©ï¼Œå¹¶åœ¨ä¸åŒæ¶æ„å°ºå¯¸å’Œæ¯”ç‰¹ç‡/å›¾åƒè´¨é‡æƒè¡¡æ–¹é¢éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦è®­ç»ƒçš„å°å‹ç½‘ç»œåœ¨ä¿æŒè¾ƒé«˜å‹ç¼©æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºå’Œèƒ½è€—ï¼Œä¸ºåœ¨èµ„æºå—é™å¹³å°ä¸Šéƒ¨ç½²æ·±åº¦å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ€§èƒ½æ•°æ®ï¼ˆå¦‚å‹ç¼©ç‡ã€PSNRç­‰ï¼‰å’Œä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºç§»åŠ¨è®¾å¤‡ã€ç‰©è”ç½‘è®¾å¤‡ã€è§†é¢‘ç›‘æ§ç³»ç»Ÿç­‰èµ„æºå—é™çš„åœºæ™¯ï¼Œå®ç°é«˜æ•ˆçš„å›¾åƒå’Œè§†é¢‘å‹ç¼©ã€‚é€šè¿‡é™ä½è®¡ç®—å¤æ‚åº¦å’Œèƒ½è€—ï¼Œå¯ä»¥å»¶é•¿è®¾å¤‡ç”µæ± ç»­èˆªæ—¶é—´ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¿ƒè¿›æ·±åº¦å­¦ä¹ å›¾åƒå‹ç¼©æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºäº‘å­˜å‚¨ã€æµåª’ä½“æœåŠ¡ç­‰é¢†åŸŸï¼Œé™ä½å­˜å‚¨å’Œä¼ è¾“æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learned image compression sits at the intersection of machine learning and image processing. With advances in deep learning, neural network-based compression methods have emerged. In this process, an encoder maps the image to a low-dimensional latent space, which is then quantized, entropy-coded into a binary bitstream, and transmitted to the receiver. At the receiver end, the bitstream is entropy-decoded, and a decoder reconstructs an approximation of the original image. Recent research suggests that these models consistently outperform conventional codecs. However, they require significant processing power, making them unsuitable for real-time use on resource-constrained platforms, which hinders their deployment in mainstream applications. This study aims to reduce the resource requirements of neural networks used for image compression by leveraging knowledge distillation, a training paradigm where smaller neural networks, partially trained on the outputs of larger, more complex models, can achieve better performance than when trained independently. Our work demonstrates that knowledge distillation can be effectively applied to image compression tasks: i) across various architecture sizes, ii) to achieve different image quality/bit rate tradeoffs, and iii) to save processing and energy resources. This approach introduces new settings and hyperparameters, and future research could explore the impact of different teacher models, as well as alternative loss functions. Knowledge distillation could also be extended to transformer-based models. The code is publicly available at: https://github.com/FABallemand/PRIM .

