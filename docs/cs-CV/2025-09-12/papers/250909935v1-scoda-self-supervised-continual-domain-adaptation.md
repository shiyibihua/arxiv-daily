---
layout: default
title: SCoDA: Self-supervised Continual Domain Adaptation
---

# SCoDA: Self-supervised Continual Domain Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09935" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09935v1</a>
  <a href="https://arxiv.org/pdf/2509.09935.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09935v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09935v1', 'SCoDA: Self-supervised Continual Domain Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chirayu Agrawal, Snehasis Mukherjee

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: Submitted to ICVGIP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCoDAï¼Œé€šè¿‡è‡ªç›‘ç£å’Œå‡ ä½•æµå½¢å¯¹é½å®ç°å…æºæŒç»­é¢†åŸŸè‡ªé€‚åº”ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `å…æºé¢†åŸŸè‡ªé€‚åº”` `è‡ªç›‘ç£å­¦ä¹ ` `å‡ ä½•æµå½¢å¯¹é½` `æŒç»­å­¦ä¹ ` `é¢†åŸŸè‡ªé€‚åº”` `æŒ‡æ•°ç§»åŠ¨å¹³å‡` `ç©ºé—´ç›¸ä¼¼æ€§æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SFDAæ–¹æ³•ä¾èµ–ç›‘ç£é¢„è®­ç»ƒå’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¿½ç•¥äº†æºæ¨¡å‹æ½œåœ¨æµå½¢çš„å‡ ä½•ä¿¡æ¯ã€‚
2. SCoDAåˆ©ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒåˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å‡ ä½•æµå½¢å¯¹é½åŸåˆ™è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSCoDAåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„SFDAæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…æºé¢†åŸŸè‡ªé€‚åº”(SFDA)æ—¨åœ¨å°†æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡åŸŸï¼Œè€Œæ— éœ€è®¿é—®æºåŸŸæ•°æ®ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»å®Œå…¨ç›‘ç£é¢„è®­ç»ƒçš„æºæ¨¡å‹å¼€å§‹ï¼Œå¹¶é€šè¿‡å¯¹é½å®ä¾‹çº§ç‰¹å¾æ¥æç‚¼çŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºL2å½’ä¸€åŒ–ç‰¹å¾å‘é‡ä¸Šçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ— æ„ä¸­ä¸¢å¼ƒäº†æºæ¨¡å‹æ½œåœ¨æµå½¢çš„å…³é”®å‡ ä½•ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªç›‘ç£æŒç»­é¢†åŸŸè‡ªé€‚åº”(SCoDA)æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬å¯¹æ ‡å‡†åšæ³•è¿›è¡Œäº†ä¸¤é¡¹å…³é”®æ”¹è¿›ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬é¿å…ä¾èµ–ç›‘ç£é¢„è®­ç»ƒï¼Œè€Œæ˜¯ä½¿ç”¨å®Œå…¨é€šè¿‡è‡ªç›‘ç£(SSL)é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹æ¥åˆå§‹åŒ–æ¡†æ¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†å‡ ä½•æµå½¢å¯¹é½çš„åŸåˆ™åº”ç”¨äºSFDAè®¾ç½®ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡ç»“åˆå®ä¾‹çº§ç‰¹å¾åŒ¹é…å’Œç©ºé—´ç›¸ä¼¼æ€§æŸå¤±çš„å¤åˆç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†å¯¹æŠ—ç¾éš¾æ€§é—å¿˜ï¼Œæ•™å¸ˆæ¨¡å‹çš„å‚æ•°é€šè¿‡å­¦ç”Ÿæ¨¡å‹å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡(EMA)è¿›è¡Œæ›´æ–°ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSCoDAæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„SFDAæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…æºé¢†åŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰é—®é¢˜ï¼Œå³åœ¨æ— æ³•è®¿é—®æºåŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°†æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡åŸŸã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç›‘ç£é¢„è®­ç»ƒçš„æºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½ç‰¹å¾ï¼Œå¿½ç•¥äº†æºåŸŸæ•°æ®æ½œåœ¨æµå½¢çš„å‡ ä½•ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´çŸ¥è¯†è¿ç§»ä¸å……åˆ†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é¿å…å¯¹ç›‘ç£é¢„è®­ç»ƒçš„ä¾èµ–ï¼Œå¹¶å¼•å…¥å‡ ä½•æµå½¢å¯¹é½çš„æ€æƒ³ï¼Œä¿ç•™æºåŸŸæ•°æ®çš„å‡ ä½•ç»“æ„ä¿¡æ¯ã€‚é€šè¿‡ç©ºé—´ç›¸ä¼¼æ€§æŸå¤±ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„å‡ ä½•ç»“æ„ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCoDAæ¡†æ¶åŒ…å«ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹å’Œä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚æ•™å¸ˆæ¨¡å‹é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡ç»“åˆå®ä¾‹çº§ç‰¹å¾åŒ¹é…å’Œç©ºé—´ç›¸ä¼¼æ€§æŸå¤±çš„å¤åˆç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œæ•™å¸ˆæ¨¡å‹çš„å‚æ•°é€šè¿‡å­¦ç”Ÿæ¨¡å‹å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰è¿›è¡Œæ›´æ–°ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼›ç„¶åï¼Œä½¿ç”¨æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”ï¼ŒåŒæ—¶ä¿æŒæ•™å¸ˆæ¨¡å‹çš„å‡ ä½•ç»“æ„ï¼›æœ€åï¼Œé€šè¿‡EMAæ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šSCoDAçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ä»£æ›¿ç›‘ç£é¢„è®­ç»ƒï¼Œé¿å…äº†å¯¹æºåŸŸæ ‡ç­¾çš„ä¾èµ–ï¼›2) å¼•å…¥ç©ºé—´ç›¸ä¼¼æ€§æŸå¤±ï¼Œä¿ç•™äº†æºåŸŸæ•°æ®çš„å‡ ä½•ç»“æ„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSCoDAæ›´åŠ å…³æ³¨æºåŸŸæ•°æ®çš„å†…åœ¨ç»“æ„ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å¯¹é½å®ä¾‹çº§ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼šå…·ä½“é‡‡ç”¨ä½•ç§è‡ªç›‘ç£æ–¹æ³•ï¼ˆä¾‹å¦‚å¯¹æ¯”å­¦ä¹ ã€ç”Ÿæˆæ¨¡å‹ç­‰ï¼‰éœ€è¦å‚è€ƒåŸæ–‡ç»†èŠ‚ï¼›2) ç©ºé—´ç›¸ä¼¼æ€§æŸå¤±ï¼šç”¨äºè¡¡é‡å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹ç‰¹å¾ç©ºé—´å‡ ä½•ç»“æ„çš„ç›¸ä¼¼æ€§ï¼Œå…·ä½“å½¢å¼éœ€è¦å‚è€ƒåŸæ–‡ç»†èŠ‚ï¼›3) æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰ï¼šç”¨äºæ›´æ–°æ•™å¸ˆæ¨¡å‹çš„å‚æ•°ï¼Œå¹³æ»‘å­¦ç”Ÿæ¨¡å‹çš„æ›´æ–°ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚EMAçš„è¡°å‡ç³»æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SCoDAåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Office-Homeæ•°æ®é›†ä¸Šï¼ŒSCoDAçš„å¹³å‡å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„SFDAæ–¹æ³•æé«˜äº†X%ã€‚æ­¤å¤–ï¼ŒSCoDAåœ¨VisDA-Cæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒSCoDAèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ å’Œå‡ ä½•æµå½¢å¯¹é½ï¼Œä»è€Œå®ç°æ›´å¥½çš„é¢†åŸŸè‡ªé€‚åº”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SCoDAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œå½“æºåŸŸæ•°æ®éš¾ä»¥è·å–æˆ–å­˜åœ¨éšç§é—®é¢˜æ—¶ï¼Œå¯ä»¥ä½¿ç”¨SCoDAå°†æ¨¡å‹è¿ç§»åˆ°ç›®æ ‡åŸŸã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæŒç»­å­¦ä¹ åœºæ™¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸æ–­é€‚åº”æ–°çš„é¢†åŸŸï¼Œè€Œä¸ä¼šå¿˜è®°ä¹‹å‰å­¦ä¹ çš„çŸ¥è¯†ã€‚æœªæ¥ï¼ŒSCoDAå¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç»“åˆï¼Œä¾‹å¦‚å¯¹æŠ—è®­ç»ƒã€å…ƒå­¦ä¹ ç­‰ï¼Œè¿›ä¸€æ­¥æé«˜é¢†åŸŸè‡ªé€‚åº”çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a model to a target domain without access to the data of the source domain. Prevailing methods typically start with a source model pre-trained with full supervision and distill the knowledge by aligning instance-level features. However, these approaches, relying on cosine similarity over L2-normalized feature vectors, inadvertently discard crucial geometric information about the latent manifold of the source model. We introduce Self-supervised Continual Domain Adaptation (SCoDA) to address these limitations. We make two key departures from standard practice: first, we avoid the reliance on supervised pre-training by initializing the proposed framework with a teacher model pre-trained entirely via self-supervision (SSL). Second, we adapt the principle of geometric manifold alignment to the SFDA setting. The student is trained with a composite objective combining instance-level feature matching with a Space Similarity Loss. To combat catastrophic forgetting, the teacher's parameters are updated via an Exponential Moving Average (EMA) of the student's parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA significantly outperforms state-of-the-art SFDA methods.

