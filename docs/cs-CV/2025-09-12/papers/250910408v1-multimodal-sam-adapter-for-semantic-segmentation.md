---
layout: default
title: Multimodal SAM-adapter for Semantic Segmentation
---

# Multimodal SAM-adapter for Semantic Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10408" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10408v1</a>
  <a href="https://arxiv.org/pdf/2509.10408.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10408v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10408v1', 'Multimodal SAM-adapter for Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Iacopo Curti, Pierluigi Zama Ramirez, Alioscia Petrelli, Luigi Di Stefano

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/iacopo97/Multimodal-SAM-Adapter)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMM SAM-adapterï¼Œç”¨äºæå‡å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰åˆ†å‰²` `å¤šæ¨¡æ€èåˆ` `Segment Anything Model` `Adapterç½‘ç»œ` `é²æ£’æ€§` `è‡ªåŠ¨é©¾é©¶` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œå¦‚å…‰ç…§ä¸è¶³ã€é®æŒ¡å’Œæ¶åŠ£å¤©æ°”ç­‰ã€‚
2. MM SAM-adapteré€šè¿‡adapterç½‘ç»œå°†èåˆçš„å¤šæ¨¡æ€ç‰¹å¾æ³¨å…¥SAMï¼Œæå‡æ¨¡å‹å¯¹å¤æ‚ç¯å¢ƒçš„é€‚åº”æ€§ã€‚
3. åœ¨DeLiVERã€FMBå’ŒMUSESæ•°æ®é›†ä¸Šï¼ŒMM SAM-adapterå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦æˆåƒå’Œæœºå™¨äººç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å…‰ç…§ä¸è¶³ã€é®æŒ¡å’Œæ¶åŠ£å¤©æ°”ç­‰å¤æ‚æ¡ä»¶ä¸‹ä»ç„¶è„†å¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ€è¿‘å‡ºç°äº†é›†æˆè¾…åŠ©ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå¦‚LiDARã€çº¢å¤–ï¼‰çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œå®ƒä»¬æä¾›äº’è¡¥ä¿¡æ¯ä»¥å¢å¼ºé²æ£’æ€§ã€‚æœ¬æ–‡æå‡ºäº†MM SAM-adapterï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ‰©å±•äº†Segment Anything Model (SAM)åœ¨å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é‡‡ç”¨adapterç½‘ç»œï¼Œå°†èåˆçš„å¤šæ¨¡æ€ç‰¹å¾æ³¨å…¥åˆ°SAMä¸°å¯Œçš„RGBç‰¹å¾ä¸­ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿä¿æŒRGBç‰¹å¾å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä»…åœ¨è¾…åŠ©æ¨¡æ€æä¾›é¢å¤–çº¿ç´¢æ—¶æ‰é€‰æ‹©æ€§åœ°åˆå¹¶å®ƒä»¬ã€‚å› æ­¤ï¼ŒMM SAM-adapterå®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯çš„å¹³è¡¡å’Œæœ‰æ•ˆåˆ©ç”¨ã€‚æˆ‘ä»¬åœ¨DeLiVERã€FMBå’ŒMUSESä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒMM SAM-adapterå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ†ææ¨¡æ€è´¡çŒ®ï¼Œæˆ‘ä»¬å°†DeLiVERå’ŒFMBåˆ’åˆ†ä¸ºRGB-easyå’ŒRGB-hardå­é›†ã€‚ç»“æœä¸€è‡´è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æœ‰åˆ©å’Œä¸åˆ©æ¡ä»¶ä¸‹å‡ä¼˜äºç«äº‰æ–¹æ³•ï¼Œçªå‡ºäº†å¤šæ¨¡æ€è‡ªé€‚åº”åœ¨é²æ£’åœºæ™¯ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯­ä¹‰åˆ†å‰²åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å…‰ç…§ä¸è¶³ã€é®æŒ¡å’Œæ¶åŠ£å¤©æ°”ç­‰æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®æä¾›çš„äº’è¡¥ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Segment Anything Model (SAM)å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªadapterç½‘ç»œå°†å¤šæ¨¡æ€ä¿¡æ¯é€‰æ‹©æ€§åœ°èå…¥SAMçš„RGBç‰¹å¾ä¸­ã€‚è¿™æ ·æ—¢èƒ½ä¿ç•™SAMåœ¨RGBå›¾åƒä¸Šçš„ä¼˜åŠ¿ï¼Œåˆèƒ½åˆ©ç”¨å…¶ä»–æ¨¡æ€çš„ä¿¡æ¯æ¥æå‡åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMM SAM-adapterçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ï¼š1) RGBå›¾åƒè¾“å…¥SAMï¼›2) å…¶ä»–æ¨¡æ€æ•°æ®ï¼ˆå¦‚LiDARã€çº¢å¤–ï¼‰è¿›è¡Œç‰¹å¾æå–ï¼›3) å¤šæ¨¡æ€ç‰¹å¾èåˆï¼›4) Adapterç½‘ç»œå°†èåˆåçš„ç‰¹å¾æ³¨å…¥SAMçš„å›¾åƒç‰¹å¾ä¸­ï¼›5) SAMè¿›è¡Œè¯­ä¹‰åˆ†å‰²é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨adapterç½‘ç»œå°†å¤šæ¨¡æ€ä¿¡æ¯æ³¨å…¥SAMã€‚è¿™ç§æ–¹å¼é¿å…äº†ç›´æ¥ä»å¤´è®­ç»ƒä¸€ä¸ªå¤šæ¨¡æ€åˆ†å‰²æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨äº†SAMé¢„è®­ç»ƒçš„å¼ºå¤§èƒ½åŠ›ï¼Œå¹¶é€šè¿‡adapterç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šAdapterç½‘ç»œçš„è®¾è®¡æ˜¯å…³é”®ã€‚å…·ä½“ç»“æ„æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªæ˜ å°„ï¼Œå°†èåˆçš„å¤šæ¨¡æ€ç‰¹å¾è½¬æ¢ä¸ºä¸SAMçš„RGBç‰¹å¾å…¼å®¹çš„å½¢å¼ï¼Œå¹¶é€‰æ‹©æ€§åœ°èåˆè¿™äº›ç‰¹å¾ã€‚æŸå¤±å‡½æ•°ç”¨äºä¼˜åŒ–adapterç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯æ¥æå‡åˆ†å‰²æ€§èƒ½ã€‚å…·ä½“çš„èåˆæ–¹å¼å’Œadapterç½‘ç»œç»“æ„åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MM SAM-adapteråœ¨DeLiVERã€FMBå’ŒMUSESä¸‰ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚é€šè¿‡å°†æ•°æ®é›†åˆ’åˆ†ä¸ºRGB-easyå’ŒRGB-hardå­é›†ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒMM SAM-adapteråœ¨å„ç§æ¡ä»¶ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å¤šæ¨¡æ€è‡ªé€‚åº”åœ¨é²æ£’åœºæ™¯ç†è§£æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æé«˜è½¦è¾†åœ¨æ¶åŠ£å¤©æ°”å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥å¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å¯¼èˆªå’Œæ“ä½œèƒ½åŠ›ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå¯ä»¥è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ›´å‡†ç¡®çš„ç—…ç¶åˆ†å‰²å’Œè¯Šæ–­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.

