---
layout: default
title: LayerLock: Non-collapsing Representation Learning with Progressive Freezing
---

# LayerLock: Non-collapsing Representation Learning with Progressive Freezing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10156" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10156v3</a>
  <a href="https://arxiv.org/pdf/2509.10156.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10156v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10156v3', 'LayerLock: Non-collapsing Representation Learning with Progressive Freezing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew A. Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi S. M. Sajjadi, Joao Carreira

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LayerLockï¼šé€šè¿‡æ¸è¿›å¼å†»ç»“å®ç°éåå¡Œçš„è‡ªç›‘ç£è¡¨å¾å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è§†è§‰è¡¨å¾å­¦ä¹ ` `æ©ç è‡ªç¼–ç å™¨` `æ¸è¿›å¼å†»ç»“` `è¡¨å¾åå¡Œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘MAEæ¨¡å‹è®­ç»ƒæ•ˆç‡ä½ï¼Œä¸”æ·±å±‚ç½‘ç»œæ”¶æ•›æ…¢ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚
2. LayerLocké€šè¿‡æ¸è¿›å¼å†»ç»“ç½‘ç»œå±‚ï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶é¿å…æ½œåœ¨ç©ºé—´é¢„æµ‹ä¸­çš„è¡¨å¾åå¡Œé—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLayerLockåœ¨å¤§å‹æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨4DSæ„ŸçŸ¥å¥—ä»¶ä¸Šè¶…è¶Šäº†éæ½œåœ¨ç©ºé—´æ©ç é¢„æµ‹æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºLayerLockï¼Œä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è‡ªç›‘ç£è§†è§‰è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ¸è¿›å¼å±‚å†»ç»“ï¼Œé€æ­¥ä»åƒç´ é¢„æµ‹è¿‡æ¸¡åˆ°æ½œåœ¨ç©ºé—´é¢„æµ‹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è§†é¢‘æ©ç è‡ªç¼–ç å™¨(MAE)æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒViTå±‚çš„æ”¶æ•›é€Ÿåº¦ä¸å…¶æ·±åº¦æœ‰å…³ï¼šæµ…å±‚æ”¶æ•›æ—©ï¼Œæ·±å±‚æ”¶æ•›æ™šã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºå¯ä»¥é€šè¿‡æ ¹æ®æ˜¾å¼çš„æ—¶é—´è¡¨é€æ­¥å†»ç»“æ¨¡å‹æ¥åŠ é€Ÿæ ‡å‡†MAEçš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥æ—¶é—´è¡¨å¯ä»¥åº”ç”¨äºä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ½œåœ¨ç©ºé—´é¢„æµ‹æ–¹æ³•ï¼Œä¸”ä¸ä¼šé­å—â€œè¡¨å¾åå¡Œâ€çš„é—®é¢˜ã€‚æœ¬æ–‡å°†æå‡ºçš„LayerLockæ–¹æ³•åº”ç”¨äºé«˜è¾¾40äº¿å‚æ•°çš„å¤§å‹æ¨¡å‹ï¼Œåœ¨4DSæ„ŸçŸ¥å¥—ä»¶ä¸Šçš„ç»“æœè¶…è¿‡äº†éæ½œåœ¨ç©ºé—´çš„æ©ç é¢„æµ‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†é¢‘æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰è®­ç»ƒæ•ˆç‡è¾ƒä½ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œæ·±å±‚ç½‘ç»œçš„æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œå¯¼è‡´æ•´ä½“è®­ç»ƒæ—¶é—´é•¿ã€‚æ­¤å¤–ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œé¢„æµ‹å®¹æ˜“å‡ºç°â€œè¡¨å¾åå¡Œâ€é—®é¢˜ï¼Œå³æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾ç¼ºä¹åŒºåˆ†æ€§ï¼Œæ‰€æœ‰è¾“å…¥éƒ½æ˜ å°„åˆ°ç›¸åŒçš„è¾“å‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLayerLockçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ViTå±‚æ”¶æ•›é€Ÿåº¦ä¸å…¶æ·±åº¦ç›¸å…³çš„ç‰¹æ€§ï¼Œé€šè¿‡æ¸è¿›å¼åœ°å†»ç»“å·²ç»æ”¶æ•›çš„æµ…å±‚ç½‘ç»œï¼Œå°†è®¡ç®—èµ„æºé›†ä¸­åœ¨å°šæœªæ”¶æ•›çš„æ·±å±‚ç½‘ç»œä¸Šï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œé€šè¿‡ç‰¹å®šçš„å†»ç»“ç­–ç•¥ï¼Œé¿å…æ½œåœ¨ç©ºé—´é¢„æµ‹ä¸­çš„è¡¨å¾åå¡Œé—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLayerLockçš„æ•´ä½“æ¡†æ¶åŸºäºæ ‡å‡†çš„MAEï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è¾“å…¥è§†é¢‘å¸§è¿›è¡Œæ©ç ï¼›2) ä½¿ç”¨ViTç¼–ç å™¨æå–ç‰¹å¾ï¼›3) æ ¹æ®é¢„è®¾çš„æ—¶é—´è¡¨ï¼Œé€æ­¥å†»ç»“ViTçš„æµ…å±‚ï¼›4) ä½¿ç”¨è§£ç å™¨é‡å»ºè¢«æ©ç çš„åŒºåŸŸæˆ–é¢„æµ‹æ½œåœ¨è¡¨å¾ï¼›5) è®¡ç®—æŸå¤±å¹¶æ›´æ–°æœªå†»ç»“çš„ç½‘ç»œå‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLayerLockçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ¸è¿›å¼å±‚å†»ç»“ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¸ä»…åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜é¿å…äº†æ½œåœ¨ç©ºé—´é¢„æµ‹ä¸­çš„è¡¨å¾åå¡Œé—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„MAEæ–¹æ³•ç›¸æ¯”ï¼ŒLayerLockèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œå¹¶å­¦ä¹ åˆ°æ›´å…·åŒºåˆ†æ€§çš„è¡¨å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šLayerLockçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ˜¾å¼çš„å†»ç»“æ—¶é—´è¡¨ï¼Œè¯¥æ—¶é—´è¡¨å†³å®šäº†æ¯ä¸€å±‚ä½•æ—¶è¢«å†»ç»“ï¼›2) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œæ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚é‡å»ºæŸå¤±æˆ–å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼›3) ç½‘ç»œç»“æ„çš„é€‰æ‹©ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„ViTå˜ä½“ä½œä¸ºç¼–ç å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LayerLockåœ¨å¤§å‹æ¨¡å‹ï¼ˆé«˜è¾¾40äº¿å‚æ•°ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºéæ½œåœ¨ç©ºé—´çš„æ©ç é¢„æµ‹æ–¹æ³•ã€‚åœ¨4DSæ„ŸçŸ¥å¥—ä»¶ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLayerLockèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°é«˜è´¨é‡çš„è§†è§‰è¡¨å¾ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“çš„æ•°æ®æŒ‡æ ‡å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†çš„å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LayerLockå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºè§†é¢‘ç†è§£ã€åŠ¨ä½œè¯†åˆ«ã€è§†é¢‘ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡åŠ é€Ÿè‡ªç›‘ç£å­¦ä¹ è¿‡ç¨‹ï¼Œå¯ä»¥æ›´é«˜æ•ˆåœ°è®­ç»ƒå¤§å‹è§†è§‰æ¨¡å‹ï¼Œä»è€Œæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLayerLocké¿å…è¡¨å¾åå¡Œçš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨éœ€è¦é«˜è´¨é‡è¡¨å¾çš„ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä¾‹å¦‚è§†é¢‘æ£€ç´¢å’Œè§†é¢‘èšç±»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from "representation collapse". We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.

