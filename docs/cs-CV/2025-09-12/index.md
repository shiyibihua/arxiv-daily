---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-12
---

# cs.CVï¼ˆ2025-09-12ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250910345v2-towards-understanding-visual-grounding-in-visual-language-models.html">Towards Understanding Visual Grounding in Visual Language Models</a></td>
  <td>ç»¼è¿°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å®šä½æŠ€æœ¯ï¼Œåˆ†ææŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10345v2" data-paper-url="./papers/250910345v2-towards-understanding-visual-grounding-in-visual-language-models.html" onclick="toggleFavorite(this, '2509.10345v2', 'Towards Understanding Visual Grounding in Visual Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250910059v1-multimodal-mathematical-reasoning-embedded-in-aerial-vehicle-imagery.html">Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration</a></td>
  <td>æå‡ºAVI-Mathæ— äººæœºå›¾åƒæ•°å­¦æ¨ç†åŸºå‡†ï¼Œæ­ç¤ºç°æœ‰VLMçš„å±€é™æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10059v1" data-paper-url="./papers/250910059v1-multimodal-mathematical-reasoning-embedded-in-aerial-vehicle-imagery.html" onclick="toggleFavorite(this, '2509.10059v1', 'Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250910683v1-a-comparison-and-evaluation-of-fine-tuned-convolutional-neural-netwo.html">A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI</a></td>
  <td>å¯¹æ¯”å¾®è°ƒLLMä¸CNNåœ¨è„‘è‚¿ç˜¤MRIå›¾åƒåˆ†ç±»ä¸åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10683v1" data-paper-url="./papers/250910683v1-a-comparison-and-evaluation-of-fine-tuned-convolutional-neural-netwo.html" onclick="toggleFavorite(this, '2509.10683v1', 'A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250910282v1-mcl-ad-multimodal-collaboration-learning-for-zero-shot-3d-anomaly-de.html">MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection</a></td>
  <td>MCL-ADï¼šæå‡ºå¤šæ¨¡æ€ååŒå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé›¶æ ·æœ¬3Då¼‚å¸¸æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10282v1" data-paper-url="./papers/250910282v1-mcl-ad-multimodal-collaboration-learning-for-zero-shot-3d-anomaly-de.html" onclick="toggleFavorite(this, '2509.10282v1', 'MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250910748v1-scope-speech-guided-collaborative-perception-framework-for-surgical-.html">SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation</a></td>
  <td>SCOPEæ¡†æ¶ï¼šè¯­éŸ³å¼•å¯¼çš„ååŒæ„ŸçŸ¥ï¼Œç”¨äºæ‰‹æœ¯åœºæ™¯åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10748v1" data-paper-url="./papers/250910748v1-scope-speech-guided-collaborative-perception-framework-for-surgical-.html" onclick="toggleFavorite(this, '2509.10748v1', 'SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250910026v3-lav-cot-language-aware-visual-cot-with-multi-aspect-reward-optimizat.html">LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA</a></td>
  <td>æå‡ºLaV-CoTæ¡†æ¶ï¼Œé€šè¿‡å¤šæ–¹é¢å¥–åŠ±ä¼˜åŒ–ï¼Œè§£å†³çœŸå®ä¸–ç•Œå¤šè¯­è¨€VQAé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10026v3" data-paper-url="./papers/250910026v3-lav-cot-language-aware-visual-cot-with-multi-aspect-reward-optimizat.html" onclick="toggleFavorite(this, '2509.10026v3', 'LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250910105v2-varco-vision-20-technical-report.html">VARCO-VISION-2.0 Technical Report</a></td>
  <td>VARCO-VISION-2.0ï¼šå¼€æºåŒè¯­è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡å¤šæ¨¡æ€ç†è§£ä¸OCRèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10105v2" data-paper-url="./papers/250910105v2-varco-vision-20-technical-report.html" onclick="toggleFavorite(this, '2509.10105v2', 'VARCO-VISION-2.0 Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250910266v2-signmouth-leveraging-mouthing-cues-for-sign-language-translation-by-.html">SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion</a></td>
  <td>SignClipï¼šåˆ©ç”¨å£å‹çº¿ç´¢çš„å¤šæ¨¡æ€å¯¹æ¯”èåˆæ‰‹è¯­ç¿»è¯‘</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10266v2" data-paper-url="./papers/250910266v2-signmouth-leveraging-mouthing-cues-for-sign-language-translation-by-.html" onclick="toggleFavorite(this, '2509.10266v2', 'SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250910620v1-building-a-general-simclr-self-supervised-foundation-model-across-ne.html">Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</a></td>
  <td>æ„å»ºé€šç”¨SimCLRè‡ªç›‘ç£è„‘MRIåŸºç¡€æ¨¡å‹ï¼Œæå‡3Dè„‘éƒ¨ç–¾ç—…è¯Šæ–­</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10620v1" data-paper-url="./papers/250910620v1-building-a-general-simclr-self-supervised-foundation-model-across-ne.html" onclick="toggleFavorite(this, '2509.10620v1', 'Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250912250v1-onlinehoi-towards-online-human-object-interaction-generation-and-per.html">OnlineHOI: Towards Online Human-Object Interaction Generation and Perception</a></td>
  <td>æå‡ºOnlineHOIæ¡†æ¶ï¼Œç”¨äºåœ¨çº¿äºº-ç‰©äº¤äº’ç”Ÿæˆä¸æ„ŸçŸ¥ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.12250v1" data-paper-url="./papers/250912250v1-onlinehoi-towards-online-human-object-interaction-generation-and-per.html" onclick="toggleFavorite(this, '2509.12250v1', 'OnlineHOI: Towards Online Human-Object Interaction Generation and Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250909988v1-flare-ssm-deep-state-space-models-with-influence-balanced-loss-for-7.html">FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction</a></td>
  <td>æå‡ºFLARE-SSMæ¨¡å‹ï¼Œåˆ©ç”¨æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œå½±å“åŠ›å¹³è¡¡æŸå¤±è¿›è¡Œ72å°æ—¶å¤ªé˜³è€€æ–‘é¢„æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09988v1" data-paper-url="./papers/250909988v1-flare-ssm-deep-state-space-models-with-influence-balanced-loss-for-7.html" onclick="toggleFavorite(this, '2509.09988v1', 'FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250910453v1-ssl-ad-spatiotemporal-self-supervised-learning-for-generalizability-.html">SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets</a></td>
  <td>SSL-ADï¼šæ—¶ç©ºè‡ªç›‘ç£å­¦ä¹ æå‡é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹ä»»åŠ¡çš„æ³›åŒ–æ€§å’Œé€‚åº”æ€§</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10453v1" data-paper-url="./papers/250910453v1-ssl-ad-spatiotemporal-self-supervised-learning-for-generalizability-.html" onclick="toggleFavorite(this, '2509.10453v1', 'SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer&#39;s Prediction Tasks and Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250910156v3-layerlock-non-collapsing-representation-learning-with-progressive-fr.html">LayerLock: Non-collapsing Representation Learning with Progressive Freezing</a></td>
  <td>LayerLockï¼šé€šè¿‡æ¸è¿›å¼å†»ç»“å®ç°éåå¡Œçš„è‡ªç›‘ç£è¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10156v3" data-paper-url="./papers/250910156v3-layerlock-non-collapsing-representation-learning-with-progressive-fr.html" onclick="toggleFavorite(this, '2509.10156v3', 'LayerLock: Non-collapsing Representation Learning with Progressive Freezing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250910366v1-efficient-learned-image-compression-through-knowledge-distillation.html">Efficient Learned Image Compression Through Knowledge Distillation</a></td>
  <td>æå‡ºåŸºäºçŸ¥è¯†è’¸é¦çš„é«˜æ•ˆå›¾åƒå‹ç¼©æ–¹æ³•ï¼Œé™ä½èµ„æºå ç”¨ï¼Œæå‡å®é™…åº”ç”¨æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10366v1" data-paper-url="./papers/250910366v1-efficient-learned-image-compression-through-knowledge-distillation.html" onclick="toggleFavorite(this, '2509.10366v1', 'Efficient Learned Image Compression Through Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250910058v1-color-me-correctly-bridging-perceptual-color-spaces-and-text-embeddi.html">Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation</a></td>
  <td>æå‡ºä¸€ç§å…è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡LLMå¢å¼ºæ–‡æœ¬åµŒå…¥ï¼Œæå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„é¢œè‰²å‡†ç¡®æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10058v1" data-paper-url="./papers/250910058v1-color-me-correctly-bridging-perceptual-color-spaces-and-text-embeddi.html" onclick="toggleFavorite(this, '2509.10058v1', 'Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250910278v1-detecting-text-manipulation-in-images-using-vision-language-models.html">Detecting Text Manipulation in Images using Vision Language Models</a></td>
  <td>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ£€æµ‹å›¾åƒä¸­çš„æ–‡æœ¬ç¯¡æ”¹</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10278v1" data-paper-url="./papers/250910278v1-detecting-text-manipulation-in-images-using-vision-language-models.html" onclick="toggleFavorite(this, '2509.10278v1', 'Detecting Text Manipulation in Images using Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250910250v1-gamma-generalizable-alignment-via-multi-task-and-manipulation-augmen.html">GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection</a></td>
  <td>GAMMAï¼šé€šè¿‡å¤šä»»åŠ¡å’Œæ“çºµå¢å¼ºè®­ç»ƒå®ç°AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ³›åŒ–å¯¹é½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10250v1" data-paper-url="./papers/250910250v1-gamma-generalizable-alignment-via-multi-task-and-manipulation-augmen.html" onclick="toggleFavorite(this, '2509.10250v1', 'GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250912248v2-humor-in-pixels-benchmarking-large-multimodal-models-understanding-o.html">Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics</a></td>
  <td>æå‡ºPixelHumoråŸºå‡†æ•°æ®é›†ï¼Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¯¹åœ¨çº¿æ¼«ç”»å¹½é»˜çš„ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">HuMoR</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.12248v2" data-paper-url="./papers/250912248v2-humor-in-pixels-benchmarking-large-multimodal-models-understanding-o.html" onclick="toggleFavorite(this, '2509.12248v2', 'Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250909935v1-scoda-self-supervised-continual-domain-adaptation.html">SCoDA: Self-supervised Continual Domain Adaptation</a></td>
  <td>æå‡ºSCoDAï¼Œé€šè¿‡è‡ªç›‘ç£å’Œå‡ ä½•æµå½¢å¯¹é½å®ç°å…æºæŒç»­é¢†åŸŸè‡ªé€‚åº”ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09935v1" data-paper-url="./papers/250909935v1-scoda-self-supervised-continual-domain-adaptation.html" onclick="toggleFavorite(this, '2509.09935v1', 'SCoDA: Self-supervised Continual Domain Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250910408v1-multimodal-sam-adapter-for-semantic-segmentation.html">Multimodal SAM-adapter for Semantic Segmentation</a></td>
  <td>æå‡ºMM SAM-adapterï¼Œç”¨äºæå‡å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10408v1" data-paper-url="./papers/250910408v1-multimodal-sam-adapter-for-semantic-segmentation.html" onclick="toggleFavorite(this, '2509.10408v1', 'Multimodal SAM-adapter for Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250910241v2-on-the-geometric-accuracy-of-implicit-and-primitive-based-representa.html">On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints</a></td>
  <td>é’ˆå¯¹ç©ºé—´æœºå™¨äººåº”ç”¨ï¼Œå¯¹æ¯”éšå¼ä¸æ˜¾å¼æ–°è§†è§’åˆæˆæ–¹æ³•çš„å‡ ä½•ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10241v2" data-paper-url="./papers/250910241v2-on-the-geometric-accuracy-of-implicit-and-primitive-based-representa.html" onclick="toggleFavorite(this, '2509.10241v2', 'On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250910651v2-usctnet-a-deep-unfolding-nuclear-norm-optimization-solver-for-physic.html">USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction</a></td>
  <td>USCTNetï¼šç”¨äºç‰©ç†ä¸€è‡´æ€§é«˜å…‰è°±å›¾åƒé‡å»ºçš„æ·±åº¦å±•å¼€æ ¸èŒƒæ•°ä¼˜åŒ–æ±‚è§£å™¨</td>
  <td class="tags-cell"><span class="paper-tag">HSI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10651v2" data-paper-url="./papers/250910651v2-usctnet-a-deep-unfolding-nuclear-norm-optimization-solver-for-physic.html" onclick="toggleFavorite(this, '2509.10651v2', 'USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)