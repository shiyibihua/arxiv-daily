---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-09
---

# cs.CVï¼ˆ2025-10-09ï¼‰

ğŸ“Š å…± **50** ç¯‡è®ºæ–‡
 | ğŸ”— **12** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (14 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ğŸ”—5)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251008565v1-navil-rethinking-scaling-properties-of-native-multimodal-large-langu.html">NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints</a></td>
  <td>NaViLï¼šæ•°æ®çº¦æŸä¸‹åŸç”Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç¼©æ”¾ç‰¹æ€§çš„å†æ€è€ƒ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08565v1" onclick="toggleFavorite(this, '2510.08565v1', 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251008003v1-cir-cot-towards-interpretable-composed-image-retrieval-via-end-to-en.html">CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning</a></td>
  <td>æå‡ºCIR-CoTï¼Œé€šè¿‡ç«¯åˆ°ç«¯æ€ç»´é“¾æ¨ç†å®ç°å¯è§£é‡Šçš„ç»„åˆå›¾åƒæ£€ç´¢</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08003v1" onclick="toggleFavorite(this, '2510.08003v1', 'CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251008759v1-bear-benchmarking-and-enhancing-multimodal-language-models-for-atomi.html">BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities</a></td>
  <td>BEARï¼šåŸå­å…·èº«èƒ½åŠ›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸å¢å¼º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08759v1" onclick="toggleFavorite(this, '2510.08759v1', 'BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251008673v1-thinking-with-camera-a-unified-multimodal-model-for-camera-centric-u.html">Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation</a></td>
  <td>Puffinï¼šæå‡ºç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°ç›¸æœºè§†è§’çš„ç†è§£ä¸ç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08673v1" onclick="toggleFavorite(this, '2510.08673v1', 'Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251007636v1-pit-qmm-a-large-multimodal-model-for-no-reference-point-cloud-qualit.html">PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment</a></td>
  <td>æå‡ºPIT-QMMï¼Œä¸€ç§ç”¨äºæ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07636v1" onclick="toggleFavorite(this, '2510.07636v1', 'PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251008508v1-moa-vr-a-mixture-of-agents-system-towards-all-in-one-video-restorati.html">MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration</a></td>
  <td>æå‡ºMoA-VRï¼Œä¸€ä¸ªæ··åˆAgentçš„é€šç”¨è§†é¢‘ä¿®å¤ç³»ç»Ÿï¼Œæœ‰æ•ˆå¤„ç†å¤æ‚é€€åŒ–ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08508v1" onclick="toggleFavorite(this, '2510.08508v1', 'MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251008485v1-instructx-towards-unified-visual-editing-with-mllm-guidance.html">InstructX: Towards Unified Visual Editing with MLLM Guidance</a></td>
  <td>InstructXï¼šåŸºäºMLLMæŒ‡å¯¼çš„ç»Ÿä¸€è§†è§‰ç¼–è¾‘æ¡†æ¶ï¼Œå®ç°å›¾åƒå’Œè§†é¢‘ç¼–è¾‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08485v1" onclick="toggleFavorite(this, '2510.08485v1', 'InstructX: Towards Unified Visual Editing with MLLM Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251008482v2-the-visual-iconicity-challenge-evaluating-vision-language-models-on-.html">The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping</a></td>
  <td>æå‡ºè§†è§‰æ ‡å¿—æ€§æŒ‘æˆ˜ï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ‰‹è¯­å½¢å¼-æ„ä¹‰æ˜ å°„ä¸Šçš„èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08482v2" onclick="toggleFavorite(this, '2510.08482v2', 'The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251008377v2-univideo-unified-understanding-generation-and-editing-for-videos.html">UniVideo: Unified Understanding, Generation, and Editing for Videos</a></td>
  <td>UniVideoï¼šç»Ÿä¸€è§†é¢‘ç†è§£ã€ç”Ÿæˆä¸ç¼–è¾‘çš„å¤šæ¨¡æ€æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08377v2" onclick="toggleFavorite(this, '2510.08377v2', 'UniVideo: Unified Understanding, Generation, and Editing for Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251008818v1-d-code-scaling-image-pretrained-vlms-to-video-via-dynamic-compressio.html">D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition</a></td>
  <td>D-CoDeï¼šé€šè¿‡åŠ¨æ€å‹ç¼©å’Œé—®é¢˜åˆ†è§£ï¼Œå°†å›¾åƒé¢„è®­ç»ƒçš„VLMæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08818v1" onclick="toggleFavorite(this, '2510.08818v1', 'D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251008551v1-artdeco-towards-efficient-and-high-fidelity-on-the-fly-3d-reconstruc.html">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a></td>
  <td>ARTDECOï¼šåŸºäºç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºçš„é«˜æ•ˆé«˜ä¿çœŸå³æ—¶3Dé‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08551v1" onclick="toggleFavorite(this, '2510.08551v1', 'ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251008510v1-to-sink-or-not-to-sink-visual-information-pathways-in-large-vision-l.html">To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</a></td>
  <td>é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè®ºæ–‡æå‡ºåˆ©ç”¨ViTæ³¨æ„åŠ›æ±‡èšå¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08510v1" onclick="toggleFavorite(this, '2510.08510v1', 'To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251008138v1-improving-temporal-understanding-logic-consistency-in-video-language.html">Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement</a></td>
  <td>æå‡ºæ—¶åºæ¡ä»¶æ³¨æ„åŠ›é”åŒ–(TCAS)æ–¹æ³•ï¼Œæå‡è§†é¢‘è¯­è¨€æ¨¡å‹æ—¶åºç†è§£é€»è¾‘ä¸€è‡´æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08138v1" onclick="toggleFavorite(this, '2510.08138v1', 'Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251007839v1-aligngs-aligning-geometry-and-semantics-for-robust-indoor-reconstruc.html">AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views</a></td>
  <td>AlignGSï¼šå¯¹é½å‡ ä½•ä¸è¯­ä¹‰ï¼Œå®ç°ç¨€ç–è§†è§’ä¸‹é²æ£’çš„å®¤å†…é‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07839v1" onclick="toggleFavorite(this, '2510.07839v1', 'AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/251008480v1-video-star-reinforcing-open-vocabulary-action-recognition-with-tools.html">Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools</a></td>
  <td>Video-STARï¼šåˆ©ç”¨å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08480v1" onclick="toggleFavorite(this, '2510.08480v1', 'Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251008849v1-folk-fast-open-vocabulary-3d-instance-segmentation-via-label-guided-.html">FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation</a></td>
  <td>æå‡ºFOLKï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„çŸ¥è¯†è’¸é¦å®ç°å¿«é€Ÿå¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08849v1" onclick="toggleFavorite(this, '2510.08849v1', 'FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251008540v2-mm-helix-boosting-multimodal-long-chain-reflective-reasoning-with-ho.html">MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization</a></td>
  <td>MM-HELIXï¼šé€šè¿‡æ•´ä½“å¹³å°å’Œè‡ªé€‚åº”æ··åˆç­–ç•¥ä¼˜åŒ–æå‡å¤šæ¨¡æ€é•¿é“¾åæ€æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08540v2" onclick="toggleFavorite(this, '2510.08540v2', 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251008567v3-matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning.html">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></td>
  <td>æå‡ºMATRIXæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€Agentè°ƒä¼˜å®ç°ç¨³å¥çš„å·¥å…·ä½¿ç”¨æ¨ç†</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08567v3" onclick="toggleFavorite(this, '2510.08567v3', 'MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251007944v2-cvd-storm-cross-view-video-diffusion-with-spatial-temporal-reconstru.html">CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving</a></td>
  <td>æå‡ºCVD-STORMï¼Œåˆ©ç”¨æ—¶ç©ºé‡å»ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªåŠ¨é©¾é©¶å¤šè§†è§’é•¿è§†é¢‘ï¼Œå¹¶å…·å¤‡4Dé‡å»ºèƒ½åŠ›ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07944v2" onclick="toggleFavorite(this, '2510.07944v2', 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251007915v1-marc-memory-augmented-rl-token-compression-for-efficient-video-under.html">MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding</a></td>
  <td>æå‡ºMARCï¼šä¸€ç§åŸºäºè®°å¿†å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„è§†é¢‘tokenå‹ç¼©æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆè§†é¢‘ç†è§£ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07915v1" onclick="toggleFavorite(this, '2510.07915v1', 'MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251008553v1-dream-to-recall-imagination-guided-experience-retrieval-for-memory-p.html">Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</a></td>
  <td>Memoirï¼šæå‡ºåŸºäºæƒ³è±¡å¼•å¯¼çš„ç»éªŒæ£€ç´¢æ–¹æ³•ï¼Œæå‡è®°å¿†æŒä¹…æ€§è§†è§‰è¯­è¨€å¯¼èˆªæ€§èƒ½ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08553v1" onclick="toggleFavorite(this, '2510.08553v1', 'Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251008442v2-gaze-on-the-prize-shaping-visual-attention-with-return-guided-contra.html">Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</a></td>
  <td>æå‡ºåŸºäºå›æŠ¥å¼•å¯¼å¯¹æ¯”å­¦ä¹ çš„è§†è§‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ ·æœ¬æ•ˆç‡</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08442v2" onclick="toggleFavorite(this, '2510.08442v2', 'Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251008531v1-spatialladder-progressive-training-for-spatial-reasoning-in-vision-l.html">SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</a></td>
  <td>SpatialLadderï¼šé€šè¿‡æ¸è¿›å¼è®­ç»ƒæå‡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08531v1" onclick="toggleFavorite(this, '2510.08531v1', 'SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251008398v2-videoverse-how-far-is-your-t2v-generator-from-a-world-model.html">VideoVerse: How Far is Your T2V Generator from a World Model?</a></td>
  <td>VideoVerseï¼šæ„å»ºæ›´å…¨é¢çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹è¯„ä¼°åŸºå‡†ï¼Œè¡¡é‡æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹çš„å·®è·</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08398v2" onclick="toggleFavorite(this, '2510.08398v2', 'VideoVerse: How Far is Your T2V Generator from a World Model?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251007953v1-simcast-enhancing-precipitation-nowcasting-with-short-to-long-term-k.html">SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation</a></td>
  <td>SimCastï¼šåˆ©ç”¨çŸ­æ—¶åˆ°é•¿æ—¶çŸ¥è¯†è’¸é¦å¢å¼ºé™æ°´ä¸´è¿‘é¢„æŠ¥</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07953v1" onclick="toggleFavorite(this, '2510.07953v1', 'SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251007878v3-flowlensing-simulating-gravitational-lensing-with-flow-matching.html">FlowLensing: Simulating Gravitational Lensing with Flow Matching</a></td>
  <td>FlowLensingï¼šåˆ©ç”¨Flow MatchingåŠ é€Ÿå¼•åŠ›é€é•œæ¨¡æ‹Ÿï¼ŒåŠ©åŠ›æš—ç‰©è´¨ç ”ç©¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07878v3" onclick="toggleFavorite(this, '2510.07878v3', 'FlowLensing: Simulating Gravitational Lensing with Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251008318v2-linvideo-a-post-training-framework-towards-on-attention-in-efficient.html">LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation</a></td>
  <td>LinVideoï¼šä¸€ç§åè®­ç»ƒæ¡†æ¶ï¼Œå®ç°é«˜æ•ˆè§†é¢‘ç”Ÿæˆä¸­O(n)å¤æ‚åº¦Attention</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08318v2" onclick="toggleFavorite(this, '2510.08318v2', 'LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251008096v1-efficient-label-refinement-for-face-parsing-under-extreme-poses-usin.html">Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting</a></td>
  <td>åˆ©ç”¨3Dé«˜æ–¯æº…å°„è¿›è¡Œäººè„¸è§£ææ ‡ç­¾ä¼˜åŒ–ï¼Œæå‡æç«¯å§¿æ€ä¸‹çš„è§£æç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08096v1" onclick="toggleFavorite(this, '2510.08096v1', 'Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251007830v1-prismgs-physically-grounded-anti-aliasing-for-high-fidelity-large-sc.html">PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting</a></td>
  <td>PrismGSï¼šé¢å‘å¤§è§„æ¨¡é«˜ä¿çœŸ3Dé«˜æ–¯æº…å°„çš„ç‰©ç†çº¦æŸæŠ—é”¯é½¿æ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07830v1" onclick="toggleFavorite(this, '2510.07830v1', 'PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251007752v2-degs-deformable-event-based-3d-gaussian-splatting-from-rgb-and-event.html">DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream</a></td>
  <td>æå‡ºDEGSï¼Œç»“åˆRGBå’Œäº‹ä»¶æµå®ç°å¯å˜å½¢çš„åŠ¨æ€3Dé«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07752v2" onclick="toggleFavorite(this, '2510.07752v2', 'DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251007856v2-xyzcylinder-towards-compatible-feed-forward-3d-gaussian-splatting-fo.html">XYZCylinder: Towards Compatible Feed-Forward 3D Gaussian Splatting for Driving Scenes via Unified Cylinder Lifting Method</a></td>
  <td>XYZCylinderï¼šé€šè¿‡ç»Ÿä¸€æŸ±é¢æå‡æ–¹æ³•å®ç°å…¼å®¹çš„é©¾é©¶åœºæ™¯3Dé«˜æ–¯æº…å°„</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07856v2" onclick="toggleFavorite(this, '2510.07856v2', 'XYZCylinder: Towards Compatible Feed-Forward 3D Gaussian Splatting for Driving Scenes via Unified Cylinder Lifting Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251008566v1-d2gs-depth-and-density-guided-gaussian-splatting-for-stable-and-accu.html">D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a></td>
  <td>D$^2$GSï¼šæ·±åº¦ä¸å¯†åº¦å¼•å¯¼çš„é«˜æ–¯æº…å°„ï¼Œç”¨äºç¨³å®šä¸”ç²¾ç¡®çš„ç¨€ç–è§†è§’é‡å»º</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08566v1" onclick="toggleFavorite(this, '2510.08566v1', 'D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251008278v2-a-multimodal-depth-aware-method-for-embodied-reference-understanding.html">A Multimodal Depth-Aware Method For Embodied Reference Understanding</a></td>
  <td>æå‡ºä¸€ç§å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥æ–¹æ³•ï¼Œç”¨äºå…·èº«å¼•ç”¨ç†è§£ä»»åŠ¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08278v2" onclick="toggleFavorite(this, '2510.08278v2', 'A Multimodal Depth-Aware Method For Embodied Reference Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251007817v1-an-end-to-end-room-geometry-constrained-depth-estimation-framework-f.html">An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images</a></td>
  <td>æå‡ºä¸€ç§å®¤å†…å…¨æ™¯å›¾åƒçš„ç«¯åˆ°ç«¯ã€åŸºäºæˆ¿é—´å‡ ä½•çº¦æŸçš„æ·±åº¦ä¼°è®¡æ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07817v1" onclick="toggleFavorite(this, '2510.07817v1', 'An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251007810v3-fmanet-a-novel-dual-phase-optical-flow-approach-with-fusion-motion-a.html">FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition</a></td>
  <td>æå‡ºFMANetï¼Œåˆ©ç”¨åŒé˜¶æ®µå…‰æµå’Œèåˆè¿åŠ¨æ³¨æ„åŠ›ç½‘ç»œæå‡å¾®è¡¨æƒ…è¯†åˆ«é²æ£’æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07810v3" onclick="toggleFavorite(this, '2510.07810v3', 'FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/251008575v2-resplat-learning-recurrent-gaussian-splats.html">ReSplat: Learning Recurrent Gaussian Splats</a></td>
  <td>æå‡ºReSplatï¼Œä¸€ç§è¿­ä»£ä¼˜åŒ–é«˜æ–¯splattingçš„å¾ªç¯æ¨¡å‹ï¼Œæå‡æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08575v2" onclick="toggleFavorite(this, '2510.08575v2', 'ReSplat: Learning Recurrent Gaussian Splats')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251007729v1-comgs-efficient-3d-object-scene-composition-via-surface-octahedral-p.html">ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes</a></td>
  <td>ComGSï¼šé€šè¿‡è¡¨é¢å…«é¢ä½“æ¢é’ˆå®ç°é«˜æ•ˆçš„3Dç‰©ä½“-åœºæ™¯åˆæˆ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07729v1" onclick="toggleFavorite(this, '2510.07729v1', 'ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/251007976v1-the-impact-of-abstract-and-object-tags-on-image-privacy-classificati.html">The impact of abstract and object tags on image privacy classification</a></td>
  <td>ç ”ç©¶æŠ½è±¡å’Œå¯¹è±¡æ ‡ç­¾å¯¹å›¾åƒéšç§åˆ†ç±»çš„å½±å“ï¼Œæ­ç¤ºæ ‡ç­¾ç±»å‹ä¸æ•°é‡çš„å…³é”®ä½œç”¨ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07976v1" onclick="toggleFavorite(this, '2510.07976v1', 'The impact of abstract and object tags on image privacy classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/251008559v1-scivideobench-benchmarking-scientific-video-reasoning-in-large-multi.html">SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</a></td>
  <td>SciVideoBenchï¼šæå‡ºç§‘å­¦è§†é¢‘æ¨ç†åŸºå‡†ï¼Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„è®¤çŸ¥èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08559v1" onclick="toggleFavorite(this, '2510.08559v1', 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/251008555v1-videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-p.html">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a></td>
  <td>VideoCanvasï¼šé€šè¿‡ä¸Šä¸‹æ–‡æ¡ä»¶åå°„å®ç°ä»»æ„æ—¶ç©ºè¡¥ä¸çš„ç»Ÿä¸€è§†é¢‘è¡¥å…¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08555v1" onclick="toggleFavorite(this, '2510.08555v1', 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/251008073v1-physics-driven-spatiotemporal-modeling-for-ai-generated-video-detect.html">Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection</a></td>
  <td>æå‡ºåŸºäºç‰©ç†é©±åŠ¨çš„æ—¶ç©ºå»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆè§†é¢‘</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08073v1" onclick="toggleFavorite(this, '2510.08073v1', 'Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/251007940v1-ttom-test-time-optimization-and-memorization-for-compositional-video.html">TTOM: Test-Time Optimization and Memorization for Compositional Video Generation</a></td>
  <td>æå‡ºTTOMï¼šä¸€ç§æµ‹è¯•æ—¶ä¼˜åŒ–ä¸è®°å¿†æ¡†æ¶ï¼Œç”¨äºç»„åˆè§†é¢‘ç”Ÿæˆã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07940v1" onclick="toggleFavorite(this, '2510.07940v1', 'TTOM: Test-Time Optimization and Memorization for Compositional Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/251008789v2-q-router-agentic-video-quality-assessment-with-expert-model-routing-.html">Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization</a></td>
  <td>Q-Routerï¼šåŸºäºä¸“å®¶æ¨¡å‹è·¯ç”±å’Œä¼ªå½±å®šä½çš„Agenticè§†é¢‘è´¨é‡è¯„ä¼°</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08789v2" onclick="toggleFavorite(this, '2510.08789v2', 'Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/251008316v1-unlocking-3d-affordance-segmentation-with-2d-semantic-knowledge.html">Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge</a></td>
  <td>æå‡ºCMATå’ŒCASTï¼Œåˆ©ç”¨2Dè¯­ä¹‰çŸ¥è¯†æå‡3Då¯ä¾›æ€§åˆ†å‰²æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08316v1" onclick="toggleFavorite(this, '2510.08316v1', 'Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/251008449v1-hierarchical-spatial-algorithms-for-high-resolution-image-quantizati.html">Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction</a></td>
  <td>æå‡ºä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒé‡åŒ–å’Œç‰¹å¾æå–çš„åˆ†å±‚ç©ºé—´ç®—æ³•æ¡†æ¶</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08449v1" onclick="toggleFavorite(this, '2510.08449v1', 'Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/251008543v1-videonorms-benchmarking-cultural-awareness-of-video-language-models.html">VideoNorms: Benchmarking Cultural Awareness of Video Language Models</a></td>
  <td>VideoNormsï¼šæ„å»ºè§†é¢‘è¯­è¨€æ¨¡å‹æ–‡åŒ–æ„è¯†åŸºå‡†ï¼Œæ­ç¤ºæ¨¡å‹åœ¨è·¨æ–‡åŒ–ç†è§£ä¸Šçš„ä¸è¶³ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08543v1" onclick="toggleFavorite(this, '2510.08543v1', 'VideoNorms: Benchmarking Cultural Awareness of Video Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/251007723v2-synchuman-synchronizing-2d-and-3d-generative-models-for-single-view-.html">SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction</a></td>
  <td>SyncHumanï¼šåŒæ­¥2Då’Œ3Dç”Ÿæˆæ¨¡å‹ï¼Œå®ç°å•è§†è§’äººä½“é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07723v2" onclick="toggleFavorite(this, '2510.07723v2', 'SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>48</td>
  <td><a href="./papers/251008157v1-beyond-textual-cot-interleaved-text-image-chains-with-deep-confidenc.html">Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing</a></td>
  <td>æå‡ºMUREæ¡†æ¶ï¼Œåˆ©ç”¨äº¤é”™æ–‡æœ¬-å›¾åƒé“¾å’Œæ·±åº¦ç½®ä¿¡æ¨ç†è¿›è¡Œå›¾åƒç¼–è¾‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08157v1" onclick="toggleFavorite(this, '2510.08157v1', 'Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>49</td>
  <td><a href="./papers/251008260v1-fine-grained-text-driven-dual-human-motion-generation-via-dynamic-hi.html">Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction</a></td>
  <td>æå‡ºFineDualï¼Œé€šè¿‡åŠ¨æ€åˆ†å±‚äº¤äº’ç”Ÿæˆç»†ç²’åº¦æ–‡æœ¬é©±åŠ¨çš„åŒäººè¿åŠ¨</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08260v1" onclick="toggleFavorite(this, '2510.08260v1', 'Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>50</td>
  <td><a href="./papers/251007828v3-mmhoi-modeling-complex-3d-multi-human-multi-object-interactions.html">MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions</a></td>
  <td>æå‡ºMMHOIæ•°æ®é›†å’ŒMMHOI-Netï¼Œç”¨äºå»ºæ¨¡å¤æ‚3Då¤šäººå¤šç‰©äº¤äº’</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07828v3" onclick="toggleFavorite(this, '2510.07828v3', 'MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)