---
layout: default
title: Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance
---

# Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05669" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05669v1</a>
  <a href="https://arxiv.org/pdf/2509.05669.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05669v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05669v1', 'Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weijie Shen, Xinrui Wang, Yuanqi Nie, Apiradee Boonmee

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAMVRæ¡†æ¶ä»¥è§£å†³å¤šè½®è§†è§‰æ–‡æœ¬æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè½®æ¨ç†` `è§†è§‰æ–‡æœ¬æ¨ç†` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `åŠ¨æ€è®°å¿†` `è‡ªé€‚åº”å¼•å¯¼` `è·¨æ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMså’ŒLVLMsåœ¨å¤šè½®äº¤äº’ä¸­ç¼ºä¹æ·±åº¦çš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œå¯¼è‡´æ¨ç†ä¸è¿è´¯å’Œä¿¡æ¯ä¸¢å¤±ã€‚
2. æå‡ºçš„CAMVRæ¡†æ¶é€šè¿‡å¼•å…¥VCMUå’ŒAVFGæœºåˆ¶ï¼ŒåŠ¨æ€ç®¡ç†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œå¢å¼ºå¤šè½®æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨VisDialã€A-OKVQAå’Œæ–°æå‡ºçš„MTIFæ•°æ®é›†ä¸Šï¼ŒCAMVRå®ç°äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å•è½®ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£å’Œå¤æ‚è§†è§‰æ¨ç†çš„å¤šè½®äº¤äº’ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¸¸å¯¼è‡´æ¨ç†ç¢ç‰‡åŒ–ã€ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œå¹»è§‰ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šè½®è§†è§‰æ¨ç†ï¼ˆCAMVRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºLVLMsåœ¨å¤šè½®è§†è§‰æ–‡æœ¬æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚CAMVRå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šè§†è§‰æ–‡æœ¬ä¸Šä¸‹æ–‡è®°å¿†å•å…ƒï¼ˆVCMUï¼‰å’Œè‡ªé€‚åº”è§†è§‰èšç„¦å¼•å¯¼ï¼ˆAVFGï¼‰æœºåˆ¶ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒCAMVRå±•ç°äº†å…¶åœ¨å¤šè½®æ¨ç†ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰LVLMsåœ¨å¤šè½®äº¤äº’ä¸­é¢ä¸´çš„ä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œæ¨ç†ç¢ç‰‡åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œå¸¸å¸¸æ— æ³•æœ‰æ•ˆæ•´åˆå†å²ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†ç»“æœä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAMVRæ¡†æ¶é€šè¿‡å¼•å…¥è§†è§‰æ–‡æœ¬ä¸Šä¸‹æ–‡è®°å¿†å•å…ƒï¼ˆVCMUï¼‰å’Œè‡ªé€‚åº”è§†è§‰èšç„¦å¼•å¯¼ï¼ˆAVFGï¼‰æœºåˆ¶ï¼ŒåŠ¨æ€ç®¡ç†å’Œè°ƒæ•´è§†è§‰ä¿¡æ¯çš„å…³æ³¨ç‚¹ï¼Œä»¥å¢å¼ºå¤šè½®æ¨ç†çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAMVRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬VCMUä½œä¸ºåŠ¨æ€è®°å¿†ç½‘ç»œï¼Œå­˜å‚¨æ¯è½®äº¤äº’çš„è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºï¼›AVFGæœºåˆ¶åˆ™æ ¹æ®VCMUçš„ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›ç„¦ç‚¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šVCMUå’ŒAVFGæ˜¯CAMVRçš„æ ¸å¿ƒåˆ›æ–°ï¼Œå‰è€…é€šè¿‡åŠ¨æ€è¯»å†™æœºåˆ¶ç®¡ç†è·¨æ¨¡æ€ä¿¡æ¯ï¼Œåè€…åˆ™ç¡®ä¿è§†è§‰ä¿¡æ¯çš„å…³æ³¨ç‚¹ä¸ä¸Šä¸‹æ–‡ç›¸å…³ï¼Œä»è€Œæå‡æ¨ç†çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒVCMUé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–è®°å¿†çš„å­˜å‚¨å’Œæ£€ç´¢æ•ˆç‡ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†å¤šè½®æ¨ç†çš„ç‰¹æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å†å²ä¸Šä¸‹æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCAMVRåœ¨å¤šè½®æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†é¢†å…ˆçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨VisDialæ•°æ®é›†ä¸Šï¼ŒCAMVRçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚è§†è§‰æ–‡æœ¬äº¤äº’ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è™šæ‹ŸåŠ©æ‰‹å’Œå¤šæ¨¡æ€äº¤äº’å¹³å°ç­‰ã€‚é€šè¿‡æå‡å¤šè½®è§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼ŒCAMVRèƒ½å¤Ÿåœ¨å¤æ‚åœºæ™¯ä¸­æä¾›æ›´å‡†ç¡®å’Œè¿è´¯çš„å“åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) excel in single-turn tasks but face significant challenges in multi-turn interactions requiring deep contextual understanding and complex visual reasoning, often leading to fragmented reasoning, context loss, and hallucinations. To address these limitations, we propose Context-Aware Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower LVLMs with robust and coherent multi-turn visual-textual inference capabilities. CAMVR introduces two key innovations: a Visual-Textual Context Memory Unit (VCMU), a dynamic read-write memory network that stores and manages critical visual features, textual semantic representations, and their cross-modal correspondences from each interaction turn; and an Adaptive Visual Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to dynamically adjust the visual encoder's attention to contextually relevant image regions. Our multi-level reasoning integration strategy ensures that response generation is deeply coherent with both current inputs and accumulated historical context. Extensive experiments on challenging datasets, including VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following (MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art performance.

