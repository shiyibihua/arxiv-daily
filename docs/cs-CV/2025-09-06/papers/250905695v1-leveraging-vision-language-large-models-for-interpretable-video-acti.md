---
layout: default
title: Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization
---

# Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05695" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05695v1</a>
  <a href="https://arxiv.org/pdf/2509.05695.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05695v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05695v1', 'Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingwei Peng, Zhixuan Qiu, Boyu Jin, Surasakdi Siripong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LVLM-VARï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€å¤§æ¨¡å‹å’Œè¯­ä¹‰æ ‡è®°å®ç°å¯è§£é‡Šçš„è§†é¢‘è¡Œä¸ºè¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘è¡Œä¸ºè¯†åˆ«` `è§†è§‰-è¯­è¨€å¤§æ¨¡å‹` `è¯­ä¹‰æ ‡è®°` `å¯è§£é‡Šæ€§` `åŠ¨ä½œå™è¿°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè¡Œä¸ºè¯†åˆ«æ–¹æ³•åœ¨å¤„ç†æ·±å±‚è¯­ä¹‰ç†è§£ã€å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç»†ç²’åº¦åŒºåˆ†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚
2. LVLM-VARé€šè¿‡VSTæ¨¡å—å°†è§†é¢‘è½¬æ¢ä¸ºè¯­ä¹‰åŠ¨ä½œæ ‡è®°ï¼Œå¹¶åˆ©ç”¨LVLMè¿›è¡ŒåŠ¨ä½œåˆ†ç±»å’Œè¯­ä¹‰æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLVLM-VARåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºLVLM-VARï¼Œä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€å¤§æ¨¡å‹(LVLM)åº”ç”¨äºè§†é¢‘è¡Œä¸ºè¯†åˆ«ï¼Œä»è€Œå¢å¼ºå‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯è§†é¢‘åˆ°è¯­ä¹‰æ ‡è®°(VST)æ¨¡å—ï¼Œå®ƒåˆ›æ–°æ€§åœ°å°†åŸå§‹è§†é¢‘åºåˆ—è½¬æ¢ä¸ºç¦»æ•£çš„ã€è¯­ä¹‰å’Œæ—¶é—´ä¸Šä¸€è‡´çš„â€œè¯­ä¹‰åŠ¨ä½œæ ‡è®°â€ï¼Œæœ‰æ•ˆåœ°æ„å»ºäº†LVLMå¯ç†è§£çš„â€œåŠ¨ä½œå™è¿°â€ã€‚è¿™äº›æ ‡è®°ä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç»“åˆï¼Œç„¶åç”±LoRAå¾®è°ƒçš„LVLMï¼ˆä¾‹å¦‚LLaVA-13Bï¼‰å¤„ç†ï¼Œä»¥å®ç°é²æ£’çš„åŠ¨ä½œåˆ†ç±»å’Œè¯­ä¹‰æ¨ç†ã€‚LVLM-VARåœ¨NTU RGB+Då’ŒNTU RGB+D 120ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›æˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨NTU RGB+D X-Subä¸Šè¾¾åˆ°94.1%ï¼Œåœ¨NTU RGB+D 120 X-Setä¸Šè¾¾åˆ°90.0%ï¼Œå¹¶ä¸”é€šè¿‡ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†é¢‘è¡Œä¸ºè¯†åˆ«æ–¹æ³•éš¾ä»¥æ•æ‰è§†é¢‘ä¸­çš„æ·±å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¹å¤æ‚ä¸Šä¸‹æ–‡çš„ç†è§£ä¸è¶³ï¼Œå¹¶ä¸”åœ¨ç»†ç²’åº¦åŠ¨ä½œåŒºåˆ†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚è¿™äº›å±€é™æ€§å¯¼è‡´æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–çš„è§†é¢‘æ•°æ®æ—¶æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘è½¬æ¢ä¸ºä¸€ç³»åˆ—å…·æœ‰è¯­ä¹‰ä¿¡æ¯çš„ç¦»æ•£æ ‡è®°ï¼ˆsemantic tokensï¼‰ï¼Œç„¶ååˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€å¤§æ¨¡å‹ï¼ˆLVLMï¼‰ç†è§£è¿™äº›æ ‡è®°ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œå¯è§£é‡Šçš„è§†é¢‘è¡Œä¸ºè¯†åˆ«ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­åˆ©ç”¨è¯­è¨€æ¨¡å‹ç†è§£æ–‡æœ¬åºåˆ—çš„æ€æƒ³ï¼Œå°†è§†é¢‘ç†è§£é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€ç†è§£é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLVLM-VARæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šè§†é¢‘åˆ°è¯­ä¹‰æ ‡è®°(VST)æ¨¡å—å’ŒLoRAå¾®è°ƒçš„LVLMã€‚VSTæ¨¡å—è´Ÿè´£å°†åŸå§‹è§†é¢‘åºåˆ—è½¬æ¢ä¸ºè¯­ä¹‰åŠ¨ä½œæ ‡è®°ã€‚è¿™äº›æ ‡è®°ä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸€èµ·è¾“å…¥åˆ°LoRAå¾®è°ƒçš„LVLMï¼ˆä¾‹å¦‚LLaVA-13Bï¼‰ä¸­ï¼ŒLVLMè´Ÿè´£è¿›è¡ŒåŠ¨ä½œåˆ†ç±»å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†è§†é¢‘åˆ°è¯­ä¹‰æ ‡è®°(VST)æ¨¡å—ï¼Œå®ƒèƒ½å¤Ÿå°†åŸå§‹è§†é¢‘åºåˆ—è½¬æ¢ä¸ºç¦»æ•£çš„ã€è¯­ä¹‰å’Œæ—¶é—´ä¸Šä¸€è‡´çš„â€œè¯­ä¹‰åŠ¨ä½œæ ‡è®°â€ã€‚ä¸ç›´æ¥å°†è§†é¢‘å¸§è¾“å…¥åˆ°æ¨¡å‹ä¸­ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æå–è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ºLVLMå¯ä»¥ç†è§£çš„å½¢å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šVSTæ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†æè¿°ã€‚LoRAå¾®è°ƒçš„LVLMä½¿ç”¨äº†LLaVA-13Bæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘è®¡ç®—æˆæœ¬å’Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LVLM-VARåœ¨NTU RGB+D X-Subæ•°æ®é›†ä¸Šå–å¾—äº†94.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨NTU RGB+D 120 X-Setæ•°æ®é›†ä¸Šå–å¾—äº†90.0%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ›´ç›´è§‚çš„ç†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LVLM-VARåœ¨è§†é¢‘ç›‘æ§ã€æ™ºèƒ½å®‰é˜²ã€äººæœºäº¤äº’ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜è§†é¢‘è¡Œä¸ºè¯†åˆ«çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºæå‡ç›¸å…³ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ›´å¤æ‚çš„è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘æ‘˜è¦ã€è§†é¢‘é—®ç­”ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent "semantic action tokens," effectively crafting an "action narrative" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions.

