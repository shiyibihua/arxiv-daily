---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-06
---

# cs.CVï¼ˆ2025-09-06ï¼‰

ğŸ“Š å…± **8** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250905614v1-specprune-vla-accelerating-vision-language-action-models-via-action-.html">SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</a></td>
  <td>SpecPrune-VLAï¼šé€šè¿‡åŠ¨ä½œæ„ŸçŸ¥è‡ªé€‚åº”æ¨æµ‹å‰ªæåŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">OpenVLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05614v1" data-paper-url="./papers/250905614v1-specprune-vla-accelerating-vision-language-action-models-via-action-.html" onclick="toggleFavorite(this, '2509.05614v1', 'SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250907010v1-human-in-the-loop-quantitative-evaluation-of-3d-models-generation-by.html">Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models</a></td>
  <td>æå‡ºäººæœºé—­ç¯æ¡†æ¶ï¼Œé‡åŒ–è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆ3Dæ¨¡å‹è´¨é‡ï¼ŒåŠ é€ŸCADè®¾è®¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.07010v1" data-paper-url="./papers/250907010v1-human-in-the-loop-quantitative-evaluation-of-3d-models-generation-by.html" onclick="toggleFavorite(this, '2509.07010v1', 'Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250905773v1-pictobi-20k-unveiling-large-multimodal-models-in-visual-decipherment.html">PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters</a></td>
  <td>æå‡ºPictOBI-20kæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”²éª¨æ–‡è±¡å½¢æ–‡å­—è§†è§‰é‡Šè¯»ä¸­çš„èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05773v1" data-paper-url="./papers/250905773v1-pictobi-20k-unveiling-large-multimodal-models-in-visual-decipherment.html" onclick="toggleFavorite(this, '2509.05773v1', 'PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250905669v1-context-aware-multi-turn-visual-textual-reasoning-in-lvlms-via-dynam.html">Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance</a></td>
  <td>æå‡ºCAMVRæ¡†æ¶ä»¥è§£å†³å¤šè½®è§†è§‰æ–‡æœ¬æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05669v1" data-paper-url="./papers/250905669v1-context-aware-multi-turn-visual-textual-reasoning-in-lvlms-via-dynam.html" onclick="toggleFavorite(this, '2509.05669v1', 'Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250905751v1-unleashing-hierarchical-reasoning-an-llm-driven-framework-for-traini.html">Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation</a></td>
  <td>æå‡ºPARSE-VOSä»¥è§£å†³åŠ¨æ€è§†é¢‘ç‰©ä½“åˆ†å‰²ä¸­çš„è¯­è¨€ä¸è§†è§‰å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05751v1" data-paper-url="./papers/250905751v1-unleashing-hierarchical-reasoning-an-llm-driven-framework-for-traini.html" onclick="toggleFavorite(this, '2509.05751v1', 'Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250905695v1-leveraging-vision-language-large-models-for-interpretable-video-acti.html">Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization</a></td>
  <td>LVLM-VARï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€å¤§æ¨¡å‹å’Œè¯­ä¹‰æ ‡è®°å®ç°å¯è§£é‡Šçš„è§†é¢‘è¡Œä¸ºè¯†åˆ«</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05695v1" data-paper-url="./papers/250905695v1-leveraging-vision-language-large-models-for-interpretable-video-acti.html" onclick="toggleFavorite(this, '2509.05695v1', 'Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/250905604v1-language-guided-recursive-spatiotemporal-graph-modeling-for-video-su.html">Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</a></td>
  <td>æå‡ºåŸºäºè¯­è¨€å¼•å¯¼çš„é€’å½’æ—¶ç©ºå›¾ç½‘ç»œVideoGraphï¼Œç”¨äºè§†é¢‘æ‘˜è¦ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05604v1" data-paper-url="./papers/250905604v1-language-guided-recursive-spatiotemporal-graph-modeling-for-video-su.html" onclick="toggleFavorite(this, '2509.05604v1', 'Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250905645v1-stereovision-image-processing-for-planetary-navigation-maps-with-sem.html">Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation</a></td>
  <td>æå‡ºåŸºäºåŠå…¨å±€åŒ¹é…å’Œè¶…åƒç´ åˆ†å‰²çš„ç«‹ä½“è§†è§‰è¡Œæ˜Ÿå¯¼èˆªåœ°å›¾æ–¹æ³•ï¼Œæå‡ç«æ˜Ÿæ¢æµ‹ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.05645v1" data-paper-url="./papers/250905645v1-stereovision-image-processing-for-planetary-navigation-maps-with-sem.html" onclick="toggleFavorite(this, '2509.05645v1', 'Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)