---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-13
---

# cs.CVï¼ˆ2025-12-13ï¼‰

ğŸ“Š å…± **10** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (6)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251212425v1-bokehdepth-enhancing-monocular-depth-estimation-through-bokeh-genera.html">BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation</a></td>
  <td>æå‡ºBokehDepthï¼Œåˆ©ç”¨æ•£ç„¦ä½œä¸ºè¾…åŠ©å‡ ä½•çº¿ç´¢ï¼Œæå‡å•ç›®æ·±åº¦ä¼°è®¡çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12425v1" onclick="toggleFavorite(this, '2512.12425v1', 'BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251212165v2-audio-visual-camera-pose-estimation-with-passive-scene-sounds-and-in.html">Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video</a></td>
  <td>æå‡ºä¸€ç§éŸ³è§†é¢‘èåˆçš„ç›¸æœºä½å§¿ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨åœºæ™¯å£°éŸ³å¢å¼ºè§†è§‰ä¿¡æ¯ï¼Œæå‡é‡å¤–è§†é¢‘çš„é²æ£’æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12165v2" onclick="toggleFavorite(this, '2512.12165v2', 'Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251212307v1-mrd-using-physically-based-differentiable-rendering-to-probe-vision-.html">MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</a></td>
  <td>æå‡ºMRDï¼Œåˆ©ç”¨å¯å¾®æ¸²æŸ“æ¢ç©¶è§†è§‰æ¨¡å‹å¯¹3Dåœºæ™¯çš„ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12307v1" onclick="toggleFavorite(this, '2512.12307v1', 'MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251212410v1-a-graph-attention-network-based-framework-for-reconstructing-missing.html">A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</a></td>
  <td>æå‡ºåŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„LiDARç¼ºå¤±æ³¢æŸé‡å»ºæ¡†æ¶ï¼Œæå‡è‡ªåŠ¨é©¾é©¶ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12410v1" onclick="toggleFavorite(this, '2512.12410v1', 'A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251212205v1-a-multi-year-urban-streetlight-imagery-dataset-for-visual-monitoring.html">A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection</a></td>
  <td>å‘å¸ƒåŸå¸‚è¡—é“ç…§æ˜å¤šå¹´åº¦å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè§†è§‰ç›‘æ§å’Œæ—¶ç©ºæ¼‚ç§»æ£€æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12205v1" onclick="toggleFavorite(this, '2512.12205v1', 'A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251212193v1-smrabooth-subject-and-motion-representation-alignment-for-customized.html">SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation</a></td>
  <td>SMRABoothï¼šé€šè¿‡ä¸»ä½“ä¸è¿åŠ¨è¡¨å¾å¯¹é½å®ç°å®šåˆ¶åŒ–è§†é¢‘ç”Ÿæˆ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12193v1" onclick="toggleFavorite(this, '2512.12193v1', 'SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>7</td>
  <td><a href="./papers/251212386v1-speedrunning-imagenet-diffusion.html">Speedrunning ImageNet Diffusion</a></td>
  <td>æå‡ºSR-DiTï¼Œé€šè¿‡é›†æˆå¤šç§ä¼˜åŒ–ç­–ç•¥åŠ é€ŸImageNetæ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12386v1" onclick="toggleFavorite(this, '2512.12386v1', 'Speedrunning ImageNet Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251212208v1-a-hybrid-deep-learning-framework-for-emotion-recognition-in-children.html">A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction</a></td>
  <td>æå‡ºä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«è‡ªé—­ç—‡å„¿ç«¥åœ¨NAOæœºå™¨äººäº¤äº’ä¸­çš„æƒ…ç»ªã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12208v1" onclick="toggleFavorite(this, '2512.12208v1', 'A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/251212430v1-endless-world-real-time-3d-aware-long-video-generation.html">Endless World: Real-Time 3D-Aware Long Video Generation</a></td>
  <td>Endless Worldï¼šå®æ—¶3Dæ„ŸçŸ¥æ— é™é•¿è§†é¢‘ç”Ÿæˆæ¡†æ¶</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12430v1" onclick="toggleFavorite(this, '2512.12430v1', 'Endless World: Real-Time 3D-Aware Long Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/251212206v1-alert-open-dataset-and-input-size-agnostic-vision-transformer-for-dr.html">ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</a></td>
  <td>æå‡ºISA-ViTå’ŒALERTæ•°æ®é›†ï¼Œç”¨äºè§£å†³åŸºäºIR-UWBé›·è¾¾çš„é©¾é©¶å‘˜è¡Œä¸ºè¯†åˆ«é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12206v1" onclick="toggleFavorite(this, '2512.12206v1', 'ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)