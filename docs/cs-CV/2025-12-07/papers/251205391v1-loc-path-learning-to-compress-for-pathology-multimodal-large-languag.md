---
layout: default
title: LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models
---

# LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models

**arXiv**: [2512.05391v1](https://arxiv.org/abs/2512.05391) | [PDF](https://arxiv.org/pdf/2512.05391.pdf)

**ä½œè€…**: Qingqiao Hu, Weimin Lyu, Meilong Xu, Kehan Qi, Xiaoling Hu, Saumya Gupta, Jiawei Zhou, Chao Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoC-Pathæ¡†æž¶ä»¥é™ä½Žç—…ç†å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è®¡ç®—æˆæœ¬**

**å…³é”®è¯**: `å…¨åˆ‡ç‰‡å›¾åƒç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ä»¤ç‰ŒåŽ‹ç¼©` `è®¡ç®—æ•ˆçŽ‡` `ç—…ç†å­¦åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨åˆ‡ç‰‡å›¾åƒç†è§£å› åƒç´ è§„æ¨¡å¤§å’Œè¯Šæ–­ç›¸å…³åŒºåŸŸç¨€ç–å¯¼è‡´è®¡ç®—æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ç¨€ç–ä»¤ç‰Œåˆå¹¶å™¨å’ŒMAEé¢„è®­ç»ƒé‡é‡‡æ ·å™¨åŽ‹ç¼©å†—ä½™ä»¤ç‰Œï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›è·¯ç”±é€‚é…å™¨é«˜æ•ˆé›†æˆè§†è§‰ä¸Žè¯­è¨€æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ€§èƒ½å¯æ¯”çŽ°æœ‰æ–¹æ³•çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.

