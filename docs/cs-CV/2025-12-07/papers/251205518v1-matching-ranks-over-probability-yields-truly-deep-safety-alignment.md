---
layout: default
title: Matching Ranks Over Probability Yields Truly Deep Safety Alignment
---

# Matching Ranks Over Probability Yields Truly Deep Safety Alignment

**arXiv**: [2512.05518v1](https://arxiv.org/abs/2512.05518) | [PDF](https://arxiv.org/pdf/2512.05518.pdf)

**ä½œè€…**: Jason Vega, Gagandeep Singh

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRESTOæ–¹æ³•ä»¥è§£å†³RAPæ”»å‡»ä¸‹LLMå®‰å…¨å¯¹é½çš„æµ…å±‚æ¼æ´ž**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨å¯¹é½` `é¢„å¡«å……æ”»å‡»` `ä»¤ç‰ŒæŽ’ååŒ¹é…` `æ³¨æ„åŠ›æ­£åˆ™åŒ–` `RAPæ”»å‡»` `æ•°æ®å¢žå¼ºé˜²å¾¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ•°æ®å¢žå¼ºé˜²å¾¡åœ¨RAPæ”»å‡»ä¸‹ä»æš´éœ²æµ…å±‚å®‰å…¨å¯¹é½ï¼Œå› SFTç›®æ ‡æ˜“è¢«â€˜åšå¼ˆâ€™å¯¼è‡´æœ‰å®³ä»¤ç‰ŒæŽ’åé«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åŒ¹é…ç›®æ ‡åˆ†å¸ƒçš„ä»¤ç‰ŒæŽ’åè€Œéžæ¦‚çŽ‡ï¼Œæå‡ºPRESTOæ–¹æ³•ï¼Œæ­£åˆ™åŒ–æœ‰å®³é¢„å¡«å……ä»¤ç‰Œçš„æ³¨æ„åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªå¼€æºLLMä¸Šï¼ŒPRESTOä½¿RAPæ”»å‡»ä¸‹çš„StrongREJECTåˆ†æ•°å¹³å‡æå‡é«˜è¾¾4.7å€ï¼Œå¯¹æ¨¡åž‹æ•ˆç”¨å½±å“ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the "deep" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability "harmful" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability "refusal" tokens). We argue that this vulnerability is enabled due to the "gaming" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.

