---
layout: default
title: VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation
---

# VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation

**arXiv**: [2512.05524v1](https://arxiv.org/abs/2512.05524) | [PDF](https://arxiv.org/pdf/2512.05524.pdf)

**ä½œè€…**: Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVOST-SGGæ¡†æž¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡åž‹å¢žå¼ºå•é˜¶æ®µæ—¶ç©ºåœºæ™¯å›¾ç”Ÿæˆï¼Œè§£å†³æŸ¥è¯¢è¯­ä¹‰ç¼ºå¤±å’Œè°“è¯åˆ†ç±»å•æ¨¡æ€é™åˆ¶é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ—¶ç©ºåœºæ™¯å›¾ç”Ÿæˆ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å•é˜¶æ®µæ£€æµ‹` `å¤šæ¨¡æ€èžåˆ` `æŸ¥è¯¢åˆå§‹åŒ–` `è§†é¢‘ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å•é˜¶æ®µST-SGGæ¨¡åž‹å­˜åœ¨æŸ¥è¯¢è¯­ä¹‰æœªåˆå§‹åŒ–åŠè°“è¯åˆ†ç±»ä»…ä¾èµ–è§†è§‰ç‰¹å¾çš„å±€é™æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŒæºæŸ¥è¯¢åˆå§‹åŒ–ç­–ç•¥å’Œå¤šæ¨¡æ€ç‰¹å¾åº“ï¼Œèžåˆè§†è§‰ã€æ–‡æœ¬å’Œç©ºé—´çº¿ç´¢ä»¥æå‡æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Action Genomeæ•°æ®é›†ä¸Šå®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†VLMé›†æˆå’Œå¤šæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.

