---
layout: default
title: Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features
---

# Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features

**arXiv**: [2512.05669v1](https://arxiv.org/abs/2512.05669) | [PDF](https://arxiv.org/pdf/2512.05669.pdf)

**ä½œè€…**: Talha Enes Koksal, Abdurrahman Gumus

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ·±åº¦å­¦ä¹ å’Œå‡ ä½•ç‰¹å¾çš„å®žæ—¶åºåˆ—é¢éƒ¨è¡¨æƒ…åˆ†æžæ–¹æ³•ï¼Œç”¨äºŽå¢žå¼ºäººæœºäº¤äº’å’Œæƒ…æ„Ÿæ„ŸçŸ¥ç³»ç»Ÿã€‚**

**å…³é”®è¯**: `é¢éƒ¨è¡¨æƒ…è¯†åˆ«` `å‡ ä½•ç‰¹å¾æå–` `æ—¶åºåŠ¨æ€åˆ†æž` `ConvLSTMç½‘ç»œ` `å®žæ—¶å¤„ç†` `äººæœºäº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå®žæ—¶åºåˆ—é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼Œéœ€å¤„ç†è¡¨æƒ…çš„èµ·å§‹ã€é¡¶ç‚¹å’Œç»“æŸé˜¶æ®µã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨MediaPipe FaceMeshæå–é¢éƒ¨å…³é”®ç‚¹ï¼Œè®¡ç®—æ¬§æ°è·ç¦»å’Œè§’åº¦ä½œä¸ºå‡ ä½•ç‰¹å¾ï¼Œç»“åˆConvLSTM1Dç½‘ç»œåˆ†æžæ—¶åºåŠ¨æ€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CK+ã€Oulu-CASIAã€MMIæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°93%ã€79%ã€77%ã€68%çš„å‡†ç¡®çŽ‡ï¼Œå®žæ—¶å¤„ç†çº¦165å¸§/ç§’ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Facial expression recognition is a crucial component in enhancing human-computer interaction and developing emotion-aware systems. Real-time detection and interpretation of facial expressions have become increasingly important for various applications, from user experience personalization to intelligent surveillance systems. This study presents a novel approach to real-time sequential facial expression recognition using deep learning and geometric features. The proposed method utilizes MediaPipe FaceMesh for rapid and accurate facial landmark detection. Geometric features, including Euclidean distances and angles, are extracted from these landmarks. Temporal dynamics are incorporated by analyzing feature differences between consecutive frames, enabling the detection of onset, apex, and offset phases of expressions. For classification, a ConvLSTM1D network followed by multilayer perceptron blocks is employed. The method's performance was evaluated on multiple publicly available datasets, including CK+, Oulu-CASIA (VIS and NIR), and MMI. Accuracies of 93%, 79%, 77%, and 68% were achieved respectively. Experiments with composite datasets were also conducted to assess the model's generalization capabilities. The approach demonstrated real-time applicability, processing approximately 165 frames per second on consumer-grade hardware. This research contributes to the field of facial expression analysis by providing a fast, accurate, and adaptable solution. The findings highlight the potential for further advancements in emotion-aware technologies and personalized user experiences, paving the way for more sophisticated human-computer interaction systems. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/facial-expression-analysis.

