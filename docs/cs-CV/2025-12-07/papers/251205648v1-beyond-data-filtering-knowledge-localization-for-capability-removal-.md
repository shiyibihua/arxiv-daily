---
layout: default
title: Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs
---

# Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs

**arXiv**: [2512.05648v1](https://arxiv.org/abs/2512.05648) | [PDF](https://arxiv.org/pdf/2512.05648.pdf)

**ä½œè€…**: Igor Shilov, Alex Cloud, Aryo Pradipta Gema, Jacob Goldman-Wetzler, Nina Panickssery, Henry Sleight, Erik Jones, Cem Anil

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€‰æ‹©æ€§æ¢¯åº¦æŽ©ç ä»¥å¢žå¼ºå¤§è¯­è¨€æ¨¡åž‹åœ¨æ ‡ç­¾å™ªå£°ä¸‹çš„èƒ½åŠ›ç§»é™¤é²æ£’æ€§**

**å…³é”®è¯**: `èƒ½åŠ›ç§»é™¤` `æ¢¯åº¦è·¯ç”±` `æ ‡ç­¾å™ªå£°é²æ£’æ€§` `é¢„è®­ç»ƒå®‰å…¨` `å‚æ•°å®šä½` `å¯¹æŠ—å¾®è°ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•°æ®è¿‡æ»¤åœ¨é¢„è®­ç»ƒæ—¶é¢ä¸´æ ‡ç­¾å™ªå£°æŒ‘æˆ˜ï¼Œå¯èƒ½å¯¼è‡´å±é™©èƒ½åŠ›æ®‹ç•™
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡é€‰æ‹©æ€§æ¢¯åº¦æŽ©ç å°†ç›®æ ‡çŸ¥è¯†å®šä½åˆ°ä¸“ç”¨å‚æ•°ï¼Œå®žçŽ°æ›´ç²¾ç¡®çš„ç§»é™¤
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŒè¯­å’Œç”Ÿç‰©å­¦çŸ¥è¯†ç§»é™¤ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•æä¾›æ›´å¥½çš„ä¿ç•™/é—å¿˜æƒè¡¡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

