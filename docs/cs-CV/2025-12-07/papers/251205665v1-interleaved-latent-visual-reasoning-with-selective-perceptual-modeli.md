---
layout: default
title: Interleaved Latent Visual Reasoning with Selective Perceptual Modeling
---

# Interleaved Latent Visual Reasoning with Selective Perceptual Modeling

**arXiv**: [2512.05665v1](https://arxiv.org/abs/2512.05665) | [PDF](https://arxiv.org/pdf/2512.05665.pdf)

**ä½œè€…**: Shuai Dong, Siyuan Wang, Xingyu Liu, Zhongyu Wei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºILVRæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ä¸­è§†è§‰æŽ¨ç†çš„è®¡ç®—æˆæœ¬ä¸Žæ„ŸçŸ¥ç²¾åº¦æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ½œåœ¨è§†è§‰æŽ¨ç†` `äº¤é”™æŽ¨ç†` `é€‰æ‹©æ€§æ„ŸçŸ¥å»ºæ¨¡` `åŠ¨é‡è’¸é¦` `åŠ¨æ€çŠ¶æ€æ¼”åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ½œåœ¨è§†è§‰æŽ¨ç†æ–¹æ³•åœ¨è®¡ç®—æ•ˆçŽ‡ä¸Žç²¾ç¡®æ„ŸçŸ¥å»ºæ¨¡é—´å­˜åœ¨çŸ›ç›¾ï¼Œå¯¼è‡´åŠ¨æ€é—®é¢˜å»ºæ¨¡å›°éš¾
2. æ–¹æ³•è¦ç‚¹ï¼šILVRé€šè¿‡äº¤é”™æ–‡æœ¬ç”Ÿæˆä¸Žæ½œåœ¨è§†è§‰è¡¨ç¤ºï¼Œç»“åˆåŠ¨é‡æ•™å¸ˆæ¨¡åž‹é€‰æ‹©æ€§è’¸é¦ç‰¹å¾ï¼Œå®žçŽ°è‡ªé€‚åº”è§†è§‰ä¿¡å·ç”Ÿæˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šé¡¹å¤šæ¨¡æ€æŽ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒILVRæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆè¿žæŽ¥ç»†ç²’åº¦æ„ŸçŸ¥ä¸Žåºåˆ—æŽ¨ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.

