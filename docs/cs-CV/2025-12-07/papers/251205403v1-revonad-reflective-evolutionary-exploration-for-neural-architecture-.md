---
layout: default
title: RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design
---

# RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design

**arXiv**: [2512.05403v1](https://arxiv.org/abs/2512.05403) | [PDF](https://arxiv.org/pdf/2512.05403.pdf)

**ä½œè€…**: Gyusam Chang, Jeongyoon Yoon, Shin han yi, JaeHyeok Lee, Sujin Jang, Sangpil Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRevoNADä»¥è§£å†³LLMé©±åŠ¨ç¥žç»æž¶æž„è®¾è®¡ä¸­çš„åé¦ˆå¼•å¯¼å’Œæ¨¡å¼å´©æºƒé—®é¢˜**

**å…³é”®è¯**: `ç¥žç»æž¶æž„è®¾è®¡` `å¤§è¯­è¨€æ¨¡åž‹` `è¿›åŒ–ç®—æ³•` `åé¦ˆå¯¹é½` `å¤šç›®æ ‡ä¼˜åŒ–` `è®¡ç®—æœºè§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMé©±åŠ¨çš„ç¥žç»æž¶æž„è®¾è®¡å­˜åœ¨ç¦»æ•£ã€ä¸å¯å¾®çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯¼è‡´åé¦ˆéš¾ä»¥å¹³æ»‘å¼•å¯¼æ”¹è¿›ï¼Œæ˜“é™·å…¥å†—ä½™ç»“æž„æˆ–ä¸å¯è¡Œè®¾è®¡
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šè½®å¤šä¸“å®¶å…±è¯†ã€è‡ªé€‚åº”åå°„æŽ¢ç´¢å’Œå¸•ç´¯æ‰˜å¼•å¯¼è¿›åŒ–é€‰æ‹©ï¼Œç»“åˆLLMæŽ¨ç†ä¸Žåé¦ˆå¯¹é½æœç´¢
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR10ã€CIFAR100ã€ImageNet16-120ã€COCO-5Kå’ŒCityscapeæ•°æ®é›†ä¸Šå®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†å®žç”¨å¯é æ€§å’Œå¯éƒ¨ç½²æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

