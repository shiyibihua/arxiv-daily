---
layout: default
title: See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors
---

# See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors

**arXiv**: [2512.05529v1](https://arxiv.org/abs/2512.05529) | [PDF](https://arxiv.org/pdf/2512.05529.pdf)

**ä½œè€…**: Kunyi Yang, Qingyu Wang, Cheng Yuan, Yutong Ban

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDepSegæ¡†æž¶ï¼Œåˆ©ç”¨å•ç›®æ·±åº¦å…ˆéªŒå®žçŽ°å…è®­ç»ƒè…¹è…”é•œåœºæ™¯åˆ†å‰²**

**å…³é”®è¯**: `è…¹è…”é•œåœºæ™¯åˆ†å‰²` `å…è®­ç»ƒåˆ†å‰²` `å•ç›®æ·±åº¦å…ˆéªŒ` `æ·±åº¦å¼•å¯¼æç¤º` `æ¨¡æ¿åŒ¹é…åˆ†ç±»` `è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè…¹è…”é•œåœºæ™¯åƒç´ çº§åˆ†å‰²æ ‡æ³¨æˆæœ¬é«˜ï¼Œéš¾ä»¥æ‰©å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå•ç›®æ·±åº¦ä¼°è®¡å’Œé¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡åž‹ï¼Œé€šè¿‡æ·±åº¦å¼•å¯¼æç¤ºå’Œæ¨¡æ¿åŒ¹é…åˆ†ç±»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CholecSeg8kæ•°æ®é›†ä¸Šï¼ŒmIoUä»Ž14.7%æå‡è‡³35.9%ï¼Œä»…éœ€10-20%æ¨¡æ¿ä¿æŒç«žäº‰åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.

