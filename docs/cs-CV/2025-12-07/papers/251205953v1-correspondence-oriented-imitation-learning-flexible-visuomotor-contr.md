---
layout: default
title: Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning
---

# Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning

**arXiv**: [2512.05953v1](https://arxiv.org/abs/2512.05953) | [PDF](https://arxiv.org/pdf/2512.05953.pdf)

**ä½œè€…**: Yunhao Cao, Zubin Bhaumik, Jessie Jia, Xingyi He, Kuan Fang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹åº”å¯¼å‘æ¨¡ä»¿å­¦ä¹ æ¡†æž¶ï¼Œé€šè¿‡3Då…³é”®ç‚¹è¿åŠ¨è¡¨ç¤ºå®žçŽ°çµæ´»è§†è§‰è¿åŠ¨æŽ§åˆ¶ã€‚**

**å…³é”®è¯**: `è§†è§‰è¿åŠ¨æŽ§åˆ¶` `æ¨¡ä»¿å­¦ä¹ ` `3Då…³é”®ç‚¹` `æ—¶ç©ºæ³¨æ„åŠ›` `è‡ªç›‘ç£è®­ç»ƒ` `ä»»åŠ¡æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¿åŠ¨æŽ§åˆ¶ä¸­ä»»åŠ¡è¡¨ç¤ºç¼ºä¹çµæ´»æ€§ï¼Œéš¾ä»¥é€‚åº”å¯å˜ç©ºé—´å’Œæ—¶é—´ç²’åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽ3Då…³é”®ç‚¹è¿åŠ¨å®šä¹‰ä»»åŠ¡ï¼Œé‡‡ç”¨æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶èžåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒå­¦ä¹ æ¡ä»¶ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­ï¼Œä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼Œæ”¯æŒç¨€ç–å’Œå¯†é›†ä»»åŠ¡è§„èŒƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.

