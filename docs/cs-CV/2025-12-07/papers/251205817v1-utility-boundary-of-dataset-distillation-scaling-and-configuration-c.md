---
layout: default
title: Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws
---

# Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws

**arXiv**: [2512.05817v1](https://arxiv.org/abs/2512.05817) | [PDF](https://arxiv.org/pdf/2512.05817.pdf)

**ä½œè€…**: Zhengquan Luo, Zhiqiang Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé…ç½®-åŠ¨æ€-è¯¯å·®åˆ†æžæ¡†æž¶ï¼Œæ­ç¤ºæ•°æ®é›†è’¸é¦çš„ç¼©æ”¾ä¸Žé…ç½®è¦†ç›–å®šå¾‹ã€‚**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `ç†è®ºæ¡†æž¶` `ç¼©æ”¾å®šå¾‹` `é…ç½®è¦†ç›–` `æ³›åŒ–è¯¯å·®` `æ ·æœ¬æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•°æ®é›†è’¸é¦ç¼ºä¹ç»Ÿä¸€ç†è®ºï¼Œæ€§èƒ½éšé…ç½®å˜åŒ–ä¸æ˜Žç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»Ÿä¸€åˆ†æžæ¡†æž¶å°†ä¸»æµæ–¹æ³•é‡æž„ä¸ºæ³›åŒ–è¯¯å·®è§†è§’ï¼ŒæŽ¨å¯¼ç¼©æ”¾ä¸Žè¦†ç›–å®šå¾‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå¤šæ–¹æ³•ä¸Žé…ç½®å®žéªŒéªŒè¯å®šå¾‹ï¼Œæ”¯æŒç†è®ºé©±åŠ¨çš„ç´§å‡‘ã€é²æ£’è’¸é¦è®¾è®¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.

