---
layout: default
title: Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales
---

# Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales

**arXiv**: [2512.05620v1](https://arxiv.org/abs/2512.05620) | [PDF](https://arxiv.org/pdf/2512.05620.pdf)

**ä½œè€…**: Shikai Qiu, Zixi Chen, Hoang Phan, Qi Lei, Andrew Gordon Wilson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡è¶…å‚æ•°è½¬ç§»å®žçŽ°çŸ©é˜µé¢„æ¡ä»¶ä¼˜åŒ–å™¨åœ¨ä¸åŒè§„æ¨¡ä¸‹çš„ç¨³å®šåŠ é€Ÿ**

**å…³é”®è¯**: `è¶…å‚æ•°è½¬ç§»` `çŸ©é˜µé¢„æ¡ä»¶ä¼˜åŒ–å™¨` `æ¨¡åž‹è§„æ¨¡æ‰©å±•` `å­¦ä¹ çŽ‡ç¼©æ”¾` `æƒé‡è¡°å‡` `ä¼˜åŒ–å™¨æ¯”è¾ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶çŸ©é˜µé¢„æ¡ä»¶ä¼˜åŒ–å™¨åœ¨è§„æ¨¡æ‰©å±•æ—¶æ€§èƒ½ä¸ç¨³å®šçš„æ ¸å¿ƒé—®é¢˜
2. æå‡ºåŸºäºŽÎ¼Pçš„è¶…å‚æ•°è½¬ç§»æ–¹æ³•ï¼Œç»“åˆåˆ†å—å’Œè°±å½’ä¸€åŒ–æå‡ç¨³å®šæ€§
3. å®žéªŒæ˜¾ç¤ºMuonå’ŒShampooåœ¨Llamaæž¶æž„æ¨¡åž‹ä¸Šå®žçŽ°1.3-1.4å€åŠ é€Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $Î¼$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $Î¼$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

