---
layout: default
title: Moving object detection from multi-depth images with an attention-enhanced CNN
---

# Moving object detection from multi-depth images with an attention-enhanced CNN

**arXiv**: [2512.05415v1](https://arxiv.org/abs/2512.05415) | [PDF](https://arxiv.org/pdf/2512.05415.pdf)

**ä½œè€…**: Masato Shibukawa, Fumi Yoshida, Toshifumi Yanagisawa, Takashi Ito, Hirohisa Kurosaki, Makoto Yoshikawa, Kohki Kamiya, Ji-an Jiang, Wesley Fraser, JJ Kavelaars, Susan Benecchi, Anne Verbiscer, Akira Hatakeyama, Hosei O, Naoya Ozaki

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¾“å…¥æ³¨æ„åŠ›å¢žå¼ºCNNä»¥æå‡å¤ªé˜³ç³»ç§»åŠ¨ç›®æ ‡æ£€æµ‹çš„è‡ªåŠ¨åŒ–æ°´å¹³**

**å…³é”®è¯**: `ç§»åŠ¨ç›®æ ‡æ£€æµ‹` `å¤šè¾“å…¥å·ç§¯ç¥žç»ç½‘ç»œ` `æ³¨æ„åŠ›æœºåˆ¶` `å¤ªé˜³ç³»å·¡å¤©` `è‡ªåŠ¨åŒ–éªŒè¯`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤ªé˜³ç³»å®½åœºå·¡å¤©æ•°æ®ä¸­ç§»åŠ¨ç›®æ ‡æ£€æµ‹ä¾èµ–äººå·¥éªŒè¯ï¼Œæˆæœ¬é«˜æ˜‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šè¾“å…¥æž¶æž„å¤„ç†å †å å›¾åƒï¼Œå¹¶é›†æˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ä»¥èšç„¦å…³é”®ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çº¦2000å¼ è§‚æµ‹å›¾åƒæ•°æ®é›†ä¸Šï¼Œæ¨¡åž‹å‡†ç¡®çŽ‡è¿‘99%ï¼ŒAUC>0.99ï¼Œå‡å°‘äººå·¥å·¥ä½œé‡è¶…99%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.

