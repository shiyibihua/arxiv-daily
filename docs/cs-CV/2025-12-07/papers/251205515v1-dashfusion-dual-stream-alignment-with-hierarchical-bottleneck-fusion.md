---
layout: default
title: DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis
---

# DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis

**arXiv**: [2512.05515v1](https://arxiv.org/abs/2512.05515) | [PDF](https://arxiv.org/pdf/2512.05515.pdf)

**ä½œè€…**: Yuhua Wen, Qifei Li, Yingying Zhou, Yingming Gao, Zhengqi Wen, Jianhua Tao, Ya Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDashFusionæ¡†æž¶ï¼Œé€šè¿‡åŒæµå¯¹é½ä¸Žåˆ†å±‚ç“¶é¢ˆèžåˆè§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžä¸­çš„å¯¹é½ä¸Žèžåˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æž` `è·¨æ¨¡æ€å¯¹é½` `åˆ†å±‚èžåˆ` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžé¢ä¸´è·¨æ¨¡æ€æ—¶åºä¸Žè¯­ä¹‰å¯¹é½åŠç‰¹å¾èžåˆçš„æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•å¸¸å­¤ç«‹å¤„ç†å¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒæµå¯¹é½æ¨¡å—å®žçŽ°æ—¶åºä¸Žè¯­ä¹‰åŒæ­¥ï¼Œç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–ç‰¹å¾ï¼Œå¹¶é€šè¿‡åˆ†å±‚ç“¶é¢ˆèžåˆå¹³è¡¡æ€§èƒ½ä¸Žæ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CMU-MOSIç­‰æ•°æ®é›†ä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œæ¶ˆèžç ”ç©¶éªŒè¯äº†å¯¹é½ä¸ŽèžåˆæŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.

