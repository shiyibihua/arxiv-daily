---
layout: default
title: ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration
---

# ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration

**arXiv**: [2512.05385v1](https://arxiv.org/abs/2512.05385) | [PDF](https://arxiv.org/pdf/2512.05385.pdf)

**ä½œè€…**: Yingjie Xia, Tao Liu, Jinglei Shi, Qingsong Xie, Heng Guo, Jian Yang, Xi Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºShaRPæ¡†æž¶ï¼Œé€šè¿‡æ”¹è¿›æ³¨æ„åŠ›å‰ªæžåŠ é€Ÿè§†é¢‘å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡åž‹` `æ³¨æ„åŠ›å‰ªæž` `æµ…å±‚åŠ é€Ÿ` `ä»¤ç‰Œé€‰æ‹©` `æŽ¨ç†ä¼˜åŒ–` `è§†é¢‘ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘å¤§è¯­è¨€æ¨¡åž‹åœ¨é¢„å¡«å……é˜¶æ®µå› è§†è§‰ä»¤ç‰Œè¿‡å¤šå¯¼è‡´é«˜è®¡ç®—è´Ÿè½½ï¼Œæµ…å±‚å‰ªæžæ˜“è‡´æ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆåˆ†æ®µæ„ŸçŸ¥å› æžœæŽ©ç ã€ä½ç½®åŽ»åå’Œä»¤ç‰ŒåŽ»é‡ï¼Œå¢žå¼ºæµ…å±‚ä»¤ç‰Œé€‰æ‹©èƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šä¿æŒç«žäº‰æ€§èƒ½ï¼Œæ”¯æŒé«˜åŽ‹ç¼©çŽ‡æ— éœ€é‡è®­ç»ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.

