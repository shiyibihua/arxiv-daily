---
layout: default
title: Bring Your Dreams to Life: Continual Text-to-Video Customization
---

# Bring Your Dreams to Life: Continual Text-to-Video Customization

**arXiv**: [2512.05802v1](https://arxiv.org/abs/2512.05802) | [PDF](https://arxiv.org/pdf/2512.05802.pdf)

**ä½œè€…**: Jiahua Dong, Xudong Wang, Wenqi Liang, Zongyan Han, Meng Cao, Duzhen Zhang, Hanbin Zhao, Zhi Han, Salman Khan, Fahad Shahbaz Khan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCCVDæ¨¡åž‹ä»¥è§£å†³æŒç»­æ–‡æœ¬åˆ°è§†é¢‘å®šåˆ¶ä¸­çš„é—å¿˜å’Œæ¦‚å¿µå¿½è§†é—®é¢˜**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ` `æ¦‚å¿µå®šåˆ¶` `æ‰©æ•£æ¨¡åž‹` `é—å¿˜ç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨æŒç»­å­¦ä¹ æ–°æ¦‚å¿µæ—¶æ˜“é—å¿˜æ—§æ¦‚å¿µå¹¶å¿½è§†ç”¨æˆ·æ¡ä»¶
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥æ¦‚å¿µç‰¹å®šå±žæ€§ä¿ç•™æ¨¡å—å’Œå¯æŽ§æ¡ä»¶åˆæˆä»¥å¢žå¼ºç‰¹å¾å¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šCCVDåœ¨å¤šé¡¹ä»»åŠ¡ä¸­ä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ï¼Œä»£ç å·²å¼€æº

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at https://github.com/JiahuaDong/CCVD.

