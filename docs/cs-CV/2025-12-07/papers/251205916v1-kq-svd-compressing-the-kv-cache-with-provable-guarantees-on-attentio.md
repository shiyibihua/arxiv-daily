---
layout: default
title: KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity
---

# KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity

**arXiv**: [2512.05916v1](https://arxiv.org/abs/2512.05916) | [PDF](https://arxiv.org/pdf/2512.05916.pdf)

**ä½œè€…**: Damien Lesens, Beheshteh T. Rakhshan, Guillaume Rabusseau

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKQ-SVDæ–¹æ³•ï¼Œé€šè¿‡æœ€ä¼˜ä½Žç§©åˆ†è§£åŽ‹ç¼©KVç¼“å­˜ä»¥æå‡æ³¨æ„åŠ›ä¿çœŸåº¦**

**å…³é”®è¯**: `KVç¼“å­˜åŽ‹ç¼©` `æ³¨æ„åŠ›çŸ©é˜µåˆ†è§£` `ä½Žç§©è¿‘ä¼¼` `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `å†…å­˜ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. KVç¼“å­˜æ˜¯LLMæŽ¨ç†æ•ˆçŽ‡çš„å…³é”®ï¼Œä½†éšåºåˆ—é•¿åº¦å’Œæ‰¹é‡å¢žé•¿æˆä¸ºå†…å­˜ç“¶é¢ˆ
2. çŽ°æœ‰åŽ‹ç¼©æ–¹æ³•ä»…åŽ‹ç¼©é”®æˆ–è”åˆåµŒå…¥æŸ¥è¯¢ä¸Žé”®ï¼Œæœªç›´æŽ¥é’ˆå¯¹æ³¨æ„åŠ›çŸ©é˜µè¿›è¡Œä¼˜åŒ–
3. KQ-SVDé€šè¿‡é—­å¼è§£ç›´æŽ¥åˆ†è§£æ³¨æ„åŠ›çŸ©é˜µï¼Œåœ¨LLaMAå’ŒMistralæ¨¡åž‹ä¸ŠéªŒè¯äº†æ›´é«˜çš„æŠ•å½±è´¨é‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

