---
layout: default
title: Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models
---

# Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models

**arXiv**: [2512.05546v1](https://arxiv.org/abs/2512.05546) | [PDF](https://arxiv.org/pdf/2512.05546.pdf)

**ä½œè€…**: Weijue Bu, Guan Yuan, Guixian Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConscious Gazeæ¡†æž¶ï¼Œé€šè¿‡æ¸¸æˆè®ºè§£é‡Šæ€§å®žçŽ°æŽ¨ç†æ—¶æ³¨æ„åŠ›æŽ§åˆ¶ï¼Œä»¥ç¼“è§£è§†è§‰è¯­è¨€æ¨¡åž‹ä¸­çš„å¹»è§‰é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¹»è§‰ç¼“è§£` `æ³¨æ„åŠ›æœºåˆ¶` `æŽ¨ç†æ—¶æŽ§åˆ¶` `æ¸¸æˆè®ºè§£é‡Šæ€§` `æ— è®­ç»ƒæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹å­˜åœ¨æ–‡æœ¬æƒ¯æ€§ï¼Œæ³¨æ„åŠ›ä»Žè§†è§‰è¯æ®æ¼‚ç§»è‡³è¯­è¨€å…ˆéªŒï¼Œå¯¼è‡´å¯¹è±¡å¹»è§‰ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽHarsanyiäº¤äº’æž„å»ºè®¤çŸ¥éœ€æ±‚ä¼ æ„Ÿå™¨ï¼Œæ£€æµ‹è§†è§‰-æ–‡æœ¬ååŒï¼›é€šè¿‡èšç„¦å…±è¯†è¯±å¯¼æ¨¡å—é€‰æ‹©æ€§é‡å®šå‘ä¸­å±‚æ³¨æ„åŠ›è‡³è§†è§‰æ ‡è®°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨POPEå’ŒCHAIRåŸºå‡†ä¸Šå®žçŽ°SOTAï¼Œé€‚ç”¨äºŽå¤šç§æ¨¡åž‹ï¼Œä¿æŒé€šç”¨èƒ½åŠ›ï¼Œæ— éœ€è®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.

