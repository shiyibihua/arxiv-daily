---
layout: default
title: Comparing the latent features of universal machine-learning interatomic potentials
---

# Comparing the latent features of universal machine-learning interatomic potentials

**arXiv**: [2512.05717v1](https://arxiv.org/abs/2512.05717) | [PDF](https://arxiv.org/pdf/2512.05717.pdf)

**ä½œè€…**: Sofiia Chorna, Davide Tisi, Cesare Malosso, Wei Bin How, Michele Ceriotti, Sanggyu Chong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒé€šç”¨æœºå™¨å­¦ä¹ åŽŸå­é—´åŠ¿çš„æ½œåœ¨ç‰¹å¾ï¼Œåˆ†æžå…¶ä¿¡æ¯å†…å®¹ä¸Žè®­ç»ƒå½±å“**

**å…³é”®è¯**: `æœºå™¨å­¦ä¹ åŽŸå­é—´åŠ¿` `æ½œåœ¨ç‰¹å¾åˆ†æž` `ç‰¹å¾é‡æž„è¯¯å·®` `åŒ–å­¦ç©ºé—´ç¼–ç ` `è®­ç»ƒåè®®å½±å“` `ç‰¹å¾åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé€šç”¨æœºå™¨å­¦ä¹ åŽŸå­é—´åŠ¿ï¼ˆuMLIPsï¼‰çš„æ½œåœ¨ç‰¹å¾å¦‚ä½•ç¼–ç åŒ–å­¦ä¿¡æ¯ï¼Œæ¨¡åž‹é—´å·®å¼‚åŠè®­ç»ƒå› ç´ å½±å“æœªçŸ¥
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç‰¹å¾é‡æž„è¯¯å·®å®šé‡è¯„ä¼°æ½œåœ¨ç‰¹å¾ä¿¡æ¯å†…å®¹ï¼Œåˆ†æžè®­ç»ƒé›†ã€åè®®å’Œå¾®è°ƒå¯¹ç‰¹å¾è¶‹åŠ¿çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šå‘çŽ°uMLIPsä»¥æ˜¾è‘—ä¸åŒæ–¹å¼ç¼–ç åŒ–å­¦ç©ºé—´ï¼Œç‰¹å¾é‡æž„è¯¯å·®é«˜ï¼Œå¾®è°ƒä¿ç•™é¢„è®­ç»ƒåå·®ï¼ŒåŽŸå­çº§ç‰¹å¾å¯åŽ‹ç¼©ä¸ºç»“æž„çº§ç‰¹å¾

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The past few years have seen the development of ``universal'' machine-learning interatomic potentials (uMLIPs) capable of approximating the ground-state potential energy surface across a wide range of chemical structures and compositions with reasonable accuracy. While these models differ in the architecture and the dataset used, they share the ability to compress a staggering amount of chemical information into descriptive latent features. Herein, we systematically analyze what the different uMLIPs have learned by quantitatively assessing the relative information content of their latent features with feature reconstruction errors as metrics, and observing how the trends are affected by the choice of training set and training protocol. We find that the uMLIPs encode chemical space in significantly distinct ways, with substantial cross-model feature reconstruction errors. When variants of the same model architecture are considered, trends become dependent on the dataset, target, and training protocol of choice. We also observe that fine-tuning of a uMLIP retains a strong pre-training bias in the latent features. Finally, we discuss how atom-level features, which are directly output by MLIPs, can be compressed into global structure-level features via concatenation of progressive cumulants, each adding significantly new information about the variability across the atomic environments within a given system.

