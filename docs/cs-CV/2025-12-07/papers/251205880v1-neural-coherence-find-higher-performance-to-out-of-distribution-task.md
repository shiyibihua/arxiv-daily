---
layout: default
title: Neural Coherence : Find higher performance to out-of-distribution tasks from few samples
---

# Neural Coherence : Find higher performance to out-of-distribution tasks from few samples

**arXiv**: [2512.05880v1](https://arxiv.org/abs/2512.05880) | [PDF](https://arxiv.org/pdf/2512.05880.pdf)

**ä½œè€…**: Simon Guiroy, Mats Richter, Sarath Chandar, Christopher Pal

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¥žç»ä¸€è‡´æ€§æ–¹æ³•ï¼Œä»¥å°‘é‡æ— æ ‡æ³¨æ ·æœ¬è§£å†³åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­çš„æ¨¡åž‹é€‰æ‹©é—®é¢˜**

**å…³é”®è¯**: `æ¨¡åž‹é€‰æ‹©` `åˆ†å¸ƒå¤–æ³›åŒ–` `ç¥žç»ä¸€è‡´æ€§` `æ— æ ‡æ³¨å­¦ä¹ ` `æ•°æ®é«˜æ•ˆå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨æ•°æ®ç¨€ç¼ºã€æ— æ ‡æ³¨ä¸”åˆ†å¸ƒå¤–çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•ä»Žå¤§è§„æ¨¡é¢„è®­ç»ƒä¸­é€‰æ‹©æœ€ä½³æ¨¡åž‹æ£€æŸ¥ç‚¹
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç¥žç»ä¸€è‡´æ€§ï¼Œé€šè¿‡åˆ†æžæºåŸŸå’Œç›®æ ‡åŸŸçš„æ¿€æ´»ç»Ÿè®¡ç‰¹å¾ï¼Œå®žçŽ°é«˜æ•ˆæ•°æ®åˆ©ç”¨çš„æ¨¡åž‹é€‰æ‹©
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet1Ké¢„è®­ç»ƒæ¨¡åž‹ä¸Šï¼ŒäºŽFood-101ç­‰ç›®æ ‡åŸŸå’Œå…ƒå­¦ä¹ è®¾ç½®ä¸­æ˜¾è‘—æå‡æ³›åŒ–æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

