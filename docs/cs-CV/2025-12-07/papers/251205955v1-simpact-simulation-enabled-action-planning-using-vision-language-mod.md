---
layout: default
title: SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models
---

# SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models

**arXiv**: [2512.05955v1](https://arxiv.org/abs/2512.05955) | [PDF](https://arxiv.org/pdf/2512.05955.pdf)

**ä½œè€…**: Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSIMPACTæ¡†æž¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æœºå™¨äººç²¾ç»†æ“ä½œä»»åŠ¡ä¸­çš„ç‰©ç†æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç‰©ç†æ¨¡æ‹Ÿ` `æœºå™¨äººæ“ä½œ` `åŠ¨ä½œè§„åˆ’` `ç‰©ç†æŽ¨ç†` `æµ‹è¯•æ—¶å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹ç¼ºä¹ç‰©ç†åŠ¨æ€ç†è§£ï¼Œéš¾ä»¥åº”ç”¨äºŽéœ€è¦ç‰©ç†æŽ¨ç†çš„æœºå™¨äººç²¾ç»†æ“ä½œä»»åŠ¡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨æµ‹è¯•æ—¶åˆ©ç”¨å•æ¬¡RGB-Dè§‚æµ‹æž„å»ºç‰©ç†æ¨¡æ‹Ÿï¼Œç»“åˆè¯­è¨€æŽ¨ç†è¿­ä»£ä¼˜åŒ–åŠ¨ä½œè§„åˆ’ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªçœŸå®žä¸–ç•Œåˆšä½“å’Œå¯å˜å½¢ç‰©ä½“æ“ä½œä»»åŠ¡ä¸­å®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶ŠçŽ°æœ‰é€šç”¨æœºå™¨äººæ“ä½œæ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io

