---
layout: default
title: To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis
---

# To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis

**arXiv**: [2512.05925v1](https://arxiv.org/abs/2512.05925) | [PDF](https://arxiv.org/pdf/2512.05925.pdf)

**ä½œè€…**: Federico Bianchi, Yongchan Kwon, Zachary Izzo, Linjun Zhang, James Zou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽGPT-5çš„è®ºæ–‡æ­£ç¡®æ€§æ£€æŸ¥å™¨ï¼Œç³»ç»Ÿé‡åŒ–AIé¡¶ä¼šè®ºæ–‡ä¸­çš„å®¢è§‚é”™è¯¯**

**å…³é”®è¯**: `è®ºæ–‡é”™è¯¯æ£€æµ‹` `GPT-5åº”ç”¨` `å¯å¤çŽ°æ€§ç ”ç©¶` `å®¢è§‚é”™è¯¯é‡åŒ–` `AIé¡¶ä¼šåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIé¡¶ä¼šè®ºæ–‡ä¸­å­˜åœ¨å®¢è§‚é”™è¯¯ï¼Œå¯èƒ½å½±å“åŽç»­ç ”ç©¶å’Œå¯å¤çŽ°æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨GPT-5æž„å»ºæ£€æŸ¥å™¨ï¼Œä¸“æ³¨äºŽå…¬å¼ã€æŽ¨å¯¼ã€è®¡ç®—ç­‰å¯éªŒè¯é”™è¯¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨NeurIPSç­‰ä¼šè®®è®ºæ–‡ä¸­ï¼Œé”™è¯¯æ•°é‡éšæ—¶é—´å¢žåŠ ï¼Œæ£€æŸ¥å™¨ç²¾åº¦è¾¾83.2%ï¼Œå¯ä¿®æ­£75.8%çš„é”™è¯¯

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

