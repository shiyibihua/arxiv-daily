---
layout: default
title: LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction
---

# LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction

**arXiv**: [2512.05915v1](https://arxiv.org/abs/2512.05915) | [PDF](https://arxiv.org/pdf/2512.05915.pdf)

**ä½œè€…**: Marius F. R. Juston, Ramavarapu S. Sreenivas, Dustin Nottage, Ahmet Soylemezoglu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽLDLTåˆ†è§£çš„å¹¿ä¹‰Lipschitzç½‘ç»œæž„å»ºæ–¹æ³•ï¼Œä»¥å¢žå¼ºå¯¹æŠ—é²æ£’æ€§å’Œç½‘ç»œå¯éªŒè¯æ€§ã€‚**

**å…³é”®è¯**: `Lipschitzç½‘ç»œ` `å¯¹æŠ—é²æ£’æ€§` `çº¿æ€§çŸ©é˜µä¸ç­‰å¼` `æ·±åº¦æ®‹å·®ç½‘ç»œ` `ç½‘ç»œå¯éªŒè¯æ€§` `å‚æ•°åŒ–æ–¹æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ§åˆ¶ç¥žç»ç½‘ç»œLipschitzå¸¸æ•°ä»¥æå‡å¯¹æŠ—é²æ£’æ€§å’Œå¯éªŒè¯æ€§ï¼Œä½†çŽ°æœ‰æ–¹æ³•å±€é™äºŽç‰¹å®šæž¶æž„ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨LDLTåˆ†è§£æ‰©å±•LMIæ¡†æž¶ï¼Œå®žçŽ°ä»»æ„éžçº¿æ€§æž¶æž„çš„Lipschitzç½‘ç»œå‚æ•°åŒ–æž„å»ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨121ä¸ªUCIæ•°æ®é›†ä¸Šï¼Œç›¸æ¯”SLL Layersï¼Œå‡†ç¡®çŽ‡æå‡3%-13%ï¼Œä¿æŒç½‘ç»œè¡¨è¾¾åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.

