---
layout: default
title: Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning
---

# Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning

**arXiv**: [2512.05513v1](https://arxiv.org/abs/2512.05513) | [PDF](https://arxiv.org/pdf/2512.05513.pdf)

**ä½œè€…**: Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKnow-ShowåŸºå‡†ä¸ŽGRAMæ’ä»¶ï¼Œè¯„ä¼°å¹¶å¢žå¼ºè§†é¢‘è¯­è¨€æ¨¡åž‹çš„æ—¶ç©ºåŸºç¡€æŽ¨ç†èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è§†é¢‘è¯­è¨€æ¨¡åž‹` `æ—¶ç©ºåŸºç¡€æŽ¨ç†` `åŸºå‡†è¯„ä¼°` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šæ¨¡æ€ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨æ—¶ç©ºåŸºç¡€æŽ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œéš¾ä»¥åŒæ—¶æŽ¨ç†åŠ¨ä½œè¯­ä¹‰å¹¶åŸºäºŽè§†è§‰å’Œæ—¶é—´è¯æ®è¿›è¡Œå®šä½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºKnow-ShowåŸºå‡†ï¼Œæ•´åˆäº”ä¸ªäº’è¡¥åœºæ™¯ï¼Œå¹¶è®¾è®¡GRAMæ’ä»¶ï¼Œé€šè¿‡æ³¨æ„åŠ›è§†é¢‘ä»¤ç‰Œé€‰æ‹©å’Œæ˜¾å¼æ—¶é—´æˆ³ç¼–ç å®žçŽ°ç»†ç²’åº¦åŸºç¡€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå¼€æ”¾å’Œå°é—­æ¨¡åž‹ä¸Šæµ‹è¯•ï¼Œæ­ç¤ºæ¨¡åž‹åœ¨ç»†ç²’åº¦æ‰‹ç‰©äº¤äº’ç­‰ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼ŒGRAMèƒ½æœ‰æ•ˆæå‡åŸºç¡€æŽ¨ç†æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to "show what they know" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.

