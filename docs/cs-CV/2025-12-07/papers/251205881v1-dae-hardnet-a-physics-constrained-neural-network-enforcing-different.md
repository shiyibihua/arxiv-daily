---
layout: default
title: DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints
---

# DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints

**arXiv**: [2512.05881v1](https://arxiv.org/abs/2512.05881) | [PDF](https://arxiv.org/pdf/2512.05881.pdf)

**ä½œè€…**: Rahul Golder, Bimol Nath Roy, M. M. Faruque Hasan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDAE-HardNetä»¥ä¸¥æ ¼æ»¡è¶³å¾®åˆ†ä»£æ•°çº¦æŸï¼Œæå‡ç‰©ç†çº¦æŸç¥žç»ç½‘ç»œæ€§èƒ½**

**å…³é”®è¯**: `ç‰©ç†çº¦æŸç¥žç»ç½‘ç»œ` `å¾®åˆ†ä»£æ•°æ–¹ç¨‹` `å¯å¾®åˆ†æŠ•å½±` `å‚æ•°ä¼°è®¡` `Lotka-Volterraç³»ç»Ÿ` `çƒ­ä¼ å¯¼`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸPINNséš¾ä»¥ä¸¥æ ¼æ»¡è¶³å«å¾®åˆ†ç®—å­çš„ç‰©ç†çº¦æŸï¼Œé€šå¸¸ä»¥è½¯æ–¹å¼æœ€å°åŒ–çº¦æŸè¿å
2. DAE-HardNeté€šè¿‡å¯å¾®åˆ†æŠ•å½±å±‚åŒæ—¶å­¦ä¹ å‡½æ•°åŠå…¶å¯¼æ•°ï¼Œå¼ºåˆ¶æ»¡è¶³ä»£æ•°ä¸Žå¾®åˆ†çº¦æŸ
3. åœ¨å¤šä¸ªDAEç³»ç»Ÿæµ‹è¯•ä¸­ï¼Œç›¸æ¯”MLPså’ŒPINNsï¼Œç‰©ç†æŸå¤±é™ä½Žæ•°ä¸ªæ•°é‡çº§ï¼Œä¿æŒé¢„æµ‹ç²¾åº¦

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.

