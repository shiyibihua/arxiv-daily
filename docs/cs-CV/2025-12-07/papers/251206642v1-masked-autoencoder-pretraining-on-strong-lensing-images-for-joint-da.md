---
layout: default
title: Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution
---

# Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.06642" target="_blank" class="toolbar-btn">arXiv: 2512.06642v1</a>
    <a href="https://arxiv.org/pdf/2512.06642.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06642v1" 
            onclick="toggleFavorite(this, '2512.06642v1', 'Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya, Putri Amelia, Sabrina Laila Mutiara, Hilman Syachr Ramadhan

**ÂàÜÁ±ª**: cs.CV, astro-ph.CO, astro-ph.IM, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-07

**Â§áÊ≥®**: 21 pages, 7 figures, 3 table

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁî®‰∫éÊöóÁâ©Ë¥®Ê®°ÂûãÂàÜÁ±ªÂíåË∂ÖÂàÜËæ®ÁéáÈáçÂª∫„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂºïÂäõÈÄèÈïú` `Êé©Á†ÅËá™ÁºñÁ†ÅÂô®` `È¢ÑËÆ≠ÁªÉ` `ÊöóÁâ©Ë¥®Ê®°ÂûãÂàÜÁ±ª` `Ë∂ÖÂàÜËæ®Áéá` `Vision Transformer` `ÂõæÂÉèÈáçÂª∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂàÜÊûê‰ΩéÂàÜËæ®Áéá„ÄÅÈ´òÂô™Â£∞ÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉè‰ª•Êè≠Á§∫ÊöóÁâ©Ë¥®Â≠êÁªìÊûÑÁöÑÂΩ±ÂìçÊòØ‰∏ÄÈ°πÊåëÊàò„ÄÇ
2. Âà©Áî®Êé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàMAEÔºâÂú®Ê®°ÊãüÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉè‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†ÂèØÊ≥õÂåñÁöÑÂõæÂÉèË°®Á§∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÊöóÁâ©Ë¥®Ê®°ÂûãÂàÜÁ±ªÂíåË∂ÖÂàÜËæ®ÁéáÈáçÂª∫‰ªªÂä°‰∏äÂùá‰ºò‰∫é‰ªéÂ§¥ËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂºïÂäõÈÄèÈïúÂèØ‰ª•Êè≠Á§∫ÊòüÁ≥ª‰∏≠ÊöóÁâ©Ë¥®Â≠êÁªìÊûÑÁöÑÂΩ±ÂìçÔºå‰ΩÜ‰ªéÂô™Â£∞ËæÉÂ§ßÁöÑ‰ΩéÂàÜËæ®ÁéáÂõæÂÉè‰∏≠ÂàÜÊûêËøô‰∫õÂΩ±ÂìçÊûÅÂÖ∑ÊåëÊàòÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàMAEÔºâÁöÑÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•Âú®DeepLense ML4SCIÂü∫ÂáÜÊµãËØï‰∏≠Ê®°ÊãüÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉè‰∏äËøõË°åÔºå‰ª•Â≠¶‰π†ÂèØÊ≥õÂåñÁöÑË°®Á§∫ÔºåÁî®‰∫é‰∏§‰∏™‰∏ãÊ∏∏‰ªªÂä°ÔºöÔºàiÔºâÂØπÊΩúÂú®ÁöÑÊöóÁâ©Ë¥®Ê®°ÂûãÔºàÂÜ∑ÊöóÁâ©Ë¥®„ÄÅÁ±ªËΩ¥Â≠êÊàñÊó†Â≠êÁªìÊûÑÔºâËøõË°åÂàÜÁ±ªÔºõÔºàiiÔºâÈÄöËøáË∂ÖÂàÜËæ®ÁéáÂ¢ûÂº∫‰ΩéÂàÜËæ®ÁéáÈÄèÈïúÂõæÂÉè„ÄÇÊàë‰ª¨‰ΩøÁî®Êé©Á†ÅÂõæÂÉèÂª∫Ê®°ÁõÆÊ†áÈ¢ÑËÆ≠ÁªÉVision TransformerÁºñÁ†ÅÂô®ÔºåÁÑ∂ÂêéÈíàÂØπÊØè‰∏™‰ªªÂä°ÂàÜÂà´ÂæÆË∞ÉÁºñÁ†ÅÂô®„ÄÇÁªìÊûúË°®ÊòéÔºåMAEÈ¢ÑËÆ≠ÁªÉ‰∏éÈÄÇÂΩìÁöÑÊé©Á†ÅÊØî‰æãË∞ÉÊï¥Áõ∏ÁªìÂêàÔºå‰∫ßÁîü‰∫Ü‰∏Ä‰∏™ÂÖ±‰∫´ÁºñÁ†ÅÂô®ÔºåÂÖ∂ÊÄßËÉΩ‰∏é‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁöÑViTÁõ∏ÂåπÈÖçÊàñË∂ÖËøá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®90%ÁöÑÊé©Á†ÅÊØî‰æã‰∏ãÔºåÂæÆË∞ÉÂêéÁöÑÂàÜÁ±ªÂô®ÂÆûÁé∞‰∫Ü0.968ÁöÑÂÆèÂπ≥ÂùáAUCÂíå88.65%ÁöÑÂáÜÁ°ÆÁéáÔºåËÄå‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁöÑÂü∫Á∫øÂàÜÂà´‰∏∫0.957Âíå82.46%„ÄÇÂØπ‰∫éË∂ÖÂàÜËæ®ÁéáÔºà16x16Âà∞64x64ÔºâÔºåMAEÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈáçÂª∫ÁöÑÂõæÂÉèÁöÑPSNRÁ∫¶‰∏∫33 dBÔºåSSIM‰∏∫0.961ÔºåÁï•‰ºò‰∫é‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÂØπMAEÊé©Á†ÅÊØî‰æãËøõË°å‰∫ÜÊ∂àËûçÁ†îÁ©∂ÔºåÊè≠Á§∫‰∫Ü‰∏Ä‰∏™‰∏ÄËá¥ÁöÑÊùÉË°°ÔºöËæÉÈ´òÁöÑÊé©Á†ÅÊØî‰æãÊèêÈ´ò‰∫ÜÂàÜÁ±ªÊÄßËÉΩÔºå‰ΩÜÁï•ÂæÆÈôç‰Ωé‰∫ÜÈáçÂª∫‰øùÁúüÂ∫¶„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂú®ÂØåÂê´Áâ©ÁêÜ‰ø°ÊÅØÁöÑÊ®°ÊãüÊï∞ÊçÆ‰∏äËøõË°åMAEÈ¢ÑËÆ≠ÁªÉÔºå‰∏∫Â§ö‰∏™Âº∫ÂºïÂäõÈÄèÈïúÂàÜÊûê‰ªªÂä°Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÁÅµÊ¥ª„ÄÅÂèØÈáçÁî®ÁöÑÁºñÁ†ÅÂô®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªé‰ΩéÂàÜËæ®Áéá„ÄÅÈ´òÂô™Â£∞ÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉè‰∏≠ÂáÜÁ°ÆÂàÜÁ±ªÊöóÁâ©Ë¥®Ê®°ÂûãÔºàÂÜ∑ÊöóÁâ©Ë¥®„ÄÅÁ±ªËΩ¥Â≠êÊàñÊó†Â≠êÁªìÊûÑÔºâÂπ∂ËøõË°åË∂ÖÂàÜËæ®ÁéáÈáçÂª∫ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÊ≠§Á±ªÂõæÂÉèÊó∂ÔºåÁî±‰∫éÂõæÂÉèË¥®ÈáèÂ∑ÆÔºåÁâπÂæÅÊèêÂèñÂõ∞ÈöæÔºåÂØºËá¥ÂàÜÁ±ªÁ≤æÂ∫¶ÂíåÈáçÂª∫Ë¥®Èáè‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Êé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàMAEÔºâËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†ÂõæÂÉèÁöÑÈÄöÁî®Ë°®Á§∫„ÄÇÈÄöËøáÂú®Â§ßÈáèÊ®°ÊãüÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉè‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊçïÊçâÂà∞ÂõæÂÉè‰∏≠ÁöÑÂÖ≥ÈîÆÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´ò‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÊé©Á†ÅÂõæÂÉèÂª∫Ê®°Ëø´‰ΩøÊ®°ÂûãÁêÜËß£ÂõæÂÉèÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂç≥‰ΩøÈÉ®ÂàÜÂõæÂÉèË¢´ÈÅÆÁõñ‰πüËÉΩËøõË°åÈáçÂª∫Ôºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) ‰ΩøÁî®Ê®°ÊãüÁöÑÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉèÊï∞ÊçÆÈõÜËøõË°åMAEÈ¢ÑËÆ≠ÁªÉÔºåËÆ≠ÁªÉ‰∏Ä‰∏™Vision Transformer (ViT) ÁºñÁ†ÅÂô®„ÄÇ2) Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑViTÁºñÁ†ÅÂô®Â∫îÁî®‰∫é‰∏§‰∏™‰∏ãÊ∏∏‰ªªÂä°ÔºöÊöóÁâ©Ë¥®Ê®°ÂûãÂàÜÁ±ªÂíåË∂ÖÂàÜËæ®ÁéáÈáçÂª∫„ÄÇ3) ÂàÜÂà´ÈíàÂØπÊØè‰∏™‰∏ãÊ∏∏‰ªªÂä°ÂØπÁºñÁ†ÅÂô®ËøõË°åÂæÆË∞É„ÄÇÂØπ‰∫éÂàÜÁ±ª‰ªªÂä°ÔºåÂú®ÁºñÁ†ÅÂô®ÂêéÊ∑ªÂä†ÂàÜÁ±ªÂ§¥ÔºõÂØπ‰∫éË∂ÖÂàÜËæ®Áéá‰ªªÂä°Ôºå‰ΩøÁî®Ëß£Á†ÅÂô®Â∞ÜÁºñÁ†ÅÂô®ÁöÑËæìÂá∫Êò†Â∞ÑÂà∞È´òÂàÜËæ®ÁéáÂõæÂÉè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜMAEÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÂ∫îÁî®‰∫éÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉèÂàÜÊûê„ÄÇ‰∏é‰º†ÁªüÁöÑ‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁõ∏ÊØîÔºåMAEÈ¢ÑËÆ≠ÁªÉËÉΩÂ§üÂ≠¶‰π†Âà∞Êõ¥ÂÖ∑Ê≥õÂåñËÉΩÂäõÁöÑÂõæÂÉèË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´ò‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÁ†îÁ©∂‰∫ÜÊé©Á†ÅÊØî‰æãÂØπÈ¢ÑËÆ≠ÁªÉÊïàÊûúÁöÑÂΩ±ÂìçÔºåÂèëÁé∞ÈÄÇÂΩìÁöÑÊé©Á†ÅÊØî‰æãÂèØ‰ª•ÊèêÈ´òÂàÜÁ±ªÊÄßËÉΩÔºå‰ΩÜÂèØËÉΩ‰ºöÁï•ÂæÆÈôç‰ΩéÈáçÂª∫‰øùÁúüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰ΩøÁî®‰∫ÜVision Transformer (ViT) ‰Ωú‰∏∫ÁºñÁ†ÅÂô®ÔºåÂπ∂ÈááÁî®‰∫ÜÊé©Á†ÅÂõæÂÉèÂª∫Ê®°‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁõÆÊ†á„ÄÇÂÖ≥ÈîÆÂèÇÊï∞ÂåÖÊã¨Êé©Á†ÅÊØî‰æãÔºàmask ratioÔºâÔºåÂÆûÈ™åË°®Êòé90%ÁöÑÊé©Á†ÅÊØî‰æãÂú®ÂàÜÁ±ª‰ªªÂä°‰∏äË°®Áé∞ÊúÄ‰Ω≥„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢ÔºåÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµ‰ΩøÁî®ÂÉèÁ¥†Á∫ßÂà´ÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâ‰Ωú‰∏∫ÈáçÂª∫ÊçüÂ§±„ÄÇÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÔºåÂàÜÁ±ª‰ªªÂä°‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÔºåË∂ÖÂàÜËæ®Áéá‰ªªÂä°‰ΩøÁî®PSNRÂíåSSIM‰Ωú‰∏∫ËØÑ‰ª∑ÊåáÊ†á„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®90%ÁöÑÊé©Á†ÅÊØî‰æã‰∏ãÔºåMAEÈ¢ÑËÆ≠ÁªÉÁöÑÂàÜÁ±ªÂô®Âú®ÊöóÁâ©Ë¥®Ê®°ÂûãÂàÜÁ±ª‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü0.968ÁöÑÂÆèÂπ≥ÂùáAUCÂíå88.65%ÁöÑÂáÜÁ°ÆÁéáÔºåÊòæËëó‰ºò‰∫é‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁöÑÂü∫Á∫øÔºàAUC 0.957ÔºåÂáÜÁ°ÆÁéá 82.46%Ôºâ„ÄÇÂØπ‰∫éË∂ÖÂàÜËæ®Áéá‰ªªÂä°ÔºåMAEÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈáçÂª∫ÁöÑÂõæÂÉèÁöÑPSNRÁ∫¶‰∏∫33 dBÔºåSSIM‰∏∫0.961ÔºåÁï•‰ºò‰∫é‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§©ÊñáÂõæÂÉèÂ§ÑÁêÜ„ÄÅÊöóÁâ©Ë¥®Á†îÁ©∂Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÈ´òÂº∫ÂºïÂäõÈÄèÈïúÂõæÂÉèÁöÑÂàÜÊûêÁ≤æÂ∫¶ÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞Á†îÁ©∂ÊöóÁâ©Ë¥®ÁöÑÊÄßË¥®ÂíåÂàÜÂ∏ÉÔºå‰ªéËÄåÂä†Ê∑±Êàë‰ª¨ÂØπÂÆáÂÆôÁªìÊûÑÁöÑÁêÜËß£„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñ‰ΩéÂàÜËæ®Áéá„ÄÅÈ´òÂô™Â£∞ÁöÑÂõæÂÉèÂ§ÑÁêÜ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

