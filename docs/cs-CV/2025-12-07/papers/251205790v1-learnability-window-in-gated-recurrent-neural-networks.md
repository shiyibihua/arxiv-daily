---
layout: default
title: Learnability Window in Gated Recurrent Neural Networks
---

# Learnability Window in Gated Recurrent Neural Networks

**arXiv**: [2512.05790v1](https://arxiv.org/abs/2512.05790) | [PDF](https://arxiv.org/pdf/2512.05790.pdf)

**ä½œè€…**: Lorenzo Livi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé—¨æŽ§å¾ªçŽ¯ç¥žç»ç½‘ç»œä¸­å¯å­¦ä¹ æ€§çª—å£çš„ç†è®ºæ¡†æž¶ï¼Œè§£é‡Šé—¨æŽ§æœºåˆ¶å¦‚ä½•å½±å“é•¿æœŸä¾èµ–å­¦ä¹ ã€‚**

**å…³é”®è¯**: `é—¨æŽ§å¾ªçŽ¯ç¥žç»ç½‘ç»œ` `å¯å­¦ä¹ æ€§çª—å£` `æ¢¯åº¦ä¼ è¾“` `æœ‰æ•ˆå­¦ä¹ çŽ‡` `é‡å°¾å™ªå£°` `æ ·æœ¬å¤æ‚åº¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé—¨æŽ§æœºåˆ¶å¦‚ä½•å†³å®šå¾ªçŽ¯ç¥žç»ç½‘ç»œçš„å¯å­¦ä¹ æ€§çª—å£ï¼Œå³æ¢¯åº¦ä¿¡æ¯å¯ç»Ÿè®¡æ¢å¤çš„æœ€å¤§æ—¶é—´èŒƒå›´ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ä¸€é˜¶å±•å¼€å®šä¹‰æœ‰æ•ˆå­¦ä¹ çŽ‡ï¼Œä½œä¸ºæ¢¯åº¦ä¼ è¾“çš„ä¹˜æ€§æ»¤æ³¢å™¨ï¼ŒæŽ§åˆ¶æ¢¯åº¦å¹…åº¦å’Œå„å‘å¼‚æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é‡å°¾æ¢¯åº¦å™ªå£°ä¸‹ï¼ŒæŽ¨å¯¼æ ·æœ¬å¤æ‚åº¦å…¬å¼ï¼Œé¢„æµ‹é—¨æŽ§è°±å’Œå™ªå£°å¯¹å¯å­¦ä¹ æ€§çª—å£çš„å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $Î¼_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($Î±$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-Î±}$, where $f(\ell)=\\|Î¼_{t,\ell}\\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.

