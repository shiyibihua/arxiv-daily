---
layout: default
title: DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model
---

# DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model

**arXiv**: [2512.05613v1](https://arxiv.org/abs/2512.05613) | [PDF](https://arxiv.org/pdf/2512.05613.pdf)

**ä½œè€…**: Pasquale De Marinis, Pieter M. Blok, Uzay Kaymak, Rogier Brussee, Gennaro Vessio, Giovanna Castellano

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDistillFSSæ¡†æž¶ï¼Œé€šè¿‡è’¸é¦å°†å°‘æ ·æœ¬çŸ¥è¯†åµŒå…¥æ¨¡åž‹å‚æ•°ï¼Œè§£å†³è·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `è·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²` `çŸ¥è¯†è’¸é¦` `è½»é‡çº§æ¨¡åž‹` `å‚æ•°åµŒå…¥` `å¤šåŸŸåŸºå‡†` `é«˜æ•ˆæŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²é¢ä¸´åŸŸåˆ†å¸ƒåç§»ã€æ ‡ç­¾ç©ºé—´ä¸é‡å å’Œæ”¯æŒæ ·æœ¬ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ï¼Œå°†å°‘æ ·æœ¬æŽ¨ç†å†…åŒ–åˆ°å­¦ç”Ÿç½‘ç»œä¸“ç”¨å±‚ï¼Œæ— éœ€æµ‹è¯•æ—¶æ”¯æŒå›¾åƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–°åŸºå‡†ä¸ŠåŒ¹é…æˆ–è¶…è¶Šå…ˆè¿›åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤šç±»å¤šæ ·æœ¬åœºæ™¯ï¼Œæ˜¾è‘—æå‡æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.

