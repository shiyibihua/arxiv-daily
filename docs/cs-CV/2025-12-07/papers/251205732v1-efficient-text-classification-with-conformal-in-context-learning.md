---
layout: default
title: Efficient Text Classification with Conformal In-Context Learning
---

# Efficient Text Classification with Conformal In-Context Learning

**arXiv**: [2512.05732v1](https://arxiv.org/abs/2512.05732) | [PDF](https://arxiv.org/pdf/2512.05732.pdf)

**ä½œè€…**: Ippokratis Pantelidis, Korbinian Randl, Aron Henriksson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCICLeæ¡†æž¶ä»¥é«˜æ•ˆæ–‡æœ¬åˆ†ç±»ï¼Œç»“åˆè½»é‡åˆ†ç±»å™¨ä¸ŽConformal PredictionæŒ‡å¯¼LLMæç¤ºã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ†ç±»` `Conformal Prediction` `å¤§è¯­è¨€æ¨¡åž‹` `æç¤ºå·¥ç¨‹` `è®¡ç®—æ•ˆçŽ‡` `ç±»åˆ«ä¸å¹³è¡¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šLLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ä¾èµ–æç¤ºè®¾è®¡ä¸”è®¡ç®—æˆæœ¬é«˜ï¼ŒCICLeæ—¨åœ¨æå‡æ•ˆçŽ‡ä¸Žé€‚ç”¨æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè½»é‡åŸºåˆ†ç±»å™¨ä¸ŽConformal Predictionï¼Œè‡ªé€‚åº”å‡å°‘å€™é€‰ç±»åˆ«ä»¥ä¼˜åŒ–LLMæç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ ·NLPåŸºå‡†ä¸Šè¯„ä¼°ï¼ŒCICLeæå‡åŸºåˆ†ç±»å™¨æ€§èƒ½ï¼Œå‡å°‘æç¤ºé•¿åº¦ä¸Žæ ·æœ¬æ•°ï¼Œå°¤å…¶é€‚ç”¨äºŽç±»åˆ«ä¸å¹³è¡¡ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.

