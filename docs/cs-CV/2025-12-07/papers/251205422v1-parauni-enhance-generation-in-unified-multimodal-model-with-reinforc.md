---
layout: default
title: ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction
---

# ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction

**arXiv**: [2512.05422v1](https://arxiv.org/abs/2512.05422) | [PDF](https://arxiv.org/pdf/2512.05422.pdf)

**ä½œè€…**: Jiangtong Tan, Lin Liu, Jie Huanng, Xiaopeng Zhang, Qi Tian, Feng Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºParaUniä»¥å¢žå¼ºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹ä¸­çš„ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡å¹¶è¡Œä¿¡æ¯äº¤äº’å’Œå¼ºåŒ–å­¦ä¹ å±‚çº§è°ƒæ•´ã€‚**

**å…³é”®è¯**: `ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹` `è§†è§‰ç”Ÿæˆ` `å¹¶è¡Œç‰¹å¾æå–` `å¼ºåŒ–å­¦ä¹ ` `å±‚çº§ä¿¡æ¯äº¤äº’` `æ‰©æ•£æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡è§†è§‰è¯­è¨€æ¨¡åž‹ä¸Žæ‰©æ•£æ¨¡åž‹é—´çš„å……åˆ†äº¤äº’ä¸Žçµæ´»å®žçŽ°ã€‚
2. ParaUniå¹¶è¡Œæå–è§†è§‰è¯­è¨€æ¨¡åž‹å„å±‚ç‰¹å¾ï¼Œé€šè¿‡å±‚é›†æˆæ¨¡å—èžåˆç»†èŠ‚ä¸Žè¯­ä¹‰ä¿¡æ¯ã€‚
3. å®žéªŒè¡¨æ˜ŽParaUniåˆ©ç”¨å¤šå±‚äº’è¡¥ç‰¹å¾æ˜¾è‘—æå‡ç”Ÿæˆè´¨é‡ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µå±•çŽ°å¤šå¥–åŠ±ä¼˜åŒ–æ½œåŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \textbf{ParaUni}. It extracts features from variants VLM's layers in a \textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at https://github.com/JosephTiTan/ParaUni.

