---
layout: default
title: Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment
---

# Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment

**arXiv**: [2512.05464v1](https://arxiv.org/abs/2512.05464) | [PDF](https://arxiv.org/pdf/2512.05464.pdf)

**ä½œè€…**: Panatchakorn Anantaprayoon, Nataliia Babina, Jad Tarifi, Nima Asgharbeygi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å¯¹é½æ¡†æž¶ä¸Žé›†ä½“ä»£ç†ä»·å€¼ï¼Œä»¥å¯æ‰©å±•è‡ªæ”¹è¿›æ–¹æ³•è§£å†³å¤§è¯­è¨€æ¨¡åž‹å¼€æ”¾å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹å¯¹é½` `è‡ªæ”¹è¿›å¯¹é½` `é›†ä½“ä»£ç†` `åŠ¨æ€å¯¹é½æ¡†æž¶` `è‡ªå¥–åŠ±æœºåˆ¶` `GRPOå­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå¯¹é½æ–¹æ³•åœ¨AGI/ASIå‘å±•ä¸­å¯èƒ½ä¸è¶³ï¼Œä¸”åŸºäºŽäººç±»åé¦ˆçš„èµ„æºå¯†é›†éš¾ä»¥æ‰©å±•
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥é›†ä½“ä»£ç†ä½œä¸ºå¼€æ”¾å¯¹é½ä»·å€¼ï¼Œç»“åˆè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆä¸Žè‡ªå¥–åŠ±æœºåˆ¶å®žçŽ°è¿­ä»£è‡ªå¯¹é½
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¡¨æ˜Žæ–¹æ³•æˆåŠŸå¯¹é½æ¨¡åž‹è‡³é›†ä½“ä»£ç†ï¼ŒåŒæ—¶ä¿æŒä¸€èˆ¬NLPèƒ½åŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.

