---
layout: default
title: LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning
---

# LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21317" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21317v1</a>
  <a href="https://arxiv.org/pdf/2506.21317.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21317v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21317v1', 'LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dewen Zhang, Tahir Hussain, Wangpeng An, Hayaru Shouno

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: arXiv admin note: substantial text overlap with arXiv:2409.09306

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Ody-trek/LLaVA-Pose)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLaVA-Poseä»¥è§£å†³äººç±»å§¿æ€ä¸åŠ¨ä½œç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»å§¿æ€ç†è§£` `åŠ¨ä½œè¯†åˆ«` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†äººç±»å§¿æ€å’ŒåŠ¨ä½œç†è§£æ—¶è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ä¸“é—¨çš„æŒ‡ä»¤è·Ÿéšæ•°æ®ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†äººä½“å…³é”®ç‚¹ä¸ä¼ ç»Ÿè§†è§‰ç‰¹å¾ç»“åˆçš„æ–¹æ³•ï¼Œä»¥ç”Ÿæˆæ›´ç²¾å‡†çš„è§†è§‰ç†è§£æ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„LLaVA-Poseæ¨¡å‹åœ¨E-HPAUBåŸºå‡†ä¸Šæ€§èƒ½æå‡äº†33.2%ï¼ŒéªŒè¯äº†å…³é”®ç‚¹é›†æˆæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸€èˆ¬è§†è§‰ç†è§£ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†ä¸äººç±»å§¿æ€å’ŒåŠ¨ä½œç›¸å…³çš„å¤æ‚è§†è§‰ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼ŒåŸå› åœ¨äºç¼ºä¹ä¸“é—¨çš„è§†è§‰è¯­è¨€æŒ‡ä»¤è·Ÿéšæ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å°†äººä½“å…³é”®ç‚¹ä¸ä¼ ç»Ÿè§†è§‰ç‰¹å¾ï¼ˆå¦‚æ ‡é¢˜å’Œè¾¹ç•Œæ¡†ï¼‰ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç”Ÿæˆæ­¤ç±»æ•°æ®ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°ç†è§£ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«200,328ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œæ—¨åœ¨å¾®è°ƒæ¨¡å‹ä»¥åº”å¯¹äººç±»ä¸­å¿ƒä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨å¯¹è¯ã€è¯¦ç»†æè¿°å’Œå¤æ‚æ¨ç†ã€‚é€šè¿‡åœ¨æ‰©å±•äººç±»å§¿æ€ä¸åŠ¨ä½œç†è§£åŸºå‡†ï¼ˆE-HPAUBï¼‰ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬å¯¹LLaVA-1.5-7Bæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œç»“æœæ˜¾ç¤ºæ˜¾è‘—æå‡ï¼Œæ•´ä½“æ€§èƒ½æé«˜äº†33.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚äººç±»å§¿æ€å’ŒåŠ¨ä½œç†è§£ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œä¸»è¦ç—›ç‚¹åœ¨äºç¼ºä¹ä¸“é—¨çš„æŒ‡ä»¤è·Ÿéšæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†äººä½“å…³é”®ç‚¹ä¸ä¼ ç»Ÿè§†è§‰ç‰¹å¾ï¼ˆå¦‚æ ‡é¢˜å’Œè¾¹ç•Œæ¡†ï¼‰ç›¸ç»“åˆï¼Œç”Ÿæˆä¸“é—¨çš„æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹å¯¹äººç±»ä¸­å¿ƒåœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸äººç±»å§¿æ€å’ŒåŠ¨ä½œç›¸å…³çš„ç»†èŠ‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¾®è°ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ„å»ºåŒ…å«200,328ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†å¯¹LLaVA-1.5-7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›æœ€åï¼Œé€šè¿‡æ‰©å±•äººç±»å§¿æ€ä¸åŠ¨ä½œç†è§£åŸºå‡†ï¼ˆE-HPAUBï¼‰è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå…³é”®ç‚¹é›†æˆæ•°æ®çš„ç”Ÿæˆæ–¹æ³•ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºé€šè¿‡å…³é”®ç‚¹ä¿¡æ¯å¢å¼ºäº†æ¨¡å‹å¯¹äººç±»å§¿æ€å’ŒåŠ¨ä½œçš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œç»“åˆäº†å¤šç§è§†è§‰ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ äººç±»å§¿æ€å’ŒåŠ¨ä½œçš„å¤æ‚å…³ç³»ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­ç»è¿‡å¤šæ¬¡è°ƒä¼˜ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„LLaVA-Poseæ¨¡å‹åœ¨E-HPAUBåŸºå‡†ä¸Šå®ç°äº†33.2%çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸå§‹çš„LLaVA-1.5-7Bæ¨¡å‹ï¼ŒéªŒè¯äº†å…³é”®ç‚¹é›†æˆæ•°æ®åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡äººç±»å§¿æ€å’ŒåŠ¨ä½œç†è§£èƒ½åŠ›ï¼ŒLLaVA-Poseèƒ½å¤Ÿåœ¨è¿™äº›é¢†åŸŸä¸­å®ç°æ›´è‡ªç„¶çš„äº¤äº’å’Œæ›´æ™ºèƒ½çš„åˆ†æï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current vision-language models (VLMs) are well-adapted for general visual understanding tasks. However, they perform inadequately when handling complex visual tasks related to human poses and actions due to the lack of specialized vision-language instruction-following data. We introduce a method for generating such data by integrating human keypoints with traditional visual features such as captions and bounding boxes, enabling more precise understanding of human-centric scenes. Our approach constructs a dataset comprising 200,328 samples tailored to fine-tune models for human-centric tasks, focusing on three areas: conversation, detailed description, and complex reasoning. We establish an Extended Human Pose and Action Understanding Benchmark (E-HPAUB) to assess model performance on human pose and action understanding. We fine-tune the LLaVA-1.5-7B model using this dataset and evaluate our resulting LLaVA-Pose model on the benchmark, achieving significant improvements. Experimental results show an overall improvement of 33.2% compared to the original LLaVA-1.5-7B model. These findings highlight the effectiveness of keypoint-integrated data in enhancing multimodal models for human-centric visual understanding. Code is available at https://github.com/Ody-trek/LLaVA-Pose.

