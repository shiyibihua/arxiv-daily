---
layout: default
title: Multimodal Prompt Alignment for Facial Expression Recognition
---

# Multimodal Prompt Alignment for Facial Expression Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21017" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21017v1</a>
  <a href="https://arxiv.org/pdf/2506.21017.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21017v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21017v1', 'Multimodal Prompt Alignment for Facial Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fuyan Ma, Yiran He, Bin Sun, Shutao Li

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: To appear in ICCV2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æç¤ºå¯¹é½æ¡†æ¶ä»¥æå‡é¢éƒ¨è¡¨æƒ…è¯†åˆ«ç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢éƒ¨è¡¨æƒ…è¯†åˆ«` `å¤šæ¨¡æ€æç¤º` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç»†ç²’åº¦å¯¹é½` `æƒ…æ„Ÿè®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ–¹æ³•éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„æ–‡æœ¬-è§†è§‰å…³ç³»ï¼Œå½±å“è¯†åˆ«ç²¾åº¦ã€‚
2. æœ¬æ–‡æå‡ºçš„MPA-FERæ¡†æ¶é€šè¿‡å¤šæ¨¡æ€æç¤ºå¯¹é½ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†æè¿°ï¼Œå¢å¼ºäº†è§†è§‰ç‰¹å¾çš„å­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMPA-FERåœ¨ä¸‰ä¸ªFERåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæå‡äº†è¯†åˆ«æ€§èƒ½å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æç¤ºå­¦ä¹ å·²è¢«å¹¿æ³›åº”ç”¨äºé«˜æ•ˆé€‚åº”è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œå¤„ç†å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºVLMçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆFERï¼‰æ–¹æ³•åœ¨æ•æ‰ç»†ç²’åº¦æ–‡æœ¬-è§†è§‰å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMPA-FERçš„å¤šæ¨¡æ€æç¤ºå¯¹é½æ¡†æ¶ï¼Œä¸ºæç¤ºçš„è§†è§‰ç‰¹å¾å­¦ä¹ è¿‡ç¨‹æä¾›ç»†ç²’åº¦çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®å’Œå¯è§£é‡Šçš„è¡¨ç¤ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šç²’åº¦ç¡¬æç¤ºç”Ÿæˆç­–ç•¥ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ¯ç§é¢éƒ¨è¡¨æƒ…çš„è¯¦ç»†æè¿°ã€‚é€šè¿‡æœ€å°åŒ–è½¯æç¤ºä¸ç¡¬æç¤ºä¹‹é—´çš„ç‰¹å¾å·®å¼‚ï¼Œå°†LLMçš„å¤–éƒ¨çŸ¥è¯†æ³¨å…¥è½¯æç¤ºä¸­ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†è·¨æ¨¡æ€å…¨å±€-å±€éƒ¨å¯¹é½æ¨¡å—ï¼Œè¿›ä¸€æ­¥æ”¹å–„æ–‡æœ¬ä¸è§†è§‰ç‰¹å¾ä¹‹é—´çš„å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªFERåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ–¹æ³•åœ¨æ•æ‰ç»†ç²’åº¦æ–‡æœ¬-è§†è§‰å…³ç³»æ–¹é¢çš„ä¸è¶³ï¼Œå¯¼è‡´å¯¹å¾®å¦™è¡¨æƒ…å·®å¼‚çš„è¯†åˆ«èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¤šæ¨¡æ€æç¤ºå¯¹é½æ¡†æ¶ï¼ˆMPA-FERï¼‰ï¼Œé€šè¿‡ç”Ÿæˆè¯¦ç»†çš„è¡¨æƒ…æè¿°å¹¶å°†å…¶ä¸è§†è§‰ç‰¹å¾å¯¹é½ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šç²’åº¦ç¡¬æç¤ºç”Ÿæˆç­–ç•¥ã€è½¯æç¤ºä¸ç¡¬æç¤ºçš„ç‰¹å¾å¯¹é½ã€åŸå‹å¼•å¯¼çš„è§†è§‰ç‰¹å¾å¯¹é½ä»¥åŠè·¨æ¨¡æ€å…¨å±€-å±€éƒ¨å¯¹é½æ¨¡å—ï¼Œç¡®ä¿æ–‡æœ¬ä¸è§†è§‰ç‰¹å¾çš„æœ‰æ•ˆå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è¯¦ç»†æè¿°ä¸è§†è§‰ç‰¹å¾çš„å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ç»†å¾®è¡¨æƒ…çš„è¯†åˆ«èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºç²¾ç¡®çš„è¯­ä¹‰æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨æœ€å°åŒ–è½¯æç¤ºä¸ç¡¬æç¤ºä¹‹é—´çš„ç‰¹å¾å·®å¼‚ï¼ŒåŒæ—¶åœ¨è§†è§‰ç‰¹å¾å¯¹é½ä¸­å¼•å…¥ç±»ç‰¹å®šåŸå‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¿æŒé¢„è®­ç»ƒä¼˜åŠ¿çš„åŒæ—¶ï¼Œå¢å¼ºäº†å¯¹ç‰¹å®šè¡¨æƒ…çš„è¯†åˆ«èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMPA-FERåœ¨ä¸‰ä¸ªFERåŸºå‡†æ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒåŒæ—¶ä¿æŒäº†é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿè®¡ç®—ã€äººæœºäº¤äº’å’Œç¤¾äº¤æœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿæå‡æœºå™¨å¯¹äººç±»æƒ…æ„Ÿçš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs) like CLIP for various downstream tasks. Despite their success, current VLM-based facial expression recognition (FER) methods struggle to capture fine-grained textual-visual relationships, which are essential for distinguishing subtle differences between facial expressions. To address this challenge, we propose a multimodal prompt alignment framework for FER, called MPA-FER, that provides fine-grained semantic guidance to the learning process of prompted visual features, resulting in more precise and interpretable representations. Specifically, we introduce a multi-granularity hard prompt generation strategy that utilizes a large language model (LLM) like ChatGPT to generate detailed descriptions for each facial expression. The LLM-based external knowledge is injected into the soft prompts by minimizing the feature discrepancy between the soft prompts and the hard prompts. To preserve the generalization abilities of the pretrained CLIP model, our approach incorporates prototype-guided visual feature alignment, ensuring that the prompted visual features from the frozen image encoder align closely with class-specific prototypes. Additionally, we propose a cross-modal global-local alignment module that focuses on expression-relevant facial features, further improving the alignment between textual and visual features. Extensive experiments demonstrate our framework outperforms state-of-the-art methods on three FER benchmark datasets, while retaining the benefits of the pretrained model and minimizing computational costs.

