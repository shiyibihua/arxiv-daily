---
layout: default
title: LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection
---

# LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21018" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21018v1</a>
  <a href="https://arxiv.org/pdf/2506.21018.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21018v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21018v1', 'LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lei Hao, Lina Xu, Chang Liu, Yanni Dong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/leileilei2000/LASFNet)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLASFNetä»¥ç®€åŒ–å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ä¸­çš„ç‰¹å¾èåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹` `ç‰¹å¾èåˆ` `è½»é‡çº§ç½‘ç»œ` `æ³¨æ„åŠ›æœºåˆ¶` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºLASFNetï¼Œé€šè¿‡å•ä¸ªç‰¹å¾çº§èåˆå•å…ƒç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å¼•å…¥ASFFæ¨¡å—è‡ªé€‚åº”è°ƒæ•´ç‰¹å¾å“åº”ã€‚
3. åœ¨ä¸‰ä¸ªä»£è¡¨æ€§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLASFNetåœ¨å‡å°‘å‚æ•°å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæ£€æµ‹å‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„æ·±åº¦ç‰¹å¾æå–é€šè¿‡ç‰¹å¾çº§èåˆå¯¹å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶é€šå¸¸æ¶‰åŠå¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé€šè¿‡å †å å¤šä¸ªç‰¹å¾çº§èåˆå•å…ƒæ¥æ•´åˆç‰¹å®šæ¨¡æ€çš„ç‰¹å¾ï¼Œå¯¼è‡´æ˜¾è‘—çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èåˆæ£€æµ‹åŸºçº¿ï¼Œä½¿ç”¨å•ä¸ªç‰¹å¾çº§èåˆå•å…ƒå®ç°é«˜æ€§èƒ½æ£€æµ‹ï¼Œä»è€Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ³¨æ„åŠ›å¼•å¯¼è‡ªè°ƒèŠ‚ç‰¹å¾èåˆç½‘ç»œï¼ˆLASFNetï¼‰ï¼Œå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ³¨æ„åŠ›å¼•å¯¼è‡ªè°ƒèŠ‚ç‰¹å¾èåˆï¼ˆASFFï¼‰æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒæ¨¡æ€çš„æ³¨æ„åŠ›ä¿¡æ¯è‡ªé€‚åº”è°ƒæ•´èåˆç‰¹å¾çš„å“åº”ï¼Œä¿ƒè¿›å…¨é¢å’Œä¸°å¯Œçš„ç‰¹å¾ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œå‚æ•°æ•°é‡å’Œè®¡ç®—æˆæœ¬åˆ†åˆ«é™ä½äº†90%å’Œ85%ï¼ŒåŒæ—¶æ£€æµ‹å‡†ç¡®ç‡æé«˜äº†1%-3%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ä¸­å¤æ‚çš„ç‰¹å¾èåˆè¿‡ç¨‹ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å †å å¤šä¸ªç‰¹å¾çº§èåˆå•å…ƒæ¥æ•´åˆæ¨¡æ€ç‰¹å¾ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºLASFNetï¼Œåˆ©ç”¨å•ä¸ªç‰¹å¾çº§èåˆå•å…ƒå®ç°é«˜æ€§èƒ½æ£€æµ‹ï¼Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œè®¾è®¡ASFFæ¨¡å—ï¼Œæ ¹æ®ä¸åŒæ¨¡æ€çš„æ³¨æ„åŠ›ä¿¡æ¯è‡ªé€‚åº”è°ƒæ•´èåˆç‰¹å¾çš„å“åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLASFNetæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–ã€ASFFæ¨¡å—ã€è½»é‡çº§ç‰¹å¾æ³¨æ„åŠ›å˜æ¢æ¨¡å—ï¼ˆFATMï¼‰ç­‰ä¸»è¦éƒ¨åˆ†ï¼Œç¡®ä¿ç‰¹å¾èåˆçš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šASFFæ¨¡å—æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼å®ç°ç‰¹å¾çš„è‡ªè°ƒèŠ‚ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾èåˆçš„æ•ˆæœï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒFATMæ¨¡å—è¢«è®¾è®¡åœ¨LASFNetçš„é¢ˆéƒ¨ï¼Œä»¥å¢å¼ºå¯¹èåˆç‰¹å¾çš„å…³æ³¨ï¼Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–ç½‘ç»œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLASFNetåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æ£€æµ‹å‡†ç¡®ç‡ï¼ˆmAPï¼‰æé«˜äº†1%-3%ï¼ŒåŒæ—¶å‚æ•°æ•°é‡å’Œè®¡ç®—æˆæœ¬åˆ†åˆ«é™ä½äº†90%å’Œ85%ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒLASFNetåœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´å®ç°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§å’Œæœºå™¨äººè§†è§‰ç­‰å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹åœºæ™¯ã€‚é€šè¿‡æé«˜æ£€æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒLASFNetèƒ½å¤Ÿåœ¨å®æ—¶åº”ç”¨ä¸­æä¾›æ›´å¥½çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective deep feature extraction via feature-level fusion is crucial for multimodal object detection. However, previous studies often involve complex training processes that integrate modality-specific features by stacking multiple feature-level fusion units, leading to significant computational overhead. To address this issue, we propose a new fusion detection baseline that uses a single feature-level fusion unit to enable high-performance detection, thereby simplifying the training process. Based on this approach, we propose a lightweight attention-guided self-modulation feature fusion network (LASFNet), which introduces a novel attention-guided self-modulation feature fusion (ASFF) module that adaptively adjusts the responses of fusion features at both global and local levels based on attention information from different modalities, thereby promoting comprehensive and enriched feature generation. Additionally, a lightweight feature attention transformation module (FATM) is designed at the neck of LASFNet to enhance the focus on fused features and minimize information loss. Extensive experiments on three representative datasets demonstrate that, compared to state-of-the-art methods, our approach achieves a favorable efficiency-accuracy trade-off, reducing the number of parameters and computational cost by as much as 90% and 85%, respectively, while improving detection accuracy (mAP) by 1%-3%. The code will be open-sourced at https://github.com/leileilei2000/LASFNet.

