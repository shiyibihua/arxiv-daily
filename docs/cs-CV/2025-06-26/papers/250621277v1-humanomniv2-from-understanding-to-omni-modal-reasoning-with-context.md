---
layout: default
title: HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context
---

# HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21277" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21277v1</a>
  <a href="https://arxiv.org/pdf/2506.21277.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21277v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21277v1', 'HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, Jingren Zhou

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HumanOmniV2‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊé®ÁêÜ` `‰∏ä‰∏ãÊñáÁêÜËß£` `Âº∫ÂåñÂ≠¶‰π†` `ÈÄªËæëÊé®ÁêÜ` `‰∫∫Á±ªÊÑèÂõæÁêÜËß£` `ÊÉÖÊÑüÂàÜÊûê` `ÂÖ®Ê®°ÊÄÅÂü∫ÂáÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÂú®ÂÖ®ÁêÉ‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÂ§ÑÁêÜÊç∑ÂæÑÈóÆÈ¢ò‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥ÈîôËØØÁ≠îÊ°àÂíå‰ø°ÊÅØÈÅóÊºè„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÁªìÂêà‰∏ä‰∏ãÊñáÂ•ñÂä±ÂíåÈÄªËæëÂ•ñÂä±ÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπÂ§öÊ®°ÊÄÅËæìÂÖ•ÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§ö‰∏™ÂÖ®Ê®°ÊÄÅÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂºÄÊ∫êÂÖ®Ê®°ÊÄÅÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÊ∑±ÂÖ•ÁêÜËß£ÂíåËß£Èáä‰∫∫Á±ªÊÑèÂõæÊàê‰∏∫‰∏ÄÈ°πÂÖ≥ÈîÆËÉΩÂäõÔºåË¶ÅÊ±ÇËøõË°åËØ¶ÁªÜÂíåÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜ„ÄÇÁé∞ÊúâÁ†îÁ©∂Ë°®ÊòéÔºåÂº∫ÂåñÂ≠¶‰π†Âú®Â¢ûÂº∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÂÖ∑ÊúâÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÈÄÇÂ∫î‰∫éÂ§öÊ®°ÊÄÅÊï∞ÊçÆÂíåÊ†ºÂºèÁöÑÊåëÊàò‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜËß£ÂÜ≥„ÄÇÊú¨ÊñáËØÜÂà´‰∫ÜÁé∞ÊúâÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑ‰∏§‰∏™ÈóÆÈ¢òÔºöÂÖ®ÁêÉ‰∏ä‰∏ãÊñáÁêÜËß£‰∏çË∂≥ÂíåÊç∑ÂæÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÂº∫Ë∞ÉÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅËæìÂÖ•‰∏≠ËøõË°åÊé®ÁêÜÊó∂ÂøÖÈ°ªÊ∏ÖÊô∞ÁêÜËß£ÂÖ®ÁêÉ‰∏ä‰∏ãÊñá„ÄÇÊàë‰ª¨ÂÆûÁé∞‰∫ÜÁî±Â§ßËØ≠Ë®ÄÊ®°ÂûãÂà§Êñ≠ÁöÑ‰∏ä‰∏ãÊñáÂ•ñÂä±Ôºå‰ª•ÂèäÊ†ºÂºèÂíåÂáÜÁ°ÆÊÄßÂ•ñÂä±Ôºå‰ª•Á°Æ‰øùÂØπÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñá‰ø°ÊÅØÁöÑÂáÜÁ°ÆËß£Èáä„ÄÇÊ≠§Â§ñÔºå‰∏∫ÊèêÈ´òÂ§çÊùÇÊé®ÁêÜËÉΩÂäõÔºåÊàë‰ª¨Âà©Áî®Â§ßËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÈÄªËæëÂ•ñÂä±ÔºåÂà§Êñ≠Êé®ÁêÜËøáÁ®ãÊòØÂê¶ÊàêÂäüÊï¥Âêà‰∫ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êé®ÁêÜÂÖ®Ê®°ÊÄÅÂü∫ÂáÜIntentBenchÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÁêÜËß£Â§çÊùÇ‰∫∫Á±ªÊÑèÂõæÂíåÊÉÖÊÑüÁöÑËÉΩÂäõ„ÄÇ‰∏éÂÖ∂‰ªñÂºÄÊ∫êÂÖ®Ê®°ÊÄÅÊ®°ÂûãÁõ∏ÊØîÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ÂÖ®Ê®°ÊÄÅÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÂú®ÂÖ®ÁêÉ‰∏ä‰∏ãÊñáÁêÜËß£‰∏çË∂≥ÂíåÊç∑ÂæÑÈóÆÈ¢òÔºåÂØºËá¥Ê®°ÂûãÂú®Â§ÑÁêÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊó∂‰∫ßÁîüÈîôËØØÁ≠îÊ°àÂíåÈÅóÊºèÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑÁóõÁÇπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•‰∏ä‰∏ãÊñáÂ•ñÂä±ÂíåÈÄªËæëÂ•ñÂä±ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËÉΩÂ§üÂÖ®Èù¢ÁêÜËß£Â§öÊ®°ÊÄÅËæìÂÖ•ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÊ∑±Â∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰∏ä‰∏ãÊñáÁêÜËß£Ê®°Âùó„ÄÅÂ•ñÂä±ËØÑ‰º∞Ê®°ÂùóÂíåÊé®ÁêÜÊ®°Âùó„ÄÇ‰∏ä‰∏ãÊñáÁêÜËß£Ê®°ÂùóË¥üË¥£Ëß£ÊûêÂ§öÊ®°ÊÄÅËæìÂÖ•ÔºåÂ•ñÂä±ËØÑ‰º∞Ê®°ÂùóÊ†πÊçÆ‰∏ä‰∏ãÊñáÂíåÈÄªËæëËøõË°åËØÑÂàÜÔºåÊé®ÁêÜÊ®°ÂùóÂàôÊâßË°åÊúÄÁªàÁöÑÊé®ÁêÜ‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫Ü‰∏ä‰∏ãÊñáÂ•ñÂä±ÂíåÈÄªËæëÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Êé®ÁêÜÊó∂‰∏ç‰ªÖÂÖ≥Ê≥®Áõ¥Êé•Á≠îÊ°àÔºåËøòËÉΩÁªºÂêàËÄÉËôëÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÂÖ®Â±Ä‰∏ä‰∏ãÊñá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫‰∏ä‰∏ãÊñáÂ•ñÂä±ÁöÑËØÑ‰º∞Ê†áÂáÜÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÂºïÂÖ•‰∫ÜÂ§öÁßçÂ•ñÂä±Êú∫Âà∂Ôºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËÉΩÂ§üÊúâÊïàÊï¥ÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåËÆ≠ÁªÉÁªÜËäÇÂú®ÂÆûÈ™åÈÉ®ÂàÜËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑHumanOmniV2Âú®Â§ö‰∏™ÂÖ®Ê®°ÊÄÅÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÂÖ∂‰ªñÂºÄÊ∫êÂÖ®Ê®°ÊÄÅÊ®°ÂûãÔºåÊé®ÁêÜÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü15%‰ª•‰∏äÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇ‰∫∫Á±ªÊÑèÂõæÁêÜËß£ÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∫∫Êú∫‰∫§‰∫í„ÄÅÊÉÖÊÑüÂàÜÊûê„ÄÅÊô∫ËÉΩÂÆ¢ÊúçÁ≠âÔºåËÉΩÂ§üÂ∏ÆÂä©Á≥ªÁªüÊõ¥Â•ΩÂú∞ÁêÜËß£Áî®Êà∑ÁöÑÂ§çÊùÇÊÑèÂõæÂíåÊÉÖÊÑüÔºå‰ªéËÄåÊèê‰æõÊõ¥‰∏∫Á≤æÂáÜÁöÑÂìçÂ∫îÂíåÊúçÂä°„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÂú®Â§öÊ®°ÊÄÅÊï∞ÊçÆÂ§ÑÁêÜÂíåÊé®ÁêÜÈ¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.

