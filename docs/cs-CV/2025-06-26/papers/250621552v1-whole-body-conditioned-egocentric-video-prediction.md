---
layout: default
title: Whole-Body Conditioned Egocentric Video Prediction
---

# Whole-Body Conditioned Egocentric Video Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21552" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21552v1</a>
  <a href="https://arxiv.org/pdf/2506.21552.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21552v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21552v1', 'Whole-Body Conditioned Egocentric Video Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.MM, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Project Page: https://dannytran123.github.io/PEVA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå…¨èº«æ¡ä»¶çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é¢„æµ‹ä»¥è§£å†³ç¯å¢ƒå»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é¢„æµ‹` `è¿åŠ¨å­¦å§¿æ€` `æ¡ä»¶æ‰©æ•£å˜æ¢å™¨` `å…·èº«ä»£ç†` `å¤æ‚ç¯å¢ƒå»ºæ¨¡` `è™šæ‹Ÿç°å®` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒå»ºæ¨¡å’Œå…·èº«ä»£ç†è¡Œä¸ºé¢„æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰äººç±»åŠ¨ä½œå¯¹ç¯å¢ƒçš„å½±å“ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è¿åŠ¨å­¦å§¿æ€è½¨è¿¹å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œåˆ©ç”¨è‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨è¿›è¡Œè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é¢„æµ‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤šé¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å…·èº«é¢„æµ‹å’Œæ§åˆ¶èƒ½åŠ›ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è®­ç»ƒæ¨¡å‹ä»¥ä»äººç±»åŠ¨ä½œä¸­é¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ï¼ˆPEVAï¼‰ï¼Œé€šè¿‡ç›¸å¯¹3Dèº«ä½“å§¿æ€è¡¨ç¤ºçš„åŠ¨ä½œï¼Œç»“åˆè¿åŠ¨å­¦å§¿æ€è½¨è¿¹ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•ä»ç¬¬ä¸€äººç§°è§†è§’æ¨¡æ‹Ÿäººç±»åŠ¨ä½œå¯¹ç¯å¢ƒçš„å½±å“ã€‚æˆ‘ä»¬åœ¨Nymeriaæ•°æ®é›†ä¸Šè®­ç»ƒäº†è‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨ï¼Œè®¾è®¡äº†åˆ†å±‚è¯„ä¼°åè®®ä»¥åˆ†ææ¨¡å‹çš„é¢„æµ‹å’Œæ§åˆ¶èƒ½åŠ›ã€‚æ­¤ç ”ç©¶é¦–æ¬¡å°è¯•ä»äººç±»è§†è§’å»ºæ¨¡å¤æ‚çš„ç°å®ç¯å¢ƒå’Œå…·èº«ä»£ç†è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¦‚ä½•ä»äººç±»åŠ¨ä½œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒå’Œå…·èº«è¡Œä¸ºå»ºæ¨¡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰äººç±»åŠ¨ä½œå¯¹ç¯å¢ƒçš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿åŠ¨å­¦å§¿æ€è½¨è¿¹å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ äººç±»åŠ¨ä½œå¦‚ä½•å½±å“ç¯å¢ƒã€‚è‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ—¶é—´åºåˆ—ä¸Šè¿›è¡Œæœ‰æ•ˆé¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œåˆ©ç”¨Nymeriaæ•°æ®é›†è¿›è¡Œæ•°æ®æ”¶é›†å’Œå¤„ç†ï¼›å…¶æ¬¡ï¼Œè®­ç»ƒè‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨ï¼›æœ€åï¼Œé‡‡ç”¨åˆ†å±‚è¯„ä¼°åè®®å¯¹æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡è¿åŠ¨å­¦å§¿æ€è½¨è¿¹å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œæå‡äº†æ¨¡å‹å¯¹å¤æ‚ç¯å¢ƒå’Œäººç±»åŠ¨ä½œçš„ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰åŸºäºé™æ€è¾“å…¥çš„é¢„æµ‹æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–é¢„æµ‹ç²¾åº¦ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å¤šå±‚æ¬¡çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå¯¹æ—¶é—´åºåˆ—ä¿¡æ¯çš„æ•æ‰èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œé¢„æµ‹ç²¾åº¦æå‡äº†çº¦15%ã€‚åˆ†å±‚è¯„ä¼°åè®®çš„è®¾è®¡ä½¿å¾—å¯¹æ¨¡å‹èƒ½åŠ›çš„åˆ†ææ›´åŠ å…¨é¢ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡å‡†ç¡®é¢„æµ‹äººç±»åŠ¨ä½œå¯¹ç¯å¢ƒçš„å½±å“ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ï¼Œæ¨åŠ¨æ™ºèƒ½ä»£ç†çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.

