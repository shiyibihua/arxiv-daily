---
layout: default
title: Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition
---

# Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21165" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21165v1</a>
  <a href="https://arxiv.org/pdf/2506.21165.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21165v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21165v1', 'Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Longkun Zou, Kangjun Liu, Ke Chen, Kailing Guo, Kui Jia, Yaowei Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zou-longkun/TAG.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡ä»¥è§£å†³æ— ç›‘ç£ä»¿çœŸåˆ°ç°å®ç‚¹äº‘è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡` `æ— ç›‘ç£é¢†åŸŸé€‚åº”` `ä»¿çœŸåˆ°ç°å®` `ç‚¹äº‘è¯†åˆ«` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `3Dç‰©ä½“è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•åœ¨ä»¿çœŸåˆ°ç°å®çš„é¢†åŸŸå·®è·ä¸Šè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ç¨³å¥çš„å…¨å±€æ‹“æ‰‘ä¿¡æ¯æ•æ‰èƒ½åŠ›ã€‚
2. æå‡ºçš„æ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡æ¡†æ¶é€šè¿‡å»ºæ¨¡å±€éƒ¨å‡ ä½•ç‰¹å¾çš„æ‹“æ‰‘å…³ç³»ï¼Œåˆ©ç”¨å…¨å±€ç©ºé—´æ‹“æ‰‘æ¥å‡è½»é¢†åŸŸå·®è·ã€‚
3. åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºTAMæ¡†æ¶åœ¨å„é¡¹ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»3Dç‰©ä½“å½¢çŠ¶çš„ç‚¹é›†å­¦ä¹ è¯­ä¹‰è¡¨ç¤ºé¢ä¸´æ˜¾è‘—çš„å‡ ä½•å˜åŒ–æŒ‘æˆ˜ï¼Œä¸»è¦æºäºæ•°æ®é‡‡é›†æ–¹æ³•çš„å·®å¼‚ã€‚è®­ç»ƒæ•°æ®é€šå¸¸é€šè¿‡ç‚¹æ¨¡æ‹Ÿå™¨ç”Ÿæˆï¼Œè€Œæµ‹è¯•æ•°æ®åˆ™ä½¿ç”¨ä¸åŒçš„3Dä¼ æ„Ÿå™¨æ”¶é›†ï¼Œå¯¼è‡´ä»¿çœŸåˆ°ç°å®çš„é¢†åŸŸå·®è·ï¼Œé™åˆ¶äº†ç‚¹åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”æŠ€æœ¯åœ¨è¿™ä¸€å·®è·ä¸Šè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹èƒ½å¤Ÿæ•æ‰å…¨å±€æ‹“æ‰‘ä¿¡æ¯çš„ç¨³å¥ã€é¢†åŸŸæ— å…³çš„æè¿°ç¬¦ï¼Œå¯¼è‡´å¯¹æºé¢†åŸŸæœ‰é™è¯­ä¹‰æ¨¡å¼çš„è¿‡æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡æ¡†æ¶ï¼ˆTAMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨ä½çº§é«˜é¢‘3Dç»“æ„çš„å…¨å±€ç©ºé—´æ‹“æ‰‘ï¼Œå»ºæ¨¡å±€éƒ¨å‡ ä½•ç‰¹å¾çš„æ‹“æ‰‘å…³ç³»ï¼Œæ¥å‡è½»é¢†åŸŸå·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å…ˆè¿›çš„è‡ªæˆ‘è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè·¨é¢†åŸŸå¯¹æ¯”å­¦ä¹ ä¸è‡ªæˆ‘è®­ç»ƒï¼Œæœ‰æ•ˆå‡å°‘å™ªå£°ä¼ªæ ‡ç­¾çš„å½±å“ï¼Œå¢å¼ºé€‚åº”è¿‡ç¨‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœåœ¨ä¸‰ä¸ªå…¬å…±çš„Sim2RealåŸºå‡†ä¸ŠéªŒè¯äº†æˆ‘ä»¬TAMæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä»3Dç‰©ä½“ç‚¹äº‘ä¸­è¿›è¡Œæ— ç›‘ç£ä»¿çœŸåˆ°ç°å®çš„è¯†åˆ«é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é¢†åŸŸå·®è·æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰å…¨å±€æ‹“æ‰‘ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹å¯¹æºé¢†åŸŸçš„è¿‡æ‹Ÿåˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡æ¡†æ¶ï¼ˆTAMï¼‰é€šè¿‡å¼•å…¥å…¨å±€ç©ºé—´æ‹“æ‰‘å’Œå±€éƒ¨å‡ ä½•ç‰¹å¾çš„æ‹“æ‰‘å…³ç³»ï¼Œæ¥å‡è½»ä»¿çœŸä¸ç°å®ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ä¸åŒæ•°æ®é‡‡é›†æ–¹æ³•ä¸‹ä»èƒ½æœ‰æ•ˆè¯†åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTAMæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šä¸€æ˜¯é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡å»ºæ¨¡å±€éƒ¨å‡ ä½•ç‰¹å¾çš„æ‹“æ‰‘å…³ç³»ï¼ŒäºŒæ˜¯ç»“åˆè·¨é¢†åŸŸå¯¹æ¯”å­¦ä¹ ä¸è‡ªæˆ‘è®­ç»ƒçš„ç­–ç•¥æ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šæ•°æ®é¢„å¤„ç† â†’ æ‹“æ‰‘ç‰¹å¾æå– â†’ è‡ªç›‘ç£å­¦ä¹  â†’ é€‚åº”æ€§è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å…¨å±€ç©ºé—´æ‹“æ‰‘å’Œå±€éƒ¨å‡ ä½•ç‰¹å¾çš„æ‹“æ‰‘å…³ç³»å»ºæ¨¡ï¼Œæ˜¾è‘—æé«˜äº†å¯¹é¢†åŸŸå·®è·çš„é€‚åº”èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç‰¹å¾æå–æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰3Då½¢çŠ¶çš„å‡ ä½•ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡ç‰¹å¾æå–æ¨¡å—ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ‹“æ‰‘ç‰¹å¾çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œä¼ªæ ‡ç­¾çš„ç”Ÿæˆä¸ç­›é€‰ç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥å‡å°‘å™ªå£°å¯¹è®­ç»ƒçš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTAMæ¡†æ¶åœ¨ä¸‰ä¸ªå…¬å…±Sim2RealåŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡3Dç‰©ä½“è¯†åˆ«çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œæ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning semantic representations from point sets of 3D object shapes is often challenged by significant geometric variations, primarily due to differences in data acquisition methods. Typically, training data is generated using point simulators, while testing data is collected with distinct 3D sensors, leading to a simulation-to-reality (Sim2Real) domain gap that limits the generalization ability of point classifiers. Current unsupervised domain adaptation (UDA) techniques struggle with this gap, as they often lack robust, domain-insensitive descriptors capable of capturing global topological information, resulting in overfitting to the limited semantic patterns of the source domain. To address this issue, we introduce a novel Topology-Aware Modeling (TAM) framework for Sim2Real UDA on object point clouds. Our approach mitigates the domain gap by leveraging global spatial topology, characterized by low-level, high-frequency 3D structures, and by modeling the topological relations of local geometric features through a novel self-supervised learning task. Additionally, we propose an advanced self-training strategy that combines cross-domain contrastive learning with self-training, effectively reducing the impact of noisy pseudo-labels and enhancing the robustness of the adaptation process. Experimental results on three public Sim2Real benchmarks validate the effectiveness of our TAM framework, showing consistent improvements over state-of-the-art methods across all evaluated tasks. The source code of this work will be available at https://github.com/zou-longkun/TAG.git.

