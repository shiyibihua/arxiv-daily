---
layout: default
title: GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction
---

# GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21121" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21121v1</a>
  <a href="https://arxiv.org/pdf/2506.21121.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21121v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21121v1', 'GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muleilan Pei, Shaoshuai Shi, Lu Zhang, Peiliang Li, Shaojie Shen

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Accepted by ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGoIRLæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `è½¨è¿¹é¢„æµ‹` `å¤šæ¨¡æ€` `å›¾ç»“æ„` `è‡ªåŠ¨é©¾é©¶` `ç‰¹å¾é€‚é…` `æœ€å¤§ç†µ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è½¨è¿¹é¢„æµ‹æ–¹æ³•å¤šä¾èµ–äºç›‘ç£å­¦ä¹ ï¼Œéš¾ä»¥å¤„ç†å¤šæ¨¡æ€å’Œä¸ç¡®å®šæ€§é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„GoIRLæ¡†æ¶é€šè¿‡é€†å¼ºåŒ–å­¦ä¹ å’Œå›¾ç»“æ„ç‰¹å¾è¡¨ç¤ºï¼Œå¢å¼ºäº†è½¨è¿¹é¢„æµ‹çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGoIRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå‘¨å›´ä»£ç†çš„è½¨è¿¹é¢„æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› å…¶å›ºæœ‰çš„ä¸ç¡®å®šæ€§å’Œå¤šæ¨¡æ€ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾å¯¼å‘é€†å¼ºåŒ–å­¦ä¹ ï¼ˆGoIRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å‘é‡åŒ–çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç‰¹å¾é€‚é…å™¨ï¼Œæœ‰æ•ˆåœ°å°†è½¦é“å›¾ç‰¹å¾èšåˆåˆ°ç½‘æ ¼ç©ºé—´ä¸­ï¼Œèƒ½å¤Ÿä¸æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ èŒƒå¼æ— ç¼é›†æˆï¼Œä»è€Œæ¨æ–­å¥–åŠ±åˆ†å¸ƒå¹¶è·å¾—å¯ç”¨äºç”Ÿæˆå¤šç§åˆç†è®¡åˆ’çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒåŸºäºé‡‡æ ·çš„è®¡åˆ’ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåˆ†å±‚å‚æ•°åŒ–çš„è½¨è¿¹ç”Ÿæˆå™¨ï¼Œå¹¶å¼•å…¥äº†ç²¾ç»†åŒ–æ¨¡å—ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œæ¦‚ç‡èåˆç­–ç•¥ä»¥å¢å¼ºé¢„æµ‹ä¿¡å¿ƒã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§è§„æ¨¡çš„Argoverseå’ŒnuScenesè¿åŠ¨é¢„æµ‹åŸºå‡†ä¸Šä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿˜å±•ç°äº†ä¼˜äºç°æœ‰ç›‘ç£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­å‘¨å›´ä»£ç†çš„è½¨è¿¹é¢„æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç›‘ç£å­¦ä¹ ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€å’Œä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGoIRLæ¡†æ¶é€šè¿‡å¼•å…¥é€†å¼ºåŒ–å­¦ä¹ å’Œå›¾å¯¼å‘ç‰¹å¾è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è½¨è¿¹çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œä»è€Œç”Ÿæˆæ›´ä¸ºåˆç†çš„è½¨è¿¹é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ç‰¹å¾é€‚é…å™¨ã€æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ æ¨¡å—ã€åˆ†å±‚å‚æ•°åŒ–è½¨è¿¹ç”Ÿæˆå™¨å’Œæ¦‚ç‡èåˆç­–ç•¥ç­‰ä¸»è¦æ¨¡å—ï¼Œæ•´ä½“æµç¨‹ä¸ºç‰¹å¾æå–ã€å¥–åŠ±æ¨æ–­ã€ç­–ç•¥ç”Ÿæˆå’Œè½¨è¿¹ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šGoIRLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å›¾ç»“æ„ç‰¹å¾ä¸é€†å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨æ–­å¥–åŠ±åˆ†å¸ƒå¹¶ç”Ÿæˆå¤šç§åˆç†çš„è½¨è¿¹è®¡åˆ’ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œç‰¹å¾é€‚é…å™¨è´Ÿè´£å°†è½¦é“å›¾ç‰¹å¾èšåˆåˆ°ç½‘æ ¼ç©ºé—´ï¼Œæœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ ç”¨äºæ¨æ–­å¥–åŠ±åˆ†å¸ƒï¼Œåˆ†å±‚å‚æ•°åŒ–è½¨è¿¹ç”Ÿæˆå™¨åˆ™é€šè¿‡ç²¾ç»†åŒ–æ¨¡å—å’Œæ¦‚ç‡èåˆç­–ç•¥æå‡é¢„æµ‹çš„å‡†ç¡®æ€§å’Œä¿¡å¿ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤§è§„æ¨¡çš„Argoverseå’ŒnuScenesè¿åŠ¨é¢„æµ‹åŸºå‡†ä¸Šï¼ŒGoIRLæ¡†æ¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜è½¨è¿¹é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ŒGoIRLæ¡†æ¶èƒ½å¤Ÿä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›æ›´å¥½çš„å†³ç­–æ”¯æŒï¼Œè¿›è€Œæå‡è¡Œè½¦å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤šæ¨¡æ€é¢„æµ‹çš„é¢†åŸŸï¼Œå¦‚æœºå™¨äººå¯¼èˆªå’Œäººç±»è¡Œä¸ºé¢„æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Trajectory prediction for surrounding agents is a challenging task in autonomous driving due to its inherent uncertainty and underlying multimodality. Unlike prevailing data-driven methods that primarily rely on supervised learning, in this paper, we introduce a novel Graph-oriented Inverse Reinforcement Learning (GoIRL) framework, which is an IRL-based predictor equipped with vectorized context representations. We develop a feature adaptor to effectively aggregate lane-graph features into grid space, enabling seamless integration with the maximum entropy IRL paradigm to infer the reward distribution and obtain the policy that can be sampled to induce multiple plausible plans. Furthermore, conditioned on the sampled plans, we implement a hierarchical parameterized trajectory generator with a refinement module to enhance prediction accuracy and a probability fusion strategy to boost prediction confidence. Extensive experimental results showcase our approach not only achieves state-of-the-art performance on the large-scale Argoverse & nuScenes motion forecasting benchmarks but also exhibits superior generalization abilities compared to existing supervised models.

