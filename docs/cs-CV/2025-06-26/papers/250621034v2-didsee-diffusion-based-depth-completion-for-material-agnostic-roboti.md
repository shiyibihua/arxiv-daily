---
layout: default
title: DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation
---

# DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21034" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21034v2</a>
  <a href="https://arxiv.org/pdf/2506.21034.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21034v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21034v2', 'DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenzhou Lyu, Jialing Lin, Wenqi Ren, Ruihao Xia, Feng Qian, Yang Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-26 (Êõ¥Êñ∞: 2025-06-27)

**Â§áÊ≥®**: Project page: https://wenzhoulyu.github.io/DidSee/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DidSee‰ª•Ëß£ÂÜ≥ÈùûÊúó‰ºØÁâ©‰ΩìÁöÑÊ∑±Â∫¶Ë°•ÂÖ®ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Ë°•ÂÖ®` `Êâ©Êï£Ê®°Âûã` `ÈùûÊúó‰ºØÁâ©‰Ωì` `Êú∫Âô®‰∫∫ÊÑüÁü•` `ËØ≠‰πâÂàÜÂâ≤` `Âô™Â£∞Ë∞ÉÂ∫¶` `‰ªªÂä°ÁâπÂÆöÊçüÂ§±` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊ∑±Â∫¶Ë°•ÂÖ®ÊñπÊ≥ïÂú®Â§ÑÁêÜÈùûÊúó‰ºØÁâ©‰ΩìÊó∂ÔºåÂõ†ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß‰∏çË∂≥ËÄåÈöæ‰ª•Ê≥õÂåñÔºåÂØºËá¥ÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫DidSeeÊ°ÜÊû∂ÔºåÈÄöËøáÂºïÂÖ•Êñ∞ÁöÑÂô™Â£∞Ë∞ÉÂ∫¶Âô®ÂíåÊó†Âô™Â£∞ËÆ≠ÁªÉÊñπÊ°àÔºåËß£ÂÜ≥‰∫Ü‰ø°Âè∑Ê≥ÑÊºèÂíåËØØÂ∑ÆÁ¥ØÁßØÈóÆÈ¢ò„ÄÇ
3. DidSeeÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëóÊèêÂçá‰∫ÜÁ±ªÂà´Á∫ßÂßøÊÄÅ‰º∞ËÆ°ÂíåÊú∫Âô®‰∫∫ÊäìÂèñÁ≠â‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂïÜ‰∏öRGB-DÁõ∏Êú∫Âú®Â§ÑÁêÜÈùûÊúó‰ºØÁâ©‰ΩìÊó∂Â∏∏Â∏∏‰∫ßÁîüÂô™Â£∞Âíå‰∏çÂÆåÊï¥ÁöÑÊ∑±Â∫¶Âõæ„ÄÇ‰º†ÁªüÁöÑÊ∑±Â∫¶Ë°•ÂÖ®ÊñπÊ≥ïÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåËßÑÊ®°ÊúâÈôêÔºåÈöæ‰ª•ÂÆûÁé∞ËâØÂ•ΩÁöÑÊ≥õÂåñ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜDidSeeÔºå‰∏Ä‰∏™Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊ∑±Â∫¶Ë°•ÂÖ®Ê°ÜÊû∂„ÄÇÈÄöËøáÂºïÂÖ•ÈáçÊñ∞Áº©ÊîæÁöÑÂô™Â£∞Ë∞ÉÂ∫¶Âô®ÂíåÊó†Âô™Â£∞ÂçïÊ≠•ËÆ≠ÁªÉÊñπÊ°àÔºåDidSeeÊúâÊïàÊ∂àÈô§‰∫Ü‰ø°Âè∑Ê≥ÑÊºèÂÅèÂ∑ÆÔºåÂπ∂‰ºòÂåñ‰∫ÜÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÁªìÂêàËØ≠‰πâÂ¢ûÂº∫Ê®°ÂùóÔºåDidSeeËÉΩÂ§üÂÆûÁé∞Ê∑±Â∫¶Ë°•ÂÖ®‰∏éËØ≠‰πâÂàÜÂâ≤ÁöÑËÅîÂêàÔºåÁîüÊàêÁ≤æÁ°ÆÁöÑÊ∑±Â∫¶Âõæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDidSeeÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÁé∞ÂÆû‰∏ñÁïåÊ≥õÂåñËÉΩÂäõÔºåÂπ∂ÊòæËëóÊèêÂçá‰∫Ü‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂïÜ‰∏öRGB-DÁõ∏Êú∫Âú®Â§ÑÁêÜÈùûÊúó‰ºØÁâ©‰ΩìÊó∂‰∫ßÁîüÁöÑÂô™Â£∞Âíå‰∏çÂÆåÊï¥Ê∑±Â∫¶ÂõæÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåËßÑÊ®°ÊúâÈôêÔºåÈöæ‰ª•ÂÆûÁé∞ÊúâÊïàÁöÑÊ∑±Â∫¶Ë°•ÂÖ®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDidSeeÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•ÈáçÊñ∞Áº©ÊîæÁöÑÂô™Â£∞Ë∞ÉÂ∫¶Âô®ÂíåÊó†Âô™Â£∞ÂçïÊ≠•ËÆ≠ÁªÉÊñπÊ°àÔºåÊ∂àÈô§‰∫Ü‰ø°Âè∑Ê≥ÑÊºèÂÅèÂ∑ÆÔºåÂπ∂‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ãÔºå‰ª•ÊèêÈ´òÊ∑±Â∫¶Ë°•ÂÖ®ÁöÑÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDidSeeÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂô™Â£∞Ë∞ÉÂ∫¶Âô®„ÄÅÊó†Âô™Â£∞ËÆ≠ÁªÉÊ®°ÂùóÂíåËØ≠‰πâÂ¢ûÂº∫Ê®°Âùó„ÄÇÂô™Â£∞Ë∞ÉÂ∫¶Âô®Áî®‰∫éÊéßÂà∂‰ø°Âè∑‰∏éÂô™Â£∞ÁöÑÊØî‰æãÔºåÊó†Âô™Â£∞ËÆ≠ÁªÉÊ®°ÂùóÂàôÈÄöËøáÁâπÂÆöÊçüÂ§±ÂáΩÊï∞‰ºòÂåñÊ®°ÂûãÔºåËØ≠‰πâÂ¢ûÂº∫Ê®°ÂùóÂÆûÁé∞Ê∑±Â∫¶Ë°•ÂÖ®‰∏éËØ≠‰πâÂàÜÂâ≤ÁöÑËÅîÂêà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDidSeeÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÈáçÊñ∞Áº©ÊîæÁöÑÂô™Â£∞Ë∞ÉÂ∫¶Âô®ÂíåÊó†Âô™Â£∞ÂçïÊ≠•ËÆ≠ÁªÉÊñπÊ°àÔºåËøô‰∏é‰º†ÁªüÁöÑÊâ©Êï£Ê®°ÂûãÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´ÔºåËÉΩÂ§üÊúâÊïàÂáèÂ∞ëËÆ≠ÁªÉ‰∏éÊé®ÁêÜ‰πãÈó¥ÁöÑÂÅèÂ∑Æ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåDidSeeÈááÁî®‰∫ÜÈõ∂ÁªàÁ´Ø‰ø°Âô™ÊØîÁöÑÂô™Â£∞Ë∞ÉÂ∫¶Âô®ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰ªªÂä°ÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÊ®°ÂûãÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ≠‰πâÂ¢ûÂº∫Ê®°ÂùóÈÄöËøáÂØπËÉåÊôØ‰∏éÁâ©‰ΩìÁöÑÂå∫ÂàÜÔºåÊèêÂçá‰∫ÜÊ∑±Â∫¶ÂõæÁöÑÁ≤æÁªÜÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåDidSeeÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊ∑±Â∫¶Ë°•ÂÖ®Á≤æÂ∫¶ÊèêÂçá‰∫ÜXX%ÔºåÂú®Á±ªÂà´Á∫ßÂßøÊÄÅ‰º∞ËÆ°ÂíåÊú∫Âô®‰∫∫ÊäìÂèñ‰ªªÂä°‰∏≠Ë°®Áé∞Â∞§‰∏∫Á™ÅÂá∫ÔºåÂ±ïÁ§∫‰∫ÜËâØÂ•ΩÁöÑÁé∞ÂÆû‰∏ñÁïåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DidSeeÊ°ÜÊû∂Âú®Êú∫Âô®‰∫∫ÊÑüÁü•ÂíåÊìç‰ΩúÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈùûÊúó‰ºØÁâ©‰ΩìÊó∂„ÄÇÂÖ∂Á≤æÁ°ÆÁöÑÊ∑±Â∫¶Ë°•ÂÖ®ËÉΩÂäõÂèØ‰ª•ÊòæËëóÊèêÂçáÊú∫Âô®‰∫∫Âú®ÊäìÂèñ„ÄÅÂØºËà™Âíå‰∫§‰∫íÁ≠â‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Commercial RGB-D cameras often produce noisy, incomplete depth maps for non-Lambertian objects. Traditional depth completion methods struggle to generalize due to the limited diversity and scale of training data. Recent advances exploit visual priors from pre-trained text-to-image diffusion models to enhance generalization in dense prediction tasks. However, we find that biases arising from training-inference mismatches in the vanilla diffusion framework significantly impair depth completion performance. Additionally, the lack of distinct visual features in non-Lambertian regions further hinders precise prediction. To address these issues, we propose \textbf{DidSee}, a diffusion-based framework for depth completion on non-Lambertian objects. First, we integrate a rescaled noise scheduler enforcing a zero terminal signal-to-noise ratio to eliminate signal leakage bias. Second, we devise a noise-agnostic single-step training formulation to alleviate error accumulation caused by exposure bias and optimize the model with a task-specific loss. Finally, we incorporate a semantic enhancer that enables joint depth completion and semantic segmentation, distinguishing objects from backgrounds and yielding precise, fine-grained depth maps. DidSee achieves state-of-the-art performance on multiple benchmarks, demonstrates robust real-world generalization, and effectively improves downstream tasks such as category-level pose estimation and robotic grasping.

