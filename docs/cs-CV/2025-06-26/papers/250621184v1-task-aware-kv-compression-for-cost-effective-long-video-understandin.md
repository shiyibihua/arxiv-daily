---
layout: default
title: Task-Aware KV Compression For Cost-Effective Long Video Understanding
---

# Task-Aware KV Compression For Cost-Effective Long Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21184" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21184v1</a>
  <a href="https://arxiv.org/pdf/2506.21184.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21184v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21184v1', 'Task-Aware KV Compression For Cost-Effective Long Video Understanding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Minghao Qin, Yan Shu, Peitian Zhang, Kun Lun, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-26

**Â§áÊ≥®**: 14 pages, 3 figures, 6 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Video-X^2L‰ª•Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑKVÂéãÁº©ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `KVÂéãÁº©` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `‰ø°ÊÅØ‰øùÁïô` `ËÆ°ÁÆóÊïàÁéá` `ËßÜÈ¢ëÂàÜÊûê` `ÈÄâÊã©ÊÄßÈáçÂä†ËΩΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑKVÂéãÁº©ÊñπÊ≥ïÂú®È´òÂéãÁº©ÊØî‰∏ãÂ∏∏Â∏∏ÂØºËá¥‰ø°ÊÅØÊçüÂ§±ÔºåÂΩ±ÂìçÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÊïàÊûú„ÄÇ
2. ÊèêÂá∫ÁöÑVideo-X^2LÈÄöËøáÂèåÂ±ÇKVÂéãÁº©ÂíåÈÄâÊã©ÊÄßKVÈáçÂä†ËΩΩÔºåÊúâÊïà‰øùÁïô‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ë‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVideo-X^2LÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâKVÂéãÁº©ÊñπÊ≥ïÔºåËÆ°ÁÆóÊàêÊú¨Â§ßÂπÖÈôç‰Ωé„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈïøËßÜÈ¢ëÁêÜËß£ÔºàLVUÔºâ‰ªçÁÑ∂ÊòØÁé∞ÊúâÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÈù¢‰∏¥ÁöÑÈáçÂ§ßÊåëÊàòÔºå‰∏ªË¶ÅÁî±‰∫éËÆ°ÁÆóÊàêÊú¨È´òÊòÇ„ÄÇËøëÊúüÊñπÊ≥ïÊé¢Á¥¢‰∫ÜKVÂéãÁº©‰ª•ÁºìËß£Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩÜÂú®È´òÂéãÁº©ÊØî‰∏ãÂ∏∏Â∏∏ÂØºËá¥ÊòæËëóÁöÑ‰ø°ÊÅØÊçüÂ§±„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜVideo-X^2LÔºåÁÅµÊ¥ªÂú∞‰∏∫ÊØè‰∏™LVU‰ªªÂä°‰øùÁïôÂÖ≥ÈîÆËßÜÈ¢ë‰ø°ÊÅØ„ÄÇVideo-X^2LÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÊìç‰ΩúÔºöÂèåÂ±ÇKVÂéãÁº©ÂíåÈÄâÊã©ÊÄßKVÈáçÂä†ËΩΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideo-X^2LÂú®Â§ö‰∏™ÊµÅË°åÁöÑLVUÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÊòæËëóËäÇÁúÅËÆ°ÁÆóÊàêÊú¨„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈïøËßÜÈ¢ëÁêÜËß£ÔºàLVUÔºâÈù¢‰∏¥ÁöÑ‰∏ªË¶ÅÈóÆÈ¢òÊòØÁé∞ÊúâÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜÈïøËßÜÈ¢ëÊó∂ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨Ôºå‰ª•ÂèäÂú®È´òÂéãÁº©ÊØî‰∏ãKVÂéãÁº©ÂØºËá¥ÁöÑ‰ø°ÊÅØÊçüÂ§±„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫Video-X^2LÔºåÈÄöËøáÂèåÂ±ÇKVÂéãÁº©ÂíåÈÄâÊã©ÊÄßKVÈáçÂä†ËΩΩÔºåÁÅµÊ¥ªÂú∞‰øùÁïôÂÖ≥ÈîÆËßÜÈ¢ë‰ø°ÊÅØÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑLVU‰ªªÂä°ÈúÄÊ±Ç„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°Êó®Âú®Âú®‰øùÊåÅ‰ø°ÊÅØÂÆåÊï¥ÊÄßÁöÑÂêåÊó∂ÔºåÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVideo-X^2LÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¢ÑÂ°´ÂÖÖÈò∂ÊÆµÂíåËß£Á†ÅÈò∂ÊÆµ„ÄÇÂú®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÔºåÁîüÊàê‰ΩéÂéãÁº©KVÔºàL-KVsÔºâÂíåÈ´òÂéãÁº©KVÔºàH-KVsÔºâÔºõÂú®Ëß£Á†ÅÈò∂ÊÆµÔºåÊ†πÊçÆÈáçË¶ÅÊÄßÈÄâÊã©ÊÄßÂú∞ÈáçÂä†ËΩΩL-KVsÂíåH-KVs„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂèåÂ±ÇKVÂéãÁº©ÂíåÈÄâÊã©ÊÄßKVÈáçÂä†ËΩΩÁöÑÁªìÂêàÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®‰∏çÂêå‰ªªÂä°‰∏≠ÁÅµÊ¥ªË∞ÉÊï¥‰ø°ÊÅØ‰øùÁïôÁ≠ñÁï•Ôºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÈ´ò‰∫Ü‰ø°ÊÅØÂà©Áî®Áéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåL-KVsÂíåH-KVsÁöÑÁîüÊàêÁ≠ñÁï•ÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùÂú®‰∏çÂêåÂéãÁº©ÊØî‰∏ã‰ªçËÉΩ‰øùÁïôÂøÖË¶ÅÁöÑ‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÔºåÁõ¥Êé•ÂÖºÂÆπÁé∞ÊúâÁöÑKVÂéãÁº©MLLMs„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™ÈïøËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÔºåVideo-X^2LÊòæËëó‰ºò‰∫éÁé∞ÊúâKVÂéãÁº©ÊñπÊ≥ïÔºåËÆ°ÁÆóÊàêÊú¨Èôç‰Ωé‰∫ÜÁ∫¶30%„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰ø°ÊÅØ‰øùÁïôÂíåËÆ°ÁÆóÊïàÁéá‰πãÈó¥ÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÂπ≥Ë°°ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®ÁöÑÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÂÜÖÂÆπÂàÜÊûê„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËßÜÈ¢ëÊëòË¶ÅÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÊïàÁéáÔºåVideo-X^2LÂèØ‰ª•Âú®ÊïôËÇ≤„ÄÅÂ®±‰πêÂíåÂÆâÂÖ®Á≠âÂ§ö‰∏™Ë°å‰∏ö‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõÊ≠•‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-video understanding (LVU) remains a severe challenge for existing multimodal large language models (MLLMs), primarily due to the prohibitive computational cost. Recent approaches have explored KV compression to mitigate this issue, but they often suffer from significant information loss at high compression ratios. In this paper, we introduce Video-X^2L, which flexibly preserves critical video information for each LVU task. Video-X^2L involves two key operations. The first one is called bi-level KV compression. During the MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs: low-compression KVs (L-KVs) to capture fine-grained video details and high-compression KVs (H-KVs) to offer compact video representations. The second one is called selective KV re-loading. During the MLLM's decoding stage, Video-X^2L selectively re-loads L-KVs for the most critical video chunks while using H-KVs for other less important ones. This allows the MLLM to fully utilize task-specific information while maintaining the overall compactness. Video-X^2L is simple yet effective: it is free from additional training and directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L with a variety of popular LVU benchmarks, including VideoMME, MLVU, LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L outperforms existing KV-compression methods by a huge advantage while substantially saving the computation cost.

