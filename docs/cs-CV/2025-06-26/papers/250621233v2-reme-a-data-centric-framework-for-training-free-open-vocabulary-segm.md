---
layout: default
title: ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation
---

# ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21233" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21233v2</a>
  <a href="https://arxiv.org/pdf/2506.21233.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21233v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21233v2', 'ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiwei Xuan, Ziquan Deng, Kwan-Liu Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-06-27)

**å¤‡æ³¨**: Accepted to ICCV 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xiweix/ReME)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReMEæ¡†æ¶ä»¥è§£å†³è®­ç»ƒæ— å…³çš„å¼€æ”¾è¯æ±‡åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡åˆ†å‰²` `æ•°æ®è´¨é‡` `è¯­ä¹‰åˆ†å‰²` `æ— ç›‘ç£å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®­ç»ƒæ— å…³å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•æ€§èƒ½å—é™äºä¾èµ–æ¨¡å‹çš„èƒ½åŠ›å’Œå‚è€ƒé›†çš„è´¨é‡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥æ•°æ®è´¨é‡ä¸ºå¯¼å‘çš„æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºé«˜è´¨é‡çš„å‚è€ƒé›†æ¥æå‡OVSçš„æ€§èƒ½ã€‚
3. åœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„è®­ç»ƒæ— å…³OVSæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒæ— å…³çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSï¼‰æ—¨åœ¨æ ¹æ®ä¸€ç»„ä»»æ„æ–‡æœ¬ç±»åˆ«å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œè€Œæ— éœ€æ˜‚è´µçš„æ¨¡å‹å¾®è°ƒã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ–ç”Ÿæˆåˆæˆæ•°æ®å¹¶è®¾è®¡å¤æ‚çš„æ£€ç´¢è¿‡ç¨‹æ¥æ‰§è¡ŒOVSã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å—åˆ°ä¾èµ–æ¨¡å‹èƒ½åŠ›æˆ–å‚è€ƒé›†è´¨é‡ä¸ä½³çš„é™åˆ¶ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™ä¸€å¯†é›†åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¢«å¿½è§†çš„æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºé«˜è´¨é‡çš„å‚è€ƒé›†èƒ½å¤Ÿæ˜¾è‘—æå‡è®­ç»ƒæ— å…³çš„OVSã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥æ•°æ®è´¨é‡ä¸ºå¯¼å‘çš„æ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«è‰¯å¥½é…å¯¹çš„åˆ†å‰²-æ–‡æœ¬åµŒå…¥çš„å‚è€ƒé›†çš„æ•°æ®ç®¡é“ï¼Œå¹¶é‡‡ç”¨ç®€å•çš„åŸºäºç›¸ä¼¼åº¦çš„æ£€ç´¢æ–¹æ³•æ¥æ­ç¤ºæ•°æ®çš„æœ¬è´¨å½±å“ã€‚æˆ‘ä»¬åœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„è®­ç»ƒæ— å…³OVSæ–¹æ³•ï¼Œå¼ºè°ƒäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®¾è®¡åœ¨æ¨åŠ¨OVSè¿›æ­¥ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è®­ç»ƒæ— å…³çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹èƒ½åŠ›å’Œå‚è€ƒé›†è´¨é‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡æ„å»ºé«˜è´¨é‡çš„å‚è€ƒé›†æ¥æå‡OVSçš„æ•ˆæœï¼Œå¼ºè°ƒæ•°æ®è´¨é‡çš„é‡è¦æ€§ï¼Œè€Œéä»…ä¾èµ–æ¨¡å‹çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç®¡é“å’Œç›¸ä¼¼åº¦æ£€ç´¢æ¨¡å—ã€‚æ•°æ®ç®¡é“è´Ÿè´£æ„å»ºé…å¯¹çš„åˆ†å‰²-æ–‡æœ¬åµŒå…¥ï¼Œè€Œç›¸ä¼¼åº¦æ£€ç´¢åˆ™ç”¨äºä»å‚è€ƒé›†ä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä»¥æ•°æ®è´¨é‡ä¸ºæ ¸å¿ƒçš„è®¾è®¡ç†å¿µï¼Œå¼ºè°ƒé«˜è´¨é‡å‚è€ƒé›†åœ¨è®­ç»ƒæ— å…³OVSä¸­çš„é‡è¦æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•çš„ä¾èµ–æ¨¡å‹èƒ½åŠ›å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ä¼˜åŒ–çš„åµŒå…¥å¯¹é½ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡ç›¸ä¼¼åº¦çš„æœ€å¤§åŒ–ï¼Œç¡®ä¿ç”Ÿæˆçš„å‚è€ƒé›†å…·æœ‰è¾ƒé«˜çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚æ•´ä½“ç½‘ç»œç»“æ„ç®€åŒ–ï¼Œä¾¿äºå®ç°é«˜æ•ˆçš„æ£€ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„è®­ç»ƒæ— å…³OVSæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°5%-15%ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†æ•°æ®è´¨é‡åœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­çš„å…³é”®ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æå’Œæœºå™¨äººè§†è§‰ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å¼€æ”¾è¯æ±‡åˆ†å‰²çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å¤šç§å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å›¾åƒç†è§£ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training-free open-vocabulary semantic segmentation (OVS) aims to segment images given a set of arbitrary textual categories without costly model fine-tuning. Existing solutions often explore attention mechanisms of pre-trained models, such as CLIP, or generate synthetic data and design complex retrieval processes to perform OVS. However, their performance is limited by the capability of reliant models or the suboptimal quality of reference sets. In this work, we investigate the largely overlooked data quality problem for this challenging dense scene understanding task, and identify that a high-quality reference set can significantly benefit training-free OVS. With this observation, we introduce a data-quality-oriented framework, comprising a data pipeline to construct a reference set with well-paired segment-text embeddings and a simple similarity-based retrieval to unveil the essential effect of data. Remarkably, extensive evaluations on ten benchmark datasets demonstrate that our method outperforms all existing training-free OVS approaches, highlighting the importance of data-centric design for advancing OVS without training. Our code is available at https://github.com/xiweix/ReME .

