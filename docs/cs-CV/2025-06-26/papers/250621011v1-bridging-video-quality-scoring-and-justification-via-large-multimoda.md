---
layout: default
title: Bridging Video Quality Scoring and Justification via Large Multimodal Models
---

# Bridging Video Quality Scoring and Justification via Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21011" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21011v1</a>
  <a href="https://arxiv.org/pdf/2506.21011.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21011v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21011v1', 'Bridging Video Quality Scoring and Justification via Large Multimodal Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Qizhi Xie, Kun Yuan, Yunpeng Qu, Jiachao Gong, Mingda Wu, Ming Sun, Chao Zhou, Jihong Zhu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-26

**Â§áÊ≥®**: 15 pages, 4 figures, 8 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éSIGÁöÑÂ§öÊ®°ÊÄÅÊ®°Âûã‰ª•ÊèêÂçáËßÜÈ¢ëË¥®ÈáèËØÑÂàÜ‰∏éËß£ÈáäËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëË¥®ÈáèËØÑ‰º∞` `Â§öÊ®°ÊÄÅÊ®°Âûã` `Êåá‰ª§Ë∞É‰ºò` `ÂàÜÂ±ÇÊÄùÁª¥Èìæ` `Ëá™Âä®ÂåñÁîüÊàê` `Êï∞ÊçÆÈõÜÊûÑÂª∫` `Ë¥®ÈáèËØÑÂàÜ` `ËßÜÈ¢ëÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëË¥®ÈáèËØÑ‰º∞ÊñπÊ≥ï‰ªÖÊèê‰æõÊï∞ÂÄºËØÑÂàÜÔºåÊó†Ê≥ïÂÖ®Èù¢ÂèçÊò†ËßÜÈ¢ëÁöÑÂ§öÁª¥Ë¥®ÈáèÁâπÂæÅÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®„ÄÇ
2. ÊèêÂá∫ÁöÑSIGÁÆ°ÈÅìÈÄöËøáËá™Âä®ÂåñÁîüÊàêÊåá‰ª§ÔºåÁªìÂêàÂàÜÂ±ÇÊÄùÁª¥ÈìæÔºåÊ®°Êãü‰∫∫Á±ªËßÜËßâÊé®ÁêÜÔºåÊèêÂçá‰∫ÜËßÜÈ¢ëË¥®ÈáèËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄß„ÄÇ
3. Âú®S2I-BenchÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ËØ•ÊñπÊ≥ïÂú®Ë¥®ÈáèËØÑÂàÜÂíåËß£ÈáäËÉΩÂäõ‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰º†ÁªüÁöÑËßÜÈ¢ëË¥®ÈáèËØÑ‰º∞ÔºàVQAÔºâÊñπÊ≥ï‰ªÖÁîüÊàêÊï∞ÂÄºËØÑÂàÜÔºåÊó†Ê≥ïÂÖ®Èù¢ÊèèËø∞ËßÜÈ¢ëÁöÑÂ§çÊùÇË¥®ÈáèÁª¥Â∫¶ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇÊú¨ÊñáÈÄöËøáÊåá‰ª§Ë∞É‰ºòÔºåÂ∞ÜÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂ∫îÁî®‰∫éVQAÔºåÊèêÂá∫‰∫ÜÂü∫‰∫éËØÑÂàÜÁöÑÊåá‰ª§ÁîüÊàêÔºàSIGÔºâÁÆ°ÈÅì„ÄÇSIGÈ¶ñÂÖàÂØπÊú™Ê†áËÆ∞ËßÜÈ¢ëÁöÑÂ§ö‰∏™Ë¥®ÈáèÁª¥Â∫¶ËøõË°åËØÑÂàÜÔºåÂπ∂Â∞ÜËØÑÂàÜÊò†Â∞ÑÂà∞ÊñáÊú¨ÂÆö‰πâÁöÑÁ≠âÁ∫ßÔºåÁªìÂêàÂàÜÂ±ÇÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊ®°ÂûãÔºåÊ®°Êãü‰∫∫Á±ªËßÜËßâÁ≥ªÁªüÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊúÄÁªàÁîüÊàêÁöÑScore2InstructÔºàS2IÔºâÊï∞ÊçÆÈõÜÂåÖÂê´Ë∂ÖËøá32‰∏á‰∏™Â§öÊ†∑ÁöÑÊåá‰ª§-ÂìçÂ∫îÂØπÔºå‰∏∫Êåá‰ª§Ë∞É‰ºòÂ•†ÂÆöÂü∫Á°Ä„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ËßÜÈ¢ëLMMs‰∏äÊòæËëóÊèêÂçá‰∫ÜË¥®ÈáèËØÑÂàÜÂíåËß£ÈáäËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüËßÜÈ¢ëË¥®ÈáèËØÑ‰º∞ÊñπÊ≥ïÊó†Ê≥ïÂÖ®Èù¢ÊèèËø∞ËßÜÈ¢ëË¥®ÈáèÁª¥Â∫¶ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫é‰∫∫Â∑•Ê†áÊ≥®ÔºåÂØºËá¥Êï∞ÊçÆÁîüÊàêÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊúâÊïàÊÄßÂèóÂà∞ÈôêÂà∂„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Âü∫‰∫éËØÑÂàÜÁöÑÊåá‰ª§ÁîüÊàêÔºàSIGÔºâÁÆ°ÈÅìÔºåËá™Âä®ÂåñÁîüÊàê‰∏éËßÜÈ¢ëË¥®ÈáèÁõ∏ÂÖ≥ÁöÑÊåá‰ª§ÔºåÂáèÂ∞ëÂØπ‰∏ìÂÆ∂Ê†áÊ≥®ÁöÑ‰æùËµñÔºåÂêåÊó∂ÊèêÂçáÊï∞ÊçÆÁîüÊàêÁöÑÊïàÁéáÂíåËßÑÊ®°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSIGÁÆ°ÈÅìÈ¶ñÂÖàÂØπÊú™Ê†áËÆ∞ËßÜÈ¢ëËøõË°åÂ§öÁª¥Ë¥®ÈáèËØÑÂàÜÔºåÂπ∂Â∞ÜÂÖ∂Êò†Â∞ÑÂà∞ÊñáÊú¨ÂÆö‰πâÁöÑÁ≠âÁ∫ß„ÄÇÊé•ÁùÄÔºåÂà©Áî®ÂàÜÂ±ÇÊÄùÁª¥ÈìæÔºàCoTÔºâÊ®°ÂûãÔºåÂª∫Á´ãÂÖ∑‰ΩìÁª¥Â∫¶‰∏éÊï¥‰ΩìË¥®Èáè‰πãÈó¥ÁöÑÂÖ≥ËÅîÔºåÊ®°Êãü‰∫∫Á±ªÁöÑËßÜËßâÊé®ÁêÜËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éSIGÁÆ°ÈÅìÁöÑËÆæËÆ°ÔºåËÉΩÂ§üËá™Âä®ÁîüÊàêË¥®ÈáèÊåá‰ª§ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ïÂØπ‰∫∫Â∑•Ê†áÊ≥®ÁöÑ‰æùËµñÔºåÊòæËëóÊèêÂçá‰∫ÜÊï∞ÊçÆÁöÑÂèØÊâ©Â±ïÊÄßÂíåÁîüÊàêÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®SIGÁÆ°ÈÅì‰∏≠ÔºåÈááÁî®‰∫ÜÂàÜÂ±ÇÊÄùÁª¥ÈìæÊ®°ÂûãÊù•Â§ÑÁêÜË¥®ÈáèÁª¥Â∫¶‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÁ°Æ‰øù‰∫ÜÁîüÊàêÊåá‰ª§ÁöÑÈÄªËæëÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇÊï∞ÊçÆÈõÜScore2InstructÔºàS2IÔºâÂåÖÂê´Ë∂ÖËøá32‰∏á‰∏™Êåá‰ª§-ÂìçÂ∫îÂØπÔºå‰∏∫ÂêéÁª≠ÁöÑÊåá‰ª§Ë∞É‰ºòÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÊï∞ÊçÆÂü∫Á°Ä„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®S2I-BenchÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ËßÜÈ¢ëË¥®ÈáèËØÑÂàÜÂíåËß£ÈáäËÉΩÂäõ‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ∞§ÂÖ∂Âú®Â§ö‰∏™ËßÜÈ¢ëLMMs‰∏äÔºåËØÑÂàÜÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜÁ∫¶15%ÔºåËß£ÈáäËÉΩÂäõÊèêÂçá‰∫Ü20%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÂÜÖÂÆπÂàõ‰Ωú„ÄÅÂú®Á∫øÊïôËÇ≤„ÄÅÂΩ±ËßÜÂà∂‰ΩúÁ≠âÔºåËÉΩÂ§ü‰∏∫ËßÜÈ¢ëË¥®ÈáèËØÑ‰º∞Êèê‰æõÊõ¥‰∏∫ÂÖ®Èù¢ÂíåÂáÜÁ°ÆÁöÑÂ∑•ÂÖ∑ÔºåÂ∏ÆÂä©Áõ∏ÂÖ≥Ë°å‰∏öÊèêÂçáËßÜÈ¢ëÂÜÖÂÆπÁöÑË¥®ÈáèÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®ËßÜÈ¢ëÂàÜÊûêÂíåÁêÜËß£ÊäÄÊúØÁöÑÂèëÂ±ïÔºå‰øÉËøõÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Classical video quality assessment (VQA) methods generate a numerical score to judge a video's perceived visual fidelity and clarity. Yet, a score fails to describe the video's complex quality dimensions, restricting its applicability. Benefiting from the linguistic output, adapting video large multimodal models (LMMs) to VQA via instruction tuning has the potential to address this issue. The core of the approach lies in the video quality-centric instruction data. Previous explorations mainly focus on the image domain, and their data generation processes heavily rely on human quality annotations and proprietary systems, limiting data scalability and effectiveness. To address these challenges, we propose the Score-based Instruction Generation (SIG) pipeline. Specifically, SIG first scores multiple quality dimensions of an unlabeled video and maps scores to text-defined levels. It then explicitly incorporates a hierarchical Chain-of-Thought (CoT) to model the correlation between specific dimensions and overall quality, mimicking the human visual system's reasoning process. The automated pipeline eliminates the reliance on expert-written quality descriptions and proprietary systems, ensuring data scalability and generation efficiency. To this end, the resulting Score2Instruct (S2I) dataset contains over 320K diverse instruction-response pairs, laying the basis for instruction tuning. Moreover, to advance video LMMs' quality scoring and justification abilities simultaneously, we devise a progressive tuning strategy to fully unleash the power of S2I. Built upon SIG, we further curate a benchmark termed S2I-Bench with 400 open-ended questions to better evaluate the quality justification capacity of video LMMs. Experimental results on the S2I-Bench and existing benchmarks indicate that our method consistently improves quality scoring and justification capabilities across multiple video LMMs.

