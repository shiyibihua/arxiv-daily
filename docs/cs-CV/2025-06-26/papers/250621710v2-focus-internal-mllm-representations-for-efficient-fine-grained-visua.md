---
layout: default
title: FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering
---

# FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21710" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21710v2</a>
  <a href="https://arxiv.org/pdf/2506.21710.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21710v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21710v2', 'FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian HÃ¼ger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-10-29)

**å¤‡æ³¨**: Accepted by NeurIPS 2025 - main track. Project page: https://focus-mllm-vqa.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFOCUSä»¥è§£å†³ç»†ç²’åº¦è§†è§‰é—®ç­”ä¸­çš„è§†è§‰è£å‰ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰è£å‰ª` `ç»†ç²’åº¦è¯†åˆ«` `å¯¹è±¡ç›¸å…³æ€§å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰é—®ç­”æ–¹æ³•åœ¨å¤„ç†å°å›¾åƒç»†èŠ‚æ—¶æ•ˆç‡ä½ä¸‹ï¼Œä¸”éœ€è¦ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒã€‚
2. FOCUSé€šè¿‡æ— è®­ç»ƒçš„è§†è§‰è£å‰ªæ–¹æ³•ï¼Œåˆ©ç”¨MLLMå†…éƒ¨è¡¨ç¤ºæ¥ä¼˜åŒ–å›¾åƒåŒºåŸŸæœç´¢è¿‡ç¨‹ã€‚
3. FOCUSåœ¨å¤šä¸ªç»†ç²’åº¦VQAæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶æ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒ-æ–‡æœ¬è¾“å…¥çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å…³æ³¨å°å›¾åƒç»†èŠ‚çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„è§†è§‰è£å‰ªæŠ€æœ¯å­˜åœ¨ä»»åŠ¡ç‰¹å®šå¾®è°ƒéœ€æ±‚ã€ä½æ•ˆç‡å’Œä¸é«˜æ•ˆæ³¨æ„åŠ›å®ç°ä¸å…¼å®¹ç­‰é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„è§†è§‰è£å‰ªæ–¹æ³•FOCUSï¼Œåˆ©ç”¨MLLMå†…éƒ¨è¡¨ç¤ºæ¥æŒ‡å¯¼æœ€ç›¸å…³å›¾åƒåŒºåŸŸçš„æœç´¢ã€‚FOCUSé€šè¿‡å››ä¸ªæ­¥éª¤å®ç°ï¼šé¦–å…ˆè¯†åˆ«VQAæç¤ºä¸­çš„ç›®æ ‡å¯¹è±¡ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜è®¡ç®—å¯¹è±¡ç›¸å…³æ€§å›¾ï¼›ç„¶åï¼Œæ ¹æ®è¯¥å›¾æå‡ºå¹¶æ’åç›¸å…³å›¾åƒåŒºåŸŸï¼›æœ€åï¼Œä½¿ç”¨æ’åæœ€é«˜çš„åŒºåŸŸæ‰§è¡Œç»†ç²’åº¦VQAä»»åŠ¡ã€‚FOCUSåœ¨å››ä¸ªç»†ç²’åº¦VQAæ•°æ®é›†å’Œä¸‰ç§ç±»å‹çš„MLLMä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¸‰ç§æµè¡Œçš„è§†è§‰è£å‰ªæ–¹æ³•ï¼Œå¹¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¦æ±‚å‡å°‘3åˆ°6.5å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç»†ç²’åº¦è§†è§‰é—®ç­”ä¸­å¯¹å°å›¾åƒç»†èŠ‚çš„å¤„ç†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´çš„ç—›ç‚¹åŒ…æ‹¬ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„éœ€æ±‚ã€ä½æ•ˆç‡çš„æ— ä¿¡æ¯æœç´¢ä»¥åŠä¸é«˜æ•ˆæ³¨æ„åŠ›å®ç°çš„ä¸å…¼å®¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFOCUSçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨MLLMå†…éƒ¨è¡¨ç¤ºè¿›è¡Œæ— è®­ç»ƒçš„è§†è§‰è£å‰ªï¼Œä¼˜åŒ–ç›¸å…³å›¾åƒåŒºåŸŸçš„æœç´¢è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒFOCUSèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæå‡VQAä»»åŠ¡çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFOCUSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ­¥éª¤ï¼šé¦–å…ˆè¯†åˆ«VQAæç¤ºä¸­çš„ç›®æ ‡å¯¹è±¡ï¼›å…¶æ¬¡è®¡ç®—å¯¹è±¡ç›¸å…³æ€§å›¾ï¼›ç„¶åæ ¹æ®è¯¥å›¾æå‡ºå¹¶æ’åç›¸å…³å›¾åƒåŒºåŸŸï¼›æœ€åæ‰§è¡Œç»†ç²’åº¦VQAä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFOCUSçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„è§†è§‰è£å‰ªæ–¹æ³•ï¼Œåˆ©ç”¨MLLMå†…éƒ¨çš„é”®å€¼ç¼“å­˜è¿›è¡Œå¯¹è±¡ç›¸å…³æ€§è®¡ç®—ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—FOCUSåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰è£å‰ªæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šFOCUSçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨é”®å€¼ç¼“å­˜æ¥ç”Ÿæˆå¯¹è±¡ç›¸å…³æ€§å›¾ï¼Œä»¥åŠåŸºäºè¯¥å›¾è¿›è¡Œå›¾åƒåŒºåŸŸçš„æ’åã€‚è¿™äº›è®¾è®¡ä½¿å¾—FOCUSåœ¨è®¡ç®—ä¸Šæ›´ä¸ºé«˜æ•ˆï¼Œä¸”èƒ½å¤Ÿåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—ä¼˜å¼‚çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FOCUSåœ¨å››ä¸ªç»†ç²’åº¦VQAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¸‰ç§æµè¡Œçš„è§†è§‰è£å‰ªæ–¹æ³•ï¼Œä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¦æ±‚å‡å°‘3åˆ°6.5å€ã€‚å…¶æ€§èƒ½ä¸æœ€ä½³åŸºçº¿ZoomEyeç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FOCUSçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒæ£€ç´¢å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡ç»†ç²’åº¦è§†è§‰é—®ç­”çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºç²¾å‡†çš„ä¿¡æ¯æ£€ç´¢å’Œäº¤äº’ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

