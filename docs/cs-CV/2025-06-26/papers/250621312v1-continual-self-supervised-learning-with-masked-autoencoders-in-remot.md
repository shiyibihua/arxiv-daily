---
layout: default
title: Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing
---

# Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21312" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21312v1</a>
  <a href="https://arxiv.org/pdf/2506.21312.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21312v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21312v1', 'Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lars MÃ¶llenbrok, Behnood Rasti, BegÃ¼m Demir

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Accepted to IEEE Geoscience and Remote Sensing Letters. Our code is available at https://git.tu-berlin.de/rsim/CoSMAE

**DOI**: [10.1109/LGRS.2025.3579585](https://doi.org/10.1109/LGRS.2025.3579585)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoSMAEä»¥è§£å†³é¥æ„Ÿä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `é¥æ„Ÿ` `çŸ¥è¯†è’¸é¦` `æ•°æ®æ··åˆ` `æ¨¡å‹æ··åˆ` `ç¾éš¾æ€§é—å¿˜` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŒç»­å­¦ä¹ æ–¹æ³•åœ¨é¥æ„Ÿé¢†åŸŸé€šå¸¸ä¾èµ–å¤§é‡æ ‡è®°æ ·æœ¬ï¼Œå¯¼è‡´æˆæœ¬é«˜ä¸”éš¾ä»¥å®ç°ã€‚
2. æœ¬æ–‡æå‡ºçš„CoSMAEæ–¹æ³•é€šè¿‡æ•°æ®æ··åˆå’Œæ¨¡å‹æ··åˆçŸ¥è¯†è’¸é¦æ¥è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoSMAEåœ¨å¤šä¸ªåŸºå‡†ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•æå‡äº†æœ€é«˜4.94%çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ–¹æ³•çš„å‘å±•æ—¨åœ¨ä»ä¸æ–­è·å–çš„è®­ç»ƒæ•°æ®ä¸­ä»¥é¡ºåºæ–¹å¼å­¦ä¹ æ–°ä»»åŠ¡ï¼Œè¿‘å¹´æ¥åœ¨é¥æ„Ÿï¼ˆRSï¼‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰çš„CLæ–¹æ³•åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œå¢å¼ºäº†å¯¹ç¾éš¾æ€§é—å¿˜çš„é²æ£’æ€§ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æ ‡è®°è®­ç»ƒæ ·æœ¬ï¼Œè¿™åœ¨é¥æ„Ÿä¸­æˆæœ¬é«˜ä¸”ä¸æ˜“è·å¾—ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æŒç»­è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•CoSMAEï¼Œç»“åˆäº†æ•°æ®æ··åˆå’Œæ¨¡å‹æ··åˆçŸ¥è¯†è’¸é¦ä¸¤ä¸ªç»„ä»¶ï¼Œä»¥æ›´å¥½åœ°åœ¨ä»»åŠ¡é—´è¿›è¡Œæ³›åŒ–å¹¶é™ä½ç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoSMAEåœ¨åº”ç”¨äºMAEçš„CLæ–¹æ³•ä¸­ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œæœ€é«˜å¯è¾¾4.94%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é¥æ„Ÿé¢†åŸŸä¸­æŒç»­å­¦ä¹ æ–¹æ³•å¯¹æ ‡è®°æ ·æœ¬ä¾èµ–è¿‡é‡çš„é—®é¢˜ï¼Œå¯¼è‡´ç¾éš¾æ€§é—å¿˜çš„ç°è±¡ã€‚ç°æœ‰æ–¹æ³•åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆä¿ç•™æ—§ä»»åŠ¡çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoSMAEçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ•°æ®æ··åˆå’Œæ¨¡å‹æ··åˆçŸ¥è¯†è’¸é¦æ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼Œç¡®ä¿åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶èƒ½å¤Ÿä¿ç•™æ—§ä»»åŠ¡çš„çŸ¥è¯†ã€‚æ•°æ®æ··åˆé€šè¿‡æ’å€¼å½“å‰ä»»åŠ¡ä¸è¿‡å»ä»»åŠ¡çš„å›¾åƒæ¥ä¿æŒä¿¡æ¯ï¼Œè€Œæ¨¡å‹æ··åˆåˆ™é€šè¿‡æ’å€¼æ¨¡å‹æƒé‡æ¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoSMAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°æ®æ··åˆæ¨¡å—å’Œæ¨¡å‹æ··åˆçŸ¥è¯†è’¸é¦æ¨¡å—ã€‚æ•°æ®æ··åˆæ¨¡å—è´Ÿè´£å¤„ç†è¾“å…¥æ•°æ®ï¼Œç¡®ä¿ä¿¡æ¯çš„ä¿ç•™ï¼›æ¨¡å‹æ··åˆæ¨¡å—åˆ™é€šè¿‡è’¸é¦æŠ€æœ¯æ•´åˆè¿‡å»å’Œå½“å‰æ¨¡å‹çš„çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoSMAEçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå°†æ•°æ®æ··åˆä¸æ¨¡å‹æ··åˆçŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨é¢å¯¹æ–°ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼ŒCoSMAEé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ•°æ®å’Œæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ”¯æŒæ··åˆæ“ä½œçš„é«˜æ•ˆæ‰§è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoSMAEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æŒç»­å­¦ä¹ æ–¹æ³•æå‡äº†æœ€é«˜4.94%çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å‡å°‘ç¾éš¾æ€§é—å¿˜å’Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é¥æ„Ÿå›¾åƒåˆ†æã€ç¯å¢ƒç›‘æµ‹å’Œå†œä¸šç›‘æ§ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨æ–°ä»»åŠ¡å­¦ä¹ ä¸­çš„è¡¨ç°ï¼ŒCoSMAEèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæ›´é«˜æ•ˆåœ°åˆ©ç”¨é¥æ„Ÿæ•°æ®ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The development of continual learning (CL) methods, which aim to learn new tasks in a sequential manner from the training data acquired continuously, has gained great attention in remote sensing (RS). The existing CL methods in RS, while learning new tasks, enhance robustness towards catastrophic forgetting. This is achieved by using a large number of labeled training samples, which is costly and not always feasible to gather in RS. To address this problem, we propose a novel continual self-supervised learning method in the context of masked autoencoders (denoted as CoSMAE). The proposed CoSMAE consists of two components: i) data mixup; and ii) model mixup knowledge distillation. Data mixup is associated with retaining information on previous data distributions by interpolating images from the current task with those from the previous tasks. Model mixup knowledge distillation is associated with distilling knowledge from past models and the current model simultaneously by interpolating their model weights to form a teacher for the knowledge distillation. The two components complement each other to regularize the MAE at the data and model levels to facilitate better generalization across tasks and reduce the risk of catastrophic forgetting. Experimental results show that CoSMAE achieves significant improvements of up to 4.94% over state-of-the-art CL methods applied to MAE. Our code is publicly available at: https://git.tu-berlin.de/rsim/CoSMAE.

