---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-26
---

# cs.CVï¼ˆ2025-06-26ï¼‰

ğŸ“Š å…± **50** ç¯‡è®ºæ–‡
 | ğŸ”— **11** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (19 ğŸ”—6)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (19 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250621448v3-thinksound-chain-of-thought-reasoning-in-multimodal-large-language-m.html">ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing</a></td>
  <td>æå‡ºThinkSoundæ¡†æ¶ä»¥è§£å†³è§†é¢‘éŸ³é¢‘ç”Ÿæˆä¸­çš„é«˜ä¿çœŸæŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21448v3" data-paper-url="./papers/250621448v3-thinksound-chain-of-thought-reasoning-in-multimodal-large-language-m.html" onclick="toggleFavorite(this, '2506.21448v3', 'ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250621316v2-drishtikon-visual-grounding-at-multiple-granularities-in-documents.html">DRISHTIKON: Visual Grounding at Multiple Granularities in Documents</a></td>
  <td>æå‡ºDRISHTIKONä»¥è§£å†³æ–‡æ¡£å›¾åƒä¸­çš„è§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21316v2" data-paper-url="./papers/250621316v2-drishtikon-visual-grounding-at-multiple-granularities-in-documents.html" onclick="toggleFavorite(this, '2506.21316v2', 'DRISHTIKON: Visual Grounding at Multiple Granularities in Documents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250621017v1-multimodal-prompt-alignment-for-facial-expression-recognition.html">Multimodal Prompt Alignment for Facial Expression Recognition</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æç¤ºå¯¹é½æ¡†æ¶ä»¥æå‡é¢éƒ¨è¡¨æƒ…è¯†åˆ«ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21017v1" data-paper-url="./papers/250621017v1-multimodal-prompt-alignment-for-facial-expression-recognition.html" onclick="toggleFavorite(this, '2506.21017v1', 'Multimodal Prompt Alignment for Facial Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250621011v1-bridging-video-quality-scoring-and-justification-via-large-multimoda.html">Bridging Video Quality Scoring and Justification via Large Multimodal Models</a></td>
  <td>æå‡ºåŸºäºSIGçš„å¤šæ¨¡æ€æ¨¡å‹ä»¥æå‡è§†é¢‘è´¨é‡è¯„åˆ†ä¸è§£é‡Šèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21011v1" data-paper-url="./papers/250621011v1-bridging-video-quality-scoring-and-justification-via-large-multimoda.html" onclick="toggleFavorite(this, '2506.21011v1', 'Bridging Video Quality Scoring and Justification via Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250621549v2-sim3d-single-instance-multiview-multimodal-and-multisetup-3d-anomaly.html">SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark</a></td>
  <td>æå‡ºSiM3Dä»¥è§£å†³å•å®ä¾‹å¤šè§†è§’å¤šæ¨¡æ€3Då¼‚å¸¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21549v2" data-paper-url="./papers/250621549v2-sim3d-single-instance-multiview-multimodal-and-multisetup-3d-anomaly.html" onclick="toggleFavorite(this, '2506.21549v2', 'SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250621444v2-benchmarking-deep-learning-and-vision-foundation-models-for-atypical.html">Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation</a></td>
  <td>åŸºäºæ·±åº¦å­¦ä¹ çš„éå…¸å‹æœ‰ä¸åˆ†è£‚åˆ†ç±»åŸºå‡†ç ”ç©¶</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21444v2" data-paper-url="./papers/250621444v2-benchmarking-deep-learning-and-vision-foundation-models-for-atypical.html" onclick="toggleFavorite(this, '2506.21444v2', 'Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250621319v3-simvecvis-a-dataset-for-enhancing-mllms-in-visualization-understandi.html">SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding</a></td>
  <td>æå‡ºSimVecä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¯è§†åŒ–ç†è§£ä¸­çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21319v3" data-paper-url="./papers/250621319v3-simvecvis-a-dataset-for-enhancing-mllms-in-visualization-understandi.html" onclick="toggleFavorite(this, '2506.21319v3', 'SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250621056v1-samurai-shape-aware-multimodal-retrieval-for-3d-object-identificatio.html">SAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification</a></td>
  <td>æå‡ºSAMURAIä»¥è§£å†³å¤æ‚å®¤å†…ç¯å¢ƒä¸­çš„3Dç‰©ä½“æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21056v1" data-paper-url="./papers/250621056v1-samurai-shape-aware-multimodal-retrieval-for-3d-object-identificatio.html" onclick="toggleFavorite(this, '2506.21056v1', 'SAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250621018v1-lasfnet-a-lightweight-attention-guided-self-modulation-feature-fusio.html">LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection</a></td>
  <td>æå‡ºLASFNetä»¥ç®€åŒ–å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ä¸­çš„ç‰¹å¾èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21018v1" data-paper-url="./papers/250621018v1-lasfnet-a-lightweight-attention-guided-self-modulation-feature-fusio.html" onclick="toggleFavorite(this, '2506.21018v1', 'LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250621710v2-focus-internal-mllm-representations-for-efficient-fine-grained-visua.html">FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering</a></td>
  <td>æå‡ºFOCUSä»¥è§£å†³ç»†ç²’åº¦è§†è§‰é—®ç­”ä¸­çš„è§†è§‰è£å‰ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21710v2" data-paper-url="./papers/250621710v2-focus-internal-mllm-representations-for-efficient-fine-grained-visua.html" onclick="toggleFavorite(this, '2506.21710v2', 'FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250621535v2-exploring-the-design-space-of-3d-mllms-for-ct-report-generation.html">Exploring the Design Space of 3D MLLMs for CT Report Generation</a></td>
  <td>æå‡º3Då¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»¥æå‡CTæŠ¥å‘Šç”Ÿæˆæ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21535v2" data-paper-url="./papers/250621535v2-exploring-the-design-space-of-3d-mllms-for-ct-report-generation.html" onclick="toggleFavorite(this, '2506.21535v2', 'Exploring the Design Space of 3D MLLMs for CT Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250621317v1-llava-pose-enhancing-human-pose-and-action-understanding-via-keypoin.html">LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning</a></td>
  <td>æå‡ºLLaVA-Poseä»¥è§£å†³äººç±»å§¿æ€ä¸åŠ¨ä½œç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21317v1" data-paper-url="./papers/250621317v1-llava-pose-enhancing-human-pose-and-action-understanding-via-keypoin.html" onclick="toggleFavorite(this, '2506.21317v1', 'LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250621188v3-groundflow-a-plug-in-module-for-temporal-reasoning-on-3d-point-cloud.html">GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding</a></td>
  <td>æå‡ºGroundFlowæ¨¡å—ä»¥è§£å†³3Dç‚¹äº‘åºåˆ—å®šä½ä¸­çš„æ—¶é—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21188v3" data-paper-url="./papers/250621188v3-groundflow-a-plug-in-module-for-temporal-reasoning-on-3d-point-cloud.html" onclick="toggleFavorite(this, '2506.21188v3', 'GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250621184v1-task-aware-kv-compression-for-cost-effective-long-video-understandin.html">Task-Aware KV Compression For Cost-Effective Long Video Understanding</a></td>
  <td>æå‡ºVideo-X^2Lä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„KVå‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21184v1" data-paper-url="./papers/250621184v1-task-aware-kv-compression-for-cost-effective-long-video-understandin.html" onclick="toggleFavorite(this, '2506.21184v1', 'Task-Aware KV Compression For Cost-Effective Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250621101v1-oraclefusion-assisting-the-decipherment-of-oracle-bone-script-with-s.html">OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography</a></td>
  <td>æå‡ºOracleFusionä»¥è§£å†³ç”²éª¨æ–‡å­—ç¬¦è§£è¯»éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21101v1" data-paper-url="./papers/250621101v1-oraclefusion-assisting-the-decipherment-of-oracle-bone-script-with-s.html" onclick="toggleFavorite(this, '2506.21101v1', 'OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250620964v1-evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-hum.html">Evidence-based diagnostic reasoning with multi-agent copilot for human pathology</a></td>
  <td>æå‡ºPathChat+ä»¥è§£å†³ç—…ç†å­¦è¯Šæ–­æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20964v1" data-paper-url="./papers/250620964v1-evidence-based-diagnostic-reasoning-with-multi-agent-copilot-for-hum.html" onclick="toggleFavorite(this, '2506.20964v1', 'Evidence-based diagnostic reasoning with multi-agent copilot for human pathology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250621476v1-global-and-local-entailment-learning-for-natural-world-imagery.html">Global and Local Entailment Learning for Natural World Imagery</a></td>
  <td>æå‡ºRadial Cross-Modal Embeddingsä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21476v1" data-paper-url="./papers/250621476v1-global-and-local-entailment-learning-for-natural-world-imagery.html" onclick="toggleFavorite(this, '2506.21476v1', 'Global and Local Entailment Learning for Natural World Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250621356v2-shotbench-expert-level-cinematic-understanding-in-vision-language-mo.html">ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models</a></td>
  <td>æå‡ºShotBenchä»¥è§£å†³ç”µå½±è¯­è¨€ç†è§£ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21356v2" data-paper-url="./papers/250621356v2-shotbench-expert-level-cinematic-understanding-in-vision-language-mo.html" onclick="toggleFavorite(this, '2506.21356v2', 'ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250620911v1-fasta-fast-slow-toolpath-agent-with-subroutine-mining-for-efficient-.html">FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing</a></td>
  <td>æå‡ºFaSTA$^*$ä»¥è§£å†³é«˜æ•ˆçš„å¤šè½®å›¾åƒç¼–è¾‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20911v1" data-paper-url="./papers/250620911v1-fasta-fast-slow-toolpath-agent-with-subroutine-mining-for-efficient-.html" onclick="toggleFavorite(this, '2506.20911v1', 'FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250621420v2-endoflow-slam-real-time-endoscopic-slam-with-flow-constrained-gaussi.html">EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting</a></td>
  <td>æå‡ºEndoFlow-SLAMä»¥è§£å†³å†…çª¥é•œSLAMä¸­çš„å…‰æµçº¦æŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21420v2" data-paper-url="./papers/250621420v2-endoflow-slam-real-time-endoscopic-slam-with-flow-constrained-gaussi.html" onclick="toggleFavorite(this, '2506.21420v2', 'EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250621117v2-cl-splats-continual-learning-of-gaussian-splatting-with-local-optimi.html">CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization</a></td>
  <td>æå‡ºCL-Splatsä»¥è§£å†³åŠ¨æ€3Dåœºæ™¯æ›´æ–°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21117v2" data-paper-url="./papers/250621117v2-cl-splats-continual-learning-of-gaussian-splatting-with-local-optimi.html" onclick="toggleFavorite(this, '2506.21117v2', 'CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250621233v2-reme-a-data-centric-framework-for-training-free-open-vocabulary-segm.html">ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation</a></td>
  <td>æå‡ºReMEæ¡†æ¶ä»¥è§£å†³è®­ç»ƒæ— å…³çš„å¼€æ”¾è¯æ±‡åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21233v2" data-paper-url="./papers/250621233v2-reme-a-data-centric-framework-for-training-free-open-vocabulary-segm.html" onclick="toggleFavorite(this, '2506.21233v2', 'ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250621401v3-curve-aware-gaussian-splatting-for-3d-parametric-curve-reconstructio.html">Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction</a></td>
  <td>æå‡ºCurveGaussianä»¥è§£å†³3Då‚æ•°æ›²çº¿é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21401v3" data-paper-url="./papers/250621401v3-curve-aware-gaussian-splatting-for-3d-parametric-curve-reconstructio.html" onclick="toggleFavorite(this, '2506.21401v3', 'Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250620998v1-dbmovi-gs-dynamic-view-synthesis-from-blurry-monocular-video-via-spa.html">DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting</a></td>
  <td>æå‡ºDBMovi-GSä»¥è§£å†³åŠ¨æ€æ¨¡ç³Šè§†é¢‘ä¸­çš„æ–°è§†è§’åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20998v1" data-paper-url="./papers/250620998v1-dbmovi-gs-dynamic-view-synthesis-from-blurry-monocular-video-via-spa.html" onclick="toggleFavorite(this, '2506.20998v1', 'DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250621520v2-madrive-memory-augmented-driving-scene-modeling.html">MADrive: Memory-Augmented Driving Scene Modeling</a></td>
  <td>æå‡ºMADriveä»¥è§£å†³è‡ªåŠ¨é©¾é©¶åœºæ™¯é‡å»ºçš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21520v2" data-paper-url="./papers/250621520v2-madrive-memory-augmented-driving-scene-modeling.html" onclick="toggleFavorite(this, '2506.21520v2', 'MADrive: Memory-Augmented Driving Scene Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250621348v1-panst3r-multi-view-consistent-panoptic-segmentation.html">PanSt3R: Multi-view Consistent Panoptic Segmentation</a></td>
  <td>æå‡ºPanSt3Rä»¥è§£å†³å¤šè§†è§’ä¸€è‡´çš„å…¨æ™¯åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21348v1" data-paper-url="./papers/250621348v1-panst3r-multi-view-consistent-panoptic-segmentation.html" onclick="toggleFavorite(this, '2506.21348v1', 'PanSt3R: Multi-view Consistent Panoptic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250621680v1-photonsplat-3d-scene-reconstruction-and-colorization-from-spad-senso.html">PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors</a></td>
  <td>æå‡ºPhotonSplatä»¥è§£å†³è¿åŠ¨æ¨¡ç³Šä¸‹çš„3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21680v1" data-paper-url="./papers/250621680v1-photonsplat-3d-scene-reconstruction-and-colorization-from-spad-senso.html" onclick="toggleFavorite(this, '2506.21680v1', 'PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250621526v2-waft-warping-alone-field-transforms-for-optical-flow.html">WAFT: Warping-Alone Field Transforms for Optical Flow</a></td>
  <td>æå‡ºWAFTä»¥è§£å†³å…‰æµä¼°è®¡ä¸­çš„é«˜å†…å­˜æ¶ˆè€—é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21526v2" data-paper-url="./papers/250621526v2-waft-warping-alone-field-transforms-for-optical-flow.html" onclick="toggleFavorite(this, '2506.21526v2', 'WAFT: Warping-Alone Field Transforms for Optical Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250621009v1-user-in-the-loop-view-sampling-with-error-peaking-visualization.html">User-in-the-Loop View Sampling with Error Peaking Visualization</a></td>
  <td>æå‡ºåŸºäºç”¨æˆ·åé¦ˆçš„è§†å›¾é‡‡æ ·æ–¹æ³•ä»¥è§£å†³ARæ•°æ®æ”¶é›†æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21009v1" data-paper-url="./papers/250621009v1-user-in-the-loop-view-sampling-with-error-peaking-visualization.html" onclick="toggleFavorite(this, '2506.21009v1', 'User-in-the-Loop View Sampling with Error Peaking Visualization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250621357v1-copa-sg-dense-scene-graphs-with-parametric-and-proto-relations.html">CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations</a></td>
  <td>æå‡ºCoPa-SGä»¥è§£å†³åœºæ™¯å›¾æ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21357v1" data-paper-url="./papers/250621357v1-copa-sg-dense-scene-graphs-with-parametric-and-proto-relations.html" onclick="toggleFavorite(this, '2506.21357v1', 'CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250621121v1-goirl-graph-oriented-inverse-reinforcement-learning-for-multimodal-t.html">GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction</a></td>
  <td>æå‡ºGoIRLæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">inverse reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21121v1" data-paper-url="./papers/250621121v1-goirl-graph-oriented-inverse-reinforcement-learning-for-multimodal-t.html" onclick="toggleFavorite(this, '2506.21121v1', 'GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250621080v1-egoadapt-adaptive-multisensory-distillation-and-policy-learning-for-.html">EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception</a></td>
  <td>æå‡ºEgoAdaptä»¥è§£å†³å¤šæ¨¡æ€è‡ªæˆ‘æ„ŸçŸ¥çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21080v1" data-paper-url="./papers/250621080v1-egoadapt-adaptive-multisensory-distillation-and-policy-learning-for-.html" onclick="toggleFavorite(this, '2506.21080v1', 'EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250621514v3-g2d-boosting-multimodal-learning-with-gradient-guided-distillation.html">G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation</a></td>
  <td>æå‡ºGÂ²Dä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21514v3" data-paper-url="./papers/250621514v3-g2d-boosting-multimodal-learning-with-gradient-guided-distillation.html" onclick="toggleFavorite(this, '2506.21514v3', 'G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250621541v3-strumamba3d-exploring-structural-mamba-for-self-supervised-point-clo.html">StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning</a></td>
  <td>æå‡ºStruMamba3Dä»¥è§£å†³SSMåœ¨ç‚¹äº‘è¡¨ç¤ºå­¦ä¹ ä¸­çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21541v3" data-paper-url="./papers/250621541v3-strumamba3d-exploring-structural-mamba-for-self-supervised-point-clo.html" onclick="toggleFavorite(this, '2506.21541v3', 'StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250621724v1-asymmetric-dual-self-distillation-for-3d-self-supervised-representat.html">Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning</a></td>
  <td>æå‡ºä¸å¯¹ç§°åŒé‡è‡ªè’¸é¦æ¡†æ¶ä»¥è§£å†³3Dè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21724v1" data-paper-url="./papers/250621724v1-asymmetric-dual-self-distillation-for-3d-self-supervised-representat.html" onclick="toggleFavorite(this, '2506.21724v1', 'Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250621312v1-continual-self-supervised-learning-with-masked-autoencoders-in-remot.html">Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing</a></td>
  <td>æå‡ºCoSMAEä»¥è§£å†³é¥æ„Ÿä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21312v1" data-paper-url="./papers/250621312v1-continual-self-supervised-learning-with-masked-autoencoders-in-remot.html" onclick="toggleFavorite(this, '2506.21312v1', 'Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250621277v1-humanomniv2-from-understanding-to-omni-modal-reasoning-with-context.html">HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context</a></td>
  <td>æå‡ºHumanOmniV2ä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¸Šä¸‹æ–‡ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21277v1" data-paper-url="./papers/250621277v1-humanomniv2-from-understanding-to-omni-modal-reasoning-with-context.html" onclick="toggleFavorite(this, '2506.21277v1', 'HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250621152v3-geometry-and-perception-guided-gaussians-for-multiview-consistent-3d.html">Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image</a></td>
  <td>æå‡ºå‡ ä½•ä¸æ„ŸçŸ¥å¼•å¯¼çš„é«˜æ–¯æ¨¡å‹ä»¥è§£å†³å•å›¾ç”Ÿæˆ3Dä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21152v3" data-paper-url="./papers/250621152v3-geometry-and-perception-guided-gaussians-for-multiview-consistent-3d.html" onclick="toggleFavorite(this, '2506.21152v3', 'Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250621656v2-fine-grained-preference-optimization-improves-spatial-reasoning-in-v.html">Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs</a></td>
  <td>æå‡ºSpatialReasoner-R1ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21656v2" data-paper-url="./papers/250621656v2-fine-grained-preference-optimization-improves-spatial-reasoning-in-v.html" onclick="toggleFavorite(this, '2506.21656v2', 'Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250620947v1-hierarchical-sub-action-tree-for-continuous-sign-language-recognitio.html">Hierarchical Sub-action Tree for Continuous Sign Language Recognition</a></td>
  <td>æå‡ºå±‚æ¬¡å­åŠ¨ä½œæ ‘ä»¥è§£å†³è¿ç»­æ‰‹è¯­è¯†åˆ«ä¸­çš„æ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20947v1" data-paper-url="./papers/250620947v1-hierarchical-sub-action-tree-for-continuous-sign-language-recognitio.html" onclick="toggleFavorite(this, '2506.20947v1', 'Hierarchical Sub-action Tree for Continuous Sign Language Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/250621552v1-whole-body-conditioned-egocentric-video-prediction.html">Whole-Body Conditioned Egocentric Video Prediction</a></td>
  <td>æå‡ºåŸºäºå…¨èº«æ¡ä»¶çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é¢„æµ‹ä»¥è§£å†³ç¯å¢ƒå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21552v1" data-paper-url="./papers/250621552v1-whole-body-conditioned-egocentric-video-prediction.html" onclick="toggleFavorite(this, '2506.21552v1', 'Whole-Body Conditioned Egocentric Video Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250621785v1-comparing-learning-paradigms-for-egocentric-video-summarization.html">Comparing Learning Paradigms for Egocentric Video Summarization</a></td>
  <td>æ¯”è¾ƒå­¦ä¹ èŒƒå¼ä»¥æå‡ç¬¬ä¸€äººç§°è§†é¢‘æ‘˜è¦æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21785v1" data-paper-url="./papers/250621785v1-comparing-learning-paradigms-for-egocentric-video-summarization.html" onclick="toggleFavorite(this, '2506.21785v1', 'Comparing Learning Paradigms for Egocentric Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250620900v1-the-role-of-cyclopean-eye-in-stereo-vision.html">The Role of Cyclopean-Eye in Stereo Vision</a></td>
  <td>æå‡ºæ–°çš„å‡ ä½•çº¦æŸä»¥æ”¹å–„ç«‹ä½“è§†è§‰ä¸­çš„æ·±åº¦é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20900v1" data-paper-url="./papers/250620900v1-the-role-of-cyclopean-eye-in-stereo-vision.html" onclick="toggleFavorite(this, '2506.20900v1', 'The Role of Cyclopean-Eye in Stereo Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250621034v2-didsee-diffusion-based-depth-completion-for-material-agnostic-roboti.html">DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation</a></td>
  <td>æå‡ºDidSeeä»¥è§£å†³éæœ—ä¼¯ç‰©ä½“çš„æ·±åº¦è¡¥å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21034v2" data-paper-url="./papers/250621034v2-didsee-diffusion-based-depth-completion-for-material-agnostic-roboti.html" onclick="toggleFavorite(this, '2506.21034v2', 'DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250621165v1-topology-aware-modeling-for-unsupervised-simulation-to-reality-point.html">Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition</a></td>
  <td>æå‡ºæ‹“æ‰‘æ„ŸçŸ¥å»ºæ¨¡ä»¥è§£å†³æ— ç›‘ç£ä»¿çœŸåˆ°ç°å®ç‚¹äº‘è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span> <span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21165v1" data-paper-url="./papers/250621165v1-topology-aware-modeling-for-unsupervised-simulation-to-reality-point.html" onclick="toggleFavorite(this, '2506.21165v1', 'Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/250620922v1-m2sformer-multi-spectral-and-multi-scale-attention-with-edge-aware-d.html">M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization</a></td>
  <td>æå‡ºM2SFormerä»¥è§£å†³å›¾åƒä¼ªé€ å®šä½ä¸­çš„ç»†èŠ‚æŸå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20922v1" data-paper-url="./papers/250620922v1-m2sformer-multi-spectral-and-multi-scale-attention-with-edge-aware-d.html" onclick="toggleFavorite(this, '2506.20922v1', 'M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/250621452v1-rethinking-oversaturation-in-classifier-free-guidance-via-low-freque.html">Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency</a></td>
  <td>æå‡ºä½é¢‘æ”¹è¿›çš„æ— åˆ†ç±»å™¨å¼•å¯¼ä»¥è§£å†³è¿‡é¥±å’Œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21452v1" data-paper-url="./papers/250621452v1-rethinking-oversaturation-in-classifier-free-guidance-via-low-freque.html" onclick="toggleFavorite(this, '2506.21452v1', 'Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250620936v2-physrig-differentiable-physics-based-skinning-and-rigging-framework-.html">PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling</a></td>
  <td>æå‡ºPhysRigæ¡†æ¶ä»¥è§£å†³ä¼ ç»Ÿçš®è‚¤ç»‘å®šä¸è£…é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.20936v2" data-paper-url="./papers/250620936v2-physrig-differentiable-physics-based-skinning-and-rigging-framework-.html" onclick="toggleFavorite(this, '2506.20936v2', 'PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>49</td>
  <td><a href="./papers/250621499v1-lightweight-physics-informed-zero-shot-ultrasound-plane-wave-denoisi.html">Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising</a></td>
  <td>æå‡ºè½»é‡çº§ç‰©ç†ä¿¡æ¯é›¶-shotè¶…å£°å¹³é¢æ³¢å»å™ªæ–¹æ³•ä»¥è§£å†³å›¾åƒå™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">structure preservation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21499v1" data-paper-url="./papers/250621499v1-lightweight-physics-informed-zero-shot-ultrasound-plane-wave-denoisi.html" onclick="toggleFavorite(this, '2506.21499v1', 'Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>50</td>
  <td><a href="./papers/250621076v1-posemaster-generating-3d-characters-in-arbitrary-poses-from-a-single.html">PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image</a></td>
  <td>æå‡ºPoseMasterä»¥è§£å†³3Dè§’è‰²å»ºæ¨¡ä¸­çš„å§¿æ€æ ‡å‡†åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">character animation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.21076v1" data-paper-url="./papers/250621076v1-posemaster-generating-3d-characters-in-arbitrary-poses-from-a-single.html" onclick="toggleFavorite(this, '2506.21076v1', 'PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)