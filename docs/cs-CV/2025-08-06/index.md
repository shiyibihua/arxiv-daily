---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-06
---

# cs.CVï¼ˆ2025-08-06ï¼‰

ğŸ“Š å…± **52** ç¯‡è®ºæ–‡
 | ğŸ”— **18** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (23 ğŸ”—9)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (12 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (23 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250804192v1-from-learning-to-unlearning-biomedical-security-protection-in-multim.html">From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models</a></td>
  <td>æå‡ºMLLMU-Medä»¥è§£å†³ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04192v1" data-paper-url="./papers/250804192v1-from-learning-to-unlearning-biomedical-security-protection-in-multim.html" onclick="toggleFavorite(this, '2508.04192v1', 'From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250804059v1-beyond-the-visible-benchmarking-occlusion-perception-in-multimodal-l.html">Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models</a></td>
  <td>æå‡ºO-Benchä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é®æŒ¡æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04059v1" data-paper-url="./papers/250804059v1-beyond-the-visible-benchmarking-occlusion-perception-in-multimodal-l.html" onclick="toggleFavorite(this, '2508.04059v1', 'Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250804136v1-unifgvc-universal-training-free-few-shot-fine-grained-vision-classif.html">UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval</a></td>
  <td>æå‡ºUniFGVCä»¥è§£å†³å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04136v1" data-paper-url="./papers/250804136v1-unifgvc-universal-training-free-few-shot-fine-grained-vision-classif.html" onclick="toggleFavorite(this, '2508.04136v1', 'UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250804900v1-revealing-temporal-label-noise-in-multimodal-hateful-video-classific.html">Revealing Temporal Label Noise in Multimodal Hateful Video Classification</a></td>
  <td>æå‡ºç»†ç²’åº¦æ ‡ç­¾å™ªå£°åˆ†æä»¥æå‡å¤šæ¨¡æ€ä»‡æ¨è§†é¢‘åˆ†ç±»å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">TAMP</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04900v1" data-paper-url="./papers/250804900v1-revealing-temporal-label-noise-in-multimodal-hateful-video-classific.html" onclick="toggleFavorite(this, '2508.04900v1', 'Revealing Temporal Label Noise in Multimodal Hateful Video Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250804625v1-finmmr-make-financial-numerical-reasoning-more-multimodal-comprehens.html">FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</a></td>
  <td>æå‡ºFinMMRä»¥æå‡é‡‘èæ•°å€¼æ¨ç†çš„å¤šæ¨¡æ€èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04625v1" data-paper-url="./papers/250804625v1-finmmr-make-financial-numerical-reasoning-more-multimodal-comprehens.html" onclick="toggleFavorite(this, '2508.04625v1', 'FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250804175v1-ad-fm-multimodal-llms-for-anomaly-detection-via-multi-stage-reasonin.html">AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization</a></td>
  <td>æå‡ºAD-FMæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹ä¸­çš„é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04175v1" data-paper-url="./papers/250804175v1-ad-fm-multimodal-llms-for-anomaly-detection-via-multi-stage-reasonin.html" onclick="toggleFavorite(this, '2508.04175v1', 'AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250804017v1-can-large-multimodal-models-actively-recognize-faulty-inputs-a-syste.html">Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability</a></td>
  <td>æå‡ºè¾“å…¥å®¡æŸ¥èƒ½åŠ›è¯„ä¼°æ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹è¾“å…¥é”™è¯¯è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04017v1" data-paper-url="./papers/250804017v1-can-large-multimodal-models-actively-recognize-faulty-inputs-a-syste.html" onclick="toggleFavorite(this, '2508.04017v1', 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250804450v1-totalregistrator-towards-a-lightweight-foundation-model-for-ct-image.html">TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration</a></td>
  <td>æå‡ºTotalRegistratorä»¥è§£å†³CTå›¾åƒå¤šå™¨å®˜é…å‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04450v1" data-paper-url="./papers/250804450v1-totalregistrator-towards-a-lightweight-foundation-model-for-ct-image.html" onclick="toggleFavorite(this, '2508.04450v1', 'TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250804441v1-benchmarking-foundation-models-for-mitotic-figure-classification.html">Benchmarking Foundation Models for Mitotic Figure Classification</a></td>
  <td>æå‡ºè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä»¥æå‡æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04441v1" data-paper-url="./papers/250804441v1-benchmarking-foundation-models-for-mitotic-figure-classification.html" onclick="toggleFavorite(this, '2508.04441v1', 'Benchmarking Foundation Models for Mitotic Figure Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250804379v3-visionts-cross-modal-time-series-foundation-model-with-continual-pre.html">VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Vision Backbones</a></td>
  <td>æå‡ºVisionTS++ä»¥è§£å†³è§†è§‰æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è·¨æ¨¡æ€è½¬ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04379v3" data-paper-url="./papers/250804379v3-visionts-cross-modal-time-series-foundation-model-with-continual-pre.html" onclick="toggleFavorite(this, '2508.04379v3', 'VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Vision Backbones')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250804229v1-intention-enhanced-diffusion-model-for-multimodal-pedestrian-traject.html">Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction</a></td>
  <td>æå‡ºæ„å›¾å¢å¼ºæ‰©æ•£æ¨¡å‹ä»¥è§£å†³å¤šæ¨¡æ€è¡Œäººè½¨è¿¹é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04229v1" data-paper-url="./papers/250804229v1-intention-enhanced-diffusion-model-for-multimodal-pedestrian-traject.html" onclick="toggleFavorite(this, '2508.04229v1', 'Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250804205v1-small-lesions-aware-bidirectional-multimodal-multiscale-fusion-netwo.html">Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification</a></td>
  <td>æå‡ºMMCAF-Netä»¥è§£å†³å°ç—…ç¶è¯¯è¯Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04205v1" data-paper-url="./papers/250804205v1-small-lesions-aware-bidirectional-multimodal-multiscale-fusion-netwo.html" onclick="toggleFavorite(this, '2508.04205v1', 'Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250804129v1-svc-2025-the-first-multimodal-deception-detection-challenge.html">SVC 2025: the First Multimodal Deception Detection Challenge</a></td>
  <td>æå‡ºSVC 2025æŒ‘æˆ˜ä»¥è§£å†³å¤šæ¨¡æ€æ¬ºéª—æ£€æµ‹çš„è·¨åŸŸæ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04129v1" data-paper-url="./papers/250804129v1-svc-2025-the-first-multimodal-deception-detection-challenge.html" onclick="toggleFavorite(this, '2508.04129v1', 'SVC 2025: the First Multimodal Deception Detection Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250804655v1-x-sam-from-segment-anything-to-any-segmentation.html">X-SAM: From Segment Anything to Any Segmentation</a></td>
  <td>æå‡ºX-SAMä»¥è§£å†³ç°æœ‰å›¾åƒåˆ†å‰²æ¨¡å‹çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04655v1" data-paper-url="./papers/250804655v1-x-sam-from-segment-anything-to-any-segmentation.html" onclick="toggleFavorite(this, '2508.04655v1', 'X-SAM: From Segment Anything to Any Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250804566v1-clasp-cross-modal-salient-anchor-based-semantic-propagation-for-weak.html">CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization</a></td>
  <td>æå‡ºåŸºäºè·¨æ¨¡æ€æ˜¾è‘—é”šç‚¹çš„è¯­ä¹‰ä¼ æ’­æ–¹æ³•ä»¥è§£å†³å¼±ç›‘ç£å¯†é›†éŸ³è§†é¢‘äº‹ä»¶å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04566v1" data-paper-url="./papers/250804566v1-clasp-cross-modal-salient-anchor-based-semantic-propagation-for-weak.html" onclick="toggleFavorite(this, '2508.04566v1', 'CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250806553v1-static-and-plugged-make-embodied-evaluation-simple.html">Static and Plugged: Make Embodied Evaluation Simple</a></td>
  <td>æå‡ºStaticEmbodiedBenchä»¥è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.06553v1" data-paper-url="./papers/250806553v1-static-and-plugged-make-embodied-evaluation-simple.html" onclick="toggleFavorite(this, '2508.06553v1', 'Static and Plugged: Make Embodied Evaluation Simple')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250804107v3-unlocking-the-potential-of-mllms-in-referring-expression-segmentatio.html">Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder</a></td>
  <td>æå‡ºMLLMSegä»¥è§£å†³å‚è€ƒè¡¨è¾¾åˆ†å‰²ä¸­çš„æ€§èƒ½ä¸æˆæœ¬é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04107v3" data-paper-url="./papers/250804107v3-unlocking-the-potential-of-mllms-in-referring-expression-segmentatio.html" onclick="toggleFavorite(this, '2508.04107v3', 'Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250804650v1-encqa-benchmarking-vision-language-models-on-visual-encodings-for-ch.html">EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts</a></td>
  <td>æå‡ºEncQAåŸºå‡†ä»¥æå‡å›¾è¡¨ç†è§£çš„è§†è§‰æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04650v1" data-paper-url="./papers/250804650v1-encqa-benchmarking-vision-language-models-on-visual-encodings-for-ch.html" onclick="toggleFavorite(this, '2508.04650v1', 'EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250804592v2-face-voice-association-in-multilingual-environments-fame-2026-challe.html">Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan</a></td>
  <td>æå‡ºFAMEæŒ‘æˆ˜ä»¥è§£å†³å¤šè¯­è¨€ç¯å¢ƒä¸­çš„äººè„¸ä¸å£°éŸ³å…³è”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04592v2" data-paper-url="./papers/250804592v2-face-voice-association-in-multilingual-environments-fame-2026-challe.html" onclick="toggleFavorite(this, '2508.04592v2', 'Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250804567v1-analyzing-and-mitigating-object-hallucination-a-training-bias-perspe.html">Analyzing and Mitigating Object Hallucination: A Training Bias Perspective</a></td>
  <td>æå‡ºObliviateä»¥è§£å†³å¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ä½“å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04567v1" data-paper-url="./papers/250804567v1-analyzing-and-mitigating-object-hallucination-a-training-bias-perspe.html" onclick="toggleFavorite(this, '2508.04567v1', 'Analyzing and Mitigating Object Hallucination: A Training Bias Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250804418v1-think-before-you-segment-an-object-aware-reasoning-agent-for-referri.html">Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation</a></td>
  <td>æå‡ºTGS-Agentä»¥è§£å†³éŸ³é¢‘è§†è§‰åˆ†å‰²ä¸­çš„å¯¹è±¡ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04418v1" data-paper-url="./papers/250804418v1-think-before-you-segment-an-object-aware-reasoning-agent-for-referri.html" onclick="toggleFavorite(this, '2508.04418v1', 'Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250804227v1-continual-learning-for-vlms-a-survey-and-taxonomy-beyond-forgetting.html">Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting</a></td>
  <td>æå‡ºé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ æ–¹æ³•ä»¥è§£å†³é—å¿˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04227v1" data-paper-url="./papers/250804227v1-continual-learning-for-vlms-a-survey-and-taxonomy-beyond-forgetting.html" onclick="toggleFavorite(this, '2508.04227v1', 'Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250804166v1-toxictags-decoding-toxic-memes-with-rich-tag-annotations.html">ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations</a></td>
  <td>æå‡ºToxicTAGSä»¥è§£å†³æœ‰å®³è¡¨æƒ…åŒ…å†…å®¹çš„æ ‡æ³¨ä¸æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04166v1" data-paper-url="./papers/250804166v1-toxictags-decoding-toxic-memes-with-rich-tag-annotations.html" onclick="toggleFavorite(this, '2508.04166v1', 'ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250804416v2-thinking-with-videos-multimodal-tool-augmented-reinforcement-learnin.html">Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</a></td>
  <td>æå‡ºVITALæ¡†æ¶ä»¥è§£å†³é•¿è§†é¢‘æ¨ç†ä¸­çš„å¤šæ¨¡æ€äº¤äº’ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04416v2" data-paper-url="./papers/250804416v2-thinking-with-videos-multimodal-tool-augmented-reinforcement-learnin.html" onclick="toggleFavorite(this, '2508.04416v2', 'Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250806558v1-on-the-effectiveness-of-multimodal-privileged-knowledge-distillation.html">On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications</a></td>
  <td>æå‡ºå¤šæ¨¡æ€ç‰¹æƒçŸ¥è¯†è’¸é¦ä»¥æå‡è§†è§‰æ¨¡å‹è¯Šæ–­èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.06558v1" data-paper-url="./papers/250806558v1-on-the-effectiveness-of-multimodal-privileged-knowledge-distillation.html" onclick="toggleFavorite(this, '2508.06558v1', 'On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250804316v1-a-foundation-model-for-das-signal-recognition-and-visual-prompt-tuni.html">A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks</a></td>
  <td>æå‡ºMAEPDæ¨¡å‹ä»¥è§£å†³DASä¿¡å·è¯†åˆ«ä¸­çš„æ•°æ®åˆ†å¸ƒä¸å‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04316v1" data-paper-url="./papers/250804316v1-a-foundation-model-for-das-signal-recognition-and-visual-prompt-tuni.html" onclick="toggleFavorite(this, '2508.04316v1', 'A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250804816v1-comad-a-multiple-teacher-self-supervised-distillation-framework.html">CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework</a></td>
  <td>æå‡ºCoMADæ¡†æ¶ä»¥è§£å†³è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„èµ„æºé™åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04816v1" data-paper-url="./papers/250804816v1-comad-a-multiple-teacher-self-supervised-distillation-framework.html" onclick="toggleFavorite(this, '2508.04816v1', 'CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250804705v1-occupancy-learning-with-spatiotemporal-memory.html">Occupancy Learning with Spatiotemporal Memory</a></td>
  <td>æå‡ºST-Occä»¥è§£å†³3Då ç”¨ç‡å­¦ä¹ ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04705v1" data-paper-url="./papers/250804705v1-occupancy-learning-with-spatiotemporal-memory.html" onclick="toggleFavorite(this, '2508.04705v1', 'Occupancy Learning with Spatiotemporal Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250804369v4-tspo-temporal-sampling-policy-optimization-for-long-form-video-langu.html">TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding</a></td>
  <td>æå‡ºTSPOä»¥è§£å†³é•¿è§†é¢‘è¯­è¨€ç†è§£ä¸­çš„é‡‡æ ·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04369v4" data-paper-url="./papers/250804369v4-tspo-temporal-sampling-policy-optimization-for-long-form-video-langu.html" onclick="toggleFavorite(this, '2508.04369v4', 'TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250804702v1-bevcon-advancing-birds-eye-view-perception-with-contrastive-learning.html">BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning</a></td>
  <td>æå‡ºBEVConä»¥æå‡è‡ªåŠ¨é©¾é©¶ä¸­çš„é¸Ÿç°è§†å›¾æ„ŸçŸ¥</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04702v1" data-paper-url="./papers/250804702v1-bevcon-advancing-birds-eye-view-perception-with-contrastive-learning.html" onclick="toggleFavorite(this, '2508.04702v1', 'BEVCon: Advancing Bird&#39;s Eye View Perception with Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250804429v1-unmasking-interstitial-lung-diseases-leveraging-masked-autoencoders-.html">Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis</a></td>
  <td>åˆ©ç”¨æ©ç è‡ªç¼–ç å™¨æå‡é—´è´¨æ€§è‚ºç—…çš„è¯Šæ–­èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04429v1" data-paper-url="./papers/250804429v1-unmasking-interstitial-lung-diseases-leveraging-masked-autoencoders-.html" onclick="toggleFavorite(this, '2508.04429v1', 'Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250804539v1-topkd-top-scaled-knowledge-distillation.html">TopKD: Top-scaled Knowledge Distillation</a></td>
  <td>æå‡ºTopKDä»¥æå‡çŸ¥è¯†è’¸é¦ä¸­çš„logitä¿¡æ¯åˆ©ç”¨</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04539v1" data-paper-url="./papers/250804539v1-topkd-top-scaled-knowledge-distillation.html" onclick="toggleFavorite(this, '2508.04539v1', 'TopKD: Top-scaled Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250804124v1-learning-using-privileged-information-for-litter-detection.html">Learning Using Privileged Information for Litter Detection</a></td>
  <td>æå‡ºç»“åˆç‰¹æƒä¿¡æ¯çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä»¥æé«˜åƒåœ¾æ£€æµ‹ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">privileged information</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04124v1" data-paper-url="./papers/250804124v1-learning-using-privileged-information-for-litter-detection.html" onclick="toggleFavorite(this, '2508.04124v1', 'Learning Using Privileged Information for Litter Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250804016v3-s2q-vdit-accurate-quantized-video-diffusion-transformer-with-salient.html">S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation</a></td>
  <td>æå‡ºS$^2$Q-VDiTä»¥è§£å†³è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é‡åŒ–ä¸å­¦ä¹ æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04016v3" data-paper-url="./papers/250804016v3-s2q-vdit-accurate-quantized-video-diffusion-transformer-with-salient.html" onclick="toggleFavorite(this, '2508.04016v3', 'S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250804201v2-vifp-a-framework-for-visual-false-positive-detection-to-enhance-reas.html">ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs</a></td>
  <td>æå‡ºViFPæ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é”™è¯¯æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04201v2" data-paper-url="./papers/250804201v2-vifp-a-framework-for-visual-false-positive-detection-to-enhance-reas.html" onclick="toggleFavorite(this, '2508.04201v2', 'ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/250804099v1-det-gs-depth-and-edge-aware-regularization-for-high-fidelity-3d-gaus.html">DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting</a></td>
  <td>æå‡ºDET-GSä»¥è§£å†³ç¨€ç–è§†å›¾ä¸‹3Dé‡å»ºç²¾åº¦ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">metric depth</span> <span class="paper-tag">3D gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04099v1" data-paper-url="./papers/250804099v1-det-gs-depth-and-edge-aware-regularization-for-high-fidelity-3d-gaus.html" onclick="toggleFavorite(this, '2508.04099v1', 'DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250804297v2-mugs-multi-baseline-generalizable-gaussian-splatting-reconstruction.html">MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction</a></td>
  <td>æå‡ºMuGSä»¥è§£å†³å¤šåŸºçº¿è§†å›¾åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04297v2" data-paper-url="./papers/250804297v2-mugs-multi-baseline-generalizable-gaussian-splatting-reconstruction.html" onclick="toggleFavorite(this, '2508.04297v2', 'MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250804929v3-cryosplat-gaussian-splatting-for-cryo-em-homogeneous-reconstruction.html">CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction</a></td>
  <td>æå‡ºCryoSplatä»¥è§£å†³å†·å†»ç”µå­æ˜¾å¾®é•œé‡å»ºä¸­çš„åˆå§‹åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04929v3" data-paper-url="./papers/250804929v3-cryosplat-gaussian-splatting-for-cryo-em-homogeneous-reconstruction.html" onclick="toggleFavorite(this, '2508.04929v3', 'CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250804211v1-what-holds-back-open-vocabulary-segmentation.html">What Holds Back Open-Vocabulary Segmentation?</a></td>
  <td>æå‡ºæ–°å‹ç»„ä»¶ä»¥è§£å†³å¼€æ”¾è¯æ±‡åˆ†å‰²çš„ç“¶é¢ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04211v1" data-paper-url="./papers/250804211v1-what-holds-back-open-vocabulary-segmentation.html" onclick="toggleFavorite(this, '2508.04211v1', 'What Holds Back Open-Vocabulary Segmentation?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250804611v2-bridgedepth-bridging-monocular-and-stereo-reasoning-with-latent-alig.html">BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment</a></td>
  <td>æå‡ºBridgeDepthä»¥è§£å†³å•ç›®ä¸ç«‹ä½“æ·±åº¦ä¼°è®¡çš„èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">stereo depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04611v2" data-paper-url="./papers/250804611v2-bridgedepth-bridging-monocular-and-stereo-reasoning-with-latent-alig.html" onclick="toggleFavorite(this, '2508.04611v2', 'BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250804928v3-extending-foundational-monocular-depth-estimators-to-fisheye-cameras.html">Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens</a></td>
  <td>æå‡ºä¸€ç§æ–¹æ³•å°†å•ç›®æ·±åº¦ä¼°è®¡æ‰©å±•è‡³é±¼çœ¼ç›¸æœº</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04928v3" data-paper-url="./papers/250804928v3-extending-foundational-monocular-depth-estimators-to-fisheye-cameras.html" onclick="toggleFavorite(this, '2508.04928v3', 'Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250804597v1-pseudo-depth-meets-gaussian-a-feed-forward-rgb-slam-baseline.html">Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline</a></td>
  <td>æå‡ºåŸºäº3Dé«˜æ–¯æ˜ å°„çš„RGB SLAMæ–¹æ³•ä»¥è§£å†³æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">optical flow</span> <span class="paper-tag">SplaTAM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04597v1" data-paper-url="./papers/250804597v1-pseudo-depth-meets-gaussian-a-feed-forward-rgb-slam-baseline.html" onclick="toggleFavorite(this, '2508.04597v1', 'Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250804224v1-splitgaussian-reconstructing-dynamic-scenes-via-visual-geometry-deco.html">SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition</a></td>
  <td>æå‡ºSplitGaussianä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„è¿åŠ¨æ³„æ¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04224v1" data-paper-url="./papers/250804224v1-splitgaussian-reconstructing-dynamic-scenes-via-visual-geometry-deco.html" onclick="toggleFavorite(this, '2508.04224v1', 'SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250804147v1-idcnet-guided-video-diffusion-for-metric-consistent-rgbd-scene-gener.html">IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control</a></td>
  <td>æå‡ºIDC-Netä»¥è§£å†³RGB-Dè§†é¢‘ç”Ÿæˆä¸­çš„å‡ ä½•ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04147v1" data-paper-url="./papers/250804147v1-idcnet-guided-video-diffusion-for-metric-consistent-rgbd-scene-gener.html" onclick="toggleFavorite(this, '2508.04147v1', 'IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250804236v2-pis3r-very-large-parallax-image-stitching-via-deep-3d-reconstruction.html">PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction</a></td>
  <td>æå‡ºPIS3Rä»¥è§£å†³å¤§è§†å·®å›¾åƒæ‹¼æ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04236v2" data-paper-url="./papers/250804236v2-pis3r-very-large-parallax-image-stitching-via-deep-3d-reconstruction.html" onclick="toggleFavorite(this, '2508.04236v2', 'PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/250804681v1-perceiving-and-acting-in-first-person-a-dataset-and-benchmark-for-eg.html">Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</a></td>
  <td>æå‡ºInterVLAæ•°æ®é›†ä»¥è§£å†³äººæœºäº¤äº’ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04681v1" data-paper-url="./papers/250804681v1-perceiving-and-acting-in-first-person-a-dataset-and-benchmark-for-eg.html" onclick="toggleFavorite(this, '2508.04681v1', 'Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/250804559v2-one-model-for-all-unified-try-on-and-try-off-in-any-pose-via-llm-ins.html">One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion</a></td>
  <td>æå‡ºOMFAæ¡†æ¶ä»¥è§£å†³è™šæ‹Ÿè¯•è¡£ä¸è¯•è„±çš„çµæ´»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SMPL</span> <span class="paper-tag">SMPL-X</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04559v2" data-paper-url="./papers/250804559v2-one-model-for-all-unified-try-on-and-try-off-in-any-pose-via-llm-ins.html" onclick="toggleFavorite(this, '2508.04559v2', 'One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250804050v1-domr-establishing-cross-view-segmentation-via-dense-object-matching.html">DOMR: Establishing Cross-View Segmentation via Dense Object Matching</a></td>
  <td>æå‡ºDOMRæ¡†æ¶ä»¥è§£å†³è·¨è§†è§’ç‰©ä½“åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04050v1" data-paper-url="./papers/250804050v1-domr-establishing-cross-view-segmentation-via-dense-object-matching.html" onclick="toggleFavorite(this, '2508.04050v1', 'DOMR: Establishing Cross-View Segmentation via Dense Object Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>49</td>
  <td><a href="./papers/250804568v1-ddtracking-a-deep-generative-framework-for-diffusion-mri-tractograph.html">DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling</a></td>
  <td>æå‡ºDDTrackingä»¥è§£å†³æ‰©æ•£MRIè½¨è¿¹é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04568v1" data-paper-url="./papers/250804568v1-ddtracking-a-deep-generative-framework-for-diffusion-mri-tractograph.html" onclick="toggleFavorite(this, '2508.04568v1', 'DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250804682v2-turbotrain-towards-efficient-and-balanced-multi-task-learning-for-mu.html">TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction</a></td>
  <td>æå‡ºTurboTrainä»¥è§£å†³å¤šä»£ç†æ„ŸçŸ¥ä¸é¢„æµ‹çš„é«˜æ•ˆè®­ç»ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04682v2" data-paper-url="./papers/250804682v2-turbotrain-towards-efficient-and-balanced-multi-task-learning-for-mu.html" onclick="toggleFavorite(this, '2508.04682v2', 'TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>51</td>
  <td><a href="./papers/250804043v1-visualtrans-a-benchmark-for-real-world-visual-transformation-reasoni.html">VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</a></td>
  <td>æå‡ºVisualTransä»¥è§£å†³ç°å®åœºæ™¯ä¸­çš„è§†è§‰è½¬åŒ–æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">human-object interaction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04043v1" data-paper-url="./papers/250804043v1-visualtrans-a-benchmark-for-real-world-visual-transformation-reasoni.html" onclick="toggleFavorite(this, '2508.04043v1', 'VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>52</td>
  <td><a href="./papers/250804049v1-motion-is-the-choreographer-learning-latent-pose-dynamics-for-seamle.html">Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation</a></td>
  <td>æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥è§£å†³æ‰‹è¯­è§†é¢‘ç”Ÿæˆä¸­çš„æ•°æ®éœ€æ±‚ä¸æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion synthesis</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.04049v1" data-paper-url="./papers/250804049v1-motion-is-the-choreographer-learning-latent-pose-dynamics-for-seamle.html" onclick="toggleFavorite(this, '2508.04049v1', 'Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)