---
layout: default
title: CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework
---

# CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04816" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04816v1</a>
  <a href="https://arxiv.org/pdf/2508.04816.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04816v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04816v1', 'CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sriram Mandalika, Lalitha V

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 8 Pages, 2 Figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoMADæ¡†æ¶ä»¥è§£å†³è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„èµ„æºé™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `è§†è§‰å˜æ¢å™¨` `æ¨¡å‹å‹ç¼©` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•é€šå¸¸åœ¨å­¤ç«‹ç¯å¢ƒä¸­é¢„è®­ç»ƒï¼Œå¿½è§†äº†æ¨¡å‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åºå¤§ä¸”ä¸é€‚åˆèµ„æºå—é™çš„éƒ¨ç½²ã€‚
2. æœ¬æ–‡æå‡ºçš„CoMADæ¡†æ¶é€šè¿‡å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ï¼Œç»“åˆä¸åŒè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ„å»ºä¸€ä¸ªç´§å‡‘çš„å­¦ç”Ÿç½‘ç»œã€‚
3. åœ¨ImageNet-1Kä¸Šï¼ŒCoMADçš„ViT-Tinyæ¨¡å‹è¾¾åˆ°äº†75.4%çš„Top-1å‡†ç¡®ç‡ï¼Œä¸”åœ¨å…¶ä»–ä»»åŠ¡ä¸Šä¹Ÿåˆ›é€ äº†æ–°çš„æ€§èƒ½è®°å½•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šè‡ªç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œå¦‚å¯¹æ¯”å­¦ä¹ å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼Œèƒ½å¤Ÿä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ å¼ºå¤§çš„è¡¨ç¤ºï¼Œä½†é€šå¸¸æ˜¯å­¤ç«‹é¢„è®­ç»ƒï¼Œå¿½è§†äº†äº’è¡¥çš„è§è§£ï¼Œå¹¶å¯¼è‡´å¤§å‹æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ— å‚æ•°æ¡†æ¶â€”â€”å…±è¯†å¯¼å‘æ©ç è’¸é¦ï¼ˆCoMADï¼‰ï¼Œå°†å¤šä¸ªå½“å‰æœ€å…ˆè¿›çš„è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨çš„çŸ¥è¯†ç»Ÿä¸€åˆ°ä¸€ä¸ªç´§å‡‘çš„å­¦ç”Ÿç½‘ç»œä¸­ã€‚CoMADä»ä¸‰ä¸ªé¢„è®­ç»ƒçš„ViT-Baseæ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ï¼Œåˆ†åˆ«æ˜¯MAEã€MoCo v3å’ŒiBOTï¼Œæ¯ä¸ªæ¨¡å‹æä¾›ç‹¬ç‰¹çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡å…ˆéªŒã€‚é€šè¿‡ä¸å¯¹ç§°æ©ç ï¼Œå­¦ç”Ÿä»…çœ‹åˆ°25%çš„å›¾åƒå—ï¼Œè€Œæ¯ä¸ªæ•™å¸ˆåˆ™æ¥æ”¶é€æ¸å‡è½»çš„ç‹¬ç‰¹æ©ç ï¼Œè¿«ä½¿å­¦ç”Ÿåœ¨æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¸­æ’å€¼ç¼ºå¤±ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoMADçš„ViT-Tinyåœ¨ImageNet-1Kä¸Šè¾¾åˆ°äº†75.4%çš„Top-1å‡†ç¡®ç‡ï¼Œæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº†0.4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å­¤ç«‹ç¯å¢ƒä¸­é¢„è®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹åºå¤§ä¸”éš¾ä»¥éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoMADæ¡†æ¶é€šè¿‡æ•´åˆå¤šä¸ªè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„çŸ¥è¯†ï¼Œé‡‡ç”¨ä¸å¯¹ç§°æ©ç ç­–ç•¥ï¼Œè¿«ä½¿å­¦ç”Ÿæ¨¡å‹åœ¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨ç°å’Œç´§å‡‘æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoMADçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•™å¸ˆæ¨¡å‹ï¼ˆMAEã€MoCo v3å’ŒiBOTï¼‰ï¼Œå­¦ç”Ÿæ¨¡å‹ï¼ˆViT-Tinyï¼‰ï¼Œä»¥åŠå…±è¯†é—¨æ§æœºåˆ¶ã€‚æ•™å¸ˆæ¨¡å‹æä¾›ä¸åŒçš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡çº¿æ€§é€‚é…å™¨å’Œå±‚å½’ä¸€åŒ–å¯¹é½æ•™å¸ˆåµŒå…¥ï¼Œå¹¶é€šè¿‡å…±è¯†é—¨æ§èåˆä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨ä¸å¯¹ç§°æ©ç ç­–ç•¥ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹ä»…èƒ½çœ‹åˆ°éƒ¨åˆ†è¾“å…¥ï¼ŒåŒæ—¶æ•™å¸ˆæ¨¡å‹æ¥æ”¶ä¸åŒçš„æ©ç ï¼Œä»è€Œå¢å¼ºäº†å­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾æ’å€¼èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ç”Ÿæ¨¡å‹ä½¿ç”¨åŒå±‚KLæ•£åº¦æŸå¤±å‡½æ•°ï¼Œåˆ†åˆ«å¯¹å¯è§æ ‡è®°å’Œé‡å»ºç‰¹å¾å›¾è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoMADçš„ViT-Tinyæ¨¡å‹åœ¨ImageNet-1Kä¸Šè¾¾åˆ°äº†75.4%çš„Top-1å‡†ç¡®ç‡ï¼Œæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº†0.4%ã€‚åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒCoMADåœ¨ADE20Kä¸Šè¾¾åˆ°äº†47.3%çš„mIoUï¼Œåœ¨MS-COCOä¸Šåˆ†åˆ«è¾¾åˆ°äº†44.5%çš„æ¡†å¹³å‡ç²¾åº¦å’Œ40.5%çš„æ©ç å¹³å‡ç²¾åº¦ï¼Œç¡®ç«‹äº†ç´§å‡‘è‡ªç›‘ç£å­¦ä¹ è’¸é¦çš„æ–°æ ‡å‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ç­‰ä»»åŠ¡ï¼Œå°¤å…¶é€‚åˆåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œé«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ã€‚æœªæ¥ï¼ŒCoMADæ¡†æ¶æœ‰æœ›æ¨åŠ¨è‡ªç›‘ç£å­¦ä¹ åœ¨è¾¹ç¼˜è®¡ç®—å’Œç§»åŠ¨è®¾å¤‡ä¸­çš„åº”ç”¨ï¼Œæå‡æ™ºèƒ½è®¾å¤‡çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Numerous self-supervised learning paradigms, such as contrastive learning and masked image modeling, learn powerful representations from unlabeled data but are typically pretrained in isolation, overlooking complementary insights and yielding large models that are impractical for resource-constrained deployment. To overcome these challenges, we introduce Consensus-oriented Masked Distillation (CoMAD), a lightweight, parameter-free framework that unifies knowledge from multiple current state-of-the-art self-supervised Vision Transformers into a compact student network. CoMAD distills from three pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct semantic and contextual priors. Rather than naively averaging teacher outputs, we apply asymmetric masking: the student sees only 25 percent of patches while each teacher receives a progressively lighter, unique mask, forcing the student to interpolate missing features under richer contexts. Teacher embeddings are aligned to the student's space via a linear adapter and layer normalization, then fused through our joint consensus gating, which weights each token by combining cosine affinity with inter-teacher agreement. The student is trained with dual-level KL divergence on visible tokens and reconstructed feature maps, capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average precision on MS-COCO, establishing a new state-of-the-art in compact SSL distillation.

