---
layout: default
title: Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting
---

# Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04227" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04227v1</a>
  <a href="https://arxiv.org/pdf/2508.04227.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04227v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04227v1', 'Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuyang Liu, Qiuhe Hong, Linlan Huang, Alexandra Gomez-Villa, Dipam Goswami, Xialei Liu, Joost van de Weijer, Yonghong Tian

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ æ–¹æ³•ä»¥è§£å†³é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `å¤šæ¨¡æ€é‡æ”¾` `è·¨æ¨¡æ€æ­£åˆ™åŒ–` `å‚æ•°é«˜æ•ˆé€‚åº”` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ¨¡æ€ç‰¹å¾æ¼‚ç§»å’Œå‚æ•°å¹²æ‰°æ–¹é¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é‡æ”¾ç­–ç•¥ã€è·¨æ¨¡æ€æ­£åˆ™åŒ–å’Œå‚æ•°é«˜æ•ˆé€‚åº”ï¼Œä»¥åº”å¯¹ä¸åŒçš„æŒ‘æˆ˜ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå¼ºè°ƒäº†ç°æœ‰åŸºå‡†çš„ä¸è¶³ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶çš„æ–¹å‘å’Œå¼€æ”¾é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éé™æ€æ•°æ®ä¸ŠæŒç»­å­¦ä¹ é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç¾éš¾æ€§é—å¿˜ã€‚ä¸ä¼ ç»Ÿçš„å•æ¨¡æ€æŒç»­å­¦ä¹ ä¸åŒï¼ŒVLMsé¢ä¸´è·¨æ¨¡æ€ç‰¹å¾æ¼‚ç§»ã€å…±äº«æ¶æ„å¯¼è‡´çš„å‚æ•°å¹²æ‰°ä»¥åŠé›¶-shotèƒ½åŠ›ä¸‹é™ç­‰ç‹¬ç‰¹é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§å›é¡¾äº†VLMçš„æŒç»­å­¦ä¹ ï¼Œè¯†åˆ«äº†ä¸‰ç§æ ¸å¿ƒå¤±æ•ˆæ¨¡å¼ï¼Œå¹¶æå‡ºäº†åŸºäºæŒ‘æˆ˜çš„åˆ†ç±»æ³•ï¼Œæ˜ å°„è§£å†³æ–¹æ¡ˆä¸ç›®æ ‡é—®é¢˜ã€‚æœ€åï¼Œåˆ†æäº†å½“å‰è¯„ä¼°åè®®å’Œæ•°æ®é›†ï¼Œå¼ºè°ƒäº†æ›´å¥½åŸºå‡†çš„å¿…è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ è¿‡ç¨‹ä¸­é­é‡çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨æ¨¡æ€ç‰¹å¾æ¼‚ç§»å’Œå‚æ•°å¹²æ‰°æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æŒ‘æˆ˜é©±åŠ¨çš„åˆ†ç±»æ³•ï¼Œå°†è§£å†³æ–¹æ¡ˆä¸ç‰¹å®šé—®é¢˜ç›¸æ˜ å°„ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€é‡æ”¾ã€è·¨æ¨¡æ€æ­£åˆ™åŒ–å’Œå‚æ•°é«˜æ•ˆé€‚åº”æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šæ¨¡æ€é‡æ”¾ç­–ç•¥ç”¨äºå¤„ç†ç‰¹å¾æ¼‚ç§»ï¼Œè·¨æ¨¡æ€æ­£åˆ™åŒ–ç¡®ä¿æ¨¡æ€å¯¹é½ï¼Œå‚æ•°é«˜æ•ˆé€‚åº”é€šè¿‡æ¨¡å—åŒ–æˆ–ä½ç§©æ›´æ–°å‡å°‘å‚æ•°å¹²æ‰°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸“é—¨åˆ†ç±»æ³•ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•æ— æ³•æœ‰æ•ˆåº”å¯¹çš„è·¨æ¨¡æ€é—å¿˜é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ˜¾å¼æˆ–éšå¼çš„è®°å¿†æœºåˆ¶æ¥å®ç°å¤šæ¨¡æ€é‡æ”¾ï¼Œä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯ä¿æŒæ¨¡æ€é—´çš„å¯¹é½ï¼Œå¹¶é€šè¿‡æ¨¡å—åŒ–æ›´æ–°æ¥æé«˜å‚æ•°çš„é€‚åº”æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è·¨æ¨¡æ€é—å¿˜æ–¹é¢ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå’Œå¤šæ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿï¼Œé€‚åº”ä¸æ–­å˜åŒ–çš„ç”¨æˆ·éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) have achieved impressive performance across diverse multimodal tasks by leveraging large-scale pre-training. However, enabling them to learn continually from non-stationary data remains a major challenge, as their cross-modal alignment and generalization capabilities are particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal continual learning (CL), VLMs face unique challenges such as cross-modal feature drift, parameter interference due to shared architectures, and zero-shot capability erosion. This survey offers the first focused and systematic review of continual learning for VLMs (VLM-CL). We begin by identifying the three core failure modes that degrade performance in VLM-CL. Based on these, we propose a challenge-driven taxonomy that maps solutions to their target problems: (1) \textit{Multi-Modal Replay Strategies} address cross-modal drift through explicit or implicit memory mechanisms; (2) \textit{Cross-Modal Regularization} preserves modality alignment during updates; and (3) \textit{Parameter-Efficient Adaptation} mitigates parameter interference with modular or low-rank updates. We further analyze current evaluation protocols, datasets, and metrics, highlighting the need for better benchmarks that capture VLM-specific forgetting and compositional generalization. Finally, we outline open problems and future directions, including continual pre-training and compositional zero-shot learning. This survey aims to serve as a comprehensive and diagnostic reference for researchers developing lifelong vision-language systems. All resources are available at: https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.

