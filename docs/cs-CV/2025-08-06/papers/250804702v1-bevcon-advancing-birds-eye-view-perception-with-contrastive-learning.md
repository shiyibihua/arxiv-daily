---
layout: default
title: BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning
---

# BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04702" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04702v1</a>
  <a href="https://arxiv.org/pdf/2508.04702.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04702v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04702v1', 'BEVCon: Advancing Bird\'s Eye View Perception with Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyang Leng, Jiawei Yang, Zhicheng Ren, Bolei Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**æœŸåˆŠ**: IEEE Robotics and Automation Letters (Volume: 10, Issue: 4, April 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBEVConä»¥æå‡è‡ªåŠ¨é©¾é©¶ä¸­çš„é¸Ÿç°è§†å›¾æ„ŸçŸ¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é¸Ÿç°è§†å›¾` `å¯¹æ¯”å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `ç‰¹å¾ä¼˜åŒ–` `3Dç‰©ä½“æ£€æµ‹` `è¡¨ç¤ºå­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨BEVç¼–ç å™¨å’Œä»»åŠ¡ç‰¹å®šå¤´éƒ¨çš„å¢å¼ºï¼Œæœªå……åˆ†æŒ–æ˜è¡¨ç¤ºå­¦ä¹ çš„æ½œåŠ›ã€‚
2. BEVConé€šè¿‡å¼•å…¥å®ä¾‹ç‰¹å¾å¯¹æ¯”æ¨¡å—å’Œè§†è§’å¯¹æ¯”æ¨¡å—ï¼Œä¼˜åŒ–BEVç‰¹å¾å’Œå›¾åƒä¸»å¹²ç½‘ç»œã€‚
3. åœ¨nuScenesæ•°æ®é›†ä¸Šï¼ŒBEVConå®ç°äº†æœ€é«˜2.4%çš„mAPæå‡ï¼Œå±•ç¤ºäº†è¡¨ç¤ºå­¦ä¹ çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†BEVConï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„è‡ªåŠ¨é©¾é©¶ä¸­çš„é¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰æ„ŸçŸ¥ã€‚BEVæ„ŸçŸ¥æä¾›äº†å‘¨å›´ç¯å¢ƒçš„ä¿¯è§†å›¾è¡¨ç¤ºï¼Œå¯¹äº3Dç‰©ä½“æ£€æµ‹ã€åˆ†å‰²å’Œè½¨è¿¹é¢„æµ‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚å°½ç®¡ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¢å¼ºBEVç¼–ç å™¨å’Œä»»åŠ¡ç‰¹å®šå¤´éƒ¨ä¸Šï¼Œæˆ‘ä»¬åˆ™å…³æ³¨äºBEVæ¨¡å‹ä¸­è¡¨ç¤ºå­¦ä¹ çš„æ½œåŠ›ã€‚BEVConå¼•å…¥äº†ä¸¤ä¸ªå¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šå®ä¾‹ç‰¹å¾å¯¹æ¯”æ¨¡å—ç”¨äºä¼˜åŒ–BEVç‰¹å¾ï¼Œè§†è§’å¯¹æ¯”æ¨¡å—å¢å¼ºäº†å›¾åƒä¸»å¹²ç½‘ç»œã€‚åŸºäºæ£€æµ‹æŸå¤±çš„å¯†é›†å¯¹æ¯”å­¦ä¹ è®¾è®¡æå‡äº†BEVç¼–ç å™¨å’Œä¸»å¹²ç½‘ç»œçš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒBEVConåœ¨æ€§èƒ½ä¸Šå–å¾—äº†ä¸€è‡´çš„æå‡ï¼Œè¾ƒæœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†2.4%çš„mAPã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†è¡¨ç¤ºå­¦ä¹ åœ¨BEVæ„ŸçŸ¥ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸ºä¼ ç»Ÿçš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–æä¾›äº†è¡¥å……é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰BEVæ„ŸçŸ¥æ–¹æ³•åœ¨è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹å¯¹æ¯”å­¦ä¹ çš„åº”ç”¨ï¼Œå¯¼è‡´ç‰¹å¾è¡¨ç¤ºä¸å¤Ÿä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBEVConçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¨¡å—æ¥æå‡BEVç‰¹å¾çš„è´¨é‡ï¼Œåˆ©ç”¨å®ä¾‹ç‰¹å¾å’Œè§†è§’å¯¹æ¯”æ¥å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä»è€Œæé«˜æ„ŸçŸ¥æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBEVConçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå®ä¾‹ç‰¹å¾å¯¹æ¯”æ¨¡å—å’Œè§†è§’å¯¹æ¯”æ¨¡å—ã€‚å®ä¾‹ç‰¹å¾å¯¹æ¯”æ¨¡å—ä¸“æ³¨äºä¼˜åŒ–BEVç‰¹å¾ï¼Œè€Œè§†è§’å¯¹æ¯”æ¨¡å—åˆ™å¢å¼ºå›¾åƒä¸»å¹²ç½‘ç»œçš„ç‰¹å¾æå–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šBEVConçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå¼•å…¥äº†å¯†é›†å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œç»“åˆæ£€æµ‹æŸå¤±æ¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œè¿™åœ¨ä»¥å¾€çš„BEVæ¨¡å‹ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å¯¹æ¯”å­¦ä¹ å’Œä»»åŠ¡ç‰¹å®šæŸå¤±çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç‰¹å¾å­¦ä¹ å’Œä»»åŠ¡æ‰§è¡Œä¹‹é—´çš„æœ‰æ•ˆååŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBEVConåœ¨mAPæŒ‡æ ‡ä¸Šè¾ƒæœ€å…ˆè¿›çš„åŸºçº¿æå‡äº†2.4%ï¼Œè¯æ˜äº†å…¶åœ¨BEVæ„ŸçŸ¥ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æå‡ä¸ä»…å±•ç¤ºäº†å¯¹æ¯”å­¦ä¹ çš„æ½œåŠ›ï¼Œä¹Ÿå¼ºè°ƒäº†è¡¨ç¤ºå­¦ä¹ åœ¨å¤æ‚åœºæ™¯ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡BEVæ„ŸçŸ¥çš„å‡†ç¡®æ€§ï¼ŒBEVConèƒ½å¤Ÿä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†æä¾›æ›´å¯é çš„ç¯å¢ƒç†è§£ï¼Œä»è€Œæé«˜å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–éœ€è¦ç©ºé—´æ„ŸçŸ¥çš„é¢†åŸŸï¼Œå¦‚æ— äººæœºç›‘æ§å’ŒåŸå¸‚è§„åˆ’ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present BEVCon, a simple yet effective contrastive learning framework designed to improve Bird's Eye View (BEV) perception in autonomous driving. BEV perception offers a top-down-view representation of the surrounding environment, making it crucial for 3D object detection, segmentation, and trajectory prediction tasks. While prior work has primarily focused on enhancing BEV encoders and task-specific heads, we address the underexplored potential of representation learning in BEV models. BEVCon introduces two contrastive learning modules: an instance feature contrast module for refining BEV features and a perspective view contrast module that enhances the image backbone. The dense contrastive learning designed on top of detection losses leads to improved feature representations across both the BEV encoder and the backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon achieves consistent performance gains, achieving up to +2.4% mAP improvement over state-of-the-art baselines. Our results highlight the critical role of representation learning in BEV perception and offer a complementary avenue to conventional task-specific optimizations.

