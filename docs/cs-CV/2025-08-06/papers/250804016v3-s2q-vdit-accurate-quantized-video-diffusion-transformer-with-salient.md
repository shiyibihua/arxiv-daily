---
layout: default
title: S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation
---

# S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04016" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04016v3</a>
  <a href="https://arxiv.org/pdf/2508.04016.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04016v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04016v3', 'S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Weilun Feng, Haotong Qin, Chuanguang Yang, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An, Libo Huang, Michele Magno, Yongjun Xu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06 (Êõ¥Êñ∞: 2025-10-27)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/wlfeng0509/s2q-vdit)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫S$^2$Q-VDiT‰ª•Ëß£ÂÜ≥ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÈáèÂåñ‰∏éÂ≠¶‰π†ÊåëÊàò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁîüÊàê` `Êâ©Êï£ÂèòÊç¢Âô®` `ÈáèÂåñÊäÄÊúØ` `Á®ÄÁñèÊ≥®ÊÑèÂäõ` `Ê®°ÂûãÂéãÁº©` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®ÂèÇÊï∞Èáè‰∏äËææÂà∞Êï∞ÂçÅ‰∫øÔºåÂØºËá¥ËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÊé®ÁêÜÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. ÊèêÂá∫S$^2$Q-VDiTÔºåÈÄöËøáHessian-awareÊòæËëóÊï∞ÊçÆÈÄâÊã©ÂíåÊ≥®ÊÑèÂäõÂºïÂØºÁöÑÁ®ÄÁñètokenËí∏È¶èÔºå‰ºòÂåñÈáèÂåñËøáÁ®ãÂíåÂ≠¶‰π†ÊïàÁéá„ÄÇ
3. Âú®W4A6ÈáèÂåñ‰∏ãÔºåS$^2$Q-VDiTÂÆûÁé∞‰∫Ü3.9ÂÄçÁöÑÊ®°ÂûãÂéãÁº©Âíå1.3ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄüÔºåË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Êï£ÂèòÊç¢Âô®Â∑≤Êàê‰∏∫ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑ‰∏ªÊµÅËåÉÂºèÔºå‰ΩÜÂÖ∂Êï∞ÂçÅ‰∫øÂèÇÊï∞ÂØºËá¥ÊòæËëóÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇÈáèÂåñÊäÄÊúØÈÄöËøáÂáèÂ∞ëÂÜÖÂ≠ò‰ΩøÁî®ÂíåÂä†ÈÄüÊé®ÁêÜÊèê‰æõ‰∫ÜÊúâÊïàËß£ÂÜ≥ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏≠Á©∫Èó¥‰∏éÊó∂Èó¥‰ø°ÊÅØÁöÑËÅîÂêàÂª∫Ê®°ÂØºËá¥ÊûÅÈïøÁöÑtokenÂ∫èÂàóÔºåÂ¢ûÂä†‰∫ÜÊ†°ÂáÜÊñπÂ∑ÆÂíåÂ≠¶‰π†ÈöæÂ∫¶„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜS$^2$Q-VDiTÔºå‰∏Ä‰∏™ÂêéËÆ≠ÁªÉÈáèÂåñÊ°ÜÊû∂ÔºåÂà©Áî®ÊòæËëóÊï∞ÊçÆÂíåÁ®ÄÁñètokenËí∏È¶è„ÄÇÂú®Ê†°ÂáÜÈò∂ÊÆµÔºåÈáèÂåñÊÄßËÉΩÂØπÊ†°ÂáÜÊï∞ÊçÆÁöÑÈÄâÊã©È´òÂ∫¶ÊïèÊÑü„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜHessian-awareÊòæËëóÊï∞ÊçÆÈÄâÊã©ÔºåÊûÑÂª∫È´òË¥®ÈáèÁöÑÊ†°ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÂêåÊó∂ÔºåÈíàÂØπÂ≠¶‰π†ÊåëÊàòÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜV-DMs‰∏≠Âõ∫ÊúâÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÊèêÂá∫‰∫ÜÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÁ®ÄÁñètokenËí∏È¶è„ÄÇS$^2$Q-VDiTÂú®W4A6ÈáèÂåñ‰∏ãÂÆûÁé∞‰∫ÜÊó†ÊçüÊÄßËÉΩÔºåÂêåÊó∂Êèê‰æõ‰∫Ü3.9ÂÄçÁöÑÊ®°ÂûãÂéãÁº©Âíå1.3ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®ÈáèÂåñËøáÁ®ã‰∏≠Èù¢‰∏¥ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨ÂíåÂ≠¶‰π†ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøtokenÂ∫èÂàóÊó∂ÔºåÂÆπÊòìÂºïÂÖ•È´òÊ†°ÂáÜÊñπÂ∑ÆÔºåÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöS$^2$Q-VDiTÈÄöËøáÂºïÂÖ•Hessian-awareÊòæËëóÊï∞ÊçÆÈÄâÊã©ÂíåÊ≥®ÊÑèÂäõÂºïÂØºÁöÑÁ®ÄÁñètokenËí∏È¶èÔºå‰ºòÂåñ‰∫ÜÊ†°ÂáÜÊï∞ÊçÆÁöÑË¥®ÈáèÂíåÂ≠¶‰π†ËøáÁ®ãÔºåÊó®Âú®ÊèêÈ´òÈáèÂåñÊÄßËÉΩÂíåÊ®°ÂûãÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöHessian-awareÊòæËëóÊï∞ÊçÆÈÄâÊã©Áî®‰∫éÊûÑÂª∫È´òË¥®ÈáèÁöÑÊ†°ÂáÜÊï∞ÊçÆÈõÜÔºåÊ≥®ÊÑèÂäõÂºïÂØºÁöÑÁ®ÄÁñètokenËí∏È¶èÂàôÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÊòæËëóÊï∞ÊçÆÈÄâÊã©ÂíåÁ®ÄÁñètokenËí∏È¶èÔºåÈíàÂØπËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÁâπÊÄßËøõË°å‰ºòÂåñÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈáèÂåñÊÄßËÉΩ‰∏éÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®W4A6ÈáèÂåñÁ≠ñÁï•ÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•ÈÄÇÂ∫îÁ®ÄÁñèÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÂπ∂ÈÄöËøátoken-wiseÊ≥®ÊÑèÂäõÂàÜÂ∏ÉÊù•Âº∫Ë∞ÉÂØπÊ®°ÂûãËæìÂá∫ÂΩ±ÂìçËæÉÂ§ßÁöÑtoken„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

S$^2$Q-VDiTÂú®W4A6ÈáèÂåñ‰∏ãÂÆûÁé∞‰∫ÜÊó†ÊçüÊÄßËÉΩÔºåÊ®°ÂûãÂéãÁº©ÁéáËææÂà∞3.9ÂÄçÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá1.3ÂÄçÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÂü∫Á∫øË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÁîüÊàê„ÄÅÂÆûÊó∂ËßÜÈ¢ëÂ§ÑÁêÜÂíåÊô∫ËÉΩÁõëÊéßÁ≠â„ÄÇÈÄöËøáÊèêÈ´òËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊïàÁéáÔºåS$^2$Q-VDiTËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÂÆûÁé∞È´òË¥®ÈáèÁöÑËßÜÈ¢ëÁîüÊàêÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion transformers have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that leverages Salient data and Sparse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the sparse attention patterns inherent in V-DMs. Based on this observation, we propose \textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT achieves lossless performance while delivering $3.9\times$ model compression and $1.3\times$ inference acceleration. Code will be available at https://github.com/wlfeng0509/s2q-vdit.

