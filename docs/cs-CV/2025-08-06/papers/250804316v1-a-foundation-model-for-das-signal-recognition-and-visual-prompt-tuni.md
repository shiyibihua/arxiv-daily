---
layout: default
title: A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks
---

# A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04316" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04316v1</a>
  <a href="https://arxiv.org/pdf/2508.04316.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04316v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04316v1', 'A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Kun Gui, Hongliang Ren, Shang Shi, Jin Lu, Changqiu Yu, Quanjun Cao, Guomin Gu, Qi Xuan

**ÂàÜÁ±ª**: cs.CV, eess.SP

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MAEPDÊ®°Âûã‰ª•Ëß£ÂÜ≥DAS‰ø°Âè∑ËØÜÂà´‰∏≠ÁöÑÊï∞ÊçÆÂàÜÂ∏É‰∏çÂùáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂàÜÂ∏ÉÂºèÂ£∞Â≠¶‰º†ÊÑü` `‰ø°Âè∑ËØÜÂà´` `Ëá™ÁõëÁù£Â≠¶‰π†` `ËßÜËßâÊèêÁ§∫Ë∞É‰ºò` `Ê∑±Â∫¶Â≠¶‰π†` `Ê®°ÂûãÂæÆË∞É` `Ë∑®ÂüüÊ≥õÂåñ` `ÁÆ°ÈÅìÊ≥ÑÊºèÊ£ÄÊµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâDAS‰ø°Âè∑ËØÜÂà´Ê®°ÂûãÈù¢‰∏¥Êï∞ÊçÆÂàÜÂ∏É‰∏çÂùáÂíåÊ†áÊ≥®Êï∞ÊçÆ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂÖ∂Ë∑®ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫MAEPDÊ®°ÂûãÔºåÈÄöËøáMasked AutoencoderËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂ÁªìÂêàËßÜËßâÊèêÁ§∫Ë∞É‰ºòÊñπÊ≥ïÔºåÊèêÂçáÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMAEPDÂú®ÂÆ§ÂÜÖÊ≠•ÊÄÅËØÜÂà´‰∏≠ÂèñÂæó96.94%ÁöÑÂáÜÁ°ÆÁéáÔºå‰∏îËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ë45%ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂È´òÊïàÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂàÜÂ∏ÉÂºèÂ£∞Â≠¶‰º†ÊÑüÔºàDASÔºâÊäÄÊúØÂú®Â§ö‰∏™È¢ÜÂüüÁöÑÂ∫îÁî®Êó•ÁõäÂ¢ûÈïø„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂºÇÊûÑ‰º†ÊÑüÁéØÂ¢ÉÂØºËá¥ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÊï∞ÊçÆÈ©±Âä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÈù¢‰∏¥Ë∑®ÂüüÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÂíåÊ†áÊ≥®ËÆ≠ÁªÉÊï∞ÊçÆÁü≠Áº∫ÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éMasked AutoencoderÁöÑDAS‰ø°Âè∑ËØÜÂà´Âü∫Á°ÄÊ®°ÂûãMAEPD„ÄÇËØ•Ê®°ÂûãÂú®635,860‰∏™Ê†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊ∂µÁõñ‰∫ÜDASÊ≠•ÊÄÅÊó∂Á©∫‰ø°Âè∑„ÄÅÁî®‰∫éÂë®ÁïåÂÆâÂÖ®ÁöÑ2D GASFÂõæÂÉè„ÄÅÁÆ°ÈÅìÊ≥ÑÊºèÁöÑ2DÊó∂È¢ëÂõæÂÉèÔºå‰ª•ÂèäÂåÖÊã¨È≤∏È±ºÂè´Â£∞ÂíåÂú∞ÈúáÊ¥ªÂä®ÁöÑÂºÄÊîæÊï∞ÊçÆÈõÜ‰ø°Âè∑ÔºåÂà©Áî®Ëá™ÁõëÁù£Êé©Á†ÅÈáçÂª∫‰ªªÂä°ÊçïÊçâDAS‰ø°Âè∑ÁöÑÊ∑±Â±ÇËØ≠‰πâÁâπÂæÅ„ÄÇÈÄöËøáËßÜËßâÊèêÁ§∫Ë∞É‰ºòÔºàVPTÔºâÊñπÊ≥ïÁî®‰∫é‰∏ãÊ∏∏ËØÜÂà´‰ªªÂä°ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéËØ•Ê®°ÂûãÂú®ÂÆ§ÂÜÖÊ≠•ÊÄÅËØÜÂà´‰ªªÂä°‰∏≠ËææÂà∞‰∫Ü96.94%ÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéáÔºå‰∏î‰ªÖÂæÆË∞É‰∫Ü0.322%ÁöÑÂèÇÊï∞ÔºåËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ë‰∫Ü45%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥ÂàÜÂ∏ÉÂºèÂ£∞Â≠¶‰º†ÊÑüÔºàDASÔºâ‰ø°Âè∑ËØÜÂà´‰∏≠ÁöÑÊï∞ÊçÆÂàÜÂ∏É‰∏çÂùáÂíåÊ†áÊ≥®Êï∞ÊçÆÁü≠Áº∫ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÂºÇÊûÑÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑMAEPDÊ®°ÂûãÈÄöËøáMasked AutoencoderËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂà©Áî®Ëá™ÁõëÁù£Â≠¶‰π†ÊçïÊçâÊ∑±Â±ÇËØ≠‰πâÁâπÂæÅÔºåÈöèÂêéÈÄöËøáËßÜËßâÊèêÁ§∫Ë∞É‰ºòÔºàVPTÔºâÊñπÊ≥ïËøõË°å‰∏ãÊ∏∏‰ªªÂä°ÁöÑÂæÆË∞ÉÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄßÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMAEPDÊ®°ÂûãÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÂíåÂæÆË∞ÉÈò∂ÊÆµ„ÄÇÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåÊ®°ÂûãÂú®Â§öÁßçDAS‰ø°Âè∑Êï∞ÊçÆÈõÜ‰∏äËøõË°åËá™ÁõëÁù£Â≠¶‰π†ÔºõÂú®ÂæÆË∞ÉÈò∂ÊÆµÔºåÂÜªÁªì‰∏ªÂπ≤ÁΩëÁªúÂèÇÊï∞Ôºå‰ªÖË∞ÉÊï¥ÊèíÂÖ•TransformerÁºñÁ†ÅÂô®Â±ÇÁöÑÂ∞ëÈáèÂèØÂ≠¶‰π†ËßÜËßâÊèêÁ§∫ÂêëÈáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMAEPDÊ®°ÂûãÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜMasked AutoencoderÂíåËßÜËßâÊèêÁ§∫Ë∞É‰ºòÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÈúÄË¶ÅÂæÆË∞ÉÁöÑÂèÇÊï∞Êï∞ÈáèÔºå‰∏é‰º†ÁªüÁöÑÂÖ®ÂæÆË∞ÉÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜËá™ÁõëÁù£Êé©Á†ÅÈáçÂª∫‰ªªÂä°‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁõÆÊ†áÔºåÁ°Æ‰øùÊ®°ÂûãËÉΩÂ§üÊúâÊïàÊçïÊçâDAS‰ø°Âè∑ÁöÑÊ∑±Â±ÇÁâπÂæÅ„ÄÇÂêåÊó∂ÔºåVPTÊñπÊ≥ïÁöÑÂºïÂÖ•‰ΩøÂæóÊ®°ÂûãÂú®ÂæÆË∞ÉÊó∂Âè™ÈúÄË∞ÉÊï¥Â∞ëÈáèÂèÇÊï∞ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊó∂Èó¥ÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑÊ∂àËÄó„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåMAEPDÊ®°ÂûãÂú®ÂÆ§ÂÜÖÊ≠•ÊÄÅËØÜÂà´‰ªªÂä°‰∏≠ËææÂà∞‰∫Ü96.94%ÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéáÔºå‰ªÖÂæÆË∞É‰∫Ü0.322%ÁöÑÂèÇÊï∞ÔºåÁõ∏ÊØî‰º†ÁªüÁöÑÂÖ®ÂæÆË∞ÉÊñπÊ≥ïÊèêÈ´ò‰∫Ü0.61%ÁöÑÂáÜÁ°ÆÁéáÔºåÂπ∂‰∏îËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ë‰∫Ü45%ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰ºòË∂äÁöÑÊÄßËÉΩÂíåÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑMAEPDÊ®°ÂûãÂú®ÂàÜÂ∏ÉÂºèÂ£∞Â≠¶‰º†ÊÑüÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåËÉΩÂ§üÊúâÊïàÊèêÂçá‰ø°Âè∑ËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÂÖ∂ÊñπÊ≥ï‰∏ç‰ªÖÈÄÇÁî®‰∫éÊ≠•ÊÄÅËØÜÂà´ÔºåËøòÂèØÊâ©Â±ïÂà∞ÁÆ°ÈÅìÊ≥ÑÊºèÊ£ÄÊµã„ÄÅÁéØÂ¢ÉÁõëÊµãÁ≠âÂ§ö‰∏™È¢ÜÂüüÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Distributed Acoustic Sensing (DAS) technology finds growing applications across various domains. However, data distribution disparities due to heterogeneous sensing environments pose challenges for data-driven artificial intelligence (AI) models, limiting cross-domain generalization and facing a shortage of labeled training data. To address these issues, this study proposes a foundational model for DAS signal recognition based on a Masked Autoencoder, named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples, encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter security, 2D time-frequency images for pipeline leakage, and open-dataset signals including whale vocalizations and seismic activities, using a self-supervised mask reconstruction task to capture deep semantic features of DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition tasks. This method freezes the pretrained backbone parameters and fine-tunes only a small set of learnable visual prompt vectors inserted into the Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super platform validate MAEPD using indoor gait recognition as a downstream task. The VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322% of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT) method by 0.61% and reducing training time by 45%. The model also exhibits robust performance in pipeline leakage detection, confirming the generality, efficiency, and scalability of MAEPD as a foundational model. This approach offers a novel paradigm for addressing the limited generalization of signal recognition models in the DAS domain.

