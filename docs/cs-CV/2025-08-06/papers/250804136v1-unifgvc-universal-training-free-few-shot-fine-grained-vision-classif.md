---
layout: default
title: UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval
---

# UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04136" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04136v1</a>
  <a href="https://arxiv.org/pdf/2508.04136.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04136v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04136v1', 'UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyu Guo, Kuan Zhu, Xiangzhao Hao, Haiyun Guo, Ming Tang, Jinqiao Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniFGVCä»¥è§£å†³å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°‘æ ·æœ¬å­¦ä¹ ` `ç»†ç²’åº¦åˆ†ç±»` `å¤šæ¨¡æ€æ£€ç´¢` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å°‘æ ·æœ¬FGVCæ–¹æ³•ä¸»è¦ä¾èµ–äºå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆå’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„UniFGVCæ¡†æ¶é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢çš„æ–¹å¼ï¼Œåˆ©ç”¨CDV-Captionerç”Ÿæˆç»†ç²’åº¦å±æ€§æè¿°ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚
3. åœ¨12ä¸ªFGVCåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniFGVCçš„è¡¨ç°æŒç»­ä¼˜äºä¼ ç»Ÿçš„CLIPæ–¹æ³•å’Œä¸€äº›å®Œå…¨ç›‘ç£çš„MLLMæ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„æ•°æ®ä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†å¾®å¦™çš„ç±»åˆ«å·®å¼‚ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ¥æé«˜æ€§èƒ½ï¼Œä½†é¢ä¸´è¿‡æ‹Ÿåˆå’Œå¼±æ³›åŒ–çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UniFGVCï¼Œä¸€ä¸ªé€šç”¨çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œå°†å°‘æ ·æœ¬FGVCé‡æ–°æ„å»ºä¸ºå¤šæ¨¡æ€æ£€ç´¢ã€‚é¦–å…ˆï¼Œæå‡ºäº†ç±»åˆ«åŒºåˆ†è§†è§‰æè¿°ç”Ÿæˆå™¨ï¼ˆCDV-Captionerï¼‰ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬æè¿°ï¼Œæ•æ‰ç»†ç²’åº¦å±æ€§ç‰¹å¾ã€‚é€šè¿‡é“¾å¼æ€ç»´æç¤ºå’Œè§†è§‰ç›¸ä¼¼å‚è€ƒå›¾åƒï¼ŒCDV-Captionerå‡å°‘äº†å¹»è§‰ç°è±¡å¹¶å¢å¼ºäº†ç”Ÿæˆæè¿°çš„åŒºåˆ†æ€§ã€‚ä½¿ç”¨è¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå›¾åƒè½¬æ¢ä¸ºå›¾åƒ-æè¿°å¯¹ï¼Œæ„å»ºå¤šæ¨¡æ€ç±»åˆ«æ¨¡æ¿ä»¥ä¾›åç»­æ£€ç´¢ã€‚æœ€åï¼Œåˆ©ç”¨ç°æˆçš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨åµŒå…¥æŸ¥è¯¢å’Œæ¨¡æ¿å¯¹ï¼Œé€šè¿‡åœ¨è”åˆç©ºé—´ä¸­æ£€ç´¢æœ€è¿‘çš„æ¨¡æ¿æ¥å®ŒæˆFGVCã€‚UniFGVCç¡®ä¿ä¸å¤šç§MLLMå’Œç¼–ç å™¨çš„å¹¿æ³›å…¼å®¹æ€§ï¼Œåœ¨å°‘æ ·æœ¬FGVCåœºæ™¯ä¸­æä¾›å¯é çš„æ³›åŒ–å’Œé€‚åº”æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨12ä¸ªFGVCåŸºå‡†ä¸Šä¼˜äºä»¥å¾€çš„å°‘æ ·æœ¬CLIPæ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº†è‹¥å¹²å®Œå…¨ç›‘ç£çš„MLLMæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä¸­çš„è¿‡æ‹Ÿåˆå’Œå¼±æ³›åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´åœ¨æœ‰é™æ ·æœ¬ä¸‹æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniFGVCæ¡†æ¶å°†FGVCé—®é¢˜é‡æ–°æ„å»ºä¸ºå¤šæ¨¡æ€æ£€ç´¢ï¼Œé€šè¿‡ç”Ÿæˆç»†ç²’åº¦å±æ€§æè¿°æ¥å¢å¼ºæ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šç±»åˆ«åŒºåˆ†è§†è§‰æè¿°ç”Ÿæˆå™¨ï¼ˆCDV-Captionerï¼‰å’Œæ£€ç´¢æ¨¡å—ã€‚CDV-Captionerç”Ÿæˆå›¾åƒçš„ç»“æ„åŒ–æ–‡æœ¬æè¿°ï¼Œè€Œæ£€ç´¢æ¨¡å—åˆ™é€šè¿‡åµŒå…¥æŸ¥è¯¢å’Œæ¨¡æ¿å¯¹æ¥å®ç°FGVCã€‚

**å…³é”®åˆ›æ–°**ï¼šCDV-Captionerçš„è®¾è®¡æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œé€šè¿‡é“¾å¼æ€ç»´æç¤ºå’Œè§†è§‰ç›¸ä¼¼å‚è€ƒå›¾åƒï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆæè¿°çš„å¹»è§‰ç°è±¡ï¼Œæå‡äº†æè¿°çš„å‡†ç¡®æ€§å’ŒåŒºåˆ†æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CDV-Captionerä¸­ï¼Œä½¿ç”¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†ï¼Œç»“åˆè§†è§‰ç›¸ä¼¼å›¾åƒè¿›è¡Œæè¿°ç”Ÿæˆã€‚æ£€ç´¢æ¨¡å—åˆ™åˆ©ç”¨ç°æˆçš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œç¡®ä¿äº†æ¨¡å‹çš„å¹¿æ³›å…¼å®¹æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨12ä¸ªFGVCåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniFGVC consistently outperform previous few-shot CLIP-based methods, achieving significant improvements in performance. å…·ä½“æ¥è¯´ï¼ŒUniFGVCåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å±•ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿç‰©ç‰©ç§è¯†åˆ«ã€äº§å“åˆ†ç±»ä»¥åŠä»»ä½•éœ€è¦ç»†ç²’åº¦åˆ†ç±»çš„è§†è§‰ä»»åŠ¡ã€‚é€šè¿‡æä¾›ä¸€ç§æ— è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆï¼ŒUniFGVCèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches.

