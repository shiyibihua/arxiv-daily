---
layout: default
title: One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion
---

# One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04559" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04559v2</a>
  <a href="https://arxiv.org/pdf/2508.04559.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04559v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04559v2', 'One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinxi Liu, Zijian He, Guangrun Wang, Guanbin Li, Liang Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-11-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOMFAæ¡†æ¶ä»¥è§£å†³è™šæ‹Ÿè¯•è¡£ä¸è¯•è„±çš„çµæ´»æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è™šæ‹Ÿè¯•è¡£` `æ‰©æ•£æ¨¡å‹` `åŒå‘å»ºæ¨¡` `Tweedieå…¬å¼` `å§¿æ€ä¼°è®¡` `æœè£…åˆæˆ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è™šæ‹Ÿè¯•è¡£æ–¹æ³•ä¾èµ–å±•ç¤ºæœè£…å’Œåˆ†å‰²æ©ç ï¼Œä¸”å¯¹å§¿æ€å˜åŒ–çš„é€‚åº”æ€§å·®ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. OMFAæ¡†æ¶é€šè¿‡åŒå‘å»ºæ¨¡å’ŒTweedieå…¬å¼ï¼Œæ”¯æŒä»»æ„å§¿æ€çš„è™šæ‹Ÿè¯•è¡£ä¸è¯•è„±ï¼Œæ— éœ€å±•ç¤ºæœè£…ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOMFAåœ¨è¯•è¡£å’Œè¯•è„±ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†åˆæˆæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨å›¾åƒè™šæ‹Ÿè¯•è¡£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿå®ç°æ›´çœŸå®çš„æœè£…åˆæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—é™äºå±•ç¤ºæœè£…å’Œåˆ†å‰²æ©ç çš„ä¾èµ–ï¼Œä»¥åŠå¯¹çµæ´»å§¿æ€å˜åŒ–çš„å¤„ç†èƒ½åŠ›ä¸è¶³ï¼Œé™ä½äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºOMFAï¼ˆOne Model For Allï¼‰ï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£æ¡†æ¶ï¼Œæ”¯æŒè™šæ‹Ÿè¯•è¡£å’Œè¯•è„±ï¼Œæ— éœ€å±•ç¤ºæœè£…ï¼Œå¹¶æ”¯æŒä»»æ„å§¿æ€ã€‚OMFAå€Ÿé‰´äº†è¯­è¨€å»ºæ¨¡çš„æ€æƒ³ï¼Œé€šè¿‡åŒå‘å»ºæ¨¡å®ç°ä»æœè£…åˆ°è¯•è¡£ç»“æœçš„ç”Ÿæˆæˆ–ä»ç©¿ç€è€…æ¢å¤è¯•è„±æœè£…ã€‚é€šè¿‡SMPL-Xå§¿æ€æ¡ä»¶ï¼ŒOMFAèƒ½å¤Ÿä»å•å¼ å›¾åƒæ”¯æŒå¤šè§†è§’å’Œä»»æ„å§¿æ€çš„è¯•è¡£ã€‚å®éªŒè¡¨æ˜ï¼ŒOMFAåœ¨è¯•è¡£å’Œè¯•è„±ä»»åŠ¡ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæä¾›äº†è™šæ‹Ÿæœè£…åˆæˆçš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è™šæ‹Ÿè¯•è¡£æ–¹æ³•åœ¨å§¿æ€å˜åŒ–å’Œå±•ç¤ºæœè£…ä¾èµ–ä¸Šçš„ä¸è¶³ï¼Œå¯¼è‡´ç”¨æˆ·æ— æ³•çµæ´»åœ°å°†æœè£…è½¬ç§»åˆ°ä¸åŒçš„äººèº«ä¸Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOMFAæ¡†æ¶é€šè¿‡åŒå‘å»ºæ¨¡ï¼Œå…è®¸ä»æœè£…ç”Ÿæˆè¯•è¡£ç»“æœæˆ–ä»ç©¿ç€è€…æ¢å¤è¯•è„±æœè£…ï¼Œé¿å…äº†å¯¹å±•ç¤ºæœè£…çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOMFAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å•å¼ äººåƒå’Œç›®æ ‡æœè£…ï¼Œé€šè¿‡SMPL-Xå§¿æ€æ¡ä»¶å®ç°å¤šè§†è§’å’Œä»»æ„å§¿æ€çš„è¯•è¡£ï¼Œæ•´ä¸ªè¿‡ç¨‹æ˜¯æ— æ©ç çš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šOMFAçš„ä¸»è¦åˆ›æ–°åœ¨äºé‡‡ç”¨åŒå‘å»ºæ¨¡å’Œä¸¥æ ¼éµå¾ªTweedieå…¬å¼ï¼Œä½¿å¾—åœ¨å»å™ªè¿‡ç¨‹ä¸­èƒ½å¤Ÿå‡†ç¡®ä¼°è®¡æ•°æ®åˆ†å¸ƒï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šOMFAè®¾è®¡ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¸”ç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†é€‚åº”æ€§å¼ºçš„æ¨¡å—ï¼Œç¡®ä¿äº†å¯¹ä¸åŒå§¿æ€çš„æ”¯æŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OMFAåœ¨è¯•è¡£å’Œè¯•è„±ä»»åŠ¡ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œè¯•è¡£æ•ˆæœæå‡äº†XX%ï¼Œè¯•è„±æ•ˆæœæå‡äº†YY%ï¼Œå±•ç¤ºäº†å…¶åœ¨è™šæ‹Ÿæœè£…åˆæˆä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OMFAæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿æœè£…é›¶å”®ã€è™šæ‹Ÿè¯•è¡£é—´å’Œç¤¾äº¤åª’ä½“å¹³å°ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´çµæ´»çš„æœè£…é€‰æ‹©å’Œä¸ªæ€§åŒ–ä½“éªŒã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒOMFAæœ‰æœ›åœ¨æ—¶å°šè¡Œä¸šå’Œç”µå­å•†åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæå‡ç”¨æˆ·çš„è´­ç‰©ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent diffusion-based approaches have made significant advances in image-based virtual try-on, enabling more realistic and end-to-end garment synthesis. However, most existing methods remain constrained by their reliance on exhibition garments and segmentation masks, as well as their limited ability to handle flexible pose variations. These limitations reduce their practicality in real-world scenarios - for instance, users cannot easily transfer garments worn by one person onto another, and the generated try-on results are typically restricted to the same pose as the reference image. In this paper, we introduce OMFA (One Model For All), a unified diffusion framework for both virtual try-on and try-off that operates without the need for exhibition garments and supports arbitrary poses. OMFA is inspired by language modeling, where generation is guided by conditioning prompts. However, our framework differs fundamentally from LLMs in two key aspects. First, it employs a bidirectional modeling paradigm that symmetrically allows prompting either from the garment to generate try-on results or from the dressed person to recover the try-off garment. Second, it strictly adheres to Tweedie's formula, enabling faithful estimation of the underlying data distribution during the denoising process. Instead of imposing lower body constraints, OMFA is an entirely mask-free framework that requires only a single portrait and a target garment as input, and is designed to support flexible outfit combinations and cross-person garment transfer, making it better aligned with practical usage scenarios. Additionally, by leveraging SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose try-on from just one image. Extensive experiments demonstrate that OMFA achieves state-of-the-art results on both try-on and try-off tasks, providing a practical solution for virtual garment synthesis.

