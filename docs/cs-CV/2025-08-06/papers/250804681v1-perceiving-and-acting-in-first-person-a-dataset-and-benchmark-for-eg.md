---
layout: default
title: Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions
---

# Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04681" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04681v1</a>
  <a href="https://arxiv.org/pdf/2508.04681.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04681v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04681v1', 'Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Accepted to ICCV 2025. Project Page: https://liangxuy.github.io/InterVLA/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInterVLAæ•°æ®é›†ä»¥è§£å†³äººæœºäº¤äº’ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `ç¬¬ä¸€äººç§°è§†è§’` `å¤šæ¨¡æ€æ•°æ®` `åŠ¨ä½œæ¨¡å‹` `æ™ºèƒ½åŠ©æ‰‹` `æ•°æ®é›†æ„å»º` `äº’åŠ¨é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºç‰¹å®šçš„äº’åŠ¨ç±»åˆ«ï¼Œç¼ºä¹å¯¹ç¬¬ä¸€äººç§°è§†è§’çš„å…³æ³¨ï¼Œé™åˆ¶äº†AIåŠ©æ‰‹çš„æ„ŸçŸ¥ä¸è¡ŒåŠ¨èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„æ¡†æ¶ï¼Œé€šè¿‡æ‰‹åŠ¨è¾…åŠ©ä»»åŠ¡æ¥å¢å¼ºAIåŠ©æ‰‹çš„äº’åŠ¨èƒ½åŠ›ã€‚
3. InterVLAæ•°æ®é›†çš„æ„å»ºå’Œæ–°åŸºå‡†çš„å»ºç«‹ï¼Œä¸ºç¬¬ä¸€äººç§°äººç±»åŠ¨ä½œä¼°è®¡å’Œäº’åŠ¨é¢„æµ‹æä¾›äº†é‡è¦çš„å®éªŒåŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ åŸºäºçœŸå®ä¸–ç•Œäººç±»äº’åŠ¨æ•°æ®é›†çš„åŠ¨ä½œæ¨¡å‹å¯¹äºæ„å»ºé«˜æ•ˆçš„é€šç”¨æ™ºèƒ½åŠ©æ‰‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†å¤šé›†ä¸­äºç‰¹å®šçš„äº’åŠ¨ç±»åˆ«ï¼Œå¿½è§†äº†AIåŠ©æ‰‹åŸºäºç¬¬ä¸€äººç§°è§†è§’è¿›è¡Œæ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¡†æ¶ï¼Œç»“åˆæ‰‹åŠ¨è¾…åŠ©ä»»åŠ¡ï¼Œåˆ©ç”¨æ··åˆRGB-MoCapç³»ç»Ÿç”Ÿæˆäº†InterVLAæ•°æ®é›†ï¼ŒåŒ…å«1140åˆ†é’Ÿå’Œ120ä¸‡å¸§çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–äº†ä¸°å¯Œçš„äººç‰©ä¸ç‰©ä½“äº’åŠ¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œå»ºç«‹äº†æ–°åŸºå‡†ä»¥è¯„ä¼°ç¬¬ä¸€äººç§°äººç±»åŠ¨ä½œä¼°è®¡ã€äº’åŠ¨åˆæˆå’Œé¢„æµ‹ï¼Œæ¨åŠ¨æœªæ¥AIä»£ç†åœ¨ç‰©ç†ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹ç¬¬ä¸€äººç§°è§†è§’çš„äººæœºäº’åŠ¨ç†è§£é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨ç‰¹å®šçš„äº’åŠ¨ç±»åˆ«ï¼Œæ— æ³•å…¨é¢æ”¯æŒæ™ºèƒ½åŠ©æ‰‹çš„åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ‰‹åŠ¨è¾…åŠ©ä»»åŠ¡åµŒå…¥è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¡†æ¶ï¼Œåˆ©ç”¨æ··åˆRGB-MoCapç³»ç»Ÿç”Ÿæˆå¤šæ¨¡æ€æ•°æ®ï¼Œå¢å¼ºAIåŠ©æ‰‹åœ¨ç¬¬ä¸€äººç§°è§†è§’ä¸‹çš„æ„ŸçŸ¥ä¸è¡ŒåŠ¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†æ¨¡å—ã€åŠ¨ä½œè¯†åˆ«æ¨¡å—å’Œäº’åŠ¨åˆæˆæ¨¡å—ã€‚æ•°æ®é‡‡é›†é€šè¿‡RGB-MoCapç³»ç»Ÿè·å–å¤šæ¨¡æ€æ•°æ®ï¼ŒåŠ¨ä½œè¯†åˆ«æ¨¡å—ç”¨äºè§£æäººç±»ä¸ç‰©ä½“çš„äº’åŠ¨ï¼Œäº’åŠ¨åˆæˆæ¨¡å—åˆ™ç”Ÿæˆç›¸åº”çš„åŠ¨ä½œå’Œè¯­è¨€æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šInterVLAæ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„äººç‰©-ç‰©ä½“-äººç‰©äº’åŠ¨æ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„ç¬¬ä¸€äººç§°å’Œå¤–éƒ¨è§†è§’è§†é¢‘ï¼Œæä¾›äº†å‡†ç¡®çš„äººç‰©å’Œç‰©ä½“è¿åŠ¨æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†äº’åŠ¨ç†è§£çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†é«˜ç²¾åº¦çš„RGB-MoCapç³»ç»Ÿï¼Œç¡®ä¿äº†æ•°æ®çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ï¼›åŒæ—¶ï¼Œè®¾è®¡äº†é€‚åº”æ€§å¼ºçš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒäº’åŠ¨åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒInterVLAæ•°æ®é›†åœ¨ç¬¬ä¸€äººç§°äººç±»åŠ¨ä½œä¼°è®¡å’Œäº’åŠ¨é¢„æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨æ¨åŠ¨AIä»£ç†ç†è§£å’Œæ‰§è¡Œäººç±»äº’åŠ¨æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…åŠ©æ‰‹ã€æœºå™¨äººäº¤äº’ç³»ç»Ÿä»¥åŠè™šæ‹Ÿç°å®ç¯å¢ƒä¸­çš„äººæœºåä½œã€‚é€šè¿‡å¢å¼ºAIåŠ©æ‰‹çš„äº’åŠ¨ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæ¨åŠ¨äººæœºäº¤äº’æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.

