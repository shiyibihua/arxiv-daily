---
layout: default
title: Benchmarking Foundation Models for Mitotic Figure Classification
---

# Benchmarking Foundation Models for Mitotic Figure Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04441" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04441v1</a>
  <a href="https://arxiv.org/pdf/2508.04441.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04441v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04441v1', 'Benchmarking Foundation Models for Mitotic Figure Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jonas Ammeling, Jonathan Ganz, Emely Rosbach, Ludwig Lausser, Christof A. Bertram, Katharina Breininger, Marc Aubreville

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä»¥æå‡æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `æœ‰ä¸åˆ†è£‚åˆ†ç±»` `åŸºç¡€æ¨¡å‹` `ä½ç§©é€‚åº”` `åŒ»å­¦å½±åƒ` `æ·±åº¦å­¦ä¹ ` `è‚¿ç˜¤è¯Šæ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç—…ç†å›¾åƒåˆ†ç±»ä¸­é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚
2. æœ¬ç ”ç©¶æå‡ºä½¿ç”¨åŸºç¡€æ¨¡å‹ç»“åˆè‡ªç›‘ç£å­¦ä¹ å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œä»¥æé«˜æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»çš„æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRAé€‚åº”çš„æ¨¡å‹åœ¨ä»…ä½¿ç”¨10%è®­ç»ƒæ•°æ®æ—¶ï¼Œæ€§èƒ½æ¥è¿‘å…¨æ•°æ®å¯ç”¨æ€§ï¼Œä¸”åœ¨æœªè§è‚¿ç˜¤é¢†åŸŸçš„è¡¨ç°æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½é€šå¸¸éšç€æ•°æ®é‡å’Œå¤šæ ·æ€§çš„å¢åŠ è€Œæå‡ã€‚ç„¶è€Œï¼Œåœ¨ç—…ç†å­¦ç­‰åŒ»å­¦å½±åƒé¢†åŸŸï¼Œç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨å›¾åƒå¾€å¾€æœ‰é™ã€‚è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ä½¿å¾—åˆ©ç”¨å¤§é‡æœªæ ‡æ³¨æ•°æ®è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œä»è€Œè§£å†³äº†æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åŸºç¡€æ¨¡å‹åœ¨æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œè¯„ä¼°å…¶åœ¨ä¸åŒè‚¿ç˜¤é¢†åŸŸçš„é²æ£’æ€§ã€‚é€šè¿‡å¯¹æ¯”çº¿æ€§æ¢æµ‹å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œç»“æœè¡¨æ˜LoRAé€‚åº”çš„åŸºç¡€æ¨¡å‹åœ¨ä»…ä½¿ç”¨10%è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ¥è¿‘100%æ•°æ®å¯ç”¨æ€§ï¼Œä¸”åœ¨æœªè§è‚¿ç˜¤é¢†åŸŸçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªæ ‡æ³¨æ•°æ®æ—¶æ•ˆæœä¸ä½³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡æ³¨æ•°æ®è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¹¶ç»“åˆä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€åŸºç¡€æ¨¡å‹è®­ç»ƒã€LoRAé€‚åº”å’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä½¿ç”¨æœªæ ‡æ³¨æ•°æ®è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œç„¶åå¯¹æ¨¡å‹è¿›è¡ŒLoRAé€‚åº”ï¼Œæœ€ååœ¨ä¸åŒè‚¿ç˜¤é¢†åŸŸè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†LoRAé€‚åº”ä¸åŸºç¡€æ¨¡å‹ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„é²æ£’æ€§ï¼Œä¸ä¼ ç»Ÿçš„çº¿æ€§æ¢æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†ç±»æ•ˆæœï¼ŒåŒæ—¶åœ¨LoRAé€‚åº”è¿‡ç¨‹ä¸­è°ƒæ•´äº†æ³¨æ„åŠ›æœºåˆ¶çš„ä½ç§©å‚æ•°è®¾ç½®ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRAé€‚åº”çš„åŸºç¡€æ¨¡å‹åœ¨ä»…ä½¿ç”¨10%è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ¥è¿‘100%æ•°æ®å¯ç”¨æ€§ï¼Œä¸”åœ¨æœªè§è‚¿ç˜¤é¢†åŸŸçš„è¡¨ç°å‡ ä¹æ¶ˆé™¤äº†æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿæ¶æ„çš„å…¨å¾®è°ƒä»ç„¶ä¿æŒç«äº‰åŠ›ï¼Œæ˜¾ç¤ºå‡ºä¸åŒæ–¹æ³•çš„äº’è¡¥æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€è‚¿ç˜¤è¯Šæ–­å’Œç—…ç†å­¦ç ”ç©¶ã€‚é€šè¿‡æå‡æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºè‚¿ç˜¤çš„é¢„åè¯„ä¼°æä¾›æ›´ä¸ºå¯é çš„ä¾æ®ï¼Œè¿›è€Œæ”¹å–„ä¸´åºŠå†³ç­–å’Œæ‚£è€…ç®¡ç†ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿è‡³å…¶ä»–åŒ»å­¦å½±åƒä»»åŠ¡ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The performance of deep learning models is known to scale with data quantity and diversity. In pathology, as in many other medical imaging domains, the availability of labeled images for a specific task is often limited. Self-supervised learning techniques have enabled the use of vast amounts of unlabeled data to train large-scale neural networks, i.e., foundation models, that can address the limited data problem by providing semantically rich feature vectors that can generalize well to new tasks with minimal training effort increasing model performance and robustness. In this work, we investigate the use of foundation models for mitotic figure classification. The mitotic count, which can be derived from this classification task, is an independent prognostic marker for specific tumors and part of certain tumor grading systems. In particular, we investigate the data scaling laws on multiple current foundation models and evaluate their robustness to unseen tumor domains. Next to the commonly used linear probing paradigm, we also adapt the models using low-rank adaptation (LoRA) of their attention mechanisms. We compare all models against end-to-end-trained baselines, both CNNs and Vision Transformers. Our results demonstrate that LoRA-adapted foundation models provide superior performance to those adapted with standard linear probing, reaching performance levels close to 100% data availability with only 10% of training data. Furthermore, LoRA-adaptation of the most recent foundation models almost closes the out-of-domain performance gap when evaluated on unseen tumor domains. However, full fine-tuning of traditional architectures still yields competitive performance.

