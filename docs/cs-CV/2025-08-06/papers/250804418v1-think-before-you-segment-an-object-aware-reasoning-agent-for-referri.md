---
layout: default
title: Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation
---

# Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04418" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04418v1</a>
  <a href="https://arxiv.org/pdf/2508.04418.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04418v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04418v1', 'Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinxing Zhou, Yanghao Zhou, Mingfei Han, Tong Wang, Xiaojun Chang, Hisham Cholakkal, Rao Muhammad Anwer

**åˆ†ç±»**: cs.MM, cs.CV, cs.MA, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Project page: https://github.com/jasongief/TGS-Agent

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jasongief/TGS-Agent)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTGS-Agentä»¥è§£å†³éŸ³é¢‘è§†è§‰åˆ†å‰²ä¸­çš„å¯¹è±¡ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘è§†è§‰åˆ†å‰²` `å¤šæ¨¡æ€åˆ†æ` `å¯¹è±¡è¯†åˆ«` `æ¨ç†æ¨¡å‹` `æ•°æ®é›†æ„å»º` `æ¨¡å‹æ³›åŒ–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Ref-AVSæ–¹æ³•ä¾èµ–äºå¼ºåƒç´ çº§ç›‘ç£ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æœ‰æ•ˆç†è§£å’Œåˆ†å‰²ç›®æ ‡å¯¹è±¡ã€‚
2. æœ¬æ–‡æå‡ºTGS-Agentï¼Œé€šè¿‡Think-Ground-Segmentè¿‡ç¨‹æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼Œä½¿ç”¨Ref-Thinkerè¿›è¡Œå¤šæ¨¡æ€åˆ†æï¼Œæå‡å¯¹è±¡è¯†åˆ«ä¸åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚
3. åœ¨æ ‡å‡†Ref-AVSBenchå’Œæ–°æå‡ºçš„RÂ²-AVSBenchä¸Šï¼ŒTGS-Agentå‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Referring Audio-Visual Segmentation (Ref-AVS)æ—¨åœ¨æ ¹æ®ç»™å®šçš„å‚è€ƒè¡¨è¾¾åœ¨å¯å¬è§†é¢‘ä¸­åˆ†å‰²ç›®æ ‡å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤šæ¨¡æ€èåˆå­¦ä¹ æ½œåœ¨åµŒå…¥ï¼Œä¿ƒä½¿å¯è°ƒçš„SAM/SAM2è§£ç å™¨è¿›è¡Œåˆ†å‰²ï¼Œè¿™éœ€è¦å¼ºå¤§çš„åƒç´ çº§ç›‘ç£ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡æå‡ºTGS-Agentï¼Œä»æ˜ç¡®çš„å‚è€ƒç†è§£è§’åº¦å‡ºå‘ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºThink-Ground-Segmentè¿‡ç¨‹ï¼Œæ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹ï¼Œé¦–å…ˆé€šè¿‡å¤šæ¨¡æ€åˆ†æè¯†åˆ«æ‰€æŒ‡å¯¹è±¡ï¼Œç„¶åè¿›è¡Œç²—ç²’åº¦å®šä½å’Œç²¾ç¡®åˆ†å‰²ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«æ˜ç¡®å¯¹è±¡æ„ŸçŸ¥æ€ç»´-å›ç­”é“¾çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œä»¥å¯¹Ref-Thinkerè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨æ ‡å‡†Ref-AVSBenchå’Œæ–°æå‡ºçš„RÂ²-AVSBenchä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Ref-AVSä¸­çš„å¯¹è±¡ç†è§£é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºåƒç´ çº§ç›‘ç£ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§ä¸è¶³å’Œæ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºTGS-Agentï¼Œé€šè¿‡Think-Ground-Segmentè¿‡ç¨‹ï¼Œé¦–å…ˆè¿›è¡Œå¤šæ¨¡æ€åˆ†æä»¥è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œç„¶åè¿›è¡Œç²—ç²’åº¦å®šä½å’Œç²¾ç¡®åˆ†å‰²ï¼Œæ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šRef-Thinkerï¼ˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼‰ã€Grounding-DINOå’ŒSAM2ã€‚Ref-Thinkerè´Ÿè´£æ¨ç†ï¼ŒGrounding-DINOå’ŒSAM2åˆ™è¿›è¡Œå®šä½å’Œåˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºRef-Thinkerçš„å¼•å…¥ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰çº¿ç´¢ä¸Šè¿›è¡Œæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å¯¹è±¡è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œè€Œä¸å†ä¾èµ–äºä¼ ç»Ÿçš„åƒç´ çº§ç›‘ç£ã€‚

**å…³é”®è®¾è®¡**ï¼šæ„å»ºäº†ä¸€ä¸ªåŒ…å«æ˜ç¡®å¯¹è±¡æ„ŸçŸ¥æ€ç»´-å›ç­”é“¾çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œä»¥å¯¹Ref-Thinkerè¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆæ¨ç†å¹¶ç”Ÿæˆå‡†ç¡®çš„å¯¹è±¡æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TGS-Agentåœ¨æ ‡å‡†Ref-AVSBenchå’Œæ–°æå‡ºçš„RÂ²-AVSBenchä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨å¯¹è±¡è¯†åˆ«å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡éŸ³é¢‘è§†è§‰åˆ†å‰²çš„å‡†ç¡®æ€§ï¼ŒTGS-Agentèƒ½å¤Ÿå¸®åŠ©å®ç°æ›´æ™ºèƒ½çš„è§†è§‰åˆ†æç³»ç»Ÿï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM/SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understanding, we propose TGS-Agent, which decomposes the task into a Think-Ground-Segment process, mimicking the human reasoning procedure by first identifying the referred object through multimodal analysis, followed by coarse-grained grounding and precise segmentation. To this end, we first propose Ref-Thinker, a multimodal language model capable of reasoning over textual, visual, and auditory cues. We construct an instruction-tuning dataset with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The object description inferred by Ref-Thinker is used as an explicit prompt for Grounding-DINO and SAM2, which perform grounding and segmentation without relying on pixel-level supervision. Additionally, we introduce R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and reasoning-intensive references for better evaluating model generalization. Our approach achieves state-of-the-art results on both standard Ref-AVSBench and proposed R\textsuperscript{2}-AVSBench. Code will be available at https://github.com/jasongief/TGS-Agent.

