---
layout: default
title: Analyzing and Mitigating Object Hallucination: A Training Bias Perspective
---

# Analyzing and Mitigating Object Hallucination: A Training Bias Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04567" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04567v1</a>
  <a href="https://arxiv.org/pdf/2508.04567.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04567v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04567v1', 'Analyzing and Mitigating Object Hallucination: A Training Bias Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Li, Kun Zhou, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºObliviateä»¥è§£å†³å¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ä½“å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹` `ç‰©ä½“å¹»è§‰` `å»å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `è®­ç»ƒåå·®` `åäº‹å®å›¾åƒ` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ã€‚
2. æœ¬æ–‡æå‡ºObliviateæ–¹æ³•ï¼Œé€šè¿‡å»å­¦ä¹ è®­ç»ƒåå·®æ¥å‡è½»ç‰©ä½“å¹»è§‰ï¼Œä¸“æ³¨äºè¯­è¨€å»ºæ¨¡å¤´çš„æ›´æ–°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒObliviateåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—é™ä½å¹»è§‰ç°è±¡ï¼Œä¸”ä»…éœ€æ›´æ–°çº¦2%çš„æ¨¡å‹å‚æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€è®­ç»ƒæ•°æ®è§„æ¨¡çš„æ‰©å¤§ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä½†ä»ç„¶å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå³ç”Ÿæˆä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´çš„æ–‡æœ¬ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†è®­ç»ƒæ•°æ®åœ¨å¹»è§‰ä¸­çš„ä½œç”¨ï¼Œå¹¶å¼•å…¥äº†æ–°åŸºå‡†POPEv2ï¼Œå‘ç°å½“å‰LVLMså­˜åœ¨è®­ç»ƒåå·®ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´åœ¨è®­ç»ƒä¸­è§è¿‡çš„å›¾åƒä¸Šå¹»è§‰é¢‘ç¹ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å»å­¦ä¹ æ–¹æ³•Obliviateï¼Œé€šè¿‡è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çœŸå®æ ‡ç­¾ä¸æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚æ¥å‡è½»ç‰©ä½“å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼ŒObliviateåœ¨æ›´æ–°çº¦2%çš„å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œä¸”åœ¨æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®é‡ä¸Šå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶çš„ç‰©ä½“å¹»è§‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´åœ¨è®­ç»ƒä¸­è§è¿‡çš„å›¾åƒä¸Šå¹»è§‰é¢‘ç¹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åäº‹å®å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Obliviateæ–¹æ³•é€šè¿‡å»å­¦ä¹ è®­ç»ƒåå·®æ¥å‡è½»ç‰©ä½“å¹»è§‰ã€‚è¯¥æ–¹æ³•è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­çœŸå®æ ‡ç­¾ä¸æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œä½œä¸ºåå·®çš„ä»£ç†ï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä»…æ›´æ–°è¯­è¨€å»ºæ¨¡å¤´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šObliviateçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€åå·®è¯†åˆ«å’Œæ¨¡å‹å¾®è°ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡POPEv2åŸºå‡†æ”¶é›†åäº‹å®å›¾åƒï¼›ç„¶åï¼Œè¯†åˆ«æ¨¡å‹åœ¨è¿™äº›å›¾åƒä¸Šçš„åå·®ï¼›æœ€åï¼Œè¿›è¡Œé’ˆå¯¹æ€§çš„å¾®è°ƒä»¥å‡å°‘å¹»è§‰ç°è±¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šObliviateçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„å»å­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸å¤§å¹…åº¦æ›´æ–°æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½å¹»è§‰ç°è±¡ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å…¨é¢é‡è®­ç»ƒç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒObliviateä»…æ›´æ–°çº¦2%çš„æ¨¡å‹å‚æ•°ï¼Œé‡‡ç”¨äº†å‚æ•°å’Œæ•°æ®é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œå…³æ³¨çœŸå®æ ‡ç­¾ä¸æ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šæ›´å¥½åœ°å­¦ä¹ å’Œé€‚åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒObliviateåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—é™ä½äº†å¹»è§‰ç°è±¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åäº‹å®å›¾åƒæ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—æ°´å¹³ã€‚å°½ç®¡ä»…æ›´æ–°äº†çº¦2%çš„å‚æ•°ï¼ŒObliviateåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆä»2Båˆ°72Bï¼‰å’Œè®­ç»ƒæ•°æ®é‡ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡å‡è½»å¹»è§‰ç°è±¡ï¼ŒObliviateèƒ½å¤Ÿæå‡è¿™äº›åº”ç”¨çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As scaling up training data has significantly improved the general multimodal capabilities of Large Vision-Language Models (LVLMs), they still suffer from the hallucination issue, generating text that is inconsistent with the visual input. This phenomenon motivates us to systematically investigate the role of training data in hallucination. We introduce a new benchmark, POPEv2, which consists of counterfactual images collected from the training data of LVLMs with certain objects masked. Through comprehensive evaluation on POPEv2, we find that current LVLMs suffer from training bias: they fail to fully leverage their training data and hallucinate more frequently on images seen during training. Specifically, they perform poorly on counterfactual images, often incorrectly answering ``Yes'' to questions about masked objects. To understand this issue, we conduct probing experiments on the models' internal components, revealing that this training bias is primarily located in the language modeling (LM) head. Based on these findings, we propose Obliviate, an efficient and lightweight unlearning method designed to mitigate object hallucination via training bias unlearning. Obliviate identifies the discrepancy between ground-truth labels and model outputs on the training data as a proxy for bias and adopts a parameter- and data-efficient fine-tuning strategy that only updates the LM head. Extensive experiments demonstrate the effectiveness of our approach. While only reusing the training data and updating approximately 2\% of the parameters, Obliviate significantly reduces hallucination across both discriminative and generative tasks. Furthermore, it demonstrates strong scalability with respect to both model size (2B to 72B) and training data volume, and exhibits promising generalization to hallucination types beyond object-level hallucination. Our code and data will be publicly released.

