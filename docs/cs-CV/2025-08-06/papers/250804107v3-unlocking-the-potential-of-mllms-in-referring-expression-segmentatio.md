---
layout: default
title: Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder
---

# Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04107" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04107v3</a>
  <a href="https://arxiv.org/pdf/2508.04107.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04107v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04107v3', 'Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingchao Wang, Zhijian Wu, Dingjiang Huang, Yefeng Zheng, Hong Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-08-19)

**å¤‡æ³¨**: 9 pages, 4 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jcwang0602/MLLMSeg)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMLLMSegä»¥è§£å†³å‚è€ƒè¡¨è¾¾åˆ†å‰²ä¸­çš„æ€§èƒ½ä¸æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚è€ƒè¡¨è¾¾åˆ†å‰²` `å¤šæ¨¡æ€å¤§æ¨¡å‹` `è½»é‡çº§è§£ç å™¨` `ç‰¹å¾èåˆ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‚è€ƒè¡¨è¾¾åˆ†å‰²æ–¹æ³•è¦ä¹ˆä¾èµ–äºåºå¤§çš„æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼Œè¦ä¹ˆé‡‡ç”¨è½»é‡çº§æ–¹æ³•ï¼Œç‰ºç‰²äº†åˆ†å‰²ç²¾åº¦ã€‚
2. æœ¬æ–‡æå‡ºçš„MLLMSegæ¡†æ¶å……åˆ†åˆ©ç”¨MLLMè§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ï¼Œè®¾è®¡äº†ç»†èŠ‚å¢å¼ºå’Œè¯­ä¹‰ä¸€è‡´çš„ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMSegåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„SAMåŸºç¡€å’Œæ— SAMçš„ç«äº‰æ–¹æ³•ï¼Œå±•ç¤ºäº†è¾ƒå¥½çš„æ€§èƒ½ä¸æˆæœ¬å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰æ—¨åœ¨å¯¹å›¾åƒä¸­ç”±å‚è€ƒè¡¨è¾¾æŒ‡å®šçš„åŒºåŸŸè¿›è¡Œåˆ†å‰²ï¼Œéšç€å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…´èµ·ï¼Œè¯¥é¢†åŸŸé€æ¸å—åˆ°å…³æ³¨ã€‚å°½ç®¡MLLMsåœ¨è¯­ä¹‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åŸºäºtokenç”Ÿæˆçš„èŒƒå¼åœ¨åƒç´ çº§å¯†é›†é¢„æµ‹ä¸­å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„RESæ–¹æ³•è¦ä¹ˆå°†MLLMsä¸å‚æ•°åºå¤§çš„Segment Anything Modelï¼ˆSAMï¼‰ç»“åˆï¼Œåè€…å…·æœ‰632Mçš„ç½‘ç»œå‚æ•°ï¼Œè¦ä¹ˆé‡‡ç”¨ä¸ä½¿ç”¨SAMçš„è½»é‡çº§ç®¡é“ï¼Œä½†ç‰ºç‰²äº†å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œæœ¬æ–‡æå‡ºäº†MLLMSegæ¡†æ¶ï¼Œå……åˆ†åˆ©ç”¨MLLMè§†è§‰ç¼–ç å™¨ä¸­ç¼–ç çš„å›ºæœ‰è§†è§‰ç»†èŠ‚ç‰¹å¾ï¼Œè€Œæ— éœ€å¼•å…¥é¢å¤–çš„è§†è§‰ç¼–ç å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»†èŠ‚å¢å¼ºå’Œè¯­ä¹‰ä¸€è‡´çš„ç‰¹å¾èåˆæ¨¡å—ï¼ˆDSFFï¼‰ï¼Œå°†ä¸ç»†èŠ‚ç›¸å…³çš„è§†è§‰ç‰¹å¾ä¸MLLMçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºçš„è¯­ä¹‰ç›¸å…³ç‰¹å¾å……åˆ†æ•´åˆã€‚æœ€åï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªä»…æœ‰34Mç½‘ç»œå‚æ•°çš„è½»é‡çº§æ©è†œè§£ç å™¨ï¼Œä¼˜åŒ–åˆ©ç”¨è§†è§‰ç¼–ç å™¨çš„ç»†èŠ‚ç©ºé—´ç‰¹å¾å’ŒLLMçš„è¯­ä¹‰ç‰¹å¾ï¼Œå®ç°ç²¾ç¡®çš„æ©è†œé¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰ä¸­çš„æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå‚æ•°åºå¤§çš„æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—é«˜ï¼Œæˆ–é‡‡ç”¨è½»é‡çº§æ–¹æ³•ï¼Œå½±å“åˆ†å‰²ç²¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„MLLMSegæ¡†æ¶é€šè¿‡å……åˆ†åˆ©ç”¨MLLMè§†è§‰ç¼–ç å™¨çš„å›ºæœ‰ç‰¹å¾ï¼Œé¿å…äº†å¼•å…¥é¢å¤–è§†è§‰ç¼–ç å™¨çš„å¤æ‚æ€§ï¼ŒåŒæ—¶è®¾è®¡äº†ç»†èŠ‚å¢å¼ºå’Œè¯­ä¹‰ä¸€è‡´çš„ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥æå‡åˆ†å‰²æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMLLMSegçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰ç¼–ç å™¨ã€ç‰¹å¾èåˆæ¨¡å—ï¼ˆDSFFï¼‰å’Œè½»é‡çº§æ©è†œè§£ç å™¨ã€‚è§†è§‰ç¼–ç å™¨æå–å›¾åƒçš„ç»†èŠ‚ç‰¹å¾ï¼ŒDSFFå°†è¿™äº›ç»†èŠ‚ç‰¹å¾ä¸LLMè¾“å‡ºçš„è¯­ä¹‰ç‰¹å¾è¿›è¡Œèåˆï¼Œæœ€åé€šè¿‡è½»é‡çº§æ©è†œè§£ç å™¨ç”Ÿæˆåˆ†å‰²æ©è†œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†DSFFæ¨¡å—ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆæ•´åˆäº†è§†è§‰ç»†èŠ‚ä¸è¯­ä¹‰ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²ç²¾åº¦ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè½»é‡çº§æ©è†œè§£ç å™¨çš„ç½‘ç»œå‚æ•°ä»…ä¸º34Mï¼Œè®¾è®¡ä¸Šä¼˜åŒ–äº†ç©ºé—´ç‰¹å¾ä¸è¯­ä¹‰ç‰¹å¾çš„ç»“åˆï¼Œç¡®ä¿äº†é«˜æ•ˆçš„æ©è†œé¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMSegåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„SAMåŸºç¡€å’Œæ— SAMçš„ç«äº‰æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°5%-10%ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒè¾ƒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜çš„åˆ†å‰²ç²¾åº¦ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€è®¡ç®—æœºè§†è§‰ä¸­çš„äººæœºäº¤äº’ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡æ£€æµ‹ç­‰ã€‚é€šè¿‡æé«˜å‚è€ƒè¡¨è¾¾åˆ†å‰²çš„ç²¾åº¦ä¸æ•ˆç‡ï¼ŒMLLMSegå¯ä¸ºå¤šæ¨¡æ€ç†è§£å’Œæ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘æä¾›é‡è¦æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.

