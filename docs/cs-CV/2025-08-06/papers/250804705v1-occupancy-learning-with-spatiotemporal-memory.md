---
layout: default
title: Occupancy Learning with Spatiotemporal Memory
---

# Occupancy Learning with Spatiotemporal Memory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04705" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04705v1</a>
  <a href="https://arxiv.org/pdf/2508.04705.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04705v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04705v1', 'Occupancy Learning with Spatiotemporal Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyang Leng, Jiawei Yang, Wenlong Yi, Bolei Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Accepted to ICCV2025. Project website: https://matthew-leng.github.io/stocc

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºST-Occä»¥è§£å†³3Då ç”¨ç‡å­¦ä¹ ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `3Då ç”¨ç‡` `æ—¶ç©ºè®°å¿†` `è‡ªåŠ¨é©¾é©¶` `ç¯å¢ƒæ„ŸçŸ¥` `åŠ¨æ€ä½“ç´ ` `æ·±åº¦å­¦ä¹ ` `æ—¶ç©ºä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šå¸§è¾“å…¥æ—¶ï¼Œéš¾ä»¥é«˜æ•ˆèšåˆ3Då ç”¨ç‡ï¼Œé¢ä¸´é«˜å¤„ç†æˆæœ¬å’Œä½“ç´ åŠ¨æ€æ€§å¸¦æ¥çš„ä¸ç¡®å®šæ€§ã€‚
2. æœ¬æ–‡æå‡ºST-Occæ¡†æ¶ï¼Œé€šè¿‡æ—¶ç©ºè®°å¿†æ•æ‰å†å²ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è®°å¿†æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå½“å‰å ç”¨ç‡è¡¨ç¤ºçš„æ—¶ç©ºä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒST-Occåœ¨3Då ç”¨ç‡é¢„æµ‹ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†3 mIoUï¼Œå¹¶å‡å°‘äº†29%çš„æ—¶é—´ä¸ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Då ç”¨ç‡ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ„ŸçŸ¥è¡¨ç¤ºï¼Œèƒ½å¤Ÿç»†è‡´åœ°å»ºæ¨¡è‡ªåŠ¨é©¾é©¶ç¯å¢ƒã€‚ç„¶è€Œï¼Œç”±äºå¤„ç†æˆæœ¬é«˜ä»¥åŠä½“ç´ çš„ä¸ç¡®å®šæ€§å’ŒåŠ¨æ€æ€§ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åœ¨å¤šä¸ªè¾“å…¥å¸§ä¸­èšåˆ3Då ç”¨ç‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ST-Occï¼Œä¸€ä¸ªåœºæ™¯çº§å ç”¨ç‡è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„æ—¶ç©ºç‰¹å¾ã€‚ST-OccåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šä¸€ä¸ªæ•æ‰å…¨é¢å†å²ä¿¡æ¯å¹¶é€šè¿‡åœºæ™¯çº§è¡¨ç¤ºé«˜æ•ˆå­˜å‚¨çš„æ—¶ç©ºè®°å¿†ï¼Œä»¥åŠä¸€ä¸ªåŸºäºæ—¶ç©ºè®°å¿†å¯¹å½“å‰å ç”¨ç‡è¡¨ç¤ºè¿›è¡Œæ¡ä»¶åŒ–çš„è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Då ç”¨ç‡é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ—¶ç©ºè¡¨ç¤ºçš„å­¦ä¹ æ•ˆæœï¼ŒmIoUæå‡äº†3ï¼Œå¹¶å‡å°‘äº†29%çš„æ—¶é—´ä¸ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šå¸§è¾“å…¥ä¸‹é«˜æ•ˆèšåˆ3Då ç”¨ç‡çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€ä½“ç´ æ—¶å­˜åœ¨é«˜å¤„ç†æˆæœ¬å’Œä¸ä¸€è‡´æ€§çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šST-Occæ¡†æ¶é€šè¿‡å¼•å…¥æ—¶ç©ºè®°å¿†å’Œè®°å¿†æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨å†å²ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºæ—¶ç©ºç‰¹å¾çš„å­¦ä¹ å’Œè¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šST-Occç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šæ—¶ç©ºè®°å¿†æ¨¡å—ç”¨äºå­˜å‚¨å†å²ä¿¡æ¯ï¼Œè®°å¿†æ³¨æ„åŠ›æ¨¡å—åˆ™æ ¹æ®æ—¶ç©ºè®°å¿†å¯¹å½“å‰å ç”¨ç‡è¡¨ç¤ºè¿›è¡Œæ¡ä»¶åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ—¶ç©ºè®°å¿†å’ŒåŠ¨æ€æ„ŸçŸ¥çš„è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†æ—¶ç©ºè¡¨ç¤ºçš„å­¦ä¹ æ•ˆæœï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åŠ¨æ€ç¯å¢ƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒST-Occé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å‚æ•°è®¾ç½®æ¥å¹³è¡¡æ—¶ç©ºè®°å¿†çš„å­˜å‚¨æ•ˆç‡ä¸ä¿¡æ¯æ•æ‰èƒ½åŠ›ã€‚å…·ä½“çš„ç½‘ç»œæ¶æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒST-Occåœ¨3Då ç”¨ç‡é¢„æµ‹ä»»åŠ¡ä¸­ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•æå‡äº†3 mIoUï¼Œå¹¶æˆåŠŸå‡å°‘äº†29%çš„æ—¶é—´ä¸ä¸€è‡´æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ—¶ç©ºç‰¹å¾å­¦ä¹ æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜3Då ç”¨ç‡çš„å­¦ä¹ æ•ˆæœï¼ŒST-Occèƒ½å¤Ÿä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›æ›´å‡†ç¡®çš„ç¯å¢ƒæ„ŸçŸ¥ï¼Œè¿›è€Œæå‡å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„å®æ—¶æ„ŸçŸ¥ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.

