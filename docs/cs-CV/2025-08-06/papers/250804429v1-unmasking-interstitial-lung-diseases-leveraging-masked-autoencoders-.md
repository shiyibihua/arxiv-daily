---
layout: default
title: Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis
---

# Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04429" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04429v1</a>
  <a href="https://arxiv.org/pdf/2508.04429.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04429v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04429v1', 'Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ethan Dack, Lorenzo Brigato, Vasilis Dedousis, Janine Gote-Schniering, Cheryl, Hanno Hoppe, Aristomenis Exadaktylos, Manuela Funke-Chambour, Thomas Geiser, Andreas Christe, Lukas Ebner, Stavroula Mougiakakou

**åˆ†ç±»**: eess.IV, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/eedack01/lung_masked_autoencoder)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨æ©ç è‡ªç¼–ç å™¨æå‡é—´è´¨æ€§è‚ºç—…çš„è¯Šæ–­èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ©ç è‡ªç¼–ç å™¨` `é—´è´¨æ€§è‚ºç—…` `åŒ»å­¦å½±åƒ` `æ— æ ‡ç­¾å­¦ä¹ ` `ç‰¹å¾æå–` `æ·±åº¦å­¦ä¹ ` `è‚ºéƒ¨CTæ‰«æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é—´è´¨æ€§è‚ºç—…è¯Šæ–­æ–¹æ³•ä¾èµ–äºæ ‡æ³¨æ•°æ®é›†ï¼Œä½†è¿™äº›æ•°æ®é›†é€šå¸¸ç¨€ç¼ºï¼Œé™åˆ¶äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¿›è€Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¾®è°ƒä»¥å®ç°æœ‰æ•ˆçš„ç–¾ç—…è¯Šæ–­ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAEèƒ½å¤Ÿæå–å‡ºå…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†é—´è´¨æ€§è‚ºç—…çš„è¯Šæ–­æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ— æ ‡ç­¾æ•°æ®é¢„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°ç¨³å¥ä¸”å¯Œæœ‰ä¿¡æ¯çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™åœ¨é—´è´¨æ€§è‚ºç—…ç ”ç©¶ä¸­å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºæ ‡æ³¨çš„å½±åƒæ•°æ®é›†ç¨€ç¼ºã€‚æœ¬æ–‡åœ¨è¶…è¿‡5000ä¸ªèƒ¸éƒ¨CTæ‰«æå›¾åƒçš„ç²¾å¿ƒæ•´ç†æ•°æ®é›†ä¸Šè®­ç»ƒMAEï¼Œç»“åˆäº†å†…éƒ¨æ•°æ®ä¸æ¥è‡ªCOVID-19å’Œç»†èŒæ€§è‚ºç‚ç­‰ç›¸å…³ç–¾ç—…çš„å…¬å¼€æ‰«ææ•°æ®ã€‚ç»è¿‡é¢„è®­ç»ƒçš„MAEéšååœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°é—´è´¨æ€§è‚ºç—…çš„è¯Šæ–­ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMAEsèƒ½å¤Ÿæœ‰æ•ˆæå–ä¸´åºŠç›¸å…³ç‰¹å¾ï¼Œå¹¶åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„æƒ…å†µä¸‹æé«˜è¯Šæ–­æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é—´è´¨æ€§è‚ºç—…è¯Šæ–­ä¸­ç¼ºä¹æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®­ç»ƒæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åœ¨æ— æ ‡ç­¾çš„èƒ¸éƒ¨CTæ‰«ææ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ åˆ°ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºï¼Œç„¶ååœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€MAEé¢„è®­ç»ƒå’Œä¸‹æ¸¸åˆ†ç±»å¾®è°ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ”¶é›†è¶…è¿‡5000ä¸ªCTæ‰«æå›¾åƒï¼Œéšåè¿›è¡ŒMAEçš„è®­ç»ƒï¼Œæœ€ååœ¨ç‰¹å®šçš„åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†MAEåº”ç”¨äºåŒ»å­¦å½±åƒé¢†åŸŸï¼Œå°¤å…¶æ˜¯é—´è´¨æ€§è‚ºç—…çš„è¯Šæ–­ä¸­ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰¹å¾æå–èƒ½åŠ›ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”åŒ»å­¦å½±åƒçš„ç‰¹å¾å­¦ä¹ éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡MAEé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨é—´è´¨æ€§è‚ºç—…çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜äº†15%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºMAEåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€è‚ºç—…è¯Šæ–­å’Œäººå·¥æ™ºèƒ½è¾…åŠ©åŒ»ç–—ã€‚é€šè¿‡æå‡é—´è´¨æ€§è‚ºç—…çš„è¯Šæ–­èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«å’Œæ²»ç–—æ‚£è€…ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Masked autoencoders (MAEs) have emerged as a powerful approach for pre-training on unlabelled data, capable of learning robust and informative feature representations. This is particularly advantageous in diffused lung disease research, where annotated imaging datasets are scarce. To leverage this, we train an MAE on a curated collection of over 5,000 chest computed tomography (CT) scans, combining in-house data with publicly available scans from related conditions that exhibit similar radiological patterns, such as COVID-19 and bacterial pneumonia. The pretrained MAE is then fine-tuned on a downstream classification task for diffused lung disease diagnosis. Our findings demonstrate that MAEs can effectively extract clinically meaningful features and improve diagnostic performance, even in the absence of large-scale labelled datasets. The code and the models are available here: https://github.com/eedack01/lung_masked_autoencoder.

