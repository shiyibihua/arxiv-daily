---
layout: default
title: Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation
---

# Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04049" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04049v1</a>
  <a href="https://arxiv.org/pdf/2508.04049.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04049v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04049v1', 'Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayi He, Xu Wang, Shengeng Tang, Yaxiong Wang, Lechao Cheng, Dan Guo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 9 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥è§£å†³æ‰‹è¯­è§†é¢‘ç”Ÿæˆä¸­çš„æ•°æ®éœ€æ±‚ä¸æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹è¯­ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `åŠ¨ä½œè§£è€¦` `ç¥ç»æ¸²æŸ“` `è§†é¢‘åˆæˆ` `ä¸ªæ€§åŒ–æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹è¯­è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¾èµ–äºå¤§é‡ç­¾åè€…ç‰¹å®šæ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ„å»ºç­¾åè€…æ— å…³çš„å¤šæ¨¡æ€åŠ¨ä½œè¯æ±‡æ¥è§£è€¦åŠ¨ä½œè¯­ä¹‰ä¸ç­¾åè€…èº«ä»½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆè´¨é‡å’Œä¸ªæ€§åŒ–çµæ´»æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰‹è¯­è§†é¢‘ç”Ÿæˆéœ€è¦åœ¨ç²¾ç¡®çš„è¯­ä¹‰æ§åˆ¶ä¸‹äº§ç”Ÿè‡ªç„¶çš„æ‰‹åŠ¿åŠ¨ä½œå’Œé€¼çœŸçš„å¤–è§‚ï¼Œä½†é¢ä¸´ç€è¿‡å¤šçš„ç­¾åè€…ç‰¹å®šæ•°æ®éœ€æ±‚å’Œè¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰‹è¯­è§†é¢‘ç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡ä¸¤é˜¶æ®µåˆæˆæ¡†æ¶å°†åŠ¨ä½œè¯­ä¹‰ä¸ç­¾åè€…èº«ä»½è§£è€¦ã€‚é¦–å…ˆï¼Œæ„å»ºäº†ä¸€ä¸ªç­¾åè€…æ— å…³çš„å¤šæ¨¡æ€åŠ¨ä½œè¯æ±‡ï¼Œæ¯ä¸ªæ‰‹åŠ¿ä»¥èº«ä»½æ— å…³çš„å§¿æ€ã€æ‰‹åŠ¿å’Œ3Dç½‘æ ¼åºåˆ—å­˜å‚¨ï¼Œä»…éœ€æ¯ä¸ªæ‰‹åŠ¿å½•åˆ¶ä¸€æ¬¡ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ç¦»æ•£åˆ°è¿ç»­çš„åŠ¨ä½œåˆæˆé˜¶æ®µï¼Œå°†æ£€ç´¢åˆ°çš„æ‰‹åŠ¿åºåˆ—è½¬åŒ–ä¸ºæ—¶é—´ä¸Šè¿è´¯çš„è¿åŠ¨è½¨è¿¹ï¼Œéšåé€šè¿‡èº«ä»½æ„ŸçŸ¥çš„ç¥ç»æ¸²æŸ“ç”Ÿæˆä»»æ„ç­¾åè€…çš„é€¼çœŸè§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼ŒåŠ¨ä½œä¸èº«ä»½çš„è§£è€¦ä¸ä»…å¯è¡Œï¼Œè€Œä¸”åœ¨åˆæˆè´¨é‡å’Œç­¾åè€…ä¸ªæ€§åŒ–çµæ´»æ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ‰‹è¯­è§†é¢‘ç”Ÿæˆé¢ä¸´è¿‡å¤šç­¾åè€…ç‰¹å®šæ•°æ®éœ€æ±‚å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”ä¸åŒç­¾åè€…çš„å¤–è§‚å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡é€šè¿‡æ„å»ºä¸€ä¸ªç­¾åè€…æ— å…³çš„å¤šæ¨¡æ€åŠ¨ä½œè¯æ±‡ï¼Œå°†åŠ¨ä½œè¯­ä¹‰ä¸ç­¾åè€…èº«ä»½è§£è€¦ï¼Œè¿›è€Œå®ç°é«˜è´¨é‡çš„æ‰‹è¯­è§†é¢‘ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ„å»ºå¤šæ¨¡æ€åŠ¨ä½œè¯æ±‡ï¼Œç¬¬äºŒé˜¶æ®µè¿›è¡Œç¦»æ•£åˆ°è¿ç»­çš„åŠ¨ä½œåˆæˆï¼Œæœ€åé€šè¿‡èº«ä»½æ„ŸçŸ¥çš„ç¥ç»æ¸²æŸ“ç”Ÿæˆè§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†åŠ¨ä½œè§†ä¸ºç¬¬ä¸€ç±»å…¬æ°‘ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„æ½œåœ¨å§¿æ€åŠ¨æ€ä½œä¸ºå¯ç§»æ¤çš„â€œç¼–èˆå±‚â€ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆçš„çµæ´»æ€§å’Œè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†èº«ä»½æ— å…³çš„å§¿æ€å’Œæ‰‹åŠ¿åºåˆ—è¡¨ç¤ºï¼Œè®¾è®¡äº†åˆé€‚çš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç”Ÿæˆè§†é¢‘çš„æ—¶é—´è¿è´¯æ€§ï¼Œå¹¶ä½¿ç”¨äº†å…ˆè¿›çš„ç¥ç»ç½‘ç»œç»“æ„è¿›è¡Œæ¸²æŸ“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆæˆè´¨é‡ä¸Šç›¸æ¯”äºåŸºçº¿æ–¹æ³•æå‡äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨ä¸ªæ€§åŒ–æ–¹é¢å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒç­¾åè€…çš„å¤–è§‚å˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†ç”¨æˆ·ä½“éªŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å¨±ä¹å’Œè¾…åŠ©æ²Ÿé€šç­‰ï¼Œèƒ½å¤Ÿä¸ºæ‰‹è¯­å­¦ä¹ è€…å’Œä½¿ç”¨è€…æä¾›æ›´è‡ªç„¶çš„äº¤æµæ–¹å¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨å¤šè¯­è¨€ç¿»è¯‘å’Œäººæœºäº¤äº’ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæå‡æ— éšœç¢æ²Ÿé€šçš„æ•ˆç‡ä¸è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sign language video generation requires producing natural signing motions with realistic appearances under precise semantic control, yet faces two critical challenges: excessive signer-specific data requirements and poor generalization. We propose a new paradigm for sign language video generation that decouples motion semantics from signer identity through a two-phase synthesis framework. First, we construct a signer-independent multimodal motion lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D mesh sequences, requiring only one recording per sign. This compact representation enables our second key innovation: a discrete-to-continuous motion synthesis stage that transforms retrieved gloss sequences into temporally coherent motion trajectories, followed by identity-aware neural rendering to produce photorealistic videos of arbitrary signers. Unlike prior work constrained by signer-specific datasets, our method treats motion as a first-class citizen: the learned latent pose dynamics serve as a portable "choreography layer" that can be visually realized through different human appearances. Extensive experiments demonstrate that disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization.

