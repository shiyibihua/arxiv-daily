---
layout: default
title: TopKD: Top-scaled Knowledge Distillation
---

# TopKD: Top-scaled Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04539" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04539v1</a>
  <a href="https://arxiv.org/pdf/2508.04539.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04539v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04539v1', 'TopKD: Top-scaled Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Wang, Jinjia Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 12 pages, 6 figures, conference, 8 Tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTopKDä»¥æå‡çŸ¥è¯†è’¸é¦ä¸­çš„logitä¿¡æ¯åˆ©ç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `logitåˆ†å¸ƒ` `Top-KçŸ¥è¯†` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å‹ç¼©` `è§†è§‰å˜æ¢å™¨` `è’¸é¦è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•å¤šé›†ä¸­äºç‰¹å¾çº§çŸ¥è¯†è½¬ç§»ï¼Œå¿½è§†äº†æ•™å¸ˆæ¨¡å‹logitåˆ†å¸ƒä¸­çš„é‡è¦ä¿¡æ¯ï¼Œå¯¼è‡´ä¿¡æ¯åˆ©ç”¨ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºTopKDï¼Œé€šè¿‡Top-K Scaling Moduleå’ŒTop-K Decoupled Lossï¼Œå¢å¼ºlogitä¿¡æ¯çš„åˆ©ç”¨ï¼Œæä¾›æ›´æœ‰æ•ˆçš„è’¸é¦ç›‘ç£ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒTopKDåœ¨è’¸é¦æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨è§†è§‰å˜æ¢å™¨çš„è’¸é¦ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å¾çº§çŸ¥è¯†è½¬ç§»ä¸Šï¼Œå¸¸å¸¸å¿½è§†æ•™å¸ˆæ¨¡å‹logitåˆ†å¸ƒä¸­è•´å«çš„é‡è¦ä¿¡æ¯ã€‚æœ¬æ–‡é‡æ–°å®¡è§†åŸºäºlogitçš„è’¸é¦ï¼Œæå‡ºäº†ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„å…³é”®å…ƒç´ ï¼šTop-KçŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Top-scaled Knowledge Distillation (TopKD)ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€é«˜æ•ˆä¸”ä¸æ¶æ„æ— å…³çš„æ¡†æ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†åŸºäºlogitçš„è’¸é¦ã€‚TopKDåŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šï¼ˆ1ï¼‰Top-K Scaling Module (TSM)ï¼Œè‡ªé€‚åº”åœ°æ”¾å¤§æœ€å…·ä¿¡æ¯é‡çš„logitsï¼›ï¼ˆ2ï¼‰Top-K Decoupled Loss (TDL)ï¼Œæä¾›æœ‰é’ˆå¯¹æ€§å’Œæœ‰æ•ˆçš„ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTopKDåœ¨CIFAR-100ã€ImageNetã€STL-10å’ŒTiny-ImageNetä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„è’¸é¦æ–¹æ³•ï¼Œå°¤å…¶åœ¨è’¸é¦è§†è§‰å˜æ¢å™¨æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§ç½‘ç»œæ¶æ„ä¸­çš„é€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ•™å¸ˆæ¨¡å‹logitåˆ†å¸ƒä¸­çš„å…³é”®ä¿¡æ¯çš„é—®é¢˜ï¼Œå¯¼è‡´è’¸é¦æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºTopKDæ¡†æ¶ï¼Œé€šè¿‡æ”¾å¤§æœ€å…·ä¿¡æ¯é‡çš„logitså’Œæä¾›é’ˆå¯¹æ€§çš„æŸå¤±å‡½æ•°ï¼Œæ¥æå‡çŸ¥è¯†è’¸é¦çš„æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTopKDæ¡†æ¶ä¸»è¦ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šTop-K Scaling Module (TSM)å’ŒTop-K Decoupled Loss (TDL)ã€‚TSMè´Ÿè´£è‡ªé€‚åº”æ”¾å¤§é‡è¦logitsï¼Œè€ŒTDLåˆ™æä¾›æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šTopKDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥Top-KçŸ¥è¯†çš„æ¦‚å¿µï¼Œé€šè¿‡æ”¾å¤§æœ€æœ‰ä¿¡æ¯é‡çš„logitsï¼Œæ˜¾è‘—æå‡äº†è’¸é¦æ•ˆæœã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç‰¹å¾çº§è’¸é¦æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œå¼ºè°ƒäº†logitä¿¡æ¯çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTSMæ¨¡å—é€šè¿‡åŠ¨æ€è°ƒæ•´logitsçš„æƒé‡æ¥æ”¾å¤§Top-Kä¿¡æ¯ï¼Œè€ŒTDLåˆ™é€šè¿‡è§£è€¦æŸå¤±å‡½æ•°æ¥æä¾›æ›´ç²¾ç¡®çš„ç›‘ç£ï¼Œç¡®ä¿è’¸é¦è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTopKDåœ¨CIFAR-100ã€ImageNetã€STL-10å’ŒTiny-ImageNetç­‰æ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„è’¸é¦æ–¹æ³•ï¼Œå°¤å…¶åœ¨è’¸é¦è§†è§‰å˜æ¢å™¨æ—¶ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§ç½‘ç»œæ¶æ„ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸçš„æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿã€‚é€šè¿‡æœ‰æ•ˆçš„çŸ¥è¯†è’¸é¦ï¼ŒTopKDèƒ½å¤Ÿå¸®åŠ©åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²é«˜æ€§èƒ½æ¨¡å‹ï¼Œæå‡å®é™…åº”ç”¨çš„æ•ˆç‡å’Œæ•ˆæœã€‚æœªæ¥ï¼ŒTopKDå¯èƒ½ä¼šåœ¨å¤šç§æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨æ¨¡å‹çš„è½»é‡åŒ–å’Œé«˜æ•ˆåŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in knowledge distillation (KD) predominantly emphasize feature-level knowledge transfer, frequently overlooking critical information embedded within the teacher's logit distributions. In this paper, we revisit logit-based distillation and reveal an underexplored yet critical element: Top-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge Distillation (TopKD), a simple, efficient, and architecture-agnostic framework that significantly enhances logit-based distillation. TopKD consists of two main components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies the most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers targeted and effective supervision. Notably, TopKD integrates seamlessly into existing KD methods without introducing extra modules or requiring architectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10, and Tiny-ImageNet demonstrate that TopKD consistently surpasses state-of-the-art distillation methods. Moreover, our method demonstrates substantial effectiveness when distilling Vision Transformers, underscoring its versatility across diverse network architectures. These findings highlight the significant potential of logits to advance knowledge distillation.

