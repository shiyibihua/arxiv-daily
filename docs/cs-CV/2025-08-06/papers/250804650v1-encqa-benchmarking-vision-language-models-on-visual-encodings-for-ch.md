---
layout: default
title: EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts
---

# EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04650" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04650v1</a>
  <a href="https://arxiv.org/pdf/2508.04650.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04650v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04650v1', 'EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kushin Mukherjee, Donghao Ren, Dominik Moritz, Yannick Assogba

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**DOI**: [10.1109/TVCG.2025.3634249](https://doi.org/10.1109/TVCG.2025.3634249)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEncQAåŸºå‡†ä»¥æå‡å›¾è¡¨ç†è§£çš„è§†è§‰æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹` `å›¾è¡¨ç†è§£` `è§†è§‰æ¨ç†` `åŸºå‡†æµ‹è¯•` `æ•°æ®å¯è§†åŒ–` `åˆæˆé—®ç­”å¯¹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£æ–¹é¢çš„è¿›å±•æœªèƒ½å…¨é¢åæ˜ è§†è§‰æ¨ç†èƒ½åŠ›çš„å¤šæ ·æ€§ã€‚
2. æœ¬æ–‡æå‡ºEncQAåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿæ€§è¦†ç›–è§†è§‰ç¼–ç å’Œåˆ†æä»»åŠ¡ï¼Œæ¥æå‡å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¸åŒç¼–ç å’Œä»»åŠ¡ä¹‹é—´çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¸”æ¨¡å‹è§„æ¨¡çš„å¢åŠ å¹¶æœªå¸¦æ¥é¢„æœŸçš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾è¡¨ç†è§£åŸºå‡†ä¸Šå–å¾—äº†æŒç»­è¿›å±•ï¼Œä½†è¿™ä¸€è¿›å±•å¹¶æœªå……åˆ†åæ˜ å‡ºå›¾è¡¨è§£è¯»æ‰€éœ€çš„è§†è§‰æ¨ç†èƒ½åŠ›çš„å¹¿åº¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†EncQAï¼Œä¸€ä¸ªåŸºäºå¯è§†åŒ–æ–‡çŒ®çš„æ–°åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§è¦†ç›–å›¾è¡¨ç†è§£æ‰€éœ€çš„è§†è§‰ç¼–ç å’Œåˆ†æä»»åŠ¡ã€‚EncQAæä¾›äº†2076å¯¹åˆæˆé—®ç­”å¯¹ï¼Œæ¶µç›–å…­ç§è§†è§‰ç¼–ç é€šé“å’Œå…«ç§ä»»åŠ¡ã€‚å¯¹ä¹ç§æœ€å…ˆè¿›çš„VLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨åŒä¸€ä»»åŠ¡ä¸­çš„ä¸åŒç¼–ç å’Œä¸åŒä»»åŠ¡ä¹‹é—´çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¸”è®¸å¤šä»»åŠ¡-ç¼–ç å¯¹çš„è¡¨ç°å¹¶æœªéšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œæå‡ã€‚ç»“æœè¡¨æ˜ï¼Œæå‡å›¾è¡¨ç†è§£èƒ½åŠ›éœ€è¦é’ˆå¯¹ç‰¹å®šè§†è§‰æ¨ç†ç¼ºå£çš„ç­–ç•¥ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‰©å¤§æ¨¡å‹æˆ–æ•°æ®é›†çš„è§„æ¨¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£ä¸­æœªèƒ½å……åˆ†æ•æ‰è§†è§‰æ¨ç†èƒ½åŠ›çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒè§†è§‰ç¼–ç å’Œä»»åŠ¡ä¹‹é—´çš„è¡¨ç°å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEncQAåŸºå‡†é€šè¿‡æä¾›2076å¯¹åˆæˆé—®ç­”å¯¹ï¼Œç³»ç»Ÿæ€§è¦†ç›–å…­ç§è§†è§‰ç¼–ç é€šé“å’Œå…«ç§åˆ†æä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å’Œæå‡å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEncQAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ã€è§†è§‰ç¼–ç é€šé“çš„å®šä¹‰ã€ä»»åŠ¡è®¾è®¡å’Œè¯„ä¼°æ¨¡å—ï¼Œç¡®ä¿å¯¹å›¾è¡¨ç†è§£çš„å¤šç»´åº¦è€ƒå¯Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šEncQAçš„åˆ›æ–°åœ¨äºå…¶ç³»ç»Ÿæ€§è®¾è®¡ï¼Œæ¶µç›–äº†å¤šç§è§†è§‰ç¼–ç å’Œä»»åŠ¡ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨è§†è§‰æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸Šçš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒEncQAç¡®ä¿äº†é—®ç­”å¯¹çš„å¹³è¡¡åˆ†å¸ƒï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†ä¸åŒä»»åŠ¡çš„ç‰¹æ€§ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨å„ä¸ªè§†è§‰ç¼–ç ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¹ç§æœ€å…ˆè¿›çš„VLMåœ¨ä¸åŒè§†è§‰ç¼–ç å’Œä»»åŠ¡ä¹‹é—´çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¸”è®¸å¤šä»»åŠ¡-ç¼–ç å¯¹çš„æ€§èƒ½å¹¶æœªéšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œæå‡ï¼Œè¡¨æ˜éœ€è¦é’ˆå¯¹æ€§ç­–ç•¥æ¥è§£å†³è§†è§‰æ¨ç†çš„ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®å¯è§†åŒ–ã€å•†ä¸šæ™ºèƒ½å’Œæ•™è‚²ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´æ™ºèƒ½çš„å›¾è¡¨åˆ†æå·¥å…·ï¼Œæé«˜ç”¨æˆ·å¯¹å¤æ‚æ•°æ®çš„ç†è§£èƒ½åŠ›ã€‚æœªæ¥ï¼ŒEncQAåŸºå‡†å¯èƒ½æ¨åŠ¨æ›´å¤šé’ˆå¯¹è§†è§‰æ¨ç†çš„ç ”ç©¶ï¼Œä¿ƒè¿›å¤šæ¨¡æ€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal vision-language models (VLMs) continue to achieve ever-improving scores on chart understanding benchmarks. Yet, we find that this progress does not fully capture the breadth of visual reasoning capabilities essential for interpreting charts. We introduce EncQA, a novel benchmark informed by the visualization literature, designed to provide systematic coverage of visual encodings and analytic tasks that are crucial for chart understanding. EncQA provides 2,076 synthetic question-answer pairs, enabling balanced coverage of six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks (find extrema, retrieve value, find anomaly, filter values, compute derived value exact, compute derived value relative, correlate values, and correlate values relative). Our evaluation of 9 state-of-the-art VLMs reveals that performance varies significantly across encodings within the same task, as well as across tasks. Contrary to expectations, we observe that performance does not improve with model size for many task-encoding pairs. Our results suggest that advancing chart understanding requires targeted strategies addressing specific visual reasoning gaps, rather than solely scaling up model or dataset size.

