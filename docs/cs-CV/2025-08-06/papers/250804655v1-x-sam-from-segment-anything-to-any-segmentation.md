---
layout: default
title: X-SAM: From Segment Anything to Any Segmentation
---

# X-SAM: From Segment Anything to Any Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04655" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04655v1</a>
  <a href="https://arxiv.org/pdf/2508.04655.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04655v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04655v1', 'X-SAM: From Segment Anything to Any Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: Technical Report

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wanghao9610/X-SAM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºX-SAMä»¥è§£å†³ç°æœ‰å›¾åƒåˆ†å‰²æ¨¡å‹çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒåˆ†å‰²` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰åŸºç¡€åˆ†å‰²` `å¤§å‹è¯­è¨€æ¨¡å‹` `åƒç´ çº§ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å¤šæ©è†œé¢„æµ‹å’Œç±»åˆ«ç‰¹å®šåˆ†å‰²ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆæ‰€æœ‰åˆ†å‰²ä»»åŠ¡ã€‚
2. X-SAMé€šè¿‡å¼•å…¥ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ‰©å±•äº†åˆ†å‰²ä»»åŠ¡çš„èŒƒç•´ï¼Œå¹¶æå‡ºäº†è§†è§‰åŸºç¡€åˆ†å‰²ä»»åŠ¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒX-SAMåœ¨å¤šä¸ªå›¾åƒåˆ†å‰²åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæå‡äº†å¤šæ¨¡æ€è§†è§‰ç†è§£çš„æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†è¡¨ç¤ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åƒç´ çº§æ„ŸçŸ¥ç†è§£ä¸Šå­˜åœ¨ä¸è¶³ã€‚å°½ç®¡Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰æç¤ºé©±åŠ¨çš„å›¾åƒåˆ†å‰²ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šæ©è†œé¢„æµ‹å’Œç±»åˆ«ç‰¹å®šåˆ†å‰²ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œä¸”æ— æ³•åœ¨ç»Ÿä¸€æ¨¡å‹æ¶æ„ä¸­æ•´åˆæ‰€æœ‰åˆ†å‰²ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†X-SAMï¼Œä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œæ‰©å±•äº†åˆ†å‰²èŒƒå¼ï¼Œä»â€œsegment anythingâ€åˆ°â€œany segmentationâ€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¢å¼ºäº†MLLMçš„åƒç´ çº§æ„ŸçŸ¥ç†è§£èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²ä»»åŠ¡ï¼Œç§°ä¸ºè§†è§‰åŸºç¡€ï¼ˆVGDï¼‰åˆ†å‰²ï¼Œèƒ½å¤Ÿé€šè¿‡äº¤äº’å¼è§†è§‰æç¤ºåˆ†å‰²æ‰€æœ‰å®ä¾‹å¯¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-SAMåœ¨å¤šç§å›¾åƒåˆ†å‰²åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨å¤šæ¨¡æ€åƒç´ çº§è§†è§‰ç†è§£ä¸­çš„æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å¤šæ©è†œé¢„æµ‹å’Œç±»åˆ«ç‰¹å®šåˆ†å‰²ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯æ— æ³•åœ¨ç»Ÿä¸€æ¶æ„ä¸­æ•´åˆæ‰€æœ‰åˆ†å‰²ä»»åŠ¡çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†X-SAMæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è§†è§‰åŸºç¡€åˆ†å‰²ä»»åŠ¡ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åƒç´ çº§æ„ŸçŸ¥ç†è§£èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„åˆ†å‰²ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šX-SAMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥çš„è§†è§‰æç¤ºï¼Œç„¶åæ˜¯åŸºäºæç¤ºçš„å®ä¾‹åˆ†å‰²æ¨¡å—ï¼Œæœ€åæ˜¯ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥ï¼Œæ”¯æŒè·¨å¤šä¸ªæ•°æ®é›†çš„å…±åŒè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šX-SAMçš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†è§†è§‰åŸºç¡€åˆ†å‰²ä»»åŠ¡ï¼Œå…è®¸é€šè¿‡äº¤äº’å¼è§†è§‰æç¤ºè¿›è¡Œå®ä¾‹å¯¹è±¡çš„åˆ†å‰²ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç»Ÿä¸€çš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„è®­ç»ƒï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†å¤šæ¨¡æ€è¾“å…¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„åƒç´ çº§ç†è§£èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œæ¶æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼ŒX-SAMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰åŸºçº¿æå‡äº†è¶…è¿‡10%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€åƒç´ çº§è§†è§‰ç†è§£ä¸­çš„é«˜æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

X-SAMçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æå’Œæ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡æå‡å›¾åƒåˆ†å‰²çš„ç²¾åº¦å’Œçµæ´»æ€§ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä¸ºå¤æ‚åœºæ™¯ä¸‹çš„è§†è§‰ç†è§£æä¾›æ›´å¼ºå¤§çš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.

