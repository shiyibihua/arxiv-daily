---
layout: default
title: Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models
---

# Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04059" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04059v1</a>
  <a href="https://arxiv.org/pdf/2508.04059.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04059v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04059v1', 'Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaochen Liu, Kaiwen Gao, Shuyi Liang, Bin Xiao, Limeng Qiao, Lin Ma, Tingting Jiang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºO-Benchä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é®æŒ¡æ„ŸçŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é®æŒ¡æ„ŸçŸ¥` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰é—®ç­”` `æ•°æ®é›†æ„å»º` `æ€§èƒ½è¯„ä¼°` `ç©ºé—´ç†è§£` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é®æŒ¡æ„ŸçŸ¥æ–¹é¢çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚
2. æœ¬æ–‡æå‡ºO-BenchåŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°é®æŒ¡æ„ŸçŸ¥ï¼Œæ„å»ºäº†åŒ…å«1,365å¹…å›¾åƒå’Œ4,588ä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ã€‚
3. å¯¹22ä¸ªMLLMsçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨é®æŒ¡æ„ŸçŸ¥ä»»åŠ¡ä¸Šæ— æ³•ä¸äººç±»æ°´å¹³ç›¸åª²ç¾ï¼Œä¸”å­˜åœ¨å¤šç§å¤±æ•ˆæ¨¡å¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é®æŒ¡æ„ŸçŸ¥æ˜¯äººç±»ç©ºé—´ç†è§£çš„é‡è¦åŸºç¡€ï¼Œæ¶‰åŠè§†è§‰è¯†åˆ«ä¸æ¨ç†çš„æ•´åˆã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†å…¶åœ¨é®æŒ¡æ„ŸçŸ¥ä¸Šçš„è¡¨ç°ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†O-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹é®æŒ¡æ„ŸçŸ¥çš„è§†è§‰é—®ç­”åŸºå‡†ã€‚åŸºäºSA-1Bï¼Œæˆ‘ä»¬é€šè¿‡æ–°é¢–çš„åˆ†å±‚åˆæˆæ–¹æ³•æ„å»ºäº†1,365å¹…å…·æœ‰è¯­ä¹‰ä¸€è‡´çš„é®æŒ¡åœºæ™¯å›¾åƒï¼Œå¹¶æ³¨é‡Šäº†4,588ä¸ªé—®ç­”å¯¹ã€‚å¯¹22ä¸ªä»£è¡¨æ€§MLLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹ä¸äººç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸”æ— æ³•é€šè¿‡æ¨¡å‹æ‰©å±•æˆ–æ€ç»´è¿‡ç¨‹å¼¥è¡¥ã€‚æˆ‘ä»¬è¿˜è¯†åˆ«å‡ºä¸‰ç§å…¸å‹å¤±æ•ˆæ¨¡å¼ï¼ŒåŒ…æ‹¬è¿‡äºä¿å®ˆçš„åè§ã€è„†å¼±çš„æ•´ä½“é¢„æµ‹å’Œå¯¹å®šé‡ä»»åŠ¡çš„å›°éš¾ã€‚æˆ‘ä»¬ç›¸ä¿¡O-Benchä¸ä»…èƒ½ä¸ºé®æŒ¡æ„ŸçŸ¥æä¾›é‡è¦çš„è¯„ä¼°å·¥å…·ï¼Œè¿˜èƒ½æ¿€åŠ±MLLMsåœ¨è§†è§‰æ™ºèƒ½æ–¹é¢çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é®æŒ¡æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é®æŒ¡åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆè§†è§‰ä¿¡æ¯ä¸æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºO-BenchåŸºå‡†ï¼Œé€šè¿‡æ„å»ºå…·æœ‰è¯­ä¹‰ä¸€è‡´æ€§çš„é®æŒ¡åœºæ™¯å›¾åƒï¼Œæ¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°å’Œæå‡MLLMsåœ¨é®æŒ¡æ„ŸçŸ¥æ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šO-BenchåŸºäºSA-1Bæ•°æ®é›†ï¼Œé‡‡ç”¨åˆ†å±‚åˆæˆæ–¹æ³•ç”Ÿæˆå›¾åƒï¼Œå¹¶é€šè¿‡åŠè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹æ³¨é‡Šé—®ç­”å¯¹ï¼Œå½¢æˆå®Œæ•´çš„è¯„ä¼°ä½“ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šO-Benchæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹é®æŒ¡æ„ŸçŸ¥çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°å·¥å…·çš„ç©ºç™½ï¼Œæä¾›äº†ç³»ç»ŸåŒ–çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ„å»ºä¸­ï¼Œé‡‡ç”¨äº†åˆ†å±‚åˆæˆæŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼›é—®ç­”å¯¹çš„æ³¨é‡Šåˆ™é€šè¿‡å¯é çš„åŠè‡ªåŠ¨åŒ–æµç¨‹å®Œæˆï¼Œä¿è¯äº†æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ22ä¸ªä»£è¡¨æ€§MLLMsåœ¨é®æŒ¡æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸äººç±»åŸºçº¿å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œæ— æ³•é€šè¿‡ç®€å•çš„æ¨¡å‹æ‰©å±•æ¥å¼¥è¡¥ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨å¤„ç†é®æŒ¡åœºæ™¯æ—¶è¡¨ç°å‡ºè¿‡äºä¿å®ˆçš„åè§å’Œå¯¹å®šé‡ä»»åŠ¡çš„å›°éš¾ï¼Œæ­ç¤ºäº†å½“å‰æŠ€æœ¯çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰ï¼Œéœ€è¦é«˜æ°´å¹³ç©ºé—´ç†è§£å’Œè§†è§‰æ¨ç†çš„åœºæ™¯ã€‚O-Benchçš„å‘å¸ƒå°†æ¨åŠ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é®æŒ¡æ„ŸçŸ¥æ–¹é¢çš„ç ”ç©¶ï¼Œæå‡å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Occlusion perception, a critical foundation for human-level spatial understanding, embodies the challenge of integrating visual recognition and reasoning. Though multimodal large language models (MLLMs) have demonstrated remarkable capabilities, their performance on occlusion perception remains under-explored. To address this gap, we introduce O-Bench, the first visual question answering (VQA) benchmark specifically designed for occlusion perception. Based on SA-1B, we construct 1,365 images featuring semantically coherent occlusion scenarios through a novel layered synthesis approach. Upon this foundation, we annotate 4,588 question-answer pairs in total across five tailored tasks, employing a reliable, semi-automatic workflow. Our extensive evaluation of 22 representative MLLMs against the human baseline reveals a significant performance gap between current MLLMs and humans, which, we find, cannot be sufficiently bridged by model scaling or thinking process. We further identify three typical failure patterns, including an overly conservative bias, a fragile gestalt prediction, and a struggle with quantitative tasks. We believe O-Bench can not only provide a vital evaluation tool for occlusion perception, but also inspire the development of MLLMs for better visual intelligence. Our benchmark will be made publicly available upon paper publication.

