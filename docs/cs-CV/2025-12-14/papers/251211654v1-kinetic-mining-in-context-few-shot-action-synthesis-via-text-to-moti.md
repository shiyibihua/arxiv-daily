---
layout: default
title: Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation
---

# Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation

**arXiv**: [2512.11654v1](https://arxiv.org/abs/2512.11654) | [PDF](https://arxiv.org/pdf/2512.11654.pdf)

**ä½œè€…**: Luca Cazzola, Ahed Alboody

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKineMICæ¡†æž¶ï¼Œé€šè¿‡æ–‡æœ¬åˆ°è¿åŠ¨è’¸é¦è§£å†³å°‘æ ·æœ¬åŠ¨ä½œåˆæˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `å°‘æ ·æœ¬åŠ¨ä½œåˆæˆ` `æ–‡æœ¬åˆ°è¿åŠ¨è’¸é¦` `éª¨éª¼åŠ¨ä½œè¯†åˆ«` `æ‰©æ•£æ¨¡åž‹å¾®è°ƒ` `æ•°æ®å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé€šç”¨æ–‡æœ¬åˆ°è¿åŠ¨æ¨¡åž‹ç”Ÿæˆçš„åŠ¨ä½œä¸é€‚åˆéª¨éª¼åŠ¨ä½œè¯†åˆ«ï¼Œå­˜åœ¨é¢†åŸŸå·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨CLIPæ–‡æœ¬åµŒå…¥å»ºç«‹è¯­ä¹‰å¯¹åº”ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡åž‹å¾®è°ƒä¸ºåŠ¨ä½œåˆ°è¿åŠ¨ç”Ÿæˆå™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨NTU RGB+D 120å­é›†ä¸Šï¼Œä»…ç”¨æ¯ç±»10æ ·æœ¬ï¼Œæå‡å‡†ç¡®çŽ‡23.1%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).

