---
layout: default
title: Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction
---

# Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction

**arXiv**: [2512.11399v1](https://arxiv.org/abs/2512.11399) | [PDF](https://arxiv.org/pdf/2512.11399.pdf)

**ä½œè€…**: Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå…³é”®ç‰‡æ®µæå–çš„é•¿è§†é¢‘å¤šæ¨¡æ€æ‘˜è¦æ–¹æ³•ï¼Œä»¥ä½Žæˆæœ¬æ•èŽ·é‡è¦è§†è§‰ä¿¡æ¯ã€‚**

**å…³é”®è¯**: `é•¿è§†é¢‘æ‘˜è¦` `å…³é”®ç‰‡æ®µæå–` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€æ‘˜è¦` `ä½Žæˆæœ¬åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šé•¿è§†é¢‘ä¸­è§†è§‰ä¿¡æ¯æ˜“ä¸¢å¤±ï¼Œéœ€ä½Žæˆæœ¬åˆ†æžå·¥å…·ã€‚
2. æ–¹æ³•ï¼šä½¿ç”¨è½»é‡è§†é¢‘æè¿°æ¨¡åž‹ç”Ÿæˆç‰‡æ®µæè¿°ï¼ŒLLMé€‰æ‹©å…³é”®ç‰‡æ®µã€‚
3. æ•ˆæžœï¼šåœ¨MovieSumæ•°æ®é›†ä¸ŠæŽ¥è¿‘å‚è€ƒç‰‡æ®µæ€§èƒ½ï¼Œè®¡ç®—æˆæœ¬ä½Žã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.

