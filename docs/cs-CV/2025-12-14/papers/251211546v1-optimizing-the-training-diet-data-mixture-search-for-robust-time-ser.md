---
layout: default
title: Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting
---

# Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting

**arXiv**: [2512.11546v1](https://arxiv.org/abs/2512.11546) | [PDF](https://arxiv.org/pdf/2512.11546.pdf)

**ä½œè€…**: Federico Pennino, Maurizio Gabbrielli

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®æ··åˆæœç´¢æ¡†æž¶ä»¥ä¼˜åŒ–æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡åž‹çš„è®­ç»ƒæ•°æ®ç»„æˆ**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `æ•°æ®é€‰æ‹©ä¼˜åŒ–` `èšç±»åˆ†æž` `Optunaä¼˜åŒ–` `ä¼ æ„Ÿå™¨æ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ ‡å‡†è®­ç»ƒèŒƒå¼å‡è®¾æ•°æ®è¶Šå¤šè¶Šå¥½ï¼Œä½†ä¼ æ„Ÿå™¨æ•°æ®å¸¸ä¸å¹³è¡¡ä¸”å†—ä½™ï¼Œå½±å“æ¨¡åž‹æ³›åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨ç¼–ç å™¨å’Œèšç±»åˆ’åˆ†æ•°æ®ä¸ºè¡Œä¸ºä¸€è‡´ç°‡ï¼Œé€šè¿‡Optunaä¼˜åŒ–æœç´¢æœ€ä½³æ•°æ®æ··åˆæ¯”ä¾‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PMSMæ•°æ®é›†ä¸Šï¼ŒMSEä»Ž1.70æå‡è‡³1.37ï¼Œæ€§èƒ½æ”¹å–„19.41%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.

