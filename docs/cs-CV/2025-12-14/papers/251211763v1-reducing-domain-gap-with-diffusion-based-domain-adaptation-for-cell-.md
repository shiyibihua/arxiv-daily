---
layout: default
title: Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting
---

# Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting

**arXiv**: [2512.11763v1](https://arxiv.org/abs/2512.11763) | [PDF](https://arxiv.org/pdf/2512.11763.pdf)

**ä½œè€…**: Mohammad Dehghanmanshadi, Wallapak Tavanapong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„InSTé£Žæ ¼è¿ç§»æ–¹æ³•ï¼Œä»¥é™ä½Žç»†èƒžè®¡æ•°ä¸­åˆæˆä¸ŽçœŸå®žæ˜¾å¾®å›¾åƒçš„åŸŸå·®è·ã€‚**

**å…³é”®è¯**: `åŸŸé€‚åº”` `æ‰©æ•£æ¨¡åž‹` `é£Žæ ¼è¿ç§»` `ç»†èƒžè®¡æ•°` `æ˜¾å¾®å›¾åƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»ŸåŸŸé€‚åº”æ–¹æ³•éš¾ä»¥å¤„ç†åˆæˆæ˜¾å¾®å›¾åƒç¼ºä¹çœŸå®žçº¹ç†å’Œè§†è§‰æ¨¡å¼çš„é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ½œåœ¨ç©ºé—´è‡ªé€‚åº”å®žä¾‹å½’ä¸€åŒ–å’Œæ‰©æ•£æ¨¡åž‹ä¸­çš„éšæœºåè½¬ï¼Œå°†çœŸå®žè§å…‰æ˜¾å¾®å›¾åƒé£Žæ ¼è¿ç§»è‡³åˆæˆå›¾åƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç»†èƒžè®¡æ•°ä»»åŠ¡ä¸­ï¼ŒInSTåˆæˆæ•°æ®è®­ç»ƒæ¨¡åž‹æ¯”ç¡¬ç¼–ç åˆæˆæ•°æ®é™ä½Ž37% MAEï¼Œæ¯”Cell200-sé™ä½Ž52% MAEã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.
>   We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.

