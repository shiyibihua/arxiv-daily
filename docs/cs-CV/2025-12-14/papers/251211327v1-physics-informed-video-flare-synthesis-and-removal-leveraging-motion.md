---
layout: default
title: Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene
---

# Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene

**arXiv**: [2512.11327v1](https://arxiv.org/abs/2512.11327) | [PDF](https://arxiv.org/pdf/2512.11327.pdf)

**ä½œè€…**: Junqiao Wang, Yuanfei Huang, Hua Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç‰©ç†å¼•å¯¼çš„è§†é¢‘å…‰æ™•åˆæˆä¸ŽåŽ»é™¤æ–¹æ³•ï¼Œåˆ©ç”¨å…‰æ™•ä¸Žåœºæ™¯è¿åŠ¨ç‹¬ç«‹æ€§æå‡è§†é¢‘æ¢å¤æ€§èƒ½ã€‚**

**å…³é”®è¯**: `è§†é¢‘å…‰æ™•åŽ»é™¤` `ç‰©ç†å¼•å¯¼åˆæˆ` `æ—¶ç©ºå»ºæ¨¡` `æ³¨æ„åŠ›æœºåˆ¶` `Mambaç½‘ç»œ` `å…‰æ™•æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†é¢‘å…‰æ™•åŽ»é™¤å› å…‰æ™•ã€å…‰æºå’Œåœºæ™¯è¿åŠ¨ç‹¬ç«‹è€Œå¤æ‚ï¼Œå¯¼è‡´é—ªçƒå’Œä¼ªå½±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ç‰©ç†å¼•å¯¼çš„åŠ¨æ€å…‰æ™•åˆæˆæµç¨‹å’ŒåŸºäºŽæ³¨æ„åŠ›ä¸ŽMambaçš„è§†é¢‘åŽ»é™¤ç½‘ç»œï¼Œæ— éœ€å¤šå¸§å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºé¦–ä¸ªè§†é¢‘å…‰æ™•æ•°æ®é›†ï¼Œå®žéªŒæ˜¾ç¤ºåœ¨çœŸå®žå’Œåˆæˆè§†é¢‘ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä¿æŒæ—¶ç©ºä¸€è‡´æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.

