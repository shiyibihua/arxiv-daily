---
layout: default
title: Fully Inductive Node Representation Learning via Graph View Transformation
---

# Fully Inductive Node Representation Learning via Graph View Transformation

**arXiv**: [2512.11561v1](https://arxiv.org/abs/2512.11561) | [PDF](https://arxiv.org/pdf/2512.11561.pdf)

**ä½œè€…**: Dooho Lee, Myeong Kong, Minho Jeong, Jaemin Yoo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾è§†å›¾å˜æ¢ä»¥å®žçŽ°è·¨æ•°æ®é›†çš„å®Œå…¨å½’çº³èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ **

**å…³é”®è¯**: `å›¾ç¥žç»ç½‘ç»œ` `å®Œå…¨å½’çº³å­¦ä¹ ` `èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ ` `è§†å›¾ç©ºé—´` `è·¨æ•°æ®é›†æ³›åŒ–` `å›¾è§†å›¾å˜æ¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾æ•°æ®ç‰¹å¾ç©ºé—´å·®å¼‚å¤§ï¼Œé˜»ç¢é¢„è®­ç»ƒæ¨¡åž‹è·¨æ•°æ®é›†å½’çº³æŽ¨ç†
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†å›¾ç©ºé—´ï¼Œè®¾è®¡èŠ‚ç‚¹å’Œç‰¹å¾ç½®æ¢ç­‰å˜çš„å›¾è§†å›¾å˜æ¢ä½œä¸ºæž„å»ºå—
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨27ä¸ªèŠ‚ç‚¹åˆ†ç±»åŸºå‡†ä¸Šï¼Œè¶…è¶ŠçŽ°æœ‰å®Œå…¨å½’çº³æ¨¡åž‹å’Œä¸ªä½“è°ƒä¼˜GNN

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

