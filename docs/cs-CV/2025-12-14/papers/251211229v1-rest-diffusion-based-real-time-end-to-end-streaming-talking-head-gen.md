---
layout: default
title: REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation
---

# REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation

**arXiv**: [2512.11229v1](https://arxiv.org/abs/2512.11229) | [PDF](https://arxiv.org/pdf/2512.11229.pdf)

**ä½œè€…**: Haotian Wang, Yuzhe Weng, Xinyi Yu, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Qingfeng Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRESTæ¡†æž¶ï¼Œé€šè¿‡ID-Contextç¼“å­˜å’Œå¼‚æ­¥æµè’¸é¦å®žçŽ°åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„å®žæ—¶ç«¯åˆ°ç«¯æµå¼è¯´è¯å¤´ç”Ÿæˆã€‚**

**å…³é”®è¯**: `è¯´è¯å¤´ç”Ÿæˆ` `æ‰©æ•£æ¨¡åž‹` `å®žæ—¶æµå¼ç”Ÿæˆ` `ID-Contextç¼“å­˜` `å¼‚æ­¥è’¸é¦è®­ç»ƒ` `è§†é¢‘æ½œåœ¨ç©ºé—´åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡åž‹åœ¨è¯´è¯å¤´ç”Ÿæˆä¸­æŽ¨ç†æ…¢ä¸”éžè‡ªå›žå½’ï¼Œé™åˆ¶å®žæ—¶åº”ç”¨ã€‚
2. å¼•å…¥ID-Contextç¼“å­˜æœºåˆ¶å’Œå¼‚æ­¥æµè’¸é¦è®­ç»ƒç­–ç•¥ï¼Œæå‡æ—¶åºä¸€è‡´æ€§å’Œèº«ä»½è¿žè´¯æ€§ã€‚
3. å®žéªŒæ˜¾ç¤ºRESTåœ¨ç”Ÿæˆé€Ÿåº¦å’Œæ•´ä½“æ€§èƒ½ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ”¯æŒå®žæ—¶æµå¼ç”Ÿæˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.

