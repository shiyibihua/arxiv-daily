---
layout: default
title: BAID: A Benchmark for Bias Assessment of AI Detectors
---

# BAID: A Benchmark for Bias Assessment of AI Detectors

**arXiv**: [2512.11505v1](https://arxiv.org/abs/2512.11505) | [PDF](https://arxiv.org/pdf/2512.11505.pdf)

**ä½œè€…**: Priyam Basu, Yunfeng Zhang, Vipul Raheja

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBAIDåŸºå‡†ä»¥ç³»ç»Ÿè¯„ä¼°AIæ–‡æœ¬æ£€æµ‹å™¨åœ¨å¹¿æ³›ç¤¾ä¼šè¯­è¨€å› ç´ ä¸­çš„åè§é—®é¢˜ã€‚**

**å…³é”®è¯**: `AIæ–‡æœ¬æ£€æµ‹å™¨` `åè§è¯„ä¼°` `ç¤¾ä¼šè¯­è¨€å› ç´ ` `åŸºå‡†æµ‹è¯•` `åˆæˆæ–‡æœ¬ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIæ–‡æœ¬æ£€æµ‹å™¨ç¼ºä¹å¯¹äººå£ç»Ÿè®¡ã€æ–¹è¨€ç­‰ç¤¾ä¼šè¯­è¨€åè§çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«20ä¸‡æ ·æœ¬çš„æ¡†æž¶ï¼Œè¦†ç›–7ç±»åè§ï¼Œå¹¶ç”Ÿæˆä¿ç•™å†…å®¹ä½†åæ˜ å­ç¾¤å†™ä½œé£Žæ ¼çš„åˆæˆæ–‡æœ¬ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°å››ä¸ªå¼€æºæ£€æµ‹å™¨ï¼Œå‘çŽ°å¯¹å°‘æ•°ç¾¤ä½“æ–‡æœ¬çš„å¬å›žçŽ‡ä½Žï¼Œå¼ºè°ƒéƒ¨ç½²å‰éœ€åè§æ„ŸçŸ¥è¯„ä¼°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

