---
layout: default
title: Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction
---

# Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction

**arXiv**: [2512.11547v1](https://arxiv.org/abs/2512.11547) | [PDF](https://arxiv.org/pdf/2512.11547.pdf)

**ä½œè€…**: Janaina MourÃ£o-Miranda, Zakria Hussain, Konstantinos Tsirlis, Christophe Phillips, John Shawe-Taylor

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼¹æ€§ç½‘å¤šæ ¸å­¦ä¹ ä»¥ç»“åˆç›¸å…³æ ¸è¿›è¡Œç¨€ç–å¯è§£é‡Šé¢„æµ‹ï¼Œåº”ç”¨äºŽç¥žç»å½±åƒå­¦ã€‚**

**å…³é”®è¯**: `å¤šæ ¸å­¦ä¹ ` `å¼¹æ€§ç½‘æ­£åˆ™åŒ–` `ç¨€ç–æ¨¡åž‹` `ç¥žç»å½±åƒå­¦` `æ”¯æŒå‘é‡æœº` `æ ¸å²­å›žå½’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ ¸å­¦ä¹ æ•´åˆå¤šæ•°æ®æºï¼Œå¼¹æ€§ç½‘æ­£åˆ™åŒ–ä¿ƒè¿›ç¨€ç–æ€§å’Œç›¸å…³æ ¸é€‰æ‹©ã€‚
2. æ–°æ–¹æ³•æä¾›æ ¸æƒé‡çš„è§£æžæ›´æ–°ï¼Œæ”¯æŒSVMå’Œæ ¸å²­å›žå½’ç®—æ³•å®žçŽ°ã€‚
3. åœ¨ç¥žç»å½±åƒåº”ç”¨ä¸­ï¼Œæ€§èƒ½ä¼˜äºŽæˆ–åŒ¹é…l1æ­£åˆ™åŒ–ï¼Œæ¨¡åž‹æ›´ç¨€ç–å¯è§£é‡Šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.

