---
layout: default
title: Bridging Streaming Continual Learning via In-Context Large Tabular Models
---

# Bridging Streaming Continual Learning via In-Context Large Tabular Models

**arXiv**: [2512.11668v1](https://arxiv.org/abs/2512.11668) | [PDF](https://arxiv.org/pdf/2512.11668.pdf)

**ä½œè€…**: Afonso LourenÃ§o, JoÃ£o Gama, Eric P. Xing, Goreti Marreiros

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤§ä¸Šä¸‹æ–‡è¡¨æ ¼æ¨¡åž‹çš„æµå¼æŒç»­å­¦ä¹ æ¡†æž¶ï¼Œä»¥æ¡¥æŽ¥æµå­¦ä¹ ä¸ŽæŒç»­å­¦ä¹ **

**å…³é”®è¯**: `æµå¼æŒç»­å­¦ä¹ ` `å¤§ä¸Šä¸‹æ–‡è¡¨æ ¼æ¨¡åž‹` `æ¦‚å¿µæ¼‚ç§»` `ç¾éš¾æ€§é—å¿˜` `æ•°æ®åŽ‹ç¼©` `ç»éªŒå›žæ”¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæµå¼åœºæ™¯ä¸­æ¨¡åž‹éœ€è¿žç»­å­¦ä¹ ï¼Œä½†çŽ°æœ‰ç ”ç©¶å­¤ç«‹å¤„ç†æµå­¦ä¹ ä¸ŽæŒç»­å­¦ä¹ ï¼Œç¼ºä¹ç®—æ³•é‡å 
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤§ä¸Šä¸‹æ–‡è¡¨æ ¼æ¨¡åž‹ï¼Œå°†æ— ç•Œæµæ•°æ®å®žæ—¶åŽ‹ç¼©ä¸ºç´§å‡‘æ‘˜è¦ï¼Œå¹³è¡¡å¯å¡‘æ€§ä¸Žç¨³å®šæ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡åˆ†å¸ƒåŒ¹é…å’ŒåŽ‹ç¼©åŽŸåˆ™ï¼Œå®žçŽ°æ•°æ®é€‰æ‹©ï¼ŒæŽ§åˆ¶å†…å­˜å¤§å°å¹¶é¿å…å†—ä½™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

