---
layout: default
title: VFMF: World Modeling by Forecasting Vision Foundation Model Features
---

# VFMF: World Modeling by Forecasting Vision Foundation Model Features

**arXiv**: [2512.11225v1](https://arxiv.org/abs/2512.11225) | [PDF](https://arxiv.org/pdf/2512.11225.pdf)

**ä½œè€…**: Gabrijel Boduljak, Yushi Lan, Christian Rupprecht, Andrea Vedaldi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVFMFæ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹è§†è§‰åŸºç¡€æ¨¡åž‹ç‰¹å¾è¿›è¡Œä¸–ç•Œå»ºæ¨¡ï¼Œä»¥è§£å†³ç¡®å®šæ€§å›žå½’ä¸­ä¸ç¡®å®šæ€§æ•èŽ·ä¸è¶³çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `ä¸–ç•Œå»ºæ¨¡` `ç‰¹å¾é¢„æµ‹` `ç”Ÿæˆå¼æ¨¡åž‹` `è§†è§‰åŸºç¡€æ¨¡åž‹` `ä¸ç¡®å®šæ€§æ•èŽ·` `å¤šæ¨¡æ€è¾“å‡º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¡®å®šæ€§å›žå½’åœ¨é¢„æµ‹è§†è§‰åŸºç¡€æ¨¡åž‹ç‰¹å¾æ—¶å¹³å‡åŒ–å¤šç§å¯èƒ½æœªæ¥ï¼Œå¯¼è‡´ä¸ç¡®å®šæ€§æ•èŽ·ä¸è¶³ï¼Œå½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è‡ªå›žå½’æµåŒ¹é…åœ¨è§†è§‰åŸºç¡€æ¨¡åž‹ç‰¹å¾ç©ºé—´è¿›è¡Œç”Ÿæˆå¼é¢„æµ‹ï¼Œå°†ç‰¹å¾ç¼–ç åˆ°ç´§å‡‘æ½œåœ¨ç©ºé—´ä»¥æ”¯æŒæ‰©æ•£æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŒ¹é…æž¶æž„å’Œè®¡ç®—ä¸‹ï¼Œç›¸æ¯”å›žå½’æ–¹æ³•ï¼Œåœ¨æ‰€æœ‰è¾“å‡ºæ¨¡æ€ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ï¼‰ä¸Šäº§ç”Ÿæ›´æ¸…æ™°å’Œå‡†ç¡®çš„é¢„æµ‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.

