---
layout: default
title: JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion
---

# JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion

**arXiv**: [2512.11423v1](https://arxiv.org/abs/2512.11423) | [PDF](https://arxiv.org/pdf/2512.11423.pdf)

**ä½œè€…**: Chaochao Li, Ruikui Wang, Liangbo Zhou, Jinheng Feng, Huaishao Luo, Huan Zhang, Youzheng Wu, Xiaodong He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJoyAvatarä»¥è§£å†³éŸ³é¢‘é©±åŠ¨è™šæ‹Ÿäººç”Ÿæˆä¸­çš„å®žæ—¶æ€§å’Œé•¿è§†é¢‘åˆæˆé—®é¢˜**

**å…³é”®è¯**: `éŸ³é¢‘é©±åŠ¨è™šæ‹Ÿäººç”Ÿæˆ` `è‡ªå›žå½’æ‰©æ•£æ¨¡åž‹` `å®žæ—¶æŽ¨ç†` `é•¿è§†é¢‘åˆæˆ` `æ—¶é—´ä¸€è‡´æ€§` `å”‡åŒæ­¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰DiTæ–¹æ³•è®¡ç®—å¼€é”€é«˜ä¸”æ— æ³•ç”Ÿæˆé•¿è§†é¢‘ï¼Œè‡ªå›žå½’æ–¹æ³•å­˜åœ¨è¯¯å·®ç´¯ç§¯å’Œè´¨é‡ä¸‹é™é—®é¢˜
2. å¼•å…¥æ¸è¿›æ­¥å¼•å¯¼ã€è¿åŠ¨æ¡ä»¶æ³¨å…¥å’Œç¼“å­˜é‡ç½®æ— ç•ŒRoPEï¼Œæå‡ç¨³å®šæ€§å’Œæ—¶é—´ä¸€è‡´æ€§
3. 1.3Bå‚æ•°æ¨¡åž‹åœ¨å•GPUä¸Šå®žçŽ°16 FPSå®žæ—¶æŽ¨ç†ï¼Œè§†è§‰è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œå”‡åŒæ­¥æ•ˆæžœç«žäº‰æ€§å¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.

