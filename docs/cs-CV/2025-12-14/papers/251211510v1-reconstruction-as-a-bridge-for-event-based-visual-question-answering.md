---
layout: default
title: Reconstruction as a Bridge for Event-Based Visual Question Answering
---

# Reconstruction as a Bridge for Event-Based Visual Question Answering

**arXiv**: [2512.11510v1](https://arxiv.org/abs/2512.11510) | [PDF](https://arxiv.org/pdf/2512.11510.pdf)

**ä½œè€…**: Hanyue Lou, Jiayi Zhou, Yang Zhang, Boyu Li, Yi Wang, Guangnan Ye, Boxin Shi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé‡å»ºçš„æ–¹æ³•ä»¥è§£å†³äº‹ä»¶ç›¸æœºä¸Žå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹èžåˆä¸­çš„æƒè¡¡é—®é¢˜**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰é—®ç­”` `é‡å»ºæ–¹æ³•` `ç¨€ç–æ€§åˆ©ç”¨` `åŸºå‡†è¯„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäº‹ä»¶ç›¸æœºä¸Žå¸§åŸºæ¨¡åž‹èžåˆéœ€å¹³è¡¡äº‹ä»¶æ•°æ®ä¼˜åŠ¿ä¸Žå…¼å®¹æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡FRTå’ŒARTæ–¹æ³•ï¼Œåˆ©ç”¨é‡å»ºä½œä¸ºæ¡¥æ¢ï¼ŒARTåˆ©ç”¨äº‹ä»¶ç¨€ç–æ€§æå‡æ•ˆçŽ‡
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨EvQAåŸºå‡†ä¸Šå®žçŽ°æœ€ä¼˜æ€§èƒ½ï¼ŒéªŒè¯MLLMsåœ¨äº‹ä»¶è§†è§‰ä¸­çš„æ½œåŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.

