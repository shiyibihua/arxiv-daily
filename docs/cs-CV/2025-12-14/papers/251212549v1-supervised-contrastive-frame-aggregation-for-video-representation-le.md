---
layout: default
title: Supervised Contrastive Frame Aggregation for Video Representation Learning
---

# Supervised Contrastive Frame Aggregation for Video Representation Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.12549" target="_blank" class="toolbar-btn">arXiv: 2512.12549v1</a>
    <a href="https://arxiv.org/pdf/2512.12549.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.12549v1" 
            onclick="toggleFavorite(this, '2512.12549v1', 'Supervised Contrastive Frame Aggregation for Video Representation Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shaif Chowdhury, Mushfika Rahman, Greg Hamerly

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-14

**Â§áÊ≥®**: 12 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÁõëÁù£ÂØπÊØîÂ∏ßËÅöÂêàÊñπÊ≥ïÔºåÁî®‰∫éÈ´òÊïàËßÜÈ¢ëË°®ÂæÅÂ≠¶‰π†„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëË°®ÂæÅÂ≠¶‰π†` `ÁõëÁù£ÂØπÊØîÂ≠¶‰π†` `Â∏ßËÅöÂêà` `Êó∂Èó¥‰∏ä‰∏ãÊñá` `Âç∑ÁßØÁ•ûÁªèÁΩëÁªú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëË°®ÂæÅÂ≠¶‰π†ÊñπÊ≥ïËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈöæ‰ª•ÊúâÊïàÂà©Áî®Êó∂Â∫èÂÖ®Â±Ä‰ø°ÊÅØ„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçËßÜÈ¢ëÂ∏ßËÅöÂêàÁ≠ñÁï•ÔºåÂ∞ÜÂ§öÂ∏ßÂõæÂÉèÁªÑÂêàÊàêÂçïÂº†ÂõæÂÉèÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉCNNÊèêÂèñÁâπÂæÅ„ÄÇ
3. ËÆæËÆ°ÁõëÁù£ÂØπÊØîÂ≠¶‰π†ÁõÆÊ†áÔºåÈÄöËøá‰∏çÂêåÊó∂Èó¥ÈááÊ†∑ÊûÑÂª∫Ê≠£Ê†∑Êú¨ÔºåÊèêÂçáÂàÜÁ±ªÁ≤æÂ∫¶Âπ∂ÂáèÂ∞ëËøáÊãüÂêà„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éËßÜÈ¢ëË°®ÂæÅÂ≠¶‰π†ÁöÑÁõëÁù£ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âà©Áî®‰∫ÜÊó∂Èó¥‰∏äÁöÑÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçËßÜÈ¢ëÂà∞ÂõæÂÉèÁöÑËÅöÂêàÁ≠ñÁï•ÔºåÂ∞ÜÊØè‰∏™ËßÜÈ¢ëÁöÑÂ§ö‰∏™Â∏ßÂú®Á©∫Èó¥‰∏äÊéíÂàóÊàêÂçï‰∏™ËæìÂÖ•ÂõæÂÉè„ÄÇËøôÁßçËÆæËÆ°ËÉΩÂ§ü‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÈ™®Âπ≤ÁΩëÁªúÔºàÂ¶ÇResNet50ÔºâÔºåÂπ∂ÈÅøÂÖç‰∫ÜÂ§çÊùÇËßÜÈ¢ëTransformerÊ®°ÂûãÂ∏¶Êù•ÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÂØπÊØîÂ≠¶‰π†ÁõÆÊ†áÔºåÁõ¥Êé•ÊØîËæÉÊ®°ÂûãÁîüÊàêÁöÑÊàêÂØπÊäïÂΩ±„ÄÇÊ≠£Ê†∑Êú¨ÂØπË¢´ÂÆö‰πâ‰∏∫Êù•Ëá™ÂÖ±‰∫´Áõ∏ÂêåÊ†áÁ≠æÁöÑËßÜÈ¢ëÁöÑÊäïÂΩ±ÔºåËÄåÊâÄÊúâÂÖ∂‰ªñÊäïÂΩ±ÈÉΩË¢´ËßÜ‰∏∫Ë¥üÊ†∑Êú¨„ÄÇÈÄöËøá‰ªéÂêå‰∏ÄÂ∫ïÂ±ÇËßÜÈ¢ëËøõË°å‰∏çÂêåÁöÑÊó∂Èó¥Â∏ßÈááÊ†∑ÔºåÂàõÂª∫Âêå‰∏ÄËßÜÈ¢ëÁöÑÂ§ö‰∏™Ëá™ÁÑ∂ËßÜÂõæ„ÄÇËøô‰∫õÂ∏ßÁ∫ßÂà´ÁöÑÂèòÂåñ‰∫ßÁîüÂÖ∑ÊúâÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁöÑÂ§öÊ†∑ÂåñÊ≠£Ê†∑Êú¨ÔºåÂπ∂ÂáèÂ∞ëËøáÊãüÂêàÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫éÊï∞ÊçÆÂ¢ûÂº∫„ÄÇÂú®Penn ActionÂíåHMDB51Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂàÜÁ±ªÁ≤æÂ∫¶ÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂ÈúÄË¶ÅÁöÑËÆ°ÁÆóËµÑÊ∫êÊõ¥Â∞ë„ÄÇÊâÄÊèêÂá∫ÁöÑÁõëÁù£ÂØπÊØîÂ∏ßËÅöÂêàÊñπÊ≥ïÂú®ÁõëÁù£ÂíåËá™ÁõëÁù£ËÆæÁΩÆ‰∏≠ÈÉΩËÉΩÂ≠¶‰π†ÊúâÊïàÁöÑËßÜÈ¢ëË°®ÂæÅÔºåÂπ∂ÊîØÊåÅÂü∫‰∫éËßÜÈ¢ëÁöÑ‰ªªÂä°ÔºåÂ¶ÇÂàÜÁ±ªÂíåÂ≠óÂπïÁîüÊàê„ÄÇËØ•ÊñπÊ≥ïÂú®Penn Action‰∏äÂÆûÁé∞‰∫Ü76%ÁöÑÂàÜÁ±ªÁ≤æÂ∫¶ÔºåËÄåViViTÁöÑÁ≤æÂ∫¶‰∏∫43%ÔºåÂú®HMDB51‰∏äÂÆûÁé∞‰∫Ü48%ÁöÑÁ≤æÂ∫¶ÔºåËÄåViViTÁöÑÁ≤æÂ∫¶‰∏∫37%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜÈ¢ëË°®ÂæÅÂ≠¶‰π†ÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éTransformerÁöÑÊ®°ÂûãÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÂπ∂‰∏îÂú®ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÂÖ®Â±ÄÊó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÊñπÈù¢Â≠òÂú®ÊåëÊàò„ÄÇÊ≠§Â§ñÔºåËøáÂ∫¶‰æùËµñÊï∞ÊçÆÂ¢ûÂº∫Êù•ÁîüÊàêÊ≠£Ê†∑Êú¨ÂèØËÉΩÂØºËá¥Ê®°ÂûãÊ≥õÂåñËÉΩÂäõ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÜÈ¢ëÂ∏ßËÅöÂêà‰∏∫Âçï‰∏™ÂõæÂÉèÔºå‰ªéËÄåËÉΩÂ§üÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑCNNÊ®°ÂûãÊèêÂèñÁâπÂæÅÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇÂêåÊó∂ÔºåÈÄöËøáÁõëÁù£ÂØπÊØîÂ≠¶‰π†ÔºåÂ∞ÜÂêå‰∏ÄËßÜÈ¢ëÁöÑ‰∏çÂêåÊó∂Èó¥ÈááÊ†∑‰Ωú‰∏∫Ê≠£Ê†∑Êú¨ÔºåÈºìÂä±Ê®°ÂûãÂ≠¶‰π†ÂÖ∑ÊúâÂÖ®Â±ÄÊó∂Èó¥‰∏ä‰∏ãÊñáÁöÑËßÜÈ¢ëË°®ÂæÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ËßÜÈ¢ëÂ∏ßËÅöÂêàÔºöÂ∞ÜËßÜÈ¢ë‰∏≠ÁöÑÂ§ö‰∏™Â∏ßÊåâÁÖß‰∏ÄÂÆöÁöÑËßÑÂàôÊéíÂàóÊàê‰∏ÄÂº†ÂõæÂÉè„ÄÇ2) ÁâπÂæÅÊèêÂèñÔºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑCNNÔºàÂ¶ÇResNet50ÔºâÊèêÂèñËÅöÂêàÂõæÂÉèÁöÑÁâπÂæÅ„ÄÇ3) ÊäïÂΩ±ÔºöÂ∞ÜÊèêÂèñÁöÑÁâπÂæÅÊäïÂΩ±Âà∞‰ΩéÁª¥Á©∫Èó¥„ÄÇ4) ÂØπÊØîÂ≠¶‰π†Ôºö‰ΩøÁî®ÁõëÁù£ÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÔºåÂ∞ÜÂêå‰∏ÄËßÜÈ¢ëÁöÑ‰∏çÂêåÊó∂Èó¥ÈááÊ†∑‰Ωú‰∏∫Ê≠£Ê†∑Êú¨Ôºå‰∏çÂêåËßÜÈ¢ëÁöÑÈááÊ†∑‰Ωú‰∏∫Ë¥üÊ†∑Êú¨ÔºåËÆ≠ÁªÉÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜËßÜÈ¢ëÂ∏ßËÅöÂêàÁ≠ñÁï•ÔºåÊúâÊïàÂà©Áî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑCNNÊ®°ÂûãÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ2) ‰ΩøÁî®ÁõëÁù£ÂØπÊØîÂ≠¶‰π†ÔºåÈÄöËøá‰∏çÂêåÁöÑÊó∂Èó¥ÈááÊ†∑ÊûÑÂª∫Ê≠£Ê†∑Êú¨ÔºåÈÅøÂÖç‰∫ÜËøáÂ∫¶‰æùËµñÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ3) Â∞ÜÊó∂Èó¥ÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰ø°ÊÅØËûçÂÖ•Âà∞ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂‰∏≠ÔºåÊèêÂçá‰∫ÜËßÜÈ¢ëË°®ÂæÅÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â∏ßËÅöÂêàÁ≠ñÁï•ÔºöÈÄâÊã©ÂêàÈÄÇÁöÑÂ∏ßÊï∞ÂíåÊéíÂàóÊñπÂºèÔºå‰ª•‰øùÁïôÂ∞ΩÂèØËÉΩÂ§öÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇ2) ÂØπÊØîÊçüÂ§±ÂáΩÊï∞Ôºö‰ΩøÁî®ÁõëÁù£ÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±Ê®°ÂûãÂ≠¶‰π†Âå∫ÂàÜ‰∏çÂêåÁ±ªÂà´ÁöÑËßÜÈ¢ëÔºåÂπ∂‰ΩøÂêå‰∏ÄËßÜÈ¢ëÁöÑ‰∏çÂêåÊó∂Èó¥ÈááÊ†∑Â∞ΩÂèØËÉΩÊé•Ëøë„ÄÇ3) Êó∂Èó¥ÈááÊ†∑Á≠ñÁï•ÔºöÈááÁî®‰∏çÂêåÁöÑÊó∂Èó¥ÈááÊ†∑ÊñπÂºèÔºåÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊ≠£Ê†∑Êú¨„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®Penn ActionÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü76%ÁöÑÂàÜÁ±ªÁ≤æÂ∫¶ÔºåÁõ∏ÊØîViViTÁöÑ43%ÊúâÊòæËëóÊèêÂçá„ÄÇÂú®HMDB51Êï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ïÂèñÂæó‰∫Ü48%ÁöÑÂàÜÁ±ªÁ≤æÂ∫¶ÔºåËÄåViViTÁöÑÁ≤æÂ∫¶‰∏∫37%„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂàÜÁ±ªÁ≤æÂ∫¶ÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂ÈúÄË¶ÅÁöÑËÆ°ÁÆóËµÑÊ∫êÊõ¥Â∞ë„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËßÜÈ¢ëÂàÜÁ±ª„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢„ÄÅËßÜÈ¢ëÂ≠óÂπïÁîüÊàêÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂ≠¶‰π†È´òÊïàÁöÑËßÜÈ¢ëË°®ÂæÅÔºåÂèØ‰ª•ÊèêÂçáËøô‰∫õ‰ªªÂä°ÁöÑÊÄßËÉΩÔºåÂπ∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇËØ•ÊñπÊ≥ïÂú®Êô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅËßÜÈ¢ëÂÜÖÂÆπÂàÜÊûêÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®ÁöÑÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

