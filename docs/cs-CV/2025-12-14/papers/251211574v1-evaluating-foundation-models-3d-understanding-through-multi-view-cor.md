---
layout: default
title: Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis
---

# Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis

**arXiv**: [2512.11574v1](https://arxiv.org/abs/2512.11574) | [PDF](https://arxiv.org/pdf/2512.11574.pdf)

**ä½œè€…**: Valentina Lilova, Toyesh Chakravorty, Julian I. Bibo, Emma Boccaletti, Brandon Li, LÃ­via BaxovÃ¡, Cees G. M. Snoek, Mohammadreza Salehi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— éœ€å¾®è°ƒçš„åŸºå‡†ä»¥è¯„ä¼°åŸºç¡€æ¨¡åž‹åœ¨3Då¤šè§†è§’å¯¹åº”ä¸­çš„å†…åœ¨ç†è§£èƒ½åŠ›**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `åŸºç¡€æ¨¡åž‹è¯„ä¼°` `å¤šè§†è§’å¯¹åº”` `æ— éœ€å¾®è°ƒåŸºå‡†` `MVImgNetæ•°æ®é›†` `è§†è§‰ç‰¹å¾è´¨é‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è¯„ä¼°ä¾èµ–ä¸‹æ¸¸å¾®è°ƒï¼Œéš¾ä»¥éš”ç¦»é¢„è®­ç»ƒç¼–ç å™¨çš„3DæŽ¨ç†èƒ½åŠ›
2. åŸºäºŽHummingbirdæ¡†æž¶æ‰©å±•è‡³3Dåœºæ™¯ï¼Œä½¿ç”¨MVImgNetæ•°æ®é›†è¿›è¡Œå¤šè§†è§’åˆ†å‰²åŸºå‡†æµ‹è¯•
3. è¯„ä¼°8ä¸ªæ¨¡åž‹ï¼Œæ˜¾ç¤ºDINOç¼–ç å™¨åœ¨å¤§è§†è§’å˜åŒ–ä¸‹ä¿æŒç«žäº‰åŠ›ï¼Œ3Dæ„ŸçŸ¥æ¨¡åž‹éœ€è°ƒæ•´

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .

