---
layout: default
title: Sliced ReLU attention: Quasi-linear contextual expressivity via sorting
---

# Sliced ReLU attention: Quasi-linear contextual expressivity via sorting

**arXiv**: [2512.11411v1](https://arxiv.org/abs/2512.11411) | [PDF](https://arxiv.org/pdf/2512.11411.pdf)

**ä½œè€…**: Siwan BoufadÃ¨ne, FranÃ§ois-Xavier Vialard

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ‡ç‰‡ReLUæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æŽ’åºå®žçŽ°å‡†çº¿æ€§å¤æ‚åº¦ï¼Œé€‚ç”¨äºŽé•¿ä¸Šä¸‹æ–‡åºåˆ—å¤„ç†ã€‚**

**å…³é”®è¯**: `æ³¨æ„åŠ›æœºåˆ¶` `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `å‡†çº¿æ€§å¤æ‚åº¦` `æŽ’åºç®—æ³•` `å¯å¾®æ ¸` `åºåˆ—è§£è€¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶å¦‚softmaxå’ŒReLUå˜ä½“åœ¨é•¿ä¸Šä¸‹æ–‡åºåˆ—ä¸­è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå½±å“æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé”®-æŸ¥è¯¢å·®å¼‚çš„ä¸€ç»´æŠ•å½±ï¼Œåˆ©ç”¨æŽ’åºæ“ä½œæž„å»ºå¯å¾®éžå¯¹ç§°æ ¸ï¼Œå®žçŽ°O(n log(n))å¤æ‚åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç†è®ºè¯æ˜Žä¿æŒå¼ºè¡¨è¾¾åŠ›ï¼Œæ”¯æŒåºåˆ—è§£è€¦ä»»åŠ¡å’Œä¸Šä¸‹æ–‡é€šç”¨é€¼è¿‘æ€§è´¨ï¼Œå°è§„æ¨¡å®žéªŒå±•ç¤ºå®žç”¨æ½œåŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

