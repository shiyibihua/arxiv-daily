---
layout: default
title: Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization
---

# Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization

**arXiv**: [2512.11391v1](https://arxiv.org/abs/2512.11391) | [PDF](https://arxiv.org/pdf/2512.11391.pdf)

**ä½œè€…**: Yifan Niu, Han Xiao, Dongyi Liu, Nuo Chen, Jia Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé›¶ç©ºé—´çº¦æŸç­–ç•¥ä¼˜åŒ–ä»¥ç¼“è§£å¤§è¯­è¨€æ¨¡åž‹å®‰å…¨å¯¹é½ä¸­çš„èƒ½åŠ›é—å¿˜é—®é¢˜**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `å®‰å…¨å¯¹é½` `å¼ºåŒ–å­¦ä¹ ` `é›¶ç©ºé—´æŠ•å½±` `ç­–ç•¥ä¼˜åŒ–` `å¯¹é½ç¨Ž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåŒ–å­¦ä¹ å®‰å…¨å¯¹é½å¯¼è‡´å¤§è¯­è¨€æ¨¡åž‹é—å¿˜é€šç”¨èƒ½åŠ›ï¼Œå³å¯¹é½ç¨Ž
2. æ–¹æ³•è¦ç‚¹ï¼šå°†å®‰å…¨ç­–ç•¥æ¢¯åº¦å‡ ä½•æŠ•å½±åˆ°é€šç”¨ä»»åŠ¡çš„é›¶ç©ºé—´ï¼Œä»¥ä¿ç•™æ ¸å¿ƒèƒ½åŠ›
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°å­¦ã€ä»£ç ç­‰ä»»åŠ¡ä¸Šå®žçŽ°æœ€ä¼˜å®‰å…¨æ€§èƒ½ï¼Œä¸”æ•°æ®æ•ˆçŽ‡é«˜

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

