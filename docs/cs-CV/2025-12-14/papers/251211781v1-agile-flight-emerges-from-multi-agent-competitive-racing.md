---
layout: default
title: Agile Flight Emerges from Multi-Agent Competitive Racing
---

# Agile Flight Emerges from Multi-Agent Competitive Racing

**arXiv**: [2512.11781v1](https://arxiv.org/abs/2512.11781) | [PDF](https://arxiv.org/pdf/2512.11781.pdf)

**ä½œè€…**: Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ™ºèƒ½ä½“ç«žäº‰å¼ºåŒ–å­¦ä¹ ä»¥è®­ç»ƒæ— äººæœºå®žçŽ°æ•æ·é£žè¡Œä¸Žç­–ç•¥ï¼Œè¶…è¶Šå•æ™ºèƒ½ä½“è®­ç»ƒèŒƒå¼ã€‚**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `æ•æ·é£žè¡Œ` `ä»¿çœŸåˆ°çœŸå®žè¿ç§»` `æ— äººæœºæŽ§åˆ¶` `ç«žäº‰è®­ç»ƒ` `ç¨€ç–å¥–åŠ±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå•æ™ºèƒ½ä½“è®­ç»ƒä¾èµ–è¡Œä¸ºå¥–åŠ±ï¼Œéš¾ä»¥åœ¨å¤æ‚çŽ¯å¢ƒä¸­å®žçŽ°é«˜æ•ˆæ•æ·é£žè¡Œä¸Žç­–ç•¥ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“ç«žäº‰å’Œç¨€ç–é«˜å±‚ç›®æ ‡ï¼ˆèµ¢å¾—æ¯”èµ›ï¼‰ï¼Œè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ— éœ€è¯¦ç»†è¡Œä¸ºå¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žä¸–ç•Œä¸­éªŒè¯ï¼Œå¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨å¤æ‚çŽ¯å¢ƒã€ä»¿çœŸåˆ°çœŸå®žè¿ç§»å’Œæ³›åŒ–æ€§æ–¹é¢è¡¨çŽ°æ›´ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
>   Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent

