---
layout: default
title: YawDD+: Frame-level Annotations for Accurate Yawn Prediction
---

# YawDD+: Frame-level Annotations for Accurate Yawn Prediction

**arXiv**: [2512.11446v1](https://arxiv.org/abs/2512.11446) | [PDF](https://arxiv.org/pdf/2512.11446.pdf)

**ä½œè€…**: Ahmed Mujtaba, Gleb Radchenko, Marc Masana, Radu Prodan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYawDD+æ•°æ®é›†ä»¥è§£å†³é©¾é©¶å‘˜ç–²åŠ³ç›‘æµ‹ä¸­è§†é¢‘çº§æ ‡æ³¨å™ªå£°é—®é¢˜ï¼Œæå‡æ‰“å“ˆæ¬ é¢„æµ‹å‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `é©¾é©¶å‘˜ç–²åŠ³ç›‘æµ‹` `æ‰“å“ˆæ¬ é¢„æµ‹` `å¸§çº§æ ‡æ³¨` `åŠè‡ªåŠ¨æ ‡æ³¨` `è¾¹ç¼˜AIç¡¬ä»¶` `è§†é¢‘åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé©¾é©¶å‘˜ç–²åŠ³å¯¼è‡´äº‹æ•…ï¼ŒçŽ°æœ‰è§†é¢‘çº§æ ‡æ³¨æ•°æ®é›†å¼•å…¥ç³»ç»Ÿå™ªå£°ï¼Œå½±å“æ¨¡åž‹è®­ç»ƒå‡†ç¡®æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘åŠè‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œç»“åˆäººå·¥éªŒè¯ï¼Œç”Ÿæˆå¸§çº§æ ‡æ³¨çš„YawDD+æ•°æ®é›†ï¼Œç”¨äºŽæ”¹è¿›æ¨¡åž‹è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨YawDD+ä¸Šè®­ç»ƒMNasNetå’ŒYOLOv11ï¼Œå¸§å‡†ç¡®çŽ‡æå‡6%ï¼ŒmAPæå‡5%ï¼Œè¾¾åˆ°99.34%åˆ†ç±»å‡†ç¡®çŽ‡å’Œ95.69%æ£€æµ‹mAPï¼Œè¾¹ç¼˜ç¡¬ä»¶å®žçŽ°59.8 FPSã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Driver fatigue remains a leading cause of road accidents, with 24\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\% and mAP by 5\% over video-level supervision, achieving 99.34\% classification accuracy and 95.69\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.

