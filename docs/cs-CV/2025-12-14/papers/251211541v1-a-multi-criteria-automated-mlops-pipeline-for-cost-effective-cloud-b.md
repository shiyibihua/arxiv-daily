---
layout: default
title: A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts
---

# A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts

**arXiv**: [2512.11541v1](https://arxiv.org/abs/2512.11541) | [PDF](https://arxiv.org/pdf/2512.11541.pdf)

**ä½œè€…**: Emmanuel K. Katalay, David O. Dimandja, Jordan F. Masakuna

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ ‡å‡†è‡ªåŠ¨åŒ–MLOpsç®¡é“ï¼Œä»¥åº”å¯¹æ•°æ®åˆ†å¸ƒæ¼‚ç§»ï¼Œå®žçŽ°äº‘ç«¯åˆ†ç±»å™¨é«˜æ•ˆé‡è®­ç»ƒã€‚**

**å…³é”®è¯**: `MLOpsè‡ªåŠ¨åŒ–` `æ•°æ®åˆ†å¸ƒæ¼‚ç§»æ£€æµ‹` `åˆ†ç±»å™¨é‡è®­ç»ƒ` `äº‘ç«¯èµ„æºä¼˜åŒ–` `å¤šæ ‡å‡†ç»Ÿè®¡æ–¹æ³•` `å¼‚å¸¸æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ•°æ®åˆ†å¸ƒæ¼‚ç§»å¯¼è‡´æœºå™¨å­¦ä¹ æ¨¡åž‹æ€§èƒ½ä¸‹é™ï¼Œä¼ ç»ŸMLOpsä¾èµ–äººå·¥è§¦å‘é‡è®­ç»ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¤šæ ‡å‡†ç»Ÿè®¡æŠ€æœ¯æ£€æµ‹åˆ†å¸ƒå˜åŒ–ï¼Œä»…åœ¨å¿…è¦æ—¶è‡ªåŠ¨è§¦å‘æ¨¡åž‹æ›´æ–°ï¼Œä¼˜åŒ–è®¡ç®—èµ„æºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªå¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œç›¸æ¯”ä¼ ç»Ÿç­–ç•¥ï¼Œæ¨¡åž‹å‡†ç¡®æ€§å’Œé²æ£’æ€§æ˜¾è‘—æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.

