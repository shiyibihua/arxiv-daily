---
layout: default
title: Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective
---

# Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective

**arXiv**: [2512.11784v1](https://arxiv.org/abs/2512.11784) | [PDF](https://arxiv.org/pdf/2512.11784.pdf)

**ä½œè€…**: Etienne Boursier, Claire Boyer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæµ‹åº¦çš„æ¡†æž¶ï¼Œè¯æ˜Žé•¿æç¤ºä¸‹Softmaxæ³¨æ„åŠ›æ”¶æ•›ä¸ºçº¿æ€§ç®—å­ï¼Œä¾¿äºŽç†è®ºåˆ†æžã€‚**

**å…³é”®è¯**: `Softmaxæ³¨æ„åŠ›` `çº¿æ€§æ³¨æ„åŠ›` `æµ‹åº¦ç†è®º` `é•¿æç¤ºåˆ†æž` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `è®­ç»ƒåŠ¨æ€`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSoftmaxæ³¨æ„åŠ›çš„éžçº¿æ€§ç»“æž„å¯¼è‡´ç†è®ºåˆ†æžå›°éš¾ï¼Œå°¤å…¶åœ¨é•¿æç¤ºåœºæ™¯ä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæµ‹åº¦æ¡†æž¶ï¼Œè¯æ˜Žåœ¨æ— é™æç¤ºæžé™ä¸‹Softmaxæ”¶æ•›ä¸ºçº¿æ€§ç®—å­ï¼Œå¹¶å»ºç«‹éžæ¸è¿‘æµ“åº¦ç•Œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸Šä¸‹æ–‡çº¿æ€§å›žå½’ä¸­ï¼Œåˆ©ç”¨æ— é™æç¤ºåŠ¨æ€åˆ†æžæœ‰é™æç¤ºè®­ç»ƒï¼Œå±•ç¤ºé•¿æç¤ºä¸‹Softmaxç»§æ‰¿çº¿æ€§ç»“æž„ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

