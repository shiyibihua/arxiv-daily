---
layout: default
title: CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks
---

# CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks

**arXiv**: [2512.11743v1](https://arxiv.org/abs/2512.11743) | [PDF](https://arxiv.org/pdf/2512.11743.pdf)

**ä½œè€…**: Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu, Changsheng Zhang, Bin Zhang, Mingkun Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCogniSNNï¼Œé€šè¿‡éšæœºå›¾æž¶æž„å®žçŽ°ç¥žç»å…ƒå¯æ‰©å±•æ€§ã€é€šè·¯å¯é‡ç”¨æ€§å’ŒåŠ¨æ€å¯é…ç½®æ€§ï¼Œä»¥æå‡è„‰å†²ç¥žç»ç½‘ç»œæ€§èƒ½ã€‚**

**å…³é”®è¯**: `è„‰å†²ç¥žç»ç½‘ç»œ` `éšæœºå›¾æž¶æž„` `é€šè·¯å¯é‡ç”¨æ€§` `åŠ¨æ€å¢žé•¿å­¦ä¹ ` `ç¥žç»å½¢æ€è®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é—®é¢˜ï¼šä¸»æµSNNé‡‡ç”¨é“¾å¼æž¶æž„ï¼Œå¿½ç•¥å¤§è„‘éšæœºè¿žæŽ¥ç‰¹æ€§ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æ–¹æ³•ï¼šå¼•å…¥éšæœºå›¾æž¶æž„ã€æ”¹è¿›æ®‹å·®æœºåˆ¶ã€è‡ªé€‚åº”æ± åŒ–ã€KP-LwFå’ŒDGLç®—æ³•ã€‚
3. æ•ˆæžœï¼šåœ¨ç¥žç»å½¢æ€æ•°æ®é›†å’ŒTiny-ImageNetä¸Šè¾¾åˆ°æˆ–è¶…è¶Šå…ˆè¿›SNNï¼Œå¢žå¼ºè¿žç»­å­¦ä¹ å’Œé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.

