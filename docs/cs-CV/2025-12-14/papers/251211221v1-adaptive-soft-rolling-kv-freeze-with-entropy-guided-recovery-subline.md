---
layout: default
title: Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference
---

# Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference

**arXiv**: [2512.11221v1](https://arxiv.org/abs/2512.11221) | [PDF](https://arxiv.org/pdf/2512.11221.pdf)

**ä½œè€…**: Adilet Metinov, Gulida M. Kudakeeva, Bolotbek uulu Nursultan, Gulnara D. Kabaeva

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è½¯æ»šåŠ¨KVå†»ç»“ä¸Žç†µå¼•å¯¼æ¢å¤æ¡†æž¶ï¼Œä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæŽ¨ç†ä¸­çš„å†…å­˜æ•ˆçŽ‡é—®é¢˜ã€‚**

**å…³é”®è¯**: `KVç¼“å­˜ä¼˜åŒ–` `æŽ¨ç†æ•ˆçŽ‡` `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `è®­ç»ƒæ— å…³æ–¹æ³•` `å†…å­˜ç®¡ç†` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡LLMæŽ¨ç†æ—¶KVç¼“å­˜å†…å­˜å¢žé•¿å¿«ï¼Œå½±å“éƒ¨ç½²æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæ»‘åŠ¨æ³¨æ„åŠ›çª—å£è¯†åˆ«ä½Žé‡è¦æ€§tokenï¼Œå¯é€†è½¯å†»ç»“å…¶KVæ›´æ–°ï¼Œç»“åˆç†µå¼•å¯¼æ¢å¤å’Œæ¬¡çº¿æ€§è°ƒåº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLaMA-3 8Bä¸Šï¼Œä¸»åŠ¨KVç¼“å­˜å¤§å°å‡å°‘55-67%ï¼Œä¿æŒç”Ÿæˆè´¨é‡å¹¶é€šè¿‡æ£€ç´¢æµ‹è¯•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

