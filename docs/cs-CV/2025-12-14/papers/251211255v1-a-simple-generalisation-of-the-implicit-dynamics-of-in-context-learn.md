---
layout: default
title: A Simple Generalisation of the Implicit Dynamics of In-Context Learning
---

# A Simple Generalisation of the Implicit Dynamics of In-Context Learning

**arXiv**: [2512.11255v1](https://arxiv.org/abs/2512.11255) | [PDF](https://arxiv.org/pdf/2512.11255.pdf)

**ä½œè€…**: Francesco Innocenti, El Mehdi Achour

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸Šä¸‹æ–‡å­¦ä¹ éšå¼åŠ¨æ€çš„ç®€å•æ³›åŒ–ï¼Œæ‰©å±•è‡³æ‰€æœ‰åºåˆ—ä½ç½®ã€ä»»æ„Transformerå—åŠæ›´çŽ°å®žçš„æ®‹å·®å—ã€‚**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `Transformer` `éšå¼åŠ¨æ€` `çº¿æ€§å›žå½’` `æ®‹å·®å—` `å±‚å½’ä¸€åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­æ¨¡åž‹å¦‚ä½•ä»Žè¾“å…¥ç¤ºä¾‹å­¦ä¹ æ–°ä»»åŠ¡ï¼Œæ— éœ€å‚æ•°æ›´æ–°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ³›åŒ–Dherinç­‰äººï¼ˆ2025ï¼‰çš„ç†è®ºï¼Œæ¶µç›–æ‰€æœ‰åºåˆ—ä½ç½®ã€ä»»æ„Transformerå—å’ŒåŒ…å«å±‚å½’ä¸€åŒ–çš„æ®‹å·®å—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç®€å•ä¸Šä¸‹æ–‡çº¿æ€§å›žå½’ä»»åŠ¡ä¸Šå®žè¯éªŒè¯ï¼Œå¹¶æŽ¢ç©¶å—å†…å’Œå—é—´ä¸åŒä»¤ç‰Œçš„éšå¼æ›´æ–°å…³ç³»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.

