---
layout: default
title: BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models
---

# BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models

**arXiv**: [2512.11769v1](https://arxiv.org/abs/2512.11769) | [PDF](https://arxiv.org/pdf/2512.11769.pdf)

**ä½œè€…**: Xiaoyu Ma, Zhengqing Yuan, Zheyuan Zhang, Kaiwen Shi, Lichao Sun, Yanfang Ye

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBLURRè½»é‡æŽ¨ç†åŒ…è£…å™¨ï¼Œä»¥åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹åŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹éƒ¨ç½²ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `è½»é‡æŽ¨ç†` `é”®å€¼ç¼“å­˜` `æ··åˆç²¾åº¦` `æœºå™¨äººæŽ§åˆ¶` `Webéƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šVLAæ¨¡åž‹æŽ¨ç†æ ˆè¿‡é‡ï¼Œéš¾ä»¥åœ¨æ™®é€šGPUä¸Šå®žçŽ°å“åº”å¼Webæ¼”ç¤ºæˆ–é«˜é¢‘æœºå™¨äººæŽ§åˆ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æŒ‡ä»¤å‰ç¼€é”®å€¼ç¼“å­˜ã€æ··åˆç²¾åº¦æ‰§è¡Œå’Œå•æ­¥å±•å¼€è°ƒåº¦ï¼Œæ— éœ€é‡è®­ç»ƒå³å¯åŠ é€ŸçŽ°æœ‰VLAæŽ§åˆ¶å™¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨SimplerEnvè¯„ä¼°ä¸­ä¿æŒä»»åŠ¡æˆåŠŸçŽ‡ï¼Œæ˜¾è‘—é™ä½ŽFLOPså’Œå»¶è¿Ÿï¼Œå¹¶æž„å»ºäº¤äº’å¼Webæ¼”ç¤ºã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.

