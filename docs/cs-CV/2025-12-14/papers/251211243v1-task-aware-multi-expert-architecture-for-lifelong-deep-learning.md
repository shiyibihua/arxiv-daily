---
layout: default
title: Task-Aware Multi-Expert Architecture For Lifelong Deep Learning
---

# Task-Aware Multi-Expert Architecture For Lifelong Deep Learning

**arXiv**: [2512.11243v1](https://arxiv.org/abs/2512.11243) | [PDF](https://arxiv.org/pdf/2512.11243.pdf)

**ä½œè€…**: Jianyu Wang, Jacob Nean-Hua Sheikh, Cat P. Le, Hoda Bidkhori

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»»åŠ¡æ„ŸçŸ¥å¤šä¸“å®¶æž¶æž„ä»¥è§£å†³ç»ˆèº«æ·±åº¦å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜**

**å…³é”®è¯**: `ç»ˆèº«æ·±åº¦å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `å¤šä¸“å®¶æž¶æž„` `ä»»åŠ¡ç›¸ä¼¼æ€§` `é‡æ’­ç¼“å†²` `æ³¨æ„åŠ›æœºåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»ˆèº«æ·±åº¦å­¦ä¹ éœ€é¡ºåºå­¦ä¹ ä»»åŠ¡åŒæ—¶é¿å…é—å¿˜æ—§çŸ¥è¯†
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä»»åŠ¡ç›¸ä¼¼æ€§é€‰æ‹©ä¸“å®¶ï¼Œç»“åˆé‡æ’­ç¼“å†²å’Œæ³¨æ„åŠ›æœºåˆ¶
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-100äºŒåˆ†ç±»ä»»åŠ¡ä¸Šæå‡æ–°ä»»åŠ¡å‡†ç¡®çŽ‡å¹¶ç»´æŒæ—§ä»»åŠ¡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.

