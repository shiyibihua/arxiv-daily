---
layout: default
title: VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing
---

# VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing

**arXiv**: [2512.11490v1](https://arxiv.org/abs/2512.11490) | [PDF](https://arxiv.org/pdf/2512.11490.pdf)

**ä½œè€…**: Emanuel SÃ¡nchez Aimar, Gulnaz Zhambulova, Fahad Shahbaz Khan, Yonghao Xu, Michael Felsberg

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLM2GeoVecï¼Œé€šè¿‡å•ç¼–ç å™¨ç»Ÿä¸€åµŒå…¥å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®ï¼Œå®žçŽ°å¯æ‰©å±•æ£€ç´¢ä¸ŽåŒºåŸŸçº§ç©ºé—´æŽ¨ç†ã€‚**

**å…³é”®è¯**: `é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€åµŒå…¥` `å¯¹æ¯”å­¦ä¹ ` `åŒºåŸŸçº§æŽ¨ç†` `åœ°ç†ç©ºé—´æ£€ç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é¥æ„Ÿå›¾åƒä¸Žè‡ªç„¶å›¾åƒå·®å¼‚æ˜¾è‘—ï¼ŒçŽ°æœ‰æ–¹æ³•åœ¨æ£€ç´¢ä¸Žç”Ÿæˆä»»åŠ¡é—´å­˜åœ¨å‰²è£‚ã€‚
2. VLM2GeoVecé‡‡ç”¨å•ç¼–ç å™¨å¤„ç†äº¤é”™è¾“å…¥ï¼Œé€šè¿‡å¯¹æ¯”æŸå¤±è®­ç»ƒç»Ÿä¸€å‘é‡ç©ºé—´ã€‚
3. åœ¨RSMEBåŸºå‡†ä¸Šï¼ŒåŒºåŸŸçº§æ£€ç´¢ä»»åŠ¡æ€§èƒ½æ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¿æŒä¼ ç»Ÿä»»åŠ¡ç«žäº‰åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.

