---
layout: default
title: EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing
---

# EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing

**arXiv**: [2512.11715v1](https://arxiv.org/abs/2512.11715) | [PDF](https://arxiv.org/pdf/2512.11715.pdf)

**ä½œè€…**: Wei Chow, Linfeng Li, Lingdong Kong, Zefeng Li, Qi Xu, Hang Song, Tian Ye, Xian Wang, Jinbin Bai, Shilin Xu, Xiangtai Li, Junting Pan, Shaoteng Liu, Ran Zhou, Tianshu Yang, Songhua Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEditMGTæ¡†æž¶ï¼Œåˆ©ç”¨æŽ©ç ç”ŸæˆTransformerè§£å†³å›¾åƒç¼–è¾‘ä¸­éžç›®æ ‡åŒºåŸŸæ„å¤–ä¿®æ”¹é—®é¢˜ã€‚**

**å…³é”®è¯**: `å›¾åƒç¼–è¾‘` `æŽ©ç ç”ŸæˆTransformer` `å±€éƒ¨è§£ç ` `æ³¨æ„åŠ›æœºåˆ¶` `åŒºåŸŸä¿æŒé‡‡æ ·` `é«˜åˆ†è¾¨çŽ‡æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡åž‹å…¨å±€åŽ»å™ªå¯¼è‡´éžç›®æ ‡åŒºåŸŸæ„å¤–ä¿®æ”¹ï¼Œè½¬å‘æŽ©ç ç”ŸæˆTransformerçš„å±€éƒ¨è§£ç èŒƒå¼ã€‚
2. åŸºäºŽäº¤å‰æ³¨æ„åŠ›å›¾ç²¾ç¡®å®šä½ç¼–è¾‘åŒºåŸŸï¼Œå¼•å…¥åŒºåŸŸä¿æŒé‡‡æ ·é™åˆ¶ä¿®æ”¹èŒƒå›´ã€‚
3. åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡åž‹å‚æ•°å°‘äºŽ10äº¿ï¼Œå®žçŽ°ç›¸ä¼¼æ€§èƒ½ä¸”ç¼–è¾‘é€Ÿåº¦æå‡6å€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.

