---
layout: default
title: Visualizing token importance for black-box language models
---

# Visualizing token importance for black-box language models

**arXiv**: [2512.11573v1](https://arxiv.org/abs/2512.11573) | [PDF](https://arxiv.org/pdf/2512.11573.pdf)

**ä½œè€…**: Paulius Rauba, Qiyao Wei, Mihaela van der Schaar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒæ•æ„Ÿæ€§åˆ†æžä»¥è¯„ä¼°é»‘ç›’è¯­è¨€æ¨¡åž‹è¾“å…¥ä»¤ç‰Œé‡è¦æ€§**

**å…³é”®è¯**: `é»‘ç›’è¯­è¨€æ¨¡åž‹` `ä»¤ç‰Œé‡è¦æ€§` `æ•æ„Ÿæ€§åˆ†æž` `æ¨¡åž‹å®¡è®¡` `å¯è§£é‡Šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå®¡è®¡é»‘ç›’LLMè¾“å‡ºå¯¹è¾“å…¥ä»¤ç‰Œçš„ä¾èµ–ï¼Œç¡®ä¿é«˜é£Žé™©é¢†åŸŸå¯é æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šå¼€å‘è½»é‡çº§æ¨¡åž‹æ— å…³çš„DBSAï¼Œæ— éœ€åˆ†å¸ƒå‡è®¾ï¼Œå¯è§†åŒ–ä»¤ç‰Œæ•æ„Ÿæ€§
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡ç¤ºä¾‹å±•ç¤ºDBSAèƒ½å‘çŽ°çŽ°æœ‰æ–¹æ³•å¿½ç•¥çš„æ•æ„Ÿæ€§ï¼Œæ”¯æŒå¿«é€ŸæŽ¢ç´¢

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.

