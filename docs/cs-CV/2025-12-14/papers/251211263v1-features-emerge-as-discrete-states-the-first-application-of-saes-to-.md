---
layout: default
title: Features Emerge as Discrete States: The First Application of SAEs to 3D Representations
---

# Features Emerge as Discrete States: The First Application of SAEs to 3D Representations

**arXiv**: [2512.11263v1](https://arxiv.org/abs/2512.11263) | [PDF](https://arxiv.org/pdf/2512.11263.pdf)

**ä½œè€…**: Albert Miao, Chenliang Zhou, Jiawei Zhou, Cengiz Oztireli

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é¦–æ¬¡å°†ç¨€ç–è‡ªç¼–ç å™¨åº”ç”¨äºŽ3Dè¡¨ç¤ºï¼Œæ­ç¤ºç‰¹å¾ä»¥ç¦»æ•£çŠ¶æ€æ¶ŒçŽ°å¹¶è§£é‡Šæ¨¡åž‹è¡Œä¸ºã€‚**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `3Dè¡¨ç¤ºå­¦ä¹ ` `ç‰¹å¾åˆ†è§£` `ç¦»æ•£çŠ¶æ€ç©ºé—´` `ç›¸å˜åˆ†æž` `VAEé‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–è‡ªç¼–ç å™¨åœ¨æ–‡æœ¬åŸŸå¤–åº”ç”¨æœ‰é™ï¼Œé˜»ç¢ç‰¹å¾åˆ†è§£ç†è®ºæŽ¢ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ†æž3Dé‡å»ºVAEåœ¨Objaverseæ•°æ®é›†ä¸Šçš„ç‰¹å¾ï¼Œå‘çŽ°ç¦»æ•£è€Œéžè¿žç»­ç¼–ç ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè§‚å¯Ÿåˆ°ç›¸å˜é©±åŠ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ï¼Œè§£é‡Šä½ç½®ç¼–ç åå¥½å’ŒæŸå¤±è¡Œä¸ºç­‰çŽ°è±¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.

