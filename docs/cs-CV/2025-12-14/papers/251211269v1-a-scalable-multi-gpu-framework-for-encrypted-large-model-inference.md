---
layout: default
title: A Scalable Multi-GPU Framework for Encrypted Large-Model Inference
---

# A Scalable Multi-GPU Framework for Encrypted Large-Model Inference

**arXiv**: [2512.11269v1](https://arxiv.org/abs/2512.11269) | [PDF](https://arxiv.org/pdf/2512.11269.pdf)

**ä½œè€…**: Siddharth Jayashankar, Joshua Kim, Michael B. Sullivan, Wenting Zheng, Dimitrios Skarlatos

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCeriumå¤šGPUæ¡†æž¶ä»¥è§£å†³å…¨åŒæ€åŠ å¯†å¤§æ¨¡åž‹æŽ¨ç†çš„æ€§èƒ½ä¸Žå†…å­˜æŒ‘æˆ˜**

**å…³é”®è¯**: `å…¨åŒæ€åŠ å¯†` `å¤§æ¨¡åž‹æŽ¨ç†` `å¤šGPUæ¡†æž¶` `ç¼–è¯‘å™¨ä¼˜åŒ–` `å†…å­˜ç®¡ç†` `å¹¶è¡Œè®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…¨åŒæ€åŠ å¯†æŽ¨ç†æ€§èƒ½æ…¢ï¼Œå¤§æ¨¡åž‹å†…å­˜éœ€æ±‚è¿œè¶…å•GPUå®¹é‡ï¼ŒGPUå¹³å°éš¾ä»¥åŒ¹æ•ŒASICæ€§èƒ½
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆé¢†åŸŸç‰¹å®šè¯­è¨€ã€ä¼˜åŒ–ç¼–è¯‘å™¨å’Œè¿è¡Œæ—¶ç³»ç»Ÿï¼Œè‡ªåŠ¨ç”ŸæˆGPUå†…æ ¸ï¼Œç®¡ç†TBçº§å†…å­˜ï¼Œå¤šGPUå¹¶è¡Œè®¡ç®—
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ€§èƒ½è¶…è¶Šæ‰‹å·¥ä¼˜åŒ–åº“2.25å€ï¼ŒåŒ¹é…å…ˆè¿›FHE ASICï¼Œé¦–æ¬¡å®žçŽ°BERT-Baseå’ŒLlama3-8BåŠ å¯†æŽ¨ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.

