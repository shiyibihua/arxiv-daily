---
layout: default
title: Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers
---

# Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers

**arXiv**: [2512.11260v1](https://arxiv.org/abs/2512.11260) | [PDF](https://arxiv.org/pdf/2512.11260.pdf)

**ä½œè€…**: Ali El Bellaj, Mohammed-Amine Cheddadi, Rhassan Berber

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒReformerä¸ŽVision Transformeråœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æ•ˆçŽ‡ä¸Žæ€§èƒ½**

**å…³é”®è¯**: `è§†è§‰Transformer` `Reformeræž¶æž„` `å±€éƒ¨æ•æ„Ÿå“ˆå¸Œæ³¨æ„åŠ›` `è®¡ç®—æ•ˆçŽ‡` `é«˜åˆ†è¾¨çŽ‡å›¾åƒ` `å®žéªŒæ¯”è¾ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ ‡å‡†Vision Transformerå› å…¨å±€è‡ªæ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦é«˜ï¼Œé™åˆ¶é«˜åˆ†è¾¨çŽ‡è¾“å…¥åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨Reformeræž¶æž„ï¼Œç»“åˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œæ³¨æ„åŠ›é™ä½Žç†è®ºå¤æ‚åº¦è‡³O(n log n)ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨CIFAR-10ä¸ŠReformeræ›´å‡†ç¡®ï¼Œä½†åœ¨æ›´å¤§è§„æ¨¡å’Œé«˜åˆ†è¾¨çŽ‡è®¾ç½®ä¸­ViTå®žé™…æ•ˆçŽ‡æ›´é«˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
>   In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
>   While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.

