---
layout: default
title: MLLM Machine Unlearning via Visual Knowledge Distillation
---

# MLLM Machine Unlearning via Visual Knowledge Distillation

**arXiv**: [2512.11325v1](https://arxiv.org/abs/2512.11325) | [PDF](https://arxiv.org/pdf/2512.11325.pdf)

**ä½œè€…**: Yuhang Wang, Zhenxing Niu, Haoxuan Ji, Guangyu He, Haichang Gao, Gang Hua

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰çŸ¥è¯†è’¸é¦æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­çš„è§†è§‰çŸ¥è¯†é€‰æ‹©æ€§é—å¿˜é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `æœºå™¨é—å¿˜` `è§†è§‰çŸ¥è¯†è’¸é¦` `é€‰æ‹©æ€§é—å¿˜` `æ¨¡åž‹æ•ˆçŽ‡` `é²æ£’æ€§è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é—å¿˜æ–¹æ³•ä¸»è¦é’ˆå¯¹LLMsï¼Œå¤šæ¨¡æ€å¤§æ¨¡åž‹é—å¿˜ç ”ç©¶å¤„äºŽæ—©æœŸé˜¶æ®µï¼Œéœ€é€‰æ‹©æ€§ç§»é™¤è§†è§‰çŸ¥è¯†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥è§†è§‰çŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œåˆ©ç”¨æ¨¡åž‹å†…éƒ¨è§†è§‰è¡¨ç¤ºä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä»…å¾®è°ƒè§†è§‰ç»„ä»¶ä»¥æå‡æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¡¨æ˜Žæ–¹æ³•åœ¨æœ‰æ•ˆæ€§å’Œæ•ˆçŽ‡ä¸Šä¼˜äºŽçŽ°æœ‰æŠ€æœ¯ï¼Œå¹¶é¦–æ¬¡è¯„ä¼°äº†é—å¿˜å¯¹å†å­¦ä¹ æ”»å‡»çš„é²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.

