---
layout: default
title: Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems
---

# Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems

**arXiv**: [2512.11532v1](https://arxiv.org/abs/2512.11532) | [PDF](https://arxiv.org/pdf/2512.11532.pdf)

**ä½œè€…**: Chong Tang, Hao Dai, Jagmohan Chauhan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºParallaxæ¡†æž¶ä»¥åŠ é€Ÿç§»åŠ¨è®¾å¤‡ä¸ŠåŠ¨æ€DNNæŽ¨ç†çš„ç®—å­å›žé€€é—®é¢˜**

**å…³é”®è¯**: `ç§»åŠ¨DNNæŽ¨ç†` `ç®—å­å›žé€€` `å¹¶è¡ŒåŒ–æ¡†æž¶` `å†…å­˜ç®¡ç†` `å¼‚æž„è¾¹ç¼˜ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç§»åŠ¨è®¾å¤‡ä¸ŠåŠ¨æ€æŽ§åˆ¶æµç®—å­å›žé€€CPUæ‰§è¡Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œå†…å­˜å³°å€¼
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è®¡ç®—å›¾åˆ†åŒºã€åˆ†æ”¯æ„ŸçŸ¥å†…å­˜ç®¡ç†å’Œè‡ªé€‚åº”è°ƒåº¦å®žçŽ°å¹¶è¡ŒåŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ç§ç§»åŠ¨è®¾å¤‡ä¸Šè¯„ä¼°ï¼Œå®žçŽ°æœ€é«˜46%å»¶è¿Ÿé™ä½Žå’Œå¹³å‡26.5%å†…å­˜å¼€é”€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

