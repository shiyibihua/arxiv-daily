---
layout: default
title: 3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation
---

# 3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation

**arXiv**: [2512.11557v1](https://arxiv.org/abs/2512.11557) | [PDF](https://arxiv.org/pdf/2512.11557.pdf)

**ä½œè€…**: Zhiguo Lu, Jianwen Lou, Mingjun Ma, Hairong Jin, Youyi Zheng, Kun Zhou

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º3DTeethSAMä»¥è§£å†³3Dç‰™é½¿åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡é€‚é…SAM2å¹¶å¼•å…¥è½»é‡æ¨¡å—æå‡æ€§èƒ½ã€‚**

**å…³é”®è¯**: `3Dç‰™é½¿åˆ†å‰²` `SAM2é€‚é…` `è½»é‡æ¨¡å—` `2D-3DæŠ•å½±` `æ•°å­—ç‰™ç§‘` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š3Dç‰™é½¿åˆ†å‰²åœ¨æ•°å­—ç‰™ç§‘ä¸­å› çœŸå®žç‰™åˆ—å¤æ‚æ€§è€Œå…·æŒ‘æˆ˜æ€§ï¼Œéœ€å®šä½å®žä¾‹å¹¶è¯­ä¹‰åˆ†ç±»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä»Žé¢„å®šä¹‰è§†å›¾æ¸²æŸ“3Dç‰™é½¿æ¨¡åž‹å›¾åƒï¼Œåº”ç”¨SAM2è¿›è¡Œ2Dåˆ†å‰²ï¼Œé€šè¿‡2D-3DæŠ•å½±é‡å»ºç»“æžœï¼Œå¹¶å¼•å…¥æç¤ºåµŒå…¥ç”Ÿæˆå™¨ã€æŽ©ç ç²¾ç‚¼å™¨å’Œåˆ†ç±»å™¨ç­‰è½»é‡æ¨¡å—ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨3DTeethSegåŸºå‡†æµ‹è¯•ä¸­ï¼Œé«˜åˆ†è¾¨çŽ‡3Dç‰™é½¿ç½‘æ ¼ä¸Šè¾¾åˆ°91.90% IoUï¼Œåˆ›ä¸‹æ–°æœ€ä¼˜æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> 3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.

