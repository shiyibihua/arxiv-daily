---
layout: default
title: Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation
---

# Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation

**arXiv**: [2512.11720v1](https://arxiv.org/abs/2512.11720) | [PDF](https://arxiv.org/pdf/2512.11720.pdf)

**ä½œè€…**: Yan Zhang, Han Zou, Lincong Feng, Cong Xie, Ruiqi Yu, Zhenpeng Zhan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šé€šé“å›¾åƒç”Ÿæˆçš„éŸ³ä¹é©±åŠ¨2Dèˆžè¹ˆå§¿æ€ç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³å¤æ‚åˆ†å¸ƒä¸‹çš„æ—¶åºä¸€è‡´æ€§å’ŒèŠ‚å¥å¯¹é½é—®é¢˜ã€‚**

**å…³é”®è¯**: `éŸ³ä¹é©±åŠ¨èˆžè¹ˆç”Ÿæˆ` `å¤šé€šé“å›¾åƒç”Ÿæˆ` `æ—¶åºä¸€è‡´æ€§` `å§¿æ€åºåˆ—ç¼–ç ` `å‚è€ƒå§¿æ€æ¡ä»¶` `é‡Žå¤–æ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»ŽéŸ³ä¹ç”Ÿæˆæ—¶åºä¸€è‡´ã€èŠ‚å¥å¯¹é½çš„2Dèˆžè¹ˆå§¿æ€ï¼Œå°¤å…¶åœ¨å¤æ‚é«˜æ–¹å·®åˆ†å¸ƒä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†å§¿æ€åºåˆ—ç¼–ç ä¸ºç‹¬çƒ­å›¾åƒï¼Œä½¿ç”¨é¢„è®­ç»ƒVAEåŽ‹ç¼©å’ŒDiTé£Žæ ¼éª¨å¹²å»ºæ¨¡ï¼Œå¼•å…¥æ—¶é—´å…±äº«ç´¢å¼•å’Œå‚è€ƒå§¿æ€æ¡ä»¶ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨é‡Žå¤–èˆžè¹ˆæ•°æ®é›†å’ŒAIST++2DåŸºå‡†ä¸Šï¼Œå§¿æ€å’Œè§†é¢‘æŒ‡æ ‡åŠäººç±»åå¥½ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io

