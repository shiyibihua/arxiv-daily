---
layout: default
title: Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols
---

# Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols

**arXiv**: [2512.11614v1](https://arxiv.org/abs/2512.11614) | [PDF](https://arxiv.org/pdf/2512.11614.pdf)

**ä½œè€…**: BjÃ¶rn Deiseroth, Max Henning HÃ¶th, Kristian Kersting, Letitia Parcalabescu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽMerlin-Arthuråè®®çš„è®­ç»ƒæ¡†æž¶ï¼Œä»¥æå‡æ£€ç´¢å¢žå¼ºç”Ÿæˆç³»ç»Ÿçš„å¯éªŒè¯æ€§ä¸Žå¯é æ€§ã€‚**

**å…³é”®è¯**: `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `äº¤äº’è¯æ˜Žç³»ç»Ÿ` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å¹»è§‰å‡å°‘` `ä¿¡æ¯è®ºä¿è¯` `è‡ªåŠ¨ç›‘ç£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰RAGç³»ç»Ÿå°†æ£€ç´¢è§†ä¸ºå¯å‘å¼è€Œéžå¯éªŒè¯è¯æ®ï¼Œå¯¼è‡´æ¨¡åž‹å¹»è§‰ã€ä¾èµ–è™šå‡è¯æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†RAGç®¡é“å»ºæ¨¡ä¸ºäº¤äº’è¯æ˜Žç³»ç»Ÿï¼Œé€šè¿‡Merlinæä¾›è¯æ®ã€Morganaæ³¨å…¥è¯¯å¯¼ä¸Šä¸‹æ–‡ï¼Œè®­ç»ƒç”Ÿæˆå™¨åŸºäºŽè¯æ®å›žç­”ã€æ‹’ç»æˆ–ä¾èµ–å…·ä½“ä¸Šä¸‹æ–‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡åž‹ä¸Šï¼ŒM/Aè®­ç»ƒæ”¹å–„äº†æ¨¡åž‹çš„åŸºç¡€æ€§ã€å®Œæ•´æ€§å’Œæ‹’ç»è¡Œä¸ºï¼Œå‡å°‘äº†å¹»è§‰ï¼Œå¹¶æå‡äº†æ£€ç´¢å™¨æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.

