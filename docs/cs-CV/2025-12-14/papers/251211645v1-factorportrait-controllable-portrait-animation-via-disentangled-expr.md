---
layout: default
title: FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint
---

# FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint

**arXiv**: [2512.11645v1](https://arxiv.org/abs/2512.11645) | [PDF](https://arxiv.org/pdf/2512.11645.pdf)

**‰ΩúËÄÖ**: Jiapeng Tang, Kai Li, Chengxiang Yin, Liuhao Ge, Fei Jiang, Jiu Xu, Matthias Nie√üner, Christian H√§ne, Timur Bagautdinov, Egor Zakharov, Peihong Guo

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FactorPortraitÔºåÈÄöËøáËß£ËÄ¶ÊéßÂà∂ÂÆûÁé∞ÂçïÂº†ËÇñÂÉèÂä®Áîª‰∏éËßÜËßíÂêàÊàê**

**ÂÖ≥ÈîÆËØç**: `ËÇñÂÉèÂä®Áîª` `ËßÜÈ¢ëÊâ©Êï£` `Ëß£ËÄ¶ÊéßÂà∂` `ËßÜËßíÂêàÊàê` `Ë°®ÊÉÖËøÅÁßª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÂ¶Ç‰Ωï‰ªéÂçïÂº†ËÇñÂÉèÁîüÊàêÂèØÊéßÂä®ÁîªÔºåÂêåÊó∂Ëß£ËÄ¶Ë°®ÊÉÖ„ÄÅÂßøÊÄÅÂíåËßÜËßí
2. ÊñπÊ≥ïË¶ÅÁÇπÔºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁºñÁ†ÅÂô®ÊèêÂèñË°®ÊÉÖÊΩúÂèòÈáèÔºåÁªìÂêàPl√ºckerÂ∞ÑÁ∫øÂõæÊéßÂà∂Áõ∏Êú∫ÂíåÂßøÊÄÅ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®ÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÂÆûÈ™åÊòæÁ§∫Âú®ÁúüÂÆûÊÑü„ÄÅÊéßÂà∂Á≤æÂ∫¶ÂíåËßÜËßí‰∏ÄËá¥ÊÄß‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Pl√ºcker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.

