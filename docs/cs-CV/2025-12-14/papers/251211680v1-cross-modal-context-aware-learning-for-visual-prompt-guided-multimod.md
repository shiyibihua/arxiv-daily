---
layout: default
title: Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing
---

# Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing

**arXiv**: [2512.11680v1](https://arxiv.org/abs/2512.11680) | [PDF](https://arxiv.org/pdf/2512.11680.pdf)

**ä½œè€…**: Xu Zhang, Jiabin Fang, Zhuoming Ding, Jin Yuan, Xuan Liu, Qianjun Zhang, Zhiyong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLV-Netä»¥è§£å†³é¥æ„Ÿå›¾åƒä¸­è§†è§‰æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€ç†è§£é—®é¢˜**

**å…³é”®è¯**: `é¥æ„Ÿå›¾åƒç†è§£` `è§†è§‰æç¤ºå¼•å¯¼` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥å­¦ä¹ ` `å¤šæ¨¡æ€å¯¹é½` `ç›®æ ‡åˆ†å‰²` `å…³ç³»å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨ç®€å•æ–‡æœ¬æç¤ºä¸‹éš¾ä»¥å¼•å¯¼æ¨¡åž‹å…³æ³¨ç”¨æˆ·ç›¸å…³åŒºåŸŸ
2. CLV-Neté€šè¿‡è§†è§‰æç¤ºå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è§£ç å™¨å¢žå¼ºç›®æ ‡è¡¨ç¤ºä¸ŽæŽ©ç è´¨é‡
3. åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œå®žçŽ°ç”¨æˆ·æ„å›¾å¯¹é½çš„å¤šæ¨¡æ€è¾“å‡º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.

