---
layout: default
title: When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents
---

# When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents

**arXiv**: [2512.11277v1](https://arxiv.org/abs/2512.11277) | [PDF](https://arxiv.org/pdf/2512.11277.pdf)

**ä½œè€…**: Mrinal Rawat, Arkajyoti Chakraborty, Neha Gupta, Roberto Pieraccini

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„æŽ¨ç†-è¡ŒåŠ¨ååŒæ–¹æ³•ï¼Œä»¥æå‡å¯¹è¯ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ä¸Žå·¥å…·è°ƒç”¨ç²¾åº¦ã€‚**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æŽ¨ç†å¢žå¼º` `å¯¹è¯ä»£ç†` `å·¥å…·è°ƒç”¨` `æ³›åŒ–èƒ½åŠ›` `ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›‘ç£å¾®è°ƒåœ¨æ•°æ®åˆ†å¸ƒå˜åŒ–æ—¶æ³›åŒ–å›°éš¾ï¼Œé«˜è´¨é‡æŽ¨ç†æ ‡æ³¨æˆæœ¬é«˜ä¸”éš¾æ‰©å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶ï¼ˆå·¥å…·å‡†ç¡®æ€§å’Œç­”æ¡ˆæ­£ç¡®æ€§ï¼‰è¿­ä»£ä¼˜åŒ–æŽ¨ç†æ­¥éª¤ä¸Žå·¥å…·è°ƒç”¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”æ— æ˜¾å¼æŽ¨ç†çš„ç›‘ç£å¾®è°ƒæ¨¡åž‹ï¼Œç›¸å¯¹æå‡1.5%ï¼›ç›¸æ¯”åŸºç¡€æ¨¡åž‹ï¼Œå¢žç›Šè¾¾40%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.

