---
layout: default
title: Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models
---

# Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models

**arXiv**: [2512.11482v1](https://arxiv.org/abs/2512.11482) | [PDF](https://arxiv.org/pdf/2512.11482.pdf)

**ä½œè€…**: Melih Catal, Pooja Rani, Harald C. Gall

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åº”ç”¨å·®åˆ†éšç§äºŽä»£ç è¯­è¨€æ¨¡åž‹ä»¥ç¼“è§£è®°å¿†é£Žé™©å¹¶ä¿æŒç”Ÿæˆèƒ½åŠ›**

**å…³é”®è¯**: `å·®åˆ†éšç§` `ä»£ç è¯­è¨€æ¨¡åž‹` `éšç§ä¿æŠ¤` `ä»£ç ç”Ÿæˆ` `æ¨¡åž‹è®­ç»ƒ` `è®°å¿†ç¼“è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä»£ç è¯­è¨€æ¨¡åž‹åœ¨è®­ç»ƒä¸­å¯èƒ½è®°å¿†å¹¶æ³„éœ²æ•æ„Ÿä»£ç ç‰‡æ®µï¼Œå¼•å‘éšç§å’ŒçŸ¥è¯†äº§æƒé£Žé™©
2. é€šè¿‡å·®åˆ†éšç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ æ ¡å‡†å™ªå£°ï¼Œæœ‰æ•ˆå‡å°‘è®°å¿†è¡Œä¸ºè€Œä¸æ˜¾è‘—æŸå®³æ¨¡åž‹æ€§èƒ½
3. å®žéªŒæ˜¾ç¤ºå·®åˆ†éšç§å¤§å¹…é™ä½Žè®°å¿†çŽ‡ï¼Œè½»å¾®å¢žåŠ å›°æƒ‘åº¦ï¼Œä¸”å¯¹è®­ç»ƒæ•ˆçŽ‡å’Œèƒ½è€—å½±å“å°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

