---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-11
---

# cs.CVï¼ˆ2025-09-11ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—7)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250909658v1-measuring-epistemic-humility-in-multimodal-large-language-models.html">Measuring Epistemic Humility in Multimodal Large Language Models</a></td>
  <td>HumbleBenchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è®¤çŸ¥è°¦é€Šæ€§çš„æ–°åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09658v1" data-paper-url="./papers/250909658v1-measuring-epistemic-humility-in-multimodal-large-language-models.html" onclick="toggleFavorite(this, '2509.09658v1', 'Measuring Epistemic Humility in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250909090v1-sqap-vla-a-synergistic-quantization-aware-pruning-framework-for-high.html">SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models</a></td>
  <td>æå‡ºSQAP-VLAæ¡†æ¶ï¼ŒååŒé‡åŒ–ä¸å‰ªæåŠ é€Ÿé«˜æ€§èƒ½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¨ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09090v1" data-paper-url="./papers/250909090v1-sqap-vla-a-synergistic-quantization-aware-pruning-framework-for-high.html" onclick="toggleFavorite(this, '2509.09090v1', 'SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250909307v1-can-multimodal-llms-see-materials-clearly-a-multimodal-benchmark-on-.html">Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization</a></td>
  <td>æå‡ºMatChaï¼šææ–™è¡¨å¾å¤šæ¨¡æ€åŸºå‡†ï¼Œè¯„ä¼°MLLMåœ¨ææ–™ç§‘å­¦å›¾åƒç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09307v1" data-paper-url="./papers/250909307v1-can-multimodal-llms-see-materials-clearly-a-multimodal-benchmark-on-.html" onclick="toggleFavorite(this, '2509.09307v1', 'Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250909584v1-visual-grounding-from-event-cameras.html">Visual Grounding from Event Cameras</a></td>
  <td>æå‡ºTalk2Eventï¼Œé¦–ä¸ªåŸºäºäº‹ä»¶ç›¸æœºçš„è¯­è¨€é©±åŠ¨ç‰©ä½“å®šä½å¤§è§„æ¨¡åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09584v1" data-paper-url="./papers/250909584v1-visual-grounding-from-event-cameras.html" onclick="toggleFavorite(this, '2509.09584v1', 'Visual Grounding from Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250909595v2-kling-avatar-grounding-multimodal-instructions-for-cascaded-long-dur.html">Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis</a></td>
  <td>Kling-Avatarï¼šé€šè¿‡å¤šæ¨¡æ€æŒ‡ä»¤é©±åŠ¨çš„çº§è”å¼é•¿æ—¶ç¨‹è™šæ‹Ÿå½¢è±¡åŠ¨ç”»åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09595v2" data-paper-url="./papers/250909595v2-kling-avatar-grounding-multimodal-instructions-for-cascaded-long-dur.html" onclick="toggleFavorite(this, '2509.09595v2', 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250909254v1-towards-better-dental-ai-a-multimodal-benchmark-and-instruction-data.html">Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis</a></td>
  <td>æå‡ºMMOralï¼šç”¨äºå…¨æ™¯Xå…‰åˆ†æçš„å¤šæ¨¡æ€åŸºå‡†å’ŒæŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶æ„å»ºOralGPTæ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09254v1" data-paper-url="./papers/250909254v1-towards-better-dental-ai-a-multimodal-benchmark-and-instruction-data.html" onclick="toggleFavorite(this, '2509.09254v1', 'Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250909572v1-peftcd-leveraging-vision-foundation-models-with-parameter-efficient-.html">PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection</a></td>
  <td>PeftCDï¼šåˆ©ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œé¥æ„Ÿå˜åŒ–æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09572v1" data-paper-url="./papers/250909572v1-peftcd-leveraging-vision-foundation-models-with-parameter-efficient-.html" onclick="toggleFavorite(this, '2509.09572v1', 'PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250909290v1-modality-agnostic-input-channels-enable-segmentation-of-brain-lesion.html">Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training</a></td>
  <td>æå‡ºæ¨¡æ€æ— å…³è¾“å…¥é€šé“çš„U-Netï¼Œå®ç°å¤šæ¨¡æ€è„‘éƒ¨MRIç—…ç¶åˆ†å‰²ï¼Œæ— éœ€è®­ç»ƒæ—¶å¯è§åºåˆ—</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09290v1" data-paper-url="./papers/250909290v1-modality-agnostic-input-channels-enable-segmentation-of-brain-lesion.html" onclick="toggleFavorite(this, '2509.09290v1', 'Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250909190v1-vquala-2025-challenge-on-visual-quality-comparison-for-large-multimo.html">VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results</a></td>
  <td>VQualA 2025æŒ‘æˆ˜èµ›ï¼šè¯„ä¼°å¹¶æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰è´¨é‡æ¯”è¾ƒæ–¹é¢çš„èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09190v1" data-paper-url="./papers/250909190v1-vquala-2025-challenge-on-visual-quality-comparison-for-large-multimo.html" onclick="toggleFavorite(this, '2509.09190v1', 'VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250909263v1-date-dynamic-absolute-time-enhancement-for-long-video-understanding.html">DATE: Dynamic Absolute Time Enhancement for Long Video Understanding</a></td>
  <td>æå‡ºDATEæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ç»å¯¹æ—¶é—´å¢å¼ºæå‡MLLMåœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09263v1" data-paper-url="./papers/250909263v1-date-dynamic-absolute-time-enhancement-for-long-video-understanding.html" onclick="toggleFavorite(this, '2509.09263v1', 'DATE: Dynamic Absolute Time Enhancement for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250909151v1-video-understanding-by-design-how-datasets-shape-architectures-and-i.html">Video Understanding by Design: How Datasets Shape Architectures and Insights</a></td>
  <td>ä»æ•°æ®é›†è§†è§’è§£è¯»è§†é¢‘ç†è§£ï¼šæ­ç¤ºæ•°æ®é›†å¦‚ä½•å¡‘é€ æ¨¡å‹æ¶æ„ä¸æ´è§</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09151v1" data-paper-url="./papers/250909151v1-video-understanding-by-design-how-datasets-shape-architectures-and-i.html" onclick="toggleFavorite(this, '2509.09151v1', 'Video Understanding by Design: How Datasets Shape Architectures and Insights')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250909828v2-dgfusion-depth-guided-sensor-fusion-for-robust-semantic-perception.html">DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</a></td>
  <td>æå‡ºDGFusionï¼Œåˆ©ç”¨æ·±åº¦ä¿¡æ¯å¼•å¯¼ä¼ æ„Ÿå™¨èåˆï¼Œæå‡è¯­ä¹‰æ„ŸçŸ¥é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09828v2" data-paper-url="./papers/250909828v2-dgfusion-depth-guided-sensor-fusion-for-robust-semantic-perception.html" onclick="toggleFavorite(this, '2509.09828v2', 'DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250909324v1-fine-grained-customized-fashion-design-with-image-into-prompt-benchm.html">Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM</a></td>
  <td>æå‡ºåŸºäºLMMçš„å›¾åƒåˆ°æç¤ºå¾®è°ƒæœè£…è®¾è®¡æ¡†æ¶ï¼Œè§£å†³æ–‡æœ¬æè¿°ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09324v1" data-paper-url="./papers/250909324v1-fine-grained-customized-fashion-design-with-image-into-prompt-benchm.html" onclick="toggleFavorite(this, '2509.09324v1', 'Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250909064v1-enhancing-3d-medical-image-understanding-with-pretraining-aided-by-2.html">Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models</a></td>
  <td>Med3DInsightï¼šåˆ©ç”¨2Då¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¢å¼º3DåŒ»å­¦å›¾åƒç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09064v1" data-paper-url="./papers/250909064v1-enhancing-3d-medical-image-understanding-with-pretraining-aided-by-2.html" onclick="toggleFavorite(this, '2509.09064v1', 'Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250909666v3-unified-multimodal-model-as-auto-encoder.html">Unified Multimodal Model as Auto-Encoder</a></td>
  <td>æå‡ºåŸºäºè‡ªç¼–ç å™¨çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹UAEï¼Œå®ç°ç†è§£ä¸ç”Ÿæˆçš„åŒå‘æå‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09666v3" data-paper-url="./papers/250909666v3-unified-multimodal-model-as-auto-encoder.html" onclick="toggleFavorite(this, '2509.09666v3', 'Unified Multimodal Model as Auto-Encoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250909427v1-fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-.html">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a></td>
  <td>FS-Diffï¼šé¢å‘å¤šæ¨¡æ€å›¾åƒèåˆä¸è¶…åˆ†è¾¨ç‡çš„è¯­ä¹‰å¼•å¯¼å’Œæ¸…æ™°åº¦æ„ŸçŸ¥æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09427v1" data-paper-url="./papers/250909427v1-fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-.html" onclick="toggleFavorite(this, '2509.09427v1', 'FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250909286v1-visual-programmability-a-guide-for-code-as-thought-in-chart-understa.html">Visual Programmability: A Guide for Code-as-Thought in Chart Understanding</a></td>
  <td>æå‡ºVisual Programmabilityï¼Œè‡ªé€‚åº”é€‰æ‹©ä»£ç æ¨ç†æˆ–è§†è§‰æ¨ç†è§£å†³å›¾è¡¨ç†è§£ä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09286v1" data-paper-url="./papers/250909286v1-visual-programmability-a-guide-for-code-as-thought-in-chart-understa.html" onclick="toggleFavorite(this, '2509.09286v1', 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250909118v1-gradient-attention-guided-dual-masking-synergetic-framework-for-robu.html">Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval</a></td>
  <td>æå‡ºGA-DMSæ¡†æ¶ï¼Œé€šè¿‡æ¢¯åº¦æ³¨æ„åŠ›å¼•å¯¼çš„åŒæ©ç æœºåˆ¶æå‡æ–‡æœ¬è¡Œäººæ£€ç´¢æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09118v1" data-paper-url="./papers/250909118v1-gradient-attention-guided-dual-masking-synergetic-framework-for-robu.html" onclick="toggleFavorite(this, '2509.09118v1', 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250909555v1-interact-advancing-large-scale-versatile-3d-human-object-interaction.html">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></td>
  <td>InterActï¼šæå‡ºå¤§è§„æ¨¡é€šç”¨3Däºº-ç‰©äº¤äº’ç”Ÿæˆæ•°æ®é›†ä¸æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span> <span class="paper-tag">penetration</span> <span class="paper-tag">human-object interaction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09555v1" data-paper-url="./papers/250909555v1-interact-advancing-large-scale-versatile-3d-human-object-interaction.html" onclick="toggleFavorite(this, '2509.09555v1', 'InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250909667v1-geometric-neural-distance-fields-for-learning-human-motion-priors.html">Geometric Neural Distance Fields for Learning Human Motion Priors</a></td>
  <td>æå‡ºç¥ç»é»æ›¼è¿åŠ¨åœºï¼ˆNRMFï¼‰ï¼Œç”¨äºå­¦ä¹ é²æ£’ã€æ—¶åºä¸€è‡´ä¸”ç‰©ç†å¯ä¿¡çš„äººä½“è¿åŠ¨å…ˆéªŒã€‚</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09667v1" data-paper-url="./papers/250909667v1-geometric-neural-distance-fields-for-learning-human-motion-priors.html" onclick="toggleFavorite(this, '2509.09667v1', 'Geometric Neural Distance Fields for Learning Human Motion Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250909130v2-all-pet-a-low-resource-and-low-shot-pet-foundation-model-in-projecti.html">ALL-PET: A Low-resource and Low-shot PET Foundation Model in Projection Domain</a></td>
  <td>ALL-PETï¼šä¸€ç§ä½èµ„æºã€ä½æ ·æœ¬çš„æŠ•å½±åŸŸPETåŸºç¡€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09130v2" data-paper-url="./papers/250909130v2-all-pet-a-low-resource-and-low-shot-pet-foundation-model-in-projecti.html" onclick="toggleFavorite(this, '2509.09130v2', 'ALL-PET: A Low-resource and Low-shot PET Foundation Model in Projection Domain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250909792v2-loc2-interpretable-cross-view-localization-via-depth-lifted-local-fe.html">Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching</a></td>
  <td>æå‡ºLoc$^2$ï¼Œé€šè¿‡æ·±åº¦æå‡çš„å±€éƒ¨ç‰¹å¾åŒ¹é…å®ç°å¯è§£é‡Šçš„è·¨è§†è§’å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span> <span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09792v2" data-paper-url="./papers/250909792v2-loc2-interpretable-cross-view-localization-via-depth-lifted-local-fe.html" onclick="toggleFavorite(this, '2509.09792v2', 'Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250909067v3-improvement-of-human-object-interaction-action-recognition-using-sce.html">Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach</a></td>
  <td>æå‡ºç»“åˆåœºæ™¯ä¿¡æ¯çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œæå‡äººä¸å›ºå®šç‰©ä½“äº¤äº’è¡Œä¸ºè¯†åˆ«ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09067v3" data-paper-url="./papers/250909067v3-improvement-of-human-object-interaction-action-recognition-using-sce.html" onclick="toggleFavorite(this, '2509.09067v3', 'Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)