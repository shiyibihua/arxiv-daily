---
layout: default
title: DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception
---

# DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09828" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09828v2</a>
  <a href="https://arxiv.org/pdf/2509.09828.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09828v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09828v2', 'DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tim Broedermannn, Christos Sakaridis, Luigi Piccinelli, Wim Abbeloos, Luc Van Gool

**åˆ†ç±»**: cs.CV, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-12-03)

**å¤‡æ³¨**: Code and models will be available at https://github.com/timbroed/DGFusion

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/timbroed/DGFusion)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDGFusionï¼Œåˆ©ç”¨æ·±åº¦ä¿¡æ¯å¼•å¯¼ä¼ æ„Ÿå™¨èåˆï¼Œæå‡è¯­ä¹‰æ„ŸçŸ¥é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ ` `è¯­ä¹‰åˆ†å‰²` `å…¨æ™¯åˆ†å‰²` `è‡ªåŠ¨é©¾é©¶` `ä¼ æ„Ÿå™¨èåˆ` `æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¼ æ„Ÿå™¨èåˆæ–¹æ³•åœ¨ç©ºé—´ä¸Šå‡åŒ€å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®ï¼Œåœ¨å¤æ‚æ¡ä»¶ä¸‹æ€§èƒ½å—é™ã€‚
2. DGFusionåˆ©ç”¨æ·±åº¦ä¿¡æ¯å¼•å¯¼ä¼ æ„Ÿå™¨èåˆï¼Œå­¦ä¹ æ·±åº¦æ„ŸçŸ¥ç‰¹å¾ï¼ŒåŠ¨æ€è°ƒæ•´ä¼ æ„Ÿå™¨èåˆç­–ç•¥ã€‚
3. åœ¨MUSESå’ŒDeLiVERæ•°æ®é›†ä¸Šï¼ŒDGFusionå®ç°äº†æœ€å…ˆè¿›çš„å…¨æ™¯å’Œè¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„é²æ£’è¯­ä¹‰æ„ŸçŸ¥ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦å¼•å¯¼å¤šæ¨¡æ€èåˆæ–¹æ³•DGFusionï¼Œé€šè¿‡æ•´åˆæ·±åº¦ä¿¡æ¯æ¥æ”¹è¿›æ¡ä»¶æ„ŸçŸ¥èåˆã€‚è¯¥ç½‘ç»œå°†å¤šæ¨¡æ€åˆ†å‰²è§†ä¸ºå¤šä»»åŠ¡é—®é¢˜ï¼Œåˆ©ç”¨æ¿€å…‰é›·è¾¾æµ‹é‡æ•°æ®ä½œä¸ºæ¨¡å‹è¾“å…¥å’Œæ·±åº¦å­¦ä¹ çš„çœŸå€¼ã€‚è¾…åŠ©æ·±åº¦å¤´æœ‰åŠ©äºå­¦ä¹ æ·±åº¦æ„ŸçŸ¥ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾è¢«ç¼–ç ä¸ºç©ºé—´å˜åŒ–çš„å±€éƒ¨æ·±åº¦ä»¤ç‰Œï¼Œä»è€Œè°ƒèŠ‚æ³¨æ„åŠ›è·¨æ¨¡æ€èåˆã€‚ç»“åˆå…¨å±€æ¡ä»¶ä»¤ç‰Œï¼Œè¿™äº›å±€éƒ¨æ·±åº¦ä»¤ç‰ŒåŠ¨æ€åœ°è°ƒæ•´ä¼ æ„Ÿå™¨èåˆï¼Œä»¥é€‚åº”åœºæ™¯ä¸­æ¯ä¸ªä¼ æ„Ÿå™¨åœ¨ç©ºé—´ä¸Šå˜åŒ–çš„å¯é æ€§ï¼Œè¿™ä¸»è¦å–å†³äºæ·±åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é²æ£’çš„æ·±åº¦æŸå¤±å‡½æ•°ï¼Œè¿™å¯¹äºä»æ¶åŠ£æ¡ä»¶ä¸‹é€šå¸¸ç¨€ç–ä¸”å˜ˆæ‚çš„æ¿€å…‰é›·è¾¾è¾“å…¥ä¸­å­¦ä¹ è‡³å…³é‡è¦ã€‚è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MUSESå’ŒDeLiVERæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å…¨æ™¯å’Œè¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè‡ªåŠ¨é©¾é©¶çš„è¯­ä¹‰æ„ŸçŸ¥ä¾èµ–äºå¤šä¼ æ„Ÿå™¨èåˆï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒä¼ æ„Ÿå™¨æ•°æ®æ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç©ºé—´ä¸Šå‡åŒ€çš„å¤„ç†æ–¹å¼ï¼Œå¿½ç•¥äº†ä¼ æ„Ÿå™¨åœ¨ä¸åŒåŒºåŸŸçš„å¯é æ€§å·®å¼‚ã€‚å°¤å…¶æ˜¯åœ¨æ¶åŠ£å¤©æ°”æˆ–å…‰ç…§æ¡ä»¶ä¸‹ï¼ŒæŸäº›ä¼ æ„Ÿå™¨çš„æ•°æ®è´¨é‡ä¼šæ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´èåˆç»“æœä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•æ ¹æ®åœºæ™¯æ¡ä»¶åŠ¨æ€è°ƒæ•´ä¼ æ„Ÿå™¨èåˆç­–ç•¥ï¼Œæ˜¯æå‡è¯­ä¹‰æ„ŸçŸ¥é²æ£’æ€§çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDGFusionçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦ä¿¡æ¯æ¥å¼•å¯¼ä¼ æ„Ÿå™¨èåˆã€‚æ·±åº¦ä¿¡æ¯å¯ä»¥åæ˜ åœºæ™¯çš„å‡ ä½•ç»“æ„å’Œä¼ æ„Ÿå™¨ä¸ç‰©ä½“çš„è·ç¦»ï¼Œä»è€Œå¸®åŠ©åˆ¤æ–­ä¸åŒä¼ æ„Ÿå™¨åœ¨ä¸åŒåŒºåŸŸçš„å¯é æ€§ã€‚é€šè¿‡å­¦ä¹ æ·±åº¦æ„ŸçŸ¥ç‰¹å¾ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºç©ºé—´å˜åŒ–çš„å±€éƒ¨æ·±åº¦ä»¤ç‰Œï¼Œå¯ä»¥åŠ¨æ€åœ°è°ƒæ•´è·¨æ¨¡æ€èåˆçš„æƒé‡ï¼Œä½¿æ¨¡å‹æ›´åŠ å…³æ³¨å¯é çš„ä¼ æ„Ÿå™¨æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDGFusionçš„ç½‘ç»œæ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥ï¼šæ¥æ”¶æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„è¾“å…¥æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’Œæ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚2) æ·±åº¦é¢„æµ‹å¤´ï¼šåˆ©ç”¨æ¿€å…‰é›·è¾¾æ•°æ®ä½œä¸ºçœŸå€¼ï¼Œå­¦ä¹ é¢„æµ‹åœºæ™¯æ·±åº¦ã€‚3) æ·±åº¦æ„ŸçŸ¥ç‰¹å¾æå–ï¼šä»å¤šæ¨¡æ€è¾“å…¥ä¸­æå–æ·±åº¦æ„ŸçŸ¥ç‰¹å¾ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºç©ºé—´å˜åŒ–çš„å±€éƒ¨æ·±åº¦ä»¤ç‰Œã€‚4) æ³¨æ„åŠ›è·¨æ¨¡æ€èåˆï¼šåˆ©ç”¨å±€éƒ¨æ·±åº¦ä»¤ç‰Œå’Œå…¨å±€æ¡ä»¶ä»¤ç‰Œï¼ŒåŠ¨æ€åœ°è°ƒæ•´è·¨æ¨¡æ€èåˆçš„æƒé‡ã€‚5) åˆ†å‰²å¤´ï¼šè¾“å‡ºè¯­ä¹‰åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šDGFusionçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†æ·±åº¦å¼•å¯¼çš„ä¼ æ„Ÿå™¨èåˆæ–¹æ³•ï¼Œåˆ©ç”¨æ·±åº¦ä¿¡æ¯åŠ¨æ€è°ƒæ•´èåˆç­–ç•¥ã€‚2) è®¾è®¡äº†æ·±åº¦æ„ŸçŸ¥ç‰¹å¾æå–æ¨¡å—ï¼Œå­¦ä¹ æ·±åº¦æ„ŸçŸ¥çš„å±€éƒ¨ç‰¹å¾è¡¨ç¤ºã€‚3) å¼•å…¥äº†é²æ£’çš„æ·±åº¦æŸå¤±å‡½æ•°ï¼Œç”¨äºä»ç¨€ç–å’Œå™ªå£°çš„æ¿€å…‰é›·è¾¾æ•°æ®ä¸­å­¦ä¹ æ·±åº¦ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šDGFusionçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformerç»“æ„è¿›è¡Œè·¨æ¨¡æ€èåˆï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ ä¸åŒä¼ æ„Ÿå™¨ä¹‹é—´çš„å…³ç³»ã€‚2) è®¾è®¡äº†å±€éƒ¨æ·±åº¦ä»¤ç‰Œå’Œå…¨å±€æ¡ä»¶ä»¤ç‰Œï¼Œç”¨äºåŠ¨æ€è°ƒæ•´èåˆæƒé‡ã€‚3) é‡‡ç”¨äº†HuberæŸå¤±å‡½æ•°ä½œä¸ºæ·±åº¦æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜å¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§ã€‚4) å°†å¤šæ¨¡æ€åˆ†å‰²é—®é¢˜å»ºæ¨¡ä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼ŒåŒæ—¶é¢„æµ‹è¯­ä¹‰åˆ†å‰²ã€å…¨æ™¯åˆ†å‰²å’Œæ·±åº¦ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DGFusionåœ¨MUSESå’ŒDeLiVERæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨MUSESæ•°æ®é›†ä¸Šï¼ŒDGFusionçš„å…¨æ™¯åˆ†å‰²è´¨é‡ï¼ˆPQï¼‰è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œåœ¨DeLiVERæ•°æ®é›†ä¸Šï¼ŒDGFusionçš„è¯­ä¹‰åˆ†å‰²ç²¾åº¦ï¼ˆmIoUï¼‰ä¹Ÿè¾¾åˆ°äº†æ–°çš„é«˜åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGFusionèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ·±åº¦ä¿¡æ¯å¼•å¯¼ä¼ æ„Ÿå™¨èåˆï¼Œæé«˜è¯­ä¹‰æ„ŸçŸ¥çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DGFusionå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è¯­ä¹‰æ„ŸçŸ¥é²æ£’æ€§ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå¢å¼ºæœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œå¹¶æ”¹å–„æ™ºèƒ½ç›‘æ§ç³»ç»Ÿçš„ç›®æ ‡æ£€æµ‹å’Œè¯†åˆ«æ€§èƒ½ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­çš„éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion

