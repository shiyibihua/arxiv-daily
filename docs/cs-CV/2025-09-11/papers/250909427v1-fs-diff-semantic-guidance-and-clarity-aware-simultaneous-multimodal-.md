---
layout: default
title: FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution
---

# FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09427" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09427v1</a>
  <a href="https://arxiv.org/pdf/2509.09427.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09427v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09427v1', 'FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchan Jie, Yushen Xu, Xiaosong Li, Fuqiang Zhou, Jianming Lv, Huafeng Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**æœŸåˆŠ**: Information Fusion, 2025, 121: 103146

**DOI**: [10.1016/j.inffus.2025.103146](https://doi.org/10.1016/j.inffus.2025.103146)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/XylonXu01/FS-Diff)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FS-Diffï¼šé¢å‘å¤šæ¨¡æ€å›¾åƒèåˆä¸è¶…åˆ†è¾¨ç‡çš„è¯­ä¹‰å¼•å¯¼å’Œæ¸…æ™°åº¦æ„ŸçŸ¥æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å›¾åƒèåˆ` `å›¾åƒè¶…åˆ†è¾¨ç‡` `æ‰©æ•£æ¨¡å‹` `è¯­ä¹‰å¼•å¯¼` `æ¸…æ™°åº¦æ„ŸçŸ¥` `æ¡ä»¶ç”Ÿæˆ` `Mamba` `èˆªç©ºè§†å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è”åˆå›¾åƒèåˆä¸è¶…åˆ†è¾¨ç‡æ–¹æ³•åœ¨å¤„ç†ä½åˆ†è¾¨ç‡ã€å¼±è¯­ä¹‰ä¿¡æ¯çš„å¤šæ¨¡æ€å›¾åƒæ—¶æ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡å’ŒèƒŒæ™¯ç»“æ„æ˜“å—æŸçš„æƒ…å†µä¸‹ã€‚
2. FS-Diffçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒèåˆå’Œè¶…åˆ†è¾¨ç‡è§†ä¸ºä¸€ä¸ªæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼å’Œæ¸…æ™°åº¦æ„ŸçŸ¥æœºåˆ¶ï¼Œå®ç°è‡ªé€‚åº”çš„è·¨æ¨¡æ€ç‰¹å¾æå–å’Œé«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFS-Diffåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæ¢å¤æ›´ä¸°å¯Œçš„ç»†èŠ‚å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è‡ªå»ºçš„AVMSæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰å¼•å¯¼å’Œæ¸…æ™°åº¦æ„ŸçŸ¥çš„è”åˆå›¾åƒèåˆä¸è¶…åˆ†è¾¨ç‡æ–¹æ³•FS-Diffã€‚é’ˆå¯¹å†›äº‹ä¾¦å¯Ÿå’Œè¿œç¨‹æ¢æµ‹ç­‰å®é™…åº”ç”¨ä¸­ï¼Œå¤šæ¨¡æ€å›¾åƒçš„ç›®æ ‡å’ŒèƒŒæ™¯ç»“æ„æ˜“å—æŸã€åˆ†è¾¨ç‡ä½ã€è¯­ä¹‰ä¿¡æ¯å¼±çš„é—®é¢˜ï¼ŒFS-Diffå°†å›¾åƒèåˆå’Œè¶…åˆ†è¾¨ç‡ç»Ÿä¸€ä¸ºæ¡ä»¶ç”Ÿæˆé—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¸…æ™°åº¦æ„ŸçŸ¥æœºåˆ¶è¿›è¡Œè‡ªé€‚åº”ä½åˆ†è¾¨ç‡æ„ŸçŸ¥å’Œè·¨æ¨¡æ€ç‰¹å¾æå–ï¼Œå¹¶å¼•å…¥åŒå‘ç‰¹å¾Mambaæå–å¤šæ¨¡æ€å›¾åƒçš„å…¨å±€ç‰¹å¾ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æºå›¾åƒå’Œè¯­ä¹‰ä½œä¸ºæ¡ä»¶ï¼Œé€šè¿‡æ”¹è¿›çš„U-Netç½‘ç»œå®ç°éšæœºè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œè¯¥ç½‘ç»œç»è¿‡å¤šå™ªå£°æ°´å¹³çš„å»å™ªè®­ç»ƒï¼Œç”Ÿæˆå…·æœ‰è·¨æ¨¡æ€ç‰¹å¾å’Œä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„é«˜åˆ†è¾¨ç‡èåˆç»“æœã€‚åŒæ—¶ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«600å¯¹å›¾åƒçš„èˆªç©ºè§†å›¾å¤šåœºæ™¯ï¼ˆAVMSï¼‰åŸºå‡†æ•°æ®é›†ã€‚åœ¨å…­ä¸ªå…¬å…±æ•°æ®é›†å’ŒAVMSæ•°æ®é›†ä¸Šçš„å¤§é‡è”åˆå›¾åƒèåˆå’Œè¶…åˆ†è¾¨ç‡å®éªŒè¡¨æ˜ï¼ŒFS-Diffåœ¨å¤šä¸ªæ”¾å¤§å€æ•°ä¸‹ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶èƒ½æ¢å¤èåˆå›¾åƒä¸­æ›´ä¸°å¯Œçš„ç»†èŠ‚å’Œè¯­ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å›¾åƒèåˆä¸è¶…åˆ†è¾¨ç‡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½åˆ†è¾¨ç‡å’Œå¼±è¯­ä¹‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯å¹¶é‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¯¼è‡´èåˆç»“æœç»†èŠ‚ä¸¢å¤±å’Œè¯­ä¹‰ä¿¡æ¯ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸æ— æ³•å¾ˆå¥½åœ°å¤„ç†çœŸå®åœºæ™¯ä¸­å›¾åƒè´¨é‡å·®ã€æ¨¡æ€å·®å¼‚å¤§çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å›¾åƒèåˆå’Œè¶…åˆ†è¾¨ç‡é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹ä»å™ªå£°ä¸­é€æ­¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„èåˆå›¾åƒã€‚åŒæ—¶ï¼Œå¼•å…¥è¯­ä¹‰å¼•å¯¼å’Œæ¸…æ™°åº¦æ„ŸçŸ¥æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæå‡èåˆç»“æœçš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFS-Diffçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ¸…æ™°åº¦æ„ŸçŸ¥æ¨¡å—ï¼šç”¨äºè¯„ä¼°è¾“å…¥å›¾åƒçš„æ¸…æ™°åº¦ï¼Œå¹¶æ ¹æ®æ¸…æ™°åº¦è‡ªé€‚åº”åœ°æå–ä½åˆ†è¾¨ç‡ç‰¹å¾ã€‚2) åŒå‘ç‰¹å¾Mambaæ¨¡å—ï¼šç”¨äºæå–å¤šæ¨¡æ€å›¾åƒçš„å…¨å±€ç‰¹å¾ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚3) æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼šä»¥æºå›¾åƒå’Œè¯­ä¹‰ä¿¡æ¯ä½œä¸ºæ¡ä»¶ï¼Œé€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡èåˆå›¾åƒã€‚è¯¥æ¨¡å‹åŸºäºæ”¹è¿›çš„U-Netç»“æ„ï¼Œå¹¶é’ˆå¯¹å¤šå™ªå£°æ°´å¹³è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è¯­ä¹‰å¼•å¯¼å’Œæ¸…æ™°åº¦æ„ŸçŸ¥æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†ä½åˆ†è¾¨ç‡å’Œå¼±è¯­ä¹‰ä¿¡æ¯çš„å¤šæ¨¡æ€å›¾åƒã€‚2) å¼•å…¥äº†åŒå‘ç‰¹å¾Mambaæ¨¡å—ï¼Œæœ‰æ•ˆæå–äº†å¤šæ¨¡æ€å›¾åƒçš„å…¨å±€ç‰¹å¾ã€‚3) å°†å›¾åƒèåˆå’Œè¶…åˆ†è¾¨ç‡é—®é¢˜è½¬åŒ–ä¸ºæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„èåˆå›¾åƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†æ”¹è¿›çš„U-Netä½œä¸ºæ‰©æ•£æ¨¡å‹çš„éª¨å¹²ç½‘ç»œï¼Œå¹¶é’ˆå¯¹å¤šå™ªå£°æ°´å¹³è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†L1æŸå¤±å’Œæ„ŸçŸ¥æŸå¤±ï¼ˆPerceptual Lossï¼‰æ¥çº¦æŸç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æ¸…æ™°åº¦æ„ŸçŸ¥æ¨¡å—çš„è®¾è®¡ç»†èŠ‚æœªçŸ¥ï¼Œä½†æ¨æµ‹å¯èƒ½ä½¿ç”¨äº†å›¾åƒæ¢¯åº¦æˆ–é¢‘ç‡åŸŸåˆ†æç­‰æ–¹æ³•æ¥è¯„ä¼°å›¾åƒçš„æ¸…æ™°åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FS-Diffåœ¨å…­ä¸ªå…¬å…±æ•°æ®é›†å’Œè‡ªå»ºçš„AVMSæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒFS-Diffåœ¨å¤šä¸ªæ”¾å¤§å€æ•°ä¸‹ä¼˜äºç°æœ‰çš„å›¾åƒèåˆå’Œè¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒFS-Diffåœ¨PSNRå’ŒSSIMç­‰æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¢å¤æ›´ä¸°å¯Œçš„ç»†èŠ‚å’Œè¯­ä¹‰ä¿¡æ¯ã€‚å°¤å…¶æ˜¯åœ¨AVMSæ•°æ®é›†ä¸Šï¼ŒFS-Diffçš„è¡¨ç°æ›´åŠ å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FS-Diffå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å†›äº‹ä¾¦å¯Ÿã€é¥æ„Ÿå›¾åƒåˆ†æã€åŒ»å­¦å›¾åƒå¤„ç†ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå¸¸å¸¸éœ€è¦èåˆæ¥è‡ªä¸åŒä¼ æ„Ÿå™¨æˆ–æ¨¡æ€çš„å›¾åƒï¼Œå¹¶æé«˜å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œä»¥è·å–æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚FS-Diffèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è¿™äº›é—®é¢˜ï¼Œæé«˜å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.

