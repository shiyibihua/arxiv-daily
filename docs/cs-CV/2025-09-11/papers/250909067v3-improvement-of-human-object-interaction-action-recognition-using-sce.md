---
layout: default
title: Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach
---

# Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09067" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09067v3</a>
  <a href="https://arxiv.org/pdf/2509.09067.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09067v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09067v3', 'Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hesham M. Shehata, Mohammad Abdolrahmani

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-09-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆåœºæ™¯ä¿¡æ¯çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œæå‡äººä¸å›ºå®šç‰©ä½“äº¤äº’è¡Œä¸ºè¯†åˆ«ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `è¡Œä¸ºè¯†åˆ«` `å›¾å·ç§¯ç½‘ç»œ` `å¤šä»»åŠ¡å­¦ä¹ ` `åœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰GCNsåœ¨äººä½“è¡Œä¸ºè¯†åˆ«ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯¹åœºæ™¯ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´äººä¸ç‰©ä½“äº¤äº’è¯†åˆ«ç²¾åº¦ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆç¯å¢ƒä¸­å›ºå®šç‰©ä½“çš„ä¿¡æ¯ï¼Œæå‡äººä¸ç‰©ä½“äº¤äº’è¡Œä¸ºçš„è¯†åˆ«æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ…å«äº¤äº’å’Œéäº¤äº’è¡Œä¸ºçš„æ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡è¾¾åˆ°99.25%ï¼Œä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨åœºæ™¯ä¿¡æ¯å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•æ¥æ”¹è¿›äººä¸ç‰©ä½“äº¤äº’è¡Œä¸ºè¯†åˆ«æ€§èƒ½çš„æ–¹æ¡ˆã€‚ç°æœ‰çš„å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆGCNsï¼‰åœ¨äººä½“è¡Œä¸ºè¯†åˆ«é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºç¼ºä¹å¯¹åœºæ™¯ä¿¡æ¯çš„æœ‰æ•ˆè¡¨ç¤ºå’Œåˆé€‚çš„å­¦ä¹ æ¶æ„ï¼Œåœ¨æ£€æµ‹äººä¸ç‰©ä½“äº¤äº’è¡Œä¸ºæ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä»å…¬å…±ç¯å¢ƒä¸­æ”¶é›†äº†çœŸå®æ•°æ®ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸å›ºå®šç‰©ä½“äº¤äº’ï¼ˆä¾‹å¦‚ï¼ŒATMå”®ç¥¨æœºã€å…¥ä½/é€€æˆ¿æœºç­‰ï¼‰ä»¥åŠè¡Œèµ°å’Œç«™ç«‹ç­‰éäº¤äº’ç±»åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆäº¤äº’åŒºåŸŸä¿¡æ¯çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•æˆåŠŸåœ°è¯†åˆ«äº†æ‰€ç ”ç©¶çš„äº¤äº’å’Œéäº¤äº’è¡Œä¸ºï¼Œå‡†ç¡®ç‡è¾¾åˆ°99.25%ï¼Œæ¯”ä»…ä½¿ç”¨äººä½“éª¨éª¼å§¿åŠ¿çš„åŸºçº¿æ¨¡å‹æé«˜äº†2.75%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºäººä½“éª¨éª¼å§¿åŠ¿çš„å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆGCNsï¼‰åœ¨äººä¸ç‰©ä½“äº¤äº’è¡Œä¸ºè¯†åˆ«æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºç¼ºä¹å¯¹åœºæ™¯ä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¯å¢ƒä¸­ç‰©ä½“çš„ä¿¡æ¯æ¥è¾…åŠ©è¡Œä¸ºè¯†åˆ«ï¼Œå¯¼è‡´äº¤äº’è¡Œä¸ºçš„è¯†åˆ«ç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åœºæ™¯ä¿¡æ¯èå…¥åˆ°äººä½“è¡Œä¸ºè¯†åˆ«æ¨¡å‹ä¸­ï¼Œå¹¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹æ³•ã€‚é€šè¿‡åŒæ—¶å­¦ä¹ äº¤äº’è¡Œä¸ºå’Œéäº¤äº’è¡Œä¸ºï¼Œä»¥åŠåˆ©ç”¨äº¤äº’åŒºåŸŸçš„ä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£äººä¸ç‰©ä½“ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜äº¤äº’è¡Œä¸ºçš„è¯†åˆ«ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) æ•°æ®é‡‡é›†ä¸æ ‡æ³¨ï¼šæ”¶é›†åŒ…å«äººä¸å›ºå®šç‰©ä½“äº¤äº’è¡Œä¸ºçš„è§†é¢‘æ•°æ®ï¼Œå¹¶è¿›è¡Œæ ‡æ³¨ã€‚2) ç‰¹å¾æå–ï¼šæå–äººä½“éª¨éª¼å§¿åŠ¿ç‰¹å¾å’Œåœºæ™¯ä¸­çš„ç‰©ä½“ä¿¡æ¯ç‰¹å¾ã€‚3) å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹æ„å»ºï¼šæ„å»ºä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹ï¼ŒåŒæ—¶å­¦ä¹ äº¤äº’è¡Œä¸ºå’Œéäº¤äº’è¡Œä¸ºçš„åˆ†ç±»ã€‚4) äº¤äº’åŒºåŸŸä¿¡æ¯èåˆï¼šå°†äº¤äº’åŒºåŸŸçš„ä¿¡æ¯èå…¥åˆ°æ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹äº¤äº’è¡Œä¸ºçš„ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†åœºæ™¯ä¿¡æ¯å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œç”¨äºäººä¸ç‰©ä½“äº¤äº’è¡Œä¸ºè¯†åˆ«ã€‚ä¸ä¼ ç»Ÿçš„ä»…ä½¿ç”¨äººä½“éª¨éª¼å§¿åŠ¿çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ç¯å¢ƒä¸­çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜è¯†åˆ«ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶èƒ½å¤ŸåŒæ—¶å­¦ä¹ äº¤äº’è¡Œä¸ºå’Œéäº¤äº’è¡Œä¸ºï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹ä¸­ï¼Œé‡‡ç”¨äº†å…±äº«ç‰¹å¾æå–å±‚å’Œç‹¬ç«‹çš„åˆ†ç±»å±‚ã€‚å…±äº«ç‰¹å¾æå–å±‚ç”¨äºæå–äººä½“éª¨éª¼å§¿åŠ¿å’Œåœºæ™¯ä¿¡æ¯çš„é€šç”¨ç‰¹å¾ï¼Œç‹¬ç«‹çš„åˆ†ç±»å±‚ç”¨äºåˆ†åˆ«å¯¹äº¤äº’è¡Œä¸ºå’Œéäº¤äº’è¡Œä¸ºè¿›è¡Œåˆ†ç±»ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¹¶å¯¹ä¸åŒä»»åŠ¡çš„æŸå¤±è¿›è¡ŒåŠ æƒï¼Œä»¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„å­¦ä¹ éš¾åº¦ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å®é™…æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ç»“åˆäº¤äº’åŒºåŸŸä¿¡æ¯ï¼Œåœ¨æ‰€æ„å»ºçš„æ•°æ®é›†ä¸Šå®ç°äº†99.25%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”äºä»…ä½¿ç”¨äººä½“éª¨éª¼å§¿åŠ¿çš„åŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†2.75%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†åœºæ™¯ä¿¡æ¯å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•åœ¨äººä¸ç‰©ä½“äº¤äº’è¡Œä¸ºè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€æ™ºèƒ½å®¶å±…ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•è¯†åˆ«ç”¨æˆ·åœ¨ATMæœºä¸Šçš„æ“ä½œè¡Œä¸ºï¼Œä»è€Œæé«˜å®‰å…¨æ€§ã€‚åœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•è¯†åˆ«ç”¨æˆ·ä¸å®¶ç”µçš„äº¤äº’è¡Œä¸ºï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„æ§åˆ¶ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººè¾…åŠ©åŒ»ç–—ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent graph convolutional neural networks (GCNs) have shown high performance in the field of human action recognition by using human skeleton poses. However, it fails to detect human-object interaction cases successfully due to the lack of effective representation of the scene information and appropriate learning architectures. In this context, we propose a methodology to utilize human action recognition performance by considering fixed object information in the environment and following a multi-task learning approach. In order to evaluate the proposed method, we collected real data from public environments and prepared our data set, which includes interaction classes of hands-on fixed objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and non-interaction classes of walking and standing. The multi-task learning approach, along with interaction area information, succeeds in recognizing the studied interaction and non-interaction actions with an accuracy of 99.25%, outperforming the accuracy of the base model using only human skeleton poses by 2.75%.

