---
layout: default
title: Visual Grounding from Event Cameras
---

# Visual Grounding from Event Cameras

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09584" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09584v1</a>
  <a href="https://arxiv.org/pdf/2509.09584.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09584v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09584v1', 'Visual Grounding from Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: Abstract Paper (Non-Archival) @ ICCV 2025 NeVi Workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTalk2Eventï¼Œé¦–ä¸ªåŸºäºäº‹ä»¶ç›¸æœºçš„è¯­è¨€é©±åŠ¨ç‰©ä½“å®šä½å¤§è§„æ¨¡åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `è§†è§‰å®šä½` `è‡ªç„¶è¯­è¨€ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `åŠ¨æ€åœºæ™¯` `é©¾é©¶åœºæ™¯` `å±æ€§æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆç»“åˆäº‹ä»¶ç›¸æœºæ•°æ®ä¸è‡ªç„¶è¯­è¨€ç†è§£ï¼Œé™åˆ¶äº†åœ¨åŠ¨æ€åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºTalk2EventåŸºå‡†ï¼Œé€šè¿‡å±æ€§ä¸°å¯Œçš„æŒ‡ç§°è¡¨è¾¾ï¼Œæ˜¾å¼æ•æ‰ç©ºé—´ã€æ—¶é—´å’Œå…³ç³»çº¿ç´¢ï¼Œä¿ƒè¿›ä¸Šä¸‹æ–‡æ¨ç†ã€‚
3. Talk2EventåŒ…å«å¤§è§„æ¨¡çœŸå®é©¾é©¶åœºæ™¯æ•°æ®ï¼Œä¸ºæ¨è¿›æœºå™¨äººã€äººæœºäº¤äº’ç­‰é¢†åŸŸçš„æ—¶åºæ„ŸçŸ¥ç ”ç©¶å¥ å®šåŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶ç›¸æœºä»¥å¾®ç§’çº§çš„ç²¾åº¦æ•æ‰äº®åº¦å˜åŒ–ï¼Œåœ¨è¿åŠ¨æ¨¡ç³Šå’Œå¤æ‚å…‰ç…§æ¡ä»¶ä¸‹ä¾ç„¶å¯é ï¼Œä¸ºå»ºæ¨¡é«˜åŠ¨æ€åœºæ™¯æä¾›äº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œäº‹ä»¶ç›¸æœºä¸è‡ªç„¶è¯­è¨€ç†è§£çš„ç»“åˆå´é²œæœ‰ç ”ç©¶ï¼Œé€ æˆäº†å¤šæ¨¡æ€æ„ŸçŸ¥é¢†åŸŸçš„ç©ºç™½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Talk2Eventï¼Œè¿™æ˜¯é¦–ä¸ªä½¿ç”¨äº‹ä»¶æ•°æ®è¿›è¡Œè¯­è¨€é©±åŠ¨ç‰©ä½“å®šä½çš„å¤§è§„æ¨¡åŸºå‡†ã€‚Talk2Eventæ„å»ºäºçœŸå®é©¾é©¶åœºæ™¯ä¹‹ä¸Šï¼ŒåŒ…å«5567ä¸ªåœºæ™¯ï¼Œ13458ä¸ªæ ‡æ³¨ç‰©ä½“ï¼Œä»¥åŠè¶…è¿‡30000ä¸ªç»è¿‡ç²¾å¿ƒéªŒè¯çš„æŒ‡ç§°è¡¨è¾¾ã€‚æ¯ä¸ªè¡¨è¾¾éƒ½é€šè¿‡å››ä¸ªç»“æ„åŒ–å±æ€§è¿›è¡Œäº†ä¸°å¯Œï¼šå¤–è§‚ã€çŠ¶æ€ã€ä¸è§‚å¯Ÿè€…çš„å…³ç³»ä»¥åŠä¸å‘¨å›´ç‰©ä½“çš„å…³ç³»ï¼Œè¿™äº›å±æ€§æ˜¾å¼åœ°æ•æ‰äº†ç©ºé—´ã€æ—¶é—´å’Œå…³ç³»çº¿ç´¢ã€‚è¿™ç§ä»¥å±æ€§ä¸ºä¸­å¿ƒçš„è®¾è®¡æ”¯æŒå¯è§£é‡Šçš„å’Œç»„åˆçš„å®šä½ï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œè¶…è¶Šç®€å•ç‰©ä½“è¯†åˆ«çš„ã€åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†çš„åˆ†æã€‚æˆ‘ä»¬æœŸæœ›Talk2Eventèƒ½å¤Ÿæˆä¸ºæ¨è¿›å¤šæ¨¡æ€å’Œæ—¶åºæ„ŸçŸ¥çš„åŸºç¡€ï¼Œå…¶åº”ç”¨èŒƒå›´æ¶µç›–æœºå™¨äººã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜åŠ¨æ€åœºæ™¯ä¸‹çš„ç‰©ä½“å®šä½ä»»åŠ¡æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨äº‹ä»¶ç›¸æœºæä¾›çš„é«˜æ—¶é—´åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œå¹¶ä¸”ç¼ºä¹å¤§è§„æ¨¡çš„ã€å¸¦æœ‰ä¸°å¯Œè¯­è¨€æè¿°çš„äº‹ä»¶æ•°æ®åŸºå‡†ï¼Œé˜»ç¢äº†å¤šæ¨¡æ€æ„ŸçŸ¥æ¨¡å‹çš„å‘å±•ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†ï¼Œæ— æ³•å……åˆ†ç†è§£ç‰©ä½“é—´çš„å…³ç³»å’ŒçŠ¶æ€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„ã€å±æ€§ä¸°å¯Œçš„äº‹ä»¶æ•°æ®åŸºå‡†ï¼Œå³Talk2Eventã€‚é€šè¿‡æä¾›åŒ…å«å¤–è§‚ã€çŠ¶æ€ã€ä¸è§‚å¯Ÿè€…å…³ç³»ä»¥åŠç‰©ä½“é—´å…³ç³»çš„æŒ‡ç§°è¡¨è¾¾ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®ã€æ›´å¯è§£é‡Šçš„ç‰©ä½“å®šä½ã€‚è¿™ç§è®¾è®¡é¼“åŠ±æ¨¡å‹è¿›è¡Œç»„åˆæ¨ç†ï¼Œç†è§£åœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTalk2EventåŸºå‡†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé‡‡é›†çœŸå®é©¾é©¶åœºæ™¯çš„äº‹ä»¶æ•°æ®ï¼›ç„¶åï¼Œå¯¹åœºæ™¯ä¸­çš„ç‰©ä½“è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æŒ‡ç§°è¡¨è¾¾ï¼›æ¥ç€ï¼Œå¯¹æŒ‡ç§°è¡¨è¾¾è¿›è¡Œå±æ€§æ ‡æ³¨ï¼ŒåŒ…æ‹¬å¤–è§‚ã€çŠ¶æ€ã€ä¸è§‚å¯Ÿè€…å…³ç³»ä»¥åŠç‰©ä½“é—´å…³ç³»ï¼›æœ€åï¼Œå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—å’ŒéªŒè¯ï¼Œç¡®ä¿æ•°æ®çš„è´¨é‡ã€‚è¯¥åŸºå‡†æä¾›äº†ä¸€å¥—æ ‡å‡†çš„æ•°æ®æ ¼å¼å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œæ–¹ä¾¿ç ”ç©¶è€…è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šTalk2Eventçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å±æ€§ä¸°å¯Œçš„æŒ‡ç§°è¡¨è¾¾ã€‚ä¸ä¼ ç»Ÿçš„æŒ‡ç§°è¡¨è¾¾ç›¸æ¯”ï¼ŒTalk2Eventçš„æŒ‡ç§°è¡¨è¾¾ä¸ä»…æè¿°äº†ç‰©ä½“çš„å¤–è§‚ï¼Œè¿˜æè¿°äº†ç‰©ä½“çš„çŠ¶æ€ã€ä¸è§‚å¯Ÿè€…çš„å…³ç³»ä»¥åŠç‰©ä½“é—´çš„å…³ç³»ã€‚è¿™ç§å±æ€§åŒ–çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®ã€æ›´å¯è§£é‡Šçš„ç‰©ä½“å®šä½ã€‚æ­¤å¤–ï¼ŒTalk2Eventæ˜¯é¦–ä¸ªåŸºäºäº‹ä»¶ç›¸æœºçš„è¯­è¨€é©±åŠ¨ç‰©ä½“å®šä½çš„å¤§è§„æ¨¡åŸºå‡†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šTalk2EventåŸºå‡†çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åœºæ™¯é€‰æ‹©ï¼šé€‰æ‹©çœŸå®é©¾é©¶åœºæ™¯ï¼Œä¿è¯æ•°æ®çš„çœŸå®æ€§å’Œå¤šæ ·æ€§ï¼›2) æŒ‡ç§°è¡¨è¾¾ç”Ÿæˆï¼šé‡‡ç”¨äººå·¥æ ‡æ³¨çš„æ–¹å¼ç”ŸæˆæŒ‡ç§°è¡¨è¾¾ï¼Œä¿è¯è¡¨è¾¾çš„å‡†ç¡®æ€§å’Œä¸°å¯Œæ€§ï¼›3) å±æ€§æ ‡æ³¨ï¼šå¯¹æŒ‡ç§°è¡¨è¾¾è¿›è¡Œå±æ€§æ ‡æ³¨ï¼ŒåŒ…æ‹¬å¤–è§‚ã€çŠ¶æ€ã€ä¸è§‚å¯Ÿè€…å…³ç³»ä»¥åŠç‰©ä½“é—´å…³ç³»ï¼Œä¸ºæ¨¡å‹æä¾›æ›´ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼›4) æ•°æ®éªŒè¯ï¼šå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—å’ŒéªŒè¯ï¼Œç¡®ä¿æ•°æ®çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Talk2EventåŸºå‡†åŒ…å«5567ä¸ªåœºæ™¯ï¼Œ13458ä¸ªæ ‡æ³¨ç‰©ä½“ï¼Œä»¥åŠè¶…è¿‡30000ä¸ªç»è¿‡ç²¾å¿ƒéªŒè¯çš„æŒ‡ç§°è¡¨è¾¾ã€‚æ¯ä¸ªè¡¨è¾¾éƒ½é€šè¿‡å››ä¸ªç»“æ„åŒ–å±æ€§è¿›è¡Œäº†ä¸°å¯Œï¼šå¤–è§‚ã€çŠ¶æ€ã€ä¸è§‚å¯Ÿè€…çš„å…³ç³»ä»¥åŠä¸å‘¨å›´ç‰©ä½“çš„å…³ç³»ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒå°†æå¤§åœ°ä¿ƒè¿›äº‹ä»¶ç›¸æœºå’Œè‡ªç„¶è¯­è¨€ç†è§£é¢†åŸŸçš„ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨äº‹ä»¶ç›¸æœºå’Œè‡ªç„¶è¯­è¨€ç†è§£æŠ€æœ¯ï¼Œå®ç°å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„ç²¾å‡†æ„ŸçŸ¥å’Œç†è§£ï¼Œä»è€Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯å®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’æ–¹å¼ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡è¯­éŸ³æŒ‡ä»¤æ§åˆ¶æœºå™¨äººå®Œæˆç‰¹å®šä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event cameras capture changes in brightness with microsecond precision and remain reliable under motion blur and challenging illumination, offering clear advantages for modeling highly dynamic scenes. Yet, their integration with natural language understanding has received little attention, leaving a gap in multimodal perception. To address this, we introduce Talk2Event, the first large-scale benchmark for language-driven object grounding using event data. Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes, 13,458 annotated objects, and more than 30,000 carefully validated referring expressions. Each expression is enriched with four structured attributes -- appearance, status, relation to the viewer, and relation to surrounding objects -- that explicitly capture spatial, temporal, and relational cues. This attribute-centric design supports interpretable and compositional grounding, enabling analysis that moves beyond simple object recognition to contextual reasoning in dynamic environments. We envision Talk2Event as a foundation for advancing multimodal and temporally-aware perception, with applications spanning robotics, human-AI interaction, and so on.

