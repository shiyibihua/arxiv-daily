---
layout: default
title: Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization
---

# Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09307v1</a>
  <a href="https://arxiv.org/pdf/2509.09307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09307v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09307v1', 'Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengzhao Lai, Youbin Zheng, Zhenyang Cai, Haonan Lyu, Jinpu Yang, Hongqing Liang, Yan Hu, Benyou Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/FreedomIntelligence/MatCha)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMatChaï¼šææ–™è¡¨å¾å¤šæ¨¡æ€åŸºå‡†ï¼Œè¯„ä¼°MLLMåœ¨ææ–™ç§‘å­¦å›¾åƒç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ææ–™è¡¨å¾` `å›¾åƒç†è§£` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†æ•°æ®é›†` `ææ–™ç§‘å­¦` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œä½†å¯¹ææ–™è¡¨å¾å›¾åƒçš„ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨çš„è¯„ä¼°åŸºå‡†ã€‚
2. è®ºæ–‡æ„å»ºäº†MatChaåŸºå‡†ï¼ŒåŒ…å«1500ä¸ªé—®é¢˜ï¼Œè¦†ç›–ææ–™ç ”ç©¶çš„å››ä¸ªé˜¶æ®µå’Œ21ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°MLLMå¯¹ææ–™è¡¨å¾å›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨MatChaä¸Šçš„è¡¨ç°ä¸äººç±»ä¸“å®¶å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å±‚æ¬¡ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ææ–™è¡¨å¾æ˜¯è·å–ææ–™ä¿¡æ¯ã€æ­ç¤ºåŠ å·¥-å¾®è§‚ç»“æ„-æ€§èƒ½å…³ç³»çš„åŸºç¡€ï¼Œå¯¹ææ–™è®¾è®¡å’Œä¼˜åŒ–è‡³å…³é‡è¦ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æœ€è¿‘åœ¨ææ–™ç§‘å­¦çš„ç”Ÿæˆå’Œé¢„æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ç†è§£çœŸå®ä¸–ç•Œè¡¨å¾å›¾åƒæ•°æ®çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MatChaï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºææ–™è¡¨å¾å›¾åƒç†è§£çš„åŸºå‡†ï¼ŒåŒ…å«1500ä¸ªéœ€è¦ä¸“å®¶çº§é¢†åŸŸçŸ¥è¯†çš„é—®é¢˜ã€‚MatChaæ¶µç›–ææ–™ç ”ç©¶çš„å››ä¸ªå…³é”®é˜¶æ®µï¼ŒåŒ…å«21ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æ—¨åœ¨åæ˜ ææ–™ç§‘å­¦å®¶é¢ä¸´çš„çœŸå®æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„MLLMåœ¨MatChaä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸äººç±»ä¸“å®¶ç›¸æ¯”å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†éœ€è¦æ›´é«˜å±‚æ¬¡ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥çš„é¢˜ç›®æ—¶è¡¨ç°ä¸‹é™ã€‚ç®€å•çš„å°‘æ ·æœ¬å’Œæ€ç»´é“¾æç¤ºéš¾ä»¥ç¼“è§£è¿™äº›é™åˆ¶ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç°æœ‰çš„MLLMåœ¨é€‚åº”çœŸå®ä¸–ç•Œçš„ææ–™è¡¨å¾åœºæ™¯æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬å¸Œæœ›MatChaå°†ä¿ƒè¿›æ–°ææ–™å‘ç°å’Œè‡ªä¸»ç§‘å­¦æ™ºèƒ½ä½“ç­‰é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚MatChaå¯åœ¨https://github.com/FreedomIntelligence/MatChaè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMåœ¨ææ–™ç§‘å­¦é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶å¯¹ææ–™è¡¨å¾å›¾åƒçš„ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹çœŸå®ä¸–ç•Œå¤æ‚å›¾åƒçš„ç†è§£ï¼Œç¼ºä¹å……åˆ†çš„è¯„ä¼°å’ŒéªŒè¯ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹éœ€è¦é«˜å±‚æ¬¡ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥çš„ææ–™è¡¨å¾ä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°MLLMåœ¨ææ–™è¡¨å¾å›¾åƒç†è§£èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†MatChaã€‚é€šè¿‡è®¾è®¡åŒ…å«ä¸åŒéš¾åº¦å’Œä¸“ä¸šçŸ¥è¯†è¦æ±‚çš„ä»»åŠ¡ï¼Œå…¨é¢è¯„ä¼°MLLMåœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºå…¶å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMatChaåŸºå‡†åŒ…å«å››ä¸ªå…³é”®é˜¶æ®µï¼Œæ¶µç›–ææ–™ç ”ç©¶çš„å…¸å‹æµç¨‹ï¼šææ–™åˆ¶å¤‡ã€å¾®è§‚ç»“æ„è¡¨å¾ã€æ€§èƒ½æµ‹è¯•å’Œæ•°æ®åˆ†æã€‚æ¯ä¸ªé˜¶æ®µåŒ…å«å¤šä¸ªä»»åŠ¡ï¼Œå…±è®¡21ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«ææ–™è¡¨å¾å›¾åƒå’Œå¯¹åº”çš„é—®é¢˜ï¼Œé—®é¢˜éœ€è¦ä¸“å®¶çº§çš„ææ–™ç§‘å­¦çŸ¥è¯†æ‰èƒ½å›ç­”ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šè¾“å…¥ææ–™è¡¨å¾å›¾åƒå’Œé—®é¢˜ï¼ŒMLLMæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè¾“å‡ºç­”æ¡ˆï¼Œä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œå¯¹æ¯”è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šMatChaæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹ææ–™è¡¨å¾å›¾åƒç†è§£çš„åŸºå‡†æ•°æ®é›†ã€‚å®ƒä¸ä»…åŒ…å«å¤§é‡çœŸå®ä¸–ç•Œçš„ææ–™è¡¨å¾å›¾åƒï¼Œè¿˜è®¾è®¡äº†éœ€è¦é«˜å±‚æ¬¡ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ï¼Œæ›´è´´è¿‘ææ–™ç§‘å­¦å®¶çš„å®é™…å·¥ä½œåœºæ™¯ã€‚æ­¤å¤–ï¼ŒMatChaçš„æ„å»ºè€ƒè™‘äº†ææ–™ç ”ç©¶çš„å®Œæ•´æµç¨‹ï¼Œè¦†ç›–äº†å¤šä¸ªå…³é”®é˜¶æ®µã€‚

**å…³é”®è®¾è®¡**ï¼šMatChaåŸºå‡†åŒ…å«1500ä¸ªé—®é¢˜ï¼Œæ¶µç›–æ‰«æç”µå­æ˜¾å¾®é•œ(SEM)ã€é€å°„ç”µå­æ˜¾å¾®é•œ(TEM)ã€å…‰å­¦æ˜¾å¾®é•œç­‰å¤šç§ææ–™è¡¨å¾æŠ€æœ¯äº§ç”Ÿçš„å›¾åƒã€‚é—®é¢˜è®¾è®¡æ¶µç›–å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€å…³ç³»æ¨ç†ç­‰å¤šç§ç±»å‹ï¼Œå¹¶æ ¹æ®éš¾åº¦è¿›è¡Œåˆ†çº§ã€‚è®ºæ–‡è¿˜å°è¯•äº†å°‘æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºç­‰æ–¹æ³•æ¥æå‡MLLMçš„æ€§èƒ½ï¼Œä½†æ•ˆæœæœ‰é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æœ€å…ˆè¿›çš„MLLMåœ¨MatChaåŸºå‡†ä¸Šçš„è¡¨ç°è¿œä½äºäººç±»ä¸“å®¶æ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å±‚æ¬¡ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒMLLMçš„å‡†ç¡®ç‡ä»…ä¸ºä¸ªä½æ•°ï¼Œè¡¨æ˜å…¶åœ¨ææ–™è¡¨å¾å›¾åƒç†è§£æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ç®€å•çš„å°‘æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºéš¾ä»¥æœ‰æ•ˆæå‡MLLMçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MatChaåŸºå‡†çš„æå‡ºï¼Œèƒ½å¤Ÿä¿ƒè¿›MLLMåœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚æ–°ææ–™å‘ç°ã€ææ–™æ€§èƒ½é¢„æµ‹ã€ææ–™ç¼ºé™·æ£€æµ‹ç­‰ã€‚é€šè¿‡ä¸æ–­æå‡MLLMåœ¨ææ–™è¡¨å¾å›¾åƒç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥åŠ é€Ÿææ–™ç ”å‘è¿›ç¨‹ï¼Œé™ä½ç ”å‘æˆæœ¬ï¼Œå¹¶ä¸ºè‡ªä¸»ç§‘å­¦æ™ºèƒ½ä½“çš„å¼€å‘å¥ å®šåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.

