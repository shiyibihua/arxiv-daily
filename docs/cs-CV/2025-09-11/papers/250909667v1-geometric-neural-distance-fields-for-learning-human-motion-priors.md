---
layout: default
title: Geometric Neural Distance Fields for Learning Human Motion Priors
---

# Geometric Neural Distance Fields for Learning Human Motion Priors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09667" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09667v1</a>
  <a href="https://arxiv.org/pdf/2509.09667.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09667v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09667v1', 'Geometric Neural Distance Fields for Learning Human Motion Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengdi Yu, Simone Foti, Linguang Zhang, Amy Zhao, Cem Keskin, Stefanos Zafeiriou, Tolga Birdal

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: 8 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¥ç»é»æ›¼è¿åŠ¨åœºï¼ˆNRMFï¼‰ï¼Œç”¨äºå­¦ä¹ é²æ£’ã€æ—¶åºä¸€è‡´ä¸”ç‰©ç†å¯ä¿¡çš„äººä½“è¿åŠ¨å…ˆéªŒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `äººä½“è¿åŠ¨å…ˆéªŒ` `ç¥ç»è·ç¦»åœº` `é»æ›¼å‡ ä½•` `è¿åŠ¨æ¢å¤` `è¿åŠ¨ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºVAEæˆ–æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨å…ˆéªŒæ–¹æ³•éš¾ä»¥ä¿è¯è¿åŠ¨çš„æ—¶åºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚
2. NRMFé€šè¿‡æ˜¾å¼å»ºæ¨¡å§¿åŠ¿ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„ç¥ç»è·ç¦»åœºï¼Œå¹¶çº¦æŸåœ¨é»æ›¼æµå½¢ä¸Šï¼Œä¿è¯è¿åŠ¨çš„åˆç†æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNRMFåœ¨è¿åŠ¨å»å™ªã€æ’å€¼å’Œä»éƒ¨åˆ†è§‚æµ‹æ¢å¤è¿åŠ¨ç­‰ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Dç”Ÿæˆå¼äººä½“è¿åŠ¨å…ˆéªŒæ–¹æ³•ï¼Œç§°ä¸ºç¥ç»é»æ›¼è¿åŠ¨åœºï¼ˆNRMFï¼‰ï¼Œå®ƒèƒ½å¤Ÿå®ç°é²æ£’ã€æ—¶åºä¸€è‡´ä¸”ç‰©ç†å¯ä¿¡çš„3Dè¿åŠ¨æ¢å¤ã€‚ä¸ç°æœ‰çš„åŸºäºVAEæˆ–æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„é«˜é˜¶è¿åŠ¨å…ˆéªŒæ˜¾å¼åœ°å°†äººä½“è¿åŠ¨å»ºæ¨¡ä¸ºä¸€ç³»åˆ—ç¥ç»è·ç¦»åœºï¼ˆNDFï¼‰çš„é›¶æ°´å¹³é›†ï¼Œè¿™äº›NDFå¯¹åº”äºå§¿åŠ¿ã€è¿‡æ¸¡ï¼ˆé€Ÿåº¦ï¼‰å’ŒåŠ é€Ÿåº¦åŠ¨åŠ›å­¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä»¥ä¸‹æ„ä¹‰ä¸Šæ˜¯ä¸¥è°¨çš„ï¼šæˆ‘ä»¬çš„NDFæ„å»ºåœ¨å…³èŠ‚æ—‹è½¬ã€è§’é€Ÿåº¦å’Œè§’åŠ é€Ÿåº¦çš„ä¹˜ç§¯ç©ºé—´ä¸Šï¼Œå°Šé‡åº•å±‚å…³èŠ‚çš„å‡ ä½•ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ï¼šï¼ˆiï¼‰ä¸€ç§æ–°é¢–çš„è‡ªé€‚åº”æ­¥é•¿æ··åˆç®—æ³•ï¼Œç”¨äºæŠ•å½±åˆ°åˆç†çš„è¿åŠ¨é›†åˆä¸Šï¼›ï¼ˆiiï¼‰ä¸€ç§æ–°é¢–çš„å‡ ä½•ç§¯åˆ†å™¨ï¼Œç”¨äºåœ¨æµ‹è¯•æ—¶ä¼˜åŒ–å’Œç”Ÿæˆè¿‡ç¨‹ä¸­â€œå±•å¼€â€çœŸå®çš„è¿åŠ¨è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨AMASSæ•°æ®é›†ä¸Šè®­ç»ƒçš„NRMFåœ¨å¤šç§è¾“å…¥æ¨¡æ€å’Œå„ç§ä»»åŠ¡ï¼ˆä»å»å™ªåˆ°è¿åŠ¨æ’å€¼ä»¥åŠæ‹Ÿåˆåˆ°éƒ¨åˆ†2D/3Dè§‚æµ‹ï¼‰ä¸­éƒ½è¡¨ç°å‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººä½“è¿åŠ¨å…ˆéªŒæ–¹æ³•ï¼Œä¾‹å¦‚åŸºäºVAEæˆ–æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šå¸¸éš¾ä»¥ä¿è¯ç”Ÿæˆè¿åŠ¨çš„æ—¶åºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚è¿™äº›æ–¹æ³•å¾€å¾€å¿½ç•¥äº†äººä½“è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦çš„çº¦æŸï¼Œå¯¼è‡´ç”Ÿæˆçš„è¿åŠ¨ä¸è‡ªç„¶æˆ–ä¸ç¬¦åˆç‰©ç†è§„å¾‹ã€‚å› æ­¤ï¼Œå¦‚ä½•å­¦ä¹ ä¸€ä¸ªæ—¢èƒ½æ•æ‰äººä½“è¿åŠ¨çš„å¤æ‚æ€§ï¼Œåˆèƒ½ä¿è¯å…¶åˆç†æ€§çš„è¿åŠ¨å…ˆéªŒæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äººä½“è¿åŠ¨å»ºæ¨¡ä¸ºä¸€ç³»åˆ—ç¥ç»è·ç¦»åœºï¼ˆNDFï¼‰çš„é›¶æ°´å¹³é›†ã€‚è¿™äº›NDFåˆ†åˆ«å¯¹åº”äºå§¿åŠ¿ã€è¿‡æ¸¡ï¼ˆé€Ÿåº¦ï¼‰å’ŒåŠ é€Ÿåº¦åŠ¨åŠ›å­¦ã€‚é€šè¿‡å°†è¿åŠ¨çº¦æŸåœ¨è¿™äº›NDFçš„é›¶æ°´å¹³é›†ä¸Šï¼Œå¯ä»¥ä¿è¯è¿åŠ¨ç¬¦åˆäººä½“è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦çš„çº¦æŸï¼Œä»è€Œæé«˜è¿åŠ¨çš„æ—¶åºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜åˆ©ç”¨é»æ›¼å‡ ä½•çš„å·¥å…·ï¼Œåœ¨å…³èŠ‚æ—‹è½¬ã€è§’é€Ÿåº¦å’Œè§’åŠ é€Ÿåº¦çš„ä¹˜ç§¯ç©ºé—´ä¸Šæ„å»ºNDFï¼Œä»è€Œæ›´å¥½åœ°å°Šé‡åº•å±‚å…³èŠ‚çš„å‡ ä½•ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNRMFçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰å§¿åŠ¿ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„NDFè¡¨ç¤ºï¼›2ï¼‰è‡ªé€‚åº”æ­¥é•¿æ··åˆç®—æ³•ï¼Œç”¨äºå°†è¿åŠ¨æŠ•å½±åˆ°åˆç†çš„è¿åŠ¨é›†åˆä¸Šï¼›3ï¼‰å‡ ä½•ç§¯åˆ†å™¨ï¼Œç”¨äºç”ŸæˆçœŸå®çš„è¿åŠ¨è½¨è¿¹ã€‚è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨AMASSæ•°æ®é›†è®­ç»ƒNDFï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®åœ°è¡¨ç¤ºäººä½“è¿åŠ¨çš„åˆ†å¸ƒã€‚æµ‹è¯•é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨è‡ªé€‚åº”æ­¥é•¿æ··åˆç®—æ³•å’Œå‡ ä½•ç§¯åˆ†å™¨ï¼Œä»NDFä¸­é‡‡æ ·æˆ–ä¼˜åŒ–è¿åŠ¨è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šNRMFçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1ï¼‰æ˜¾å¼åœ°å»ºæ¨¡äº†å§¿åŠ¿ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦çš„ç¥ç»è·ç¦»åœºï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººä½“è¿åŠ¨çš„åŠ¨åŠ›å­¦ä¿¡æ¯ï¼›2ï¼‰åˆ©ç”¨é»æ›¼å‡ ä½•çš„å·¥å…·ï¼Œåœ¨å…³èŠ‚æ—‹è½¬ã€è§’é€Ÿåº¦å’Œè§’åŠ é€Ÿåº¦çš„ä¹˜ç§¯ç©ºé—´ä¸Šæ„å»ºNDFï¼Œä»è€Œæ›´å¥½åœ°å°Šé‡åº•å±‚å…³èŠ‚çš„å‡ ä½•ç»“æ„ï¼›3ï¼‰æå‡ºäº†è‡ªé€‚åº”æ­¥é•¿æ··åˆç®—æ³•å’Œå‡ ä½•ç§¯åˆ†å™¨ï¼Œç”¨äºä»NDFä¸­é‡‡æ ·æˆ–ä¼˜åŒ–è¿åŠ¨è½¨è¿¹ã€‚

**å…³é”®è®¾è®¡**ï¼šNRMFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¥è¡¨ç¤ºNDFï¼›2ï¼‰ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºè·ç¦»åº¦é‡ï¼›3ï¼‰ä½¿ç”¨Adamä¼˜åŒ–å™¨æ¥è®­ç»ƒNDFï¼›4ï¼‰è‡ªé€‚åº”æ­¥é•¿æ··åˆç®—æ³•æ ¹æ®NDFçš„æ¢¯åº¦ä¿¡æ¯æ¥è°ƒæ•´æ­¥é•¿ï¼›5ï¼‰å‡ ä½•ç§¯åˆ†å™¨ä½¿ç”¨æŒ‡æ•°æ˜ å°„æ¥æ›´æ–°å…³èŠ‚æ—‹è½¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNRMFåœ¨è¿åŠ¨å»å™ªã€æ’å€¼å’Œä»éƒ¨åˆ†è§‚æµ‹æ¢å¤è¿åŠ¨ç­‰ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿åŠ¨å»å™ªä»»åŠ¡ä¸­ï¼ŒNRMFèƒ½å¤Ÿå°†å™ªå£°æ°´å¹³é™ä½åˆ°ç°æœ‰æ–¹æ³•çš„1/2ã€‚åœ¨è¿åŠ¨æ’å€¼ä»»åŠ¡ä¸­ï¼ŒNRMFèƒ½å¤Ÿç”Ÿæˆæ›´åŠ è‡ªç„¶å’Œæµç•…çš„è¿åŠ¨è½¨è¿¹ã€‚åœ¨ä»éƒ¨åˆ†è§‚æµ‹æ¢å¤è¿åŠ¨ä»»åŠ¡ä¸­ï¼ŒNRMFèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡ç¼ºå¤±çš„è¿åŠ¨ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NRMFå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼š1ï¼‰è¿åŠ¨æ•æ‰æ•°æ®çš„å»å™ªå’Œä¿®å¤ï¼›2ï¼‰è¿åŠ¨æ’å€¼å’Œç”Ÿæˆï¼›3ï¼‰åŸºäºéƒ¨åˆ†è§‚æµ‹çš„è¿åŠ¨é‡å»ºï¼›4ï¼‰è™šæ‹Ÿç°å®å’Œæ¸¸æˆä¸­çš„è§’è‰²åŠ¨ç”»ï¼›5ï¼‰æœºå™¨äººæ§åˆ¶ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºèƒ½å¤Ÿæé«˜äººä½“è¿åŠ¨å»ºæ¨¡çš„ç²¾åº¦å’ŒçœŸå®æ„Ÿï¼Œä¸ºç›¸å…³åº”ç”¨æä¾›æ›´å¯é çš„è¿åŠ¨æ•°æ®ã€‚æœªæ¥ï¼Œå¯ä»¥å°†NRMFæ‰©å±•åˆ°æ›´å¤æ‚çš„è¿åŠ¨åœºæ™¯ï¼Œä¾‹å¦‚å¤šäººäº¤äº’å’Œç‰©ä½“æ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to "roll out" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.

