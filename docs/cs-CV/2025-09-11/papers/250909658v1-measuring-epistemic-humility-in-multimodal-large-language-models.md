---
layout: default
title: Measuring Epistemic Humility in Multimodal Large Language Models
---

# Measuring Epistemic Humility in Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09658" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09658v1</a>
  <a href="https://arxiv.org/pdf/2509.09658.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09658v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09658v1', 'Measuring Epistemic Humility in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingkui Tong, Jiaer Xia, Sifeng Shang, Kaiyang Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/maifoundations/HumbleBench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HumbleBenchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è®¤çŸ¥è°¦é€Šæ€§çš„æ–°åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è®¤çŸ¥è°¦é€Šæ€§` `å¹»è§‰æ£€æµ‹` `åŸºå‡†æµ‹è¯•` `è§†è§‰é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMè¯„ä¼°ä¾§é‡è¯†åˆ«å‡†ç¡®ç‡ï¼Œå¿½ç•¥äº†æ¨¡å‹è¯†åˆ«é”™è¯¯ç­”æ¡ˆï¼ˆè®¤çŸ¥è°¦é€Šæ€§ï¼‰çš„èƒ½åŠ›ï¼Œè¿™åœ¨å®‰å…¨æ”¸å…³åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚
2. HumbleBenché€šè¿‡å¼•å…¥â€œä»¥ä¸Šçš†éâ€é€‰é¡¹ï¼Œè¿«ä½¿æ¨¡å‹ä¸ä»…è¯†åˆ«æ­£ç¡®ä¿¡æ¯ï¼Œè¿˜è¦åˆ¤æ–­æ‰€æœ‰é€‰é¡¹æ˜¯å¦éƒ½é”™è¯¯ï¼Œä»è€Œè¯„ä¼°è®¤çŸ¥è°¦é€Šæ€§ã€‚
3. å®éªŒè¯„ä¼°äº†å¤šç§MLLMåœ¨HumbleBenchä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨è®¤çŸ¥è°¦é€Šæ€§æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„å¹»è§‰ç°è±¡â€”â€”æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ä¸è¾“å…¥å›¾åƒä¸ä¸€è‡´â€”â€”åœ¨ç°å®åº”ç”¨ä¸­æ„æˆé‡å¤§é£é™©ï¼Œä»è§†è§‰é—®ç­”ä¸­çš„é”™è¯¯ä¿¡æ¯åˆ°å†³ç­–ä¸­çš„ä¸å®‰å…¨é”™è¯¯ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦æµ‹è¯•è¯†åˆ«å‡†ç¡®ç‡ï¼Œå³è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½åœ¨å¹²æ‰°é¡¹ä¸­é€‰æ‹©æ­£ç¡®ç­”æ¡ˆã€‚è¿™å¿½ç•¥äº†å¯ä¿¡AIçš„ä¸€ä¸ªåŒæ ·å…³é”®çš„èƒ½åŠ›ï¼šè¯†åˆ«ä½•æ—¶æä¾›çš„é€‰é¡¹éƒ½ä¸æ­£ç¡®ï¼Œè¿™ç§è¡Œä¸ºåæ˜ äº†è®¤çŸ¥è°¦é€Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†HumbleBenchï¼Œä¸€ä¸ªæ–°çš„å¹»è§‰åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°MLLMæ‹’ç»è·¨ä¸‰ç§å¹»è§‰ç±»å‹ï¼ˆå¯¹è±¡ã€å…³ç³»å’Œå±æ€§ï¼‰çš„çœ‹ä¼¼åˆç†ä½†é”™è¯¯çš„ç­”æ¡ˆçš„èƒ½åŠ›ã€‚HumbleBenchæ„å»ºäºå…¨æ™¯åœºæ™¯å›¾æ•°æ®é›†ä¹‹ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨ç»†ç²’åº¦çš„åœºæ™¯å›¾æ³¨é‡Šæ¥æå–ground-truthå®ä½“å’Œå…³ç³»ï¼Œå¹¶æç¤ºGPT-4-Turboç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œç„¶åè¿›è¡Œä¸¥æ ¼çš„äººå·¥è¿‡æ»¤ã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…å«ä¸€ä¸ªâ€œä»¥ä¸Šçš†éâ€é€‰é¡¹ï¼Œè¦æ±‚æ¨¡å‹ä¸ä»…è¦è¯†åˆ«æ­£ç¡®çš„è§†è§‰ä¿¡æ¯ï¼Œè¿˜è¦è¯†åˆ«ä½•æ—¶æ²¡æœ‰æä¾›æœ‰æ•ˆçš„ç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨HumbleBenchä¸Šè¯„ä¼°äº†å„ç§æœ€å…ˆè¿›çš„MLLMâ€”â€”åŒ…æ‹¬é€šç”¨æ¨¡å‹å’Œä¸“é—¨çš„æ¨ç†æ¨¡å‹â€”â€”å¹¶ä¸ç¤¾åŒºåˆ†äº«æœ‰ä»·å€¼çš„å‘ç°å’Œè§è§£ã€‚é€šè¿‡ç»“åˆæ˜¾å¼çš„é”™è¯¯é€‰é¡¹æ‹’ç»ï¼ŒHumbleBenchå¡«è¡¥äº†å½“å‰è¯„ä¼°å¥—ä»¶ä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ï¼Œä¸ºå®‰å…¨å…³é”®è®¾ç½®ä¸­MLLMçš„å¯é æ€§æä¾›äº†æ›´çœŸå®çš„è¡¡é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œå¯åœ¨https://github.com/maifoundations/HumbleBenchè®¿é—®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ï¼Œå³ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸ä¸€è‡´çš„å†…å®¹ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹è¯†åˆ«æ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹è¯†åˆ«æ‰€æœ‰é€‰é¡¹å‡ä¸ºé”™è¯¯ç­”æ¡ˆçš„èƒ½åŠ›ï¼Œè¿™åœ¨å®‰å…¨æ”¸å…³åœºæ™¯ä¸‹ä¼šé€ æˆæ½œåœ¨é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨çš„åŸºå‡†æµ‹è¯•é›†ï¼Œè¯¥æµ‹è¯•é›†åŒ…å«â€œä»¥ä¸Šçš†éâ€é€‰é¡¹ï¼Œè¿«ä½¿æ¨¡å‹ä¸ä»…è¦è¯†åˆ«æ­£ç¡®çš„è§†è§‰ä¿¡æ¯ï¼Œè¿˜è¦èƒ½å¤Ÿåˆ¤æ–­ä½•æ—¶æ‰€æœ‰æä¾›çš„ç­”æ¡ˆéƒ½æ˜¯æ— æ•ˆçš„ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°MLLMçš„è®¤çŸ¥è°¦é€Šæ€§ï¼Œå³æ¨¡å‹åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹æ‹’ç»é”™è¯¯ç­”æ¡ˆçš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHumbleBenchçš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åŸºäºå…¨æ™¯åœºæ™¯å›¾æ•°æ®é›†ï¼Œæå–ground-truthå®ä½“å’Œå…³ç³»ï¼›2) åˆ©ç”¨GPT-4-Turboç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å«ä¸€ä¸ªâ€œä»¥ä¸Šçš†éâ€é€‰é¡¹ï¼›3) è¿›è¡Œä¸¥æ ¼çš„äººå·¥è¿‡æ»¤ï¼Œç¡®ä¿é—®é¢˜çš„è´¨é‡å’Œéš¾åº¦ã€‚è¯„ä¼°æ—¶ï¼Œå°†MLLMåº”ç”¨äºHumbleBenchæ•°æ®é›†ï¼Œå¹¶æ ¹æ®æ¨¡å‹é€‰æ‹©â€œä»¥ä¸Šçš†éâ€é€‰é¡¹çš„é¢‘ç‡å’Œå‡†ç¡®æ€§æ¥è¯„ä¼°å…¶è®¤çŸ¥è°¦é€Šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šHumbleBenchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¯„ä¼°MLLMè®¤çŸ¥è°¦é€Šæ€§çš„æ–¹æ³•ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒHumbleBenché€šè¿‡å¼•å…¥â€œä»¥ä¸Šçš†éâ€é€‰é¡¹ï¼Œè¿«ä½¿æ¨¡å‹åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹åšå‡ºåˆ¤æ–­ï¼Œä»è€Œæ›´çœŸå®åœ°åæ˜ äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚è¿™ç§è¯„ä¼°æ–¹æ³•å¡«è¡¥äº†å½“å‰è¯„ä¼°å¥—ä»¶ä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šHumbleBenchä½¿ç”¨äº†å…¨æ™¯åœºæ™¯å›¾æ•°æ®é›†ï¼Œä¿è¯äº†ground-truthä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚åˆ©ç”¨GPT-4-Turboç”Ÿæˆé—®é¢˜ï¼Œæé«˜äº†æ•°æ®é›†çš„æ„å»ºæ•ˆç‡ã€‚äººå·¥è¿‡æ»¤è¿‡ç¨‹ç¡®ä¿äº†é—®é¢˜çš„è´¨é‡å’Œéš¾åº¦ï¼Œé¿å…äº†ç®€å•æˆ–æ¨¡æ£±ä¸¤å¯çš„é—®é¢˜ã€‚ â€œä»¥ä¸Šçš†éâ€é€‰é¡¹çš„è®¾è®¡æ˜¯è¯„ä¼°è®¤çŸ¥è°¦é€Šæ€§çš„å…³é”®ï¼Œéœ€è¦ä»”ç»†è€ƒè™‘å…¶æªè¾å’Œå«ä¹‰ï¼Œä»¥é¿å…è¯¯å¯¼æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HumbleBenchè¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„MLLMï¼Œç»“æœè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨è®¤çŸ¥è°¦é€Šæ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚ä¾‹å¦‚ï¼Œå³ä½¿åœ¨æ˜ç¡®åŒ…å«â€œä»¥ä¸Šçš†éâ€é€‰é¡¹çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»ç„¶å€¾å‘äºé€‰æ‹©é”™è¯¯çš„ç­”æ¡ˆã€‚è¿™äº›å®éªŒç»“æœçªå‡ºäº†HumbleBenchçš„ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HumbleBenchçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„å¯é æ€§ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­å’Œé‡‘èé£æ§ã€‚é€šè¿‡æé«˜æ¨¡å‹è¯†åˆ«é”™è¯¯ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¯ä»¥å‡å°‘å› å¹»è§‰å¯¼è‡´çš„é”™è¯¯å†³ç­–ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¹Ÿæœ‰åŠ©äºæ¨åŠ¨ç›¸å…³ç®—æ³•çš„æ”¹è¿›å’Œä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a "None of the above" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.

