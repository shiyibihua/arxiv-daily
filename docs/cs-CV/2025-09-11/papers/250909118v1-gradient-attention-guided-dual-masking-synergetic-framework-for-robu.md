---
layout: default
title: Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval
---

# Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09118" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09118v1</a>
  <a href="https://arxiv.org/pdf/2509.09118.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09118v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09118v1', 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, Qichuan Ding

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: Accepted by EMNLP2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGA-DMSæ¡†æ¶ï¼Œé€šè¿‡æ¢¯åº¦æ³¨æ„åŠ›å¼•å¯¼çš„åŒæ©ç æœºåˆ¶æå‡æ–‡æœ¬è¡Œäººæ£€ç´¢æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡Œäººæ£€ç´¢` `è·¨æ¨¡æ€å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `æ¢¯åº¦æ³¨æ„åŠ›` `æ©ç é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CLIPæ¨¡å‹åœ¨è¡Œäººæ£€ç´¢ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œå…¨å±€å¯¹æ¯”å­¦ä¹ éš¾ä»¥æ•æ‰å±€éƒ¨ç‰¹å¾çš„é—®é¢˜ã€‚
2. æå‡ºGA-DMSæ¡†æ¶ï¼Œåˆ©ç”¨æ¢¯åº¦æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°æ©ç›–å™ªå£°æ–‡æœ¬tokenï¼Œå¹¶å¼•å…¥æ©ç tokené¢„æµ‹ç›®æ ‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGA-DMSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶åœ¨è¡Œäººè¡¨å¾å­¦ä¹ ä¸­çš„åº”ç”¨é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆiï¼‰ç¼ºä¹ä»¥äººä¸ºä¸­å¿ƒçš„å›¾åƒçš„å¤§è§„æ¨¡æ ‡æ³¨è§†è§‰-è¯­è¨€æ•°æ®ï¼Œä»¥åŠï¼ˆiiï¼‰å…¨å±€å¯¹æ¯”å­¦ä¹ çš„å›ºæœ‰å±€é™æ€§ï¼Œå®ƒéš¾ä»¥ç»´æŒå¯¹ç»†ç²’åº¦åŒ¹é…è‡³å…³é‡è¦çš„åˆ¤åˆ«æ€§å±€éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶å®¹æ˜“å—åˆ°å™ªå£°æ–‡æœ¬tokençš„å½±å“ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•°æ®ç®¡ç†å’Œæ¨¡å‹æ¶æ„çš„ååŒæ”¹è¿›æ¥æ¨è¿›CLIPåœ¨è¡Œäººè¡¨å¾å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æŠ—å™ªå£°çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œè¯¥æµç¨‹åˆ©ç”¨MLLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ¥è‡ªåŠ¨è¿‡æ»¤å’Œæ ‡æ³¨ç½‘ç»œæ¥æºçš„å›¾åƒã€‚è¿™äº§ç”Ÿäº†WebPersonï¼Œä¸€ä¸ªåŒ…å«500ä¸‡é«˜è´¨é‡ä»¥äººä¸ºä¸­å¿ƒçš„å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†GA-DMSï¼ˆæ¢¯åº¦æ³¨æ„åŠ›å¼•å¯¼çš„åŒæ©ç ååŒï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”åœ°æ©ç›–åŸºäºæ¢¯åº¦-æ³¨æ„åŠ›ç›¸ä¼¼åº¦åˆ†æ•°çš„å™ªå£°æ–‡æœ¬tokenæ¥æ”¹å–„è·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†æ©ç tokené¢„æµ‹ç›®æ ‡ï¼Œè¿«ä½¿æ¨¡å‹é¢„æµ‹ä¿¡æ¯ä¸°å¯Œçš„æ–‡æœ¬tokenï¼Œä»è€Œå¢å¼ºç»†ç²’åº¦çš„è¯­ä¹‰è¡¨å¾å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGA-DMSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºCLIPçš„è¡Œäººæ£€ç´¢æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹å¤§è§„æ¨¡çš„ã€ä»¥äººä¸ºä¸­å¿ƒçš„å›¾åƒ-æ–‡æœ¬å¯¹è®­ç»ƒæ•°æ®ï¼›äºŒæ˜¯CLIPçš„å…¨å±€å¯¹æ¯”å­¦ä¹ æ–¹æ³•éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„å±€éƒ¨ç‰¹å¾ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°å™ªå£°æ–‡æœ¬tokençš„å¹²æ‰°ï¼Œå¯¼è‡´æ£€ç´¢æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°æ®å¢å¼ºå’Œæ¨¡å‹æ”¹è¿›æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œåˆ©ç”¨MLLMè‡ªåŠ¨è¿‡æ»¤å’Œæ ‡æ³¨ç½‘ç»œå›¾åƒï¼Œæ„å»ºå¤§è§„æ¨¡æ•°æ®é›†WebPersonã€‚åœ¨æ¨¡å‹æ–¹é¢ï¼Œæå‡ºGA-DMSæ¡†æ¶ï¼Œåˆ©ç”¨æ¢¯åº¦æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°æ©ç›–å™ªå£°æ–‡æœ¬tokenï¼Œå¹¶å¼•å…¥æ©ç tokené¢„æµ‹ç›®æ ‡ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹ç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGA-DMSæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ•°æ®æ„å»ºæ¨¡å—ï¼šåˆ©ç”¨MLLMè‡ªåŠ¨ç”Ÿæˆå’Œè¿‡æ»¤å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ„å»ºå¤§è§„æ¨¡æ•°æ®é›†WebPersonã€‚2) ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«æå–å›¾åƒå’Œæ–‡æœ¬çš„ç‰¹å¾ã€‚3) æ¢¯åº¦æ³¨æ„åŠ›æ¨¡å—ï¼šè®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„æ¢¯åº¦æ³¨æ„åŠ›ç›¸ä¼¼åº¦ï¼Œç”¨äºæŒ‡å¯¼æ–‡æœ¬tokençš„æ©ç ã€‚4) åŒæ©ç æ¨¡å—ï¼šæ ¹æ®æ¢¯åº¦æ³¨æ„åŠ›ç›¸ä¼¼åº¦è‡ªé€‚åº”åœ°æ©ç›–å™ªå£°æ–‡æœ¬tokenï¼Œå¹¶å¼•å…¥æ©ç tokené¢„æµ‹ç›®æ ‡ã€‚5) å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å¾—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾åœ¨è¯­ä¹‰ç©ºé—´ä¸­å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ¢¯åº¦æ³¨æ„åŠ›å¼•å¯¼çš„åŒæ©ç ååŒæ¡†æ¶ï¼ˆGA-DMSï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å’Œæ©ç›–å™ªå£°æ–‡æœ¬tokenï¼Œä»è€Œæé«˜è·¨æ¨¡æ€å¯¹é½çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥çš„æ©ç tokené¢„æµ‹ç›®æ ‡èƒ½å¤Ÿå¢å¼ºæ¨¡å‹å¯¹ç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæå‡è¡Œäººæ£€ç´¢çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGA-DMSèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤§è§„æ¨¡çš„å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å™ªå£°æ–‡æœ¬tokençš„å¹²æ‰°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¢¯åº¦æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„æ¢¯åº¦ä¿¡æ¯æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯†åˆ«å™ªå£°æ–‡æœ¬tokenã€‚åœ¨åŒæ©ç æ¨¡å—ä¸­ï¼Œæ ¹æ®æ¢¯åº¦æ³¨æ„åŠ›ç›¸ä¼¼åº¦è‡ªé€‚åº”åœ°è®¾ç½®æ©ç æ¦‚ç‡ï¼Œä»è€Œé¿å…è¿‡åº¦æ©ç æˆ–æ¬ æ©ç ã€‚åœ¨æ©ç tokené¢„æµ‹ç›®æ ‡ä¸­ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°é¢„æµ‹è¢«æ©ç çš„æ–‡æœ¬tokenã€‚æ•°æ®é›†WebPersonåŒ…å«5Må›¾åƒ-æ–‡æœ¬å¯¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GA-DMSåœ¨å¤šä¸ªè¡Œäººæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨CUHK-PEDESæ•°æ®é›†ä¸Šï¼ŒGA-DMSçš„Rank-1å‡†ç¡®ç‡è¾¾åˆ°äº†XX%ï¼Œç›¸æ¯”äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•æå‡äº†YY%ã€‚åœ¨å¦ä¸€ä¸ªæ•°æ®é›†ä¸Šï¼ŒGA-DMSä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®‰é˜²ã€æ™ºæ…§åŸå¸‚ç­‰é¢†åŸŸï¼Œä¾‹å¦‚åœ¨ç›‘æ§è§†é¢‘ä¸­é€šè¿‡æ–‡æœ¬æè¿°å¿«é€Ÿæ£€ç´¢ç›®æ ‡äººç‰©ï¼Œæˆ–åœ¨ç”µå•†å¹³å°ä¸­æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æè¿°æ¨èç›¸å…³çš„æœè£…å•†å“ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡è¡Œäººæ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.

