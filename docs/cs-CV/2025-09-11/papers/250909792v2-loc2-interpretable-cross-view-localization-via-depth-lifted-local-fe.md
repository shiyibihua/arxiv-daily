---
layout: default
title: Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching
---

# Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09792" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09792v2</a>
  <a href="https://arxiv.org/pdf/2509.09792.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09792v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09792v2', 'Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zimin Xia, Chenghao Xu, Alexandre Alahi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoc$^2$ï¼Œé€šè¿‡æ·±åº¦æå‡çš„å±€éƒ¨ç‰¹å¾åŒ¹é…å®ç°å¯è§£é‡Šçš„è·¨è§†è§’å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è·¨è§†è§’å®šä½` `å±€éƒ¨ç‰¹å¾åŒ¹é…` `æ·±åº¦ä¼°è®¡` `é¸Ÿç°å›¾` `Procrusteså¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨è§†è§’å®šä½æ–¹æ³•ä¾èµ–å…¨å±€æè¿°ç¬¦æˆ–BEVå˜æ¢ï¼Œç¼ºä¹å±€éƒ¨å¯¹åº”å…³ç³»å’Œå¯è§£é‡Šæ€§ã€‚
2. Loc$^2$é€šè¿‡å­¦ä¹ åœ°é¢-èˆªæ‹å›¾åƒå¹³é¢å¯¹åº”å…³ç³»ï¼Œå¹¶å°†åœ°é¢ç‚¹æå‡åˆ°BEVç©ºé—´è¿›è¡Œä½å§¿ä¼°è®¡ï¼Œå®ç°ç²¾ç¡®å®šä½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLoc$^2$åœ¨è·¨åŒºåŸŸæµ‹è¯•å’ŒæœªçŸ¥æ–¹å‘ç­‰åœºæ™¯ä¸­è¾¾åˆ°SOTAï¼Œå¹¶æä¾›å®šä½ç²¾åº¦çš„ç›´è§‚å¯è§†åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾ç¡®ä¸”å¯è§£é‡Šçš„ç»†ç²’åº¦è·¨è§†è§’å®šä½æ–¹æ³•Loc$^2$ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†åœ°é¢å›¾åƒçš„å±€éƒ¨ç‰¹å¾ä¸å‚è€ƒèˆªæ‹å›¾åƒè¿›è¡ŒåŒ¹é…ï¼Œæ¥ä¼°è®¡åœ°é¢å›¾åƒçš„3è‡ªç”±åº¦(DoF)ä½å§¿ã€‚ä¸ä¾èµ–å…¨å±€æè¿°ç¬¦æˆ–é¸Ÿç°å›¾(BEV)å˜æ¢çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å­¦ä¹ åœ°é¢-èˆªæ‹å›¾åƒå¹³é¢å¯¹åº”å…³ç³»ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªç›¸æœºä½å§¿çš„å¼±ç›‘ç£ã€‚åŒ¹é…çš„åœ°é¢ç‚¹é€šè¿‡å•ç›®æ·±åº¦é¢„æµ‹è¢«æå‡åˆ°BEVç©ºé—´ï¼Œç„¶ååº”ç”¨å°ºåº¦æ„ŸçŸ¥çš„Procrusteså¯¹é½æ¥ä¼°è®¡ç›¸æœºæ—‹è½¬ã€å¹³ç§»ï¼Œä»¥åŠå¯é€‰çš„ç›¸å¯¹æ·±åº¦å’Œèˆªæ‹åº¦é‡ç©ºé—´ä¹‹é—´çš„å°ºåº¦ã€‚è¿™ç§å…¬å¼è½»é‡çº§ã€ç«¯åˆ°ç«¯å¯è®­ç»ƒï¼Œå¹¶ä¸”ä¸éœ€è¦åƒç´ çº§æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è·¨åŒºåŸŸæµ‹è¯•å’ŒæœªçŸ¥æ–¹å‘ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„å¯è§£é‡Šæ€§ï¼šå¯¹åº”è´¨é‡ç›´æ¥åæ˜ å®šä½ç²¾åº¦ï¼Œå¹¶æ”¯æŒé€šè¿‡RANSACè¿›è¡Œå¼‚å¸¸å€¼å‰”é™¤ï¼ŒåŒæ—¶å°†é‡æ–°ç¼©æ”¾çš„åœ°é¢å¸ƒå±€å åŠ åœ¨èˆªæ‹å›¾åƒä¸Šï¼Œæä¾›äº†å®šä½ç²¾åº¦çš„ç›´è§‚è§†è§‰æç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè·¨è§†è§’å®šä½æ—¨åœ¨ç¡®å®šåœ°é¢å›¾åƒç›¸å¯¹äºèˆªæ‹å›¾åƒçš„ä½å§¿ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…¨å±€å›¾åƒæè¿°ç¬¦æˆ–é¸Ÿç°å›¾(BEV)å˜æ¢ï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹å¯¹å±€éƒ¨å¯¹åº”å…³ç³»çš„å»ºæ¨¡ï¼Œå¯è§£é‡Šæ€§è¾ƒå·®ï¼Œä¸”åœ¨è§†è§’å·®å¼‚å¤§ã€å…‰ç…§å˜åŒ–å‰§çƒˆç­‰å¤æ‚åœºæ™¯ä¸‹æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„åƒç´ çº§æ ‡æ³¨æ•°æ®ï¼Œå¢åŠ äº†è®­ç»ƒæˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLoc$^2$çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥å­¦ä¹ åœ°é¢å›¾åƒå’Œèˆªæ‹å›¾åƒä¹‹é—´çš„å±€éƒ¨ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡å°†åœ°é¢ç‰¹å¾ç‚¹æå‡åˆ°BEVç©ºé—´ã€‚é€šè¿‡åœ¨BEVç©ºé—´ä¸­è¿›è¡Œå°ºåº¦æ„ŸçŸ¥çš„Procrusteså¯¹é½ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä¼°è®¡ç›¸æœºä½å§¿ï¼ŒåŒæ—¶é¿å…äº†å¯¹å…¨å±€æè¿°ç¬¦çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å®šä½ç²¾åº¦ï¼Œè¿˜æä¾›äº†æ›´å¼ºçš„å¯è§£é‡Šæ€§ï¼Œå› ä¸ºå¯ä»¥é€šè¿‡åˆ†æå±€éƒ¨ç‰¹å¾å¯¹åº”å…³ç³»æ¥è¯„ä¼°å®šä½ç»“æœçš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoc$^2$çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å±€éƒ¨ç‰¹å¾æå–ï¼šåˆ†åˆ«ä»åœ°é¢å›¾åƒå’Œèˆªæ‹å›¾åƒä¸­æå–å±€éƒ¨ç‰¹å¾ç‚¹ã€‚2) ç‰¹å¾åŒ¹é…ï¼šå»ºç«‹åœ°é¢å›¾åƒå’Œèˆªæ‹å›¾åƒä¹‹é—´çš„ç‰¹å¾å¯¹åº”å…³ç³»ã€‚3) å•ç›®æ·±åº¦ä¼°è®¡ï¼šåˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œé¢„æµ‹åœ°é¢å›¾åƒçš„æ·±åº¦å›¾ã€‚4) BEVæå‡ï¼šå°†åŒ¹é…çš„åœ°é¢ç‰¹å¾ç‚¹æ ¹æ®æ·±åº¦ä¿¡æ¯æå‡åˆ°BEVç©ºé—´ã€‚5) ä½å§¿ä¼°è®¡ï¼šåœ¨BEVç©ºé—´ä¸­è¿›è¡Œå°ºåº¦æ„ŸçŸ¥çš„Procrusteså¯¹é½ï¼Œä¼°è®¡ç›¸æœºæ—‹è½¬ã€å¹³ç§»å’Œå°ºåº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šLoc$^2$çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç›´æ¥å­¦ä¹ åœ°é¢-èˆªæ‹å›¾åƒå¹³é¢å¯¹åº”å…³ç³»ï¼Œé¿å…äº†å¯¹å…¨å±€æè¿°ç¬¦çš„ä¾èµ–ã€‚2) åˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡å°†åœ°é¢ç‰¹å¾ç‚¹æå‡åˆ°BEVç©ºé—´ï¼Œä»è€Œå¯ä»¥åœ¨BEVç©ºé—´ä¸­è¿›è¡Œä½å§¿ä¼°è®¡ã€‚3) æå‡ºäº†ä¸€ç§å°ºåº¦æ„ŸçŸ¥çš„Procrusteså¯¹é½æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶ä¼°è®¡ç›¸æœºæ—‹è½¬ã€å¹³ç§»å’Œå°ºåº¦ã€‚4) è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„å¯è§£é‡Šæ€§ï¼Œå¯ä»¥é€šè¿‡åˆ†æå±€éƒ¨ç‰¹å¾å¯¹åº”å…³ç³»æ¥è¯„ä¼°å®šä½ç»“æœçš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLoc$^2$çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetä½œä¸ºç‰¹å¾æå–å™¨ã€‚2) ä½¿ç”¨åŸºäºTransformerçš„æ¶æ„è¿›è¡Œç‰¹å¾åŒ¹é…ã€‚3) ä½¿ç”¨é¢„è®­ç»ƒçš„å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œã€‚4) å°ºåº¦æ„ŸçŸ¥çš„Procrusteså¯¹é½æŸå¤±å‡½æ•°ï¼Œç”¨äºä¼˜åŒ–ç›¸æœºä½å§¿å’Œå°ºåº¦ã€‚5) ä½¿ç”¨RANSACè¿›è¡Œå¼‚å¸¸å€¼å‰”é™¤ï¼Œæé«˜å®šä½ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLoc$^2$åœ¨è·¨åŒºåŸŸæµ‹è¯•å’ŒæœªçŸ¥æ–¹å‘ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨University-1700æ•°æ®é›†ä¸Šï¼ŒLoc$^2$çš„å®šä½ç²¾åº¦æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒLoc$^2$çš„å¯è§£é‡Šæ€§ä½¿å…¶å¯ä»¥é€šè¿‡RANSACè¿›è¡Œå¼‚å¸¸å€¼å‰”é™¤ï¼Œè¿›ä¸€æ­¥æé«˜å®šä½ç²¾åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Loc$^2$å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ã€åŸå¸‚è§„åˆ’ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥ç”¨äºåœ¨ç¼ºä¹GPSä¿¡å·æˆ–è§†è§‰ç¯å¢ƒå¤æ‚çš„åœºæ™¯ä¸­è¿›è¡Œç²¾ç¡®å®šä½ã€‚æ­¤å¤–ï¼ŒLoc$^2$çš„å¯è§£é‡Šæ€§ä½¿å…¶å¯ä»¥ç”¨äºè¯„ä¼°å®šä½ç»“æœçš„å¯é æ€§ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°ä¸‰ç»´é‡å»ºã€åœºæ™¯ç†è§£ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose an accurate and interpretable fine-grained cross-view localization method that estimates the 3 Degrees of Freedom (DoF) pose of a ground-level image by matching its local features with a reference aerial image. Unlike prior approaches that rely on global descriptors or bird's-eye-view (BEV) transformations, our method directly learns ground-aerial image-plane correspondences using weak supervision from camera poses. The matched ground points are lifted into BEV space with monocular depth predictions, and scale-aware Procrustes alignment is then applied to estimate camera rotation, translation, and optionally the scale between relative depth and the aerial metric space. This formulation is lightweight, end-to-end trainable, and requires no pixel-level annotations. Experiments show state-of-the-art accuracy in challenging scenarios such as cross-area testing and unknown orientation. Furthermore, our method offers strong interpretability: correspondence quality directly reflects localization accuracy and enables outlier rejection via RANSAC, while overlaying the re-scaled ground layout on the aerial image provides an intuitive visual cue of localization accuracy.

