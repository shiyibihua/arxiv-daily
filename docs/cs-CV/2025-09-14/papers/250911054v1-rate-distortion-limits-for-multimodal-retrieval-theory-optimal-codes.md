---
layout: default
title: Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees
---

# Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.11054" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.11054v1</a>
  <a href="https://arxiv.org/pdf/2509.11054.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.11054v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.11054v1', 'Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thomas Y. Chen

**åˆ†ç±»**: cs.IT, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-14

**å¤‡æ³¨**: ICCV MRR 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ£€ç´¢çš„ä¿¡æ¯è®ºæé™ä»¥ä¼˜åŒ–æ£€ç´¢æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `ä¿¡æ¯è®ºæé™` `ç†µåŠ æƒé‡åŒ–` `æœ‰æŸæºç¼–ç ` `è‡ªé€‚åº”è§£ç å™¨` `Blahut-Arimotoæ–¹æ³•` `æœ‰é™æ ·æœ¬åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ£€ç´¢æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„ä¿¡æ¯è®ºæé™ï¼Œå¯¼è‡´æ£€ç´¢æ•ˆæœä¸ç†æƒ³ã€‚
2. è®ºæ–‡é€šè¿‡å°†æ’åè§†ä¸ºæœ‰æŸæºç¼–ç ï¼Œæå‡ºäº†ç†µåŠ æƒéšæœºé‡åŒ–å™¨å’Œé€‚åº”æ€§è§£ç å™¨ï¼Œä¼˜åŒ–äº†å¤šæ¨¡æ€æ£€ç´¢çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨æ§åˆ¶çš„é«˜æ–¯æ··åˆå’ŒFlickr30kæ•°æ®é›†ä¸Šï¼Œæ€§èƒ½æ¥è¿‘ç†è®ºæé™ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é¦–æ¬¡å»ºç«‹äº†å¤šæ¨¡æ€æ£€ç´¢çš„ä¿¡æ¯è®ºæé™ã€‚å°†æ’åé—®é¢˜è§†ä¸ºæœ‰æŸæºç¼–ç ï¼Œæ¨å¯¼å‡ºäº’æƒ æ’åå¤±çœŸä¸‹çš„å•ä¿¡é“ç‡-å¤±çœŸå‡½æ•°R(D)ï¼Œå¹¶è¯æ˜äº†ä¸€ä¸ªåŒ…å«æ¨¡æ€å¹³è¡¡é¡¹å’Œåæ–œæƒ©ç½šçš„å¯¹å¶ç•Œé™ã€‚æ„å»ºäº†æ˜¾å¼çš„ç†µåŠ æƒéšæœºé‡åŒ–å™¨ï¼Œå¹¶é€šè¿‡Blahut-Arimotoæ–¹æ³•è¯æ˜è¯¥æ–¹æ¡ˆåœ¨O(n^{-1})çš„å¤±çœŸèŒƒå›´å†…å®ç°R(D)ã€‚æ­¤å¤–ï¼Œæå‡ºäº†é¦–ä¸ªæœ‰é™æ ·æœ¬è¿‡åº¦é£é™©ç•Œé™ï¼Œå¤æ‚åº¦åœ¨æ¨¡æ€æ•°é‡å’Œç†µå·®ä¸Šå‡ä»¥æ¬¡çº¿æ€§æ–¹å¼ç¼©æ”¾ã€‚å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„è‡ªé€‚åº”ç¼–ç åœ¨ç†è®ºè¾¹ç•Œå†…è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå›ºå®šæ¸©åº¦å’Œç®€å•CLIPåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ£€ç´¢ä¸­çš„ä¿¡æ¯è®ºæé™é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æ˜ç¡®ç•Œå®šæ‰€éœ€çš„ä¿¡æ¯é‡ï¼Œå¯¼è‡´æ£€ç´¢è´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å¤šæ¨¡æ€æ£€ç´¢ä¸­çš„æ’åé—®é¢˜è§†ä¸ºæœ‰æŸæºç¼–ç ï¼Œæ¨å¯¼å‡ºäº’æƒ æ’åå¤±çœŸä¸‹çš„å•ä¿¡é“ç‡-å¤±çœŸå‡½æ•°R(D)ï¼Œå¹¶æ„å»ºç†µåŠ æƒçš„éšæœºé‡åŒ–å™¨ï¼Œä»¥é€‚åº”ä¸åŒæ¨¡æ€çš„ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¿¡æ¯è®ºæé™çš„æ¨å¯¼ã€ç†µåŠ æƒéšæœºé‡åŒ–å™¨çš„è®¾è®¡ï¼Œä»¥åŠåŸºäºBlahut-Arimotoæ–¹æ³•çš„å¤±çœŸåˆ†æã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ¨¡æ€å¹³è¡¡é¡¹ã€åæ–œæƒ©ç½šé¡¹å’Œé€‚åº”æ€§è§£ç å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡æ˜ç¡®äº†å¤šæ¨¡æ€æ£€ç´¢çš„ä¿¡æ¯è®ºæé™ï¼Œå¹¶æå‡ºäº†ç†µåŠ æƒçš„é‡åŒ–æ–¹æ¡ˆï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•æ›´å¥½åœ°å¤„ç†äº†æ¨¡æ€é—´çš„å†—ä½™å’Œç†µä¸å¹³è¡¡é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”çš„æ¯æ¨¡æ€æ¸©åº¦è§£ç å™¨ï¼Œå¹¶é€šè¿‡Blahut-Arimotoæ–¹æ³•ç¡®ä¿åœ¨O(n^{-1})çš„å¤±çœŸèŒƒå›´å†…å®ç°R(D)ã€‚æ­¤å¤–ï¼Œæœ‰é™æ ·æœ¬è¿‡åº¦é£é™©ç•Œé™çš„åˆ†æä¹Ÿä¸ºå®é™…åº”ç”¨æä¾›äº†ç†è®ºæ”¯æŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„è‡ªé€‚åº”ç¼–ç æ–¹æ³•åœ¨æ§åˆ¶çš„é«˜æ–¯æ··åˆå’ŒFlickr30kæ•°æ®é›†ä¸Šï¼Œæ€§èƒ½æ¥è¿‘ç†è®ºæé™ï¼Œè¯¯å·®ä»…åœ¨ä¸¤ä¸ªç™¾åˆ†ç‚¹ä»¥å†…ã€‚è€Œä¼ ç»Ÿçš„å›ºå®šæ¸©åº¦å’Œç®€å•CLIPåŸºçº¿æ–¹æ³•åˆ™æ˜¾è‘—è½åï¼ŒéªŒè¯äº†æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¤šæ¨¡æ€æ£€ç´¢é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒä¸æ–‡æœ¬æ£€ç´¢ã€è§†é¢‘ç†è§£å’Œä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä¸­ã€‚é€šè¿‡ä¼˜åŒ–æ£€ç´¢è¿‡ç¨‹ï¼Œå¯ä»¥æé«˜ç”¨æˆ·ä½“éªŒå’Œä¿¡æ¯è·å–æ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½æœç´¢å¼•æ“å’Œæ¨èç³»ç»Ÿçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½åº”ç”¨äºæŒç»­å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹ä¸­ï¼Œè¿›ä¸€æ­¥æå‡å¤šæ¨¡æ€äº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We establish the first information-theoretic limits for multimodal retrieval. Casting ranking as lossy source coding, we derive a single-letter rate-distortion function $R(D)$ for reciprocal-rank distortion and prove a converse bound that splits into a modality-balanced term plus a skew penalty $Îº\,Î”H$ capturing entropy imbalance and cross-modal redundancy. We then construct an explicit entropy-weighted stochastic quantizer with an adaptive, per-modality temperature decoder; a Blahut-Arimoto argument shows this scheme achieves distortion within $O(n^{-1})$ of $R(D)$ using $n$ training triples. A VC-type analysis yields the first finite-sample excess-risk bound whose complexity scales sub-linearly in both the number of modalities and the entropy gap. Experiments on controlled Gaussian mixtures and Flickr30k confirm that our adaptive codes sit within two percentage points of the theoretical frontier, while fixed-temperature and naive CLIP baselines lag significantly. Taken together, our results give a principled answer to "how many bits per query are necessary" for high-quality multimodal retrieval and provide design guidance for entropy-aware contrastive objectives, continual-learning retrievers, and retrieval-augmented generators.

