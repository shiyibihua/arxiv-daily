---
layout: default
title: VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection
---

# VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection

**arXiv**: [2511.18075v1](https://arxiv.org/abs/2511.18075) | [PDF](https://arxiv.org/pdf/2511.18075.pdf)

**ä½œè€…**: Jianhang Yao, Yongbin Zheng, Siqi Lu, Wanying Xu, Peng Sun

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

**å¤‡æ³¨**: 15 pages, 8 figures, accepted by AAAI 2026

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VK-Detï¼šè§†è§‰çŸ¥è¯†å¼•å¯¼çš„åŽŸåž‹å­¦ä¹ ç”¨äºŽå¼€æ”¾è¯æ±‡ç©ºä¸­ç›®æ ‡æ£€æµ‹**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹` `è§†è§‰çŸ¥è¯†å¼•å¯¼` `åŽŸåž‹å­¦ä¹ ` `ä¼ªæ ‡ç­¾` `ç©ºä¸­ç›®æ ‡æ£€æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¾èµ–æ–‡æœ¬ç›‘ç£ï¼Œæ˜“äº§ç”Ÿè¯­ä¹‰åå·®ï¼Œé™åˆ¶äº†å¯¹æ–‡æœ¬æœªæŒ‡å®šæ¦‚å¿µçš„æ³›åŒ–èƒ½åŠ›ã€‚
2. VK-Detæ¡†æž¶åˆ©ç”¨è§†è§‰ç¼–ç å™¨å›ºæœ‰çš„åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›è¿›è¡Œç»†ç²’åº¦å®šä½å’Œè‡ªé€‚åº”è’¸é¦ï¼Œæ— éœ€é¢å¤–æ–‡æœ¬ç›‘ç£ã€‚
3. æå‡ºçš„åŽŸåž‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç­–ç•¥é€šè¿‡ç‰¹å¾èšç±»å’ŒåŽŸåž‹åŒ¹é…ï¼Œå¢žå¼ºå¯¹æ–°ç›®æ ‡çš„å…³æ³¨ï¼Œå¹¶åœ¨DIORå’ŒDOTAæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è¯†åˆ«è¶…å‡ºé¢„å®šä¹‰ç±»åˆ«çš„ç›®æ ‡ï¼Œå¼€æ”¾è¯æ±‡ç©ºä¸­ç›®æ ‡æ£€æµ‹(OVAD)åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡åž‹(VLM)çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä»ŽåŸºç¡€ç±»åˆ«æŽ¨å¹¿åˆ°æ–°ç±»åˆ«ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨è‡ªå­¦ä¹ æœºåˆ¶å’Œå¼±æ–‡æœ¬ç›‘ç£æ¥ç”ŸæˆåŒºåŸŸçº§ä¼ªæ ‡ç­¾ï¼Œä»¥ä½¿æ£€æµ‹å™¨ä¸ŽVLMè¯­ä¹‰ç©ºé—´å¯¹é½ã€‚ç„¶è€Œï¼Œæ–‡æœ¬ä¾èµ–æ€§ä¼šå¼•å…¥è¯­ä¹‰åå·®ï¼Œé™åˆ¶å¼€æ”¾è¯æ±‡æ‰©å±•åˆ°æ–‡æœ¬æŒ‡å®šçš„æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†VK-Detï¼Œä¸€ä¸ªæ— éœ€é¢å¤–ç›‘ç£çš„è§†è§‰çŸ¥è¯†å¼•å¯¼çš„å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ¡†æž¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‘çŽ°å¹¶åˆ©ç”¨è§†è§‰ç¼–ç å™¨å›ºæœ‰çš„ä¿¡æ¯åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥èŽ·å¾—ç»†ç²’åº¦çš„å®šä½å’Œè‡ªé€‚åº”è’¸é¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŽŸåž‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç­–ç•¥ã€‚å®ƒé€šè¿‡ç‰¹å¾èšç±»å¯¹ç±»é—´å†³ç­–è¾¹ç•Œè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡åŽŸåž‹åŒ¹é…å°†æ£€æµ‹åŒºåŸŸæ˜ å°„åˆ°æ½œåœ¨ç±»åˆ«ã€‚è¿™å¢žå¼ºäº†å¯¹æ–°ç›®æ ‡çš„å…³æ³¨ï¼ŒåŒæ—¶å¼¥è¡¥äº†ç¼ºå¤±çš„ç›‘ç£ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨DIORä¸Šè¾¾åˆ°30.1 mAP^Nï¼Œåœ¨DOTAä¸Šè¾¾åˆ°23.3 mAP^Nï¼Œç”šè‡³ä¼˜äºŽé¢å¤–çš„ç›‘ç£æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼€æ”¾è¯æ±‡ç©ºä¸­ç›®æ ‡æ£€æµ‹æ—¨åœ¨æ£€æµ‹é¢„å®šä¹‰ç±»åˆ«ä¹‹å¤–çš„æ–°ç±»åˆ«ç›®æ ‡ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–äºŽæ–‡æœ¬ç›‘ç£ï¼Œé€šè¿‡ç”Ÿæˆä¼ªæ ‡ç­¾æ¥å¯¹é½æ£€æµ‹å™¨å’Œè§†è§‰-è¯­è¨€æ¨¡åž‹çš„è¯­ä¹‰ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™ç§æ–‡æœ¬ä¾èµ–æ€§å¼•å…¥äº†è¯­ä¹‰åå·®ï¼Œé™åˆ¶äº†æ¨¡åž‹å¯¹æœªåœ¨æ–‡æœ¬ä¸­æ˜Žç¡®æŒ‡å®šçš„æ¦‚å¿µçš„æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•å‡å°‘å¯¹æ–‡æœ¬ç›‘ç£çš„ä¾èµ–ï¼Œæé«˜æ¨¡åž‹å¯¹æœªçŸ¥ç›®æ ‡çš„æ£€æµ‹èƒ½åŠ›æ˜¯å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVK-Detçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰ç¼–ç å™¨è‡ªèº«çš„èƒ½åŠ›æ¥å‘çŽ°å’Œåˆ©ç”¨å›¾åƒä¸­çš„ä¿¡æ¯åŒºåŸŸï¼Œä»Žè€Œå‡å°‘å¯¹æ–‡æœ¬ç›‘ç£çš„ä¾èµ–ã€‚é€šè¿‡è§†è§‰çŸ¥è¯†å¼•å¯¼ï¼Œæ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ï¼Œå¹¶ç”Ÿæˆæ›´å‡†ç¡®çš„ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼ŒåŽŸåž‹å­¦ä¹ ç”¨äºŽå»ºæ¨¡ç±»é—´å†³ç­–è¾¹ç•Œï¼Œä»Žè€Œæ›´å¥½åœ°è¯†åˆ«æ–°ç±»åˆ«ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šVK-Detæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºŽæå–å›¾åƒç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å…¶å›ºæœ‰çš„åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›è¿›è¡Œç»†ç²’åº¦å®šä½ã€‚2) è‡ªé€‚åº”è’¸é¦æ¨¡å—ï¼šåˆ©ç”¨è§†è§‰ç¼–ç å™¨çš„åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯¹æ£€æµ‹å™¨è¿›è¡Œè‡ªé€‚åº”è’¸é¦ï¼Œæé«˜æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚3) åŽŸåž‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾æ¨¡å—ï¼šé€šè¿‡ç‰¹å¾èšç±»å’ŒåŽŸåž‹åŒ¹é…ï¼Œç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç”¨äºŽè®­ç»ƒæ£€æµ‹å™¨ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆåˆ©ç”¨è§†è§‰ç¼–ç å™¨æå–ç‰¹å¾ï¼Œç„¶åŽè¿›è¡Œè‡ªé€‚åº”è’¸é¦ï¼Œæœ€åŽåˆ©ç”¨åŽŸåž‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šVK-Detçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) åˆ©ç”¨è§†è§‰ç¼–ç å™¨å›ºæœ‰çš„åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›ï¼Œå‡å°‘å¯¹æ–‡æœ¬ç›‘ç£çš„ä¾èµ–ã€‚2) æå‡ºäº†ä¸€ç§æ–°çš„åŽŸåž‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å¾èšç±»å’ŒåŽŸåž‹åŒ¹é…ï¼Œå¢žå¼ºå¯¹æ–°ç›®æ ‡çš„å…³æ³¨ã€‚3) æå‡ºäº†è‡ªé€‚åº”è’¸é¦æ¨¡å—ï¼Œåˆ©ç”¨è§†è§‰ç¼–ç å™¨çš„åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›ï¼Œæé«˜æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVK-Detæ— éœ€é¢å¤–çš„æ–‡æœ¬ç›‘ç£ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ–°ç±»åˆ«ç›®æ ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŽŸåž‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾æ¨¡å—ä¸­ï¼Œä½¿ç”¨K-meansç®—æ³•è¿›è¡Œç‰¹å¾èšç±»ï¼Œç”ŸæˆåŽŸåž‹ã€‚åŽŸåž‹æ•°é‡çš„é€‰æ‹©æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œéœ€è¦æ ¹æ®æ•°æ®é›†çš„ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åˆ†ç±»æŸå¤±å’Œå®šä½æŸå¤±ï¼Œç”¨äºŽè®­ç»ƒæ£€æµ‹å™¨ã€‚è‡ªé€‚åº”è’¸é¦æ¨¡å—ä¸­ï¼Œä½¿ç”¨KLæ•£åº¦ä½œä¸ºè’¸é¦æŸå¤±ï¼Œç”¨äºŽå°†è§†è§‰ç¼–ç å™¨çš„çŸ¥è¯†è¿ç§»åˆ°æ£€æµ‹å™¨ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

VK-Detåœ¨DIORæ•°æ®é›†ä¸Šå–å¾—äº†30.1 mAP^Nï¼Œåœ¨DOTAæ•°æ®é›†ä¸Šå–å¾—äº†23.3 mAP^Nï¼Œè¶…è¶Šäº†çŽ°æœ‰çš„SOTAæ–¹æ³•ï¼Œç”šè‡³ä¼˜äºŽä½¿ç”¨é¢å¤–ç›‘ç£çš„æ–¹æ³•ã€‚è¿™è¡¨æ˜ŽVK-Detèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è§†è§‰çŸ¥è¯†è¿›è¡Œå¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽé¥æ„Ÿå›¾åƒåˆ†æžã€æ™ºèƒ½äº¤é€šã€åŸŽå¸‚è§„åˆ’ã€ç¾å®³ç›‘æµ‹ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç¾å®³ç›‘æµ‹ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å¿«é€Ÿæ£€æµ‹å‡ºå—ç¾åŒºåŸŸçš„æ–°å¢žå»ºç­‘ç‰©æˆ–å—æŸæƒ…å†µï¼Œä¸ºæ•‘æ´å·¥ä½œæä¾›æ”¯æŒã€‚åœ¨æ™ºèƒ½äº¤é€šä¸­ï¼Œå¯ä»¥æ£€æµ‹å‡ºæ–°çš„äº¤é€šæ ‡å¿—æˆ–é“è·¯éšœç¢ç‰©ï¼Œæé«˜äº¤é€šå®‰å…¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

