---
layout: default
title: VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection
---

# VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18075" target="_blank" class="toolbar-btn">arXiv: 2511.18075v1</a>
    <a href="https://arxiv.org/pdf/2511.18075.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18075v1" 
            onclick="toggleFavorite(this, '2511.18075v1', 'VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jianhang Yao, Yongbin Zheng, Siqi Lu, Wanying Xu, Peng Sun

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-22

**Â§áÊ≥®**: 15 pages, 8 figures, accepted by AAAI 2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VK-DetÔºöËßÜËßâÁü•ËØÜÂºïÂØºÁöÑÂéüÂûãÂ≠¶‰π†Áî®‰∫éÂºÄÊîæËØçÊ±áÁ©∫‰∏≠ÁõÆÊ†áÊ£ÄÊµã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµã` `ËßÜËßâÁü•ËØÜÂºïÂØº` `ÂéüÂûãÂ≠¶‰π†` `‰º™Ê†áÁ≠æ` `Á©∫‰∏≠ÁõÆÊ†áÊ£ÄÊµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ï‰æùËµñÊñáÊú¨ÁõëÁù£ÔºåÊòì‰∫ßÁîüËØ≠‰πâÂÅèÂ∑ÆÔºåÈôêÂà∂‰∫ÜÂØπÊñáÊú¨Êú™ÊåáÂÆöÊ¶ÇÂøµÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. VK-DetÊ°ÜÊû∂Âà©Áî®ËßÜËßâÁºñÁ†ÅÂô®Âõ∫ÊúâÁöÑÂå∫ÂüüÊÑüÁü•ËÉΩÂäõËøõË°åÁªÜÁ≤íÂ∫¶ÂÆö‰ΩçÂíåËá™ÈÄÇÂ∫îËí∏È¶èÔºåÊó†ÈúÄÈ¢ùÂ§ñÊñáÊú¨ÁõëÁù£„ÄÇ
3. ÊèêÂá∫ÁöÑÂéüÂûãÊÑüÁü•‰º™Ê†áÁ≠æÁ≠ñÁï•ÈÄöËøáÁâπÂæÅËÅöÁ±ªÂíåÂéüÂûãÂåπÈÖçÔºåÂ¢ûÂº∫ÂØπÊñ∞ÁõÆÊ†áÁöÑÂÖ≥Ê≥®ÔºåÂπ∂Âú®DIORÂíåDOTAÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜËØÜÂà´Ë∂ÖÂá∫È¢ÑÂÆö‰πâÁ±ªÂà´ÁöÑÁõÆÊ†áÔºåÂºÄÊîæËØçÊ±áÁ©∫‰∏≠ÁõÆÊ†áÊ£ÄÊµã(OVAD)Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÁöÑÈõ∂Ê†∑Êú¨ËÉΩÂäõÔºå‰ªéÂü∫Á°ÄÁ±ªÂà´Êé®ÂπøÂà∞Êñ∞Á±ªÂà´„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®Ëá™Â≠¶‰π†Êú∫Âà∂ÂíåÂº±ÊñáÊú¨ÁõëÁù£Êù•ÁîüÊàêÂå∫ÂüüÁ∫ß‰º™Ê†áÁ≠æÔºå‰ª•‰ΩøÊ£ÄÊµãÂô®‰∏éVLMËØ≠‰πâÁ©∫Èó¥ÂØπÈΩê„ÄÇÁÑ∂ËÄåÔºåÊñáÊú¨‰æùËµñÊÄß‰ºöÂºïÂÖ•ËØ≠‰πâÂÅèÂ∑ÆÔºåÈôêÂà∂ÂºÄÊîæËØçÊ±áÊâ©Â±ïÂà∞ÊñáÊú¨ÊåáÂÆöÁöÑÊ¶ÇÂøµ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜVK-DetÔºå‰∏Ä‰∏™Êó†ÈúÄÈ¢ùÂ§ñÁõëÁù£ÁöÑËßÜËßâÁü•ËØÜÂºïÂØºÁöÑÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÊ°ÜÊû∂„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ÂèëÁé∞Âπ∂Âà©Áî®ËßÜËßâÁºñÁ†ÅÂô®Âõ∫ÊúâÁöÑ‰ø°ÊÅØÂå∫ÂüüÊÑüÁü•ËÉΩÂäõÔºå‰ª•Ëé∑ÂæóÁªÜÁ≤íÂ∫¶ÁöÑÂÆö‰ΩçÂíåËá™ÈÄÇÂ∫îËí∏È¶è„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂéüÂûãÊÑüÁü•‰º™Ê†áÁ≠æÁ≠ñÁï•„ÄÇÂÆÉÈÄöËøáÁâπÂæÅËÅöÁ±ªÂØπÁ±ªÈó¥ÂÜ≥Á≠ñËæπÁïåËøõË°åÂª∫Ê®°ÔºåÂπ∂ÈÄöËøáÂéüÂûãÂåπÈÖçÂ∞ÜÊ£ÄÊµãÂå∫ÂüüÊò†Â∞ÑÂà∞ÊΩúÂú®Á±ªÂà´„ÄÇËøôÂ¢ûÂº∫‰∫ÜÂØπÊñ∞ÁõÆÊ†áÁöÑÂÖ≥Ê≥®ÔºåÂêåÊó∂Âº•Ë°•‰∫ÜÁº∫Â§±ÁöÑÁõëÁù£„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂÖ∑ÊúâÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂú®DIOR‰∏äËææÂà∞30.1 mAP^NÔºåÂú®DOTA‰∏äËææÂà∞23.3 mAP^NÔºåÁîöËá≥‰ºò‰∫éÈ¢ùÂ§ñÁöÑÁõëÁù£ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂºÄÊîæËØçÊ±áÁ©∫‰∏≠ÁõÆÊ†áÊ£ÄÊµãÊó®Âú®Ê£ÄÊµãÈ¢ÑÂÆö‰πâÁ±ªÂà´‰πãÂ§ñÁöÑÊñ∞Á±ªÂà´ÁõÆÊ†á„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÊñáÊú¨ÁõëÁù£ÔºåÈÄöËøáÁîüÊàê‰º™Ê†áÁ≠æÊù•ÂØπÈΩêÊ£ÄÊµãÂô®ÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËØ≠‰πâÁ©∫Èó¥„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñáÊú¨‰æùËµñÊÄßÂºïÂÖ•‰∫ÜËØ≠‰πâÂÅèÂ∑ÆÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÂØπÊú™Âú®ÊñáÊú¨‰∏≠ÊòéÁ°ÆÊåáÂÆöÁöÑÊ¶ÇÂøµÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂáèÂ∞ëÂØπÊñáÊú¨ÁõëÁù£ÁöÑ‰æùËµñÔºåÊèêÈ´òÊ®°ÂûãÂØπÊú™Áü•ÁõÆÊ†áÁöÑÊ£ÄÊµãËÉΩÂäõÊòØÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVK-DetÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâÁºñÁ†ÅÂô®Ëá™Ë∫´ÁöÑËÉΩÂäõÊù•ÂèëÁé∞ÂíåÂà©Áî®ÂõæÂÉè‰∏≠ÁöÑ‰ø°ÊÅØÂå∫ÂüüÔºå‰ªéËÄåÂáèÂ∞ëÂØπÊñáÊú¨ÁõëÁù£ÁöÑ‰æùËµñ„ÄÇÈÄöËøáËßÜËßâÁü•ËØÜÂºïÂØºÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂõæÂÉèÂÜÖÂÆπÔºåÂπ∂ÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑ‰º™Ê†áÁ≠æ„ÄÇÊ≠§Â§ñÔºåÂéüÂûãÂ≠¶‰π†Áî®‰∫éÂª∫Ê®°Á±ªÈó¥ÂÜ≥Á≠ñËæπÁïåÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ËØÜÂà´Êñ∞Á±ªÂà´ÁõÆÊ†á„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVK-DetÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ËßÜËßâÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÊèêÂèñÂõæÂÉèÁâπÂæÅÔºåÂπ∂Âà©Áî®ÂÖ∂Âõ∫ÊúâÁöÑÂå∫ÂüüÊÑüÁü•ËÉΩÂäõËøõË°åÁªÜÁ≤íÂ∫¶ÂÆö‰Ωç„ÄÇ2) Ëá™ÈÄÇÂ∫îËí∏È¶èÊ®°ÂùóÔºöÂà©Áî®ËßÜËßâÁºñÁ†ÅÂô®ÁöÑÂå∫ÂüüÊÑüÁü•ËÉΩÂäõÔºåÂØπÊ£ÄÊµãÂô®ËøõË°åËá™ÈÄÇÂ∫îËí∏È¶èÔºåÊèêÈ´òÊ£ÄÊµãÂô®ÁöÑÊÄßËÉΩ„ÄÇ3) ÂéüÂûãÊÑüÁü•‰º™Ê†áÁ≠æÊ®°ÂùóÔºöÈÄöËøáÁâπÂæÅËÅöÁ±ªÂíåÂéüÂûãÂåπÈÖçÔºåÁîüÊàê‰º™Ê†áÁ≠æÔºåÁî®‰∫éËÆ≠ÁªÉÊ£ÄÊµãÂô®„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÂÖàÂà©Áî®ËßÜËßâÁºñÁ†ÅÂô®ÊèêÂèñÁâπÂæÅÔºåÁÑ∂ÂêéËøõË°åËá™ÈÄÇÂ∫îËí∏È¶èÔºåÊúÄÂêéÂà©Áî®ÂéüÂûãÊÑüÁü•‰º™Ê†áÁ≠æËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVK-DetÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) Âà©Áî®ËßÜËßâÁºñÁ†ÅÂô®Âõ∫ÊúâÁöÑÂå∫ÂüüÊÑüÁü•ËÉΩÂäõÔºåÂáèÂ∞ëÂØπÊñáÊú¨ÁõëÁù£ÁöÑ‰æùËµñ„ÄÇ2) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂéüÂûãÊÑüÁü•‰º™Ê†áÁ≠æÁ≠ñÁï•ÔºåÈÄöËøáÁâπÂæÅËÅöÁ±ªÂíåÂéüÂûãÂåπÈÖçÔºåÂ¢ûÂº∫ÂØπÊñ∞ÁõÆÊ†áÁöÑÂÖ≥Ê≥®„ÄÇ3) ÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îËí∏È¶èÊ®°ÂùóÔºåÂà©Áî®ËßÜËßâÁºñÁ†ÅÂô®ÁöÑÂå∫ÂüüÊÑüÁü•ËÉΩÂäõÔºåÊèêÈ´òÊ£ÄÊµãÂô®ÁöÑÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåVK-DetÊó†ÈúÄÈ¢ùÂ§ñÁöÑÊñáÊú¨ÁõëÁù£ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Ê≥õÂåñÂà∞Êñ∞Á±ªÂà´ÁõÆÊ†á„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂéüÂûãÊÑüÁü•‰º™Ê†áÁ≠æÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®K-meansÁÆóÊ≥ïËøõË°åÁâπÂæÅËÅöÁ±ªÔºåÁîüÊàêÂéüÂûã„ÄÇÂéüÂûãÊï∞ÈáèÁöÑÈÄâÊã©ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÊï∞ÊçÆÈõÜÁöÑÁâπÁÇπËøõË°åË∞ÉÊï¥„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂàÜÁ±ªÊçüÂ§±ÂíåÂÆö‰ΩçÊçüÂ§±ÔºåÁî®‰∫éËÆ≠ÁªÉÊ£ÄÊµãÂô®„ÄÇËá™ÈÄÇÂ∫îËí∏È¶èÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®KLÊï£Â∫¶‰Ωú‰∏∫Ëí∏È¶èÊçüÂ§±ÔºåÁî®‰∫éÂ∞ÜËßÜËßâÁºñÁ†ÅÂô®ÁöÑÁü•ËØÜËøÅÁßªÂà∞Ê£ÄÊµãÂô®„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VK-DetÂú®DIORÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü30.1 mAP^NÔºåÂú®DOTAÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü23.3 mAP^NÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑSOTAÊñπÊ≥ïÔºåÁîöËá≥‰ºò‰∫é‰ΩøÁî®È¢ùÂ§ñÁõëÁù£ÁöÑÊñπÊ≥ï„ÄÇËøôË°®ÊòéVK-DetËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®ËßÜËßâÁü•ËØÜËøõË°åÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÔºåÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÈÅ•ÊÑüÂõæÂÉèÂàÜÊûê„ÄÅÊô∫ËÉΩ‰∫§ÈÄö„ÄÅÂüéÂ∏ÇËßÑÂàí„ÄÅÁÅæÂÆ≥ÁõëÊµãÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁÅæÂÆ≥ÁõëÊµã‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊñπÊ≥ïÂø´ÈÄüÊ£ÄÊµãÂá∫ÂèóÁÅæÂå∫ÂüüÁöÑÊñ∞Â¢ûÂª∫Á≠ëÁâ©ÊàñÂèóÊçüÊÉÖÂÜµÔºå‰∏∫ÊïëÊè¥Â∑•‰ΩúÊèê‰æõÊîØÊåÅ„ÄÇÂú®Êô∫ËÉΩ‰∫§ÈÄö‰∏≠ÔºåÂèØ‰ª•Ê£ÄÊµãÂá∫Êñ∞ÁöÑ‰∫§ÈÄöÊ†áÂøóÊàñÈÅìË∑ØÈöúÁ¢çÁâ©ÔºåÊèêÈ´ò‰∫§ÈÄöÂÆâÂÖ®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

