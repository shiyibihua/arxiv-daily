---
layout: default
title: SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation
---

# SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation

**arXiv**: [2511.18127v1](https://arxiv.org/abs/2511.18127) | [PDF](https://arxiv.org/pdf/2511.18127.pdf)

**ä½œè€…**: Ruicong Liu, Yifei Huang, Liangyang Ouyang, Caixin Kang, Yoichi Sato

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ut-vision/SFHand) | [HUGGINGFACE](https://huggingface.co/datasets/ut-vision)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SFHandï¼šç”¨äºŽè¯­è¨€å¼•å¯¼çš„3Dæ‰‹éƒ¨é¢„æµ‹å’Œå…·èº«æ“ä½œçš„æµå¼æ¡†æž¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `3Dæ‰‹éƒ¨é¢„æµ‹` `æµå¼æ¡†æž¶` `è¯­è¨€å¼•å¯¼` `å…·èº«æ“ä½œ` `è‡ªå›žå½’æ¨¡åž‹` `ROIå¢žå¼º` `äººæœºäº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰3Dæ‰‹éƒ¨é¢„æµ‹æ–¹æ³•é€šå¸¸éœ€è¦ç¦»çº¿è®¿é—®ç´¯ç§¯çš„è§†é¢‘åºåˆ—ï¼Œä¸”æ— æ³•ç»“åˆè¯­è¨€æŒ‡å¯¼ï¼Œä¸é€‚ç”¨äºŽARå’Œè¾…åŠ©æœºå™¨äººç­‰åœºæ™¯ã€‚
2. SFHandé‡‡ç”¨æµå¼è‡ªå›žå½’æž¶æž„ï¼Œç»“åˆROIå¢žå¼ºçš„è®°å¿†å±‚ï¼Œä»Žè¿žç»­è§†é¢‘æµå’Œè¯­è¨€æŒ‡ä»¤ä¸­é¢„æµ‹æœªæ¥3Dæ‰‹éƒ¨çŠ¶æ€ã€‚
3. SFHandåœ¨3Dæ‰‹éƒ¨é¢„æµ‹ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•35.8%ï¼Œè¿ç§»åˆ°å…·èº«æ“ä½œä»»åŠ¡åŽï¼Œä»»åŠ¡æˆåŠŸçŽ‡æå‡é«˜è¾¾13.4%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSFHandï¼Œé¦–ä¸ªç”¨äºŽè¯­è¨€å¼•å¯¼çš„3Dæ‰‹éƒ¨é¢„æµ‹æµå¼æ¡†æž¶ã€‚è¯¥æ¡†æž¶ä»Žè¿žç»­çš„è§†é¢‘æµå’Œè¯­è¨€æŒ‡ä»¤ä¸­è‡ªå›žå½’åœ°é¢„æµ‹æœªæ¥3Dæ‰‹éƒ¨çŠ¶æ€ï¼ŒåŒ…æ‹¬æ‰‹éƒ¨ç±»åž‹ã€2Dè¾¹ç•Œæ¡†ã€3Då§¿åŠ¿å’Œè½¨è¿¹ã€‚SFHandç»“åˆäº†æµå¼è‡ªå›žå½’æž¶æž„å’ŒROIå¢žå¼ºçš„è®°å¿†å±‚ï¼Œåœ¨æ•èŽ·æ—¶é—´ä¸Šä¸‹æ–‡çš„åŒæ—¶ï¼Œä¸“æ³¨äºŽä»¥æ‰‹ä¸ºä¸­å¿ƒçš„æ˜¾è‘—åŒºåŸŸã€‚åŒæ—¶ï¼Œæœ¬æ–‡å‘å¸ƒäº†EgoHaFLï¼Œé¦–ä¸ªåŒ…å«åŒæ­¥3Dæ‰‹éƒ¨å§¿åŠ¿å’Œè¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®žéªŒè¡¨æ˜Žï¼ŒSFHandåœ¨3Dæ‰‹éƒ¨é¢„æµ‹æ–¹é¢å–å¾—äº†æ–°çš„state-of-the-artç»“æžœï¼Œæ€§èƒ½æå‡é«˜è¾¾35.8%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†å­¦ä¹ åˆ°çš„è¡¨å¾è¿ç§»åˆ°ä¸‹æ¸¸çš„å…·èº«æ“ä½œä»»åŠ¡ä¸­ï¼Œä»»åŠ¡æˆåŠŸçŽ‡æé«˜äº†é«˜è¾¾13.4%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰3Dæ‰‹éƒ¨é¢„æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºŽç¦»çº¿è§†é¢‘åºåˆ—ï¼Œæ— æ³•å¤„ç†å®žæ—¶æµæ•°æ®ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹è¯­è¨€æŒ‡ä»¤çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œé™åˆ¶äº†å…¶åœ¨ARã€æœºå™¨äººç­‰äº¤äº’åœºæ™¯ä¸­çš„åº”ç”¨ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥æ•æ‰æ‰‹éƒ¨åŠ¨ä½œçš„æ—¶åºä¾èµ–å…³ç³»ï¼Œä¹Ÿæ— æ³•æ ¹æ®ä»»åŠ¡æ„å›¾è¿›è¡Œé¢„æµ‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSFHandçš„æ ¸å¿ƒåœ¨äºŽæž„å»ºä¸€ä¸ªæµå¼è‡ªå›žå½’æ¡†æž¶ï¼Œèƒ½å¤Ÿä»Žè¿žç»­çš„è§†é¢‘æµå’Œè¯­è¨€æŒ‡ä»¤ä¸­é¢„æµ‹æœªæ¥çš„3Dæ‰‹éƒ¨çŠ¶æ€ã€‚é€šè¿‡ç»“åˆROIå¢žå¼ºçš„è®°å¿†å±‚ï¼Œæ¨¡åž‹èƒ½å¤Ÿä¸“æ³¨äºŽæ‰‹éƒ¨åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ•èŽ·æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»Žè€Œå®žçŽ°æ›´å‡†ç¡®ã€æ›´å®žæ—¶çš„æ‰‹éƒ¨é¢„æµ‹ã€‚è¯­è¨€æŒ‡ä»¤çš„å¼•å…¥ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿç†è§£ä»»åŠ¡æ„å›¾ï¼Œä»Žè€Œè¿›è¡Œæ›´å…·é’ˆå¯¹æ€§çš„é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSFHandæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†é¢‘æµè¾“å…¥æ¨¡å—ï¼Œç”¨äºŽæŽ¥æ”¶è¿žç»­çš„è§†é¢‘å¸§ï¼›2) è¯­è¨€æŒ‡ä»¤è¾“å…¥æ¨¡å—ï¼Œç”¨äºŽæŽ¥æ”¶ä»»åŠ¡ç›¸å…³çš„è¯­è¨€æè¿°ï¼›3) ç‰¹å¾æå–æ¨¡å—ï¼Œç”¨äºŽæå–è§†é¢‘å¸§å’Œè¯­è¨€æŒ‡ä»¤çš„ç‰¹å¾è¡¨ç¤ºï¼›4) ROIå¢žå¼ºçš„è®°å¿†å±‚ï¼Œç”¨äºŽå­˜å‚¨å’Œæ›´æ–°æ‰‹éƒ¨åŒºåŸŸçš„æ—¶åºä¿¡æ¯ï¼›5) è‡ªå›žå½’é¢„æµ‹æ¨¡å—ï¼Œç”¨äºŽæ ¹æ®å½“å‰çŠ¶æ€å’ŒåŽ†å²ä¿¡æ¯é¢„æµ‹æœªæ¥çš„3Dæ‰‹éƒ¨çŠ¶æ€ï¼ŒåŒ…æ‹¬æ‰‹éƒ¨ç±»åž‹ã€2Dè¾¹ç•Œæ¡†ã€3Då§¿åŠ¿å’Œè½¨è¿¹ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œå¯ä»¥è¿›è¡Œå®žæ—¶é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSFHandçš„å…³é”®åˆ›æ–°åœ¨äºŽä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†é¦–ä¸ªç”¨äºŽè¯­è¨€å¼•å¯¼çš„3Dæ‰‹éƒ¨é¢„æµ‹æµå¼æ¡†æž¶ï¼›2) ç»“åˆäº†æµå¼è‡ªå›žå½’æž¶æž„å’ŒROIå¢žå¼ºçš„è®°å¿†å±‚ï¼Œæœ‰æ•ˆæ•èŽ·äº†æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ‰‹éƒ¨åŒºåŸŸç‰¹å¾ï¼›3) å¼•å…¥äº†è¯­è¨€æŒ‡ä»¤ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿç†è§£ä»»åŠ¡æ„å›¾ï¼Œä»Žè€Œè¿›è¡Œæ›´å…·é’ˆå¯¹æ€§çš„é¢„æµ‹ï¼›4) å‘å¸ƒäº†EgoHaFLæ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æ•°æ®æ”¯æŒã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼ŒSFHandèƒ½å¤Ÿå¤„ç†å®žæ—¶æµæ•°æ®ï¼Œå¹¶ç»“åˆè¯­è¨€æŒ‡ä»¤è¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ROIå¢žå¼ºçš„è®°å¿†å±‚ä¸­ï¼Œä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥é€‰æ‹©æ€§åœ°å…³æ³¨é‡è¦çš„æ‰‹éƒ¨åŒºåŸŸã€‚è‡ªå›žå½’é¢„æµ‹æ¨¡å—é‡‡ç”¨äº†GRUæˆ–LSTMç­‰å¾ªçŽ¯ç¥žç»ç½‘ç»œç»“æž„ï¼Œä»¥æ•æ‰æ—¶é—´ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬3Då§¿åŠ¿é¢„æµ‹æŸå¤±ã€2Dè¾¹ç•Œæ¡†é¢„æµ‹æŸå¤±ã€æ‰‹éƒ¨ç±»åž‹åˆ†ç±»æŸå¤±å’Œè½¨è¿¹é¢„æµ‹æŸå¤±ã€‚å…·ä½“å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚å­¦ä¹ çŽ‡ã€batch sizeã€å¾ªçŽ¯ç¥žç»ç½‘ç»œçš„éšè—å±‚å¤§å°ç­‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SFHandåœ¨EgoHaFLæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨3Dæ‰‹éƒ¨é¢„æµ‹æ–¹é¢ï¼Œç›¸è¾ƒäºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡é«˜è¾¾35.8%ã€‚åŒæ—¶ï¼Œé€šè¿‡å°†å­¦ä¹ åˆ°çš„è¡¨å¾è¿ç§»åˆ°ä¸‹æ¸¸çš„å…·èº«æ“ä½œä»»åŠ¡ä¸­ï¼Œä»»åŠ¡æˆåŠŸçŽ‡æé«˜äº†é«˜è¾¾13.4%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒSFHandèƒ½å¤Ÿæœ‰æ•ˆåœ°é¢„æµ‹æœªæ¥çš„3Dæ‰‹éƒ¨çŠ¶æ€ï¼Œå¹¶èƒ½å¤Ÿæ³›åŒ–åˆ°å…¶ä»–ç›¸å…³ä»»åŠ¡ä¸­ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SFHandåœ¨å¢žå¼ºçŽ°å®žï¼ˆARï¼‰ã€è¾…åŠ©æœºå™¨äººã€äººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ARæ¸¸æˆä¸­ï¼Œå¯ä»¥æ ¹æ®çŽ©å®¶çš„è¯­éŸ³æŒ‡ä»¤é¢„æµ‹æ‰‹éƒ¨åŠ¨ä½œï¼Œå®žçŽ°æ›´è‡ªç„¶ã€æ›´æµç•…çš„äº¤äº’ä½“éªŒã€‚åœ¨è¾…åŠ©æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£äººç±»çš„æ„å›¾ï¼Œä»Žè€Œæ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒSFHandè¿˜å¯ä»¥åº”ç”¨äºŽæ‰‹è¯­è¯†åˆ«ã€è™šæ‹ŸçŽ°å®žç­‰é¢†åŸŸï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

