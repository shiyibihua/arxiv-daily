---
layout: default
title: SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation
---

# SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18127" target="_blank" class="toolbar-btn">arXiv: 2511.18127v1</a>
    <a href="https://arxiv.org/pdf/2511.18127.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18127v1" 
            onclick="toggleFavorite(this, '2511.18127v1', 'SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ruicong Liu, Yifei Huang, Liangyang Ouyang, Caixin Kang, Yoichi Sato

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-22

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ut-vision/SFHand) | [HUGGINGFACE](https://huggingface.co/datasets/ut-vision)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SFHandÔºöÁî®‰∫éËØ≠Ë®ÄÂºïÂØºÁöÑ3DÊâãÈÉ®È¢ÑÊµãÂíåÂÖ∑Ë∫´Êìç‰ΩúÁöÑÊµÅÂºèÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `3DÊâãÈÉ®È¢ÑÊµã` `ÊµÅÂºèÊ°ÜÊû∂` `ËØ≠Ë®ÄÂºïÂØº` `ÂÖ∑Ë∫´Êìç‰Ωú` `Ëá™ÂõûÂΩíÊ®°Âûã` `ROIÂ¢ûÂº∫` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DÊâãÈÉ®È¢ÑÊµãÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÁ¶ªÁ∫øËÆøÈóÆÁ¥ØÁßØÁöÑËßÜÈ¢ëÂ∫èÂàóÔºå‰∏îÊó†Ê≥ïÁªìÂêàËØ≠Ë®ÄÊåáÂØºÔºå‰∏çÈÄÇÁî®‰∫éARÂíåËæÖÂä©Êú∫Âô®‰∫∫Á≠âÂú∫ÊôØ„ÄÇ
2. SFHandÈááÁî®ÊµÅÂºèËá™ÂõûÂΩíÊû∂ÊûÑÔºåÁªìÂêàROIÂ¢ûÂº∫ÁöÑËÆ∞ÂøÜÂ±ÇÔºå‰ªéËøûÁª≠ËßÜÈ¢ëÊµÅÂíåËØ≠Ë®ÄÊåá‰ª§‰∏≠È¢ÑÊµãÊú™Êù•3DÊâãÈÉ®Áä∂ÊÄÅ„ÄÇ
3. SFHandÂú®3DÊâãÈÉ®È¢ÑÊµã‰∏äË∂ÖË∂äÁé∞ÊúâÊñπÊ≥ï35.8%ÔºåËøÅÁßªÂà∞ÂÖ∑Ë∫´Êìç‰Ωú‰ªªÂä°ÂêéÔºå‰ªªÂä°ÊàêÂäüÁéáÊèêÂçáÈ´òËææ13.4%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫SFHandÔºåÈ¶ñ‰∏™Áî®‰∫éËØ≠Ë®ÄÂºïÂØºÁöÑ3DÊâãÈÉ®È¢ÑÊµãÊµÅÂºèÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂‰ªéËøûÁª≠ÁöÑËßÜÈ¢ëÊµÅÂíåËØ≠Ë®ÄÊåá‰ª§‰∏≠Ëá™ÂõûÂΩíÂú∞È¢ÑÊµãÊú™Êù•3DÊâãÈÉ®Áä∂ÊÄÅÔºåÂåÖÊã¨ÊâãÈÉ®Á±ªÂûã„ÄÅ2DËæπÁïåÊ°Ü„ÄÅ3DÂßøÂäøÂíåËΩ®Ëøπ„ÄÇSFHandÁªìÂêà‰∫ÜÊµÅÂºèËá™ÂõûÂΩíÊû∂ÊûÑÂíåROIÂ¢ûÂº∫ÁöÑËÆ∞ÂøÜÂ±ÇÔºåÂú®ÊçïËé∑Êó∂Èó¥‰∏ä‰∏ãÊñáÁöÑÂêåÊó∂Ôºå‰∏ìÊ≥®‰∫é‰ª•Êâã‰∏∫‰∏≠ÂøÉÁöÑÊòæËëóÂå∫Âüü„ÄÇÂêåÊó∂ÔºåÊú¨ÊñáÂèëÂ∏É‰∫ÜEgoHaFLÔºåÈ¶ñ‰∏™ÂåÖÂê´ÂêåÊ≠•3DÊâãÈÉ®ÂßøÂäøÂíåËØ≠Ë®ÄÊåá‰ª§ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ„ÄÇÂÆûÈ™åË°®ÊòéÔºåSFHandÂú®3DÊâãÈÉ®È¢ÑÊµãÊñπÈù¢ÂèñÂæó‰∫ÜÊñ∞ÁöÑstate-of-the-artÁªìÊûúÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ35.8%„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑË°®ÂæÅËøÅÁßªÂà∞‰∏ãÊ∏∏ÁöÑÂÖ∑Ë∫´Êìç‰Ωú‰ªªÂä°‰∏≠Ôºå‰ªªÂä°ÊàêÂäüÁéáÊèêÈ´ò‰∫ÜÈ´òËææ13.4%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ3DÊâãÈÉ®È¢ÑÊµãÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÁ¶ªÁ∫øËßÜÈ¢ëÂ∫èÂàóÔºåÊó†Ê≥ïÂ§ÑÁêÜÂÆûÊó∂ÊµÅÊï∞ÊçÆÔºåÂπ∂‰∏îÁº∫‰πèÂØπËØ≠Ë®ÄÊåá‰ª§ÁöÑÊúâÊïàÂà©Áî®ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®AR„ÄÅÊú∫Âô®‰∫∫Á≠â‰∫§‰∫íÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇËøô‰∫õÊñπÊ≥ïÈöæ‰ª•ÊçïÊçâÊâãÈÉ®Âä®‰ΩúÁöÑÊó∂Â∫è‰æùËµñÂÖ≥Á≥ªÔºå‰πüÊó†Ê≥ïÊ†πÊçÆ‰ªªÂä°ÊÑèÂõæËøõË°åÈ¢ÑÊµã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSFHandÁöÑÊ†∏ÂøÉÂú®‰∫éÊûÑÂª∫‰∏Ä‰∏™ÊµÅÂºèËá™ÂõûÂΩíÊ°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéËøûÁª≠ÁöÑËßÜÈ¢ëÊµÅÂíåËØ≠Ë®ÄÊåá‰ª§‰∏≠È¢ÑÊµãÊú™Êù•ÁöÑ3DÊâãÈÉ®Áä∂ÊÄÅ„ÄÇÈÄöËøáÁªìÂêàROIÂ¢ûÂº∫ÁöÑËÆ∞ÂøÜÂ±ÇÔºåÊ®°ÂûãËÉΩÂ§ü‰∏ìÊ≥®‰∫éÊâãÈÉ®Âå∫ÂüüÔºåÂπ∂ÊúâÊïàÊçïËé∑Êó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÂÆûÊó∂ÁöÑÊâãÈÉ®È¢ÑÊµã„ÄÇËØ≠Ë®ÄÊåá‰ª§ÁöÑÂºïÂÖ•‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÁêÜËß£‰ªªÂä°ÊÑèÂõæÔºå‰ªéËÄåËøõË°åÊõ¥ÂÖ∑ÈíàÂØπÊÄßÁöÑÈ¢ÑÊµã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSFHandÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ËßÜÈ¢ëÊµÅËæìÂÖ•Ê®°ÂùóÔºåÁî®‰∫éÊé•Êî∂ËøûÁª≠ÁöÑËßÜÈ¢ëÂ∏ßÔºõ2) ËØ≠Ë®ÄÊåá‰ª§ËæìÂÖ•Ê®°ÂùóÔºåÁî®‰∫éÊé•Êî∂‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËØ≠Ë®ÄÊèèËø∞Ôºõ3) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºåÁî®‰∫éÊèêÂèñËßÜÈ¢ëÂ∏ßÂíåËØ≠Ë®ÄÊåá‰ª§ÁöÑÁâπÂæÅË°®Á§∫Ôºõ4) ROIÂ¢ûÂº∫ÁöÑËÆ∞ÂøÜÂ±ÇÔºåÁî®‰∫éÂ≠òÂÇ®ÂíåÊõ¥Êñ∞ÊâãÈÉ®Âå∫ÂüüÁöÑÊó∂Â∫è‰ø°ÊÅØÔºõ5) Ëá™ÂõûÂΩíÈ¢ÑÊµãÊ®°ÂùóÔºåÁî®‰∫éÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÂíåÂéÜÂè≤‰ø°ÊÅØÈ¢ÑÊµãÊú™Êù•ÁöÑ3DÊâãÈÉ®Áä∂ÊÄÅÔºåÂåÖÊã¨ÊâãÈÉ®Á±ªÂûã„ÄÅ2DËæπÁïåÊ°Ü„ÄÅ3DÂßøÂäøÂíåËΩ®Ëøπ„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÁ´ØÂà∞Á´ØÁöÑÔºåÂèØ‰ª•ËøõË°åÂÆûÊó∂È¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSFHandÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1) ÊèêÂá∫‰∫ÜÈ¶ñ‰∏™Áî®‰∫éËØ≠Ë®ÄÂºïÂØºÁöÑ3DÊâãÈÉ®È¢ÑÊµãÊµÅÂºèÊ°ÜÊû∂Ôºõ2) ÁªìÂêà‰∫ÜÊµÅÂºèËá™ÂõûÂΩíÊû∂ÊûÑÂíåROIÂ¢ûÂº∫ÁöÑËÆ∞ÂøÜÂ±ÇÔºåÊúâÊïàÊçïËé∑‰∫ÜÊó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÂíåÊâãÈÉ®Âå∫ÂüüÁâπÂæÅÔºõ3) ÂºïÂÖ•‰∫ÜËØ≠Ë®ÄÊåá‰ª§Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÁêÜËß£‰ªªÂä°ÊÑèÂõæÔºå‰ªéËÄåËøõË°åÊõ¥ÂÖ∑ÈíàÂØπÊÄßÁöÑÈ¢ÑÊµãÔºõ4) ÂèëÂ∏É‰∫ÜEgoHaFLÊï∞ÊçÆÈõÜÔºå‰∏∫Áõ∏ÂÖ≥Á†îÁ©∂Êèê‰æõ‰∫ÜÊï∞ÊçÆÊîØÊåÅ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåSFHandËÉΩÂ§üÂ§ÑÁêÜÂÆûÊó∂ÊµÅÊï∞ÊçÆÔºåÂπ∂ÁªìÂêàËØ≠Ë®ÄÊåá‰ª§ËøõË°åÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ROIÂ¢ûÂº∫ÁöÑËÆ∞ÂøÜÂ±Ç‰∏≠Ôºå‰ΩøÁî®‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ÈÄâÊã©ÊÄßÂú∞ÂÖ≥Ê≥®ÈáçË¶ÅÁöÑÊâãÈÉ®Âå∫Âüü„ÄÇËá™ÂõûÂΩíÈ¢ÑÊµãÊ®°ÂùóÈááÁî®‰∫ÜGRUÊàñLSTMÁ≠âÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºå‰ª•ÊçïÊçâÊó∂Èó¥‰æùËµñÂÖ≥Á≥ª„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨3DÂßøÂäøÈ¢ÑÊµãÊçüÂ§±„ÄÅ2DËæπÁïåÊ°ÜÈ¢ÑÊµãÊçüÂ§±„ÄÅÊâãÈÉ®Á±ªÂûãÂàÜÁ±ªÊçüÂ§±ÂíåËΩ®ËøπÈ¢ÑÊµãÊçüÂ§±„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÊï∞ÊçÆÈõÜÂíå‰ªªÂä°ËøõË°åË∞ÉÊï¥Ôºå‰æãÂ¶ÇÂ≠¶‰π†Áéá„ÄÅbatch size„ÄÅÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑÈöêËóèÂ±ÇÂ§ßÂ∞èÁ≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SFHandÂú®EgoHaFLÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú®3DÊâãÈÉ®È¢ÑÊµãÊñπÈù¢ÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ35.8%„ÄÇÂêåÊó∂ÔºåÈÄöËøáÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑË°®ÂæÅËøÅÁßªÂà∞‰∏ãÊ∏∏ÁöÑÂÖ∑Ë∫´Êìç‰Ωú‰ªªÂä°‰∏≠Ôºå‰ªªÂä°ÊàêÂäüÁéáÊèêÈ´ò‰∫ÜÈ´òËææ13.4%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåSFHandËÉΩÂ§üÊúâÊïàÂú∞È¢ÑÊµãÊú™Êù•ÁöÑ3DÊâãÈÉ®Áä∂ÊÄÅÔºåÂπ∂ËÉΩÂ§üÊ≥õÂåñÂà∞ÂÖ∂‰ªñÁõ∏ÂÖ≥‰ªªÂä°‰∏≠„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SFHandÂú®Â¢ûÂº∫Áé∞ÂÆûÔºàARÔºâ„ÄÅËæÖÂä©Êú∫Âô®‰∫∫„ÄÅ‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®ARÊ∏∏Êàè‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÁé©ÂÆ∂ÁöÑËØ≠Èü≥Êåá‰ª§È¢ÑÊµãÊâãÈÉ®Âä®‰ΩúÔºåÂÆûÁé∞Êõ¥Ëá™ÁÑ∂„ÄÅÊõ¥ÊµÅÁïÖÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇÂú®ËæÖÂä©Êú∫Âô®‰∫∫È¢ÜÂüüÔºåÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£‰∫∫Á±ªÁöÑÊÑèÂõæÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÂÆåÊàê‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåSFHandËøòÂèØ‰ª•Â∫îÁî®‰∫éÊâãËØ≠ËØÜÂà´„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

