---
layout: default
title: PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning
---

# PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.17927" target="_blank" class="toolbar-btn">arXiv: 2511.17927v1</a>
    <a href="https://arxiv.org/pdf/2511.17927.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.17927v1" 
            onclick="toggleFavorite(this, '2511.17927v1', 'PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yingjie Ma, Xun Lin, Yong Xu, Weicheng Xie, Zitong Yu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-22

**Â§áÊ≥®**: Accepted by AAAI 2026 (Oral)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PA-FASÔºåÈÄöËøáË∑ØÂæÑÂ¢ûÂº∫Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅ‰∫∫ËÑ∏ÂèçÊ¨∫È™óÁöÑÊ≥õÂåñÊÄßÂíåÂèØËß£ÈáäÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫ËÑ∏ÂèçÊ¨∫È™ó` `Â§öÊ®°ÊÄÅËûçÂêà` `Âº∫ÂåñÂ≠¶‰π†` `Ë∑ØÂæÑÂ¢ûÂº∫` `Ë∑®ÂüüÊ≥õÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅ‰∫∫ËÑ∏ÂèçÊ¨∫È™ó‰∏≠ÔºåÈù¢‰∏¥Êé®ÁêÜË∑ØÂæÑÂèóÈôêÂíåÂçï‰ªªÂä°ÁõëÁù£‰∏éÂ§öÊ†∑Êé®ÁêÜË∑ØÂæÑ‰∏çÂåπÈÖçÁöÑÈóÆÈ¢òÔºåÂØºËá¥Ê≥õÂåñÊÄßÂíåÂèØËß£ÈáäÊÄß‰∏çË∂≥„ÄÇ
2. PA-FASÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÁöÑÊâ©Â±ïÊé®ÁêÜÂ∫èÂàóÊù•Â¢ûÂº∫Êé®ÁêÜË∑ØÂæÑÔºåÂπ∂ÂºïÂÖ•Á≠îÊ°àÊ¥óÁâåÊú∫Âà∂ÔºåÈÅøÂÖçÊ®°ÂûãÂà©Áî®Êç∑ÂæÑÔºå‰ªéËÄåÊèêÂçáÊé®ÁêÜÊ∑±Â∫¶„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPA-FASÊòæËëóÊèêÈ´ò‰∫ÜÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåË∑®ÂüüÊ≥õÂåñËÉΩÂäõÔºåÂÆûÁé∞‰∫ÜÊõ¥ÂèØ‰ø°ËµñÁöÑ‰∫∫ËÑ∏ÂèçÊ¨∫È™ó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∫∫ËÑ∏ÂèçÊ¨∫È™ó(FAS)ËøëÂπ¥Êù•Âú®Â§öÊ®°ÊÄÅËûçÂêà„ÄÅË∑®ÂüüÊ≥õÂåñÂíåÂèØËß£ÈáäÊÄßÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ï„ÄÇÂÄüÂä©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂº∫ÂåñÂ≠¶‰π†(RL)ÔºåÂü∫‰∫éÁ≠ñÁï•ÁöÑËÆ≠ÁªÉ‰∏∫ËÅîÂêàÂª∫Ê®°Ëøô‰∫õÊñπÈù¢Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊú∫‰ºö„ÄÇÁÑ∂ËÄåÔºåÂ§öÊ®°ÊÄÅÊé®ÁêÜÊØîÂçïÊ®°ÊÄÅÊé®ÁêÜÊõ¥Â§çÊùÇÔºåÈúÄË¶ÅÂáÜÁ°ÆÁöÑÁâπÂæÅË°®Á§∫ÂíåË∑®Ê®°ÊÄÅÈ™åËØÅÔºåÂêåÊó∂Èù¢‰∏¥È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢òÔºåËøô‰ΩøÂæóÁõ¥Êé•Â∫îÁî®RLÊïàÊûúÊ¨†‰Ω≥„ÄÇÊàë‰ª¨ÂèëÁé∞ÁõëÁù£ÂæÆË∞ÉÂä†RL(SFT+RL)Áî®‰∫éÂ§öÊ®°ÊÄÅFASÂ≠òÂú®‰∏§‰∏™ÂÖ≥ÈîÆÈôêÂà∂Ôºö(1)ÊúâÈôêÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜË∑ØÂæÑÈôêÂà∂‰∫Ü‰∫íË°•Ê®°ÊÄÅÁöÑ‰ΩøÁî®ÔºåÂπ∂Áº©Â∞è‰∫ÜSFTÂêéÁöÑÊé¢Á¥¢Á©∫Èó¥ÔºåÂâäÂº±‰∫ÜRLÁöÑÊïàÊûúÔºõ(2)Âçï‰ªªÂä°ÁõëÁù£‰∏éÂ§öÊ†∑ÂåñÊé®ÁêÜË∑ØÂæÑ‰∏çÂåπÈÖçÂØºËá¥Êé®ÁêÜÊ∑∑Ê∑ÜÔºåÊ®°ÂûãÂèØËÉΩÂà©Áî®Êç∑ÂæÑÂ∞ÜÂõæÂÉèÁõ¥Êé•Êò†Â∞ÑÂà∞Á≠îÊ°àÔºåËÄåÂøΩÁï•‰∫ÜÈ¢ÑÊúüÁöÑÊé®ÁêÜ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜPA-FASÔºåÂÆÉÈÄöËøá‰ªéÊúâÈôêÁöÑÊ†áÊ≥®‰∏≠ÊûÑÂª∫È´òË¥®ÈáèÁöÑÊâ©Â±ïÊé®ÁêÜÂ∫èÂàóÊù•Â¢ûÂº∫Êé®ÁêÜË∑ØÂæÑÔºå‰∏∞ÂØåË∑ØÂæÑÂπ∂ÊîæÊùæÊé¢Á¥¢Á∫¶Êùü„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•Âú®SFTÊúüÈó¥ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁ≠îÊ°àÊ¥óÁâåÊú∫Âà∂Ôºå‰ª•Âº∫Âà∂ËøõË°åÂÖ®Èù¢ÁöÑÂ§öÊ®°ÊÄÅÂàÜÊûêÔºåËÄå‰∏çÊòØ‰ΩøÁî®Ë°®Èù¢Á∫øÁ¥¢Ôºå‰ªéËÄåÈºìÂä±Êõ¥Ê∑±ÂÖ•ÁöÑÊé®ÁêÜÂπ∂ÂáèËΩªÊç∑ÂæÑÂ≠¶‰π†„ÄÇPA-FASÊòæËëóÊèêÈ´ò‰∫ÜÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåË∑®ÂüüÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Êõ¥Â•ΩÂú∞Áªü‰∏Ä‰∫ÜÂ§öÊ®°ÊÄÅËûçÂêà„ÄÅÊ≥õÂåñÂíåÂèØËß£ÈáäÊÄßÔºå‰ªéËÄåÂÆûÁé∞ÂèØ‰ø°ËµñÁöÑFAS„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅ‰∫∫ËÑ∏ÂèçÊ¨∫È™ó‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÁî±‰∫éÊé®ÁêÜË∑ØÂæÑÂèóÈôêÂíåÁõëÁù£ÊñπÂºè‰∏çÂΩìÂØºËá¥ÁöÑÊ≥õÂåñËÉΩÂäõÂ∑ÆÂíåÂèØËß£ÈáäÊÄß‰ΩéÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ÁõëÁù£ÂæÆË∞ÉÂä†Âº∫ÂåñÂ≠¶‰π†(SFT+RL)ÁöÑÊ°ÜÊû∂Ôºå‰ΩÜÂ≠òÂú®‰∏§‰∏™ÁóõÁÇπÔºö‰∏ÄÊòØÂ§öÊ®°ÊÄÅÊé®ÁêÜË∑ØÂæÑÊúâÈôêÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®‰∏çÂêåÊ®°ÊÄÅÁöÑ‰∫íË°•‰ø°ÊÅØÔºõ‰∫åÊòØÂçï‰ªªÂä°ÁõëÁù£ÂÆπÊòìÂØºËá¥Ê®°ÂûãÂ≠¶‰π†Êç∑ÂæÑÔºåÂøΩÁï•Ê∑±Â±ÇÊé®ÁêÜËøáÁ®ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂ¢ûÂº∫Êé®ÁêÜË∑ØÂæÑÂíåÊîπËøõÁõëÁù£ÊñπÂºèÊù•ÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÁöÑÊâ©Â±ïÊé®ÁêÜÂ∫èÂàóÊù•‰∏∞ÂØåÊé®ÁêÜË∑ØÂæÑÔºåÂπ∂ÈááÁî®Á≠îÊ°àÊ¥óÁâåÊú∫Âà∂Êù•ÈÅøÂÖçÊ®°ÂûãÂ≠¶‰π†Êç∑ÂæÑÔºå‰ªéËÄåÈºìÂä±Ê®°ÂûãËøõË°åÊõ¥Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÊÄÅÂàÜÊûê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPA-FASÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÊòØÁõëÁù£ÂæÆË∞É(SFT)Èò∂ÊÆµÔºåËØ•Èò∂ÊÆµ‰ΩøÁî®Á≠îÊ°àÊ¥óÁâåÊú∫Âà∂Êù•ËÆ≠ÁªÉÊ®°ÂûãÔºåÈÅøÂÖçÊ®°ÂûãÂ≠¶‰π†Êç∑ÂæÑ„ÄÇÁÑ∂ÂêéÊòØÂº∫ÂåñÂ≠¶‰π†(RL)Èò∂ÊÆµÔºåËØ•Èò∂ÊÆµ‰ΩøÁî®Â¢ûÂº∫ÁöÑÊé®ÁêÜË∑ØÂæÑÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºåÈºìÂä±Ê®°ÂûãËøõË°åÊõ¥Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÊÄÅÂàÜÊûê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜË∑ØÂæÑÂ¢ûÂº∫ÂíåÁ≠îÊ°àÊ¥óÁâå‰∏§ÁßçÊú∫Âà∂„ÄÇË∑ØÂæÑÂ¢ûÂº∫ÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÁöÑÊâ©Â±ïÊé®ÁêÜÂ∫èÂàóÊù•‰∏∞ÂØåÊé®ÁêÜË∑ØÂæÑÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁ≠îÊ°àÊ¥óÁâåÈÄöËøáÈöèÊú∫Êâì‰π±Á≠îÊ°àÁöÑÈ°∫Â∫èÊù•ÈÅøÂÖçÊ®°ÂûãÂ≠¶‰π†Êç∑ÂæÑÔºå‰ªéËÄåÈºìÂä±Ê®°ÂûãËøõË°åÊõ¥Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÊÄÅÂàÜÊûê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®SFTÈò∂ÊÆµÔºåÁ≠îÊ°àÊ¥óÁâåÊú∫Âà∂ÈÄöËøáÈöèÊú∫Êâì‰π±ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÁ≠îÊ°àÈ°∫Â∫èÔºåËø´‰ΩøÊ®°Âûã‰∏çËÉΩÁÆÄÂçïÂú∞Â∞ÜÂõæÂÉèÊò†Â∞ÑÂà∞Á≠îÊ°àÔºåËÄåÊòØÈúÄË¶ÅËøõË°åÊõ¥Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÊÄÅÂàÜÊûê„ÄÇÂú®RLÈò∂ÊÆµÔºåË∑ØÂæÑÂ¢ûÂº∫ÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÁöÑÊâ©Â±ïÊé®ÁêÜÂ∫èÂàóÔºå‰∏∫Ê®°ÂûãÊèê‰æõÊõ¥Â§öÁöÑÊé¢Á¥¢Á©∫Èó¥Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PA-FASÂú®Â§ö‰∏™‰∫∫ËÑ∏ÂèçÊ¨∫È™óÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPA-FAS‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÊòæËëóÊèêÂçá‰∫ÜË∑®ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éË∫´‰ªΩÈ™åËØÅ„ÄÅËÆøÈóÆÊéßÂà∂„ÄÅÈáëËûçÂÆâÂÖ®Á≠âÈ¢ÜÂüüÔºåÊúâÊïàÈò≤Âæ°Âü∫‰∫é‰∫∫ËÑ∏Ê¨∫È™óÁöÑÊîªÂáªÔºåÊèêÂçáÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄß‰∏éÂèØÈù†ÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊâ©Â±ïÂà∞Êõ¥Â§öÊ®°ÊÄÅÂíåÊõ¥Â§çÊùÇÁöÑÂèçÊ¨∫È™óÂú∫ÊôØÔºå‰æãÂ¶ÇËØ≠Èü≥ÂèçÊ¨∫È™ó„ÄÅË°å‰∏∫ÂèçÊ¨∫È™óÁ≠âÔºåÂÖ∑ÊúâÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.

