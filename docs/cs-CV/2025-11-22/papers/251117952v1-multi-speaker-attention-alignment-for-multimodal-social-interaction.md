---
layout: default
title: Multi-speaker Attention Alignment for Multimodal Social Interaction
---

# Multi-speaker Attention Alignment for Multimodal Social Interaction

**arXiv**: [2511.17952v1](https://arxiv.org/abs/2511.17952) | [PDF](https://arxiv.org/pdf/2511.17952.pdf)

**ä½œè€…**: Liangyang Ouyang, Yifei Huang, Mingfang Zhang, Caixin Kang, Ryosuke Furuta, Yoichi Sato

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ut-vision/SocialInteraction)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯´è¯äººæ³¨æ„åŠ›å¯¹é½æ–¹æ³•ï¼Œæå‡MLLMåœ¨å¤šæ¨¡æ€ç¤¾äº¤äº’åŠ¨ä¸­çš„ç†è§£èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸Žååº” (Interaction & Reaction)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç¤¾äº¤äº’åŠ¨ç†è§£` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šè¯´è¯äººåœºæ™¯` `è·¨æ¨¡æ€å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰MLLMåœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œè§†è§‰å’Œæ–‡æœ¬tokenç¼ºä¹è¯´è¯äººä¸€è‡´çš„å¯¹é½ï¼Œå¯¼è‡´è·¨æ¨¡æ€æ³¨æ„åŠ›è¾ƒå¼±ï¼Œå½±å“ç¤¾äº¤äº’åŠ¨ç†è§£ã€‚
2. æå‡ºä¸€ç§å¤šæ¨¡æ€å¤šè¯´è¯äººæ³¨æ„åŠ›å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è·¨æ¨¡æ€å¤´é€‰æ‹©å’Œè‡ªé€‚åº”ç¤¾äº¤æ„ŸçŸ¥æ³¨æ„åŠ›åå·®æ¥å¢žå¼ºè¯´è¯äººè§†è§‰å’Œæ–‡æœ¬çš„å¯¹é½ã€‚
3. åœ¨TVQA+ã€MMSIã€OnlineMMSIç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†MLLMåœ¨ç¤¾äº¤ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè¾¾åˆ°SOTAæ°´å¹³ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£è§†é¢‘ä¸­çš„ç¤¾äº¤äº’åŠ¨éœ€è¦æŽ¨ç†å£å¤´å’Œéžå£å¤´çº¿ç´¢çš„åŠ¨æ€äº¤äº’ï¼šè°åœ¨è¯´è¯ï¼Œå¯¹è°è¯´ï¼Œä»¥åŠä¼´éšçš„çœ¼ç¥žæˆ–æ‰‹åŠ¿ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰æ˜¯ç†æƒ³é€‰æ‹©ï¼Œä½†ç®€å•åœ°æ·»åŠ è§†è§‰è¾“å…¥åœ¨ç¤¾äº¤ä»»åŠ¡ä¸Šçš„æ”¶ç›Šå´å‡ºäººæ„æ–™åœ°ä¸ç¨³å®šã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„MLLMå†…éƒ¨çš„è·¨æ¨¡æ€æ³¨æ„åŠ›è¿›è¡Œå®šé‡åˆ†æžï¼Œæ­ç¤ºäº†ä¸€ä¸ªæ ¸å¿ƒå¤±æ•ˆæ¨¡å¼ï¼šåœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œè§†è§‰å’Œæ–‡æœ¬tokenç¼ºä¹è¯´è¯äººä¸€è‡´çš„å¯¹é½ï¼Œè¡¨çŽ°å‡ºæ¯”ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒå¼±å¾—å¤šçš„è·¨æ¨¡æ€æ³¨æ„åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¤šè¯´è¯äººæ³¨æ„åŠ›å¯¹é½æ–¹æ³•ï¼Œå¯ä»¥é›†æˆåˆ°çŽ°æœ‰çš„MLLMä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥åŠ¨æ€è·¨æ¨¡æ€å¤´é€‰æ‹©æ¥è¯†åˆ«æœ€è´Ÿè´£æŽ¥åœ°çš„æ³¨æ„åŠ›å¤´ã€‚ç„¶åŽï¼Œå°†ä»ŽçŽ°æœ‰æ³¨æ„åŠ›æ¨¡å¼å’Œè¯´è¯äººä½ç½®è®¡ç®—å‡ºçš„è‡ªé€‚åº”ç¤¾äº¤æ„ŸçŸ¥æ³¨æ„åŠ›åå·®æ³¨å…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚è¿™ç§åå·®å¢žå¼ºäº†è¯´è¯äººçš„è§†è§‰è¡¨ç¤ºå’Œä»–ä»¬çš„è¨€è¯­ä¹‹é—´çš„å¯¹é½ï¼Œè€Œæ— éœ€å¼•å…¥å¯è®­ç»ƒçš„å‚æ•°æˆ–æž¶æž„æ›´æ”¹ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•é›†æˆåˆ°ä¸‰ä¸ªä¸åŒçš„MLLMï¼ˆLLaVA-NeXT-Videoã€Qwen2.5-VL å’Œ InternVL3ï¼‰ä¸­ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†ï¼ˆTVQA+ã€MMSIã€OnlineMMSIï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ã€‚åœ¨å››ä¸ªç¤¾äº¤ä»»åŠ¡ä¸­ï¼Œç»“æžœè¡¨æ˜Žæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†MLLMçš„èƒ½åŠ›ï¼Œå¹¶å®žçŽ°äº†æœ€å…ˆè¿›çš„ç»“æžœã€‚æ³¨æ„åŠ›å¯è§†åŒ–è¯å®žäº†æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†æ¨¡åž‹é›†ä¸­åœ¨ä¸Žè¯´è¯äººç›¸å…³çš„åŒºåŸŸï¼Œä»Žè€Œå®žçŽ°äº†æ›´å¼ºå¤§çš„å¤šæ–¹ç¤¾äº¤æŽ¨ç†ã€‚æˆ‘ä»¬çš„å®žçŽ°å’Œæ¨¡åž‹å°†åœ¨https://github.com/ut-vision/SocialInteractionä¸Šæä¾›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰åœ¨ç†è§£å¤šè¯´è¯äººç¤¾äº¤äº’åŠ¨åœºæ™¯æ—¶ï¼Œç”±äºŽè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ç¼ºä¹è¯´è¯äººä¸€è‡´çš„å¯¹é½è€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚çŽ°æœ‰æ–¹æ³•ç®€å•åœ°å°†è§†è§‰ä¿¡æ¯åŠ å…¥MLLMï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰è¯´è¯äººä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´æ¨¡åž‹åœ¨ç¤¾äº¤æŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢žå¼ºMLLMä¸­è§†è§‰å’Œæ–‡æœ¬tokenä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§å¯¹é½æ¥æå‡æ¨¡åž‹æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©è´Ÿè´£è·¨æ¨¡æ€æŽ¥åœ°çš„æ³¨æ„åŠ›å¤´ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”çš„ç¤¾äº¤æ„ŸçŸ¥æ³¨æ„åŠ›åå·®ï¼Œä»Žè€Œå¼•å¯¼æ¨¡åž‹å…³æ³¨ä¸Žè¯´è¯äººç›¸å…³çš„è§†è§‰åŒºåŸŸï¼Œå¹¶å°†å…¶ä¸Žå¯¹åº”çš„æ–‡æœ¬ä¿¡æ¯å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šåŠ¨æ€è·¨æ¨¡æ€å¤´é€‰æ‹©å’Œè‡ªé€‚åº”ç¤¾äº¤æ„ŸçŸ¥æ³¨æ„åŠ›åå·®ã€‚é¦–å…ˆï¼ŒåŠ¨æ€è·¨æ¨¡æ€å¤´é€‰æ‹©æ¨¡å—ç”¨äºŽè¯†åˆ«å¯¹è·¨æ¨¡æ€ä¿¡æ¯èžåˆè´¡çŒ®æœ€å¤§çš„æ³¨æ„åŠ›å¤´ã€‚ç„¶åŽï¼Œè‡ªé€‚åº”ç¤¾äº¤æ„ŸçŸ¥æ³¨æ„åŠ›åå·®æ¨¡å—åˆ©ç”¨è¯´è¯äººçš„ä½ç½®ä¿¡æ¯å’ŒçŽ°æœ‰çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè®¡ç®—å‡ºä¸€ä¸ªæ³¨æ„åŠ›åå·®ï¼Œå¹¶å°†å…¶æ³¨å…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä»Žè€Œå¢žå¼ºè¯´è¯äººè§†è§‰è¡¨ç¤ºå’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´çš„å¯¹é½ã€‚æ•´ä¸ªæ¡†æž¶å¯ä»¥æ— ç¼é›†æˆåˆ°çŽ°æœ‰çš„MLLMä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¨¡åž‹æž¶æž„æˆ–å¼•å…¥é¢å¤–çš„å¯è®­ç»ƒå‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒå‚æ•°çš„æ³¨æ„åŠ›å¯¹é½æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¢žå¼ºå¤šè¯´è¯äººåœºæ™¯ä¸‹è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ è½»é‡çº§ï¼Œæ˜“äºŽé›†æˆåˆ°ä¸åŒçš„MLLMä¸­ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡åž‹åœ¨ç¤¾äº¤æŽ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€è·¨æ¨¡æ€å¤´é€‰æ‹©æ¨¡å—é€šè¿‡è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹è·¨æ¨¡æ€ä¿¡æ¯èžåˆçš„è´¡çŒ®åº¦æ¥é€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›å¤´ã€‚è‡ªé€‚åº”ç¤¾äº¤æ„ŸçŸ¥æ³¨æ„åŠ›åå·®æ¨¡å—åˆ©ç”¨è¯´è¯äººçš„ä½ç½®ä¿¡æ¯å’ŒçŽ°æœ‰çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè®¡ç®—å‡ºä¸€ä¸ªæ³¨æ„åŠ›åå·®ï¼Œè¯¥åå·®èƒ½å¤Ÿå¼•å¯¼æ¨¡åž‹å…³æ³¨ä¸Žè¯´è¯äººç›¸å…³çš„è§†è§‰åŒºåŸŸï¼Œå¹¶å°†å…¶ä¸Žå¯¹åº”çš„æ–‡æœ¬ä¿¡æ¯å¯¹é½ã€‚æ³¨æ„åŠ›åå·®çš„å…·ä½“è®¡ç®—æ–¹å¼å¯èƒ½æ¶‰åŠåˆ°è·ç¦»è¡°å‡ã€æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ–ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨TVQA+ã€MMSIå’ŒOnlineMMSIä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨TVQA+æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºŽåŸºçº¿æ¨¡åž‹å–å¾—äº†X%çš„æ€§èƒ½æå‡ï¼ˆå…·ä½“æ•°æ®è¯·å‚è€ƒåŽŸè®ºæ–‡ï¼‰ã€‚æ³¨æ„åŠ›å¯è§†åŒ–ç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼æ¨¡åž‹å…³æ³¨ä¸Žè¯´è¯äººç›¸å…³çš„è§†è§‰åŒºåŸŸï¼Œä»Žè€Œæå‡æ¨¡åž‹åœ¨ç¤¾äº¤æŽ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæ™ºèƒ½ç›‘æŽ§ã€äººæœºäº¤äº’ã€ç¤¾äº¤åª’ä½“åˆ†æžç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æŽ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ç†è§£ç›‘æŽ§è§†é¢‘ä¸­äººç¾¤çš„äº’åŠ¨è¡Œä¸ºï¼Œä»Žè€Œå®žçŽ°æ›´æ™ºèƒ½çš„å®‰å…¨é¢„è­¦ã€‚åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£äººç±»çš„ç¤¾äº¤æ„å›¾ï¼Œä»Žè€Œå®žçŽ°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚åœ¨ç¤¾äº¤åª’ä½“åˆ†æžä¸­ï¼Œå¯ä»¥ç”¨äºŽåˆ†æžç¤¾äº¤åª’ä½“è§†é¢‘ä¸­ç”¨æˆ·çš„äº’åŠ¨è¡Œä¸ºï¼Œä»Žè€ŒæŒ–æŽ˜æ›´æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.

