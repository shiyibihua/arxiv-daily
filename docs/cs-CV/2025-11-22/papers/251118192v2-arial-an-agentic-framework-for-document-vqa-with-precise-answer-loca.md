---
layout: default
title: ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization
---

# ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization

**arXiv**: [2511.18192v2](https://arxiv.org/abs/2511.18192) | [PDF](https://arxiv.org/pdf/2511.18192.pdf)

**ä½œè€…**: Ahmad Mohammadshirazi, Pinaki Prasad Guha Neogi, Dheeraj Kulshrestha, Rajiv Ramnath

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22 (æ›´æ–°: 2025-11-28)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARIALæ¡†æž¶ï¼Œé€šè¿‡Agenticæ–¹å¼å®žçŽ°æ–‡æ¡£VQAçš„ç²¾ç¡®ç­”æ¡ˆå®šä½ä¸ŽæŠ½å–ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ–‡æ¡£è§†è§‰é—®ç­”` `Agenticæ¡†æž¶` `ç­”æ¡ˆå®šä½` `LLMè§„åˆ’ä»£ç†` `æ¨¡å—åŒ–è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–‡æ¡£VQAç³»ç»Ÿåœ¨æ–‡æœ¬å‡†ç¡®çŽ‡å’Œç©ºé—´å®šä½ç²¾åº¦ä¹‹é—´å­˜åœ¨trade-offï¼Œéš¾ä»¥åŒæ—¶ä¿è¯ä¸¤è€…ã€‚
2. ARIALæ¡†æž¶åˆ©ç”¨LLMä½œä¸ºè§„åˆ’ä»£ç†ï¼Œåè°ƒOCRã€æ£€ç´¢ã€ç­”æ¡ˆç”Ÿæˆå’Œå®šä½ç­‰æ¨¡å—ï¼Œå®žçŽ°ç²¾ç¡®ç­”æ¡ˆæŠ½å–å’Œå®šä½ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒARIALåœ¨DocVQAç­‰æ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰SOTAæ–¹æ³•ï¼Œåœ¨æ–‡æœ¬å‡†ç¡®çŽ‡å’Œç©ºé—´ç²¾åº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£è§†è§‰é—®ç­”(VQA)ä¸ä»…è¦æ±‚æ¨¡åž‹æå–å‡†ç¡®çš„æ–‡æœ¬ç­”æ¡ˆï¼Œè¿˜éœ€è¦åœ¨æ–‡æ¡£å›¾åƒä¸­ç²¾ç¡®å®šä½ç­”æ¡ˆï¼Œè¿™å¯¹äºŽé«˜é£Žé™©åº”ç”¨ä¸­çš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰ç³»ç»Ÿåœ¨å®žçŽ°å¼ºå¤§çš„æ–‡æœ¬å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œäº§ç”Ÿäº†ä¸å¯é çš„ç©ºé—´å®šä½ï¼Œæˆ–è€…ç‰ºç‰²æ€§èƒ½æ¥æ¢å–å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ARIALï¼ˆAgentic Reasoning for Interpretable Answer Localizationï¼‰ï¼Œä¸€ä¸ªæ¨¡å—åŒ–æ¡†æž¶ï¼Œé€šè¿‡åŸºäºŽLLMçš„è§„åˆ’ä»£ç†æ¥åè°ƒä¸“é—¨çš„å·¥å…·ï¼Œä»¥å®žçŽ°ç²¾ç¡®çš„ç­”æ¡ˆæå–å’Œå¯é çš„ç©ºé—´å®šä½ã€‚ARIALå°†æ–‡æ¡£VQAåˆ†è§£ä¸ºç»“æž„åŒ–çš„å­ä»»åŠ¡ï¼šä½¿ç”¨TrOCRè¿›è¡ŒåŸºäºŽOCRçš„æ–‡æœ¬æå–ï¼Œä½¿ç”¨è¯­ä¹‰æœç´¢è¿›è¡Œæ£€ç´¢å¢žå¼ºçš„ä¸Šä¸‹æ–‡é€‰æ‹©ï¼Œé€šè¿‡å¾®è°ƒçš„Gemma 3-27Bæ¨¡åž‹ç”Ÿæˆç­”æ¡ˆï¼Œä»¥åŠé€šè¿‡æ–‡æœ¬åˆ°åŒºåŸŸå¯¹é½è¿›è¡Œæ˜¾å¼çš„è¾¹ç•Œæ¡†å®šä½ã€‚è¿™ç§æ¨¡å—åŒ–æž¶æž„äº§ç”Ÿäº†é€æ˜Žçš„æŽ¨ç†è½¨è¿¹ï¼Œä»Žè€Œå®žçŽ°äº†å·¥å…·çº§åˆ«çš„å¯å®¡è®¡æ€§å’Œç‹¬ç«‹çš„ç»„ä»¶ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆDocVQAã€FUNSDã€CORDå’ŒSROIEï¼‰ä¸Šè¯„ä¼°äº†ARIALï¼Œä½¿ç”¨äº†æ–‡æœ¬å‡†ç¡®æ€§ï¼ˆANLSï¼‰å’Œç©ºé—´ç²¾åº¦ï¼ˆIoU 0.50åˆ°0.95çš„mAPï¼‰ã€‚ARIALåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æžœï¼šåœ¨DocVQAä¸Šè¾¾åˆ°88.7 ANLSå’Œ50.1 mAPï¼Œåœ¨FUNSDä¸Šè¾¾åˆ°90.0 ANLSå’Œ50.3 mAPï¼Œåœ¨CORDä¸Šè¾¾åˆ°85.5 ANLSå’Œ60.2 mAPï¼Œåœ¨SROIEä¸Šè¾¾åˆ°93.1 ANLSï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å¥½çš„æ–¹æ³•ï¼ˆDLaVAï¼‰ï¼Œåœ¨DocVQAä¸Šæå‡äº†+2.8 ANLSå’Œ+3.9 mAPã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜Žï¼Œä¸“é—¨å·¥å…·çš„Agenticç¼–æŽ’å¯ä»¥åŒæ—¶æé«˜æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºå¯ä¿¡èµ–ã€å¯è§£é‡Šçš„æ–‡æ¡£AIç³»ç»Ÿæä¾›äº†ä¸€æ¡é€”å¾„ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocument VQAï¼‰ä»»åŠ¡æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æ¡£å›¾åƒå’Œé—®é¢˜ï¼Œæå–å‡†ç¡®çš„æ–‡æœ¬ç­”æ¡ˆï¼Œå¹¶ç²¾ç¡®å®šä½ç­”æ¡ˆåœ¨å›¾åƒä¸­çš„ä½ç½®ã€‚çŽ°æœ‰æ–¹æ³•è¦ä¹ˆä¾§é‡äºŽæé«˜æ–‡æœ¬ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè€Œå¿½ç•¥äº†ç©ºé—´å®šä½çš„å¯é æ€§ï¼›è¦ä¹ˆä¸ºäº†æé«˜å¯è§£é‡Šæ€§è€Œç‰ºç‰²äº†æ•´ä½“æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•åŒæ—¶å®žçŽ°é«˜å‡†ç¡®çŽ‡çš„ç­”æ¡ˆæå–å’Œé«˜ç²¾åº¦çš„ç©ºé—´å®šä½æ˜¯å½“å‰æ–‡æ¡£VQAé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šARIALçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ–‡æ¡£VQAä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç»“æž„åŒ–çš„å­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ä½œä¸ºè§„åˆ’ä»£ç†ï¼Œåè°ƒä¸åŒçš„ä¸“ä¸šå·¥å…·æ¥å®Œæˆè¿™äº›å­ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§æ¨¡å—åŒ–çš„Agenticæ–¹å¼ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨æ¯ä¸ªå·¥å…·çš„ä¼˜åŠ¿ï¼Œå¹¶å®žçŽ°é€æ˜Žçš„æŽ¨ç†è¿‡ç¨‹ï¼Œä»Žè€Œæé«˜æ•´ä½“æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šARIALæ¡†æž¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **OCRæ–‡æœ¬æå–**ï¼šä½¿ç”¨TrOCRä»Žæ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬ä¿¡æ¯ã€‚2) **æ£€ç´¢å¢žå¼ºçš„ä¸Šä¸‹æ–‡é€‰æ‹©**ï¼šåˆ©ç”¨è¯­ä¹‰æœç´¢æŠ€æœ¯ï¼Œä»Žæå–çš„æ–‡æœ¬ä¸­é€‰æ‹©ä¸Žé—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚3) **ç­”æ¡ˆç”Ÿæˆ**ï¼šä½¿ç”¨å¾®è°ƒçš„Gemma 3-27Bæ¨¡åž‹ï¼Œæ ¹æ®é—®é¢˜å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆã€‚4) **è¾¹ç•Œæ¡†å®šä½**ï¼šé€šè¿‡æ–‡æœ¬åˆ°åŒºåŸŸçš„å¯¹é½ï¼Œå°†ç”Ÿæˆçš„ç­”æ¡ˆå®šä½åˆ°æ–‡æ¡£å›¾åƒä¸­çš„å…·ä½“ä½ç½®ã€‚LLMä½œä¸ºè§„åˆ’ä»£ç†ï¼Œè´Ÿè´£åè°ƒè¿™äº›æ¨¡å—çš„æ‰§è¡Œé¡ºåºå’Œå‚æ•°è®¾ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šARIALçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶Agenticæ¡†æž¶çš„è®¾è®¡ï¼Œå®ƒå°†æ–‡æ¡£VQAä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨LLMä½œä¸ºè§„åˆ’ä»£ç†æ¥åè°ƒè¿™äº›å­ä»»åŠ¡çš„æ‰§è¡Œã€‚è¿™ç§æ¨¡å—åŒ–çš„è®¾è®¡ä¸ä»…æé«˜äº†æ•´ä½“æ€§èƒ½ï¼Œè¿˜å¢žå¼ºäº†æ¨¡åž‹çš„å¯è§£é‡Šæ€§å’Œå¯å®¡è®¡æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒARIALèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å„ç§ä¸“ä¸šå·¥å…·çš„ä¼˜åŠ¿ï¼Œå¹¶å®žçŽ°æ›´ç²¾ç¡®çš„ç­”æ¡ˆå®šä½ã€‚

**å…³é”®è®¾è®¡**ï¼šARIALæ¡†æž¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨TrOCRè¿›è¡ŒOCRæ–‡æœ¬æå–ï¼Œç¡®ä¿æ–‡æœ¬ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚2) åˆ©ç”¨è¯­ä¹‰æœç´¢æŠ€æœ¯è¿›è¡Œä¸Šä¸‹æ–‡é€‰æ‹©ï¼Œæé«˜ç­”æ¡ˆç”Ÿæˆçš„è´¨é‡ã€‚3) ä½¿ç”¨å¾®è°ƒçš„Gemma 3-27Bæ¨¡åž‹ç”Ÿæˆç­”æ¡ˆï¼Œå……åˆ†åˆ©ç”¨LLMçš„å¼ºå¤§èƒ½åŠ›ã€‚4) é€šè¿‡æ–‡æœ¬åˆ°åŒºåŸŸçš„å¯¹é½è¿›è¡Œè¾¹ç•Œæ¡†å®šä½ï¼Œå®žçŽ°ç²¾ç¡®çš„ç©ºé—´å®šä½ã€‚æ­¤å¤–ï¼ŒLLMè§„åˆ’ä»£ç†çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°åè°ƒå„ä¸ªæ¨¡å—çš„æ‰§è¡Œï¼Œå¹¶æ ¹æ®ä»»åŠ¡éœ€æ±‚è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ARIALåœ¨DocVQAã€FUNSDã€CORDå’ŒSROIEå››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†SOTAç»“æžœã€‚åœ¨DocVQAæ•°æ®é›†ä¸Šï¼ŒARIALçš„ANLSæŒ‡æ ‡è¾¾åˆ°88.7ï¼ŒmAPæŒ‡æ ‡è¾¾åˆ°50.1ï¼Œç›¸æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•DLaVAï¼Œåˆ†åˆ«æå‡äº†+2.8 ANLSå’Œ+3.9 mAPã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒARIALæ¡†æž¶åœ¨æ–‡æœ¬å‡†ç¡®çŽ‡å’Œç©ºé—´ç²¾åº¦ä¸Šå‡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ARIALæ¡†æž¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨é‡‘èžã€æ³•å¾‹ã€åŒ»ç–—ç­‰é¢†åŸŸï¼Œå¯ä»¥ç”¨äºŽè‡ªåŠ¨å¤„ç†æ–‡æ¡£ã€æå–å…³é”®ä¿¡æ¯ã€å›žç­”ç”¨æˆ·æé—®ç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºŽæå‡æ–‡æ¡£AIç³»ç»Ÿçš„å¯ä¿¡åº¦å’Œå¯è§£é‡Šæ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºŽé«˜é£Žé™©åº”ç”¨åœºæ™¯ï¼Œå¹¶ä¸ºæœªæ¥çš„æ–‡æ¡£æ™ºèƒ½ç ”ç©¶æä¾›æ–°çš„æ€è·¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.

