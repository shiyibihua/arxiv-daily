---
layout: default
title: ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models
---

# ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models

**arXiv**: [2511.18082v1](https://arxiv.org/abs/2511.18082) | [PDF](https://arxiv.org/pdf/2511.18082.pdf)

**ä½œè€…**: Wencheng Ye, Tianshi Wang, Lei Zhu, Fengling Li, Guoli Yang

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ActDistillï¼šé¢å‘é«˜æ•ˆVLAæ¨¡åž‹çš„åŠ¨ä½œå¼•å¯¼è‡ªè’¸é¦æ¡†æž¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æ¨¡åž‹è’¸é¦` `çŸ¥è¯†è¿ç§»` `åŠ¨ä½œå¼•å¯¼` `åŠ¨æ€è·¯ç”±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„VLAæ¨¡åž‹è®¡ç®—å¼€é”€å¤§ã€æŽ¨ç†å»¶è¿Ÿé«˜ï¼Œéš¾ä»¥éƒ¨ç½²äºŽæœºå™¨äººæ“ä½œç­‰å®žé™…åœºæ™¯ã€‚
2. ActDistillåˆ©ç”¨åŠ¨ä½œå…ˆéªŒå¼•å¯¼çŸ¥è¯†è¿ç§»å’Œæ¨¡åž‹åŽ‹ç¼©ï¼Œå°†å¤§åž‹VLAæ¨¡åž‹çš„åŠ¨ä½œé¢„æµ‹èƒ½åŠ›è¿ç§»åˆ°è½»é‡çº§æ¨¡åž‹ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒActDistillåœ¨æ˜¾è‘—é™ä½Žè®¡ç®—é‡å’ŒæŽ¨ç†å»¶è¿Ÿçš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡äº†VLAæ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºActDistillï¼Œä¸€ç§é€šç”¨çš„åŠ¨ä½œå¼•å¯¼è‡ªè’¸é¦æ¡†æž¶ï¼Œæ—¨åœ¨å°†çŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹çš„åŠ¨ä½œé¢„æµ‹èƒ½åŠ›è¿ç§»åˆ°è½»é‡çº§æ¨¡åž‹ï¼Œä»Žè€Œé™ä½Žè®¡ç®—å¼€é”€å’ŒæŽ¨ç†å»¶è¿Ÿã€‚ä¸Žä»¥å¾€ä¾§é‡è§†è§‰-è¯­è¨€ç›¸å…³æ€§çš„æ•ˆçŽ‡ç­–ç•¥ä¸åŒï¼ŒActDistillåˆ©ç”¨åŠ¨ä½œå…ˆéªŒæ¥æŒ‡å¯¼çŸ¥è¯†è¿ç§»å’Œæ¨¡åž‹åŽ‹ç¼©ï¼Œå®žçŽ°VLAæ¨¡åž‹é¢å‘åŠ¨ä½œçš„æ•ˆçŽ‡æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è®­ç»ƒå¥½çš„VLAæ¨¡åž‹ä½œä¸ºæ•™å¸ˆï¼Œå¹¶å¼•å…¥å›¾ç»“æž„å°è£…ç­–ç•¥æ¥æ˜¾å¼å»ºæ¨¡åŠ¨ä½œé¢„æµ‹çš„å±‚çº§æ¼”åŒ–ã€‚ä»Žå›¾å°è£…çš„æ•™å¸ˆæ¨¡åž‹æ´¾ç”Ÿå‡ºçš„å­¦ç”Ÿæ¨¡åž‹ï¼Œé…å¤‡äº†åŠ¨æ€è·¯ç”±ï¼Œå¯ä»¥æ ¹æ®åŠ¨ä½œé¢„æµ‹éœ€æ±‚è‡ªé€‚åº”åœ°é€‰æ‹©è®¡ç®—è·¯å¾„ï¼Œå¹¶åœ¨å±‚çº§å›¾ä¿¡æ¯çš„ç›‘ç£ä¸‹å¹³æ»‘é«˜æ•ˆåœ°æ¼”åŒ–ã€‚åœ¨æŽ¨ç†é˜¶æ®µï¼Œç§»é™¤å›¾ç›¸å…³çš„è¾…åŠ©ç»„ä»¶ï¼Œå­¦ç”Ÿæ¨¡åž‹ä»…æ‰§è¡ŒåŠ¨æ€è·¯ç”±çš„å±‚ï¼Œä»¥æœ€å°çš„è®¡ç®—å’Œå»¶è¿Ÿé¢„æµ‹é«˜ç²¾åº¦åŠ¨ä½œã€‚åœ¨å…·èº«æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒActDistillåœ¨è®¡ç®—é‡å‡å°‘50%ä»¥ä¸Šï¼Œé€Ÿåº¦æå‡é«˜è¾¾1.67å€çš„æƒ…å†µä¸‹ï¼Œå®žçŽ°äº†ä¸Žå…¨å°ºå¯¸VLAæ¨¡åž‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ï¼Œä»Žè€Œä¸ºé«˜æ•ˆå…·èº«æ™ºèƒ½å»ºç«‹äº†ä¸€ä¸ªé€šç”¨èŒƒä¾‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹è™½ç„¶å±•çŽ°å‡ºå¼ºå¤§çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å…¶åºžå¤§çš„è®¡ç®—å¼€é”€å’Œé«˜æŽ¨ç†å»¶è¿Ÿé™åˆ¶äº†å®ƒä»¬åœ¨æœºå™¨äººæ“ä½œç­‰é¢†åŸŸçš„å®žé™…åº”ç”¨ã€‚çŽ°æœ‰çš„æ¨¡åž‹åŽ‹ç¼©æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¿½ç•¥äº†åŠ¨ä½œé¢„æµ‹æœ¬èº«çš„é‡è¦æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šActDistillçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨åŠ¨ä½œå…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼æ¨¡åž‹è’¸é¦å’ŒåŽ‹ç¼©ï¼Œä»Žè€Œå®žçŽ°é¢å‘åŠ¨ä½œé¢„æµ‹çš„é«˜æ•ˆVLAæ¨¡åž‹ã€‚é€šè¿‡å°†å¤§åž‹VLAæ¨¡åž‹çš„çŸ¥è¯†è¿ç§»åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡åž‹ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å¼åœ°å»ºæ¨¡åŠ¨ä½œé¢„æµ‹çš„å±‚çº§æ¼”åŒ–è¿‡ç¨‹ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡åž‹èƒ½å¤Ÿä»¥æ›´å°‘çš„è®¡ç®—èµ„æºå®žçŽ°ä¸Žæ•™å¸ˆæ¨¡åž‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šActDistillæ¡†æž¶ä¸»è¦åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ•™å¸ˆæ¨¡åž‹ã€å›¾ç»“æž„å°è£…ç­–ç•¥å’ŒåŠ¨æ€è·¯ç”±å­¦ç”Ÿæ¨¡åž‹ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„VLAæ¨¡åž‹ä½œä¸ºæ•™å¸ˆæ¨¡åž‹ã€‚ç„¶åŽï¼Œå¼•å…¥å›¾ç»“æž„å°è£…ç­–ç•¥æ¥æ˜¾å¼åœ°å»ºæ¨¡æ•™å¸ˆæ¨¡åž‹ä¸­åŠ¨ä½œé¢„æµ‹çš„å±‚çº§æ¼”åŒ–è¿‡ç¨‹ï¼Œå°†æ•™å¸ˆæ¨¡åž‹å°è£…æˆä¸€ä¸ªå›¾ç»“æž„ã€‚æœ€åŽï¼ŒåŸºäºŽè¯¥å›¾ç»“æž„ï¼Œæž„å»ºä¸€ä¸ªåŠ¨æ€è·¯ç”±å­¦ç”Ÿæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹å¯ä»¥æ ¹æ®åŠ¨ä½œé¢„æµ‹çš„éœ€æ±‚è‡ªé€‚åº”åœ°é€‰æ‹©è®¡ç®—è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šActDistillçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶åŠ¨ä½œå¼•å¯¼çš„è‡ªè’¸é¦æ–¹æ³•å’Œå›¾ç»“æž„å°è£…ç­–ç•¥ã€‚ä¼ ç»Ÿçš„æ¨¡åž‹è’¸é¦æ–¹æ³•é€šå¸¸åªå…³æ³¨è¾“å…¥-è¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œè€ŒActDistillåˆ™æ˜¾å¼åœ°åˆ©ç”¨äº†åŠ¨ä½œå…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼çŸ¥è¯†è¿ç§»ã€‚å›¾ç»“æž„å°è£…ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡åŠ¨ä½œé¢„æµ‹çš„å±‚çº§æ¼”åŒ–è¿‡ç¨‹ï¼Œä»Žè€Œä½¿å¾—å­¦ç”Ÿæ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ æ•™å¸ˆæ¨¡åž‹çš„çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šå›¾ç»“æž„å°è£…ç­–ç•¥å°†æ•™å¸ˆæ¨¡åž‹çš„æ¯ä¸€å±‚æˆ–å‡ å±‚ç»„åˆæˆå›¾ä¸­çš„èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥è¡¨ç¤ºä¿¡æ¯ä¼ é€’å…³ç³»ã€‚åŠ¨æ€è·¯ç”±å­¦ç”Ÿæ¨¡åž‹ä½¿ç”¨ä¸€ä¸ªåŠ¨æ€è·¯ç”±å™¨æ¥å†³å®šæ¯ä¸€å±‚åº”è¯¥é€‰æ‹©å“ªæ¡è®¡ç®—è·¯å¾„ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ¨¡ä»¿æŸå¤±ï¼ˆæ¨¡ä»¿æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºï¼‰å’Œå›¾ä¿¡æ¯ç›‘ç£æŸå¤±ï¼ˆé¼“åŠ±å­¦ç”Ÿæ¨¡åž‹å­¦ä¹ å›¾ç»“æž„ä¸­çš„å±‚çº§å…³ç³»ï¼‰ã€‚å…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚å­¦ä¹ çŽ‡ã€batch sizeç­‰ï¼‰å’Œç½‘ç»œç»“æž„ç»†èŠ‚ï¼ˆå¦‚å›¾çš„æž„å»ºæ–¹å¼ã€åŠ¨æ€è·¯ç”±å™¨çš„è®¾è®¡ç­‰ï¼‰æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ActDistillåœ¨å…·èº«æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œåœ¨è®¡ç®—é‡å‡å°‘50%ä»¥ä¸Šçš„æƒ…å†µä¸‹ï¼Œå®žçŽ°äº†ä¸Žå…¨å°ºå¯¸VLAæ¨¡åž‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒæŽ¨ç†é€Ÿåº¦æå‡é«˜è¾¾1.67å€ï¼Œè¯æ˜Žäº†è¯¥æ–¹æ³•åœ¨æé«˜VLAæ¨¡åž‹æ•ˆçŽ‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æžœè¡¨æ˜ŽActDistillä¸ºé«˜æ•ˆå…·èº«æ™ºèƒ½æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ActDistillå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½ŽVLAæ¨¡åž‹çš„è®¡ç®—å¼€é”€å’ŒæŽ¨ç†å»¶è¿Ÿï¼Œå¯ä»¥ä½¿å¾—è¿™äº›æ¨¡åž‹èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°éƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œä»Žè€Œå®žçŽ°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽå…¶ä»–éœ€è¦é«˜æ•ˆå¤šæ¨¡æ€ç†è§£å’Œå†³ç­–çš„ä»»åŠ¡ä¸­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.

