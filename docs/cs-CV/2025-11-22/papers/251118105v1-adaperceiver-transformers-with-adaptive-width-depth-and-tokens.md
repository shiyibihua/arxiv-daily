---
layout: default
title: AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens
---

# AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.18105" target="_blank" class="toolbar-btn">arXiv: 2511.18105v1</a>
    <a href="https://arxiv.org/pdf/2511.18105.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18105v1" 
            onclick="toggleFavorite(this, '2511.18105v1', 'AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Purvish Jajal, Nick John Eliopoulos, Benjamin Shiue-Hal Chou, George K. Thiruvathukal, Yung-Hsiang Lu, James C. Davis

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-22

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**AdaPerceiverÔºöÊèêÂá∫È¶ñ‰∏™Âú®Ê∑±Â∫¶„ÄÅÂÆΩÂ∫¶Âíåtokens‰∏äËá™ÈÄÇÂ∫îÁöÑTransformerÊû∂ÊûÑ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Ëá™ÈÄÇÂ∫îTransformer` `Âä®ÊÄÅËÆ°ÁÆó` `Ê∑±Â∫¶Ëá™ÈÄÇÂ∫î` `ÂÆΩÂ∫¶Ëá™ÈÄÇÂ∫î` `TokenÈÄâÊã©` `ÂõæÂÉèÂàÜÁ±ª` `ËØ≠‰πâÂàÜÂâ≤`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâTransformerÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ËÆ°ÁÆóËµÑÊ∫êÂàÜÈÖçÂõ∫ÂÆöÔºåÈöæ‰ª•ÈÄÇÂ∫î‰∏çÂêåÁ°¨‰ª∂ÂíåÂª∂ËøüÁ∫¶Êùü„ÄÇ
2. AdaPerceiverÊèêÂá∫‰∏ÄÁßçÊñ∞ÁöÑTransformerÊû∂ÊûÑÔºåÂèØÂú®Ê∑±Â∫¶„ÄÅÂÆΩÂ∫¶Âíåtokens‰∏â‰∏™Áª¥Â∫¶‰∏äËøõË°åËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåAdaPerceiverÂú®ÂõæÂÉèÂàÜÁ±ª„ÄÅËØ≠‰πâÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞‰ª£TransformerÊû∂ÊûÑÂú®ÂêÑÁßç‰ªªÂä°ÂíåÈ¢ÜÂüü‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Êé®ÁêÜÊó∂Â¶Ç‰ΩïÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫êÊñπÈù¢‰ªçÁÑ∂ÂÉµÂåñ„ÄÇÂÆûÈôÖÈÉ®ÁΩ≤ÈÄöÂ∏∏ÈúÄË¶ÅÊ®°ÂûãÈÄÇÂ∫î‰∏çÂêåÁöÑÁ°¨‰ª∂ÂíåÂª∂ËøüÁ∫¶ÊùüÔºå‰ΩÜÂ§ßÂ§öÊï∞Âä®ÊÄÅËÆ°ÁÆóÊñπÊ≥ïÈÉΩÈõÜ‰∏≠Âú®Âçï‰∏™ËΩ¥‰∏äÔºå‰æãÂ¶ÇÂáèÂ∞ëtokensÁöÑÊï∞Èáè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÉΩÂäõÔºöAdaPerceiverÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Âú®Âçï‰∏™Ê®°Âûã‰∏≠Áªü‰∏ÄÂÆûÁé∞Ê∑±Â∫¶„ÄÅÂÆΩÂ∫¶ÂíåtokensËá™ÈÄÇÂ∫îÁöÑTransformerÊû∂ÊûÑ„ÄÇËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÊîØÊåÅÊ≤øËøô‰∫õËΩ¥ËøõË°åËá™ÈÄÇÂ∫îÁöÑÊû∂ÊûÑÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÊúâÊïàÁöÑËÅîÂêàËÆ≠ÁªÉÊñπÊ°àÁõ∏ÁªìÂêàÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®ÂÖ∂ÂêÑÁßçÈÖçÁΩÆ‰∏≠‰øùÊåÅÊÄßËÉΩ„ÄÇÂú®ÂõæÂÉèÂàÜÁ±ª„ÄÅËØ≠‰πâÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°‰ªªÂä°‰∏äËØÑ‰º∞‰∫ÜAdaPerceiver„ÄÇÂú®ÂõæÂÉèÂàÜÁ±ª‰∏äÔºåAdaPerceiverÊâ©Â±ï‰∫ÜÁ≤æÂ∫¶-ÂêûÂêêÈáèParetoÂâçÊ≤øÔºåÂÆûÁé∞‰∫Ü85.4%ÁöÑÂáÜÁ°ÆÁéáÔºåÂêåÊó∂ÊØîFlexiViT-L‰∫ßÁîüÈ´ò36%ÁöÑÂêûÂêêÈáè„ÄÇÂú®ÂØÜÈõÜÈ¢ÑÊµãÊñπÈù¢ÔºåAdaPerceiverÂú®ËØ≠‰πâÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°‰∏ä‰∏éViT-H/14Áõ∏ÂåπÈÖçÔºåÂêåÊó∂ÂÖ∑ÊúâÁ∫¶26ÂÄçÊõ¥Â∞ëÁöÑÁºñÁ†ÅÂô®FLOPsÔºàÊµÆÁÇπËøêÁÆóÔºâ„ÄÇÊúÄÂêéÔºåËÆ∫ÊñáÂ±ïÁ§∫‰∫ÜÈÖçÂ§áÁ≠ñÁï•ÁöÑAdaPerceiverÂ¶Ç‰ΩïÂú®‰øùÊåÅImageNet1KÂáÜÁ°ÆÁéáÔºà¬±0.1‰∏™ÁôæÂàÜÁÇπÔºâÁöÑÂêåÊó∂ÔºåÂ∞ÜFLOPsÈôç‰Ωé24-33%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâTransformerÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ËÆ°ÁÆóËµÑÊ∫êÂàÜÈÖçÁ≠ñÁï•ÊòØÈùôÊÄÅÁöÑÔºåÊó†Ê≥ïÊ†πÊçÆÂÆûÈôÖÁöÑÁ°¨‰ª∂ÁéØÂ¢ÉÂíåÂª∂ËøüË¶ÅÊ±ÇËøõË°åÂä®ÊÄÅË∞ÉÊï¥„ÄÇËøôÂØºËá¥Ê®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØ‰∏ãÈöæ‰ª•ËææÂà∞ÊúÄ‰Ω≥ÊÄßËÉΩÔºåÊàñËÄÖÂú®ËµÑÊ∫êÂÖÖË∂≥ÁöÑÂú∫ÊôØ‰∏ãÈÄ†ÊàêËÆ°ÁÆóËµÑÊ∫êÁöÑÊµ™Ë¥π„ÄÇÁé∞ÊúâÁöÑÂä®ÊÄÅËÆ°ÁÆóÊñπÊ≥ïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®Âçï‰∏™Áª¥Â∫¶ÁöÑËá™ÈÄÇÂ∫îÔºå‰æãÂ¶ÇÂáèÂ∞ëtokenÊï∞ÈáèÔºåËÄåÂøΩÁï•‰∫ÜÊ®°ÂûãÊ∑±Â∫¶ÂíåÂÆΩÂ∫¶ÁöÑÈáçË¶ÅÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAdaPerceiverÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÆæËÆ°‰∏ÄÁßçÂèØ‰ª•Âú®Ê∑±Â∫¶„ÄÅÂÆΩÂ∫¶Âíåtokens‰∏â‰∏™Áª¥Â∫¶‰∏äËøõË°åËá™ÈÄÇÂ∫îË∞ÉÊï¥ÁöÑTransformerÊû∂ÊûÑ„ÄÇÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠¶‰π†‰∏çÂêåÈÖçÁΩÆ‰∏ãÁöÑÊ®°ÂûãÂèÇÊï∞ÔºåAdaPerceiverÂèØ‰ª•Âú®Êé®ÁêÜÊó∂Ê†πÊçÆÂÆûÈôÖÈúÄÊ±ÇÈÄâÊã©ÂêàÈÄÇÁöÑÈÖçÁΩÆÔºå‰ªéËÄåÂú®Á≤æÂ∫¶ÂíåÊïàÁéá‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇËøôÁßçÁªü‰∏ÄÁöÑËá™ÈÄÇÂ∫îËÉΩÂäõ‰ΩøÂæóAdaPerceiverËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑÁ°¨‰ª∂ÁéØÂ¢ÉÂíåÂª∂ËøüÁ∫¶Êùü„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAdaPerceiverÁöÑÊï¥‰ΩìÊû∂ÊûÑÂü∫‰∫éTransformerÔºå‰ΩÜÂºïÂÖ•‰∫ÜËá™ÈÄÇÂ∫îÊ®°ÂùóÊù•ÂÆûÁé∞Ê∑±Â∫¶„ÄÅÂÆΩÂ∫¶ÂíåtokensÁöÑÂä®ÊÄÅË∞ÉÊï¥„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊ®°ÂûãÂåÖÂê´Â§ö‰∏™TransformerÂ±ÇÔºåÊØè‰∏ÄÂ±ÇÈÉΩÂèØ‰ª•ÈÄâÊã©ÊòØÂê¶Ë¢´ÊøÄÊ¥ª„ÄÇÂêåÊó∂ÔºåÊØè‰∏ÄÂ±ÇÁöÑÂÆΩÂ∫¶ÔºàÂç≥ÈöêËóèÂ±ÇÁª¥Â∫¶Ôºâ‰πüÂèØ‰ª•Âä®ÊÄÅË∞ÉÊï¥„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãËøòÈááÁî®‰∫Ü‰∏ÄÁßçtokenÈÄâÊã©Êú∫Âà∂ÔºåÂèØ‰ª•Ê†πÊçÆËæìÂÖ•ÂõæÂÉèÁöÑÂÜÖÂÆπÈÄâÊã©‰øùÁïôÂì™‰∫õtokens„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÈÄöËøáËÅîÂêàËÆ≠ÁªÉÁöÑÊñπÂºèÂ≠¶‰π†‰∏çÂêåÈÖçÁΩÆ‰∏ãÁöÑÂèÇÊï∞ÔºåÂπ∂‰ΩøÁî®‰∏ÄÁßçÁâπÊÆäÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ÈºìÂä±Ê®°ÂûãÂú®‰∏çÂêåÈÖçÁΩÆ‰∏ã‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAdaPerceiverÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂Áªü‰∏ÄÁöÑËá™ÈÄÇÂ∫îËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåAdaPerceiverÂèØ‰ª•ÂêåÊó∂Âú®Ê∑±Â∫¶„ÄÅÂÆΩÂ∫¶Âíåtokens‰∏â‰∏™Áª¥Â∫¶‰∏äËøõË°åÂä®ÊÄÅË∞ÉÊï¥Ôºå‰ªéËÄåÊõ¥Âä†ÁÅµÊ¥ªÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑÁ°¨‰ª∂ÁéØÂ¢ÉÂíåÂª∂ËøüÁ∫¶Êùü„ÄÇÊ≠§Â§ñÔºåAdaPerceiverËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑËÅîÂêàËÆ≠ÁªÉÊñπÊ°àÔºåÂèØ‰ª•Á°Æ‰øùÊ®°ÂûãÂú®‰∏çÂêåÈÖçÁΩÆ‰∏ã‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAdaPerceiverÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÂèØË∑≥ËøáÁöÑTransformerÂ±ÇÔºåÂÖÅËÆ∏Ê®°ÂûãÂä®ÊÄÅË∞ÉÊï¥Ê∑±Â∫¶Ôºõ2) ÂèØÂèòÂÆΩÂ∫¶ÁöÑÈöêËóèÂ±ÇÔºåÂÖÅËÆ∏Ê®°ÂûãÂä®ÊÄÅË∞ÉÊï¥ÂÆΩÂ∫¶Ôºõ3) Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑtokenÈÄâÊã©Êú∫Âà∂ÔºåÂÖÅËÆ∏Ê®°ÂûãÂä®ÊÄÅË∞ÉÊï¥tokensÊï∞ÈáèÔºõ4) ‰∏ÄÁßçÁâπÊÆäÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÈºìÂä±Ê®°ÂûãÂú®‰∏çÂêåÈÖçÁΩÆ‰∏ã‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AdaPerceiverÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏äÔºåÂÆûÁé∞‰∫Ü85.4%ÁöÑÂáÜÁ°ÆÁéáÔºåÂêåÊó∂ÊØîFlexiViT-LÊèêÈ´ò‰∫Ü36%ÁöÑÂêûÂêêÈáè„ÄÇÂú®ÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°‰∏äÔºåAdaPerceiverÂú®ËØ≠‰πâÂàÜÂâ≤ÂíåÊ∑±Â∫¶‰º∞ËÆ°‰∏ä‰∏éViT-H/14ÁöÑÊÄßËÉΩÁõ∏ÂΩìÔºå‰ΩÜÁºñÁ†ÅÂô®ÁöÑFLOPsÂáèÂ∞ë‰∫ÜÁ∫¶26ÂÄç„ÄÇÊ≠§Â§ñÔºåÈÖçÂ§áÁ≠ñÁï•ÁöÑAdaPerceiverÂèØ‰ª•Âú®‰øùÊåÅImageNet1KÂáÜÁ°ÆÁéá‰∏çÂèòÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞ÜFLOPsÈôç‰Ωé24-33%„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAdaPerceiverÂú®Á≤æÂ∫¶ÂíåÊïàÁéá‰πãÈó¥ÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÂπ≥Ë°°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AdaPerceiverÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§áÂíåÈúÄË¶ÅÂÆûÊó∂ÂìçÂ∫îÁöÑÂú∫ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂÆÉÂèØ‰ª•Â∫îÁî®‰∫éÁßªÂä®Á´ØÁöÑÂõæÂÉèËØÜÂà´„ÄÅËá™Âä®È©æÈ©∂‰∏≠ÁöÑÁõÆÊ†áÊ£ÄÊµã„ÄÅ‰ª•ÂèäËßÜÈ¢ëÁõëÊéß‰∏≠ÁöÑÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµãÁ≠â„ÄÇÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥Ê®°ÂûãÁöÑÊ∑±Â∫¶„ÄÅÂÆΩÂ∫¶ÂíåtokensÊï∞ÈáèÔºåAdaPerceiverÂèØ‰ª•Âú®‰øùËØÅÁ≤æÂ∫¶ÁöÑÂâçÊèê‰∏ãÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÂíåÂª∂ËøüÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

