---
layout: default
title: AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens
---

# AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens

**arXiv**: [2511.18105v1](https://arxiv.org/abs/2511.18105) | [PDF](https://arxiv.org/pdf/2511.18105.pdf)

**ä½œè€…**: Purvish Jajal, Nick John Eliopoulos, Benjamin Shiue-Hal Chou, George K. Thiruvathukal, Yung-Hsiang Lu, James C. Davis

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AdaPerceiverï¼šæå‡ºé¦–ä¸ªåœ¨æ·±åº¦ã€å®½åº¦å’Œtokensä¸Šè‡ªé€‚åº”çš„Transformeræž¶æž„ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è‡ªé€‚åº”Transformer` `åŠ¨æ€è®¡ç®—` `æ·±åº¦è‡ªé€‚åº”` `å®½åº¦è‡ªé€‚åº”` `Tokené€‰æ‹©` `å›¾åƒåˆ†ç±»` `è¯­ä¹‰åˆ†å‰²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰Transformeræ¨¡åž‹åœ¨æŽ¨ç†æ—¶è®¡ç®—èµ„æºåˆ†é…å›ºå®šï¼Œéš¾ä»¥é€‚åº”ä¸åŒç¡¬ä»¶å’Œå»¶è¿Ÿçº¦æŸã€‚
2. AdaPerceiveræå‡ºä¸€ç§æ–°çš„Transformeræž¶æž„ï¼Œå¯åœ¨æ·±åº¦ã€å®½åº¦å’Œtokensä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒAdaPerceiveråœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŽ°ä»£Transformeræž¶æž„åœ¨å„ç§ä»»åŠ¡å’Œé¢†åŸŸä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨æŽ¨ç†æ—¶å¦‚ä½•åˆ†é…è®¡ç®—èµ„æºæ–¹é¢ä»ç„¶åƒµåŒ–ã€‚å®žé™…éƒ¨ç½²é€šå¸¸éœ€è¦æ¨¡åž‹é€‚åº”ä¸åŒçš„ç¡¬ä»¶å’Œå»¶è¿Ÿçº¦æŸï¼Œä½†å¤§å¤šæ•°åŠ¨æ€è®¡ç®—æ–¹æ³•éƒ½é›†ä¸­åœ¨å•ä¸ªè½´ä¸Šï¼Œä¾‹å¦‚å‡å°‘tokensçš„æ•°é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èƒ½åŠ›ï¼šAdaPerceiverï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨å•ä¸ªæ¨¡åž‹ä¸­ç»Ÿä¸€å®žçŽ°æ·±åº¦ã€å®½åº¦å’Œtokensè‡ªé€‚åº”çš„Transformeræž¶æž„ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ”¯æŒæ²¿è¿™äº›è½´è¿›è¡Œè‡ªé€‚åº”çš„æž¶æž„ï¼Œå¹¶å°†å…¶ä¸Žæœ‰æ•ˆçš„è”åˆè®­ç»ƒæ–¹æ¡ˆç›¸ç»“åˆï¼Œä»¥ç¡®ä¿æ¨¡åž‹åœ¨å…¶å„ç§é…ç½®ä¸­ä¿æŒæ€§èƒ½ã€‚åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šè¯„ä¼°äº†AdaPerceiverã€‚åœ¨å›¾åƒåˆ†ç±»ä¸Šï¼ŒAdaPerceiveræ‰©å±•äº†ç²¾åº¦-åžåé‡Paretoå‰æ²¿ï¼Œå®žçŽ°äº†85.4%çš„å‡†ç¡®çŽ‡ï¼ŒåŒæ—¶æ¯”FlexiViT-Läº§ç”Ÿé«˜36%çš„åžåé‡ã€‚åœ¨å¯†é›†é¢„æµ‹æ–¹é¢ï¼ŒAdaPerceiveråœ¨è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä¸Šä¸ŽViT-H/14ç›¸åŒ¹é…ï¼ŒåŒæ—¶å…·æœ‰çº¦26å€æ›´å°‘çš„ç¼–ç å™¨FLOPsï¼ˆæµ®ç‚¹è¿ç®—ï¼‰ã€‚æœ€åŽï¼Œè®ºæ–‡å±•ç¤ºäº†é…å¤‡ç­–ç•¥çš„AdaPerceiverå¦‚ä½•åœ¨ä¿æŒImageNet1Kå‡†ç¡®çŽ‡ï¼ˆÂ±0.1ä¸ªç™¾åˆ†ç‚¹ï¼‰çš„åŒæ—¶ï¼Œå°†FLOPsé™ä½Ž24-33%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰Transformeræ¨¡åž‹åœ¨æŽ¨ç†æ—¶è®¡ç®—èµ„æºåˆ†é…ç­–ç•¥æ˜¯é™æ€çš„ï¼Œæ— æ³•æ ¹æ®å®žé™…çš„ç¡¬ä»¶çŽ¯å¢ƒå’Œå»¶è¿Ÿè¦æ±‚è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚è¿™å¯¼è‡´æ¨¡åž‹åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹éš¾ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæˆ–è€…åœ¨èµ„æºå……è¶³çš„åœºæ™¯ä¸‹é€ æˆè®¡ç®—èµ„æºçš„æµªè´¹ã€‚çŽ°æœ‰çš„åŠ¨æ€è®¡ç®—æ–¹æ³•é€šå¸¸åªå…³æ³¨å•ä¸ªç»´åº¦çš„è‡ªé€‚åº”ï¼Œä¾‹å¦‚å‡å°‘tokenæ•°é‡ï¼Œè€Œå¿½ç•¥äº†æ¨¡åž‹æ·±åº¦å’Œå®½åº¦çš„é‡è¦æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAdaPerceiverçš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§å¯ä»¥åœ¨æ·±åº¦ã€å®½åº¦å’Œtokensä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´çš„Transformeræž¶æž„ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ä¸åŒé…ç½®ä¸‹çš„æ¨¡åž‹å‚æ•°ï¼ŒAdaPerceiverå¯ä»¥åœ¨æŽ¨ç†æ—¶æ ¹æ®å®žé™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„é…ç½®ï¼Œä»Žè€Œåœ¨ç²¾åº¦å’Œæ•ˆçŽ‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¿™ç§ç»Ÿä¸€çš„è‡ªé€‚åº”èƒ½åŠ›ä½¿å¾—AdaPerceiverèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ç¡¬ä»¶çŽ¯å¢ƒå’Œå»¶è¿Ÿçº¦æŸã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šAdaPerceiverçš„æ•´ä½“æž¶æž„åŸºäºŽTransformerï¼Œä½†å¼•å…¥äº†è‡ªé€‚åº”æ¨¡å—æ¥å®žçŽ°æ·±åº¦ã€å®½åº¦å’Œtokensçš„åŠ¨æ€è°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡åž‹åŒ…å«å¤šä¸ªTransformerå±‚ï¼Œæ¯ä¸€å±‚éƒ½å¯ä»¥é€‰æ‹©æ˜¯å¦è¢«æ¿€æ´»ã€‚åŒæ—¶ï¼Œæ¯ä¸€å±‚çš„å®½åº¦ï¼ˆå³éšè—å±‚ç»´åº¦ï¼‰ä¹Ÿå¯ä»¥åŠ¨æ€è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæ¨¡åž‹è¿˜é‡‡ç”¨äº†ä¸€ç§tokené€‰æ‹©æœºåˆ¶ï¼Œå¯ä»¥æ ¹æ®è¾“å…¥å›¾åƒçš„å†…å®¹é€‰æ‹©ä¿ç•™å“ªäº›tokensã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡åž‹é€šè¿‡è”åˆè®­ç»ƒçš„æ–¹å¼å­¦ä¹ ä¸åŒé…ç½®ä¸‹çš„å‚æ•°ï¼Œå¹¶ä½¿ç”¨ä¸€ç§ç‰¹æ®Šçš„æŸå¤±å‡½æ•°æ¥é¼“åŠ±æ¨¡åž‹åœ¨ä¸åŒé…ç½®ä¸‹ä¿æŒä¸€è‡´çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šAdaPerceiveræœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå…¶ç»Ÿä¸€çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAdaPerceiverå¯ä»¥åŒæ—¶åœ¨æ·±åº¦ã€å®½åº¦å’Œtokensä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œä»Žè€Œæ›´åŠ çµæ´»åœ°é€‚åº”ä¸åŒçš„ç¡¬ä»¶çŽ¯å¢ƒå’Œå»¶è¿Ÿçº¦æŸã€‚æ­¤å¤–ï¼ŒAdaPerceiverè¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è”åˆè®­ç»ƒæ–¹æ¡ˆï¼Œå¯ä»¥ç¡®ä¿æ¨¡åž‹åœ¨ä¸åŒé…ç½®ä¸‹ä¿æŒä¸€è‡´çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šAdaPerceiverçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯è·³è¿‡çš„Transformerå±‚ï¼Œå…è®¸æ¨¡åž‹åŠ¨æ€è°ƒæ•´æ·±åº¦ï¼›2) å¯å˜å®½åº¦çš„éšè—å±‚ï¼Œå…è®¸æ¨¡åž‹åŠ¨æ€è°ƒæ•´å®½åº¦ï¼›3) åŸºäºŽæ³¨æ„åŠ›çš„tokené€‰æ‹©æœºåˆ¶ï¼Œå…è®¸æ¨¡åž‹åŠ¨æ€è°ƒæ•´tokensæ•°é‡ï¼›4) ä¸€ç§ç‰¹æ®Šçš„æŸå¤±å‡½æ•°ï¼Œç”¨äºŽé¼“åŠ±æ¨¡åž‹åœ¨ä¸åŒé…ç½®ä¸‹ä¿æŒä¸€è‡´çš„æ€§èƒ½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

AdaPerceiveråœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå®žçŽ°äº†85.4%çš„å‡†ç¡®çŽ‡ï¼ŒåŒæ—¶æ¯”FlexiViT-Læé«˜äº†36%çš„åžåé‡ã€‚åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šï¼ŒAdaPerceiveråœ¨è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ä¸Šä¸ŽViT-H/14çš„æ€§èƒ½ç›¸å½“ï¼Œä½†ç¼–ç å™¨çš„FLOPså‡å°‘äº†çº¦26å€ã€‚æ­¤å¤–ï¼Œé…å¤‡ç­–ç•¥çš„AdaPerceiverå¯ä»¥åœ¨ä¿æŒImageNet1Kå‡†ç¡®çŽ‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå°†FLOPsé™ä½Ž24-33%ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAdaPerceiveråœ¨ç²¾åº¦å’Œæ•ˆçŽ‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

AdaPerceiverå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶é€‚ç”¨äºŽèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡å’Œéœ€è¦å®žæ—¶å“åº”çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥åº”ç”¨äºŽç§»åŠ¨ç«¯çš„å›¾åƒè¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡æ£€æµ‹ã€ä»¥åŠè§†é¢‘ç›‘æŽ§ä¸­çš„å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ç­‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡åž‹çš„æ·±åº¦ã€å®½åº¦å’Œtokensæ•°é‡ï¼ŒAdaPerceiverå¯ä»¥åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½Žè®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿï¼Œä»Žè€Œå®žçŽ°æ›´é«˜æ•ˆçš„éƒ¨ç½²å’Œåº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

