---
layout: default
title: Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling
---

# Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00877" target="_blank" class="toolbar-btn">arXiv: 2512.00877v1</a>
    <a href="https://arxiv.org/pdf/2512.00877.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00877v1" 
            onclick="toggleFavorite(this, '2512.00877v1', 'Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhening Liu, Rui Song, Yushi Huang, Yingdong Hu, Xinjie Zhang, Jiawei Shao, Zehong Lin, Jun Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÈïøÁ®ã‰∏ä‰∏ãÊñáÂª∫Ê®°ÁöÑÂâçÈ¶à3DÈ´òÊñØÊ∫ÖÂ∞ÑÂéãÁº©ÊñπÊ≥ïÔºåÂÆûÁé∞È´òÂéãÁº©Áéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `3DÈ´òÊñØÊ∫ÖÂ∞Ñ` `ÂéãÁº©` `ÈïøÁ®ã‰∏ä‰∏ãÊñáÂª∫Ê®°` `Ëá™ÂõûÂΩíÁÜµÊ®°Âûã` `Ê≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂâçÈ¶à3DGSÂéãÁº©ÊñπÊ≥ïÈöæ‰ª•Âª∫Ê®°ÈïøÁ®ãÁ©∫Èó¥‰æùËµñÔºåÈôêÂà∂‰∫ÜÂéãÁº©ÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. ÊûÑÂª∫Â§ßËßÑÊ®°‰∏ä‰∏ãÊñáÁªìÊûÑÔºåËÆæËÆ°ÁªÜÁ≤íÂ∫¶Ëá™ÂõûÂΩíÁÜµÊ®°ÂûãÂíåÊ≥®ÊÑèÂäõÂèòÊç¢ÁºñÁ†ÅÊ®°ÂûãÔºåÊúâÊïàÂª∫Ê®°ÈïøÁ®ãÁõ∏ÂÖ≥ÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®3DGSÂéãÁº©‰∏≠ÂÆûÁé∞‰∫Ü20ÂÄçÁöÑÂéãÁº©ÁéáÔºåÂπ∂Âú®ÈÄöÁî®ÁºñËß£Á†ÅÂô®‰∏≠ËææÂà∞SOTAÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

3DÈ´òÊñØÊ∫ÖÂ∞Ñ(3DGS)‰Ωú‰∏∫‰∏ÄÁßçÈù©ÂëΩÊÄßÁöÑ3DË°®Á§∫ÊñπÊ≥ïÂ∑≤ÁªèÂá∫Áé∞„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Â∫ûÂ§ßÁöÑÊï∞ÊçÆÈáèÂØπÂπøÊ≥õÂ∫îÁî®ÊûÑÊàê‰∫Ü‰∏ªË¶ÅÈöúÁ¢ç„ÄÇËôΩÁÑ∂ÂâçÈ¶à3DGSÂéãÁº©‰∏∫ÊòÇË¥µÁöÑÂçïÂú∫ÊôØÂçïËÆ≠ÁªÉÂéãÁº©Âô®Êèê‰æõ‰∫Ü‰∏ÄÁßçÂÆûÁî®ÁöÑÊõø‰ª£ÊñπÊ°àÔºå‰ΩÜÁî±‰∫éÂèòÊç¢ÁºñÁ†ÅÁΩëÁªúÁöÑÊúâÈôêÊÑüÂèóÈáéÂíåÁÜµÊ®°Âûã‰∏≠‰∏çË∂≥ÁöÑ‰∏ä‰∏ãÊñáÂÆπÈáèÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Âª∫Ê®°ÈïøÁ®ãÁ©∫Èó¥‰æùËµñÂÖ≥Á≥ª„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂâçÈ¶à3DGSÂéãÁº©Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÊúâÊïàÂú∞Âª∫Ê®°‰∫ÜÈïøÁ®ãÁõ∏ÂÖ≥ÊÄßÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÈ´òÂ∫¶Á¥ßÂáëÂíåÂèØÊ≥õÂåñÁöÑ3DË°®Á§∫„ÄÇÊàë‰ª¨ÊñπÊ≥ïÁöÑ‰∏≠ÂøÉÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑ‰∏ä‰∏ãÊñáÁªìÊûÑÔºåÂÆÉÂåÖÂê´Êï∞ÂçÉ‰∏™Âü∫‰∫éMortonÂ∫èÂàóÂåñÁöÑGaussian„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÁªÜÁ≤íÂ∫¶ÁöÑÁ©∫Èó¥-ÈÄöÈÅìËá™ÂõûÂΩíÁÜµÊ®°ÂûãÔºå‰ª•ÂÖÖÂàÜÂà©Áî®ËøôÁßçÂπøÊ≥õÁöÑ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂèòÊç¢ÁºñÁ†ÅÊ®°ÂûãÔºåÈÄöËøáËÅöÂêàÊù•Ëá™ÂπøÊ≥õÈÇªÂüüGaussianÁöÑÁâπÂæÅÊù•ÊèêÂèñ‰ø°ÊÅØ‰∏∞ÂØåÁöÑÊΩúÂú®ÂÖàÈ™å„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Ê≠£ÂêëÊé®ÁêÜ‰∏≠‰∏∫3DGS‰∫ßÁîü‰∫Ü20ÂÄçÁöÑÂéãÁº©ÁéáÔºåÂπ∂Âú®ÈÄöÁî®ÁºñËß£Á†ÅÂô®‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**Ôºö3DÈ´òÊñØÊ∫ÖÂ∞Ñ(3DGS)Êï∞ÊçÆÈáèÂ∑®Â§ßÔºåÈòªÁ¢ç‰∫ÜÂÖ∂ÂπøÊ≥õÂ∫îÁî®„ÄÇÁé∞ÊúâÁöÑÂâçÈ¶àÂéãÁº©ÊñπÊ≥ïÂèóÈôê‰∫éÊÑüÂèóÈáéÂíå‰∏ä‰∏ãÊñáÂÆπÈáèÔºåÊó†Ê≥ïÊúâÊïàÂª∫Ê®°È´òÊñØÂàÜÂ∏É‰πãÈó¥ÁöÑÈïøÁ®ãÁ©∫Èó¥‰æùËµñÂÖ≥Á≥ªÔºåÂØºËá¥ÂéãÁº©Áéá‰∏çÈ´òÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÊûÑÂª∫Â§ßËßÑÊ®°‰∏ä‰∏ãÊñáÁªìÊûÑÔºåÂπ∂ËÆæËÆ°Á≤æÁªÜÁöÑËá™ÂõûÂΩíÁÜµÊ®°ÂûãÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂèòÊç¢ÁºñÁ†ÅÊ®°ÂûãÔºåÊù•ÂÖÖÂàÜÂà©Áî®È´òÊñØÂàÜÂ∏É‰πãÈó¥ÁöÑÈïøÁ®ãÁõ∏ÂÖ≥ÊÄßÔºå‰ªéËÄåÊèêÈ´òÂéãÁº©ÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊ†∏ÂøÉÂú®‰∫éÂà©Áî®Êõ¥Â§ßËåÉÂõ¥ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÊù•ÊåáÂØºÂéãÁº©ËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™ÈÉ®ÂàÜÔºö1) Â§ßËßÑÊ®°‰∏ä‰∏ãÊñáÁªìÊûÑÊûÑÂª∫Ôºö‰ΩøÁî®MortonÂ∫èÂàóÂåñÊñπÊ≥ïÁªÑÁªáÊï∞ÂçÉ‰∏™È´òÊñØÂàÜÂ∏ÉÔºåÂΩ¢Êàê‰∏ä‰∏ãÊñáÁªìÊûÑ„ÄÇ2) ÁªÜÁ≤íÂ∫¶Á©∫Èó¥-ÈÄöÈÅìËá™ÂõûÂΩíÁÜµÊ®°ÂûãÔºöÂà©Áî®‰∏ä‰∏ãÊñáÁªìÊûÑÔºåÂØπÈ´òÊñØÂàÜÂ∏ÉÁöÑÂêÑ‰∏™ÂèÇÊï∞ËøõË°åËá™ÂõûÂΩíÂª∫Ê®°ÔºåÊèêÈ´òÁÜµÁºñÁ†ÅÊïàÁéá„ÄÇ3) Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂèòÊç¢ÁºñÁ†ÅÊ®°ÂûãÔºöÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ËÅöÂêàÈÇªÂüüÈ´òÊñØÂàÜÂ∏ÉÁöÑÁâπÂæÅÔºåÊèêÂèñÊΩúÂú®ÂÖàÈ™å‰ø°ÊÅØÔºåËæÖÂä©ÂéãÁº©„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö1) Â§ßËßÑÊ®°‰∏ä‰∏ãÊñáÁªìÊûÑÔºöÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÊÑüÂèóÈáéÈôêÂà∂ÔºåËÉΩÂ§üÂª∫Ê®°Êõ¥ËøúË∑ùÁ¶ªÁöÑÈ´òÊñØÂàÜÂ∏É‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ2) ÁªÜÁ≤íÂ∫¶Á©∫Èó¥-ÈÄöÈÅìËá™ÂõûÂΩíÁÜµÊ®°ÂûãÔºöÂÖÖÂàÜÂà©Áî®‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂØπÈ´òÊñØÂàÜÂ∏ÉÁöÑÂêÑ‰∏™ÂèÇÊï∞ËøõË°åÁ≤æÁªÜÂª∫Ê®°ÔºåÊèêÈ´ò‰∫ÜÁÜµÁºñÁ†ÅÁöÑÊïàÁéá„ÄÇ3) Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂºïÂÖ•Ôºö‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©ÈáçË¶ÅÁöÑÈÇªÂüüÈ´òÊñØÂàÜÂ∏ÉÁâπÂæÅÔºåÊèêÈ´ò‰∫ÜÁâπÂæÅÊèêÂèñÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö1) MortonÂ∫èÂàóÂåñÔºöÁî®‰∫éÊûÑÂª∫Â§ßËßÑÊ®°‰∏ä‰∏ãÊñáÁªìÊûÑÔºåÊèêÈ´òÁ©∫Èó¥Á¥¢ÂºïÊïàÁéá„ÄÇ2) Ëá™ÂõûÂΩíÁÜµÊ®°ÂûãÔºöÈááÁî®Á©∫Èó¥ÂíåÈÄöÈÅì‰∏§‰∏™Áª¥Â∫¶ËøõË°åËá™ÂõûÂΩíÂª∫Ê®°ÔºåÂÖÖÂàÜÂà©Áî®‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ3) Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºöÈááÁî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊèêÈ´òÁâπÂæÅÊèêÂèñÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ4) ÊçüÂ§±ÂáΩÊï∞ÔºöÈááÁî®ÁéáÂ§±Áúü‰ºòÂåñ(Rate-Distortion Optimization)ÊçüÂ§±ÂáΩÊï∞ÔºåÂπ≥Ë°°ÂéãÁº©ÁéáÂíåÈáçÂª∫Ë¥®Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®3DGSÂéãÁº©‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂÆûÁé∞‰∫Ü20ÂÄçÁöÑÂéãÁº©ÁéáÔºåÂπ∂Âú®ÈÄöÁî®ÁºñËß£Á†ÅÂô®‰∏≠ËææÂà∞‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞Âª∫Ê®°ÈïøÁ®ãÁ©∫Èó¥‰æùËµñÂÖ≥Á≥ªÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂéãÁº©ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰∏âÁª¥Âú∫ÊôØÁöÑÂø´ÈÄü‰º†Ëæì‰∏éÂ≠òÂÇ®Ôºå‰æãÂ¶ÇVR/AR„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÈ´òÊïàÂéãÁº©3DGSÊï∞ÊçÆÔºåÂèØ‰ª•Èôç‰ΩéÂ≠òÂÇ®ÊàêÊú¨„ÄÅÂáèÂ∞ë‰º†ËæìÂ∏¶ÂÆΩÈúÄÊ±ÇÔºåÂπ∂ÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÊé®Âä®3DGSÂú®Êõ¥Â§öÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> 3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.

