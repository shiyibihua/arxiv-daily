---
layout: default
title: Optimizing Multimodal Language Models through Attention-based Interpretability
---

# Optimizing Multimodal Language Models through Attention-based Interpretability

**arXiv**: [2511.23375v1](https://arxiv.org/abs/2511.23375) | [PDF](https://arxiv.org/pdf/2511.23375.pdf)

**ä½œè€…**: Alexander Sergeev, Evgeny Kotelnikov

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `æ³¨æ„åŠ›æœºåˆ¶` `å¯è§£é‡Šæ€§` `å›¾åƒç†è§£` `å›¾åƒæè¿°ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹éš¾ä»¥è§£é‡Šï¼Œå½±å“å‚æ•°é«˜æ•ˆå¾®è°ƒä¸­å…³é”®ç»„ä»¶çš„é€‰æ‹©
2. é€šè¿‡åˆ†æžæ³¨æ„åŠ›åˆ†æ•°è¯†åˆ«å…³æ³¨å›¾åƒå…³é”®å¯¹è±¡çš„æ³¨æ„åŠ›å¤´ï¼Œå¹¶è®¡ç®—Head Impactåˆ†æ•°é‡åŒ–å…¶é‡è¦æ€§
3. å®žéªŒè¡¨æ˜Žå¾®è°ƒé«˜HIåˆ†æ•°å±‚èƒ½æ˜¾è‘—æå‡å›¾åƒç†è§£èƒ½åŠ›ï¼Œä»…éœ€å¾®è°ƒçº¦0.01%å‚æ•°

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.

