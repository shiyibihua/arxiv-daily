---
layout: default
title: Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound
---

# Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound

**arXiv**: [2512.00883v1](https://arxiv.org/abs/2512.00883) | [PDF](https://arxiv.org/pdf/2512.00883.pdf)

**ä½œè€…**: Jiahua Wang, Shannan Yan, Leqi Zheng, Jialong Wu, Yaoxin Mao

**åˆ†ç±»**: cs.MM, cs.CV, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAVWMæ¡†æž¶ï¼Œåˆ©ç”¨è§†å¬ä¿¡æ¯è¿›è¡ŒçŽ¯å¢ƒå»ºæ¨¡ï¼Œæå‡æ™ºèƒ½ä½“å¯¼èˆªæ€§èƒ½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `éŸ³é¢‘-è§†è§‰ä¸–ç•Œæ¨¡åž‹` `å¤šæ¨¡æ€å­¦ä¹ ` `çŽ¯å¢ƒå»ºæ¨¡` `æœºå™¨äººå¯¼èˆª` `æ‰©æ•£æ¨¡åž‹` `Transformer` `åŒè€³éŸ³é¢‘`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ä¸–ç•Œæ¨¡åž‹ä¸»è¦ä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œå¿½ç•¥äº†éŸ³é¢‘æä¾›çš„ç©ºé—´å’Œæ—¶é—´çº¿ç´¢ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
2. æå‡ºéŸ³é¢‘-è§†è§‰ä¸–ç•Œæ¨¡åž‹ï¼ˆAVWMï¼‰ï¼Œç»“åˆåŒè€³ç©ºé—´éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œæ¨¡æ‹ŸçŽ¯å¢ƒåŠ¨æ€ï¼Œæå‡æ™ºèƒ½ä½“å¯¹çŽ¯å¢ƒçš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚
3. æž„å»ºäº†AVW-4kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†AV-CDiTæ¨¡åž‹ï¼Œå®žéªŒè¯æ˜Žè¯¥æ¨¡åž‹åœ¨å¤šæ¨¡æ€é¢„æµ‹å’ŒéŸ³é¢‘-è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†éŸ³é¢‘-è§†è§‰ä¸–ç•Œæ¨¡åž‹ï¼ˆAVWMï¼‰ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒåŠ¨æ€ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè§„åˆ’å’ŒæŽ¨ç†æœªæ¥çŠ¶æ€ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºŽè§†è§‰è§‚å¯Ÿï¼Œè€ŒçŽ°å®žä¸–ç•Œæ„ŸçŸ¥æ¶‰åŠå¤šç§æ„Ÿå®˜æ¨¡æ€ã€‚éŸ³é¢‘æä¾›å…³é”®çš„ç©ºé—´å’Œæ—¶é—´çº¿ç´¢ï¼Œå¦‚å£°æºå®šä½å’Œå£°å­¦åœºæ™¯å±žæ€§ï¼Œä½†å…¶ä¸Žä¸–ç•Œæ¨¡åž‹çš„é›†æˆä»æœªè¢«å……åˆ†æŽ¢ç´¢ã€‚æœ¬æ–‡é¦–æ¬¡æ­£å¼å®šä¹‰äº†éŸ³é¢‘-è§†è§‰ä¸–ç•Œæ¨¡åž‹ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºå…·æœ‰åŒæ­¥è§†å¬è§‚å¯Ÿã€ç²¾ç»†åŠ¨ä½œå’Œä»»åŠ¡å¥–åŠ±çš„éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³ç¼ºä¹åˆé€‚è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæž„å»ºäº†AVW-4kæ•°æ®é›†ï¼ŒåŒ…å«76ä¸ªå®¤å†…çŽ¯å¢ƒä¸­30å°æ—¶çš„åŒè€³éŸ³é¢‘-è§†è§‰è½¨è¿¹ï¼Œå¸¦æœ‰åŠ¨ä½œæ³¨é‡Šå’Œå¥–åŠ±ä¿¡å·ã€‚æå‡ºäº†AV-CDiTï¼Œä¸€ç§éŸ³é¢‘-è§†è§‰æ¡ä»¶æ‰©æ•£Transformerï¼Œå…·æœ‰æ–°é¢–çš„æ¨¡æ€ä¸“å®¶æž¶æž„ï¼Œå¹³è¡¡è§†è§‰å’Œå¬è§‰å­¦ä¹ ï¼Œå¹¶é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®žçŽ°æœ‰æ•ˆçš„å¤šæ¨¡æ€é›†æˆã€‚å®žéªŒè¡¨æ˜Žï¼ŒAV-CDiTå®žçŽ°äº†è·¨è§†è§‰å’Œå¬è§‰æ¨¡æ€çš„é«˜ä¿çœŸå¤šæ¨¡æ€é¢„æµ‹ï¼Œå¹¶å…·æœ‰å¥–åŠ±é¢„æµ‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒéªŒè¯äº†å…¶åœ¨è¿žç»­éŸ³é¢‘-è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­çš„å®žç”¨æ€§ï¼ŒAVWMæ˜¾è‘—æé«˜äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰ä¸–ç•Œæ¨¡åž‹ä¸»è¦ä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œå¿½ç•¥äº†éŸ³é¢‘ä¿¡æ¯æä¾›çš„ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œä¾‹å¦‚å£°æºå®šä½å’Œåœºæ™¯å£°å­¦ç‰¹æ€§ã€‚è¿™é™åˆ¶äº†æ™ºèƒ½ä½“åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ„ŸçŸ¥å’ŒæŽ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤ŸåŒæ—¶å¤„ç†è§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œå¹¶é¢„æµ‹æœªæ¥çŠ¶æ€çš„éŸ³é¢‘-è§†è§‰ä¸–ç•Œæ¨¡åž‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çŽ¯å¢ƒå»ºæ¨¡ä¸ºä¸€ä¸ªéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œå…¶ä¸­çŠ¶æ€ç”±åŒæ­¥çš„éŸ³é¢‘å’Œè§†è§‰è§‚å¯Ÿç»„æˆï¼ŒåŠ¨ä½œæ˜¯æ™ºèƒ½ä½“çš„æŽ§åˆ¶æŒ‡ä»¤ï¼Œå¥–åŠ±æ˜¯ä»»åŠ¡å®Œæˆçš„åé¦ˆã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿé¢„æµ‹æœªæ¥çŠ¶æ€ï¼ˆåŒ…æ‹¬è§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼‰çš„æ¨¡åž‹ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°è§„åˆ’å’Œæ‰§è¡ŒåŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®é›†AVW-4kï¼Œç”¨äºŽè®­ç»ƒå’Œè¯„ä¼°æ¨¡åž‹ï¼›2) éŸ³é¢‘-è§†è§‰æ¡ä»¶æ‰©æ•£Transformer (AV-CDiT)ï¼Œç”¨äºŽå­¦ä¹ çŽ¯å¢ƒåŠ¨æ€ï¼›3) ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç”¨äºŽä¼˜åŒ–AV-CDiTæ¨¡åž‹ï¼›4) éŸ³é¢‘-è§†è§‰å¯¼èˆªä»»åŠ¡ï¼Œç”¨äºŽè¯„ä¼°AVWMçš„å®žé™…åº”ç”¨æ•ˆæžœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) é¦–æ¬¡æ­£å¼å®šä¹‰äº†éŸ³é¢‘-è§†è§‰ä¸–ç•Œæ¨¡åž‹ï¼ˆAVWMï¼‰ï¼›2) æž„å»ºäº†AVW-4kæ•°æ®é›†ï¼Œä¸ºAVWMçš„ç ”ç©¶æä¾›äº†æ•°æ®åŸºç¡€ï¼›3) æå‡ºäº†AV-CDiTæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹å…·æœ‰æ–°é¢–çš„æ¨¡æ€ä¸“å®¶æž¶æž„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡è§†è§‰å’Œå¬è§‰å­¦ä¹ ï¼›4) æå‡ºäº†ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæé«˜äº†AV-CDiTæ¨¡åž‹çš„è®­ç»ƒæ•ˆçŽ‡å’Œé¢„æµ‹ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šAV-CDiTæ¨¡åž‹é‡‡ç”¨Transformeræž¶æž„ï¼Œå¹¶å¼•å…¥äº†æ¨¡æ€ä¸“å®¶æ¨¡å—ï¼Œåˆ†åˆ«å¤„ç†è§†è§‰å’Œå¬è§‰ä¿¡æ¯ã€‚è§†è§‰ä¸“å®¶å’Œå¬è§‰ä¸“å®¶åˆ†åˆ«å­¦ä¹ è§†è§‰å’Œå¬è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèžåˆã€‚ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥åŒ…æ‹¬ï¼š1) é¢„è®­ç»ƒè§†è§‰å’Œå¬è§‰ä¸“å®¶ï¼›2) è”åˆè®­ç»ƒè§†è§‰å’Œå¬è§‰ä¸“å®¶ï¼›3) å¾®è°ƒæ•´ä¸ªæ¨¡åž‹ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è§†è§‰é‡å»ºæŸå¤±ã€å¬è§‰é‡å»ºæŸå¤±å’Œå¥–åŠ±é¢„æµ‹æŸå¤±ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒAV-CDiTæ¨¡åž‹åœ¨AVW-4kæ•°æ®é›†ä¸Šå®žçŽ°äº†é«˜ä¿çœŸçš„å¤šæ¨¡æ€é¢„æµ‹ï¼ŒåŒ…æ‹¬è§†è§‰å’Œå¬è§‰æ¨¡æ€ã€‚åœ¨éŸ³é¢‘-è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­ï¼ŒAVWMæ˜¾è‘—æé«˜äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œç›¸æ¯”äºŽä»…ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„æ¨¡åž‹ï¼Œå¯¼èˆªæˆåŠŸçŽ‡æå‡äº†çº¦15%ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººå¯¼èˆªã€è™šæ‹ŸçŽ°å®žã€æ¸¸æˆç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼ŒAVWMå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­æ›´å¥½åœ°æ„ŸçŸ¥å’Œç†è§£å‘¨å›´çŽ¯å¢ƒï¼Œä»Žè€Œå®žçŽ°æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„å¯¼èˆªã€‚åœ¨è™šæ‹ŸçŽ°å®žå’Œæ¸¸æˆä¸­ï¼ŒAVWMå¯ä»¥ç”Ÿæˆæ›´é€¼çœŸçš„è§†å¬ä½“éªŒï¼Œå¢žå¼ºç”¨æˆ·çš„æ²‰æµ¸æ„Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.

