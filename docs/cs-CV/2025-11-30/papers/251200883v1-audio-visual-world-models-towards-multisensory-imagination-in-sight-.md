---
layout: default
title: Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound
---

# Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00883" target="_blank" class="toolbar-btn">arXiv: 2512.00883v1</a>
    <a href="https://arxiv.org/pdf/2512.00883.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00883v1" 
            onclick="toggleFavorite(this, '2512.00883v1', 'Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiahua Wang, Shannan Yan, Leqi Zheng, Jialong Wu, Yaoxin Mao

**ÂàÜÁ±ª**: cs.MM, cs.CV, cs.SD

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AVWMÊ°ÜÊû∂ÔºåÂà©Áî®ËßÜÂê¨‰ø°ÊÅØËøõË°åÁéØÂ¢ÉÂª∫Ê®°ÔºåÊèêÂçáÊô∫ËÉΩ‰ΩìÂØºËà™ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Èü≥È¢ë-ËßÜËßâ‰∏ñÁïåÊ®°Âûã` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÁéØÂ¢ÉÂª∫Ê®°` `Êú∫Âô®‰∫∫ÂØºËà™` `Êâ©Êï£Ê®°Âûã` `Transformer` `ÂèåËÄ≥Èü≥È¢ë`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰∏ñÁïåÊ®°Âûã‰∏ªË¶Å‰æùËµñËßÜËßâ‰ø°ÊÅØÔºåÂøΩÁï•‰∫ÜÈü≥È¢ëÊèê‰æõÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥Á∫øÁ¥¢ÔºåÈôêÂà∂‰∫ÜÊô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ
2. ÊèêÂá∫Èü≥È¢ë-ËßÜËßâ‰∏ñÁïåÊ®°ÂûãÔºàAVWMÔºâÔºåÁªìÂêàÂèåËÄ≥Á©∫Èó¥Èü≥È¢ëÂíåËßÜËßâ‰ø°ÊÅØÔºåÊ®°ÊãüÁéØÂ¢ÉÂä®ÊÄÅÔºåÊèêÂçáÊô∫ËÉΩ‰ΩìÂØπÁéØÂ¢ÉÁöÑÁêÜËß£ÂíåÈ¢ÑÊµãËÉΩÂäõ„ÄÇ
3. ÊûÑÂª∫‰∫ÜAVW-4kÊï∞ÊçÆÈõÜÔºåÂπ∂ÊèêÂá∫‰∫ÜAV-CDiTÊ®°ÂûãÔºåÂÆûÈ™åËØÅÊòéËØ•Ê®°ÂûãÂú®Â§öÊ®°ÊÄÅÈ¢ÑÊµãÂíåÈü≥È¢ë-ËßÜËßâÂØºËà™‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜÈü≥È¢ë-ËßÜËßâ‰∏ñÁïåÊ®°ÂûãÔºàAVWMÔºâÔºåÊó®Âú®Ê®°ÊãüÁéØÂ¢ÉÂä®ÊÄÅÔºå‰ΩøÊô∫ËÉΩ‰ΩìËÉΩÂ§üËßÑÂàíÂíåÊé®ÁêÜÊú™Êù•Áä∂ÊÄÅ„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠‰∫éËßÜËßâËßÇÂØüÔºåËÄåÁé∞ÂÆû‰∏ñÁïåÊÑüÁü•Ê∂âÂèäÂ§öÁßçÊÑüÂÆòÊ®°ÊÄÅ„ÄÇÈü≥È¢ëÊèê‰æõÂÖ≥ÈîÆÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥Á∫øÁ¥¢ÔºåÂ¶ÇÂ£∞Ê∫êÂÆö‰ΩçÂíåÂ£∞Â≠¶Âú∫ÊôØÂ±ûÊÄßÔºå‰ΩÜÂÖ∂‰∏é‰∏ñÁïåÊ®°ÂûãÁöÑÈõÜÊàê‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊú¨ÊñáÈ¶ñÊ¨°Ê≠£ÂºèÂÆö‰πâ‰∫ÜÈü≥È¢ë-ËßÜËßâ‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂Â∞ÜÂÖ∂ÂΩ¢ÂºèÂåñ‰∏∫ÂÖ∑ÊúâÂêåÊ≠•ËßÜÂê¨ËßÇÂØü„ÄÅÁ≤æÁªÜÂä®‰ΩúÂíå‰ªªÂä°Â•ñÂä±ÁöÑÈÉ®ÂàÜÂèØËßÇÂØüÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Áº∫‰πèÂêàÈÄÇËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈóÆÈ¢òÔºåÊûÑÂª∫‰∫ÜAVW-4kÊï∞ÊçÆÈõÜÔºåÂåÖÂê´76‰∏™ÂÆ§ÂÜÖÁéØÂ¢É‰∏≠30Â∞èÊó∂ÁöÑÂèåËÄ≥Èü≥È¢ë-ËßÜËßâËΩ®ËøπÔºåÂ∏¶ÊúâÂä®‰ΩúÊ≥®ÈáäÂíåÂ•ñÂä±‰ø°Âè∑„ÄÇÊèêÂá∫‰∫ÜAV-CDiTÔºå‰∏ÄÁßçÈü≥È¢ë-ËßÜËßâÊù°‰ª∂Êâ©Êï£TransformerÔºåÂÖ∑ÊúâÊñ∞È¢ñÁöÑÊ®°ÊÄÅ‰∏ìÂÆ∂Êû∂ÊûÑÔºåÂπ≥Ë°°ËßÜËßâÂíåÂê¨ËßâÂ≠¶‰π†ÔºåÂπ∂ÈÄöËøá‰∏âÈò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ËøõË°å‰ºòÂåñÔºå‰ª•ÂÆûÁé∞ÊúâÊïàÁöÑÂ§öÊ®°ÊÄÅÈõÜÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºåAV-CDiTÂÆûÁé∞‰∫ÜË∑®ËßÜËßâÂíåÂê¨ËßâÊ®°ÊÄÅÁöÑÈ´ò‰øùÁúüÂ§öÊ®°ÊÄÅÈ¢ÑÊµãÔºåÂπ∂ÂÖ∑ÊúâÂ•ñÂä±È¢ÑÊµãËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ËøûÁª≠Èü≥È¢ë-ËßÜËßâÂØºËà™‰ªªÂä°‰∏≠ÁöÑÂÆûÁî®ÊÄßÔºåAVWMÊòæËëóÊèêÈ´ò‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ‰∏ñÁïåÊ®°Âûã‰∏ªË¶Å‰æùËµñËßÜËßâ‰ø°ÊÅØÔºåÂøΩÁï•‰∫ÜÈü≥È¢ë‰ø°ÊÅØÊèê‰æõÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥‰ø°ÊÅØÔºå‰æãÂ¶ÇÂ£∞Ê∫êÂÆö‰ΩçÂíåÂú∫ÊôØÂ£∞Â≠¶ÁâπÊÄß„ÄÇËøôÈôêÂà∂‰∫ÜÊô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜËßÜËßâÂíåÂê¨Ëßâ‰ø°ÊÅØÔºåÂπ∂È¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÁöÑÈü≥È¢ë-ËßÜËßâ‰∏ñÁïåÊ®°Âûã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁéØÂ¢ÉÂª∫Ê®°‰∏∫‰∏Ä‰∏™ÈÉ®ÂàÜÂèØËßÇÂØüÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàPOMDPÔºâÔºåÂÖ∂‰∏≠Áä∂ÊÄÅÁî±ÂêåÊ≠•ÁöÑÈü≥È¢ëÂíåËßÜËßâËßÇÂØüÁªÑÊàêÔºåÂä®‰ΩúÊòØÊô∫ËÉΩ‰ΩìÁöÑÊéßÂà∂Êåá‰ª§ÔºåÂ•ñÂä±ÊòØ‰ªªÂä°ÂÆåÊàêÁöÑÂèçÈ¶à„ÄÇÈÄöËøáÂ≠¶‰π†‰∏Ä‰∏™ËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÔºàÂåÖÊã¨ËßÜËßâÂíåÂê¨Ëßâ‰ø°ÊÅØÔºâÁöÑÊ®°ÂûãÔºåÊô∫ËÉΩ‰ΩìÂèØ‰ª•Êõ¥Â•ΩÂú∞ËßÑÂàíÂíåÊâßË°åÂä®‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Êï∞ÊçÆÈõÜAVW-4kÔºåÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞Ê®°ÂûãÔºõ2) Èü≥È¢ë-ËßÜËßâÊù°‰ª∂Êâ©Êï£Transformer (AV-CDiT)ÔºåÁî®‰∫éÂ≠¶‰π†ÁéØÂ¢ÉÂä®ÊÄÅÔºõ3) ‰∏âÈò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁî®‰∫é‰ºòÂåñAV-CDiTÊ®°ÂûãÔºõ4) Èü≥È¢ë-ËßÜËßâÂØºËà™‰ªªÂä°ÔºåÁî®‰∫éËØÑ‰º∞AVWMÁöÑÂÆûÈôÖÂ∫îÁî®ÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) È¶ñÊ¨°Ê≠£ÂºèÂÆö‰πâ‰∫ÜÈü≥È¢ë-ËßÜËßâ‰∏ñÁïåÊ®°ÂûãÔºàAVWMÔºâÔºõ2) ÊûÑÂª∫‰∫ÜAVW-4kÊï∞ÊçÆÈõÜÔºå‰∏∫AVWMÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊï∞ÊçÆÂü∫Á°ÄÔºõ3) ÊèêÂá∫‰∫ÜAV-CDiTÊ®°ÂûãÔºåËØ•Ê®°ÂûãÂÖ∑ÊúâÊñ∞È¢ñÁöÑÊ®°ÊÄÅ‰∏ìÂÆ∂Êû∂ÊûÑÔºåËÉΩÂ§üÊúâÊïàÂú∞Âπ≥Ë°°ËßÜËßâÂíåÂê¨ËßâÂ≠¶‰π†Ôºõ4) ÊèêÂá∫‰∫Ü‰∏âÈò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊèêÈ´ò‰∫ÜAV-CDiTÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÈ¢ÑÊµãÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAV-CDiTÊ®°ÂûãÈááÁî®TransformerÊû∂ÊûÑÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊ®°ÊÄÅ‰∏ìÂÆ∂Ê®°ÂùóÔºåÂàÜÂà´Â§ÑÁêÜËßÜËßâÂíåÂê¨Ëßâ‰ø°ÊÅØ„ÄÇËßÜËßâ‰∏ìÂÆ∂ÂíåÂê¨Ëßâ‰∏ìÂÆ∂ÂàÜÂà´Â≠¶‰π†ËßÜËßâÂíåÂê¨ËßâÁâπÂæÅÔºåÂπ∂ÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åËûçÂêà„ÄÇ‰∏âÈò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÂåÖÊã¨Ôºö1) È¢ÑËÆ≠ÁªÉËßÜËßâÂíåÂê¨Ëßâ‰∏ìÂÆ∂Ôºõ2) ËÅîÂêàËÆ≠ÁªÉËßÜËßâÂíåÂê¨Ëßâ‰∏ìÂÆ∂Ôºõ3) ÂæÆË∞ÉÊï¥‰∏™Ê®°Âûã„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ËßÜËßâÈáçÂª∫ÊçüÂ§±„ÄÅÂê¨ËßâÈáçÂª∫ÊçüÂ§±ÂíåÂ•ñÂä±È¢ÑÊµãÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAV-CDiTÊ®°ÂûãÂú®AVW-4kÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÈ´ò‰øùÁúüÁöÑÂ§öÊ®°ÊÄÅÈ¢ÑÊµãÔºåÂåÖÊã¨ËßÜËßâÂíåÂê¨ËßâÊ®°ÊÄÅ„ÄÇÂú®Èü≥È¢ë-ËßÜËßâÂØºËà™‰ªªÂä°‰∏≠ÔºåAVWMÊòæËëóÊèêÈ´ò‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊÄßËÉΩÔºåÁõ∏ÊØî‰∫é‰ªÖ‰ΩøÁî®ËßÜËßâ‰ø°ÊÅØÁöÑÊ®°ÂûãÔºåÂØºËà™ÊàêÂäüÁéáÊèêÂçá‰∫ÜÁ∫¶15%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåAVWMÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠Êõ¥Â•ΩÂú∞ÊÑüÁü•ÂíåÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂÆâÂÖ®„ÄÅÊõ¥È´òÊïàÁöÑÂØºËà™„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÂíåÊ∏∏Êàè‰∏≠ÔºåAVWMÂèØ‰ª•ÁîüÊàêÊõ¥ÈÄºÁúüÁöÑËßÜÂê¨‰ΩìÈ™åÔºåÂ¢ûÂº∫Áî®Êà∑ÁöÑÊ≤âÊµ∏ÊÑü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.

