---
layout: default
title: DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline
---

# DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline

**arXiv**: [2511.23377v1](https://arxiv.org/abs/2511.23377) | [PDF](https://arxiv.org/pdf/2511.23377.pdf)

**ä½œè€…**: Rui Zhang, Hongxia Wang, Hangqing Liu, Yang Zhou, Qiang Zeng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDEAL-300Kæ•°æ®é›†ä¸Žé¢‘çŽ‡æç¤ºåŸºçº¿ï¼Œç”¨äºŽæ‰©æ•£ç¼–è¾‘åŒºåŸŸå®šä½ï¼Œä»¥è§£å†³å±€éƒ¨ä¼ªé€ æ£€æµ‹éš¾é¢˜ã€‚**

**å…³é”®è¯**: `æ‰©æ•£ç¼–è¾‘å®šä½` `å¤§è§„æ¨¡æ•°æ®é›†` `é¢‘çŽ‡åŸŸåˆ†æž` `è§†è§‰åŸºç¡€æ¨¡åž‹` `åƒç´ çº§æ ‡æ³¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£ç¼–è¾‘å›¾åƒå±€éƒ¨ä¼ªé€ éš¾ä»¥å®šä½ï¼ŒçŽ°æœ‰åŸºå‡†ä¸åæ˜ å…¶å¹³æ»‘èžåˆç‰¹æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œç»“åˆå¤šæ¨¡æ€æŒ‡ä»¤ç”Ÿæˆã€æ— æŽ©ç ç¼–è¾‘å™¨å’Œä¸»åŠ¨å­¦ä¹ æ ‡æ³¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåŸºäºŽå†»ç»“è§†è§‰åŸºç¡€æ¨¡åž‹ä¸Žå¤šé¢‘çŽ‡æç¤ºè°ƒä¼˜ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°82.56%åƒç´ çº§F1åˆ†æ•°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.

