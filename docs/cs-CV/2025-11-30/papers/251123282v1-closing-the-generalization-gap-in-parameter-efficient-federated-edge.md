---
layout: default
title: Closing the Generalization Gap in Parameter-efficient Federated Edge Learning
---

# Closing the Generalization Gap in Parameter-efficient Federated Edge Learning

**arXiv**: [2511.23282v1](https://arxiv.org/abs/2511.23282) | [PDF](https://arxiv.org/pdf/2511.23282.pdf)

**ä½œè€…**: Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‚æ•°é«˜æ•ˆçš„è”é‚¦è¾¹ç¼˜å­¦ä¹ æ¡†æž¶ï¼Œé€šè¿‡æ¨¡åž‹å‰ªæžä¸Žå®¢æˆ·ç«¯é€‰æ‹©è§£å†³æ³›åŒ–ä¸Žèµ„æºåˆ©ç”¨é—®é¢˜ã€‚**

**å…³é”®è¯**: `è”é‚¦è¾¹ç¼˜å­¦ä¹ ` `æ¨¡åž‹æ³›åŒ–` `å‚æ•°é«˜æ•ˆ` `æ¨¡åž‹å‰ªæž` `å®¢æˆ·ç«¯é€‰æ‹©` `ç³»ç»Ÿä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè”é‚¦è¾¹ç¼˜å­¦ä¹ ä¸­æ•°æ®æœ‰é™ã€å¼‚æž„åŠèµ„æºå—é™å¯¼è‡´æ¨¡åž‹æ³›åŒ–å·®å’Œæ€§èƒ½ä¸‹é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆä¿¡æ¯è®ºæ³›åŒ–åˆ†æžä¸Žç³»ç»Ÿä¼˜åŒ–ï¼Œè”åˆä¼˜åŒ–å‰ªæžçŽ‡ã€å®¢æˆ·ç«¯é€‰æ‹©å’Œèµ„æºåˆ†é…ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºä¼˜äºŽçŽ°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†æ³›åŒ–æ„ŸçŸ¥åˆ†æžä¸Žç³»ç»Ÿçº§ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.

