---
layout: default
title: Buffer replay enhances the robustness of multimodal learning under missing-modality
---

# Buffer replay enhances the robustness of multimodal learning under missing-modality

**arXiv**: [2511.23070v1](https://arxiv.org/abs/2511.23070) | [PDF](https://arxiv.org/pdf/2511.23070.pdf)

**ä½œè€…**: Hongye Zhu, Xuan Liu, Yanwen Ba, Jingye Xue, Shigeng Zhang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREplay Promptingä»¥å¢žå¼ºå¤šæ¨¡æ€å­¦ä¹ åœ¨ç¼ºå¤±æ¨¡æ€ä¸‹çš„é²æ£’æ€§**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç¼ºå¤±æ¨¡æ€é²æ£’æ€§` `ç‰¹å¾ç¼“å†²åŒº` `æ®‹å·®æ—è·¯` `ç§æœ‰-å…±äº«ç‰¹å¾è§£è€¦` `ä»»åŠ¡æ„ŸçŸ¥åˆå§‹åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºå¤±æ¨¡æ€å¯¼è‡´å¤šæ¨¡æ€æ¨¡åž‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ŒçŽ°æœ‰æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æˆ–å¿½ç•¥é•¿è·ç¦»ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ®‹å·®æ—è·¯æž„å»ºæ¨¡æ€ç‰¹å¾ç¼“å†²åŒºï¼Œç»“åˆç§æœ‰-å…±äº«ç‰¹å¾è§£è€¦å’Œä»»åŠ¡æ„ŸçŸ¥åŠ¨æ€åˆå§‹åŒ–æœºåˆ¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†è§‰-è¯­è¨€ã€è§†è§‰-è¯­è¨€-éŸ³é¢‘ç­‰åŸºå‡†ä¸Šï¼ŒREPåœ¨å•/å¤šæ¨¡æ€ç¼ºå¤±åœºæ™¯ä¸‹ä¼˜äºŽå…ˆå‰æ–¹æ³•ï¼Œå‚æ•°å¼€é”€å¯å¿½ç•¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.

