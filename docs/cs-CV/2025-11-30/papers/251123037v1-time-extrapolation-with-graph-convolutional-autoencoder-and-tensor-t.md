---
layout: default
title: Time Extrapolation with Graph Convolutional Autoencoder and Tensor Train Decomposition
---

# Time Extrapolation with Graph Convolutional Autoencoder and Tensor Train Decomposition

**arXiv**: [2511.23037v1](https://arxiv.org/abs/2511.23037) | [PDF](https://arxiv.org/pdf/2511.23037.pdf)

**ä½œè€…**: Yuanhong Chen, Federico Pichi, Zhen Gao, Gianluigi Rozza

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆå›¾å·ç§¯è‡ªç¼–ç å™¨ä¸Žå¼ é‡åˆ—è½¦åˆ†è§£çš„æ¨¡åž‹ï¼Œä»¥è§£å†³å‚æ•°åŒ–åå¾®åˆ†æ–¹ç¨‹åœ¨å¤æ‚å‡ ä½•ä¸Šçš„æ—¶é—´å¤–æŽ¨é—®é¢˜ã€‚**

**å…³é”®è¯**: `å›¾å·ç§¯è‡ªç¼–ç å™¨` `å¼ é‡åˆ—è½¦åˆ†è§£` `æ—¶é—´å¤–æŽ¨` `å‚æ•°åŒ–åå¾®åˆ†æ–¹ç¨‹` `æ·±åº¦ç®—å­ç½‘ç»œ` `ç®—å­æŽ¨æ–­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾è‡ªç¼–ç å™¨åœ¨å‚æ•°åŒ–åŠ¨æ€ç³»ç»Ÿçš„æ—¶é—´å¤–æŽ¨ä¸­ï¼Œéœ€åŒæ—¶å¤„ç†æ—¶é—´å› æžœæ€§å’Œå‚æ•°ç©ºé—´æ³›åŒ–æ€§ï¼Œé¢ä¸´æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¼ é‡åˆ—è½¦åˆ†è§£å°†é«˜ä¿çœŸå¿«ç…§åˆ†è§£ä¸ºå‚æ•°ã€ç©ºé—´å’Œæ—¶é—´æ ¸å¿ƒï¼Œç»“åˆç®—å­æŽ¨æ–­å­¦ä¹ æ—¶é—´æ¼”åŒ–ï¼Œå¹¶åˆ©ç”¨æ·±åº¦ç®—å­ç½‘ç»œå¢žå¼ºæ³›åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çƒ­ä¼ å¯¼ã€å¯¹æµæ‰©æ•£å’Œæ¶¡æ—‹è„±è½ç­‰æ•°å€¼å®žéªŒä¸­ï¼Œç›¸æ¯”MeshGraphNetsç­‰å…ˆè¿›æ–¹æ³•ï¼Œå±•çŽ°å‡ºä¼˜å¼‚çš„å¤–æŽ¨æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graph autoencoders have gained attention in nonlinear reduced-order modeling of parameterized partial differential equations defined on unstructured grids. Despite they provide a geometrically consistent way of treating complex domains, applying such architectures to parameterized dynamical systems for temporal prediction beyond the training data, i.e. the extrapolation regime, is still a challenging task due to the simultaneous need of temporal causality and generalizability in the parametric space. In this work, we explore the integration of graph convolutional autoencoders (GCAs) with tensor train (TT) decomposition and Operator Inference (OpInf) to develop a time-consistent reduced-order model. In particular, high-fidelity snapshots are represented as a combination of parametric, spatial, and temporal cores via TT decomposition, while OpInf is used to learn the evolution of the latter. Moreover, we enhance the generalization performance by developing a multi-fidelity two-stages approach in the framework of Deep Operator Networks (DeepONet), treating the spatial and temporal cores as the trunk networks, and the parametric core as the branch network. Numerical results, including heat-conduction, advection-diffusion and vortex-shedding phenomena, demonstrate great performance in effectively learning the dynamic in the extrapolation regime for complex geometries, also in comparison with state-of-the-art approaches e.g. MeshGraphNets.

