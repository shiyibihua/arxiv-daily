---
layout: default
title: Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering
---

# Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering

**arXiv**: [2511.23231v1](https://arxiv.org/abs/2511.23231) | [PDF](https://arxiv.org/pdf/2511.23231.pdf)

**ä½œè€…**: Qiming Li, Xiaocheng Feng, Yixuan Ma, Zekai Ye, Ruihan Chen, Xiachong Feng, Bing Qin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMRREæ–¹æ³•ä»¥å¢žå¼ºå¤§æ¨¡åž‹åœ¨ä½Žèµ„æºè¯­è¨€ä¸Šçš„æŽ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–ç¿»è¯‘å·¥å…·ã€‚**

**å…³é”®è¯**: `å¤šè¯­è¨€æŽ¨ç†` `è¡¨ç¤ºå·¥ç¨‹` `æŽ¨ç†æ—¶ä¼˜åŒ–` `ä½Žèµ„æºè¯­è¨€` `å¤§è¯­è¨€æ¨¡åž‹` `å¤§è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§æ¨¡åž‹åœ¨è‹±è¯­ä¸Žä½Žèµ„æºè¯­è¨€é—´æŽ¨ç†æ€§èƒ½å·®è·å¤§ï¼Œå½±å“å¤šè¯­è¨€å…¬å¹³æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æŽ¨ç†æ—¶æ³¨å…¥é¢„è®¡ç®—å‘é‡ï¼Œåˆ†æ­¥æå‡è·¨è¯­è¨€æŽ¨ç†å’Œä¿æŒè¯­è¨€ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…­ä¸ªæ¨¡åž‹ä¸Šå¹³å‡æå‡éžè‹±è¯­æŽ¨ç†5.48%ï¼Œä½Žèµ„æºè¯­è¨€æœ€é«˜è¾¾7.54%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.

