---
layout: default
title: VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction
---

# VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction

**arXiv**: [2511.23386v1](https://arxiv.org/abs/2511.23386) | [PDF](https://arxiv.org/pdf/2511.23386.pdf)

**ä½œè€…**: Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVQRAEä»¥ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆä¸Žé‡å»ºçš„è¡¨ç¤ºï¼Œé€šè¿‡å‘é‡é‡åŒ–è‡ªç¼–ç å™¨å®žçŽ°è¿žç»­è¯­ä¹‰ç‰¹å¾ä¸Žç¦»æ•£ä»¤ç‰Œçš„èžåˆã€‚**

**å…³é”®è¯**: `å‘é‡é‡åŒ–è‡ªç¼–ç å™¨` `å¤šæ¨¡æ€ç»Ÿä¸€è¡¨ç¤º` `è¯­ä¹‰VQç æœ¬` `è§†è§‰ç†è§£ä¸Žç”Ÿæˆ` `ä¸¤é˜¶æ®µè®­ç»ƒ` `è‡ªå›žå½’æ‰©å±•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆä¸Žé‡å»ºçš„è¡¨ç¤ºåœ¨å•ä¸€æ ‡è®°å™¨ä¸­æ˜¯æŒ‘æˆ˜ï¼ŒçŽ°æœ‰æ–¹æ³•å¤šé‡‡ç”¨åŒç¼–ç å™¨èŒƒå¼ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡åž‹ï¼Œé‡‡ç”¨å¯¹ç§°ViTè§£ç å™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå­¦ä¹ é«˜ç»´è¯­ä¹‰VQç æœ¬ï¼Œå®žçŽ°è¿žç»­è¯­ä¹‰ç‰¹å¾ä¸Žç¦»æ•£ä»¤ç‰Œçš„ç»Ÿä¸€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨è§†è§‰ç†è§£ã€ç”Ÿæˆå’Œé‡å»ºåŸºå‡†ä¸Šè¡¨çŽ°ç«žäº‰æ€§ï¼Œé«˜ç»´ç æœ¬åˆ©ç”¨çŽ‡è¾¾100%ï¼Œåœ¨è‡ªå›žå½’èŒƒå¼ä¸­å±•çŽ°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.

