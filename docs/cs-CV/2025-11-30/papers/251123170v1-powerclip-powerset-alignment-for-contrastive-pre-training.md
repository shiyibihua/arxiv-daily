---
layout: default
title: PowerCLIP: Powerset Alignment for Contrastive Pre-Training
---

# PowerCLIP: Powerset Alignment for Contrastive Pre-Training

**arXiv**: [2511.23170v1](https://arxiv.org/abs/2511.23170) | [PDF](https://arxiv.org/pdf/2511.23170.pdf)

**ä½œè€…**: Masaki Kawamura, Nakamasa Inoue, Rintaro Yanagi, Hirokatsu Kataoka, Rio Yokota

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPowerCLIPæ¡†æž¶ï¼Œé€šè¿‡å¹‚é›†å¯¹é½è§£å†³å¤šå›¾åƒåŒºåŸŸç»„åˆè¯­ä¹‰æ•èŽ·é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¯¹æ¯”é¢„è®­ç»ƒ` `å¹‚é›†å¯¹é½` `é›¶æ ·æœ¬å­¦ä¹ ` `å›¾åƒ-æ–‡æœ¬å¯¹é½` `è®¡ç®—æ•ˆçŽ‡ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥æ•èŽ·è·¨å¤šä¸ªå›¾åƒåŒºåŸŸçš„ç»„åˆè¯­ä¹‰ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¹‚é›†å¯¹é½ï¼Œä¼˜åŒ–åŒºåŸŸåˆ°çŸ­è¯­çš„å…¨é¢å¯¹é½ï¼Œå¹¶ä½¿ç”¨éžçº¿æ€§èšåˆå™¨é™ä½Žè®¡ç®—å¤æ‚åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œå±•çŽ°ç»„åˆæ€§å’Œé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.

