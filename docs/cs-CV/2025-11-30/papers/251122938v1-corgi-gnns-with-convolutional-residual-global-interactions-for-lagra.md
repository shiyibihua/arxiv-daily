---
layout: default
title: CORGI: GNNs with Convolutional Residual Global Interactions for Lagrangian Simulation
---

# CORGI: GNNs with Convolutional Residual Global Interactions for Lagrangian Simulation

**arXiv**: [2511.22938v1](https://arxiv.org/abs/2511.22938) | [PDF](https://arxiv.org/pdf/2511.22938.pdf)

**ä½œè€…**: Ethan Ji, Yuanzhou Chen, Arush Ramteke, Fang Sun, Tianrun Yu, Jai Parera, Wei Wang, Yizhou Sun

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCORGIä»¥å¢žå¼ºGNNæ±‚è§£å™¨åœ¨æ‹‰æ ¼æœ—æ—¥æ¨¡æ‹Ÿä¸­çš„å…¨å±€äº¤äº’èƒ½åŠ›**

**å…³é”®è¯**: `æ‹‰æ ¼æœ—æ—¥æ¨¡æ‹Ÿ` `å›¾ç¥žç»ç½‘ç»œ` `å…¨å±€äº¤äº’` `æµä½“åŠ¨åŠ›å­¦` `å·ç§¯æ®‹å·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæ‹‰æ ¼æœ—æ—¥ç¥žç»ä»£ç†æ¨¡åž‹å› æ„Ÿå—é‡Žæœ‰é™ï¼Œéš¾ä»¥æ•æ‰æµä½“æµåŠ¨çš„å…¨å±€äº¤äº’ã€‚
2. CORGIé€šè¿‡è½»é‡æ¬§æ‹‰ç»„ä»¶ï¼Œå°†ç²’å­ç‰¹å¾æŠ•å½±åˆ°ç½‘æ ¼è¿›è¡Œå·ç§¯æ›´æ–°ï¼Œå†æ˜ å°„å›žç²’å­åŸŸã€‚
3. åœ¨GNSå’ŒSEGNNä¸Šï¼ŒCORGIæ˜¾è‘—æå‡ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½Žè®¡ç®—å¼€é”€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Partial differential equations (PDEs) are central to dynamical systems modeling, particularly in hydrodynamics, where traditional solvers often struggle with nonlinearity and computational cost. Lagrangian neural surrogates such as GNS and SEGNN have emerged as strong alternatives by learning from particle-based simulations. However, these models typically operate with limited receptive fields, making them inaccurate for capturing the inherently global interactions in fluid flows. Motivated by this observation, we introduce Convolutional Residual Global Interactions (CORGI), a hybrid architecture that augments any GNN-based solver with a lightweight Eulerian component for global context aggregation. By projecting particle features onto a grid, applying convolutional updates, and mapping them back to the particle domain, CORGI captures long-range dependencies without significant overhead. When applied to a GNS backbone, CORGI achieves a 57% improvement in rollout accuracy with only 13% more inference time and 31% more training time. Compared to SEGNN, CORGI improves accuracy by 49% while reducing inference time by 48% and training time by 30%. Even under identical runtime constraints, CORGI outperforms GNS by 47% on average, highlighting its versatility and performance on varied compute budgets.

