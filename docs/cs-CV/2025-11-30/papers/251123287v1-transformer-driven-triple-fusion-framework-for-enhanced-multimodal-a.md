---
layout: default
title: Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla
---

# Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla

**arXiv**: [2511.23287v1](https://arxiv.org/abs/2511.23287) | [PDF](https://arxiv.org/pdf/2511.23287.pdf)

**ä½œè€…**: Ariful Islam, Tanvir Mahmud, Md Rifat Hossen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽTransformerçš„ä¸‰é‡èžåˆæ¡†æž¶BangACMMï¼Œä»¥å¢žå¼ºä½Žèµ„æºå­ŸåŠ æ‹‰è¯­ç¤¾äº¤åª’ä½“ä½œè€…æ„å›¾åˆ†ç±»ã€‚**

**å…³é”®è¯**: `ä½œè€…æ„å›¾åˆ†ç±»` `å¤šæ¨¡æ€èžåˆ` `Transformeræ¨¡åž‹` `ä½Žèµ„æºè¯­è¨€` `å­ŸåŠ æ‹‰è¯­ç¤¾äº¤åª’ä½“` `ä¸­é—´èžåˆç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é’ˆå¯¹å­ŸåŠ æ‹‰è¯­ç¤¾äº¤åª’ä½“ä½œè€…æ„å›¾åˆ†ç±»ï¼Œèžåˆæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä»¥å…‹æœå•æ¨¡æ€æ–¹æ³•å±€é™ã€‚
2. å¼•å…¥ä¸­é—´èžåˆç­–ç•¥ï¼Œä¼˜äºŽæ—©æœŸå’Œæ™šæœŸèžåˆï¼Œç»“åˆmBERTå’ŒSwin Transformerå®žçŽ°æœ€ä¼˜æ€§èƒ½ã€‚
3. åœ¨Uddesshoæ•°æ®é›†ä¸Šè¾¾åˆ°84.11%å®F1åˆ†æ•°ï¼Œæ¯”å…ˆå‰æ–¹æ³•æå‡8.4ä¸ªç™¾åˆ†ç‚¹ï¼Œå»ºç«‹æ–°åŸºå‡†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).

