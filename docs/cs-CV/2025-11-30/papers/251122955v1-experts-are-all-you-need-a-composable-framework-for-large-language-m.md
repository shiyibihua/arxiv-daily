---
layout: default
title: Experts are all you need: A Composable Framework for Large Language Model Inference
---

# Experts are all you need: A Composable Framework for Large Language Model Inference

**arXiv**: [2511.22955v1](https://arxiv.org/abs/2511.22955) | [PDF](https://arxiv.org/pdf/2511.22955.pdf)

**ä½œè€…**: Shrihari Sridharan, Sourjya Roy, Anand Raghunathan, Kaushik Roy

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºComp-LLMå¯ç»„åˆæŽ¨ç†æ¡†æž¶ï¼Œé€šè¿‡å­æŸ¥è¯¢ä¾èµ–å›¾å®žçŽ°ä¸“å®¶åä½œï¼Œæå‡å¤§è¯­è¨€æ¨¡åž‹æ•ˆçŽ‡ä¸Žå‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `æ··åˆä¸“å®¶æ¨¡åž‹` `å¯ç»„åˆæ¡†æž¶` `å­æŸ¥è¯¢ä¾èµ–å›¾` `å¹¶è¡Œå¤„ç†` `å‡†ç¡®æ€§æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹è®¡ç®—è´Ÿæ‹…é‡ï¼Œæ··åˆä¸“å®¶æ¨¡åž‹éœ€è”åˆé¢„è®­ç»ƒä¸”ä¸æ”¯æŒå¤šæ­¥æŽ¨ç†ï¼Œå¤šæ™ºèƒ½ä½“æ¡†æž¶å»¶è¿Ÿé«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šComp-LLMåŒ…å«å­æŸ¥è¯¢ç”Ÿæˆå™¨ã€æŸ¥è¯¢æ‰§è¡Œå™¨å’Œå“åº”èšåˆå™¨ï¼Œæž„å»ºä¾èµ–å›¾ä»¥å¹¶è¡Œå¤„ç†å­æŸ¥è¯¢ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒComp-LLMæå‡å‡†ç¡®æ€§è¾¾11.01%ï¼Œæ¨¡åž‹å¤§å°å‡å°‘1.67xâ€“3.56xï¼Œå»¶è¿Ÿæ”¹å–„1.1xâ€“1.7xã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

