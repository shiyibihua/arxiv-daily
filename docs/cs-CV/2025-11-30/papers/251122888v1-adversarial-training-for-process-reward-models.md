---
layout: default
title: Adversarial Training for Process Reward Models
---

# Adversarial Training for Process Reward Models

**arXiv**: [2511.22888v1](https://arxiv.org/abs/2511.22888) | [PDF](https://arxiv.org/pdf/2511.22888.pdf)

**ä½œè€…**: Gurusha Juneja, Deepak Nathani, William Yang Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ä»¥è§£å†³æŽ¨ç†é”™è¯¯æ£€æµ‹çš„æ³›åŒ–é—®é¢˜**

**å…³é”®è¯**: `è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹` `å¯¹æŠ—è®­ç»ƒ` `æŽ¨ç†é”™è¯¯æ£€æµ‹` `æ•°å­¦æŽ¨ç†` `æ³›åŒ–èƒ½åŠ›` `è‡ªåŠ¨æ ‡æ³¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ä¾èµ–æ˜‚è´µäººå·¥æ ‡æ³¨ä¸”å¯¹æ–°é¢–é”™è¯¯æ³›åŒ–å·®
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç”Ÿæˆå™¨ä¸Žå¥–åŠ±æ¨¡åž‹å¯¹æŠ—è®­ç»ƒï¼Œè‡ªåŠ¨ç”Ÿæˆæ¸è¿›å¼è´Ÿæ ·æœ¬
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°å­¦æŽ¨ç†åŸºå‡†ä¸Šå¹³å‡æå‡3.4ä¸ªç™¾åˆ†ç‚¹ï¼Œåˆ†å¸ƒå¤–ä»»åŠ¡æå‡5.3ä¸ªç™¾åˆ†ç‚¹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.

