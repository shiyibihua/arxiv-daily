---
layout: default
title: MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?
---

# MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?

**arXiv**: [2511.23112v1](https://arxiv.org/abs/2511.23112) | [PDF](https://arxiv.org/pdf/2511.23112.pdf)

**ä½œè€…**: Yuandong Wang, Yao Cui, Yuxin Zhao, Zhen Yang, Yangfu Zhu, Zhenzhou Shao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMathSightåŸºå‡†ä»¥é‡åŒ–è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨å¤§å­¦æ•°å­¦æŽ¨ç†ä¸­è§†è§‰è¾“å…¥çš„çœŸå®žè´¡çŒ®**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ•°å­¦æŽ¨ç†åŸºå‡†` `å¤šæ¨¡æ€è¯„ä¼°` `è§†è§‰è´¡çŒ®é‡åŒ–` `å¤§å­¦çº§æ•°å­¦`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†æœªåˆ†ç¦»è§†è§‰æ¨¡æ€ä½œç”¨ï¼Œè§†è§‰ä¿¡æ¯è´¡çŒ®åº¦ä¸æ˜Žç¡®
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡å¤šè§†è§‰å˜ä½“ï¼ˆåŽŸå§‹ã€æ‰‹ç»˜ã€ç…§ç‰‡ï¼‰å’Œçº¯æ–‡æœ¬æ¡ä»¶è¿›è¡ŒæŽ§åˆ¶æ¯”è¾ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒæ˜¾ç¤ºè§†è§‰è´¡çŒ®éšé—®é¢˜éš¾åº¦å¢žåŠ è€Œå‡å°‘ï¼ŒQwen3-VLæ— å›¾åƒè¾“å…¥è¶…è¶Šå¤šæ¨¡æ€å˜ä½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.

