---
layout: default
title: LAHNet: Local Attentive Hashing Network for Point Cloud Registration
---

# LAHNet: Local Attentive Hashing Network for Point Cloud Registration

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00927" target="_blank" class="toolbar-btn">arXiv: 2512.00927v1</a>
    <a href="https://arxiv.org/pdf/2512.00927.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00927v1" 
            onclick="toggleFavorite(this, '2512.00927v1', 'LAHNet: Local Attentive Hashing Network for Point Cloud Registration')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Wentao Qu, Xiaoshui Huang, Liang Xiao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**LAHNetÔºöÈù¢ÂêëÁÇπ‰∫ëÈÖçÂáÜÁöÑÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÂìàÂ∏åÁΩëÁªúÔºåÊèêÂçáÁâπÂæÅÂå∫ÂàÜÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)** **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `ÁÇπ‰∫ëÈÖçÂáÜ` `Â±ÄÈÉ®Ê≥®ÊÑèÂäõ` `ÂìàÂ∏åÁΩëÁªú` `Group Transformer` `ÈïøÁ®ã‰æùËµñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÂ≠¶‰π†ÁöÑÁÇπ‰∫ëÈÖçÂáÜÊèèËø∞Â≠ê‰æßÈáç‰∫éÊÑüÁü•Â±ÄÈÉ®‰ø°ÊÅØÔºåÁâπÂæÅÂå∫ÂàÜÊÄß‰∏çË∂≥ÔºåÊÑüÂèóÈáéÂèóÈôê„ÄÇ
2. LAHNetÈÄöËøáGroup TransformerÊçïËé∑ÈïøÁ®ã‰∏ä‰∏ãÊñáÔºåÂà©Áî®Â±ÄÈÉ®ÊïèÊÑüÂìàÂ∏åËøõË°åÁ™óÂè£ÂàíÂàÜÔºåÂπ∂ÈááÁî®Ë∑®Á™óÂè£Á≠ñÁï•Êâ©Â§ßÊÑüÂèóÈáé„ÄÇ
3. LAHNetÂú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ≠¶‰π†Âà∞È≤ÅÊ£í‰∏îÂÖ∑ÊúâÂå∫ÂàÜÊÄßÁöÑÁâπÂæÅÔºåÊòæËëóÊèêÂçá‰∫ÜÁÇπ‰∫ëÈÖçÂáÜÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éÁÇπ‰∫ëÈÖçÂáÜÁöÑÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÂìàÂ∏åÁΩëÁªúÔºàLAHNetÔºâÔºåËØ•ÁΩëÁªúÂ∞ÜÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏éÂç∑ÁßØÁ±ªÁÆóÂ≠êÁöÑÂ±ÄÈÉ®ÊÄßÂΩíÁ∫≥ÂÅèÁΩÆÂºïÂÖ•Âà∞ÁÇπ‰∫ëÊèèËø∞Â≠ê‰∏≠„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåËÆæËÆ°‰∫Ü‰∏Ä‰∏™Group TransformerÊù•ÊçïËé∑ÁÇπ‰∫ë‰∏≠ÁÇπ‰πãÈó¥ÂêàÁêÜÁöÑÈïøÁ®ã‰∏ä‰∏ãÊñáÂÖ≥Á≥ª„ÄÇËØ•Ê®°ÂùóÈááÁî®Á∫øÊÄßÈÇªÂüüÊêúÁ¥¢Á≠ñÁï•ÔºåÂç≥Â±ÄÈÉ®ÊïèÊÑüÂìàÂ∏åÔºå‰ªéËÄåËÉΩÂ§üÂ∞ÜÁÇπ‰∫ëÂùáÂåÄÂú∞ÂàíÂàÜ‰∏∫ÈùûÈáçÂè†Á™óÂè£„ÄÇÂêåÊó∂ÔºåÈááÁî®È´òÊïàÁöÑË∑®Á™óÂè£Á≠ñÁï•Êù•Ëøõ‰∏ÄÊ≠•Êâ©Â§ßÂêàÁêÜÁöÑÁâπÂæÅÊÑüÂèóÈáé„ÄÇÊ≠§Â§ñÔºåÂü∫‰∫éËøôÁßçÊúâÊïàÁöÑÁ™óÂè£ÂåñÁ≠ñÁï•ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰∫§‰∫íTransformerÊù•Â¢ûÂº∫ÁÇπ‰∫ëÂØπ‰∏≠ÈáçÂè†Âå∫ÂüüÁöÑÁâπÂæÅ‰∫§‰∫íÔºåÈÄöËøáÂ∞ÜÊØè‰∏™Á™óÂè£Ë°®Á§∫‰∏∫ÂÖ®Â±Ä‰ø°Âè∑Êù•ËÆ°ÁÆóÈáçÂè†Áü©ÈòµÔºå‰ªéËÄåÂåπÈÖçÁÇπ‰∫ëÂØπ‰πãÈó¥ÁöÑÈáçÂè†Âå∫Âüü„ÄÇÂ§ßÈáèÁªìÊûúË°®ÊòéÔºåLAHNetÂèØ‰ª•Â≠¶‰π†È≤ÅÊ£í‰∏îÂÖ∑ÊúâÂå∫ÂàÜÊÄßÁöÑÁâπÂæÅÔºåÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÂÆ§ÂÜÖÂíåÂÆ§Â§ñÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÈÖçÂáÜÁªìÊûú„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂü∫‰∫éÂ≠¶‰π†ÁöÑÁÇπ‰∫ëÈÖçÂáÜÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Â±ÄÈÉ®‰ø°ÊÅØÁöÑÊèêÂèñÔºåÂøΩÁï•‰∫ÜÁÇπ‰∫ë‰∏≠ÁÇπ‰∏éÁÇπ‰πãÈó¥ÁöÑÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÔºåÂØºËá¥ÁâπÂæÅÁöÑÂå∫ÂàÜÊÄß‰∏çË∂≥ÔºåÈöæ‰ª•Â∫îÂØπÂ§çÊùÇÁöÑÂú∫ÊôØ„ÄÇÊ≠§Â§ñÔºåÊÑüÂèóÈáéÁöÑÈôêÂà∂‰πüÈòªÁ¢ç‰∫ÜÊ®°ÂûãÂØπÂÖ®Â±Ä‰ø°ÊÅØÁöÑÁêÜËß£ÔºåÂΩ±Âìç‰∫ÜÈÖçÂáÜÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLAHNetÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•Â±ÄÈÉ®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂπ∂ÁªìÂêàÂ±ÄÈÉ®ÊïèÊÑüÂìàÂ∏åÔºàLSHÔºâËøõË°åÁ™óÂè£ÂàíÂàÜÔºå‰ªéËÄåÂú®Â±ÄÈÉ®ËåÉÂõ¥ÂÜÖÊçïËé∑ÈïøÁ®ã‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂπ∂Êâ©Â§ßÁâπÂæÅÁöÑÊÑüÂèóÈáé„ÄÇÈÄöËøáGroup TransformerÂíåInteraction TransformerÔºåÂ¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅÁöÑË°®ËææËÉΩÂäõÂíåÂå∫ÂàÜÊÄßÔºåÊúÄÁªàÊèêÂçáÁÇπ‰∫ëÈÖçÂáÜÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLAHNetÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Group TransformerÔºöÂà©Áî®LSHÂ∞ÜÁÇπ‰∫ëÂàíÂàÜ‰∏∫ÈùûÈáçÂè†Á™óÂè£ÔºåÂπ∂Âú®ÊØè‰∏™Á™óÂè£ÂÜÖ‰ΩøÁî®TransformerÁªìÊûÑÊçïËé∑ÈïøÁ®ã‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ2) Ë∑®Á™óÂè£Á≠ñÁï•ÔºöÈÄöËøáÈ´òÊïàÁöÑË∑®Á™óÂè£Êìç‰ΩúÔºåËøõ‰∏ÄÊ≠•Êâ©Â§ßÁâπÂæÅÁöÑÊÑüÂèóÈáéÔºåÂ¢ûÂº∫ÂÖ®Â±Ä‰ø°ÊÅØÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ3) Interaction TransformerÔºöËÆ°ÁÆóÁÇπ‰∫ëÂØπ‰πãÈó¥ÁöÑÈáçÂè†Áü©ÈòµÔºåÂπ∂Âà©Áî®TransformerÁªìÊûÑÂ¢ûÂº∫ÈáçÂè†Âå∫ÂüüÁöÑÁâπÂæÅ‰∫§‰∫íÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑÈÖçÂáÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLAHNetÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏éLSHÁõ∏ÁªìÂêàÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁÇπ‰∫ëÁâπÂæÅÊèêÂèñÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂç∑ÁßØÊàñPointNetÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåLAHNetËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïËé∑ÁÇπ‰∫ë‰∏≠ÁöÑÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂Êâ©Â§ßÁâπÂæÅÁöÑÊÑüÂèóÈáé„ÄÇÊ≠§Â§ñÔºåInteraction TransformerÁöÑËÆæËÆ°‰πüÊúâÊïàÂú∞Â¢ûÂº∫‰∫ÜÁÇπ‰∫ëÂØπ‰πãÈó¥ÁöÑÁâπÂæÅ‰∫§‰∫íÔºåÊèêÂçá‰∫ÜÈÖçÂáÜÁöÑÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLAHNetÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) LSHÁöÑÂèÇÊï∞ËÆæÁΩÆÔºöÈÄâÊã©ÂêàÈÄÇÁöÑÂìàÂ∏åÂáΩÊï∞ÂíåÂìàÂ∏åË°®Êï∞ÈáèÔºå‰ª•‰øùËØÅÁ™óÂè£ÂàíÂàÜÁöÑÂùáÂåÄÊÄßÂíåÊïàÁéá„ÄÇ2) Group TransformerÁöÑÁªìÊûÑÔºöÈááÁî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÊÆãÂ∑ÆËøûÊé•Ôºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑË°®ËææËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ3) Ë∑®Á™óÂè£Á≠ñÁï•ÁöÑÂÆûÁé∞ÔºöËÆæËÆ°È´òÊïàÁöÑË∑®Á™óÂè£Êìç‰ΩúÔºå‰ª•ÂáèÂ∞ëËÆ°ÁÆóÈáèÂíåÂÜÖÂ≠òÊ∂àËÄó„ÄÇ4) Interaction TransformerÁöÑÊçüÂ§±ÂáΩÊï∞ÔºöËÆæËÆ°ÂêàÈÄÇÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•ÊåáÂØºÊ®°ÂûãÂ≠¶‰π†Êõ¥ÂÖ∑ÊúâÂå∫ÂàÜÊÄßÁöÑÁâπÂæÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LAHNetÂú®Â§ö‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÂÆ§ÂÜÖÂíåÂÆ§Â§ñÁÇπ‰∫ëÈÖçÂáÜÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊàêÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLAHNetËÉΩÂ§üÂ≠¶‰π†Âà∞È≤ÅÊ£í‰∏îÂÖ∑ÊúâÂå∫ÂàÜÊÄßÁöÑÁâπÂæÅÔºåÁõ∏ÊØî‰∫éÁé∞ÊúâÁöÑÊñπÊ≥ïÔºåÂú®ÈÖçÂáÜÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÈÉΩÊúâÊòæËëóÁöÑÊèêÂçá„ÄÇÂÖ∑‰ΩìÁöÑÊï∞ÊçÆÊèêÂçáÂπÖÂ∫¶Êú™Áü•Ôºå‰ΩÜÊëòË¶ÅÂº∫Ë∞É‰∫Ü‚Äúsignificant registration results‚Äù„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LAHNetÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅ‰∏âÁª¥ÈáçÂª∫„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁ≤æÁ°ÆÂú∞ÈÖçÂáÜ‰∏çÂêåËßÜËßíÁöÑÁÇπ‰∫ëÊï∞ÊçÆÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑÁéØÂ¢ÉÊÑüÁü•ÂíåÂÆö‰Ωç„ÄÇÊ≠§Â§ñÔºåLAHNetËøòÂèØ‰ª•Â∫îÁî®‰∫éÊñáÁâ©‰øùÊä§„ÄÅÂåªÁñóÂΩ±ÂÉèÂàÜÊûêÁ≠âÈ¢ÜÂüüÔºå‰∏∫Áõ∏ÂÖ≥Á†îÁ©∂Êèê‰æõÊäÄÊúØÊîØÊåÅ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.

