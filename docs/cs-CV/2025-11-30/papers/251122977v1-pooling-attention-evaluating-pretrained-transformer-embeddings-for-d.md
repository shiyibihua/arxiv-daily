---
layout: default
title: Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification
---

# Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification

**arXiv**: [2511.22977v1](https://arxiv.org/abs/2511.22977) | [PDF](https://arxiv.org/pdf/2511.22977.pdf)

**ä½œè€…**: Sumit Mamtani, Abhijeet Bhure

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°é¢„è®­ç»ƒTransformeråµŒå…¥ç”¨äºŽæ¬ºéª—åˆ†ç±»ï¼Œé€šè¿‡æ± åŒ–æ³¨æ„åŠ›æ–¹æ³•éªŒè¯BERTç­‰æ¨¡åž‹ä½œä¸ºç¨³å¥åŸºç¡€ã€‚**

**å…³é”®è¯**: `å‡æ–°é—»æ£€æµ‹` `TransformeråµŒå…¥` `æ± åŒ–æ³¨æ„åŠ›` `æ¬ºéª—åˆ†ç±»` `é¢„è®­ç»ƒæ¨¡åž‹è¯„ä¼°` `è½»é‡çº§åˆ†ç±»å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç ”ç©¶å‡æ–°é—»æ£€æµ‹ä½œä¸ºTransformerè¡¨ç¤ºçš„ä¸‹æ¸¸è¯„ä¼°ä»»åŠ¡ï¼Œèšç„¦æ¬ºéª—åˆ†ç±»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å†»ç»“çš„ç¼–ç å™¨-ä»…å’Œè§£ç å™¨-ä»…é¢„è®­ç»ƒæ¨¡åž‹ï¼ˆå¦‚BERTã€GPT-2ï¼‰ä½œä¸ºåµŒå…¥å™¨ï¼Œç»“åˆè½»é‡çº§åˆ†ç±»å™¨ï¼Œæ¯”è¾ƒæ± åŒ–ä¸Žå¡«å……ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LIARæ•°æ®é›†ä¸Šï¼ŒBERTåµŒå…¥ä¸Žé€»è¾‘å›žå½’ç»„åˆä¼˜äºŽç¥žç»ç½‘ç»œåŸºçº¿ï¼Œæ± åŒ–æ–¹æ³•å¯¹æˆªæ–­é²æ£’ä¸”æœ‰æ•ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.

