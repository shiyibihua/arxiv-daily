---
layout: default
title: Silhouette-based Gait Foundation Model
---

# Silhouette-based Gait Foundation Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.00691" target="_blank" class="toolbar-btn">arXiv: 2512.00691v1</a>
    <a href="https://arxiv.org/pdf/2512.00691.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00691v1" 
            onclick="toggleFavorite(this, '2512.00691v1', 'Silhouette-based Gait Foundation Model')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Dingqiang Ye, Chao Fan, Kartik Narayan, Bingzhe Wu, Chengwen Luo, Jianqiang Li, Vishal M. Patel

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ShiqiYu/OpenGait)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFoundationGaitï¼Œé¦–ä¸ªå¯æ‰©å±•çš„æ­¥æ€è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œæå‡å¤šç§æ­¥æ€ä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ­¥æ€è¯†åˆ«` `åŸºç¡€æ¨¡å‹` `è‡ªç›‘ç£å­¦ä¹ ` `é¢„è®­ç»ƒ` `Transformer` `äººä½“è¯†åˆ«` `å¥åº·åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ­¥æ€æ¨¡å‹éš¾ä»¥æ‰©å±•å’Œæ³›åŒ–ï¼Œæ— æ³•æ»¡è¶³å¤šæ ·åŒ–çš„æ­¥æ€åˆ†æä»»åŠ¡éœ€æ±‚ã€‚
2. æå‡ºFoundationGaitï¼Œé€šè¿‡å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ é€šç”¨çš„æ­¥æ€è¡¨å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFoundationGaitåœ¨å¤šç§æ­¥æ€ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬è¯†åˆ«ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ­¥æ€æ¨¡å¼åœ¨äººä½“è¯†åˆ«å’Œå¥åº·åˆ†æä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å½“å‰è¿›å±•å—åˆ°å°å‹ã€è®¾è®¡ç‹­éš˜çš„æ¨¡å‹é™åˆ¶ï¼Œè¿™äº›æ¨¡å‹æ— æ³•æ‰©å±•æˆ–æ³›åŒ–ã€‚æ„å»ºç»Ÿä¸€çš„æ­¥æ€åŸºç¡€æ¨¡å‹éœ€è¦è§£å†³ä¸¤ä¸ªé•¿æœŸå­˜åœ¨çš„éšœç¢ï¼šï¼ˆaï¼‰å¯æ‰©å±•æ€§ã€‚ä¸ºä»€ä¹ˆæ­¥æ€æ¨¡å‹åœ¨å†å²ä¸Šæœªèƒ½éµå¾ªç¼©æ”¾å®šå¾‹ï¼Ÿï¼ˆbï¼‰æ³›åŒ–æ€§ã€‚ä¸€ä¸ªæ¨¡å‹èƒ½å¦æœåŠ¡äºä¼ ç»Ÿä¸Šå­¤ç«‹ç ”ç©¶çš„å„ç§æ­¥æ€ä»»åŠ¡ï¼Ÿæˆ‘ä»¬ä»‹ç»äº†FoundationGaitï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯æ‰©å±•çš„ã€ç”¨äºæ­¥æ€ç†è§£çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ã€‚å…¶æœ€å¤§ç‰ˆæœ¬æ‹¥æœ‰è¿‘0.13äº¿ä¸ªå‚æ•°ï¼Œå¹¶åœ¨åŒ…å«è¶…è¿‡200ä¸‡ä¸ªè¡Œèµ°åºåˆ—çš„12ä¸ªå…¬å…±æ­¥æ€æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFoundationGaitæ— è®ºæ˜¯å¦ç»è¿‡å¾®è°ƒï¼Œéƒ½èƒ½åœ¨å„ç§æ­¥æ€æ•°æ®é›†ã€æ¡ä»¶ã€ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œäººä½“è¯†åˆ«ã€è„ŠæŸ±ä¾§å¼¯ç­›æŸ¥ã€æŠ‘éƒç—‡é¢„æµ‹å’Œå±æ€§ä¼°è®¡ï¼‰ç”šè‡³è¾“å…¥æ¨¡æ€ä¸Šè¡¨ç°å‡ºè‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é‡å¤–Gait3Dæ•°æ®é›†ï¼ˆ1,000ä¸ªæµ‹è¯•å¯¹è±¡ï¼‰ä¸Šå®ç°äº†48.0%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®ç‡ï¼Œåœ¨æœ€å¤§çš„å®éªŒå®¤OU-MVLPæ•°æ®é›†ï¼ˆ5,000+ä¸ªæµ‹è¯•å¯¹è±¡ï¼‰ä¸Šå®ç°äº†64.5%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®ç‡ï¼Œä¸ºé²æ£’æ­¥æ€è¯†åˆ«æ ‘ç«‹äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚ä»£ç å’Œæ¨¡å‹å³å°†å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ­¥æ€è¯†åˆ«æ¨¡å‹é€šå¸¸è§„æ¨¡è¾ƒå°ï¼Œé’ˆå¯¹ç‰¹å®šæ•°æ®é›†å’Œä»»åŠ¡è®¾è®¡ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥é€‚åº”çœŸå®åœºæ™¯ä¸­å¤æ‚å¤šå˜çš„æ­¥æ€æ•°æ®ã€‚ç¼ºä¹ä¸€ä¸ªé€šç”¨çš„ã€å¯æ‰©å±•çš„æ­¥æ€åŸºç¡€æ¨¡å‹ï¼Œé˜»ç¢äº†æ­¥æ€åˆ†ææŠ€æœ¯çš„å‘å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤§è§„æ¨¡çš„è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ ä¸€ä¸ªé€šç”¨çš„æ­¥æ€è¡¨å¾ã€‚é€šè¿‡åœ¨å¤§é‡æ­¥æ€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ­¥æ€æ•°æ®çš„å†…åœ¨ç»“æ„å’Œç‰¹å¾ï¼Œä»è€Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ­¥æ€è¯†åˆ«é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºå’Œæ³›åŒ–æ€§é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFoundationGaitçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è‡ªç›‘ç£é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œå¯¹æ”¶é›†åˆ°çš„æ­¥æ€æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚æå–è½®å»“ä¿¡æ¯ã€‚ç„¶åï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå­¦ä¹ æ­¥æ€è¡¨å¾ã€‚æœ€åï¼Œå°†é¢„è®­ç»ƒå¥½çš„æ¨¡å‹è¿ç§»åˆ°å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚äººä½“è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ç­‰ï¼Œå¹¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç¬¬ä¸€ä¸ªå¯æ‰©å±•çš„æ­¥æ€åŸºç¡€æ¨¡å‹FoundationGaitï¼Œå¹¶é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ä»¥å¾€çš„æ­¥æ€è¯†åˆ«æ¨¡å‹ç›¸æ¯”ï¼ŒFoundationGaitå…·æœ‰æ›´å¤§çš„è§„æ¨¡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§æ­¥æ€æ•°æ®é›†ã€æ¡ä»¶å’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†æ­¥æ€æ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œä¸ºæœªæ¥æ­¥æ€æ¨¡å‹çš„è®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šFoundationGaitä½¿ç”¨äº†Transformeræ¶æ„ä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œç§°ä¸ºâ€œmasked gait modelingâ€ã€‚è¯¥ä»»åŠ¡ç±»ä¼¼äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„masked language modelingï¼Œé€šè¿‡éšæœºé®è”½éƒ¨åˆ†æ­¥æ€åºåˆ—ï¼Œå¹¶è®©æ¨¡å‹é¢„æµ‹è¢«é®è”½çš„éƒ¨åˆ†ï¼Œä»è€Œå­¦ä¹ æ­¥æ€æ•°æ®çš„å†…åœ¨ç»“æ„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†å¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºè£å‰ªã€æ—‹è½¬ç­‰ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FoundationGaitåœ¨å¤šä¸ªæ­¥æ€æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é‡å¤–Gait3Dæ•°æ®é›†ä¸Šï¼Œå®ç°äº†48.0%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®ç‡ï¼Œåœ¨æœ€å¤§çš„å®éªŒå®¤OU-MVLPæ•°æ®é›†ä¸Šå®ç°äº†64.5%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFoundationGaitå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§å¤æ‚çš„æ­¥æ€åœºæ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FoundationGaitå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå®‰å…¨ç›‘æ§ï¼ˆæ­¥æ€è¯†åˆ«ï¼‰ã€åŒ»ç–—å¥åº·ï¼ˆç–¾ç—…è¯Šæ–­ã€åº·å¤è¯„ä¼°ï¼‰ã€äººæœºäº¤äº’ï¼ˆæ­¥æ€æ§åˆ¶ï¼‰ã€è¿åŠ¨åˆ†æç­‰ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æ­¥æ€åˆ†ææŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ä¸ºäººä»¬çš„ç”Ÿæ´»å¸¦æ¥ä¾¿åˆ©å’Œæ”¹å–„ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.

