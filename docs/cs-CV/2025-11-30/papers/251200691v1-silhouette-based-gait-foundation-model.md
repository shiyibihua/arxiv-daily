---
layout: default
title: Silhouette-based Gait Foundation Model
---

# Silhouette-based Gait Foundation Model

**arXiv**: [2512.00691v1](https://arxiv.org/abs/2512.00691) | [PDF](https://arxiv.org/pdf/2512.00691.pdf)

**ä½œè€…**: Dingqiang Ye, Chao Fan, Kartik Narayan, Bingzhe Wu, Chengwen Luo, Jianqiang Li, Vishal M. Patel

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ShiqiYu/OpenGait)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFoundationGaitï¼Œé¦–ä¸ªå¯æ‰©å±•çš„æ­¥æ€è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æž¶ï¼Œæå‡å¤šç§æ­¥æ€ä»»åŠ¡æ€§èƒ½ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ­¥æ€è¯†åˆ«` `åŸºç¡€æ¨¡åž‹` `è‡ªç›‘ç£å­¦ä¹ ` `é¢„è®­ç»ƒ` `Transformer` `äººä½“è¯†åˆ«` `å¥åº·åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ­¥æ€æ¨¡åž‹éš¾ä»¥æ‰©å±•å’Œæ³›åŒ–ï¼Œæ— æ³•æ»¡è¶³å¤šæ ·åŒ–çš„æ­¥æ€åˆ†æžä»»åŠ¡éœ€æ±‚ã€‚
2. æå‡ºFoundationGaitï¼Œé€šè¿‡å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ é€šç”¨çš„æ­¥æ€è¡¨å¾ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒFoundationGaitåœ¨å¤šç§æ­¥æ€ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬è¯†åˆ«ç²¾åº¦ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ­¥æ€æ¨¡å¼åœ¨äººä½“è¯†åˆ«å’Œå¥åº·åˆ†æžä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å½“å‰è¿›å±•å—åˆ°å°åž‹ã€è®¾è®¡ç‹­éš˜çš„æ¨¡åž‹é™åˆ¶ï¼Œè¿™äº›æ¨¡åž‹æ— æ³•æ‰©å±•æˆ–æ³›åŒ–ã€‚æž„å»ºç»Ÿä¸€çš„æ­¥æ€åŸºç¡€æ¨¡åž‹éœ€è¦è§£å†³ä¸¤ä¸ªé•¿æœŸå­˜åœ¨çš„éšœç¢ï¼šï¼ˆaï¼‰å¯æ‰©å±•æ€§ã€‚ä¸ºä»€ä¹ˆæ­¥æ€æ¨¡åž‹åœ¨åŽ†å²ä¸Šæœªèƒ½éµå¾ªç¼©æ”¾å®šå¾‹ï¼Ÿï¼ˆbï¼‰æ³›åŒ–æ€§ã€‚ä¸€ä¸ªæ¨¡åž‹èƒ½å¦æœåŠ¡äºŽä¼ ç»Ÿä¸Šå­¤ç«‹ç ”ç©¶çš„å„ç§æ­¥æ€ä»»åŠ¡ï¼Ÿæˆ‘ä»¬ä»‹ç»äº†FoundationGaitï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯æ‰©å±•çš„ã€ç”¨äºŽæ­¥æ€ç†è§£çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æž¶ã€‚å…¶æœ€å¤§ç‰ˆæœ¬æ‹¥æœ‰è¿‘0.13äº¿ä¸ªå‚æ•°ï¼Œå¹¶åœ¨åŒ…å«è¶…è¿‡200ä¸‡ä¸ªè¡Œèµ°åºåˆ—çš„12ä¸ªå…¬å…±æ­¥æ€æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒFoundationGaitæ— è®ºæ˜¯å¦ç»è¿‡å¾®è°ƒï¼Œéƒ½èƒ½åœ¨å„ç§æ­¥æ€æ•°æ®é›†ã€æ¡ä»¶ã€ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œäººä½“è¯†åˆ«ã€è„ŠæŸ±ä¾§å¼¯ç­›æŸ¥ã€æŠ‘éƒç—‡é¢„æµ‹å’Œå±žæ€§ä¼°è®¡ï¼‰ç”šè‡³è¾“å…¥æ¨¡æ€ä¸Šè¡¨çŽ°å‡ºè‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é‡Žå¤–Gait3Dæ•°æ®é›†ï¼ˆ1,000ä¸ªæµ‹è¯•å¯¹è±¡ï¼‰ä¸Šå®žçŽ°äº†48.0%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®çŽ‡ï¼Œåœ¨æœ€å¤§çš„å®žéªŒå®¤OU-MVLPæ•°æ®é›†ï¼ˆ5,000+ä¸ªæµ‹è¯•å¯¹è±¡ï¼‰ä¸Šå®žçŽ°äº†64.5%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®çŽ‡ï¼Œä¸ºé²æ£’æ­¥æ€è¯†åˆ«æ ‘ç«‹äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚ä»£ç å’Œæ¨¡åž‹å³å°†å‘å¸ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ­¥æ€è¯†åˆ«æ¨¡åž‹é€šå¸¸è§„æ¨¡è¾ƒå°ï¼Œé’ˆå¯¹ç‰¹å®šæ•°æ®é›†å’Œä»»åŠ¡è®¾è®¡ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥é€‚åº”çœŸå®žåœºæ™¯ä¸­å¤æ‚å¤šå˜çš„æ­¥æ€æ•°æ®ã€‚ç¼ºä¹ä¸€ä¸ªé€šç”¨çš„ã€å¯æ‰©å±•çš„æ­¥æ€åŸºç¡€æ¨¡åž‹ï¼Œé˜»ç¢äº†æ­¥æ€åˆ†æžæŠ€æœ¯çš„å‘å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤§è§„æ¨¡çš„è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ ä¸€ä¸ªé€šç”¨çš„æ­¥æ€è¡¨å¾ã€‚é€šè¿‡åœ¨å¤§é‡æ­¥æ€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¨¡åž‹å¯ä»¥å­¦ä¹ åˆ°æ­¥æ€æ•°æ®çš„å†…åœ¨ç»“æž„å’Œç‰¹å¾ï¼Œä»Žè€Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºŽè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ï¼Œæ—¨åœ¨è§£å†³æ­¥æ€è¯†åˆ«é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºå’Œæ³›åŒ–æ€§é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šFoundationGaitçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è‡ªç›‘ç£é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œå¯¹æ”¶é›†åˆ°çš„æ­¥æ€æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚æå–è½®å»“ä¿¡æ¯ã€‚ç„¶åŽï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡åž‹ï¼Œå­¦ä¹ æ­¥æ€è¡¨å¾ã€‚æœ€åŽï¼Œå°†é¢„è®­ç»ƒå¥½çš„æ¨¡åž‹è¿ç§»åˆ°å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚äººä½“è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ç­‰ï¼Œå¹¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†ç¬¬ä¸€ä¸ªå¯æ‰©å±•çš„æ­¥æ€åŸºç¡€æ¨¡åž‹FoundationGaitï¼Œå¹¶é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸Žä»¥å¾€çš„æ­¥æ€è¯†åˆ«æ¨¡åž‹ç›¸æ¯”ï¼ŒFoundationGaitå…·æœ‰æ›´å¤§çš„è§„æ¨¡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§æ­¥æ€æ•°æ®é›†ã€æ¡ä»¶å’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æŽ¢ç´¢äº†æ­¥æ€æ¨¡åž‹çš„ç¼©æ”¾è§„å¾‹ï¼Œä¸ºæœªæ¥æ­¥æ€æ¨¡åž‹çš„è®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šFoundationGaitä½¿ç”¨äº†Transformeræž¶æž„ä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œç§°ä¸ºâ€œmasked gait modelingâ€ã€‚è¯¥ä»»åŠ¡ç±»ä¼¼äºŽè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„masked language modelingï¼Œé€šè¿‡éšæœºé®è”½éƒ¨åˆ†æ­¥æ€åºåˆ—ï¼Œå¹¶è®©æ¨¡åž‹é¢„æµ‹è¢«é®è”½çš„éƒ¨åˆ†ï¼Œä»Žè€Œå­¦ä¹ æ­¥æ€æ•°æ®çš„å†…åœ¨ç»“æž„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†å¤šç§æ•°æ®å¢žå¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºè£å‰ªã€æ—‹è½¬ç­‰ï¼Œä»¥æé«˜æ¨¡åž‹çš„é²æ£’æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

FoundationGaitåœ¨å¤šä¸ªæ­¥æ€æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é‡Žå¤–Gait3Dæ•°æ®é›†ä¸Šï¼Œå®žçŽ°äº†48.0%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®çŽ‡ï¼Œåœ¨æœ€å¤§çš„å®žéªŒå®¤OU-MVLPæ•°æ®é›†ä¸Šå®žçŽ°äº†64.5%çš„é›¶æ ·æœ¬rank-1å‡†ç¡®çŽ‡ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒFoundationGaitå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§å¤æ‚çš„æ­¥æ€åœºæ™¯ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

FoundationGaitå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºŽï¼šå®‰å…¨ç›‘æŽ§ï¼ˆæ­¥æ€è¯†åˆ«ï¼‰ã€åŒ»ç–—å¥åº·ï¼ˆç–¾ç—…è¯Šæ–­ã€åº·å¤è¯„ä¼°ï¼‰ã€äººæœºäº¤äº’ï¼ˆæ­¥æ€æŽ§åˆ¶ï¼‰ã€è¿åŠ¨åˆ†æžç­‰ã€‚è¯¥ç ”ç©¶æœ‰æœ›æŽ¨åŠ¨æ­¥æ€åˆ†æžæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ä¸ºäººä»¬çš„ç”Ÿæ´»å¸¦æ¥ä¾¿åˆ©å’Œæ”¹å–„ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.

