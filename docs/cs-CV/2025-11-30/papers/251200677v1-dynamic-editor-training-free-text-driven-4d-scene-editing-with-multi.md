---
layout: default
title: Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer
---

# Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer

**arXiv**: [2512.00677v1](https://arxiv.org/abs/2512.00677) | [PDF](https://arxiv.org/pdf/2512.00677.pdf)

**ä½œè€…**: Dong In Lee, Hyungjun Doh, Seunggeun Chi, Runlin Duan, Sangpil Kim, Karthik Ramani

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: 4D Scene Editing

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://di-lee.github.io/dynamic-eDiTor/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Dynamic-eDiTorï¼šåŸºäºŽå¤šæ¨¡æ€æ‰©æ•£Transformerçš„å…è®­ç»ƒæ–‡æœ¬é©±åŠ¨4Dåœºæ™¯ç¼–è¾‘**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `4Dåœºæ™¯ç¼–è¾‘` `æ–‡æœ¬é©±åŠ¨` `å¤šæ¨¡æ€æ‰©æ•£Transformer` `4Dé«˜æ–¯æº…å°„` `æ—¶ç©ºä¸€è‡´æ€§` `å…è®­ç»ƒ` `åŠ¨æ€NeRF`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–‡æœ¬é©±åŠ¨4Dåœºæ™¯ç¼–è¾‘æ–¹æ³•éš¾ä»¥ä¿è¯ç¼–è¾‘è¿‡ç¨‹ä¸­è·¨è§†è§’å’Œæ—¶é—´çš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´è¿åŠ¨æ‰­æ›²å’Œå‡ ä½•æ¼‚ç§»ã€‚
2. Dynamic-eDiToråˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£Transformerå’Œ4DGSï¼Œé€šè¿‡æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶å’Œä¸Šä¸‹æ–‡ä»¤ç‰Œä¼ æ’­å®žçŽ°ä¸€è‡´æ€§ç¼–è¾‘ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒDynamic-eDiToråœ¨DyNeRFæ•°æ®é›†ä¸Šå®žçŽ°äº†æ›´é«˜çš„ç¼–è¾‘ä¿çœŸåº¦ä»¥åŠå¤šè§†è§’å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºDynamic-eDiTorï¼Œä¸€ä¸ªå…è®­ç»ƒçš„æ–‡æœ¬é©±åŠ¨4Dç¼–è¾‘æ¡†æž¶ï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£Transformer (MM-DiT) å’Œ4Dé«˜æ–¯æº…å°„ (4DGS) å®žçŽ°åŠ¨æ€4Dåœºæ™¯ç¼–è¾‘ã€‚ç”±äºŽç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­è·¨ç©ºé—´å’Œæ—¶é—´çš„å¤šè§†è§’å’Œæ—¶é—´ä¸€è‡´æ€§å­˜åœ¨æŒ‘æˆ˜ï¼Œæ–‡æœ¬é©±åŠ¨çš„4Dåœºæ™¯ç¼–è¾‘ä»æœ‰å¾…æŽ¢ç´¢ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–äºŽç‹¬ç«‹ç¼–è¾‘å¸§çš„2Dæ‰©æ•£æ¨¡åž‹ï¼Œå¸¸å¯¼è‡´è¿åŠ¨æ‰­æ›²ã€å‡ ä½•æ¼‚ç§»å’Œä¸å®Œæ•´çš„ç¼–è¾‘ã€‚Dynamic-eDiToråŒ…å«æ—¶ç©ºå­ç½‘æ ¼æ³¨æ„åŠ›(STGA)ï¼Œç”¨äºŽå±€éƒ¨ä¸€è‡´çš„è·¨è§†è§’å’Œæ—¶é—´èžåˆï¼Œä»¥åŠä¸Šä¸‹æ–‡ä»¤ç‰Œä¼ æ’­(CTP)ï¼Œç”¨äºŽé€šè¿‡ä»¤ç‰Œç»§æ‰¿å’Œå…‰æµå¼•å¯¼çš„ä»¤ç‰Œæ›¿æ¢è¿›è¡Œå…¨å±€ä¼ æ’­ã€‚è¿™äº›ç»„ä»¶ä½¿Dynamic-eDiTorèƒ½å¤Ÿæ‰§è¡Œæ— ç¼ã€å…¨å±€ä¸€è‡´çš„å¤šè§†è§’è§†é¢‘ç¼–è¾‘ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¹¶ç›´æŽ¥ä¼˜åŒ–é¢„è®­ç»ƒçš„æº4DGSã€‚åœ¨å¤šè§†è§’è§†é¢‘æ•°æ®é›†DyNeRFä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®žçŽ°äº†ä¼˜äºŽçŽ°æœ‰æ–¹æ³•çš„ç¼–è¾‘ä¿çœŸåº¦ä»¥åŠå¤šè§†è§’å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬é©±åŠ¨çš„åŠ¨æ€4Dåœºæ™¯ç¼–è¾‘é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽ2Dæ‰©æ•£æ¨¡åž‹çš„æ–¹æ³•ï¼Œåœ¨ç‹¬ç«‹ç¼–è¾‘è§†é¢‘å¸§æ—¶ï¼Œéš¾ä»¥ä¿è¯ç¼–è¾‘ç»“æžœåœ¨ä¸åŒè§†è§’å’Œæ—¶é—´ä¸Šçš„è¿žç»­æ€§å’Œä¸€è‡´æ€§ï¼Œå¯¼è‡´è¿åŠ¨æ‰­æ›²ã€å‡ ä½•æ¼‚ç§»ä»¥åŠç¼–è¾‘ä¸å®Œæ•´ç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†4Dåœºæ™¯ç¼–è¾‘çš„å®žç”¨æ€§å’ŒçœŸå®žæ„Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDynamic-eDiTorçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£Transformer (MM-DiT) çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ç»“åˆ4Dé«˜æ–¯æº…å°„ (4DGS) çš„é«˜æ•ˆæ¸²æŸ“ç‰¹æ€§ï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®žçŽ°å¯¹4Dåœºæ™¯çš„æ–‡æœ¬é©±åŠ¨ç¼–è¾‘ã€‚é€šè¿‡å¼•å…¥æ—¶ç©ºå­ç½‘æ ¼æ³¨æ„åŠ› (STGA) å’Œä¸Šä¸‹æ–‡ä»¤ç‰Œä¼ æ’­ (CTP) æœºåˆ¶ï¼Œç¡®ä¿ç¼–è¾‘ç»“æžœåœ¨æ—¶é—´å’Œç©ºé—´ä¸Šçš„å±€éƒ¨å’Œå…¨å±€ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šDynamic-eDiTorçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„4DGSé‡å»ºåŠ¨æ€åœºæ™¯ã€‚2) å°†åœºæ™¯æŠ•å½±åˆ°å¤šä¸ªè§†è§’ï¼Œç”Ÿæˆå¤šè§†è§’å›¾åƒåºåˆ—ã€‚3) ä½¿ç”¨æ–‡æœ¬æç¤ºå’ŒMM-DiTå¯¹å›¾åƒåºåˆ—è¿›è¡Œç¼–è¾‘ï¼Œå…¶ä¸­STGAè´Ÿè´£å±€éƒ¨ä¸€è‡´æ€§ï¼ŒCTPè´Ÿè´£å…¨å±€ä¸€è‡´æ€§ã€‚4) å°†ç¼–è¾‘åŽçš„å›¾åƒåºåˆ—åæŠ•å½±å›ž4Dç©ºé—´ï¼Œæ›´æ–°4DGSå‚æ•°ï¼Œå¾—åˆ°ç¼–è¾‘åŽçš„åŠ¨æ€åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šDynamic-eDiTorçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„4Dåœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼Œé¿å…äº†è€—æ—¶çš„è®­ç»ƒè¿‡ç¨‹ã€‚2) å¼•å…¥äº†æ—¶ç©ºå­ç½‘æ ¼æ³¨æ„åŠ› (STGA) æœºåˆ¶ï¼Œç”¨äºŽåœ¨å±€éƒ¨èŒƒå›´å†…èžåˆè·¨è§†è§’å’Œæ—¶é—´çš„ä¿¡æ¯ï¼Œä¿è¯ç¼–è¾‘ç»“æžœçš„å±€éƒ¨ä¸€è‡´æ€§ã€‚3) æå‡ºäº†ä¸Šä¸‹æ–‡ä»¤ç‰Œä¼ æ’­ (CTP) æœºåˆ¶ï¼Œé€šè¿‡ä»¤ç‰Œç»§æ‰¿å’Œå…‰æµå¼•å¯¼çš„ä»¤ç‰Œæ›¿æ¢ï¼Œå®žçŽ°å…¨å±€èŒƒå›´å†…çš„ä¿¡æ¯ä¼ æ’­ï¼Œä¿è¯ç¼–è¾‘ç»“æžœçš„å…¨å±€ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSTGAå°†å›¾åƒåˆ’åˆ†ä¸ºå­ç½‘æ ¼ï¼Œå¹¶åœ¨æ¯ä¸ªå­ç½‘æ ¼å†…è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œä»Žè€Œæ•æ‰å±€éƒ¨æ—¶ç©ºå…³ç³»ã€‚CTPåˆ©ç”¨å…‰æµä¼°è®¡ç›¸é‚»å¸§ä¹‹é—´çš„è¿åŠ¨ä¿¡æ¯ï¼Œå°†ä¸Šä¸‹æ–‡ä¿¡æ¯ä»Žä¸€å¸§ä¼ é€’åˆ°å¦ä¸€å¸§ï¼Œä»Žè€Œå®žçŽ°å…¨å±€ä¸€è‡´æ€§ã€‚æŸå¤±å‡½æ•°ä¸»è¦åŒ…æ‹¬æ‰©æ•£æ¨¡åž‹çš„é‡å»ºæŸå¤±å’Œæ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºŽçº¦æŸ4DGSå‚æ•°çš„æ›´æ–°ï¼Œé¿å…è¿‡åº¦å˜å½¢ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDynamic-eDiToråœ¨DyNeRFæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œåœ¨ç¼–è¾‘ä¿çœŸåº¦å’Œæ—¶ç©ºä¸€è‡´æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å®šæ€§ç»“æžœå±•ç¤ºäº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ç¼–è¾‘èƒ½åŠ›ï¼Œä¾‹å¦‚æ”¹å˜ç‰©ä½“çš„æè´¨ã€å½¢çŠ¶å’Œé¢œè‰²ï¼ŒåŒæ—¶ä¿æŒåœºæ™¯çš„åŠ¨æ€æ€§å’ŒçœŸå®žæ„Ÿã€‚å®šé‡æŒ‡æ ‡ä¹ŸéªŒè¯äº†å…¶åœ¨å¤šè§†è§’ä¸€è‡´æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

Dynamic-eDiTorå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ç”µå½±ç‰¹æ•ˆåˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€è™šæ‹ŸçŽ°å®ž/å¢žå¼ºçŽ°å®žå†…å®¹åˆ›ä½œã€ä»¥åŠäº§å“è®¾è®¡å’Œå¯è§†åŒ–ç­‰é¢†åŸŸã€‚å®ƒèƒ½å¤Ÿè®©ç”¨æˆ·é€šè¿‡ç®€å•çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œè½»æ¾åœ°å¯¹åŠ¨æ€4Dåœºæ™¯è¿›è¡Œç¼–è¾‘å’Œä¿®æ”¹ï¼Œæžå¤§åœ°é™ä½Žäº†å†…å®¹åˆ›ä½œçš„é—¨æ§›ï¼Œå¹¶æå‡äº†åˆ›ä½œæ•ˆçŽ‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºŽæ›´å¤æ‚çš„åœºæ™¯ç¼–è¾‘ä»»åŠ¡ï¼Œä¾‹å¦‚äººç‰©åŠ¨ä½œç¼–è¾‘ã€åœºæ™¯å…‰ç…§è°ƒæ•´ç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/

