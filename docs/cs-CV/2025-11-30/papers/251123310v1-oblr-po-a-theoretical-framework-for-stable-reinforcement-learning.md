---
layout: default
title: OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning
---

# OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning

**arXiv**: [2511.23310v1](https://arxiv.org/abs/2511.23310) | [PDF](https://arxiv.org/pdf/2511.23310.pdf)

**ä½œè€…**: Zixun Huang, Jiayi Sheng, Zeyu Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOBLR-POæ¡†æž¶ä»¥ç¨³å®šå¤§è¯­è¨€æ¨¡åž‹å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒ**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒ` `ç­–ç•¥æ¢¯åº¦ä¼°è®¡` `æ–¹å·®ä¼˜åŒ–` `è‡ªé€‚åº”å­¦ä¹ çŽ‡` `å¤§è¯­è¨€æ¨¡åž‹ä¼˜åŒ–` `ç†è®ºæ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰RLåŽè®­ç»ƒæ–¹æ³•ç¼ºä¹ç†è®ºæŒ‡å¯¼ï¼Œå½±å“æ¢¯åº¦ä¼°è®¡å™¨ç†è§£å’Œä¼˜åŒ–ç¨³å®šæ€§
2. å»ºç«‹ç»Ÿä¸€ç†è®ºæ¡†æž¶åˆ†æžç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨ç»Ÿè®¡æ€§è´¨ï¼ŒæŽ¨å¯¼æ–¹å·®å’Œæ”¶æ•›ä¿è¯
3. å®žéªŒåœ¨Qwen3æ¨¡åž‹ä¸ŠéªŒè¯OBLR-POä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.

