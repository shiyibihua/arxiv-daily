---
layout: default
title: Video-CoM: Interactive Video Reasoning via Chain of Manipulations
---

# Video-CoM: Interactive Video Reasoning via Chain of Manipulations

**arXiv**: [2511.23477v1](https://arxiv.org/abs/2511.23477) | [PDF](https://arxiv.org/pdf/2511.23477.pdf)

**ä½œè€…**: Hanoona Rasheed, Mohammed Zumri, Muhammad Maaz, Ming-Hsuan Yang, Fahad Shahbaz Khan, Salman Khan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo-CoMæ¨¡åž‹ï¼Œé€šè¿‡æ“ä½œé“¾å®žçŽ°äº¤äº’å¼è§†é¢‘æŽ¨ç†ï¼Œä»¥è§£å†³çŽ°æœ‰æ¨¡åž‹è¢«åŠ¨å¤„ç†è§†é¢‘å¯¼è‡´çš„è¯­ä¹‰ç“¶é¢ˆé—®é¢˜ã€‚**

**å…³é”®è¯**: `äº¤äº’å¼è§†é¢‘æŽ¨ç†` `æ“ä½œé“¾` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–` `è§†é¢‘ç†è§£åŸºå‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘ç†è§£ä¸­è¢«åŠ¨ç¼–ç è§†é¢‘ï¼Œæ— æ³•åŠ¨æ€é‡çœ‹æˆ–éªŒè¯è¯æ®ï¼Œé™åˆ¶äº†ç»†ç²’åº¦æ—¶ç©ºæŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥äº¤äº’å¼è§†é¢‘æŽ¨ç†èŒƒå¼ï¼Œæ¨¡åž‹é€šè¿‡æ“ä½œé“¾æ‰§è¡Œè¿­ä»£è§†è§‰åŠ¨ä½œï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥ï¼Œç»“åˆæ­¥éª¤çº§æŽ¨ç†å¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¹ä¸ªè§†é¢‘æŽ¨ç†åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œå¹³å‡æ€§èƒ½æå‡3.6%ï¼Œè®­ç»ƒæ•°æ®é‡æ˜¾è‘—å°‘äºŽå¯æ¯”å¤§è§„æ¨¡æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM

