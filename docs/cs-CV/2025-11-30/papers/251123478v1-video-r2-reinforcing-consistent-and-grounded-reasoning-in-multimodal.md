---
layout: default
title: Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models
---

# Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models

**arXiv**: [2511.23478v1](https://arxiv.org/abs/2511.23478) | [PDF](https://arxiv.org/pdf/2511.23478.pdf)

**ä½œè€…**: Muhammad Maaz, Hanoona Rasheed, Fahad Shahbaz Khan, Salman Khan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo-R2æ¨¡åž‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢žå¼ºå¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘æŽ¨ç†ä¸­çš„ä¸€è‡´æ€§å’Œè§†è§‰åŸºç¡€æ€§ã€‚**

**å…³é”®è¯**: `è§†é¢‘æŽ¨ç†` `å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `æ—¶é—´å¯¹é½` `è§†è§‰åŸºç¡€æ€§` `åŸºå‡†æµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå½“å‰æ¨¡åž‹åœ¨è§†é¢‘æŽ¨ç†ä¸­å¸¸å‡ºçŽ°é€»è¾‘ä¸ä¸€è‡´æˆ–è§†è§‰è¯æ®å¼±çš„é—®é¢˜ï¼Œä¾èµ–è¯­è¨€å…ˆéªŒè€Œéžè§†è§‰å†…å®¹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æ—¶é—´æ„ŸçŸ¥ç›‘ç£å¾®è°ƒå’ŒåŸºäºŽæ—¶é—´å¯¹é½å¥–åŠ±çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæå‡æ—¶é—´å¯¹é½å’ŒæŽ¨ç†ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVideo-R2åœ¨ä¸€è‡´æ€§ã€è§†è§‰æ³¨æ„åŠ›å’Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.

