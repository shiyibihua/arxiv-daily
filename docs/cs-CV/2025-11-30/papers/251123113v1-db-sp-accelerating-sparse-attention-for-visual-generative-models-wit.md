---
layout: default
title: db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism
---

# db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism

**arXiv**: [2511.23113v1](https://arxiv.org/abs/2511.23113) | [PDF](https://arxiv.org/pdf/2511.23113.pdf)

**ä½œè€…**: Siqi Chen, Ke Hong, Tianchen Zhao, Ruiqi Xie, Zhenhua Zhu, Xudong Zhang, Yu Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºdb-SPä»¥è§£å†³è§†è§‰ç”Ÿæˆæ¨¡åž‹ä¸­ç¨€ç–æ³¨æ„åŠ›åºåˆ—å¹¶è¡Œçš„å·¥ä½œè´Ÿè½½ä¸å¹³è¡¡é—®é¢˜**

**å…³é”®è¯**: `åºåˆ—å¹¶è¡Œ` `ç¨€ç–æ³¨æ„åŠ›` `è§†è§‰ç”Ÿæˆæ¨¡åž‹` `å·¥ä½œè´Ÿè½½å¹³è¡¡` `æ‰©æ•£å˜æ¢å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåºåˆ—å¹¶è¡Œåº”ç”¨äºŽå—ç¨€ç–æ³¨æ„åŠ›æ—¶ï¼Œå› ç¨€ç–åº¦å˜åŒ–å’Œå—åˆ†å¸ƒä¸è§„åˆ™å¯¼è‡´å·¥ä½œè´Ÿè½½ä¸¥é‡å¤±è¡¡
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒçº§åˆ†åŒºç­–ç•¥ï¼Œåœ¨å¤´å’Œå—ç»´åº¦å®žçŽ°è¿‘å®Œç¾Žè´Ÿè½½å¹³è¡¡ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å¹¶è¡Œåº¦ä»¥é€‚åº”ç¨€ç–æ¨¡å¼å˜åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šç›¸æ¯”çŽ°æœ‰åºåˆ—å¹¶è¡Œæ–¹æ³•ï¼Œå¹³å‡ç«¯åˆ°ç«¯åŠ é€Ÿ1.25å€ï¼Œæ³¨æ„åŠ›éƒ¨åˆ†åŠ é€Ÿ1.40å€

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.

