---
layout: default
title: AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement
---

# AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement

**arXiv**: [2511.23475v1](https://arxiv.org/abs/2511.23475) | [PDF](https://arxiv.org/pdf/2511.23475.pdf)

**ä½œè€…**: Zhizhou Zhong, Yicheng Ji, Zhe Kong, Yiying Liu, Jiarui Wang, Jiasun Feng, Lupeng Liu, Xiangyi Wang, Yanjia Li, Yuqing She, Ying Qin, Huan Li, Shuiyang Mao, Wei Liu, Wenhan Luo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAnyTalkeræ¡†æž¶ä»¥è§£å†³å¤šäººç‰©è¯´è¯è§†é¢‘ç”Ÿæˆä¸­çš„æ•°æ®æˆæœ¬é«˜å’Œäº¤äº’æ€§å·®é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šäººç‰©è§†é¢‘ç”Ÿæˆ` `æ‰©æ•£å˜æ¢å™¨` `èº«ä»½æ„ŸçŸ¥æ³¨æ„åŠ›` `äº¤äº’æ€§ä¼˜åŒ–` `å”‡åŒæ­¥` `æ•°æ®é«˜æ•ˆè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šäººç‰©æ•°æ®æ”¶é›†æˆæœ¬é«˜ï¼Œé©±åŠ¨å¤šèº«ä»½æ—¶äº¤äº’æ€§éš¾ä»¥ä¿æŒè¿žè´¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¯æ‰©å±•å¤šæµå¤„ç†æž¶æž„ï¼Œå¼•å…¥èº«ä»½æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶è¿­ä»£å¤„ç†èº«ä»½-éŸ³é¢‘å¯¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šä»…éœ€å•äººè§†é¢‘è®­ç»ƒï¼Œå°‘é‡å¤šäººç‰‡æ®µä¼˜åŒ–äº¤äº’æ€§ï¼Œåœ¨å”‡åŒæ­¥ã€è§†è§‰è´¨é‡å’Œè‡ªç„¶äº¤äº’æ€§ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.

