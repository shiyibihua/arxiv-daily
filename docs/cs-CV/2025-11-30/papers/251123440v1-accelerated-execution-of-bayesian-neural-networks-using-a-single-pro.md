---
layout: default
title: Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation
---

# Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation

**arXiv**: [2511.23440v1](https://arxiv.org/abs/2511.23440) | [PDF](https://arxiv.org/pdf/2511.23440.pdf)

**ä½œè€…**: Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger FrÃ¶ning

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæ¦‚çŽ‡å‰å‘ä¼ é€’å’Œä»£ç ç”Ÿæˆçš„è´å¶æ–¯ç¥žç»ç½‘ç»œåŠ é€Ÿæ–¹æ³•ï¼Œç”¨äºŽèµ„æºå—é™ç³»ç»Ÿéƒ¨ç½²ã€‚**

**å…³é”®è¯**: `è´å¶æ–¯ç¥žç»ç½‘ç»œ` `æ¦‚çŽ‡å‰å‘ä¼ é€’` `ä¸ç¡®å®šæ€§ä¼°è®¡` `ä»£ç ç”Ÿæˆ` `åµŒå…¥å¼éƒ¨ç½²` `TVMç¼–è¯‘å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿç¥žç»ç½‘ç»œåœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å› ä¸ç¡®å®šæ€§å¤„ç†ä¸è¶³è€Œå—é™ï¼Œè´å¶æ–¯ç¥žç»ç½‘ç»œè®¡ç®—æˆæœ¬é«˜ã€‚
2. æ¦‚çŽ‡å‰å‘ä¼ é€’é€šè¿‡é«˜æ–¯åˆ†å¸ƒå‡è®¾å®žçŽ°å•æ¬¡ç¡®å®šæ€§å‰å‘ä¼ æ’­ï¼Œæ›¿ä»£é‡‡æ ·ï¼Œæå‡æ•ˆçŽ‡ã€‚
3. ç»“åˆTVMç¼–è¯‘å™¨å’Œä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨åµŒå…¥å¼ARM CPUä¸Šå®žçŽ°é«˜æ•ˆéƒ¨ç½²ï¼Œé€Ÿåº¦æå‡è¾¾4200å€ï¼Œæ€§èƒ½åŒ¹é…SVIã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

