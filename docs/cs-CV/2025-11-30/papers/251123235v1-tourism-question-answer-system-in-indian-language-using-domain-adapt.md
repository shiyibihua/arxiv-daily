---
layout: default
title: Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models
---

# Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models

**arXiv**: [2511.23235v1](https://arxiv.org/abs/2511.23235) | [PDF](https://arxiv.org/pdf/2511.23235.pdf)

**ä½œè€…**: Praveen Gatla, Anushka, Nikita Kanwar, Gouri Sahoo, Rajesh Kumar Mundotiya

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé¢†åŸŸé€‚åº”åŸºç¡€æ¨¡åž‹çš„å°åœ°è¯­æ—…æ¸¸é—®ç­”ç³»ç»Ÿï¼Œä»¥è§£å†³æ–‡åŒ–ç‰¹å®šåœºæ™¯ä¸‹ä½Žèµ„æºQAé—®é¢˜ã€‚**

**å…³é”®è¯**: `å°åœ°è¯­é—®ç­”ç³»ç»Ÿ` `æ—…æ¸¸é¢†åŸŸé€‚åº”` `åŸºç¡€æ¨¡åž‹å¾®è°ƒ` `ä½Žç§©é€‚åº”` `æ–‡åŒ–ç‰¹å®šNLP` `ä½Žèµ„æºæ•°æ®é›†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¼ºä¹é’ˆå¯¹å°åœ°è¯­æ—…æ¸¸é¢†åŸŸï¼ˆå¦‚ç“¦æ‹‰çº³è¥¿æ–‡åŒ–ï¼‰çš„é—®ç­”æ•°æ®é›†å’Œèµ„æºï¼Œå½±å“æ–‡åŒ–ç»†å¾®åº”ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¹¶å¢žå¼ºå°åœ°è¯­QAæ•°æ®é›†ï¼Œä½¿ç”¨BERTå’ŒRoBERTaåŸºç¡€æ¨¡åž‹ï¼Œé€šè¿‡SFTå’ŒLoRAå¾®è°ƒä¼˜åŒ–å‚æ•°æ•ˆçŽ‡å’Œæ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šLoRAå¾®è°ƒåœ¨F1è¾¾85.3%çš„åŒæ—¶å‡å°‘98%å¯è®­ç»ƒå‚æ•°ï¼ŒRoBERTaåœ¨æ•æ‰æ–‡åŒ–æœ¯è¯­ï¼ˆå¦‚Aartiï¼‰æ–¹é¢è¡¨çŽ°æ›´ä¼˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\% F1) while reducing trainable parameters by 98\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.

