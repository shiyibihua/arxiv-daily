---
layout: default
title: Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering
---

# Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering

**arXiv**: [2511.23304v1](https://arxiv.org/abs/2511.23304) | [PDF](https://arxiv.org/pdf/2511.23304.pdf)

**ä½œè€…**: Zijian Fu, Changsheng Lv, Mengshi Qi, Huadong Ma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€åœºæ™¯å›¾ä¸ŽKolmogorov-Arnoldä¸“å®¶ç½‘ç»œä»¥è§£å†³éŸ³é¢‘-è§†è§‰é—®ç­”ä¸­çš„ç»“æž„å»ºæ¨¡ä¸è¶³é—®é¢˜ã€‚**

**å…³é”®è¯**: `éŸ³é¢‘-è§†è§‰é—®ç­”` `å¤šæ¨¡æ€åœºæ™¯å›¾` `Kolmogorov-Arnoldç½‘ç»œ` `ä¸“å®¶æ··åˆ` `è·¨æ¨¡æ€äº¤äº’` `æ—¶é—´æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥ä»Žå¤æ‚éŸ³é¢‘-è§†è§‰å†…å®¹ä¸­æå–é—®é¢˜ç›¸å…³çº¿ç´¢ï¼Œç¼ºä¹è§†é¢‘ç»“æž„ä¿¡æ¯å»ºæ¨¡å’Œç»†ç²’åº¦å¤šæ¨¡æ€ç‰¹å¾èžåˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé¦–æ¬¡å¼•å…¥å¤šæ¨¡æ€åœºæ™¯å›¾æ˜¾å¼å»ºæ¨¡å¯¹è±¡å…³ç³»ï¼Œå¹¶è®¾è®¡åŸºäºŽKANçš„ä¸“å®¶æ··åˆç½‘ç»œå¢žå¼ºè·¨æ¨¡æ€äº¤äº’çš„ç»†ç²’åº¦å»ºæ¨¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MUSIC-AVQAå’ŒMUSIC-AVQA v2åŸºå‡†ä¸Šå®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œä»£ç å’Œæ¨¡åž‹å°†å…¬å¼€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.

