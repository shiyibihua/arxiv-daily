---
layout: default
title: Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models
---

# Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models

**arXiv**: [2511.23319v1](https://arxiv.org/abs/2511.23319) | [PDF](https://arxiv.org/pdf/2511.23319.pdf)

**ä½œè€…**: Xiang Hu, Zhanchao Zhou, Ruiqi Liang, Zehuan Li, Wei Wu, Jianguo Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHSA-UltraLongæ¨¡åž‹ä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹è¶…é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é—®é¢˜**

**å…³é”®è¯**: `è¶…é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡` `åˆ†å±‚ç¨€ç–æ³¨æ„åŠ›` `å¤§è¯­è¨€æ¨¡åž‹` `é•¿æœŸè®°å¿†` `MoEæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæž„å»ºå…·æœ‰é•¿æœŸè®°å¿†çš„æœºå™¨ï¼Œéœ€æ»¡è¶³ç¨€ç–æ€§ã€éšæœºè®¿é—®çµæ´»æ€§å’Œé•¿åº¦æ³›åŒ–æ€§
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé›†æˆåˆ°Transformerä¸­æž„å»º8Bå‚æ•°MoEæ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨16Mä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ï¼Œå¤šæ•°æ£€ç´¢ä»»åŠ¡å‡†ç¡®çŽ‡è¶…90%ï¼Œæ€§èƒ½ä¸Žå…¨æ³¨æ„åŠ›åŸºçº¿ç›¸å½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \textbf{sparsity}, \textbf{random-access flexibility}, and \textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.

