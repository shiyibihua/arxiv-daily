---
layout: default
title: SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data
---

# SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data

**arXiv**: [2511.23238v1](https://arxiv.org/abs/2511.23238) | [PDF](https://arxiv.org/pdf/2511.23238.pdf)

**ä½œè€…**: Yuting Fang, Qouc Le Gia, Flora Salim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSDE-Attentionï¼Œé€šè¿‡æ½œåœ¨æ³¨æ„åŠ›å¢žå¼ºSDE-RNNï¼Œå¤„ç†ä¸è§„åˆ™é‡‡æ ·å’Œç¼ºå¤±æ•°æ®çš„æ—¶é—´åºåˆ—ã€‚**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—åˆ†æž` `æ³¨æ„åŠ›æœºåˆ¶` `ç¼ºå¤±æ•°æ®å¤„ç†` `SDE-RNN` `ä¸è§„åˆ™é‡‡æ ·` `æ½œåœ¨çŠ¶æ€å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤„ç†åŒ»ç–—å’Œä¼ æ„Ÿå™¨ç½‘ç»œä¸­ä¸è§„åˆ™é‡‡æ ·ä¸”å¤§é‡ç¼ºå¤±çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨SDE-RNNçš„æ½œåœ¨çŠ¶æ€å¼•å…¥é€šé“çº§æ³¨æ„åŠ›ï¼ŒåŒ…æ‹¬é€šé“é‡æ ¡å‡†ã€æ—¶å˜ç‰¹å¾æ³¨æ„åŠ›å’Œé‡‘å­—å¡”å¤šå°ºåº¦è‡ªæ³¨æ„åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åˆæˆå’ŒçœŸå®žæ•°æ®é›†ä¸Šï¼Œæ³¨æ„åŠ›æ¨¡åž‹ä¼˜äºŽåŸºçº¿ï¼ŒSDE-TVF-Låœ¨UCRæ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®çŽ‡æå‡æœ€é«˜è¾¾10ä¸ªç™¾åˆ†ç‚¹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.

