---
layout: default
title: Language-conditioned world model improves policy generalization by reading environmental descriptions
---

# Language-conditioned world model improves policy generalization by reading environmental descriptions

**arXiv**: [2511.22904v1](https://arxiv.org/abs/2511.22904) | [PDF](https://arxiv.org/pdf/2511.22904.pdf)

**ä½œè€…**: Anh Nguyen, Stefan Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€æ„ŸçŸ¥ç¼–ç å™¨LED-WMï¼ŒåŸºäºŽDreamerV3æž„å»ºè¯­è¨€æ¡ä»¶ä¸–ç•Œæ¨¡åž‹ï¼Œä»¥æå‡ç­–ç•¥åœ¨æœªè§æ¸¸æˆä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è¯­è¨€æ¡ä»¶ä¸–ç•Œæ¨¡åž‹` `ç­–ç•¥æ³›åŒ–` `æ¨¡åž‹å¼ºåŒ–å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `DreamerV3` `çŽ¯å¢ƒæè¿°ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•åœ¨ç­–ç•¥æ³›åŒ–ä¸Šå—é™ï¼Œä¾èµ–è§„åˆ’æˆ–ä¸“å®¶æ¼”ç¤ºï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€æè¿°è¯­è¨€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†è¯­è¨€æè¿°æ˜¾å¼åœ°å…³è”åˆ°è§‚å¯Ÿå®žä½“ï¼Œè®­ç»ƒè¯­è¨€æ¡ä»¶ä¸–ç•Œæ¨¡åž‹ï¼Œæ— éœ€è§„åˆ’æˆ–æ¼”ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MESSENGERå’ŒMESSENGER-WMçŽ¯å¢ƒä¸­ï¼ŒLED-WMç›¸æ¯”åŸºçº¿åœ¨æœªè§æ¸¸æˆä¸Šæ³›åŒ–æ›´æœ‰æ•ˆï¼Œå¹¶å¯é€šè¿‡ä¸–ç•Œæ¨¡åž‹ç”Ÿæˆè½¨è¿¹å¾®è°ƒç­–ç•¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.

