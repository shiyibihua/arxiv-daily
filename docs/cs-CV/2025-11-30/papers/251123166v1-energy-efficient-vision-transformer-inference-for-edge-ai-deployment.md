---
layout: default
title: Energy-Efficient Vision Transformer Inference for Edge-AI Deployment
---

# Energy-Efficient Vision Transformer Inference for Edge-AI Deployment

**arXiv**: [2511.23166v1](https://arxiv.org/abs/2511.23166) | [PDF](https://arxiv.org/pdf/2511.23166.pdf)

**ä½œè€…**: Nursultan Amanzhol, Jurn-Gyu Park

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤é˜¶æ®µè¯„ä¼°æ–¹æ³•ä»¥ä¼˜åŒ–è¾¹ç¼˜AIéƒ¨ç½²ä¸­è§†è§‰Transformerçš„èƒ½æ•ˆã€‚**

**å…³é”®è¯**: `è§†è§‰Transformer` `èƒ½æ•ˆè¯„ä¼°` `è¾¹ç¼˜AIéƒ¨ç½²` `NetScore` `Sustainable Accuracy Metric` `æ··åˆæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²è§†è§‰Transformeréœ€è¶…è¶Šå‡†ç¡®çŽ‡çš„èƒ½æ•ˆè¯„ä¼°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè®¾å¤‡æ— å…³æ¨¡åž‹ç­›é€‰ä¸Žè®¾å¤‡ç›¸å…³æµ‹é‡è¿›è¡Œä¸¤é˜¶æ®µè¯„ä¼°ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨NVIDIA Jetson TX2å’ŒRTX 3050ä¸Šæµ‹è¯•ï¼Œæ··åˆæ¨¡åž‹å’Œè’¸é¦æ¨¡åž‹åˆ†åˆ«æ˜¾è‘—é™ä½Žèƒ½è€—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

