---
layout: default
title: SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models
---

# SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models

**arXiv**: [2511.23075v1](https://arxiv.org/abs/2511.23075) | [PDF](https://arxiv.org/pdf/2511.23075.pdf)

**ä½œè€…**: Ruosen Zhao, Zhikang Zhang, Jialei Xu, Jiahao Chang, Dong Chen, Lingyun Li, Weijian Sun, Zizhuang Wei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpaceMindæ¨¡åž‹ï¼Œé€šè¿‡ç›¸æœºå¼•å¯¼æ¨¡æ€èžåˆå¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹çš„ç©ºé—´æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `ç©ºé—´æŽ¨ç†` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç›¸æœºå¼•å¯¼èžåˆ` `æ¨¡æ€èžåˆ` `3Dæ„ŸçŸ¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨3Dç©ºé—´æŽ¨ç†ï¼ˆå¦‚è·ç¦»ä¼°è®¡ï¼‰ä¸Šè¡¨çŽ°ä¸è¶³ï¼Œä¾èµ–è¾…åŠ©3Dä¿¡æ¯æˆ–æµ…å±‚ç‰¹å¾èžåˆ
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åŒç¼–ç å™¨æž¶æž„ï¼Œå¼•å…¥ç›¸æœºå¼•å¯¼æ¨¡æ€èžåˆæ¨¡å—ï¼Œå°†ç›¸æœºè¡¨ç¤ºä½œä¸ºä¸»åŠ¨å¼•å¯¼æ¨¡æ€è¿›è¡Œæ·±åº¦èžåˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VSI-Benchã€SQA3Då’ŒSPBenchåŸºå‡†ä¸Šå–å¾—æ–°æœ€ä¼˜ç»“æžœï¼Œè¶…è¶Šå¼€æ”¾å’Œä¸“æœ‰ç³»ç»Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.

