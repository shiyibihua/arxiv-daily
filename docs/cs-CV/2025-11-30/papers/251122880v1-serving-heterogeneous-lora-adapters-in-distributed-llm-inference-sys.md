---
layout: default
title: Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems
---

# Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems

**arXiv**: [2511.22880v1](https://arxiv.org/abs/2511.22880) | [PDF](https://arxiv.org/pdf/2511.22880.pdf)

**ä½œè€…**: Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor RÃ¼hle, Josep Torrellas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoRAServeæ¡†æž¶ä»¥è§£å†³åˆ†å¸ƒå¼LLMæŽ¨ç†ä¸­å¼‚æž„LoRAé€‚é…å™¨æœåŠ¡æ€§èƒ½ä¸å‡é—®é¢˜**

**å…³é”®è¯**: `LoRAé€‚é…å™¨æœåŠ¡` `åˆ†å¸ƒå¼LLMæŽ¨ç†` `æ€§èƒ½ä¼˜åŒ–` `åŠ¨æ€èµ„æºç®¡ç†` `GPU Direct RDMA`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç³»ç»Ÿåœ¨æœåŠ¡å¼‚æž„LoRAé€‚é…å™¨æ—¶å¿½ç•¥ç§©ï¼ˆå¤§å°ï¼‰å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½å€¾æ–œå’ŒGPUèµ„æºæœªå……åˆ†åˆ©ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šLoRAServeé€šè¿‡åŠ¨æ€é€‚é…å™¨æ”¾ç½®å’Œè·¯ç”±ï¼Œç»“åˆGPU Direct RDMAè¿œç¨‹è®¿é—®ï¼Œä¼˜åŒ–èµ„æºåˆ†é…ä»¥åº”å¯¹å·¥ä½œè´Ÿè½½å˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žç”Ÿäº§è½¨è¿¹è¯„ä¼°ä¸­ï¼Œç›¸æ¯”å…ˆè¿›ç³»ç»Ÿï¼ŒLoRAServeåœ¨æ»¡è¶³SLOä¸‹æå‡åžåé‡è¾¾2å€ï¼Œé™ä½ŽTTFTè¾¾9å€ï¼Œå‡å°‘GPUä½¿ç”¨è¾¾50%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

