---
layout: default
title: Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models
---

# Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models

**arXiv**: [2511.23136v1](https://arxiv.org/abs/2511.23136) | [PDF](https://arxiv.org/pdf/2511.23136.pdf)

**ä½œè€…**: Yujiao Yang, Jing Lian, Linhui Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šé“¾å›¾ç²¾ç‚¼ä¸Žé€‰æ‹©æ¡†æž¶ä»¥å¢žå¼ºå¤§è¯­è¨€æ¨¡åž‹çš„å¯é æŽ¨ç†èƒ½åŠ›**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `å›¾ç»“æž„æŽ¨ç†` `å¤šé“¾ç²¾ç‚¼` `éªŒè¯ç­–ç•¥` `æŽ¨ç†æ•ˆçŽ‡æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æŽ¨ç†å¢žå¼ºæ–¹æ³•å­˜åœ¨å¤šæ ·æ€§ä¸è¶³ã€å†—ä½™åˆ†æ”¯å’Œè·¨è·¯å¾„æ•´åˆä¸Žçº é”™èƒ½åŠ›å¼±çš„é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç”Ÿæˆå¤šæ ·æŽ¨ç†è½¨è¿¹ã€å¤åˆéªŒè¯ã€æž„å»ºå…³ç³»å›¾å¹¶è®¡ç®—ç´¯ç§¯æˆåŠŸçŽ‡æ¥é€‰æ‹©å’Œç²¾ç‚¼ç­”æ¡ˆ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…­ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®çŽ‡è¾¾82.9%ï¼Œä¼˜äºŽåŸºçº¿2.1%ï¼Œå¹¶åœ¨24ç‚¹æ¸¸æˆä¸­å®žçŽ°100%å‡†ç¡®çŽ‡å’Œ13.6å€åŠ é€Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.

