---
layout: default
title: HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model
---

# HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model

**arXiv**: [2511.22961v1](https://arxiv.org/abs/2511.22961) | [PDF](https://arxiv.org/pdf/2511.22961.pdf)

**ä½œè€…**: Chen Li, Eric Peh, Basura Fernando

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHMR3Dï¼šåŸºäºŽå¤§è§†è§‰è¯­è¨€æ¨¡åž‹çš„åˆ†å±‚å¤šæ¨¡æ€è¡¨ç¤ºï¼Œç”¨äºŽ3Dåœºæ™¯ç†è§£**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¤§è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€è¡¨ç¤º` `åˆ†å±‚ç‰¹å¾` `æ˜¾å¼å¯¹é½` `å¤šè§†è§’å›¾åƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLMæ–¹æ³•éšå¼å¯¹é½3Dç‰¹å¾å¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œå› æ•°æ®ç¨€ç¼ºå’Œç©ºé—´å…³ç³»å¤æ‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨å¤šè§†è§’å›¾åƒå’Œæ–‡æœ¬æè¿°åœ¨è¾“å…¥ç©ºé—´æ˜¾å¼å¯¹é½ï¼Œæ–‡æœ¬å¼•ç”¨3Dåæ ‡æ•èŽ·ç©ºé—´å…³ç³»ï¼Œå›¾åƒè¦†ç›–å…¨é¢è§†è§’ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨3Dé—®ç­”åŸºå‡†ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ï¼Œæå‡åœºæ™¯ç†è§£æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&A and general 3D Q&A benchmarks demonstrate the effectiveness of our approach.

