---
layout: default
title: DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection
---

# DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection

**arXiv**: [2511.23222v1](https://arxiv.org/abs/2511.23222) | [PDF](https://arxiv.org/pdf/2511.23222.pdf)

**ä½œè€…**: Yefeng Wu, Shan Wan, Ling Wu, Yecheng Zhao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDAONet-YOLOv8ä»¥è§£å†³èŒ¶å›­å¤æ‚èƒŒæ™¯ä¸‹èŒ¶å¶ç—…è™«å®³æ£€æµ‹ä¸­çš„é®æŒ¡é—®é¢˜**

**å…³é”®è¯**: `èŒ¶å¶ç—…è™«å®³æ£€æµ‹` `é®æŒ¡æ„ŸçŸ¥ç½‘ç»œ` `åŒæ³¨æ„åŠ›æœºåˆ¶` `YOLOv8æ”¹è¿›` `åŠ¨æ€åˆæˆå·ç§¯` `èŒ¶å›­å›¾åƒåˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šèŒ¶å›­å¤æ‚èƒŒæ™¯ã€å…‰ç…§å˜åŒ–å’Œæžå¶é®æŒ¡å¯¼è‡´çŽ°æœ‰æ£€æµ‹å™¨æ¼æ£€å’Œè¯¯æ£€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŒæ³¨æ„åŠ›èžåˆæ¨¡å—ã€é®æŒ¡æ„ŸçŸ¥æ£€æµ‹å¤´å’ŒåŠ¨æ€åˆæˆå·ç§¯æ¨¡å—ï¼Œå¢žå¼ºç‰¹å¾æå–å’Œé®æŒ¡å¤„ç†èƒ½åŠ›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žèŒ¶å›­æ•°æ®é›†ä¸Šï¼Œç²¾åº¦ã€å¬å›žçŽ‡å’ŒmAPå‡ä¼˜äºŽYOLOv8nåŸºçº¿ï¼Œå‚æ•°å‡å°‘16.7%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate detection of tea leaf pests and diseases in real plantations remains challenging due to complex backgrounds, variable illumination, and frequent occlusions among dense branches and leaves. Existing detectors often suffer from missed detections and false positives in such scenarios. To address these issues, we propose DAONet-YOLOv8, an enhanced YOLOv8 variant with three key improvements: (1) a Dual-Attention Fusion Module (DAFM) that combines convolutional local feature extraction with self-attention based global context modeling to focus on subtle lesion regions while suppressing background noise; (2) an occlusion-aware detection head (Detect-OAHead) that learns the relationship between visible and occluded parts to compensate for missing lesion features; and (3) a C2f-DSConv module employing dynamic synthesis convolutions with multiple kernel shapes to better capture irregular lesion boundaries. Experiments on our real-world tea plantation dataset containing six pest and disease categories demonstrate that DAONet-YOLOv8 achieves 92.97% precision, 92.80% recall, 97.10% mAP@50 and 76.90% mAP@50:95, outperforming the YOLOv8n baseline by 2.34, 4.68, 1.40 and 1.80 percentage points respectively, while reducing parameters by 16.7%. Comparative experiments further confirm that DAONet-YOLOv8 achieves superior performance over mainstream detection models.

