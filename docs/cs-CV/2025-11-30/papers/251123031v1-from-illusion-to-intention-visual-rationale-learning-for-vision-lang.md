---
layout: default
title: From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning
---

# From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning

**arXiv**: [2511.23031v1](https://arxiv.org/abs/2511.23031) | [PDF](https://arxiv.org/pdf/2511.23031.pdf)

**ä½œè€…**: Changpeng Wang, Haozhe Wang, Xi Chen, Junhan Liu, Taofeng Xue, Chong Peng, Donglian Qi, Fangzhen Lin, Yunfeng Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰ç†æ€§å­¦ä¹ ä»¥è§£å†³è§†è§‰è¯­è¨€æŽ¨ç†ä¸­è§†è§‰åŠ¨ä½œæœªæœ‰æ•ˆæŽ¥åœ°çš„é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æŽ¨ç†` `è§†è§‰ç†æ€§åŒ–` `è¿‡ç¨‹ç›‘ç£` `å¼ºåŒ–å­¦ä¹ ` `ç«¯åˆ°ç«¯è®­ç»ƒ` `å¯è§£é‡Šæ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ¨¡åž‹ä¾èµ–ä¸Žä¸Šä¸‹æ–‡æ— å…³çš„è§†è§‰åŠ¨ä½œï¼Œå¯¼è‡´æŽ¨ç†æœªçœŸæ­£åŸºäºŽè§†è§‰è¯æ®ï¼Œå½¢æˆâ€˜å›¾åƒæ€è€ƒå¹»è§‰â€™
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è§†è§‰åŠ¨ä½œé‡æž„ä¸ºæ ¸å¿ƒæŽ¨ç†åŽŸè¯­ï¼Œé€šè¿‡è¿‡ç¨‹ç›‘ç£ã€ç›®æ ‡å¯¹é½å’Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…å®žçŽ°ç«¯åˆ°ç«¯è®­ç»ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ„ŸçŸ¥ã€å¹»è§‰å’ŒæŽ¨ç†åŸºå‡†ä¸Šå®žçŽ°æœ€å…ˆè¿›ç»“æžœï¼Œå»ºç«‹ä»»åŠ¡æ— å…³ã€è¿‡ç¨‹æŽ¥åœ°çš„è§†è§‰ç†æ€§åŒ–èŒƒå¼

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to "get the right answer for the right visual reason". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.

