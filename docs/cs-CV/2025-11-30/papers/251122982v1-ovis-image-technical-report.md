---
layout: default
title: Ovis-Image Technical Report
---

# Ovis-Image Technical Report

**arXiv**: [2511.22982v1](https://arxiv.org/abs/2511.22982) | [PDF](https://arxiv.org/pdf/2511.22982.pdf)

**ä½œè€…**: Guo-Hua Wang, Liangfu Cao, Tianyu Cui, Minghao Fu, Xiaohao Chen, Pengxin Zhan, Jianshan Zhao, Lan Li, Bowen Fu, Jiaqi Liu, Qing-Guo Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOvis-Imageä»¥åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹å®žçŽ°é«˜è´¨é‡æ–‡æœ¬æ¸²æŸ“**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ–‡æœ¬æ¸²æŸ“ä¼˜åŒ–` `å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œ` `æ‰©æ•£æ¨¡åž‹` `é«˜æ•ˆéƒ¨ç½²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨ä¸¥æ ¼è®¡ç®—çº¦æŸä¸‹å®žçŽ°é«˜è´¨é‡æ–‡æœ¬æ¸²æŸ“ï¼Œç¼©å°å‰æ²¿æ€§èƒ½ä¸Žå®žç”¨éƒ¨ç½²çš„å·®è·ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽOvis-U1æ¡†æž¶ï¼Œé›†æˆæ‰©æ•£è§†è§‰è§£ç å™¨å’ŒOvis 2.5å¤šæ¨¡æ€éª¨å¹²ï¼Œé‡‡ç”¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è®­ç»ƒæµç¨‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ–‡æœ¬æ¸²æŸ“æ€§èƒ½ä¸Šåª²ç¾Žæ›´å¤§å¼€æºæ¨¡åž‹ï¼ŒæŽ¥è¿‘é—­æºç³»ç»Ÿï¼Œå¯åœ¨å•é«˜ç«¯GPUä¸Šéƒ¨ç½²ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce $\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.

