---
layout: default
title: Provable Benefits of Sinusoidal Activation for Modular Addition
---

# Provable Benefits of Sinusoidal Activation for Modular Addition

**arXiv**: [2511.23443v1](https://arxiv.org/abs/2511.23443) | [PDF](https://arxiv.org/pdf/2511.23443.pdf)

**ä½œè€…**: Tianlong Huang, Zhiyuan Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯æ˜Žæ­£å¼¦æ¿€æ´»åœ¨æ¨¡åŠ æ³•å­¦ä¹ ä¸­çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½**

**å…³é”®è¯**: `æ¿€æ´»å‡½æ•°` `æ¨¡åŠ æ³•å­¦ä¹ ` `ç¥žç»ç½‘ç»œè¡¨è¾¾èƒ½åŠ›` `æ³›åŒ–ç†è®º` `æ­£å¼¦ç½‘ç»œ` `ä¸¤å±‚ç¥žç»ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶ä¸¤å±‚ç¥žç»ç½‘ç»œä¸­æ¿€æ´»å‡½æ•°å¯¹æ¨¡åŠ æ³•å­¦ä¹ çš„å½±å“ï¼Œæ­ç¤ºæ­£å¼¦ä¸ŽReLUçš„è¡¨è¾¾èƒ½åŠ›å·®å¼‚
2. æå‡ºæ­£å¼¦ç½‘ç»œçš„Natarajanç»´æ³›åŒ–ç•Œï¼ŒèŽ·å¾—æŽ¥è¿‘æœ€ä¼˜çš„æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶æŽ¨å¯¼è¿‡å‚æ•°åŒ–ä¸‹çš„å®½åº¦æ— å…³æ³›åŒ–
3. å®žéªŒéªŒè¯æ­£å¼¦ç½‘ç»œåœ¨æ³›åŒ–å’Œé•¿åº¦å¤–æŽ¨ä¸Šä¼˜äºŽReLUç½‘ç»œï¼Œæ”¯æŒç†è®ºç»“æžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper studies the role of activation functions in learning modular addition with two-layer neural networks. We first establish a sharp expressivity gap: sine MLPs admit width-$2$ exact realizations for any fixed length $m$ and, with bias, width-$2$ exact realizations uniformly over all lengths. In contrast, the width of ReLU networks must scale linearly with $m$ to interpolate, and they cannot simultaneously fit two lengths with different residues modulo $p$. We then provide a novel Natarajan-dimension generalization bound for sine networks, yielding nearly optimal sample complexity $\widetilde{\mathcal{O}}(p)$ for ERM over constant-width sine networks. We also derive width-independent, margin-based generalization for sine networks in the overparametrized regime and validate it. Empirically, sine networks generalize consistently better than ReLU networks across regimes and exhibit strong length extrapolation.

