---
layout: default
title: Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model
---

# Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model

**arXiv**: [2511.23429v1](https://arxiv.org/abs/2511.23429) | [PDF](https://arxiv.org/pdf/2511.23429.pdf)

**ä½œè€…**: Junshu Tang, Jiacheng Liu, Jiaqi Li, Longhuang Wu, Haoyu Yang, Penghao Zhao, Siruis Gong, Xiang Yuan, Shuai Shao, Qinglin Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHunyuan-GameCraft-2ï¼Œé€šè¿‡æŒ‡ä»¤é©±åŠ¨äº¤äº’è§£å†³æ¸¸æˆä¸–ç•Œå»ºæ¨¡ä¸­åŠ¨ä½œæ¨¡å¼åƒµåŒ–å’Œæ ‡æ³¨æˆæœ¬é«˜çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `æŒ‡ä»¤é©±åŠ¨äº¤äº’` `ç”Ÿæˆæ¸¸æˆä¸–ç•Œæ¨¡åž‹` `äº¤äº’è§†é¢‘æ•°æ®` `æ–‡æœ¬é©±åŠ¨æŽ§åˆ¶` `å› æžœå¯¹é½æ•°æ®é›†` `æ··åˆä¸“å®¶æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰ç”Ÿæˆä¸–ç•Œæ¨¡åž‹å—é™äºŽå›ºå®šåŠ¨ä½œæ¨¡å¼å’Œæ ‡æ³¨æˆæœ¬ï¼Œéš¾ä»¥å»ºæ¨¡å¤šæ ·æ¸¸æˆäº¤äº’å’ŒçŽ©å®¶é©±åŠ¨åŠ¨æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽ14Bå›¾åƒåˆ°è§†é¢‘MoEåŸºç¡€æ¨¡åž‹ï¼Œå¼•å…¥æ–‡æœ¬é©±åŠ¨äº¤äº’æ³¨å…¥æœºåˆ¶ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€ã€é”®ç›˜æˆ–é¼ æ ‡æŽ§åˆ¶æ¸¸æˆè§†é¢‘å†…å®¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨InterBenchåŸºå‡†ä¸ŠéªŒè¯ï¼Œæ¨¡åž‹èƒ½ç”Ÿæˆæ—¶é—´è¿žè´¯ã€å› æžœåˆç†çš„äº¤äº’æ¸¸æˆè§†é¢‘ï¼Œå“åº”è‡ªç”±å½¢å¼æŒ‡ä»¤å¦‚â€œå¼€é—¨â€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as "open the door", "draw a torch", or "trigger an explosion".

