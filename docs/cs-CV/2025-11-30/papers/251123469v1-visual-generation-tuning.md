---
layout: default
title: Visual Generation Tuning
---

# Visual Generation Tuning

**arXiv**: [2511.23469v1](https://arxiv.org/abs/2511.23469) | [PDF](https://arxiv.org/pdf/2511.23469.pdf)

**ä½œè€…**: Jiahao Guo, Sinan Du, Jingfeng Yao, Wenyu Liu, Bo Li, Haoxiang Cao, Kun Gai, Chun Yuan, Kai Wu, Xinggang Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰ç”Ÿæˆè°ƒä¼˜ï¼ˆVGTï¼‰ä»¥æ¿€å‘è§†è§‰è¯­è¨€æ¨¡åž‹çš„è§†è§‰ç”Ÿæˆæ½œåŠ›ï¼Œé™ä½Žå¯¹é½æˆæœ¬å¹¶åŠ é€Ÿæ”¶æ•›ã€‚**

**å…³é”®è¯**: `è§†è§‰ç”Ÿæˆè°ƒä¼˜` `è§†è§‰è¯­è¨€æ¨¡åž‹` `è‡ªå›žå½’å»ºæ¨¡` `è¯­ä¹‰å¯¹é½` `å›¾åƒé‡å»º` `ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›æœªå……åˆ†æŽ¢ç´¢ï¼ŒçŽ°æœ‰æ–¹æ³•å¯¹é½æˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è¯­ä¹‰ç¼–ç å™¨ä¸Žåƒç´ è§£ç å™¨å¯¹é½ï¼Œæž„å»ºVGT-AEï¼Œå®žçŽ°é«˜æ•ˆè§†è§‰ç”Ÿæˆè°ƒä¼˜ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚æ€§èƒ½ï¼Œå¦‚26.67 PSNRå’Œ0.77 GenEvalåˆ†æ•°ï¼Œå¹¶å±•ç¤ºæ‰©å±•æ½œåŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.

