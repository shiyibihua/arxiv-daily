---
layout: default
title: McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning
---

# McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning

**arXiv**: [2511.22974v1](https://arxiv.org/abs/2511.22974) | [PDF](https://arxiv.org/pdf/2511.22974.pdf)

**ä½œè€…**: Qiushi Yang, Yingjie Chen, Yuan Yao, Yifang Men, Huaizhuo Liu, Miaomiao Cui

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMcScæ¡†æž¶ä»¥è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­äººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ï¼Œé€šè¿‡è¿åŠ¨æ ¡æ­£ä¼˜åŒ–åŠ¨æ€å†…å®¹ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ` `åå¥½å¯¹é½` `å¼ºåŒ–å­¦ä¹ ` `è¿åŠ¨æ ¡æ­£` `è‡ªæ‰¹åˆ¤æŽ¨ç†` `å±‚æ¬¡å¥–åŠ±ç›‘ç£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µäººå·¥æ ‡æ³¨æˆ–ä»£ç†æŒ‡æ ‡ï¼Œå¿½ç•¥äººç±»åå¥½é€»è¾‘å’Œè¿åŠ¨ä¸Žè§†è§‰è´¨é‡å†²çªç»´åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬è‡ªæ‰¹åˆ¤ç»´åº¦æŽ¨ç†ã€å±‚æ¬¡æ¯”è¾ƒæŽ¨ç†å’Œè¿åŠ¨æ ¡æ­£ç›´æŽ¥åå¥½ä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šMcScåœ¨äººç±»åå¥½å¯¹é½ä¸Šè¡¨çŽ°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆé«˜åŠ¨æ€è§†é¢‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.

