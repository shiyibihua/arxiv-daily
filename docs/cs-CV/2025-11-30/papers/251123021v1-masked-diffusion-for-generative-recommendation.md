---
layout: default
title: Masked Diffusion for Generative Recommendation
---

# Masked Diffusion for Generative Recommendation

**arXiv**: [2511.23021v1](https://arxiv.org/abs/2511.23021) | [PDF](https://arxiv.org/pdf/2511.23021.pdf)

**ä½œè€…**: Kulin Shah, Bhuvesh Kumar, Neil Shah, Liam Collins

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŽ©ç æ‰©æ•£æ–¹æ³•ä»¥æ”¹è¿›ç”Ÿæˆå¼æŽ¨èä¸­çš„è¯­ä¹‰IDåºåˆ—å»ºæ¨¡**

**å…³é”®è¯**: `ç”Ÿæˆå¼æŽ¨è` `è¯­ä¹‰ID` `æŽ©ç æ‰©æ•£` `å¹¶è¡Œè§£ç ` `åºåˆ—å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽè¯­ä¹‰IDçš„è‡ªå›žå½’ç”Ÿæˆå¼æŽ¨èå­˜åœ¨æŽ¨ç†æˆæœ¬é«˜ã€æ•°æ®åˆ©ç”¨æ•ˆçŽ‡ä½Žå’ŒçŸ­ä¸Šä¸‹æ–‡åç½®é—®é¢˜
2. é‡‡ç”¨æŽ©ç æ‰©æ•£æ¨¡åž‹ï¼Œé€šè¿‡ç¦»æ•£æŽ©ç å™ªå£°å­¦ä¹ åºåˆ—åˆ†å¸ƒï¼Œå®žçŽ°æ¡ä»¶ç‹¬ç«‹å¹¶è¡Œè§£ç 
3. å®žéªŒè¡¨æ˜Žè¯¥æ–¹æ³•åœ¨æ•°æ®å—é™å’Œç²—ç²’åº¦å¬å›žæ–¹é¢ä¼˜äºŽè‡ªå›žå½’æ¨¡åž‹ï¼Œæ”¯æŒå¹¶è¡ŒæŽ¨ç†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

