---
layout: default
title: Accelerating Inference of Masked Image Generators via Reinforcement Learning
---

# Accelerating Inference of Masked Image Generators via Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.01094" target="_blank" class="toolbar-btn">arXiv: 2512.01094v1</a>
    <a href="https://arxiv.org/pdf/2512.01094.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.01094v1" 
            onclick="toggleFavorite(this, '2512.01094v1', 'Accelerating Inference of Masked Image Generators via Reinforcement Learning')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Pranav Subbaraman, Shufan Li, Siyan Zhao, Aditya Grover

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: 15 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpeed-RLï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åŠ é€Ÿæ©ç å›¾åƒç”Ÿæˆæ¨¡å‹æ¨ç†ï¼Œæ˜¾è‘—å‡å°‘é‡‡æ ·æ­¥éª¤ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ©ç å›¾åƒç”Ÿæˆæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹åŠ é€Ÿ` `å›¾åƒç”Ÿæˆ` `æ¨ç†ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ©ç ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒèƒ½åŠ›å¼ºï¼Œä½†æ¨ç†é€Ÿåº¦æ…¢ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ã€‚
2. Speed-RLå°†åŠ é€Ÿé—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œç»“åˆè´¨é‡å’Œé€Ÿåº¦å¥–åŠ±å¾®è°ƒæ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSpeed-RLèƒ½åœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œå°†æ¨¡å‹æ¨ç†é€Ÿåº¦æå‡3å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ©ç ç”Ÿæˆæ¨¡å‹(MGM)åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤æ‰èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ç¼“æ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼Speed-RLï¼Œç”¨äºåŠ é€Ÿé¢„è®­ç»ƒçš„MGMï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å°‘çš„æ­¥éª¤ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚ä¸ä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•å°†åŠ é€Ÿé—®é¢˜å®šä¹‰ä¸ºåˆ†å¸ƒåŒ¹é…é—®é¢˜ä¸åŒï¼ˆå³è®­ç»ƒä¸€ä¸ªå°‘æ­¥æ•°çš„å­¦ç”Ÿæ¨¡å‹æ¥åŒ¹é…å¤šæ­¥æ•°æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆ†å¸ƒï¼‰ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜è§†ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ç”±äºåŠ é€Ÿçš„ç›®æ ‡æ˜¯åœ¨æ›´å°‘çš„æ­¥éª¤ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥å°†è´¨é‡å¥–åŠ±ä¸é€Ÿåº¦å¥–åŠ±ç›¸ç»“åˆï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶å°†ç»„åˆå¥–åŠ±ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒç›¸å½“å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œå°†åŸºç¡€æ¨¡å‹åŠ é€Ÿ3å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ©ç å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆMGMï¼‰æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ç°æœ‰çš„MGMéœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤æ‰èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ä¼ ç»Ÿçš„åŠ é€Ÿæ–¹æ³•ï¼Œå¦‚è’¸é¦ï¼Œé€šå¸¸å°†åŠ é€Ÿé—®é¢˜è§†ä¸ºåˆ†å¸ƒåŒ¹é…é—®é¢˜ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½éš¾ä»¥æ•æ‰åˆ°MGMçš„å¤æ‚ç”Ÿæˆè¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†MGMçš„åŠ é€Ÿé—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚é€šè¿‡å®šä¹‰ä¸€ä¸ªç»“åˆå›¾åƒè´¨é‡å’Œæ¨ç†é€Ÿåº¦çš„å¥–åŠ±å‡½æ•°ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥ä¼˜åŒ–MGMçš„é‡‡æ ·ç­–ç•¥ï¼Œä»è€Œåœ¨æ›´å°‘çš„æ­¥éª¤å†…ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚è¿™ç§æ–¹æ³•ç›´æ¥ä¼˜åŒ–äº†åŠ é€Ÿçš„ç›®æ ‡ï¼Œé¿å…äº†ä¼ ç»Ÿè’¸é¦æ–¹æ³•ä¸­åˆ†å¸ƒåŒ¹é…çš„é—´æ¥æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpeed-RLçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) é¢„è®­ç»ƒçš„MGMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›2) å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œå…¶ä¸­çŠ¶æ€æ˜¯å½“å‰ç”Ÿæˆçš„å›¾åƒï¼ŒåŠ¨ä½œæ˜¯MGMçš„é‡‡æ ·æ­¥éª¤ï¼›3) å¥–åŠ±å‡½æ•°ï¼Œç»“åˆå›¾åƒè´¨é‡ï¼ˆå¦‚FIDåˆ†æ•°ï¼‰å’Œæ¨ç†é€Ÿåº¦ï¼ˆé‡‡æ ·æ­¥éª¤æ•°ï¼‰ï¼›4) å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰ï¼Œç”¨äºä¼˜åŒ–MGMçš„é‡‡æ ·ç­–ç•¥ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼ŒMGMåœ¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œæ ¹æ®ç”Ÿæˆçš„å›¾åƒå’Œé‡‡æ ·æ­¥éª¤è®¡ç®—å¥–åŠ±ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ›´æ–°MGMçš„å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒï¼ŒåŒæ—¶å‡å°‘é‡‡æ ·æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šSpeed-RLçš„å…³é”®åˆ›æ–°åœ¨äºå°†MGMçš„åŠ é€Ÿé—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•ä¸åŒï¼ŒSpeed-RLç›´æ¥ä¼˜åŒ–äº†åŠ é€Ÿçš„ç›®æ ‡ï¼Œå³åœ¨æ›´å°‘çš„æ­¥éª¤å†…ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚æ­¤å¤–ï¼ŒSpeed-RLä½¿ç”¨äº†ä¸€ä¸ªç»“åˆå›¾åƒè´¨é‡å’Œæ¨ç†é€Ÿåº¦çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸¤è€…ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œä»è€Œè·å¾—æ›´å¥½çš„åŠ é€Ÿæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯Speed-RLçš„å…³é”®ã€‚è®ºæ–‡ä¸­ä½¿ç”¨çš„å¥–åŠ±å‡½æ•°é€šå¸¸åŒ…å«ä¸¤éƒ¨åˆ†ï¼šå›¾åƒè´¨é‡å¥–åŠ±å’Œé€Ÿåº¦å¥–åŠ±ã€‚å›¾åƒè´¨é‡å¥–åŠ±å¯ä»¥ä½¿ç”¨FIDåˆ†æ•°æˆ–Inception Scoreç­‰æŒ‡æ ‡æ¥è¡¡é‡ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚é€Ÿåº¦å¥–åŠ±åˆ™ä¸é‡‡æ ·æ­¥éª¤æ•°æˆåæ¯”ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ›´å°‘çš„æ­¥éª¤å†…å®Œæˆç”Ÿæˆã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ä¹Ÿå¾ˆé‡è¦ï¼Œè®ºæ–‡ä¸­é€šå¸¸ä½¿ç”¨PPOç­‰ç®—æ³•æ¥ä¼˜åŒ–MGMçš„é‡‡æ ·ç­–ç•¥ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ï¼Œå¦‚å¥–åŠ±å‡½æ•°çš„æƒé‡ã€å­¦ä¹ ç‡ç­‰ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSpeed-RLèƒ½å¤Ÿåœ¨ä¿æŒç›¸å½“å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œå°†åŸºç¡€MGMçš„æ¨ç†é€Ÿåº¦æå‡3å€ã€‚å…·ä½“æ¥è¯´ï¼ŒSpeed-RLåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœï¼Œå¹¶ä¸”ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¸åŸå§‹MGMç›¸å½“ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSpeed-RLæ˜¯ä¸€ç§æœ‰æ•ˆçš„MGMåŠ é€Ÿæ–¹æ³•ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Speed-RLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¿«é€Ÿå›¾åƒç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚å®æ—¶å›¾åƒç¼–è¾‘ã€è§†é¢‘ç”Ÿæˆã€æ¸¸æˆå¼€å‘ç­‰ã€‚é€šè¿‡åŠ é€ŸMGMçš„æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¿ƒè¿›MGMåœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’ŒéŸ³é¢‘ç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.

