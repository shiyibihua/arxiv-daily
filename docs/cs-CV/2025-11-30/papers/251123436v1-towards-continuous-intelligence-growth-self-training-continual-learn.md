---
layout: default
title: Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent
---

# Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent

**arXiv**: [2511.23436v1](https://arxiv.org/abs/2511.23436) | [PDF](https://arxiv.org/pdf/2511.23436.pdf)

**ä½œè€…**: Jianzhe Lin, Zeyu Pan, Yun Zhu, Ruiqi Song, Jining Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSuperIntelliAgentæ¡†æž¶ï¼Œé€šè¿‡è‡ªè®­ç»ƒä¸ŽåŒå°ºåº¦è®°å¿†å®žçŽ°æŒç»­æ™ºèƒ½å¢žé•¿**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `è‡ªè®­ç»ƒ` `åŒå°ºåº¦è®°å¿†` `ç›´æŽ¥åå¥½ä¼˜åŒ–` `æ™ºèƒ½ä½“æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿç›‘ç£å¾®è°ƒä¾èµ–æ ‡æ³¨ï¼Œéš¾ä»¥å®žçŽ°æŒç»­æ™ºèƒ½å¢žé•¿
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¯è®­ç»ƒæ‰©æ•£æ¨¡åž‹ä¸Žå†»ç»“å¤§è¯­è¨€æ¨¡åž‹ï¼Œé€šè¿‡è‡ªç›‘ç£äº¤äº’ç”ŸæˆDPOå¯¹è¿›è¡Œä¼˜åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šå°‘é‡è‡ªåŠ¨ç”ŸæˆDPOå¯¹å³å¯æå‡åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œè¡¨æ˜Žæœºåˆ¶æœ‰æ•ˆ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

