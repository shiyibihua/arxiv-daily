---
layout: default
title: Distributed Dynamic Associative Memory via Online Convex Optimization
---

# Distributed Dynamic Associative Memory via Online Convex Optimization

**arXiv**: [2511.23347v1](https://arxiv.org/abs/2511.23347) | [PDF](https://arxiv.org/pdf/2511.23347.pdf)

**ä½œè€…**: Bowen Wang, Matteo Zecchin, Osvaldo Simeone

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒå¼åŠ¨æ€å…³è”è®°å¿†æ¡†æž¶ï¼Œé€šè¿‡åœ¨çº¿å‡¸ä¼˜åŒ–è§£å†³å¤šæ™ºèƒ½ä½“æ—¶å˜æ•°æ®æµä¸­çš„è®°å¿†æ›´æ–°é—®é¢˜ã€‚**

**å…³é”®è¯**: `åˆ†å¸ƒå¼å…³è”è®°å¿†` `åœ¨çº¿å‡¸ä¼˜åŒ–` `åŠ¨æ€çŽ¯å¢ƒ` `é€šä¿¡æ ‘ä¼˜åŒ–` `å¤šæ™ºèƒ½ä½“å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©å±•ç»å…¸å…³è”è®°å¿†è‡³å¤šæ™ºèƒ½ä½“æ—¶å˜çŽ¯å¢ƒï¼Œéœ€æœ¬åœ°è®°å¿†å­˜å‚¨å¹¶é€‰æ‹©æ€§è®°å¿†å…¶ä»–æ™ºèƒ½ä½“ä¿¡æ¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡åŸºäºŽæ ‘çš„åˆ†å¸ƒå¼åœ¨çº¿æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ”¯æŒåŠ¨æ€æ›´æ–°è®°å¿†å¹¶ä¼˜åŒ–é€šä¿¡æ ‘ä»¥å‡å°‘å»¶è¿Ÿã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç†è®ºåˆ†æžæä¾›æ€§èƒ½ä¿è¯ï¼Œå®žéªŒæ˜¾ç¤ºä¼˜äºŽå…±è¯†åˆ†å¸ƒå¼ä¼˜åŒ–åŸºçº¿ï¼Œæå‡å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings with multiple agents and time-varying data streams. In DDAM, each agent maintains a local AM that must not only store its own associations but also selectively memorize information from other agents based on a specified interest matrix. To address this problem, we propose a novel tree-based distributed online gradient descent algorithm, termed DDAM-TOGD, which enables each agent to update its memory on the fly via inter-agent communication over designated routing trees. We derive rigorous performance guarantees for DDAM-TOGD, proving sublinear static regret in stationary environments and a path-length dependent dynamic regret bound in non-stationary environments. These theoretical results provide insights into how communication delays and network structure impact performance. Building on the regret analysis, we further introduce a combinatorial tree design strategy that optimizes the routing trees to minimize communication delays, thereby improving regret bounds. Numerical experiments demonstrate that the proposed DDAM-TOGD framework achieves superior accuracy and robustness compared to representative online learning baselines such as consensus-based distributed optimization, confirming the benefits of the proposed approach in dynamic, distributed environments.

