---
layout: default
title: Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction
---

# Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction

**arXiv**: [2511.23476v1](https://arxiv.org/abs/2511.23476) | [PDF](https://arxiv.org/pdf/2511.23476.pdf)

**ä½œè€…**: Bao Shu, Yan Cai, Jianjian Sun, Chunrui Han, En Yu, Liang Zhao, Jingcheng Hu, Yinmin Zhang, Haoran Lv, Yuang Peng, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Xiangyu Yue

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWMActæ–¹æ³•ï¼Œé€šè¿‡å¤šè½®äº¤äº’æž„å»ºé«˜æ•ˆä¸–ç•Œæ¨¡åž‹æŽ¨ç†ï¼Œè§£å†³LLMåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„è§„åˆ’é—®é¢˜ã€‚**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡åž‹æŽ¨ç†` `å¤šè½®äº¤äº’` `å¥–åŠ±é‡ç¼©æ”¾` `äº¤äº’é¢‘çŽ‡é€€ç«` `LLMä»£ç†` `ä¸»åŠ¨å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤šè½®äº¤äº’æ–¹æ³•é‡‡ç”¨åƒµåŒ–æŽ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶æ¨¡åž‹ä¸»åŠ¨å­¦ä¹ ï¼Œé˜»ç¢é«˜æ•ˆä¸–ç•Œæ¨¡åž‹æŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¥–åŠ±é‡ç¼©æ”¾æœºåˆ¶å’Œäº¤äº’é¢‘çŽ‡é€€ç«ç­–ç•¥ï¼Œè§£æ”¾æ¨¡åž‹ç»“æž„åŒ–æŽ¨ç†ï¼Œä¿ƒè¿›ä¸»åŠ¨å­¦ä¹ å’ŒçŽ¯å¢ƒåŠ¨æ€å†…åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Sokobanã€Mazeå’ŒTaxiç­‰ä»»åŠ¡ä¸ŠéªŒè¯ï¼ŒWMActå®žçŽ°å•è½®è§£å†³å¤šè½®ä»»åŠ¡ï¼Œæå‡æŽ¨ç†åŸºå‡†æ€§èƒ½å¹¶å¢žå¼ºå¯è¿ç§»æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.

