---
layout: default
title: From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products
---

# From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products

**arXiv**: [2511.23407v1](https://arxiv.org/abs/2511.23407) | [PDF](https://arxiv.org/pdf/2511.23407.pdf)

**ä½œè€…**: Jan BaumgÃ¤rtner, Malte Hansjosten, David Hald, Adrian Hauptmannl, Alexander Puchta, JÃ¼rgen Fleischer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽPOMDPçš„æ¦‚çŽ‡è§„åˆ’æ¡†æž¶ï¼Œç”¨äºŽæœºå™¨äººå¯¹ä¸ç¡®å®šçŠ¶æ€æŠ¥åºŸäº§å“çš„æ‹†å¸ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `æœºå™¨äººæ‹†å¸è§„åˆ’` `éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ¦‚çŽ‡è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `è´å¶æ–¯æ»¤æ³¢` `æŠ¥åºŸäº§å“å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŠ¥åºŸäº§å“å› ç£¨æŸæˆ–ç»´ä¿®å¯¼è‡´çŠ¶æ€ä¸ç¡®å®šï¼Œä¼ ç»Ÿç¡®å®šæ€§è§„åˆ’æ–¹æ³•å¤±æ•ˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ‹†å¸å»ºæ¨¡ä¸ºPOMDPï¼Œä»ŽCADæ•°æ®è‡ªåŠ¨ç”Ÿæˆæ¨¡åž‹ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯æ»¤æ³¢å¤„ç†ä¸ç¡®å®šæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸¤ç§æœºå™¨äººç³»ç»Ÿä¸ŠéªŒè¯ï¼Œç›¸æ¯”åŸºçº¿å‡å°‘å¹³å‡æ‹†å¸æ—¶é—´å’Œæ–¹å·®ï¼Œé€‚åº”æ¨¡åž‹åå·®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts.

