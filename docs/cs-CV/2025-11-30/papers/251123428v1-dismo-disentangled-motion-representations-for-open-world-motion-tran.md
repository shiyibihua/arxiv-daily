---
layout: default
title: DisMo: Disentangled Motion Representations for Open-World Motion Transfer
---

# DisMo: Disentangled Motion Representations for Open-World Motion Transfer

**arXiv**: [2511.23428v1](https://arxiv.org/abs/2511.23428) | [PDF](https://arxiv.org/pdf/2511.23428.pdf)

**ä½œè€…**: Thomas Ressler-Antal, Frank Fundel, Malek Ben Alaya, Stefan Andreas Baumann, Felix Krause, Ming Gui, BjÃ¶rn Ommer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDisMoä»¥å­¦ä¹ è§£è€¦çš„è¿åŠ¨è¡¨ç¤ºï¼Œå®žçŽ°å¼€æ”¾ä¸–ç•Œè¿åŠ¨è¿ç§»**

**å…³é”®è¯**: `è¿åŠ¨è¡¨ç¤ºå­¦ä¹ ` `å¼€æ”¾ä¸–ç•Œè¿åŠ¨è¿ç§»` `è§†é¢‘ç”Ÿæˆ` `è§£è€¦è¡¨ç¤º` `é›¶æ ·æœ¬åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘æ¨¡åž‹ç¼ºä¹æ˜¾å¼è¿åŠ¨è¡¨ç¤ºï¼Œé™åˆ¶å†…å®¹åˆ›ä½œåº”ç”¨
2. é€šè¿‡å›¾åƒç©ºé—´é‡å»ºç›®æ ‡ä»ŽåŽŸå§‹è§†é¢‘å­¦ä¹ é€šç”¨è¿åŠ¨è¡¨ç¤ºï¼Œç‹¬ç«‹äºŽå¤–è§‚å’Œå§¿æ€
3. åœ¨è¿åŠ¨è¿ç§»ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œå¹¶åœ¨é›¶æ ·æœ¬åŠ¨ä½œåˆ†ç±»ä¸Šè¶…è¶ŠV-JEPAç­‰æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo

