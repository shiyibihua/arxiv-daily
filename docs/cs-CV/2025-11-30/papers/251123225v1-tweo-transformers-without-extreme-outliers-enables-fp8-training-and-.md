---
layout: default
title: TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies
---

# TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies

**arXiv**: [2511.23225v1](https://arxiv.org/abs/2511.23225) | [PDF](https://arxiv.org/pdf/2511.23225.pdf)

**ä½œè€…**: Guang Liang, Jie Shao, Ningyuan Tang, Xinyao Liu, Jianxin Wu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTWEOæŸå¤±å‡½æ•°ä»¥è§£å†³Transformerè®­ç»ƒä¸­æžç«¯å¼‚å¸¸å€¼é—®é¢˜ï¼Œå®žçŽ°FP8è®­ç»ƒä¸Žé‡åŒ–**

**å…³é”®è¯**: `Transformerè®­ç»ƒ` `FP8é‡åŒ–` `å¼‚å¸¸å€¼æŠ‘åˆ¶` `æŸå¤±å‡½æ•°è®¾è®¡` `ç¡¬ä»¶åŠ é€Ÿ` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šTransformerè®­ç»ƒä¸­æžç«¯æ¿€æ´»å¼‚å¸¸å€¼é˜»ç¢FP8ç¡¬ä»¶æ”¯æŒï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤æ‚å·¥ç¨‹æˆ–æž¶æž„ä¿®æ”¹
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæƒé‡çŸ©é˜µå…±çº¿æ€§åˆ†æžï¼Œæå‡ºéžä¾µå…¥æ€§TWEOæŸå¤±å‡½æ•°ï¼Œç®€å•æœ‰æ•ˆæŠ‘åˆ¶å¼‚å¸¸å€¼
3. å®žéªŒæˆ–æ•ˆæžœï¼šTWEOä½¿FP8è®­ç»ƒæ€§èƒ½åª²ç¾ŽBF16åŸºçº¿ï¼Œæå‡è®­ç»ƒåžåé‡36%ï¼Œå¹¶å®žçŽ°W8A8é‡åŒ–SOTAæ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.

