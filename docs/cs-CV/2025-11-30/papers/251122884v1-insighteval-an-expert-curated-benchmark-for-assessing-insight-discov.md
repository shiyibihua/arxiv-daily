---
layout: default
title: InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents
---

# InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents

**arXiv**: [2511.22884v1](https://arxiv.org/abs/2511.22884) | [PDF](https://arxiv.org/pdf/2511.22884.pdf)

**ä½œè€…**: Zhenghao Zhu, Yuanfeng Song, Xin Chen, Chengzhong Liu, Yakun Cui, Caleb Chen Cao, Sirui Han, Yike Guo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInsightEvalåŸºå‡†ä»¥è¯„ä¼°LLMé©±åŠ¨æ•°æ®ä»£ç†çš„æ´žå¯Ÿå‘çŽ°èƒ½åŠ›**

**å…³é”®è¯**: `æ´žå¯Ÿå‘çŽ°è¯„ä¼°` `LLMé©±åŠ¨æ•°æ®ä»£ç†` `åŸºå‡†æž„å»º` `æ•°æ®æ•´ç†æµç¨‹` `æŽ¢ç´¢æ€§èƒ½æŒ‡æ ‡` `è‡ªåŠ¨æ•°æ®åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†å¦‚InsightBenchå­˜åœ¨æ ¼å¼ä¸ä¸€è‡´ã€ç›®æ ‡è®¾è®¡ä¸ä½³å’Œæ´žå¯Ÿå†—ä½™ç­‰ç¼ºé™·ï¼Œå½±å“è¯„ä¼°è´¨é‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽé«˜è´¨é‡åŸºå‡†æ ‡å‡†ï¼Œå¼€å‘æ•°æ®æ•´ç†æµç¨‹æž„å»ºInsightEvalæ•°æ®é›†ï¼Œå¹¶å¼•å…¥æ–°æŒ‡æ ‡è¡¡é‡ä»£ç†æŽ¢ç´¢æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡å¹¿æ³›å®žéªŒï¼Œæ­ç¤ºè‡ªåŠ¨æ´žå¯Ÿå‘çŽ°çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›å…³é”®å‘çŽ°æŒ‡å¯¼æœªæ¥ç ”ç©¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Data analysis has become an indispensable part of scientific research. To discover the latent knowledge and insights hidden within massive datasets, we need to perform deep exploratory analysis to realize their full value. With the advent of large language models (LLMs) and multi-agent systems, more and more researchers are making use of these technologies for insight discovery. However, there are few benchmarks for evaluating insight discovery capabilities. As one of the most comprehensive existing frameworks, InsightBench also suffers from many critical flaws: format inconsistencies, poorly conceived objectives, and redundant insights. These issues may significantly affect the quality of data and the evaluation of agents. To address these issues, we thoroughly investigate shortcomings in InsightBench and propose essential criteria for a high-quality insight benchmark. Regarding this, we develop a data-curation pipeline to construct a new dataset named InsightEval. We further introduce a novel metric to measure the exploratory performance of agents. Through extensive experiments on InsightEval, we highlight prevailing challenges in automated insight discovery and raise some key findings to guide future research in this promising direction.

