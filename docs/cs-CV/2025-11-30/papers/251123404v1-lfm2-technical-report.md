---
layout: default
title: LFM2 Technical Report
---

# LFM2 Technical Report

**arXiv**: [2511.23404v1](https://arxiv.org/abs/2511.23404) | [PDF](https://arxiv.org/pdf/2511.23404.pdf)

**ä½œè€…**: Alexander Amini, Anna Banaszak, Harold Benoit, Arthur BÃ¶Ã¶k, Tarek Dakhran, Song Duong, Alfred Eng, Fernando Fernandes, Marc HÃ¤rkÃ¶nen, Anne Harrington, Ramin Hasani, Saniya Karwa, Yuri Khrustalev, Maxime Labonne, Mathias Lechner, Valentine Lechner, Simon Lee, Zetian Li, Noel Loo, Jacob Marks, Edoardo Mosca, Samuel J. Paech, Paul Pak, Rom N. Parnichkun, Alex Quach, Ryan Rogers, Daniela Rus, Nayan Saxena, Bettina Schlager, Tim Seyde, Jimmy T. H. Smith, Aditya Tadimeti, Neehal Tumma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLFM2ç³»åˆ—æ¶²ä½“åŸºç¡€æ¨¡åž‹ï¼Œç”¨äºŽé«˜æ•ˆè¾¹ç¼˜éƒ¨ç½²å’Œå¼ºä»»åŠ¡èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è¾¹ç¼˜è®¡ç®—` `æž¶æž„æœç´¢` `çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€æ¨¡åž‹` `æ£€ç´¢å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®žçŽ°é«˜æ•ˆæŽ¨ç†å’Œå¼ºä»»åŠ¡æ€§èƒ½ï¼Œå—é™äºŽå»¶è¿Ÿå’Œå†…å­˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡ç¡¬ä»¶åœ¨çŽ¯æž¶æž„æœç´¢ï¼Œç»“åˆé—¨æŽ§çŸ­å·ç§¯å’Œåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œé‡‡ç”¨è’¸é¦ã€è¯¾ç¨‹å­¦ä¹ å’Œä¸‰é˜¶æ®µåŽè®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹å‚æ•°350M-8.3Bï¼Œåœ¨IFEvalå’ŒGSM8Kç­‰åŸºå‡†ä¸Šè¡¨çŽ°å¼ºåŠ²ï¼Œæ”¯æŒå¤šæ¨¡æ€å’Œæ£€ç´¢å˜ä½“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.

