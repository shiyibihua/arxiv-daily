---
layout: default
title: DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking
---

# DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking

**arXiv**: [2511.22896v1](https://arxiv.org/abs/2511.22896) | [PDF](https://arxiv.org/pdf/2511.22896.pdf)

**ä½œè€…**: Weiran Li, Yeqiang Liu, Yijie Wei, Mina Han, Qiannan Guo, Zhenbo Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDMÂ³Tæ¡†æž¶ï¼Œé€šè¿‡æ‰©æ•£æ¨¡åž‹è¿­ä»£å¯¹é½ç‰¹å¾ä»¥è§£å†³å¤šæ¨¡æ€å¤šç›®æ ‡è·Ÿè¸ªä¸­çš„æ¨¡æ€èžåˆæŒ‘æˆ˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€èžåˆ` `æ‰©æ•£æ¨¡åž‹` `å¤šç›®æ ‡è·Ÿè¸ª` `ç‰¹å¾å¯¹é½` `è‡ªåŠ¨é©¾é©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯è§å…‰ä¸Žçƒ­çº¢å¤–æ¨¡æ€ç‰¹å¾åˆ†å¸ƒå·®å¼‚å¤§ï¼Œä¼ ç»Ÿèžåˆæ–¹æ³•æ˜“å¯¼è‡´å†²çªï¼Œé™ä½Žè·Ÿè¸ªç²¾åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡è·¨æ¨¡æ€æ‰©æ•£èžåˆæ¨¡å—ï¼Œè¿­ä»£æŠ•å½±ç‰¹å¾åˆ°å…±äº«æµå½¢ï¼Œå¹¶å¼•å…¥æ‰©æ•£ç²¾ç‚¼å™¨å¢žå¼ºç»Ÿä¸€è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VT-MOTåŸºå‡†ä¸Šè¾¾åˆ°41.7 HOTAï¼Œç›¸å¯¹çŽ°æœ‰æœ€ä¼˜æ–¹æ³•æå‡1.54%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.

