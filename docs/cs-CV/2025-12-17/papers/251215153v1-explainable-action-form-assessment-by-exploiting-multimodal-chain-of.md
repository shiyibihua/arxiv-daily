---
layout: default
title: Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning
---

# Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15153" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15153v1</a>
  <a href="https://arxiv.org/pdf/2512.15153.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15153v1" onclick="toggleFavorite(this, '2512.15153v1', 'Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengshi Qi, Yeteng Wu, Xianlin Zhang, Huadong Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MICLAB-BUPT/EFA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ¨¡æ€CoTæ¨ç†çš„å¯è§£é‡ŠåŠ¨ä½œå½¢æ€è¯„ä¼°æ–¹æ³•ä¸æ•°æ®é›†ï¼Œè§£å†³åŠ¨ä½œæ ‡å‡†åŒ–è¯„ä¼°é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨ä½œå½¢æ€è¯„ä¼°` `å¤šæ¨¡æ€èåˆ` `Chain-of-Thought` `å¯è§£é‡Šæ€§` `åŠ¨ä½œè´¨é‡è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•éš¾ä»¥æ»¡è¶³åŠ¨ä½œæ ‡å‡†åŒ–è¯„ä¼°éœ€æ±‚ï¼Œç¼ºä¹å¯¹åŠ¨ä½œè´¨é‡çš„ç»†è‡´åˆ†æå’Œå¯è§£é‡Šæ€§ã€‚
2. æå‡ºExplainable Fitness Assessoræ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯èåˆå’ŒCoTæ¨ç†ï¼Œå®ç°åŠ¨ä½œè¯„ä¼°ã€è§£é‡Šå’Œæ”¹è¿›å»ºè®®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨ä½œåˆ†ç±»ã€è´¨é‡è¯„ä¼°å’Œè§£é‡Šç”Ÿæˆæ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†CoT-AFAæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°å®åœºæ™¯ä¸­ï¼Œè¯„ä¼°äººä½“åŠ¨ä½œæ˜¯å¦è§„èŒƒå¹¶æä¾›åˆç†çš„åé¦ˆä»¥æ”¹è¿›åŠ¨ä½œæ ‡å‡†åŒ–è‡³å…³é‡è¦ï¼Œä½†ä¹Ÿæå…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„è§†é¢‘ç†è§£æ–¹æ³•ä¸»è¦å…³æ³¨åŠ¨ä½œæ˜¯ä»€ä¹ˆä»¥åŠåœ¨å“ªé‡Œï¼Œæ— æ³•æ»¡è¶³åŠ¨ä½œæ ‡å‡†åŒ–è¯„ä¼°çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œç°æœ‰æ•°æ®é›†ç¼ºä¹æŒ‡ç¤ºåŠ¨ä½œæ ‡å‡†åŒ–ç¨‹åº¦çš„æ ‡ç­¾ï¼Œå¹¶ä¸”åŠ¨ä½œè´¨é‡è¯„ä¼°æ•°æ®é›†ç¼ºä¹å¯è§£é‡Šæ€§å’Œè¯¦ç»†åé¦ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„äººä½“åŠ¨ä½œå½¢æ€è¯„ä¼°ï¼ˆAFAï¼‰ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤šæ ·åŒ–æ•°æ®é›†CoT-AFAï¼Œå…¶ä¸­åŒ…å«å¤§é‡çš„å¥èº«å’Œæ­¦æœ¯è§†é¢‘ï¼Œå…·æœ‰å¤šå±‚æ¬¡çš„æ³¨é‡Šï¼Œç”¨äºå…¨é¢çš„è§†é¢‘åˆ†æã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ–°é¢–çš„Chain-of-Thoughtè§£é‡ŠèŒƒå¼ä¸°å¯Œäº†CoT-AFAæ•°æ®é›†ã€‚æˆ‘ä»¬çš„è§£é‡Šä¸æ˜¯æä¾›å­¤ç«‹çš„åé¦ˆï¼Œè€Œæ˜¯æä¾›ä¸€ä¸ªå®Œæ•´çš„æ¨ç†è¿‡ç¨‹â€”â€”ä»è¯†åˆ«ä¸€ä¸ªåŠ¨ä½œæ­¥éª¤åˆ°åˆ†æå…¶ç»“æœå¹¶æå‡ºä¸€ä¸ªå…·ä½“çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºExplainable Fitness Assessorçš„æ¡†æ¶ï¼Œå®ƒä¸ä»…å¯ä»¥åˆ¤æ–­ä¸€ä¸ªåŠ¨ä½œï¼Œè¿˜å¯ä»¥è§£é‡ŠåŸå› å¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤ä¸ªå¹¶è¡Œå¤„ç†æµå’Œä¸€ä¸ªåŠ¨æ€é—¨æ§æœºåˆ¶æ¥èåˆè§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜å…¶åˆ†æèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£é‡Šç”Ÿæˆï¼ˆä¾‹å¦‚ï¼ŒCIDEr +16.0%ï¼‰ã€åŠ¨ä½œåˆ†ç±»ï¼ˆå‡†ç¡®ç‡ +2.7%ï¼‰å’Œè´¨é‡è¯„ä¼°ï¼ˆå‡†ç¡®ç‡ +2.1%ï¼‰æ–¹é¢å–å¾—äº†æ”¹è¿›ï¼Œæ­ç¤ºäº†CoT-AFAåœ¨æœªæ¥ç ”ç©¶ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæºä»£ç å¯åœ¨https://github.com/MICLAB-BUPT/EFA è·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³äººä½“åŠ¨ä½œå½¢æ€è¯„ä¼°ï¼ˆAFAï¼‰é—®é¢˜ï¼Œå³åˆ¤æ–­åŠ¨ä½œæ˜¯å¦æ ‡å‡†ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„æ”¹è¿›å»ºè®®ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åŠ¨ä½œè¯†åˆ«ï¼Œç¼ºä¹å¯¹åŠ¨ä½œè´¨é‡çš„è¯„ä¼°å’Œè§£é‡Šèƒ½åŠ›ï¼ŒåŒæ—¶ç¼ºä¹é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆè§†è§‰å’Œè¯­ä¹‰ï¼‰å’ŒChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†ï¼Œæ„å»ºä¸€ä¸ªå¯è§£é‡Šçš„åŠ¨ä½œè¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡CoTæ¨ç†ï¼Œæ¨¡å‹å¯ä»¥é€æ­¥åˆ†æåŠ¨ä½œæ­¥éª¤ã€è¯„ä¼°ç»“æœå¹¶æå‡ºæ”¹è¿›æ–¹æ¡ˆï¼Œä»è€Œæä¾›æ›´å…¨é¢çš„åé¦ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªå¹¶è¡Œå¤„ç†æµï¼šè§†è§‰æµå’Œè¯­ä¹‰æµã€‚è§†è§‰æµå¤„ç†è§†é¢‘ä¿¡æ¯ï¼Œæå–åŠ¨ä½œç‰¹å¾ï¼›è¯­ä¹‰æµå¤„ç†æ–‡æœ¬ä¿¡æ¯ï¼Œä¾‹å¦‚åŠ¨ä½œæè¿°å’Œè¯„ä¼°æ ‡å‡†ã€‚ä¸¤ä¸ªæµçš„ä¿¡æ¯é€šè¿‡åŠ¨æ€é—¨æ§æœºåˆ¶è¿›è¡Œèåˆï¼Œç„¶åè¾“å…¥åˆ°CoTæ¨ç†æ¨¡å—ï¼Œç”ŸæˆåŠ¨ä½œè¯„ä¼°å’Œè§£é‡Šã€‚æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼šç‰¹å¾æå–æ¨¡å—ï¼ˆè§†è§‰å’Œè¯­ä¹‰ï¼‰ã€å¤šæ¨¡æ€èåˆæ¨¡å—ï¼ˆåŠ¨æ€é—¨æ§æœºåˆ¶ï¼‰ã€CoTæ¨ç†æ¨¡å—ã€è¯„ä¼°å’Œè§£é‡Šç”Ÿæˆæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†äººä½“åŠ¨ä½œå½¢æ€è¯„ä¼°ï¼ˆAFAï¼‰ä»»åŠ¡ï¼›2) æ„å»ºäº†CoT-AFAæ•°æ®é›†ï¼ŒåŒ…å«å¤šå±‚æ¬¡æ ‡æ³¨å’ŒCoTè§£é‡Šï¼›3) æå‡ºäº†Explainable Fitness Assessoræ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€ä¿¡æ¯å’ŒCoTæ¨ç†ï¼Œå®ç°å¯è§£é‡Šçš„åŠ¨ä½œè¯„ä¼°ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè¯¥æ–¹æ³•ä¸ä»…è¯„ä¼°åŠ¨ä½œè´¨é‡ï¼Œè¿˜æä¾›å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹å’Œæ”¹è¿›å»ºè®®ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€é—¨æ§æœºåˆ¶ç”¨äºèåˆè§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå…¶æƒé‡ç”±æ¨¡å‹è‡ªåŠ¨å­¦ä¹ ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚CoTæ¨ç†æ¨¡å—é‡‡ç”¨Transformerç»“æ„ï¼Œé€šè¿‡è‡ªå›å½’çš„æ–¹å¼ç”ŸæˆåŠ¨ä½œè¯„ä¼°å’Œè§£é‡Šã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŠ¨ä½œåˆ†ç±»æŸå¤±ã€è´¨é‡è¯„ä¼°æŸå¤±å’Œè§£é‡Šç”ŸæˆæŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„å„ä¸ªéƒ¨åˆ†ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15153v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15153v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15153v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒExplainable Fitness Assessoræ¡†æ¶åœ¨CoT-AFAæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨è§£é‡Šç”Ÿæˆæ–¹é¢ï¼ŒCIDEræŒ‡æ ‡æå‡äº†16.0%ï¼›åœ¨åŠ¨ä½œåˆ†ç±»æ–¹é¢ï¼Œå‡†ç¡®ç‡æå‡äº†2.7%ï¼›åœ¨è´¨é‡è¯„ä¼°æ–¹é¢ï¼Œå‡†ç¡®ç‡æå‡äº†2.1%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åŠ¨ä½œè¯„ä¼°å’Œè§£é‡Šæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåœ¨çº¿å¥èº«æŒ‡å¯¼ã€è¿åŠ¨åº·å¤ã€æ­¦æœ¯æ•™å­¦ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›ä¸ªæ€§åŒ–çš„åŠ¨ä½œè¯„ä¼°å’Œæ”¹è¿›å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·æé«˜åŠ¨ä½œè§„èŒƒæ€§ï¼Œå‡å°‘è¿åŠ¨æŸä¼¤ã€‚æœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„äººä½“åŠ¨ä½œè¯„ä¼°ï¼Œä¾‹å¦‚èˆè¹ˆã€ç‘œä¼½ç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.

