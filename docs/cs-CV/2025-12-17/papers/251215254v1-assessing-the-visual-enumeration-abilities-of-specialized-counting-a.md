---
layout: default
title: Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models
---

# Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15254" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15254v1</a>
  <a href="https://arxiv.org/pdf/2512.15254.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15254v1" onclick="toggleFavorite(this, '2512.15254v1', 'Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kuinan Hou, Jing Mi, Marco Zorzi, Lamberto Ballan, Alberto Testolin

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹æ¯”åˆ†æä¸“ç”¨è®¡æ•°æ¶æ„ä¸è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æšä¸¾ä»»åŠ¡ä¸­çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è®¡æ•°` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼€æ”¾é›†å­¦ä¹ ` `è§†è§‰æšä¸¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è®¡æ•°æ–¹æ³•ä¾èµ–äºç‰¹å®šé¢†åŸŸæ¶æ„ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ï¼Œéš¾ä»¥å¤„ç†å¼€æ”¾åœºæ™¯ã€‚
2. è®ºæ–‡å¯¹æ¯”ä¸“ç”¨è®¡æ•°æ¶æ„ä¸è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ¢ç´¢é€šç”¨æ¨¡å‹åœ¨è§†è§‰æšä¸¾ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æšä¸¾ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šä¸“ç”¨æ¶æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰åœºæ™¯ä¸­çš„ç‰©ä½“è®¡æ•°æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€é¡¹åŸºç¡€ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„è®¡æ•°æ¶æ„ï¼Œè¿™äº›æ¶æ„ä½¿ç”¨é¢„å®šä¹‰å¯¹è±¡ç±»åˆ«çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œè¿™äº›é€šç”¨æ¶æ„å¯èƒ½ä¸ºå¼€æ”¾é›†å¯¹è±¡è®¡æ•°æä¾›çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¯”è¾ƒäº†æœ€å…ˆè¿›çš„ä¸“ç”¨è®¡æ•°æ¶æ„ä¸VLMsåœ¨ä¸¤ä¸ªæµè¡Œçš„è®¡æ•°æ•°æ®é›†ä»¥åŠä¸€ä¸ªä¸“é—¨åˆ›å»ºçš„ã€å¯ä»¥æ›´ç²¾ç»†åœ°æ§åˆ¶æµ‹è¯•å›¾åƒè§†è§‰å±æ€§çš„æ–°åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°VLMså¯ä»¥è¿‘ä¼¼æšä¸¾è§†è§‰åœºæ™¯ä¸­çš„ç‰©ä½“æ•°é‡ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¿‡ä¸“ç”¨è®¡ç®—æœºè§†è§‰æ¶æ„çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“VLMsè¢«æç¤ºç”Ÿæˆæ¯ä¸ªè¦è®¡æ•°çš„ç‰©ä½“çš„ä¸­é—´è¡¨ç¤ºï¼ˆå³ä½ç½®å’Œå£å¤´æ ‡ç­¾ï¼‰æ—¶ï¼Œæšä¸¾ç²¾åº¦ä¼šæ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œæ²¡æœ‰ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿå¯é åœ°è®¡ç®—å¤æ‚è§†è§‰åœºæ™¯ä¸­çš„ç‰©ä½“æ•°é‡ï¼Œè¡¨æ˜ä»ç„¶éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥åˆ›å»ºèƒ½å¤Ÿåœ¨çœŸå®ç¯å¢ƒä¸­å¯é åœ°éƒ¨ç½²è®¡æ•°ç¨‹åºçš„AIç³»ç»Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°è§†è§‰åœºæ™¯ä¸­ç‰©ä½“è®¡æ•°ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„è®¡æ•°æ¶æ„ï¼Œè¿™äº›æ¶æ„éœ€è¦é’ˆå¯¹ç‰¹å®šå¯¹è±¡ç±»åˆ«è¿›è¡Œè®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå¼±ï¼Œéš¾ä»¥é€‚åº”å¼€æ”¾åœºæ™¯ä¸‹çš„ç‰©ä½“è®¡æ•°éœ€æ±‚ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚è§†è§‰åœºæ™¯æ—¶ï¼Œè®¡æ•°å‡†ç¡®ç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è§„æ¨¡å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é€šç”¨æ€§ï¼Œå°†å…¶åº”ç”¨äºå¼€æ”¾é›†ç‰©ä½“è®¡æ•°ä»»åŠ¡ã€‚VLMsé€šè¿‡å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³è”ï¼Œå…·å¤‡äº†ç†è§£å’Œæ¨ç†è§†è§‰åœºæ™¯çš„èƒ½åŠ›ï¼Œä»è€Œå¯ä»¥ç”¨äºä¼°è®¡åœºæ™¯ä¸­ç‰©ä½“çš„æ•°é‡ã€‚é€šè¿‡æç¤ºVLMsç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼ˆå¦‚ç‰©ä½“çš„ä½ç½®å’Œæ ‡ç­¾ï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜è®¡æ•°å‡†ç¡®ç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨å¯¹æ¯”å®éªŒçš„æ–¹æ³•ï¼Œæ¯”è¾ƒäº†æœ€å…ˆè¿›çš„ä¸“ç”¨è®¡æ•°æ¶æ„å’ŒVLMsåœ¨ç‰©ä½“è®¡æ•°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å®éªŒä½¿ç”¨äº†ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†å’Œä¸€ä¸ªæ–°æ„å»ºçš„åŸºå‡†æ•°æ®é›†ï¼Œè¯¥åŸºå‡†æ•°æ®é›†å¯ä»¥æ›´ç²¾ç»†åœ°æ§åˆ¶æµ‹è¯•å›¾åƒçš„è§†è§‰å±æ€§ã€‚VLMsé€šè¿‡æ¥æ”¶å›¾åƒå’Œè®¡æ•°æç¤ºä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºåœºæ™¯ä¸­ç‰©ä½“çš„æ•°é‡ã€‚ä¸ºäº†æé«˜è®¡æ•°å‡†ç¡®ç‡ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†æç¤ºVLMsç”Ÿæˆä¸­é—´è¡¨ç¤ºçš„æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ¢ç´¢äº†è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾é›†ç‰©ä½“è®¡æ•°ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå¹¶è¯æ˜äº†VLMså¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¿‡ä¸“ç”¨è®¡æ•°æ¶æ„çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§é€šè¿‡æç¤ºVLMsç”Ÿæˆä¸­é—´è¡¨ç¤ºæ¥æé«˜è®¡æ•°å‡†ç¡®ç‡çš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„VLMsï¼Œå¦‚CLIPå’ŒBLIPç­‰ï¼›2) è®¾è®¡åˆé€‚çš„æç¤ºï¼Œå¼•å¯¼VLMsè¿›è¡Œç‰©ä½“è®¡æ•°ï¼›3) æ¢ç´¢ä¸åŒçš„ä¸­é—´è¡¨ç¤ºç”Ÿæˆç­–ç•¥ï¼Œå¦‚ç”Ÿæˆç‰©ä½“çš„ä½ç½®å’Œæ ‡ç­¾ï¼›4) ä½¿ç”¨å¤šä¸ªæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å…¬å¼€æ•°æ®é›†å’Œæ–°æ„å»ºçš„åŸºå‡†æ•°æ®é›†ï¼›5) é‡‡ç”¨åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15254v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15254v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15254v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°VLMså¯ä»¥è¿‘ä¼¼æšä¸¾è§†è§‰åœºæ™¯ä¸­çš„ç‰©ä½“æ•°é‡ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¿‡ä¸“ç”¨è®¡ç®—æœºè§†è§‰æ¶æ„çš„æ€§èƒ½ã€‚å½“VLMsè¢«æç¤ºç”Ÿæˆæ¯ä¸ªè¦è®¡æ•°çš„ç‰©ä½“çš„ä¸­é—´è¡¨ç¤ºï¼ˆå³ä½ç½®å’Œå£å¤´æ ‡ç­¾ï¼‰æ—¶ï¼Œæšä¸¾ç²¾åº¦ä¼šæ˜¾è‘—æé«˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼ŒVLMsçš„è®¡æ•°å‡†ç¡®ç‡æå‡äº†10%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€é›¶å”®åˆ†æç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è‡ªåŠ¨ç»Ÿè®¡åœºæ™¯ä¸­çš„äººæ•°æˆ–è½¦è¾†æ•°ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºæ£€æµ‹å’Œè®¡æ•°é“è·¯ä¸Šçš„è¡Œäººã€è½¦è¾†å’Œäº¤é€šæ ‡å¿—ï¼›åœ¨é›¶å”®åˆ†æä¸­ï¼Œå¯ä»¥ç”¨äºç»Ÿè®¡å•†åº—ä¸­çš„é¡¾å®¢æ•°é‡å’Œå•†å“æ•°é‡ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„è§†è§‰è®¡æ•°ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.

