---
layout: default
title: IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning
---

# IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15635v1</a>
  <a href="https://arxiv.org/pdf/2512.15635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15635v1" onclick="toggleFavorite(this, '2512.15635v1', 'IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIC-Effectï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°ç²¾ç¡®é«˜æ•ˆçš„è§†é¢‘ç‰¹æ•ˆç¼–è¾‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç‰¹æ•ˆç¼–è¾‘` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `DiT` `å°‘æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç¼–è¾‘æ¨¡å‹éš¾ä»¥åœ¨æ³¨å…¥ç‰¹æ•ˆçš„åŒæ—¶ä¿æŒèƒŒæ™¯ä¸å˜ï¼Œä¸”éš¾ä»¥ä»å°‘é‡æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„ç‰¹æ•ˆæ¨¡å¼ã€‚
2. IC-Effectåˆ©ç”¨DiTæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå°†æºè§†é¢‘ä½œä¸ºä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œå®ç°ç²¾ç¡®çš„èƒŒæ™¯ä¿æŒå’Œè‡ªç„¶çš„ç‰¹æ•ˆæ³¨å…¥ã€‚
3. é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒå’Œæ—¶ç©ºç¨€ç–tokenizationï¼ŒIC-Effectåœ¨ä¿è¯é«˜ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIC-Effectï¼Œä¸€ä¸ªåŸºäºDiTçš„ã€æŒ‡ä»¤å¼•å¯¼çš„å°‘æ ·æœ¬è§†é¢‘VFXç¼–è¾‘æ¡†æ¶ï¼Œç”¨äºåˆæˆå¤æ‚çš„ç‰¹æ•ˆï¼ˆä¾‹å¦‚ç«ç„°ã€ç²’å­å’Œå¡é€šäººç‰©ï¼‰ï¼ŒåŒæ—¶ä¸¥æ ¼ä¿æŒç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚è§†é¢‘VFXç¼–è¾‘æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ³¨å…¥çš„ç‰¹æ•ˆå¿…é¡»ä¸èƒŒæ™¯æ— ç¼èåˆï¼ŒèƒŒæ™¯å¿…é¡»å®Œå…¨ä¿æŒä¸å˜ï¼Œå¹¶ä¸”ç‰¹æ•ˆæ¨¡å¼å¿…é¡»ä»æœ‰é™çš„é…å¯¹æ•°æ®ä¸­é«˜æ•ˆå­¦ä¹ ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç¼–è¾‘æ¨¡å‹æ— æ³•æ»¡è¶³è¿™äº›è¦æ±‚ã€‚IC-Effectåˆ©ç”¨æºè§†é¢‘ä½œä¸ºå¹²å‡€çš„ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œåˆ©ç”¨DiTæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ¥å®ç°ç²¾ç¡®çš„èƒŒæ™¯ä¿æŒå’Œè‡ªç„¶çš„ç‰¹æ•ˆæ³¨å…¥ã€‚ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é€šç”¨ç¼–è¾‘é€‚åº”å’Œé€šè¿‡Effect-LoRAè¿›è¡Œçš„ç‰¹æ•ˆç‰¹å®šå­¦ä¹ ï¼Œç¡®ä¿äº†å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªå’Œé²æ£’çš„ç‰¹æ•ˆå»ºæ¨¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶ç©ºç¨€ç–tokenizationï¼Œä»è€Œä»¥å¤§å¤§å‡å°‘çš„è®¡ç®—é‡å®ç°é«˜ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«15ç§é«˜è´¨é‡è§†è§‰é£æ ¼çš„é…å¯¹VFXç¼–è¾‘æ•°æ®é›†ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒIC-Effectæä¾›äº†é«˜è´¨é‡ã€å¯æ§ä¸”æ—¶é—´ä¸€è‡´çš„VFXç¼–è¾‘ï¼Œä¸ºè§†é¢‘åˆ›ä½œå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘ç‰¹æ•ˆç¼–è¾‘æ—¨åœ¨å‘è§†é¢‘ä¸­æ·»åŠ ç«ç„°ã€ç²’å­ã€å¡é€šäººç‰©ç­‰è§†è§‰ç‰¹æ•ˆï¼ŒåŒæ—¶ä¿æŒåŸå§‹è§†é¢‘èƒŒæ™¯ä¸å˜ï¼Œå¹¶ä¿è¯ç‰¹æ•ˆåœ¨æ—¶é—´å’Œç©ºé—´ä¸Šçš„ä¸€è‡´æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥åœ¨æœ‰é™çš„é…å¯¹æ•°æ®ä¸‹ï¼Œå®ç°ç‰¹æ•ˆä¸èƒŒæ™¯çš„æ— ç¼èåˆï¼Œä»¥åŠå¯¹å¤æ‚ç‰¹æ•ˆæ¨¡å¼çš„æœ‰æ•ˆå»ºæ¨¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIC-Effectçš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨Diffusion Transformer (DiT) æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚é€šè¿‡å°†åŸå§‹è§†é¢‘ä½œä¸ºå¹²å‡€çš„ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•åœ¨ä¿æŒèƒŒæ™¯ä¸å˜çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®æŒ‡ä»¤æ³¨å…¥é€¼çœŸçš„ç‰¹æ•ˆã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤§é‡é…å¯¹æ•°æ®çš„ä¾èµ–ï¼Œå¹¶æé«˜äº†ç‰¹æ•ˆç¼–è¾‘çš„ç²¾åº¦å’Œå¯æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIC-Effecté‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯é€šç”¨ç¼–è¾‘é€‚åº”ï¼Œä½¿æ¨¡å‹å…·å¤‡åŸºæœ¬çš„è§†é¢‘ç¼–è¾‘èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ç‰¹æ•ˆç‰¹å®šå­¦ä¹ ï¼Œé€šè¿‡Effect-LoRA (Low-Rank Adaptation) å¯¹ç‰¹å®šç‰¹æ•ˆè¿›è¡Œå¾®è°ƒï¼Œå¢å¼ºæ¨¡å‹å¯¹ç‰¹å®šç‰¹æ•ˆçš„å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒIC-Effectè¿˜å¼•å…¥äº†æ—¶ç©ºç¨€ç–tokenizationï¼Œä»¥å‡å°‘è®¡ç®—é‡ï¼Œæé«˜æ•ˆç‡ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šè¾“å…¥åŸå§‹è§†é¢‘å’Œç¼–è¾‘æŒ‡ä»¤ï¼Œé€šè¿‡DiTæ¨¡å‹ç”Ÿæˆå¸¦æœ‰ç‰¹æ•ˆçš„è§†é¢‘ï¼Œå¹¶åˆ©ç”¨æ—¶ç©ºä¸€è‡´æ€§æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šIC-Effectçš„å…³é”®åˆ›æ–°åœ¨äºå°†ä¸Šä¸‹æ–‡å­¦ä¹ å¼•å…¥è§†é¢‘ç‰¹æ•ˆç¼–è¾‘é¢†åŸŸï¼Œå¹¶ç»“åˆDiTæ¨¡å‹å®ç°äº†ç²¾ç¡®çš„èƒŒæ™¯ä¿æŒå’Œè‡ªç„¶çš„ç‰¹æ•ˆæ³¨å…¥ã€‚Effect-LoRAæ¨¡å—é’ˆå¯¹ç‰¹å®šç‰¹æ•ˆè¿›è¡Œä¼˜åŒ–ï¼Œæé«˜äº†æ¨¡å‹å¯¹å¤æ‚ç‰¹æ•ˆçš„å»ºæ¨¡èƒ½åŠ›ã€‚æ—¶ç©ºç¨€ç–tokenizationåˆ™åœ¨ä¿è¯é«˜ä¿çœŸåº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šEffect-LoRAæ¨¡å—é‡‡ç”¨ä½ç§©åˆ†è§£çš„æ–¹å¼ï¼Œå¯¹DiTæ¨¡å‹çš„å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šç‰¹æ•ˆçš„å­¦ä¹ ã€‚æ—¶ç©ºç¨€ç–tokenizationé€šè¿‡é€‰æ‹©æ€§åœ°ä¿ç•™é‡è¦çš„æ—¶ç©ºtokensï¼Œå‡å°‘äº†è®¡ç®—é‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åƒç´ çº§åˆ«çš„é‡å»ºæŸå¤±ã€å¯¹æŠ—æŸå¤±å’Œæ—¶ç©ºä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä¿è¯ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œæ—¶ç©ºä¸€è‡´æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15635v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15635v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15635v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIC-Effectåœ¨è§†é¢‘ç‰¹æ•ˆç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIC-Effectèƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´å¯æ§ä¸”æ—¶é—´ä¸€è‡´çš„ç‰¹æ•ˆè§†é¢‘ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶åœ¨è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IC-Effectå¯å¹¿æ³›åº”ç”¨äºç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€å¹¿å‘Šè®¾è®¡ç­‰é¢†åŸŸï¼Œä¸ºè§†é¢‘åˆ›ä½œè€…æä¾›é«˜æ•ˆã€å¯æ§çš„ç‰¹æ•ˆç¼–è¾‘å·¥å…·ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿé™ä½ç‰¹æ•ˆåˆ¶ä½œçš„é—¨æ§›ï¼Œæé«˜åˆ›ä½œæ•ˆç‡ï¼Œå¹¶ä¸ºç”¨æˆ·å¸¦æ¥æ›´åŠ ä¸°å¯Œå’Œä¸ªæ€§åŒ–çš„è§†é¢‘å†…å®¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.

