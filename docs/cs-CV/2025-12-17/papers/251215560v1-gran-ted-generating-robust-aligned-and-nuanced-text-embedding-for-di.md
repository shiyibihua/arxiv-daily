---
layout: default
title: GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models
---

# GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15560v1</a>
  <a href="https://arxiv.org/pdf/2512.15560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15560v1" onclick="toggleFavorite(this, '2512.15560v1', 'GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bozhou Li, Sihan Yang, Yushuo Guan, Ruichuan An, Xinlong Chen, Yang Shi, Pengfei Wan, Wentao Zhang, Yuanxing zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRAN-TEDæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé²æ£’ã€å¯¹é½å’Œç»†è‡´çš„æ‰©æ•£æ¨¡å‹æ–‡æœ¬åµŒå…¥ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬ç¼–ç å™¨` `æ‰©æ•£æ¨¡å‹` `æ–‡æœ¬åˆ°å›¾åƒ` `æ–‡æœ¬åˆ°è§†é¢‘` `å¤šæ¨¡æ€å­¦ä¹ ` `æ–‡æœ¬åµŒå…¥` `è¯„ä¼°åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆä¾èµ–æ–‡æœ¬ç¼–ç å™¨ï¼Œä½†ç¼ºä¹æœ‰æ•ˆè¯„ä¼°æ¡†æ¶å’Œè§†è§‰åˆæˆçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
2. GRAN-TEDé€šè¿‡TED-6KåŸºå‡†è¯„ä¼°ç¼–ç å™¨ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæå‡æ–‡æœ¬ç‰¹å¾çš„ç»†è‡´æ€§å’Œé²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGRAN-TEDåœ¨TED-6Kä¸Šè¾¾åˆ°SOTAï¼Œå¹¶åœ¨å›¾åƒ/è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºGRAN-TEDï¼Œä¸€ç§ç”¨äºç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­é²æ£’ã€å¯¹é½å’Œç»†è‡´æ–‡æœ¬åµŒå…¥çš„èŒƒä¾‹ã€‚æ–‡æœ¬ç¼–ç å™¨æ˜¯æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä»æ ¹æœ¬ä¸Šå†³å®šäº†ç”Ÿæˆå†…å®¹çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œå…¶å‘å±•å—åˆ°ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜çš„é˜»ç¢ï¼šç¼ºä¹èƒ½å¤Ÿå¯é åœ°é¢„æµ‹ä¸‹æ¸¸ç”Ÿæˆæ€§èƒ½çš„æœ‰æ•ˆè¯„ä¼°æ¡†æ¶ï¼Œä»¥åŠéš¾ä»¥æœ‰æ•ˆåœ°è°ƒæ•´é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œè§†è§‰åˆæˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TED-6Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„çº¯æ–‡æœ¬åŸºå‡†ï¼Œæ— éœ€æ˜‚è´µçš„ç«¯åˆ°ç«¯æ¨¡å‹è®­ç»ƒå³å¯å¯¹ç¼–ç å™¨çš„è¡¨å¾è´¨é‡è¿›è¡Œé«˜æ•ˆè€Œç¨³å¥çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¯æ˜äº†TED-6Kä¸Šçš„æ€§èƒ½ï¼ˆé€šè¿‡è½»é‡çº§ç»Ÿä¸€é€‚é…å™¨æ ‡å‡†åŒ–ï¼‰ä¸ç¼–ç å™¨åœ¨ä¸‹æ¸¸ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å¯†åˆ‡ç›¸å…³ã€‚å…¶æ¬¡ï¼Œåœ¨è¯¥éªŒè¯æ¡†æ¶çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒèŒƒä¾‹å¼€å‘äº†ä¸€ç§å“è¶Šçš„æ–‡æœ¬ç¼–ç å™¨ã€‚æ­¤è¿‡ç¨‹åŒ…æ‹¬åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œåˆå§‹å¾®è°ƒé˜¶æ®µï¼Œä»¥è·å¾—æ›´å¥½çš„è§†è§‰è¡¨ç¤ºï¼Œç„¶åé‡‡ç”¨åˆ†å±‚åŠ æƒæ–¹æ³•æ¥æå–æ›´ç»†è‡´å’Œæ›´å¼ºå¤§çš„æ–‡æœ¬ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œç”±æ­¤äº§ç”Ÿçš„GRAN-TEDç¼–ç å™¨ä¸ä»…åœ¨TED-6Kä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”è¿˜åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢å¸¦æ¥äº†æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ–‡æœ¬ç¼–ç å™¨æ˜¯å…³é”®ç»„ä»¶ï¼Œå†³å®šäº†ç”Ÿæˆå†…å®¹çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶æ¥å¯é åœ°é¢„æµ‹ä¸‹æ¸¸ç”Ÿæˆæ€§èƒ½ï¼Œå¹¶ä¸”éš¾ä»¥æœ‰æ•ˆåœ°è°ƒæ•´é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œè§†è§‰åˆæˆï¼Œå¯¼è‡´æ–‡æœ¬ç¼–ç å™¨çš„å‘å±•å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRAN-TEDçš„æ ¸å¿ƒæ€è·¯æ˜¯é¦–å…ˆæ„å»ºä¸€ä¸ªé«˜æ•ˆçš„æ–‡æœ¬è¯„ä¼°åŸºå‡†TED-6Kï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬ç¼–ç å™¨çš„è´¨é‡ï¼Œç„¶ååŸºäºæ­¤åŸºå‡†ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­è¯„ä¼°å›°éš¾å’Œé¢„è®­ç»ƒæ¨¡å‹é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRAN-TEDæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šTED-6Kæ–‡æœ¬è¯„ä¼°åŸºå‡†å’Œä¸¤é˜¶æ®µè®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ã€‚TED-6Kç”¨äºè¯„ä¼°æ–‡æœ¬ç¼–ç å™¨çš„è¡¨å¾è´¨é‡ã€‚ä¸¤é˜¶æ®µè®­ç»ƒåŒ…æ‹¬ï¼š1) åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡è§†è§‰è¡¨ç¤ºèƒ½åŠ›ï¼›2) é‡‡ç”¨åˆ†å±‚åŠ æƒæ–¹æ³•ï¼Œæå–æ›´ç»†è‡´å’Œå¼ºå¤§çš„æ–‡æœ¬ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRAN-TEDçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†TED-6Kï¼Œä¸€ä¸ªçº¯æ–‡æœ¬åŸºå‡†ï¼Œç”¨äºé«˜æ•ˆè¯„ä¼°æ–‡æœ¬ç¼–ç å™¨çš„è´¨é‡ï¼Œé¿å…äº†æ˜‚è´µçš„ç«¯åˆ°ç«¯æ¨¡å‹è®­ç»ƒï¼›2) æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆåœ¨å¤šæ¨¡æ€LLMä¸Šå¾®è°ƒï¼Œç„¶åè¿›è¡Œåˆ†å±‚åŠ æƒï¼Œä»è€Œæå‡æ–‡æœ¬ç¼–ç å™¨çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šTED-6KåŒ…å«6000ä¸ªæ–‡æœ¬æè¿°ï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§†è§‰æ¦‚å¿µã€‚ä¸¤é˜¶æ®µè®­ç»ƒä¸­ï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤šæ¨¡æ€LLMï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰è¿›è¡Œå¾®è°ƒï¼ŒæŸå¤±å‡½æ•°æœªçŸ¥ã€‚ç¬¬äºŒé˜¶æ®µçš„åˆ†å±‚åŠ æƒæ–¹æ³•ï¼Œå…·ä½“æƒé‡è®¡ç®—æ–¹å¼æœªçŸ¥ï¼Œä½†ç›®æ ‡æ˜¯æå–æ›´ç»†è‡´å’Œæ›´å¼ºå¤§çš„æ–‡æœ¬ç‰¹å¾ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15560v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15560v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15560v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

GRAN-TEDåœ¨TED-6KåŸºå‡†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå°†GRAN-TEDç¼–ç å™¨åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”Ÿæˆè´¨é‡ï¼Œå…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†è¡¨æ˜äº†GRAN-TEDçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRAN-TEDçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæé«˜ç”Ÿæˆå†…å®¹ä¸æ–‡æœ¬æè¿°çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚è¯¥æ–¹æ³•åœ¨åˆ›æ„è®¾è®¡ã€å†…å®¹ç”Ÿæˆã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.

