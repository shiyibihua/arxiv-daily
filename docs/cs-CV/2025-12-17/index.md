---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-17
---

# cs.CVï¼ˆ2025-12-17ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ğŸ”—5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251215153v1-explainable-action-form-assessment-by-exploiting-multimodal-chain-of.html">Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning</a></td>
  <td>æå‡ºåŸºäºå¤šæ¨¡æ€CoTæ¨ç†çš„å¯è§£é‡ŠåŠ¨ä½œå½¢æ€è¯„ä¼°æ–¹æ³•ä¸æ•°æ®é›†ï¼Œè§£å†³åŠ¨ä½œæ ‡å‡†åŒ–è¯„ä¼°é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15153v1" onclick="toggleFavorite(this, '2512.15153v1', 'Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251215693v1-skyra-ai-generated-video-detection-via-grounded-artifact-reasoning.html">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</a></td>
  <td>Skyraï¼šé€šè¿‡å¯ä¿¡çš„ä¼ªå½±æ¨ç†å®ç°AIç”Ÿæˆè§†é¢‘æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15693v1" onclick="toggleFavorite(this, '2512.15693v1', 'Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251215560v1-gran-ted-generating-robust-aligned-and-nuanced-text-embedding-for-di.html">GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</a></td>
  <td>æå‡ºGRAN-TEDæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé²æ£’ã€å¯¹é½å’Œç»†è‡´çš„æ‰©æ•£æ¨¡å‹æ–‡æœ¬åµŒå…¥ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15560v1" onclick="toggleFavorite(this, '2512.15560v1', 'GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251215528v1-emocaliber-advancing-reliable-visual-emotion-comprehension-via-confi.html">EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</a></td>
  <td>EmoCaliberï¼šé€šè¿‡ç½®ä¿¡åº¦è¡¨è¾¾ä¸æ ¡å‡†ï¼Œæå‡è§†è§‰æƒ…æ„Ÿç†è§£çš„å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15528v1" onclick="toggleFavorite(this, '2512.15528v1', 'EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251215431v1-step-gui-technical-report.html">Step-GUI Technical Report</a></td>
  <td>æå‡ºStep-GUIï¼Œé€šè¿‡è‡ªè¿›åŒ–è®­ç»ƒå’ŒGUI-MCPåè®®ï¼Œå®ç°é«˜æ•ˆã€å®‰å…¨ã€é€šç”¨çš„GUIè‡ªåŠ¨åŒ–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15431v1" onclick="toggleFavorite(this, '2512.15431v1', 'Step-GUI Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251215713v1-diffusionvl-translating-any-autoregressive-models-into-diffusion-vis.html">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</a></td>
  <td>DiffusionVLï¼šå°†ä»»æ„è‡ªå›å½’æ¨¡å‹è½¬åŒ–ä¸ºæ‰©æ•£è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ¨ç†é€Ÿåº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15713v1" onclick="toggleFavorite(this, '2512.15713v1', 'DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251215340v1-towards-seamless-interaction-causal-turn-level-modeling-of-interacti.html">Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics</a></td>
  <td>æå‡ºTIMARï¼Œç”¨äºå»ºæ¨¡äº¤äº’å¼3Då¯¹è¯å¤´éƒ¨çš„å› æœturnçº§åŠ¨æ€ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15340v1" onclick="toggleFavorite(this, '2512.15340v1', 'Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251215254v1-assessing-the-visual-enumeration-abilities-of-specialized-counting-a.html">Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</a></td>
  <td>å¯¹æ¯”åˆ†æä¸“ç”¨è®¡æ•°æ¶æ„ä¸è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æšä¸¾ä»»åŠ¡ä¸­çš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15254v1" onclick="toggleFavorite(this, '2512.15254v1', 'Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251215098v1-uni-parser-technical-report.html">Uni-Parser Technical Report</a></td>
  <td>Uni-Parserï¼šé¢å‘ç§‘å­¦æ–‡çŒ®å’Œä¸“åˆ©çš„é«˜é€šé‡æ–‡æ¡£è§£æå¼•æ“</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15098v1" onclick="toggleFavorite(this, '2512.15098v1', 'Uni-Parser Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251215069v1-pmmd-a-pose-guided-multi-view-multi-modal-diffusion-for-person-gener.html">PMMD: A pose-guided multi-view multi-modal diffusion for person generation</a></td>
  <td>æå‡ºPMMDæ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹å®ç°å§¿æ€å¼•å¯¼ä¸‹çš„é«˜è´¨é‡äººç‰©ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15069v1" onclick="toggleFavorite(this, '2512.15069v1', 'PMMD: A pose-guided multi-view multi-modal diffusion for person generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/251215715v1-in-pursuit-of-pixel-supervision-for-visual-pre-training.html">In Pursuit of Pixel Supervision for Visual Pre-training</a></td>
  <td>Pixioï¼šåŸºäºåƒç´ ç›‘ç£çš„è§†è§‰é¢„è®­ç»ƒï¼Œå®ç°ç®€å•ã€é«˜æ•ˆä¸”å¼ºå¤§çš„è¡¨å¾å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">visual pre-training</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15715v1" onclick="toggleFavorite(this, '2512.15715v1', 'In Pursuit of Pixel Supervision for Visual Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251215160v1-eaglevision-a-dual-stage-framework-with-bev-grounding-based-chain-of.html">EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</a></td>
  <td>EagleVisionï¼šåŸºäºBEVçš„é“¾å¼æ€è€ƒåŒé˜¶æ®µæ¡†æ¶ï¼Œæå‡ç©ºé—´æ™ºèƒ½</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15160v1" onclick="toggleFavorite(this, '2512.15160v1', 'EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251215410v1-preserving-marker-specificity-with-lightweight-channel-independent-r.html">Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</a></td>
  <td>æå‡ºè½»é‡çº§é€šé“ç‹¬ç«‹è¡¨ç¤ºå­¦ä¹ ä»¥æå‡æ ‡è®°ç‰¹å¼‚æ€§</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15410v1" onclick="toggleFavorite(this, '2512.15410v1', 'Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251215261v1-mmmamba-a-versatile-cross-modal-in-context-fusion-framework-for-pan-.html">MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement</a></td>
  <td>æå‡ºMMMambaï¼Œä¸€ç§ç”¨äºå…¨è‰²é”åŒ–å’Œé›¶æ ·æœ¬å›¾åƒå¢å¼ºçš„è·¨æ¨¡æ€ä¸Šä¸‹æ–‡èåˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15261v1" onclick="toggleFavorite(this, '2512.15261v1', 'MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251215423v1-photorealistic-phantom-roads-in-real-scenes-disentangling-3d-halluci.html">Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</a></td>
  <td>æå‡ºGrounded Self-Distillationæ¡†æ¶ï¼Œè§£å†³å•ç›®æ·±åº¦ä¼°è®¡ä¸­çš„3Då¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15423v1" onclick="toggleFavorite(this, '2512.15423v1', 'Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251215581v1-imkd-intensity-aware-multi-level-knowledge-distillation-for-camera-r.html">IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</a></td>
  <td>æå‡ºIMKDï¼Œé€šè¿‡å¼ºåº¦æ„ŸçŸ¥å¤šå±‚çŸ¥è¯†è’¸é¦æå‡é›·è¾¾-ç›¸æœºèåˆ3Dç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15581v1" onclick="toggleFavorite(this, '2512.15581v1', 'IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251215577v1-moonseg3r-monocular-online-zero-shot-segment-anything-in-3d-with-rec.html">MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors</a></td>
  <td>MoonSeg3Rï¼šåˆ©ç”¨é‡å»ºåŸºç¡€å…ˆéªŒå®ç°å•ç›®åœ¨çº¿é›¶æ ·æœ¬3Dåˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15577v1" onclick="toggleFavorite(this, '2512.15577v1', 'MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251215396v1-smart-semantic-matching-contrastive-learning-for-partially-view-alig.html">SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</a></td>
  <td>æå‡ºSMARTæ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰åŒ¹é…å¯¹æ¯”å­¦ä¹ è§£å†³éƒ¨åˆ†è§†å›¾å¯¹é½èšç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15396v1" onclick="toggleFavorite(this, '2512.15396v1', 'SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/251215508v1-off-the-grid-detection-of-primitives-for-feed-forward-3d-gaussian-sp.html">Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</a></td>
  <td>æå‡ºä¸€ç§æ–°æ¶æ„ä»¥è§£å†³3Dé«˜æ–¯åŸè¯­æ£€æµ‹çš„åƒç´ å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15508v1" onclick="toggleFavorite(this, '2512.15508v1', 'Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251215048v1-mvgsr-multi-view-consistent-3d-gaussian-super-resolution-via-epipola.html">MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</a></td>
  <td>æå‡ºMVGSRï¼Œé€šè¿‡æçº¿å¼•å¯¼å®ç°å¤šè§†è§’ä¸€è‡´çš„3Dé«˜æ–¯è¶…åˆ†è¾¨ç‡é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15048v1" onclick="toggleFavorite(this, '2512.15048v1', 'MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251215711v1-gaussian-pixel-codec-avatars-a-hybrid-representation-for-efficient-r.html">Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</a></td>
  <td>æå‡ºé«˜æ–¯åƒç´ ç¼–è§£ç å¤´åƒ(GPiCA)ï¼Œç”¨äºé«˜æ•ˆæ¸²æŸ“çš„æ··åˆäººåƒè¡¨ç¤º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15711v1" onclick="toggleFavorite(this, '2512.15711v1', 'Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251215716v1-spatia-video-generation-with-updatable-spatial-memory.html">Spatia: Video Generation with Updatable Spatial Memory</a></td>
  <td>Spatiaï¼šåˆ©ç”¨å¯æ›´æ–°ç©ºé—´è®°å¿†å®ç°è§†é¢‘ç”Ÿæˆï¼Œæå‡æ—¶ç©ºä¸€è‡´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15716v1" onclick="toggleFavorite(this, '2512.15716v1', 'Spatia: Video Generation with Updatable Spatial Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251215445v1-st-detrack-identity-preserving-branch-tracking-in-entangled-plant-ca.html">ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence</a></td>
  <td>ST-DETrackï¼šåˆ©ç”¨æ—¶ç©ºåŒé‡è¯æ®ï¼Œè§£å†³å¤æ‚æ¤ç‰©å† å±‚ä¸­åˆ†æ”¯çš„èº«ä»½ä¿æŒè·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15445v1" onclick="toggleFavorite(this, '2512.15445v1', 'ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251215635v1-ic-effect-precise-and-efficient-video-effects-editing-via-in-context.html">IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</a></td>
  <td>æå‡ºIC-Effectï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°ç²¾ç¡®é«˜æ•ˆçš„è§†é¢‘ç‰¹æ•ˆç¼–è¾‘</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15635v1" onclick="toggleFavorite(this, '2512.15635v1', 'IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251215055v1-asynchronous-event-stream-noise-filtering-for-high-frequency-structu.html">Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement</a></td>
  <td>æå‡ºåŸºäºäº‹ä»¶ç›¸æœºå’ŒLEDæ ‡è®°çš„å¼‚æ­¥äº‹ä»¶æµå™ªå£°æ»¤æ³¢æ–¹æ³•ï¼Œç”¨äºé«˜é¢‘ç»“æ„å½¢å˜æµ‹é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15055v1" onclick="toggleFavorite(this, '2512.15055v1', 'Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251215708v1-multi-view-foundation-models.html">Multi-View Foundation Models</a></td>
  <td>æå‡ºå¤šè§†è§’åŸºç¡€æ¨¡å‹ï¼Œæå‡å¤šè§†è§’åœºæ™¯ä¸‹ç‰¹å¾ä¸€è‡´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">feature matching</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15708v1" onclick="toggleFavorite(this, '2512.15708v1', 'Multi-View Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251215512v1-vaas-vision-attention-anomaly-scoring-for-image-manipulation-detecti.html">VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics</a></td>
  <td>VAASï¼šç”¨äºæ•°å­—å–è¯ä¸­å›¾åƒç¯¡æ”¹æ£€æµ‹çš„è§†è§‰æ³¨æ„åŠ›å¼‚å¸¸è¯„åˆ†æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15512v1" onclick="toggleFavorite(this, '2512.15512v1', 'VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251215707v1-gatefusion-hierarchical-gated-cross-modal-fusion-for-active-speaker-.html">GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</a></td>
  <td>æå‡ºGateFusionï¼Œé€šè¿‡åˆ†å±‚é—¨æ§è·¨æ¨¡æ€èåˆæå‡ä¸»åŠ¨è¯´è¯äººæ£€æµ‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">Ego4D</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15707v1" onclick="toggleFavorite(this, '2512.15707v1', 'GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251215524v1-dex-portrait-disentangled-and-expressive-portrait-animation-via-expl.html">DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations</a></td>
  <td>DeX-Portraitï¼šé€šè¿‡æ˜¾å¼å’Œéšå¼è¿åŠ¨è¡¨å¾å®ç°è§£è€¦ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„äººåƒåŠ¨ç”»</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.15524v1" onclick="toggleFavorite(this, '2512.15524v1', 'DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)