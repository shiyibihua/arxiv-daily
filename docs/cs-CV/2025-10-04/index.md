---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-04
---

# cs.CVï¼ˆ2025-10-04ï¼‰

ğŸ“Š å…± **14** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251003827v1-libero-pro-towards-robust-and-fair-evaluation-of-vision-language-act.html">LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization</a></td>
  <td>æå‡ºLIBERO-PROä»¥è§£å†³ç°æœ‰VLAæ¨¡å‹è¯„ä¼°ä¸å…¬æ­£é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03827v1" onclick="toggleFavorite(this, '2510.03827v1', 'LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251003896v1-bridge-thinking-and-acting-unleashing-physical-potential-of-vlm-with.html">Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert</a></td>
  <td>æå‡ºåŸºäºå¯æ³›åŒ–åŠ¨ä½œä¸“å®¶çš„æ¡†æ¶ï¼Œæå‡VLMåœ¨ç‰©ç†ä¸–ç•Œçš„åŠ¨ä½œæ‰§è¡Œèƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03896v1" onclick="toggleFavorite(this, '2510.03896v1', 'Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251003880v1-exploring-instruction-data-quality-for-explainable-image-quality-ass.html">Exploring Instruction Data Quality for Explainable Image Quality Assessment</a></td>
  <td>é’ˆå¯¹å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°ï¼Œæå‡ºåŸºäºèšç±»çš„æ•°æ®é€‰æ‹©æ–¹æ³•IQA-Selectï¼Œæå‡æ•°æ®æ•ˆç‡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03880v1" onclick="toggleFavorite(this, '2510.03880v1', 'Exploring Instruction Data Quality for Explainable Image Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251003955v1-harnessing-synthetic-preference-data-for-enhancing-temporal-understa.html">Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs</a></td>
  <td>TimeWarpï¼šåˆ©ç”¨åˆæˆåå¥½æ•°æ®å¢å¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´ç†è§£èƒ½åŠ›</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03955v1" onclick="toggleFavorite(this, '2510.03955v1', 'Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251003921v1-talking-tennis-language-feedback-from-3d-biomechanical-action-recogn.html">Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition</a></td>
  <td>æå‡ºåŸºäº3Dç”Ÿç‰©åŠ›å­¦åŠ¨ä½œè¯†åˆ«çš„ç½‘çƒåŠ¨ä½œè¯­è¨€åé¦ˆæ¡†æ¶ï¼Œæå‡è®­ç»ƒæ•ˆæœã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03921v1" onclick="toggleFavorite(this, '2510.03921v1', 'Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251003874v1-dhqa-4d-perceptual-quality-assessment-of-dynamic-4d-digital-human.html">DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human</a></td>
  <td>æå‡ºDHQA-4Dæ•°æ®é›†ä¸DynaMesh-Rateræ¨¡å‹ï¼Œç”¨äºåŠ¨æ€4Dæ•°å­—äººæ„ŸçŸ¥è´¨é‡è¯„ä¼°</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03874v1" onclick="toggleFavorite(this, '2510.03874v1', 'DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251003751v1-the-overlooked-value-of-test-time-reference-sets-in-visual-place-rec.html">The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</a></td>
  <td>æå‡ºå‚è€ƒé›†å¾®è°ƒæ–¹æ³•ï¼Œæå‡è§†è§‰å®šä½åœ¨è·¨åŸŸåœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03751v1" onclick="toggleFavorite(this, '2510.03751v1', 'The Overlooked Value of Test-time Reference Sets in Visual Place Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/251003853v1-uground-towards-unified-visual-grounding-with-unrolled-transformers.html">UGround: Towards Unified Visual Grounding with Unrolled Transformers</a></td>
  <td>UGroundï¼šæå‡ºåŸºäºè§£ç¼ Transformerçš„ç»Ÿä¸€è§†è§‰å®šä½æ¡†æ¶ï¼Œè§£å†³è¯¯å·®ç´¯ç§¯å’Œç¼ºä¹ç©ºé—´ä¿¡æ¯é—®é¢˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03853v1" onclick="toggleFavorite(this, '2510.03853v1', 'UGround: Towards Unified Visual Grounding with Unrolled Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251003821v1-contrastive-sde-guiding-stochastic-differential-equations-with-contr.html">Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation</a></td>
  <td>æå‡ºContrastive-SDEï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¼•å¯¼éšæœºå¾®åˆ†æ–¹ç¨‹ï¼Œè§£å†³éé…å¯¹å›¾åƒè½¬æ¢é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03821v1" onclick="toggleFavorite(this, '2510.03821v1', 'Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251006254v1-enhanced-self-distillation-framework-for-efficient-spiking-neural-ne.html">Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training</a></td>
  <td>æå‡ºå¢å¼ºå‹è‡ªè’¸é¦æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆè„‰å†²ç¥ç»ç½‘ç»œè®­ç»ƒ</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.06254v1" onclick="toggleFavorite(this, '2510.06254v1', 'Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251003606v1-unsupervised-transformer-pre-training-for-images-self-distillation-m.html">Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops</a></td>
  <td>DINOv2æ·±åº¦è§£è¯»ï¼šéç›‘ç£Transformeré¢„è®­ç»ƒï¼Œè‡ªè’¸é¦ä¸å‡å€¼æ•™å¸ˆæ–¹æ³•</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03606v1" onclick="toggleFavorite(this, '2510.03606v1', 'Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/251003858v1-cross-view-open-vocabulary-object-detection-in-aerial-imagery.html">Cross-View Open-Vocabulary Object Detection in Aerial Imagery</a></td>
  <td>æå‡ºè·¨è§†è§’å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œè§£å†³èˆªæ‹å›¾åƒç›®æ ‡è¯†åˆ«éš¾é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03858v1" onclick="toggleFavorite(this, '2510.03858v1', 'Cross-View Open-Vocabulary Object Detection in Aerial Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251003857v1-optimized-minimal-4d-gaussian-splatting.html">Optimized Minimal 4D Gaussian Splatting</a></td>
  <td>OMG4ï¼šä¼˜åŒ–æœ€å°4Dé«˜æ–¯æº…å°„ï¼Œæ˜¾è‘—é™ä½åŠ¨æ€åœºæ™¯è¡¨ç¤ºçš„å­˜å‚¨å¼€é”€ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03857v1" onclick="toggleFavorite(this, '2510.03857v1', 'Optimized Minimal 4D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251003909v1-generating-human-motion-videos-using-a-cascaded-text-to-video-framew.html">Generating Human Motion Videos using a Cascaded Text-to-Video Framework</a></td>
  <td>æå‡ºCAMEOçº§è”æ¡†æ¶ï¼Œç”¨äºä»æ–‡æœ¬ç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨è§†é¢‘</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03909v1" onclick="toggleFavorite(this, '2510.03909v1', 'Generating Human Motion Videos using a Cascaded Text-to-Video Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)