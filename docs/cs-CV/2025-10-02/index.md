---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-02
---

# cs.CVï¼ˆ2025-10-02ï¼‰

ğŸ“Š å…± **33** ç¯‡è®ºæ–‡
 | ğŸ”— **9** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251001576v1-guiding-multimodal-large-language-models-with-blind-and-low-vision-p.html">Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations</a></td>
  <td>åˆ©ç”¨ç›²äººå’Œä½è§†åŠ›äººç¾¤è§†è§‰é—®é¢˜å¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä¸»åŠ¨è§†è§‰è§£è¯»</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01576v1" onclick="toggleFavorite(this, '2510.01576v1', 'Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251002311v1-inferring-dynamic-physical-properties-from-video-foundation-models.html">Inferring Dynamic Physical Properties from Video Foundation Models</a></td>
  <td>åˆ©ç”¨è§†é¢‘åŸºç¡€æ¨¡å‹æ¨æ–­è§†é¢‘ä¸­çš„åŠ¨æ€ç‰©ç†å±æ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02311v1" onclick="toggleFavorite(this, '2510.02311v1', 'Inferring Dynamic Physical Properties from Video Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251001582v1-imagenet-think-250k-a-large-scale-synthetic-dataset-for-multimodal-r.html">ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models</a></td>
  <td>æå‡ºImageNet-Think-250Kï¼Œç”¨äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01582v1" onclick="toggleFavorite(this, '2510.01582v1', 'ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251002270v1-microclip-unsupervised-clip-adaptation-via-coarse-fine-token-fusion-.html">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a></td>
  <td>microCLIPï¼šé€šè¿‡ç²—ç»†ç²’åº¦Tokenèåˆå®ç°æ— ç›‘ç£CLIPå¾®è°ƒï¼Œæå‡ç»†ç²’åº¦å›¾åƒåˆ†ç±»æ€§èƒ½</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02270v1" onclick="toggleFavorite(this, '2510.02270v1', 'microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251002001v2-generating-findings-for-jaw-cysts-in-dental-panoramic-radiographs-us.html">Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework</a></td>
  <td>åˆ©ç”¨GPT-4oå’ŒSLSOæ¡†æ¶è‡ªåŠ¨ç”Ÿæˆç‰™ç§‘å…¨æ™¯ç‰‡ä¸­é¢Œéª¨å›Šè‚¿çš„è¯Šæ–­ç»“æœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02001v2" onclick="toggleFavorite(this, '2510.02001v2', 'Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251001954v1-patch-as-decodable-token-towards-unified-multi-modal-vision-tasks-in.html">Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs</a></td>
  <td>æå‡ºPatch-as-Decodable-Token (PaDT)ï¼Œå®ç°MLLMä¸­ç»Ÿä¸€çš„å¤šæ¨¡æ€è§†è§‰ä»»åŠ¡å¤„ç†ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01954v1" onclick="toggleFavorite(this, '2510.01954v1', 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251001546v1-growing-visual-generative-capacity-for-pre-trained-mllms.html">Growing Visual Generative Capacity for Pre-Trained MLLMs</a></td>
  <td>æå‡ºBridgeï¼šä¸€ç§åŸºäºæ··åˆTransformeræ¶æ„çš„çº¯è‡ªå›å½’ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡è§†è§‰ç”Ÿæˆèƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01546v1" onclick="toggleFavorite(this, '2510.01546v1', 'Growing Visual Generative Capacity for Pre-Trained MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251002571v1-how-confident-are-video-models-empowering-video-models-to-express-th.html">How Confident are Video Models? Empowering Video Models to Express their Uncertainty</a></td>
  <td>æå‡ºä¸€ç§æ¡†æ¶ä»¥é‡åŒ–è§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02571v1" onclick="toggleFavorite(this, '2510.02571v1', 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251002295v1-videonsa-native-sparse-attention-scales-video-understanding.html">VideoNSA: Native Sparse Attention Scales Video Understanding</a></td>
  <td>æå‡ºVideoNSAï¼Œé€šè¿‡åŸç”Ÿç¨€ç–æ³¨æ„åŠ›æœ‰æ•ˆæ‰©å±•è§†é¢‘ç†è§£æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02295v1" onclick="toggleFavorite(this, '2510.02295v1', 'VideoNSA: Native Sparse Attention Scales Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251002282v2-vidguard-r1-ai-generated-video-detection-and-explanation-via-reasoni.html">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a></td>
  <td>VidGuard-R1ï¼šåˆ©ç”¨æ¨ç†MLLMå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡ŒAIç”Ÿæˆè§†é¢‘æ£€æµ‹ä¸è§£é‡Š</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02282v2" onclick="toggleFavorite(this, '2510.02282v2', 'VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251002262v1-from-frames-to-clips-efficient-key-clip-selection-for-long-form-vide.html">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a></td>
  <td>æå‡ºF2Cï¼šé€šè¿‡é«˜æ•ˆå…³é”®ç‰‡æ®µé€‰æ‹©æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02262v1" onclick="toggleFavorite(this, '2510.02262v1', 'From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251002114v1-frieren-federated-learning-with-vision-language-regularization-for-s.html">FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation</a></td>
  <td>æå‡ºFRIERENæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ­£åˆ™åŒ–è¿›è¡Œè”é‚¦å­¦ä¹ è¯­ä¹‰åˆ†å‰²ï¼Œè§£å†³æ— æ ‡ç­¾æ•°æ®ä¸‹çš„é¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02114v1" onclick="toggleFavorite(this, '2510.02114v1', 'FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251003341v1-opusanimation-code-based-dynamic-chart-generation.html">OpusAnimation: Code-Based Dynamic Chart Generation</a></td>
  <td>æå‡ºDCG-BenchåŸºå‡†å’ŒQwen2.5-VL-DCG-3Bæ¨¡å‹ï¼Œç”¨äºè§£å†³åŠ¨æ€å›¾è¡¨ç”Ÿæˆä»»åŠ¡ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03341v1" onclick="toggleFavorite(this, '2510.03341v1', 'OpusAnimation: Code-Based Dynamic Chart Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251001623v1-vla-r1-enhancing-reasoning-in-vision-language-action-models.html">VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</a></td>
  <td>æå‡ºVLA-R1ä»¥è§£å†³è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01623v1" onclick="toggleFavorite(this, '2510.01623v1', 'VLA-R1: Enhancing Reasoning in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251002186v1-geopurify-a-data-efficient-geometric-distillation-framework-for-open.html">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a></td>
  <td>GeoPurifyé€šè¿‡å‡ ä½•è’¸é¦ï¼Œä»¥æ•°æ®é«˜æ•ˆçš„æ–¹å¼å®ç°å¼€æ”¾è¯æ±‡3Dåˆ†å‰²ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02186v1" onclick="toggleFavorite(this, '2510.02186v1', 'GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251002240v1-rewardmap-tackling-sparse-rewards-in-fine-grained-visual-reasoning-v.html">RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning</a></td>
  <td>æå‡ºRewardMapï¼Œé€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è§£å†³ç»†ç²’åº¦è§†è§‰æ¨ç†ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02240v1" onclick="toggleFavorite(this, '2510.02240v1', 'RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251002287v1-multimodal-action-conditioned-video-generation.html">MultiModal Action Conditioned Video Generation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€åŠ¨ä½œæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæå‡æœºå™¨äººç²¾ç»†æ“ä½œçš„æ¨¡æ‹Ÿç²¾åº¦</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02287v1" onclick="toggleFavorite(this, '2510.02287v1', 'MultiModal Action Conditioned Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251002253v2-dragflow-unleashing-dit-priors-with-region-based-supervision-for-dra.html">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a></td>
  <td>DragFlowï¼šåˆ©ç”¨åŒºåŸŸç›‘ç£é‡Šæ”¾DiTå…ˆéªŒï¼Œå®ç°å“è¶Šçš„æ‹–æ‹½ç¼–è¾‘æ•ˆæœ</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02253v2" onclick="toggleFavorite(this, '2510.02253v2', 'DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251001912v1-flow-matching-guided-deep-unfolding-for-hyperspectral-image-reconstr.html">Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction</a></td>
  <td>æå‡ºFlow-Matchingå¼•å¯¼çš„æ·±åº¦å±•å¼€ç½‘ç»œFMUï¼Œç”¨äºé«˜å…‰è°±å›¾åƒé‡å»ºã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01912v1" onclick="toggleFavorite(this, '2510.01912v1', 'Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251001540v1-towards-better-optimization-for-listwise-preference-in-diffusion-mod.html">Towards Better Optimization For Listwise Preference in Diffusion Models</a></td>
  <td>æå‡ºDiffusion-LPOï¼Œç”¨äºæ‰©æ•£æ¨¡å‹ä¸­åŸºäºåˆ—è¡¨åå¥½çš„ä¼˜åŒ–ï¼Œæå‡å›¾åƒè´¨é‡å’Œåå¥½å¯¹é½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01540v1" onclick="toggleFavorite(this, '2510.01540v1', 'Towards Better Optimization For Listwise Preference in Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251001662v1-discrete-facial-encoding-a-framework-for-data-driven-facial-display-.html">Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery</a></td>
  <td>æå‡ºç¦»æ•£é¢éƒ¨ç¼–ç (DFE)ï¼Œç”¨äºæ•°æ®é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…å‘ç°ï¼Œæ›¿ä»£FACSã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01662v1" onclick="toggleFavorite(this, '2510.01662v1', 'Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251002561v1-oracle-rlaif-an-improved-fine-tuning-framework-for-multi-modal-video.html">Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback</a></td>
  <td>æå‡ºOracle-RLAIFæ¡†æ¶ï¼Œé€šè¿‡æ’åºåé¦ˆå¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€è§†é¢‘æ¨¡å‹æ€§èƒ½ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02561v1" onclick="toggleFavorite(this, '2510.02561v1', 'Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251001681v1-look-less-reason-more-rollout-guided-adaptive-pixel-space-reasoning.html">Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning</a></td>
  <td>æå‡ºåŸºäºRolloutå¼•å¯¼çš„è‡ªé€‚åº”åƒç´ ç©ºé—´æ¨ç†æ¡†æ¶ï¼Œæå‡VLMåœ¨ç»†ç²’åº¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01681v1" onclick="toggleFavorite(this, '2510.01681v1', 'Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/251001767v1-lobe-gs-load-balanced-and-efficient-3d-gaussian-splatting-for-large-.html">LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction</a></td>
  <td>LoBE-GSï¼šé¢å‘å¤§è§„æ¨¡åœºæ™¯é‡å»ºçš„è´Ÿè½½å‡è¡¡é«˜æ•ˆ3Dé«˜æ–¯æº…å°„</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01767v1" onclick="toggleFavorite(this, '2510.01767v1', 'LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251002314v1-stealthattack-robust-3d-gaussian-splatting-poisoning-via-density-gui.html">StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions</a></td>
  <td>StealthAttackï¼šæå‡ºä¸€ç§åŸºäºå¯†åº¦å¼•å¯¼çš„3Dé«˜æ–¯æº…å°„éšè”½æŠ•æ¯’æ”»å‡»æ–¹æ³•</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02314v1" onclick="toggleFavorite(this, '2510.02314v1', 'StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251001991v1-4dgs-craft-consistent-and-interactive-4d-gaussian-splatting-editing.html">4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing</a></td>
  <td>æå‡º4DGS-Craftä»¥è§£å†³4Dé«˜æ–¯ç‚¹äº‘ç¼–è¾‘ä¸€è‡´æ€§é—®é¢˜</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01991v1" onclick="toggleFavorite(this, '2510.01991v1', '4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251003348v2-visual-odometry-with-transformers.html">Visual Odometry with Transformers</a></td>
  <td>æå‡ºåŸºäºTransformerçš„è§†è§‰é‡Œç¨‹è®¡VoTï¼Œå®ç°ç«¯åˆ°ç«¯å•ç›®ä½å§¿å›å½’ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.03348v2" onclick="toggleFavorite(this, '2510.03348v2', 'Visual Odometry with Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251002034v1-gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-.html">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a></td>
  <td>GaussianMorphingï¼šæå‡ºç½‘æ ¼å¼•å¯¼çš„3Dé«˜æ–¯æ–¹æ³•ï¼Œå®ç°è¯­ä¹‰æ„ŸçŸ¥çš„ç‰©ä½“å½¢å˜ã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02034v1" onclick="toggleFavorite(this, '2510.02034v1', 'GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251001665v1-non-rigid-structure-from-motion-via-differential-geometry-with-recov.html">Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale</a></td>
  <td>æå‡ºCon-NRSfMï¼Œé€šè¿‡å¯æ¢å¤å…±å½¢å°ºåº¦å¾®åˆ†å‡ ä½•è§£å†³éåˆšæ€§ç»“æ„é‡å»ºé—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01665v1" onclick="toggleFavorite(this, '2510.01665v1', 'Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/251002313v1-clink-chop-thud-learning-object-sounds-from-real-world-interactions.html">Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions</a></td>
  <td>æå‡ºåŸºäºçœŸå®ä¸–ç•Œäº¤äº’å­¦ä¹ ç‰©ä½“å£°éŸ³çš„æ£€æµ‹æ¡†æ¶ï¼Œè§£å†³å£°éŸ³ä¸ç‰©ä½“çš„å…³è”é—®é¢˜ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02313v1" onclick="toggleFavorite(this, '2510.02313v1', 'Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251002601v1-ego-exo-3d-hand-tracking-in-the-wild-with-a-mobile-multi-camera-rig.html">Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig</a></td>
  <td>æå‡ºä¸€ç§ç§»åŠ¨å¤šç›¸æœºç³»ç»Ÿï¼Œç”¨äºåœ¨çœŸå®åœºæ™¯ä¸­è¿›è¡Œego-exo 3Dæ‰‹éƒ¨è¿½è¸ªã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02601v1" onclick="toggleFavorite(this, '2510.02601v1', 'Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>32</td>
  <td><a href="./papers/251002566v1-physhmr-learning-humanoid-control-policies-from-vision-for-physicall.html">PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction</a></td>
  <td>PhysHMRï¼šä»è§†è§‰å­¦ä¹ äººå½¢æ§åˆ¶ç­–ç•¥ï¼Œå®ç°ç‰©ç†ä¸Šåˆç†çš„äººä½“è¿åŠ¨é‡å»º</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02566v1" onclick="toggleFavorite(this, '2510.02566v1', 'PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/251002284v2-learning-to-generate-rigid-body-interactions-with-video-diffusion-mo.html">Learning to Generate Rigid Body Interactions with Video Diffusion Models</a></td>
  <td>KineMaskï¼šåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·æœ‰åˆšä½“äº¤äº’çš„è§†é¢‘</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.02284v2" onclick="toggleFavorite(this, '2510.02284v2', 'Learning to Generate Rigid Body Interactions with Video Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)