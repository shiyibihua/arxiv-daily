---
layout: default
title: Decoding Visual Neural Representations by Multimodal with Dynamic Balancing
---

# Decoding Visual Neural Representations by Multimodal with Dynamic Balancing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03433" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03433v1</a>
  <a href="https://arxiv.org/pdf/2509.03433.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03433v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03433v1', 'Decoding Visual Neural Representations by Multimodal with Dynamic Balancing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaili sun, Xingyu Miao, Bing Zhai, Haoran Duan, Yang Long

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€è§£ç æ¡†æ¶ï¼Œæå‡è„‘ç”µä¿¡å·è§£ç è§†è§‰ç¥ç»è¡¨å¾çš„å‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‘ç”µä¿¡å·è§£ç ` `å¤šæ¨¡æ€èåˆ` `è§†è§‰ç¥ç»è¡¨å¾` `åŠ¨æ€å¹³è¡¡` `è¯­ä¹‰å¯¹åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£ç ä½ä¿¡å™ªæ¯”è„‘ç”µä¿¡å·ä¸­çš„è§†è§‰ç¥ç»è¡¨å¾ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚
2. åˆ©ç”¨æ–‡æœ¬æ¨¡æ€å¢å¼ºè¯­ä¹‰å¯¹åº”ï¼Œè®¾è®¡é€‚é…å™¨æ¨¡å—ç¨³å®šé«˜ç»´è¡¨å¾ï¼Œå¹¶æå‡ºåŠ¨æ€å¹³è¡¡ç­–ç•¥è°ƒæ•´æ¨¡æ€è´¡çŒ®ã€‚
3. åœ¨ThingsEEGæ•°æ®é›†ä¸Šï¼ŒTop-1å‡†ç¡®ç‡æå‡2.0%ï¼ŒTop-5å‡†ç¡®ç‡æå‡4.7%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¡†æ¶ï¼Œé›†æˆäº†è„‘ç”µï¼ˆEEGï¼‰ã€å›¾åƒå’Œæ–‡æœ¬æ•°æ®ï¼Œæ—¨åœ¨ä»ä½ä¿¡å™ªæ¯”çš„è„‘ç”µä¿¡å·ä¸­è§£ç è§†è§‰ç¥ç»è¡¨å¾ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥æ–‡æœ¬æ¨¡æ€ä»¥å¢å¼ºè„‘ç”µä¿¡å·å’Œè§†è§‰å†…å®¹ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚å€ŸåŠ©æ–‡æœ¬æä¾›çš„æ˜¾å¼è¯­ä¹‰æ ‡ç­¾ï¼ŒåŒä¸€ç±»åˆ«çš„å›¾åƒå’Œè„‘ç”µç‰¹å¾å¯ä»¥åœ¨å…±äº«çš„å¤šæ¨¡æ€ç©ºé—´ä¸­ä¸ç›¸åº”çš„æ–‡æœ¬è¡¨å¾æ›´ç´§å¯†åœ°å¯¹é½ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰å’Œæ–‡æœ¬è¡¨å¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€‚é…å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—å‡è½»äº†é«˜ç»´è¡¨å¾çš„ä¸ç¨³å®šæ€§ï¼ŒåŒæ—¶ä¿ƒè¿›äº†è·¨æ¨¡æ€ç‰¹å¾çš„å¯¹é½å’Œèåˆã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£æ–‡æœ¬è¡¨å¾å¼•å…¥çš„å¤šæ¨¡æ€ç‰¹å¾è´¡çŒ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ€ä¸€è‡´æ€§åŠ¨æ€å¹³è¡¡ï¼ˆMCDBï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„è´¡çŒ®æƒé‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§éšæœºæ‰°åŠ¨æ­£åˆ™åŒ–ï¼ˆSPRï¼‰é¡¹ï¼Œé€šè¿‡åœ¨æ¨¡æ€ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥åŠ¨æ€é«˜æ–¯å™ªå£°æ¥å¢å¼ºåŸºäºè¯­ä¹‰æ‰°åŠ¨çš„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ThingsEEGæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Top-1å’ŒTop-5å‡†ç¡®ç‡æŒ‡æ ‡ä¸Šå‡è¶…è¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†2.0ï¼…å’Œ4.7ï¼…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»ä½ä¿¡å™ªæ¯”çš„è„‘ç”µï¼ˆEEGï¼‰ä¿¡å·ä¸­å‡†ç¡®è§£ç è§†è§‰ç¥ç»è¡¨å¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å™ªå£°å¤§çš„è„‘ç”µä¿¡å·æ—¶è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”ç¼ºä¹æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ï¼‰æ¥å¢å¼ºè§£ç æ•ˆæœçš„æœºåˆ¶ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°èåˆè„‘ç”µã€å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶å‡è½»å™ªå£°çš„å½±å“ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ–‡æœ¬æ¨¡æ€ä½œä¸ºæ¡¥æ¢ï¼Œå¢å¼ºè„‘ç”µä¿¡å·å’Œè§†è§‰å†…å®¹ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚é€šè¿‡å°†è„‘ç”µã€å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾æ˜ å°„åˆ°å…±äº«çš„å¤šæ¨¡æ€ç©ºé—´ï¼Œå¯ä»¥åˆ©ç”¨æ–‡æœ¬æä¾›çš„æ˜¾å¼è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼è„‘ç”µä¿¡å·çš„è§£ç è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŠ¨æ€å¹³è¡¡ä¸åŒæ¨¡æ€çš„è´¡çŒ®ï¼Œå¯ä»¥ç¼“è§£ç”±äºæ–‡æœ¬æ¨¡æ€å¼•å…¥çš„ç‰¹å¾ä¸å¹³è¡¡é—®é¢˜ï¼Œä»è€Œæé«˜è§£ç çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šåˆ†åˆ«ä»è„‘ç”µã€å›¾åƒå’Œæ–‡æœ¬æ•°æ®ä¸­æå–ç‰¹å¾ã€‚å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–ã€‚2) é€‚é…å™¨æ¨¡å—ï¼šç”¨äºç¨³å®šé«˜ç»´è¡¨å¾ï¼Œå¹¶ä¿ƒè¿›è·¨æ¨¡æ€ç‰¹å¾çš„å¯¹é½å’Œèåˆã€‚3) å¤šæ¨¡æ€èåˆæ¨¡å—ï¼šå°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå½¢æˆç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨å¾ã€‚4) è§£ç æ¨¡å—ï¼šæ ¹æ®å¤šæ¨¡æ€è¡¨å¾è§£ç è§†è§‰ç¥ç»è¡¨å¾ã€‚5) æ¨¡æ€ä¸€è‡´æ€§åŠ¨æ€å¹³è¡¡ï¼ˆMCDBï¼‰æ¨¡å—ï¼šåŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„è´¡çŒ®æƒé‡ã€‚6) éšæœºæ‰°åŠ¨æ­£åˆ™åŒ–ï¼ˆSPRï¼‰æ¨¡å—ï¼šé€šè¿‡å¼•å…¥åŠ¨æ€é«˜æ–¯å™ªå£°æ¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) å¼•å…¥æ–‡æœ¬æ¨¡æ€æ¥å¢å¼ºè„‘ç”µä¿¡å·å’Œè§†è§‰å†…å®¹ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚2) æå‡ºæ¨¡æ€ä¸€è‡´æ€§åŠ¨æ€å¹³è¡¡ï¼ˆMCDBï¼‰ç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„è´¡çŒ®æƒé‡ï¼Œç¼“è§£æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚3) æå‡ºéšæœºæ‰°åŠ¨æ­£åˆ™åŒ–ï¼ˆSPRï¼‰é¡¹ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šé€‚é…å™¨æ¨¡å—çš„è®¾è®¡æ—¨åœ¨ç¼“è§£é«˜ç»´è¡¨å¾çš„ä¸ç¨³å®šæ€§ï¼Œé€šå¸¸é‡‡ç”¨çº¿æ€§å±‚æˆ–éçº¿æ€§å±‚è¿›è¡Œç‰¹å¾è½¬æ¢ã€‚MCDBç­–ç•¥é€šè¿‡è®¡ç®—æ¯ä¸ªæ¨¡æ€ç‰¹å¾çš„æ¢¯åº¦ï¼Œå¹¶æ ¹æ®æ¢¯åº¦çš„å¤§å°åŠ¨æ€è°ƒæ•´æ¨¡æ€æƒé‡ã€‚SPRæ¨¡å—åˆ™æ˜¯åœ¨æ¨¡æ€ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥åŠ¨æ€é«˜æ–¯å™ªå£°ï¼Œå™ªå£°çš„å¤§å°å¯ä»¥æ ¹æ®è®­ç»ƒçš„è¿›åº¦è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°é€šå¸¸åŒ…æ‹¬åˆ†ç±»æŸå¤±å’Œæ¨¡æ€ä¸€è‡´æ€§æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ThingsEEGæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨Top-1å‡†ç¡®ç‡ä¸Šï¼Œè¯¥æ–¹æ³•æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•æé«˜äº†2.0%ï¼›åœ¨Top-5å‡†ç¡®ç‡ä¸Šï¼Œæé«˜äº†4.7%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è§£ç è„‘ç”µä¿¡å·ä¸­çš„è§†è§‰ç¥ç»è¡¨å¾ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè„‘æœºæ¥å£ï¼ˆBCIï¼‰é¢†åŸŸï¼Œä¾‹å¦‚è¾…åŠ©è§†è§‰éšœç¢äººå£«ç†è§£å‘¨å›´ç¯å¢ƒï¼Œæˆ–ç”¨äºç¥ç»åé¦ˆæ²»ç–—ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°è§£ç è„‘ç”µä¿¡å·ä¸­çš„è§†è§‰ä¿¡æ¯ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆæ§åˆ¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ ä¸ªæ€§åŒ–å’Œæ™ºèƒ½åŒ–çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we propose an innovative framework that integrates EEG, image, and text data, aiming to decode visual neural representations from low signal-to-noise ratio EEG signals. Specifically, we introduce text modality to enhance the semantic correspondence between EEG signals and visual content. With the explicit semantic labels provided by text, image and EEG features of the same category can be more closely aligned with the corresponding text representations in a shared multimodal space. To fully utilize pre-trained visual and textual representations, we propose an adapter module that alleviates the instability of high-dimensional representation while facilitating the alignment and fusion of cross-modal features. Additionally, to alleviate the imbalance in multimodal feature contributions introduced by the textual representations, we propose a Modal Consistency Dynamic Balance (MCDB) strategy that dynamically adjusts the contribution weights of each modality. We further propose a stochastic perturbation regularization (SPR) term to enhance the generalization ability of semantic perturbation-based models by introducing dynamic Gaussian noise in the modality optimization process. The evaluation results on the ThingsEEG dataset show that our method surpasses previous state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by 2.0\% and 4.7\% respectively.

