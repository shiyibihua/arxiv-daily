---
layout: default
title: Empowering Lightweight MLLMs with Reasoning via Long CoT SFT
---

# Empowering Lightweight MLLMs with Reasoning via Long CoT SFT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03321v2</a>
  <a href="https://arxiv.org/pdf/2509.03321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03321v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03321v2', 'Empowering Lightweight MLLMs with Reasoning via Long CoT SFT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Linyu Ou, YuYang Yin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é•¿CoT SFTèµ‹èƒ½è½»é‡çº§MLLMæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è½»é‡çº§MLLM` `æ¨ç†èƒ½åŠ›` `é•¿é“¾å¼æ€è€ƒ` `ç›‘ç£å¼å¾®è°ƒ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæå‡è½»é‡çº§å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚
2. é€šè¿‡é•¿é“¾å¼æ€è€ƒï¼ˆlong CoTï¼‰æ•°æ®çš„ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰æ¥å¢å¼ºMLLMçš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œé•¿CoT SFTèƒ½æ˜¾è‘—æå‡MLLMæ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸ºåç»­å¼ºåŒ–å­¦ä¹ å¥ å®šåŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ å·²ç»å¢å¼ºäº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨å‚æ•°å°‘äº70äº¿çš„è½»é‡çº§å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸Šçš„æœ‰æ•ˆæ€§ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶äº†é•¿é“¾å¼æ€è€ƒï¼ˆlong CoTï¼‰æ•°æ®åœ¨å¢å¼ºæ­¤ç±»MLLMæ¨ç†èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨é•¿CoTæ•°æ®è¿›è¡Œç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥æ˜¾è‘—æé«˜MLLMçš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨åˆå§‹SFTé˜¶æ®µä¹‹åï¼ŒMLLMå¯ä»¥é€šè¿‡åç»­çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè·å¾—é¢å¤–çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œä½¿ç”¨é•¿CoTæ•°æ®è¿›è¡ŒSFTé˜¶æ®µæ˜¯å¼€å‘è½»é‡çº§MLLMæ¨ç†èƒ½åŠ›çš„å…³é”®å…ˆå†³æ¡ä»¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è½»é‡çº§å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨ç›´æ¥åº”ç”¨äºè½»é‡çº§MLLMæ—¶æ•ˆæœä¸ä½³ï¼Œæ— æ³•å……åˆ†æŒ–æ˜å…¶æ¨ç†æ½œåŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆæå‡è½»é‡çº§MLLMçš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é•¿é“¾å¼æ€è€ƒï¼ˆlong CoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»è€Œä½¿MLLMèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚é•¿CoTæ•°æ®æä¾›äº†è¯¦ç»†çš„æ¨ç†æ­¥éª¤ï¼Œæœ‰åŠ©äºæ¨¡å‹ç†è§£é—®é¢˜å¹¶ç”Ÿæˆæ›´åˆç†çš„ç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨é€šè¿‡SFTä¸ºåç»­çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µå¥ å®šåŸºç¡€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯ä½¿ç”¨é•¿CoTæ•°æ®è¿›è¡Œç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶åæ˜¯å¯é€‰çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚SFTé˜¶æ®µçš„ç›®æ ‡æ˜¯ä½¿MLLMå­¦ä¹ é•¿CoTæ•°æ®ä¸­çš„æ¨ç†æ¨¡å¼ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µåˆ™è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å¤æ‚é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼ºè°ƒäº†é•¿CoTæ•°æ®åœ¨è½»é‡çº§MLLMæ¨ç†èƒ½åŠ›æå‡ä¸­çš„å…³é”®ä½œç”¨ã€‚ä¸ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œè®ºæ–‡æå‡ºé¦–å…ˆé€šè¿‡SFTä½¿æ¨¡å‹å…·å¤‡åŸºæœ¬çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶åå†è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨äº†è½»é‡çº§MLLMçš„è®¡ç®—èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åœ¨äºé•¿CoTæ•°æ®çš„é€‰æ‹©å’ŒSFTé˜¶æ®µçš„è®­ç»ƒç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œéœ€è¦é€‰æ‹©åŒ…å«è¯¦ç»†æ¨ç†æ­¥éª¤çš„é•¿CoTæ•°æ®ï¼Œå¹¶è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒå‚æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†å­¦ä¹ åˆ°æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µçš„å…·ä½“å®ç°ï¼ˆä¾‹å¦‚ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼‰ä¹Ÿæ˜¯å½±å“æœ€ç»ˆæ€§èƒ½çš„å…³é”®å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡çš„ä¸»è¦å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é•¿CoT SFTï¼Œè½»é‡çº§MLLMçš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œåœ¨SFTä¹‹åè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé•¿CoT SFTæ˜¯æå‡è½»é‡çº§MLLMæ¨ç†èƒ½åŠ›çš„å…³é”®æ­¥éª¤ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è½»é‡çº§å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ™ºèƒ½åŠ©æ‰‹ã€åµŒå…¥å¼è§†è§‰é—®ç­”ç³»ç»Ÿã€ä»¥åŠèµ„æºå—é™ç¯å¢ƒä¸‹çš„æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡è½»é‡çº§MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å¯é å’Œé«˜æ•ˆï¼Œä»è€Œæ‹“å±•å…¶åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Reinforcement Learning with Verifiable Rewards has enhanced the reasoning of large-scale language models (LLMs), its efficacy for lightweight multimodal language models (MLLMs) with fewer than seven billion parameters remains underexplored. This paper investigates the role of long Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT data significantly improves MLLM reasoning. Furthermore, we observe that after this initial SFT phase, MLLMs can achieve additional performance gains through a subsequent RL stage. We conclude that a SFT stage with long CoT data is a critical prerequisite for developing the reasoning capabilities of lightweight MLLMs.

