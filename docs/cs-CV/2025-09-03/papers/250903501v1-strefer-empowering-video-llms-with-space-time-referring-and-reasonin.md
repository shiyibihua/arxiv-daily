---
layout: default
title: Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data
---

# Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03501" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03501v1</a>
  <a href="https://arxiv.org/pdf/2509.03501.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03501v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03501v1', 'Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles

**åˆ†ç±»**: cs.CV, cs.AI, cs.HC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

**å¤‡æ³¨**: This technical report serves as the archival version of our paper accepted at the ICCV 2025 Workshop. For more information, please visit our project website: https://strefer.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Streferï¼šé€šè¿‡åˆæˆæŒ‡ä»¤æ•°æ®å¢å¼ºè§†é¢‘LLMçš„æ—¶ç©ºæŒ‡ä»£ä¸æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹` `æ—¶ç©ºæ¨ç†` `åˆæˆæ•°æ®` `æŒ‡ä»¤è°ƒä¼˜` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘LLMåœ¨ç»†ç²’åº¦æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤„ç†ä¾èµ–æ—¶åºäº‹ä»¶æˆ–æ‰‹åŠ¿çš„ç©ºé—´æŒ‡ä»£æ—¶ã€‚
2. Streferé€šè¿‡åˆæˆæŒ‡ä»¤æ•°æ®ï¼Œä½¿è§†é¢‘LLMå…·å¤‡æ—¶ç©ºæŒ‡ä»£å’Œæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–ä¸“æœ‰æ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Streferè®­ç»ƒçš„æ¨¡å‹åœ¨æ—¶ç©ºæ¶ˆæ­§ä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿ï¼Œå¹¶æå‡äº†æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸‹ä¸€ä»£AIåŠ©æ‰‹éœ€è¦è¶…è¶Šä¸€èˆ¬çš„è§†é¢‘ç†è§£ï¼Œèƒ½å¤Ÿè§£å†³åŠ¨æ€çœŸå®ç¯å¢ƒä¸­çš„æ—¶ç©ºæŒ‡ä»£é—®é¢˜ã€‚ç°æœ‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video LLM)è™½ç„¶å…·å¤‡ç²—ç²’åº¦çš„ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç»†ç²’åº¦çš„æ—¶ç©ºæ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·æŸ¥è¯¢ä¾èµ–äºåŸºäºæ—¶é—´çš„äº‹ä»¶å‚è€ƒè¿›è¡Œæ—¶é—´é”šå®šï¼Œæˆ–ä¾èµ–äºæ‰‹åŠ¿çº¿ç´¢è¿›è¡Œç©ºé—´é”šå®šä»¥æ¾„æ¸…å¯¹è±¡å‚è€ƒå’Œä½ç½®æ—¶ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Streferï¼Œä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿Video LLMå…·å¤‡æ—¶ç©ºæŒ‡ä»£å’Œæ¨ç†èƒ½åŠ›ã€‚Streferä½¿ç”¨æ•°æ®å¼•æ“ç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®ï¼Œè¯¥å¼•æ“ä¼ªæ³¨é‡Šæ—¶é—´å¯†é›†çš„ç»†ç²’åº¦è§†é¢‘å…ƒæ•°æ®ï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼æ•è·ä¸°å¯Œçš„ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€å¯¹è±¡ã€å®ƒä»¬çš„ä½ç½®ï¼ˆä½œä¸ºmaskletsï¼‰ä»¥åŠå®ƒä»¬çš„åŠ¨ä½œæè¿°å’Œæ—¶é—´çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†Video LLMè§£é‡Šç©ºé—´å’Œæ—¶é—´å‚è€ƒçš„èƒ½åŠ›ï¼ŒåŸ¹å…»äº†æ›´é€šç”¨çš„ã€å…·æœ‰æ—¶ç©ºæ„è¯†çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œçš„AIåŠ©æ‰‹è‡³å…³é‡è¦ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨ä¸éœ€è¦ä¸“æœ‰æ¨¡å‹ã€æ˜‚è´µçš„äººå·¥æ³¨é‡Šæˆ–æ³¨é‡Šå¤§é‡æ–°è§†é¢‘çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨Streferç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶é—´æ¶ˆæ­§çš„ä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹è¡¨ç°å‡ºå¢å¼ºçš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ„ŸçŸ¥åŸºç¡€çš„ã€æŒ‡ä»¤è°ƒä¼˜çš„Video LLMå¥ å®šäº†æ–°çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘LLMéš¾ä»¥å¤„ç†éœ€è¦ç²¾ç¡®å®šä½å’Œæ—¶åºç†è§£çš„æ—¶ç©ºæŒ‡ä»£é—®é¢˜ã€‚å®ƒä»¬æ— æ³•æœ‰æ•ˆåˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶åºäº‹ä»¶å’Œç©ºé—´çº¿ç´¢ï¼ˆå¦‚æ‰‹åŠ¿ï¼‰æ¥è§£æç”¨æˆ·æŸ¥è¯¢ï¼Œå¯¼è‡´åœ¨éœ€è¦ç»†ç²’åº¦æ—¶ç©ºæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚äººå·¥æ ‡æ³¨å¤§é‡è§†é¢‘æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥è¦†ç›–æ‰€æœ‰å¯èƒ½çš„æ—¶ç©ºæŒ‡ä»£æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šStreferçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œè‡ªåŠ¨åˆ›å»ºåŒ…å«ä¸°å¯Œæ—¶ç©ºä¿¡æ¯çš„æŒ‡ä»¤æ•°æ®ã€‚é€šè¿‡ä¼ªæ ‡æ³¨è§†é¢‘å…ƒæ•°æ®ï¼Œæ„å»ºç»“æ„åŒ–çš„æ—¶ç©ºçŸ¥è¯†åº“ï¼Œå¹¶åŸºäºæ­¤ç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤-å“åº”å¯¹ï¼Œä»è€Œè®­ç»ƒè§†é¢‘LLMçš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é¿å…äº†äººå·¥æ ‡æ³¨çš„æˆæœ¬å’Œå±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStreferæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **è§†é¢‘å…ƒæ•°æ®ä¼ªæ ‡æ³¨æ¨¡å—**ï¼šè‡ªåŠ¨æå–è§†é¢‘ä¸­çš„ä¸»ä½“ã€å¯¹è±¡ã€åŠ¨ä½œã€ä½ç½®ç­‰ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆæ—¶é—´çº¿ã€‚2) **æŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¨¡å—**ï¼šåŸºäºä¼ªæ ‡æ³¨çš„å…ƒæ•°æ®ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤-å“åº”å¯¹ï¼Œæ¶µç›–æ—¶ç©ºæŒ‡ä»£ã€æ—¶åºæ¨ç†ç­‰ä»»åŠ¡ã€‚3) **è§†é¢‘LLMè®­ç»ƒæ¨¡å—**ï¼šä½¿ç”¨ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®å¯¹è§†é¢‘LLMè¿›è¡Œå¾®è°ƒï¼Œæå‡å…¶æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šStreferçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åˆæˆæŒ‡ä»¤æ•°æ®çš„ç”Ÿæˆæ–¹å¼ã€‚å®ƒä¸æ˜¯ç®€å•åœ°å¤åˆ¶æˆ–ä¿®æ”¹ç°æœ‰æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ä¼ªæ ‡æ³¨å’Œç»“æ„åŒ–è¡¨ç¤ºï¼Œæ„å»ºäº†åŒ…å«ä¸°å¯Œæ—¶ç©ºä¿¡æ¯çš„çŸ¥è¯†åº“ï¼Œå¹¶åŸºäºæ­¤ç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤æ•°æ®ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡è§†é¢‘LLMçš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œä¸”æ— éœ€äººå·¥æ ‡æ³¨ã€‚

**å…³é”®è®¾è®¡**ï¼šStreferä½¿ç”¨maskletsæ¥è¡¨ç¤ºå¯¹è±¡çš„ä½ç½®ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æè¿°å¯¹è±¡åœ¨è§†é¢‘ä¸­çš„ç©ºé—´ä½ç½®ã€‚åœ¨æŒ‡ä»¤æ•°æ®ç”Ÿæˆæ–¹é¢ï¼ŒStreferé‡‡ç”¨äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬éšæœºé‡‡æ ·ã€æ¨¡æ¿ç”Ÿæˆç­‰ï¼Œä»¥ä¿è¯æ•°æ®çš„å¤šæ ·æ€§å’Œè¦†ç›–æ€§ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚äº¤å‰ç†µæŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Streferç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„è§†é¢‘LLMï¼Œåœ¨æ—¶ç©ºæ¶ˆæ­§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨éœ€è¦ç©ºé—´å’Œæ—¶é—´æ¨ç†çš„ä»»åŠ¡ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æå‡äº†XX%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆæˆæŒ‡ä»¤æ•°æ®åœ¨æå‡è§†é¢‘LLMæ—¶ç©ºæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåç»­ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

StreferæŠ€æœ¯å¯åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€æœºå™¨äººå¯¼èˆªã€è§†é¢‘ç›‘æ§ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½åŠ©æ‰‹å¯ä»¥æ ¹æ®ç”¨æˆ·çš„æ—¶ç©ºæŒ‡ä»¤ï¼Œåœ¨è§†é¢‘ä¸­å®šä½ç‰¹å®šå¯¹è±¡æˆ–äº‹ä»¶ï¼›æœºå™¨äººå¯ä»¥æ ¹æ®è§†é¢‘ä¸­çš„æ‰‹åŠ¿æŒ‡ä»¤ï¼Œæ‰§è¡Œç›¸åº”çš„åŠ¨ä½œï¼›è§†é¢‘ç›‘æ§ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼Œå¹¶å‘å‡ºè­¦æŠ¥ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡AIç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œäº¤äº’èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.

