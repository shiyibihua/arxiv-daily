---
layout: default
title: OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation
---

# OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03498" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03498v3</a>
  <a href="https://arxiv.org/pdf/2509.03498.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03498v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03498v3', 'OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-10-07)

**å¤‡æ³¨**: technical report, project url:https://onecat-ai.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OneCATï¼šæå‡ºçº¯Decoderè‡ªå›å½’å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆç†è§£ã€ç”Ÿæˆä¸ç¼–è¾‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è‡ªå›å½’æ¨¡å‹` `Decoder-Only` `æ··åˆä¸“å®¶` `ç»Ÿä¸€å»ºæ¨¡` `è§†è§‰è¯­è¨€` `å›¾åƒç¼–è¾‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–ViTç­‰å¤–éƒ¨è§†è§‰æ¨¡å—ï¼Œæ¨ç†æ•ˆç‡ä½ï¼Œå°¤å…¶åœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸‹ã€‚
2. OneCATé‡‡ç”¨çº¯Decoderæ¶æ„å’Œæ¨¡æ€ç‰¹å®šMoEï¼Œä»¥è‡ªå›å½’æ–¹å¼ç»Ÿä¸€å»ºæ¨¡ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOneCATåœ¨å¤šæ¨¡æ€ç”Ÿæˆã€ç¼–è¾‘å’Œç†è§£ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰å¼€æºæ¨¡å‹ï¼Œæ€§èƒ½è¾¾åˆ°æ–°é«˜åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºOneCATï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒåœ¨ä¸€ä¸ªæ–°é¢–çš„ã€çº¯Decoder-Only Transformeræ¶æ„ä¸­æ— ç¼é›†æˆäº†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç‹¬ç‰¹åœ°æ¶ˆé™¤äº†æ¨ç†æœŸé—´å¯¹å¤–éƒ¨ç»„ä»¶ï¼ˆå¦‚Vision Transformers (ViT) æˆ–è§†è§‰tokenizerï¼‰çš„éœ€æ±‚ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªç‰¹å®šæ¨¡æ€çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ç»“æ„å®ç°çš„ï¼Œè¯¥ç»“æ„ä½¿ç”¨å•ä¸€çš„è‡ªå›å½’ï¼ˆARï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”åŸç”Ÿæ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é¦–åˆ›äº†ä¸€ç§å¤šå°ºåº¦è§†è§‰è‡ªå›å½’æœºåˆ¶ï¼Œä¸åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶å¤§å¤§å‡å°‘äº†è§£ç æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œçº¯è‡ªå›å½’å»ºæ¨¡ä½œä¸ºç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½çš„å……åˆ†è€Œä¼˜é›…çš„åŸºç¡€ï¼Œå…·æœ‰å¼ºå¤§çš„æ½œåŠ›ã€‚å› æ­¤ï¼ŒOneCAT è®¾å®šäº†æ–°çš„æ€§èƒ½æ ‡å‡†ï¼Œåœ¨å¤šæ¨¡æ€ç”Ÿæˆã€ç¼–è¾‘å’Œç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¼˜äºç°æœ‰çš„å¼€æºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸ä¾èµ–äºé¢å¤–çš„è§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚ViTï¼‰æˆ–è§†è§‰tokenizersï¼Œè¿™å¢åŠ äº†è®¡ç®—å¤æ‚æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚è¿™äº›é¢å¤–çš„ç»„ä»¶ä½¿å¾—æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„æ•ˆç‡é™ä½ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°èåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶å®ç°ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„ç»Ÿä¸€å»ºæ¨¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOneCATçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨çº¯Decoder-Onlyçš„è‡ªå›å½’æ¨¡å‹ï¼Œé¿å…ä½¿ç”¨é¢å¤–çš„è§†è§‰ç¼–ç å™¨æˆ–tokenizerã€‚é€šè¿‡æ¨¡æ€ç‰¹å®šçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ç»“æ„ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å•ä¸€çš„è‡ªå›å½’ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„ç»Ÿä¸€å»ºæ¨¡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼Œå¹¶ç®€åŒ–æ¨¡å‹ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOneCATçš„æ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªçº¯Decoder-Only Transformeræ¨¡å‹ã€‚å®ƒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è¾“å…¥åµŒå…¥å±‚ï¼šå°†ä¸åŒæ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰çš„è¾“å…¥è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚2) æ¨¡æ€ç‰¹å®šçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ï¼šæ¯ä¸ªæ¨¡æ€éƒ½æœ‰è‡ªå·±çš„MoEæ¨¡å—ï¼Œç”¨äºå­¦ä¹ è¯¥æ¨¡æ€çš„è¡¨ç¤ºã€‚3) Transformer Decoderå±‚ï¼šä½¿ç”¨æ ‡å‡†çš„Transformer Decoderå±‚è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ã€‚4) è¾“å‡ºå±‚ï¼šç”Ÿæˆæ–‡æœ¬æˆ–å›¾åƒçš„è¾“å‡ºã€‚æ¨¡å‹ä½¿ç”¨å•ä¸€çš„è‡ªå›å½’ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå³é¢„æµ‹ä¸‹ä¸€ä¸ªtokenæˆ–åƒç´ ã€‚

**å…³é”®åˆ›æ–°**ï¼šOneCATæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶çº¯Decoder-Onlyçš„æ¶æ„å’Œæ¨¡æ€ç‰¹å®šçš„MoEç»“æ„ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOneCATä¸éœ€è¦é¢å¤–çš„è§†è§‰ç¼–ç å™¨æˆ–tokenizerï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒOneCATè¿˜å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦è§†è§‰è‡ªå›å½’æœºåˆ¶ï¼Œå¯ä»¥å‡å°‘è§£ç æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šOneCATçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ¨¡æ€ç‰¹å®šçš„MoEç»“æ„ï¼šæ¯ä¸ªæ¨¡æ€éƒ½æœ‰è‡ªå·±çš„MoEæ¨¡å—ï¼Œå¯ä»¥æ›´å¥½åœ°å­¦ä¹ è¯¥æ¨¡æ€çš„è¡¨ç¤ºã€‚2) å¤šå°ºåº¦è§†è§‰è‡ªå›å½’æœºåˆ¶ï¼šé€šè¿‡åœ¨ä¸åŒå°ºåº¦ä¸Šè¿›è¡Œè‡ªå›å½’å»ºæ¨¡ï¼Œå¯ä»¥å‡å°‘è§£ç æ­¥éª¤ã€‚3) æŸå¤±å‡½æ•°ï¼šä½¿ç”¨æ ‡å‡†çš„è‡ªå›å½’æŸå¤±å‡½æ•°ï¼Œå³é¢„æµ‹ä¸‹ä¸€ä¸ªtokenæˆ–åƒç´ çš„äº¤å‰ç†µæŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OneCATåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒOneCATä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ï¼ŒOneCATåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®çš„ç¼–è¾‘ã€‚åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼ŒOneCATèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å›ç­”ä¸å›¾åƒç›¸å…³çš„é—®é¢˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒOneCATåœ¨å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OneCATå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€å›¾åƒç¼–è¾‘ã€å¤šæ¨¡æ€å¯¹è¯ç­‰ã€‚è¯¥æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œç»Ÿä¸€æ€§ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºèµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚æœªæ¥ï¼ŒOneCATå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œè§†é¢‘ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„å¤šæ¨¡æ€æ™ºèƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding.

