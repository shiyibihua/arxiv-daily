---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-03
---

# cs.CVï¼ˆ2025-09-03ï¼‰

ğŸ“Š å…± **24** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250903421v1-generalist-versus-specialist-vision-foundation-models-for-ocular-dis.html">Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics</a></td>
  <td>é¢†åŸŸä¸“ç²¾çš„RETFoundåœ¨çœ¼ç§‘ç–¾ç—…å’Œçœ¼åŸºå› ç»„å­¦ä»»åŠ¡ä¸­ä¼˜äºé€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03421v1" data-paper-url="./papers/250903421v1-generalist-versus-specialist-vision-foundation-models-for-ocular-dis.html" onclick="toggleFavorite(this, '2509.03421v1', 'Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250903214v1-rtgmff-enhanced-fmri-based-brain-disorder-diagnosis-via-roi-driven-t.html">RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion</a></td>
  <td>æå‡ºRTGMFFæ¡†æ¶ä»¥æå‡fMRIè„‘éƒ¨ç–¾ç—…è¯Šæ–­å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03214v1" data-paper-url="./papers/250903214v1-rtgmff-enhanced-fmri-based-brain-disorder-diagnosis-via-roi-driven-t.html" onclick="toggleFavorite(this, '2509.03214v1', 'RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250902962v1-resilient-multimodal-industrial-surface-defect-detection-with-uncert.html">Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability</a></td>
  <td>æå‡ºä¸€ç§é²æ£’çš„å¤šæ¨¡æ€å·¥ä¸šè¡¨é¢ç¼ºé™·æ£€æµ‹æ–¹æ³•ï¼Œè§£å†³ä¼ æ„Ÿå™¨å¯ç”¨æ€§ä¸ç¡®å®šé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02962v1" data-paper-url="./papers/250902962v1-resilient-multimodal-industrial-surface-defect-detection-with-uncert.html" onclick="toggleFavorite(this, '2509.02962v1', 'Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250903321v2-empowering-lightweight-mllms-with-reasoning-via-long-cot-sft.html">Empowering Lightweight MLLMs with Reasoning via Long CoT SFT</a></td>
  <td>é•¿CoT SFTèµ‹èƒ½è½»é‡çº§MLLMæ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03321v2" data-paper-url="./papers/250903321v2-empowering-lightweight-mllms-with-reasoning-via-long-cot-sft.html" onclick="toggleFavorite(this, '2509.03321v2', 'Empowering Lightweight MLLMs with Reasoning via Long CoT SFT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250903212v1-aiva-an-ai-based-virtual-companion-for-emotion-aware-interaction.html">AIVA: An AI-based Virtual Companion for Emotion-aware Interaction</a></td>
  <td>AIVAï¼šä¸€ç§åŸºäºAIçš„æƒ…æ„Ÿæ„ŸçŸ¥äº¤äº’è™šæ‹ŸåŠ©æ‰‹</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03212v1" data-paper-url="./papers/250903212v1-aiva-an-ai-based-virtual-companion-for-emotion-aware-interaction.html" onclick="toggleFavorite(this, '2509.03212v1', 'AIVA: An AI-based Virtual Companion for Emotion-aware Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250903614v1-teacher-student-model-for-detecting-and-classifying-mitosis-in-the-m.html">Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</a></td>
  <td>æå‡ºåŸºäºTeacher-Studentæ¨¡å‹çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ä¸åˆ†ç±»æ–¹æ³•ï¼Œæå‡é¢†åŸŸæ³›åŒ–æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">teacher-student</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03614v1" data-paper-url="./papers/250903614v1-teacher-student-model-for-detecting-and-classifying-mitosis-in-the-m.html" onclick="toggleFavorite(this, '2509.03614v1', 'Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250903185v2-pporld-ednetldct-a-proximal-policy-optimization-based-reinforcement-.html">PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising</a></td>
  <td>æå‡ºåŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶PPORLD-EDNetLDCTï¼Œç”¨äºè‡ªé€‚åº”ä½å‰‚é‡CTé™å™ªã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03185v2" data-paper-url="./papers/250903185v2-pporld-ednetldct-a-proximal-policy-optimization-based-reinforcement-.html" onclick="toggleFavorite(this, '2509.03185v2', 'PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250903616v1-multi-attribute-bias-mitigation-via-representation-learning.html">Multi Attribute Bias Mitigation via Representation Learning</a></td>
  <td>æå‡ºGMBMæ¡†æ¶ï¼Œé€šè¿‡è¡¨å¾å­¦ä¹ ç¼“è§£è§†è§‰æ¨¡å‹ä¸­çš„å¤šé‡å±æ€§åå·®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03616v1" data-paper-url="./papers/250903616v1-multi-attribute-bias-mitigation-via-representation-learning.html" onclick="toggleFavorite(this, '2509.03616v1', 'Multi Attribute Bias Mitigation via Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250903277v5-pointad-learning-hierarchical-representations-for-zero-shot-3d-anoma.html">PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection</a></td>
  <td>PointAD+ï¼šå­¦ä¹ åˆ†å±‚è¡¨ç¤ºï¼Œå®ç°é›¶æ ·æœ¬3Då¼‚å¸¸æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03277v5" data-paper-url="./papers/250903277v5-pointad-learning-hierarchical-representations-for-zero-shot-3d-anoma.html" onclick="toggleFavorite(this, '2509.03277v5', 'PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250903609v1-towards-efficient-general-feature-prediction-in-masked-skeleton-mode.html">Towards Efficient General Feature Prediction in Masked Skeleton Modeling</a></td>
  <td>æå‡ºé€šç”¨ç‰¹å¾é¢„æµ‹æ¡†æ¶ï¼ŒåŠ é€Ÿå¹¶æå‡æ©ç éª¨éª¼å»ºæ¨¡çš„åŠ¨ä½œè¯†åˆ«æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03609v1" data-paper-url="./papers/250903609v1-towards-efficient-general-feature-prediction-in-masked-skeleton-mode.html" onclick="toggleFavorite(this, '2509.03609v1', 'Towards Efficient General Feature Prediction in Masked Skeleton Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250903113v3-mitigating-multimodal-hallucinations-via-gradient-based-self-reflect.html">Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</a></td>
  <td>æå‡ºåŸºäºæ¢¯åº¦çš„è‡ªåæ€æ–¹æ³•GACDï¼Œç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03113v3" data-paper-url="./papers/250903113v3-mitigating-multimodal-hallucinations-via-gradient-based-self-reflect.html" onclick="toggleFavorite(this, '2509.03113v3', 'Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250903433v1-decoding-visual-neural-representations-by-multimodal-with-dynamic-ba.html">Decoding Visual Neural Representations by Multimodal with Dynamic Balancing</a></td>
  <td>æå‡ºä¸€ç§åŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€è§£ç æ¡†æ¶ï¼Œæå‡è„‘ç”µä¿¡å·è§£ç è§†è§‰ç¥ç»è¡¨å¾çš„å‡†ç¡®æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03433v1" data-paper-url="./papers/250903433v1-decoding-visual-neural-representations-by-multimodal-with-dynamic-ba.html" onclick="toggleFavorite(this, '2509.03433v1', 'Decoding Visual Neural Representations by Multimodal with Dynamic Balancing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250903408v1-scalable-and-loosely-coupled-multimodal-deep-learning-for-breast-can.html">Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping</a></td>
  <td>æå‡ºä¸€ç§å¯æ‰©å±•çš„æ¾è€¦åˆå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¹³è…ºç™Œåˆ†å­äºšå‹åˆ†ç±»ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03408v1" data-paper-url="./papers/250903408v1-scalable-and-loosely-coupled-multimodal-deep-learning-for-breast-can.html" onclick="toggleFavorite(this, '2509.03408v1', 'Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250903498v3-onecat-decoder-only-auto-regressive-model-for-unified-understanding-.html">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a></td>
  <td>OneCATï¼šæå‡ºçº¯Decoderè‡ªå›å½’å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆç†è§£ã€ç”Ÿæˆä¸ç¼–è¾‘</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03498v3" data-paper-url="./papers/250903498v3-onecat-decoder-only-auto-regressive-model-for-unified-understanding-.html" onclick="toggleFavorite(this, '2509.03498v3', 'OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250903494v2-parameter-efficient-adaptation-of-mplug-owl2-via-pixel-level-visual-.html">Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA</a></td>
  <td>æå‡ºåŸºäºåƒç´ çº§è§†è§‰æç¤ºçš„mPLUG-Owl2å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03494v2" data-paper-url="./papers/250903494v2-parameter-efficient-adaptation-of-mplug-owl2-via-pixel-level-visual-.html" onclick="toggleFavorite(this, '2509.03494v2', 'Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250902973v2-instada-augmenting-instance-segmentation-data-with-dual-agent-system.html">InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System</a></td>
  <td>InstaDAï¼šåˆ©ç”¨åŒAgentç³»ç»Ÿå¢å¼ºå®ä¾‹åˆ†å‰²æ•°æ®ï¼Œæ— éœ€è®­ç»ƒã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02973v2" data-paper-url="./papers/250902973v2-instada-augmenting-instance-segmentation-data-with-dual-agent-system.html" onclick="toggleFavorite(this, '2509.02973v2', 'InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250902904v1-high-fidelity-digital-twins-for-bridging-the-sim2real-gap-in-lidar-b.html">High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception</a></td>
  <td>æå‡ºé«˜ä¿çœŸæ•°å­—å­ªç”Ÿæ¡†æ¶ï¼Œè§£å†³LiDARæ„ŸçŸ¥ä¸­Sim2Realè¿ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02904v1" data-paper-url="./papers/250902904v1-high-fidelity-digital-twins-for-bridging-the-sim2real-gap-in-lidar-b.html" onclick="toggleFavorite(this, '2509.02904v1', 'High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250902903v2-urbantwin-building-high-fidelity-digital-twins-for-sim2real-lidar-pe.html">UrbanTwin: Building High-Fidelity Digital Twins for Sim2Real LiDAR Perception and Evaluation</a></td>
  <td>UrbanTwinï¼šæ„å»ºé«˜ä¿çœŸæ•°å­—å­ªç”Ÿï¼Œç”¨äºSim2Real LiDARæ„ŸçŸ¥ä¸è¯„ä¼°</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.02903v2" data-paper-url="./papers/250902903v2-urbantwin-building-high-fidelity-digital-twins-for-sim2real-lidar-pe.html" onclick="toggleFavorite(this, '2509.02903v2', 'UrbanTwin: Building High-Fidelity Digital Twins for Sim2Real LiDAR Perception and Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250903737v1-layoutgkn-graph-similarity-learning-of-floor-plans.html">LayoutGKN: Graph Similarity Learning of Floor Plans</a></td>
  <td>LayoutGKNï¼šé€šè¿‡å›¾ç›¸ä¼¼æ€§å­¦ä¹ æå‡æ¥¼å±‚å¹³é¢å›¾åŒ¹é…æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03737v1" data-paper-url="./papers/250903737v1-layoutgkn-graph-similarity-learning-of-floor-plans.html" onclick="toggleFavorite(this, '2509.03737v1', 'LayoutGKN: Graph Similarity Learning of Floor Plans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250903324v1-infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models.html">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a></td>
  <td>InfraDiffusionï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œæç¤ºåˆ†å‰²å®ç°é›¶æ ·æœ¬æ·±åº¦å›¾ä¿®å¤ï¼Œç”¨äºç¨€ç–åŸºç¡€è®¾æ–½ç‚¹äº‘</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03324v1" data-paper-url="./papers/250903324v1-infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models.html" onclick="toggleFavorite(this, '2509.03324v1', 'InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250903635v1-reg3d-reconstructive-geometry-instruction-tuning-for-3d-scene-unders.html">Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding</a></td>
  <td>æå‡ºReg3Dï¼Œé€šè¿‡é‡å»ºå‡ ä½•æŒ‡ä»¤å¾®è°ƒæå‡3Dåœºæ™¯ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">geometric consistency</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03635v1" data-paper-url="./papers/250903635v1-reg3d-reconstructive-geometry-instruction-tuning-for-3d-scene-unders.html" onclick="toggleFavorite(this, '2509.03635v1', 'Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250903385v1-human-preference-aligned-concept-customization-benchmark-via-decompo.html">Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation</a></td>
  <td>æå‡ºD-GPTScoreï¼Œé€šè¿‡åˆ†è§£è¯„ä¼°è§£å†³æ¦‚å¿µå®šåˆ¶è¯„ä¼°ä¸äººç±»åå¥½ä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multi-person interaction</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03385v1" data-paper-url="./papers/250903385v1-human-preference-aligned-concept-customization-benchmark-via-decompo.html" onclick="toggleFavorite(this, '2509.03385v1', 'Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250903501v1-strefer-empowering-video-llms-with-space-time-referring-and-reasonin.html">Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</a></td>
  <td>Streferï¼šé€šè¿‡åˆæˆæŒ‡ä»¤æ•°æ®å¢å¼ºè§†é¢‘LLMçš„æ—¶ç©ºæŒ‡ä»£ä¸æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03501v1" data-paper-url="./papers/250903501v1-strefer-empowering-video-llms-with-space-time-referring-and-reasonin.html" onclick="toggleFavorite(this, '2509.03501v1', 'Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250903114v1-towards-realistic-hand-object-interaction-with-gravity-field-based-d.html">Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge</a></td>
  <td>æå‡ºé‡åŠ›åœºé©±åŠ¨æ‰©æ•£æ¡¥ä»¥è§£å†³æ‰‹-ç‰©ä½“äº¤äº’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span> <span class="paper-tag">penetration</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.03114v1" data-paper-url="./papers/250903114v1-towards-realistic-hand-object-interaction-with-gravity-field-based-d.html" onclick="toggleFavorite(this, '2509.03114v1', 'Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)