---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-01
---

# cs.CVï¼ˆ2025-10-01ï¼‰

ğŸ“Š å…± **33** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251000413v2-pal-ui-planning-with-active-look-back-for-vision-based-gui-agents.html">PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents</a></td>
  <td>æå‡ºPAL-UIæ¡†æ¶ï¼Œé€šè¿‡ä¸»åŠ¨å›æº¯æœºåˆ¶æå‡è§†è§‰GUI Agentåœ¨é•¿ç¨‹ä»»åŠ¡ä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00413v2" data-paper-url="./papers/251000413v2-pal-ui-planning-with-active-look-back-for-vision-based-gui-agents.html" onclick="toggleFavorite(this, '2510.00413v2', 'PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251000392v1-a-deep-learning-pipeline-for-epilepsy-genomic-analysis-using-gpt-2-x.html">A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL and NVIDIA H100</a></td>
  <td>æå‡ºåŸºäºGPT-2 XLå’ŒNVIDIA H100çš„æ·±åº¦å­¦ä¹ ç®¡çº¿ï¼Œç”¨äºç™«ç—«åŸºå› ç»„åˆ†æã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00392v1" data-paper-url="./papers/251000392v1-a-deep-learning-pipeline-for-epilepsy-genomic-analysis-using-gpt-2-x.html" onclick="toggleFavorite(this, '2510.00392v1', 'A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL and NVIDIA H100')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251000797v1-solar-pv-installation-potential-assessment-on-building-facades-based.html">Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models</a></td>
  <td>æå‡ºSF-SPAæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¯„ä¼°å»ºç­‘ç«‹é¢çš„å…‰ä¼å®‰è£…æ½œåŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00797v1" data-paper-url="./papers/251000797v1-solar-pv-installation-potential-assessment-on-building-facades-based.html" onclick="toggleFavorite(this, '2510.00797v1', 'Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251001513v1-from-videos-to-indexed-knowledge-graphs-framework-to-marry-methods-f.html">From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding</a></td>
  <td>æå‡ºè§†é¢‘åˆ°ç´¢å¼•çŸ¥è¯†å›¾è°±æ¡†æ¶ï¼Œèåˆå¤šæ¨¡æ€å†…å®¹åˆ†æä¸ç†è§£æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01513v1" data-paper-url="./papers/251001513v1-from-videos-to-indexed-knowledge-graphs-framework-to-marry-methods-f.html" onclick="toggleFavorite(this, '2510.01513v1', 'From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251001370v1-spus-a-lightweight-and-parameter-efficient-foundation-model-for-pdes.html">SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs</a></td>
  <td>SPUSï¼šä¸€ç§è½»é‡çº§ä¸”å‚æ•°é«˜æ•ˆçš„åå¾®åˆ†æ–¹ç¨‹åŸºç¡€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01370v1" data-paper-url="./papers/251001370v1-spus-a-lightweight-and-parameter-efficient-foundation-model-for-pdes.html" onclick="toggleFavorite(this, '2510.01370v1', 'SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251000701v1-graph-integrated-multimodal-concept-bottleneck-model.html">Graph Integrated Multimodal Concept Bottleneck Model</a></td>
  <td>æå‡ºMoE-SGTï¼Œé€šè¿‡å›¾Transformerå’Œæ··åˆä¸“å®¶æ¨¡å‹å¢å¼ºå¤šæ¨¡æ€æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼Œæå‡å¤æ‚æ¦‚å¿µæ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00701v1" data-paper-url="./papers/251000701v1-graph-integrated-multimodal-concept-bottleneck-model.html" onclick="toggleFavorite(this, '2510.00701v1', 'Graph Integrated Multimodal Concept Bottleneck Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251000561v1-assessing-foundation-models-for-mold-colony-detection-with-limited-t.html">Assessing Foundation Models for Mold Colony Detection with Limited Training Data</a></td>
  <td>åˆ©ç”¨å°‘é‡è®­ç»ƒæ•°æ®ï¼Œè¯„ä¼°çœŸèŒèŒè½æ£€æµ‹çš„åŸºç¡€æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00561v1" data-paper-url="./papers/251000561v1-assessing-foundation-models-for-mold-colony-detection-with-limited-t.html" onclick="toggleFavorite(this, '2510.00561v1', 'Assessing Foundation Models for Mold Colony Detection with Limited Training Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251000520v1-cardiobench-do-echocardiography-foundation-models-generalize-beyond-.html">CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?</a></td>
  <td>CardioBenchï¼šè¯„ä¼°å¿ƒåŠ¨è¶…å£°å½±åƒåŸºç¡€æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ ‡å‡†åŒ–åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00520v1" data-paper-url="./papers/251000520v1-cardiobench-do-echocardiography-foundation-models-generalize-beyond-.html" onclick="toggleFavorite(this, '2510.00520v1', 'CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251000705v1-training-free-uncertainty-guidance-for-complex-visual-tasks-with-mll.html">Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs</a></td>
  <td>æå‡ºä¸€ç§å…è®­ç»ƒçš„MLLMä¸ç¡®å®šæ€§å¼•å¯¼æ¡†æ¶ï¼Œç”¨äºå¤æ‚è§†è§‰ä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00705v1" data-paper-url="./papers/251000705v1-training-free-uncertainty-guidance-for-complex-visual-tasks-with-mll.html" onclick="toggleFavorite(this, '2510.00705v1', 'Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251001454v1-data-selection-for-fine-tuning-vision-language-models-via-cross-moda.html">Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories</a></td>
  <td>æå‡ºXMASæ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€å¯¹é½è½¨è¿¹è¿›è¡Œè§†è§‰è¯­è¨€æ¨¡å‹é«˜æ•ˆæ•°æ®é€‰æ‹©ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01454v1" data-paper-url="./papers/251001454v1-data-selection-for-fine-tuning-vision-language-models-via-cross-moda.html" onclick="toggleFavorite(this, '2510.01454v1', 'Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251001186v1-imagedit-let-any-subject-transform.html">IMAGEdit: Let Any Subject Transform</a></td>
  <td>IMAGEditï¼šæå‡ºä¸€ç§å…è®­ç»ƒæ¡†æ¶ï¼Œå®ç°ä»»æ„æ•°é‡è§†é¢‘ä¸»ä½“çš„å¤–è§‚å˜æ¢ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01186v1" data-paper-url="./papers/251001186v1-imagedit-let-any-subject-transform.html" onclick="toggleFavorite(this, '2510.01186v1', 'IMAGEdit: Let Any Subject Transform')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251001049v1-keysg-hierarchical-keyframe-based-3d-scene-graphs.html">KeySG: Hierarchical Keyframe-Based 3D Scene Graphs</a></td>
  <td>KeySGï¼šåŸºäºåˆ†å±‚å…³é”®å¸§çš„3Dåœºæ™¯å›¾æ„å»ºï¼Œæå‡è¯­ä¹‰ä¸°å¯Œæ€§å’Œå¯æ‰©å±•æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01049v1" data-paper-url="./papers/251001049v1-keysg-hierarchical-keyframe-based-3d-scene-graphs.html" onclick="toggleFavorite(this, '2510.01049v1', 'KeySG: Hierarchical Keyframe-Based 3D Scene Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251000683v1-protomask-segmentation-guided-prototype-learning.html">ProtoMask: Segmentation-Guided Prototype Learning</a></td>
  <td>ProtoMaskï¼šæå‡ºä¸€ç§åŸºäºåˆ†å‰²å¼•å¯¼çš„åŸå‹å­¦ä¹ æ–¹æ³•ï¼Œæå‡åŸå‹å¯è§£é‡Šæ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00683v1" data-paper-url="./papers/251000683v1-protomask-segmentation-guided-prototype-learning.html" onclick="toggleFavorite(this, '2510.00683v1', 'ProtoMask: Segmentation-Guided Prototype Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251006231v1-cml-bench-a-framework-for-evaluating-and-enhancing-llm-powered-movie.html">CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation</a></td>
  <td>CML-Benchï¼šç”¨äºè¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç”µå½±å‰§æœ¬çš„æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.06231v1" data-paper-url="./papers/251006231v1-cml-bench-a-framework-for-evaluating-and-enhancing-llm-powered-movie.html" onclick="toggleFavorite(this, '2510.06231v1', 'CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251000604v1-disentangling-foreground-and-background-for-vision-language-navigati.html">Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation</a></td>
  <td>æå‡ºCOFAï¼Œé€šè¿‡åœ¨çº¿å¢å¼ºè§£è€¦å‰æ™¯ä¸èƒŒæ™¯ç‰¹å¾ï¼Œæå‡è§†è§‰è¯­è¨€å¯¼èˆªæ³›åŒ–æ€§</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00604v1" data-paper-url="./papers/251000604v1-disentangling-foreground-and-background-for-vision-language-navigati.html" onclick="toggleFavorite(this, '2510.00604v1', 'Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/251000681v1-adaptive-event-stream-slicing-for-open-vocabulary-event-based-object.html">Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation</a></td>
  <td>æå‡ºè‡ªé€‚åº”äº‹ä»¶æµåˆ‡ç‰‡ä¸çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå®ç°å¼€æ”¾è¯æ±‡äº‹ä»¶ç›¸æœºç›®æ ‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00681v1" data-paper-url="./papers/251000681v1-adaptive-event-stream-slicing-for-open-vocabulary-event-based-object.html" onclick="toggleFavorite(this, '2510.00681v1', 'Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251000515v1-efficient-multi-modal-large-language-models-via-progressive-consiste.html">Efficient Multi-modal Large Language Models via Progressive Consistency Distillation</a></td>
  <td>æå‡ºEPICæ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›ä¸€è‡´æ€§è’¸é¦æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00515v1" data-paper-url="./papers/251000515v1-efficient-multi-modal-large-language-models-via-progressive-consiste.html" onclick="toggleFavorite(this, '2510.00515v1', 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251000862v1-gather-scatter-mamba-accelerating-propagation-with-efficient-state-s.html">Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model</a></td>
  <td>æå‡ºGather-Scatter Mambaï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶ä¸é€‰æ‹©æ€§SSMåŠ é€Ÿè§†é¢‘è¶…åˆ†ä¸­çš„æ—¶åºä¼ æ’­ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00862v1" data-paper-url="./papers/251000862v1-gather-scatter-mamba-accelerating-propagation-with-efficient-state-s.html" onclick="toggleFavorite(this, '2510.00862v1', 'Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251000974v1-jepa-t-joint-embedding-predictive-architecture-with-text-fusion-for-.html">JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation</a></td>
  <td>æå‡ºJEPA-Tï¼Œé€šè¿‡æ–‡æœ¬èåˆçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„æå‡å›¾åƒç”Ÿæˆæ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00974v1" data-paper-url="./papers/251000974v1-jepa-t-joint-embedding-predictive-architecture-with-text-fusion-for-.html" onclick="toggleFavorite(this, '2510.00974v1', 'JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251000855v1-can-world-models-benefit-vlms-for-world-dynamics.html">Can World Models Benefit VLMs for World Dynamics?</a></td>
  <td>æå‡ºWorldLMï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹å…ˆéªŒå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒåŠ¨æ€ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00855v1" data-paper-url="./papers/251000855v1-can-world-models-benefit-vlms-for-world-dynamics.html" onclick="toggleFavorite(this, '2510.00855v1', 'Can World Models Benefit VLMs for World Dynamics?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251001183v1-evoworld-evolving-panoramic-world-generation-with-explicit-3d-memory.html">EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</a></td>
  <td>EvoWorldï¼šåˆ©ç”¨æ˜¾å¼3Dè®°å¿†æ¼”åŒ–çš„å…¨æ™¯ä¸–ç•Œç”Ÿæˆæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01183v1" data-paper-url="./papers/251001183v1-evoworld-evolving-panoramic-world-generation-with-explicit-3d-memory.html" onclick="toggleFavorite(this, '2510.01183v1', 'EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251000837v1-feature-identification-for-hierarchical-contrastive-learning.html">Feature Identification for Hierarchical Contrastive Learning</a></td>
  <td>æå‡ºä¸¤ç§å±‚çº§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å±‚çº§å…³ç³»æå‡ç»†ç²’åº¦åˆ†ç±»æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00837v1" data-paper-url="./papers/251000837v1-feature-identification-for-hierarchical-contrastive-learning.html" onclick="toggleFavorite(this, '2510.00837v1', 'Feature Identification for Hierarchical Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251001009v1-povqa-preference-optimized-video-question-answering-with-rationales-.html">POVQA: Preference-Optimized Video Question Answering with Rationales for Data Efficiency</a></td>
  <td>æå‡ºPOVQAï¼šä¸€ç§æ•°æ®é«˜æ•ˆçš„åå¥½ä¼˜åŒ–è§†é¢‘é—®ç­”æ–¹æ³•ï¼Œåˆ©ç”¨ç†ç”±æå‡æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01009v1" data-paper-url="./papers/251001009v1-povqa-preference-optimized-video-question-answering-with-rationales-.html" onclick="toggleFavorite(this, '2510.01009v1', 'POVQA: Preference-Optimized Video Question Answering with Rationales for Data Efficiency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/251000506v1-affordance-guided-diffusion-prior-for-3d-hand-reconstruction.html">Affordance-Guided Diffusion Prior for 3D Hand Reconstruction</a></td>
  <td>æå‡ºåŸºäºå¯ä¾›æ€§çš„æ‰©æ•£å…ˆéªŒï¼Œç”¨äºè§£å†³3Dæ‰‹éƒ¨é‡å»ºä¸­ä¸¥é‡é®æŒ¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">affordance</span> <span class="paper-tag">HOI</span> <span class="paper-tag">affordance-aware</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00506v1" data-paper-url="./papers/251000506v1-affordance-guided-diffusion-prior-for-3d-hand-reconstruction.html" onclick="toggleFavorite(this, '2510.00506v1', 'Affordance-Guided Diffusion Prior for 3D Hand Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251000818v1-phrasestereo-the-first-open-vocabulary-stereo-image-segmentation-dat.html">PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation Dataset</a></td>
  <td>æå‡ºPhraseStereoï¼šé¦–ä¸ªå¼€æ”¾è¯æ±‡ç«‹ä½“å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼Œä¿ƒè¿›å¤šæ¨¡æ€è¯­ä¹‰ç†è§£ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00818v1" data-paper-url="./papers/251000818v1-phrasestereo-the-first-open-vocabulary-stereo-image-segmentation-dat.html" onclick="toggleFavorite(this, '2510.00818v1', 'PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251001119v1-instant4d-4d-gaussian-splatting-in-minutes.html">Instant4D: 4D Gaussian Splatting in Minutes</a></td>
  <td>Instant4Dï¼šåˆ†é’Ÿçº§å®ç°åŸºäºå•ç›®è§†é¢‘çš„4Dé«˜æ–¯æº…å°„åŠ¨æ€åœºæ™¯é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01119v1" data-paper-url="./papers/251001119v1-instant4d-4d-gaussian-splatting-in-minutes.html" onclick="toggleFavorite(this, '2510.01119v1', 'Instant4D: 4D Gaussian Splatting in Minutes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251000652v1-otter-open-tagging-via-text-image-representation-for-multi-modal-und.html">OTTER: Open-Tagging via Text-Image Representation for Multi-modal Understanding</a></td>
  <td>OTTERï¼šé€šè¿‡æ–‡æœ¬-å›¾åƒè¡¨å¾è¿›è¡Œå¼€æ”¾æ ‡ç­¾å¤šæ¨¡æ€ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00652v1" data-paper-url="./papers/251000652v1-otter-open-tagging-via-text-image-representation-for-multi-modal-und.html" onclick="toggleFavorite(this, '2510.00652v1', 'OTTER: Open-Tagging via Text-Image Representation for Multi-modal Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/251001362v1-evostruggle-a-dataset-capturing-the-evolution-of-struggle-across-act.html">EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels</a></td>
  <td>EvoStruggleï¼šæ„å»ºæŠ€èƒ½å­¦ä¹ è¿‡ç¨‹ä¸­æŒ£æ‰æ¼”å˜æ•°æ®é›†ï¼Œç”¨äºæå‡è¾…åŠ©ç³»ç»Ÿæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01362v1" data-paper-url="./papers/251001362v1-evostruggle-a-dataset-capturing-the-evolution-of-struggle-across-act.html" onclick="toggleFavorite(this, '2510.01362v1', 'EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251001174v1-code2video-a-code-centric-paradigm-for-educational-video-generation.html">Code2Video: A Code-centric Paradigm for Educational Video Generation</a></td>
  <td>æå‡ºCode2Videoæ¡†æ¶ï¼Œé€šè¿‡å¯æ‰§è¡Œä»£ç ç”Ÿæˆä¸“ä¸šæ•™è‚²è§†é¢‘ï¼Œæå‡å¯æ§æ€§å’Œæ•™å­¦è´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.01174v1" data-paper-url="./papers/251001174v1-code2video-a-code-centric-paradigm-for-educational-video-generation.html" onclick="toggleFavorite(this, '2510.01174v1', 'Code2Video: A Code-centric Paradigm for Educational Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251000483v1-mathsticks-a-benchmark-for-visual-symbolic-compositional-reasoning-w.html">MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles</a></td>
  <td>æå‡ºMathSticksï¼šä¸€ä¸ªç”¨äºè§†è§‰ç¬¦å·ç»„åˆæ¨ç†çš„ç«æŸ´æ£è°œé¢˜åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00483v1" data-paper-url="./papers/251000483v1-mathsticks-a-benchmark-for-visual-symbolic-compositional-reasoning-w.html" onclick="toggleFavorite(this, '2510.00483v1', 'MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/251000578v1-arbitrary-generative-video-interpolation.html">Arbitrary Generative Video Interpolation</a></td>
  <td>æå‡ºArbInterpï¼Œå®ç°ä»»æ„æ—¶é—´æˆ³å’Œé•¿åº¦çš„ç”Ÿæˆå¼è§†é¢‘æ’å¸§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00578v1" data-paper-url="./papers/251000578v1-arbitrary-generative-video-interpolation.html" onclick="toggleFavorite(this, '2510.00578v1', 'Arbitrary Generative Video Interpolation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251000570v1-adaptive-shared-experts-with-lora-based-mixture-of-experts-for-multi.html">Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning</a></td>
  <td>æå‡ºåŸºäºLoRAçš„è‡ªé€‚åº”å…±äº«ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œæå‡å¤šä»»åŠ¡å­¦ä¹ æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">ASE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00570v1" data-paper-url="./papers/251000570v1-adaptive-shared-experts-with-lora-based-mixture-of-experts-for-multi.html" onclick="toggleFavorite(this, '2510.00570v1', 'Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/251000438v1-bindweave-subject-consistent-video-generation-via-cross-modal-integr.html">BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration</a></td>
  <td>BindWeaveï¼šé€šè¿‡è·¨æ¨¡æ€èåˆå®ç°ä¸»ä½“ä¸€è‡´çš„è§†é¢‘ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.00438v1" data-paper-url="./papers/251000438v1-bindweave-subject-consistent-video-generation-via-cross-modal-integr.html" onclick="toggleFavorite(this, '2510.00438v1', 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)