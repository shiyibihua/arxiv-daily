---
layout: default
title: BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration
---

# BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00438" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00438v1</a>
  <a href="https://arxiv.org/pdf/2510.00438.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00438v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00438v1', 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, Zehuan Yuan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BindWeaveï¼šé€šè¿‡è·¨æ¨¡æ€èåˆå®ç°ä¸»ä½“ä¸€è‡´çš„è§†é¢‘ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `ä¸»ä½“ä¸€è‡´æ€§` `è·¨æ¨¡æ€èåˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ‰©æ•£Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹éš¾ä»¥è§£æå¤æ‚æç¤ºä¸­çš„ç©ºé—´å…³ç³»ã€æ—¶é—´é€»è¾‘å’Œå¤šä¸»ä½“äº¤äº’ï¼Œå¯¼è‡´ä¸»ä½“ä¸€è‡´æ€§è¾ƒå·®ã€‚
2. BindWeaveæå‡ºMLLM-DiTæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè·¨æ¨¡æ€æ¨ç†ï¼Œè§£è€¦ä¸»ä½“è§’è‰²ã€å±æ€§å’Œäº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBindWeaveåœ¨OpenS2VåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸»ä½“ä¸€è‡´æ€§ã€è‡ªç„¶æ€§å’Œæ–‡æœ¬ç›¸å…³æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£Transformeråœ¨ç”Ÿæˆé«˜ä¿çœŸè§†é¢‘æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæä¾›è§†è§‰ä¸Šè¿è´¯çš„å¸§å’Œä¸°å¯Œçš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œç”±äºéš¾ä»¥è§£ææŒ‡å®šå¤æ‚ç©ºé—´å…³ç³»ã€æ—¶é—´é€»è¾‘ä»¥åŠå¤šä¸ªä¸»ä½“ä¹‹é—´äº¤äº’çš„æç¤ºï¼Œç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ä¸»ä½“ä¸€è‡´æ€§è§†é¢‘ç”Ÿæˆæ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BindWeaveï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥å¤„ç†ä»å•ä¸»ä½“åˆ°å…·æœ‰å¼‚æ„å®ä½“çš„å¤æ‚å¤šä¸»ä½“åœºæ™¯çš„å¹¿æ³›ä¸»ä½“åˆ°è§†é¢‘çš„åœºæ™¯ã€‚ä¸ºäº†å°†å¤æ‚çš„æç¤ºè¯­ä¹‰ç»‘å®šåˆ°å…·ä½“çš„è§†è§‰ä¸»ä½“ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªMLLM-DiTæ¡†æ¶ï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œæ·±åº¦è·¨æ¨¡æ€æ¨ç†ï¼Œä»¥ç¡®å®šå®ä½“å¹¶è§£è€¦è§’è‰²ã€å±æ€§å’Œäº¤äº’ï¼Œä»è€Œäº§ç”Ÿä¸»ä½“æ„ŸçŸ¥çš„éšè—çŠ¶æ€ï¼Œä»è€Œè°ƒèŠ‚æ‰©æ•£Transformerä»¥å®ç°é«˜ä¿çœŸä¸»ä½“ä¸€è‡´çš„è§†é¢‘ç”Ÿæˆã€‚åœ¨OpenS2VåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè§†é¢‘çš„ä¸»ä½“ä¸€è‡´æ€§ã€è‡ªç„¶æ€§å’Œæ–‡æœ¬ç›¸å…³æ€§æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤æ‚åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¤šä¸ªä¸»ä½“åŠå…¶ç›¸äº’ä½œç”¨æ—¶ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆè§†é¢‘ä¸­ä¸»ä½“çš„ä¸€è‡´æ€§ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹éš¾ä»¥å‡†ç¡®è§£æå’Œç†è§£æç¤ºä¸­å…³äºä¸»ä½“é—´çš„ç©ºé—´å…³ç³»ã€æ—¶é—´é€»è¾‘ä»¥åŠäº¤äº’æ–¹å¼çš„æè¿°ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘æ—¶ä¸»ä½“èº«ä»½æ··ä¹±æˆ–è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBindWeaveçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œå°†å¤æ‚çš„æ–‡æœ¬æç¤ºä¸å…·ä½“çš„è§†è§‰ä¸»ä½“è¿›è¡Œç»‘å®šã€‚é€šè¿‡è·¨æ¨¡æ€çš„æ·±åº¦ç†è§£ï¼Œæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æç¤ºä¸­çš„å®ä½“ï¼Œå¹¶è§£è€¦å…¶è§’è‰²ã€å±æ€§å’Œäº¤äº’å…³ç³»ï¼Œä»è€Œç”Ÿæˆä¸»ä½“æ„ŸçŸ¥çš„éšè—çŠ¶æ€ï¼Œç”¨äºæŒ‡å¯¼åç»­çš„è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBindWeaveé‡‡ç”¨MLLM-DiTæ¡†æ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯¹æ–‡æœ¬æç¤ºè¿›è¡Œè§£æï¼Œæå–å‡ºä¸»ä½“ä¿¡æ¯å’Œå…³ç³»ï¼Œç”Ÿæˆä¸»ä½“æ„ŸçŸ¥çš„éšè—çŠ¶æ€ã€‚ç„¶åï¼Œå°†è¿™äº›éšè—çŠ¶æ€ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°æ‰©æ•£Transformerï¼ˆDiTï¼‰ä¸­ï¼ŒæŒ‡å¯¼å…¶ç”Ÿæˆé«˜ä¿çœŸã€ä¸»ä½“ä¸€è‡´çš„è§†é¢‘ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æç¤ºè§£æã€è·¨æ¨¡æ€æ¨ç†ã€ä¸»ä½“æ„ŸçŸ¥éšè—çŠ¶æ€ç”Ÿæˆå’Œè§†é¢‘ç”Ÿæˆå››ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šBindWeaveçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼•å…¥åˆ°è§†é¢‘ç”Ÿæˆæµç¨‹ä¸­ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›æ¥è§£å†³ä¸»ä½“ä¸€è‡´æ€§é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒBindWeaveèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£å’Œåˆ©ç”¨æ–‡æœ¬æç¤ºä¸­çš„å¤æ‚ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„è§†é¢‘å†…å®¹ã€‚

**å…³é”®è®¾è®¡**ï¼šMLLMéƒ¨åˆ†ä½¿ç”¨äº†é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹è§†é¢‘ç”Ÿæˆä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚æ‰©æ•£Transformeréƒ¨åˆ†é‡‡ç”¨äº†æ ‡å‡†çš„DiTæ¶æ„ï¼Œå¹¶é’ˆå¯¹ä¸»ä½“ä¸€è‡´æ€§ç”Ÿæˆè¿›è¡Œäº†ä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé™¤äº†å¸¸ç”¨çš„é‡å»ºæŸå¤±å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸»ä½“ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆè§†é¢‘çš„ä¸»ä½“ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BindWeaveåœ¨OpenS2VåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨ä¸»ä½“ä¸€è‡´æ€§ã€è‡ªç„¶æ€§å’Œæ–‡æœ¬ç›¸å…³æ€§ä¸‰ä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚å…·ä½“æ•°æ®æ–¹é¢ï¼ŒBindWeaveåœ¨ä¸»ä½“ä¸€è‡´æ€§æŒ‡æ ‡ä¸Šæå‡äº†XX%ï¼Œåœ¨è‡ªç„¶æ€§æŒ‡æ ‡ä¸Šæå‡äº†YY%ï¼Œåœ¨æ–‡æœ¬ç›¸å…³æ€§æŒ‡æ ‡ä¸Šæå‡äº†ZZ%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼Œè¯·æ ¹æ®è®ºæ–‡è¡¥å……ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜BindWeaveåœ¨ä¸»ä½“ä¸€è‡´æ€§è§†é¢‘ç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BindWeaveæŠ€æœ¯å¯åº”ç”¨äºç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€å¹¿å‘Šè®¾è®¡ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬æè¿°è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€ä¸»ä½“ä¸€è‡´çš„è§†é¢‘å†…å®¹ã€‚è¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºè™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œç”Ÿæˆé€¼çœŸçš„è™šæ‹Ÿåœºæ™¯å’Œè§’è‰²äº’åŠ¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒBindWeaveæœ‰æœ›æˆä¸ºå†…å®¹åˆ›ä½œçš„é‡è¦å·¥å…·ï¼Œé™ä½è§†é¢‘åˆ¶ä½œæˆæœ¬ï¼Œæé«˜åˆ›ä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.

