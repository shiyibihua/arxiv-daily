---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-09-13
---

# cs.CVï¼ˆ2025-09-13ï¼‰

ğŸ“Š å…± **6** ç¯‡è®ºæ–‡


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250910842v1-openurban3d-annotation-free-open-vocabulary-semantic-segmentation-of.html">OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</a></td>
  <td>OpenUrban3Dï¼šæ— éœ€æ ‡æ³¨çš„å¤§è§„æ¨¡åŸå¸‚ç‚¹äº‘å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">open-vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10842v1" data-paper-url="./papers/250910842v1-openurban3d-annotation-free-open-vocabulary-semantic-segmentation-of.html" onclick="toggleFavorite(this, '2509.10842v1', 'OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250910919v1-lightweight-metadata-aware-mixture-of-experts-masked-autoencoder-for.html">Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation</a></td>
  <td>æå‡ºå…ƒæ•°æ®æ„ŸçŸ¥çš„è½»é‡çº§æ··åˆä¸“å®¶æ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºé«˜æ•ˆåœ°çƒè§‚æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10919v1" data-paper-url="./papers/250910919v1-lightweight-metadata-aware-mixture-of-experts-masked-autoencoder-for.html" onclick="toggleFavorite(this, '2509.10919v1', 'Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>3</td>
  <td><a href="./papers/250910784v2-adapting-medical-vision-foundation-models-for-volumetric-medical-ima.html">Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning</a></td>
  <td>æå‡ºASFDAæ–¹æ³•ï¼Œé€šè¿‡ä¸»åŠ¨å­¦ä¹ å’Œé€‰æ‹©æ€§åŠç›‘ç£å¾®è°ƒï¼Œé«˜æ•ˆé€‚é…åŒ»å­¦è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10784v2" data-paper-url="./papers/250910784v2-adapting-medical-vision-foundation-models-for-volumetric-medical-ima.html" onclick="toggleFavorite(this, '2509.10784v2', 'Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250910813v2-internscenes-a-large-scale-simulatable-indoor-scene-dataset-with-rea.html">InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts</a></td>
  <td>InternScenesï¼šä¸€ä¸ªå…·æœ‰çœŸå®å¸ƒå±€çš„å¤§è§„æ¨¡å¯æ¨¡æ‹Ÿå®¤å†…åœºæ™¯æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10813v2" data-paper-url="./papers/250910813v2-internscenes-a-large-scale-simulatable-indoor-scene-dataset-with-rea.html" onclick="toggleFavorite(this, '2509.10813v2', 'InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250910759v2-every-camera-effect-every-time-all-at-once-4d-gaussian-ray-tracing-f.html">Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation</a></td>
  <td>æå‡º4Dé«˜æ–¯å°„çº¿è¿½è¸ªï¼Œç”¨äºç”Ÿæˆå…·æœ‰ç‰©ç†ç²¾ç¡®ç›¸æœºæ•ˆæœçš„è®­ç»ƒæ•°æ®</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10759v2" data-paper-url="./papers/250910759v2-every-camera-effect-every-time-all-at-once-4d-gaussian-ray-tracing-f.html" onclick="toggleFavorite(this, '2509.10759v2', 'Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>6</td>
  <td><a href="./papers/250910961v1-simulating-sinogram-domain-motion-and-correcting-image-domain-artifa.html">Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging</a></td>
  <td>æå‡ºESWGAN-GPæ¨¡å‹ï¼Œç”¨äºHR-pQCTéª¨éª¼æˆåƒä¸­è¿åŠ¨ä¼ªå½±çš„æ¨¡æ‹Ÿä¸å›¾åƒåŸŸæ ¡æ­£</td>
  <td class="tags-cell"><span class="paper-tag">ReMoS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.10961v1" data-paper-url="./papers/250910961v1-simulating-sinogram-domain-motion-and-correcting-image-domain-artifa.html" onclick="toggleFavorite(this, '2509.10961v1', 'Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)