---
layout: default
title: Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation
---

# Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10759v2</a>
  <a href="https://arxiv.org/pdf/2509.10759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10759v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10759v2', 'Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi-Ruei Liu, You-Zhe Xie, Yu-Hsiang Hsu, I-Sheng Fang, Yu-Lun Liu, Jun-Cheng Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-13 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Paper accepted to NeurIPS 2025 Workshop SpaVLE. Project page: https://shigon255.github.io/4DGRT-project-page/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4Dé«˜æ–¯å°„çº¿è¿½è¸ªï¼Œç”¨äºç”Ÿæˆå…·æœ‰ç‰©ç†ç²¾ç¡®ç›¸æœºæ•ˆæœçš„è®­ç»ƒæ•°æ®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `ç›¸æœºæ•ˆæœ` `æ•°æ®ç”Ÿæˆ` `4Dé«˜æ–¯æº…å°„` `å°„çº¿è¿½è¸ª` `åŠ¨æ€åœºæ™¯é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹åŒ…å«ç›¸æœºæ•ˆæœçš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´è§†è§‰ç³»ç»Ÿåœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚
2. 4D-GRTç»“åˆ4Dé«˜æ–¯æº…å°„å’Œç‰©ç†å°„çº¿è¿½è¸ªï¼Œå®ç°å¯æ§ä¸”ç‰©ç†ç²¾ç¡®çš„ç›¸æœºæ•ˆæœæ¨¡æ‹Ÿã€‚
3. å®éªŒè¡¨æ˜ï¼Œ4D-GRTåœ¨æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º4Dé«˜æ–¯å°„çº¿è¿½è¸ªï¼ˆ4D-GRTï¼‰çš„æ–°å‹ä¸¤é˜¶æ®µæµç¨‹ï¼Œç”¨äºç›¸æœºæ•ˆæœæ¨¡æ‹Ÿï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨é¢å¯¹é±¼çœ¼ç•¸å˜å’Œå·å¸˜å¿«é—¨ç­‰çœŸå®ç›¸æœºæ•ˆæœæ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ•°æ®ç”Ÿæˆæ–¹æ³•å­˜åœ¨æˆæœ¬é«˜ã€æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·å¤§æˆ–æ— æ³•å‡†ç¡®å»ºæ¨¡ç›¸æœºæ•ˆæœç­‰é—®é¢˜ã€‚4D-GRTé¦–å…ˆåˆ©ç”¨4Dé«˜æ–¯æº…å°„é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œç„¶ååº”ç”¨å°„çº¿è¿½è¸ªç”Ÿæˆå…·æœ‰å¯æ§ã€ç‰©ç†ç²¾ç¡®ç›¸æœºæ•ˆæœçš„è§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œ4D-GRTå®ç°äº†æœ€å¿«çš„æ¸²æŸ“é€Ÿåº¦ï¼ŒåŒæ—¶æ¸²æŸ“è´¨é‡ä¸ç°æœ‰åŸºçº¿æ–¹æ³•ç›¸æ¯”æ›´å¥½æˆ–ç›¸å½“ã€‚æ­¤å¤–ï¼Œä½œè€…æ„å»ºäº†å…«ä¸ªå®¤å†…åŠ¨æ€åœºæ™¯ï¼Œæ¶µç›–å››ç§ç›¸æœºæ•ˆæœï¼Œä½œä¸ºè¯„ä¼°ç”Ÿæˆè§†é¢‘ç›¸æœºæ•ˆæœçš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è®¡ç®—æœºè§†è§‰ç³»ç»Ÿé€šå¸¸å‡è®¾ç†æƒ³çš„é’ˆå­”ç›¸æœºæ¨¡å‹ï¼Œä½†åœ¨å¤„ç†çœŸå®ä¸–ç•Œçš„ç›¸æœºæ•ˆæœï¼ˆå¦‚é±¼çœ¼ç•¸å˜ã€å·å¸˜å¿«é—¨ç­‰ï¼‰æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸»è¦åŸå› æ˜¯ç¼ºä¹åŒ…å«è¿™äº›ç›¸æœºæ•ˆæœçš„è®­ç»ƒæ•°æ®ã€‚ç°æœ‰çš„æ•°æ®ç”Ÿæˆæ–¹æ³•è¦ä¹ˆæˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆå­˜åœ¨æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ï¼Œè¦ä¹ˆæ— æ³•å‡†ç¡®åœ°å»ºæ¨¡è¿™äº›ç›¸æœºæ•ˆæœï¼Œé™åˆ¶äº†ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åŠ¨æ€åœºæ™¯é‡å»ºä¸ç‰©ç†æ¸²æŸ“ç›¸ç»“åˆï¼Œåˆ©ç”¨4Dé«˜æ–¯æº…å°„ï¼ˆ4D Gaussian Splattingï¼‰æŠ€æœ¯é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œç„¶åé€šè¿‡ç‰©ç†å°„çº¿è¿½è¸ªæ¨¡æ‹Ÿå„ç§ç›¸æœºæ•ˆæœã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ä»¥è¾ƒä½çš„æˆæœ¬ç”Ÿæˆé«˜è´¨é‡ã€å…·æœ‰ç‰©ç†ç²¾ç¡®æ€§çš„ç›¸æœºæ•ˆæœæ•°æ®ï¼Œä»è€Œå¼¥è¡¥ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼š4D-GRTåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) **åŠ¨æ€åœºæ™¯é‡å»º**ï¼šåˆ©ç”¨å¤šè§†è§’è§†é¢‘ï¼Œé€šè¿‡4Dé«˜æ–¯æº…å°„æŠ€æœ¯é‡å»ºåŠ¨æ€åœºæ™¯ã€‚4Dé«˜æ–¯æº…å°„èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºå’Œæ¸²æŸ“åŠ¨æ€åœºæ™¯ï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚2) **ç›¸æœºæ•ˆæœæ¨¡æ‹Ÿ**ï¼šå¯¹é‡å»ºçš„åŠ¨æ€åœºæ™¯è¿›è¡Œç‰©ç†å°„çº¿è¿½è¸ªï¼Œæ¨¡æ‹Ÿå„ç§ç›¸æœºæ•ˆæœï¼Œå¦‚é±¼çœ¼ç•¸å˜ã€å·å¸˜å¿«é—¨ç­‰ã€‚é€šè¿‡æ§åˆ¶å°„çº¿è¿½è¸ªè¿‡ç¨‹ä¸­çš„å‚æ•°ï¼Œå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒç›¸æœºæ•ˆæœçš„è§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†4Dé«˜æ–¯æº…å°„ä¸ç‰©ç†å°„çº¿è¿½è¸ªç›¸ç»“åˆï¼Œå®ç°äº†ä¸€ç§é«˜æ•ˆä¸”ç²¾ç¡®çš„ç›¸æœºæ•ˆæœæ•°æ®ç”Ÿæˆæµç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œ4D-GRTèƒ½å¤Ÿä»¥æ›´å¿«çš„é€Ÿåº¦ç”Ÿæˆæ›´é«˜è´¨é‡çš„ç›¸æœºæ•ˆæœæ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å‡†ç¡®åœ°å»ºæ¨¡å„ç§ç›¸æœºæ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆè§†é¢‘çš„ç›¸æœºæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨4Dé«˜æ–¯æº…å°„é˜¶æ®µï¼Œä½¿ç”¨äº†æ ‡å‡†çš„é«˜æ–¯æº…å°„ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶é’ˆå¯¹åŠ¨æ€åœºæ™¯è¿›è¡Œäº†æ‰©å±•ã€‚åœ¨å°„çº¿è¿½è¸ªé˜¶æ®µï¼Œä½¿ç”¨äº†åŸºäºç‰©ç†çš„æ¸²æŸ“æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹ä¸åŒçš„ç›¸æœºæ•ˆæœè®¾è®¡äº†ç›¸åº”çš„å°„çº¿è¿½è¸ªç®—æ³•ã€‚ä¾‹å¦‚ï¼Œå¯¹äºé±¼çœ¼ç•¸å˜ï¼Œä½¿ç”¨äº†éçº¿æ€§æŠ•å½±æ¨¡å‹ï¼›å¯¹äºå·å¸˜å¿«é—¨ï¼Œåˆ™æ¨¡æ‹Ÿäº†é€è¡Œæ‰«æçš„è¿‡ç¨‹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œ4D-GRTåœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ¸²æŸ“è´¨é‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«å…«ä¸ªåŠ¨æ€åœºæ™¯çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å››ç§ç›¸æœºæ•ˆæœï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†è¯„ä¼°æ ‡å‡†ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”ç»“æœåœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†çœŸå®ç›¸æœºæ•ˆæœçš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰ã€‚é€šè¿‡ä½¿ç”¨4D-GRTç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æé«˜è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒä¸­çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè™šæ‹Ÿç°å®å’Œæ¸¸æˆå¼€å‘ç­‰é¢†åŸŸï¼Œç”Ÿæˆæ›´é€¼çœŸçš„è§†è§‰æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.

