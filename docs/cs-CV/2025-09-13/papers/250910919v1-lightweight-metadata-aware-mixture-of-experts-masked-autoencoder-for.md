---
layout: default
title: Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation
---

# Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10919" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10919v1</a>
  <a href="https://arxiv.org/pdf/2509.10919.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10919v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10919v1', 'Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohanad Albughdadi

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…ƒæ•°æ®æ„ŸçŸ¥çš„è½»é‡çº§æ··åˆä¸“å®¶æ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºé«˜æ•ˆåœ°çƒè§‚æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åœ°çƒè§‚æµ‹` `æ©ç è‡ªç¼–ç å™¨` `æ··åˆä¸“å®¶æ¨¡å‹` `å…ƒæ•°æ®æ„ŸçŸ¥` `è½»é‡çº§æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶å¯è®¿é—®æ€§å’Œä¸‹æ¸¸ä»»åŠ¡çš„å¤ç”¨ã€‚
2. æå‡ºä¸€ç§å…ƒæ•°æ®æ„ŸçŸ¥çš„æ··åˆä¸“å®¶æ©ç è‡ªç¼–ç å™¨ï¼Œç»“åˆç¨€ç–è·¯ç”±å’Œåœ°ç†æ—¶åºä¿¡æ¯ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‚æ•°é‡å°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å¯ä¸å¤§å‹æ¨¡å‹åª²ç¾ï¼Œä¸”æ³›åŒ–èƒ½åŠ›å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å…ƒæ•°æ®æ„ŸçŸ¥æ··åˆä¸“å®¶æ©ç è‡ªç¼–ç å™¨(MoE-MAE)ï¼Œä»…åŒ…å«250ä¸‡å‚æ•°ï¼Œæ—¨åœ¨è§£å†³åœ°çƒè§‚æµ‹é¢†åŸŸå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ç¨€ç–ä¸“å®¶è·¯ç”±å’Œåœ°ç†æ—¶åºæ¡ä»¶ï¼Œå°†å›¾åƒä¸ç»çº¬åº¦ä»¥åŠå­£èŠ‚/æ¯æ—¥å¾ªç¯ç¼–ç ç›¸ç»“åˆã€‚MoE-MAEåœ¨BigEarthNet-Landsatæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨çº¿æ€§æ¢é’ˆè¯„ä¼°å…¶å†»ç»“ç¼–ç å™¨çš„åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹ä½“ç§¯å°ï¼Œä½†å…¶æ€§èƒ½å¯ä¸æ›´å¤§çš„æ¶æ„ç›¸åª²ç¾ï¼Œè¯æ˜äº†å…ƒæ•°æ®æ„ŸçŸ¥çš„é¢„è®­ç»ƒèƒ½å¤Ÿæé«˜è¿ç§»èƒ½åŠ›å’Œæ ‡ç­¾æ•ˆç‡ã€‚åœ¨ç¼ºä¹æ˜¾å¼å…ƒæ•°æ®çš„EuroSAT-Landsatæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶æ€§èƒ½ä»ç„¶ä¼˜äºå…·æœ‰æ•°äº¿å‚æ•°çš„æ¨¡å‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç´§å‡‘çš„ã€å…ƒæ•°æ®æ„ŸçŸ¥çš„MoE-MAEæ˜¯æœªæ¥åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹é«˜æ•ˆä¸”å¯æ‰©å±•çš„ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åœ°çƒè§‚æµ‹é¢†åŸŸçš„å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹è™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†è®¡ç®—èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œéƒ¨ç½²å’Œåº”ç”¨æˆæœ¬é«˜æ˜‚ï¼Œé˜»ç¢äº†å…¶åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§è½»é‡çº§ä¸”æ€§èƒ½ä¼˜å¼‚çš„åœ°çƒè§‚æµ‹æ¨¡å‹æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥å…ƒæ•°æ®ä¿¡æ¯ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œä»è€Œåœ¨å‡å°‘æ¨¡å‹å‚æ•°é‡çš„åŒæ—¶ï¼Œæå‡æ¨¡å‹çš„è¡¨å¾å­¦ä¹ èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚MoEé€šè¿‡ç¨€ç–æ¿€æ´»ä¸åŒçš„ä¸“å®¶ç½‘ç»œï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œè€Œå…ƒæ•°æ®ä¿¡æ¯åˆ™ä¸ºæ¨¡å‹æä¾›äº†é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°ç†è§£åœ°çƒè§‚æµ‹æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å›¾åƒç¼–ç å™¨ï¼šå°†è¾“å…¥çš„åœ°çƒè§‚æµ‹å›¾åƒç¼–ç æˆæ½œåœ¨è¡¨ç¤ºã€‚2) å…ƒæ•°æ®ç¼–ç å™¨ï¼šå°†è¾“å…¥çš„åœ°ç†æ—¶åºå…ƒæ•°æ®ç¼–ç æˆåµŒå…¥å‘é‡ã€‚3) æ··åˆä¸“å®¶å±‚ï¼šæ ¹æ®å›¾åƒç¼–ç å’Œå…ƒæ•°æ®åµŒå…¥ï¼ŒåŠ¨æ€åœ°é€‰æ‹©æ¿€æ´»ä¸åŒçš„ä¸“å®¶ç½‘ç»œã€‚4) æ©ç è‡ªç¼–ç å™¨ï¼šå¯¹éƒ¨åˆ†æ©ç çš„å›¾åƒç¼–ç è¿›è¡Œé‡å»ºï¼Œä»è€Œå­¦ä¹ å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆå°†å›¾åƒå’Œå…ƒæ•°æ®åˆ†åˆ«ç¼–ç ï¼Œç„¶åé€šè¿‡æ··åˆä¸“å®¶å±‚è¿›è¡Œèåˆï¼Œæœ€åä½¿ç”¨æ©ç è‡ªç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å…ƒæ•°æ®ä¿¡æ¯èå…¥åˆ°æ··åˆä¸“å®¶æ©ç è‡ªç¼–ç å™¨ä¸­ã€‚ä¼ ç»Ÿçš„MAEæ¨¡å‹é€šå¸¸åªå…³æ³¨å›¾åƒæœ¬èº«çš„ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†åœ°çƒè§‚æµ‹æ•°æ®ä¸­ä¸°å¯Œçš„å…ƒæ•°æ®ä¿¡æ¯ã€‚é€šè¿‡å°†å…ƒæ•°æ®ä¿¡æ¯ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åœ°çƒè§‚æµ‹æ•°æ®çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæå‡æ¨¡å‹çš„è¡¨å¾å­¦ä¹ èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä½¿ç”¨2.5Må‚æ•°ã€‚å…ƒæ•°æ®ç¼–ç å™¨ä½¿ç”¨å¾ªç¯ç¼–ç æ¥è¡¨ç¤ºå­£èŠ‚å’Œæ¯æ—¥å‘¨æœŸã€‚æŸå¤±å‡½æ•°æ˜¯æ ‡å‡†çš„MAEé‡å»ºæŸå¤±ã€‚æ¨¡å‹åœ¨BigEarthNet-Landsatæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨çº¿æ€§æ¢é’ˆè¯„ä¼°å…¶å†»ç»“ç¼–ç å™¨çš„åµŒå…¥ã€‚ä¸“å®¶ç½‘ç»œçš„æ•°é‡å’Œå®¹é‡æ˜¯æ ¹æ®å®éªŒç»“æœè¿›è¡Œè°ƒæ•´çš„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨BigEarthNet-Landsatæ•°æ®é›†ä¸Šé¢„è®­ç»ƒåï¼Œåœ¨EuroSAT-Landsatæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼Œå…¶æ€§èƒ½å¯ä¸å…·æœ‰æ•°äº¿å‚æ•°çš„æ¨¡å‹ç›¸åª²ç¾ã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿åœ¨ç¼ºä¹æ˜¾å¼å…ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä»ç„¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ ‡ç­¾æ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…ƒæ•°æ®æ„ŸçŸ¥çš„é¢„è®­ç»ƒèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé¥æ„Ÿå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€åœºæ™¯ç†è§£ç­‰é¢†åŸŸã€‚è½»é‡çº§æ¨¡å‹æ›´æ˜“äºéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œä¾‹å¦‚æ— äººæœºã€å«æ˜Ÿç­‰ã€‚é€šè¿‡å…ƒæ•°æ®ä¿¡æ¯çš„èåˆï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨ä¸åŒåœ°ç†åŒºåŸŸå’Œæ—¶é—´æ®µçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå†œä¸šç›‘æµ‹ã€ç¯å¢ƒç›‘æµ‹ã€ç¾å®³è¯„ä¼°ç­‰åº”ç”¨æä¾›æ›´å¯é çš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Earth Observation have focused on large-scale foundation models. However, these models are computationally expensive, limiting their accessibility and reuse for downstream tasks. In this work, we investigate compact architectures as a practical pathway toward smaller general-purpose EO models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder (MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing with geo-temporal conditioning, incorporating imagery alongside latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen encoder using linear probes. Despite its small size, the model competes with much larger architectures, demonstrating that metadata-aware pretraining improves transfer and label efficiency. To further assess generalization, we evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and still observe competitive performance compared to models with hundreds of millions of parameters. These results suggest that compact, metadata-aware MoE-MAEs are an efficient and scalable step toward future EO foundation models.

