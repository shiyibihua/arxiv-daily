---
layout: default
title: MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning
---

# MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05523" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05523v1</a>
  <a href="https://arxiv.org/pdf/2506.05523.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05523v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05523v1', 'MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zikui Cai, Andrew Wang, Anirudh Satheesh, Ankit Nakhawa, Hyunwoo Jae, Keenan Powell, Minghui Liu, Neel Jay, Sungbin Oh, Xiyao Wang, Yongyuan Liang, Tom Goldstein, Furong Huang

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MORSE-500‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜ‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊé®ÁêÜ` `ËßÜÈ¢ëÂü∫ÂáÜ` `Á®ãÂ∫èÂåñÁîüÊàê` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Âä®ÊÄÅÈöæÂ∫¶Ë∞ÉÊï¥` `ÊäΩË±°Êé®ÁêÜ` `ËßÑÂàíËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜ‰∏ªË¶Å‰æùËµñÈùôÊÄÅÂõæÂÉèÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâÊó∂Èó¥Âä®ÊÄÅÂíåÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ
2. MORSE-500ÈÄöËøáÁ®ãÂ∫èÂåñÁîüÊàê500‰∏™ËßÜÈ¢ëÁâáÊÆµÔºåÊ∂µÁõñÂ§öÁßçÊé®ÁêÜÁ±ªÂà´ÔºåÊîØÊåÅÂä®ÊÄÅÈöæÂ∫¶Ë∞ÉÊï¥„ÄÇ
3. ÂàùÊ≠•ÂÆûÈ™åÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®ÊäΩË±°ÂíåËßÑÂàí‰ªªÂä°‰∏äÂ≠òÂú®ÊòæËëóÊÄßËÉΩÂ∑ÆË∑ùÔºåMORSE-500‰∏∫Êú™Êù•Á†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËøÖÈÄüÂèëÂ±ïÔºå‰ΩÜÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜÂú®‰∏â‰∏™ÂÖ≥ÈîÆÁª¥Â∫¶‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇÈ¶ñÂÖàÔºåÂÆÉ‰ª¨Ëøá‰∫é‰æùËµñÈùôÊÄÅÂõæÂÉèÔºåÊú™ËÉΩÊçïÊçâÁé∞ÂÆûÁéØÂ¢ÉÁöÑÊó∂Èó¥Â§çÊùÇÊÄß„ÄÇÂÖ∂Ê¨°ÔºåÂÆÉ‰ª¨‰∏ªË¶ÅÈõÜ‰∏≠Âú®Êï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥‰∏äÔºåÂøΩËßÜ‰∫ÜÊäΩË±°„ÄÅÁâ©ÁêÜ„ÄÅËßÑÂàí„ÄÅÁ©∫Èó¥ÂíåÊó∂Èó¥Á≠âÊõ¥ÂπøÊ≥õÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊúÄÂêéÔºåËÆ∏Â§öÂü∫ÂáÜÂæàÂø´È•±ÂíåÔºåÁº∫‰πèËØäÊñ≠Â§±Ë¥•Ê®°ÂºèÊàñË°°ÈáèÊåÅÁª≠ËøõÂ±ïÁöÑÁ©∫Èó¥„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜMORSE-500Ôºå‰∏Ä‰∏™Áî±500‰∏™ÂÆåÂÖ®ËÑöÊú¨ÂåñÁöÑÁü≠ÁâáÁªÑÊàêÁöÑËßÜÈ¢ëÂü∫ÂáÜÔºåÊ∂µÁõñÂÖ≠‰∏™‰∫íË°•ÁöÑÊé®ÁêÜÁ±ªÂà´„ÄÇÊØè‰∏™ÂÆû‰æãÈÄöËøáÁ°ÆÂÆöÊÄßÁöÑPythonËÑöÊú¨ÁîüÊàêÔºåÊîØÊåÅÂØπËßÜËßâÂ§çÊùÇÊÄß„ÄÅÂπ≤Êâ∞ÂØÜÂ∫¶ÂíåÊó∂Èó¥Âä®ÊÄÅÁöÑÁ≤æÁªÜÊéßÂà∂Ôºå‰ΩøÂæóÈöèÁùÄÊ®°ÂûãÁöÑÊîπËøõÔºåÈöæÂ∫¶ÂèØ‰ª•Á≥ªÁªüÊÄßÂú∞Ë∞ÉÊï¥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜÂú®Êó∂Èó¥Â§çÊùÇÊÄß„ÄÅÊé®ÁêÜËÉΩÂäõÂπøÂ∫¶ÂíåÈ•±ÂíåÂ∫¶ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÈùôÊÄÅÂõæÂÉèÔºåÊó†Ê≥ïÂÖ®Èù¢ËØÑ‰º∞Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMORSE-500ÈÄöËøáÁ®ãÂ∫èÂåñÁîüÊàêËßÜÈ¢ëÁâáÊÆµÔºåÊ∂µÁõñÂÖ≠ÁßçÊé®ÁêÜÁ±ªÂà´ÔºåÂÖÅËÆ∏ÂØπÈöæÂ∫¶ËøõË°åÁ≤æÁªÜÊéßÂà∂Ôºå‰ª•ÈÄÇÂ∫îÊ®°ÂûãÁöÑËøõÊ≠•„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°‰ΩøÂæóÂü∫ÂáÜËÉΩÂ§üÈöèÁùÄÊäÄÊúØÁöÑÂèëÂ±ïËÄåÊºîÂèò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMORSE-500ÁöÑÁîüÊàêÊµÅÁ®ãÂåÖÊã¨‰ΩøÁî®PythonËÑöÊú¨ÔºàÈÄöËøáManim„ÄÅMatplotlib„ÄÅMoviePyÔºâÁîüÊàêËßÜÈ¢ëÔºåÁªìÂêàÁîüÊàêËßÜÈ¢ëÊ®°ÂûãÂíåÁ≠ñÂàíÁöÑÁúüÂÆûËßÜÈ¢ëÁ¥†Êùê„ÄÇÊï¥‰ΩìÊû∂ÊûÑÊîØÊåÅÂØπËßÜËßâÂ§çÊùÇÊÄßÂíåÊó∂Èó¥Âä®ÊÄÅÁöÑÁ≥ªÁªüÊÄßË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMORSE-500ÁöÑÊúÄÂ§ßÂàõÊñ∞Âú®‰∫éÂÖ∂Á®ãÂ∫èÂåñÁîüÊàêÁöÑÁâπÊÄßÔºå‰ΩøÂæóÂü∫ÂáÜËÉΩÂ§üÂä®ÊÄÅÊºîÂèòÔºåÈÅøÂÖç‰∫ÜÈùôÊÄÅÂü∫ÂáÜÁöÑÈ•±ÂíåÈóÆÈ¢ò„ÄÇËøô‰∏ÄËÆæËÆ°‰ΩøÂæóÁ†îÁ©∂‰∫∫ÂëòËÉΩÂ§üÊåÅÁª≠ÊµãËØïÂíåËØÑ‰º∞Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ÂåÖÊã¨ËßÜËßâÂ§çÊùÇÊÄß„ÄÅÂπ≤Êâ∞Áâ©‰ΩìÁöÑÂØÜÂ∫¶ÂíåÊó∂Èó¥Âä®ÊÄÅÁöÑËÆæÁΩÆ„ÄÇÈÄöËøáËøô‰∫õÂèÇÊï∞ÁöÑË∞ÉÊï¥ÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•ÂàõÂª∫ÂÖ∑Êúâ‰∏çÂêåÊåëÊàòÊÄßÁöÑÂÆû‰æãÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂàùÊ≠•ÂÆûÈ™åË°®ÊòéÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®MORSE-500Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ùÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊäΩË±°ÂíåËßÑÂàí‰ªªÂä°‰∏äÔºåÊòæÁ§∫Âá∫ËæÉÂ§ßÁöÑÊîπËøõÁ©∫Èó¥„ÄÇËøô‰∏ÄÂèëÁé∞Âº∫Ë∞É‰∫ÜMORSE-500Âú®Êé®Âä®Â§öÊ®°ÊÄÅÊé®ÁêÜÁ†îÁ©∂‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MORSE-500ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂ§öÊ®°ÊÄÅÊé®ÁêÜÁ†îÁ©∂ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËØÑ‰º∞ÂíåÊîπËøõÊñπÈù¢„ÄÇÂÖ∂Âä®ÊÄÅÁîüÊàêÁâπÊÄß‰ΩøÂæóÁ†îÁ©∂‰∫∫ÂëòËÉΩÂ§üÊåÅÁª≠Êé¢Á¥¢ÂíåÊµãËØïÊñ∞ÁÆóÊ≥ïÔºåÊé®Âä®Â§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇÊú™Êù•ÔºåËØ•Âü∫ÂáÜÂèØËÉΩÊàê‰∏∫ËØÑ‰º∞Êñ∞‰∏Ä‰ª£Ê®°ÂûãÁöÑÊ†áÂáÜÂ∑•ÂÖ∑Ôºå‰øÉËøõÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊäÄÊúØËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.

