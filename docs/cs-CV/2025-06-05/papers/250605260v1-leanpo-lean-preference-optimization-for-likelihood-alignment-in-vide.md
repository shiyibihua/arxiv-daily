---
layout: default
title: LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs
---

# LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05260" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05260v1</a>
  <a href="https://arxiv.org/pdf/2506.05260.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05260v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05260v1', 'LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaodong Wang, Jinfa Huang, Li Yuan, Peixi Peng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Code: https://github.com/Wang-Xiaodong1899/LeanPO

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLeanPOä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åå¥½å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹` `åå¥½å¯¹é½` `ä¼¼ç„¶ä½ç§»` `è‡ªç”Ÿæˆæ•°æ®` `åŠ¨æ€æ ‡ç­¾å¹³æ»‘` `æ¨¡å‹ä¼˜åŒ–` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨åå¥½å¯¹é½æ—¶å­˜åœ¨ä¼¼ç„¶ä½ç§»é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹å¯¹éç›®æ ‡å“åº”çš„æ¦‚ç‡æå‡ã€‚
2. æœ¬æ–‡æå‡ºLeanåå¥½ä¼˜åŒ–ï¼ˆLeanPOï¼‰ï¼Œé€šè¿‡æ— å‚è€ƒçš„æ–¹æ³•é‡æ–°å®šä¹‰å¥–åŠ±ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆåå¥½æ•°æ®æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLeanPOåœ¨å¤šç§åŸºçº¿æ¨¡å‹ä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä¸”è®­ç»ƒå¼€é”€è¾ƒå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å¤šæ•°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰é‡‡ç”¨åå¥½å¯¹é½æŠ€æœ¯ï¼Œå¦‚DPOï¼Œæ¥ä¼˜åŒ–è·èƒœå“åº”ä¸å¤±è´¥å“åº”ä¹‹é—´çš„å¥–åŠ±å·®è·ã€‚ç„¶è€Œï¼ŒDPOä¸­è§‚å¯Ÿåˆ°çš„ä¼¼ç„¶ä½ç§»è¡¨æ˜ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­è·èƒœå’Œå¤±è´¥å“åº”çš„å¯¹æ•°æ¦‚ç‡å¾€å¾€åŒæ—¶ä¸‹é™ï¼Œæ„å¤–åœ°æå‡äº†éç›®æ ‡å“åº”çš„æ¦‚ç‡ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°é‡æ–°å®¡è§†äº†è¿™ä¸€ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— å‚è€ƒçš„Leanåå¥½ä¼˜åŒ–ï¼ˆLeanPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†éšå¼å¥–åŠ±é‡æ–°è¡¨è¿°ä¸ºå“åº”çš„å¹³å‡ä¼¼ç„¶æ€§ï¼Œæ¥ç¼“è§£è¿™ä¸€ç°è±¡çš„å½±å“ã€‚LeanPOçš„å…³é”®ç»„ä»¶æ˜¯å¥–åŠ±å¯ä¿¡åº¦ç›¸å…³çš„è‡ªç”Ÿæˆåå¥½æ•°æ®ç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ç›¸å…³å…ˆéªŒçŸ¥è¯†æ³¨å…¥æ¨¡å‹ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘åæ€ä¸æ–­ä¼˜åŒ–åå¥½æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLeanPOæ˜¾è‘—æå‡äº†æœ€å…ˆè¿›çš„Video-LLMsçš„æ€§èƒ½ï¼Œä¸”è®­ç»ƒå¼€é”€æå°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨åå¥½å¯¹é½è¿‡ç¨‹ä¸­å‡ºç°çš„ä¼¼ç„¶ä½ç§»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚DPOåœ¨è®­ç»ƒä¸­å¯¼è‡´ç›®æ ‡å“åº”æ¦‚ç‡ä¸‹é™ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLeanPOé€šè¿‡æ— å‚è€ƒçš„æ–¹å¼å°†éšå¼å¥–åŠ±é‡æ–°å®šä¹‰ä¸ºå“åº”çš„å¹³å‡ä¼¼ç„¶æ€§ï¼Œç»“åˆè‡ªç”Ÿæˆçš„åå¥½æ•°æ®ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç›®æ ‡å“åº”çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLeanPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¥–åŠ±å¯ä¿¡åº¦ç›¸å…³çš„è‡ªç”Ÿæˆåå¥½æ•°æ®ç®¡é“å’ŒåŠ¨æ€æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œå‰è€…ç”¨äºä¼˜åŒ–åå¥½æ•°æ®ï¼Œåè€…ç”¨äºå‡å°‘å™ªå£°å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šLeanPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ— å‚è€ƒçš„å¥–åŠ±å®šä¹‰å’Œè‡ªç”Ÿæˆåå¥½æ•°æ®çš„ä½¿ç”¨ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ä¾èµ–äºå¤–éƒ¨å‚è€ƒçš„åå¥½å¯¹é½æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLeanPOå¼•å…¥äº†åŠ¨æ€æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œä»¥åº”å¯¹å¤šæ ·åŒ–è§†é¢‘å†…å®¹å¸¦æ¥çš„å™ªå£°ï¼ŒåŒæ—¶ä¼˜åŒ–äº†å¥–åŠ±çš„ä¼°è®¡è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ é«˜è´¨é‡çš„é…å¯¹æ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLeanPOåœ¨å¤šç§åŸºçº¿æ¨¡å‹ä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»…å¢åŠ äº†æå°‘çš„é¢å¤–å¼€é”€ã€‚è¿™è¡¨æ˜LeanPOåœ¨ä¼˜åŒ–è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åå¥½å¯¹é½æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LeanPOçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ç†è§£ã€å†…å®¹ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼ŒLeanPOèƒ½å¤Ÿä¸ºæ™ºèƒ½è§†é¢‘åˆ†æã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œä¸ªæ€§åŒ–æ¨èç­‰åº”ç”¨æä¾›æ›´å¯é çš„æŠ€æœ¯æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›æ­¥ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Most Video Large Language Models (Video-LLMs) adopt preference alignment techniques, e.g., DPO~\citep{rafailov2024dpo}, to optimize the reward margin between a winning response ($y_w$) and a losing response ($y_l$). However, the likelihood displacement observed in DPO indicates that both $\log Ï€_Î¸(y_w\mid x)$ and $\log Ï€_Î¸(y_l\mid x) $ often decrease during training, inadvertently boosting the probabilities of non-target responses. In this paper, we systematically revisit this phenomenon from LLMs to Video-LLMs, showing that it intensifies when dealing with the redundant complexity of video content. To alleviate the impact of this phenomenon, we propose \emph{Lean Preference Optimization} (LeanPO), a reference-free approach that reformulates the implicit reward as the average likelihood of the response with respect to the policy model. A key component of LeanPO is the reward-trustworthiness correlated self-generated preference data pipeline, which carefully infuses relevant prior knowledge into the model while continuously refining the preference data via self-reflection. This allows the policy model to obtain high-quality paired data and accurately estimate the newly defined reward, thus mitigating the unintended drop. In addition, we introduce a dynamic label smoothing strategy that mitigates the impact of noise in responses from diverse video content, preventing the model from overfitting to spurious details. Extensive experiments demonstrate that LeanPO significantly enhances the performance of state-of-the-art Video-LLMs, consistently boosting baselines of varying capacities with minimal additional training overhead. Moreover, LeanPO offers a simple yet effective solution for aligning Video-LLM preferences with human trustworthiness, paving the way toward the reliable and efficient Video-LLMs.

