---
layout: default
title: Line of Sight: On Linear Representations in VLLMs
---

# Line of Sight: On Linear Representations in VLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04706v1</a>
  <a href="https://arxiv.org/pdf/2506.04706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04706v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04706v1', 'Line of Sight: On Linear Representations in VLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas, Arthur Conmy

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: 8 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€ç¨€ç–è‡ªç¼–ç å™¨ä»¥å¢å¼ºVLLMçš„å›¾åƒè¡¨ç¤ºèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `ç¨€ç–è‡ªç¼–ç å™¨` `å›¾åƒè¡¨ç¤º` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè¡¨ç¤ºçš„éšè—æ¿€æ´»æ–¹é¢ç¼ºä¹æ·±å…¥çš„ç†è§£ï¼Œå¯¼è‡´å…¶å¤šæ¨¡æ€èƒ½åŠ›çš„æ½œåŠ›æœªè¢«å……åˆ†æŒ–æ˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è®­ç»ƒå¤šæ¨¡æ€ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æ¥åˆ›å»ºå¯è§£é‡Šçš„æ–‡æœ¬å’Œå›¾åƒç‰¹å¾å­—å…¸ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹å›¾åƒæ¦‚å¿µçš„è¡¨ç¤ºèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºè™½ç„¶åˆå§‹åˆ†ç¦»ï¼Œä½†åœ¨æ·±å±‚ç½‘ç»œä¸­é€æ¸å…±äº«ï¼Œå±•ç°å‡ºæ›´å¼ºçš„å¤šæ¨¡æ€èåˆèƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€æ¨¡å‹é€šè¿‡å¯¹è§†è§‰è¾“å…¥çš„åµŒå…¥è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥å…·å¤‡å¤šæ¨¡æ€èƒ½åŠ›ã€‚ä½†è¿™äº›å¤šæ¨¡æ€æ¨¡å‹å¦‚ä½•åœ¨å…¶éšè—æ¿€æ´»ä¸­è¡¨ç¤ºå›¾åƒæ¦‚å¿µä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£ä¹‹è°œã€‚æœ¬æ–‡æ¢è®¨äº†æµè¡Œçš„å¼€æºVLLM LlaVA-Nextä¸­å›¾åƒæ¦‚å¿µçš„è¡¨ç¤ºï¼Œå‘ç°æ®‹å·®æµä¸­å­˜åœ¨ä¸€ç»„å¤šæ ·çš„å¯çº¿æ€§è§£ç çš„ImageNetç±»åˆ«ç‰¹å¾ã€‚é€šè¿‡å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç¼–è¾‘ï¼ŒéªŒè¯äº†è¿™äº›ç‰¹å¾çš„å› æœæ€§ã€‚ä¸ºå¢åŠ ç ”ç©¶çš„çº¿æ€§ç‰¹å¾å¤šæ ·æ€§ï¼Œæœ¬æ–‡è®­ç»ƒäº†å¤šæ¨¡æ€ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé«˜åº¦å¯è§£é‡Šçš„æ–‡æœ¬å’Œå›¾åƒç‰¹å¾å­—å…¸ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸åŒæ¨¡æ€çš„æ¨¡å‹è¡¨ç¤ºç›¸å¯¹åˆ†ç¦»ï¼Œä½†åœ¨æ›´æ·±å±‚æ¬¡ä¸Šï¼Œå®ƒä»¬çš„å…±äº«ç¨‹åº¦é€æ¸å¢åŠ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè¡¨ç¤ºæ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºå›¾åƒæ¦‚å¿µåœ¨éšè—æ¿€æ´»ä¸­çš„å…·ä½“è¡¨ç¤ºå½¢å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®­ç»ƒå¤šæ¨¡æ€ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ï¼Œæœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„ç‰¹å¾å­—å…¸ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹å›¾åƒå’Œæ–‡æœ¬çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å¥½çš„å¤šæ¨¡æ€èåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€SAEè®­ç»ƒå’Œæ¨¡å‹è¾“å‡ºåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå¯¹è§†è§‰è¾“å…¥è¿›è¡ŒåµŒå…¥ï¼Œç„¶åè®­ç»ƒSAEä»¥ç”Ÿæˆç‰¹å¾å­—å…¸ï¼Œæœ€ååˆ†ææ¨¡å‹çš„éšè—æ¿€æ´»ä»¥éªŒè¯ç‰¹å¾çš„å› æœæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡SAEè®­ç»ƒç”Ÿæˆçš„å¯è§£é‡Šç‰¹å¾å­—å…¸ï¼Œæä¾›äº†å¯¹å¤šæ¨¡æ€æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„æ·±å…¥ç†è§£ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é»‘ç®±ç‰¹æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SAEçš„è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç‰¹å¾çš„ç¨€ç–æ€§å’Œå¯è§£ç æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè®¾è®¡äº†é€‚åº”å¤šæ¨¡æ€è¾“å…¥çš„å±‚æ¬¡ç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlaVA-Nextæ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶ï¼Œæ·±å±‚ç‰¹å¾çš„å…±äº«ç¨‹åº¦æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€é—´çš„æœ‰æ•ˆèåˆã€‚é€šè¿‡å¯¹æ¯”åŸºçº¿ï¼Œæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°20%ï¼Œå±•ç°äº†å¤šæ¨¡æ€ç¨€ç–è‡ªç¼–ç å™¨çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆå’Œå¤šæ¨¡æ€å†…å®¹æ£€ç´¢ç­‰ã€‚é€šè¿‡å¢å¼ºè¯­è¨€æ¨¡å‹çš„å›¾åƒè¡¨ç¤ºèƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’å’Œæ›´ç²¾å‡†çš„ä¿¡æ¯æ£€ç´¢ï¼Œæœªæ¥å¯èƒ½å¯¹æ•™è‚²ã€åŒ»ç–—å’Œå¨±ä¹ç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language models can be equipped with multimodal capabilities by fine-tuning on embeddings of visual inputs. But how do such multimodal models represent images in their hidden activations? We explore representations of image concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set of ImageNet classes represented via linearly decodable features in the residual stream. We show that the features are causal by performing targeted edits on the model output. In order to increase the diversity of the studied linear features, we train multimodal Sparse Autoencoders (SAEs), creating a highly interpretable dictionary of text and image features. We find that although model representations across modalities are quite disjoint, they become increasingly shared in deeper layers.

