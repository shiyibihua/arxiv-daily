---
layout: default
title: BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models
---

# BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05440" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05440v1</a>
  <a href="https://arxiv.org/pdf/2506.05440.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05440v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05440v1', 'BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ludovic Arnould, Salim Khazem, Hugues Ali Mehenni

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/byoeval/BYO-EVAL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBYO-Evalä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€è¯„ä¼°` `åˆæˆå›¾åƒ` `æ„ŸçŸ¥å¤±è´¥` `ç»†ç²’åº¦åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸå†…å­˜åœ¨é«˜æ˜‚çš„æ³¨é‡Šæˆæœ¬å’Œä¿¡æ¯æ³„éœ²é£é™©ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«VLMsçš„å¤±è´¥åŸå› ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¨‹åºç”Ÿæˆåˆæˆå›¾åƒçš„è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è§†è§‰å±æ€§å¹¶æ­ç¤ºæ„ŸçŸ¥å¤±è´¥ã€‚
3. é€šè¿‡æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§å˜åŒ–çš„å›¾åƒé›†åˆï¼Œæœ¬æ–‡å®ç°äº†ç³»ç»Ÿæ€§çš„å‹åŠ›æµ‹è¯•å’Œç»†ç²’åº¦çš„å¤±è´¥åˆ†æï¼Œæå‡äº†è¯„ä¼°çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²è¶³å¤Ÿå…ˆè¿›ï¼Œèƒ½å¤Ÿæ”¯æŒå¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å›ç­”å¤æ‚çš„è§†è§‰é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€é›†ä¸­äºç‰¹å®šé¢†åŸŸï¼Œæ„å»ºçš„æ³¨é‡Šæ•°æ®é›†ä¼´éšé¢„å®šä¹‰çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œè¿™å¯¼è‡´é«˜æ˜‚çš„æ³¨é‡Šæˆæœ¬å’Œä¿¡æ¯æ³„éœ²é£é™©ï¼Œå¹¶æœªæ˜ç¡®å¤±è´¥æ˜¯å¦æºäºè§†è§‰æ„ŸçŸ¥ã€æ¨ç†æˆ–ä¸€èˆ¬çŸ¥è¯†çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œå€Ÿé‰´çœ¼ç§‘è¯Šæ–­ï¼Œé€šè¿‡ç¨‹åºç”Ÿæˆåˆæˆå›¾åƒæ¥æ§åˆ¶è§†è§‰å±æ€§ï¼Œç²¾ç¡®æ­ç¤ºVLMsçš„æ„ŸçŸ¥å¤±è´¥ã€‚æˆ‘ä»¬æ„å»ºäº†å…·æœ‰é€æ¸å¢åŠ çš„æŒ‘æˆ˜æ€§å˜åŒ–çš„å›¾åƒé›†åˆï¼Œç³»ç»Ÿæ€§åœ°è¿›è¡Œå‹åŠ›æµ‹è¯•å’Œç»†ç²’åº¦å¤±è´¥åˆ†æï¼Œè½¬å˜äº†è¯„ä¼°é‡ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯é«˜æ˜‚çš„æ³¨é‡Šæˆæœ¬å’Œä¿¡æ¯æ³„éœ²é£é™©ï¼Œä»¥åŠæ— æ³•æ˜ç¡®è¯†åˆ«å¤±è´¥åŸå› çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œå€Ÿé‰´çœ¼ç§‘è¯Šæ–­ï¼Œé€šè¿‡ç¨‹åºç”Ÿæˆåˆæˆå›¾åƒï¼Œæ§åˆ¶è§†è§‰å±æ€§ï¼Œæ­ç¤ºVLMsçš„æ„ŸçŸ¥å¤±è´¥ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—è¯„ä¼°æ›´åŠ ç²¾ç¡®å’Œå¯æ§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åˆæˆå›¾åƒç”Ÿæˆæ¨¡å—ã€è§†è§‰å±æ€§æ§åˆ¶æ¨¡å—å’Œè¯„ä¼°åˆ†ææ¨¡å—ã€‚åˆæˆå›¾åƒç”Ÿæˆæ¨¡å—è´Ÿè´£åˆ›å»ºå…·æœ‰ä¸åŒè§†è§‰å±æ€§çš„å›¾åƒï¼Œè§†è§‰å±æ€§æ§åˆ¶æ¨¡å—ç¡®ä¿å…¶ä»–å‚æ•°ä¿æŒä¸å˜ï¼Œè¯„ä¼°åˆ†ææ¨¡å—åˆ™è¿›è¡Œç³»ç»Ÿæ€§æµ‹è¯•å’Œåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºä½¿ç”¨ç¨‹åºç”Ÿæˆçš„åˆæˆå›¾åƒè¿›è¡Œè¯„ä¼°ï¼Œè¿™ä¸ä¼ ç»Ÿä¾èµ–çœŸå®å›¾åƒå’Œæ³¨é‡Šçš„è¯„ä¼°æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶å˜é‡å¹¶æ­ç¤ºæ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œåˆæˆå›¾åƒçš„è§†è§‰å±æ€§å¦‚å¯¹è±¡æ•°é‡ã€é¢œè‰²ç­‰å¯è°ƒèŠ‚ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ç”¨äºé‡åŒ–æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„VLMæ¶æ„è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆå›¾åƒè¿›è¡Œè¯„ä¼°èƒ½å¤Ÿæ˜¾è‘—æé«˜å¯¹VLMsæ„ŸçŸ¥èƒ½åŠ›çš„è¯†åˆ«ç²¾åº¦ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œç»†ç²’åº¦å¤±è´¥åˆ†æçš„å‡†ç¡®æ€§æå‡äº†çº¦30%ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´æ¸…æ™°åœ°æ­ç¤ºæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„å¼±ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¯„ä¼°ã€è§†è§‰é—®ç­”ç³»ç»Ÿçš„ä¼˜åŒ–ä»¥åŠæ•™è‚²å’ŒåŒ»ç–—ç­‰é¢†åŸŸçš„æ™ºèƒ½è¾…åŠ©å†³ç­–ã€‚é€šè¿‡æä¾›æ›´ç²¾ç¡®çš„è¯„ä¼°å·¥å…·ï¼Œç ”ç©¶èƒ½å¤Ÿæ¨åŠ¨VLMsåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæœªæ¥å¯èƒ½å½±å“ç›¸å…³æŠ€æœ¯çš„å‘å±•æ–¹å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Language Models (VLMs) are now sufficiently advanced to support a broad range of applications, including answering complex visual questions, and are increasingly expected to interact with images in varied ways. To evaluate them, current benchmarks often focus on specific domains (e.g., reading charts), constructing datasets of annotated real images paired with pre-defined Multiple Choice Questions (MCQs) to report aggregate accuracy scores. However, such benchmarks entail high annotation costs, risk information leakage, and do not clarify whether failures stem from limitations in visual perception, reasoning, or general knowledge. We propose a new evaluation methodology, inspired by ophthalmologic diagnostics, leveraging procedural generation of synthetic images to obtain control over visual attributes and precisely reveal perception failures in VLMs. Specifically, we build collections of images with gradually more challenging variations in the content of interest (e.g., number of objects in a counting task) while holding other visual parameters constant. This diagnostic allows systematic stress testing and fine-grained failure analysis, shifting the focus from coarse benchmarking toward targeted and interpretable assessment of VLM capabilities. Our code is available at https://github.com/byoeval/BYO-EVAL.

