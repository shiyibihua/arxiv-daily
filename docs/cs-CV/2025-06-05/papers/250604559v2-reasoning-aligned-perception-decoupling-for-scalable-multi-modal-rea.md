---
layout: default
title: Reasoning-Aligned Perception Decoupling for Scalable Multi-modal Reasoning
---

# Reasoning-Aligned Perception Decoupling for Scalable Multi-modal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04559" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04559v2</a>
  <a href="https://arxiv.org/pdf/2506.04559.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04559v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04559v2', 'Reasoning-Aligned Perception Decoupling for Scalable Multi-modal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Xin Jin, Zhenguo Li, James T. Kwok, Yu Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„ŸçŸ¥-æ¨ç†è§£è€¦ä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†çš„å¯æ‰©å±•æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `æ„ŸçŸ¥-æ¨ç†è§£è€¦` `è§†è§‰æ„ŸçŸ¥ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šç›¸å¯¹æ»åï¼Œä¸»è¦ç”±äºå…¶å†…éƒ¨è¯­è¨€æ¨¡å‹è¿‡æ—¶ï¼Œå‡çº§æˆæœ¬é«˜æ˜‚ã€‚
2. æœ¬æ–‡æå‡ºçš„æ„ŸçŸ¥-æ¨ç†è§£è€¦æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å—åŒ–æ¨ç†ç»„ä»¶ï¼Œä½¿å¾—MLLMèƒ½å¤Ÿä¸ä»»ä½•å¤–éƒ¨æ–‡æœ¬æ¨ç†æ¨¡å‹æ— ç¼å¯¹æ¥ã€‚
3. å®éªŒè¯æ˜ï¼ŒRAPIDæ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä¸”æ”¯æŒæ¨ç†æ—¶çš„å¯æ‰©å±•æ€§ï¼Œæ— éœ€é‡è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ¨ç†è¯­è¨€æ¨¡å‹åœ¨åŸºäºæ–‡æœ¬çš„æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»ç„¶æ»åï¼Œä¸»è¦å—é™äºå…¶è¿‡æ—¶çš„å†…éƒ¨è¯­è¨€æ¨¡å‹ã€‚å‡çº§è¿™äº›æ¨¡å‹é€šå¸¸ä»£ä»·é«˜æ˜‚ï¼Œå› ä¸ºéœ€è¦è¿›è¡Œå…¨é¢çš„è§†è§‰-è¯­è¨€å¯¹é½é‡è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ„ŸçŸ¥-æ¨ç†è§£è€¦æ–¹æ³•ï¼Œæ¨¡å—åŒ–MLLMçš„æ¨ç†ç»„ä»¶ï¼Œä½¿å…¶æ˜“äºæ›¿æ¢ã€‚è¯¥æ–¹æ³•é‡æ–°å®šä¹‰äº†MLLMçš„è§’è‰²ï¼Œå°†å¤šæ¨¡æ€è¾“å…¥è½¬æ¢ä¸ºå¯ä»¥è¢«ä»»ä½•å¼ºå¤§çš„å¤–éƒ¨æ–‡æœ¬æ¨ç†æ¨¡å‹å¤„ç†çš„è¯¦ç»†æ–‡æœ¬è¾“å‡ºã€‚ä¸ºä½¿MLLMçš„æ„ŸçŸ¥è¾“å‡ºä¸æœ€ç»ˆæ¨ç†ä»»åŠ¡å¯¹é½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”è§†è§‰æ„ŸçŸ¥ä¼˜åŒ–ï¼ˆVPOï¼‰ã€‚VPOæ ¹æ®å¤–éƒ¨æ¨ç†å™¨ç”Ÿæˆç­”æ¡ˆçš„æ­£ç¡®æ€§å¯¹MLLMè¿›è¡Œå¥–åŠ±ï¼Œä»¥ç”ŸæˆçœŸå®ä¸”ä¸æŸ¥è¯¢ç›¸å…³çš„æè¿°ã€‚å®éªŒè¯æ˜ï¼ŒRAPIDåœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç”±äºè¿‡æ—¶çš„å†…éƒ¨è¯­è¨€æ¨¡å‹å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•åœ¨å‡çº§æ—¶éœ€è¦è¿›è¡Œå…¨é¢çš„è§†è§‰-è¯­è¨€å¯¹é½é‡è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ„ŸçŸ¥-æ¨ç†è§£è€¦æ–¹æ³•ï¼Œé€šè¿‡å°†MLLMçš„æ¨ç†ç»„ä»¶æ¨¡å—åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿä¸å¤–éƒ¨å¼ºå¤§çš„æ–‡æœ¬æ¨ç†æ¨¡å‹è¿›è¡Œæœ‰æ•ˆå¯¹æ¥ï¼Œä»è€Œæå‡å¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—å’Œæ¨ç†æ¨¡å—ã€‚æ„ŸçŸ¥æ¨¡å—è´Ÿè´£å¤„ç†å¤šæ¨¡æ€è¾“å…¥å¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºï¼Œè€Œæ¨ç†æ¨¡å—åˆ™åˆ©ç”¨å¤–éƒ¨æ–‡æœ¬æ¨ç†æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚é€šè¿‡è§†è§‰æ„ŸçŸ¥ä¼˜åŒ–ï¼ˆVPOï¼‰ç®—æ³•ï¼Œæ„ŸçŸ¥æ¨¡å—çš„è¾“å‡ºä¸æ¨ç†ä»»åŠ¡è¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ„ŸçŸ¥-æ¨ç†è§£è€¦çš„è®¾è®¡ï¼Œä½¿å¾—MLLMçš„æ¨ç†èƒ½åŠ›ä¸å†ä¾èµ–äºå…¶å†…éƒ¨ç»“æ„ï¼Œè€Œæ˜¯å¯ä»¥çµæ´»åœ°ä¸ä»»ä½•å¤–éƒ¨æ¨ç†æ¨¡å‹ç»“åˆï¼Œæå¤§åœ°æé«˜äº†å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨VPOç®—æ³•ä¸­ï¼Œè®¾è®¡äº†å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®å¤–éƒ¨æ¨ç†å™¨ç”Ÿæˆç­”æ¡ˆçš„æ­£ç¡®æ€§å¯¹MLLMè¿›è¡Œå¥–åŠ±ï¼Œä»¥ä¼˜åŒ–å…¶è¾“å‡ºçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAPIDåœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒæè¿°ç”Ÿæˆå’Œå¤šæ¨¡æ€æœç´¢å¼•æ“ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼ŒRAPIDå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´ä¸ºå‡†ç¡®å’Œç›¸å…³çš„ç»“æœï¼Œå…·æœ‰å¹¿æ³›çš„å•†ä¸šä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent breakthroughs in reasoning language models have significantly advanced text-based reasoning. On the other hand, Multi-modal Large Language Models (MLLMs) still lag behind, hindered by their outdated internal LLMs. Upgrading these is often prohibitively expensive, as it requires complete vision-language alignment retraining which is costly. To address this issue, we introduce Perception-Reasoning Decoupling, which modularizes the MLLM's reasoning component and makes it easily replaceable. This approach redefines the MLLM's role to convert multi-modal inputs into detailed textual outputs that can be processed by any powerful, external, text-only LLM reasoners. To align the MLLM's perceptual output with the final reasoning task, we propose a novel reinforcement learning algorithm called Visual Perception Optimization (VPO). VPO rewards the MLLM based on the correctness of answers generated by the external reasoner to produce faithful and query-relevant captions. Together, this decoupling pipeline and VPO form our Reasoning-Aligned PerceptIon Decoupling (RAPID) approach. Empirical results show that RAPID achieves significant performance gains on multi-modal reasoning benchmarks. Crucially, RAPID enables a novel inference-time scaling paradigm: Once trained with VPO, the MLLM can be paired with any state-of-the-art LLM reasoner for consistent performance improvement without retraining.

