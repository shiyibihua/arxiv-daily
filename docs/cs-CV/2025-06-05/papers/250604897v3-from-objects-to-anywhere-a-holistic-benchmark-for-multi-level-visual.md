---
layout: default
title: From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes
---

# From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04897" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04897v3</a>
  <a href="https://arxiv.org/pdf/2506.04897.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04897v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04897v3', 'From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianxu Wang, Zhuofan Zhang, Ziyu Zhu, Yue Fan, Jing Xiong, Pengxiang Li, Xiaojian Ma, Qing Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: Update v3 of the NeurIPS 2025 Datasets and Benchmarks paper (v2), including additional evaluations of state-of-the-art multimodal large language models. Project page: https://anywhere-3d.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAnywhere3D-Benchä»¥è§£å†³3Dåœºæ™¯ä¸­çš„å¤šå±‚æ¬¡è§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šå±‚æ¬¡åŸºå‡†` `ç©ºé—´å…³ç³»` `ç»†ç²’åº¦æ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•ä¸»è¦é›†ä¸­äºç‰©ä½“å±‚æ¬¡ï¼Œç¼ºä¹å¯¹æ›´å¤æ‚ç©ºé—´å…³ç³»å’Œç»†ç²’åº¦éƒ¨åˆ†çš„ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒAnywhere3D-Benchï¼Œæ¶µç›–å¤šå±‚æ¬¡çš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œä¿ƒè¿›å¯¹3Dåœºæ™¯çš„å…¨é¢ç†è§£ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç©ºé—´å±‚æ¬¡å’Œéƒ¨åˆ†å±‚æ¬¡ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä»…çº¦30%å’Œ40%ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dè§†è§‰å®šä½åœ¨å¤æ‚3Dåœºæ™¯ä¸­å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¶…è¶Šç‰©ä½“çš„æŒ‡ç§°è¡¨è¾¾å®šä½ä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†Anywhere3D-Benchï¼Œä¸€ä¸ªåŒ…å«2886ä¸ªæŒ‡ç§°è¡¨è¾¾-3Dè¾¹ç•Œæ¡†å¯¹çš„å…¨é¢3Dè§†è§‰å®šä½åŸºå‡†ï¼Œæ¶µç›–å››ä¸ªä¸åŒçš„å®šä½å±‚æ¬¡ï¼šäººç±»æ´»åŠ¨åŒºåŸŸã€ç‰©ä½“ä¹‹å¤–çš„ç©ºé—²ç©ºé—´ã€åœºæ™¯ä¸­çš„å•ä¸ªç‰©ä½“ä»¥åŠç»†ç²’åº¦ç‰©ä½“éƒ¨åˆ†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç©ºé—´å±‚æ¬¡å’Œéƒ¨åˆ†å±‚æ¬¡çš„è§†è§‰å®šä½é¢ä¸´æœ€å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä½äºåŒºåŸŸå±‚æ¬¡å’Œç‰©ä½“å±‚æ¬¡ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³3Dåœºæ™¯ä¸­è¶…è¶Šç‰©ä½“çš„è§†è§‰å®šä½é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç©ºé—´å…³ç³»å’Œç»†ç²’åº¦éƒ¨åˆ†æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ•°æ®é›†ï¼ŒAnywhere3D-Benchï¼Œè®ºæ–‡é¼“åŠ±ç ”ç©¶è€…å…³æ³¨æ›´å¤æ‚çš„ç©ºé—´å’Œéƒ¨åˆ†å±‚æ¬¡çš„è§†è§‰å®šä½ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½æ¯”è¾ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œæ¶µç›–äººç±»æ´»åŠ¨åŒºåŸŸã€ç©ºé—²ç©ºé—´ã€ç‰©ä½“å’Œç‰©ä½“éƒ¨åˆ†çš„å®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¤šå±‚æ¬¡çš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯ç©ºé—´å±‚æ¬¡å’Œéƒ¨åˆ†å±‚æ¬¡çš„æŒ‘æˆ˜ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šæœªå¾—åˆ°å……åˆ†é‡è§†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è¯„ä¼°ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§å…ˆè¿›çš„3Dè§†è§‰å®šä½æ–¹æ³•ï¼Œå¹¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿äº†å®éªŒçš„å…¨é¢æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç©ºé—´å±‚æ¬¡ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º30%å·¦å³ï¼Œè€Œåœ¨éƒ¨åˆ†å±‚æ¬¡ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡çº¦ä¸º40%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†3Dåœºæ™¯æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç©ºé—´å’Œç»†ç²’åº¦å±‚æ¬¡çš„ä»»åŠ¡ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æœºå™¨æ›´å¥½åœ°ç†è§£å’Œäº’åŠ¨å¤æ‚çš„3Dç¯å¢ƒã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D visual grounding has made notable progress in localizing objects within complex 3D scenes. However, grounding referring expressions beyond objects in 3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a holistic 3D visual grounding benchmark consisting of 2,886 referring expression-3D bounding box pairs spanning four different grounding levels: human-activity areas, unoccupied space beyond objects, individual objects in the scene, and fine-grained object parts. We assess a range of state-of-the-art 3D visual grounding methods alongside large language models (LLMs) and multimodal LLMs (MLLMs) on Anywhere3D-Bench. Experimental results reveal that space-level and part-level visual grounding pose the greatest challenges: space-level tasks require a more comprehensive spatial reasoning ability, for example, modeling distances and spatial relations within 3D space, while part-level tasks demand fine-grained perception of object composition. Even the best-performing models, Google Gemini-2.5-Pro and OpenAI o3, achieve just around 30% accuracy on space-level tasks and around 40% on part-level tasks, significantly lower than its performance on area-level and object-level tasks. These findings underscore a critical gap in current models' capacity to understand and reason about 3D scenes beyond object-level semantics.

