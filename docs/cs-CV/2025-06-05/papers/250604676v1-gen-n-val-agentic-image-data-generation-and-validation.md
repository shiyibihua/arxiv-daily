---
layout: default
title: Gen-n-Val: Agentic Image Data Generation and Validation
---

# Gen-n-Val: Agentic Image Data Generation and Validation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04676" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04676v1</a>
  <a href="https://arxiv.org/pdf/2506.04676.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04676v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04676v1', 'Gen-n-Val: Agentic Image Data Generation and Validation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing-En Huang, I-Sheng Fang, Tzuhsuan Huang, Chih-Yu Wang, Jun-Cheng Chen

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGen-n-Valæ¡†æ¶ä»¥è§£å†³è®¡ç®—æœºè§†è§‰ä¸­çš„æ•°æ®ç¨€ç¼ºä¸æ ‡ç­¾å™ªå£°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆæˆæ•°æ®ç”Ÿæˆ` `è®¡ç®—æœºè§†è§‰` `ç›®æ ‡æ£€æµ‹` `å®ä¾‹åˆ†å‰²` `å±‚æ‰©æ•£` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®éªŒè¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†å¤šå¯¹è±¡æ©è†œã€ä¸å‡†ç¡®çš„åˆ†å‰²å’Œé”™è¯¯çš„ç±»åˆ«æ ‡ç­¾ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚
2. Gen-n-Valæ¡†æ¶é€šè¿‡å¼•å…¥å±‚æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–ç”Ÿæˆå•å¯¹è±¡åˆæˆæ•°æ®çš„è¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„å®ä¾‹æ©è†œç²¾ç¡®ä¸”èƒŒæ™¯å¹²å‡€ã€‚
3. ä¸MosaicFusionç­‰æœ€å…ˆè¿›çš„åˆæˆæ•°æ®æ–¹æ³•ç›¸æ¯”ï¼ŒGen-n-Valå°†æ— æ•ˆåˆæˆæ•°æ®æ¯”ä¾‹ä»50%é™ä½è‡³7%ï¼Œå¹¶åœ¨COCOå®ä¾‹åˆ†å‰²ä¸­æå‡äº†1%çš„mAPã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­ï¼Œæ•°æ®ç¨€ç¼ºå’Œæ ‡ç­¾å™ªå£°ä»ç„¶æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Gen-n-Valï¼Œä¸€ä¸ªæ–°é¢–çš„ä»£ç†æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å±‚æ‰©æ•£ï¼ˆLayer Diffusionï¼‰ã€LLMså’ŒVLLMsç”Ÿæˆé«˜è´¨é‡çš„å•å¯¹è±¡æ©è†œå’Œå¤šæ ·åŒ–èƒŒæ™¯ã€‚Gen-n-Valç”±ä¸¤ä¸ªä»£ç†ç»„æˆï¼šLDæç¤ºä»£ç†å’Œæ•°æ®éªŒè¯ä»£ç†ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„åˆæˆæ•°æ®æ–¹æ³•ç›¸æ¯”ï¼ŒGen-n-Valæ˜¾è‘—å‡å°‘äº†æ— æ•ˆåˆæˆæ•°æ®ï¼Œå¹¶åœ¨COCOå®ä¾‹åˆ†å‰²å’Œå¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹åŸºå‡†ä¸Šæé«˜äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æ•°æ®ç¨€ç¼ºå’Œæ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œç°æœ‰åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆå¤šå¯¹è±¡æ©è†œå’Œå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGen-n-Valæ¡†æ¶é€šè¿‡ç»“åˆå±‚æ‰©æ•£ã€LLMså’ŒVLLMsï¼Œä¼˜åŒ–åˆæˆæ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„å•å¯¹è±¡æ©è†œé«˜è´¨é‡ä¸”èƒŒæ™¯å¤šæ ·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGen-n-Valç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šLDæç¤ºä»£ç†å’Œæ•°æ®éªŒè¯ä»£ç†ã€‚LDæç¤ºä»£ç†ä¼˜åŒ–ç”Ÿæˆæç¤ºä»¥äº§ç”Ÿé«˜è´¨é‡å‰æ™¯å›¾åƒå’Œåˆ†å‰²æ©è†œï¼Œè€Œæ•°æ®éªŒè¯ä»£ç†åˆ™è¿‡æ»¤ä½è´¨é‡åˆæˆå®ä¾‹å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šGen-n-Valçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡ä¼˜åŒ–æç¤ºç”Ÿæˆå•å¯¹è±¡åˆæˆæ•°æ®ï¼Œå¹¶åˆ©ç”¨å›¾åƒå’Œè°åŒ–æŠ€æœ¯å°†å¤šä¸ªå®ä¾‹ç»“åˆåœ¨åœºæ™¯ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆæ•°æ®çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä½¿ç”¨TextGradå¯¹ä¸¤ä¸ªä»£ç†çš„æç¤ºè¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œç¡®ä¿ç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨å®ä¾‹æ©è†œå’ŒèƒŒæ™¯æ–¹é¢è¾¾åˆ°é«˜æ ‡å‡†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGen-n-Valåœ¨COCOå®ä¾‹åˆ†å‰²ä¸­å°†æ— æ•ˆåˆæˆæ•°æ®æ¯”ä¾‹ä»50%é™ä½è‡³7%ï¼Œå¹¶åœ¨ç¨€æœ‰ç±»åˆ«ä¸Šæå‡äº†1%çš„mAPã€‚æ­¤å¤–ï¼Œåœ¨å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹åŸºå‡†ä¸Šï¼ŒGen-n-Valç›¸è¾ƒäºYOLO-Worldv2-Mæå‡äº†7.1%çš„mAPï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Gen-n-Valæ¡†æ¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ã€‚é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆæœï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, Large Language Models (LLMs) and Vision Large Language Models (VLLMs) have demonstrated impressive performance as agents across various tasks while data scarcity and label noise remain significant challenges in computer vision tasks, such as object detection and instance segmentation. A common solution for resolving these issues is to generate synthetic data. However, current synthetic data generation methods struggle with issues, such as multiple objects per mask, inaccurate segmentation, and incorrect category labels, limiting their effectiveness. To address these issues, we introduce Gen-n-Val, a novel agentic data generation framework that leverages Layer Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt agent, an LLM, optimizes prompts for LD to generate high-quality foreground instance images and segmentation masks. These optimized prompts ensure the generation of single-object synthetic data with precise instance masks and clean backgrounds. (2) The data validation agent, a VLLM, which filters out low-quality synthetic instance images. The system prompts for both agents are refined through TextGrad. Additionally, we use image harmonization to combine multiple instances within scenes. Compared to state-of-the-art synthetic data approaches like MosaicFusion, our approach reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance of YOLOv9 and YOLO11 families in instance segmentation and object detection.

