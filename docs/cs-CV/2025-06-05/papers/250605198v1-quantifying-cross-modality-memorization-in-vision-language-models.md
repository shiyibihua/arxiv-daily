---
layout: default
title: Quantifying Cross-Modality Memorization in Vision-Language Models
---

# Quantifying Cross-Modality Memorization in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05198" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05198v1</a>
  <a href="https://arxiv.org/pdf/2506.05198.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05198v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05198v1', 'Quantifying Cross-Modality Memorization in Vision-Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuxin Wen, Yangsibo Huang, Tom Goldstein, Ravi Kumar, Badih Ghazi, Chiyuan Zhang

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÈáèÂåñËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑË∑®Ê®°ÊÄÅËÆ∞ÂøÜ‰ª•ÊèêÂçáÁü•ËØÜËøÅÁßªËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ë∑®Ê®°ÊÄÅËÆ∞ÂøÜ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Áü•ËØÜËøÅÁßª` `ÂêàÊàêÊï∞ÊçÆÈõÜ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Á•ûÁªèÁΩëÁªú` `ÊÄßËÉΩËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âçï‰∏ÄÊ®°ÊÄÅÁöÑËÆ∞ÂøÜÔºåÁº∫‰πèÂØπË∑®Ê®°ÊÄÅËÆ∞ÂøÜÁöÑÁ≥ªÁªüÊÄßÁ†îÁ©∂ÔºåÂØºËá¥Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªËÉΩÂäõ‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÈÄöËøáÂºïÂÖ•ÂêàÊàêÁöÑ‰∫∫Áâ©Êï∞ÊçÆÈõÜÔºåÈáèÂåñË∑®Ê®°ÊÄÅËÆ∞ÂøÜÂíåÂèØËøÅÁßªÊÄßÔºåÊé¢Á¥¢Âú®ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠Â¶Ç‰ΩïÊúâÊïàÂÆûÁé∞Áü•ËØÜËøÅÁßª„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Ë∑®Ê®°ÊÄÅËÆ∞ÂøÜÂ≠òÂú®Ôºå‰ΩÜÊ∫êÊ®°ÊÄÅ‰∏éÁõÆÊ†áÊ®°ÊÄÅ‰πãÈó¥ÁöÑÁü•ËØÜÂõûÂøÜÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÊèêÂá∫ÁöÑÂü∫Á∫øÊñπÊ≥ïÊúâÂä©‰∫éÊîπÂñÑËøô‰∏ÄÈóÆÈ¢ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁêÜËß£Á•ûÁªèÁΩëÁªúÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â¶Ç‰ΩïËÆ∞ÂøÜËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÊΩúÂú®ÊïèÊÑü‰ø°ÊÅØÂíåÁü•ËØÜËé∑ÂèñÊñπÈù¢„ÄÇÂ∞ΩÁÆ°‰ª•ÂæÄÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âçï‰∏ÄÊ®°ÊÄÅÁöÑËÆ∞ÂøÜ‰∏äÔºåÁªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ë∂äÊù•Ë∂äÊôÆÈÅç„ÄÇÊú¨ÊñáËÅöÁÑ¶‰∫éË∑®Ê®°ÊÄÅËÆ∞ÂøÜÁöÑÁã¨ÁâπÁâπÂæÅÔºåÁ≥ªÁªüÁ†îÁ©∂ËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨ÂºïÂÖ•ÂêàÊàêÁöÑ‰∫∫Áâ©Êï∞ÊçÆÈõÜÔºåÈÄöËøáÂú®Âçï‰∏ÄÊ®°ÊÄÅ‰∏äËÆ≠ÁªÉÊ®°ÂûãÂπ∂ËØÑ‰º∞ÂÖ∂Âú®Âè¶‰∏ÄÊ®°ÊÄÅ‰∏äÁöÑË°®Áé∞ÔºåÈáèÂåñ‰∫ãÂÆûÁü•ËØÜÁöÑËÆ∞ÂøÜÂíåË∑®Ê®°ÊÄÅÂèØËøÅÁßªÊÄß„ÄÇÁªìÊûúË°®ÊòéÔºåÂçï‰∏ÄÊ®°ÊÄÅÂ≠¶‰π†ÁöÑ‰∫ãÂÆûËÉΩÂ§üËøÅÁßªËá≥Âè¶‰∏ÄÊ®°ÊÄÅÔºå‰ΩÜÊ∫êÊ®°ÊÄÅ‰∏éÁõÆÊ†áÊ®°ÊÄÅ‰πãÈó¥Â≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫Á∫øÊñπÊ≥ï‰ª•ÁºìËß£Ëøô‰∏ÄÊåëÊàòÔºåÊúüÊúõÊøÄÂèëÊú™Êù•Âú®Â§öÊ®°ÊÄÅÂ≠¶‰π†ÊäÄÊúØ‰∏äÁöÑÁ†îÁ©∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ë∑®Ê®°ÊÄÅËÆ∞ÂøÜÁöÑÈáèÂåñÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§öÊ®°ÊÄÅÁü•ËØÜËøÅÁßªÊó∂Â≠òÂú®ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ùÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∫êÊ®°ÊÄÅ‰∏éÁõÆÊ†áÊ®°ÊÄÅ‰πãÈó¥ÁöÑÁü•ËØÜÂõûÂøÜËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÊûÑÂª∫ÂêàÊàêÁöÑ‰∫∫Áâ©Êï∞ÊçÆÈõÜÔºåËøõË°åÁ≥ªÁªüÂÆûÈ™åÔºåÈáèÂåñ‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑÁü•ËØÜËøÅÁßªËÉΩÂäõÔºåÊé¢Á¥¢Â¶Ç‰ΩïÊèêÈ´òË∑®Ê®°ÊÄÅÁöÑËÆ∞ÂøÜÊïàÊûú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÈõÜÊûÑÂª∫„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÈ¶ñÂÖàÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÂêàÊàê‰∫∫Áâ©ÂõæÂÉèÂèäÂÖ∂ÊñáÊú¨ÊèèËø∞ÔºåÁÑ∂ÂêéÂú®Âçï‰∏ÄÊ®°ÊÄÅ‰∏äËÆ≠ÁªÉÊ®°ÂûãÔºåÊúÄÂêéÂú®Âè¶‰∏ÄÊ®°ÊÄÅ‰∏äËøõË°åÊÄßËÉΩËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑÂàõÊñ∞Âú®‰∫éÁ≥ªÁªüÊÄßÂú∞ÈáèÂåñË∑®Ê®°ÊÄÅËÆ∞ÂøÜÔºåÊè≠Á§∫‰∫Ü‰∏çÂêåÊ®°ÊÄÅÈó¥Áü•ËØÜËøÅÁßªÁöÑÊΩúÂäõ‰∏éÂ±ÄÈôêÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫Á∫øÊñπÊ≥ïÊù•ÁºìËß£‰ø°ÊÅØÂõûÂøÜÁöÑÂ∑ÆË∑ù„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÊ®°ÂûãÁöÑË∑®Ê®°ÊÄÅÂ≠¶‰π†ËÉΩÂäõÔºåÂπ∂ÈÄöËøáÂ§öÁßçÊ®°ÂûãÊû∂ÊûÑËøõË°åÂØπÊØîÔºåÁ°Æ‰øùÁªìÊûúÁöÑÂèØÈù†ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Ë∑®Ê®°ÊÄÅÁü•ËØÜËøÅÁßªÂ≠òÂú®Ôºå‰ΩÜÊ∫êÊ®°ÊÄÅ‰∏éÁõÆÊ†áÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂõûÂøÜÂ∑ÆË∑ùÊòæËëó„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊ®°ÂûãÂú®Ê∫êÊ®°ÊÄÅÁöÑÁü•ËØÜÂõûÂøÜËÉΩÂäõ‰∏éÁõÆÊ†áÊ®°ÊÄÅÁõ∏ÊØîÔºåÂ≠òÂú®Á∫¶30%ÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÊèêÂá∫ÁöÑÂü∫Á∫øÊñπÊ≥ïÊúâÊïàÊîπÂñÑ‰∫ÜËøô‰∏ÄÈóÆÈ¢òÔºåÊèêÂçá‰∫ÜË∑®Ê®°ÊÄÅÁöÑÁü•ËØÜËøÅÁßªÊïàÊûú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Â§öÊ®°ÊÄÅÂ≠¶‰π†„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆóÊú∫ËßÜËßâÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÂçáË∑®Ê®°ÊÄÅÁü•ËØÜËøÅÁßªËÉΩÂäõÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊîØÊåÅÊô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®ÂåñÂÜÖÂÆπÁîüÊàêÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÂÆûÈôÖÂ∫îÁî®ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõÊ≠•‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding what and how neural networks memorize during training is crucial, both from the perspective of unintentional memorization of potentially sensitive information and from the standpoint of effective knowledge acquisition for real-world, knowledge-intensive tasks. While previous studies primarily investigate memorization within a single modality, such as text memorization in large language models or image memorization in diffusion models, unified multimodal models are becoming increasingly prevalent in practical applications. In this work, we focus on the unique characteristics of cross-modality memorization and conduct a systematic study centered on vision-language models. To facilitate controlled experiments, we first introduce a synthetic persona dataset comprising diverse synthetic person images and textual descriptions. We quantify factual knowledge memorization and cross-modal transferability by training models on a single modality and evaluating their performance in the other. Our results reveal that facts learned in one modality transfer to the other, but a significant gap exists between recalling information in the source and target modalities. Furthermore, we observe that this gap exists across various scenarios, including more capable models, machine unlearning, and the multi-hop case. At the end, we propose a baseline method to mitigate this challenge. We hope our study can inspire future research on developing more robust multimodal learning techniques to enhance cross-modal transferability.

