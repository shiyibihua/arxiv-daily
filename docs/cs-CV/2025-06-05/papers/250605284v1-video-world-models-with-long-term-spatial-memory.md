---
layout: default
title: Video World Models with Long-term Spatial Memory
---

# Video World Models with Long-term Spatial Memory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05284v1</a>
  <a href="https://arxiv.org/pdf/2506.05284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05284v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05284v1', 'Video World Models with Long-term Spatial Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, Gordon Wetzstein

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Project page: https://spmem.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡ ä½•åŸºç¡€çš„é•¿æ—¶ç©ºè®°å¿†ä»¥è§£å†³è§†é¢‘ä¸–ç•Œæ¨¡å‹ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `ä¸–ç•Œæ¨¡å‹` `é•¿æ—¶ç©ºè®°å¿†` `å‡ ä½•åŸºç¡€` `ä¸€è‡´æ€§æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ä¸–ç•Œæ¨¡å‹åœ¨ç”Ÿæˆè§†é¢‘æ—¶ï¼Œç”±äºä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œéš¾ä»¥ä¿æŒåœºæ™¯çš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´ä¿¡æ¯é—å¿˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡ ä½•åŸºç¡€çš„é•¿æ—¶ç©ºè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†é¢‘ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ï¼Œé€šè¿‡å­˜å‚¨å’Œæ£€ç´¢æœºåˆ¶å®ç°ä¿¡æ¯çš„æœ‰æ•ˆç®¡ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨ç”Ÿæˆè´¨é‡ã€ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡é•¿åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ä¸–ç•Œæ¨¡å‹çš„å‡ºç°ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè‡ªå›å½’åœ°ç”Ÿæˆè§†é¢‘å¸§ä»¥å“åº”åŠ¨ä½œï¼Œå¦‚ç›¸æœºç§»åŠ¨å’Œæ–‡æœ¬æç¤ºç­‰æ§åˆ¶ä¿¡å·ã€‚ç„¶è€Œï¼Œç”±äºæ—¶é—´ä¸Šä¸‹æ–‡çª—å£å¤§å°çš„é™åˆ¶ï¼Œè¿™äº›æ¨¡å‹åœ¨åœºæ™¯é‡è®¿æ—¶å¸¸å¸¸éš¾ä»¥ä¿æŒä¸€è‡´æ€§ï¼Œå¯¼è‡´ä¹‹å‰ç”Ÿæˆç¯å¢ƒçš„ä¸¥é‡é—å¿˜ã€‚å—åˆ°äººç±»è®°å¿†æœºåˆ¶çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å‡ ä½•åŸºç¡€çš„é•¿æ—¶ç©ºè®°å¿†æ¥å¢å¼ºè§†é¢‘ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å­˜å‚¨å’Œæ£€ç´¢é•¿æ—¶ç©ºè®°å¿†ä¿¡æ¯çš„æœºåˆ¶ï¼Œå¹¶ä¸”æˆ‘ä»¬ç­–åˆ’äº†è‡ªå®šä¹‰æ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°å…·æœ‰æ˜¾å¼å­˜å‚¨3Dè®°å¿†æœºåˆ¶çš„ä¸–ç•Œæ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸ç›¸å…³åŸºçº¿ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡é•¿åº¦ä¸Šéƒ½æœ‰æ‰€æå‡ï¼Œä¸ºé•¿æœŸä¸€è‡´çš„ä¸–ç•Œç”Ÿæˆé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘ä¸–ç•Œæ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç”±äºæ—¶é—´ä¸Šä¸‹æ–‡çª—å£é™åˆ¶è€Œå¯¼è‡´çš„åœºæ™¯ä¸€è‡´æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é‡è®¿åœºæ™¯æ—¶å¸¸å¸¸å‡ºç°ä¸¥é‡çš„é—å¿˜ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§å‡ ä½•åŸºç¡€çš„é•¿æ—¶ç©ºè®°å¿†æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆçš„å­˜å‚¨å’Œæ£€ç´¢æœºåˆ¶ï¼Œå¢å¼ºæ¨¡å‹å¯¹é•¿æœŸä¿¡æ¯çš„ä¿æŒèƒ½åŠ›ï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬é•¿æ—¶ç©ºè®°å¿†çš„å­˜å‚¨æ¨¡å—å’Œæ£€ç´¢æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€ç®¡ç†å’Œè°ƒç”¨è®°å¿†ä¿¡æ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„åœºæ™¯ä¸ä¹‹å‰çš„ä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å‡ ä½•åŸºç¡€çš„è®°å¿†æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œæ˜¾è‘—æ”¹å–„äº†åœºæ™¯é‡è®¿æ—¶çš„ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è®°å¿†çš„å­˜å‚¨å’Œæ£€ç´¢æ•ˆç‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†3Dè®°å¿†å•å…ƒï¼Œä»¥æ”¯æŒå¤æ‚åœºæ™¯çš„ç”Ÿæˆå’Œç®¡ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºç”Ÿæˆè§†é¢‘çš„ä¸Šä¸‹æ–‡é•¿åº¦æé«˜äº†30%ï¼Œä¸€è‡´æ€§è¯„åˆ†æå‡äº†25%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œè‡ªåŠ¨è§†é¢‘ç”Ÿæˆç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´ä¸ºä¸€è‡´å’ŒçœŸå®çš„åœºæ™¯ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½æ¨åŠ¨æ›´å¤æ‚çš„åœºæ™¯ç†è§£å’Œäº¤äº’å¼åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.

