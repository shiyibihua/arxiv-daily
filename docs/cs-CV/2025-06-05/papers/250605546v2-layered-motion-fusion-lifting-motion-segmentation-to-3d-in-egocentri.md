---
layout: default
title: Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos
---

# Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05546" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05546v2</a>
  <a href="https://arxiv.org/pdf/2506.05546.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05546v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05546v2', 'Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vadim Tschernezki, Diane Larlus, Iro Laina, Andrea Vedaldi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-22)

**å¤‡æ³¨**: Camera-ready for CVPR25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å±‚è¿åŠ¨èåˆä»¥è§£å†³åŠ¨æ€è§†é¢‘ä¸­çš„è¿åŠ¨åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `åŠ¨æ€è§†é¢‘åˆ†æ` `è¿åŠ¨åˆ†å‰²` `3Dè§†è§‰` `åˆ†å±‚è¾å°„åœº` `è®¡ç®—æœºè§†è§‰` `è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘` `æµ‹è¯•æ—¶ç»†åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3DæŠ€æœ¯åœ¨å¤„ç†åŠ¨æ€ç°è±¡æ—¶æ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨ç‰©ä½“çš„åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å°†2Dè¿åŠ¨åˆ†å‰²é¢„æµ‹èåˆåˆ°åˆ†å±‚è¾å°„åœºä¸­ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶ç»†åŒ–æ¥é™ä½æ•°æ®å¤æ‚æ€§ï¼Œä»è€Œæ”¹å–„åŠ¨æ€åˆ†å‰²ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨3Dæ¨¡å‹çš„åˆ†å‰²é¢„æµ‹ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„2DåŸºçº¿ï¼Œå±•ç¤ºäº†3DæŠ€æœ¯åœ¨åŠ¨æ€åœºæ™¯åˆ†æä¸­çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¡ç®—æœºè§†è§‰ä¸»è¦åŸºäº2DæŠ€æœ¯ï¼Œè€Œ3Dè§†è§‰ä»ç„¶å±€é™äºç›¸å¯¹ç‹­çª„çš„åº”ç”¨é¢†åŸŸã€‚è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡èåˆç‹¬ç«‹çš„2Dè§†å›¾å¹¶è¿›è¡Œå»å™ªï¼Œ3DæŠ€æœ¯èƒ½å¤Ÿæ”¹å–„è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­ã€‚ç„¶è€Œï¼ŒEPIC Fieldsçš„åˆ†ææ˜¾ç¤ºï¼Œ3DæŠ€æœ¯åœ¨åŠ¨æ€ç°è±¡ç ”ç©¶ä¸­æ•ˆæœä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§»åŠ¨ç‰©ä½“çš„åˆ†å‰²ä¸Šã€‚æœ¬æ–‡æå‡ºé€šè¿‡å°†2Dæ¨¡å‹çš„è¿åŠ¨åˆ†å‰²é¢„æµ‹èåˆåˆ°åˆ†å±‚è¾å°„åœºä¸­æ¥æ”¹å–„3DåŠ¨æ€åˆ†å‰²ï¼Œå¹¶é€šè¿‡æµ‹è¯•æ—¶çš„ç»†åŒ–æ¥åº”å¯¹é•¿åŠ¨æ€è§†é¢‘çš„å¤æ‚æ€§ï¼Œä»è€Œå®ç°è¿åŠ¨èåˆä¸ç»†åŒ–çš„ååŒï¼Œæ˜¾è‘—æå‡3Dæ¨¡å‹çš„åˆ†å‰²é¢„æµ‹æ•ˆæœï¼Œè¶…è¶Š2DåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨åŠ¨æ€è§†é¢‘ä¸­è¿›è¡Œè¿åŠ¨åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶éš¾ä»¥æ•æ‰å‡ ä½•ç»“æ„ï¼Œå¯¼è‡´è¿åŠ¨çº¿ç´¢çš„èåˆå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†2Dæ¨¡å‹çš„è¿åŠ¨åˆ†å‰²é¢„æµ‹èåˆåˆ°åˆ†å±‚è¾å°„åœºä¸­ï¼Œç»“åˆæµ‹è¯•æ—¶çš„ç»†åŒ–ç­–ç•¥ï¼Œèšç„¦äºç‰¹å®šå¸§ä»¥é™ä½æ•°æ®å¤æ‚æ€§ï¼Œä»è€Œæå‡åŠ¨æ€åˆ†å‰²æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯2Dè¿åŠ¨åˆ†å‰²æ¨¡å‹ï¼Œå…¶æ¬¡æ˜¯åˆ†å±‚è¾å°„åœºçš„èåˆæ¨¡å—ã€‚æµ‹è¯•æ—¶ç»†åŒ–è¿‡ç¨‹åˆ™åœ¨æ¨¡å‹æ¨ç†é˜¶æ®µè¿›è¡Œï¼Œä»¥å¢å¼ºå¯¹åŠ¨æ€åœºæ™¯çš„ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†åˆ†å±‚è¿åŠ¨èåˆçš„æ–¹æ³•ï¼Œé€šè¿‡å°†2Dè¿åŠ¨åˆ†å‰²ä¿¡æ¯æœ‰æ•ˆæ•´åˆåˆ°3Dæ¨¡å‹ä¸­ï¼Œå…‹æœäº†ä¼ ç»Ÿ3DæŠ€æœ¯åœ¨åŠ¨æ€ç°è±¡åˆ†æä¸­çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è¿åŠ¨åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„ä»¥é€‚åº”é•¿è§†é¢‘çš„å¤æ‚æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„åˆ†å±‚è¿åŠ¨èåˆæ–¹æ³•åœ¨3Dæ¨¡å‹çš„åˆ†å‰²é¢„æµ‹ä¸Šç›¸è¾ƒäºä¼ ç»Ÿ2DåŸºçº¿æå‡äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼ŒéªŒè¯äº†3DæŠ€æœ¯åœ¨åŠ¨æ€ç°è±¡åˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œæ™ºèƒ½ç›‘æ§ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡åŠ¨æ€è§†é¢‘åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Computer vision is largely based on 2D techniques, with 3D vision still relegated to a relatively narrow subset of applications. However, by building on recent advances in 3D models such as neural radiance fields, some authors have shown that 3D techniques can at last improve outputs extracted from independent 2D views, by fusing them into 3D and denoising them. This is particularly helpful in egocentric videos, where the camera motion is significant, but only under the assumption that the scene itself is static. In fact, as shown in the recent analysis conducted by EPIC Fields, 3D techniques are ineffective when it comes to studying dynamic phenomena, and, in particular, when segmenting moving objects. In this paper, we look into this issue in more detail. First, we propose to improve dynamic segmentation in 3D by fusing motion segmentation predictions from a 2D-based model into layered radiance fields (Layered Motion Fusion). However, the high complexity of long, dynamic videos makes it challenging to capture the underlying geometric structure, and, as a result, hinders the fusion of motion cues into the (incomplete) scene geometry. We address this issue through test-time refinement, which helps the model to focus on specific frames, thereby reducing the data complexity. This results in a synergy between motion fusion and the refinement, and in turn leads to segmentation predictions of the 3D model that surpass the 2D baseline by a large margin. This demonstrates that 3D techniques can enhance 2D analysis even for dynamic phenomena in a challenging and realistic setting.

