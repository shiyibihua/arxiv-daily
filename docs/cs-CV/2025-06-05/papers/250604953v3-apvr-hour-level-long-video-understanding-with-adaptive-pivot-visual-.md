---
layout: default
title: APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval
---

# APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04953" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04953v3</a>
  <a href="https://arxiv.org/pdf/2506.04953.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04953v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04953v3', 'APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hong Gao, Yiming Bao, Xuezhen Tu, Bin Zhong, Linan Yue, Minling Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-11-15)

**å¤‡æ³¨**: Accepted by AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAPVRä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¿¡æ¯æ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `è‡ªé€‚åº”æ£€ç´¢` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰ä¿¡æ¯å¤„ç†` `æ— è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å°æ—¶çº§è§†é¢‘æ—¶é¢ä¸´å†…å­˜å’Œèµ„æºé™åˆ¶ï¼Œå¯¼è‡´ç†è§£èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„APVRæ¡†æ¶é€šè¿‡åˆ†å±‚æ£€ç´¢é‡è¦è§†è§‰ä¿¡æ¯ï¼Œé‡‡ç”¨æ¢è½´å¸§å’Œæ¢è½´ä»¤ç‰Œæ£€ç´¢æŠ€æœ¯ï¼Œè§£å†³äº†ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAPVRåœ¨LongVideoBenchã€VideoMMEå’ŒMLVUåŸºå‡†ä¸Šåˆ†åˆ«æå‡äº†9.5%ã€4.6%å’Œ9.7%çš„æ€§èƒ½ï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°æ—¶çº§è§†é¢‘ç†è§£æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸ä»…éœ€è¦å¤„ç†å¤§é‡ä¿¡æ¯ï¼Œè¿˜éœ€å…‹æœè®­ç»ƒå’Œæ¨ç†ä¸­çš„å†…å­˜é™åˆ¶ã€‚å°½ç®¡è¿‘æœŸçš„æ— è®­ç»ƒæ–¹æ³•é€šè¿‡å‹ç¼©è§†è§‰ç‰¹å¾å‡è½»äº†èµ„æºéœ€æ±‚ï¼Œä½†å¯¹ä¸å®Œæ•´è§†è§‰ä¿¡æ¯çš„ä¾èµ–é™åˆ¶äº†æ€§èƒ½æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”æ¢è½´è§†è§‰ä¿¡æ¯æ£€ç´¢ï¼ˆAPVRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåˆ†å±‚æ£€ç´¢å’Œä¿ç•™è¶³å¤Ÿä¸”é‡è¦çš„è§†è§‰ä¿¡æ¯ã€‚APVRé€šè¿‡ä¸¤ä¸ªäº’è¡¥ç»„ä»¶çªç ´äº†å†…å­˜é™åˆ¶ï¼šæ¢è½´å¸§æ£€ç´¢å’Œæ¢è½´ä»¤ç‰Œæ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPVRåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°æ—¶çº§è§†é¢‘ç†è§£ä¸­çš„å†…å­˜å’Œä¿¡æ¯å¤„ç†ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œå¸¸å¸¸é¢ä¸´ä¿¡æ¯é‡å¤§å’Œèµ„æºæ¶ˆè€—é«˜çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAPVRæ¡†æ¶é€šè¿‡è‡ªé€‚åº”åœ°æ£€ç´¢å’Œä¿ç•™é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œé‡‡ç”¨åˆ†å±‚æ£€ç´¢ç­–ç•¥æ¥å…‹æœå†…å­˜é™åˆ¶ã€‚é€šè¿‡æ¢è½´å¸§æ£€ç´¢å’Œæ¢è½´ä»¤ç‰Œæ£€ç´¢ï¼Œç¡®ä¿åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ä¿æŒè¯­ä¹‰çš„å®Œæ•´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAPVRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ¢è½´å¸§æ£€ç´¢æ¨¡å—å’Œæ¢è½´ä»¤ç‰Œæ£€ç´¢æ¨¡å—ã€‚å‰è€…é€šè¿‡æŸ¥è¯¢æ‰©å±•å’Œè¿­ä»£çš„æ—¶ç©ºè¯­ä¹‰ç½®ä¿¡åº¦è¯„åˆ†æ¥è¯†åˆ«ç›¸å…³è§†é¢‘å¸§ï¼Œåè€…åˆ™åœ¨æœ€å¤š1024ä¸ªæ¢è½´å¸§å†…è¿›è¡ŒåŸºäºæŸ¥è¯¢çš„æ³¨æ„åŠ›é©±åŠ¨çš„ä»¤ç‰Œé€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šAPVRçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åŒç²’åº¦æ£€ç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é•¿è§†é¢‘ä¸­æå–é‡è¦ä¿¡æ¯ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å†…å­˜å’Œä¿¡æ¯å®Œæ•´æ€§ä¸Šçš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒAPVRé‡‡ç”¨äº†æŸ¥è¯¢æ‰©å±•å’Œæ—¶ç©ºè¯­ä¹‰ç½®ä¿¡åº¦è¯„åˆ†ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œä»¥ç¡®ä¿æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„èµ„æºæ¶ˆè€—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

APVRåœ¨LongVideoBenchã€VideoMMEå’ŒMLVUåŸºå‡†ä¸Šåˆ†åˆ«å®ç°äº†9.5%ã€4.6%å’Œ9.7%çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è®­ç»ƒå’Œæ— è®­ç»ƒæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„æœ‰æ•ˆæ€§å’Œå…ˆè¿›æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

APVRçš„ç ”ç©¶æˆæœåœ¨è§†é¢‘åˆ†æã€å†…å®¹æ£€ç´¢å’Œæ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜é•¿è§†é¢‘ç†è§£çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä¸ºå¤šåª’ä½“ä¿¡æ¯å¤„ç†å’Œæ™ºèƒ½å†³ç­–æä¾›æ›´å¼ºå¤§çš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current multimodal large language models (MLLMs) struggle with hour-level video understanding, facing significant challenges not only in modeling the substantial information volume of long videos but also in overcoming the memory wall and resource constraints during both training and inference. Although recent training-free approaches have alleviated resource demands by compressing visual features, their reliance on incomplete visual information limits the performance potential. To address these limitations, we propose Adaptive Pivot Visual information Retrieval (APVR), a training-free framework that hierarchically retrieves and retains sufficient and important visual information. It breakthroughs the memory wall limitation via two complementary components: Pivot Frame Retrieval employs query expansion and iterative spatio-semantic confidence scoring to identify relevant video frames, and Pivot Token Retrieval performs query-aware attention-driven token selection within up to 1024 pivot frames. This dual granularity approach enables the processing of hour-long videos while maintaining semantic fidelity. Experimental validations on three different baseline MLLMs demonstrate significant performance improvements up to 9.5\%, 4.6\% and 9.7\% on LongVideoBench, VideoMME and MLVU, respectively. APVR achieves state-of-the-art results for both training-free and training-based approaches.

