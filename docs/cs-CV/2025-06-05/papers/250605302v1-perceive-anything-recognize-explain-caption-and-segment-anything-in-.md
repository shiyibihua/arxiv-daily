---
layout: default
title: Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos
---

# Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05302" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05302v1</a>
  <a href="https://arxiv.org/pdf/2506.05302.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05302v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05302v1', 'Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, Hongsheng Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: 19 pages, 13 figures, Website: https://Perceive-Anything.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPerceive Anythingæ¨¡å‹ä»¥è§£å†³å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒºåŸŸç†è§£` `å¤šæ¨¡æ€è¾“å‡º` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰ç‰¹å¾è½¬åŒ–` `æ•°æ®å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŒºåŸŸçº§è§†è§‰ç†è§£ä¸­å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œå¤šæ¨¡æ€è¾“å‡ºä¸è¶³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºçš„PAMæ¨¡å‹é€šè¿‡æ•´åˆLLMsä¸SAM 2ï¼Œå®ç°äº†é«˜æ•ˆçš„å¯¹è±¡åˆ†å‰²å’Œå¤šæ ·åŒ–è¯­ä¹‰è¾“å‡ºã€‚
3. PAMåœ¨å¤šç§åŒºåŸŸç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿è¡Œé€Ÿåº¦æå‡1.2-2.4å€ï¼Œå†…å­˜æ¶ˆè€—æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Perceive Anything Modelï¼ˆPAMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¦‚å¿µä¸Šç®€å•ä¸”é«˜æ•ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å›¾åƒå’Œè§†é¢‘çš„å…¨é¢åŒºåŸŸçº§è§†è§‰ç†è§£ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ‰©å±•äº†å¼ºå¤§çš„åˆ†å‰²æ¨¡å‹SAM 2ï¼Œå®ç°äº†å¯¹è±¡åˆ†å‰²ä¸å¤šæ ·åŒ–åŒºåŸŸç‰¹å®šè¯­ä¹‰è¾“å‡ºçš„ç”Ÿæˆï¼ŒåŒ…æ‹¬ç±»åˆ«ã€æ ‡ç­¾å®šä¹‰ã€åŠŸèƒ½è§£é‡Šå’Œè¯¦ç»†æè¿°ã€‚å¼•å…¥çš„è¯­ä¹‰æ„ŸçŸ¥å™¨æœ‰æ•ˆåœ°å°†SAM 2çš„ä¸°å¯Œè§†è§‰ç‰¹å¾è½¬åŒ–ä¸ºLLMå¯ç†è§£çš„å¤šæ¨¡æ€æ ‡è®°ã€‚ä¸ºäº†æ”¯æŒç¨³å¥çš„å¤šç²’åº¦ç†è§£ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸“é—¨çš„æ•°æ®ç²¾ç‚¼å’Œå¢å¼ºç®¡é“ï¼Œç”Ÿæˆäº†150ä¸‡å¼ å›¾åƒå’Œ60ä¸‡æ®µè§†é¢‘çš„åŒºåŸŸè¯­ä¹‰æ³¨é‡Šæ•°æ®é›†ã€‚PAMè®¾è®¡è½»é‡é«˜æ•ˆï¼Œåœ¨å¤šç§åŒºåŸŸç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿è¡Œé€Ÿåº¦æ¯”ä»¥å¾€æ–¹æ³•å¿«1.2-2.4å€ï¼ŒGPUå†…å­˜æ¶ˆè€—æ›´å°‘ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å›¾åƒå’Œè§†é¢‘ä¸­çš„åŒºåŸŸçº§è§†è§‰ç†è§£é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å‡ºå’Œæ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPAMæ¨¡å‹é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç°æœ‰çš„åˆ†å‰²æ¨¡å‹SAM 2ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œå¯¹è±¡åˆ†å‰²å’Œç”Ÿæˆä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæå‡ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPAMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¯­ä¹‰æ„ŸçŸ¥å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†SAM 2çš„è§†è§‰ç‰¹å¾è½¬åŒ–ä¸ºå¤šæ¨¡æ€æ ‡è®°ï¼Œæ”¯æŒä¸LLMsçš„æœ‰æ•ˆäº¤äº’ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†æ•°æ®ç²¾ç‚¼å’Œå¢å¼ºç®¡é“ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŒºåŸŸè¯­ä¹‰æ³¨é‡Šæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šPAMçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥å™¨ï¼Œä½¿å¾—è§†è§‰ç‰¹å¾èƒ½å¤Ÿé«˜æ•ˆè½¬åŒ–ä¸ºLLMså¯ç†è§£çš„æ ¼å¼ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç†è§£çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¼˜åŒ–çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡æ—¶çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PAMæ¨¡å‹åœ¨å¤šç§åŒºåŸŸç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿è¡Œé€Ÿåº¦æ¯”ä»¥å¾€æ–¹æ³•å¿«1.2-2.4å€ï¼Œä¸”GPUå†…å­˜æ¶ˆè€—æ˜¾è‘—é™ä½ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒPAMåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„èµ„æºéœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œäººæœºäº¤äº’ç­‰ã€‚PAMæ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œèƒ½å¤Ÿæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Perceive Anything Model (PAM), a conceptually straightforward and efficient framework for comprehensive region-level visual understanding in images and videos. Our approach extends the powerful segmentation model SAM 2 by integrating Large Language Models (LLMs), enabling simultaneous object segmentation with the generation of diverse, region-specific semantic outputs, including categories, label definition, functional explanations, and detailed captions. A key component, Semantic Perceiver, is introduced to efficiently transform SAM 2's rich visual features, which inherently carry general vision, localization, and semantic priors into multi-modal tokens for LLM comprehension. To support robust multi-granularity understanding, we also develop a dedicated data refinement and augmentation pipeline, yielding a high-quality dataset of 1.5M image and 0.6M video region-semantic annotations, including novel region-level streaming video caption data. PAM is designed for lightweightness and efficiency, while also demonstrates strong performance across a diverse range of region understanding tasks. It runs 1.2-2.4x faster and consumes less GPU memory than prior approaches, offering a practical solution for real-world applications. We believe that our effective approach will serve as a strong baseline for future research in region-level visual understanding.

