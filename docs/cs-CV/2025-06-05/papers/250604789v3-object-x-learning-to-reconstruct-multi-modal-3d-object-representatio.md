---
layout: default
title: Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations
---

# Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04789" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04789v3</a>
  <a href="https://arxiv.org/pdf/2506.04789.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04789v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04789v3', 'Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gaia Di Lorenzo, Federico Tombari, Marc Pollefeys, Daniel Barath

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-11-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºObject-Xä»¥è§£å†³å¤šæ¨¡æ€3Dç‰©ä½“è¡¨ç¤ºé‡å»ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `3Dé‡å»º` `ç‰©ä½“è¡¨ç¤º` `é«˜æ–¯ç‚¹äº‘` `åœºæ™¯å¯¹é½` `æœºå™¨äººæŠ€æœ¯` `å¢å¼ºç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„åµŒå…¥ï¼Œæ— æ³•åŒæ—¶ç”¨äºå‡ ä½•é‡å»ºå’Œè¯­ä¹‰ç†è§£ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚
2. Object-Xæ¡†æ¶é€šè¿‡å‡ ä½•æ€§åœ°å°†å¤šæ¨¡æ€ä¿¡æ¯å›ºå®šåœ¨3Dä½“ç´ ç½‘æ ¼ä¸­ï¼Œå­¦ä¹ èåˆä¿¡æ¯çš„éç»“æ„åŒ–åµŒå…¥ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚
3. åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®æ•°æ®é›†ä¸Šï¼ŒObject-Xåœ¨æ–°è§†è§’åˆæˆå’Œå‡ ä½•ç²¾åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”å­˜å‚¨éœ€æ±‚æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„å¤šæ¨¡æ€3Dç‰©ä½“è¡¨ç¤ºå­¦ä¹ å¯¹äºå¢å¼ºç°å®å’Œæœºå™¨äººç­‰å¤šä¸ªåº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„åµŒå…¥ï¼Œæ— æ³•åŒæ—¶ç”¨äºå‡ ä½•é‡å»ºå’Œè¯­ä¹‰ç†è§£ã€‚æœ¬æ–‡æå‡ºäº†Object-Xï¼Œä¸€ä¸ªå¤šåŠŸèƒ½çš„ç‰©ä½“è¡¨ç¤ºæ¡†æ¶ï¼Œèƒ½å¤Ÿç¼–ç ä¸°å¯Œçš„ç‰©ä½“åµŒå…¥ï¼ˆå¦‚å›¾åƒã€ç‚¹äº‘ã€æ–‡æœ¬ï¼‰ï¼Œå¹¶å°†å…¶è§£ç ä¸ºè¯¦ç»†çš„å‡ ä½•å’Œè§†è§‰é‡å»ºã€‚Object-Xé€šè¿‡åœ¨3Dä½“ç´ ç½‘æ ¼ä¸­å‡ ä½•æ€§åœ°å›ºå®šæ•è·çš„æ¨¡æ€ï¼Œå­¦ä¹ ä¸€ä¸ªèåˆä½“ç´ ä¿¡æ¯ä¸ç‰©ä½“å±æ€§çš„éç»“æ„åŒ–åµŒå…¥ã€‚è¯¥åµŒå…¥æ”¯æŒåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„ç‰©ä½“é‡å»ºï¼Œå¹¶é€‚ç”¨äºåœºæ™¯å¯¹é½ã€å•å›¾åƒ3Dç‰©ä½“é‡å»ºå’Œå®šä½ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼ŒObject-Xåœ¨æ–°è§†è§’åˆæˆæ–¹é¢ä¸æ ‡å‡†3Dé«˜æ–¯ç‚¹äº‘ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†å‡ ä½•ç²¾åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€3Dç‰©ä½“è¡¨ç¤ºæ–¹æ³•çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å®ƒä»¬åœ¨å‡ ä½•é‡å»ºå’Œè¯­ä¹‰ç†è§£ä¸Šçš„ä¸å…¼å®¹æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œå¯¼è‡´åµŒå…¥æ— æ³•é‡ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šObject-Xé€šè¿‡åœ¨3Dä½“ç´ ç½‘æ ¼ä¸­å‡ ä½•æ€§åœ°å›ºå®šå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå­¦ä¹ ä¸€ä¸ªèåˆä½“ç´ ä¸ç‰©ä½“å±æ€§çš„éç»“æ„åŒ–åµŒå…¥ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç‰©ä½“é‡å»ºä¸å¤šä»»åŠ¡æ”¯æŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šObject-Xçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ•è·ã€ä½“ç´ ç½‘æ ¼æ„å»ºã€åµŒå…¥å­¦ä¹ å’Œé‡å»ºæ¨¡å—ã€‚é¦–å…ˆæ•è·å¤šæ¨¡æ€æ•°æ®ï¼Œç„¶ååœ¨3Dä½“ç´ ç½‘æ ¼ä¸­è¿›è¡Œå‡ ä½•å›ºå®šï¼Œæœ€åé€šè¿‡å­¦ä¹ çš„åµŒå…¥è¿›è¡Œç‰©ä½“é‡å»ºå’Œä¸‹æ¸¸ä»»åŠ¡å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šObject-Xçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶éç»“æ„åŒ–åµŒå…¥çš„è®¾è®¡ï¼Œä½¿å¾—ä¸åŒæ¨¡æ€çš„ä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆèåˆï¼Œå¹¶ä¸”èƒ½å¤Ÿæ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜äº†å‡ ä½•é‡å»ºçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒObject-Xé‡‡ç”¨äº†3Dé«˜æ–¯ç‚¹äº‘é‡å»ºæŠ€æœ¯ï¼Œå¹¶è®¾è®¡äº†é€‚åº”å¤šæ¨¡æ€ä¿¡æ¯çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–åµŒå…¥å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒObject-Xåœ¨æ–°è§†è§’åˆæˆæ–¹é¢çš„è¡¨ç°ä¸æ ‡å‡†3Dé«˜æ–¯ç‚¹äº‘ç›¸å½“ï¼ŒåŒæ—¶åœ¨å‡ ä½•ç²¾åº¦ä¸Šæ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå…¶ç‰©ä½“ä¸­å¿ƒæè¿°ç¬¦çš„å­˜å‚¨éœ€æ±‚æ¯”ä¼ ç»Ÿå›¾åƒæˆ–ç‚¹äº‘æ–¹æ³•ä½3-4ä¸ªæ•°é‡çº§ï¼Œå±•ç¤ºäº†å…¶åœ¨å­˜å‚¨æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Object-Xåœ¨å¢å¼ºç°å®ã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„å¤šæ¨¡æ€3Dè¡¨ç¤ºèƒ½åŠ›èƒ½å¤Ÿæå‡ç‰©ä½“è¯†åˆ«ã€åœºæ™¯ç†è§£å’Œäº¤äº’ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½åœ¨æ™ºèƒ½å®¶å±…ã€è™šæ‹Ÿç°å®ç­‰æ–°å…´é¢†åŸŸå‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.

