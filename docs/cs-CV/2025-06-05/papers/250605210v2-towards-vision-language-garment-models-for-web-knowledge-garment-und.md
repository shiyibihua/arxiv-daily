---
layout: default
title: Towards Vision-Language-Garment Models for Web Knowledge Garment Understanding and Generation
---

# Towards Vision-Language-Garment Models for Web Knowledge Garment Understanding and Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05210" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05210v2</a>
  <a href="https://arxiv.org/pdf/2506.05210.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05210v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05210v2', 'Towards Vision-Language-Garment Models for Web Knowledge Garment Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jan Ackermann, Kiyohiro Nakayama, Guandao Yang, Tong Wu, Gordon Wetzstein

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-30)

**å¤‡æ³¨**: Presented at MMFM CVPRW'25, Project Page: https://www.computationalimaging.org/publications/vision-language-garment-models/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLGæ¨¡å‹ä»¥è§£å†³æœè£…ç”Ÿæˆé¢†åŸŸçš„çŸ¥è¯†è½¬ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹` `æœè£…ç”Ÿæˆ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è½¬ç§»` `æ—¶å°šè®¾è®¡` `é›¶-shotå­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå¦‚æœè£…ç”Ÿæˆä¸­çš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå­˜åœ¨åº”ç”¨å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºVLGæ¨¡å‹ï¼Œé€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æè¿°å’Œå›¾åƒä¸­åˆæˆæœè£…ï¼Œæå‡ç”Ÿæˆæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºVLGåœ¨é›¶-shotæ¡ä»¶ä¸‹å¯¹æœªè§æœè£…é£æ ¼çš„é€‚åº”èƒ½åŠ›è‰¯å¥½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ—¶å°šè®¾è®¡é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨å¹¿æ³›åº”ç”¨ä¸­å±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å…¶åœ¨æœè£…ç”Ÿæˆç­‰ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†VLGï¼Œä¸€ä¸ªè§†è§‰-è¯­è¨€-æœè£…æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°å’Œè§†è§‰å›¾åƒåˆæˆæœè£…ã€‚å®éªŒè¯„ä¼°äº†VLGçš„é›¶-shot æ³›åŒ–èƒ½åŠ›ï¼Œè€ƒå¯Ÿå…¶å°†ç½‘ç»œè§„æ¨¡æ¨ç†è½¬ç§»åˆ°æœªè§æœè£…é£æ ¼å’Œæç¤ºçš„èƒ½åŠ›ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çŸ¥è¯†è½¬ç§»æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ï¼Œçªæ˜¾äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨æ—¶å°šè®¾è®¡ç­‰ä¸“ä¸šé¢†åŸŸçš„é€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨æœè£…ç”Ÿæˆé¢†åŸŸçš„çŸ¥è¯†è½¬ç§»ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æ•ˆæœæœ‰é™ï¼Œç¼ºä¹å¯¹æ–°é£æ ¼å’Œæç¤ºçš„æœ‰æ•ˆé€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLGæ¨¡å‹é€šè¿‡æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œåˆ©ç”¨æ–‡æœ¬æè¿°å’Œå›¾åƒæ•°æ®ç”Ÿæˆæœè£…ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§çš„æœè£…é£æ ¼ä¸­è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLGæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æœ¬ç¼–ç æ¨¡å—ã€å›¾åƒç¼–ç æ¨¡å—å’Œç”Ÿæˆæ¨¡å—ã€‚æ–‡æœ¬ç¼–ç æ¨¡å—å°†æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºå‘é‡è¡¨ç¤ºï¼Œå›¾åƒç¼–ç æ¨¡å—æå–è§†è§‰ç‰¹å¾ï¼Œç”Ÿæˆæ¨¡å—åˆ™ç»“åˆè¿™ä¸¤éƒ¨åˆ†ä¿¡æ¯ç”Ÿæˆæœè£…å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šVLGæ¨¡å‹çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€èåˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨é›¶-shotæ¡ä»¶ä¸‹æœ‰æ•ˆè½¬ç§»çŸ¥è¯†åˆ°æ–°çš„æœè£…é£æ ¼ã€‚è¿™ä¸€ç‰¹æ€§ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„è´¡çŒ®ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå¯¹é‡è¦ç‰¹å¾çš„å…³æ³¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVLGæ¨¡å‹åœ¨é›¶-shotæ¡ä»¶ä¸‹å¯¹æœªè§æœè£…é£æ ¼çš„ç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒVLGåœ¨ç”Ÿæˆè´¨é‡å’Œé€‚åº”æ€§ä¸Šå‡æœ‰æ˜æ˜¾æ”¹å–„ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªæŠ«éœ²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ—¶å°šè®¾è®¡ã€åœ¨çº¿æœè£…é›¶å”®å’Œä¸ªæ€§åŒ–æœè£…æ¨èç­‰ã€‚é€šè¿‡VLGæ¨¡å‹ï¼Œè®¾è®¡å¸ˆå’Œæ¶ˆè´¹è€…èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç”Ÿæˆå’Œé€‰æ‹©ç¬¦åˆéœ€æ±‚çš„æœè£…ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå¸‚åœºå“åº”é€Ÿåº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æœè£…è¡Œä¸šçš„æ•°å­—åŒ–è½¬å‹ï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–å’Œå®šåˆ¶åŒ–æœåŠ¡çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal foundation models have demonstrated strong generalization, yet their ability to transfer knowledge to specialized domains such as garment generation remains underexplored. We introduce VLG, a vision-language-garment model that synthesizes garments from textual descriptions and visual imagery. Our experiments assess VLG's zero-shot generalization, investigating its ability to transfer web-scale reasoning to unseen garment styles and prompts. Preliminary results indicate promising transfer capabilities, highlighting the potential for multimodal foundation models to adapt effectively to specialized domains like fashion design.

