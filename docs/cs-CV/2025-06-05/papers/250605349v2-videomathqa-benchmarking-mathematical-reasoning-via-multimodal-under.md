---
layout: default
title: VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos
---

# VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05349" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05349v2</a>
  <a href="https://arxiv.org/pdf/2506.05349.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05349v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05349v2', 'VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, Fahad Shahbaz Khan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-24)

**å¤‡æ³¨**: VideoMathQA Technical Report

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://mbzuai-oryx.github.io/VideoMathQA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoMathQAä»¥è§£å†³è§†é¢‘ä¸­çš„æ•°å­¦æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ•°å­¦æ¨ç†` `è·¨æ¨¡æ€å­¦ä¹ ` `å¤šæ­¥æ¨ç†` `åŸºå‡†æµ‹è¯•` `æ•™è‚²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†é¢‘ä¸­çš„æ•°å­¦æ¨ç†æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºVideoMathQAåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ä¸­è¿›è¡Œè·¨æ¨¡æ€æ¨ç†çš„èƒ½åŠ›ï¼Œæ¶µç›–å¤šç§æ•°å­¦é¢†åŸŸã€‚
3. é€šè¿‡ä¸ç ”ç©¶ç”Ÿä¸“å®¶çš„åˆä½œï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„æ ‡æ³¨ï¼Œå¹¶è®¾è®¡äº†å¤šæ­¥æ¨ç†é—®é¢˜ä»¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨çœŸå®è§†é¢‘ç¯å¢ƒä¸­è¿›è¡Œæ•°å­¦æ¨ç†é¢ä¸´ç€ä¸é™æ€å›¾åƒæˆ–æ–‡æœ¬æˆªç„¶ä¸åŒçš„æŒ‘æˆ˜ã€‚å®ƒéœ€è¦è§£é‡Šç»†è‡´çš„è§†è§‰ä¿¡æ¯ï¼Œå‡†ç¡®è¯»å–æ‰‹å†™æˆ–æ•°å­—æ–‡æœ¬ï¼Œå¹¶æ•´åˆåˆ†æ•£åœ¨æ—¶é—´ä¸Šçš„å£å¤´æç¤ºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†VideoMathQAï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ä¸­è¿›è¡Œè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›–10ä¸ªå¤šæ ·çš„æ•°å­¦é¢†åŸŸï¼Œè§†é¢‘æ—¶é•¿ä»10ç§’åˆ°è¶…è¿‡1å°æ—¶ä¸ç­‰ï¼Œè¦æ±‚æ¨¡å‹ç†è§£ç»“æ„åŒ–çš„è§†è§‰å†…å®¹ã€æŒ‡ä»¤å™è¿°ï¼Œå¹¶åœ¨è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡æ€é—´å…±åŒå®šä½æ¦‚å¿µã€‚é€šè¿‡ç ”ç©¶ç”Ÿä¸“å®¶çš„å‚ä¸ï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„æ ‡æ³¨ï¼Œæ€»è®¡è¶…è¿‡920å°æ—¶çš„äººå·¥æ ‡æ³¨ã€‚é—®é¢˜å›´ç»•ç›´æ¥é—®é¢˜è§£å†³ã€æ¦‚å¿µè½¬ç§»å’Œæ·±åº¦æŒ‡ä»¤ç†è§£ä¸‰å¤§æ ¸å¿ƒæ¨ç†æŒ‘æˆ˜è®¾è®¡ï¼Œæ”¯æŒå¤šæ­¥æ¨ç†çš„æ ‡æ³¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½åŠ›çš„ç»†è‡´è¯Šæ–­æˆä¸ºå¯èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è§†é¢‘ä¸­è¿›è¡Œæ•°å­¦æ¨ç†æ—¶ï¼Œç°æœ‰æ¨¡å‹åœ¨æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯å’Œå¤„ç†å¤æ‚é—®é¢˜æ—¶çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹è§†é¢‘ä¸­çš„åŠ¨æ€ä¿¡æ¯å’Œå¤šæ ·åŒ–çš„è¡¨è¾¾å½¢å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡VideoMathQAåŸºå‡†ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ä¸­è¿›è¡Œè·¨æ¨¡æ€æ¨ç†çš„èƒ½åŠ›ï¼Œå¼ºè°ƒæ¨ç†è¿‡ç¨‹è€Œéå•çº¯çš„æ„ŸçŸ¥ã€‚è®¾è®¡é—®é¢˜æ—¶è€ƒè™‘äº†å¤šæ­¥æ¨ç†å’Œæ¦‚å¿µè½¬ç§»ï¼Œä»¥åæ˜ çœŸå®åœºæ™¯ä¸­çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†é¢‘å†…å®¹çš„è§£æã€é—®é¢˜ç†è§£ã€æ¨¡æ€èåˆå’Œæ¨ç†è¿‡ç¨‹å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ¨¡å‹éœ€è¦ä»è§†é¢‘ä¸­æå–è§†è§‰ä¿¡æ¯ï¼Œç»“åˆæ–‡æœ¬å’ŒéŸ³é¢‘ä¿¡æ¯è¿›è¡Œç»¼åˆæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šæ­¥æ¨ç†çš„æ ‡æ³¨æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶èƒ½å¤Ÿè¿›è¡Œç»†è‡´çš„èƒ½åŠ›è¯Šæ–­ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ„ŸçŸ¥èƒ½åŠ›å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä»¥æå‡æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨VideoMathQAåŸºå‡†çš„æ¨¡å‹åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡äº†çº¦20%ã€‚é€šè¿‡ä¸ç°æœ‰åŸºçº¿çš„å¯¹æ¯”ï¼Œå±•ç¤ºäº†è¯¥åŸºå‡†åœ¨è¯„ä¼°æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œè§†é¢‘å†…å®¹åˆ†æç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨è§†é¢‘ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿæä¾›æ›´ä¸ºç²¾å‡†çš„å­¦ä¹ æ”¯æŒï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–æ•™è‚²çš„å‘å±•ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†ä¹Ÿå¯ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over $920$ man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA

