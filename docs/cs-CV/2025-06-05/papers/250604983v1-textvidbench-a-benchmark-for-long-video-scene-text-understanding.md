---
layout: default
title: TextVidBench: A Benchmark for Long Video Scene Text Understanding
---

# TextVidBench: A Benchmark for Long Video Scene Text Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04983" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04983v1</a>
  <a href="https://arxiv.org/pdf/2506.04983.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04983v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04983v1', 'TextVidBench: A Benchmark for Long Video Scene Text Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yangyang Zhong, Ji Qi, Yuan Yao, Pengxin Luo, Yunfeng Yan, Donglian Qi, Zhiyuan Liu, Tat-Seng Chua

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTextVidBenchä»¥è§£å†³é•¿è§†é¢‘åœºæ™¯æ–‡æœ¬ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `æ–‡æœ¬é—®ç­”` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ—¶é—´æ„ŸçŸ¥` `ç»†ç²’åº¦æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ­è§†é¢‘æ–‡æœ¬é—®ç­”ä»»åŠ¡çš„æ•°æ®é›†åœ¨è§†é¢‘æ—¶é•¿å’Œè¯„ä¼°èŒƒå›´ä¸Šå­˜åœ¨å±€é™ï¼Œéš¾ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚
2. æå‡ºTextVidBenchåŸºå‡†ï¼Œä¸“æ³¨äºé•¿è§†é¢‘æ–‡æœ¬é—®ç­”ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå¹¶å¼•å…¥ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ã€‚
3. åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†åŠTextVidBenchä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºæ‰€ææ–¹æ³•æ˜¾è‘—æå‡äº†é•¿è§†é¢‘åœºæ™¯æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡çŸ­è§†é¢‘æ–‡æœ¬-è§†è§‰é—®ç­”ä»»åŠ¡ï¼ˆViteVQAï¼‰å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ•°æ®é›†åœ¨è§†é¢‘æ—¶é•¿å’Œè¯„ä¼°èŒƒå›´ä¸Šä»å­˜åœ¨å±€é™ï¼Œéš¾ä»¥å……åˆ†è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TextVidBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹é•¿è§†é¢‘æ–‡æœ¬é—®ç­”ï¼ˆ>3åˆ†é’Ÿï¼‰è®¾è®¡çš„åŸºå‡†ã€‚TextVidBenchçš„ä¸‰å¤§è´¡çŒ®åŒ…æ‹¬ï¼šè·¨é¢†åŸŸé•¿è§†é¢‘è¦†ç›–ï¼Œæ¶µç›–9ä¸ªç±»åˆ«ï¼Œå¹³å‡è§†é¢‘é•¿åº¦ä¸º2306ç§’ï¼›ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼›é«˜è´¨é‡ç»†ç²’åº¦æ ‡æ³¨ï¼ŒåŒ…å«5000å¤šä¸ªé—®ç­”å¯¹åŠè¯¦ç»†è¯­ä¹‰æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ”¹è¿›å¤§æ¨¡å‹çš„èŒƒå¼ï¼ŒåŒ…æ‹¬IT-Ropeæœºåˆ¶å’Œæ—¶é—´æç¤ºå·¥ç¨‹ã€éå‡åŒ€ä½ç½®ç¼–ç åŠè½»é‡çº§å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTextVidBenchå¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œæ‰€ææ–¹æ³•ä¸ºæå‡é•¿è§†é¢‘åœºæ™¯æ–‡æœ¬ç†è§£èƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çŸ­è§†é¢‘æ–‡æœ¬é—®ç­”ä»»åŠ¡åœ¨è§†é¢‘æ—¶é•¿å’Œè¯„ä¼°èŒƒå›´ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•æœ‰æ•ˆç†è§£é•¿è§†é¢‘ä¸­çš„æ–‡æœ¬ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºTextVidBenchåŸºå‡†ï¼Œä¸“é—¨é’ˆå¯¹é•¿è§†é¢‘æ–‡æœ¬é—®ç­”è®¾è®¡ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå¹¶å¼•å…¥ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥IT-Ropeæœºåˆ¶å’Œæ—¶é—´æç¤ºå·¥ç¨‹ï¼Œå¢å¼ºæ¨¡å‹çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ–‡æœ¬é’ˆåœ¨å¹²è‰å †ä¸­ï¼ˆText Needle-in-Haystackï¼‰ã€æ—¶é—´å®šä½ï¼ˆTemporal Groundingï¼‰å’Œæ–‡æœ¬åŠ¨æ€å­—å¹•ï¼ˆText Dynamics Captioningï¼‰ã€‚æ¯ä¸ªé˜¶æ®µé’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„ä¸åŒæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†IT-Ropeæœºåˆ¶å’Œéå‡åŒ€ä½ç½®ç¼–ç ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹é•¿è§†é¢‘åºåˆ—çš„å¤„ç†èƒ½åŠ›ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºæ›´å¥½åœ°é€‚åº”é•¿è§†é¢‘çš„æ—¶åºç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†éå‡åŒ€ä½ç½®ç¼–ç ä»¥å¤„ç†é•¿è§†é¢‘åºåˆ—ï¼Œå¹¶è¿›è¡Œäº†è½»é‡çº§å¾®è°ƒä»¥é€‚åº”è§†é¢‘-æ–‡æœ¬æ•°æ®çš„ç‰¹æ€§ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTextVidBenchå¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢ã€‚é€šè¿‡æ‰€ææ–¹æ³•ï¼Œæ¨¡å‹åœ¨é•¿è§†é¢‘æ–‡æœ¬é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–°é—»æŠ¥é“ã€ä½“è‚²è§£è¯´ã€æ¸¸æˆè§£è¯´ç­‰é•¿è§†é¢‘å†…å®¹çš„è‡ªåŠ¨ç†è§£ä¸åˆ†æã€‚é€šè¿‡æå‡é•¿è§†é¢‘åœºæ™¯æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºä¿¡æ¯æ£€ç´¢ã€å†…å®¹æ¨èå’Œæ™ºèƒ½é—®ç­”ç­‰åº”ç”¨æä¾›æ›´ä¸ºç²¾å‡†çš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite recent progress on the short-video Text-Visual Question Answering (ViteVQA) task - largely driven by benchmarks such as M4-ViteVQA - existing datasets still suffer from limited video duration and narrow evaluation scopes, making it difficult to adequately assess the growing capabilities of powerful multimodal large language models (MLLMs). To address these limitations, we introduce TextVidBench, the first benchmark specifically designed for long-video text question answering (>3 minutes). TextVidBench makes three key contributions: 1) Cross-domain long-video coverage: Spanning 9 categories (e.g., news, sports, gaming), with an average video length of 2306 seconds, enabling more realistic evaluation of long-video understanding. 2) A three-stage evaluation framework: "Text Needle-in-Haystack -> Temporal Grounding -> Text Dynamics Captioning". 3) High-quality fine-grained annotations: Containing over 5,000 question-answer pairs with detailed semantic labeling. Furthermore, we propose an efficient paradigm for improving large models through: (i) introducing the IT-Rope mechanism and temporal prompt engineering to enhance temporal perception, (ii) adopting non-uniform positional encoding to better handle long video sequences, and (iii) applying lightweight fine-tuning on video-text data. Extensive experiments on multiple public datasets as well as TextVidBench demonstrate that our new benchmark presents significant challenges to existing models, while our proposed method offers valuable insights into improving long-video scene text understanding capabilities.

