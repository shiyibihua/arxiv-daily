---
layout: default
title: MokA: Multimodal Low-Rank Adaptation for MLLMs
---

# MokA: Multimodal Low-Rank Adaptation for MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05191" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05191v2</a>
  <a href="https://arxiv.org/pdf/2506.05191.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05191v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05191v2', 'MokA: Multimodal Low-Rank Adaptation for MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yake Wei, Yu Miao, Dongzhan Zhou, Di Hu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-12-11)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://gewu-lab.github.io/MokA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMokAä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€é€‚åº”` `ä½ç§©é€‚åº”` `å¤§è¯­è¨€æ¨¡å‹` `è·¨æ¨¡æ€äº¤äº’` `é«˜æ•ˆå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¾®è°ƒæ–¹æ³•å¾€å¾€ç›´æ¥å€Ÿé‰´è‡ªå¤§è¯­è¨€æ¨¡å‹ï¼Œå¿½è§†äº†å¤šæ¨¡æ€åœºæ™¯çš„å†…åœ¨å·®å¼‚ï¼Œå¯¼è‡´é€‚åº”æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„MokAæ–¹æ³•é€šè¿‡æ¨¡æ€ç‰¹å®šå‚æ•°è¿›è¡Œå•æ¨¡æ€é€‚åº”ï¼ŒåŒæ—¶å¢å¼ºè·¨æ¨¡æ€äº¤äº’ï¼Œé’ˆå¯¹æ€§åœ°è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMokAåœ¨å¤šç§å¤šæ¨¡æ€åœºæ™¯ä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ­ç¤ºäº†å½“å‰é«˜æ•ˆå¤šæ¨¡æ€å¾®è°ƒæ–¹æ³•çš„ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šè¿™äº›æ–¹æ³•ç›´æ¥å€Ÿé‰´è‡ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¾€å¾€å¿½è§†äº†å¤šæ¨¡æ€åœºæ™¯çš„å†…åœ¨å·®å¼‚ï¼Œå½±å“äº†æ‰€æœ‰æ¨¡æ€çš„å……åˆ†åˆ©ç”¨ã€‚åŸºäºæˆ‘ä»¬çš„å®è¯è§‚å¯Ÿï¼Œæˆ‘ä»¬è®¤ä¸ºå•æ¨¡æ€é€‚åº”å’Œè·¨æ¨¡æ€é€‚åº”æ˜¯æœ‰æ•ˆå¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸¤ä¸ªé‡è¦éƒ¨åˆ†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€ä½ç§©é€‚åº”ï¼ˆMokAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è€ƒè™‘å¤šæ¨¡æ€ç‰¹å¾çš„é«˜æ•ˆå¾®è°ƒç­–ç•¥ã€‚MokAé€šè¿‡æ¨¡æ€ç‰¹å®šå‚æ•°å‹ç¼©å•æ¨¡æ€ä¿¡æ¯ï¼ŒåŒæ—¶æ˜¾è‘—å¢å¼ºè·¨æ¨¡æ€äº¤äº’ï¼Œç¡®ä¿å•æ¨¡æ€å’Œè·¨æ¨¡æ€çš„é€‚åº”æ€§ã€‚å¤§é‡å®éªŒè¦†ç›–äº†ä¸‰ç§ä»£è¡¨æ€§å¤šæ¨¡æ€åœºæ™¯ï¼ˆéŸ³é¢‘-è§†è§‰-æ–‡æœ¬ã€è§†è§‰-æ–‡æœ¬å’Œè¯­éŸ³-æ–‡æœ¬ï¼‰ï¼Œä»¥åŠå¤šä¸ªLLMéª¨å¹²ï¼ˆå¦‚LLaMA2/3ã€Qwen2ã€Qwen2.5-VLç­‰ï¼‰ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘å¤šæ¨¡æ€ç‰¹æ€§ï¼Œå¯¼è‡´é€‚åº”æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMokAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å•æ¨¡æ€é€‚åº”ä¸è·¨æ¨¡æ€é€‚åº”ç›¸ç»“åˆï¼Œé€šè¿‡æ¨¡æ€ç‰¹å®šå‚æ•°å‹ç¼©å•æ¨¡æ€ä¿¡æ¯ï¼ŒåŒæ—¶å¢å¼ºæ¨¡æ€é—´çš„äº¤äº’ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å¾®è°ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMokAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå•æ¨¡æ€é€‚åº”æ¨¡å—å’Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—ã€‚å•æ¨¡æ€é€‚åº”æ¨¡å—è´Ÿè´£å¤„ç†æ¯ç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œè€Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—åˆ™ç¡®ä¿ä¸åŒæ¨¡æ€ä¹‹é—´çš„æœ‰æ•ˆä¿¡æ¯ä¼ é€’ã€‚

**å…³é”®åˆ›æ–°**ï¼šMokAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€æ„è¯†çš„ä½ç§©é€‚åº”ç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä»…å…³æ³¨å•ä¸€æ¨¡æ€çš„é€‚åº”ï¼Œç¡®ä¿äº†å¤šæ¨¡æ€ç‰¹å¾çš„å……åˆ†åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒMokAé‡‡ç”¨äº†æ¨¡æ€ç‰¹å®šçš„å‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–å•æ¨¡æ€ä¿¡æ¯çš„å‹ç¼©ï¼ŒåŒæ—¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å•æ¨¡æ€ä¸è·¨æ¨¡æ€çš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMokAåœ¨éŸ³é¢‘-è§†è§‰-æ–‡æœ¬ã€è§†è§‰-æ–‡æœ¬å’Œè¯­éŸ³-æ–‡æœ¬ç­‰å¤šç§åœºæ™¯ä¸­å‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†X%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MokAçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨å¤šæ¨¡æ€ç†è§£ã€æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§ï¼ŒMokAèƒ½å¤Ÿæ¨åŠ¨è¿™äº›é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA.

