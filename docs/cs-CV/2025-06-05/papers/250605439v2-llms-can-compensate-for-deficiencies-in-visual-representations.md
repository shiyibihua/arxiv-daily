---
layout: default
title: LLMs Can Compensate for Deficiencies in Visual Representations
---

# LLMs Can Compensate for Deficiencies in Visual Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05439" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05439v2</a>
  <a href="https://arxiv.org/pdf/2506.05439.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05439v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05439v2', 'LLMs Can Compensate for Deficiencies in Visual Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sho Takishita, Jay Gala, Abdelrahman Mohamed, Kentaro Inui, Yova Kementchedjhieva

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹ä»¥å¼¥è¡¥è§†è§‰è¡¨ç¤ºçš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `CLIP` `å¤šæ¨¡æ€ä»»åŠ¡` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `ä¸Šä¸‹æ–‡åŒ–` `åŠ¨æ€åŠ³åŠ¨åˆ†å·¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–äºCLIPç¼–ç å™¨ï¼Œä½†å…¶è§†è§‰ç‰¹å¾å­˜åœ¨ä¸è¶³ï¼Œå½±å“å¤šæ¨¡æ€ä»»åŠ¡çš„è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è¯­è¨€è§£ç å™¨å¯¹è§†è§‰ç‰¹å¾è¿›è¡Œä¸Šä¸‹æ–‡åŒ–ï¼Œä»¥å¼¥è¡¥è§†è§‰è¡¨ç¤ºçš„ä¸è¶³ï¼Œå¢å¼ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è§†è§‰ä¸Šä¸‹æ–‡åŒ–å‡å°‘çš„æƒ…å†µä¸‹ï¼Œè¯­è¨€è§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆæ¢å¤æ€§èƒ½ï¼Œå±•ç¤ºäº†åŠ¨æ€åŠ³åŠ¨åˆ†å·¥çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šæœ‰æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åŸºäºCLIPè§†è§‰ç¼–ç å™¨ï¼Œä½†è¿™äº›ç¼–ç å™¨å­˜åœ¨å¤šç§å±€é™æ€§ã€‚æœ¬æ–‡æ¢è®¨äº†VLMsä¸­å¼ºå¤§çš„è¯­è¨€åŸºç¡€æ˜¯å¦èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡åŒ–æˆ–ä¸°å¯Œè§†è§‰ç‰¹å¾æ¥å¼¥è¡¥è¿™äº›ä¸è¶³ã€‚é€šè¿‡å¯¹ä¸‰ç§åŸºäºCLIPçš„VLMsè¿›è¡Œæ§åˆ¶è‡ªæ³¨æ„åŠ›æ¶ˆèå®éªŒï¼Œå‘ç°å°½ç®¡CLIPè§†è§‰è¡¨ç¤ºå­˜åœ¨å·²çŸ¥çš„å±€é™æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸ºè¯­è¨€è§£ç å™¨æä¾›äº†å¯è¯»çš„è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨è§†è§‰è¡¨ç¤ºä¸Šä¸‹æ–‡åŒ–å‡å°‘çš„æƒ…å†µä¸‹ï¼Œè¯­è¨€è§£ç å™¨èƒ½å¤Ÿåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¼¥è¡¥è¿™ä¸€ä¸è¶³å¹¶æ¢å¤æ€§èƒ½ã€‚è¿™è¡¨æ˜VLMsä¸­å­˜åœ¨åŠ¨æ€çš„åŠ³åŠ¨åˆ†å·¥ï¼Œå¹¶æ¿€åŠ±æœªæ¥çš„æ¶æ„å°†æ›´å¤šè§†è§‰å¤„ç†è½¬ç§»åˆ°è¯­è¨€è§£ç å™¨ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºCLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¡¨ç¤ºæ–¹é¢çš„ä¸è¶³ï¼Œæ¢è®¨å¦‚ä½•é€šè¿‡è¯­è¨€è§£ç å™¨å¼¥è¡¥è¿™äº›ç¼ºé™·ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¸å‡ï¼Œå°¤å…¶åœ¨è§†è§‰ç‰¹å¾è¾ƒå¼±æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºå¤§çš„è¯­è¨€è§£ç å™¨å¯¹è§†è§‰ç‰¹å¾è¿›è¡Œä¸Šä¸‹æ–‡åŒ–ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡æ§åˆ¶è‡ªæ³¨æ„åŠ›æ¶ˆèå®éªŒï¼ŒéªŒè¯è¯­è¨€è§£ç å™¨çš„è¡¥å¿èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬CLIPè§†è§‰ç¼–ç å™¨å’Œè¯­è¨€è§£ç å™¨ï¼Œé‡‡ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾æå–å’Œä¸Šä¸‹æ–‡åŒ–ã€‚å®éªŒè®¾è®¡ä¸­ï¼Œä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æ¢æµ‹ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ­ç¤ºäº†è¯­è¨€è§£ç å™¨åœ¨è§†è§‰ç‰¹å¾ä¸è¶³æ—¶çš„è¡¥å¿èƒ½åŠ›ï¼Œæå‡ºäº†åŠ¨æ€åŠ³åŠ¨åˆ†å·¥çš„æ¦‚å¿µï¼Œæ¨åŠ¨äº†æœªæ¥æ¨¡å‹æ¶æ„çš„è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„ä¸Šä¸‹æ–‡åŒ–ç¨‹åº¦ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è°ƒèŠ‚è§†è§‰ç‰¹å¾å¯¹è¯­è¨€è§£ç å™¨çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ€§èƒ½è¯„ä¼°ã€‚å®éªŒä½¿ç”¨äº†å¤šç§åŸºçº¿è¿›è¡Œå¯¹æ¯”ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨è§†è§‰è¡¨ç¤ºä¸Šä¸‹æ–‡åŒ–å‡å°‘çš„æƒ…å†µä¸‹ï¼Œè¯­è¨€è§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆæ¢å¤æ€§èƒ½ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†åŠ¨æ€åŠ³åŠ¨åˆ†å·¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€ç†è§£ã€å›¾åƒæè¿°ç”Ÿæˆå’Œè§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹å’Œå†…å®¹ç”Ÿæˆç­‰å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many vision-language models (VLMs) that prove very effective at a range of multimodal task, build on CLIP-based vision encoders, which are known to have various limitations. We investigate the hypothesis that the strong language backbone in VLMs compensates for possibly weak visual features by contextualizing or enriching them. Using three CLIP-based VLMs, we perform controlled self-attention ablations on a carefully designed probing task. Our findings show that despite known limitations, CLIP visual representations offer ready-to-read semantic information to the language decoder. However, in scenarios of reduced contextualization in the visual representations, the language decoder can largely compensate for the deficiency and recover performance. This suggests a dynamic division of labor in VLMs and motivates future architectures that offload more visual processing to the language decoder.

