---
layout: default
title: SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs
---

# SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05344" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05344v2</a>
  <a href="https://arxiv.org/pdf/2506.05344.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05344v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05344v2', 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-07-05)

**å¤‡æ³¨**: Accepted to ICCV 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CR400AF-A/SparseMM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSparseMMä»¥ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰ç†è§£` `æ³¨æ„åŠ›æœºåˆ¶` `KV-Cacheä¼˜åŒ–` `ç¨€ç–æ€§` `è®¡ç®—èµ„æºåˆ†é…` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†è§‰è¾“å…¥æ—¶ï¼Œå­˜åœ¨æ³¨æ„åŠ›å¤´åˆ©ç”¨ä¸å‡çš„é—®é¢˜ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚
2. æœ¬ç ”ç©¶æå‡ºSparseMMï¼Œé€šè¿‡åˆ†ææ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«å‡ºå¯¹è§†è§‰ç†è§£æœ‰è´¡çŒ®çš„ç¨€ç–æ³¨æ„åŠ›å¤´ï¼Œå¹¶ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseMMåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°äº†1.38å€çš„å®æ—¶åŠ é€Ÿå’Œ52%çš„å†…å­˜å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šå¸¸é€šè¿‡æ‰©å±•é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å®ç°è§†è§‰èƒ½åŠ›çš„å¢å¼ºã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ†ææ³¨æ„åŠ›æœºåˆ¶ï¼Œæ­ç¤ºäº†ä¸€ä¸ªæ„å¤–çš„ç¨€ç–ç°è±¡ï¼šåœ¨LLMsä¸­ï¼Œåªæœ‰çº¦5%çš„æ³¨æ„åŠ›å¤´ç§¯æå‚ä¸è§†è§‰ç†è§£ï¼Œç§°ä¸ºè§†è§‰å¤´ã€‚ä¸ºé«˜æ•ˆè¯†åˆ«è¿™äº›å¤´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ— è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç›®æ ‡å“åº”åˆ†æé‡åŒ–å¤´çº§è§†è§‰ç›¸å…³æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†SparseMMï¼Œä¸€ç§KV-Cacheä¼˜åŒ–ç­–ç•¥ï¼Œæ ¹æ®è§†è§‰å¾—åˆ†ä¸ºLLMsä¸­çš„å¤´åˆ†é…ä¸å¯¹ç§°è®¡ç®—é¢„ç®—ï¼Œåˆ©ç”¨è§†è§‰å¤´çš„ç¨€ç–æ€§åŠ é€ŸMLLMsçš„æ¨ç†ã€‚ä¸å¿½è§†è§†è§‰ç‰¹æ€§çš„å…ˆå‰KV-CacheåŠ é€Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSparseMMåœ¨è§£ç è¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘è§†è§‰è¯­ä¹‰çš„ä¿ç•™å’Œå‹åŠ›ã€‚å¹¿æ³›çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSparseMMåœ¨ä¸»æµå¤šæ¨¡æ€åŸºå‡†ä¸Šå®ç°äº†ä¼˜è¶Šçš„å‡†ç¡®æ€§ä¸æ•ˆç‡å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£ä¸­æ³¨æ„åŠ›å¤´åˆ©ç”¨ä¸å‡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨å¯¹è§†è§‰ç†è§£æœ‰è´¡çŒ®çš„æ³¨æ„åŠ›å¤´ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ä¸€ç§æ— è®­ç»ƒçš„æ¡†æ¶ï¼Œé‡åŒ–å¤´çº§è§†è§‰ç›¸å…³æ€§ï¼Œä»è€Œè¯†åˆ«å‡ºç¨€ç–çš„è§†è§‰å¤´ï¼Œå¹¶åŸºäºè¿™äº›è§†è§‰å¾—åˆ†ä¼˜åŒ–è®¡ç®—é¢„ç®—ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é€šè¿‡ç›®æ ‡å“åº”åˆ†æè¯†åˆ«è§†è§‰å¤´ï¼Œå…¶æ¬¡æ˜¯SparseMMç­–ç•¥ï¼Œæ ¹æ®è§†è§‰å¾—åˆ†ä¸ºä¸åŒçš„æ³¨æ„åŠ›å¤´åˆ†é…è®¡ç®—èµ„æºï¼Œä¼˜åŒ–KV-Cacheçš„ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†SparseMMç­–ç•¥ï¼Œåˆ©ç”¨è§†è§‰å¤´çš„ç¨€ç–æ€§è¿›è¡Œè®¡ç®—ä¼˜åŒ–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSparseMMæ›´å…³æ³¨è§†è§‰è¯­ä¹‰çš„ä¿ç•™å’Œè§£ç è¿‡ç¨‹ä¸­çš„å‹åŠ›ç®¡ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSparseMMé€šè¿‡åˆ†ææ³¨æ„åŠ›å¤´çš„è§†è§‰å¾—åˆ†æ¥åŠ¨æ€è°ƒæ•´è®¡ç®—é¢„ç®—ï¼Œç¡®ä¿é«˜æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ”¯æŒè¿™ä¸€ç­–ç•¥çš„å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSparseMMåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†1.38å€çš„å®æ—¶åŠ é€Ÿå’Œ52%çš„å†…å­˜å‡å°‘ï¼ŒåŒæ—¶åœ¨æ•ˆç‡æµ‹è¯•ä¸­ä¿æŒäº†æ€§èƒ½çš„å¹³è¡¡ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¯æ˜äº†SparseMMåœ¨ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è§†è§‰ç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒSparseMMèƒ½å¤Ÿåœ¨å®æ—¶å¤„ç†å’Œèµ„æºå—é™çš„ç¯å¢ƒä¸­æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.

