---
layout: default
title: EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?
---

# EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05287" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05287v1</a>
  <a href="https://arxiv.org/pdf/2506.05287.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05287v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05287v1', 'EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqian Yuan, Ronghao Dang, Long Li, Wentong Li, Dian Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: 32pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEOC-Benchä»¥è§£å†³åŠ¨æ€è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒè§†è§‰` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åŠ¨æ€åœºæ™¯ç†è§£` `ç‰©ä½“ä¸­å¿ƒè®¤çŸ¥` `æ—¶é—´ç»´åº¦è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é™æ€åœºæ™¯ï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°ç”¨æˆ·äº¤äº’å¯¼è‡´çš„åŠ¨æ€å˜åŒ–ã€‚
2. æå‡ºEOC-BenchåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°åŠ¨æ€è‡ªæˆ‘ä¸­å¿ƒåœºæ™¯ä¸­çš„ç‰©ä½“è®¤çŸ¥èƒ½åŠ›ï¼Œå¡«è¡¥ç°æœ‰ç ”ç©¶ç©ºç™½ã€‚
3. åŸºäºEOC-Benchï¼Œå¯¹å¤šç§MLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¨åŠ¨äº†ç‰©ä½“è®¤çŸ¥èƒ½åŠ›çš„æå‡ï¼Œå¥ å®šäº†å¯é çš„åŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°æ¨åŠ¨äº†è‡ªæˆ‘ä¸­å¿ƒè§†è§‰åº”ç”¨çš„çªç ´ã€‚è¿™äº›åº”ç”¨éœ€è¦å¯¹ç‰©ä½“è¿›è¡ŒæŒç»­çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€å’Œæ‚ä¹±çš„ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨é™æ€åœºæ™¯æ¢ç´¢ä¸Šï¼Œå¿½è§†äº†ç”¨æˆ·äº¤äº’æ‰€å¸¦æ¥çš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†EOC-Benchï¼Œä¸€ä¸ªåˆ›æ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°åŠ¨æ€è‡ªæˆ‘ä¸­å¿ƒåœºæ™¯ä¸­çš„ç‰©ä½“ä¸­å¿ƒè®¤çŸ¥ã€‚EOC-BenchåŒ…å«3,277ä¸ªç²¾å¿ƒæ³¨é‡Šçš„é—®ç­”å¯¹ï¼Œæ¶µç›–è¿‡å»ã€ç°åœ¨å’Œæœªæ¥ä¸‰ä¸ªæ—¶é—´ç±»åˆ«ï¼Œæ¶‰åŠ11ä¸ªç»†ç²’åº¦è¯„ä¼°ç»´åº¦å’Œ3ç§è§†è§‰ç‰©ä½“å¼•ç”¨ç±»å‹ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†æ··åˆæ ¼å¼çš„äººæœºåä½œæ³¨é‡Šæ¡†æ¶å’Œæ–°é¢–çš„å¤šå°ºåº¦æ—¶é—´å‡†ç¡®æ€§æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å…¨é¢è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨åŠ¨æ€è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç†è§£ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹å¯¹ç”¨æˆ·äº¤äº’å¼•å‘çš„åŠ¨æ€å˜åŒ–çš„è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEOC-Benché€šè¿‡å¼•å…¥æ—¶é—´ç»´åº¦çš„è¯„ä¼°ï¼Œç³»ç»Ÿæ€§åœ°è€ƒå¯Ÿç‰©ä½“åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå¼ºè°ƒç‰©ä½“çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEOC-Benchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°æ®é›†æ„å»ºã€é—®ç­”å¯¹è®¾è®¡å’Œè¯„ä¼°æŒ‡æ ‡å¼€å‘ã€‚æ•°æ®é›†åŒ…å«3,277ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–ä¸åŒæ—¶é—´ç±»åˆ«å’Œè¯„ä¼°ç»´åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šEOC-Benchçš„åˆ›æ–°ç‚¹åœ¨äºå…¶å¤šå°ºåº¦æ—¶é—´å‡†ç¡®æ€§æŒ‡æ ‡å’Œæ··åˆæ ¼å¼çš„äººæœºåä½œæ³¨é‡Šæ¡†æ¶ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—è¯„ä¼°æ›´ä¸ºå…¨é¢å’Œæ·±å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é—®ç­”å¯¹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å››ç§ç±»å‹çš„é—®é¢˜ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å¤šæ ·æ€§å’Œå…¨é¢æ€§ï¼ŒåŒæ—¶åœ¨è¯„ä¼°æŒ‡æ ‡ä¸­å¼•å…¥äº†æ—¶é—´ç»´åº¦ï¼Œä»¥é€‚åº”åŠ¨æ€åœºæ™¯çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡EOC-Benchçš„è¯„ä¼°ï¼Œå¤šä¸ªMLLMsåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„ç‰©ä½“è®¤çŸ¥èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶é—´ç»´åº¦çš„ç†è§£ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºå‡†æµ‹è¯•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºè¯¥åŸºå‡†çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EOC-Benchçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸï¼Œæå‡è¿™äº›ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ç‰©ä½“è¯†åˆ«å’Œç†è§£èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å°†ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç³»ç»Ÿæä¾›é‡è¦æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.

