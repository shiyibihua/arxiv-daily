---
layout: default
title: Refer to Any Segmentation Mask Group With Vision-Language Prompts
---

# Refer to Any Segmentation Mask Group With Vision-Language Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05342" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05342v2</a>
  <a href="https://arxiv.org/pdf/2506.05342.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05342v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05342v2', 'Refer to Any Segmentation Mask Group With Vision-Language Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shengcao Cao, Zijun Wei, Jason Kuen, Kangning Liu, Lingzhi Zhang, Jiuxiang Gu, HyunJoon Jung, Liang-Yan Gui, Yu-Xiong Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…¨æ¨¡æ€å‚è€ƒè¡¨è¾¾åˆ†å‰²ä»¥è§£å†³è§†è§‰è¯­è¨€äº¤äº’ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…¨æ¨¡æ€åˆ†å‰²` `è§†è§‰è¯­è¨€äº¤äº’` `æ©è†œç”Ÿæˆ` `å¤šæ¨¡æ€æ¨¡å‹` `å‚è€ƒè¡¨è¾¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„è§†è§‰è¯­è¨€æŸ¥è¯¢æ—¶ç¼ºä¹å…¨é¢çš„è¯­ä¹‰ç†è§£ï¼Œé™åˆ¶äº†å…¶åœ¨ç”¨æˆ·å‹å¥½äº¤äº’ä¸­çš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…¨æ¨¡æ€å‚è€ƒè¡¨è¾¾åˆ†å‰²ä»»åŠ¡ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰å®ä½“çš„ç»„åˆæç¤ºç”Ÿæˆæ©è†œç»„ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚
3. é€šè¿‡åˆ›å»ºæ–°çš„æ•°æ®é›†å¹¶è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„RASæ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†åˆ†å‰²æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå›¾åƒåˆ†å‰²æ¨¡å‹åœ¨é«˜è´¨é‡æ©è†œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†åŸºäºè¯­è¨€å’Œè§†è§‰çš„å¤æ‚æŸ¥è¯¢æ—¶ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”å…¨æ¨¡æ€å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆORESï¼‰ï¼Œæ—¨åœ¨æ ¹æ®æ–‡æœ¬æˆ–æ–‡æœ¬åŠ å‚è€ƒè§†è§‰å®ä½“ç”Ÿæˆæ©è†œç»„ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†â€œå‚è€ƒä»»æ„åˆ†å‰²æ©è†œç»„â€ï¼ˆRASï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ©è†œä¸­å¿ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¢å¼ºåˆ†å‰²æ¨¡å‹çš„å¤šæ¨¡æ€äº¤äº’å’Œç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†MaskGroups-2Må’ŒMaskGroups-HQæ•°æ®é›†ï¼Œä»¥æ”¯æŒORESæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRASåœ¨ORESä»»åŠ¡åŠç»å…¸çš„å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰å’Œå¹¿ä¹‰å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆGRESï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰è¯­è¨€æŸ¥è¯¢æ—¶çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”ŸæˆåŸºäºæ–‡æœ¬å’Œè§†è§‰æç¤ºçš„æ©è†œç»„æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå…¨æ¨¡æ€å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆORESï¼‰ä»»åŠ¡ï¼Œé€šè¿‡å¼•å…¥â€œå‚è€ƒä»»æ„åˆ†å‰²æ©è†œç»„â€ï¼ˆRASï¼‰æ¡†æ¶ï¼Œå¢å¼ºæ¨¡å‹çš„å¤šæ¨¡æ€äº¤äº’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å¤æ‚çš„ç”¨æˆ·æç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRASæ¡†æ¶åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥å¤„ç†æ¨¡å—ï¼Œæ¥ç€æ˜¯æ©è†œç”Ÿæˆæ¨¡å—ï¼Œæœ€åæ˜¯å¤šæ¨¡æ€äº¤äº’æ¨¡å—ï¼Œæ•´ä½“æµç¨‹é€šè¿‡æ©è†œä¸­å¿ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ©è†œä¸­å¿ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è§†è§‰è¯­è¨€æç¤ºï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ©è†œç”Ÿæˆçš„è´¨é‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€è¾“å…¥çš„æœ‰æ•ˆå¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRASæ¡†æ¶åœ¨ORESä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†çº¦15%çš„åˆ†å‰²å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰å’Œå¹¿ä¹‰å‚è€ƒè¡¨è¾¾åˆ†å‰²ï¼ˆGRESï¼‰ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å›¾åƒç¼–è¾‘ã€å¢å¼ºç°å®å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å›¾åƒåˆ†å‰²æ¨¡å‹å¯¹å¤æ‚è§†è§‰è¯­è¨€æç¤ºçš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°æ›´è‡ªç„¶çš„ç”¨æˆ·äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent image segmentation models have advanced to segment images into high-quality masks for visual entities, and yet they cannot provide comprehensive semantic understanding for complex queries based on both language and vision. This limitation reduces their effectiveness in applications that require user-friendly interactions driven by vision-language prompts. To bridge this gap, we introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. To address this new challenge, we propose a novel framework to "Refer to Any Segmentation Mask Group" (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. For training and benchmarking ORES models, we create datasets MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by text and reference entities. Through extensive evaluation, we demonstrate superior performance of RAS on our new ORES task, as well as classic referring expression segmentation (RES) and generalized referring expression segmentation (GRES) tasks. Project page: https://Ref2Any.github.io.

