---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-05
---

# cs.CVï¼ˆ2025-06-05ï¼‰

ğŸ“Š å…± **69** ç¯‡è®ºæ–‡
 | ğŸ”— **20** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (29 ğŸ”—8)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (17 ğŸ”—6)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (29 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250605331v1-mint-cot-enabling-interleaved-visual-tokens-in-mathematical-chain-of.html">MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning</a></td>
  <td>æå‡ºMINT-CoTä»¥è§£å†³å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„è§†è§‰ä¿¡å·æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05331v1" data-paper-url="./papers/250605331v1-mint-cot-enabling-interleaved-visual-tokens-in-mathematical-chain-of.html" onclick="toggleFavorite(this, '2506.05331v1', 'MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250604897v3-from-objects-to-anywhere-a-holistic-benchmark-for-multi-level-visual.html">From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes</a></td>
  <td>æå‡ºAnywhere3D-Benchä»¥è§£å†³3Dåœºæ™¯ä¸­çš„å¤šå±‚æ¬¡è§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04897v3" data-paper-url="./papers/250604897v3-from-objects-to-anywhere-a-holistic-benchmark-for-multi-level-visual.html" onclick="toggleFavorite(this, '2506.04897v3', 'From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250605087v1-interpretable-multimodal-framework-for-human-centered-street-assessm.html">Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics</a></td>
  <td>æå‡ºå¤šæ¨¡æ€è¡—é“è¯„ä¼°æ¡†æ¶è§£å†³åŸå¸‚è®¾è®¡ä¸»è§‚æ„ŸçŸ¥ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05087v1" data-paper-url="./papers/250605087v1-interpretable-multimodal-framework-for-human-centered-street-assessm.html" onclick="toggleFavorite(this, '2506.05087v1', 'Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250604633v1-unfolding-spatial-cognition-evaluating-multimodal-models-on-visual-s.html">Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations</a></td>
  <td>æå‡ºSTAREåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰æ¨¡æ‹Ÿä¸­çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04633v1" data-paper-url="./papers/250604633v1-unfolding-spatial-cognition-evaluating-multimodal-models-on-visual-s.html" onclick="toggleFavorite(this, '2506.04633v1', 'Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250605551v2-when-semantics-mislead-vision-mitigating-large-multimodal-models-hal.html">When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding</a></td>
  <td>æå‡ºZoomTextä¸Grounded Layer Correctionä»¥ç¼“è§£åœºæ™¯æ–‡æœ¬ç†è§£ä¸­çš„è¯­ä¹‰å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05551v2" data-paper-url="./papers/250605551v2-when-semantics-mislead-vision-mitigating-large-multimodal-models-hal.html" onclick="toggleFavorite(this, '2506.05551v2', 'When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250605523v1-morse-500-a-programmatically-controllable-video-benchmark-to-stress-.html">MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning</a></td>
  <td>æå‡ºMORSE-500ä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05523v1" data-paper-url="./papers/250605523v1-morse-500-a-programmatically-controllable-video-benchmark-to-stress-.html" onclick="toggleFavorite(this, '2506.05523v1', 'MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250605349v2-videomathqa-benchmarking-mathematical-reasoning-via-multimodal-under.html">VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos</a></td>
  <td>æå‡ºVideoMathQAä»¥è§£å†³è§†é¢‘ä¸­çš„æ•°å­¦æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05349v2" data-paper-url="./papers/250605349v2-videomathqa-benchmarking-mathematical-reasoning-via-multimodal-under.html" onclick="toggleFavorite(this, '2506.05349v2', 'VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250605263v1-can-foundation-models-generalise-the-presentation-attack-detection-c.html">Can Foundation Models Generalise the Presentation Attack Detection Capabilities on ID Cards?</a></td>
  <td>åˆ©ç”¨åŸºç¡€æ¨¡å‹æå‡èº«ä»½è¯ä»¶çš„å‘ˆç°æ”»å‡»æ£€æµ‹èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05263v1" data-paper-url="./papers/250605263v1-can-foundation-models-generalise-the-presentation-attack-detection-c.html" onclick="toggleFavorite(this, '2506.05263v1', 'Can Foundation Models Generalise the Presentation Attack Detection Capabilities on ID Cards?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250605191v2-moka-multimodal-low-rank-adaptation-for-mllms.html">MokA: Multimodal Low-Rank Adaptation for MLLMs</a></td>
  <td>æå‡ºMokAä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05191v2" data-paper-url="./papers/250605191v2-moka-multimodal-low-rank-adaptation-for-mllms.html" onclick="toggleFavorite(this, '2506.05191v2', 'MokA: Multimodal Low-Rank Adaptation for MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250605184v1-single-gpu-task-adaptation-of-pathology-foundation-models-for-whole-.html">Single GPU Task Adaptation of Pathology Foundation Models for Whole Slide Image Analysis</a></td>
  <td>æå‡ºTAPFMä»¥è§£å†³ç—…ç†åŸºç¡€æ¨¡å‹åœ¨å…¨åˆ‡ç‰‡å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05184v1" data-paper-url="./papers/250605184v1-single-gpu-task-adaptation-of-pathology-foundation-models-for-whole-.html" onclick="toggleFavorite(this, '2506.05184v1', 'Single GPU Task Adaptation of Pathology Foundation Models for Whole Slide Image Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250605127v2-pixcell-a-generative-foundation-model-for-digital-histopathology-ima.html">PixCell: A generative foundation model for digital histopathology images</a></td>
  <td>æå‡ºPixCellä»¥è§£å†³æ•°å­—ç—…ç†å›¾åƒç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05127v2" data-paper-url="./papers/250605127v2-pixcell-a-generative-foundation-model-for-digital-histopathology-ima.html" onclick="toggleFavorite(this, '2506.05127v2', 'PixCell: A generative foundation model for digital histopathology images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250605441v1-deep-histological-synthesis-from-mass-spectrometry-imaging-for-multi.html">Deep histological synthesis from mass spectrometry imaging for multimodal registration</a></td>
  <td>æå‡ºåŸºäºpix2pixæ¨¡å‹çš„ç»„ç»‡å­¦å›¾åƒåˆæˆä»¥è§£å†³å¤šæ¨¡æ€é…å‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05441v1" data-paper-url="./papers/250605441v1-deep-histological-synthesis-from-mass-spectrometry-imaging-for-multi.html" onclick="toggleFavorite(this, '2506.05441v1', 'Deep histological synthesis from mass spectrometry imaging for multimodal registration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250605440v1-byo-eval-build-your-own-dataset-for-fine-grained-visual-assessment-o.html">BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models</a></td>
  <td>æå‡ºBYO-Evalä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05440v1" data-paper-url="./papers/250605440v1-byo-eval-build-your-own-dataset-for-fine-grained-visual-assessment-o.html" onclick="toggleFavorite(this, '2506.05440v1', 'BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250604837v1-openmaskdino3d-reasoning-3d-segmentation-via-large-language-model.html">OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model</a></td>
  <td>æå‡ºOpenMaskDINO3Dä»¥è§£å†³3Dåˆ†å‰²æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04837v1" data-paper-url="./papers/250604837v1-openmaskdino3d-reasoning-3d-segmentation-via-large-language-model.html" onclick="toggleFavorite(this, '2506.04837v1', 'OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250605344v2-sparsemm-head-sparsity-emerges-from-visual-concept-responses-in-mllm.html">SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs</a></td>
  <td>æå‡ºSparseMMä»¥ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05344v2" data-paper-url="./papers/250605344v2-sparsemm-head-sparsity-emerges-from-visual-concept-responses-in-mllm.html" onclick="toggleFavorite(this, '2506.05344v2', 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250605210v2-towards-vision-language-garment-models-for-web-knowledge-garment-und.html">Towards Vision-Language-Garment Models for Web Knowledge Garment Understanding and Generation</a></td>
  <td>æå‡ºVLGæ¨¡å‹ä»¥è§£å†³æœè£…ç”Ÿæˆé¢†åŸŸçš„çŸ¥è¯†è½¬ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05210v2" data-paper-url="./papers/250605210v2-towards-vision-language-garment-models-for-web-knowledge-garment-und.html" onclick="toggleFavorite(this, '2506.05210v2', 'Towards Vision-Language-Garment Models for Web Knowledge Garment Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250605198v1-quantifying-cross-modality-memorization-in-vision-language-models.html">Quantifying Cross-Modality Memorization in Vision-Language Models</a></td>
  <td>é‡åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è·¨æ¨¡æ€è®°å¿†ä»¥æå‡çŸ¥è¯†è¿ç§»èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05198v1" data-paper-url="./papers/250605198v1-quantifying-cross-modality-memorization-in-vision-language-models.html" onclick="toggleFavorite(this, '2506.05198v1', 'Quantifying Cross-Modality Memorization in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250605061v1-a-survey-on-vietnamese-document-analysis-and-recognition-challenges-.html">A Survey on Vietnamese Document Analysis and Recognition: Challenges and Future Directions</a></td>
  <td>ç»¼è¿°è¶Šå—æ–‡æ¡£åˆ†æä¸è¯†åˆ«æŠ€æœ¯ä»¥åº”å¯¹ç‹¬ç‰¹æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05061v1" data-paper-url="./papers/250605061v1-a-survey-on-vietnamese-document-analysis-and-recognition-challenges-.html" onclick="toggleFavorite(this, '2506.05061v1', 'A Survey on Vietnamese Document Analysis and Recognition: Challenges and Future Directions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250604983v1-textvidbench-a-benchmark-for-long-video-scene-text-understanding.html">TextVidBench: A Benchmark for Long Video Scene Text Understanding</a></td>
  <td>æå‡ºTextVidBenchä»¥è§£å†³é•¿è§†é¢‘åœºæ™¯æ–‡æœ¬ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04983v1" data-paper-url="./papers/250604983v1-textvidbench-a-benchmark-for-long-video-scene-text-understanding.html" onclick="toggleFavorite(this, '2506.04983v1', 'TextVidBench: A Benchmark for Long Video Scene Text Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250604953v3-apvr-hour-level-long-video-understanding-with-adaptive-pivot-visual-.html">APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval</a></td>
  <td>æå‡ºAPVRä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¿¡æ¯æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04953v3" data-paper-url="./papers/250604953v3-apvr-hour-level-long-video-understanding-with-adaptive-pivot-visual-.html" onclick="toggleFavorite(this, '2506.04953v3', 'APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250605342v2-refer-to-any-segmentation-mask-group-with-vision-language-prompts.html">Refer to Any Segmentation Mask Group With Vision-Language Prompts</a></td>
  <td>æå‡ºå…¨æ¨¡æ€å‚è€ƒè¡¨è¾¾åˆ†å‰²ä»¥è§£å†³è§†è§‰è¯­è¨€äº¤äº’ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05342v2" data-paper-url="./papers/250605342v2-refer-to-any-segmentation-mask-group-with-vision-language-prompts.html" onclick="toggleFavorite(this, '2506.05342v2', 'Refer to Any Segmentation Mask Group With Vision-Language Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250605302v1-perceive-anything-recognize-explain-caption-and-segment-anything-in-.html">Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos</a></td>
  <td>æå‡ºPerceive Anythingæ¨¡å‹ä»¥è§£å†³å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05302v1" data-paper-url="./papers/250605302v1-perceive-anything-recognize-explain-caption-and-segment-anything-in-.html" onclick="toggleFavorite(this, '2506.05302v1', 'Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250605218v1-monkeyocr-document-parsing-with-a-structure-recognition-relation-tri.html">MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm</a></td>
  <td>æå‡ºMonkeyOCRä»¥è§£å†³æ–‡æ¡£è§£ææ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05218v1" data-paper-url="./papers/250605218v1-monkeyocr-document-parsing-with-a-structure-recognition-relation-tri.html" onclick="toggleFavorite(this, '2506.05218v1', 'MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250605083v2-seededit-30-fast-and-high-quality-generative-image-editing.html">SeedEdit 3.0: Fast and High-Quality Generative Image Editing</a></td>
  <td>æå‡ºSeedEdit 3.0ä»¥è§£å†³é«˜è´¨é‡å›¾åƒç¼–è¾‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05083v2" data-paper-url="./papers/250605083v2-seededit-30-fast-and-high-quality-generative-image-editing.html" onclick="toggleFavorite(this, '2506.05083v2', 'SeedEdit 3.0: Fast and High-Quality Generative Image Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250605046v2-flowdirector-training-free-flow-steering-for-precise-text-to-video-e.html">FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing</a></td>
  <td>æå‡ºFlowDirectorä»¥è§£å†³è§†é¢‘ç¼–è¾‘ä¸­çš„é€†å‘è¿‡ç¨‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05046v2" data-paper-url="./papers/250605046v2-flowdirector-training-free-flow-steering-for-precise-text-to-video-e.html" onclick="toggleFavorite(this, '2506.05046v2', 'FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250605439v2-llms-can-compensate-for-deficiencies-in-visual-representations.html">LLMs Can Compensate for Deficiencies in Visual Representations</a></td>
  <td>æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹ä»¥å¼¥è¡¥è§†è§‰è¡¨ç¤ºçš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05439v2" data-paper-url="./papers/250605439v2-llms-can-compensate-for-deficiencies-in-visual-representations.html" onclick="toggleFavorite(this, '2506.05439v2', 'LLMs Can Compensate for Deficiencies in Visual Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250604715v2-towards-holistic-visual-quality-assessment-of-ai-generated-videos-a-.html">Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model</a></td>
  <td>æå‡ºå¤šç»´åº¦è¯„ä¼°æ¨¡å‹ä»¥è§£å†³AIç”Ÿæˆè§†é¢‘çš„è§†è§‰è´¨é‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04715v2" data-paper-url="./papers/250604715v2-towards-holistic-visual-quality-assessment-of-ai-generated-videos-a-.html" onclick="toggleFavorite(this, '2506.04715v2', 'Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250604706v1-line-of-sight-on-linear-representations-in-vllms.html">Line of Sight: On Linear Representations in VLLMs</a></td>
  <td>æå‡ºå¤šæ¨¡æ€ç¨€ç–è‡ªç¼–ç å™¨ä»¥å¢å¼ºVLLMçš„å›¾åƒè¡¨ç¤ºèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04706v1" data-paper-url="./papers/250604706v1-line-of-sight-on-linear-representations-in-vllms.html" onclick="toggleFavorite(this, '2506.04706v1', 'Line of Sight: On Linear Representations in VLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250604704v5-holisafe-holistic-safety-benchmarking-and-modeling-for-vision-langua.html">HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language Model</a></td>
  <td>æå‡ºHoliSafeä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹å®‰å…¨æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04704v5" data-paper-url="./papers/250604704v5-holisafe-holistic-safety-benchmarking-and-modeling-for-vision-langua.html" onclick="toggleFavorite(this, '2506.04704v5', 'HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (17 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250604908v1-generating-synthetic-stereo-datasets-using-3d-gaussian-splatting-and.html">Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and Expert Knowledge Transfer</a></td>
  <td>æå‡ºåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„ç«‹ä½“æ•°æ®é›†ç”Ÿæˆæ–¹æ³•ä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04908v1" data-paper-url="./papers/250604908v1-generating-synthetic-stereo-datasets-using-3d-gaussian-splatting-and.html" onclick="toggleFavorite(this, '2506.04908v1', 'Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and Expert Knowledge Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250605327v1-revisiting-depth-representations-for-feed-forward-3d-gaussian-splatt.html">Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting</a></td>
  <td>æå‡ºPM-Lossä»¥è§£å†³æ·±åº¦å›¾å¯¼è‡´çš„ç‚¹äº‘ç¨€ç–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05327v1" data-paper-url="./papers/250605327v1-revisiting-depth-representations-for-feed-forward-3d-gaussian-splatt.html" onclick="toggleFavorite(this, '2506.05327v1', 'Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250605009v1-point-cloud-segmentation-of-agricultural-vehicles-using-3d-gaussian-.html">Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting</a></td>
  <td>æå‡º3Dé«˜æ–¯ç‚¹äº‘åˆ†å‰²æ–¹æ³•ä»¥è§£å†³å†œä¸šè½¦è¾†è¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05009v1" data-paper-url="./papers/250605009v1-point-cloud-segmentation-of-agricultural-vehicles-using-3d-gaussian-.html" onclick="toggleFavorite(this, '2506.05009v1', 'Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250605011v1-uav4d-dynamic-neural-rendering-of-human-centric-uav-imagery-using-ga.html">UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting</a></td>
  <td>æå‡ºUAV4Dä»¥è§£å†³æ— äººæœºå›¾åƒåŠ¨æ€æ¸²æŸ“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">SMPL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05011v1" data-paper-url="./papers/250605011v1-uav4d-dynamic-neural-rendering-of-human-centric-uav-imagery-using-ga.html" onclick="toggleFavorite(this, '2506.05011v1', 'UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250605563v1-voxelsplat-dynamic-gaussian-splatting-as-an-effective-loss-for-occup.html">VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction</a></td>
  <td>æå‡ºVoxelSplatä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸‹çš„å ç”¨ä¸æµé¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05563v1" data-paper-url="./papers/250605563v1-voxelsplat-dynamic-gaussian-splatting-as-an-effective-loss-for-occup.html" onclick="toggleFavorite(this, '2506.05563v1', 'VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250605280v3-unifying-appearance-codes-and-bilateral-grids-for-driving-scene-gaus.html">Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting</a></td>
  <td>æå‡ºå¤šå°ºåº¦åŒè¾¹ç½‘æ ¼ä»¥æå‡åŠ¨æ€é©¾é©¶åœºæ™¯é‡å»ºç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05280v3" data-paper-url="./papers/250605280v3-unifying-appearance-codes-and-bilateral-grids-for-driving-scene-gaus.html" onclick="toggleFavorite(this, '2506.05280v3', 'Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250605341v2-direct-numerical-layout-generation-for-3d-indoor-scene-synthesis-via.html">Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning</a></td>
  <td>æå‡ºDirectLayoutä»¥è§£å†³3Då®¤å†…åœºæ™¯åˆæˆä¸­çš„å¸ƒå±€ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05341v2" data-paper-url="./papers/250605341v2-direct-numerical-layout-generation-for-3d-indoor-scene-synthesis-via.html" onclick="toggleFavorite(this, '2506.05341v2', 'Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250605204v1-oggsplat-open-gaussian-growing-for-generalizable-reconstruction-with.html">OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View</a></td>
  <td>æå‡ºOGGSplatä»¥è§£å†³ç¨€ç–è§†å›¾ä¸‹çš„3Dåœºæ™¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05204v1" data-paper-url="./papers/250605204v1-oggsplat-open-gaussian-growing-for-generalizable-reconstruction-with.html" onclick="toggleFavorite(this, '2506.05204v1', 'OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250605558v1-on-the-fly-reconstruction-for-large-scale-novel-view-synthesis-from-.html">On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images</a></td>
  <td>æå‡ºä¸€ç§å³æ—¶é‡å»ºæ–¹æ³•ä»¥è§£å†³å¤§è§„æ¨¡æ–°è§†è§’åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05558v1" data-paper-url="./papers/250605558v1-on-the-fly-reconstruction-for-large-scale-novel-view-synthesis-from-.html" onclick="toggleFavorite(this, '2506.05558v1', 'On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250605546v2-layered-motion-fusion-lifting-motion-segmentation-to-3d-in-egocentri.html">Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos</a></td>
  <td>æå‡ºåˆ†å±‚è¿åŠ¨èåˆä»¥è§£å†³åŠ¨æ€è§†é¢‘ä¸­çš„è¿åŠ¨åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">neural radiance field</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05546v2" data-paper-url="./papers/250605546v2-layered-motion-fusion-lifting-motion-segmentation-to-3d-in-egocentri.html" onclick="toggleFavorite(this, '2506.05546v2', 'Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250605008v3-structure-aware-radar-camera-depth-estimation.html">Structure-Aware Radar-Camera Depth Estimation</a></td>
  <td>æå‡ºç»“æ„æ„ŸçŸ¥é›·è¾¾-ç›¸æœºæ·±åº¦ä¼°è®¡ä»¥è§£å†³ç¨€ç–å™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">metric depth</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05008v3" data-paper-url="./papers/250605008v3-structure-aware-radar-camera-depth-estimation.html" onclick="toggleFavorite(this, '2506.05008v3', 'Structure-Aware Radar-Camera Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250605425v1-siv-bench-a-video-benchmark-for-social-interaction-understanding-and.html">SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning</a></td>
  <td>æå‡ºSIV-Benchä»¥è§£å†³ç¤¾äº¤äº’åŠ¨ç†è§£ä¸æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05425v1" data-paper-url="./papers/250605425v1-siv-bench-a-video-benchmark-for-social-interaction-understanding-and.html" onclick="toggleFavorite(this, '2506.05425v1', 'SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250604676v1-gen-n-val-agentic-image-data-generation-and-validation.html">Gen-n-Val: Agentic Image Data Generation and Validation</a></td>
  <td>æå‡ºGen-n-Valæ¡†æ¶ä»¥è§£å†³è®¡ç®—æœºè§†è§‰ä¸­çš„æ•°æ®ç¨€ç¼ºä¸æ ‡ç­¾å™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04676v1" data-paper-url="./papers/250604676v1-gen-n-val-agentic-image-data-generation-and-validation.html" onclick="toggleFavorite(this, '2506.04676v1', 'Gen-n-Val: Agentic Image Data Generation and Validation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250605348v2-freetimegs-free-gaussian-primitives-at-anytime-and-anywhere-for-dyna.html">FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction</a></td>
  <td>æå‡ºFreeTimeGSä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„å¤æ‚è¿åŠ¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05348v2" data-paper-url="./papers/250605348v2-freetimegs-free-gaussian-primitives-at-anytime-and-anywhere-for-dyna.html" onclick="toggleFavorite(this, '2506.05348v2', 'FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250604789v3-object-x-learning-to-reconstruct-multi-modal-3d-object-representatio.html">Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations</a></td>
  <td>æå‡ºObject-Xä»¥è§£å†³å¤šæ¨¡æ€3Dç‰©ä½“è¡¨ç¤ºé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04789v3" data-paper-url="./papers/250604789v3-object-x-learning-to-reconstruct-multi-modal-3d-object-representatio.html" onclick="toggleFavorite(this, '2506.04789v3', 'Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250604612v1-perfecting-depth-uncertainty-aware-enhancement-of-metric-depth.html">Perfecting Depth: Uncertainty-Aware Enhancement of Metric Depth</a></td>
  <td>æå‡ºPerfecting Depthæ¡†æ¶ä»¥å¢å¼ºä¼ æ„Ÿå™¨æ·±åº¦æ•°æ®çš„å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04612v1" data-paper-url="./papers/250604612v1-perfecting-depth-uncertainty-aware-enhancement-of-metric-depth.html" onclick="toggleFavorite(this, '2506.04612v1', 'Perfecting Depth: Uncertainty-Aware Enhancement of Metric Depth')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/250605317v2-projo4d-progressive-joint-optimization-for-sparse-view-inverse-physi.html">ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation</a></td>
  <td>æå‡ºProJo4Dä»¥è§£å†³ç¨€ç–è§†å›¾é€†ç‰©ç†ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05317v2" data-paper-url="./papers/250605317v2-projo4d-progressive-joint-optimization-for-sparse-view-inverse-physi.html" onclick="toggleFavorite(this, '2506.05317v2', 'ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/250604758v1-toward-better-ssim-loss-for-unsupervised-monocular-depth-estimation.html">Toward Better SSIM Loss for Unsupervised Monocular Depth Estimation</a></td>
  <td>æå‡ºæ–°å‹SSIMæŸå¤±å‡½æ•°ä»¥æ”¹å–„æ— ç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04758v1" data-paper-url="./papers/250604758v1-toward-better-ssim-loss-for-unsupervised-monocular-depth-estimation.html" onclick="toggleFavorite(this, '2506.04758v1', 'Toward Better SSIM Loss for Unsupervised Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250605250v1-spatiotemporal-contrastive-learning-for-cross-view-video-localizatio.html">Spatiotemporal Contrastive Learning for Cross-View Video Localization in Unstructured Off-road Terrains</a></td>
  <td>æå‡ºMoViXä»¥è§£å†³GPSç¼ºå¤±ä¸‹çš„è¶Šé‡è§†é¢‘å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05250v1" data-paper-url="./papers/250605250v1-spatiotemporal-contrastive-learning-for-cross-view-video-localizatio.html" onclick="toggleFavorite(this, '2506.05250v1', 'Spatiotemporal Contrastive Learning for Cross-View Video Localization in Unstructured Off-road Terrains')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250604716v1-learning-dissection-trajectories-from-expert-surgical-videos-via-imi.html">Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion</a></td>
  <td>æå‡ºiDPOEä»¥è§£å†³å†…é•œä¸‹ç²˜è†œå‰¥ç¦»æœ¯è½¨è¿¹é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04716v1" data-paper-url="./papers/250604716v1-learning-dissection-trajectories-from-expert-surgical-videos-via-imi.html" onclick="toggleFavorite(this, '2506.04716v1', 'Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250605297v1-dm-segnet-dual-mamba-architecture-for-3d-medical-image-segmentation-.html">DM-SegNet: Dual-Mamba Architecture for 3D Medical Image Segmentation with Global Context Modeling</a></td>
  <td>æå‡ºDM-SegNetä»¥è§£å†³3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05297v1" data-paper-url="./papers/250605297v1-dm-segnet-dual-mamba-architecture-for-3d-medical-image-segmentation-.html" onclick="toggleFavorite(this, '2506.05297v1', 'DM-SegNet: Dual-Mamba Architecture for 3D Medical Image Segmentation with Global Context Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>51</td>
  <td><a href="./papers/250605419v1-dream-to-generalize-zero-shot-model-based-reinforcement-learning-for.html">Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions</a></td>
  <td>æå‡ºDream to Generalizeä»¥è§£å†³è§†è§‰å¹²æ‰°ä¸‹çš„é›¶-shotæ¨¡å‹å¼ºåŒ–å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">world model</span> <span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05419v1" data-paper-url="./papers/250605419v1-dream-to-generalize-zero-shot-model-based-reinforcement-learning-for.html" onclick="toggleFavorite(this, '2506.05419v1', 'Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>52</td>
  <td><a href="./papers/250605328v2-av-reasoner-improving-and-benchmarking-clue-grounded-audio-visual-co.html">AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs</a></td>
  <td>æå‡ºCG-AV-CountingåŸºå‡†ä¸AV-Reasoneræ¨¡å‹ä»¥æå‡å¤šæ¨¡æ€è®¡æ•°èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">curriculum learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05328v2" data-paper-url="./papers/250605328v2-av-reasoner-improving-and-benchmarking-clue-grounded-audio-visual-co.html" onclick="toggleFavorite(this, '2506.05328v2', 'AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>53</td>
  <td><a href="./papers/250605284v1-video-world-models-with-long-term-spatial-memory.html">Video World Models with Long-term Spatial Memory</a></td>
  <td>æå‡ºå‡ ä½•åŸºç¡€çš„é•¿æ—¶ç©ºè®°å¿†ä»¥è§£å†³è§†é¢‘ä¸–ç•Œæ¨¡å‹ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05284v1" data-paper-url="./papers/250605284v1-video-world-models-with-long-term-spatial-memory.html" onclick="toggleFavorite(this, '2506.05284v1', 'Video World Models with Long-term Spatial Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>54</td>
  <td><a href="./papers/250605274v2-from-play-to-replay-composed-video-retrieval-for-temporally-fine-gra.html">From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos</a></td>
  <td>æå‡ºTF-CoVRä»¥è§£å†³ç»†ç²’åº¦è§†é¢‘æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05274v2" data-paper-url="./papers/250605274v2-from-play-to-replay-composed-video-retrieval-for-temporally-fine-gra.html" onclick="toggleFavorite(this, '2506.05274v2', 'From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>55</td>
  <td><a href="./papers/250605260v1-leanpo-lean-preference-optimization-for-likelihood-alignment-in-vide.html">LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs</a></td>
  <td>æå‡ºLeanPOä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åå¥½å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05260v1" data-paper-url="./papers/250605260v1-leanpo-lean-preference-optimization-for-likelihood-alignment-in-vide.html" onclick="toggleFavorite(this, '2506.05260v1', 'LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/250604892v1-learning-to-plan-via-supervised-contrastive-learning-and-strategic-i.html">Learning to Plan via Supervised Contrastive Learning and Strategic Interpolation: A Chess Case Study</a></td>
  <td>é€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸æˆ˜ç•¥æ’å€¼æå‡ºæ£‹ç±»è§„åˆ’æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04892v1" data-paper-url="./papers/250604892v1-learning-to-plan-via-supervised-contrastive-learning-and-strategic-i.html" onclick="toggleFavorite(this, '2506.04892v1', 'Learning to Plan via Supervised Contrastive Learning and Strategic Interpolation: A Chess Case Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>57</td>
  <td><a href="./papers/250604755v1-truth-in-the-few-high-value-data-selection-for-efficient-multi-modal.html">Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning</a></td>
  <td>æå‡ºRAPæ–¹æ³•ä»¥é«˜æ•ˆé€‰æ‹©å¤šæ¨¡æ€æ¨ç†ä¸­çš„é«˜ä»·å€¼æ•°æ®</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04755v1" data-paper-url="./papers/250604755v1-truth-in-the-few-high-value-data-selection-for-efficient-multi-modal.html" onclick="toggleFavorite(this, '2506.04755v1', 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>58</td>
  <td><a href="./papers/250605431v1-robustness-evaluation-for-video-models-with-reinforcement-learning.html">Robustness Evaluation for Video Models with Reinforcement Learning</a></td>
  <td>æå‡ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è¯„ä¼°è§†é¢‘æ¨¡å‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05431v1" data-paper-url="./papers/250605431v1-robustness-evaluation-for-video-models-with-reinforcement-learning.html" onclick="toggleFavorite(this, '2506.05431v1', 'Robustness Evaluation for Video Models with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>59</td>
  <td><a href="./papers/250604559v2-reasoning-aligned-perception-decoupling-for-scalable-multi-modal-rea.html">Reasoning-Aligned Perception Decoupling for Scalable Multi-modal Reasoning</a></td>
  <td>æå‡ºæ„ŸçŸ¥-æ¨ç†è§£è€¦ä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†çš„å¯æ‰©å±•æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04559v2" data-paper-url="./papers/250604559v2-reasoning-aligned-perception-decoupling-for-scalable-multi-modal-rea.html" onclick="toggleFavorite(this, '2506.04559v2', 'Reasoning-Aligned Perception Decoupling for Scalable Multi-modal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>60</td>
  <td><a href="./papers/250605466v2-towards-reliable-identification-of-diffusion-based-image-manipulatio.html">Towards Reliable Identification of Diffusion-based Image Manipulations</a></td>
  <td>æå‡ºRADARä»¥è§£å†³åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¯¡æ”¹è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05466v2" data-paper-url="./papers/250605466v2-towards-reliable-identification-of-diffusion-based-image-manipulatio.html" onclick="toggleFavorite(this, '2506.05466v2', 'Towards Reliable Identification of Diffusion-based Image Manipulations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>61</td>
  <td><a href="./papers/250605217v1-dsg-world-learning-a-3d-gaussian-world-model-from-dual-state-videos.html">DSG-World: Learning a 3D Gaussian World Model from Dual State Videos</a></td>
  <td>æå‡ºDSG-Worldä»¥è§£å†³3Dä¸–ç•Œå»ºæ¨¡ä¸­çš„ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05217v1" data-paper-url="./papers/250605217v1-dsg-world-learning-a-3d-gaussian-world-model-from-dual-state-videos.html" onclick="toggleFavorite(this, '2506.05217v1', 'DSG-World: Learning a 3D Gaussian World Model from Dual State Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>62</td>
  <td><a href="./papers/250605119v1-practical-manipulation-model-for-robust-deepfake-detection.html">Practical Manipulation Model for Robust Deepfake Detection</a></td>
  <td>æå‡ºå®ç”¨æ“æ§æ¨¡å‹ä»¥å¢å¼ºæ·±ä¼ªæ£€æµ‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05119v1" data-paper-url="./papers/250605119v1-practical-manipulation-model-for-robust-deepfake-detection.html" onclick="toggleFavorite(this, '2506.05119v1', 'Practical Manipulation Model for Robust Deepfake Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>63</td>
  <td><a href="./papers/250604951v1-robustness-as-architecture-designing-iqa-models-to-withstand-adversa.html">Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations</a></td>
  <td>æå‡ºåŸºäºæ¶æ„è®¾è®¡çš„IQæ¨¡å‹ä»¥å¢å¼ºé²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04951v1" data-paper-url="./papers/250604951v1-robustness-as-architecture-designing-iqa-models-to-withstand-adversa.html" onclick="toggleFavorite(this, '2506.04951v1', 'Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>64</td>
  <td><a href="./papers/250604606v1-smartavatar-text-and-image-guided-human-avatar-generation-with-vlm-a.html">SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents</a></td>
  <td>æå‡ºSmartAvatarä»¥è§£å†³3Däººç±»å¤´åƒç”Ÿæˆçš„ç²¾ç¡®æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04606v1" data-paper-url="./papers/250604606v1-smartavatar-text-and-image-guided-human-avatar-generation-with-vlm-a.html" onclick="toggleFavorite(this, '2506.04606v1', 'SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>65</td>
  <td><a href="./papers/250605287v1-eoc-bench-can-mllms-identify-recall-and-forecast-objects-in-an-egoce.html">EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?</a></td>
  <td>æå‡ºEOC-Benchä»¥è§£å†³åŠ¨æ€è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05287v1" data-paper-url="./papers/250605287v1-eoc-bench-can-mllms-identify-recall-and-forecast-objects-in-an-egoce.html" onclick="toggleFavorite(this, '2506.05287v1', 'EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>66</td>
  <td><a href="./papers/250605336v2-videomolmo-spatio-temporal-grounding-meets-pointing.html">VideoMolmo: Spatio-Temporal Grounding Meets Pointing</a></td>
  <td>æå‡ºVideoMolmoä»¥è§£å†³è§†é¢‘æ—¶ç©ºå®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05336v2" data-paper-url="./papers/250605336v2-videomolmo-spatio-temporal-grounding-meets-pointing.html" onclick="toggleFavorite(this, '2506.05336v2', 'VideoMolmo: Spatio-Temporal Grounding Meets Pointing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>67</td>
  <td><a href="./papers/250605332v2-unleashing-hour-scale-video-training-for-long-video-language-underst.html">Unleashing Hour-Scale Video Training for Long Video-Language Understanding</a></td>
  <td>æå‡ºVideoMarathonæ•°æ®é›†ä»¥è§£å†³é•¿è§†é¢‘è¯­è¨€ç†è§£è®­ç»ƒä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05332v2" data-paper-url="./papers/250605332v2-unleashing-hour-scale-video-training-for-long-video-language-underst.html" onclick="toggleFavorite(this, '2506.05332v2', 'Unleashing Hour-Scale Video Training for Long Video-Language Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>68</td>
  <td><a href="./papers/250605554v1-ex-4d-extreme-viewpoint-4d-video-synthesis-via-depth-watertight-mesh.html">EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh</a></td>
  <td>æå‡ºEX-4Dä»¥è§£å†³æç«¯è§†è§’è§†é¢‘åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05554v1" data-paper-url="./papers/250605554v1-ex-4d-extreme-viewpoint-4d-video-synthesis-via-depth-watertight-mesh.html" onclick="toggleFavorite(this, '2506.05554v1', 'EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>69</td>
  <td><a href="./papers/250605207v2-follow-your-motion-video-motion-transfer-via-efficient-spatial-tempo.html">Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</a></td>
  <td>æå‡ºFollow-Your-Motionä»¥è§£å†³è§†é¢‘è¿åŠ¨è½¬ç§»ä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05207v2" data-paper-url="./papers/250605207v2-follow-your-motion-video-motion-transfer-via-efficient-spatial-tempo.html" onclick="toggleFavorite(this, '2506.05207v2', 'Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)