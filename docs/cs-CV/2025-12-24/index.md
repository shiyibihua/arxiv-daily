---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-24
---

# cs.CVï¼ˆ2025-12-24ï¼‰

ğŸ“Š å…± **25** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251220907v1-panogrounder-bridging-2d-and-3d-with-panoramic-scene-representations.html">PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding</a></td>
  <td>PanoGrounderï¼šåˆ©ç”¨å…¨æ™¯åœºæ™¯è¡¨ç¤ºæ¡¥æ¥2Då’Œ3Dï¼Œå®ç°åŸºäºVLMçš„3Dè§†è§‰å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20907v1" data-paper-url="./papers/251220907v1-panogrounder-bridging-2d-and-3d-with-panoramic-scene-representations.html" onclick="toggleFavorite(this, '2512.20907v1', 'PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251221334v1-streaming-video-instruction-tuning.html">Streaming Video Instruction Tuning</a></td>
  <td>æå‡ºStreamoï¼Œä¸€ä¸ªç”¨äºå®æ—¶æµè§†é¢‘ç†è§£çš„é€šç”¨äº¤äº’å¼åŠ©æ‰‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21334v1" data-paper-url="./papers/251221334v1-streaming-video-instruction-tuning.html" onclick="toggleFavorite(this, '2512.21334v1', 'Streaming Video Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251221135v1-tgc-net-a-structure-aware-and-semantically-aligned-framework-for-tex.html">TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation</a></td>
  <td>æå‡ºTGC-Netä»¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ–‡æœ¬å¼•å¯¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21135v1" data-paper-url="./papers/251221135v1-tgc-net-a-structure-aware-and-semantically-aligned-framework-for-tex.html" onclick="toggleFavorite(this, '2512.21135v1', 'TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251221333v1-fast-sam2-with-text-driven-token-pruning.html">Fast SAM2 with Text-Driven Token Pruning</a></td>
  <td>æå‡ºåŸºäºæ–‡æœ¬é©±åŠ¨çš„tokenå‰ªæFast SAM2ï¼ŒåŠ é€Ÿè§†é¢‘åˆ†å‰²å¹¶é™ä½èµ„æºæ¶ˆè€—ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21333v1" data-paper-url="./papers/251221333v1-fast-sam2-with-text-driven-token-pruning.html" onclick="toggleFavorite(this, '2512.21333v1', 'Fast SAM2 with Text-Driven Token Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251221221v1-leveraging-lightweight-entity-extraction-for-scalable-event-based-im.html">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a></td>
  <td>æå‡ºåŸºäºäº‹ä»¶ä¸­å¿ƒå®ä½“æå–çš„ä¸¤é˜¶æ®µå›¾åƒæ£€ç´¢æ–¹æ³•ï¼Œæå‡å¤æ‚åœºæ™¯ä¸‹çš„æ£€ç´¢ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21221v1" data-paper-url="./papers/251221221v1-leveraging-lightweight-entity-extraction-for-scalable-event-based-im.html" onclick="toggleFavorite(this, '2512.21221v1', 'Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251221218v1-latent-implicit-visual-reasoning.html">Latent Implicit Visual Reasoning</a></td>
  <td>æå‡ºLatent Implicit Visual Reasoningï¼Œæ— éœ€æ˜¾å¼ç›‘ç£å³å¯æå‡LMMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21218v1" data-paper-url="./papers/251221218v1-latent-implicit-visual-reasoning.html" onclick="toggleFavorite(this, '2512.21218v1', 'Latent Implicit Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251221094v1-t2av-compass-towards-unified-evaluation-for-text-to-audio-video-gene.html">T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation</a></td>
  <td>æå‡ºT2AV-Compassï¼Œç”¨äºç»Ÿä¸€è¯„ä¼°æ–‡æœ¬åˆ°éŸ³è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21094v1" data-paper-url="./papers/251221094v1-t2av-compass-towards-unified-evaluation-for-text-to-audio-video-gene.html" onclick="toggleFavorite(this, '2512.21094v1', 'T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251220936v1-reasoning-driven-amodal-completion-collaborative-agents-and-perceptu.html">Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation</a></td>
  <td>æå‡ºåŸºäºååŒå¤šæ™ºèƒ½ä½“æ¨ç†çš„éæ¨¡æ€è¡¥å…¨æ¡†æ¶ï¼Œè§£å†³è¯­ä¹‰ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20936v1" data-paper-url="./papers/251220936v1-reasoning-driven-amodal-completion-collaborative-agents-and-perceptu.html" onclick="toggleFavorite(this, '2512.20936v1', 'Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251220892v1-beyond-weight-adaptation-feature-space-domain-injection-for-cross-mo.html">Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification</a></td>
  <td>æå‡ºé¢†åŸŸè¡¨ç¤ºæ³¨å…¥æ–¹æ³•ä»¥è§£å†³è·¨æ¨¡æ€èˆ¹èˆ¶å†è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20892v1" data-paper-url="./papers/251220892v1-beyond-weight-adaptation-feature-space-domain-injection-for-cross-mo.html" onclick="toggleFavorite(this, '2512.20892v1', 'Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/251221064v1-multimodal-skeleton-based-action-representation-learning-via-decompo.html">Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition</a></td>
  <td>æå‡ºåˆ†è§£ä¸ç»„åˆçš„å¤šæ¨¡æ€éª¨éª¼åŠ¨ä½œè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæå‡æ•ˆç‡ä¸æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21064v1" data-paper-url="./papers/251221064v1-multimodal-skeleton-based-action-representation-learning-via-decompo.html" onclick="toggleFavorite(this, '2512.21064v1', 'Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251221237v1-segmo-segment-aligned-text-to-3d-human-motion-generation.html">SegMo: Segment-aligned Text to 3D Human Motion Generation</a></td>
  <td>æå‡ºSegMoæ¡†æ¶ï¼Œé€šè¿‡å¯¹é½æ–‡æœ¬å’Œè¿åŠ¨ç‰‡æ®µå®ç°æ›´ç²¾ç»†çš„æ–‡æœ¬é©±åŠ¨3Däººä½“åŠ¨ä½œç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21237v1" data-paper-url="./papers/251221237v1-segmo-segment-aligned-text-to-3d-human-motion-generation.html" onclick="toggleFavorite(this, '2512.21237v1', 'SegMo: Segment-aligned Text to 3D Human Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251221284v1-surgical-scene-segmentation-using-a-spike-driven-video-transformer-w.html">Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential</a></td>
  <td>æå‡ºSpikeSurgSegï¼Œä¸€ç§åŸºäºè„‰å†²ç¥ç»ç½‘ç»œçš„è§†é¢‘Transformerï¼Œç”¨äºå®æ—¶æ‰‹æœ¯åœºæ™¯åˆ†å‰²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21284v1" data-paper-url="./papers/251221284v1-surgical-scene-segmentation-using-a-spike-driven-video-transformer-w.html" onclick="toggleFavorite(this, '2512.21284v1', 'Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251221331v1-ticon-a-slide-level-tile-contextualizer-for-histopathology-represent.html">TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</a></td>
  <td>TICONï¼šä¸€ç§ç”¨äºç»„ç»‡ç—…ç†å­¦è¡¨å¾å­¦ä¹ çš„åˆ‡ç‰‡çº§ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21331v1" data-paper-url="./papers/251221331v1-ticon-a-slide-level-tile-contextualizer-for-histopathology-represent.html" onclick="toggleFavorite(this, '2512.21331v1', 'TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251221004v1-learning-from-next-frame-prediction-autoregressive-video-modeling-en.html">Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations</a></td>
  <td>NExT-Vidï¼šæå‡ºåŸºäºä¸‹ä¸€å¸§é¢„æµ‹çš„è‡ªå›å½’è§†é¢‘å»ºæ¨¡æ¡†æ¶ï¼Œæå‡è§†é¢‘è¡¨å¾å­¦ä¹ æ•ˆæœã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">visual pre-training</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21004v1" data-paper-url="./papers/251221004v1-learning-from-next-frame-prediction-autoregressive-video-modeling-en.html" onclick="toggleFavorite(this, '2512.21004v1', 'Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251220921v1-self-supervised-multiplex-consensus-mamba-for-general-image-fusion.html">Self-supervised Multiplex Consensus Mamba for General Image Fusion</a></td>
  <td>æå‡ºSMC-Mambaæ¡†æ¶ï¼Œç”¨äºé€šç”¨å›¾åƒèåˆï¼Œæå‡å¤šç§èåˆä»»åŠ¡æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20921v1" data-paper-url="./papers/251220921v1-self-supervised-multiplex-consensus-mamba-for-general-image-fusion.html" onclick="toggleFavorite(this, '2512.20921v1', 'Self-supervised Multiplex Consensus Mamba for General Image Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251220988v1-pufm-point-cloud-upsampling-via-enhanced-flow-matching.html">PUFM++: Point Cloud Upsampling via Enhanced Flow Matching</a></td>
  <td>æå‡ºPUFM++ä»¥è§£å†³ç¨€ç–ç‚¹äº‘ä¸Šé‡‡æ ·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20988v1" data-paper-url="./papers/251220988v1-pufm-point-cloud-upsampling-via-enhanced-flow-matching.html" onclick="toggleFavorite(this, '2512.20988v1', 'PUFM++: Point Cloud Upsampling via Enhanced Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251220976v1-xgrid-mapping-explicit-implicit-hybrid-grid-submaps-for-efficient-in.html">XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping</a></td>
  <td>æå‡ºXGrid-Mappingï¼Œåˆ©ç”¨æ˜¾éšæ··åˆç½‘æ ¼å­å›¾å®ç°é«˜æ•ˆå¢é‡å¼ç¥ç»æ¿€å…‰é›·è¾¾å»ºå›¾</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">implicit representation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20976v1" data-paper-url="./papers/251220976v1-xgrid-mapping-explicit-implicit-hybrid-grid-submaps-for-efficient-in.html" onclick="toggleFavorite(this, '2512.20976v1', 'XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251220927v1-quantile-rendering-efficiently-embedding-high-dimensional-feature-on.html">Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting</a></td>
  <td>æå‡ºQuantile Renderingï¼Œé«˜æ•ˆåµŒå…¥é«˜ç»´ç‰¹å¾äº3Dé«˜æ–¯æº…å°„ï¼Œæå‡å¼€æ”¾è¯æ±‡åˆ†å‰²æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20927v1" data-paper-url="./papers/251220927v1-quantile-rendering-efficiently-embedding-high-dimensional-feature-on.html" onclick="toggleFavorite(this, '2512.20927v1', 'Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251221150v1-orca-object-recognition-and-comprehension-for-archiving-marine-speci.html">ORCA: Object Recognition and Comprehension for Archiving Marine Species</a></td>
  <td>ORCAï¼šç”¨äºæµ·æ´‹ç‰©ç§å­˜æ¡£çš„ç›®æ ‡è¯†åˆ«ä¸ç†è§£å¤šæ¨¡æ€åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21150v1" data-paper-url="./papers/251221150v1-orca-object-recognition-and-comprehension-for-archiving-marine-speci.html" onclick="toggleFavorite(this, '2512.21150v1', 'ORCA: Object Recognition and Comprehension for Archiving Marine Species')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251221053v1-optical-flow-guided-6dof-object-pose-tracking-with-an-event-camera.html">Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera</a></td>
  <td>æå‡ºåŸºäºå…‰æµå¼•å¯¼çš„äº‹ä»¶ç›¸æœº6DoFç‰©ä½“å§¿æ€è·Ÿè¸ªæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21053v1" data-paper-url="./papers/251221053v1-optical-flow-guided-6dof-object-pose-tracking-with-an-event-camera.html" onclick="toggleFavorite(this, '2512.21053v1', 'Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251221183v1-towards-arbitrary-motion-completing-via-hierarchical-continuous-repr.html">Towards Arbitrary Motion Completing via Hierarchical Continuous Representation</a></td>
  <td>æå‡ºåŸºäºåˆ†å±‚è¿ç»­è¡¨ç¤ºçš„NAMEæ¡†æ¶ï¼Œå®ç°ä»»æ„å¸§ç‡çš„è¿åŠ¨è¡¥å…¨</td>
  <td class="tags-cell"><span class="paper-tag">implicit representation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21183v1" data-paper-url="./papers/251221183v1-towards-arbitrary-motion-completing-via-hierarchical-continuous-repr.html" onclick="toggleFavorite(this, '2512.21183v1', 'Towards Arbitrary Motion Completing via Hierarchical Continuous Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251221078v1-unipr-3d-towards-universal-visual-place-recognition-with-visual-geom.html">UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</a></td>
  <td>æå‡ºUniPR-3Dï¼Œåˆ©ç”¨è§†è§‰å‡ ä½•Transformerå®ç°é€šç”¨è§†è§‰å®šä½è¯†åˆ«ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">VGGT</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21078v1" data-paper-url="./papers/251221078v1-unipr-3d-towards-universal-visual-place-recognition-with-visual-geom.html" onclick="toggleFavorite(this, '2512.21078v1', 'UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/251220898v1-dgsan-dual-graph-spatiotemporal-attention-network-for-pulmonary-nodu.html">DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction</a></td>
  <td>æå‡ºåŒå›¾æ—¶ç©ºæ³¨æ„åŠ›ç½‘ç»œä»¥æé«˜è‚ºç»“èŠ‚æ¶æ€§é¢„æµ‹å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">mutual attention</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.20898v1" data-paper-url="./papers/251220898v1-dgsan-dual-graph-spatiotemporal-attention-network-for-pulmonary-nodu.html" onclick="toggleFavorite(this, '2512.20898v1', 'DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/251221209v1-human-motion-estimation-with-everyday-wearables.html">Human Motion Estimation with Everyday Wearables</a></td>
  <td>EveryWearï¼šåˆ©ç”¨æ—¥å¸¸å¯ç©¿æˆ´è®¾å¤‡è¿›è¡Œè½»é‡çº§ã€å…æ ‡å®šçš„å…¨èº«äººä½“è¿åŠ¨ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">teacher-student</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21209v1" data-paper-url="./papers/251221209v1-human-motion-estimation-with-everyday-wearables.html" onclick="toggleFavorite(this, '2512.21209v1', 'Human Motion Estimation with Everyday Wearables')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/251221268v1-acd-direct-conditional-control-for-video-diffusion-models-via-attent.html">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a></td>
  <td>ACDï¼šé€šè¿‡æ³¨æ„åŠ›ç›‘ç£å®ç°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç›´æ¥æ¡ä»¶æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.21268v1" data-paper-url="./papers/251221268v1-acd-direct-conditional-control-for-video-diffusion-models-via-attent.html" onclick="toggleFavorite(this, '2512.21268v1', 'ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)