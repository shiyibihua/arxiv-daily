---
layout: default
title: "Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification"
---

# Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20892" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20892v1</a>
  <a href="https://arxiv.org/pdf/2512.20892.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20892v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20892v1', 'Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Zhelin Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-24

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TingfengXian/DRI)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢†åŸŸè¡¨ç¤ºæ³¨å…¥æ–¹æ³•ä»¥è§£å†³è·¨æ¨¡æ€èˆ¹èˆ¶å†è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨æ¨¡æ€å†è¯†åˆ«` `é¢†åŸŸè¡¨ç¤ºæ³¨å…¥` `è§†è§‰åŸºç¡€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `æµ·æ´‹ç›®æ ‡è·Ÿè¸ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è·¨æ¨¡æ€èˆ¹èˆ¶å†è¯†åˆ«æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®é›†ï¼Œå¯¼è‡´æ¨¡æ€å¯¹é½æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºé¢†åŸŸè¡¨ç¤ºæ³¨å…¥ï¼ˆDRIï¼‰ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å¾ç©ºé—´ä¼˜åŒ–è€Œéæƒé‡ç©ºé—´ï¼Œæå‡è·¨æ¨¡æ€å†è¯†åˆ«æ€§èƒ½ã€‚
3. åœ¨HOSS-ReIDæ•°æ®é›†ä¸Šï¼ŒDRIæ–¹æ³•ä»¥æœ€å°‘çš„å¯è®­ç»ƒå‚æ•°å®ç°äº†SOTAæ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†è¯†åˆ«ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è·¨æ¨¡æ€èˆ¹èˆ¶å†è¯†åˆ«ï¼ˆCMS Re-IDï¼‰å¯¹å…¨å¤©å€™æµ·æ´‹ç›®æ ‡è·Ÿè¸ªè‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ˜¾è‘—çš„æ¨¡æ€å·®å¼‚æŒ‘æˆ˜ã€‚ç°æœ‰ä¸»æµè§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–äºæ˜¾å¼æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œç„¶è€Œè¿™ç§æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ„å»ºå¤§è§„æ¨¡é…å¯¹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŸºäºæŸæ‹‰å›¾è¡¨å¾å‡è®¾ï¼Œæœ¬æ–‡æ¢è®¨äº†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰åœ¨å¼¥åˆæ¨¡æ€å·®è·æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºé¢†åŸŸè¡¨ç¤ºæ³¨å…¥ï¼ˆDRIï¼‰ï¼Œé€šè¿‡ä¿æŒVFMå®Œå…¨å†»ç»“ï¼Œè®¾è®¡è½»é‡çº§å¯å­¦ä¹ çš„åç§»ç¼–ç å™¨ï¼Œä»åŸå§‹è¾“å…¥ä¸­æå–ä¸°å¯Œçš„æ¨¡æ€å’Œèº«ä»½å±æ€§çš„é¢†åŸŸç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HOSS-ReIDæ•°æ®é›†ä¸Šå®ç°äº†57.9%å’Œ60.5%çš„mAPï¼Œå‚æ•°é‡ä»…ä¸º1.54Må’Œ7.05Mã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è·¨æ¨¡æ€èˆ¹èˆ¶å†è¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯¼è‡´åœ¨å°å‹æ¨¡å‹ä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé¢†åŸŸè¡¨ç¤ºæ³¨å…¥ï¼ˆDRIï¼‰ç­–ç•¥ï¼Œè½¬å‘ç‰¹å¾ç©ºé—´çš„ä¼˜åŒ–ï¼Œé€šè¿‡å†»ç»“VFMæ¥ä¿ç•™é€šç”¨çŸ¥è¯†ï¼ŒåŒæ—¶æå–é¢†åŸŸç‰¹å®šçš„è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªåç§»ç¼–ç å™¨å’Œä¸€ä¸ªè°ƒåˆ¶å™¨ã€‚åç§»ç¼–ç å™¨ä»åŸå§‹è¾“å…¥ä¸­æå–é¢†åŸŸç‰¹å¾ï¼Œè°ƒåˆ¶å™¨æ ¹æ®ä¸­é—´ç‰¹å¾çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œè‡ªé€‚åº”è½¬æ¢ï¼Œæœ€åé€šè¿‡åŠ æ³•èåˆå°†è¿™äº›è¡¨ç¤ºæ³¨å…¥åˆ°ä¸­é—´å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šDRIæ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡ç‰¹å¾ç©ºé—´çš„åŠ¨æ€é‡å¡‘æ¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œä¸æ”¹å˜VFMçš„é¢„è®­ç»ƒæƒé‡ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šæˆ‘ä»¬è®¾è®¡äº†è½»é‡çº§çš„åç§»ç¼–ç å™¨å’Œè°ƒåˆ¶å™¨ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é¢†åŸŸç‰¹å¾çš„æå–ä¸èåˆï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ã€‚é€šè¿‡ç²¾ç®€çš„å‚æ•°è®¾ç½®ï¼ŒDRIæ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20892v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20892v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20892v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDRIæ–¹æ³•åœ¨HOSS-ReIDæ•°æ®é›†ä¸Šå–å¾—äº†57.9%å’Œ60.5%çš„mAPï¼Œåˆ†åˆ«ä½¿ç”¨äº†1.54Må’Œ7.05Mçš„å‚æ•°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å‚æ•°æ•ˆç‡å’Œè¯†åˆ«ç²¾åº¦ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æµ·æ´‹ç›‘æµ‹ã€èˆ¹èˆ¶è¯†åˆ«å’Œæ™ºèƒ½äº¤é€šç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è·¨æ¨¡æ€å†è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå…¨å¤©å€™çš„æµ·æ´‹ç›®æ ‡è·Ÿè¸ªï¼Œæå‡æµ·æ´‹å®‰å…¨å’Œç®¡ç†æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•è‡³å…¶ä»–å¤šæ¨¡æ€è¯†åˆ«ä»»åŠ¡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

