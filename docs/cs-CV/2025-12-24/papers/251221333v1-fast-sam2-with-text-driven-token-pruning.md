---
layout: default
title: Fast SAM2 with Text-Driven Token Pruning
---

# Fast SAM2 with Text-Driven Token Pruning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.21333" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.21333v1</a>
  <a href="https://arxiv.org/pdf/2512.21333.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.21333v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.21333v1', 'Fast SAM2 with Text-Driven Token Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-24

**å¤‡æ³¨**: 28 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ–‡æœ¬é©±åŠ¨çš„tokenå‰ªæFast SAM2ï¼ŒåŠ é€Ÿè§†é¢‘åˆ†å‰²å¹¶é™ä½èµ„æºæ¶ˆè€—ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘åˆ†å‰²` `tokenå‰ªæ` `æ–‡æœ¬å¼•å¯¼` `æ¨¡å‹åŠ é€Ÿ` `SAM2`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. SAM2åœ¨è§†é¢‘åˆ†å‰²ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¤„ç†å¯†é›†è§†è§‰tokenå¯¼è‡´è®¡ç®—å’Œå†…å­˜æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å®é™…éƒ¨ç½²ã€‚
2. è¯¥æ–‡æå‡ºæ–‡æœ¬å¼•å¯¼çš„tokenå‰ªææ¡†æ¶ï¼Œåœ¨æ—¶åºä¼ æ’­å‰é€‰æ‹©æ€§å‡å°‘tokenå¯†åº¦ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒåˆ†å‰²ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦å¹¶é™ä½GPUå†…å­˜ä½¿ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬å¼•å¯¼çš„tokenå‰ªææ¡†æ¶ï¼Œæ—¨åœ¨æé«˜Segment Anything Model 2 (SAM2) åœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²ä¸­çš„æ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨è§†è§‰ç¼–ç åã€æ—¶åºä¼ æ’­å‰é€‰æ‹©æ€§åœ°å‡å°‘tokenå¯†åº¦ï¼Œæ— éœ€ä¿®æ”¹åº•å±‚åˆ†å‰²æ¶æ„ã€‚é€šè¿‡è½»é‡çº§çš„è·¯ç”±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•èåˆå±€éƒ¨è§†è§‰ä¸Šä¸‹æ–‡ã€æ¥è‡ªå¯¹è±¡ä¸­å¿ƒæ–‡æœ¬æè¿°çš„è¯­ä¹‰ç›¸å…³æ€§ä»¥åŠä¸ç¡®å®šæ€§çº¿ç´¢æ¥è¯„ä¼°tokençš„é‡è¦æ€§ã€‚ä»…ä¿ç•™ä¿¡æ¯é‡æœ€å¤§çš„tokenç”¨äºä¸‹æ¸¸å¤„ç†ï¼Œä»è€Œå‡å°‘å†—ä½™è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒåˆ†å‰²ç²¾åº¦ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœªå‰ªæçš„SAM2åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†é«˜è¾¾42.50%çš„æ¨ç†é€Ÿåº¦æå‡å’Œ37.41%çš„GPUå†…å­˜ä½¿ç”¨é‡é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„Jå’ŒFæ€§èƒ½ã€‚è¿™çªæ˜¾äº†æ—©æœŸtokené€‰æ‹©åœ¨æé«˜åŸºäºTransformerçš„è§†é¢‘åˆ†å‰²ç³»ç»Ÿåœ¨å®æ—¶å’Œèµ„æºå—é™åº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šSAM2åœ¨è§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œéœ€è¦å¤„ç†å¤§é‡çš„è§†è§‰tokensï¼Œå¯¼è‡´è®¡ç®—å’Œå†…å­˜å¼€é”€å·¨å¤§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼ŒäºŒæ¬¡æ–¹çº§åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—èµ„æºæ¶ˆè€—æ›´åŠ ä¸¥é‡ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ç›´æ¥å°†æ‰€æœ‰è§†è§‰tokensä¼ é€’åˆ°ä¸‹æ¸¸æ¨¡å—ï¼Œè€Œå¿½ç•¥äº†å…¶ä¸­å¯èƒ½å­˜åœ¨å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†SAM2åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨è§†è§‰ç¼–ç åï¼Œæ—¶åºä¼ æ’­å‰ï¼Œå¯¹è§†è§‰tokensè¿›è¡Œå‰ªæï¼Œåªä¿ç•™å¯¹ç›®æ ‡å¯¹è±¡åˆ†å‰²æœ‰ç”¨çš„tokensã€‚é€šè¿‡å¼•å…¥æ–‡æœ¬ä¿¡æ¯ï¼Œå¼•å¯¼tokené€‰æ‹©ï¼Œä½¿å¾—å‰ªæè¿‡ç¨‹æ›´åŠ æ™ºèƒ½ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘è®¡ç®—é‡çš„åŒæ—¶ï¼Œä¿æŒåˆ†å‰²ç²¾åº¦ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹SAM2åº•å±‚æ¶æ„çš„ä¿®æ”¹ï¼Œæ˜“äºé›†æˆå’Œéƒ¨ç½²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šè§†è§‰ç¼–ç ã€tokenè·¯ç”±å’Œæ—¶åºä¼ æ’­ã€‚é¦–å…ˆï¼Œä½¿ç”¨SAM2çš„å›¾åƒç¼–ç å™¨æå–è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„è·¯ç”±æœºåˆ¶å¯¹tokensè¿›è¡Œæ’åºï¼Œè¯¥æœºåˆ¶èåˆäº†å±€éƒ¨è§†è§‰ä¸Šä¸‹æ–‡ã€æ–‡æœ¬è¯­ä¹‰ç›¸å…³æ€§å’Œä¸ç¡®å®šæ€§çº¿ç´¢ã€‚æœ€åï¼Œåªä¿ç•™æ’åé å‰çš„tokensè¿›è¡Œæ—¶åºä¼ æ’­å’Œåˆ†å‰²ã€‚æ–‡æœ¬ä¿¡æ¯å¯ä»¥æ˜¯ç”¨æˆ·æä¾›çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†æ–‡æœ¬ä¿¡æ¯æ¥æŒ‡å¯¼tokenå‰ªæã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§†è§‰ç‰¹å¾çš„tokené€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼Œæ–‡æœ¬ä¿¡æ¯èƒ½å¤Ÿæä¾›æ›´é«˜çº§åˆ«çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä½¿å¾—å‰ªæè¿‡ç¨‹æ›´åŠ å…³æ³¨ç›®æ ‡å¯¹è±¡ï¼Œä»è€Œåœ¨å‡å°‘è®¡ç®—é‡çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ä¿æŒåˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜è€ƒè™‘äº†ä¸ç¡®å®šæ€§çº¿ç´¢ï¼Œä¿ç•™äº†è¾¹ç•ŒåŒºåŸŸçš„tokensï¼Œè¿›ä¸€æ­¥æå‡äº†åˆ†å‰²çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼štokenè·¯ç”±æœºåˆ¶æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒã€‚è¯¥æœºåˆ¶ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œæ¥é¢„æµ‹æ¯ä¸ªtokençš„é‡è¦æ€§å¾—åˆ†ã€‚è¾“å…¥åŒ…æ‹¬å±€éƒ¨è§†è§‰ç‰¹å¾ã€æ–‡æœ¬åµŒå…¥å’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å±€éƒ¨è§†è§‰ç‰¹å¾é€šè¿‡å·ç§¯æ“ä½œæå–ï¼Œæ–‡æœ¬åµŒå…¥é€šè¿‡é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨è·å¾—ï¼Œä¸ç¡®å®šæ€§ä¼°è®¡é€šè¿‡è®¡ç®—tokenç‰¹å¾çš„æ–¹å·®å¾—åˆ°ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¿ç•™çš„tokensä¸ç›®æ ‡å¯¹è±¡ä¹‹é—´çš„IoUã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21333v1/tokens_1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21333v1/miansam.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.21333v1/architechture_1.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœªå‰ªæçš„SAM2åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†é«˜è¾¾42.50%ï¼ŒGPUå†…å­˜ä½¿ç”¨é‡é™ä½äº†37.41%ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„Jå’ŒFæ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§†é¢‘åˆ†å‰²åŠ é€Ÿæ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå®æ—¶è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œä½¿å¾—SAM2èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œä»è€Œæ‰©å±•äº†å…¶åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè§†é¢‘ç¼–è¾‘ã€å†…å®¹åˆ›ä½œç­‰é¢†åŸŸï¼Œæé«˜è§†é¢‘å¤„ç†çš„æ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

