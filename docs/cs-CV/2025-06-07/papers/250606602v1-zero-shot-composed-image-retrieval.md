---
layout: default
title: Zero Shot Composed Image Retrieval
---

# Zero Shot Composed Image Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06602" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06602v1</a>
  <a href="https://arxiv.org/pdf/2506.06602.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06602v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06602v1', 'Zero Shot Composed Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Santhosh Kakarla, Gautama Shastry Bulusu Venkata

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-07

**å¤‡æ³¨**: 8 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºBLIP-2çš„é›¶-shotå¤åˆå›¾åƒæ£€ç´¢æ–¹æ³•ä»¥æå‡æ£€ç´¢æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤åˆå›¾åƒæ£€ç´¢` `é›¶-shotå­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `BLIP-2` `Q-Former` `ç‰¹å¾èåˆ` `æ£€ç´¢æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶-shotå¤åˆå›¾åƒæ£€ç´¢æ–¹æ³•åœ¨FashionIQåŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œä»…è¾¾åˆ°20-25%çš„Recall@10ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ç“¶é¢ˆã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¾®è°ƒBLIP-2ä¸Q-Formerçš„ç»“åˆï¼Œè¿›è¡Œè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„æœ‰æ•ˆèåˆï¼Œä»è€Œæå‡æ£€ç´¢æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æ”¹è¿›åï¼ŒRecall@10åœ¨ä¸åŒç±»åˆ«ä¸Šæ˜¾è‘—æå‡ï¼Œå¹³å‡Recall@50ä¹Ÿæœ‰æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰å…è®¸ç”¨æˆ·é€šè¿‡å¯¹å‚è€ƒå›¾åƒè¿›è¡Œç»†ç²’åº¦æ–‡æœ¬ç¼–è¾‘æ¥å®šä½ç›®æ ‡å›¾åƒã€‚é›¶-shot CIRåœ¨FashionIQåŸºå‡†ä¸Šä»…è¾¾åˆ°20-25%çš„Recall@10ã€‚æœ¬æ–‡é€šè¿‡ä½¿ç”¨è½»é‡çº§çš„Q-Formerå¯¹BLIP-2è¿›è¡Œå¾®è°ƒï¼Œå°†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾èåˆä¸ºå•ä¸€åµŒå…¥ï¼ŒRecall@10æå‡è‡³45.6%ï¼ˆè¡¬è¡«ï¼‰ã€40.1%ï¼ˆè£™å­ï¼‰å’Œ50.4%ï¼ˆTæ¤ï¼‰ï¼Œå¹³å‡Recall@50æå‡è‡³67.6%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†Retrieval-DPOæ–¹æ³•ï¼Œä½†ç”±äºç¼ºä¹å›¾åƒ-æ–‡æœ¬è”åˆèåˆç­‰é—®é¢˜ï¼Œå…¶æ€§èƒ½è¿œä½äºé›¶-shotå’Œæç¤ºè°ƒä¼˜åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é›¶-shotå¤åˆå›¾åƒæ£€ç´¢ä¸­ç°æœ‰æ–¹æ³•æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨FashionIQåŸºå‡†ä¸Šçš„ä½Recall@10è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç‹¬ç«‹çš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç‰¹å¾èåˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¾®è°ƒBLIP-2å¹¶å¼•å…¥è½»é‡çº§çš„Q-Formerï¼Œå°†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾èåˆä¸ºå•ä¸€åµŒå…¥ï¼Œä»¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°æ›´å¥½çš„å¤šæ¨¡æ€ç‰¹å¾æ•´åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯BLIP-2çš„å¾®è°ƒï¼Œéšåæ˜¯Q-Formerçš„å¼•å…¥ä»¥å®ç°è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„èåˆã€‚æ•´ä¸ªæµç¨‹ä»è¾“å…¥å›¾åƒå’Œæ–‡æœ¬å¼€å§‹ï¼Œé€šè¿‡ç¼–ç å™¨ç”ŸæˆåµŒå…¥ï¼Œæœ€åè¿›è¡Œæ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥Q-Formerè¿›è¡Œå›¾åƒå’Œæ–‡æœ¬çš„è”åˆåµŒå…¥ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾åˆ†ç¦»çš„é—®é¢˜ï¼Œä»è€Œæå‡äº†æ£€ç´¢æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡å¯¹BLIP-2è¿›è¡Œäº†ç»†è‡´çš„å¾®è°ƒï¼Œå¹¶è®¾è®¡äº†é€‚åº”äºç‰¹å®šä»»åŠ¡çš„æŸå¤±å‡½æ•°ã€‚æ­¤å¤–ï¼ŒRetrieval-DPOæ–¹æ³•çš„å®éªŒä¸­ï¼Œå°½ç®¡è¿›è¡Œäº†å¤šæ¬¡è°ƒä¼˜ï¼Œä½†ç”±äºç¼ºä¹æœ‰æ•ˆçš„è´Ÿæ ·æœ¬å’Œå›¾åƒ-æ–‡æœ¬èåˆï¼Œå¯¼è‡´å…¶æ€§èƒ½æœªèƒ½è¾¾åˆ°é¢„æœŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„BLIP-2ä¸Q-Formerçš„ç»“åˆï¼Œä½¿å¾—Recall@10åœ¨è¡¬è¡«ã€è£™å­å’ŒTæ¤ç±»åˆ«ä¸Šåˆ†åˆ«è¾¾åˆ°äº†45.6%ã€40.1%å’Œ50.4%ï¼Œå¹³å‡Recall@50æå‡è‡³67.6%ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„é›¶-shotå’Œæç¤ºè°ƒä¼˜åŸºçº¿ï¼Œæ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå­å•†åŠ¡ã€ç¤¾äº¤åª’ä½“å’Œæ•°å­—å†…å®¹ç®¡ç†ç­‰åœºæ™¯ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ–‡æœ¬æè¿°å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€çš„å›¾åƒï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨å¤šæ¨¡æ€æ£€ç´¢ã€æ™ºèƒ½æ¨èç³»ç»Ÿç­‰é¢†åŸŸå‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Composed image retrieval (CIR) allows a user to locate a target image by applying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove stripes'') to a reference image. Zero-shot CIR, which embeds the image and the text with separate pretrained vision-language encoders, reaches only 20-25\% Recall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2 with a lightweight Q-Former that fuses visual and textual features into a single embedding, raising Recall@10 to 45.6\% (shirt), 40.1\% (dress), and 50.4\% (top-tee) and increasing the average Recall@50 to 67.6\%. We also examine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct Preference Optimization loss applied to FAISS-mined hard negatives. Despite extensive tuning of the scaling factor, index, and sampling strategy, Retrieval-DPO attains only 0.02\% Recall@10 -- far below zero-shot and prompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii) uses a margin objective misaligned with top-$K$ metrics, (iii) relies on low-quality negatives, and (iv) keeps the vision and Transformer layers frozen. Our results show that effective preference-based CIR requires genuine multimodal fusion, ranking-aware objectives, and carefully curated negatives.

