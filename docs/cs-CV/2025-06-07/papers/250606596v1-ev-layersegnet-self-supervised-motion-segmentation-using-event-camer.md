---
layout: default
title: EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras
---

# EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06596" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06596v1</a>
  <a href="https://arxiv.org/pdf/2506.06596.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06596v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06596v1', 'EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youssef Farah, Federico Paredes-VallÃ©s, Guido De Croon, Muhammad Ahmed Humais, Hussain Sajwani, Yahya Zweiri

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-07

**å¤‡æ³¨**: This paper has been accepted for publication at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Nashville, 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEV-LayerSegNetä»¥è§£å†³äº‹ä»¶ç›¸æœºè¿åŠ¨åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `è¿åŠ¨åˆ†å‰²` `è‡ªç›‘ç£å­¦ä¹ ` `ä»¿å°„å…‰æµ` `å·ç§¯ç¥ç»ç½‘ç»œ` `å»æ¨¡ç³Šå¤„ç†` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨äº‹ä»¶ç›¸æœºè¿åŠ¨åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´çœŸå®æ ‡ç­¾è·å–å›°éš¾ã€æˆæœ¬é«˜å’Œé¢‘ç‡æœ‰é™ç­‰æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºEV-LayerSegNetï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ å®ç°ä»¿å°„å…‰æµå’Œåˆ†å‰²æ©ç çš„ç‹¬ç«‹å­¦ä¹ ï¼Œè¿›è€Œå¯¹è¾“å…¥äº‹ä»¶è¿›è¡Œå»æ¨¡ç³Šå¤„ç†ã€‚
3. åœ¨ä»…åŒ…å«ä»¿å°„è¿åŠ¨çš„æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼ŒEV-LayerSegNetå®ç°äº†71%çš„IoUå’Œ87%çš„æ£€æµ‹ç‡ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶ç›¸æœºæ˜¯ä¸€ç§æ–°å‹çš„ç”Ÿç‰©å¯å‘ä¼ æ„Ÿå™¨ï¼Œèƒ½å¤Ÿä»¥é«˜äºä¼ ç»Ÿç›¸æœºçš„æ—¶é—´åˆ†è¾¨ç‡æ•æ‰è¿åŠ¨åŠ¨æ€ã€‚ç„¶è€Œï¼Œè®­ç»ƒåŸºäºäº‹ä»¶çš„ç½‘ç»œä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè·å–çœŸå®æ ‡ç­¾æ—¢æ˜‚è´µåˆå®¹æ˜“å‡ºé”™ã€‚æœ¬æ–‡æå‡ºEV-LayerSegNetï¼Œä¸€ç§è‡ªç›‘ç£å·ç§¯ç¥ç»ç½‘ç»œï¼Œç”¨äºäº‹ä»¶åŸºç¡€çš„è¿åŠ¨åˆ†å‰²ã€‚é€šè¿‡åˆ†å±‚è¡¨ç¤ºåœºæ™¯åŠ¨æ€ï¼Œè®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ†åˆ«å­¦ä¹ ä»¿å°„å…‰æµå’Œåˆ†å‰²æ©ç ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯å¯¹è¾“å…¥äº‹ä»¶è¿›è¡Œå»æ¨¡ç³Šå¤„ç†ã€‚å»æ¨¡ç³Šè´¨é‡è¢«ç”¨ä½œè‡ªç›‘ç£å­¦ä¹ æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»…åŒ…å«ä»¿å°„è¿åŠ¨çš„æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼Œç½‘ç»œçš„IoUå’Œæ£€æµ‹ç‡åˆ†åˆ«è¾¾åˆ°äº†71%å’Œ87%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äº‹ä»¶ç›¸æœºåœ¨è¿åŠ¨åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´çš„çœŸå®æ ‡ç­¾è·å–å›°éš¾å’Œè®­ç»ƒæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–æ˜‚è´µä¸”ä¸å¯é çš„æ ‡ç­¾ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ†åˆ«å­¦ä¹ ä»¿å°„å…‰æµå’Œåˆ†å‰²æ©ç ï¼Œä»è€Œå®ç°å¯¹è¾“å…¥äº‹ä»¶çš„å»æ¨¡ç³Šå¤„ç†ã€‚è¿™ç§åˆ†å±‚è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åœºæ™¯åŠ¨æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEV-LayerSegNetçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä»¿å°„å…‰æµä¼°è®¡æ¨¡å—å’Œåˆ†å‰²æ©ç ç”Ÿæˆæ¨¡å—ã€‚è¾“å…¥äº‹ä»¶é¦–å…ˆç»è¿‡å…‰æµä¼°è®¡ï¼Œéšåç”Ÿæˆåˆ†å‰²æ©ç ï¼Œæœ€ååˆ©ç”¨å»æ¨¡ç³Šå¤„ç†æå‡è¾“å…¥äº‹ä»¶çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè‡ªç›‘ç£å­¦ä¹ æŸå¤±çš„è®¾è®¡ï¼Œé€šè¿‡å»æ¨¡ç³Šè´¨é‡ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œé¿å…äº†å¯¹çœŸå®æ ‡ç­¾çš„ä¾èµ–ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡å»æ¨¡ç³Šæ•ˆæœï¼Œå¹¶è®¾è®¡äº†é€‚åˆäº‹ä»¶æ•°æ®çš„å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEV-LayerSegNetåœ¨ä»…åŒ…å«ä»¿å°„è¿åŠ¨çš„æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼ŒIoUè¾¾åˆ°äº†71%ï¼Œæ£€æµ‹ç‡è¾¾åˆ°äº†87%ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è¿åŠ¨åˆ†å‰²ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰åŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰å’Œç›‘æ§ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿåˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡ç‰¹æ€§ï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°æ›´ç²¾ç¡®çš„è¿åŠ¨åˆ†å‰²ã€‚è¿™å°†æå¤§æå‡ç›¸å…³é¢†åŸŸçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå®æ—¶å¤„ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event cameras are novel bio-inspired sensors that capture motion dynamics with much higher temporal resolution than traditional cameras, since pixels react asynchronously to brightness changes. They are therefore better suited for tasks involving motion such as motion segmentation. However, training event-based networks still represents a difficult challenge, as obtaining ground truth is very expensive, error-prone and limited in frequency. In this article, we introduce EV-LayerSegNet, a self-supervised CNN for event-based motion segmentation. Inspired by a layered representation of the scene dynamics, we show that it is possible to learn affine optical flow and segmentation masks separately, and use them to deblur the input events. The deblurring quality is then measured and used as self-supervised learning loss. We train and test the network on a simulated dataset with only affine motion, achieving IoU and detection rate up to 71% and 87% respectively.

