---
layout: default
title: Scaling Learned Image Compression Models up to 1 Billion
---

# Scaling Learned Image Compression Models up to 1 Billion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09075" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09075v1</a>
  <a href="https://arxiv.org/pdf/2508.09075.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09075v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09075v1', 'Scaling Learned Image Compression Models up to 1 Billion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqi Li, Haotian Zhang, Li Li, Dong Liu, Feng Wu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 11 pages, technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤§è§„æ¨¡å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹ä»¥æå‡å‹ç¼©æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å­¦ä¹ å›¾åƒå‹ç¼©` `æ¨¡å‹æ‰©å±•` `å‹ç¼©æ€§èƒ½` `æ·±åº¦å­¦ä¹ ` `ç‡å¤±çœŸä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹è§„æ¨¡æœ‰é™ï¼Œå¯¼è‡´è¡¨ç¤ºèƒ½åŠ›ä¸è¶³ï¼Œå½±å“å‹ç¼©æ€§èƒ½çš„æå‡ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ‰©å±•æ¨¡å‹å‚æ•°è§„æ¨¡ï¼Œæ¢ç´¢æ¨¡å‹å¤§å°ä¸å‹ç¼©æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œæ­ç¤ºç¼©æ”¾æ³•åˆ™ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHPCM-1Bæ¨¡å‹åœ¨ç‡å¤±çœŸæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æœ€ä¼˜æ°´å¹³ï¼Œå±•ç¤ºäº†å¤§è§„æ¨¡æ¨¡å‹çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå­¦ä¹ å›¾åƒå‹ç¼©ä½œä¸ºç°ä»£æ•°æ®å‹ç¼©çš„åŸºç¡€ä»»åŠ¡å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹çš„è§„æ¨¡æœ‰é™ï¼Œé™åˆ¶äº†å…¶è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸”å¦‚ä½•é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡æ¥å½±å“å‹ç¼©æ€§èƒ½å°šæœªè¢«æ·±å…¥æ¢è®¨ã€‚æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹çš„è§„æ¨¡æ‰©å±•ï¼Œå¹¶é€šè¿‡ç¼©æ”¾æ³•åˆ™æ­ç¤ºäº†æ€§èƒ½è¶‹åŠ¿ã€‚ä»¥æœ€æ–°çš„HPCMæ¨¡å‹ä¸ºåŸºçº¿ï¼Œå°†æ¨¡å‹å‚æ•°ä»6850ä¸‡æ‰©å±•è‡³10äº¿ï¼Œå¹¶æ‹Ÿåˆæµ‹è¯•æŸå¤±ä¸æ¨¡å‹è§„æ¨¡åŠæœ€ä½³è®­ç»ƒè®¡ç®—ç­‰å…³é”®ç¼©æ”¾å˜é‡ä¹‹é—´çš„å¹‚å¾‹å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©å±•åçš„HPCM-1Bæ¨¡å‹åœ¨ç‡å¤±çœŸæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æœ€ä¼˜æ°´å¹³ã€‚å¸Œæœ›æœ¬ç ”ç©¶èƒ½æ¿€åŠ±æœªæ¥å¯¹å¤§è§„æ¨¡å‹ç¼©æ¨¡å‹çš„æ¢ç´¢åŠå‹ç¼©ä¸æ™ºèƒ½ä¹‹é—´å…³ç³»çš„æ·±å…¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹è§„æ¨¡é™åˆ¶çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¡¨ç¤ºèƒ½åŠ›å’Œå‹ç¼©æ€§èƒ½ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ¨¡å‹å‚æ•°ä»6850ä¸‡æ‰©å±•è‡³10äº¿ï¼Œæ¢ç´¢æ¨¡å‹è§„æ¨¡å¯¹å‹ç¼©æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ‹Ÿåˆå¹‚å¾‹å…³ç³»ï¼Œä»¥æ­ç¤ºæ€§èƒ½è¶‹åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä»¥HPCMæ¨¡å‹ä¸ºåŸºçº¿ï¼Œé‡‡ç”¨é€æ­¥æ‰©å±•æ¨¡å‹å‚æ•°çš„æ–¹å¼ï¼Œåˆ†ææµ‹è¯•æŸå¤±ä¸æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒè®¡ç®—ä¹‹é—´çš„å…³ç³»ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ¨¡å‹è®¾è®¡ã€è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹çš„è§„æ¨¡æ‰©å±•ï¼Œæ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ä¸å‹ç¼©æ€§èƒ½ä¹‹é—´çš„ç¼©æ”¾æ³•åˆ™ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ï¼Œç¡®ä¿åœ¨æ‰©å±•è¿‡ç¨‹ä¸­ä¿æŒè®­ç»ƒæ•ˆç‡å’Œå‹ç¼©æ€§èƒ½çš„å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰©å±•åçš„HPCM-1Bæ¨¡å‹åœ¨ç‡å¤±çœŸæ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä¼˜æ¨¡å‹ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨å‹ç¼©æ•ˆç‡ä¸Šæå‡æ˜¾è‘—ï¼Œå±•ç¤ºäº†å¤§è§„æ¨¡å­¦ä¹ æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘å‹ç¼©ã€æµåª’ä½“ä¼ è¾“ã€å­˜å‚¨ä¼˜åŒ–ç­‰ã€‚é€šè¿‡æå‡å‹ç¼©æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å¸¦å®½éœ€æ±‚å’Œå­˜å‚¨æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤§è§„æ¨¡çš„å‹ç¼©æ¨¡å‹ç ”ç©¶ï¼Œæ·±åŒ–å¯¹å‹ç¼©ä¸æ™ºèƒ½ä¹‹é—´å…³ç³»çš„ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) highlight a strong connection between intelligence and compression. Learned image compression, a fundamental task in modern data compression, has made significant progress in recent years. However, current models remain limited in scale, restricting their representation capacity, and how scaling model size influences compression performance remains unexplored. In this work, we present a pioneering study on scaling up learned image compression models and revealing the performance trends through scaling laws. Using the recent state-of-the-art HPCM model as baseline, we scale model parameters from 68.5 millions to 1 billion and fit power-law relations between test loss and key scaling variables, including model size and optimal training compute. The results reveal a scaling trend, enabling extrapolation to larger scale models. Experimental results demonstrate that the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion performance. We hope this work inspires future exploration of large-scale compression models and deeper investigations into the connection between compression and intelligence.

