---
layout: default
title: AME: Aligned Manifold Entropy for Robust Vision-Language Distillation
---

# AME: Aligned Manifold Entropy for Robust Vision-Language Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08644" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08644v1</a>
  <a href="https://arxiv.org/pdf/2508.08644.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08644v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08644v1', 'AME: Aligned Manifold Entropy for Robust Vision-Language Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guiming Cao, Yuming Ou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAMEä»¥è§£å†³è§†è§‰-è¯­è¨€è’¸é¦ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ç†µæœ€å°åŒ–` `æµå½¢å­¦ä¹ ` `æ¨¡å‹æ³›åŒ–` `ä½æ•°æ®ç¯å¢ƒ` `è·¨æ¨¡æ€ç‰¹å¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨é¢å¯¹æ¨¡ç³Šæ ·æœ¬æ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡ç‰¹å®šä»»åŠ¡çš„æ•°æ®ï¼Œéš¾ä»¥åœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„AMEæ–¹æ³•é€šè¿‡ç†µæœ€å°åŒ–åœ¨å…±äº«æµå½¢ä¸Šå®ç°è·¨æ¨¡æ€ç‰¹å¾çš„å‹ç¼©ï¼Œå¢å¼ºäº†çŸ¥è¯†è’¸é¦çš„ç¨³å¥æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAMEåœ¨å¤šç§è’¸é¦æ¶æ„ä¸­å‡èƒ½æ˜¾è‘—æå‡æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§é•¿æœŸå­˜åœ¨çš„çŸ¥è¯†è½¬ç§»æŠ€æœ¯ï¼Œè¿‘å¹´æ¥åœ¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­é‡æ–°å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè§†è§‰-è¯­è¨€çŸ¥è¯†è’¸é¦é€šå¸¸éœ€è¦è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œä»¥åœ¨å…·æœ‰æ¨¡ç³Šæˆ–è¾¹ç•Œç›¸é‚»è¡¨ç¤ºçš„æ ·æœ¬ä¸Šå®ç°ç¨³å¥çš„æ³›åŒ–ï¼Œè¿™äº›æ ·æœ¬é€šå¸¸ä¼´éšé«˜é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯¹é½æµå½¢ç†µï¼ˆAMEï¼‰ï¼Œæ—¨åœ¨åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚AMEé€šè¿‡å¯¹é‡æ–°é…ç½®çš„å…±äº«æµå½¢è¿›è¡Œç†µæœ€å°åŒ–ï¼Œå°†å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰é€šè¿‡ä¸€å¯¹æŠ•å½±å‡½æ•°è¿æ¥ï¼Œä»è€Œä¿ƒè¿›è·¨æ¨¡æ€ç‰¹å¾è¡¨ç¤ºçš„ç»“æ„å‹ç¼©ã€‚è¿™ä½¿å¾—åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹å®ç°ç¨³å¥çš„çŸ¥è¯†è’¸é¦æˆä¸ºå¯èƒ½ï¼Œè€Œæ— éœ€å¯¹ä¸»å¹²ç½‘ç»œè¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAMEåœ¨å¤šç§è’¸é¦æ¶æ„å’Œè®­ç»ƒè®¾ç½®ä¸‹å‡èƒ½æœ‰æ•ˆä¿ƒè¿›çŸ¥è¯†è’¸é¦ï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€çŸ¥è¯†è’¸é¦ä¸­ç”±äºæ•°æ®ä¸è¶³è€Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰é«˜ä¸ç¡®å®šæ€§çš„æ¨¡ç³Šæ ·æœ¬æ—¶ï¼Œå¾€å¾€ä¾èµ–äºå¤§é‡ç‰¹å®šä»»åŠ¡çš„æ•°æ®ï¼Œéš¾ä»¥åœ¨å®é™…åº”ç”¨ä¸­è·å¾—æœ‰æ•ˆçš„è®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„AMEæ–¹æ³•é€šè¿‡åœ¨é‡æ–°é…ç½®çš„å…±äº«æµå½¢ä¸Šè¿›è¡Œç†µæœ€å°åŒ–ï¼Œè¿æ¥å¤šæ¨¡æ€æ•°æ®ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰ï¼Œä»è€Œå®ç°è·¨æ¨¡æ€ç‰¹å¾çš„ç»“æ„å‹ç¼©ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜çŸ¥è¯†è’¸é¦çš„ç¨³å¥æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAMEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®çš„å¤šæ¨¡æ€è¾“å…¥ã€å…±äº«æµå½¢çš„æ„å»ºä»¥åŠç†µæœ€å°åŒ–çš„å®æ–½ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡ä¸€å¯¹æŠ•å½±å‡½æ•°å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°å…±äº«æµå½¢ä¸Šï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç†µæœ€å°åŒ–ä»¥ä¿ƒè¿›çŸ¥è¯†çš„æœ‰æ•ˆè½¬ç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šAMEçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†çŸ¥è¯†è’¸é¦ä¸ç†µæœ€å°åŒ–ç›¸ç»“åˆï¼Œåˆ©ç”¨å…±äº«æµå½¢çš„ç»“æ„ç‰¹æ€§æ¥é™ä½æ³›åŒ–è¯¯å·®ç•Œé™ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨¡å‹å¤æ‚åº¦çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒAMEé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡ç†µçš„æœ€å°åŒ–ï¼Œå¹¶è®¾è®¡äº†é€‚åˆçš„æŠ•å½±å‡½æ•°ä»¥ç¡®ä¿å¤šæ¨¡æ€æ•°æ®çš„æœ‰æ•ˆå¯¹é½ã€‚æ­¤å¤–ï¼Œæ–¹æ³•ä¸éœ€è¦å¯¹ä¸»å¹²ç½‘ç»œè¿›è¡Œæ¶æ„ä¸Šçš„ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—åº”ç”¨äºå¤šç§è’¸é¦æ¡†æ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAMEåœ¨å¤šç§è’¸é¦æ¶æ„ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨ä½æ•°æ®ç¯å¢ƒä¸­ï¼ŒAMEçš„å¼•å…¥ä½¿å¾—æ¨¡å‹çš„æ³›åŒ–è¯¯å·®ç•Œé™æ›´ç´§ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ›´ä¼˜çš„æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ä»¥åŠå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒAMEå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ›´å¥½åœ°å¤„ç†æ¨¡ç³Šæˆ–ä¸ç¡®å®šçš„æ•°æ®ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge distillation is a long-established technique for knowledge transfer, and has regained attention in the context of the recent emergence of large vision-language models (VLMs). However, vision-language knowledge distillation often requires sufficient training data to achieve robust generalization on amples with ambiguous or boundary-adjacent representations, which are associated with high predictive uncertainty. Critically, collecting such large-scale, task-specific data for training is often impractical in real-world scenarios. To address this major challenge arising from the entanglement of uncertainty and cross-modal feature representation, we propose Aligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming to achieve robust generalization under real-world conditions. AME applies entropy minimization over a reconfigured shared manifold, where multi-modal data (i.e., image and text) are bridged through a pair of projection functions, conducive to structural compression for cross-modal feature representations. This enables robust knowledge distillation under low-data regimes, while requiring no architectural modifications to the backbone. As a result, it can serve as a plug-and-play module compatible with a wide range of vision-language distillation frameworks. Notably, our theoretical analysis reveals that integrating knowledge distillation with entropy minimization over the shared manifold leads to a tighter generalization error bound. Extensive experiments across diverse distillation architectures and training settings demonstrate that AME consistently facilitates robust knowledge distillation, resulting in superior generalization performance across a wide spectrum of downstream tasks.

