---
layout: default
title: ViPE: Video Pose Engine for 3D Geometric Perception
---

# ViPE: Video Pose Engine for 3D Geometric Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10934" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10934v1</a>
  <a href="https://arxiv.org/pdf/2508.10934.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10934v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10934v1', 'ViPE: Video Pose Engine for 3D Geometric Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, Jiawei Ren, Kevin Xie, Joydeep Biswas, Laura Leal-Taixe, Sanja Fidler

**åˆ†ç±»**: cs.CV, cs.GR, cs.RO, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViPEä»¥è§£å†³3Då‡ ä½•æ„ŸçŸ¥ä¸­çš„è§†é¢‘æ ‡æ³¨æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Då‡ ä½•æ„ŸçŸ¥` `è§†é¢‘å¤„ç†` `ç›¸æœºå§¿æ€ä¼°è®¡` `æ·±åº¦å›¾ç”Ÿæˆ` `ç©ºé—´AIç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è·å–ä¸€è‡´ä¸”ç²¾ç¡®çš„3Dæ ‡æ³¨æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ— çº¦æŸè§†é¢‘æ—¶ã€‚
2. ViPEé€šè¿‡é«˜æ•ˆä¼°è®¡ç›¸æœºå†…å‚ã€ç›¸æœºè¿åŠ¨å’Œå¯†é›†æ·±åº¦å›¾ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†é¢‘å¤„ç†å¼•æ“ã€‚
3. ViPEåœ¨TUM/KITTIåºåˆ—ä¸Šåˆ†åˆ«æ¯”ç°æœ‰åŸºçº¿æé«˜äº†18%å’Œ50%çš„æ€§èƒ½ï¼Œä¸”åœ¨å•ä¸ªGPUä¸Šè¿è¡Œé€Ÿåº¦è¾¾åˆ°3-5FPSã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®çš„3Då‡ ä½•æ„ŸçŸ¥æ˜¯å¤šç§ç©ºé—´AIç³»ç»Ÿçš„é‡è¦å‰æã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼Œè€Œä»ç°å®è§†é¢‘ä¸­è·å–ä¸€è‡´ä¸”ç²¾ç¡®çš„3Dæ ‡æ³¨ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ViPEï¼Œä¸€ä¸ªé«˜æ•ˆä¸”å¤šåŠŸèƒ½çš„è§†é¢‘å¤„ç†å¼•æ“ï¼Œæ—¨åœ¨å¼¥è¡¥è¿™ä¸€ç©ºç™½ã€‚ViPEèƒ½å¤Ÿä»æ— çº¦æŸçš„åŸå§‹è§†é¢‘ä¸­é«˜æ•ˆä¼°è®¡ç›¸æœºå†…å‚ã€ç›¸æœºè¿åŠ¨å’Œå¯†é›†çš„è¿‘åº¦é‡æ·±åº¦å›¾ï¼Œä¸”å¯¹åŠ¨æ€è‡ªæ‹è§†é¢‘ã€ç”µå½±é•œå¤´å’Œè¡Œè½¦è®°å½•ä»ªç­‰å¤šç§åœºæ™¯å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¯¹ViPEè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨TUM/KITTIåºåˆ—ä¸Šåˆ†åˆ«æ¯”ç°æœ‰çš„æœªæ ‡å®šå§¿æ€ä¼°è®¡åŸºçº¿æé«˜äº†18%å’Œ50%ï¼Œå¹¶ä¸”åœ¨å•ä¸ªGPUä¸Šä»¥3-5FPSçš„é€Ÿåº¦è¿è¡Œã€‚æˆ‘ä»¬åˆ©ç”¨ViPEå¯¹å¤§è§„æ¨¡è§†é¢‘é›†åˆè¿›è¡Œäº†æ ‡æ³¨ï¼ŒåŒ…å«çº¦10ä¸‡æ¡çœŸå®äº’è”ç½‘è§†é¢‘ã€100ä¸‡æ¡é«˜è´¨é‡AIç”Ÿæˆè§†é¢‘å’Œ2000æ¡å…¨æ™¯è§†é¢‘ï¼Œæ€»è®¡çº¦9600ä¸‡å¸§ï¼Œæ‰€æœ‰è§†é¢‘å‡æ ‡æ³¨äº†å‡†ç¡®çš„ç›¸æœºå§¿æ€å’Œå¯†é›†æ·±åº¦å›¾ã€‚æˆ‘ä»¬å¼€æºäº†ViPEåŠæ ‡æ³¨æ•°æ®é›†ï¼Œå¸Œæœ›åŠ é€Ÿç©ºé—´AIç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»æ— çº¦æŸè§†é¢‘ä¸­è·å–å‡†ç¡®3Då‡ ä½•æ„ŸçŸ¥çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥è·å–ä¸€è‡´ä¸”ç²¾ç¡®çš„3Dæ ‡æ³¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šViPEçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é«˜æ•ˆçš„ç®—æ³•ä»åŸå§‹è§†é¢‘ä¸­æå–ç›¸æœºå†…å‚ã€è¿åŠ¨ä¿¡æ¯å’Œæ·±åº¦å›¾ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä»è€Œæé«˜3Då‡ ä½•æ„ŸçŸ¥çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViPEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç›¸æœºå†…å‚ä¼°è®¡ã€ç›¸æœºè¿åŠ¨ä¼°è®¡å’Œæ·±åº¦å›¾ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»è§†é¢‘ä¸­æå–å…³é”®å¸§ï¼Œç„¶åé€šè¿‡è¿™äº›å¸§è¿›è¡Œç›¸æœºå‚æ•°çš„ä¼°è®¡ï¼Œæœ€åç”Ÿæˆå¯†é›†çš„æ·±åº¦å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šViPEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯¹å¤šç§ç›¸æœºæ¨¡å‹çš„æ”¯æŒï¼ŒåŒ…æ‹¬é’ˆå­”ã€å¹¿è§’å’Œ360Â°å…¨æ™¯ç›¸æœºï¼Œä¸”åœ¨åŠ¨æ€åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰²ã€‚è¿™ä½¿å¾—ViPEåœ¨å¤„ç†å¤šæ ·åŒ–è§†é¢‘æ—¶å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒViPEé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç›¸æœºè¿åŠ¨å’Œæ·±åº¦å›¾çš„ä¼°è®¡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†é’ˆå¯¹æ€§çš„è®¾è®¡ï¼Œä»¥æé«˜å¤„ç†é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ViPEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨TUMå’ŒKITTIåºåˆ—ä¸Šï¼Œåˆ†åˆ«æ¯”ç°æœ‰æœªæ ‡å®šå§¿æ€ä¼°è®¡åŸºçº¿æé«˜äº†18%å’Œ50%ã€‚æ­¤å¤–ï¼ŒViPEåœ¨æ ‡å‡†è¾“å…¥åˆ†è¾¨ç‡ä¸‹èƒ½å¤Ÿä»¥3-5FPSçš„é€Ÿåº¦è¿è¡Œï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ViPEçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰ã€‚é€šè¿‡æä¾›å‡†ç¡®çš„3Då‡ ä½•æ„ŸçŸ¥ï¼ŒViPEèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸçš„ç©ºé—´AIç³»ç»Ÿæä¾›æ›´å¯é çš„åŸºç¡€ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate 3D geometric perception is an important prerequisite for a wide range of spatial AI systems. While state-of-the-art methods depend on large-scale training data, acquiring consistent and precise 3D annotations from in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a handy and versatile video processing engine designed to bridge this gap. ViPE efficiently estimates camera intrinsics, camera motion, and dense, near-metric depth maps from unconstrained raw videos. It is robust to diverse scenarios, including dynamic selfie videos, cinematic shots, or dashcams, and supports various camera models such as pinhole, wide-angle, and 360Â° panoramas. We have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to annotate a large-scale collection of videos. This collection includes around 100K real-world internet videos, 1M high-quality AI-generated videos, and 2K panoramic videos, totaling approximately 96M frames -- all annotated with accurate camera poses and dense depth maps. We open-source ViPE and the annotated dataset with the hope of accelerating the development of spatial AI systems.

