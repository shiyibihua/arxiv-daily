---
layout: default
title: RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space
---

# RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08588" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08588v1</a>
  <a href="https://arxiv.org/pdf/2508.08588.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08588v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08588v1', 'RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyun Liang, Jingkai Zhou, Shikai Li, Chenjie Cao, Lei Sun, Yichen Qian, Weihua Chen, Fan Wang

**åˆ†ç±»**: cs.CV, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: Project page: https://jingyunliang.github.io/RealisMotion

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRealisMotionä»¥è§£å†³äººç±»è¿åŠ¨æ§åˆ¶ä¸è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `äººç±»åŠ¨ä½œç”Ÿæˆ` `è§†é¢‘ç”Ÿæˆ` `3Dè¿åŠ¨æ§åˆ¶` `å…ƒç´ è§£è€¦` `æ–‡æœ¬åˆ°è§†é¢‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆè§†é¢‘æ—¶æ— æ³•ç‹¬ç«‹æ§åˆ¶å‰æ™¯ã€èƒŒæ™¯ã€è½¨è¿¹å’ŒåŠ¨ä½œï¼Œé™åˆ¶äº†çµæ´»æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡3Dç©ºé—´ä¸­çš„è¿åŠ¨ç¼–è¾‘å’Œå…ƒç´ è§£è€¦ï¼Œå®ç°äº†çµæ´»çš„ç»„åˆä¸æ§åˆ¶ã€‚
3. åœ¨åŸºå‡†æ•°æ®é›†å’Œå®é™…æ¡ˆä¾‹ä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿå’Œå¯æ§åŠ¨ä½œçš„äººç±»è§†é¢‘æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶èƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„è§†é¢‘ï¼Œä½†åœ¨å‰æ™¯ä¸»ä½“ã€èƒŒæ™¯è§†é¢‘ã€äººç±»è½¨è¿¹å’ŒåŠ¨ä½œæ¨¡å¼å››ä¸ªå…³é”®è§†é¢‘å…ƒç´ çš„ç‹¬ç«‹æ§åˆ¶ä¸Šå­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ†è§£çš„äººç±»è¿åŠ¨æ§åˆ¶ä¸è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜ç¡®å°†è¿åŠ¨ä¸å¤–è§‚ã€ä¸»ä½“ä¸èƒŒæ™¯ã€åŠ¨ä½œä¸è½¨è¿¹è§£è€¦ï¼Œä»è€Œå®ç°è¿™äº›å…ƒç´ çš„çµæ´»ç»„åˆã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŸºäºåœ°é¢çš„3Dä¸–ç•Œåæ ‡ç³»ç»Ÿï¼Œå¹¶åœ¨3Dç©ºé—´ä¸­ç›´æ¥è¿›è¡Œè¿åŠ¨ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…ƒç´ å¯æ§æ€§å’Œæ•´ä½“è§†é¢‘è´¨é‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨è¿åŠ¨æ§åˆ¶å’Œå…ƒç´ ç»„åˆä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯æ— æ³•ç‹¬ç«‹æ§åˆ¶è§†é¢‘ä¸­çš„å‰æ™¯ã€èƒŒæ™¯ã€è½¨è¿¹å’ŒåŠ¨ä½œçš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ¡†æ¶é€šè¿‡åœ¨3Dç©ºé—´ä¸­è¿›è¡Œè¿åŠ¨ç¼–è¾‘ï¼Œæ˜ç¡®è§£è€¦å„ä¸ªå…ƒç´ ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥çµæ´»ç»„åˆä¸åŒçš„åŠ¨ä½œå’ŒèƒŒæ™¯ï¼Œä»è€Œå®ç°æ›´é«˜çš„æ§åˆ¶èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ„å»ºä¸€ä¸ªåœ°é¢æ„ŸçŸ¥çš„3Dä¸–ç•Œåæ ‡ç³»ç»Ÿï¼›å…¶æ¬¡ï¼Œé€šè¿‡å°†ç¼–è¾‘åçš„2Dè½¨è¿¹åæŠ•å½±åˆ°3Dç©ºé—´è¿›è¡Œè½¨è¿¹æ§åˆ¶ï¼›æœ€åï¼Œåˆ©ç”¨ç°ä»£æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£å˜æ¢æ¨¡å‹è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¿åŠ¨ä¸å¤–è§‚ã€ä¸»ä½“ä¸èƒŒæ™¯ã€åŠ¨ä½œä¸è½¨è¿¹è¿›è¡Œæ˜ç¡®è§£è€¦ï¼Œè¿™ç§è®¾è®¡ä½¿å¾—ç”Ÿæˆçš„è§†é¢‘åœ¨å…ƒç´ ç»„åˆä¸Šå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œå¯æ§æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç„¦è·æ ¡å‡†å’Œåæ ‡å˜æ¢æ¥å®ç°2Dåˆ°3Dçš„è½¨è¿¹åæŠ•å½±ï¼Œå¹¶é€šè¿‡é€Ÿåº¦å¯¹é½å’Œæ–¹å‘è°ƒæ•´æ¥ä¼˜åŒ–è¿åŠ¨æ§åˆ¶ã€‚åŒæ—¶ï¼ŒåŠ¨ä½œå¯ä»¥é€šè¿‡åŠ¨ä½œåº“æä¾›æˆ–é€šè¿‡æ–‡æœ¬ç”Ÿæˆï¼Œç¡®ä¿äº†å¤šæ ·æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRealisMotionåœ¨å…ƒç´ å¯æ§æ€§å’Œè§†é¢‘è´¨é‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå…ƒç´ æ§åˆ¶èƒ½åŠ›æå‡äº†XX%ï¼Œæ•´ä½“è§†é¢‘è´¨é‡è¯„åˆ†æé«˜äº†YY%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºåˆ›ä½œè€…æä¾›æ›´é«˜æ•ˆçš„å·¥å…·æ¥ç”Ÿæˆå¤æ‚çš„äººç±»åŠ¨ä½œè§†é¢‘ï¼Œæå‡å†…å®¹åˆ›ä½œçš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨ç¤¾äº¤åª’ä½“å’Œåœ¨çº¿æ•™è‚²ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating human videos with realistic and controllable motions is a challenging task. While existing methods can generate visually compelling videos, they lack separate control over four key video elements: foreground subject, background video, human trajectory and action patterns. In this paper, we propose a decomposed human motion control and video generation framework that explicitly decouples motion from appearance, subject from background, and action from trajectory, enabling flexible mix-and-match composition of these elements. Concretely, we first build a ground-aware 3D world coordinate system and perform motion editing directly in the 3D space. Trajectory control is implemented by unprojecting edited 2D trajectories into 3D with focal-length calibration and coordinate transformation, followed by speed alignment and orientation adjustment; actions are supplied by a motion bank or generated via text-to-motion methods. Then, based on modern text-to-video diffusion transformer models, we inject the subject as tokens for full attention, concatenate the background along the channel dimension, and add motion (trajectory and action) control signals by addition. Such a design opens up the possibility for us to generate realistic videos of anyone doing anything anywhere. Extensive experiments on benchmark datasets and real-world cases demonstrate that our method achieves state-of-the-art performance on both element-wise controllability and overall video quality.

