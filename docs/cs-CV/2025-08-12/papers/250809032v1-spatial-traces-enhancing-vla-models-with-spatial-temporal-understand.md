---
layout: default
title: Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding
---

# Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09032" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09032v1</a>
  <a href="https://arxiv.org/pdf/2508.09032.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09032v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09032v1', 'Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maxim A. Patratskiy, Alexey K. Kovalev, Aleksandr I. Panov

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://ampiromax.github.io/ST-VLA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´è½¨è¿¹æ–¹æ³•ä»¥å¢å¼ºVLAæ¨¡å‹çš„æ—¶ç©ºç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-è¡ŒåŠ¨` `æ—¶ç©ºç†è§£` `æ·±åº¦å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `æœºå™¨äººå¯¼èˆª` `è‡ªåŠ¨é©¾é©¶` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAæ¨¡å‹åœ¨ç©ºé—´å’Œæ—¶é—´ç†è§£æ–¹é¢çš„æå‡å¤šä¸ºç‹¬ç«‹è¿›è¡Œï¼Œç¼ºä¹æœ‰æ•ˆçš„æ•´åˆæ–¹æ³•ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è§†è§‰æç¤ºå°†å…³é”®ç‚¹çš„è§†è§‰è½¨è¿¹æŠ•å½±åˆ°æ·±åº¦å›¾ä¸Šï¼Œä»è€ŒåŒæ—¶æ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ã€‚
3. åœ¨SimplerEnvå®éªŒä¸­ï¼Œæ¨¡å‹æˆåŠŸè§£å†³çš„ä»»åŠ¡æ•°é‡æ˜¾è‘—æå‡ï¼Œä¸”å¯¹è®­ç»ƒæ•°æ®çš„éœ€æ±‚è¾ƒä½ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ ¹æ®è§†è§‰è§‚å¯Ÿå’Œæ–‡æœ¬æŒ‡ä»¤é¢„æµ‹ä»£ç†åœ¨è™šæ‹Ÿç¯å¢ƒåŠç°å®åœºæ™¯ä¸­çš„è¿åŠ¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶åˆ†åˆ«å¢å¼ºäº†ç©ºé—´å’Œæ—¶é—´ç†è§£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è§†è§‰æç¤ºæ•´åˆä¸¤è€…çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å°†è§‚å¯Ÿä¸­çš„å…³é”®ç‚¹è§†è§‰è½¨è¿¹æŠ•å½±åˆ°æ·±åº¦å›¾ä¸Šçš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨SimplerEnvä¸­ï¼ŒæˆåŠŸè§£å†³çš„ä»»åŠ¡å¹³å‡æ•°é‡ç›¸æ¯”SpatialVLAæé«˜äº†4%ï¼Œç›¸æ¯”TraceVLAæé«˜äº†19%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥å¢å¼ºå¯ä»¥åœ¨æœ€å°è®­ç»ƒæ•°æ®ä¸‹å®ç°ï¼Œç‰¹åˆ«é€‚ç”¨äºæ•°æ®æ”¶é›†å›°éš¾çš„ç°å®åº”ç”¨åœºæ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰VLAæ¨¡å‹åœ¨ç©ºé—´å’Œæ—¶é—´ç†è§£æ–¹é¢çš„ç‹¬ç«‹æ€§é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†è§†è§‰è½¨è¿¹ä¸æ·±åº¦å›¾ç»“åˆï¼Œæ¨¡å‹èƒ½å¤ŸåŒæ—¶è·å–ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œä»è€Œæå‡ç†è§£èƒ½åŠ›å’Œä»»åŠ¡æ‰§è¡Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰è½¨è¿¹æå–æ¨¡å—ã€æ·±åº¦å›¾æŠ•å½±æ¨¡å—å’Œä»»åŠ¡æ‰§è¡Œæ¨¡å—ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆä¸åˆ©ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè§†è§‰è½¨è¿¹çš„æŠ•å½±æ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨åŒä¸€æ—¶é—´å†…å¤„ç†ç©ºé—´å’Œæ—¶é—´æ•°æ®ï¼ŒåŒºåˆ«äºä»¥å¾€çš„å•ä¸€å¤„ç†æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ç©ºé—´ä¸æ—¶é—´ä¿¡æ¯çš„æƒé‡ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨SimplerEnvä¸­æˆåŠŸè§£å†³çš„ä»»åŠ¡æ•°é‡ç›¸æ¯”SpatialVLAæå‡äº†4%ï¼Œç›¸æ¯”TraceVLAæå‡äº†19%ã€‚è¿™ä¸€æå‡åœ¨æœ€å°è®­ç»ƒæ•°æ®ä¸‹å®ç°ï¼Œå±•ç¤ºäº†æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œæ‰§è¡Œæ•ˆç‡ã€‚æœªæ¥ï¼Œéšç€æ•°æ®æ”¶é›†æŠ€æœ¯çš„è¿›æ­¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å¾—åˆ°æ¨å¹¿ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at https://ampiromax.github.io/ST-VLA.

