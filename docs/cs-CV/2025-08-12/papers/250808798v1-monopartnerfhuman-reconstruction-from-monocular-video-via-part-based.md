---
layout: default
title: MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields
---

# MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08798" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08798v1</a>
  <a href="https://arxiv.org/pdf/2508.08798.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08798v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08798v1', 'MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yao Lu, Jiawei Li, Ming Jiang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMonoPartNeRFä»¥è§£å†³å•ç›®è§†é¢‘ä¸­äººç±»é‡å»ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `åŠ¨æ€äººç±»é‡å»º` `ç¥ç»è¾å°„åœº` `å•ç›®è§†é¢‘` `å§¿æ€åµŒå…¥` `é®æŒ¡æ¢å¤` `çº¹ç†å»ºæ¨¡` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å§¿æ€å˜åŒ–ä¸‹å¸¸å¯¼è‡´éƒ¨ä»¶è¾¹ç•Œå¤„çš„ä¸è‡ªç„¶è¿‡æ¸¡ï¼Œä¸”æ— æ³•å‡†ç¡®é‡å»ºè¢«é®æŒ¡åŒºåŸŸã€‚
2. æå‡ºMonoPartNeRFæ¡†æ¶ï¼Œé€šè¿‡åŒå‘å˜å½¢æ¨¡å‹å’Œéƒ¨åˆ†å§¿æ€åµŒå…¥æœºåˆ¶æ¥è§£å†³åŠ¨æ€äººç±»æ¸²æŸ“ä¸­çš„æŒ‘æˆ˜ã€‚
3. åœ¨ZJU-MoCapå’ŒMonoCapæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…³èŠ‚å¯¹é½å’Œçº¹ç†ä¿çœŸåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åŠ¨æ€äººç±»é‡å»ºå’Œæ¸²æŸ“æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åŸºäºéƒ¨åˆ†çš„æ¸²æŸ“èŒƒå¼é€šè¿‡äººä½“åˆ†å‰²å¼•å¯¼çµæ´»çš„å‚æ•°åˆ†é…ï¼Œæå‡äº†è¡¨ç¤ºæ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚å§¿æ€å˜åŒ–ä¸‹ä»å­˜åœ¨å›°éš¾ï¼Œå¸¸å¯¼è‡´éƒ¨ä»¶è¾¹ç•Œå¤„çš„ä¸è‡ªç„¶è¿‡æ¸¡ï¼Œå¹¶ä¸”åœ¨å•ç›®è®¾ç½®ä¸­æ— æ³•å‡†ç¡®é‡å»ºè¢«é®æŒ¡åŒºåŸŸã€‚æˆ‘ä»¬æå‡ºäº†MonoPartNeRFï¼Œä¸€ä¸ªæ–°é¢–çš„å•ç›®åŠ¨æ€äººç±»æ¸²æŸ“æ¡†æ¶ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡å’Œå¼ºå¤§çš„é®æŒ¡æ¢å¤ã€‚é€šè¿‡å»ºç«‹åŒå‘å˜å½¢æ¨¡å‹ï¼Œç»“åˆåˆšæ€§å’Œéåˆšæ€§å˜æ¢ï¼Œæ„å»ºè§‚å¯Ÿç©ºé—´ä¸å…¸å‹ç©ºé—´ä¹‹é—´çš„è¿ç»­å¯é€†æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚å§¿æ€å’Œé®æŒ¡æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ›´å¥½çš„å…³èŠ‚å¯¹é½ã€çº¹ç†ä¿çœŸåº¦å’Œç»“æ„è¿ç»­æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å•ç›®è§†é¢‘ä¸­åŠ¨æ€äººç±»é‡å»ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚å§¿æ€å’Œé®æŒ¡æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´é‡å»ºç»“æœä¸è‡ªç„¶ä¸”ç¼ºä¹å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMonoPartNeRFæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆåˆšæ€§å’Œéåˆšæ€§å˜æ¢çš„åŒå‘å˜å½¢æ¨¡å‹ï¼Œå»ºç«‹è§‚å¯Ÿç©ºé—´ä¸å…¸å‹ç©ºé—´ä¹‹é—´çš„è¿ç»­å¯é€†æ˜ å°„ï¼Œä»¥æ”¹å–„å§¿æ€å˜åŒ–å’Œé®æŒ¡æ¢å¤çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŒå‘å˜å½¢æ¨¡å‹ã€éƒ¨åˆ†å§¿æ€åµŒå…¥æœºåˆ¶å’Œå¯å­¦ä¹ çš„å¤–è§‚ç¼–ç ã€‚é¦–å…ˆï¼Œé€šè¿‡å˜å½¢æ¨¡å‹å¤„ç†è¾“å…¥æ•°æ®ï¼Œç„¶ååˆ©ç”¨å§¿æ€åµŒå…¥è¿›è¡Œç‰¹å¾é‡‡æ ·ï¼Œæœ€åé€šè¿‡å¤–è§‚ç¼–ç å®ç°åŠ¨æ€çº¹ç†å˜åŒ–çš„å»ºæ¨¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒå‘å˜å½¢æ¨¡å‹å’Œéƒ¨åˆ†å§¿æ€åµŒå…¥æœºåˆ¶ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„å§¿æ€å˜åŒ–å’Œé®æŒ¡é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†é‡å»ºæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†ä¸€è‡´æ€§æŸå¤±æ¥æŠ‘åˆ¶å˜å½¢å¼•èµ·çš„ä¼ªå½±å’Œä¸è¿ç»­æ€§ï¼ŒåŒæ—¶åœ¨å§¿æ€åµŒå…¥ä¸­é‡‡ç”¨äº†åŸºäºèº«ä½“åŒºåŸŸçš„å±€éƒ¨å…³èŠ‚åµŒå…¥ï¼Œä»¥å¢å¼ºç‰¹å¾çš„å§¿æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚æ•´ä½“ç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”åŠ¨æ€äººç±»çš„é‡å»ºéœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMonoPartNeRFåœ¨ZJU-MoCapå’ŒMonoCapæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…³èŠ‚å¯¹é½ç²¾åº¦æå‡äº†XX%ï¼Œçº¹ç†ä¿çœŸåº¦æé«˜äº†YY%ï¼Œåœ¨å¤æ‚å§¿æ€å’Œé®æŒ¡æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’ŒåŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸã€‚é€šè¿‡å®ç°é«˜è´¨é‡çš„äººç±»é‡å»ºï¼ŒMonoPartNeRFèƒ½å¤Ÿä¸ºæ¸¸æˆå¼€å‘ã€å½±è§†ç‰¹æ•ˆå’Œäººæœºäº¤äº’æä¾›æ›´ä¸ºçœŸå®çš„è§†è§‰ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, Neural Radiance Fields (NeRF) have achieved remarkable progress in dynamic human reconstruction and rendering. Part-based rendering paradigms, guided by human segmentation, allow for flexible parameter allocation based on structural complexity, thereby enhancing representational efficiency. However, existing methods still struggle with complex pose variations, often producing unnatural transitions at part boundaries and failing to reconstruct occluded regions accurately in monocular settings. We propose MonoPartNeRF, a novel framework for monocular dynamic human rendering that ensures smooth transitions and robust occlusion recovery. First, we build a bidirectional deformation model that combines rigid and non-rigid transformations to establish a continuous, reversible mapping between observation and canonical spaces. Sampling points are projected into a parameterized surface-time space (u, v, t) to better capture non-rigid motion. A consistency loss further suppresses deformation-induced artifacts and discontinuities. We introduce a part-based pose embedding mechanism that decomposes global pose vectors into local joint embeddings based on body regions. This is combined with keyframe pose retrieval and interpolation, along three orthogonal directions, to guide pose-aware feature sampling. A learnable appearance code is integrated via attention to model dynamic texture changes effectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that our method significantly outperforms prior approaches under complex pose and occlusion conditions, achieving superior joint alignment, texture fidelity, and structural continuity.

