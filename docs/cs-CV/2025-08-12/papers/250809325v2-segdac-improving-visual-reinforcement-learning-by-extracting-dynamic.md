---
layout: default
title: SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Objectc-Centric Representations from Pretrained Vision Models
---

# SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Objectc-Centric Representations from Pretrained Vision Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09325" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09325v2</a>
  <a href="https://arxiv.org/pdf/2508.09325.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09325v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09325v2', 'SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Objectc-Centric Representations from Pretrained Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexandre Brown, Glen Berseth

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-10-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSegDACä»¥è§£å†³è§†è§‰å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨æ€å¯¹è±¡è¡¨ç¤ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€å¯¹è±¡è¡¨ç¤º` `åˆ†å‰²é©±åŠ¨æ–¹æ³•` `æ ·æœ¬æ•ˆç‡` `è§†è§‰æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é«˜ç»´è¾“å…¥ä¸­æå–æœ‰æ•ˆè¡¨ç¤ºçš„èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚
2. SegDACé€šè¿‡ä½¿ç”¨SAMè¿›è¡Œå¯¹è±¡ä¸­å¿ƒåˆ†è§£å’ŒYOLO-Worldè¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²é©±åŠ¨çš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ã€‚
3. åœ¨Maniskill3åŸºå‡†ä¸Šï¼ŒSegDACåœ¨è§†è§‰æ³›åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæœ€å›°éš¾è®¾ç½®ä¸‹æ€§èƒ½ç¿»å€ï¼Œæ ·æœ¬æ•ˆç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢ä¸´ä»é«˜ç»´è¾“å…¥ä¸­æå–æœ‰ç”¨è¡¨ç¤ºçš„æŒ‘æˆ˜ï¼ŒåŒæ—¶è¿˜éœ€åœ¨ç¨€ç–å’Œå™ªå£°å¥–åŠ±ä¸­å­¦ä¹ æœ‰æ•ˆæ§åˆ¶ã€‚å°½ç®¡å­˜åœ¨å¤§å‹æ„ŸçŸ¥æ¨¡å‹ï¼Œä½†å°†å…¶æœ‰æ•ˆæ•´åˆåˆ°RLä¸­ä»¥å®ç°è§†è§‰æ³›åŒ–å’Œæé«˜æ ·æœ¬æ•ˆç‡ä»ç„¶å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†SegDACï¼Œä¸€ç§åŸºäºåˆ†å‰²é©±åŠ¨çš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ã€‚SegDACåˆ©ç”¨Segment Anythingï¼ˆSAMï¼‰è¿›è¡Œå¯¹è±¡ä¸­å¿ƒåˆ†è§£ï¼Œå¹¶é€šè¿‡æ–‡æœ¬è¾“å…¥å°†å›¾åƒåˆ†å‰²è¿‡ç¨‹ä¸YOLO-Worldç»“åˆã€‚å®ƒåŒ…å«ä¸€ç§æ–°é¢–çš„åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œæ”¯æŒæ¯ä¸ªæ—¶é—´æ­¥åŠ¨æ€æ•°é‡çš„åˆ†æ®µï¼Œå¹¶é€šè¿‡åœ¨çº¿RLæœ‰æ•ˆå­¦ä¹ å…³æ³¨å“ªäº›åˆ†æ®µï¼Œè€Œæ— éœ€ä½¿ç”¨äººå·¥æ ‡ç­¾ã€‚é€šè¿‡åœ¨Maniskill3ä¸Šè¯„ä¼°SegDACï¼Œè¯¥åŸºå‡†æ¶µç›–äº†åœ¨å¼ºè§†è§‰æ‰°åŠ¨ä¸‹çš„å¤šæ ·åŒ–æ“ä½œä»»åŠ¡ï¼Œæˆ‘ä»¬è¯æ˜SegDACåœ¨è§†è§‰æ³›åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œåœ¨æœ€å›°éš¾çš„è®¾ç½®ä¸‹æ€§èƒ½ç¿»å€ï¼Œå¹¶åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­ä¸ä¹‹å‰çš„æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸ŠæŒå¹³æˆ–è¶…è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰å¼ºåŒ–å­¦ä¹ ä¸­ä»é«˜ç»´è¾“å…¥ä¸­æå–åŠ¨æ€å¯¹è±¡è¡¨ç¤ºçš„å›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨è§†è§‰æ³›åŒ–å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„æ“ä½œä»»åŠ¡æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSegDACçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Segment Anythingï¼ˆSAMï¼‰è¿›è¡Œå¯¹è±¡ä¸­å¿ƒåˆ†è§£ï¼Œå¹¶ç»“åˆYOLO-Worldé€šè¿‡æ–‡æœ¬è¾“å…¥å¢å¼ºå›¾åƒåˆ†å‰²è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´å…³æ³¨çš„å¯¹è±¡åˆ†æ®µï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSegDACé‡‡ç”¨åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œæ”¯æŒåœ¨æ¯ä¸ªæ—¶é—´æ­¥åŠ¨æ€è°ƒæ•´åˆ†æ®µæ•°é‡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å¯¹è±¡åˆ†è§£ã€å›¾åƒåˆ†å‰²å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„å­¦ä¹ ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šSegDACçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€åˆ†æ®µé€‰æ‹©æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰äººå·¥æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åœ¨çº¿å­¦ä¹ è‡ªåŠ¨è¯†åˆ«é‡è¦çš„å¯¹è±¡åˆ†æ®µï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œè§†è§‰æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSegDACä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åˆ†æ®µé€‰æ‹©ï¼Œå¹¶é‡‡ç”¨äº†å˜æ¢å™¨ç½‘ç»œç»“æ„ä»¥æ”¯æŒåŠ¨æ€è¾“å…¥ã€‚å…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬åˆ†æ®µæ•°é‡çš„åŠ¨æ€è°ƒæ•´å’Œä¸YOLO-Worldçš„æœ‰æ•ˆç»“åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SegDACåœ¨Maniskill3åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨æœ€å›°éš¾çš„è®¾ç½®ä¸‹å®ç°äº†æ€§èƒ½ç¿»å€ï¼Œå¹¶åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­ä¸ä¹‹å‰çš„æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸ŠæŒå¹³æˆ–è¶…è¶Šï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰æ³›åŒ–æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–å’Œæ§åˆ¶ã€‚æœªæ¥ï¼ŒSegDACçš„æŠ€æœ¯å¯ä»¥æ‰©å±•åˆ°æ›´å¤šçš„è§†è§‰ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨è§†è§‰å¼ºåŒ–å­¦ä¹ çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual reinforcement learning (RL) is challenging due to the need to extract useful representations from high-dimensional inputs while learning effective control from sparse and noisy rewards. Although large perception models exist, integrating them effectively into RL for visual generalization and improved sample efficiency remains difficult. We propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground the image segmentation process via text inputs. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels. By evaluating SegDAC over a challenging visual generalization benchmark using Maniskill3, which covers diverse manipulation tasks under strong visual perturbations, we demonstrate that SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks.

