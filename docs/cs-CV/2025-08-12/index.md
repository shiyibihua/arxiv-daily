---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-08-12
---

# cs.CVï¼ˆ2025-08-12ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250808987v1-colorgpt-leveraging-large-language-models-for-multimodal-color-recom.html">ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation</a></td>
  <td>æå‡ºColorGPTä»¥è§£å†³å¤šæ¨¡æ€é¢œè‰²æ¨èé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08987v1" data-paper-url="./papers/250808987v1-colorgpt-leveraging-large-language-models-for-multimodal-color-recom.html" onclick="toggleFavorite(this, '2508.08987v1', 'ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250809032v1-spatial-traces-enhancing-vla-models-with-spatial-temporal-understand.html">Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</a></td>
  <td>æå‡ºç©ºé—´è½¨è¿¹æ–¹æ³•ä»¥å¢å¼ºVLAæ¨¡å‹çš„æ—¶ç©ºç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09032v1" data-paper-url="./papers/250809032v1-spatial-traces-enhancing-vla-models-with-spatial-temporal-understand.html" onclick="toggleFavorite(this, '2508.09032v1', 'Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250808821v1-3dfromllm-3d-prototype-generation-only-from-pretrained-multimodal-ll.html">3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs</a></td>
  <td>æå‡º3DFroMLLMä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08821v1" data-paper-url="./papers/250808821v1-3dfromllm-3d-prototype-generation-only-from-pretrained-multimodal-ll.html" onclick="toggleFavorite(this, '2508.08821v1', '3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250808783v2-diffpose-animal-a-language-conditioned-diffusion-framework-for-anima.html">DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</a></td>
  <td>æå‡ºDiffPose-Animalä»¥è§£å†³åŠ¨ç‰©å§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">language conditioned</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08783v2" data-paper-url="./papers/250808783v2-diffpose-animal-a-language-conditioned-diffusion-framework-for-anima.html" onclick="toggleFavorite(this, '2508.08783v2', 'DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250809262v1-harnessing-input-adaptive-inference-for-efficient-vln.html">Harnessing Input-Adaptive Inference for Efficient VLN</a></td>
  <td>æå‡ºè¾“å…¥è‡ªé€‚åº”æ¨ç†æ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€å¯¼èˆªæ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09262v1" data-paper-url="./papers/250809262v1-harnessing-input-adaptive-inference-for-efficient-vln.html" onclick="toggleFavorite(this, '2508.09262v1', 'Harnessing Input-Adaptive Inference for Efficient VLN')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250808679v2-mmif-amin-adaptive-loss-driven-multi-scale-invertible-dense-network-.html">MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion</a></td>
  <td>æå‡ºMMIF-AMINä»¥è§£å†³å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08679v2" data-paper-url="./papers/250808679v2-mmif-amin-adaptive-loss-driven-multi-scale-invertible-dense-network-.html" onclick="toggleFavorite(this, '2508.08679v2', 'MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250808939v1-madprompts-unlocking-zero-shot-morphing-attack-detection-with-multip.html">MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation</a></td>
  <td>æå‡ºMADPromptSä»¥è§£å†³é›¶-shotäººè„¸å˜å½¢æ”»å‡»æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08939v1" data-paper-url="./papers/250808939v1-madprompts-unlocking-zero-shot-morphing-attack-detection-with-multip.html" onclick="toggleFavorite(this, '2508.08939v1', 'MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250809075v1-scaling-learned-image-compression-models-up-to-1-billion.html">Scaling Learned Image Compression Models up to 1 Billion</a></td>
  <td>æå‡ºå¤§è§„æ¨¡å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹ä»¥æå‡å‹ç¼©æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09075v1" data-paper-url="./papers/250809075v1-scaling-learned-image-compression-models-up-to-1-billion.html" onclick="toggleFavorite(this, '2508.09075v1', 'Scaling Learned Image Compression Models up to 1 Billion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250808978v1-taocache-structure-maintained-video-generation-acceleration.html">TaoCache: Structure-Maintained Video Generation Acceleration</a></td>
  <td>æå‡ºTaoCacheä»¥è§£å†³è§†é¢‘ç”ŸæˆåŠ é€Ÿä¸­çš„ç»“æ„ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08978v1" data-paper-url="./papers/250808978v1-taocache-structure-maintained-video-generation-acceleration.html" onclick="toggleFavorite(this, '2508.08978v1', 'TaoCache: Structure-Maintained Video Generation Acceleration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250809239v1-gradient-direction-aware-density-control-for-3d-gaussian-splatting.html">Gradient-Direction-Aware Density Control for 3D Gaussian Splatting</a></td>
  <td>æå‡ºæ¢¯åº¦æ–¹å‘æ„ŸçŸ¥å¯†åº¦æ§åˆ¶ä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09239v1" data-paper-url="./papers/250809239v1-gradient-direction-aware-density-control-for-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2508.09239v1', 'Gradient-Direction-Aware Density Control for 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250808867v1-gaussianupdate-continual-3d-gaussian-splatting-update-for-changing-e.html">GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments</a></td>
  <td>æå‡ºGaussianUpdateä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸‹çš„3Dåœºæ™¯æ›´æ–°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08867v1" data-paper-url="./papers/250808867v1-gaussianupdate-continual-3d-gaussian-splatting-update-for-changing-e.html" onclick="toggleFavorite(this, '2508.08867v1', 'GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250810936v2-vision-only-gaussian-splatting-for-collaborative-semantic-occupancy-.html">Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction</a></td>
  <td>æå‡ºç¨€ç–3Dè¯­ä¹‰é«˜æ–¯ç‚¹äº‘ä»¥è§£å†³åä½œè¯­ä¹‰å ç”¨é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10936v2" data-paper-url="./papers/250810936v2-vision-only-gaussian-splatting-for-collaborative-semantic-occupancy-.html" onclick="toggleFavorite(this, '2508.10936v2', 'Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250809068v2-a-new-dataset-and-comparison-for-multi-camera-frame-synthesis.html">A new dataset and comparison for multi-camera frame synthesis</a></td>
  <td>æå‡ºå¤šæ‘„åƒå¤´æ•°æ®é›†ä»¥è§£å†³å¸§åˆæˆæ–¹æ³•æ¯”è¾ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09068v2" data-paper-url="./papers/250809068v2-a-new-dataset-and-comparison-for-multi-camera-frame-synthesis.html" onclick="toggleFavorite(this, '2508.09068v2', 'A new dataset and comparison for multi-camera frame synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250808798v1-monopartnerfhuman-reconstruction-from-monocular-video-via-part-based.html">MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields</a></td>
  <td>æå‡ºMonoPartNeRFä»¥è§£å†³å•ç›®è§†é¢‘ä¸­äººç±»é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08798v1" data-paper-url="./papers/250808798v1-monopartnerfhuman-reconstruction-from-monocular-video-via-part-based.html" onclick="toggleFavorite(this, '2508.08798v1', 'MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250810935v2-hq-ov3d-a-high-box-quality-open-world-3d-detection-framework-based-o.html">HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model</a></td>
  <td>æå‡ºHQ-OV3Dä»¥è§£å†³å¼€æ”¾ä¸–ç•Œ3Dæ£€æµ‹ä¸­çš„ä¼ªæ ‡ç­¾è´¨é‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10935v2" data-paper-url="./papers/250810935v2-hq-ov3d-a-high-box-quality-open-world-3d-detection-framework-based-o.html" onclick="toggleFavorite(this, '2508.10935v2', 'HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250810934v1-vipe-video-pose-engine-for-3d-geometric-perception.html">ViPE: Video Pose Engine for 3D Geometric Perception</a></td>
  <td>æå‡ºViPEä»¥è§£å†³3Då‡ ä½•æ„ŸçŸ¥ä¸­çš„è§†é¢‘æ ‡æ³¨æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.10934v1" data-paper-url="./papers/250810934v1-vipe-video-pose-engine-for-3d-geometric-perception.html" onclick="toggleFavorite(this, '2508.10934v1', 'ViPE: Video Pose Engine for 3D Geometric Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250808811v1-revisiting-efficient-semantic-segmentation-learning-offsets-for-bett.html">Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment</a></td>
  <td>æå‡ºåç§»å­¦ä¹ æ–¹æ³•ä»¥è§£å†³è¯­ä¹‰åˆ†å‰²ä¸­çš„ç‰¹å¾å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08811v1" data-paper-url="./papers/250808811v1-revisiting-efficient-semantic-segmentation-learning-offsets-for-bett.html" onclick="toggleFavorite(this, '2508.08811v1', 'Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250808589v1-docthinker-explainable-multimodal-large-language-models-with-rule-ba.html">DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding</a></td>
  <td>æå‡ºDocThinkerä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08589v1" data-paper-url="./papers/250808589v1-docthinker-explainable-multimodal-large-language-models-with-rule-ba.html" onclick="toggleFavorite(this, '2508.08589v1', 'DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250809339v1-ultralight-med-vision-mamba-for-classification-of-neoplastic-progres.html">UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas</a></td>
  <td>æå‡ºUltralight Med-Vision Mambaä»¥è§£å†³è‚ é“è…ºç˜¤åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09339v1" data-paper-url="./papers/250809339v1-ultralight-med-vision-mamba-for-classification-of-neoplastic-progres.html" onclick="toggleFavorite(this, '2508.09339v1', 'UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250809087v1-addressing-bias-in-vlms-for-glaucoma-detection-without-protected-att.html">Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision</a></td>
  <td>æå‡ºæ— ç›‘ç£å±æ€§å»åè§æ–¹æ³•ä»¥æ”¹å–„é’å…‰çœ¼æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09087v1" data-paper-url="./papers/250809087v1-addressing-bias-in-vlms-for-glaucoma-detection-without-protected-att.html" onclick="toggleFavorite(this, '2508.09087v1', 'Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250808644v1-ame-aligned-manifold-entropy-for-robust-vision-language-distillation.html">AME: Aligned Manifold Entropy for Robust Vision-Language Distillation</a></td>
  <td>æå‡ºAMEä»¥è§£å†³è§†è§‰-è¯­è¨€è’¸é¦ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08644v1" data-paper-url="./papers/250808644v1-ame-aligned-manifold-entropy-for-robust-vision-language-distillation.html" onclick="toggleFavorite(this, '2508.08644v1', 'AME: Aligned Manifold Entropy for Robust Vision-Language Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250809362v1-fusionensemble-net-an-attention-based-ensemble-of-spatiotemporal-net.html">FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition</a></td>
  <td>æå‡ºFusionEnsemble-Netä»¥è§£å†³å¤šæ¨¡æ€æ‰‹è¯­è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09362v1" data-paper-url="./papers/250809362v1-fusionensemble-net-an-attention-based-ensemble-of-spatiotemporal-net.html" onclick="toggleFavorite(this, '2508.09362v1', 'FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250808989v1-kffocus-highlighting-keyframes-for-enhanced-video-understanding.html">KFFocus: Highlighting Keyframes for Enhanced Video Understanding</a></td>
  <td>æå‡ºKFFocusä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„å…³é”®å¸§å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08989v1" data-paper-url="./papers/250808989v1-kffocus-highlighting-keyframes-for-enhanced-video-understanding.html" onclick="toggleFavorite(this, '2508.08989v1', 'KFFocus: Highlighting Keyframes for Enhanced Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250809000v1-uniconvnet-expanding-effective-receptive-field-while-maintaining-asy.html">UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale</a></td>
  <td>æå‡ºUniConvNetä»¥æ‰©å±•æœ‰æ•ˆæ„Ÿå—é‡å¹¶ä¿æŒé«˜æ–¯åˆ†å¸ƒ</td>
  <td class="tags-cell"><span class="paper-tag">UniCon</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09000v1" data-paper-url="./papers/250809000v1-uniconvnet-expanding-effective-receptive-field-while-maintaining-asy.html" onclick="toggleFavorite(this, '2508.09000v1', 'UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250809383v1-x-unimotion-animating-human-images-with-expressive-unified-and-ident.html">X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents</a></td>
  <td>æå‡ºX-UniMotionä»¥å®ç°é«˜ä¿çœŸã€èº«ä»½æ— å…³çš„äººä½“åŠ¨ç”»</td>
  <td class="tags-cell"><span class="paper-tag">motion latent</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09383v1" data-paper-url="./papers/250809383v1-x-unimotion-animating-human-images-with-expressive-unified-and-ident.html" onclick="toggleFavorite(this, '2508.09383v1', 'X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250808991v1-spatial-temporal-multi-scale-quantization-for-flexible-motion-genera.html">Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</a></td>
  <td>æå‡ºå¤šå°ºåº¦é‡åŒ–æ–¹æ³•ä»¥è§£å†³äººç±»åŠ¨ä½œç”Ÿæˆçš„çµæ´»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08991v1" data-paper-url="./papers/250808991v1-spatial-temporal-multi-scale-quantization-for-flexible-motion-genera.html" onclick="toggleFavorite(this, '2508.08991v1', 'Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250808588v1-realismotion-decomposed-human-motion-control-and-video-generation-in.html">RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</a></td>
  <td>æå‡ºRealisMotionä»¥è§£å†³äººç±»è¿åŠ¨æ§åˆ¶ä¸è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08588v1" data-paper-url="./papers/250808588v1-realismotion-decomposed-human-motion-control-and-video-generation-in.html" onclick="toggleFavorite(this, '2508.08588v1', 'RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>28</td>
  <td><a href="./papers/250808590v1-querycraft-transformer-guided-query-initialization-for-enhanced-huma.html">QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection</a></td>
  <td>æå‡ºQueryCraftä»¥è§£å†³HOIæ£€æµ‹ä¸­æŸ¥è¯¢åˆå§‹åŒ–ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.08590v1" data-paper-url="./papers/250808590v1-querycraft-transformer-guided-query-initialization-for-enhanced-huma.html" onclick="toggleFavorite(this, '2508.08590v1', 'QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250809325v2-segdac-improving-visual-reinforcement-learning-by-extracting-dynamic.html">SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Objectc-Centric Representations from Pretrained Vision Models</a></td>
  <td>æå‡ºSegDACä»¥è§£å†³è§†è§‰å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨æ€å¯¹è±¡è¡¨ç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.09325v2" data-paper-url="./papers/250809325v2-segdac-improving-visual-reinforcement-learning-by-extracting-dynamic.html" onclick="toggleFavorite(this, '2508.09325v2', 'SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Objectc-Centric Representations from Pretrained Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)