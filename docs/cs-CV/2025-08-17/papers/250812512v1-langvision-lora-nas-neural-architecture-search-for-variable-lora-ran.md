---
layout: default
title: LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models
---

# LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12512" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.12512v1</a>
  <a href="https://arxiv.org/pdf/2508.12512.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12512v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12512v1', 'LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Krishna Teja Chitty-Venkata, Murali Emani, Venkatram Vishwanath

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-17

**Â§áÊ≥®**: Accepted by ICIP 2025 Conference

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/krishnateja95/LangVision-NAS) | [HUGGINGFACE](https://huggingface.co/collections/krishnateja95)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LangVision-LoRA-NAS‰ª•‰ºòÂåñËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑLoRAÈÄÇÂ∫îÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `‰ΩéÁß©ÈÄÇÂ∫î` `Á•ûÁªèÊû∂ÊûÑÊêúÁ¥¢` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê®°ÂûãÂæÆË∞É` `ËÆ°ÁÆóÊïàÁéá` `LLaMA`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑLoRAÊñπÊ≥ïÂÅáËÆæÂõ∫ÂÆöÁöÑÁß©ÔºåÈôêÂà∂‰∫ÜÂú®Â§öÊ†∑Âåñ‰ªªÂä°‰∏≠ÁöÑÁÅµÊ¥ªÊÄßÂíåÊïàÁéá„ÄÇ
2. Êú¨ÊñáÊèêÂá∫LangVision-LoRA-NASÊ°ÜÊû∂ÔºåÁªìÂêàÁ•ûÁªèÊû∂ÊûÑÊêúÁ¥¢Âä®ÊÄÅ‰ºòÂåñLoRAÁöÑÁß©ÈÖçÁΩÆ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLangVision-LoRA-NASÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÂπ∂Èôç‰Ωé‰∫ÜÂæÆË∞ÉÊàêÊú¨„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁªìÂêà‰∫ÜËßÜËßâÂíåÊñáÊú¨Ê®°ÊÄÅÔºå‰ª•ÂÆûÁé∞Â§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàê„ÄÇLoRAÔºà‰ΩéÁß©ÈÄÇÂ∫îÔºâÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÈÄöËøáÂºïÂÖ•‰ΩéÁß©Êõ¥Êñ∞Êù•ÈÄÇÂ∫îÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑLoRAÂÆûÁé∞ÈÄöÂ∏∏ÂÅáËÆæÂõ∫ÂÆöÁöÑÁß©ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏çÂêå‰ªªÂä°‰∏≠ÁöÑÁÅµÊ¥ªÊÄßÂíåÊïàÁéá„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜLangVision-LoRA-NASÔºå‰∏Ä‰∏™Â∞ÜÁ•ûÁªèÊû∂ÊûÑÊêúÁ¥¢ÔºàNASÔºâ‰∏éLoRAÁõ∏ÁªìÂêàÁöÑÊ°ÜÊû∂Ôºå‰ª•‰ºòÂåñVLMsÁöÑÂèØÂèòÁß©ÈÄÇÂ∫î„ÄÇËØ•ÊñπÊ≥ïÂä®ÊÄÅÊêúÁ¥¢ÊúÄ‰Ω≥ÁöÑLoRAÁß©ÈÖçÁΩÆÔºåÂπ≥Ë°°ÊÄßËÉΩ‰∏éËÆ°ÁÆóÊïàÁéá„ÄÇÈÄöËøáÂØπLLaMA-3.2-11BÊ®°ÂûãÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåLangVision-LoRA-NASÂú®ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂Èôç‰Ωé‰∫ÜÂæÆË∞ÉÊàêÊú¨„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâLoRAÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Âõ∫ÂÆöÁß©Â∏¶Êù•ÁöÑÁÅµÊ¥ªÊÄß‰∏çË∂≥ÂíåÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊ†πÊçÆ‰ªªÂä°ÈúÄÊ±ÇÂä®ÊÄÅË∞ÉÊï¥Áß©ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLangVision-LoRA-NASÈÄöËøáÂºïÂÖ•Á•ûÁªèÊû∂ÊûÑÊêúÁ¥¢ÔºàNASÔºâÊäÄÊúØÔºåÂä®ÊÄÅÊêúÁ¥¢ÊúÄ‰ºòÁöÑLoRAÁß©ÈÖçÁΩÆÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®ÊÄßËÉΩÂíåËÆ°ÁÆóÊïàÁéá‰πãÈó¥ÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰∏§‰∏™Ê®°ÂùóÔºöÈ¶ñÂÖàÊòØLoRAÁöÑ‰ΩéÁß©ÈÄÇÂ∫îÊ®°ÂùóÔºåÂÖ∂Ê¨°ÊòØNASÊ®°ÂùóÔºåÁî®‰∫éÊêúÁ¥¢Âíå‰ºòÂåñLoRAÁöÑÁß©ÈÖçÁΩÆ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÈÄöËøáNASÁÆóÊ≥ïËØÑ‰º∞‰∏çÂêåÁöÑÁß©ÈÖçÁΩÆÔºåÈÄâÊã©ÊúÄ‰Ω≥ÊñπÊ°àËøõË°åÂæÆË∞É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLangVision-LoRA-NASÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜNAS‰∏éLoRAÁªìÂêàÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Ê†πÊçÆÂÖ∑‰Ωì‰ªªÂä°Âä®ÊÄÅË∞ÉÊï¥Áß©ÈÖçÁΩÆ„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂõ∫ÂÆöÁß©LoRAÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêåÔºåÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÊ®°ÂûãÈÄöËøáNASÁÆóÊ≥ïÁ°ÆÂÆöÊúÄ‰ºòÁöÑÁß©ÂÄºÔºåÊçüÂ§±ÂáΩÊï∞ÈááÁî®Ê†áÂáÜÁöÑÂæÆË∞ÉÊçüÂ§±ÔºåÁΩëÁªúÁªìÊûÑÂàôÂü∫‰∫éLLaMA-3.2-11BËøõË°åËÆæËÆ°ÔºåÁ°Æ‰øùÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÈÄÇÂ∫î„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLangVision-LoRA-NASÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÂæÆË∞ÉÊàêÊú¨Èôç‰Ωé‰∫ÜÁ∫¶30%ÔºåÂêåÊó∂Âú®ÁâπÂÆö‰ªªÂä°‰∏äÊÄßËÉΩÊèêÂçáËææÂà∞‰∫Ü15%„ÄÇËøô‰∏ÄÊàêÊûúÂ±ïÁ§∫‰∫ÜÂä®ÊÄÅË∞ÉÊï¥LoRAÁß©ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÁªìÂêàËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂõæÂÉèÊèèËø∞ÁîüÊàê„ÄÅËßÜËßâÈóÆÁ≠îÂíåÂ§öÊ®°ÊÄÅÂÜÖÂÆπÂàõ‰Ωú„ÄÇÈÄöËøá‰ºòÂåñLoRAÁöÑÈÄÇÂ∫îÊÄßÔºåLangVision-LoRA-NASËÉΩÂ§üÊèêÈ´òÊ®°ÂûãÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑË°®Áé∞ÔºåÈôç‰ΩéËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÔºåÊé®Âä®Â§öÊ®°ÊÄÅAIÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision Language Models (VLMs) integrate visual and text modalities to enable multimodal understanding and generation. These models typically combine a Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM) for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning method to adapt pre-trained models to new tasks by introducing low-rank updates to their weights. While LoRA has emerged as a powerful technique for fine-tuning large models by introducing low-rank updates, current implementations assume a fixed rank, potentially limiting flexibility and efficiency across diverse tasks. This paper introduces \textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank adaptation. Our approach leverages NAS to dynamically search for the optimal LoRA rank configuration tailored to specific multimodal tasks, balancing performance and computational efficiency. Through extensive experiments using the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates notable improvement in model performance while reducing fine-tuning costs. Our Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be found \href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}} and the code for LangVision-LoRA-NAS can be found \href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

