---
layout: default
title: MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation
---

# MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18678" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.18678v2</a>
  <a href="https://arxiv.org/pdf/2506.18678.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18678v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18678v2', 'MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-23 (Êõ¥Êñ∞: 2025-08-19)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/dtc111111/mcnslam)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MCN-SLAM‰ª•Ëß£ÂÜ≥Â§ö‰ª£ÁêÜÂçè‰ΩúSLAM‰∏≠ÁöÑÈÄö‰ø°Â∏¶ÂÆΩÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ö‰ª£ÁêÜSLAM` `Á•ûÁªèÈöêÂºèË°®Á§∫` `Âú∫ÊôØÈáçÂª∫` `Âú®Á∫øËí∏È¶è` `ÂõûÁéØÈó≠Âêà` `Êï∞ÊçÆÈõÜ` `ËßÜËßâSLAM` `ÂàÜÂ∏ÉÂºèÁ≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈöêÂºèSLAMÁÆóÊ≥ïÂ±ÄÈôê‰∫éÂçï‰ª£ÁêÜÂú∫ÊôØÔºåÈöæ‰ª•Â§ÑÁêÜÂ§ßËßÑÊ®°ÁéØÂ¢ÉÂíåÈïøÂ∫èÂàóÔºå‰∏îÈÄö‰ø°Â∏¶ÂÆΩÈôêÂà∂ÂΩ±ÂìçÂ§ö‰ª£ÁêÜÂçè‰Ωú„ÄÇ
2. ÊèêÂá∫‰∫ÜÂàÜÂ∏ÉÂºèÂ§ö‰ª£ÁêÜÂçè‰ΩúÁ•ûÁªèSLAMÊ°ÜÊû∂ÔºåÁªìÂêàÊ∑∑ÂêàÂú∫ÊôØË°®Á§∫ÂíåÂú®Á∫øËí∏È¶èÊñπÊ≥ïÔºå‰ª•ÂÆûÁé∞Â§ö‰∏™Â≠êÂõæÁöÑÊúâÊïàËûçÂêàÂíå‰∏ÄËá¥ÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Êò†Â∞ÑÁ≤æÂ∫¶„ÄÅË∑üË∏™Á®≥ÂÆöÊÄßÂíåÈÄö‰ø°ÊïàÁéáÊñπÈù¢Ë°®Áé∞Á™ÅÂá∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á•ûÁªèÈöêÂºèÂú∫ÊôØË°®Á§∫Âú®ÂØÜÈõÜËßÜËßâSLAM‰∏≠Â±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÊïàÊûú„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÈöêÂºèSLAMÁÆóÊ≥ï‰ªÖÈôê‰∫éÂçï‰ª£ÁêÜÂú∫ÊôØÔºå‰∏îÂú®Â§ßËßÑÊ®°Âú∫ÊôØÂíåÈïøÂ∫èÂàó‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÂü∫‰∫éNeRFÁöÑÂ§ö‰ª£ÁêÜSLAMÊ°ÜÊû∂Êó†Ê≥ïÊª°Ë∂≥ÈÄö‰ø°Â∏¶ÂÆΩÁöÑÈôêÂà∂„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÈ¶ñ‰∏™ÂàÜÂ∏ÉÂºèÂ§ö‰ª£ÁêÜÂçè‰ΩúÁ•ûÁªèSLAMÊ°ÜÊû∂ÔºåÁªìÂêàÊ∑∑ÂêàÂú∫ÊôØË°®Á§∫„ÄÅÂàÜÂ∏ÉÂºèÁõ∏Êú∫Ë∑üË∏™„ÄÅÂÜÖÈÉ®Âà∞Â§ñÈÉ®ÂõûÁéØÈó≠ÂêàÂèäÂú®Á∫øËí∏È¶è‰ª•ÂÆûÁé∞Â§ö‰∏™Â≠êÂõæÁöÑËûçÂêà„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏âÂπ≥Èù¢ÁΩëÊ†ºËÅîÂêàÂú∫ÊôØË°®Á§∫ÊñπÊ≥ïÔºå‰ª•ÊèêÂçáÂú∫ÊôØÈáçÂª∫ÊïàÊûú„ÄÇÊ≠§Â§ñÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÜÖÈÉ®Âà∞Â§ñÈÉ®ÂõûÁéØÈó≠ÂêàÊñπÊ≥ïÔºå‰ª•ÂÆûÁé∞Â±ÄÈÉ®ÂíåÂÖ®Â±Ä‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÈ¶ñ‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑDense SLAMÔºàDESÔºâÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñÂçï‰ª£ÁêÜÂíåÂ§ö‰ª£ÁêÜÂú∫ÊôØÔºåÊèê‰æõÈ´òÁ≤æÂ∫¶ÁöÑ3DÁΩëÊ†ºÂíåËøûÁª≠Êó∂Èó¥Áõ∏Êú∫ËΩ®ËøπÁöÑÁúüÂÆûÂÄº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Êò†Â∞Ñ„ÄÅË∑üË∏™ÂíåÈÄö‰ø°ÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÈöêÂºèSLAMÁÆóÊ≥ïÂú®Â§ö‰ª£ÁêÜÂú∫ÊôØ‰∏≠ÁöÑÈÄö‰ø°Â∏¶ÂÆΩÈôêÂà∂ÂíåÂ§ßËßÑÊ®°Âú∫ÊôØÂ§ÑÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÈïøÂ∫èÂàóÂíåÂ§çÊùÇÁéØÂ¢É‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈöæ‰ª•ÂÆûÁé∞È´òÊïàÁöÑÂçè‰Ωú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑMCN-SLAMÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•Ê∑∑ÂêàÂú∫ÊôØË°®Á§∫ÂíåÂú®Á∫øËí∏È¶èÊäÄÊúØÔºåÂÆûÁé∞‰∫ÜÂ§ö‰ª£ÁêÜ‰πãÈó¥ÁöÑ‰ø°ÊÅØÂÖ±‰∫´‰∏éËûçÂêàÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÁ≥ªÁªüÁöÑÊï¥‰ΩìÊÄßËÉΩÂíå‰∏ÄËá¥ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨Â§ö‰∏™Ê®°ÂùóÔºöÊ∑∑ÂêàÂú∫ÊôØË°®Á§∫Ê®°Âùó„ÄÅÂàÜÂ∏ÉÂºèÁõ∏Êú∫Ë∑üË∏™Ê®°Âùó„ÄÅÂÜÖÈÉ®Âà∞Â§ñÈÉ®ÂõûÁéØÈó≠ÂêàÊ®°ÂùóÂíåÂú®Á∫øËí∏È¶èÊ®°Âùó„ÄÇÊØè‰∏™Ê®°ÂùóÂçèÂêåÂ∑•‰ΩúÔºå‰ª•Á°Æ‰øùÂú®‰∏çÂêå‰ª£ÁêÜ‰πãÈó¥ÂÆûÁé∞È´òÊïàÁöÑ‰ø°ÊÅØ‰º†ÈÄíÂíå‰∏ÄËá¥ÊÄßÁª¥Êä§„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏âÂπ≥Èù¢ÁΩëÊ†ºËÅîÂêàÂú∫ÊôØË°®Á§∫ÊñπÊ≥ïÂíåÊñ∞ÁöÑÂÜÖÈÉ®Âà∞Â§ñÈÉ®ÂõûÁéØÈó≠ÂêàÊñπÊ≥ïÔºåËøô‰∫õÊñπÊ≥ïÊúâÊïàÊèêÂçá‰∫ÜÂú∫ÊôØÈáçÂª∫ÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏ÄËá¥ÊÄßÔºåÂå∫Âà´‰∫é‰º†ÁªüÁöÑÂçï‰ª£ÁêÜSLAMÊñπÊ≥ï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÂú∫ÊôØÈáçÂª∫Ë¥®ÈáèÔºåÂπ∂ÈÄöËøáË∞ÉÊï¥ÁΩëÁªúÁªìÊûÑÊù•ÈÄÇÂ∫îÂ§ö‰ª£ÁêÜÂçè‰ΩúÁöÑÈúÄÊ±ÇÔºåÁ°Æ‰øùÂú®‰∏çÂêåÁéØÂ¢É‰∏ãÁöÑÈ´òÊïàÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÊû∂ÊûÑÁªÜËäÇÂ∞ÜÂú®ËÆ∫Êñá‰∏≠ËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMCN-SLAMÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁõ∏ËæÉ‰∫éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÂú®Êò†Â∞ÑÁ≤æÂ∫¶‰∏äÊèêÂçá‰∫ÜÁ∫¶20%ÔºåÂú®Ë∑üË∏™Á®≥ÂÆöÊÄß‰∏äÊèêÂçá‰∫Ü15%ÔºåÂêåÊó∂ÈÄö‰ø°ÊïàÁéáÊèêÈ´ò‰∫Ü30%„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜÊâÄÊèêÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™ÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠â„ÄÇÈÄöËøáÂÆûÁé∞È´òÊïàÁöÑÂ§ö‰ª£ÁêÜÂçè‰ΩúSLAMÔºåËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Êèê‰æõÊõ¥‰∏∫Á≤æÂáÜÁöÑÂÆö‰ΩçÂíåÂú∞ÂõæÊûÑÂª∫ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÊúâÊúõÂú®Êô∫ËÉΩÂüéÂ∏ÇÂíåÊó†‰∫∫Êú∫Á≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on https://github.com/dtc111111/mcnslam.

