---
layout: default
title: Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations
---

# Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18898" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18898v1</a>
  <a href="https://arxiv.org/pdf/2506.18898.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18898v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18898v1', 'Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

**å¤‡æ³¨**: Project page: https://tar.csuhan.com

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ¡†æ¶ä»¥ç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¡†æ¶` `è§†è§‰ç†è§£` `ç”Ÿæˆæ¨¡å‹` `æ–‡æœ¬å¯¹é½` `è·¨æ¨¡æ€èåˆ` `é«˜ä¿çœŸè¾“å‡º` `è‡ªå›å½’æ¨¡å‹` `æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ–¹æ³•åœ¨è§†è§‰ç†è§£ä¸ç”Ÿæˆä¹‹é—´ç¼ºä¹æœ‰æ•ˆçš„ç»Ÿä¸€ï¼Œå¯¼è‡´è·¨æ¨¡æ€ä»»åŠ¡çš„æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„TA-Toké€šè¿‡æ–‡æœ¬å¯¹é½çš„æ–¹å¼å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ï¼Œæ•´åˆè§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯ï¼Œå½¢æˆç»Ÿä¸€çš„è¡¨ç¤ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTaråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€LLMæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å…±äº«çš„ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºæ¥ç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆã€‚æ ¸å¿ƒæ˜¯æ–‡æœ¬å¯¹é½çš„æ ‡è®°å™¨ï¼ˆTA-Tokï¼‰ï¼Œè¯¥æ ‡è®°å™¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯æ±‡å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ã€‚é€šè¿‡å°†è§†è§‰å’Œæ–‡æœ¬æ•´åˆåˆ°ä¸€ä¸ªæ‰©å±•çš„è¯æ±‡ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€LLM Tarå®ç°äº†è·¨æ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œæ— éœ€ç‰¹å®šäºæ¨¡æ€çš„è®¾è®¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†è§„æ¨¡è‡ªé€‚åº”çš„ç¼–ç å’Œè§£ç æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ•ˆç‡ä¸è§†è§‰ç»†èŠ‚ï¼Œå¹¶å¼•å…¥ç”Ÿæˆå»æ ‡è®°å™¨ä»¥ç”Ÿæˆé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚ä¸ºæ»¡è¶³å¤šæ ·åŒ–çš„è§£ç éœ€æ±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ç§äº’è¡¥çš„å»æ ‡è®°å™¨ï¼šå¿«é€Ÿè‡ªå›å½’æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹ã€‚é€šè¿‡å…ˆè¿›çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¢å¼ºæ¨¡æ€èåˆï¼Œå®éªŒç»“æœè¡¨æ˜Taråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸ç°æœ‰å¤šæ¨¡æ€LLMæ–¹æ³•ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œå±•ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ–¹æ³•åœ¨è§†è§‰ç†è§£ä¸ç”Ÿæˆä¹‹é—´ç¼ºä¹æœ‰æ•ˆç»Ÿä¸€çš„é—®é¢˜ï¼Œå¯¼è‡´è·¨æ¨¡æ€ä»»åŠ¡çš„æ•ˆç‡ä½ä¸‹å’Œæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ–‡æœ¬å¯¹é½çš„æ ‡è®°å™¨ï¼ˆTA-Tokï¼‰å°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ï¼Œä»è€Œå®ç°è§†è§‰ä¸æ–‡æœ¬çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œæ¶ˆé™¤æ¨¡æ€ç‰¹å®šè®¾è®¡çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬TA-Tokæ¨¡å—ã€è·¨æ¨¡æ€LLM Tarã€è§„æ¨¡è‡ªé€‚åº”ç¼–ç è§£ç æ¨¡å—ä»¥åŠç”Ÿæˆå»æ ‡è®°å™¨ã€‚TA-Tokè´Ÿè´£å°†å›¾åƒè½¬åŒ–ä¸ºç¦»æ•£æ ‡è®°ï¼ŒTarå®ç°è·¨æ¨¡æ€è¾“å…¥è¾“å‡ºï¼Œè§£ç æ¨¡å—åˆ™æ ¹æ®éœ€æ±‚ç”Ÿæˆé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†TA-Tokå’Œç”Ÿæˆå»æ ‡è®°å™¨çš„ç»“åˆï¼Œåˆ©ç”¨æ–‡æœ¬å¯¹é½çš„æ–¹å¼å®ç°äº†è§†è§‰ä¸æ–‡æœ¬çš„æ·±åº¦èåˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€ä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†è§„æ¨¡è‡ªé€‚åº”çš„ç¼–ç å’Œè§£ç ç­–ç•¥ï¼Œä»¥å¹³è¡¡æ•ˆç‡ä¸è§†è§‰ç»†èŠ‚ï¼ŒåŒæ—¶å¼•å…¥äº†å¿«é€Ÿè‡ªå›å½’æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„å»æ ‡è®°å™¨ï¼Œä»¥æ»¡è¶³ä¸åŒçš„è§£ç éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTaråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ”¶æ•›é€Ÿåº¦æ¯”ç°æœ‰å¤šæ¨¡æ€LLMæ–¹æ³•å¿«ï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†æ˜¾è‘—ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªè¯¦è¿°ï¼Œä½†æ•´ä½“æ•ˆæœè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è§†è§‰ç³»ç»Ÿã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆã€è·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡ç»Ÿä¸€çš„è§†è§‰ç†è§£ä¸ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥æå‡å¤šæ¨¡æ€ä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com

