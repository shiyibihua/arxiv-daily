---
layout: default
title: Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning
---

# Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18234" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18234v1</a>
  <a href="https://arxiv.org/pdf/2506.18234.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18234v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18234v1', 'Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Li, Meng Tian, Dechang Zhu, Jiangtong Zhu, Zhenyu Lin, Zhiwei Xiong, Xinhai Zhao

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDrive-R1ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†ä¸è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨é©¾é©¶` `è¿åŠ¨è§„åˆ’` `æ¨ç†ä¸è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `é•¿çŸ­æ¨ç†é“¾` `æ•°æ®é›†å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿åŠ¨è§„åˆ’ä¸­å­˜åœ¨ä¾èµ–å†å²è¾“å…¥ã€ç¼ºä¹å¯¹è§†è§‰ä¿¡æ¯ç†è§£çš„çŸ­æ¿ã€‚
2. Drive-R1é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä¿ƒè¿›é€æ­¥æ¨ç†ä¸è¿åŠ¨è§„åˆ’çš„æœ‰æ•ˆç»“åˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºDrive-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„VLMï¼Œæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸæ­£é€æ­¥ä»æ„ŸçŸ¥å’Œè®¤çŸ¥ä»»åŠ¡å‘è¿åŠ¨è§„åˆ’è½¬å˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯VLMsè¿‡äºä¾èµ–å†å²è¾“å…¥ä¿¡æ¯ï¼Œå¯¼è‡´è§„åˆ’ç»“æœçœ‹ä¼¼å¼ºå¤§ä½†ç¼ºä¹å¯¹è§†è§‰è¾“å…¥çš„çœŸæ­£ç†è§£ï¼›äºŒæ˜¯æ¨ç†è¿‡ç¨‹ä¸è¿åŠ¨è§„åˆ’ç»“æœä¹‹é—´çš„é”™ä½ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤æ‚æ¨ç†èƒ½åŠ›ä»¥å¢å¼ºè§„åˆ’ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºDrive-R1ï¼Œæ—¨åœ¨æ¡¥æ¥åœºæ™¯æ¨ç†ä¸è¿åŠ¨è§„åˆ’ã€‚Drive-R1é¦–å…ˆåœ¨åŒ…å«é•¿çŸ­æ¨ç†é“¾çš„æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œé¼“åŠ±é€æ­¥æ¨ç†ã€‚éšåï¼Œåœ¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹è®­ç»ƒDrive-R1ï¼Œä»¥å‘ç°æ›´å…·ä¿¡æ¯é‡çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDrive-R1åœ¨nuSceneså’ŒDriveLM-nuScenesåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå±•ç°äº†åœ¨ADé¢†åŸŸä¸­æ¨ç†ä¸è§„åˆ’ç»“åˆçš„å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ¨ç†ä¸è§„åˆ’çš„é”™ä½é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å†å²è¾“å…¥ï¼Œç¼ºä¹å¯¹è§†è§‰ä¿¡æ¯çš„æ·±åˆ»ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDrive-R1é€šè¿‡åœ¨å°è§„æ¨¡é¢†åŸŸç‰¹å®šVLMä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œé¼“åŠ±æ¨¡å‹é€æ­¥æ¨ç†ï¼Œä»è§†è§‰è¾“å…¥åˆ°æœ€ç»ˆè§„åˆ’å†³ç­–ï¼Œè¿›è€Œåœ¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹ä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDrive-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯ç›‘ç£å¾®è°ƒï¼Œä½¿ç”¨åŒ…å«é•¿çŸ­æ¨ç†é“¾çš„æ•°æ®é›†ï¼›å…¶æ¬¡æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è®­ç»ƒï¼Œåˆ©ç”¨åŸºäºé¢„æµ‹è½¨è¿¹å’Œå…ƒåŠ¨ä½œçš„å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼æ¨ç†è·¯å¾„çš„å‘ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDrive-R1çš„åˆ›æ–°åœ¨äºæœ‰æ•ˆç»“åˆæ¨ç†ä¸è§„åˆ’ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼æ¨¡å‹å‘ç°æ›´å…·ä¿¡æ¯é‡çš„æ¨ç†è·¯å¾„ï¼Œè¿™ä¸€æ–¹æ³•åœ¨ç°æœ‰VLMä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDrive-R1é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ¨ç†ä¸è§„åˆ’çš„ç›®æ ‡ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§å¥–åŠ±æœºåˆ¶ï¼Œä»¥é¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†è°ƒä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨nuSceneså’ŒDriveLM-nuScenesåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDrive-R1çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†ä¸è§„åˆ’ç»“åˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Drive-R1çš„ç ”ç©¶æˆæœåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤æ‚æ¨ç†ä¸è§„åˆ’çš„æ™ºèƒ½ç³»ç»Ÿä¸­ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (VLMs) for autonomous driving (AD) are evolving beyond perception and cognition tasks toward motion planning. However, we identify two critical challenges in this direction: (1) VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs; and (2) the chain-ofthought (COT) reasoning processes are always misaligned with the motion planning outcomes, and how to effectively leverage the complex reasoning capability to enhance planning remains largely underexplored. In this paper, we start from a small-scale domain-specific VLM and propose Drive-R1 designed to bridges the scenario reasoning and motion planning for AD. Drive-R1 first undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions. Subsequently, Drive-R1 is trained within a reinforcement learning framework that incentivizes the discovery of reasoning paths that are more informative for planning, guided by rewards based on predicted trajectories and meta actions. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that Drive-R1 achieves superior performance compared to existing state-of-the-art VLMs. We believe that Drive-R1 presents a promising direction for bridging reasoning and planning in AD, offering methodological insights for future research and applications.

