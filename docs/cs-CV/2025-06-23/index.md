---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-23
---

# cs.CVï¼ˆ2025-06-23ï¼‰

ğŸ“Š å…± **6** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250618871v3-omnigen2-exploration-to-advanced-multimodal-generation.html">OmniGen2: Exploration to Advanced Multimodal Generation</a></td>
  <td>æå‡ºOmniGen2ä»¥è§£å†³å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„ç»Ÿä¸€é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.18871v3" data-paper-url="./papers/250618871v3-omnigen2-exploration-to-advanced-multimodal-generation.html" onclick="toggleFavorite(this, '2506.18871v3', 'OmniGen2: Exploration to Advanced Multimodal Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250618898v1-vision-as-a-dialect-unifying-visual-understanding-and-generation-via.html">Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ¡†æ¶ä»¥ç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.18898v1" data-paper-url="./papers/250618898v1-vision-as-a-dialect-unifying-visual-understanding-and-generation-via.html" onclick="toggleFavorite(this, '2506.18898v1', 'Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250700045v1-caughtcheating-is-your-mllm-a-good-cheating-detective-exploring-the-.html">CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning</a></td>
  <td>æå‡ºCaughtCheatingä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.00045v1" data-paper-url="./papers/250700045v1-caughtcheating-is-your-mllm-a-good-cheating-detective-exploring-the-.html" onclick="toggleFavorite(this, '2507.00045v1', 'CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>4</td>
  <td><a href="./papers/250618678v2-mcn-slam-multi-agent-collaborative-neural-slam-with-hybrid-implicit-.html">MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation</a></td>
  <td>æå‡ºMCN-SLAMä»¥è§£å†³å¤šä»£ç†åä½œSLAMä¸­çš„é€šä¿¡å¸¦å®½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">visual SLAM</span> <span class="paper-tag">NeRF</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.18678v2" data-paper-url="./papers/250618678v2-mcn-slam-multi-agent-collaborative-neural-slam-with-hybrid-implicit-.html" onclick="toggleFavorite(this, '2506.18678v2', 'MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250618234v1-drive-r1-bridging-reasoning-and-planning-in-vlms-for-autonomous-driv.html">Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning</a></td>
  <td>æå‡ºDrive-R1ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†ä¸è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.18234v1" data-paper-url="./papers/250618234v1-drive-r1-bridging-reasoning-and-planning-in-vlms-for-autonomous-driv.html" onclick="toggleFavorite(this, '2506.18234v1', 'Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>6</td>
  <td><a href="./papers/250702900v1-advancing-talking-head-generation-a-comprehensive-survey-of-multi-mo.html">Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions</a></td>
  <td>ç»¼è¿°å¤šæ¨¡æ€æ–¹æ³•ä»¥æ¨è¿›å¯¹è¯å¤´ç”ŸæˆæŠ€æœ¯</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2507.02900v1" data-paper-url="./papers/250702900v1-advancing-talking-head-generation-a-comprehensive-survey-of-multi-mo.html" onclick="toggleFavorite(this, '2507.02900v1', 'Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)