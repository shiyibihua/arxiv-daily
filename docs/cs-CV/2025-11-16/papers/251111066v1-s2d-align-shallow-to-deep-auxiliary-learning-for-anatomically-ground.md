---
layout: default
title: S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation
---

# S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation

**arXiv**: [2511.11066v1](https://arxiv.org/abs/2511.11066) | [PDF](https://arxiv.org/pdf/2511.11066.pdf)

**ä½œè€…**: Jiechao Gao, Chang Liu, Yuangang Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS2D-ALIGNæ–¹æ³•ï¼Œé€šè¿‡æµ…åˆ°æ·±è¾…åŠ©å­¦ä¹ è§£å†³æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­è§£å‰–åŸºç¡€å¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `ç›‘ç£å¾®è°ƒ` `è§£å‰–åŸºç¡€å¯¹é½` `è¾…åŠ©å­¦ä¹ ` `è®°å¿†é€‚é…å™¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ ‡å‡†ç›‘ç£å¾®è°ƒåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­ç¼ºä¹è§£å‰–åŸºç¡€å¯¹é½ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨æµ…åˆ°æ·±ç­–ç•¥ï¼Œä»Žç²—åˆ°ç»†å¼•å…¥è¾…åŠ©ä¿¡å·ï¼Œå¹¶ä½¿ç”¨è®°å¿†é€‚é…å™¨æ•´åˆç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MIMIC-CXRå’ŒIU X-RayåŸºå‡†ä¸Šå®žçŽ°æœ€ä¼˜æ€§èƒ½ï¼Œæ¶ˆèžç ”ç©¶éªŒè¯å¤šé˜¶æ®µæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

