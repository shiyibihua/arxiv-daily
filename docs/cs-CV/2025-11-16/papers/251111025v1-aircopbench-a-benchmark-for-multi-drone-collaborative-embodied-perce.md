---
layout: default
title: AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning
---

# AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning

**arXiv**: [2511.11025v1](https://arxiv.org/abs/2511.11025) | [PDF](https://arxiv.org/pdf/2511.11025.pdf)

**ä½œè€…**: Jirong Zha, Yuxuan Fan, Tianyu Zhang, Geng Chen, Yingfeng Chen, Chen Gao, Xinlei Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAirCopBenchåŸºå‡†ä»¥è¯„ä¼°å¤šæ— äººæœºåœ¨é€€åŒ–æ„ŸçŸ¥æ¡ä»¶ä¸‹çš„åä½œæ„ŸçŸ¥ä¸ŽæŽ¨ç†**

**å…³é”®è¯**: `å¤šæ— äººæœºåä½œ` `å¤šæ¨¡æ€å¤§æ¨¡åž‹è¯„ä¼°` `é€€åŒ–æ„ŸçŸ¥åœºæ™¯` `åä½œæ„ŸçŸ¥åŸºå‡†` `ä»¿çœŸåˆ°çœŸå®žè¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨å•æ™ºèƒ½ä½“è§†è§‰ä»»åŠ¡ä¸­è¡¨çŽ°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¤šæ™ºèƒ½ä½“åä½œæ„ŸçŸ¥çš„è¯„ä¼°åŸºå‡†
2. æž„å»ºåŒ…å«14.6k+é—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–åœºæ™¯ç†è§£ã€ç‰©ä½“ç†è§£ã€æ„ŸçŸ¥è¯„ä¼°å’Œåä½œå†³ç­–ç­‰ä»»åŠ¡ç»´åº¦
3. è¯„ä¼°40ä¸ªæ¨¡åž‹æ˜¾ç¤ºåä½œæ„ŸçŸ¥ä»»åŠ¡å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œæœ€ä½³æ¨¡åž‹å¹³å‡è½åŽäººç±»24.38%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.

