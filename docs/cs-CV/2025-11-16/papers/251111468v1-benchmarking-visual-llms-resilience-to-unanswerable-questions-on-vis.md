---
layout: default
title: Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents
---

# Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents

**arXiv**: [2511.11468v1](https://arxiv.org/abs/2511.11468) | [PDF](https://arxiv.org/pdf/2511.11468.pdf)

**ä½œè€…**: Davide Napolitano, Luca Cagliero, Fabrizio Battiloro

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVRD-UQAåŸºå‡†ä»¥è¯„ä¼°è§†è§‰å¤§è¯­è¨€æ¨¡åž‹å¯¹è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸­ä¸å¯ç­”é—®é¢˜çš„éŸ§æ€§**

**å…³é”®è¯**: `è§†è§‰å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰ä¸°å¯Œæ–‡æ¡£` `ä¸å¯ç­”é—®é¢˜æ£€æµ‹` `åŸºå‡†è¯„ä¼°` `å¤šé¡µæ–‡æ¡£ç†è§£` `çŸ¥è¯†æ³¨å…¥ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸­æ£€æµ‹ä¸å¯ç­”é—®é¢˜çš„èƒ½åŠ›å°šä¸æ˜Žç¡®
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡æ›¿æ¢å®žä½“ç”Ÿæˆä¸å¯ç­”é—®é¢˜ï¼Œå¹¶åˆ©ç”¨VLLMä½œä¸ºè¯„åˆ¤è€…éªŒè¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°12ä¸ªæ¨¡åž‹ï¼Œåˆ†æžæ£€æµ‹å‡†ç¡®æ€§å’Œä¸åŒçŸ¥è¯†æ³¨å…¥ç­–ç•¥æ•ˆæžœ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.

