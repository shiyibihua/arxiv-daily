---
layout: default
title: VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models
---

# VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models

**arXiv**: [2511.11438v1](https://arxiv.org/abs/2511.11438) | [PDF](https://arxiv.org/pdf/2511.11438.pdf)

**ä½œè€…**: Mingjie Xu, Jinpeng Chen, Yuzhi Zhao, Jason Chun Lok Li, Yue Qiu, Zekang Du, Mengyang Wu, Pingping Zhang, Kun Li, Hongzheng Yang, Wenao Ma, Jiaheng Wei, Qinbin Li, Kangcheng Liu, Wenqiang Lei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVP-BenchåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†è§‰æç¤ºç†è§£ä¸Žåˆ©ç”¨ä¸­çš„èƒ½åŠ›**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰æç¤ºåŸºå‡†` `è§†è§‰è¯­è¨€ç†è§£` `æ¨¡åž‹è¯„ä¼°` `ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŸºå‡†ç¼ºä¹å¯¹MLLMsè§£é‡Šè§†è§‰æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†ï¼‰èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æž¶ï¼Œè¯„ä¼°è§†è§‰æç¤ºæ„ŸçŸ¥åŠå…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å½±å“
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°28ä¸ªMLLMsï¼Œåˆ†æžè§†è§‰æç¤ºå±žæ€§ã€é—®é¢˜å®‰æŽ’å’Œæ¨¡åž‹è§„æ¨¡ç­‰å› ç´ 

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use "visual prompts" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.

