---
layout: default
title: MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding
---

# MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding

**arXiv**: [2511.12449v1](https://arxiv.org/abs/2511.12449) | [PDF](https://arxiv.org/pdf/2511.12449.pdf)

**ä½œè€…**: Zhanheng Nie, Chenghan Fu, Daoze Zhang, Junxian Wu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng

**åˆ†ç±»**: cs.CV, cs.AI, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-16

**å¤‡æ³¨**: 11 pages, 7 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMOON2.0ä»¥è§£å†³ç”µå•†äº§å“ç†è§£ä¸­çš„å¤šæ¨¡æ€ä¸å¹³è¡¡é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ` `ç”µå•†äº§å“ç†è§£` `æ¨¡æ€å¹³è¡¡` `å›¾æ–‡ååŒå¢žå¼º` `è¯­ä¹‰å¯¹é½` `åŠ¨æ€æ ·æœ¬è¿‡æ»¤` `ä¸“å®¶æ··åˆæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨ç”µå•†äº§å“ç†è§£ä¸­å­˜åœ¨æ¨¡æ€ä¸å¹³è¡¡ã€å†…åœ¨å¯¹é½å…³ç³»åˆ©ç”¨ä¸è¶³å’Œå™ªå£°å¤„ç†èƒ½åŠ›æœ‰é™ç­‰æŒ‘æˆ˜ã€‚
2. MOON2.0é€šè¿‡æ¨¡æ€é©±åŠ¨çš„ä¸“å®¶æ··åˆæ¨¡å—ã€åŒå±‚å¯¹é½æ–¹æ³•å’Œå›¾æ–‡ååŒå¢žå¼ºç­–ç•¥ï¼ŒåŠ¨æ€å¹³è¡¡æ¨¡æ€å¹¶æå‡è¡¨ç¤ºå­¦ä¹ æ•ˆæžœã€‚
3. å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒMOON2.0åœ¨MBE2.0å’Œå¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„é›¶-shotæ€§èƒ½ï¼Œä¸”å¯è§†åŒ–ç»“æžœæ”¯æŒäº†å¤šæ¨¡æ€å¯¹é½çš„æ”¹å–„ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç”µå­å•†åŠ¡çš„å¿«é€Ÿå‘å±•ï¼Œéœ€æ±‚æ—¥ç›Šå¢žé•¿çš„å¤šæ¨¡æ€æ¨¡åž‹éœ€è¦ç†è§£ä¸°å¯Œçš„è§†è§‰å’Œæ–‡æœ¬äº§å“ä¿¡æ¯ã€‚å°½ç®¡è¿‘æœŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨äº§å“ç†è§£æ–¹é¢å±•çŽ°å‡ºå¼ºå¤§çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ¨¡æ€æ··åˆè®­ç»ƒå¯¼è‡´çš„æ¨¡æ€ä¸å¹³è¡¡ã€æœªå……åˆ†åˆ©ç”¨è§†è§‰ä¸Žæ–‡æœ¬ä¿¡æ¯ä¹‹é—´çš„å†…åœ¨å¯¹é½å…³ç³»ï¼Œä»¥åŠå¯¹ç”µå•†å¤šæ¨¡æ€æ•°æ®å™ªå£°çš„å¤„ç†èƒ½åŠ›æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MOON2.0ï¼Œä¸€ä¸ªåŠ¨æ€æ¨¡æ€å¹³è¡¡çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æž¶ã€‚MOON2.0åŒ…æ‹¬æ¨¡æ€é©±åŠ¨çš„ä¸“å®¶æ··åˆæ¨¡å—ã€åŒå±‚å¯¹é½æ–¹æ³•ä»¥åŠåŸºäºŽMLLMçš„å›¾æ–‡ååŒå¢žå¼ºç­–ç•¥ã€‚å®žéªŒè¡¨æ˜Žï¼ŒMOON2.0åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„é›¶-shotæ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç”µå•†äº§å“ç†è§£ä¸­çš„å¤šæ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•åœ¨æ¨¡æ€æ··åˆè®­ç»ƒä¸­å¯¼è‡´æ¨¡æ€ä¿¡æ¯åˆ©ç”¨ä¸å‡ï¼Œä¸”æœªèƒ½æœ‰æ•ˆå¤„ç†è§†è§‰ä¸Žæ–‡æœ¬ä¿¡æ¯çš„å†…åœ¨å¯¹é½å…³ç³»ï¼Œæ­¤å¤–ï¼Œå¯¹å™ªå£°çš„å¤„ç†èƒ½åŠ›ä¹Ÿè¾ƒå¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMOON2.0çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€æ¨¡æ€å¹³è¡¡çš„æ¡†æž¶æ¥æå‡å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæžœã€‚é€šè¿‡æ¨¡æ€é©±åŠ¨çš„ä¸“å®¶æ··åˆæ¨¡å—ï¼Œé€‚åº”æ€§åœ°å¤„ç†è¾“å…¥æ ·æœ¬ï¼Œç¼“è§£æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶é€šè¿‡åŒå±‚å¯¹é½æ–¹æ³•æ›´å¥½åœ°åˆ©ç”¨äº§å“å†…éƒ¨çš„è¯­ä¹‰å¯¹é½ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMOON2.0çš„æ•´ä½“æž¶æž„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨¡æ€é©±åŠ¨çš„ä¸“å®¶æ··åˆæ¨¡å—ã€åŒå±‚å¯¹é½æ–¹æ³•å’ŒåŸºäºŽMLLMçš„å›¾æ–‡ååŒå¢žå¼ºç­–ç•¥ã€‚è¯¥æ¡†æž¶é€šè¿‡åŠ¨æ€æ ·æœ¬è¿‡æ»¤æå‡è®­ç»ƒæ•°æ®è´¨é‡ï¼Œå½¢æˆä¸€ä¸ªé—­çŽ¯çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMOON2.0çš„å…³é”®åˆ›æ–°åœ¨äºŽæ¨¡æ€é©±åŠ¨çš„ä¸“å®¶æ··åˆæ¨¡å—å’ŒåŒå±‚å¯¹é½æ–¹æ³•ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„å†…åœ¨å…³ç³»ï¼Œæ˜¾è‘—æå‡äº†è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæžœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒMOON2.0é‡‡ç”¨äº†åŠ¨æ€æ ·æœ¬è¿‡æ»¤æœºåˆ¶ï¼Œä»¥æé«˜è®­ç»ƒæ•°æ®çš„è´¨é‡ï¼Œå¹¶ç»“åˆäº†å¤šæ¨¡æ€çš„ååŒå¢žå¼ºç­–ç•¥ï¼Œç¡®ä¿è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMOON2.0åœ¨MBE2.0åŸºå‡†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„é›¶-shotæ€§èƒ½ï¼Œç›¸è¾ƒäºŽçŽ°æœ‰åŸºçº¿æ¨¡åž‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚æ­¤å¤–ï¼ŒåŸºäºŽæ³¨æ„åŠ›çš„çƒ­å›¾å¯è§†åŒ–ç»“æžœè¿›ä¸€æ­¥éªŒè¯äº†MOON2.0åœ¨å¤šæ¨¡æ€å¯¹é½æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå•†äº§å“æŽ¨èã€æ™ºèƒ½æœç´¢å¼•æ“Žå’Œç”¨æˆ·è¡Œä¸ºåˆ†æžç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒMOON2.0èƒ½å¤Ÿä¸ºç”µå•†å¹³å°æä¾›æ›´ç²¾å‡†çš„äº§å“åŒ¹é…å’ŒæŽ¨èæœåŠ¡ï¼Œè¿›è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œè½¬åŒ–çŽ‡ï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

