---
layout: default
title: DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition
---

# DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition

**arXiv**: [2511.10948v1](https://arxiv.org/abs/2511.10948) | [PDF](https://arxiv.org/pdf/2511.10948.pdf)

**ä½œè€…**: Ren Zhang, Huilai Li, Chao qi, Guoliang Xu, Tianyu Zhou, Wei wei, Jianqin Yin

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDEFT-LLMä»¥è§£å†³å¾®è¡¨æƒ…è¯†åˆ«ä¸­çš„è¿åŠ¨è¯­ä¹‰å¯¹é½é—®é¢˜**

**å…³é”®è¯**: `å¾®è¡¨æƒ…è¯†åˆ«` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è¿åŠ¨è¯­ä¹‰å¯¹é½` `ä¸“å®¶è§£è€¦` `æŒ‡ä»¤æ•°æ®é›†` `å¯è§£é‡Šå»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé™æ€å¤–è§‚ä¸ŽåŠ¨æ€è¿åŠ¨çº¿ç´¢çº ç¼ ï¼Œæ–‡æœ¬æ ‡ç­¾ä¸Žé¢éƒ¨è‚Œè‚‰è¿åŠ¨å­˜åœ¨è¯­ä¹‰é¸¿æ²Ÿã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šä¸“å®¶è§£è€¦å°†é¢éƒ¨åŠ¨æ€åˆ†è§£ä¸ºç»“æž„ã€åŠ¨æ€çº¹ç†å’Œè¿åŠ¨è¯­ä¹‰è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªMERåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œæå‡å±€éƒ¨é¢éƒ¨è¿åŠ¨çš„å¯è§£é‡Šå»ºæ¨¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

