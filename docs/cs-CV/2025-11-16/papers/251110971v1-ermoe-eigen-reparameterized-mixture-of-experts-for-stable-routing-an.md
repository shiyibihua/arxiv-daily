---
layout: default
title: ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization
---

# ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization

**arXiv**: [2511.10971v1](https://arxiv.org/abs/2511.10971) | [PDF](https://arxiv.org/pdf/2511.10971.pdf)

**ä½œè€…**: Anzhe Cheng, Shukai Duan, Shixuan Li, Chenzhong Yin, Mingxi Cheng, Heng Ping, Tamoghna Chattopadhyay, Sophia I Thomopoulos, Shahin Nazarian, Paul Thompson, Paul Bogdan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºERMoEä»¥è§£å†³MoEè·¯ç”±ä¸ç¨³å®šå’Œä¸“å®¶åˆ©ç”¨ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡åž‹` `ç¨€ç–æ¿€æ´»` `è·¯ç”±ç¨³å®šæ€§` `ç‰¹å¾å‘é‡åŸº` `è·¨æ¨¡æ€æ£€ç´¢` `è„‘å¹´é¾„é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MoEæž¶æž„ä¸­è·¯ç”±å™¨ä¸Žä¸“å®¶ç»“æž„ä¸åŒ¹é…å¯¼è‡´è·¯ç”±ä¸ç¨³å®šå’Œè´Ÿè½½ä¸å‡
2. ä½¿ç”¨ç‰¹å¾å‘é‡åŸºé‡å‚æ•°åŒ–ä¸“å®¶ï¼ŒåŸºäºŽä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œå†…å®¹æ„ŸçŸ¥è·¯ç”±
3. åœ¨ImageNetå’Œè·¨æ¨¡æ€æ£€ç´¢ä¸­å®žçŽ°SOTAï¼Œæå‡è„‘å¹´é¾„é¢„æµ‹å‡†ç¡®çŽ‡è¶…7%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an "Eigenbasis Score", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

