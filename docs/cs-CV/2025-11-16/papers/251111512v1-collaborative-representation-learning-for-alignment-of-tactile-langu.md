---
layout: default
title: Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities
---

# Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities

**arXiv**: [2511.11512v1](https://arxiv.org/abs/2511.11512) | [PDF](https://arxiv.org/pdf/2511.11512.pdf)

**ä½œè€…**: Yiyun Zhou, Mingjing Xu, Jingwei Shi, Quanjiang Li, Jingyuan Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTLV-CoReæ–¹æ³•ä»¥è§£å†³è§¦è§‰ã€è¯­è¨€å’Œè§†è§‰æ¨¡æ€å¯¹é½ä¸­çš„ä¼ æ„Ÿå™¨å†—ä½™å’Œäº¤äº’ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `è§¦è§‰è¡¨ç¤ºå­¦ä¹ ` `å¤šæ¨¡æ€å¯¹é½` `ä¼ æ„Ÿå™¨æ³›åŒ–` `CLIPæ¨¡åž‹` `è·¨æ¨¡æ€äº¤äº’` `RSSè¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§¦è§‰ä¼ æ„Ÿå™¨ç¼ºä¹æ ‡å‡†åŒ–ï¼Œå¯¼è‡´å†—ä½™ç‰¹å¾å’Œè·¨ä¼ æ„Ÿå™¨æ³›åŒ–å›°éš¾ï¼Œä¸”çŽ°æœ‰æ–¹æ³•æœªå……åˆ†æ•´åˆå¤šæ¨¡æ€ä¸­é—´é€šä¿¡
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥ä¼ æ„Ÿå™¨æ„ŸçŸ¥è°ƒåˆ¶å™¨ç»Ÿä¸€è§¦è§‰ç‰¹å¾ï¼Œé‡‡ç”¨è§¦è§‰æ— å…³è§£è€¦å­¦ä¹ åˆ†ç¦»æ— å…³ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€æ¡¥æŽ¥é€‚é…å™¨å¢žå¼ºä¸‰æ¨¡æ€äº¤äº’
3. å®žéªŒæˆ–æ•ˆæžœï¼šæå‡ºRSSè¯„ä¼°æ¡†æž¶ï¼Œå®žéªŒæ˜¾ç¤ºTLV-CoReæ˜¾è‘—æå‡ä¼ æ„Ÿå™¨æ— å…³è¡¨ç¤ºå­¦ä¹ å’Œè·¨æ¨¡æ€å¯¹é½æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.

