---
layout: default
title: DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding
---

# DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding

**arXiv**: [2511.11232v1](https://arxiv.org/abs/2511.11232) | [PDF](https://arxiv.org/pdf/2511.11232.pdf)

**ä½œè€…**: Mingwei Xing, Xinliang Wang, Yifeng Shi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDoReMiæ¡†æž¶ä»¥è§£å†³å¤šæºç‚¹äº‘æ³›åŒ–é—®é¢˜**

**å…³é”®è¯**: `3Dç‚¹äº‘ç†è§£` `å¤šåŸŸæ³›åŒ–` `æ··åˆä¸“å®¶æ¡†æž¶` `åŠ¨æ€è·¯ç”±` `è‡ªç›‘ç£å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæºç‚¹äº‘å¯†åº¦å’Œå™ªå£°å·®å¼‚å¯¼è‡´è´Ÿè¿ç§»ï¼Œé™åˆ¶3Dæ¨¡åž‹æ³›åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ··åˆä¸“å®¶æ¡†æž¶ç»“åˆé¢†åŸŸæ„ŸçŸ¥å’Œç»Ÿä¸€è¡¨ç¤ºåˆ†æ”¯ï¼ŒåŠ¨æ€è·¯ç”±ä¼˜åŒ–ä¸“å®¶åˆ©ç”¨ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ScanNetå’ŒS3DISåŸºå‡†ä¸Šè¾¾åˆ°80.1%å’Œ77.2% mIoUï¼Œæ€§èƒ½ä¼˜è¶Šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The generalization of 3D deep learning across multiple domains remains limited by the limited scale of existing datasets and the high heterogeneity of multi-source point clouds. Point clouds collected from different sensors (e.g., LiDAR scans and mesh-derived point clouds) exhibit substantial discrepancies in density and noise distribution, resulting in negative transfer during multi-domain fusion. Most existing approaches focus exclusively on either domain-aware or domain-general features, overlooking the potential synergy between them. To address this, we propose DoReMi (Domain-Representation Mixture), a Mixture-of-Experts (MoE) framework that jointly models Domain-aware Experts branch and a unified Representation branch to enable cooperative learning between specialized and generalizable knowledge. DoReMi dynamically activates domain-aware expert branch via Domain-Guided Spatial Routing (DSR) for context-aware expert selection and employs Entropy-Controlled Dynamic Allocation (EDA) for stable and efficient expert utilization, thereby adaptively modeling diverse domain distributions. Complemented by a frozen unified representation branch pretrained through robust multi-attribute self-supervised learning, DoReMi preserves cross-domain geometric and structural priors while maintaining global consistency. We evaluate DoReMi across multiple 3D understanding benchmarks. Notably, DoReMi achieves 80.1% mIoU on ScanNet Val and 77.2% mIoU on S3DIS, demonstrating competitive or superior performance compared to existing approaches, and showing strong potential as a foundation framework for future 3D understanding research. The code will be released soon.

