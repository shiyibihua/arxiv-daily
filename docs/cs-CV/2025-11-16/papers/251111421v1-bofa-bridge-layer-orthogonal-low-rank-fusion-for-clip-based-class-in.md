---
layout: default
title: BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning
---

# BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning

**arXiv**: [2511.11421v1](https://arxiv.org/abs/2511.11421) | [PDF](https://arxiv.org/pdf/2511.11421.pdf)

**ä½œè€…**: Lan Li, Tao Hu, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBOFAæ¡†æž¶ä»¥è§£å†³CLIPåœ¨ç±»å¢žé‡å­¦ä¹ ä¸­çš„é—å¿˜ä¸Žæ¨¡æ€èžåˆé—®é¢˜**

**å…³é”®è¯**: `ç±»å¢žé‡å­¦ä¹ ` `CLIPæ¨¡åž‹` `æ­£äº¤ä½Žç§©èžåˆ` `å¤šæ¨¡æ€èžåˆ` `æ¡¥æŽ¥å±‚é€‚åº”` `æ— å›žæ”¾å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šCLIPåº”ç”¨äºŽç±»å¢žé‡å­¦ä¹ æ—¶ï¼Œé¢å¤–æ¨¡å—å¢žåŠ å¤æ‚æ€§ä¸”æ˜“é—å¿˜ï¼Œå¤šæ¨¡æ€æ½œåŠ›æœªå……åˆ†æŒ–æŽ˜
2. æ–¹æ³•è¦ç‚¹ï¼šä»…åœ¨CLIPæ¡¥æŽ¥å±‚è¿›è¡Œæ­£äº¤ä½Žç§©èžåˆï¼Œæ— é¢å¤–å‚æ•°ï¼Œçº¦æŸæ›´æ–°è‡³å®‰å…¨å­ç©ºé—´é˜²é—å¿˜
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ ‡å‡†åŸºå‡†æµ‹è¯•æ˜¾ç¤ºBOFAåœ¨å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæ— éœ€æ•°æ®å›žæ”¾

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.

