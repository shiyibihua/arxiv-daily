---
layout: default
title: Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion
---

# Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion

**arXiv**: [2511.11126v1](https://arxiv.org/abs/2511.11126) | [PDF](https://arxiv.org/pdf/2511.11126.pdf)

**ä½œè€…**: Yi Shi, Wenlong Meng, Zhenyuan Guo, Chengkun Wei, Wenzhi Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMemoDetectoræ¡†æž¶ï¼Œé€šè¿‡å¤šçº§æ¨¡æ€å¢žå¼ºä¸ŽåŒé˜¶æ®µèžåˆæå‡è¡¨æƒ…åŒ…æƒ…æ„Ÿç†è§£**

**å…³é”®è¯**: `è¡¨æƒ…åŒ…æƒ…æ„Ÿç†è§£` `å¤šæ¨¡æ€èžåˆ` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `éšå«æ„ä¹‰æŒ–æŽ˜` `åŒé˜¶æ®µèžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¡¨æƒ…åŒ…æƒ…æ„Ÿç†è§£ä¸­ç¼ºä¹ç»†ç²’åº¦å¤šæ¨¡æ€èžåˆç­–ç•¥å’Œéšå«æ„ä¹‰æŒ–æŽ˜
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨MLLMå¢žå¼ºæ–‡æœ¬ï¼Œå¹¶è®¾è®¡åŒé˜¶æ®µæ¨¡æ€èžåˆç­–ç•¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨MET-MEMEå’ŒMOODæ•°æ®é›†ä¸ŠF1åˆ†æ•°åˆ†åˆ«æå‡4.3%å’Œ3.4%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

