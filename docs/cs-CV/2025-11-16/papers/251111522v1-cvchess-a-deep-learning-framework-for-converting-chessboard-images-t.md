---
layout: default
title: CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation
---

# CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation

**arXiv**: [2511.11522v1](https://arxiv.org/abs/2511.11522) | [PDF](https://arxiv.org/pdf/2511.11522.pdf)

**ä½œè€…**: Luthira Abeykoon, Ved Patel, Gawthaman Senthilvelan, Darshan Kasundra

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCVChessæ¡†æž¶ï¼Œå°†æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºFENä»¥å¼¥åˆç‰©ç†ä¸Žæ•°å­—è±¡æ£‹ä½“éªŒå·®è·**

**å…³é”®è¯**: `æ£‹ç›˜å›¾åƒè¯†åˆ«` `æ®‹å·®å·ç§¯ç¥žç»ç½‘ç»œ` `FENè½¬æ¢` `è±¡æ£‹å¼•æ“Žé›†æˆ` `å›¾åƒé¢„å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç‰©ç†è±¡æ£‹æ¸¸æˆç¼ºä¹æ•°å­—è¾…åŠ©ï¼Œå¯¼è‡´ä¸Žåœ¨çº¿ä½“éªŒè„±èŠ‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ®‹å·®CNNè¿›è¡Œæ£‹å­è¯†åˆ«ï¼Œç»“åˆå›¾åƒé¢„å¤„ç†å’Œåˆ†å‰²æ­¥éª¤
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ChessReDæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¾“å‡ºFENä¾›å¼•æ“Žç”Ÿæˆæœ€ä¼˜ç§»åŠ¨

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move

