---
layout: default
title: Î¦eat: Physically-Grounded Feature Representation
---

# Î¦eat: Physically-Grounded Feature Representation

**arXiv**: [2511.11270v1](https://arxiv.org/abs/2511.11270) | [PDF](https://arxiv.org/pdf/2511.11270.pdf)

**ä½œè€…**: Giuseppe Vecchio, Adrien Kaiser, Rouffet Romain, Rosalie Martin, Elena Garces, Tamy Boubekeur

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºÎ¦eatè‡ªç›‘ç£è§†è§‰éª¨å¹²ï¼Œå­¦ä¹ ç‰©ç†åŸºç¡€ç‰¹å¾ä»¥å¢žå¼ºç‰©ç†æ„ŸçŸ¥ä»»åŠ¡**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `ç‰©ç†åŸºç¡€ç‰¹å¾` `ææ–™è¯†åˆ«` `å¯¹æ¯”å­¦ä¹ ` `è§†è§‰éª¨å¹²ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è‡ªç›‘ç£ç‰¹å¾å°†é«˜å±‚è¯­ä¹‰ä¸Žå‡ ä½•ã€å…‰ç…§ç­‰ç‰©ç†å› ç´ çº ç¼ ï¼Œé˜»ç¢ç‰©ç†æŽ¨ç†ä»»åŠ¡
2. é‡‡ç”¨å¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ç©ºé—´è£å‰ªå’Œç‰©ç†å¢žå¼ºå­¦ä¹ ææ–™èº«ä»½ç‰¹å¾
3. è¯„ä¼°æ˜¾ç¤ºç‰¹å¾ç›¸ä¼¼æ€§åˆ†æžå’Œææ–™é€‰æ‹©ä¸­ï¼ŒÎ¦eatæ•èŽ·ç‰©ç†ç»“æž„ï¼Œè¶…è¶Šè¯­ä¹‰åˆ†ç»„

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce $Î¦$eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that $Î¦$eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.

