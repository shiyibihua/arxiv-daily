---
layout: default
title: Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA
---

# Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA

**arXiv**: [2511.11169v1](https://arxiv.org/abs/2511.11169) | [PDF](https://arxiv.org/pdf/2511.11169.pdf)

**ä½œè€…**: Ayush Pandey, Jai Bardhan, Ishita Jain, Ramya S Hebbalaguppe, Rohan Raju Dhanakshirur, Lovekesh Vig

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAlignVQAå¤šæ™ºèƒ½ä½“æ¡†æž¶ä»¥è§£å†³VQAä¸­ç½®ä¿¡åº¦æ ¡å‡†é—®é¢˜**

**å…³é”®è¯**: `è§†è§‰é—®ç­”` `ç½®ä¿¡åº¦æ ¡å‡†` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `è¾©è®ºæ¡†æž¶` `æ ¡å‡†æŸå¤±å‡½æ•°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VQAç³»ç»Ÿåœ¨è§†è§‰ä¸ç¡®å®šæ€§ä¸‹å¸¸äº§ç”Ÿè¿‡åº¦è‡ªä¿¡ï¼Œå½±å“é«˜é£Žé™©åº”ç”¨å¯é æ€§ã€‚
2. é‡‡ç”¨å¤šæ™ºèƒ½ä½“è¾©è®ºæ¡†æž¶ï¼Œé€šè¿‡ä¸“ä¸šä»£ç†ç”Ÿæˆç­”æ¡ˆå¹¶äº¤äº’ä¼˜åŒ–ç½®ä¿¡åº¦ä¼°è®¡ã€‚
3. å®žéªŒæ˜¾ç¤ºæ ¡å‡†è¯¯å·®æ˜¾è‘—é™ä½Žï¼Œæ–°æŸå¤±å‡½æ•°æå‡ä¸ªä½“ä»£ç†ç½®ä¿¡åº¦è´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.

