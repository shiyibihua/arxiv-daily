---
layout: default
title: D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces
---

# D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces

**arXiv**: [2511.11286v1](https://arxiv.org/abs/2511.11286) | [PDF](https://arxiv.org/pdf/2511.11286.pdf)

**ä½œè€…**: Ruoqi Wang, Haitao Wang, Shaojie Guo, Qiong Luo

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD-GAPæ–¹æ³•ï¼Œé€šè¿‡é¢‘çŽ‡å’Œåƒç´ ç©ºé—´å¢žå¼ºæå‡æ¨¡åž‹åœ¨åŸŸå¤–åœºæ™¯çš„é²æ£’æ€§**

**å…³é”®è¯**: `åŸŸå¤–é²æ£’æ€§` `é¢‘çŽ‡ç©ºé—´å¢žå¼º` `åƒç´ ç©ºé—´æ··åˆ` `æ¢¯åº¦å¼•å¯¼å¢žå¼º` `æ•°æ®é›†æ— å…³å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ¨¡åž‹åœ¨åŸŸå¤–åœºæ™¯ä¸­å› å­¦ä¹ åå‘ç‰¹å®šé¢‘çŽ‡ç»„ä»¶è€Œæ€§èƒ½ä¸‹é™
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä»»åŠ¡æ¢¯åº¦è®¡ç®—é¢‘çŽ‡æ•æ„Ÿå›¾ï¼Œè‡ªé€‚åº”æ’å€¼æŒ¯å¹…å¹¶æ··åˆåƒç´ ç»†èŠ‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žä¸–ç•Œå’ŒåŸºå‡†æ•°æ®é›†ä¸Šå¹³å‡æå‡OODæ€§èƒ½5.3%å’Œ1.8%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Out-of-domain (OOD) robustness is challenging to achieve in real-world computer vision applications, where shifts in image background, style, and acquisition instruments always degrade model performance. Generic augmentations show inconsistent gains under such shifts, whereas dataset-specific augmentations require expert knowledge and prior analysis. Moreover, prior studies show that neural networks adapt poorly to domain shifts because they exhibit a learning bias to domain-specific frequency components. Perturbing frequency values can mitigate such bias but overlooks pixel-level details, leading to suboptimal performance. To address these problems, we propose D-GAP (Dataset-agnostic and Gradient-guided augmentation in Amplitude and Pixel spaces), improving OOD robustness by introducing targeted augmentation in both the amplitude space (frequency space) and pixel space. Unlike conventional handcrafted augmentations, D-GAP computes sensitivity maps in the frequency space from task gradients, which reflect how strongly the model responds to different frequency components, and uses the maps to adaptively interpolate amplitudes between source and target samples. This way, D-GAP reduces the learning bias in frequency space, while a complementary pixel-space blending procedure restores fine spatial details. Extensive experiments on four real-world datasets and three domain-adaptation benchmarks show that D-GAP consistently outperforms both generic and dataset-specific augmentations, improving average OOD performance by +5.3% on real-world datasets and +1.8% on benchmark datasets.

