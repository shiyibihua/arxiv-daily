---
layout: default
title: Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models
---

# Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models

**arXiv**: [2511.11410v1](https://arxiv.org/abs/2511.11410) | [PDF](https://arxiv.org/pdf/2511.11410.pdf)

**ä½œè€…**: Jiaxi Huang, Dongxu Wu, Hanwei Zhu, Lingyu Zhu, Jun Xing, Xu Wang, Baoliang Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-Docæ¡†æž¶ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°èƒ½åŠ›**

**å…³é”®è¯**: `æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è¯„ä¼°æ¡†æž¶` `æ€ç»´é“¾æç¤º` `å¤±çœŸè¯†åˆ«`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°æ–¹é¢çš„æ½œåŠ›æœªè¢«å……åˆ†æŽ¢ç´¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡ä¸‰å±‚è¯„ä¼°æ¡†æž¶ï¼ŒåŒ…æ‹¬ç²—ç²’åº¦è¯„åˆ†ã€ä¸­ç²’åº¦å¤±çœŸç±»åž‹è¯†åˆ«å’Œç»†ç²’åº¦å¤±çœŸå¼ºåº¦åˆ†ç±»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°æ˜¾ç¤ºæ¨¡åž‹å­˜åœ¨è¯„åˆ†ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œä½†æ€ç»´é“¾æç¤ºæ˜¾è‘—æå‡æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:
>   https://github.com/cydxf/Q-Doc.

