---
layout: default
title: MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model
---

# MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model

**arXiv**: [2511.11407v1](https://arxiv.org/abs/2511.11407) | [PDF](https://arxiv.org/pdf/2511.11407.pdf)

**ä½œè€…**: Manyu Li, Ruian He, Chenxi Ma, Weimin Tan, Bo Yan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMicroVQA++æ•°æ®é›†ä¸ŽHiCQA-Graphæ–¹æ³•ä»¥è§£å†³æ˜¾å¾®é•œæŽ¨ç†æ•°æ®ç¨€ç¼ºé—®é¢˜**

**å…³é”®è¯**: `æ˜¾å¾®é•œè§†è§‰é—®ç­”` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `å¼‚æž„å›¾è¿‡æ»¤` `æ•°æ®è´¨é‡æŽ§åˆ¶` `ç”Ÿç‰©åŒ»å­¦æˆåƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ˜¾å¾®é•œæŽ¨ç†å—é™äºŽå¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼º
2. ä½¿ç”¨ä¸‰é˜¶æ®µæ–¹æ³•æž„å»ºæ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸“å®¶å¼•å¯¼ã€å›¾è¿‡æ»¤å’ŒMLLMç”Ÿæˆ
3. å®žéªŒæ˜¾ç¤º4Bè§„æ¨¡MLLMåœ¨æ˜¾å¾®é•œæŽ¨ç†ä¸­è¾¾åˆ°ç«žäº‰æ€§æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.

