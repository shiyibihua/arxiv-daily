---
layout: default
title: Questioning the Stability of Visual Question Answering
---

# Questioning the Stability of Visual Question Answering

**arXiv**: [2511.11206v1](https://arxiv.org/abs/2511.11206) | [PDF](https://arxiv.org/pdf/2511.11206.pdf)

**ä½œè€…**: Amir Rosenfeld, Neta Glazer, Ethan Fetaya

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡åž‹å¯¹å¾®å°è¯­ä¹‰ä¸å˜æ‰°åŠ¨çš„è„†å¼±æ€§ï¼Œå¹¶åˆ©ç”¨ç¨³å®šæ€§é¢„æµ‹æ¨¡åž‹æ­£ç¡®æ€§ã€‚**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `é²æ£’æ€§è¯„ä¼°` `è¯­ä¹‰ä¸å˜æ‰°åŠ¨` `ç¨³å®šæ€§åˆ†æž` `æ¨¡åž‹é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°ä»£è§†è§‰è¯­è¨€æ¨¡åž‹å¯¹åƒç´ çº§å˜æ¢ã€é‡è¿°ç­‰è¯­ä¹‰ä¸å˜æ‰°åŠ¨é«˜åº¦æ•æ„Ÿï¼Œå½±å“å¯é æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç³»ç»Ÿè¯„ä¼°å¤šç§æ‰°åŠ¨ç±»åž‹ï¼Œåˆ†æžç¨³å®šæ€§ä¸Žæ¨¡åž‹ã€é—®é¢˜ç±»åˆ«çš„å…³ç³»ã€‚
3. å®žéªŒæ•ˆæžœï¼šç¨³å®šæ ·æœ¬æ›´å¯èƒ½æ­£ç¡®ï¼Œå°æ¨¡åž‹ç¨³å®šæ€§å¯é¢„æµ‹å¤§æ¨¡åž‹æ­£ç¡®æ€§ï¼Œç²¾åº¦é«˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.

