---
layout: default
title: CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging
---

# CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging

**arXiv**: [2511.11034v1](https://arxiv.org/abs/2511.11034) | [PDF](https://arxiv.org/pdf/2511.11034.pdf)

**ä½œè€…**: Pooja Singh, Siddhant Ujjain, Tapan Kumar Gandhi, Sandeep Kumar

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCrossMedåŸºå‡†ä»¥è¯„ä¼°åŒ»å­¦å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨ç»„åˆæ³›åŒ–ä¸­çš„è¡¨çŽ°**

**å…³é”®è¯**: `ç»„åˆæ³›åŒ–` `åŒ»å­¦å¤šæ¨¡æ€åŸºå‡†` `è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `é›¶æ ·æœ¬æ³›åŒ–` `è·¨ä»»åŠ¡è¿ç§»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šåŒ»å­¦å¤šæ¨¡æ€å¤§æ¨¡åž‹åœ¨æœªè§æ¨¡æ€-è§£å‰–-ä»»åŠ¡ç»„åˆä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽMATæ¡†æž¶ç»Ÿä¸€å››ä¸ªå…¬å…±æ•°æ®é›†ä¸ºè§†è§‰é—®ç­”æ ¼å¼ï¼Œæž„å»º20,200ä¸ªå¤šé€‰é—®é¢˜
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹åœ¨ç›¸å…³åˆ†å‰²ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨æ— å…³å’Œé›¶é‡å æ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

