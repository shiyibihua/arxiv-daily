---
layout: default
title: PhaseWin Search Framework Enable Efficient Object-Level Interpretation
---

# PhaseWin Search Framework Enable Efficient Object-Level Interpretation

**arXiv**: [2511.10914v1](https://arxiv.org/abs/2511.10914) | [PDF](https://arxiv.org/pdf/2511.10914.pdf)

**ä½œè€…**: Zihan Gu, Ruoyu Chen, Junchi Zhang, Yue Hu, Hua Zhang, Xiaochun Cao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPhaseWinæœç´¢æ¡†æž¶ä»¥é«˜æ•ˆå®žçŽ°å¯¹è±¡çº§åŸºç¡€æ¨¡åž‹çš„å¿ å®žå½’å› **

**å…³é”®è¯**: `å¯¹è±¡çº§å½’å› ` `å­æ¨¡ä¼˜åŒ–` `é«˜æ•ˆæœç´¢ç®—æ³•` `è§†è§‰åŸºç¡€æ¨¡åž‹` `è®¡ç®—æ•ˆçŽ‡æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å­æ¨¡å­é›†é€‰æ‹©æ–¹æ³•å¿ å®žåº¦é«˜ä½†æ•ˆçŽ‡ä½Žï¼Œé˜»ç¢å®žé™…éƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†é˜¶æ®µç²—åˆ°ç»†æœç´¢ï¼Œç»“åˆè‡ªé€‚åº”å‰ªæžå’Œçª—å£é€‰æ‹©ï¼Œè¿‘ä¼¼è´ªå©ªè¡Œä¸ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè®¡ç®—é¢„ç®—ä»…20%æ—¶è¾¾åˆ°95%ä»¥ä¸Šè´ªå©ªå¿ å®žåº¦ï¼Œåœ¨æ£€æµ‹å’Œè§†è§‰å®šä½ä»»åŠ¡ä¸­é¢†å…ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.

