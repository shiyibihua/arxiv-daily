---
layout: default
title: MCN-CL: Multimodal Cross-Attention Network and Contrastive Learning for Multimodal Emotion Recognition
---

# MCN-CL: Multimodal Cross-Attention Network and Contrastive Learning for Multimodal Emotion Recognition

**arXiv**: [2511.10892v1](https://arxiv.org/abs/2511.10892) | [PDF](https://arxiv.org/pdf/2511.10892.pdf)

**ä½œè€…**: Feng Li, Ke Wu, Yongwei Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMCN-CLæ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œå¯¹æ¯”å­¦ä¹ è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ¨¡æ€å¼‚è´¨æ€§å’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `è·¨æ¨¡æ€æ³¨æ„åŠ›` `å¯¹æ¯”å­¦ä¹ ` `ç‰¹å¾èžåˆ` `ç±»åˆ«ä¸å¹³è¡¡` `åŠ¨æ€å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«é¢ä¸´æ¨¡æ€å¼‚è´¨æ€§ã€ç±»åˆ«ä¸å¹³è¡¡å’ŒåŠ¨æ€é¢éƒ¨åŠ¨ä½œå•å…ƒå»ºæ¨¡å¤æ‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸‰é‡æŸ¥è¯¢æœºåˆ¶å’Œç¡¬è´Ÿæ ·æœ¬æŒ–æŽ˜ï¼Œå‡å°‘ç‰¹å¾å†—ä½™å¹¶ä¿ç•™æƒ…æ„Ÿçº¿ç´¢ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šï¼ŒåŠ æƒF1åˆ†æ•°åˆ†åˆ«æå‡3.42%å’Œ5.73%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal emotion recognition plays a key role in many domains, including mental health monitoring, educational interaction, and human-computer interaction. However, existing methods often face three major challenges: unbalanced category distribution, the complexity of dynamic facial action unit time modeling, and the difficulty of feature fusion due to modal heterogeneity. With the explosive growth of multimodal data in social media scenarios, the need for building an efficient cross-modal fusion framework for emotion recognition is becoming increasingly urgent. To this end, this paper proposes Multimodal Cross-Attention Network and Contrastive Learning (MCN-CL) for multimodal emotion recognition. It uses a triple query mechanism and hard negative mining strategy to remove feature redundancy while preserving important emotional cues, effectively addressing the issues of modal heterogeneity and category imbalance. Experiment results on the IEMOCAP and MELD datasets show that our proposed method outperforms state-of-the-art approaches, with Weighted F1 scores improving by 3.42% and 5.73%, respectively.

