---
layout: default
title: DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding
---

# DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding

**arXiv**: [2511.11313v1](https://arxiv.org/abs/2511.11313) | [PDF](https://arxiv.org/pdf/2511.11313.pdf)

**ä½œè€…**: Tanveer Hannan, Dimitrios Mallios, Parth Pathak, Faegheh Sardari, Thomas Seidl, Gedas Bertasius, Mohsen Fayyaz, Sunando Sengupta

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDocSLMå°è§†è§‰è¯­è¨€æ¨¡åž‹ä»¥è§£å†³èµ„æºå—é™è®¾å¤‡ä¸Šçš„é•¿å¤šæ¨¡æ€æ–‡æ¡£ç†è§£é—®é¢˜**

**å…³é”®è¯**: `å°è§†è§‰è¯­è¨€æ¨¡åž‹` `é•¿æ–‡æ¡£ç†è§£` `å¤šæ¨¡æ€åŽ‹ç¼©` `è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²` `æµå¼å¤„ç†` `ä¸ç¡®å®šæ€§æ ¡å‡†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹å†…å­˜å ç”¨é«˜ï¼Œéš¾ä»¥éƒ¨ç½²äºŽèµ„æºå—é™è¾¹ç¼˜è®¾å¤‡
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨åˆ†å±‚å¤šæ¨¡æ€åŽ‹ç¼©å™¨å’Œæµå¼å¼ƒæƒæœºåˆ¶ï¼Œå‡å°‘å†…å­˜æ¶ˆè€—å¹¶å¤„ç†é•¿è¾“å…¥
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šå…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½Žè§†è§‰ä»¤ç‰Œã€å‚æ•°å’Œå»¶è¿Ÿ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.

