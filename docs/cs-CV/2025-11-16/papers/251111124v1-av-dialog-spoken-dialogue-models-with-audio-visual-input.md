---
layout: default
title: AV-Dialog: Spoken Dialogue Models with Audio-Visual Input
---

# AV-Dialog: Spoken Dialogue Models with Audio-Visual Input

**arXiv**: [2511.11124v1](https://arxiv.org/abs/2511.11124) | [PDF](https://arxiv.org/pdf/2511.11124.pdf)

**ä½œè€…**: Tuochao Chen, Bandhav Veluri, Hongyu Gong, Shyamnath Gollakota

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAV-Dialogå¤šæ¨¡æ€å¯¹è¯æ¡†æž¶ï¼Œç»“åˆè§†å¬è¾“å…¥ä»¥æå‡å˜ˆæ‚çŽ¯å¢ƒä¸‹çš„å¯¹è¯é²æ£’æ€§ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹è¯` `è§†å¬èžåˆ` `è½®è½¬é¢„æµ‹` `å˜ˆæ‚çŽ¯å¢ƒé²æ£’æ€§` `è¯´è¯è€…è·Ÿè¸ª`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹è¯æ¨¡åž‹åœ¨å˜ˆæ‚å¤šè¯´è¯è€…çŽ¯å¢ƒä¸­å“åº”ä¸ç›¸å…³ä¸”è½®è½¬ä¸è‡ªç„¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šèžåˆéŸ³é¢‘å’Œè§†è§‰çº¿ç´¢ï¼Œè¿›è¡Œå¤šä»»åŠ¡å¤šé˜¶æ®µè®­ç»ƒï¼Œå®žçŽ°è¯´è¯è€…è·Ÿè¸ªå’Œè½®è½¬é¢„æµ‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¹²æ‰°ä¸‹ä¼˜äºŽçº¯éŸ³é¢‘æ¨¡åž‹ï¼Œå‡å°‘è½¬å½•é”™è¯¯å¹¶æå‡äººç±»è¯„ä»·å¯¹è¯è´¨é‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

