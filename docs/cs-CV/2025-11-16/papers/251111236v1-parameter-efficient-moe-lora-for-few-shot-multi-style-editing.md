---
layout: default
title: Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing
---

# Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing

**arXiv**: [2511.11236v1](https://arxiv.org/abs/2511.11236) | [PDF](https://arxiv.org/pdf/2511.11236.pdf)

**ä½œè€…**: Cong Cao, Yujie Xu, Xiaodong Xu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‚æ•°é«˜æ•ˆMoE LoRAæ¡†æž¶ä»¥è§£å†³å°‘æ ·æœ¬å¤šé£Žæ ¼å›¾åƒç¼–è¾‘é—®é¢˜**

**å…³é”®è¯**: `å°‘æ ·æœ¬å­¦ä¹ ` `å¤šé£Žæ ¼å›¾åƒç¼–è¾‘` `MoE LoRA` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `æ‰©æ•£æ¨¡åž‹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé€šç”¨å›¾åƒç¼–è¾‘æ¨¡åž‹åœ¨å°‘æ ·æœ¬æ–°é£Žæ ¼ä¸‹æ•ˆæžœä¸ä½³ï¼Œéœ€é«˜æ•ˆå¾®è°ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆé£Žæ ¼ç‰¹å®šä¸Žå…±äº«è·¯ç”±çš„MoE LoRAï¼Œè‡ªåŠ¨ä¼˜åŒ–ç§©å¹¶é›†æˆå¯¹æŠ—å­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”é£Žæ ¼æ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼Œå‚æ•°æ˜¾è‘—å‡å°‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.

