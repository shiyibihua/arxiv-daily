---
layout: default
title: NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion
---

# NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion

**arXiv**: [2511.11051v1](https://arxiv.org/abs/2511.11051) | [PDF](https://arxiv.org/pdf/2511.11051.pdf)

**ä½œè€…**: Chuheng Chen, Xiaofei Zhou, Geyuan Zhang, Yong Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNP-LoRAä»¥è§£å†³LoRAèžåˆä¸­çš„ç»“æž„å¹²æ‰°é—®é¢˜**

**å…³é”®è¯**: `LoRAèžåˆ` `é›¶ç©ºé—´æŠ•å½±` `ç»“æž„å¹²æ‰°` `å¯æŽ§ç”Ÿæˆ` `å­ç©ºé—´åˆ†ç¦»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰LoRAèžåˆæ–¹æ³•å­˜åœ¨æƒé‡åˆå¹¶å¹²æ‰°ï¼Œå¯¼è‡´è¡¨ç¤ºé‡å å’Œä¿çœŸåº¦ä¸‹é™
2. é€šè¿‡é›¶ç©ºé—´æŠ•å½±åˆ†ç¦»å­ç©ºé—´ï¼Œä¿æŠ¤ä¸»æ–¹å‘å…å—å¹²æ‰°ï¼Œå¹¶å¼•å…¥è½¯æŠ•å½±æœºåˆ¶
3. å®žéªŒæ˜¾ç¤ºNP-LoRAåœ¨èžåˆè´¨é‡ä¸Šä¼˜äºŽåŸºçº¿ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy low-rank high-dimensional subspaces, leading to non-orthogonal and overlapping representations. In this work, we analyze the internal structure of LoRAs and find their generative behavior is dominated by a few principal directions in the low-rank subspace, which should remain free from interference during fusion. To achieve this, we propose Null Space Projection LoRA (NP-LoRA), a projection-based framework for LoRA fusion that enforces subspace separation to prevent structural interference among principal directions. Specifically, we first extract principal style directions via singular value decomposition (SVD) and then project the subject LoRA into its orthogonal null space. Furthermore, we introduce a soft projection mechanism that enables smooth control over the trade-off between subject fidelity and style consistency. Experiments show NP-LoRA consistently improves fusion quality over strong baselines (e.g., DINO and CLIP-based metrics, with human and LLM preference scores), and applies broadly across backbones and LoRA pairs without retraining.

