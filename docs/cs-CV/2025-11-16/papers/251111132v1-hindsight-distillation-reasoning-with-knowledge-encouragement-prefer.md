---
layout: default
title: Hindsight Distillation Reasoning with Knowledge Encouragement Preference for Knowledge-based Visual Question Answering
---

# Hindsight Distillation Reasoning with Knowledge Encouragement Preference for Knowledge-based Visual Question Answering

**arXiv**: [2511.11132v1](https://arxiv.org/abs/2511.11132) | [PDF](https://arxiv.org/pdf/2511.11132.pdf)

**ä½œè€…**: Yu Zhao, Ying Zhang, Xuhui Sui, Baohang Zhou, Li Shen, Dacheng Tao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHinDæ¡†æž¶ä¸ŽKEPOä¼˜åŒ–ï¼Œä»¥è§£å†³çŸ¥è¯†è§†è§‰é—®ç­”ä¸­æŽ¨ç†è¿‡ç¨‹éšå¼çš„é—®é¢˜**

**å…³é”®è¯**: `çŸ¥è¯†è§†è§‰é—®ç­”` `åŽè§è’¸é¦æŽ¨ç†` `çŸ¥è¯†é¼“åŠ±åå¥½ä¼˜åŒ–` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æŽ¨ç†è½¨è¿¹ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŸ¥è¯†è§†è§‰é—®ç­”ä¸­æŽ¨ç†è¿‡ç¨‹éšå¼ï¼Œç¼ºä¹å¤šæ­¥è½¨è¿¹
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŽè§è’¸é¦æž„å»ºè®­ç»ƒæ•°æ®ï¼Œå¹¶ä¼˜åŒ–çŸ¥è¯†ç”Ÿæˆå™¨åå¥½
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨OK-VQAå’ŒA-OKVQAä¸ŠéªŒè¯ï¼Œ7Bæ¨¡åž‹å®žçŽ°é«˜æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Knowledge-based Visual Question Answering (KBVQA) necessitates external knowledge incorporation beyond cross-modal understanding. Existing KBVQA methods either utilize implicit knowledge in multimodal large language models (MLLMs) via in-context learning or explicit knowledge via retrieval augmented generation. However, their reasoning processes remain implicit, without explicit multi-step trajectories from MLLMs. To address this gap, we provide a Hindsight Distilled Reasoning (HinD) framework with Knowledge Encouragement Preference Optimization (KEPO), designed to elicit and harness internal knowledge reasoning ability in MLLMs. First, to tackle the reasoning supervision problem, we propose to emphasize the hindsight wisdom of MLLM by prompting a frozen 7B-size MLLM to complete the reasoning process between the question and its ground truth answer, constructing Hindsight-Zero training data. Then we self-distill Hindsight-Zero into Chain-of-Thought (CoT) Generator and Knowledge Generator, enabling the generation of sequential steps and discrete facts. Secondly, to tackle the misalignment between knowledge correctness and confidence, we optimize the Knowledge Generator with KEPO, preferring under-confident but helpful knowledge over the over-confident but unhelpful one. The generated CoT and sampled knowledge are then exploited for answer prediction. Experiments on OK-VQA and A-OKVQA validate the effectiveness of HinD, showing that HinD with elicited reasoning from 7B-size MLLM achieves superior performance without commercial model APIs or outside knowledge.

