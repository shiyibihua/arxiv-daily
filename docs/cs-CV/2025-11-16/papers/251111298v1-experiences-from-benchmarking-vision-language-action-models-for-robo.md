---
layout: default
title: Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation
---

# Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation

**arXiv**: [2511.11298v1](https://arxiv.org/abs/2511.11298) | [PDF](https://arxiv.org/pdf/2511.11298.pdf)

**ä½œè€…**: Yihao Zhang, Yuankai Qi, Xi Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åŸºå‡†æµ‹è¯•å››ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ€§èƒ½ä¸Žé€‚åº”æ€§**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•` `åˆ†å¸ƒå¤–é€‚åº”æ€§` `æ ‡å‡†åŒ–è¯„ä¼°æ¡†æž¶` `æ¨¡åž‹æ€§èƒ½æ¯”è¾ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œä¸­ç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°ä¸Žè·¨æ¨¡åž‹æ¯”è¾ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°æ¡†æž¶ï¼Œæµ‹é‡å‡†ç¡®æ€§ã€æ•ˆçŽ‡å’Œé€‚åº”æ€§ç­‰ç»´åº¦ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šÏ€_0åœ¨åˆ†å¸ƒå¤–åœºæ™¯é€‚åº”æ€§æœ€ä½³ï¼ŒACTåœ¨åˆ†å¸ƒå†…ç¨³å®šæ€§æœ€é«˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Foundation models applied in robotics, particularly \textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \textbf{empirical experiences} from benchmarking four representative VLAs -- \textbf{ACT}, \textbf{OpenVLA--OFT}, \textbf{RDT-1B}, and \boldmath{$Ï€_0$} -- across four manipulation tasks conducted in both simulation and on the \textbf{ALOHA Mobile} platform. We establish a \textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \textit{accuracy and efficiency} (success rate and time-to-success), (2) \textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \textit{language instruction-following accuracy}. Through this process, we observe that \boldmath{$Ï€_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.

