---
layout: default
title: AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models
---

# AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models

**arXiv**: [2511.11299v1](https://arxiv.org/abs/2511.11299) | [PDF](https://arxiv.org/pdf/2511.11299.pdf)

**ä½œè€…**: Haokun Chen, Jianing Li, Yao Zhang, Jinhe Bi, Yan Xia, Jindong Gu, Volker Tresp

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAUVICæ¡†æž¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§æ¨¡åž‹ä¸­è§†è§‰æ¦‚å¿µç²¾ç¡®é—å¿˜é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡åž‹` `è§†è§‰æ¦‚å¿µé—å¿˜` `å¯¹æŠ—æ‰°åŠ¨` `æ•°æ®éšç§` `æœºå™¨é—å¿˜` `åŸºå‡†è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§æ¨¡åž‹æ•°æ®éšç§é—®é¢˜ï¼Œéœ€å®žçŽ°è§†è§‰æ¦‚å¿µé—å¿˜è€Œä¸å½±å“ç›¸å…³å®žä½“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨å¯¹æŠ—æ‰°åŠ¨æŠ€æœ¯ï¼Œç²¾ç¡®éš”ç¦»ç›®æ ‡æ¦‚å¿µï¼Œé¿å…å‰¯ä½œç”¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨VCUBenchåŸºå‡†ä¸Šï¼Œå®žçŽ°é«˜ç›®æ ‡é—å¿˜çŽ‡ï¼Œæ€§èƒ½é€€åŒ–æœ€å°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.

