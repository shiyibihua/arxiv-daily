---
layout: default
title: CLIPPan: Adapting CLIP as A Supervisor for Unsupervised Pansharpening
---

# CLIPPan: Adapting CLIP as A Supervisor for Unsupervised Pansharpening

**arXiv**: [2511.10896v1](https://arxiv.org/abs/2511.10896) | [PDF](https://arxiv.org/pdf/2511.10896.pdf)

**ä½œè€…**: Lihua Jian, Jiabo Liu, Shaowu Wu, Lihui Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLIPPanæ¡†æž¶ï¼Œåˆ©ç”¨CLIPä½œä¸ºç›‘ç£å™¨è§£å†³æ— ç›‘ç£å…¨åˆ†è¾¨çŽ‡å…¨è‰²é”åŒ–é—®é¢˜**

**å…³é”®è¯**: `æ— ç›‘ç£å…¨è‰²é”åŒ–` `CLIPé€‚åº”` `è¯­ä¹‰è¯­è¨€çº¦æŸ` `å…¨åˆ†è¾¨çŽ‡è®­ç»ƒ` `å›¾åƒèžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç›‘ç£å…¨è‰²é”åŒ–æ–¹æ³•å› æ¨¡æ‹Ÿä½Žåˆ†è¾¨çŽ‡è®­ç»ƒæ•°æ®ä¸ŽçœŸå®žå…¨åˆ†è¾¨çŽ‡åœºæ™¯å·®å¼‚è€Œé¢ä¸´åŸŸé€‚åº”æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡è½»é‡å¾®è°ƒCLIPé€‚åº”å…¨è‰²é”åŒ–ä»»åŠ¡ï¼Œå¹¶è®¾è®¡è¯­ä¹‰è¯­è¨€çº¦æŸæŸå¤±ä»¥è¯­è¨€ä¸ºç›‘ç£ä¿¡å·
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çœŸå®žæ•°æ®é›†ä¸Šæå‡å…‰è°±å’Œç©ºé—´ä¿çœŸåº¦ï¼Œä¸ºæ— ç›‘ç£å…¨åˆ†è¾¨çŽ‡å…¨è‰²é”åŒ–è®¾å®šæ–°åŸºå‡†

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Despite remarkable advancements in supervised pansharpening neural networks, these methods face domain adaptation challenges of resolution due to the intrinsic disparity between simulated reduced-resolution training data and real-world full-resolution scenarios.To bridge this gap, we propose an unsupervised pansharpening framework, CLIPPan, that enables model training at full resolution directly by taking CLIP, a visual-language model, as a supervisor. However, directly applying CLIP to supervise pansharpening remains challenging due to its inherent bias toward natural images and limited understanding of pansharpening tasks. Therefore, we first introduce a lightweight fine-tuning pipeline that adapts CLIP to recognize low-resolution multispectral, panchromatic, and high-resolution multispectral images, as well as to understand the pansharpening process. Then, building on the adapted CLIP, we formulate a novel \textit{loss integrating semantic language constraints}, which aligns image-level fusion transitions with protocol-aligned textual prompts (e.g., Wald's or Khan's descriptions), thus enabling CLIPPan to use language as a powerful supervisory signal and guide fusion learning without ground truth. Extensive experiments demonstrate that CLIPPan consistently improves spectral and spatial fidelity across various pansharpening backbones on real-world datasets, setting a new state of the art for unsupervised full-resolution pansharpening.

