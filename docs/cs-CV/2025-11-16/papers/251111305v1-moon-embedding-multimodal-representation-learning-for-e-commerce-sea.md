---
layout: default
title: MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising
---

# MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising

**arXiv**: [2511.11305v1](https://arxiv.org/abs/2511.11305) | [PDF](https://arxiv.org/pdf/2511.11305.pdf)

**ä½œè€…**: Chenghan Fu, Daoze Zhang, Yukang Lin, Zhanheng Nie, Xiang Zhang, Jianyu Liu, Yueran Liu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMOONå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æž¶ä»¥ä¼˜åŒ–ç”µå•†æœç´¢å¹¿å‘Šç³»ç»Ÿ**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ` `ç”µå•†æœç´¢å¹¿å‘Š` `ç‚¹å‡»çŽ‡é¢„æµ‹` `ä¸‰é˜¶æ®µè®­ç»ƒ` `äº¤æ¢çŽ‡åˆ†æž` `ç¼©æ”¾å®šå¾‹ç ”ç©¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸Žä¸‹æ¸¸ä»»åŠ¡ç›®æ ‡ä¸åŒ¹é…ï¼Œå½±å“ç”µå•†å¹¿å‘Šæ•ˆæžœã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œå®šä¹‰äº¤æ¢çŽ‡é‡åŒ–ä¸­é—´æŒ‡æ ‡ä¸Žä¸‹æ¸¸å¢žç›Šå…³ç³»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çº¿ç‚¹å‡»çŽ‡æå‡20.00%ï¼Œå·²éƒ¨ç½²äºŽæ·˜å®æœç´¢å¹¿å‘Šå…¨é˜¶æ®µã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.

