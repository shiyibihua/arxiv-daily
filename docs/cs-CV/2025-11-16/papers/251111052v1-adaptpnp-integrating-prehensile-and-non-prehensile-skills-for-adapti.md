---
layout: default
title: AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation
---

# AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation

**arXiv**: [2511.11052v1](https://arxiv.org/abs/2511.11052) | [PDF](https://arxiv.org/pdf/2511.11052.pdf)

**ä½œè€…**: Jinxuan Zhu, Chenrui Tie, Xinyi Cao, Yuran Wang, Jingxiang Guo, Zixuan Chen, Haonan Chen, Junting Chen, Yangyu Xiao, Ruihai Wu, Lin Shao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdaptPNPæ¡†æž¶ï¼Œé›†æˆæŠ“å–ä¸ŽéžæŠ“å–æŠ€èƒ½ä»¥å®žçŽ°è‡ªé€‚åº”æœºå™¨äººæ“ä½œ**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä»»åŠ¡ä¸Žè¿åŠ¨è§„åˆ’` `æ•°å­—å­ªç”Ÿ` `è‡ªé€‚åº”æŽ§åˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç»Ÿä¸€æ¡†æž¶éš¾ä»¥æ³›åŒ–ä»»åŠ¡ã€å¯¹è±¡å’ŒçŽ¯å¢ƒï¼Œå¹¶èžåˆæŠ“å–ä¸ŽéžæŠ“å–åŠ¨ä½œ
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ç”Ÿæˆé«˜å±‚è®¡åˆ’ï¼Œç»“åˆæ•°å­—å­ªç”Ÿé¢„æµ‹å¯¹è±¡å§¿æ€
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä»¿çœŸå’ŒçœŸå®žçŽ¯å¢ƒä¸­è¯„ä¼°æ··åˆæ“ä½œä»»åŠ¡ï¼Œå±•ç¤ºæ³›åŒ–æ½œåŠ›

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P&NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P&NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities. Project Website: https://sites.google.com/view/adaptpnp/home

