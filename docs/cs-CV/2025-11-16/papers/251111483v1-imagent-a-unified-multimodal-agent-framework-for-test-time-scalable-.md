---
layout: default
title: ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation
---

# ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation

**arXiv**: [2511.11483v1](https://arxiv.org/abs/2511.11483) | [PDF](https://arxiv.org/pdf/2511.11483.pdf)

**ä½œè€…**: Kaishen Wang, Ruibo Chen, Tong Zheng, Heng Huang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºImAgentç»Ÿä¸€å¤šæ¨¡æ€ä»£ç†æ¡†æž¶ï¼Œä»¥è§£å†³æµ‹è¯•æ—¶æ‰©å±•ä¸­å›¾åƒç”Ÿæˆçš„éšæœºæ€§å’Œä¸ä¸€è‡´æ€§é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¤šæ¨¡æ€ä»£ç†` `æµ‹è¯•æ—¶æ‰©å±•` `å›¾åƒç¼–è¾‘` `è‡ªè¯„ä¼°æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹åœ¨æç¤ºæ¨¡ç³Šæ—¶ç”Ÿæˆéšæœºä¸”ä¸ä¸€è‡´çš„å›¾åƒï¼ŒçŽ°æœ‰æ–¹æ³•æ•ˆçŽ‡ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆæŽ¨ç†ã€ç”Ÿæˆå’Œè‡ªè¯„ä¼°äºŽå•ä¸€æ¡†æž¶ï¼Œé€šè¿‡ç­–ç•¥æŽ§åˆ¶å™¨åŠ¨æ€äº¤äº’æå‡æ•ˆçŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­ï¼ŒImAgentè¶…è¶Šéª¨å¹²æ¨¡åž‹å’Œå…¶ä»–åŸºçº¿ï¼Œæé«˜ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.

