---
layout: default
title: Discovering Meaningful Units with Visually Grounded Semantics from Image Captions
---

# Discovering Meaningful Units with Visually Grounded Semantics from Image Captions

**arXiv**: [2511.11262v1](https://arxiv.org/abs/2511.11262) | [PDF](https://arxiv.org/pdf/2511.11262.pdf)

**ä½œè€…**: Melika Behjati, James Henderson

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†ç»„æ ‡è®°æ¨¡åž‹ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„ç»†ç²’åº¦ç†è§£**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `ç»†ç²’åº¦è¡¨ç¤º` `æ ‡è®°åˆ†ç»„` `è¯­ä¹‰å¯¹é½` `å›¾åƒæ ‡é¢˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå›¾åƒå—å’Œå•ä¸ªæ ‡è®°ç¼ºä¹è§†è§‰å¯æŽ¥åœ°è¯­ä¹‰ï¼Œå½±å“ç»†ç²’åº¦çŸ¥è¯†èŽ·å–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¨¡åž‹åœ¨æž¶æž„ä¸­åˆ†ç»„æ ‡é¢˜æ ‡è®°ï¼Œå¯¹é½å›¾åƒç¼–ç å™¨å‘çŽ°çš„ç‰©ä½“è¡¨ç¤ºã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåˆ†ç»„æ ‡è®°æå‡æ¨¡åž‹ç†è§£ï¼Œå‘çŽ°ç»„ä¸Žå¯æŽ¥åœ°çŸ­è¯­é«˜åº¦ç›¸ä¼¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.

