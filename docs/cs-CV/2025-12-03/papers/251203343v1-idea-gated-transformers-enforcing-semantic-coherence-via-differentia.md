---
layout: default
title: Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning
---

# Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning

**arXiv**: [2512.03343v1](https://arxiv.org/abs/2512.03343) | [PDF](https://arxiv.org/pdf/2512.03343.pdf)

**ä½œè€…**: Darshan Fofadiya

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIdea-Gated Transformerä»¥è§£å†³è‡ªå›žå½’è¯­è¨€æ¨¡åž‹ä¸­çš„ä¸»é¢˜æ¼‚ç§»é—®é¢˜**

**å…³é”®è¯**: `è‡ªå›žå½’è¯­è¨€æ¨¡åž‹` `ä¸»é¢˜æ¼‚ç§»` `å¯å¾®åˆ†é—¨æŽ§` `è¯­ä¹‰è§„åˆ’` `è¯æ±‡å‰ªæž` `å¯æŽ§ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå›žå½’è¯­è¨€æ¨¡åž‹åŸºäºŽä¸‹ä¸€è¯é¢„æµ‹è®­ç»ƒï¼Œæ˜“å› å±€éƒ¨å…³è”å¯¼è‡´ç”Ÿæˆå†…å®¹åç¦»åˆå§‹æç¤ºï¼Œç§°ä¸ºä¸»é¢˜æ¼‚ç§»ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥Idea-Gated Transformerï¼Œé€šè¿‡è¾…åŠ©Idea Headé¢„æµ‹æœªæ¥ä¸Šä¸‹æ–‡è¯è¢‹åˆ†å¸ƒï¼Œç”Ÿæˆæ¦‚å¿µå‘é‡å®žæ—¶é—¨æŽ§ä¸»è¯æ±‡ï¼ŒæŠ‘åˆ¶è¯­ä¹‰æ— å…³è¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨WikiText-103ä¸ŠéªŒè¯ï¼Œæ¨¡åž‹ä¿æŒä¸ŽGPT-2ç›¸å½“çš„å›°æƒ‘åº¦ï¼Œä½†æ˜¾è‘—æå‡é¢†åŸŸä¿æŒèƒ½åŠ›ï¼Œæœ‰æ•ˆé”å®šè¯­ä¹‰èšç±»ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.

