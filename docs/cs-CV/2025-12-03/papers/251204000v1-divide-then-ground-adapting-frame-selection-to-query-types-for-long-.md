---
layout: default
title: Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding
---

# Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding

**arXiv**: [2512.04000v1](https://arxiv.org/abs/2512.04000) | [PDF](https://arxiv.org/pdf/2512.04000.pdf)

**ä½œè€…**: Jialuo Li, Bin Li, Jiahao Li, Yan Lu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDIGæ¡†æž¶ï¼Œæ ¹æ®æŸ¥è¯¢ç±»åž‹è‡ªé€‚åº”é€‰æ‹©å¸§ç­–ç•¥ä»¥æå‡é•¿è§†é¢‘ç†è§£æ•ˆçŽ‡ä¸Žæ€§èƒ½**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å¸§é€‰æ‹©` `æŸ¥è¯¢ç±»åž‹` `å¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹` `è‡ªé€‚åº”ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿è§†é¢‘ç†è§£ä¸­ï¼ŒçŽ°æœ‰æŸ¥è¯¢æ„ŸçŸ¥å¸§é€‰æ‹©æ–¹æ³•è®¡ç®—å¼€é”€å¤§ï¼Œä¸”æœªåŒºåˆ†æŸ¥è¯¢ç±»åž‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽæŸ¥è¯¢ç±»åž‹ï¼ˆå…¨å±€ä¸Žå±€éƒ¨ï¼‰ï¼ŒDIGé‡‡ç”¨å‡åŒ€é‡‡æ ·æˆ–ä¸“ç”¨ç®¡é“è‡ªé€‚åº”é€‰æ‹©å¸§ï¼Œæ— éœ€è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIGä¼˜äºŽåŸºçº¿ï¼Œè¾“å…¥å¸§æ•°è¾¾256æ—¶ä»èƒ½æå‡LMMæ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.

