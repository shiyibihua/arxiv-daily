---
layout: default
title: TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning
---

# TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.03963" target="_blank" class="toolbar-btn">arXiv: 2512.03963v2</a>
    <a href="https://arxiv.org/pdf/2512.03963.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03963v2" 
            onclick="toggleFavorite(this, '2512.03963v2', 'TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Tao Wu, Li Yang, Gen Zhan, Yabin Zhang, Yiting Liao, Junlin Li, Deliang Fu, Li Zhang, Limin Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03 (Êõ¥Êñ∞: 2025-12-04)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TempR1ÔºåÈÄöËøáÊó∂Â∫èÊÑüÁü•Â§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáMLLMÂØπÈïøËßÜÈ¢ëÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Êó∂Â∫èÁêÜËß£` `Âº∫ÂåñÂ≠¶‰π†` `Â§ö‰ªªÂä°Â≠¶‰π†` `ÈïøËßÜÈ¢ëÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÊèêÂçáMLLMÊó∂Â∫èÁêÜËß£ÊñπÈù¢Â≠òÂú®‰∏çË∂≥Ôºå‰∏ªË¶Å‰ΩìÁé∞Âú®‰ªªÂä°Á±ªÂûãÂíåÊï∞ÊçÆÊúâÈôêÔºåÈöæ‰ª•Ê≥õÂåñÂà∞Â§öÊ†∑ÂåñÁöÑÊó∂Â∫èÁêÜËß£Âú∫ÊôØ„ÄÇ
2. TempR1ÁöÑÊ†∏ÂøÉÂú®‰∫éÊûÑÂª∫Êó∂Â∫èÊÑüÁü•ÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÈÄöËøáÂ§ö‰ªªÂä°ËØ≠ÊñôÂ∫ìÂíåÂÆöÂà∂ÂåñÂ•ñÂä±ÔºåÊèêÂçáÊ®°ÂûãÂØπ‰∏çÂêåÊó∂Â∫èÊ®°ÂºèÁöÑÈÄÇÂ∫îÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTempR1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂‰∏îÈÄöËøáËÅîÂêà‰ºòÂåñÂ¢ûÂº∫‰∫ÜÊ≥õÂåñËÉΩÂäõÂíåÂçï‰ªªÂä°ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)ÂØπÈïøËßÜÈ¢ëÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõÔºå‰ªéËÄåÊé®ËøõÊó∂Â∫èÂÆö‰Ωç„ÄÅÂä®‰ΩúÊ£ÄÊµãÂíåÊó∂Èó¥ÊïèÊÑüÈóÆÁ≠îÁ≠â‰ªªÂä°ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜTempR1Ôºå‰∏Ä‰∏™Êó∂Â∫èÊÑüÁü•ÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®Á≥ªÁªüÊÄßÂú∞Â¢ûÂº∫MLLMÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ö‰ªªÂä°ËØ≠ÊñôÂ∫ìÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊé•Ëß¶Âà∞‰∏çÂêåÁöÑÊó∂Â∫èÁªìÊûÑÂíåËØ≠‰πâ„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨Âü∫‰∫éGroup Relative Policy Optimization (GRPO)ÁÆóÊ≥ïÔºåÂÆûÁé∞‰∫ÜÁ®≥ÂÆöÊúâÊïàÁöÑË∑®‰ªªÂä°‰ºòÂåñ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â∞ÜÊó∂Â∫è‰ªªÂä°ÂàÜ‰∏∫È¢ÑÊµãÂå∫Èó¥ÂíåÁúüÂÆûÂÆû‰æã‰πãÈó¥ÁöÑ‰∏âÁßçÂØπÂ∫îÁ±ªÂûãÔºåÂπ∂‰∏∫ÊØèÁßçÁ±ªÂûãËÆæËÆ°‰∫ÜÂÆöÂà∂ÂåñÁöÑÂÆö‰ΩçÂ•ñÂä±Ôºå‰ΩøTempR1ËÉΩÂ§üÊçïËé∑ÁªÜÁ≤íÂ∫¶ÁöÑÊó∂Â∫è‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂ÈÄÇÂ∫î‰∏çÂêåÁöÑÊó∂Â∫èÊ®°Âºè„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåTempR1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂØπ‰∫íË°•‰ªªÂä°ÁöÑËÅîÂêà‰ºòÂåñ‰∫ßÁîü‰∫ÜÂº∫Â§ßÁöÑÂçèÂêåÊïàÂ∫îÔºåÂ¢ûÂº∫‰∫ÜÊ≥õÂåñËÉΩÂäõÂíåÂçï‰ªªÂä°ÊÄßËÉΩÔºå‰∏∫MLLM‰∏≠ÁöÑÊó∂Â∫èÊé®ÁêÜÂª∫Á´ã‰∫Ü‰∏Ä‰∏™ÂèØÊâ©Â±ï‰∏îÊúâÂéüÂàôÁöÑËåÉ‰æã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºå‰∏ªË¶Å‰ΩìÁé∞Âú®ÂÆÉ‰ª¨ÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®ÊúâÈôêÁöÑ‰ªªÂä°Á±ªÂûãÂíåÊï∞ÊçÆÈõÜÔºåÂØºËá¥Ê®°ÂûãÈöæ‰ª•Ê≥õÂåñÂà∞Êõ¥ÂπøÊ≥õÁöÑÊó∂Â∫èÁêÜËß£Âú∫ÊôØ‰∏≠„ÄÇËøô‰∫õÊñπÊ≥ïÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®‰∏çÂêåÊó∂Â∫è‰ªªÂä°‰πãÈó¥ÁöÑ‰∫íË°•‰ø°ÊÅØÔºåÂπ∂‰∏îÁº∫‰πèÈíàÂØπ‰∏çÂêåÊó∂Â∫èÊ®°ÂºèÁöÑÁªÜÁ≤íÂ∫¶‰ºòÂåñÁ≠ñÁï•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTempR1ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáËÅîÂêà‰ºòÂåñÂ§ö‰∏™ÂÖ∑Êúâ‰∏çÂêåÊó∂Â∫èÁªìÊûÑÂíåËØ≠‰πâÁöÑ‰ªªÂä°ÔºåÊù•ÊèêÂçáMLLMÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõ„ÄÇÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´Â§öÊ†∑ÂåñÊó∂Â∫è‰ªªÂä°ÁöÑËØ≠ÊñôÂ∫ìÔºåÂπ∂ËÆæËÆ°ÈíàÂØπ‰∏çÂêå‰ªªÂä°ÁöÑÂÆöÂà∂ÂåñÂ•ñÂä±ÂáΩÊï∞ÔºåTempR1ËÉΩÂ§üÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Êõ¥È≤ÅÊ£íÂíåÊ≥õÂåñÁöÑÊó∂Â∫èË°®Á§∫„ÄÇ ËøôÁßçÂ§ö‰ªªÂä°Â≠¶‰π†ÁöÑÊñπÂºèËÉΩÂ§üÂÖÖÂàÜÂà©Áî®‰∏çÂêå‰ªªÂä°‰πãÈó¥ÁöÑ‰∫íË°•‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTempR1ÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â§ö‰ªªÂä°ËØ≠ÊñôÂ∫ìÊûÑÂª∫Ê®°ÂùóÔºåÁî®‰∫éÊî∂ÈõÜÂíåÊï¥ÁêÜÂåÖÂê´‰∏çÂêåÊó∂Â∫èÁªìÊûÑÂíåËØ≠‰πâÁöÑËßÜÈ¢ëÊï∞ÊçÆÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÈÄÇÂêàÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÊ†ºÂºè„ÄÇ2) Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊ®°ÂùóÔºåËØ•Ê®°ÂùóÂü∫‰∫éGroup Relative Policy Optimization (GRPO)ÁÆóÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉMLLMÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõ„ÄÇ3) Â•ñÂä±ÂáΩÊï∞ËÆæËÆ°Ê®°ÂùóÔºåËØ•Ê®°ÂùóÊ†πÊçÆ‰∏çÂêåÊó∂Â∫è‰ªªÂä°ÁöÑÁâπÁÇπÔºåËÆæËÆ°ÂÆöÂà∂ÂåñÁöÑÂ•ñÂä±ÂáΩÊï∞Ôºå‰ª•ÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Ê≠£Á°ÆÁöÑÊó∂Â∫èË°å‰∏∫„ÄÇ4) MLLMÈõÜÊàêÊ®°ÂùóÔºåÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÊó∂Â∫èÁêÜËß£ËÉΩÂäõÈõÜÊàêÂà∞Áé∞ÊúâÁöÑMLLM‰∏≠Ôºå‰ªéËÄåÊèêÂçáÂÖ∂Âú®Êó∂Â∫èÁõ∏ÂÖ≥‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTempR1ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Êó∂Â∫èÊÑüÁü•ÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåTempR1ËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂ§öÁßç‰∏çÂêåÁ±ªÂûãÁöÑÊó∂Â∫è‰ªªÂä°ÔºåÂπ∂‰∏îËÉΩÂ§üÊ†πÊçÆ‰∏çÂêå‰ªªÂä°ÁöÑÁâπÁÇπÔºåËÆæËÆ°ÂÆöÂà∂ÂåñÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇÊ≠§Â§ñÔºåTempR1ËøòÈááÁî®‰∫ÜGroup Relative Policy Optimization (GRPO)ÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Ëß£ÂÜ≥Â§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË¥üËøÅÁßªÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTempR1ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â∞ÜÊó∂Â∫è‰ªªÂä°ÂàÜ‰∏∫È¢ÑÊµãÂå∫Èó¥ÂíåÁúüÂÆûÂÆû‰æã‰πãÈó¥ÁöÑ‰∏âÁßçÂØπÂ∫îÁ±ªÂûãÔºåÂπ∂‰∏∫ÊØèÁßçÁ±ªÂûãËÆæËÆ°‰∫ÜÂÆöÂà∂ÂåñÁöÑÂÆö‰ΩçÂ•ñÂä±„ÄÇ2) ÈááÁî®Group Relative Policy Optimization (GRPO)ÁÆóÊ≥ïÔºå‰ª•ÂÆûÁé∞Á®≥ÂÆöÊúâÊïàÁöÑË∑®‰ªªÂä°‰ºòÂåñ„ÄÇ3) ÊûÑÂª∫ÂåÖÂê´Â§öÊ†∑ÂåñÊó∂Â∫èÁªìÊûÑÂíåËØ≠‰πâÁöÑÂ§ö‰ªªÂä°ËØ≠ÊñôÂ∫ì„ÄÇ4) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑMLLM‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåÂπ∂ÂØπÂÖ∂ËøõË°åÂæÆË∞ÉÔºå‰ª•ÈÄÇÂ∫îÊó∂Â∫èÁêÜËß£‰ªªÂä°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

TempR1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™Êó∂Â∫èÂÆö‰Ωç‰ªªÂä°‰∏äÔºåTempR1ÁöÑÊÄßËÉΩÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫ÜË∂ÖËøá5%„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åÁªìÊûúËøòË°®ÊòéÔºåTempR1ÁöÑËÅîÂêà‰ºòÂåñÁ≠ñÁï•ËÉΩÂ§üÊòæËëóÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂçï‰ªªÂä°ÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂ§ö‰ªªÂä°Â≠¶‰π†ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TempR1ÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈïøËßÜÈ¢ëÁêÜËß£È¢ÜÂüüÔºå‰æãÂ¶ÇËßÜÈ¢ëÂÜÖÂÆπÂàÜÊûê„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂Á≠â„ÄÇÈÄöËøáÊèêÂçáMLLMÂØπËßÜÈ¢ëÊó∂Â∫è‰ø°ÊÅØÁöÑÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Á≤æÂáÜÁöÑ‰∫ã‰ª∂Ê£ÄÊµã„ÄÅË°å‰∏∫ËØÜÂà´ÂíåÂºÇÂ∏∏Ë°å‰∏∫È¢ÑË≠¶„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ËøòÂèØ‰ª•Â∫îÁî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅÊïôËÇ≤Â®±‰πêÁ≠âÈ¢ÜÂüüÔºå‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥‰∏™ÊÄßÂåñÁöÑÊúçÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

