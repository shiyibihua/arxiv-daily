---
layout: default
title: C3G: Learning Compact 3D Representations with 2K Gaussians
---

# C3G: Learning Compact 3D Representations with 2K Gaussians

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.04021" target="_blank" class="toolbar-btn">arXiv: 2512.04021v1</a>
    <a href="https://arxiv.org/pdf/2512.04021.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04021v1" 
            onclick="toggleFavorite(this, '2512.04021v1', 'C3G: Learning Compact 3D Representations with 2K Gaussians')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Honggyu An, Jaewoo Jung, Mungyeom Kim, Sunghwan Hong, Chaehyun Kim, Kazumi Fukuda, Minkyeong Jeon, Jisang Han, Takuya Narihira, Hyuna Ko, Junsu Kim, Yuki Mitsufuji, Seungryong Kim

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: Project Page : https://cvlab-kaist.github.io/C3G/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**C3Gï¼šä½¿ç”¨2Ké«˜æ–¯å­¦ä¹ ç´§å‡‘çš„3Dè¡¨ç¤ºï¼Œæå‡åœºæ™¯é‡å»ºä¸ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dåœºæ™¯é‡å»º` `é«˜æ–¯æº…å°„` `å¤šè§†å›¾å­¦ä¹ ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `ç‰¹å¾æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨3Dåœºæ™¯é‡å»ºä¸­ç”Ÿæˆå¤§é‡å†—ä½™é«˜æ–¯åˆ†å¸ƒï¼Œå¯¼è‡´å†…å­˜å¼€é”€å¤§ï¼Œå¤šè§†å›¾ç‰¹å¾èšåˆæ•ˆæœå·®ã€‚
2. C3Gé€šè¿‡å­¦ä¹ tokensèšåˆå¤šè§†å›¾ç‰¹å¾ï¼ŒæŒ‡å¯¼é«˜æ–¯åˆ†å¸ƒç”Ÿæˆï¼Œä»…åœ¨å…³é”®ä½ç½®ä¼°è®¡ç´§å‡‘çš„3Dé«˜æ–¯åˆ†å¸ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒC3Gåœ¨å†…å­˜æ•ˆç‡å’Œç‰¹å¾ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ–°è§†è§’åˆæˆå’Œåœºæ™¯ç†è§£æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºC3Gçš„æ–°å‹å‰é¦ˆæ¡†æ¶ï¼Œç”¨äºä»æ— å§¿æ€çš„ç¨€ç–è§†å›¾ä¸­é‡å»ºå’Œç†è§£3Dåœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨åŸºäºåƒç´ çš„3Dé«˜æ–¯æº…å°„è¿›è¡Œé‡å»ºï¼Œç„¶åè¿›è¡Œ2Dåˆ°3Dçš„ç‰¹å¾æå‡ä»¥è¿›è¡Œåœºæ™¯ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¼šç”Ÿæˆè¿‡å¤šçš„å†—ä½™é«˜æ–¯åˆ†å¸ƒï¼Œå¯¼è‡´é«˜å†…å­˜å¼€é”€å’Œæ¬¡ä¼˜çš„å¤šè§†å›¾ç‰¹å¾èšåˆï¼Œä»è€Œé™ä½æ–°è§†è§’åˆæˆå’Œåœºæ™¯ç†è§£çš„æ€§èƒ½ã€‚C3Gä»…åœ¨å¿…è¦çš„ç©ºé—´ä½ç½®ä¼°è®¡ç´§å‡‘çš„3Dé«˜æ–¯åˆ†å¸ƒï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘å†—ä½™ï¼ŒåŒæ—¶å®ç°æœ‰æ•ˆçš„ç‰¹å¾æå‡ã€‚C3Gå¼•å…¥å¯å­¦ä¹ çš„tokensï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›èšåˆå¤šè§†å›¾ç‰¹å¾æ¥æŒ‡å¯¼é«˜æ–¯åˆ†å¸ƒçš„ç”Ÿæˆï¼Œç¡®ä¿æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒæ•´åˆæ¥è‡ªå¤šä¸ªè§†å›¾çš„ç›¸å…³è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›æ¨¡å¼è¿›è¡Œé«˜æ–¯è§£ç ï¼Œä»¥é«˜æ•ˆåœ°æå‡ç‰¹å¾ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒC3Gåœ¨æ— å§¿æ€æ–°è§†è§’åˆæˆã€3Då¼€æ”¾è¯æ±‡åˆ†å‰²å’Œè§†è§’ä¸å˜ç‰¹å¾èšåˆæ–¹é¢å…·æœ‰æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œç´§å‡‘ä½†å…·æœ‰å‡ ä½•æ„ä¹‰çš„è¡¨ç¤ºè¶³ä»¥å®ç°é«˜è´¨é‡çš„åœºæ™¯é‡å»ºå’Œç†è§£ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†å“è¶Šçš„å†…å­˜æ•ˆç‡å’Œç‰¹å¾ä¿çœŸåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨ä»ç¨€ç–è§†å›¾é‡å»º3Dåœºæ™¯æ—¶ï¼Œä¼šç”Ÿæˆå¤§é‡å†—ä½™çš„3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¯¼è‡´å†…å­˜å ç”¨è¿‡é«˜ï¼Œå¹¶ä¸”å½±å“å¤šè§†å›¾ç‰¹å¾çš„æœ‰æ•ˆèšåˆï¼Œæœ€ç»ˆé™ä½äº†æ–°è§†è§’åˆæˆå’Œåœºæ™¯ç†è§£çš„æ€§èƒ½ã€‚è¿™äº›å†—ä½™çš„é«˜æ–¯åˆ†å¸ƒå¹¶æ²¡æœ‰æä¾›é¢å¤–çš„å‡ ä½•ä¿¡æ¯ï¼Œåè€Œå¢åŠ äº†è®¡ç®—è´Ÿæ‹…ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šC3Gçš„æ ¸å¿ƒæ€è·¯æ˜¯å­¦ä¹ ä¸€ç§ç´§å‡‘çš„3Dè¡¨ç¤ºï¼Œåªåœ¨å¿…è¦çš„ç©ºé—´ä½ç½®ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒï¼Œä»è€Œå‡å°‘å†—ä½™ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„tokensï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶èšåˆå¤šè§†å›¾ç‰¹å¾ï¼ŒæŒ‡å¯¼é«˜æ–¯åˆ†å¸ƒçš„ç”Ÿæˆï¼Œç¡®ä¿æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒéƒ½åŒ…å«æ¥è‡ªå¤šä¸ªè§†è§’çš„ç›¸å…³è§†è§‰ä¿¡æ¯ã€‚è¿™æ ·æ—¢èƒ½å‡å°‘å†…å­˜å ç”¨ï¼Œåˆèƒ½æé«˜ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šC3Gæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å¤šè§†å›¾ç‰¹å¾æå–**ï¼šä»å¤šä¸ªè§†è§’çš„å›¾åƒä¸­æå–ç‰¹å¾ã€‚2) **Tokenç”Ÿæˆä¸ç‰¹å¾èšåˆ**ï¼šç”Ÿæˆå¯å­¦ä¹ çš„tokensï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶èšåˆå¤šè§†å›¾ç‰¹å¾ï¼Œä¸ºæ¯ä¸ªtokenèµ‹äºˆæ¥è‡ªä¸åŒè§†è§’çš„è§†è§‰ä¿¡æ¯ã€‚3) **é«˜æ–¯ç”Ÿæˆ**ï¼šåŸºäºèšåˆåçš„tokenç‰¹å¾ï¼Œç”Ÿæˆ3Dé«˜æ–¯åˆ†å¸ƒï¼Œè¿™äº›é«˜æ–¯åˆ†å¸ƒä½äºåœºæ™¯çš„å…³é”®ä½ç½®ã€‚4) **ç‰¹å¾æå‡**ï¼šåˆ©ç”¨å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå°†2Dç‰¹å¾æå‡åˆ°3Dç©ºé—´ï¼Œç”¨äºåç»­çš„åœºæ™¯ç†è§£ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šC3Gçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **ç´§å‡‘çš„é«˜æ–¯è¡¨ç¤º**ï¼šåªåœ¨å¿…è¦çš„ç©ºé—´ä½ç½®ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒï¼Œå‡å°‘äº†å†—ä½™ï¼Œæé«˜äº†å†…å­˜æ•ˆç‡ã€‚2) **åŸºäºTokençš„å¤šè§†å›¾ç‰¹å¾èšåˆ**ï¼šé€šè¿‡å¯å­¦ä¹ çš„tokenså’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°èšåˆäº†æ¥è‡ªå¤šä¸ªè§†è§’çš„ç‰¹å¾ï¼Œæé«˜äº†ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ã€‚3) **åŸºäºæ³¨æ„åŠ›æ¨¡å¼çš„ç‰¹å¾æå‡**ï¼šåˆ©ç”¨å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œé«˜æ•ˆåœ°å°†2Dç‰¹å¾æå‡åˆ°3Dç©ºé—´ã€‚

**å…³é”®è®¾è®¡**ï¼šC3Gçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **Tokenæ•°é‡**ï¼šé€‰æ‹©åˆé€‚çš„tokenæ•°é‡ï¼Œä»¥å¹³è¡¡è¡¨ç¤ºèƒ½åŠ›å’Œè®¡ç®—æˆæœ¬ã€‚2) **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ•æ‰ä¸åŒè§†è§’ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚3) **æŸå¤±å‡½æ•°**ï¼šè®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–é«˜æ–¯åˆ†å¸ƒçš„ä½ç½®ã€å½¢çŠ¶å’Œé¢œè‰²ï¼Œå¹¶é¼“åŠ±ç”Ÿæˆç´§å‡‘çš„è¡¨ç¤ºã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬é‡å»ºæŸå¤±ã€æ­£åˆ™åŒ–æŸå¤±ç­‰ã€‚4) **é«˜æ–¯åˆ†å¸ƒå‚æ•°åŒ–**ï¼šä½¿ç”¨åˆé€‚çš„å‚æ•°åŒ–æ–¹æ³•æ¥è¡¨ç¤º3Dé«˜æ–¯åˆ†å¸ƒï¼Œä¾‹å¦‚ä½ç½®ã€æ—‹è½¬ã€ç¼©æ”¾å’Œé¢œè‰²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

C3Gåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ— å§¿æ€æ–°è§†è§’åˆæˆä»»åŠ¡ä¸­ï¼ŒC3Gåœ¨ä¿æŒé«˜è§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚åœ¨3Då¼€æ”¾è¯æ±‡åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒC3Gçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¡¨æ˜å…¶å­¦ä¹ åˆ°çš„3Dè¡¨ç¤ºå…·æœ‰æ›´å¥½çš„è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒC3Gåœ¨è§†è§’ä¸å˜ç‰¹å¾èšåˆæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆæ¥è‡ªä¸åŒè§†è§’çš„ç‰¹å¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

C3Gåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºä»ç¨€ç–çš„ä¼ æ„Ÿå™¨æ•°æ®ä¸­é‡å»ºé«˜è´¨é‡çš„3Dåœºæ™¯ï¼Œå¹¶è¿›è¡Œåœºæ™¯ç†è§£ï¼Œä»è€Œå¸®åŠ©æœºå™¨äººæ›´å¥½åœ°æ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒã€‚æ­¤å¤–ï¼ŒC3Gè¿˜å¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„è™šæ‹Ÿåœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›æ²‰æµ¸å¼çš„ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.

