---
layout: default
title: C3G: Learning Compact 3D Representations with 2K Gaussians
---

# C3G: Learning Compact 3D Representations with 2K Gaussians

**arXiv**: [2512.04021v1](https://arxiv.org/abs/2512.04021) | [PDF](https://arxiv.org/pdf/2512.04021.pdf)

**ä½œè€…**: Honggyu An, Jaewoo Jung, Mungyeom Kim, Sunghwan Hong, Chaehyun Kim, Kazumi Fukuda, Minkyeong Jeon, Jisang Han, Takuya Narihira, Hyuna Ko, Junsu Kim, Yuki Mitsufuji, Seungryong Kim

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºC3Gæ¡†æž¶ä»¥è§£å†³æ— å§¿æ€ç¨€ç–è§†å›¾ä¸‹3Dé‡å»ºä¸Žç†è§£ä¸­é«˜æ–¯å†—ä½™å’Œç‰¹å¾èšåˆé—®é¢˜**

**å…³é”®è¯**: `3Dé«˜æ–¯æº…å°„` `ç´§å‡‘è¡¨ç¤º` `å¤šè§†å›¾ç‰¹å¾èšåˆ` `æ— å§¿æ€é‡å»º` `å¼€æ”¾è¯æ±‡åˆ†å‰²` `è§†å›¾ä¸å˜ç‰¹å¾`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ç”Ÿæˆè¿‡å¤šå†—ä½™3Dé«˜æ–¯ï¼Œå¯¼è‡´å†…å­˜å¼€é”€å¤§å’Œå¤šè§†å›¾ç‰¹å¾èšåˆä¸ä½³ï¼Œå½±å“æ–°è§†å›¾åˆæˆå’Œåœºæ™¯ç†è§£æ€§èƒ½ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å¯å­¦ä¹ ä»¤ç‰Œé€šè¿‡è‡ªæ³¨æ„åŠ›èšåˆå¤šè§†å›¾ç‰¹å¾ï¼ŒæŒ‡å¯¼åœ¨å…³é”®ç©ºé—´ä½ç½®ç”Ÿæˆç´§å‡‘3Dé«˜æ–¯ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æ¨¡å¼é«˜æ•ˆæå‡ç‰¹å¾ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ— å§¿æ€æ–°è§†å›¾åˆæˆã€3Då¼€æ”¾è¯æ±‡åˆ†å‰²å’Œè§†å›¾ä¸å˜ç‰¹å¾èšåˆå®žéªŒä¸­ï¼ŒC3Gåœ¨å†…å­˜æ•ˆçŽ‡å’Œç‰¹å¾ä¿çœŸåº¦ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.

