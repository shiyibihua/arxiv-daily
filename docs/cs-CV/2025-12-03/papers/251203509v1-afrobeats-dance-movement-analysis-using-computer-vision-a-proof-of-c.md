---
layout: default
title: AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model
---

# AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model

**arXiv**: [2512.03509v1](https://arxiv.org/abs/2512.03509) | [PDF](https://arxiv.org/pdf/2512.03509.pdf)

**ä½œè€…**: Kwaku Opoku-Ware, Gideon Opoku

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆYOLOä¸ŽSAMçš„æ¡†æž¶ï¼Œç”¨äºŽæ— æ ‡è®°è‡ªåŠ¨åˆ†æžéžæ´²èŠ‚æ‹èˆžè¹ˆåŠ¨ä½œ**

**å…³é”®è¯**: `èˆžè¹ˆåŠ¨ä½œåˆ†æž` `ç›®æ ‡æ£€æµ‹` `å›¾åƒåˆ†å‰²` `è¿åŠ¨é‡åŒ–` `è®¡ç®—æœºè§†è§‰æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªåŠ¨åŒ–èˆžè¹ˆåŠ¨ä½œåˆ†æžï¼Œæ— éœ€ä¸“ä¸šè®¾å¤‡æˆ–æ ‡è®°ï¼Œé‡åŒ–è§†é¢‘ä¸­çš„èˆžè€…è¿åŠ¨
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆYOLOv8/v11æ£€æµ‹èˆžè€…ï¼ŒSAMè¿›è¡Œç²¾ç¡®åˆ†å‰²ï¼Œå®žçŽ°åŠ¨ä½œè·Ÿè¸ªä¸Žé‡åŒ–
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å•è§†é¢‘æµ‹è¯•ä¸­ï¼Œæ£€æµ‹ç²¾åº¦çº¦94%ï¼Œåˆ†å‰²IoUçº¦83%ï¼Œé‡åŒ–æ˜¾ç¤ºä¸»èˆžè€…åŠ¨ä½œæ›´é¢‘ç¹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.

