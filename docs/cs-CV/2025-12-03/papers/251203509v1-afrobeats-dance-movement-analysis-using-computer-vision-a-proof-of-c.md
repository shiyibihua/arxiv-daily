---
layout: default
title: AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model
---

# AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.03509" target="_blank" class="toolbar-btn">arXiv: 2512.03509v1</a>
    <a href="https://arxiv.org/pdf/2512.03509.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03509v1" 
            onclick="toggleFavorite(this, '2512.03509v1', 'AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kwaku Opoku-Ware, Gideon Opoku

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

**DOI**: [10.48550/arXiv.2512.03509](https://doi.org/10.48550/arXiv.2512.03509)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÁªìÂêàYOLOÂíåSAMÁöÑAfroBeatsËàûËπàÂä®‰ΩúÂàÜÊûêÊ°ÜÊû∂ÔºåÊó†ÈúÄ‰∏ì‰∏öËÆæÂ§á„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËàûËπàÂä®‰ΩúÂàÜÊûê` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `YOLO` `Segment Anything Model` `ÁõÆÊ†áÊ£ÄÊµã` `ÂõæÂÉèÂàÜÂâ≤` `AfroBeatsËàûËπà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËàûËπàÂä®‰ΩúÂàÜÊûêÊñπÊ≥ï‰æùËµñ‰∏ì‰∏öËÆæÂ§áÊàñÊ†áËÆ∞ÔºåÊàêÊú¨È´ò‰∏î‰∏ç‰æøÊê∫ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫ÁªìÂêàYOLOÂíåSAMÁöÑÊ°ÜÊû∂ÔºåÂÆûÁé∞Êó†ÈúÄÊ†áËÆ∞ÁöÑËàûËπàÂä®‰ΩúÂàÜÊûêÔºåÈôç‰Ωé‰∫Ü‰ΩøÁî®Èó®Êßõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®AfroBeatsËàûËπàËßÜÈ¢ë‰∏äÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ£ÄÊµãÂíåÂàÜÂâ≤ÊÄßËÉΩÔºå‰∏∫ÂÆöÈáèËàûËπàÂàÜÊûêÊèê‰æõÂü∫Á°Ä„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÂàùÊ≠•Á†îÁ©∂‰∫Ü‰ΩøÁî®Áé∞‰ª£ËÆ°ÁÆóÊú∫ËßÜËßâÊäÄÊúØËøõË°åËá™Âä®ËàûËπàÂä®‰ΩúÂàÜÊûê„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ê¶ÇÂøµÈ™åËØÅÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÈõÜÊàê‰∫ÜYOLOv8Âíåv11ËøõË°åËàûËÄÖÊ£ÄÊµãÔºåÂπ∂ÁªìÂêàSegment Anything Model (SAM) ËøõË°åÁ≤æÁ°ÆÂàÜÂâ≤Ôºå‰ªéËÄåËÉΩÂ§üÂú®Êó†ÈúÄ‰∏ìÁî®ËÆæÂ§áÊàñÊ†áËÆ∞ÁöÑÊÉÖÂÜµ‰∏ãÔºåË∑üË∏™ÂíåÈáèÂåñËßÜÈ¢ëËÆ∞ÂΩï‰∏≠ÁöÑËàûËÄÖÂä®‰Ωú„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïËØÜÂà´ËßÜÈ¢ëÂ∏ß‰∏≠ÁöÑËàûËÄÖÔºåËÆ°ÁÆóÁ¶ªÊï£ÁöÑËàûÊ≠•ÔºåËÆ°ÁÆóÁ©∫Èó¥Ë¶ÜÁõñÊ®°ÂºèÔºåÂπ∂ÊµãÈáèË°®ÊºîÂ∫èÂàó‰∏≠ÁöÑËäÇÂ•è‰∏ÄËá¥ÊÄß„ÄÇÂú®‰∏ÄÊÆµ49ÁßíÁöÑÂä†Á∫≥AfroBeatsËàûËπàÂΩïÂÉè‰∏äÊµãËØïËØ•Ê°ÜÊû∂ÔºåËØÅÊòé‰∫ÜÊäÄÊúØÂèØË°åÊÄßÔºåÁ≥ªÁªüÂú®ÊâãÂä®Ê£ÄÊü•ÁöÑÊ†∑Êú¨‰∏äÂÆûÁé∞‰∫ÜÁ∫¶94%ÁöÑÊ£ÄÊµãÁ≤æÂ∫¶Âíå89%ÁöÑÂè¨ÂõûÁéá„ÄÇSAMÊèê‰æõÁöÑÂÉèÁ¥†Á∫ßÂàÜÂâ≤Ôºå‰∏éËßÜËßâÊ£ÄÊü•Áõ∏ÊØîÂÆûÁé∞‰∫ÜÁ∫¶83%ÁöÑ‰∫§Âπ∂ÊØîÔºåËÉΩÂ§üÈáèÂåñË∂ÖÂá∫ËæπÁïåÊ°ÜÊñπÊ≥ïÊâÄËÉΩË°®Á§∫ÁöÑË∫´‰ΩìÂßøÊÄÅÂèòÂåñ„ÄÇÂàùÊ≠•Ê°à‰æãÁ†îÁ©∂ÂàÜÊûêË°®ÊòéÔºåÊàë‰ª¨ÁöÑÁ≥ªÁªüÂàÜÁ±ª‰∏∫‰∏ªË¶ÅÁöÑËàûËÄÖÊØîÂàÜÁ±ª‰∏∫Ê¨°Ë¶ÅÁöÑËàûËÄÖÂ§öÊâßË°å‰∫Ü23%ÁöÑÊ≠•Êï∞ÔºåËøêÂä®Âº∫Â∫¶È´òÂá∫37%ÔºåÂπ∂‰∏î‰ΩøÁî®ÁöÑË°®ÊºîÁ©∫Èó¥Â§öÂá∫42%„ÄÇÁÑ∂ËÄåÔºåËøôÈ°πÂ∑•‰Ωú‰ª£Ë°®‰∫Ü‰∏Ä‰∏™Êó©ÊúüÈò∂ÊÆµÁöÑÁ†îÁ©∂ÔºåÂ≠òÂú®ÂæàÂ§ßÁöÑÂ±ÄÈôêÊÄßÔºåÂåÖÊã¨ÂçïËßÜÈ¢ëÈ™åËØÅ„ÄÅÁº∫‰πèÁ≥ªÁªüÁöÑground truthÊ†áÊ≥®‰ª•ÂèäÁº∫‰πè‰∏éÁé∞ÊúâÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÁöÑÊØîËæÉ„ÄÇÊàë‰ª¨ÊèêÂá∫Ëøô‰∏™Ê°ÜÊû∂ÊòØ‰∏∫‰∫ÜËØÅÊòéÊäÄÊúØÂèØË°åÊÄßÔºåÁ°ÆÂÆöÂÆöÈáèËàûËπàÊåáÊ†áÁöÑÊúâÂ∏åÊúõÁöÑÊñπÂêëÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ≥ªÁªüÈ™åËØÅÁ†îÁ©∂Â•†ÂÆöÂü∫Á°Ä„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËàûËπàÂä®‰ΩúÂàÜÊûê‰∏≠ÂØπ‰∏ì‰∏öËÆæÂ§áÊàñÊ†áËÆ∞ÁöÑ‰æùËµñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊàêÊú¨È´òÊòÇ‰∏îËÆæÁΩÆÂ§çÊùÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Êõ¥ÂπøÊ≥õÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®Ôºå‰æãÂ¶ÇÈùû‰∏ì‰∏öËàûËπàÊïôÂ≠¶„ÄÅÂä®‰ΩúÊçïÊçâÂàÜÊûêÁ≠â„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊó†ÈúÄÁâπÊÆäËÆæÂ§áÔºå‰ªÖÈÄöËøáËßÜÈ¢ëÂç≥ÂèØËøõË°åËàûËπàÂä®‰ΩúÂàÜÊûêÁöÑÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËÆ°ÁÆóÊú∫ËßÜËßâÊäÄÊúØÔºåÁâπÂà´ÊòØÁõÆÊ†áÊ£ÄÊµãÂíåÂõæÂÉèÂàÜÂâ≤ÔºåËá™Âä®ËØÜÂà´ÂíåÂàÜÂâ≤ËßÜÈ¢ë‰∏≠ÁöÑËàûËÄÖÔºåËøõËÄåÂàÜÊûêÂÖ∂Âä®‰Ωú„ÄÇÈÄöËøáYOLOËøõË°åÂø´ÈÄüÂáÜÁ°ÆÁöÑËàûËÄÖÊ£ÄÊµãÔºåÂÜçÂà©Áî®SAMËøõË°åÂÉèÁ¥†Á∫ßÂà´ÁöÑÁ≤æÁ°ÆÂàÜÂâ≤Ôºå‰ªéËÄåËÉΩÂ§üÊõ¥Á≤æÁªÜÂú∞ÊçïÊçâËàûËÄÖÁöÑË∫´‰ΩìÂßøÊÄÅÂíåÂä®‰ΩúÂèòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ËàûËÄÖÊ£ÄÊµãÔºö‰ΩøÁî®YOLOv8Êàñv11Ê£ÄÊµãËßÜÈ¢ëÂ∏ß‰∏≠ÁöÑËàûËÄÖÔºåÂæóÂà∞ËàûËÄÖÁöÑËæπÁïåÊ°Ü„ÄÇ2) ËàûËÄÖÂàÜÂâ≤ÔºöÂà©Áî®SAMÂØπÊ£ÄÊµãÂà∞ÁöÑËàûËÄÖËøõË°åÂÉèÁ¥†Á∫ßÂà´ÁöÑÂàÜÂâ≤ÔºåÂæóÂà∞ËàûËÄÖÁöÑÁ≤æÁ°ÆËΩÆÂªì„ÄÇ3) Âä®‰ΩúÈáèÂåñÔºöÂü∫‰∫éÂàÜÂâ≤ÁªìÊûúÔºåËÆ°ÁÆóËàûÊ≠•Êï∞Èáè„ÄÅÁ©∫Èó¥Ë¶ÜÁõñÊ®°ÂºèÂíåËäÇÂ•è‰∏ÄËá¥ÊÄßÁ≠âÊåáÊ†á„ÄÇ4) ÁªìÊûúÂàÜÊûêÔºöÂØπÈáèÂåñÂêéÁöÑÂä®‰ΩúÊåáÊ†áËøõË°åÂàÜÊûêÔºåÊØîËæÉ‰∏çÂêåËàûËÄÖÁöÑÂä®‰ΩúÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•Á†îÁ©∂ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜYOLOÂíåSAMÁªìÂêàÂ∫îÁî®‰∫éËàûËπàÂä®‰ΩúÂàÜÊûê„ÄÇYOLOÊèê‰æõÂø´ÈÄüÂáÜÁ°ÆÁöÑËàûËÄÖÊ£ÄÊµãÔºåËÄåSAMÊèê‰æõÂÉèÁ¥†Á∫ßÂà´ÁöÑÁ≤æÁ°ÆÂàÜÂâ≤Ôºå‰∏§ËÄÖÁªìÂêàËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊçïÊçâËàûËÄÖÁöÑÂä®‰ΩúÁªÜËäÇÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÂü∫‰∫éËæπÁïåÊ°ÜÁöÑÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠Êú™ÊòéÁ°ÆËØ¥ÊòéYOLOÂíåSAMÁöÑÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆ„ÄÇ‰ΩÜÊèêÂà∞‰ΩøÁî®YOLOv8Âíåv11ËøõË°åÂÆûÈ™åÔºåÂπ∂‰ΩøÁî®SAMËøõË°åÂÉèÁ¥†Á∫ßÂàÜÂâ≤ÔºåÈÄöËøáËÆ°ÁÆó‰∫§Âπ∂ÊØîÔºàIoUÔºâËØÑ‰º∞ÂàÜÂâ≤ÊïàÊûú„ÄÇÂä®‰ΩúÈáèÂåñÊñπÈù¢ÔºåÈÄöËøáÁªüËÆ°ÂÉèÁ¥†ÂèòÂåñÊù•‰º∞ËÆ°ËàûÊ≠•Êï∞ÈáèÂíåËøêÂä®Âº∫Â∫¶ÔºåÈÄöËøáËÆ°ÁÆóËàûËÄÖÂú®ËßÜÈ¢ëÂ∏ß‰∏≠ÁöÑ‰ΩçÁΩÆÂèòÂåñÊù•‰º∞ËÆ°Á©∫Èó¥Ë¶ÜÁõñÊ®°Âºè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Ê°ÜÊû∂Âú®AfroBeatsËàûËπàËßÜÈ¢ë‰∏äËøõË°å‰∫ÜÂàùÊ≠•È™åËØÅÔºåÂÆûÁé∞‰∫ÜÁ∫¶94%ÁöÑÊ£ÄÊµãÁ≤æÂ∫¶Âíå89%ÁöÑÂè¨ÂõûÁéá„ÄÇSAMÊèê‰æõÁöÑÂÉèÁ¥†Á∫ßÂàÜÂâ≤‰∏éËßÜËßâÊ£ÄÊü•Áõ∏ÊØîÂÆûÁé∞‰∫ÜÁ∫¶83%ÁöÑ‰∫§Âπ∂ÊØî„ÄÇÊ°à‰æãÁ†îÁ©∂Ë°®ÊòéÔºå‰∏ªË¶ÅËàûËÄÖÊØîÊ¨°Ë¶ÅËàûËÄÖÂ§öÊâßË°å‰∫Ü23%ÁöÑÊ≠•Êï∞ÔºåËøêÂä®Âº∫Â∫¶È´òÂá∫37%ÔºåÂπ∂‰∏î‰ΩøÁî®ÁöÑË°®ÊºîÁ©∫Èó¥Â§öÂá∫42%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËàûËπàÊïôÂ≠¶„ÄÅÂä®‰ΩúÊçïÊçâÂàÜÊûê„ÄÅËøêÂä®Â∫∑Â§çÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ËàûËπàÊïôÂ≠¶‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•Á≥ªÁªüËá™Âä®ËØÑ‰º∞Â≠¶ÁîüÁöÑÂä®‰ΩúËßÑËåÉÊÄßÔºåÊèê‰æõ‰∏™ÊÄßÂåñÁöÑÊåáÂØº„ÄÇÂú®ËøêÂä®Â∫∑Â§ç‰∏≠ÔºåÂèØ‰ª•Áî®‰∫éÁõëÊµãÊÇ£ËÄÖÁöÑÂ∫∑Â§çËøõÂ∫¶ÔºåËØÑ‰º∞Ê≤ªÁñóÊïàÊûú„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØÁî®‰∫éÊ∏∏ÊàèÂºÄÂèë„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.

