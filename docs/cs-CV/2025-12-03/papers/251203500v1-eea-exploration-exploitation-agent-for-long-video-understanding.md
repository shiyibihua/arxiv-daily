---
layout: default
title: EEA: Exploration-Exploitation Agent for Long Video Understanding
---

# EEA: Exploration-Exploitation Agent for Long Video Understanding

**arXiv**: [2512.03500v1](https://arxiv.org/abs/2512.03500) | [PDF](https://arxiv.org/pdf/2512.03500.pdf)

**ä½œè€…**: Te Yang, Xiangyu Zhu, Bo Wang, Quan Chen, Peng Jiang, Zhen Lei

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEEAæ¡†æž¶ï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼çš„å±‚æ¬¡æ ‘æœç´¢å¹³è¡¡æŽ¢ç´¢ä¸Žåˆ©ç”¨ï¼Œä»¥é«˜æ•ˆè§£å†³é•¿è§†é¢‘ç†è§£é—®é¢˜ã€‚**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `æŽ¢ç´¢ä¸Žåˆ©ç”¨å¹³è¡¡` `è¯­ä¹‰å¼•å¯¼` `å±‚æ¬¡æ ‘æœç´¢` `è§†è§‰è¯­è¨€æ¨¡åž‹` `ä¸ç¡®å®šæ€§å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿è§†é¢‘ç†è§£ä¸­å¯†é›†é¢„å¤„ç†è®¡ç®—å¼€é”€å¤§ï¼ŒæŽ¢ç´¢ä¸Žåˆ©ç”¨å¤±è¡¡å¯¼è‡´ä¿¡æ¯è¦†ç›–ä¸å…¨å’Œæ•ˆçŽ‡ä½Žä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šEEAåŸºäºŽè¯­ä¹‰æŸ¥è¯¢åŠ¨æ€æ›´æ–°é”šç‚¹ï¼Œåœ¨æ ‘æœç´¢ä¸­ä¼˜å…ˆæŽ¢ç´¢è¯­ä¹‰ç›¸å…³å¸§ï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡åž‹å¥–åŠ±ä¸Žè¯­ä¹‰å…ˆéªŒå»ºæ¨¡ä¸ç¡®å®šæ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œè®¡ç®—æ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

