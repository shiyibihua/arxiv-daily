---
layout: default
title: EEA: Exploration-Exploitation Agent for Long Video Understanding
---

# EEA: Exploration-Exploitation Agent for Long Video Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.03500" target="_blank" class="toolbar-btn">arXiv: 2512.03500v1</a>
    <a href="https://arxiv.org/pdf/2512.03500.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03500v1" 
            onclick="toggleFavorite(this, '2512.03500v1', 'EEA: Exploration-Exploitation Agent for Long Video Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Te Yang, Xiangyu Zhu, Bo Wang, Quan Chen, Peng Jiang, Zhen Lei

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EEAÔºö‰∏ÄÁßçÁî®‰∫éÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÊé¢Á¥¢-Âà©Áî®Êô∫ËÉΩ‰ΩìÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `Êé¢Á¥¢-Âà©Áî®` `ËØ≠‰πâÂºïÂØº` `ÂàÜÂ±ÇÊ†ëÊêúÁ¥¢` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈïøËßÜÈ¢ëÁêÜËß£ÊñπÊ≥ïÂú®ËÆ°ÁÆóÂºÄÈîÄÂíåÊé¢Á¥¢-Âà©Áî®Âπ≥Ë°°ÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÂíå‰ø°ÊÅØË¶ÜÁõñ‰∏çÂÆåÊï¥„ÄÇ
2. EEAÈÄöËøáËØ≠‰πâÂºïÂØºÁöÑÂàÜÂ±ÇÊ†ëÊêúÁ¥¢ÔºåËá™‰∏ªÂèëÁé∞Âπ∂Âä®ÊÄÅÊõ¥Êñ∞‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËØ≠‰πâÊü•ËØ¢ÔºåÂπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEEAÂú®Â§ö‰∏™ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÂçìË∂äÁöÑÊÄßËÉΩÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈïøËßÜÈ¢ëÁêÜËß£ÈúÄË¶ÅÈ´òÊïàÂú∞ÂØºËà™Â§ßÈáèÁöÑËßÜËßâÊï∞ÊçÆÔºå‰ª•Á≤æÁ°ÆÂÆö‰ΩçÁ®ÄÁñè‰ΩÜÂÖ≥ÈîÆÁöÑ‰ø°ÊÅØ„ÄÇÁõÆÂâçÁöÑÊñπÊ≥ïË¶Å‰πàÁî±‰∫éÂØÜÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜËÄåÂØºËá¥‰∏•ÈáçÁöÑËÆ°ÁÆóÂºÄÈîÄÔºåË¶Å‰πàÊó†Ê≥ïÊúâÊïàÂú∞Âπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®Ôºå‰ªéËÄåÂØºËá¥‰ø°ÊÅØË¶ÜÁõñ‰∏çÂÆåÊï¥ÂíåÊïàÁéá‰Ωé‰∏ã„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂EEAÔºåÈÄöËøáÂÖ∑ÊúâÂàÜÂ±ÇÊ†ëÊêúÁ¥¢ËøáÁ®ãÁöÑËØ≠‰πâÊåáÂØºÊù•ÂÆûÁé∞Êé¢Á¥¢-Âà©Áî®ÁöÑÂπ≥Ë°°„ÄÇEEAËá™‰∏ªÂèëÁé∞Âπ∂Âä®ÊÄÅÊõ¥Êñ∞‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËØ≠‰πâÊü•ËØ¢ÔºåÂπ∂Êî∂ÈõÜ‰∏éËøô‰∫õÊü•ËØ¢Á¥ßÂØÜÂåπÈÖçÁöÑËßÜÈ¢ëÂ∏ß‰Ωú‰∏∫ËØ≠‰πâÈîöÁÇπ„ÄÇÂú®Ê†ëÊêúÁ¥¢ËøáÁ®ã‰∏≠ÔºåEEA‰ºòÂÖàÊé¢Á¥¢ËØ≠‰πâÁõ∏ÂÖ≥ÁöÑÂ∏ßÔºåÂêåÊó∂Á°Æ‰øùÂú®Êú™Áü•ÁâáÊÆµÂÜÖÊúâË∂≥Â§üÁöÑË¶ÜÁõñÔºåËÄå‰∏çÊòØÁªü‰∏ÄÊâ©Â±ï„ÄÇÊ≠§Â§ñÔºåEEAÈÄöËøáÊòæÂºèÂª∫Ê®°‰∏çÁ°ÆÂÆöÊÄßÔºåËá™ÈÄÇÂ∫îÂú∞Â∞ÜÊù•Ëá™ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂÜÖÂú®Â•ñÂä±‰∏éËØ≠‰πâÂÖàÈ™åÁõ∏ÁªìÂêàÔºå‰ª•ÂÆûÁé∞ÂØπËßÜÈ¢ëÁâáÊÆµÁöÑÁ®≥ÂÆöÂíåÁ≤æÁ°ÆËØÑ‰º∞„ÄÇÂú®ÂêÑÁßçÈïøËßÜÈ¢ëÂü∫ÂáÜ‰∏äÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑ‰ºòË∂äÊÄßËÉΩÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°Èù¢‰∏¥ÁöÑÂÖ≥ÈîÆÊåëÊàòÊòØÂ¶Ç‰ΩïÂú®Êµ∑ÈáèËßÜÈ¢ëÊï∞ÊçÆ‰∏≠È´òÊïàÂú∞ÂÆö‰ΩçÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂ≠òÂú®‰∏§‰∏™ÁóõÁÇπÔºö‰∏ÄÊòØËøõË°åÂØÜÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜÔºåÂØºËá¥ËÆ°ÁÆóÂºÄÈîÄÂ∑®Â§ßÔºõ‰∫åÊòØÊó†Ê≥ïÊúâÊïàÂú∞Âπ≥Ë°°Êé¢Á¥¢ÔºàÂØªÊâæÊñ∞ÁöÑ‰ø°ÊÅØÔºâÂíåÂà©Áî®ÔºàÂà©Áî®Â∑≤Áü•‰ø°ÊÅØÔºâÔºåÂØºËá¥‰ø°ÊÅØË¶ÜÁõñ‰∏çÂÆåÊï¥ÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEEAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËØ≠‰πâÂºïÂØºÁöÑÊé¢Á¥¢-Âà©Áî®Á≠ñÁï•ÔºåÂú®ÈïøËßÜÈ¢ë‰∏≠È´òÊïàÂú∞ÂÆö‰ΩçÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇÂÆÉÂà©Áî®ËØ≠‰πâÊü•ËØ¢‰Ωú‰∏∫ÊåáÂØºÔºå‰ºòÂÖàÊé¢Á¥¢‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ëÂ∏ßÔºåÂêåÊó∂‰øùËØÅÂØπÊú™Áü•Âå∫ÂüüÁöÑÂÖÖÂàÜË¶ÜÁõñ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåEEAËÉΩÂ§üÂú®ËÆ°ÁÆóËµÑÊ∫êÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåÊúÄÂ§ßÂåñ‰ø°ÊÅØËé∑ÂèñÁöÑÊïàÁéáÂíåÂÆåÊï¥ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEEAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) **ËØ≠‰πâÊü•ËØ¢ÁîüÊàêÊ®°Âùó**ÔºöËá™‰∏ªÂèëÁé∞Âπ∂Âä®ÊÄÅÊõ¥Êñ∞‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËØ≠‰πâÊü•ËØ¢„ÄÇ2) **ËØ≠‰πâÈîöÁÇπÊûÑÂª∫Ê®°Âùó**ÔºöÊî∂ÈõÜ‰∏éËØ≠‰πâÊü•ËØ¢Á¥ßÂØÜÂåπÈÖçÁöÑËßÜÈ¢ëÂ∏ß‰Ωú‰∏∫ËØ≠‰πâÈîöÁÇπ„ÄÇ3) **ÂàÜÂ±ÇÊ†ëÊêúÁ¥¢Ê®°Âùó**ÔºöÂú®ËßÜÈ¢ë‰∏≠ËøõË°åÂàÜÂ±ÇÊ†ëÊêúÁ¥¢Ôºå‰ºòÂÖàÊé¢Á¥¢ËØ≠‰πâÁõ∏ÂÖ≥ÁöÑÂ∏ßÔºåÂêåÊó∂‰øùËØÅÂØπÊú™Áü•Âå∫ÂüüÁöÑË¶ÜÁõñ„ÄÇ4) **Â•ñÂä±ËØÑ‰º∞Ê®°Âùó**ÔºöËá™ÈÄÇÂ∫îÂú∞ÁªìÂêàËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂÜÖÂú®Â•ñÂä±ÂíåËØ≠‰πâÂÖàÈ™åÔºåÂØπËßÜÈ¢ëÁâáÊÆµËøõË°åËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEEAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Êé¢Á¥¢-Âà©Áî®ÁöÑÂπ≥Ë°°Á≠ñÁï•„ÄÇ‰∏é‰º†ÁªüÁöÑÂùáÂåÄÊé¢Á¥¢ÊàñË¥™Â©™Âà©Áî®ÊñπÊ≥ï‰∏çÂêåÔºåEEAÈÄöËøáËØ≠‰πâÂºïÂØºÔºåËÉΩÂ§üÊõ¥Âä†Êô∫ËÉΩÂú∞ÈÄâÊã©Êé¢Á¥¢Âå∫ÂüüÔºå‰ªéËÄåÊèêÈ´ò‰ø°ÊÅØËé∑ÂèñÁöÑÊïàÁéá„ÄÇÊ≠§Â§ñÔºåEEAËøòÈÄöËøáÊòæÂºèÂª∫Ê®°‰∏çÁ°ÆÂÆöÊÄßÔºåÂÆûÁé∞‰∫ÜÂØπËßÜÈ¢ëÁâáÊÆµÁöÑÁ®≥ÂÆöÂíåÁ≤æÁ°ÆËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöEEAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) **ËØ≠‰πâÊü•ËØ¢ÁöÑË°®Á§∫ÂíåÊõ¥Êñ∞ÊñπÂºè**ÔºöÂÖ∑‰ΩìÂ¶Ç‰ΩïË°®Á§∫ËØ≠‰πâÊü•ËØ¢Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÊ†πÊçÆÂ∑≤Êé¢Á¥¢ÁöÑ‰ø°ÊÅØÂä®ÊÄÅÊõ¥Êñ∞Êü•ËØ¢„ÄÇ2) **ÂàÜÂ±ÇÊ†ëÊêúÁ¥¢ÁöÑÁ≠ñÁï•**ÔºöÂ¶Ç‰ΩïËÆæËÆ°Ê†ëÁöÑÁªìÊûÑÂíåÊêúÁ¥¢ÁÆóÊ≥ïÔºå‰ª•ÂÆûÁé∞È´òÊïàÁöÑÊé¢Á¥¢ÂíåÂà©Áî®„ÄÇ3) **Â•ñÂä±ÂáΩÊï∞ÁöÑÊûÑÂª∫**ÔºöÂ¶Ç‰ΩïÁªìÂêàVLMÁöÑÂÜÖÂú®Â•ñÂä±ÂíåËØ≠‰πâÂÖàÈ™åÔºåÂπ∂ËÄÉËôë‰∏çÁ°ÆÂÆöÊÄßÔºåÊù•ÊûÑÂª∫‰∏Ä‰∏™Á®≥ÂÆöÂíåÁ≤æÁ°ÆÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇËøô‰∫õÁªÜËäÇÂÜ≥ÂÆö‰∫ÜEEAÁöÑÊÄßËÉΩÂíåÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEEAÂú®Â§ö‰∏™ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®XXXÊï∞ÊçÆÈõÜ‰∏äÔºåEEAÁöÑÊÄßËÉΩÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜX%„ÄÇÊ≠§Â§ñÔºåEEAËøòÂ±ïÁé∞Âá∫Êõ¥È´òÁöÑËÆ°ÁÆóÊïàÁéáÔºåÂú®ËææÂà∞Áõ∏ÂêåÊÄßËÉΩÊ∞¥Âπ≥ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâÄÈúÄÁöÑËÆ°ÁÆóËµÑÊ∫êÊõ¥Â∞ë„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜEEAÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EEAÂú®ÈïøËßÜÈ¢ëÁêÜËß£È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇËßÜÈ¢ëÊëòË¶Å„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅÊïôËÇ≤ËßÜÈ¢ëÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÈ´òÊïàÂú∞ÂÆö‰ΩçÂÖ≥ÈîÆ‰ø°ÊÅØÔºåEEAÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑Âø´ÈÄüÁêÜËß£ÈïøËßÜÈ¢ëÁöÑÂÜÖÂÆπÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÔºåÂπ∂‰∏∫Áõ∏ÂÖ≥È¢ÜÂüüÁöÑÊô∫ËÉΩÂåñÂ∫îÁî®Êèê‰æõÊäÄÊúØÊîØÊåÅ„ÄÇÊú™Êù•ÔºåEEAËøòÂèØ‰ª•Â∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑËßÜÈ¢ëÂàÜÊûê‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÊïÖ‰∫ãÁêÜËß£„ÄÅËßÜÈ¢ëÈóÆÁ≠îÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

