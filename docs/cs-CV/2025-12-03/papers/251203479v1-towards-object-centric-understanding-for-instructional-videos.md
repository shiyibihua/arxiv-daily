---
layout: default
title: Towards Object-centric Understanding for Instructional Videos
---

# Towards Object-centric Understanding for Instructional Videos

**arXiv**: [2512.03479v1](https://arxiv.org/abs/2512.03479) | [PDF](https://arxiv.org/pdf/2512.03479.pdf)

**ä½œè€…**: Wenliang Guo, Yu Kong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹è±¡ä¸­å¿ƒèŒƒå¼ä¸ŽObject-IVQAåŸºå‡†ï¼Œä»¥æå‡æ•™å­¦è§†é¢‘ä¸­åŸºäºŽå¯¹è±¡çŠ¶æ€çš„ç†è§£èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `æ•™å­¦è§†é¢‘ç†è§£` `å¯¹è±¡ä¸­å¿ƒæŽ¨ç†` `é•¿è§†é¢‘åŸºå‡†` `çŠ¶æ€è½¬æ¢` `å¤šè·³æŽ¨ç†` `å¼€æ”¾é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰åŠ¨ä½œä¸­å¿ƒæ–¹æ³•éš¾ä»¥å¤„ç†çœŸå®žæ•™å­¦è§†é¢‘ä¸­æ­¥éª¤é¡ºåºéšå¯¹è±¡çŠ¶æ€å˜åŒ–çš„çµæ´»æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†åŠ¨ä½œè§†ä¸ºé©±åŠ¨çŠ¶æ€è½¬æ¢çš„æœºåˆ¶ï¼Œå¹¶å¼•å…¥å¯¹è±¡ä¸­å¿ƒè§„åˆ’ã€æ„ŸçŸ¥ã€åˆ†æžå’Œç”Ÿæˆå·¥å…·æ¡†æž¶ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨Object-IVQAåŸºå‡†ä¸Šï¼ŒçŽ°æœ‰å¤§è§†è§‰è¯­è¨€æ¨¡åž‹è¡¨çŽ°ä¸ä½³ï¼Œè€Œæ‰€ææ¡†æž¶æ˜¾è‘—æå‡å¯¹è±¡çº§è¯†åˆ«ä¸ŽæŽ¨ç†èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.

