---
layout: default
title: Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles
---

# Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles

**arXiv**: [2512.03454v1](https://arxiv.org/abs/2512.03454) | [PDF](https://arxiv.org/pdf/2512.03454.pdf)

**ä½œè€…**: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThinkDeeperæ¡†æž¶ï¼Œé€šè¿‡ä¸–ç•Œæ¨¡åž‹æŽ¨ç†æœªæ¥ç©ºé—´çŠ¶æ€ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ¨¡ç³Šå®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶è§†è§‰å®šä½` `ä¸–ç•Œæ¨¡åž‹æŽ¨ç†` `å¤šæ¨¡æ€èžåˆ` `ç©ºé—´çŠ¶æ€é¢„æµ‹` `è¶…å›¾è§£ç å™¨` `è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç†è§£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰è‡ªåŠ¨é©¾é©¶è§†è§‰å®šä½æ–¹æ³•éš¾ä»¥å¤„ç†æ¨¡ç³Šã€ä¾èµ–ä¸Šä¸‹æ–‡çš„æŒ‡ä»¤ï¼Œç¼ºä¹å¯¹3Dç©ºé—´å…³ç³»å’Œåœºæ™¯æ¼”å˜çš„æŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽä¸–ç•Œæ¨¡åž‹åŽŸç†ï¼Œè®¾è®¡Spatial-Aware World ModelæŽ¨ç†æœªæ¥æ½œåœ¨çŠ¶æ€ï¼Œç»“åˆè¶…å›¾å¼•å¯¼è§£ç å™¨èžåˆå¤šæ¨¡æ€è¾“å…¥è¿›è¡Œå®šä½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŽ’åé¢†å…ˆï¼Œå°¤å…¶åœ¨æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹è¡¨çŽ°å‡ºå¼ºé²æ£’æ€§å’Œé«˜æ•ˆæ€§ï¼Œæ•°æ®æ•ˆçŽ‡é«˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.

