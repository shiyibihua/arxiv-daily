---
layout: default
title: Lean Unet: A Compact Model for Image Segmentation
---

# Lean Unet: A Compact Model for Image Segmentation

**arXiv**: [2512.03834v1](https://arxiv.org/abs/2512.03834) | [PDF](https://arxiv.org/pdf/2512.03834.pdf)

**ä½œè€…**: Ture Hassler, Ida Ã…kerholm, Marcus NordstrÃ¶m, Gabriele Balletti, Orcun Goksel

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç´§å‡‘åž‹LUnetæž¶æž„ä»¥è§£å†³Unetå†…å­˜å ç”¨å¤§å’ŒæŽ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜**

**å…³é”®è¯**: `å›¾åƒåˆ†å‰²` `Unetæž¶æž„` `æ¨¡åž‹åŽ‹ç¼©` `åŒ»å­¦å½±åƒ` `é€šé“å‰ªæž` `è½»é‡æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. UnetåŠå…¶å˜ä½“åœ¨å›¾åƒåˆ†å‰²ä¸­å†…å­˜éœ€æ±‚å¤§ï¼Œé™åˆ¶è®­ç»ƒæ‰¹æ¬¡å’Œå¢žåŠ æŽ¨ç†å»¶è¿Ÿ
2. é€šè¿‡åˆ†æžå‰ªæžï¼Œå‡è®¾æœ€ç»ˆç»“æž„æ˜¯å…³é”®ï¼Œæå‡ºé€šé“æ•°ä¸éšåˆ†è¾¨çŽ‡å‡åŠè€Œç¿»å€çš„ç´§å‡‘æž¶æž„
3. åœ¨MRIå’ŒCTæ•°æ®é›†ä¸Šï¼ŒLUnetå‚æ•°å‡å°‘è¶…30å€ï¼Œæ€§èƒ½ä¸Žä¼ ç»ŸUnetå’Œå‰ªæžç½‘ç»œç›¸å½“

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Unet and its variations have been standard in semantic image segmentation, especially for computer assisted radiology. Current Unet architectures iteratively downsample spatial resolution while increasing channel dimensions to preserve information content. Such a structure demands a large memory footprint, limiting training batch sizes and increasing inference latency. Channel pruning compresses Unet architecture without accuracy loss, but requires lengthy optimization and may not generalize across tasks and datasets. By investigating Unet pruning, we hypothesize that the final structure is the crucial factor, not the channel selection strategy of pruning. Based on our observations, we propose a lean Unet architecture (LUnet) with a compact, flat hierarchy where channels are not doubled as resolution is halved. We evaluate on a public MRI dataset allowing comparable reporting, as well as on two internal CT datasets. We show that a state-of-the-art pruning solution (STAMP) mainly prunes from the layers with the highest number of channels. Comparatively, simply eliminating a random channel at the pruning-identified layer or at the largest layer achieves similar or better performance. Our proposed LUnet with fixed architectures and over 30 times fewer parameters achieves performance comparable to both conventional Unet counterparts and data-adaptively pruned networks. The proposed lean Unet with constant channel count across layers requires far fewer parameters while achieving performance superior to standard Unet for the same total number of parameters. Skip connections allow Unet bottleneck channels to be largely reduced, unlike standard encoder-decoder architectures requiring increased bottleneck channels for information propagation.

