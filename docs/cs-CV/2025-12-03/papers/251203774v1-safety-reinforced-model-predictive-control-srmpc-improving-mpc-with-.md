---
layout: default
title: Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving
---

# Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving

**arXiv**: [2512.03774v1](https://arxiv.org/abs/2512.03774) | [PDF](https://arxiv.org/pdf/2512.03774.pdf)

**ä½œè€…**: Johannes Fischer, Marlon Steiner, Ã–mer Sahin Tas, Christoph Stiller

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®‰å…¨å¼ºåŒ–æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶ï¼ˆSRMPCï¼‰ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ æ”¹è¿›è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’ä¸­çš„MPCæ€§èƒ½ã€‚**

**å…³é”®è¯**: `æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶` `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’` `çº¦æŸå¼ºåŒ–å­¦ä¹ ` `å…¨å±€ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šMPCåœ¨å®žæ—¶è§„åˆ’ä¸­ä¾èµ–å‡¸è¿‘ä¼¼ï¼Œå¯èƒ½é™åˆ¶è§£ç©ºé—´ï¼Œæ— æ³•æ‰¾åˆ°å…¨å±€æœ€ä¼˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆSRLï¼‰åœ¨MPCä¸­ç”Ÿæˆå®‰å…¨å‚è€ƒè½¨è¿¹ï¼Œç»“åˆçº¦æŸå¼ºåŒ–å­¦ä¹ ï¼ˆCRLï¼‰ç¡®ä¿å®‰å…¨ï¼Œå­¦ä¹ çŠ¶æ€ä¾èµ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨é«˜é€Ÿå…¬è·¯åœºæ™¯ä¸­å®žéªŒï¼ŒSRMPCåœ¨å®‰å…¨æ€§å’Œæ€§èƒ½ä¸Šä¼˜äºŽMPCå’ŒSRLã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.

