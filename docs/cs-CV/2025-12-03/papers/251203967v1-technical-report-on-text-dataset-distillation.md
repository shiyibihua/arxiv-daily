---
layout: default
title: Technical Report on Text Dataset Distillation
---

# Technical Report on Text Dataset Distillation

**arXiv**: [2512.03967v1](https://arxiv.org/abs/2512.03967) | [PDF](https://arxiv.org/pdf/2512.03967.pdf)

**ä½œè€…**: Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Victor Zacarias, Edson Bollis, Lucas Pellicer, Rosimeire Pereira Costa, Anna Helena Reali Costa, Artur Jordao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ–‡æœ¬æ•°æ®é›†è’¸é¦æŠ€æœ¯è¿›å±•ï¼Œæ¶µç›–æ–¹æ³•ã€æŒ‘æˆ˜ä¸Žæœªæ¥æ–¹å‘**

**å…³é”®è¯**: `æ–‡æœ¬æ•°æ®é›†è’¸é¦` `Transformeræ¨¡åž‹` `åˆæˆæ–‡æœ¬ç”Ÿæˆ` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `è’¸é¦ç­–ç•¥` `åŸºå‡†æ ‡å‡†åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ–‡æœ¬æ•°æ®é›†è’¸é¦ç ”ç©¶è¾ƒå°‘ï¼Œé¢ä¸´ç¦»æ•£æ€§ã€å¤æ‚ä»»åŠ¡å’Œæ ‡å‡†åŒ–ç­‰æŒ‘æˆ˜
2. æ–¹æ³•è¦ç‚¹ï¼šåŒ…æ‹¬åŸºäºŽTransformerçš„æ–¹æ³•ã€ç”Ÿæˆç¦»æ•£åˆæˆæ–‡æœ¬å’Œæ‰©å±•åˆ°å¤§åž‹è§£ç å™¨æ¨¡åž‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šæœªçŸ¥å…·ä½“å®žéªŒç»†èŠ‚ï¼Œä½†æŠ¥å‘Šå›žé¡¾äº†å…³é”®è´¡çŒ®å’Œé¢†åŸŸå‘å±•é‡Œç¨‹ç¢‘

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

