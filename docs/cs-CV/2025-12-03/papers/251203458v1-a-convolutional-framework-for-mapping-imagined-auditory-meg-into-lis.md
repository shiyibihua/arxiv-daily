---
layout: default
title: A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses
---

# A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses

**arXiv**: [2512.03458v1](https://arxiv.org/abs/2512.03458) | [PDF](https://arxiv.org/pdf/2512.03458.pdf)

**ä½œè€…**: Maryam Maghsoudi, Mohsen Rezaeizadeh, Shihab Shamma

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå·ç§¯ç¥žç»ç½‘ç»œçš„æ¡†æž¶ï¼Œå°†æƒ³è±¡å¬è§‰MEGæ˜ å°„ä¸ºæ„ŸçŸ¥å“åº”ï¼Œä»¥æ”¯æŒè„‘æœºæŽ¥å£åº”ç”¨ã€‚**

**å…³é”®è¯**: `è„‘ç£å›¾è§£ç ` `æƒ³è±¡è¯­éŸ³æ˜ å°„` `å·ç§¯ç¥žç»ç½‘ç»œ` `è„‘æœºæŽ¥å£` `å¬è§‰å“åº”é¢„æµ‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§£ç æƒ³è±¡è¯­éŸ³çš„ç¥žç»æ´»åŠ¨å­˜åœ¨æ—¶åºä¸ç¡®å®šæ€§å’Œæ•°æ®é›†æœ‰é™æ€§ï¼Œéš¾ä»¥è§£é‡Šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨æ»‘åŠ¨çª—å£å²­å›žå½’å’Œå¸¦æ ¡å‡†å±‚çš„å·ç§¯ç¥žç»ç½‘ç»œï¼Œåœ¨ä¸ªä½“å’Œç¾¤ä½“æ°´å¹³æ˜ å°„æƒ³è±¡åˆ°æ„ŸçŸ¥å“åº”ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šCNNåœ¨å¤šæ•°å—è¯•è€…ä¸Šæ˜¾è‘—ä¼˜äºŽåŸºçº¿ï¼Œé¢„æµ‹ä¸ŽçœŸå®žæ„ŸçŸ¥å“åº”ç›¸å…³æ€§æ›´é«˜ï¼Œè¯æ˜Žæ˜ å°„å¯è¡Œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.

