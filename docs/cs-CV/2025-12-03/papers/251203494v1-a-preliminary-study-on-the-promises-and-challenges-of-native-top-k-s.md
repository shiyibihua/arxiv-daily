---
layout: default
title: A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention
---

# A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention

**arXiv**: [2512.03494v1](https://arxiv.org/abs/2512.03494) | [PDF](https://arxiv.org/pdf/2512.03494.pdf)

**ä½œè€…**: Di Xiu, Hongyin Tang, Bolin Rong, Lizhi Yan, Jingang Wang, Yifan Lu, Xunliang Cai

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶åŽŸç”ŸTop-kç¨€ç–æ³¨æ„åŠ›åœ¨è§£ç ä¸Žè®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§åŠç†è®ºæœºåˆ¶ï¼Œä»¥é™ä½Žå¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†æˆæœ¬ã€‚**

**å…³é”®è¯**: `Top-kæ³¨æ„åŠ›` `ç¨€ç–æ³¨æ„åŠ›` `å¤§è¯­è¨€æ¨¡åž‹` `æŽ¨ç†ä¼˜åŒ–` `ç†µç†è®º` `é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡åž‹åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­æŽ¨ç†è®¡ç®—æˆæœ¬é«˜ï¼Œé˜»ç¢ä»£ç†å’Œå¤šæ¨¡æ€åº”ç”¨å‘å±•ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šéªŒè¯ç²¾ç¡®Top-kè§£ç çš„æœ‰æ•ˆæ€§ï¼ŒæŽ¢ç´¢åŽŸç”ŸTop-kæ³¨æ„åŠ›è®­ç»ƒç­–ç•¥ï¼Œå¹¶åˆ†æžè¿‘ä¼¼ç®—æ³•ç²¾åº¦å½±å“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šTop-kè§£ç åœ¨HELMETå’ŒLongBench v2ä»»åŠ¡ä¸Šæ€§èƒ½åª²ç¾Žæˆ–è¶…è¶Šå…¨æ³¨æ„åŠ›ï¼Œè®­ç»ƒä¸€è‡´æ€§æå‡æ¨¡åž‹è¡¨çŽ°ï¼Œä¸‹æ¸¸ä»»åŠ¡ç†µé™ä½ŽéªŒè¯ä½Žç†µçŠ¶æ€é€‚åº”å‡è®¾ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.

