---
layout: default
title: DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation
---

# DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation

**arXiv**: [2512.03992v1](https://arxiv.org/abs/2512.03992) | [PDF](https://arxiv.org/pdf/2512.03992.pdf)

**ä½œè€…**: Zexin Lin, Hawen Wan, Yebin Zhong, Xiaoqiang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDIQ-HåŸºå‡†ä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨æ—¶åºè§†è§‰é€€åŒ–ä¸‹çš„å¹»è§‰æŒç»­æ€§**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ—¶åºè§†è§‰é€€åŒ–` `å¹»è§‰æŒç»­æ€§` `åŸºå‡†è¯„ä¼°` `é²æ£’æ€§æµ‹è¯•` `å¤šè½®é—®ç­”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºå‡†å¿½ç•¥æ—¶åºé€€åŒ–ä¸Žé”™è¯¯ä¼ æ’­ï¼Œå¯¼è‡´VLMåœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å­˜åœ¨å…³é”®å¤±æ•ˆæ¨¡å¼
2. DIQ-Håº”ç”¨åŸºäºŽç‰©ç†çš„è§†è§‰é€€åŒ–ï¼Œé€šè¿‡å¤šè½®é—®ç­”ä»»åŠ¡è¯„ä¼°å¹»è§‰æŒç»­æ€§ã€é”™è¯¯æ¢å¤å’Œæ—¶åºä¸€è‡´æ€§
3. å®žéªŒæ˜¾ç¤º16ä¸ªå…ˆè¿›VLMå­˜åœ¨æ˜¾è‘—é²æ£’æ€§å·®è·ï¼ŒGPT-4oæ¢å¤çŽ‡ä»…78.5%ï¼Œå¼€æºæ¨¡åž‹æ—¶åºä¸€è‡´æ€§ä½ŽäºŽ60%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.

