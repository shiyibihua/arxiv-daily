---
layout: default
title: SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows
---

# SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows

**arXiv**: [2512.04084v1](https://arxiv.org/abs/2512.04084) | [PDF](https://arxiv.org/pdf/2512.04084.pdf)

**ä½œè€…**: Qinyu Zhao, Guangting Zheng, Tao Yang, Rui Zhu, Xingjian Leng, Stephen Gould, Liang Zheng

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSimFlowï¼Œé€šè¿‡å›ºå®šæ–¹å·®ç®€åŒ–å¹¶ç«¯åˆ°ç«¯è®­ç»ƒæ½œåœ¨å½’ä¸€åŒ–æµï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡ã€‚**

**å…³é”®è¯**: `å½’ä¸€åŒ–æµ` `å˜åˆ†è‡ªç¼–ç å™¨` `å›¾åƒç”Ÿæˆ` `ç«¯åˆ°ç«¯è®­ç»ƒ` `å›ºå®šæ–¹å·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•ä¾èµ–å™ªå£°å¢žå¼ºæˆ–å†»ç»“VAEç¼–ç å™¨ï¼Œå¯¼è‡´æµç¨‹å¤æ‚ä¸”ç”Ÿæˆè´¨é‡å—é™ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå›ºå®šVAEç¼–ç å™¨è¾“å‡ºçš„æ–¹å·®ä¸ºå¸¸æ•°ï¼Œç®€åŒ–è®­ç»ƒå¹¶å®žçŽ°ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ImageNet 256Ã—256ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒSimFlowå–å¾—gFID 2.15ï¼Œä¼˜äºŽSTARFlowï¼Œç»“åˆREPA-EåŽè¾¾1.91ï¼Œåˆ›NFsæ–°çºªå½•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.

