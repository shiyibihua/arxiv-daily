---
layout: default
title: RELIC: Interactive Video World Model with Long-Horizon Memory
---

# RELIC: Interactive Video World Model with Long-Horizon Memory

**arXiv**: [2512.04040v1](https://arxiv.org/abs/2512.04040) | [PDF](https://arxiv.org/pdf/2512.04040.pdf)

**ä½œè€…**: Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRELICäº¤äº’å¼è§†é¢‘ä¸–ç•Œæ¨¡åž‹ï¼Œé€šè¿‡åŽ‹ç¼©åŽ†å²è®°å¿†å’Œå› æžœè’¸é¦å®žçŽ°å®žæ—¶é•¿æ—¶æŽ¢ç´¢**

**å…³é”®è¯**: `äº¤äº’å¼ä¸–ç•Œæ¨¡åž‹` `é•¿æ—¶è®°å¿†` `è§†é¢‘æ‰©æ•£è’¸é¦` `å®žæ—¶ç”Ÿæˆ` `ç©ºé—´ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶å®žçŽ°å®žæ—¶é•¿æ—¶æµã€ä¸€è‡´ç©ºé—´è®°å¿†å’Œç²¾ç¡®ç”¨æˆ·æŽ§åˆ¶
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨åŽ‹ç¼©åŽ†å²æ½œåœ¨ä»¤ç‰Œå’Œç›¸æœºæ„ŸçŸ¥è®°å¿†ç»“æž„ï¼Œç»“åˆå› æžœè’¸é¦è®­ç»ƒ
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨16 FPSä¸‹å®žæ—¶ç”Ÿæˆï¼Œå±•ç¤ºæ›´å‡†ç¡®åŠ¨ä½œè·Ÿéšå’Œç¨³å®šé•¿æ—¶æµ

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.

