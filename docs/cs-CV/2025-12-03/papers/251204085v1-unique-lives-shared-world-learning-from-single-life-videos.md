---
layout: default
title: Unique Lives, Shared World: Learning from Single-Life Videos
---

# Unique Lives, Shared World: Learning from Single-Life Videos

**arXiv**: [2512.04085v1](https://arxiv.org/abs/2512.04085) | [PDF](https://arxiv.org/pdf/2512.04085.pdf)

**ä½œè€…**: Tengda Han, Sayna Ebrahimi, Dilara Gokay, Li Yang Ku, Maks Ovsjanikov, Iva Babukova, Daniel Zoran, Viorica Patraucean, Joao Carreira, Andrew Zisserman, Dima Damen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•ä¸€ç”Ÿå­¦ä¹ èŒƒå¼ï¼Œåˆ©ç”¨ä¸ªäººç¬¬ä¸€è§†è§’è§†é¢‘è¿›è¡Œè‡ªç›‘ç£è§†è§‰ç¼–ç å™¨è®­ç»ƒã€‚**

**å…³é”®è¯**: `å•ä¸€ç”Ÿå­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `ç¬¬ä¸€è§†è§’è§†é¢‘` `å‡ ä½•è¡¨ç¤ºå­¦ä¹ ` `è·¨æ³¨æ„åŠ›åº¦é‡` `æ·±åº¦ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæŽ¢ç´¢ä»Žå•ä¸€ç”Ÿå‘½è§†é¢‘ä¸­å­¦ä¹ è§†è§‰è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ï¼Œä»¥åº”å¯¹æ•°æ®å¤šæ ·æ€§å’Œæ³›åŒ–æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¤šè§†è§’è‡ªç›‘ç£å­¦ä¹ ï¼Œè®­ç»ƒç‹¬ç«‹æ¨¡åž‹ï¼Œå¹¶å¼•å…¥è·¨æ³¨æ„åŠ›åº¦é‡è¯„ä¼°è¡¨ç¤ºå¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ¨¡åž‹åœ¨å‡ ä½•ç†è§£ä¸Šå¯¹é½ï¼Œèƒ½æ³›åŒ–åˆ°æ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ï¼Œå•å‘¨æ•°æ®æ€§èƒ½åª²ç¾Žå¤šæ ·ç½‘ç»œæ•°æ®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.

