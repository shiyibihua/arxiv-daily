---
layout: default
title: BERnaT: Basque Encoders for Representing Natural Textual Diversity
---

# BERnaT: Basque Encoders for Representing Natural Textual Diversity

**arXiv**: [2512.03903v1](https://arxiv.org/abs/2512.03903) | [PDF](https://arxiv.org/pdf/2512.03903.pdf)

**ä½œè€…**: Ekhi Azurmendi, Joseba Fernandez de Landa, Jaione Bengoetxea, Maite Heredia, Julen Etxaniz, Mikel Zubillaga, Ander Soraluze, Aitor Soroa

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBERnaTæ¨¡åž‹ä»¥è§£å†³å·´æ–¯å…‹è¯­è¯­è¨€å¤šæ ·æ€§å»ºæ¨¡é—®é¢˜ï¼Œé€šè¿‡ç»“åˆæ ‡å‡†ä¸Žéžæ ‡å‡†æ–‡æœ¬æå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹` `è¯­è¨€å¤šæ ·æ€§` `å·´æ–¯å…‹è¯­` `ç¼–ç å™¨æ¨¡åž‹` `è‡ªç„¶è¯­è¨€ç†è§£` `è¯­æ–™åº“æž„å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè¯­è¨€æ¨¡åž‹ä¾èµ–è¿‡æ»¤åŽçš„æ ‡å‡†æ–‡æœ¬ï¼Œå¯èƒ½æŽ’é™¤éžæ ‡å‡†è¯­è¨€å˜ä½“ï¼Œå¯¼è‡´åè§å’Œé²æ£’æ€§é™ä½Žã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºåŒ…å«æ ‡å‡†ã€ç¤¾äº¤åª’ä½“å’ŒåŽ†å²æ¥æºçš„å·´æ–¯å…‹è¯­è¯­æ–™åº“ï¼Œé¢„è®­ç»ƒä¸‰ç§é…ç½®çš„ç¼–ç å™¨æ¨¡åž‹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†ä¸Žå¤šæ ·åŒ–å­é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºç»“åˆå¤šæ ·åŒ–æ•°æ®çš„æ¨¡åž‹åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åž‹ä¸­è¡¨çŽ°æ›´ä¼˜ï¼Œä¸æŸå®³æ ‡å‡†åŸºå‡†å‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.

