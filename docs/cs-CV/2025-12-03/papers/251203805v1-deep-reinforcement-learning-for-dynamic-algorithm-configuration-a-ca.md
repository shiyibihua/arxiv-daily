---
layout: default
title: Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($Î»$,$Î»$))-GA
---

# Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($Î»$,$Î»$))-GA

**arXiv**: [2512.03805v1](https://arxiv.org/abs/2512.03805) | [PDF](https://arxiv.org/pdf/2512.03805.pdf)

**ä½œè€…**: Tai Nguyen, Phong Le, AndrÃ© Biedenkapp, Carola Doerr, Nguyen Dang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”å¥–åŠ±åç§»æœºåˆ¶ä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨åŠ¨æ€ç®—æ³•é…ç½®ä¸­çš„æŽ¢ç´¢ä¸è¶³é—®é¢˜**

**å…³é”®è¯**: `åŠ¨æ€ç®—æ³•é…ç½®` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”å¥–åŠ±åç§»` `DDQN` `PPO` `OneMaxä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨åŠ¨æ€ç®—æ³•é…ç½®ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯æ‰©å±•æ€§ä¸‹é™å’Œå­¦ä¹ ä¸ç¨³å®šæ€§
2. å¼•å…¥è‡ªé€‚åº”å¥–åŠ±åç§»æœºåˆ¶ï¼ŒåŸºäºŽå¥–åŠ±åˆ†å¸ƒç»Ÿè®¡å¢žå¼ºDDQNæŽ¢ç´¢ï¼Œæ— éœ€å®žä¾‹ç‰¹å®šè¶…å‚æ•°è°ƒä¼˜
3. åœ¨OneMaxé—®é¢˜ä¸Šï¼ŒDDQNç»“åˆè¯¥æœºåˆ¶å®žçŽ°ä¸Žç†è®ºç­–ç•¥ç›¸å½“çš„æ€§èƒ½ï¼Œæ ·æœ¬æ•ˆçŽ‡å¤§å¹…æå‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($Î»$,$Î»$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

