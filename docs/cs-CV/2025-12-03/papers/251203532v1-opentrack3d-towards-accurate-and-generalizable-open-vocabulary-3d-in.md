---
layout: default
title: OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation
---

# OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation

**arXiv**: [2512.03532v1](https://arxiv.org/abs/2512.03532) | [PDF](https://arxiv.org/pdf/2512.03532.pdf)

**ä½œè€…**: Zhishan Zhou, Siyuan Wei, Zengran Wang, Chunjie Wang, Xiaosheng Yan, Xiao Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOpenTrack3Dæ¡†æž¶ï¼Œé€šè¿‡è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œå®žçŽ°æ— ç½‘æ ¼åœºæ™¯ä¸‹çš„å¼€æ”¾è¯æ±‡3Då®žä¾‹åˆ†å‰²ã€‚**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡3Då®žä¾‹åˆ†å‰²` `è§†è§‰-ç©ºé—´è·Ÿè¸ª` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ— ç½‘æ ¼å¤„ç†` `è·¨è§†å›¾ä¸€è‡´æ€§` `ç»„åˆæŸ¥è¯¢æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–æ•°æ®é›†ç‰¹å®šææ¡ˆæˆ–ç½‘æ ¼è¶…ç‚¹ï¼Œéš¾ä»¥æ³›åŒ–è‡³æ— ç½‘æ ¼æ–°åœºæ™¯ï¼›CLIPåˆ†ç±»å™¨å¯¹ç»„åˆæŸ¥è¯¢æŽ¨ç†å¼±ã€‚
2. é‡‡ç”¨è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨åœ¨çº¿æž„å»ºè·¨è§†å›¾ä¸€è‡´ææ¡ˆï¼Œå¹¶å¯é€‰è¶…ç‚¹ç»†åŒ–ï¼›ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹æ›¿æ¢CLIPå¢žå¼ºç»„åˆæŽ¨ç†ã€‚
3. åœ¨ScanNet200ç­‰åŸºå‡†ä¸Šå®žçŽ°å…ˆè¿›æ€§èƒ½ï¼Œå±•ç¤ºå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæ ¸å¿ƒæµç¨‹å®Œå…¨æ— ç½‘æ ¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.

