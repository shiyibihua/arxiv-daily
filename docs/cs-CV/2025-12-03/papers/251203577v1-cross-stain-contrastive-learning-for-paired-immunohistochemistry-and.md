---
layout: default
title: Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning
---

# Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning

**arXiv**: [2512.03577v1](https://arxiv.org/abs/2512.03577) | [PDF](https://arxiv.org/pdf/2512.03577.pdf)

**ä½œè€…**: Yizhi Zhang, Lei Fan, Zhulin Tao, Donglin Di, Yang Song, Sidong Liu, Cong Cong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æŸ“è‰²å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œåˆ©ç”¨å¯¹é½å¤šæŸ“è‰²æ•°æ®é›†æå‡å…¨åˆ‡ç‰‡å›¾åƒè¡¨ç¤ºè´¨é‡ã€‚**

**å…³é”®è¯**: `è®¡ç®—ç—…ç†å­¦` `å…¨åˆ‡ç‰‡å›¾åƒè¡¨ç¤º` `å¯¹æ¯”å­¦ä¹ ` `å¤šæŸ“è‰²èžåˆ` `å¤šå®žä¾‹å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæŸ“è‰²æ•°æ®å¯¹é½ä¸è¶³å¯¼è‡´ç‰¹å¾ä¸ä¸€è‡´ï¼Œé™åˆ¶ç—…ç†å›¾åƒè¡¨ç¤ºå­¦ä¹ ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼ŒåŒ…æ‹¬è¡¥ä¸çº§å¯¹æ¯”å¯¹é½å’Œåˆ‡ç‰‡çº§å¤šå®žä¾‹å­¦ä¹ èžåˆæ¨¡å—ã€‚
3. å®žéªŒæ•ˆæžœï¼šåœ¨ç™Œç—‡äºšåž‹åˆ†ç±»ã€ç”Ÿç‰©æ ‡å¿—ç‰©çŠ¶æ€åˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸­è¡¨çŽ°æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.

