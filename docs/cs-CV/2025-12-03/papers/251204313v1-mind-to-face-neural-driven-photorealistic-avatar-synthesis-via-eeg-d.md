---
layout: default
title: Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding
---

# Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.04313" target="_blank" class="toolbar-btn">arXiv: 2512.04313v1</a>
    <a href="https://arxiv.org/pdf/2512.04313.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04313v1" 
            onclick="toggleFavorite(this, '2512.04313v1', 'Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Haolin Xiong, Tianwen Fu, Pratusha Bhuvana Prasad, Yunxuan Cai, Haiwei Chen, Wenbin Teng, Hanyuan Xiao, Yajie Zhao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

**Â§áÊ≥®**: 16 pages, 11 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Mind-to-FaceÔºöÈ¶ñ‰∏™Âü∫‰∫éËÑëÁîµ‰ø°Âè∑Ëß£Á†ÅÁöÑÈÄºÁúü‰∫∫ËÑ∏AvatarÁîüÊàêÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `ËÑëÊú∫Êé•Âè£` `‰∫∫ËÑ∏Avatar` `ËÑëÁîµ‰ø°Âè∑Ëß£Á†Å` `3DÈ´òÊñØÊ∫ÖÂ∞Ñ` `ÊÉÖÊÑüËØÜÂà´`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâAvatarÁ≥ªÁªü‰∏•Èáç‰æùËµñËßÜËßâÁ∫øÁ¥¢ÔºåÂú®Èù¢ÈÉ®Ë¢´ÈÅÆÊå°ÊàñÊÉÖÁª™ÂÜÖÊïõÊó∂Â§±ÊïàÔºåÊó†Ê≥ïÂáÜÁ°ÆÊçïÊçâÂÜÖÂú®ÊÉÖÊÑü„ÄÇ
2. Mind-to-FaceÈÄöËøáCNN-TransformerÂ∞ÜËÑëÁîµ‰ø°Âè∑Ëß£Á†Å‰∏∫È´òÁ≤æÂ∫¶3DÈù¢ÈÉ®Ê®°ÂûãÔºåÂπ∂‰ΩøÁî®3DÈ´òÊñØÊ∫ÖÂ∞ÑÊ∏≤ÊüìÈÄºÁúüAvatar„ÄÇ
3. ÂÆûÈ™åËØÅÊòéÔºå‰ªÖ‰ΩøÁî®ËÑëÁîµ‰ø°Âè∑Âç≥ÂèØÈ¢ÑÊµã‰∏™‰ΩìÂåñÁöÑÂä®ÊÄÅÈù¢ÈÉ®Ë°®ÊÉÖÔºåÂåÖÊã¨ÁªÜÂæÆÁöÑÊÉÖÁª™ÂèçÂ∫îÔºåÊïàÊûúÊòæËëó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫Mind-to-FaceÔºåÊòØÈ¶ñ‰∏™Â∞ÜÈùû‰æµÂÖ•ÂºèËÑëÁîµÂõæ(EEG)‰ø°Âè∑Áõ¥Êé•Ëß£Á†Å‰∏∫È´ò‰øùÁúüÈù¢ÈÉ®Ë°®ÊÉÖÁöÑÊ°ÜÊû∂„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèåÊ®°ÊÄÅËÆ∞ÂΩïÁ≥ªÁªüÔºåÂú®ËØ±ÂèëÊÉÖÁª™ÁöÑÂà∫ÊøÄ‰∏ãÔºåÂêåÊ≠•Ëé∑ÂèñEEGÂíåÂ§öËßÜËßíÈù¢ÈÉ®ËßÜÈ¢ëÔºå‰ªéËÄå‰∏∫Á•ûÁªè-ËßÜËßâÂ≠¶‰π†Êèê‰æõÁ≤æÁ°ÆÁöÑÁõëÁù£„ÄÇÊàë‰ª¨ÁöÑÊ®°Âûã‰ΩøÁî®CNN-TransformerÁºñÁ†ÅÂô®Â∞ÜEEG‰ø°Âè∑Êò†Â∞ÑÂà∞ÂØÜÈõÜÁöÑ3D‰ΩçÁΩÆÂõæÔºåËÉΩÂ§üÈááÊ†∑Ë∂ÖËøá65k‰∏™È°∂ÁÇπÔºåÊçïÊçâÁ≤æÁªÜÁöÑÂá†‰ΩïÁªìÊûÑÂíåÂæÆÂ¶ôÁöÑÊÉÖÁª™Âä®ÊÄÅÔºåÂπ∂ÈÄöËøáÊîπËøõÁöÑ3DÈ´òÊñØÊ∫ÖÂ∞ÑÊ∏≤ÊüìÁÆ°Á∫øÁîüÊàêÈÄºÁúü‰∏îËßÜËßí‰∏ÄËá¥ÁöÑÁªìÊûú„ÄÇÈÄöËøáÂπøÊ≥õÁöÑËØÑ‰º∞ÔºåÊàë‰ª¨ËØÅÊòé‰ªÖÂá≠EEGÂ∞±ËÉΩÂèØÈù†Âú∞È¢ÑÊµãÂä®ÊÄÅÁöÑ„ÄÅ‰∏™‰ΩìÂåñÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÔºåÂåÖÊã¨ÂæÆÂ¶ôÁöÑÊÉÖÁª™ÂèçÂ∫îÔºåË°®ÊòéÁ•ûÁªè‰ø°Âè∑ÂåÖÂê´ÊØî‰πãÂâçËÆ§‰∏∫ÁöÑÊõ¥‰∏∞ÂØåÁöÑÊÉÖÊÑüÂíåÂá†‰Ωï‰ø°ÊÅØ„ÄÇMind-to-Face‰∏∫Á•ûÁªèÈ©±Âä®ÁöÑAvatarÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËåÉ‰æãÔºåËÉΩÂ§üÂú®Ê≤âÊµ∏ÂºèÁéØÂ¢É‰∏≠ÂÆûÁé∞‰∏™ÊÄßÂåñÁöÑ„ÄÅÊÉÖÊÑüÊÑüÁü•ÁöÑËøúÁ®ãÂëàÁé∞ÂíåËÆ§Áü•‰∫§‰∫í„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑ‰∫∫ËÑ∏AvatarÁîüÊàêÁ≥ªÁªü‰∏ªË¶Å‰æùËµñ‰∫éËßÜËßâ‰ø°ÊÅØÔºå‰æãÂ¶ÇÈù¢ÈÉ®ÂõæÂÉèÊàñËßÜÈ¢ë„ÄÇÂΩìÈù¢ÈÉ®Ë¢´ÈÅÆÊå°ÔºåÊàñËÄÖ‰∫∫‰ª¨ËØïÂõæÈöêËóèËá™Â∑±ÁöÑÊÉÖÁª™Êó∂ÔºåËøô‰∫õÁ≥ªÁªüÂ∞±Êó†Ê≥ïÂáÜÁ°ÆÂú∞ÊçïÊçâÂà∞ÁúüÂÆûÁöÑÊÉÖÊÑüË°®Ëææ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰Ωï‰ªÖÈÄöËøáÈùû‰æµÂÖ•ÂºèÁ•ûÁªè‰ø°Âè∑ÔºàÂ¶ÇËÑëÁîµÂõæEEGÔºâÊù•È©±Âä®ÈÄºÁúüÁöÑ‰∫∫ËÑ∏AvatarÔºåÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËÑëÁîµ‰ø°Âè∑Áõ¥Êé•Êò†Â∞ÑÂà∞È´òÁ≤æÂ∫¶ÁöÑ3DÈù¢ÈÉ®Ê®°ÂûãÔºåÂπ∂Âà©Áî®3DÈ´òÊñØÊ∫ÖÂ∞ÑÊäÄÊúØËøõË°åÊ∏≤ÊüìÔºå‰ªéËÄåÁîüÊàêÈÄºÁúüÁöÑ‰∫∫ËÑ∏Avatar„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπËßÜËßâ‰ø°ÊÅØÁöÑ‰æùËµñÔºåÂèØ‰ª•Áõ¥Êé•ÂèçÊò†‰∏™‰ΩìÁöÑÊÉÖÁª™Áä∂ÊÄÅÂíåÂÜÖÂú®ÊÉ≥Ê≥ï„ÄÇÈÄöËøáÂª∫Á´ãEEG‰ø°Âè∑‰∏éÈù¢ÈÉ®Ë°®ÊÉÖ‰πãÈó¥ÁöÑÁõ¥Êé•ËÅîÁ≥ªÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Âä†Ëá™ÁÑ∂Âíå‰∏™ÊÄßÂåñÁöÑAvatarÊéßÂà∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMind-to-FaceÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) **ÂèåÊ®°ÊÄÅÊï∞ÊçÆÈááÈõÜ**ÔºöÂêåÊ≠•ËÆ∞ÂΩïEEG‰ø°Âè∑ÂíåÂ§öËßÜËßíÈù¢ÈÉ®ËßÜÈ¢ëÔºåÁî®‰∫éËÆ≠ÁªÉÊ®°Âûã„ÄÇ2) **CNN-TransformerÁºñÁ†ÅÂô®**ÔºöÂ∞ÜEEG‰ø°Âè∑ÁºñÁ†Å‰∏∫È´òÁª¥ÁâπÂæÅÂêëÈáè„ÄÇ3) **3D‰ΩçÁΩÆÂõæÁîüÊàêÂô®**ÔºöÂ∞ÜÁâπÂæÅÂêëÈáèÊò†Â∞ÑÂà∞ÂØÜÈõÜÁöÑ3D‰ΩçÁΩÆÂõæÔºåËØ•‰ΩçÁΩÆÂõæÂåÖÂê´Ë∂ÖËøá65k‰∏™È°∂ÁÇπÔºåËÉΩÂ§üÊçïÊçâÁ≤æÁªÜÁöÑÈù¢ÈÉ®Âá†‰ΩïÁªìÊûÑ„ÄÇ4) **3DÈ´òÊñØÊ∫ÖÂ∞ÑÊ∏≤ÊüìÁÆ°Á∫ø**ÔºöÂ∞Ü3D‰ΩçÁΩÆÂõæÊ∏≤ÊüìÊàêÈÄºÁúü‰∏îËßÜËßí‰∏ÄËá¥ÁöÑ‰∫∫ËÑ∏ÂõæÂÉè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) **È¶ñÊ¨°ÊèêÂá∫Âü∫‰∫éEEG‰ø°Âè∑Áõ¥Êé•ÁîüÊàêÈÄºÁúü‰∫∫ËÑ∏AvatarÁöÑÊ°ÜÊû∂**ÔºåÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÂØπËßÜËßâ‰ø°ÊÅØÁöÑ‰æùËµñ„ÄÇ2) **Âà©Áî®CNN-TransformerÁªìÊûÑÊúâÊïàÊèêÂèñEEG‰ø°Âè∑‰∏≠ÁöÑÊÉÖÊÑüÂíåÂá†‰Ωï‰ø°ÊÅØ**„ÄÇ3) **ÈááÁî®ÊîπËøõÁöÑ3DÈ´òÊñØÊ∫ÖÂ∞ÑÊ∏≤ÊüìÁÆ°Á∫øÔºåÂÆûÁé∞‰∫ÜÈ´òË¥®ÈáèÁöÑAvatarÊ∏≤ÊüìÊïàÊûú**„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåÈááÁî®‰∫ÜCNN-TransformerÊ∑∑ÂêàÁªìÊûÑÔºåCNNÁî®‰∫éÊèêÂèñÂ±ÄÈÉ®ÁâπÂæÅÔºåTransformerÁî®‰∫éÊçïÊçâÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ª„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢Ôºå‰ΩøÁî®‰∫ÜL1ÊçüÂ§±ÂíåÊÑüÁü•ÊçüÂ§±Ôºå‰ª•‰øùËØÅÁîüÊàêÁöÑ‰∫∫ËÑ∏ÂõæÂÉèÁöÑÈÄºÁúüÂ∫¶ÂíåÁªÜËäÇ„ÄÇÂú®3DÈ´òÊñØÊ∫ÖÂ∞ÑÊ∏≤ÊüìÁÆ°Á∫ø‰∏≠ÔºåÂØπÈ´òÊñØÂàÜÂ∏ÉÁöÑÂèÇÊï∞ËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÊèêÈ´òÊ∏≤ÊüìË¥®ÈáèÂíåÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMind-to-FaceËÉΩÂ§ü‰ªÖÂá≠EEG‰ø°Âè∑ÂèØÈù†Âú∞È¢ÑÊµãÂä®ÊÄÅÁöÑ„ÄÅ‰∏™‰ΩìÂåñÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÔºåÂåÖÊã¨ÂæÆÂ¶ôÁöÑÊÉÖÁª™ÂèçÂ∫î„ÄÇ‰∏éÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåMind-to-FaceÂú®Èù¢ÈÉ®Ë°®ÊÉÖËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÂíåAvatarÊ∏≤ÊüìÁöÑÈÄºÁúüÂ∫¶ÊñπÈù¢ÂùáÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®‰∏ªËßÇËØÑ‰º∞‰∏≠ÔºåÁî®Êà∑ÊôÆÈÅçËÆ§‰∏∫Mind-to-FaceÁîüÊàêÁöÑAvatarÊõ¥Âä†Ëá™ÁÑ∂ÂíåÂØåÊúâË°®Áé∞Âäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Mind-to-FaceÊäÄÊúØÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂú®ËøúÁ®ãÂëàÁé∞ÂíåËôöÊãü‰ºöËÆÆ‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®Êà∑ÁöÑAvatarËÉΩÂ§üÁúüÂÆûÂú∞ÂèçÊò†ÂÖ∂ÊÉÖÁª™Áä∂ÊÄÅÔºå‰ªéËÄåÂ¢ûÂº∫Ê≤üÈÄöÁöÑËá™ÁÑ∂ÊÄßÂíåÊúâÊïàÊÄß„ÄÇÂú®Ê∏∏ÊàèÂíåÂ®±‰πêÈ¢ÜÂüüÔºåÂèØ‰ª•ÂàõÂª∫Êõ¥Âä†‰∏™ÊÄßÂåñÂíåÊ≤âÊµ∏ÂºèÁöÑËßíËâ≤‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Â∫îÁî®‰∫éÁ•ûÁªèÂèçÈ¶àÊ≤ªÁñóÂíåËÆ§Áü•Â∫∑Â§çÔºåÂ∏ÆÂä©ÊÇ£ËÄÖÊõ¥Â•ΩÂú∞‰∫ÜËß£ÂíåÊéßÂà∂Ëá™Â∑±ÁöÑÊÉÖÁª™„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.

