---
layout: default
title: Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models
---

# Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models

**arXiv**: [2512.03463v1](https://arxiv.org/abs/2512.03463) | [PDF](https://arxiv.org/pdf/2512.03463.pdf)

**ä½œè€…**: Shojiro Yamabe, Futa Waseda, Daiki Shiono, Tsubasa Takahashi

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–‡æœ¬æ‰“å°å›¾åƒä»¥å¼¥åˆå›¾åƒ-æ–‡æœ¬æ¨¡æ€å·®è·ï¼Œæ”¯æŒä½Žæˆæœ¬æ–‡æœ¬ä¸­å¿ƒè®­ç»ƒå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹**

**å…³é”®è¯**: `æ–‡æœ¬ä¸­å¿ƒè®­ç»ƒ` `æ¨¡æ€å·®è·` `åˆæˆå›¾åƒç”Ÿæˆ` `å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹` `ä½Žæˆæœ¬æ•°æ®æ‰©å±•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä»…ç”¨æ–‡æœ¬è®­ç»ƒå¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹å› æ¨¡æ€å·®è·å¯¼è‡´æ€§èƒ½æœ‰é™ï¼Œå›¾åƒæ”¶é›†æˆæœ¬é«˜ä¸”å—é™
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å°†æ–‡æœ¬ç›´æŽ¥æ¸²æŸ“åˆ°ç™½è‰²ç”»å¸ƒç”Ÿæˆåˆæˆå›¾åƒï¼Œä½Žæˆæœ¬æŠ•å½±æ–‡æœ¬åˆ°å›¾åƒæ¨¡æ€ï¼Œä¿ç•™è¯­ä¹‰
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å››ä¸ªæ¨¡åž‹å’Œä¸ƒä¸ªåŸºå‡†ä¸Šï¼Œæ–‡æœ¬æ‰“å°å›¾åƒæ¯”æ‰©æ•£æ¨¡åž‹ç”Ÿæˆçš„å›¾åƒæ›´æœ‰æ•ˆï¼Œæ”¯æŒæ•°æ®å¢žå¼º

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.

