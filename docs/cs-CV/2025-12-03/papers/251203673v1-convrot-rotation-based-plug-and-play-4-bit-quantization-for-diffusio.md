---
layout: default
title: ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers
---

# ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers

**arXiv**: [2512.03673v1](https://arxiv.org/abs/2512.03673) | [PDF](https://arxiv.org/pdf/2512.03673.pdf)

**ä½œè€…**: Feice Huang, Zuliang Han, Xing Zhou, Yihuang Chen, Lifei Zhu, Haoqian Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConvRotï¼Œä¸€ç§åŸºäºŽæ—‹è½¬çš„4ä½é‡åŒ–æ–¹æ³•ï¼Œç”¨äºŽæ‰©æ•£å˜æ¢å™¨çš„å³æ’å³ç”¨éƒ¨ç½²ã€‚**

**å…³é”®è¯**: `æ‰©æ•£å˜æ¢å™¨` `4ä½é‡åŒ–` `æ—‹è½¬æŠ€æœ¯` `å³æ’å³ç”¨æ¨¡å—` `å†…å­˜ä¼˜åŒ–` `æŽ¨ç†åŠ é€Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£å˜æ¢å™¨æ¨¡åž‹å¢žå¤§å¯¼è‡´å†…å­˜å’Œå»¶è¿Ÿé—®é¢˜ï¼ŒçŽ°æœ‰æ—‹è½¬æ–¹æ³•å¤„ç†è¡Œå‘å¼‚å¸¸å€¼å›°éš¾ä¸”å¼€é”€å¤§ã€‚
2. ConvRotåˆ©ç”¨æ­£åˆ™Hadamardå˜æ¢è¿›è¡Œåˆ†ç»„æ—‹è½¬ï¼ŒæŠ‘åˆ¶è¡Œåˆ—å¼‚å¸¸å€¼ï¼Œå¤æ‚åº¦ä»ŽäºŒæ¬¡é™è‡³çº¿æ€§ã€‚
3. å®žéªŒåœ¨FLUX.1-devä¸Šå®žçŽ°2.26å€åŠ é€Ÿå’Œ4.05å€å†…å­˜å‡å°‘ï¼Œä¿æŒå›¾åƒè´¨é‡ï¼Œæ”¯æŒW4A4æŽ¨ç†æ— éœ€é‡è®­ç»ƒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.

