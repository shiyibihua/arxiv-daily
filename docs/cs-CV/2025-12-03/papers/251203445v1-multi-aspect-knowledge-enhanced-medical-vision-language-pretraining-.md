---
layout: default
title: Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation
---

# Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation

**arXiv**: [2512.03445v1](https://arxiv.org/abs/2512.03445) | [PDF](https://arxiv.org/pdf/2512.03445.pdf)

**ä½œè€…**: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆä¸Žæœ¬ä½“å¤šçŸ¥è¯†å¢žå¼ºçš„åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æž¶ï¼Œä»¥è§£å†³æ•°æ®å™ªå£°å’Œé•¿æ–‡æœ¬å¤æ‚æ€§ã€‚**

**å…³é”®è¯**: `åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒ` `å¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆ` `æœ¬ä½“çŸ¥è¯†å¢žå¼º` `çš®è‚¤ç—…å­¦åˆ†æž` `é›¶æ ·æœ¬å­¦ä¹ ` `è·¨æ¨¡æ€æ£€ç´¢`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ç½‘ç»œæ”¶é›†æ•°æ®çš„å™ªå£°å’Œéžç»“æž„åŒ–é•¿åŒ»å­¦æ–‡æœ¬çš„å¤æ‚æ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆç³»ç»Ÿæå‡æ•°æ®è´¨é‡ï¼Œå¹¶åˆ©ç”¨æœ¬ä½“å¤šçŸ¥è¯†å¢žå¼ºé¢„è®­ç»ƒåˆ†è§£é•¿æ–‡æœ¬è¿›è¡Œç»†ç²’åº¦å¯¹é½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨çš®è‚¤ç—…å­¦é¢†åŸŸéªŒè¯ï¼Œé›¶æ ·æœ¬æ€§èƒ½åœ¨ç–¾ç—…åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä¼˜ï¼Œå¹¶å‘å¸ƒå¢žå¼ºæ•°æ®é›†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.

