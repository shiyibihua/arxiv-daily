---
layout: default
title: PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention
---

# PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention

**arXiv**: [2512.03724v1](https://arxiv.org/abs/2512.03724) | [PDF](https://arxiv.org/pdf/2512.03724.pdf)

**ä½œè€…**: Ziwen Li, Xin Wang, Hanlue Zhang, Runnan Chen, Runqi Lin, Xiao He, Han Huang, Yandong Guo, Fakhri Karray, Tongliang Liu, Mingming Gong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPosA-VLAæ¡†æž¶ï¼Œé€šè¿‡å§¿æ€æ¡ä»¶é”šå®šæ³¨æ„åŠ›å¢žå¼ºåŠ¨ä½œç”Ÿæˆï¼Œè§£å†³VLAæ¨¡åž‹åœ¨å¤æ‚çŽ¯å¢ƒä¸­åŠ¨ä½œå†—ä½™é—®é¢˜ã€‚**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å§¿æ€æ¡ä»¶æ³¨æ„åŠ›` `æœºå™¨äººæ“ä½œ` `åŠ¨ä½œç”Ÿæˆ` `è½»é‡æž¶æž„` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰VLAæ¨¡åž‹å› ç©ºé—´å‡åŒ€æ„ŸçŸ¥åœºï¼Œåœ¨å¤æ‚çŽ¯å¢ƒä¸­æ˜“å—ç›®æ ‡æ— å…³å¯¹è±¡å¹²æ‰°ï¼Œå¯¼è‡´åŠ¨ä½œå†—ä½™å’Œä¸ç¨³å®šã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥å§¿æ€æ¡ä»¶é”šå®šæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼•å¯¼æ¨¡åž‹èšç„¦ä»»åŠ¡ç›¸å…³åŒºåŸŸï¼Œæ— éœ€è¾…åŠ©æ„ŸçŸ¥æ¨¡å—ï¼Œå®žçŽ°è½»é‡é«˜æ•ˆæŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šæ ·åŒ–æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­éªŒè¯ï¼Œæ–¹æ³•èƒ½ç”Ÿæˆç²¾ç¡®ã€é«˜æ•ˆåŠ¨ä½œï¼Œå¹¶åœ¨æŒ‘æˆ˜æ€§çŽ¯å¢ƒä¸­å±•çŽ°é²æ£’æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.

