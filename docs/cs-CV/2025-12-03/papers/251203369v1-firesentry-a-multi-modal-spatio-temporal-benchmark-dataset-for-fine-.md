---
layout: default
title: FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting
---

# FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting

**arXiv**: [2512.03369v1](https://arxiv.org/abs/2512.03369) | [PDF](https://arxiv.org/pdf/2512.03369.pdf)

**ä½œè€…**: Nan Zhou, Huandong Wang, Jiahao Li, Han Li, Yali Song, Qiuhua Wang, Yong Li, Xinlei Chen

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFireSentryæ•°æ®é›†ä¸ŽFiReDiffèŒƒå¼ä»¥è§£å†³ç»†ç²’åº¦é‡Žç«è”“å»¶é¢„æµ‹é—®é¢˜**

**å…³é”®è¯**: `é‡Žç«è”“å»¶é¢„æµ‹` `å¤šæ¨¡æ€æ•°æ®é›†` `ç»†ç²’åº¦å»ºæ¨¡` `ç”Ÿæˆæ¨¡åž‹` `è§†é¢‘é¢„æµ‹` `æŽ©ç åˆ†å‰²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰é‡Žç«é¢„æµ‹ç ”ç©¶ä¾èµ–ä½Žåˆ†è¾¨çŽ‡æ•°æ®ï¼Œéš¾ä»¥å»ºæ¨¡å±€éƒ¨åŠ¨æ€ï¼Œé™åˆ¶äº†é«˜ç²¾åº¦é¢„æµ‹èƒ½åŠ›ã€‚
2. FireSentryæä¾›äºšç±³çº§ç©ºé—´å’Œäºšç§’çº§æ—¶é—´åˆ†è¾¨çŽ‡çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒåŒ…æ‹¬å¯è§å…‰/çº¢å¤–è§†é¢‘å’ŒçŽ¯å¢ƒæµ‹é‡ã€‚
3. FiReDiffé€šè¿‡å…ˆé¢„æµ‹çº¢å¤–è§†é¢‘å†åˆ†å‰²ç«æŽ©ç ï¼Œåœ¨ç”Ÿæˆæ¨¡åž‹ä¸­æ˜¾è‘—æå‡è§†é¢‘è´¨é‡å’ŒæŽ©ç å‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.

