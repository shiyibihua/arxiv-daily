---
layout: default
title: Diminishing Returns in Self-Supervised Learning
---

# Diminishing Returns in Self-Supervised Learning

**arXiv**: [2512.03862v1](https://arxiv.org/abs/2512.03862) | [PDF](https://arxiv.org/pdf/2512.03862.pdf)

**ä½œè€…**: Oli Bridge, Huey Sun, Botond Branyicskai-Nagy, Charles D'Ornano, Shomit Basu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æŽ¢ç´¢å°è§„æ¨¡è§†è§‰Transformeråœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­çš„æ”¶ç›Šé€’å‡ä¸Žä¸­é—´å¾®è°ƒæ½œåœ¨å±å®³**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è§†è§‰Transformer` `æ”¶ç›Šé€’å‡` `ä¸­é—´å¾®è°ƒ` `å°è§„æ¨¡æ¨¡åž‹` `æ•°æ®é€‰æ‹©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç ”ç©¶å°è§„æ¨¡ViTåœ¨é¢„è®­ç»ƒã€ä¸­é—´å¾®è°ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½è¾¹é™…æ”¶ç›Šé€’å‡çŽ°è±¡
2. å‘çŽ°ä¸­é—´å¾®è°ƒå¯èƒ½å› ä»»åŠ¡æœºåˆ¶å·®å¼‚å¯¹ä¸‹æ¸¸æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“
3. å»ºè®®é’ˆå¯¹å°è§„æ¨¡ViTé‡‡ç”¨å®šå‘é¢„è®­ç»ƒå’Œè°¨æ…Žæ•°æ®é€‰æ‹©ä»¥ä¼˜åŒ–è®¡ç®—æ•ˆçŽ‡

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.

