---
layout: default
title: TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning
---

# TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning

**arXiv**: [2512.03963v1](https://arxiv.org/abs/2512.03963) | [PDF](https://arxiv.org/pdf/2512.03963.pdf)

**ä½œè€…**: Tao Wu, Li Yang, Gen Zhan, Yiting Liao, Junlin Li, Deliang Fu, Li Zhang, Limin Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTempR1æ¡†æž¶ï¼Œé€šè¿‡æ—¶æ€æ„ŸçŸ¥å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ å¢žå¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ—¶åºç†è§£` `å¼ºåŒ–å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `é•¿è§†é¢‘åˆ†æž` `æ—¶åºå®šä½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ—¶åºç†è§£ä¸­ä»»åŠ¡ç±»åž‹å’Œæ•°æ®æœ‰é™ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºå¤šä»»åŠ¡è¯­æ–™åº“ï¼ŒåŸºäºŽGRPOç®—æ³•è®¾è®¡é’ˆå¯¹ä¸åŒæ—¶åºå¯¹åº”ç±»åž‹çš„å®šä½å¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œè”åˆä¼˜åŒ–äº§ç”ŸååŒæ•ˆåº”ï¼Œæå‡æ³›åŒ–å’Œå•ä»»åŠ¡è¡¨çŽ°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

