---
layout: default
title: Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding
---

# Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding

**arXiv**: [2512.03601v1](https://arxiv.org/abs/2512.03601) | [PDF](https://arxiv.org/pdf/2512.03601.pdf)

**ä½œè€…**: Haoran Zhou, Gim Hee Lee

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMotion4Dæ¡†æž¶ï¼Œé€šè¿‡4Dé«˜æ–¯æº…å°„æ•´åˆ2Då…ˆéªŒï¼Œè§£å†³åŠ¨æ€åœºæ™¯ç†è§£ä¸­çš„3Dä¸ä¸€è‡´é—®é¢˜ã€‚**

**å…³é”®è¯**: `4Dåœºæ™¯ç†è§£` `é«˜æ–¯æº…å°„` `è¿åŠ¨ä¼°è®¡` `è¯­ä¹‰åˆ†å‰²` `å•ç›®è§†é¢‘åˆ†æž` `è¿­ä»£ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼š2DåŸºç¡€æ¨¡åž‹åœ¨å•ç›®è§†é¢‘åˆ†æžä¸­ç¼ºä¹3Dä¸€è‡´æ€§ï¼Œå¯¼è‡´ç©ºé—´é”™ä½å’Œæ—¶é—´é—ªçƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè¿­ä»£ä¼˜åŒ–ï¼Œç»“åˆ3Dç½®ä¿¡åº¦å›¾å’Œè‡ªé€‚åº”é‡é‡‡æ ·ï¼Œæå‡è¿åŠ¨å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç‚¹è·Ÿè¸ªã€è§†é¢‘å¯¹è±¡åˆ†å‰²å’Œæ–°è§†å›¾åˆæˆç­‰ä»»åŠ¡ä¸­ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

