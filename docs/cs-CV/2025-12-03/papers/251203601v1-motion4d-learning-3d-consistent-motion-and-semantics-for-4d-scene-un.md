---
layout: default
title: Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding
---

# Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.03601" target="_blank" class="toolbar-btn">arXiv: 2512.03601v1</a>
    <a href="https://arxiv.org/pdf/2512.03601.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03601v1" 
            onclick="toggleFavorite(this, '2512.03601v1', 'Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Haoran Zhou, Gim Hee Lee

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

**Â§áÊ≥®**: Accepted to NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://hrzhou2.github.io/motion4d-web/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Motion4DÔºöÂ≠¶‰π†3D‰∏ÄËá¥ÁöÑËøêÂä®ÂíåËØ≠‰πâ‰ø°ÊÅØÔºåÁî®‰∫é4DÂú∫ÊôØÁêÜËß£**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü• (Perception & SLAM)**

**ÂÖ≥ÈîÆËØç**: `4DÂú∫ÊôØÁêÜËß£` `Âä®ÊÄÅÂú∫ÊôØÂàÜÊûê` `3D‰∏ÄËá¥ÊÄß` `È´òÊñØÊ∫ÖÂ∞Ñ` `ËøêÂä®‰º∞ËÆ°` `ËØ≠‰πâÂàÜÂâ≤` `ÂçïÁõÆËßÜÈ¢ë` `Âü∫Á°ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ2DËßÜËßâÂü∫Á°ÄÊ®°ÂûãÂú®Âä®ÊÄÅÂú∫ÊôØÂàÜÊûê‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁº∫‰πè3D‰∏ÄËá¥ÊÄßÔºåÂØºËá¥Á©∫Èó¥Èîô‰ΩçÂíåÊó∂Èó¥Èó™ÁÉÅ„ÄÇ
2. Motion4DÂ∞Ü2DÂÖàÈ™åÁü•ËØÜËûçÂÖ•4DÈ´òÊñØÊ∫ÖÂ∞ÑË°®Á§∫ÔºåÈÄöËøáÈ°∫Â∫èÂíåÂÖ®Â±Ä‰ºòÂåñÔºåÂÆûÁé∞3D‰∏ÄËá¥ÁöÑËøêÂä®ÂíåËØ≠‰πâÁêÜËß£„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMotion4DÂú®ÁÇπ‰∫ëË∑üË∏™„ÄÅËßÜÈ¢ëÂàÜÂâ≤ÂíåÊñ∞ËßÜËßíÂêàÊàêÁ≠â‰ªªÂä°‰∏≠ÔºåÊòæËëó‰ºò‰∫éÁé∞Êúâ2DÂíå3DÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫Motion4DÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÂçïÁõÆËßÜÈ¢ëÂä®ÊÄÅÂú∫ÊôØÂàÜÊûê‰∏≠ÔºåÁé∞Êúâ2DÂü∫Á°ÄÊ®°ÂûãÁº∫‰πè3D‰∏ÄËá¥ÊÄßÁöÑÈóÆÈ¢ò„ÄÇMotion4DÂ∞Ü2DÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜÊï¥ÂêàÂà∞Áªü‰∏ÄÁöÑ4DÈ´òÊñØÊ∫ÖÂ∞ÑË°®Á§∫‰∏≠„ÄÇËØ•ÊñπÊ≥ïÂåÖÂê´‰∏Ä‰∏™‰∏§Èò∂ÊÆµËø≠‰ª£‰ºòÂåñÊ°ÜÊû∂Ôºö1) È°∫Â∫è‰ºòÂåñÔºåÂàÜÈò∂ÊÆµÊõ¥Êñ∞ËøêÂä®ÂíåËØ≠‰πâÂú∫Ôºå‰ª•‰øùÊåÅÂ±ÄÈÉ®‰∏ÄËá¥ÊÄßÔºõ2) ÂÖ®Â±Ä‰ºòÂåñÔºåËÅîÂêà‰ºòÂåñÊâÄÊúâÂ±ûÊÄßÔºå‰ª•ÂÆûÁé∞ÈïøÊúüËøûË¥ØÊÄß„ÄÇ‰∏∫‰∫ÜÊèêÈ´òËøêÂä®Á≤æÂ∫¶ÔºåÂºïÂÖ•‰∫Ü3DÁΩÆ‰ø°Â∫¶ÂõæÔºåÂä®ÊÄÅË∞ÉÊï¥ËøêÂä®ÂÖàÈ™åÔºåÂπ∂ÈááÁî®Ëá™ÈÄÇÂ∫îÈáçÈááÊ†∑ËøáÁ®ãÔºåÂü∫‰∫éÂÉèÁ¥†RGBÂíåËØ≠‰πâËØØÂ∑ÆÔºåÂú®Ê¨†Ë°®Á§∫Âå∫ÂüüÊèíÂÖ•Êñ∞ÁöÑÈ´òÊñØÂàÜÂ∏É„ÄÇÊ≠§Â§ñÔºåÈÄöËøáËø≠‰ª£‰ºòÂåñËØ≠‰πâÂú∫ÂíåÊõ¥Êñ∞SAMÁöÑÊèêÁ§∫ÔºåÂ¢ûÂº∫ËØ≠‰πâËøûË¥ØÊÄß„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåMotion4DÂú®Âü∫‰∫éÁÇπÁöÑË∑üË∏™„ÄÅËßÜÈ¢ëÂØπË±°ÂàÜÂâ≤ÂíåÊñ∞ËßÜËßíÂêàÊàêÁ≠âÂ§öÁßçÂú∫ÊôØÁêÜËß£‰ªªÂä°‰∏≠ÔºåÊòæËëó‰ºò‰∫é2DÂü∫Á°ÄÊ®°ÂûãÂíåÁé∞Êúâ3DÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫é2DËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÂú®Â§ÑÁêÜÂçïÁõÆËßÜÈ¢ëÁöÑÂä®ÊÄÅÂú∫ÊôØÁêÜËß£Êó∂ÔºåËôΩÁÑ∂ÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩÜÁº∫‰πè3D‰∏ÄËá¥ÊÄß„ÄÇËøôÂØºËá¥Âú®Â§çÊùÇÁöÑ3DÁéØÂ¢É‰∏≠Âá∫Áé∞‰∏•ÈáçÁöÑÂá†‰ΩïÈîô‰ΩçÂíåÊó∂Èó¥‰∏äÁöÑÈó™ÁÉÅÁé∞Ë±°ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÈúÄË¶ÅÁ≤æÁ°Æ3D‰ø°ÊÅØÁöÑ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§ü‰øùËØÅ3D‰∏ÄËá¥ÊÄßÁöÑÂä®ÊÄÅÂú∫ÊôØÁêÜËß£ÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMotion4DÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü2DËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÂº∫Â§ßÂÖàÈ™åÁü•ËØÜ‰∏é3DÂú∫ÊôØË°®Á§∫Áõ∏ÁªìÂêàÔºåÂà©Áî®È´òÊñØÊ∫ÖÂ∞ÑÔºàGaussian SplattingÔºâ‰Ωú‰∏∫Áªü‰∏ÄÁöÑ4DË°®Á§∫ÔºåÂπ∂ÈÄöËøáËø≠‰ª£‰ºòÂåñÊ°ÜÊû∂Êù•‰øùËØÅËøêÂä®ÂíåËØ≠‰πâÁöÑ3D‰∏ÄËá¥ÊÄß„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•ÊúâÊïàÂú∞Âà©Áî®2DÊ®°ÂûãÁöÑ‰ºòÂäøÔºåÂêåÊó∂ÂÖãÊúçÂÖ∂Âú®3DÁ©∫Èó¥‰∏≠ÁöÑ‰∏çË∂≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMotion4DÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÁöÑËø≠‰ª£‰ºòÂåñÈò∂ÊÆµÔºöÈ°∫Â∫è‰ºòÂåñÂíåÂÖ®Â±Ä‰ºòÂåñ„ÄÇÈ°∫Â∫è‰ºòÂåñÈ¶ñÂÖàÊõ¥Êñ∞ËøêÂä®Âú∫ÔºåÁÑ∂ÂêéÊõ¥Êñ∞ËØ≠‰πâÂú∫Ôºå‰ª•‰øùÊåÅÂ±ÄÈÉ®‰∏ÄËá¥ÊÄß„ÄÇÂÖ®Â±Ä‰ºòÂåñÂàôËÅîÂêà‰ºòÂåñÊâÄÊúâÂ±ûÊÄßÔºå‰ª•ÂÆûÁé∞ÈïøÊúüËøûË¥ØÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ËøòÂåÖÊã¨‰∏Ä‰∏™3DÁΩÆ‰ø°Â∫¶ÂõæÔºåÁî®‰∫éÂä®ÊÄÅË∞ÉÊï¥ËøêÂä®ÂÖàÈ™åÔºå‰ª•Âèä‰∏Ä‰∏™Ëá™ÈÄÇÂ∫îÈáçÈááÊ†∑ËøáÁ®ãÔºåÁî®‰∫éÂú®Ê¨†Ë°®Á§∫Âå∫ÂüüÊèíÂÖ•Êñ∞ÁöÑÈ´òÊñØÂàÜÂ∏É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMotion4DÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Â∞Ü2DËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜÊúâÊïàÂú∞ËûçÂÖ•Âà∞3DÂú∫ÊôØË°®Á§∫‰∏≠ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏§Èò∂ÊÆµÁöÑËø≠‰ª£‰ºòÂåñÊ°ÜÊû∂Ôºå‰ª•‰øùËØÅËøêÂä®ÂíåËØ≠‰πâÁöÑ3D‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºå3DÁΩÆ‰ø°Â∫¶ÂõæÂíåËá™ÈÄÇÂ∫îÈáçÈááÊ†∑ËøáÁ®ãËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜËøêÂä®Á≤æÂ∫¶ÂíåÂú∫ÊôØË°®Á§∫ÁöÑÂÆåÊï¥ÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåMotion4DËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§çÊùÇÁöÑ3DÂä®ÊÄÅÂú∫ÊôØÔºåÂπ∂Êèê‰æõÊõ¥Á≤æÁ°ÆÁöÑÂú∫ÊôØÁêÜËß£„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö3DÁΩÆ‰ø°Â∫¶ÂõæÁöÑËÆæËÆ°Áî®‰∫éÂä®ÊÄÅË∞ÉÊï¥ËøêÂä®ÂÖàÈ™åÔºåÂÖ∂ÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÊú™Áü•„ÄÇËá™ÈÄÇÂ∫îÈáçÈááÊ†∑ËøáÁ®ãÂü∫‰∫éÂÉèÁ¥†RGBÂíåËØ≠‰πâËØØÂ∑ÆÊù•Á°ÆÂÆöÈúÄË¶ÅÊèíÂÖ•Êñ∞È´òÊñØÂàÜÂ∏ÉÁöÑÂå∫ÂüüÔºåÂÖ∑‰ΩìÁöÑËØØÂ∑ÆËÆ°ÁÆóÊñπÂºèÂíåÈòàÂÄºËÆæÁΩÆÊú™Áü•„ÄÇËØ≠‰πâ‰∏ÄËá¥ÊÄßÈÄöËøáËø≠‰ª£‰ºòÂåñËØ≠‰πâÂú∫ÂíåÊõ¥Êñ∞SAMÁöÑÊèêÁ§∫Êù•ÂÆûÁé∞ÔºåÂÖ∑‰ΩìÁöÑÊèêÁ§∫Êõ¥Êñ∞Á≠ñÁï•Êú™Áü•„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÁªÜËäÇÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Motion4DÂú®Â§ö‰∏™Âú∫ÊôØÁêÜËß£‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂú®ÁÇπ‰∫ëË∑üË∏™„ÄÅËßÜÈ¢ëÂØπË±°ÂàÜÂâ≤ÂíåÊñ∞ËßÜËßíÂêàÊàê‰ªªÂä°‰∏≠ÔºåMotion4DÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑ2DÂü∫Á°ÄÊ®°ÂûãÂíå3DÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜÊëòË¶ÅÂº∫Ë∞É‰∫ÜÂÖ∂Âú®Â§öÁßç‰ªªÂä°‰∏≠ÁöÑ‰ºòË∂äÊÄßÔºåË°®Êòé‰∫ÜËØ•ÊñπÊ≥ïÂÖ∑ÊúâÂæàÂº∫ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Motion4DÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèê‰æõ3D‰∏ÄËá¥ÁöÑÂä®ÊÄÅÂú∫ÊôØÁêÜËß£ÔºåÂèØ‰ª•ÊèêÂçáËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºåÂ¢ûÂº∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõÔºåÂπ∂‰∏∫ARÂ∫îÁî®Êèê‰æõÊõ¥ÈÄºÁúüÁöÑÂú∫ÊôØ‰∫§‰∫í‰ΩìÈ™å„ÄÇËØ•Á†îÁ©∂ËøòÊúâÂä©‰∫éÊé®Âä®ËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèëÁ≠âÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

