---
layout: default
title: Learning Steerable Clarification Policies with Collaborative Self-play
---

# Learning Steerable Clarification Policies with Collaborative Self-play

**arXiv**: [2512.04068v1](https://arxiv.org/abs/2512.04068) | [PDF](https://arxiv.org/pdf/2512.04068.pdf)

**ä½œè€…**: Jonathan Berant, Maximillian Chen, Adam Fisch, Reza Aghajani, Fantine Huot, Mirella Lapata, Jacob Eisenstein

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽè‡ªåšå¼ˆçš„å¯è°ƒæŽ§æ¾„æ¸…ç­–ç•¥ï¼Œä»¥å¤„ç†AIåŠ©æ‰‹åœ¨ä¸ç¡®å®šæŸ¥è¯¢ä¸‹çš„å“åº”å†³ç­–é—®é¢˜ã€‚**

**å…³é”®è¯**: `ä¸ç¡®å®šæ€§ç®¡ç†` `è‡ªåšå¼ˆè®­ç»ƒ` `å¯è°ƒæŽ§ç­–ç•¥` `å¼ºåŒ–è‡ªè®­ç»ƒ` `æ¾„æ¸…ç­–ç•¥`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šAIåŠ©æ‰‹éœ€åœ¨ä¸ç¡®å®šæŸ¥è¯¢æ—¶å†³å®šä½•æ—¶çŒœæµ‹æ„å›¾ã€æžšä¸¾æ„å›¾æˆ–æé—®æ¾„æ¸…ï¼Œç­–ç•¥å—ç”¨æˆ·åå¥½å’Œæ¨¡æ€ç­‰å› ç´ å½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šä½¿ç”¨è‡ªåšå¼ˆè®­ç»ƒå¯è°ƒæŽ§ç­–ç•¥ï¼Œè¾“å…¥æ¾„æ¸…é—®é¢˜å’Œç”Ÿæˆè¯çš„æˆæœ¬ï¼Œé€šè¿‡å¼ºåŒ–è‡ªè®­ç»ƒæœ€å¤§åŒ–æˆæœ¬æƒ©ç½šåŽçš„å‡†ç¡®çŽ‡å¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šç­–ç•¥èƒ½æ ¹æ®æˆæœ¬å¯é¢„æµ‹åœ°è°ƒæ•´è¡Œä¸ºï¼Œæé«˜å¥–åŠ±å’Œå‡†ç¡®çŽ‡ï¼Œå¹¶æ³›åŒ–åˆ°è®­ç»ƒæ—¶æœªè§çš„æˆæœ¬å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.

