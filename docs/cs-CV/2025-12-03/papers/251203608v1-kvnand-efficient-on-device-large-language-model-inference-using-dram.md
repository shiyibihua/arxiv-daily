---
layout: default
title: KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing
---

# KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing

**arXiv**: [2512.03608v1](https://arxiv.org/abs/2512.03608) | [PDF](https://arxiv.org/pdf/2512.03608.pdf)

**ä½œè€…**: Lishuo Deng, Shaojie Xu, Jinwu Chen, Changwei Yan, Jiajie Wang, Zhe Jiang, Weiwei Shan

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKVNANDæž¶æž„ï¼Œåˆ©ç”¨é—ªå­˜å†…è®¡ç®—å®žçŽ°æ— DRAMçš„å¤§è¯­è¨€æ¨¡åž‹è¾¹ç¼˜æŽ¨ç†ï¼Œè§£å†³é•¿ä¸Šä¸‹æ–‡KVç¼“å­˜å­˜å‚¨ç“¶é¢ˆã€‚**

**å…³é”®è¯**: `é—ªå­˜å†…è®¡ç®—` `å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†` `KVç¼“å­˜ä¼˜åŒ–` `è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²` `3D NANDé—ªå­˜` `é•¿ä¸Šä¸‹æ–‡å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡ä¸‹KVç¼“å­˜è¶…è¿‡æ¨¡åž‹æƒé‡ï¼Œå¯¼è‡´DRAMæˆæœ¬é«˜ä¸”å®¹é‡ä¸è¶³ï¼Œé—ªå­˜å­˜å‚¨æ€§èƒ½å·®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ¨¡åž‹æƒé‡å’ŒKVç¼“å­˜å…¨å­˜å‚¨åœ¨3D NANDé—ªå­˜ï¼Œé€šè¿‡é—ªå­˜å†…è®¡ç®—ã€å¤´ç»„å¹¶è¡Œå’Œé¡µé¢çº§æ˜ å°„ä¼˜åŒ–è®¿é—®æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨7Bå’Œ70Bæ¨¡åž‹ä¸Šï¼Œç›¸æ¯”DRAMæ–¹æ¡ˆï¼Œåœ¨128/1K/10Kä¸Šä¸‹æ–‡é•¿åº¦ä¸‹å®žçŽ°1.98Ã—/1.94Ã—/2.05Ã—å‡ ä½•å¹³å‡åŠ é€Ÿï¼Œæ”¯æŒ100Kä¸Šä¸‹æ–‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.
>   We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.

