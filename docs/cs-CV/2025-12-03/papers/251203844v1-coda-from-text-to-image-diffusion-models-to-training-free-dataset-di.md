---
layout: default
title: CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation
---

# CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation

**arXiv**: [2512.03844v1](https://arxiv.org/abs/2512.03844) | [PDF](https://arxiv.org/pdf/2512.03844.pdf)

**ä½œè€…**: Letian Zhou, Songhua Liu, Xinchao Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoDAæ¡†æž¶ï¼Œåˆ©ç”¨çŽ°æˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹å®žçŽ°å…è®­ç»ƒæ•°æ®é›†è’¸é¦**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹` `æ ¸å¿ƒåˆ†å¸ƒå¯¹é½` `å…è®­ç»ƒè’¸é¦` `ç”Ÿæˆæ¨¡åž‹` `å›¾åƒåˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•ä¾èµ–ç›®æ ‡æ•°æ®é›†é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹ï¼Œæˆæœ¬é«˜ä¸”è¿èƒŒè’¸é¦åˆè¡·
2. CoDAé€šè¿‡è¯†åˆ«ç›®æ ‡æ•°æ®é›†æ ¸å¿ƒåˆ†å¸ƒå¹¶å¼•å¯¼ç”Ÿæˆå¯¹é½ï¼Œå¼¥åˆé€šç”¨ç”Ÿæˆå…ˆéªŒä¸Žç›®æ ‡è¯­ä¹‰å·®è·
3. å®žéªŒæ˜¾ç¤ºCoDAåœ¨ImageNet-1Kç­‰åŸºå‡†ä¸Šæ€§èƒ½åª²ç¾Žæˆ–è¶…è¶Šä¾èµ–ç›®æ ‡ç‰¹å®šè®­ç»ƒçš„æ–¹æ³•

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA

