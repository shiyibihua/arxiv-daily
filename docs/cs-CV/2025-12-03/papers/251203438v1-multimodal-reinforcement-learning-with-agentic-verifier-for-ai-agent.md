---
layout: default
title: Multimodal Reinforcement Learning with Agentic Verifier for AI Agents
---

# Multimodal Reinforcement Learning with Agentic Verifier for AI Agents

**arXiv**: [2512.03438v1](https://arxiv.org/abs/2512.03438) | [PDF](https://arxiv.org/pdf/2512.03438.pdf)

**ä½œè€…**: Reuben Tan, Baolin Peng, Zhengyuan Yang, Hao Cheng, Oier Mees, Theodore Zhao, Andrea Tupini, Isar Meijier, Qianhui Wu, Yuncong Yang, Lars Liden, Yu Gu, Sheng Zhang, Xiaodong Liu, Lijuan Wang, Marc Pollefeys, Yong Jae Lee, Jianfeng Gao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºArgosä»£ç†å¥–åŠ±ç³»ç»Ÿï¼Œä»¥è§£å†³å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±ä¿¡å·ç¨€ç–å’Œå™ªå£°é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ` `ä»£ç†å¥–åŠ±ç³»ç»Ÿ` `æ—¶ç©ºå®šä½` `æŽ¨ç†è¿‡ç¨‹è¯„ä¼°` `å¥–åŠ±é»‘å®¢å‡å°‘` `å¸•ç´¯æ‰˜æœ€ä¼˜æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¾èµ–ç¨€ç–ç»“æžœå¥–åŠ±ï¼Œç¼ºä¹ç»†ç²’åº¦æŒ‡å¯¼ï¼Œä¸”æ•™å¸ˆæ¨¡åž‹å¥–åŠ±å¯èƒ½å«å™ªå£°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šArgosä»Žæ•™å¸ˆæ¨¡åž‹å’Œè§„åˆ™è¯„åˆ†å‡½æ•°æ± ä¸­é€‰æ‹©ï¼Œè¯„ä¼°å“åº”å‡†ç¡®æ€§ã€æ—¶ç©ºå®šä½å’ŒæŽ¨ç†è¿‡ç¨‹è´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ç©ºé—´æŽ¨ç†ã€è§†è§‰å¹»è§‰åŠæœºå™¨äººä»»åŠ¡ä¸­å®žçŽ°SOTAï¼Œå‡å°‘å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œç†è®ºåŸºäºŽå¸•ç´¯æ‰˜æœ€ä¼˜æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.

