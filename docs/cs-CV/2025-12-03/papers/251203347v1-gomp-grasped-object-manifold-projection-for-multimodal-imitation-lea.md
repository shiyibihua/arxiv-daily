---
layout: default
title: GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation
---

# GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation

**arXiv**: [2512.03347v1](https://arxiv.org/abs/2512.03347) | [PDF](https://arxiv.org/pdf/2512.03347.pdf)

**ä½œè€…**: William van den Bogert, Gregory Linkowski, Nima Fazeli

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGOMPæ–¹æ³•ï¼Œé€šè¿‡ä½Žç»´æµå½¢æŠ•å½±è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­è½¨è¿¹ç²¾åº¦ä¸è¶³çš„é—®é¢˜ï¼Œåº”ç”¨äºŽç²¾ç¡®è£…é…ä»»åŠ¡ã€‚**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `ç²¾ç¡®è£…é…` `ä½Žç»´æµå½¢æŠ•å½±` `è§¦è§‰åé¦ˆ` `äº¤äº’å¼å­¦ä¹ ` `ç´¯ç§¯è¯¯å·®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ¨¡ä»¿å­¦ä¹ åœ¨å·¥ä¸šè£…é…ç­‰é‡å¤æ€§æ“ä½œä»»åŠ¡ä¸­æ½œåŠ›å¤§ï¼Œä½†å¸¸å› ç´¯ç§¯è¯¯å·®å¯¼è‡´è½¨è¿¹ç²¾åº¦ä¸è¶³ã€‚
2. GOMPé€šè¿‡å°†éžåˆšæ€§æŠ“å–ç‰©ä½“çº¦æŸåˆ°ä½Žç»´æµå½¢ï¼Œå‡å°‘è¯¯å·®ï¼Œæ–¹æ³•åŸºäºŽä¸“å®¶æ•°æ®å­¦ä¹ å¹¶é‡‡ç”¨äº¤äº’å¼è°ƒæ•´ã€‚
3. åœ¨å››ä¸ªç²¾ç¡®è£…é…ä»»åŠ¡ä¸­éªŒè¯ï¼Œä½¿ç”¨è§¦è§‰åé¦ˆï¼Œæ–¹æ³•ä¿æŒæ¨¡æ€æ— å…³æ€§ï¼Œç†è®ºåˆ†æžæ”¹è¿›è¯¯å·®ç•Œé™ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GOMPsite.

