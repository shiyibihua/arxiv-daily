---
layout: default
title: V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention
---

# V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention

**arXiv**: [2512.03542v1](https://arxiv.org/abs/2512.03542) | [PDF](https://arxiv.org/pdf/2512.03542.pdf)

**ä½œè€…**: Nan Sun, Zhenyu Zhang, Xixun Lin, Kun Wang, Yanmin Shang, Naibin Gu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang, Yanan Cao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºV-ITIæ¡†æž¶ï¼Œé€šè¿‡è§†è§‰æŽ¨ç†æ—¶å¹²é¢„ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ä¸­çš„å¹»è§‰é—®é¢˜**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `è§†è§‰å¹»è§‰ç¼“è§£` `æŽ¨ç†æ—¶å¹²é¢„` `è§†è§‰å¿½è§†æ£€æµ‹` `æ¿€æ´»è°ƒåˆ¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹å­˜åœ¨è§†è§‰å¿½è§†å¯¼è‡´çš„å¹»è§‰ï¼ŒçŽ°æœ‰æ–¹æ³•å› å¿½è§†å¹²é¢„æ—¶æœºè€Œäº§ç”Ÿè¿‡å¹²é¢„é—®é¢˜
2. æ–¹æ³•è¦ç‚¹ï¼šé›†æˆè§†è§‰å¿½è§†æ£€æµ‹å™¨ä¸Žè§†è§‰å›žå¿†å¹²é¢„å™¨ï¼Œä»…åœ¨æ£€æµ‹åˆ°è§†è§‰å¿½è§†æ—¶è°ƒåˆ¶æ¿€æ´»ï¼Œé¿å…è¿‡å¹²é¢„
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å…«ä¸ªåŸºå‡†å’Œä¸åŒæ¨¡åž‹å®¶æ—ä¸ŠéªŒè¯ï¼Œæœ‰æ•ˆç¼“è§£å¹»è§‰å¹¶ä¿æŒä¸€èˆ¬ä»»åŠ¡æ€§èƒ½

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.

