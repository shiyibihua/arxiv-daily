---
layout: default
title: Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value
---

# Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value

**arXiv**: [2512.03399v1](https://arxiv.org/abs/2512.03399) | [PDF](https://arxiv.org/pdf/2512.03399.pdf)

**ä½œè€…**: Joe Edelman, Tan Zhi-Xuan, Ryan Lowe, Oliver Klingefjord, Vincent Wang-Mascianica, Matija Franklin, Ryan Othniel Kearns, Ellie Hain, Atrisha Sarkar, Michiel Bakker, Fazl Barez, David Duvenaud, Jakob Foerster, Iason Gabriel, Joseph Gubbels, Bryce Goodman, Andreas Haupt, Jobst Heitzig, Julian Jara-Ettinger, Atoosa Kasirzadeh, James Ravi Kirkpatrick, Andrew Koh, W. Bradley Knox, Philipp Koralus, Joel Lehman, Sydney Levine, Samuele Marro, Manon Revel, Toby Shorin, Morgan Sutherland, Michael Henry Tessler, Ivan Vendrov, James Wilken-Smith

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŽšä»·å€¼æ¨¡åž‹ä»¥å®žçŽ°AIä¸Žæœºæž„çš„å…¨æ ˆå¯¹é½ï¼Œè§£å†³ä»·å€¼è¡¨ç¤ºä¸Žé›†ä½“åˆ©ç›Šé—®é¢˜ã€‚**

**å…³é”®è¯**: `AIå¯¹é½` `ä»·å€¼è¡¨ç¤º` `è§„èŒƒæ€§æŽ¨ç†` `é›†ä½“åˆ©ç›Š` `æœºæž„è®¾è®¡` `åŽšä»·å€¼æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰AIå¯¹é½æ–¹æ³•éš¾ä»¥åŒºåˆ†ä»·å€¼ä¸Žåå¥½ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡é›†ä½“åˆ©ç›Šå’Œè¿›è¡Œè§„èŒƒæ€§æŽ¨ç†ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŽšä»·å€¼æ¨¡åž‹ï¼Œç»“æž„åŒ–è¡¨ç¤ºä»·å€¼å’Œè§„èŒƒï¼Œæ”¯æŒåŒºåˆ†æŒä¹…ä»·å€¼ä¸ŽçŸ­æš‚åå¥½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨AIä»·å€¼ç®¡ç†ã€è§„èŒƒæ€§æ™ºèƒ½ä½“ã€åŒèµ¢è°ˆåˆ¤ç³»ç»Ÿç­‰äº”ä¸ªé¢†åŸŸå±•ç¤ºåº”ç”¨æ½œåŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

