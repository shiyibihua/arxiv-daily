---
layout: default
title: Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective
---

# Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective

**arXiv**: [2512.03759v1](https://arxiv.org/abs/2512.03759) | [PDF](https://arxiv.org/pdf/2512.03759.pdf)

**ä½œè€…**: Jingyang Ou, Jiaqi Han, Minkai Xu, Shaoxuan Xu, Jianwen Xie, Stefano Ermon, Yi Wu, Chongxuan Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽELBOçš„åºåˆ—çº§ç­–ç•¥ä¼˜åŒ–ï¼ˆESPOï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å¤§è¯­è¨€æ¨¡åž‹ä¸­çš„å¼ºåŒ–å­¦ä¹ é€‚é…éš¾é¢˜ã€‚**

**å…³é”®è¯**: `æ‰©æ•£å¤§è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `åºåˆ—çº§ä¼˜åŒ–` `ELBO` `ç­–ç•¥ä¼˜åŒ–` `æ•°å­¦æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šæ‰©æ•£å¤§è¯­è¨€æ¨¡åž‹ç¼ºä¹è‡ªå›žå½’æ¨¡åž‹çš„tokençº§æ¦‚çŽ‡åˆ†è§£ï¼Œå¯¼è‡´ä¼ ç»Ÿtokençº§RLæ–¹æ³•ä¸é€‚ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†åºåˆ—ç”Ÿæˆè§†ä¸ºå•ä¸€åŠ¨ä½œï¼Œä½¿ç”¨ELBOä½œä¸ºåºåˆ—çº§ä¼¼ç„¶ä»£ç†ï¼Œç»“åˆtokençº§å½’ä¸€åŒ–å’Œç¨³å¥KLæ•£åº¦ä¼°è®¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ•°å­¦æŽ¨ç†ã€ç¼–ç å’Œè§„åˆ’ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶ŠåŸºçº¿ï¼Œå¦‚åœ¨Countdownä»»åŠ¡ä¸Šæå‡20-40åˆ†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.

