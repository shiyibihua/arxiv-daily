---
layout: default
title: NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation
---

# NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation

**arXiv**: [2512.03499v1](https://arxiv.org/abs/2512.03499) | [PDF](https://arxiv.org/pdf/2512.03499.pdf)

**ä½œè€…**: Renqi Chen, Haoyang Su, Shixiang Tang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNAS-LoRAä»¥å¢žå¼ºè§†è§‰åŸºç¡€æ¨¡åž‹åœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å‚æ•°é«˜æ•ˆå¾®è°ƒèƒ½åŠ›**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ç¥žç»æž¶æž„æœç´¢` `è§†è§‰åŸºç¡€æ¨¡åž‹` `å›¾åƒåˆ†å‰²` `ä½Žç§©é€‚åº”`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šSAMç¼ºä¹ç©ºé—´å…ˆéªŒï¼Œéš¾ä»¥é€‚åº”åŒ»å­¦å’Œå†œä¸šç­‰ä¸“ä¸šé¢†åŸŸã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåœ¨LoRAä¸­é›†æˆè½»é‡çº§NASå—ï¼ŒåŠ¨æ€ä¼˜åŒ–å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨é˜¶æ®µä¼˜åŒ–ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæå‡çŽ°æœ‰PEFTæ–¹æ³•æ€§èƒ½ï¼Œè®­ç»ƒæˆæœ¬é™ä½Ž24.14%ï¼ŒæŽ¨ç†æˆæœ¬ä¸å˜ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.

