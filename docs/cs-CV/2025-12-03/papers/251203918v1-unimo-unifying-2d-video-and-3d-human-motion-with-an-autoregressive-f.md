---
layout: default
title: UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework
---

# UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2512.03918" target="_blank" class="toolbar-btn">arXiv: 2512.03918v1</a>
    <a href="https://arxiv.org/pdf/2512.03918.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03918v1" 
            onclick="toggleFavorite(this, '2512.03918v1', 'UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Youxin Pang, Yong Zhang, Ruizhi Shao, Xiang Deng, Feng Gao, Xu Xiaoming, Xiaoming Wei, Yebin Liu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-03

**Â§áÊ≥®**: https://carlyx.github.io/UniMo/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**UniMoÔºöÊèêÂá∫‰∏Ä‰∏™Ëá™ÂõûÂΩíÊ°ÜÊû∂ÔºåÁªü‰∏ÄÂª∫Ê®°2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ÔºåÂÆûÁé∞ÂêåÊ≠•ÁîüÊàê‰∏éÁêÜËß£„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `2DËßÜÈ¢ëÁîüÊàê` `3D‰∫∫‰ΩìËøêÂä®` `Ëá™ÂõûÂΩíÊ®°Âûã` `Â§öÊ®°ÊÄÅËûçÂêà` `Áªü‰∏ÄÂª∫Ê®°` `ËøêÂä®ÊçïÊçâ` `VQ-VAE`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂêåÊó∂ÁîüÊàêÂíåÁêÜËß£2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®ÁªìÊûÑÂíåÂàÜÂ∏É‰∏äÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇ„ÄÇ
2. UniMoÂ∞Ü2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®Âª∫Ê®°‰∏∫Áªü‰∏ÄÁöÑtokensÂ∫èÂàóÔºåÂà©Áî®Ëá™ÂõûÂΩíÊ®°ÂûãÂÆûÁé∞ÂêåÊ≠•ÁîüÊàêÂíåÁêÜËß£„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåUniMoËÉΩÂ§üÂêåÊó∂ÁîüÊàêÂØπÂ∫îÁöÑËßÜÈ¢ëÂíåËøêÂä®ÔºåÂπ∂ÊâßË°åÁ≤æÁ°ÆÁöÑËøêÂä®ÊçïÊçâÔºåÈ™åËØÅ‰∫ÜÁªü‰∏ÄÂª∫Ê®°ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êàë‰ª¨ÊèêÂá∫‰∫ÜUniMoÔºå‰∏Ä‰∏™ÂàõÊñ∞ÁöÑËá™ÂõûÂΩíÊ®°ÂûãÔºåÁî®‰∫éÂú®Áªü‰∏ÄÊ°ÜÊû∂ÂÜÖËÅîÂêàÂª∫Ê®°2D‰∫∫‰ΩìËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ÔºåÈ¶ñÊ¨°ÂÆûÁé∞‰∫ÜËøô‰∏§ÁßçÊ®°ÊÄÅÁöÑÂêåÊ≠•ÁîüÊàêÂíåÁêÜËß£„ÄÇÁõÆÂâçÁöÑÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ª•‰∏ÄÁßçÊ®°ÊÄÅ‰∏∫Êù°‰ª∂ÁîüÊàêÂè¶‰∏ÄÁßçÊ®°ÊÄÅÔºåÊàñËÄÖÂ∞ÜÂÆÉ‰ª¨‰∏éÊñáÊú¨ÂíåÈü≥È¢ëÁ≠âÂÖ∂‰ªñÊ®°ÊÄÅÈõÜÊàê„ÄÇÁî±‰∫é2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®Âú®ÁªìÊûÑÂíåÂàÜÂ∏É‰∏äÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇÔºåÂõ†Ê≠§Áªü‰∏ÄÂÆÉ‰ª¨ËøõË°åÂêåÊ≠•‰ºòÂåñÂíåÁîüÊàê‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÈ¢ÜÂüüÔºåÈù¢‰∏¥ÁùÄÂ∑®Â§ßÁöÑÊåëÊàò„ÄÇÂèóLLMÁªü‰∏Ä‰∏çÂêåÊ®°ÊÄÅËÉΩÂäõÁöÑÂêØÂèëÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞ÜËßÜÈ¢ëÂíå3DËøêÂä®Âª∫Ê®°‰∏∫Áªü‰∏ÄÁöÑtokensÂ∫èÂàóÔºåÂπ∂Âà©Áî®ÂçïÁã¨ÁöÑÂµåÂÖ•Â±ÇÊù•ÁºìËß£ÂàÜÂ∏ÉÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ∫èÂàóÂª∫Ê®°Á≠ñÁï•ÔºåÂú®‰∏Ä‰∏™Ê°ÜÊû∂ÂÜÖÈõÜÊàê‰∫Ü‰∏§‰∏™‰∏çÂêåÁöÑ‰ªªÂä°ÔºåËØÅÊòé‰∫ÜÁªü‰∏ÄÂª∫Ê®°ÁöÑÊúâÊïàÊÄß„ÄÇËÄå‰∏îÔºå‰∏∫‰∫ÜÊúâÊïàÂú∞‰∏éËßÜËßâtokensÂØπÈΩêÂπ∂‰øùÁïô3DÁ©∫Èó¥‰ø°ÊÅØÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂÖ∑ÊúâÊó∂Èó¥Êâ©Â±ïÁ≠ñÁï•ÁöÑÊñ∞Âûã3DËøêÂä®tokenizerÔºå‰ΩøÁî®Âçï‰∏™VQ-VAEÊù•ÁîüÊàêÈáèÂåñÁöÑËøêÂä®tokens„ÄÇÂÆÉÂÖ∑ÊúâÂ§ö‰∏™‰∏ìÂÆ∂Ëß£Á†ÅÂô®ÔºåÁî®‰∫éÂ§ÑÁêÜË∫´‰ΩìÂΩ¢Áä∂„ÄÅÂπ≥Áßª„ÄÅÂÖ®Â±ÄÊñπÂêëÂíåË∫´‰ΩìÂßøÂäøÔºå‰ª•ÂÆûÁé∞ÂèØÈù†ÁöÑ3DËøêÂä®ÈáçÂª∫„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÊâßË°åÁ≤æÁ°ÆÁöÑËøêÂä®ÊçïÊçâÁöÑÂêåÊó∂ÔºåÂèØ‰ª•ÂêåÊó∂ÁîüÊàêÁõ∏Â∫îÁöÑËßÜÈ¢ëÂíåËøêÂä®„ÄÇËøôÈ°πÂ∑•‰ΩúÊåñÊéò‰∫ÜLLMËûçÂêà‰∏çÂêåÊï∞ÊçÆÁ±ªÂûãÁöÑËÉΩÂäõÔºå‰∏∫Â∞Ü‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑ‰ø°ÊÅØÈõÜÊàêÂà∞Áé∞ÊúâÊ®°Âûã‰∏≠Èì∫Âπ≥‰∫ÜÈÅìË∑ØÔºåÂπ∂ÊúâÂèØËÉΩÂÆûÁé∞‰∫∫Á±ª„ÄÅÁâ©‰ΩìÂíåÂú∫ÊôØÁöÑÂ§öÊ®°ÊÄÅ„ÄÅÂèØÊéßÁöÑËÅîÂêàÂª∫Ê®°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®ÂçïÊ®°ÊÄÅÁîüÊàêÊàñÂ∞Ü2D/3D‰∫∫‰ΩìËøêÂä®‰∏éÂÖ∂‰ªñÊ®°ÊÄÅËûçÂêàÔºåÁº∫‰πèÂØπ2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ÁöÑÁªü‰∏ÄÂª∫Ê®°ÂíåÂêåÊ≠•ÁîüÊàêËÉΩÂäõ„ÄÇÁî±‰∫é2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®Âú®ÁªìÊûÑÂíåÂàÜÂ∏É‰∏äÂ≠òÂú®Â∑®Â§ßÂ∑ÆÂºÇÔºåÁõ¥Êé•ËøõË°åËÅîÂêàÂª∫Ê®°Èù¢‰∏¥ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂÄüÈâ¥LLMÁªü‰∏Ä‰∏çÂêåÊ®°ÊÄÅÁöÑËÉΩÂäõÔºåÂ∞Ü2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ËßÜ‰∏∫Áªü‰∏ÄÁöÑtokensÂ∫èÂàóÔºåÈÄöËøáËá™ÂõûÂΩíÊ®°ÂûãÂ≠¶‰π†ÂÆÉ‰ª¨‰πãÈó¥ÁöÑËÅîÂêàÂàÜÂ∏É„ÄÇÈÄöËøáÁªü‰∏ÄÁöÑÂª∫Ê®°Ê°ÜÊû∂ÔºåÂÆûÁé∞2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ÁöÑÂêåÊ≠•ÁîüÊàêÂíåÁêÜËß£„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöUniMoÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) 2DËßÜÈ¢ëÁºñÁ†ÅÂô®ÔºöÂ∞ÜËßÜÈ¢ëÂ∏ßÁºñÁ†Å‰∏∫ËßÜËßâtokensÂ∫èÂàóÔºõ2) 3DËøêÂä®tokenizerÔºöÂ∞Ü3D‰∫∫‰ΩìËøêÂä®Êï∞ÊçÆÈáèÂåñ‰∏∫ËøêÂä®tokensÂ∫èÂàóÔºõ3) Áªü‰∏ÄÁöÑËá™ÂõûÂΩíÊ®°ÂûãÔºö‰ª•ËßÜËßâtokensÂíåËøêÂä®tokens‰Ωú‰∏∫ËæìÂÖ•ÔºåÂ≠¶‰π†ÂÆÉ‰ª¨ÁöÑËÅîÂêàÂàÜÂ∏ÉÔºåÂÆûÁé∞ÂêåÊ≠•ÁîüÊàêÔºõ4) 3DËøêÂä®Ëß£Á†ÅÂô®ÔºöÂ∞ÜËøêÂä®tokensËß£Á†Å‰∏∫3D‰∫∫‰ΩìËøêÂä®Êï∞ÊçÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö1) Áªü‰∏ÄÂª∫Ê®°Ê°ÜÊû∂ÔºöÈ¶ñÊ¨°Â∞Ü2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®Áªü‰∏ÄÂà∞Âêå‰∏Ä‰∏™Ëá™ÂõûÂΩíÊ®°Âûã‰∏≠ÔºåÂÆûÁé∞ÂêåÊ≠•ÁîüÊàêÂíåÁêÜËß£Ôºõ2) 3DËøêÂä®tokenizerÔºöËÆæËÆ°‰∫Ü‰∏ÄÁßçÂÖ∑ÊúâÊó∂Èó¥Êâ©Â±ïÁ≠ñÁï•ÁöÑÊñ∞Âûã3DËøêÂä®tokenizerÔºå‰ΩøÁî®Âçï‰∏™VQ-VAEÁîüÊàêÈáèÂåñÁöÑËøêÂä®tokensÔºåÂπ∂‰ΩøÁî®Â§ö‰∏™‰∏ìÂÆ∂Ëß£Á†ÅÂô®Â§ÑÁêÜË∫´‰ΩìÂΩ¢Áä∂„ÄÅÂπ≥Áßª„ÄÅÂÖ®Â±ÄÊñπÂêëÂíåË∫´‰ΩìÂßøÂäøÔºå‰ª•ÂÆûÁé∞ÂèØÈù†ÁöÑ3DËøêÂä®ÈáçÂª∫„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö1) ‰ΩøÁî®ÂçïÁã¨ÁöÑÂµåÂÖ•Â±ÇÊù•ÁºìËß£2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®Âú®ÂàÜÂ∏É‰∏äÁöÑÂ∑ÆË∑ùÔºõ2) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ∫èÂàóÂª∫Ê®°Á≠ñÁï•ÔºåÂú®‰∏Ä‰∏™Ê°ÜÊû∂ÂÜÖÈõÜÊàê‰∫ÜËßÜÈ¢ëÁîüÊàêÂíåËøêÂä®ÁîüÊàê‰∏§‰∏™‰ªªÂä°Ôºõ3) 3DËøêÂä®tokenizerÈááÁî®VQ-VAEËøõË°åÈáèÂåñÔºåÂπ∂‰ΩøÁî®Â§ö‰∏™‰∏ìÂÆ∂Ëß£Á†ÅÂô®ÂàÜÂà´Â§ÑÁêÜË∫´‰ΩìÂΩ¢Áä∂„ÄÅÂπ≥Áßª„ÄÅÂÖ®Â±ÄÊñπÂêëÂíåË∫´‰ΩìÂßøÂäø„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

UniMoÂú®ÂêåÊ≠•ÁîüÊàê2DËßÜÈ¢ëÂíå3D‰∫∫‰ΩìËøêÂä®ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniMoËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëÂíåËøêÂä®ÔºåÂπ∂‰∏îËÉΩÂ§üÂáÜÁ°ÆÂú∞ÊçïÊçâ‰∫∫‰ΩìËøêÂä®ÁöÑÁªÜËäÇ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåUniMoÂú®ËøêÂä®ÊçïÊçâÁ≤æÂ∫¶ÂíåÁîüÊàêËßÜÈ¢ëÁöÑÁúüÂÆûÊÄßÊñπÈù¢ÂùáÊúâÊèêÂçá„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

UniMoÂú®ËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÂä®ÁîªÂà∂‰ΩúÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁîüÊàêÈÄºÁúüÁöÑ‰∫∫‰ΩìËøêÂä®ËßÜÈ¢ëÔºå‰πüÂèØ‰ª•Áî®‰∫éÊ†πÊçÆÁªôÂÆöÁöÑËßÜÈ¢ëÁîüÊàêÁõ∏Â∫îÁöÑ3D‰∫∫‰ΩìËøêÂä®„ÄÇÊ≠§Â§ñÔºåUniMoËøòÂèØ‰ª•‰Ωú‰∏∫ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÁªÑÊàêÈÉ®ÂàÜÔºåÁî®‰∫éÊûÑÂª∫Êõ¥Â§çÊùÇÁöÑ‰∫∫Êú∫‰∫§‰∫íÁ≥ªÁªüÔºå‰æãÂ¶ÇÔºåÂèØ‰ª•ÁªìÂêàÊñáÊú¨ÊàñËØ≠Èü≥ËæìÂÖ•Êù•ÊéßÂà∂ËôöÊãüËßíËâ≤ÁöÑËøêÂä®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

