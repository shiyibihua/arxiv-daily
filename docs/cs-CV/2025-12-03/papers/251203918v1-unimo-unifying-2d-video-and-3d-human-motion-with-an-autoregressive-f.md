---
layout: default
title: UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework
---

# UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework

**arXiv**: [2512.03918v1](https://arxiv.org/abs/2512.03918) | [PDF](https://arxiv.org/pdf/2512.03918.pdf)

**ä½œè€…**: Youxin Pang, Yong Zhang, Ruizhi Shao, Xiang Deng, Feng Gao, Xu Xiaoming, Xiaoming Wei, Yebin Liu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniMoä»¥ç»Ÿä¸€å»ºæ¨¡2Dè§†é¢‘ä¸Ž3Däººä½“è¿åŠ¨ï¼Œå®žçŽ°åŒæ—¶ç”Ÿæˆä¸Žç†è§£**

**å…³é”®è¯**: `2Dè§†é¢‘ç”Ÿæˆ` `3Däººä½“è¿åŠ¨` `è‡ªå›žå½’æ¨¡åž‹` `å¤šæ¨¡æ€ç»Ÿä¸€` `è¿åŠ¨æ•æ‰` `ä»¤ç‰Œåºåˆ—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•éš¾ä»¥ç»Ÿä¸€ä¼˜åŒ–å’Œç”Ÿæˆç»“æž„å·®å¼‚å¤§çš„2Dè§†é¢‘ä¸Ž3Dè¿åŠ¨
2. æ–¹æ³•è¦ç‚¹ï¼šå°†è§†é¢‘å’Œè¿åŠ¨å»ºæ¨¡ä¸ºç»Ÿä¸€ä»¤ç‰Œåºåˆ—ï¼Œè®¾è®¡3Dè¿åŠ¨åˆ†è¯å™¨ä¿ç•™ç©ºé—´ä¿¡æ¯
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¯æ˜Žèƒ½åŒæ—¶ç”Ÿæˆå¯¹åº”è§†é¢‘å’Œè¿åŠ¨ï¼Œå¹¶å®žçŽ°å‡†ç¡®è¿åŠ¨æ•æ‰

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

