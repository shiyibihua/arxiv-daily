---
layout: default
title: ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos
---

# ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos

**arXiv**: [2512.03666v1](https://arxiv.org/abs/2512.03666) | [PDF](https://arxiv.org/pdf/2512.03666.pdf)

**ä½œè€…**: Qi'ao Xu, Tianwen Qian, Yuqian Fu, Kailing Li, Yang Jiao, Jiacheng Zhang, Xiaoling Wang, Liang He

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToG-BenchåŸºå‡†ä»¥è§£å†³ç¬¬ä¸€äººç§°è§†é¢‘ä¸­ä»»åŠ¡å¯¼å‘çš„æ—¶ç©ºå®šä½é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ—¶ç©ºè§†é¢‘å®šä½` `ä»»åŠ¡å¯¼å‘åŸºå‡†` `ç¬¬ä¸€äººç§°è§†é¢‘` `å¤šæ¨¡æ€å¤§æ¨¡åž‹è¯„ä¼°` `æ˜¾éšå¯¹è±¡æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ—¶ç©ºè§†é¢‘å®šä½ç ”ç©¶å±€é™äºŽå¯¹è±¡ä¸­å¿ƒæè¿°ï¼Œç¼ºä¹ä»»åŠ¡å¯¼å‘æŽ¨ç†ï¼Œé˜»ç¢å…·èº«æ™ºèƒ½äº¤äº’ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæž„å»ºé¦–ä¸ªä»»åŠ¡å¯¼å‘æ—¶ç©ºå®šä½åŸºå‡†ï¼ŒåŒ…å«ä»»åŠ¡å¯¼å‘ã€æ˜¾éšåŒé‡å’Œä¸€å¯¹å¤šå®šä½ç‰¹å¾ï¼ŒåŸºäºŽScanNetè§†é¢‘åŠè‡ªåŠ¨æ ‡æ³¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°ä¸ƒç§å…ˆè¿›å¤šæ¨¡æ€å¤§æ¨¡åž‹ï¼Œæ­ç¤ºä»»åŠ¡å¯¼å‘æ—¶ç©ºå®šä½çš„æŒ‘æˆ˜åŠæ˜¾éšä¸Žå¤šå¯¹è±¡å®šä½æ€§èƒ½å·®è·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..

