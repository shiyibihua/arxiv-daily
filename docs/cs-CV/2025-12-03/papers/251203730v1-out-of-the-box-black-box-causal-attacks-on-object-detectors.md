---
layout: default
title: Out-of-the-box: Black-box Causal Attacks on Object Detectors
---

# Out-of-the-box: Black-box Causal Attacks on Object Detectors

**arXiv**: [2512.03730v1](https://arxiv.org/abs/2512.03730) | [PDF](https://arxiv.org/pdf/2512.03730.pdf)

**ä½œè€…**: Melane Navaratnarajah, David A. Kelly, Hana Chockler

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBlackCAtté»‘ç›’å› æžœæ”»å‡»ç®—æ³•ï¼Œåˆ©ç”¨å› æžœåƒç´ é›†å¯¹ç›®æ ‡æ£€æµ‹å™¨è¿›è¡Œå¯è§£é‡Šã€ä¸å¯æ„ŸçŸ¥çš„æ”»å‡»ã€‚**

**å…³é”®è¯**: `é»‘ç›’æ”»å‡»` `ç›®æ ‡æ£€æµ‹å™¨` `å› æžœåƒç´ ` `å¯¹æŠ—æ‰°åŠ¨` `å¯è§£é‡Šæ€§` `æž¶æž„æ— å…³`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¯¹æŠ—æ‰°åŠ¨æ–¹æ³•å¤šä¸ºç™½ç›’ä¸”æž¶æž„ç‰¹å®šï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥åˆ†æžæ”»å‡»æœºåˆ¶ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šBlackCAttç»“åˆå› æžœåƒç´ å’Œç›®æ ‡æ£€æµ‹å™¨çš„è¾¹ç•Œæ¡†ï¼Œæž„å»ºé»‘ç›’ã€æž¶æž„æ— å…³çš„æ”»å‡»ï¼Œå¯¼è‡´æ£€æµ‹æ¡†ä¸¢å¤±ã€ä¿®æ”¹æˆ–æ–°å¢žã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨COCOæ•°æ®é›†ä¸Šï¼Œæ”»å‡»æ•ˆæžœä¼˜äºŽåŸºçº¿ï¼Œç§»é™¤æ£€æµ‹æå‡2.7å€ï¼Œä¿®æ”¹æ£€æµ‹æå‡3.86å€ï¼Œè§¦å‘è™šå‡æ£€æµ‹æå‡5.75å€ï¼Œæ”»å‡»æŽ¥è¿‘åŽŸå›¾ä¸å¯æ„ŸçŸ¥ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.

