---
layout: default
title: Grokked Models are Better Unlearners
---

# Grokked Models are Better Unlearners

**arXiv**: [2512.03437v1](https://arxiv.org/abs/2512.03437) | [PDF](https://arxiv.org/pdf/2512.03437.pdf)

**ä½œè€…**: Yuanbang Liang, Yang Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å‘çŽ°Grokkedæ¨¡åž‹ä½œä¸ºèµ·ç‚¹èƒ½æå‡æœºå™¨é—å¿˜æ•ˆçŽ‡ä¸Žç¨³å®šæ€§ï¼Œæ— éœ€ä¿®æ”¹ç®—æ³•ã€‚**

**å…³é”®è¯**: `æœºå™¨é—å¿˜` `Grokking` `æ¨¡åž‹é²æ£’æ€§` `é€‰æ‹©æ€§é—å¿˜` `ç‰¹å¾æ¨¡å—åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šGrokkingè®­ç»ƒæœºåˆ¶æ˜¯å¦æœ‰åŠ©äºŽæœºå™¨é—å¿˜ï¼Œå³é€‰æ‹©æ€§ç§»é™¤æ•°æ®å½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæ¯”è¾ƒæ ‡å‡†é—å¿˜æ–¹æ³•åœ¨Grokkingè¿‡æ¸¡å‰åŽçš„åº”ç”¨ï¼Œåˆ†æžç‰¹å¾ä¸Žæ›²çŽ‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šGrokkedæ£€æŸ¥ç‚¹å¸¦æ¥æ›´é«˜æ•ˆé—å¿˜ã€æ›´å°‘å‰¯ä½œç”¨å’Œæ›´ç¨³å®šæ›´æ–°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

