---
layout: default
title: Stable Signer: Hierarchical Sign Language Generative Model
---

# Stable Signer: Hierarchical Sign Language Generative Model

**arXiv**: [2512.04048v1](https://arxiv.org/abs/2512.04048) | [PDF](https://arxiv.org/pdf/2512.04048.pdf)

**ä½œè€…**: Sen Fang, Yalin Feng, Hongbin Zhong, Yanxin Zhang, Dimitris N. Metaxas

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStable Signeræ¨¡åž‹ï¼Œé€šè¿‡ç®€åŒ–ä»»åŠ¡ç»“æž„å’Œå¼•å…¥æ–°æ¨¡å—ï¼Œç«¯åˆ°ç«¯ç”Ÿæˆé«˜è´¨é‡å¤šé£Žæ ¼æ‰‹è¯­è§†é¢‘ã€‚**

**å…³é”®è¯**: `æ‰‹è¯­ç”Ÿæˆ` `ç«¯åˆ°ç«¯æ¨¡åž‹` `åˆ†å±‚ç”Ÿæˆ` `æ‰‹åŠ¿æ¸²æŸ“` `è¯­ä¹‰æ„ŸçŸ¥æŸå¤±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿæ‰‹è¯­ç”Ÿæˆæµç¨‹å†—ä½™ï¼Œå¯¼è‡´æ–‡æœ¬è½¬æ¢ã€å§¿æ€ç”Ÿæˆå’Œè§†é¢‘æ¸²æŸ“è¯¯å·®ç´¯ç§¯ï¼Œè¿›å±•ç¼“æ…¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåˆ†å±‚ç”Ÿæˆï¼Œä»…åŒ…æ‹¬æ–‡æœ¬ç†è§£å’ŒPose2Vidï¼Œä½¿ç”¨SLULè¿›è¡Œæ–‡æœ¬ç†è§£ï¼ŒSLP-MoEå—æ¸²æŸ“æ‰‹åŠ¿ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæ€§èƒ½ç›¸æ¯”å½“å‰SOTAæ–¹æ³•æå‡48.6%ï¼Œé€šè¿‡SAGMæŸå¤±è®­ç»ƒSLULï¼Œç”Ÿæˆé«˜è´¨é‡å¤šé£Žæ ¼è§†é¢‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.

