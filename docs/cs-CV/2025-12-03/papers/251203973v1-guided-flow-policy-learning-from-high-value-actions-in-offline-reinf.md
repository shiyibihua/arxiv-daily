---
layout: default
title: Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning
---

# Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning

**arXiv**: [2512.03973v1](https://arxiv.org/abs/2512.03973) | [PDF](https://arxiv.org/pdf/2512.03973.pdf)

**ä½œè€…**: Franki Nguimatsia Tiofack, ThÃ©otime Le Hellard, Fabian Schramm, Nicolas Perrin-Gilbert, Justin Carpentier

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼•å¯¼æµç­–ç•¥ä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­è¡Œä¸ºæ­£åˆ™åŒ–æ— æ³•åŒºåˆ†é«˜ä»·å€¼åŠ¨ä½œçš„é—®é¢˜**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `è¡Œä¸ºæ­£åˆ™åŒ–` `æµåŒ¹é…ç­–ç•¥` `åŠ æƒè¡Œä¸ºå…‹éš†` `é«˜ä»·å€¼åŠ¨ä½œå­¦ä¹ ` `å¤šæ­¥ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„è¡Œä¸ºæ­£åˆ™åŒ–æ–¹æ³•å¯¹æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹è¿›è¡Œæ— å·®åˆ«æ¨¡ä»¿ï¼ŒæœªåŒºåˆ†é«˜ä»·å€¼å’Œä½Žä»·å€¼åŠ¨ä½œã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆå¤šæ­¥æµåŒ¹é…ç­–ç•¥å’Œè’¸é¦å•æ­¥æ¼”å‘˜ï¼Œé€šè¿‡åŠ æƒè¡Œä¸ºå…‹éš†å¼•å¯¼æµç­–ç•¥ä¸“æ³¨äºŽå…‹éš†æ•°æ®é›†ä¸­çš„é«˜ä»·å€¼åŠ¨ä½œã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨OGBenchã€Minariå’ŒD4RLåŸºå‡†çš„144ä¸ªä»»åŠ¡ä¸­å®žçŽ°å…ˆè¿›æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ¬¡ä¼˜æ•°æ®é›†å’ŒæŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

