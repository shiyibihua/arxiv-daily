---
layout: default
title: RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL
---

# RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL

**arXiv**: [2512.03556v1](https://arxiv.org/abs/2512.03556) | [PDF](https://arxiv.org/pdf/2512.03556.pdf)

**ä½œè€…**: Yinzhou Tang, Yu Shang, Yinuo Chen, Bingwen Wei, Xin Zhang, Shu'ang Yu, Liangzhi Shi, Chao Yu, Chen Gao, Wei Wu, Yong Li

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboScape-Ræ¡†æž¶ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡åž‹ä½œä¸ºé€šç”¨çŽ¯å¢ƒä»£ç†ä»¥å¢žå¼ºæœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººç­–ç•¥` `æ³›åŒ–è®­ç»ƒ` `å†…ç”Ÿå¥–åŠ±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç¼ºä¹ç»Ÿä¸€å¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´ç­–ç•¥åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå¼•å…¥åŸºäºŽä¸–ç•Œæ¨¡åž‹çš„å†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼Œä»ŽçŠ¶æ€è½¬ç§»åŠ¨æ€ä¸­ç”Ÿæˆé€šç”¨å¥–åŠ±ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨åŸŸå¤–åœºæ™¯ä¸‹å¹³å‡æ€§èƒ½æå‡37.5%ï¼ŒéªŒè¯äº†æ¡†æž¶çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–ä¼˜åŠ¿ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.

