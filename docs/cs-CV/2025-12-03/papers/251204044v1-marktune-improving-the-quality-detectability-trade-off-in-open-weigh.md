---
layout: default
title: MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking
---

# MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking

**arXiv**: [2512.04044v1](https://arxiv.org/abs/2512.04044) | [PDF](https://arxiv.org/pdf/2512.04044.pdf)

**ä½œè€…**: Yizhou Zhao, Zhiwei Steven Wu, Adam Block

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMarkTuneæ¡†æž¶ä»¥æ”¹å–„å¼€æºå¤§è¯­è¨€æ¨¡åž‹æ°´å°çš„è´¨é‡ä¸Žå¯æ£€æµ‹æ€§æƒè¡¡**

**å…³é”®è¯**: `å¼€æºå¤§è¯­è¨€æ¨¡åž‹` `æ°´å°æŠ€æœ¯` `ç­–ç•¥å¾®è°ƒ` `è´¨é‡æ£€æµ‹æƒè¡¡` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¼€æºæ¨¡åž‹æ°´å°éœ€å¹³è¡¡æ£€æµ‹èƒ½åŠ›ä¸Žæ–‡æœ¬è´¨é‡ï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚GaussMarkå­˜åœ¨è´¨é‡ä¸‹é™é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽç†è®ºæŽ¨å¯¼ï¼Œé‡‡ç”¨ç­–ç•¥å¾®è°ƒæ¡†æž¶ï¼Œå°†æ°´å°ä¿¡å·ä½œä¸ºå¥–åŠ±å¹¶æ­£åˆ™åŒ–ä»¥ä¿æŒè´¨é‡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šæå‡GaussMarkçš„æƒè¡¡æ›²çº¿ï¼ŒæŽ¥è¿‘æŽ¨ç†æ—¶æ°´å°æ€§èƒ½ï¼Œå¹¶å±•ç¤ºæŠ—æ”»å‡»å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

