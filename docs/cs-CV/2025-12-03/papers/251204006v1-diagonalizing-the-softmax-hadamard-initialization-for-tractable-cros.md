---
layout: default
title: Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics
---

# Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics

**arXiv**: [2512.04006v1](https://arxiv.org/abs/2512.04006) | [PDF](https://arxiv.org/pdf/2512.04006.pdf)

**ä½œè€…**: Connall Garrod, Jonathan P. Keating, Christos Thrampoulidis

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHadamardåˆå§‹åŒ–ä»¥åˆ†æžäº¤å‰ç†µè®­ç»ƒåŠ¨æ€ï¼Œè¯æ˜Žæ¢¯åº¦æµæ”¶æ•›åˆ°ç¥žç»å´©æºƒå‡ ä½•**

**å…³é”®è¯**: `äº¤å‰ç†µè®­ç»ƒ` `éžå‡¸ä¼˜åŒ–` `ç¥žç»å´©æºƒ` `æ¢¯åº¦æµ` `Hadamardåˆå§‹åŒ–` `ä¸¤å±‚çº¿æ€§ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šäº¤å‰ç†µè®­ç»ƒåŠ¨æ€ç¼ºä¹éžå‡¸ç†è®ºåˆ†æžï¼ŒçŽ°æœ‰ç®€åŒ–æ–¹æ³•å¿½ç•¥å…¶ä¸Žå¹³æ–¹æŸå¤±çš„æœ¬è´¨å·®å¼‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡Hadamardåˆå§‹åŒ–å¯¹è§’åŒ–softmaxç®—å­ï¼Œç®€åŒ–ä¸¤å±‚çº¿æ€§ç¥žç»ç½‘ç»œåŠ¨æ€åˆ†æž
3. å®žéªŒæˆ–æ•ˆæžœï¼šæž„å»ºLyapunovå‡½æ•°è¯æ˜Žå…¨å±€æ”¶æ•›ï¼Œé¦–æ¬¡ç†è®ºè¯å®žç¥žç»å´©æºƒå‡ ä½•çš„æ”¶æ•›æ€§

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.

