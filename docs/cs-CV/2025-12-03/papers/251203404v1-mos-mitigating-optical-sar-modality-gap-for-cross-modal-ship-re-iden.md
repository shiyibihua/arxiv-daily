---
layout: default
title: MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification
---

# MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification

**arXiv**: [2512.03404v1](https://arxiv.org/abs/2512.03404) | [PDF](https://arxiv.org/pdf/2512.03404.pdf)

**ä½œè€…**: Yujian Zhao, Hankun Liu, Guanglin Niu

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMOSæ¡†æž¶ä»¥ç¼“è§£å…‰å­¦ä¸ŽSARæ¨¡æ€å·®å¼‚ï¼Œå®žçŽ°è·¨æ¨¡æ€èˆ¹èˆ¶é‡è¯†åˆ«**

**å…³é”®è¯**: `è·¨æ¨¡æ€é‡è¯†åˆ«` `å…‰å­¦-SARæ¨¡æ€å¯¹é½` `èˆ¹èˆ¶è¯†åˆ«` `æ‰©æ•£æ¨¡åž‹` `ç‰¹å¾èžåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå…‰å­¦ä¸ŽSARå›¾åƒé—´æ¨¡æ€å·®å¼‚å¤§ï¼Œé˜»ç¢è·¨æ¨¡æ€èˆ¹èˆ¶é‡è¯†åˆ«ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆæ¨¡æ€ä¸€è‡´è¡¨ç¤ºå­¦ä¹ å’Œè·¨æ¨¡æ€æ•°æ®ç”Ÿæˆä¸Žç‰¹å¾èžåˆï¼Œå¯¹é½ç‰¹å¾åˆ†å¸ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨HOSSæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ï¼ŒR1å‡†ç¡®çŽ‡æå‡æœ€é«˜è¾¾16.4%ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.

