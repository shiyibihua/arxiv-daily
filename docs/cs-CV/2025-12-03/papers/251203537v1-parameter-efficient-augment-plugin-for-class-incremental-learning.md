---
layout: default
title: Parameter-Efficient Augment Plugin for Class-Incremental Learning
---

# Parameter-Efficient Augment Plugin for Class-Incremental Learning

**arXiv**: [2512.03537v1](https://arxiv.org/abs/2512.03537) | [PDF](https://arxiv.org/pdf/2512.03537.pdf)

**ä½œè€…**: Zhiming Xu, Baile Xu, Jian Zhao, Furao Shen, Suorong Yang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDLCæ’ä»¶èŒƒå¼ï¼Œé€šè¿‡LoRAç»„ä»¶å¢žå¼ºéžé¢„è®­ç»ƒç±»å¢žé‡å­¦ä¹ ï¼Œæå‡æ•ˆçŽ‡ä¸Žå‡†ç¡®æ€§ã€‚**

**å…³é”®è¯**: `ç±»å¢žé‡å­¦ä¹ ` `å‚æ•°é«˜æ•ˆ` `LoRAé€‚é…` `æ’ä»¶æ‰©å±•` `éžé¢„è®­ç»ƒåœºæ™¯` `ç‰¹å¾èšåˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç±»å¢žé‡å­¦ä¹ æ–¹æ³•å—é—å¿˜æˆ–ç¨³å®šæ€§-å¯å¡‘æ€§å›°å¢ƒé™åˆ¶ï¼Œæ‰©å±•æ–¹æ³•å‚æ•°å¼€é”€å¤§ã€‚
2. DLCä½¿ç”¨LoRAæ³¨å…¥ä»»åŠ¡ç‰¹å®šæ®‹å·®åˆ°åŸºç¡€æ¨¡åž‹ï¼Œå¹¶å¼•å…¥è½»é‡åŠ æƒå•å…ƒå‡å°‘å¹²æ‰°ã€‚
3. åœ¨ImageNet-100ä¸Šï¼Œä»…ç”¨4%å‚æ•°å®žçŽ°8%å‡†ç¡®çŽ‡æå‡ï¼Œè¶…è¶Šå›ºå®šå†…å­˜é¢„ç®—ä¸‹å…ˆè¿›æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

