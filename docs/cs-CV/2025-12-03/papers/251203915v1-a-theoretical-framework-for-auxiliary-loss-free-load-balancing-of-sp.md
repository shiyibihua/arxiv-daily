---
layout: default
title: A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models
---

# A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models

**arXiv**: [2512.03915v1](https://arxiv.org/abs/2512.03915) | [PDF](https://arxiv.org/pdf/2512.03915.pdf)

**ä½œè€…**: X. Y. Han, Yuan Zhong

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç†è®ºæ¡†æž¶åˆ†æžæ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡ï¼Œç”¨äºŽå¤§è§„æ¨¡AIæ¨¡åž‹ä¸­ç¨€ç–ä¸“å®¶æ··åˆçš„è´Ÿè½½å¹³è¡¡ã€‚**

**å…³é”®è¯**: `ç¨€ç–ä¸“å®¶æ··åˆ` `è´Ÿè½½å‡è¡¡` `åŽŸå§‹-å¯¹å¶æ–¹æ³•` `åœ¨çº¿ä¼˜åŒ–` `ç†è®ºåˆ†æž` `å¤§è§„æ¨¡AIè®­ç»ƒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç¨€ç–ä¸“å®¶æ··åˆå±‚ä¸­è´Ÿè½½ä¸å‡è¡¡å¯¼è‡´GPUåˆ©ç”¨çŽ‡ä½Žï¼Œéœ€ä¼˜åŒ–ä»¤ç‰Œè·¯ç”±ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šå°†æ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡å»ºæ¨¡ä¸ºåŽŸå§‹-å¯¹å¶æ–¹æ³•ï¼Œåˆ†æžå•è°ƒæ”¹è¿›ã€åå¥½è§„åˆ™å’Œè¿‘ä¼¼å¹³è¡¡ä¿è¯ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨1Bå‚æ•°DeepSeekMoEæ¨¡åž‹ä¸Šè¿›è¡Œå®žéªŒï¼ŒéªŒè¯ç†è®ºæ¡†æž¶çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.

