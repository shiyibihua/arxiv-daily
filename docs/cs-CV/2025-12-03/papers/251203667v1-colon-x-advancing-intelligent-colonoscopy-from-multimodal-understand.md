---
layout: default
title: Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning
---

# Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning

**arXiv**: [2512.03667v1](https://arxiv.org/abs/2512.03667) | [PDF](https://arxiv.org/pdf/2512.03667.pdf)

**ä½œè€…**: Ge-Peng Ji, Jingyi Liu, Deng-Ping Fan, Nick Barnes

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºColon-Xä»¥æŽ¨è¿›ç»“è‚ é•œä»Žå¤šæ¨¡æ€ç†è§£åˆ°ä¸´åºŠæŽ¨ç†çš„æ™ºèƒ½åˆ†æž**

**å…³é”®è¯**: `ç»“è‚ é•œå¤šæ¨¡æ€ç†è§£` `ä¸´åºŠæŽ¨ç†æ•°æ®é›†` `è§†è§‰é—®ç­”` `ä»»åŠ¡è‡ªé€‚åº”å¥–åŠ±` `æ¢¯åº¦ç¨³å®šä¼˜åŒ–` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æž„å»ºColonVQAæ•°æ®é›†ï¼ŒåŒ…å«110ä¸‡+è§†è§‰é—®ç­”æ¡ç›®ï¼Œè¦†ç›–76ç§ä¸´åºŠå‘çŽ°å’Œ18ä¸ªå¤šæ¨¡æ€ä»»åŠ¡
2. è¯„ä¼°22ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œå‘çŽ°å…¶ä¸´åºŠè¾“å‡ºåœ¨æ‰°åŠ¨ä¸‹å¯é æ€§ä¸è¶³ï¼Œéœ€æå‡ç¨³å¥æ€§
3. å¼€å‘ColonR1æ¨¡åž‹ï¼Œé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”å¥–åŠ±å’Œæ¢¯åº¦ç¨³å®šä¼˜åŒ–ï¼Œåœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹å‡†ç¡®çŽ‡è¾¾56.61%ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒ25.22%

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.

