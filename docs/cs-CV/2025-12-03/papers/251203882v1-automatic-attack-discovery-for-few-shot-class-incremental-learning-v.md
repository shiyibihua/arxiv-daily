---
layout: default
title: Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models
---

# Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models

**arXiv**: [2512.03882v1](https://arxiv.org/abs/2512.03882) | [PDF](https://arxiv.org/pdf/2512.03882.pdf)

**ä½œè€…**: Haidong Kang, Wei Wu, Hanling Wang

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºACraftæ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹è‡ªåŠ¨å‘çŽ°é’ˆå¯¹å°‘æ ·æœ¬ç±»å¢žé‡å­¦ä¹ çš„æ”»å‡»æ–¹æ³•**

**å…³é”®è¯**: `å°‘æ ·æœ¬ç±»å¢žé‡å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡åž‹` `è‡ªåŠ¨æ”»å‡»å‘çŽ°` `å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨è¯„ä¼°`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå°‘æ ·æœ¬ç±»å¢žé‡å­¦ä¹ çš„å®‰å…¨é—®é¢˜æœªå—å……åˆ†å…³æ³¨ï¼ŒçŽ°æœ‰æ”»å‡»æ–¹æ³•æ•ˆæžœæœ‰é™æˆ–æˆæœ¬é«˜
2. æ–¹æ³•è¦ç‚¹ï¼šåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹è‡ªåŠ¨ç”Ÿæˆæ”»å‡»æ–¹æ³•ï¼Œç»“åˆPPOå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ä¸»æµåŸºå‡†ä¸Šæ˜¾è‘—é™ä½Žå…ˆè¿›FSCILæ–¹æ³•æ€§èƒ½ï¼Œè¶…è¶Šäººå·¥æ”»å‡»æ–¹æ³•ä¸”æˆæœ¬æœ€ä½Ž

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.

