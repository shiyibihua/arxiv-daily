---
layout: default
title: Jina-VLM: Small Multilingual Vision Language Model
---

# Jina-VLM: Small Multilingual Vision Language Model

**arXiv**: [2512.04032v1](https://arxiv.org/abs/2512.04032) | [PDF](https://arxiv.org/pdf/2512.04032.pdf)

**ä½œè€…**: Andreas Koukounas, Georgios Mastrapas, Florian HÃ¶nicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJina-VLMï¼Œä¸€ç§2.4Bå‚æ•°å¤šè¯­è¨€è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œåœ¨å¼€æ”¾2Bè§„æ¨¡æ¨¡åž‹ä¸­å®žçŽ°æœ€å…ˆè¿›çš„å¤šè¯­è¨€è§†è§‰é—®ç­”ã€‚**

**å…³é”®è¯**: `å¤šè¯­è¨€è§†è§‰é—®ç­”` `å°è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡åž‹` `æ³¨æ„åŠ›æ± åŒ–è¿žæŽ¥å™¨` `ä»»æ„åˆ†è¾¨çŽ‡å›¾åƒå¤„ç†` `SigLIP2è§†è§‰ç¼–ç å™¨` `Qwen3è¯­è¨€ä¸»å¹²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè§£å†³å¤šè¯­è¨€è§†è§‰é—®ç­”ä¸­æ¨¡åž‹è§„æ¨¡ä¸Žæ€§èƒ½çš„å¹³è¡¡é—®é¢˜ï¼Œæå‡å°è§„æ¨¡æ¨¡åž‹åœ¨å¤šè¯­è¨€åœºæ™¯ä¸‹çš„è¡¨çŽ°ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆSigLIP2è§†è§‰ç¼–ç å™¨å’ŒQwen3è¯­è¨€ä¸»å¹²ï¼Œé€šè¿‡æ³¨æ„åŠ›æ± åŒ–è¿žæŽ¥å™¨å®žçŽ°ä»»æ„åˆ†è¾¨çŽ‡å›¾åƒçš„é«˜æ•ˆå¤„ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨æ ‡å‡†VQAåŸºå‡†å’Œå¤šè¯­è¨€è¯„ä¼°ä¸­è¶…è¶Šå¯æ¯”æ¨¡åž‹ï¼ŒåŒæ—¶ä¿æŒç«žäº‰åŠ›çš„çº¯æ–‡æœ¬æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.

