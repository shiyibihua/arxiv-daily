---
layout: default
title: Convergence for Discrete Parameter Updates
---

# Convergence for Discrete Parameter Updates

**arXiv**: [2512.04051v1](https://arxiv.org/abs/2512.04051) | [PDF](https://arxiv.org/pdf/2512.04051.pdf)

**ä½œè€…**: Paul Wilson, Fabio Zanasi, George Constantinides

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»æ•£å‚æ•°æ›´æ–°æ–¹æ³•ä»¥è§£å†³ä½Žç²¾åº¦è®­ç»ƒä¸­çš„é‡åŒ–é—®é¢˜**

**å…³é”®è¯**: `ä½Žç²¾åº¦è®­ç»ƒ` `ç¦»æ•£å‚æ•°æ›´æ–°` `æ”¶æ•›ä¿è¯` `å¤šé¡¹æ›´æ–°è§„åˆ™` `é«˜æ•ˆè®­ç»ƒ` `æ·±åº¦å­¦ä¹ ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°ä»£æ·±åº¦å­¦ä¹ æ¨¡åž‹è®¡ç®—éœ€æ±‚å¤§ï¼Œä½Žç²¾åº¦è®­ç»ƒä¾èµ–è¿žç»­æ›´æ–°çš„é‡åŒ–ï¼Œå¯èƒ½å¼•å…¥è¯¯å·®
2. æ–¹æ³•è¦ç‚¹ï¼šè®¾è®¡æ›´æ–°è§„åˆ™æœ¬èº«ä¸ºç¦»æ•£ï¼Œé¿å…é‡åŒ–è¿žç»­æ›´æ–°ï¼Œæä¾›æ”¶æ•›ä¿è¯ï¼Œå¹¶ä»¥å¤šé¡¹æ›´æ–°ä¸ºä¾‹
3. å®žéªŒæˆ–æ•ˆæžœï¼šé€šè¿‡å®žè¯è¯„ä¼°æ”¯æŒç¦»æ•£æ›´æ–°è§„åˆ™ï¼Œä¸ºé«˜æ•ˆè®­ç»ƒå¼€è¾Ÿæ–°é€”å¾„ï¼Œå°¤å…¶é€‚ç”¨äºŽç¦»æ•£ç»“æž„æ¨¡åž‹

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

