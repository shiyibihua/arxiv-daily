---
layout: default
title: Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks
---

# Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16407" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16407v1</a>
  <a href="https://arxiv.org/pdf/2506.16407.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16407v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16407v1', 'Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dong Nguyen Tien, Dung D. Le

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: 8 pages, 1 figure, under review at EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥è¯„ä¼°OCRåŸºç¡€è§†è§‰æ–‡æ¡£ç†è§£çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰æ–‡æ¡£ç†è§£` `å¯¹æŠ—æ”»å‡»` `OCRæŠ€æœ¯` `å¤šæ¨¡æ€èåˆ` `é²æ£’æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æ–‡æ¡£ç†è§£ç³»ç»Ÿåœ¨é¢å¯¹ç°å®ä¸–ç•Œä¸­çš„å¯¹æŠ—æ‰°åŠ¨æ—¶ï¼Œé²æ£’æ€§ä¸è¶³ï¼Œå½±å“ä¿¡æ¯æå–çš„å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå’Œè¯„ä¼°å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ï¼Œæ¶µç›–å¤šç§å¸ƒå±€æ”»å‡»åœºæ™¯ï¼Œç¡®ä¿æ‰°åŠ¨çš„åˆç†æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¡Œçº§æ”»å‡»å’Œå¤åˆæ‰°åŠ¨å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æœ€ä¸ºæ˜¾è‘—ï¼ŒPGDæ–¹æ³•åœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰æ–‡æ¡£ç†è§£ï¼ˆVDUï¼‰ç³»ç»Ÿé€šè¿‡æ•´åˆæ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰ä¿¡å·åœ¨ä¿¡æ¯æå–æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç°å®å¯¹æŠ—æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°é’ˆå¯¹OCRåŸºç¡€VDUæ¨¡å‹çš„å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ã€‚è¯¥æ–¹æ³•æ¶µç›–å…­ç§åŸºäºæ¢¯åº¦çš„å¸ƒå±€æ”»å‡»åœºæ™¯ï¼Œæ¶‰åŠOCRè¾¹ç•Œæ¡†ã€åƒç´ å’Œæ–‡æœ¬çš„æ“æ§ï¼Œå¹¶åœ¨å¸ƒå±€æ‰°åŠ¨é¢„ç®—ï¼ˆå¦‚IoU >= 0.6ï¼‰ä¸‹ä¿æŒåˆç†æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¡Œçº§æ”»å‡»å’Œå¤åˆæ‰°åŠ¨ï¼ˆè¾¹ç•Œæ¡† + åƒç´  + æ–‡æœ¬ï¼‰å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åŸºäºæŠ•å½±æ¢¯åº¦ä¸‹é™ï¼ˆPGDï¼‰çš„è¾¹ç•Œæ¡†æ‰°åŠ¨åœ¨æ‰€æœ‰æ¨¡å‹ä¸­å‡ä¼˜äºéšæœºåç§»åŸºçº¿ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å¸ƒå±€é¢„ç®—ã€æ–‡æœ¬ä¿®æ”¹å’Œå¯¹æŠ—å¯è½¬ç§»æ€§çš„å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³OCRåŸºç¡€è§†è§‰æ–‡æ¡£ç†è§£æ¨¡å‹åœ¨é¢å¯¹å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»æ—¶çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘å¯¹æŠ—æ‰°åŠ¨å¯¹ä¿¡æ¯æå–çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ç»Ÿä¸€æ¡†æ¶é€šè¿‡ç”Ÿæˆå¤šç§å¯¹æŠ—æ”»å‡»ï¼Œè¯„ä¼°å…¶å¯¹VDUæ¨¡å‹çš„å½±å“ï¼Œç¡®ä¿æ”»å‡»çš„åˆç†æ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹å¸ƒå±€ã€æ–‡æœ¬å’Œåƒç´ çš„ç»¼åˆæ“æ§ï¼Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬æ”»å‡»ç”Ÿæˆæ¨¡å—å’Œè¯„ä¼°æ¨¡å—ã€‚æ”»å‡»ç”Ÿæˆæ¨¡å—è´Ÿè´£åˆ›å»ºä¸åŒç±»å‹çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œè€Œè¯„ä¼°æ¨¡å—åˆ™é€šè¿‡å®éªŒéªŒè¯æ¨¡å‹åœ¨è¿™äº›æ‰°åŠ¨ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»åº”ç”¨äºOCRåŸºç¡€VDUæ¨¡å‹ï¼Œå¹¶é€šè¿‡å¸ƒå±€é¢„ç®—çº¦æŸç¡®ä¿æ‰°åŠ¨çš„åˆç†æ€§ï¼ŒåŒºåˆ«äºä»¥å¾€çš„å•ä¸€æ”»å‡»æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®¾ç½®äº†å¸ƒå±€æ‰°åŠ¨é¢„ç®—ï¼ˆå¦‚IoU >= 0.6ï¼‰ï¼Œå¹¶é‡‡ç”¨äº†åŸºäºPGDçš„è¾¹ç•Œæ¡†æ‰°åŠ¨æ–¹æ³•ï¼Œç¡®ä¿äº†æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œå¯è½¬ç§»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¡Œçº§æ”»å‡»å’Œå¤åˆæ‰°åŠ¨ï¼ˆè¾¹ç•Œæ¡† + åƒç´  + æ–‡æœ¬ï¼‰å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯PGDæ–¹æ³•åœ¨æ‰€æœ‰æ¨¡å‹ä¸­å‡ä¼˜äºéšæœºåç§»åŸºçº¿ï¼ŒéªŒè¯äº†å¸ƒå±€é¢„ç®—å’Œæ–‡æœ¬ä¿®æ”¹çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–‡æ¡£è‡ªåŠ¨åŒ–å¤„ç†ã€ä¿¡æ¯æå–å’Œæ™ºèƒ½æ–‡æ¡£åˆ†æç­‰ã€‚é€šè¿‡æé«˜VDUç³»ç»Ÿåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æ–‡æ¡£å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Document Understanding (VDU) systems have achieved strong performance in information extraction by integrating textual, layout, and visual signals. However, their robustness under realistic adversarial perturbations remains insufficiently explored. We introduce the first unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based VDU models. Our method covers six gradient-based layout attack scenarios, incorporating manipulations of OCR bounding boxes, pixels, and texts across both word and line granularities, with constraints on layout perturbation budget (e.g., IoU >= 0.6) to preserve plausibility.
>   Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and six model families demonstrate that line-level attacks and compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation. Projected Gradient Descent (PGD)-based BBox perturbations outperform random-shift baselines in all investigated models. Ablation studies further validate the impact of layout budget, text modification, and adversarial transferability.

