---
layout: default
title: EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training
---

# EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16017" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16017v1</a>
  <a href="https://arxiv.org/pdf/2506.16017.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16017v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16017v1', 'EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liangjing Shao, Linxin Bai, Chenkang Du, Xinrong Chen

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Accepted by IROS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/BaymaxShao/EndoMUST)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEndoMUSTä»¥è§£å†³æœºå™¨äººå†…çª¥é•œä¸­çš„å•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `è‡ªæˆ‘ç›‘ç£å­¦ä¹ ` `æœºå™¨äººå†…çª¥é•œ` `å…‰æµé…å‡†` `å¤šå°ºåº¦å›¾åƒåˆ†è§£` `æ·±åº¦å­¦ä¹ ` `å›¾åƒå¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å†…çª¥é•œåœºæ™¯ä¸­é¢ä¸´å…‰ç…§å˜åŒ–å’Œç¨€ç–çº¹ç†çš„æŒ‘æˆ˜ï¼Œå½±å“æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ­¥é«˜æ•ˆå¾®è°ƒçš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†æ­¥éª¤è®­ç»ƒç›¸å…³ç½‘ç»œï¼Œå‡å°‘ä¿¡æ¯å¹²æ‰°ã€‚
3. åœ¨SCAREDæ•°æ®é›†ä¸Šï¼Œæ‰€ææ–¹æ³•å®ç°äº†è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡çš„æœ€æ–°æ€§èƒ½ï¼Œè¯¯å·®é™ä½4%è‡³10%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å•ç›®æ·±åº¦ä¼°è®¡å’Œè‡ªæˆ‘è¿åŠ¨ä¼°è®¡æ˜¯æœºå™¨äººè¾…åŠ©å†…çª¥é•œä¸­åœºæ™¯æ„ŸçŸ¥å’Œå¯¼èˆªçš„é‡è¦ä»»åŠ¡ã€‚ä¸ºåº”å¯¹å†…çª¥é•œåœºæ™¯ä¸­çš„å…‰ç…§å˜åŒ–å’Œç¨€ç–çº¹ç†ï¼Œç°æœ‰æ–¹æ³•å¼•å…¥äº†å…‰æµã€å¤–è§‚æµå’Œå†…åœ¨å›¾åƒåˆ†è§£ç­‰å¤šç§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œé’ˆå¯¹å¤šä¸ªæ¨¡å—çš„æœ‰æ•ˆè®­ç»ƒç­–ç•¥ä»ç„¶æ˜¯è§£å†³å…‰ç…§é—®é¢˜å’Œä¿¡æ¯å¹²æ‰°çš„å…³é”®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ­¥é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œåœ¨æ¯ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å‘¨æœŸä¸­ï¼Œå°†è¿‡ç¨‹åˆ†ä¸ºå…‰æµé…å‡†ã€å¤šå°ºåº¦å›¾åƒåˆ†è§£å’Œå¤šé‡å˜æ¢å¯¹é½ä¸‰ä¸ªæ­¥éª¤ã€‚æ¯ä¸€æ­¥ä»…è®­ç»ƒç›¸å…³ç½‘ç»œï¼Œé¿å…æ— å…³ä¿¡æ¯çš„å¹²æ‰°ã€‚åŸºäºå¯¹åŸºç¡€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œæ‰€ææ–¹æ³•åœ¨SCAREDæ•°æ®é›†ä¸Šå®ç°äº†è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡çš„æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨Hamlynæ•°æ®é›†ä¸Šå®ç°äº†é›¶-shotæ·±åº¦ä¼°è®¡ï¼Œè¯¯å·®é™ä½äº†4%è‡³10%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººå†…çª¥é•œä¸­çš„å•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å…‰ç…§å˜åŒ–å’Œç¨€ç–çº¹ç†ä¸‹è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§å¤šæ­¥é«˜æ•ˆå¾®è°ƒçš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å°†è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºå…‰æµé…å‡†ã€å¤šå°ºåº¦å›¾åƒåˆ†è§£å’Œå¤šé‡å˜æ¢å¯¹é½ä¸‰ä¸ªæ­¥éª¤ï¼Œç¡®ä¿æ¯ä¸€æ­¥åªå…³æ³¨ç›¸å…³ç½‘ç»œï¼Œå‡å°‘ä¿¡æ¯å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå…‰æµé…å‡†ç”¨äºå¯¹é½å›¾åƒï¼Œæ¥ç€è¿›è¡Œå¤šå°ºåº¦å›¾åƒåˆ†è§£ä»¥æå–ä¸åŒå±‚æ¬¡çš„ç‰¹å¾ï¼Œæœ€åé€šè¿‡å¤šé‡å˜æ¢å¯¹é½æ¥å¢å¼ºç‰¹å¾çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨å¤šæ­¥è®­ç»ƒç­–ç•¥ï¼Œå…è®¸åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œä»è€Œæé«˜äº†è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€è®­ç»ƒæµç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒçš„ç­–ç•¥ï¼Œè®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¯ä¸ªæ¨¡å—çš„æ€§èƒ½ï¼ŒåŒæ—¶ç¡®ä¿ç½‘ç»œç»“æ„èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å†…çª¥é•œå›¾åƒçš„ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨SCAREDæ•°æ®é›†ä¸Šå®ç°äº†è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡çš„æœ€æ–°æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿ï¼Œè¯¯å·®é™ä½äº†4%è‡³10%ã€‚åœ¨Hamlynæ•°æ®é›†ä¸Šï¼Œæ–¹æ³•è¿˜å®ç°äº†é›¶-shotæ·±åº¦ä¼°è®¡ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººè¾…åŠ©å†…çª¥é•œé¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæå‡å†…çª¥é•œæ‰‹æœ¯ä¸­çš„æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿›è€Œæé«˜æ‰‹æœ¯çš„å®‰å…¨æ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–éœ€è¦æ·±åº¦ä¼°è®¡çš„åŒ»ç–—å½±åƒåˆ†æé¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Monocular depth estimation and ego-motion estimation are significant tasks for scene perception and navigation in stable, accurate and efficient robot-assisted endoscopy. To tackle lighting variations and sparse textures in endoscopic scenes, multiple techniques including optical flow, appearance flow and intrinsic image decomposition have been introduced into the existing methods. However, the effective training strategy for multiple modules are still critical to deal with both illumination issues and information interference for self-supervised depth estimation in endoscopy. Therefore, a novel framework with multistep efficient finetuning is proposed in this work. In each epoch of end-to-end training, the process is divided into three steps, including optical flow registration, multiscale image decomposition and multiple transformation alignments. At each step, only the related networks are trained without interference of irrelevant information. Based on parameter-efficient finetuning on the foundation model, the proposed method achieves state-of-the-art performance on self-supervised depth estimation on SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with 4\%$\sim$10\% lower error. The evaluation code of this work has been published on https://github.com/BaymaxShao/EndoMUST.

