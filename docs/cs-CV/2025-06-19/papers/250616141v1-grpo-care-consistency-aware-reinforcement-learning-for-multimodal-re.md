---
layout: default
title: GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning
---

# GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16141" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.16141v1</a>
  <a href="https://arxiv.org/pdf/2506.16141.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16141v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16141v1', 'GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, Xihui Liu

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-19

**Â§áÊ≥®**: Code released at: https://github.com/TencentARC/GRPO-CARE

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫GRPO-CARE‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊé®ÁêÜ` `‰∏ÄËá¥ÊÄßÂ•ñÂä±` `Âº∫ÂåñÂ≠¶‰π†` `ËßÜÈ¢ëÁêÜËß£` `Ê®°ÂûãËØÑ‰º∞` `ÈÄªËæë‰∏ÄËá¥ÊÄß` `ÂêéËÆ≠ÁªÉÊñπÊ≥ï`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑGRPOÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠Â≠òÂú®ÈÄªËæë‰∏ÄËá¥ÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥Êé®ÁêÜÊ≠•È™§‰∏éÁ≠îÊ°à‰πãÈó¥ÁöÑÂÖ≥ËÅîÊÄßËæÉ‰Ωé„ÄÇ
2. Êú¨ÊñáÊèêÂá∫GRPO-CAREÊ°ÜÊû∂ÔºåÈÄöËøáÂºïÂÖ•‰∏ÄËá¥ÊÄßÂ•ñÂä±Êú∫Âà∂Ôºå‰ºòÂåñÁ≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÂíåÊé®ÁêÜÁöÑ‰∏ÄËá¥ÊÄßÔºåÈÅøÂÖç‰∫ÜÊòæÂºèÁõëÁù£ÁöÑÈúÄÊ±Ç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGRPO-CAREÂú®SEED-Bench-R1Âü∫ÂáÜ‰∏äË°®Áé∞‰ºò‰∫éÊ†áÂáÜGRPOÔºåÂ∞§ÂÖ∂Âú®ÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑËØÑ‰º∞‰∏äÊÄßËÉΩÊèêÂçá‰∫Ü6.7%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂ¶ÇÁªìÊûúÁõëÁù£ÁöÑGRPOÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ‰∏äÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂÖ∂Âú®Â§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂ∞öÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Â§öÊ®°ÊÄÅÂêéËÆ≠ÁªÉÊñπÊ≥ïÁº∫‰πè‰∏•Ê†ºËØÑ‰º∞ÁöÑÈóÆÈ¢òÔºåÊú¨ÊñáÂºïÂÖ•‰∫ÜSEED-Bench-R1Âü∫ÂáÜÔºåÊ∂µÁõñÂ§çÊùÇÁöÑÁúüÂÆûËßÜÈ¢ëÔºåË¶ÅÊ±ÇÂπ≥Ë°°ÊÑüÁü•‰∏éÊé®ÁêÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊ†áÂáÜGRPOÂú®ÊèêÈ´òÁ≠îÊ°àÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÈÄªËæë‰∏ÄËá¥ÊÄßÂç¥Èôç‰ΩéÔºåÂè™Êúâ57.9%ÁöÑ‰∏ÄËá¥ÊÄßÁéá„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜGRPO-CAREÔºå‰∏Ä‰∏™ÂÖ≥Ê≥®‰∏ÄËá¥ÊÄßÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ºòÂåñÁ≠îÊ°àÊ≠£Á°ÆÊÄß‰∏éÊé®ÁêÜ‰∏ÄËá¥ÊÄßÔºå‰∏îÊó†ÈúÄÊòæÂºèÁõëÁù£„ÄÇGRPO-CAREÈÄöËøáÂèåÈáçÂ•ñÂä±Êú∫Âà∂ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂Âú®ÊúÄÈöæËØÑ‰º∞Á∫ßÂà´‰∏äÊèêÂçá‰∫Ü6.7%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®‰∏ÄËá¥ÊÄß‰∏äÊèêÈ´ò‰∫Ü24.5%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈÄªËæë‰∏ÄËá¥ÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑGRPOÊñπÊ≥ïËôΩÁÑ∂ÊèêÈ´ò‰∫ÜÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄßÔºå‰ΩÜÂç¥ÂØºËá¥Êé®ÁêÜÊ≠•È™§‰∏éÁ≠îÊ°à‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥ËÅîÊÄßËæÉ‰ΩéÔºåË°®Áé∞Âá∫57.9%ÁöÑ‰Ωé‰∏ÄËá¥ÊÄßÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGRPO-CAREÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•‰∏ÄËá¥ÊÄßÂ•ñÂä±Êú∫Âà∂ÔºåÈÄöËøáÊØîËæÉÊ®°ÂûãÁöÑÊé®ÁêÜ‰∏éÁ≠îÊ°àÁöÑÂèØËÉΩÊÄßÔºå‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÁöÑÈÄªËæë‰∏ÄËá¥ÊÄßÔºå‰ªéËÄåÊèêÂçáÊï¥‰ΩìÊé®ÁêÜË¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGRPO-CAREÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂü∫Á°ÄÂ•ñÂä±Ê®°ÂùóÂíåÈÄÇÂ∫îÊÄß‰∏ÄËá¥ÊÄßÂ•ñÂä±Ê®°Âùó„ÄÇÂü∫Á°ÄÂ•ñÂä±Áî®‰∫éËØÑ‰º∞Á≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÔºåËÄå‰∏ÄËá¥ÊÄßÂ•ñÂä±ÂàôÈÄöËøá‰∏éÂèÇËÄÉÊ®°ÂûãÁöÑÊØîËæÉÊù•Âä®ÊÄÅË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGRPO-CAREÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂèåÈáçÂ•ñÂä±Êú∫Âà∂ÔºåÁâπÂà´ÊòØÈÄÇÂ∫îÊÄß‰∏ÄËá¥ÊÄßÂ•ñÂä±ÁöÑÂºïÂÖ•Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®Ê≤°ÊúâÊòæÂºèÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ºòÂåñÊé®ÁêÜËøáÁ®ãÁöÑÈÄªËæë‰∏ÄËá¥ÊÄß„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑKLÊÉ©ÁΩöÊú∫Âà∂ÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåGRPO-CARE‰ΩøÁî®‰∫Ü‰∏Ä‰∏™ÁºìÊÖ¢ÊºîÂèòÁöÑÂèÇËÄÉÊ®°ÂûãÊù•ËÆ°ÁÆó‰∏ÄËá¥ÊÄßÂ•ñÂä±ÔºåÂπ∂Êõø‰ª£‰∫Ü‰º†ÁªüÁöÑKLÊÉ©ÁΩö„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Êé¢Á¥¢Êé®ÁêÜË∑ØÂæÑÔºåÊèêÈ´ò‰∫ÜÈÄªËæë‰∏ÄËá¥ÊÄßÂíåÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®SEED-Bench-R1Âü∫ÂáÜÊµãËØï‰∏≠ÔºåGRPO-CAREÁõ∏ÊØîÊ†áÂáÜGRPOÂú®ÊúÄÈöæËØÑ‰º∞Á∫ßÂà´‰∏äÊèêÂçá‰∫Ü6.7%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®‰∏ÄËá¥ÊÄßÊñπÈù¢ÊèêÈ´ò‰∫Ü24.5%„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑËøÅÁßªËÉΩÂäõÔºåËÉΩÂ§üÂú®Â§öÁßçËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜ‰∏äÊîπÂñÑÊ®°ÂûãÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÁêÜËß£„ÄÅÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÂíåÂ§öÊ®°ÊÄÅ‰∫§‰∫íÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜ‰∏ÄËá¥ÊÄßÔºåGRPO-CAREËÉΩÂ§ü‰∏∫ÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊõ¥ÂèØÈù†ÁöÑÊé®ÁêÜÊîØÊåÅÔºåËøõËÄåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄßÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.

