---
layout: default
title: Loss-Oriented Ranking for Automated Visual Prompting in LVLMs
---

# Loss-Oriented Ranking for Automated Visual Prompting in LVLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16112" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16112v2</a>
  <a href="https://arxiv.org/pdf/2506.16112.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16112v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16112v2', 'Loss-Oriented Ranking for Automated Visual Prompting in LVLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuan Zhang, Chun-Kai Fan, Tao Huang, Ming Lu, Sicheng Yu, Junwen Pan, Kuan Cheng, Qi She, Shanghang Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-11-21)

**å¤‡æ³¨**: 17 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoVä»¥è§£å†³è§†è§‰æç¤ºé€‰æ‹©çš„è‡ªåŠ¨åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æç¤º` `è‡ªåŠ¨åŒ–é€‰æ‹©` `å¤šæ¨¡æ€æ¨¡å‹` `å›¾åƒç†è§£` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è®¾è®¡è§†è§‰æç¤ºæ—¶ä¾èµ–äººå·¥ï¼Œæ•ˆç‡ä½ä¸”éš¾ä»¥æ¢ç´¢ä¸åŒæç¤ºçš„æ½œåŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºAutoVï¼Œé€šè¿‡è‡ªåŠ¨é€‰æ‹©æœ€ä½³è§†è§‰æç¤ºï¼Œåˆ©ç”¨é¢„è®­ç»ƒLVLMçš„é¢„æµ‹æŸå¤±è¿›è¡Œæ’åï¼Œä¼˜åŒ–è§†è§‰æç¤ºé€‰æ‹©è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoVæ˜¾è‘—æå‡äº†LVLMçš„æ€§èƒ½ï¼Œå¦‚LLaVA-OVåœ¨VizWizä»»åŠ¡ä¸Šæå‡10.2%çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ–‡æœ¬æç¤ºçš„å¯å‘ï¼Œè§†è§‰æç¤ºè¢«æ¢ç´¢ç”¨äºå¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•è®¾è®¡çš„å¯å‘å¼è§†è§‰æç¤ºï¼Œå¦‚åœ¨åŸå§‹è¾“å…¥å›¾åƒä¸Šå åŠ æ–‡æœ¬æŸ¥è¯¢å¼•å¯¼çš„æ³¨æ„åŠ›çƒ­å›¾ï¼Œæ‰‹åŠ¨è®¾è®¡æœ‰æ•ˆæç¤ºæ—¢å›°éš¾åˆè€—æ—¶ï¼Œä¸”å¾€å¾€æœªèƒ½å……åˆ†æŒ–æ˜ä¸åŒè§†è§‰æç¤ºçš„ä¼˜åŠ¿ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AutoVï¼Œèƒ½å¤Ÿæ ¹æ®ç»™å®šçš„æ–‡æœ¬æŸ¥è¯¢å’Œè¾“å…¥å›¾åƒè‡ªåŠ¨é€‰æ‹©æœ€ä½³è§†è§‰æç¤ºã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªåŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨ç®¡é“ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„LVLMè¯„ä¼°å„ç§è§†è§‰æç¤ºï¼Œå¹¶æ ¹æ®æ¨¡å‹ç”Ÿæˆçš„é¢„æµ‹æŸå¤±å¯¹å…¶è¿›è¡Œæ’åã€‚åˆ©ç”¨è¿™ä¸€æ’åä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒAutoVè‡ªåŠ¨é€‰æ‹©æœ€ä½³è§†è§‰æç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒAutoVåœ¨å¤šä¸ªå›¾åƒç†è§£ä»»åŠ¡ä¸­æå‡äº†å¤šç§LVLMçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰æç¤ºè®¾è®¡æ–¹æ³•çš„ä½æ•ˆå’Œä¸ä¼˜åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥è®¾è®¡ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨ä¸åŒè§†è§‰æç¤ºçš„ä¼˜åŠ¿ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„AutoVé€šè¿‡è‡ªåŠ¨åŒ–é€‰æ‹©æœ€ä½³è§†è§‰æç¤ºï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„LVLMå¯¹ä¸åŒè§†è§‰æç¤ºè¿›è¡Œè¯„ä¼°å’Œæ’åï¼Œä»è€Œä¼˜åŒ–æç¤ºé€‰æ‹©è¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ä¸æ ‡æ³¨ç®¡é“ã€è§†è§‰æç¤ºè¾“å…¥ã€LVLMè¯„ä¼°å’Œæ’åæ¨¡å—ã€‚é¦–å…ˆæ”¶é›†å¤šç§è§†è§‰æç¤ºï¼Œç„¶åå°†å…¶è¾“å…¥LVLMï¼Œä¾æ®æ¨¡å‹çš„é¢„æµ‹æŸå¤±è¿›è¡Œæ’åï¼Œæœ€åè®­ç»ƒAutoVé€‰æ‹©æœ€ä½³æç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šAutoVçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªåŠ¨åŒ–é€‰æ‹©è§†è§‰æç¤ºçš„èƒ½åŠ›ï¼Œåˆ©ç”¨æ¨¡å‹é¢„æµ‹æŸå¤±ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä¸ç°æœ‰çš„æ‰‹åŠ¨è®¾è®¡æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†é€‰æ‹©æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒAutoVä½¿ç”¨äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æç¤ºé€‰æ‹©ï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„ç½‘ç»œç»“æ„ä»¥å¤„ç†å¤šç§è§†è§‰æç¤ºçš„è¾“å…¥å’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoVåœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†LVLMçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒLLaVA-OVåœ¨VizWizä»»åŠ¡ä¸Šå®ç°äº†10.2%çš„å‡†ç¡®ç‡æå‡ï¼Œè€ŒQwen2.5-VLåœ¨MMMUä»»åŠ¡ä¸Šæå‡äº†3.8%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†AutoVä½œä¸ºä¸€ç§æœ€ä½³è§†è§‰æç¤ºæ–¹æ³•çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å›¾åƒç†è§£ã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡ä¼˜åŒ–è§†è§‰æç¤ºé€‰æ‹©ï¼ŒAutoVèƒ½å¤Ÿæå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we develop an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experiments indicate that AutoV enhances the performance of various LVLMs across multiple image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\textbf{10.2}\%$ accuracy gain on VizWiz, and AutoV boosts Qwen2.5-VL by $\textbf{3.8}\%$ on MMMU, highlighting its potential as an optimal visual prompting method.

