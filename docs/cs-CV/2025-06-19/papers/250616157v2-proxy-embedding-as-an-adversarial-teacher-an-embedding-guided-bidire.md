---
layout: default
title: Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models
---

# Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16157" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16157v2</a>
  <a href="https://arxiv.org/pdf/2506.16157.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16157v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16157v2', 'Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, Chao Yi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: 20pages, 5figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPEATä»¥è§£å†³REFæ¨¡å‹çš„å¯¹æŠ—æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼•ç”¨è¡¨è¾¾åˆ†å‰²` `å¯¹æŠ—æ”»å‡»` `å¤šæ¨¡æ€å­¦ä¹ ` `é²æ£’æ€§` `åµŒå…¥å¼•å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•åœ¨ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨RESæ¨¡å‹ä¸Šæ•ˆæœä¸ä½³ï¼Œæœªèƒ½æ­ç¤ºå…¶å¤šæ¨¡æ€ç»“æ„çš„è„†å¼±æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åµŒå…¥å¼•å¯¼çš„åŒå‘æ”»å‡»æ–¹æ³•ï¼ˆPEATï¼‰ï¼Œæ—¨åœ¨æé«˜RESæ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ·åŒ–æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPEATåœ¨å¤šä¸ªRESæ¶æ„å’Œæ ‡å‡†åŸºå‡†ä¸Šå‡ä¼˜äºç°æœ‰çš„ç«äº‰åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼•ç”¨è¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰å…è®¸åŸºäºè‡ªç„¶è¯­è¨€æè¿°åœ¨å›¾åƒä¸­è¿›è¡Œç²¾ç¡®çš„ç‰©ä½“åˆ†å‰²ï¼Œå…·æœ‰é«˜åº¦çµæ´»æ€§å’Œå¹¿æ³›çš„å®é™…åº”ç”¨ã€‚ç„¶è€Œï¼ŒRESæ¨¡å‹å¯¹æŠ—å¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•åœ¨ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç›´æ¥åº”ç”¨äºRESæ¨¡å‹æ—¶æ•ˆæœä¸ä½³ï¼Œæœªèƒ½æ­ç¤ºå…¶å¤šæ¨¡æ€ç»“æ„çš„è„†å¼±æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†PEATï¼Œå³ä¸€ç§åµŒå…¥å¼•å¯¼çš„åŒå‘æ”»å‡»æ–¹æ³•ã€‚é€šè¿‡åœ¨å¤šä¸ªRESæ¶æ„å’Œæ ‡å‡†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒPEATå§‹ç»ˆä¼˜äºç«äº‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼•ç”¨è¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ ·æœ¬æ—¶çš„è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œä½†åœ¨RESæ¨¡å‹ä¸­è¡¨ç°ä¸ä½³ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶å¤šæ¨¡æ€ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„PEATæ–¹æ³•é€šè¿‡åµŒå…¥å¼•å¯¼çš„åŒå‘æ”»å‡»ç­–ç•¥ï¼Œæ—¨åœ¨ç”Ÿæˆèƒ½å¤Ÿè·¨å¤šç§æ–‡æœ¬è¾“å…¥çš„å¯¹æŠ—æ ·æœ¬ï¼Œä»è€Œå¢å¼ºRESæ¨¡å‹çš„é²æ£’æ€§ã€‚è¿™æ ·çš„è®¾è®¡è€ƒè™‘åˆ°äº†ç”¨æˆ·åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½ä½¿ç”¨çš„å¤šæ ·åŒ–è¡¨è¾¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPEATçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ¨¡å—å’ŒåŒå‘æ”»å‡»ç­–ç•¥ã€‚é¦–å…ˆï¼Œé€šè¿‡åµŒå…¥å¼•å¯¼ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œç„¶ååˆ©ç”¨åŒå‘æ”»å‡»ç­–ç•¥è¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šPEATçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åµŒå…¥å¼•å¯¼çš„åŒå‘æ”»å‡»æ–¹æ³•ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•å‘æ”»å‡»æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ­ç¤ºRESæ¨¡å‹çš„è„†å¼±æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒPEATé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹æŠ—æ ·æœ¬çš„ç”Ÿæˆï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€è¾“å…¥çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPEATåœ¨å¤šä¸ªRESæ¶æ„ä¸Šå‡æ˜¾è‘—ä¼˜äºç«äº‰åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼ŒéªŒè¯äº†å…¶åœ¨å¯¹æŠ—æ”»å‡»ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸåŠ©æ‰‹ç­‰å¤šæ¨¡æ€è§†è§‰ä»»åŠ¡ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼ŒRESæ¨¡å‹çš„é²æ£’æ€§å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†æ•æ„Ÿä¿¡æ¯æ—¶ä¸æ³„éœ²éšç§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tasks. Despite its impressive performance, the robustness of RES models against adversarial examples remains largely unexplored. While prior adversarial attack methods have explored adversarial robustness on conventional segmentation models, they perform poorly when directly applied to RES models, failing to expose vulnerabilities in its multimodal structure. In practical open-world scenarios, users typically issue multiple, diverse referring expressions to interact with the same image, highlighting the need for adversarial examples that generalize across varied textual inputs. Furthermore, from the perspective of privacy protection, ensuring that RES models do not segment sensitive content without explicit authorization is a crucial aspect of enhancing the robustness and security of multimodal vision-language systems. To address these challenges, we present PEAT, an Embedding-Guided Bidirectional Attack for RES models. Extensive experiments across multiple RES architectures and standard benchmarks show that PEAT consistently outperforms competitive baselines.

