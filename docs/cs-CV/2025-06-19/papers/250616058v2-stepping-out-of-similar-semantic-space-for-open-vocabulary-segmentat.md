---
layout: default
title: Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation
---

# Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16058" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16058v2</a>
  <a href="https://arxiv.org/pdf/2506.16058.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16058v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16058v2', 'Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yong Liu, SongLi Wu, Sule Bai, Jiahao Wang, Yitong Wang, Yansong Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-06-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOVSNetä»¥è§£å†³å¼€æ”¾è¯æ±‡åˆ†å‰²æ€§èƒ½ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡åˆ†å‰²` `å¤šæ¨¡æ€å­¦ä¹ ` `ç‰¹å¾èåˆ` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•åœ¨è¯„ä¼°æ¨¡å‹å¯¹å¼€æ”¾è¯æ±‡æ¦‚å¿µçš„ç†è§£æ—¶å­˜åœ¨å±€é™ï¼Œæµ‹è¯•é›†ä¸è®­ç»ƒé›†è¯­ä¹‰ç›¸ä¼¼ã€‚
2. æœ¬æ–‡æå‡ºçš„æ–°åŸºå‡†OpenBenchèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œæ¦‚å¿µçš„ç†è§£ï¼Œæå‡ºOVSNetä»¥æå‡åˆ†å‰²æ€§èƒ½ã€‚
3. OVSNetåœ¨ç°æœ‰æ•°æ®é›†å’ŒOpenBenchä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒéªŒè¯äº†åŸºå‡†å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼€æ”¾è¯æ±‡åˆ†å‰²æ—¨åœ¨åˆ©ç”¨æ— é™æ–‡æœ¬è¾“å…¥å®ç°ä»»æ„ç±»åˆ«çš„åˆ†å‰²ã€‚ç°æœ‰æ–¹æ³•åœ¨æµ‹è¯„æ¨¡å‹å¯¹â€œå¼€æ”¾è¯æ±‡â€æ¦‚å¿µçš„ç†è§£èƒ½åŠ›æ—¶å­˜åœ¨å±€é™ï¼Œå› ä¸ºæµ‹è¯•é›†çš„è¯­ä¹‰ç©ºé—´ä¸è®­ç»ƒç©ºé—´ç›¸ä¼¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°åŸºå‡†OpenBenchï¼Œæ—¨åœ¨æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œæ¦‚å¿µçš„ç†è§£å’Œåˆ†å‰²èƒ½åŠ›ã€‚é€šè¿‡åœ¨OpenBenchä¸Šæµ‹è¯•ç°æœ‰æ–¹æ³•ï¼Œå‘ç°å…¶æ€§èƒ½ä¸ä¹‹å‰æµ‹è¯•é›†çš„ç»“è®ºå­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†OVSNetï¼Œé€šè¿‡å¼‚æ„ç‰¹å¾çš„ç²¾ç»†èåˆå’Œè®­ç»ƒç©ºé—´çš„æ— æˆæœ¬æ‰©å±•ï¼Œæå‡äº†å¤šæ ·åŒ–å¼€æ”¾åœºæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½ï¼Œåœ¨ç°æœ‰æ•°æ®é›†å’ŒOpenBenchä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­ç°æœ‰æ–¹æ³•å¯¹æ¨¡å‹ç†è§£èƒ½åŠ›è¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æµ‹è¯•é›†çš„è¯­ä¹‰ç©ºé—´ä¸è®­ç»ƒç©ºé—´ç›¸ä¼¼ï¼Œæ— æ³•æœ‰æ•ˆæµ‹é‡æ¨¡å‹çš„å¼€æ”¾è¯æ±‡æ¦‚å¿µç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºOpenBenchåŸºå‡†ä»¥è¯„ä¼°æ¨¡å‹å¯¹å¤šæ ·åŒ–çœŸå®ä¸–ç•Œæ¦‚å¿µçš„ç†è§£ï¼ŒåŒæ—¶æå‡ºOVSNetï¼Œé€šè¿‡èåˆå¼‚æ„ç‰¹å¾å’Œæ‰©å±•è®­ç»ƒç©ºé—´æ¥æå‡åˆ†å‰²æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOVSNetçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–æ¨¡å—ã€ç‰¹å¾èåˆæ¨¡å—å’Œåˆ†å‰²å†³ç­–æ¨¡å—ã€‚ç‰¹å¾æå–æ¨¡å—ä»è¾“å…¥å›¾åƒä¸­æå–å¤šç§ç‰¹å¾ï¼Œç‰¹å¾èåˆæ¨¡å—å°†ä¸åŒæ¥æºçš„ç‰¹å¾è¿›è¡Œèåˆï¼Œæœ€åé€šè¿‡åˆ†å‰²å†³ç­–æ¨¡å—ç”Ÿæˆåˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†OpenBenchåŸºå‡†å’ŒOVSNetæ–¹æ³•ï¼ŒOpenBenchæ˜¾è‘—ä¸åŒäºè®­ç»ƒè¯­ä¹‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„å¼€æ”¾è¯æ±‡ç†è§£èƒ½åŠ›ï¼Œè€ŒOVSNeté€šè¿‡å¼‚æ„ç‰¹å¾èåˆæå‡äº†åˆ†å‰²æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šOVSNeté‡‡ç”¨äº†å¤šå±‚æ¬¡ç‰¹å¾èåˆç­–ç•¥ï¼Œè®¾è®¡äº†æ–°çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†æ®‹å·®è¿æ¥ä»¥å¢å¼ºç‰¹å¾ä¼ é€’èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨OpenBenchåŸºå‡†ä¸Šï¼ŒOVSNetç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨å¤šä¸ªç±»åˆ«çš„åˆ†å‰²ä»»åŠ¡ä¸­å‡å–å¾—äº†è¶…è¿‡10%çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æå’Œæ™ºèƒ½ç›‘æ§ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤šæ ·åŒ–çš„è§†è§‰ä¿¡æ¯ã€‚æœªæ¥ï¼Œéšç€å¼€æ”¾è¯æ±‡åˆ†å‰²æŠ€æœ¯çš„è¿›æ­¥ï¼Œå¯èƒ½ä¼šæ¨åŠ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Open-vocabulary segmentation aims to achieve segmentation of arbitrary categories given unlimited text inputs as guidance. To achieve this, recent works have focused on developing various technical routes to exploit the potential of large-scale pre-trained vision-language models and have made significant progress on existing benchmarks. However, we find that existing test sets are limited in measuring the models' comprehension of ``open-vocabulary" concepts, as their semantic space closely resembles the training space, even with many overlapping categories. To this end, we present a new benchmark named OpenBench that differs significantly from the training semantics. It is designed to better assess the model's ability to understand and segment a wide range of real-world concepts. When testing existing methods on OpenBench, we find that their performance diverges from the conclusions drawn on existing test sets. In addition, we propose a method named OVSNet to improve the segmentation performance for diverse and open scenarios. Through elaborate fusion of heterogeneous features and cost-free expansion of the training space, OVSNet achieves state-of-the-art results on both existing datasets and our proposed OpenBench. Corresponding analysis demonstrate the soundness and effectiveness of our proposed benchmark and method.

