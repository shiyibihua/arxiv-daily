---
layout: default
title: LBMamba: Locally Bi-directional Mamba
---

# LBMamba: Locally Bi-directional Mamba

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15976" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.15976v2</a>
  <a href="https://arxiv.org/pdf/2506.15976.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15976v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15976v2', 'LBMamba: Locally Bi-directional Mamba')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jingwei Zhang, Xi Han, Hong Qin, Mahdi S. Hosseini, Dimitris Samaras

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-19 (Êõ¥Êñ∞: 2025-11-11)

**Â§áÊ≥®**: Accepted to TMLR

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/cvlab-stonybrook/LBMamba)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LBMamba‰ª•ÊèêÂçáMambaÊ®°ÂûãÁöÑËÆ°ÁÆóÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Áä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `ÂõæÂÉèÂàÜÁ±ª` `ËØ≠‰πâÂàÜÂâ≤` `ÁõÆÊ†áÊ£ÄÊµã` `Ê∑±Â∫¶Â≠¶‰π†` `È´òÊïàËÆ°ÁÆó`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑMambaÊ®°ÂûãÁî±‰∫éÂÖ∂ÂçïÂêëÁâπÊÄßÔºåÊó†Ê≥ïÊúâÊïàÂà©Áî®ÂêéÁª≠Áä∂ÊÄÅÁöÑ‰ø°ÊÅØÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊÄßËÉΩ„ÄÇ
2. LBMambaÈÄöËøáÂú®ÂâçÂêëÊâ´Êèè‰∏≠ÂµåÂÖ•Â±ÄÈÉ®ÂèçÂêëÊâ´ÊèèÔºåÈÅøÂÖç‰∫ÜÂÖ®Â±ÄÂèçÂêëÊâ´ÊèèÂ∏¶Êù•ÁöÑÈ¢ùÂ§ñËÆ°ÁÆóË¥üÊãÖ„ÄÇ
3. Âú®ImageNet-1K„ÄÅADE20KÂíåCOCOÁ≠âÂ§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÔºåLBMambaÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊÄßËÉΩÔºå‰∏îÂú®ÁóÖÁêÜÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠‰πüÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

MambaÊòØ‰∏ÄÁßçÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâÔºåÈÄöËøáÂ∞ÜÈÄíÂΩíÈáçÊûÑ‰∏∫Âπ∂Ë°åÊâ´ÊèèÊù•Âä†ÈÄüËÆ≠ÁªÉÔºåÊàê‰∏∫Ëá™Ê≥®ÊÑèÂäõÁöÑÁ∫øÊÄßÊâ©Â±ïÊõø‰ª£ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåMambaÁöÑÂçïÂêëÁâπÊÄßÈôêÂà∂‰∫ÜÂÖ∂‰ø°ÊÅØËé∑ÂèñËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÂΩìÂâçÁöÑMambaËÆ°ÁÆóÊú∫ËßÜËßâÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂ¢ûÂä†ÂÖ®Â±ÄÂèçÂêëÊâ´ÊèèÊù•ÂÆûÁé∞ÂèåÂêëÊâ´ÊèèÔºå‰ΩÜËøô‰ºöÂØºËá¥ËÆ°ÁÆóË¥üÊãÖÂä†Èáç„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜLBMambaÔºå‰∏ÄÁßçÂ±ÄÈÉ®ÂèåÂêëSSMÊ®°ÂùóÔºåËÉΩÂ§üÂú®ÂâçÂêëÊâ´Êèè‰∏≠ÂµåÂÖ•ËΩªÈáèÁ∫ßÁöÑÂ±ÄÈÉ®ÂèçÂêëÊâ´ÊèèÔºå‰ªéËÄåÊ∂àÈô§È¢ùÂ§ñÁöÑËÆ°ÁÆóË¥üÊãÖ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLBMambaÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂùáË°®Áé∞Âá∫‰ºòË∂äÁöÑÊÄßËÉΩ‰∏éÂêûÂêêÈáèÂπ≥Ë°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥MambaÊ®°ÂûãÁöÑÂçïÂêëÁâπÊÄßÊâÄÂØºËá¥ÁöÑ‰ø°ÊÅØËé∑Âèñ‰∏çË∂≥ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöËøáÂÖ®Â±ÄÂèçÂêëÊâ´ÊèèÊù•Âº•Ë°•Ëøô‰∏ÄÁº∫Èô∑Ôºå‰ΩÜÂ¢ûÂä†‰∫ÜËÆ°ÁÆóË¥üÊãÖ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLBMambaÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂú®ÂâçÂêëÊâ´Êèè‰∏≠ÂµåÂÖ•Â±ÄÈÉ®ÂèçÂêëÊâ´ÊèèÔºå‰ª•ÂÆûÁé∞ÂèåÂêë‰ø°ÊÅØËé∑ÂèñÔºåÂêåÊó∂ÈÅøÂÖçÂÖ®Â±ÄÂèçÂêëÊâ´ÊèèÁöÑÈ¢ùÂ§ñËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLBMambaÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂâçÂêëÊâ´ÊèèÂíåÂ±ÄÈÉ®ÂèçÂêëÊâ´Êèè‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºåÂâçÂêëÊâ´ÊèèË¥üË¥£Ëé∑ÂèñÂΩìÂâçÁä∂ÊÄÅÁöÑ‰ø°ÊÅØÔºåËÄåÂ±ÄÈÉ®ÂèçÂêëÊâ´ÊèèÂàôÂú®ÊØè‰∏™Á∫øÁ®ãÂØÑÂ≠òÂô®‰∏≠ÊâßË°åÔºå‰ª•ÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLBMambaÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Â±ÄÈÉ®ÂèåÂêëËÆæËÆ°ÔºåËÉΩÂ§üÂú®‰∏çÂ¢ûÂä†ËÆ°ÁÆóË¥üÊãÖÁöÑÊÉÖÂÜµ‰∏ãÊÅ¢Â§çÂÖ®Â±ÄÊÑüÂèóÈáéÔºåËøô‰∏ÄËÆæËÆ°‰∏é‰º†ÁªüÁöÑÂÖ®Â±ÄÂèåÂêëÊâ´ÊèèÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêå„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®LBMamba‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ËÆæÁΩÆÂåÖÊã¨Â±ÄÈÉ®ÂèçÂêëÊâ´ÊèèÁöÑËåÉÂõ¥ÂíåÂâçÂêëÊâ´ÊèèÁöÑÊ≠•ÂπÖÔºåÊçüÂ§±ÂáΩÊï∞ÈááÁî®‰∫§ÂèâÁÜµÊçüÂ§±‰ª•‰ºòÂåñÂàÜÁ±ªÊÄßËÉΩÔºåÁΩëÁªúÁªìÊûÑÂàôÈÄöËøá‰∫§ÊõøÊâ´ÊèèÊñπÂêëÁöÑÊñπÂºèÊù•Â¢ûÂº∫Ê®°ÂûãÁöÑË°®ËææËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÔºåLBMambaÂú®Áõ∏ÂêåÁöÑÂêûÂêêÈáè‰∏ãÂÆûÁé∞‰∫Ü0.8%Ëá≥1.6%ÁöÑImageNet-1KÂàÜÁ±ªÂáÜÁ°ÆÁéáÊèêÂçáÔºå0.6%Ëá≥2.7%ÁöÑADE20KËØ≠‰πâÂàÜÂâ≤mIoUÊèêÂçáÔºå‰ª•Âèä0.9%Âíå1.1%ÁöÑCOCOÊ£ÄÊµãAPbÂíåAPmÊèêÂçá„ÄÇÊ≠§Â§ñÔºåLBMambaËøòÊèêÂçá‰∫ÜÂõõ‰∏™SOTA MambaÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂ¢ûÂπÖ‰∏∫0.5%Ëá≥3.4%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LBMambaÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÂõæÂÉèÂàÜÁ±ª„ÄÅËØ≠‰πâÂàÜÂâ≤ÂíåÁõÆÊ†áÊ£ÄÊµãÁ≠â‰ªªÂä°‰∏≠„ÄÇÂÖ∂È´òÊïàÁöÑËÆ°ÁÆóÊÄßËÉΩÂíåÂáÜÁ°ÆÊÄßÊèêÂçá‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂÆûÊó∂Â§ÑÁêÜÂíåÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÁöÑÂàÜÊûêÔºåÊú™Êù•ÂèØÊúõÂú®ÂåªÁñóÂõæÂÉèÂàÜÊûêÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Mamba, a State Space Model (SSM) that accelerates training by recasting recurrence as a parallel scan, has recently emerged as a linearly-scaling alternative to self-attention. Because of its unidirectional nature, each state in Mamba only has information of its previous states and is blind to states after. Current Mamba-based computer-vision methods typically overcome this by augmenting Mamba's global forward scan with a global backward scan, forming a bi-directional scan to restore a full receptive field. However, this operation doubles the computational load, eroding much of the efficiency advantage that originally Mamba have. To eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM block that embeds a lightweight locally backward scan inside the forward scan and executes it in per-thread registers. Building on LBMamba, we present LBVim, a backbone that alternates scan directions every two layers to recover a global receptive field without extra backward sweeps. We validate our approach on both natural images and whole slide images (WSIs) and show that it constantly offers a superior performance-throughput trade-off. Under the same throughput, LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection dataset. Our method also boosts the accuracy of four SOTA Mamba models, namely VMamba, LocalVim, PlainMamba and Adventurer, by 0.5% to 3.4%. We integrate LBMamba into the SOTA pathology multiple instance learning (MIL) model, MambaMIL, which is unidirectional. Experiments on 3 public WSI classification datasets show that our method achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1, 1.67% better accuracy. Our code is available at https://github.com/cvlab-stonybrook/LBMamba.

