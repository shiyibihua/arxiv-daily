---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-06-06
---

# cs.CVï¼ˆ2025-06-06ï¼‰

ğŸ“Š å…± **29** ç¯‡è®ºæ–‡
 | ğŸ”— **7** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250605965v1-dy3dgs-slam-monocular-3d-gaussian-splatting-slam-for-dynamic-environ.html">Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments</a></td>
  <td>æå‡ºDy3DGS-SLAMä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸‹å•ç›®SLAMé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05965v1" data-paper-url="./papers/250605965v1-dy3dgs-slam-monocular-3d-gaussian-splatting-slam-for-dynamic-environ.html" onclick="toggleFavorite(this, '2506.05965v1', 'Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250605689v1-pts3d-llm-studying-the-impact-of-token-structure-for-3d-scene-unders.html">Pts3D-LLM: Studying the Impact of Token Structure for 3D Scene Understanding With Large Language Models</a></td>
  <td>æå‡ºPts3D-LLMä»¥æå‡3Dåœºæ™¯ç†è§£çš„æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05689v1" data-paper-url="./papers/250605689v1-pts3d-llm-studying-the-impact-of-token-structure-for-3d-scene-unders.html" onclick="toggleFavorite(this, '2506.05689v1', 'Pts3D-LLM: Studying the Impact of Token Structure for 3D Scene Understanding With Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250606517v3-gs4-generalizable-sparse-splatting-semantic-slam.html">GS4: Generalizable Sparse Splatting Semantic SLAM</a></td>
  <td>æå‡ºGS4ä»¥è§£å†³ä¼ ç»ŸSLAMåœ¨è¯­ä¹‰æ˜ å°„ä¸­çš„ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">semantic mapping</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06517v3" data-paper-url="./papers/250606517v3-gs4-generalizable-sparse-splatting-semantic-slam.html" onclick="toggleFavorite(this, '2506.06517v3', 'GS4: Generalizable Sparse Splatting Semantic SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250606569v1-textile-analysis-for-recycling-automation-using-transfer-learning-an.html">Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models</a></td>
  <td>æå‡ºåŸºäºè¿ç§»å­¦ä¹ å’Œé›¶æ ·æœ¬æ¨¡å‹çš„çººç»‡å“å›æ”¶è‡ªåŠ¨åŒ–åˆ†ææ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06569v1" data-paper-url="./papers/250606569v1-textile-analysis-for-recycling-automation-using-transfer-learning-an.html" onclick="toggleFavorite(this, '2506.06569v1', 'Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250606218v1-stsbench-a-spatio-temporal-scenario-benchmark-for-multi-modal-large-.html">STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving</a></td>
  <td>æå‡ºSTSBenchä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ—¶ç©ºæ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06218v1" data-paper-url="./papers/250606218v1-stsbench-a-spatio-temporal-scenario-benchmark-for-multi-modal-large-.html" onclick="toggleFavorite(this, '2506.06218v1', 'STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250605651v1-hallucinate-ground-repeat-a-framework-for-generalized-visual-relatio.html">Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection</a></td>
  <td>æå‡ºè¿­ä»£è§†è§‰åŸºç¡€æ¡†æ¶ä»¥è§£å†³è§†è§‰å…³ç³»æ£€æµ‹çš„æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05651v1" data-paper-url="./papers/250605651v1-hallucinate-ground-repeat-a-framework-for-generalized-visual-relatio.html" onclick="toggleFavorite(this, '2506.05651v1', 'Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250609063v1-reconstructing-heterogeneous-biomolecules-via-hierarchical-gaussian-.html">Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery</a></td>
  <td>æå‡ºCryoSPIREä»¥è§£å†³å†·å†»ç”µå­æ˜¾å¾®é•œä¸­ç”Ÿç‰©åˆ†å­é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.09063v1" data-paper-url="./papers/250609063v1-reconstructing-heterogeneous-biomolecules-via-hierarchical-gaussian-.html" onclick="toggleFavorite(this, '2506.09063v1', 'Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250605883v1-hmvlm-multistage-reasoning-enhanced-vision-language-model-for-long-t.html">HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios</a></td>
  <td>æå‡ºHMVLMä»¥è§£å†³é•¿å°¾é©¾é©¶åœºæ™¯ä¸­çš„å†³ç­–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05883v1" data-paper-url="./papers/250605883v1-hmvlm-multistage-reasoning-enhanced-vision-language-model-for-long-t.html" onclick="toggleFavorite(this, '2506.05883v1', 'HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250605655v1-aerial-multi-view-stereo-via-adaptive-depth-range-inference-and-norm.html">Aerial Multi-View Stereo via Adaptive Depth Range Inference and Normal Cues</a></td>
  <td>æå‡ºè‡ªé€‚åº”æ·±åº¦èŒƒå›´MVSä»¥è§£å†³èˆªç©ºå¤šè§†å›¾ç«‹ä½“é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05655v1" data-paper-url="./papers/250605655v1-aerial-multi-view-stereo-via-adaptive-depth-range-inference-and-norm.html" onclick="toggleFavorite(this, '2506.05655v1', 'Aerial Multi-View Stereo via Adaptive Depth Range Inference and Normal Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250605709v1-token-transforming-a-unified-and-training-free-token-compression-fra.html">Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration</a></td>
  <td>æå‡ºToken Transformingæ¡†æ¶ä»¥åŠ é€Ÿè§†è§‰Transformerå¹¶å‡å°‘ä¿¡æ¯æŸå¤±</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05709v1" data-paper-url="./papers/250605709v1-token-transforming-a-unified-and-training-free-token-compression-fra.html" onclick="toggleFavorite(this, '2506.05709v1', 'Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250606242v1-visual-graph-arena-evaluating-visual-conceptualization-of-vision-and.html">Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models</a></td>
  <td>æå‡ºè§†è§‰å›¾å½¢ç«æŠ€åœºä»¥è§£å†³è§†è§‰æ¦‚å¿µåŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06242v1" data-paper-url="./papers/250606242v1-visual-graph-arena-evaluating-visual-conceptualization-of-vision-and.html" onclick="toggleFavorite(this, '2506.06242v1', 'Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250605667v2-driveaction-a-benchmark-for-exploring-human-like-driving-decisions-i.html">DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models</a></td>
  <td>æå‡ºDriveActionåŸºå‡†ä»¥è§£å†³VLAæ¨¡å‹å†³ç­–å¤šæ ·æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05667v2" data-paper-url="./papers/250605667v2-driveaction-a-benchmark-for-exploring-human-like-driving-decisions-i.html" onclick="toggleFavorite(this, '2506.05667v2', 'DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250606144v1-clamr-contextualized-late-interaction-for-multimodal-content-retriev.html">CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval</a></td>
  <td>æå‡ºCLaMRä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘å†…å®¹æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06144v1" data-paper-url="./papers/250606144v1-clamr-contextualized-late-interaction-for-multimodal-content-retriev.html" onclick="toggleFavorite(this, '2506.06144v1', 'CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250605982v6-mca-bench-a-multimodal-benchmark-for-evaluating-captcha-robustness-a.html">MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</a></td>
  <td>æå‡ºMCA-Benchä»¥è¯„ä¼°CAPTCHAå¯¹VLMæ”»å‡»çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05982v6" data-paper-url="./papers/250605982v6-mca-bench-a-multimodal-benchmark-for-evaluating-captcha-robustness-a.html" onclick="toggleFavorite(this, '2506.05982v6', 'MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250606279v1-comemo-lvlms-need-image-context-with-image-memory.html">CoMemo: LVLMs Need Image Context with Image Memory</a></td>
  <td>æå‡ºCoMemoä»¥è§£å†³LVLMåœ¨å›¾åƒä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„ä¿¡æ¯å¿½è§†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06279v1" data-paper-url="./papers/250606279v1-comemo-lvlms-need-image-context-with-image-memory.html" onclick="toggleFavorite(this, '2506.06279v1', 'CoMemo: LVLMs Need Image Context with Image Memory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250606097v1-videochat-a1-thinking-with-long-videos-by-chain-of-shot-reasoning.html">VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning</a></td>
  <td>æå‡ºVideoChat-A1ä»¥è§£å†³é•¿è§†é¢‘ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06097v1" data-paper-url="./papers/250606097v1-videochat-a1-thinking-with-long-videos-by-chain-of-shot-reasoning.html" onclick="toggleFavorite(this, '2506.06097v1', 'VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250605856v1-cross-view-multi-modal-segmentation-ego-exo4d-challenges-2025.html">Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025</a></td>
  <td>æå‡ºè·¨è§†è§’å¤šæ¨¡æ€ç‰©ä½“åˆ†å‰²æ–¹æ³•ä»¥è§£å†³Ego-Exo4DæŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05856v1" data-paper-url="./papers/250605856v1-cross-view-multi-modal-segmentation-ego-exo4d-challenges-2025.html" onclick="toggleFavorite(this, '2506.05856v1', 'Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250605696v2-moralclip-contrastive-alignment-of-vision-and-language-representatio.html">MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory</a></td>
  <td>æå‡ºMoralCLIPä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹é“å¾·ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05696v2" data-paper-url="./papers/250605696v2-moralclip-contrastive-alignment-of-vision-and-language-representatio.html" onclick="toggleFavorite(this, '2506.05696v2', 'MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250606174v1-technical-report-for-egocentric-mistake-detection-for-the-holoassist.html">Technical Report for Egocentric Mistake Detection for the HoloAssist Challenge</a></td>
  <td>æå‡ºåœ¨çº¿é”™è¯¯æ£€æµ‹æ¡†æ¶ä»¥è§£å†³å·¥ä¸šè‡ªåŠ¨åŒ–ä¸­çš„å®æ—¶çº é”™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06174v1" data-paper-url="./papers/250606174v1-technical-report-for-egocentric-mistake-detection-for-the-holoassist.html" onclick="toggleFavorite(this, '2506.06174v1', 'Technical Report for Egocentric Mistake Detection for the HoloAssist Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250605787v2-easg-bench-video-qa-benchmark-with-egocentric-action-scene-graphs.html">EASG-Bench: Video Q&A Benchmark with Egocentric Action Scene Graphs</a></td>
  <td>æå‡ºEASG-Benchä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„é—®ç­”æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05787v2" data-paper-url="./papers/250605787v2-easg-bench-video-qa-benchmark-with-egocentric-action-scene-graphs.html" onclick="toggleFavorite(this, '2506.05787v2', 'EASG-Bench: Video Q&A Benchmark with Egocentric Action Scene Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250606253v1-bridging-perspectives-a-survey-on-cross-view-collaborative-intellige.html">Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision</a></td>
  <td>æå‡ºè·¨è§†è§’åä½œæ™ºèƒ½ä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„è§†è§’èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06253v1" data-paper-url="./papers/250606253v1-bridging-perspectives-a-survey-on-cross-view-collaborative-intellige.html" onclick="toggleFavorite(this, '2506.06253v1', 'Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250606026v2-o-mama-learning-object-mask-matching-between-egocentric-and-exocentr.html">O-MaMa: Learning Object Mask Matching between Egocentric and Exocentric Views</a></td>
  <td>æå‡ºO-MaMaä»¥è§£å†³ä¸åŒè§†è§’ä¸‹ç‰©ä½“åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06026v2" data-paper-url="./papers/250606026v2-o-mama-learning-object-mask-matching-between-egocentric-and-exocentr.html" onclick="toggleFavorite(this, '2506.06026v2', 'O-MaMa: Learning Object Mask Matching between Egocentric and Exocentric Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250606006v1-bootstrapping-world-models-from-dynamics-models-in-multimodal-founda.html">Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models</a></td>
  <td>æå‡ºé€šè¿‡åŠ¨æ€æ¨¡å‹å¼•å¯¼ä¸–ç•Œæ¨¡å‹ä»¥è§£å†³å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06006v1" data-paper-url="./papers/250606006v1-bootstrapping-world-models-from-dynamics-models-in-multimodal-founda.html" onclick="toggleFavorite(this, '2506.06006v1', 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250606281v1-terrafm-a-scalable-foundation-model-for-unified-multisensor-earth-ob.html">TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation</a></td>
  <td>æå‡ºTerraFMä»¥è§£å†³å¤šä¼ æ„Ÿå™¨åœ°çƒè§‚æµ‹æ•°æ®çš„ç»Ÿä¸€å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06281v1" data-paper-url="./papers/250606281v1-terrafm-a-scalable-foundation-model-for-unified-multisensor-earth-ob.html" onclick="toggleFavorite(this, '2506.06281v1', 'TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250605782v1-gazenlq-ego4d-natural-language-queries-challenge-2025.html">GazeNLQ @ Ego4D Natural Language Queries Challenge 2025</a></td>
  <td>æå‡ºGazeNLQä»¥è§£å†³Ego4Dè‡ªç„¶è¯­è¨€æŸ¥è¯¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">egocentric</span> <span class="paper-tag">Ego4D</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05782v1" data-paper-url="./papers/250605782v1-gazenlq-ego4d-natural-language-queries-challenge-2025.html" onclick="toggleFavorite(this, '2506.05782v1', 'GazeNLQ @ Ego4D Natural Language Queries Challenge 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250605719v1-you-only-estimate-once-unified-one-stage-real-time-category-level-ar.html">You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping</a></td>
  <td>æå‡ºYOEOæ–¹æ³•ä»¥è§£å†³å…³èŠ‚ç‰©ä½“ç±»åˆ«çº§6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05719v1" data-paper-url="./papers/250605719v1-you-only-estimate-once-unified-one-stage-real-time-category-level-ar.html" onclick="toggleFavorite(this, '2506.05719v1', 'You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250606578v1-a-deep-learning-approach-for-facial-attribute-manipulation-and-recon.html">A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance</a></td>
  <td>æå‡ºæ·±åº¦å­¦ä¹ æ–¹æ³•ä»¥è§£å†³ç›‘æ§è§†é¢‘ä¸­çš„é¢éƒ¨å±æ€§æ“æ§ä¸é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06578v1" data-paper-url="./papers/250606578v1-a-deep-learning-approach-for-facial-attribute-manipulation-and-recon.html" onclick="toggleFavorite(this, '2506.06578v1', 'A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250605890v1-unleashing-the-potential-of-consistency-learning-for-detecting-and-g.html">Unleashing the Potential of Consistency Learning for Detecting and Grounding Multi-Modal Media Manipulation</a></td>
  <td>æå‡ºä¸Šä¸‹æ–‡è¯­ä¹‰ä¸€è‡´æ€§å­¦ä¹ ä»¥è§£å†³å¤šæ¨¡æ€åª’ä½“æ“æ§æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05890v1" data-paper-url="./papers/250605890v1-unleashing-the-potential-of-consistency-learning-for-detecting-and-g.html" onclick="toggleFavorite(this, '2506.05890v1', 'Unleashing the Potential of Consistency Learning for Detecting and Grounding Multi-Modal Media Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250605952v2-mogo-residual-quantized-hierarchical-causal-transformer-for-high-qua.html">MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation</a></td>
  <td>æå‡ºMOGOä»¥è§£å†³é«˜è´¨é‡å®æ—¶3Däººç±»è¿åŠ¨ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05952v2" data-paper-url="./papers/250605952v2-mogo-residual-quantized-hierarchical-causal-transformer-for-high-qua.html" onclick="toggleFavorite(this, '2506.05952v2', 'MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)