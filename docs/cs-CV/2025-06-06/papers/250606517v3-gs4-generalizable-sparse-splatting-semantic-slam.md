---
layout: default
title: GS4: Generalizable Sparse Splatting Semantic SLAM
---

# GS4: Generalizable Sparse Splatting Semantic SLAM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06517" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06517v3</a>
  <a href="https://arxiv.org/pdf/2506.06517.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06517v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06517v3', 'GS4: Generalizable Sparse Splatting Semantic SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingqi Jiang, Chanho Kim, Chen Ziwen, Li Fuxin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-12-03)

**å¤‡æ³¨**: 15 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGS4ä»¥è§£å†³ä¼ ç»ŸSLAMåœ¨è¯­ä¹‰æ˜ å°„ä¸­çš„ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰SLAM` `é«˜æ–¯ç‚¹äº‘` `3Dæ˜ å°„` `æ·±åº¦å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª` `å¢å¼ºç°å®` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SLAMæ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡è¯­ä¹‰åœ°å›¾æ—¶é¢ä¸´é€Ÿåº¦æ…¢å’Œé«˜æ–¯ç‚¹ä½¿ç”¨è¿‡å¤šçš„é—®é¢˜ã€‚
2. GS4é€šè¿‡å‰é¦ˆç½‘ç»œå¢é‡æ„å»º3Dé«˜æ–¯ç‚¹ï¼Œç»“åˆé¢œè‰²å’Œè¯­ä¹‰é¢„æµ‹ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚
3. åœ¨ScanNetå’ŒScanNet++åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGS4å±•ç°å‡ºæœ€å…ˆè¿›çš„è¯­ä¹‰SLAMæ€§èƒ½ï¼Œå¹¶åœ¨NYUv2å’ŒTUM RGB-Dæ•°æ®é›†ä¸Šå®ç°é›¶æ ·æœ¬è¿ç§»çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„SLAMç®—æ³•åœ¨ç›¸æœºè·Ÿè¸ªæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†é€šå¸¸ç”Ÿæˆçš„ä¸å®Œæ•´ä¸”ä½åˆ†è¾¨ç‡çš„åœ°å›¾ä¸è¯­ä¹‰é¢„æµ‹çš„é›†æˆåº¦è¾ƒä½ã€‚è¿‘æœŸçš„ç ”ç©¶å°†é«˜æ–¯ç‚¹äº‘ï¼ˆGaussian Splatting, GSï¼‰é›†æˆåˆ°SLAMä¸­ï¼Œä»¥å®ç°å¯†é›†ä¸”é€¼çœŸçš„3Dæ˜ å°„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºGSçš„SLAMæ–¹æ³•éœ€è¦é€åœºæ™¯ä¼˜åŒ–ï¼Œé€Ÿåº¦æ…¢ä¸”æ¶ˆè€—è¿‡å¤šé«˜æ–¯ç‚¹ã€‚æœ¬æ–‡æå‡ºGS4ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯æ³›åŒ–çš„åŸºäºGSçš„è¯­ä¹‰SLAMç³»ç»Ÿã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒGS4è¿è¡Œé€Ÿåº¦å¿«10å€ï¼Œä½¿ç”¨çš„é«˜æ–¯ç‚¹å°‘10å€ï¼Œå¹¶åœ¨é¢œè‰²ã€æ·±åº¦ã€è¯­ä¹‰æ˜ å°„å’Œç›¸æœºè·Ÿè¸ªæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚GS4ä»RGB-Dè§†é¢‘æµä¸­å¢é‡æ„å»ºå’Œæ›´æ–°ä¸€ç»„3Dé«˜æ–¯ç‚¹ï¼Œé‡‡ç”¨å‰é¦ˆç½‘ç»œè¿›è¡Œå¤„ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸSLAMåœ¨è¯­ä¹‰æ˜ å°„ä¸­ç”Ÿæˆä½åˆ†è¾¨ç‡å’Œä¸å®Œæ•´åœ°å›¾çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•éœ€è¦é€åœºæ™¯ä¼˜åŒ–ï¼Œå¯¼è‡´é€Ÿåº¦æ…¢ä¸”é«˜æ–¯ç‚¹æ¶ˆè€—è¿‡å¤šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGS4é€šè¿‡å‰é¦ˆç½‘ç»œä»RGB-Dè§†é¢‘æµä¸­å¢é‡æ„å»º3Dé«˜æ–¯ç‚¹ï¼Œç»“åˆé¢œè‰²å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œé¿å…äº†å†—ä½™å¹¶æé«˜äº†å¤„ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGS4çš„æ•´ä½“æ¶æ„åŒ…æ‹¬é«˜æ–¯é¢„æµ‹æ¨¡å‹å’Œé«˜æ–¯ç²¾ç‚¼ç½‘ç»œã€‚é«˜æ–¯é¢„æµ‹æ¨¡å‹ä»è¾“å…¥å¸§ä¸­ä¼°è®¡ç¨€ç–çš„é«˜æ–¯å‚æ•°ï¼Œè€Œé«˜æ–¯ç²¾ç‚¼ç½‘ç»œåˆ™åˆå¹¶æ–°é«˜æ–¯ç‚¹ä¸ç°æœ‰é›†åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šGS4æ˜¯é¦–ä¸ªå¯æ³›åŒ–çš„åŸºäºGSçš„è¯­ä¹‰SLAMç³»ç»Ÿï¼Œè¿è¡Œé€Ÿåº¦å¿«10å€ï¼Œä½¿ç”¨çš„é«˜æ–¯ç‚¹å°‘10å€ï¼Œæ˜¾è‘—æå‡äº†è·Ÿè¸ªç²¾åº¦å’Œè¯­ä¹‰æ˜ å°„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é«˜æ–¯é¢„æµ‹æ¨¡å‹ä¸­ï¼Œé‡‡ç”¨äº†ç»Ÿä¸€çš„éª¨å¹²ç½‘ç»œæ¥å¤„ç†é¢œè‰²å’Œè¯­ä¹‰é¢„æµ‹ï¼›é«˜æ–¯ç²¾ç‚¼ç½‘ç»œé€šè¿‡é¿å…å†—ä½™æ¥ä¼˜åŒ–é«˜æ–¯ç‚¹çš„ä½¿ç”¨ï¼›åœ¨æ˜¾è‘—å§¿æ€å˜åŒ–æ—¶ï¼Œä»…è¿›è¡Œ1-5æ¬¡è”åˆé«˜æ–¯-å§¿æ€ä¼˜åŒ–ä»¥å‡å°‘æ¼‚ç§»å’Œæé«˜è·Ÿè¸ªç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨çœŸå®ä¸–ç•Œçš„ScanNetå’ŒScanNet++åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGS4å±•ç°å‡ºæœ€å…ˆè¿›çš„è¯­ä¹‰SLAMæ€§èƒ½ï¼Œè¿è¡Œé€Ÿåº¦æå‡10å€ï¼Œä½¿ç”¨çš„é«˜æ–¯ç‚¹å‡å°‘10å€ã€‚æ­¤å¤–ï¼ŒGS4åœ¨NYUv2å’ŒTUM RGB-Dæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬è¿ç§»ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GS4çš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„è¯­ä¹‰åœ°å›¾ï¼ŒGS4èƒ½å¤Ÿæ”¯æŒæ›´æ™ºèƒ½çš„ç¯å¢ƒç†è§£å’Œå†³ç­–åˆ¶å®šï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒGS4å¯èƒ½åœ¨å®æ—¶åœºæ™¯é‡å»ºå’Œäººæœºäº¤äº’ç­‰æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional SLAM algorithms excel at camera tracking, but typically produce incomplete and low-resolution maps that are not tightly integrated with semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods require per-scene optimization that is slow and consumes an excessive number of Gaussians. We present GS4, the first generalizable GS-based semantic SLAM system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer Gaussians, and achieves state-of-the-art performance across color, depth, semantic mapping and camera tracking. From an RGB-D video stream, GS4 incrementally builds and updates a set of 3D Gaussians using a feed-forward network. First, the Gaussian Prediction Model estimates a sparse set of Gaussian parameters from input frame, which integrates both color and semantic prediction with the same backbone. Then, the Gaussian Refinement Network merges new Gaussians with the existing set while avoiding redundancy. Finally, when significant pose changes are detected, we perform only 1-5 iterations of joint Gaussian-pose optimization to correct drift, remove floaters, and further improve tracking accuracy. Experiments on the real-world ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM performance, with strong generalization capability shown through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.

