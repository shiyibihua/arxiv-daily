---
layout: default
title: GazeNLQ @ Ego4D Natural Language Queries Challenge 2025
---

# GazeNLQ @ Ego4D Natural Language Queries Challenge 2025

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05782" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05782v1</a>
  <a href="https://arxiv.org/pdf/2506.05782.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05782v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05782v1', 'GazeNLQ @ Ego4D Natural Language Queries Challenge 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei-Cheng Lin, Chih-Ming Lien, Chen Lo, Chia-Hung Yeh

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/stevenlin510/GazeNLQ)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGazeNLQä»¥è§£å†³Ego4Dè‡ªç„¶è¯­è¨€æŸ¥è¯¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ³¨è§†ä¼°è®¡` `å¯¹æ¯”å­¦ä¹ ` `è§†é¢‘æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆåˆ©ç”¨æ³¨è§†ä¿¡æ¯æ¥å¢å¼ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„åŒ¹é…èƒ½åŠ›ã€‚
2. GazeNLQé€šè¿‡å¼•å…¥æ³¨è§†ä¼°è®¡å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæå‡äº†è§†é¢‘ç‰‡æ®µæ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGazeNLQåœ¨å®šä½ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå–å¾—äº†è¾ƒé«˜çš„R1å¾—åˆ†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æŠ¥å‘Šå±•ç¤ºäº†æˆ‘ä»¬åœ¨CVPR 2025çš„Ego4Dè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆNLQï¼‰æŒ‘æˆ˜ä¸­çš„è§£å†³æ–¹æ¡ˆã€‚ä»¥ä½©æˆ´è€…è§†è§’æ•æ‰çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­ï¼Œæ³¨è§†ä½œä¸ºä¸€ç§å…³é”®çš„éè¯­è¨€äº¤æµçº¿ç´¢ï¼Œåæ˜ äº†è§†è§‰æ³¨æ„åŠ›å¹¶æä¾›äº†å¯¹äººç±»æ„å›¾å’Œè®¤çŸ¥çš„æ´å¯Ÿã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•GazeNLQï¼Œåˆ©ç”¨æ³¨è§†æ¥æ£€ç´¢ä¸ç»™å®šè‡ªç„¶è¯­è¨€æŸ¥è¯¢åŒ¹é…çš„è§†é¢‘ç‰‡æ®µã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»è§†é¢‘ä¸­ç›´æ¥è¿›è¡Œæ³¨è§†ä¼°è®¡ã€‚ä¼°è®¡çš„æ³¨è§†ç”¨äºå¢å¼ºæ¨¡å‹ä¸­çš„è§†é¢‘è¡¨ç¤ºï¼Œä»è€Œæé«˜å®šä½ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGazeNLQåœ¨R1@IoU0.3å’ŒR1@IoU0.5çš„å¾—åˆ†åˆ†åˆ«ä¸º27.82å’Œ18.68ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨https://github.com/stevenlin510/GazeNLQè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„æ³¨è§†ä¿¡æ¯æ¥å¢å¼ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„åŒ¹é…èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€é¢†åŸŸçš„è¡¨ç°ä¸è¶³ï¼Œæœªèƒ½å……åˆ†æŒ–æ˜æ³¨è§†ä½œä¸ºéè¯­è¨€çº¿ç´¢çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGazeNLQçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¿›è¡Œæ³¨è§†ä¼°è®¡ï¼Œä»è€Œå¢å¼ºè§†é¢‘è¡¨ç¤ºï¼Œè¿›è€Œæé«˜è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„æ£€ç´¢ç²¾åº¦ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„è§†è§‰å…³æ³¨ç‚¹ä¸æŸ¥è¯¢ä¹‹é—´çš„å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ³¨è§†ä¼°è®¡æ¨¡å—å’Œè§†é¢‘è¡¨ç¤ºå¢å¼ºæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹æ³¨è§†è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå°†ä¼°è®¡çš„æ³¨è§†ä¿¡æ¯èå…¥è§†é¢‘è¡¨ç¤ºä¸­ï¼Œä»¥æå‡åç»­çš„æŸ¥è¯¢åŒ¹é…æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šGazeNLQçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ³¨è§†ä¼°è®¡ä¸è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢ç›¸ç»“åˆï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æå‡æ³¨è§†çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºè§†è§‰ç‰¹å¾çš„æ£€ç´¢æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…å¾€å¾€å¿½è§†äº†æ³¨è§†ä¿¡æ¯çš„ä½œç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ³¨è§†ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶è®¾è®¡äº†é€‚åˆè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åŠ¨æ€åœºæ™¯ä¸­çš„æ³¨è§†å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGazeNLQåœ¨R1@IoU0.3å’ŒR1@IoU0.5çš„å¾—åˆ†åˆ†åˆ«è¾¾åˆ°äº†27.82å’Œ18.68ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§†é¢‘æ£€ç´¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è™šæ‹Ÿç°å®å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸è§†é¢‘å†…å®¹çš„åŒ¹é…ç²¾åº¦ï¼ŒGazeNLQèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºç²¾å‡†çš„ä¿¡æ¯æ£€ç´¢ä½“éªŒï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This report presents our solution to the Ego4D Natural Language Queries (NLQ) Challenge at CVPR 2025. Egocentric video captures the scene from the wearer's perspective, where gaze serves as a key non-verbal communication cue that reflects visual attention and offer insights into human intention and cognition. Motivated by this, we propose a novel approach, GazeNLQ, which leverages gaze to retrieve video segments that match given natural language queries. Specifically, we introduce a contrastive learning-based pretraining strategy for gaze estimation directly from video. The estimated gaze is used to augment video representations within proposed model, thereby enhancing localization accuracy. Experimental results show that GazeNLQ achieves R1@IoU0.3 and R1@IoU0.5 scores of 27.82 and 18.68, respectively. Our code is available at https://github.com/stevenlin510/GazeNLQ.

