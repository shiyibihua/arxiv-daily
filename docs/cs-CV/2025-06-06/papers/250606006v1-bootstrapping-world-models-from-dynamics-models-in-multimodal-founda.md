---
layout: default
title: Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models
---

# Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06006" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06006v1</a>
  <a href="https://arxiv.org/pdf/2506.06006.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06006v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06006v1', 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡åŠ¨æ€æ¨¡å‹å¼•å¯¼ä¸–ç•Œæ¨¡å‹ä»¥è§£å†³å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹` `åŠ¨æ€æ¨¡å‹` `ä¸–ç•Œæ¨¡å‹` `å¼±ç›‘ç£å­¦ä¹ ` `å›¾åƒç¼–è¾‘` `æ¨ç†éªŒè¯` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼€æºåŸºç¡€æ¨¡å‹åœ¨æ„å»ºç°å®ä¸–ç•Œæ¨¡å‹å’ŒåŠ¨æ€æ¨¡å‹æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é€šè¿‡è¯­è¨€è¡¨è¾¾åŠ¨ä½œæ—¶ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åŠ¨æ€æ¨¡å‹å¼•å¯¼ä¸–ç•Œæ¨¡å‹çš„ä¸¤ç§ç­–ç•¥ï¼Œåˆ†åˆ«æ˜¯å¼±ç›‘ç£å­¦ä¹ å’Œæ¨ç†æ—¶éªŒè¯ï¼Œä»¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨Aurora-Benchçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°15%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰ä¸è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨æ„å»ºç°å®ä¸–ç•Œæ¨¡å‹å’ŒåŠ¨æ€æ¨¡å‹æ–¹é¢çš„èƒ½åŠ›ï¼Œå‘ç°é€šè¿‡ç›‘ç£å¾®è°ƒåŠ¨æ€æ¨¡å‹æ¯”è·å–ä¸–ç•Œæ¨¡å‹æ›´ä¸ºç®€å•ã€‚ç ”ç©¶æå‡ºäº†ä¸¤ç§ç­–ç•¥æ¥åˆ©ç”¨åŠ¨æ€æ¨¡å‹å¼•å¯¼ä¸–ç•Œæ¨¡å‹ï¼š1) ä»åˆæˆæ•°æ®ä¸­è¿›è¡Œå¼±ç›‘ç£å­¦ä¹ ï¼Œ2) æ¨ç†æ—¶éªŒè¯ã€‚é€šè¿‡è¿™ä¸¤ç§ç­–ç•¥ï¼ŒåŠ¨æ€æ¨¡å‹èƒ½å¤Ÿä¸ºæœªæ ‡è®°çš„è§†é¢‘å¸§å¯¹æ³¨é‡ŠåŠ¨ä½œï¼Œå¹¶ä¸ºä¸–ç•Œæ¨¡å‹çš„å¤šä¸ªæ ·æœ¬åˆ†é…å¥–åŠ±ï¼Œä»è€Œåœ¨æ¨ç†æ—¶æœ‰æ•ˆæŒ‡å¯¼æœç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨Aurora-Benchçš„ä»¥åŠ¨ä½œä¸ºä¸­å¿ƒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œå°¤å…¶åœ¨çœŸå®ä¸–ç•Œå­é›†ä¸Šæå‡äº†15%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è§†è§‰ä¸è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨æ„å»ºç°å®ä¸–ç•Œæ¨¡å‹å’ŒåŠ¨æ€æ¨¡å‹æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é€šè¿‡è¯­è¨€è¡¨è¾¾åŠ¨ä½œçš„åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•çš„æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŠ¨æ€æ¨¡å‹æ¥å¼•å¯¼ä¸–ç•Œæ¨¡å‹çš„æ„å»ºï¼Œé€šè¿‡ä¸¤ç§ç­–ç•¥æ¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ¨ç†æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å…‹æœç°æœ‰æ¨¡å‹åœ¨æ•°æ®æ ‡æ³¨å’Œæ¨ç†æ•ˆç‡ä¸Šçš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨æ€æ¨¡å‹å’Œä¸–ç•Œæ¨¡å‹ã€‚åŠ¨æ€æ¨¡å‹é€šè¿‡å¼±ç›‘ç£å­¦ä¹ ä»åˆæˆæ•°æ®ä¸­ç”Ÿæˆæ ‡æ³¨ï¼Œå¹¶åœ¨æ¨ç†æ—¶ä¸ºä¸–ç•Œæ¨¡å‹æä¾›å¥–åŠ±ä¿¡å·ï¼Œä»¥ä¼˜åŒ–å…¶è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†é€šè¿‡åŠ¨æ€æ¨¡å‹è¿›è¡Œä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„ä¸¤ç§ç­–ç•¥ï¼Œå°¤å…¶æ˜¯å¼•å…¥äº†åŸºäºé‡è¦æ€§åŠ æƒçš„å›¾åƒæ ‡è®°æœºåˆ¶ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŠ¨æ€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„ç›®æ ‡å‡½æ•°ï¼Œä½¿å¾—è§‚å¯Ÿå¯¹ä¸­çš„å›¾åƒæ ‡è®°æ ¹æ®å…¶é‡è¦æ€§è¿›è¡ŒåŠ æƒã€‚æ­¤å¤–ï¼Œæ¨ç†æ—¶çš„å¥–åŠ±åˆ†é…æœºåˆ¶ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜æœç´¢æ•ˆç‡å’Œç»“æœè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨Aurora-Benchçš„ä»¥åŠ¨ä½œä¸ºä¸­å¿ƒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œå­é›†ä¸Šæå‡äº†15%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ‰€æœ‰å­é›†ä¸Šçš„å¹³å‡äººç±»è¯„ä¼°è¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººæ§åˆ¶å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒç¼–è¾‘ã€è§†é¢‘ç†è§£å’Œè‡ªåŠ¨åŒ–å†³ç­–ç­‰å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To what extent do vision-and-language foundation models possess a realistic world model (observation $\times$ action $\rightarrow$ observation) and a dynamics model (observation $\times$ observation $\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.

