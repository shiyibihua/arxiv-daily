---
layout: default
title: VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning
---

# VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06097" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.06097v1</a>
  <a href="https://arxiv.org/pdf/2506.06097.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06097v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06097v1', 'VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, Yali Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VideoChat-A1‰ª•Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁêÜËß£ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `ÈìæÂºèÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ËßÜÈ¢ëÈóÆÁ≠î` `Êô∫ËÉΩ‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈïøËßÜÈ¢ëÁêÜËß£‰∏äÂ≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÈöæ‰ª•ÊúâÊïàÂ§ÑÁêÜÈïøÊó∂Èó¥‰∏ä‰∏ãÊñá„ÄÇ
2. Êú¨ÊñáÊèêÂá∫VideoChat-A1ÔºåÈÄöËøáÈìæÂºèÈïúÂ§¥Êé®ÁêÜÈÄêÊ≠•ÂàÜÊûêÈïøËßÜÈ¢ëÔºåÊ®°Êãü‰∫∫Á±ªÊÄùÁª¥ËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVideoChat-A1Âú®ÈïøËßÜÈ¢ëÈóÆÁ≠îÂü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂ§ö‰∏™Âº∫Âü∫Á∫øÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåËßÜÈ¢ëÁêÜËß£ÁöÑËøõÂ±ï‰∏ªË¶ÅÂæóÁõä‰∫éÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®ÂàÜÊûêÁü≠ËßÜÈ¢ëÊó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÁêÜËß£ÈïøËßÜÈ¢ëÊó∂Âç¥Èù¢‰∏¥Âõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜVideoChat-A1Ôºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÈïøËßÜÈ¢ë‰ª£ÁêÜËåÉÂºè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåVideoChat-A1ÈÄöËøáÈìæÂºèÈïúÂ§¥Êé®ÁêÜÔºåÈÄêÊ≠•ÈÄâÊã©‰∏éÁî®Êà∑ÈóÆÈ¢òÁõ∏ÂÖ≥ÁöÑÈïúÂ§¥ÔºåÂπ∂ËøõË°åÁ≤óÂà∞ÁªÜÁöÑÂàÜÊûê„ÄÇÈÄöËøáÊ≤øÁùÄÈïúÂ§¥ÈìæËøõË°åÂ§öÊ®°ÊÄÅÊé®ÁêÜÔºåVideoChat-A1ËÉΩÂ§üÊúâÊïàÊ®°Êãü‰∫∫Á±ªÁöÑÊÄùÁª¥ËøáÁ®ãÔºå‰∫§‰∫íÂºèÂú∞ÂèëÁé∞ÈÄÇÂêàÁöÑÊó∂Èó¥‰∏ä‰∏ãÊñá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideoChat-A1Âú®‰∏ªÊµÅÈïøËßÜÈ¢ëÈóÆÁ≠îÂü∫ÂáÜ‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÂº∫Âü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑÂÖ≥ÈîÆÈóÆÈ¢òÔºåÂç≥Áé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàËØÜÂà´ÂíåÂà©Áî®ÈïøËßÜÈ¢ë‰∏≠ÁöÑÁõ∏ÂÖ≥ÈïúÂ§¥ÔºåÂØºËá¥ÁêÜËß£ËÉΩÂäõÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVideoChat-A1ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÈìæÂºèÈïúÂ§¥Êé®ÁêÜÔºåÈÄêÊ≠•ÈÄâÊã©‰∏éÁî®Êà∑ÈóÆÈ¢òÁõ∏ÂÖ≥ÁöÑÈïúÂ§¥ÔºåÂπ∂ËøõË°åÊ∑±ÂÖ•ÂàÜÊûêÔºå‰ª•Ê®°Êãü‰∫∫Á±ªÁöÑÊÄùÁª¥ËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÈïúÂ§¥ÈÄâÊã©Ê®°Âùó„ÄÅÁ≤óÂà∞ÁªÜÂàÜÊûêÊ®°ÂùóÂíåÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÁ≥ªÁªüÊ†πÊçÆÁî®Êà∑ÈóÆÈ¢òÈÄâÊã©Áõ∏ÂÖ≥ÈïúÂ§¥ÔºåÁÑ∂ÂêéÂØπËøô‰∫õÈïúÂ§¥ËøõË°åÁªÜËá¥ÁöÑÂàÜÊûêÔºåÊúÄÂêéËøõË°åÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ª•Êï¥Âêà‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVideoChat-A1ÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÈìæÂºèÈïúÂ§¥Êé®ÁêÜÊú∫Âà∂ÔºåËøô‰∏ÄÊú∫Âà∂‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÈÄêÊ≠•ÁêÜËß£ÈïøËßÜÈ¢ëÁöÑÂ§çÊùÇÁªìÊûÑÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞Â§ÑÁêÜÂÜó‰ΩôÊàñÂô™Â£∞‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåVideoChat-A1ÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÈïúÂ§¥ÈÄâÊã©ÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏≠ÂºïÂÖ•‰∫ÜÂ§öÊ®°ÊÄÅËûçÂêàÊäÄÊúØÔºå‰ª•ÊèêÈ´òÊé®ÁêÜÁöÑÊúâÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideoChat-A1Âú®VideoMMEÂíåEgoSchemaÁ≠â‰∏ªÊµÅÈïøËßÜÈ¢ëÈóÆÁ≠îÂü∫ÂáÜ‰∏äÂàÜÂà´ËææÂà∞‰∫Ü77.0Âíå70.1ÁöÑÂàÜÊï∞ÔºåË∂ÖË∂ä‰∫ÜÂº∫Âü∫Á∫øÊ®°ÂûãÔºàÂ¶ÇIntern2.5VL-8BÂíåInternVideo2.5-8BÔºâËææ10.8%Âíå6.2%„ÄÇ‰∏éÈ¢ÜÂÖàÁöÑÈó≠Ê∫êÊ®°ÂûãGPT-4oÂíåGemini 1.5 ProÁõ∏ÊØîÔºåVideoChat-A1Âú®ËæìÂÖ•Â∏ßÂíåÊé®ÁêÜÊó∂Èó¥‰∏äÂàÜÂà´ÂáèÂ∞ë‰∫Ü7%Âíå12%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÈóÆÁ≠îÁ≥ªÁªü„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅÊïôËÇ≤ËßÜÈ¢ëÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÊèêÂçáÈïøËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºåVideoChat-A1ÂèØ‰ª•‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Á≤æÂáÜÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢Âíå‰∫§‰∫í‰ΩìÈ™åÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The recent advance in video understanding has been driven by multimodal large language models (MLLMs). But these MLLMs are good at analyzing short videos, while suffering from difficulties in understanding videos with a longer context. To address this difficulty, several agent paradigms have recently been proposed, using MLLMs as agents for retrieving extra contextual knowledge in a long video. However, most existing agents ignore the key fact that a long video is composed with multiple shots, i.e., to answer the user question from a long video, it is critical to deeply understand its relevant shots like human. Without such insight, these agents often mistakenly find redundant even noisy temporal context, restricting their capacity for long video understanding. To fill this gap, we propose VideoChat-A1, a novel long video agent paradigm. Different from the previous works, our VideoChat-A1 can deeply think with long videos, via a distinct chain-of-shot reasoning paradigm. More specifically, it can progressively select the relevant shots of user question, and look into these shots in a coarse-to-fine partition. By multi-modal reasoning along the shot chain, VideoChat-A1 can effectively mimic step-by-step human thinking process, allowing to interactively discover preferable temporal context for thoughtful understanding in long videos. Extensive experiments show that, our VideoChat-A1 achieves the state-of-the-art performance on the mainstream long video QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema, outperforming its strong baselines (e.g., Intern2.5VL-8B and InternVideo2.5-8B), by up to 10.8\% and 6.2\%. Compared to leading close-source GPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with 7\% input frames and 12\% inference time on average.

