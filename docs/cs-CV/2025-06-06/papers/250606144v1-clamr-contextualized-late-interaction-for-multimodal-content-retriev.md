---
layout: default
title: CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval
---

# CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06144" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06144v1</a>
  <a href="https://arxiv.org/pdf/2506.06144.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06144v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06144v1', 'CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David Wan, Han Wang, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal

**åˆ†ç±»**: cs.CV, cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 18 pages. Code and data: https://github.com/meetdavidwan/clamr

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLaMRä»¥è§£å†³å¤šæ¨¡æ€è§†é¢‘å†…å®¹æ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `è§†é¢‘ç†è§£` `åŠ¨æ€æ¨¡æ€é€‰æ‹©` `åˆæˆæ•°æ®é›†` `æ¨¡æ€æ„ŸçŸ¥æŸå¤±` `é•¿è§†é¢‘é—®ç­”` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€è§†é¢‘æ£€ç´¢æ–¹æ³•é€šå¸¸å°†ä¸åŒæ¨¡æ€è§†ä¸ºç‹¬ç«‹æ¥æºï¼Œå¯¼è‡´æ£€ç´¢ç»“æœå™ªå£°å¤§ä¸”æ•ˆæœä¸ä½³ã€‚
2. CLaMRé€šè¿‡ç»Ÿä¸€ç¼–ç å¤šæ¨¡æ€æ•°æ®ï¼Œåˆ©ç”¨åŠ¨æ€æ¨¡æ€é€‰æ‹©æœºåˆ¶ï¼Œæå‡æ£€ç´¢çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ã€‚
3. åœ¨MultiVENT 2.0++å’ŒMSRVTTæµ‹è¯•é›†ä¸Šï¼ŒCLaMRåœ¨nDCG@10ä¸Šåˆ†åˆ«æ¯”æœ€ä½³å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ£€ç´¢å™¨æå‡äº†25.6å’Œ35.4ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨çº¿è§†é¢‘å†…å®¹å…·æœ‰ä¸°å¯Œçš„å¤šæ¨¡æ€ç‰¹æ€§ï¼Œé€šå¸¸åŒ…æ‹¬è§†è§‰ã€è¯­éŸ³ã€ç¯å¢ƒéŸ³é¢‘å’Œå±å¹•æ–‡æœ¬ã€‚ç°æœ‰çš„æ£€ç´¢ç³»ç»Ÿå¾€å¾€å°†è¿™äº›æ¨¡æ€è§†ä¸ºç‹¬ç«‹çš„æ£€ç´¢æºï¼Œå¯¼è‡´æ£€ç´¢æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºCLaMRï¼Œä¸€ä¸ªå¤šæ¨¡æ€çš„æ™šæœŸäº¤äº’æ£€ç´¢å™¨ï¼Œèƒ½å¤Ÿè”åˆç´¢å¼•è§†é¢‘å¸§ã€è½¬å½•è¯­éŸ³ã€å±å¹•æ–‡æœ¬å’Œå…ƒæ•°æ®ã€‚CLaMRé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œè¿›è¡Œç¼–ç ï¼Œä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼Œå¹¶é€šè¿‡å¼•å…¥MultiVENT 2.0++åˆæˆæ•°æ®é›†å’Œæ¨¡æ€æ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥æå‡åŠ¨æ€æ¨¡æ€é€‰æ‹©èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLaMRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ£€ç´¢å™¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è§†é¢‘å†…å®¹æ£€ç´¢ä¸­æ¨¡æ€ç‹¬ç«‹å¤„ç†å¯¼è‡´çš„å™ªå£°å’Œæ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ¨¡æ€é—´çš„å…³è”æ€§ï¼Œå½±å“äº†æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLaMRé€šè¿‡è”åˆç¼–ç è§†é¢‘å¸§ã€è½¬å½•è¯­éŸ³ã€å±å¹•æ–‡æœ¬å’Œå…ƒæ•°æ®ï¼Œåˆ©ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œæ¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼Œå¹¶é€šè¿‡åŠ¨æ€æ¨¡æ€é€‰æ‹©æœºåˆ¶æ¥æå‡æ£€ç´¢æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLaMRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡æ€ç¼–ç ã€åŠ¨æ€æ¨¡æ€é€‰æ‹©å’Œæ£€ç´¢æ¨¡å—ã€‚é¦–å…ˆï¼Œè¾“å…¥çš„å¤šæ¨¡æ€æ•°æ®ç»è¿‡ç»Ÿä¸€ç¼–ç ï¼Œç„¶åæ ¹æ®æŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„æ¨¡æ€è¿›è¡Œæ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLaMRçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†MultiVENT 2.0++åˆæˆæ•°æ®é›†å’Œæ¨¡æ€æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œå‰è€…ä¸ºå¤šæ¨¡æ€æ£€ç´¢æä¾›äº†ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼Œåè€…åˆ™ä¼˜åŒ–äº†æ¨¡æ€ä½¿ç”¨çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒCLaMRç»“åˆäº†æ ‡å‡†å¯¹æ¯”æŸå¤±å’Œæ¨¡æ€æ„ŸçŸ¥æŸå¤±ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å„æ¨¡æ€çš„ä½¿ç”¨ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†å…ˆè¿›çš„å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œï¼Œä»¥æå‡ç‰¹å¾æå–çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CLaMRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨MultiVENT 2.0++ä¸Šï¼ŒCLaMRçš„nDCG@10æ¯”æœ€ä½³å•æ¨¡æ€æ£€ç´¢å™¨æå‡äº†25.6ï¼Œæ¯”æœ€ä½³å¤šæ¨¡æ€æ£€ç´¢å™¨æå‡äº†35.4ã€‚æ­¤å¤–ï¼Œåœ¨é•¿è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­ï¼ŒCLaMRåœ¨Video-MMEä¸Šæ¯”LanguageBindæå‡äº†3.50%ï¼Œåœ¨LongVideoBenchä¸Šæ¯”å¯†é›†é‡‡æ ·æå‡äº†1.42%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLaMRçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€è§†é¢‘æ£€ç´¢é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è§†é¢‘é—®ç­”ã€è§†é¢‘å†…å®¹æ¨èå’Œä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ä¸­ã€‚é€šè¿‡æå‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼ŒCLaMRèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºç²¾å‡†çš„å†…å®¹æ¨èï¼Œè¿›è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€æ•°æ®å¤„ç†é¢†åŸŸï¼Œå¦‚å›¾åƒä¸æ–‡æœ¬çš„è”åˆæ£€ç´¢ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Online video web content is richly multimodal: a single video blends vision, speech, ambient audio, and on-screen text. Retrieval systems typically treat these modalities as independent retrieval sources, which can lead to noisy and subpar retrieval. We explore multimodal video content retrieval, where relevance can be scored from one particular modality or jointly across multiple modalities simultaneously. Consequently, an effective retriever must dynamically choose which modality (or set of modalities) best addresses the query. We introduce CLaMR, a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. CLaMR jointly encodes all modalities with a unified multimodal backbone for improved contextualization and is trained to enhance dynamic modality selection via two key innovations. First, given the lack of training data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale synthetic training dataset built on MultiVENT 2.0 (event-centric videos in various languages paired with queries) with modality-targeted queries. Next, we propose a modality-aware loss that jointly trains according to a standard contrastive objective alongside an objective for learning correct modality usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation strategies, such as averaging similarities for baseline retrievers, degrade performance by introducing noise from irrelevant modalities. In contrast, CLaMR consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever. We illustrate CLaMR's downstream utility on long-video QA, retrieving relevant frames and obtaining a 3.50% boost over LanguageBind on Video-MME and 1.42% over dense sampling on LongVideoBench.

