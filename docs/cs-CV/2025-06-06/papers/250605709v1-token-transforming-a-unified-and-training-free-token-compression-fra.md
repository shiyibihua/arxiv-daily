---
layout: default
title: Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration
---

# Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05709" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05709v1</a>
  <a href="https://arxiv.org/pdf/2506.05709.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05709v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05709v1', 'Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fanhu Zeng, Deli Yu, Zhenglun Kong, Hao Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToken Transformingæ¡†æ¶ä»¥åŠ é€Ÿè§†è§‰Transformerå¹¶å‡å°‘ä¿¡æ¯æŸå¤±**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†è§‰Transformer` `ä»¤ç‰Œå‹ç¼©` `æ¨¡å‹åŠ é€Ÿ` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `ä¿¡æ¯ä¿ç•™` `æ— è®­ç»ƒåŠ é€Ÿ` `å¯†é›†é¢„æµ‹ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰Transformerå‹ç¼©æ–¹æ³•ä¸»è¦ä¾èµ–äºä»¤ç‰Œå‰ªææˆ–åˆå¹¶ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œæ€§èƒ½æ¢å¤çš„éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå¯¹å¤šçš„Token Transformingæ¡†æ¶ï¼Œå°†ä»¤ç‰Œå‹ç¼©è§†ä¸ºä»¤ç‰ŒçŸ©é˜µå˜æ¢ï¼Œæ—¨åœ¨ä¿ç•™æ›´å¤šä¿¡æ¯å¹¶å®ç°æ— è®­ç»ƒåŠ é€Ÿã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå‡å°‘è®¡ç®—é‡ï¼Œæå‡æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­ä¿æŒè‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰Transformeråœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†ç”±äºå…¶é«˜è®¡ç®—æˆæœ¬ï¼ŒåŠ¨æ€å‹ç¼©è§†è§‰Transformerçš„ç ”ç©¶å—åˆ°å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»¤ç‰Œå‰ªææˆ–åˆå¹¶ä¸Šï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å¹¶éœ€åç»­è®­ç»ƒæ¢å¤æ€§èƒ½ã€‚æœ¬æ–‡é‡æ–°æ€è€ƒä»¤ç‰Œå‡å°‘ï¼Œå°†å…¶ç»Ÿä¸€ä¸ºä»¤ç‰ŒçŸ©é˜µå˜æ¢çš„æ˜¾å¼å½¢å¼ï¼Œæå‡ºäº†ä¸€ç§å¤šå¯¹å¤šçš„Token Transformingæ¡†æ¶ï¼Œèƒ½å¤Ÿä¿ç•™æ›´å¤šä¿¡æ¯ï¼Œå®ç°æ— è®­ç»ƒåŠ é€Ÿã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯å‡å°‘40%çš„FLOPsï¼Œå¹¶å°†DeiT-SåŠ é€Ÿ1.5å€ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™0.1%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ‰©å±•åˆ°å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨è®¡ç®—æ€§èƒ½æƒè¡¡ã€é¢„ç®—å‡å°‘å’Œæ¨ç†åŠ é€Ÿæ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰Transformerå‹ç¼©æ–¹æ³•ä¸­ä¿¡æ¯æŸå¤±ä¸¥é‡çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»¤ç‰Œå‰ªæå’Œåˆå¹¶è¿‡ç¨‹ä¸­ï¼Œå¯¼è‡´åç»­è®­ç»ƒæ¢å¤æ€§èƒ½çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºToken Transformingæ¡†æ¶ï¼Œå°†ä»¤ç‰Œå‹ç¼©è¿‡ç¨‹è§†ä¸ºä»¤ç‰ŒçŸ©é˜µçš„å˜æ¢ï¼Œç»Ÿä¸€ç°æœ‰æ–¹æ³•ï¼Œç¡®ä¿ä¿¡æ¯ä¿ç•™æœ€å¤§åŒ–ï¼Œå¹¶å®ç°è®­ç»ƒæ— å…³çš„åŠ é€Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆå¯¹è¾“å…¥çš„ä»¤ç‰ŒçŸ©é˜µè¿›è¡Œå˜æ¢ï¼Œç„¶åé€šè¿‡å¤šå¯¹å¤šçš„æ˜ å°„å…³ç³»è¿›è¡Œå‹ç¼©ï¼Œæœ€åè¾“å‡ºå‹ç¼©åçš„ä»¤ç‰ŒçŸ©é˜µä¾›åç»­å¤„ç†ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ‰€æœ‰ç°æœ‰çš„ä»¤ç‰Œå‹ç¼©æ–¹æ³•æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±ï¼Œå¹¶å®ç°äº†è®­ç»ƒæ— å…³çš„åŠ é€Ÿï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„çŸ©é˜µå˜æ¢æŠ€æœ¯ï¼Œç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¿ç•™ä¿¡æ¯ï¼ŒåŒæ—¶è®¾ç½®äº†é€‚å½“çš„å‚æ•°ä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€‚åº”ä¸åŒçš„è§†è§‰ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒToken Transformingæ¡†æ¶èƒ½å¤Ÿå‡å°‘40%çš„FLOPsï¼Œå¹¶å°†DeiT-Sçš„æ¨ç†é€Ÿåº¦æå‡1.5å€ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™0.1%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„è®¡ç®—æ€§èƒ½æƒè¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è§†è§‰Transformerçš„æ¨ç†é€Ÿåº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‹ç¼©å’ŒåŠ é€Ÿä¸­ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision transformers have been widely explored in various vision tasks. Due to heavy computational cost, much interest has aroused for compressing vision transformer dynamically in the aspect of tokens. Current methods mainly pay attention to token pruning or merging to reduce token numbers, in which tokens are compressed exclusively, causing great information loss and therefore post-training is inevitably required to recover the performance. In this paper, we rethink token reduction and unify the process as an explicit form of token matrix transformation, in which all existing methods are constructing special forms of matrices within the framework. Furthermore, we propose a many-to-many Token Transforming framework that serves as a generalization of all existing methods and reserves the most information, even enabling training-free acceleration. We conduct extensive experiments to validate our framework. Specifically, we reduce 40% FLOPs and accelerate DeiT-S by $\times$1.5 with marginal 0.1% accuracy drop. Furthermore, we extend the method to dense prediction tasks including segmentation, object detection, depth estimation, and language model generation. Results demonstrate that the proposed method consistently achieves substantial improvements, offering a better computation-performance trade-off, impressive budget reduction and inference acceleration.

