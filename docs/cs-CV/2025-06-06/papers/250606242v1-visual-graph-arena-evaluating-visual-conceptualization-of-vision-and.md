---
layout: default
title: Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models
---

# Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06242" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06242v1</a>
  <a href="https://arxiv.org/pdf/2506.06242.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06242v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06242v1', 'Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰å›¾å½¢ç«æŠ€åœºä»¥è§£å†³è§†è§‰æ¦‚å¿µåŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å›¾å½¢ä»»åŠ¡` `è§†è§‰æŠ½è±¡` `åŒæ„æ£€æµ‹` `AIè§†è§‰ç†è§£` `æ¦‚å¿µåŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨æ¦‚å¿µåŒ–èƒ½åŠ›ä¸Šä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºè§†è§‰å›¾å½¢ç«æŠ€åœºï¼ˆVGAï¼‰ï¼Œé€šè¿‡å…­ä¸ªå›¾å½¢ä»»åŠ¡è¯„ä¼°AIçš„è§†è§‰æŠ½è±¡èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³è§†è§‰å½¢å¼å˜åŒ–å¸¦æ¥çš„æ¨ç†æŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºäººç±»åœ¨ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œç°æœ‰æ¨¡å‹åœ¨åŒæ„æ£€æµ‹å’Œè·¯å¾„/å¾ªç¯ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†AIæ¨¡å‹çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¿›å±•æ¨åŠ¨äº†è§†è§‰é—®ç­”é¢†åŸŸçš„çªç ´ã€‚ç„¶è€Œï¼Œâ€˜æ¦‚å¿µåŒ–â€™èƒ½åŠ›çš„ç¼ºå¤±ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå³åœ¨è§†è§‰å½¢å¼å˜åŒ–çš„æƒ…å†µä¸‹è¯†åˆ«å’Œæ¨ç†ç›¸åŒæ¦‚å¿µçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è§†è§‰å›¾å½¢ç«æŠ€åœºï¼ˆVGAï¼‰ï¼Œä¸€ä¸ªåŒ…å«å…­ä¸ªåŸºäºå›¾å½¢çš„ä»»åŠ¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡AIç³»ç»Ÿçš„è§†è§‰æŠ½è±¡èƒ½åŠ›ã€‚VGAä½¿ç”¨å¤šæ ·çš„å›¾å½¢å¸ƒå±€ï¼ˆå¦‚Kamada-Kawaiä¸å¹³é¢å›¾ï¼‰æ¥æµ‹è¯•ä¸è§†è§‰å½¢å¼æ— å…³çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡äººç±»åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡ ä¹è¾¾åˆ°äº†å®Œç¾å‡†ç¡®ç‡ï¼Œä½†æ¨¡å‹åœ¨åŒæ„æ£€æµ‹ä¸Šå®Œå…¨å¤±è´¥ï¼Œå¹¶ä¸”åœ¨è·¯å¾„/å¾ªç¯ä»»åŠ¡ä¸­è¡¨ç°æœ‰é™ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰AIæ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„åŸºæœ¬å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIç³»ç»Ÿåœ¨è§†è§‰æ¦‚å¿µåŒ–ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹è§†è§‰å½¢å¼å˜åŒ–æ—¶çš„æ¨ç†èƒ½åŠ›ç¼ºå¤±ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†åŒæ„æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œå¯¼è‡´æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºè§†è§‰å›¾å½¢ç«æŠ€åœºï¼ˆVGAï¼‰ï¼Œé€šè¿‡è®¾è®¡å¤šæ ·åŒ–çš„å›¾å½¢ä»»åŠ¡æ¥è¯„ä¼°å’Œæå‡AIçš„è§†è§‰æŠ½è±¡èƒ½åŠ›ï¼Œå¼ºè°ƒä¸è§†è§‰å½¢å¼æ— å…³çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGAåŒ…å«å…­ä¸ªåŸºäºå›¾å½¢çš„ä»»åŠ¡ï¼Œåˆ©ç”¨ä¸åŒçš„å›¾å½¢å¸ƒå±€ï¼ˆå¦‚Kamada-Kawaiä¸å¹³é¢å›¾ï¼‰æ¥æµ‹è¯•æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGAçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶ä¸“æ³¨äºè¡¨ç¤ºä¸å˜æ¨ç†çš„æŒ‘æˆ˜ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶æ¥æ¨åŠ¨AIè§†è§‰æ¨¡å‹å‘äººç±»æ¦‚å¿µåŒ–èƒ½åŠ›çš„è¿›æ­¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»»åŠ¡è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ ·çš„å›¾å½¢å¸ƒå±€å’Œç»“æ„ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸å—è§†è§‰å½¢å¼çš„å½±å“ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥å‡†ç¡®åæ˜ æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œäººç±»åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡ ä¹è¾¾åˆ°äº†å®Œç¾çš„å‡†ç¡®ç‡ï¼Œè€Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒæ„æ£€æµ‹ä¸Šå®Œå…¨å¤±è´¥ï¼Œè·¯å¾„/å¾ªç¯ä»»åŠ¡çš„æˆåŠŸç‡ä¹Ÿéå¸¸æœ‰é™ã€‚è¿™ä¸€ç»“æœæ­ç¤ºäº†å½“å‰AIæ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ ¹æœ¬æ€§å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è§†è§‰ç³»ç»Ÿã€æœºå™¨äººè§†è§‰ç†è§£å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡AIåœ¨è§†è§‰æ¦‚å¿µåŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œæœªæ¥å¯å®ç°æ›´è‡ªç„¶çš„äº¤äº’å’Œæ›´é«˜æ•ˆçš„è§†è§‰ä¿¡æ¯å¤„ç†ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in multimodal large language models have driven breakthroughs in visual question answering. Yet, a critical gap persists, `conceptualization'-the ability to recognize and reason about the same concept despite variations in visual form, a basic ability of human reasoning. To address this challenge, we introduce the Visual Graph Arena (VGA), a dataset featuring six graph-based tasks designed to evaluate and improve AI systems' capacity for visual abstraction. VGA uses diverse graph layouts (e.g., Kamada-Kawai vs. planar) to test reasoning independent of visual form. Experiments with state-of-the-art vision models and multimodal LLMs reveal a striking divide: humans achieved near-perfect accuracy across tasks, while models totally failed on isomorphism detection and showed limited success in path/cycle tasks. We further identify behavioral anomalies suggesting pseudo-intelligent pattern matching rather than genuine understanding. These findings underscore fundamental limitations in current AI models for visual understanding. By isolating the challenge of representation-invariant reasoning, the VGA provides a framework to drive progress toward human-like conceptualization in AI visual models. The Visual Graph Arena is available at: \href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}

