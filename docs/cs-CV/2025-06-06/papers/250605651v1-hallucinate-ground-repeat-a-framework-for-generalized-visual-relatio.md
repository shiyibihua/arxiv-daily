---
layout: default
title: Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection
---

# Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05651" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05651v1</a>
  <a href="https://arxiv.org/pdf/2506.05651.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05651v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05651v1', 'Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 22 pages, 9 figures, 5 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿­ä»£è§†è§‰åŸºç¡€æ¡†æ¶ä»¥è§£å†³è§†è§‰å…³ç³»æ£€æµ‹çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å…³ç³»æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼€æ”¾ä¸–ç•Œ` `åœºæ™¯ç†è§£` `æ™ºèƒ½ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰å…³ç³»æ£€æµ‹æ¨¡å‹ä¾èµ–å›ºå®šçš„è°“è¯é›†ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†æ–°é¢–çš„ç‰©ä½“å…³ç³»ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è¿­ä»£è§†è§‰åŸºç¡€æ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå€™é€‰å…³ç³»ï¼Œå¹¶é€šè¿‡è§†è§‰æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œå¢å¼ºäº†å…³ç³»ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¸åŒè®¾ç½®ä¸‹çš„å¹³å‡å¬å›ç‡åˆ†åˆ«ä¸º15.9ã€13.1å’Œ11.7ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMã€å°‘æ ·æœ¬å’Œå»ååŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£ç‰©ä½“ä¹‹é—´çš„å…³ç³»æ˜¯è§†è§‰æ™ºèƒ½çš„æ ¸å¿ƒï¼Œå¹¿æ³›åº”ç”¨äºå…·èº«äººå·¥æ™ºèƒ½ã€è¾…åŠ©ç³»ç»Ÿå’Œåœºæ™¯ç†è§£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è§†è§‰å…³ç³»æ£€æµ‹æ¨¡å‹ä¾èµ–å›ºå®šçš„è°“è¯é›†ï¼Œé™åˆ¶äº†å…¶å¯¹æ–°é¢–äº¤äº’çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¿­ä»£è§†è§‰åŸºç¡€æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç»“æ„åŒ–å…³ç³»å…ˆéªŒã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå€™é€‰åœºæ™¯å›¾å’Œè®­ç»ƒè§†è§‰æ¨¡å‹äº¤æ›¿è¿›è¡Œï¼Œè¶…è¶Šäº†æ³¨é‡Šæ•°æ®çš„é™åˆ¶ï¼Œå®ç°äº†å¯¹æœªè§è°“è¯çš„æ³›åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åœ¨Visual Genomeä¸Šå¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¼€æ”¾ä¸–ç•Œè§†è§‰å…³ç³»æ£€æµ‹åŸºå‡†ï¼ŒåŒ…å«21ä¸ªä¿ç•™è°“è¯ï¼Œå¹¶åœ¨ä¸åŒè®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è°“è¯åˆ†ç±»ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†åŸºäºåŸºç¡€çš„LLMå…ˆéªŒåœ¨å¯æ‰©å±•å¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰å…³ç³»æ£€æµ‹ä¸­å¯¹æ–°é¢–äº¤äº’çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å›ºå®šçš„è°“è¯é›†ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†æœªæ ‡æ³¨çš„è¯­ä¹‰å…³ç³»ï¼Œé™åˆ¶äº†æ¨¡å‹çš„åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è¿­ä»£è§†è§‰åŸºç¡€æ¡†æ¶é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå€™é€‰å…³ç³»ï¼Œå¹¶ä¸è§†è§‰æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿçš„æ³¨é‡Šæ•°æ®é™åˆ¶ï¼Œå®ç°å¯¹æœªè§è°“è¯çš„æ³›åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶é‡‡ç”¨ç±»ä¼¼æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰çš„æ–¹æ³•ï¼Œäº¤æ›¿è¿›è¡Œä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé¦–å…ˆä½¿ç”¨LLMç”Ÿæˆå€™é€‰åœºæ™¯å›¾ï¼ˆæœŸæœ›æ­¥éª¤ï¼‰ï¼Œç„¶åè®­ç»ƒè§†è§‰æ¨¡å‹ä»¥å¯¹é½è¿™äº›å‡è®¾ä¸æ„ŸçŸ¥è¯æ®ï¼ˆæœ€å¤§åŒ–æ­¥éª¤ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåˆ©ç”¨LLMä½œä¸ºç»“æ„åŒ–å…³ç³»å…ˆéªŒï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¨æµ‹å‡ºè¯­ä¹‰ä¸Šåˆç†çš„å…³ç³»ï¼Œä»è€Œå®ç°æ›´å¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å€™é€‰å…³ç³»ç”Ÿæˆçš„ç­–ç•¥ã€è§†è§‰æ¨¡å‹çš„è®­ç»ƒæŸå¤±å‡½æ•°ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆLLMè¾“å‡ºä¸è§†è§‰è¾“å…¥çš„ç½‘ç»œç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹åœ¨ä¸åŒè®¾ç½®ä¸‹çš„å¹³å‡å¬å›ç‡ï¼ˆmR@50ï¼‰åˆ†åˆ«ä¸º15.9ã€13.1å’Œ11.7ï¼Œæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨LLMã€å°‘æ ·æœ¬å­¦ä¹ å’Œå»ååŸºçº¿ï¼Œå±•ç¤ºäº†åŸºäºLLMçš„è§†è§‰å…³ç³»æ£€æµ‹åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å¤æ‚åœºæ™¯ä¸­çš„ç‰©ä½“å…³ç³»ï¼Œæå‡å†³ç­–èƒ½åŠ›å’Œäº¤äº’æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨åŠ¨å¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£çš„å‘å±•ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ„å»ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding relationships between objects is central to visual intelligence, with applications in embodied AI, assistive systems, and scene understanding. Yet, most visual relationship detection (VRD) models rely on a fixed predicate set, limiting their generalization to novel interactions. A key challenge is the inability to visually ground semantically plausible, but unannotated, relationships hypothesized from external knowledge. This work introduces an iterative visual grounding framework that leverages large language models (LLMs) as structured relational priors. Inspired by expectation-maximization (EM), our method alternates between generating candidate scene graphs from detected objects using an LLM (expectation) and training a visual model to align these hypotheses with perceptual evidence (maximization). This process bootstraps relational understanding beyond annotated data and enables generalization to unseen predicates. Additionally, we introduce a new benchmark for open-world VRD on Visual Genome with 21 held-out predicates and evaluate under three settings: seen, unseen, and mixed. Our model outperforms LLM-only, few-shot, and debiased baselines, achieving mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on these three sets. These results highlight the promise of grounded LLM priors for scalable open-world visual understanding.

