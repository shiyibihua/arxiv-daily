---
layout: default
title: You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping
---

# You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05719" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05719v1</a>
  <a href="https://arxiv.org/pdf/2506.05719.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05719v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05719v1', 'You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingshun Huang, Haitao Lin, Tianyu Wang, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: To appear in ICRA 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYOEOæ–¹æ³•ä»¥è§£å†³å…³èŠ‚ç‰©ä½“ç±»åˆ«çº§6Då§¿æ€ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å…³èŠ‚ç‰©ä½“` `6Då§¿æ€ä¼°è®¡` `æœºå™¨äººæŠ“å–` `å®æ—¶åé¦ˆ` `å•é˜¶æ®µæ–¹æ³•` `å®ä¾‹åˆ†å‰²` `NPCSè¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å…³èŠ‚ç‰©ä½“çš„ç±»åˆ«çº§å§¿æ€ä¼°è®¡ä¸­å­˜åœ¨é«˜è®¡ç®—æˆæœ¬å’Œå®æ—¶æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚
2. æå‡ºYOEOæ–¹æ³•ï¼Œé€šè¿‡å•é˜¶æ®µç½‘ç»œåŒæ—¶å®ç°å®ä¾‹åˆ†å‰²å’ŒNPCSè¡¨ç¤ºï¼Œç®€åŒ–äº†æµç¨‹å¹¶æé«˜äº†æ•ˆç‡ã€‚
3. åœ¨GAPartæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å®ç°200Hzçš„å®æ—¶åé¦ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å…³èŠ‚ç‰©ä½“çš„ç±»åˆ«çº§å§¿æ€ä¼°è®¡é—®é¢˜è¿›è¡Œç ”ç©¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¤æ‚çš„å¤šé˜¶æ®µæµç¨‹ï¼Œé¦–å…ˆåœ¨ç‚¹äº‘ä¸­åˆ†å‰²éƒ¨åˆ†å®ä¾‹ï¼Œç„¶åä¼°è®¡å½’ä¸€åŒ–éƒ¨åˆ†åæ ‡ç©ºé—´ï¼ˆNPCSï¼‰è¡¨ç¤ºä»¥è·å–6Då§¿æ€ã€‚è¿™äº›æ–¹æ³•åœ¨å®æ—¶æœºå™¨äººä»»åŠ¡ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œä½æ€§èƒ½çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†YOEOï¼Œä¸€ç§å•é˜¶æ®µæ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼åŒæ—¶è¾“å‡ºå®ä¾‹åˆ†å‰²å’ŒNPCSè¡¨ç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨ç»Ÿä¸€ç½‘ç»œç”Ÿæˆé€ç‚¹è¯­ä¹‰æ ‡ç­¾å’Œè´¨å¿ƒåç§»ï¼Œå…è®¸æ¥è‡ªåŒä¸€éƒ¨åˆ†å®ä¾‹çš„ç‚¹æŠ•ç¥¨ç»™ç›¸åŒçš„è´¨å¿ƒã€‚é€šè¿‡èšç±»ç®—æ³•åŒºåˆ†åŸºäºä¼°è®¡è´¨å¿ƒè·ç¦»çš„ç‚¹ï¼Œæœ€ç»ˆåˆ†ç¦»æ¯ä¸ªå®ä¾‹çš„NPCSåŒºåŸŸï¼Œå¹¶å°†å…¶ä¸çœŸå®ç‚¹äº‘å¯¹é½ä»¥æ¢å¤æœ€ç»ˆå§¿æ€å’Œå¤§å°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨GAPartæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„å§¿æ€ä¼°è®¡èƒ½åŠ›ï¼Œå¹¶åœ¨å®é™…ç¯å¢ƒä¸­ä»¥200Hzçš„é€Ÿåº¦æä¾›å®æ—¶è§†è§‰åé¦ˆï¼Œä½¿Kinovaæœºå™¨äººèƒ½å¤Ÿä¸æœªè§çš„å…³èŠ‚ç‰©ä½“è¿›è¡Œäº¤äº’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­å…³èŠ‚ç‰©ä½“çš„ç±»åˆ«çº§6Då§¿æ€ä¼°è®¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚çš„å¤šé˜¶æ®µæµç¨‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”å®æ—¶æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šYOEOæ–¹æ³•é€šè¿‡å•é˜¶æ®µç½‘ç»œè®¾è®¡ï¼Œèƒ½å¤ŸåŒæ—¶è¾“å‡ºå®ä¾‹åˆ†å‰²å’ŒNPCSè¡¨ç¤ºï¼Œä»è€Œç®€åŒ–äº†ä¼ ç»Ÿæ–¹æ³•çš„æµç¨‹ï¼Œæé«˜äº†å®æ—¶æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç»Ÿä¸€çš„ç½‘ç»œï¼Œè¯¥ç½‘ç»œç”Ÿæˆé€ç‚¹çš„è¯­ä¹‰æ ‡ç­¾å’Œè´¨å¿ƒåç§»ã€‚é€šè¿‡èšç±»ç®—æ³•ï¼ŒåŸºäºè´¨å¿ƒè·ç¦»å¯¹ç‚¹è¿›è¡ŒåŒºåˆ†ï¼Œæœ€ç»ˆåˆ†ç¦»å‡ºæ¯ä¸ªå®ä¾‹çš„NPCSåŒºåŸŸï¼Œå¹¶ä¸çœŸå®ç‚¹äº‘å¯¹é½ä»¥æ¢å¤å§¿æ€å’Œå¤§å°ã€‚

**å…³é”®åˆ›æ–°**ï¼šYOEOçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å•é˜¶æ®µè®¾è®¡ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿå¤šé˜¶æ®µæ–¹æ³•çš„é™åˆ¶ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„å§¿æ€ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ—¶åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

**å…³é”®è®¾è®¡**ï¼šç½‘ç»œç»“æ„é‡‡ç”¨äº†ç»Ÿä¸€çš„å·ç§¯ç½‘ç»œï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†è¯­ä¹‰åˆ†å‰²å’Œå§¿æ€ä¼°è®¡çš„è”åˆä¼˜åŒ–ï¼Œç¡®ä¿äº†è¾“å‡ºçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒYOEOæ–¹æ³•åœ¨GAPartæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„å§¿æ€ä¼°è®¡æ€§èƒ½ï¼Œæä¾›äº†é«˜è¾¾200Hzçš„å®æ—¶åé¦ˆï¼ŒæˆåŠŸä½¿Kinovaæœºå™¨äººä¸æœªè§çš„å…³èŠ‚ç‰©ä½“è¿›è¡Œäº¤äº’ã€‚è¿™ä¸€æ€§èƒ½åœ¨ä¸ä¼ ç»Ÿå¤šé˜¶æ®µæ–¹æ³•çš„å¯¹æ¯”ä¸­ï¼Œå±•ç°å‡ºæ˜æ˜¾çš„æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæŠ“å–ã€è‡ªåŠ¨åŒ–è£…é…å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡å®æ—¶å§¿æ€ä¼°è®¡ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ“ä½œå¤æ‚çš„å…³èŠ‚ç‰©ä½“ï¼Œæå‡å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the problem of category-level pose estimation for articulated objects in robotic manipulation tasks. Recent works have shown promising results in estimating part pose and size at the category level. However, these approaches primarily follow a complex multi-stage pipeline that first segments part instances in the point cloud and then estimates the Normalized Part Coordinate Space (NPCS) representation for 6D poses. These approaches suffer from high computational costs and low performance in real-time robotic tasks. To address these limitations, we propose YOEO, a single-stage method that simultaneously outputs instance segmentation and NPCS representations in an end-to-end manner. We use a unified network to generate point-wise semantic labels and centroid offsets, allowing points from the same part instance to vote for the same centroid. We further utilize a clustering algorithm to distinguish points based on their estimated centroid distances. Finally, we first separate the NPCS region of each instance. Then, we align the separated regions with the real point cloud to recover the final pose and size. Experimental results on the GAPart dataset demonstrate the pose estimation capabilities of our proposed single-shot method. We also deploy our synthetically-trained model in a real-world setting, providing real-time visual feedback at 200Hz, enabling a physical Kinova robot to interact with unseen articulated objects. This showcases the utility and effectiveness of our proposed method.

