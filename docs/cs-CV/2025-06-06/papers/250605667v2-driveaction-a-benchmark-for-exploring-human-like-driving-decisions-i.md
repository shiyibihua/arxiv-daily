---
layout: default
title: DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models
---

# DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05667" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05667v2</a>
  <a href="https://arxiv.org/pdf/2506.05667.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05667v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05667v2', 'DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong Qin, Mofan Zhou, Yifei Zhan, Xianpeng Lang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-09-26)

**å¤‡æ³¨**: Benchmark: https://huggingface.co/datasets/LiAuto-DriveAction/drive-action

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDriveActionåŸºå‡†ä»¥è§£å†³VLAæ¨¡å‹å†³ç­–å¤šæ ·æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-è¡ŒåŠ¨` `è‡ªåŠ¨é©¾é©¶` `åŸºå‡†æµ‹è¯•` `å¤šæ¨¡æ€å­¦ä¹ ` `åŠ¨ä½œé¢„æµ‹` `è¯„ä¼°æ¡†æ¶` `çœŸå®ä¸–ç•Œæ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAæ¨¡å‹åŸºå‡†ç¼ºä¹åœºæ™¯å¤šæ ·æ€§å’Œå¯é çš„åŠ¨ä½œæ³¨é‡Šï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚
2. DriveActionåŸºå‡†é€šè¿‡çœŸå®é©¾é©¶æ•°æ®ç”Ÿæˆé—®ç­”å¯¹ï¼Œæä¾›é«˜è´¨é‡çš„åŠ¨ä½œæ ‡ç­¾ï¼Œå¹¶å»ºç«‹äº†æ ‘çŠ¶è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§†è§‰å’Œè¯­è¨€è¾“å…¥å¯¹åŠ¨ä½œé¢„æµ‹è‡³å…³é‡è¦ï¼Œç¼ºå¤±ä»»ä¸€è¾“å…¥éƒ½ä¼šæ˜¾è‘—é™ä½å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†ç¼ºä¹åœºæ™¯å¤šæ ·æ€§ã€å¯é çš„åŠ¨ä½œçº§åˆ«æ³¨é‡Šå’Œä¸äººç±»åå¥½ä¸€è‡´çš„è¯„ä¼°åè®®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DriveActionï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºVLAæ¨¡å‹è®¾è®¡çš„ä»¥åŠ¨ä½œä¸ºé©±åŠ¨çš„åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ª2610ä¸ªé©¾é©¶åœºæ™¯çš„16185ä¸ªé—®ç­”å¯¹ã€‚DriveActionåˆ©ç”¨çœŸå®ä¸–ç•Œçš„é©¾é©¶æ•°æ®ï¼Œç¡®ä¿å¹¿æ³›ä¸”å…·æœ‰ä»£è¡¨æ€§çš„åœºæ™¯è¦†ç›–ï¼Œæä¾›ç›´æ¥æ¥è‡ªé©¾é©¶å‘˜å®é™…é©¾é©¶æ“ä½œçš„é«˜æ°´å¹³ç¦»æ•£åŠ¨ä½œæ ‡ç­¾ï¼Œå¹¶å®æ–½ä¸€ä¸ªä»¥åŠ¨ä½œä¸ºæ ¹çš„æ ‘çŠ¶è¯„ä¼°æ¡†æ¶ï¼Œæ˜ç¡®é“¾æ¥è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä»»åŠ¡ï¼Œæ”¯æŒå…¨é¢å’Œä»»åŠ¡ç‰¹å®šçš„è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å‡†ç¡®çš„åŠ¨ä½œé¢„æµ‹ä¸­éœ€è¦è§†è§‰å’Œè¯­è¨€çš„æŒ‡å¯¼ï¼šæ²¡æœ‰è§†è§‰è¾“å…¥æ—¶ï¼Œå‡†ç¡®ç‡å¹³å‡ä¸‹é™3.3%ï¼›æ²¡æœ‰è¯­è¨€è¾“å…¥æ—¶ä¸‹é™4.1%ï¼›æ²¡æœ‰ä»»ä½•è¾“å…¥æ—¶ä¸‹é™8.0%ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ”¯æŒç²¾ç¡®è¯†åˆ«æ¨¡å‹ç“¶é¢ˆï¼Œæä¾›æ–°çš„è§è§£å’Œä¸¥æ ¼çš„åŸºç¡€ï¼Œä»¥æ¨åŠ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„ç±»äººå†³ç­–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰VLAæ¨¡å‹åŸºå‡†åœ¨åœºæ™¯å¤šæ ·æ€§ã€åŠ¨ä½œæ³¨é‡Šå¯é æ€§å’Œè¯„ä¼°åè®®ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDriveActionåŸºå‡†ï¼Œé€šè¿‡çœŸå®ä¸–ç•Œçš„é©¾é©¶æ•°æ®ç”Ÿæˆé—®ç­”å¯¹ï¼Œç¡®ä¿åœºæ™¯çš„å¹¿æ³›è¦†ç›–ï¼Œå¹¶æä¾›é«˜è´¨é‡çš„åŠ¨ä½œæ ‡ç­¾ï¼Œè¿›è€Œå®ç°æ›´å‡†ç¡®çš„åŠ¨ä½œé¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDriveActionçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é—®ç­”å¯¹ç”Ÿæˆã€åŠ¨ä½œæ ‡ç­¾æ ‡æ³¨å’Œæ ‘çŠ¶è¯„ä¼°æ¡†æ¶ï¼Œæ˜ç¡®é“¾æ¥è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä»»åŠ¡ï¼Œæ”¯æŒå…¨é¢è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDriveActionæ˜¯é¦–ä¸ªä»¥åŠ¨ä½œä¸ºé©±åŠ¨çš„åŸºå‡†ï¼Œåˆ›æ–°æ€§åœ°ç»“åˆäº†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä»»åŠ¡çš„è¯„ä¼°ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ”¶é›†é˜¶æ®µï¼Œé‡‡ç”¨çœŸå®é©¾é©¶æ•°æ®ï¼Œç¡®ä¿åœºæ™¯çš„å¤šæ ·æ€§ï¼›åœ¨è¯„ä¼°æ¡†æ¶ä¸­ï¼Œè®¾è®¡äº†æ ‘çŠ¶ç»“æ„ä»¥æ”¯æŒä»»åŠ¡ç‰¹å®šè¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç¼ºå¤±è§†è§‰è¾“å…¥æ—¶ï¼Œå‡†ç¡®ç‡å¹³å‡ä¸‹é™3.3%ï¼›ç¼ºå¤±è¯­è¨€è¾“å…¥æ—¶ä¸‹é™4.1%ï¼›è€Œç¼ºå¤±ä»»ä¸€è¾“å…¥æ—¶ï¼Œå‡†ç¡®ç‡ä¸‹é™é«˜è¾¾8.0%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†è§†è§‰å’Œè¯­è¨€è¾“å…¥åœ¨å‡†ç¡®åŠ¨ä½œé¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DriveActionåŸºå‡†å¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¼€å‘ä¸è¯„ä¼°ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–VLAæ¨¡å‹çš„å†³ç­–èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†æœ‰æœ›æ¨åŠ¨æ›´æ™ºèƒ½çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ï¼Œæå‡é“è·¯å®‰å…¨æ€§å’Œé©¾é©¶ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by drivers of autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from drivers' actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving.

