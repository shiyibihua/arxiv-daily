---
layout: default
title: Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025
---

# Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05856" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05856v1</a>
  <a href="https://arxiv.org/pdf/2506.05856.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05856v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05856v1', 'Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqian Fu, Runze Wang, Yanwei Fu, Danda Pani Paudel, Luc Van Gool

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: The 2nd Price Award of EgoExo4D Relations, Second Joint EgoVis Workshop with CVPR2025, technical report paper is accepted by CVPRW 25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lovelyqian/ObjectRelator)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨è§†è§’å¤šæ¨¡æ€ç‰©ä½“åˆ†å‰²æ–¹æ³•ä»¥è§£å†³Ego-Exo4DæŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åˆ†å‰²` `ç‰©ä½“å¯¹åº”` `è·¨è§†è§’å¯¹é½` `è§†è§‰æ©è†œ` `æ–‡æœ¬æè¿°` `Ego-Exo4D` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è‡ªæˆ‘è§†è§’ä¸å¤–éƒ¨è§†è§’ä¹‹é—´çš„ç‰©ä½“å¯¹åº”ä»»åŠ¡æ—¶ï¼Œé¢ä¸´è§†è§‰é¢†åŸŸå·®è·å’Œç‰©ä½“å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡ä»¶èåˆæ¨¡å—ï¼Œç»“åˆè§†è§‰ä¿¡æ¯å’Œæ–‡æœ¬æè¿°æ¥å¢å¼ºç‰©ä½“çš„å®šä½ç²¾åº¦ï¼ŒåŒæ—¶å¼•å…¥è·¨è§†è§’ç‰©ä½“å¯¹é½æ¨¡å—ä»¥æé«˜é²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨Ego-Exo4Dç‰©ä½“å¯¹åº”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ç¬¬äºŒåçš„ä¼˜å¼‚æˆç»©ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨è§†è§’å¤šæ¨¡æ€ç‰©ä½“åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³Ego-Exo4Då¯¹åº”æŒ‘æˆ˜ä¸­çš„ç‰©ä½“å¯¹åº”ä»»åŠ¡ã€‚ç»™å®šæ¥è‡ªä¸€ä¸ªè§†è§’ï¼ˆä¾‹å¦‚è‡ªæˆ‘è§†è§’ï¼‰çš„ç‰©ä½“æŸ¥è¯¢ï¼Œç›®æ ‡æ˜¯é¢„æµ‹åœ¨å¦ä¸€ä¸ªè§†è§’ï¼ˆä¾‹å¦‚å¤–éƒ¨è§†è§’ï¼‰ä¸­çš„å¯¹åº”ç‰©ä½“æ©è†œã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡ä»¶èåˆæ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰æ©è†œå’Œæ–‡æœ¬æè¿°ä½œä¸ºåˆ†å‰²æ¡ä»¶æ¥å¢å¼ºç‰©ä½“å®šä½ã€‚æ­¤å¤–ï¼Œä¸ºäº†åº”å¯¹è‡ªæˆ‘è§†è§’å’Œå¤–éƒ¨è§†è§’ä¹‹é—´çš„è§†è§‰é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨è§†è§’ç‰©ä½“å¯¹é½æ¨¡å—ï¼Œå¼ºåˆ¶å®ç°è§†è§’é—´çš„ç‰©ä½“çº§ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹è§†è§’å˜åŒ–çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§è§„æ¨¡Ego-Exo4Dç‰©ä½“å¯¹åº”åŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬äºŒã€‚ä»£ç å°†å‘å¸ƒåœ¨https://github.com/lovelyqian/ObjectRelatorã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³Ego-Exo4DæŒ‘æˆ˜ä¸­çš„ç‰©ä½“å¯¹åº”ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•åœ¨è‡ªæˆ‘è§†è§’ä¸å¤–éƒ¨è§†è§’ä¹‹é—´å­˜åœ¨è§†è§‰é¢†åŸŸå·®è·ï¼Œå¯¼è‡´ç‰©ä½“å®šä½ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¡ä»¶èåˆæ¨¡å—ï¼Œé€šè¿‡ç»“åˆè§†è§‰æ©è†œå’Œæ–‡æœ¬æè¿°æ¥å¢å¼ºç‰©ä½“çš„å®šä½èƒ½åŠ›ï¼ŒåŒæ—¶å¼•å…¥è·¨è§†è§’ç‰©ä½“å¯¹é½æ¨¡å—ä»¥ç¡®ä¿ä¸åŒè§†è§’é—´çš„ç‰©ä½“ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šæ¨¡æ€æ¡ä»¶èåˆæ¨¡å—å’Œè·¨è§†è§’ç‰©ä½“å¯¹é½æ¨¡å—ã€‚å‰è€…ç”¨äºå¢å¼ºç‰©ä½“å®šä½ï¼Œåè€…ç”¨äºè§£å†³è§†è§’é—´çš„ä¸€è‡´æ€§é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šæ¨¡æ€æ¡ä»¶èåˆå’Œè·¨è§†è§’å¯¹é½çš„ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„é²æ£’æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è§†è§’å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰©ä½“æ©è†œçš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„ä»¥é€‚åº”å¤šæ¨¡æ€è¾“å…¥ï¼Œç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Ego-Exo4Dç‰©ä½“å¯¹åº”åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†ç¬¬äºŒåçš„æˆç»©ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰©ä½“åˆ†å‰²ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨ç‰©ä½“å®šä½ç²¾åº¦ä¸Šæœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€æ¡ä»¶èåˆå’Œè·¨è§†è§’å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰å’Œå¢å¼ºç°å®ç­‰åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå‡†ç¡®çš„ç‰©ä½“è¯†åˆ«å’Œå®šä½å¯¹äºå®ç°å®‰å…¨å’Œé«˜æ•ˆçš„æ“ä½œè‡³å…³é‡è¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€å­¦ä¹ å’Œè·¨è§†è§’ç†è§£çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this report, we present a cross-view multi-modal object segmentation approach for the object correspondence task in the Ego-Exo4D Correspondence Challenges 2025. Given object queries from one perspective (e.g., ego view), the goal is to predict the corresponding object masks in another perspective (e.g., exo view). To tackle this task, we propose a multimodal condition fusion module that enhances object localization by leveraging both visual masks and textual descriptions as segmentation conditions. Furthermore, to address the visual domain gap between ego and exo views, we introduce a cross-view object alignment module that enforces object-level consistency across perspectives, thereby improving the model's robustness to viewpoint changes. Our proposed method ranked second on the leaderboard of the large-scale Ego-Exo4D object correspondence benchmark. Code will be made available at https://github.com/lovelyqian/ObjectRelator.

